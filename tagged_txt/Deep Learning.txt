deep	O
learning	O
website	O
www.deeplearningbook.org	O
this	O
book	O
is	O
accompanied	O
by	O
the	O
above	O
website	O
.	O
the	O
website	O
provides	O
a	O
variety	O
of	O
supplementary	O
material	O
,	O
including	O
exercises	O
,	O
lecture	O
slides	O
,	O
corrections	O
of	O
mistakes	O
,	O
and	O
other	O
resources	O
that	O
should	O
be	O
useful	O
to	O
both	O
readers	O
and	O
instructors	O
.	O
vii	O
acknowledgments	O
this	O
book	O
would	O
not	O
have	O
been	O
possible	O
without	O
the	O
contributions	O
of	O
many	O
people	O
.	O
we	O
would	O
like	O
to	O
thank	O
those	O
who	O
commented	O
on	O
our	O
proposal	O
for	O
the	O
book	O
and	O
helped	O
plan	O
its	O
contents	O
and	O
organization	O
:	O
guillaume	O
alain	O
,	O
kyunghyun	O
cho	O
,	O
çağlar	O
gülçehre	O
,	O
david	O
krueger	O
,	O
hugo	O
larochelle	O
,	O
razvan	O
pascanu	O
and	O
thomas	O
rohée	O
.	O
we	O
would	O
like	O
to	O
thank	O
the	O
people	O
who	O
oﬀered	O
feedback	O
on	O
the	O
content	O
of	O
the	O
book	O
itself	O
.	O
some	O
oﬀered	O
feedback	O
on	O
many	O
chapters	O
:	O
martín	O
abadi	O
,	O
guillaume	O
alain	O
,	O
ion	O
androutsopoulos	O
,	O
fred	O
bertsch	O
,	O
olexa	O
bilaniuk	O
,	O
ufuk	O
can	O
biçici	O
,	O
matko	O
bošnjak	O
,	O
john	O
boersma	O
,	O
greg	O
brockman	O
,	O
alexandre	O
de	O
brébisson	O
,	O
pierre	O
luc	O
carrier	O
,	O
sarath	O
chandar	O
,	O
pawel	O
chilinski	O
,	O
mark	O
daoust	O
,	O
oleg	O
dashevskii	O
,	O
laurent	O
dinh	O
,	O
stephan	O
dreseitl	O
,	O
jim	O
fan	O
,	O
miao	O
fan	O
,	O
meire	O
fortunato	O
,	O
frédéric	O
francis	O
,	O
nando	O
de	O
freitas	O
,	O
çağlar	O
gülçehre	O
,	O
jurgen	O
van	O
gael	O
,	O
javier	O
alonso	O
garcía	O
,	O
jonathan	O
hunt	O
,	O
gopi	O
jeyaram	O
,	O
chingiz	O
kabytayev	O
,	O
lukasz	O
kaiser	O
,	O
varun	O
kanade	O
,	O
asifullah	O
khan	O
,	O
akiel	O
khan	O
,	O
john	O
king	O
,	O
diederik	O
p.	O
kingma	O
,	O
yann	O
lecun	O
,	O
rudolf	O
mathey	O
,	O
matías	O
mattamala	O
,	O
abhinav	O
maurya	O
,	O
kevin	O
murphy	O
,	O
oleg	O
mürk	O
,	O
roman	O
novak	O
,	O
augustus	O
q.	O
odena	O
,	O
simon	O
pavlik	O
,	O
karl	O
pichotta	O
,	O
eddie	O
pierce	O
,	O
kari	O
pulli	O
,	O
roussel	O
rahman	O
,	O
tapani	O
raiko	O
,	O
anurag	O
ranjan	O
,	O
johannes	O
roith	O
,	O
mihaela	O
rosca	O
,	O
halis	O
sak	O
,	O
césar	O
salgado	O
,	O
grigory	O
sapunov	O
,	O
yoshinori	O
sasaki	O
,	O
mike	O
schuster	O
,	O
julian	O
serban	O
,	O
nir	O
shabat	O
,	O
ken	O
shirriﬀ	O
,	O
andre	O
simpelo	O
,	O
scott	O
stanley	O
,	O
david	O
sussillo	O
,	O
ilya	O
sutskever	O
,	O
carles	O
gelada	O
sáez	O
,	O
graham	O
taylor	O
,	O
valentin	O
tolmer	O
,	O
massimiliano	O
tomassoli	O
,	O
an	O
tran	O
,	O
shubhendu	O
trivedi	O
,	O
alexey	O
umnov	O
,	O
vincent	O
vanhoucke	O
,	O
marco	O
visentini-scarzanella	O
,	O
martin	O
vita	O
,	O
david	O
warde-farley	O
,	O
dustin	O
webb	O
,	O
kelvin	O
xu	O
,	O
wei	O
xue	O
,	O
ke	O
yang	O
,	O
li	O
yao	O
,	O
zygmunt	O
zając	O
and	O
ozan	O
çağlayan	O
.	O
we	O
would	O
also	O
like	O
to	O
thank	O
those	O
who	O
provided	O
us	O
with	O
useful	O
feedback	O
on	O
individual	O
chapters	O
:	O
•	O
•	O
notation	O
:	O
zhang	O
yuanhang	O
.	O
chapter	O
,	O
1	O
introduction	O
:	O
yusuf	O
akgul	O
,	O
sebastien	O
bratieres	O
,	O
samira	O
ebrahimi	O
,	O
viii	O
contents	O
•	O
•	O
•	O
•	O
•	O
•	O
•	O
•	O
•	O
•	O
•	O
•	O
charlie	O
gorichanaz	O
,	O
brendan	O
loudermilk	O
,	O
eric	O
morris	O
,	O
cosmin	O
pârvulescu	O
and	O
alfredo	O
solano	O
.	O
,	O
2	O
linear	O
algebra	O
chapter	O
:	O
amjad	O
almahairi	O
,	O
nikola	O
banić	O
,	O
kevin	O
bennett	O
,	O
philippe	O
castonguay	O
,	O
oscar	O
chang	O
,	O
eric	O
fosler-lussier	O
,	O
andrey	O
khalyavin	O
,	O
sergey	O
oreshkov	O
,	O
istván	O
petrás	O
,	O
dennis	O
prangle	O
,	O
thomas	O
rohée	O
,	O
gitanjali	O
gulve	O
sehgal	O
,	O
colby	O
toland	O
,	O
alessandro	O
vitale	O
and	O
bob	O
welland	O
.	O
,	O
3	O
probability	O
and	O
information	O
theory	O
chapter	O
:	O
john	O
philip	O
anderson	O
,	O
kai	O
arulkumaran	O
,	O
vincent	O
dumoulin	O
,	O
rui	O
fa	O
,	O
stephan	O
gouws	O
,	O
artem	O
oboturov	O
,	O
antti	O
rasmus	O
,	O
alexey	O
surkov	O
and	O
volker	O
tresp	O
.	O
chapter	O
,	O
yuhuang	O
.	O
4	O
numerical	O
computation	O
:	O
tran	O
lam	O
anian	O
fischer	O
and	O
hu	O
,	O
5	O
machine	O
learning	O
basics	O
chapter	O
:	O
dzmitry	O
bahdanau	O
,	O
justin	O
domingue	O
,	O
nikhil	O
garg	O
,	O
makoto	O
otsuka	O
,	O
bob	O
pepin	O
,	O
philip	O
popien	O
,	O
emmanuel	O
rayner	O
,	O
peter	O
shepard	O
,	O
kee-bong	O
song	O
,	O
zheng	O
sun	O
and	O
andy	O
wu	O
.	O
chapter	O
,6	O
deep	O
feedforward	O
networks	O
:	O
uriel	O
berdugo	O
,	O
fabrizio	O
bottarel	O
,	O
elizabeth	O
burl	O
,	O
ishan	O
durugkar	O
,	O
jeﬀ	O
hlywa	O
,	O
jong	O
wook	O
kim	O
,	O
david	O
krueger	O
and	O
aditya	O
kumar	O
praharaj	O
.	O
chapter	O
,	O
inkyu	O
lee	O
,	O
sunil	O
mohan	O
,	O
hai	O
phong	O
phan	O
and	O
joshua	O
salisbury	O
.	O
7	O
regularization	O
for	O
deep	O
learning	O
:	O
morten	O
kolbæk	O
,	O
kshitij	O
lauria	O
,	O
chapter	O
,8	O
optimization	O
for	O
training	O
deep	O
models	O
:	O
marcel	O
ackermann	O
,	O
peter	O
armitage	O
,	O
rowel	O
atienza	O
,	O
andrew	O
brock	O
,	O
tegan	O
maharaj	O
,	O
james	O
martens	O
,	O
kashif	O
rasul	O
,	O
klaus	O
strobl	O
and	O
nicholas	O
turner	O
.	O
chapter	O
,9	O
convolutional	O
networks	O
:	O
martín	O
arjovsky	O
,	O
eugene	O
brevdo	O
,	O
kon-	O
stantin	O
divilov	O
,	O
eric	O
jensen	O
,	O
mehdi	O
mirza	O
,	O
alex	O
paino	O
,	O
marjorie	O
sayer	O
,	O
ryan	O
stout	O
and	O
wentao	O
wu	O
.	O
chapter	O
,10	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
:	O
gökçen	O
eraslan	O
,	O
steven	O
hickson	O
,	O
razvan	O
pascanu	O
,	O
lorenzo	O
von	O
ritter	O
,	O
rui	O
rodrigues	O
,	O
dmitriy	O
serdyuk	O
,	O
dongyu	O
shi	O
and	O
kaiyu	O
yang	O
.	O
chapter	O
chapter	O
roscher	O
.	O
11	O
practical	O
methodology	O
,	O
:	O
daniel	O
beckstein	O
.	O
12	O
applications	O
,	O
:	O
george	O
dahl	O
,	O
vladimir	O
nekrasov	O
and	O
ribana	O
chapter	O
,13	O
linear	O
factor	O
models	O
:	O
jayanth	O
koushik	O
.	O
ix	O
contents	O
•	O
•	O
•	O
•	O
•	O
•	O
chapter	O
15	O
representation	O
learning	O
,	O
:	O
kunal	O
ghosh	O
.	O
chapter	O
and	O
anton	O
varfolom	O
.	O
16	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
,	O
:	O
minh	O
lê	O
chapter	O
,18	O
confronting	O
the	O
partition	O
function	O
:	O
sam	O
bowman	O
.	O
chapter	O
19	O
approximate	O
inference	O
,	O
:	O
yujia	O
bao	O
.	O
chapter	O
wenming	O
ma	O
,	O
fady	O
medhat	O
,	O
shakir	O
mohamed	O
and	O
grégoire	O
montavon	O
.	O
,20	O
deep	O
generative	O
models	O
:	O
nicolas	O
chapados	O
,	O
daniel	O
galvez	O
,	O
bibliography	O
:	O
lukas	O
michelbacher	O
and	O
leslie	O
n.	O
smith	O
.	O
we	O
also	O
want	O
to	O
thank	O
those	O
who	O
allowed	O
us	O
to	O
reproduce	O
images	O
,	O
ﬁgures	O
or	O
data	O
from	O
their	O
publications	O
.	O
we	O
indicate	O
their	O
contributions	O
in	O
the	O
ﬁgure	O
captions	O
throughout	O
the	O
text	O
.	O
we	O
would	O
like	O
to	O
thank	O
lu	O
wang	O
for	O
writing	O
pdf2htmlex	O
,	O
which	O
we	O
used	O
to	O
make	O
the	O
web	O
version	O
of	O
the	O
book	O
,	O
and	O
for	O
oﬀering	O
support	O
to	O
improve	O
the	O
quality	O
of	O
the	O
resulting	O
html	O
.	O
we	O
would	O
like	O
to	O
thank	O
ian	O
’	O
s	O
wife	O
daniela	O
flori	O
goodfellow	O
for	O
patiently	O
supporting	O
ian	O
during	O
the	O
writing	O
of	O
the	O
book	O
as	O
well	O
as	O
for	O
help	O
with	O
proofreading	O
.	O
we	O
would	O
like	O
to	O
thank	O
the	O
google	O
brain	O
team	O
for	O
providing	O
an	O
intellectual	O
environment	O
where	O
ian	O
could	O
devote	O
a	O
tremendous	O
amount	O
of	O
time	O
to	O
writing	O
this	O
book	O
and	O
receive	O
feedback	O
and	O
guidance	O
from	O
colleagues	O
.	O
we	O
would	O
especially	O
like	O
to	O
thank	O
ian	O
’	O
s	O
former	O
manager	O
,	O
greg	O
corrado	O
,	O
and	O
his	O
current	O
manager	O
,	O
samy	O
bengio	O
,	O
for	O
their	O
support	O
of	O
this	O
project	O
.	O
finally	O
,	O
we	O
would	O
like	O
to	O
thank	O
geoﬀrey	O
hinton	O
for	O
encouragement	O
when	O
writing	O
was	O
diﬃcult	O
.	O
x	O
notation	O
this	O
section	O
provides	O
a	O
concise	O
reference	O
describing	O
the	O
notation	O
used	O
throughout	O
this	O
book	O
.	O
if	O
you	O
are	O
unfamiliar	O
with	O
any	O
of	O
the	O
corresponding	O
mathematical	O
concepts	O
,	O
we	O
describe	O
most	O
of	O
these	O
ideas	O
in	O
chapters	O
2–4	O
.	O
a	O
a	O
a	O
a	O
in	O
i	O
e	O
(	O
)	O
i	O
numbers	O
and	O
arrays	O
a	O
scalar	O
(	O
integer	O
or	O
real	O
)	O
a	O
vector	O
a	O
matrix	O
a	O
tensor	O
identity	O
matrix	O
with	O
n	O
rows	O
and	O
n	O
columns	O
identity	O
matrix	O
with	O
dimensionality	O
implied	O
by	O
context	O
standard	O
basis	O
vector	O
[	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
,	O
1	O
,	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
]	O
with	O
a	O
1	O
at	O
position	B
i	O
diag	O
(	O
)	O
a	O
a	O
square	O
,	O
diagonal	O
matrix	O
with	O
diagonal	O
entries	O
given	O
by	O
a	O
a	O
scalar	O
random	O
variable	O
a	O
vector-valued	O
random	O
variable	O
a	O
matrix-valued	O
random	O
variable	O
a	O
a	O
a	O
xi	O
contents	O
a	O
a	O
set	O
sets	O
and	O
graphs	O
r	O
{	O
}	O
0	O
1	O
,	O
{	O
0	O
1	O
,	O
}	O
,	O
.	O
.	O
.	O
,	O
n	O
[	O
]	O
a	O
,	O
b	O
]	O
a	O
,	O
b	O
(	O
\	O
a	O
b	O
g	O
the	O
set	O
of	O
real	O
numbers	O
the	O
set	O
containing	O
0	O
and	O
1	O
the	O
set	O
of	O
all	O
integers	O
between	O
0	O
and	O
n	O
the	O
real	O
interval	O
including	O
a	O
and	O
b	O
the	O
real	O
interval	O
excluding	O
a	O
but	O
including	O
b	O
set	O
subtraction	O
,	O
i.e.	O
,	O
the	O
set	O
containing	O
the	O
ele-	O
ments	O
of	O
that	O
are	O
not	O
in	O
a	O
b	O
a	O
graph	O
p	O
ag	O
(	O
xi	O
)	O
the	O
parents	O
of	O
xi	O
in	O
g	O
indexing	O
ai	O
a−	O
i	O
ai	O
,	O
j	O
element	O
i	O
of	O
vector	O
a	O
,	O
with	O
indexing	O
starting	O
at	O
1	O
all	O
elements	O
of	O
vector	O
a	O
except	O
for	O
element	O
i	O
element	O
i	O
,	O
j	O
of	O
matrix	O
a	O
ai	O
,	O
:	O
row	O
of	O
matrix	O
i	O
a	O
a	O
:	O
,i	O
column	O
of	O
matrix	O
i	O
a	O
ai	O
,	O
j	O
,	O
k	O
element	O
(	O
i	O
,	O
j	O
,	O
k	O
)	O
of	O
a	O
3-d	O
tensor	O
a	O
a	O
:	O
:	O
,	O
,	O
i	O
ai	O
	O
a	O
a+	O
	O
a	O
b	O
2-d	O
slice	O
of	O
a	O
3-d	O
tensor	O
element	O
of	O
the	O
random	O
vector	O
i	O
a	O
linear	O
algebra	O
operations	O
transpose	O
of	O
matrix	O
a	O
moore-penrose	O
pseudoinverse	O
of	O
a	O
element-wise	O
(	O
hadamard	O
)	O
product	O
of	O
anda	O
b	O
det	O
(	O
)	O
a	O
determinant	O
of	O
a	O
xii	O
contents	O
dy	O
dx	O
∂y	O
∇	O
∂x	O
xy	O
∇	O
x	O
y	O
∇	O
y	O
x	O
∂f	O
∂x	O
	O
	O
∇2	O
xf	O
calculus	O
derivative	O
of	O
with	O
respect	O
to	O
y	O
x	O
partial	O
derivative	O
of	O
with	O
respect	O
to	O
y	O
x	O
gradient	O
of	O
with	O
respect	O
to	O
y	O
x	O
matrix	O
derivatives	O
of	O
with	O
respect	O
to	O
y	O
x	O
tensor	O
containing	O
derivatives	O
of	O
y	O
with	O
respect	O
to	O
x	O
jacobian	O
matrix	O
j	O
∈	O
×	O
m	O
n	O
r	O
→	O
m	O
r	O
n	O
of	O
f	O
:	O
r	O
the	O
hessian	O
matrix	O
of	O
f	O
at	O
input	O
point	O
x	O
x	O
or	O
h	O
)	O
(	O
)	O
x	O
(	O
)	O
f	O
(	O
f	O
(	O
)	O
x	O
x	O
d	O
deﬁnite	O
integral	O
over	O
the	O
entire	O
domain	O
of	O
x	O
f	O
(	O
)	O
x	O
x	O
d	O
deﬁnite	O
integral	O
with	O
respect	O
to	O
x	O
over	O
the	O
set	O
s	O
s	O
⊥	O
a	O
b	O
⊥	O
|	O
a	O
b	O
c	O
p	O
(	O
)	O
a	O
p	O
(	O
)	O
a	O
∼	O
a	O
p	O
probability	O
and	O
information	O
theory	O
the	O
random	O
variables	O
a	O
and	O
b	O
are	O
independent	O
they	O
are	O
conditionally	O
independent	O
given	O
c	O
a	O
probability	O
distribution	O
over	O
a	O
discrete	O
variable	O
a	O
probability	O
distribution	O
over	O
a	O
continuous	O
vari-	O
able	O
,	O
or	O
over	O
a	O
variable	O
whose	O
type	O
has	O
not	O
been	O
speciﬁed	O
random	O
variable	O
a	O
has	O
distribution	O
p	O
∼	O
p	O
[	O
(	O
)	O
]	O
ex	O
f	O
x	O
or	O
ef	O
x	O
(	O
)	O
expectation	O
of	O
f	O
x	O
with	O
respect	O
to	O
p	O
x	O
(	O
)	O
(	O
)	O
var	O
(	O
(	O
)	O
)	O
f	O
x	O
variance	O
of	O
f	O
x	O
(	O
)	O
under	O
x	O
p	O
(	O
)	O
cov	O
(	O
(	O
)	O
f	O
x	O
,	O
g	O
x	O
(	O
)	O
)	O
h	O
(	O
)	O
x	O
	O
p	O
q	O
)	O
dkl	O
(	O
n	O
(	O
;	O
)	O
x	O
µ	O
,	O
σ	O
covariance	O
of	O
f	O
x	O
(	O
)	O
and	O
g	O
x	O
(	O
)	O
under	O
x	O
p	O
(	O
)	O
shannon	O
entropy	O
of	O
the	O
random	O
variable	O
x	O
kullback-leibler	O
divergence	O
of	O
p	O
and	O
q	O
gaussian	O
distribution	O
over	O
x	O
with	O
mean	O
µ	O
and	O
covariance	O
σ	O
xiii	B
contents	O
→	O
g	O
f	O
:	O
a	O
◦	O
f	O
functions	O
b	O
the	O
function	O
with	O
domain	O
f	O
and	O
range	O
b	O
a	O
composition	O
of	O
the	O
functions	O
f	O
and	O
g	O
f	O
(	O
;	O
)	O
x	O
θ	O
a	O
function	O
of	O
x	O
parametrized	O
by	O
θ	O
.	O
(	O
sometimes	O
we	O
write	O
f	O
(	O
x	O
)	O
and	O
omit	O
the	O
argument	O
θ	O
to	O
lighten	O
notation	O
)	O
log	O
x	O
σ	O
x	O
(	O
)	O
ζ	O
x	O
(	O
)	O
||	O
||	O
x	O
p	O
||	O
||	O
x	O
x+	O
natural	O
logarithm	O
of	O
x	O
logistic	O
sigmoid	O
,	O
1	O
−	O
)	O
1	O
+	O
exp	O
(	O
x	O
softplus	O
,	O
log	O
(	O
1	O
+	O
exp	O
(	O
x	O
)	O
)	O
lp	O
norm	O
of	O
x	O
l2	O
norm	O
of	O
x	O
positive	O
part	O
of	O
x	O
,	O
i.e.	O
,	O
max	O
(	O
0	O
)	O
,	O
x	O
1condition	O
is	O
1	O
if	O
the	O
condition	O
is	O
true	O
,	O
0	O
otherwise	O
sometimes	O
we	O
use	O
a	O
function	O
f	O
whose	O
argument	O
is	O
a	O
scalar	O
but	O
apply	O
it	O
to	O
a	O
vector	O
,	O
matrix	O
,	O
or	O
tensor	O
:	O
f	O
(	O
x	O
)	O
,	O
f	O
(	O
x	O
)	O
,	O
or	O
f	O
(	O
x	O
)	O
.	O
this	O
denotes	O
the	O
application	O
of	O
f	O
to	O
the	O
array	O
element-wise	O
.	O
for	O
example	O
,	O
if	O
c	O
=	O
σ	O
(	O
x	O
)	O
,	O
then	O
c	O
i	O
,	O
j	O
,	O
k	O
=	O
σ	O
(	O
xi	O
,	O
j	O
,	O
k	O
)	O
for	O
all	O
valid	O
values	O
of	O
and	O
.	O
k	O
,	O
i	O
j	O
pdata	O
ˆpdata	O
x	O
x	O
(	O
)	O
i	O
y	O
(	O
)	O
i	O
or	O
y	O
(	O
)	O
i	O
x	O
datasets	O
and	O
distributions	O
the	O
data	O
generating	O
distribution	O
the	O
empirical	O
distribution	O
deﬁned	O
by	O
the	O
training	O
set	O
a	O
set	O
of	O
training	O
examples	O
the	O
-th	O
example	O
(	O
input	O
)	O
from	O
a	O
dataset	O
i	O
the	O
target	O
associated	O
with	O
x	O
(	O
)	O
i	O
for	O
supervised	O
learn-	O
ing	O
×	O
the	O
m	O
n	O
xi	O
,	O
:	O
matrix	O
with	O
input	O
example	O
x	O
(	O
)	O
i	O
in	O
row	O
xiv	B
chapter	O
1	O
introduction	O
inventors	O
have	O
long	O
dreamed	O
of	O
creating	O
machines	O
that	O
think	O
.	O
this	O
desire	O
dates	O
back	O
to	O
at	O
least	O
the	O
time	O
of	O
ancient	O
greece	O
.	O
the	O
mythical	O
ﬁgures	O
pygmalion	O
,	O
daedalus	O
,	O
and	O
hephaestus	O
may	O
all	O
be	O
interpreted	O
as	O
legendary	O
inventors	O
,	O
and	O
galatea	O
,	O
talos	O
,	O
and	O
pandora	O
may	O
all	O
be	O
regarded	O
as	O
artiﬁcial	O
life	O
(	O
ovid	O
and	O
martin	O
,	O
2004	O
sparkes	O
1996	O
tandy	O
1997	O
)	O
.	O
,	O
;	O
,	O
;	O
when	O
programmable	O
computers	O
were	O
ﬁrst	O
conceived	O
,	O
people	O
wondered	O
whether	O
such	O
machines	O
might	O
become	O
intelligent	O
,	O
over	O
a	O
hundred	O
years	O
before	O
one	O
was	O
built	O
(	O
lovelace	O
1842	O
)	O
.	O
today	O
,	O
artiﬁcial	O
intelligence	O
(	O
ai	O
)	O
is	O
a	O
thriving	O
ﬁeld	O
with	O
many	O
practical	O
applications	O
and	O
active	O
research	O
topics	O
.	O
we	O
look	O
to	O
intelligent	O
software	O
to	O
automate	O
routine	O
labor	O
,	O
understand	O
speech	O
or	O
images	O
,	O
make	O
diagnoses	O
in	O
medicine	O
and	O
support	O
basic	O
scientiﬁc	O
research	O
.	O
,	O
in	O
the	O
early	O
days	O
of	O
artiﬁcial	O
intelligence	O
,	O
the	O
ﬁeld	O
rapidly	O
tackled	O
and	O
solved	O
problems	O
that	O
are	O
intellectually	O
diﬃcult	O
for	O
human	O
beings	O
but	O
relatively	O
straight-	O
forward	O
for	O
computers—problems	O
that	O
can	O
be	O
described	O
by	O
a	O
list	O
of	O
formal	O
,	O
math-	O
ematical	O
rules	O
.	O
the	O
true	O
challenge	O
to	O
artiﬁcial	O
intelligence	O
proved	O
to	O
be	O
solving	O
the	O
tasks	O
that	O
are	O
easy	O
for	O
people	O
to	O
perform	O
but	O
hard	O
for	O
people	O
to	O
describe	O
formally—problems	O
that	O
we	O
solve	O
intuitively	O
,	O
that	O
feel	O
automatic	O
,	O
like	O
recognizing	O
spoken	O
words	O
or	O
faces	O
in	O
images	O
.	O
this	O
book	O
is	O
about	O
a	O
solution	O
to	O
these	O
more	O
intuitive	O
problems	O
.	O
this	O
solution	O
is	O
to	O
allow	O
computers	O
to	O
learn	O
from	O
experience	O
and	O
understand	O
the	O
world	O
in	O
terms	O
of	O
a	O
hierarchy	O
of	O
concepts	O
,	O
with	O
each	O
concept	O
deﬁned	O
in	O
terms	O
of	O
its	O
relation	O
to	O
simpler	O
concepts	O
.	O
by	O
gathering	O
knowledge	O
from	O
experience	O
,	O
this	O
approach	O
avoids	O
the	O
need	O
for	O
human	O
operators	O
to	O
formally	O
specify	O
all	O
of	O
the	O
knowledge	O
that	O
the	O
computer	O
needs	O
.	O
the	O
hierarchy	O
of	O
concepts	O
allows	O
the	O
computer	O
to	O
learn	O
complicated	O
concepts	O
by	O
building	O
them	O
out	O
of	O
simpler	O
ones	O
.	O
if	O
we	O
draw	O
a	O
graph	O
showing	O
how	O
these	O
1	O
chapter	O
1.	O
introduction	O
concepts	O
are	O
built	O
on	O
top	O
of	O
each	O
other	O
,	O
the	O
graph	O
is	O
deep	O
,	O
with	O
many	O
layers	O
.	O
for	O
this	O
reason	O
,	O
we	O
call	O
this	O
approach	O
to	O
ai	O
.	O
deep	O
learning	O
many	O
of	O
the	O
early	O
successes	O
of	O
ai	O
took	O
place	O
in	O
relatively	O
sterile	O
and	O
formal	O
environments	O
and	O
did	O
not	O
require	O
computers	O
to	O
have	O
much	O
knowledge	O
about	O
the	O
world	O
.	O
for	O
example	O
,	O
ibm	O
’	O
s	O
deep	O
blue	O
chess-playing	O
system	O
defeated	O
world	O
champion	O
garry	O
kasparov	O
in	O
1997	O
(	O
)	O
.	O
chess	O
is	O
of	O
course	O
a	O
very	O
simple	O
world	O
,	O
containing	O
only	O
sixty-four	O
locations	O
and	O
thirty-two	O
pieces	O
that	O
can	O
move	O
in	O
only	O
rigidly	O
circumscribed	O
ways	O
.	O
devising	O
a	O
successful	O
chess	O
strategy	O
is	O
a	O
tremendous	O
accomplishment	O
,	O
but	O
the	O
challenge	O
is	O
not	O
due	O
to	O
the	O
diﬃculty	O
of	O
describing	O
the	O
set	O
of	O
chess	O
pieces	O
and	O
allowable	O
moves	O
to	O
the	O
computer	O
.	O
chess	O
can	O
be	O
completely	O
described	O
by	O
a	O
very	O
brief	O
list	O
of	O
completely	O
formal	O
rules	O
,	O
easily	O
provided	O
ahead	O
of	O
time	O
by	O
the	O
programmer	O
.	O
hsu	O
2002	O
,	O
ironically	O
,	O
abstract	O
and	O
formal	O
tasks	O
that	O
are	O
among	O
the	O
most	O
diﬃcult	O
mental	O
undertakings	O
for	O
a	O
human	O
being	O
are	O
among	O
the	O
easiest	O
for	O
a	O
computer	O
.	O
computers	O
have	O
long	O
been	O
able	O
to	O
defeat	O
even	O
the	O
best	O
human	O
chess	O
player	O
,	O
but	O
are	O
only	O
recently	O
matching	O
some	O
of	O
the	O
abilities	O
of	O
average	O
human	O
beings	O
to	O
recognize	O
objects	O
or	O
speech	O
.	O
a	O
person	O
’	O
s	O
everyday	O
life	O
requires	O
an	O
immense	O
amount	O
of	O
knowledge	O
about	O
the	O
world	O
.	O
much	O
of	O
this	O
knowledge	O
is	O
subjective	O
and	O
intuitive	O
,	O
and	O
therefore	O
diﬃcult	O
to	O
articulate	O
in	O
a	O
formal	O
way	O
.	O
computers	O
need	O
to	O
capture	O
this	O
same	O
knowledge	O
in	O
order	O
to	O
behave	O
in	O
an	O
intelligent	O
way	O
.	O
one	O
of	O
the	O
key	O
challenges	O
in	O
artiﬁcial	O
intelligence	O
is	O
how	O
to	O
get	O
this	O
informal	O
knowledge	O
into	O
a	O
computer	O
.	O
several	O
artiﬁcial	O
intelligence	O
projects	O
have	O
sought	O
to	O
hard-code	O
knowledge	O
about	O
the	O
world	O
in	O
formal	O
languages	O
.	O
a	O
computer	O
can	O
reason	O
about	O
statements	O
in	O
these	O
formal	O
languages	O
automatically	O
using	O
logical	O
inference	O
rules	O
.	O
this	O
is	O
known	O
as	O
the	O
knowledge	O
base	O
approach	O
to	O
artiﬁcial	O
intelligence	O
.	O
none	O
of	O
these	O
projects	O
has	O
led	O
to	O
a	O
major	O
success	O
.	O
one	O
of	O
the	O
most	O
famous	O
such	O
projects	O
is	O
cyc	O
(	O
lenat	O
and	O
guha	O
,	O
1989	O
)	O
.	O
cyc	O
is	O
an	O
inference	O
engine	O
and	O
a	O
database	O
of	O
statements	O
in	O
a	O
language	O
called	O
cycl	O
.	O
these	O
statements	O
are	O
entered	O
by	O
a	O
staﬀ	O
of	O
human	O
supervisors	O
.	O
it	O
is	O
an	O
unwieldy	O
process	O
.	O
people	O
struggle	O
to	O
devise	O
formal	O
rules	O
with	O
enough	O
complexity	O
to	O
accurately	O
describe	O
the	O
world	O
.	O
for	O
example	O
,	O
cyc	O
failed	O
to	O
understand	O
a	O
story	O
about	O
a	O
person	O
named	O
fred	O
shaving	O
in	O
the	O
morning	O
(	O
)	O
.	O
its	O
inference	O
engine	O
detected	O
an	O
inconsistency	O
in	O
the	O
story	O
:	O
it	O
knew	O
that	O
people	O
do	O
not	O
have	O
electrical	O
parts	O
,	O
but	O
because	O
fred	O
was	O
holding	O
an	O
electric	O
razor	O
,	O
it	O
believed	O
the	O
entity	O
“	O
fredwhileshaving	O
”	O
contained	O
electrical	O
parts	O
.	O
it	O
therefore	O
asked	O
whether	O
fred	O
was	O
still	O
a	O
person	O
while	O
he	O
was	O
shaving	O
.	O
linde	O
1992	O
,	O
the	O
diﬃculties	O
faced	O
by	O
systems	O
relying	O
on	O
hard-coded	O
knowledge	O
suggest	O
that	O
ai	O
systems	O
need	O
the	O
ability	O
to	O
acquire	O
their	O
own	O
knowledge	O
,	O
by	O
extracting	O
patterns	O
from	O
raw	O
data	O
.	O
this	O
capability	O
is	O
known	O
as	O
machine	O
learning	O
.	O
the	O
2	O
chapter	O
1.	O
introduction	O
introduction	O
of	O
machine	O
learning	O
allowed	O
computers	O
to	O
tackle	O
problems	O
involving	O
knowledge	O
of	O
the	O
real	O
world	O
and	O
make	O
decisions	O
that	O
appear	O
subjective	O
.	O
a	O
simple	O
machine	O
learning	O
algorithm	O
called	O
logistic	O
regression	O
can	O
determine	O
whether	O
to	O
recommend	O
cesarean	O
delivery	O
(	O
mor-yosef	O
)	O
.	O
a	O
simple	O
machine	O
learning	O
algorithm	O
called	O
naive	O
bayes	O
can	O
separate	O
legitimate	O
e-mail	O
from	O
spam	O
e-mail	O
.	O
et	O
al.	O
,	O
1990	O
the	O
performance	O
of	O
these	O
simple	O
machine	O
learning	O
algorithms	O
depends	O
heavily	O
on	O
the	O
representation	O
of	O
the	O
data	O
they	O
are	O
given	O
.	O
for	O
example	O
,	O
when	O
logistic	O
regression	O
is	O
used	O
to	O
recommend	O
cesarean	O
delivery	O
,	O
the	O
ai	O
system	O
does	O
not	O
examine	O
the	O
patient	O
directly	O
.	O
instead	O
,	O
the	O
doctor	O
tells	O
the	O
system	O
several	O
pieces	O
of	O
relevant	O
information	O
,	O
such	O
as	O
the	O
presence	O
or	O
absence	O
of	O
a	O
uterine	O
scar	O
.	O
each	O
piece	O
of	O
information	O
included	O
in	O
the	O
representation	O
of	O
the	O
patient	O
is	O
known	O
as	O
a	O
feature	O
.	O
logistic	O
regression	O
learns	O
how	O
each	O
of	O
these	O
features	O
of	O
the	O
patient	O
correlates	O
with	O
various	O
outcomes	O
.	O
however	O
,	O
it	O
can	O
not	O
inﬂuence	O
the	O
way	O
that	O
the	O
features	O
are	O
deﬁned	O
in	O
any	O
way	O
.	O
if	O
logistic	O
regression	O
was	O
given	O
an	O
mri	O
scan	O
of	O
the	O
patient	O
,	O
rather	O
than	O
the	O
doctor	O
’	O
s	O
formalized	O
report	O
,	O
it	O
would	O
not	O
be	O
able	O
to	O
make	O
useful	O
predictions	O
.	O
individual	O
pixels	O
in	O
an	O
mri	O
scan	O
have	O
negligible	O
correlation	O
with	O
any	O
complications	O
that	O
might	O
occur	O
during	O
delivery	O
.	O
this	O
dependence	O
on	O
representations	O
is	O
a	O
general	O
phenomenon	O
that	O
appears	O
throughout	O
computer	O
science	O
and	O
even	O
daily	O
life	O
.	O
in	O
computer	O
science	O
,	O
opera-	O
tions	O
such	O
as	O
searching	O
a	O
collection	O
of	O
data	O
can	O
proceed	O
exponentially	O
faster	O
if	O
the	O
collection	O
is	O
structured	O
and	O
indexed	O
intelligently	O
.	O
people	O
can	O
easily	O
perform	O
arithmetic	O
on	O
arabic	O
numerals	O
,	O
but	O
ﬁnd	O
arithmetic	O
on	O
roman	O
numerals	O
much	O
more	O
time-consuming	O
.	O
it	O
is	O
not	O
surprising	O
that	O
the	O
choice	O
of	O
representation	O
has	O
an	O
enormous	O
eﬀect	O
on	O
the	O
performance	O
of	O
machine	O
learning	O
algorithms	O
.	O
for	O
a	O
simple	O
visual	O
example	O
,	O
see	O
ﬁgure	O
.1.1	O
many	O
artiﬁcial	O
intelligence	O
tasks	O
can	O
be	O
solved	O
by	O
designing	O
the	O
right	O
set	O
of	O
features	O
to	O
extract	O
for	O
that	O
task	O
,	O
then	O
providing	O
these	O
features	O
to	O
a	O
simple	O
machine	O
learning	O
algorithm	O
.	O
for	O
example	O
,	O
a	O
useful	O
feature	O
for	O
speaker	O
identiﬁcation	O
from	O
sound	O
is	O
an	O
estimate	O
of	O
the	O
size	O
of	O
speaker	O
’	O
s	O
vocal	O
tract	O
.	O
it	O
therefore	O
gives	O
a	O
strong	O
clue	O
as	O
to	O
whether	O
the	O
speaker	O
is	O
a	O
man	O
,	O
woman	O
,	O
or	O
child	O
.	O
however	O
,	O
for	O
many	O
tasks	O
,	O
it	O
is	O
diﬃcult	O
to	O
know	O
what	O
features	O
should	O
be	O
extracted	O
.	O
for	O
example	O
,	O
suppose	O
that	O
we	O
would	O
like	O
to	O
write	O
a	O
program	O
to	O
detect	O
cars	O
in	O
photographs	O
.	O
we	O
know	O
that	O
cars	O
have	O
wheels	O
,	O
so	O
we	O
might	O
like	O
to	O
use	O
the	O
presence	O
of	O
a	O
wheel	O
as	O
a	O
feature	O
.	O
unfortunately	O
,	O
it	O
is	O
diﬃcult	O
to	O
describe	O
exactly	O
what	O
a	O
wheel	O
looks	O
like	O
in	O
terms	O
of	O
pixel	O
values	O
.	O
a	O
wheel	O
has	O
a	O
simple	O
geometric	O
shape	O
but	O
its	O
image	O
may	O
be	O
complicated	O
by	O
shadows	O
falling	O
on	O
the	O
wheel	O
,	O
the	O
sun	O
glaring	O
oﬀ	O
the	O
metal	O
parts	O
of	O
the	O
wheel	O
,	O
the	O
fender	O
of	O
the	O
car	O
or	O
an	O
object	O
in	O
the	O
foreground	O
obscuring	O
part	O
of	O
the	O
wheel	O
,	O
and	O
so	O
on	O
.	O
3	O
chapter	O
1.	O
introduction	O
	O
	O
	O
	O
	O
	O
figure	O
1.1	O
:	O
example	O
of	O
diﬀerent	O
representations	O
:	O
suppose	O
we	O
want	O
to	O
separate	O
two	O
categories	O
of	O
data	O
by	O
drawing	O
a	O
line	O
between	O
them	O
in	O
a	O
scatterplot	O
.	O
in	O
the	O
plot	O
on	O
the	O
left	O
,	O
we	O
represent	O
some	O
data	O
using	O
cartesian	O
coordinates	O
,	O
and	O
the	O
task	O
is	O
impossible	O
.	O
in	O
the	O
plot	O
on	O
the	O
right	O
,	O
we	O
represent	O
the	O
data	O
with	O
polar	O
coordinates	O
and	O
the	O
task	O
becomes	O
simple	O
to	O
solve	O
with	O
a	O
vertical	O
line	O
.	O
figure	O
produced	O
in	O
collaboration	O
with	O
david	O
warde-farley	O
.	O
one	O
solution	O
to	O
this	O
problem	O
is	O
to	O
use	O
machine	O
learning	O
to	O
discover	O
not	O
only	O
the	O
mapping	O
from	O
representation	O
to	O
output	O
but	O
also	O
the	O
representation	O
itself	O
.	O
this	O
approach	O
is	O
known	O
as	O
representation	O
learning	O
.	O
learned	O
representations	O
often	O
result	O
in	O
much	O
better	O
performance	O
than	O
can	O
be	O
obtained	O
with	O
hand-designed	O
representations	O
.	O
they	O
also	O
allow	O
ai	O
systems	O
to	O
rapidly	O
adapt	O
to	O
new	O
tasks	O
,	O
with	O
minimal	O
human	O
intervention	O
.	O
a	O
representation	O
learning	O
algorithm	O
can	O
discover	O
a	O
good	O
set	O
of	O
features	O
for	O
a	O
simple	O
task	O
in	O
minutes	O
,	O
or	O
a	O
complex	O
task	O
in	O
hours	O
to	O
months	O
.	O
manually	O
designing	O
features	O
for	O
a	O
complex	O
task	O
requires	O
a	O
great	O
deal	O
of	O
human	O
time	O
and	O
eﬀort	O
;	O
it	O
can	O
take	O
decades	O
for	O
an	O
entire	O
community	O
of	O
researchers	O
.	O
the	O
quintessential	O
example	O
of	O
a	O
representation	O
learning	O
algorithm	O
is	O
the	O
au-	O
toencoder	O
.	O
an	O
autoencoder	O
is	O
the	O
combination	O
of	O
an	O
encoder	O
function	O
that	O
converts	O
the	O
input	O
data	O
into	O
a	O
diﬀerent	O
representation	O
,	O
and	O
a	O
decoder	O
function	O
that	O
converts	O
the	O
new	O
representation	O
back	O
into	O
the	O
original	O
format	O
.	O
autoencoders	O
are	O
trained	O
to	O
preserve	O
as	O
much	O
information	O
as	O
possible	O
when	O
an	O
input	O
is	O
run	O
through	O
the	O
encoder	O
and	O
then	O
the	O
decoder	O
,	O
but	O
are	O
also	O
trained	O
to	O
make	O
the	O
new	O
representation	O
have	O
various	O
nice	O
properties	O
.	O
diﬀerent	O
kinds	O
of	O
autoencoders	O
aim	O
to	O
achieve	O
diﬀerent	O
kinds	O
of	O
properties	O
.	O
when	O
designing	O
features	O
or	O
algorithms	O
for	O
learning	O
features	O
,	O
our	O
goal	O
is	O
usually	O
to	O
separate	O
the	O
factors	O
of	O
variation	O
that	O
explain	O
the	O
observed	O
data	O
.	O
in	O
this	O
context	O
,	O
we	O
use	O
the	O
word	O
“	O
factors	O
”	O
simply	O
to	O
refer	O
to	O
separate	O
sources	O
of	O
inﬂuence	O
;	O
the	O
factors	O
are	O
usually	O
not	O
combined	O
by	O
multiplication	O
.	O
such	O
factors	O
are	O
often	O
not	O
4	O
chapter	O
1.	O
introduction	O
quantities	O
that	O
are	O
directly	O
observed	O
.	O
instead	O
,	O
they	O
may	O
exist	O
either	O
as	O
unobserved	O
objects	O
or	O
unobserved	O
forces	O
in	O
the	O
physical	O
world	O
that	O
aﬀect	O
observable	O
quantities	O
.	O
they	O
may	O
also	O
exist	O
as	O
constructs	O
in	O
the	O
human	O
mind	O
that	O
provide	O
useful	O
simplifying	O
explanations	O
or	O
inferred	O
causes	O
of	O
the	O
observed	O
data	O
.	O
they	O
can	O
be	O
thought	O
of	O
as	O
concepts	O
or	O
abstractions	O
that	O
help	O
us	O
make	O
sense	O
of	O
the	O
rich	O
variability	O
in	O
the	O
data	O
.	O
when	O
analyzing	O
a	O
speech	O
recording	O
,	O
the	O
factors	O
of	O
variation	O
include	O
the	O
speaker	O
’	O
s	O
age	O
,	O
their	O
sex	O
,	O
their	O
accent	O
and	O
the	O
words	O
that	O
they	O
are	O
speaking	O
.	O
when	O
analyzing	O
an	O
image	O
of	O
a	O
car	O
,	O
the	O
factors	O
of	O
variation	O
include	O
the	O
position	B
of	O
the	O
car	O
,	O
its	O
color	O
,	O
and	O
the	O
angle	O
and	O
brightness	O
of	O
the	O
sun	O
.	O
a	O
major	O
source	O
of	O
diﬃculty	O
in	O
many	O
real-world	O
artiﬁcial	O
intelligence	O
applications	O
is	O
that	O
many	O
of	O
the	O
factors	O
of	O
variation	O
inﬂuence	O
every	O
single	O
piece	O
of	O
data	O
we	O
are	O
able	O
to	O
observe	O
.	O
the	O
individual	O
pixels	O
in	O
an	O
image	O
of	O
a	O
red	O
car	O
might	O
be	O
very	O
close	O
to	O
black	O
at	O
night	O
.	O
the	O
shape	O
of	O
the	O
car	O
’	O
s	O
silhouette	O
depends	O
on	O
the	O
viewing	O
angle	O
.	O
most	O
applications	O
require	O
us	O
to	O
the	O
factors	O
of	O
variation	O
and	O
discard	O
the	O
ones	O
that	O
we	O
do	O
not	O
care	O
about	O
.	O
disentangle	O
of	O
course	O
,	O
it	O
can	O
be	O
very	O
diﬃcult	O
to	O
extract	O
such	O
high-level	O
,	O
abstract	O
features	O
from	O
raw	O
data	O
.	O
many	O
of	O
these	O
factors	O
of	O
variation	O
,	O
such	O
as	O
a	O
speaker	O
’	O
s	O
accent	O
,	O
can	O
be	O
identiﬁed	O
only	O
using	O
sophisticated	O
,	O
nearly	O
human-level	O
understanding	O
of	O
the	O
data	O
.	O
when	O
it	O
is	O
nearly	O
as	O
diﬃcult	O
to	O
obtain	O
a	O
representation	O
as	O
to	O
solve	O
the	O
original	O
problem	O
,	O
representation	O
learning	O
does	O
not	O
,	O
at	O
ﬁrst	O
glance	O
,	O
seem	O
to	O
help	O
us	O
.	O
deep	O
learning	O
solves	O
this	O
central	O
problem	O
in	O
representation	O
learning	O
by	O
intro-	O
ducing	O
representations	O
that	O
are	O
expressed	O
in	O
terms	O
of	O
other	O
,	O
simpler	O
representations	O
.	O
deep	O
learning	O
allows	O
the	O
computer	O
to	O
build	O
complex	O
concepts	O
out	O
of	O
simpler	O
con-	O
cepts	O
.	O
figure	O
shows	O
how	O
a	O
deep	O
learning	O
system	O
can	O
represent	O
the	O
concept	O
of	O
an	O
image	O
of	O
a	O
person	O
by	O
combining	O
simpler	O
concepts	O
,	O
such	O
as	O
corners	O
and	O
contours	O
,	O
which	O
are	O
in	O
turn	O
deﬁned	O
in	O
terms	O
of	O
edges	O
.	O
1.2	O
the	O
quintessential	O
example	O
of	O
a	O
deep	O
learning	O
model	B
is	O
the	O
feedforward	O
deep	O
network	O
or	O
multilayer	O
perceptron	O
(	O
mlp	O
)	O
.	O
a	O
multilayer	O
perceptron	O
is	O
just	O
a	O
mathematical	O
function	O
mapping	O
some	O
set	O
of	O
input	O
values	O
to	O
output	O
values	O
.	O
the	O
function	O
is	O
formed	O
by	O
composing	O
many	O
simpler	O
functions	O
.	O
we	O
can	O
think	O
of	O
each	O
application	O
of	O
a	O
diﬀerent	O
mathematical	O
function	O
as	O
providing	O
a	O
new	O
representation	O
of	O
the	O
input	O
.	O
the	O
idea	O
of	O
learning	O
the	O
right	O
representation	O
for	O
the	O
data	O
provides	O
one	O
perspec-	O
tive	O
on	O
deep	O
learning	O
.	O
another	O
perspective	O
on	O
deep	O
learning	O
is	O
that	O
depth	O
allows	O
the	O
computer	O
to	O
learn	O
a	O
multi-step	O
computer	O
program	O
.	O
each	O
layer	O
of	O
the	O
representation	O
can	O
be	O
thought	O
of	O
as	O
the	O
state	O
of	O
the	O
computer	O
’	O
s	O
memory	O
after	O
executing	O
another	O
set	O
of	O
instructions	O
in	O
parallel	O
.	O
networks	O
with	O
greater	O
depth	O
can	O
execute	O
more	O
instructions	O
in	O
sequence	O
.	O
sequential	O
instructions	O
oﬀer	O
great	O
power	O
because	O
later	O
5	O
chapter	O
1.	O
introduction	O
car	O
person	O
animal	O
output	O
(	O
object	O
identity	O
)	O
3rd	O
hidden	O
layer	O
(	O
object	O
parts	O
)	O
2nd	O
hidden	O
layer	O
(	O
corners	O
and	O
contours	O
)	O
1st	O
hidden	O
layer	O
(	O
edges	O
)	O
visible	O
layer	O
(	O
input	O
pixels	O
)	O
figure	O
1.2	O
:	O
illustration	O
of	O
a	O
deep	O
learning	O
model	B
.	O
it	O
is	O
diﬃcult	O
for	O
a	O
computer	O
to	O
understand	O
the	O
meaning	O
of	O
raw	O
sensory	O
input	O
data	O
,	O
such	O
as	O
this	O
image	O
represented	O
as	O
a	O
collection	O
of	O
pixel	O
values	O
.	O
the	O
function	O
mapping	O
from	O
a	O
set	O
of	O
pixels	O
to	O
an	O
object	O
identity	O
is	O
very	O
complicated	O
.	O
learning	O
or	O
evaluating	O
this	O
mapping	O
seems	O
insurmountable	O
if	O
tackled	O
directly	O
.	O
deep	O
learning	O
resolves	O
this	O
diﬃculty	O
by	O
breaking	O
the	O
desired	O
complicated	O
mapping	O
into	O
a	O
series	O
of	O
nested	O
simple	O
mappings	O
,	O
each	O
described	O
by	O
a	O
diﬀerent	O
layer	O
of	O
the	O
model	B
.	O
the	O
input	O
is	O
presented	O
at	O
the	O
visible	O
layer	O
,	O
so	O
named	O
because	O
it	O
contains	O
the	O
variables	O
that	O
we	O
are	O
able	O
to	O
observe	O
.	O
then	O
a	O
series	O
of	O
hidden	O
layers	O
extracts	O
increasingly	O
abstract	O
features	O
from	O
the	O
image	O
.	O
these	O
layers	O
are	O
called	O
“	O
hidden	O
”	O
because	O
their	O
values	O
are	O
not	O
given	O
in	O
the	O
data	O
;	O
instead	O
the	O
model	B
must	O
determine	O
which	O
concepts	O
are	O
useful	O
for	O
explaining	O
the	O
relationships	O
in	O
the	O
observed	O
data	O
.	O
the	O
images	O
here	O
are	O
visualizations	O
of	O
the	O
kind	O
of	O
feature	O
represented	O
by	O
each	O
hidden	O
unit	O
.	O
given	O
the	O
pixels	O
,	O
the	O
ﬁrst	O
layer	O
can	O
easily	O
identify	O
edges	O
,	O
by	O
comparing	O
the	O
brightness	O
of	O
neighboring	O
pixels	O
.	O
given	O
the	O
ﬁrst	O
hidden	O
layer	O
’	O
s	O
description	O
of	O
the	O
edges	O
,	O
the	O
second	O
hidden	O
layer	O
can	O
easily	O
search	O
for	O
corners	O
and	O
extended	O
contours	O
,	O
which	O
are	O
recognizable	O
as	O
collections	O
of	O
edges	O
.	O
given	O
the	O
second	O
hidden	O
layer	O
’	O
s	O
description	O
of	O
the	O
image	O
in	O
terms	O
of	O
corners	O
and	O
contours	O
,	O
the	O
third	O
hidden	O
layer	O
can	O
detect	O
entire	O
parts	O
of	O
speciﬁc	O
objects	O
,	O
by	O
ﬁnding	O
speciﬁc	O
collections	O
of	O
contours	O
and	O
corners	O
.	O
finally	O
,	O
this	O
description	O
of	O
the	O
image	O
in	O
terms	O
of	O
the	O
object	O
parts	O
it	O
contains	O
can	O
be	O
used	O
to	O
recognize	O
the	O
objects	O
present	O
in	O
the	O
image	O
.	O
images	O
reproduced	O
with	O
permission	O
from	O
zeiler	O
and	O
fergus	O
2014	O
)	O
.	O
(	O
6	O
chapter	O
1.	O
introduction	O
σ	O
×+	O
×	O
element	O
set	O
logistic	O
logistic	O
regression	O
regression	O
element	O
set+×	O
σ	O
w1w1	O
x1x1	O
w2w2	O
x2x2	O
ww	O
xx	O
figure	O
1.3	O
:	O
illustration	O
of	O
computational	O
graphs	O
mapping	O
an	O
input	O
to	O
an	O
output	O
where	O
each	O
node	O
performs	O
an	O
operation	O
.	O
depth	O
is	O
the	O
length	O
of	O
the	O
longest	O
path	O
from	O
input	O
to	O
output	O
but	O
depends	O
on	O
the	O
deﬁnition	O
of	O
what	O
constitutes	O
a	O
possible	O
computational	O
step	O
.	O
the	O
computation	O
depicted	O
in	O
these	O
graphs	O
is	O
the	O
output	O
of	O
a	O
logistic	O
regression	O
model	B
,	O
σ	O
(	O
wt	O
x	O
)	O
,	O
where	O
σ	O
is	O
the	O
logistic	O
sigmoid	O
function	O
.	O
if	O
we	O
use	O
addition	O
,	O
multiplication	O
and	O
logistic	O
sigmoids	O
as	O
the	O
elements	O
of	O
our	O
computer	O
language	O
,	O
then	O
this	O
model	B
has	O
depth	O
three	O
.	O
if	O
we	O
view	O
logistic	O
regression	O
as	O
an	O
element	O
itself	O
,	O
then	O
this	O
model	B
has	O
depth	O
one	O
.	O
instructions	O
can	O
refer	O
back	O
to	O
the	O
results	O
of	O
earlier	O
instructions	O
.	O
according	O
to	O
this	O
view	O
of	O
deep	O
learning	O
,	O
not	O
all	O
of	O
the	O
information	O
in	O
a	O
layer	O
’	O
s	O
activations	O
necessarily	O
encodes	O
factors	O
of	O
variation	O
that	O
explain	O
the	O
input	O
.	O
the	O
representation	O
also	O
stores	O
state	O
information	O
that	O
helps	O
to	O
execute	O
a	O
program	O
that	O
can	O
make	O
sense	O
of	O
the	O
input	O
.	O
this	O
state	O
information	O
could	O
be	O
analogous	O
to	O
a	O
counter	O
or	O
pointer	O
in	O
a	O
traditional	O
computer	O
program	O
.	O
it	O
has	O
nothing	O
to	O
do	O
with	O
the	O
content	O
of	O
the	O
input	O
speciﬁcally	O
,	O
but	O
it	O
helps	O
the	O
model	B
to	O
organize	O
its	O
processing	O
.	O
there	O
are	O
two	O
main	O
ways	O
of	O
measuring	O
the	O
depth	O
of	O
a	O
model	B
.	O
the	O
ﬁrst	O
view	O
is	O
based	O
on	O
the	O
number	O
of	O
sequential	O
instructions	O
that	O
must	O
be	O
executed	O
to	O
evaluate	O
the	O
architecture	O
.	O
we	O
can	O
think	O
of	O
this	O
as	O
the	O
length	O
of	O
the	O
longest	O
path	O
through	O
a	O
ﬂow	O
chart	O
that	O
describes	O
how	O
to	O
compute	O
each	O
of	O
the	O
model	B
’	O
s	O
outputs	O
given	O
its	O
inputs	O
.	O
just	O
as	O
two	O
equivalent	O
computer	O
programs	O
will	O
have	O
diﬀerent	O
lengths	O
depending	O
on	O
which	O
language	O
the	O
program	O
is	O
written	O
in	O
,	O
the	O
same	O
function	O
may	O
be	O
drawn	O
as	O
a	O
ﬂowchart	O
with	O
diﬀerent	O
depths	O
depending	O
on	O
which	O
functions	O
we	O
allow	O
to	O
be	O
used	O
as	O
individual	O
steps	O
in	O
the	O
ﬂowchart	O
.	O
figure	O
illustrates	O
how	O
this	O
choice	O
of	O
language	O
can	O
give	O
two	O
diﬀerent	O
measurements	O
for	O
the	O
same	O
architecture	O
.	O
1.3	O
another	O
approach	O
,	O
used	O
by	O
deep	O
probabilistic	O
models	O
,	O
regards	O
the	O
depth	O
of	O
a	O
model	B
as	O
being	O
not	O
the	O
depth	O
of	O
the	O
computational	O
graph	O
but	O
the	O
depth	O
of	O
the	O
graph	O
describing	O
how	O
concepts	O
are	O
related	O
to	O
each	O
other	O
.	O
in	O
this	O
case	O
,	O
the	O
depth	O
7	O
chapter	O
1.	O
introduction	O
of	O
the	O
ﬂowchart	O
of	O
the	O
computations	O
needed	O
to	O
compute	O
the	O
representation	O
of	O
each	O
concept	O
may	O
be	O
much	O
deeper	O
than	O
the	O
graph	O
of	O
the	O
concepts	O
themselves	O
.	O
this	O
is	O
because	O
the	O
system	O
’	O
s	O
understanding	O
of	O
the	O
simpler	O
concepts	O
can	O
be	O
reﬁned	O
given	O
information	O
about	O
the	O
more	O
complex	O
concepts	O
.	O
for	O
example	O
,	O
an	O
ai	O
system	O
observing	O
an	O
image	O
of	O
a	O
face	O
with	O
one	O
eye	O
in	O
shadow	O
may	O
initially	O
only	O
see	O
one	O
eye	O
.	O
after	O
detecting	O
that	O
a	O
face	O
is	O
present	O
,	O
it	O
can	O
then	O
infer	O
that	O
a	O
second	O
eye	O
is	O
probably	O
present	O
as	O
well	O
.	O
in	O
this	O
case	O
,	O
the	O
graph	O
of	O
concepts	O
only	O
includes	O
two	O
layers—a	O
layer	O
for	O
eyes	O
and	O
a	O
layer	O
for	O
faces—but	O
the	O
graph	O
of	O
computations	O
includes	O
2n	O
layers	O
if	O
we	O
reﬁne	O
our	O
estimate	O
of	O
each	O
concept	O
given	O
the	O
other	O
times	O
.	O
n	O
because	O
it	O
is	O
not	O
always	O
clear	O
which	O
of	O
these	O
two	O
views—the	O
depth	O
of	O
the	O
computational	O
graph	O
,	O
or	O
the	O
depth	O
of	O
the	O
probabilistic	O
modeling	O
graph—is	O
most	O
relevant	O
,	O
and	O
because	O
diﬀerent	O
people	O
choose	O
diﬀerent	O
sets	O
of	O
smallest	O
elements	O
from	O
which	O
to	O
construct	O
their	O
graphs	O
,	O
there	O
is	O
no	O
single	O
correct	O
value	O
for	O
the	O
depth	O
of	O
an	O
architecture	O
,	O
just	O
as	O
there	O
is	O
no	O
single	O
correct	O
value	O
for	O
the	O
length	O
of	O
a	O
computer	O
program	O
.	O
nor	O
is	O
there	O
a	O
consensus	O
about	O
how	O
much	O
depth	O
a	O
model	B
requires	O
to	O
qualify	O
as	O
“	O
deep.	O
”	O
however	O
,	O
deep	O
learning	O
can	O
safely	O
be	O
regarded	O
as	O
the	O
study	O
of	O
models	O
that	O
either	O
involve	O
a	O
greater	O
amount	O
of	O
composition	O
of	O
learned	O
functions	O
or	O
learned	O
concepts	O
than	O
traditional	O
machine	O
learning	O
does	O
.	O
to	O
summarize	O
,	O
deep	O
learning	O
,	O
the	O
subject	O
of	O
this	O
book	O
,	O
is	O
an	O
approach	O
to	O
ai	O
.	O
speciﬁcally	O
,	O
it	O
is	O
a	O
type	O
of	O
machine	O
learning	O
,	O
a	O
technique	O
that	O
allows	O
computer	O
systems	O
to	O
improve	O
with	O
experience	O
and	O
data	O
.	O
according	O
to	O
the	O
authors	O
of	O
this	O
book	O
,	O
machine	O
learning	O
is	O
the	O
only	O
viable	O
approach	O
to	O
building	O
ai	O
systems	O
that	O
can	O
operate	O
in	O
complicated	O
,	O
real-world	O
environments	O
.	O
deep	O
learning	O
is	O
a	O
particular	O
kind	O
of	O
machine	O
learning	O
that	O
achieves	O
great	O
power	O
and	O
ﬂexibility	O
by	O
learning	O
to	O
represent	O
the	O
world	O
as	O
a	O
nested	O
hierarchy	O
of	O
concepts	O
,	O
with	O
each	O
concept	O
deﬁned	O
in	O
relation	O
to	O
simpler	O
concepts	O
,	O
and	O
more	O
abstract	O
representations	O
computed	O
in	O
terms	O
of	O
less	O
abstract	O
ones	O
.	O
figure	O
illustrates	O
the	O
relationship	O
between	O
these	O
diﬀerent	O
ai	O
disciplines	O
.	O
figure	O
gives	O
a	O
high-level	O
schematic	O
of	O
how	O
each	O
works	O
.	O
1.5	O
1.4	O
1.1	O
who	O
should	O
read	O
this	O
book	O
?	O
this	O
book	O
can	O
be	O
useful	O
for	O
a	O
variety	O
of	O
readers	O
,	O
but	O
we	O
wrote	O
it	O
with	O
two	O
main	O
target	O
audiences	O
in	O
mind	O
.	O
one	O
of	O
these	O
target	O
audiences	O
is	O
university	O
students	O
(	O
undergraduate	O
or	O
graduate	O
)	O
learning	O
about	O
machine	O
learning	O
,	O
including	O
those	O
who	O
are	O
beginning	O
a	O
career	O
in	O
deep	O
learning	O
and	O
artiﬁcial	O
intelligence	O
research	O
.	O
the	O
other	O
target	O
audience	O
is	O
software	O
engineers	O
who	O
do	O
not	O
have	O
a	O
machine	O
learning	O
or	O
statistics	O
background	O
,	O
but	O
want	O
to	O
rapidly	O
acquire	O
one	O
and	O
begin	O
using	O
deep	O
learning	O
in	O
their	O
product	O
or	O
platform	O
.	O
deep	O
learning	O
has	O
already	O
proven	O
useful	O
in	O
8	O
chapter	O
1.	O
introduction	O
deep	O
learning	O
example	O
:	O
mlps	O
example	O
:	O
shallow	O
autoencoders	O
example	O
:	O
logistic	O
regression	O
example	O
:	O
knowledge	O
bases	O
representation	O
learning	O
machine	O
learning	O
ai	O
figure	O
1.4	O
:	O
a	O
venn	O
diagram	O
showing	O
how	O
deep	O
learning	O
is	O
a	O
kind	O
of	O
representation	O
learning	O
,	O
which	O
is	O
in	O
turn	O
a	O
kind	O
of	O
machine	O
learning	O
,	O
which	O
is	O
used	O
for	O
many	O
but	O
not	O
all	O
approaches	O
to	O
ai	O
.	O
each	O
section	O
of	O
the	O
venn	O
diagram	O
includes	O
an	O
example	O
of	O
an	O
ai	O
technology	O
.	O
9	O
chapter	O
1.	O
introduction	O
output	O
output	O
output	O
mapping	O
from	O
features	O
output	O
mapping	O
from	O
mapping	O
from	O
layers	O
of	O
more	O
additional	O
features	O
features	O
abstract	O
features	O
hand-	O
designed	O
program	O
hand-	O
designed	O
features	O
features	O
simple	O
features	O
input	O
input	O
input	O
input	O
rule-based	O
systems	O
classic	O
machine	O
learning	O
deep	O
learning	O
representation	O
learning	O
figure	O
1.5	O
:	O
flowcharts	O
showing	O
how	O
the	O
diﬀerent	O
parts	O
of	O
an	O
ai	O
system	O
relate	O
to	O
each	O
other	O
within	O
diﬀerent	O
ai	O
disciplines	O
.	O
shaded	O
boxes	O
indicate	O
components	O
that	O
are	O
able	O
to	O
learn	O
from	O
data	O
.	O
10	O
chapter	O
1.	O
introduction	O
many	O
software	O
disciplines	O
including	O
computer	O
vision	O
,	O
speech	O
and	O
audio	O
processing	O
,	O
natural	O
language	O
processing	O
,	O
robotics	O
,	O
bioinformatics	O
and	O
chemistry	O
,	O
video	O
games	O
,	O
search	O
engines	O
,	O
online	O
advertising	O
and	O
ﬁnance	O
.	O
this	O
book	O
has	O
been	O
organized	O
into	O
three	O
parts	O
in	O
order	O
to	O
best	O
accommodate	O
a	O
introduces	O
basic	O
mathematical	O
tools	O
and	O
machine	O
learning	O
describes	O
the	O
most	O
established	O
deep	O
learning	O
algorithms	O
that	O
are	O
describes	O
more	O
speculative	O
ideas	O
that	O
are	O
variety	O
of	O
readers	O
.	O
part	O
concepts	O
.	O
part	O
essentially	O
solved	O
technologies	O
.	O
part	O
widely	O
believed	O
to	O
be	O
important	O
for	O
future	O
research	O
in	O
deep	O
learning	O
.	O
iii	O
ii	O
i	O
readers	O
should	O
feel	O
free	O
to	O
skip	O
parts	O
that	O
are	O
not	O
relevant	O
given	O
their	O
interests	O
or	O
background	O
.	O
readers	O
familiar	O
with	O
linear	O
algebra	O
,	O
probability	O
,	O
and	O
fundamental	O
machine	O
learning	O
concepts	O
can	O
skip	O
part	O
,	O
for	O
example	O
,	O
while	O
readers	O
who	O
just	O
want	O
to	O
implement	O
a	O
working	O
system	O
need	O
not	O
read	O
beyond	O
part	O
.	O
to	O
help	O
choose	O
which	O
provides	O
a	O
ﬂowchart	O
showing	O
the	O
high-level	O
organization	O
chapters	O
to	O
read	O
,	O
ﬁgure	O
of	O
the	O
book	O
.	O
1.6	O
ii	O
i	O
we	O
do	O
assume	O
that	O
all	O
readers	O
come	O
from	O
a	O
computer	O
science	O
background	O
.	O
we	O
assume	O
familiarity	O
with	O
programming	O
,	O
a	O
basic	O
understanding	O
of	O
computational	O
performance	O
issues	O
,	O
complexity	O
theory	O
,	O
introductory	O
level	O
calculus	O
and	O
some	O
of	O
the	O
terminology	O
of	O
graph	O
theory	O
.	O
1.2	O
historical	O
trends	O
in	O
deep	O
learning	O
it	O
is	O
easiest	O
to	O
understand	O
deep	O
learning	O
with	O
some	O
historical	O
context	O
.	O
rather	O
than	O
providing	O
a	O
detailed	O
history	O
of	O
deep	O
learning	O
,	O
we	O
identify	O
a	O
few	O
key	O
trends	O
:	O
•	O
•	O
•	O
•	O
deep	O
learning	O
has	O
had	O
a	O
long	O
and	O
rich	O
history	O
,	O
but	O
has	O
gone	O
by	O
many	O
names	O
reﬂecting	O
diﬀerent	O
philosophical	O
viewpoints	O
,	O
and	O
has	O
waxed	O
and	O
waned	O
in	O
popularity	O
.	O
deep	O
learning	O
has	O
become	O
more	O
useful	O
as	O
the	O
amount	O
of	O
available	O
training	O
data	O
has	O
increased	O
.	O
deep	O
learning	O
models	O
have	O
grown	O
in	O
size	O
over	O
time	O
as	O
computer	O
infrastructure	O
(	O
both	O
hardware	O
and	O
software	O
)	O
for	O
deep	O
learning	O
has	O
improved	O
.	O
deep	O
learning	O
has	O
solved	O
increasingly	O
complicated	O
applications	O
with	O
increasing	O
accuracy	O
over	O
time	O
.	O
11	O
chapter	O
1.	O
introduction	O
1.	O
introduction	O
part	O
i	O
:	O
applied	O
math	O
and	O
machine	O
learning	O
basics	O
2.	O
linear	O
algebra	O
3.	O
probability	O
and	O
information	O
theory	O
4.	O
numerical	O
computation	O
5.	O
machine	O
learning	O
basics	O
part	O
ii	O
:	O
deep	O
networks	O
:	O
modern	O
practices	O
6.	O
deep	O
feedforward	O
networks	O
7.	O
regularization	O
8.	O
optimization	O
9.	O
cnns	O
10.	O
rnns	O
11.	O
practical	O
methodology	O
12.	O
applications	O
part	O
iii	O
:	O
deep	O
learning	O
research	O
13.	O
linear	O
factor	O
models	O
14.	O
autoencoders	O
15.	O
representation	O
learning	O
16.	O
structured	O
probabilistic	O
models	O
19.	O
inference	O
17.	O
monte	O
carlo	O
methods	O
18.	O
partition	O
function	O
20.	O
deep	O
generative	O
models	O
figure	O
1.6	O
:	O
the	O
high-level	O
organization	O
of	O
the	O
book	O
.	O
an	O
arrow	O
from	O
one	O
chapter	O
to	O
another	O
indicates	O
that	O
the	O
former	O
chapter	O
is	O
prerequisite	O
material	O
for	O
understanding	O
the	O
latter	O
.	O
12	O
chapter	O
1.	O
introduction	O
1.2.1	O
the	O
many	O
names	O
and	O
changing	O
fortunes	O
of	O
neural	O
net-	O
works	O
we	O
expect	O
that	O
many	O
readers	O
of	O
this	O
book	O
have	O
heard	O
of	O
deep	O
learning	O
as	O
an	O
exciting	O
new	O
technology	O
,	O
and	O
are	O
surprised	O
to	O
see	O
a	O
mention	O
of	O
“	O
history	O
”	O
in	O
a	O
book	O
about	O
an	O
emerging	O
ﬁeld	O
.	O
in	O
fact	O
,	O
deep	O
learning	O
dates	O
back	O
to	O
the	O
1940s	O
.	O
deep	O
learning	O
only	O
appears	O
to	O
be	O
new	O
,	O
because	O
it	O
was	O
relatively	O
unpopular	O
for	O
several	O
years	O
preceding	O
its	O
current	O
popularity	O
,	O
and	O
because	O
it	O
has	O
gone	O
through	O
many	O
diﬀerent	O
names	O
,	O
and	O
has	O
only	O
recently	O
become	O
called	O
“	O
deep	O
learning.	O
”	O
the	O
ﬁeld	O
has	O
been	O
rebranded	O
many	O
times	O
,	O
reﬂecting	O
the	O
inﬂuence	O
of	O
diﬀerent	O
researchers	O
and	O
diﬀerent	O
perspectives	O
.	O
a	O
comprehensive	O
history	O
of	O
deep	O
learning	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
textbook	O
.	O
however	O
,	O
some	O
basic	O
context	O
is	O
useful	O
for	O
understanding	O
deep	O
learning	O
.	O
broadly	O
speaking	O
,	O
there	O
have	O
been	O
three	O
waves	O
of	O
development	O
of	O
deep	O
learning	O
:	O
deep	O
learning	O
known	O
as	O
cybernetics	O
in	O
the	O
1940s–1960s	O
,	O
deep	O
learning	O
known	O
as	O
connectionism	O
in	O
the	O
1980s–1990s	O
,	O
and	O
the	O
current	O
resurgence	O
under	O
the	O
name	O
deep	O
learning	O
beginning	O
in	O
2006.	O
this	O
is	O
quantitatively	O
illustrated	O
in	O
ﬁgure	O
.1.7	O
some	O
of	O
the	O
earliest	O
learning	O
algorithms	O
we	O
recognize	O
today	O
were	O
intended	O
to	O
be	O
computational	O
models	O
of	O
biological	O
learning	O
,	O
i.e	O
.	O
models	O
of	O
how	O
learning	O
happens	O
or	O
could	O
happen	O
in	O
the	O
brain	O
.	O
as	O
a	O
result	O
,	O
one	O
of	O
the	O
names	O
that	O
deep	O
learning	O
has	O
gone	O
by	O
is	O
artiﬁcial	O
neural	O
networks	O
(	O
anns	O
)	O
.	O
the	O
corresponding	O
perspective	O
on	O
deep	O
learning	O
models	O
is	O
that	O
they	O
are	O
engineered	O
systems	O
inspired	O
by	O
the	O
biological	O
brain	O
(	O
whether	O
the	O
human	O
brain	O
or	O
the	O
brain	O
of	O
another	O
animal	O
)	O
.	O
while	O
the	O
kinds	O
of	O
neural	O
networks	O
used	O
for	O
machine	O
learning	O
have	O
sometimes	O
been	O
used	O
to	O
understand	O
brain	O
function	O
(	O
)	O
,	O
they	O
are	O
generally	O
not	O
designed	O
to	O
be	O
realistic	O
models	O
of	O
biological	O
function	O
.	O
the	O
neural	O
perspective	O
on	O
deep	O
learning	O
is	O
motivated	O
by	O
two	O
main	O
ideas	O
.	O
one	O
idea	O
is	O
that	O
the	O
brain	O
provides	O
a	O
proof	O
by	O
example	O
that	O
intelligent	O
behavior	O
is	O
possible	O
,	O
and	O
a	O
conceptually	O
straightforward	O
path	O
to	O
building	O
intelligence	O
is	O
to	O
reverse	O
engineer	O
the	O
computational	O
principles	O
behind	O
the	O
brain	O
and	O
duplicate	O
its	O
functionality	O
.	O
another	O
perspective	O
is	O
that	O
it	O
would	O
be	O
deeply	O
interesting	O
to	O
understand	O
the	O
brain	O
and	O
the	O
principles	O
that	O
underlie	O
human	O
intelligence	O
,	O
so	O
machine	O
learning	O
models	O
that	O
shed	O
light	O
on	O
these	O
basic	O
scientiﬁc	O
questions	O
are	O
useful	O
apart	O
from	O
their	O
ability	O
to	O
solve	O
engineering	O
applications	O
.	O
hinton	O
and	O
shallice	O
1991	O
,	O
the	O
modern	O
term	O
“	O
deep	O
learning	O
”	O
goes	O
beyond	O
the	O
neuroscientiﬁc	O
perspective	O
on	O
the	O
current	O
breed	O
of	O
machine	O
learning	O
models	O
.	O
it	O
appeals	O
to	O
a	O
more	O
general	O
principle	O
of	O
learning	O
multiple	O
levels	O
of	O
composition	O
,	O
which	O
can	O
be	O
applied	O
in	O
machine	O
learning	O
frameworks	O
that	O
are	O
not	O
necessarily	O
neurally	O
inspired	O
.	O
13	O
chapter	O
1.	O
introduction	O
e	O
s	O
a	O
r	O
h	O
p	O
r	O
o	O
d	O
r	O
o	O
w	O
f	O
o	O
y	O
c	O
n	O
e	O
u	O
q	O
e	O
r	O
f	O
cybernetics	O
(	O
connectionism	O
+	O
neural	O
networks	O
)	O
0.000250	O
0.000200	O
0.000150	O
0.000100	O
0.000050	O
0.000000	O
1940	O
1950	O
1960	O
1970	O
1980	O
1990	O
2000	O
year	O
mcculloch	O
and	O
pitts	O
1943	O
hebb	O
1949	O
figure	O
1.7	O
:	O
the	O
ﬁgure	O
shows	O
two	O
of	O
the	O
three	O
historical	O
waves	O
of	O
artiﬁcial	O
neural	O
nets	O
research	O
,	O
as	O
measured	O
by	O
the	O
frequency	O
of	O
the	O
phrases	O
“	O
cybernetics	O
”	O
and	O
“	O
connectionism	O
”	O
or	O
“	O
neural	O
networks	O
”	O
according	O
to	O
google	O
books	O
(	O
the	O
third	O
wave	O
is	O
too	O
recent	O
to	O
appear	O
)	O
.	O
the	O
ﬁrst	O
wave	O
started	O
with	O
cybernetics	O
in	O
the	O
1940s–1960s	O
,	O
with	O
the	O
development	O
of	O
theories	O
,	O
of	O
biological	O
learning	O
(	O
)	O
and	O
implementations	O
of	O
the	O
ﬁrst	O
models	O
such	O
as	O
the	O
perceptron	O
(	O
rosenblatt	O
1958	O
)	O
allowing	O
the	O
training	O
of	O
a	O
single	O
neuron	O
.	O
the	O
second	O
wave	O
started	O
with	O
the	O
connectionist	O
approach	O
of	O
the	O
1980–1995	O
period	O
,	O
with	O
back-propagation	O
(	O
)	O
to	O
train	O
a	O
neural	O
network	O
with	O
one	O
or	O
two	O
hidden	O
layers	O
.	O
the	O
current	O
and	O
third	O
wave	O
,	O
deep	O
learning	O
,	O
started	O
around	O
2006	O
(	O
hinton	O
)	O
,	O
and	O
is	O
just	O
now	O
appearing	O
in	O
book	O
,	O
et	O
al	O
.	O
form	O
as	O
of	O
2016.	O
the	O
other	O
two	O
waves	O
similarly	O
appeared	O
in	O
book	O
form	O
much	O
later	O
than	O
the	O
corresponding	O
scientiﬁc	O
activity	O
occurred	O
.	O
2007	O
ranzato	O
rumelhart	O
et	O
al	O
.	O
1986a	O
,	O
,	O
et	O
al	O
.	O
2007a	O
2006	O
bengio	O
;	O
,	O
et	O
al	O
.	O
;	O
,	O
;	O
,	O
14	O
chapter	O
1.	O
introduction	O
the	O
earliest	O
predecessors	O
of	O
modern	O
deep	O
learning	O
were	O
simple	O
linear	O
models	O
motivated	O
from	O
a	O
neuroscientiﬁc	O
perspective	O
.	O
these	O
models	O
were	O
designed	O
to	O
take	O
a	O
set	O
of	O
n	O
input	O
values	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
and	O
associate	O
them	O
with	O
an	O
output	O
y.	O
these	O
models	O
would	O
learn	O
a	O
set	O
of	O
weights	O
w1	O
,	O
.	O
.	O
.	O
,	O
wn	O
and	O
compute	O
their	O
output	O
f	O
(	O
x	O
w	O
,	O
+	O
xnwn	O
.	O
this	O
ﬁrst	O
wave	O
of	O
neural	O
networks	O
research	O
was	O
known	O
as	O
cybernetics	O
,	O
as	O
illustrated	O
in	O
ﬁgure	O
)	O
=	O
x1w1	O
+	O
···	O
.1.7	O
,	O
mcculloch	O
and	O
pitts	O
1943	O
the	O
mcculloch-pitts	O
neuron	O
(	O
)	O
was	O
an	O
early	O
model	B
of	O
brain	O
function	O
.	O
this	O
linear	O
model	B
could	O
recognize	O
two	O
diﬀerent	O
categories	O
of	O
inputs	O
by	O
testing	O
whether	O
f	O
(	O
x	O
w	O
,	O
)	O
is	O
positive	O
or	O
negative	O
.	O
of	O
course	O
,	O
for	O
the	O
model	B
to	O
correspond	O
to	O
the	O
desired	O
deﬁnition	O
of	O
the	O
categories	O
,	O
the	O
weights	O
needed	O
to	O
be	O
set	O
correctly	O
.	O
these	O
weights	O
could	O
be	O
set	O
by	O
the	O
human	O
operator	O
.	O
in	O
the	O
1950s	O
,	O
the	O
perceptron	O
(	O
rosenblatt	O
1958	O
1962	O
)	O
became	O
the	O
ﬁrst	O
model	B
that	O
could	O
learn	O
the	O
weights	O
deﬁning	O
the	O
categories	O
given	O
examples	O
of	O
inputs	O
from	O
each	O
category	O
.	O
the	O
adaptive	O
linear	O
element	O
(	O
adaline	O
)	O
,	O
which	O
dates	O
from	O
about	O
the	O
same	O
time	O
,	O
simply	O
returned	O
the	O
value	O
of	O
f	O
(	O
x	O
)	O
itself	O
to	O
predict	O
a	O
real	O
number	O
(	O
widrow	O
and	O
hoﬀ	O
1960	O
)	O
,	O
and	O
could	O
also	O
learn	O
to	O
predict	O
these	O
numbers	O
from	O
data	O
.	O
,	O
,	O
,	O
these	O
simple	O
learning	O
algorithms	O
greatly	O
aﬀected	O
the	O
modern	O
landscape	O
of	O
ma-	O
chine	B
learning	O
.	O
the	O
training	O
algorithm	O
used	O
to	O
adapt	O
the	O
weights	O
of	O
the	O
adaline	O
was	O
a	O
special	O
case	O
of	O
an	O
algorithm	O
called	O
stochastic	O
gradient	O
descent	B
.	O
slightly	O
modiﬁed	O
versions	O
of	O
the	O
stochastic	O
gradient	O
descent	B
algorithm	O
remain	O
the	O
dominant	O
training	O
algorithms	O
for	O
deep	O
learning	O
models	O
today	O
.	O
models	O
based	O
on	O
the	O
f	O
(	O
x	O
w	O
,	O
)	O
used	O
by	O
the	O
perceptron	O
and	O
adaline	O
are	O
called	O
linear	O
models	O
.	O
these	O
models	O
remain	O
some	O
of	O
the	O
most	O
widely	O
used	O
machine	O
learning	O
models	O
,	O
though	O
in	O
many	O
cases	O
they	O
are	O
trained	O
in	O
diﬀerent	O
ways	O
than	O
the	O
original	O
models	O
were	O
trained	O
.	O
linear	O
models	O
have	O
many	O
limitations	O
.	O
most	O
famously	O
,	O
they	O
can	O
not	O
learn	O
the	O
xor	O
function	O
,	O
where	O
f	O
(	O
[	O
0	O
,	O
1	O
]	O
,	O
w	O
)	O
=	O
1	O
and	O
f	O
(	O
[	O
1	O
,	O
0	O
]	O
,	O
w	O
)	O
=	O
1	O
but	O
f	O
(	O
[	O
1	O
,	O
1	O
]	O
,	O
w	O
)	O
=	O
0	O
and	O
f	O
(	O
[	O
0	O
,	O
0	O
]	O
,	O
w	O
)	O
=	O
0.	O
critics	O
who	O
observed	O
these	O
ﬂaws	O
in	O
linear	O
models	O
caused	O
a	O
backlash	O
against	O
biologically	O
inspired	O
learning	O
in	O
general	O
(	O
minsky	O
and	O
papert	O
,	O
1969	O
)	O
.	O
this	O
was	O
the	O
ﬁrst	O
major	O
dip	O
in	O
the	O
popularity	O
of	O
neural	O
networks	O
.	O
today	O
,	O
neuroscience	O
is	O
regarded	O
as	O
an	O
important	O
source	O
of	O
inspiration	O
for	O
deep	O
learning	O
researchers	O
,	O
but	O
it	O
is	O
no	O
longer	O
the	O
predominant	O
guide	O
for	O
the	O
ﬁeld	O
.	O
the	O
main	O
reason	O
for	O
the	O
diminished	O
role	O
of	O
neuroscience	O
in	O
deep	O
learning	O
research	O
today	O
is	O
that	O
we	O
simply	O
do	O
not	O
have	O
enough	O
information	O
about	O
the	O
brain	O
to	O
use	O
it	O
as	O
a	O
guide	O
.	O
to	O
obtain	O
a	O
deep	O
understanding	O
of	O
the	O
actual	O
algorithms	O
used	O
by	O
the	O
brain	O
,	O
we	O
would	O
need	O
to	O
be	O
able	O
to	O
monitor	O
the	O
activity	O
of	O
(	O
at	O
the	O
very	O
least	O
)	O
thousands	O
of	O
interconnected	O
neurons	O
simultaneously	O
.	O
because	O
we	O
are	O
not	O
able	O
to	O
do	O
this	O
,	O
we	O
are	O
far	O
from	O
understanding	O
even	O
some	O
of	O
the	O
most	O
simple	O
and	O
15	O
chapter	O
1.	O
introduction	O
well-studied	O
parts	O
of	O
the	O
brain	O
(	O
olshausen	O
and	O
field	O
2005	O
,	O
)	O
.	O
neuroscience	O
has	O
given	O
us	O
a	O
reason	O
to	O
hope	O
that	O
a	O
single	O
deep	O
learning	O
algorithm	O
can	O
solve	O
many	O
diﬀerent	O
tasks	O
.	O
neuroscientists	O
have	O
found	O
that	O
ferrets	O
can	O
learn	O
to	O
“	O
see	O
”	O
with	O
the	O
auditory	O
processing	O
region	O
of	O
their	O
brain	O
if	O
their	O
brains	O
are	O
rewired	O
to	O
send	O
visual	O
signals	O
to	O
that	O
area	O
(	O
von	O
melchner	O
)	O
.	O
this	O
suggests	O
that	O
much	O
of	O
the	O
mammalian	O
brain	O
might	O
use	O
a	O
single	O
algorithm	O
to	O
solve	O
most	O
of	O
the	O
diﬀerent	O
tasks	O
that	O
the	O
brain	O
solves	O
.	O
before	O
this	O
hypothesis	O
,	O
machine	O
learning	O
research	O
was	O
more	O
fragmented	O
,	O
with	O
diﬀerent	O
communities	O
of	O
researchers	O
studying	O
natural	O
language	O
processing	O
,	O
vision	O
,	O
motion	O
planning	O
and	O
speech	O
recognition	B
.	O
today	O
,	O
these	O
application	O
communities	O
are	O
still	O
separate	O
,	O
but	O
it	O
is	O
common	O
for	O
deep	O
learning	O
research	O
groups	O
to	O
study	O
many	O
or	O
even	O
all	O
of	O
these	O
application	O
areas	O
simultaneously	O
.	O
et	O
al.	O
,	O
2000	O
,	O
,	O
,	O
9.10	O
lecun	O
et	O
al	O
.	O
1998b	O
we	O
are	O
able	O
to	O
draw	O
some	O
rough	O
guidelines	O
from	O
neuroscience	O
.	O
the	O
basic	O
idea	O
of	O
having	O
many	O
computational	O
units	O
that	O
become	O
intelligent	O
only	O
via	O
their	O
interactions	O
with	O
each	O
other	O
is	O
inspired	O
by	O
the	O
brain	O
.	O
the	O
neocognitron	O
(	O
fukushima	O
1980	O
)	O
introduced	O
a	O
powerful	O
model	B
architecture	O
for	O
processing	O
images	O
that	O
was	O
inspired	O
by	O
the	O
structure	O
of	O
the	O
mammalian	O
visual	O
system	O
and	O
later	O
became	O
the	O
basis	O
for	O
the	O
modern	O
convolutional	O
network	O
(	O
)	O
,	O
as	O
we	O
will	O
see	O
in	O
.	O
most	O
neural	O
networks	O
today	O
are	O
based	O
on	O
a	O
model	B
neuron	O
called	O
section	O
the	O
rectiﬁed	O
linear	O
unit	O
.	O
the	O
original	O
cognitron	O
(	O
fukushima	O
1975	O
)	O
introduced	O
a	O
more	O
complicated	O
version	O
that	O
was	O
highly	O
inspired	O
by	O
our	O
knowledge	O
of	O
brain	O
function	O
.	O
the	O
simpliﬁed	O
modern	O
version	O
was	O
developed	O
incorporating	O
ideas	O
from	O
many	O
viewpoints	O
,	O
with	O
)	O
citing	O
neuroscience	O
as	O
an	O
inﬂuence	O
,	O
and	O
)	O
citing	O
more	O
engineering-	O
oriented	O
inﬂuences	O
.	O
while	O
neuroscience	O
is	O
an	O
important	O
source	O
of	O
inspiration	O
,	O
it	O
need	O
not	O
be	O
taken	O
as	O
a	O
rigid	O
guide	O
.	O
we	O
know	O
that	O
actual	O
neurons	O
compute	O
very	O
diﬀerent	O
functions	O
than	O
modern	O
rectiﬁed	O
linear	O
units	O
,	O
but	O
greater	O
neural	O
realism	O
has	O
not	O
yet	O
led	O
to	O
an	O
improvement	O
in	O
machine	O
learning	O
performance	O
.	O
also	O
,	O
while	O
neuroscience	O
has	O
successfully	O
inspired	O
several	O
neural	O
network	O
architectures	O
,	O
we	O
do	O
not	O
yet	O
know	O
enough	O
about	O
biological	O
learning	O
for	O
neuroscience	O
to	O
oﬀer	O
much	O
guidance	O
for	O
the	O
learning	O
algorithms	O
we	O
use	O
to	O
train	O
these	O
architectures	O
.	O
nair	O
and	O
hinton	O
2010	O
jarrett	O
et	O
al	O
.	O
2009	O
glorot	O
et	O
al	O
.	O
2011a	O
(	O
)	O
and	O
(	O
(	O
media	O
accounts	O
often	O
emphasize	O
the	O
similarity	O
of	O
deep	O
learning	O
to	O
the	O
brain	O
.	O
while	O
it	O
is	O
true	O
that	O
deep	O
learning	O
researchers	O
are	O
more	O
likely	O
to	O
cite	O
the	O
brain	O
as	O
an	O
inﬂuence	O
than	O
researchers	O
working	O
in	O
other	O
machine	O
learning	O
ﬁelds	O
such	O
as	O
kernel	O
machines	O
or	O
bayesian	O
statistics	O
,	O
one	O
should	O
not	O
view	O
deep	O
learning	O
as	O
an	O
attempt	O
to	O
simulate	O
the	O
brain	O
.	O
modern	O
deep	O
learning	O
draws	O
inspiration	O
from	O
many	O
ﬁelds	O
,	O
especially	O
applied	O
math	O
fundamentals	O
like	O
linear	O
algebra	O
,	O
probability	O
,	O
information	O
theory	O
,	O
and	O
numerical	O
optimization	O
.	O
while	O
some	O
deep	O
learning	O
researchers	O
cite	O
neuroscience	O
as	O
an	O
important	O
source	O
of	O
inspiration	O
,	O
others	O
are	O
not	O
concerned	O
with	O
16	O
chapter	O
1.	O
introduction	O
neuroscience	O
at	O
all	O
.	O
it	O
is	O
worth	O
noting	O
that	O
the	O
eﬀort	O
to	O
understand	O
how	O
the	O
brain	O
works	O
on	O
an	O
algorithmic	O
level	O
is	O
alive	O
and	O
well	O
.	O
this	O
endeavor	O
is	O
primarily	O
known	O
as	O
“	O
computational	O
neuroscience	O
”	O
and	O
is	O
a	O
separate	O
ﬁeld	O
of	O
study	O
from	O
deep	O
learning	O
.	O
it	O
is	O
common	O
for	O
researchers	O
to	O
move	O
back	O
and	O
forth	O
between	O
both	O
ﬁelds	O
.	O
the	O
ﬁeld	O
of	O
deep	O
learning	O
is	O
primarily	O
concerned	O
with	O
how	O
to	O
build	O
computer	O
systems	O
that	O
are	O
able	O
to	O
successfully	O
solve	O
tasks	O
requiring	O
intelligence	O
,	O
while	O
the	O
ﬁeld	O
of	O
computational	O
neuroscience	O
is	O
primarily	O
concerned	O
with	O
building	O
more	O
accurate	O
models	O
of	O
how	O
the	O
brain	O
actually	O
works	O
.	O
,	O
;	O
,	O
rumelhart	O
et	O
al	O
.	O
1986c	O
mcclelland	O
et	O
al	O
.	O
1995	O
in	O
the	O
1980s	O
,	O
the	O
second	O
wave	O
of	O
neural	O
network	O
research	O
emerged	O
in	O
great	O
part	O
via	O
a	O
movement	O
called	O
connectionism	O
or	O
parallel	O
distributed	O
process-	O
ing	O
(	O
)	O
.	O
connectionism	O
arose	O
in	O
the	O
context	O
of	O
cognitive	O
science	O
.	O
cognitive	O
science	O
is	O
an	O
interdisciplinary	O
approach	O
to	O
understanding	O
the	O
mind	O
,	O
combining	O
multiple	O
diﬀerent	O
levels	O
of	O
analysis	O
.	O
during	O
the	O
early	O
1980s	O
,	O
most	O
cognitive	O
scientists	O
studied	O
models	O
of	O
symbolic	O
reasoning	O
.	O
despite	O
their	O
popularity	O
,	O
symbolic	O
models	O
were	O
diﬃcult	O
to	O
explain	O
in	O
terms	O
of	O
how	O
the	O
brain	O
could	O
actually	O
implement	O
them	O
using	O
neurons	O
.	O
the	O
connectionists	O
began	O
to	O
study	O
models	O
of	O
cognition	O
that	O
could	O
actually	O
be	O
grounded	O
in	O
neural	O
implementations	O
(	O
touretzky	O
and	O
minton	O
1985	O
)	O
,	O
reviving	O
many	O
ideas	O
dating	O
back	O
to	O
the	O
work	B
of	O
psychologist	O
donald	O
hebb	O
in	O
the	O
1940s	O
(	O
)	O
.	O
hebb	O
1949	O
,	O
,	O
the	O
central	O
idea	O
in	O
connectionism	O
is	O
that	O
a	O
large	O
number	O
of	O
simple	O
computational	O
units	O
can	O
achieve	O
intelligent	O
behavior	O
when	O
networked	O
together	O
.	O
this	O
insight	O
applies	O
equally	O
to	O
neurons	O
in	O
biological	O
nervous	O
systems	O
and	O
to	O
hidden	O
units	O
in	O
computational	O
models	O
.	O
several	O
key	O
concepts	O
arose	O
during	O
the	O
connectionism	O
movement	O
of	O
the	O
1980s	O
that	O
remain	O
central	O
to	O
today	O
’	O
s	O
deep	O
learning	O
.	O
one	O
of	O
these	O
concepts	O
is	O
that	O
of	O
distributed	O
representation	O
(	O
hinton	O
et	O
al.	O
,	O
1986	O
)	O
.	O
this	O
is	O
the	O
idea	O
that	O
each	O
input	O
to	O
a	O
system	O
should	O
be	O
represented	O
by	O
many	O
features	O
,	O
and	O
each	O
feature	O
should	O
be	O
involved	O
in	O
the	O
representation	O
of	O
many	O
possible	O
inputs	O
.	O
for	O
example	O
,	O
suppose	O
we	O
have	O
a	O
vision	O
system	O
that	O
can	O
recognize	O
cars	O
,	O
trucks	O
,	O
and	O
birds	O
and	O
these	O
objects	O
can	O
each	O
be	O
red	O
,	O
green	O
,	O
or	O
blue	O
.	O
one	O
way	O
of	O
representing	O
these	O
inputs	O
would	O
be	O
to	O
have	O
a	O
separate	O
neuron	O
or	O
hidden	O
unit	O
that	O
activates	O
for	O
each	O
of	O
the	O
nine	O
possible	O
combinations	O
:	O
red	O
truck	O
,	O
red	O
car	O
,	O
red	O
bird	O
,	O
green	O
truck	O
,	O
and	O
so	O
on	O
.	O
this	O
requires	O
nine	O
diﬀerent	O
neurons	O
,	O
and	O
each	O
neuron	O
must	O
independently	O
learn	O
the	O
concept	O
of	O
color	O
and	O
object	O
identity	O
.	O
one	O
way	O
to	O
improve	O
on	O
this	O
situation	O
is	O
to	O
use	O
a	O
distributed	O
representation	O
,	O
with	O
three	O
neurons	O
describing	O
the	O
color	O
and	O
three	O
neurons	O
describing	O
the	O
object	O
identity	O
.	O
this	O
requires	O
only	O
six	O
neurons	O
total	O
instead	O
of	O
nine	O
,	O
and	O
the	O
neuron	O
describing	O
redness	O
is	O
able	O
to	O
17	O
chapter	O
1.	O
introduction	O
learn	O
about	O
redness	O
from	O
images	O
of	O
cars	O
,	O
trucks	O
and	O
birds	O
,	O
not	O
only	O
from	O
images	O
of	O
one	O
speciﬁc	O
category	O
of	O
objects	O
.	O
the	O
concept	O
of	O
distributed	O
representation	O
is	O
central	O
to	O
this	O
book	O
,	O
and	O
will	O
be	O
described	O
in	O
greater	O
detail	O
in	O
chapter	O
.15	O
another	O
major	O
accomplishment	O
of	O
the	O
connectionist	O
movement	O
was	O
the	O
suc-	O
cessful	O
use	O
of	O
back-propagation	O
to	O
train	O
deep	O
neural	O
networks	O
with	O
internal	O
repre-	O
sentations	O
and	O
the	O
popularization	O
of	O
the	O
back-propagation	O
algorithm	O
(	O
rumelhart	O
et	O
al.	O
,	O
)	O
.	O
this	O
algorithm	O
has	O
waxed	O
and	O
waned	O
in	O
popularity	O
but	O
as	O
of	O
this	O
writing	O
is	O
currently	O
the	O
dominant	O
approach	O
to	O
training	O
deep	O
models	O
.	O
1986a	O
lecun	O
1987	O
;	O
,	O
(	O
hochreiter	O
1991	O
during	O
the	O
1990s	O
,	O
researchers	O
made	O
important	O
advances	O
in	O
modeling	O
sequences	O
)	O
identiﬁed	O
some	O
of	O
with	O
neural	O
networks	O
.	O
the	O
fundamental	O
mathematical	O
diﬃculties	O
in	O
modeling	O
long	O
sequences	O
,	O
described	O
in	O
section	O
)	O
introduced	O
the	O
long	O
short-term	O
memory	O
or	O
lstm	O
network	O
to	O
resolve	O
some	O
of	O
these	O
diﬃculties	O
.	O
today	O
,	O
the	O
lstm	O
is	O
widely	O
used	O
for	O
many	O
sequence	O
modeling	O
tasks	O
,	O
including	O
many	O
natural	O
language	O
processing	O
tasks	O
at	O
google	O
.	O
10.7	O
hochreiter	O
and	O
schmidhuber	O
1997	O
bengio	O
et	O
al	O
.	O
1994	O
)	O
and	O
(	O
(	O
.	O
the	O
second	O
wave	O
of	O
neural	O
networks	O
research	O
lasted	O
until	O
the	O
mid-1990s	O
.	O
ven-	O
tures	O
based	O
on	O
neural	O
networks	O
and	O
other	O
ai	O
technologies	O
began	O
to	O
make	O
unrealisti-	O
cally	O
ambitious	O
claims	O
while	O
seeking	O
investments	O
.	O
when	O
ai	O
research	O
did	O
not	O
fulﬁll	O
these	O
unreasonable	O
expectations	O
,	O
investors	O
were	O
disappointed	O
.	O
simultaneously	O
,	O
,	O
other	O
ﬁelds	O
of	O
machine	O
learning	O
made	O
advances	O
.	O
kernel	O
machines	O
(	O
boser	O
et	O
al	O
.	O
1992	O
cortes	O
and	O
vapnik	O
1995	O
schölkopf	O
jor-	O
)	O
both	O
achieved	O
good	O
results	O
on	O
many	O
important	O
tasks	O
.	O
these	O
two	O
factors	O
dan	O
1998	O
led	O
to	O
a	O
decline	O
in	O
the	O
popularity	O
of	O
neural	O
networks	O
that	O
lasted	O
until	O
2007.	O
)	O
and	O
graphical	O
models	O
(	O
et	O
al.	O
,	O
1999	O
;	O
,	O
;	O
,	O
,	O
;	O
,	O
lecun	O
et	O
al	O
.	O
1998b	O
bengio	O
et	O
al	O
.	O
2001	O
during	O
this	O
time	O
,	O
neural	O
networks	O
continued	O
to	O
obtain	O
impressive	O
performance	O
)	O
.	O
the	O
canadian	O
institute	O
on	O
some	O
tasks	O
(	O
for	O
advanced	O
research	O
(	O
cifar	O
)	O
helped	O
to	O
keep	O
neural	O
networks	O
research	O
alive	O
via	O
its	O
neural	O
computation	O
and	O
adaptive	O
perception	O
(	O
ncap	O
)	O
research	O
initiative	O
.	O
this	O
program	O
united	O
machine	O
learning	O
research	O
groups	O
led	O
by	O
geoﬀrey	O
hinton	O
at	O
university	O
of	O
toronto	O
,	O
yoshua	O
bengio	O
at	O
university	O
of	O
montreal	O
,	O
and	O
yann	O
lecun	O
at	O
new	O
york	O
university	O
.	O
the	O
cifar	O
ncap	O
research	O
initiative	O
had	O
a	O
multi-disciplinary	O
nature	O
that	O
also	O
included	O
neuroscientists	O
and	O
experts	O
in	O
human	O
and	O
computer	O
vision	O
.	O
at	O
this	O
point	O
in	O
time	O
,	O
deep	O
networks	O
were	O
generally	O
believed	O
to	O
be	O
very	O
diﬃcult	O
to	O
train	O
.	O
we	O
now	O
know	O
that	O
algorithms	O
that	O
have	O
existed	O
since	O
the	O
1980s	O
work	B
quite	O
well	O
,	O
but	O
this	O
was	O
not	O
apparent	O
circa	O
2006.	O
the	O
issue	O
is	O
perhaps	O
simply	O
that	O
these	O
algorithms	O
were	O
too	O
computationally	O
costly	O
to	O
allow	O
much	O
experimentation	O
with	O
the	O
hardware	O
available	O
at	O
the	O
time	O
.	O
the	O
third	O
wave	O
of	O
neural	O
networks	O
research	O
began	O
with	O
a	O
breakthrough	O
in	O
18	O
chapter	O
1.	O
introduction	O
,	O
;	O
;	O
,	O
2007a	O
et	O
al.	O
,	O
hinton	O
et	O
al	O
.	O
2006	O
)	O
,	O
which	O
will	O
be	O
described	O
in	O
more	O
detail	O
in	O
section	O
2006.	O
geoﬀrey	O
hinton	O
showed	O
that	O
a	O
kind	O
of	O
neural	O
network	O
called	O
a	O
deep	O
belief	O
network	O
could	O
be	O
eﬃciently	O
trained	O
using	O
a	O
strategy	O
called	O
greedy	O
layer-wise	O
pre-	O
training	O
(	O
.	O
15.1	O
the	O
other	O
cifar-aﬃliated	O
research	O
groups	O
quickly	O
showed	O
that	O
the	O
same	O
strategy	O
could	O
be	O
used	O
to	O
train	O
many	O
other	O
kinds	O
of	O
deep	O
networks	O
(	O
bengio	O
et	O
al	O
.	O
2007	O
;	O
ranzato	O
)	O
and	O
systematically	O
helped	O
to	O
improve	O
generalization	O
on	O
test	O
examples	O
.	O
this	O
wave	O
of	O
neural	O
networks	O
research	O
popularized	O
the	O
use	O
of	O
the	O
term	O
“	O
deep	O
learning	O
”	O
to	O
emphasize	O
that	O
researchers	O
were	O
now	O
able	O
to	O
train	O
deeper	O
neural	O
networks	O
than	O
had	O
been	O
possible	O
before	O
,	O
and	O
to	O
focus	O
attention	O
on	O
the	O
theoretical	O
importance	O
of	O
depth	O
(	O
bengio	O
and	O
lecun	O
2007	O
delalleau	O
and	O
bengio	O
,	O
,	O
2011	O
pascanu	O
)	O
.	O
at	O
this	O
time	O
,	O
deep	O
neural	O
networks	O
outperformed	O
competing	O
ai	O
systems	O
based	O
on	O
other	O
machine	O
learning	O
technologies	O
as	O
well	O
as	O
hand-designed	O
functionality	O
.	O
this	O
third	O
wave	O
of	O
popularity	O
of	O
neural	O
networks	O
continues	O
to	O
the	O
time	O
of	O
this	O
writing	O
,	O
though	O
the	O
focus	O
of	O
deep	O
learning	O
research	O
has	O
changed	O
dramatically	O
within	O
the	O
time	O
of	O
this	O
wave	O
.	O
the	O
third	O
wave	O
began	O
with	O
a	O
focus	O
on	O
new	O
unsupervised	O
learning	O
techniques	O
and	O
the	O
ability	O
of	O
deep	O
models	O
to	O
generalize	O
well	O
from	O
small	O
datasets	O
,	O
but	O
today	O
there	O
is	O
more	O
interest	O
in	O
much	O
older	O
supervised	O
learning	O
algorithms	O
and	O
the	O
ability	O
of	O
deep	O
models	O
to	O
leverage	O
large	O
labeled	O
datasets	O
.	O
et	O
al.	O
,	O
2014a	O
montufar	O
;	O
et	O
al.	O
,	O
2014	O
1.2.2	O
increasing	O
dataset	O
sizes	O
one	O
may	O
wonder	O
why	O
deep	O
learning	O
has	O
only	O
recently	O
become	O
recognized	O
as	O
a	O
crucial	O
technology	O
though	O
the	O
ﬁrst	O
experiments	O
with	O
artiﬁcial	O
neural	O
networks	O
were	O
conducted	O
in	O
the	O
1950s	O
.	O
deep	O
learning	O
has	O
been	O
successfully	O
used	O
in	O
commercial	O
applications	O
since	O
the	O
1990s	O
,	O
but	O
was	O
often	O
regarded	O
as	O
being	O
more	O
of	O
an	O
art	O
than	O
a	O
technology	O
and	O
something	O
that	O
only	O
an	O
expert	O
could	O
use	O
,	O
until	O
recently	O
.	O
it	O
is	O
true	O
that	O
some	O
skill	O
is	O
required	O
to	O
get	O
good	O
performance	O
from	O
a	O
deep	O
learning	O
algorithm	O
.	O
fortunately	O
,	O
the	O
amount	O
of	O
skill	O
required	O
reduces	O
as	O
the	O
amount	O
of	O
training	O
data	O
increases	O
.	O
the	O
learning	O
algorithms	O
reaching	O
human	O
performance	O
on	O
complex	O
tasks	O
today	O
are	O
nearly	O
identical	O
to	O
the	O
learning	O
algorithms	O
that	O
struggled	O
to	O
solve	O
toy	O
problems	O
in	O
the	O
1980s	O
,	O
though	O
the	O
models	O
we	O
train	O
with	O
these	O
algorithms	O
have	O
undergone	O
changes	O
that	O
simplify	O
the	O
training	O
of	O
very	O
deep	O
architectures	O
.	O
the	O
most	O
important	O
new	O
development	O
is	O
that	O
today	O
we	O
can	O
provide	O
these	O
algorithms	O
with	O
the	O
resources	O
they	O
need	O
to	O
succeed	O
.	O
figure	O
shows	O
how	O
the	O
size	O
of	O
benchmark	O
datasets	O
has	O
increased	O
remarkably	O
over	O
time	O
.	O
this	O
trend	O
is	O
driven	O
by	O
the	O
increasing	O
digitization	O
of	O
society	O
.	O
as	O
more	O
and	O
more	O
of	O
our	O
activities	O
take	O
place	O
on	O
computers	O
,	O
more	O
and	O
more	O
of	O
what	O
we	O
do	O
is	O
recorded	O
.	O
as	O
our	O
computers	O
are	O
increasingly	O
networked	O
together	O
,	O
it	O
becomes	O
easier	O
to	O
centralize	O
these	O
records	O
and	O
curate	O
them	O
1.8	O
19	O
chapter	O
1.	O
introduction	O
into	O
a	O
dataset	O
appropriate	O
for	O
machine	O
learning	O
applications	O
.	O
the	O
age	O
of	O
“	O
big	O
data	O
”	O
has	O
made	O
machine	O
learning	O
much	O
easier	O
because	O
the	O
key	O
burden	O
of	O
statistical	O
estimation—generalizing	O
well	O
to	O
new	O
data	O
after	O
observing	O
only	O
a	O
small	O
amount	O
of	O
data—has	O
been	O
considerably	O
lightened	O
.	O
as	O
of	O
2016	O
,	O
a	O
rough	O
rule	O
of	O
thumb	O
is	O
that	O
a	O
supervised	O
deep	O
learning	O
algorithm	O
will	O
generally	O
achieve	O
acceptable	O
performance	O
with	O
around	O
5,000	O
labeled	O
examples	O
per	O
category	O
,	O
and	O
will	O
match	O
or	O
exceed	O
human	O
performance	O
when	O
trained	O
with	O
a	O
dataset	O
containing	O
at	O
least	O
10	O
million	O
labeled	O
examples	O
.	O
working	O
successfully	O
with	O
datasets	O
smaller	O
than	O
this	O
is	O
an	O
important	O
research	O
area	O
,	O
focusing	O
in	O
particular	O
on	O
how	O
we	O
can	O
take	O
advantage	O
of	O
large	O
quantities	O
of	O
unlabeled	O
examples	O
,	O
with	O
unsupervised	O
or	O
semi-supervised	O
learning	O
.	O
1.2.3	O
increasing	O
model	B
sizes	O
another	O
key	O
reason	O
that	O
neural	O
networks	O
are	O
wildly	O
successful	O
today	O
after	O
enjoying	O
comparatively	O
little	O
success	O
since	O
the	O
1980s	O
is	O
that	O
we	O
have	O
the	O
computational	O
resources	O
to	O
run	O
much	O
larger	O
models	O
today	O
.	O
one	O
of	O
the	O
main	O
insights	O
of	O
connection-	O
ism	O
is	O
that	O
animals	O
become	O
intelligent	O
when	O
many	O
of	O
their	O
neurons	O
work	B
together	O
.	O
an	O
individual	O
neuron	O
or	O
small	O
collection	O
of	O
neurons	O
is	O
not	O
particularly	O
useful	O
.	O
biological	O
neurons	O
are	O
not	O
especially	O
densely	O
connected	O
.	O
as	O
seen	O
in	O
ﬁgure	O
1.10	O
,	O
our	O
machine	O
learning	O
models	O
have	O
had	O
a	O
number	O
of	O
connections	O
per	O
neuron	O
that	O
was	O
within	O
an	O
order	O
of	O
magnitude	O
of	O
even	O
mammalian	O
brains	O
for	O
decades	O
.	O
1.11	O
in	O
terms	O
of	O
the	O
total	O
number	O
of	O
neurons	O
,	O
neural	O
networks	O
have	O
been	O
astonishingly	O
small	O
until	O
quite	O
recently	O
,	O
as	O
shown	O
in	O
ﬁgure	O
.	O
since	O
the	O
introduction	O
of	O
hidden	O
units	O
,	O
artiﬁcial	O
neural	O
networks	O
have	O
doubled	O
in	O
size	O
roughly	O
every	O
2.4	O
years	O
.	O
this	O
growth	O
is	O
driven	O
by	O
faster	O
computers	O
with	O
larger	O
memory	O
and	O
by	O
the	O
availability	O
of	O
larger	O
datasets	O
.	O
larger	O
networks	O
are	O
able	O
to	O
achieve	O
higher	O
accuracy	O
on	O
more	O
complex	O
tasks	O
.	O
this	O
trend	O
looks	O
set	O
to	O
continue	O
for	O
decades	O
.	O
unless	O
new	O
technologies	O
allow	O
faster	O
scaling	O
,	O
artiﬁcial	O
neural	O
networks	O
will	O
not	O
have	O
the	O
same	O
number	O
of	O
neurons	O
as	O
the	O
human	O
brain	O
until	O
at	O
least	O
the	O
2050s	O
.	O
biological	O
neurons	O
may	O
represent	O
more	O
complicated	O
functions	O
than	O
current	O
artiﬁcial	O
neurons	O
,	O
so	O
biological	O
neural	O
networks	O
may	O
be	O
even	O
larger	O
than	O
this	O
plot	O
portrays	O
.	O
in	O
retrospect	O
,	O
it	O
is	O
not	O
particularly	O
surprising	O
that	O
neural	O
networks	O
with	O
fewer	O
neurons	O
than	O
a	O
leech	O
were	O
unable	O
to	O
solve	O
sophisticated	O
artiﬁcial	O
intelligence	O
prob-	O
lems	O
.	O
even	O
today	O
’	O
s	O
networks	O
,	O
which	O
we	O
consider	O
quite	O
large	O
from	O
a	O
computational	O
systems	O
point	O
of	O
view	O
,	O
are	O
smaller	O
than	O
the	O
nervous	O
system	O
of	O
even	O
relatively	O
primitive	O
vertebrate	O
animals	O
like	O
frogs	O
.	O
the	O
increase	O
in	O
model	B
size	O
over	O
time	O
,	O
due	O
to	O
the	O
availability	O
of	O
faster	O
cpus	O
,	O
20	O
chapter	O
1.	O
introduction	O
109	O
108	O
107	O
106	O
105	O
104	O
103	O
102	O
101	O
100	O
)	O
s	O
e	O
l	O
p	O
m	O
a	O
x	O
e	O
r	O
e	O
b	O
m	O
u	O
n	O
(	O
e	O
z	O
i	O
s	O
t	O
e	O
s	O
a	O
t	O
a	O
d	O
canadian	O
hansard	O
wmt	O
sports-1m	O
imagenet10k	O
public	O
svhn	O
criminals	O
imagenet	O
ilsvrc	O
2014	O
mnist	O
cifar-10	O
t	O
vs.	O
g	O
vs.	O
f	O
rotated	O
t	O
vs.	O
c	O
iris	O
1900	O
1950	O
1985	O
2000	O
2015	O
year	O
;	O
,	O
;	O
,	O
;	O
,	O
,	O
;	O
et	O
al.	O
,	O
1986b	O
figure	O
1.8	O
:	O
dataset	O
sizes	O
have	O
increased	O
greatly	O
over	O
time	O
.	O
in	O
the	O
early	O
1900s	O
,	O
statisticians	O
studied	O
datasets	O
using	O
hundreds	O
or	O
thousands	O
of	O
manually	O
compiled	O
measurements	O
(	O
garson	O
,	O
)	O
.	O
in	O
the	O
1950s	O
through	O
1980s	O
,	O
the	O
pioneers	O
1900	O
gosset	O
1908	O
anderson	O
1935	O
fisher	O
1936	O
of	O
biologically	O
inspired	O
machine	O
learning	O
often	O
worked	O
with	O
small	O
,	O
synthetic	O
datasets	O
,	O
such	O
as	O
low-resolution	O
bitmaps	O
of	O
letters	O
,	O
that	O
were	O
designed	O
to	O
incur	O
low	O
computational	O
cost	O
and	O
demonstrate	O
that	O
neural	O
networks	O
were	O
able	O
to	O
learn	O
speciﬁc	O
kinds	O
of	O
functions	O
(	O
widrow	O
and	O
hoﬀ	O
1960	O
rumelhart	O
)	O
.	O
in	O
the	O
1980s	O
and	O
1990s	O
,	O
machine	O
learning	O
became	O
more	O
statistical	O
in	O
nature	O
and	O
began	O
to	O
leverage	O
larger	O
datasets	O
containing	O
tens	O
of	O
thousands	O
of	O
examples	O
such	O
as	O
the	O
mnist	O
dataset	O
(	O
shown	O
in	O
ﬁgure	O
)	O
of	O
scans	O
of	O
handwritten	O
numbers	O
(	O
)	O
.	O
in	O
the	O
ﬁrst	O
decade	O
of	O
the	O
2000s	O
,	O
more	O
sophisticated	O
datasets	O
of	O
this	O
same	O
size	O
,	O
such	O
as	O
the	O
cifar-10	O
dataset	O
(	O
krizhevsky	O
and	O
hinton	O
2009	O
)	O
continued	O
to	O
be	O
produced	O
.	O
toward	O
the	O
end	O
of	O
that	O
decade	O
and	O
throughout	O
the	O
ﬁrst	O
half	O
of	O
the	O
2010s	O
,	O
signiﬁcantly	O
larger	O
datasets	O
,	O
containing	O
hundreds	O
of	O
thousands	O
to	O
tens	O
of	O
millions	O
of	O
examples	O
,	O
completely	O
changed	O
what	O
was	O
possible	O
with	O
deep	O
learning	O
.	O
these	O
datasets	O
included	O
the	O
public	O
street	O
view	O
house	O
numbers	O
dataset	O
(	O
netzer	O
et	O
al	O
.	O
,	O
,	O
2011	O
)	O
,	O
various	O
versions	O
of	O
the	O
imagenet	O
dataset	O
(	O
deng	O
et	O
al	O
.	O
2009	O
2010a	O
russakovsky	O
;	O
2014	O
)	O
.	O
at	O
the	O
top	O
of	O
the	O
,	O
et	O
al	O
.	O
graph	O
,	O
we	O
see	O
that	O
datasets	O
of	O
translated	O
sentences	O
,	O
such	O
as	O
ibm	O
’	O
s	O
dataset	O
constructed	O
from	O
the	O
canadian	O
hansard	O
(	O
)	O
and	O
the	O
wmt	O
2014	O
english	O
to	O
french	O
dataset	O
(	O
schwenk	O
2014	O
)	O
are	O
typically	O
far	O
ahead	O
of	O
other	O
dataset	O
sizes	O
.	O
)	O
,	O
and	O
the	O
sports-1m	O
dataset	O
(	O
lecun	O
et	O
al	O
.	O
1998b	O
,	O
brown	O
et	O
al	O
.	O
1990	O
,	O
,	O
,	O
et	O
al	O
.	O
karpathy	O
1.9	O
,	O
2014a	O
,	O
21	O
chapter	O
1.	O
introduction	O
figure	O
1.9	O
:	O
example	O
inputs	O
from	O
the	O
mnist	O
dataset	O
.	O
the	O
“	O
nist	O
”	O
stands	O
for	O
national	O
institute	O
of	O
standards	O
and	O
technology	O
,	O
the	O
agency	O
that	O
originally	O
collected	O
this	O
data	O
.	O
the	O
“	O
m	O
”	O
stands	O
for	O
“	O
modiﬁed	O
,	O
”	O
since	O
the	O
data	O
has	O
been	O
preprocessed	O
for	O
easier	O
use	O
with	O
machine	O
learning	O
algorithms	O
.	O
the	O
mnist	O
dataset	O
consists	O
of	O
scans	O
of	O
handwritten	O
digits	O
and	O
associated	O
labels	O
describing	O
which	O
digit	O
0–9	O
is	O
contained	O
in	O
each	O
image	O
.	O
this	O
simple	O
classiﬁcation	O
problem	O
is	O
one	O
of	O
the	O
simplest	O
and	O
most	O
widely	O
used	O
tests	O
in	O
deep	O
learning	O
research	O
.	O
it	O
remains	O
popular	O
despite	O
being	O
quite	O
easy	O
for	O
modern	O
techniques	O
to	O
solve	O
.	O
geoﬀrey	O
hinton	O
has	O
described	O
it	O
as	O
“	O
the	O
drosophila	O
of	O
machine	O
learning	O
,	O
”	O
meaning	O
that	O
it	O
allows	O
machine	O
learning	O
researchers	O
to	O
study	O
their	O
algorithms	O
in	O
controlled	O
laboratory	O
conditions	B
,	O
much	O
as	O
biologists	O
often	O
study	O
fruit	O
ﬂies	O
.	O
22	O
chapter	O
1.	O
introduction	O
)	O
,	O
faster	O
network	O
the	O
advent	O
of	O
general	O
purpose	O
gpus	O
(	O
described	O
in	O
section	O
connectivity	O
and	O
better	O
software	O
infrastructure	O
for	O
distributed	O
computing	O
,	O
is	O
one	O
of	O
the	O
most	O
important	O
trends	O
in	O
the	O
history	O
of	O
deep	O
learning	O
.	O
this	O
trend	O
is	O
generally	O
expected	O
to	O
continue	O
well	O
into	O
the	O
future	O
.	O
12.1.2	O
1.2.4	O
increasing	O
accuracy	O
,	O
complexity	O
and	O
real-world	O
impact	O
since	O
the	O
1980s	O
,	O
deep	O
learning	O
has	O
consistently	O
improved	O
in	O
its	O
ability	O
to	O
provide	O
accurate	O
recognition	B
or	O
prediction	O
.	O
moreover	O
,	O
deep	O
learning	O
has	O
consistently	O
been	O
applied	O
with	O
success	O
to	O
broader	O
and	O
broader	O
sets	O
of	O
applications	O
.	O
,	O
,	O
rumelhart	O
et	O
al	O
.	O
1986a	O
the	O
earliest	O
deep	O
models	O
were	O
used	O
to	O
recognize	O
individual	O
objects	O
in	O
tightly	O
cropped	O
,	O
extremely	O
small	O
images	O
(	O
)	O
.	O
since	O
then	O
there	O
has	O
been	O
a	O
gradual	O
increase	O
in	O
the	O
size	O
of	O
images	O
neural	O
networks	O
could	O
process	O
.	O
modern	O
object	O
recognition	B
networks	O
process	O
rich	O
high-resolution	O
photographs	O
and	O
do	O
not	O
have	O
a	O
requirement	O
that	O
the	O
photo	O
be	O
cropped	O
near	O
the	O
object	O
to	O
be	O
recognized	O
(	O
krizhevsky	O
et	O
al	O
.	O
2012	O
)	O
.	O
similarly	O
,	O
the	O
earliest	O
networks	O
could	O
only	O
recognize	O
two	O
kinds	O
of	O
objects	O
(	O
or	O
in	O
some	O
cases	O
,	O
the	O
absence	O
or	O
presence	O
of	O
a	O
single	O
kind	O
of	O
object	O
)	O
,	O
while	O
these	O
modern	O
networks	O
typically	O
recognize	O
at	O
least	O
1,000	O
diﬀerent	O
categories	O
of	O
objects	O
.	O
the	O
largest	O
contest	O
in	O
object	O
recognition	B
is	O
the	O
imagenet	O
large	O
scale	O
visual	O
recognition	B
challenge	O
(	O
ilsvrc	O
)	O
held	O
each	O
year	O
.	O
a	O
dramatic	O
moment	O
in	O
the	O
meteoric	O
rise	O
of	O
deep	O
learning	O
came	O
when	O
a	O
convolutional	O
network	O
won	O
this	O
challenge	O
for	O
the	O
ﬁrst	O
time	O
and	O
by	O
a	O
wide	O
margin	O
,	O
bringing	O
down	O
the	O
state-of-the-art	O
top-5	O
error	O
rate	O
from	O
26.1	O
%	O
to	O
15.3	O
%	O
(	O
krizhevsky	O
et	O
al	O
.	O
2012	O
)	O
,	O
meaning	O
that	O
the	O
convolutional	O
network	O
produces	O
a	O
ranked	O
list	O
of	O
possible	O
categories	O
for	O
each	O
image	O
and	O
the	O
correct	O
category	O
appeared	O
in	O
the	O
ﬁrst	O
ﬁve	O
entries	O
of	O
this	O
list	O
for	O
all	O
but	O
15.3	O
%	O
of	O
the	O
test	O
examples	O
.	O
since	O
then	O
,	O
these	O
competitions	O
are	O
consistently	O
won	O
by	O
deep	O
convolutional	O
nets	O
,	O
and	O
as	O
of	O
this	O
writing	O
,	O
advances	O
in	O
deep	O
learning	O
have	O
brought	O
the	O
latest	O
top-5	O
error	O
rate	O
in	O
this	O
contest	O
down	O
to	O
3.6	O
%	O
,	O
as	O
shown	O
in	O
ﬁgure	O
.	O
1.12	O
,	O
deep	O
learning	O
has	O
also	O
had	O
a	O
dramatic	O
impact	O
on	O
speech	O
recognition	B
.	O
after	O
improving	O
throughout	O
the	O
1990s	O
,	O
the	O
error	O
rates	O
for	O
speech	O
recognition	B
stagnated	O
starting	O
in	O
about	O
2000.	O
the	O
introduction	O
of	O
deep	O
learning	O
(	O
dahl	O
et	O
al	O
.	O
2010	O
deng	O
et	O
al.	O
)	O
to	O
speech	O
recognition	B
resulted	O
,	O
in	O
a	O
sudden	O
drop	O
of	O
error	O
rates	O
,	O
with	O
some	O
error	O
rates	O
cut	O
in	O
half	O
.	O
we	O
will	O
explore	O
this	O
history	O
in	O
more	O
detail	O
in	O
section	O
2011	O
hinton	O
2010b	O
seide	O
2012a	O
et	O
al	O
.	O
,	O
et	O
al	O
.	O
,	O
12.3	O
.	O
;	O
;	O
,	O
;	O
deep	O
networks	O
have	O
also	O
had	O
spectacular	O
successes	O
for	O
pedestrian	O
detection	O
and	O
image	O
segmentation	O
(	O
et	O
al.	O
,	O
2013	O
)	O
and	O
yielded	O
superhuman	O
performance	O
in	O
traﬃc	O
sign	O
classiﬁcation	O
(	O
ciresan	O
sermanet	O
et	O
al	O
.	O
2013	O
farabet	O
2013	O
couprie	O
et	O
al.	O
,	O
,	O
;	O
;	O
23	O
chapter	O
1.	O
introduction	O
104	O
103	O
102	O
101	O
n	O
o	O
r	O
u	O
e	O
n	O
r	O
e	O
p	O
s	O
n	O
o	O
i	O
t	O
c	O
e	O
n	O
n	O
o	O
c	O
1	O
1950	O
2	O
9	O
7	O
10	O
6	O
4	O
5	O
8	O
3	O
human	O
cat	O
mouse	O
fruit	O
ﬂy	O
1985	O
2000	O
2015	O
year	O
figure	O
1.10	O
:	O
initially	O
,	O
the	O
number	O
of	O
connections	O
between	O
neurons	O
in	O
artiﬁcial	O
neural	O
networks	O
was	O
limited	O
by	O
hardware	O
capabilities	O
.	O
today	O
,	O
the	O
number	O
of	O
connections	O
between	O
neurons	O
is	O
mostly	O
a	O
design	O
consideration	O
.	O
some	O
artiﬁcial	O
neural	O
networks	O
have	O
nearly	O
as	O
many	O
connections	O
per	O
neuron	O
as	O
a	O
cat	O
,	O
and	O
it	O
is	O
quite	O
common	O
for	O
other	O
neural	O
networks	O
to	O
have	O
as	O
many	O
connections	O
per	O
neuron	O
as	O
smaller	O
mammals	O
like	O
mice	O
.	O
even	O
the	O
human	O
brain	O
does	O
not	O
have	O
an	O
exorbitant	O
amount	O
of	O
connections	O
per	O
neuron	O
.	O
biological	O
neural	O
network	O
sizes	O
from	O
wikipedia	O
2015	O
)	O
.	O
(	O
1.	O
adaptive	O
linear	O
element	O
(	O
widrow	O
and	O
hoﬀ	O
1960	O
)	O
,	O
2.	O
neocognitron	O
(	O
fukushima	O
1980	O
)	O
,	O
3.	O
gpu-accelerated	O
convolutional	O
network	O
(	O
chellapilla	O
et	O
al	O
.	O
2006	O
)	O
,	O
4.	O
deep	O
boltzmann	O
machine	O
(	O
salakhutdinov	O
and	O
hinton	O
2009a	O
)	O
,	O
5.	O
unsupervised	O
convolutional	O
network	O
(	O
jarrett	O
et	O
al	O
.	O
2009	O
)	O
,	O
6.	O
gpu-accelerated	O
multilayer	O
perceptron	O
(	O
ciresan	O
et	O
al	O
.	O
2010	O
)	O
,	O
7.	O
distributed	O
autoencoder	O
(	O
le	O
et	O
al	O
.	O
2012	O
)	O
,	O
8.	O
multi-gpu	O
convolutional	O
network	O
(	O
krizhevsky	O
et	O
al	O
.	O
2012	O
)	O
,	O
9.	O
cots	O
hpc	O
unsupervised	O
convolutional	O
network	O
(	O
coates	O
et	O
al	O
.	O
2013	O
)	O
,	O
10.	O
googlenet	O
(	O
szegedy	O
et	O
al	O
.	O
2014a	O
)	O
,	O
24	O
chapter	O
1.	O
introduction	O
et	O
al.	O
,	O
2012	O
)	O
.	O
at	O
the	O
same	O
time	O
that	O
the	O
scale	O
and	O
accuracy	O
of	O
deep	O
networks	O
has	O
increased	O
,	O
so	O
has	O
the	O
complexity	O
of	O
the	O
tasks	O
that	O
they	O
can	O
solve	O
.	O
goodfellow	O
et	O
al	O
.	O
2014d	O
)	O
showed	O
that	O
neural	O
networks	O
could	O
learn	O
to	O
output	O
an	O
entire	O
sequence	O
of	O
characters	O
transcribed	O
from	O
an	O
image	O
,	O
rather	O
than	O
just	O
identifying	O
a	O
single	O
object	O
.	O
previously	O
,	O
it	O
was	O
widely	O
believed	O
that	O
this	O
kind	O
of	O
learning	O
required	O
labeling	O
of	O
the	O
individual	O
elements	O
of	O
the	O
sequence	O
(	O
)	O
.	O
recurrent	O
neural	O
networks	O
,	O
such	O
as	O
the	O
lstm	O
sequence	O
model	B
mentioned	O
above	O
,	O
are	O
now	O
used	O
to	O
model	B
relationships	O
between	O
sequences	O
rather	O
than	O
just	O
ﬁxed	O
inputs	O
.	O
this	O
sequence-to-sequence	O
learning	O
seems	O
to	O
be	O
on	O
the	O
cusp	O
of	O
revolutionizing	O
another	O
application	O
:	O
machine	O
translation	O
(	O
sutskever	O
et	O
al.	O
,	O
2015	O
)	O
.	O
gülçehre	O
and	O
bengio	O
2013	O
2014	O
bahdanau	O
and	O
other	O
sequences	O
et	O
al.	O
,	O
(	O
;	O
,	O
this	O
trend	O
of	O
increasing	O
complexity	O
has	O
been	O
pushed	O
to	O
its	O
logical	O
conclusion	O
with	O
the	O
introduction	O
of	O
neural	O
turing	O
machines	O
(	O
graves	O
)	O
that	O
learn	O
to	O
read	O
from	O
memory	O
cells	O
and	O
write	O
arbitrary	O
content	O
to	O
memory	O
cells	O
.	O
such	O
neural	O
networks	O
can	O
learn	O
simple	O
programs	O
from	O
examples	O
of	O
desired	O
behavior	O
.	O
for	O
example	O
,	O
they	O
can	O
learn	O
to	O
sort	O
lists	O
of	O
numbers	O
given	O
examples	O
of	O
scrambled	O
and	O
sorted	O
sequences	O
.	O
this	O
self-programming	O
technology	O
is	O
in	O
its	O
infancy	O
,	O
but	O
in	O
the	O
future	O
could	O
in	O
principle	O
be	O
applied	O
to	O
nearly	O
any	O
task	O
.	O
2014a	O
et	O
al.	O
,	O
another	O
crowning	O
achievement	O
of	O
deep	O
learning	O
is	O
its	O
extension	O
to	O
the	O
domain	O
of	O
reinforcement	O
learning	O
.	O
in	O
the	O
context	O
of	O
reinforcement	O
learning	O
,	O
an	O
autonomous	O
agent	O
must	O
learn	O
to	O
perform	O
a	O
task	O
by	O
trial	O
and	O
error	O
,	O
without	O
any	O
guidance	O
from	O
the	O
human	O
operator	O
.	O
deepmind	O
demonstrated	O
that	O
a	O
reinforcement	O
learning	O
system	O
based	O
on	O
deep	O
learning	O
is	O
capable	O
of	O
learning	O
to	O
play	O
atari	O
video	O
games	O
,	O
reaching	O
human-level	O
performance	O
on	O
many	O
tasks	O
(	O
)	O
.	O
deep	O
learning	O
has	O
also	O
signiﬁcantly	O
improved	O
the	O
performance	O
of	O
reinforcement	O
learning	O
for	O
robotics	O
(	O
finn	O
et	O
al	O
.	O
2015	O
mnih	O
et	O
al	O
.	O
2015	O
)	O
.	O
,	O
,	O
many	O
of	O
these	O
applications	O
of	O
deep	O
learning	O
are	O
highly	O
proﬁtable	O
.	O
deep	O
learning	O
is	O
now	O
used	O
by	O
many	O
top	O
technology	O
companies	O
including	O
google	O
,	O
microsoft	O
,	O
facebook	O
,	O
ibm	O
,	O
baidu	O
,	O
apple	O
,	O
adobe	O
,	O
netﬂix	O
,	O
nvidia	O
and	O
nec	O
.	O
advances	O
in	O
deep	O
learning	O
have	O
also	O
depended	O
heavily	O
on	O
advances	O
in	O
software	O
bergstra	O
et	O
al	O
.	O
2010	O
bastien	O
)	O
,	O
)	O
,	O
and	O
)	O
have	O
all	O
supported	O
important	O
research	O
projects	O
or	O
infrastructure	O
.	O
software	O
libraries	O
such	O
as	O
theano	O
(	O
et	O
al	O
.	O
2012	O
,	O
distbelief	O
(	O
tensorflow	O
(	O
commercial	O
products	O
.	O
collobert	O
et	O
al	O
.	O
2011b	O
chen	O
et	O
al	O
.	O
2015	O
)	O
,	O
pylearn2	O
(	O
dean	O
et	O
al	O
.	O
2012	O
,	O
)	O
,	O
torch	O
(	O
)	O
,	O
mxnet	O
(	O
abadi	O
et	O
al	O
.	O
2015	O
et	O
al	O
.	O
,	O
,	O
goodfellow	O
)	O
,	O
caﬀe	O
(	O
jia	O
2013	O
2013c	O
;	O
,	O
,	O
,	O
,	O
deep	O
learning	O
has	O
also	O
made	O
contributions	O
back	O
to	O
other	O
sciences	O
.	O
modern	O
convolutional	O
networks	O
for	O
object	O
recognition	B
provide	O
a	O
model	B
of	O
visual	O
processing	O
25	O
chapter	O
1.	O
introduction	O
dicarlo	O
2013	O
)	O
.	O
deep	O
learning	O
also	O
provides	O
useful	O
that	O
neuroscientists	O
can	O
study	O
(	O
tools	O
for	O
processing	O
massive	O
amounts	O
of	O
data	O
and	O
making	O
useful	O
predictions	O
in	O
scientiﬁc	O
ﬁelds	O
.	O
it	O
has	O
been	O
successfully	O
used	O
to	O
predict	O
how	O
molecules	O
will	O
interact	O
in	O
order	O
to	O
help	O
pharmaceutical	O
companies	O
design	O
new	O
drugs	O
(	O
)	O
,	O
to	O
search	O
for	O
subatomic	O
particles	O
(	O
)	O
,	O
and	O
to	O
automatically	O
parse	O
microscope	O
images	O
used	O
to	O
construct	O
a	O
3-d	O
map	O
of	O
the	O
human	O
brain	O
(	O
knowles-	O
barley	O
)	O
.	O
we	O
expect	O
deep	O
learning	O
to	O
appear	O
in	O
more	O
and	O
more	O
scientiﬁc	O
ﬁelds	O
in	O
the	O
future	O
.	O
baldi	O
et	O
al	O
.	O
2014	O
dahl	O
et	O
al	O
.	O
2014	O
,	O
et	O
al.	O
,	O
2014	O
,	O
,	O
in	O
summary	O
,	O
deep	O
learning	O
is	O
an	O
approach	O
to	O
machine	O
learning	O
that	O
has	O
drawn	O
heavily	O
on	O
our	O
knowledge	O
of	O
the	O
human	O
brain	O
,	O
statistics	O
and	O
applied	O
math	O
as	O
it	O
developed	O
over	O
the	O
past	O
several	O
decades	O
.	O
in	O
recent	O
years	O
,	O
it	O
has	O
seen	O
tremendous	O
growth	O
in	O
its	O
popularity	O
and	O
usefulness	O
,	O
due	O
in	O
large	O
part	O
to	O
more	O
powerful	O
com-	O
puters	O
,	O
larger	O
datasets	O
and	O
techniques	O
to	O
train	O
deeper	O
networks	O
.	O
the	O
years	O
ahead	O
are	O
full	O
of	O
challenges	O
and	O
opportunities	O
to	O
improve	O
deep	O
learning	O
even	O
further	O
and	O
bring	O
it	O
to	O
new	O
frontiers	O
.	O
26	O
chapter	O
1.	O
introduction	O
1011	O
1010	O
109	O
108	O
107	O
106	O
105	O
104	O
103	O
102	O
101	O
100	O
−	O
1	O
10	O
−	O
2	O
10	O
)	O
e	O
l	O
a	O
c	O
s	O
c	O
i	O
m	O
h	O
t	O
i	O
r	O
a	O
g	O
o	O
l	O
(	O
s	O
n	O
o	O
r	O
u	O
e	O
n	O
f	O
o	O
r	O
e	O
b	O
m	O
u	O
n	O
16	O
19	O
20	O
18	O
17	O
14	O
11	O
8	O
3	O
1	O
2	O
1950	O
13	O
15	O
12	O
10	O
6	O
4	O
1985	O
2000	O
year	O
5	O
9	O
7	O
2015	O
human	O
octopus	O
frog	O
bee	O
ant	O
leech	O
roundworm	O
sponge	O
2056	O
figure	O
1.11	O
:	O
since	O
the	O
introduction	O
of	O
hidden	O
units	O
,	O
artiﬁcial	O
neural	O
networks	O
have	O
doubled	O
in	O
size	O
roughly	O
every	O
2.4	O
years	O
.	O
biological	O
neural	O
network	O
sizes	O
from	O
wikipedia	O
2015	O
)	O
.	O
(	O
1.	O
perceptron	O
(	O
rosenblatt	O
1958	O
1962	O
)	O
,	O
,	O
2.	O
adaptive	O
linear	O
element	O
(	O
widrow	O
and	O
hoﬀ	O
1960	O
)	O
,	O
3.	O
neocognitron	O
(	O
fukushima	O
1980	O
)	O
,	O
4.	O
early	O
back-propagation	O
network	O
(	O
rumelhart	O
et	O
al	O
.	O
1986b	O
)	O
,	O
5.	O
recurrent	O
neural	O
network	O
for	O
speech	O
recognition	B
(	O
robinson	O
and	O
fallside	O
1991	O
)	O
,	O
6.	O
multilayer	O
perceptron	O
for	O
speech	O
recognition	B
(	O
7.	O
mean	O
ﬁeld	O
sigmoid	O
belief	O
network	O
(	O
saul	O
et	O
al	O
.	O
1996	O
)	O
bengio	O
et	O
al	O
.	O
1991	O
)	O
,	O
,	O
8.	O
lenet-5	O
(	O
lecun	O
et	O
al	O
.	O
1998b	O
)	O
,	O
9.	O
echo	O
state	O
network	O
(	O
jaeger	O
and	O
haas	O
2004	O
)	O
,	O
10.	O
deep	O
belief	O
network	O
(	O
hinton	O
et	O
al	O
.	O
2006	O
)	O
11.	O
gpu-accelerated	O
convolutional	O
network	O
(	O
,	O
chellapilla	O
et	O
al	O
.	O
2006	O
)	O
,	O
12.	O
deep	O
boltzmann	O
machine	O
(	O
salakhutdinov	O
and	O
hinton	O
2009a	O
)	O
,	O
13.	O
gpu-accelerated	O
deep	O
belief	O
network	O
(	O
14.	O
unsupervised	O
convolutional	O
network	O
(	O
raina	O
et	O
al	O
.	O
2009	O
)	O
)	O
jarrett	O
et	O
al	O
.	O
2009	O
,	O
,	O
15.	O
gpu-accelerated	O
multilayer	O
perceptron	O
(	O
ciresan	O
et	O
al	O
.	O
2010	O
)	O
,	O
16.	O
omp-1	O
network	O
(	O
coates	O
and	O
ng	O
2011	O
)	O
,	O
17.	O
distributed	O
autoencoder	O
(	O
le	O
et	O
al	O
.	O
2012	O
)	O
,	O
18.	O
multi-gpu	O
convolutional	O
network	O
(	O
krizhevsky	O
et	O
al	O
.	O
2012	O
)	O
,	O
19.	O
cots	O
hpc	O
unsupervised	O
convolutional	O
network	O
(	O
coates	O
et	O
al	O
.	O
2013	O
)	O
,	O
20.	O
googlenet	O
(	O
szegedy	O
et	O
al	O
.	O
2014a	O
)	O
,	O
27	O
chapter	O
1.	O
introduction	O
e	O
t	O
a	O
r	O
r	O
o	O
r	O
r	O
e	O
n	O
o	O
i	O
t	O
a	O
c	O
ﬁ	O
i	O
s	O
s	O
a	O
l	O
c	O
c	O
r	O
v	O
s	O
l	O
i	O
0	O
30	O
.	O
0	O
25	O
.	O
0	O
20	O
.	O
0	O
15	O
.	O
0	O
10	O
.	O
0	O
05	O
.	O
0	O
00	O
.	O
2010	O
2011	O
2012	O
2013	O
2014	O
2015	O
year	O
figure	O
1.12	O
:	O
since	O
deep	O
networks	O
reached	O
the	O
scale	O
necessary	O
to	O
compete	O
in	O
the	O
imagenet	O
large	O
scale	O
visual	O
recognition	B
challenge	O
,	O
they	O
have	O
consistently	O
won	O
the	O
competition	O
every	O
year	O
,	O
and	O
yielded	O
lower	O
and	O
lower	O
error	O
rates	O
each	O
time	O
.	O
data	O
from	O
russakovsky	O
et	O
al	O
.	O
(	O
2014b	O
)	O
and	O
he	O
et	O
al	O
.	O
(	O
2015	O
)	O
.	O
28	O
part	O
i	O
applied	O
math	O
and	O
machine	O
learning	O
basics	O
29	O
this	O
part	O
of	O
the	O
book	O
introduces	O
the	O
basic	O
mathematical	O
concepts	O
needed	O
to	O
understand	O
deep	O
learning	O
.	O
we	O
begin	O
with	O
general	O
ideas	O
from	O
applied	O
math	O
that	O
allow	O
us	O
to	O
deﬁne	O
functions	O
of	O
many	O
variables	O
,	O
ﬁnd	O
the	O
highest	O
and	O
lowest	O
points	O
on	O
these	O
functions	O
and	O
quantify	O
degrees	O
of	O
belief	O
.	O
next	O
,	O
we	O
describe	O
the	O
fundamental	O
goals	O
of	O
machine	O
learning	O
.	O
we	O
describe	O
how	O
to	O
accomplish	O
these	O
goals	O
by	O
specifying	O
a	O
model	B
that	O
represents	O
certain	O
beliefs	O
,	O
designing	O
a	O
cost	O
function	O
that	O
measures	O
how	O
well	O
those	O
beliefs	O
correspond	O
with	O
reality	O
and	O
using	O
a	O
training	O
algorithm	O
to	O
minimize	O
that	O
cost	O
function	O
.	O
this	O
elementary	O
framework	O
is	O
the	O
basis	O
for	O
a	O
broad	O
variety	O
of	O
machine	O
learning	O
algorithms	O
,	O
including	O
approaches	O
to	O
machine	O
learning	O
that	O
are	O
not	O
deep	O
.	O
in	O
the	O
subsequent	O
parts	O
of	O
the	O
book	O
,	O
we	O
develop	O
deep	O
learning	O
algorithms	O
within	O
this	O
framework	O
.	O
30	O
chapter	O
2	O
linear	O
algebra	O
linear	O
algebra	O
is	O
a	O
branch	O
of	O
mathematics	O
that	O
is	O
widely	O
used	O
throughout	O
science	O
and	O
engineering	O
.	O
however	O
,	O
because	O
linear	O
algebra	O
is	O
a	O
form	O
of	O
continuous	O
rather	O
than	O
discrete	O
mathematics	O
,	O
many	O
computer	O
scientists	O
have	O
little	O
experience	O
with	O
it	O
.	O
a	O
good	O
understanding	O
of	O
linear	O
algebra	O
is	O
essential	O
for	O
understanding	O
and	O
working	O
with	O
many	O
machine	O
learning	O
algorithms	O
,	O
especially	O
deep	O
learning	O
algorithms	O
.	O
we	O
therefore	O
precede	O
our	O
introduction	O
to	O
deep	O
learning	O
with	O
a	O
focused	O
presentation	O
of	O
the	O
key	O
linear	O
algebra	O
prerequisites	O
.	O
if	O
you	O
are	O
already	O
familiar	O
with	O
linear	O
algebra	O
,	O
feel	O
free	O
to	O
skip	O
this	O
chapter	O
.	O
if	O
you	O
have	O
previous	O
experience	O
with	O
these	O
concepts	O
but	O
need	O
a	O
detailed	O
reference	O
sheet	O
to	O
review	O
key	O
formulas	O
,	O
we	O
recommend	O
the	O
matrix	O
cookbook	O
(	O
petersen	O
and	O
pedersen	O
2006	O
)	O
.	O
if	O
you	O
have	O
no	O
exposure	O
at	O
all	O
to	O
linear	O
algebra	O
,	O
this	O
chapter	O
will	O
teach	O
you	O
enough	O
to	O
read	O
this	O
book	O
,	O
but	O
we	O
highly	O
recommend	O
that	O
you	O
also	O
consult	O
another	O
resource	O
focused	O
exclusively	O
on	O
teaching	O
linear	O
algebra	O
,	O
such	O
as	O
shilov	O
1977	O
)	O
.	O
this	O
chapter	O
will	O
completely	O
omit	O
many	O
important	O
linear	O
algebra	O
topics	O
that	O
are	O
not	O
essential	O
for	O
understanding	O
deep	O
learning	O
.	O
(	O
,	O
2.1	O
scalars	O
,	O
vectors	O
,	O
matrices	O
and	O
tensors	O
the	O
study	O
of	O
linear	O
algebra	O
involves	O
several	O
types	O
of	O
mathematical	O
objects	O
:	O
•	O
scalars	O
:	O
a	O
scalar	O
is	O
just	O
a	O
single	O
number	O
,	O
in	O
contrast	O
to	O
most	O
of	O
the	O
other	O
objects	O
studied	O
in	O
linear	O
algebra	O
,	O
which	O
are	O
usually	O
arrays	O
of	O
multiple	O
numbers	O
.	O
we	O
write	O
scalars	O
in	O
italics	O
.	O
we	O
usually	O
give	O
scalars	O
lower-case	O
variable	O
names	O
.	O
when	O
we	O
introduce	O
them	O
,	O
we	O
specify	O
what	O
kind	O
of	O
number	O
they	O
are	O
.	O
for	O
31	O
chapter	O
2.	O
linear	O
algebra	O
•	O
•	O
∈	O
∈	O
example	O
,	O
we	O
might	O
say	O
“	O
let	O
s	O
real-valued	O
scalar	O
,	O
or	O
“	O
let	O
n	O
natural	O
number	O
scalar	O
.	O
r	O
be	O
the	O
slope	O
of	O
the	O
line	O
,	O
”	O
while	O
deﬁning	O
a	O
n	O
be	O
the	O
number	O
of	O
units	O
,	O
”	O
while	O
deﬁning	O
a	O
vectors	O
:	O
a	O
vector	O
is	O
an	O
array	O
of	O
numbers	O
.	O
the	O
numbers	O
are	O
arranged	O
in	O
order	O
.	O
we	O
can	O
identify	O
each	O
individual	O
number	O
by	O
its	O
index	O
in	O
that	O
ordering	O
.	O
typically	O
we	O
give	O
vectors	O
lower	O
case	O
names	O
written	O
in	O
bold	O
typeface	O
,	O
such	O
as	O
x.	O
the	O
elements	O
of	O
the	O
vector	O
are	O
identiﬁed	O
by	O
writing	O
its	O
name	O
in	O
italic	O
typeface	O
,	O
with	O
a	O
subscript	O
.	O
the	O
ﬁrst	O
element	O
of	O
x	O
is	O
x1	O
,	O
the	O
second	O
element	O
is	O
x2	O
and	O
so	O
on	O
.	O
we	O
also	O
need	O
to	O
say	O
what	O
kind	O
of	O
numbers	O
are	O
stored	O
in	O
the	O
vector	O
.	O
if	O
each	O
element	O
is	O
in	O
r	O
,	O
and	O
the	O
vector	O
has	O
n	O
elements	O
,	O
then	O
the	O
vector	O
lies	O
in	O
the	O
set	O
formed	O
by	O
taking	O
the	O
cartesian	O
product	O
of	O
r	O
n	O
times	O
,	O
n.	O
when	O
we	O
need	O
to	O
explicitly	O
identify	O
the	O
elements	O
of	O
a	O
vector	O
,	O
denoted	O
as	O
r	O
we	O
write	O
them	O
as	O
a	O
column	O
enclosed	O
in	O
square	O
brackets	O
:	O
	O
x1	O
	O
x	O
=	O
.	O
x2	O
...	O
xn	O
(	O
2.1	O
)	O
we	O
can	O
think	O
of	O
vectors	O
as	O
identifying	O
points	O
in	O
space	O
,	O
with	O
each	O
element	O
giving	O
the	O
coordinate	O
along	O
a	O
diﬀerent	O
axis	O
.	O
sometimes	O
we	O
need	O
to	O
index	O
a	O
set	O
of	O
elements	O
of	O
a	O
vector	O
.	O
in	O
this	O
case	O
,	O
we	O
deﬁne	O
a	O
set	O
containing	O
the	O
indices	O
and	O
write	O
the	O
set	O
as	O
a	O
subscript	O
.	O
for	O
example	O
,	O
to	O
access	O
x1	O
,	O
x3	O
and	O
x6	O
,	O
we	O
deﬁne	O
the	O
set	O
s	O
=	O
and	O
write	O
xs	O
.	O
we	O
use	O
the	O
1	O
is	O
the	O
vector	O
containing	O
all	O
elements	O
of	O
x	O
except	O
for	O
x1	O
,	O
and	O
x−	O
s	O
is	O
the	O
vector	O
containing	O
all	O
of	O
the	O
elements	O
of	O
x1	O
,	O
x3	O
and	O
x6	O
.	O
sign	O
to	O
index	O
the	O
complement	O
of	O
a	O
set	O
.	O
for	O
example	O
x−	O
except	O
for	O
1	O
,	O
3	O
,	O
6	O
−	O
{	O
}	O
x	O
matrices	O
:	O
a	O
matrix	O
is	O
a	O
2-d	O
array	O
of	O
numbers	O
,	O
so	O
each	O
element	O
is	O
identiﬁed	O
by	O
two	O
indices	O
instead	O
of	O
just	O
one	O
.	O
we	O
usually	O
give	O
matrices	O
upper-case	O
variable	O
names	O
with	O
bold	O
typeface	O
,	O
such	O
as	O
a.	O
if	O
a	O
real-valued	O
matrix	O
a	O
has	O
a	O
height	O
of	O
m	O
and	O
a	O
width	O
of	O
n	O
,	O
then	O
we	O
say	O
that	O
a	O
.	O
we	O
usually	O
identify	O
the	O
elements	O
of	O
a	O
matrix	O
using	O
its	O
name	O
in	O
italic	O
but	O
not	O
bold	O
font	O
,	O
and	O
the	O
indices	O
are	O
listed	O
with	O
separating	O
commas	O
.	O
for	O
example	O
,	O
a1	O
1	O
,	O
is	O
the	O
upper	O
left	O
entry	O
of	O
a	O
and	O
am	O
,	O
n	O
is	O
the	O
bottom	O
right	O
entry	O
.	O
we	O
can	O
identify	O
all	O
of	O
the	O
numbers	O
with	O
vertical	O
coordinate	O
i	O
by	O
writing	O
a	O
“	O
”	O
for	O
the	O
horizontal	O
coordinate	O
.	O
for	O
example	O
,	O
ai	O
,	O
:	O
denotes	O
the	O
horizontal	O
cross	O
section	O
of	O
a	O
with	O
vertical	O
coordinate	O
i.	O
this	O
is	O
known	O
as	O
the	O
i-th	O
row	O
of	O
a.	O
likewise	O
,	O
a	O
:	O
,i	O
is	O
×	O
m	O
n	O
∈	O
r	O
:	O
32	O
chapter	O
2.	O
linear	O
algebra	O
	O
	O
	O
	O
a	O
=	O
a1	O
1	O
,	O
a1	O
2	O
,	O
a2	O
1	O
,	O
a2	O
2	O
,	O
a3	O
1	O
,	O
a3	O
2	O
,	O
⇒	O
	O
a	O
=	O
a1	O
1	O
,	O
a2	O
1	O
,	O
a1	O
2	O
,	O
a2	O
2	O
,	O
a3	O
1	O
,	O
a3	O
2	O
,	O
figure	O
2.1	O
:	O
the	O
transpose	O
of	O
the	O
matrix	O
can	O
be	O
thought	O
of	O
as	O
a	O
mirror	O
image	O
across	O
the	O
main	O
diagonal	O
.	O
	O
	O
i	O
column	O
a	O
the	O
-th	O
a	O
matrix	O
,	O
we	O
write	O
them	O
as	O
an	O
array	O
enclosed	O
in	O
square	O
brackets	O
:	O
.	O
when	O
we	O
need	O
to	O
explicitly	O
identify	O
the	O
elements	O
of	O
of	O
a1	O
1	O
,	O
a	O
1	O
2	O
,	O
a2	O
1	O
,	O
a	O
2	O
2	O
,	O
.	O
(	O
2.2	O
)	O
•	O
sometimes	O
we	O
may	O
need	O
to	O
index	O
matrix-valued	O
expressions	O
that	O
are	O
not	O
just	O
a	O
single	O
letter	O
.	O
in	O
this	O
case	O
,	O
we	O
use	O
subscripts	O
after	O
the	O
expression	O
,	O
but	O
do	O
not	O
convert	O
anything	O
to	O
lower	O
case	O
.	O
for	O
example	O
,	O
f	O
(	O
a	O
)	O
i	O
,	O
j	O
gives	O
element	O
(	O
i	O
,	O
j	O
)	O
of	O
the	O
matrix	O
computed	O
by	O
applying	O
the	O
function	O
.	O
f	O
a	O
to	O
tensors	O
:	O
in	O
some	O
cases	O
we	O
will	O
need	O
an	O
array	O
with	O
more	O
than	O
two	O
axes	O
.	O
in	O
the	O
general	O
case	O
,	O
an	O
array	O
of	O
numbers	O
arranged	O
on	O
a	O
regular	O
grid	O
with	O
a	O
variable	O
number	O
of	O
axes	O
is	O
known	O
as	O
a	O
tensor	O
.	O
we	O
denote	O
a	O
tensor	O
named	O
“	O
a	O
”	O
with	O
this	O
typeface	O
:	O
a.	O
we	O
identify	O
the	O
element	O
of	O
a	O
at	O
coordinates	O
(	O
i	O
,	O
j	O
,	O
k	O
)	O
by	O
writing	O
ai	O
,	O
j	O
,	O
k	O
.	O
one	O
important	O
operation	O
on	O
matrices	O
is	O
the	O
transpose	O
.	O
the	O
transpose	O
of	O
a	O
matrix	O
is	O
the	O
mirror	O
image	O
of	O
the	O
matrix	O
across	O
a	O
diagonal	O
line	O
,	O
called	O
the	O
main	O
diagonal	O
,	O
running	O
down	O
and	O
to	O
the	O
right	O
,	O
starting	O
from	O
its	O
upper	O
left	O
corner	O
.	O
see	O
ﬁgure	O
for	O
a	O
graphical	O
depiction	O
of	O
this	O
operation	O
.	O
we	O
denote	O
the	O
transpose	O
of	O
a	O
matrix	O
,	O
and	O
it	O
is	O
deﬁned	O
such	O
that	O
asa	O
a	O
2.1	O
	O
	O
(	O
a	O
)	O
i	O
,	O
j	O
=	O
aj	O
,	O
i	O
.	O
(	O
2.3	O
)	O
vectors	O
can	O
be	O
thought	O
of	O
as	O
matrices	O
that	O
contain	O
only	O
one	O
column	O
.	O
the	O
transpose	O
of	O
a	O
vector	O
is	O
therefore	O
a	O
matrix	O
with	O
only	O
one	O
row	O
.	O
sometimes	O
we	O
33	O
chapter	O
2.	O
linear	O
algebra	O
deﬁne	O
a	O
vector	O
by	O
writing	O
out	O
its	O
elements	O
in	O
the	O
text	O
inline	O
as	O
a	O
row	O
matrix	O
,	O
then	O
using	O
the	O
transpose	O
operator	O
to	O
turn	O
it	O
into	O
a	O
standard	O
column	O
vector	O
,	O
e.g.	O
,	O
x	O
=	O
[	O
x1	O
,	O
x2	O
,	O
x3	O
]	O
	O
.	O
a	O
scalar	O
can	O
be	O
thought	O
of	O
as	O
a	O
matrix	O
with	O
only	O
a	O
single	O
entry	O
.	O
from	O
this	O
,	O
we	O
can	O
see	O
that	O
a	O
scalar	O
is	O
its	O
own	O
transpose	O
:	O
a	O
	O
a=	O
.	O
we	O
can	O
add	O
matrices	O
to	O
each	O
other	O
,	O
as	O
long	O
as	O
they	O
have	O
the	O
same	O
shape	O
,	O
just	O
by	O
adding	O
their	O
corresponding	O
elements	O
:	O
c	O
a	O
b	O
=	O
+	O
where	O
ci	O
,	O
j	O
=	O
ai	O
,	O
j	O
+	O
b	O
i	O
,	O
j	O
.	O
we	O
can	O
also	O
add	O
a	O
scalar	O
to	O
a	O
matrix	O
or	O
multiply	O
a	O
matrix	O
by	O
a	O
scalar	O
,	O
just	O
b	O
+	O
c	O
where	O
by	O
performing	O
that	O
operation	O
on	O
each	O
element	O
of	O
a	O
matrix	O
:	O
d	O
=	O
a	O
di	O
,	O
j	O
=	O
a	O
b	O
i	O
,	O
j	O
+	O
c.	O
·	O
·	O
in	O
the	O
context	O
of	O
deep	O
learning	O
,	O
we	O
also	O
use	O
some	O
less	O
conventional	O
notation	O
.	O
we	O
allow	O
the	O
addition	O
of	O
matrix	O
and	O
a	O
vector	O
,	O
yielding	O
another	O
matrix	O
:	O
c	O
=	O
a	O
+	O
b	O
,	O
where	O
ci	O
,	O
j	O
=	O
ai	O
,	O
j	O
+	O
bj	O
.	O
in	O
other	O
words	O
,	O
the	O
vector	O
b	O
is	O
added	O
to	O
each	O
row	O
of	O
the	O
matrix	O
.	O
this	O
shorthand	O
eliminates	O
the	O
need	O
to	O
deﬁne	O
a	O
matrix	O
with	O
b	O
copied	O
into	O
each	O
row	O
before	O
doing	O
the	O
addition	O
.	O
this	O
implicit	O
copying	O
of	O
b	O
to	O
many	O
locations	O
is	O
called	O
.	O
broadcasting	O
2.2	O
multiplying	O
matrices	O
and	O
vectors	O
one	O
of	O
the	O
most	O
important	O
operations	O
involving	O
matrices	O
is	O
multiplication	O
of	O
two	O
matrices	O
.	O
the	O
matrix	O
product	O
of	O
matrices	O
a	O
and	O
b	O
is	O
a	O
third	O
matrix	O
c	O
.	O
in	O
×	O
order	O
for	O
this	O
product	O
to	O
be	O
deﬁned	O
,	O
a	O
must	O
have	O
the	O
same	O
number	O
of	O
columns	O
as	O
×	O
b	O
has	O
rows	O
.	O
if	O
a	O
is	O
of	O
shape	O
m	O
n	O
p	O
,	O
then	O
c	O
is	O
of	O
shape	O
m	O
p	O
.	O
we	O
can	O
write	O
the	O
matrix	O
product	O
just	O
by	O
placing	O
two	O
or	O
more	O
matrices	O
together	O
,	O
e.g	O
.	O
and	O
b	O
is	O
of	O
shape	O
n	O
×	O
	O
c	O
ab=	O
.	O
(	O
2.4	O
)	O
(	O
2.5	O
)	O
the	O
product	O
operation	O
is	O
deﬁned	O
by	O
ci	O
,	O
j	O
=	O
ai	O
,	O
kbk	O
,	O
j	O
.	O
k	O
note	O
that	O
the	O
standard	O
product	O
of	O
two	O
matrices	O
is	O
just	O
a	O
matrix	O
containing	O
the	O
product	O
of	O
the	O
individual	O
elements	O
.	O
such	O
an	O
operation	O
exists	O
and	O
is	O
called	O
the	O
element-wise	O
product	O
hadamard	O
product	O
,	O
and	O
is	O
denoted	O
as	O
a	O
b	O
not	O
	O
or	O
.	O
the	O
dot	O
product	O
between	O
two	O
vectors	O
x	O
and	O
y	O
of	O
the	O
same	O
dimensionality	O
y.	O
we	O
can	O
think	O
of	O
the	O
matrix	O
product	O
c	O
=	O
ab	O
as	O
	O
is	O
the	O
matrix	O
product	O
x	O
computing	O
ci	O
,	O
j	O
as	O
the	O
dot	O
product	O
between	O
row	O
of	O
and	O
column	O
j	O
b	O
of	O
.	O
i	O
a	O
34	O
chapter	O
2.	O
linear	O
algebra	O
matrix	O
product	O
operations	O
have	O
many	O
useful	O
properties	O
that	O
make	O
mathematical	O
analysis	O
of	O
matrices	O
more	O
convenient	O
.	O
for	O
example	O
,	O
matrix	O
multiplication	O
is	O
distributive	O
:	O
a	O
b	O
c	O
(	O
+	O
)	O
=	O
ab	O
ac	O
+	O
it	O
is	O
also	O
associative	O
:	O
a	O
bc	O
(	O
)	O
=	O
(	O
ab	O
c	O
)	O
.	O
.	O
(	O
2.6	O
)	O
(	O
2.7	O
)	O
ab	O
=	O
ba	O
does	O
not	O
matrix	O
multiplication	O
is	O
always	O
hold	O
)	O
,	O
unlike	O
scalar	O
multiplication	O
.	O
however	O
,	O
the	O
dot	O
product	O
between	O
two	O
vectors	O
is	O
commutative	O
:	O
commutative	O
(	O
the	O
condition	O
not	O
the	O
transpose	O
of	O
a	O
matrix	O
product	O
has	O
a	O
simple	O
form	O
:	O
(	O
2.8	O
)	O
(	O
2.9	O
)	O
	O
x	O
	O
y	O
y=	O
x	O
.	O
(	O
)	O
ab	O
=	O
b	O
	O
	O
a	O
.	O
	O
	O
	O
	O
this	O
allows	O
us	O
to	O
demonstrate	O
equation	O
of	O
such	O
a	O
product	O
is	O
a	O
scalar	O
and	O
therefore	O
equal	O
to	O
its	O
own	O
transpose	O
:	O
,	O
by	O
exploiting	O
the	O
fact	O
that	O
the	O
value	O
2.8	O
	O
x	O
y	O
=	O
	O
y	O
x	O
	O
=	O
y	O
x	O
.	O
(	O
2.10	O
)	O
since	O
the	O
focus	O
of	O
this	O
textbook	O
is	O
not	O
linear	O
algebra	O
,	O
we	O
do	O
not	O
attempt	O
to	O
develop	O
a	O
comprehensive	O
list	O
of	O
useful	O
properties	O
of	O
the	O
matrix	O
product	O
here	O
,	O
but	O
the	O
reader	O
should	O
be	O
aware	O
that	O
many	O
more	O
exist	O
.	O
we	O
now	O
know	O
enough	O
linear	O
algebra	O
notation	O
to	O
write	O
down	O
a	O
system	O
of	O
linear	O
equations	O
:	O
∈	O
×	O
m	O
n	O
ax	O
b=	O
∈	O
(	O
2.11	O
)	O
∈	O
r	O
is	O
a	O
known	O
matrix	O
,	O
b	O
n	O
is	O
a	O
where	O
a	O
vector	O
of	O
unknown	O
variables	O
we	O
would	O
like	O
to	O
solve	O
for	O
.	O
each	O
element	O
xi	O
of	O
x	O
is	O
one	O
of	O
these	O
unknown	O
variables	O
.	O
each	O
row	O
of	O
a	O
and	O
each	O
element	O
of	O
b	O
provide	O
another	O
constraint	O
.	O
we	O
can	O
rewrite	O
equation	O
m	O
is	O
a	O
known	O
vector	O
,	O
and	O
x	O
2.11	O
as	O
:	O
r	O
r	O
a1	O
:	O
,	O
x	O
=	O
b1	O
a2	O
:	O
,	O
x	O
=	O
b2	O
.	O
.	O
.	O
am	O
,	O
:x	O
=	O
bm	O
or	O
,	O
even	O
more	O
explicitly	O
,	O
as	O
:	O
a1	O
1	O
,	O
x1	O
+	O
a1	O
2	O
,	O
x2	O
+	O
···	O
+	O
a1	O
,	O
nxn	O
=	O
b1	O
35	O
(	O
2.12	O
)	O
(	O
2.13	O
)	O
(	O
2.14	O
)	O
(	O
2.15	O
)	O
(	O
2.16	O
)	O
chapter	O
2.	O
linear	O
algebra	O
	O
	O
1	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
1	O
figure	O
2.2	O
:	O
example	O
identity	O
matrix	O
:	O
this	O
is	O
i	O
3.	O
a2	O
1	O
,	O
x1	O
+	O
a2	O
2	O
,	O
x2	O
+	O
···	O
+	O
a2	O
,	O
nxn	O
=	O
b2	O
a	O
m,1x1	O
+	O
am,2x2	O
+	O
.	O
.	O
.	O
···	O
+	O
a	O
m	O
,	O
nxn	O
=	O
bm	O
.	O
(	O
2.17	O
)	O
(	O
2.18	O
)	O
(	O
2.19	O
)	O
matrix-vector	O
product	O
notation	O
provides	O
a	O
more	O
compact	O
representation	O
for	O
equations	O
of	O
this	O
form	O
.	O
2.3	O
identity	O
and	O
inverse	O
matrices	O
linear	O
algebra	O
oﬀers	O
a	O
powerful	O
tool	O
called	O
matrix	O
inversion	O
that	O
allows	O
us	O
to	O
analytically	O
solve	O
equation	O
for	O
many	O
values	O
of	O
2.11	O
.	O
a	O
to	O
describe	O
matrix	O
inversion	O
,	O
we	O
ﬁrst	O
need	O
to	O
deﬁne	O
the	O
concept	O
of	O
an	O
identity	O
matrix	O
.	O
an	O
identity	O
matrix	O
is	O
a	O
matrix	O
that	O
does	O
not	O
change	O
any	O
vector	O
when	O
we	O
multiply	O
that	O
vector	O
by	O
that	O
matrix	O
.	O
we	O
denote	O
the	O
identity	O
matrix	O
that	O
preserves	O
n-dimensional	O
vectors	O
as	O
in	O
.	O
formally	O
,	O
i	O
n	O
×	O
n	O
n	O
,	O
and	O
∈	O
r	O
∀	O
∈	O
x	O
r	O
n	O
,	O
inx	O
x=	O
.	O
(	O
2.20	O
)	O
(	O
2.21	O
)	O
(	O
2.22	O
)	O
(	O
2.23	O
)	O
(	O
2.24	O
)	O
the	O
structure	O
of	O
the	O
identity	O
matrix	O
is	O
simple	O
:	O
all	O
of	O
the	O
entries	O
along	O
the	O
main	O
diagonal	O
are	O
1	O
,	O
while	O
all	O
of	O
the	O
other	O
entries	O
are	O
zero	O
.	O
see	O
ﬁgure	O
for	O
an	O
example	O
.	O
2.2	O
−	O
1	O
,	O
and	O
it	O
is	O
deﬁned	O
as	O
the	O
matrix	O
the	O
matrix	O
inverse	O
of	O
a	O
is	O
denoted	O
as	O
a	O
such	O
that	O
−	O
1a	O
i=	O
n.	O
a	O
we	O
can	O
now	O
solve	O
equation	O
2.11	O
by	O
the	O
following	O
steps	O
:	O
a	O
ax	O
b=	O
−	O
1b	O
−	O
1ax	O
a=	O
−	O
1b	O
inx	O
a=	O
36	O
chapter	O
2.	O
linear	O
algebra	O
−	O
1b	O
.	O
x	O
a=	O
(	O
2.25	O
)	O
−	O
1.	O
we	O
discuss	O
of	O
course	O
,	O
this	O
process	O
depends	O
on	O
it	O
being	O
possible	O
to	O
ﬁnd	O
a	O
−	O
1	O
in	O
the	O
following	O
section	O
.	O
the	O
conditions	B
for	O
the	O
existence	O
of	O
a	O
−	O
1	O
exists	O
,	O
several	O
diﬀerent	O
algorithms	O
exist	O
for	O
ﬁnding	O
it	O
in	O
closed	O
form	O
.	O
when	O
a	O
in	O
theory	O
,	O
the	O
same	O
inverse	O
matrix	O
can	O
then	O
be	O
used	O
to	O
solve	O
the	O
equation	O
many	O
−	O
1	O
is	O
primarily	O
useful	O
as	O
a	O
theoretical	O
times	O
for	O
diﬀerent	O
values	O
of	O
b.	O
however	O
,	O
a	O
tool	O
,	O
and	O
should	O
not	O
actually	O
be	O
used	O
in	O
practice	O
for	O
most	O
software	O
applications	O
.	O
−	O
1	O
can	O
be	O
represented	O
with	O
only	O
limited	O
precision	O
on	O
a	O
digital	O
computer	O
,	O
because	O
a	O
algorithms	O
that	O
make	O
use	O
of	O
the	O
value	O
of	O
b	O
can	O
usually	O
obtain	O
more	O
accurate	O
estimates	O
of	O
.x	O
2.4	O
linear	O
dependence	O
and	O
span	O
−	O
1	O
to	O
exist	O
,	O
equation	O
in	O
order	O
for	O
a	O
must	O
have	O
exactly	O
one	O
solution	O
for	O
every	O
value	O
of	O
b.	O
however	O
,	O
it	O
is	O
also	O
possible	O
for	O
the	O
system	O
of	O
equations	O
to	O
have	O
no	O
solutions	O
or	O
inﬁnitely	O
many	O
solutions	O
for	O
some	O
values	O
of	O
b.	O
it	O
is	O
not	O
possible	O
to	O
have	O
more	O
than	O
one	O
but	O
less	O
than	O
inﬁnitely	O
many	O
solutions	O
for	O
a	O
particular	O
b	O
;	O
if	O
both	O
are	O
solutions	O
then	O
2.11	O
and	O
x	O
y	O
−	O
)	O
y	O
α	O
(	O
2.26	O
)	O
is	O
also	O
a	O
solution	O
for	O
any	O
real	O
.α	O
z	O
=	O
α	O
+	O
(	O
1	O
x	O
to	O
analyze	O
how	O
many	O
solutions	O
the	O
equation	O
has	O
,	O
we	O
can	O
think	O
of	O
the	O
columns	O
of	O
a	O
as	O
specifying	O
diﬀerent	O
directions	O
we	O
can	O
travel	O
from	O
the	O
origin	O
(	O
the	O
point	O
speciﬁed	O
by	O
the	O
vector	O
of	O
all	O
zeros	O
)	O
,	O
and	O
determine	O
how	O
many	O
ways	O
there	O
are	O
of	O
reaching	O
b.	O
in	O
this	O
view	O
,	O
each	O
element	O
of	O
x	O
speciﬁes	O
how	O
far	O
we	O
should	O
travel	O
in	O
each	O
of	O
these	O
directions	O
,	O
with	O
xi	O
specifying	O
how	O
far	O
to	O
move	O
in	O
the	O
direction	O
of	O
column	O
:	O
i	O
	O
ax	O
=	O
xia	O
:	O
,i.	O
i	O
(	O
2.27	O
)	O
in	O
general	O
,	O
this	O
kind	O
of	O
operation	O
is	O
called	O
a	O
linear	O
combination	O
.	O
formally	O
,	O
a	O
linear	O
combination	O
of	O
some	O
set	O
of	O
vectors	O
is	O
given	O
by	O
multiplying	O
each	O
vector	O
v	O
(	O
)	O
i	O
by	O
a	O
corresponding	O
scalar	O
coeﬃcient	O
and	O
adding	O
the	O
results	O
:	O
{	O
v	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
v	O
(	O
)	O
n	O
}	O
	O
civ	O
(	O
)	O
i	O
.	O
i	O
(	O
2.28	O
)	O
the	O
span	O
of	O
a	O
set	O
of	O
vectors	O
is	O
the	O
set	O
of	O
all	O
points	O
obtainable	O
by	O
linear	O
combination	O
of	O
the	O
original	O
vectors	O
.	O
37	O
chapter	O
2.	O
linear	O
algebra	O
determining	O
whether	O
ax	O
=	O
b	O
has	O
a	O
solution	O
thus	O
amounts	O
to	O
testing	O
whether	O
b	O
is	O
in	O
the	O
span	O
of	O
the	O
columns	O
of	O
a.	O
this	O
particular	O
span	O
is	O
known	O
as	O
the	O
column	O
space	O
range	O
or	O
the	O
.a	O
of	O
∈	O
r	O
m.	O
if	O
any	O
point	O
in	O
r	O
m	O
,	O
m	O
in	O
order	O
for	O
the	O
system	O
ax	O
=	O
b	O
to	O
have	O
a	O
solution	O
for	O
all	O
values	O
of	O
b	O
we	O
therefore	O
require	O
that	O
the	O
column	O
space	O
of	O
a	O
be	O
all	O
of	O
r	O
is	O
excluded	O
from	O
the	O
column	O
space	O
,	O
that	O
point	O
is	O
a	O
potential	O
value	O
of	O
b	O
that	O
has	O
no	O
solution	O
.	O
the	O
requirement	O
that	O
the	O
column	O
space	O
of	O
a	O
be	O
all	O
of	O
r	O
m	O
implies	O
immediately	O
that	O
a	O
must	O
have	O
at	O
least	O
m	O
columns	O
,	O
i.e.	O
,	O
n	O
m	O
.	O
otherwise	O
,	O
the	O
×	O
dimensionality	O
of	O
the	O
column	O
space	O
would	O
be	O
less	O
than	O
m.	O
for	O
example	O
,	O
consider	O
a	O
3	O
2	O
matrix	O
.	O
the	O
target	O
b	O
is	O
3-d	O
,	O
but	O
x	O
is	O
only	O
2-d	O
,	O
so	O
modifying	O
the	O
value	O
of	O
x	O
3.	O
the	O
equation	O
has	O
a	O
solution	O
at	O
best	O
allows	O
us	O
to	O
trace	O
out	O
a	O
2-d	O
plane	O
within	O
r	O
if	O
and	O
only	O
if	O
lies	O
on	O
that	O
plane	O
.	O
≥	O
b	O
≥	O
having	O
n	O
m	O
is	O
only	O
a	O
necessary	O
condition	O
for	O
every	O
point	O
to	O
have	O
a	O
solution	O
.	O
it	O
is	O
not	O
a	O
suﬃcient	O
condition	O
,	O
because	O
it	O
is	O
possible	O
for	O
some	O
of	O
the	O
columns	O
to	O
2	O
matrix	O
where	O
both	O
of	O
the	O
columns	O
are	O
identical	O
.	O
be	O
redundant	O
.	O
consider	O
a	O
2	O
this	O
has	O
the	O
same	O
column	O
space	O
as	O
a	O
2	O
1	O
matrix	O
containing	O
only	O
one	O
copy	O
of	O
the	O
replicated	O
column	O
.	O
in	O
other	O
words	O
,	O
the	O
column	O
space	O
is	O
still	O
just	O
a	O
line	O
,	O
and	O
fails	O
to	O
encompass	O
all	O
of	O
r	O
2	O
,	O
even	O
though	O
there	O
are	O
two	O
columns	O
.	O
×	O
×	O
formally	O
,	O
this	O
kind	O
of	O
redundancy	O
is	O
known	O
as	O
linear	O
dependence	O
.	O
a	O
set	O
of	O
vectors	O
is	O
linearly	O
independent	O
if	O
no	O
vector	O
in	O
the	O
set	O
is	O
a	O
linear	O
combination	O
of	O
the	O
other	O
vectors	O
.	O
if	O
we	O
add	O
a	O
vector	O
to	O
a	O
set	O
that	O
is	O
a	O
linear	O
combination	O
of	O
the	O
other	O
vectors	O
in	O
the	O
set	O
,	O
the	O
new	O
vector	O
does	O
not	O
add	O
any	O
points	O
to	O
the	O
set	O
’	O
s	O
m	O
,	O
span	O
.	O
this	O
means	O
that	O
for	O
the	O
column	O
space	O
of	O
the	O
matrix	O
to	O
encompass	O
all	O
of	O
r	O
the	O
matrix	O
must	O
contain	O
at	O
least	O
one	O
set	O
of	O
m	O
linearly	O
independent	O
columns	O
.	O
this	O
condition	O
is	O
both	O
necessary	O
and	O
suﬃcient	O
for	O
equation	O
to	O
have	O
a	O
solution	O
for	O
every	O
value	O
of	O
b.	O
note	O
that	O
the	O
requirement	O
is	O
for	O
a	O
set	O
to	O
have	O
exactly	O
m	O
linear	O
independent	O
columns	O
,	O
not	O
at	O
least	O
m.	O
no	O
set	O
of	O
m-dimensional	O
vectors	O
can	O
have	O
more	O
than	O
m	O
mutually	O
linearly	O
independent	O
columns	O
,	O
but	O
a	O
matrix	O
with	O
more	O
than	O
m	O
columns	O
may	O
have	O
more	O
than	O
one	O
such	O
set	O
.	O
2.11	O
in	O
order	O
for	O
the	O
matrix	O
to	O
have	O
an	O
inverse	O
,	O
we	O
additionally	O
need	O
to	O
ensure	O
that	O
b.	O
to	O
do	O
so	O
,	O
we	O
need	O
to	O
equation	O
ensure	O
that	O
the	O
matrix	O
has	O
at	O
most	O
m	O
columns	O
.	O
otherwise	O
there	O
is	O
more	O
than	O
one	O
way	O
of	O
parametrizing	O
each	O
solution	O
.	O
one	O
solution	O
for	O
each	O
value	O
of	O
at	O
most	O
2.11	O
has	O
together	O
,	O
this	O
means	O
that	O
the	O
matrix	O
must	O
be	O
square	O
,	O
that	O
is	O
,	O
we	O
require	O
that	O
m	O
=	O
n	O
and	O
that	O
all	O
of	O
the	O
columns	O
must	O
be	O
linearly	O
independent	O
.	O
a	O
square	O
matrix	O
with	O
linearly	O
dependent	O
columns	O
is	O
known	O
as	O
.	O
singular	O
if	O
a	O
is	O
not	O
square	O
or	O
is	O
square	O
but	O
singular	O
,	O
it	O
can	O
still	O
be	O
possible	O
to	O
solve	O
the	O
equation	O
.	O
however	O
,	O
we	O
can	O
not	O
use	O
the	O
method	O
of	O
matrix	O
inversion	O
to	O
ﬁnd	O
the	O
38	O
chapter	O
2.	O
linear	O
algebra	O
solution	O
.	O
so	O
far	O
we	O
have	O
discussed	O
matrix	O
inverses	O
as	O
being	O
multiplied	O
on	O
the	O
left	O
.	O
it	O
is	O
also	O
possible	O
to	O
deﬁne	O
an	O
inverse	O
that	O
is	O
multiplied	O
on	O
the	O
right	O
:	O
−	O
1	O
=	O
i.	O
aa	O
(	O
2.29	O
)	O
for	O
square	O
matrices	O
,	O
the	O
left	O
inverse	O
and	O
right	O
inverse	O
are	O
equal	O
.	O
2.5	O
norms	O
	O
	O
sometimes	O
we	O
need	O
to	O
measure	O
the	O
size	O
of	O
a	O
vector	O
.	O
in	O
machine	O
learning	O
,	O
we	O
usually	O
measure	O
the	O
size	O
of	O
vectors	O
using	O
a	O
function	O
called	O
a	O
norm	O
.	O
formally	O
,	O
the	O
lp	O
norm	O
is	O
given	O
by	O
||	O
||	O
x	O
p	O
=	O
|	O
|	O
x	O
i	O
p	O
1	O
p	O
(	O
2.30	O
)	O
∈	O
≥	O
.	O
1	O
,	O
p	O
r	O
for	O
p	O
i	O
norms	O
,	O
including	O
the	O
lp	O
norm	O
,	O
are	O
functions	O
mapping	O
vectors	O
to	O
non-negative	O
values	O
.	O
on	O
an	O
intuitive	O
level	O
,	O
the	O
norm	O
of	O
a	O
vector	O
x	O
measures	O
the	O
distance	O
from	O
the	O
origin	O
to	O
the	O
point	O
x.	O
more	O
rigorously	O
,	O
a	O
norm	O
is	O
any	O
function	O
f	O
that	O
satisﬁes	O
the	O
following	O
properties	O
:	O
x	O
⇒	O
f	O
(	O
)	O
=	O
0	O
≤	O
f	O
(	O
+	O
)	O
•	O
•	O
•	O
∀	O
∈	O
x	O
y	O
x	O
=	O
0	O
f	O
(	O
)	O
+x	O
|	O
f	O
|	O
α	O
r	O
,	O
f	O
α	O
(	O
x	O
)	O
=	O
α	O
f	O
(	O
)	O
x	O
(	O
)	O
y	O
(	O
the	O
triangle	O
inequality	O
)	O
the	O
l2	O
norm	O
,	O
with	O
p	O
=	O
2	O
,	O
is	O
known	O
as	O
the	O
euclidean	O
norm	O
.	O
it	O
is	O
simply	O
the	O
||	O
||	O
euclidean	O
distance	O
from	O
the	O
origin	O
to	O
the	O
point	O
identiﬁed	O
by	O
x.	O
the	O
l	O
2	O
norm	O
is	O
x	O
,	O
with	O
used	O
so	O
frequently	O
in	O
machine	O
learning	O
that	O
it	O
is	O
often	O
denoted	O
simply	O
as	O
omitted	O
.	O
it	O
is	O
also	O
common	O
to	O
measure	O
the	O
size	O
of	O
a	O
vector	O
using	O
the	O
subscript	O
the	O
squared	O
l2	O
norm	O
,	O
which	O
can	O
be	O
calculated	O
simply	O
as	O
x	O
	O
x	O
.	O
2	O
the	O
squared	O
l2	O
norm	O
is	O
more	O
convenient	O
to	O
work	B
with	O
mathematically	O
and	O
computationally	O
than	O
the	O
l	O
2	O
norm	O
itself	O
.	O
for	O
example	O
,	O
the	O
derivatives	O
of	O
the	O
squared	O
l2	O
norm	O
with	O
respect	O
to	O
each	O
element	O
of	O
x	O
each	O
depend	O
only	O
on	O
the	O
corresponding	O
element	O
of	O
x	O
,	O
while	O
all	O
of	O
the	O
derivatives	O
of	O
the	O
l2	O
norm	O
depend	O
on	O
the	O
entire	O
vector	O
.	O
in	O
many	O
contexts	O
,	O
the	O
squared	O
l2	O
norm	O
may	O
be	O
undesirable	O
in	O
several	O
machine	O
learning	O
because	O
it	O
increases	O
very	O
slowly	O
near	O
the	O
origin	O
.	O
39	O
chapter	O
2.	O
linear	O
algebra	O
applications	O
,	O
it	O
is	O
important	O
to	O
discriminate	O
between	O
elements	O
that	O
are	O
exactly	O
zero	O
and	O
elements	O
that	O
are	O
small	O
but	O
nonzero	O
.	O
in	O
these	O
cases	O
,	O
we	O
turn	O
to	O
a	O
function	O
that	O
grows	O
at	O
the	O
same	O
rate	O
in	O
all	O
locations	O
,	O
but	O
retains	O
mathematical	O
simplicity	O
:	O
the	O
l1	O
norm	O
.	O
the	O
l1	O
norm	O
may	O
be	O
simpliﬁed	O
to	O
|	O
.	O
(	O
2.31	O
)	O
||	O
||	O
x	O
1	O
=	O
|	O
xi	O
	O
the	O
l1	O
norm	O
is	O
commonly	O
used	O
in	O
machine	O
learning	O
when	O
the	O
diﬀerence	O
between	O
zero	O
and	O
nonzero	O
elements	O
is	O
very	O
important	O
.	O
every	O
time	O
an	O
element	O
of	O
x	O
moves	O
away	O
from	O
0	O
by	O
,	O
the	O
l1	O
norm	O
increases	O
by	O
.	O
	O
i	O
we	O
sometimes	O
measure	O
the	O
size	O
of	O
the	O
vector	O
by	O
counting	O
its	O
number	O
of	O
nonzero	O
elements	O
.	O
some	O
authors	O
refer	O
to	O
this	O
function	O
as	O
the	O
“	O
l0	O
norm	O
,	O
”	O
but	O
this	O
is	O
incorrect	O
terminology	O
.	O
the	O
number	O
of	O
non-zero	O
entries	O
in	O
a	O
vector	O
is	O
not	O
a	O
norm	O
,	O
because	O
scaling	O
the	O
vector	O
by	O
α	O
does	O
not	O
change	O
the	O
number	O
of	O
nonzero	O
entries	O
.	O
the	O
l1	O
norm	O
is	O
often	O
used	O
as	O
a	O
substitute	O
for	O
the	O
number	O
of	O
nonzero	O
entries	O
.	O
one	O
other	O
norm	O
that	O
commonly	O
arises	O
in	O
machine	O
learning	O
is	O
the	O
l	O
norm	O
,	O
also	O
known	O
as	O
the	O
max	O
norm	O
.	O
this	O
norm	O
simpliﬁes	O
to	O
the	O
absolute	O
value	O
of	O
the	O
element	O
with	O
the	O
largest	O
magnitude	O
in	O
the	O
vector	O
,	O
|	O
.	O
||	O
||	O
x	O
∞	O
=	O
max	O
(	O
2.32	O
)	O
|	O
xi	O
∞	O
i	O
	O
||	O
||	O
a	O
f	O
=	O
sometimes	O
we	O
may	O
also	O
wish	O
to	O
measure	O
the	O
size	O
of	O
a	O
matrix	O
.	O
in	O
the	O
context	O
of	O
deep	O
learning	O
,	O
the	O
most	O
common	O
way	O
to	O
do	O
this	O
is	O
with	O
the	O
otherwise	O
obscure	O
frobenius	O
norm	O
:	O
a	O
2	O
i	O
,	O
j	O
,	O
i	O
,	O
j	O
(	O
2.33	O
)	O
which	O
is	O
analogous	O
to	O
the	O
l	O
2	O
norm	O
of	O
a	O
vector	O
.	O
the	O
dot	O
product	O
of	O
two	O
vectors	O
can	O
be	O
rewritten	O
in	O
terms	O
of	O
norms	O
.	O
speciﬁcally	O
,	O
	O
x	O
y	O
||	O
||	O
x=	O
||	O
||	O
y	O
2	O
cos	O
θ	O
2	O
(	O
2.34	O
)	O
where	O
θ	O
is	O
the	O
angle	O
between	O
x	O
and	O
.	O
y	O
2.6	O
special	O
kinds	O
of	O
matrices	O
and	O
vectors	O
some	O
special	O
kinds	O
of	O
matrices	O
and	O
vectors	O
are	O
particularly	O
useful	O
.	O
diagonal	O
matrices	O
consist	O
mostly	O
of	O
zeros	O
and	O
have	O
non-zero	O
entries	O
only	O
along	O
the	O
main	O
diagonal	O
.	O
formally	O
,	O
a	O
matrix	O
d	O
is	O
diagonal	O
if	O
and	O
only	O
if	O
di	O
,	O
j	O
=	O
0	O
for	O
40	O
chapter	O
2.	O
linear	O
algebra	O
all	O
i	O
=	O
j	O
.	O
we	O
have	O
already	O
seen	O
one	O
example	O
of	O
a	O
diagonal	O
matrix	O
:	O
the	O
identity	O
matrix	O
,	O
where	O
all	O
of	O
the	O
diagonal	O
entries	O
are	O
1.	O
we	O
write	O
diag	O
(	O
v	O
)	O
to	O
denote	O
a	O
square	O
diagonal	O
matrix	O
whose	O
diagonal	O
entries	O
are	O
given	O
by	O
the	O
entries	O
of	O
the	O
vector	O
v.	O
diagonal	O
matrices	O
are	O
of	O
interest	O
in	O
part	O
because	O
multiplying	O
by	O
a	O
diagonal	O
matrix	O
	O
is	O
very	O
computationally	O
eﬃcient	O
.	O
to	O
compute	O
diag	O
(	O
v	O
)	O
x	O
,	O
we	O
only	O
need	O
to	O
scale	O
each	O
element	O
xi	O
by	O
vi	O
.	O
in	O
other	O
words	O
,	O
diag	O
(	O
v	O
)	O
x	O
=	O
v	O
x	O
.	O
inverting	O
a	O
square	O
diagonal	O
matrix	O
is	O
also	O
eﬃcient	O
.	O
the	O
inverse	O
exists	O
only	O
if	O
every	O
diagonal	O
entry	O
is	O
nonzero	O
,	O
−	O
	O
1	O
=	O
diag	O
(	O
[	O
1/v	O
1	O
,	O
.	O
.	O
.	O
,	O
1/vn	O
]	O
and	O
in	O
that	O
case	O
,	O
diag	O
(	O
v	O
)	O
)	O
.	O
in	O
many	O
cases	O
,	O
we	O
may	O
derive	O
some	O
very	O
general	O
machine	O
learning	O
algorithm	O
in	O
terms	O
of	O
arbitrary	O
matrices	O
,	O
but	O
obtain	O
a	O
less	O
expensive	O
(	O
and	O
less	O
descriptive	O
)	O
algorithm	O
by	O
restricting	O
some	O
matrices	O
to	O
be	O
diagonal	O
.	O
not	O
all	O
diagonal	O
matrices	O
need	O
be	O
square	O
.	O
it	O
is	O
possible	O
to	O
construct	O
a	O
rectangular	O
diagonal	O
matrix	O
.	O
non-square	O
diagonal	O
matrices	O
do	O
not	O
have	O
inverses	O
but	O
it	O
is	O
still	O
possible	O
to	O
multiply	O
by	O
them	O
cheaply	O
.	O
for	O
a	O
non-square	O
diagonal	O
matrix	O
d	O
,	O
the	O
product	O
dx	O
will	O
involve	O
scaling	O
each	O
element	O
of	O
x	O
,	O
and	O
either	O
concatenating	O
some	O
zeros	O
to	O
the	O
result	O
if	O
d	O
is	O
taller	O
than	O
it	O
is	O
wide	O
,	O
or	O
discarding	O
some	O
of	O
the	O
last	O
elements	O
of	O
the	O
vector	O
if	O
is	O
wider	O
than	O
it	O
is	O
tall	O
.	O
d	O
a	O
symmetric	O
matrix	O
is	O
any	O
matrix	O
that	O
is	O
equal	O
to	O
its	O
own	O
transpose	O
:	O
	O
.	O
a	O
a=	O
(	O
2.35	O
)	O
symmetric	O
matrices	O
often	O
arise	O
when	O
the	O
entries	O
are	O
generated	O
by	O
some	O
function	O
of	O
two	O
arguments	O
that	O
does	O
not	O
depend	O
on	O
the	O
order	O
of	O
the	O
arguments	O
.	O
for	O
example	O
,	O
if	O
a	O
is	O
a	O
matrix	O
of	O
distance	O
measurements	O
,	O
with	O
ai	O
,	O
j	O
giving	O
the	O
distance	O
from	O
point	O
i	O
to	O
point	O
ai	O
,	O
j	O
=	O
aj	O
,	O
i	O
because	O
distance	O
functions	O
are	O
symmetric	O
.	O
,	O
then	O
j	O
a	O
unit	O
vector	O
is	O
a	O
vector	O
with	O
:	O
unit	O
norm	O
||	O
||	O
x	O
2	O
=	O
1	O
.	O
(	O
2.36	O
)	O
a	O
vector	O
x	O
and	O
a	O
vector	O
y	O
are	O
orthogonal	O
to	O
each	O
other	O
if	O
x	O
y	O
=	O
0.	O
if	O
both	O
vectors	O
have	O
nonzero	O
norm	O
,	O
this	O
means	O
that	O
they	O
are	O
at	O
a	O
90	O
degree	O
angle	O
to	O
each	O
n	O
,	O
at	O
most	O
n	O
vectors	O
may	O
be	O
mutually	O
orthogonal	O
with	O
nonzero	O
norm	O
.	O
other	O
.	O
in	O
r	O
if	O
the	O
vectors	O
are	O
not	O
only	O
orthogonal	O
but	O
also	O
have	O
unit	O
norm	O
,	O
we	O
call	O
them	O
orthonormal	O
.	O
	O
an	O
orthogonal	O
matrix	O
is	O
a	O
square	O
matrix	O
whose	O
rows	O
are	O
mutually	O
orthonor-	O
mal	O
and	O
whose	O
columns	O
are	O
mutually	O
orthonormal	O
:	O
	O
	O
a	O
aa=	O
a	O
=	O
i	O
.	O
41	O
(	O
2.37	O
)	O
	O
chapter	O
2.	O
linear	O
algebra	O
this	O
implies	O
that	O
−	O
	O
1	O
=	O
a	O
,	O
a	O
(	O
2.38	O
)	O
so	O
orthogonal	O
matrices	O
are	O
of	O
interest	O
because	O
their	O
inverse	O
is	O
very	O
cheap	O
to	O
compute	O
.	O
pay	O
careful	O
attention	O
to	O
the	O
deﬁnition	O
of	O
orthogonal	O
matrices	O
.	O
counterintuitively	O
,	O
their	O
rows	O
are	O
not	O
merely	O
orthogonal	O
but	O
fully	O
orthonormal	O
.	O
there	O
is	O
no	O
special	O
term	O
for	O
a	O
matrix	O
whose	O
rows	O
or	O
columns	O
are	O
orthogonal	O
but	O
not	O
orthonormal	O
.	O
2.7	O
eigendecomposition	O
many	O
mathematical	O
objects	O
can	O
be	O
understood	O
better	O
by	O
breaking	O
them	O
into	O
constituent	O
parts	O
,	O
or	O
ﬁnding	O
some	O
properties	O
of	O
them	O
that	O
are	O
universal	O
,	O
not	O
caused	O
by	O
the	O
way	O
we	O
choose	O
to	O
represent	O
them	O
.	O
for	O
example	O
,	O
integers	O
can	O
be	O
decomposed	O
into	O
prime	O
factors	O
.	O
the	O
way	O
we	O
will	O
change	O
depending	O
on	O
whether	O
we	O
write	O
it	O
in	O
base	O
ten	O
3.	O
from	O
this	O
representation	O
is	O
not	O
divisible	O
by	O
,	O
or	O
that	O
any	O
represent	O
the	O
number	O
or	O
in	O
binary	O
,	O
but	O
it	O
will	O
always	O
be	O
true	O
that	O
12	O
=	O
2	O
we	O
can	O
conclude	O
useful	O
properties	O
,	O
such	O
as	O
that	O
12	O
integer	O
multiple	O
of	O
will	O
be	O
divisible	O
by	O
.	O
3	O
×	O
×	O
12	O
12	O
2	O
5	O
much	O
as	O
we	O
can	O
discover	O
something	O
about	O
the	O
true	O
nature	O
of	O
an	O
integer	O
by	O
decomposing	O
it	O
into	O
prime	O
factors	O
,	O
we	O
can	O
also	O
decompose	O
matrices	O
in	O
ways	O
that	O
show	O
us	O
information	O
about	O
their	O
functional	O
properties	O
that	O
is	O
not	O
obvious	O
from	O
the	O
representation	O
of	O
the	O
matrix	O
as	O
an	O
array	O
of	O
elements	O
.	O
one	O
of	O
the	O
most	O
widely	O
used	O
kinds	O
of	O
matrix	O
decomposition	O
is	O
called	O
eigen-	O
decomposition	O
,	O
in	O
which	O
we	O
decompose	O
a	O
matrix	O
into	O
a	O
set	O
of	O
eigenvectors	O
and	O
eigenvalues	O
.	O
an	O
eigenvector	O
of	O
a	O
square	O
matrix	O
a	O
is	O
a	O
non-zero	O
vector	O
v	O
such	O
that	O
multi-	O
plication	O
by	O
a	O
alters	O
only	O
the	O
scale	O
of	O
v	O
:	O
av	O
v=	O
λ	O
.	O
(	O
2.39	O
)	O
the	O
scalar	O
λ	O
is	O
known	O
as	O
the	O
eigenvalue	O
corresponding	O
to	O
this	O
eigenvector	O
.	O
(	O
one	O
can	O
also	O
ﬁnd	O
a	O
left	O
eigenvector	O
such	O
that	O
v	O
,	O
but	O
we	O
are	O
usually	O
concerned	O
with	O
right	O
eigenvectors	O
)	O
.	O
	O
a	O
=	O
λv	O
	O
if	O
v	O
is	O
an	O
eigenvector	O
of	O
a	O
,	O
then	O
so	O
is	O
any	O
rescaled	O
vector	O
sv	O
for	O
s	O
=	O
0.	O
moreover	O
,	O
sv	O
still	O
has	O
the	O
same	O
eigenvalue	O
.	O
for	O
this	O
reason	O
,	O
we	O
usually	O
only	O
look	O
for	O
unit	O
eigenvectors	O
.	O
,	O
s	O
r	O
∈	O
}	O
suppose	O
that	O
a	O
matrix	O
a	O
has	O
n	O
linearly	O
independent	O
eigenvectors	O
,	O
v	O
(	O
)	O
n	O
,	O
with	O
corresponding	O
eigenvalues	O
{	O
v	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
.	O
we	O
may	O
concatenate	O
all	O
of	O
the	O
{	O
}	O
λ1	O
,	O
.	O
.	O
.	O
,	O
λ	O
n	O
42	O
	O
chapter	O
2.	O
linear	O
algebra	O
	O
	O
	O
	O
	O
 	O
 	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
 	O
 	O
 	O
 	O
 	O
 	O
	O
	O
	O
	O
	O
	O
 	O
 	O
 	O
 	O
	O
	O
	O
	O
	O
	O
	O
figure	O
2.3	O
:	O
an	O
example	O
of	O
the	O
eﬀect	O
of	O
eigenvectors	O
and	O
eigenvalues	O
.	O
here	O
,	O
we	O
have	O
a	O
matrix	O
a	O
with	O
two	O
orthonormal	O
eigenvectors	O
,	O
v	O
(	O
1	O
)	O
with	O
eigenvalue	O
λ1	O
and	O
v	O
(	O
2	O
)	O
with	O
eigenvalue	O
λ2	O
.	O
(	O
left	O
)	O
we	O
plot	O
the	O
set	O
of	O
all	O
unit	O
vectors	O
u	O
2	O
as	O
a	O
unit	O
circle	O
.	O
(	O
right	O
)	O
we	O
plot	O
the	O
set	O
of	O
all	O
points	O
au	O
.	O
by	O
observing	O
the	O
way	O
that	O
a	O
distorts	O
the	O
unit	O
circle	O
,	O
we	O
can	O
see	O
that	O
it	O
scales	O
space	O
in	O
direction	O
v	O
(	O
)	O
i	O
by	O
λi	O
.	O
∈	O
r	O
eigenvectors	O
to	O
form	O
a	O
matrix	O
v	O
with	O
one	O
eigenvector	O
per	O
column	O
:	O
v	O
=	O
[	O
v	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
v	O
(	O
)	O
n	O
]	O
.	O
likewise	O
,	O
we	O
can	O
concatenate	O
the	O
eigenvalues	O
to	O
form	O
a	O
vector	O
λ	O
=	O
[	O
λ1	O
,	O
.	O
.	O
.	O
,	O
	O
λn	O
]	O
.	O
the	O
eigendecomposition	O
a	O
is	O
then	O
given	O
by	O
of	O
(	O
2.40	O
)	O
a	O
v	O
=	O
diag	O
(	O
)	O
λ	O
v	O
−	O
1.	O
we	O
have	O
seen	O
that	O
constructing	O
matrices	O
with	O
speciﬁc	O
eigenvalues	O
and	O
eigenvec-	O
tors	O
allows	O
us	O
to	O
stretch	O
space	O
in	O
desired	O
directions	O
.	O
however	O
,	O
we	O
often	O
want	O
to	O
decompose	O
matrices	O
into	O
their	O
eigenvalues	O
and	O
eigenvectors	O
.	O
doing	O
so	O
can	O
help	O
us	O
to	O
analyze	O
certain	O
properties	O
of	O
the	O
matrix	O
,	O
much	O
as	O
decomposing	O
an	O
integer	O
into	O
its	O
prime	O
factors	O
can	O
help	O
us	O
understand	O
the	O
behavior	O
of	O
that	O
integer	O
.	O
not	O
every	O
matrix	O
can	O
be	O
decomposed	O
into	O
eigenvalues	O
and	O
eigenvectors	O
.	O
in	O
some	O
43	O
chapter	O
2.	O
linear	O
algebra	O
cases	O
,	O
the	O
decomposition	O
exists	O
,	O
but	O
may	O
involve	O
complex	O
rather	O
than	O
real	O
numbers	O
.	O
fortunately	O
,	O
in	O
this	O
book	O
,	O
we	O
usually	O
need	O
to	O
decompose	O
only	O
a	O
speciﬁc	O
class	O
of	O
matrices	O
that	O
have	O
a	O
simple	O
decomposition	O
.	O
speciﬁcally	O
,	O
every	O
real	O
symmetric	O
matrix	O
can	O
be	O
decomposed	O
into	O
an	O
expression	O
using	O
only	O
real-valued	O
eigenvectors	O
and	O
eigenvalues	O
:	O
a	O
q	O
q	O
=	O
λ	O
	O
,	O
(	O
2.41	O
)	O
where	O
q	O
is	O
an	O
orthogonal	O
matrix	O
composed	O
of	O
eigenvectors	O
of	O
a	O
,	O
and	O
λ	O
is	O
a	O
diagonal	O
matrix	O
.	O
the	O
eigenvalue	O
λi	O
,	O
i	O
is	O
associated	O
with	O
the	O
eigenvector	O
in	O
column	O
i	O
of	O
q	O
,	O
denoted	O
as	O
q	O
:	O
,i.	O
because	O
q	O
is	O
an	O
orthogonal	O
matrix	O
,	O
we	O
can	O
think	O
of	O
a	O
as	O
scaling	O
space	O
by	O
λi	O
in	O
direction	O
v	O
(	O
)	O
i	O
.	O
see	O
ﬁgure	O
for	O
an	O
example	O
.	O
2.3	O
while	O
any	O
real	O
symmetric	O
matrix	O
a	O
is	O
guaranteed	O
to	O
have	O
an	O
eigendecomposi-	O
tion	B
,	O
the	O
eigendecomposition	O
may	O
not	O
be	O
unique	O
.	O
if	O
any	O
two	O
or	O
more	O
eigenvectors	O
share	O
the	O
same	O
eigenvalue	O
,	O
then	O
any	O
set	O
of	O
orthogonal	O
vectors	O
lying	O
in	O
their	O
span	O
are	O
also	O
eigenvectors	O
with	O
that	O
eigenvalue	O
,	O
and	O
we	O
could	O
equivalently	O
choose	O
a	O
q	O
using	O
those	O
eigenvectors	O
instead	O
.	O
by	O
convention	O
,	O
we	O
usually	O
sort	O
the	O
entries	O
of	O
λ	O
in	O
descending	O
order	O
.	O
under	O
this	O
convention	O
,	O
the	O
eigendecomposition	O
is	O
unique	O
only	O
if	O
all	O
of	O
the	O
eigenvalues	O
are	O
unique	O
.	O
the	O
eigendecomposition	O
of	O
a	O
matrix	O
tells	O
us	O
many	O
useful	O
facts	O
about	O
the	O
matrix	O
.	O
the	O
matrix	O
is	O
singular	O
if	O
and	O
only	O
if	O
any	O
of	O
the	O
eigenvalues	O
are	O
zero	O
.	O
||	O
||	O
the	O
eigendecomposition	O
of	O
a	O
real	O
symmetric	O
matrix	O
can	O
also	O
be	O
used	O
to	O
optimize	O
	O
quadratic	O
expressions	O
of	O
the	O
form	O
f	O
(	O
x	O
)	O
=	O
x	O
x	O
2	O
=	O
1.	O
whenever	O
x	O
is	O
equal	O
to	O
an	O
eigenvector	O
of	O
a	O
,	O
f	O
takes	O
on	O
the	O
value	O
of	O
the	O
corresponding	O
eigenvalue	O
.	O
the	O
maximum	O
value	O
of	O
f	O
within	O
the	O
constraint	O
region	O
is	O
the	O
maximum	O
eigenvalue	O
and	O
its	O
minimum	O
value	O
within	O
the	O
constraint	O
region	O
is	O
the	O
minimum	O
eigenvalue	O
.	O
ax	O
subject	O
to	O
a	O
matrix	O
whose	O
eigenvalues	O
are	O
all	O
positive	O
is	O
called	O
positive	O
deﬁnite	O
.	O
a	O
matrix	O
whose	O
eigenvalues	O
are	O
all	O
positive	O
or	O
zero-valued	O
is	O
called	O
positive	O
semideﬁ-	O
nite	O
.	O
likewise	O
,	O
if	O
all	O
eigenvalues	O
are	O
negative	O
,	O
the	O
matrix	O
is	O
negative	O
deﬁnite	O
,	O
and	O
≥	O
if	O
all	O
eigenvalues	O
are	O
negative	O
or	O
zero-valued	O
,	O
it	O
is	O
negative	O
semideﬁnite	O
.	O
positive	O
semideﬁnite	O
matrices	O
are	O
interesting	O
because	O
they	O
guarantee	O
that	O
0	O
.	O
=	O
0	O
positive	O
deﬁnite	O
matrices	O
additionally	O
guarantee	O
that	O
x	O
∀	O
⇒	O
x	O
x	O
,	O
=	O
0.	O
ax	O
	O
ax	O
	O
x	O
2.8	O
singular	O
value	O
decomposition	O
2.7	O
in	O
section	O
,	O
we	O
saw	O
how	O
to	O
decompose	O
a	O
matrix	O
into	O
eigenvectors	O
and	O
eigenvalues	O
.	O
the	O
singular	O
value	O
decomposition	O
(	O
svd	O
)	O
provides	O
another	O
way	O
to	O
factorize	O
a	O
matrix	O
,	O
into	O
singular	O
vectors	O
and	O
singular	O
values	O
.	O
the	O
svd	O
allows	O
us	O
to	O
discover	O
some	O
of	O
the	O
same	O
kind	O
of	O
information	O
as	O
the	O
eigendecomposition	O
.	O
however	O
,	O
44	O
chapter	O
2.	O
linear	O
algebra	O
the	O
svd	O
is	O
more	O
generally	O
applicable	O
.	O
every	O
real	O
matrix	O
has	O
a	O
singular	O
value	O
decomposition	O
,	O
but	O
the	O
same	O
is	O
not	O
true	O
of	O
the	O
eigenvalue	O
decomposition	O
.	O
for	O
example	O
,	O
if	O
a	O
matrix	O
is	O
not	O
square	O
,	O
the	O
eigendecomposition	O
is	O
not	O
deﬁned	O
,	O
and	O
we	O
must	O
use	O
a	O
singular	O
value	O
decomposition	O
instead	O
.	O
recall	O
that	O
the	O
eigendecomposition	O
involves	O
analyzing	O
a	O
matrix	O
a	O
to	O
discover	O
a	O
matrix	O
v	O
of	O
eigenvectors	O
and	O
a	O
vector	O
of	O
eigenvalues	O
λ	O
such	O
that	O
we	O
can	O
rewrite	O
a	O
as	O
a	O
v	O
=	O
diag	O
(	O
)	O
λ	O
v	O
−	O
1	O
.	O
(	O
2.42	O
)	O
the	O
singular	O
value	O
decomposition	O
is	O
similar	O
,	O
except	O
this	O
time	O
we	O
will	O
write	O
a	O
as	O
a	O
product	O
of	O
three	O
matrices	O
:	O
a	O
u	O
dv	O
=	O
	O
.	O
×	O
suppose	O
that	O
a	O
is	O
an	O
m	O
n	O
to	O
be	O
an	O
matrix	O
,	O
and	O
×	O
m	O
n	O
d	O
matrix	O
.	O
then	O
u	O
is	O
deﬁned	O
to	O
be	O
an	O
m	O
m	O
v	O
to	O
be	O
an	O
matrix	O
.	O
n	O
×	O
n	O
×	O
(	O
2.43	O
)	O
matrix	O
,	O
each	O
of	O
these	O
matrices	O
is	O
deﬁned	O
to	O
have	O
a	O
special	O
structure	O
.	O
the	O
matrices	O
u	O
and	O
v	O
are	O
both	O
deﬁned	O
to	O
be	O
orthogonal	O
matrices	O
.	O
the	O
matrix	O
d	O
is	O
deﬁned	O
to	O
be	O
a	O
diagonal	O
matrix	O
.	O
note	O
that	O
is	O
not	O
necessarily	O
square	O
.	O
d	O
the	O
elements	O
along	O
the	O
diagonal	O
of	O
d	O
are	O
known	O
as	O
the	O
singular	O
values	O
of	O
the	O
matrix	O
a.	O
the	O
columns	O
of	O
u	O
are	O
known	O
as	O
the	O
left-singular	O
vectors	O
.	O
the	O
columns	O
of	O
right-singular	O
vectors	O
.	O
are	O
known	O
as	O
as	O
the	O
v	O
we	O
can	O
actually	O
interpret	O
the	O
singular	O
value	O
decomposition	O
of	O
a	O
in	O
terms	O
of	O
the	O
eigendecomposition	O
of	O
functions	O
of	O
a	O
.	O
the	O
left-singular	O
vectors	O
of	O
a	O
are	O
the	O
	O
	O
a.	O
.	O
the	O
right-singular	O
vectors	O
of	O
a	O
are	O
the	O
eigenvectors	O
of	O
a	O
eigenvectors	O
of	O
aa	O
	O
the	O
non-zero	O
singular	O
values	O
of	O
a	O
are	O
the	O
square	O
roots	O
of	O
the	O
eigenvalues	O
of	O
a	O
a.	O
the	O
same	O
is	O
true	O
for	O
aa	O
	O
.	O
perhaps	O
the	O
most	O
useful	O
feature	O
of	O
the	O
svd	O
is	O
that	O
we	O
can	O
use	O
it	O
to	O
partially	O
generalize	O
matrix	O
inversion	O
to	O
non-square	O
matrices	O
,	O
as	O
we	O
will	O
see	O
in	O
the	O
next	O
section	O
.	O
2.9	O
the	O
moore-penrose	O
pseudoinverse	O
matrix	O
inversion	O
is	O
not	O
deﬁned	O
for	O
matrices	O
that	O
are	O
not	O
square	O
.	O
suppose	O
we	O
want	O
to	O
make	O
a	O
left-inverse	O
,	O
so	O
that	O
we	O
can	O
solve	O
a	O
linear	O
equation	O
of	O
a	O
matrix	O
a	O
b	O
ax	O
y=	O
45	O
(	O
2.44	O
)	O
chapter	O
2.	O
linear	O
algebra	O
by	O
left-multiplying	O
each	O
side	O
to	O
obtain	O
x	O
by=	O
.	O
(	O
2.45	O
)	O
depending	O
on	O
the	O
structure	O
of	O
the	O
problem	O
,	O
it	O
may	O
not	O
be	O
possible	O
to	O
design	O
a	O
unique	O
mapping	O
from	O
to	O
a	O
b	O
.	O
if	O
a	O
is	O
taller	O
than	O
it	O
is	O
wide	O
,	O
then	O
it	O
is	O
possible	O
for	O
this	O
equation	O
to	O
have	O
no	O
solution	O
.	O
if	O
a	O
is	O
wider	O
than	O
it	O
is	O
tall	O
,	O
then	O
there	O
could	O
be	O
multiple	O
possible	O
solutions	O
.	O
the	O
moore-penrose	O
pseudoinverse	O
allows	O
us	O
to	O
make	O
some	O
headway	O
in	O
these	O
cases	O
.	O
the	O
pseudoinverse	O
of	O
a	O
is	O
deﬁned	O
as	O
a	O
matrix	O
a+	O
=	O
lim	O
	O
0	O
α	O
(	O
a	O
	O
a	O
	O
−	O
1a	O
.	O
i+	O
α	O
)	O
(	O
2.46	O
)	O
practical	O
algorithms	O
for	O
computing	O
the	O
pseudoinverse	O
are	O
not	O
based	O
on	O
this	O
deﬁni-	O
tion	B
,	O
but	O
rather	O
the	O
formula	O
a	O
+	O
=	O
v	O
d	O
+u	O
	O
,	O
(	O
2.47	O
)	O
where	O
u	O
,	O
d	O
and	O
v	O
are	O
the	O
singular	O
value	O
decomposition	O
of	O
a	O
,	O
and	O
the	O
pseudoinverse	O
d+	O
of	O
a	O
diagonal	O
matrix	O
d	O
is	O
obtained	O
by	O
taking	O
the	O
reciprocal	O
of	O
its	O
non-zero	O
elements	O
then	O
taking	O
the	O
transpose	O
of	O
the	O
resulting	O
matrix	O
.	O
when	O
a	O
has	O
more	O
columns	O
than	O
rows	O
,	O
then	O
solving	O
a	O
linear	O
equation	O
using	O
the	O
||	O
||	O
pseudoinverse	O
provides	O
one	O
of	O
the	O
many	O
possible	O
solutions	O
.	O
speciﬁcally	O
,	O
it	O
provides	O
the	O
solution	O
x	O
=	O
a+	O
y	O
with	O
minimal	O
euclidean	O
norm	O
x	O
2	O
among	O
all	O
possible	O
solutions	O
.	O
when	O
a	O
has	O
more	O
rows	O
than	O
columns	O
,	O
it	O
is	O
possible	O
for	O
there	O
to	O
be	O
no	O
solution	O
.	O
in	O
this	O
case	O
,	O
using	O
the	O
pseudoinverse	O
gives	O
us	O
the	O
x	O
for	O
which	O
ax	O
is	O
as	O
close	O
as	O
possible	O
to	O
in	O
terms	O
of	O
euclidean	O
norm	O
||	O
−	O
||	O
ax	O
y	O
2.	O
y	O
2.10	O
the	O
trace	O
operator	O
	O
the	O
trace	O
operator	O
gives	O
the	O
sum	O
of	O
all	O
of	O
the	O
diagonal	O
entries	O
of	O
a	O
matrix	O
:	O
tr	O
(	O
)	O
=a	O
ai	O
,	O
i	O
.	O
i	O
(	O
2.48	O
)	O
the	O
trace	O
operator	O
is	O
useful	O
for	O
a	O
variety	O
of	O
reasons	O
.	O
some	O
operations	O
that	O
are	O
diﬃcult	O
to	O
specify	O
without	O
resorting	O
to	O
summation	O
notation	O
can	O
be	O
speciﬁed	O
using	O
46	O
chapter	O
2.	O
linear	O
algebra	O
	O
matrix	O
products	O
and	O
the	O
trace	O
operator	O
.	O
for	O
example	O
,	O
the	O
trace	O
operator	O
provides	O
an	O
alternative	O
way	O
of	O
writing	O
the	O
frobenius	O
norm	O
of	O
a	O
matrix	O
:	O
||	O
||	O
a	O
f	O
=	O
	O
tr	O
(	O
aa	O
)	O
.	O
(	O
2.49	O
)	O
writing	O
an	O
expression	O
in	O
terms	O
of	O
the	O
trace	O
operator	O
opens	O
up	O
opportunities	O
to	O
manipulate	O
the	O
expression	O
using	O
many	O
useful	O
identities	O
.	O
for	O
example	O
,	O
the	O
trace	O
operator	O
is	O
invariant	O
to	O
the	O
transpose	O
operator	O
:	O
	O
a	O
)	O
=	O
tr	O
(	O
(	O
2.50	O
)	O
tr	O
(	O
a	O
)	O
.	O
the	O
trace	O
of	O
a	O
square	O
matrix	O
composed	O
of	O
many	O
factors	O
is	O
also	O
invariant	O
to	O
moving	O
the	O
last	O
factor	O
into	O
the	O
ﬁrst	O
position	B
,	O
if	O
the	O
shapes	O
of	O
the	O
corresponding	O
matrices	O
allow	O
the	O
resulting	O
product	O
to	O
be	O
deﬁned	O
:	O
	O
	O
tr	O
(	O
abc	O
)	O
=	O
tr	O
(	O
cab	O
)	O
=	O
tr	O
(	O
)	O
bca	O
(	O
2.51	O
)	O
or	O
more	O
generally	O
,	O
n	O
−	O
1	O
n	O
tr	O
(	O
f	O
(	O
)	O
i	O
)	O
=	O
tr	O
(	O
f	O
(	O
)	O
n	O
f	O
(	O
)	O
i	O
)	O
.	O
(	O
2.52	O
)	O
i=1	O
i=1	O
this	O
invariance	O
to	O
cyclic	O
permutation	O
holds	O
even	O
if	O
the	O
resulting	O
product	O
has	O
a	O
diﬀerent	O
shape	O
.	O
for	O
example	O
,	O
for	O
a	O
,	O
we	O
have	O
×	O
m	O
n	O
×	O
n	O
m	O
and	O
b	O
∈	O
r	O
∈	O
r	O
even	O
though	O
ab	O
∈	O
×	O
m	O
m	O
r	O
tr	O
(	O
ab	O
∈	O
)	O
=	O
tr	O
(	O
)	O
ba	O
×	O
n	O
n	O
r	O
.	O
(	O
2.53	O
)	O
and	O
ba	O
another	O
useful	O
fact	O
to	O
keep	O
in	O
mind	O
is	O
that	O
a	O
scalar	O
is	O
its	O
own	O
trace	O
:	O
a	O
=	O
tr	O
(	O
a	O
)	O
.	O
2.11	O
the	O
determinant	O
the	O
determinant	O
of	O
a	O
square	O
matrix	O
,	O
denoted	O
det	O
(	O
a	O
)	O
,	O
is	O
a	O
function	O
mapping	O
matrices	O
to	O
real	O
scalars	O
.	O
the	O
determinant	O
is	O
equal	O
to	O
the	O
product	O
of	O
all	O
the	O
eigenvalues	O
of	O
the	O
matrix	O
.	O
the	O
absolute	O
value	O
of	O
the	O
determinant	O
can	O
be	O
thought	O
of	O
as	O
a	O
measure	O
of	O
how	O
much	O
multiplication	O
by	O
the	O
matrix	O
expands	O
or	O
contracts	O
space	O
.	O
if	O
the	O
determinant	O
is	O
0	O
,	O
then	O
space	O
is	O
contracted	O
completely	O
along	O
at	O
least	O
one	O
dimension	O
,	O
causing	O
it	O
to	O
lose	O
all	O
of	O
its	O
volume	O
.	O
if	O
the	O
determinant	O
is	O
1	O
,	O
then	O
the	O
transformation	O
preserves	O
volume	O
.	O
47	O
chapter	O
2.	O
linear	O
algebra	O
2.12	O
example	O
:	O
principal	O
components	O
analysis	O
one	O
simple	O
machine	O
learning	O
algorithm	O
,	O
principal	O
components	O
analysis	O
or	O
pca	O
can	O
be	O
derived	O
using	O
only	O
knowledge	O
of	O
basic	O
linear	O
algebra	O
.	O
suppose	O
we	O
have	O
a	O
collection	O
of	O
m	O
points	O
n.	O
suppose	O
we	O
would	O
like	O
to	O
apply	O
lossy	O
compression	O
to	O
these	O
points	O
.	O
lossy	O
compression	O
means	O
storing	O
the	O
points	O
in	O
a	O
way	O
that	O
requires	O
less	O
memory	O
but	O
may	O
lose	O
some	O
precision	O
.	O
we	O
would	O
like	O
to	O
lose	O
as	O
little	O
precision	O
as	O
possible	O
.	O
in	O
r	O
)	O
m	O
{	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
}	O
∈	O
one	O
way	O
we	O
can	O
encode	O
these	O
points	O
is	O
to	O
represent	O
a	O
lower-dimensional	O
version	O
l.	O
of	O
them	O
.	O
for	O
each	O
point	O
x	O
(	O
)	O
i	O
if	O
l	O
is	O
smaller	O
than	O
n	O
,	O
it	O
will	O
take	O
less	O
memory	O
to	O
store	O
the	O
code	O
points	O
than	O
the	O
original	O
data	O
.	O
we	O
will	O
want	O
to	O
ﬁnd	O
some	O
encoding	O
function	O
that	O
produces	O
the	O
code	O
for	O
an	O
input	O
,	O
f	O
(	O
x	O
)	O
=	O
c	O
,	O
and	O
a	O
decoding	O
function	O
that	O
produces	O
the	O
reconstructed	O
input	O
given	O
its	O
code	O
,	O
n	O
we	O
will	O
ﬁnd	O
a	O
corresponding	O
code	O
vector	O
c	O
(	O
)	O
i	O
.	O
g	O
f	O
(	O
(	O
)	O
)	O
≈	O
x	O
x	O
r	O
r	O
∈	O
pca	O
is	O
deﬁned	O
by	O
our	O
choice	O
of	O
the	O
decoding	O
function	O
.	O
speciﬁcally	O
,	O
to	O
make	O
the	O
decoder	O
very	O
simple	O
,	O
we	O
choose	O
to	O
use	O
matrix	O
multiplication	O
to	O
map	O
the	O
code	O
back	O
into	O
r	O
is	O
the	O
matrix	O
deﬁning	O
the	O
decoding	O
.	O
c	O
dc	O
,	O
where	O
g	O
(	O
)	O
=	O
n.	O
let	O
×	O
n	O
l	O
∈	O
d	O
r	O
computing	O
the	O
optimal	O
code	O
for	O
this	O
decoder	O
could	O
be	O
a	O
diﬃcult	O
problem	O
.	O
to	O
keep	O
the	O
encoding	O
problem	O
easy	O
,	O
pca	O
constrains	O
the	O
columns	O
of	O
d	O
to	O
be	O
orthogonal	O
to	O
each	O
other	O
.	O
(	O
note	O
that	O
d	O
is	O
still	O
not	O
technically	O
“	O
an	O
orthogonal	O
matrix	O
”	O
unless	O
l	O
n=	O
)	O
with	O
the	O
problem	O
as	O
described	O
so	O
far	O
,	O
many	O
solutions	O
are	O
possible	O
,	O
because	O
we	O
can	O
increase	O
the	O
scale	O
of	O
d	O
:	O
,i	O
if	O
we	O
decrease	O
ci	O
proportionally	O
for	O
all	O
points	O
.	O
to	O
give	O
the	O
problem	O
a	O
unique	O
solution	O
,	O
we	O
constrain	O
all	O
of	O
the	O
columns	O
of	O
to	O
have	O
unit	O
norm	O
.	O
d	O
in	O
order	O
to	O
turn	O
this	O
basic	O
idea	O
into	O
an	O
algorithm	O
we	O
can	O
implement	O
,	O
the	O
ﬁrst	O
∗	O
thing	O
we	O
need	O
to	O
do	O
is	O
ﬁgure	O
out	O
how	O
to	O
generate	O
the	O
optimal	O
code	O
point	O
c	O
for	O
each	O
input	O
point	O
x.	O
one	O
way	O
to	O
do	O
this	O
is	O
to	O
minimize	O
the	O
distance	O
between	O
the	O
∗	O
input	O
point	O
x	O
and	O
its	O
reconstruction	O
,	O
g	O
(	O
c	O
)	O
.	O
we	O
can	O
measure	O
this	O
distance	O
using	O
a	O
norm	O
.	O
in	O
the	O
principal	O
components	O
algorithm	O
,	O
we	O
use	O
the	O
l2	O
norm	O
:	O
∗	O
c	O
=	O
arg	O
min	O
c	O
||	O
−	O
||	O
x	O
g	O
(	O
)	O
c	O
2	O
.	O
(	O
2.54	O
)	O
we	O
can	O
switch	O
to	O
the	O
squared	O
l	O
2	O
norm	O
instead	O
of	O
the	O
l2	O
norm	O
itself	O
,	O
because	O
both	O
are	O
minimized	O
by	O
the	O
same	O
value	O
of	O
c.	O
both	O
are	O
minimized	O
by	O
the	O
same	O
value	O
of	O
c	O
because	O
the	O
l2	O
norm	O
is	O
non-negative	O
and	O
the	O
squaring	O
operation	O
is	O
48	O
chapter	O
2.	O
linear	O
algebra	O
monotonically	O
increasing	O
for	O
non-negative	O
arguments	O
.	O
||	O
||	O
−	O
x	O
g	O
(	O
)	O
c	O
2	O
2	O
.	O
=	O
arg	O
min	O
c	O
∗	O
c	O
the	O
function	O
being	O
minimized	O
simpliﬁes	O
to	O
−	O
(	O
x	O
	O
(	O
x	O
(	O
)	O
)	O
g	O
c	O
−	O
(	O
)	O
)	O
g	O
c	O
(	O
by	O
the	O
deﬁnition	O
of	O
the	O
l2	O
norm	O
,	O
equation	O
	O
=	O
x	O
−	O
	O
x	O
x	O
2.30	O
)	O
	O
−	O
g	O
(	O
)	O
c	O
g	O
(	O
)	O
c	O
x	O
	O
c+	O
(	O
g	O
)	O
(	O
2.55	O
)	O
(	O
2.56	O
)	O
g	O
(	O
)	O
c	O
(	O
2.57	O
)	O
(	O
by	O
the	O
distributive	O
property	O
)	O
	O
=	O
x	O
−	O
	O
x	O
2	O
x	O
g	O
(	O
)	O
+c	O
g	O
(	O
)	O
c	O
	O
g	O
(	O
)	O
c	O
	O
(	O
because	O
the	O
scalar	O
g	O
(	O
)	O
c	O
x	O
is	O
equal	O
to	O
the	O
transpose	O
of	O
itself	O
)	O
.	O
(	O
2.58	O
)	O
we	O
can	O
now	O
change	O
the	O
function	O
being	O
minimized	O
again	O
,	O
to	O
omit	O
the	O
ﬁrst	O
term	O
,	O
since	O
this	O
term	O
does	O
not	O
depend	O
on	O
:	O
c	O
−	O
2x	O
=	O
arg	O
min	O
∗	O
c	O
	O
g	O
(	O
)	O
+c	O
g	O
	O
(	O
)	O
c	O
g	O
.	O
(	O
)	O
c	O
c	O
to	O
make	O
further	O
progress	O
,	O
we	O
must	O
substitute	O
in	O
the	O
deﬁnition	O
of	O
∗	O
c	O
=	O
arg	O
min	O
c	O
=	O
arg	O
min	O
c	O
−	O
2x	O
−	O
	O
2x	O
	O
dc	O
	O
	O
d	O
c+	O
dc	O
dc	O
c+	O
	O
il	O
c	O
(	O
by	O
the	O
orthogonality	O
and	O
unit	O
norm	O
constraints	O
on	O
−	O
2x	O
	O
dc	O
c+	O
)	O
d	O
	O
c	O
=	O
arg	O
min	O
c	O
:	O
g	O
(	O
)	O
c	O
(	O
2.59	O
)	O
(	O
2.60	O
)	O
(	O
2.61	O
)	O
(	O
2.62	O
)	O
we	O
can	O
solve	O
this	O
optimization	O
problem	O
using	O
vector	O
calculus	O
(	O
see	O
section	O
4.3	O
if	O
you	O
do	O
not	O
know	O
how	O
to	O
do	O
this	O
)	O
:	O
∇	O
−	O
c	O
(	O
2	O
−	O
	O
x	O
dc	O
	O
	O
c+	O
c	O
)	O
=	O
0	O
2d	O
x	O
c+	O
2	O
=	O
0	O
	O
c	O
d=	O
x	O
.	O
49	O
(	O
2.63	O
)	O
(	O
2.64	O
)	O
(	O
2.65	O
)	O
chapter	O
2.	O
linear	O
algebra	O
this	O
makes	O
the	O
algorithm	O
eﬃcient	O
:	O
we	O
can	O
optimally	O
encode	O
x	O
just	O
using	O
a	O
matrix-vector	O
operation	O
.	O
to	O
encode	O
a	O
vector	O
,	O
we	O
apply	O
the	O
encoder	O
function	O
f	O
(	O
)	O
=	O
x	O
d	O
	O
x	O
.	O
(	O
2.66	O
)	O
using	O
a	O
further	O
matrix	O
multiplication	O
,	O
we	O
can	O
also	O
deﬁne	O
the	O
pca	O
reconstruction	O
operation	O
:	O
	O
r	O
(	O
)	O
=	O
x	O
g	O
f	O
(	O
(	O
)	O
)	O
=	O
x	O
dd	O
x	O
.	O
(	O
2.67	O
)	O
next	O
,	O
we	O
need	O
to	O
choose	O
the	O
encoding	O
matrix	O
d.	O
to	O
do	O
so	O
,	O
we	O
revisit	O
the	O
idea	O
of	O
minimizing	O
the	O
l	O
2	O
distance	O
between	O
inputs	O
and	O
reconstructions	O
.	O
since	O
we	O
will	O
use	O
the	O
same	O
matrix	O
d	O
to	O
decode	O
all	O
of	O
the	O
points	O
,	O
we	O
can	O
no	O
longer	O
consider	O
the	O
points	O
in	O
isolation	O
.	O
instead	O
,	O
we	O
must	O
minimize	O
the	O
frobenius	O
norm	O
of	O
the	O
matrix	O
of	O
errors	O
computed	O
over	O
all	O
dimensions	O
and	O
all	O
points	O
:	O
	O
	O
	O
∗	O
d	O
=	O
arg	O
min	O
d	O
i	O
,	O
j	O
−	O
x	O
(	O
)	O
i	O
j	O
r	O
(	O
x	O
(	O
)	O
i	O
)	O
j	O
2	O
	O
subject	O
to	O
d	O
d	O
i=	O
l	O
(	O
2.68	O
)	O
∗	O
to	O
derive	O
the	O
algorithm	O
for	O
ﬁnding	O
d	O
,	O
we	O
will	O
start	O
by	O
considering	O
the	O
case	O
where	O
l	O
=	O
1.	O
in	O
this	O
case	O
,	O
d	O
is	O
just	O
a	O
single	O
vector	O
,	O
d.	O
substituting	O
equation	O
2.67	O
into	O
equation	O
into	O
,	O
the	O
problem	O
reduces	O
to	O
and	O
simplifying	O
2.68	O
d	O
	O
d	O
−	O
∗	O
d	O
=	O
arg	O
min	O
d	O
i	O
||	O
x	O
(	O
)	O
i	O
	O
||	O
x	O
(	O
)	O
i	O
dd	O
2	O
2	O
subject	O
to	O
||	O
||	O
d	O
2	O
=	O
1	O
.	O
(	O
2.69	O
)	O
the	O
above	O
formulation	O
is	O
the	O
most	O
direct	O
way	O
of	O
performing	O
the	O
substitution	O
,	O
but	O
is	O
not	O
the	O
most	O
stylistically	O
pleasing	O
way	O
to	O
write	O
the	O
equation	O
.	O
it	O
places	O
the	O
x	O
(	O
)	O
i	O
on	O
the	O
right	O
of	O
the	O
vector	O
d.	O
it	O
is	O
more	O
conventional	O
to	O
write	O
scalar	O
value	O
d	O
scalar	O
coeﬃcients	O
on	O
the	O
left	O
of	O
vector	O
they	O
operate	O
on	O
.	O
we	O
therefore	O
usually	O
write	O
such	O
a	O
formula	O
as	O
	O
∗	O
d	O
=	O
arg	O
min	O
d	O
||	O
x	O
(	O
)	O
i	O
−	O
	O
d	O
||	O
x	O
(	O
)	O
i	O
d	O
2	O
2	O
subject	O
to	O
||	O
||	O
d	O
2	O
=	O
1	O
,	O
	O
	O
i	O
or	O
,	O
exploiting	O
the	O
fact	O
that	O
a	O
scalar	O
is	O
its	O
own	O
transpose	O
,	O
as	O
||	O
||	O
d	O
2	O
=	O
1	O
.	O
||	O
x	O
(	O
)	O
i	O
	O
x	O
(	O
)	O
i	O
||	O
dd	O
=	O
arg	O
min	O
2	O
2	O
subject	O
to	O
−	O
d	O
∗	O
d	O
i	O
the	O
reader	O
should	O
aim	O
to	O
become	O
familiar	O
with	O
such	O
cosmetic	O
rearrangements	O
.	O
50	O
(	O
2.70	O
)	O
(	O
2.71	O
)	O
chapter	O
2.	O
linear	O
algebra	O
at	O
this	O
point	O
,	O
it	O
can	O
be	O
helpful	O
to	O
rewrite	O
the	O
problem	O
in	O
terms	O
of	O
a	O
single	O
design	O
matrix	O
of	O
examples	O
,	O
rather	O
than	O
as	O
a	O
sum	O
over	O
separate	O
example	O
vectors	O
.	O
this	O
will	O
allow	O
us	O
to	O
use	O
more	O
compact	O
notation	O
.	O
let	O
x	O
be	O
the	O
matrix	O
	O
deﬁned	O
by	O
stacking	O
all	O
of	O
the	O
vectors	O
describing	O
the	O
points	O
,	O
such	O
that	O
xi	O
,	O
:	O
=	O
x	O
(	O
)	O
i	O
.	O
we	O
can	O
now	O
rewrite	O
the	O
problem	O
as	O
||	O
−	O
x	O
xdd	O
	O
2	O
f	O
subject	O
to	O
d	O
=	O
arg	O
min	O
×	O
m	O
n	O
d	O
=	O
1	O
.	O
(	O
2.72	O
)	O
||	O
∈	O
d	O
r	O
∗	O
disregarding	O
the	O
constraint	O
for	O
the	O
moment	O
,	O
we	O
can	O
simplify	O
the	O
frobenius	O
norm	O
portion	O
as	O
follows	O
:	O
d	O
	O
	O
	O
||	O
2	O
f	O
||	O
−	O
x	O
xdd	O
	O
	O
arg	O
min	O
−	O
d	O
x	O
xdd	O
−	O
x	O
xdd	O
	O
	O
(	O
2.73	O
)	O
(	O
2.74	O
)	O
	O
)	O
(	O
2.75	O
)	O
	O
	O
xdd	O
)	O
x	O
(	O
2.78	O
)	O
)	O
2.52	O
=	O
arg	O
min	O
d	O
tr	O
(	O
by	O
equation	O
2.49	O
)	O
=	O
arg	O
min	O
tr	O
(	O
x	O
	O
d	O
=	O
arg	O
min	O
d	O
	O
tr	O
(	O
x	O
−	O
=	O
arg	O
min	O
tr	O
(	O
x	O
−	O
	O
x	O
x	O
−	O
x	O
)	O
tr	O
(	O
x	O
xdd	O
	O
xdd	O
−	O
)	O
tr	O
(	O
	O
−	O
	O
	O
dd	O
−	O
)	O
tr	O
(	O
x	O
x	O
dd+	O
	O
dd	O
x	O
	O
	O
	O
	O
x	O
xdd	O
	O
x	O
)	O
+	O
tr	O
(	O
dd	O
	O
	O
xdd	O
	O
dd	O
	O
	O
x	O
)	O
+	O
tr	O
(	O
dd	O
x	O
	O
xdd	O
	O
)	O
x	O
(	O
2.76	O
)	O
(	O
2.77	O
)	O
d	O
(	O
because	O
terms	O
not	O
involving	O
−	O
2	O
tr	O
(	O
x	O
=	O
arg	O
min	O
d	O
do	O
not	O
aﬀect	O
the	O
	O
	O
xdd	O
	O
)	O
+	O
tr	O
(	O
dd	O
x	O
	O
xdd	O
)	O
)	O
arg	O
min	O
	O
d	O
(	O
because	O
we	O
can	O
cycle	O
the	O
order	O
of	O
the	O
matrices	O
inside	O
a	O
trace	O
,	O
equation	O
−	O
2	O
tr	O
(	O
x	O
	O
	O
xdd	O
)	O
+	O
tr	O
(	O
x	O
xdd	O
)	O
(	O
2.79	O
)	O
	O
	O
	O
dd	O
=	O
arg	O
min	O
d	O
(	O
using	O
the	O
same	O
property	O
again	O
)	O
at	O
this	O
point	O
,	O
we	O
re-introduce	O
the	O
constraint	O
:	O
	O
xdd	O
	O
−	O
2	O
tr	O
(	O
x	O
−	O
2	O
tr	O
(	O
x	O
arg	O
min	O
d	O
=	O
arg	O
min	O
)	O
+	O
tr	O
(	O
x	O
xdd	O
	O
	O
dd	O
	O
)	O
subject	O
to	O
d	O
	O
d	O
=	O
1	O
(	O
2.80	O
)	O
	O
	O
xdd	O
)	O
+	O
tr	O
(	O
x	O
	O
	O
	O
)	O
subject	O
to	O
d	O
xdd	O
d	O
=	O
1	O
(	O
2.81	O
)	O
d	O
(	O
due	O
to	O
the	O
constraint	O
)	O
=	O
arg	O
min	O
d	O
−	O
	O
	O
xdd	O
	O
)	O
subject	O
to	O
d	O
d	O
=	O
1	O
tr	O
(	O
x	O
(	O
2.82	O
)	O
51	O
chapter	O
2.	O
linear	O
algebra	O
	O
xdd	O
	O
x	O
xd	O
tr	O
(	O
x	O
	O
tr	O
(	O
d	O
	O
	O
)	O
subject	O
to	O
d	O
	O
d	O
)	O
subject	O
to	O
d	O
=	O
1	O
d	O
=	O
1	O
(	O
2.83	O
)	O
(	O
2.84	O
)	O
=	O
arg	O
max	O
d	O
=	O
arg	O
max	O
d	O
this	O
optimization	O
problem	O
may	O
be	O
solved	O
using	O
eigendecomposition	O
.	O
speciﬁcally	O
,	O
x	O
corresponding	O
to	O
the	O
largest	O
	O
the	O
optimal	O
d	O
is	O
given	O
by	O
the	O
eigenvector	O
of	O
x	O
eigenvalue	O
.	O
this	O
derivation	O
is	O
speciﬁc	O
to	O
the	O
case	O
of	O
l	O
=	O
1	O
and	O
recovers	O
only	O
the	O
ﬁrst	O
principal	O
component	O
.	O
more	O
generally	O
,	O
when	O
we	O
wish	O
to	O
recover	O
a	O
basis	O
of	O
principal	O
components	O
,	O
the	O
matrix	O
d	O
is	O
given	O
by	O
the	O
l	O
eigenvectors	O
corresponding	O
to	O
the	O
largest	O
eigenvalues	O
.	O
this	O
may	O
be	O
shown	O
using	O
proof	O
by	O
induction	O
.	O
we	O
recommend	O
writing	O
this	O
proof	O
as	O
an	O
exercise	O
.	O
linear	O
algebra	O
is	O
one	O
of	O
the	O
fundamental	O
mathematical	O
disciplines	O
that	O
is	O
necessary	O
to	O
understand	O
deep	O
learning	O
.	O
another	O
key	O
area	O
of	O
mathematics	O
that	O
is	O
ubiquitous	O
in	O
machine	O
learning	O
is	O
probability	O
theory	O
,	O
presented	O
next	O
.	O
52	O
chapter	O
3	O
probability	O
and	O
information	O
theory	O
in	O
this	O
chapter	O
,	O
we	O
describe	O
probability	O
theory	O
and	O
information	O
theory	O
.	O
probability	O
theory	O
is	O
a	O
mathematical	O
framework	O
for	O
representing	O
uncertain	O
statements	O
.	O
it	O
provides	O
a	O
means	O
of	O
quantifying	O
uncertainty	O
and	O
axioms	O
for	O
deriving	O
new	O
uncertain	O
statements	O
.	O
in	O
artiﬁcial	O
intelligence	O
applications	O
,	O
we	O
use	O
probability	O
theory	O
in	O
two	O
major	O
ways	O
.	O
first	O
,	O
the	O
laws	O
of	O
probability	O
tell	O
us	O
how	O
ai	O
systems	O
should	O
reason	O
,	O
so	O
we	O
design	O
our	O
algorithms	O
to	O
compute	O
or	O
approximate	O
various	O
expressions	O
derived	O
using	O
probability	O
theory	O
.	O
second	O
,	O
we	O
can	O
use	O
probability	O
and	O
statistics	O
to	O
theoretically	O
analyze	O
the	O
behavior	O
of	O
proposed	O
ai	O
systems	O
.	O
probability	O
theory	O
is	O
a	O
fundamental	O
tool	O
of	O
many	O
disciplines	O
of	O
science	O
and	O
engineering	O
.	O
we	O
provide	O
this	O
chapter	O
to	O
ensure	O
that	O
readers	O
whose	O
background	O
is	O
primarily	O
in	O
software	O
engineering	O
with	O
limited	O
exposure	O
to	O
probability	O
theory	O
can	O
understand	O
the	O
material	O
in	O
this	O
book	O
.	O
while	O
probability	O
theory	O
allows	O
us	O
to	O
make	O
uncertain	O
statements	O
and	O
reason	O
in	O
the	O
presence	O
of	O
uncertainty	O
,	O
information	O
theory	O
allows	O
us	O
to	O
quantify	O
the	O
amount	O
of	O
uncertainty	O
in	O
a	O
probability	O
distribution	O
.	O
if	O
you	O
are	O
already	O
familiar	O
with	O
probability	O
theory	O
and	O
information	O
theory	O
,	O
you	O
may	O
wish	O
to	O
skip	O
all	O
of	O
this	O
chapter	O
except	O
for	O
section	O
,	O
which	O
describes	O
the	O
graphs	O
we	O
use	O
to	O
describe	O
structured	O
probabilistic	O
models	O
for	O
machine	O
learning	O
.	O
if	O
you	O
have	O
absolutely	O
no	O
prior	O
experience	O
with	O
these	O
subjects	O
,	O
this	O
chapter	O
should	O
be	O
suﬃcient	O
to	O
successfully	O
carry	O
out	O
deep	O
learning	O
research	O
projects	O
,	O
but	O
we	O
do	O
suggest	O
that	O
you	O
consult	O
an	O
additional	O
resource	O
,	O
such	O
as	O
jaynes	O
2003	O
3.14	O
(	O
)	O
.	O
53	O
chapter	O
3.	O
probability	O
and	O
information	O
theory	O
3.1	O
why	O
probability	O
?	O
many	O
branches	O
of	O
computer	O
science	O
deal	O
mostly	O
with	O
entities	O
that	O
are	O
entirely	O
deterministic	O
and	O
certain	O
.	O
a	O
programmer	O
can	O
usually	O
safely	O
assume	O
that	O
a	O
cpu	O
will	O
execute	O
each	O
machine	O
instruction	O
ﬂawlessly	O
.	O
errors	O
in	O
hardware	O
do	O
occur	O
,	O
but	O
are	O
rare	O
enough	O
that	O
most	O
software	O
applications	O
do	O
not	O
need	O
to	O
be	O
designed	O
to	O
account	O
for	O
them	O
.	O
given	O
that	O
many	O
computer	O
scientists	O
and	O
software	O
engineers	O
work	B
in	O
a	O
relatively	O
clean	O
and	O
certain	O
environment	O
,	O
it	O
can	O
be	O
surprising	O
that	O
machine	O
learning	O
makes	O
heavy	O
use	O
of	O
probability	O
theory	O
.	O
this	O
is	O
because	O
machine	O
learning	O
must	O
always	O
deal	O
with	O
uncertain	O
quantities	O
,	O
and	O
sometimes	O
may	O
also	O
need	O
to	O
deal	O
with	O
stochastic	O
(	O
non-deterministic	O
)	O
quantities	O
.	O
uncertainty	O
and	O
stochasticity	O
can	O
arise	O
from	O
many	O
sources	O
.	O
researchers	O
have	O
made	O
compelling	O
arguments	O
for	O
quantifying	O
uncertainty	O
using	O
probability	O
since	O
at	O
least	O
the	O
1980s	O
.	O
many	O
of	O
the	O
arguments	O
presented	O
here	O
are	O
summarized	O
from	O
or	O
inspired	O
by	O
pearl	O
1988	O
)	O
.	O
(	O
nearly	O
all	O
activities	O
require	O
some	O
ability	O
to	O
reason	O
in	O
the	O
presence	O
of	O
uncertainty	O
.	O
in	O
fact	O
,	O
beyond	O
mathematical	O
statements	O
that	O
are	O
true	O
by	O
deﬁnition	O
,	O
it	O
is	O
diﬃcult	O
to	O
think	O
of	O
any	O
proposition	O
that	O
is	O
absolutely	O
true	O
or	O
any	O
event	O
that	O
is	O
absolutely	O
guaranteed	O
to	O
occur	O
.	O
there	O
are	O
three	O
possible	O
sources	O
of	O
uncertainty	O
:	O
1.	O
inherent	O
stochasticity	O
in	O
the	O
system	O
being	O
modeled	O
.	O
for	O
example	O
,	O
most	O
interpretations	O
of	O
quantum	O
mechanics	O
describe	O
the	O
dynamics	O
of	O
subatomic	O
particles	O
as	O
being	O
probabilistic	O
.	O
we	O
can	O
also	O
create	O
theoretical	O
scenarios	O
that	O
we	O
postulate	O
to	O
have	O
random	O
dynamics	O
,	O
such	O
as	O
a	O
hypothetical	O
card	O
game	O
where	O
we	O
assume	O
that	O
the	O
cards	O
are	O
truly	O
shuﬄed	O
into	O
a	O
random	O
order	O
.	O
2.	O
incomplete	O
observability	O
.	O
even	O
deterministic	O
systems	O
can	O
appear	O
stochastic	O
when	O
we	O
can	O
not	O
observe	O
all	O
of	O
the	O
variables	O
that	O
drive	O
the	O
behavior	O
of	O
the	O
system	O
.	O
for	O
example	O
,	O
in	O
the	O
monty	O
hall	O
problem	O
,	O
a	O
game	O
show	O
contestant	O
is	O
asked	O
to	O
choose	O
between	O
three	O
doors	O
and	O
wins	O
a	O
prize	O
held	O
behind	O
the	O
chosen	O
door	O
.	O
two	O
doors	O
lead	O
to	O
a	O
goat	O
while	O
a	O
third	O
leads	O
to	O
a	O
car	O
.	O
the	O
outcome	O
given	O
the	O
contestant	O
’	O
s	O
choice	O
is	O
deterministic	O
,	O
but	O
from	O
the	O
contestant	O
’	O
s	O
point	O
of	O
view	O
,	O
the	O
outcome	O
is	O
uncertain	O
.	O
3.	O
incomplete	O
modeling	O
.	O
when	O
we	O
use	O
a	O
model	B
that	O
must	O
discard	O
some	O
of	O
the	O
information	O
we	O
have	O
observed	O
,	O
the	O
discarded	O
information	O
results	O
in	O
uncertainty	O
in	O
the	O
model	B
’	O
s	O
predictions	O
.	O
for	O
example	O
,	O
suppose	O
we	O
build	O
a	O
robot	O
that	O
can	O
exactly	O
observe	O
the	O
location	O
of	O
every	O
object	O
around	O
it	O
.	O
if	O
the	O
54	O
chapter	O
3.	O
probability	O
and	O
information	O
theory	O
robot	O
discretizes	O
space	O
when	O
predicting	O
the	O
future	O
location	O
of	O
these	O
objects	O
,	O
then	O
the	O
discretization	O
makes	O
the	O
robot	O
immediately	O
become	O
uncertain	O
about	O
the	O
precise	O
position	B
of	O
objects	O
:	O
each	O
object	O
could	O
be	O
anywhere	O
within	O
the	O
discrete	O
cell	O
that	O
it	O
was	O
observed	O
to	O
occupy	O
.	O
in	O
many	O
cases	O
,	O
it	O
is	O
more	O
practical	O
to	O
use	O
a	O
simple	O
but	O
uncertain	O
rule	O
rather	O
than	O
a	O
complex	O
but	O
certain	O
one	O
,	O
even	O
if	O
the	O
true	O
rule	O
is	O
deterministic	O
and	O
our	O
modeling	O
system	O
has	O
the	O
ﬁdelity	O
to	O
accommodate	O
a	O
complex	O
rule	O
.	O
for	O
example	O
,	O
the	O
simple	O
rule	O
“	O
most	O
birds	O
ﬂy	O
”	O
is	O
cheap	O
to	O
develop	O
and	O
is	O
broadly	O
useful	O
,	O
while	O
a	O
rule	O
of	O
the	O
form	O
,	O
“	O
birds	O
ﬂy	O
,	O
except	O
for	O
very	O
young	O
birds	O
that	O
have	O
not	O
yet	O
learned	O
to	O
ﬂy	O
,	O
sick	O
or	O
injured	O
birds	O
that	O
have	O
lost	O
the	O
ability	O
to	O
ﬂy	O
,	O
ﬂightless	O
species	O
of	O
birds	O
including	O
the	O
cassowary	O
,	O
ostrich	O
and	O
kiwi	O
.	O
.	O
.	O
”	O
is	O
expensive	O
to	O
develop	O
,	O
maintain	O
and	O
communicate	O
,	O
and	O
after	O
all	O
of	O
this	O
eﬀort	O
is	O
still	O
very	O
brittle	O
and	O
prone	O
to	O
failure	O
.	O
while	O
it	O
should	O
be	O
clear	O
that	O
we	O
need	O
a	O
means	O
of	O
representing	O
and	O
reasoning	O
about	O
uncertainty	O
,	O
it	O
is	O
not	O
immediately	O
obvious	O
that	O
probability	O
theory	O
can	O
provide	O
all	O
of	O
the	O
tools	O
we	O
want	O
for	O
artiﬁcial	O
intelligence	O
applications	O
.	O
probability	O
theory	O
was	O
originally	O
developed	O
to	O
analyze	O
the	O
frequencies	O
of	O
events	O
.	O
it	O
is	O
easy	O
to	O
see	O
how	O
probability	O
theory	O
can	O
be	O
used	O
to	O
study	O
events	O
like	O
drawing	O
a	O
certain	O
hand	O
of	O
cards	O
in	O
a	O
game	O
of	O
poker	O
.	O
these	O
kinds	O
of	O
events	O
are	O
often	O
repeatable	O
.	O
when	O
we	O
say	O
that	O
an	O
outcome	O
has	O
a	O
probability	O
p	O
of	O
occurring	O
,	O
it	O
means	O
that	O
if	O
we	O
repeated	O
the	O
experiment	O
(	O
e.g.	O
,	O
draw	O
a	O
hand	O
of	O
cards	O
)	O
inﬁnitely	O
many	O
times	O
,	O
then	O
proportion	O
p	O
of	O
the	O
repetitions	O
would	O
result	O
in	O
that	O
outcome	O
.	O
this	O
kind	O
of	O
reasoning	O
does	O
not	O
seem	O
immediately	O
applicable	O
to	O
propositions	O
that	O
are	O
not	O
repeatable	O
.	O
if	O
a	O
doctor	O
analyzes	O
a	O
patient	O
and	O
says	O
that	O
the	O
patient	O
has	O
a	O
40	O
%	O
chance	O
of	O
having	O
the	O
ﬂu	O
,	O
this	O
means	O
something	O
very	O
diﬀerent—we	O
can	O
not	O
make	O
inﬁnitely	O
many	O
replicas	O
of	O
the	O
patient	O
,	O
nor	O
is	O
there	O
any	O
reason	O
to	O
believe	O
that	O
diﬀerent	O
replicas	O
of	O
the	O
patient	O
would	O
present	O
with	O
the	O
same	O
symptoms	O
yet	O
have	O
varying	O
underlying	O
conditions	B
.	O
in	O
the	O
case	O
of	O
the	O
doctor	O
diagnosing	O
the	O
patient	O
,	O
we	O
use	O
probability	O
to	O
represent	O
a	O
degree	O
of	O
belief	O
,	O
with	O
1	O
indicating	O
absolute	O
certainty	O
that	O
the	O
patient	O
has	O
the	O
ﬂu	O
and	O
0	O
indicating	O
absolute	O
certainty	O
that	O
the	O
patient	O
does	O
not	O
have	O
the	O
ﬂu	O
.	O
the	O
former	O
kind	O
of	O
probability	O
,	O
related	O
directly	O
to	O
the	O
rates	O
at	O
which	O
events	O
occur	O
,	O
is	O
known	O
as	O
frequentist	O
probability	O
,	O
while	O
the	O
latter	O
,	O
related	O
to	O
qualitative	O
levels	O
of	O
certainty	O
,	O
is	O
known	O
as	O
bayesian	O
probability	O
.	O
if	O
we	O
list	O
several	O
properties	O
that	O
we	O
expect	O
common	O
sense	O
reasoning	O
about	O
uncertainty	O
to	O
have	O
,	O
then	O
the	O
only	O
way	O
to	O
satisfy	O
those	O
properties	O
is	O
to	O
treat	O
bayesian	O
probabilities	O
as	O
behaving	O
exactly	O
the	O
same	O
as	O
frequentist	O
probabilities	O
.	O
for	O
example	O
,	O
if	O
we	O
want	O
to	O
compute	O
the	O
probability	O
that	O
a	O
player	O
will	O
win	O
a	O
poker	O
game	O
given	O
that	O
she	O
has	O
a	O
certain	O
set	O
of	O
cards	O
,	O
we	O
use	O
exactly	O
the	O
same	O
formulas	O
as	O
when	O
we	O
compute	O
the	O
probability	O
that	O
a	O
patient	O
has	O
a	O
disease	O
given	O
that	O
she	O
55	O
chapter	O
3.	O
probability	O
and	O
information	O
theory	O
has	O
certain	O
symptoms	O
.	O
for	O
more	O
details	O
about	O
why	O
a	O
small	O
set	O
of	O
common	O
sense	O
assumptions	O
implies	O
that	O
the	O
same	O
axioms	O
must	O
control	O
both	O
kinds	O
of	O
probability	O
,	O
see	O
ramsey	O
1926	O
(	O
)	O
.	O
probability	O
can	O
be	O
seen	O
as	O
the	O
extension	O
of	O
logic	O
to	O
deal	O
with	O
uncertainty	O
.	O
logic	O
provides	O
a	O
set	O
of	O
formal	O
rules	O
for	O
determining	O
what	O
propositions	O
are	O
implied	O
to	O
be	O
true	O
or	O
false	O
given	O
the	O
assumption	O
that	O
some	O
other	O
set	O
of	O
propositions	O
is	O
true	O
or	O
false	O
.	O
probability	O
theory	O
provides	O
a	O
set	O
of	O
formal	O
rules	O
for	O
determining	O
the	O
likelihood	O
of	O
a	O
proposition	O
being	O
true	O
given	O
the	O
likelihood	O
of	O
other	O
propositions	O
.	O
3.2	O
random	O
variables	O
a	O
random	O
variable	O
is	O
a	O
variable	O
that	O
can	O
take	O
on	O
diﬀerent	O
values	O
randomly	O
.	O
we	O
typically	O
denote	O
the	O
random	O
variable	O
itself	O
with	O
a	O
lower	O
case	O
letter	O
in	O
plain	O
typeface	O
,	O
and	O
the	O
values	O
it	O
can	O
take	O
on	O
with	O
lower	O
case	O
script	O
letters	O
.	O
for	O
example	O
,	O
x1	O
and	O
x2	O
are	O
both	O
possible	O
values	O
that	O
the	O
random	O
variable	O
x	O
can	O
take	O
on	O
.	O
for	O
vector-valued	O
variables	O
,	O
we	O
would	O
write	O
the	O
random	O
variable	O
as	O
x	O
and	O
one	O
of	O
its	O
values	O
as	O
x.	O
on	O
its	O
own	O
,	O
a	O
random	O
variable	O
is	O
just	O
a	O
description	O
of	O
the	O
states	O
that	O
are	O
possible	O
;	O
it	O
must	O
be	O
coupled	O
with	O
a	O
probability	O
distribution	O
that	O
speciﬁes	O
how	O
likely	O
each	O
of	O
these	O
states	O
are	O
.	O
random	O
variables	O
may	O
be	O
discrete	O
or	O
continuous	O
.	O
a	O
discrete	O
random	O
variable	O
is	O
one	O
that	O
has	O
a	O
ﬁnite	O
or	O
countably	O
inﬁnite	O
number	O
of	O
states	O
.	O
note	O
that	O
these	O
states	O
are	O
not	O
necessarily	O
the	O
integers	O
;	O
they	O
can	O
also	O
just	O
be	O
named	O
states	O
that	O
are	O
not	O
considered	O
to	O
have	O
any	O
numerical	O
value	O
.	O
a	O
continuous	O
random	O
variable	O
is	O
associated	O
with	O
a	O
real	O
value	O
.	O
3.3	O
probability	O
distributions	O
a	O
probability	O
distribution	O
is	O
a	O
description	O
of	O
how	O
likely	O
a	O
random	O
variable	O
or	O
set	O
of	O
random	O
variables	O
is	O
to	O
take	O
on	O
each	O
of	O
its	O
possible	O
states	O
.	O
the	O
way	O
we	O
describe	O
probability	O
distributions	O
depends	O
on	O
whether	O
the	O
variables	O
are	O
discrete	O
or	O
continuous	O
.	O
3.3.1	O
discrete	O
variables	O
and	O
probability	O
mass	O
functions	O
a	O
probability	O
distribution	O
over	O
discrete	O
variables	O
may	O
be	O
described	O
using	O
a	O
proba-	O
bility	O
mass	O
function	O
(	O
pmf	O
)	O
.	O
we	O
typically	O
denote	O
probability	O
mass	O
functions	O
with	O
a	O
capital	O
p	O
.	O
often	O
we	O
associate	O
each	O
random	O
variable	O
with	O
a	O
diﬀerent	O
probability	O
56	O
chapter	O
3.	O
probability	O
and	O
information	O
theory	O
mass	O
function	O
and	O
the	O
reader	O
must	O
infer	O
which	O
probability	O
mass	O
function	O
to	O
use	O
based	O
on	O
the	O
identity	O
of	O
the	O
random	O
variable	O
,	O
rather	O
than	O
the	O
name	O
of	O
the	O
function	O
;	O
p	O
(	O
)	O
x	O
is	O
usually	O
not	O
the	O
same	O
as	O
(	O
)	O
y	O
.	O
p	O
the	O
probability	O
mass	O
function	O
maps	O
from	O
a	O
state	O
of	O
a	O
random	O
variable	O
to	O
the	O
probability	O
of	O
that	O
random	O
variable	O
taking	O
on	O
that	O
state	O
.	O
the	O
probability	O
that	O
x	O
=	O
x	O
is	O
denoted	O
as	O
p	O
(	O
x	O
)	O
,	O
with	O
a	O
probability	O
of	O
1	O
indicating	O
that	O
x	O
=	O
x	O
is	O
certain	O
and	O
a	O
probability	O
of	O
0	O
indicating	O
that	O
x	O
=	O
x	O
is	O
impossible	O
.	O
sometimes	O
to	O
disambiguate	O
which	O
pmf	O
to	O
use	O
,	O
we	O
write	O
the	O
name	O
of	O
the	O
random	O
variable	O
explicitly	O
:	O
p	O
(	O
x	O
=	O
x	O
)	O
.	O
sometimes	O
we	O
deﬁne	O
a	O
variable	O
ﬁrst	O
,	O
then	O
use	O
notation	O
to	O
specify	O
which	O
distribution	O
it	O
follows	O
later	O
:	O
x	O
x	O
.	O
p	O
(	O
)	O
∼	O
∼	O
probability	O
mass	O
functions	O
can	O
act	O
on	O
many	O
variables	O
at	O
the	O
same	O
time	O
.	O
such	O
a	O
probability	O
distribution	O
over	O
many	O
variables	O
is	O
known	O
as	O
a	O
joint	O
probability	O
distribution	O
.	O
p	O
(	O
x	O
=	O
x	O
,	O
y	O
=	O
y	O
)	O
denotes	O
the	O
probability	O
that	O
x	O
=	O
x	O
and	O
y	O
=	O
y	O
simultaneously	O
.	O
we	O
may	O
also	O
write	O
for	O
brevity	O
.	O
p	O
x	O
,	O
y	O
(	O
)	O
to	O
be	O
a	O
probability	O
mass	O
function	O
on	O
a	O
random	O
variable	O
x	O
,	O
a	O
function	O
p	O
must	O
satisfy	O
the	O
following	O
properties	O
:	O
•	O
•	O
∀	O
∈	O
	O
•	O
the	O
domain	O
of	O
must	O
be	O
the	O
set	O
of	O
all	O
possible	O
states	O
of	O
x.	O
p	O
≤	O
≤	O
x	O
x	O
,	O
0	O
p	O
(	O
x	O
)	O
1.	O
an	O
impossible	O
event	O
has	O
probability	O
and	O
no	O
state	O
can	O
be	O
less	O
probable	O
than	O
that	O
.	O
likewise	O
,	O
an	O
event	O
that	O
is	O
guaranteed	O
to	O
happen	O
has	O
probability	O
,	O
and	O
no	O
state	O
can	O
have	O
a	O
greater	O
chance	O
of	O
occurring	O
.	O
0	O
1	O
∈	O
x	O
x	O
p	O
(	O
x	O
)	O
=	O
1.	O
we	O
refer	O
to	O
this	O
property	O
as	O
being	O
normalized	O
.	O
without	O
this	O
property	O
,	O
we	O
could	O
obtain	O
probabilities	O
greater	O
than	O
one	O
by	O
computing	O
the	O
probability	O
of	O
one	O
of	O
many	O
events	O
occurring	O
.	O
for	O
example	O
,	O
consider	O
a	O
single	O
discrete	O
random	O
variable	O
x	O
with	O
k	O
diﬀerent	O
states	O
.	O
we	O
can	O
place	O
a	O
uniform	O
distribution	O
on	O
x—that	O
is	O
,	O
make	O
each	O
of	O
its	O
states	O
equally	O
likely—by	O
setting	O
its	O
probability	O
mass	O
function	O
to	O
	O
p	O
(	O
=	O
x	O
x	O
i	O
)	O
=	O
	O
1	O
k	O
(	O
3.1	O
)	O
for	O
all	O
i.	O
we	O
can	O
see	O
that	O
this	O
ﬁts	O
the	O
requirements	O
for	O
a	O
probability	O
mass	O
function	O
.	O
the	O
value	O
1	O
k	O
is	O
a	O
positive	O
integer	O
.	O
we	O
also	O
see	O
that	O
is	O
positive	O
because	O
k	O
p	O
(	O
=	O
x	O
i	O
)	O
=	O
x	O
i	O
i	O
1	O
k	O
=	O
k	O
k	O
=	O
1	O
,	O
(	O
3.2	O
)	O
so	O
the	O
distribution	O
is	O
properly	O
normalized	O
.	O
57	O
chapter	O
3.	O
probability	O
and	O
information	O
theory	O
3.3.2	O
continuous	O
variables	O
and	O
probability	O
density	O
functions	O
when	O
working	O
with	O
continuous	O
random	O
variables	O
,	O
we	O
describe	O
probability	O
distri-	O
butions	O
using	O
a	O
probability	O
density	O
function	O
(	O
pdf	O
)	O
rather	O
than	O
a	O
probability	O
mass	O
function	O
.	O
to	O
be	O
a	O
probability	O
density	O
function	O
,	O
a	O
function	O
p	O
must	O
satisfy	O
the	O
following	O
properties	O
:	O
the	O
domain	O
of	O
must	O
be	O
the	O
set	O
of	O
all	O
possible	O
states	O
of	O
x.	O
p	O
0	O
.	O
note	O
that	O
we	O
do	O
not	O
require	O
p	O
≤	O
(	O
)	O
x	O
1	O
.	O
	O
•	O
•	O
∀	O
∈	O
•	O
x	O
≥	O
x	O
,	O
p	O
x	O
(	O
)	O
p	O
x	O
dx	O
(	O
)	O
=	O
1.	O
a	O
probability	O
density	O
function	O
p	O
(	O
x	O
)	O
does	O
not	O
give	O
the	O
probability	O
of	O
a	O
speciﬁc	O
state	O
directly	O
,	O
instead	O
the	O
probability	O
of	O
landing	O
inside	O
an	O
inﬁnitesimal	O
region	O
with	O
volume	O
is	O
given	O
by	O
.	O
p	O
x	O
δx	O
(	O
)	O
δx	O
we	O
can	O
integrate	O
the	O
density	O
function	O
to	O
ﬁnd	O
the	O
actual	O
probability	O
mass	O
of	O
a	O
set	O
of	O
points	O
.	O
speciﬁcally	O
,	O
the	O
probability	O
that	O
x	O
lies	O
in	O
some	O
set	O
s	O
is	O
given	O
by	O
the	O
integral	O
of	O
p	O
(	O
x	O
)	O
over	O
that	O
set	O
.	O
in	O
the	O
univariate	O
example	O
,	O
the	O
probability	O
that	O
x	O
lies	O
in	O
the	O
interval	O
is	O
given	O
by	O
.	O
p	O
x	O
dx	O
[	O
]	O
a	O
,	O
b	O
(	O
)	O
	O
[	O
]	O
a	O
,	O
b	O
for	O
an	O
example	O
of	O
a	O
probability	O
density	O
function	O
corresponding	O
to	O
a	O
speciﬁc	O
probability	O
density	O
over	O
a	O
continuous	O
random	O
variable	O
,	O
consider	O
a	O
uniform	O
distribu-	O
tion	B
on	O
an	O
interval	O
of	O
the	O
real	O
numbers	O
.	O
we	O
can	O
do	O
this	O
with	O
a	O
function	O
u	O
(	O
x	O
;	O
a	O
,	O
b	O
)	O
,	O
where	O
a	O
and	O
b	O
are	O
the	O
endpoints	O
of	O
the	O
interval	O
,	O
with	O
b	O
>	O
a.	O
the	O
“	O
;	O
”	O
notation	O
means	O
“	O
parametrized	O
by	O
”	O
;	O
we	O
consider	O
x	O
to	O
be	O
the	O
argument	O
of	O
the	O
function	O
,	O
while	O
a	O
and	O
b	O
are	O
parameters	O
that	O
deﬁne	O
the	O
function	O
.	O
to	O
ensure	O
that	O
there	O
is	O
no	O
probability	O
mass	O
outside	O
the	O
interval	O
,	O
we	O
say	O
u	O
(	O
x	O
;	O
a	O
,	O
b	O
)	O
=	O
0	O
for	O
all	O
x	O
.	O
within	O
a	O
,	O
b	O
]	O
,	O
−	O
.	O
we	O
can	O
see	O
that	O
this	O
is	O
nonnegative	O
everywhere	O
.	O
additionally	O
,	O
it	O
)	O
=	O
1	O
u	O
x	O
a	O
,	O
b	O
b	O
a	O
∼	O
integrates	O
to	O
1.	O
we	O
often	O
denote	O
that	O
x	O
follows	O
the	O
uniform	O
distribution	O
on	O
[	O
a	O
,	O
b	O
]	O
by	O
writing	O
x	O
.	O
u	O
a	O
,	O
b	O
)	O
[	O
a	O
,	O
b	O
]	O
(	O
;	O
∈	O
(	O
[	O
3.4	O
marginal	O
probability	O
sometimes	O
we	O
know	O
the	O
probability	O
distribution	O
over	O
a	O
set	O
of	O
variables	O
and	O
we	O
want	O
to	O
know	O
the	O
probability	O
distribution	O
over	O
just	O
a	O
subset	O
of	O
them	O
.	O
the	O
probability	O
distribution	O
over	O
the	O
subset	O
is	O
known	O
as	O
the	O
distribution	O
.	O
marginal	O
probability	O
for	O
example	O
,	O
suppose	O
we	O
have	O
discrete	O
random	O
variables	O
x	O
and	O
y	O
,	O
and	O
we	O
know	O
p	O
,	O
(	O
x	O
y	O
.	O
we	O
can	O
ﬁnd	O
)	O
x	O
with	O
the	O
:	O
sum	O
rule	O
p	O
(	O
)	O
∀	O
∈	O
x	O
x	O
,	O
p	O
(	O
=	O
)	O
=x	O
x	O
p	O
(	O
=	O
x	O
x	O
,	O
y	O
=	O
)	O
y	O
.	O
(	O
3.3	O
)	O
	O
y	O
58	O
chapter	O
3.	O
probability	O
and	O
information	O
theory	O
the	O
name	O
“	O
marginal	O
probability	O
”	O
comes	O
from	O
the	O
process	O
of	O
computing	O
marginal	O
probabilities	O
on	O
paper	O
.	O
when	O
the	O
values	O
of	O
p	O
(	O
x	O
y	O
,	O
)	O
are	O
written	O
in	O
a	O
grid	O
with	O
diﬀerent	O
values	O
of	O
x	O
in	O
rows	O
and	O
diﬀerent	O
values	O
of	O
y	O
in	O
columns	O
,	O
it	O
is	O
natural	O
to	O
sum	O
across	O
a	O
row	O
of	O
the	O
grid	O
,	O
then	O
write	O
p	O
(	O
x	O
)	O
in	O
the	O
margin	O
of	O
the	O
paper	O
just	O
to	O
the	O
right	O
of	O
the	O
row	O
.	O
	O
for	O
continuous	O
variables	O
,	O
we	O
need	O
to	O
use	O
integration	O
instead	O
of	O
summation	O
:	O
p	O
x	O
(	O
)	O
=	O
p	O
x	O
,	O
y	O
dy	O
.	O
(	O
)	O
(	O
3.4	O
)	O
3.5	O
conditional	O
probability	O
in	O
many	O
cases	O
,	O
we	O
are	O
interested	O
in	O
the	O
probability	O
of	O
some	O
event	O
,	O
given	O
that	O
some	O
other	O
event	O
has	O
happened	O
.	O
this	O
is	O
called	O
a	O
conditional	O
probability	O
.	O
we	O
denote	O
x	O
=	O
x	O
)	O
.	O
this	O
the	O
conditional	O
probability	O
that	O
y	O
=	O
y	O
given	O
x	O
=	O
x	O
as	O
p	O
(	O
y	O
=	O
y	O
conditional	O
probability	O
can	O
be	O
computed	O
with	O
the	O
formula	O
|	O
|	O
p	O
(	O
=	O
y	O
y	O
x	O
=	O
)	O
=	O
x	O
p	O
(	O
=	O
y	O
y	O
,	O
x	O
=	O
)	O
x	O
x	O
)	O
p	O
(	O
=	O
x	O
.	O
(	O
3.5	O
)	O
the	O
conditional	O
probability	O
is	O
only	O
deﬁned	O
when	O
p	O
(	O
x	O
=	O
x	O
)	O
>	O
0.	O
we	O
can	O
not	O
compute	O
the	O
conditional	O
probability	O
conditioned	O
on	O
an	O
event	O
that	O
never	O
happens	O
.	O
it	O
is	O
important	O
not	O
to	O
confuse	O
conditional	O
probability	O
with	O
computing	O
what	O
would	O
happen	O
if	O
some	O
action	O
were	O
undertaken	O
.	O
the	O
conditional	O
probability	O
that	O
a	O
person	O
is	O
from	O
germany	O
given	O
that	O
they	O
speak	O
german	O
is	O
quite	O
high	O
,	O
but	O
if	O
a	O
randomly	O
selected	O
person	O
is	O
taught	O
to	O
speak	O
german	O
,	O
their	O
country	O
of	O
origin	O
does	O
not	O
change	O
.	O
computing	O
the	O
consequences	O
of	O
an	O
action	O
is	O
called	O
making	O
an	O
intervention	O
query	O
.	O
intervention	O
queries	O
are	O
the	O
domain	O
of	O
causal	O
modeling	O
,	O
which	O
we	O
do	O
not	O
explore	O
in	O
this	O
book	O
.	O
3.6	O
the	O
chain	O
rule	O
of	O
conditional	O
probabilities	O
any	O
joint	O
probability	O
distribution	O
over	O
many	O
random	O
variables	O
may	O
be	O
decomposed	O
into	O
conditional	O
distributions	O
over	O
only	O
one	O
variable	O
:	O
|	O
−	O
1	O
)	O
i	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
.	O
p	O
(	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
n	O
)	O
=	O
(	O
p	O
x	O
(	O
1	O
)	O
)	O
πn	O
i=2p	O
(	O
x	O
(	O
)	O
i	O
(	O
3.6	O
)	O
this	O
observation	O
is	O
known	O
as	O
the	O
chain	O
rule	O
or	O
product	O
rule	O
of	O
probability	O
.	O
.3.5	O
it	O
follows	O
immediately	O
from	O
the	O
deﬁnition	O
of	O
conditional	O
probability	O
in	O
equation	O
59	O
chapter	O
3.	O
probability	O
and	O
information	O
theory	O
for	O
example	O
,	O
applying	O
the	O
deﬁnition	O
twice	O
,	O
we	O
get	O
p	O
,	O
(	O
a	O
b	O
c	O
)	O
=	O
,	O
(	O
b	O
c	O
)	O
=	O
p	O
,	O
p	O
,	O
(	O
a	O
b	O
c	O
)	O
=	O
,	O
p	O
p	O
p	O
|	O
(	O
a	O
b	O
c	O
)	O
(	O
b	O
c	O
)	O
,	O
p	O
,	O
|	O
b	O
c	O
(	O
)	O
p	O
|	O
|	O
b	O
c	O
(	O
a	O
b	O
(	O
p	O
)	O
c	O
)	O
,	O
p	O
(	O
)	O
c	O
(	O
)	O
c	O
.	O
3.7	O
independence	O
and	O
conditional	O
independence	O
two	O
random	O
variables	O
x	O
and	O
y	O
are	O
independent	O
if	O
their	O
probability	O
distribution	O
can	O
be	O
expressed	O
as	O
a	O
product	O
of	O
two	O
factors	O
,	O
one	O
involving	O
only	O
x	O
and	O
one	O
involving	O
only	O
y	O
:	O
∀	O
∈	O
x	O
x	O
,	O
y	O
∈	O
y	O
,	O
p	O
(	O
=	O
x	O
,	O
=	O
)	O
=	O
(	O
p	O
=	O
)	O
(	O
x	O
p	O
=	O
)	O
y	O
.	O
x	O
y	O
x	O
y	O
y	O
(	O
3.7	O
)	O
two	O
random	O
variables	O
x	O
and	O
y	O
are	O
conditionally	O
independent	O
given	O
a	O
random	O
variable	O
z	O
if	O
the	O
conditional	O
probability	O
distribution	O
over	O
x	O
and	O
y	O
factorizes	O
in	O
this	O
way	O
for	O
every	O
value	O
of	O
z	O
:	O
∀	O
∈	O
x	O
x	O
,	O
y	O
∈	O
y	O
,	O
z	O
∈	O
|	O
z	O
,	O
p	O
(	O
=	O
x	O
,	O
=	O
y	O
z	O
x	O
y	O
|	O
p	O
=	O
x	O
z	O
x	O
=	O
)	O
=	O
(	O
z	O
|	O
=	O
)	O
(	O
z	O
p	O
=	O
y	O
z	O
=	O
)	O
z	O
.	O
y	O
we	O
can	O
denote	O
independence	O
and	O
conditional	O
independence	O
with	O
compact	O
means	O
that	O
x	O
⊥	O
|	O
means	O
that	O
x	O
and	O
y	O
are	O
independent	O
,	O
while	O
x	O
y	O
z	O
⊥	O
notation	O
:	O
x	O
y	O
and	O
y	O
are	O
conditionally	O
independent	O
given	O
z	O
.	O
(	O
3.8	O
)	O
3.8	O
expectation	O
,	O
variance	O
and	O
covariance	O
the	O
expectation	O
or	O
expected	O
value	O
of	O
some	O
function	O
f	O
(	O
x	O
)	O
with	O
respect	O
to	O
a	O
probability	O
distribution	O
p	O
(	O
x	O
)	O
is	O
the	O
average	O
or	O
mean	O
value	O
that	O
f	O
takes	O
on	O
when	O
x	O
is	O
drawn	O
from	O
.	O
for	O
discrete	O
variables	O
this	O
can	O
be	O
computed	O
with	O
a	O
summation	O
:	O
p	O
	O
	O
x	O
∼	O
p	O
[	O
(	O
)	O
]	O
=	O
ex	O
f	O
x	O
p	O
x	O
f	O
x	O
,	O
(	O
)	O
(	O
)	O
(	O
3.9	O
)	O
while	O
for	O
continuous	O
variables	O
,	O
it	O
is	O
computed	O
with	O
an	O
integral	O
:	O
∼	O
p	O
[	O
(	O
)	O
]	O
=	O
ex	O
f	O
x	O
p	O
x	O
f	O
x	O
dx	O
.	O
(	O
)	O
(	O
)	O
(	O
3.10	O
)	O
60	O
chapter	O
3.	O
probability	O
and	O
information	O
theory	O
when	O
the	O
identity	O
of	O
the	O
distribution	O
is	O
clear	O
from	O
the	O
context	O
,	O
we	O
may	O
simply	O
write	O
the	O
name	O
of	O
the	O
random	O
variable	O
that	O
the	O
expectation	O
is	O
over	O
,	O
as	O
in	O
ex	O
[	O
f	O
(	O
x	O
)	O
]	O
.	O
·	O
if	O
it	O
is	O
clear	O
which	O
random	O
variable	O
the	O
expectation	O
is	O
over	O
,	O
we	O
may	O
omit	O
the	O
subscript	O
entirely	O
,	O
as	O
in	O
e	O
[	O
f	O
(	O
x	O
)	O
]	O
.	O
by	O
default	O
,	O
we	O
can	O
assume	O
that	O
e	O
[	O
]	O
averages	O
over	O
the	O
values	O
of	O
all	O
the	O
random	O
variables	O
inside	O
the	O
brackets	O
.	O
likewise	O
,	O
when	O
there	O
is	O
no	O
ambiguity	O
,	O
we	O
may	O
omit	O
the	O
square	O
brackets	O
.	O
expectations	O
are	O
linear	O
,	O
for	O
example	O
,	O
ex	O
[	O
αf	O
x	O
(	O
)	O
+	O
(	O
)	O
]	O
=	O
βg	O
x	O
αex	O
[	O
(	O
)	O
]	O
+	O
f	O
x	O
βex	O
[	O
(	O
)	O
]	O
g	O
x	O
,	O
(	O
3.11	O
)	O
when	O
α	O
and	O
β	O
are	O
not	O
dependent	O
on	O
.	O
x	O
the	O
variance	O
gives	O
a	O
measure	O
of	O
how	O
much	O
the	O
values	O
of	O
a	O
function	O
of	O
a	O
random	O
variable	O
x	O
vary	O
as	O
we	O
sample	O
diﬀerent	O
values	O
of	O
x	O
from	O
its	O
probability	O
distribution	O
:	O
	O
	O
−	O
var	O
(	O
(	O
)	O
)	O
=	O
f	O
x	O
e	O
(	O
(	O
)	O
f	O
x	O
e	O
f	O
x	O
2	O
[	O
(	O
)	O
]	O
)	O
.	O
(	O
3.12	O
)	O
when	O
the	O
variance	O
is	O
low	O
,	O
the	O
values	O
of	O
f	O
(	O
x	O
)	O
cluster	O
near	O
their	O
expected	O
value	O
.	O
the	O
square	O
root	O
of	O
the	O
variance	O
is	O
known	O
as	O
the	O
.	O
standard	O
deviation	O
the	O
covariance	O
gives	O
some	O
sense	O
of	O
how	O
much	O
two	O
values	O
are	O
linearly	O
related	O
to	O
each	O
other	O
,	O
as	O
well	O
as	O
the	O
scale	O
of	O
these	O
variables	O
:	O
cov	O
(	O
(	O
)	O
f	O
x	O
,	O
g	O
y	O
(	O
)	O
)	O
=	O
e	O
f	O
x	O
[	O
(	O
(	O
)	O
e	O
f	O
x	O
[	O
(	O
)	O
]	O
)	O
(	O
(	O
)	O
g	O
y	O
−	O
−	O
e	O
g	O
y	O
[	O
(	O
)	O
]	O
)	O
]	O
.	O
(	O
3.13	O
)	O
high	O
absolute	O
values	O
of	O
the	O
covariance	O
mean	O
that	O
the	O
values	O
change	O
very	O
much	O
and	O
are	O
both	O
far	O
from	O
their	O
respective	O
means	O
at	O
the	O
same	O
time	O
.	O
if	O
the	O
sign	O
of	O
the	O
covariance	O
is	O
positive	O
,	O
then	O
both	O
variables	O
tend	O
to	O
take	O
on	O
relatively	O
high	O
values	O
simultaneously	O
.	O
if	O
the	O
sign	O
of	O
the	O
covariance	O
is	O
negative	O
,	O
then	O
one	O
variable	O
tends	O
to	O
take	O
on	O
a	O
relatively	O
high	O
value	O
at	O
the	O
times	O
that	O
the	O
other	O
takes	O
on	O
a	O
relatively	O
low	O
value	O
and	O
vice	O
versa	O
.	O
other	O
measures	O
such	O
as	O
correlation	O
normalize	O
the	O
contribution	O
of	O
each	O
variable	O
in	O
order	O
to	O
measure	O
only	O
how	O
much	O
the	O
variables	O
are	O
related	O
,	O
rather	O
than	O
also	O
being	O
aﬀected	O
by	O
the	O
scale	O
of	O
the	O
separate	O
variables	O
.	O
the	O
notions	O
of	O
covariance	O
and	O
dependence	O
are	O
related	O
,	O
but	O
are	O
in	O
fact	O
distinct	O
concepts	O
.	O
they	O
are	O
related	O
because	O
two	O
variables	O
that	O
are	O
independent	O
have	O
zero	O
covariance	O
,	O
and	O
two	O
variables	O
that	O
have	O
non-zero	O
covariance	O
are	O
dependent	O
.	O
how-	O
ever	O
,	O
independence	O
is	O
a	O
distinct	O
property	O
from	O
covariance	O
.	O
for	O
two	O
variables	O
to	O
have	O
zero	O
covariance	O
,	O
there	O
must	O
be	O
no	O
linear	O
dependence	O
between	O
them	O
.	O
independence	O
is	O
a	O
stronger	O
requirement	O
than	O
zero	O
covariance	O
,	O
because	O
independence	O
also	O
excludes	O
nonlinear	O
relationships	O
.	O
it	O
is	O
possible	O
for	O
two	O
variables	O
to	O
be	O
dependent	O
but	O
have	O
−	O
zero	O
covariance	O
.	O
for	O
example	O
,	O
suppose	O
we	O
ﬁrst	O
sample	O
a	O
real	O
number	O
x	O
from	O
a	O
1	O
,	O
1	O
]	O
.	O
we	O
next	O
sample	O
a	O
random	O
variable	O
uniform	O
distribution	O
over	O
the	O
interval	O
[	O
61	O
chapter	O
3.	O
probability	O
and	O
information	O
theory	O
−	O
s.	O
with	O
probability	O
1	O
,	O
we	O
choose	O
the	O
value	O
of	O
s	O
to	O
be	O
.	O
otherwise	O
,	O
we	O
choose	O
2	O
the	O
value	O
of	O
s	O
to	O
be	O
1.	O
we	O
can	O
then	O
generate	O
a	O
random	O
variable	O
y	O
by	O
assigning	O
y	O
=	O
sx	O
.	O
clearly	O
,	O
x	O
and	O
y	O
are	O
not	O
independent	O
,	O
because	O
x	O
completely	O
determines	O
the	O
magnitude	O
of	O
.	O
however	O
,	O
.	O
)	O
=	O
0	O
cov	O
(	O
x	O
,	O
y	O
1	O
y	O
∈	O
×	O
n	O
is	O
an	O
n	O
r	O
the	O
covariance	O
matrix	O
of	O
a	O
random	O
vector	O
x	O
that	O
the	O
diagonal	O
elements	O
of	O
the	O
covariance	O
give	O
the	O
variance	O
:	O
cov	O
(	O
)	O
x	O
i	O
,	O
j	O
=	O
cov	O
(	O
xi	O
,	O
x	O
j	O
)	O
.	O
cov	O
(	O
xi	O
,	O
xi	O
)	O
=	O
var	O
(	O
xi	O
)	O
.	O
3.9	O
common	O
probability	O
distributions	O
n	O
matrix	O
,	O
such	O
(	O
3.14	O
)	O
(	O
3.15	O
)	O
several	O
simple	O
probability	O
distributions	O
are	O
useful	O
in	O
many	O
contexts	O
in	O
machine	O
learning	O
.	O
3.9.1	O
bernoulli	O
distribution	O
the	O
bernoulli	O
distribution	O
is	O
a	O
distribution	O
over	O
a	O
single	O
binary	O
random	O
variable	O
.	O
[	O
0	O
,	O
1	O
]	O
,	O
which	O
gives	O
the	O
probability	O
of	O
the	O
it	O
is	O
controlled	O
by	O
a	O
single	O
parameter	O
φ	O
random	O
variable	O
being	O
equal	O
to	O
1.	O
it	O
has	O
the	O
following	O
properties	O
:	O
∈	O
p	O
p	O
x	O
φ	O
(	O
=	O
1	O
)	O
=	O
−	O
x	O
(	O
=	O
0	O
)	O
=	O
1	O
−	O
)	O
=	O
x	O
(	O
1	O
φ	O
p	O
x	O
(	O
=	O
x	O
φ	O
−	O
x	O
φ	O
1	O
)	O
x	O
ex	O
[	O
]	O
=	O
φ	O
x	O
var	O
x	O
(	O
)	O
=	O
(	O
1	O
φ	O
−	O
)	O
φ	O
(	O
3.16	O
)	O
(	O
3.17	O
)	O
(	O
3.18	O
)	O
(	O
3.19	O
)	O
(	O
3.20	O
)	O
3.9.2	O
multinoulli	O
distribution	O
the	O
multinoulli	O
or	O
categorical	O
distribution	O
is	O
a	O
distribution	O
over	O
a	O
single	O
discrete	O
variable	O
with	O
k	O
diﬀerent	O
states	O
,	O
where	O
k	O
is	O
ﬁnite.1	O
the	O
multinoulli	O
distribution	O
is	O
(	O
1	O
“	O
multinoulli	O
”	O
is	O
a	O
term	O
that	O
was	O
recently	O
coined	O
by	O
gustavo	O
lacerdo	O
and	O
popularized	O
by	O
)	O
.	O
the	O
multinoulli	O
distribution	O
is	O
a	O
special	O
case	O
of	O
the	O
multinomial	O
distribution	O
.	O
murphy	O
2012	O
k	O
representing	O
how	O
many	O
a	O
multinomial	O
distribution	O
is	O
the	O
distribution	O
over	O
vectors	O
in	O
times	O
each	O
of	O
the	O
k	O
categories	O
is	O
visited	O
when	O
n	O
samples	O
are	O
drawn	O
from	O
a	O
multinoulli	O
distribution	O
.	O
many	O
texts	O
use	O
the	O
term	O
“	O
multinomial	O
”	O
to	O
refer	O
to	O
multinoulli	O
distributions	O
without	O
clarifying	O
that	O
they	O
refer	O
only	O
to	O
the	O
}	O
0	O
,	O
.	O
.	O
.	O
,	O
n	O
n	O
=	O
1	O
case	O
.	O
{	O
62	O
chapter	O
3.	O
probability	O
and	O
information	O
theory	O
∈	O
≤	O
−	O
parametrized	O
by	O
a	O
vector	O
p	O
[	O
0	O
,	O
1	O
]	O
k	O
1	O
,	O
where	O
pi	O
gives	O
the	O
probability	O
of	O
the	O
i-th	O
p.	O
note	O
that	O
we	O
must	O
state	O
.	O
the	O
ﬁnal	O
,	O
k-th	O
state	O
’	O
s	O
probability	O
is	O
given	O
by	O
1	O
	O
constrain	O
1	O
1.	O
multinoulli	O
distributions	O
are	O
often	O
used	O
to	O
refer	O
to	O
distributions	O
over	O
categories	O
of	O
objects	O
,	O
so	O
we	O
do	O
not	O
usually	O
assume	O
that	O
state	O
1	O
has	O
numerical	O
value	O
1	O
,	O
etc	O
.	O
for	O
this	O
reason	O
,	O
we	O
do	O
not	O
usually	O
need	O
to	O
compute	O
the	O
expectation	O
or	O
variance	O
of	O
multinoulli-distributed	O
random	O
variables	O
.	O
−	O
	O
p	O
1	O
the	O
bernoulli	O
and	O
multinoulli	O
distributions	O
are	O
suﬃcient	O
to	O
describe	O
any	O
distri-	O
bution	O
over	O
their	O
domain	O
.	O
they	O
are	O
able	O
to	O
describe	O
any	O
distribution	O
over	O
their	O
domain	O
not	O
so	O
much	O
because	O
they	O
are	O
particularly	O
powerful	O
but	O
rather	O
because	O
their	O
domain	O
is	O
simple	O
;	O
they	O
model	B
discrete	O
variables	O
for	O
which	O
it	O
is	O
feasible	O
to	O
enumerate	O
all	O
of	O
the	O
states	O
.	O
when	O
dealing	O
with	O
continuous	O
variables	O
,	O
there	O
are	O
uncountably	O
many	O
states	O
,	O
so	O
any	O
distribution	O
described	O
by	O
a	O
small	O
number	O
of	O
parameters	O
must	O
impose	O
strict	O
limits	O
on	O
the	O
distribution	O
.	O
3.9.3	O
gaussian	O
distribution	O
	O
	O
	O
the	O
most	O
commonly	O
used	O
distribution	O
over	O
real	O
numbers	O
is	O
the	O
normal	O
distribu-	O
tion	B
,	O
also	O
known	O
as	O
the	O
:	O
gaussian	O
distribution	O
−	O
1	O
2σ	O
2	O
2πσ2	O
exp	O
1	O
(	O
;	O
x	O
µ	O
,	O
σ2	O
)	O
=	O
n	O
−	O
(	O
x	O
2	O
)	O
µ	O
.	O
(	O
3.21	O
)	O
see	O
ﬁgure	O
3.1	O
for	O
a	O
plot	O
of	O
the	O
density	O
function	O
.	O
∈	O
∈	O
∞	O
the	O
two	O
parameters	O
µ	O
)	O
control	O
the	O
normal	O
distribution	O
.	O
the	O
parameter	O
µ	O
gives	O
the	O
coordinate	O
of	O
the	O
central	O
peak	O
.	O
this	O
is	O
also	O
the	O
mean	O
of	O
the	O
distribution	O
:	O
e	O
[	O
x	O
]	O
=	O
µ.	O
the	O
standard	O
deviation	O
of	O
the	O
distribution	O
is	O
given	O
by	O
σ	O
,	O
and	O
the	O
variance	O
by	O
σ2	O
.	O
r	O
and	O
σ	O
(	O
0	O
,	O
when	O
we	O
evaluate	O
the	O
pdf	O
,	O
we	O
need	O
to	O
square	O
and	O
invert	O
σ.	O
when	O
we	O
need	O
to	O
frequently	O
evaluate	O
the	O
pdf	O
with	O
diﬀerent	O
parameter	O
values	O
,	O
a	O
more	O
eﬃcient	O
way	O
of	O
parametrizing	O
the	O
distribution	O
is	O
to	O
use	O
a	O
parameter	O
β	O
)	O
to	O
control	O
the	O
precision	O
or	O
inverse	O
variance	O
of	O
the	O
distribution	O
:	O
−1	O
2	O
−	O
1	O
)	O
=	O
	O
(	O
;	O
x	O
µ	O
,	O
β	O
	O
	O
β	O
2π	O
(	O
3.22	O
)	O
∞	O
β	O
x	O
exp	O
n	O
−	O
µ	O
)	O
2	O
(	O
∈	O
(	O
0	O
,	O
.	O
normal	O
distributions	O
are	O
a	O
sensible	O
choice	O
for	O
many	O
applications	O
.	O
in	O
the	O
absence	O
of	O
prior	O
knowledge	O
about	O
what	O
form	O
a	O
distribution	O
over	O
the	O
real	O
numbers	O
should	O
take	O
,	O
the	O
normal	O
distribution	O
is	O
a	O
good	O
default	O
choice	O
for	O
two	O
major	O
reasons	O
.	O
63	O
chapter	O
3.	O
probability	O
and	O
information	O
theory	O
)	O
x	O
(	O
p	O
0	O
40	O
.	O
0	O
35	O
.	O
0	O
30	O
.	O
0	O
25	O
.	O
0	O
20	O
.	O
0	O
15	O
.	O
0	O
10	O
.	O
0	O
05	O
.	O
−	O
0	O
00	O
.	O
2	O
0.	O
maximum	O
at	O
=	O
x	O
µ	O
±	O
inﬂection	O
points	O
at	O
x	O
=	O
µ	O
σ	O
−	O
1	O
5	O
.	O
−	O
1	O
0	O
.	O
−	O
0	O
5	O
.	O
0	O
0	O
.	O
0	O
5	O
.	O
1	O
0	O
.	O
figure	O
3.1	O
:	O
the	O
normal	O
distribution	O
:	O
the	O
normal	O
distribution	O
(	O
x	O
;	O
µ	O
,	O
σ2	O
)	O
exhibits	O
a	O
classic	O
“	O
bell	O
curve	O
”	O
shape	O
,	O
with	O
the	O
x	O
coordinate	O
of	O
its	O
central	O
peak	O
given	O
by	O
µ	O
,	O
and	O
the	O
width	O
of	O
its	O
peak	O
controlled	O
by	O
σ.	O
in	O
this	O
example	O
,	O
we	O
depict	O
the	O
standard	O
normal	O
distribution	O
,	O
with	O
.	O
σ	O
=	O
1	O
µ	O
=	O
0	O
and	O
2	O
0	O
.	O
.	O
1	O
5	O
n	O
first	O
,	O
many	O
distributions	O
we	O
wish	O
to	O
model	B
are	O
truly	O
close	O
to	O
being	O
normal	O
distributions	O
.	O
the	O
central	O
limit	O
theorem	O
shows	O
that	O
the	O
sum	O
of	O
many	O
indepen-	O
dent	O
random	O
variables	O
is	O
approximately	O
normally	O
distributed	O
.	O
this	O
means	O
that	O
in	O
practice	O
,	O
many	O
complicated	O
systems	O
can	O
be	O
modeled	O
successfully	O
as	O
normally	O
distributed	O
noise	O
,	O
even	O
if	O
the	O
system	O
can	O
be	O
decomposed	O
into	O
parts	O
with	O
more	O
structured	O
behavior	O
.	O
x	O
	O
second	O
,	O
out	O
of	O
all	O
possible	O
probability	O
distributions	O
with	O
the	O
same	O
variance	O
,	O
the	O
normal	O
distribution	O
encodes	O
the	O
maximum	O
amount	O
of	O
uncertainty	O
over	O
the	O
real	O
numbers	O
.	O
we	O
can	O
thus	O
think	O
of	O
the	O
normal	O
distribution	O
as	O
being	O
the	O
one	O
that	O
inserts	O
the	O
least	O
amount	O
of	O
prior	O
knowledge	O
into	O
a	O
model	B
.	O
fully	O
developing	O
and	O
justifying	O
this	O
idea	O
requires	O
more	O
mathematical	O
tools	O
,	O
and	O
is	O
postponed	O
to	O
section	O
.	O
19.4.2	O
the	O
normal	O
distribution	O
generalizes	O
to	O
r	O
n	O
,	O
in	O
which	O
case	O
it	O
is	O
known	O
as	O
the	O
multivariate	O
normal	O
distribution	O
.	O
it	O
may	O
be	O
parametrized	O
with	O
a	O
positive	O
deﬁnite	O
symmetric	O
matrix	O
:	O
σ	O
	O
	O
n	O
(	O
;	O
x	O
µ	O
,	O
σ	O
)	O
=	O
1	O
(	O
2	O
)	O
π	O
ndet	O
(	O
)	O
σ	O
exp	O
−	O
	O
)	O
(	O
x	O
µ	O
−	O
1	O
2	O
64	O
−	O
−	O
1	O
(	O
σ	O
)	O
x	O
µ	O
.	O
(	O
3.23	O
)	O
chapter	O
3.	O
probability	O
and	O
information	O
theory	O
the	O
parameter	O
µ	O
still	O
gives	O
the	O
mean	O
of	O
the	O
distribution	O
,	O
though	O
now	O
it	O
is	O
vector-valued	O
.	O
the	O
parameter	O
σ	O
gives	O
the	O
covariance	O
matrix	O
of	O
the	O
distribution	O
.	O
as	O
in	O
the	O
univariate	O
case	O
,	O
when	O
we	O
wish	O
to	O
evaluate	O
the	O
pdf	O
several	O
times	O
for	O
many	O
diﬀerent	O
values	O
of	O
the	O
parameters	O
,	O
the	O
covariance	O
is	O
not	O
a	O
computationally	O
eﬃcient	O
way	O
to	O
parametrize	O
the	O
distribution	O
,	O
since	O
we	O
need	O
to	O
invert	O
σ	O
to	O
evaluate	O
the	O
pdf	O
.	O
we	O
can	O
instead	O
use	O
a	O
precision	O
matrix	O
β	O
	O
	O
	O
:	O
n	O
−	O
1	O
)	O
=	O
(	O
;	O
x	O
µ	O
β	O
,	O
det	O
(	O
)	O
β	O
(	O
2	O
)	O
π	O
n	O
exp	O
−	O
	O
)	O
(	O
x	O
µ	O
−	O
1	O
2	O
−	O
β	O
x	O
µ	O
)	O
(	O
.	O
(	O
3.24	O
)	O
we	O
often	O
ﬁx	O
the	O
covariance	O
matrix	O
to	O
be	O
a	O
diagonal	O
matrix	O
.	O
an	O
even	O
simpler	O
version	O
is	O
the	O
isotropic	O
gaussian	O
distribution	O
,	O
whose	O
covariance	O
matrix	O
is	O
a	O
scalar	O
times	O
the	O
identity	O
matrix	O
.	O
3.9.4	O
exponential	O
and	O
laplace	O
distributions	O
in	O
the	O
context	O
of	O
deep	O
learning	O
,	O
we	O
often	O
want	O
to	O
have	O
a	O
probability	O
distribution	O
with	O
a	O
sharp	O
point	O
at	O
x	O
=	O
0.	O
to	O
accomplish	O
this	O
,	O
we	O
can	O
use	O
the	O
exponential	O
distribution	O
:	O
−	O
≥	O
λx	O
.	O
0	O
exp	O
(	O
)	O
(	O
;	O
)	O
=	O
1x	O
p	O
x	O
λ	O
λ	O
(	O
3.25	O
)	O
≥	O
the	O
exponential	O
distribution	O
uses	O
the	O
indicator	O
function	O
1x	O
0	O
to	O
assign	O
probability	O
zero	O
to	O
all	O
negative	O
values	O
of	O
.x	O
a	O
closely	O
related	O
probability	O
distribution	O
that	O
allows	O
us	O
to	O
place	O
a	O
sharp	O
peak	O
of	O
probability	O
mass	O
at	O
an	O
arbitrary	O
point	O
µ	O
is	O
the	O
laplace	O
distribution	O
	O
	O
−|	O
−	O
|	O
µ	O
x	O
laplace	O
(	O
;	O
x	O
µ	O
,	O
γ	O
)	O
=	O
1	O
2γ	O
exp	O
γ	O
.	O
(	O
3.26	O
)	O
3.9.5	O
the	O
dirac	O
distribution	O
and	O
empirical	O
distribution	O
in	O
some	O
cases	O
,	O
we	O
wish	O
to	O
specify	O
that	O
all	O
of	O
the	O
mass	O
in	O
a	O
probability	O
distribution	O
clusters	O
around	O
a	O
single	O
point	O
.	O
this	O
can	O
be	O
accomplished	O
by	O
deﬁning	O
a	O
pdf	O
using	O
the	O
dirac	O
delta	O
function	O
,	O
:	O
δ	O
x	O
(	O
)	O
−	O
p	O
x	O
(	O
)	O
=	O
(	O
δ	O
x	O
µ	O
.	O
)	O
(	O
3.27	O
)	O
the	O
dirac	O
delta	O
function	O
is	O
deﬁned	O
such	O
that	O
it	O
is	O
zero-valued	O
everywhere	O
except	O
0	O
,	O
yet	O
integrates	O
to	O
1.	O
the	O
dirac	O
delta	O
function	O
is	O
not	O
an	O
ordinary	O
function	O
that	O
associates	O
each	O
value	O
x	O
with	O
a	O
real-valued	O
output	O
,	O
instead	O
it	O
is	O
a	O
diﬀerent	O
kind	O
of	O
65	O
chapter	O
3.	O
probability	O
and	O
information	O
theory	O
	O
mathematical	O
object	O
called	O
a	O
generalized	O
function	O
that	O
is	O
deﬁned	O
in	O
terms	O
of	O
its	O
properties	O
when	O
integrated	O
.	O
we	O
can	O
think	O
of	O
the	O
dirac	O
delta	O
function	O
as	O
being	O
the	O
limit	O
point	O
of	O
a	O
series	O
of	O
functions	O
that	O
put	O
less	O
and	O
less	O
mass	O
on	O
all	O
points	O
other	O
than	O
zero	O
.	O
−	O
µ	O
we	O
obtain	O
an	O
inﬁnitely	O
narrow	O
and	O
by	O
deﬁning	O
p	O
(	O
x	O
)	O
to	O
be	O
δ	O
shifted	O
by	O
inﬁnitely	O
high	O
peak	O
of	O
probability	O
mass	O
where	O
x	O
.	O
µ=	O
a	O
common	O
use	O
of	O
the	O
dirac	O
delta	O
distribution	O
is	O
as	O
a	O
component	O
of	O
an	O
empirical	O
distribution	O
,	O
ˆp	O
(	O
)	O
=x	O
1	O
m	O
m	O
i=1	O
−	O
δ	O
(	O
x	O
x	O
(	O
)	O
i	O
)	O
(	O
3.28	O
)	O
on	O
each	O
of	O
the	O
m	O
points	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
which	O
puts	O
probability	O
mass	O
1	O
)	O
m	O
forming	O
a	O
m	O
given	O
dataset	O
or	O
collection	O
of	O
samples	O
.	O
the	O
dirac	O
delta	O
distribution	O
is	O
only	O
necessary	O
to	O
deﬁne	O
the	O
empirical	O
distribution	O
over	O
continuous	O
variables	O
.	O
for	O
discrete	O
variables	O
,	O
the	O
situation	O
is	O
simpler	O
:	O
an	O
empirical	O
distribution	O
can	O
be	O
conceptualized	O
as	O
a	O
multinoulli	O
distribution	O
,	O
with	O
a	O
probability	O
associated	O
to	O
each	O
possible	O
input	O
value	O
that	O
is	O
simply	O
equal	O
to	O
the	O
empirical	O
frequency	O
of	O
that	O
value	O
in	O
the	O
training	O
set	O
.	O
we	O
can	O
view	O
the	O
empirical	O
distribution	O
formed	O
from	O
a	O
dataset	O
of	O
training	O
examples	O
as	O
specifying	O
the	O
distribution	O
that	O
we	O
sample	O
from	O
when	O
we	O
train	O
a	O
model	B
on	O
this	O
dataset	O
.	O
another	O
important	O
perspective	O
on	O
the	O
empirical	O
distribution	O
is	O
that	O
it	O
is	O
the	O
probability	O
density	O
that	O
maximizes	O
the	O
likelihood	O
of	O
the	O
training	O
data	O
(	O
see	O
section	O
5.5	O
)	O
.	O
3.9.6	O
mixtures	O
of	O
distributions	O
it	O
is	O
also	O
common	O
to	O
deﬁne	O
probability	O
distributions	O
by	O
combining	O
other	O
simpler	O
probability	O
distributions	O
.	O
one	O
common	O
way	O
of	O
combining	O
distributions	O
is	O
to	O
construct	O
a	O
mixture	O
distribution	O
.	O
a	O
mixture	O
distribution	O
is	O
made	O
up	O
of	O
several	O
component	O
distributions	O
.	O
on	O
each	O
trial	O
,	O
the	O
choice	O
of	O
which	O
component	O
distribution	O
generates	O
the	O
sample	O
is	O
determined	O
by	O
sampling	O
a	O
component	O
identity	O
from	O
a	O
multinoulli	O
distribution	O
:	O
	O
|	O
x	O
c	O
c	O
(	O
=	O
i	O
p	O
)	O
(	O
i	O
=	O
)	O
(	O
3.29	O
)	O
p	O
(	O
)	O
=x	O
p	O
i	O
where	O
p	O
(	O
)	O
c	O
is	O
the	O
multinoulli	O
distribution	O
over	O
component	O
identities	O
.	O
we	O
have	O
already	O
seen	O
one	O
example	O
of	O
a	O
mixture	O
distribution	O
:	O
the	O
empirical	O
distribution	O
over	O
real-valued	O
variables	O
is	O
a	O
mixture	O
distribution	O
with	O
one	O
dirac	O
component	O
for	O
each	O
training	O
example	O
.	O
66	O
chapter	O
3.	O
probability	O
and	O
information	O
theory	O
the	O
mixture	O
model	B
is	O
one	O
simple	O
strategy	O
for	O
combining	O
probability	O
distributions	O
,	O
we	O
explore	O
the	O
art	O
of	O
building	O
complex	O
to	O
create	O
a	O
richer	O
distribution	O
.	O
in	O
chapter	O
probability	O
distributions	O
from	O
simple	O
ones	O
in	O
more	O
detail	O
.	O
16	O
the	O
mixture	O
model	B
allows	O
us	O
to	O
brieﬂy	O
glimpse	O
a	O
concept	O
that	O
will	O
be	O
of	O
paramount	O
importance	O
later—the	O
latent	O
variable	O
.	O
a	O
latent	O
variable	O
is	O
a	O
random	O
variable	O
that	O
we	O
can	O
not	O
observe	O
directly	O
.	O
the	O
component	O
identity	O
variable	O
c	O
of	O
the	O
|	O
mixture	O
model	B
provides	O
an	O
example	O
.	O
latent	O
variables	O
may	O
be	O
related	O
to	O
x	O
through	O
|	O
)	O
p	O
(	O
c	O
)	O
.	O
the	O
distribution	O
p	O
(	O
c	O
)	O
the	O
joint	O
distribution	O
,	O
in	O
this	O
case	O
,	O
p	O
(	O
x	O
c	O
,	O
)	O
=	O
p	O
(	O
x	O
c	O
over	O
the	O
latent	O
variable	O
and	O
the	O
distribution	O
p	O
(	O
x	O
c	O
)	O
relating	O
the	O
latent	O
variables	O
to	O
the	O
visible	O
variables	O
determines	O
the	O
shape	O
of	O
the	O
distribution	O
p	O
(	O
x	O
)	O
even	O
though	O
it	O
is	O
possible	O
to	O
describe	O
p	O
(	O
x	O
)	O
without	O
reference	O
to	O
the	O
latent	O
variable	O
.	O
latent	O
variables	O
are	O
discussed	O
further	O
in	O
section	O
.	O
16.5	O
|	O
a	O
very	O
powerful	O
and	O
common	O
type	O
of	O
mixture	O
model	B
is	O
the	O
gaussian	O
mixture	O
c	O
=	O
i	O
)	O
are	O
gaussians	O
.	O
each	O
component	O
has	O
model	B
,	O
in	O
which	O
the	O
components	O
p	O
(	O
x	O
a	O
separately	O
parametrized	O
mean	O
µ	O
(	O
)	O
i	O
and	O
covariance	O
σ	O
(	O
)	O
i	O
.	O
some	O
mixtures	O
can	O
have	O
more	O
constraints	O
.	O
for	O
example	O
,	O
the	O
covariances	O
could	O
be	O
shared	O
across	O
components	O
via	O
the	O
constraint	O
σ	O
(	O
)	O
i	O
=	O
σ	O
,	O
.	O
as	O
with	O
a	O
single	O
gaussian	O
distribution	O
,	O
the	O
mixture	O
of	O
gaussians	O
might	O
constrain	O
the	O
covariance	O
matrix	O
for	O
each	O
component	O
to	O
be	O
diagonal	O
or	O
isotropic	O
.	O
∀	O
i	O
in	O
addition	O
to	O
the	O
means	O
and	O
covariances	O
,	O
the	O
parameters	O
of	O
a	O
gaussian	O
mixture	O
specify	O
the	O
prior	O
probability	O
αi	O
=	O
p	O
(	O
c	O
=	O
i	O
)	O
given	O
to	O
each	O
component	O
i.	O
the	O
word	O
“	O
prior	O
”	O
indicates	O
that	O
it	O
expresses	O
the	O
model	B
’	O
s	O
beliefs	O
about	O
c	O
before	O
it	O
has	O
observed	O
x.	O
by	O
comparison	O
,	O
p	O
(	O
c	O
x	O
)	O
is	O
a	O
posterior	O
probability	O
,	O
because	O
it	O
is	O
computed	O
after	O
observation	O
of	O
x.	O
a	O
gaussian	O
mixture	O
model	B
is	O
a	O
universal	O
approximator	O
of	O
densities	O
,	O
in	O
the	O
sense	O
that	O
any	O
smooth	O
density	O
can	O
be	O
approximated	O
with	O
any	O
speciﬁc	O
,	O
non-zero	O
amount	O
of	O
error	O
by	O
a	O
gaussian	O
mixture	O
model	B
with	O
enough	O
components	O
.	O
|	O
figure	O
3.2	O
shows	O
samples	O
from	O
a	O
gaussian	O
mixture	O
model	B
.	O
3.10	O
useful	O
properties	O
of	O
common	O
functions	O
certain	O
functions	O
arise	O
often	O
while	O
working	O
with	O
probability	O
distributions	O
,	O
especially	O
the	O
probability	O
distributions	O
used	O
in	O
deep	O
learning	O
models	O
.	O
one	O
of	O
these	O
functions	O
is	O
the	O
:	O
logistic	O
sigmoid	O
σ	O
x	O
(	O
)	O
=	O
1	O
−	O
)	O
1	O
+	O
exp	O
(	O
x	O
.	O
(	O
3.30	O
)	O
the	O
logistic	O
sigmoid	O
is	O
commonly	O
used	O
to	O
produce	O
the	O
φ	O
parameter	O
of	O
a	O
bernoulli	O
67	O
chapter	O
3.	O
probability	O
and	O
information	O
theory	O
2	O
x	O
x1	O
figure	O
3.2	O
:	O
samples	O
from	O
a	O
gaussian	O
mixture	O
model	B
.	O
in	O
this	O
example	O
,	O
there	O
are	O
three	O
components	O
.	O
from	O
left	O
to	O
right	O
,	O
the	O
ﬁrst	O
component	O
has	O
an	O
isotropic	O
covariance	O
matrix	O
,	O
meaning	O
it	O
has	O
the	O
same	O
amount	O
of	O
variance	O
in	O
each	O
direction	O
.	O
the	O
second	O
has	O
a	O
diagonal	O
covariance	O
matrix	O
,	O
meaning	O
it	O
can	O
control	O
the	O
variance	O
separately	O
along	O
each	O
axis-aligned	O
direction	O
.	O
this	O
example	O
has	O
more	O
variance	O
along	O
the	O
x	O
2	O
axis	O
than	O
along	O
the	O
x1	O
axis	O
.	O
the	O
third	O
component	O
has	O
a	O
full-rank	O
covariance	O
matrix	O
,	O
allowing	O
it	O
to	O
control	O
the	O
variance	O
separately	O
along	O
an	O
arbitrary	O
basis	O
of	O
directions	O
.	O
distribution	O
because	O
its	O
range	O
is	O
(	O
0,1	O
)	O
,	O
which	O
lies	O
within	O
the	O
valid	O
range	O
of	O
values	O
for	O
the	O
φ	O
parameter	O
.	O
see	O
ﬁgure	O
for	O
a	O
graph	O
of	O
the	O
sigmoid	O
function	O
.	O
the	O
sigmoid	O
function	O
saturates	O
when	O
its	O
argument	O
is	O
very	O
positive	O
or	O
very	O
negative	O
,	O
meaning	O
that	O
the	O
function	O
becomes	O
very	O
ﬂat	O
and	O
insensitive	O
to	O
small	O
changes	O
in	O
its	O
input	O
.	O
3.3	O
another	O
commonly	O
encountered	O
function	O
is	O
the	O
softplus	O
function	O
(	O
dugas	O
et	O
al	O
.	O
,	O
2001	O
)	O
:	O
ζ	O
x	O
x	O
.	O
(	O
)	O
=	O
log	O
(	O
1	O
+	O
exp	O
(	O
)	O
)	O
(	O
3.31	O
)	O
the	O
softplus	O
function	O
can	O
be	O
useful	O
for	O
producing	O
the	O
β	O
or	O
σ	O
parameter	O
of	O
a	O
normal	O
distribution	O
because	O
its	O
range	O
is	O
(	O
0	O
,	O
)	O
.	O
it	O
also	O
arises	O
commonly	O
when	O
manipulating	O
expressions	O
involving	O
sigmoids	O
.	O
the	O
name	O
of	O
the	O
softplus	O
function	O
comes	O
from	O
the	O
fact	O
that	O
it	O
is	O
a	O
smoothed	O
or	O
“	O
softened	O
”	O
version	O
of	O
∞	O
x+	O
=	O
max	O
(	O
0	O
)	O
,	O
x	O
.	O
(	O
3.32	O
)	O
see	O
ﬁgure	O
3.4	O
for	O
a	O
graph	O
of	O
the	O
softplus	O
function	O
.	O
the	O
following	O
properties	O
are	O
all	O
useful	O
enough	O
that	O
you	O
may	O
wish	O
to	O
memorize	O
them	O
:	O
68	O
chapter	O
3.	O
probability	O
and	O
information	O
theory	O
−	O
5	O
0	O
x	O
5	O
10	O
figure	O
3.3	O
:	O
the	O
logistic	O
sigmoid	O
function.	O
)	O
x	O
(	O
σ	O
1	O
0	O
.	O
0	O
8	O
.	O
0	O
6	O
.	O
0	O
4	O
.	O
0	O
2	O
.	O
0	O
0	O
.	O
−	O
10	O
10	O
8	O
6	O
4	O
2	O
)	O
x	O
(	O
ζ	O
−	O
0	O
10	O
−	O
5	O
0	O
x	O
5	O
10	O
figure	O
3.4	O
:	O
the	O
softplus	O
function	O
.	O
69	O
chapter	O
3.	O
probability	O
and	O
information	O
theory	O
σ	O
x	O
(	O
)	O
=	O
exp	O
(	O
)	O
x	O
x	O
exp	O
(	O
)	O
+	O
exp	O
(	O
0	O
)	O
d	O
dx	O
σ	O
x	O
σ	O
x	O
(	O
)	O
=	O
(	O
)	O
(	O
1	O
−	O
−	O
−	O
(	O
)	O
=	O
(	O
)	O
σ	O
x	O
−	O
−	O
)	O
ζ	O
log	O
(	O
)	O
=	O
(	O
x	O
σ	O
x	O
σ	O
x	O
1	O
σ	O
x	O
(	O
)	O
)	O
	O
	O
d	O
dx	O
ζ	O
x	O
σ	O
x	O
(	O
)	O
=	O
(	O
)	O
	O
∀	O
∈	O
x	O
∀	O
x	O
>	O
,	O
ζ0	O
−	O
1	O
(	O
)	O
=	O
log	O
x	O
(	O
0	O
1	O
)	O
,	O
,	O
σ	O
−	O
1	O
(	O
)	O
=	O
log	O
(	O
exp	O
(	O
)	O
x	O
x	O
1	O
−	O
x	O
x	O
−	O
1	O
)	O
x	O
ζ	O
x	O
(	O
)	O
=	O
−	O
ζ	O
x	O
(	O
)	O
(	O
)	O
−∞	O
σ	O
y	O
dy	O
−	O
(	O
x	O
)	O
=	O
x	O
ζ	O
(	O
3.33	O
)	O
(	O
3.34	O
)	O
(	O
3.35	O
)	O
(	O
3.36	O
)	O
(	O
3.37	O
)	O
(	O
3.38	O
)	O
(	O
3.39	O
)	O
(	O
3.40	O
)	O
(	O
3.41	O
)	O
−	O
1	O
(	O
x	O
)	O
is	O
called	O
the	O
logit	O
in	O
statistics	O
,	O
but	O
this	O
term	O
is	O
more	O
rarely	O
the	O
function	O
σ	O
used	O
in	O
machine	O
learning	O
.	O
3.41	O
}	O
0	O
,	O
x	O
equation	O
provides	O
extra	O
justiﬁcation	O
for	O
the	O
name	O
“	O
softplus.	O
”	O
the	O
softplus	O
{	O
function	O
is	O
intended	O
as	O
a	O
smoothed	O
version	O
of	O
the	O
positive	O
part	O
function	O
,	O
x+	O
=	O
.	O
the	O
positive	O
part	O
function	O
is	O
the	O
counterpart	O
of	O
the	O
negative	O
part	O
max	O
−	O
−	O
function	O
,	O
x	O
.	O
to	O
obtain	O
a	O
smooth	O
function	O
that	O
is	O
analogous	O
to	O
the	O
x	O
)	O
.	O
just	O
as	O
x	O
can	O
be	O
recovered	O
from	O
its	O
positive	O
part	O
negative	O
part	O
,	O
one	O
can	O
use	O
ζ	O
(	O
and	O
negative	O
part	O
via	O
the	O
identity	O
x+	O
=	O
x	O
,	O
it	O
is	O
also	O
possible	O
to	O
recover	O
x	O
3.41	O
.	O
using	O
the	O
same	O
relationship	O
between	O
−	O
,	O
as	O
shown	O
in	O
equation	O
x	O
(	O
)	O
{	O
=	O
max	O
−	O
}	O
0	O
,	O
x	O
x	O
and	O
ζ	O
x	O
(	O
)	O
−	O
−	O
ζ	O
3.11	O
bayes	O
’	O
rule	O
|	O
|	O
we	O
often	O
ﬁnd	O
ourselves	O
in	O
a	O
situation	O
where	O
we	O
know	O
p	O
(	O
y	O
x	O
p	O
(	O
x	O
y	O
using	O
bayes	O
’	O
rule	O
:	O
)	O
and	O
need	O
to	O
know	O
)	O
.	O
fortunately	O
,	O
if	O
we	O
also	O
know	O
p	O
(	O
x	O
)	O
,	O
we	O
can	O
compute	O
the	O
desired	O
quantity	O
	O
|	O
x	O
y	O
p	O
(	O
)	O
=	O
|	O
y	O
x	O
(	O
)	O
p	O
p	O
(	O
)	O
x	O
p	O
(	O
)	O
y	O
.	O
(	O
3.42	O
)	O
note	O
that	O
while	O
p	O
(	O
y	O
)	O
appears	O
in	O
the	O
formula	O
,	O
it	O
is	O
usually	O
feasible	O
to	O
compute	O
p	O
(	O
)	O
=y	O
)	O
(	O
)	O
,	O
so	O
we	O
do	O
not	O
need	O
to	O
begin	O
with	O
knowledge	O
of	O
x	O
p	O
x	O
(	O
)	O
y	O
.	O
(	O
y	O
p	O
|	O
x	O
p	O
70	O
chapter	O
3.	O
probability	O
and	O
information	O
theory	O
bayes	O
’	O
rule	O
is	O
straightforward	O
to	O
derive	O
from	O
the	O
deﬁnition	O
of	O
conditional	O
probability	O
,	O
but	O
it	O
is	O
useful	O
to	O
know	O
the	O
name	O
of	O
this	O
formula	O
since	O
many	O
texts	O
refer	O
to	O
it	O
by	O
name	O
.	O
it	O
is	O
named	O
after	O
the	O
reverend	O
thomas	O
bayes	O
,	O
who	O
ﬁrst	O
discovered	O
a	O
special	O
case	O
of	O
the	O
formula	O
.	O
the	O
general	O
version	O
presented	O
here	O
was	O
independently	O
discovered	O
by	O
pierre-simon	O
laplace	O
.	O
3.12	O
technical	O
details	O
of	O
continuous	O
variables	O
a	O
proper	O
formal	O
understanding	O
of	O
continuous	O
random	O
variables	O
and	O
probability	O
density	O
functions	O
requires	O
developing	O
probability	O
theory	O
in	O
terms	O
of	O
a	O
branch	O
of	O
mathematics	O
known	O
as	O
measure	O
theory	O
.	O
measure	O
theory	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
textbook	O
,	O
but	O
we	O
can	O
brieﬂy	O
sketch	O
some	O
of	O
the	O
issues	O
that	O
measure	O
theory	O
is	O
employed	O
to	O
resolve	O
.	O
∈	O
∈	O
s1	O
)	O
+	O
p	O
(	O
x	O
in	O
section	O
3.3.2	O
,	O
we	O
saw	O
that	O
the	O
probability	O
of	O
a	O
continuous	O
vector-valued	O
x	O
lying	O
in	O
some	O
set	O
s	O
is	O
given	O
by	O
the	O
integral	O
of	O
p	O
(	O
x	O
)	O
over	O
the	O
set	O
s.	O
some	O
choices	O
of	O
set	O
s	O
can	O
produce	O
paradoxes	O
.	O
for	O
example	O
,	O
it	O
is	O
possible	O
to	O
construct	O
two	O
sets	O
s1	O
and	O
s2	O
such	O
that	O
p	O
(	O
x	O
.	O
these	O
sets	O
are	O
generally	O
constructed	O
making	O
very	O
heavy	O
use	O
of	O
the	O
inﬁnite	O
precision	O
of	O
real	O
numbers	O
,	O
for	O
example	O
by	O
making	O
fractal-shaped	O
sets	O
or	O
sets	O
that	O
are	O
deﬁned	O
by	O
transforming	O
the	O
set	O
of	O
rational	O
numbers.2	O
one	O
of	O
the	O
key	O
contributions	O
of	O
measure	O
theory	O
is	O
to	O
provide	O
a	O
characterization	O
of	O
the	O
set	O
of	O
sets	O
that	O
we	O
can	O
compute	O
the	O
probability	O
of	O
without	O
encountering	O
paradoxes	O
.	O
in	O
this	O
book	O
,	O
we	O
only	O
integrate	O
over	O
sets	O
with	O
relatively	O
simple	O
descriptions	O
,	O
so	O
this	O
aspect	O
of	O
measure	O
theory	O
never	O
becomes	O
a	O
relevant	O
concern	O
.	O
s2	O
)	O
>	O
1	O
but	O
s1	O
∩	O
s2	O
=	O
∅	O
for	O
our	O
purposes	O
,	O
measure	O
theory	O
is	O
more	O
useful	O
for	O
describing	O
theorems	O
that	O
n	O
but	O
do	O
not	O
apply	O
to	O
some	O
corner	O
cases	O
.	O
measure	O
theory	O
apply	O
to	O
most	O
points	O
in	O
r	O
provides	O
a	O
rigorous	O
way	O
of	O
describing	O
that	O
a	O
set	O
of	O
points	O
is	O
negligibly	O
small	O
.	O
such	O
a	O
set	O
is	O
said	O
to	O
have	O
measure	O
zero	O
.	O
we	O
do	O
not	O
formally	O
deﬁne	O
this	O
concept	O
in	O
this	O
textbook	O
.	O
for	O
our	O
purposes	O
,	O
it	O
is	O
suﬃcient	O
to	O
understand	O
the	O
intuition	O
that	O
a	O
set	O
of	O
measure	O
zero	O
occupies	O
no	O
volume	O
in	O
the	O
space	O
we	O
are	O
measuring	O
.	O
for	O
example	O
,	O
2	O
,	O
a	O
line	O
has	O
measure	O
zero	O
,	O
while	O
a	O
ﬁlled	O
polygon	O
has	O
positive	O
measure	O
.	O
within	O
r	O
likewise	O
,	O
an	O
individual	O
point	O
has	O
measure	O
zero	O
.	O
any	O
union	O
of	O
countably	O
many	O
sets	O
that	O
each	O
have	O
measure	O
zero	O
also	O
has	O
measure	O
zero	O
(	O
so	O
the	O
set	O
of	O
all	O
the	O
rational	O
numbers	O
has	O
measure	O
zero	O
,	O
for	O
instance	O
)	O
.	O
another	O
useful	O
term	O
from	O
measure	O
theory	O
is	O
almost	O
everywhere	O
.	O
a	O
property	O
that	O
holds	O
almost	O
everywhere	O
holds	O
throughout	O
all	O
of	O
space	O
except	O
for	O
on	O
a	O
set	O
of	O
2the	O
banach-tarski	O
theorem	O
provides	O
a	O
fun	O
example	O
of	O
such	O
sets	O
.	O
71	O
chapter	O
3.	O
probability	O
and	O
information	O
theory	O
measure	O
zero	O
.	O
because	O
the	O
exceptions	O
occupy	O
a	O
negligible	O
amount	O
of	O
space	O
,	O
they	O
can	O
be	O
safely	O
ignored	O
for	O
many	O
applications	O
.	O
some	O
important	O
results	O
in	O
probability	O
theory	O
hold	O
for	O
all	O
discrete	O
values	O
but	O
only	O
hold	O
“	O
almost	O
everywhere	O
”	O
for	O
continuous	O
values	O
.	O
another	O
technical	O
detail	O
of	O
continuous	O
variables	O
relates	O
to	O
handling	O
continuous	O
random	O
variables	O
that	O
are	O
deterministic	O
functions	O
of	O
one	O
another	O
.	O
suppose	O
we	O
have	O
two	O
random	O
variables	O
,	O
x	O
and	O
y	O
,	O
such	O
that	O
y	O
=	O
g	O
(	O
x	O
)	O
,	O
where	O
g	O
is	O
an	O
invertible	O
,	O
con-	O
−	O
1	O
(	O
y	O
)	O
)	O
.	O
tinuous	O
,	O
diﬀerentiable	O
transformation	O
.	O
one	O
might	O
expect	O
that	O
py	O
(	O
y	O
)	O
=	O
px	O
(	O
g	O
this	O
is	O
actually	O
not	O
the	O
case	O
.	O
∼	O
as	O
a	O
simple	O
example	O
,	O
suppose	O
we	O
have	O
scalar	O
random	O
variables	O
x	O
and	O
y.	O
suppose	O
if	O
we	O
use	O
the	O
rule	O
py	O
(	O
y	O
)	O
=	O
px	O
(	O
2y	O
)	O
then	O
py	O
will	O
be	O
0	O
on	O
this	O
interval	O
.	O
this	O
means	O
y	O
=	O
x	O
2	O
everywhere	O
except	O
the	O
interval	O
[	O
0	O
,	O
1	O
2	O
,	O
and	O
it	O
will	O
be	O
]	O
u	O
(	O
0	O
,	O
1	O
)	O
.	O
and	O
x	O
1	O
	O
py	O
(	O
)	O
=	O
y	O
dy	O
1	O
2	O
,	O
(	O
3.43	O
)	O
which	O
violates	O
the	O
deﬁnition	O
of	O
a	O
probability	O
distribution	O
.	O
this	O
is	O
a	O
common	O
mistake	O
.	O
the	O
problem	O
with	O
this	O
approach	O
is	O
that	O
it	O
fails	O
to	O
account	O
for	O
the	O
distortion	O
of	O
space	O
introduced	O
by	O
the	O
function	O
g.	O
recall	O
that	O
the	O
probability	O
of	O
x	O
lying	O
in	O
an	O
inﬁnitesimally	O
small	O
region	O
with	O
volume	O
δx	O
is	O
given	O
by	O
p	O
(	O
x	O
)	O
δx	O
.	O
since	O
g	O
can	O
expand	O
or	O
contract	O
space	O
,	O
the	O
inﬁnitesimal	O
volume	O
surrounding	O
x	O
in	O
x	O
space	O
may	O
have	O
diﬀerent	O
volume	O
in	O
space	O
.	O
y	O
to	O
see	O
how	O
to	O
correct	O
the	O
problem	O
,	O
we	O
return	O
to	O
the	O
scalar	O
case	O
.	O
we	O
need	O
to	O
preserve	O
the	O
property	O
|	O
py	O
(	O
(	O
)	O
)	O
g	O
x	O
dy	O
|	O
solving	O
from	O
this	O
,	O
we	O
obtain	O
py	O
(	O
)	O
=	O
y	O
px	O
(	O
g	O
or	O
equivalently	O
	O
|	O
|	O
px	O
(	O
)	O
x	O
dx	O
.	O
=	O
−	O
1	O
(	O
)	O
)	O
y	O
	O
∂x	O
∂y	O
	O
	O
	O
∂g	O
x	O
(	O
)	O
∂x	O
	O
.	O
	O
(	O
3.44	O
)	O
(	O
3.45	O
)	O
(	O
3.46	O
)	O
px	O
(	O
)	O
=	O
x	O
py	O
(	O
(	O
)	O
)	O
g	O
x	O
in	O
higher	O
dimensions	O
,	O
the	O
derivative	O
generalizes	O
to	O
the	O
determinant	O
of	O
the	O
jacobian	O
matrix—the	O
matrix	O
with	O
ji	O
,	O
j	O
=	O
∂xi	O
∂yj	O
.	O
thus	O
,	O
for	O
real-valued	O
vectors	O
and	O
,	O
y	O
x	O
px	O
(	O
)	O
=	O
x	O
py	O
(	O
(	O
)	O
)	O
g	O
x	O
det	O
∂g	O
(	O
)	O
x	O
∂x	O
.	O
(	O
3.47	O
)	O
72	O
chapter	O
3.	O
probability	O
and	O
information	O
theory	O
3.13	O
information	O
theory	O
information	O
theory	O
is	O
a	O
branch	O
of	O
applied	O
mathematics	O
that	O
revolves	O
around	O
quantifying	O
how	O
much	O
information	O
is	O
present	O
in	O
a	O
signal	O
.	O
it	O
was	O
originally	O
invented	O
to	O
study	O
sending	O
messages	O
from	O
discrete	O
alphabets	O
over	O
a	O
noisy	O
channel	O
,	O
such	O
as	O
communication	O
via	O
radio	O
transmission	O
.	O
in	O
this	O
context	O
,	O
information	O
theory	O
tells	O
how	O
to	O
design	O
optimal	O
codes	O
and	O
calculate	O
the	O
expected	O
length	O
of	O
messages	O
sampled	O
from	O
speciﬁc	O
probability	O
distributions	O
using	O
various	O
encoding	O
schemes	O
.	O
in	O
the	O
context	O
of	O
machine	O
learning	O
,	O
we	O
can	O
also	O
apply	O
information	O
theory	O
to	O
continuous	O
variables	O
where	O
some	O
of	O
these	O
message	O
length	O
interpretations	O
do	O
not	O
apply	O
.	O
this	O
ﬁeld	O
is	O
fundamental	O
to	O
many	O
areas	O
of	O
electrical	O
engineering	O
and	O
computer	O
science	O
.	O
in	O
this	O
textbook	O
,	O
we	O
mostly	O
use	O
a	O
few	O
key	O
ideas	O
from	O
information	O
theory	O
to	O
characterize	O
probability	O
distributions	O
or	O
quantify	O
similarity	O
between	O
probability	O
distributions	O
.	O
for	O
more	O
detail	O
on	O
information	O
theory	O
,	O
see	O
cover	O
and	O
thomas	O
2006	O
mackay	O
(	O
2003	O
)	O
or	O
)	O
.	O
(	O
the	O
basic	O
intuition	O
behind	O
information	O
theory	O
is	O
that	O
learning	O
that	O
an	O
unlikely	O
event	O
has	O
occurred	O
is	O
more	O
informative	O
than	O
learning	O
that	O
a	O
likely	O
event	O
has	O
occurred	O
.	O
a	O
message	O
saying	O
“	O
the	O
sun	O
rose	O
this	O
morning	O
”	O
is	O
so	O
uninformative	O
as	O
to	O
be	O
unnecessary	O
to	O
send	O
,	O
but	O
a	O
message	O
saying	O
“	O
there	O
was	O
a	O
solar	O
eclipse	O
this	O
morning	O
”	O
is	O
very	O
informative	O
.	O
we	O
would	O
like	O
to	O
quantify	O
information	O
in	O
a	O
way	O
that	O
formalizes	O
this	O
intuition	O
.	O
speciﬁcally	O
,	O
•	O
•	O
•	O
likely	O
events	O
should	O
have	O
low	O
information	O
content	O
,	O
and	O
in	O
the	O
extreme	O
case	O
,	O
events	O
that	O
are	O
guaranteed	O
to	O
happen	O
should	O
have	O
no	O
information	O
content	O
whatsoever	O
.	O
less	O
likely	O
events	O
should	O
have	O
higher	O
information	O
content	O
.	O
independent	O
events	O
should	O
have	O
additive	O
information	O
.	O
for	O
example	O
,	O
ﬁnding	O
out	O
that	O
a	O
tossed	O
coin	O
has	O
come	O
up	O
as	O
heads	O
twice	O
should	O
convey	O
twice	O
as	O
much	O
information	O
as	O
ﬁnding	O
out	O
that	O
a	O
tossed	O
coin	O
has	O
come	O
up	O
as	O
heads	O
once	O
.	O
in	O
order	O
to	O
satisfy	O
all	O
three	O
of	O
these	O
properties	O
,	O
we	O
deﬁne	O
the	O
self-information	O
of	O
an	O
event	O
x	O
=	O
x	O
to	O
be	O
−	O
(	O
)	O
=	O
i	O
x	O
log	O
p	O
x	O
.	O
(	O
)	O
(	O
3.48	O
)	O
in	O
this	O
book	O
,	O
we	O
always	O
use	O
log	O
to	O
mean	O
the	O
natural	O
logarithm	O
,	O
with	O
base	O
e.	O
our	O
deﬁnition	O
of	O
i	O
(	O
x	O
)	O
is	O
therefore	O
written	O
in	O
units	O
of	O
nats	O
.	O
one	O
nat	O
is	O
the	O
amount	O
of	O
73	O
chapter	O
3.	O
probability	O
and	O
information	O
theory	O
information	O
gained	O
by	O
observing	O
an	O
event	O
of	O
probability	O
1	O
.	O
other	O
texts	O
use	O
base-2	O
e	O
logarithms	O
and	O
units	O
called	O
bits	O
or	O
shannons	O
;	O
information	O
measured	O
in	O
bits	O
is	O
just	O
a	O
rescaling	O
of	O
information	O
measured	O
in	O
nats	O
.	O
when	O
x	O
is	O
continuous	O
,	O
we	O
use	O
the	O
same	O
deﬁnition	O
of	O
information	O
by	O
analogy	O
,	O
but	O
some	O
of	O
the	O
properties	O
from	O
the	O
discrete	O
case	O
are	O
lost	O
.	O
for	O
example	O
,	O
an	O
event	O
with	O
unit	O
density	O
still	O
has	O
zero	O
information	O
,	O
despite	O
not	O
being	O
an	O
event	O
that	O
is	O
guaranteed	O
to	O
occur	O
.	O
self-information	O
deals	O
only	O
with	O
a	O
single	O
outcome	O
.	O
we	O
can	O
quantify	O
the	O
amount	O
of	O
uncertainty	O
in	O
an	O
entire	O
probability	O
distribution	O
using	O
the	O
shannon	O
entropy	O
:	O
−	O
∼	O
p	O
[	O
(	O
)	O
]	O
=	O
ex	O
i	O
x	O
h	O
(	O
)	O
=	O
x	O
∼	O
p	O
[	O
log	O
(	O
)	O
]	O
p	O
x	O
.	O
ex	O
(	O
3.49	O
)	O
	O
	O
also	O
denoted	O
h	O
(	O
p	O
)	O
.	O
in	O
other	O
words	O
,	O
the	O
shannon	O
entropy	O
of	O
a	O
distribution	O
is	O
the	O
expected	O
amount	O
of	O
information	O
in	O
an	O
event	O
drawn	O
from	O
that	O
distribution	O
.	O
it	O
gives	O
a	O
lower	O
bound	B
on	O
the	O
number	O
of	O
bits	O
(	O
if	O
the	O
logarithm	O
is	O
base	O
2	O
,	O
otherwise	O
the	O
units	O
are	O
diﬀerent	O
)	O
needed	O
on	O
average	O
to	O
encode	O
symbols	O
drawn	O
from	O
a	O
distribution	O
p.	O
distributions	O
that	O
are	O
nearly	O
deterministic	O
(	O
where	O
the	O
outcome	O
is	O
nearly	O
certain	O
)	O
have	O
low	O
entropy	O
;	O
distributions	O
that	O
are	O
closer	O
to	O
uniform	O
have	O
high	O
entropy	O
.	O
see	O
ﬁgure	O
x	O
is	O
continuous	O
,	O
the	O
shannon	O
entropy	O
is	O
known	O
as	O
the	O
diﬀerential	O
entropy	O
.	O
for	O
a	O
demonstration	O
.	O
when	O
3.5	O
if	O
we	O
have	O
two	O
separate	O
probability	O
distributions	O
p	O
(	O
x	O
)	O
and	O
q	O
(	O
x	O
)	O
over	O
the	O
same	O
random	O
variable	O
x	O
,	O
we	O
can	O
measure	O
how	O
diﬀerent	O
these	O
two	O
distributions	O
are	O
using	O
the	O
kullback-leibler	O
(	O
kl	O
)	O
divergence	O
:	O
	O
p	O
q	O
d	O
kl	O
(	O
)	O
=	O
∼	O
ex	O
p	O
log	O
p	O
x	O
(	O
)	O
q	O
x	O
(	O
)	O
∼	O
p	O
[	O
log	O
(	O
)	O
p	O
x	O
=	O
e	O
x	O
−	O
log	O
(	O
)	O
]	O
q	O
x	O
.	O
(	O
3.50	O
)	O
in	O
the	O
case	O
of	O
discrete	O
variables	O
,	O
it	O
is	O
the	O
extra	O
amount	O
of	O
information	O
(	O
measured	O
logarithm	O
,	O
but	O
in	O
machine	O
learning	O
we	O
usually	O
use	O
nats	O
in	O
bits	O
if	O
we	O
use	O
the	O
base	O
and	O
the	O
natural	O
logarithm	O
)	O
needed	O
to	O
send	O
a	O
message	O
containing	O
symbols	O
drawn	O
from	O
probability	O
distribution	O
p	O
,	O
when	O
we	O
use	O
a	O
code	O
that	O
was	O
designed	O
to	O
minimize	O
the	O
length	O
of	O
messages	O
drawn	O
from	O
probability	O
distribution	O
.q	O
2	O
the	O
kl	O
divergence	O
has	O
many	O
useful	O
properties	O
,	O
most	O
notably	O
that	O
it	O
is	O
non-	O
negative	O
.	O
the	O
kl	O
divergence	O
is	O
0	O
if	O
and	O
only	O
if	O
p	O
and	O
q	O
are	O
the	O
same	O
distribution	O
in	O
the	O
case	O
of	O
discrete	O
variables	O
,	O
or	O
equal	O
“	O
almost	O
everywhere	O
”	O
in	O
the	O
case	O
of	O
continuous	O
variables	O
.	O
because	O
the	O
kl	O
divergence	O
is	O
non-negative	O
and	O
measures	O
the	O
diﬀerence	O
between	O
two	O
distributions	O
,	O
it	O
is	O
often	O
conceptualized	O
as	O
measuring	O
some	O
sort	O
of	O
	O
distance	O
between	O
these	O
distributions	O
.	O
however	O
,	O
it	O
is	O
not	O
a	O
true	O
distance	O
measure	O
because	O
it	O
is	O
not	O
symmetric	O
:	O
dkl	O
(	O
p	O
q	O
)	O
for	O
some	O
p	O
and	O
q.	O
this	O
	O
=	O
dkl	O
(	O
q	O
p	O
)	O
74	O
	O
chapter	O
3.	O
probability	O
and	O
information	O
theory	O
0	O
7	O
.	O
0	O
6	O
.	O
0	O
5	O
.	O
0	O
4	O
.	O
0	O
3	O
.	O
0	O
2	O
.	O
0	O
1.	O
s	O
t	O
a	O
n	O
n	O
i	O
y	O
p	O
o	O
r	O
t	O
n	O
e	O
n	O
o	O
n	O
n	O
a	O
h	O
s	O
0	O
0	O
.	O
0	O
0	O
.	O
0	O
2	O
.	O
0	O
4	O
.	O
0	O
6	O
.	O
0	O
8	O
.	O
1	O
0	O
.	O
p	O
1	O
figure	O
3.5	O
:	O
this	O
plot	O
shows	O
how	O
distributions	O
that	O
are	O
closer	O
to	O
deterministic	O
have	O
low	O
shannon	O
entropy	O
while	O
distributions	O
that	O
are	O
close	O
to	O
uniform	O
have	O
high	O
shannon	O
entropy	O
.	O
on	O
the	O
horizontal	O
axis	O
,	O
we	O
plot	O
p	O
,	O
the	O
probability	O
of	O
a	O
binary	O
random	O
variable	O
being	O
equal	O
to	O
.	O
the	O
entropy	O
is	O
given	O
by	O
log	O
.	O
when	O
p	O
is	O
near	O
0	O
,	O
the	O
distribution	O
is	O
nearly	O
deterministic	O
,	O
because	O
the	O
random	O
variable	O
is	O
nearly	O
always	O
0.	O
when	O
p	O
is	O
near	O
1	O
,	O
the	O
distribution	O
is	O
nearly	O
deterministic	O
,	O
because	O
the	O
random	O
variable	O
is	O
nearly	O
always	O
1.	O
when	O
p	O
=	O
0.5	O
,	O
the	O
entropy	O
is	O
maximal	O
,	O
because	O
the	O
distribution	O
is	O
uniform	O
over	O
the	O
two	O
outcomes	O
.	O
1	O
)	O
log	O
(	O
1	O
−	O
−	O
−	O
(	O
p	O
p	O
)	O
p	O
p	O
asymmetry	O
means	O
that	O
there	O
are	O
important	O
consequences	O
to	O
the	O
choice	O
of	O
whether	O
to	O
use	O
dkl	O
(	O
for	O
more	O
detail	O
.	O
.	O
see	O
ﬁgure	O
3.6	O
a	O
quantity	O
that	O
is	O
closely	O
related	O
to	O
the	O
kl	O
divergence	O
is	O
the	O
cross-entropy	O
)	O
,	O
which	O
is	O
similar	O
to	O
the	O
kl	O
divergence	O
but	O
lacking	O
	O
p	O
q	O
)	O
	O
)	O
q	O
p	O
	O
h	O
(	O
p	O
,	O
q	O
)	O
=	O
h	O
(	O
p	O
)	O
+	O
dkl	O
(	O
p	O
q	O
the	O
term	O
on	O
the	O
left	O
:	O
or	O
dkl	O
(	O
−	O
)	O
=	O
h	O
p	O
,	O
q	O
(	O
∼	O
p	O
log	O
(	O
)	O
q	O
x	O
.	O
ex	O
(	O
3.51	O
)	O
minimizing	O
the	O
cross-entropy	O
with	O
respect	O
to	O
q	O
is	O
equivalent	O
to	O
minimizing	O
the	O
kl	O
divergence	O
,	O
because	O
does	O
not	O
participate	O
in	O
the	O
omitted	O
term	O
.	O
q	O
when	O
computing	O
many	O
of	O
these	O
quantities	O
,	O
it	O
is	O
common	O
to	O
encounter	O
expres-	O
sions	O
of	O
the	O
form	O
0	O
log	O
0.	O
by	O
convention	O
,	O
in	O
the	O
context	O
of	O
information	O
theory	O
,	O
we	O
treat	O
these	O
expressions	O
as	O
limx	O
log	O
=	O
0	O
.	O
0	O
x	O
→	O
x	O
3.14	O
structured	O
probabilistic	O
models	O
machine	O
learning	O
algorithms	O
often	O
involve	O
probability	O
distributions	O
over	O
a	O
very	O
large	O
number	O
of	O
random	O
variables	O
.	O
often	O
,	O
these	O
probability	O
distributions	O
involve	O
direct	O
interactions	O
between	O
relatively	O
few	O
variables	O
.	O
using	O
a	O
single	O
function	O
to	O
75	O
chapter	O
3.	O
probability	O
and	O
information	O
theory	O
∗	O
q	O
=	O
argminqdkl	O
(	O
	O
)	O
p	O
q	O
∗	O
q	O
=	O
argminqdkl	O
(	O
	O
q	O
p	O
)	O
y	O
t	O
i	O
s	O
n	O
e	O
d	O
y	O
t	O
i	O
l	O
i	O
b	O
a	O
b	O
o	O
r	O
p	O
p	O
x	O
(	O
)	O
∗	O
(	O
)	O
x	O
q	O
y	O
t	O
i	O
s	O
n	O
e	O
d	O
y	O
t	O
i	O
l	O
i	O
b	O
a	O
b	O
o	O
r	O
p	O
p	O
(	O
)	O
x	O
∗	O
q	O
(	O
)	O
x	O
x	O
x	O
	O
)	O
or	O
dkl	O
(	O
q	O
p	O
figure	O
3.6	O
:	O
the	O
kl	O
divergence	O
is	O
asymmetric	O
.	O
suppose	O
we	O
have	O
a	O
distribution	O
p	O
(	O
x	O
)	O
and	O
	O
wish	O
to	O
approximate	O
it	O
with	O
another	O
distribution	O
q	O
(	O
x	O
)	O
.	O
we	O
have	O
the	O
choice	O
of	O
minimizing	O
either	O
dkl	O
(	O
p	O
q	O
)	O
.	O
we	O
illustrate	O
the	O
eﬀect	O
of	O
this	O
choice	O
using	O
a	O
mixture	O
of	O
two	O
gaussians	O
for	O
p	O
,	O
and	O
a	O
single	O
gaussian	O
for	O
q.	O
the	O
choice	O
of	O
which	O
direction	O
of	O
the	O
kl	O
divergence	O
to	O
use	O
is	O
problem-dependent	O
.	O
some	O
applications	O
require	O
an	O
approximation	O
that	O
usually	O
places	O
high	O
probability	O
anywhere	O
that	O
the	O
true	O
distribution	O
places	O
high	O
probability	O
,	O
while	O
other	O
applications	O
require	O
an	O
approximation	O
that	O
rarely	O
places	O
high	O
probability	O
anywhere	O
that	O
the	O
true	O
distribution	O
places	O
low	O
probability	O
.	O
the	O
choice	O
of	O
the	O
	O
direction	O
of	O
the	O
kl	O
divergence	O
reﬂects	O
which	O
of	O
these	O
considerations	O
takes	O
priority	O
for	O
each	O
application	O
.	O
(	O
left	O
)	O
the	O
eﬀect	O
of	O
minimizing	O
dkl	O
(	O
p	O
q	O
)	O
.	O
in	O
this	O
case	O
,	O
we	O
select	O
a	O
q	O
that	O
has	O
high	O
probability	O
where	O
p	O
has	O
high	O
probability	O
.	O
when	O
p	O
has	O
multiple	O
modes	O
,	O
q	O
chooses	O
to	O
	O
blur	O
the	O
modes	O
together	O
,	O
in	O
order	O
to	O
put	O
high	O
probability	O
mass	O
on	O
all	O
of	O
them	O
.	O
(	O
right	O
)	O
the	O
eﬀect	O
of	O
minimizing	O
dkl	O
(	O
q	O
p	O
)	O
.	O
in	O
this	O
case	O
,	O
we	O
select	O
a	O
q	O
that	O
has	O
low	O
probability	O
where	O
p	O
has	O
low	O
probability	O
.	O
when	O
p	O
has	O
multiple	O
modes	O
that	O
are	O
suﬃciently	O
widely	O
separated	O
,	O
as	O
in	O
this	O
ﬁgure	O
,	O
the	O
kl	O
divergence	O
is	O
minimized	O
by	O
choosing	O
a	O
single	O
mode	O
,	O
in	O
order	O
to	O
avoid	O
putting	O
probability	O
mass	O
in	O
the	O
low-probability	O
areas	O
between	O
modes	O
of	O
p.	O
here	O
,	O
we	O
illustrate	O
the	O
outcome	O
when	O
q	O
is	O
chosen	O
to	O
emphasize	O
the	O
left	O
mode	O
.	O
we	O
could	O
also	O
have	O
achieved	O
an	O
equal	O
value	O
of	O
the	O
kl	O
divergence	O
by	O
choosing	O
the	O
right	O
mode	O
.	O
if	O
the	O
modes	O
are	O
not	O
separated	O
by	O
a	O
suﬃciently	O
strong	O
low	O
probability	O
region	O
,	O
then	O
this	O
direction	O
of	O
the	O
kl	O
divergence	O
can	O
still	O
choose	O
to	O
blur	O
the	O
modes	O
.	O
76	O
chapter	O
3.	O
probability	O
and	O
information	O
theory	O
describe	O
the	O
entire	O
joint	O
probability	O
distribution	O
can	O
be	O
very	O
ineﬃcient	O
(	O
both	O
computationally	O
and	O
statistically	O
)	O
.	O
instead	O
of	O
using	O
a	O
single	O
function	O
to	O
represent	O
a	O
probability	O
distribution	O
,	O
we	O
can	O
split	O
a	O
probability	O
distribution	O
into	O
many	O
factors	O
that	O
we	O
multiply	O
together	O
.	O
for	O
example	O
,	O
suppose	O
we	O
have	O
three	O
random	O
variables	O
:	O
a	O
,	O
b	O
and	O
c.	O
suppose	O
that	O
a	O
inﬂuences	O
the	O
value	O
of	O
b	O
and	O
b	O
inﬂuences	O
the	O
value	O
of	O
c	O
,	O
but	O
that	O
a	O
and	O
c	O
are	O
independent	O
given	O
b.	O
we	O
can	O
represent	O
the	O
probability	O
distribution	O
over	O
all	O
three	O
variables	O
as	O
a	O
product	O
of	O
probability	O
distributions	O
over	O
two	O
variables	O
:	O
p	O
,	O
(	O
a	O
b	O
c	O
)	O
=	O
(	O
)	O
a	O
(	O
p	O
|	O
|	O
c	O
b	O
b	O
a	O
(	O
.	O
)	O
p	O
)	O
,	O
p	O
(	O
3.52	O
)	O
these	O
factorizations	O
can	O
greatly	O
reduce	O
the	O
number	O
of	O
parameters	O
needed	O
to	O
describe	O
the	O
distribution	O
.	O
each	O
factor	O
uses	O
a	O
number	O
of	O
parameters	O
that	O
is	O
exponential	O
in	O
the	O
number	O
of	O
variables	O
in	O
the	O
factor	O
.	O
this	O
means	O
that	O
we	O
can	O
greatly	O
reduce	O
the	O
cost	O
of	O
representing	O
a	O
distribution	O
if	O
we	O
are	O
able	O
to	O
ﬁnd	O
a	O
factorization	O
into	O
distributions	O
over	O
fewer	O
variables	O
.	O
we	O
can	O
describe	O
these	O
kinds	O
of	O
factorizations	O
using	O
graphs	O
.	O
here	O
we	O
use	O
the	O
word	O
“	O
graph	O
”	O
in	O
the	O
sense	O
of	O
graph	O
theory	O
:	O
a	O
set	O
of	O
vertices	O
that	O
may	O
be	O
connected	O
to	O
each	O
other	O
with	O
edges	O
.	O
when	O
we	O
represent	O
the	O
factorization	O
of	O
a	O
probability	O
distribution	O
with	O
a	O
graph	O
,	O
we	O
call	O
it	O
a	O
structured	O
probabilistic	O
model	B
or	O
graphical	O
model	B
.	O
there	O
are	O
two	O
main	O
kinds	O
of	O
structured	O
probabilistic	O
models	O
:	O
directed	O
and	O
undirected	O
.	O
both	O
kinds	O
of	O
graphical	O
models	O
use	O
a	O
graph	O
in	O
which	O
each	O
node	O
in	O
the	O
graph	O
corresponds	O
to	O
a	O
random	O
variable	O
,	O
and	O
an	O
edge	O
connecting	O
two	O
random	O
variables	O
means	O
that	O
the	O
probability	O
distribution	O
is	O
able	O
to	O
represent	O
direct	O
interactions	O
between	O
those	O
two	O
random	O
variables	O
.	O
g	O
directed	O
models	O
use	O
graphs	O
with	O
directed	O
edges	O
,	O
and	O
they	O
represent	O
fac-	O
torizations	O
into	O
conditional	O
probability	O
distributions	O
,	O
as	O
in	O
the	O
example	O
above	O
.	O
speciﬁcally	O
,	O
a	O
directed	O
model	B
contains	O
one	O
factor	O
for	O
every	O
random	O
variable	O
xi	O
in	O
the	O
distribution	O
,	O
and	O
that	O
factor	O
consists	O
of	O
the	O
conditional	O
distribution	O
over	O
xi	O
given	O
the	O
parents	O
of	O
xi	O
,	O
denoted	O
p	O
ag	O
(	O
xi	O
)	O
:	O
	O
|	O
p	O
(	O
)	O
=x	O
p	O
(	O
xi	O
i	O
p	O
ag	O
(	O
xi	O
)	O
)	O
.	O
(	O
3.53	O
)	O
see	O
ﬁgure	O
distributions	O
it	O
represents	O
.	O
3.7	O
for	O
an	O
example	O
of	O
a	O
directed	O
graph	O
and	O
the	O
factorization	O
of	O
probability	O
undirected	O
models	O
use	O
graphs	O
with	O
undirected	O
edges	O
,	O
and	O
they	O
represent	O
factorizations	O
into	O
a	O
set	O
of	O
functions	O
;	O
unlike	O
in	O
the	O
directed	O
case	O
,	O
these	O
functions	O
77	O
chapter	O
3.	O
probability	O
and	O
information	O
theory	O
aa	O
bb	O
dd	O
cc	O
ee	O
figure	O
3.7	O
:	O
a	O
directed	O
graphical	O
model	B
over	O
random	O
variables	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
and	O
e.	O
this	O
graph	O
corresponds	O
to	O
probability	O
distributions	O
that	O
can	O
be	O
factored	O
as	O
(	O
3.54	O
)	O
(	O
a	O
b	O
c	O
d	O
e	O
)	O
=	O
(	O
)	O
a	O
(	O
p	O
,	O
p	O
p	O
,	O
,	O
,	O
|	O
b	O
a	O
|	O
(	O
c	O
a	O
p	O
)	O
|	O
|	O
e	O
c	O
d	O
b	O
(	O
.	O
)	O
p	O
)	O
,	O
b	O
)	O
(	O
p	O
this	O
graph	O
allows	O
us	O
to	O
quickly	O
see	O
some	O
properties	O
of	O
the	O
distribution	O
.	O
for	O
example	O
,	O
a	O
and	O
c	O
interact	O
directly	O
,	O
but	O
a	O
and	O
e	O
interact	O
only	O
indirectly	O
via	O
c.	O
g	O
are	O
usually	O
not	O
probability	O
distributions	O
of	O
any	O
kind	O
.	O
any	O
set	O
of	O
nodes	O
that	O
are	O
all	O
c	O
connected	O
to	O
each	O
other	O
in	O
in	O
an	O
undirected	O
model	B
is	O
associated	O
with	O
a	O
factor	O
φ	O
(	O
)	O
i	O
(	O
(	O
)	O
i	O
)	O
.	O
these	O
factors	O
are	O
just	O
functions	O
,	O
not	O
probability	O
distributions	O
.	O
the	O
output	O
of	O
each	O
factor	O
must	O
be	O
non-negative	O
,	O
but	O
there	O
is	O
no	O
constraint	O
that	O
the	O
factor	O
must	O
sum	O
or	O
integrate	O
to	O
1	O
like	O
a	O
probability	O
distribution	O
.	O
is	O
called	O
a	O
clique	O
.	O
each	O
clique	O
(	O
)	O
i	O
c	O
the	O
probability	O
of	O
a	O
conﬁguration	O
of	O
random	O
variables	O
is	O
proportional	O
to	O
the	O
product	O
of	O
all	O
of	O
these	O
factors—assignments	O
that	O
result	O
in	O
larger	O
factor	O
values	O
are	O
more	O
likely	O
.	O
of	O
course	O
,	O
there	O
is	O
no	O
guarantee	O
that	O
this	O
product	O
will	O
sum	O
to	O
1.	O
we	O
therefore	O
divide	O
by	O
a	O
normalizing	O
constant	O
z	O
,	O
deﬁned	O
to	O
be	O
the	O
sum	O
or	O
integral	O
over	O
all	O
states	O
of	O
the	O
product	O
of	O
the	O
φ	O
functions	O
,	O
in	O
order	O
to	O
obtain	O
a	O
normalized	O
probability	O
distribution	O
:	O
	O
	O
	O
c	O
(	O
)	O
i	O
.	O
φ	O
(	O
)	O
i	O
p	O
(	O
)	O
=x	O
1	O
z	O
i	O
(	O
3.55	O
)	O
see	O
ﬁgure	O
probability	O
distributions	O
it	O
represents	O
.	O
3.8	O
for	O
an	O
example	O
of	O
an	O
undirected	O
graph	O
and	O
the	O
factorization	O
of	O
keep	O
in	O
mind	O
that	O
these	O
graphical	O
representations	O
of	O
factorizations	O
are	O
a	O
language	O
for	O
describing	O
probability	O
distributions	O
.	O
they	O
are	O
not	O
mutually	O
exclusive	O
families	O
of	O
probability	O
distributions	O
.	O
being	O
directed	O
or	O
undirected	O
is	O
not	O
a	O
property	O
of	O
a	O
probability	O
distribution	O
;	O
it	O
is	O
a	O
property	O
of	O
a	O
particular	O
description	O
of	O
a	O
78	O
chapter	O
3.	O
probability	O
and	O
information	O
theory	O
aa	O
bb	O
dd	O
cc	O
ee	O
figure	O
3.8	O
:	O
an	O
undirected	O
graphical	O
model	B
over	O
random	O
variables	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
and	O
e.	O
this	O
graph	O
corresponds	O
to	O
probability	O
distributions	O
that	O
can	O
be	O
factored	O
as	O
(	O
a	O
b	O
c	O
d	O
e	O
)	O
=	O
p	O
,	O
,	O
,	O
,	O
1	O
z	O
φ	O
(	O
1	O
)	O
(	O
a	O
b	O
c	O
,	O
)	O
,	O
φ	O
(	O
2	O
)	O
(	O
)	O
b	O
d	O
,	O
φ	O
(	O
3	O
)	O
(	O
)	O
c	O
e	O
,	O
.	O
(	O
3.56	O
)	O
this	O
graph	O
allows	O
us	O
to	O
quickly	O
see	O
some	O
properties	O
of	O
the	O
distribution	O
.	O
for	O
example	O
,	O
a	O
and	O
c	O
interact	O
directly	O
,	O
but	O
a	O
and	O
e	O
interact	O
only	O
indirectly	O
via	O
c.	O
probability	O
distribution	O
,	O
but	O
any	O
probability	O
distribution	O
may	O
be	O
described	O
in	O
both	O
ways	O
.	O
i	O
ii	O
and	O
throughout	O
parts	O
of	O
this	O
book	O
,	O
we	O
will	O
use	O
structured	O
probabilistic	O
models	O
merely	O
as	O
a	O
language	O
to	O
describe	O
which	O
direct	O
probabilistic	O
relationships	O
diﬀerent	O
machine	O
learning	O
algorithms	O
choose	O
to	O
represent	O
.	O
no	O
further	O
understanding	O
of	O
structured	O
probabilistic	O
models	O
is	O
needed	O
until	O
the	O
discussion	O
of	O
research	O
topics	O
,	O
in	O
part	O
,	O
where	O
we	O
will	O
explore	O
structured	O
probabilistic	O
models	O
in	O
much	O
greater	O
detail	O
.	O
iii	O
this	O
chapter	O
has	O
reviewed	O
the	O
basic	O
concepts	O
of	O
probability	O
theory	O
that	O
are	O
most	O
relevant	O
to	O
deep	O
learning	O
.	O
one	O
more	O
set	O
of	O
fundamental	O
mathematical	O
tools	O
remains	O
:	O
numerical	O
methods	O
.	O
79	O
chapter	O
4	O
numerical	O
computation	O
machine	O
learning	O
algorithms	O
usually	O
require	O
a	O
high	O
amount	O
of	O
numerical	O
compu-	O
tation	O
.	O
this	O
typically	O
refers	O
to	O
algorithms	O
that	O
solve	O
mathematical	O
problems	O
by	O
methods	O
that	O
update	O
estimates	O
of	O
the	O
solution	O
via	O
an	O
iterative	O
process	O
,	O
rather	O
than	O
analytically	O
deriving	O
a	O
formula	O
providing	O
a	O
symbolic	O
expression	O
for	O
the	O
correct	O
so-	O
lution	O
.	O
common	O
operations	O
include	O
optimization	O
(	O
ﬁnding	O
the	O
value	O
of	O
an	O
argument	O
that	O
minimizes	O
or	O
maximizes	O
a	O
function	O
)	O
and	O
solving	O
systems	O
of	O
linear	O
equations	O
.	O
even	O
just	O
evaluating	O
a	O
mathematical	O
function	O
on	O
a	O
digital	O
computer	O
can	O
be	O
diﬃcult	O
when	O
the	O
function	O
involves	O
real	O
numbers	O
,	O
which	O
can	O
not	O
be	O
represented	O
precisely	O
using	O
a	O
ﬁnite	O
amount	O
of	O
memory	O
.	O
4.1	O
overﬂow	O
and	O
underﬂow	O
the	O
fundamental	O
diﬃculty	O
in	O
performing	O
continuous	O
math	O
on	O
a	O
digital	O
computer	O
is	O
that	O
we	O
need	O
to	O
represent	O
inﬁnitely	O
many	O
real	O
numbers	O
with	O
a	O
ﬁnite	O
number	O
of	O
bit	O
patterns	O
.	O
this	O
means	O
that	O
for	O
almost	O
all	O
real	O
numbers	O
,	O
we	O
incur	O
some	O
approximation	O
error	O
when	O
we	O
represent	O
the	O
number	O
in	O
the	O
computer	O
.	O
in	O
many	O
cases	O
,	O
this	O
is	O
just	O
rounding	O
error	O
.	O
rounding	O
error	O
is	O
problematic	O
,	O
especially	O
when	O
it	O
compounds	O
across	O
many	O
operations	O
,	O
and	O
can	O
cause	O
algorithms	O
that	O
work	B
in	O
theory	O
to	O
fail	O
in	O
practice	O
if	O
they	O
are	O
not	O
designed	O
to	O
minimize	O
the	O
accumulation	O
of	O
rounding	O
error	O
.	O
one	O
form	O
of	O
rounding	O
error	O
that	O
is	O
particularly	O
devastating	O
is	O
underﬂow	O
.	O
underﬂow	O
occurs	O
when	O
numbers	O
near	O
zero	O
are	O
rounded	O
to	O
zero	O
.	O
many	O
functions	O
behave	O
qualitatively	O
diﬀerently	O
when	O
their	O
argument	O
is	O
zero	O
rather	O
than	O
a	O
small	O
positive	O
number	O
.	O
for	O
example	O
,	O
we	O
usually	O
want	O
to	O
avoid	O
division	O
by	O
zero	O
(	O
some	O
80	O
chapter	O
4.	O
numerical	O
computation	O
software	O
environments	O
will	O
raise	O
exceptions	O
when	O
this	O
occurs	O
,	O
others	O
will	O
return	O
a	O
result	O
with	O
a	O
placeholder	O
not-a-number	O
value	O
)	O
or	O
taking	O
the	O
logarithm	O
of	O
zero	O
(	O
this	O
is	O
usually	O
treated	O
as	O
,	O
which	O
then	O
becomes	O
not-a-number	O
if	O
it	O
is	O
used	O
for	O
many	O
further	O
arithmetic	O
operations	O
)	O
.	O
−∞	O
another	O
highly	O
damaging	O
form	O
of	O
numerical	O
error	O
is	O
overﬂow	O
.	O
overﬂow	O
occurs	O
.	O
further	O
when	O
numbers	O
with	O
large	O
magnitude	O
are	O
approximated	O
as	O
arithmetic	O
will	O
usually	O
change	O
these	O
inﬁnite	O
values	O
into	O
not-a-number	O
values	O
.	O
or	O
∞	O
−∞	O
one	O
example	O
of	O
a	O
function	O
that	O
must	O
be	O
stabilized	O
against	O
underﬂow	O
and	O
overﬂow	O
is	O
the	O
softmax	O
function	O
.	O
the	O
softmax	O
function	O
is	O
often	O
used	O
to	O
predict	O
the	O
probabilities	O
associated	O
with	O
a	O
multinoulli	O
distribution	O
.	O
the	O
softmax	O
function	O
is	O
deﬁned	O
to	O
be	O
	O
softmax	O
(	O
)	O
x	O
i	O
=	O
.	O
(	O
4.1	O
)	O
exp	O
(	O
xi	O
)	O
n	O
j=1	O
exp	O
(	O
xj	O
)	O
consider	O
what	O
happens	O
when	O
all	O
of	O
the	O
xi	O
are	O
equal	O
to	O
some	O
constant	O
c.	O
analytically	O
,	O
we	O
can	O
see	O
that	O
all	O
of	O
the	O
outputs	O
should	O
be	O
equal	O
to	O
1	O
.	O
numerically	O
,	O
this	O
may	O
n	O
not	O
occur	O
when	O
c	O
has	O
large	O
magnitude	O
.	O
if	O
c	O
is	O
very	O
negative	O
,	O
then	O
exp	O
(	O
c	O
)	O
will	O
underﬂow	O
.	O
this	O
means	O
the	O
denominator	O
of	O
the	O
softmax	O
will	O
become	O
0	O
,	O
so	O
the	O
ﬁnal	O
result	O
is	O
undeﬁned	O
.	O
when	O
c	O
is	O
very	O
large	O
and	O
positive	O
,	O
exp	O
(	O
c	O
)	O
will	O
overﬂow	O
,	O
again	O
resulting	O
in	O
the	O
expression	O
as	O
a	O
whole	O
being	O
undeﬁned	O
.	O
both	O
of	O
these	O
diﬃculties	O
can	O
be	O
resolved	O
by	O
instead	O
evaluating	O
softmax	O
(	O
z	O
)	O
where	O
z	O
=	O
x	O
maxi	O
xi	O
.	O
simple	O
algebra	O
shows	O
that	O
the	O
value	O
of	O
the	O
softmax	O
function	O
is	O
not	O
changed	O
analytically	O
by	O
adding	O
or	O
subtracting	O
a	O
scalar	O
from	O
the	O
input	O
vector	O
.	O
subtracting	O
maxi	O
xi	O
results	O
in	O
the	O
largest	O
argument	O
to	O
exp	O
being	O
0	O
,	O
which	O
rules	O
out	O
the	O
possibility	O
of	O
overﬂow	O
.	O
likewise	O
,	O
at	O
least	O
one	O
term	O
in	O
the	O
denominator	O
has	O
a	O
value	O
of	O
1	O
,	O
which	O
rules	O
out	O
the	O
possibility	O
of	O
underﬂow	O
in	O
the	O
denominator	O
leading	O
to	O
a	O
division	O
by	O
zero	O
.	O
−	O
there	O
is	O
still	O
one	O
small	O
problem	O
.	O
underﬂow	O
in	O
the	O
numerator	O
can	O
still	O
cause	O
the	O
expression	O
as	O
a	O
whole	O
to	O
evaluate	O
to	O
zero	O
.	O
this	O
means	O
that	O
if	O
we	O
implement	O
log	O
softmax	O
(	O
x	O
)	O
by	O
ﬁrst	O
running	O
the	O
softmax	O
subroutine	O
then	O
passing	O
the	O
result	O
to	O
the	O
log	O
function	O
,	O
we	O
could	O
erroneously	O
obtain	O
.	O
instead	O
,	O
we	O
must	O
implement	O
a	O
separate	O
function	O
that	O
calculates	O
log	O
softmax	O
in	O
a	O
numerically	O
stable	O
way	O
.	O
the	O
log	O
softmax	O
function	O
can	O
be	O
stabilized	O
using	O
the	O
same	O
trick	B
as	O
we	O
used	O
to	O
stabilize	O
the	O
function	O
.	O
softmax	O
−∞	O
for	O
the	O
most	O
part	O
,	O
we	O
do	O
not	O
explicitly	O
detail	O
all	O
of	O
the	O
numerical	O
considerations	O
involved	O
in	O
implementing	O
the	O
various	O
algorithms	O
described	O
in	O
this	O
book	O
.	O
developers	O
of	O
low-level	O
libraries	O
should	O
keep	O
numerical	O
issues	O
in	O
mind	O
when	O
implementing	O
deep	O
learning	O
algorithms	O
.	O
most	O
readers	O
of	O
this	O
book	O
can	O
simply	O
rely	O
on	O
low-	O
level	O
libraries	O
that	O
provide	O
stable	O
implementations	O
.	O
in	O
some	O
cases	O
,	O
it	O
is	O
possible	O
to	O
implement	O
a	O
new	O
algorithm	O
and	O
have	O
the	O
new	O
implementation	O
automatically	O
81	O
chapter	O
4.	O
numerical	O
computation	O
)	O
is	O
an	O
example	O
stabilized	O
.	O
theano	O
(	O
of	O
a	O
software	O
package	O
that	O
automatically	O
detects	O
and	O
stabilizes	O
many	O
common	O
numerically	O
unstable	O
expressions	O
that	O
arise	O
in	O
the	O
context	O
of	O
deep	O
learning	O
.	O
bergstra	O
et	O
al	O
.	O
2010	O
bastien	O
et	O
al	O
.	O
2012	O
,	O
;	O
,	O
4.2	O
poor	O
conditioning	O
conditioning	O
refers	O
to	O
how	O
rapidly	O
a	O
function	O
changes	O
with	O
respect	O
to	O
small	O
changes	O
in	O
its	O
inputs	O
.	O
functions	O
that	O
change	O
rapidly	O
when	O
their	O
inputs	O
are	O
perturbed	O
slightly	O
can	O
be	O
problematic	O
for	O
scientiﬁc	O
computation	O
because	O
rounding	O
errors	O
in	O
the	O
inputs	O
can	O
result	O
in	O
large	O
changes	O
in	O
the	O
output	O
.	O
	O
	O
max	O
i	O
,	O
j	O
λi	O
λ	O
j	O
.	O
consider	O
the	O
function	O
f	O
(	O
x	O
)	O
=	O
a	O
decomposition	O
,	O
its	O
condition	O
number	O
is	O
−	O
1x	O
.	O
when	O
a	O
∈	O
×	O
n	O
n	O
r	O
has	O
an	O
eigenvalue	O
(	O
4.2	O
)	O
this	O
is	O
the	O
ratio	O
of	O
the	O
magnitude	O
of	O
the	O
largest	O
and	O
smallest	O
eigenvalue	O
.	O
when	O
this	O
number	O
is	O
large	O
,	O
matrix	O
inversion	O
is	O
particularly	O
sensitive	O
to	O
error	O
in	O
the	O
input	O
.	O
this	O
sensitivity	O
is	O
an	O
intrinsic	O
property	O
of	O
the	O
matrix	O
itself	O
,	O
not	O
the	O
result	O
of	O
rounding	O
error	O
during	O
matrix	O
inversion	O
.	O
poorly	O
conditioned	O
matrices	O
amplify	O
pre-existing	O
errors	O
when	O
we	O
multiply	O
by	O
the	O
true	O
matrix	O
inverse	O
.	O
in	O
practice	O
,	O
the	O
error	O
will	O
be	O
compounded	O
further	O
by	O
numerical	O
errors	O
in	O
the	O
inversion	O
process	O
itself	O
.	O
4.3	O
gradient-based	O
optimization	O
most	O
deep	O
learning	O
algorithms	O
involve	O
optimization	O
of	O
some	O
sort	O
.	O
optimization	O
refers	O
to	O
the	O
task	O
of	O
either	O
minimizing	O
or	O
maximizing	O
some	O
function	O
f	O
(	O
x	O
)	O
by	O
altering	O
x.	O
we	O
usually	O
phrase	O
most	O
optimization	O
problems	O
in	O
terms	O
of	O
minimizing	O
f	O
(	O
x	O
)	O
.	O
−	O
maximization	O
may	O
be	O
accomplished	O
via	O
a	O
minimization	O
algorithm	O
by	O
minimizing	O
f	O
(	O
)	O
x	O
.	O
the	O
function	O
we	O
want	O
to	O
minimize	O
or	O
maximize	O
is	O
called	O
the	O
objective	O
func-	O
tion	B
or	O
criterion	O
.	O
when	O
we	O
are	O
minimizing	O
it	O
,	O
we	O
may	O
also	O
call	O
it	O
the	O
cost	O
function	O
,	O
loss	O
function	O
,	O
or	O
error	O
function	O
.	O
in	O
this	O
book	O
,	O
we	O
use	O
these	O
terms	O
interchangeably	O
,	O
though	O
some	O
machine	O
learning	O
publications	O
assign	O
special	O
meaning	O
to	O
some	O
of	O
these	O
terms	O
.	O
we	O
often	O
denote	O
the	O
value	O
that	O
minimizes	O
or	O
maximizes	O
a	O
function	O
with	O
a	O
superscript	O
∗	O
.	O
for	O
example	O
,	O
we	O
might	O
say	O
∗	O
x	O
f	O
x	O
.	O
=	O
arg	O
min	O
(	O
)	O
82	O
chapter	O
4.	O
numerical	O
computation	O
2	O
0	O
.	O
1	O
5	O
.	O
1	O
0	O
.	O
0	O
5	O
.	O
0	O
0	O
.	O
−	O
0	O
5	O
.	O
−	O
1	O
0	O
.	O
−	O
1	O
5	O
.	O
−	O
−	O
2	O
0	O
.	O
2	O
0	O
.	O
	O
global	O
minimum	O
at	O
=	O
0.	O
since	O
f	O
descent	B
halts	O
here	O
.	O
(	O
)	O
=	O
0	O
,	O
gradient	O
x	O
x	O
x	O
<	O
for	O
0	O
,	O
we	O
have	O
so	O
we	O
can	O
decrease	O
moving	O
rightward	O
.	O
	O
(	O
)	O
x	O
<	O
f	O
byf	O
0	O
,	O
x	O
>	O
for	O
0	O
,	O
we	O
have	O
so	O
we	O
can	O
decrease	O
moving	O
leftward	O
.	O
	O
(	O
)	O
x	O
>	O
0	O
,	O
f	O
byf	O
x2	O
f	O
x	O
(	O
)	O
=	O
1	O
2	O
	O
(	O
)	O
=	O
x	O
x	O
f	O
−	O
1	O
5	O
.	O
−	O
1	O
0	O
.	O
−	O
0	O
5	O
.	O
0	O
0	O
.	O
x	O
0	O
5	O
.	O
1	O
0	O
.	O
1	O
5	O
.	O
2	O
0	O
.	O
figure	O
4.1	O
:	O
an	O
illustration	O
of	O
how	O
the	O
gradient	O
descent	B
algorithm	O
uses	O
the	O
derivatives	O
of	O
a	O
function	O
can	O
be	O
used	O
to	O
follow	O
the	O
function	O
downhill	O
to	O
a	O
minimum	O
.	O
we	O
assume	O
the	O
reader	O
is	O
already	O
familiar	O
with	O
calculus	O
,	O
but	O
provide	O
a	O
brief	O
review	O
of	O
how	O
calculus	O
concepts	O
relate	O
to	O
optimization	O
here	O
.	O
suppose	O
we	O
have	O
a	O
function	O
y	O
=	O
f	O
(	O
x	O
)	O
,	O
where	O
both	O
x	O
and	O
y	O
are	O
real	O
numbers	O
.	O
the	O
derivative	O
of	O
this	O
function	O
is	O
denoted	O
as	O
f	O
(	O
x	O
)	O
gives	O
the	O
slope	O
of	O
f	O
(	O
x	O
)	O
at	O
the	O
point	O
x.	O
in	O
other	O
words	O
,	O
it	O
speciﬁes	O
how	O
to	O
scale	O
a	O
small	O
change	O
in	O
the	O
input	O
in	O
order	O
to	O
obtain	O
the	O
corresponding	O
change	O
in	O
the	O
output	O
:	O
f	O
x	O
≈	O
(	O
+	O
)	O
(	O
x	O
)	O
or	O
as	O
dy	O
dx	O
.	O
the	O
derivative	O
f	O
	O
(	O
)	O
x	O
.	O
(	O
)	O
+	O
f	O
x	O
f	O
	O
	O
	O
−	O
the	O
derivative	O
is	O
therefore	O
useful	O
for	O
minimizing	O
a	O
function	O
because	O
it	O
tells	O
us	O
how	O
to	O
change	O
x	O
in	O
order	O
to	O
make	O
a	O
small	O
improvement	O
in	O
y.	O
for	O
example	O
,	O
we	O
know	O
that	O
f	O
(	O
x	O
(	O
x	O
)	O
)	O
)	O
is	O
less	O
than	O
f	O
(	O
x	O
)	O
for	O
small	O
enough	O
	O
.	O
we	O
can	O
thus	O
reduce	O
f	O
(	O
x	O
)	O
by	O
moving	O
x	O
in	O
small	O
steps	O
with	O
opposite	O
sign	O
of	O
the	O
derivative	O
.	O
this	O
technique	O
is	O
called	O
gradient	O
descent	B
(	O
cauchy	O
1847	O
for	O
an	O
example	O
of	O
this	O
technique	O
.	O
)	O
.	O
see	O
ﬁgure	O
sign	O
(	O
f	O
4.1	O
	O
,	O
	O
	O
when	O
f	O
(	O
x	O
)	O
=	O
0	O
,	O
the	O
derivative	O
provides	O
no	O
information	O
about	O
which	O
direction	O
to	O
move	O
.	O
points	O
where	O
f	O
(	O
x	O
)	O
=	O
0	O
are	O
known	O
as	O
critical	O
points	O
or	O
stationary	O
points	O
.	O
a	O
local	O
minimum	O
is	O
a	O
point	O
where	O
f	O
(	O
x	O
)	O
is	O
lower	O
than	O
at	O
all	O
neighboring	O
points	O
,	O
so	O
it	O
is	O
no	O
longer	O
possible	O
to	O
decrease	O
f	O
(	O
x	O
)	O
by	O
making	O
inﬁnitesimal	O
steps	O
.	O
a	O
local	O
maximum	O
is	O
a	O
point	O
where	O
f	O
(	O
x	O
)	O
is	O
higher	O
than	O
at	O
all	O
neighboring	O
points	O
,	O
	O
83	O
chapter	O
4.	O
numerical	O
computation	O
minimum	O
maximum	O
saddle	O
point	O
figure	O
4.2	O
:	O
examples	O
of	O
each	O
of	O
the	O
three	O
types	O
of	O
critical	O
points	O
in	O
1-d.	O
a	O
critical	O
point	O
is	O
a	O
point	O
with	O
zero	O
slope	O
.	O
such	O
a	O
point	O
can	O
either	O
be	O
a	O
local	O
minimum	O
,	O
which	O
is	O
lower	O
than	O
the	O
neighboring	O
points	O
,	O
a	O
local	O
maximum	O
,	O
which	O
is	O
higher	O
than	O
the	O
neighboring	O
points	O
,	O
or	O
a	O
saddle	O
point	O
,	O
which	O
has	O
neighbors	O
that	O
are	O
both	O
higher	O
and	O
lower	O
than	O
the	O
point	O
itself	O
.	O
so	O
it	O
is	O
not	O
possible	O
to	O
increase	O
f	O
(	O
x	O
)	O
by	O
making	O
inﬁnitesimal	O
steps	O
.	O
some	O
critical	O
points	O
are	O
neither	O
maxima	O
nor	O
minima	O
.	O
these	O
are	O
known	O
as	O
saddle	O
points	O
.	O
see	O
ﬁgure	O
for	O
examples	O
of	O
each	O
type	O
of	O
critical	O
point	O
.	O
4.2	O
a	O
point	O
that	O
obtains	O
the	O
absolute	O
lowest	O
value	O
of	O
f	O
(	O
x	O
)	O
is	O
a	O
global	O
minimum	O
.	O
it	O
is	O
possible	O
for	O
there	O
to	O
be	O
only	O
one	O
global	O
minimum	O
or	O
multiple	O
global	O
minima	O
of	O
the	O
function	O
.	O
it	O
is	O
also	O
possible	O
for	O
there	O
to	O
be	O
local	O
minima	O
that	O
are	O
not	O
globally	O
optimal	O
.	O
in	O
the	O
context	O
of	O
deep	O
learning	O
,	O
we	O
optimize	O
functions	O
that	O
may	O
have	O
many	O
local	O
minima	O
that	O
are	O
not	O
optimal	O
,	O
and	O
many	O
saddle	O
points	O
surrounded	O
by	O
very	O
ﬂat	O
regions	O
.	O
all	O
of	O
this	O
makes	O
optimization	O
very	O
diﬃcult	O
,	O
especially	O
when	O
the	O
input	O
to	O
the	O
function	O
is	O
multidimensional	O
.	O
we	O
therefore	O
usually	O
settle	O
for	O
ﬁnding	O
a	O
value	O
of	O
f	O
that	O
is	O
very	O
low	O
,	O
but	O
not	O
necessarily	O
minimal	O
in	O
any	O
formal	O
sense	O
.	O
see	O
ﬁgure	O
for	O
an	O
example	O
.	O
4.3	O
→	O
we	O
often	O
minimize	O
functions	O
that	O
have	O
multiple	O
inputs	O
:	O
f	O
:	O
r	O
n	O
r.	O
for	O
the	O
concept	O
of	O
“	O
minimization	O
”	O
to	O
make	O
sense	O
,	O
there	O
must	O
still	O
be	O
only	O
one	O
(	O
scalar	O
)	O
output	O
.	O
for	O
functions	O
with	O
multiple	O
inputs	O
,	O
we	O
must	O
make	O
use	O
of	O
the	O
concept	O
of	O
partial	O
derivatives	O
.	O
the	O
partial	O
derivative	O
∂	O
f	O
(	O
x	O
)	O
measures	O
how	O
f	O
changes	O
as	O
only	O
the	O
∂xi	O
variable	O
xi	O
increases	O
at	O
point	O
x.	O
the	O
gradient	O
generalizes	O
the	O
notion	O
of	O
derivative	O
to	O
the	O
case	O
where	O
the	O
derivative	O
is	O
with	O
respect	O
to	O
a	O
vector	O
:	O
the	O
gradient	O
of	O
f	O
is	O
the	O
xf	O
(	O
x	O
)	O
.	O
element	O
i	O
of	O
the	O
vector	O
containing	O
all	O
of	O
the	O
partial	O
derivatives	O
,	O
denoted	O
gradient	O
is	O
the	O
partial	O
derivative	O
of	O
f	O
with	O
respect	O
to	O
xi	O
.	O
in	O
multiple	O
dimensions	O
,	O
∇	O
84	O
chapter	O
4.	O
numerical	O
computation	O
this	O
local	O
minimum	O
performs	O
nearly	O
as	O
well	O
as	O
the	O
global	O
one	O
,	O
so	O
it	O
is	O
an	O
acceptable	O
halting	O
point.	O
)	O
ideally	O
,	O
we	O
would	O
like	O
x	O
(	O
to	O
arrive	O
at	O
the	O
global	O
f	O
minimum	O
,	O
but	O
this	O
might	O
not	O
be	O
possible	O
.	O
this	O
local	O
minimum	O
performs	O
poorly	O
and	O
should	O
be	O
avoided	O
.	O
x	O
figure	O
4.3	O
:	O
optimization	O
algorithms	O
may	O
fail	O
to	O
ﬁnd	O
a	O
global	O
minimum	O
when	O
there	O
are	O
multiple	O
local	O
minima	O
or	O
plateaus	O
present	O
.	O
in	O
the	O
context	O
of	O
deep	O
learning	O
,	O
we	O
generally	O
accept	O
such	O
solutions	O
even	O
though	O
they	O
are	O
not	O
truly	O
minimal	O
,	O
so	O
long	O
as	O
they	O
correspond	O
to	O
signiﬁcantly	O
low	O
values	O
of	O
the	O
cost	O
function	O
.	O
critical	O
points	O
are	O
points	O
where	O
every	O
element	O
of	O
the	O
gradient	O
is	O
equal	O
to	O
zero	O
.	O
the	O
directional	O
derivative	O
in	O
direction	O
(	O
a	O
unit	O
vector	O
)	O
is	O
the	O
slope	O
of	O
the	O
function	O
f	O
in	O
direction	O
u.	O
in	O
other	O
words	O
,	O
the	O
directional	O
derivative	O
is	O
the	O
derivative	O
of	O
the	O
function	O
f	O
(	O
x	O
+	O
αu	O
)	O
with	O
respect	O
to	O
α	O
,	O
evaluated	O
at	O
α	O
=	O
0.	O
using	O
the	O
chain	O
rule	O
,	O
we	O
can	O
see	O
that	O
∂	O
∂α	O
u	O
)	O
evaluates	O
to	O
u	O
(	O
)	O
x	O
when	O
=	O
0	O
.	O
∇	O
(	O
+x	O
x	O
f	O
u	O
α	O
α	O
f	O
to	O
minimize	O
f	O
,	O
we	O
would	O
like	O
to	O
ﬁnd	O
the	O
direction	O
in	O
which	O
f	O
decreases	O
the	O
fastest	O
.	O
we	O
can	O
do	O
this	O
using	O
the	O
directional	O
derivative	O
:	O
∇	O
u	O
min	O
	O
||	O
||∇	O
||	O
u	O
2	O
u=1	O
u	O
u	O
,	O
=	O
min	O
	O
xf	O
(	O
)	O
x	O
||	O
2	O
cos	O
θ	O
x	O
f	O
(	O
)	O
x	O
(	O
4.3	O
)	O
(	O
4.4	O
)	O
u	O
u	O
,	O
u=1	O
||	O
||	O
where	O
θ	O
is	O
the	O
angle	O
between	O
u	O
and	O
the	O
gradient	O
.	O
substituting	O
in	O
u	O
2	O
=	O
1	O
and	O
ignoring	O
factors	O
that	O
do	O
not	O
depend	O
on	O
u	O
,	O
this	O
simpliﬁes	O
to	O
minu	O
cos	O
θ.	O
this	O
is	O
minimized	O
when	O
u	O
points	O
in	O
the	O
opposite	O
direction	O
as	O
the	O
gradient	O
.	O
in	O
other	O
words	O
,	O
the	O
gradient	O
points	O
directly	O
uphill	O
,	O
and	O
the	O
negative	O
gradient	O
points	O
directly	O
downhill	O
.	O
we	O
can	O
decrease	O
f	O
by	O
moving	O
in	O
the	O
direction	O
of	O
the	O
negative	O
gradient	O
.	O
this	O
is	O
known	O
as	O
the	O
method	O
of	O
steepest	O
descent	B
.	O
gradient	O
descent	B
or	O
steepest	O
descent	B
proposes	O
a	O
new	O
point	O
−	O
∇	O
	O
x	O
=	O
x	O
	O
x	O
f	O
(	O
)	O
x	O
85	O
(	O
4.5	O
)	O
chapter	O
4.	O
numerical	O
computation	O
where	O
	O
is	O
the	O
learning	O
rate	O
,	O
a	O
positive	O
scalar	O
determining	O
the	O
size	O
of	O
the	O
step	O
.	O
we	O
can	O
choose	O
	O
in	O
several	O
diﬀerent	O
ways	O
.	O
a	O
popular	O
approach	O
is	O
to	O
set	O
	O
to	O
a	O
small	O
constant	O
.	O
sometimes	O
,	O
we	O
can	O
solve	O
for	O
the	O
step	O
size	O
that	O
makes	O
the	O
directional	O
xf	O
(	O
)	O
)	O
x	O
for	O
several	O
derivative	O
vanish	O
.	O
another	O
approach	O
is	O
to	O
evaluate	O
f	O
values	O
of	O
	O
and	O
choose	O
the	O
one	O
that	O
results	O
in	O
the	O
smallest	O
objective	O
function	O
value	O
.	O
this	O
last	O
strategy	O
is	O
called	O
a	O
line	O
search	O
.	O
−	O
∇	O
	O
(	O
x	O
steepest	O
descent	B
converges	O
when	O
every	O
element	O
of	O
the	O
gradient	O
is	O
zero	O
(	O
or	O
,	O
in	O
practice	O
,	O
very	O
close	O
to	O
zero	O
)	O
.	O
in	O
some	O
cases	O
,	O
we	O
may	O
be	O
able	O
to	O
avoid	O
running	O
this	O
iterative	O
algorithm	O
,	O
and	O
just	O
jump	O
directly	O
to	O
the	O
critical	O
point	O
by	O
solving	O
the	O
equation	O
∇	O
x	O
f	O
(	O
)	O
=	O
0	O
for	O
.x	O
x	O
although	O
gradient	O
descent	B
is	O
limited	O
to	O
optimization	O
in	O
continuous	O
spaces	O
,	O
the	O
general	O
concept	O
of	O
repeatedly	O
making	O
a	O
small	O
move	O
(	O
that	O
is	O
approximately	O
the	O
best	O
small	O
move	O
)	O
towards	O
better	O
conﬁgurations	O
can	O
be	O
generalized	O
to	O
discrete	O
spaces	O
.	O
ascending	O
an	O
objective	O
function	O
of	O
discrete	O
parameters	O
is	O
called	O
hill	O
climbing	O
(	O
russel	O
and	O
norvig	O
2003	O
)	O
.	O
,	O
4.3.1	O
beyond	O
the	O
gradient	O
:	O
jacobian	O
and	O
hessian	O
matrices	O
sometimes	O
we	O
need	O
to	O
ﬁnd	O
all	O
of	O
the	O
partial	O
derivatives	O
of	O
a	O
function	O
whose	O
input	O
and	O
output	O
are	O
both	O
vectors	O
.	O
the	O
matrix	O
containing	O
all	O
such	O
partial	O
derivatives	O
is	O
n	O
,	O
known	O
as	O
a	O
jacobian	O
matrix	O
.	O
speciﬁcally	O
,	O
if	O
we	O
have	O
a	O
function	O
f	O
:	O
r	O
then	O
the	O
jacobian	O
matrix	O
j	O
is	O
deﬁned	O
such	O
that	O
×	O
n	O
m	O
→	O
∈	O
m	O
r	O
f	O
(	O
)	O
x	O
i.	O
of	O
f	O
r	O
j	O
i	O
,	O
j	O
=	O
∂	O
∂xj	O
→	O
∂2	O
∂xi	O
∂xj	O
f	O
by	O
f	O
we	O
are	O
also	O
sometimes	O
interested	O
in	O
a	O
derivative	O
of	O
a	O
derivative	O
.	O
this	O
is	O
known	O
r	O
,	O
the	O
derivative	O
f.	O
n	O
as	O
a	O
second	O
derivative	O
.	O
for	O
example	O
,	O
for	O
a	O
function	O
f	O
:	O
r	O
with	O
respect	O
to	O
xi	O
of	O
the	O
derivative	O
of	O
f	O
with	O
respect	O
to	O
xj	O
is	O
denoted	O
as	O
	O
in	O
a	O
single	O
dimension	O
,	O
we	O
can	O
denote	O
d2	O
(	O
x	O
)	O
.	O
the	O
second	O
derivative	O
tells	O
dx2	O
us	O
how	O
the	O
ﬁrst	O
derivative	O
will	O
change	O
as	O
we	O
vary	O
the	O
input	O
.	O
this	O
is	O
important	O
because	O
it	O
tells	O
us	O
whether	O
a	O
gradient	O
step	O
will	O
cause	O
as	O
much	O
of	O
an	O
improvement	O
as	O
we	O
would	O
expect	O
based	O
on	O
the	O
gradient	O
alone	O
.	O
we	O
can	O
think	O
of	O
the	O
second	O
derivative	O
as	O
measuring	O
curvature	O
.	O
suppose	O
we	O
have	O
a	O
quadratic	O
function	O
(	O
many	O
functions	O
that	O
arise	O
in	O
practice	O
are	O
not	O
quadratic	O
but	O
can	O
be	O
approximated	O
well	O
as	O
quadratic	O
,	O
at	O
least	O
locally	O
)	O
.	O
if	O
such	O
a	O
function	O
has	O
a	O
second	O
derivative	O
of	O
zero	O
,	O
then	O
there	O
is	O
no	O
curvature	O
.	O
it	O
is	O
a	O
perfectly	O
ﬂat	O
line	O
,	O
and	O
its	O
value	O
can	O
be	O
predicted	O
	O
using	O
only	O
the	O
gradient	O
.	O
if	O
the	O
gradient	O
is	O
along	O
the	O
negative	O
gradient	O
,	O
and	O
the	O
cost	O
function	O
will	O
decrease	O
by	O
	O
.	O
if	O
the	O
second	O
derivative	O
is	O
negative	O
,	O
the	O
function	O
curves	O
downward	O
,	O
so	O
the	O
cost	O
function	O
will	O
actually	O
decrease	O
by	O
more	O
than	O
	O
.	O
finally	O
,	O
if	O
the	O
second	O
derivative	O
is	O
positive	O
,	O
the	O
function	O
curves	O
upward	O
,	O
so	O
the	O
cost	O
function	O
can	O
decrease	O
by	O
less	O
than	O
	O
.	O
see	O
,	O
then	O
we	O
can	O
make	O
a	O
step	O
of	O
size	O
1	O
86	O
chapter	O
4.	O
numerical	O
computation	O
negative	O
curvature	O
no	O
curvature	O
positive	O
curvature	O
)	O
x	O
(	O
f	O
)	O
x	O
(	O
f	O
)	O
x	O
(	O
f	O
x	O
x	O
x	O
figure	O
4.4	O
:	O
the	O
second	O
derivative	O
determines	O
the	O
curvature	O
of	O
a	O
function	O
.	O
here	O
we	O
show	O
quadratic	O
functions	O
with	O
various	O
curvature	O
.	O
the	O
dashed	O
line	O
indicates	O
the	O
value	O
of	O
the	O
cost	O
function	O
we	O
would	O
expect	O
based	O
on	O
the	O
gradient	O
information	O
alone	O
as	O
we	O
make	O
a	O
gradient	O
step	O
downhill	O
.	O
in	O
the	O
case	O
of	O
negative	O
curvature	O
,	O
the	O
cost	O
function	O
actually	O
decreases	O
faster	O
than	O
the	O
gradient	O
predicts	O
.	O
in	O
the	O
case	O
of	O
no	O
curvature	O
,	O
the	O
gradient	O
predicts	O
the	O
decrease	O
correctly	O
.	O
in	O
the	O
case	O
of	O
positive	O
curvature	O
,	O
the	O
function	O
decreases	O
slower	O
than	O
expected	O
and	O
eventually	O
begins	O
to	O
increase	O
,	O
so	O
steps	O
that	O
are	O
too	O
large	O
can	O
actually	O
increase	O
the	O
function	O
inadvertently	O
.	O
4.4	O
ﬁgure	O
the	O
value	O
of	O
the	O
cost	O
function	O
predicted	O
by	O
the	O
gradient	O
and	O
the	O
true	O
value	O
.	O
to	O
see	O
how	O
diﬀerent	O
forms	O
of	O
curvature	O
aﬀect	O
the	O
relationship	O
between	O
when	O
our	O
function	O
has	O
multiple	O
input	O
dimensions	O
,	O
there	O
are	O
many	O
second	O
derivatives	O
.	O
these	O
derivatives	O
can	O
be	O
collected	O
together	O
into	O
a	O
matrix	O
called	O
the	O
hessian	O
matrix	O
.	O
the	O
hessian	O
matrix	O
is	O
deﬁned	O
such	O
that	O
h	O
x	O
)	O
(	O
)	O
(	O
f	O
h	O
x	O
(	O
)	O
(	O
f	O
)	O
i	O
,	O
j	O
=	O
∂	O
2	O
∂xi∂xj	O
f	O
.	O
(	O
)	O
x	O
(	O
4.6	O
)	O
equivalently	O
,	O
the	O
hessian	O
is	O
the	O
jacobian	O
of	O
the	O
gradient	O
.	O
anywhere	O
that	O
the	O
second	O
partial	O
derivatives	O
are	O
continuous	O
,	O
the	O
diﬀerential	O
operators	O
are	O
commutative	O
,	O
i.e	O
.	O
their	O
order	O
can	O
be	O
swapped	O
:	O
∂2	O
∂xi∂xj	O
f	O
(	O
)	O
=x	O
∂2	O
∂x	O
j∂xi	O
f	O
.	O
(	O
)	O
x	O
(	O
4.7	O
)	O
this	O
implies	O
that	O
hi	O
,	O
j	O
=	O
h	O
j	O
,	O
i	O
,	O
so	O
the	O
hessian	O
matrix	O
is	O
symmetric	O
at	O
such	O
points	O
.	O
most	O
of	O
the	O
functions	O
we	O
encounter	O
in	O
the	O
context	O
of	O
deep	O
learning	O
have	O
a	O
symmetric	O
hessian	O
almost	O
everywhere	O
.	O
because	O
the	O
hessian	O
matrix	O
is	O
real	O
and	O
symmetric	O
,	O
we	O
can	O
decompose	O
it	O
into	O
a	O
set	O
of	O
real	O
eigenvalues	O
and	O
an	O
orthogonal	O
basis	O
of	O
87	O
chapter	O
4.	O
numerical	O
computation	O
	O
eigenvectors	O
.	O
the	O
second	O
derivative	O
in	O
a	O
speciﬁc	O
direction	O
represented	O
by	O
a	O
unit	O
vector	O
d	O
is	O
given	O
by	O
d	O
hd	O
.	O
when	O
d	O
is	O
an	O
eigenvector	O
of	O
h	O
,	O
the	O
second	O
derivative	O
in	O
that	O
direction	O
is	O
given	O
by	O
the	O
corresponding	O
eigenvalue	O
.	O
for	O
other	O
directions	O
of	O
d	O
,	O
the	O
directional	O
second	O
derivative	O
is	O
a	O
weighted	O
average	O
of	O
all	O
of	O
the	O
eigenvalues	O
,	O
with	O
weights	O
between	O
0	O
and	O
1	O
,	O
and	O
eigenvectors	O
that	O
have	O
smaller	O
angle	O
with	O
d	O
receiving	O
more	O
weight	O
.	O
the	O
maximum	O
eigenvalue	O
determines	O
the	O
maximum	O
second	O
derivative	O
and	O
the	O
minimum	O
eigenvalue	O
determines	O
the	O
minimum	O
second	O
derivative	O
.	O
the	O
(	O
directional	O
)	O
second	O
derivative	O
tells	O
us	O
how	O
well	O
we	O
can	O
expect	O
a	O
gradient	O
descent	B
step	O
to	O
perform	O
.	O
we	O
can	O
make	O
a	O
second-order	O
taylor	O
series	O
approximation	O
to	O
the	O
function	O
around	O
the	O
current	O
point	O
f	O
(	O
)	O
x	O
≈	O
(	O
)	O
x	O
f	O
f	O
x	O
(	O
0	O
)	O
:	O
−	O
−	O
	O
(	O
0	O
)	O
)	O
g	O
+	O
1	O
2	O
(	O
x	O
(	O
0	O
)	O
)	O
+	O
(	O
x	O
x	O
(	O
x	O
x	O
(	O
0	O
)	O
)	O
	O
−	O
h	O
x	O
x	O
(	O
(	O
0	O
)	O
)	O
.	O
(	O
4.8	O
)	O
where	O
g	O
is	O
the	O
gradient	O
and	O
h	O
is	O
the	O
hessian	O
at	O
x	O
(	O
0	O
)	O
.	O
if	O
we	O
use	O
a	O
learning	O
rate	O
of	O
	O
,	O
then	O
the	O
new	O
point	O
x	O
will	O
be	O
given	O
by	O
x	O
(	O
0	O
)	O
g	O
.	O
substituting	O
this	O
into	O
our	O
approximation	O
,	O
we	O
obtain	O
−	O
−	O
	O
−	O
f	O
(	O
x	O
(	O
0	O
)	O
f	O
(	O
x	O
(	O
0	O
)	O
)	O
2	O
g	O
hg	O
.	O
≈	O
g	O
)	O
(	O
4.9	O
)	O
	O
g	O
+	O
g	O
1	O
2	O
there	O
are	O
three	O
terms	O
here	O
:	O
the	O
original	O
value	O
of	O
the	O
function	O
,	O
the	O
expected	O
improvement	O
due	O
to	O
the	O
slope	O
of	O
the	O
function	O
,	O
and	O
the	O
correction	O
we	O
must	O
apply	O
to	O
account	O
for	O
the	O
curvature	O
of	O
the	O
function	O
.	O
when	O
this	O
last	O
term	O
is	O
too	O
large	O
,	O
the	O
	O
hg	O
is	O
zero	O
or	O
negative	O
,	O
gradient	O
descent	B
step	O
can	O
actually	O
move	O
uphill	O
.	O
when	O
g	O
the	O
taylor	O
series	O
approximation	O
predicts	O
that	O
increasing	O
	O
forever	O
will	O
decrease	O
f	O
forever	O
.	O
in	O
practice	O
,	O
the	O
taylor	O
series	O
is	O
unlikely	O
to	O
remain	O
accurate	O
for	O
large	O
	O
,	O
so	O
one	O
must	O
resort	O
to	O
more	O
heuristic	O
choices	O
of	O
	O
in	O
this	O
case	O
.	O
when	O
g	O
hg	O
is	O
positive	O
,	O
solving	O
for	O
the	O
optimal	O
step	O
size	O
that	O
decreases	O
the	O
taylor	O
series	O
approximation	O
of	O
the	O
function	O
the	O
most	O
yields	O
	O
∗	O
	O
=	O
	O
g	O
	O
g	O
g	O
hg	O
.	O
(	O
4.10	O
)	O
in	O
the	O
worst	O
case	O
,	O
when	O
g	O
aligns	O
with	O
the	O
eigenvector	O
of	O
h	O
corresponding	O
to	O
the	O
maximal	O
eigenvalue	O
λmax	O
,	O
then	O
this	O
optimal	O
step	O
size	O
is	O
given	O
by	O
.	O
to	O
the	O
extent	O
that	O
the	O
function	O
we	O
minimize	O
can	O
be	O
approximated	O
well	O
by	O
a	O
quadratic	O
function	O
,	O
the	O
eigenvalues	O
of	O
the	O
hessian	O
thus	O
determine	O
the	O
scale	O
of	O
the	O
learning	O
rate	O
.	O
λ	O
max	O
1	O
the	O
second	O
derivative	O
can	O
be	O
used	O
to	O
determine	O
whether	O
a	O
critical	O
point	O
is	O
a	O
local	O
maximum	O
,	O
a	O
local	O
minimum	O
,	O
or	O
saddle	O
point	O
.	O
recall	O
that	O
on	O
a	O
critical	O
point	O
,	O
f	O
(	O
x	O
)	O
increases	O
as	O
we	O
move	O
to	O
the	O
right	O
and	O
decreases	O
as	O
we	O
move	O
to	O
the	O
left	O
.	O
this	O
means	O
	O
(	O
x	O
)	O
>	O
0	O
,	O
the	O
ﬁrst	O
derivative	O
f	O
(	O
x	O
)	O
=	O
0.	O
when	O
the	O
second	O
derivative	O
f	O
	O
	O
88	O
chapter	O
4.	O
numerical	O
computation	O
−	O
	O
(	O
x	O
	O
	O
)	O
<	O
0	O
and	O
f	O
(	O
x	O
+	O
	O
)	O
>	O
0	O
for	O
small	O
enough	O
	O
.	O
in	O
other	O
words	O
,	O
as	O
we	O
move	O
f	O
right	O
,	O
the	O
slope	O
begins	O
to	O
point	O
uphill	O
to	O
the	O
right	O
,	O
and	O
as	O
we	O
move	O
left	O
,	O
the	O
slope	O
(	O
x	O
)	O
>	O
0	O
,	O
we	O
can	O
begins	O
to	O
point	O
uphill	O
to	O
the	O
left	O
.	O
thus	O
,	O
when	O
f	O
conclude	O
that	O
x	O
is	O
a	O
local	O
minimum	O
.	O
similarly	O
,	O
when	O
f	O
(	O
x	O
)	O
<	O
0	O
,	O
we	O
can	O
conclude	O
that	O
x	O
is	O
a	O
local	O
maximum	O
.	O
this	O
is	O
known	O
as	O
the	O
second	O
derivative	O
test	O
.	O
unfortunately	O
,	O
when	O
f	O
(	O
x	O
)	O
=	O
0	O
,	O
the	O
test	O
is	O
inconclusive	O
.	O
in	O
this	O
case	O
x	O
may	O
be	O
a	O
saddle	O
point	O
,	O
or	O
a	O
part	O
of	O
a	O
ﬂat	O
region	O
.	O
	O
(	O
x	O
)	O
=	O
0	O
and	O
f	O
	O
(	O
x	O
)	O
=	O
0	O
and	O
f	O
	O
	O
	O
in	O
multiple	O
dimensions	O
,	O
we	O
need	O
to	O
examine	O
all	O
of	O
the	O
second	O
derivatives	O
of	O
the	O
function	O
.	O
using	O
the	O
eigendecomposition	O
of	O
the	O
hessian	O
matrix	O
,	O
we	O
can	O
generalize	O
∇	O
the	O
second	O
derivative	O
test	O
to	O
multiple	O
dimensions	O
.	O
at	O
a	O
critical	O
point	O
,	O
where	O
xf	O
(	O
x	O
)	O
=	O
0	O
,	O
we	O
can	O
examine	O
the	O
eigenvalues	O
of	O
the	O
hessian	O
to	O
determine	O
whether	O
the	O
critical	O
point	O
is	O
a	O
local	O
maximum	O
,	O
local	O
minimum	O
,	O
or	O
saddle	O
point	O
.	O
when	O
the	O
hessian	O
is	O
positive	O
deﬁnite	O
(	O
all	O
its	O
eigenvalues	O
are	O
positive	O
)	O
,	O
the	O
point	O
is	O
a	O
local	O
minimum	O
.	O
this	O
can	O
be	O
seen	O
by	O
observing	O
that	O
the	O
directional	O
second	O
derivative	O
in	O
any	O
direction	O
must	O
be	O
positive	O
,	O
and	O
making	O
reference	O
to	O
the	O
univariate	O
second	O
derivative	O
test	O
.	O
likewise	O
,	O
when	O
the	O
hessian	O
is	O
negative	O
deﬁnite	O
(	O
all	O
its	O
eigenvalues	O
are	O
negative	O
)	O
,	O
the	O
point	O
is	O
a	O
local	O
maximum	O
.	O
in	O
multiple	O
dimensions	O
,	O
it	O
is	O
actually	O
possible	O
to	O
ﬁnd	O
positive	O
evidence	O
of	O
saddle	O
points	O
in	O
some	O
cases	O
.	O
when	O
at	O
least	O
one	O
eigenvalue	O
is	O
positive	O
and	O
at	O
least	O
one	O
eigenvalue	O
is	O
negative	O
,	O
we	O
know	O
that	O
x	O
is	O
a	O
local	O
maximum	O
on	O
one	O
cross	O
section	O
of	O
f	O
but	O
a	O
local	O
minimum	O
on	O
another	O
cross	O
section	O
.	O
see	O
ﬁgure	O
for	O
an	O
example	O
.	O
finally	O
,	O
the	O
multidimensional	O
second	O
derivative	O
test	O
can	O
be	O
inconclusive	O
,	O
just	O
like	O
the	O
univariate	O
version	O
.	O
the	O
test	O
is	O
inconclusive	O
whenever	O
all	O
of	O
the	O
non-zero	O
eigenvalues	O
have	O
the	O
same	O
sign	O
,	O
but	O
at	O
least	O
one	O
eigenvalue	O
is	O
zero	O
.	O
this	O
is	O
because	O
the	O
univariate	O
second	O
derivative	O
test	O
is	O
inconclusive	O
in	O
the	O
cross	O
section	O
corresponding	O
to	O
the	O
zero	O
eigenvalue	O
.	O
4.5	O
in	O
multiple	O
dimensions	O
,	O
there	O
is	O
a	O
diﬀerent	O
second	O
derivative	O
for	O
each	O
direction	O
at	O
a	O
single	O
point	O
.	O
the	O
condition	O
number	O
of	O
the	O
hessian	O
at	O
this	O
point	O
measures	O
how	O
much	O
the	O
second	O
derivatives	O
diﬀer	O
from	O
each	O
other	O
.	O
when	O
the	O
hessian	O
has	O
a	O
poor	O
condition	O
number	O
,	O
gradient	O
descent	B
performs	O
poorly	O
.	O
this	O
is	O
because	O
in	O
one	O
direction	O
,	O
the	O
derivative	O
increases	O
rapidly	O
,	O
while	O
in	O
another	O
direction	O
,	O
it	O
increases	O
slowly	O
.	O
gradient	O
descent	B
is	O
unaware	O
of	O
this	O
change	O
in	O
the	O
derivative	O
so	O
it	O
does	O
not	O
know	O
that	O
it	O
needs	O
to	O
explore	O
preferentially	O
in	O
the	O
direction	O
where	O
the	O
derivative	O
remains	O
negative	O
for	O
longer	O
.	O
it	O
also	O
makes	O
it	O
diﬃcult	O
to	O
choose	O
a	O
good	O
step	O
size	O
.	O
the	O
step	O
size	O
must	O
be	O
small	O
enough	O
to	O
avoid	O
overshooting	O
the	O
minimum	O
and	O
going	O
uphill	O
in	O
directions	O
with	O
strong	O
positive	O
curvature	O
.	O
this	O
usually	O
means	O
that	O
the	O
step	O
size	O
is	O
too	O
small	O
to	O
make	O
signiﬁcant	O
progress	O
in	O
other	O
directions	O
with	O
less	O
curvature	O
.	O
see	O
ﬁgure	O
for	O
an	O
example	O
.	O
4.6	O
this	O
issue	O
can	O
be	O
resolved	O
by	O
using	O
information	O
from	O
the	O
hessian	O
matrix	O
to	O
guide	O
89	O
chapter	O
4.	O
numerical	O
computation	O
	O
	O
	O
	O
	O
	O
	O
 	O
	O
	O
	O
 	O
	O
	O
	O
	O
	O
	O
	O
	O
 	O
−	O
figure	O
4.5	O
:	O
a	O
saddle	O
point	O
containing	O
both	O
positive	O
and	O
negative	O
curvature	O
.	O
the	O
function	O
in	O
this	O
example	O
is	O
f	O
(	O
x	O
)	O
=	O
x2	O
2.	O
along	O
the	O
axis	O
corresponding	O
to	O
x1	O
,	O
the	O
function	O
x2	O
1	O
curves	O
upward	O
.	O
this	O
axis	O
is	O
an	O
eigenvector	O
of	O
the	O
hessian	O
and	O
has	O
a	O
positive	O
eigenvalue	O
.	O
along	O
the	O
axis	O
corresponding	O
to	O
x2	O
,	O
the	O
function	O
curves	O
downward	O
.	O
this	O
direction	O
is	O
an	O
eigenvector	O
of	O
the	O
hessian	O
with	O
negative	O
eigenvalue	O
.	O
the	O
name	O
“	O
saddle	O
point	O
”	O
derives	O
from	O
the	O
saddle-like	O
shape	O
of	O
this	O
function	O
.	O
this	O
is	O
the	O
quintessential	O
example	O
of	O
a	O
function	O
with	O
a	O
saddle	O
point	O
.	O
in	O
more	O
than	O
one	O
dimension	O
,	O
it	O
is	O
not	O
necessary	O
to	O
have	O
an	O
eigenvalue	O
of	O
0	O
in	O
order	O
to	O
get	O
a	O
saddle	O
point	O
:	O
it	O
is	O
only	O
necessary	O
to	O
have	O
both	O
positive	O
and	O
negative	O
eigenvalues	O
.	O
we	O
can	O
think	O
of	O
a	O
saddle	O
point	O
with	O
both	O
signs	O
of	O
eigenvalues	O
as	O
being	O
a	O
local	O
maximum	O
within	O
one	O
cross	O
section	O
and	O
a	O
local	O
minimum	O
within	O
another	O
cross	O
section	O
.	O
90	O
chapter	O
4.	O
numerical	O
computation	O
2	O
x	O
20	O
10	O
0	O
−	O
10	O
−	O
20	O
−	O
−	O
−	O
−	O
30	O
30	O
20	O
10	O
10	O
20	O
0	O
x1	O
and	O
the	O
least	O
curvature	O
is	O
in	O
the	O
direction	O
[	O
1	O
,	O
figure	O
4.6	O
:	O
gradient	O
descent	B
fails	O
to	O
exploit	O
the	O
curvature	O
information	O
contained	O
in	O
the	O
hessian	O
matrix	O
.	O
here	O
we	O
use	O
gradient	O
descent	B
to	O
minimize	O
a	O
quadratic	O
function	O
f	O
(	O
x	O
)	O
whose	O
hessian	O
matrix	O
has	O
condition	O
number	O
5.	O
this	O
means	O
that	O
the	O
direction	O
of	O
most	O
curvature	O
has	O
ﬁve	O
times	O
more	O
curvature	O
than	O
the	O
direction	O
of	O
least	O
curvature	O
.	O
in	O
this	O
case	O
,	O
the	O
most	O
	O
curvature	O
is	O
in	O
the	O
direction	O
[	O
1	O
,	O
1	O
]	O
.	O
the	O
red	O
lines	O
indicate	O
the	O
path	O
followed	O
by	O
gradient	O
descent	B
.	O
this	O
very	O
elongated	O
quadratic	O
function	O
resembles	O
a	O
long	O
canyon	O
.	O
gradient	O
descent	B
wastes	O
time	O
repeatedly	O
descending	O
canyon	O
walls	O
,	O
because	O
they	O
are	O
the	O
steepest	O
feature	O
.	O
because	O
the	O
step	O
size	O
is	O
somewhat	O
too	O
large	O
,	O
it	O
has	O
a	O
tendency	O
to	O
overshoot	O
the	O
bottom	O
of	O
the	O
function	O
and	O
thus	O
needs	O
to	O
descend	O
the	O
opposite	O
canyon	O
wall	O
on	O
the	O
next	O
iteration	O
.	O
the	O
large	O
positive	O
eigenvalue	O
of	O
the	O
hessian	O
corresponding	O
to	O
the	O
eigenvector	O
pointed	O
in	O
this	O
direction	O
indicates	O
that	O
this	O
directional	O
derivative	O
is	O
rapidly	O
increasing	O
,	O
so	O
an	O
optimization	O
algorithm	O
based	O
on	O
the	O
hessian	O
could	O
predict	O
that	O
the	O
steepest	O
direction	O
is	O
not	O
actually	O
a	O
promising	O
search	O
direction	O
in	O
this	O
context	O
.	O
	O
1	O
]	O
−	O
91	O
chapter	O
4.	O
numerical	O
computation	O
the	O
search	O
.	O
the	O
simplest	O
method	O
for	O
doing	O
so	O
is	O
known	O
as	O
newton	O
’	O
s	O
method	O
.	O
newton	O
’	O
s	O
method	O
is	O
based	O
on	O
using	O
a	O
second-order	O
taylor	O
series	O
expansion	O
to	O
approximate	O
x	O
(	O
0	O
)	O
:	O
≈	O
(	O
)	O
x	O
f	O
f	O
f	O
(	O
)	O
x	O
near	O
some	O
point	O
−	O
(	O
x	O
(	O
0	O
)	O
)	O
+	O
(	O
x	O
x	O
∇	O
(	O
0	O
)	O
)	O
xf	O
(	O
x	O
(	O
0	O
)	O
)	O
+	O
−	O
(	O
x	O
x	O
(	O
0	O
)	O
)	O
	O
1	O
2	O
−	O
(	O
0	O
)	O
)	O
(	O
x	O
x	O
h	O
x	O
(	O
)	O
(	O
f	O
(	O
0	O
)	O
)	O
.	O
(	O
4.11	O
)	O
if	O
we	O
then	O
solve	O
for	O
the	O
critical	O
point	O
of	O
this	O
function	O
,	O
we	O
obtain	O
:	O
∗	O
−	O
1∇	O
−	O
(	O
0	O
)	O
)	O
x	O
=	O
x	O
(	O
0	O
)	O
h	O
x	O
(	O
)	O
(	O
f	O
xf	O
(	O
x	O
(	O
0	O
)	O
)	O
.	O
(	O
4.12	O
)	O
4.12	O
when	O
f	O
is	O
a	O
positive	O
deﬁnite	O
quadratic	O
function	O
,	O
newton	O
’	O
s	O
method	O
consists	O
of	O
applying	O
equation	O
once	O
to	O
jump	O
to	O
the	O
minimum	O
of	O
the	O
function	O
directly	O
.	O
when	O
f	O
is	O
not	O
truly	O
quadratic	O
but	O
can	O
be	O
locally	O
approximated	O
as	O
a	O
positive	O
deﬁnite	O
quadratic	O
,	O
newton	O
’	O
s	O
method	O
consists	O
of	O
applying	O
equation	O
multiple	O
times	O
.	O
iteratively	O
updating	O
the	O
approximation	O
and	O
jumping	O
to	O
the	O
minimum	O
of	O
the	O
approximation	O
can	O
reach	O
the	O
critical	O
point	O
much	O
faster	O
than	O
gradient	O
descent	B
would	O
.	O
this	O
is	O
a	O
useful	O
property	O
near	O
a	O
local	O
minimum	O
,	O
but	O
it	O
can	O
be	O
a	O
harmful	O
property	O
near	O
a	O
saddle	O
point	O
.	O
as	O
discussed	O
in	O
section	O
,	O
newton	O
’	O
s	O
method	O
is	O
only	O
appropriate	O
when	O
the	O
nearby	O
critical	O
point	O
is	O
a	O
minimum	O
(	O
all	O
the	O
eigenvalues	O
of	O
the	O
hessian	O
are	O
positive	O
)	O
,	O
whereas	O
gradient	O
descent	B
is	O
not	O
attracted	O
to	O
saddle	O
points	O
unless	O
the	O
gradient	O
points	O
toward	O
them	O
.	O
8.2.3	O
4.12	O
optimization	O
algorithms	O
that	O
use	O
only	O
the	O
gradient	O
,	O
such	O
as	O
gradient	O
descent	B
,	O
are	O
called	O
ﬁrst-order	O
optimization	O
algorithms	O
.	O
optimization	O
algorithms	O
that	O
also	O
use	O
the	O
hessian	O
matrix	O
,	O
such	O
as	O
newton	O
’	O
s	O
method	O
,	O
are	O
called	O
second-order	O
optimization	O
algorithms	O
(	O
nocedal	O
and	O
wright	O
2006	O
)	O
.	O
,	O
the	O
optimization	O
algorithms	O
employed	O
in	O
most	O
contexts	O
in	O
this	O
book	O
are	O
applicable	O
to	O
a	O
wide	O
variety	O
of	O
functions	O
,	O
but	O
come	O
with	O
almost	O
no	O
guarantees	O
.	O
deep	O
learning	O
algorithms	O
tend	O
to	O
lack	O
guarantees	O
because	O
the	O
family	O
of	O
functions	O
used	O
in	O
deep	O
learning	O
is	O
quite	O
complicated	O
.	O
in	O
many	O
other	O
ﬁelds	O
,	O
the	O
dominant	O
approach	O
to	O
optimization	O
is	O
to	O
design	O
optimization	O
algorithms	O
for	O
a	O
limited	O
family	O
of	O
functions	O
.	O
in	O
the	O
context	O
of	O
deep	O
learning	O
,	O
we	O
sometimes	O
gain	O
some	O
guarantees	O
by	O
restrict-	O
ing	O
ourselves	O
to	O
functions	O
that	O
are	O
either	O
lipschitz	O
continuous	O
or	O
have	O
lipschitz	O
continuous	O
derivatives	O
.	O
a	O
lipschitz	O
continuous	O
function	O
is	O
a	O
function	O
f	O
whose	O
rate	O
of	O
change	O
is	O
bounded	O
by	O
a	O
lipschitz	O
constant	O
l	O
:	O
∀	O
∀	O
|	O
x	O
,	O
y	O
,	O
f	O
(	O
)	O
x	O
−	O
|	O
≤	O
l||	O
−	O
||	O
x	O
y	O
2.	O
f	O
(	O
)	O
y	O
(	O
4.13	O
)	O
this	O
property	O
is	O
useful	O
because	O
it	O
allows	O
us	O
to	O
quantify	O
our	O
assumption	O
that	O
a	O
small	O
change	O
in	O
the	O
input	O
made	O
by	O
an	O
algorithm	O
such	O
as	O
gradient	O
descent	B
will	O
have	O
92	O
chapter	O
4.	O
numerical	O
computation	O
a	O
small	O
change	O
in	O
the	O
output	O
.	O
lipschitz	O
continuity	O
is	O
also	O
a	O
fairly	O
weak	O
constraint	O
,	O
and	O
many	O
optimization	O
problems	O
in	O
deep	O
learning	O
can	O
be	O
made	O
lipschitz	O
continuous	O
with	O
relatively	O
minor	O
modiﬁcations	O
.	O
perhaps	O
the	O
most	O
successful	O
ﬁeld	O
of	O
specialized	O
optimization	O
is	O
convex	O
op-	O
timization	O
.	O
convex	O
optimization	O
algorithms	O
are	O
able	O
to	O
provide	O
many	O
more	O
guarantees	O
by	O
making	O
stronger	O
restrictions	O
.	O
convex	O
optimization	O
algorithms	O
are	O
applicable	O
only	O
to	O
convex	O
functions—functions	O
for	O
which	O
the	O
hessian	O
is	O
positive	O
semideﬁnite	O
everywhere	O
.	O
such	O
functions	O
are	O
well-behaved	O
because	O
they	O
lack	O
saddle	O
points	O
and	O
all	O
of	O
their	O
local	O
minima	O
are	O
necessarily	O
global	O
minima	O
.	O
however	O
,	O
most	O
problems	O
in	O
deep	O
learning	O
are	O
diﬃcult	O
to	O
express	O
in	O
terms	O
of	O
convex	O
optimization	O
.	O
convex	O
optimization	O
is	O
used	O
only	O
as	O
a	O
subroutine	O
of	O
some	O
deep	O
learning	O
algorithms	O
.	O
ideas	O
from	O
the	O
analysis	O
of	O
convex	O
optimization	O
algorithms	O
can	O
be	O
useful	O
for	O
proving	O
the	O
convergence	O
of	O
deep	O
learning	O
algorithms	O
.	O
however	O
,	O
in	O
general	O
,	O
the	O
importance	O
of	O
convex	O
optimization	O
is	O
greatly	O
diminished	O
in	O
the	O
context	O
of	O
deep	O
learning	O
.	O
for	O
more	O
information	O
about	O
convex	O
optimization	O
,	O
see	O
boyd	O
and	O
vandenberghe	O
2004	O
)	O
or	O
rockafellar	O
1997	O
)	O
.	O
(	O
(	O
4.4	O
constrained	O
optimization	O
sometimes	O
we	O
wish	O
not	O
only	O
to	O
maximize	O
or	O
minimize	O
a	O
function	O
f	O
(	O
x	O
)	O
over	O
all	O
possible	O
values	O
of	O
x.	O
instead	O
we	O
may	O
wish	O
to	O
ﬁnd	O
the	O
maximal	O
or	O
minimal	O
value	O
of	O
f	O
(	O
x	O
)	O
for	O
values	O
of	O
x	O
in	O
some	O
set	O
s.	O
this	O
is	O
known	O
as	O
constrained	O
optimization	O
.	O
points	O
x	O
that	O
lie	O
within	O
the	O
set	O
s	O
are	O
called	O
feasible	O
points	O
in	O
constrained	O
optimization	O
terminology	O
.	O
we	O
often	O
wish	O
to	O
ﬁnd	O
a	O
solution	O
that	O
is	O
small	O
in	O
some	O
sense	O
.	O
a	O
common	O
approach	O
in	O
such	O
situations	O
is	O
to	O
impose	O
a	O
norm	O
constraint	O
,	O
such	O
as	O
||	O
≤	O
||	O
x	O
.	O
1	O
one	O
simple	O
approach	O
to	O
constrained	O
optimization	O
is	O
simply	O
to	O
modify	O
gradient	O
descent	B
taking	O
the	O
constraint	O
into	O
account	O
.	O
if	O
we	O
use	O
a	O
small	O
constant	O
step	O
size	O
	O
,	O
we	O
can	O
make	O
gradient	O
descent	B
steps	O
,	O
then	O
project	O
the	O
result	O
back	O
into	O
s.	O
if	O
we	O
use	O
a	O
line	O
search	O
,	O
we	O
can	O
search	O
only	O
over	O
step	O
sizes	O
	O
that	O
yield	O
new	O
x	O
points	O
that	O
are	O
feasible	O
,	O
or	O
we	O
can	O
project	O
each	O
point	O
on	O
the	O
line	O
back	O
into	O
the	O
constraint	O
region	O
.	O
when	O
possible	O
,	O
this	O
method	O
can	O
be	O
made	O
more	O
eﬃcient	O
by	O
projecting	O
the	O
gradient	O
into	O
the	O
tangent	O
space	O
of	O
the	O
feasible	O
region	O
before	O
taking	O
the	O
step	O
or	O
beginning	O
the	O
line	O
search	O
(	O
rosen	O
1960	O
)	O
.	O
,	O
a	O
more	O
sophisticated	O
approach	O
is	O
to	O
design	O
a	O
diﬀerent	O
,	O
unconstrained	O
opti-	O
mization	O
problem	O
whose	O
solution	O
can	O
be	O
converted	O
into	O
a	O
solution	O
to	O
the	O
original	O
,	O
constrained	O
optimization	O
problem	O
.	O
for	O
example	O
,	O
if	O
we	O
want	O
to	O
minimize	O
f	O
(	O
x	O
)	O
for	O
93	O
chapter	O
4.	O
numerical	O
computation	O
∈	O
	O
θ	O
]	O
r	O
θ	O
,	O
sin	O
2	O
with	O
x	O
constrained	O
to	O
have	O
exactly	O
unit	O
l2	O
norm	O
,	O
we	O
can	O
instead	O
minimize	O
x	O
g	O
(	O
θ	O
)	O
=	O
f	O
(	O
[	O
cos	O
θ	O
]	O
as	O
the	O
solution	O
to	O
the	O
original	O
problem	O
.	O
this	O
approach	O
requires	O
creativity	O
;	O
the	O
transformation	O
between	O
optimization	O
problems	O
must	O
be	O
designed	O
speciﬁcally	O
for	O
each	O
case	O
we	O
encounter.	O
)	O
with	O
respect	O
to	O
θ	O
,	O
then	O
return	O
[	O
cos	O
sin	O
θ	O
,	O
the	O
karush–kuhn–tucker	O
(	O
kkt	O
)	O
approach1	O
provides	O
a	O
very	O
general	O
so-	O
lution	O
to	O
constrained	O
optimization	O
.	O
with	O
the	O
kkt	O
approach	O
,	O
we	O
introduce	O
a	O
new	O
function	O
called	O
the	O
generalized	O
lagrangian	O
or	O
generalized	O
lagrange	O
function	O
.	O
	O
	O
to	O
deﬁne	O
the	O
lagrangian	O
,	O
we	O
ﬁrst	O
need	O
to	O
describe	O
s	O
in	O
terms	O
of	O
equations	O
and	O
inequalities	O
.	O
we	O
want	O
a	O
description	O
of	O
s	O
in	O
terms	O
of	O
m	O
functions	O
g	O
(	O
)	O
i	O
and	O
n	O
functions	O
h	O
(	O
)	O
j	O
.	O
the	O
equations	O
involving	O
g	O
(	O
)	O
i	O
are	O
called	O
the	O
equality	O
constraints	O
and	O
the	O
inequalities	O
involving	O
h	O
(	O
)	O
j	O
are	O
called	O
∀	O
j	O
,	O
h	O
(	O
)	O
j	O
(	O
x	O
)	O
.	O
inequality	O
constraints	O
i	O
,	O
g	O
(	O
)	O
i	O
(	O
x	O
)	O
=	O
0	O
and	O
{	O
|	O
∀	O
x	O
so	O
that	O
s	O
=	O
}	O
0	O
≤	O
we	O
introduce	O
new	O
variables	O
λi	O
and	O
α	O
j	O
for	O
each	O
constraint	O
,	O
these	O
are	O
called	O
the	O
kkt	O
multipliers	O
.	O
the	O
generalized	O
lagrangian	O
is	O
then	O
deﬁned	O
as	O
l	O
,	O
(	O
x	O
λ	O
α	O
)	O
=	O
(	O
)	O
+x	O
f	O
,	O
λi	O
g	O
(	O
)	O
i	O
(	O
)	O
+x	O
αj	O
h	O
(	O
)	O
j	O
(	O
)	O
x	O
.	O
(	O
4.14	O
)	O
i	O
j	O
we	O
can	O
now	O
solve	O
a	O
constrained	O
minimization	O
problem	O
using	O
unconstrained	O
optimization	O
of	O
the	O
generalized	O
lagrangian	O
.	O
observe	O
that	O
,	O
so	O
long	O
as	O
at	O
least	O
one	O
feasible	O
point	O
exists	O
and	O
is	O
not	O
permitted	O
to	O
have	O
value	O
,	O
then	O
f	O
(	O
)	O
x	O
∞	O
min	O
x	O
max	O
λ	O
≥	O
max	O
α	O
α	O
,	O
0	O
l	O
,	O
(	O
x	O
λ	O
α	O
)	O
.	O
,	O
(	O
4.15	O
)	O
has	O
the	O
same	O
optimal	O
objective	O
function	O
value	O
and	O
set	O
of	O
optimal	O
points	O
asx	O
this	O
follows	O
because	O
any	O
time	O
the	O
constraints	O
are	O
satisﬁed	O
,	O
f	O
.	O
(	O
)	O
x	O
∈	O
min	O
x	O
s	O
max	O
λ	O
≥	O
max	O
α	O
α	O
,	O
0	O
l	O
,	O
(	O
x	O
λ	O
α	O
)	O
=	O
(	O
)	O
x	O
,	O
f	O
,	O
while	O
any	O
time	O
a	O
constraint	O
is	O
violated	O
,	O
max	O
λ	O
≥	O
max	O
α	O
α	O
,	O
0	O
l	O
,	O
(	O
x	O
λ	O
α	O
)	O
=	O
,	O
∞	O
.	O
(	O
4.16	O
)	O
(	O
4.17	O
)	O
(	O
4.18	O
)	O
1the	O
kkt	O
approach	O
generalizes	O
the	O
method	O
of	O
lagrange	O
multipliers	O
which	O
allows	O
equality	O
constraints	O
but	O
not	O
inequality	O
constraints	O
.	O
94	O
chapter	O
4.	O
numerical	O
computation	O
these	O
properties	O
guarantee	O
that	O
no	O
infeasible	O
point	O
can	O
be	O
optimal	O
,	O
and	O
that	O
the	O
optimum	O
within	O
the	O
feasible	O
points	O
is	O
unchanged	O
.	O
to	O
perform	O
constrained	O
maximization	O
,	O
we	O
can	O
construct	O
the	O
generalized	O
la-	O
grange	O
function	O
of	O
,	O
which	O
leads	O
to	O
this	O
optimization	O
problem	O
:	O
−	O
f	O
(	O
)	O
x	O
−	O
f	O
(	O
)	O
+x	O
min	O
max	O
x	O
λ	O
≥	O
max	O
0	O
α	O
α	O
,	O
	O
	O
i	O
λ	O
ig	O
(	O
)	O
i	O
(	O
)	O
+x	O
αjh	O
(	O
)	O
j	O
(	O
)	O
x	O
.	O
(	O
4.19	O
)	O
	O
	O
j	O
−	O
we	O
may	O
also	O
convert	O
this	O
to	O
a	O
problem	O
with	O
maximization	O
in	O
the	O
outer	O
loop	O
:	O
max	O
x	O
min	O
λ	O
≥	O
min	O
0	O
α	O
α	O
,	O
f	O
(	O
)	O
+x	O
λig	O
(	O
)	O
i	O
(	O
)	O
x	O
αjh	O
(	O
)	O
j	O
(	O
)	O
x	O
.	O
(	O
4.20	O
)	O
i	O
j	O
the	O
sign	O
of	O
the	O
term	O
for	O
the	O
equality	O
constraints	O
does	O
not	O
matter	O
;	O
we	O
may	O
deﬁne	O
it	O
with	O
addition	O
or	O
subtraction	O
as	O
we	O
wish	O
,	O
because	O
the	O
optimization	O
is	O
free	O
to	O
choose	O
any	O
sign	O
for	O
each	O
λi	O
.	O
the	O
inequality	O
constraints	O
are	O
particularly	O
interesting	O
.	O
we	O
say	O
that	O
a	O
constraint	O
∗	O
h	O
(	O
)	O
i	O
(	O
x	O
)	O
is	O
active	O
if	O
h	O
(	O
)	O
i	O
(	O
x	O
)	O
=	O
0.	O
if	O
a	O
constraint	O
is	O
not	O
active	O
,	O
then	O
the	O
solution	O
to	O
the	O
problem	O
found	O
using	O
that	O
constraint	O
would	O
remain	O
at	O
least	O
a	O
local	O
solution	O
if	O
that	O
constraint	O
were	O
removed	O
.	O
it	O
is	O
possible	O
that	O
an	O
inactive	O
constraint	O
excludes	O
other	O
solutions	O
.	O
for	O
example	O
,	O
a	O
convex	O
problem	O
with	O
an	O
entire	O
region	O
of	O
globally	O
optimal	O
points	O
(	O
a	O
wide	O
,	O
ﬂat	O
,	O
region	O
of	O
equal	O
cost	O
)	O
could	O
have	O
a	O
subset	O
of	O
this	O
region	O
eliminated	O
by	O
constraints	O
,	O
or	O
a	O
non-convex	O
problem	O
could	O
have	O
better	O
local	O
stationary	O
points	O
excluded	O
by	O
a	O
constraint	O
that	O
is	O
inactive	O
at	O
convergence	O
.	O
however	O
,	O
the	O
point	O
found	O
at	O
convergence	O
remains	O
a	O
stationary	O
point	O
whether	O
or	O
not	O
the	O
inactive	O
constraints	O
are	O
included	O
.	O
because	O
an	O
inactive	O
h	O
(	O
)	O
i	O
has	O
negative	O
value	O
,	O
then	O
≥	O
	O
the	O
solution	O
to	O
minx	O
maxλ	O
max	O
α	O
α	O
,	O
)	O
will	O
have	O
αi	O
=	O
0.	O
we	O
can	O
thus	O
0	O
l	O
(	O
x	O
λ	O
α	O
,	O
(	O
x	O
)	O
=	O
0.	O
in	O
other	O
words	O
,	O
for	O
all	O
i	O
,	O
we	O
know	O
observe	O
that	O
at	O
the	O
solution	O
,	O
α	O
h	O
that	O
at	O
least	O
one	O
of	O
the	O
constraints	O
αi	O
0	O
must	O
be	O
active	O
at	O
the	O
solution	O
.	O
to	O
gain	O
some	O
intuition	O
for	O
this	O
idea	O
,	O
we	O
can	O
say	O
that	O
either	O
the	O
solution	O
is	O
on	O
the	O
boundary	O
imposed	O
by	O
the	O
inequality	O
and	O
we	O
must	O
use	O
its	O
kkt	O
multiplier	O
to	O
inﬂuence	O
the	O
solution	O
to	O
x	O
,	O
or	O
the	O
inequality	O
has	O
no	O
inﬂuence	O
on	O
the	O
solution	O
and	O
we	O
represent	O
this	O
by	O
zeroing	O
out	O
its	O
kkt	O
multiplier	O
.	O
0	O
and	O
h	O
(	O
)	O
i	O
(	O
x	O
)	O
≥	O
≤	O
,	O
a	O
simple	O
set	O
of	O
properties	O
describe	O
the	O
optimal	O
points	O
of	O
constrained	O
opti-	O
mization	O
problems	O
.	O
these	O
properties	O
are	O
called	O
the	O
karush-kuhn-tucker	O
(	O
kkt	O
)	O
conditions	B
(	O
)	O
.	O
they	O
are	O
necessary	O
conditions	B
,	O
but	O
not	O
always	O
suﬃcient	O
conditions	B
,	O
for	O
a	O
point	O
to	O
be	O
optimal	O
.	O
the	O
conditions	B
are	O
:	O
karush	O
1939	O
kuhn	O
and	O
tucker	O
1951	O
,	O
,	O
;	O
•	O
•	O
the	O
gradient	O
of	O
the	O
generalized	O
lagrangian	O
is	O
zero	O
.	O
all	O
constraints	O
on	O
both	O
x	O
and	O
the	O
kkt	O
multipliers	O
are	O
satisﬁed	O
.	O
95	O
chapter	O
4.	O
numerical	O
computation	O
•	O
	O
the	O
inequality	O
constraints	O
exhibit	O
“	O
complementary	O
slackness	O
”	O
:	O
α	O
h	O
(	O
x	O
)	O
=	O
0.	O
for	O
more	O
information	O
about	O
the	O
kkt	O
approach	O
,	O
see	O
nocedal	O
and	O
wright	O
2006	O
(	O
)	O
.	O
4.5	O
example	O
:	O
linear	O
least	O
squares	O
suppose	O
we	O
want	O
to	O
ﬁnd	O
the	O
value	O
of	O
x	O
f	O
(	O
)	O
=x	O
1	O
2	O
that	O
minimizes	O
−	O
||	O
||	O
ax	O
b	O
2	O
2	O
.	O
(	O
4.21	O
)	O
there	O
are	O
specialized	O
linear	O
algebra	O
algorithms	O
that	O
can	O
solve	O
this	O
problem	O
eﬃciently	O
.	O
however	O
,	O
we	O
can	O
also	O
explore	O
how	O
to	O
solve	O
it	O
using	O
gradient-based	O
optimization	O
as	O
a	O
simple	O
example	O
of	O
how	O
these	O
techniques	O
work	B
.	O
first	O
,	O
we	O
need	O
to	O
obtain	O
the	O
gradient	O
:	O
−	O
ax	O
b	O
(	O
	O
x	O
a	O
∇	O
x	O
f	O
(	O
)	O
=	O
	O
a	O
−	O
	O
ax	O
a	O
b	O
.	O
(	O
4.22	O
)	O
)	O
=	O
we	O
can	O
then	O
follow	O
this	O
gradient	O
downhill	O
,	O
taking	O
small	O
steps	O
.	O
see	O
algorithm	O
4.1	O
for	O
details	O
.	O
algorithm	O
4.1	O
an	O
algorithm	O
to	O
minimize	O
f	O
(	O
x	O
)	O
=	O
1	O
2	O
using	O
gradient	O
descent	B
,	O
starting	O
from	O
an	O
arbitrary	O
value	O
of	O
.x	O
2	O
with	O
respect	O
to	O
x	O
	O
	O
−	O
||	O
||	O
ax	O
b	O
2	O
set	O
the	O
step	O
size	O
(	O
)	O
and	O
tolerance	O
(	O
)	O
to	O
small	O
,	O
positive	O
numbers	O
.	O
while	O
δ	O
2	O
>	O
δ	O
do	O
||	O
	O
←	O
−	O
a	O
||	O
−	O
	O
	O
−	O
	O
b	O
ax	O
a	O
	O
ax	O
a	O
	O
a	O
b	O
x	O
x	O
end	O
while	O
one	O
can	O
also	O
solve	O
this	O
problem	O
using	O
newton	O
’	O
s	O
method	O
.	O
in	O
this	O
case	O
,	O
because	O
the	O
true	O
function	O
is	O
quadratic	O
,	O
the	O
quadratic	O
approximation	O
employed	O
by	O
newton	O
’	O
s	O
method	O
is	O
exact	O
,	O
and	O
the	O
algorithm	O
converges	O
to	O
the	O
global	O
minimum	O
in	O
a	O
single	O
step	O
.	O
now	O
suppose	O
we	O
wish	O
to	O
minimize	O
the	O
same	O
function	O
,	O
but	O
subject	O
to	O
the	O
	O
	O
	O
constraint	O
x	O
x	O
≤	O
1.	O
to	O
do	O
so	O
,	O
we	O
introduce	O
the	O
lagrangian	O
l	O
,	O
λ	O
(	O
x	O
)	O
=	O
(	O
)	O
+x	O
f	O
	O
x	O
x	O
λ	O
−	O
1	O
.	O
(	O
4.23	O
)	O
we	O
can	O
now	O
solve	O
the	O
problem	O
min	O
x	O
≥	O
max	O
λ	O
,	O
λ	O
0	O
l	O
,	O
λ	O
.	O
(	O
x	O
)	O
96	O
(	O
4.24	O
)	O
chapter	O
4.	O
numerical	O
computation	O
the	O
smallest-norm	O
solution	O
to	O
the	O
unconstrained	O
least	O
squares	O
problem	O
may	O
be	O
found	O
using	O
the	O
moore-penrose	O
pseudoinverse	O
:	O
x	O
=	O
a+	O
b.	O
if	O
this	O
point	O
is	O
feasible	O
,	O
then	O
it	O
is	O
the	O
solution	O
to	O
the	O
constrained	O
problem	O
.	O
otherwise	O
,	O
we	O
must	O
ﬁnd	O
a	O
solution	O
where	O
the	O
constraint	O
is	O
active	O
.	O
by	O
diﬀerentiating	O
the	O
lagrangian	O
with	O
respect	O
to	O
,	O
we	O
obtain	O
the	O
equation	O
x	O
	O
a	O
−	O
	O
ax	O
a	O
b	O
x+	O
2λ	O
=	O
0.	O
this	O
tells	O
us	O
that	O
the	O
solution	O
will	O
take	O
the	O
form	O
−	O
1a	O
x	O
a=	O
(	O
i+	O
2λ	O
)	O
	O
a	O
	O
b	O
.	O
(	O
4.25	O
)	O
(	O
4.26	O
)	O
the	O
magnitude	O
of	O
λ	O
must	O
be	O
chosen	O
such	O
that	O
the	O
result	O
obeys	O
the	O
constraint	O
.	O
we	O
can	O
ﬁnd	O
this	O
value	O
by	O
performing	O
gradient	O
ascent	O
on	O
.	O
to	O
do	O
so	O
,	O
observe	O
λ	O
	O
l	O
,	O
λ	O
(	O
x	O
)	O
=	O
x	O
x	O
−	O
1	O
.	O
∂	O
∂λ	O
(	O
4.27	O
)	O
	O
when	O
the	O
norm	O
of	O
x	O
exceeds	O
1	O
,	O
this	O
derivative	O
is	O
positive	O
,	O
so	O
to	O
follow	O
the	O
derivative	O
uphill	O
and	O
increase	O
the	O
lagrangian	O
with	O
respect	O
to	O
λ	O
,	O
we	O
increase	O
λ.	O
because	O
the	O
coeﬃcient	O
on	O
the	O
x	O
x	O
penalty	O
has	O
increased	O
,	O
solving	O
the	O
linear	O
equation	O
for	O
x	O
will	O
now	O
yield	O
a	O
solution	O
with	O
smaller	O
norm	O
.	O
the	O
process	O
of	O
solving	O
the	O
linear	O
equation	O
and	O
adjusting	O
λ	O
continues	O
until	O
x	O
has	O
the	O
correct	O
norm	O
and	O
the	O
derivative	O
on	O
λ	O
is	O
0.	O
this	O
concludes	O
the	O
mathematical	O
preliminaries	O
that	O
we	O
use	O
to	O
develop	O
machine	O
learning	O
algorithms	O
.	O
we	O
are	O
now	O
ready	O
to	O
build	O
and	O
analyze	O
some	O
full-ﬂedged	O
learning	O
systems	O
.	O
97	O
chapter	O
5	O
machine	O
learning	O
basics	O
deep	O
learning	O
is	O
a	O
speciﬁc	O
kind	O
of	O
machine	O
learning	O
.	O
in	O
order	O
to	O
understand	O
deep	O
learning	O
well	O
,	O
one	O
must	O
have	O
a	O
solid	O
understanding	O
of	O
the	O
basic	O
principles	O
of	O
machine	O
learning	O
.	O
this	O
chapter	O
provides	O
a	O
brief	O
course	O
in	O
the	O
most	O
important	O
general	O
principles	O
that	O
will	O
be	O
applied	O
throughout	O
the	O
rest	O
of	O
the	O
book	O
.	O
novice	O
readers	O
or	O
those	O
who	O
want	O
a	O
wider	O
perspective	O
are	O
encouraged	O
to	O
consider	O
machine	O
learning	O
textbooks	O
with	O
a	O
more	O
comprehensive	O
coverage	O
of	O
the	O
fundamentals	O
,	O
such	O
as	O
murphy	O
)	O
.	O
if	O
you	O
are	O
already	O
familiar	O
with	O
machine	O
learning	O
basics	O
,	O
(	O
2012	O
feel	O
free	O
to	O
skip	O
ahead	O
to	O
section	O
.	O
that	O
section	O
covers	O
some	O
perspectives	O
on	O
traditional	O
machine	O
learning	O
techniques	O
that	O
have	O
strongly	O
inﬂuenced	O
the	O
development	O
of	O
deep	O
learning	O
algorithms	O
.	O
bishop	O
2006	O
5.11	O
)	O
or	O
(	O
we	O
begin	O
with	O
a	O
deﬁnition	O
of	O
what	O
a	O
learning	O
algorithm	O
is	O
,	O
and	O
present	O
an	O
example	O
:	O
the	O
linear	O
regression	O
algorithm	O
.	O
we	O
then	O
proceed	O
to	O
describe	O
how	O
the	O
challenge	O
of	O
ﬁtting	O
the	O
training	O
data	O
diﬀers	O
from	O
the	O
challenge	O
of	O
ﬁnding	O
patterns	O
that	O
generalize	O
to	O
new	O
data	O
.	O
most	O
machine	O
learning	O
algorithms	O
have	O
settings	O
called	O
hyperparameters	O
that	O
must	O
be	O
determined	O
external	O
to	O
the	O
learning	O
algorithm	O
itself	O
;	O
we	O
discuss	O
how	O
to	O
set	O
these	O
using	O
additional	O
data	O
.	O
machine	O
learning	O
is	O
essentially	O
a	O
form	O
of	O
applied	O
statistics	O
with	O
increased	O
emphasis	O
on	O
the	O
use	O
of	O
computers	O
to	O
statistically	O
estimate	O
complicated	O
functions	O
and	O
a	O
decreased	O
emphasis	O
on	O
proving	O
conﬁdence	O
intervals	O
around	O
these	O
functions	O
;	O
we	O
therefore	O
present	O
the	O
two	O
central	O
approaches	O
to	O
statistics	O
:	O
frequentist	O
estimators	O
and	O
bayesian	O
inference	O
.	O
most	O
machine	O
learning	O
algorithms	O
can	O
be	O
divided	O
into	O
the	O
categories	O
of	O
supervised	O
learning	O
and	O
unsupervised	O
learning	O
;	O
we	O
describe	O
these	O
categories	O
and	O
give	O
some	O
examples	O
of	O
simple	O
learning	O
algorithms	O
from	O
each	O
category	O
.	O
most	O
deep	O
learning	O
algorithms	O
are	O
based	O
on	O
an	O
optimization	O
algorithm	O
called	O
stochastic	O
gradient	O
descent	B
.	O
we	O
describe	O
how	O
to	O
combine	O
various	O
algorithm	O
components	O
such	O
as	O
98	O
chapter	O
5.	O
machine	O
learning	O
basics	O
an	O
optimization	O
algorithm	O
,	O
a	O
cost	O
function	O
,	O
a	O
model	B
,	O
and	O
a	O
dataset	O
to	O
build	O
a	O
machine	O
learning	O
algorithm	O
.	O
finally	O
,	O
in	O
section	O
,	O
we	O
describe	O
some	O
of	O
the	O
factors	O
that	O
have	O
limited	O
the	O
ability	O
of	O
traditional	O
machine	O
learning	O
to	O
generalize	O
.	O
these	O
challenges	O
have	O
motivated	O
the	O
development	O
of	O
deep	O
learning	O
algorithms	O
that	O
overcome	O
these	O
obstacles	O
.	O
5.11	O
5.1	O
learning	O
algorithms	O
(	O
a	O
machine	O
learning	O
algorithm	O
is	O
an	O
algorithm	O
that	O
is	O
able	O
to	O
learn	O
from	O
data	O
.	O
but	O
what	O
do	O
we	O
mean	O
by	O
learning	O
?	O
mitchell	O
1997	O
)	O
provides	O
the	O
deﬁnition	O
“	O
a	O
computer	O
program	O
is	O
said	O
to	O
learn	O
from	O
experience	O
e	O
with	O
respect	O
to	O
some	O
class	O
of	O
tasks	O
t	O
and	O
performance	O
measure	O
p	O
,	O
if	O
its	O
performance	O
at	O
tasks	O
in	O
t	O
,	O
as	O
measured	O
by	O
p	O
,	O
improves	O
with	O
experience	O
e.	O
”	O
one	O
can	O
imagine	O
a	O
very	O
wide	O
variety	O
of	O
experiences	O
e	O
,	O
tasks	O
t	O
,	O
and	O
performance	O
measures	O
p	O
,	O
and	O
we	O
do	O
not	O
make	O
any	O
attempt	O
in	O
this	O
book	O
to	O
provide	O
a	O
formal	O
deﬁnition	O
of	O
what	O
may	O
be	O
used	O
for	O
each	O
of	O
these	O
entities	O
.	O
instead	O
,	O
the	O
following	O
sections	O
provide	O
intuitive	O
descriptions	O
and	O
examples	O
of	O
the	O
diﬀerent	O
kinds	O
of	O
tasks	O
,	O
performance	O
measures	O
and	O
experiences	O
that	O
can	O
be	O
used	O
to	O
construct	O
machine	O
learning	O
algorithms	O
.	O
5.1.1	O
the	O
task	O
,	O
t	O
machine	O
learning	O
allows	O
us	O
to	O
tackle	O
tasks	O
that	O
are	O
too	O
diﬃcult	O
to	O
solve	O
with	O
ﬁxed	O
programs	O
written	O
and	O
designed	O
by	O
human	O
beings	O
.	O
from	O
a	O
scientiﬁc	O
and	O
philosophical	O
point	O
of	O
view	O
,	O
machine	O
learning	O
is	O
interesting	O
because	O
developing	O
our	O
understanding	O
of	O
machine	O
learning	O
entails	O
developing	O
our	O
understanding	O
of	O
the	O
principles	O
that	O
underlie	O
intelligence	O
.	O
in	O
this	O
relatively	O
formal	O
deﬁnition	O
of	O
the	O
word	O
“	O
task	O
,	O
”	O
the	O
process	O
of	O
learning	O
itself	O
is	O
not	O
the	O
task	O
.	O
learning	O
is	O
our	O
means	O
of	O
attaining	O
the	O
ability	O
to	O
perform	O
the	O
task	O
.	O
for	O
example	O
,	O
if	O
we	O
want	O
a	O
robot	O
to	O
be	O
able	O
to	O
walk	O
,	O
then	O
walking	O
is	O
the	O
task	O
.	O
we	O
could	O
program	O
the	O
robot	O
to	O
learn	O
to	O
walk	O
,	O
or	O
we	O
could	O
attempt	O
to	O
directly	O
write	O
a	O
program	O
that	O
speciﬁes	O
how	O
to	O
walk	O
manually	O
.	O
machine	O
learning	O
tasks	O
are	O
usually	O
described	O
in	O
terms	O
of	O
how	O
the	O
machine	O
learning	O
system	O
should	O
process	O
an	O
example	O
.	O
an	O
example	O
is	O
a	O
collection	O
of	O
features	O
that	O
have	O
been	O
quantitatively	O
measured	O
from	O
some	O
object	O
or	O
event	O
that	O
we	O
want	O
the	O
machine	O
learning	O
system	O
to	O
process	O
.	O
we	O
typically	O
represent	O
an	O
example	O
as	O
a	O
n	O
where	O
each	O
entry	O
xi	O
of	O
the	O
vector	O
is	O
another	O
feature	O
.	O
for	O
example	O
,	O
vector	O
x	O
the	O
features	O
of	O
an	O
image	O
are	O
usually	O
the	O
values	O
of	O
the	O
pixels	O
in	O
the	O
image	O
.	O
∈	O
r	O
99	O
chapter	O
5.	O
machine	O
learning	O
basics	O
many	O
kinds	O
of	O
tasks	O
can	O
be	O
solved	O
with	O
machine	O
learning	O
.	O
some	O
of	O
the	O
most	O
common	O
machine	O
learning	O
tasks	O
include	O
the	O
following	O
:	O
•	O
•	O
→	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
classiﬁcation	O
:	O
in	O
this	O
type	O
of	O
task	O
,	O
the	O
computer	O
program	O
is	O
asked	O
to	O
specify	O
}	O
which	O
of	O
k	O
categories	O
some	O
input	O
belongs	O
to	O
.	O
to	O
solve	O
this	O
task	O
,	O
the	O
learning	O
n	O
algorithm	O
is	O
usually	O
asked	O
to	O
produce	O
a	O
function	O
f	O
:	O
r	O
.	O
when	O
y	O
=	O
f	O
(	O
x	O
)	O
,	O
the	O
model	B
assigns	O
an	O
input	O
described	O
by	O
vector	O
x	O
to	O
a	O
category	O
identiﬁed	O
by	O
numeric	O
code	O
y.	O
there	O
are	O
other	O
variants	O
of	O
the	O
classiﬁcation	O
task	O
,	O
for	O
example	O
,	O
where	O
f	O
outputs	O
a	O
probability	O
distribution	O
over	O
classes	O
.	O
an	O
example	O
of	O
a	O
classiﬁcation	O
task	O
is	O
object	O
recognition	B
,	O
where	O
the	O
input	O
is	O
an	O
image	O
(	O
usually	O
described	O
as	O
a	O
set	O
of	O
pixel	O
brightness	O
values	O
)	O
,	O
and	O
the	O
output	O
is	O
a	O
numeric	O
code	O
identifying	O
the	O
object	O
in	O
the	O
image	O
.	O
for	O
example	O
,	O
the	O
willow	O
garage	O
pr2	O
robot	O
is	O
able	O
to	O
act	O
as	O
a	O
waiter	O
that	O
can	O
recognize	O
diﬀerent	O
kinds	O
of	O
drinks	O
and	O
deliver	O
them	O
to	O
people	O
on	O
command	O
(	O
good-	O
)	O
.	O
modern	O
object	O
recognition	B
is	O
best	O
accomplished	O
with	O
fellow	O
deep	O
learning	O
(	O
)	O
.	O
object	O
recognition	B
is	O
the	O
same	O
basic	O
technology	O
that	O
allows	O
computers	O
to	O
recognize	O
faces	O
(	O
taigman	O
)	O
,	O
which	O
can	O
be	O
used	O
to	O
automatically	O
tag	O
people	O
in	O
photo	O
collections	O
and	O
allow	O
computers	O
to	O
interact	O
more	O
naturally	O
with	O
their	O
users	O
.	O
krizhevsky	O
et	O
al	O
.	O
2012	O
ioﬀe	O
and	O
szegedy	O
2015	O
et	O
al.	O
,	O
2010	O
et	O
al.	O
,	O
2014	O
,	O
;	O
,	O
set	O
single	O
classiﬁcation	O
with	O
missing	O
inputs	O
:	O
classiﬁcation	O
becomes	O
more	O
chal-	O
lenging	O
if	O
the	O
computer	O
program	O
is	O
not	O
guaranteed	O
that	O
every	O
measurement	O
in	O
its	O
input	O
vector	O
will	O
always	O
be	O
provided	O
.	O
in	O
order	O
to	O
solve	O
the	O
classiﬁcation	O
task	O
,	O
the	O
learning	O
algorithm	O
only	O
has	O
to	O
deﬁne	O
a	O
function	O
mapping	O
from	O
a	O
vector	O
input	O
to	O
a	O
categorical	O
output	O
.	O
when	O
some	O
of	O
the	O
inputs	O
may	O
be	O
missing	O
,	O
rather	O
than	O
providing	O
a	O
single	O
classiﬁcation	O
function	O
,	O
the	O
learning	O
algorithm	O
must	O
learn	O
a	O
of	O
functions	O
.	O
each	O
function	O
corresponds	O
to	O
classi-	O
fying	O
x	O
with	O
a	O
diﬀerent	O
subset	O
of	O
its	O
inputs	O
missing	O
.	O
this	O
kind	O
of	O
situation	O
arises	O
frequently	O
in	O
medical	O
diagnosis	O
,	O
because	O
many	O
kinds	O
of	O
medical	O
tests	O
are	O
expensive	O
or	O
invasive	O
.	O
one	O
way	O
to	O
eﬃciently	O
deﬁne	O
such	O
a	O
large	O
set	O
of	O
functions	O
is	O
to	O
learn	O
a	O
probability	O
distribution	O
over	O
all	O
of	O
the	O
relevant	O
variables	O
,	O
then	O
solve	O
the	O
classiﬁcation	O
task	O
by	O
marginalizing	O
out	O
the	O
missing	O
variables	O
.	O
with	O
n	O
input	O
variables	O
,	O
we	O
can	O
now	O
obtain	O
all	O
2n	O
diﬀerent	O
classiﬁ-	O
cation	O
functions	O
needed	O
for	O
each	O
possible	O
set	O
of	O
missing	O
inputs	O
,	O
but	O
we	O
only	O
need	O
to	O
learn	O
a	O
single	O
function	O
describing	O
the	O
joint	O
probability	O
distribution	O
.	O
see	O
goodfellow	O
)	O
for	O
an	O
example	O
of	O
a	O
deep	O
probabilistic	O
model	B
applied	O
to	O
such	O
a	O
task	O
in	O
this	O
way	O
.	O
many	O
of	O
the	O
other	O
tasks	O
described	O
in	O
this	O
section	O
can	O
also	O
be	O
generalized	O
to	O
work	B
with	O
missing	O
inputs	O
;	O
classiﬁcation	O
with	O
missing	O
inputs	O
is	O
just	O
one	O
example	O
of	O
what	O
machine	O
learning	O
can	O
do	O
.	O
et	O
al	O
.	O
(	O
2013b	O
100	O
chapter	O
5.	O
machine	O
learning	O
basics	O
•	O
•	O
•	O
•	O
→	O
regression	O
:	O
in	O
this	O
type	O
of	O
task	O
,	O
the	O
computer	O
program	O
is	O
asked	O
to	O
predict	O
a	O
numerical	O
value	O
given	O
some	O
input	O
.	O
to	O
solve	O
this	O
task	O
,	O
the	O
learning	O
algorithm	O
n	O
is	O
asked	O
to	O
output	O
a	O
function	O
f	O
:	O
r	O
r.	O
this	O
type	O
of	O
task	O
is	O
similar	O
to	O
classiﬁcation	O
,	O
except	O
that	O
the	O
format	O
of	O
output	O
is	O
diﬀerent	O
.	O
an	O
example	O
of	O
a	O
regression	O
task	O
is	O
the	O
prediction	O
of	O
the	O
expected	O
claim	O
amount	O
that	O
an	O
insured	O
person	O
will	O
make	O
(	O
used	O
to	O
set	O
insurance	O
premiums	O
)	O
,	O
or	O
the	O
prediction	O
of	O
future	O
prices	O
of	O
securities	O
.	O
these	O
kinds	O
of	O
predictions	O
are	O
also	O
used	O
for	O
algorithmic	O
trading	O
.	O
transcription	O
:	O
in	O
this	O
type	O
of	O
task	O
,	O
the	O
machine	O
learning	O
system	O
is	O
asked	O
to	O
observe	O
a	O
relatively	O
unstructured	O
representation	O
of	O
some	O
kind	O
of	O
data	O
and	O
transcribe	O
it	O
into	O
discrete	O
,	O
textual	O
form	O
.	O
for	O
example	O
,	O
in	O
optical	O
character	O
recognition	B
,	O
the	O
computer	O
program	O
is	O
shown	O
a	O
photograph	O
containing	O
an	O
image	O
of	O
text	O
and	O
is	O
asked	O
to	O
return	O
this	O
text	O
in	O
the	O
form	O
of	O
a	O
sequence	O
of	O
characters	O
(	O
e.g.	O
,	O
in	O
ascii	O
or	O
unicode	O
format	O
)	O
.	O
google	O
street	O
view	O
uses	O
deep	O
learning	O
to	O
process	O
address	O
numbers	O
in	O
this	O
way	O
(	O
goodfellow	O
et	O
al	O
.	O
,	O
2014d	O
)	O
.	O
another	O
example	O
is	O
speech	O
recognition	B
,	O
where	O
the	O
computer	O
program	O
is	O
provided	O
an	O
audio	O
waveform	O
and	O
emits	O
a	O
sequence	O
of	O
characters	O
or	O
word	O
id	O
codes	O
describing	O
the	O
words	O
that	O
were	O
spoken	O
in	O
the	O
audio	O
recording	O
.	O
deep	O
learning	O
is	O
a	O
crucial	O
component	O
of	O
modern	O
speech	O
recognition	B
systems	O
used	O
at	O
major	O
companies	O
including	O
microsoft	O
,	O
ibm	O
and	O
google	O
(	O
hinton	O
et	O
al	O
.	O
,	O
2012b	O
)	O
.	O
machine	O
translation	O
:	O
in	O
a	O
machine	O
translation	O
task	O
,	O
the	O
input	O
already	O
consists	O
of	O
a	O
sequence	O
of	O
symbols	O
in	O
some	O
language	O
,	O
and	O
the	O
computer	O
program	O
must	O
convert	O
this	O
into	O
a	O
sequence	O
of	O
symbols	O
in	O
another	O
language	O
.	O
this	O
is	O
commonly	O
applied	O
to	O
natural	O
languages	O
,	O
such	O
as	O
translating	O
from	O
english	O
to	O
french	O
.	O
deep	O
learning	O
has	O
recently	O
begun	O
to	O
have	O
an	O
important	O
impact	O
on	O
this	O
kind	O
of	O
task	O
(	O
sutskever	O
2014	O
bahdanau	O
et	O
al.	O
,	O
et	O
al.	O
,	O
;	O
2015	O
)	O
.	O
structured	O
output	O
:	O
structured	O
output	O
tasks	O
involve	O
any	O
task	O
where	O
the	O
output	O
is	O
a	O
vector	O
(	O
or	O
other	O
data	O
structure	O
containing	O
multiple	O
values	O
)	O
with	O
important	O
relationships	O
between	O
the	O
diﬀerent	O
elements	O
.	O
this	O
is	O
a	O
broad	O
category	O
,	O
and	O
subsumes	O
the	O
transcription	O
and	O
translation	O
tasks	O
described	O
above	O
,	O
but	O
also	O
many	O
other	O
tasks	O
.	O
one	O
example	O
is	O
parsing—mapping	O
a	O
natural	O
language	O
sentence	O
into	O
a	O
tree	O
that	O
describes	O
its	O
grammatical	O
structure	O
and	O
tagging	O
nodes	O
of	O
the	O
trees	O
as	O
being	O
verbs	O
,	O
nouns	O
,	O
or	O
adverbs	O
,	O
and	O
so	O
on	O
.	O
see	O
)	O
for	O
an	O
example	O
of	O
deep	O
learning	O
applied	O
to	O
a	O
parsing	O
task	O
.	O
another	O
example	O
is	O
pixel-wise	O
segmentation	O
of	O
images	O
,	O
where	O
the	O
computer	O
program	O
assigns	O
every	O
pixel	O
in	O
an	O
image	O
to	O
a	O
speciﬁc	O
category	O
.	O
for	O
collobert	O
2011	O
(	O
101	O
chapter	O
5.	O
machine	O
learning	O
basics	O
,	O
example	O
,	O
deep	O
learning	O
can	O
be	O
used	O
to	O
annotate	O
the	O
locations	O
of	O
roads	O
in	O
aerial	O
photographs	O
(	O
mnih	O
and	O
hinton	O
2010	O
)	O
.	O
the	O
output	O
need	O
not	O
have	O
its	O
form	O
mirror	O
the	O
structure	O
of	O
the	O
input	O
as	O
closely	O
as	O
in	O
these	O
annotation-style	O
tasks	O
.	O
for	O
example	O
,	O
in	O
image	O
captioning	O
,	O
the	O
computer	O
program	O
observes	O
an	O
image	O
and	O
outputs	O
a	O
natural	O
language	O
sentence	O
describing	O
the	O
image	O
(	O
kiros	O
et	O
al	O
.	O
2014	O
;	O
,	O
)	O
.	O
these	O
tasks	O
are	O
karpathy	O
and	O
li	O
2015	O
fang	O
called	O
structured	O
output	O
tasks	O
because	O
the	O
program	O
must	O
output	O
several	O
values	O
that	O
are	O
all	O
tightly	O
inter-related	O
.	O
for	O
example	O
,	O
the	O
words	O
produced	O
by	O
an	O
image	O
captioning	O
program	O
must	O
form	O
a	O
valid	O
sentence	O
.	O
2015b	O
donahue	O
2014a	O
b	O
mao	O
2015	O
vinyals	O
et	O
al.	O
,	O
;	O
;	O
2015	O
et	O
al.	O
,	O
2015	O
xu	O
et	O
al.	O
,	O
et	O
al	O
.	O
,	O
;	O
et	O
al.	O
,	O
,	O
;	O
,	O
;	O
•	O
•	O
•	O
anomaly	O
detection	O
:	O
in	O
this	O
type	O
of	O
task	O
,	O
the	O
computer	O
program	O
sifts	O
through	O
a	O
set	O
of	O
events	O
or	O
objects	O
,	O
and	O
ﬂags	O
some	O
of	O
them	O
as	O
being	O
unusual	O
or	O
atypical	O
.	O
an	O
example	O
of	O
an	O
anomaly	O
detection	O
task	O
is	O
credit	O
card	O
fraud	O
detection	O
.	O
by	O
modeling	O
your	O
purchasing	O
habits	O
,	O
a	O
credit	O
card	O
company	O
can	O
detect	O
misuse	O
of	O
your	O
cards	O
.	O
if	O
a	O
thief	O
steals	O
your	O
credit	O
card	O
or	O
credit	O
card	O
information	O
,	O
the	O
thief	O
’	O
s	O
purchases	O
will	O
often	O
come	O
from	O
a	O
diﬀerent	O
probability	O
distribution	O
over	O
purchase	O
types	O
than	O
your	O
own	O
.	O
the	O
credit	O
card	O
company	O
can	O
prevent	O
fraud	O
by	O
placing	O
a	O
hold	O
on	O
an	O
account	O
as	O
soon	O
as	O
that	O
card	O
has	O
been	O
used	O
for	O
an	O
uncharacteristic	O
purchase	O
.	O
see	O
)	O
for	O
a	O
survey	O
of	O
anomaly	O
detection	O
methods	O
.	O
chandola	O
et	O
al	O
.	O
2009	O
(	O
synthesis	O
and	O
sampling	O
:	O
in	O
this	O
type	O
of	O
task	O
,	O
the	O
machine	O
learning	O
al-	O
gorithm	O
is	O
asked	O
to	O
generate	O
new	O
examples	O
that	O
are	O
similar	O
to	O
those	O
in	O
the	O
training	O
data	O
.	O
synthesis	O
and	O
sampling	O
via	O
machine	O
learning	O
can	O
be	O
useful	O
for	O
media	O
applications	O
where	O
it	O
can	O
be	O
expensive	O
or	O
boring	O
for	O
an	O
artist	O
to	O
generate	O
large	O
volumes	O
of	O
content	O
by	O
hand	O
.	O
for	O
example	O
,	O
video	O
games	O
can	O
automatically	O
generate	O
textures	O
for	O
large	O
objects	O
or	O
landscapes	O
,	O
rather	O
than	O
requiring	O
an	O
artist	O
to	O
manually	O
label	O
each	O
pixel	O
(	O
)	O
.	O
in	O
some	O
cases	O
,	O
we	O
want	O
the	O
sampling	O
or	O
synthesis	O
procedure	O
to	O
generate	O
some	O
speciﬁc	O
kind	O
of	O
output	O
given	O
the	O
input	O
.	O
for	O
example	O
,	O
in	O
a	O
speech	O
synthesis	O
task	O
,	O
we	O
provide	O
a	O
written	O
sentence	O
and	O
ask	O
the	O
program	O
to	O
emit	O
an	O
audio	O
waveform	O
containing	O
a	O
spoken	O
version	O
of	O
that	O
sentence	O
.	O
this	O
is	O
a	O
kind	O
of	O
structured	O
output	O
task	O
,	O
but	O
with	O
the	O
added	O
qualiﬁcation	O
that	O
there	O
is	O
no	O
single	O
correct	O
output	O
for	O
each	O
input	O
,	O
and	O
we	O
explicitly	O
desire	O
a	O
large	O
amount	O
of	O
variation	O
in	O
the	O
output	O
,	O
in	O
order	O
for	O
the	O
output	O
to	O
seem	O
more	O
natural	O
and	O
realistic	O
.	O
luo	O
et	O
al	O
.	O
2013	O
,	O
imputation	O
of	O
missing	O
values	O
:	O
in	O
this	O
type	O
of	O
task	O
,	O
the	O
machine	O
learning	O
n	O
,	O
but	O
with	O
some	O
entries	O
xi	O
of	O
x	O
algorithm	O
is	O
given	O
a	O
new	O
example	O
x	O
missing	O
.	O
the	O
algorithm	O
must	O
provide	O
a	O
prediction	O
of	O
the	O
values	O
of	O
the	O
missing	O
entries	O
.	O
r	O
∈	O
102	O
chapter	O
5.	O
machine	O
learning	O
basics	O
•	O
•	O
∈	O
denoising	O
:	O
in	O
this	O
type	O
of	O
task	O
,	O
the	O
machine	O
learning	O
algorithm	O
is	O
given	O
in	O
∈	O
n	O
obtained	O
by	O
an	O
unknown	O
corruption	O
process	O
input	O
a	O
corrupted	O
example	O
˜x	O
n.	O
the	O
learner	O
must	O
predict	O
the	O
clean	O
example	O
from	O
a	O
clean	O
example	O
x	O
|	O
x	O
from	O
its	O
corrupted	O
version	O
˜x	O
,	O
or	O
more	O
generally	O
predict	O
the	O
conditional	O
probability	O
distribution	O
p	O
(	O
x	O
˜x	O
)	O
.	O
r	O
r	O
n	O
→	O
density	O
estimation	O
or	O
probability	O
mass	O
function	O
estimation	O
:	O
in	O
the	O
density	O
estimation	O
problem	O
,	O
the	O
machine	O
learning	O
algorithm	O
is	O
asked	O
to	O
learn	O
a	O
function	O
pmodel	O
:	O
r	O
r	O
,	O
where	O
pmodel	O
(	O
x	O
)	O
can	O
be	O
interpreted	O
as	O
a	O
probability	O
density	O
function	O
(	O
if	O
x	O
is	O
continuous	O
)	O
or	O
a	O
probability	O
mass	O
function	O
(	O
if	O
x	O
is	O
discrete	O
)	O
on	O
the	O
space	O
that	O
the	O
examples	O
were	O
drawn	O
from	O
.	O
to	O
do	O
such	O
a	O
task	O
well	O
(	O
we	O
will	O
specify	O
exactly	O
what	O
that	O
means	O
when	O
we	O
discuss	O
performance	O
measures	O
p	O
)	O
,	O
the	O
algorithm	O
needs	O
to	O
learn	O
the	O
structure	O
of	O
the	O
data	O
it	O
has	O
seen	O
.	O
it	O
must	O
know	O
where	O
examples	O
cluster	O
tightly	O
and	O
where	O
they	O
are	O
unlikely	O
to	O
occur	O
.	O
most	O
of	O
the	O
tasks	O
described	O
above	O
require	O
the	O
learning	O
algorithm	O
to	O
at	O
least	O
implicitly	O
capture	O
the	O
structure	O
of	O
the	O
probability	O
distribution	O
.	O
density	O
estimation	O
allows	O
us	O
to	O
explicitly	O
capture	O
that	O
distribution	O
.	O
in	O
principle	O
,	O
we	O
can	O
then	O
perform	O
computations	O
on	O
that	O
distribution	O
in	O
order	O
to	O
solve	O
the	O
other	O
tasks	O
as	O
well	O
.	O
for	O
example	O
,	O
if	O
we	O
have	O
performed	O
density	O
estimation	O
to	O
obtain	O
a	O
probability	O
distribution	O
p	O
(	O
x	O
)	O
,	O
we	O
can	O
use	O
that	O
distribution	O
to	O
solve	O
the	O
missing	O
value	O
imputation	O
task	O
.	O
if	O
a	O
value	O
xi	O
is	O
missing	O
and	O
all	O
of	O
the	O
other	O
values	O
,	O
denoted	O
x−	O
i	O
,	O
are	O
given	O
,	O
then	O
we	O
know	O
the	O
distribution	O
over	O
it	O
is	O
given	O
by	O
p	O
(	O
xi	O
i	O
)	O
.	O
in	O
practice	O
,	O
density	O
estimation	O
does	O
not	O
always	O
allow	O
us	O
to	O
solve	O
all	O
of	O
these	O
related	O
tasks	O
,	O
because	O
in	O
many	O
cases	O
the	O
required	O
operations	O
on	O
p	O
(	O
x	O
)	O
are	O
computationally	O
intractable	O
.	O
x−	O
|	O
of	O
course	O
,	O
many	O
other	O
tasks	O
and	O
types	O
of	O
tasks	O
are	O
possible	O
.	O
the	O
types	O
of	O
tasks	O
we	O
list	O
here	O
are	O
intended	O
only	O
to	O
provide	O
examples	O
of	O
what	O
machine	O
learning	O
can	O
do	O
,	O
not	O
to	O
deﬁne	O
a	O
rigid	O
taxonomy	O
of	O
tasks	O
.	O
5.1.2	O
the	O
performance	O
measure	O
,	O
p	O
in	O
order	O
to	O
evaluate	O
the	O
abilities	O
of	O
a	O
machine	O
learning	O
algorithm	O
,	O
we	O
must	O
design	O
a	O
quantitative	O
measure	O
of	O
its	O
performance	O
.	O
usually	O
this	O
performance	O
measure	O
p	O
is	O
speciﬁc	O
to	O
the	O
task	O
being	O
carried	O
out	O
by	O
the	O
system	O
.	O
t	O
for	O
tasks	O
such	O
as	O
classiﬁcation	O
,	O
classiﬁcation	O
with	O
missing	O
inputs	O
,	O
and	O
tran-	O
scription	O
,	O
we	O
often	O
measure	O
the	O
accuracy	O
of	O
the	O
model	B
.	O
accuracy	O
is	O
just	O
the	O
proportion	O
of	O
examples	O
for	O
which	O
the	O
model	B
produces	O
the	O
correct	O
output	O
.	O
we	O
can	O
103	O
chapter	O
5.	O
machine	O
learning	O
basics	O
also	O
obtain	O
equivalent	O
information	O
by	O
measuring	O
the	O
error	O
rate	O
,	O
the	O
proportion	O
of	O
examples	O
for	O
which	O
the	O
model	B
produces	O
an	O
incorrect	O
output	O
.	O
we	O
often	O
refer	O
to	O
the	O
error	O
rate	O
as	O
the	O
expected	O
0-1	B
loss	I
.	O
the	O
0-1	B
loss	I
on	O
a	O
particular	O
example	O
is	O
0	O
if	O
it	O
is	O
correctly	O
classiﬁed	O
and	O
1	O
if	O
it	O
is	O
not	O
.	O
for	O
tasks	O
such	O
as	O
density	O
estimation	O
,	O
it	O
does	O
not	O
make	O
sense	O
to	O
measure	O
accuracy	O
,	O
error	O
rate	O
,	O
or	O
any	O
other	O
kind	O
of	O
0-1	B
loss	I
.	O
instead	O
,	O
we	O
must	O
use	O
a	O
diﬀerent	O
performance	O
metric	O
that	O
gives	O
the	O
model	B
a	O
continuous-valued	O
score	O
for	O
each	O
example	O
.	O
the	O
most	O
common	O
approach	O
is	O
to	O
report	O
the	O
average	O
log-probability	O
the	O
model	B
assigns	O
to	O
some	O
examples	O
.	O
usually	O
we	O
are	O
interested	O
in	O
how	O
well	O
the	O
machine	O
learning	O
algorithm	O
performs	O
on	O
data	O
that	O
it	O
has	O
not	O
seen	O
before	O
,	O
since	O
this	O
determines	O
how	O
well	O
it	O
will	O
work	B
when	O
deployed	O
in	O
the	O
real	O
world	O
.	O
we	O
therefore	O
evaluate	O
these	O
performance	O
measures	O
using	O
a	O
test	O
set	O
of	O
data	O
that	O
is	O
separate	O
from	O
the	O
data	O
used	O
for	O
training	O
the	O
machine	O
learning	O
system	O
.	O
the	O
choice	O
of	O
performance	O
measure	O
may	O
seem	O
straightforward	O
and	O
objective	O
,	O
but	O
it	O
is	O
often	O
diﬃcult	O
to	O
choose	O
a	O
performance	O
measure	O
that	O
corresponds	O
well	O
to	O
the	O
desired	O
behavior	O
of	O
the	O
system	O
.	O
in	O
some	O
cases	O
,	O
this	O
is	O
because	O
it	O
is	O
diﬃcult	O
to	O
decide	O
what	O
should	O
be	O
measured	O
.	O
for	O
example	O
,	O
when	O
performing	O
a	O
transcription	O
task	O
,	O
should	O
we	O
measure	O
the	O
accuracy	O
of	O
the	O
system	O
at	O
transcribing	O
entire	O
sequences	O
,	O
or	O
should	O
we	O
use	O
a	O
more	O
ﬁne-grained	O
performance	O
measure	O
that	O
gives	O
partial	O
credit	O
for	O
getting	O
some	O
elements	O
of	O
the	O
sequence	O
correct	O
?	O
when	O
performing	O
a	O
regression	O
task	O
,	O
should	O
we	O
penalize	O
the	O
system	O
more	O
if	O
it	O
frequently	O
makes	O
medium-sized	O
mistakes	O
or	O
if	O
it	O
rarely	O
makes	O
very	O
large	O
mistakes	O
?	O
these	O
kinds	O
of	O
design	O
choices	O
depend	O
on	O
the	O
application	O
.	O
in	O
other	O
cases	O
,	O
we	O
know	O
what	O
quantity	O
we	O
would	O
ideally	O
like	O
to	O
measure	O
,	O
but	O
measuring	O
it	O
is	O
impractical	O
.	O
for	O
example	O
,	O
this	O
arises	O
frequently	O
in	O
the	O
context	O
of	O
density	O
estimation	O
.	O
many	O
of	O
the	O
best	O
probabilistic	O
models	O
represent	O
probability	O
distributions	O
only	O
implicitly	O
.	O
computing	O
the	O
actual	O
probability	O
value	O
assigned	O
to	O
a	O
speciﬁc	O
point	O
in	O
space	O
in	O
many	O
such	O
models	O
is	O
intractable	O
.	O
in	O
these	O
cases	O
,	O
one	O
must	O
design	O
an	O
alternative	O
criterion	O
that	O
still	O
corresponds	O
to	O
the	O
design	O
objectives	O
,	O
or	O
design	O
a	O
good	O
approximation	O
to	O
the	O
desired	O
criterion	O
.	O
5.1.3	O
the	O
experience	O
,	O
e	O
machine	O
learning	O
algorithms	O
can	O
be	O
broadly	O
categorized	O
as	O
unsupervised	O
or	O
supervised	O
by	O
what	O
kind	O
of	O
experience	O
they	O
are	O
allowed	O
to	O
have	O
during	O
the	O
learning	O
process	O
.	O
most	O
of	O
the	O
learning	O
algorithms	O
in	O
this	O
book	O
can	O
be	O
understood	O
as	O
being	O
allowed	O
to	O
experience	O
an	O
entire	O
dataset	O
.	O
a	O
dataset	O
is	O
a	O
collection	O
of	O
many	O
examples	O
,	O
as	O
104	O
chapter	O
5.	O
machine	O
learning	O
basics	O
deﬁned	O
in	O
section	O
5.1.1	O
.	O
sometimes	O
we	O
will	O
also	O
call	O
examples	O
.	O
data	O
points	O
fisher	O
1936	O
one	O
of	O
the	O
oldest	O
datasets	O
studied	O
by	O
statisticians	O
and	O
machine	O
learning	O
re-	O
searchers	O
is	O
the	O
iris	O
dataset	O
(	O
)	O
.	O
it	O
is	O
a	O
collection	O
of	O
measurements	O
of	O
diﬀerent	O
parts	O
of	O
150	O
iris	O
plants	O
.	O
each	O
individual	O
plant	O
corresponds	O
to	O
one	O
example	O
.	O
the	O
features	O
within	O
each	O
example	O
are	O
the	O
measurements	O
of	O
each	O
of	O
the	O
parts	O
of	O
the	O
plant	O
:	O
the	O
sepal	O
length	O
,	O
sepal	O
width	O
,	O
petal	O
length	O
and	O
petal	O
width	O
.	O
the	O
dataset	O
also	O
records	O
which	O
species	O
each	O
plant	O
belonged	O
to	O
.	O
three	O
diﬀerent	O
species	O
are	O
represented	O
in	O
the	O
dataset	O
.	O
,	O
unsupervised	O
learning	O
algorithms	O
experience	O
a	O
dataset	O
containing	O
many	O
features	O
,	O
then	O
learn	O
useful	O
properties	O
of	O
the	O
structure	O
of	O
this	O
dataset	O
.	O
in	O
the	O
context	O
of	O
deep	O
learning	O
,	O
we	O
usually	O
want	O
to	O
learn	O
the	O
entire	O
probability	O
distribution	O
that	O
generated	O
a	O
dataset	O
,	O
whether	O
explicitly	O
as	O
in	O
density	O
estimation	O
or	O
implicitly	O
for	O
tasks	O
like	O
synthesis	O
or	O
denoising	O
.	O
some	O
other	O
unsupervised	O
learning	O
algorithms	O
perform	O
other	O
roles	O
,	O
like	O
clustering	O
,	O
which	O
consists	O
of	O
dividing	O
the	O
dataset	O
into	O
clusters	O
of	O
similar	O
examples	O
.	O
supervised	O
learning	O
algorithms	O
experience	O
a	O
dataset	O
containing	O
features	O
,	O
but	O
each	O
example	O
is	O
also	O
associated	O
with	O
a	O
label	O
or	O
target	O
.	O
for	O
example	O
,	O
the	O
iris	O
dataset	O
is	O
annotated	O
with	O
the	O
species	O
of	O
each	O
iris	O
plant	O
.	O
a	O
supervised	O
learning	O
algorithm	O
can	O
study	O
the	O
iris	O
dataset	O
and	O
learn	O
to	O
classify	O
iris	O
plants	O
into	O
three	O
diﬀerent	O
species	O
based	O
on	O
their	O
measurements	O
.	O
roughly	O
speaking	O
,	O
unsupervised	O
learning	O
involves	O
observing	O
several	O
examples	O
of	O
a	O
random	O
vector	O
x	O
,	O
and	O
attempting	O
to	O
implicitly	O
or	O
explicitly	O
learn	O
the	O
proba-	O
bility	O
distribution	O
p	O
(	O
x	O
)	O
,	O
or	O
some	O
interesting	O
properties	O
of	O
that	O
distribution	O
,	O
while	O
supervised	O
learning	O
involves	O
observing	O
several	O
examples	O
of	O
a	O
random	O
vector	O
x	O
and	O
an	O
associated	O
value	O
or	O
vector	O
y	O
,	O
and	O
learning	O
to	O
predict	O
y	O
from	O
x	O
,	O
usually	O
by	O
estimating	O
p	O
(	O
y	O
x	O
)	O
.	O
the	O
term	O
supervised	O
learning	O
originates	O
from	O
the	O
view	O
of	O
the	O
target	O
y	O
being	O
provided	O
by	O
an	O
instructor	O
or	O
teacher	O
who	O
shows	O
the	O
machine	O
learning	O
system	O
what	O
to	O
do	O
.	O
in	O
unsupervised	O
learning	O
,	O
there	O
is	O
no	O
instructor	O
or	O
teacher	O
,	O
and	O
the	O
algorithm	O
must	O
learn	O
to	O
make	O
sense	O
of	O
the	O
data	O
without	O
this	O
guide	O
.	O
|	O
unsupervised	O
learning	O
and	O
supervised	O
learning	O
are	O
not	O
formally	O
deﬁned	O
terms	O
.	O
the	O
lines	O
between	O
them	O
are	O
often	O
blurred	O
.	O
many	O
machine	O
learning	O
technologies	O
can	O
be	O
used	O
to	O
perform	O
both	O
tasks	O
.	O
for	O
example	O
,	O
the	O
chain	O
rule	O
of	O
probability	O
states	O
that	O
for	O
a	O
vector	O
x	O
n	O
,	O
the	O
joint	O
distribution	O
can	O
be	O
decomposed	O
as	O
	O
∈	O
r	O
n	O
p	O
(	O
)	O
=x	O
p	O
(	O
xi	O
|	O
−	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
xi	O
1	O
)	O
.	O
i=1	O
(	O
5.1	O
)	O
this	O
decomposition	O
means	O
that	O
we	O
can	O
solve	O
the	O
ostensibly	O
unsupervised	O
problem	O
of	O
modeling	O
p	O
(	O
x	O
)	O
by	O
splitting	O
it	O
into	O
n	O
supervised	O
learning	O
problems	O
.	O
alternatively	O
,	O
we	O
105	O
chapter	O
5.	O
machine	O
learning	O
basics	O
x	O
)	O
by	O
using	O
traditional	O
can	O
solve	O
the	O
supervised	O
learning	O
problem	O
of	O
learning	O
p	O
(	O
y	O
unsupervised	O
learning	O
technologies	O
to	O
learn	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
,	O
y	O
)	O
and	O
inferring	O
|	O
p	O
y	O
(	O
x	O
)	O
=	O
p	O
,	O
y	O
(	O
x	O
)	O
	O
p	O
,	O
y	O
(	O
x	O
y	O
	O
.	O
)	O
(	O
5.2	O
)	O
	O
|	O
though	O
unsupervised	O
learning	O
and	O
supervised	O
learning	O
are	O
not	O
completely	O
formal	O
or	O
distinct	O
concepts	O
,	O
they	O
do	O
help	O
to	O
roughly	O
categorize	O
some	O
of	O
the	O
things	O
we	O
do	O
with	O
machine	O
learning	O
algorithms	O
.	O
traditionally	O
,	O
people	O
refer	O
to	O
regression	O
,	O
classiﬁcation	O
and	O
structured	O
output	O
problems	O
as	O
supervised	O
learning	O
.	O
density	O
estimation	O
in	O
support	O
of	O
other	O
tasks	O
is	O
usually	O
considered	O
unsupervised	O
learning	O
.	O
other	O
variants	O
of	O
the	O
learning	O
paradigm	O
are	O
possible	O
.	O
for	O
example	O
,	O
in	O
semi-	O
supervised	O
learning	O
,	O
some	O
examples	O
include	O
a	O
supervision	O
target	O
but	O
others	O
do	O
not	O
.	O
in	O
multi-instance	O
learning	O
,	O
an	O
entire	O
collection	O
of	O
examples	O
is	O
labeled	O
as	O
containing	O
or	O
not	O
containing	O
an	O
example	O
of	O
a	O
class	O
,	O
but	O
the	O
individual	O
members	O
of	O
the	O
collection	O
are	O
not	O
labeled	O
.	O
for	O
a	O
recent	O
example	O
of	O
multi-instance	O
learning	O
with	O
deep	O
models	O
,	O
see	O
kotzias	O
et	O
al	O
.	O
(	O
2015	O
)	O
.	O
some	O
machine	O
learning	O
algorithms	O
do	O
not	O
just	O
experience	O
a	O
ﬁxed	O
dataset	O
.	O
for	O
example	O
,	O
reinforcement	O
learning	O
algorithms	O
interact	O
with	O
an	O
environment	O
,	O
so	O
there	O
is	O
a	O
feedback	O
loop	O
between	O
the	O
learning	O
system	O
and	O
its	O
experiences	O
.	O
such	O
algorithms	O
are	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
.	O
please	O
see	O
sutton	O
and	O
barto	O
1998	O
)	O
or	O
bertsekas	O
and	O
tsitsiklis	O
1996	O
)	O
for	O
information	O
about	O
reinforcement	O
learning	O
,	O
)	O
for	O
the	O
deep	O
learning	O
approach	O
to	O
reinforcement	O
learning	O
.	O
and	O
mnih	O
et	O
al	O
.	O
2013	O
(	O
(	O
(	O
most	O
machine	O
learning	O
algorithms	O
simply	O
experience	O
a	O
dataset	O
.	O
a	O
dataset	O
can	O
be	O
described	O
in	O
many	O
ways	O
.	O
in	O
all	O
cases	O
,	O
a	O
dataset	O
is	O
a	O
collection	O
of	O
examples	O
,	O
which	O
are	O
in	O
turn	O
collections	O
of	O
features	O
.	O
one	O
common	O
way	O
of	O
describing	O
a	O
dataset	O
is	O
with	O
a	O
.	O
a	O
design	O
matrix	O
is	O
a	O
matrix	O
containing	O
a	O
diﬀerent	O
example	O
in	O
each	O
row	O
.	O
each	O
column	O
of	O
the	O
matrix	O
corresponds	O
to	O
a	O
diﬀerent	O
feature	O
.	O
for	O
instance	O
,	O
the	O
iris	O
dataset	O
contains	O
150	O
examples	O
with	O
four	O
features	O
for	O
each	O
example	O
.	O
this	O
means	O
we	O
can	O
represent	O
the	O
dataset	O
with	O
a	O
design	O
matrix	O
x	O
,	O
where	O
xi,1	O
is	O
the	O
sepal	O
length	O
of	O
plant	O
i	O
,	O
xi,2	O
is	O
the	O
sepal	O
width	O
of	O
plant	O
i	O
,	O
etc	O
.	O
we	O
will	O
describe	O
most	O
of	O
the	O
learning	O
algorithms	O
in	O
this	O
book	O
in	O
terms	O
of	O
how	O
they	O
operate	O
on	O
design	O
matrix	O
datasets	O
.	O
design	O
matrix	O
×	O
150	O
4	O
∈	O
r	O
of	O
course	O
,	O
to	O
describe	O
a	O
dataset	O
as	O
a	O
design	O
matrix	O
,	O
it	O
must	O
be	O
possible	O
to	O
describe	O
each	O
example	O
as	O
a	O
vector	O
,	O
and	O
each	O
of	O
these	O
vectors	O
must	O
be	O
the	O
same	O
size	O
.	O
this	O
is	O
not	O
always	O
possible	O
.	O
for	O
example	O
,	O
if	O
you	O
have	O
a	O
collection	O
of	O
photographs	O
with	O
diﬀerent	O
widths	O
and	O
heights	O
,	O
then	O
diﬀerent	O
photographs	O
will	O
contain	O
diﬀerent	O
numbers	O
of	O
pixels	O
,	O
so	O
not	O
all	O
of	O
the	O
photographs	O
may	O
be	O
described	O
with	O
the	O
same	O
describe	O
how	O
to	O
handle	O
diﬀerent	O
length	O
of	O
vector	O
.	O
section	O
and	O
chapter	O
9.7	O
10	O
106	O
chapter	O
5.	O
machine	O
learning	O
basics	O
types	O
of	O
such	O
heterogeneous	O
data	O
.	O
in	O
cases	O
like	O
these	O
,	O
rather	O
than	O
describing	O
the	O
{	O
dataset	O
as	O
a	O
matrix	O
with	O
m	O
rows	O
,	O
we	O
will	O
describe	O
it	O
as	O
a	O
set	O
containing	O
m	O
elements	O
:	O
x	O
(	O
1	O
)	O
,	O
x	O
(	O
2	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
.	O
this	O
notation	O
does	O
not	O
imply	O
that	O
any	O
two	O
example	O
vectors	O
x	O
(	O
)	O
i	O
and	O
x	O
(	O
)	O
j	O
have	O
the	O
same	O
size.	O
}	O
)	O
m	O
in	O
the	O
case	O
of	O
supervised	O
learning	O
,	O
the	O
example	O
contains	O
a	O
label	O
or	O
target	O
as	O
well	O
as	O
a	O
collection	O
of	O
features	O
.	O
for	O
example	O
,	O
if	O
we	O
want	O
to	O
use	O
a	O
learning	O
algorithm	O
to	O
perform	O
object	O
recognition	B
from	O
photographs	O
,	O
we	O
need	O
to	O
specify	O
which	O
object	O
appears	O
in	O
each	O
of	O
the	O
photos	O
.	O
we	O
might	O
do	O
this	O
with	O
a	O
numeric	O
code	O
,	O
with	O
0	O
signifying	O
a	O
person	O
,	O
1	O
signifying	O
a	O
car	O
,	O
2	O
signifying	O
a	O
cat	O
,	O
etc	O
.	O
often	O
when	O
working	O
with	O
a	O
dataset	O
containing	O
a	O
design	O
matrix	O
of	O
feature	O
observations	O
x	O
,	O
we	O
also	O
provide	O
a	O
vector	O
of	O
labels	O
yi	O
providing	O
the	O
label	O
for	O
example	O
.i	O
,	O
with	O
y	O
of	O
course	O
,	O
sometimes	O
the	O
label	O
may	O
be	O
more	O
than	O
just	O
a	O
single	O
number	O
.	O
for	O
example	O
,	O
if	O
we	O
want	O
to	O
train	O
a	O
speech	O
recognition	B
system	O
to	O
transcribe	O
entire	O
sentences	O
,	O
then	O
the	O
label	O
for	O
each	O
example	O
sentence	O
is	O
a	O
sequence	O
of	O
words	O
.	O
just	O
as	O
there	O
is	O
no	O
formal	O
deﬁnition	O
of	O
supervised	O
and	O
unsupervised	O
learning	O
,	O
there	O
is	O
no	O
rigid	O
taxonomy	O
of	O
datasets	O
or	O
experiences	O
.	O
the	O
structures	O
described	O
here	O
cover	O
most	O
cases	O
,	O
but	O
it	O
is	O
always	O
possible	O
to	O
design	O
new	O
ones	O
for	O
new	O
applications	O
.	O
5.1.4	O
example	O
:	O
linear	O
regression	O
our	O
deﬁnition	O
of	O
a	O
machine	O
learning	O
algorithm	O
as	O
an	O
algorithm	O
that	O
is	O
capable	O
of	O
improving	O
a	O
computer	O
program	O
’	O
s	O
performance	O
at	O
some	O
task	O
via	O
experience	O
is	O
somewhat	O
abstract	O
.	O
to	O
make	O
this	O
more	O
concrete	O
,	O
we	O
present	O
an	O
example	O
of	O
a	O
simple	O
machine	O
learning	O
algorithm	O
:	O
linear	O
regression	O
.	O
we	O
will	O
return	O
to	O
this	O
example	O
repeatedly	O
as	O
we	O
introduce	O
more	O
machine	O
learning	O
concepts	O
that	O
help	O
to	O
understand	O
its	O
behavior	O
.	O
as	O
the	O
name	O
implies	O
,	O
linear	O
regression	O
solves	O
a	O
regression	O
problem	O
.	O
in	O
other	O
n	O
as	O
input	O
and	O
words	O
,	O
the	O
goal	O
is	O
to	O
build	O
a	O
system	O
that	O
can	O
take	O
a	O
vector	O
x	O
predict	O
the	O
value	O
of	O
a	O
scalar	O
y	O
r	O
as	O
its	O
output	O
.	O
in	O
the	O
case	O
of	O
linear	O
regression	O
,	O
the	O
output	O
is	O
a	O
linear	O
function	O
of	O
the	O
input	O
.	O
let	O
ˆy	O
be	O
the	O
value	O
that	O
our	O
model	B
predicts	O
should	O
take	O
on	O
.	O
we	O
deﬁne	O
the	O
output	O
to	O
be	O
∈	O
r	O
y	O
∈	O
∈	O
where	O
w	O
ˆy	O
=	O
w	O
n	O
is	O
a	O
vector	O
of	O
r	O
.	O
parameters	O
	O
x	O
(	O
5.3	O
)	O
parameters	O
are	O
values	O
that	O
control	O
the	O
behavior	O
of	O
the	O
system	O
.	O
in	O
this	O
case	O
,	O
wi	O
is	O
the	O
coeﬃcient	O
that	O
we	O
multiply	O
by	O
feature	O
xi	O
before	O
summing	O
up	O
the	O
contributions	O
from	O
all	O
the	O
features	O
.	O
we	O
can	O
think	O
of	O
w	O
as	O
a	O
set	O
of	O
weights	O
that	O
determine	O
how	O
each	O
feature	O
aﬀects	O
the	O
prediction	O
.	O
if	O
a	O
feature	O
xi	O
receives	O
a	O
positive	O
weight	O
wi	O
,	O
107	O
chapter	O
5.	O
machine	O
learning	O
basics	O
then	O
increasing	O
the	O
value	O
of	O
that	O
feature	O
increases	O
the	O
value	O
of	O
our	O
prediction	O
ˆy	O
.	O
if	O
a	O
feature	O
receives	O
a	O
negative	O
weight	O
,	O
then	O
increasing	O
the	O
value	O
of	O
that	O
feature	O
decreases	O
the	O
value	O
of	O
our	O
prediction	O
.	O
if	O
a	O
feature	O
’	O
s	O
weight	O
is	O
large	O
in	O
magnitude	O
,	O
then	O
it	O
has	O
a	O
large	O
eﬀect	O
on	O
the	O
prediction	O
.	O
if	O
a	O
feature	O
’	O
s	O
weight	O
is	O
zero	O
,	O
it	O
has	O
no	O
eﬀect	O
on	O
the	O
prediction	O
.	O
we	O
thus	O
have	O
a	O
deﬁnition	O
of	O
our	O
task	O
t	O
:	O
to	O
predict	O
y	O
from	O
x	O
by	O
outputting	O
	O
ˆy	O
=	O
w	O
x.	O
next	O
we	O
need	O
a	O
deﬁnition	O
of	O
our	O
performance	O
measure	O
,	O
.p	O
suppose	O
that	O
we	O
have	O
a	O
design	O
matrix	O
of	O
m	O
example	O
inputs	O
that	O
we	O
will	O
not	O
use	O
for	O
training	O
,	O
only	O
for	O
evaluating	O
how	O
well	O
the	O
model	B
performs	O
.	O
we	O
also	O
have	O
a	O
vector	O
of	O
regression	O
targets	O
providing	O
the	O
correct	O
value	O
of	O
y	O
for	O
each	O
of	O
these	O
examples	O
.	O
because	O
this	O
dataset	O
will	O
only	O
be	O
used	O
for	O
evaluation	O
,	O
we	O
call	O
it	O
the	O
test	O
test	O
and	O
the	O
vector	O
of	O
regression	O
set	O
.	O
we	O
refer	O
to	O
the	O
design	O
matrix	O
of	O
inputs	O
as	O
x	O
(	O
targets	O
as	O
y	O
(	O
test	O
.	O
)	O
)	O
one	O
way	O
of	O
measuring	O
the	O
performance	O
of	O
the	O
model	B
is	O
to	O
compute	O
the	O
mean	O
test	O
gives	O
the	O
predictions	O
of	O
the	O
squared	O
error	O
of	O
the	O
model	B
on	O
the	O
test	O
set	O
.	O
if	O
ˆy	O
(	O
model	B
on	O
the	O
test	O
set	O
,	O
then	O
the	O
mean	O
squared	O
error	O
is	O
given	O
by	O
)	O
	O
msetest	O
=	O
1	O
m	O
i	O
test	O
−	O
)	O
(	O
ˆy	O
(	O
y	O
(	O
)	O
test	O
)	O
2	O
i	O
.	O
(	O
5.4	O
)	O
intuitively	O
,	O
one	O
can	O
see	O
that	O
this	O
error	O
measure	O
decreases	O
to	O
0	O
when	O
ˆy	O
(	O
we	O
can	O
also	O
see	O
that	O
test	O
=	O
y	O
(	O
)	O
test	O
.	O
)	O
msetest	O
=	O
1	O
m	O
||	O
test	O
−	O
ˆy	O
(	O
)	O
test	O
||	O
2	O
y	O
(	O
2	O
,	O
)	O
(	O
5.5	O
)	O
so	O
the	O
error	O
increases	O
whenever	O
the	O
euclidean	O
distance	O
between	O
the	O
predictions	O
and	O
the	O
targets	O
increases	O
.	O
to	O
make	O
a	O
machine	O
learning	O
algorithm	O
,	O
we	O
need	O
to	O
design	O
an	O
algorithm	O
that	O
will	O
improve	O
the	O
weights	O
w	O
in	O
a	O
way	O
that	O
reduces	O
msetest	O
when	O
the	O
algorithm	O
train	O
)	O
.	O
one	O
is	O
allowed	O
to	O
gain	O
experience	O
by	O
observing	O
a	O
training	O
set	O
(	O
x	O
(	O
intuitive	O
way	O
of	O
doing	O
this	O
(	O
which	O
we	O
will	O
justify	O
later	O
,	O
in	O
section	O
)	O
is	O
just	O
to	O
minimize	O
the	O
mean	O
squared	O
error	O
on	O
the	O
training	O
set	O
,	O
msetrain	O
.	O
train	O
,	O
y	O
(	O
5.5.1	O
)	O
)	O
to	O
minimize	O
mse	O
train	O
,	O
we	O
can	O
simply	O
solve	O
for	O
where	O
its	O
gradient	O
is	O
:0	O
∇	O
wmsetrain	O
=	O
0	O
train	O
−	O
||	O
train	O
||	O
ˆy	O
(	O
y	O
(	O
train	O
||	O
−	O
train	O
w	O
y	O
1	O
m	O
||	O
x	O
(	O
)	O
)	O
)	O
(	O
)	O
2	O
2	O
=	O
0	O
2	O
2	O
=	O
0	O
w	O
⇒	O
∇	O
⇒	O
1	O
∇	O
m	O
w	O
108	O
(	O
5.6	O
)	O
(	O
5.7	O
)	O
(	O
5.8	O
)	O
chapter	O
5.	O
machine	O
learning	O
basics	O
3	O
2	O
1	O
0	O
−	O
1	O
−	O
2	O
−	O
3	O
y	O
linear	O
regression	O
example	O
−	O
1	O
0	O
.	O
−	O
0	O
5	O
.	O
0	O
5	O
.	O
1	O
0	O
.	O
0	O
0	O
.	O
x1	O
0	O
55	O
.	O
0	O
50	O
.	O
0	O
45	O
.	O
0	O
40	O
.	O
0	O
35	O
.	O
0	O
30	O
.	O
0	O
25	O
.	O
0	O
20.	O
)	O
n	O
i	O
a	O
r	O
t	O
(	O
e	O
s	O
m	O
optimization	O
of	O
w	O
0	O
5	O
.	O
1	O
5	O
.	O
1	O
0	O
.	O
w1	O
figure	O
5.1	O
:	O
a	O
linear	O
regression	O
problem	O
,	O
with	O
a	O
training	O
set	O
consisting	O
of	O
ten	O
data	O
points	O
,	O
each	O
containing	O
one	O
feature	O
.	O
because	O
there	O
is	O
only	O
one	O
feature	O
,	O
the	O
weight	O
vector	O
w	O
contains	O
only	O
a	O
single	O
parameter	O
to	O
learn	O
,	O
w1	O
.	O
(	O
left	O
)	O
observe	O
that	O
linear	O
regression	O
learns	O
to	O
set	O
w1	O
such	O
that	O
the	O
line	O
y	O
=	O
w1x	O
comes	O
as	O
close	O
as	O
possible	O
to	O
passing	O
through	O
all	O
the	O
training	O
points	O
.	O
w1	O
found	O
by	O
the	O
normal	O
equations	O
,	O
which	O
we	O
can	O
see	O
minimizes	O
the	O
mean	O
squared	O
error	O
on	O
the	O
training	O
set	O
.	O
the	O
plotted	O
point	O
indicates	O
the	O
value	O
of	O
(	O
right	O
)	O
	O
	O
	O
	O
	O
⇒	O
∇	O
)	O
−	O
train	O
w	O
y	O
train	O
(	O
)	O
−	O
	O
train	O
w	O
)	O
w	O
x	O
(	O
	O
x	O
(	O
train	O
)	O
⇒	O
∇	O
w	O
w	O
	O
x	O
(	O
	O
x	O
(	O
−	O
train	O
train	O
w	O
y	O
(	O
)	O
	O
train	O
)	O
)	O
)	O
=	O
0	O
	O
train	O
)	O
y	O
(	O
train	O
)	O
	O
train	O
)	O
2x	O
(	O
⇒	O
⇒	O
y	O
(	O
w	O
2	O
x	O
(	O
−	O
train	O
+	O
y	O
(	O
	O
train	O
w	O
x	O
train	O
(	O
)	O
−	O
	O
1	O
train	O
)	O
train	O
=	O
0	O
	O
train	O
)	O
x	O
(	O
train	O
)	O
train	O
)	O
y	O
(	O
2	O
)	O
)	O
x	O
(	O
y	O
(	O
w	O
=	O
x	O
(	O
x	O
(	O
	O
	O
	O
(	O
5.9	O
)	O
=	O
0	O
(	O
5.10	O
)	O
(	O
5.11	O
)	O
(	O
5.12	O
)	O
the	O
system	O
of	O
equations	O
whose	O
solution	O
is	O
given	O
by	O
equation	O
is	O
known	O
as	O
constitutes	O
a	O
simple	O
learning	O
the	O
normal	O
equations	O
.	O
evaluating	O
equation	O
algorithm	O
.	O
for	O
an	O
example	O
of	O
the	O
linear	O
regression	O
learning	O
algorithm	O
in	O
action	O
,	O
see	O
ﬁgure	O
5.12	O
5.12	O
.5.1	O
it	O
is	O
worth	O
noting	O
that	O
the	O
term	O
linear	O
regression	O
is	O
often	O
used	O
to	O
refer	O
to	O
a	O
slightly	O
more	O
sophisticated	O
model	B
with	O
one	O
additional	O
parameter—an	O
intercept	O
term	O
.	O
in	O
this	O
model	B
b	O
	O
ˆy	O
=	O
w	O
x	O
+	O
b	O
(	O
5.13	O
)	O
so	O
the	O
mapping	O
from	O
parameters	O
to	O
predictions	O
is	O
still	O
a	O
linear	O
function	O
but	O
the	O
mapping	O
from	O
features	O
to	O
predictions	O
is	O
now	O
an	O
aﬃne	O
function	O
.	O
this	O
extension	O
to	O
aﬃne	O
functions	O
means	O
that	O
the	O
plot	O
of	O
the	O
model	B
’	O
s	O
predictions	O
still	O
looks	O
like	O
a	O
line	O
,	O
but	O
it	O
need	O
not	O
pass	O
through	O
the	O
origin	O
.	O
instead	O
of	O
adding	O
the	O
bias	O
parameter	O
109	O
chapter	O
5.	O
machine	O
learning	O
basics	O
b	O
,	O
one	O
can	O
continue	O
to	O
use	O
the	O
model	B
with	O
only	O
weights	O
but	O
augment	O
x	O
with	O
an	O
extra	O
entry	O
that	O
is	O
always	O
set	O
to	O
.	O
the	O
weight	O
corresponding	O
to	O
the	O
extra	O
entry	O
plays	O
the	O
role	O
of	O
the	O
bias	O
parameter	O
.	O
we	O
will	O
frequently	O
use	O
the	O
term	O
“	O
linear	O
”	O
when	O
referring	O
to	O
aﬃne	O
functions	O
throughout	O
this	O
book	O
.	O
1	O
1	O
the	O
intercept	O
term	O
b	O
is	O
often	O
called	O
the	O
bias	O
parameter	O
of	O
the	O
aﬃne	O
transfor-	O
mation	O
.	O
this	O
terminology	O
derives	O
from	O
the	O
point	O
of	O
view	O
that	O
the	O
output	O
of	O
the	O
transformation	O
is	O
biased	O
toward	O
being	O
b	O
in	O
the	O
absence	O
of	O
any	O
input	O
.	O
this	O
term	O
is	O
diﬀerent	O
from	O
the	O
idea	O
of	O
a	O
statistical	O
bias	O
,	O
in	O
which	O
a	O
statistical	O
estimation	O
algorithm	O
’	O
s	O
expected	O
estimate	O
of	O
a	O
quantity	O
is	O
not	O
equal	O
to	O
the	O
true	O
quantity	O
.	O
linear	O
regression	O
is	O
of	O
course	O
an	O
extremely	O
simple	O
and	O
limited	O
learning	O
algorithm	O
,	O
but	O
it	O
provides	O
an	O
example	O
of	O
how	O
a	O
learning	O
algorithm	O
can	O
work	B
.	O
in	O
the	O
subsequent	O
sections	O
we	O
will	O
describe	O
some	O
of	O
the	O
basic	O
principles	O
underlying	O
learning	O
algorithm	O
design	O
and	O
demonstrate	O
how	O
these	O
principles	O
can	O
be	O
used	O
to	O
build	O
more	O
complicated	O
learning	O
algorithms	O
.	O
5.2	O
capacity	O
,	O
overﬁtting	O
and	O
underﬁtting	O
the	O
central	O
challenge	O
in	O
machine	O
learning	O
is	O
that	O
we	O
must	O
perform	O
well	O
on	O
new	O
,	O
previously	O
unseen	O
inputs—not	O
just	O
those	O
on	O
which	O
our	O
model	B
was	O
trained	O
.	O
the	O
ability	O
to	O
perform	O
well	O
on	O
previously	O
unobserved	O
inputs	O
is	O
called	O
generalization	O
.	O
typically	O
,	O
when	O
training	O
a	O
machine	O
learning	O
model	B
,	O
we	O
have	O
access	O
to	O
a	O
training	O
set	O
,	O
we	O
can	O
compute	O
some	O
error	O
measure	O
on	O
the	O
training	O
set	O
called	O
the	O
training	O
error	O
,	O
and	O
we	O
reduce	O
this	O
training	O
error	O
.	O
so	O
far	O
,	O
what	O
we	O
have	O
described	O
is	O
simply	O
an	O
optimization	O
problem	O
.	O
what	O
separates	O
machine	O
learning	O
from	O
optimization	O
is	O
that	O
we	O
want	O
the	O
generalization	O
error	O
,	O
also	O
called	O
the	O
test	O
error	O
,	O
to	O
be	O
low	O
as	O
well	O
.	O
the	O
generalization	O
error	O
is	O
deﬁned	O
as	O
the	O
expected	O
value	O
of	O
the	O
error	O
on	O
a	O
new	O
input	O
.	O
here	O
the	O
expectation	O
is	O
taken	O
across	O
diﬀerent	O
possible	O
inputs	O
,	O
drawn	O
from	O
the	O
distribution	O
of	O
inputs	O
we	O
expect	O
the	O
system	O
to	O
encounter	O
in	O
practice	O
.	O
we	O
typically	O
estimate	O
the	O
generalization	O
error	O
of	O
a	O
machine	O
learning	O
model	B
by	O
measuring	O
its	O
performance	O
on	O
a	O
test	O
set	O
of	O
examples	O
that	O
were	O
collected	O
separately	O
from	O
the	O
training	O
set	O
.	O
in	O
our	O
linear	O
regression	O
example	O
,	O
we	O
trained	O
the	O
model	B
by	O
minimizing	O
the	O
training	O
error	O
,	O
1	O
train	O
)	O
m	O
(	O
||	O
x	O
(	O
−	O
train	O
w	O
y	O
(	O
)	O
but	O
we	O
actually	O
care	O
about	O
the	O
test	O
error	O
,	O
1	O
)	O
test	O
m	O
(	O
)	O
train	O
||	O
||	O
x	O
(	O
2	O
2	O
,	O
−	O
test	O
w	O
y	O
)	O
(	O
5.14	O
)	O
||	O
2	O
2.	O
test	O
(	O
)	O
how	O
can	O
we	O
aﬀect	O
performance	O
on	O
the	O
test	O
set	O
when	O
we	O
get	O
to	O
observe	O
only	O
the	O
110	O
chapter	O
5.	O
machine	O
learning	O
basics	O
training	O
set	O
?	O
the	O
ﬁeld	O
of	O
statistical	O
learning	O
theory	O
provides	O
some	O
answers	O
.	O
if	O
the	O
training	O
and	O
the	O
test	O
set	O
are	O
collected	O
arbitrarily	O
,	O
there	O
is	O
indeed	O
little	O
we	O
can	O
do	O
.	O
if	O
we	O
are	O
allowed	O
to	O
make	O
some	O
assumptions	O
about	O
how	O
the	O
training	O
and	O
test	O
set	O
are	O
collected	O
,	O
then	O
we	O
can	O
make	O
some	O
progress	O
.	O
the	O
train	O
and	O
test	O
data	O
are	O
generated	O
by	O
a	O
probability	O
distribution	O
over	O
datasets	O
called	O
the	O
data	O
generating	O
process	O
.	O
we	O
typically	O
make	O
a	O
set	O
of	O
assumptions	O
known	O
collectively	O
as	O
the	O
i.i.d	O
.	O
assumptions	O
.	O
these	O
assumptions	O
are	O
that	O
the	O
examples	O
in	O
each	O
dataset	O
are	O
independent	O
from	O
each	O
other	O
,	O
and	O
that	O
the	O
train	O
set	O
and	O
test	O
set	O
are	O
identically	O
distributed	O
,	O
drawn	O
from	O
the	O
same	O
probability	O
distribution	O
as	O
each	O
other	O
.	O
this	O
assumption	O
allows	O
us	O
to	O
describe	O
the	O
data	O
gen-	O
erating	O
process	O
with	O
a	O
probability	O
distribution	O
over	O
a	O
single	O
example	O
.	O
the	O
same	O
distribution	O
is	O
then	O
used	O
to	O
generate	O
every	O
train	O
example	O
and	O
every	O
test	O
example	O
.	O
we	O
call	O
that	O
shared	O
underlying	O
distribution	O
the	O
data	O
generating	O
distribution	O
,	O
denoted	O
pdata	O
.	O
this	O
probabilistic	O
framework	O
and	O
the	O
i.i.d	O
.	O
assumptions	O
allow	O
us	O
to	O
mathematically	O
study	O
the	O
relationship	O
between	O
training	O
error	O
and	O
test	O
error	O
.	O
one	O
immediate	O
connection	O
we	O
can	O
observe	O
between	O
the	O
training	O
and	O
test	O
error	O
is	O
that	O
the	O
expected	O
training	O
error	O
of	O
a	O
randomly	O
selected	O
model	B
is	O
equal	O
to	O
the	O
expected	O
test	O
error	O
of	O
that	O
model	B
.	O
suppose	O
we	O
have	O
a	O
probability	O
distribution	O
p	O
(	O
x	O
,	O
y	O
)	O
and	O
we	O
sample	O
from	O
it	O
repeatedly	O
to	O
generate	O
the	O
train	O
set	O
and	O
the	O
test	O
set	O
.	O
for	O
some	O
ﬁxed	O
value	O
w	O
,	O
the	O
expected	O
training	O
set	O
error	O
is	O
exactly	O
the	O
same	O
as	O
the	O
expected	O
test	O
set	O
error	O
,	O
because	O
both	O
expectations	O
are	O
formed	O
using	O
the	O
same	O
dataset	O
sampling	O
process	O
.	O
the	O
only	O
diﬀerence	O
between	O
the	O
two	O
conditions	B
is	O
the	O
name	O
we	O
assign	O
to	O
the	O
dataset	O
we	O
sample	O
.	O
of	O
course	O
,	O
when	O
we	O
use	O
a	O
machine	O
learning	O
algorithm	O
,	O
we	O
do	O
not	O
ﬁx	O
the	O
parameters	O
ahead	O
of	O
time	O
,	O
then	O
sample	O
both	O
datasets	O
.	O
we	O
sample	O
the	O
training	O
set	O
,	O
then	O
use	O
it	O
to	O
choose	O
the	O
parameters	O
to	O
reduce	O
training	O
set	O
error	O
,	O
then	O
sample	O
the	O
test	O
set	O
.	O
under	O
this	O
process	O
,	O
the	O
expected	O
test	O
error	O
is	O
greater	O
than	O
or	O
equal	O
to	O
the	O
expected	O
value	O
of	O
training	O
error	O
.	O
the	O
factors	O
determining	O
how	O
well	O
a	O
machine	O
learning	O
algorithm	O
will	O
perform	O
are	O
its	O
ability	O
to	O
:	O
1.	O
make	O
the	O
training	O
error	O
small	O
.	O
2.	O
make	O
the	O
gap	O
between	O
training	O
and	O
test	O
error	O
small	O
.	O
these	O
two	O
factors	O
correspond	O
to	O
the	O
two	O
central	O
challenges	O
in	O
machine	O
learning	O
:	O
underﬁtting	O
and	O
overﬁtting	O
.	O
underﬁtting	O
occurs	O
when	O
the	O
model	B
is	O
not	O
able	O
to	O
obtain	O
a	O
suﬃciently	O
low	O
error	O
value	O
on	O
the	O
training	O
set	O
.	O
overﬁtting	O
occurs	O
when	O
the	O
gap	O
between	O
the	O
training	O
error	O
and	O
test	O
error	O
is	O
too	O
large	O
.	O
we	O
can	O
control	O
whether	O
a	O
model	B
is	O
more	O
likely	O
to	O
overﬁt	O
or	O
underﬁt	O
by	O
altering	O
its	O
capacity	O
.	O
informally	O
,	O
a	O
model	B
’	O
s	O
capacity	O
is	O
its	O
ability	O
to	O
ﬁt	O
a	O
wide	O
variety	O
of	O
111	O
chapter	O
5.	O
machine	O
learning	O
basics	O
functions	O
.	O
models	O
with	O
low	O
capacity	O
may	O
struggle	O
to	O
ﬁt	O
the	O
training	O
set	O
.	O
models	O
with	O
high	O
capacity	O
can	O
overﬁt	O
by	O
memorizing	O
properties	O
of	O
the	O
training	O
set	O
that	O
do	O
not	O
serve	O
them	O
well	O
on	O
the	O
test	O
set	O
.	O
one	O
way	O
to	O
control	O
the	O
capacity	O
of	O
a	O
learning	O
algorithm	O
is	O
by	O
choosing	O
its	O
hypothesis	O
space	O
,	O
the	O
set	O
of	O
functions	O
that	O
the	O
learning	O
algorithm	O
is	O
allowed	O
to	O
select	O
as	O
being	O
the	O
solution	O
.	O
for	O
example	O
,	O
the	O
linear	O
regression	O
algorithm	O
has	O
the	O
set	O
of	O
all	O
linear	O
functions	O
of	O
its	O
input	O
as	O
its	O
hypothesis	O
space	O
.	O
we	O
can	O
generalize	O
linear	O
regression	O
to	O
include	O
polynomials	O
,	O
rather	O
than	O
just	O
linear	O
functions	O
,	O
in	O
its	O
hypothesis	O
space	O
.	O
doing	O
so	O
increases	O
the	O
model	B
’	O
s	O
capacity	O
.	O
a	O
polynomial	O
of	O
degree	O
one	O
gives	O
us	O
the	O
linear	O
regression	O
model	B
with	O
which	O
we	O
are	O
already	O
familiar	O
,	O
with	O
prediction	O
ˆy	O
=	O
+	O
b	O
wx	O
.	O
(	O
5.15	O
)	O
by	O
introducing	O
x2	O
as	O
another	O
feature	O
provided	O
to	O
the	O
linear	O
regression	O
model	B
,	O
we	O
can	O
learn	O
a	O
model	B
that	O
is	O
quadratic	O
as	O
a	O
function	O
of	O
:	O
x	O
ˆy	O
=	O
+	O
1x	O
w+	O
2x2	O
.	O
b	O
w	O
(	O
5.16	O
)	O
,	O
the	O
output	O
is	O
though	O
this	O
model	B
implements	O
a	O
quadratic	O
function	O
of	O
its	O
still	O
a	O
linear	O
function	O
of	O
the	O
parameters	O
,	O
so	O
we	O
can	O
still	O
use	O
the	O
normal	O
equations	O
to	O
train	O
the	O
model	B
in	O
closed	O
form	O
.	O
we	O
can	O
continue	O
to	O
add	O
more	O
powers	O
of	O
x	O
as	O
additional	O
features	O
,	O
for	O
example	O
to	O
obtain	O
a	O
polynomial	O
of	O
degree	O
9	O
:	O
	O
input	O
ˆy	O
b=	O
+	O
9	O
i=1	O
wixi	O
.	O
(	O
5.17	O
)	O
machine	O
learning	O
algorithms	O
will	O
generally	O
perform	O
best	O
when	O
their	O
capacity	O
is	O
appropriate	O
for	O
the	O
true	O
complexity	O
of	O
the	O
task	O
they	O
need	O
to	O
perform	O
and	O
the	O
amount	O
of	O
training	O
data	O
they	O
are	O
provided	O
with	O
.	O
models	O
with	O
insuﬃcient	O
capacity	O
are	O
unable	O
to	O
solve	O
complex	O
tasks	O
.	O
models	O
with	O
high	O
capacity	O
can	O
solve	O
complex	O
tasks	O
,	O
but	O
when	O
their	O
capacity	O
is	O
higher	O
than	O
needed	O
to	O
solve	O
the	O
present	O
task	O
they	O
may	O
overﬁt	O
.	O
5.2	O
figure	O
shows	O
this	O
principle	O
in	O
action	O
.	O
we	O
compare	O
a	O
linear	O
,	O
quadratic	O
and	O
degree-9	O
predictor	O
attempting	O
to	O
ﬁt	O
a	O
problem	O
where	O
the	O
true	O
underlying	O
function	O
is	O
quadratic	O
.	O
the	O
linear	O
function	O
is	O
unable	O
to	O
capture	O
the	O
curvature	O
in	O
the	O
true	O
underlying	O
problem	O
,	O
so	O
it	O
underﬁts	O
.	O
the	O
degree-9	O
predictor	O
is	O
capable	O
of	O
representing	O
the	O
correct	O
function	O
,	O
but	O
it	O
is	O
also	O
capable	O
of	O
representing	O
inﬁnitely	O
many	O
other	O
functions	O
that	O
pass	O
exactly	O
through	O
the	O
training	O
points	O
,	O
because	O
we	O
112	O
chapter	O
5.	O
machine	O
learning	O
basics	O
have	O
more	O
parameters	O
than	O
training	O
examples	O
.	O
we	O
have	O
little	O
chance	O
of	O
choosing	O
a	O
solution	O
that	O
generalizes	O
well	O
when	O
so	O
many	O
wildly	O
diﬀerent	O
solutions	O
exist	O
.	O
in	O
this	O
example	O
,	O
the	O
quadratic	O
model	B
is	O
perfectly	O
matched	O
to	O
the	O
true	O
structure	O
of	O
the	O
task	O
so	O
it	O
generalizes	O
well	O
to	O
new	O
data	O
.	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
figure	O
5.2	O
:	O
we	O
ﬁt	O
three	O
models	O
to	O
this	O
example	O
training	O
set	O
.	O
the	O
training	O
data	O
was	O
generated	O
synthetically	O
,	O
by	O
randomly	O
sampling	O
x	O
values	O
and	O
choosing	O
y	O
deterministically	O
by	O
evaluating	O
a	O
quadratic	O
function	O
.	O
(	O
left	O
)	O
a	O
linear	O
function	O
ﬁt	O
to	O
the	O
data	O
suﬀers	O
from	O
underﬁtting—it	O
can	O
not	O
capture	O
the	O
curvature	O
that	O
is	O
present	O
in	O
the	O
data	O
.	O
a	O
quadratic	O
function	O
ﬁt	O
to	O
the	O
data	O
generalizes	O
well	O
to	O
unseen	O
points	O
.	O
it	O
does	O
not	O
suﬀer	O
from	O
a	O
signiﬁcant	O
amount	O
of	O
overﬁtting	O
or	O
underﬁtting	O
.	O
a	O
polynomial	O
of	O
degree	O
9	O
ﬁt	O
to	O
the	O
data	O
suﬀers	O
from	O
overﬁtting	O
.	O
here	O
we	O
used	O
the	O
moore-penrose	O
pseudoinverse	O
to	O
solve	O
the	O
underdetermined	O
normal	O
equations	O
.	O
the	O
solution	O
passes	O
through	O
all	O
of	O
the	O
training	O
points	O
exactly	O
,	O
but	O
we	O
have	O
not	O
been	O
lucky	O
enough	O
for	O
it	O
to	O
extract	O
the	O
correct	O
structure	O
.	O
it	O
now	O
has	O
a	O
deep	O
valley	O
in	O
between	O
two	O
training	O
points	O
that	O
does	O
not	O
appear	O
in	O
the	O
true	O
underlying	O
function	O
.	O
it	O
also	O
increases	O
sharply	O
on	O
the	O
left	O
side	O
of	O
the	O
data	O
,	O
while	O
the	O
true	O
function	O
decreases	O
in	O
this	O
area	O
.	O
(	O
center	O
)	O
(	O
right	O
)	O
so	O
far	O
we	O
have	O
described	O
only	O
one	O
way	O
of	O
changing	O
a	O
model	B
’	O
s	O
capacity	O
:	O
by	O
changing	O
the	O
number	O
of	O
input	O
features	O
it	O
has	O
,	O
and	O
simultaneously	O
adding	O
new	O
parameters	O
associated	O
with	O
those	O
features	O
.	O
there	O
are	O
in	O
fact	O
many	O
ways	O
of	O
changing	O
a	O
model	B
’	O
s	O
capacity	O
.	O
capacity	O
is	O
not	O
determined	O
only	O
by	O
the	O
choice	O
of	O
model	B
.	O
the	O
model	B
speciﬁes	O
which	O
family	O
of	O
functions	O
the	O
learning	O
algorithm	O
can	O
choose	O
from	O
when	O
varying	O
the	O
parameters	O
in	O
order	O
to	O
reduce	O
a	O
training	O
objective	O
.	O
this	O
is	O
called	O
the	O
representational	O
capacity	O
of	O
the	O
model	B
.	O
in	O
many	O
cases	O
,	O
ﬁnding	O
the	O
best	O
function	O
within	O
this	O
family	O
is	O
a	O
very	O
diﬃcult	O
optimization	O
problem	O
.	O
in	O
practice	O
,	O
the	O
learning	O
algorithm	O
does	O
not	O
actually	O
ﬁnd	O
the	O
best	O
function	O
,	O
but	O
merely	O
one	O
that	O
signiﬁcantly	O
reduces	O
the	O
training	O
error	O
.	O
these	O
additional	O
limitations	O
,	O
such	O
as	O
113	O
chapter	O
5.	O
machine	O
learning	O
basics	O
the	O
imperfection	O
of	O
the	O
optimization	O
algorithm	O
,	O
mean	O
that	O
the	O
learning	O
algorithm	O
’	O
s	O
eﬀective	O
capacity	O
may	O
be	O
less	O
than	O
the	O
representational	O
capacity	O
of	O
the	O
model	B
family	O
.	O
our	O
modern	O
ideas	O
about	O
improving	O
the	O
generalization	O
of	O
machine	O
learning	O
models	O
are	O
reﬁnements	O
of	O
thought	O
dating	O
back	O
to	O
philosophers	O
at	O
least	O
as	O
early	O
as	O
ptolemy	O
.	O
many	O
early	O
scholars	O
invoke	O
a	O
principle	O
of	O
parsimony	O
that	O
is	O
now	O
most	O
widely	O
known	O
as	O
occam	O
’	O
s	O
razor	O
(	O
c.	O
1287-1347	O
)	O
.	O
this	O
principle	O
states	O
that	O
among	O
competing	O
hypotheses	O
that	O
explain	O
known	O
observations	O
equally	O
well	O
,	O
one	O
should	O
choose	O
the	O
“	O
simplest	O
”	O
one	O
.	O
this	O
idea	O
was	O
formalized	O
and	O
made	O
more	O
precise	O
in	O
the	O
20th	O
century	O
by	O
the	O
founders	O
of	O
statistical	O
learning	O
theory	O
(	O
vapnik	O
and	O
chervonenkis	O
1971	O
vapnik	O
1982	O
blumer	O
1989	O
vapnik	O
1995	O
et	O
al.	O
,	O
)	O
.	O
,	O
;	O
,	O
;	O
;	O
,	O
statistical	O
learning	O
theory	O
provides	O
various	O
means	O
of	O
quantifying	O
model	B
capacity	O
.	O
among	O
these	O
,	O
the	O
most	O
well-known	O
is	O
the	O
vapnik-chervonenkis	O
dimension	O
,	O
or	O
vc	O
dimension	O
.	O
the	O
vc	O
dimension	O
measures	O
the	O
capacity	O
of	O
a	O
binary	O
classiﬁer	O
.	O
the	O
vc	O
dimension	O
is	O
deﬁned	O
as	O
being	O
the	O
largest	O
possible	O
value	O
of	O
m	O
for	O
which	O
there	O
exists	O
a	O
training	O
set	O
of	O
m	O
diﬀerent	O
x	O
points	O
that	O
the	O
classiﬁer	O
can	O
label	O
arbitrarily	O
.	O
;	O
,	O
;	O
quantifying	O
the	O
capacity	O
of	O
the	O
model	B
allows	O
statistical	O
learning	O
theory	O
to	O
make	O
quantitative	O
predictions	O
.	O
the	O
most	O
important	O
results	O
in	O
statistical	O
learning	O
theory	O
show	O
that	O
the	O
discrepancy	O
between	O
training	O
error	O
and	O
generalization	O
error	O
is	O
bounded	O
from	O
above	O
by	O
a	O
quantity	O
that	O
grows	O
as	O
the	O
model	B
capacity	O
grows	O
but	O
shrinks	O
as	O
the	O
number	O
of	O
training	O
examples	O
increases	O
(	O
vapnik	O
and	O
chervonenkis	O
,	O
1971	O
vapnik	O
1982	O
blumer	O
)	O
.	O
these	O
bounds	O
provide	O
intellectual	O
justiﬁcation	O
that	O
machine	O
learning	O
algorithms	O
can	O
work	B
,	O
but	O
they	O
are	O
rarely	O
used	O
in	O
practice	O
when	O
working	O
with	O
deep	O
learning	O
algorithms	O
.	O
this	O
is	O
in	O
part	O
because	O
the	O
bounds	O
are	O
often	O
quite	O
loose	O
and	O
in	O
part	O
because	O
it	O
can	O
be	O
quite	O
diﬃcult	O
to	O
determine	O
the	O
capacity	O
of	O
deep	O
learning	O
algorithms	O
.	O
the	O
problem	O
of	O
determining	O
the	O
capacity	O
of	O
a	O
deep	O
learning	O
model	B
is	O
especially	O
diﬃcult	O
because	O
the	O
eﬀective	O
capacity	O
is	O
limited	O
by	O
the	O
capabilities	O
of	O
the	O
optimization	O
algorithm	O
,	O
and	O
we	O
have	O
little	O
theoretical	O
understanding	O
of	O
the	O
very	O
general	O
non-convex	O
optimization	O
problems	O
involved	O
in	O
deep	O
learning	O
.	O
1989	O
vapnik	O
1995	O
et	O
al.	O
,	O
;	O
,	O
we	O
must	O
remember	O
that	O
while	O
simpler	O
functions	O
are	O
more	O
likely	O
to	O
generalize	O
(	O
to	O
have	O
a	O
small	O
gap	O
between	O
training	O
and	O
test	O
error	O
)	O
we	O
must	O
still	O
choose	O
a	O
suﬃciently	O
complex	O
hypothesis	O
to	O
achieve	O
low	O
training	O
error	O
.	O
typically	O
,	O
training	O
error	O
decreases	O
until	O
it	O
asymptotes	O
to	O
the	O
minimum	O
possible	O
error	O
value	O
as	O
model	B
capacity	O
increases	O
(	O
assuming	O
the	O
error	O
measure	O
has	O
a	O
minimum	O
value	O
)	O
.	O
typically	O
,	O
generalization	O
error	O
has	O
a	O
u-shaped	O
curve	O
as	O
a	O
function	O
of	O
model	B
capacity	O
.	O
this	O
is	O
illustrated	O
in	O
ﬁgure	O
.5.3	O
to	O
reach	O
the	O
most	O
extreme	O
case	O
of	O
arbitrarily	O
high	O
capacity	O
,	O
we	O
introduce	O
114	O
chapter	O
5.	O
machine	O
learning	O
basics	O
underﬁtting	O
zone	O
overﬁtting	O
zone	O
training	O
error	O
generalization	O
error	O
r	O
o	O
r	O
r	O
e	O
0	O
optimal	O
capacity	O
capacity	O
generalization	O
gap	O
figure	O
5.3	O
:	O
typical	O
relationship	O
between	O
capacity	O
and	O
error	O
.	O
training	O
and	O
test	O
error	O
behave	O
diﬀerently	O
.	O
at	O
the	O
left	O
end	O
of	O
the	O
graph	O
,	O
training	O
error	O
and	O
generalization	O
error	O
are	O
both	O
high	O
.	O
this	O
is	O
the	O
underﬁtting	O
regime	O
.	O
as	O
we	O
increase	O
capacity	O
,	O
training	O
error	O
decreases	O
,	O
but	O
the	O
gap	O
between	O
training	O
and	O
generalization	O
error	O
increases	O
.	O
eventually	O
,	O
the	O
size	O
of	O
this	O
gap	O
outweighs	O
the	O
decrease	O
in	O
training	O
error	O
,	O
and	O
we	O
enter	O
the	O
overﬁtting	O
regime	O
,	O
where	O
capacity	O
is	O
too	O
large	O
,	O
above	O
the	O
optimal	O
capacity	O
.	O
the	O
concept	O
of	O
non-parametric	O
models	O
.	O
so	O
far	O
,	O
we	O
have	O
seen	O
only	O
parametric	O
models	O
,	O
such	O
as	O
linear	O
regression	O
.	O
parametric	O
models	O
learn	O
a	O
function	O
described	O
by	O
a	O
parameter	O
vector	O
whose	O
size	O
is	O
ﬁnite	O
and	O
ﬁxed	O
before	O
any	O
data	O
is	O
observed	O
.	O
non-parametric	O
models	O
have	O
no	O
such	O
limitation	O
.	O
sometimes	O
,	O
non-parametric	O
models	O
are	O
just	O
theoretical	O
abstractions	O
(	O
such	O
as	O
an	O
algorithm	O
that	O
searches	O
over	O
all	O
possible	O
probability	O
distributions	O
)	O
that	O
can	O
not	O
be	O
implemented	O
in	O
practice	O
.	O
however	O
,	O
we	O
can	O
also	O
design	O
practical	O
non-parametric	O
models	O
by	O
making	O
their	O
complexity	O
a	O
function	O
of	O
the	O
training	O
set	O
size	O
.	O
one	O
example	O
of	O
such	O
an	O
algorithm	O
is	O
nearest	O
neighbor	O
regression	O
.	O
unlike	O
linear	O
regression	O
,	O
which	O
has	O
a	O
ﬁxed-length	O
vector	O
of	O
weights	O
,	O
the	O
nearest	O
neighbor	O
regression	O
model	B
simply	O
stores	O
the	O
x	O
and	O
y	O
from	O
the	O
training	O
set	O
.	O
when	O
asked	O
to	O
classify	O
a	O
test	O
−	O
||	O
point	O
x	O
,	O
the	O
model	B
looks	O
up	O
the	O
nearest	O
entry	O
in	O
the	O
training	O
set	O
and	O
returns	O
the	O
x	O
2	O
associated	O
regression	O
target	O
.	O
in	O
other	O
words	O
,	O
ˆy	O
=	O
yi	O
where	O
i	O
=	O
arg	O
min	O
2.	O
the	O
algorithm	O
can	O
also	O
be	O
generalized	O
to	O
distance	O
metrics	O
other	O
than	O
the	O
l2	O
norm	O
,	O
such	O
as	O
learned	O
distance	O
metrics	O
(	O
)	O
.	O
if	O
the	O
algorithm	O
is	O
allowed	O
to	O
break	O
ties	O
by	O
averaging	O
the	O
yi	O
values	O
for	O
all	O
xi	O
,	O
:	O
that	O
are	O
tied	O
for	O
nearest	O
,	O
then	O
this	O
algorithm	O
is	O
able	O
to	O
achieve	O
the	O
minimum	O
possible	O
training	O
error	O
(	O
which	O
might	O
be	O
greater	O
than	O
zero	O
,	O
if	O
two	O
identical	O
inputs	O
are	O
associated	O
with	O
diﬀerent	O
outputs	O
)	O
on	O
any	O
regression	O
dataset	O
.	O
goldberger	O
et	O
al	O
.	O
2005	O
||	O
xi	O
,	O
:	O
,	O
finally	O
,	O
we	O
can	O
also	O
create	O
a	O
non-parametric	O
learning	O
algorithm	O
by	O
wrapping	O
a	O
115	O
chapter	O
5.	O
machine	O
learning	O
basics	O
parametric	O
learning	O
algorithm	O
inside	O
another	O
algorithm	O
that	O
increases	O
the	O
number	O
of	O
parameters	O
as	O
needed	O
.	O
for	O
example	O
,	O
we	O
could	O
imagine	O
an	O
outer	O
loop	O
of	O
learning	O
that	O
changes	O
the	O
degree	O
of	O
the	O
polynomial	O
learned	O
by	O
linear	O
regression	O
on	O
top	O
of	O
a	O
polynomial	O
expansion	O
of	O
the	O
input	O
.	O
the	O
ideal	O
model	B
is	O
an	O
oracle	O
that	O
simply	O
knows	O
the	O
true	O
probability	O
distribution	O
that	O
generates	O
the	O
data	O
.	O
even	O
such	O
a	O
model	B
will	O
still	O
incur	O
some	O
error	O
on	O
many	O
problems	O
,	O
because	O
there	O
may	O
still	O
be	O
some	O
noise	O
in	O
the	O
distribution	O
.	O
in	O
the	O
case	O
of	O
supervised	O
learning	O
,	O
the	O
mapping	O
from	O
x	O
to	O
y	O
may	O
be	O
inherently	O
stochastic	O
,	O
or	O
y	O
may	O
be	O
a	O
deterministic	O
function	O
that	O
involves	O
other	O
variables	O
besides	O
those	O
included	O
in	O
x.	O
the	O
error	O
incurred	O
by	O
an	O
oracle	O
making	O
predictions	O
from	O
the	O
true	O
distribution	O
bayes	O
error	O
.	O
is	O
called	O
the	O
p	O
(	O
x	O
)	O
,	O
y	O
training	O
and	O
generalization	O
error	O
vary	O
as	O
the	O
size	O
of	O
the	O
training	O
set	O
varies	O
.	O
expected	O
generalization	O
error	O
can	O
never	O
increase	O
as	O
the	O
number	O
of	O
training	O
examples	O
increases	O
.	O
for	O
non-parametric	O
models	O
,	O
more	O
data	O
yields	O
better	O
generalization	O
until	O
the	O
best	O
possible	O
error	O
is	O
achieved	O
.	O
any	O
ﬁxed	O
parametric	O
model	B
with	O
less	O
than	O
optimal	O
capacity	O
will	O
asymptote	O
to	O
an	O
error	O
value	O
that	O
exceeds	O
the	O
bayes	O
error	O
.	O
see	O
ﬁgure	O
for	O
an	O
illustration	O
.	O
note	O
that	O
it	O
is	O
possible	O
for	O
the	O
model	B
to	O
have	O
optimal	O
capacity	O
and	O
yet	O
still	O
have	O
a	O
large	O
gap	O
between	O
training	O
and	O
generalization	O
error	O
.	O
in	O
this	O
situation	O
,	O
we	O
may	O
be	O
able	O
to	O
reduce	O
this	O
gap	O
by	O
gathering	O
more	O
training	O
examples	O
.	O
5.4	O
5.2.1	O
the	O
no	O
free	O
lunch	O
theorem	O
learning	O
theory	O
claims	O
that	O
a	O
machine	O
learning	O
algorithm	O
can	O
generalize	O
well	O
from	O
a	O
ﬁnite	O
training	O
set	O
of	O
examples	O
.	O
this	O
seems	O
to	O
contradict	O
some	O
basic	O
principles	O
of	O
logic	O
.	O
inductive	O
reasoning	O
,	O
or	O
inferring	O
general	O
rules	O
from	O
a	O
limited	O
set	O
of	O
examples	O
,	O
is	O
not	O
logically	O
valid	O
.	O
to	O
logically	O
infer	O
a	O
rule	O
describing	O
every	O
member	O
of	O
a	O
set	O
,	O
one	O
must	O
have	O
information	O
about	O
every	O
member	O
of	O
that	O
set	O
.	O
in	O
part	O
,	O
machine	O
learning	O
avoids	O
this	O
problem	O
by	O
oﬀering	O
only	O
probabilistic	O
rules	O
,	O
rather	O
than	O
the	O
entirely	O
certain	O
rules	O
used	O
in	O
purely	O
logical	O
reasoning	O
.	O
machine	O
learning	O
promises	O
to	O
ﬁnd	O
rules	O
that	O
are	O
probably	O
members	O
of	O
the	O
set	O
they	O
concern	O
.	O
correct	O
about	O
most	O
unfortunately	O
,	O
even	O
this	O
does	O
not	O
resolve	O
the	O
entire	O
problem	O
.	O
the	O
no	O
free	O
)	O
states	O
that	O
,	O
averaged	O
over	O
lunch	O
theorem	O
for	O
machine	O
learning	O
(	O
wolpert	O
1996	O
all	O
possible	O
data	O
generating	O
distributions	O
,	O
every	O
classiﬁcation	O
algorithm	O
has	O
the	O
same	O
error	O
rate	O
when	O
classifying	O
previously	O
unobserved	O
points	O
.	O
in	O
other	O
words	O
,	O
in	O
some	O
sense	O
,	O
no	O
machine	O
learning	O
algorithm	O
is	O
universally	O
any	O
better	O
than	O
any	O
other	O
.	O
the	O
most	O
sophisticated	O
algorithm	O
we	O
can	O
conceive	O
of	O
has	O
the	O
same	O
average	O
,	O
116	O
chapter	O
5.	O
machine	O
learning	O
basics	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
figure	O
5.4	O
:	O
the	O
eﬀect	O
of	O
the	O
training	O
dataset	O
size	O
on	O
the	O
train	O
and	O
test	O
error	O
,	O
as	O
well	O
as	O
on	O
the	O
optimal	O
model	B
capacity	O
.	O
we	O
constructed	O
a	O
synthetic	O
regression	O
problem	O
based	O
on	O
adding	O
a	O
moderate	O
amount	O
of	O
noise	O
to	O
a	O
degree-5	O
polynomial	O
,	O
generated	O
a	O
single	O
test	O
set	O
,	O
and	O
then	O
generated	O
several	O
diﬀerent	O
sizes	O
of	O
training	O
set	O
.	O
for	O
each	O
size	O
,	O
we	O
generated	O
40	O
diﬀerent	O
training	O
sets	O
in	O
order	O
to	O
plot	O
error	O
bars	O
showing	O
95	O
percent	O
conﬁdence	O
intervals	O
.	O
(	O
top	O
)	O
the	O
mse	O
on	O
the	O
training	O
and	O
test	O
set	O
for	O
two	O
diﬀerent	O
models	O
:	O
a	O
quadratic	O
model	B
,	O
and	O
a	O
model	B
with	O
degree	O
chosen	O
to	O
minimize	O
the	O
test	O
error	O
.	O
both	O
are	O
ﬁt	O
in	O
closed	O
form	O
.	O
for	O
the	O
quadratic	O
model	B
,	O
the	O
training	O
error	O
increases	O
as	O
the	O
size	O
of	O
the	O
training	O
set	O
increases	O
.	O
this	O
is	O
because	O
larger	O
datasets	O
are	O
harder	O
to	O
ﬁt	O
.	O
simultaneously	O
,	O
the	O
test	O
error	O
decreases	O
,	O
because	O
fewer	O
incorrect	O
hypotheses	O
are	O
consistent	O
with	O
the	O
training	O
data	O
.	O
the	O
quadratic	O
model	B
does	O
not	O
have	O
enough	O
capacity	O
to	O
solve	O
the	O
task	O
,	O
so	O
its	O
test	O
error	O
asymptotes	O
to	O
a	O
high	O
value	O
.	O
the	O
test	O
error	O
at	O
optimal	O
capacity	O
asymptotes	O
to	O
the	O
bayes	O
error	O
.	O
the	O
training	O
error	O
can	O
fall	O
below	O
the	O
bayes	O
error	O
,	O
due	O
to	O
the	O
ability	O
of	O
the	O
training	O
algorithm	O
to	O
memorize	O
speciﬁc	O
instances	O
of	O
the	O
training	O
set	O
.	O
as	O
the	O
training	O
size	O
increases	O
to	O
inﬁnity	O
,	O
the	O
training	O
error	O
of	O
any	O
ﬁxed-capacity	O
model	B
(	O
here	O
,	O
the	O
quadratic	O
model	B
)	O
must	O
rise	O
to	O
at	O
least	O
the	O
bayes	O
error	O
.	O
as	O
the	O
training	O
set	O
size	O
increases	O
,	O
the	O
optimal	O
capacity	O
(	O
shown	O
here	O
as	O
the	O
degree	O
of	O
the	O
optimal	O
polynomial	O
regressor	O
)	O
increases	O
.	O
the	O
optimal	O
capacity	O
plateaus	O
after	O
reaching	O
suﬃcient	O
complexity	O
to	O
solve	O
the	O
task	O
.	O
(	O
bottom	O
)	O
117	O
chapter	O
5.	O
machine	O
learning	O
basics	O
performance	O
(	O
over	O
all	O
possible	O
tasks	O
)	O
as	O
merely	O
predicting	O
that	O
every	O
point	O
belongs	O
to	O
the	O
same	O
class	O
.	O
fortunately	O
,	O
these	O
results	O
hold	O
only	O
when	O
we	O
average	O
over	O
possible	O
data	O
generating	O
distributions	O
.	O
if	O
we	O
make	O
assumptions	O
about	O
the	O
kinds	O
of	O
probability	O
distributions	O
we	O
encounter	O
in	O
real-world	O
applications	O
,	O
then	O
we	O
can	O
design	O
learning	O
algorithms	O
that	O
perform	O
well	O
on	O
these	O
distributions	O
.	O
all	O
this	O
means	O
that	O
the	O
goal	O
of	O
machine	O
learning	O
research	O
is	O
not	O
to	O
seek	O
a	O
universal	O
learning	O
algorithm	O
or	O
the	O
absolute	O
best	O
learning	O
algorithm	O
.	O
instead	O
,	O
our	O
goal	O
is	O
to	O
understand	O
what	O
kinds	O
of	O
distributions	O
are	O
relevant	O
to	O
the	O
“	O
real	O
world	O
”	O
that	O
an	O
ai	O
agent	O
experiences	O
,	O
and	O
what	O
kinds	O
of	O
machine	O
learning	O
algorithms	O
perform	O
well	O
on	O
data	O
drawn	O
from	O
the	O
kinds	O
of	O
data	O
generating	O
distributions	O
we	O
care	O
about	O
.	O
5.2.2	O
regularization	O
the	O
no	O
free	O
lunch	O
theorem	O
implies	O
that	O
we	O
must	O
design	O
our	O
machine	O
learning	O
algorithms	O
to	O
perform	O
well	O
on	O
a	O
speciﬁc	O
task	O
.	O
we	O
do	O
so	O
by	O
building	O
a	O
set	O
of	O
preferences	O
into	O
the	O
learning	O
algorithm	O
.	O
when	O
these	O
preferences	O
are	O
aligned	O
with	O
the	O
learning	O
problems	O
we	O
ask	O
the	O
algorithm	O
to	O
solve	O
,	O
it	O
performs	O
better	O
.	O
so	O
far	O
,	O
the	O
only	O
method	O
of	O
modifying	O
a	O
learning	O
algorithm	O
that	O
we	O
have	O
discussed	O
concretely	O
is	O
to	O
increase	O
or	O
decrease	O
the	O
model	B
’	O
s	O
representational	O
capacity	O
by	O
adding	O
or	O
removing	O
functions	O
from	O
the	O
hypothesis	O
space	O
of	O
solutions	O
the	O
learning	O
algorithm	O
is	O
able	O
to	O
choose	O
.	O
we	O
gave	O
the	O
speciﬁc	O
example	O
of	O
increasing	O
or	O
decreasing	O
the	O
degree	O
of	O
a	O
polynomial	O
for	O
a	O
regression	O
problem	O
.	O
the	O
view	O
we	O
have	O
described	O
so	O
far	O
is	O
oversimpliﬁed	O
.	O
the	O
behavior	O
of	O
our	O
algorithm	O
is	O
strongly	O
aﬀected	O
not	O
just	O
by	O
how	O
large	O
we	O
make	O
the	O
set	O
of	O
functions	O
allowed	O
in	O
its	O
hypothesis	O
space	O
,	O
but	O
by	O
the	O
speciﬁc	O
identity	O
of	O
those	O
functions	O
.	O
the	O
learning	O
algorithm	O
we	O
have	O
studied	O
so	O
far	O
,	O
linear	O
regression	O
,	O
has	O
a	O
hypothesis	O
space	O
consisting	O
of	O
the	O
set	O
of	O
linear	O
functions	O
of	O
its	O
input	O
.	O
these	O
linear	O
functions	O
can	O
be	O
very	O
useful	O
for	O
problems	O
where	O
the	O
relationship	O
between	O
inputs	O
and	O
outputs	O
truly	O
is	O
close	O
to	O
linear	O
.	O
they	O
are	O
less	O
useful	O
for	O
problems	O
that	O
behave	O
in	O
a	O
very	O
nonlinear	O
fashion	O
.	O
for	O
example	O
,	O
linear	O
regression	O
would	O
not	O
perform	O
very	O
well	O
if	O
we	O
tried	O
to	O
use	O
it	O
to	O
predict	O
sin	O
(	O
x	O
)	O
from	O
x.	O
we	O
can	O
thus	O
control	O
the	O
performance	O
of	O
our	O
algorithms	O
by	O
choosing	O
what	O
kind	O
of	O
functions	O
we	O
allow	O
them	O
to	O
draw	O
solutions	O
from	O
,	O
as	O
well	O
as	O
by	O
controlling	O
the	O
amount	O
of	O
these	O
functions	O
.	O
we	O
can	O
also	O
give	O
a	O
learning	O
algorithm	O
a	O
preference	O
for	O
one	O
solution	O
in	O
its	O
hypothesis	O
space	O
to	O
another	O
.	O
this	O
means	O
that	O
both	O
functions	O
are	O
eligible	O
,	O
but	O
one	O
is	O
preferred	O
.	O
the	O
unpreferred	O
solution	O
will	O
be	O
chosen	O
only	O
if	O
it	O
ﬁts	O
the	O
training	O
118	O
chapter	O
5.	O
machine	O
learning	O
basics	O
data	O
signiﬁcantly	O
better	O
than	O
the	O
preferred	O
solution	O
.	O
for	O
example	O
,	O
we	O
can	O
modify	O
the	O
training	O
criterion	O
for	O
linear	O
regression	O
to	O
include	O
weight	O
decay	O
.	O
to	O
perform	O
linear	O
regression	O
with	O
weight	O
decay	O
,	O
we	O
minimize	O
a	O
sum	O
comprising	O
both	O
the	O
mean	O
squared	O
error	O
on	O
the	O
training	O
and	O
a	O
criterion	O
j	O
(	O
w	O
)	O
that	O
expresses	O
a	O
preference	O
for	O
the	O
weights	O
to	O
have	O
smaller	O
squared	O
l2	O
norm	O
.	O
speciﬁcally	O
,	O
	O
)	O
=	O
w	O
msetrain	O
+	O
λw	O
j	O
(	O
w	O
,	O
(	O
5.18	O
)	O
where	O
λ	O
is	O
a	O
value	O
chosen	O
ahead	O
of	O
time	O
that	O
controls	O
the	O
strength	O
of	O
our	O
preference	O
for	O
smaller	O
weights	O
.	O
when	O
λ	O
=	O
0	O
,	O
we	O
impose	O
no	O
preference	O
,	O
and	O
larger	O
λ	O
forces	O
the	O
weights	O
to	O
become	O
smaller	O
.	O
minimizing	O
j	O
(	O
w	O
)	O
results	O
in	O
a	O
choice	O
of	O
weights	O
that	O
make	O
a	O
tradeoﬀ	O
between	O
ﬁtting	O
the	O
training	O
data	O
and	O
being	O
small	O
.	O
this	O
gives	O
us	O
solutions	O
that	O
have	O
a	O
smaller	O
slope	O
,	O
or	O
put	O
weight	O
on	O
fewer	O
of	O
the	O
features	O
.	O
as	O
an	O
example	O
of	O
how	O
we	O
can	O
control	O
a	O
model	B
’	O
s	O
tendency	O
to	O
overﬁt	O
or	O
underﬁt	O
via	O
weight	O
decay	O
,	O
we	O
can	O
train	O
a	O
high-degree	O
polynomial	O
regression	O
model	B
with	O
diﬀerent	O
values	O
of	O
for	O
the	O
results	O
.	O
.	O
see	O
ﬁgure	O
5.5	O
λ	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
5.2	O
figure	O
5.5	O
:	O
we	O
ﬁt	O
a	O
high-degree	O
polynomial	O
regression	O
model	B
to	O
our	O
example	O
training	O
set	O
from	O
ﬁgure	O
.	O
the	O
true	O
function	O
is	O
quadratic	O
,	O
but	O
here	O
we	O
use	O
only	O
models	O
with	O
degree	O
9.	O
we	O
vary	O
the	O
amount	O
of	O
weight	O
decay	O
to	O
prevent	O
these	O
high-degree	O
models	O
from	O
overﬁtting	O
.	O
(	O
left	O
)	O
with	O
very	O
large	O
λ	O
,	O
we	O
can	O
force	O
the	O
model	B
to	O
learn	O
a	O
function	O
with	O
no	O
slope	O
at	O
all	O
.	O
this	O
underﬁts	O
because	O
it	O
can	O
only	O
represent	O
a	O
constant	O
function	O
.	O
with	O
a	O
medium	O
value	O
of	O
,	O
the	O
learning	O
algorithm	O
recovers	O
a	O
curve	O
with	O
the	O
right	O
general	O
shape	O
.	O
even	O
though	O
the	O
model	B
is	O
capable	O
of	O
representing	O
functions	O
with	O
much	O
more	O
complicated	O
shape	O
,	O
weight	O
decay	O
has	O
encouraged	O
it	O
to	O
use	O
a	O
simpler	O
function	O
described	O
by	O
smaller	O
coeﬃcients	O
.	O
with	O
weight	O
decay	O
approaching	O
zero	O
(	O
i.e.	O
,	O
using	O
the	O
moore-penrose	O
pseudoinverse	O
to	O
solve	O
the	O
underdetermined	O
problem	O
with	O
minimal	O
regularization	O
)	O
,	O
the	O
degree-9	O
polynomial	O
overﬁts	O
signiﬁcantly	O
,	O
as	O
we	O
saw	O
in	O
ﬁgure	O
(	O
center	O
)	O
(	O
right	O
)	O
.5.2	O
λ	O
119	O
chapter	O
5.	O
machine	O
learning	O
basics	O
more	O
generally	O
,	O
we	O
can	O
regularize	O
a	O
model	B
that	O
learns	O
a	O
function	O
f	O
(	O
x	O
;	O
θ	O
)	O
by	O
adding	O
a	O
penalty	O
called	O
a	O
regularizer	O
to	O
the	O
cost	O
function	O
.	O
in	O
the	O
case	O
of	O
weight	O
decay	O
,	O
the	O
regularizer	O
is	O
ω	O
(	O
w	O
)	O
=	O
w	O
,	O
we	O
will	O
see	O
that	O
many	O
other	O
7	O
regularizers	O
are	O
possible	O
.	O
w.	O
in	O
chapter	O
	O
expressing	O
preferences	O
for	O
one	O
function	O
over	O
another	O
is	O
a	O
more	O
general	O
way	O
of	O
controlling	O
a	O
model	B
’	O
s	O
capacity	O
than	O
including	O
or	O
excluding	O
members	O
from	O
the	O
hypothesis	O
space	O
.	O
we	O
can	O
think	O
of	O
excluding	O
a	O
function	O
from	O
a	O
hypothesis	O
space	O
as	O
expressing	O
an	O
inﬁnitely	O
strong	O
preference	O
against	O
that	O
function	O
.	O
in	O
our	O
weight	O
decay	O
example	O
,	O
we	O
expressed	O
our	O
preference	O
for	O
linear	O
functions	O
deﬁned	O
with	O
smaller	O
weights	O
explicitly	O
,	O
via	O
an	O
extra	O
term	O
in	O
the	O
criterion	O
we	O
minimize	O
.	O
there	O
are	O
many	O
other	O
ways	O
of	O
expressing	O
preferences	O
for	O
diﬀerent	O
solutions	O
,	O
both	O
implicitly	O
and	O
explicitly	O
.	O
together	O
,	O
these	O
diﬀerent	O
approaches	O
are	O
known	O
as	O
regularization	O
.	O
regularization	O
is	O
any	O
modiﬁcation	O
we	O
make	O
to	O
a	O
learning	O
algorithm	O
that	O
is	O
intended	O
to	O
reduce	O
its	O
generalization	O
error	O
but	O
not	O
its	O
training	O
error	O
.	O
regularization	O
is	O
one	O
of	O
the	O
central	O
concerns	O
of	O
the	O
ﬁeld	O
of	O
machine	O
learning	O
,	O
rivaled	O
in	O
its	O
importance	O
only	O
by	O
optimization	O
.	O
the	O
no	O
free	O
lunch	O
theorem	O
has	O
made	O
it	O
clear	O
that	O
there	O
is	O
no	O
best	O
machine	O
learning	O
algorithm	O
,	O
and	O
,	O
in	O
particular	O
,	O
no	O
best	O
form	O
of	O
regularization	O
.	O
instead	O
we	O
must	O
choose	O
a	O
form	O
of	O
regularization	O
that	O
is	O
well-suited	O
to	O
the	O
particular	O
task	O
we	O
want	O
to	O
solve	O
.	O
the	O
philosophy	O
of	O
deep	O
learning	O
in	O
general	O
and	O
this	O
book	O
in	O
particular	O
is	O
that	O
a	O
very	O
wide	O
range	O
of	O
tasks	O
(	O
such	O
as	O
all	O
of	O
the	O
intellectual	O
tasks	O
that	O
people	O
can	O
do	O
)	O
may	O
all	O
be	O
solved	O
eﬀectively	O
using	O
very	O
general-purpose	O
forms	O
of	O
regularization	O
.	O
5.3	O
hyperparameters	O
and	O
validation	O
sets	O
most	O
machine	O
learning	O
algorithms	O
have	O
several	O
settings	O
that	O
we	O
can	O
use	O
to	O
control	O
the	O
behavior	O
of	O
the	O
learning	O
algorithm	O
.	O
these	O
settings	O
are	O
called	O
hyperparame-	O
ters	O
.	O
the	O
values	O
of	O
hyperparameters	O
are	O
not	O
adapted	O
by	O
the	O
learning	O
algorithm	O
itself	O
(	O
though	O
we	O
can	O
design	O
a	O
nested	O
learning	O
procedure	O
where	O
one	O
learning	O
algorithm	O
learns	O
the	O
best	O
hyperparameters	O
for	O
another	O
learning	O
algorithm	O
)	O
.	O
in	O
the	O
polynomial	O
regression	O
example	O
we	O
saw	O
in	O
ﬁgure	O
,	O
there	O
is	O
a	O
single	O
hyperparameter	O
:	O
the	O
degree	O
of	O
the	O
polynomial	O
,	O
which	O
acts	O
as	O
a	O
capacity	O
hyper-	O
parameter	O
.	O
the	O
λ	O
value	O
used	O
to	O
control	O
the	O
strength	O
of	O
weight	O
decay	O
is	O
another	O
example	O
of	O
a	O
hyperparameter	O
.	O
5.2	O
sometimes	O
a	O
setting	O
is	O
chosen	O
to	O
be	O
a	O
hyperparameter	O
that	O
the	O
learning	O
al-	O
gorithm	O
does	O
not	O
learn	O
because	O
it	O
is	O
diﬃcult	O
to	O
optimize	O
.	O
more	O
frequently	O
,	O
the	O
120	O
chapter	O
5.	O
machine	O
learning	O
basics	O
setting	O
must	O
be	O
a	O
hyperparameter	O
because	O
it	O
is	O
not	O
appropriate	O
to	O
learn	O
that	O
hyperparameter	O
on	O
the	O
training	O
set	O
.	O
this	O
applies	O
to	O
all	O
hyperparameters	O
that	O
control	O
model	B
capacity	O
.	O
if	O
learned	O
on	O
the	O
training	O
set	O
,	O
such	O
hyperparameters	O
would	O
always	O
choose	O
the	O
maximum	O
possible	O
model	B
capacity	O
,	O
resulting	O
in	O
overﬁtting	O
(	O
refer	O
to	O
ﬁgure	O
)	O
.	O
for	O
example	O
,	O
we	O
can	O
always	O
ﬁt	O
the	O
training	O
set	O
better	O
with	O
a	O
higher	O
degree	O
polynomial	O
and	O
a	O
weight	O
decay	O
setting	O
of	O
λ	O
=	O
0	O
than	O
we	O
could	O
with	O
a	O
lower	O
degree	O
polynomial	O
and	O
a	O
positive	O
weight	O
decay	O
setting	O
.	O
5.3	O
to	O
solve	O
this	O
problem	O
,	O
we	O
need	O
a	O
validation	O
set	O
of	O
examples	O
that	O
the	O
training	O
algorithm	O
does	O
not	O
observe	O
.	O
earlier	O
we	O
discussed	O
how	O
a	O
held-out	O
test	O
set	O
,	O
composed	O
of	O
examples	O
coming	O
from	O
the	O
same	O
distribution	O
as	O
the	O
training	O
set	O
,	O
can	O
be	O
used	O
to	O
estimate	O
the	O
generalization	O
error	O
of	O
a	O
learner	O
,	O
after	O
the	O
learning	O
process	O
has	O
completed	O
.	O
it	O
is	O
important	O
that	O
the	O
test	O
examples	O
are	O
not	O
used	O
in	O
any	O
way	O
to	O
make	O
choices	O
about	O
the	O
model	B
,	O
including	O
its	O
hyperparameters	O
.	O
for	O
this	O
reason	O
,	O
no	O
example	O
from	O
the	O
test	O
set	O
can	O
be	O
used	O
in	O
the	O
validation	O
set	O
.	O
therefore	O
,	O
we	O
always	O
construct	O
the	O
validation	O
set	O
from	O
the	O
training	O
data	O
.	O
speciﬁcally	O
,	O
we	O
split	O
the	O
training	O
data	O
into	O
two	O
disjoint	O
subsets	O
.	O
one	O
of	O
these	O
subsets	O
is	O
used	O
to	O
learn	O
the	O
parameters	O
.	O
the	O
other	O
subset	O
is	O
our	O
validation	O
set	O
,	O
used	O
to	O
estimate	O
the	O
generalization	O
error	O
during	O
or	O
after	O
training	O
,	O
allowing	O
for	O
the	O
hyperparameters	O
to	O
be	O
updated	O
accordingly	O
.	O
the	O
subset	O
of	O
data	O
used	O
to	O
learn	O
the	O
parameters	O
is	O
still	O
typically	O
called	O
the	O
training	O
set	O
,	O
even	O
though	O
this	O
may	O
be	O
confused	O
with	O
the	O
larger	O
pool	O
of	O
data	O
used	O
for	O
the	O
entire	O
training	O
process	O
.	O
the	O
subset	O
of	O
data	O
used	O
to	O
guide	O
the	O
selection	O
of	O
hyperparameters	O
is	O
called	O
the	O
validation	O
set	O
.	O
typically	O
,	O
one	O
uses	O
about	O
80	O
%	O
of	O
the	O
training	O
data	O
for	O
training	O
and	O
20	O
%	O
for	O
validation	O
.	O
since	O
the	O
validation	O
set	O
is	O
used	O
to	O
“	O
train	O
”	O
the	O
hyperparameters	O
,	O
the	O
validation	O
set	O
error	O
will	O
underestimate	O
the	O
generalization	O
error	O
,	O
though	O
typically	O
by	O
a	O
smaller	O
amount	O
than	O
the	O
training	O
error	O
.	O
after	O
all	O
hyperparameter	O
optimization	O
is	O
complete	O
,	O
the	O
generalization	O
error	O
may	O
be	O
estimated	O
using	O
the	O
test	O
set	O
.	O
in	O
practice	O
,	O
when	O
the	O
same	O
test	O
set	O
has	O
been	O
used	O
repeatedly	O
to	O
evaluate	O
performance	O
of	O
diﬀerent	O
algorithms	O
over	O
many	O
years	O
,	O
and	O
especially	O
if	O
we	O
consider	O
all	O
the	O
attempts	O
from	O
the	O
scientiﬁc	O
community	O
at	O
beating	O
the	O
reported	O
state-of-	O
the-art	O
performance	O
on	O
that	O
test	O
set	O
,	O
we	O
end	O
up	O
having	O
optimistic	O
evaluations	O
with	O
the	O
test	O
set	O
as	O
well	O
.	O
benchmarks	O
can	O
thus	O
become	O
stale	O
and	O
then	O
do	O
not	O
reﬂect	O
the	O
true	O
ﬁeld	O
performance	O
of	O
a	O
trained	O
system	O
.	O
thankfully	O
,	O
the	O
community	O
tends	O
to	O
move	O
on	O
to	O
new	O
(	O
and	O
usually	O
more	O
ambitious	O
and	O
larger	O
)	O
benchmark	O
datasets	O
.	O
121	O
chapter	O
5.	O
machine	O
learning	O
basics	O
5.3.1	O
cross-validation	O
dividing	O
the	O
dataset	O
into	O
a	O
ﬁxed	O
training	O
set	O
and	O
a	O
ﬁxed	O
test	O
set	O
can	O
be	O
problematic	O
if	O
it	O
results	O
in	O
the	O
test	O
set	O
being	O
small	O
.	O
a	O
small	O
test	O
set	O
implies	O
statistical	O
uncertainty	O
around	O
the	O
estimated	O
average	O
test	O
error	O
,	O
making	O
it	O
diﬃcult	O
to	O
claim	O
that	O
algorithm	O
a	O
works	O
better	O
than	O
algorithm	O
on	O
the	O
given	O
task	O
.	O
b	O
when	O
the	O
dataset	O
has	O
hundreds	O
of	O
thousands	O
of	O
examples	O
or	O
more	O
,	O
this	O
is	O
not	O
a	O
serious	O
issue	O
.	O
when	O
the	O
dataset	O
is	O
too	O
small	O
,	O
are	O
alternative	O
procedures	O
enable	O
one	O
to	O
use	O
all	O
of	O
the	O
examples	O
in	O
the	O
estimation	O
of	O
the	O
mean	O
test	O
error	O
,	O
at	O
the	O
price	O
of	O
increased	O
computational	O
cost	O
.	O
these	O
procedures	O
are	O
based	O
on	O
the	O
idea	O
of	O
repeating	O
the	O
training	O
and	O
testing	O
computation	O
on	O
diﬀerent	O
randomly	O
chosen	O
subsets	O
or	O
splits	O
of	O
the	O
original	O
dataset	O
.	O
the	O
most	O
common	O
of	O
these	O
is	O
the	O
k-fold	O
cross-validation	O
procedure	O
,	O
shown	O
in	O
algorithm	O
,	O
in	O
which	O
a	O
partition	O
of	O
the	O
dataset	O
is	O
formed	O
by	O
splitting	O
it	O
into	O
k	O
non-overlapping	O
subsets	O
.	O
the	O
test	O
error	O
may	O
then	O
be	O
estimated	O
by	O
taking	O
the	O
average	O
test	O
error	O
across	O
k	O
trials	O
.	O
on	O
trial	O
i	O
,	O
the	O
i	O
-th	O
subset	O
of	O
the	O
data	O
is	O
used	O
as	O
the	O
test	O
set	O
and	O
the	O
rest	O
of	O
the	O
data	O
is	O
used	O
as	O
the	O
training	O
set	O
.	O
one	O
problem	O
is	O
that	O
there	O
exist	O
no	O
unbiased	O
estimators	O
of	O
the	O
variance	O
of	O
such	O
average	O
error	O
estimators	O
(	O
bengio	O
and	O
grandvalet	O
2004	O
)	O
,	O
but	O
approximations	O
are	O
typically	O
used	O
.	O
5.1	O
,	O
5.4	O
estimators	O
,	O
bias	O
and	O
variance	O
the	O
ﬁeld	O
of	O
statistics	O
gives	O
us	O
many	O
tools	O
that	O
can	O
be	O
used	O
to	O
achieve	O
the	O
machine	O
learning	O
goal	O
of	O
solving	O
a	O
task	O
not	O
only	O
on	O
the	O
training	O
set	O
but	O
also	O
to	O
generalize	O
.	O
foundational	O
concepts	O
such	O
as	O
parameter	O
estimation	O
,	O
bias	O
and	O
variance	O
are	O
useful	O
to	O
formally	O
characterize	O
notions	O
of	O
generalization	O
,	O
underﬁtting	O
and	O
overﬁtting	O
.	O
5.4.1	O
point	O
estimation	O
point	O
estimation	O
is	O
the	O
attempt	O
to	O
provide	O
the	O
single	O
“	O
best	O
”	O
prediction	O
of	O
some	O
quantity	O
of	O
interest	O
.	O
in	O
general	O
the	O
quantity	O
of	O
interest	O
can	O
be	O
a	O
single	O
parameter	O
or	O
a	O
vector	O
of	O
parameters	O
in	O
some	O
parametric	O
model	B
,	O
such	O
as	O
the	O
weights	O
in	O
our	O
linear	O
regression	O
example	O
in	O
section	O
,	O
but	O
it	O
can	O
also	O
be	O
a	O
whole	O
function	O
.	O
5.1.4	O
in	O
order	O
to	O
distinguish	O
estimates	O
of	O
parameters	O
from	O
their	O
true	O
value	O
,	O
our	O
convention	O
will	O
be	O
to	O
denote	O
a	O
point	O
estimate	O
of	O
a	O
parameter	O
byθ	O
ˆθ	O
.	O
{	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
m	O
}	O
let	O
be	O
a	O
set	O
of	O
m	O
independent	O
and	O
identically	O
distributed	O
122	O
chapter	O
5.	O
machine	O
learning	O
basics	O
algorithm	O
5.1	O
the	O
k-fold	O
cross-validation	O
algorithm	O
.	O
it	O
can	O
be	O
used	O
to	O
estimate	O
generalization	O
error	O
of	O
a	O
learning	O
algorithm	O
a	O
when	O
the	O
given	O
dataset	O
d	O
is	O
too	O
small	O
for	O
a	O
simple	O
train/test	O
or	O
train/valid	O
split	O
to	O
yield	O
accurate	O
estimation	O
of	O
generalization	O
error	O
,	O
because	O
the	O
mean	O
of	O
a	O
loss	O
l	O
on	O
a	O
small	O
test	O
set	O
may	O
have	O
too	O
high	O
variance	O
.	O
the	O
dataset	O
d	O
contains	O
as	O
elements	O
the	O
abstract	O
examples	O
z	O
(	O
)	O
i	O
(	O
for	O
the	O
i-th	O
example	O
)	O
,	O
which	O
could	O
stand	O
for	O
an	O
(	O
input	O
,	O
target	O
)	O
pair	O
z	O
(	O
)	O
i	O
=	O
(	O
x	O
(	O
)	O
i	O
,	O
y	O
(	O
)	O
i	O
)	O
in	O
the	O
case	O
of	O
supervised	O
learning	O
,	O
or	O
for	O
just	O
an	O
input	O
z	O
(	O
)	O
i	O
=	O
x	O
(	O
)	O
i	O
in	O
the	O
case	O
of	O
unsupervised	O
learning	O
.	O
the	O
algorithm	O
returns	O
the	O
vector	O
of	O
errors	O
e	O
for	O
each	O
example	O
in	O
d	O
,	O
whose	O
mean	O
is	O
the	O
estimated	O
generalization	O
error	O
.	O
the	O
errors	O
on	O
individual	O
examples	O
can	O
be	O
used	O
to	O
compute	O
a	O
conﬁdence	O
interval	O
around	O
the	O
mean	O
(	O
equation	O
)	O
.	O
while	O
these	O
conﬁdence	O
intervals	O
are	O
not	O
well-justiﬁed	O
after	O
the	O
use	O
of	O
cross-validation	O
,	O
it	O
is	O
still	O
common	O
practice	O
to	O
use	O
them	O
to	O
declare	O
that	O
algorithm	O
a	O
is	O
better	O
than	O
algorithm	O
b	O
only	O
if	O
the	O
conﬁdence	O
interval	O
of	O
the	O
error	O
of	O
algorithm	O
a	O
lies	O
below	O
and	O
does	O
not	O
intersect	O
the	O
conﬁdence	O
interval	O
of	O
algorithm	O
b.	O
deﬁne	O
kfoldxv	O
(	O
require	O
:	O
d	O
,	O
the	O
given	O
dataset	O
,	O
with	O
elements	O
z	O
(	O
)	O
i	O
require	O
:	O
a	O
,	O
the	O
learning	O
algorithm	O
,	O
seen	O
as	O
a	O
function	O
that	O
takes	O
a	O
dataset	O
as	O
d	O
,	O
a	O
,	O
l	O
,	O
k	O
5.47	O
)	O
:	O
input	O
and	O
outputs	O
a	O
learned	O
function	O
require	O
:	O
l	O
,	O
the	O
loss	O
function	O
,	O
seen	O
as	O
a	O
function	O
from	O
a	O
learned	O
function	O
f	O
and	O
an	O
example	O
z	O
(	O
)	O
i	O
d	O
to	O
a	O
scalar	O
r	O
require	O
:	O
k	O
,	O
the	O
number	O
of	O
folds	O
∈	O
∈	O
into	O
mutually	O
exclusive	O
subsets	O
di	O
,	O
whose	O
union	O
is	O
.d	O
do	O
split	O
i	O
for	O
k	O
d	O
\	O
from	O
to1	O
k	O
fi	O
=	O
(	O
a	O
d	O
d	O
i	O
)	O
for	O
z	O
(	O
)	O
j	O
in	O
d	O
i	O
do	O
ej	O
=	O
(	O
l	O
fi	O
,	O
z	O
(	O
)	O
j	O
)	O
end	O
for	O
end	O
for	O
return	O
e	O
123	O
chapter	O
5.	O
machine	O
learning	O
basics	O
(	O
i.i.d	O
.	O
)	O
data	O
points	O
.	O
a	O
point	O
estimator	O
or	O
statistic	O
is	O
any	O
function	O
of	O
the	O
data	O
:	O
ˆθm	O
=	O
(	O
g	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
m	O
)	O
.	O
(	O
5.19	O
)	O
the	O
deﬁnition	O
does	O
not	O
require	O
that	O
g	O
return	O
a	O
value	O
that	O
is	O
close	O
to	O
the	O
true	O
θ	O
or	O
even	O
that	O
the	O
range	O
of	O
g	O
is	O
the	O
same	O
as	O
the	O
set	O
of	O
allowable	O
values	O
of	O
θ.	O
this	O
deﬁnition	O
of	O
a	O
point	O
estimator	O
is	O
very	O
general	O
and	O
allows	O
the	O
designer	O
of	O
an	O
estimator	O
great	O
ﬂexibility	O
.	O
while	O
almost	O
any	O
function	O
thus	O
qualiﬁes	O
as	O
an	O
estimator	O
,	O
a	O
good	O
estimator	O
is	O
a	O
function	O
whose	O
output	O
is	O
close	O
to	O
the	O
true	O
underlying	O
θ	O
that	O
generated	O
the	O
training	O
data	O
.	O
for	O
now	O
,	O
we	O
take	O
the	O
frequentist	O
perspective	O
on	O
statistics	O
.	O
that	O
is	O
,	O
we	O
assume	O
that	O
the	O
true	O
parameter	O
value	O
θ	O
is	O
ﬁxed	O
but	O
unknown	O
,	O
while	O
the	O
point	O
estimate	O
ˆθ	O
is	O
a	O
function	O
of	O
the	O
data	O
.	O
since	O
the	O
data	O
is	O
drawn	O
from	O
a	O
random	O
process	O
,	O
any	O
function	O
of	O
the	O
data	O
is	O
random	O
.	O
therefore	O
ˆθ	O
is	O
a	O
random	O
variable	O
.	O
point	O
estimation	O
can	O
also	O
refer	O
to	O
the	O
estimation	O
of	O
the	O
relationship	O
between	O
input	O
and	O
target	O
variables	O
.	O
we	O
refer	O
to	O
these	O
types	O
of	O
point	O
estimates	O
as	O
function	O
estimators	O
.	O
function	O
estimation	O
as	O
we	O
mentioned	O
above	O
,	O
sometimes	O
we	O
are	O
interested	O
in	O
performing	O
function	O
estimation	O
(	O
or	O
function	O
approximation	O
)	O
.	O
here	O
we	O
are	O
trying	O
to	O
predict	O
a	O
variable	O
y	O
given	O
an	O
input	O
vector	O
x.	O
we	O
assume	O
that	O
there	O
is	O
a	O
function	O
f	O
(	O
x	O
)	O
that	O
describes	O
the	O
approximate	O
relationship	O
between	O
y	O
and	O
x.	O
for	O
example	O
,	O
we	O
may	O
assume	O
that	O
y	O
=	O
f	O
(	O
x	O
)	O
+	O
	O
,	O
where	O
	O
stands	O
for	O
the	O
part	O
of	O
y	O
that	O
is	O
not	O
predictable	O
from	O
x.	O
in	O
function	O
estimation	O
,	O
we	O
are	O
interested	O
in	O
approximating	O
f	O
with	O
a	O
model	B
or	O
estimate	O
ˆf	O
.	O
function	O
estimation	O
is	O
really	O
just	O
the	O
same	O
as	O
estimating	O
a	O
parameter	O
θ	O
;	O
the	O
function	O
estimator	O
ˆf	O
is	O
simply	O
a	O
point	O
estimator	O
in	O
function	O
space	O
.	O
the	O
linear	O
regression	O
example	O
(	O
discussed	O
above	O
in	O
section	O
)	O
and	O
the	O
polynomial	O
regression	O
example	O
(	O
discussed	O
in	O
section	O
)	O
are	O
both	O
examples	O
of	O
scenarios	O
that	O
may	O
be	O
interpreted	O
either	O
as	O
estimating	O
a	O
parameter	O
w	O
or	O
estimating	O
a	O
function	O
ˆf	O
mapping	O
from	O
tox	O
5.1.4	O
5.2	O
y	O
.	O
we	O
now	O
review	O
the	O
most	O
commonly	O
studied	O
properties	O
of	O
point	O
estimators	O
and	O
discuss	O
what	O
they	O
tell	O
us	O
about	O
these	O
estimators	O
.	O
5.4.2	O
bias	O
the	O
bias	O
of	O
an	O
estimator	O
is	O
deﬁned	O
as	O
:	O
bias	O
(	O
ˆθm	O
)	O
=	O
(	O
e	O
ˆθm	O
)	O
124	O
−	O
θ	O
(	O
5.20	O
)	O
chapter	O
5.	O
machine	O
learning	O
basics	O
where	O
the	O
expectation	O
is	O
over	O
the	O
data	O
(	O
seen	O
as	O
samples	O
from	O
a	O
random	O
variable	O
)	O
and	O
θ	O
is	O
the	O
true	O
underlying	O
value	O
of	O
θ	O
used	O
to	O
deﬁne	O
the	O
data	O
generating	O
distri-	O
bution	O
.	O
an	O
estimator	O
ˆθm	O
is	O
said	O
to	O
be	O
unbiased	O
if	O
bias	O
(	O
ˆθm	O
)	O
=	O
0	O
,	O
which	O
implies	O
that	O
e	O
(	O
ˆθm	O
)	O
=	O
θ.	O
an	O
estimator	O
ˆθm	O
is	O
said	O
to	O
be	O
asymptotically	O
unbiased	O
if	O
limm	O
→∞	O
bias	O
(	O
ˆθm	O
)	O
=	O
0	O
,	O
which	O
implies	O
that	O
limm	O
e	O
(	O
ˆθ	O
m	O
)	O
=	O
θ	O
.	O
→∞	O
example	O
:	O
bernoulli	O
distribution	O
consider	O
a	O
set	O
of	O
samples	O
that	O
are	O
independently	O
and	O
identically	O
distributed	O
according	O
to	O
a	O
bernoulli	O
distri-	O
bution	O
with	O
mean	O
:	O
θ	O
)	O
m	O
p	O
x	O
(	O
(	O
)	O
i	O
;	O
)	O
=	O
θ	O
θx	O
(	O
)	O
i	O
(	O
1	O
(	O
5.21	O
)	O
−	O
x	O
(	O
)	O
i	O
)	O
.	O
θ	O
(	O
1	O
)	O
−	O
	O
a	O
common	O
estimator	O
for	O
the	O
θ	O
parameter	O
of	O
this	O
distribution	O
is	O
the	O
mean	O
of	O
the	O
training	O
samples	O
:	O
{	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
}	O
m	O
i=1	O
x	O
(	O
)	O
i	O
.	O
(	O
5.22	O
)	O
to	O
determine	O
whether	O
this	O
estimator	O
is	O
biased	O
,	O
we	O
can	O
substitute	O
equation	O
5.22	O
into	O
equation	O
5.20	O
:	O
(	O
5.23	O
)	O
(	O
5.24	O
)	O
(	O
5.25	O
)	O
(	O
5.26	O
)	O
(	O
5.27	O
)	O
(	O
5.28	O
)	O
bias	O
(	O
ˆθm	O
)	O
=	O
[	O
e	O
1	O
m	O
	O
	O
	O
ˆθm	O
=	O
−	O
θ	O
	O
	O
	O
i=1	O
m	O
e	O
x	O
(	O
)	O
i	O
x	O
(	O
)	O
i	O
−	O
−	O
ˆθm	O
]	O
1	O
m	O
	O
	O
	O
	O
i=1	O
m	O
m	O
i=1	O
m	O
1	O
m	O
1	O
m	O
1	O
m	O
−	O
=	O
e	O
=	O
=	O
=	O
i=1	O
θ	O
=	O
0	O
=	O
θ	O
θ	O
θ	O
	O
1	O
x	O
(	O
)	O
i	O
θx	O
(	O
)	O
i	O
(	O
1	O
−	O
−	O
x	O
(	O
)	O
i	O
)	O
θ	O
(	O
1	O
)	O
−	O
θ	O
x	O
(	O
)	O
i	O
=0	O
−	O
(	O
)	O
θ	O
θ	O
since	O
bias	O
(	O
ˆθ	O
)	O
=	O
0	O
,	O
we	O
say	O
that	O
our	O
estimator	O
ˆθ	O
is	O
unbiased	O
.	O
example	O
:	O
gaussian	O
distribution	O
estimator	O
of	O
the	O
mean	O
now	O
,	O
consider	O
}	O
that	O
are	O
independently	O
and	O
identically	O
distributed	O
a	O
set	O
of	O
samples	O
according	O
to	O
a	O
gaussian	O
distribution	O
p	O
(	O
x	O
(	O
)	O
i	O
)	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
.	O
(	O
x	O
(	O
)	O
i	O
;	O
µ	O
,	O
σ2	O
)	O
,	O
where	O
i	O
{	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
∈	O
{	O
n	O
}	O
)	O
m	O
125	O
chapter	O
5.	O
machine	O
learning	O
basics	O
	O
	O
recall	O
that	O
the	O
gaussian	O
probability	O
density	O
function	O
is	O
given	O
by	O
p	O
x	O
(	O
(	O
)	O
i	O
;	O
µ	O
,	O
σ	O
2	O
)	O
=	O
1√	O
2πσ2	O
−	O
σ2	O
−1	O
2	O
exp	O
	O
(	O
x	O
(	O
)	O
i	O
µ	O
)	O
2	O
.	O
(	O
5.29	O
)	O
a	O
common	O
estimator	O
of	O
the	O
gaussian	O
mean	O
parameter	O
is	O
known	O
as	O
the	O
sample	O
mean	O
:	O
ˆµm	O
=	O
to	O
determine	O
the	O
bias	O
of	O
the	O
sample	O
mean	O
,	O
we	O
are	O
again	O
interested	O
in	O
calculating	O
its	O
expectation	O
:	O
bias	O
(	O
ˆµm	O
)	O
=	O
[	O
ˆe	O
µm	O
]	O
	O
	O
1	O
m	O
m	O
i=1	O
x	O
(	O
)	O
i	O
µ	O
−	O
	O
	O
	O
i=1	O
m	O
m	O
	O
	O
x	O
(	O
)	O
i	O
	O
	O
=	O
e	O
1	O
m	O
1	O
m	O
1	O
m	O
−	O
x	O
(	O
)	O
i	O
e	O
−	O
µ	O
i=1	O
m	O
µ	O
i=1	O
µ	O
=	O
0	O
=	O
=	O
=	O
µ	O
	O
−	O
µ	O
−	O
µ	O
(	O
5.30	O
)	O
(	O
5.31	O
)	O
(	O
5.32	O
)	O
(	O
5.33	O
)	O
(	O
5.34	O
)	O
(	O
5.35	O
)	O
thus	O
we	O
ﬁnd	O
that	O
the	O
sample	O
mean	O
is	O
an	O
unbiased	O
estimator	O
of	O
gaussian	O
mean	O
parameter	O
.	O
example	O
:	O
estimators	O
of	O
the	O
variance	O
of	O
a	O
gaussian	O
distribution	O
as	O
an	O
example	O
,	O
we	O
compare	O
two	O
diﬀerent	O
estimators	O
of	O
the	O
variance	O
parameter	O
σ2	O
of	O
a	O
gaussian	O
distribution	O
.	O
we	O
are	O
interested	O
in	O
knowing	O
if	O
either	O
estimator	O
is	O
biased	O
.	O
	O
	O
	O
the	O
ﬁrst	O
estimator	O
of	O
σ2	O
we	O
consider	O
is	O
known	O
as	O
the	O
sample	O
variance	O
:	O
ˆσ2	O
m	O
=	O
1	O
m	O
m	O
i=1	O
−	O
x	O
(	O
)	O
i	O
2	O
,	O
ˆµ	O
m	O
(	O
5.36	O
)	O
where	O
ˆµm	O
is	O
the	O
sample	O
mean	O
,	O
deﬁned	O
above	O
.	O
more	O
formally	O
,	O
we	O
are	O
interested	O
in	O
computing	O
σ2	O
(	O
5.37	O
)	O
−	O
bias	O
(	O
ˆσ	O
2	O
m	O
)	O
=	O
[	O
ˆe	O
σ	O
2	O
m	O
]	O
126	O
chapter	O
5.	O
machine	O
learning	O
basics	O
we	O
begin	O
by	O
evaluating	O
the	O
term	O
e	O
[	O
ˆσ	O
2	O
m	O
]	O
:	O
	O
	O
e	O
[	O
ˆσ2	O
m	O
]	O
=e	O
m	O
=	O
m	O
i=1	O
σ2	O
1	O
m	O
−	O
1	O
m	O
	O
x	O
(	O
)	O
i	O
−	O
	O
	O
2	O
ˆµm	O
(	O
5.38	O
)	O
(	O
5.39	O
)	O
−	O
σ2/m	O
.	O
therefore	O
,	O
ˆσ	O
2	O
m	O
is	O
returning	O
to	O
equation	O
the	O
sample	O
variance	O
is	O
a	O
biased	O
estimator	O
.	O
5.37	O
,	O
we	O
conclude	O
that	O
the	O
bias	O
of	O
the	O
unbiased	O
sample	O
variance	O
estimator	O
	O
	O
	O
˜σ2	O
m	O
=	O
−	O
1	O
1	O
m	O
i=1	O
m	O
	O
x	O
(	O
)	O
i	O
	O
	O
−	O
2	O
ˆµm	O
	O
	O
(	O
5.40	O
)	O
provides	O
an	O
alternative	O
approach	O
.	O
as	O
the	O
name	O
suggests	O
this	O
estimator	O
is	O
unbiased	O
.	O
that	O
is	O
,	O
we	O
ﬁnd	O
that	O
e	O
[	O
˜σ2	O
m	O
]	O
=	O
σ2	O
:	O
e	O
[	O
˜σ2	O
m	O
]	O
=	O
e	O
m	O
−	O
1	O
	O
1	O
m	O
−	O
m	O
−	O
m	O
i=1	O
1e	O
[	O
ˆσ	O
2	O
m	O
]	O
−	O
m	O
1	O
m	O
−	O
x	O
(	O
)	O
i	O
	O
2	O
ˆµm	O
1	O
σ	O
2	O
(	O
5.41	O
)	O
(	O
5.42	O
)	O
(	O
5.43	O
)	O
(	O
5.44	O
)	O
=	O
m	O
=	O
m	O
=	O
σ2	O
.	O
we	O
have	O
two	O
estimators	O
:	O
one	O
is	O
biased	O
and	O
the	O
other	O
is	O
not	O
.	O
while	O
unbiased	O
estimators	O
are	O
clearly	O
desirable	O
,	O
they	O
are	O
not	O
always	O
the	O
“	O
best	O
”	O
estimators	O
.	O
as	O
we	O
will	O
see	O
we	O
often	O
use	O
biased	O
estimators	O
that	O
possess	O
other	O
important	O
properties	O
.	O
5.4.3	O
variance	O
and	O
standard	O
error	O
another	O
property	O
of	O
the	O
estimator	O
that	O
we	O
might	O
want	O
to	O
consider	O
is	O
how	O
much	O
we	O
expect	O
it	O
to	O
vary	O
as	O
a	O
function	O
of	O
the	O
data	O
sample	O
.	O
just	O
as	O
we	O
computed	O
the	O
expectation	O
of	O
the	O
estimator	O
to	O
determine	O
its	O
bias	O
,	O
we	O
can	O
compute	O
its	O
variance	O
.	O
the	O
variance	O
of	O
an	O
estimator	O
is	O
simply	O
the	O
variance	O
var	O
(	O
ˆθ	O
)	O
(	O
5.45	O
)	O
where	O
the	O
random	O
variable	O
is	O
the	O
training	O
set	O
.	O
alternately	O
,	O
the	O
square	O
root	O
of	O
the	O
variance	O
is	O
called	O
the	O
standard	O
error	O
,	O
denoted	O
se	O
(	O
ˆθ	O
)	O
.	O
127	O
chapter	O
5.	O
machine	O
learning	O
basics	O
the	O
variance	O
or	O
the	O
standard	O
error	O
of	O
an	O
estimator	O
provides	O
a	O
measure	O
of	O
how	O
we	O
would	O
expect	O
the	O
estimate	O
we	O
compute	O
from	O
data	O
to	O
vary	O
as	O
we	O
independently	O
resample	O
the	O
dataset	O
from	O
the	O
underlying	O
data	O
generating	O
process	O
.	O
just	O
as	O
we	O
might	O
like	O
an	O
estimator	O
to	O
exhibit	O
low	O
bias	O
we	O
would	O
also	O
like	O
it	O
to	O
have	O
relatively	O
low	O
variance	O
.	O
when	O
we	O
compute	O
any	O
statistic	O
using	O
a	O
ﬁnite	O
number	O
of	O
samples	O
,	O
our	O
estimate	O
of	O
the	O
true	O
underlying	O
parameter	O
is	O
uncertain	O
,	O
in	O
the	O
sense	O
that	O
we	O
could	O
have	O
obtained	O
other	O
samples	O
from	O
the	O
same	O
distribution	O
and	O
their	O
statistics	O
would	O
have	O
been	O
diﬀerent	O
.	O
the	O
expected	O
degree	O
of	O
variation	O
in	O
any	O
estimator	O
is	O
a	O
source	O
of	O
error	O
that	O
we	O
want	O
to	O
quantify	O
.	O
	O
	O
	O
	O
the	O
standard	O
error	O
of	O
the	O
mean	O
is	O
given	O
by	O
se	O
(	O
ˆµm	O
)	O
=	O
var	O
1	O
m	O
m	O
i=1	O
x	O
(	O
)	O
i	O
=	O
σ√	O
m	O
,	O
(	O
5.46	O
)	O
where	O
σ2	O
is	O
the	O
true	O
variance	O
of	O
the	O
samples	O
xi	O
.	O
the	O
standard	O
error	O
is	O
often	O
estimated	O
by	O
using	O
an	O
estimate	O
of	O
σ.	O
unfortunately	O
,	O
neither	O
the	O
square	O
root	O
of	O
the	O
sample	O
variance	O
nor	O
the	O
square	O
root	O
of	O
the	O
unbiased	O
estimator	O
of	O
the	O
variance	O
provide	O
an	O
unbiased	O
estimate	O
of	O
the	O
standard	O
deviation	O
.	O
both	O
approaches	O
tend	O
to	O
underestimate	O
the	O
true	O
standard	O
deviation	O
,	O
but	O
are	O
still	O
used	O
in	O
practice	O
.	O
the	O
square	O
root	O
of	O
the	O
unbiased	O
estimator	O
of	O
the	O
variance	O
is	O
less	O
of	O
an	O
underestimate	O
.	O
for	O
large	O
,	O
the	O
approximation	O
is	O
quite	O
reasonable	O
.	O
m	O
the	O
standard	O
error	O
of	O
the	O
mean	O
is	O
very	O
useful	O
in	O
machine	O
learning	O
experiments	O
.	O
we	O
often	O
estimate	O
the	O
generalization	O
error	O
by	O
computing	O
the	O
sample	O
mean	O
of	O
the	O
error	O
on	O
the	O
test	O
set	O
.	O
the	O
number	O
of	O
examples	O
in	O
the	O
test	O
set	O
determines	O
the	O
accuracy	O
of	O
this	O
estimate	O
.	O
taking	O
advantage	O
of	O
the	O
central	O
limit	O
theorem	O
,	O
which	O
tells	O
us	O
that	O
the	O
mean	O
will	O
be	O
approximately	O
distributed	O
with	O
a	O
normal	O
distribution	O
,	O
we	O
can	O
use	O
the	O
standard	O
error	O
to	O
compute	O
the	O
probability	O
that	O
the	O
true	O
expectation	O
falls	O
in	O
any	O
chosen	O
interval	O
.	O
for	O
example	O
,	O
the	O
95	O
%	O
conﬁdence	O
interval	O
centered	O
on	O
the	O
mean	O
ˆµm	O
is	O
−	O
(	O
ˆµm	O
1	O
96se	O
(	O
ˆ	O
.	O
µ	O
m	O
)	O
ˆ	O
,	O
µm	O
+	O
1	O
96se	O
(	O
ˆ	O
µm	O
)	O
)	O
,	O
.	O
(	O
5.47	O
)	O
under	O
the	O
normal	O
distribution	O
with	O
mean	O
ˆµm	O
and	O
variance	O
se	O
(	O
ˆµm	O
)	O
2	O
.	O
in	O
machine	O
learning	O
experiments	O
,	O
it	O
is	O
common	O
to	O
say	O
that	O
algorithm	O
a	O
is	O
better	O
than	O
algorithm	O
b	O
if	O
the	O
upper	O
bound	B
of	O
the	O
95	O
%	O
conﬁdence	O
interval	O
for	O
the	O
error	O
of	O
algorithm	O
a	O
is	O
less	O
than	O
the	O
lower	O
bound	B
of	O
the	O
95	O
%	O
conﬁdence	O
interval	O
for	O
the	O
error	O
of	O
algorithm	O
b	O
.	O
128	O
chapter	O
5.	O
machine	O
learning	O
basics	O
}	O
{	O
example	O
:	O
bernoulli	O
distribution	O
we	O
once	O
again	O
consider	O
a	O
set	O
of	O
samples	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
drawn	O
independently	O
and	O
identically	O
from	O
a	O
bernoulli	O
distribution	O
−	O
x	O
(	O
)	O
i	O
)	O
)	O
.	O
this	O
time	O
we	O
are	O
interested	O
in	O
computing	O
θ	O
)	O
(	O
1	O
−	O
)	O
m	O
	O
	O
(	O
recall	O
p	O
(	O
x	O
(	O
)	O
i	O
;	O
θ	O
)	O
=	O
θ	O
x	O
(	O
)	O
i	O
(	O
1	O
the	O
variance	O
of	O
the	O
estimator	O
ˆθm	O
=	O
1	O
m	O
var	O
ˆθm	O
=	O
var	O
x	O
(	O
)	O
i	O
	O
	O
	O
m	O
i=1	O
x	O
(	O
)	O
i	O
.	O
m	O
	O
	O
i=1	O
	O
	O
	O
m	O
1	O
m	O
=	O
=	O
=	O
=	O
1	O
m2	O
1	O
m2	O
1	O
m2	O
1	O
m	O
var	O
x	O
(	O
)	O
i	O
i=1	O
m	O
i=1	O
θ	O
−	O
θ	O
)	O
θ	O
)	O
(	O
1	O
−	O
mθ	O
(	O
1	O
−	O
θ	O
(	O
1	O
θ	O
)	O
(	O
5.48	O
)	O
(	O
5.49	O
)	O
(	O
5.50	O
)	O
(	O
5.51	O
)	O
(	O
5.52	O
)	O
the	O
variance	O
of	O
the	O
estimator	O
decreases	O
as	O
a	O
function	O
of	O
m	O
,	O
the	O
number	O
of	O
examples	O
in	O
the	O
dataset	O
.	O
this	O
is	O
a	O
common	O
property	O
of	O
popular	O
estimators	O
that	O
we	O
will	O
return	O
to	O
when	O
we	O
discuss	O
consistency	O
(	O
see	O
section	O
5.4.5	O
)	O
.	O
5.4.4	O
trading	O
oﬀ	O
bias	O
and	O
variance	O
to	O
minimize	O
mean	O
squared	O
error	O
bias	O
and	O
variance	O
measure	O
two	O
diﬀerent	O
sources	O
of	O
error	O
in	O
an	O
estimator	O
.	O
bias	O
measures	O
the	O
expected	O
deviation	O
from	O
the	O
true	O
value	O
of	O
the	O
function	O
or	O
parameter	O
.	O
variance	O
on	O
the	O
other	O
hand	O
,	O
provides	O
a	O
measure	O
of	O
the	O
deviation	O
from	O
the	O
expected	O
estimator	O
value	O
that	O
any	O
particular	O
sampling	O
of	O
the	O
data	O
is	O
likely	O
to	O
cause	O
.	O
what	O
happens	O
when	O
we	O
are	O
given	O
a	O
choice	O
between	O
two	O
estimators	O
,	O
one	O
with	O
more	O
bias	O
and	O
one	O
with	O
more	O
variance	O
?	O
how	O
do	O
we	O
choose	O
between	O
them	O
?	O
for	O
example	O
,	O
imagine	O
that	O
we	O
are	O
interested	O
in	O
approximating	O
the	O
function	O
shown	O
in	O
ﬁgure	O
and	O
we	O
are	O
only	O
oﬀered	O
the	O
choice	O
between	O
a	O
model	B
with	O
large	O
bias	O
and	O
one	O
that	O
suﬀers	O
from	O
large	O
variance	O
.	O
how	O
do	O
we	O
choose	O
between	O
them	O
?	O
5.2	O
the	O
most	O
common	O
way	O
to	O
negotiate	O
this	O
trade-oﬀ	O
is	O
to	O
use	O
cross-validation	O
.	O
empirically	O
,	O
cross-validation	O
is	O
highly	O
successful	O
on	O
many	O
real-world	O
tasks	O
.	O
alter-	O
natively	O
,	O
we	O
can	O
also	O
compare	O
the	O
mean	O
squared	O
error	O
(	O
mse	O
)	O
of	O
the	O
estimates	O
:	O
−	O
mse	O
=	O
[	O
(	O
e	O
ˆθm	O
θ	O
)	O
2	O
]	O
=	O
bias	O
(	O
ˆθm	O
)	O
2	O
+	O
var	O
(	O
ˆθm	O
)	O
129	O
(	O
5.53	O
)	O
(	O
5.54	O
)	O
chapter	O
5.	O
machine	O
learning	O
basics	O
the	O
mse	O
measures	O
the	O
overall	O
expected	O
deviation—in	O
a	O
squared	O
error	O
sense—	O
between	O
the	O
estimator	O
and	O
the	O
true	O
value	O
of	O
the	O
parameter	O
θ.	O
as	O
is	O
clear	O
from	O
equation	O
,	O
evaluating	O
the	O
mse	O
incorporates	O
both	O
the	O
bias	O
and	O
the	O
variance	O
.	O
desirable	O
estimators	O
are	O
those	O
with	O
small	O
mse	O
and	O
these	O
are	O
estimators	O
that	O
manage	O
to	O
keep	O
both	O
their	O
bias	O
and	O
variance	O
somewhat	O
in	O
check	O
.	O
5.54	O
underﬁtting	O
zone	O
overﬁtting	O
zone	O
bias	O
generalization	O
error	O
optimal	O
capacity	O
variance	O
capacity	O
figure	O
5.6	O
:	O
as	O
capacity	O
increases	O
(	O
x-axis	O
)	O
,	O
bias	O
(	O
dotted	O
)	O
tends	O
to	O
decrease	O
and	O
variance	O
(	O
dashed	O
)	O
tends	O
to	O
increase	O
,	O
yielding	O
another	O
u-shaped	O
curve	O
for	O
generalization	O
error	O
(	O
bold	O
curve	O
)	O
.	O
if	O
we	O
vary	O
capacity	O
along	O
one	O
axis	O
,	O
there	O
is	O
an	O
optimal	O
capacity	O
,	O
with	O
underﬁtting	O
when	O
the	O
capacity	O
is	O
below	O
this	O
optimum	O
and	O
overﬁtting	O
when	O
it	O
is	O
above	O
.	O
this	O
relationship	O
is	O
similar	O
to	O
the	O
relationship	O
between	O
capacity	O
,	O
underﬁtting	O
,	O
and	O
overﬁtting	O
,	O
discussed	O
in	O
section	O
and	O
ﬁgure	O
5.2	O
.	O
5.3	O
the	O
relationship	O
between	O
bias	O
and	O
variance	O
is	O
tightly	O
linked	O
to	O
the	O
machine	O
learning	O
concepts	O
of	O
capacity	O
,	O
underﬁtting	O
and	O
overﬁtting	O
.	O
in	O
the	O
case	O
where	O
gen-	O
eralization	O
error	O
is	O
measured	O
by	O
the	O
mse	O
(	O
where	O
bias	O
and	O
variance	O
are	O
meaningful	O
components	O
of	O
generalization	O
error	O
)	O
,	O
increasing	O
capacity	O
tends	O
to	O
increase	O
variance	O
and	O
decrease	O
bias	O
.	O
this	O
is	O
illustrated	O
in	O
ﬁgure	O
,	O
where	O
we	O
see	O
again	O
the	O
u-shaped	O
curve	O
of	O
generalization	O
error	O
as	O
a	O
function	O
of	O
capacity	O
.	O
5.6	O
5.4.5	O
consistency	O
so	O
far	O
we	O
have	O
discussed	O
the	O
properties	O
of	O
various	O
estimators	O
for	O
a	O
training	O
set	O
of	O
ﬁxed	O
size	O
.	O
usually	O
,	O
we	O
are	O
also	O
concerned	O
with	O
the	O
behavior	O
of	O
an	O
estimator	O
as	O
the	O
amount	O
of	O
training	O
data	O
grows	O
.	O
in	O
particular	O
,	O
we	O
usually	O
wish	O
that	O
,	O
as	O
the	O
number	O
of	O
data	O
points	O
m	O
in	O
our	O
dataset	O
increases	O
,	O
our	O
point	O
estimates	O
converge	O
to	O
the	O
true	O
130	O
chapter	O
5.	O
machine	O
learning	O
basics	O
value	O
of	O
the	O
corresponding	O
parameters	O
.	O
more	O
formally	O
,	O
we	O
would	O
like	O
that	O
plimm	O
→∞	O
ˆθm	O
=	O
θ	O
.	O
(	O
5.55	O
)	O
−	O
|	O
|ˆθm	O
→	O
→	O
∞	O
0	O
as	O
m	O
θ	O
>	O
	O
)	O
the	O
symbol	O
plim	O
indicates	O
convergence	O
in	O
probability	O
,	O
meaning	O
that	O
for	O
any	O
	O
>	O
0	O
,	O
p	O
(	O
is	O
known	O
as	O
consistency	O
.	O
it	O
is	O
sometimes	O
referred	O
to	O
as	O
weak	O
consistency	O
,	O
with	O
strong	O
consistency	O
referring	O
to	O
the	O
almost	O
sure	O
convergence	O
of	O
ˆθ	O
to	O
θ.	O
almost	O
sure	O
convergence	O
of	O
a	O
sequence	O
of	O
random	O
variables	O
x	O
(	O
1	O
)	O
,	O
x	O
(	O
2	O
)	O
,	O
.	O
.	O
.	O
to	O
a	O
value	O
x	O
occurs	O
when	O
p	O
(	O
limm	O
.	O
the	O
condition	O
described	O
by	O
equation	O
)	O
m	O
=	O
)	O
=	O
1	O
.	O
→∞	O
x	O
(	O
5.55	O
x	O
consistency	O
ensures	O
that	O
the	O
bias	O
induced	O
by	O
the	O
estimator	O
diminishes	O
as	O
the	O
number	O
of	O
data	O
examples	O
grows	O
.	O
however	O
,	O
the	O
reverse	O
is	O
not	O
true—asymptotic	O
unbiasedness	O
does	O
not	O
imply	O
consistency	O
.	O
for	O
example	O
,	O
consider	O
estimating	O
the	O
(	O
x	O
;	O
µ	O
,	O
σ2	O
)	O
,	O
with	O
a	O
dataset	O
consisting	O
mean	O
parameter	O
µ	O
of	O
a	O
normal	O
distribution	O
.	O
we	O
could	O
use	O
the	O
ﬁrst	O
sample	O
x	O
(	O
1	O
)	O
of	O
the	O
dataset	O
of	O
m	O
samples	O
:	O
as	O
an	O
unbiased	O
estimator	O
:	O
ˆθ	O
=	O
x	O
(	O
1	O
)	O
.	O
in	O
that	O
case	O
,	O
e	O
(	O
ˆθm	O
)	O
=	O
θ	O
so	O
the	O
estimator	O
is	O
unbiased	O
no	O
matter	O
how	O
many	O
data	O
points	O
are	O
seen	O
.	O
this	O
,	O
of	O
course	O
,	O
implies	O
that	O
the	O
estimate	O
is	O
asymptotically	O
unbiased	O
.	O
however	O
,	O
this	O
is	O
not	O
a	O
consistent	O
estimator	O
as	O
it	O
is	O
{	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
→	O
∞	O
.	O
the	O
case	O
that	O
θ	O
mas	O
→	O
n	O
not	O
}	O
)	O
m	O
ˆθm	O
5.5	O
maximum	O
likelihood	O
estimation	O
previously	O
,	O
we	O
have	O
seen	O
some	O
deﬁnitions	O
of	O
common	O
estimators	O
and	O
analyzed	O
their	O
properties	O
.	O
but	O
where	O
did	O
these	O
estimators	O
come	O
from	O
?	O
rather	O
than	O
guessing	O
that	O
some	O
function	O
might	O
make	O
a	O
good	O
estimator	O
and	O
then	O
analyzing	O
its	O
bias	O
and	O
variance	O
,	O
we	O
would	O
like	O
to	O
have	O
some	O
principle	O
from	O
which	O
we	O
can	O
derive	O
speciﬁc	O
functions	O
that	O
are	O
good	O
estimators	O
for	O
diﬀerent	O
models.	O
}	O
the	O
most	O
common	O
such	O
principle	O
is	O
the	O
maximum	O
likelihood	O
principle	O
.	O
consider	O
a	O
set	O
of	O
m	O
examples	O
x	O
=	O
{	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
drawn	O
independently	O
from	O
)	O
m	O
the	O
true	O
but	O
unknown	O
data	O
generating	O
distribution	O
pdata	O
(	O
)	O
x	O
.	O
let	O
pmodel	O
(	O
x	O
;	O
θ	O
)	O
be	O
a	O
parametric	O
family	O
of	O
probability	O
distributions	O
over	O
the	O
same	O
space	O
indexed	O
by	O
θ.	O
in	O
other	O
words	O
,	O
pmodel	O
(	O
x	O
;	O
θ	O
)	O
maps	O
any	O
conﬁguration	O
x	O
to	O
a	O
real	O
number	O
estimating	O
the	O
true	O
probability	O
pdata	O
(	O
)	O
x	O
.	O
the	O
maximum	O
likelihood	O
estimator	O
for	O
θ	O
is	O
then	O
deﬁned	O
as	O
	O
θml	O
=	O
arg	O
max	O
pmodel	O
(	O
;	O
)	O
x	O
θ	O
θ	O
m	O
=	O
arg	O
max	O
pmodel	O
(	O
x	O
(	O
)	O
i	O
;	O
)	O
θ	O
θ	O
i=1	O
131	O
(	O
5.56	O
)	O
(	O
5.57	O
)	O
chapter	O
5.	O
machine	O
learning	O
basics	O
this	O
product	O
over	O
many	O
probabilities	O
can	O
be	O
inconvenient	O
for	O
a	O
variety	O
of	O
reasons	O
.	O
for	O
example	O
,	O
it	O
is	O
prone	O
to	O
numerical	O
underﬂow	O
.	O
to	O
obtain	O
a	O
more	O
convenient	O
but	O
equivalent	O
optimization	O
problem	O
,	O
we	O
observe	O
that	O
taking	O
the	O
logarithm	O
of	O
the	O
likelihood	O
does	O
not	O
change	O
its	O
arg	O
max	O
but	O
does	O
conveniently	O
transform	O
a	O
product	O
into	O
a	O
sum	O
:	O
	O
m	O
θml	O
=	O
arg	O
max	O
log	O
pmodel	O
(	O
x	O
(	O
)	O
i	O
;	O
)	O
θ	O
.	O
(	O
5.58	O
)	O
θ	O
i=1	O
because	O
the	O
arg	O
max	O
does	O
not	O
change	O
when	O
we	O
rescale	O
the	O
cost	O
function	O
,	O
we	O
can	O
divide	O
by	O
m	O
to	O
obtain	O
a	O
version	O
of	O
the	O
criterion	O
that	O
is	O
expressed	O
as	O
an	O
expectation	O
with	O
respect	O
to	O
the	O
empirical	O
distribution	O
ˆpdata	O
deﬁned	O
by	O
the	O
training	O
data	O
:	O
θml	O
=	O
arg	O
max	O
θ	O
∼	O
ˆpdata	O
ex	O
log	O
pmodel	O
(	O
;	O
)	O
x	O
θ	O
.	O
(	O
5.59	O
)	O
one	O
way	O
to	O
interpret	O
maximum	O
likelihood	O
estimation	O
is	O
to	O
view	O
it	O
as	O
minimizing	O
the	O
dissimilarity	O
between	O
the	O
empirical	O
distribution	O
ˆpdata	O
deﬁned	O
by	O
the	O
training	O
set	O
and	O
the	O
model	B
distribution	O
,	O
with	O
the	O
degree	O
of	O
dissimilarity	O
between	O
the	O
two	O
measured	O
by	O
the	O
kl	O
divergence	O
.	O
the	O
kl	O
divergence	O
is	O
given	O
by	O
dkl	O
(	O
ˆpdata	O
	O
∼	O
pmodel	O
)	O
=	O
e	O
x	O
ˆpdata	O
−	O
[	O
log	O
ˆpdata	O
(	O
)	O
x	O
log	O
pmodel	O
(	O
)	O
]	O
x	O
.	O
(	O
5.60	O
)	O
the	O
term	O
on	O
the	O
left	O
is	O
a	O
function	O
only	O
of	O
the	O
data	O
generating	O
process	O
,	O
not	O
the	O
model	B
.	O
this	O
means	O
when	O
we	O
train	O
the	O
model	B
to	O
minimize	O
the	O
kl	O
divergence	O
,	O
we	O
need	O
only	O
minimize	O
[	O
log	O
pmodel	O
(	O
)	O
]	O
x	O
(	O
5.61	O
)	O
−	O
∼	O
ex	O
ˆpdata	O
which	O
is	O
of	O
course	O
the	O
same	O
as	O
the	O
maximization	O
in	O
equation	O
.	O
5.59	O
minimizing	O
this	O
kl	O
divergence	O
corresponds	O
exactly	O
to	O
minimizing	O
the	O
cross-	O
entropy	O
between	O
the	O
distributions	O
.	O
many	O
authors	O
use	O
the	O
term	O
“	O
cross-entropy	O
”	O
to	O
identify	O
speciﬁcally	O
the	O
negative	O
log-likelihood	O
of	O
a	O
bernoulli	O
or	O
softmax	O
distribution	O
,	O
but	O
that	O
is	O
a	O
misnomer	O
.	O
any	O
loss	O
consisting	O
of	O
a	O
negative	O
log-likelihood	O
is	O
a	O
cross-	O
entropy	O
between	O
the	O
empirical	O
distribution	O
deﬁned	O
by	O
the	O
training	O
set	O
and	O
the	O
probability	O
distribution	O
deﬁned	O
by	O
model	B
.	O
for	O
example	O
,	O
mean	O
squared	O
error	O
is	O
the	O
cross-entropy	O
between	O
the	O
empirical	O
distribution	O
and	O
a	O
gaussian	O
model	B
.	O
we	O
can	O
thus	O
see	O
maximum	O
likelihood	O
as	O
an	O
attempt	O
to	O
make	O
the	O
model	B
dis-	O
tribution	B
match	O
the	O
empirical	O
distribution	O
ˆpdata	O
.	O
ideally	O
,	O
we	O
would	O
like	O
to	O
match	O
the	O
true	O
data	O
generating	O
distribution	O
pdata	O
,	O
but	O
we	O
have	O
no	O
direct	O
access	O
to	O
this	O
distribution	O
.	O
while	O
the	O
optimal	O
θ	O
is	O
the	O
same	O
regardless	O
of	O
whether	O
we	O
are	O
maximizing	O
the	O
likelihood	O
or	O
minimizing	O
the	O
kl	O
divergence	O
,	O
the	O
values	O
of	O
the	O
objective	O
functions	O
132	O
θml	O
=	O
arg	O
max	O
p	O
(	O
y	O
x	O
;	O
)	O
.	O
θ	O
(	O
5.62	O
)	O
|	O
	O
θ	O
chapter	O
5.	O
machine	O
learning	O
basics	O
are	O
diﬀerent	O
.	O
in	O
software	O
,	O
we	O
often	O
phrase	O
both	O
as	O
minimizing	O
a	O
cost	O
function	O
.	O
maximum	O
likelihood	O
thus	O
becomes	O
minimization	O
of	O
the	O
negative	O
log-likelihood	O
(	O
nll	O
)	O
,	O
or	O
equivalently	O
,	O
minimization	O
of	O
the	O
cross	O
entropy	O
.	O
the	O
perspective	O
of	O
maximum	O
likelihood	O
as	O
minimum	O
kl	O
divergence	O
becomes	O
helpful	O
in	O
this	O
case	O
because	O
the	O
kl	O
divergence	O
has	O
a	O
known	O
minimum	O
value	O
of	O
zero	O
.	O
the	O
negative	O
log-likelihood	O
can	O
actually	O
become	O
negative	O
when	O
is	O
real-valued	O
.	O
x	O
5.5.1	O
conditional	O
log-likelihood	O
and	O
mean	O
squared	O
error	O
|	O
the	O
maximum	O
likelihood	O
estimator	O
can	O
readily	O
be	O
generalized	O
to	O
the	O
case	O
where	O
our	O
goal	O
is	O
to	O
estimate	O
a	O
conditional	O
probability	O
p	O
(	O
y	O
x	O
;	O
θ	O
)	O
in	O
order	O
to	O
predict	O
y	O
given	O
x	O
.	O
this	O
is	O
actually	O
the	O
most	O
common	O
situation	O
because	O
it	O
forms	O
the	O
basis	O
for	O
most	O
supervised	O
learning	O
.	O
if	O
x	O
represents	O
all	O
our	O
inputs	O
and	O
y	O
all	O
our	O
observed	O
targets	O
,	O
then	O
the	O
conditional	O
maximum	O
likelihood	O
estimator	O
is	O
if	O
the	O
examples	O
are	O
assumed	O
to	O
be	O
i.i.d.	O
,	O
then	O
this	O
can	O
be	O
decomposed	O
into	O
m	O
θml	O
=	O
arg	O
max	O
log	O
(	O
p	O
y	O
(	O
)	O
i	O
θ	O
i=1	O
|	O
x	O
(	O
)	O
i	O
;	O
)	O
θ	O
.	O
(	O
5.63	O
)	O
5.1.4	O
example	O
:	O
linear	O
regression	O
as	O
maximum	O
likelihood	O
linear	O
regression	O
,	O
introduced	O
earlier	O
in	O
section	O
,	O
may	O
be	O
justiﬁed	O
as	O
a	O
maximum	O
likelihood	O
procedure	O
.	O
previously	O
,	O
we	O
motivated	O
linear	O
regression	O
as	O
an	O
algorithm	O
that	O
learns	O
to	O
take	O
an	O
input	O
x	O
and	O
produce	O
an	O
output	O
value	O
ˆy	O
.	O
the	O
mapping	O
from	O
x	O
to	O
ˆy	O
is	O
chosen	O
to	O
minimize	O
mean	O
squared	O
error	O
,	O
a	O
criterion	O
that	O
we	O
introduced	O
more	O
or	O
less	O
arbitrarily	O
.	O
we	O
now	O
revisit	O
linear	O
regression	O
from	O
the	O
point	O
of	O
view	O
of	O
maximum	O
likelihood	O
estimation	O
.	O
instead	O
of	O
producing	O
a	O
single	O
prediction	O
ˆy	O
,	O
we	O
now	O
think	O
of	O
the	O
model	B
as	O
producing	O
a	O
conditional	O
distribution	O
p	O
(	O
y	O
x	O
)	O
.	O
we	O
can	O
imagine	O
that	O
with	O
an	O
inﬁnitely	O
large	O
training	O
set	O
,	O
we	O
might	O
see	O
several	O
training	O
examples	O
with	O
the	O
same	O
input	O
value	O
x	O
but	O
diﬀerent	O
values	O
of	O
y.	O
the	O
goal	O
of	O
the	O
learning	O
algorithm	O
is	O
now	O
to	O
ﬁt	O
the	O
distribution	O
p	O
(	O
y	O
x	O
)	O
to	O
all	O
of	O
those	O
diﬀerent	O
y	O
values	O
n	O
that	O
are	O
all	O
compatible	O
with	O
x.	O
to	O
derive	O
the	O
same	O
linear	O
regression	O
algorithm	O
(	O
y	O
;	O
ˆy	O
(	O
x	O
;	O
w	O
)	O
,	O
σ2	O
)	O
.	O
the	O
function	O
ˆy	O
(	O
x	O
;	O
w	O
)	O
we	O
obtained	O
before	O
,	O
we	O
deﬁne	O
p	O
(	O
y	O
gives	O
the	O
prediction	O
of	O
the	O
mean	O
of	O
the	O
gaussian	O
.	O
in	O
this	O
example	O
,	O
we	O
assume	O
that	O
the	O
variance	O
is	O
ﬁxed	O
to	O
some	O
constant	O
σ	O
2	O
chosen	O
by	O
the	O
user	O
.	O
we	O
will	O
see	O
that	O
this	O
choice	O
of	O
the	O
functional	O
form	O
of	O
p	O
(	O
y	O
x	O
)	O
causes	O
the	O
maximum	O
likelihood	O
estimation	O
procedure	O
to	O
yield	O
the	O
same	O
learning	O
algorithm	O
as	O
we	O
developed	O
before	O
.	O
since	O
the	O
x	O
)	O
=	O
|	O
|	O
|	O
|	O
133	O
	O
m	O
log	O
(	O
p	O
y	O
(	O
)	O
i	O
|	O
x	O
(	O
)	O
i	O
;	O
)	O
θ	O
i=1	O
−	O
m	O
σ	O
log	O
=	O
−	O
m	O
2	O
log	O
(	O
2	O
)	O
π	O
	O
	O
	O
	O
−	O
m	O
ˆy	O
(	O
)	O
i	O
i=1	O
−	O
y	O
(	O
)	O
i	O
2σ	O
2	O
2	O
,	O
5.63	O
)	O
is	O
(	O
5.64	O
)	O
(	O
5.65	O
)	O
(	O
5.66	O
)	O
chapter	O
5.	O
machine	O
learning	O
basics	O
examples	O
are	O
assumed	O
to	O
be	O
i.i.d.	O
,	O
the	O
conditional	O
log-likelihood	O
(	O
equation	O
given	O
by	O
where	O
ˆy	O
(	O
)	O
i	O
is	O
the	O
output	O
of	O
the	O
linear	O
regression	O
on	O
the	O
i-th	O
input	O
x	O
(	O
)	O
i	O
and	O
m	O
is	O
the	O
number	O
of	O
the	O
training	O
examples	O
.	O
comparing	O
the	O
log-likelihood	O
with	O
the	O
mean	O
squared	O
error	O
,	O
msetrain	O
=	O
1	O
m	O
m	O
i=1	O
||	O
ˆy	O
(	O
)	O
i	O
−	O
y	O
(	O
)	O
i	O
||2	O
,	O
we	O
immediately	O
see	O
that	O
maximizing	O
the	O
log-likelihood	O
with	O
respect	O
to	O
w	O
yields	O
the	O
same	O
estimate	O
of	O
the	O
parameters	O
w	O
as	O
does	O
minimizing	O
the	O
mean	O
squared	O
error	O
.	O
the	O
two	O
criteria	O
have	O
diﬀerent	O
values	O
but	O
the	O
same	O
location	O
of	O
the	O
optimum	O
.	O
this	O
justiﬁes	O
the	O
use	O
of	O
the	O
mse	O
as	O
a	O
maximum	O
likelihood	O
estimation	O
procedure	O
.	O
as	O
we	O
will	O
see	O
,	O
the	O
maximum	O
likelihood	O
estimator	O
has	O
several	O
desirable	O
properties	O
.	O
5.5.2	O
properties	O
of	O
maximum	O
likelihood	O
the	O
main	O
appeal	O
of	O
the	O
maximum	O
likelihood	O
estimator	O
is	O
that	O
it	O
can	O
be	O
shown	O
to	O
be	O
the	O
best	O
estimator	O
asymptotically	O
,	O
as	O
the	O
number	O
of	O
examples	O
m	O
,	O
in	O
terms	O
of	O
its	O
rate	O
of	O
convergence	O
as	O
increases	O
.	O
m	O
→	O
∞	O
under	O
appropriate	O
conditions	B
,	O
the	O
maximum	O
likelihood	O
estimator	O
has	O
the	O
property	O
of	O
consistency	O
(	O
see	O
section	O
above	O
)	O
,	O
meaning	O
that	O
as	O
the	O
number	O
of	O
training	O
examples	O
approaches	O
inﬁnity	O
,	O
the	O
maximum	O
likelihood	O
estimate	O
of	O
a	O
parameter	O
converges	O
to	O
the	O
true	O
value	O
of	O
the	O
parameter	O
.	O
these	O
conditions	B
are	O
:	O
·	O
;	O
θ	O
)	O
.	O
the	O
true	O
distribution	O
pdata	O
must	O
lie	O
within	O
the	O
model	B
family	O
pmodel	O
(	O
otherwise	O
,	O
no	O
estimator	O
can	O
recover	O
pdata	O
.	O
5.4.5	O
•	O
•	O
the	O
true	O
distribution	O
pdata	O
must	O
correspond	O
to	O
exactly	O
one	O
value	O
of	O
θ.	O
other-	O
wise	O
,	O
maximum	O
likelihood	O
can	O
recover	O
the	O
correct	O
pdata	O
,	O
but	O
will	O
not	O
be	O
able	O
to	O
determine	O
which	O
value	O
of	O
was	O
used	O
by	O
the	O
data	O
generating	O
processing	O
.	O
θ	O
there	O
are	O
other	O
inductive	O
principles	O
besides	O
the	O
maximum	O
likelihood	O
estima-	O
tor	O
,	O
many	O
of	O
which	O
share	O
the	O
property	O
of	O
being	O
consistent	O
estimators	O
.	O
however	O
,	O
134	O
chapter	O
5.	O
machine	O
learning	O
basics	O
consistent	O
estimators	O
can	O
diﬀer	O
in	O
their	O
statistic	O
eﬃciency	O
,	O
meaning	O
that	O
one	O
consistent	O
estimator	O
may	O
obtain	O
lower	O
generalization	O
error	O
for	O
a	O
ﬁxed	O
number	O
of	O
samples	O
m	O
,	O
or	O
equivalently	O
,	O
may	O
require	O
fewer	O
examples	O
to	O
obtain	O
a	O
ﬁxed	O
level	O
of	O
generalization	O
error	O
.	O
statistical	O
eﬃciency	O
is	O
typically	O
studied	O
in	O
the	O
parametric	O
case	O
(	O
like	O
in	O
linear	O
regression	O
)	O
where	O
our	O
goal	O
is	O
to	O
estimate	O
the	O
value	O
of	O
a	O
parameter	O
(	O
and	O
assuming	O
it	O
is	O
possible	O
to	O
identify	O
the	O
true	O
parameter	O
)	O
,	O
not	O
the	O
value	O
of	O
a	O
function	O
.	O
a	O
way	O
to	O
measure	O
how	O
close	O
we	O
are	O
to	O
the	O
true	O
parameter	O
is	O
by	O
the	O
expected	O
mean	O
squared	O
error	O
,	O
computing	O
the	O
squared	O
diﬀerence	O
between	O
the	O
estimated	O
and	O
true	O
parameter	O
values	O
,	O
where	O
the	O
expectation	O
is	O
over	O
m	O
training	O
samples	O
from	O
the	O
data	O
generating	O
distribution	O
.	O
that	O
parametric	O
mean	O
squared	O
error	O
decreases	O
as	O
m	O
increases	O
,	O
and	O
for	O
m	O
large	O
,	O
the	O
cramér-rao	O
lower	O
bound	B
(	O
)	O
shows	O
that	O
no	O
consistent	O
estimator	O
has	O
a	O
lower	O
mean	O
squared	O
error	O
than	O
the	O
maximum	O
likelihood	O
estimator	O
.	O
rao	O
1945	O
cramér	O
1946	O
,	O
,	O
;	O
for	O
these	O
reasons	O
(	O
consistency	O
and	O
eﬃciency	O
)	O
,	O
maximum	O
likelihood	O
is	O
often	O
considered	O
the	O
preferred	O
estimator	O
to	O
use	O
for	O
machine	O
learning	O
.	O
when	O
the	O
number	O
of	O
examples	O
is	O
small	O
enough	O
to	O
yield	O
overﬁtting	O
behavior	O
,	O
regularization	O
strategies	O
such	O
as	O
weight	O
decay	O
may	O
be	O
used	O
to	O
obtain	O
a	O
biased	O
version	O
of	O
maximum	O
likelihood	O
that	O
has	O
less	O
variance	O
when	O
training	O
data	O
is	O
limited	O
.	O
5.6	O
bayesian	O
statistics	O
so	O
far	O
we	O
have	O
discussed	O
frequentist	O
statistics	O
and	O
approaches	O
based	O
on	O
estimat-	O
ing	O
a	O
single	O
value	O
of	O
θ	O
,	O
then	O
making	O
all	O
predictions	O
thereafter	O
based	O
on	O
that	O
one	O
estimate	O
.	O
another	O
approach	O
is	O
to	O
consider	O
all	O
possible	O
values	O
of	O
θ	O
when	O
making	O
a	O
prediction	O
.	O
the	O
latter	O
is	O
the	O
domain	O
of	O
bayesian	O
statistics	O
.	O
as	O
discussed	O
in	O
section	O
,	O
the	O
frequentist	O
perspective	O
is	O
that	O
the	O
true	O
parameter	O
value	O
θ	O
is	O
ﬁxed	O
but	O
unknown	O
,	O
while	O
the	O
point	O
estimate	O
ˆθ	O
is	O
a	O
random	O
variable	O
on	O
account	O
of	O
it	O
being	O
a	O
function	O
of	O
the	O
dataset	O
(	O
which	O
is	O
seen	O
as	O
random	O
)	O
.	O
5.4.1	O
the	O
bayesian	O
perspective	O
on	O
statistics	O
is	O
quite	O
diﬀerent	O
.	O
the	O
bayesian	O
uses	O
probability	O
to	O
reﬂect	O
degrees	O
of	O
certainty	O
of	O
states	O
of	O
knowledge	O
.	O
the	O
dataset	O
is	O
directly	O
observed	O
and	O
so	O
is	O
not	O
random	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
true	O
parameter	O
θ	O
is	O
unknown	O
or	O
uncertain	O
and	O
thus	O
is	O
represented	O
as	O
a	O
random	O
variable	O
.	O
before	O
observing	O
the	O
data	O
,	O
we	O
represent	O
our	O
knowledge	O
of	O
θ	O
using	O
the	O
prior	O
probability	O
distribution	O
,	O
p	O
(	O
θ	O
)	O
(	O
sometimes	O
referred	O
to	O
as	O
simply	O
“	O
the	O
prior	O
”	O
)	O
.	O
generally	O
,	O
the	O
machine	O
learning	O
practitioner	O
selects	O
a	O
prior	O
distribution	O
that	O
is	O
quite	O
broad	O
(	O
i.e	O
.	O
with	O
high	O
entropy	O
)	O
to	O
reﬂect	O
a	O
high	O
degree	O
of	O
uncertainty	O
in	O
the	O
135	O
chapter	O
5.	O
machine	O
learning	O
basics	O
value	O
of	O
θ	O
before	O
observing	O
any	O
data	O
.	O
for	O
example	O
,	O
one	O
might	O
assume	O
that	O
θ	O
lies	O
in	O
some	O
ﬁnite	O
range	O
or	O
volume	O
,	O
with	O
a	O
uniform	O
distribution	O
.	O
many	O
priors	O
instead	O
reﬂect	O
a	O
preference	O
for	O
“	O
simpler	O
”	O
solutions	O
(	O
such	O
as	O
smaller	O
magnitude	O
coeﬃcients	O
,	O
or	O
a	O
function	O
that	O
is	O
closer	O
to	O
being	O
constant	O
)	O
.	O
{	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
}	O
.	O
we	O
can	O
recover	O
the	O
eﬀect	O
of	O
data	O
on	O
our	O
belief	O
about	O
θ	O
by	O
combining	O
the	O
data	O
likelihood	O
p	O
x	O
(	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
now	O
consider	O
that	O
we	O
have	O
a	O
set	O
of	O
data	O
samples	O
θ	O
)	O
with	O
the	O
prior	O
via	O
bayes	O
’	O
rule	O
:	O
a	O
priori	O
)	O
m	O
)	O
m	O
|	O
|	O
p	O
(	O
θ	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
m	O
)	O
=	O
p	O
x	O
(	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
m	O
p	O
x	O
(	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
|	O
θ	O
)	O
(	O
p	O
)	O
θ	O
)	O
m	O
)	O
(	O
5.67	O
)	O
in	O
the	O
scenarios	O
where	O
bayesian	O
estimation	O
is	O
typically	O
used	O
,	O
the	O
prior	O
begins	O
as	O
a	O
relatively	O
uniform	O
or	O
gaussian	O
distribution	O
with	O
high	O
entropy	O
,	O
and	O
the	O
observation	O
of	O
the	O
data	O
usually	O
causes	O
the	O
posterior	O
to	O
lose	O
entropy	O
and	O
concentrate	O
around	O
a	O
few	O
highly	O
likely	O
values	O
of	O
the	O
parameters	O
.	O
relative	O
to	O
maximum	O
likelihood	O
estimation	O
,	O
bayesian	O
estimation	O
oﬀers	O
two	O
important	O
diﬀerences	O
.	O
first	O
,	O
unlike	O
the	O
maximum	O
likelihood	O
approach	O
that	O
makes	O
predictions	O
using	O
a	O
point	O
estimate	O
of	O
θ	O
,	O
the	O
bayesian	O
approach	O
is	O
to	O
make	O
predictions	O
using	O
a	O
full	O
distribution	O
over	O
θ.	O
for	O
example	O
,	O
after	O
observing	O
m	O
examples	O
,	O
the	O
predicted	O
distribution	O
over	O
the	O
next	O
data	O
sample	O
,	O
x	O
(	O
+1	O
)	O
m	O
,	O
is	O
given	O
by	O
	O
|	O
|	O
|	O
p	O
x	O
(	O
(	O
+1	O
)	O
m	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
m	O
)	O
=	O
p	O
x	O
(	O
(	O
+1	O
)	O
m	O
θ	O
θ	O
)	O
(	O
p	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
m	O
)	O
d	O
.θ	O
(	O
5.68	O
)	O
here	O
each	O
value	O
of	O
θ	O
with	O
positive	O
probability	O
density	O
contributes	O
to	O
the	O
prediction	O
of	O
the	O
next	O
example	O
,	O
with	O
the	O
contribution	O
weighted	O
by	O
the	O
posterior	O
density	O
itself	O
.	O
after	O
having	O
observed	O
,	O
if	O
we	O
are	O
still	O
quite	O
uncertain	O
about	O
the	O
value	O
of	O
θ	O
,	O
then	O
this	O
uncertainty	O
is	O
incorporated	O
directly	O
into	O
any	O
predictions	O
we	O
might	O
make	O
.	O
{	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
}	O
)	O
m	O
5.4	O
in	O
section	O
,	O
we	O
discussed	O
how	O
the	O
frequentist	O
approach	O
addresses	O
the	O
uncer-	O
tainty	O
in	O
a	O
given	O
point	O
estimate	O
of	O
θ	O
by	O
evaluating	O
its	O
variance	O
.	O
the	O
variance	O
of	O
the	O
estimator	O
is	O
an	O
assessment	O
of	O
how	O
the	O
estimate	O
might	O
change	O
with	O
alternative	O
samplings	O
of	O
the	O
observed	O
data	O
.	O
the	O
bayesian	O
answer	O
to	O
the	O
question	O
of	O
how	O
to	O
deal	O
with	O
the	O
uncertainty	O
in	O
the	O
estimator	O
is	O
to	O
simply	O
integrate	O
over	O
it	O
,	O
which	O
tends	O
to	O
protect	O
well	O
against	O
overﬁtting	O
.	O
this	O
integral	O
is	O
of	O
course	O
just	O
an	O
application	O
of	O
the	O
laws	O
of	O
probability	O
,	O
making	O
the	O
bayesian	O
approach	O
simple	O
to	O
justify	O
,	O
while	O
the	O
frequentist	O
machinery	O
for	O
constructing	O
an	O
estimator	O
is	O
based	O
on	O
the	O
rather	O
ad	O
hoc	O
decision	O
to	O
summarize	O
all	O
knowledge	O
contained	O
in	O
the	O
dataset	O
with	O
a	O
single	O
point	O
estimate	O
.	O
the	O
second	O
important	O
diﬀerence	O
between	O
the	O
bayesian	O
approach	O
to	O
estimation	O
and	O
the	O
maximum	O
likelihood	O
approach	O
is	O
due	O
to	O
the	O
contribution	O
of	O
the	O
bayesian	O
136	O
chapter	O
5.	O
machine	O
learning	O
basics	O
prior	O
distribution	O
.	O
the	O
prior	O
has	O
an	O
inﬂuence	O
by	O
shifting	O
probability	O
mass	O
density	O
towards	O
regions	O
of	O
the	O
parameter	O
space	O
that	O
are	O
preferred	O
.	O
in	O
practice	O
,	O
the	O
prior	O
often	O
expresses	O
a	O
preference	O
for	O
models	O
that	O
are	O
simpler	O
or	O
more	O
smooth	O
.	O
critics	O
of	O
the	O
bayesian	O
approach	O
identify	O
the	O
prior	O
as	O
a	O
source	O
of	O
subjective	O
human	O
judgment	O
impacting	O
the	O
predictions	O
.	O
a	O
priori	O
bayesian	O
methods	O
typically	O
generalize	O
much	O
better	O
when	O
limited	O
training	O
data	O
is	O
available	O
,	O
but	O
typically	O
suﬀer	O
from	O
high	O
computational	O
cost	O
when	O
the	O
number	O
of	O
training	O
examples	O
is	O
large	O
.	O
example	O
:	O
bayesian	O
linear	O
regression	O
here	O
we	O
consider	O
the	O
bayesian	O
esti-	O
mation	O
approach	O
to	O
learning	O
the	O
linear	O
regression	O
parameters	O
.	O
in	O
linear	O
regression	O
,	O
we	O
learn	O
a	O
linear	O
mapping	O
from	O
an	O
input	O
vector	O
x	O
n	O
to	O
predict	O
the	O
value	O
of	O
a	O
scalar	O
.	O
the	O
prediction	O
is	O
parametrized	O
by	O
the	O
vector	O
∈	O
∈	O
∈	O
w	O
r	O
y	O
n	O
:	O
r	O
r	O
given	O
a	O
set	O
of	O
m	O
training	O
samples	O
(	O
x	O
(	O
of	O
over	O
the	O
entire	O
training	O
set	O
as	O
:	O
y	O
train	O
,	O
y	O
(	O
)	O
train	O
)	O
,	O
we	O
can	O
express	O
the	O
prediction	O
)	O
(	O
5.69	O
)	O
(	O
5.70	O
)	O
	O
(	O
5.71	O
)	O
	O
ˆy	O
=	O
w	O
x	O
.	O
ˆy	O
(	O
train	O
=	O
x	O
(	O
)	O
train	O
w.	O
)	O
	O
n	O
(	O
y	O
(	O
exp	O
)	O
=	O
∝	O
expressed	O
as	O
a	O
gaussian	O
conditional	O
distribution	O
on	O
y	O
(	O
train	O
,	O
we	O
have	O
)	O
train	O
|	O
)	O
p	O
(	O
y	O
(	O
x	O
(	O
train	O
,	O
w	O
)	O
)	O
train	O
;	O
x	O
(	O
−	O
1	O
2	O
(	O
y	O
(	O
)	O
train	O
w	O
i	O
,	O
)	O
train	O
−	O
)	O
x	O
(	O
	O
train	O
w	O
)	O
)	O
train	O
−	O
)	O
(	O
y	O
(	O
x	O
(	O
train	O
w	O
)	O
)	O
,	O
where	O
we	O
follow	O
the	O
standard	O
mse	O
formulation	O
in	O
assuming	O
that	O
the	O
gaussian	O
variance	O
on	O
y	O
is	O
one	O
.	O
in	O
what	O
follows	O
,	O
to	O
reduce	O
the	O
notational	O
burden	O
,	O
we	O
refer	O
to	O
(	O
x	O
(	O
as	O
simply	O
x	O
y	O
,	O
train	O
,	O
y	O
(	O
train	O
)	O
)	O
.	O
)	O
(	O
)	O
(	O
5.72	O
)	O
to	O
determine	O
the	O
posterior	O
distribution	O
over	O
the	O
model	B
parameter	O
vector	O
w	O
,	O
we	O
ﬁrst	O
need	O
to	O
specify	O
a	O
prior	O
distribution	O
.	O
the	O
prior	O
should	O
reﬂect	O
our	O
naive	O
belief	O
about	O
the	O
value	O
of	O
these	O
parameters	O
.	O
while	O
it	O
is	O
sometimes	O
diﬃcult	O
or	O
unnatural	O
to	O
express	O
our	O
prior	O
beliefs	O
in	O
terms	O
of	O
the	O
parameters	O
of	O
the	O
model	B
,	O
in	O
practice	O
we	O
typically	O
assume	O
a	O
fairly	O
broad	O
distribution	O
expressing	O
a	O
high	O
degree	O
of	O
uncertainty	O
about	O
θ.	O
for	O
real-valued	O
parameters	O
it	O
is	O
common	O
to	O
use	O
a	O
gaussian	O
as	O
a	O
prior	O
distribution	O
:	O
	O
	O
n	O
)	O
=	O
∝	O
w	O
µ0	O
,	O
λ0	O
)	O
(	O
;	O
p	O
(	O
w	O
exp	O
−	O
−	O
	O
−	O
1	O
λ	O
0	O
(	O
w	O
µ	O
0	O
)	O
(	O
w	O
µ	O
0	O
)	O
,	O
(	O
5.73	O
)	O
−	O
1	O
2	O
137	O
chapter	O
5.	O
machine	O
learning	O
basics	O
where	O
µ0	O
and	O
λ0	O
are	O
the	O
prior	O
distribution	O
mean	O
vector	O
and	O
covariance	O
matrix	O
respectively	O
.	O
1	O
distribution	O
over	O
the	O
model	B
parameters	O
.	O
with	O
the	O
prior	O
thus	O
speciﬁed	O
,	O
we	O
can	O
now	O
proceed	O
in	O
determining	O
the	O
posterior	O
|	O
|	O
	O
	O
	O
p	O
(	O
w	O
x	O
,	O
∝	O
y	O
)	O
∝	O
	O
	O
	O
	O
	O
p	O
,	O
(	O
y	O
x	O
−1	O
2	O
exp	O
)	O
w	O
p	O
w	O
)	O
(	O
−	O
y	O
xw	O
(	O
	O
	O
)	O
−	O
y	O
xw	O
(	O
)	O
exp	O
−	O
1	O
2	O
−	O
(	O
w	O
µ	O
	O
λ	O
0	O
)	O
∝	O
exp	O
−1	O
2	O
−	O
2y	O
	O
	O
xw	O
w+	O
	O
	O
x	O
	O
xw	O
w+	O
	O
λ	O
−	O
−	O
1	O
0	O
w	O
	O
µ	O
2	O
0	O
λ	O
	O
−	O
1	O
0	O
w	O
(	O
5.74	O
)	O
−	O
	O
0	O
)	O
−	O
1	O
0	O
(	O
w	O
µ	O
(	O
5.75	O
)	O
.	O
(	O
5.76	O
)	O
we	O
now	O
deﬁne	O
λm	O
=	O
.	O
using	O
these	O
new	O
variables	O
,	O
we	O
ﬁnd	O
that	O
the	O
posterior	O
may	O
be	O
rewritten	O
as	O
a	O
gaussian	O
distribution	O
:	O
x	O
x	O
	O
−	O
1	O
x	O
+	O
λ	O
0	O
−	O
1	O
and	O
µm	O
=	O
λm	O
	O
−	O
1	O
0	O
µ0	O
y	O
+	O
λ	O
	O
|	O
∝	O
y	O
)	O
p	O
(	O
w	O
x	O
,	O
exp	O
exp	O
∝	O
−	O
1	O
2	O
−	O
1	O
2	O
−	O
−	O
	O
m	O
)	O
	O
m	O
)	O
−	O
−	O
1	O
m	O
(	O
w	O
µ	O
λ	O
−	O
1	O
λ	O
m	O
(	O
w	O
µ	O
−	O
m	O
)	O
+	O
m	O
)	O
.	O
(	O
w	O
µ	O
(	O
w	O
µ	O
	O
−	O
1	O
mλ	O
m	O
µm	O
µ	O
1	O
2	O
(	O
5.77	O
)	O
(	O
5.78	O
)	O
	O
all	O
terms	O
that	O
do	O
not	O
include	O
the	O
parameter	O
vector	O
w	O
have	O
been	O
omitted	O
;	O
they	O
are	O
implied	O
by	O
the	O
fact	O
that	O
the	O
distribution	O
must	O
be	O
normalized	O
to	O
integrate	O
to	O
.1	O
equation	O
shows	O
how	O
to	O
normalize	O
a	O
multivariate	O
gaussian	O
distribution	O
.	O
3.23	O
examining	O
this	O
posterior	O
distribution	O
allows	O
us	O
to	O
gain	O
some	O
intuition	O
for	O
the	O
eﬀect	O
of	O
bayesian	O
inference	O
.	O
in	O
most	O
situations	O
,	O
we	O
set	O
µ0	O
to	O
0.	O
if	O
we	O
set	O
λ0	O
=	O
1	O
i	O
,	O
α	O
then	O
µm	O
gives	O
the	O
same	O
estimate	O
of	O
w	O
as	O
does	O
frequentist	O
linear	O
regression	O
with	O
a	O
	O
weight	O
decay	O
penalty	O
of	O
αw	O
w.	O
one	O
diﬀerence	O
is	O
that	O
the	O
bayesian	O
estimate	O
is	O
undeﬁned	O
if	O
α	O
is	O
set	O
to	O
zero—-we	O
are	O
not	O
allowed	O
to	O
begin	O
the	O
bayesian	O
learning	O
process	O
with	O
an	O
inﬁnitely	O
wide	O
prior	O
on	O
w.	O
the	O
more	O
important	O
diﬀerence	O
is	O
that	O
the	O
bayesian	O
estimate	O
provides	O
a	O
covariance	O
matrix	O
,	O
showing	O
how	O
likely	O
all	O
the	O
diﬀerent	O
values	O
of	O
are	O
,	O
rather	O
than	O
providing	O
only	O
the	O
estimate	O
w	O
µm	O
.	O
5.6.1	O
maximum	O
a	O
posteriori	O
(	O
map	O
)	O
estimation	O
while	O
the	O
most	O
principled	O
approach	O
is	O
to	O
make	O
predictions	O
using	O
the	O
full	O
bayesian	O
posterior	O
distribution	O
over	O
the	O
parameter	O
θ	O
,	O
it	O
is	O
still	O
often	O
desirable	O
to	O
have	O
a	O
1	O
unless	O
there	O
is	O
a	O
reason	O
to	O
assume	O
a	O
particular	O
covariance	O
structure	O
,	O
we	O
typically	O
assume	O
a	O
diagonal	O
covariance	O
matrix	O
λ0	O
=	O
diag	O
(	O
λ	O
0	O
)	O
.	O
138	O
chapter	O
5.	O
machine	O
learning	O
basics	O
θmap	O
=	O
arg	O
max	O
θ	O
θ	O
|	O
θ	O
x	O
p	O
(	O
|	O
p	O
x	O
θ	O
|	O
single	O
point	O
estimate	O
.	O
one	O
common	O
reason	O
for	O
desiring	O
a	O
point	O
estimate	O
is	O
that	O
most	O
operations	O
involving	O
the	O
bayesian	O
posterior	O
for	O
most	O
interesting	O
models	O
are	O
intractable	O
,	O
and	O
a	O
point	O
estimate	O
oﬀers	O
a	O
tractable	O
approximation	O
.	O
rather	O
than	O
simply	O
returning	O
to	O
the	O
maximum	O
likelihood	O
estimate	O
,	O
we	O
can	O
still	O
gain	O
some	O
of	O
the	O
beneﬁt	O
of	O
the	O
bayesian	O
approach	O
by	O
allowing	O
the	O
prior	O
to	O
inﬂuence	O
the	O
choice	O
of	O
the	O
point	O
estimate	O
.	O
one	O
rational	O
way	O
to	O
do	O
this	O
is	O
to	O
choose	O
the	O
maximum	O
a	O
posteriori	O
(	O
map	O
)	O
point	O
estimate	O
.	O
the	O
map	O
estimate	O
chooses	O
the	O
point	O
of	O
maximal	O
posterior	O
probability	O
(	O
or	O
maximal	O
probability	O
density	O
in	O
the	O
more	O
common	O
case	O
of	O
continuous	O
)	O
:	O
θ	O
)	O
=	O
arg	O
max	O
log	O
(	O
)	O
+	O
log	O
(	O
)	O
p	O
θ	O
.	O
(	O
5.79	O
)	O
we	O
recognize	O
,	O
above	O
on	O
the	O
right	O
hand	O
side	O
,	O
log	O
p	O
(	O
x	O
θ	O
likelihood	O
term	O
,	O
and	O
,	O
corresponding	O
to	O
the	O
prior	O
distribution	O
.	O
log	O
(	O
)	O
p	O
θ	O
)	O
,	O
i.e	O
.	O
the	O
standard	O
log-	O
n	O
as	O
an	O
example	O
,	O
consider	O
a	O
linear	O
regression	O
model	B
with	O
a	O
gaussian	O
prior	O
on	O
(	O
w	O
;	O
0	O
,	O
1	O
i	O
2	O
)	O
,	O
then	O
the	O
log-prior	O
term	O
in	O
the	O
weights	O
w.	O
if	O
this	O
prior	O
is	O
given	O
by	O
	O
λ	O
w	O
weight	O
decay	O
penalty	O
,	O
plus	O
a	O
equation	O
term	O
that	O
does	O
not	O
depend	O
on	O
w	O
and	O
does	O
not	O
aﬀect	O
the	O
learning	O
process	O
.	O
map	O
bayesian	O
inference	O
with	O
a	O
gaussian	O
prior	O
on	O
the	O
weights	O
thus	O
corresponds	O
to	O
weight	O
decay	O
.	O
is	O
proportional	O
to	O
the	O
familiar	O
5.79	O
λw	O
as	O
with	O
full	O
bayesian	O
inference	O
,	O
map	O
bayesian	O
inference	O
has	O
the	O
advantage	O
of	O
leveraging	O
information	O
that	O
is	O
brought	O
by	O
the	O
prior	O
and	O
can	O
not	O
be	O
found	O
in	O
the	O
training	O
data	O
.	O
this	O
additional	O
information	O
helps	O
to	O
reduce	O
the	O
variance	O
in	O
the	O
map	O
point	O
estimate	O
(	O
in	O
comparison	O
to	O
the	O
ml	O
estimate	O
)	O
.	O
however	O
,	O
it	O
does	O
so	O
at	O
the	O
price	O
of	O
increased	O
bias	O
.	O
many	O
regularized	O
estimation	O
strategies	O
,	O
such	O
as	O
maximum	O
likelihood	O
learning	O
regularized	O
with	O
weight	O
decay	O
,	O
can	O
be	O
interpreted	O
as	O
making	O
the	O
map	O
approxima-	O
tion	B
to	O
bayesian	O
inference	O
.	O
this	O
view	O
applies	O
when	O
the	O
regularization	O
consists	O
of	O
adding	O
an	O
extra	O
term	O
to	O
the	O
objective	O
function	O
that	O
corresponds	O
to	O
log	O
p	O
(	O
θ	O
)	O
.	O
not	O
all	O
regularization	O
penalties	O
correspond	O
to	O
map	O
bayesian	O
inference	O
.	O
for	O
example	O
,	O
some	O
regularizer	O
terms	O
may	O
not	O
be	O
the	O
logarithm	O
of	O
a	O
probability	O
distribution	O
.	O
other	O
regularization	O
terms	O
depend	O
on	O
the	O
data	O
,	O
which	O
of	O
course	O
a	O
prior	O
probability	O
distribution	O
is	O
not	O
allowed	O
to	O
do	O
.	O
map	O
bayesian	O
inference	O
provides	O
a	O
straightforward	O
way	O
to	O
design	O
complicated	O
yet	O
interpretable	O
regularization	O
terms	O
.	O
for	O
example	O
,	O
a	O
more	O
complicated	O
penalty	O
term	O
can	O
be	O
derived	O
by	O
using	O
a	O
mixture	O
of	O
gaussians	O
,	O
rather	O
than	O
a	O
single	O
gaussian	O
distribution	O
,	O
as	O
the	O
prior	O
(	O
nowlan	O
and	O
hinton	O
1992	O
)	O
.	O
,	O
139	O
chapter	O
5.	O
machine	O
learning	O
basics	O
5.7	O
supervised	O
learning	O
algorithms	O
5.1.3	O
recall	O
from	O
section	O
that	O
supervised	O
learning	O
algorithms	O
are	O
,	O
roughly	O
speaking	O
,	O
learning	O
algorithms	O
that	O
learn	O
to	O
associate	O
some	O
input	O
with	O
some	O
output	O
,	O
given	O
a	O
training	O
set	O
of	O
examples	O
of	O
inputs	O
x	O
and	O
outputs	O
y.	O
in	O
many	O
cases	O
the	O
outputs	O
y	O
may	O
be	O
diﬃcult	O
to	O
collect	O
automatically	O
and	O
must	O
be	O
provided	O
by	O
a	O
human	O
“	O
supervisor	O
,	O
”	O
but	O
the	O
term	O
still	O
applies	O
even	O
when	O
the	O
training	O
set	O
targets	O
were	O
collected	O
automatically	O
.	O
5.7.1	O
probabilistic	O
supervised	O
learning	O
most	O
supervised	O
learning	O
algorithms	O
in	O
this	O
book	O
are	O
based	O
on	O
estimating	O
a	O
probability	O
distribution	O
p	O
(	O
y	O
x	O
)	O
.	O
we	O
can	O
do	O
this	O
simply	O
by	O
using	O
maximum	O
likelihood	O
estimation	O
to	O
ﬁnd	O
the	O
best	O
parameter	O
vector	O
θ	O
for	O
a	O
parametric	O
family	O
of	O
distributions	O
.	O
x	O
θ	O
;	O
)	O
p	O
y	O
(	O
|	O
|	O
we	O
have	O
already	O
seen	O
that	O
linear	O
regression	O
corresponds	O
to	O
the	O
family	O
|	O
n	O
x	O
θ	O
;	O
)	O
=	O
p	O
y	O
(	O
	O
y	O
(	O
;	O
θ	O
x	O
i	O
,	O
)	O
.	O
(	O
5.80	O
)	O
we	O
can	O
generalize	O
linear	O
regression	O
to	O
the	O
classiﬁcation	O
scenario	O
by	O
deﬁning	O
a	O
diﬀerent	O
family	O
of	O
probability	O
distributions	O
.	O
if	O
we	O
have	O
two	O
classes	O
,	O
class	O
0	O
and	O
class	O
1	O
,	O
then	O
we	O
need	O
only	O
specify	O
the	O
probability	O
of	O
one	O
of	O
these	O
classes	O
.	O
the	O
probability	O
of	O
class	O
1	O
determines	O
the	O
probability	O
of	O
class	O
0	O
,	O
because	O
these	O
two	O
values	O
must	O
add	O
up	O
to	O
1.	O
the	O
normal	O
distribution	O
over	O
real-valued	O
numbers	O
that	O
we	O
used	O
for	O
linear	O
regression	O
is	O
parametrized	O
in	O
terms	O
of	O
a	O
mean	O
.	O
any	O
value	O
we	O
supply	O
for	O
this	O
mean	O
is	O
valid	O
.	O
a	O
distribution	O
over	O
a	O
binary	O
variable	O
is	O
slightly	O
more	O
complicated	O
,	O
because	O
its	O
mean	O
must	O
always	O
be	O
between	O
0	O
and	O
1.	O
one	O
way	O
to	O
solve	O
this	O
problem	O
is	O
to	O
use	O
the	O
logistic	O
sigmoid	O
function	O
to	O
squash	O
the	O
output	O
of	O
the	O
linear	O
function	O
into	O
the	O
interval	O
(	O
0	O
,	O
1	O
)	O
and	O
interpret	O
that	O
value	O
as	O
a	O
probability	O
:	O
|	O
(	O
=	O
1	O
p	O
y	O
	O
(	O
θ	O
x	O
θ	O
;	O
)	O
=	O
σ	O
x	O
)	O
.	O
(	O
5.81	O
)	O
this	O
approach	O
is	O
known	O
as	O
logistic	O
regression	O
(	O
a	O
somewhat	O
strange	O
name	O
since	O
we	O
use	O
the	O
model	B
for	O
classiﬁcation	O
rather	O
than	O
regression	O
)	O
.	O
in	O
the	O
case	O
of	O
linear	O
regression	O
,	O
we	O
were	O
able	O
to	O
ﬁnd	O
the	O
optimal	O
weights	O
by	O
solving	O
the	O
normal	O
equations	O
.	O
logistic	O
regression	O
is	O
somewhat	O
more	O
diﬃcult	O
.	O
there	O
is	O
no	O
closed-form	O
solution	O
for	O
its	O
optimal	O
weights	O
.	O
instead	O
,	O
we	O
must	O
search	O
for	O
them	O
by	O
maximizing	O
the	O
log-likelihood	O
.	O
we	O
can	O
do	O
this	O
by	O
minimizing	O
the	O
negative	O
log-likelihood	O
(	O
nll	O
)	O
using	O
gradient	O
descent	B
.	O
140	O
chapter	O
5.	O
machine	O
learning	O
basics	O
this	O
same	O
strategy	O
can	O
be	O
applied	O
to	O
essentially	O
any	O
supervised	O
learning	O
problem	O
,	O
by	O
writing	O
down	O
a	O
parametric	O
family	O
of	O
conditional	O
probability	O
distributions	O
over	O
the	O
right	O
kind	O
of	O
input	O
and	O
output	O
variables	O
.	O
5.7.2	O
support	O
vector	O
machines	O
;	O
,	O
boser	O
et	O
al	O
.	O
1992	O
cortes	O
and	O
vapnik	O
1995	O
one	O
of	O
the	O
most	O
inﬂuential	O
approaches	O
to	O
supervised	O
learning	O
is	O
the	O
support	O
vector	O
)	O
.	O
this	O
model	B
is	O
similar	O
to	O
machine	O
(	O
logistic	O
regression	O
in	O
that	O
it	O
is	O
driven	O
by	O
a	O
linear	O
function	O
w	O
x	O
+	O
b.	O
unlike	O
logistic	O
regression	O
,	O
the	O
support	O
vector	O
machine	O
does	O
not	O
provide	O
probabilities	O
,	O
but	O
only	O
outputs	O
a	O
class	O
identity	O
.	O
the	O
svm	O
predicts	O
that	O
the	O
positive	O
class	O
is	O
present	O
when	O
	O
x	O
+	O
b	O
is	O
positive	O
.	O
likewise	O
,	O
it	O
predicts	O
that	O
the	O
negative	O
class	O
is	O
present	O
when	O
w	O
	O
w	O
x	O
+	O
b	O
is	O
negative	O
.	O
	O
,	O
one	O
key	O
innovation	O
associated	O
with	O
support	O
vector	O
machines	O
is	O
the	O
kernel	O
trick	B
.	O
the	O
kernel	O
trick	B
consists	O
of	O
observing	O
that	O
many	O
machine	O
learning	O
algorithms	O
can	O
be	O
written	O
exclusively	O
in	O
terms	O
of	O
dot	O
products	O
between	O
examples	O
.	O
for	O
example	O
,	O
it	O
can	O
be	O
shown	O
that	O
the	O
linear	O
function	O
used	O
by	O
the	O
support	O
vector	O
machine	O
can	O
be	O
re-written	O
as	O
	O
(	O
5.82	O
)	O
	O
w	O
x	O
+	O
=	O
+	O
b	O
b	O
αix	O
i=1	O
m	O
	O
x	O
(	O
)	O
i	O
·	O
where	O
x	O
(	O
)	O
i	O
is	O
a	O
training	O
example	O
and	O
α	O
is	O
a	O
vector	O
of	O
coeﬃcients	O
.	O
rewriting	O
the	O
learning	O
algorithm	O
this	O
way	O
allows	O
us	O
to	O
replace	O
x	O
by	O
the	O
output	O
of	O
a	O
given	O
feature	O
φ	O
(	O
x	O
(	O
)	O
i	O
)	O
called	O
function	O
φ	O
(	O
x	O
)	O
and	O
the	O
dot	O
product	O
with	O
a	O
function	O
k	O
(	O
x	O
x	O
,	O
φ	O
(	O
x	O
(	O
)	O
i	O
)	O
.	O
a	O
kernel	O
.	O
the	O
for	O
some	O
feature	O
spaces	O
,	O
we	O
may	O
not	O
use	O
literally	O
the	O
vector	O
inner	O
product	O
.	O
in	O
some	O
inﬁnite	O
dimensional	O
spaces	O
,	O
we	O
need	O
to	O
use	O
other	O
kinds	O
of	O
inner	O
products	O
,	O
for	O
example	O
,	O
inner	O
products	O
based	O
on	O
integration	O
rather	O
than	O
summation	O
.	O
a	O
complete	O
development	O
of	O
these	O
kinds	O
of	O
inner	O
products	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
.	O
operator	O
represents	O
an	O
inner	O
product	O
analogous	O
to	O
φ	O
(	O
x	O
)	O
(	O
)	O
i	O
)	O
=	O
φ	O
(	O
x	O
)	O
	O
·	O
	O
after	O
replacing	O
dot	O
products	O
with	O
kernel	O
evaluations	O
,	O
we	O
can	O
make	O
predictions	O
using	O
the	O
function	O
f	O
(	O
)	O
=	O
b	O
x	O
+	O
αik	O
,	O
(	O
x	O
x	O
(	O
)	O
i	O
)	O
.	O
(	O
5.83	O
)	O
i	O
this	O
function	O
is	O
nonlinear	O
with	O
respect	O
to	O
x	O
,	O
but	O
the	O
relationship	O
between	O
φ	O
(	O
x	O
)	O
and	O
f	O
(	O
x	O
)	O
is	O
linear	O
.	O
also	O
,	O
the	O
relationship	O
between	O
α	O
and	O
f	O
(	O
x	O
)	O
is	O
linear	O
.	O
the	O
kernel-based	O
function	O
is	O
exactly	O
equivalent	O
to	O
preprocessing	O
the	O
data	O
by	O
applying	O
φ	O
(	O
)	O
x	O
to	O
all	O
inputs	O
,	O
then	O
learning	O
a	O
linear	O
model	B
in	O
the	O
new	O
transformed	O
space	O
.	O
the	O
kernel	O
trick	B
is	O
powerful	O
for	O
two	O
reasons	O
.	O
first	O
,	O
it	O
allows	O
us	O
to	O
learn	O
models	O
that	O
are	O
nonlinear	O
as	O
a	O
function	O
of	O
x	O
using	O
convex	O
optimization	O
techniques	O
that	O
are	O
141	O
chapter	O
5.	O
machine	O
learning	O
basics	O
guaranteed	O
to	O
converge	O
eﬃciently	O
.	O
this	O
is	O
possible	O
because	O
we	O
consider	O
φ	O
ﬁxed	O
and	O
optimize	O
only	O
α	O
,	O
i.e.	O
,	O
the	O
optimization	O
algorithm	O
can	O
view	O
the	O
decision	O
function	O
as	O
being	O
linear	O
in	O
a	O
diﬀerent	O
space	O
.	O
second	O
,	O
the	O
kernel	O
function	O
k	O
often	O
admits	O
an	O
implementation	O
that	O
is	O
signiﬁcantly	O
more	O
computational	O
eﬃcient	O
than	O
naively	O
constructing	O
two	O
vectors	O
and	O
explicitly	O
taking	O
their	O
dot	O
product	O
.	O
φ	O
(	O
)	O
x	O
	O
in	O
some	O
cases	O
,	O
φ	O
(	O
x	O
)	O
can	O
even	O
be	O
inﬁnite	O
dimensional	O
,	O
which	O
would	O
result	O
in	O
an	O
inﬁnite	O
computational	O
cost	O
for	O
the	O
naive	O
,	O
explicit	O
approach	O
.	O
in	O
many	O
cases	O
,	O
)	O
is	O
a	O
nonlinear	O
,	O
tractable	O
function	O
of	O
x	O
even	O
when	O
φ	O
(	O
x	O
)	O
is	O
intractable	O
.	O
as	O
k	O
(	O
x	O
x	O
,	O
an	O
example	O
of	O
an	O
inﬁnite-dimensional	O
feature	O
space	O
with	O
a	O
tractable	O
kernel	O
,	O
we	O
construct	O
a	O
feature	O
mapping	O
φ	O
(	O
x	O
)	O
over	O
the	O
non-negative	O
integers	O
x.	O
suppose	O
that	O
this	O
mapping	O
returns	O
a	O
vector	O
containing	O
x	O
ones	O
followed	O
by	O
inﬁnitely	O
many	O
zeros	O
.	O
we	O
can	O
write	O
a	O
kernel	O
function	O
k	O
(	O
x	O
,	O
x	O
(	O
)	O
i	O
)	O
=	O
min	O
(	O
x	O
,	O
x	O
(	O
)	O
i	O
)	O
that	O
is	O
exactly	O
equivalent	O
to	O
the	O
corresponding	O
inﬁnite-dimensional	O
dot	O
product	O
.	O
the	O
most	O
commonly	O
used	O
kernel	O
is	O
the	O
gaussian	O
kernel	O
k	O
(	O
u	O
v	O
,	O
)	O
=	O
;	O
0	O
,	O
σ	O
2i	O
)	O
(	O
5.84	O
)	O
n	O
−	O
u	O
v	O
(	O
n	O
(	O
x	O
;	O
µ	O
,	O
σ	O
)	O
is	O
the	O
standard	O
normal	O
density	O
.	O
this	O
kernel	O
is	O
also	O
known	O
as	O
where	O
the	O
radial	O
basis	O
function	O
(	O
rbf	O
)	O
kernel	O
,	O
because	O
its	O
value	O
decreases	O
along	O
lines	O
in	O
v	O
space	O
radiating	O
outward	O
from	O
u.	O
the	O
gaussian	O
kernel	O
corresponds	O
to	O
a	O
dot	O
product	O
in	O
an	O
inﬁnite-dimensional	O
space	O
,	O
but	O
the	O
derivation	O
of	O
this	O
space	O
is	O
less	O
straightforward	O
than	O
in	O
our	O
example	O
of	O
the	O
kernel	O
over	O
the	O
integers	O
.	O
min	O
we	O
can	O
think	O
of	O
the	O
gaussian	O
kernel	O
as	O
performing	O
a	O
kind	O
of	O
template	O
match-	O
ing	O
.	O
a	O
training	O
example	O
x	O
associated	O
with	O
training	O
label	O
y	O
becomes	O
a	O
template	O
	O
is	O
near	O
x	O
according	O
to	O
euclidean	O
distance	O
,	O
the	O
for	O
class	O
y.	O
when	O
a	O
test	O
point	O
x	O
gaussian	O
kernel	O
has	O
a	O
large	O
response	O
,	O
indicating	O
that	O
x	O
is	O
very	O
similar	O
to	O
the	O
x	O
template	O
.	O
the	O
model	B
then	O
puts	O
a	O
large	O
weight	O
on	O
the	O
associated	O
training	O
label	O
y.	O
overall	O
,	O
the	O
prediction	O
will	O
combine	O
many	O
such	O
training	O
labels	O
weighted	O
by	O
the	O
similarity	O
of	O
the	O
corresponding	O
training	O
examples	O
.	O
	O
support	O
vector	O
machines	O
are	O
not	O
the	O
only	O
algorithm	O
that	O
can	O
be	O
enhanced	O
using	O
the	O
kernel	O
trick	B
.	O
many	O
other	O
linear	O
models	O
can	O
be	O
enhanced	O
in	O
this	O
way	O
.	O
the	O
category	O
of	O
algorithms	O
that	O
employ	O
the	O
kernel	O
trick	B
is	O
known	O
as	O
kernel	O
machines	O
or	O
kernel	O
methods	O
(	O
williams	O
and	O
rasmussen	O
1996	O
schölkopf	O
et	O
al.	O
,	O
1999	O
)	O
.	O
,	O
;	O
a	O
major	O
drawback	O
to	O
kernel	O
machines	O
is	O
that	O
the	O
cost	O
of	O
evaluating	O
the	O
decision	O
function	O
is	O
linear	O
in	O
the	O
number	O
of	O
training	O
examples	O
,	O
because	O
the	O
i-th	O
example	O
(	O
)	O
i	O
)	O
to	O
the	O
decision	O
function	O
.	O
support	O
vector	O
machines	O
contributes	O
a	O
term	O
αik	O
(	O
x	O
x	O
,	O
are	O
able	O
to	O
mitigate	O
this	O
by	O
learning	O
an	O
α	O
vector	O
that	O
contains	O
mostly	O
zeros	O
.	O
classifying	O
a	O
new	O
example	O
then	O
requires	O
evaluating	O
the	O
kernel	O
function	O
only	O
for	O
the	O
training	O
examples	O
that	O
have	O
non-zero	O
αi	O
.	O
these	O
training	O
examples	O
are	O
known	O
142	O
chapter	O
5.	O
machine	O
learning	O
basics	O
as	O
support	O
vectors	O
.	O
kernel	O
machines	O
also	O
suﬀer	O
from	O
a	O
high	O
computational	O
cost	O
of	O
training	O
when	O
.	O
kernel	O
machines	O
with	O
the	O
dataset	O
is	O
large	O
.	O
we	O
will	O
revisit	O
this	O
idea	O
in	O
section	O
.	O
the	O
generic	O
kernels	O
struggle	O
to	O
generalize	O
well	O
.	O
we	O
will	O
explain	O
why	O
in	O
section	O
modern	O
incarnation	O
of	O
deep	O
learning	O
was	O
designed	O
to	O
overcome	O
these	O
limitations	O
of	O
kernel	O
machines	O
.	O
the	O
current	O
deep	O
learning	O
renaissance	O
began	O
when	O
hinton	O
et	O
al	O
.	O
(	O
)	O
demonstrated	O
that	O
a	O
neural	O
network	O
could	O
outperform	O
the	O
rbf	O
kernel	O
svm	O
2006	O
on	O
the	O
mnist	O
benchmark	O
.	O
5.11	O
5.9	O
5.7.3	O
other	O
simple	O
supervised	O
learning	O
algorithms	O
we	O
have	O
already	O
brieﬂy	O
encountered	O
another	O
non-probabilistic	O
supervised	O
learning	O
algorithm	O
,	O
nearest	O
neighbor	O
regression	O
.	O
more	O
generally	O
,	O
k-nearest	B
neighbors	I
is	O
a	O
family	O
of	O
techniques	O
that	O
can	O
be	O
used	O
for	O
classiﬁcation	O
or	O
regression	O
.	O
as	O
a	O
non-parametric	O
learning	O
algorithm	O
,	O
k-nearest	B
neighbors	I
is	O
not	O
restricted	O
to	O
a	O
ﬁxed	O
number	O
of	O
parameters	O
.	O
we	O
usually	O
think	O
of	O
the	O
k-nearest	B
neighbors	I
algorithm	O
as	O
not	O
having	O
any	O
parameters	O
,	O
but	O
rather	O
implementing	O
a	O
simple	O
function	O
of	O
the	O
training	O
data	O
.	O
in	O
fact	O
,	O
there	O
is	O
not	O
even	O
really	O
a	O
training	O
stage	O
or	O
learning	O
process	O
.	O
instead	O
,	O
at	O
test	O
time	O
,	O
when	O
we	O
want	O
to	O
produce	O
an	O
output	O
y	O
for	O
a	O
new	O
test	O
input	O
x	O
,	O
we	O
ﬁnd	O
the	O
k-nearest	B
neighbors	I
to	O
x	O
in	O
the	O
training	O
data	O
x.	O
we	O
then	O
return	O
the	O
average	O
of	O
the	O
corresponding	O
y	O
values	O
in	O
the	O
training	O
set	O
.	O
this	O
works	O
for	O
essentially	O
any	O
kind	O
of	O
supervised	O
learning	O
where	O
we	O
can	O
deﬁne	O
an	O
average	O
over	O
y	O
values	O
.	O
in	O
the	O
case	O
of	O
classiﬁcation	O
,	O
we	O
can	O
average	O
over	O
one-hot	O
code	O
vectors	O
c	O
with	O
cy	O
=	O
1	O
and	O
ci	O
=	O
0	O
for	O
all	O
other	O
values	O
of	O
i.	O
we	O
can	O
then	O
interpret	O
the	O
average	O
over	O
these	O
one-hot	O
codes	O
as	O
giving	O
a	O
probability	O
distribution	O
over	O
classes	O
.	O
as	O
a	O
non-parametric	O
learning	O
algorithm	O
,	O
k-nearest	O
neighbor	O
can	O
achieve	O
very	O
high	O
capacity	O
.	O
for	O
example	O
,	O
suppose	O
we	O
have	O
a	O
multiclass	O
classiﬁcation	O
task	O
and	O
measure	O
performance	O
with	O
0-1	B
loss	I
.	O
in	O
this	O
setting	O
,	O
-nearest	O
neighbor	O
converges	O
to	O
double	O
the	O
bayes	O
error	O
as	O
the	O
1	O
number	O
of	O
training	O
examples	O
approaches	O
inﬁnity	O
.	O
the	O
error	O
in	O
excess	O
of	O
the	O
bayes	O
error	O
results	O
from	O
choosing	O
a	O
single	O
neighbor	O
by	O
breaking	O
ties	O
between	O
equally	O
distant	O
neighbors	O
randomly	O
.	O
when	O
there	O
is	O
inﬁnite	O
training	O
data	O
,	O
all	O
test	O
points	O
x	O
will	O
have	O
inﬁnitely	O
many	O
training	O
set	O
neighbors	O
at	O
distance	O
zero	O
.	O
if	O
we	O
allow	O
the	O
algorithm	O
to	O
use	O
all	O
of	O
these	O
neighbors	O
to	O
vote	O
,	O
rather	O
than	O
randomly	O
choosing	O
one	O
of	O
them	O
,	O
the	O
procedure	O
converges	O
to	O
the	O
bayes	O
error	O
rate	O
.	O
the	O
high	O
capacity	O
of	O
k-nearest	B
neighbors	I
allows	O
it	O
to	O
obtain	O
high	O
accuracy	O
given	O
a	O
large	O
training	O
set	O
.	O
however	O
,	O
it	O
does	O
so	O
at	O
high	O
computational	O
cost	O
,	O
and	O
it	O
may	O
generalize	O
very	O
badly	O
given	O
a	O
small	O
,	O
ﬁnite	O
training	O
set	O
.	O
one	O
weakness	O
of	O
k-nearest	B
neighbors	I
is	O
that	O
it	O
can	O
not	O
learn	O
that	O
one	O
feature	O
is	O
more	O
discriminative	O
than	O
another	O
.	O
for	O
example	O
,	O
100	O
drawn	O
from	O
an	O
isotropic	O
gaussian	O
imagine	O
we	O
have	O
a	O
regression	O
task	O
with	O
x	O
∈	O
r	O
143	O
chapter	O
5.	O
machine	O
learning	O
basics	O
distribution	O
,	O
but	O
only	O
a	O
single	O
variable	O
x1	O
is	O
relevant	O
to	O
the	O
output	O
.	O
suppose	O
further	O
that	O
this	O
feature	O
simply	O
encodes	O
the	O
output	O
directly	O
,	O
i.e	O
.	O
that	O
y	O
=	O
x1	O
in	O
all	O
cases	O
.	O
nearest	O
neighbor	O
regression	O
will	O
not	O
be	O
able	O
to	O
detect	O
this	O
simple	O
pattern	O
.	O
the	O
nearest	O
neighbor	O
of	O
most	O
points	O
x	O
will	O
be	O
determined	O
by	O
the	O
large	O
number	O
of	O
features	O
x2	O
through	O
x100	O
,	O
not	O
by	O
the	O
lone	O
feature	O
x1	O
.	O
thus	O
the	O
output	O
on	O
small	O
training	O
sets	O
will	O
essentially	O
be	O
random	O
.	O
144	O
chapter	O
5.	O
machine	O
learning	O
basics	O
0	O
1	O
00	O
01	O
10	O
11	O
010	O
011	O
110	O
111	O
1110	O
1111	O
00	O
0	O
010	O
01	O
011	O
1	O
110	O
11	O
10	O
1110	O
111	O
1111	O
figure	O
5.7	O
:	O
diagrams	O
describing	O
how	O
a	O
decision	O
tree	O
works	O
.	O
(	O
top	O
)	O
each	O
node	O
of	O
the	O
tree	O
chooses	O
to	O
send	O
the	O
input	O
example	O
to	O
the	O
child	O
node	O
on	O
the	O
left	O
(	O
0	O
)	O
or	O
or	O
the	O
child	O
node	O
on	O
the	O
right	O
(	O
1	O
)	O
.	O
internal	O
nodes	O
are	O
drawn	O
as	O
circles	O
and	O
leaf	O
nodes	O
as	O
squares	O
.	O
each	O
node	O
is	O
displayed	O
with	O
a	O
binary	O
string	O
identiﬁer	O
corresponding	O
to	O
its	O
position	B
in	O
the	O
tree	O
,	O
obtained	O
by	O
appending	O
a	O
bit	O
to	O
its	O
parent	O
identiﬁer	O
(	O
0=choose	O
left	O
or	O
top	O
,	O
1=choose	O
right	O
or	O
bottom	O
)	O
.	O
(	O
bottom	O
)	O
the	O
tree	O
divides	O
space	O
into	O
regions	O
.	O
the	O
2d	O
plane	O
shows	O
how	O
a	O
decision	O
tree	O
might	O
divide	O
r	O
2.	O
the	O
nodes	O
of	O
the	O
tree	O
are	O
plotted	O
in	O
this	O
plane	O
,	O
with	O
each	O
internal	O
node	O
drawn	O
along	O
the	O
dividing	O
line	O
it	O
uses	O
to	O
categorize	O
examples	O
,	O
and	O
leaf	O
nodes	O
drawn	O
in	O
the	O
center	O
of	O
the	O
region	O
of	O
examples	O
they	O
receive	O
.	O
the	O
result	O
is	O
a	O
piecewise-constant	O
function	O
,	O
with	O
one	O
piece	O
per	O
leaf	O
.	O
each	O
leaf	O
requires	O
at	O
least	O
one	O
training	O
example	O
to	O
deﬁne	O
,	O
so	O
it	O
is	O
not	O
possible	O
for	O
the	O
decision	O
tree	O
to	O
learn	O
a	O
function	O
that	O
has	O
more	O
local	O
maxima	O
than	O
the	O
number	O
of	O
training	O
examples	O
.	O
145	O
chapter	O
5.	O
machine	O
learning	O
basics	O
5.7	O
another	O
type	O
of	O
learning	O
algorithm	O
that	O
also	O
breaks	O
the	O
input	O
space	O
into	O
regions	O
breiman	O
et	O
al	O
.	O
,	O
and	O
has	O
separate	O
parameters	O
for	O
each	O
region	O
is	O
the	O
decision	O
tree	O
(	O
1984	O
)	O
and	O
its	O
many	O
variants	O
.	O
as	O
shown	O
in	O
ﬁgure	O
,	O
each	O
node	O
of	O
the	O
decision	O
tree	O
is	O
associated	O
with	O
a	O
region	O
in	O
the	O
input	O
space	O
,	O
and	O
internal	O
nodes	O
break	O
that	O
region	O
into	O
one	O
sub-region	O
for	O
each	O
child	O
of	O
the	O
node	O
(	O
typically	O
using	O
an	O
axis-aligned	O
cut	O
)	O
.	O
space	O
is	O
thus	O
sub-divided	O
into	O
non-overlapping	O
regions	O
,	O
with	O
a	O
one-to-one	O
correspondence	O
between	O
leaf	O
nodes	O
and	O
input	O
regions	O
.	O
each	O
leaf	O
node	O
usually	O
maps	O
every	O
point	O
in	O
its	O
input	O
region	O
to	O
the	O
same	O
output	O
.	O
decision	O
trees	O
are	O
usually	O
trained	O
with	O
specialized	O
algorithms	O
that	O
are	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
.	O
the	O
learning	O
algorithm	O
can	O
be	O
considered	O
non-parametric	O
if	O
it	O
is	O
allowed	O
to	O
learn	O
a	O
tree	O
of	O
arbitrary	O
size	O
,	O
though	O
decision	O
trees	O
are	O
usually	O
regularized	O
with	O
size	O
constraints	O
that	O
turn	O
them	O
into	O
parametric	O
models	O
in	O
practice	O
.	O
decision	O
trees	O
as	O
they	O
are	O
typically	O
used	O
,	O
with	O
axis-aligned	O
splits	O
and	O
constant	O
outputs	O
within	O
each	O
node	O
,	O
struggle	O
to	O
solve	O
some	O
problems	O
that	O
are	O
easy	O
even	O
for	O
logistic	O
regression	O
.	O
for	O
example	O
,	O
if	O
we	O
have	O
a	O
two-class	O
problem	O
and	O
the	O
positive	O
class	O
occurs	O
wherever	O
x2	O
>	O
x1	O
,	O
the	O
decision	O
boundary	O
is	O
not	O
axis-aligned	O
.	O
the	O
decision	O
tree	O
will	O
thus	O
need	O
to	O
approximate	O
the	O
decision	O
boundary	O
with	O
many	O
nodes	O
,	O
implementing	O
a	O
step	O
function	O
that	O
constantly	O
walks	O
back	O
and	O
forth	O
across	O
the	O
true	O
decision	O
function	O
with	O
axis-aligned	O
steps	O
.	O
as	O
we	O
have	O
seen	O
,	O
nearest	O
neighbor	O
predictors	O
and	O
decision	O
trees	O
have	O
many	O
limitations	O
.	O
nonetheless	O
,	O
they	O
are	O
useful	O
learning	O
algorithms	O
when	O
computational	O
resources	O
are	O
constrained	O
.	O
we	O
can	O
also	O
build	O
intuition	O
for	O
more	O
sophisticated	O
learning	O
algorithms	O
by	O
thinking	O
about	O
the	O
similarities	O
and	O
diﬀerences	O
between	O
sophisticated	O
algorithms	O
and	O
-nn	O
or	O
decision	O
tree	O
baselines	O
.	O
k	O
see	O
murphy	O
2012	O
bishop	O
2006	O
hastie	O
et	O
al	O
.	O
2001	O
)	O
or	O
other	O
machine	O
learning	O
textbooks	O
for	O
more	O
material	O
on	O
traditional	O
supervised	O
learning	O
algorithms	O
.	O
(	O
)	O
,	O
(	O
)	O
,	O
(	O
5.8	O
unsupervised	O
learning	O
algorithms	O
5.1.3	O
recall	O
from	O
section	O
that	O
unsupervised	O
algorithms	O
are	O
those	O
that	O
experience	O
only	O
“	O
features	O
”	O
but	O
not	O
a	O
supervision	O
signal	O
.	O
the	O
distinction	O
between	O
supervised	O
and	O
unsupervised	O
algorithms	O
is	O
not	O
formally	O
and	O
rigidly	O
deﬁned	O
because	O
there	O
is	O
no	O
objective	O
test	O
for	O
distinguishing	O
whether	O
a	O
value	O
is	O
a	O
feature	O
or	O
a	O
target	O
provided	O
by	O
a	O
supervisor	O
.	O
informally	O
,	O
unsupervised	O
learning	O
refers	O
to	O
most	O
attempts	O
to	O
extract	O
information	O
from	O
a	O
distribution	O
that	O
do	O
not	O
require	O
human	O
labor	O
to	O
annotate	O
examples	O
.	O
the	O
term	O
is	O
usually	O
associated	O
with	O
density	O
estimation	O
,	O
learning	O
to	O
draw	O
samples	O
from	O
a	O
distribution	O
,	O
learning	O
to	O
denoise	O
data	O
from	O
some	O
distribution	O
,	O
ﬁnding	O
a	O
manifold	O
that	O
the	O
data	O
lies	O
near	O
,	O
or	O
clustering	O
the	O
data	O
into	O
groups	O
of	O
146	O
chapter	O
5.	O
machine	O
learning	O
basics	O
related	O
examples	O
.	O
a	O
classic	O
unsupervised	O
learning	O
task	O
is	O
to	O
ﬁnd	O
the	O
“	O
best	O
”	O
representation	O
of	O
the	O
data	O
.	O
by	O
‘	O
best	O
’	O
we	O
can	O
mean	O
diﬀerent	O
things	O
,	O
but	O
generally	O
speaking	O
we	O
are	O
looking	O
for	O
a	O
representation	O
that	O
preserves	O
as	O
much	O
information	O
about	O
x	O
as	O
possible	O
while	O
obeying	O
some	O
penalty	O
or	O
constraint	O
aimed	O
at	O
keeping	O
the	O
representation	O
or	O
more	O
accessible	O
than	O
simpler	O
itself	O
.	O
x	O
,	O
,	O
simpler	O
there	O
are	O
multiple	O
ways	O
of	O
deﬁning	O
a	O
representation	O
.	O
three	O
of	O
the	O
most	O
common	O
include	O
lower	O
dimensional	O
representations	O
,	O
sparse	O
representations	O
and	O
independent	O
representations	O
.	O
low-dimensional	O
representations	O
attempt	O
to	O
compress	O
as	O
much	O
information	O
about	O
x	O
as	O
possible	O
in	O
a	O
smaller	O
representation	O
.	O
sparse	O
representations	O
(	O
barlow	O
1989	O
olshausen	O
and	O
field	O
1996	O
hinton	O
and	O
ghahramani	O
1997	O
)	O
embed	O
the	O
dataset	O
into	O
a	O
representation	O
whose	O
entries	O
are	O
mostly	O
zeroes	O
for	O
most	O
inputs	O
.	O
the	O
use	O
of	O
sparse	O
representations	O
typically	O
requires	O
increasing	O
the	O
dimensionality	O
of	O
the	O
representation	O
,	O
so	O
that	O
the	O
representation	O
becoming	O
mostly	O
zeroes	O
does	O
not	O
discard	O
too	O
much	O
information	O
.	O
this	O
results	O
in	O
an	O
overall	O
structure	O
of	O
the	O
representation	O
that	O
tends	O
to	O
distribute	O
data	O
along	O
the	O
axes	O
of	O
the	O
representation	O
space	O
.	O
independent	O
representations	O
attempt	O
to	O
disentangle	O
the	O
sources	O
of	O
variation	O
underlying	O
the	O
data	O
distribution	O
such	O
that	O
the	O
dimensions	O
of	O
the	O
representation	O
are	O
statistically	O
independent	O
.	O
;	O
,	O
;	O
of	O
course	O
these	O
three	O
criteria	O
are	O
certainly	O
not	O
mutually	O
exclusive	O
.	O
low-	O
dimensional	O
representations	O
often	O
yield	O
elements	O
that	O
have	O
fewer	O
or	O
weaker	O
de-	O
pendencies	O
than	O
the	O
original	O
high-dimensional	O
data	O
.	O
this	O
is	O
because	O
one	O
way	O
to	O
reduce	O
the	O
size	O
of	O
a	O
representation	O
is	O
to	O
ﬁnd	O
and	O
remove	O
redundancies	O
.	O
identifying	O
and	O
removing	O
more	O
redundancy	O
allows	O
the	O
dimensionality	O
reduction	O
algorithm	O
to	O
achieve	O
more	O
compression	O
while	O
discarding	O
less	O
information	O
.	O
the	O
notion	O
of	O
representation	O
is	O
one	O
of	O
the	O
central	O
themes	O
of	O
deep	O
learning	O
and	O
therefore	O
one	O
of	O
the	O
central	O
themes	O
in	O
this	O
book	O
.	O
in	O
this	O
section	O
,	O
we	O
develop	O
some	O
simple	O
examples	O
of	O
representation	O
learning	O
algorithms	O
.	O
together	O
,	O
these	O
example	O
algorithms	O
show	O
how	O
to	O
operationalize	O
all	O
three	O
of	O
the	O
criteria	O
above	O
.	O
most	O
of	O
the	O
remaining	O
chapters	O
introduce	O
additional	O
representation	O
learning	O
algorithms	O
that	O
develop	O
these	O
criteria	O
in	O
diﬀerent	O
ways	O
or	O
introduce	O
other	O
criteria	O
.	O
5.8.1	O
principal	O
components	O
analysis	O
2.12	O
in	O
section	O
,	O
we	O
saw	O
that	O
the	O
principal	O
components	O
analysis	O
algorithm	O
provides	O
a	O
means	O
of	O
compressing	O
data	O
.	O
we	O
can	O
also	O
view	O
pca	O
as	O
an	O
unsupervised	O
learning	O
algorithm	O
that	O
learns	O
a	O
representation	O
of	O
data	O
.	O
this	O
representation	O
is	O
based	O
on	O
two	O
of	O
the	O
criteria	O
for	O
a	O
simple	O
representation	O
described	O
above	O
.	O
pca	O
learns	O
a	O
147	O
chapter	O
5.	O
machine	O
learning	O
basics	O
20	O
10	O
0	O
−	O
10	O
−	O
20	O
2	O
x	O
20	O
10	O
0	O
−	O
10	O
−	O
20	O
2	O
z	O
−	O
20	O
−	O
10	O
10	O
20	O
0	O
z1	O
−	O
20	O
−	O
10	O
10	O
20	O
0	O
x1	O
figure	O
5.8	O
:	O
pca	O
learns	O
a	O
linear	O
projection	O
that	O
aligns	O
the	O
direction	O
of	O
greatest	O
variance	O
with	O
the	O
axes	O
of	O
the	O
new	O
space	O
.	O
(	O
left	O
)	O
the	O
original	O
data	O
consists	O
of	O
samples	O
of	O
x	O
.	O
in	O
this	O
space	O
,	O
the	O
variance	O
might	O
occur	O
along	O
directions	O
that	O
are	O
not	O
axis-aligned	O
.	O
(	O
right	O
)	O
the	O
transformed	O
data	O
z	O
=	O
x	O
w	O
now	O
varies	O
most	O
along	O
the	O
axis	O
z1	O
.	O
the	O
direction	O
of	O
second	O
most	O
variance	O
is	O
now	O
along	O
z2	O
.	O
	O
representation	O
that	O
has	O
lower	O
dimensionality	O
than	O
the	O
original	O
input	O
.	O
it	O
also	O
learns	O
a	O
representation	O
whose	O
elements	O
have	O
no	O
linear	O
correlation	O
with	O
each	O
other	O
.	O
this	O
is	O
a	O
ﬁrst	O
step	O
toward	O
the	O
criterion	O
of	O
learning	O
representations	O
whose	O
elements	O
are	O
statistically	O
independent	O
.	O
to	O
achieve	O
full	O
independence	O
,	O
a	O
representation	O
learning	O
algorithm	O
must	O
also	O
remove	O
the	O
nonlinear	O
relationships	O
between	O
variables	O
.	O
5.8	O
2.12	O
pca	O
learns	O
an	O
orthogonal	O
,	O
linear	O
transformation	O
of	O
the	O
data	O
that	O
projects	O
an	O
input	O
x	O
to	O
a	O
representation	O
z	O
as	O
shown	O
in	O
ﬁgure	O
,	O
we	O
saw	O
that	O
we	O
could	O
learn	O
a	O
one-dimensional	O
representation	O
that	O
best	O
reconstructs	O
the	O
original	O
data	O
(	O
in	O
the	O
sense	O
of	O
mean	O
squared	O
error	O
)	O
and	O
that	O
this	O
representation	O
actually	O
corresponds	O
to	O
the	O
ﬁrst	O
principal	O
component	O
of	O
the	O
data	O
.	O
thus	O
we	O
can	O
use	O
pca	O
as	O
a	O
simple	O
and	O
eﬀective	O
dimensionality	O
reduction	O
method	O
that	O
preserves	O
as	O
much	O
of	O
the	O
information	O
in	O
the	O
data	O
as	O
possible	O
(	O
again	O
,	O
as	O
measured	O
by	O
least-squares	O
reconstruction	O
error	O
)	O
.	O
in	O
the	O
following	O
,	O
we	O
will	O
study	O
how	O
the	O
pca	O
representation	O
decorrelates	O
the	O
original	O
data	O
representation	O
.	O
in	O
section	O
.x	O
×	O
let	O
us	O
consider	O
the	O
m	O
n	O
-dimensional	O
design	O
matrix	O
x.	O
we	O
will	O
assume	O
that	O
the	O
data	O
has	O
a	O
mean	O
of	O
zero	O
,	O
e	O
[	O
x	O
]	O
=	O
0.	O
if	O
this	O
is	O
not	O
the	O
case	O
,	O
the	O
data	O
can	O
easily	O
be	O
centered	O
by	O
subtracting	O
the	O
mean	O
from	O
all	O
examples	O
in	O
a	O
preprocessing	O
step	O
.	O
the	O
unbiased	O
sample	O
covariance	O
matrix	O
associated	O
with	O
x	O
is	O
given	O
by	O
:	O
var	O
[	O
]	O
=x	O
−	O
1	O
x	O
1	O
m	O
	O
x	O
.	O
148	O
(	O
5.85	O
)	O
chapter	O
5.	O
machine	O
learning	O
basics	O
	O
pca	O
ﬁnds	O
a	O
representation	O
(	O
through	O
linear	O
transformation	O
)	O
z	O
=	O
x	O
var	O
[	O
]	O
z	O
is	O
diagonal	O
.	O
w	O
where	O
in	O
section	O
2.12	O
,	O
we	O
saw	O
that	O
the	O
principal	O
components	O
of	O
a	O
design	O
matrix	O
x	O
	O
are	O
given	O
by	O
the	O
eigenvectors	O
of	O
x	O
	O
x.	O
from	O
this	O
view	O
,	O
	O
x	O
x	O
w	O
w	O
=	O
λ	O
.	O
(	O
5.86	O
)	O
in	O
this	O
section	O
,	O
we	O
exploit	O
an	O
alternative	O
derivation	O
of	O
the	O
principal	O
components	O
.	O
the	O
principal	O
components	O
may	O
also	O
be	O
obtained	O
via	O
the	O
singular	O
value	O
decomposition	O
.	O
speciﬁcally	O
,	O
they	O
are	O
the	O
right	O
singular	O
vectors	O
of	O
x	O
.	O
to	O
see	O
this	O
,	O
let	O
w	O
be	O
the	O
right	O
singular	O
vectors	O
in	O
the	O
decomposition	O
x	O
=	O
u	O
wς	O
.	O
we	O
then	O
recover	O
the	O
original	O
eigenvector	O
equation	O
with	O
	O
	O
	O
	O
x	O
x	O
=	O
u	O
wς	O
w	O
	O
as	O
the	O
eigenvector	O
basis	O
:	O
	O
	O
	O
u	O
wς	O
=	O
w	O
σ2w	O
.	O
(	O
5.87	O
)	O
the	O
svd	O
is	O
helpful	O
to	O
show	O
that	O
pca	O
results	O
in	O
a	O
diagonal	O
var	O
[	O
z	O
]	O
.	O
using	O
the	O
svd	O
of	O
x	O
,	O
we	O
can	O
express	O
the	O
variance	O
of	O
	O
x	O
]	O
=x	O
var	O
[	O
x	O
as	O
:	O
x	O
−	O
1	O
−	O
1	O
−	O
1	O
−	O
1	O
m	O
m	O
m	O
m	O
1	O
1	O
1	O
1	O
=	O
=	O
=	O
	O
(	O
u	O
wς	O
	O
w	O
σ	O
u	O
	O
	O
)	O
	O
u	O
wς	O
	O
u	O
wς	O
	O
w	O
σ2	O
w	O
,	O
	O
(	O
5.88	O
)	O
(	O
5.89	O
)	O
(	O
5.90	O
)	O
(	O
5.91	O
)	O
is	O
diagonal	O
as	O
required	O
:	O
u	O
=	O
i	O
because	O
the	O
u	O
matrix	O
of	O
the	O
singular	O
value	O
w	O
,	O
where	O
we	O
use	O
the	O
fact	O
that	O
u	O
	O
decomposition	O
is	O
deﬁned	O
to	O
be	O
orthogonal	O
.	O
this	O
shows	O
that	O
if	O
we	O
take	O
z	O
=	O
x	O
we	O
can	O
ensure	O
that	O
the	O
covariance	O
of	O
z	O
−	O
1	O
−	O
1	O
−	O
1	O
−	O
1	O
xw	O
	O
w	O
	O
z	O
	O
w	O
σ	O
2w	O
(	O
5.93	O
)	O
(	O
5.94	O
)	O
(	O
5.92	O
)	O
]	O
=z	O
var	O
[	O
w	O
w	O
x	O
m	O
1	O
1	O
m	O
m	O
=	O
=	O
z	O
	O
	O
=	O
1	O
1	O
m	O
σ2	O
,	O
	O
w	O
=	O
i	O
,	O
again	O
from	O
the	O
deﬁnition	O
of	O
the	O
(	O
5.95	O
)	O
where	O
this	O
time	O
we	O
use	O
the	O
fact	O
that	O
w	O
svd	O
.	O
149	O
chapter	O
5.	O
machine	O
learning	O
basics	O
the	O
above	O
analysis	O
shows	O
that	O
when	O
we	O
project	O
the	O
data	O
x	O
to	O
z	O
,	O
via	O
the	O
linear	O
transformation	O
w	O
,	O
the	O
resulting	O
representation	O
has	O
a	O
diagonal	O
covariance	O
matrix	O
(	O
as	O
given	O
by	O
σ2	O
)	O
which	O
immediately	O
implies	O
that	O
the	O
individual	O
elements	O
of	O
z	O
are	O
mutually	O
uncorrelated	O
.	O
this	O
ability	O
of	O
pca	O
to	O
transform	O
data	O
into	O
a	O
representation	O
where	O
the	O
elements	O
are	O
mutually	O
uncorrelated	O
is	O
a	O
very	O
important	O
property	O
of	O
pca	O
.	O
it	O
is	O
a	O
simple	O
example	O
of	O
a	O
representation	O
that	O
attempts	O
to	O
disentangle	O
the	O
unknown	O
factors	O
of	O
variation	O
underlying	O
the	O
data	O
.	O
in	O
the	O
case	O
of	O
pca	O
,	O
this	O
disentangling	O
takes	O
the	O
form	O
of	O
ﬁnding	O
a	O
rotation	O
of	O
the	O
input	O
space	O
(	O
described	O
by	O
w	O
)	O
that	O
aligns	O
the	O
principal	O
axes	O
of	O
variance	O
with	O
the	O
basis	O
of	O
the	O
new	O
representation	O
space	O
associated	O
with	O
.z	O
while	O
correlation	O
is	O
an	O
important	O
category	O
of	O
dependency	O
between	O
elements	O
of	O
the	O
data	O
,	O
we	O
are	O
also	O
interested	O
in	O
learning	O
representations	O
that	O
disentangle	O
more	O
complicated	O
forms	O
of	O
feature	O
dependencies	O
.	O
for	O
this	O
,	O
we	O
will	O
need	O
more	O
than	O
what	O
can	O
be	O
done	O
with	O
a	O
simple	O
linear	O
transformation	O
.	O
5.8.2	O
k	O
-means	O
clustering	O
another	O
example	O
of	O
a	O
simple	O
representation	O
learning	O
algorithm	O
is	O
k	O
-means	O
clustering	O
.	O
the	O
k-means	B
clustering	O
algorithm	O
divides	O
the	O
training	O
set	O
into	O
k	O
diﬀerent	O
clusters	O
of	O
examples	O
that	O
are	O
near	O
each	O
other	O
.	O
we	O
can	O
thus	O
think	O
of	O
the	O
algorithm	O
as	O
providing	O
a	O
k-dimensional	O
one-hot	O
code	O
vector	O
h	O
representing	O
an	O
input	O
x.	O
if	O
x	O
belongs	O
to	O
cluster	O
i	O
,	O
then	O
h	O
i	O
=	O
1	O
and	O
all	O
other	O
entries	O
of	O
the	O
representation	O
h	O
are	O
zero	O
.	O
the	O
one-hot	O
code	O
provided	O
by	O
k-means	B
clustering	O
is	O
an	O
example	O
of	O
a	O
sparse	O
representation	O
,	O
because	O
the	O
majority	O
of	O
its	O
entries	O
are	O
zero	O
for	O
every	O
input	O
.	O
later	O
,	O
we	O
will	O
develop	O
other	O
algorithms	O
that	O
learn	O
more	O
ﬂexible	O
sparse	O
representations	O
,	O
where	O
more	O
than	O
one	O
entry	O
can	O
be	O
non-zero	O
for	O
each	O
input	O
x.	O
one-hot	O
codes	O
are	O
an	O
extreme	O
example	O
of	O
sparse	O
representations	O
that	O
lose	O
many	O
of	O
the	O
beneﬁts	O
of	O
a	O
distributed	O
representation	O
.	O
the	O
one-hot	O
code	O
still	O
confers	O
some	O
statistical	O
advantages	O
(	O
it	O
naturally	O
conveys	O
the	O
idea	O
that	O
all	O
examples	O
in	O
the	O
same	O
cluster	O
are	O
similar	O
to	O
each	O
other	O
)	O
and	O
it	O
confers	O
the	O
computational	O
advantage	O
that	O
the	O
entire	O
representation	O
may	O
be	O
captured	O
by	O
a	O
single	O
integer.	O
}	O
{	O
µ	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
µ	O
(	O
)	O
k	O
to	O
diﬀerent	O
values	O
,	O
then	O
alternating	O
between	O
two	O
diﬀerent	O
steps	O
until	O
convergence	O
.	O
in	O
one	O
step	O
,	O
each	O
training	O
example	O
is	O
assigned	O
to	O
cluster	O
i	O
,	O
where	O
i	O
is	O
the	O
index	O
of	O
the	O
nearest	O
centroid	O
µ	O
(	O
)	O
i	O
.	O
in	O
the	O
other	O
step	O
,	O
each	O
centroid	O
µ	O
(	O
)	O
i	O
is	O
updated	O
to	O
the	O
mean	O
of	O
all	O
training	O
examples	O
x	O
(	O
)	O
j	O
assigned	O
to	O
cluster	O
.i	O
the	O
k-means	B
algorithm	O
works	O
by	O
initializing	O
k	O
diﬀerent	O
centroids	O
150	O
chapter	O
5.	O
machine	O
learning	O
basics	O
one	O
diﬃculty	O
pertaining	O
to	O
clustering	O
is	O
that	O
the	O
clustering	O
problem	O
is	O
inherently	O
ill-posed	O
,	O
in	O
the	O
sense	O
that	O
there	O
is	O
no	O
single	O
criterion	O
that	O
measures	O
how	O
well	O
a	O
clustering	O
of	O
the	O
data	O
corresponds	O
to	O
the	O
real	O
world	O
.	O
we	O
can	O
measure	O
properties	O
of	O
the	O
clustering	O
such	O
as	O
the	O
average	O
euclidean	O
distance	O
from	O
a	O
cluster	O
centroid	O
to	O
the	O
members	O
of	O
the	O
cluster	O
.	O
this	O
allows	O
us	O
to	O
tell	O
how	O
well	O
we	O
are	O
able	O
to	O
reconstruct	O
the	O
training	O
data	O
from	O
the	O
cluster	O
assignments	O
.	O
we	O
do	O
not	O
know	O
how	O
well	O
the	O
cluster	O
assignments	O
correspond	O
to	O
properties	O
of	O
the	O
real	O
world	O
.	O
moreover	O
,	O
there	O
may	O
be	O
many	O
diﬀerent	O
clusterings	O
that	O
all	O
correspond	O
well	O
to	O
some	O
property	O
of	O
the	O
real	O
world	O
.	O
we	O
may	O
hope	O
to	O
ﬁnd	O
a	O
clustering	O
that	O
relates	O
to	O
one	O
feature	O
but	O
obtain	O
a	O
diﬀerent	O
,	O
equally	O
valid	O
clustering	O
that	O
is	O
not	O
relevant	O
to	O
our	O
task	O
.	O
for	O
example	O
,	O
suppose	O
that	O
we	O
run	O
two	O
clustering	O
algorithms	O
on	O
a	O
dataset	O
consisting	O
of	O
images	O
of	O
red	O
trucks	O
,	O
images	O
of	O
red	O
cars	O
,	O
images	O
of	O
gray	O
trucks	O
,	O
and	O
images	O
of	O
gray	O
cars	O
.	O
if	O
we	O
ask	O
each	O
clustering	O
algorithm	O
to	O
ﬁnd	O
two	O
clusters	O
,	O
one	O
algorithm	O
may	O
ﬁnd	O
a	O
cluster	O
of	O
cars	O
and	O
a	O
cluster	O
of	O
trucks	O
,	O
while	O
another	O
may	O
ﬁnd	O
a	O
cluster	O
of	O
red	O
vehicles	O
and	O
a	O
cluster	O
of	O
gray	O
vehicles	O
.	O
suppose	O
we	O
also	O
run	O
a	O
third	O
clustering	O
algorithm	O
,	O
which	O
is	O
allowed	O
to	O
determine	O
the	O
number	O
of	O
clusters	O
.	O
this	O
may	O
assign	O
the	O
examples	O
to	O
four	O
clusters	O
,	O
red	O
cars	O
,	O
red	O
trucks	O
,	O
gray	O
cars	O
,	O
and	O
gray	O
trucks	O
.	O
this	O
new	O
clustering	O
now	O
at	O
least	O
captures	O
information	O
about	O
both	O
attributes	O
,	O
but	O
it	O
has	O
lost	O
information	O
about	O
similarity	O
.	O
red	O
cars	O
are	O
in	O
a	O
diﬀerent	O
cluster	O
from	O
gray	O
cars	O
,	O
just	O
as	O
they	O
are	O
in	O
a	O
diﬀerent	O
cluster	O
from	O
gray	O
trucks	O
.	O
the	O
output	O
of	O
the	O
clustering	O
algorithm	O
does	O
not	O
tell	O
us	O
that	O
red	O
cars	O
are	O
more	O
similar	O
to	O
gray	O
cars	O
than	O
they	O
are	O
to	O
gray	O
trucks	O
.	O
they	O
are	O
diﬀerent	O
from	O
both	O
things	O
,	O
and	O
that	O
is	O
all	O
we	O
know	O
.	O
these	O
issues	O
illustrate	O
some	O
of	O
the	O
reasons	O
that	O
we	O
may	O
prefer	O
a	O
distributed	O
representation	O
to	O
a	O
one-hot	O
representation	O
.	O
a	O
distributed	O
representation	O
could	O
have	O
two	O
attributes	O
for	O
each	O
vehicle—one	O
representing	O
its	O
color	O
and	O
one	O
representing	O
whether	O
it	O
is	O
a	O
car	O
or	O
a	O
truck	O
.	O
it	O
is	O
still	O
not	O
entirely	O
clear	O
what	O
the	O
optimal	O
distributed	O
representation	O
is	O
(	O
how	O
can	O
the	O
learning	O
algorithm	O
know	O
whether	O
the	O
two	O
attributes	O
we	O
are	O
interested	O
in	O
are	O
color	O
and	O
car-versus-truck	O
rather	O
than	O
manufacturer	O
and	O
age	O
?	O
)	O
but	O
having	O
many	O
attributes	O
reduces	O
the	O
burden	O
on	O
the	O
algorithm	O
to	O
guess	O
which	O
single	O
attribute	O
we	O
care	O
about	O
,	O
and	O
allows	O
us	O
to	O
measure	O
similarity	O
between	O
objects	O
in	O
a	O
ﬁne-grained	O
way	O
by	O
comparing	O
many	O
attributes	O
instead	O
of	O
just	O
testing	O
whether	O
one	O
attribute	O
matches	O
.	O
5.9	O
stochastic	O
gradient	O
descent	B
nearly	O
all	O
of	O
deep	O
learning	O
is	O
powered	O
by	O
one	O
very	O
important	O
algorithm	O
:	O
stochastic	O
gradient	O
descent	B
or	O
sgd	O
.	O
stochastic	O
gradient	O
descent	B
is	O
an	O
extension	O
of	O
the	O
151	O
chapter	O
5.	O
machine	O
learning	O
basics	O
gradient	O
descent	B
algorithm	O
introduced	O
in	O
section	O
.4.3	O
a	O
recurring	O
problem	O
in	O
machine	O
learning	O
is	O
that	O
large	O
training	O
sets	O
are	O
necessary	O
for	O
good	O
generalization	O
,	O
but	O
large	O
training	O
sets	O
are	O
also	O
more	O
computationally	O
expensive	O
.	O
the	O
cost	O
function	O
used	O
by	O
a	O
machine	O
learning	O
algorithm	O
often	O
decomposes	O
as	O
a	O
sum	O
over	O
training	O
examples	O
of	O
some	O
per-example	O
loss	O
function	O
.	O
for	O
example	O
,	O
the	O
negative	O
conditional	O
log-likelihood	O
of	O
the	O
training	O
data	O
can	O
be	O
written	O
as	O
	O
m	O
i=1	O
j	O
(	O
)	O
=	O
θ	O
∼	O
ex	O
,	O
y	O
ˆpdata	O
where	O
l	O
is	O
the	O
per-example	O
loss	O
θ	O
)	O
=	O
1	O
m	O
−	O
θ	O
)	O
=	O
	O
l	O
,	O
y	O
,	O
(	O
x	O
log	O
p	O
y	O
(	O
|	O
x	O
θ	O
;	O
)	O
.	O
l	O
,	O
y	O
,	O
(	O
x	O
l	O
(	O
x	O
(	O
)	O
i	O
,	O
y	O
(	O
)	O
i	O
,	O
θ	O
)	O
(	O
5.96	O
)	O
for	O
these	O
additive	O
cost	O
functions	O
,	O
gradient	O
descent	B
requires	O
computing	O
∇	O
θ	O
j	O
(	O
)	O
=θ	O
1	O
m	O
m	O
i=1	O
∇	O
θl	O
(	O
x	O
(	O
)	O
i	O
,	O
y	O
(	O
)	O
i	O
,	O
.θ	O
)	O
(	O
5.97	O
)	O
the	O
computational	O
cost	O
of	O
this	O
operation	O
is	O
o	O
(	O
m	O
)	O
.	O
as	O
the	O
training	O
set	O
size	O
grows	O
to	O
billions	O
of	O
examples	O
,	O
the	O
time	O
to	O
take	O
a	O
single	O
gradient	O
step	O
becomes	O
prohibitively	O
long	O
.	O
	O
}	O
{	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
m	O
the	O
insight	O
of	O
stochastic	O
gradient	O
descent	B
is	O
that	O
the	O
gradient	O
is	O
an	O
expectation	O
.	O
the	O
expectation	O
may	O
be	O
approximately	O
estimated	O
using	O
a	O
small	O
set	O
of	O
samples	O
.	O
speciﬁcally	O
,	O
on	O
each	O
step	O
of	O
the	O
algorithm	O
,	O
we	O
can	O
sample	O
a	O
minibatch	O
of	O
examples	O
b	O
=	O
drawn	O
uniformly	O
from	O
the	O
training	O
set	O
.	O
the	O
minibatch	O
size	O
	O
m	O
is	O
typically	O
chosen	O
to	O
be	O
a	O
relatively	O
small	O
number	O
of	O
examples	O
,	O
ranging	O
from	O
	O
1	O
to	O
a	O
few	O
hundred	O
.	O
crucially	O
,	O
m	O
is	O
usually	O
held	O
ﬁxed	O
as	O
the	O
training	O
set	O
size	O
m	O
grows	O
.	O
we	O
may	O
ﬁt	O
a	O
training	O
set	O
with	O
billions	O
of	O
examples	O
using	O
updates	O
computed	O
on	O
only	O
a	O
hundred	O
examples.	O
)	O
the	O
estimate	O
of	O
the	O
gradient	O
is	O
formed	O
as	O
g	O
=	O
∇	O
θ	O
1	O
	O
m	O
	O
m	O
i=1	O
l	O
(	O
x	O
(	O
)	O
i	O
,	O
y	O
(	O
)	O
i	O
,	O
.θ	O
)	O
(	O
5.98	O
)	O
using	O
examples	O
from	O
the	O
minibatch	O
.	O
the	O
stochastic	O
gradient	O
descent	B
algorithm	O
then	O
follows	O
the	O
estimated	O
gradient	O
downhill	O
:	O
b	O
←	O
−	O
θ	O
θ	O
g	O
	O
,	O
(	O
5.99	O
)	O
where	O
	O
is	O
the	O
learning	O
rate	O
.	O
152	O
	O
chapter	O
5.	O
machine	O
learning	O
basics	O
gradient	O
descent	B
in	O
general	O
has	O
often	O
been	O
regarded	O
as	O
slow	O
or	O
unreliable	O
.	O
in	O
the	O
past	O
,	O
the	O
application	O
of	O
gradient	O
descent	B
to	O
non-convex	O
optimization	O
problems	O
was	O
regarded	O
as	O
foolhardy	O
or	O
unprincipled	O
.	O
today	O
,	O
we	O
know	O
that	O
the	O
machine	O
learning	O
models	O
described	O
in	O
part	O
work	B
very	O
well	O
when	O
trained	O
with	O
gradient	O
descent	B
.	O
the	O
optimization	O
algorithm	O
may	O
not	O
be	O
guaranteed	O
to	O
arrive	O
at	O
even	O
a	O
local	O
minimum	O
in	O
a	O
reasonable	O
amount	O
of	O
time	O
,	O
but	O
it	O
often	O
ﬁnds	O
a	O
very	O
low	O
value	O
of	O
the	O
cost	O
function	O
quickly	O
enough	O
to	O
be	O
useful	O
.	O
ii	O
stochastic	O
gradient	O
descent	B
has	O
many	O
important	O
uses	O
outside	O
the	O
context	O
of	O
deep	O
learning	O
.	O
it	O
is	O
the	O
main	O
way	O
to	O
train	O
large	O
linear	O
models	O
on	O
very	O
large	O
datasets	O
.	O
for	O
a	O
ﬁxed	O
model	B
size	O
,	O
the	O
cost	O
per	O
sgd	O
update	O
does	O
not	O
depend	O
on	O
the	O
training	O
set	O
size	O
m.	O
in	O
practice	O
,	O
we	O
often	O
use	O
a	O
larger	O
model	B
as	O
the	O
training	O
set	O
size	O
increases	O
,	O
but	O
we	O
are	O
not	O
forced	O
to	O
do	O
so	O
.	O
the	O
number	O
of	O
updates	O
required	O
to	O
reach	O
convergence	O
usually	O
increases	O
with	O
training	O
set	O
size	O
.	O
however	O
,	O
as	O
m	O
approaches	O
inﬁnity	O
,	O
the	O
model	B
will	O
eventually	O
converge	O
to	O
its	O
best	O
possible	O
test	O
error	O
before	O
sgd	O
has	O
sampled	O
every	O
example	O
in	O
the	O
training	O
set	O
.	O
increasing	O
m	O
further	O
will	O
not	O
extend	O
the	O
amount	O
of	O
training	O
time	O
needed	O
to	O
reach	O
the	O
model	B
’	O
s	O
best	O
possible	O
test	O
error	O
.	O
from	O
this	O
point	O
of	O
view	O
,	O
one	O
can	O
argue	O
that	O
the	O
asymptotic	O
cost	O
of	O
training	O
a	O
model	B
with	O
sgd	O
is	O
as	O
a	O
function	O
of	O
o	O
(	O
1	O
)	O
.	O
m	O
×	O
prior	O
to	O
the	O
advent	O
of	O
deep	O
learning	O
,	O
the	O
main	O
way	O
to	O
learn	O
nonlinear	O
models	O
was	O
to	O
use	O
the	O
kernel	O
trick	B
in	O
combination	O
with	O
a	O
linear	O
model	B
.	O
many	O
kernel	O
learning	O
matrix	O
gi	O
,	O
j	O
=	O
k	O
(	O
x	O
(	O
)	O
i	O
,	O
x	O
(	O
)	O
j	O
)	O
.	O
constructing	O
algorithms	O
require	O
constructing	O
an	O
m	O
m	O
this	O
matrix	O
has	O
computational	O
cost	O
o	O
(	O
m2	O
)	O
,	O
which	O
is	O
clearly	O
undesirable	O
for	O
datasets	O
with	O
billions	O
of	O
examples	O
.	O
in	O
academia	O
,	O
starting	O
in	O
2006	O
,	O
deep	O
learning	O
was	O
initially	O
interesting	O
because	O
it	O
was	O
able	O
to	O
generalize	O
to	O
new	O
examples	O
better	O
than	O
competing	O
algorithms	O
when	O
trained	O
on	O
medium-sized	O
datasets	O
with	O
tens	O
of	O
thousands	O
of	O
examples	O
.	O
soon	O
after	O
,	O
deep	O
learning	O
garnered	O
additional	O
interest	O
in	O
industry	O
,	O
because	O
it	O
provided	O
a	O
scalable	O
way	O
of	O
training	O
nonlinear	O
models	O
on	O
large	O
datasets	O
.	O
stochastic	O
gradient	O
descent	B
and	O
many	O
enhancements	O
to	O
it	O
are	O
described	O
further	O
in	O
chapter	O
.8	O
5.10	O
building	O
a	O
machine	O
learning	O
algorithm	O
nearly	O
all	O
deep	O
learning	O
algorithms	O
can	O
be	O
described	O
as	O
particular	O
instances	O
of	O
a	O
fairly	O
simple	O
recipe	O
:	O
combine	O
a	O
speciﬁcation	O
of	O
a	O
dataset	O
,	O
a	O
cost	O
function	O
,	O
an	O
optimization	O
procedure	O
and	O
a	O
model	B
.	O
for	O
example	O
,	O
the	O
linear	O
regression	O
algorithm	O
combines	O
a	O
dataset	O
consisting	O
of	O
153	O
chapter	O
5.	O
machine	O
learning	O
basics	O
x	O
and	O
,	O
the	O
cost	O
function	O
y	O
j	O
−	O
(	O
w	O
)	O
=	O
|	O
,	O
b	O
∼	O
ex	O
,	O
y	O
ˆpdata	O
n	O
log	O
pmodel	O
(	O
y	O
	O
|	O
)	O
x	O
,	O
(	O
5.100	O
)	O
the	O
model	B
speciﬁcation	O
pmodel	O
(	O
y	O
w	O
+	O
b	O
,	O
1	O
)	O
,	O
and	O
,	O
in	O
most	O
cases	O
,	O
the	O
optimization	O
algorithm	O
deﬁned	O
by	O
solving	O
for	O
where	O
the	O
gradient	O
of	O
the	O
cost	O
is	O
zero	O
using	O
the	O
normal	O
equations	O
.	O
x	O
)	O
=	O
(	O
y	O
;	O
x	O
by	O
realizing	O
that	O
we	O
can	O
replace	O
any	O
of	O
these	O
components	O
mostly	O
independently	O
from	O
the	O
others	O
,	O
we	O
can	O
obtain	O
a	O
very	O
wide	O
variety	O
of	O
algorithms	O
.	O
the	O
cost	O
function	O
typically	O
includes	O
at	O
least	O
one	O
term	O
that	O
causes	O
the	O
learning	O
process	O
to	O
perform	O
statistical	O
estimation	O
.	O
the	O
most	O
common	O
cost	O
function	O
is	O
the	O
negative	O
log-likelihood	O
,	O
so	O
that	O
minimizing	O
the	O
cost	O
function	O
causes	O
maximum	O
likelihood	O
estimation	O
.	O
the	O
cost	O
function	O
may	O
also	O
include	O
additional	O
terms	O
,	O
such	O
as	O
regularization	O
terms	O
.	O
for	O
example	O
,	O
we	O
can	O
add	O
weight	O
decay	O
to	O
the	O
linear	O
regression	O
cost	O
function	O
to	O
obtain	O
j	O
(	O
w	O
)	O
=	O
,	O
b	O
||	O
||	O
w	O
2	O
λ	O
2	O
−	O
∼	O
ex	O
,	O
y	O
ˆpdata	O
|	O
log	O
pmodel	O
(	O
y	O
)	O
x	O
.	O
(	O
5.101	O
)	O
this	O
still	O
allows	O
closed-form	O
optimization	O
.	O
if	O
we	O
change	O
the	O
model	B
to	O
be	O
nonlinear	O
,	O
then	O
most	O
cost	O
functions	O
can	O
no	O
longer	O
be	O
optimized	O
in	O
closed	O
form	O
.	O
this	O
requires	O
us	O
to	O
choose	O
an	O
iterative	O
numerical	O
optimization	O
procedure	O
,	O
such	O
as	O
gradient	O
descent	B
.	O
the	O
recipe	O
for	O
constructing	O
a	O
learning	O
algorithm	O
by	O
combining	O
models	O
,	O
costs	O
,	O
and	O
optimization	O
algorithms	O
supports	O
both	O
supervised	O
and	O
unsupervised	O
learning	O
.	O
the	O
linear	O
regression	O
example	O
shows	O
how	O
to	O
support	O
supervised	O
learning	O
.	O
unsupervised	O
learning	O
can	O
be	O
supported	O
by	O
deﬁning	O
a	O
dataset	O
that	O
contains	O
only	O
x	O
and	O
providing	O
an	O
appropriate	O
unsupervised	O
cost	O
and	O
model	B
.	O
for	O
example	O
,	O
we	O
can	O
obtain	O
the	O
ﬁrst	O
pca	O
vector	O
by	O
specifying	O
that	O
our	O
loss	O
function	O
is	O
j	O
(	O
)	O
=	O
w	O
∼	O
ˆpdata	O
ex	O
||	O
−	O
x	O
r	O
(	O
;	O
||	O
x	O
w	O
2	O
)	O
2	O
(	O
5.102	O
)	O
while	O
our	O
model	B
is	O
deﬁned	O
to	O
have	O
w	O
with	O
norm	O
one	O
and	O
reconstruction	O
function	O
r	O
(	O
)	O
=	O
	O
xw	O
.	O
w	O
x	O
in	O
some	O
cases	O
,	O
the	O
cost	O
function	O
may	O
be	O
a	O
function	O
that	O
we	O
can	O
not	O
actually	O
evaluate	O
,	O
for	O
computational	O
reasons	O
.	O
in	O
these	O
cases	O
,	O
we	O
can	O
still	O
approximately	O
minimize	O
it	O
using	O
iterative	O
numerical	O
optimization	O
so	O
long	O
as	O
we	O
have	O
some	O
way	O
of	O
approximating	O
its	O
gradients	O
.	O
most	O
machine	O
learning	O
algorithms	O
make	O
use	O
of	O
this	O
recipe	O
,	O
though	O
it	O
may	O
not	O
immediately	O
be	O
obvious	O
.	O
if	O
a	O
machine	O
learning	O
algorithm	O
seems	O
especially	O
unique	O
or	O
154	O
chapter	O
5.	O
machine	O
learning	O
basics	O
hand-designed	O
,	O
it	O
can	O
usually	O
be	O
understood	O
as	O
using	O
a	O
special-case	O
optimizer	O
.	O
some	O
models	O
such	O
as	O
decision	O
trees	O
or	O
k-means	B
require	O
special-case	O
optimizers	O
because	O
their	O
cost	O
functions	O
have	O
ﬂat	O
regions	O
that	O
make	O
them	O
inappropriate	O
for	O
minimization	O
by	O
gradient-based	O
optimizers	O
.	O
recognizing	O
that	O
most	O
machine	O
learning	O
algorithms	O
can	O
be	O
described	O
using	O
this	O
recipe	O
helps	O
to	O
see	O
the	O
diﬀerent	O
algorithms	O
as	O
part	O
of	O
a	O
taxonomy	O
of	O
methods	O
for	O
doing	O
related	O
tasks	O
that	O
work	B
for	O
similar	O
reasons	O
,	O
rather	O
than	O
as	O
a	O
long	O
list	O
of	O
algorithms	O
that	O
each	O
have	O
separate	O
justiﬁcations	O
.	O
5.11	O
challenges	O
motivating	O
deep	O
learning	O
the	O
simple	O
machine	O
learning	O
algorithms	O
described	O
in	O
this	O
chapter	O
work	B
very	O
well	O
on	O
a	O
wide	O
variety	O
of	O
important	O
problems	O
.	O
however	O
,	O
they	O
have	O
not	O
succeeded	O
in	O
solving	O
the	O
central	O
problems	O
in	O
ai	O
,	O
such	O
as	O
recognizing	O
speech	O
or	O
recognizing	O
objects	O
.	O
the	O
development	O
of	O
deep	O
learning	O
was	O
motivated	O
in	O
part	O
by	O
the	O
failure	O
of	O
traditional	O
algorithms	O
to	O
generalize	O
well	O
on	O
such	O
ai	O
tasks	O
.	O
this	O
section	O
is	O
about	O
how	O
the	O
challenge	O
of	O
generalizing	O
to	O
new	O
examples	O
becomes	O
exponentially	O
more	O
diﬃcult	O
when	O
working	O
with	O
high-dimensional	O
data	O
,	O
and	O
how	O
the	O
mechanisms	O
used	O
to	O
achieve	O
generalization	O
in	O
traditional	O
machine	O
learning	O
are	O
insuﬃcient	O
to	O
learn	O
complicated	O
functions	O
in	O
high-dimensional	O
spaces	O
.	O
such	O
spaces	O
also	O
often	O
impose	O
high	O
computational	O
costs	O
.	O
deep	O
learning	O
was	O
designed	O
to	O
overcome	O
these	O
and	O
other	O
obstacles	O
.	O
5.11.1	O
the	O
curse	O
of	O
dimensionality	O
many	O
machine	O
learning	O
problems	O
become	O
exceedingly	O
diﬃcult	O
when	O
the	O
number	O
of	O
dimensions	O
in	O
the	O
data	O
is	O
high	O
.	O
this	O
phenomenon	O
is	O
known	O
as	O
the	O
curse	O
of	O
dimensionality	O
.	O
of	O
particular	O
concern	O
is	O
that	O
the	O
number	O
of	O
possible	O
distinct	O
conﬁgurations	O
of	O
a	O
set	O
of	O
variables	O
increases	O
exponentially	O
as	O
the	O
number	O
of	O
variables	O
increases	O
.	O
155	O
chapter	O
5.	O
machine	O
learning	O
basics	O
figure	O
5.9	O
:	O
as	O
the	O
number	O
of	O
relevant	O
dimensions	O
of	O
the	O
data	O
increases	O
(	O
from	O
left	O
to	O
right	O
)	O
,	O
the	O
number	O
of	O
conﬁgurations	O
of	O
interest	O
may	O
grow	O
exponentially	O
.	O
(	O
left	O
)	O
in	O
this	O
one-dimensional	O
example	O
,	O
we	O
have	O
one	O
variable	O
for	O
which	O
we	O
only	O
care	O
to	O
distinguish	O
10	O
regions	O
of	O
interest	O
.	O
with	O
enough	O
examples	O
falling	O
within	O
each	O
of	O
these	O
regions	O
(	O
each	O
region	O
corresponds	O
to	O
a	O
cell	O
in	O
the	O
illustration	O
)	O
,	O
learning	O
algorithms	O
can	O
easily	O
generalize	O
correctly	O
.	O
a	O
straightforward	O
way	O
to	O
generalize	O
is	O
to	O
estimate	O
the	O
value	O
of	O
the	O
target	O
function	O
within	O
each	O
region	O
(	O
and	O
possibly	O
interpolate	O
between	O
neighboring	O
regions	O
)	O
.	O
with	O
2	O
×	O
dimensions	O
it	O
is	O
more	O
diﬃcult	O
to	O
distinguish	O
10	O
diﬀerent	O
values	O
of	O
each	O
variable	O
.	O
we	O
need	O
to	O
keep	O
track	O
of	O
up	O
to	O
10	O
10=100	O
regions	O
,	O
and	O
we	O
need	O
at	O
least	O
that	O
many	O
examples	O
to	O
cover	O
all	O
those	O
regions	O
.	O
103	O
=	O
1000	O
regions	O
and	O
at	O
least	O
that	O
many	O
examples	O
.	O
for	O
d	O
dimensions	O
and	O
v	O
values	O
to	O
be	O
distinguished	O
along	O
each	O
axis	O
,	O
we	O
seem	O
to	O
need	O
o	O
(	O
v	O
d	O
)	O
regions	O
and	O
examples	O
.	O
this	O
is	O
an	O
instance	O
of	O
the	O
curse	O
of	O
dimensionality	O
.	O
figure	O
graciously	O
provided	O
by	O
nicolas	O
chapados	O
.	O
with	O
3	O
dimensions	O
this	O
grows	O
to	O
(	O
center	O
)	O
(	O
right	O
)	O
the	O
curse	O
of	O
dimensionality	O
arises	O
in	O
many	O
places	O
in	O
computer	O
science	O
,	O
and	O
especially	O
so	O
in	O
machine	O
learning	O
.	O
5.9	O
one	O
challenge	O
posed	O
by	O
the	O
curse	O
of	O
dimensionality	O
is	O
a	O
statistical	O
challenge	O
.	O
as	O
illustrated	O
in	O
ﬁgure	O
,	O
a	O
statistical	O
challenge	O
arises	O
because	O
the	O
number	O
of	O
possible	O
conﬁgurations	O
of	O
x	O
is	O
much	O
larger	O
than	O
the	O
number	O
of	O
training	O
examples	O
.	O
to	O
understand	O
the	O
issue	O
,	O
let	O
us	O
consider	O
that	O
the	O
input	O
space	O
is	O
organized	O
into	O
a	O
grid	O
,	O
like	O
in	O
the	O
ﬁgure	O
.	O
we	O
can	O
describe	O
low-dimensional	O
space	O
with	O
a	O
low	O
number	O
of	O
grid	O
cells	O
that	O
are	O
mostly	O
occupied	O
by	O
the	O
data	O
.	O
when	O
generalizing	O
to	O
a	O
new	O
data	O
point	O
,	O
we	O
can	O
usually	O
tell	O
what	O
to	O
do	O
simply	O
by	O
inspecting	O
the	O
training	O
examples	O
that	O
lie	O
in	O
the	O
same	O
cell	O
as	O
the	O
new	O
input	O
.	O
for	O
example	O
,	O
if	O
estimating	O
the	O
probability	O
density	O
at	O
some	O
point	O
x	O
,	O
we	O
can	O
just	O
return	O
the	O
number	O
of	O
training	O
examples	O
in	O
the	O
same	O
unit	O
volume	O
cell	O
as	O
x	O
,	O
divided	O
by	O
the	O
total	O
number	O
of	O
training	O
examples	O
.	O
if	O
we	O
wish	O
to	O
classify	O
an	O
example	O
,	O
we	O
can	O
return	O
the	O
most	O
common	O
class	O
of	O
training	O
examples	O
in	O
the	O
same	O
cell	O
.	O
if	O
we	O
are	O
doing	O
regression	O
we	O
can	O
average	O
the	O
target	O
values	O
observed	O
over	O
the	O
examples	O
in	O
that	O
cell	O
.	O
but	O
what	O
about	O
the	O
cells	O
for	O
which	O
we	O
have	O
seen	O
no	O
example	O
?	O
because	O
in	O
high-dimensional	O
spaces	O
the	O
number	O
of	O
conﬁgurations	O
is	O
huge	O
,	O
much	O
larger	O
than	O
our	O
number	O
of	O
examples	O
,	O
a	O
typical	O
grid	O
cell	O
has	O
no	O
training	O
example	O
associated	O
with	O
it	O
.	O
how	O
could	O
we	O
possibly	O
say	O
something	O
156	O
chapter	O
5.	O
machine	O
learning	O
basics	O
meaningful	O
about	O
these	O
new	O
conﬁgurations	O
?	O
many	O
traditional	O
machine	O
learning	O
algorithms	O
simply	O
assume	O
that	O
the	O
output	O
at	O
a	O
new	O
point	O
should	O
be	O
approximately	O
the	O
same	O
as	O
the	O
output	O
at	O
the	O
nearest	O
training	O
point	O
.	O
5.11.2	O
local	O
constancy	O
and	O
smoothness	O
regularization	O
in	O
order	O
to	O
generalize	O
well	O
,	O
machine	O
learning	O
algorithms	O
need	O
to	O
be	O
guided	O
by	O
prior	O
beliefs	O
about	O
what	O
kind	O
of	O
function	O
they	O
should	O
learn	O
.	O
previously	O
,	O
we	O
have	O
seen	O
these	O
priors	O
incorporated	O
as	O
explicit	O
beliefs	O
in	O
the	O
form	O
of	O
probability	O
distributions	O
over	O
parameters	O
of	O
the	O
model	B
.	O
more	O
informally	O
,	O
we	O
may	O
also	O
discuss	O
prior	O
beliefs	O
as	O
directly	O
inﬂuencing	O
the	O
itself	O
and	O
only	O
indirectly	O
acting	O
on	O
the	O
parameters	O
via	O
their	O
eﬀect	O
on	O
the	O
function	O
.	O
additionally	O
,	O
we	O
informally	O
discuss	O
prior	O
beliefs	O
as	O
being	O
expressed	O
implicitly	O
,	O
by	O
choosing	O
algorithms	O
that	O
are	O
biased	O
toward	O
choosing	O
some	O
class	O
of	O
functions	O
over	O
another	O
,	O
even	O
though	O
these	O
biases	O
may	O
not	O
be	O
expressed	O
(	O
or	O
even	O
possible	O
to	O
express	O
)	O
in	O
terms	O
of	O
a	O
probability	O
distribution	O
representing	O
our	O
degree	O
of	O
belief	O
in	O
various	O
functions	O
.	O
function	O
among	O
the	O
most	O
widely	O
used	O
of	O
these	O
implicit	O
“	O
priors	O
”	O
is	O
the	O
smoothness	O
prior	O
or	O
local	O
constancy	O
prior	O
.	O
this	O
prior	O
states	O
that	O
the	O
function	O
we	O
learn	O
should	O
not	O
change	O
very	O
much	O
within	O
a	O
small	O
region	O
.	O
many	O
simpler	O
algorithms	O
rely	O
exclusively	O
on	O
this	O
prior	O
to	O
generalize	O
well	O
,	O
and	O
as	O
a	O
result	O
they	O
fail	O
to	O
scale	O
to	O
the	O
statistical	O
challenges	O
involved	O
in	O
solving	O
ai-	O
level	O
tasks	O
.	O
throughout	O
this	O
book	O
,	O
we	O
will	O
describe	O
how	O
deep	O
learning	O
introduces	O
additional	O
(	O
explicit	O
and	O
implicit	O
)	O
priors	O
in	O
order	O
to	O
reduce	O
the	O
generalization	O
error	O
on	O
sophisticated	O
tasks	O
.	O
here	O
,	O
we	O
explain	O
why	O
the	O
smoothness	O
prior	O
alone	O
is	O
insuﬃcient	O
for	O
these	O
tasks	O
.	O
there	O
are	O
many	O
diﬀerent	O
ways	O
to	O
implicitly	O
or	O
explicitly	O
express	O
a	O
prior	O
belief	O
that	O
the	O
learned	O
function	O
should	O
be	O
smooth	O
or	O
locally	O
constant	O
.	O
all	O
of	O
these	O
diﬀerent	O
methods	O
are	O
designed	O
to	O
encourage	O
the	O
learning	O
process	O
to	O
learn	O
a	O
function	O
f	O
that	O
satisﬁes	O
the	O
condition	O
∗	O
(	O
5.103	O
)	O
∗	O
≈	O
(	O
)	O
x	O
f	O
∗	O
(	O
+	O
)	O
x	O
	O
f	O
for	O
most	O
conﬁgurations	O
x	O
and	O
small	O
change	O
	O
.	O
in	O
other	O
words	O
,	O
if	O
we	O
know	O
a	O
good	O
answer	O
for	O
an	O
input	O
x	O
(	O
for	O
example	O
,	O
if	O
x	O
is	O
a	O
labeled	O
training	O
example	O
)	O
then	O
that	O
answer	O
is	O
probably	O
good	O
in	O
the	O
neighborhood	O
of	O
x.	O
if	O
we	O
have	O
several	O
good	O
answers	O
in	O
some	O
neighborhood	O
we	O
would	O
combine	O
them	O
(	O
by	O
some	O
form	O
of	O
averaging	O
or	O
interpolation	O
)	O
to	O
produce	O
an	O
answer	O
that	O
agrees	O
with	O
as	O
many	O
of	O
them	O
as	O
much	O
as	O
possible	O
.	O
an	O
extreme	O
example	O
of	O
the	O
local	O
constancy	O
approach	O
is	O
the	O
k-nearest	B
neighbors	I
family	O
of	O
learning	O
algorithms	O
.	O
these	O
predictors	O
are	O
literally	O
constant	O
over	O
each	O
157	O
chapter	O
5.	O
machine	O
learning	O
basics	O
region	O
containing	O
all	O
the	O
points	O
x	O
that	O
have	O
the	O
same	O
set	O
of	O
k	O
nearest	O
neighbors	O
in	O
the	O
training	O
set	O
.	O
for	O
k	O
=	O
1	O
,	O
the	O
number	O
of	O
distinguishable	O
regions	O
can	O
not	O
be	O
more	O
than	O
the	O
number	O
of	O
training	O
examples	O
.	O
while	O
the	O
k-nearest	B
neighbors	I
algorithm	O
copies	O
the	O
output	O
from	O
nearby	O
training	O
examples	O
,	O
most	O
kernel	O
machines	O
interpolate	O
between	O
training	O
set	O
outputs	O
associated	O
with	O
nearby	O
training	O
examples	O
.	O
an	O
important	O
class	O
of	O
kernels	O
is	O
the	O
family	O
of	O
local	O
kernels	O
where	O
k	O
(	O
u	O
v	O
,	O
)	O
is	O
large	O
when	O
u	O
=	O
v	O
and	O
decreases	O
as	O
u	O
and	O
v	O
grow	O
farther	O
apart	O
from	O
each	O
other	O
.	O
a	O
local	O
kernel	O
can	O
be	O
thought	O
of	O
as	O
a	O
similarity	O
function	O
that	O
performs	O
template	O
matching	O
,	O
by	O
measuring	O
how	O
closely	O
a	O
test	O
example	O
x	O
resembles	O
each	O
training	O
example	O
x	O
(	O
)	O
i	O
.	O
much	O
of	O
the	O
modern	O
motivation	O
for	O
deep	O
learning	O
is	O
derived	O
from	O
studying	O
the	O
limitations	O
of	O
local	O
template	O
matching	O
and	O
how	O
deep	O
models	O
are	O
able	O
to	O
succeed	O
in	O
cases	O
where	O
local	O
template	O
matching	O
fails	O
(	O
bengio	O
et	O
al	O
.	O
2006b	O
)	O
.	O
,	O
decision	O
trees	O
also	O
suﬀer	O
from	O
the	O
limitations	O
of	O
exclusively	O
smoothness-based	O
learning	O
because	O
they	O
break	O
the	O
input	O
space	O
into	O
as	O
many	O
regions	O
as	O
there	O
are	O
leaves	O
and	O
use	O
a	O
separate	O
parameter	O
(	O
or	O
sometimes	O
many	O
parameters	O
for	O
extensions	O
of	O
decision	O
trees	O
)	O
in	O
each	O
region	O
.	O
if	O
the	O
target	O
function	O
requires	O
a	O
tree	O
with	O
at	O
least	O
n	O
leaves	O
to	O
be	O
represented	O
accurately	O
,	O
then	O
at	O
least	O
n	O
training	O
examples	O
are	O
required	O
to	O
ﬁt	O
the	O
tree	O
.	O
a	O
multiple	O
of	O
n	O
is	O
needed	O
to	O
achieve	O
some	O
level	O
of	O
statistical	O
conﬁdence	O
in	O
the	O
predicted	O
output	O
.	O
in	O
general	O
,	O
to	O
distinguish	O
o	O
(	O
k	O
)	O
regions	O
in	O
input	O
space	O
,	O
all	O
of	O
these	O
methods	O
require	O
o	O
(	O
k	O
)	O
examples	O
.	O
typically	O
there	O
are	O
o	O
(	O
k	O
)	O
parameters	O
,	O
with	O
o	O
(	O
1	O
)	O
parameters	O
associated	O
with	O
each	O
of	O
the	O
o	O
(	O
k	O
)	O
regions	O
.	O
the	O
case	O
of	O
a	O
nearest	O
neighbor	O
scenario	O
,	O
where	O
each	O
training	O
example	O
can	O
be	O
used	O
to	O
deﬁne	O
at	O
most	O
one	O
region	O
,	O
is	O
illustrated	O
in	O
ﬁgure	O
.	O
5.10	O
is	O
there	O
a	O
way	O
to	O
represent	O
a	O
complex	O
function	O
that	O
has	O
many	O
more	O
regions	O
to	O
be	O
distinguished	O
than	O
the	O
number	O
of	O
training	O
examples	O
?	O
clearly	O
,	O
assuming	O
only	O
smoothness	O
of	O
the	O
underlying	O
function	O
will	O
not	O
allow	O
a	O
learner	O
to	O
do	O
that	O
.	O
for	O
example	O
,	O
imagine	O
that	O
the	O
target	O
function	O
is	O
a	O
kind	O
of	O
checkerboard	O
.	O
a	O
checkerboard	O
contains	O
many	O
variations	O
but	O
there	O
is	O
a	O
simple	O
structure	O
to	O
them	O
.	O
imagine	O
what	O
happens	O
when	O
the	O
number	O
of	O
training	O
examples	O
is	O
substantially	O
smaller	O
than	O
the	O
number	O
of	O
black	O
and	O
white	O
squares	O
on	O
the	O
checkerboard	O
.	O
based	O
on	O
only	O
local	O
generalization	O
and	O
the	O
smoothness	O
or	O
local	O
constancy	O
prior	O
,	O
we	O
would	O
be	O
guaranteed	O
to	O
correctly	O
guess	O
the	O
color	O
of	O
a	O
new	O
point	O
if	O
it	O
lies	O
within	O
the	O
same	O
checkerboard	O
square	O
as	O
a	O
training	O
example	O
.	O
there	O
is	O
no	O
guarantee	O
that	O
the	O
learner	O
could	O
correctly	O
extend	O
the	O
checkerboard	O
pattern	O
to	O
points	O
lying	O
in	O
squares	O
that	O
do	O
not	O
contain	O
training	O
examples	O
.	O
with	O
this	O
prior	O
alone	O
,	O
the	O
only	O
information	O
that	O
an	O
example	O
tells	O
us	O
is	O
the	O
color	O
of	O
its	O
square	O
,	O
and	O
the	O
only	O
way	O
to	O
get	O
the	O
colors	O
of	O
the	O
158	O
chapter	O
5.	O
machine	O
learning	O
basics	O
figure	O
5.10	O
:	O
illustration	O
of	O
how	O
the	O
nearest	O
neighbor	O
algorithm	O
breaks	O
up	O
the	O
input	O
space	O
into	O
regions	O
.	O
an	O
example	O
(	O
represented	O
here	O
by	O
a	O
circle	O
)	O
within	O
each	O
region	O
deﬁnes	O
the	O
region	O
boundary	O
(	O
represented	O
here	O
by	O
the	O
lines	O
)	O
.	O
the	O
y	O
value	O
associated	O
with	O
each	O
example	O
deﬁnes	O
what	O
the	O
output	O
should	O
be	O
for	O
all	O
points	O
within	O
the	O
corresponding	O
region	O
.	O
the	O
regions	O
deﬁned	O
by	O
nearest	O
neighbor	O
matching	O
form	O
a	O
geometric	O
pattern	O
called	O
a	O
voronoi	O
diagram	O
.	O
the	O
number	O
of	O
these	O
contiguous	O
regions	O
can	O
not	O
grow	O
faster	O
than	O
the	O
number	O
of	O
training	O
examples	O
.	O
while	O
this	O
ﬁgure	O
illustrates	O
the	O
behavior	O
of	O
the	O
nearest	O
neighbor	O
algorithm	O
speciﬁcally	O
,	O
other	O
machine	O
learning	O
algorithms	O
that	O
rely	O
exclusively	O
on	O
the	O
local	O
smoothness	O
prior	O
for	O
generalization	O
exhibit	O
similar	O
behaviors	O
:	O
each	O
training	O
example	O
only	O
informs	O
the	O
learner	O
about	O
how	O
to	O
generalize	O
in	O
some	O
neighborhood	O
immediately	O
surrounding	O
that	O
example	O
.	O
159	O
chapter	O
5.	O
machine	O
learning	O
basics	O
entire	O
checkerboard	O
right	O
is	O
to	O
cover	O
each	O
of	O
its	O
cells	O
with	O
at	O
least	O
one	O
example	O
.	O
the	O
smoothness	O
assumption	O
and	O
the	O
associated	O
non-parametric	O
learning	O
algo-	O
rithms	O
work	B
extremely	O
well	O
so	O
long	O
as	O
there	O
are	O
enough	O
examples	O
for	O
the	O
learning	O
algorithm	O
to	O
observe	O
high	O
points	O
on	O
most	O
peaks	O
and	O
low	O
points	O
on	O
most	O
valleys	O
of	O
the	O
true	O
underlying	O
function	O
to	O
be	O
learned	O
.	O
this	O
is	O
generally	O
true	O
when	O
the	O
function	O
to	O
be	O
learned	O
is	O
smooth	O
enough	O
and	O
varies	O
in	O
few	O
enough	O
dimensions	O
.	O
in	O
high	O
dimensions	O
,	O
even	O
a	O
very	O
smooth	O
function	O
can	O
change	O
smoothly	O
but	O
in	O
a	O
diﬀerent	O
way	O
along	O
each	O
dimension	O
.	O
if	O
the	O
function	O
additionally	O
behaves	O
diﬀerently	O
in	O
diﬀerent	O
regions	O
,	O
it	O
can	O
become	O
extremely	O
complicated	O
to	O
describe	O
with	O
a	O
set	O
of	O
training	O
examples	O
.	O
if	O
the	O
function	O
is	O
complicated	O
(	O
we	O
want	O
to	O
distinguish	O
a	O
huge	O
number	O
of	O
regions	O
compared	O
to	O
the	O
number	O
of	O
examples	O
)	O
,	O
is	O
there	O
any	O
hope	O
to	O
generalize	O
well	O
?	O
the	O
answer	O
to	O
both	O
of	O
these	O
questions—whether	O
it	O
is	O
possible	O
to	O
represent	O
a	O
complicated	O
function	O
eﬃciently	O
,	O
and	O
whether	O
it	O
is	O
possible	O
for	O
the	O
estimated	O
function	O
to	O
generalize	O
well	O
to	O
new	O
inputs—is	O
yes	O
.	O
the	O
key	O
insight	O
is	O
that	O
a	O
very	O
large	O
number	O
of	O
regions	O
,	O
e.g.	O
,	O
o	O
(	O
2k	O
)	O
,	O
can	O
be	O
deﬁned	O
with	O
o	O
(	O
k	O
)	O
examples	O
,	O
so	O
long	O
as	O
we	O
introduce	O
some	O
dependencies	O
between	O
the	O
regions	O
via	O
additional	O
assumptions	O
about	O
the	O
underlying	O
data	O
generating	O
distribution	O
.	O
in	O
this	O
way	O
,	O
we	O
can	O
actually	O
generalize	O
non-locally	O
(	O
)	O
.	O
many	O
diﬀerent	O
deep	O
learning	O
algorithms	O
provide	O
implicit	O
or	O
explicit	O
assumptions	O
that	O
are	O
reasonable	O
for	O
a	O
broad	O
range	O
of	O
ai	O
tasks	O
in	O
order	O
to	O
capture	O
these	O
advantages	O
.	O
bengio	O
and	O
monperrus	O
2005	O
bengio	O
et	O
al	O
.	O
2006c	O
,	O
;	O
,	O
other	O
approaches	O
to	O
machine	O
learning	O
often	O
make	O
stronger	O
,	O
task-speciﬁc	O
as-	O
sumptions	O
.	O
for	O
example	O
,	O
we	O
could	O
easily	O
solve	O
the	O
checkerboard	O
task	O
by	O
providing	O
the	O
assumption	O
that	O
the	O
target	O
function	O
is	O
periodic	O
.	O
usually	O
we	O
do	O
not	O
include	O
such	O
strong	O
,	O
task-speciﬁc	O
assumptions	O
into	O
neural	O
networks	O
so	O
that	O
they	O
can	O
generalize	O
to	O
a	O
much	O
wider	O
variety	O
of	O
structures	O
.	O
ai	O
tasks	O
have	O
structure	O
that	O
is	O
much	O
too	O
complex	O
to	O
be	O
limited	O
to	O
simple	O
,	O
manually	O
speciﬁed	O
properties	O
such	O
as	O
periodicity	O
,	O
so	O
we	O
want	O
learning	O
algorithms	O
that	O
embody	O
more	O
general-purpose	O
assumptions	O
.	O
the	O
core	O
idea	O
in	O
deep	O
learning	O
is	O
that	O
we	O
assume	O
that	O
the	O
data	O
was	O
generated	O
by	O
the	O
composition	O
of	O
factors	O
or	O
features	O
,	O
potentially	O
at	O
multiple	O
levels	O
in	O
a	O
hierarchy	O
.	O
many	O
other	O
similarly	O
generic	O
assumptions	O
can	O
further	O
improve	O
deep	O
learning	O
al-	O
gorithms	O
.	O
these	O
apparently	O
mild	O
assumptions	O
allow	O
an	O
exponential	O
gain	O
in	O
the	O
relationship	O
between	O
the	O
number	O
of	O
examples	O
and	O
the	O
number	O
of	O
regions	O
that	O
can	O
be	O
distinguished	O
.	O
these	O
exponential	O
gains	O
are	O
described	O
more	O
precisely	O
in	O
sections	O
6.4.1	O
15.4	O
.	O
the	O
exponential	O
advantages	O
conferred	O
by	O
the	O
use	O
of	O
deep	O
,	O
distributed	O
representations	O
counter	O
the	O
exponential	O
challenges	O
posed	O
by	O
the	O
curse	O
of	O
dimensionality	O
.	O
15.5	O
and	O
,	O
160	O
chapter	O
5.	O
machine	O
learning	O
basics	O
5.11.3	O
manifold	O
learning	O
an	O
important	O
concept	O
underlying	O
many	O
ideas	O
in	O
machine	O
learning	O
is	O
that	O
of	O
a	O
manifold	O
.	O
a	O
manifold	O
is	O
a	O
connected	O
region	O
.	O
mathematically	O
,	O
it	O
is	O
a	O
set	O
of	O
points	O
,	O
associated	O
with	O
a	O
neighborhood	O
around	O
each	O
point	O
.	O
from	O
any	O
given	O
point	O
,	O
the	O
manifold	O
locally	O
appears	O
to	O
be	O
a	O
euclidean	O
space	O
.	O
in	O
everyday	O
life	O
,	O
we	O
experience	O
the	O
surface	O
of	O
the	O
world	O
as	O
a	O
2-d	O
plane	O
,	O
but	O
it	O
is	O
in	O
fact	O
a	O
spherical	O
manifold	O
in	O
3-d	O
space	O
.	O
the	O
deﬁnition	O
of	O
a	O
neighborhood	O
surrounding	O
each	O
point	O
implies	O
the	O
existence	O
of	O
transformations	O
that	O
can	O
be	O
applied	O
to	O
move	O
on	O
the	O
manifold	O
from	O
one	O
position	B
to	O
a	O
neighboring	O
one	O
.	O
in	O
the	O
example	O
of	O
the	O
world	O
’	O
s	O
surface	O
as	O
a	O
manifold	O
,	O
one	O
can	O
walk	O
north	O
,	O
south	O
,	O
east	O
,	O
or	O
west	O
.	O
although	O
there	O
is	O
a	O
formal	O
mathematical	O
meaning	O
to	O
the	O
term	O
“	O
manifold	O
,	O
”	O
in	O
machine	O
learning	O
it	O
tends	O
to	O
be	O
used	O
more	O
loosely	O
to	O
designate	O
a	O
connected	O
set	O
of	O
points	O
that	O
can	O
be	O
approximated	O
well	O
by	O
considering	O
only	O
a	O
small	O
number	O
of	O
degrees	O
of	O
freedom	O
,	O
or	O
dimensions	O
,	O
embedded	O
in	O
a	O
higher-dimensional	O
space	O
.	O
each	O
dimension	O
corresponds	O
to	O
a	O
local	O
direction	O
of	O
variation	O
.	O
see	O
ﬁgure	O
for	O
an	O
example	O
of	O
training	O
data	O
lying	O
near	O
a	O
one-dimensional	O
manifold	O
embedded	O
in	O
two-	O
dimensional	O
space	O
.	O
in	O
the	O
context	O
of	O
machine	O
learning	O
,	O
we	O
allow	O
the	O
dimensionality	O
of	O
the	O
manifold	O
to	O
vary	O
from	O
one	O
point	O
to	O
another	O
.	O
this	O
often	O
happens	O
when	O
a	O
manifold	O
intersects	O
itself	O
.	O
for	O
example	O
,	O
a	O
ﬁgure	O
eight	O
is	O
a	O
manifold	O
that	O
has	O
a	O
single	O
dimension	O
in	O
most	O
places	O
but	O
two	O
dimensions	O
at	O
the	O
intersection	O
at	O
the	O
center	O
.	O
5.11	O
2	O
5	O
.	O
2	O
0	O
.	O
1	O
5	O
.	O
1	O
0	O
.	O
0	O
5	O
.	O
0	O
0	O
.	O
−	O
0	O
5	O
.	O
−	O
1	O
0	O
.	O
0	O
5	O
.	O
1	O
0	O
.	O
1	O
5	O
.	O
2	O
0	O
.	O
2	O
5	O
.	O
3	O
0	O
.	O
3	O
5	O
.	O
4	O
0	O
.	O
figure	O
5.11	O
:	O
data	O
sampled	O
from	O
a	O
distribution	O
in	O
a	O
two-dimensional	O
space	O
that	O
is	O
actually	O
concentrated	O
near	O
a	O
one-dimensional	O
manifold	O
,	O
like	O
a	O
twisted	O
string	O
.	O
the	O
solid	O
line	O
indicates	O
the	O
underlying	O
manifold	O
that	O
the	O
learner	O
should	O
infer	O
.	O
161	O
chapter	O
5.	O
machine	O
learning	O
basics	O
many	O
machine	O
learning	O
problems	O
seem	O
hopeless	O
if	O
we	O
expect	O
the	O
machine	O
n.	O
learning	O
algorithm	O
to	O
learn	O
functions	O
with	O
interesting	O
variations	O
across	O
all	O
of	O
r	O
manifold	O
learning	O
algorithms	O
surmount	O
this	O
obstacle	O
by	O
assuming	O
that	O
most	O
n	O
consists	O
of	O
invalid	O
inputs	O
,	O
and	O
that	O
interesting	O
inputs	O
occur	O
only	O
along	O
of	O
r	O
a	O
collection	O
of	O
manifolds	O
containing	O
a	O
small	O
subset	O
of	O
points	O
,	O
with	O
interesting	O
variations	O
in	O
the	O
output	O
of	O
the	O
learned	O
function	O
occurring	O
only	O
along	O
directions	O
that	O
lie	O
on	O
the	O
manifold	O
,	O
or	O
with	O
interesting	O
variations	O
happening	O
only	O
when	O
we	O
move	O
from	O
one	O
manifold	O
to	O
another	O
.	O
manifold	O
learning	O
was	O
introduced	O
in	O
the	O
case	O
of	O
continuous-valued	O
data	O
and	O
the	O
unsupervised	O
learning	O
setting	O
,	O
although	O
this	O
probability	O
concentration	O
idea	O
can	O
be	O
generalized	O
to	O
both	O
discrete	O
data	O
and	O
the	O
supervised	O
learning	O
setting	O
:	O
the	O
key	O
assumption	O
remains	O
that	O
probability	O
mass	O
is	O
highly	O
concentrated	O
.	O
the	O
assumption	O
that	O
the	O
data	O
lies	O
along	O
a	O
low-dimensional	O
manifold	O
may	O
not	O
always	O
be	O
correct	O
or	O
useful	O
.	O
we	O
argue	O
that	O
in	O
the	O
context	O
of	O
ai	O
tasks	O
,	O
such	O
as	O
those	O
that	O
involve	O
processing	O
images	O
,	O
sounds	O
,	O
or	O
text	O
,	O
the	O
manifold	O
assumption	O
is	O
at	O
least	O
approximately	O
correct	O
.	O
the	O
evidence	O
in	O
favor	O
of	O
this	O
assumption	O
consists	O
of	O
two	O
categories	O
of	O
observations	O
.	O
the	O
ﬁrst	O
observation	O
in	O
favor	O
of	O
the	O
manifold	O
hypothesis	O
is	O
that	O
the	O
proba-	O
bility	O
distribution	O
over	O
images	O
,	O
text	O
strings	O
,	O
and	O
sounds	O
that	O
occur	O
in	O
real	O
life	O
is	O
highly	O
concentrated	O
.	O
uniform	O
noise	O
essentially	O
never	O
resembles	O
structured	O
inputs	O
from	O
these	O
domains	O
.	O
figure	O
shows	O
how	O
,	O
instead	O
,	O
uniformly	O
sampled	O
points	O
look	O
like	O
the	O
patterns	O
of	O
static	O
that	O
appear	O
on	O
analog	O
television	O
sets	O
when	O
no	O
signal	O
is	O
available	O
.	O
similarly	O
,	O
if	O
you	O
generate	O
a	O
document	O
by	O
picking	O
letters	O
uniformly	O
at	O
random	O
,	O
what	O
is	O
the	O
probability	O
that	O
you	O
will	O
get	O
a	O
meaningful	O
english-language	O
text	O
?	O
almost	O
zero	O
,	O
again	O
,	O
because	O
most	O
of	O
the	O
long	O
sequences	O
of	O
letters	O
do	O
not	O
correspond	O
to	O
a	O
natural	O
language	O
sequence	O
:	O
the	O
distribution	O
of	O
natural	O
language	O
sequences	O
occupies	O
a	O
very	O
small	O
volume	O
in	O
the	O
total	O
space	O
of	O
sequences	O
of	O
letters	O
.	O
5.12	O
162	O
chapter	O
5.	O
machine	O
learning	O
basics	O
figure	O
5.12	O
:	O
sampling	O
images	O
uniformly	O
at	O
random	O
(	O
by	O
randomly	O
picking	O
each	O
pixel	O
according	O
to	O
a	O
uniform	O
distribution	O
)	O
gives	O
rise	O
to	O
noisy	O
images	O
.	O
although	O
there	O
is	O
a	O
non-	O
zero	O
probability	O
to	O
generate	O
an	O
image	O
of	O
a	O
face	O
or	O
any	O
other	O
object	O
frequently	O
encountered	O
in	O
ai	O
applications	O
,	O
we	O
never	O
actually	O
observe	O
this	O
happening	O
in	O
practice	O
.	O
this	O
suggests	O
that	O
the	O
images	O
encountered	O
in	O
ai	O
applications	O
occupy	O
a	O
negligible	O
proportion	O
of	O
the	O
volume	O
of	O
image	O
space	O
.	O
of	O
course	O
,	O
concentrated	O
probability	O
distributions	O
are	O
not	O
suﬃcient	O
to	O
show	O
that	O
the	O
data	O
lies	O
on	O
a	O
reasonably	O
small	O
number	O
of	O
manifolds	O
.	O
we	O
must	O
also	O
establish	O
that	O
the	O
examples	O
we	O
encounter	O
are	O
connected	O
to	O
each	O
other	O
by	O
other	O
163	O
chapter	O
5.	O
machine	O
learning	O
basics	O
examples	O
,	O
with	O
each	O
example	O
surrounded	O
by	O
other	O
highly	O
similar	O
examples	O
that	O
may	O
be	O
reached	O
by	O
applying	O
transformations	O
to	O
traverse	O
the	O
manifold	O
.	O
the	O
second	O
argument	O
in	O
favor	O
of	O
the	O
manifold	O
hypothesis	O
is	O
that	O
we	O
can	O
also	O
imagine	O
such	O
neighborhoods	O
and	O
transformations	O
,	O
at	O
least	O
informally	O
.	O
in	O
the	O
case	O
of	O
images	O
,	O
we	O
can	O
certainly	O
think	O
of	O
many	O
possible	O
transformations	O
that	O
allow	O
us	O
to	O
trace	O
out	O
a	O
manifold	O
in	O
image	O
space	O
:	O
we	O
can	O
gradually	O
dim	O
or	O
brighten	O
the	O
lights	O
,	O
gradually	O
move	O
or	O
rotate	O
objects	O
in	O
the	O
image	O
,	O
gradually	O
alter	O
the	O
colors	O
on	O
the	O
surfaces	O
of	O
objects	O
,	O
etc	O
.	O
it	O
remains	O
likely	O
that	O
there	O
are	O
multiple	O
manifolds	O
involved	O
in	O
most	O
applications	O
.	O
for	O
example	O
,	O
the	O
manifold	O
of	O
images	O
of	O
human	O
faces	O
may	O
not	O
be	O
connected	O
to	O
the	O
manifold	O
of	O
images	O
of	O
cat	O
faces	O
.	O
these	O
thought	O
experiments	O
supporting	O
the	O
manifold	O
hypotheses	O
convey	O
some	O
in-	O
tuitive	O
reasons	O
supporting	O
it	O
.	O
more	O
rigorous	O
experiments	O
(	O
cayton	O
2005	O
narayanan	O
and	O
mitter	O
2010	O
schölkopf	O
et	O
al.	O
,	O
2000	O
brand	O
2003	O
belkin	O
and	O
niyogi	O
2003	O
donoho	O
and	O
grimes	O
2003	O
weinberger	O
and	O
saul	O
2004	O
)	O
clearly	O
support	O
the	O
hypothesis	O
for	O
a	O
large	O
class	O
of	O
datasets	O
of	O
interest	O
in	O
ai	O
.	O
1998	O
roweis	O
and	O
saul	O
2000	O
tenenbaum	O
et	O
al.	O
,	O
,	O
;	O
,	O
;	O
;	O
;	O
;	O
,	O
,	O
;	O
;	O
,	O
;	O
,	O
,	O
when	O
the	O
data	O
lies	O
on	O
a	O
low-dimensional	O
manifold	O
,	O
it	O
can	O
be	O
most	O
natural	O
for	O
machine	O
learning	O
algorithms	O
to	O
represent	O
the	O
data	O
in	O
terms	O
of	O
coordinates	O
on	O
the	O
manifold	O
,	O
rather	O
than	O
in	O
terms	O
of	O
coordinates	O
in	O
r	O
n.	O
in	O
everyday	O
life	O
,	O
we	O
can	O
think	O
of	O
roads	O
as	O
1-d	O
manifolds	O
embedded	O
in	O
3-d	O
space	O
.	O
we	O
give	O
directions	O
to	O
speciﬁc	O
addresses	O
in	O
terms	O
of	O
address	O
numbers	O
along	O
these	O
1-d	O
roads	O
,	O
not	O
in	O
terms	O
of	O
coordinates	O
in	O
3-d	O
space	O
.	O
extracting	O
these	O
manifold	O
coordinates	O
is	O
challenging	O
,	O
but	O
holds	O
the	O
promise	O
to	O
improve	O
many	O
machine	O
learning	O
algorithms	O
.	O
this	O
general	O
principle	O
is	O
applied	O
in	O
many	O
contexts	O
.	O
figure	O
shows	O
the	O
manifold	O
structure	O
of	O
a	O
dataset	O
consisting	O
of	O
faces	O
.	O
by	O
the	O
end	O
of	O
this	O
book	O
,	O
we	O
will	O
have	O
developed	O
the	O
methods	O
necessary	O
to	O
learn	O
such	O
a	O
manifold	O
structure	O
.	O
in	O
ﬁgure	O
,	O
we	O
will	O
see	O
how	O
a	O
machine	O
learning	O
algorithm	O
can	O
successfully	O
accomplish	O
this	O
goal	O
.	O
20.6	O
5.13	O
this	O
concludes	O
part	O
,	O
which	O
has	O
provided	O
the	O
basic	O
concepts	O
in	O
mathematics	O
and	O
machine	O
learning	O
which	O
are	O
employed	O
throughout	O
the	O
remaining	O
parts	O
of	O
the	O
book	O
.	O
you	O
are	O
now	O
prepared	O
to	O
embark	O
upon	O
your	O
study	O
of	O
deep	O
learning	O
.	O
i	O
164	O
chapter	O
5.	O
machine	O
learning	O
basics	O
gong	O
et	O
al	O
.	O
2000	O
)	O
figure	O
5.13	O
:	O
training	O
examples	O
from	O
the	O
qmul	O
multiview	O
face	O
dataset	O
(	O
for	O
which	O
the	O
subjects	O
were	O
asked	O
to	O
move	O
in	O
such	O
a	O
way	O
as	O
to	O
cover	O
the	O
two-dimensional	O
manifold	O
corresponding	O
to	O
two	O
angles	O
of	O
rotation	O
.	O
we	O
would	O
like	O
learning	O
algorithms	O
to	O
be	O
able	O
to	O
discover	O
and	O
disentangle	O
such	O
manifold	O
coordinates	O
.	O
figure	O
illustrates	O
such	O
a	O
feat	O
.	O
20.6	O
,	O
165	O
part	O
ii	O
deep	O
networks	O
:	O
modern	O
practices	O
166	O
this	O
part	O
of	O
the	O
book	O
summarizes	O
the	O
state	O
of	O
modern	O
deep	O
learning	O
as	O
it	O
is	O
used	O
to	O
solve	O
practical	O
applications	O
.	O
deep	O
learning	O
has	O
a	O
long	O
history	O
and	O
many	O
aspirations	O
.	O
several	O
approaches	O
have	O
been	O
proposed	O
that	O
have	O
yet	O
to	O
entirely	O
bear	O
fruit	O
.	O
several	O
ambitious	O
goals	O
have	O
yet	O
to	O
be	O
realized	O
.	O
these	O
less-developed	O
branches	O
of	O
deep	O
learning	O
appear	O
in	O
the	O
ﬁnal	O
part	O
of	O
the	O
book	O
.	O
this	O
part	O
focuses	O
only	O
on	O
those	O
approaches	O
that	O
are	O
essentially	O
working	O
tech-	O
nologies	O
that	O
are	O
already	O
used	O
heavily	O
in	O
industry	O
.	O
modern	O
deep	O
learning	O
provides	O
a	O
very	O
powerful	O
framework	O
for	O
supervised	O
learning	O
.	O
by	O
adding	O
more	O
layers	O
and	O
more	O
units	O
within	O
a	O
layer	O
,	O
a	O
deep	O
network	O
can	O
represent	O
functions	O
of	O
increasing	O
complexity	O
.	O
most	O
tasks	O
that	O
consist	O
of	O
mapping	O
an	O
input	O
vector	O
to	O
an	O
output	O
vector	O
,	O
and	O
that	O
are	O
easy	O
for	O
a	O
person	O
to	O
do	O
rapidly	O
,	O
can	O
be	O
accomplished	O
via	O
deep	O
learning	O
,	O
given	O
suﬃciently	O
large	O
models	O
and	O
suﬃciently	O
large	O
datasets	O
of	O
labeled	O
training	O
examples	O
.	O
other	O
tasks	O
,	O
that	O
can	O
not	O
be	O
described	O
as	O
associating	O
one	O
vector	O
to	O
another	O
,	O
or	O
that	O
are	O
diﬃcult	O
enough	O
that	O
a	O
person	O
would	O
require	O
time	O
to	O
think	O
and	O
reﬂect	O
in	O
order	O
to	O
accomplish	O
the	O
task	O
,	O
remain	O
beyond	O
the	O
scope	O
of	O
deep	O
learning	O
for	O
now	O
.	O
this	O
part	O
of	O
the	O
book	O
describes	O
the	O
core	O
parametric	O
function	O
approximation	O
technology	O
that	O
is	O
behind	O
nearly	O
all	O
modern	O
practical	O
applications	O
of	O
deep	O
learning	O
.	O
we	O
begin	O
by	O
describing	O
the	O
feedforward	O
deep	O
network	O
model	B
that	O
is	O
used	O
to	O
represent	O
these	O
functions	O
.	O
next	O
,	O
we	O
present	O
advanced	O
techniques	O
for	O
regularization	O
and	O
optimization	O
of	O
such	O
models	O
.	O
scaling	O
these	O
models	O
to	O
large	O
inputs	O
such	O
as	O
high	O
resolution	O
images	O
or	O
long	O
temporal	O
sequences	O
requires	O
specialization	O
.	O
we	O
introduce	O
the	O
convolutional	O
network	O
for	O
scaling	O
to	O
large	O
images	O
and	O
the	O
recurrent	O
neural	O
network	O
for	O
processing	O
temporal	O
sequences	O
.	O
finally	O
,	O
we	O
present	O
general	O
guidelines	O
for	O
the	O
practical	O
methodology	O
involved	O
in	O
designing	O
,	O
building	O
,	O
and	O
conﬁguring	O
an	O
application	O
involving	O
deep	O
learning	O
,	O
and	O
review	O
some	O
of	O
the	O
applications	O
of	O
deep	O
learning	O
.	O
these	O
chapters	O
are	O
the	O
most	O
important	O
for	O
a	O
practitioner—someone	O
who	O
wants	O
to	O
begin	O
implementing	O
and	O
using	O
deep	O
learning	O
algorithms	O
to	O
solve	O
real-world	O
problems	O
today	O
.	O
167	O
chapter	O
6	O
deep	O
feedforward	O
networks	O
deep	O
feedforward	O
networks	O
,	O
also	O
often	O
called	O
feedforward	O
neural	O
networks	O
,	O
or	O
multilayer	O
perceptrons	O
(	O
mlps	O
)	O
,	O
are	O
the	O
quintessential	O
deep	O
learning	O
models	O
.	O
the	O
goal	O
of	O
a	O
feedforward	O
network	O
is	O
to	O
approximate	O
some	O
function	O
f	O
.	O
for	O
example	O
,	O
for	O
a	O
classiﬁer	O
,	O
y	O
=	O
f	O
(	O
x	O
)	O
maps	O
an	O
input	O
x	O
to	O
a	O
category	O
y.	O
a	O
feedforward	O
network	O
deﬁnes	O
a	O
mapping	O
y	O
=	O
f	O
(	O
x	O
;	O
θ	O
)	O
and	O
learns	O
the	O
value	O
of	O
the	O
parameters	O
θ	O
that	O
result	O
in	O
the	O
best	O
function	O
approximation	O
.	O
∗	O
∗	O
these	O
models	O
are	O
called	O
feedforward	O
because	O
information	O
ﬂows	O
through	O
the	O
function	O
being	O
evaluated	O
from	O
x	O
,	O
through	O
the	O
intermediate	O
computations	O
used	O
to	O
deﬁne	O
f	O
,	O
and	O
ﬁnally	O
to	O
the	O
output	O
y.	O
there	O
are	O
no	O
feedback	O
connections	O
in	O
which	O
outputs	O
of	O
the	O
model	B
are	O
fed	O
back	O
into	O
itself	O
.	O
when	O
feedforward	O
neural	O
networks	O
are	O
extended	O
to	O
include	O
feedback	O
connections	O
,	O
they	O
are	O
called	O
recurrent	O
neural	O
networks	O
,	O
presented	O
in	O
chapter	O
.10	O
feedforward	O
networks	O
are	O
of	O
extreme	O
importance	O
to	O
machine	O
learning	O
practi-	O
tioners	O
.	O
they	O
form	O
the	O
basis	O
of	O
many	O
important	O
commercial	O
applications	O
.	O
for	O
example	O
,	O
the	O
convolutional	O
networks	O
used	O
for	O
object	O
recognition	B
from	O
photos	O
are	O
a	O
specialized	O
kind	O
of	O
feedforward	O
network	O
.	O
feedforward	O
networks	O
are	O
a	O
conceptual	O
stepping	O
stone	O
on	O
the	O
path	O
to	O
recurrent	O
networks	O
,	O
which	O
power	O
many	O
natural	O
language	O
applications	O
.	O
feedforward	O
neural	O
networks	O
are	O
called	O
networks	O
because	O
they	O
are	O
typically	O
represented	O
by	O
composing	O
together	O
many	O
diﬀerent	O
functions	O
.	O
the	O
model	B
is	O
asso-	O
ciated	O
with	O
a	O
directed	O
acyclic	O
graph	O
describing	O
how	O
the	O
functions	O
are	O
composed	O
together	O
.	O
for	O
example	O
,	O
we	O
might	O
have	O
three	O
functions	O
f	O
(	O
1	O
)	O
,	O
f	O
(	O
2	O
)	O
,	O
and	O
f	O
(	O
3	O
)	O
connected	O
in	O
a	O
chain	O
,	O
to	O
form	O
f	O
(	O
x	O
)	O
=	O
f	O
(	O
3	O
)	O
(	O
f	O
(	O
2	O
)	O
(	O
f	O
(	O
1	O
)	O
(	O
x	O
)	O
)	O
)	O
.	O
these	O
chain	O
structures	O
are	O
the	O
most	O
commonly	O
used	O
structures	O
of	O
neural	O
networks	O
.	O
in	O
this	O
case	O
,	O
f	O
(	O
1	O
)	O
is	O
called	O
the	O
ﬁrst	O
layer	O
of	O
the	O
network	O
,	O
f	O
(	O
2	O
)	O
is	O
called	O
the	O
second	O
layer	O
,	O
and	O
so	O
on	O
.	O
the	O
overall	O
168	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
length	O
of	O
the	O
chain	O
gives	O
the	O
depth	O
of	O
the	O
model	B
.	O
it	O
is	O
from	O
this	O
terminology	O
that	O
the	O
name	O
“	O
deep	O
learning	O
”	O
arises	O
.	O
the	O
ﬁnal	O
layer	O
of	O
a	O
feedforward	O
network	O
is	O
called	O
∗	O
the	O
output	O
layer	O
.	O
during	O
neural	O
network	O
training	O
,	O
we	O
drive	O
f	O
(	O
x	O
)	O
to	O
match	O
f	O
(	O
x	O
)	O
.	O
∗	O
≈	O
∗	O
(	O
x	O
)	O
evaluated	O
the	O
training	O
data	O
provides	O
us	O
with	O
noisy	O
,	O
approximate	O
examples	O
of	O
f	O
at	O
diﬀerent	O
training	O
points	O
.	O
each	O
example	O
x	O
is	O
accompanied	O
by	O
a	O
label	O
y	O
(	O
x	O
)	O
.	O
f	O
the	O
training	O
examples	O
specify	O
directly	O
what	O
the	O
output	O
layer	O
must	O
do	O
at	O
each	O
point	O
x	O
;	O
it	O
must	O
produce	O
a	O
value	O
that	O
is	O
close	O
to	O
y.	O
the	O
behavior	O
of	O
the	O
other	O
layers	O
is	O
not	O
directly	O
speciﬁed	O
by	O
the	O
training	O
data	O
.	O
the	O
learning	O
algorithm	O
must	O
decide	O
how	O
to	O
use	O
those	O
layers	O
to	O
produce	O
the	O
desired	O
output	O
,	O
but	O
the	O
training	O
data	O
does	O
not	O
say	O
what	O
each	O
individual	O
layer	O
should	O
do	O
.	O
instead	O
,	O
the	O
learning	O
algorithm	O
must	O
decide	O
how	O
to	O
use	O
these	O
layers	O
to	O
best	O
implement	O
an	O
approximation	O
of	O
f	O
.	O
because	O
the	O
training	O
data	O
does	O
not	O
show	O
the	O
desired	O
output	O
for	O
each	O
of	O
these	O
layers	O
,	O
these	O
layers	O
are	O
called	O
hidden	O
layers	O
.	O
∗	O
finally	O
,	O
these	O
networks	O
are	O
called	O
neural	O
because	O
they	O
are	O
loosely	O
inspired	O
by	O
neuroscience	O
.	O
each	O
hidden	O
layer	O
of	O
the	O
network	O
is	O
typically	O
vector-valued	O
.	O
the	O
dimensionality	O
of	O
these	O
hidden	O
layers	O
determines	O
the	O
width	O
of	O
the	O
model	B
.	O
each	O
element	O
of	O
the	O
vector	O
may	O
be	O
interpreted	O
as	O
playing	O
a	O
role	O
analogous	O
to	O
a	O
neuron	O
.	O
rather	O
than	O
thinking	O
of	O
the	O
layer	O
as	O
representing	O
a	O
single	O
vector-to-vector	O
function	O
,	O
we	O
can	O
also	O
think	O
of	O
the	O
layer	O
as	O
consisting	O
of	O
many	O
units	O
that	O
act	O
in	O
parallel	O
,	O
each	O
representing	O
a	O
vector-to-scalar	O
function	O
.	O
each	O
unit	O
resembles	O
a	O
neuron	O
in	O
the	O
sense	O
that	O
it	O
receives	O
input	O
from	O
many	O
other	O
units	O
and	O
computes	O
its	O
own	O
activation	O
value	O
.	O
the	O
idea	O
of	O
using	O
many	O
layers	O
of	O
vector-valued	O
representation	O
is	O
drawn	O
from	O
neuroscience	O
.	O
the	O
choice	O
of	O
the	O
functions	O
f	O
(	O
)	O
i	O
(	O
x	O
)	O
used	O
to	O
compute	O
these	O
representations	O
is	O
also	O
loosely	O
guided	O
by	O
neuroscientiﬁc	O
observations	O
about	O
the	O
functions	O
that	O
biological	O
neurons	O
compute	O
.	O
however	O
,	O
modern	O
neural	O
network	O
research	O
is	O
guided	O
by	O
many	O
mathematical	O
and	O
engineering	O
disciplines	O
,	O
and	O
the	O
goal	O
of	O
neural	O
networks	O
is	O
not	O
to	O
perfectly	O
model	B
the	O
brain	O
.	O
it	O
is	O
best	O
to	O
think	O
of	O
feedforward	O
networks	O
as	O
function	O
approximation	O
machines	O
that	O
are	O
designed	O
to	O
achieve	O
statistical	O
generalization	O
,	O
occasionally	O
drawing	O
some	O
insights	O
from	O
what	O
we	O
know	O
about	O
the	O
brain	O
,	O
rather	O
than	O
as	O
models	O
of	O
brain	O
function	O
.	O
one	O
way	O
to	O
understand	O
feedforward	O
networks	O
is	O
to	O
begin	O
with	O
linear	O
models	O
and	O
consider	O
how	O
to	O
overcome	O
their	O
limitations	O
.	O
linear	O
models	O
,	O
such	O
as	O
logistic	O
regression	O
and	O
linear	O
regression	O
,	O
are	O
appealing	O
because	O
they	O
may	O
be	O
ﬁt	O
eﬃciently	O
and	O
reliably	O
,	O
either	O
in	O
closed	O
form	O
or	O
with	O
convex	O
optimization	O
.	O
linear	O
models	O
also	O
have	O
the	O
obvious	O
defect	O
that	O
the	O
model	B
capacity	O
is	O
limited	O
to	O
linear	O
functions	O
,	O
so	O
the	O
model	B
can	O
not	O
understand	O
the	O
interaction	O
between	O
any	O
two	O
input	O
variables	O
.	O
to	O
extend	O
linear	O
models	O
to	O
represent	O
nonlinear	O
functions	O
of	O
x	O
,	O
we	O
can	O
apply	O
the	O
linear	O
model	B
not	O
to	O
x	O
itself	O
but	O
to	O
a	O
transformed	O
input	O
φ	O
(	O
x	O
)	O
,	O
where	O
φ	O
is	O
a	O
169	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
5.7.2	O
nonlinear	O
transformation	O
.	O
equivalently	O
,	O
we	O
can	O
apply	O
the	O
kernel	O
trick	B
described	O
in	O
section	O
,	O
to	O
obtain	O
a	O
nonlinear	O
learning	O
algorithm	O
based	O
on	O
implicitly	O
applying	O
the	O
φ	O
mapping	O
.	O
we	O
can	O
think	O
of	O
φ	O
as	O
providing	O
a	O
set	O
of	O
features	O
describing	O
x	O
,	O
or	O
as	O
providing	O
a	O
new	O
representation	O
for	O
.x	O
the	O
question	O
is	O
then	O
how	O
to	O
choose	O
the	O
mapping	O
.φ	O
1.	O
one	O
option	O
is	O
to	O
use	O
a	O
very	O
generic	O
φ	O
,	O
such	O
as	O
the	O
inﬁnite-dimensional	O
φ	O
that	O
is	O
implicitly	O
used	O
by	O
kernel	O
machines	O
based	O
on	O
the	O
rbf	O
kernel	O
.	O
if	O
φ	O
(	O
x	O
)	O
is	O
of	O
high	O
enough	O
dimension	O
,	O
we	O
can	O
always	O
have	O
enough	O
capacity	O
to	O
ﬁt	O
the	O
training	O
set	O
,	O
but	O
generalization	O
to	O
the	O
test	O
set	O
often	O
remains	O
poor	O
.	O
very	O
generic	O
feature	O
mappings	O
are	O
usually	O
based	O
only	O
on	O
the	O
principle	O
of	O
local	O
smoothness	O
and	O
do	O
not	O
encode	O
enough	O
prior	O
information	O
to	O
solve	O
advanced	O
problems	O
.	O
2.	O
another	O
option	O
is	O
to	O
manually	O
engineer	O
φ.	O
until	O
the	O
advent	O
of	O
deep	O
learning	O
,	O
this	O
was	O
the	O
dominant	O
approach	O
.	O
this	O
approach	O
requires	O
decades	O
of	O
human	O
eﬀort	O
for	O
each	O
separate	O
task	O
,	O
with	O
practitioners	O
specializing	O
in	O
diﬀerent	O
domains	O
such	O
as	O
speech	O
recognition	B
or	O
computer	O
vision	O
,	O
and	O
with	O
little	O
transfer	O
between	O
domains	O
.	O
	O
)	O
=	O
φ	O
(	O
x	O
;	O
θ	O
)	O
3.	O
the	O
strategy	O
of	O
deep	O
learning	O
is	O
to	O
learn	O
φ.	O
in	O
this	O
approach	O
,	O
we	O
have	O
a	O
model	B
y	O
=	O
f	O
(	O
x	O
;	O
θ	O
w	O
,	O
w.	O
we	O
now	O
have	O
parameters	O
θ	O
that	O
we	O
use	O
to	O
learn	O
φ	O
from	O
a	O
broad	O
class	O
of	O
functions	O
,	O
and	O
parameters	O
w	O
that	O
map	O
from	O
φ	O
(	O
x	O
)	O
to	O
the	O
desired	O
output	O
.	O
this	O
is	O
an	O
example	O
of	O
a	O
deep	O
feedforward	O
network	O
,	O
with	O
φ	O
deﬁning	O
a	O
hidden	O
layer	O
.	O
this	O
approach	O
is	O
the	O
only	O
one	O
of	O
the	O
three	O
that	O
gives	O
up	O
on	O
the	O
convexity	O
of	O
the	O
training	O
problem	O
,	O
but	O
the	O
beneﬁts	O
outweigh	O
the	O
harms	O
.	O
in	O
this	O
approach	O
,	O
we	O
parametrize	O
the	O
representation	O
as	O
φ	O
(	O
x	O
;	O
θ	O
)	O
and	O
use	O
the	O
optimization	O
algorithm	O
to	O
ﬁnd	O
the	O
θ	O
that	O
corresponds	O
to	O
a	O
good	O
representation	O
.	O
if	O
we	O
wish	O
,	O
this	O
approach	O
can	O
capture	O
the	O
beneﬁt	O
of	O
the	O
ﬁrst	O
approach	O
by	O
being	O
highly	O
generic—we	O
do	O
so	O
by	O
using	O
a	O
very	O
broad	O
family	O
φ	O
(	O
x	O
;	O
θ	O
)	O
.	O
this	O
approach	O
can	O
also	O
capture	O
the	O
beneﬁt	O
of	O
the	O
second	O
approach	O
.	O
human	O
practitioners	O
can	O
encode	O
their	O
knowledge	O
to	O
help	O
generalization	O
by	O
designing	O
families	O
φ	O
(	O
x	O
;	O
θ	O
)	O
that	O
they	O
expect	O
will	O
perform	O
well	O
.	O
the	O
advantage	O
is	O
that	O
the	O
human	O
designer	O
only	O
needs	O
to	O
ﬁnd	O
the	O
right	O
general	O
function	O
family	O
rather	O
than	O
ﬁnding	O
precisely	O
the	O
right	O
function	O
.	O
this	O
general	O
principle	O
of	O
improving	O
models	O
by	O
learning	O
features	O
extends	O
beyond	O
the	O
feedforward	O
networks	O
described	O
in	O
this	O
chapter	O
.	O
it	O
is	O
a	O
recurring	O
theme	O
of	O
deep	O
learning	O
that	O
applies	O
to	O
all	O
of	O
the	O
kinds	O
of	O
models	O
described	O
throughout	O
this	O
book	O
.	O
feedforward	O
networks	O
are	O
the	O
application	O
of	O
this	O
principle	O
to	O
learning	O
deterministic	O
170	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
mappings	O
from	O
x	O
to	O
y	O
that	O
lack	O
feedback	O
connections	O
.	O
other	O
models	O
presented	O
later	O
will	O
apply	O
these	O
principles	O
to	O
learning	O
stochastic	O
mappings	O
,	O
learning	O
functions	O
with	O
feedback	O
,	O
and	O
learning	O
probability	O
distributions	O
over	O
a	O
single	O
vector	O
.	O
we	O
begin	O
this	O
chapter	O
with	O
a	O
simple	O
example	O
of	O
a	O
feedforward	O
network	O
.	O
next	O
,	O
we	O
address	O
each	O
of	O
the	O
design	O
decisions	O
needed	O
to	O
deploy	O
a	O
feedforward	O
network	O
.	O
first	O
,	O
training	O
a	O
feedforward	O
network	O
requires	O
making	O
many	O
of	O
the	O
same	O
design	O
decisions	O
as	O
are	O
necessary	O
for	O
a	O
linear	O
model	B
:	O
choosing	O
the	O
optimizer	O
,	O
the	O
cost	O
function	O
,	O
and	O
the	O
form	O
of	O
the	O
output	O
units	O
.	O
we	O
review	O
these	O
basics	O
of	O
gradient-based	O
learning	O
,	O
then	O
proceed	O
to	O
confront	O
some	O
of	O
the	O
design	O
decisions	O
that	O
are	O
unique	O
to	O
feedforward	O
networks	O
.	O
feedforward	O
networks	O
have	O
introduced	O
the	O
concept	O
of	O
a	O
hidden	O
layer	O
,	O
and	O
this	O
requires	O
us	O
to	O
choose	O
the	O
activation	O
functions	O
that	O
will	O
be	O
used	O
to	O
compute	O
the	O
hidden	O
layer	O
values	O
.	O
we	O
must	O
also	O
design	O
the	O
architecture	O
of	O
the	O
network	O
,	O
including	O
how	O
many	O
layers	O
the	O
network	O
should	O
contain	O
,	O
how	O
these	O
layers	O
should	O
be	O
connected	O
to	O
each	O
other	O
,	O
and	O
how	O
many	O
units	O
should	O
be	O
in	O
each	O
layer	O
.	O
learning	O
in	O
deep	O
neural	O
networks	O
requires	O
computing	O
the	O
gradients	O
of	O
complicated	O
functions	O
.	O
we	O
present	O
the	O
back-propagation	O
algorithm	O
and	O
its	O
modern	O
generalizations	O
,	O
which	O
can	O
be	O
used	O
to	O
eﬃciently	O
compute	O
these	O
gradients	O
.	O
finally	O
,	O
we	O
close	O
with	O
some	O
historical	O
perspective	O
.	O
6.1	O
example	O
:	O
learning	O
xor	O
to	O
make	O
the	O
idea	O
of	O
a	O
feedforward	O
network	O
more	O
concrete	O
,	O
we	O
begin	O
with	O
an	O
example	O
of	O
a	O
fully	O
functioning	O
feedforward	O
network	O
on	O
a	O
very	O
simple	O
task	O
:	O
learning	O
the	O
xor	O
function	O
.	O
the	O
xor	O
function	O
(	O
“	O
exclusive	O
or	O
”	O
)	O
is	O
an	O
operation	O
on	O
two	O
binary	O
values	O
,	O
x1	O
and	O
x2	O
.	O
when	O
exactly	O
one	O
of	O
these	O
binary	O
values	O
is	O
equal	O
to	O
,	O
the	O
xor	O
function	O
.	O
otherwise	O
,	O
it	O
returns	O
0.	O
the	O
xor	O
function	O
provides	O
the	O
target	O
function	O
returns	O
1	O
∗	O
y	O
=	O
f	O
(	O
x	O
)	O
that	O
we	O
want	O
to	O
learn	O
.	O
our	O
model	B
provides	O
a	O
function	O
y	O
=	O
f	O
(	O
x	O
;	O
θ	O
)	O
and	O
our	O
learning	O
algorithm	O
will	O
adapt	O
the	O
parameters	O
θ	O
to	O
make	O
f	O
as	O
similar	O
as	O
possible	O
∗	O
to	O
f	O
1	O
.	O
in	O
this	O
simple	O
example	O
,	O
we	O
will	O
not	O
be	O
concerned	O
with	O
statistical	O
generalization	O
.	O
	O
,	O
	O
.	O
we	O
will	O
train	O
the	O
network	O
on	O
all	O
four	O
of	O
these	O
points	O
.	O
the	O
we	O
want	O
our	O
network	O
to	O
perform	O
correctly	O
on	O
the	O
four	O
points	O
x	O
=	O
[	O
1	O
,	O
0	O
]	O
only	O
challenge	O
is	O
to	O
ﬁt	O
the	O
training	O
set	O
.	O
,	O
and	O
[	O
1	O
,	O
1	O
]	O
	O
}	O
[	O
0	O
,	O
0	O
]	O
,	O
[	O
0,1	O
]	O
{	O
	O
we	O
can	O
treat	O
this	O
problem	O
as	O
a	O
regression	O
problem	O
and	O
use	O
a	O
mean	O
squared	O
error	O
loss	O
function	O
.	O
we	O
choose	O
this	O
loss	O
function	O
to	O
simplify	O
the	O
math	O
for	O
this	O
example	O
as	O
much	O
as	O
possible	O
.	O
in	O
practical	O
applications	O
,	O
mse	O
is	O
usually	O
not	O
an	O
171	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
appropriate	O
cost	O
function	O
for	O
modeling	O
binary	O
data	O
.	O
more	O
appropriate	O
approaches	O
are	O
described	O
in	O
section	O
.	O
6.2.2.2	O
	O
evaluated	O
on	O
our	O
whole	O
training	O
set	O
,	O
the	O
mse	O
loss	O
function	O
is	O
−	O
∗	O
(	O
f	O
(	O
)	O
x	O
f	O
x	O
θ	O
2	O
.	O
(	O
;	O
)	O
)	O
j	O
(	O
)	O
=θ	O
1	O
4	O
∈	O
x	O
x	O
(	O
6.1	O
)	O
now	O
we	O
must	O
choose	O
the	O
form	O
of	O
our	O
model	B
,	O
f	O
(	O
x	O
;	O
θ	O
)	O
.	O
suppose	O
that	O
we	O
choose	O
a	O
linear	O
model	B
,	O
with	O
θ	O
consisting	O
of	O
w	O
and	O
.	O
our	O
model	B
is	O
deﬁned	O
to	O
be	O
b	O
	O
w	O
+	O
b.	O
f	O
(	O
;	O
x	O
w	O
)	O
=	O
x	O
,	O
b	O
(	O
6.2	O
)	O
we	O
can	O
minimize	O
j	O
(	O
θ	O
)	O
in	O
closed	O
form	O
with	O
respect	O
to	O
w	O
and	O
b	O
using	O
the	O
normal	O
equations	O
.	O
after	O
solving	O
the	O
normal	O
equations	O
,	O
we	O
obtain	O
w	O
=	O
0	O
and	O
b	O
=	O
1	O
2	O
.	O
the	O
linear	O
model	B
simply	O
outputs	O
0.5	O
everywhere	O
.	O
why	O
does	O
this	O
happen	O
?	O
figure	O
shows	O
how	O
a	O
linear	O
model	B
is	O
not	O
able	O
to	O
represent	O
the	O
xor	O
function	O
.	O
one	O
way	O
to	O
solve	O
this	O
problem	O
is	O
to	O
use	O
a	O
model	B
that	O
learns	O
a	O
diﬀerent	O
feature	O
space	O
in	O
which	O
a	O
linear	O
model	B
is	O
able	O
to	O
represent	O
the	O
solution	O
.	O
6.1	O
speciﬁcally	O
,	O
we	O
will	O
introduce	O
a	O
very	O
simple	O
feedforward	O
network	O
with	O
one	O
hidden	O
layer	O
containing	O
two	O
hidden	O
units	O
.	O
see	O
ﬁgure	O
for	O
an	O
illustration	O
of	O
this	O
model	B
.	O
this	O
feedforward	O
network	O
has	O
a	O
vector	O
of	O
hidden	O
units	O
h	O
that	O
are	O
computed	O
by	O
a	O
function	O
f	O
(	O
1	O
)	O
(	O
x	O
;	O
w	O
c	O
,	O
)	O
.	O
the	O
values	O
of	O
these	O
hidden	O
units	O
are	O
then	O
used	O
as	O
the	O
input	O
for	O
a	O
second	O
layer	O
.	O
the	O
second	O
layer	O
is	O
the	O
output	O
layer	O
of	O
the	O
network	O
.	O
the	O
output	O
layer	O
is	O
still	O
just	O
a	O
linear	O
regression	O
model	B
,	O
but	O
now	O
it	O
is	O
applied	O
to	O
h	O
rather	O
than	O
to	O
x	O
.	O
the	O
network	O
now	O
contains	O
two	O
functions	O
chained	O
together	O
:	O
h	O
=	O
f	O
(	O
1	O
)	O
(	O
x	O
;	O
w	O
c	O
,	O
)	O
and	O
y	O
=	O
f	O
(	O
2	O
)	O
(	O
h	O
;	O
w	O
,	O
b	O
)	O
,	O
with	O
the	O
complete	O
model	B
being	O
f	O
(	O
;	O
x	O
w	O
c	O
w	O
)	O
=	O
(	O
2	O
)	O
(	O
f	O
(	O
1	O
)	O
(	O
)	O
)	O
x	O
.	O
6.2	O
,	O
b	O
f	O
,	O
,	O
what	O
function	O
should	O
f	O
(	O
1	O
)	O
compute	O
?	O
linear	O
models	O
have	O
served	O
us	O
well	O
so	O
far	O
,	O
and	O
it	O
may	O
be	O
tempting	O
to	O
make	O
f	O
(	O
1	O
)	O
be	O
linear	O
as	O
well	O
.	O
unfortunately	O
,	O
if	O
f	O
(	O
1	O
)	O
were	O
linear	O
,	O
then	O
the	O
feedforward	O
network	O
as	O
a	O
whole	O
would	O
remain	O
a	O
linear	O
function	O
of	O
its	O
input	O
.	O
ignoring	O
the	O
intercept	O
terms	O
for	O
the	O
moment	O
,	O
suppose	O
f	O
(	O
1	O
)	O
(	O
x	O
)	O
=	O
w	O
x	O
and	O
f	O
(	O
2	O
)	O
(	O
h	O
)	O
=	O
h	O
x.	O
we	O
could	O
represent	O
this	O
function	O
as	O
f	O
(	O
)	O
=	O
w.	O
then	O
f	O
(	O
x	O
)	O
=	O
w	O
	O
where	O
w	O
	O
w	O
=	O
w	O
w.	O
	O
x	O
	O
w	O
	O
	O
	O
x	O
clearly	O
,	O
we	O
must	O
use	O
a	O
nonlinear	O
function	O
to	O
describe	O
the	O
features	O
.	O
most	O
neural	O
networks	O
do	O
so	O
using	O
an	O
aﬃne	O
transformation	O
controlled	O
by	O
learned	O
parameters	O
,	O
followed	O
by	O
a	O
ﬁxed	O
,	O
nonlinear	O
function	O
called	O
an	O
activation	O
function	O
.	O
we	O
use	O
that	O
strategy	O
here	O
,	O
by	O
deﬁning	O
h	O
=	O
g	O
(	O
w	O
x	O
+	O
c	O
)	O
,	O
where	O
w	O
provides	O
the	O
weights	O
of	O
a	O
linear	O
transformation	O
and	O
c	O
the	O
biases	O
.	O
previously	O
,	O
to	O
describe	O
a	O
linear	O
regression	O
	O
172	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
original	O
x	O
space	O
learned	O
h	O
space	O
1	O
0	O
2	O
x	O
1	O
0	O
2	O
h	O
0	O
1	O
0	O
x1	O
1	O
h	O
1	O
2	O
figure	O
6.1	O
:	O
solving	O
the	O
xor	O
problem	O
by	O
learning	O
a	O
representation	O
.	O
the	O
bold	O
numbers	O
printed	O
on	O
the	O
plot	O
indicate	O
the	O
value	O
that	O
the	O
learned	O
function	O
must	O
output	O
at	O
each	O
point	O
.	O
(	O
left	O
)	O
a	O
linear	O
model	B
applied	O
directly	O
to	O
the	O
original	O
input	O
can	O
not	O
implement	O
the	O
xor	O
function	O
.	O
when	O
x1	O
=	O
0	O
,	O
the	O
model	B
’	O
s	O
output	O
must	O
increase	O
as	O
x2	O
increases	O
.	O
when	O
x1	O
=	O
1	O
,	O
the	O
model	B
’	O
s	O
output	O
must	O
decrease	O
as	O
x2	O
increases	O
.	O
a	O
linear	O
model	B
must	O
apply	O
a	O
ﬁxed	O
coeﬃcient	O
w2	O
to	O
x2	O
.	O
the	O
linear	O
model	B
therefore	O
can	O
not	O
use	O
the	O
value	O
of	O
x1	O
to	O
change	O
the	O
coeﬃcient	O
on	O
x2	O
and	O
can	O
not	O
solve	O
this	O
problem	O
.	O
(	O
right	O
)	O
in	O
the	O
transformed	O
space	O
represented	O
by	O
the	O
features	O
extracted	O
by	O
a	O
neural	O
network	O
,	O
a	O
linear	O
model	B
can	O
now	O
solve	O
have	O
been	O
the	O
problem	O
.	O
in	O
our	O
example	O
solution	O
,	O
the	O
two	O
points	O
that	O
must	O
have	O
output	O
collapsed	O
into	O
a	O
single	O
point	O
in	O
feature	O
space	O
.	O
in	O
other	O
words	O
,	O
the	O
nonlinear	O
features	O
have	O
	O
mapped	O
both	O
x	O
=	O
[	O
1	O
,	O
0	O
]	O
.	O
the	O
linear	O
model	B
can	O
now	O
describe	O
the	O
function	O
as	O
increasing	O
in	O
h1	O
and	O
decreasing	O
in	O
h2	O
.	O
in	O
this	O
example	O
,	O
the	O
motivation	O
for	O
learning	O
the	O
feature	O
space	O
is	O
only	O
to	O
make	O
the	O
model	B
capacity	O
greater	O
so	O
that	O
it	O
can	O
ﬁt	O
the	O
training	O
set	O
.	O
in	O
more	O
realistic	O
applications	O
,	O
learned	O
representations	O
can	O
also	O
help	O
the	O
model	B
to	O
generalize	O
.	O
to	O
a	O
single	O
point	O
in	O
feature	O
space	O
,	O
h	O
=	O
[	O
1	O
,0	O
]	O
	O
and	O
x	O
=	O
[	O
0	O
,	O
1	O
]	O
	O
1	O
173	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
yy	O
h1h1	O
x1x1	O
h2h2	O
x2x2	O
yy	O
hh	O
xx	O
w	O
w	O
figure	O
6.2	O
:	O
an	O
example	O
of	O
a	O
feedforward	O
network	O
,	O
drawn	O
in	O
two	O
diﬀerent	O
styles	O
.	O
speciﬁcally	O
,	O
this	O
is	O
the	O
feedforward	O
network	O
we	O
use	O
to	O
solve	O
the	O
xor	O
example	O
.	O
it	O
has	O
a	O
single	O
hidden	O
layer	O
containing	O
two	O
units	O
.	O
(	O
left	O
)	O
in	O
this	O
style	O
,	O
we	O
draw	O
every	O
unit	O
as	O
a	O
node	O
in	O
the	O
graph	O
.	O
this	O
style	O
is	O
very	O
explicit	O
and	O
unambiguous	O
but	O
for	O
networks	O
larger	O
than	O
this	O
example	O
it	O
can	O
consume	O
too	O
much	O
space	O
.	O
in	O
this	O
style	O
,	O
we	O
draw	O
a	O
node	O
in	O
the	O
graph	O
for	O
each	O
entire	O
vector	O
representing	O
a	O
layer	O
’	O
s	O
activations	O
.	O
this	O
style	O
is	O
much	O
more	O
compact	O
.	O
sometimes	O
we	O
annotate	O
the	O
edges	O
in	O
this	O
graph	O
with	O
the	O
name	O
of	O
the	O
parameters	O
that	O
describe	O
the	O
relationship	O
between	O
two	O
layers	O
.	O
here	O
,	O
we	O
indicate	O
that	O
a	O
matrix	O
w	O
describes	O
the	O
mapping	O
from	O
x	O
to	O
h	O
,	O
and	O
a	O
vector	O
w	O
describes	O
the	O
mapping	O
from	O
h	O
to	O
y.	O
we	O
typically	O
omit	O
the	O
intercept	O
parameters	O
associated	O
with	O
each	O
layer	O
when	O
labeling	O
this	O
kind	O
of	O
drawing	O
.	O
(	O
right	O
)	O
model	B
,	O
we	O
used	O
a	O
vector	O
of	O
weights	O
and	O
a	O
scalar	O
bias	O
parameter	O
to	O
describe	O
an	O
aﬃne	O
transformation	O
from	O
an	O
input	O
vector	O
to	O
an	O
output	O
scalar	O
.	O
now	O
,	O
we	O
describe	O
an	O
aﬃne	O
transformation	O
from	O
a	O
vector	O
x	O
to	O
a	O
vector	O
h	O
,	O
so	O
an	O
entire	O
vector	O
of	O
bias	O
parameters	O
is	O
needed	O
.	O
the	O
activation	O
function	O
g	O
is	O
typically	O
chosen	O
to	O
be	O
a	O
function	O
that	O
is	O
applied	O
element-wise	O
,	O
with	O
hi	O
=	O
g	O
(	O
x	O
w	O
:	O
,i	O
+	O
ci	O
)	O
.	O
in	O
modern	O
neural	O
networks	O
,	O
the	O
default	O
recommendation	O
is	O
to	O
use	O
the	O
rectiﬁed	O
linear	O
unit	O
or	O
relu	O
(	O
jarrett	O
et	O
al.	O
)	O
deﬁned	O
by	O
the	O
activation	O
,	O
function	O
2009	O
nair	O
and	O
hinton	O
2010	O
glorot	O
{	O
;	O
(	O
)	O
=	O
max	O
0	O
g	O
z	O
depicted	O
in	O
ﬁgure	O
et	O
al	O
.	O
,	O
6.3	O
.	O
2011a	O
	O
}	O
,	O
z	O
,	O
;	O
we	O
can	O
now	O
specify	O
our	O
complete	O
network	O
as	O
	O
(	O
;	O
x	O
w	O
c	O
w	O
)	O
=	O
w	O
,	O
b	O
,	O
,	O
f	O
{	O
max	O
0	O
,	O
w	O
	O
}	O
x	O
c+	O
+	O
b.	O
we	O
can	O
now	O
specify	O
a	O
solution	O
to	O
the	O
xor	O
problem	O
.	O
let	O
	O
	O
	O
	O
w	O
=	O
1	O
1	O
1	O
1	O
,	O
c	O
=	O
0−	O
1	O
,	O
174	O
(	O
6.3	O
)	O
(	O
6.4	O
)	O
(	O
6.5	O
)	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
}	O
{	O
z	O
,	O
0	O
x	O
a	O
m	O
=	O
)	O
z	O
(	O
g	O
0	O
0	O
z	O
figure	O
6.3	O
:	O
the	O
rectiﬁed	O
linear	O
activation	O
function	O
.	O
this	O
activation	O
function	O
is	O
the	O
default	O
activation	O
function	O
recommended	O
for	O
use	O
with	O
most	O
feedforward	O
neural	O
networks	O
.	O
applying	O
this	O
function	O
to	O
the	O
output	O
of	O
a	O
linear	O
transformation	O
yields	O
a	O
nonlinear	O
transformation	O
.	O
however	O
,	O
the	O
function	O
remains	O
very	O
close	O
to	O
linear	O
,	O
in	O
the	O
sense	O
that	O
is	O
a	O
piecewise	O
linear	O
function	O
with	O
two	O
linear	O
pieces	O
.	O
because	O
rectiﬁed	O
linear	O
units	O
are	O
nearly	O
linear	O
,	O
they	O
preserve	O
many	O
of	O
the	O
properties	O
that	O
make	O
linear	O
models	O
easy	O
to	O
optimize	O
with	O
gradient-	O
based	O
methods	O
.	O
they	O
also	O
preserve	O
many	O
of	O
the	O
properties	O
that	O
make	O
linear	O
models	O
generalize	O
well	O
.	O
a	O
common	O
principle	O
throughout	O
computer	O
science	O
is	O
that	O
we	O
can	O
build	O
complicated	O
systems	O
from	O
minimal	O
components	O
.	O
much	O
as	O
a	O
turing	O
machine	O
’	O
s	O
memory	O
needs	O
only	O
to	O
be	O
able	O
to	O
store	O
0	O
or	O
1	O
states	O
,	O
we	O
can	O
build	O
a	O
universal	O
function	O
approximator	O
from	O
rectiﬁed	O
linear	O
functions	O
.	O
175	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
	O
	O
w	O
=	O
,	O
(	O
6.6	O
)	O
and	O
.	O
b	O
=	O
0	O
we	O
can	O
now	O
walk	O
through	O
the	O
way	O
that	O
the	O
model	B
processes	O
a	O
batch	O
of	O
inputs	O
.	O
let	O
x	O
be	O
the	O
design	O
matrix	O
containing	O
all	O
four	O
points	O
in	O
the	O
binary	O
input	O
space	O
,	O
with	O
one	O
example	O
per	O
row	O
:	O
x	O
=	O
(	O
6.7	O
)	O
the	O
ﬁrst	O
step	O
in	O
the	O
neural	O
network	O
is	O
to	O
multiply	O
the	O
input	O
matrix	O
by	O
the	O
ﬁrst	O
layer	O
’	O
s	O
weight	O
matrix	O
:	O
1−	O
2	O
.	O
0	O
1	O
1	O
0	O
1	O
1	O
	O
0	O
0	O
	O
	O
	O
0	O
0	O
	O
	O
xw	O
=	O
1	O
1	O
1	O
1	O
2	O
2	O
,	O
to	O
obtain	O
−	O
1	O
0	O
0	O
1	O
0	O
1	O
1	O
2	O
.	O
	O
0	O
0	O
	O
next	O
,	O
we	O
add	O
the	O
bias	O
vector	O
c	O
.	O
(	O
6.8	O
)	O
(	O
6.9	O
)	O
in	O
this	O
space	O
,	O
all	O
of	O
the	O
examples	O
lie	O
along	O
a	O
line	O
with	O
slope	O
.	O
as	O
we	O
move	O
along	O
this	O
line	O
,	O
the	O
output	O
needs	O
to	O
begin	O
at	O
,	O
then	O
rise	O
to	O
,	O
then	O
drop	O
back	O
down	O
to	O
.	O
0	O
0	O
a	O
linear	O
model	B
can	O
not	O
implement	O
such	O
a	O
function	O
.	O
to	O
ﬁnish	O
computing	O
the	O
value	O
of	O
for	O
each	O
example	O
,	O
we	O
apply	O
the	O
rectiﬁed	O
linear	O
transformation	O
:	O
h	O
1	O
1	O
.	O
1	O
0	O
1	O
0	O
2	O
1	O
(	O
6.10	O
)	O
this	O
transformation	O
has	O
changed	O
the	O
relationship	O
between	O
the	O
examples	O
.	O
they	O
no	O
longer	O
lie	O
on	O
a	O
single	O
line	O
.	O
as	O
shown	O
in	O
ﬁgure	O
,	O
they	O
now	O
lie	O
in	O
a	O
space	O
where	O
a	O
linear	O
model	B
can	O
solve	O
the	O
problem	O
.	O
6.1	O
we	O
ﬁnish	O
by	O
multiplying	O
by	O
the	O
weight	O
vector	O
:	O
w	O
(	O
6.11	O
)	O
	O
0	O
	O
.	O
1	O
1	O
0	O
176	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
the	O
neural	O
network	O
has	O
obtained	O
the	O
correct	O
answer	O
for	O
every	O
example	O
in	O
the	O
batch	O
.	O
in	O
this	O
example	O
,	O
we	O
simply	O
speciﬁed	O
the	O
solution	O
,	O
then	O
showed	O
that	O
it	O
obtained	O
zero	O
error	O
.	O
in	O
a	O
real	O
situation	O
,	O
there	O
might	O
be	O
billions	O
of	O
model	B
parameters	O
and	O
billions	O
of	O
training	O
examples	O
,	O
so	O
one	O
can	O
not	O
simply	O
guess	O
the	O
solution	O
as	O
we	O
did	O
here	O
.	O
instead	O
,	O
a	O
gradient-based	O
optimization	O
algorithm	O
can	O
ﬁnd	O
parameters	O
that	O
produce	O
very	O
little	O
error	O
.	O
the	O
solution	O
we	O
described	O
to	O
the	O
xor	O
problem	O
is	O
at	O
a	O
global	O
minimum	O
of	O
the	O
loss	O
function	O
,	O
so	O
gradient	O
descent	B
could	O
converge	O
to	O
this	O
point	O
.	O
there	O
are	O
other	O
equivalent	O
solutions	O
to	O
the	O
xor	O
problem	O
that	O
gradient	O
descent	B
could	O
also	O
ﬁnd	O
.	O
the	O
convergence	O
point	O
of	O
gradient	O
descent	B
depends	O
on	O
the	O
initial	O
values	O
of	O
the	O
parameters	O
.	O
in	O
practice	O
,	O
gradient	O
descent	B
would	O
usually	O
not	O
ﬁnd	O
clean	O
,	O
easily	O
understood	O
,	O
integer-valued	O
solutions	O
like	O
the	O
one	O
we	O
presented	O
here	O
.	O
6.2	O
gradient-based	O
learning	O
designing	O
and	O
training	O
a	O
neural	O
network	O
is	O
not	O
much	O
diﬀerent	O
from	O
training	O
any	O
other	O
machine	O
learning	O
model	B
with	O
gradient	O
descent	B
.	O
in	O
section	O
,	O
we	O
described	O
how	O
to	O
build	O
a	O
machine	O
learning	O
algorithm	O
by	O
specifying	O
an	O
optimization	O
procedure	O
,	O
a	O
cost	O
function	O
,	O
and	O
a	O
model	B
family	O
.	O
5.10	O
the	O
largest	O
diﬀerence	O
between	O
the	O
linear	O
models	O
we	O
have	O
seen	O
so	O
far	O
and	O
neural	O
networks	O
is	O
that	O
the	O
nonlinearity	O
of	O
a	O
neural	O
network	O
causes	O
most	O
interesting	O
loss	O
functions	O
to	O
become	O
non-convex	O
.	O
this	O
means	O
that	O
neural	O
networks	O
are	O
usually	O
trained	O
by	O
using	O
iterative	O
,	O
gradient-based	O
optimizers	O
that	O
merely	O
drive	O
the	O
cost	O
function	O
to	O
a	O
very	O
low	O
value	O
,	O
rather	O
than	O
the	O
linear	O
equation	O
solvers	O
used	O
to	O
train	O
linear	O
regression	O
models	O
or	O
the	O
convex	O
optimization	O
algorithms	O
with	O
global	O
conver-	O
gence	B
guarantees	O
used	O
to	O
train	O
logistic	O
regression	O
or	O
svms	O
.	O
convex	O
optimization	O
converges	O
starting	O
from	O
any	O
initial	O
parameters	O
(	O
in	O
theory—in	O
practice	O
it	O
is	O
very	O
robust	O
but	O
can	O
encounter	O
numerical	O
problems	O
)	O
.	O
stochastic	O
gradient	O
descent	B
applied	O
to	O
non-convex	O
loss	O
functions	O
has	O
no	O
such	O
convergence	O
guarantee	O
,	O
and	O
is	O
sensitive	O
to	O
the	O
values	O
of	O
the	O
initial	O
parameters	O
.	O
for	O
feedforward	O
neural	O
networks	O
,	O
it	O
is	O
important	O
to	O
initialize	O
all	O
weights	O
to	O
small	O
random	O
values	O
.	O
the	O
biases	O
may	O
be	O
initialized	O
to	O
zero	O
or	O
to	O
small	O
positive	O
values	O
.	O
the	O
iterative	O
gradient-based	O
opti-	O
mization	O
algorithms	O
used	O
to	O
train	O
feedforward	O
networks	O
and	O
almost	O
all	O
other	O
deep	O
models	O
will	O
be	O
described	O
in	O
detail	O
in	O
chapter	O
,	O
with	O
parameter	O
initialization	O
in	O
particular	O
discussed	O
in	O
section	O
.	O
for	O
the	O
moment	O
,	O
it	O
suﬃces	O
to	O
understand	O
that	O
the	O
training	O
algorithm	O
is	O
almost	O
always	O
based	O
on	O
using	O
the	O
gradient	O
to	O
descend	O
the	O
cost	O
function	O
in	O
one	O
way	O
or	O
another	O
.	O
the	O
speciﬁc	O
algorithms	O
are	O
improvements	O
,	O
and	O
,	O
and	O
reﬁnements	O
on	O
the	O
ideas	O
of	O
gradient	O
descent	B
,	O
introduced	O
in	O
section	O
4.3	O
8	O
8.4	O
177	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
more	O
speciﬁcally	O
,	O
are	O
most	O
often	O
improvements	O
of	O
the	O
stochastic	O
gradient	O
descent	B
algorithm	O
,	O
introduced	O
in	O
section	O
.5.9	O
we	O
can	O
of	O
course	O
,	O
train	O
models	O
such	O
as	O
linear	O
regression	O
and	O
support	O
vector	O
machines	O
with	O
gradient	O
descent	B
too	O
,	O
and	O
in	O
fact	O
this	O
is	O
common	O
when	O
the	O
training	O
set	O
is	O
extremely	O
large	O
.	O
from	O
this	O
point	O
of	O
view	O
,	O
training	O
a	O
neural	O
network	O
is	O
not	O
much	O
diﬀerent	O
from	O
training	O
any	O
other	O
model	B
.	O
computing	O
the	O
gradient	O
is	O
slightly	O
more	O
complicated	O
for	O
a	O
neural	O
network	O
,	O
but	O
can	O
still	O
be	O
done	O
eﬃciently	O
and	O
exactly	O
.	O
section	O
will	O
describe	O
how	O
to	O
obtain	O
the	O
gradient	O
using	O
the	O
back-propagation	O
algorithm	O
and	O
modern	O
generalizations	O
of	O
the	O
back-propagation	O
algorithm	O
.	O
6.5	O
as	O
with	O
other	O
machine	O
learning	O
models	O
,	O
to	O
apply	O
gradient-based	O
learning	O
we	O
must	O
choose	O
a	O
cost	O
function	O
,	O
and	O
we	O
must	O
choose	O
how	O
to	O
represent	O
the	O
output	O
of	O
the	O
model	B
.	O
we	O
now	O
revisit	O
these	O
design	O
considerations	O
with	O
special	O
emphasis	O
on	O
the	O
neural	O
networks	O
scenario	O
.	O
6.2.1	O
cost	O
functions	O
an	O
important	O
aspect	O
of	O
the	O
design	O
of	O
a	O
deep	O
neural	O
network	O
is	O
the	O
choice	O
of	O
the	O
cost	O
function	O
.	O
fortunately	O
,	O
the	O
cost	O
functions	O
for	O
neural	O
networks	O
are	O
more	O
or	O
less	O
the	O
same	O
as	O
those	O
for	O
other	O
parametric	O
models	O
,	O
such	O
as	O
linear	O
models	O
.	O
in	O
most	O
cases	O
,	O
our	O
parametric	O
model	B
deﬁnes	O
a	O
distribution	O
p	O
(	O
y	O
x	O
;	O
θ	O
)	O
and	O
we	O
simply	O
use	O
the	O
principle	O
of	O
maximum	O
likelihood	O
.	O
this	O
means	O
we	O
use	O
the	O
cross-entropy	O
between	O
the	O
training	O
data	O
and	O
the	O
model	B
’	O
s	O
predictions	O
as	O
the	O
cost	O
function	O
.	O
|	O
sometimes	O
,	O
we	O
take	O
a	O
simpler	O
approach	O
,	O
where	O
rather	O
than	O
predicting	O
a	O
complete	O
probability	O
distribution	O
over	O
y	O
,	O
we	O
merely	O
predict	O
some	O
statistic	O
of	O
y	O
conditioned	O
on	O
.	O
specialized	O
loss	O
functions	O
allow	O
us	O
to	O
train	O
a	O
predictor	O
of	O
these	O
estimates	O
.	O
x	O
the	O
total	O
cost	O
function	O
used	O
to	O
train	O
a	O
neural	O
network	O
will	O
often	O
combine	O
one	O
of	O
the	O
primary	O
cost	O
functions	O
described	O
here	O
with	O
a	O
regularization	O
term	O
.	O
we	O
have	O
already	O
seen	O
some	O
simple	O
examples	O
of	O
regularization	O
applied	O
to	O
linear	O
models	O
in	O
section	O
.	O
the	O
weight	O
decay	O
approach	O
used	O
for	O
linear	O
models	O
is	O
also	O
directly	O
applicable	O
to	O
deep	O
neural	O
networks	O
and	O
is	O
among	O
the	O
most	O
popular	O
regularization	O
strategies	O
.	O
more	O
advanced	O
regularization	O
strategies	O
for	O
neural	O
networks	O
will	O
be	O
described	O
in	O
chapter	O
5.2.2	O
.7	O
6.2.1.1	O
learning	O
conditional	O
distributions	O
with	O
maximum	O
likelihood	O
most	O
modern	O
neural	O
networks	O
are	O
trained	O
using	O
maximum	O
likelihood	O
.	O
this	O
means	O
that	O
the	O
cost	O
function	O
is	O
simply	O
the	O
negative	O
log-likelihood	O
,	O
equivalently	O
described	O
178	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
as	O
the	O
cross-entropy	O
between	O
the	O
training	O
data	O
and	O
the	O
model	B
distribution	O
.	O
this	O
cost	O
function	O
is	O
given	O
by	O
log	O
pmodel	O
(	O
)	O
y	O
x	O
.	O
(	O
6.12	O
)	O
−	O
j	O
(	O
)	O
=	O
θ	O
∼	O
ex	O
y	O
,	O
ˆpdata	O
|	O
the	O
speciﬁc	O
form	O
of	O
the	O
cost	O
function	O
changes	O
from	O
model	B
to	O
model	B
,	O
depending	O
on	O
the	O
speciﬁc	O
form	O
of	O
log	O
pmodel	O
.	O
the	O
expansion	O
of	O
the	O
above	O
equation	O
typically	O
yields	O
some	O
terms	O
that	O
do	O
not	O
depend	O
on	O
the	O
model	B
parameters	O
and	O
may	O
be	O
dis-	O
(	O
y	O
;	O
f	O
(	O
x	O
;	O
θ	O
)	O
,	O
i	O
)	O
,	O
carded	O
.	O
for	O
example	O
,	O
as	O
we	O
saw	O
in	O
section	O
5.5.1	O
then	O
we	O
recover	O
the	O
mean	O
squared	O
error	O
cost	O
,	O
||	O
−	O
y	O
f	O
(	O
;	O
)	O
x	O
θ	O
2	O
+	O
const	O
,	O
pmodel	O
(	O
y	O
x	O
j	O
θ	O
(	O
)	O
=	O
(	O
6.13	O
)	O
n	O
)	O
=	O
,	O
if	O
||	O
|	O
1	O
∼	O
2e	O
x	O
y	O
,	O
ˆpdata	O
up	O
to	O
a	O
scaling	O
factor	O
of	O
1	O
and	O
a	O
term	O
that	O
does	O
not	O
depend	O
on	O
.	O
the	O
discarded	O
2	O
constant	O
is	O
based	O
on	O
the	O
variance	O
of	O
the	O
gaussian	O
distribution	O
,	O
which	O
in	O
this	O
case	O
we	O
chose	O
not	O
to	O
parametrize	O
.	O
previously	O
,	O
we	O
saw	O
that	O
the	O
equivalence	O
between	O
maximum	O
likelihood	O
estimation	O
with	O
an	O
output	O
distribution	O
and	O
minimization	O
of	O
mean	O
squared	O
error	O
holds	O
for	O
a	O
linear	O
model	B
,	O
but	O
in	O
fact	O
,	O
the	O
equivalence	O
holds	O
regardless	O
of	O
the	O
used	O
to	O
predict	O
the	O
mean	O
of	O
the	O
gaussian	O
.	O
f	O
(	O
;	O
)	O
x	O
θ	O
θ	O
an	O
advantage	O
of	O
this	O
approach	O
of	O
deriving	O
the	O
cost	O
function	O
from	O
maximum	O
likelihood	O
is	O
that	O
it	O
removes	O
the	O
burden	O
of	O
designing	O
cost	O
functions	O
for	O
each	O
model	B
.	O
specifying	O
a	O
model	B
p	O
(	O
y	O
x	O
)	O
.	O
)	O
automatically	O
determines	O
a	O
cost	O
function	O
log	O
p	O
(	O
y	O
x	O
|	O
|	O
one	O
recurring	O
theme	O
throughout	O
neural	O
network	O
design	O
is	O
that	O
the	O
gradient	O
of	O
the	O
cost	O
function	O
must	O
be	O
large	O
and	O
predictable	O
enough	O
to	O
serve	O
as	O
a	O
good	O
guide	O
for	O
the	O
learning	O
algorithm	O
.	O
functions	O
that	O
saturate	O
(	O
become	O
very	O
ﬂat	O
)	O
undermine	O
this	O
objective	O
because	O
they	O
make	O
the	O
gradient	O
become	O
very	O
small	O
.	O
in	O
many	O
cases	O
this	O
happens	O
because	O
the	O
activation	O
functions	O
used	O
to	O
produce	O
the	O
output	O
of	O
the	O
hidden	O
units	O
or	O
the	O
output	O
units	O
saturate	O
.	O
the	O
negative	O
log-likelihood	O
helps	O
to	O
avoid	O
this	O
problem	O
for	O
many	O
models	O
.	O
many	O
output	O
units	O
involve	O
an	O
exp	O
function	O
that	O
can	O
saturate	O
when	O
its	O
argument	O
is	O
very	O
negative	O
.	O
the	O
log	O
function	O
in	O
the	O
negative	O
log-likelihood	O
cost	O
function	O
undoes	O
the	O
exp	O
of	O
some	O
output	O
units	O
.	O
we	O
will	O
discuss	O
the	O
interaction	O
between	O
the	O
cost	O
function	O
and	O
the	O
choice	O
of	O
output	O
unit	O
in	O
section	O
.	O
6.2.2	O
one	O
unusual	O
property	O
of	O
the	O
cross-entropy	O
cost	O
used	O
to	O
perform	O
maximum	O
likelihood	O
estimation	O
is	O
that	O
it	O
usually	O
does	O
not	O
have	O
a	O
minimum	O
value	O
when	O
applied	O
to	O
the	O
models	O
commonly	O
used	O
in	O
practice	O
.	O
for	O
discrete	O
output	O
variables	O
,	O
most	O
models	O
are	O
parametrized	O
in	O
such	O
a	O
way	O
that	O
they	O
can	O
not	O
represent	O
a	O
probability	O
of	O
zero	O
or	O
one	O
,	O
but	O
can	O
come	O
arbitrarily	O
close	O
to	O
doing	O
so	O
.	O
logistic	O
regression	O
is	O
an	O
example	O
of	O
such	O
a	O
model	B
.	O
for	O
real-valued	O
output	O
variables	O
,	O
if	O
the	O
model	B
179	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
can	O
control	O
the	O
density	O
of	O
the	O
output	O
distribution	O
(	O
for	O
example	O
,	O
by	O
learning	O
the	O
variance	O
parameter	O
of	O
a	O
gaussian	O
output	O
distribution	O
)	O
then	O
it	O
becomes	O
possible	O
to	O
assign	O
extremely	O
high	O
density	O
to	O
the	O
correct	O
training	O
set	O
outputs	O
,	O
resulting	O
in	O
cross-entropy	O
approaching	O
negative	O
inﬁnity	O
.	O
regularization	O
techniques	O
described	O
in	O
chapter	O
provide	O
several	O
diﬀerent	O
ways	O
of	O
modifying	O
the	O
learning	O
problem	O
so	O
that	O
the	O
model	B
can	O
not	O
reap	O
unlimited	O
reward	O
in	O
this	O
way	O
.	O
7	O
6.2.1.2	O
learning	O
conditional	O
statistics	O
|	O
instead	O
of	O
learning	O
a	O
full	O
probability	O
distribution	O
p	O
(	O
y	O
x	O
just	O
one	O
conditional	O
statistic	O
of	O
given	O
.	O
x	O
y	O
;	O
θ	O
)	O
we	O
often	O
want	O
to	O
learn	O
for	O
example	O
,	O
we	O
may	O
have	O
a	O
predictor	O
f	O
(	O
x	O
;	O
θ	O
)	O
that	O
we	O
wish	O
to	O
predict	O
the	O
mean	O
.y	O
of	O
if	O
we	O
use	O
a	O
suﬃciently	O
powerful	O
neural	O
network	O
,	O
we	O
can	O
think	O
of	O
the	O
neural	O
network	O
as	O
being	O
able	O
to	O
represent	O
any	O
function	O
f	O
from	O
a	O
wide	O
class	O
of	O
functions	O
,	O
with	O
this	O
class	O
being	O
limited	O
only	O
by	O
features	O
such	O
as	O
continuity	O
and	O
boundedness	O
rather	O
than	O
by	O
having	O
a	O
speciﬁc	O
parametric	O
form	O
.	O
from	O
this	O
point	O
of	O
view	O
,	O
we	O
can	O
view	O
the	O
cost	O
function	O
as	O
being	O
a	O
functional	O
rather	O
than	O
just	O
a	O
function	O
.	O
a	O
functional	O
is	O
a	O
mapping	O
from	O
functions	O
to	O
real	O
numbers	O
.	O
we	O
can	O
thus	O
think	O
of	O
learning	O
as	O
choosing	O
a	O
function	O
rather	O
than	O
merely	O
choosing	O
a	O
set	O
of	O
parameters	O
.	O
we	O
can	O
design	O
our	O
cost	O
functional	O
to	O
have	O
its	O
minimum	O
occur	O
at	O
some	O
speciﬁc	O
function	O
we	O
desire	O
.	O
for	O
example	O
,	O
we	O
can	O
design	O
the	O
cost	O
functional	O
to	O
have	O
its	O
minimum	O
lie	O
on	O
the	O
function	O
that	O
maps	O
x	O
to	O
the	O
expected	O
value	O
of	O
y	O
given	O
x.	O
solving	O
an	O
optimization	O
problem	O
with	O
respect	O
to	O
a	O
function	O
requires	O
a	O
mathematical	O
tool	O
called	O
calculus	O
of	O
variations	O
,	O
described	O
in	O
section	O
.	O
it	O
is	O
not	O
necessary	O
to	O
understand	O
calculus	O
of	O
variations	O
to	O
understand	O
the	O
content	O
of	O
this	O
chapter	O
.	O
at	O
the	O
moment	O
,	O
it	O
is	O
only	O
necessary	O
to	O
understand	O
that	O
calculus	O
of	O
variations	O
may	O
be	O
used	O
to	O
derive	O
the	O
following	O
two	O
results	O
.	O
19.4.2	O
our	O
ﬁrst	O
result	O
derived	O
using	O
calculus	O
of	O
variations	O
is	O
that	O
solving	O
the	O
optimiza-	O
tion	B
problem	O
yields	O
∗	O
f	O
=	O
arg	O
min	O
f	O
∼	O
pdata	O
ex	O
y	O
,	O
||	O
−	O
y	O
||	O
f	O
(	O
)	O
x	O
2	O
∗	O
f	O
(	O
)	O
=	O
x	O
∼	O
ey	O
pdata	O
(	O
|	O
)	O
y	O
x	O
[	O
]	O
y	O
,	O
(	O
6.14	O
)	O
(	O
6.15	O
)	O
so	O
long	O
as	O
this	O
function	O
lies	O
within	O
the	O
class	O
we	O
optimize	O
over	O
.	O
in	O
other	O
words	O
,	O
if	O
we	O
could	O
train	O
on	O
inﬁnitely	O
many	O
samples	O
from	O
the	O
true	O
data	O
generating	O
distribution	O
,	O
minimizing	O
the	O
mean	O
squared	O
error	O
cost	O
function	O
gives	O
a	O
function	O
that	O
predicts	O
the	O
mean	O
of	O
for	O
each	O
value	O
of	O
.	O
x	O
y	O
180	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
diﬀerent	O
cost	O
functions	O
give	O
diﬀerent	O
statistics	O
.	O
a	O
second	O
result	O
derived	O
using	O
calculus	O
of	O
variations	O
is	O
that	O
∗	O
f	O
=	O
arg	O
min	O
f	O
∼	O
pdata	O
ex	O
y	O
,	O
||	O
−	O
y	O
||	O
f	O
(	O
)	O
x	O
1	O
(	O
6.16	O
)	O
yields	O
a	O
function	O
that	O
predicts	O
the	O
median	O
value	O
of	O
y	O
for	O
each	O
x	O
,	O
so	O
long	O
as	O
such	O
a	O
function	O
may	O
be	O
described	O
by	O
the	O
family	O
of	O
functions	O
we	O
optimize	O
over	O
.	O
this	O
cost	O
function	O
is	O
commonly	O
called	O
.	O
mean	O
absolute	O
error	O
unfortunately	O
,	O
mean	O
squared	O
error	O
and	O
mean	O
absolute	O
error	O
often	O
lead	O
to	O
poor	O
results	O
when	O
used	O
with	O
gradient-based	O
optimization	O
.	O
some	O
output	O
units	O
that	O
saturate	O
produce	O
very	O
small	O
gradients	O
when	O
combined	O
with	O
these	O
cost	O
functions	O
.	O
this	O
is	O
one	O
reason	O
that	O
the	O
cross-entropy	O
cost	O
function	O
is	O
more	O
popular	O
than	O
mean	O
squared	O
error	O
or	O
mean	O
absolute	O
error	O
,	O
even	O
when	O
it	O
is	O
not	O
necessary	O
to	O
estimate	O
an	O
entire	O
distribution	O
.	O
y	O
x	O
)	O
p	O
(	O
|	O
6.2.2	O
output	O
units	O
the	O
choice	O
of	O
cost	O
function	O
is	O
tightly	O
coupled	O
with	O
the	O
choice	O
of	O
output	O
unit	O
.	O
most	O
of	O
the	O
time	O
,	O
we	O
simply	O
use	O
the	O
cross-entropy	O
between	O
the	O
data	O
distribution	O
and	O
the	O
model	B
distribution	O
.	O
the	O
choice	O
of	O
how	O
to	O
represent	O
the	O
output	O
then	O
determines	O
the	O
form	O
of	O
the	O
cross-entropy	O
function	O
.	O
any	O
kind	O
of	O
neural	O
network	O
unit	O
that	O
may	O
be	O
used	O
as	O
an	O
output	O
can	O
also	O
be	O
used	O
as	O
a	O
hidden	O
unit	O
.	O
here	O
,	O
we	O
focus	O
on	O
the	O
use	O
of	O
these	O
units	O
as	O
outputs	O
of	O
the	O
model	B
,	O
but	O
in	O
principle	O
they	O
can	O
be	O
used	O
internally	O
as	O
well	O
.	O
we	O
revisit	O
these	O
units	O
with	O
additional	O
detail	O
about	O
their	O
use	O
as	O
hidden	O
units	O
in	O
section	O
.6.3	O
throughout	O
this	O
section	O
,	O
we	O
suppose	O
that	O
the	O
feedforward	O
network	O
provides	O
a	O
set	O
of	O
hidden	O
features	O
deﬁned	O
by	O
h	O
=	O
f	O
(	O
x	O
;	O
θ	O
)	O
.	O
the	O
role	O
of	O
the	O
output	O
layer	O
is	O
then	O
to	O
provide	O
some	O
additional	O
transformation	O
from	O
the	O
features	O
to	O
complete	O
the	O
task	O
that	O
the	O
network	O
must	O
perform	O
.	O
6.2.2.1	O
linear	O
units	O
for	O
gaussian	O
output	O
distributions	O
one	O
simple	O
kind	O
of	O
output	O
unit	O
is	O
an	O
output	O
unit	O
based	O
on	O
an	O
aﬃne	O
transformation	O
with	O
no	O
nonlinearity	O
.	O
these	O
are	O
often	O
just	O
called	O
linear	O
units	O
.	O
given	O
features	O
h	O
,	O
a	O
layer	O
of	O
linear	O
output	O
units	O
produces	O
a	O
vector	O
ˆy	O
=	O
w	O
h+b	O
.	O
	O
linear	O
output	O
layers	O
are	O
often	O
used	O
to	O
produce	O
the	O
mean	O
of	O
a	O
conditional	O
gaussian	O
distribution	O
:	O
|	O
n	O
)	O
=	O
p	O
(	O
y	O
x	O
181	O
y	O
ˆy	O
i	O
,	O
)	O
.	O
(	O
;	O
(	O
6.17	O
)	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
maximizing	O
the	O
log-likelihood	O
is	O
then	O
equivalent	O
to	O
minimizing	O
the	O
mean	O
squared	O
error	O
.	O
the	O
maximum	O
likelihood	O
framework	O
makes	O
it	O
straightforward	O
to	O
learn	O
the	O
covariance	O
of	O
the	O
gaussian	O
too	O
,	O
or	O
to	O
make	O
the	O
covariance	O
of	O
the	O
gaussian	O
be	O
a	O
function	O
of	O
the	O
input	O
.	O
however	O
,	O
the	O
covariance	O
must	O
be	O
constrained	O
to	O
be	O
a	O
positive	O
deﬁnite	O
matrix	O
for	O
all	O
inputs	O
.	O
it	O
is	O
diﬃcult	O
to	O
satisfy	O
such	O
constraints	O
with	O
a	O
linear	O
output	O
layer	O
,	O
so	O
typically	O
other	O
output	O
units	O
are	O
used	O
to	O
parametrize	O
the	O
covariance	O
.	O
approaches	O
to	O
modeling	O
the	O
covariance	O
are	O
described	O
shortly	O
,	O
in	O
section	O
6.2.2.4	O
.	O
because	O
linear	O
units	O
do	O
not	O
saturate	O
,	O
they	O
pose	O
little	O
diﬃculty	O
for	O
gradient-	O
based	O
optimization	O
algorithms	O
and	O
may	O
be	O
used	O
with	O
a	O
wide	O
variety	O
of	O
optimization	O
algorithms	O
.	O
6.2.2.2	O
sigmoid	O
units	O
for	O
bernoulli	O
output	O
distributions	O
many	O
tasks	O
require	O
predicting	O
the	O
value	O
of	O
a	O
binary	O
variable	O
y	O
.	O
classiﬁcation	O
problems	O
with	O
two	O
classes	O
can	O
be	O
cast	O
in	O
this	O
form	O
.	O
the	O
maximum-likelihood	O
approach	O
is	O
to	O
deﬁne	O
a	O
bernoulli	O
distribution	O
over	O
y	O
conditioned	O
on	O
.x	O
a	O
bernoulli	O
distribution	O
is	O
deﬁned	O
by	O
just	O
a	O
single	O
number	O
.	O
the	O
neural	O
net	O
x	O
)	O
.	O
for	O
this	O
number	O
to	O
be	O
a	O
valid	O
probability	O
,	O
it	O
|	O
needs	O
to	O
predict	O
only	O
p	O
(	O
y	O
=	O
1	O
must	O
lie	O
in	O
the	O
interval	O
[	O
0	O
,	O
1	O
]	O
.	O
	O
	O
	O
satisfying	O
this	O
constraint	O
requires	O
some	O
careful	O
design	O
eﬀort	O
.	O
suppose	O
we	O
were	O
to	O
use	O
a	O
linear	O
unit	O
,	O
and	O
threshold	O
its	O
value	O
to	O
obtain	O
a	O
valid	O
probability	O
:	O
|	O
p	O
y	O
(	O
=	O
1	O
x	O
)	O
=	O
max	O
0	O
min	O
,	O
1	O
,	O
w	O
	O
h	O
+	O
b	O
.	O
(	O
6.18	O
)	O
this	O
would	O
indeed	O
deﬁne	O
a	O
valid	O
conditional	O
distribution	O
,	O
but	O
we	O
would	O
not	O
be	O
able	O
to	O
train	O
it	O
very	O
eﬀectively	O
with	O
gradient	O
descent	B
.	O
any	O
time	O
that	O
w	O
h	O
+	O
b	O
strayed	O
outside	O
the	O
unit	O
interval	O
,	O
the	O
gradient	O
of	O
the	O
output	O
of	O
the	O
model	B
with	O
respect	O
to	O
its	O
parameters	O
would	O
be	O
0.	O
a	O
gradient	O
of	O
0	O
is	O
typically	O
problematic	O
because	O
the	O
learning	O
algorithm	O
no	O
longer	O
has	O
a	O
guide	O
for	O
how	O
to	O
improve	O
the	O
corresponding	O
parameters	O
.	O
	O
instead	O
,	O
it	O
is	O
better	O
to	O
use	O
a	O
diﬀerent	O
approach	O
that	O
ensures	O
there	O
is	O
always	O
a	O
strong	O
gradient	O
whenever	O
the	O
model	B
has	O
the	O
wrong	O
answer	O
.	O
this	O
approach	O
is	O
based	O
on	O
using	O
sigmoid	O
output	O
units	O
combined	O
with	O
maximum	O
likelihood	O
.	O
	O
	O
a	O
sigmoid	O
output	O
unit	O
is	O
deﬁned	O
by	O
ˆy	O
σ=	O
w	O
	O
h	O
+	O
b	O
182	O
(	O
6.19	O
)	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
where	O
σ	O
is	O
the	O
logistic	O
sigmoid	O
function	O
described	O
in	O
section	O
.	O
3.10	O
we	O
can	O
think	O
of	O
the	O
sigmoid	O
output	O
unit	O
as	O
having	O
two	O
components	O
.	O
first	O
,	O
it	O
	O
h	O
+	O
b.	O
next	O
,	O
it	O
uses	O
the	O
sigmoid	O
activation	O
uses	O
a	O
linear	O
layer	O
to	O
compute	O
z	O
=	O
w	O
function	O
to	O
convert	O
into	O
a	O
probability	O
.	O
z	O
we	O
omit	O
the	O
dependence	O
on	O
x	O
for	O
the	O
moment	O
to	O
discuss	O
how	O
to	O
deﬁne	O
a	O
probability	O
distribution	O
over	O
y	O
using	O
the	O
value	O
z	O
.	O
the	O
sigmoid	O
can	O
be	O
motivated	O
by	O
constructing	O
an	O
unnormalized	O
probability	O
distribution	O
˜p	O
(	O
y	O
)	O
,	O
which	O
does	O
not	O
sum	O
to	O
1.	O
we	O
can	O
then	O
divide	O
by	O
an	O
appropriate	O
constant	O
to	O
obtain	O
a	O
valid	O
probability	O
distribution	O
.	O
if	O
we	O
begin	O
with	O
the	O
assumption	O
that	O
the	O
unnormalized	O
log	O
probabilities	O
are	O
linear	O
in	O
y	O
and	O
z	O
,	O
we	O
can	O
exponentiate	O
to	O
obtain	O
the	O
unnormalized	O
probabilities	O
.	O
we	O
then	O
normalize	O
to	O
see	O
that	O
this	O
yields	O
a	O
bernoulli	O
distribution	O
controlled	O
by	O
a	O
sigmoidal	O
transformation	O
of	O
:	O
z	O
	O
yz	O
(	O
)	O
=	O
log	O
˜p	O
y	O
˜p	O
y	O
(	O
)	O
=	O
exp	O
(	O
yz	O
)	O
exp	O
(	O
)	O
yz	O
	O
	O
1	O
=0	O
exp	O
(	O
y	O
−	O
y	O
(	O
(	O
2	O
y	O
z	O
.	O
1	O
)	O
)	O
z	O
)	O
p	O
y	O
(	O
)	O
=	O
p	O
y	O
(	O
)	O
=	O
σ	O
(	O
6.20	O
)	O
(	O
6.21	O
)	O
(	O
6.22	O
)	O
(	O
6.23	O
)	O
probability	O
distributions	O
based	O
on	O
exponentiation	O
and	O
normalization	O
are	O
common	O
throughout	O
the	O
statistical	O
modeling	O
literature	O
.	O
the	O
z	O
variable	O
deﬁning	O
such	O
a	O
distribution	O
over	O
binary	O
variables	O
is	O
called	O
a	O
.	O
logit	O
−	O
this	O
approach	O
to	O
predicting	O
the	O
probabilities	O
in	O
log-space	O
is	O
natural	O
to	O
use	O
with	O
maximum	O
likelihood	O
learning	O
.	O
because	O
the	O
cost	O
function	O
used	O
with	O
maximum	O
x	O
)	O
,	O
the	O
log	O
in	O
the	O
cost	O
function	O
undoes	O
the	O
exp	O
of	O
the	O
likelihood	O
is	O
sigmoid	O
.	O
without	O
this	O
eﬀect	O
,	O
the	O
saturation	O
of	O
the	O
sigmoid	O
could	O
prevent	O
gradient-	O
based	O
learning	O
from	O
making	O
good	O
progress	O
.	O
the	O
loss	O
function	O
for	O
maximum	O
likelihood	O
learning	O
of	O
a	O
bernoulli	O
parametrized	O
by	O
a	O
sigmoid	O
is	O
log	O
p	O
(	O
y	O
|	O
j	O
−	O
(	O
)	O
=	O
θ	O
−	O
=	O
|	O
p	O
y	O
x	O
)	O
log	O
(	O
−	O
log	O
(	O
(	O
2	O
σ	O
y	O
−	O
2	O
)	O
)	O
y	O
z	O
.	O
1	O
)	O
)	O
z	O
(	O
6.24	O
)	O
(	O
6.25	O
)	O
(	O
6.26	O
)	O
=	O
(	O
(	O
1	O
ζ	O
this	O
derivation	O
makes	O
use	O
of	O
some	O
properties	O
from	O
section	O
−	O
.	O
by	O
rewriting	O
the	O
loss	O
in	O
terms	O
of	O
the	O
softplus	O
function	O
,	O
we	O
can	O
see	O
that	O
it	O
saturates	O
only	O
when	O
(	O
1	O
2y	O
)	O
z	O
is	O
very	O
negative	O
.	O
saturation	O
thus	O
occurs	O
only	O
when	O
the	O
model	B
already	O
has	O
the	O
right	O
answer—when	O
y	O
=	O
1	O
and	O
z	O
is	O
very	O
positive	O
,	O
or	O
y	O
=	O
0	O
and	O
z	O
is	O
very	O
negative	O
.	O
when	O
z	O
has	O
the	O
wrong	O
sign	O
,	O
the	O
argument	O
to	O
the	O
softplus	O
function	O
,	O
3.10	O
183	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
−	O
|	O
|	O
z	O
.	O
as	O
2y	O
)	O
z	O
,	O
may	O
be	O
simpliﬁed	O
to	O
|	O
|	O
|	O
|	O
z	O
becomes	O
large	O
while	O
z	O
has	O
the	O
wrong	O
sign	O
,	O
(	O
1	O
z	O
.	O
the	O
the	O
softplus	O
function	O
asymptotes	O
toward	O
simply	O
returning	O
its	O
argument	O
derivative	O
with	O
respect	O
to	O
z	O
asymptotes	O
to	O
sign	O
(	O
z	O
)	O
,	O
so	O
,	O
in	O
the	O
limit	O
of	O
extremely	O
incorrect	O
z	O
,	O
the	O
softplus	O
function	O
does	O
not	O
shrink	O
the	O
gradient	O
at	O
all	O
.	O
this	O
property	O
is	O
very	O
useful	O
because	O
it	O
means	O
that	O
gradient-based	O
learning	O
can	O
act	O
to	O
quickly	O
correct	O
a	O
mistaken	O
.z	O
when	O
we	O
use	O
other	O
loss	O
functions	O
,	O
such	O
as	O
mean	O
squared	O
error	O
,	O
the	O
loss	O
can	O
saturate	O
anytime	O
σ	O
(	O
z	O
)	O
saturates	O
.	O
the	O
sigmoid	O
activation	O
function	O
saturates	O
to	O
0	O
when	O
z	O
becomes	O
very	O
negative	O
and	O
saturates	O
to	O
when	O
z	O
becomes	O
very	O
positive	O
.	O
the	O
gradient	O
can	O
shrink	O
too	O
small	O
to	O
be	O
useful	O
for	O
learning	O
whenever	O
this	O
happens	O
,	O
whether	O
the	O
model	B
has	O
the	O
correct	O
answer	O
or	O
the	O
incorrect	O
answer	O
.	O
for	O
this	O
reason	O
,	O
maximum	O
likelihood	O
is	O
almost	O
always	O
the	O
preferred	O
approach	O
to	O
training	O
sigmoid	O
output	O
units	O
.	O
1	O
analytically	O
,	O
the	O
logarithm	O
of	O
the	O
sigmoid	O
is	O
always	O
deﬁned	O
and	O
ﬁnite	O
,	O
because	O
the	O
sigmoid	O
returns	O
values	O
restricted	O
to	O
the	O
open	O
interval	O
(	O
0	O
,	O
1	O
)	O
,	O
rather	O
than	O
using	O
the	O
entire	O
closed	O
interval	O
of	O
valid	O
probabilities	O
[	O
0	O
,	O
1	O
]	O
.	O
in	O
software	O
implementations	O
,	O
to	O
avoid	O
numerical	O
problems	O
,	O
it	O
is	O
best	O
to	O
write	O
the	O
negative	O
log-likelihood	O
as	O
a	O
function	O
of	O
z	O
,	O
rather	O
than	O
as	O
a	O
function	O
of	O
ˆy	O
=	O
σ	O
(	O
z	O
)	O
.	O
if	O
the	O
sigmoid	O
function	O
underﬂows	O
to	O
zero	O
,	O
then	O
taking	O
the	O
logarithm	O
of	O
ˆy	O
yields	O
negative	O
inﬁnity	O
.	O
6.2.2.3	O
softmax	O
units	O
for	O
multinoulli	O
output	O
distributions	O
any	O
time	O
we	O
wish	O
to	O
represent	O
a	O
probability	O
distribution	O
over	O
a	O
discrete	O
variable	O
with	O
n	O
possible	O
values	O
,	O
we	O
may	O
use	O
the	O
softmax	O
function	O
.	O
this	O
can	O
be	O
seen	O
as	O
a	O
generalization	O
of	O
the	O
sigmoid	O
function	O
which	O
was	O
used	O
to	O
represent	O
a	O
probability	O
distribution	O
over	O
a	O
binary	O
variable	O
.	O
softmax	O
functions	O
are	O
most	O
often	O
used	O
as	O
the	O
output	O
of	O
a	O
classiﬁer	O
,	O
to	O
represent	O
the	O
probability	O
distribution	O
over	O
n	O
diﬀerent	O
classes	O
.	O
more	O
rarely	O
,	O
softmax	O
functions	O
can	O
be	O
used	O
inside	O
the	O
model	B
itself	O
,	O
if	O
we	O
wish	O
the	O
model	B
to	O
choose	O
between	O
one	O
of	O
n	O
diﬀerent	O
options	O
for	O
some	O
internal	O
variable	O
.	O
in	O
the	O
case	O
of	O
binary	O
variables	O
,	O
we	O
wished	O
to	O
produce	O
a	O
single	O
number	O
|	O
(	O
=	O
1	O
=	O
p	O
y	O
ˆy	O
)	O
.	O
x	O
(	O
6.27	O
)	O
and	O
,	O
and	O
because	O
we	O
wanted	O
the	O
because	O
this	O
number	O
needed	O
to	O
lie	O
between	O
logarithm	O
of	O
the	O
number	O
to	O
be	O
well-behaved	O
for	O
gradient-based	O
optimization	O
of	O
the	O
log-likelihood	O
,	O
we	O
chose	O
to	O
instead	O
predict	O
a	O
number	O
z	O
=	O
log	O
˜p	O
(	O
y	O
=	O
1	O
x	O
)	O
.	O
exponentiating	O
and	O
normalizing	O
gave	O
us	O
a	O
bernoulli	O
distribution	O
controlled	O
by	O
the	O
sigmoid	O
function	O
.	O
0	O
1	O
|	O
184	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
to	O
generalize	O
to	O
the	O
case	O
of	O
a	O
discrete	O
variable	O
with	O
n	O
values	O
,	O
we	O
now	O
need	O
x	O
)	O
.	O
we	O
require	O
not	O
only	O
that	O
each	O
to	O
produce	O
a	O
vector	O
ˆy	O
,	O
with	O
ˆyi	O
=	O
p	O
(	O
y	O
=	O
i	O
element	O
of	O
ˆyi	O
be	O
between	O
so	O
that	O
it	O
represents	O
a	O
valid	O
probability	O
distribution	O
.	O
the	O
same	O
approach	O
that	O
worked	O
for	O
the	O
bernoulli	O
distribution	O
generalizes	O
to	O
the	O
multinoulli	O
distribution	O
.	O
first	O
,	O
a	O
linear	O
layer	O
predicts	O
unnormalized	O
log	O
probabilities	O
:	O
and	O
,	O
but	O
also	O
that	O
the	O
entire	O
vector	O
sums	O
to	O
1	O
0	O
1	O
|	O
	O
z	O
w=	O
h	O
b+	O
,	O
(	O
6.28	O
)	O
where	O
zi	O
=	O
log	O
˜p	O
(	O
y	O
=	O
i	O
normalize	O
z	O
|	O
x	O
)	O
.	O
the	O
softmax	O
function	O
can	O
then	O
exponentiate	O
and	O
to	O
obtain	O
the	O
desired	O
ˆy	O
.	O
formally	O
,	O
the	O
softmax	O
function	O
is	O
given	O
by	O
softmax	O
(	O
)	O
z	O
i	O
=	O
exp	O
(	O
zi	O
)	O
j	O
exp	O
(	O
zj	O
)	O
.	O
(	O
6.29	O
)	O
	O
−	O
as	O
with	O
the	O
logistic	O
sigmoid	O
,	O
the	O
use	O
of	O
the	O
exp	O
function	O
works	O
very	O
well	O
when	O
training	O
the	O
softmax	O
to	O
output	O
a	O
target	O
value	O
y	O
using	O
maximum	O
log-likelihood	O
.	O
in	O
this	O
case	O
,	O
we	O
wish	O
to	O
maximize	O
log	O
p	O
(	O
y	O
=	O
i	O
;	O
z	O
)	O
=	O
log	O
softmax	O
(	O
z	O
)	O
i.	O
deﬁning	O
the	O
softmax	O
in	O
terms	O
of	O
exp	O
is	O
natural	O
because	O
the	O
log	O
in	O
the	O
log-likelihood	O
can	O
undo	O
the	O
of	O
the	O
softmax	O
:	O
	O
exp	O
log	O
softmax	O
(	O
)	O
z	O
i	O
=	O
zi	O
log	O
exp	O
(	O
zj	O
)	O
.	O
j	O
(	O
6.30	O
)	O
6.30	O
6.30	O
	O
shows	O
that	O
the	O
input	O
the	O
ﬁrst	O
term	O
of	O
equation	O
z	O
i	O
always	O
has	O
a	O
direct	O
contribution	O
to	O
the	O
cost	O
function	O
.	O
because	O
this	O
term	O
can	O
not	O
saturate	O
,	O
we	O
know	O
that	O
learning	O
can	O
proceed	O
,	O
even	O
if	O
the	O
contribution	O
of	O
zi	O
to	O
the	O
second	O
term	O
of	O
equation	O
becomes	O
very	O
small	O
.	O
when	O
maximizing	O
the	O
log-likelihood	O
,	O
the	O
ﬁrst	O
term	O
encourages	O
z	O
i	O
to	O
be	O
pushed	O
up	O
,	O
while	O
the	O
second	O
term	O
encourages	O
all	O
of	O
z	O
to	O
be	O
pushed	O
down	O
.	O
to	O
gain	O
some	O
intuition	O
for	O
the	O
second	O
term	O
,	O
log	O
j	O
exp	O
(	O
zj	O
)	O
,	O
observe	O
that	O
this	O
term	O
can	O
be	O
roughly	O
approximated	O
by	O
maxj	O
zj	O
.	O
this	O
approximation	O
is	O
based	O
on	O
the	O
idea	O
that	O
exp	O
(	O
zk	O
)	O
is	O
insigniﬁcant	O
for	O
any	O
zk	O
that	O
is	O
noticeably	O
less	O
than	O
maxj	O
zj	O
.	O
the	O
intuition	O
we	O
can	O
gain	O
from	O
this	O
approximation	O
is	O
that	O
the	O
negative	O
log-likelihood	O
cost	O
function	O
always	O
strongly	O
penalizes	O
the	O
most	O
active	O
incorrect	O
prediction	O
.	O
if	O
the	O
correct	O
answer	O
already	O
has	O
the	O
largest	O
input	O
to	O
the	O
softmax	O
,	O
then	O
maxj	O
zj	O
=	O
zi	O
terms	O
will	O
roughly	O
cancel	O
.	O
the	O
this	O
example	O
will	O
then	O
contribute	O
little	O
to	O
the	O
overall	O
training	O
cost	O
,	O
which	O
will	O
be	O
dominated	O
by	O
other	O
examples	O
that	O
are	O
not	O
yet	O
correctly	O
classiﬁed	O
.	O
−	O
zi	O
term	O
and	O
the	O
log	O
j	O
exp	O
(	O
z	O
j	O
)	O
	O
≈	O
so	O
far	O
we	O
have	O
discussed	O
only	O
a	O
single	O
example	O
.	O
overall	O
,	O
unregularized	O
maximum	O
likelihood	O
will	O
drive	O
the	O
model	B
to	O
learn	O
parameters	O
that	O
drive	O
the	O
softmax	O
to	O
predict	O
185	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
	O
	O
the	O
fraction	O
of	O
counts	O
of	O
each	O
outcome	O
observed	O
in	O
the	O
training	O
set	O
:	O
softmax	O
(	O
(	O
;	O
)	O
)	O
z	O
x	O
θ	O
i	O
≈	O
m	O
j=1	O
1y	O
(	O
)	O
j	O
=i	O
,	O
x	O
(	O
)	O
j	O
=x	O
m	O
j=1	O
1	O
x	O
(	O
)	O
j	O
=x	O
.	O
(	O
6.31	O
)	O
because	O
maximum	O
likelihood	O
is	O
a	O
consistent	O
estimator	O
,	O
this	O
is	O
guaranteed	O
to	O
happen	O
so	O
long	O
as	O
the	O
model	B
family	O
is	O
capable	O
of	O
representing	O
the	O
training	O
distribution	O
.	O
in	O
practice	O
,	O
limited	O
model	B
capacity	O
and	O
imperfect	O
optimization	O
will	O
mean	O
that	O
the	O
model	B
is	O
only	O
able	O
to	O
approximate	O
these	O
fractions	O
.	O
many	O
objective	O
functions	O
other	O
than	O
the	O
log-likelihood	O
do	O
not	O
work	B
as	O
well	O
with	O
the	O
softmax	O
function	O
.	O
speciﬁcally	O
,	O
objective	O
functions	O
that	O
do	O
not	O
use	O
a	O
log	O
to	O
undo	O
the	O
exp	O
of	O
the	O
softmax	O
fail	O
to	O
learn	O
when	O
the	O
argument	O
to	O
the	O
exp	O
becomes	O
very	O
negative	O
,	O
causing	O
the	O
gradient	O
to	O
vanish	O
.	O
in	O
particular	O
,	O
squared	O
error	O
is	O
a	O
poor	O
loss	O
function	O
for	O
softmax	O
units	O
,	O
and	O
can	O
fail	O
to	O
train	O
the	O
model	B
to	O
change	O
its	O
output	O
,	O
even	O
when	O
the	O
model	B
makes	O
highly	O
conﬁdent	O
incorrect	O
predictions	O
(	O
bridle	O
,	O
1990	O
)	O
.	O
to	O
understand	O
why	O
these	O
other	O
loss	O
functions	O
can	O
fail	O
,	O
we	O
need	O
to	O
examine	O
the	O
softmax	O
function	O
itself	O
.	O
like	O
the	O
sigmoid	O
,	O
the	O
softmax	O
activation	O
can	O
saturate	O
.	O
the	O
sigmoid	O
function	O
has	O
a	O
single	O
output	O
that	O
saturates	O
when	O
its	O
input	O
is	O
extremely	O
negative	O
or	O
extremely	O
positive	O
.	O
in	O
the	O
case	O
of	O
the	O
softmax	O
,	O
there	O
are	O
multiple	O
output	O
values	O
.	O
these	O
output	O
values	O
can	O
saturate	O
when	O
the	O
diﬀerences	O
between	O
input	O
values	O
become	O
extreme	O
.	O
when	O
the	O
softmax	O
saturates	O
,	O
many	O
cost	O
functions	O
based	O
on	O
the	O
softmax	O
also	O
saturate	O
,	O
unless	O
they	O
are	O
able	O
to	O
invert	O
the	O
saturating	O
activating	O
function	O
.	O
to	O
see	O
that	O
the	O
softmax	O
function	O
responds	O
to	O
the	O
diﬀerence	O
between	O
its	O
inputs	O
,	O
observe	O
that	O
the	O
softmax	O
output	O
is	O
invariant	O
to	O
adding	O
the	O
same	O
scalar	O
to	O
all	O
of	O
its	O
inputs	O
:	O
softmax	O
(	O
)	O
=	O
softmax	O
(	O
+	O
)	O
c	O
.	O
z	O
z	O
(	O
6.32	O
)	O
using	O
this	O
property	O
,	O
we	O
can	O
derive	O
a	O
numerically	O
stable	O
variant	O
of	O
the	O
softmax	O
:	O
softmax	O
(	O
)	O
=	O
softmax	O
(	O
z	O
z	O
max	O
i	O
zi	O
)	O
.	O
(	O
6.33	O
)	O
−	O
the	O
reformulated	O
version	O
allows	O
us	O
to	O
evaluate	O
softmax	O
with	O
only	O
small	O
numerical	O
errors	O
even	O
when	O
z	O
contains	O
extremely	O
large	O
or	O
extremely	O
negative	O
numbers	O
.	O
ex-	O
amining	O
the	O
numerically	O
stable	O
variant	O
,	O
we	O
see	O
that	O
the	O
softmax	O
function	O
is	O
driven	O
by	O
the	O
amount	O
that	O
its	O
arguments	O
deviate	O
from	O
maxi	O
zi	O
.	O
an	O
output	O
softmax	O
(	O
z	O
)	O
i	O
saturates	O
to	O
when	O
the	O
corresponding	O
input	O
is	O
maximal	O
(	O
zi	O
=	O
maxi	O
zi	O
)	O
and	O
zi	O
is	O
much	O
greater	O
than	O
all	O
of	O
the	O
other	O
inputs	O
.	O
the	O
output	O
softmax	O
(	O
z	O
)	O
i	O
can	O
also	O
saturate	O
to	O
when	O
zi	O
is	O
not	O
maximal	O
and	O
the	O
maximum	O
is	O
much	O
greater	O
.	O
this	O
is	O
a	O
generalization	O
of	O
the	O
way	O
that	O
sigmoid	O
units	O
saturate	O
,	O
and	O
0	O
1	O
186	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
can	O
cause	O
similar	O
diﬃculties	O
for	O
learning	O
if	O
the	O
loss	O
function	O
is	O
not	O
designed	O
to	O
compensate	O
for	O
it	O
.	O
the	O
argument	O
z	O
to	O
the	O
softmax	O
function	O
can	O
be	O
produced	O
in	O
two	O
diﬀerent	O
ways	O
.	O
the	O
most	O
common	O
is	O
simply	O
to	O
have	O
an	O
earlier	O
layer	O
of	O
the	O
neural	O
network	O
output	O
	O
every	O
element	O
of	O
z	O
,	O
as	O
described	O
above	O
using	O
the	O
linear	O
layer	O
z	O
=	O
w	O
h	O
+	O
b.	O
while	O
straightforward	O
,	O
this	O
approach	O
actually	O
overparametrizes	O
the	O
distribution	O
.	O
the	O
constraint	O
that	O
the	O
n	O
outputs	O
must	O
sum	O
to	O
means	O
that	O
only	O
1	O
parameters	O
are	O
necessary	O
;	O
the	O
probability	O
of	O
the	O
n-th	O
value	O
may	O
be	O
obtained	O
by	O
subtracting	O
the	O
ﬁrst	O
n	O
probabilities	O
from	O
.	O
we	O
can	O
thus	O
impose	O
a	O
requirement	O
that	O
one	O
element	O
of	O
z	O
be	O
ﬁxed	O
.	O
for	O
example	O
,	O
we	O
can	O
require	O
that	O
zn	O
=	O
0.	O
indeed	O
,	O
this	O
is	O
exactly	O
−	O
x	O
)	O
=	O
σ	O
(	O
z	O
)	O
is	O
equivalent	O
to	O
deﬁning	O
what	O
the	O
sigmoid	O
unit	O
does	O
.	O
deﬁning	O
p	O
(	O
y	O
=	O
1	O
p	O
(	O
y	O
=	O
1	O
1	O
argument	O
and	O
the	O
n	O
argument	O
approaches	O
to	O
the	O
softmax	O
can	O
describe	O
the	O
same	O
set	O
of	O
probability	O
distributions	O
,	O
but	O
have	O
diﬀerent	O
learning	O
dynamics	O
.	O
in	O
practice	O
,	O
there	O
is	O
rarely	O
much	O
diﬀerence	O
between	O
using	O
the	O
overparametrized	O
version	O
or	O
the	O
restricted	O
version	O
,	O
and	O
it	O
is	O
simpler	O
to	O
implement	O
the	O
overparametrized	O
version	O
.	O
x	O
)	O
=	O
softmax	O
(	O
z	O
)	O
1	O
with	O
a	O
two-dimensional	O
z	O
and	O
z1	O
=	O
0.	O
both	O
the	O
n	O
−	O
n	O
−	O
1	O
|	O
1	O
1	O
|	O
from	O
a	O
neuroscientiﬁc	O
point	O
of	O
view	O
,	O
it	O
is	O
interesting	O
to	O
think	O
of	O
the	O
softmax	O
as	O
a	O
way	O
to	O
create	O
a	O
form	O
of	O
competition	O
between	O
the	O
units	O
that	O
participate	O
in	O
it	O
:	O
the	O
softmax	O
outputs	O
always	O
sum	O
to	O
1	O
so	O
an	O
increase	O
in	O
the	O
value	O
of	O
one	O
unit	O
necessarily	O
corresponds	O
to	O
a	O
decrease	O
in	O
the	O
value	O
of	O
others	O
.	O
this	O
is	O
analogous	O
to	O
the	O
lateral	O
inhibition	O
that	O
is	O
believed	O
to	O
exist	O
between	O
nearby	O
neurons	O
in	O
the	O
cortex	O
.	O
at	O
the	O
extreme	O
(	O
when	O
the	O
diﬀerence	O
between	O
the	O
maximal	O
ai	O
and	O
the	O
others	O
is	O
large	O
in	O
magnitude	O
)	O
it	O
becomes	O
a	O
form	O
of	O
winner-take-all	O
(	O
one	O
of	O
the	O
outputs	O
is	O
nearly	O
1	O
and	O
the	O
others	O
are	O
nearly	O
0	O
)	O
.	O
the	O
name	O
“	O
softmax	O
”	O
can	O
be	O
somewhat	O
confusing	O
.	O
the	O
function	O
is	O
more	O
closely	O
related	O
to	O
the	O
arg	O
max	O
function	O
than	O
the	O
max	O
function	O
.	O
the	O
term	O
“	O
soft	O
”	O
derives	O
from	O
the	O
fact	O
that	O
the	O
softmax	O
function	O
is	O
continuous	O
and	O
diﬀerentiable	O
.	O
the	O
arg	O
max	O
function	O
,	O
with	O
its	O
result	O
represented	O
as	O
a	O
one-hot	O
vector	O
,	O
is	O
not	O
continuous	O
or	O
diﬀerentiable	O
.	O
the	O
softmax	O
function	O
thus	O
provides	O
a	O
“	O
softened	O
”	O
version	O
of	O
the	O
arg	O
max	O
.	O
the	O
corresponding	O
soft	O
version	O
of	O
the	O
maximum	O
function	O
is	O
softmax	O
(	O
z	O
)	O
z.	O
it	O
would	O
perhaps	O
be	O
better	O
to	O
call	O
the	O
softmax	O
function	O
“	O
softargmax	O
,	O
”	O
but	O
the	O
current	O
name	O
is	O
an	O
entrenched	O
convention	O
.	O
	O
6.2.2.4	O
other	O
output	O
types	O
the	O
linear	O
,	O
sigmoid	O
,	O
and	O
softmax	O
output	O
units	O
described	O
above	O
are	O
the	O
most	O
common	O
.	O
neural	O
networks	O
can	O
generalize	O
to	O
almost	O
any	O
kind	O
of	O
output	O
layer	O
that	O
we	O
wish	O
.	O
the	O
principle	O
of	O
maximum	O
likelihood	O
provides	O
a	O
guide	O
for	O
how	O
to	O
design	O
187	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
a	O
good	O
cost	O
function	O
for	O
nearly	O
any	O
kind	O
of	O
output	O
layer	O
.	O
−	O
|	O
|	O
in	O
general	O
,	O
if	O
we	O
deﬁne	O
a	O
conditional	O
distribution	O
p	O
(	O
y	O
x	O
;	O
θ	O
)	O
,	O
the	O
principle	O
of	O
maximum	O
likelihood	O
suggests	O
we	O
use	O
log	O
(	O
p	O
y	O
x	O
θ	O
;	O
)	O
as	O
our	O
cost	O
function	O
.	O
in	O
general	O
,	O
we	O
can	O
think	O
of	O
the	O
neural	O
network	O
as	O
representing	O
a	O
function	O
f	O
(	O
x	O
;	O
θ	O
)	O
.	O
the	O
outputs	O
of	O
this	O
function	O
are	O
not	O
direct	O
predictions	O
of	O
the	O
value	O
y.	O
instead	O
,	O
f	O
(	O
x	O
;	O
θ	O
)	O
=	O
ω	O
provides	O
the	O
parameters	O
for	O
a	O
distribution	O
over	O
y.	O
our	O
loss	O
function	O
can	O
then	O
be	O
interpreted	O
as	O
p	O
y	O
ω	O
x	O
log	O
(	O
;	O
.	O
(	O
)	O
)	O
−	O
|	O
for	O
example	O
,	O
we	O
may	O
wish	O
to	O
learn	O
the	O
variance	O
of	O
a	O
conditional	O
gaussian	O
for	O
y	O
,	O
given	O
x.	O
in	O
the	O
simple	O
case	O
,	O
where	O
the	O
variance	O
σ2	O
is	O
a	O
constant	O
,	O
there	O
is	O
a	O
closed	O
form	O
expression	O
because	O
the	O
maximum	O
likelihood	O
estimator	O
of	O
variance	O
is	O
simply	O
the	O
empirical	O
mean	O
of	O
the	O
squared	O
diﬀerence	O
between	O
observations	O
y	O
and	O
their	O
expected	O
value	O
.	O
a	O
computationally	O
more	O
expensive	O
approach	O
that	O
does	O
not	O
require	O
writing	O
special-case	O
code	O
is	O
to	O
simply	O
include	O
the	O
variance	O
as	O
one	O
of	O
the	O
properties	O
of	O
the	O
−	O
distribution	O
p	O
(	O
y	O
x	O
)	O
that	O
is	O
controlled	O
by	O
ω	O
=	O
f	O
(	O
x	O
;	O
θ	O
)	O
.	O
the	O
negative	O
log-likelihood	O
log	O
p	O
(	O
y	O
;	O
ω	O
(	O
x	O
)	O
)	O
will	O
then	O
provide	O
a	O
cost	O
function	O
with	O
the	O
appropriate	O
terms	O
necessary	O
to	O
make	O
our	O
optimization	O
procedure	O
incrementally	O
learn	O
the	O
variance	O
.	O
in	O
the	O
simple	O
case	O
where	O
the	O
standard	O
deviation	O
does	O
not	O
depend	O
on	O
the	O
input	O
,	O
we	O
can	O
make	O
a	O
new	O
parameter	O
in	O
the	O
network	O
that	O
is	O
copied	O
directly	O
into	O
ω	O
.	O
this	O
new	O
parameter	O
might	O
be	O
σ	O
itself	O
or	O
could	O
be	O
a	O
parameter	O
v	O
representing	O
σ2	O
or	O
it	O
could	O
be	O
a	O
parameter	O
β	O
representing	O
1	O
σ	O
2	O
,	O
depending	O
on	O
how	O
we	O
choose	O
to	O
parametrize	O
the	O
distribution	O
.	O
we	O
may	O
wish	O
our	O
model	B
to	O
predict	O
a	O
diﬀerent	O
amount	O
of	O
variance	O
in	O
y	O
for	O
diﬀerent	O
values	O
of	O
x.	O
this	O
is	O
called	O
a	O
heteroscedastic	O
model	B
.	O
in	O
the	O
heteroscedastic	O
case	O
,	O
we	O
simply	O
make	O
the	O
speciﬁcation	O
of	O
the	O
variance	O
be	O
one	O
of	O
the	O
values	O
output	O
by	O
f	O
(	O
x	O
;	O
θ	O
)	O
.	O
a	O
typical	O
way	O
to	O
do	O
this	O
is	O
to	O
formulate	O
the	O
gaussian	O
distribution	O
using	O
precision	O
,	O
rather	O
than	O
variance	O
,	O
as	O
described	O
in	O
equation	O
3.22	O
.	O
in	O
the	O
multivariate	O
case	O
it	O
is	O
most	O
common	O
to	O
use	O
a	O
diagonal	O
precision	O
matrix	O
diag	O
(	O
)	O
β	O
.	O
(	O
6.34	O
)	O
this	O
formulation	O
works	O
well	O
with	O
gradient	O
descent	B
because	O
the	O
formula	O
for	O
the	O
log-likelihood	O
of	O
the	O
gaussian	O
distribution	O
parametrized	O
by	O
β	O
involves	O
only	O
mul-	O
tiplication	O
by	O
βi	O
and	O
addition	O
of	O
log	O
βi	O
.	O
the	O
gradient	O
of	O
multiplication	O
,	O
addition	O
,	O
and	O
logarithm	O
operations	O
is	O
well-behaved	O
.	O
by	O
comparison	O
,	O
if	O
we	O
parametrized	O
the	O
output	O
in	O
terms	O
of	O
variance	O
,	O
we	O
would	O
need	O
to	O
use	O
division	O
.	O
the	O
division	O
function	O
becomes	O
arbitrarily	O
steep	O
near	O
zero	O
.	O
while	O
large	O
gradients	O
can	O
help	O
learning	O
,	O
arbitrarily	O
large	O
gradients	O
usually	O
result	O
in	O
instability	O
.	O
if	O
we	O
parametrized	O
the	O
output	O
in	O
terms	O
of	O
standard	O
deviation	O
,	O
the	O
log-likelihood	O
would	O
still	O
involve	O
division	O
,	O
and	O
would	O
also	O
involve	O
squaring	O
.	O
the	O
gradient	O
through	O
the	O
squaring	O
operation	O
can	O
vanish	O
near	O
zero	O
,	O
making	O
it	O
diﬃcult	O
to	O
learn	O
parameters	O
that	O
are	O
squared	O
.	O
188	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
regardless	O
of	O
whether	O
we	O
use	O
standard	O
deviation	O
,	O
variance	O
,	O
or	O
precision	O
,	O
we	O
must	O
ensure	O
that	O
the	O
covariance	O
matrix	O
of	O
the	O
gaussian	O
is	O
positive	O
deﬁnite	O
.	O
because	O
the	O
eigenvalues	O
of	O
the	O
precision	O
matrix	O
are	O
the	O
reciprocals	O
of	O
the	O
eigenvalues	O
of	O
the	O
covariance	O
matrix	O
,	O
this	O
is	O
equivalent	O
to	O
ensuring	O
that	O
the	O
precision	O
matrix	O
is	O
positive	O
deﬁnite	O
.	O
if	O
we	O
use	O
a	O
diagonal	O
matrix	O
,	O
or	O
a	O
scalar	O
times	O
the	O
diagonal	O
matrix	O
,	O
then	O
the	O
only	O
condition	O
we	O
need	O
to	O
enforce	O
on	O
the	O
output	O
of	O
the	O
model	B
is	O
positivity	O
.	O
if	O
we	O
suppose	O
that	O
a	O
is	O
the	O
raw	O
activation	O
of	O
the	O
model	B
used	O
to	O
determine	O
the	O
diagonal	O
precision	O
,	O
we	O
can	O
use	O
the	O
softplus	O
function	O
to	O
obtain	O
a	O
positive	O
precision	O
vector	O
:	O
β	O
=	O
ζ	O
(	O
a	O
)	O
.	O
this	O
same	O
strategy	O
applies	O
equally	O
if	O
using	O
variance	O
or	O
standard	O
deviation	O
rather	O
than	O
precision	O
or	O
if	O
using	O
a	O
scalar	O
times	O
identity	O
rather	O
than	O
diagonal	O
matrix	O
.	O
it	O
is	O
rare	O
to	O
learn	O
a	O
covariance	O
or	O
precision	O
matrix	O
with	O
richer	O
structure	O
than	O
diagonal	O
.	O
if	O
the	O
covariance	O
is	O
full	O
and	O
conditional	O
,	O
then	O
a	O
parametrization	O
must	O
be	O
chosen	O
that	O
guarantees	O
positive-deﬁniteness	O
of	O
the	O
predicted	O
covariance	O
matrix	O
.	O
this	O
can	O
be	O
achieved	O
by	O
writing	O
σ	O
(	O
)	O
=	O
(	O
)	O
x	O
,	O
where	O
b	O
is	O
an	O
unconstrained	O
square	O
matrix	O
.	O
one	O
practical	O
issue	O
if	O
the	O
matrix	O
is	O
full	O
rank	O
is	O
that	O
computing	O
the	O
matrix	O
requiring	O
o	O
(	O
d3	O
)	O
computation	O
for	O
the	O
likelihood	O
is	O
expensive	O
,	O
with	O
a	O
d	O
determinant	O
and	O
inverse	O
of	O
σ	O
(	O
x	O
)	O
(	O
or	O
equivalently	O
,	O
and	O
more	O
commonly	O
done	O
,	O
its	O
eigendecomposition	O
or	O
that	O
of	O
x	O
b	O
x	O
b	O
×	O
b	O
x	O
(	O
)	O
(	O
)	O
	O
)	O
.	O
d	O
|	O
we	O
often	O
want	O
to	O
perform	O
multimodal	O
regression	O
,	O
that	O
is	O
,	O
to	O
predict	O
real	O
values	O
that	O
come	O
from	O
a	O
conditional	O
distribution	O
p	O
(	O
y	O
x	O
)	O
that	O
can	O
have	O
several	O
diﬀerent	O
peaks	O
in	O
y	O
space	O
for	O
the	O
same	O
value	O
of	O
x.	O
in	O
this	O
case	O
,	O
a	O
gaussian	O
mixture	O
is	O
a	O
natural	O
representation	O
for	O
the	O
output	O
(	O
neural	O
networks	O
with	O
gaussian	O
mixtures	O
as	O
their	O
output	O
are	O
often	O
called	O
mixture	O
density	O
networks	O
.	O
a	O
gaussian	O
mixture	O
output	O
with	O
n	O
components	O
is	O
deﬁned	O
by	O
the	O
conditional	O
probability	O
distribution	O
jacobs	O
et	O
al	O
.	O
1991	O
bishop	O
1994	O
	O
)	O
.	O
,	O
;	O
,	O
|	O
)	O
=	O
p	O
(	O
y	O
x	O
|	O
n	O
x	O
)	O
p	O
c	O
(	O
=	O
i	O
n	O
i=1	O
(	O
;	O
y	O
µ	O
(	O
)	O
i	O
(	O
)	O
x	O
,	O
σ	O
(	O
)	O
i	O
(	O
)	O
)	O
x	O
.	O
(	O
6.35	O
)	O
|	O
the	O
neural	O
network	O
must	O
have	O
three	O
outputs	O
:	O
a	O
vector	O
deﬁning	O
p	O
(	O
c	O
=	O
i	O
x	O
)	O
,	O
a	O
matrix	O
providing	O
µ	O
(	O
)	O
i	O
(	O
x	O
)	O
for	O
all	O
i	O
,	O
and	O
a	O
tensor	O
providing	O
σ	O
(	O
)	O
i	O
(	O
x	O
)	O
for	O
all	O
i.	O
these	O
outputs	O
must	O
satisfy	O
diﬀerent	O
constraints	O
:	O
1.	O
mixture	O
components	O
p	O
(	O
c	O
=	O
i	O
x	O
)	O
:	O
these	O
form	O
a	O
multinoulli	O
distribution	O
over	O
the	O
n	O
diﬀerent	O
components	O
associated	O
with	O
latent	O
variable1	O
c	O
,	O
and	O
can	O
1we	O
consider	O
c	O
to	O
be	O
latent	O
because	O
we	O
do	O
not	O
observe	O
it	O
in	O
the	O
data	O
:	O
given	O
input	O
x	O
and	O
target	O
y	O
,	O
it	O
is	O
not	O
possible	O
to	O
know	O
with	O
certainty	O
which	O
gaussian	O
component	O
was	O
responsible	O
for	O
y	O
,	O
but	O
we	O
can	O
imagine	O
that	O
y	O
was	O
generated	O
by	O
picking	O
one	O
of	O
them	O
,	O
and	O
make	O
that	O
unobserved	O
choice	O
a	O
random	O
variable	O
.	O
189	O
|	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
typically	O
be	O
obtained	O
by	O
a	O
softmax	O
over	O
an	O
n-dimensional	O
vector	O
,	O
to	O
guarantee	O
that	O
these	O
outputs	O
are	O
positive	O
and	O
sum	O
to	O
1.	O
d	O
×	O
2.	O
means	O
µ	O
(	O
)	O
i	O
(	O
x	O
)	O
:	O
these	O
indicate	O
the	O
center	O
or	O
mean	O
associated	O
with	O
the	O
i-th	O
gaussian	O
component	O
,	O
and	O
are	O
unconstrained	O
(	O
typically	O
with	O
no	O
nonlinearity	O
at	O
all	O
for	O
these	O
output	O
units	O
)	O
.	O
if	O
y	O
is	O
a	O
d-vector	O
,	O
then	O
the	O
network	O
must	O
output	O
an	O
n	O
matrix	O
containing	O
all	O
n	O
of	O
these	O
d-dimensional	O
vectors	O
.	O
learning	O
these	O
means	O
with	O
maximum	O
likelihood	O
is	O
slightly	O
more	O
complicated	O
than	O
learning	O
the	O
means	O
of	O
a	O
distribution	O
with	O
only	O
one	O
output	O
mode	O
.	O
we	O
only	O
want	O
to	O
update	O
the	O
mean	O
for	O
the	O
component	O
that	O
actually	O
produced	O
the	O
observation	O
.	O
in	O
practice	O
,	O
we	O
do	O
not	O
know	O
which	O
component	O
produced	O
each	O
observation	O
.	O
the	O
expression	O
for	O
the	O
negative	O
log-likelihood	O
naturally	O
weights	O
each	O
example	O
’	O
s	O
contribution	O
to	O
the	O
loss	O
for	O
each	O
component	O
by	O
the	O
probability	O
that	O
the	O
component	O
produced	O
the	O
example	O
.	O
3.	O
covariances	O
σ	O
(	O
)	O
i	O
(	O
x	O
)	O
:	O
these	O
specify	O
the	O
covariance	O
matrix	O
for	O
each	O
component	O
i.	O
as	O
when	O
learning	O
a	O
single	O
gaussian	O
component	O
,	O
we	O
typically	O
use	O
a	O
diagonal	O
matrix	O
to	O
avoid	O
needing	O
to	O
compute	O
determinants	O
.	O
as	O
with	O
learning	O
the	O
means	O
of	O
the	O
mixture	O
,	O
maximum	O
likelihood	O
is	O
complicated	O
by	O
needing	O
to	O
assign	O
partial	O
responsibility	O
for	O
each	O
point	O
to	O
each	O
mixture	O
component	O
.	O
gradient	O
descent	B
will	O
automatically	O
follow	O
the	O
correct	O
process	O
if	O
given	O
the	O
correct	O
speciﬁcation	O
of	O
the	O
negative	O
log-likelihood	O
under	O
the	O
mixture	O
model	B
.	O
it	O
has	O
been	O
reported	O
that	O
gradient-based	O
optimization	O
of	O
conditional	O
gaussian	O
mixtures	O
(	O
on	O
the	O
output	O
of	O
neural	O
networks	O
)	O
can	O
be	O
unreliable	O
,	O
in	O
part	O
because	O
one	O
gets	O
divisions	O
(	O
by	O
the	O
variance	O
)	O
which	O
can	O
be	O
numerically	O
unstable	O
(	O
when	O
some	O
variance	O
gets	O
to	O
be	O
small	O
for	O
a	O
particular	O
example	O
,	O
yielding	O
very	O
large	O
gradients	O
)	O
.	O
one	O
solution	O
is	O
to	O
clip	O
gradients	O
(	O
see	O
section	O
)	O
while	O
another	O
is	O
to	O
scale	O
the	O
gradients	O
heuristically	O
(	O
murray	O
and	O
larochelle	O
2014	O
10.11.1	O
)	O
.	O
,	O
,	O
)	O
or	O
movements	O
of	O
physical	O
objects	O
(	O
graves	O
2013	O
gaussian	O
mixture	O
outputs	O
are	O
particularly	O
eﬀective	O
in	O
generative	O
models	O
of	O
speech	O
(	O
schuster	O
1999	O
)	O
.	O
the	O
mixture	O
density	O
strategy	O
gives	O
a	O
way	O
for	O
the	O
network	O
to	O
represent	O
multiple	O
output	O
modes	O
and	O
to	O
control	O
the	O
variance	O
of	O
its	O
output	O
,	O
which	O
is	O
crucial	O
for	O
obtaining	O
a	O
high	O
degree	O
of	O
quality	O
in	O
these	O
real-valued	O
domains	O
.	O
an	O
example	O
of	O
a	O
mixture	O
density	O
network	O
is	O
shown	O
in	O
ﬁgure	O
.6.4	O
,	O
in	O
general	O
,	O
we	O
may	O
wish	O
to	O
continue	O
to	O
model	B
larger	O
vectors	O
y	O
containing	O
more	O
variables	O
,	O
and	O
to	O
impose	O
richer	O
and	O
richer	O
structures	O
on	O
these	O
output	O
variables	O
.	O
for	O
example	O
,	O
we	O
may	O
wish	O
for	O
our	O
neural	O
network	O
to	O
output	O
a	O
sequence	O
of	O
characters	O
that	O
forms	O
a	O
sentence	O
.	O
in	O
these	O
cases	O
,	O
we	O
may	O
continue	O
to	O
use	O
the	O
principle	O
of	O
maximum	O
likelihood	O
applied	O
to	O
our	O
model	B
p	O
(	O
y	O
;	O
ω	O
(	O
x	O
)	O
)	O
,	O
but	O
the	O
model	B
we	O
use	O
190	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
y	O
x	O
figure	O
6.4	O
:	O
samples	O
drawn	O
from	O
a	O
neural	O
network	O
with	O
a	O
mixture	O
density	O
output	O
layer	O
.	O
|	O
the	O
input	O
x	O
is	O
sampled	O
from	O
a	O
uniform	O
distribution	O
and	O
the	O
output	O
y	O
is	O
sampled	O
from	O
)	O
.	O
the	O
neural	O
network	O
is	O
able	O
to	O
learn	O
nonlinear	O
mappings	O
from	O
the	O
input	O
to	O
pmodel	O
(	O
y	O
x	O
the	O
parameters	O
of	O
the	O
output	O
distribution	O
.	O
these	O
parameters	O
include	O
the	O
probabilities	O
governing	O
which	O
of	O
three	O
mixture	O
components	O
will	O
generate	O
the	O
output	O
as	O
well	O
as	O
the	O
parameters	O
for	O
each	O
mixture	O
component	O
.	O
each	O
mixture	O
component	O
is	O
gaussian	O
with	O
predicted	O
mean	O
and	O
variance	O
.	O
all	O
of	O
these	O
aspects	O
of	O
the	O
output	O
distribution	O
are	O
able	O
to	O
vary	O
with	O
respect	O
to	O
the	O
input	O
,	O
and	O
to	O
do	O
so	O
in	O
nonlinear	O
ways	O
.	O
x	O
to	O
describe	O
y	O
becomes	O
complex	O
enough	O
to	O
be	O
beyond	O
the	O
scope	O
of	O
this	O
chapter	O
.	O
chapter	O
describes	O
how	O
to	O
use	O
recurrent	O
neural	O
networks	O
to	O
deﬁne	O
such	O
models	O
over	O
sequences	O
,	O
and	O
part	O
describes	O
advanced	O
techniques	O
for	O
modeling	O
arbitrary	O
probability	O
distributions	O
.	O
iii	O
10	O
6.3	O
hidden	O
units	O
so	O
far	O
we	O
have	O
focused	O
our	O
discussion	O
on	O
design	O
choices	O
for	O
neural	O
networks	O
that	O
are	O
common	O
to	O
most	O
parametric	O
machine	O
learning	O
models	O
trained	O
with	O
gradient-	O
based	O
optimization	O
.	O
now	O
we	O
turn	O
to	O
an	O
issue	O
that	O
is	O
unique	O
to	O
feedforward	O
neural	O
networks	O
:	O
how	O
to	O
choose	O
the	O
type	O
of	O
hidden	O
unit	O
to	O
use	O
in	O
the	O
hidden	O
layers	O
of	O
the	O
model	B
.	O
the	O
design	O
of	O
hidden	O
units	O
is	O
an	O
extremely	O
active	O
area	O
of	O
research	O
and	O
does	O
not	O
yet	O
have	O
many	O
deﬁnitive	O
guiding	O
theoretical	O
principles	O
.	O
rectiﬁed	O
linear	O
units	O
are	O
an	O
excellent	O
default	O
choice	O
of	O
hidden	O
unit	O
.	O
many	O
other	O
types	O
of	O
hidden	O
units	O
are	O
available	O
.	O
it	O
can	O
be	O
diﬃcult	O
to	O
determine	O
when	O
to	O
use	O
which	O
kind	O
(	O
though	O
rectiﬁed	O
linear	O
units	O
are	O
usually	O
an	O
acceptable	O
choice	O
)	O
.	O
we	O
191	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
describe	O
here	O
some	O
of	O
the	O
basic	O
intuitions	O
motivating	O
each	O
type	O
of	O
hidden	O
units	O
.	O
these	O
intuitions	O
can	O
help	O
decide	O
when	O
to	O
try	O
out	O
each	O
of	O
these	O
units	O
.	O
it	O
is	O
usually	O
impossible	O
to	O
predict	O
in	O
advance	O
which	O
will	O
work	B
best	O
.	O
the	O
design	O
process	O
consists	O
of	O
trial	O
and	O
error	O
,	O
intuiting	O
that	O
a	O
kind	O
of	O
hidden	O
unit	O
may	O
work	B
well	O
,	O
and	O
then	O
training	O
a	O
network	O
with	O
that	O
kind	O
of	O
hidden	O
unit	O
and	O
evaluating	O
its	O
performance	O
on	O
a	O
validation	O
set.	O
}	O
.	O
these	O
ideas	O
will	O
be	O
described	O
further	O
in	O
chapter	O
{	O
some	O
of	O
the	O
hidden	O
units	O
included	O
in	O
this	O
list	O
are	O
not	O
actually	O
diﬀerentiable	O
at	O
all	O
input	O
points	O
.	O
for	O
example	O
,	O
the	O
rectiﬁed	O
linear	O
function	O
g	O
(	O
z	O
)	O
=	O
max	O
is	O
not	O
0	O
,	O
z	O
diﬀerentiable	O
at	O
z	O
=	O
0.	O
this	O
may	O
seem	O
like	O
it	O
invalidates	O
g	O
for	O
use	O
with	O
a	O
gradient-	O
based	O
learning	O
algorithm	O
.	O
in	O
practice	O
,	O
gradient	O
descent	B
still	O
performs	O
well	O
enough	O
for	O
these	O
models	O
to	O
be	O
used	O
for	O
machine	O
learning	O
tasks	O
.	O
this	O
is	O
in	O
part	O
because	O
neural	O
network	O
training	O
algorithms	O
do	O
not	O
usually	O
arrive	O
at	O
a	O
local	O
minimum	O
of	O
the	O
cost	O
function	O
,	O
but	O
instead	O
merely	O
reduce	O
its	O
value	O
signiﬁcantly	O
,	O
as	O
shown	O
in	O
ﬁgure	O
.	O
because	O
we	O
do	O
not	O
8	O
expect	O
training	O
to	O
actually	O
reach	O
a	O
point	O
where	O
the	O
gradient	O
is	O
0	O
,	O
it	O
is	O
acceptable	O
for	O
the	O
minima	O
of	O
the	O
cost	O
function	O
to	O
correspond	O
to	O
points	O
with	O
undeﬁned	O
gradient	O
.	O
hidden	O
units	O
that	O
are	O
not	O
diﬀerentiable	O
are	O
usually	O
non-diﬀerentiable	O
at	O
only	O
a	O
small	O
number	O
of	O
points	O
.	O
in	O
general	O
,	O
a	O
function	O
g	O
(	O
z	O
)	O
has	O
a	O
left	O
derivative	O
deﬁned	O
by	O
the	O
slope	O
of	O
the	O
function	O
immediately	O
to	O
the	O
left	O
of	O
z	O
and	O
a	O
right	O
derivative	O
deﬁned	O
by	O
the	O
slope	O
of	O
the	O
function	O
immediately	O
to	O
the	O
right	O
of	O
z.	O
a	O
function	O
is	O
diﬀerentiable	O
at	O
z	O
only	O
if	O
both	O
the	O
left	O
derivative	O
and	O
the	O
right	O
derivative	O
are	O
deﬁned	O
and	O
equal	O
to	O
each	O
other	O
.	O
the	O
functions	O
used	O
in	O
the	O
context	O
of	O
neural	O
{	O
networks	O
usually	O
have	O
deﬁned	O
left	O
derivatives	O
and	O
deﬁned	O
right	O
derivatives	O
.	O
in	O
the	O
case	O
of	O
g	O
(	O
z	O
)	O
=	O
max	O
and	O
the	O
right	O
derivative	O
is	O
.	O
software	O
implementations	O
of	O
neural	O
network	O
training	O
usually	O
return	O
one	O
of	O
the	O
one-sided	O
derivatives	O
rather	O
than	O
reporting	O
that	O
the	O
derivative	O
is	O
undeﬁned	O
or	O
raising	O
an	O
error	O
.	O
this	O
may	O
be	O
heuristically	O
justiﬁed	O
by	O
observing	O
that	O
gradient-	O
based	O
optimization	O
on	O
a	O
digital	O
computer	O
is	O
subject	O
to	O
numerical	O
error	O
anyway	O
.	O
when	O
a	O
function	O
is	O
asked	O
to	O
evaluate	O
g	O
(	O
0	O
)	O
,	O
it	O
is	O
very	O
unlikely	O
that	O
the	O
underlying	O
	O
that	O
was	O
rounded	O
value	O
truly	O
was	O
to	O
.	O
in	O
some	O
contexts	O
,	O
more	O
theoretically	O
pleasing	O
justiﬁcations	O
are	O
available	O
,	O
but	O
these	O
usually	O
do	O
not	O
apply	O
to	O
neural	O
network	O
training	O
.	O
the	O
important	O
point	O
is	O
that	O
in	O
practice	O
one	O
can	O
safely	O
disregard	O
the	O
non-diﬀerentiability	O
of	O
the	O
hidden	O
unit	O
activation	O
functions	O
described	O
below	O
.	O
.	O
instead	O
,	O
it	O
was	O
likely	O
to	O
be	O
some	O
small	O
value	O
0	O
0	O
,	O
z	O
,	O
the	O
left	O
derivative	O
at	O
z	O
=	O
0	O
0is	O
}	O
4.3	O
1	O
0	O
unless	O
indicated	O
otherwise	O
,	O
most	O
hidden	O
units	O
can	O
be	O
described	O
as	O
accepting	O
a	O
vector	O
of	O
inputs	O
x	O
,	O
computing	O
an	O
aﬃne	O
transformation	O
z	O
=	O
w	O
x	O
+	O
b	O
,	O
and	O
then	O
applying	O
an	O
element-wise	O
nonlinear	O
function	O
g	O
(	O
z	O
)	O
.	O
most	O
hidden	O
units	O
are	O
distinguished	O
from	O
each	O
other	O
only	O
by	O
the	O
choice	O
of	O
the	O
form	O
of	O
the	O
activation	O
function	O
.	O
g	O
(	O
)	O
z	O
	O
192	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
{	O
(	O
)	O
=	O
max	O
0	O
g	O
z	O
,	O
z	O
}	O
.	O
6.3.1	O
rectiﬁed	O
linear	O
units	O
and	O
their	O
generalizations	O
rectiﬁed	O
linear	O
units	O
use	O
the	O
activation	O
function	O
rectiﬁed	O
linear	O
units	O
are	O
easy	O
to	O
optimize	O
because	O
they	O
are	O
so	O
similar	O
to	O
linear	O
units	O
.	O
the	O
only	O
diﬀerence	O
between	O
a	O
linear	O
unit	O
and	O
a	O
rectiﬁed	O
linear	O
unit	O
is	O
that	O
a	O
rectiﬁed	O
linear	O
unit	O
outputs	O
zero	O
across	O
half	O
its	O
domain	O
.	O
this	O
makes	O
the	O
derivatives	O
through	O
a	O
rectiﬁed	O
linear	O
unit	O
remain	O
large	O
whenever	O
the	O
unit	O
is	O
active	O
.	O
the	O
gradients	O
are	O
not	O
only	O
large	O
but	O
also	O
consistent	O
.	O
the	O
second	O
derivative	O
of	O
the	O
rectifying	O
operation	O
is	O
almost	O
everywhere	O
,	O
and	O
the	O
derivative	O
of	O
the	O
rectifying	O
operation	O
is	O
everywhere	O
that	O
the	O
unit	O
is	O
active	O
.	O
this	O
means	O
that	O
the	O
gradient	O
direction	O
is	O
far	O
more	O
useful	O
for	O
learning	O
than	O
it	O
would	O
be	O
with	O
activation	O
functions	O
that	O
introduce	O
second-order	O
eﬀects	O
.	O
0	O
1	O
rectiﬁed	O
linear	O
units	O
are	O
typically	O
used	O
on	O
top	O
of	O
an	O
aﬃne	O
transformation	O
:	O
h	O
w=	O
(	O
g	O
	O
x	O
b+	O
)	O
.	O
(	O
6.36	O
)	O
when	O
initializing	O
the	O
parameters	O
of	O
the	O
aﬃne	O
transformation	O
,	O
it	O
can	O
be	O
a	O
good	O
practice	O
to	O
set	O
all	O
elements	O
of	O
b	O
to	O
a	O
small	O
,	O
positive	O
value	O
,	O
such	O
as	O
0.1.	O
this	O
makes	O
it	O
very	O
likely	O
that	O
the	O
rectiﬁed	O
linear	O
units	O
will	O
be	O
initially	O
active	O
for	O
most	O
inputs	O
in	O
the	O
training	O
set	O
and	O
allow	O
the	O
derivatives	O
to	O
pass	O
through	O
.	O
several	O
generalizations	O
of	O
rectiﬁed	O
linear	O
units	O
exist	O
.	O
most	O
of	O
these	O
general-	O
izations	O
perform	O
comparably	O
to	O
rectiﬁed	O
linear	O
units	O
and	O
occasionally	O
perform	O
better	O
.	O
one	O
drawback	O
to	O
rectiﬁed	O
linear	O
units	O
is	O
that	O
they	O
can	O
not	O
learn	O
via	O
gradient-	O
based	O
methods	O
on	O
examples	O
for	O
which	O
their	O
activation	O
is	O
zero	O
.	O
a	O
variety	O
of	O
generalizations	O
of	O
rectiﬁed	O
linear	O
units	O
guarantee	O
that	O
they	O
receive	O
gradient	O
every-	O
where	O
.	O
−	O
three	O
generalizations	O
of	O
rectiﬁed	O
linear	O
units	O
are	O
based	O
on	O
using	O
a	O
non-zero	O
|	O
|	O
)	O
i	O
=	O
max	O
(	O
0	O
,	O
zi	O
)	O
+	O
αi	O
min	O
(	O
0	O
,	O
zi	O
)	O
.	O
absolute	O
value	O
slope	O
αi	O
when	O
zi	O
<	O
0	O
:	O
hi	O
=	O
g	O
(	O
z	O
α	O
,	O
z	O
.	O
it	O
is	O
used	O
for	O
object	O
recognition	B
rectiﬁcation	O
ﬁxes	O
αi	O
=	O
from	O
images	O
(	O
)	O
,	O
where	O
it	O
makes	O
sense	O
to	O
seek	O
features	O
that	O
are	O
invariant	O
under	O
a	O
polarity	O
reversal	O
of	O
the	O
input	O
illumination	O
.	O
other	O
generalizations	O
of	O
rectiﬁed	O
linear	O
units	O
are	O
more	O
broadly	O
applicable	O
.	O
a	O
leaky	O
relu	O
(	O
maas	O
et	O
al	O
.	O
,	O
2013	O
)	O
ﬁxes	O
αi	O
to	O
a	O
small	O
value	O
like	O
0.01	O
while	O
a	O
parametric	O
relu	O
or	O
prelu	O
treats	O
αi	O
as	O
a	O
learnable	O
parameter	O
(	O
1	O
to	O
obtain	O
g	O
(	O
z	O
)	O
=	O
,	O
jarrett	O
et	O
al	O
.	O
2009	O
he	O
et	O
al	O
.	O
2015	O
)	O
.	O
,	O
maxout	O
units	O
(	O
)	O
generalize	O
rectiﬁed	O
linear	O
units	O
further	O
.	O
instead	O
of	O
applying	O
an	O
element-wise	O
function	O
g	O
(	O
z	O
)	O
,	O
maxout	O
units	O
divide	O
z	O
into	O
groups	O
of	O
k	O
values	O
.	O
each	O
maxout	O
unit	O
then	O
outputs	O
the	O
maximum	O
element	O
of	O
goodfellow	O
et	O
al	O
.	O
2013a	O
,	O
193	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
one	O
of	O
these	O
groups	O
:	O
g	O
(	O
)	O
z	O
i	O
=	O
max	O
(	O
6.37	O
)	O
}	O
.	O
where	O
g	O
this	O
provides	O
a	O
way	O
of	O
learning	O
a	O
piecewise	O
linear	O
function	O
that	O
responds	O
to	O
multiple	O
directions	O
in	O
the	O
input	O
is	O
the	O
set	O
of	O
indices	O
into	O
the	O
inputs	O
for	O
group	O
i	O
,	O
1	O
)	O
k	O
+	O
1	O
,	O
.	O
.	O
.	O
,	O
ik	O
space	O
.	O
−	O
{	O
zj	O
∈	O
j	O
(	O
)	O
i	O
g	O
(	O
)	O
i	O
(	O
i	O
x	O
a	O
maxout	O
unit	O
can	O
learn	O
a	O
piecewise	O
linear	O
,	O
convex	O
function	O
with	O
up	O
to	O
k	O
pieces	O
.	O
maxout	O
units	O
can	O
thus	O
be	O
seen	O
as	O
learning	O
the	O
activation	O
function	O
itself	O
rather	O
than	O
just	O
the	O
relationship	O
between	O
units	O
.	O
with	O
large	O
enough	O
k	O
,	O
a	O
maxout	O
unit	O
can	O
learn	O
to	O
approximate	O
any	O
convex	O
function	O
with	O
arbitrary	O
ﬁdelity	O
.	O
in	O
particular	O
,	O
a	O
maxout	O
layer	O
with	O
two	O
pieces	O
can	O
learn	O
to	O
implement	O
the	O
same	O
function	O
of	O
the	O
input	O
x	O
as	O
a	O
traditional	O
layer	O
using	O
the	O
rectiﬁed	O
linear	O
activation	O
function	O
,	O
absolute	O
value	O
rectiﬁcation	O
function	O
,	O
or	O
the	O
leaky	O
or	O
parametric	O
relu	O
,	O
or	O
can	O
learn	O
to	O
implement	O
a	O
totally	O
diﬀerent	O
function	O
altogether	O
.	O
the	O
maxout	O
layer	O
will	O
of	O
course	O
be	O
parametrized	O
diﬀerently	O
from	O
any	O
of	O
these	O
other	O
layer	O
types	O
,	O
so	O
the	O
learning	O
dynamics	O
will	O
be	O
diﬀerent	O
even	O
in	O
the	O
cases	O
where	O
maxout	O
learns	O
to	O
implement	O
the	O
same	O
function	O
of	O
as	O
one	O
of	O
the	O
other	O
layer	O
types	O
.	O
x	O
each	O
maxout	O
unit	O
is	O
now	O
parametrized	O
by	O
k	O
weight	O
vectors	O
instead	O
of	O
just	O
one	O
,	O
so	O
maxout	O
units	O
typically	O
need	O
more	O
regularization	O
than	O
rectiﬁed	O
linear	O
units	O
.	O
they	O
can	O
work	B
well	O
without	O
regularization	O
if	O
the	O
training	O
set	O
is	O
large	O
and	O
the	O
number	O
of	O
pieces	O
per	O
unit	O
is	O
kept	O
low	O
(	O
cai	O
et	O
al	O
.	O
2013	O
)	O
.	O
,	O
maxout	O
units	O
have	O
a	O
few	O
other	O
beneﬁts	O
.	O
in	O
some	O
cases	O
,	O
one	O
can	O
gain	O
some	O
sta-	O
tistical	O
and	O
computational	O
advantages	O
by	O
requiring	O
fewer	O
parameters	O
.	O
speciﬁcally	O
,	O
if	O
the	O
features	O
captured	O
by	O
n	O
diﬀerent	O
linear	O
ﬁlters	O
can	O
be	O
summarized	O
without	O
losing	O
information	O
by	O
taking	O
the	O
max	O
over	O
each	O
group	O
of	O
k	O
features	O
,	O
then	O
the	O
next	O
layer	O
can	O
get	O
by	O
with	O
times	O
fewer	O
weights	O
.	O
k	O
because	O
each	O
unit	O
is	O
driven	O
by	O
multiple	O
ﬁlters	O
,	O
maxout	O
units	O
have	O
some	O
redun-	O
dancy	O
that	O
helps	O
them	O
to	O
resist	O
a	O
phenomenon	O
called	O
catastrophic	O
forgetting	O
in	O
which	O
neural	O
networks	O
forget	O
how	O
to	O
perform	O
tasks	O
that	O
they	O
were	O
trained	O
on	O
in	O
the	O
past	O
(	O
goodfellow	O
et	O
al	O
.	O
2014a	O
)	O
.	O
,	O
rectiﬁed	O
linear	O
units	O
and	O
all	O
of	O
these	O
generalizations	O
of	O
them	O
are	O
based	O
on	O
the	O
principle	O
that	O
models	O
are	O
easier	O
to	O
optimize	O
if	O
their	O
behavior	O
is	O
closer	O
to	O
linear	O
.	O
this	O
same	O
general	O
principle	O
of	O
using	O
linear	O
behavior	O
to	O
obtain	O
easier	O
optimization	O
also	O
applies	O
in	O
other	O
contexts	O
besides	O
deep	O
linear	O
networks	O
.	O
recurrent	O
networks	O
can	O
learn	O
from	O
sequences	O
and	O
produce	O
a	O
sequence	O
of	O
states	O
and	O
outputs	O
.	O
when	O
training	O
them	O
,	O
one	O
needs	O
to	O
propagate	O
information	O
through	O
several	O
time	O
steps	O
,	O
which	O
is	O
much	O
easier	O
when	O
some	O
linear	O
computations	O
(	O
with	O
some	O
directional	O
derivatives	O
being	O
of	O
magnitude	O
near	O
1	O
)	O
are	O
involved	O
.	O
one	O
of	O
the	O
best-performing	O
recurrent	O
network	O
194	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
architectures	O
,	O
the	O
lstm	O
,	O
propagates	O
information	O
through	O
time	O
via	O
summation—a	O
particular	O
straightforward	O
kind	O
of	O
such	O
linear	O
activation	O
.	O
this	O
is	O
discussed	O
further	O
in	O
section	O
10.10	O
.	O
6.3.2	O
logistic	O
sigmoid	O
and	O
hyperbolic	O
tangent	O
prior	O
to	O
the	O
introduction	O
of	O
rectiﬁed	O
linear	O
units	O
,	O
most	O
neural	O
networks	O
used	O
the	O
logistic	O
sigmoid	O
activation	O
function	O
g	O
z	O
(	O
)	O
=	O
(	O
)	O
σ	O
z	O
(	O
6.38	O
)	O
or	O
the	O
hyperbolic	O
tangent	O
activation	O
function	O
g	O
z	O
(	O
)	O
=	O
tanh	B
(	O
)	O
z	O
.	O
these	O
activation	O
functions	O
are	O
closely	O
related	O
because	O
tanh	B
(	O
)	O
=	O
2	O
(	O
2	O
)	O
σ	O
z	O
z	O
−	O
(	O
6.39	O
)	O
.	O
1	O
1	O
we	O
have	O
already	O
seen	O
sigmoid	O
units	O
as	O
output	O
units	O
,	O
used	O
to	O
predict	O
the	O
probability	O
that	O
a	O
binary	O
variable	O
is	O
.	O
unlike	O
piecewise	O
linear	O
units	O
,	O
sigmoidal	O
units	O
saturate	O
across	O
most	O
of	O
their	O
domain—they	O
saturate	O
to	O
a	O
high	O
value	O
when	O
z	O
is	O
very	O
positive	O
,	O
saturate	O
to	O
a	O
low	O
value	O
when	O
z	O
is	O
very	O
negative	O
,	O
and	O
are	O
only	O
strongly	O
sensitive	O
to	O
their	O
input	O
when	O
z	O
is	O
near	O
0.	O
the	O
widespread	O
saturation	O
of	O
sigmoidal	O
units	O
can	O
make	O
gradient-based	O
learning	O
very	O
diﬃcult	O
.	O
for	O
this	O
reason	O
,	O
their	O
use	O
as	O
hidden	O
units	O
in	O
feedforward	O
networks	O
is	O
now	O
discouraged	O
.	O
their	O
use	O
as	O
output	O
units	O
is	O
compatible	O
with	O
the	O
use	O
of	O
gradient-based	O
learning	O
when	O
an	O
appropriate	O
cost	O
function	O
can	O
undo	O
the	O
saturation	O
of	O
the	O
sigmoid	O
in	O
the	O
output	O
layer	O
.	O
when	O
a	O
sigmoidal	O
activation	O
function	O
must	O
be	O
used	O
,	O
the	O
hyperbolic	O
tangent	O
activation	O
function	O
typically	O
performs	O
better	O
than	O
the	O
logistic	O
sigmoid	O
.	O
it	O
resembles	O
the	O
identity	O
function	O
more	O
closely	O
,	O
in	O
the	O
sense	O
that	O
tanh	B
(	O
0	O
)	O
=	O
0	O
while	O
σ	O
(	O
0	O
)	O
=	O
1	O
.	O
2	O
because	O
tanh	B
is	O
similar	O
to	O
the	O
identity	O
function	O
near	O
,	O
training	O
a	O
deep	O
neural	O
	O
network	O
ˆy	O
=	O
w	O
x	O
)	O
)	O
resembles	O
training	O
a	O
linear	O
model	B
ˆy	O
=	O
	O
w	O
x	O
so	O
long	O
as	O
the	O
activations	O
of	O
the	O
network	O
can	O
be	O
kept	O
small	O
.	O
this	O
makes	O
training	O
the	O
network	O
easier	O
.	O
tanh	B
(	O
u	O
tanh	B
(	O
v	O
tanh	B
	O
	O
u	O
v	O
	O
	O
0	O
sigmoidal	O
activation	O
functions	O
are	O
more	O
common	O
in	O
settings	O
other	O
than	O
feed-	O
forward	O
networks	O
.	O
recurrent	O
networks	O
,	O
many	O
probabilistic	O
models	O
,	O
and	O
some	O
autoencoders	O
have	O
additional	O
requirements	O
that	O
rule	O
out	O
the	O
use	O
of	O
piecewise	O
linear	O
activation	O
functions	O
and	O
make	O
sigmoidal	O
units	O
more	O
appealing	O
despite	O
the	O
drawbacks	O
of	O
saturation	O
.	O
195	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
6.3.3	O
other	O
hidden	O
units	O
many	O
other	O
types	O
of	O
hidden	O
units	O
are	O
possible	O
,	O
but	O
are	O
used	O
less	O
frequently	O
.	O
in	O
general	O
,	O
a	O
wide	O
variety	O
of	O
diﬀerentiable	O
functions	O
perform	O
perfectly	O
well	O
.	O
many	O
unpublished	O
activation	O
functions	O
perform	O
just	O
as	O
well	O
as	O
the	O
popular	O
ones	O
.	O
to	O
provide	O
a	O
concrete	O
example	O
,	O
the	O
authors	O
tested	O
a	O
feedforward	O
network	O
using	O
h	O
=	O
cos	O
(	O
w	O
x	O
+	O
b	O
)	O
on	O
the	O
mnist	O
dataset	O
and	O
obtained	O
an	O
error	O
rate	O
of	O
less	O
than	O
1	O
%	O
,	O
which	O
is	O
competitive	O
with	O
results	O
obtained	O
using	O
more	O
conventional	O
activation	O
functions	O
.	O
during	O
research	O
and	O
development	O
of	O
new	O
techniques	O
,	O
it	O
is	O
common	O
to	O
test	O
many	O
diﬀerent	O
activation	O
functions	O
and	O
ﬁnd	O
that	O
several	O
variations	O
on	O
standard	O
practice	O
perform	O
comparably	O
.	O
this	O
means	O
that	O
usually	O
new	O
hidden	O
unit	O
types	O
are	O
published	O
only	O
if	O
they	O
are	O
clearly	O
demonstrated	O
to	O
provide	O
a	O
signiﬁcant	O
improvement	O
.	O
new	O
hidden	O
unit	O
types	O
that	O
perform	O
roughly	O
comparably	O
to	O
known	O
types	O
are	O
so	O
common	O
as	O
to	O
be	O
uninteresting	O
.	O
it	O
would	O
be	O
impractical	O
to	O
list	O
all	O
of	O
the	O
hidden	O
unit	O
types	O
that	O
have	O
appeared	O
in	O
the	O
literature	O
.	O
we	O
highlight	O
a	O
few	O
especially	O
useful	O
and	O
distinctive	O
ones	O
.	O
one	O
possibility	O
is	O
to	O
not	O
have	O
an	O
activation	O
g	O
(	O
z	O
)	O
at	O
all	O
.	O
one	O
can	O
also	O
think	O
of	O
this	O
as	O
using	O
the	O
identity	O
function	O
as	O
the	O
activation	O
function	O
.	O
we	O
have	O
already	O
seen	O
that	O
a	O
linear	O
unit	O
can	O
be	O
useful	O
as	O
the	O
output	O
of	O
a	O
neural	O
network	O
.	O
it	O
may	O
also	O
be	O
used	O
as	O
a	O
hidden	O
unit	O
.	O
if	O
every	O
layer	O
of	O
the	O
neural	O
network	O
consists	O
of	O
only	O
linear	O
transformations	O
,	O
then	O
the	O
network	O
as	O
a	O
whole	O
will	O
be	O
linear	O
.	O
however	O
,	O
it	O
is	O
acceptable	O
for	O
some	O
layers	O
of	O
the	O
neural	O
network	O
to	O
be	O
purely	O
linear	O
.	O
consider	O
a	O
neural	O
network	O
layer	O
with	O
n	O
inputs	O
and	O
p	O
outputs	O
,	O
h	O
=	O
g	O
(	O
w	O
x	O
+	O
b	O
)	O
.	O
we	O
may	O
replace	O
this	O
with	O
two	O
layers	O
,	O
with	O
one	O
layer	O
using	O
weight	O
matrix	O
u	O
and	O
the	O
other	O
using	O
weight	O
matrix	O
v	O
.	O
if	O
the	O
ﬁrst	O
layer	O
has	O
no	O
activation	O
function	O
,	O
then	O
we	O
have	O
essentially	O
factored	O
the	O
weight	O
matrix	O
of	O
the	O
original	O
layer	O
based	O
on	O
w	O
.	O
the	O
factored	O
approach	O
is	O
to	O
compute	O
h	O
=	O
g	O
(	O
v	O
x	O
+	O
b	O
)	O
.	O
if	O
u	O
produces	O
q	O
outputs	O
,	O
then	O
u	O
and	O
v	O
together	O
contain	O
only	O
(	O
n	O
+	O
p	O
)	O
q	O
parameters	O
,	O
while	O
w	O
contains	O
np	O
parameters	O
.	O
for	O
small	O
q	O
,	O
this	O
can	O
be	O
a	O
considerable	O
saving	O
in	O
parameters	O
.	O
it	O
comes	O
at	O
the	O
cost	O
of	O
constraining	O
the	O
linear	O
transformation	O
to	O
be	O
low-rank	O
,	O
but	O
these	O
low-rank	O
relationships	O
are	O
often	O
suﬃcient	O
.	O
linear	O
hidden	O
units	O
thus	O
oﬀer	O
an	O
eﬀective	O
way	O
of	O
reducing	O
the	O
number	O
of	O
parameters	O
in	O
a	O
network	O
.	O
	O
u	O
	O
	O
6.2.2.3	O
softmax	O
units	O
are	O
another	O
kind	O
of	O
unit	O
that	O
is	O
usually	O
used	O
as	O
an	O
output	O
(	O
as	O
described	O
in	O
section	O
)	O
but	O
may	O
sometimes	O
be	O
used	O
as	O
a	O
hidden	O
unit	O
.	O
softmax	O
units	O
naturally	O
represent	O
a	O
probability	O
distribution	O
over	O
a	O
discrete	O
variable	O
with	O
k	O
possible	O
values	O
,	O
so	O
they	O
may	O
be	O
used	O
as	O
a	O
kind	O
of	O
switch	O
.	O
these	O
kinds	O
of	O
hidden	O
units	O
are	O
usually	O
only	O
used	O
in	O
more	O
advanced	O
architectures	O
that	O
explicitly	O
learn	O
to	O
manipulate	O
memory	O
,	O
described	O
in	O
section	O
.	O
10.12	O
196	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
	O
	O
a	O
few	O
other	O
reasonably	O
common	O
hidden	O
unit	O
types	O
include	O
:	O
||	O
•	O
w	O
:	O
,i	O
radial	O
basis	O
function	O
or	O
rbf	O
unit	O
:	O
hi	O
=	O
exp	O
.	O
this	O
function	O
becomes	O
more	O
active	O
as	O
x	O
approaches	O
a	O
template	O
w	O
:	O
,i.	O
because	O
it	O
saturates	O
to	O
,	O
it	O
can	O
be	O
diﬃcult	O
to	O
optimize	O
.	O
−	O
||	O
x	O
2	O
for	O
most	O
1	O
σ	O
2	O
i	O
−	O
x	O
0	O
•	O
•	O
(	O
(	O
dugas	O
et	O
al	O
.	O
2001	O
glorot	O
et	O
al	O
.	O
2011a	O
)	O
for	O
function	O
approximation	O
and	O
by	O
softplus	O
:	O
g	O
(	O
a	O
)	O
=	O
ζ	O
(	O
a	O
)	O
=	O
log	O
(	O
1	O
+	O
ea	O
)	O
.	O
this	O
is	O
a	O
smooth	O
version	O
of	O
the	O
rectiﬁer	O
,	O
introduced	O
by	O
nair	O
)	O
for	O
the	O
conditional	O
distributions	O
of	O
undirected	O
probabilistic	O
and	O
hinton	O
2010	O
models.	O
)	O
compared	O
the	O
softplus	O
and	O
rectiﬁer	O
and	O
found	O
better	O
results	O
with	O
the	O
latter	O
.	O
the	O
use	O
of	O
the	O
softplus	O
is	O
generally	O
discouraged	O
.	O
the	O
softplus	O
demonstrates	O
that	O
the	O
performance	O
of	O
hidden	O
unit	O
types	O
can	O
be	O
very	O
counterintuitive—one	O
might	O
expect	O
it	O
to	O
have	O
an	O
advantage	O
over	O
the	O
rectiﬁer	O
due	O
to	O
being	O
diﬀerentiable	O
everywhere	O
or	O
due	O
to	O
saturating	O
less	O
completely	O
,	O
but	O
empirically	O
it	O
does	O
not	O
.	O
(	O
−	O
hard	O
tanh	B
:	O
this	O
is	O
shaped	O
similarly	O
to	O
the	O
tanh	B
and	O
the	O
rectiﬁer	O
but	O
unlike	O
the	O
latter	O
,	O
it	O
is	O
bounded	O
,	O
g	O
(	O
a	O
)	O
=	O
max	O
(	O
it	O
was	O
introduced	O
by	O
collobert	O
2004	O
1	O
,	O
min	O
(	O
1	O
,	O
a	O
)	O
)	O
.	O
)	O
.	O
(	O
hidden	O
unit	O
design	O
remains	O
an	O
active	O
area	O
of	O
research	O
and	O
many	O
useful	O
hidden	O
unit	O
types	O
remain	O
to	O
be	O
discovered	O
.	O
6.4	O
architecture	O
design	O
another	O
key	O
design	O
consideration	O
for	O
neural	O
networks	O
is	O
determining	O
the	O
architecture	O
.	O
the	O
word	O
architecture	O
refers	O
to	O
the	O
overall	O
structure	O
of	O
the	O
network	O
:	O
how	O
many	O
units	O
it	O
should	O
have	O
and	O
how	O
these	O
units	O
should	O
be	O
connected	O
to	O
each	O
other	O
.	O
most	O
neural	O
networks	O
are	O
organized	O
into	O
groups	O
of	O
units	O
called	O
layers	O
.	O
most	O
neural	O
network	O
architectures	O
arrange	O
these	O
layers	O
in	O
a	O
chain	O
structure	O
,	O
with	O
each	O
layer	O
being	O
a	O
function	O
of	O
the	O
layer	O
that	O
preceded	O
it	O
.	O
in	O
this	O
structure	O
,	O
the	O
ﬁrst	O
layer	O
is	O
given	O
by	O
h	O
(	O
1	O
)	O
=	O
g	O
(	O
1	O
)	O
	O
w	O
(	O
1	O
)	O
x	O
b+	O
(	O
1	O
)	O
	O
	O
	O
	O
,	O
the	O
second	O
layer	O
is	O
given	O
by	O
h	O
(	O
2	O
)	O
=	O
g	O
(	O
2	O
)	O
	O
w	O
(	O
2	O
)	O
h	O
(	O
1	O
)	O
+	O
b	O
(	O
2	O
)	O
,	O
and	O
so	O
on	O
.	O
197	O
(	O
6.40	O
)	O
(	O
6.41	O
)	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
in	O
these	O
chain-based	O
architectures	O
,	O
the	O
main	O
architectural	O
considerations	O
are	O
to	O
choose	O
the	O
depth	O
of	O
the	O
network	O
and	O
the	O
width	O
of	O
each	O
layer	O
.	O
as	O
we	O
will	O
see	O
,	O
a	O
network	O
with	O
even	O
one	O
hidden	O
layer	O
is	O
suﬃcient	O
to	O
ﬁt	O
the	O
training	O
set	O
.	O
deeper	O
networks	O
often	O
are	O
able	O
to	O
use	O
far	O
fewer	O
units	O
per	O
layer	O
and	O
far	O
fewer	O
parameters	O
and	O
often	O
generalize	O
to	O
the	O
test	O
set	O
,	O
but	O
are	O
also	O
often	O
harder	O
to	O
optimize	O
.	O
the	O
ideal	O
network	O
architecture	O
for	O
a	O
task	O
must	O
be	O
found	O
via	O
experimentation	O
guided	O
by	O
monitoring	O
the	O
validation	O
set	O
error	O
.	O
6.4.1	O
universal	O
approximation	O
properties	O
and	O
depth	O
a	O
linear	O
model	B
,	O
mapping	O
from	O
features	O
to	O
outputs	O
via	O
matrix	O
multiplication	O
,	O
can	O
by	O
deﬁnition	O
represent	O
only	O
linear	O
functions	O
.	O
it	O
has	O
the	O
advantage	O
of	O
being	O
easy	O
to	O
train	O
because	O
many	O
loss	O
functions	O
result	O
in	O
convex	O
optimization	O
problems	O
when	O
applied	O
to	O
linear	O
models	O
.	O
unfortunately	O
,	O
we	O
often	O
want	O
to	O
learn	O
nonlinear	O
functions	O
.	O
;	O
,	O
1989	O
cybenko	O
1989	O
at	O
ﬁrst	O
glance	O
,	O
we	O
might	O
presume	O
that	O
learning	O
a	O
nonlinear	O
function	O
requires	O
designing	O
a	O
specialized	O
model	B
family	O
for	O
the	O
kind	O
of	O
nonlinearity	O
we	O
want	O
to	O
learn	O
.	O
fortunately	O
,	O
feedforward	O
networks	O
with	O
hidden	O
layers	O
provide	O
a	O
universal	O
approxi-	O
mation	O
framework	O
.	O
speciﬁcally	O
,	O
the	O
universal	O
approximation	O
theorem	O
(	O
hornik	O
et	O
al.	O
,	O
)	O
states	O
that	O
a	O
feedforward	O
network	O
with	O
a	O
linear	O
output	O
layer	O
and	O
at	O
least	O
one	O
hidden	O
layer	O
with	O
any	O
“	O
squashing	O
”	O
activation	O
function	O
(	O
such	O
as	O
the	O
logistic	O
sigmoid	O
activation	O
function	O
)	O
can	O
approximate	O
any	O
borel	O
measurable	O
function	O
from	O
one	O
ﬁnite-dimensional	O
space	O
to	O
another	O
with	O
any	O
desired	O
non-zero	O
amount	O
of	O
error	O
,	O
provided	O
that	O
the	O
network	O
is	O
given	O
enough	O
hidden	O
units	O
.	O
the	O
derivatives	O
of	O
the	O
feedforward	O
network	O
can	O
also	O
approximate	O
the	O
derivatives	O
of	O
the	O
function	O
arbitrarily	O
well	O
(	O
)	O
.	O
the	O
concept	O
of	O
borel	O
measurability	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
;	O
for	O
our	O
purposes	O
it	O
suﬃces	O
to	O
say	O
that	O
any	O
continuous	O
function	O
on	O
a	O
closed	O
and	O
bounded	O
subset	O
of	O
r	O
n	O
is	O
borel	O
measurable	O
and	O
therefore	O
may	O
be	O
approximated	O
by	O
a	O
neural	O
network	O
.	O
a	O
neural	O
network	O
may	O
also	O
approximate	O
any	O
function	O
mapping	O
from	O
any	O
ﬁnite	O
dimensional	O
discrete	O
space	O
to	O
another	O
.	O
while	O
the	O
original	O
theorems	O
were	O
ﬁrst	O
stated	O
in	O
terms	O
of	O
units	O
with	O
activation	O
functions	O
that	O
saturate	O
both	O
for	O
very	O
negative	O
and	O
for	O
very	O
positive	O
arguments	O
,	O
universal	O
approximation	O
theorems	O
have	O
also	O
been	O
proved	O
for	O
a	O
wider	O
class	O
of	O
activation	O
functions	O
,	O
which	O
includes	O
the	O
now	O
commonly	O
used	O
rectiﬁed	O
linear	O
unit	O
(	O
hornik	O
et	O
al	O
.	O
1990	O
,	O
leshno	O
et	O
al	O
.	O
1993	O
,	O
)	O
.	O
the	O
universal	O
approximation	O
theorem	O
means	O
that	O
regardless	O
of	O
what	O
function	O
we	O
are	O
trying	O
to	O
learn	O
,	O
we	O
know	O
that	O
a	O
large	O
mlp	O
will	O
be	O
able	O
to	O
represent	O
this	O
function	O
.	O
however	O
,	O
we	O
are	O
not	O
guaranteed	O
that	O
the	O
training	O
algorithm	O
will	O
be	O
able	O
to	O
learn	O
that	O
function	O
.	O
even	O
if	O
the	O
mlp	O
is	O
able	O
to	O
represent	O
the	O
function	O
,	O
learning	O
can	O
fail	O
for	O
two	O
diﬀerent	O
reasons	O
.	O
first	O
,	O
the	O
optimization	O
algorithm	O
used	O
for	O
training	O
198	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
may	O
not	O
be	O
able	O
to	O
ﬁnd	O
the	O
value	O
of	O
the	O
parameters	O
that	O
corresponds	O
to	O
the	O
desired	O
function	O
.	O
second	O
,	O
the	O
training	O
algorithm	O
might	O
choose	O
the	O
wrong	O
function	O
due	O
to	O
overﬁtting	O
.	O
recall	O
from	O
section	O
that	O
the	O
“	O
no	O
free	O
lunch	O
”	O
theorem	O
shows	O
that	O
there	O
is	O
no	O
universally	O
superior	O
machine	O
learning	O
algorithm	O
.	O
feedforward	O
networks	O
provide	O
a	O
universal	O
system	O
for	O
representing	O
functions	O
,	O
in	O
the	O
sense	O
that	O
,	O
given	O
a	O
function	O
,	O
there	O
exists	O
a	O
feedforward	O
network	O
that	O
approximates	O
the	O
function	O
.	O
there	O
is	O
no	O
universal	O
procedure	O
for	O
examining	O
a	O
training	O
set	O
of	O
speciﬁc	O
examples	O
and	O
choosing	O
a	O
function	O
that	O
will	O
generalize	O
to	O
points	O
not	O
in	O
the	O
training	O
set	O
.	O
5.2.1	O
barron	O
1993	O
the	O
universal	O
approximation	O
theorem	O
says	O
that	O
there	O
exists	O
a	O
network	O
large	O
enough	O
to	O
achieve	O
any	O
degree	O
of	O
accuracy	O
we	O
desire	O
,	O
but	O
the	O
theorem	O
does	O
not	O
say	O
how	O
large	O
this	O
network	O
will	O
be.	O
)	O
provides	O
some	O
bounds	O
on	O
the	O
size	O
of	O
a	O
single-layer	O
network	O
needed	O
to	O
approximate	O
a	O
broad	O
class	O
of	O
functions	O
.	O
unfortunately	O
,	O
in	O
the	O
worse	O
case	O
,	O
an	O
exponential	O
number	O
of	O
hidden	O
units	O
(	O
possibly	O
with	O
one	O
hidden	O
unit	O
corresponding	O
to	O
each	O
input	O
conﬁguration	O
that	O
needs	O
to	O
be	O
distinguished	O
)	O
may	O
be	O
required	O
.	O
this	O
is	O
easiest	O
to	O
see	O
in	O
the	O
binary	O
case	O
:	O
the	O
n	O
is	O
22	O
n	O
and	O
selecting	O
number	O
of	O
possible	O
binary	O
functions	O
on	O
vectors	O
v	O
one	O
such	O
function	O
requires	O
2n	O
bits	O
,	O
which	O
will	O
in	O
general	O
require	O
o	O
(	O
2	O
n	O
)	O
degrees	O
of	O
freedom.	O
}	O
0	O
,	O
1	O
∈	O
{	O
(	O
in	O
summary	O
,	O
a	O
feedforward	O
network	O
with	O
a	O
single	O
layer	O
is	O
suﬃcient	O
to	O
represent	O
any	O
function	O
,	O
but	O
the	O
layer	O
may	O
be	O
infeasibly	O
large	O
and	O
may	O
fail	O
to	O
learn	O
and	O
generalize	O
correctly	O
.	O
in	O
many	O
circumstances	O
,	O
using	O
deeper	O
models	O
can	O
reduce	O
the	O
number	O
of	O
units	O
required	O
to	O
represent	O
the	O
desired	O
function	O
and	O
can	O
reduce	O
the	O
amount	O
of	O
generalization	O
error	O
.	O
there	O
exist	O
families	O
of	O
functions	O
which	O
can	O
be	O
approximated	O
eﬃciently	O
by	O
an	O
architecture	O
with	O
depth	O
greater	O
than	O
some	O
value	O
d	O
,	O
but	O
which	O
require	O
a	O
much	O
larger	O
model	B
if	O
depth	O
is	O
restricted	O
to	O
be	O
less	O
than	O
or	O
equal	O
to	O
d.	O
in	O
many	O
cases	O
,	O
the	O
number	O
of	O
hidden	O
units	O
required	O
by	O
the	O
shallow	O
model	B
is	O
exponential	O
in	O
n.	O
such	O
results	O
were	O
ﬁrst	O
proved	O
for	O
models	O
that	O
do	O
not	O
resemble	O
the	O
continuous	O
,	O
diﬀerentiable	O
neural	O
networks	O
used	O
for	O
machine	O
learning	O
,	O
but	O
have	O
since	O
been	O
extended	O
to	O
these	O
models	O
.	O
the	O
ﬁrst	O
results	O
were	O
for	O
circuits	O
of	O
logic	O
gates	O
(	O
)	O
.	O
later	O
work	B
extended	O
these	O
results	O
to	O
linear	O
threshold	O
units	O
with	O
non-negative	O
weights	O
(	O
håstad	O
and	O
goldmann	O
1991	O
hajnal	O
et	O
al	O
.	O
1993	O
)	O
,	O
and	O
then	O
to	O
networks	O
with	O
;	O
)	O
.	O
many	O
modern	O
continuous-valued	O
activations	O
(	O
neural	O
networks	O
use	O
rectiﬁed	O
linear	O
units.	O
)	O
demonstrated	O
that	O
shallow	O
networks	O
with	O
a	O
broad	O
family	O
of	O
non-polynomial	O
activation	O
functions	O
,	O
including	O
rectiﬁed	O
linear	O
units	O
,	O
have	O
universal	O
approximation	O
properties	O
,	O
but	O
these	O
results	O
do	O
not	O
address	O
the	O
questions	O
of	O
depth	O
or	O
eﬃciency—they	O
specify	O
only	O
that	O
a	O
suﬃciently	O
wide	O
rectiﬁer	O
network	O
could	O
represent	O
any	O
function	O
.	O
montufar	O
et	O
al	O
.	O
maass	O
1992	O
maass	O
et	O
al	O
.	O
1994	O
leshno	O
et	O
al	O
.	O
1993	O
håstad	O
1986	O
(	O
,	O
,	O
,	O
,	O
;	O
,	O
199	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
(	O
)	O
showed	O
that	O
functions	O
representable	O
with	O
a	O
deep	O
rectiﬁer	O
net	O
can	O
require	O
2014	O
an	O
exponential	O
number	O
of	O
hidden	O
units	O
with	O
a	O
shallow	O
(	O
one	O
hidden	O
layer	O
)	O
network	O
.	O
more	O
precisely	O
,	O
they	O
showed	O
that	O
piecewise	O
linear	O
networks	O
(	O
which	O
can	O
be	O
obtained	O
from	O
rectiﬁer	O
nonlinearities	O
or	O
maxout	O
units	O
)	O
can	O
represent	O
functions	O
with	O
a	O
number	O
of	O
regions	O
that	O
is	O
exponential	O
in	O
the	O
depth	O
of	O
the	O
network	O
.	O
figure	O
illustrates	O
how	O
a	O
network	O
with	O
absolute	O
value	O
rectiﬁcation	O
creates	O
mirror	O
images	O
of	O
the	O
function	O
computed	O
on	O
top	O
of	O
some	O
hidden	O
unit	O
,	O
with	O
respect	O
to	O
the	O
input	O
of	O
that	O
hidden	O
unit	O
.	O
each	O
hidden	O
unit	O
speciﬁes	O
where	O
to	O
fold	O
the	O
input	O
space	O
in	O
order	O
to	O
create	O
mirror	O
responses	O
(	O
on	O
both	O
sides	O
of	O
the	O
absolute	O
value	O
nonlinearity	O
)	O
.	O
by	O
composing	O
these	O
folding	O
operations	O
,	O
we	O
obtain	O
an	O
exponentially	O
large	O
number	O
of	O
piecewise	O
linear	O
regions	O
which	O
can	O
capture	O
all	O
kinds	O
of	O
regular	O
(	O
e.g.	O
,	O
repeating	O
)	O
patterns	O
.	O
6.5	O
(	O
)	O
.	O
montufar	O
et	O
al	O
.	O
2014	O
figure	O
6.5	O
:	O
an	O
intuitive	O
,	O
geometric	O
explanation	O
of	O
the	O
exponential	O
advantage	O
of	O
deeper	O
rectiﬁer	O
networks	O
formally	O
by	O
(	O
left	O
)	O
an	O
absolute	O
value	O
rectiﬁcation	O
unit	O
has	O
the	O
same	O
output	O
for	O
every	O
pair	O
of	O
mirror	O
points	O
in	O
its	O
input	O
.	O
the	O
mirror	O
axis	O
of	O
symmetry	O
is	O
given	O
by	O
the	O
hyperplane	O
deﬁned	O
by	O
the	O
weights	O
and	O
bias	O
of	O
the	O
unit	O
.	O
a	O
function	O
computed	O
on	O
top	O
of	O
that	O
unit	O
(	O
the	O
green	O
decision	O
surface	O
)	O
will	O
be	O
a	O
mirror	O
image	O
the	O
function	O
can	O
be	O
obtained	O
of	O
a	O
simpler	O
pattern	O
across	O
that	O
axis	O
of	O
symmetry	O
.	O
by	O
folding	O
the	O
space	O
around	O
the	O
axis	O
of	O
symmetry	O
.	O
another	O
repeating	O
pattern	O
can	O
be	O
folded	O
on	O
top	O
of	O
the	O
ﬁrst	O
(	O
by	O
another	O
downstream	O
unit	O
)	O
to	O
obtain	O
another	O
symmetry	O
(	O
which	O
is	O
now	O
repeated	O
four	O
times	O
,	O
with	O
two	O
hidden	O
layers	O
)	O
.	O
figure	O
reproduced	O
with	O
permission	O
from	O
montufar	O
et	O
al	O
.	O
2014	O
(	O
center	O
)	O
(	O
right	O
)	O
)	O
.	O
(	O
more	O
precisely	O
,	O
the	O
main	O
theorem	O
in	O
)	O
states	O
that	O
the	O
number	O
of	O
linear	O
regions	O
carved	O
out	O
by	O
a	O
deep	O
rectiﬁer	O
network	O
with	O
d	O
inputs	O
,	O
depth	O
,	O
and	O
units	O
per	O
hidden	O
layer	O
,	O
is	O
montufar	O
et	O
al	O
.	O
2014	O
n	O
(	O
l	O
	O
	O
	O
o	O
,	O
(	O
6.42	O
)	O
i.e.	O
,	O
exponential	O
in	O
the	O
depth	O
.	O
in	O
the	O
case	O
of	O
maxout	O
networks	O
with	O
unit	O
,	O
the	O
number	O
of	O
linear	O
regions	O
is	O
l	O
k	O
ﬁlters	O
per	O
(	O
6.43	O
)	O
−	O
1	O
)	O
d	O
l	O
(	O
nd	O
	O
n	O
d	O
	O
−	O
1	O
)	O
+	O
l	O
k	O
(	O
d	O
.	O
o	O
200	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
of	O
course	O
,	O
there	O
is	O
no	O
guarantee	O
that	O
the	O
kinds	O
of	O
functions	O
we	O
want	O
to	O
learn	O
in	O
applications	O
of	O
machine	O
learning	O
(	O
and	O
in	O
particular	O
for	O
ai	O
)	O
share	O
such	O
a	O
property	O
.	O
we	O
may	O
also	O
want	O
to	O
choose	O
a	O
deep	O
model	B
for	O
statistical	O
reasons	O
.	O
any	O
time	O
we	O
choose	O
a	O
speciﬁc	O
machine	O
learning	O
algorithm	O
,	O
we	O
are	O
implicitly	O
stating	O
some	O
set	O
of	O
prior	O
beliefs	O
we	O
have	O
about	O
what	O
kind	O
of	O
function	O
the	O
algorithm	O
should	O
learn	O
.	O
choosing	O
a	O
deep	O
model	B
encodes	O
a	O
very	O
general	O
belief	O
that	O
the	O
function	O
we	O
want	O
to	O
learn	O
should	O
involve	O
composition	O
of	O
several	O
simpler	O
functions	O
.	O
this	O
can	O
be	O
interpreted	O
from	O
a	O
representation	O
learning	O
point	O
of	O
view	O
as	O
saying	O
that	O
we	O
believe	O
the	O
learning	O
problem	O
consists	O
of	O
discovering	O
a	O
set	O
of	O
underlying	O
factors	O
of	O
variation	O
that	O
can	O
in	O
turn	O
be	O
described	O
in	O
terms	O
of	O
other	O
,	O
simpler	O
underlying	O
factors	O
of	O
variation	O
.	O
alternately	O
,	O
we	O
can	O
interpret	O
the	O
use	O
of	O
a	O
deep	O
architecture	O
as	O
expressing	O
a	O
belief	O
that	O
the	O
function	O
we	O
want	O
to	O
learn	O
is	O
a	O
computer	O
program	O
consisting	O
of	O
multiple	O
steps	O
,	O
where	O
each	O
step	O
makes	O
use	O
of	O
the	O
previous	O
step	O
’	O
s	O
output	O
.	O
these	O
intermediate	O
outputs	O
are	O
not	O
necessarily	O
factors	O
of	O
variation	O
,	O
but	O
can	O
instead	O
be	O
analogous	O
to	O
counters	O
or	O
pointers	O
that	O
the	O
network	O
uses	O
to	O
organize	O
its	O
internal	O
processing	O
.	O
empirically	O
,	O
greater	O
depth	O
does	O
seem	O
to	O
result	O
in	O
better	O
generalization	O
for	O
a	O
wide	O
variety	O
of	O
tasks	O
(	O
bengio	O
et	O
al	O
.	O
2007	O
erhan	O
et	O
al	O
.	O
2009	O
bengio	O
2009	O
;	O
2011	O
ciresan	O
;	O
mesnil	O
et	O
al.	O
,	O
et	O
al.	O
,	O
et	O
al.	O
,	O
2013	O
couprie	O
2013	O
farabet	O
;	O
et	O
al.	O
,	O
;	O
2013	O
goodfellow	O
et	O
al	O
.	O
2014d	O
szegedy	O
et	O
al	O
.	O
;	O
for	O
examples	O
of	O
,	O
2014a	O
,	O
some	O
of	O
these	O
empirical	O
results	O
.	O
this	O
suggests	O
that	O
using	O
deep	O
architectures	O
does	O
indeed	O
express	O
a	O
useful	O
prior	O
over	O
the	O
space	O
of	O
functions	O
the	O
model	B
learns	O
.	O
,	O
,	O
2012	O
sermanet	O
2012	O
krizhevsky	O
2013	O
kahou	O
and	O
ﬁgure	O
6.7	O
)	O
.	O
see	O
ﬁgure	O
et	O
al.	O
,	O
;	O
;	O
;	O
6.6	O
,	O
;	O
et	O
al.	O
,	O
;	O
;	O
et	O
al.	O
,	O
6.4.2	O
other	O
architectural	O
considerations	O
so	O
far	O
we	O
have	O
described	O
neural	O
networks	O
as	O
being	O
simple	O
chains	O
of	O
layers	O
,	O
with	O
the	O
main	O
considerations	O
being	O
the	O
depth	O
of	O
the	O
network	O
and	O
the	O
width	O
of	O
each	O
layer	O
.	O
in	O
practice	O
,	O
neural	O
networks	O
show	O
considerably	O
more	O
diversity	O
.	O
many	O
neural	O
network	O
architectures	O
have	O
been	O
developed	O
for	O
speciﬁc	O
tasks	O
.	O
specialized	O
architectures	O
for	O
computer	O
vision	O
called	O
convolutional	O
networks	O
are	O
.	O
feedforward	O
networks	O
may	O
also	O
be	O
generalized	O
to	O
the	O
described	O
in	O
chapter	O
recurrent	O
neural	O
networks	O
for	O
sequence	O
processing	O
,	O
described	O
in	O
chapter	O
,	O
which	O
have	O
their	O
own	O
architectural	O
considerations	O
.	O
10	O
9	O
in	O
general	O
,	O
the	O
layers	O
need	O
not	O
be	O
connected	O
in	O
a	O
chain	O
,	O
even	O
though	O
this	O
is	O
the	O
most	O
common	O
practice	O
.	O
many	O
architectures	O
build	O
a	O
main	O
chain	O
but	O
then	O
add	O
extra	O
architectural	O
features	O
to	O
it	O
,	O
such	O
as	O
skip	O
connections	O
going	O
from	O
layer	O
i	O
to	O
layer	O
i	O
+	O
2	O
or	O
higher	O
.	O
these	O
skip	O
connections	O
make	O
it	O
easier	O
for	O
the	O
gradient	O
to	O
ﬂow	O
from	O
output	O
layers	O
to	O
layers	O
nearer	O
the	O
input	O
.	O
201	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
96	O
5	O
.	O
96	O
0	O
.	O
95	O
5	O
.	O
95	O
0	O
.	O
94	O
5	O
.	O
94	O
0	O
.	O
93	O
5	O
.	O
93	O
0	O
.	O
92	O
5	O
.	O
92	O
0.	O
)	O
t	O
n	O
e	O
c	O
r	O
e	O
p	O
(	O
y	O
c	O
a	O
r	O
u	O
c	O
c	O
a	O
t	O
s	O
e	O
t	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
11	O
number	O
of	O
hidden	O
layers	O
figure	O
6.6	O
:	O
empirical	O
results	O
showing	O
that	O
deeper	O
networks	O
generalize	O
better	O
when	O
used	O
to	O
transcribe	O
multi-digit	O
numbers	O
from	O
photographs	O
of	O
addresses	O
.	O
data	O
from	O
goodfellow	O
)	O
.	O
the	O
test	O
set	O
accuracy	O
consistently	O
increases	O
with	O
increasing	O
depth	O
.	O
see	O
et	O
al	O
.	O
(	O
ﬁgure	O
for	O
a	O
control	O
experiment	O
demonstrating	O
that	O
other	O
increases	O
to	O
the	O
model	B
size	O
do	O
not	O
yield	O
the	O
same	O
eﬀect	O
.	O
2014d	O
6.7	O
another	O
key	O
consideration	O
of	O
architecture	O
design	O
is	O
exactly	O
how	O
to	O
connect	O
a	O
pair	O
of	O
layers	O
to	O
each	O
other	O
.	O
in	O
the	O
default	O
neural	O
network	O
layer	O
described	O
by	O
a	O
linear	O
transformation	O
via	O
a	O
matrix	O
w	O
,	O
every	O
input	O
unit	O
is	O
connected	O
to	O
every	O
output	O
unit	O
.	O
many	O
specialized	O
networks	O
in	O
the	O
chapters	O
ahead	O
have	O
fewer	O
connections	O
,	O
so	O
that	O
each	O
unit	O
in	O
the	O
input	O
layer	O
is	O
connected	O
to	O
only	O
a	O
small	O
subset	O
of	O
units	O
in	O
the	O
output	O
layer	O
.	O
these	O
strategies	O
for	O
reducing	O
the	O
number	O
of	O
connections	O
reduce	O
the	O
number	O
of	O
parameters	O
and	O
the	O
amount	O
of	O
computation	O
required	O
to	O
evaluate	O
the	O
network	O
,	O
but	O
are	O
often	O
highly	O
problem-dependent	O
.	O
for	O
example	O
,	O
convolutional	O
networks	O
,	O
described	O
in	O
chapter	O
,	O
use	O
specialized	O
patterns	O
of	O
sparse	O
connections	O
that	O
are	O
very	O
eﬀective	O
for	O
computer	O
vision	O
problems	O
.	O
in	O
this	O
chapter	O
,	O
it	O
is	O
diﬃcult	O
to	O
give	O
much	O
more	O
speciﬁc	O
advice	O
concerning	O
the	O
architecture	O
of	O
a	O
generic	O
neural	O
network	O
.	O
subsequent	O
chapters	O
develop	O
the	O
particular	O
architectural	O
strategies	O
that	O
have	O
been	O
found	O
to	O
work	B
well	O
for	O
diﬀerent	O
application	O
domains	O
.	O
9	O
202	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
97	O
96	O
95	O
94	O
93	O
92	O
)	O
t	O
n	O
e	O
c	O
r	O
e	O
p	O
(	O
y	O
c	O
a	O
r	O
u	O
c	O
c	O
a	O
t	O
s	O
e	O
t	O
91	O
0	O
0	O
.	O
0	O
2	O
.	O
0	O
4	O
.	O
number	O
of	O
parameters	O
0	O
6	O
.	O
3	O
,	O
convolutional	O
3	O
,	O
fully	O
connected	O
11	O
,	O
convolutional	O
0	O
8	O
.	O
.	O
×	O
1	O
0	O
108	O
2014d	O
et	O
al	O
.	O
(	O
figure	O
6.7	O
:	O
deeper	O
models	O
tend	O
to	O
perform	O
better	O
.	O
this	O
is	O
not	O
merely	O
because	O
the	O
model	B
is	O
larger	O
.	O
this	O
experiment	O
from	O
goodfellow	O
)	O
shows	O
that	O
increasing	O
the	O
number	O
of	O
parameters	O
in	O
layers	O
of	O
convolutional	O
networks	O
without	O
increasing	O
their	O
depth	O
is	O
not	O
nearly	O
as	O
eﬀective	O
at	O
increasing	O
test	O
set	O
performance	O
.	O
the	O
legend	O
indicates	O
the	O
depth	O
of	O
network	O
used	O
to	O
make	O
each	O
curve	O
and	O
whether	O
the	O
curve	O
represents	O
variation	O
in	O
the	O
size	O
of	O
the	O
convolutional	O
or	O
the	O
fully	O
connected	O
layers	O
.	O
we	O
observe	O
that	O
shallow	O
models	O
in	O
this	O
context	O
overﬁt	O
at	O
around	O
20	O
million	O
parameters	O
while	O
deep	O
ones	O
can	O
beneﬁt	O
from	O
having	O
over	O
60	O
million	O
.	O
this	O
suggests	O
that	O
using	O
a	O
deep	O
model	B
expresses	O
a	O
useful	O
preference	O
over	O
the	O
space	O
of	O
functions	O
the	O
model	B
can	O
learn	O
.	O
speciﬁcally	O
,	O
it	O
expresses	O
a	O
belief	O
that	O
the	O
function	O
should	O
consist	O
of	O
many	O
simpler	O
functions	O
composed	O
together	O
.	O
this	O
could	O
result	O
either	O
in	O
learning	O
a	O
representation	O
that	O
is	O
composed	O
in	O
turn	O
of	O
simpler	O
representations	O
(	O
e.g.	O
,	O
corners	O
deﬁned	O
in	O
terms	O
of	O
edges	O
)	O
or	O
in	O
learning	O
a	O
program	O
with	O
sequentially	O
dependent	O
steps	O
(	O
e.g.	O
,	O
ﬁrst	O
locate	O
a	O
set	O
of	O
objects	O
,	O
then	O
segment	O
them	O
from	O
each	O
other	O
,	O
then	O
recognize	O
them	O
)	O
.	O
203	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
6.5	O
back-propagation	O
and	O
other	O
diﬀerentiation	O
algo-	O
rithms	O
when	O
we	O
use	O
a	O
feedforward	O
neural	O
network	O
to	O
accept	O
an	O
input	O
x	O
and	O
produce	O
an	O
output	O
ˆy	O
,	O
information	O
ﬂows	O
forward	O
through	O
the	O
network	O
.	O
the	O
inputs	O
x	O
provide	O
the	O
initial	O
information	O
that	O
then	O
propagates	O
up	O
to	O
the	O
hidden	O
units	O
at	O
each	O
layer	O
and	O
ﬁnally	O
produces	O
ˆy	O
.	O
this	O
is	O
called	O
forward	O
propagation	O
.	O
during	O
training	O
,	O
forward	O
propagation	O
can	O
continue	O
onward	O
until	O
it	O
produces	O
a	O
scalar	O
cost	O
j	O
(	O
θ	O
)	O
.	O
the	O
back-propagation	O
algorithm	O
(	O
)	O
,	O
often	O
simply	O
called	O
backprop	O
,	O
allows	O
the	O
information	O
from	O
the	O
cost	O
to	O
then	O
ﬂow	O
backwards	O
through	O
the	O
network	O
,	O
in	O
order	O
to	O
compute	O
the	O
gradient	O
.	O
rumelhart	O
et	O
al	O
.	O
1986a	O
,	O
computing	O
an	O
analytical	O
expression	O
for	O
the	O
gradient	O
is	O
straightforward	O
,	O
but	O
numerically	O
evaluating	O
such	O
an	O
expression	O
can	O
be	O
computationally	O
expensive	O
.	O
the	O
back-propagation	O
algorithm	O
does	O
so	O
using	O
a	O
simple	O
and	O
inexpensive	O
procedure	O
.	O
the	O
term	O
back-propagation	O
is	O
often	O
misunderstood	O
as	O
meaning	O
the	O
whole	O
learning	O
algorithm	O
for	O
multi-layer	O
neural	O
networks	O
.	O
actually	O
,	O
back-propagation	O
refers	O
only	O
to	O
the	O
method	O
for	O
computing	O
the	O
gradient	O
,	O
while	O
another	O
algorithm	O
,	O
such	O
as	O
stochastic	O
gradient	O
descent	B
,	O
is	O
used	O
to	O
perform	O
learning	O
using	O
this	O
gradient	O
.	O
furthermore	O
,	O
back-propagation	O
is	O
often	O
misunderstood	O
as	O
being	O
speciﬁc	O
to	O
multi-	O
layer	O
neural	O
networks	O
,	O
but	O
in	O
principle	O
it	O
can	O
compute	O
derivatives	O
of	O
any	O
function	O
(	O
for	O
some	O
functions	O
,	O
the	O
correct	O
response	O
is	O
to	O
report	O
that	O
the	O
derivative	O
of	O
the	O
∇	O
function	O
is	O
undeﬁned	O
)	O
.	O
speciﬁcally	O
,	O
we	O
will	O
describe	O
how	O
to	O
compute	O
the	O
gradient	O
x	O
f	O
(	O
x	O
y	O
,	O
)	O
for	O
an	O
arbitrary	O
function	O
f	O
,	O
where	O
x	O
is	O
a	O
set	O
of	O
variables	O
whose	O
derivatives	O
are	O
desired	O
,	O
and	O
y	O
is	O
an	O
additional	O
set	O
of	O
variables	O
that	O
are	O
inputs	O
to	O
the	O
function	O
but	O
whose	O
derivatives	O
are	O
not	O
required	O
.	O
in	O
learning	O
algorithms	O
,	O
the	O
gradient	O
we	O
most	O
∇	O
often	O
require	O
is	O
the	O
gradient	O
of	O
the	O
cost	O
function	O
with	O
respect	O
to	O
the	O
parameters	O
,	O
θj	O
(	O
θ	O
)	O
.	O
many	O
machine	O
learning	O
tasks	O
involve	O
computing	O
other	O
derivatives	O
,	O
either	O
as	O
part	O
of	O
the	O
learning	O
process	O
,	O
or	O
to	O
analyze	O
the	O
learned	O
model	B
.	O
the	O
back-	O
propagation	O
algorithm	O
can	O
be	O
applied	O
to	O
these	O
tasks	O
as	O
well	O
,	O
and	O
is	O
not	O
restricted	O
to	O
computing	O
the	O
gradient	O
of	O
the	O
cost	O
function	O
with	O
respect	O
to	O
the	O
parameters	O
.	O
the	O
idea	O
of	O
computing	O
derivatives	O
by	O
propagating	O
information	O
through	O
a	O
network	O
is	O
very	O
general	O
,	O
and	O
can	O
be	O
used	O
to	O
compute	O
values	O
such	O
as	O
the	O
jacobian	O
of	O
a	O
function	O
f	O
with	O
multiple	O
outputs	O
.	O
we	O
restrict	O
our	O
description	O
here	O
to	O
the	O
most	O
commonly	O
used	O
case	O
where	O
has	O
a	O
single	O
output	O
.	O
f	O
204	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
6.5.1	O
computational	O
graphs	O
so	O
far	O
we	O
have	O
discussed	O
neural	O
networks	O
with	O
a	O
relatively	O
informal	O
graph	O
language	O
.	O
to	O
describe	O
the	O
back-propagation	O
algorithm	O
more	O
precisely	O
,	O
it	O
is	O
helpful	O
to	O
have	O
a	O
more	O
precise	O
computational	O
graph	O
language	O
.	O
many	O
ways	O
of	O
formalizing	O
computation	O
as	O
graphs	O
are	O
possible	O
.	O
here	O
,	O
we	O
use	O
each	O
node	O
in	O
the	O
graph	O
to	O
indicate	O
a	O
variable	O
.	O
the	O
variable	O
may	O
be	O
a	O
scalar	O
,	O
vector	O
,	O
matrix	O
,	O
tensor	O
,	O
or	O
even	O
a	O
variable	O
of	O
another	O
type	O
.	O
to	O
formalize	O
our	O
graphs	O
,	O
we	O
also	O
need	O
to	O
introduce	O
the	O
idea	O
of	O
an	O
operation	O
.	O
an	O
operation	O
is	O
a	O
simple	O
function	O
of	O
one	O
or	O
more	O
variables	O
.	O
our	O
graph	O
language	O
is	O
accompanied	O
by	O
a	O
set	O
of	O
allowable	O
operations	O
.	O
functions	O
more	O
complicated	O
than	O
the	O
operations	O
in	O
this	O
set	O
may	O
be	O
described	O
by	O
composing	O
many	O
operations	O
together	O
.	O
without	O
loss	O
of	O
generality	O
,	O
we	O
deﬁne	O
an	O
operation	O
to	O
return	O
only	O
a	O
single	O
output	O
variable	O
.	O
this	O
does	O
not	O
lose	O
generality	O
because	O
the	O
output	O
variable	O
can	O
have	O
multiple	O
entries	O
,	O
such	O
as	O
a	O
vector	O
.	O
software	O
implementations	O
of	O
back-propagation	O
usually	O
support	O
operations	O
with	O
multiple	O
outputs	O
,	O
but	O
we	O
avoid	O
this	O
case	O
in	O
our	O
description	O
because	O
it	O
introduces	O
many	O
extra	O
details	O
that	O
are	O
not	O
important	O
to	O
conceptual	O
understanding	O
.	O
if	O
a	O
variable	O
y	O
is	O
computed	O
by	O
applying	O
an	O
operation	O
to	O
a	O
variable	O
x	O
,	O
then	O
we	O
draw	O
a	O
directed	O
edge	O
from	O
x	O
to	O
y	O
.	O
we	O
sometimes	O
annotate	O
the	O
output	O
node	O
with	O
the	O
name	O
of	O
the	O
operation	O
applied	O
,	O
and	O
other	O
times	O
omit	O
this	O
label	O
when	O
the	O
operation	O
is	O
clear	O
from	O
context	O
.	O
examples	O
of	O
computational	O
graphs	O
are	O
shown	O
in	O
ﬁgure	O
.6.8	O
6.5.2	O
chain	O
rule	O
of	O
calculus	O
the	O
chain	O
rule	O
of	O
calculus	O
(	O
not	O
to	O
be	O
confused	O
with	O
the	O
chain	O
rule	O
of	O
probability	O
)	O
is	O
used	O
to	O
compute	O
the	O
derivatives	O
of	O
functions	O
formed	O
by	O
composing	O
other	O
functions	O
whose	O
derivatives	O
are	O
known	O
.	O
back-propagation	O
is	O
an	O
algorithm	O
that	O
computes	O
the	O
chain	O
rule	O
,	O
with	O
a	O
speciﬁc	O
order	O
of	O
operations	O
that	O
is	O
highly	O
eﬃcient	O
.	O
let	O
x	O
be	O
a	O
real	O
number	O
,	O
and	O
let	O
f	O
and	O
g	O
both	O
be	O
functions	O
mapping	O
from	O
a	O
real	O
number	O
to	O
a	O
real	O
number	O
.	O
suppose	O
that	O
y	O
=	O
g	O
(	O
x	O
)	O
and	O
z	O
=	O
f	O
(	O
g	O
(	O
x	O
)	O
)	O
=	O
f	O
(	O
y	O
)	O
.	O
then	O
the	O
chain	O
rule	O
states	O
that	O
dz	O
dx	O
=	O
dz	O
dy	O
dy	O
dx	O
.	O
we	O
can	O
generalize	O
this	O
beyond	O
the	O
scalar	O
case	O
.	O
suppose	O
that	O
x	O
205	O
∈	O
m	O
,	O
y	O
r	O
(	O
6.44	O
)	O
∈	O
n	O
,	O
r	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
ˆyˆy	O
σ	O
u	O
(	O
2	O
)	O
u	O
(	O
2	O
)	O
+	O
u	O
(	O
1	O
)	O
u	O
(	O
1	O
)	O
dot	O
zz	O
×	O
xx	O
yy	O
xx	O
ww	O
bb	O
(	O
a	O
)	O
hh	O
relu	O
u	O
(	O
1	O
)	O
u	O
(	O
1	O
)	O
u	O
(	O
2	O
)	O
u	O
(	O
2	O
)	O
+	O
matmul	O
xx	O
ww	O
bb	O
(	O
c	O
)	O
(	O
b	O
)	O
u	O
(	O
2	O
)	O
u	O
(	O
2	O
)	O
u	O
(	O
1	O
)	O
u	O
(	O
1	O
)	O
sum	O
dot	O
sqr	O
ww	O
(	O
d	O
)	O
ˆyˆy	O
xx	O
u	O
(	O
3	O
)	O
u	O
(	O
3	O
)	O
×	O
λλ	O
	O
	O
×	O
	O
ˆy	O
=	O
σ	O
x	O
(	O
b	O
)	O
(	O
a	O
)	O
the	O
graph	O
using	O
the	O
the	O
graph	O
for	O
the	O
logistic	O
regression	O
prediction	O
figure	O
6.8	O
:	O
examples	O
of	O
computational	O
graphs	O
.	O
operation	O
to	O
compute	O
z	O
=	O
xy	O
.	O
.	O
w	O
+	O
b	O
some	O
of	O
the	O
intermediate	O
expressions	O
do	O
not	O
have	O
names	O
in	O
the	O
algebraic	O
expression	O
{	O
but	O
need	O
names	O
in	O
the	O
graph	O
.	O
we	O
simply	O
name	O
the	O
i-th	O
such	O
variable	O
u	O
(	O
)	O
i	O
.	O
the	O
,	O
which	O
computes	O
a	O
design	O
computational	O
graph	O
for	O
the	O
expression	O
h	O
=	O
max	O
matrix	O
of	O
rectiﬁed	O
linear	O
unit	O
activations	O
h	O
given	O
a	O
design	O
matrix	O
containing	O
a	O
minibatch	O
of	O
inputs	O
x	O
.	O
examples	O
a–c	O
applied	O
at	O
most	O
one	O
operation	O
to	O
each	O
variable	O
,	O
but	O
it	O
is	O
possible	O
to	O
apply	O
more	O
than	O
one	O
operation	O
.	O
here	O
we	O
show	O
a	O
computation	O
graph	O
that	O
applies	O
more	O
than	O
one	O
operation	O
to	O
the	O
weights	O
w	O
of	O
a	O
linear	O
regression	O
model	B
.	O
the	O
weights	O
are	O
used	O
to	O
make	O
both	O
the	O
prediction	O
ˆy	O
and	O
the	O
weight	O
decay	O
penalty	O
λ	O
}	O
0	O
,	O
xw	O
+	O
b	O
	O
(	O
d	O
)	O
(	O
c	O
)	O
i	O
.	O
w2	O
i	O
206	O
g	O
maps	O
from	O
r	O
m	O
to	O
r	O
n	O
,	O
and	O
f	O
maps	O
from	O
r	O
n	O
to	O
r	O
.	O
if	O
y	O
=	O
g	O
(	O
x	O
)	O
and	O
z	O
=	O
f	O
(	O
y	O
)	O
,	O
then	O
∂z	O
∂yj	O
∂yj	O
∂xi	O
(	O
6.45	O
)	O
.	O
∂z	O
∂xi	O
=	O
in	O
vector	O
notation	O
,	O
this	O
may	O
be	O
equivalently	O
written	O
as	O
∇	O
xz	O
=	O
∂y	O
∂x	O
y	O
z	O
,	O
(	O
6.46	O
)	O
	O
	O
j	O
	O
	O
∇	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
×	O
n	O
m	O
is	O
the	O
where	O
∂y	O
∂x	O
jacobian	O
matrix	O
of	O
g	O
.	O
∇	O
from	O
this	O
we	O
see	O
that	O
the	O
gradient	O
of	O
a	O
variable	O
x	O
can	O
be	O
obtained	O
by	O
multiplying	O
a	O
jacobian	O
matrix	O
∂y	O
yz	O
.	O
the	O
back-propagation	O
algorithm	O
consists	O
∂x	O
of	O
performing	O
such	O
a	O
jacobian-gradient	O
product	O
for	O
each	O
operation	O
in	O
the	O
graph	O
.	O
by	O
a	O
gradient	O
usually	O
we	O
do	O
not	O
apply	O
the	O
back-propagation	O
algorithm	O
merely	O
to	O
vectors	O
,	O
but	O
rather	O
to	O
tensors	O
of	O
arbitrary	O
dimensionality	O
.	O
conceptually	O
,	O
this	O
is	O
exactly	O
the	O
same	O
as	O
back-propagation	O
with	O
vectors	O
.	O
the	O
only	O
diﬀerence	O
is	O
how	O
the	O
numbers	O
are	O
arranged	O
in	O
a	O
grid	O
to	O
form	O
a	O
tensor	O
.	O
we	O
could	O
imagine	O
ﬂattening	O
each	O
tensor	O
into	O
a	O
vector	O
before	O
we	O
run	O
back-propagation	O
,	O
computing	O
a	O
vector-valued	O
gradient	O
,	O
and	O
then	O
reshaping	O
the	O
gradient	O
back	O
into	O
a	O
tensor	O
.	O
in	O
this	O
rearranged	O
view	O
,	O
back-propagation	O
is	O
still	O
just	O
multiplying	O
jacobians	O
by	O
gradients	O
.	O
to	O
denote	O
the	O
gradient	O
of	O
a	O
value	O
z	O
with	O
respect	O
to	O
a	O
tensor	O
x	O
,	O
we	O
write	O
z	O
,	O
just	O
as	O
if	O
x	O
were	O
a	O
vector	O
.	O
the	O
indices	O
into	O
x	O
now	O
have	O
multiple	O
coordinates—for	O
example	O
,	O
a	O
3-d	O
tensor	O
is	O
indexed	O
by	O
three	O
coordinates	O
.	O
we	O
can	O
abstract	O
this	O
away	O
by	O
using	O
a	O
single	O
variable	O
i	O
to	O
represent	O
the	O
complete	O
tuple	O
of	O
indices	O
.	O
for	O
all	O
z	O
)	O
i	O
gives	O
∂z	O
∇	O
possible	O
index	O
tuples	O
i	O
,	O
(	O
.	O
this	O
is	O
exactly	O
the	O
same	O
as	O
how	O
for	O
all	O
∂xi	O
x	O
z	O
)	O
i	O
gives	O
∂z	O
possible	O
integer	O
indices	O
i	O
into	O
a	O
vector	O
,	O
(	O
.	O
using	O
this	O
notation	O
,	O
we	O
∂x	O
i	O
,	O
then	O
can	O
write	O
the	O
chain	O
rule	O
as	O
it	O
applies	O
to	O
tensors	O
.	O
if	O
x=	O
(	O
g	O
)	O
	O
f=	O
(	O
)	O
y	O
and	O
∇	O
y	O
z	O
x	O
x	O
∇	O
∇	O
x	O
z	O
=	O
j	O
∇	O
(	O
xyj	O
)	O
∂z	O
∂	O
yj	O
.	O
(	O
6.47	O
)	O
6.5.3	O
recursively	O
applying	O
the	O
chain	O
rule	O
to	O
obtain	O
backprop	O
using	O
the	O
chain	O
rule	O
,	O
it	O
is	O
straightforward	O
to	O
write	O
down	O
an	O
algebraic	O
expression	O
for	O
the	O
gradient	O
of	O
a	O
scalar	O
with	O
respect	O
to	O
any	O
node	O
in	O
the	O
computational	O
graph	O
that	O
produced	O
that	O
scalar	O
.	O
however	O
,	O
actually	O
evaluating	O
that	O
expression	O
in	O
a	O
computer	O
introduces	O
some	O
extra	O
considerations	O
.	O
speciﬁcally	O
,	O
many	O
subexpressions	O
may	O
be	O
repeated	O
several	O
times	O
within	O
the	O
overall	O
expression	O
for	O
the	O
gradient	O
.	O
any	O
procedure	O
that	O
computes	O
the	O
gradient	O
207	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
6.9	O
will	O
need	O
to	O
choose	O
whether	O
to	O
store	O
these	O
subexpressions	O
or	O
to	O
recompute	O
them	O
several	O
times	O
.	O
an	O
example	O
of	O
how	O
these	O
repeated	O
subexpressions	O
arise	O
is	O
given	O
in	O
ﬁgure	O
.	O
in	O
some	O
cases	O
,	O
computing	O
the	O
same	O
subexpression	O
twice	O
would	O
simply	O
be	O
wasteful	O
.	O
for	O
complicated	O
graphs	O
,	O
there	O
can	O
be	O
exponentially	O
many	O
of	O
these	O
wasted	O
computations	O
,	O
making	O
a	O
naive	O
implementation	O
of	O
the	O
chain	O
rule	O
infeasible	O
.	O
in	O
other	O
cases	O
,	O
computing	O
the	O
same	O
subexpression	O
twice	O
could	O
be	O
a	O
valid	O
way	O
to	O
reduce	O
memory	O
consumption	O
at	O
the	O
cost	O
of	O
higher	O
runtime	O
.	O
6.2	O
along	O
with	O
algorithm	O
we	O
ﬁrst	O
begin	O
by	O
a	O
version	O
of	O
the	O
back-propagation	O
algorithm	O
that	O
speciﬁes	O
the	O
actual	O
gradient	O
computation	O
directly	O
(	O
algorithm	O
for	O
the	O
associated	O
forward	O
computation	O
)	O
,	O
in	O
the	O
order	O
it	O
will	O
actually	O
be	O
done	O
and	O
according	O
to	O
the	O
recursive	O
application	O
of	O
chain	O
rule	O
.	O
one	O
could	O
either	O
directly	O
perform	O
these	O
computations	O
or	O
view	O
the	O
description	O
of	O
the	O
algorithm	O
as	O
a	O
symbolic	O
speciﬁcation	O
of	O
the	O
computational	O
graph	O
for	O
computing	O
the	O
back-propagation	O
.	O
however	O
,	O
this	O
formulation	O
does	O
not	O
make	O
explicit	O
the	O
manipulation	O
and	O
the	O
construction	O
of	O
the	O
symbolic	O
graph	O
that	O
performs	O
the	O
gradient	O
computation	O
.	O
such	O
a	O
formulation	O
is	O
presented	O
below	O
in	O
section	O
,	O
with	O
algorithm	O
,	O
where	O
we	O
also	O
generalize	O
to	O
nodes	O
that	O
contain	O
arbitrary	O
tensors	O
.	O
6.5.6	O
6.5	O
6.1	O
first	O
consider	O
a	O
computational	O
graph	O
describing	O
how	O
to	O
compute	O
a	O
single	O
scalar	O
u	O
(	O
)	O
n	O
(	O
say	O
the	O
loss	O
on	O
a	O
training	O
example	O
)	O
.	O
this	O
scalar	O
is	O
the	O
quantity	O
whose	O
gradient	O
we	O
want	O
to	O
obtain	O
,	O
with	O
respect	O
to	O
the	O
ni	O
input	O
nodes	O
u	O
(	O
1	O
)	O
to	O
u	O
(	O
ni	O
)	O
.	O
in	O
other	O
words	O
we	O
wish	O
to	O
compute	O
∂u	O
(	O
)	O
n	O
.	O
in	O
the	O
application	O
∂u	O
(	O
)	O
i	O
of	O
back-propagation	O
to	O
computing	O
gradients	O
for	O
gradient	O
descent	B
over	O
parameters	O
,	O
u	O
(	O
)	O
n	O
will	O
be	O
the	O
cost	O
associated	O
with	O
an	O
example	O
or	O
a	O
minibatch	O
,	O
while	O
u	O
(	O
1	O
)	O
to	O
u	O
(	O
ni	O
)	O
correspond	O
to	O
the	O
parameters	O
of	O
the	O
model.	O
}	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
ni	O
for	O
all	O
i	O
∈	O
{	O
we	O
will	O
assume	O
that	O
the	O
nodes	O
of	O
the	O
graph	O
have	O
been	O
ordered	O
in	O
such	O
a	O
way	O
that	O
we	O
can	O
compute	O
their	O
output	O
one	O
after	O
the	O
other	O
,	O
starting	O
at	O
u	O
(	O
ni	O
+1	O
)	O
and	O
going	O
up	O
to	O
u	O
(	O
)	O
n	O
.	O
as	O
deﬁned	O
in	O
algorithm	O
,	O
each	O
node	O
is	O
associated	O
with	O
an	O
operation	O
f	O
(	O
)	O
i	O
and	O
is	O
computed	O
by	O
evaluating	O
the	O
function	O
u	O
(	O
)	O
i	O
6.1	O
where	O
a	O
(	O
)	O
i	O
is	O
the	O
set	O
of	O
all	O
nodes	O
that	O
are	O
parents	O
of	O
u	O
(	O
)	O
i	O
.	O
u	O
(	O
)	O
i	O
=	O
(	O
f	O
a	O
(	O
)	O
i	O
)	O
(	O
6.48	O
)	O
g	O
.	O
b	O
that	O
algorithm	O
speciﬁes	O
the	O
forward	O
propagation	O
computation	O
,	O
which	O
we	O
could	O
in	O
order	O
to	O
perform	O
back-propagation	O
,	O
we	O
can	O
construct	O
a	O
and	O
adds	O
to	O
it	O
an	O
extra	O
set	O
of	O
nodes	O
.	O
these	O
b	O
proceeds	O
in	O
computes	O
associated	O
with	O
the	O
forward	O
graph	O
node	O
u	O
(	O
)	O
i	O
.	O
this	O
is	O
done	O
put	O
in	O
a	O
graph	O
computational	O
graph	O
that	O
depends	O
on	O
form	O
a	O
subgraph	O
exactly	O
the	O
reverse	O
of	O
the	O
order	O
of	O
computation	O
in	O
the	O
derivative	O
∂u	O
(	O
)	O
n	O
∂u	O
(	O
)	O
i	O
g	O
.	O
computation	O
in	O
with	O
one	O
node	O
per	O
node	O
of	O
,	O
and	O
each	O
node	O
of	O
b	O
g	O
g	O
208	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
(	O
)	O
i	O
that	O
comprises	O
the	O
values	O
of	O
previous	O
nodes	O
u	O
(	O
)	O
j	O
,	O
j	O
<	O
i	O
,	O
with	O
j	O
p	O
a	O
algorithm	O
6.1	O
a	O
procedure	O
that	O
performs	O
the	O
computations	O
mapping	O
ni	O
inputs	O
u	O
(	O
1	O
)	O
to	O
u	O
(	O
ni	O
)	O
to	O
an	O
output	O
u	O
(	O
)	O
n	O
.	O
this	O
deﬁnes	O
a	O
computational	O
graph	O
where	O
each	O
node	O
computes	O
numerical	O
value	O
u	O
(	O
)	O
i	O
by	O
applying	O
a	O
function	O
f	O
(	O
)	O
i	O
to	O
the	O
set	O
of	O
arguments	O
(	O
u	O
(	O
)	O
i	O
)	O
.	O
the	O
a	O
input	O
to	O
the	O
computational	O
graph	O
is	O
the	O
vector	O
x	O
,	O
and	O
is	O
set	O
into	O
the	O
ﬁrst	O
ni	O
nodes	O
u	O
(	O
1	O
)	O
to	O
u	O
(	O
ni	O
)	O
.	O
the	O
output	O
of	O
the	O
computational	O
graph	O
is	O
read	O
oﬀ	O
the	O
last	O
(	O
output	O
)	O
node	O
u	O
(	O
)	O
n	O
.	O
←	O
=	O
1	O
i	O
do	O
∈	O
for	O
i	O
u	O
(	O
)	O
i	O
,	O
.	O
.	O
.	O
,	O
n	O
xi	O
end	O
for	O
for	O
i	O
(	O
)	O
i	O
a	O
u	O
(	O
)	O
i	O
←	O
{	O
n=	O
i	O
+	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
do	O
←	O
|	O
∈	O
}	O
j	O
p	O
a	O
u	O
(	O
(	O
)	O
i	O
)	O
(	O
)	O
i	O
)	O
u	O
(	O
)	O
j	O
f	O
(	O
)	O
i	O
(	O
a	O
end	O
for	O
return	O
u	O
(	O
)	O
n	O
	O
using	O
the	O
chain	O
rule	O
with	O
respect	O
to	O
scalar	O
output	O
u	O
(	O
)	O
n	O
:	O
∂u	O
(	O
)	O
n	O
∂u	O
(	O
)	O
j	O
=	O
∈	O
i	O
j	O
p	O
a	O
u	O
:	O
(	O
(	O
)	O
i	O
)	O
∂u	O
(	O
)	O
n	O
∂u	O
(	O
)	O
i	O
b	O
∂u	O
(	O
)	O
i	O
∂u	O
(	O
)	O
j	O
(	O
6.49	O
)	O
g	O
6.2	O
.	O
the	O
edge	O
from	O
u	O
(	O
)	O
j	O
to	O
u	O
(	O
)	O
i	O
contains	O
exactly	O
one	O
edge	O
for	O
each	O
as	O
speciﬁed	O
by	O
algorithm	O
.	O
the	O
subgraph	O
edge	O
from	O
node	O
u	O
(	O
)	O
j	O
to	O
node	O
u	O
(	O
)	O
i	O
of	O
is	O
associated	O
with	O
the	O
computation	O
of	O
∂u	O
(	O
)	O
i	O
.	O
in	O
addition	O
,	O
a	O
dot	O
product	O
is	O
performed	O
for	O
each	O
node	O
,	O
∂u	O
(	O
)	O
j	O
between	O
the	O
gradient	O
already	O
computed	O
with	O
respect	O
to	O
nodes	O
u	O
(	O
)	O
i	O
that	O
are	O
children	O
of	O
u	O
(	O
)	O
j	O
and	O
the	O
vector	O
containing	O
the	O
partial	O
derivatives	O
∂u	O
(	O
)	O
i	O
for	O
the	O
same	O
children	O
∂u	O
(	O
)	O
j	O
nodes	O
u	O
(	O
)	O
i	O
.	O
to	O
summarize	O
,	O
the	O
amount	O
of	O
computation	O
required	O
for	O
performing	O
the	O
back-propagation	O
scales	O
linearly	O
with	O
the	O
number	O
of	O
edges	O
in	O
,	O
where	O
the	O
computation	O
for	O
each	O
edge	O
corresponds	O
to	O
computing	O
a	O
partial	O
derivative	O
(	O
of	O
one	O
node	O
with	O
respect	O
to	O
one	O
of	O
its	O
parents	O
)	O
as	O
well	O
as	O
performing	O
one	O
multiplication	O
and	O
one	O
addition	O
.	O
below	O
,	O
we	O
generalize	O
this	O
analysis	O
to	O
tensor-valued	O
nodes	O
,	O
which	O
is	O
just	O
a	O
way	O
to	O
group	O
multiple	O
scalar	O
values	O
in	O
the	O
same	O
node	O
and	O
enable	O
more	O
eﬃcient	O
implementations	O
.	O
g	O
the	O
back-propagation	O
algorithm	O
is	O
designed	O
to	O
reduce	O
the	O
number	O
of	O
common	O
subexpressions	O
without	O
regard	O
to	O
memory	O
.	O
speciﬁcally	O
,	O
it	O
performs	O
on	O
the	O
order	O
of	O
one	O
jacobian	O
product	O
per	O
node	O
in	O
the	O
graph	O
.	O
this	O
can	O
be	O
seen	O
from	O
the	O
fact	O
u	O
(	O
)	O
j	O
to	O
node	O
u	O
(	O
)	O
i	O
of	O
that	O
backprop	O
(	O
algorithm	O
the	O
graph	O
exactly	O
once	O
in	O
order	O
to	O
obtain	O
the	O
associated	O
partial	O
derivative	O
∂u	O
(	O
)	O
i	O
.	O
∂u	O
(	O
)	O
j	O
)	O
visits	O
each	O
edge	O
from	O
node	O
6.2	O
209	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
algorithm	O
6.2	O
simpliﬁed	O
version	O
of	O
the	O
back-propagation	O
algorithm	O
for	O
computing	O
the	O
derivatives	O
of	O
u	O
(	O
)	O
n	O
with	O
respect	O
to	O
the	O
variables	O
in	O
the	O
graph	O
.	O
this	O
example	O
is	O
intended	O
to	O
further	O
understanding	O
by	O
showing	O
a	O
simpliﬁed	O
case	O
where	O
all	O
variables	O
are	O
scalars	O
,	O
and	O
we	O
wish	O
to	O
compute	O
the	O
derivatives	O
with	O
respect	O
to	O
u	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
u	O
(	O
ni	O
)	O
.	O
this	O
simpliﬁed	O
version	O
computes	O
the	O
derivatives	O
of	O
all	O
nodes	O
in	O
the	O
graph	O
.	O
the	O
computational	O
cost	O
of	O
this	O
algorithm	O
is	O
proportional	O
to	O
the	O
number	O
of	O
edges	O
in	O
the	O
graph	O
,	O
assuming	O
that	O
the	O
partial	O
derivative	O
associated	O
with	O
each	O
edge	O
requires	O
a	O
constant	O
time	O
.	O
this	O
is	O
of	O
the	O
same	O
order	O
as	O
the	O
number	O
of	O
computations	O
for	O
the	O
forward	O
propagation	O
.	O
each	O
∂u	O
(	O
)	O
i	O
is	O
a	O
function	O
of	O
the	O
parents	O
u	O
(	O
)	O
j	O
of	O
u	O
(	O
)	O
i	O
,	O
thus	O
∂u	O
(	O
)	O
j	O
linking	O
the	O
nodes	O
of	O
the	O
forward	O
graph	O
to	O
those	O
added	O
for	O
the	O
back-propagation	O
graph	O
.	O
6.1	O
for	O
this	O
example	O
)	O
to	O
obtain	O
the	O
activa-	O
run	O
forward	O
propagation	O
(	O
algorithm	O
tions	O
of	O
the	O
network	O
initialize	O
grad_table	O
,	O
a	O
data	O
structure	O
that	O
will	O
store	O
the	O
derivatives	O
that	O
have	O
[	O
u	O
(	O
)	O
i	O
]	O
will	O
store	O
the	O
computed	O
value	O
of	O
been	O
computed	O
.	O
the	O
entry	O
grad	O
table	O
∂u	O
(	O
)	O
n	O
∂u	O
(	O
)	O
i	O
grad	O
table	O
for	O
.	O
_	O
j	O
n=	O
	O
	O
←	O
−	O
do	O
_	O
using	O
stored	O
values	O
:	O
1	O
[	O
u	O
(	O
)	O
n	O
]	O
1	O
down	O
to	O
1	O
←	O
the	O
next	O
line	O
computes	O
∂u	O
(	O
)	O
n	O
∂u	O
(	O
)	O
j	O
∈	O
|	O
grad	O
table	O
[	O
u	O
(	O
)	O
j	O
]	O
_	O
{	O
end	O
for	O
return	O
grad	O
table	O
_	O
[	O
u	O
(	O
)	O
i	O
]	O
=	O
1	O
i	O
=	O
_	O
}	O
,	O
.	O
.	O
.	O
,	O
ni	O
∈	O
i	O
j	O
p	O
a	O
u	O
:	O
(	O
(	O
)	O
i	O
)	O
∂u	O
(	O
)	O
i	O
∂u	O
(	O
)	O
j	O
∂u	O
(	O
)	O
n	O
∂u	O
(	O
)	O
i	O
[	O
u	O
(	O
)	O
i	O
]	O
∂u	O
(	O
)	O
i	O
∂u	O
(	O
)	O
j	O
i	O
j	O
p	O
a	O
u	O
:	O
(	O
(	O
)	O
i	O
)	O
grad	O
table	O
back-propagation	O
thus	O
avoids	O
the	O
exponential	O
explosion	O
in	O
repeated	O
subexpressions	O
.	O
however	O
,	O
other	O
algorithms	O
may	O
be	O
able	O
to	O
avoid	O
more	O
subexpressions	O
by	O
performing	O
simpliﬁcations	O
on	O
the	O
computational	O
graph	O
,	O
or	O
may	O
be	O
able	O
to	O
conserve	O
memory	O
by	O
recomputing	O
rather	O
than	O
storing	O
some	O
subexpressions	O
.	O
we	O
will	O
revisit	O
these	O
ideas	O
after	O
describing	O
the	O
back-propagation	O
algorithm	O
itself	O
.	O
6.5.4	O
back-propagation	O
computation	O
in	O
fully-connected	O
mlp	O
to	O
clarify	O
the	O
above	O
deﬁnition	O
of	O
the	O
back-propagation	O
computation	O
,	O
let	O
us	O
consider	O
the	O
speciﬁc	O
graph	O
associated	O
with	O
a	O
fully-connected	O
multi-layer	O
mlp	O
.	O
algorithm	O
6.3	O
the	O
supervised	O
loss	O
l	O
(	O
ˆy	O
y	O
,	O
(	O
)	O
x	O
y	O
,	O
ﬁrst	O
shows	O
the	O
forward	O
propagation	O
,	O
which	O
maps	O
parameters	O
to	O
)	O
associated	O
with	O
a	O
single	O
(	O
input	O
,	O
target	O
)	O
training	O
example	O
,	O
with	O
ˆy	O
the	O
output	O
of	O
the	O
neural	O
network	O
when	O
x	O
is	O
provided	O
in	O
input	O
.	O
algorithm	O
6.4	O
then	O
shows	O
the	O
corresponding	O
computation	O
to	O
be	O
done	O
for	O
210	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
f	O
f	O
f	O
zz	O
yy	O
xx	O
ww	O
∈	O
figure	O
6.9	O
:	O
a	O
computational	O
graph	O
that	O
results	O
in	O
repeated	O
subexpressions	O
when	O
computing	O
the	O
gradient	O
.	O
let	O
w	O
r	O
as	O
the	O
operation	O
that	O
we	O
apply	O
at	O
every	O
step	O
of	O
a	O
chain	O
:	O
x	O
=	O
f	O
(	O
w	O
)	O
,	O
y	O
=	O
f	O
(	O
x	O
)	O
,	O
z	O
=	O
f	O
(	O
y	O
)	O
.	O
to	O
compute	O
∂z	O
∂w	O
r	O
be	O
the	O
input	O
to	O
the	O
graph	O
.	O
we	O
use	O
the	O
same	O
function	O
f	O
:	O
r	O
,	O
we	O
apply	O
equation	O
and	O
obtain	O
:	O
6.44	O
→	O
∂z	O
∂w	O
∂z	O
∂y	O
∂y	O
∂x	O
	O
(	O
)	O
y	O
f	O
	O
(	O
(	O
(	O
)	O
)	O
)	O
f	O
f	O
w	O
f	O
∂x	O
∂w	O
	O
(	O
)	O
x	O
f	O
	O
=	O
=f	O
=f	O
(	O
)	O
w	O
	O
(	O
(	O
)	O
)	O
f	O
w	O
f	O
(	O
6.50	O
)	O
(	O
6.51	O
)	O
(	O
6.52	O
)	O
(	O
6.53	O
)	O
	O
(	O
)	O
w	O
6.52	O
suggests	O
an	O
implementation	O
in	O
which	O
we	O
compute	O
the	O
value	O
of	O
f	O
(	O
w	O
)	O
only	O
equation	O
once	O
and	O
store	O
it	O
in	O
the	O
variable	O
x.	O
this	O
is	O
the	O
approach	O
taken	O
by	O
the	O
back-propagation	O
algorithm	O
.	O
an	O
alternative	O
approach	O
is	O
suggested	O
by	O
equation	O
,	O
where	O
the	O
subexpression	O
f	O
(	O
w	O
)	O
appears	O
more	O
than	O
once	O
.	O
in	O
the	O
alternative	O
approach	O
,	O
f	O
(	O
w	O
)	O
is	O
recomputed	O
each	O
time	O
it	O
is	O
needed	O
.	O
when	O
the	O
memory	O
required	O
to	O
store	O
the	O
value	O
of	O
these	O
expressions	O
is	O
low	O
,	O
the	O
back-propagation	O
approach	O
of	O
equation	O
is	O
clearly	O
preferable	O
because	O
of	O
its	O
reduced	O
runtime	O
.	O
however	O
,	O
equation	O
is	O
also	O
a	O
valid	O
implementation	O
of	O
the	O
chain	O
rule	O
,	O
and	O
is	O
useful	O
when	O
memory	O
is	O
limited	O
.	O
6.52	O
6.53	O
6.53	O
211	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
applying	O
the	O
back-propagation	O
algorithm	O
to	O
this	O
graph	O
.	O
6.3	O
algorithms	O
are	O
demonstrations	O
that	O
are	O
chosen	O
to	O
be	O
simple	O
and	O
straightforward	O
to	O
understand	O
.	O
however	O
,	O
they	O
are	O
specialized	O
to	O
one	O
speciﬁc	O
problem	O
.	O
and	O
6.4	O
modern	O
software	O
implementations	O
are	O
based	O
on	O
the	O
generalized	O
form	O
of	O
back-	O
propagation	O
described	O
in	O
section	O
below	O
,	O
which	O
can	O
accommodate	O
any	O
compu-	O
tational	O
graph	O
by	O
explicitly	O
manipulating	O
a	O
data	O
structure	O
for	O
representing	O
symbolic	O
computation	O
.	O
6.5.6	O
6.2.1.1	O
algorithm	O
6.3	O
forward	O
propagation	O
through	O
a	O
typical	O
deep	O
neural	O
network	O
and	O
the	O
computation	O
of	O
the	O
cost	O
function	O
.	O
the	O
loss	O
l	O
(	O
ˆy	O
y	O
,	O
)	O
depends	O
on	O
the	O
output	O
ˆy	O
and	O
on	O
the	O
target	O
y	O
(	O
see	O
section	O
for	O
examples	O
of	O
loss	O
functions	O
)	O
.	O
to	O
obtain	O
the	O
total	O
cost	O
j	O
,	O
the	O
loss	O
may	O
be	O
added	O
to	O
a	O
regularizer	O
ω	O
(	O
θ	O
)	O
,	O
where	O
θ	O
contains	O
all	O
the	O
parameters	O
(	O
weights	O
and	O
biases	O
)	O
.	O
algorithm	O
shows	O
how	O
to	O
compute	O
gradients	O
of	O
j	O
with	O
respect	O
to	O
parameters	O
w	O
and	O
b.	O
for	O
simplicity	O
,	O
this	O
demonstration	O
uses	O
only	O
a	O
single	O
input	O
example	O
x.	O
practical	O
applications	O
should	O
use	O
a	O
minibatch	O
.	O
see	O
section	O
6.5.7	O
∈	O
{	O
}	O
require	O
:	O
network	O
depth	O
,	O
l	O
∈	O
{	O
}	O
require	O
:	O
w	O
(	O
)	O
i	O
,	O
i	O
1	O
,	O
.	O
.	O
.	O
,	O
l	O
,	O
require	O
:	O
b	O
(	O
)	O
i	O
,	O
i	O
,	O
.	O
.	O
.	O
,	O
l	O
,	O
1	O
require	O
:	O
x	O
,	O
the	O
input	O
to	O
process	O
require	O
:	O
y	O
,	O
the	O
target	O
output	O
for	O
a	O
more	O
realistic	O
demonstration	O
.	O
the	O
bias	O
parameters	O
of	O
the	O
model	B
the	O
weight	O
matrices	O
of	O
the	O
model	B
6.4	O
h	O
(	O
0	O
)	O
=	O
x	O
=	O
1	O
for	O
k	O
do	O
,	O
.	O
.	O
.	O
,	O
l	O
−	O
a	O
(	O
)	O
k	O
=	O
b	O
(	O
)	O
k	O
+	O
w	O
(	O
)	O
k	O
h	O
(	O
1	O
)	O
k	O
h	O
(	O
)	O
k	O
=	O
(	O
f	O
a	O
(	O
)	O
k	O
)	O
end	O
for	O
ˆy	O
(	O
)	O
l	O
j	O
h=	O
l=	O
(	O
ˆy	O
y	O
,	O
)	O
+	O
ω	O
(	O
)	O
λ	O
θ	O
6.5.5	O
symbol-to-symbol	O
derivatives	O
algebraic	O
expressions	O
and	O
computational	O
graphs	O
both	O
operate	O
on	O
symbols	O
,	O
or	O
variables	O
that	O
do	O
not	O
have	O
speciﬁc	O
values	O
.	O
these	O
algebraic	O
and	O
graph-based	O
representations	O
are	O
called	O
symbolic	O
representations	O
.	O
when	O
we	O
actually	O
use	O
or	O
train	O
a	O
neural	O
network	O
,	O
we	O
must	O
assign	O
speciﬁc	O
values	O
to	O
these	O
symbols	O
.	O
we	O
replace	O
a	O
symbolic	O
input	O
to	O
the	O
network	O
x	O
with	O
a	O
speciﬁc	O
numeric	O
value	O
,	O
such	O
as	O
[	O
1	O
2	O
3	O
765	O
,	O
−	O
	O
1	O
8	O
]	O
.	O
.	O
,	O
.	O
.	O
212	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
6.3	O
algorithm	O
6.4	O
backward	O
computation	O
for	O
the	O
deep	O
neural	O
network	O
of	O
algo-	O
x	O
a	O
target	O
y.	O
this	O
computation	O
rithm	O
,	O
which	O
uses	O
in	O
addition	O
to	O
the	O
input	O
yields	O
the	O
gradients	O
on	O
the	O
activations	O
a	O
(	O
)	O
k	O
for	O
each	O
layer	O
k	O
,	O
starting	O
from	O
the	O
output	O
layer	O
and	O
going	O
backwards	O
to	O
the	O
ﬁrst	O
hidden	O
layer	O
.	O
from	O
these	O
gradients	O
,	O
which	O
can	O
be	O
interpreted	O
as	O
an	O
indication	O
of	O
how	O
each	O
layer	O
’	O
s	O
output	O
should	O
change	O
to	O
reduce	O
error	O
,	O
one	O
can	O
obtain	O
the	O
gradient	O
on	O
the	O
parameters	O
of	O
each	O
layer	O
.	O
the	O
gradients	O
on	O
weights	O
and	O
biases	O
can	O
be	O
immediately	O
used	O
as	O
part	O
of	O
a	O
stochas-	O
tic	O
gradient	O
update	O
(	O
performing	O
the	O
update	O
right	O
after	O
the	O
gradients	O
have	O
been	O
computed	O
)	O
or	O
used	O
with	O
other	O
gradient-based	O
optimization	O
methods	O
.	O
←	O
∇	O
after	O
the	O
forward	O
computation	O
,	O
compute	O
the	O
gradient	O
on	O
the	O
output	O
layer	O
:	O
g	O
for	O
∇	O
−	O
ˆyj	O
=	O
=	O
ˆy	O
l	O
(	O
ˆy	O
y	O
,	O
1	O
,	O
.	O
.	O
.	O
,	O
1	O
)	O
do	O
l	O
,	O
l	O
k	O
=	O
g	O
(	O
a	O
(	O
)	O
k	O
)	O
a	O
(	O
)	O
k	O
j	O
	O
	O
f	O
convert	O
the	O
gradient	O
on	O
the	O
layer	O
’	O
s	O
output	O
into	O
a	O
gradient	O
into	O
the	O
pre-	O
←	O
∇	O
nonlinearity	O
activation	O
(	O
element-wise	O
multiplication	O
if	O
g	O
compute	O
gradients	O
on	O
weights	O
and	O
biases	O
(	O
including	O
the	O
regularization	O
term	O
,	O
∇	O
∇	O
where	O
needed	O
)	O
:	O
∇	O
∇	O
b	O
(	O
)	O
k	O
j	O
λ	O
b	O
(	O
)	O
k	O
ω	O
(	O
)	O
θ	O
−	O
	O
+	O
λ	O
w	O
(	O
)	O
k	O
j	O
=	O
g	O
h	O
(	O
k	O
1	O
)	O
w	O
(	O
)	O
k	O
ω	O
(	O
)	O
θ	O
←	O
∇	O
propagate	O
the	O
gradients	O
w.r.t	O
.	O
the	O
next	O
lower-level	O
hidden	O
layer	O
’	O
s	O
activations	O
:	O
	O
−	O
j	O
=	O
w	O
(	O
)	O
k	O
g	O
is	O
element-wise	O
)	O
:	O
=	O
+g	O
f	O
k	O
h	O
(	O
1	O
)	O
end	O
for	O
g	O
213	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
f	O
f	O
f	O
zz	O
yy	O
xx	O
ww	O
f	O
f	O
f	O
zz	O
yy	O
xx	O
ww	O
	O
f	O
	O
f	O
	O
f	O
dz	O
dz	O
dy	O
dy	O
dy	O
dy	O
dx	O
dx	O
dx	O
dx	O
dw	O
dw	O
×	O
×	O
dz	O
dz	O
dx	O
dx	O
dz	O
dz	O
dw	O
dw	O
figure	O
6.10	O
:	O
an	O
example	O
of	O
the	O
symbol-to-symbol	O
approach	O
to	O
computing	O
derivatives	O
.	O
in	O
this	O
approach	O
,	O
the	O
back-propagation	O
algorithm	O
does	O
not	O
need	O
to	O
ever	O
access	O
any	O
actual	O
speciﬁc	O
numeric	O
values	O
.	O
instead	O
,	O
it	O
adds	O
nodes	O
to	O
a	O
computational	O
graph	O
describing	O
how	O
to	O
compute	O
these	O
derivatives	O
.	O
a	O
generic	O
graph	O
evaluation	O
engine	O
can	O
later	O
compute	O
the	O
derivatives	O
for	O
any	O
speciﬁc	O
numeric	O
values	O
.	O
(	O
left	O
)	O
in	O
this	O
example	O
,	O
we	O
begin	O
with	O
a	O
graph	O
we	O
run	O
the	O
back-propagation	O
algorithm	O
,	O
instructing	O
representing	O
z	O
=	O
f	O
(	O
f	O
(	O
f	O
(	O
w	O
)	O
)	O
)	O
.	O
it	O
to	O
construct	O
the	O
graph	O
for	O
the	O
expression	O
corresponding	O
to	O
dz	O
.	O
in	O
this	O
example	O
,	O
we	O
do	O
dw	O
not	O
explain	O
how	O
the	O
back-propagation	O
algorithm	O
works	O
.	O
the	O
purpose	O
is	O
only	O
to	O
illustrate	O
what	O
the	O
desired	O
result	O
is	O
:	O
a	O
computational	O
graph	O
with	O
a	O
symbolic	O
description	O
of	O
the	O
derivative	O
.	O
(	O
right	O
)	O
some	O
approaches	O
to	O
back-propagation	O
take	O
a	O
computational	O
graph	O
and	O
a	O
set	O
of	O
numerical	O
values	O
for	O
the	O
inputs	O
to	O
the	O
graph	O
,	O
then	O
return	O
a	O
set	O
of	O
numerical	O
values	O
describing	O
the	O
gradient	O
at	O
those	O
input	O
values	O
.	O
we	O
call	O
this	O
approach	O
“	O
symbol-	O
to-number	O
”	O
diﬀerentiation	O
.	O
this	O
is	O
the	O
approach	O
used	O
by	O
libraries	O
such	O
as	O
torch	O
(	O
collobert	O
et	O
al	O
.	O
2011b	O
)	O
and	O
caﬀe	O
(	O
jia	O
2013	O
)	O
.	O
,	O
,	O
another	O
approach	O
is	O
to	O
take	O
a	O
computational	O
graph	O
and	O
add	O
additional	O
nodes	O
to	O
the	O
graph	O
that	O
provide	O
a	O
symbolic	O
description	O
of	O
the	O
desired	O
derivatives	O
.	O
this	O
is	O
the	O
approach	O
taken	O
by	O
theano	O
(	O
bergstra	O
et	O
al	O
.	O
2010	O
bastien	O
et	O
al	O
.	O
2012	O
)	O
and	O
tensorflow	O
(	O
abadi	O
et	O
al	O
.	O
2015	O
)	O
.	O
an	O
example	O
of	O
how	O
this	O
approach	O
works	O
is	O
illustrated	O
in	O
ﬁgure	O
.	O
the	O
primary	O
advantage	O
of	O
this	O
approach	O
is	O
that	O
the	O
derivatives	O
are	O
described	O
in	O
the	O
same	O
language	O
as	O
the	O
original	O
expression	O
.	O
because	O
the	O
derivatives	O
are	O
just	O
another	O
computational	O
graph	O
,	O
it	O
is	O
possible	O
to	O
run	O
back-propagation	O
again	O
,	O
diﬀerentiating	O
the	O
derivatives	O
in	O
order	O
to	O
obtain	O
higher	O
derivatives	O
.	O
computation	O
of	O
higher-order	O
derivatives	O
is	O
described	O
in	O
section	O
6.5.10	O
.	O
6.10	O
,	O
,	O
;	O
,	O
we	O
will	O
use	O
the	O
latter	O
approach	O
and	O
describe	O
the	O
back-propagation	O
algorithm	O
in	O
214	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
terms	O
of	O
constructing	O
a	O
computational	O
graph	O
for	O
the	O
derivatives	O
.	O
any	O
subset	O
of	O
the	O
graph	O
may	O
then	O
be	O
evaluated	O
using	O
speciﬁc	O
numerical	O
values	O
at	O
a	O
later	O
time	O
.	O
this	O
allows	O
us	O
to	O
avoid	O
specifying	O
exactly	O
when	O
each	O
operation	O
should	O
be	O
computed	O
.	O
instead	O
,	O
a	O
generic	O
graph	O
evaluation	O
engine	O
can	O
evaluate	O
every	O
node	O
as	O
soon	O
as	O
its	O
parents	O
’	O
values	O
are	O
available	O
.	O
the	O
description	O
of	O
the	O
symbol-to-symbol	O
based	O
approach	O
subsumes	O
the	O
symbol-	O
to-number	O
approach	O
.	O
the	O
symbol-to-number	O
approach	O
can	O
be	O
understood	O
as	O
performing	O
exactly	O
the	O
same	O
computations	O
as	O
are	O
done	O
in	O
the	O
graph	O
built	O
by	O
the	O
symbol-to-symbol	O
approach	O
.	O
the	O
key	O
diﬀerence	O
is	O
that	O
the	O
symbol-to-number	O
approach	O
does	O
not	O
expose	O
the	O
graph	O
.	O
6.5.6	O
general	O
back-propagation	O
the	O
back-propagation	O
algorithm	O
is	O
very	O
simple	O
.	O
to	O
compute	O
the	O
gradient	O
of	O
some	O
scalar	O
z	O
with	O
respect	O
to	O
one	O
of	O
its	O
ancestors	O
x	O
in	O
the	O
graph	O
,	O
we	O
begin	O
by	O
observing	O
that	O
the	O
gradient	O
with	O
respect	O
to	O
z	O
is	O
given	O
by	O
dz	O
=	O
1.	O
we	O
can	O
then	O
compute	O
dz	O
the	O
gradient	O
with	O
respect	O
to	O
each	O
parent	O
of	O
z	O
in	O
the	O
graph	O
by	O
multiplying	O
the	O
current	O
gradient	O
by	O
the	O
jacobian	O
of	O
the	O
operation	O
that	O
produced	O
z.	O
we	O
continue	O
multiplying	O
by	O
jacobians	O
traveling	O
backwards	O
through	O
the	O
graph	O
in	O
this	O
way	O
until	O
we	O
reach	O
x.	O
for	O
any	O
node	O
that	O
may	O
be	O
reached	O
by	O
going	O
backwards	O
from	O
z	O
through	O
two	O
or	O
more	O
paths	O
,	O
we	O
simply	O
sum	O
the	O
gradients	O
arriving	O
from	O
diﬀerent	O
paths	O
at	O
that	O
node	O
.	O
more	O
formally	O
,	O
each	O
node	O
in	O
the	O
graph	O
corresponds	O
to	O
a	O
variable	O
.	O
to	O
achieve	O
maximum	O
generality	O
,	O
we	O
describe	O
this	O
variable	O
as	O
being	O
a	O
tensor	O
v.	O
tensor	O
can	O
in	O
general	O
have	O
any	O
number	O
of	O
dimensions	O
.	O
they	O
subsume	O
scalars	O
,	O
vectors	O
,	O
and	O
matrices	O
.	O
g	O
we	O
assume	O
that	O
each	O
variable	O
•	O
is	O
associated	O
with	O
the	O
following	O
subroutines	O
:	O
v	O
_	O
(	O
v	O
)	O
:	O
this	O
returns	O
the	O
operation	O
that	O
computes	O
v	O
,	O
repre-	O
get	O
operation	O
sented	O
by	O
the	O
edges	O
coming	O
into	O
v	O
in	O
the	O
computational	O
graph	O
.	O
for	O
example	O
,	O
there	O
may	O
be	O
a	O
python	O
or	O
c++	O
class	O
representing	O
the	O
matrix	O
multiplication	O
operation	O
,	O
and	O
the	O
get_operation	O
function	O
.	O
suppose	O
we	O
have	O
a	O
variable	O
that	O
is	O
created	O
by	O
matrix	O
multiplication	O
,	O
c	O
=	O
ab	O
.	O
then	O
get	O
operation	O
(	O
v	O
)	O
returns	O
a	O
pointer	O
to	O
an	O
instance	O
of	O
the	O
corresponding	O
c++	O
class	O
.	O
_	O
•	O
•	O
g	O
(	O
v	O
,	O
g	O
_	O
g	O
get	O
consumers	O
v	O
in	O
the	O
computational	O
graph	O
.	O
_	O
g	O
get	O
inputs	O
in	O
the	O
computational	O
graph	O
.	O
(	O
v	O
,	O
215	O
)	O
:	O
this	O
returns	O
the	O
list	O
of	O
variables	O
that	O
are	O
children	O
of	O
)	O
:	O
this	O
returns	O
the	O
list	O
of	O
variables	O
that	O
are	O
parents	O
of	O
v	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
each	O
operation	O
op	O
is	O
also	O
associated	O
with	O
a	O
bprop	O
operation	O
.	O
this	O
bprop	O
operation	O
can	O
compute	O
a	O
jacobian-vector	O
product	O
as	O
described	O
by	O
equation	O
6.47	O
.	O
this	O
is	O
how	O
the	O
back-propagation	O
algorithm	O
is	O
able	O
to	O
achieve	O
great	O
generality	O
.	O
each	O
operation	O
is	O
responsible	O
for	O
knowing	O
how	O
to	O
back-propagate	O
through	O
the	O
edges	O
in	O
the	O
graph	O
that	O
it	O
participates	O
in	O
.	O
for	O
example	O
,	O
we	O
might	O
use	O
a	O
matrix	O
multiplication	O
operation	O
to	O
create	O
a	O
variable	O
c	O
=	O
ab	O
.	O
suppose	O
that	O
the	O
gradient	O
of	O
a	O
scalar	O
z	O
with	O
respect	O
to	O
c	O
is	O
given	O
by	O
g.	O
the	O
matrix	O
multiplication	O
operation	O
is	O
responsible	O
for	O
deﬁning	O
two	O
back-propagation	O
rules	O
,	O
one	O
for	O
each	O
of	O
its	O
input	O
arguments	O
.	O
if	O
we	O
call	O
the	O
bprop	O
method	O
to	O
request	O
the	O
gradient	O
with	O
respect	O
to	O
a	O
given	O
that	O
the	O
gradient	O
on	O
the	O
output	O
is	O
g	O
,	O
then	O
the	O
bprop	O
method	O
of	O
the	O
matrix	O
multiplication	O
operation	O
must	O
state	O
that	O
the	O
gradient	O
with	O
respect	O
to	O
a	O
is	O
given	O
by	O
gb	O
.	O
likewise	O
,	O
if	O
we	O
call	O
the	O
bprop	O
method	O
to	O
request	O
the	O
gradient	O
with	O
respect	O
to	O
b	O
,	O
then	O
the	O
matrix	O
operation	O
is	O
responsible	O
for	O
implementing	O
the	O
	O
bprop	O
method	O
and	O
specifying	O
that	O
the	O
desired	O
gradient	O
is	O
given	O
by	O
a	O
g.	O
the	O
back-propagation	O
algorithm	O
itself	O
does	O
not	O
need	O
to	O
know	O
any	O
diﬀerentiation	O
rules	O
.	O
it	O
only	O
needs	O
to	O
call	O
each	O
operation	O
’	O
s	O
bprop	O
rules	O
with	O
the	O
right	O
arguments	O
.	O
formally	O
,	O
,	O
op	O
bprop	O
inputs	O
	O
,	O
x	O
g	O
)	O
must	O
return	O
	O
.	O
(	O
∇	O
(	O
op	O
f	O
inputs	O
.	O
(	O
)	O
i	O
)	O
gi	O
,	O
x	O
(	O
6.54	O
)	O
i	O
which	O
is	O
just	O
an	O
implementation	O
of	O
the	O
chain	O
rule	O
as	O
expressed	O
in	O
equation	O
6.47	O
.	O
here	O
,	O
inputs	O
is	O
a	O
list	O
of	O
inputs	O
that	O
are	O
supplied	O
to	O
the	O
operation	O
,	O
op.f	O
is	O
the	O
mathematical	O
function	O
that	O
the	O
operation	O
implements	O
,	O
x	O
is	O
the	O
input	O
whose	O
gradient	O
we	O
wish	O
to	O
compute	O
,	O
and	O
is	O
the	O
gradient	O
on	O
the	O
output	O
of	O
the	O
operation	O
.	O
g	O
the	O
op.bprop	O
method	O
should	O
always	O
pretend	O
that	O
all	O
of	O
its	O
inputs	O
are	O
distinct	O
from	O
each	O
other	O
,	O
even	O
if	O
they	O
are	O
not	O
.	O
for	O
example	O
,	O
if	O
the	O
mul	O
operator	O
is	O
passed	O
two	O
copies	O
of	O
x	O
to	O
compute	O
x2	O
,	O
the	O
op.bprop	O
method	O
should	O
still	O
return	O
x	O
as	O
the	O
derivative	O
with	O
respect	O
to	O
both	O
inputs	O
.	O
the	O
back-propagation	O
algorithm	O
will	O
later	O
add	O
both	O
of	O
these	O
arguments	O
together	O
to	O
obtain	O
2x	O
,	O
which	O
is	O
the	O
correct	O
total	O
derivative	O
on	O
.x	O
software	O
implementations	O
of	O
back-propagation	O
usually	O
provide	O
both	O
the	O
opera-	O
tions	O
and	O
their	O
bprop	O
methods	O
,	O
so	O
that	O
users	O
of	O
deep	O
learning	O
software	O
libraries	O
are	O
able	O
to	O
back-propagate	O
through	O
graphs	O
built	O
using	O
common	O
operations	O
like	O
matrix	O
multiplication	O
,	O
exponents	O
,	O
logarithms	O
,	O
and	O
so	O
on	O
.	O
software	O
engineers	O
who	O
build	O
a	O
new	O
implementation	O
of	O
back-propagation	O
or	O
advanced	O
users	O
who	O
need	O
to	O
add	O
their	O
own	O
operation	O
to	O
an	O
existing	O
library	O
must	O
usually	O
derive	O
the	O
op.bprop	O
method	O
for	O
any	O
new	O
operations	O
manually	O
.	O
the	O
back-propagation	O
algorithm	O
is	O
formally	O
described	O
in	O
algorithm	O
.6.5	O
216	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
build_grad	O
algorithm	O
6.5	O
the	O
outermost	O
skeleton	O
of	O
the	O
back-propagation	O
algorithm	O
.	O
this	O
portion	O
does	O
simple	O
setup	O
and	O
cleanup	O
work	B
.	O
most	O
of	O
the	O
important	O
work	B
happens	O
in	O
the	O
.	O
require	O
:	O
t	O
,	O
the	O
target	O
set	O
of	O
variables	O
whose	O
gradients	O
must	O
be	O
computed	O
.	O
require	O
:	O
g	O
require	O
:	O
z	O
,	O
the	O
variable	O
to	O
be	O
diﬀerentiated	O
g	O
,	O
the	O
computational	O
graph	O
g	O
subroutine	O
of	O
algorithm	O
pruned	O
to	O
contain	O
only	O
nodes	O
that	O
are	O
ancestors	O
of	O
z	O
and	O
descendents	O
6.6	O
grad_table	O
,	O
a	O
data	O
structure	O
associating	O
tensors	O
to	O
their	O
gradients	O
be	O
let	O
of	O
nodes	O
in	O
.t	O
initialize	O
grad	O
table	O
for	O
[	O
]	O
z	O
do	O
build	O
grad	O
_	O
v	O
in	O
t	O
_	O
1	O
g	O
g	O
,	O
,	O
grad	O
table	O
)	O
_	O
←	O
(	O
v	O
,	O
end	O
for	O
return	O
grad_table	O
restricted	O
to	O
t	O
6.5.2	O
in	O
section	O
,	O
we	O
explained	O
that	O
back-propagation	O
was	O
developed	O
in	O
order	O
to	O
avoid	O
computing	O
the	O
same	O
subexpression	O
in	O
the	O
chain	O
rule	O
multiple	O
times	O
.	O
the	O
naive	O
algorithm	O
could	O
have	O
exponential	O
runtime	O
due	O
to	O
these	O
repeated	O
subexpressions	O
.	O
now	O
that	O
we	O
have	O
speciﬁed	O
the	O
back-propagation	O
algorithm	O
,	O
we	O
can	O
understand	O
its	O
computational	O
cost	O
.	O
if	O
we	O
assume	O
that	O
each	O
operation	O
evaluation	O
has	O
roughly	O
the	O
same	O
cost	O
,	O
then	O
we	O
may	O
analyze	O
the	O
computational	O
cost	O
in	O
terms	O
of	O
the	O
number	O
of	O
operations	O
executed	O
.	O
keep	O
in	O
mind	O
here	O
that	O
we	O
refer	O
to	O
an	O
operation	O
as	O
the	O
fundamental	O
unit	O
of	O
our	O
computational	O
graph	O
,	O
which	O
might	O
actually	O
consist	O
of	O
very	O
many	O
arithmetic	O
operations	O
(	O
for	O
example	O
,	O
we	O
might	O
have	O
a	O
graph	O
that	O
treats	O
matrix	O
multiplication	O
as	O
a	O
single	O
operation	O
)	O
.	O
computing	O
a	O
gradient	O
in	O
a	O
graph	O
with	O
n	O
nodes	O
will	O
never	O
execute	O
more	O
than	O
o	O
(	O
n2	O
)	O
operations	O
or	O
store	O
the	O
output	O
of	O
more	O
than	O
o	O
(	O
n	O
2	O
)	O
operations	O
.	O
here	O
we	O
are	O
counting	O
operations	O
in	O
the	O
computational	O
graph	O
,	O
not	O
individual	O
operations	O
executed	O
by	O
the	O
underlying	O
hardware	O
,	O
so	O
it	O
is	O
important	O
to	O
remember	O
that	O
the	O
runtime	O
of	O
each	O
operation	O
may	O
be	O
highly	O
variable	O
.	O
for	O
example	O
,	O
multiplying	O
two	O
matrices	O
that	O
each	O
contain	O
millions	O
of	O
entries	O
might	O
correspond	O
to	O
a	O
single	O
operation	O
in	O
the	O
graph	O
.	O
we	O
can	O
see	O
that	O
computing	O
the	O
gradient	O
requires	O
as	O
most	O
o	O
(	O
n2	O
)	O
operations	O
because	O
the	O
forward	O
propagation	O
stage	O
will	O
at	O
worst	O
execute	O
all	O
n	O
nodes	O
in	O
the	O
original	O
graph	O
(	O
depending	O
on	O
which	O
values	O
we	O
want	O
to	O
compute	O
,	O
we	O
may	O
not	O
need	O
to	O
execute	O
the	O
entire	O
graph	O
)	O
.	O
the	O
back-propagation	O
algorithm	O
adds	O
one	O
jacobian-vector	O
product	O
,	O
which	O
should	O
be	O
expressed	O
with	O
o	O
(	O
1	O
)	O
nodes	O
,	O
per	O
edge	O
in	O
the	O
original	O
graph	O
.	O
because	O
the	O
computational	O
graph	O
is	O
a	O
directed	O
acyclic	O
graph	O
it	O
has	O
at	O
most	O
o	O
(	O
n2	O
)	O
edges	O
.	O
for	O
the	O
kinds	O
of	O
graphs	O
that	O
are	O
commonly	O
used	O
in	O
practice	O
,	O
the	O
situation	O
is	O
even	O
better	O
.	O
most	O
neural	O
network	O
cost	O
functions	O
are	O
217	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
g	O
g	O
,	O
_	O
)	O
of	O
algorithm	O
6.6	O
the	O
inner	O
loop	O
subroutine	O
build	O
grad	O
the	O
back-propagation	O
algorithm	O
,	O
called	O
by	O
the	O
back-propagation	O
algorithm	O
deﬁned	O
in	O
algorithm	O
.6.5	O
require	O
:	O
v	O
,	O
the	O
variable	O
whose	O
gradient	O
should	O
be	O
added	O
to	O
require	O
:	O
to	O
nodes	O
that	O
participate	O
in	O
the	O
gradient	O
.	O
require	O
:	O
require	O
:	O
grad_table	O
,	O
a	O
data	O
structure	O
mapping	O
nodes	O
to	O
their	O
gradients	O
g	O
g	O
g	O
,	O
the	O
graph	O
to	O
modify	O
.	O
,	O
the	O
restriction	O
of	O
,	O
grad	O
table	O
.	O
grad_table	O
(	O
v	O
,	O
and	O
g	O
_	O
if	O
_	O
then	O
]	O
v	O
v	O
is	O
in	O
grad_table	O
return	O
grad	O
table	O
[	O
←	O
end	O
if	O
1	O
i	O
←	O
in	O
for	O
c	O
←	O
op	O
←	O
←	O
g	O
i	O
←	O
_get	O
consumers	O
(	O
,	O
v	O
g	O
g	O
(	O
)	O
c	O
get	O
operation	O
,	O
(	O
,	O
build	O
grad	O
c	O
(	O
i	O
end	O
for	O
	O
+	O
1	O
_	O
_	O
_	O
.	O
d	O
(	O
)	O
i	O
op	O
bprop	O
get	O
inputs	O
(	O
)	O
i	O
i	O
g	O
g	O
_	O
[	O
]	O
=	O
v	O
grad	O
table	O
insert	O
g	O
return	O
g	O
g	O
g	O
)	O
do	O
)	O
,	O
grad	O
table	O
)	O
,	O
v	O
d	O
g	O
_	O
(	O
c	O
,	O
)	O
,	O
	O
and	O
the	O
operations	O
creating	O
it	O
into	O
g	O
roughly	O
chain-structured	O
,	O
causing	O
back-propagation	O
to	O
have	O
o	O
(	O
n	O
)	O
cost	O
.	O
this	O
is	O
far	O
better	O
than	O
the	O
naive	O
approach	O
,	O
which	O
might	O
need	O
to	O
execute	O
exponentially	O
many	O
nodes	O
.	O
this	O
potentially	O
exponential	O
cost	O
can	O
be	O
seen	O
by	O
expanding	O
and	O
rewriting	O
the	O
recursive	O
chain	O
rule	O
(	O
equation	O
)	O
non-recursively	O
:	O
	O
	O
6.49	O
∂u	O
(	O
)	O
n	O
∂u	O
(	O
)	O
j	O
=	O
t	O
path	O
(	O
u	O
(	O
π1	O
)	O
,	O
u	O
(	O
π2	O
)	O
,	O
...	O
,	O
u	O
(	O
π	O
t	O
)	O
)	O
,	O
k=2	O
from	O
π1=	O
toj	O
π	O
t=n	O
∂u	O
(	O
πk	O
)	O
−	O
∂u	O
(	O
πk	O
1	O
)	O
.	O
(	O
6.55	O
)	O
since	O
the	O
number	O
of	O
paths	O
from	O
node	O
j	O
to	O
node	O
n	O
can	O
grow	O
exponentially	O
in	O
the	O
length	O
of	O
these	O
paths	O
,	O
the	O
number	O
of	O
terms	O
in	O
the	O
above	O
sum	O
,	O
which	O
is	O
the	O
number	O
of	O
such	O
paths	O
,	O
can	O
grow	O
exponentially	O
with	O
the	O
depth	O
of	O
the	O
forward	O
propagation	O
graph	O
.	O
this	O
large	O
cost	O
would	O
be	O
incurred	O
because	O
the	O
same	O
computation	O
for	O
∂u	O
(	O
)	O
i	O
would	O
be	O
redone	O
many	O
times	O
.	O
to	O
avoid	O
such	O
recomputation	O
,	O
we	O
can	O
think	O
∂u	O
(	O
)	O
j	O
of	O
back-propagation	O
as	O
a	O
table-ﬁlling	O
algorithm	O
that	O
takes	O
advantage	O
of	O
storing	O
intermediate	O
results	O
∂u	O
(	O
)	O
n	O
.	O
each	O
node	O
in	O
the	O
graph	O
has	O
a	O
corresponding	O
slot	O
in	O
a	O
∂u	O
(	O
)	O
i	O
table	O
to	O
store	O
the	O
gradient	O
for	O
that	O
node	O
.	O
by	O
ﬁlling	O
in	O
these	O
table	O
entries	O
in	O
order	O
,	O
218	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
back-propagation	O
avoids	O
repeating	O
many	O
common	O
subexpressions	O
.	O
this	O
table-ﬁlling	O
strategy	O
is	O
sometimes	O
called	O
.	O
dynamic	O
programming	O
6.5.7	O
example	O
:	O
back-propagation	O
for	O
mlp	O
training	O
as	O
an	O
example	O
,	O
we	O
walk	O
through	O
the	O
back-propagation	O
algorithm	O
as	O
it	O
is	O
used	O
to	O
train	O
a	O
multilayer	O
perceptron	O
.	O
here	O
we	O
develop	O
a	O
very	O
simple	O
multilayer	O
perception	O
with	O
a	O
single	O
hidden	O
layer	O
.	O
to	O
train	O
this	O
model	B
,	O
we	O
will	O
use	O
minibatch	O
stochastic	O
gradient	O
descent	B
.	O
the	O
back-propagation	O
algorithm	O
is	O
used	O
to	O
compute	O
the	O
gradient	O
of	O
the	O
cost	O
on	O
a	O
single	O
minibatch	O
.	O
speciﬁcally	O
,	O
we	O
use	O
a	O
minibatch	O
of	O
examples	O
from	O
the	O
training	O
{	O
}	O
set	O
formatted	O
as	O
a	O
design	O
matrix	O
x	O
and	O
a	O
vector	O
of	O
associated	O
class	O
labels	O
y	O
.	O
0	O
,	O
xw	O
(	O
1	O
)	O
the	O
network	O
computes	O
a	O
layer	O
of	O
hidden	O
features	O
h	O
=	O
max	O
.	O
to	O
{	O
simplify	O
the	O
presentation	O
we	O
do	O
not	O
use	O
biases	O
in	O
this	O
model	B
.	O
we	O
assume	O
that	O
our	O
graph	O
language	O
includes	O
a	O
relu	O
operation	O
that	O
can	O
compute	O
max	O
element-	O
wise	O
.	O
the	O
predictions	O
of	O
the	O
unnormalized	O
log	O
probabilities	O
over	O
classes	O
are	O
then	O
given	O
by	O
hw	O
(	O
2	O
)	O
.	O
we	O
assume	O
that	O
our	O
graph	O
language	O
includes	O
a	O
cross_entropy	O
operation	O
that	O
computes	O
the	O
cross-entropy	O
between	O
the	O
targets	O
y	O
and	O
the	O
probability	O
distribution	O
deﬁned	O
by	O
these	O
unnormalized	O
log	O
probabilities	O
.	O
the	O
resulting	O
cross-	O
entropy	O
deﬁnes	O
the	O
cost	O
jmle	O
.	O
minimizing	O
this	O
cross-entropy	O
performs	O
maximum	O
likelihood	O
estimation	O
of	O
the	O
classiﬁer	O
.	O
however	O
,	O
to	O
make	O
this	O
example	O
more	O
realistic	O
,	O
we	O
also	O
include	O
a	O
regularization	O
term	O
.	O
the	O
total	O
cost	O
	O
	O
	O
	O
}	O
0	O
,	O
z	O
	O
	O
	O
j	O
j=	O
mle	O
+	O
λ	O
w	O
(	O
1	O
)	O
i	O
,	O
j	O
2	O
+	O
2	O
w	O
(	O
2	O
)	O
i	O
,	O
j	O
i	O
,	O
j	O
i	O
,	O
j	O
(	O
6.56	O
)	O
consists	O
of	O
the	O
cross-entropy	O
and	O
a	O
weight	O
decay	O
term	O
with	O
coeﬃcient	O
λ.	O
the	O
computational	O
graph	O
is	O
illustrated	O
in	O
ﬁgure	O
.	O
6.11	O
the	O
computational	O
graph	O
for	O
the	O
gradient	O
of	O
this	O
example	O
is	O
large	O
enough	O
that	O
it	O
would	O
be	O
tedious	O
to	O
draw	O
or	O
to	O
read	O
.	O
this	O
demonstrates	O
one	O
of	O
the	O
beneﬁts	O
of	O
the	O
back-propagation	O
algorithm	O
,	O
which	O
is	O
that	O
it	O
can	O
automatically	O
generate	O
gradients	O
that	O
would	O
be	O
straightforward	O
but	O
tedious	O
for	O
a	O
software	O
engineer	O
to	O
derive	O
manually	O
.	O
we	O
can	O
roughly	O
trace	O
out	O
the	O
behavior	O
of	O
the	O
back-propagation	O
algorithm	O
∇	O
.	O
to	O
train	O
,	O
we	O
wish	O
by	O
looking	O
at	O
the	O
forward	O
propagation	O
graph	O
in	O
ﬁgure	O
w	O
(	O
2	O
)	O
j.	O
there	O
are	O
two	O
diﬀerent	O
paths	O
leading	O
to	O
compute	O
both	O
backward	O
from	O
j	O
to	O
the	O
weights	O
:	O
one	O
through	O
the	O
cross-entropy	O
cost	O
,	O
and	O
one	O
through	O
the	O
weight	O
decay	O
cost	O
.	O
the	O
weight	O
decay	O
cost	O
is	O
relatively	O
simple	O
;	O
it	O
will	O
always	O
contribute	O
2λw	O
(	O
)	O
i	O
to	O
the	O
gradient	O
on	O
w	O
(	O
)	O
i	O
.	O
∇	O
w	O
(	O
1	O
)	O
j	O
and	O
6.11	O
219	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
j	O
mle	O
j	O
mle	O
cross_entropy	O
jj	O
+	O
u	O
(	O
2	O
)	O
u	O
(	O
2	O
)	O
yy	O
u	O
(	O
8	O
)	O
u	O
(	O
8	O
)	O
×	O
matmul	O
relu	O
hh	O
w	O
(	O
2	O
)	O
w	O
(	O
2	O
)	O
sqr	O
u	O
(	O
5	O
)	O
u	O
(	O
5	O
)	O
sum	O
u	O
(	O
6	O
)	O
u	O
(	O
6	O
)	O
u	O
(	O
7	O
)	O
u	O
(	O
7	O
)	O
λλ	O
+	O
u	O
(	O
1	O
)	O
u	O
(	O
1	O
)	O
matmul	O
xx	O
w	O
(	O
1	O
)	O
w	O
(	O
1	O
)	O
sqr	O
u	O
(	O
3	O
)	O
u	O
(	O
3	O
)	O
sum	O
u	O
(	O
4	O
)	O
u	O
(	O
4	O
)	O
figure	O
6.11	O
:	O
the	O
computational	O
graph	O
used	O
to	O
compute	O
the	O
cost	O
used	O
to	O
train	O
our	O
example	O
of	O
a	O
single-layer	O
mlp	O
using	O
the	O
cross-entropy	O
loss	O
and	O
weight	O
decay	O
.	O
the	O
other	O
path	O
through	O
the	O
cross-entropy	O
cost	O
is	O
slightly	O
more	O
complicated	O
.	O
let	O
g	O
be	O
the	O
gradient	O
on	O
the	O
unnormalized	O
log	O
probabilities	O
u	O
(	O
2	O
)	O
provided	O
by	O
the	O
cross_entropy	O
operation	O
.	O
the	O
back-propagation	O
algorithm	O
now	O
needs	O
to	O
explore	O
two	O
diﬀerent	O
branches	O
.	O
on	O
the	O
shorter	O
branch	O
,	O
it	O
adds	O
h	O
g	O
to	O
the	O
gradient	O
on	O
w	O
(	O
2	O
)	O
,	O
using	O
the	O
back-propagation	O
rule	O
for	O
the	O
second	O
argument	O
to	O
the	O
matrix	O
multiplication	O
operation	O
.	O
the	O
other	O
branch	O
corresponds	O
to	O
the	O
longer	O
chain	O
descending	O
further	O
along	O
the	O
network	O
.	O
first	O
,	O
the	O
back-propagation	O
algorithm	O
computes	O
using	O
the	O
back-propagation	O
rule	O
for	O
the	O
ﬁrst	O
argument	O
to	O
the	O
matrix	O
multiplication	O
operation	O
.	O
next	O
,	O
the	O
relu	O
operation	O
uses	O
its	O
back-	O
propagation	O
rule	O
to	O
zero	O
out	O
components	O
of	O
the	O
gradient	O
corresponding	O
to	O
entries	O
of	O
u	O
(	O
1	O
)	O
that	O
were	O
less	O
than	O
.	O
let	O
the	O
result	O
be	O
called	O
.	O
the	O
last	O
step	O
of	O
the	O
back-propagation	O
algorithm	O
is	O
to	O
use	O
the	O
back-propagation	O
rule	O
for	O
the	O
second	O
argument	O
of	O
the	O
∇	O
	O
hj	O
=	O
gw	O
(	O
2	O
)	O
to	O
the	O
gradient	O
on	O
w	O
(	O
1	O
)	O
.	O
	O
g	O
	O
matmul	O
operation	O
to	O
add	O
x	O
g	O
0	O
	O
	O
after	O
these	O
gradients	O
have	O
been	O
computed	O
,	O
it	O
is	O
the	O
responsibility	O
of	O
the	O
gradient	O
descent	B
algorithm	O
,	O
or	O
another	O
optimization	O
algorithm	O
,	O
to	O
use	O
these	O
gradients	O
to	O
update	O
the	O
parameters	O
.	O
for	O
the	O
mlp	O
,	O
the	O
computational	O
cost	O
is	O
dominated	O
by	O
the	O
cost	O
of	O
matrix	O
multiplication	O
.	O
during	O
the	O
forward	O
propagation	O
stage	O
,	O
we	O
multiply	O
by	O
each	O
weight	O
220	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
matrix	O
,	O
resulting	O
in	O
o	O
(	O
w	O
)	O
multiply-adds	O
,	O
where	O
w	O
is	O
the	O
number	O
of	O
weights	O
.	O
during	O
the	O
backward	O
propagation	O
stage	O
,	O
we	O
multiply	O
by	O
the	O
transpose	O
of	O
each	O
weight	O
matrix	O
,	O
which	O
has	O
the	O
same	O
computational	O
cost	O
.	O
the	O
main	O
memory	O
cost	O
of	O
the	O
algorithm	O
is	O
that	O
we	O
need	O
to	O
store	O
the	O
input	O
to	O
the	O
nonlinearity	O
of	O
the	O
hidden	O
layer	O
.	O
this	O
value	O
is	O
stored	O
from	O
the	O
time	O
it	O
is	O
computed	O
until	O
the	O
backward	O
pass	O
has	O
returned	O
to	O
the	O
same	O
point	O
.	O
the	O
memory	O
cost	O
is	O
thus	O
o	O
(	O
mnh	O
)	O
,	O
where	O
m	O
is	O
the	O
number	O
of	O
examples	O
in	O
the	O
minibatch	O
and	O
nh	O
is	O
the	O
number	O
of	O
hidden	O
units	O
.	O
6.5.8	O
complications	O
our	O
description	O
of	O
the	O
back-propagation	O
algorithm	O
here	O
is	O
simpler	O
than	O
the	O
imple-	O
mentations	O
actually	O
used	O
in	O
practice	O
.	O
as	O
noted	O
above	O
,	O
we	O
have	O
restricted	O
the	O
deﬁnition	O
of	O
an	O
operation	O
to	O
be	O
a	O
function	O
that	O
returns	O
a	O
single	O
tensor	O
.	O
most	O
software	O
implementations	O
need	O
to	O
support	O
operations	O
that	O
can	O
return	O
more	O
than	O
one	O
tensor	O
.	O
for	O
example	O
,	O
if	O
we	O
wish	O
to	O
compute	O
both	O
the	O
maximum	O
value	O
in	O
a	O
tensor	O
and	O
the	O
index	O
of	O
that	O
value	O
,	O
it	O
is	O
best	O
to	O
compute	O
both	O
in	O
a	O
single	O
pass	O
through	O
memory	O
,	O
so	O
it	O
is	O
most	O
eﬃcient	O
to	O
implement	O
this	O
procedure	O
as	O
a	O
single	O
operation	O
with	O
two	O
outputs	O
.	O
we	O
have	O
not	O
described	O
how	O
to	O
control	O
the	O
memory	O
consumption	O
of	O
back-	O
propagation	O
.	O
back-propagation	O
often	O
involves	O
summation	O
of	O
many	O
tensors	O
together	O
.	O
in	O
the	O
naive	O
approach	O
,	O
each	O
of	O
these	O
tensors	O
would	O
be	O
computed	O
separately	O
,	O
then	O
all	O
of	O
them	O
would	O
be	O
added	O
in	O
a	O
second	O
step	O
.	O
the	O
naive	O
approach	O
has	O
an	O
overly	O
high	O
memory	O
bottleneck	O
that	O
can	O
be	O
avoided	O
by	O
maintaining	O
a	O
single	O
buﬀer	O
and	O
adding	O
each	O
value	O
to	O
that	O
buﬀer	O
as	O
it	O
is	O
computed	O
.	O
real-world	O
implementations	O
of	O
back-propagation	O
also	O
need	O
to	O
handle	O
various	O
data	O
types	O
,	O
such	O
as	O
32-bit	O
ﬂoating	O
point	O
,	O
64-bit	O
ﬂoating	O
point	O
,	O
and	O
integer	O
values	O
.	O
the	O
policy	O
for	O
handling	O
each	O
of	O
these	O
types	O
takes	O
special	O
care	O
to	O
design	O
.	O
some	O
operations	O
have	O
undeﬁned	O
gradients	O
,	O
and	O
it	O
is	O
important	O
to	O
track	O
these	O
cases	O
and	O
determine	O
whether	O
the	O
gradient	O
requested	O
by	O
the	O
user	O
is	O
undeﬁned	O
.	O
various	O
other	O
technicalities	O
make	O
real-world	O
diﬀerentiation	O
more	O
complicated	O
.	O
these	O
technicalities	O
are	O
not	O
insurmountable	O
,	O
and	O
this	O
chapter	O
has	O
described	O
the	O
key	O
intellectual	O
tools	O
needed	O
to	O
compute	O
derivatives	O
,	O
but	O
it	O
is	O
important	O
to	O
be	O
aware	O
that	O
many	O
more	O
subtleties	O
exist	O
.	O
6.5.9	O
diﬀerentiation	O
outside	O
the	O
deep	O
learning	O
community	O
the	O
deep	O
learning	O
community	O
has	O
been	O
somewhat	O
isolated	O
from	O
the	O
broader	O
computer	O
science	O
community	O
and	O
has	O
largely	O
developed	O
its	O
own	O
cultural	O
attitudes	O
221	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
concerning	O
how	O
to	O
perform	O
diﬀerentiation	O
.	O
more	O
generally	O
,	O
the	O
ﬁeld	O
of	O
automatic	O
diﬀerentiation	O
is	O
concerned	O
with	O
how	O
to	O
compute	O
derivatives	O
algorithmically	O
.	O
the	O
back-propagation	O
algorithm	O
described	O
here	O
is	O
only	O
one	O
approach	O
to	O
automatic	O
diﬀerentiation	O
.	O
it	O
is	O
a	O
special	O
case	O
of	O
a	O
broader	O
class	O
of	O
techniques	O
called	O
reverse	O
mode	O
accumulation	O
.	O
other	O
approaches	O
evaluate	O
the	O
subexpressions	O
of	O
the	O
chain	O
rule	O
in	O
diﬀerent	O
orders	O
.	O
in	O
general	O
,	O
determining	O
the	O
order	O
of	O
evaluation	O
that	O
results	O
in	O
the	O
lowest	O
computational	O
cost	O
is	O
a	O
diﬃcult	O
problem	O
.	O
finding	O
the	O
optimal	O
sequence	O
of	O
operations	O
to	O
compute	O
the	O
gradient	O
is	O
np-complete	O
(	O
)	O
,	O
in	O
the	O
sense	O
that	O
it	O
may	O
require	O
simplifying	O
algebraic	O
expressions	O
into	O
their	O
least	O
expensive	O
form	O
.	O
naumann	O
2008	O
,	O
for	O
example	O
,	O
suppose	O
we	O
have	O
variables	O
p1	O
,	O
p2	O
,	O
.	O
.	O
.	O
,	O
pn	O
representing	O
probabilities	O
and	O
variables	O
z1	O
,	O
z2	O
,	O
.	O
.	O
.	O
,	O
zn	O
representing	O
unnormalized	O
log	O
probabilities	O
.	O
suppose	O
we	O
deﬁne	O
	O
	O
(	O
6.57	O
)	O
qi	O
=	O
exp	O
(	O
zi	O
)	O
i	O
exp	O
(	O
zi	O
)	O
,	O
g	O
−	O
−	O
where	O
we	O
build	O
the	O
softmax	O
function	O
out	O
of	O
exponentiation	O
,	O
summation	O
and	O
division	O
i	O
pi	O
log	O
qi	O
.	O
a	O
human	O
operations	O
,	O
and	O
construct	O
a	O
cross-entropy	O
loss	O
j	O
=	O
mathematician	O
can	O
observe	O
that	O
the	O
derivative	O
of	O
j	O
with	O
respect	O
to	O
zi	O
takes	O
a	O
very	O
pi	O
.	O
the	O
back-propagation	O
algorithm	O
is	O
not	O
capable	O
of	O
simplifying	O
simple	O
form	O
:	O
qi	O
the	O
gradient	O
this	O
way	O
,	O
and	O
will	O
instead	O
explicitly	O
propagate	O
gradients	O
through	O
all	O
of	O
the	O
logarithm	O
and	O
exponentiation	O
operations	O
in	O
the	O
original	O
graph	O
.	O
some	O
software	O
libraries	O
such	O
as	O
theano	O
(	O
)	O
are	O
able	O
to	O
perform	O
some	O
kinds	O
of	O
algebraic	O
substitution	O
to	O
improve	O
over	O
the	O
graph	O
proposed	O
by	O
the	O
pure	O
back-propagation	O
algorithm	O
.	O
bergstra	O
et	O
al	O
.	O
2010	O
bastien	O
et	O
al	O
.	O
2012	O
,	O
;	O
,	O
6.2	O
when	O
the	O
forward	O
graph	O
because	O
each	O
local	O
partial	O
derivative	O
has	O
a	O
single	O
output	O
node	O
and	O
each	O
partial	O
derivative	O
∂u	O
(	O
)	O
i	O
can	O
be	O
computed	O
with	O
a	O
constant	O
amount	O
of	O
computation	O
,	O
back-propagation	O
∂u	O
(	O
)	O
j	O
guarantees	O
that	O
the	O
number	O
of	O
computations	O
for	O
the	O
gradient	O
computation	O
is	O
of	O
the	O
same	O
order	O
as	O
the	O
number	O
of	O
computations	O
for	O
the	O
forward	O
computation	O
:	O
this	O
can	O
be	O
seen	O
in	O
algorithm	O
needs	O
to	O
be	O
computed	O
only	O
once	O
along	O
with	O
an	O
associated	O
multiplication	O
and	O
addition	O
for	O
the	O
recursive	O
chain-rule	O
formulation	O
(	O
equation	O
)	O
.	O
the	O
overall	O
computation	O
is	O
therefore	O
o	O
(	O
#	O
edges	O
)	O
.	O
however	O
,	O
it	O
can	O
potentially	O
be	O
reduced	O
by	O
simplifying	O
the	O
computational	O
graph	O
constructed	O
by	O
back-propagation	O
,	O
and	O
this	O
is	O
an	O
np-complete	O
task	O
.	O
implementations	O
such	O
as	O
theano	O
and	O
tensorflow	O
use	O
heuristics	O
based	O
on	O
matching	O
known	O
simpliﬁcation	O
patterns	O
in	O
order	O
to	O
iteratively	O
attempt	O
to	O
simplify	O
the	O
graph	O
.	O
we	O
deﬁned	O
back-propagation	O
only	O
for	O
the	O
computation	O
of	O
a	O
gradient	O
of	O
a	O
scalar	O
output	O
but	O
back-propagation	O
can	O
be	O
extended	O
to	O
compute	O
a	O
jacobian	O
(	O
either	O
of	O
k	O
diﬀerent	O
scalar	O
nodes	O
in	O
the	O
graph	O
,	O
or	O
of	O
a	O
tensor-valued	O
node	O
containing	O
k	O
values	O
)	O
.	O
a	O
naive	O
implementation	O
may	O
then	O
need	O
k	O
times	O
more	O
computation	O
:	O
for	O
∂u	O
(	O
)	O
i	O
∂u	O
(	O
)	O
j	O
6.49	O
222	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
each	O
scalar	O
internal	O
node	O
in	O
the	O
original	O
forward	O
graph	O
,	O
the	O
naive	O
implementation	O
computes	O
k	O
gradients	O
instead	O
of	O
a	O
single	O
gradient	O
.	O
when	O
the	O
number	O
of	O
outputs	O
of	O
the	O
graph	O
is	O
larger	O
than	O
the	O
number	O
of	O
inputs	O
,	O
it	O
is	O
sometimes	O
preferable	O
to	O
use	O
another	O
form	O
of	O
automatic	O
diﬀerentiation	O
called	O
forward	O
mode	O
accumulation	O
.	O
forward	O
mode	O
computation	O
has	O
been	O
proposed	O
for	O
obtaining	O
real-time	O
computation	O
of	O
gradients	O
in	O
recurrent	O
networks	O
,	O
for	O
example	O
(	O
)	O
.	O
this	O
also	O
avoids	O
the	O
need	O
to	O
store	O
the	O
values	O
and	O
gradients	O
for	O
the	O
whole	O
graph	O
,	O
trading	O
oﬀ	O
computational	O
eﬃciency	O
for	O
memory	O
.	O
the	O
relationship	O
between	O
forward	O
mode	O
and	O
backward	O
mode	O
is	O
analogous	O
to	O
the	O
relationship	O
between	O
left-multiplying	O
versus	O
right-multiplying	O
a	O
sequence	O
of	O
matrices	O
,	O
such	O
as	O
williams	O
and	O
zipser	O
1989	O
,	O
abcd	O
,	O
(	O
6.58	O
)	O
where	O
the	O
matrices	O
can	O
be	O
thought	O
of	O
as	O
jacobian	O
matrices	O
.	O
for	O
example	O
,	O
if	O
d	O
is	O
a	O
column	O
vector	O
while	O
a	O
has	O
many	O
rows	O
,	O
this	O
corresponds	O
to	O
a	O
graph	O
with	O
a	O
single	O
output	O
and	O
many	O
inputs	O
,	O
and	O
starting	O
the	O
multiplications	O
from	O
the	O
end	O
and	O
going	O
backwards	O
only	O
requires	O
matrix-vector	O
products	O
.	O
this	O
corresponds	O
to	O
the	O
backward	O
mode	O
.	O
instead	O
,	O
starting	O
to	O
multiply	O
from	O
the	O
left	O
would	O
involve	O
a	O
series	O
of	O
matrix-matrix	O
products	O
,	O
which	O
makes	O
the	O
whole	O
computation	O
much	O
more	O
expensive	O
.	O
however	O
,	O
if	O
a	O
has	O
fewer	O
rows	O
than	O
d	O
has	O
columns	O
,	O
it	O
is	O
cheaper	O
to	O
run	O
the	O
multiplications	O
left-to-right	O
,	O
corresponding	O
to	O
the	O
forward	O
mode	O
.	O
in	O
many	O
communities	O
outside	O
of	O
machine	O
learning	O
,	O
it	O
is	O
more	O
common	O
to	O
im-	O
plement	O
diﬀerentiation	O
software	O
that	O
acts	O
directly	O
on	O
traditional	O
programming	O
language	O
code	O
,	O
such	O
as	O
python	O
or	O
c	O
code	O
,	O
and	O
automatically	O
generates	O
programs	O
that	O
diﬀerentiate	O
functions	O
written	O
in	O
these	O
languages	O
.	O
in	O
the	O
deep	O
learning	O
com-	O
munity	O
,	O
computational	O
graphs	O
are	O
usually	O
represented	O
by	O
explicit	O
data	O
structures	O
created	O
by	O
specialized	O
libraries	O
.	O
the	O
specialized	O
approach	O
has	O
the	O
drawback	O
of	O
requiring	O
the	O
library	O
developer	O
to	O
deﬁne	O
the	O
bprop	O
methods	O
for	O
every	O
operation	O
and	O
limiting	O
the	O
user	O
of	O
the	O
library	O
to	O
only	O
those	O
operations	O
that	O
have	O
been	O
deﬁned	O
.	O
however	O
,	O
the	O
specialized	O
approach	O
also	O
has	O
the	O
beneﬁt	O
of	O
allowing	O
customized	O
back-propagation	O
rules	O
to	O
be	O
developed	O
for	O
each	O
operation	O
,	O
allowing	O
the	O
developer	O
to	O
improve	O
speed	O
or	O
stability	O
in	O
non-obvious	O
ways	O
that	O
an	O
automatic	O
procedure	O
would	O
presumably	O
be	O
unable	O
to	O
replicate	O
.	O
back-propagation	O
is	O
therefore	O
not	O
the	O
only	O
way	O
or	O
the	O
optimal	O
way	O
of	O
computing	O
the	O
gradient	O
,	O
but	O
it	O
is	O
a	O
very	O
practical	O
method	O
that	O
continues	O
to	O
serve	O
the	O
deep	O
learning	O
community	O
very	O
well	O
.	O
in	O
the	O
future	O
,	O
diﬀerentiation	O
technology	O
for	O
deep	O
networks	O
may	O
improve	O
as	O
deep	O
learning	O
practitioners	O
become	O
more	O
aware	O
of	O
advances	O
in	O
the	O
broader	O
ﬁeld	O
of	O
automatic	O
diﬀerentiation	O
.	O
223	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
6.5.10	O
higher-order	O
derivatives	O
some	O
software	O
frameworks	O
support	O
the	O
use	O
of	O
higher-order	O
derivatives	O
.	O
among	O
the	O
deep	O
learning	O
software	O
frameworks	O
,	O
this	O
includes	O
at	O
least	O
theano	O
and	O
tensorflow	O
.	O
these	O
libraries	O
use	O
the	O
same	O
kind	O
of	O
data	O
structure	O
to	O
describe	O
the	O
expressions	O
for	O
derivatives	O
as	O
they	O
use	O
to	O
describe	O
the	O
original	O
function	O
being	O
diﬀerentiated	O
.	O
this	O
means	O
that	O
the	O
symbolic	O
diﬀerentiation	O
machinery	O
can	O
be	O
applied	O
to	O
derivatives	O
.	O
in	O
the	O
context	O
of	O
deep	O
learning	O
,	O
it	O
is	O
rare	O
to	O
compute	O
a	O
single	O
second	O
derivative	O
×	O
of	O
a	O
scalar	O
function	O
.	O
instead	O
,	O
we	O
are	O
usually	O
interested	O
in	O
properties	O
of	O
the	O
hessian	O
n	O
matrix	O
.	O
if	O
we	O
have	O
a	O
function	O
f	O
:	O
r	O
r	O
,	O
then	O
the	O
hessian	O
matrix	O
is	O
of	O
size	O
n	O
n	O
.	O
in	O
typical	O
deep	O
learning	O
applications	O
,	O
n	O
will	O
be	O
the	O
number	O
of	O
parameters	O
in	O
the	O
model	B
,	O
which	O
could	O
easily	O
number	O
in	O
the	O
billions	O
.	O
the	O
entire	O
hessian	O
matrix	O
is	O
thus	O
infeasible	O
to	O
even	O
represent	O
.	O
→	O
instead	O
of	O
explicitly	O
computing	O
the	O
hessian	O
,	O
the	O
typical	O
deep	O
learning	O
approach	O
is	O
to	O
use	O
krylov	O
methods	O
.	O
krylov	O
methods	O
are	O
a	O
set	O
of	O
iterative	O
techniques	O
for	O
performing	O
various	O
operations	O
like	O
approximately	O
inverting	O
a	O
matrix	O
or	O
ﬁnding	O
approximations	O
to	O
its	O
eigenvectors	O
or	O
eigenvalues	O
,	O
without	O
using	O
any	O
operation	O
other	O
than	O
matrix-vector	O
products	O
.	O
in	O
order	O
to	O
use	O
krylov	O
methods	O
on	O
the	O
hessian	O
,	O
we	O
only	O
need	O
to	O
be	O
able	O
to	O
compute	O
the	O
product	O
between	O
the	O
hessian	O
matrix	O
h	O
and	O
an	O
arbitrary	O
vector	O
v.	O
a	O
straightforward	O
technique	O
(	O
)	O
for	O
doing	O
so	O
is	O
to	O
compute	O
christianson	O
1992	O
	O
	O
∇	O
hv	O
=	O
x	O
,	O
∇	O
xf	O
x	O
(	O
)	O
)	O
(	O
	O
v	O
.	O
(	O
6.59	O
)	O
both	O
of	O
the	O
gradient	O
computations	O
in	O
this	O
expression	O
may	O
be	O
computed	O
automati-	O
cally	O
by	O
the	O
appropriate	O
software	O
library	O
.	O
note	O
that	O
the	O
outer	O
gradient	O
expression	O
takes	O
the	O
gradient	O
of	O
a	O
function	O
of	O
the	O
inner	O
gradient	O
expression	O
.	O
if	O
v	O
is	O
itself	O
a	O
vector	O
produced	O
by	O
a	O
computational	O
graph	O
,	O
it	O
is	O
important	O
to	O
specify	O
that	O
the	O
automatic	O
diﬀerentiation	O
software	O
should	O
not	O
diﬀerentiate	O
through	O
the	O
graph	O
that	O
produced	O
.v	O
while	O
computing	O
the	O
hessian	O
is	O
usually	O
not	O
advisable	O
,	O
it	O
is	O
possible	O
to	O
do	O
with	O
for	O
all	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
where	O
hessian	O
vector	O
products	O
.	O
one	O
simply	O
computes	O
he	O
(	O
)	O
i	O
e	O
(	O
)	O
i	O
is	O
the	O
one-hot	O
vector	O
with	O
e	O
(	O
)	O
i	O
i	O
=	O
1	O
and	O
all	O
other	O
entries	O
equal	O
to	O
0	O
.	O
6.6	O
historical	O
notes	O
feedforward	O
networks	O
can	O
be	O
seen	O
as	O
eﬃcient	O
nonlinear	O
function	O
approximators	O
based	O
on	O
using	O
gradient	O
descent	B
to	O
minimize	O
the	O
error	O
in	O
a	O
function	O
approximation	O
.	O
224	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
from	O
this	O
point	O
of	O
view	O
,	O
the	O
modern	O
feedforward	O
network	O
is	O
the	O
culmination	O
of	O
centuries	O
of	O
progress	O
on	O
the	O
general	O
function	O
approximation	O
task	O
.	O
the	O
chain	O
rule	O
that	O
underlies	O
the	O
back-propagation	O
algorithm	O
was	O
invented	O
)	O
.	O
calculus	O
and	O
algebra	O
have	O
in	O
the	O
17th	O
century	O
(	O
long	O
been	O
used	O
to	O
solve	O
optimization	O
problems	O
in	O
closed	O
form	O
,	O
but	O
gradient	O
descent	B
was	O
not	O
introduced	O
as	O
a	O
technique	O
for	O
iteratively	O
approximating	O
the	O
solution	O
to	O
optimization	O
problems	O
until	O
the	O
19th	O
century	O
(	O
cauchy	O
1847	O
leibniz	O
1676	O
l	O
’	O
hôpital	O
1696	O
)	O
.	O
,	O
;	O
,	O
,	O
beginning	O
in	O
the	O
1940s	O
,	O
these	O
function	O
approximation	O
techniques	O
were	O
used	O
to	O
motivate	O
machine	O
learning	O
models	O
such	O
as	O
the	O
perceptron	O
.	O
however	O
,	O
the	O
earliest	O
models	O
were	O
based	O
on	O
linear	O
models	O
.	O
critics	O
including	O
marvin	O
minsky	O
pointed	O
out	O
several	O
of	O
the	O
ﬂaws	O
of	O
the	O
linear	O
model	B
family	O
,	O
such	O
as	O
its	O
inability	O
to	O
learn	O
the	O
xor	O
function	O
,	O
which	O
led	O
to	O
a	O
backlash	O
against	O
the	O
entire	O
neural	O
network	O
approach	O
.	O
,	O
;	O
,	O
;	O
,	O
;	O
,	O
;	O
,	O
)	O
.	O
,	O
,	O
linnainmaa	O
1976	O
werbos	O
1981	O
learning	O
nonlinear	O
functions	O
required	O
the	O
development	O
of	O
a	O
multilayer	O
per-	O
ceptron	O
and	O
a	O
means	O
of	O
computing	O
the	O
gradient	O
through	O
such	O
a	O
model	B
.	O
eﬃcient	O
applications	O
of	O
the	O
chain	O
rule	O
based	O
on	O
dynamic	O
programming	O
began	O
to	O
appear	O
in	O
the	O
1960s	O
and	O
1970s	O
,	O
mostly	O
for	O
control	O
applications	O
(	O
kelley	O
1960	O
bryson	O
and	O
;	O
denham	O
1961	O
dreyfus	O
1962	O
bryson	O
and	O
ho	O
1969	O
dreyfus	O
1973	O
)	O
but	O
also	O
for	O
sensitivity	O
analysis	O
(	O
)	O
proposed	O
applying	O
these	O
techniques	O
to	O
training	O
artiﬁcial	O
neural	O
networks	O
.	O
the	O
idea	O
was	O
ﬁnally	O
developed	O
in	O
practice	O
after	O
being	O
independently	O
rediscovered	O
in	O
diﬀerent	O
ways	O
(	O
lecun	O
1985	O
;	O
parker	O
1985	O
rumelhart	O
)	O
.	O
the	O
book	O
parallel	O
distributed	O
pro-	O
cessing	O
presented	O
the	O
results	O
of	O
some	O
of	O
the	O
ﬁrst	O
successful	O
experiments	O
with	O
back-propagation	O
in	O
a	O
chapter	O
(	O
)	O
that	O
contributed	O
greatly	O
to	O
the	O
popularization	O
of	O
back-propagation	O
and	O
initiated	O
a	O
very	O
active	O
period	O
of	O
research	O
in	O
multi-layer	O
neural	O
networks	O
.	O
however	O
,	O
the	O
ideas	O
put	O
forward	O
by	O
the	O
authors	O
of	O
that	O
book	O
and	O
in	O
particular	O
by	O
rumelhart	O
and	O
hinton	O
go	O
much	O
beyond	O
back-propagation	O
.	O
they	O
include	O
crucial	O
ideas	O
about	O
the	O
possible	O
computational	O
implementation	O
of	O
several	O
central	O
aspects	O
of	O
cognition	O
and	O
learning	O
,	O
which	O
came	O
under	O
the	O
name	O
of	O
“	O
connectionism	O
”	O
because	O
of	O
the	O
importance	O
this	O
school	O
of	O
thought	O
places	O
on	O
the	O
connections	O
between	O
neurons	O
as	O
the	O
locus	O
of	O
learning	O
and	O
memory	O
.	O
in	O
particular	O
,	O
these	O
ideas	O
include	O
the	O
notion	O
of	O
distributed	O
representation	O
(	O
hinton	O
et	O
al.	O
,	O
rumelhart	O
et	O
al	O
.	O
1986b	O
et	O
al.	O
,	O
1986a	O
1986	O
)	O
.	O
(	O
,	O
,	O
following	O
the	O
success	O
of	O
back-propagation	O
,	O
neural	O
network	O
research	O
gained	O
pop-	O
ularity	O
and	O
reached	O
a	O
peak	O
in	O
the	O
early	O
1990s	O
.	O
afterwards	O
,	O
other	O
machine	O
learning	O
techniques	O
became	O
more	O
popular	O
until	O
the	O
modern	O
deep	O
learning	O
renaissance	O
that	O
began	O
in	O
2006.	O
the	O
core	O
ideas	O
behind	O
modern	O
feedforward	O
networks	O
have	O
not	O
changed	O
sub-	O
stantially	O
since	O
the	O
1980s	O
.	O
the	O
same	O
back-propagation	O
algorithm	O
and	O
the	O
same	O
225	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
approaches	O
to	O
gradient	O
descent	B
are	O
still	O
in	O
use	O
.	O
most	O
of	O
the	O
improvement	O
in	O
neural	O
network	O
performance	O
from	O
1986	O
to	O
2015	O
can	O
be	O
attributed	O
to	O
two	O
factors	O
.	O
first	O
,	O
larger	O
datasets	O
have	O
reduced	O
the	O
degree	O
to	O
which	O
statistical	O
generalization	O
is	O
a	O
challenge	O
for	O
neural	O
networks	O
.	O
second	O
,	O
neural	O
networks	O
have	O
become	O
much	O
larger	O
,	O
due	O
to	O
more	O
powerful	O
computers	O
,	O
and	O
better	O
software	O
infrastructure	O
.	O
however	O
,	O
a	O
small	O
number	O
of	O
algorithmic	O
changes	O
have	O
improved	O
the	O
performance	O
of	O
neural	O
networks	O
noticeably	O
.	O
one	O
of	O
these	O
algorithmic	O
changes	O
was	O
the	O
replacement	O
of	O
mean	O
squared	O
error	O
with	O
the	O
cross-entropy	O
family	O
of	O
loss	O
functions	O
.	O
mean	O
squared	O
error	O
was	O
popular	O
in	O
the	O
1980s	O
and	O
1990s	O
,	O
but	O
was	O
gradually	O
replaced	O
by	O
cross-entropy	O
losses	O
and	O
the	O
principle	O
of	O
maximum	O
likelihood	O
as	O
ideas	O
spread	O
between	O
the	O
statistics	O
community	O
and	O
the	O
machine	O
learning	O
community	O
.	O
the	O
use	O
of	O
cross-entropy	O
losses	O
greatly	O
improved	O
the	O
performance	O
of	O
models	O
with	O
sigmoid	O
and	O
softmax	O
outputs	O
,	O
which	O
had	O
previously	O
suﬀered	O
from	O
saturation	O
and	O
slow	O
learning	O
when	O
using	O
the	O
mean	O
squared	O
error	O
loss	O
.	O
0	O
,	O
z	O
the	O
other	O
major	O
algorithmic	O
change	O
that	O
has	O
greatly	O
improved	O
the	O
performance	O
{	O
}	O
of	O
feedforward	O
networks	O
was	O
the	O
replacement	O
of	O
sigmoid	O
hidden	O
units	O
with	O
piecewise	O
linear	O
hidden	O
units	O
,	O
such	O
as	O
rectiﬁed	O
linear	O
units	O
.	O
rectiﬁcation	O
using	O
the	O
max	O
function	O
was	O
introduced	O
in	O
early	O
neural	O
network	O
models	O
and	O
dates	O
back	O
at	O
least	O
as	O
far	O
as	O
the	O
cognitron	O
and	O
neocognitron	O
(	O
fukushima	O
1975	O
1980	O
)	O
.	O
these	O
early	O
models	O
did	O
not	O
use	O
rectiﬁed	O
linear	O
units	O
,	O
but	O
instead	O
applied	O
rectiﬁcation	O
to	O
nonlinear	O
functions	O
.	O
despite	O
the	O
early	O
popularity	O
of	O
rectiﬁcation	O
,	O
rectiﬁcation	O
was	O
largely	O
replaced	O
by	O
sigmoids	O
in	O
the	O
1980s	O
,	O
perhaps	O
because	O
sigmoids	O
perform	O
better	O
when	O
neural	O
networks	O
are	O
very	O
small	O
.	O
as	O
of	O
the	O
early	O
2000s	O
,	O
rectiﬁed	O
linear	O
units	O
were	O
avoided	O
due	O
to	O
a	O
somewhat	O
superstitious	O
belief	O
that	O
activation	O
functions	O
with	O
non-diﬀerentiable	O
points	O
must	O
be	O
avoided	O
.	O
this	O
began	O
to	O
change	O
in	O
about	O
2009.	O
jarrett	O
)	O
observed	O
that	O
“	O
using	O
a	O
rectifying	O
nonlinearity	O
is	O
the	O
single	O
most	O
important	O
factor	O
in	O
improving	O
the	O
performance	O
of	O
a	O
recognition	B
system	O
”	O
among	O
several	O
diﬀerent	O
factors	O
of	O
neural	O
network	O
architecture	O
design	O
.	O
et	O
al	O
.	O
(	O
2009	O
,	O
,	O
for	O
small	O
datasets	O
,	O
jarrett	O
et	O
al	O
.	O
2009	O
)	O
observed	O
that	O
using	O
rectifying	O
non-	O
linearities	O
is	O
even	O
more	O
important	O
than	O
learning	O
the	O
weights	O
of	O
the	O
hidden	O
layers	O
.	O
random	O
weights	O
are	O
suﬃcient	O
to	O
propagate	O
useful	O
information	O
through	O
a	O
rectiﬁed	O
linear	O
network	O
,	O
allowing	O
the	O
classiﬁer	O
layer	O
at	O
the	O
top	O
to	O
learn	O
how	O
to	O
map	O
diﬀerent	O
feature	O
vectors	O
to	O
class	O
identities	O
.	O
(	O
when	O
more	O
data	O
is	O
available	O
,	O
learning	O
begins	O
to	O
extract	O
enough	O
useful	O
knowledge	O
to	O
exceed	O
the	O
performance	O
of	O
randomly	O
chosen	O
parameters	O
.	O
glorot	O
et	O
al	O
.	O
2011a	O
)	O
showed	O
that	O
learning	O
is	O
far	O
easier	O
in	O
deep	O
rectiﬁed	O
linear	O
networks	O
than	O
in	O
deep	O
networks	O
that	O
have	O
curvature	O
or	O
two-sided	O
saturation	O
in	O
their	O
activation	O
functions	O
.	O
(	O
226	O
chapter	O
6.	O
deep	O
feedforward	O
networks	O
glorot	O
et	O
al	O
.	O
2011a	O
rectiﬁed	O
linear	O
units	O
are	O
also	O
of	O
historical	O
interest	O
because	O
they	O
show	O
that	O
neuroscience	O
has	O
continued	O
to	O
have	O
an	O
inﬂuence	O
on	O
the	O
development	O
of	O
deep	O
learning	O
algorithms.	O
)	O
motivate	O
rectiﬁed	O
linear	O
units	O
from	O
biological	O
considerations	O
.	O
the	O
half-rectifying	O
nonlinearity	O
was	O
intended	O
to	O
capture	O
these	O
properties	O
of	O
biological	O
neurons	O
:	O
1	O
)	O
for	O
some	O
inputs	O
,	O
biological	O
neurons	O
are	O
completely	O
inactive	O
.	O
2	O
)	O
for	O
some	O
inputs	O
,	O
a	O
biological	O
neuron	O
’	O
s	O
output	O
is	O
proportional	O
to	O
its	O
input	O
.	O
3	O
)	O
most	O
of	O
the	O
time	O
,	O
biological	O
neurons	O
operate	O
in	O
the	O
regime	O
where	O
they	O
are	O
inactive	O
(	O
i.e.	O
,	O
they	O
should	O
have	O
sparse	O
activations	O
)	O
.	O
(	O
when	O
the	O
modern	O
resurgence	O
of	O
deep	O
learning	O
began	O
in	O
2006	O
,	O
feedforward	O
networks	O
continued	O
to	O
have	O
a	O
bad	O
reputation	O
.	O
from	O
about	O
2006-2012	O
,	O
it	O
was	O
widely	O
believed	O
that	O
feedforward	O
networks	O
would	O
not	O
perform	O
well	O
unless	O
they	O
were	O
assisted	O
by	O
other	O
models	O
,	O
such	O
as	O
probabilistic	O
models	O
.	O
today	O
,	O
it	O
is	O
now	O
known	O
that	O
with	O
the	O
right	O
resources	O
and	O
engineering	O
practices	O
,	O
feedforward	O
networks	O
perform	O
very	O
well	O
.	O
today	O
,	O
gradient-based	O
learning	O
in	O
feedforward	O
networks	O
is	O
used	O
as	O
a	O
tool	O
to	O
develop	O
probabilistic	O
models	O
,	O
such	O
as	O
the	O
variational	O
autoencoder	O
and	O
generative	O
adversarial	O
networks	O
,	O
described	O
in	O
chapter	O
.	O
rather	O
than	O
being	O
viewed	O
as	O
an	O
unreliable	O
technology	O
that	O
must	O
be	O
supported	O
by	O
other	O
techniques	O
,	O
gradient-based	O
learning	O
in	O
feedforward	O
networks	O
has	O
been	O
viewed	O
since	O
2012	O
as	O
a	O
powerful	O
technology	O
that	O
may	O
be	O
applied	O
to	O
many	O
other	O
machine	O
learning	O
tasks	O
.	O
in	O
2006	O
,	O
the	O
community	O
used	O
unsupervised	O
learning	O
to	O
support	O
supervised	O
learning	O
,	O
and	O
now	O
,	O
ironically	O
,	O
it	O
is	O
more	O
common	O
to	O
use	O
supervised	O
learning	O
to	O
support	O
unsupervised	O
learning	O
.	O
20	O
feedforward	O
networks	O
continue	O
to	O
have	O
unfulﬁlled	O
potential	O
.	O
in	O
the	O
future	O
,	O
we	O
expect	O
they	O
will	O
be	O
applied	O
to	O
many	O
more	O
tasks	O
,	O
and	O
that	O
advances	O
in	O
optimization	O
algorithms	O
and	O
model	B
design	O
will	O
improve	O
their	O
performance	O
even	O
further	O
.	O
this	O
chapter	O
has	O
primarily	O
described	O
the	O
neural	O
network	O
family	O
of	O
models	O
.	O
in	O
the	O
subsequent	O
chapters	O
,	O
we	O
turn	O
to	O
how	O
to	O
use	O
these	O
models—how	O
to	O
regularize	O
and	O
train	O
them	O
.	O
227	O
chapter	O
7	O
regularization	O
for	O
deep	O
learning	O
a	O
central	O
problem	O
in	O
machine	O
learning	O
is	O
how	O
to	O
make	O
an	O
algorithm	O
that	O
will	O
perform	O
well	O
not	O
just	O
on	O
the	O
training	O
data	O
,	O
but	O
also	O
on	O
new	O
inputs	O
.	O
many	O
strategies	O
used	O
in	O
machine	O
learning	O
are	O
explicitly	O
designed	O
to	O
reduce	O
the	O
test	O
error	O
,	O
possibly	O
at	O
the	O
expense	O
of	O
increased	O
training	O
error	O
.	O
these	O
strategies	O
are	O
known	O
collectively	O
as	O
regularization	O
.	O
as	O
we	O
will	O
see	O
there	O
are	O
a	O
great	O
many	O
forms	O
of	O
regularization	O
available	O
to	O
the	O
deep	O
learning	O
practitioner	O
.	O
in	O
fact	O
,	O
developing	O
more	O
eﬀective	O
regularization	O
strategies	O
has	O
been	O
one	O
of	O
the	O
major	O
research	O
eﬀorts	O
in	O
the	O
ﬁeld	O
.	O
5	O
chapter	O
introduced	O
the	O
basic	O
concepts	O
of	O
generalization	O
,	O
underﬁtting	O
,	O
overﬁt-	O
ting	O
,	O
bias	O
,	O
variance	O
and	O
regularization	O
.	O
if	O
you	O
are	O
not	O
already	O
familiar	O
with	O
these	O
notions	O
,	O
please	O
refer	O
to	O
that	O
chapter	O
before	O
continuing	O
with	O
this	O
one	O
.	O
in	O
this	O
chapter	O
,	O
we	O
describe	O
regularization	O
in	O
more	O
detail	O
,	O
focusing	O
on	O
regular-	O
ization	O
strategies	O
for	O
deep	O
models	O
or	O
models	O
that	O
may	O
be	O
used	O
as	O
building	O
blocks	O
to	O
form	O
deep	O
models	O
.	O
some	O
sections	O
of	O
this	O
chapter	O
deal	O
with	O
standard	O
concepts	O
in	O
machine	O
learning	O
.	O
if	O
you	O
are	O
already	O
familiar	O
with	O
these	O
concepts	O
,	O
feel	O
free	O
to	O
skip	O
the	O
relevant	O
sections	O
.	O
however	O
,	O
most	O
of	O
this	O
chapter	O
is	O
concerned	O
with	O
the	O
extension	O
of	O
these	O
basic	O
concepts	O
to	O
the	O
particular	O
case	O
of	O
neural	O
networks	O
.	O
5.2.2	O
in	O
section	O
,	O
we	O
deﬁned	O
regularization	O
as	O
“	O
any	O
modiﬁcation	O
we	O
make	O
to	O
a	O
learning	O
algorithm	O
that	O
is	O
intended	O
to	O
reduce	O
its	O
generalization	O
error	O
but	O
not	O
its	O
training	O
error.	O
”	O
there	O
are	O
many	O
regularization	O
strategies	O
.	O
some	O
put	O
extra	O
constraints	O
on	O
a	O
machine	O
learning	O
model	B
,	O
such	O
as	O
adding	O
restrictions	O
on	O
the	O
parameter	O
values	O
.	O
some	O
add	O
extra	O
terms	O
in	O
the	O
objective	O
function	O
that	O
can	O
be	O
thought	O
of	O
as	O
corresponding	O
to	O
a	O
soft	O
constraint	O
on	O
the	O
parameter	O
values	O
.	O
if	O
chosen	O
carefully	O
,	O
these	O
extra	O
constraints	O
and	O
penalties	O
can	O
lead	O
to	O
improved	O
performance	O
228	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
on	O
the	O
test	O
set	O
.	O
sometimes	O
these	O
constraints	O
and	O
penalties	O
are	O
designed	O
to	O
encode	O
speciﬁc	O
kinds	O
of	O
prior	O
knowledge	O
.	O
other	O
times	O
,	O
these	O
constraints	O
and	O
penalties	O
are	O
designed	O
to	O
express	O
a	O
generic	O
preference	O
for	O
a	O
simpler	O
model	B
class	O
in	O
order	O
to	O
promote	O
generalization	O
.	O
sometimes	O
penalties	O
and	O
constraints	O
are	O
necessary	O
to	O
make	O
an	O
underdetermined	O
problem	O
determined	O
.	O
other	O
forms	O
of	O
regularization	O
,	O
known	O
as	O
ensemble	O
methods	O
,	O
combine	O
multiple	O
hypotheses	O
that	O
explain	O
the	O
training	O
data	O
.	O
in	O
the	O
context	O
of	O
deep	O
learning	O
,	O
most	O
regularization	O
strategies	O
are	O
based	O
on	O
regularizing	O
estimators	O
.	O
regularization	O
of	O
an	O
estimator	O
works	O
by	O
trading	O
increased	O
bias	O
for	O
reduced	O
variance	O
.	O
an	O
eﬀective	O
regularizer	O
is	O
one	O
that	O
makes	O
a	O
proﬁtable	O
trade	O
,	O
reducing	O
variance	O
signiﬁcantly	O
while	O
not	O
overly	O
increasing	O
the	O
bias	O
.	O
when	O
we	O
discussed	O
generalization	O
and	O
overﬁtting	O
in	O
chapter	O
,	O
we	O
focused	O
on	O
three	O
situations	O
,	O
where	O
the	O
model	B
family	O
being	O
trained	O
either	O
(	O
1	O
)	O
excluded	O
the	O
true	O
data	O
generating	O
process—corresponding	O
to	O
underﬁtting	O
and	O
inducing	O
bias	O
,	O
or	O
(	O
2	O
)	O
matched	O
the	O
true	O
data	O
generating	O
process	O
,	O
or	O
(	O
3	O
)	O
included	O
the	O
generating	O
process	O
but	O
also	O
many	O
other	O
possible	O
generating	O
processes—the	O
overﬁtting	O
regime	O
where	O
variance	O
rather	O
than	O
bias	O
dominates	O
the	O
estimation	O
error	O
.	O
the	O
goal	O
of	O
regularization	O
is	O
to	O
take	O
a	O
model	B
from	O
the	O
third	O
regime	O
into	O
the	O
second	O
regime	O
.	O
5	O
in	O
practice	O
,	O
an	O
overly	O
complex	O
model	B
family	O
does	O
not	O
necessarily	O
include	O
the	O
target	O
function	O
or	O
the	O
true	O
data	O
generating	O
process	O
,	O
or	O
even	O
a	O
close	O
approximation	O
of	O
either	O
.	O
we	O
almost	O
never	O
have	O
access	O
to	O
the	O
true	O
data	O
generating	O
process	O
so	O
we	O
can	O
never	O
know	O
for	O
sure	O
if	O
the	O
model	B
family	O
being	O
estimated	O
includes	O
the	O
generating	O
process	O
or	O
not	O
.	O
however	O
,	O
most	O
applications	O
of	O
deep	O
learning	O
algorithms	O
are	O
to	O
domains	O
where	O
the	O
true	O
data	O
generating	O
process	O
is	O
almost	O
certainly	O
outside	O
the	O
model	B
family	O
.	O
deep	O
learning	O
algorithms	O
are	O
typically	O
applied	O
to	O
extremely	O
complicated	O
domains	O
such	O
as	O
images	O
,	O
audio	O
sequences	O
and	O
text	O
,	O
for	O
which	O
the	O
true	O
generation	O
process	O
essentially	O
involves	O
simulating	O
the	O
entire	O
universe	O
.	O
to	O
some	O
extent	O
,	O
we	O
are	O
always	O
trying	O
to	O
ﬁt	O
a	O
square	O
peg	O
(	O
the	O
data	O
generating	O
process	O
)	O
into	O
a	O
round	O
hole	O
(	O
our	O
model	B
family	O
)	O
.	O
what	O
this	O
means	O
is	O
that	O
controlling	O
the	O
complexity	O
of	O
the	O
model	B
is	O
not	O
a	O
simple	O
matter	O
of	O
ﬁnding	O
the	O
model	B
of	O
the	O
right	O
size	O
,	O
with	O
the	O
right	O
number	O
of	O
parameters	O
.	O
instead	O
,	O
we	O
might	O
ﬁnd—and	O
indeed	O
in	O
practical	O
deep	O
learning	O
scenarios	O
,	O
we	O
almost	O
always	O
do	O
ﬁnd—that	O
the	O
best	O
ﬁtting	O
model	B
(	O
in	O
the	O
sense	O
of	O
minimizing	O
generalization	O
error	O
)	O
is	O
a	O
large	O
model	B
that	O
has	O
been	O
regularized	O
appropriately	O
.	O
we	O
now	O
review	O
several	O
strategies	O
for	O
how	O
to	O
create	O
such	O
a	O
large	O
,	O
deep	O
,	O
regularized	O
model	B
.	O
229	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
7.1	O
parameter	O
norm	O
penalties	O
regularization	O
has	O
been	O
used	O
for	O
decades	O
prior	O
to	O
the	O
advent	O
of	O
deep	O
learning	O
.	O
linear	O
models	O
such	O
as	O
linear	O
regression	O
and	O
logistic	O
regression	O
allow	O
simple	O
,	O
straightforward	O
,	O
and	O
eﬀective	O
regularization	O
strategies	O
.	O
many	O
regularization	O
approaches	O
are	O
based	O
on	O
limiting	O
the	O
capacity	O
of	O
models	O
,	O
such	O
as	O
neural	O
networks	O
,	O
linear	O
regression	O
,	O
or	O
logistic	O
regression	O
,	O
by	O
adding	O
a	O
pa-	O
rameter	O
norm	O
penalty	O
ω	O
(	O
θ	O
)	O
to	O
the	O
objective	O
function	O
j	O
.	O
we	O
denote	O
the	O
regularized	O
objective	O
function	O
by	O
˜j	O
:	O
˜j	O
(	O
;	O
θ	O
x	O
y	O
)	O
=	O
(	O
;	O
θ	O
x	O
y	O
)	O
+	O
ω	O
(	O
)	O
θ	O
α	O
j	O
,	O
,	O
(	O
7.1	O
)	O
∈	O
∞	O
ω	O
[	O
0	O
,	O
,	O
relative	O
to	O
the	O
standard	O
objective	O
function	O
where	O
α	O
)	O
is	O
a	O
hyperparameter	O
that	O
weights	O
the	O
relative	O
contribution	O
of	O
the	O
j	O
.	O
setting	O
α	O
to	O
0	O
norm	O
penalty	O
term	O
,	O
results	O
in	O
no	O
regularization	O
.	O
larger	O
values	O
of	O
α	O
correspond	O
to	O
more	O
regularization	O
.	O
when	O
our	O
training	O
algorithm	O
minimizes	O
the	O
regularized	O
objective	O
function	O
˜j	O
it	O
will	O
decrease	O
both	O
the	O
original	O
objective	O
j	O
on	O
the	O
training	O
data	O
and	O
some	O
measure	O
of	O
the	O
size	O
of	O
the	O
parameters	O
θ	O
(	O
or	O
some	O
subset	O
of	O
the	O
parameters	O
)	O
.	O
diﬀerent	O
choices	O
for	O
the	O
parameter	O
norm	O
can	O
result	O
in	O
diﬀerent	O
solutions	O
being	O
preferred	O
.	O
in	O
this	O
section	O
,	O
we	O
discuss	O
the	O
eﬀects	O
of	O
the	O
various	O
norms	O
when	O
used	O
as	O
penalties	O
on	O
the	O
model	B
parameters	O
.	O
ω	O
ω	O
only	O
the	O
weights	O
before	O
delving	O
into	O
the	O
regularization	O
behavior	O
of	O
diﬀerent	O
norms	O
,	O
we	O
note	O
that	O
that	O
for	O
neural	O
networks	O
,	O
we	O
typically	O
choose	O
to	O
use	O
a	O
parameter	O
norm	O
penalty	O
penalizes	O
of	O
the	O
aﬃne	O
transformation	O
at	O
each	O
layer	O
and	O
leaves	O
the	O
biases	O
unregularized	O
.	O
the	O
biases	O
typically	O
require	O
less	O
data	O
to	O
ﬁt	O
accurately	O
than	O
the	O
weights	O
.	O
each	O
weight	O
speciﬁes	O
how	O
two	O
variables	O
interact	O
.	O
fitting	O
the	O
weight	O
well	O
requires	O
observing	O
both	O
variables	O
in	O
a	O
variety	O
of	O
conditions	B
.	O
each	O
bias	O
controls	O
only	O
a	O
single	O
variable	O
.	O
this	O
means	O
that	O
we	O
do	O
not	O
induce	O
too	O
much	O
variance	O
by	O
leaving	O
the	O
biases	O
unregularized	O
.	O
also	O
,	O
regularizing	O
the	O
bias	O
parameters	O
can	O
introduce	O
a	O
signiﬁcant	O
amount	O
of	O
underﬁtting	O
.	O
we	O
therefore	O
use	O
the	O
vector	O
w	O
to	O
indicate	O
all	O
of	O
the	O
weights	O
that	O
should	O
be	O
aﬀected	O
by	O
a	O
norm	O
penalty	O
,	O
while	O
the	O
vector	O
θ	O
denotes	O
all	O
of	O
the	O
parameters	O
,	O
including	O
both	O
w	O
and	O
the	O
unregularized	O
parameters	O
.	O
in	O
the	O
context	O
of	O
neural	O
networks	O
,	O
it	O
is	O
sometimes	O
desirable	O
to	O
use	O
a	O
separate	O
penalty	O
with	O
a	O
diﬀerent	O
α	O
coeﬃcient	O
for	O
each	O
layer	O
of	O
the	O
network	O
.	O
because	O
it	O
can	O
be	O
expensive	O
to	O
search	O
for	O
the	O
correct	O
value	O
of	O
multiple	O
hyperparameters	O
,	O
it	O
is	O
still	O
reasonable	O
to	O
use	O
the	O
same	O
weight	O
decay	O
at	O
all	O
layers	O
just	O
to	O
reduce	O
the	O
size	O
of	O
search	O
space	O
.	O
230	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
7.1.1	O
l	O
2	O
parameter	O
regularization	O
5.2.2	O
we	O
have	O
already	O
seen	O
,	O
in	O
section	O
,	O
one	O
of	O
the	O
simplest	O
and	O
most	O
common	O
kinds	O
of	O
parameter	O
norm	O
penalty	O
:	O
the	O
l2	O
parameter	O
norm	O
penalty	O
commonly	O
known	O
as	O
weight	O
decay	O
.	O
this	O
regularization	O
strategy	O
drives	O
the	O
weights	O
closer	O
to	O
the	O
origin1	O
by	O
adding	O
a	O
regularization	O
term	O
ω	O
(	O
θ	O
)	O
=	O
1	O
2	O
to	O
the	O
objective	O
function	O
.	O
in	O
other	O
2	O
academic	O
communities	O
,	O
l2	O
regularization	O
is	O
also	O
known	O
as	O
ridge	O
regression	O
or	O
tikhonov	O
regularization	O
.	O
	O
	O
w	O
2	O
we	O
can	O
gain	O
some	O
insight	O
into	O
the	O
behavior	O
of	O
weight	O
decay	O
regularization	O
by	O
studying	O
the	O
gradient	O
of	O
the	O
regularized	O
objective	O
function	O
.	O
to	O
simplify	O
the	O
presentation	O
,	O
we	O
assume	O
no	O
bias	O
parameter	O
,	O
so	O
θ	O
is	O
just	O
w.	O
such	O
a	O
model	B
has	O
the	O
following	O
total	O
objective	O
function	O
:	O
˜j	O
(	O
;	O
w	O
x	O
y	O
)	O
=	O
,	O
	O
w	O
α	O
2	O
w	O
+	O
(	O
j	O
w	O
x	O
y	O
;	O
,	O
)	O
,	O
with	O
the	O
corresponding	O
parameter	O
gradient	O
∇	O
w	O
˜j	O
(	O
;	O
w	O
x	O
y	O
)	O
=	O
w	O
+	O
α	O
,	O
∇	O
wj	O
(	O
;	O
w	O
x	O
y	O
)	O
.	O
,	O
(	O
7.2	O
)	O
(	O
7.3	O
)	O
(	O
7.4	O
)	O
(	O
7.5	O
)	O
to	O
take	O
a	O
single	O
gradient	O
step	O
to	O
update	O
the	O
weights	O
,	O
we	O
perform	O
this	O
update	O
:	O
←	O
−	O
∇	O
w	O
w	O
	O
α	O
(	O
w	O
+	O
wj	O
(	O
;	O
w	O
x	O
y	O
)	O
)	O
,	O
.	O
written	O
another	O
way	O
,	O
the	O
update	O
is	O
:	O
←	O
−	O
w	O
(	O
1	O
α	O
)	O
w	O
−	O
∇	O
	O
wj	O
(	O
;	O
w	O
x	O
y	O
)	O
.	O
,	O
we	O
can	O
see	O
that	O
the	O
addition	O
of	O
the	O
weight	O
decay	O
term	O
has	O
modiﬁed	O
the	O
learning	O
rule	O
to	O
multiplicatively	O
shrink	O
the	O
weight	O
vector	O
by	O
a	O
constant	O
factor	O
on	O
each	O
step	O
,	O
just	O
before	O
performing	O
the	O
usual	O
gradient	O
update	O
.	O
this	O
describes	O
what	O
happens	O
in	O
a	O
single	O
step	O
.	O
but	O
what	O
happens	O
over	O
the	O
entire	O
course	O
of	O
training	O
?	O
we	O
will	O
further	O
simplify	O
the	O
analysis	O
by	O
making	O
a	O
quadratic	O
approximation	O
to	O
the	O
objective	O
function	O
in	O
the	O
neighborhood	O
of	O
the	O
value	O
of	O
the	O
weights	O
that	O
obtains	O
minimal	O
unregularized	O
training	O
cost	O
,	O
w	O
=	O
arg	O
minw	O
j	O
(	O
w	O
)	O
.	O
if	O
the	O
objective	O
function	O
is	O
truly	O
quadratic	O
,	O
as	O
in	O
the	O
case	O
of	O
ﬁtting	O
a	O
linear	O
regression	O
model	B
with	O
∗	O
1more	O
generally	O
,	O
we	O
could	O
regularize	O
the	O
parameters	O
to	O
be	O
near	O
any	O
speciﬁc	O
point	O
in	O
space	O
and	O
,	O
surprisingly	O
,	O
still	O
get	O
a	O
regularization	O
eﬀect	O
,	O
but	O
better	O
results	O
will	O
be	O
obtained	O
for	O
a	O
value	O
closer	O
to	O
the	O
true	O
one	O
,	O
with	O
zero	O
being	O
a	O
default	O
value	O
that	O
makes	O
sense	O
when	O
we	O
do	O
not	O
know	O
if	O
the	O
correct	O
value	O
should	O
be	O
positive	O
or	O
negative	O
.	O
since	O
it	O
is	O
far	O
more	O
common	O
to	O
regularize	O
the	O
model	B
parameters	O
towards	O
zero	O
,	O
we	O
will	O
focus	O
on	O
this	O
special	O
case	O
in	O
our	O
exposition	O
.	O
231	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
mean	O
squared	O
error	O
,	O
then	O
the	O
approximation	O
is	O
perfect	O
.	O
the	O
approximation	O
ˆj	O
is	O
given	O
by	O
ˆj	O
(	O
)	O
=	O
θ	O
j	O
(	O
w	O
)	O
+	O
∗	O
−	O
∗	O
(	O
w	O
w	O
)	O
	O
−	O
∗	O
h	O
w	O
w	O
(	O
)	O
,	O
1	O
2	O
(	O
7.6	O
)	O
∗	O
where	O
h	O
is	O
the	O
hessian	O
matrix	O
of	O
j	O
with	O
respect	O
to	O
w	O
evaluated	O
at	O
w	O
no	O
ﬁrst-order	O
term	O
in	O
this	O
quadratic	O
approximation	O
,	O
because	O
w	O
∗	O
minimum	O
,	O
where	O
the	O
gradient	O
vanishes	O
.	O
likewise	O
,	O
because	O
w	O
minimum	O
of	O
is	O
positive	O
semideﬁnite	O
.	O
,	O
we	O
can	O
conclude	O
that	O
.	O
there	O
is	O
is	O
deﬁned	O
to	O
be	O
a	O
is	O
the	O
location	O
of	O
a	O
h	O
j	O
∗	O
the	O
minimum	O
of	O
ˆj	O
occurs	O
where	O
its	O
gradient	O
∇	O
−	O
∗	O
w	O
h	O
w	O
w	O
)	O
=	O
(	O
ˆj	O
(	O
)	O
(	O
7.7	O
)	O
is	O
equal	O
to	O
.0	O
w	O
to	O
study	O
the	O
eﬀect	O
of	O
weight	O
decay	O
,	O
we	O
modify	O
equation	O
by	O
adding	O
the	O
weight	O
decay	O
gradient	O
.	O
we	O
can	O
now	O
solve	O
for	O
the	O
minimum	O
of	O
the	O
regularized	O
version	O
of	O
ˆj	O
.	O
we	O
use	O
the	O
variable	O
˜w	O
to	O
represent	O
the	O
location	O
of	O
the	O
minimum	O
.	O
7.7	O
−	O
∗	O
α	O
˜w	O
h+	O
(	O
˜w	O
w	O
)	O
=	O
0	O
∗	O
h	O
αi	O
˜w	O
hw=	O
(	O
+	O
)	O
−	O
∗	O
1hw	O
˜w	O
h	O
i	O
=	O
(	O
+	O
α	O
)	O
.	O
(	O
7.8	O
)	O
(	O
7.9	O
)	O
(	O
7.10	O
)	O
(	O
7.11	O
)	O
(	O
7.12	O
)	O
(	O
7.13	O
)	O
∗	O
as	O
α	O
approaches	O
0	O
,	O
the	O
regularized	O
solution	O
˜w	O
approaches	O
w	O
.	O
but	O
what	O
happens	O
as	O
α	O
grows	O
?	O
because	O
h	O
is	O
real	O
and	O
symmetric	O
,	O
we	O
can	O
decompose	O
it	O
into	O
a	O
diagonal	O
matrix	O
λ	O
and	O
an	O
orthonormal	O
basis	O
of	O
eigenvectors	O
,	O
q	O
,	O
such	O
that	O
h	O
q	O
q	O
,	O
we	O
obtain	O
:	O
=	O
λ	O
7.10	O
	O
	O
	O
=	O
q	O
.	O
applying	O
the	O
decomposition	O
to	O
equation	O
∗	O
	O
˜w	O
q	O
q	O
=	O
(	O
λ	O
−	O
1q	O
qλ	O
+	O
)	O
αi	O
−	O
	O
1	O
i	O
q	O
(	O
+λ	O
α	O
)	O
−	O
	O
1λq	O
q	O
qλ	O
∗	O
w	O
	O
w	O
	O
.	O
∗	O
w	O
=	O
(	O
+	O
)	O
q	O
λ	O
αi	O
we	O
see	O
that	O
the	O
eﬀect	O
of	O
weight	O
decay	O
is	O
to	O
rescale	O
w	O
the	O
eigenvectors	O
of	O
h.	O
speciﬁcally	O
,	O
the	O
component	O
of	O
w	O
i-th	O
eigenvector	O
of	O
h	O
is	O
rescaled	O
by	O
a	O
factor	O
of	O
how	O
this	O
kind	O
of	O
scaling	O
works	O
,	O
ﬁrst	O
explained	O
in	O
ﬁgure	O
λi	O
+α	O
λi	O
2.3	O
)	O
.	O
along	O
the	O
axes	O
deﬁned	O
by	O
∗	O
that	O
is	O
aligned	O
with	O
the	O
.	O
(	O
you	O
may	O
wish	O
to	O
review	O
∗	O
along	O
the	O
directions	O
where	O
the	O
eigenvalues	O
of	O
h	O
are	O
relatively	O
large	O
,	O
for	O
example	O
,	O
α	O
,	O
the	O
eﬀect	O
of	O
regularization	O
is	O
relatively	O
small	O
.	O
however	O
,	O
components	O
α	O
will	O
be	O
shrunk	O
to	O
have	O
nearly	O
zero	O
magnitude	O
.	O
this	O
eﬀect	O
is	O
illustrated	O
	O
	O
where	O
λi	O
with	O
λi	O
in	O
ﬁgure	O
.7.1	O
232	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
∗	O
w	O
2	O
w	O
˜w	O
w1	O
figure	O
7.1	O
:	O
an	O
illustration	O
of	O
the	O
eﬀect	O
of	O
l2	O
(	O
or	O
weight	O
decay	O
)	O
regularization	O
on	O
the	O
value	O
of	O
the	O
optimal	O
w.	O
the	O
solid	O
ellipses	O
represent	O
contours	O
of	O
equal	O
value	O
of	O
the	O
unregularized	O
objective	O
.	O
the	O
dotted	O
circles	O
represent	O
contours	O
of	O
equal	O
value	O
of	O
the	O
l2	O
regularizer	O
.	O
at	O
the	O
point	O
˜w	O
,	O
these	O
competing	O
objectives	O
reach	O
an	O
equilibrium	O
.	O
in	O
the	O
ﬁrst	O
dimension	O
,	O
the	O
eigenvalue	O
of	O
the	O
hessian	O
of	O
j	O
is	O
small	O
.	O
the	O
objective	O
function	O
does	O
not	O
increase	O
much	O
when	O
moving	O
horizontally	O
away	O
from	O
w	O
.	O
because	O
the	O
objective	O
function	O
does	O
not	O
express	O
a	O
strong	O
preference	O
along	O
this	O
direction	O
,	O
the	O
regularizer	O
has	O
a	O
strong	O
eﬀect	O
on	O
this	O
axis	O
.	O
the	O
regularizer	O
pulls	O
w	O
1	O
close	O
to	O
zero	O
.	O
in	O
the	O
second	O
dimension	O
,	O
the	O
objective	O
function	O
is	O
very	O
sensitive	O
to	O
movements	O
away	O
from	O
w	O
.	O
the	O
corresponding	O
eigenvalue	O
is	O
large	O
,	O
indicating	O
high	O
curvature	O
.	O
as	O
a	O
result	O
,	O
weight	O
decay	O
aﬀects	O
the	O
position	B
of	O
w2	O
relatively	O
little	O
.	O
∗	O
∗	O
only	O
directions	O
along	O
which	O
the	O
parameters	O
contribute	O
signiﬁcantly	O
to	O
reducing	O
the	O
objective	O
function	O
are	O
preserved	O
relatively	O
intact	O
.	O
in	O
directions	O
that	O
do	O
not	O
contribute	O
to	O
reducing	O
the	O
objective	O
function	O
,	O
a	O
small	O
eigenvalue	O
of	O
the	O
hessian	O
tells	O
us	O
that	O
movement	O
in	O
this	O
direction	O
will	O
not	O
signiﬁcantly	O
increase	O
the	O
gradient	O
.	O
components	O
of	O
the	O
weight	O
vector	O
corresponding	O
to	O
such	O
unimportant	O
directions	O
are	O
decayed	O
away	O
through	O
the	O
use	O
of	O
the	O
regularization	O
throughout	O
training	O
.	O
so	O
far	O
we	O
have	O
discussed	O
weight	O
decay	O
in	O
terms	O
of	O
its	O
eﬀect	O
on	O
the	O
optimization	O
of	O
an	O
abstract	O
,	O
general	O
,	O
quadratic	O
cost	O
function	O
.	O
how	O
do	O
these	O
eﬀects	O
relate	O
to	O
machine	O
learning	O
in	O
particular	O
?	O
we	O
can	O
ﬁnd	O
out	O
by	O
studying	O
linear	O
regression	O
,	O
a	O
model	B
for	O
which	O
the	O
true	O
cost	O
function	O
is	O
quadratic	O
and	O
therefore	O
amenable	O
to	O
the	O
same	O
kind	O
of	O
analysis	O
we	O
have	O
used	O
so	O
far	O
.	O
applying	O
the	O
analysis	O
again	O
,	O
we	O
will	O
be	O
able	O
to	O
obtain	O
a	O
special	O
case	O
of	O
the	O
same	O
results	O
,	O
but	O
with	O
the	O
solution	O
now	O
phrased	O
in	O
terms	O
of	O
the	O
training	O
data	O
.	O
for	O
linear	O
regression	O
,	O
the	O
cost	O
function	O
is	O
233	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
the	O
sum	O
of	O
squared	O
errors	O
:	O
−	O
	O
)	O
(	O
xw	O
y	O
−	O
xw	O
y	O
(	O
)	O
.	O
when	O
we	O
add	O
l2	O
regularization	O
,	O
the	O
objective	O
function	O
changes	O
to	O
−	O
	O
)	O
(	O
xw	O
y	O
−	O
xw	O
y	O
(	O
)	O
+	O
	O
w.	O
αw	O
1	O
2	O
this	O
changes	O
the	O
normal	O
equations	O
for	O
the	O
solution	O
from	O
(	O
7.14	O
)	O
(	O
7.15	O
)	O
(	O
7.16	O
)	O
(	O
7.17	O
)	O
	O
w	O
x=	O
(	O
	O
−	O
1x	O
x	O
)	O
	O
y	O
	O
w	O
x=	O
(	O
x	O
i+	O
α	O
)	O
−	O
1x	O
	O
y	O
.	O
	O
	O
to	O
	O
7.16	O
x	O
in	O
equation	O
is	O
proportional	O
to	O
the	O
covariance	O
matrix	O
x.	O
the	O
matrix	O
x	O
using	O
l2	O
regularization	O
replaces	O
this	O
matrix	O
with	O
.	O
7.17	O
the	O
new	O
matrix	O
is	O
the	O
same	O
as	O
the	O
original	O
one	O
,	O
but	O
with	O
the	O
addition	O
of	O
α	O
to	O
the	O
diagonal	O
.	O
the	O
diagonal	O
entries	O
of	O
this	O
matrix	O
correspond	O
to	O
the	O
variance	O
of	O
each	O
input	O
feature	O
.	O
we	O
can	O
see	O
that	O
l2	O
regularization	O
causes	O
the	O
learning	O
algorithm	O
to	O
“	O
perceive	O
”	O
the	O
input	O
x	O
as	O
having	O
higher	O
variance	O
,	O
which	O
makes	O
it	O
shrink	O
the	O
weights	O
on	O
features	O
whose	O
covariance	O
with	O
the	O
output	O
target	O
is	O
low	O
compared	O
to	O
this	O
added	O
variance	O
.	O
x	O
−	O
1	O
in	O
equation	O
	O
x	O
i+	O
α	O
x	O
1	O
m	O
7.1.2	O
l	O
1	O
regularization	O
while	O
l	O
2	O
weight	O
decay	O
is	O
the	O
most	O
common	O
form	O
of	O
weight	O
decay	O
,	O
there	O
are	O
other	O
ways	O
to	O
penalize	O
the	O
size	O
of	O
the	O
model	B
parameters	O
.	O
another	O
option	O
is	O
to	O
use	O
l	O
1	O
regularization	O
.	O
	O
formally	O
,	O
l1	O
regularization	O
on	O
the	O
model	B
parameter	O
||	O
||	O
w	O
1	O
=	O
ω	O
(	O
)	O
=	O
θ	O
|	O
wi	O
|	O
,	O
w	O
is	O
deﬁned	O
as	O
:	O
(	O
7.18	O
)	O
i	O
that	O
is	O
,	O
as	O
the	O
sum	O
of	O
absolute	O
values	O
of	O
the	O
individual	O
parameters.2	O
we	O
will	O
now	O
discuss	O
the	O
eﬀect	O
of	O
l1	O
regularization	O
on	O
the	O
simple	O
linear	O
regression	O
model	B
,	O
with	O
no	O
bias	O
parameter	O
,	O
that	O
we	O
studied	O
in	O
our	O
analysis	O
of	O
l2	O
regularization	O
.	O
in	O
particular	O
,	O
we	O
are	O
interested	O
in	O
delineating	O
the	O
diﬀerences	O
between	O
l1	O
and	O
l2	O
forms	O
2as	O
with	O
l2	O
regularization	O
,	O
we	O
could	O
regularize	O
the	O
parameters	O
towards	O
a	O
value	O
that	O
is	O
not	O
−	O
|	O
(	O
)	O
o	O
.	O
in	O
that	O
case	O
the	O
l1	O
regularization	O
would	O
zero	O
,	O
but	O
instead	O
towards	O
some	O
parameter	O
value	O
w	O
introduce	O
the	O
term	O
ω	O
(	O
)	O
=	O
wi	O
||	O
−	O
w	O
(	O
)	O
o	O
1	O
=	O
|	O
.	O
	O
||	O
(	O
)	O
o	O
w	O
w	O
θ	O
i	O
i	O
234	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
of	O
regularization	O
.	O
as	O
with	O
l2	O
weight	O
decay	O
,	O
l1	O
weight	O
decay	O
controls	O
the	O
strength	O
α.	O
of	O
the	O
regularization	O
by	O
scaling	O
the	O
penalty	O
ω	O
thus	O
,	O
the	O
regularized	O
objective	O
function	O
˜j	O
;	O
w	O
x	O
y	O
)	O
is	O
given	O
by	O
(	O
||	O
||	O
w	O
1	O
+	O
(	O
α	O
using	O
a	O
positive	O
hyperparameter	O
˜j	O
(	O
;	O
w	O
x	O
y	O
)	O
=	O
j	O
w	O
x	O
y	O
,	O
(	O
7.19	O
)	O
)	O
,	O
,	O
,	O
;	O
with	O
the	O
corresponding	O
gradient	O
(	O
actually	O
,	O
sub-gradient	O
)	O
:	O
∇	O
w	O
˜j	O
(	O
;	O
w	O
x	O
y	O
)	O
=	O
sign	O
(	O
α	O
,	O
)	O
+w	O
(	O
x	O
y	O
w	O
;	O
,	O
)	O
(	O
7.20	O
)	O
∇	O
wj	O
where	O
sign	O
(	O
)	O
w	O
is	O
simply	O
the	O
sign	O
of	O
w	O
applied	O
element-wise	O
.	O
7.20	O
by	O
inspecting	O
equation	O
,	O
we	O
can	O
see	O
immediately	O
that	O
the	O
eﬀect	O
of	O
l	O
1	O
regularization	O
is	O
quite	O
diﬀerent	O
from	O
that	O
of	O
l2	O
regularization	O
.	O
speciﬁcally	O
,	O
we	O
can	O
see	O
that	O
the	O
regularization	O
contribution	O
to	O
the	O
gradient	O
no	O
longer	O
scales	O
linearly	O
with	O
each	O
wi	O
;	O
instead	O
it	O
is	O
a	O
constant	O
factor	O
with	O
a	O
sign	O
equal	O
to	O
sign	O
(	O
wi	O
)	O
.	O
one	O
consequence	O
of	O
this	O
form	O
of	O
the	O
gradient	O
is	O
that	O
we	O
will	O
not	O
necessarily	O
see	O
clean	O
;	O
w	O
)	O
as	O
we	O
did	O
for	O
l	O
2	O
algebraic	O
solutions	O
to	O
quadratic	O
approximations	O
of	O
j	O
(	O
x	O
y	O
,	O
regularization	O
.	O
our	O
simple	O
linear	O
model	B
has	O
a	O
quadratic	O
cost	O
function	O
that	O
we	O
can	O
represent	O
via	O
its	O
taylor	O
series	O
.	O
alternately	O
,	O
we	O
could	O
imagine	O
that	O
this	O
is	O
a	O
truncated	O
taylor	O
series	O
approximating	O
the	O
cost	O
function	O
of	O
a	O
more	O
sophisticated	O
model	B
.	O
the	O
gradient	O
in	O
this	O
setting	O
is	O
given	O
by	O
∇	O
w	O
−	O
∗	O
w	O
h	O
w	O
w	O
)	O
,	O
)	O
=	O
(	O
ˆj	O
(	O
(	O
7.21	O
)	O
∗	O
	O
	O
1	O
2	O
	O
235	O
where	O
,	O
again	O
,	O
h	O
is	O
the	O
hessian	O
matrix	O
of	O
with	O
respect	O
to	O
j	O
w	O
evaluated	O
at	O
w	O
.	O
because	O
the	O
l1	O
penalty	O
does	O
not	O
admit	O
clean	O
algebraic	O
expressions	O
in	O
the	O
case	O
of	O
a	O
fully	O
general	O
hessian	O
,	O
we	O
will	O
also	O
make	O
the	O
further	O
simplifying	O
assumption	O
that	O
the	O
hessian	O
is	O
diagonal	O
,	O
h	O
=	O
diag	O
(	O
[	O
h1	O
1	O
,	O
,	O
.	O
.	O
.	O
,	O
hn	O
,	O
n	O
]	O
)	O
,	O
where	O
each	O
hi	O
,	O
i	O
>	O
0.	O
this	O
assumption	O
holds	O
if	O
the	O
data	O
for	O
the	O
linear	O
regression	O
problem	O
has	O
been	O
preprocessed	O
to	O
remove	O
all	O
correlation	O
between	O
the	O
input	O
features	O
,	O
which	O
may	O
be	O
accomplished	O
using	O
pca	O
.	O
our	O
quadratic	O
approximation	O
of	O
the	O
l1	O
regularized	O
objective	O
function	O
decom-	O
	O
poses	O
into	O
a	O
sum	O
over	O
the	O
parameters	O
:	O
∗	O
ˆj	O
;	O
w	O
x	O
y	O
)	O
=	O
(	O
w	O
(	O
j	O
,	O
;	O
)	O
+x	O
y	O
,	O
i	O
−	O
|	O
∗	O
i	O
)	O
2	O
+	O
α	O
w	O
w	O
|	O
i	O
.	O
(	O
7.22	O
)	O
h	O
i	O
,	O
i	O
(	O
wi	O
	O
the	O
problem	O
of	O
minimizing	O
this	O
approximate	O
cost	O
function	O
has	O
an	O
analytical	O
solution	O
(	O
for	O
each	O
dimension	O
)	O
,	O
with	O
the	O
following	O
form	O
:	O
∗	O
i	O
∗	O
i	O
)	O
max	O
wi	O
=	O
sign	O
(	O
w	O
|	O
w	O
(	O
7.23	O
)	O
,	O
0	O
i	O
.	O
|	O
−	O
α	O
hi	O
,	O
i	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
∗	O
consider	O
the	O
situation	O
where	O
w	O
i	O
>	O
0	O
for	O
all	O
.	O
there	O
are	O
two	O
possible	O
outcomes	O
:	O
i	O
α	O
hi	O
,	O
i	O
1.	O
the	O
case	O
where	O
w	O
.	O
here	O
the	O
optimal	O
value	O
of	O
wi	O
under	O
the	O
regularized	O
objective	O
is	O
simply	O
wi	O
=	O
0.	O
this	O
occurs	O
because	O
the	O
contribution	O
of	O
j	O
(	O
w	O
;	O
x	O
y	O
,	O
)	O
to	O
the	O
regularized	O
objective	O
˜j	O
(	O
w	O
;	O
x	O
y	O
,	O
)	O
is	O
overwhelmed—in	O
direction	O
i—by	O
the	O
l1	O
regularization	O
which	O
pushes	O
the	O
value	O
of	O
wi	O
to	O
zero	O
.	O
≤	O
∗	O
i	O
∗	O
i	O
>	O
α	O
hi	O
,	O
i	O
2.	O
the	O
case	O
where	O
w	O
.	O
in	O
this	O
case	O
,	O
the	O
regularization	O
does	O
not	O
move	O
the	O
optimal	O
value	O
of	O
wi	O
to	O
zero	O
but	O
instead	O
it	O
just	O
shifts	O
it	O
in	O
that	O
direction	O
by	O
a	O
distance	O
equal	O
to	O
α	O
h	O
i	O
,	O
i	O
.	O
a	O
similar	O
process	O
happens	O
when	O
w	O
negative	O
by	O
α	O
hi	O
,	O
i	O
,	O
or	O
0	O
.	O
∗	O
i	O
<	O
0	O
,	O
but	O
with	O
the	O
l1	O
penalty	O
making	O
wi	O
less	O
in	O
comparison	O
to	O
l	O
2	O
regularization	O
,	O
l1	O
regularization	O
results	O
in	O
a	O
solution	O
that	O
is	O
more	O
sparse	O
.	O
sparsity	O
in	O
this	O
context	O
refers	O
to	O
the	O
fact	O
that	O
some	O
parameters	O
have	O
an	O
optimal	O
value	O
of	O
zero	O
.	O
the	O
sparsity	O
of	O
l1	O
regularization	O
is	O
a	O
qualitatively	O
diﬀerent	O
behavior	O
than	O
arises	O
with	O
l	O
2	O
regularization	O
.	O
equation	O
gave	O
the	O
solution	O
˜w	O
for	O
l2	O
regularization	O
.	O
if	O
we	O
revisit	O
that	O
equation	O
using	O
the	O
assumption	O
of	O
a	O
diagonal	O
and	O
positive	O
deﬁnite	O
hessian	O
h	O
that	O
we	O
introduced	O
for	O
our	O
analysis	O
of	O
∗	O
l1	O
regularization	O
,	O
we	O
ﬁnd	O
that	O
˜wi	O
=	O
hi	O
,	O
i	O
i	O
was	O
nonzero	O
,	O
then	O
˜wi	O
remains	O
h	O
i	O
,	O
i+α	O
nonzero	O
.	O
this	O
demonstrates	O
that	O
l2	O
regularization	O
does	O
not	O
cause	O
the	O
parameters	O
to	O
become	O
sparse	O
,	O
while	O
l1	O
regularization	O
may	O
do	O
so	O
for	O
large	O
enough	O
.α	O
∗	O
i	O
.	O
if	O
w	O
7.13	O
w	O
the	O
sparsity	O
property	O
induced	O
by	O
l1	O
regularization	O
has	O
been	O
used	O
extensively	O
as	O
a	O
feature	O
selection	O
mechanism	O
.	O
feature	O
selection	O
simpliﬁes	O
a	O
machine	O
learning	O
problem	O
by	O
choosing	O
which	O
subset	O
of	O
the	O
available	O
features	O
should	O
be	O
used	O
.	O
in	O
particular	O
,	O
the	O
well	O
known	O
lasso	O
(	O
)	O
(	O
least	O
absolute	O
shrinkage	O
and	O
selection	O
operator	O
)	O
model	B
integrates	O
an	O
l1	O
penalty	O
with	O
a	O
linear	O
model	B
and	O
a	O
least	O
squares	O
cost	O
function	O
.	O
the	O
l1	O
penalty	O
causes	O
a	O
subset	O
of	O
the	O
weights	O
to	O
become	O
zero	O
,	O
suggesting	O
that	O
the	O
corresponding	O
features	O
may	O
safely	O
be	O
discarded	O
.	O
tibshirani	O
1995	O
,	O
5.6.1	O
in	O
section	O
,	O
we	O
saw	O
that	O
many	O
regularization	O
strategies	O
can	O
be	O
interpreted	O
as	O
map	O
bayesian	O
inference	O
,	O
and	O
that	O
in	O
particular	O
,	O
l2	O
regularization	O
is	O
equivalent	O
to	O
map	O
bayesian	O
inference	O
with	O
a	O
gaussian	O
prior	O
on	O
the	O
weights	O
.	O
for	O
l1	O
regu-	O
larization	O
,	O
the	O
penalty	O
αω	O
(	O
w	O
)	O
=	O
α	O
used	O
to	O
regularize	O
a	O
cost	O
function	O
is	O
equivalent	O
to	O
the	O
log-prior	O
term	O
that	O
is	O
maximized	O
by	O
map	O
bayesian	O
inference	O
n	O
:	O
when	O
the	O
prior	O
is	O
an	O
isotropic	O
laplace	O
distribution	O
(	O
equation	O
	O
|	O
wi	O
)	O
over	O
∈	O
w	O
|	O
i	O
r	O
	O
3.26	O
−	O
α	O
n	O
log	O
2	O
.	O
(	O
7.24	O
)	O
log	O
(	O
p	O
w	O
)	O
=	O
log	O
laplace	O
(	O
w	O
i	O
;	O
0	O
,	O
i	O
1	O
α	O
−	O
||	O
α	O
w	O
1	O
+	O
log	O
||	O
n	O
)	O
=	O
236	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
from	O
the	O
point	O
of	O
view	O
of	O
learning	O
via	O
maximization	O
with	O
respect	O
to	O
w	O
,	O
we	O
can	O
ignore	O
the	O
terms	O
because	O
they	O
do	O
not	O
depend	O
on	O
log	O
2	O
log	O
w	O
α	O
.	O
−	O
7.2	O
norm	O
penalties	O
as	O
constrained	O
optimization	O
consider	O
the	O
cost	O
function	O
regularized	O
by	O
a	O
parameter	O
norm	O
penalty	O
:	O
˜j	O
(	O
;	O
θ	O
x	O
y	O
)	O
=	O
(	O
;	O
θ	O
x	O
y	O
)	O
+	O
ω	O
(	O
)	O
θ	O
.	O
α	O
j	O
,	O
,	O
(	O
7.25	O
)	O
4.4	O
recall	O
from	O
section	O
that	O
we	O
can	O
minimize	O
a	O
function	O
subject	O
to	O
constraints	O
by	O
constructing	O
a	O
generalized	O
lagrange	O
function	O
,	O
consisting	O
of	O
the	O
original	O
objective	O
function	O
plus	O
a	O
set	O
of	O
penalties	O
.	O
each	O
penalty	O
is	O
a	O
product	O
between	O
a	O
coeﬃcient	O
,	O
called	O
a	O
karush–kuhn–tucker	O
(	O
kkt	O
)	O
multiplier	O
,	O
and	O
a	O
function	O
representing	O
whether	O
the	O
constraint	O
is	O
satisﬁed	O
.	O
if	O
we	O
wanted	O
to	O
constrain	O
ω	O
(	O
θ	O
)	O
to	O
be	O
less	O
than	O
some	O
constant	O
,	O
we	O
could	O
construct	O
a	O
generalized	O
lagrange	O
function	O
k	O
l	O
(	O
θ	O
,	O
α	O
x	O
y	O
,	O
;	O
)	O
=	O
(	O
;	O
j	O
θ	O
x	O
y	O
,	O
)	O
+	O
(	O
ω	O
(	O
)	O
θ	O
α	O
−	O
k	O
.	O
)	O
the	O
solution	O
to	O
the	O
constrained	O
problem	O
is	O
given	O
by	O
∗	O
θ	O
=	O
arg	O
min	O
θ	O
≥	O
max	O
α	O
,	O
α	O
0	O
l	O
(	O
)	O
θ	O
,	O
α	O
.	O
(	O
7.26	O
)	O
(	O
7.27	O
)	O
4.5	O
4.4	O
as	O
described	O
in	O
section	O
,	O
solving	O
this	O
problem	O
requires	O
modifying	O
both	O
provides	O
a	O
worked	O
example	O
of	O
linear	O
regression	O
with	O
an	O
θ	O
l	O
2	O
and	O
α.	O
section	O
constraint	O
.	O
many	O
diﬀerent	O
procedures	O
are	O
possible—some	O
may	O
use	O
gradient	O
descent	B
,	O
while	O
others	O
may	O
use	O
analytical	O
solutions	O
for	O
where	O
the	O
gradient	O
is	O
zero—but	O
in	O
all	O
procedures	O
α	O
must	O
increase	O
whenever	O
ω	O
(	O
θ	O
)	O
>	O
k	O
and	O
decrease	O
whenever	O
ω	O
(	O
θ	O
)	O
<	O
k	O
.	O
all	O
positive	O
α	O
encourage	O
ω	O
(	O
θ	O
)	O
to	O
shrink	O
.	O
the	O
optimal	O
value	O
α	O
will	O
encourage	O
ω	O
(	O
θ	O
)	O
become	O
less	O
than	O
.	O
to	O
shrink	O
,	O
but	O
not	O
so	O
strongly	O
to	O
make	O
k	O
ω	O
(	O
)	O
θ	O
∗	O
to	O
gain	O
some	O
insight	O
into	O
the	O
eﬀect	O
of	O
the	O
constraint	O
,	O
we	O
can	O
ﬁx	O
α	O
and	O
view	O
∗	O
the	O
problem	O
as	O
just	O
a	O
function	O
of	O
:	O
θ	O
l	O
∗	O
(	O
θ	O
,	O
α	O
∗	O
θ	O
=	O
arg	O
min	O
θ	O
j	O
(	O
;	O
θ	O
x	O
y	O
)	O
+	O
,	O
α	O
∗	O
ω	O
(	O
)	O
θ	O
.	O
(	O
7.28	O
)	O
)	O
=	O
arg	O
min	O
θ	O
this	O
is	O
exactly	O
the	O
same	O
as	O
the	O
regularized	O
training	O
problem	O
of	O
minimizing	O
˜j	O
.	O
we	O
can	O
thus	O
think	O
of	O
a	O
parameter	O
norm	O
penalty	O
as	O
imposing	O
a	O
constraint	O
on	O
the	O
l2	O
norm	O
,	O
then	O
the	O
weights	O
are	O
constrained	O
to	O
lie	O
in	O
an	O
l	O
2	O
weights	O
.	O
if	O
l1	O
norm	O
,	O
then	O
the	O
weights	O
are	O
constrained	O
to	O
lie	O
in	O
a	O
region	O
of	O
ball	O
.	O
if	O
is	O
the	O
is	O
the	O
ω	O
ω	O
237	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
limited	O
l1	O
norm	O
.	O
usually	O
we	O
do	O
not	O
know	O
the	O
size	O
of	O
the	O
constraint	O
region	O
that	O
we	O
∗	O
impose	O
by	O
using	O
weight	O
decay	O
with	O
coeﬃcient	O
α	O
does	O
not	O
directly	O
tell	O
us	O
the	O
value	O
of	O
k.	O
in	O
principle	O
,	O
one	O
can	O
solve	O
for	O
k	O
,	O
but	O
the	O
relationship	O
between	O
k	O
and	O
α	O
depends	O
on	O
the	O
form	O
of	O
j	O
.	O
while	O
we	O
do	O
not	O
know	O
the	O
exact	O
size	O
of	O
the	O
constraint	O
region	O
,	O
we	O
can	O
control	O
it	O
roughly	O
by	O
increasing	O
or	O
decreasing	O
α	O
in	O
order	O
to	O
grow	O
or	O
shrink	O
the	O
constraint	O
region	O
.	O
larger	O
α	O
will	O
result	O
in	O
a	O
smaller	O
constraint	O
region	O
.	O
smaller	O
will	O
result	O
in	O
a	O
larger	O
constraint	O
region	O
.	O
because	O
the	O
value	O
of	O
α	O
α	O
∗	O
∗	O
4.4	O
sometimes	O
we	O
may	O
wish	O
to	O
use	O
explicit	O
constraints	O
rather	O
than	O
penalties	O
.	O
as	O
described	O
in	O
section	O
,	O
we	O
can	O
modify	O
algorithms	O
such	O
as	O
stochastic	O
gradient	O
descent	B
to	O
take	O
a	O
step	O
downhill	O
on	O
j	O
(	O
θ	O
)	O
and	O
then	O
project	O
θ	O
back	O
to	O
the	O
nearest	O
point	O
that	O
satisﬁes	O
ω	O
(	O
θ	O
)	O
<	O
k.	O
this	O
can	O
be	O
useful	O
if	O
we	O
have	O
an	O
idea	O
of	O
what	O
value	O
of	O
k	O
is	O
appropriate	O
and	O
do	O
not	O
want	O
to	O
spend	O
time	O
searching	O
for	O
the	O
value	O
of	O
α	O
that	O
corresponds	O
to	O
this	O
.k	O
another	O
reason	O
to	O
use	O
explicit	O
constraints	O
and	O
reprojection	O
rather	O
than	O
enforcing	O
constraints	O
with	O
penalties	O
is	O
that	O
penalties	O
can	O
cause	O
non-convex	O
optimization	O
procedures	O
to	O
get	O
stuck	O
in	O
local	O
minima	O
corresponding	O
to	O
small	O
θ.	O
when	O
training	O
neural	O
networks	O
,	O
this	O
usually	O
manifests	O
as	O
neural	O
networks	O
that	O
train	O
with	O
several	O
“	O
dead	O
units.	O
”	O
these	O
are	O
units	O
that	O
do	O
not	O
contribute	O
much	O
to	O
the	O
behavior	O
of	O
the	O
function	O
learned	O
by	O
the	O
network	O
because	O
the	O
weights	O
going	O
into	O
or	O
out	O
of	O
them	O
are	O
all	O
very	O
small	O
.	O
when	O
training	O
with	O
a	O
penalty	O
on	O
the	O
norm	O
of	O
the	O
weights	O
,	O
these	O
conﬁgurations	O
can	O
be	O
locally	O
optimal	O
,	O
even	O
if	O
it	O
is	O
possible	O
to	O
signiﬁcantly	O
reduce	O
j	O
by	O
making	O
the	O
weights	O
larger	O
.	O
explicit	O
constraints	O
implemented	O
by	O
re-projection	O
can	O
work	B
much	O
better	O
in	O
these	O
cases	O
because	O
they	O
do	O
not	O
encourage	O
the	O
weights	O
to	O
approach	O
the	O
origin	O
.	O
explicit	O
constraints	O
implemented	O
by	O
re-projection	O
only	O
have	O
an	O
eﬀect	O
when	O
the	O
weights	O
become	O
large	O
and	O
attempt	O
to	O
leave	O
the	O
constraint	O
region	O
.	O
finally	O
,	O
explicit	O
constraints	O
with	O
reprojection	O
can	O
be	O
useful	O
because	O
they	O
impose	O
some	O
stability	O
on	O
the	O
optimization	O
procedure	O
.	O
when	O
using	O
high	O
learning	O
rates	O
,	O
it	O
is	O
possible	O
to	O
encounter	O
a	O
positive	O
feedback	O
loop	O
in	O
which	O
large	O
weights	O
induce	O
large	O
gradients	O
which	O
then	O
induce	O
a	O
large	O
update	O
to	O
the	O
weights	O
.	O
if	O
these	O
updates	O
consistently	O
increase	O
the	O
size	O
of	O
the	O
weights	O
,	O
then	O
θ	O
rapidly	O
moves	O
away	O
from	O
the	O
origin	O
until	O
numerical	O
overﬂow	O
occurs	O
.	O
explicit	O
constraints	O
with	O
reprojection	O
prevent	O
this	O
feedback	O
loop	O
from	O
continuing	O
to	O
increase	O
the	O
magnitude	O
of	O
the	O
weights	O
without	O
bound.	O
)	O
recommend	O
using	O
constraints	O
combined	O
with	O
a	O
high	O
learning	O
rate	O
to	O
allow	O
rapid	O
exploration	O
of	O
parameter	O
space	O
while	O
maintaining	O
some	O
stability	O
.	O
hinton	O
et	O
al	O
.	O
2012c	O
(	O
in	O
particular	O
,	O
hinton	O
and	O
shraibman	O
2005	O
(	O
et	O
al	O
.	O
(	O
)	O
recommend	O
a	O
strategy	O
introduced	O
by	O
srebro	O
)	O
:	O
constraining	O
the	O
norm	O
of	O
each	O
column	O
of	O
the	O
weight	O
matrix	O
2012c	O
238	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
of	O
a	O
neural	O
net	O
layer	O
,	O
rather	O
than	O
constraining	O
the	O
frobenius	O
norm	O
of	O
the	O
entire	O
weight	O
matrix	O
.	O
constraining	O
the	O
norm	O
of	O
each	O
column	O
separately	O
prevents	O
any	O
one	O
hidden	O
unit	O
from	O
having	O
very	O
large	O
weights	O
.	O
if	O
we	O
converted	O
this	O
constraint	O
into	O
a	O
penalty	O
in	O
a	O
lagrange	O
function	O
,	O
it	O
would	O
be	O
similar	O
to	O
l2	O
weight	O
decay	O
but	O
with	O
a	O
separate	O
kkt	O
multiplier	O
for	O
the	O
weights	O
of	O
each	O
hidden	O
unit	O
.	O
each	O
of	O
these	O
kkt	O
multipliers	O
would	O
be	O
dynamically	O
updated	O
separately	O
to	O
make	O
each	O
hidden	O
unit	O
obey	O
the	O
constraint	O
.	O
in	O
practice	O
,	O
column	O
norm	O
limitation	O
is	O
always	O
implemented	O
as	O
an	O
explicit	O
constraint	O
with	O
reprojection	O
.	O
7.3	O
regularization	O
and	O
under-constrained	O
problems	O
in	O
some	O
cases	O
,	O
regularization	O
is	O
necessary	O
for	O
machine	O
learning	O
problems	O
to	O
be	O
prop-	O
erly	O
deﬁned	O
.	O
many	O
linear	O
models	O
in	O
machine	O
learning	O
,	O
including	O
linear	O
regression	O
and	O
pca	O
,	O
depend	O
on	O
inverting	O
the	O
matrix	O
x	O
x.	O
this	O
is	O
not	O
possible	O
whenever	O
x	O
x	O
is	O
singular	O
.	O
this	O
matrix	O
can	O
be	O
singular	O
whenever	O
the	O
data	O
generating	O
distri-	O
bution	O
truly	O
has	O
no	O
variance	O
in	O
some	O
direction	O
,	O
or	O
when	O
no	O
variance	O
is	O
observed	O
in	O
some	O
direction	O
because	O
there	O
are	O
fewer	O
examples	O
(	O
rows	O
of	O
x	O
)	O
than	O
input	O
features	O
(	O
columns	O
of	O
x	O
)	O
.	O
in	O
this	O
case	O
,	O
many	O
forms	O
of	O
regularization	O
correspond	O
to	O
inverting	O
x	O
i+	O
α	O
instead	O
.	O
this	O
regularized	O
matrix	O
is	O
guaranteed	O
to	O
be	O
invertible	O
.	O
	O
	O
	O
x	O
these	O
linear	O
problems	O
have	O
closed	O
form	O
solutions	O
when	O
the	O
relevant	O
matrix	O
is	O
invertible	O
.	O
it	O
is	O
also	O
possible	O
for	O
a	O
problem	O
with	O
no	O
closed	O
form	O
solution	O
to	O
be	O
underdetermined	O
.	O
an	O
example	O
is	O
logistic	O
regression	O
applied	O
to	O
a	O
problem	O
where	O
the	O
classes	O
are	O
linearly	O
separable	O
.	O
if	O
a	O
weight	O
vector	O
w	O
is	O
able	O
to	O
achieve	O
perfect	O
classiﬁcation	O
,	O
then	O
2w	O
will	O
also	O
achieve	O
perfect	O
classiﬁcation	O
and	O
higher	O
likelihood	O
.	O
an	O
iterative	O
optimization	O
procedure	O
like	O
stochastic	O
gradient	O
descent	B
will	O
continually	O
increase	O
the	O
magnitude	O
of	O
w	O
and	O
,	O
in	O
theory	O
,	O
will	O
never	O
halt	O
.	O
in	O
practice	O
,	O
a	O
numerical	O
implementation	O
of	O
gradient	O
descent	B
will	O
eventually	O
reach	O
suﬃciently	O
large	O
weights	O
to	O
cause	O
numerical	O
overﬂow	O
,	O
at	O
which	O
point	O
its	O
behavior	O
will	O
depend	O
on	O
how	O
the	O
programmer	O
has	O
decided	O
to	O
handle	O
values	O
that	O
are	O
not	O
real	O
numbers	O
.	O
most	O
forms	O
of	O
regularization	O
are	O
able	O
to	O
guarantee	O
the	O
convergence	O
of	O
iterative	O
methods	O
applied	O
to	O
underdetermined	O
problems	O
.	O
for	O
example	O
,	O
weight	O
decay	O
will	O
cause	O
gradient	O
descent	B
to	O
quit	O
increasing	O
the	O
magnitude	O
of	O
the	O
weights	O
when	O
the	O
slope	O
of	O
the	O
likelihood	O
is	O
equal	O
to	O
the	O
weight	O
decay	O
coeﬃcient	O
.	O
the	O
idea	O
of	O
using	O
regularization	O
to	O
solve	O
underdetermined	O
problems	O
extends	O
beyond	O
machine	O
learning	O
.	O
the	O
same	O
idea	O
is	O
useful	O
for	O
several	O
basic	O
linear	O
algebra	O
problems	O
.	O
as	O
we	O
saw	O
in	O
section	O
2.9	O
,	O
we	O
can	O
solve	O
underdetermined	O
linear	O
equations	O
using	O
239	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
the	O
moore-penrose	O
pseudoinverse	O
.	O
recall	O
that	O
one	O
deﬁnition	O
of	O
the	O
pseudoinverse	O
x+	O
of	O
a	O
matrix	O
isx	O
−	O
1x	O
i+	O
α	O
)	O
	O
.	O
(	O
7.29	O
)	O
x+	O
=	O
lim	O
	O
(	O
x	O
0	O
α	O
	O
x	O
we	O
can	O
now	O
recognize	O
equation	O
as	O
performing	O
linear	O
regression	O
with	O
weight	O
decay	O
.	O
speciﬁcally	O
,	O
equation	O
as	O
the	O
regularization	O
7.29	O
coeﬃcient	O
shrinks	O
to	O
zero	O
.	O
we	O
can	O
thus	O
interpret	O
the	O
pseudoinverse	O
as	O
stabilizing	O
underdetermined	O
problems	O
using	O
regularization	O
.	O
7.29	O
is	O
the	O
limit	O
of	O
equation	O
7.17	O
7.4	O
dataset	O
augmentation	O
the	O
best	O
way	O
to	O
make	O
a	O
machine	O
learning	O
model	B
generalize	O
better	O
is	O
to	O
train	O
it	O
on	O
more	O
data	O
.	O
of	O
course	O
,	O
in	O
practice	O
,	O
the	O
amount	O
of	O
data	O
we	O
have	O
is	O
limited	O
.	O
one	O
way	O
to	O
get	O
around	O
this	O
problem	O
is	O
to	O
create	O
fake	O
data	O
and	O
add	O
it	O
to	O
the	O
training	O
set	O
.	O
for	O
some	O
machine	O
learning	O
tasks	O
,	O
it	O
is	O
reasonably	O
straightforward	O
to	O
create	O
new	O
fake	O
data	O
.	O
this	O
approach	O
is	O
easiest	O
for	O
classiﬁcation	O
.	O
a	O
classiﬁer	O
needs	O
to	O
take	O
a	O
compli-	O
cated	O
,	O
high	O
dimensional	O
input	O
x	O
and	O
summarize	O
it	O
with	O
a	O
single	O
category	O
identity	O
y.	O
this	O
means	O
that	O
the	O
main	O
task	O
facing	O
a	O
classiﬁer	O
is	O
to	O
be	O
invariant	O
to	O
a	O
wide	O
variety	O
of	O
transformations	O
.	O
we	O
can	O
generate	O
new	O
(	O
x	O
,	O
y	O
)	O
pairs	O
easily	O
just	O
by	O
transforming	O
the	O
inputs	O
in	O
our	O
training	O
set	O
.	O
x	O
this	O
approach	O
is	O
not	O
as	O
readily	O
applicable	O
to	O
many	O
other	O
tasks	O
.	O
for	O
example	O
,	O
it	O
is	O
diﬃcult	O
to	O
generate	O
new	O
fake	O
data	O
for	O
a	O
density	O
estimation	O
task	O
unless	O
we	O
have	O
already	O
solved	O
the	O
density	O
estimation	O
problem	O
.	O
dataset	O
augmentation	O
has	O
been	O
a	O
particularly	O
eﬀective	O
technique	O
for	O
a	O
speciﬁc	O
classiﬁcation	O
problem	O
:	O
object	O
recognition	B
.	O
images	O
are	O
high	O
dimensional	O
and	O
include	O
an	O
enormous	O
variety	O
of	O
factors	O
of	O
variation	O
,	O
many	O
of	O
which	O
can	O
be	O
easily	O
simulated	O
.	O
operations	O
like	O
translating	O
the	O
training	O
images	O
a	O
few	O
pixels	O
in	O
each	O
direction	O
can	O
often	O
greatly	O
improve	O
generalization	O
,	O
even	O
if	O
the	O
model	B
has	O
already	O
been	O
designed	O
to	O
be	O
partially	O
translation	O
invariant	O
by	O
using	O
the	O
convolution	O
and	O
pooling	O
techniques	O
described	O
in	O
chapter	O
.	O
many	O
other	O
operations	O
such	O
as	O
rotating	O
the	O
image	O
or	O
scaling	O
the	O
image	O
have	O
also	O
proven	O
quite	O
eﬀective	O
.	O
9	O
one	O
must	O
be	O
careful	O
not	O
to	O
apply	O
transformations	O
that	O
would	O
change	O
the	O
correct	O
class	O
.	O
for	O
example	O
,	O
optical	O
character	O
recognition	B
tasks	O
require	O
recognizing	O
the	O
diﬀerence	O
between	O
‘	O
b	O
’	O
and	O
‘	O
d	O
’	O
and	O
the	O
diﬀerence	O
between	O
‘	O
6	O
’	O
and	O
‘	O
9	O
’	O
,	O
so	O
horizontal	O
◦	O
ﬂips	O
and	O
180	O
rotations	O
are	O
not	O
appropriate	O
ways	O
of	O
augmenting	O
datasets	O
for	O
these	O
tasks	O
.	O
240	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
there	O
are	O
also	O
transformations	O
that	O
we	O
would	O
like	O
our	O
classiﬁers	O
to	O
be	O
invariant	O
to	O
,	O
but	O
which	O
are	O
not	O
easy	O
to	O
perform	O
.	O
for	O
example	O
,	O
out-of-plane	O
rotation	O
can	O
not	O
be	O
implemented	O
as	O
a	O
simple	O
geometric	O
operation	O
on	O
the	O
input	O
pixels	O
.	O
dataset	O
augmentation	O
is	O
eﬀective	O
for	O
speech	O
recognition	B
tasks	O
as	O
well	O
(	O
jaitly	O
and	O
hinton	O
2013	O
,	O
)	O
.	O
,	O
,	O
injecting	O
noise	O
in	O
the	O
input	O
to	O
a	O
neural	O
network	O
(	O
sietsma	O
and	O
dow	O
1991	O
)	O
can	O
also	O
be	O
seen	O
as	O
a	O
form	O
of	O
data	O
augmentation	O
.	O
for	O
many	O
classiﬁcation	O
and	O
even	O
some	O
regression	O
tasks	O
,	O
the	O
task	O
should	O
still	O
be	O
possible	O
to	O
solve	O
even	O
if	O
small	O
random	O
noise	O
is	O
added	O
to	O
the	O
input	O
.	O
neural	O
networks	O
prove	O
not	O
to	O
be	O
very	O
robust	O
to	O
noise	O
,	O
however	O
(	O
tang	O
and	O
eliasmith	O
2010	O
)	O
.	O
one	O
way	O
to	O
improve	O
the	O
robustness	O
of	O
neural	O
networks	O
is	O
simply	O
to	O
train	O
them	O
with	O
random	O
noise	O
applied	O
to	O
their	O
inputs	O
.	O
input	O
noise	O
injection	O
is	O
part	O
of	O
some	O
unsupervised	O
learning	O
algorithms	O
such	O
as	O
the	O
denoising	O
autoencoder	O
(	O
vincent	O
)	O
.	O
noise	O
injection	O
also	O
works	O
when	O
the	O
noise	O
is	O
applied	O
to	O
the	O
hidden	O
units	O
,	O
which	O
can	O
be	O
seen	O
as	O
doing	O
dataset	O
augmentation	O
at	O
multiple	O
levels	O
of	O
abstraction	O
.	O
poole	O
)	O
recently	O
showed	O
that	O
this	O
approach	O
can	O
be	O
highly	O
eﬀective	O
provided	O
that	O
the	O
magnitude	O
of	O
the	O
noise	O
is	O
carefully	O
tuned	O
.	O
dropout	O
,	O
a	O
powerful	O
regularization	O
strategy	O
that	O
will	O
be	O
described	O
in	O
section	O
,	O
can	O
be	O
seen	O
as	O
a	O
process	O
of	O
constructing	O
new	O
inputs	O
by	O
multiplying	O
by	O
noise	O
.	O
et	O
al	O
.	O
(	O
et	O
al.	O
,	O
7.12	O
2008	O
2014	O
when	O
comparing	O
machine	O
learning	O
benchmark	O
results	O
,	O
it	O
is	O
important	O
to	O
take	O
the	O
eﬀect	O
of	O
dataset	O
augmentation	O
into	O
account	O
.	O
often	O
,	O
hand-designed	O
dataset	O
augmentation	O
schemes	O
can	O
dramatically	O
reduce	O
the	O
generalization	O
error	O
of	O
a	O
machine	O
learning	O
technique	O
.	O
to	O
compare	O
the	O
performance	O
of	O
one	O
machine	O
learning	O
algorithm	O
to	O
another	O
,	O
it	O
is	O
necessary	O
to	O
perform	O
controlled	O
experiments	O
.	O
when	O
comparing	O
machine	O
learning	O
algorithm	O
a	O
and	O
machine	O
learning	O
algorithm	O
b	O
,	O
it	O
is	O
necessary	O
to	O
make	O
sure	O
that	O
both	O
algorithms	O
were	O
evaluated	O
using	O
the	O
same	O
hand-designed	O
dataset	O
augmentation	O
schemes	O
.	O
suppose	O
that	O
algorithm	O
a	O
performs	O
poorly	O
with	O
no	O
dataset	O
augmentation	O
and	O
algorithm	O
b	O
performs	O
well	O
when	O
combined	O
with	O
numerous	O
synthetic	O
transformations	O
of	O
the	O
input	O
.	O
in	O
such	O
a	O
case	O
it	O
is	O
likely	O
the	O
synthetic	O
transformations	O
caused	O
the	O
improved	O
performance	O
,	O
rather	O
than	O
the	O
use	O
of	O
machine	O
learning	O
algorithm	O
b.	O
sometimes	O
deciding	O
whether	O
an	O
experiment	O
has	O
been	O
properly	O
controlled	O
requires	O
subjective	O
judgment	O
.	O
for	O
example	O
,	O
machine	O
learning	O
algorithms	O
that	O
inject	O
noise	O
into	O
the	O
input	O
are	O
performing	O
a	O
form	O
of	O
dataset	O
augmentation	O
.	O
usually	O
,	O
operations	O
that	O
are	O
generally	O
applicable	O
(	O
such	O
as	O
adding	O
gaussian	O
noise	O
to	O
the	O
input	O
)	O
are	O
considered	O
part	O
of	O
the	O
machine	O
learning	O
algorithm	O
,	O
while	O
operations	O
that	O
are	O
speciﬁc	O
to	O
one	O
application	O
domain	O
(	O
such	O
as	O
randomly	O
cropping	O
an	O
image	O
)	O
are	O
considered	O
to	O
be	O
separate	O
pre-processing	O
steps	O
.	O
241	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
7.5	O
noise	O
robustness	O
7.4	O
section	O
has	O
motivated	O
the	O
use	O
of	O
noise	O
applied	O
to	O
the	O
inputs	O
as	O
a	O
dataset	O
augmentation	O
strategy	O
.	O
for	O
some	O
models	O
,	O
the	O
addition	O
of	O
noise	O
with	O
inﬁnitesimal	O
variance	O
at	O
the	O
input	O
of	O
the	O
model	B
is	O
equivalent	O
to	O
imposing	O
a	O
penalty	O
on	O
the	O
norm	O
of	O
the	O
weights	O
(	O
,	O
)	O
.	O
in	O
the	O
general	O
case	O
,	O
it	O
is	O
important	O
to	O
remember	O
that	O
noise	O
injection	O
can	O
be	O
much	O
more	O
powerful	O
than	O
simply	O
shrinking	O
the	O
parameters	O
,	O
especially	O
when	O
the	O
noise	O
is	O
added	O
to	O
the	O
hidden	O
units	O
.	O
noise	O
applied	O
to	O
the	O
hidden	O
units	O
is	O
such	O
an	O
important	O
topic	O
that	O
it	O
merit	O
its	O
own	O
separate	O
discussion	O
;	O
the	O
dropout	O
algorithm	O
described	O
in	O
section	O
is	O
the	O
main	O
development	O
of	O
that	O
approach	O
.	O
bishop	O
1995a	O
b	O
7.12	O
,	O
another	O
way	O
that	O
noise	O
has	O
been	O
used	O
in	O
the	O
service	O
of	O
regularizing	O
models	O
is	O
by	O
adding	O
it	O
to	O
the	O
weights	O
.	O
this	O
technique	O
has	O
been	O
used	O
primarily	O
in	O
the	O
context	O
of	O
recurrent	O
neural	O
networks	O
(	O
)	O
.	O
this	O
can	O
be	O
interpreted	O
as	O
a	O
stochastic	O
implementation	O
of	O
bayesian	O
inference	O
over	O
the	O
weights	O
.	O
the	O
bayesian	O
treatment	O
of	O
learning	O
would	O
consider	O
the	O
model	B
weights	O
to	O
be	O
uncertain	O
and	O
representable	O
via	O
a	O
probability	O
distribution	O
that	O
reﬂects	O
this	O
uncertainty	O
.	O
adding	O
noise	O
to	O
the	O
weights	O
is	O
a	O
practical	O
,	O
stochastic	O
way	O
to	O
reﬂect	O
this	O
uncertainty	O
.	O
jim	O
et	O
al	O
.	O
1996	O
graves	O
2011	O
,	O
;	O
,	O
noise	O
applied	O
to	O
the	O
weights	O
can	O
also	O
be	O
interpreted	O
as	O
equivalent	O
(	O
under	O
some	O
assumptions	O
)	O
to	O
a	O
more	O
traditional	O
form	O
of	O
regularization	O
,	O
encouraging	O
stability	O
of	O
the	O
function	O
to	O
be	O
learned	O
.	O
consider	O
the	O
regression	O
setting	O
,	O
where	O
we	O
wish	O
to	O
train	O
a	O
function	O
ˆy	O
(	O
x	O
)	O
that	O
maps	O
a	O
set	O
of	O
features	O
x	O
to	O
a	O
scalar	O
using	O
the	O
least-squares	O
cost	O
function	O
between	O
the	O
model	B
predictions	O
ˆy	O
(	O
)	O
x	O
and	O
the	O
true	O
values	O
	O
	O
:	O
y	O
j	O
=	O
ep	O
x	O
,	O
y	O
(	O
)	O
(	O
ˆy	O
(	O
)	O
x	O
−	O
.	O
)	O
2	O
y	O
{	O
(	O
x	O
(	O
1	O
)	O
,	O
y	O
(	O
1	O
)	O
)	O
(	O
7.30	O
)	O
}	O
)	O
m	O
)	O
.	O
the	O
training	O
set	O
consists	O
of	O
m	O
labeled	O
examples	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
(	O
)	O
m	O
,	O
y	O
(	O
∼	O
n	O
we	O
now	O
assume	O
that	O
with	O
each	O
input	O
presentation	O
we	O
also	O
include	O
a	O
random	O
(	O
	O
;	O
0	O
,	O
ηi	O
)	O
of	O
the	O
network	O
weights	O
.	O
let	O
us	O
imagine	O
that	O
we	O
perturbation	O
w	O
have	O
a	O
standard	O
l-layer	O
mlp	O
.	O
we	O
denote	O
the	O
perturbed	O
model	B
as	O
ˆy	O
(	O
x	O
)	O
.	O
despite	O
the	O
injection	O
of	O
noise	O
,	O
we	O
are	O
still	O
interested	O
in	O
minimizing	O
the	O
squared	O
error	O
of	O
the	O
output	O
of	O
the	O
network	O
.	O
the	O
objective	O
function	O
thus	O
becomes	O
:	O
	O
w	O
	O
	O
	O
˜jw	O
=	O
ep	O
=	O
ep	O
,	O
y	O
,	O
(	O
x	O
	O
,	O
y	O
,	O
(	O
x	O
	O
)	O
)	O
w	O
w	O
w	O
(	O
ˆy	O
ˆy	O
2	O
	O
w	O
−	O
(	O
)	O
x	O
−	O
y	O
2	O
)	O
(	O
)	O
x	O
2	O
ˆ	O
yy	O
(	O
)	O
+x	O
y	O
2	O
.	O
w	O
(	O
7.31	O
)	O
(	O
7.32	O
)	O
for	O
small	O
η	O
,	O
the	O
minimization	O
of	O
j	O
with	O
added	O
weight	O
noise	O
(	O
with	O
covariance	O
ηi	O
)	O
is	O
equivalent	O
to	O
minimization	O
of	O
j	O
with	O
an	O
additional	O
regularization	O
term	O
:	O
242	O
	O
	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
∇	O
	O
w	O
ˆy	O
(	O
)	O
x	O
2	O
,	O
y	O
(	O
x	O
)	O
	O
	O
ηep	O
.	O
this	O
form	O
of	O
regularization	O
encourages	O
the	O
parameters	O
to	O
go	O
to	O
regions	O
of	O
parameter	O
space	O
where	O
small	O
perturbations	O
of	O
the	O
weights	O
have	O
a	O
relatively	O
small	O
inﬂuence	O
on	O
the	O
output	O
.	O
in	O
other	O
words	O
,	O
it	O
pushes	O
the	O
model	B
into	O
regions	O
where	O
the	O
model	B
is	O
relatively	O
insensitive	O
to	O
small	O
variations	O
in	O
the	O
weights	O
,	O
ﬁnding	O
points	O
that	O
are	O
not	O
merely	O
minima	O
,	O
but	O
minima	O
surrounded	O
by	O
,	O
ﬂat	O
regions	O
(	O
hochreiter	O
and	O
schmidhuber	O
1995	O
)	O
.	O
in	O
the	O
simpliﬁed	O
case	O
of	O
linear	O
	O
x	O
+	O
b	O
)	O
,	O
this	O
regularization	O
term	O
collapses	O
regression	O
(	O
where	O
,	O
for	O
instance	O
,	O
ˆy	O
(	O
x	O
)	O
=	O
w	O
into	O
ηep	O
(	O
)	O
x	O
,	O
which	O
is	O
not	O
a	O
function	O
of	O
parameters	O
and	O
therefore	O
does	O
not	O
contribute	O
to	O
the	O
gradient	O
of	O
˜jw	O
with	O
respect	O
to	O
the	O
model	B
parameters	O
.	O
	O
	O
x	O
2	O
7.5.1	O
injecting	O
noise	O
at	O
the	O
output	O
targets	O
|	O
−	O
most	O
datasets	O
have	O
some	O
amount	O
of	O
mistakes	O
in	O
the	O
y	O
labels	O
.	O
it	O
can	O
be	O
harmful	O
to	O
maximize	O
log	O
p	O
(	O
y	O
x	O
)	O
when	O
y	O
is	O
a	O
mistake	O
.	O
one	O
way	O
to	O
prevent	O
this	O
is	O
to	O
explicitly	O
model	B
the	O
noise	O
on	O
the	O
labels	O
.	O
for	O
example	O
,	O
we	O
can	O
assume	O
that	O
for	O
some	O
small	O
constant	O
	O
,	O
the	O
training	O
set	O
label	O
y	O
is	O
correct	O
with	O
probability	O
1	O
	O
,	O
and	O
otherwise	O
any	O
of	O
the	O
other	O
possible	O
labels	O
might	O
be	O
correct	O
.	O
this	O
assumption	O
is	O
easy	O
to	O
incorporate	O
into	O
the	O
cost	O
function	O
analytically	O
,	O
rather	O
than	O
by	O
explicitly	O
drawing	O
noise	O
samples	O
.	O
for	O
example	O
,	O
label	O
smoothing	O
regularizes	O
a	O
model	B
based	O
on	O
a	O
softmax	O
with	O
k	O
output	O
values	O
by	O
replacing	O
the	O
hard	O
classiﬁcation	O
targets	O
	O
,	O
respectively	O
.	O
the	O
standard	O
cross-entropy	O
loss	O
may	O
with	O
targets	O
of	O
then	O
be	O
used	O
with	O
these	O
soft	O
targets	O
.	O
maximum	O
likelihood	O
learning	O
with	O
a	O
softmax	O
classiﬁer	O
and	O
hard	O
targets	O
may	O
actually	O
never	O
converge—the	O
softmax	O
can	O
never	O
predict	O
a	O
probability	O
of	O
exactly	O
or	O
exactly	O
,	O
so	O
it	O
will	O
continue	O
to	O
learn	O
larger	O
and	O
larger	O
weights	O
,	O
making	O
more	O
extreme	O
predictions	O
forever	O
.	O
it	O
is	O
possible	O
to	O
prevent	O
this	O
scenario	O
using	O
other	O
regularization	O
strategies	O
like	O
weight	O
decay	O
.	O
label	O
smoothing	O
has	O
the	O
advantage	O
of	O
preventing	O
the	O
pursuit	O
of	O
hard	O
probabilities	O
without	O
discouraging	O
correct	O
classiﬁcation	O
.	O
this	O
strategy	O
has	O
been	O
used	O
since	O
the	O
1980s	O
and	O
continues	O
to	O
be	O
featured	O
prominently	O
in	O
modern	O
neural	O
networks	O
(	O
szegedy	O
et	O
al.	O
,	O
−	O
	O
1	O
k	O
and	O
1	O
2015	O
and	O
−	O
)	O
.	O
0	O
1	O
0	O
1	O
7.6	O
semi-supervised	O
learning	O
in	O
the	O
paradigm	O
of	O
semi-supervised	O
learning	O
,	O
both	O
unlabeled	O
examples	O
from	O
p	O
(	O
x	O
)	O
and	O
labeled	O
examples	O
from	O
p	O
(	O
x	O
y	O
,	O
)	O
or	O
predict	O
y	O
from	O
x	O
.	O
|	O
)	O
are	O
used	O
to	O
estimate	O
p	O
(	O
y	O
x	O
in	O
the	O
context	O
of	O
deep	O
learning	O
,	O
semi-supervised	O
learning	O
usually	O
refers	O
to	O
learning	O
a	O
representation	O
h	O
=	O
f	O
(	O
x	O
)	O
.	O
the	O
goal	O
is	O
to	O
learn	O
a	O
representation	O
so	O
243	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
that	O
examples	O
from	O
the	O
same	O
class	O
have	O
similar	O
representations	O
.	O
unsupervised	O
learning	O
can	O
provide	O
useful	O
cues	O
for	O
how	O
to	O
group	O
examples	O
in	O
representation	O
space	O
.	O
examples	O
that	O
cluster	O
tightly	O
in	O
the	O
input	O
space	O
should	O
be	O
mapped	O
to	O
similar	O
representations	O
.	O
a	O
linear	O
classiﬁer	O
in	O
the	O
new	O
space	O
may	O
achieve	O
better	O
generalization	O
in	O
many	O
cases	O
(	O
belkin	O
and	O
niyogi	O
2002	O
chapelle	O
)	O
.	O
a	O
long-standing	O
variant	O
of	O
this	O
approach	O
is	O
the	O
application	O
of	O
principal	O
components	O
analysis	O
as	O
a	O
pre-processing	O
step	O
before	O
applying	O
a	O
classiﬁer	O
(	O
on	O
the	O
projected	O
data	O
)	O
.	O
et	O
al.	O
,	O
2003	O
,	O
;	O
|	O
|	O
−	O
−	O
log	O
p	O
(	O
x	O
)	O
or	O
log	O
p	O
(	O
y	O
x	O
instead	O
of	O
having	O
separate	O
unsupervised	O
and	O
supervised	O
components	O
in	O
the	O
model	B
,	O
one	O
can	O
construct	O
models	O
in	O
which	O
a	O
generative	O
model	B
of	O
either	O
p	O
(	O
x	O
)	O
or	O
p	O
(	O
x	O
y	O
,	O
)	O
shares	O
parameters	O
with	O
a	O
discriminative	O
model	B
of	O
p	O
(	O
y	O
x	O
)	O
.	O
one	O
can	O
−	O
then	O
trade-oﬀ	O
the	O
supervised	O
criterion	O
)	O
with	O
the	O
unsupervised	O
or	O
)	O
)	O
.	O
the	O
generative	O
criterion	O
then	O
generative	O
one	O
(	O
such	O
as	O
expresses	O
a	O
particular	O
form	O
of	O
prior	O
belief	O
about	O
the	O
solution	O
to	O
the	O
supervised	O
p	O
(	O
x	O
)	O
is	O
learning	O
problem	O
(	O
connected	O
to	O
the	O
structure	O
of	O
p	O
(	O
y	O
x	O
)	O
in	O
a	O
way	O
that	O
is	O
captured	O
by	O
the	O
shared	O
parametrization	O
.	O
by	O
controlling	O
how	O
much	O
of	O
the	O
generative	O
criterion	O
is	O
included	O
in	O
the	O
total	O
criterion	O
,	O
one	O
can	O
ﬁnd	O
a	O
better	O
trade-oﬀ	O
than	O
with	O
a	O
purely	O
generative	O
or	O
a	O
purely	O
discriminative	O
training	O
criterion	O
(	O
lasserre	O
et	O
al	O
.	O
2006	O
larochelle	O
and	O
bengio	O
2008	O
)	O
,	O
namely	O
that	O
the	O
structure	O
of	O
lasserre	O
et	O
al	O
.	O
2006	O
log	O
p	O
(	O
x	O
y	O
,	O
)	O
.	O
,	O
|	O
,	O
;	O
,	O
salakhutdinov	O
and	O
hinton	O
2008	O
)	O
describe	O
a	O
method	O
for	O
learning	O
the	O
kernel	O
function	O
of	O
a	O
kernel	O
machine	O
used	O
for	O
regression	O
,	O
in	O
which	O
the	O
usage	O
of	O
unlabeled	O
examples	O
for	O
modeling	O
quite	O
signiﬁcantly	O
.	O
|	O
y	O
x	O
)	O
improves	O
p	O
(	O
)	O
x	O
p	O
(	O
(	O
see	O
chapelle	O
et	O
al	O
.	O
2006	O
(	O
)	O
for	O
more	O
information	O
about	O
semi-supervised	O
learning	O
.	O
7.7	O
multi-task	O
learning	O
,	O
caruana	O
1993	O
multi-task	O
learning	O
(	O
)	O
is	O
a	O
way	O
to	O
improve	O
generalization	O
by	O
pooling	O
the	O
examples	O
(	O
which	O
can	O
be	O
seen	O
as	O
soft	O
constraints	O
imposed	O
on	O
the	O
parameters	O
)	O
arising	O
out	O
of	O
several	O
tasks	O
.	O
in	O
the	O
same	O
way	O
that	O
additional	O
training	O
examples	O
put	O
more	O
pressure	O
on	O
the	O
parameters	O
of	O
the	O
model	B
towards	O
values	O
that	O
generalize	O
well	O
,	O
when	O
part	O
of	O
a	O
model	B
is	O
shared	O
across	O
tasks	O
,	O
that	O
part	O
of	O
the	O
model	B
is	O
more	O
constrained	O
towards	O
good	O
values	O
(	O
assuming	O
the	O
sharing	O
is	O
justiﬁed	O
)	O
,	O
often	O
yielding	O
better	O
generalization	O
.	O
7.2	O
figure	O
illustrates	O
a	O
very	O
common	O
form	O
of	O
multi-task	O
learning	O
,	O
in	O
which	O
diﬀerent	O
supervised	O
tasks	O
(	O
predicting	O
y	O
(	O
)	O
i	O
given	O
x	O
)	O
share	O
the	O
same	O
input	O
x	O
,	O
as	O
well	O
as	O
some	O
intermediate-level	O
representation	O
h	O
(	O
shared	O
)	O
capturing	O
a	O
common	O
pool	O
of	O
244	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
factors	O
.	O
the	O
model	B
can	O
generally	O
be	O
divided	O
into	O
two	O
kinds	O
of	O
parts	O
and	O
associated	O
parameters	O
:	O
1.	O
task-speciﬁc	O
parameters	O
(	O
which	O
only	O
beneﬁt	O
from	O
the	O
examples	O
of	O
their	O
task	O
to	O
achieve	O
good	O
generalization	O
)	O
.	O
these	O
are	O
the	O
upper	O
layers	O
of	O
the	O
neural	O
network	O
in	O
ﬁgure	O
.7.2	O
2.	O
generic	O
parameters	O
,	O
shared	O
across	O
all	O
the	O
tasks	O
(	O
which	O
beneﬁt	O
from	O
the	O
pooled	O
data	O
of	O
all	O
the	O
tasks	O
)	O
.	O
these	O
are	O
the	O
lower	O
layers	O
of	O
the	O
neural	O
network	O
in	O
ﬁgure	O
.7.2	O
y	O
(	O
1	O
)	O
y	O
(	O
1	O
)	O
y	O
(	O
2	O
)	O
y	O
(	O
2	O
)	O
h	O
(	O
1	O
)	O
h	O
(	O
1	O
)	O
h	O
(	O
2	O
)	O
h	O
(	O
2	O
)	O
h	O
(	O
3	O
)	O
h	O
(	O
3	O
)	O
h	O
(	O
shared	O
)	O
h	O
(	O
shared	O
)	O
xx	O
figure	O
7.2	O
:	O
multi-task	O
learning	O
can	O
be	O
cast	O
in	O
several	O
ways	O
in	O
deep	O
learning	O
frameworks	O
and	O
this	O
ﬁgure	O
illustrates	O
the	O
common	O
situation	O
where	O
the	O
tasks	O
share	O
a	O
common	O
input	O
but	O
involve	O
diﬀerent	O
target	O
random	O
variables	O
.	O
the	O
lower	O
layers	O
of	O
a	O
deep	O
network	O
(	O
whether	O
it	O
is	O
supervised	O
and	O
feedforward	O
or	O
includes	O
a	O
generative	O
component	O
with	O
downward	O
arrows	O
)	O
can	O
be	O
shared	O
across	O
such	O
tasks	O
,	O
while	O
task-speciﬁc	O
parameters	O
(	O
associated	O
respectively	O
with	O
the	O
weights	O
into	O
and	O
from	O
h	O
(	O
1	O
)	O
and	O
h	O
(	O
2	O
)	O
)	O
can	O
be	O
learned	O
on	O
top	O
of	O
those	O
yielding	O
a	O
shared	O
representation	O
h	O
(	O
shared	O
)	O
.	O
the	O
underlying	O
assumption	O
is	O
that	O
there	O
exists	O
a	O
common	O
pool	O
of	O
factors	O
that	O
explain	O
the	O
variations	O
in	O
the	O
input	O
x	O
,	O
while	O
each	O
task	O
is	O
associated	O
with	O
a	O
subset	O
of	O
these	O
factors	O
.	O
in	O
this	O
example	O
,	O
it	O
is	O
additionally	O
assumed	O
that	O
top-level	O
hidden	O
units	O
h	O
(	O
1	O
)	O
and	O
h	O
(	O
2	O
)	O
are	O
specialized	O
to	O
each	O
task	O
(	O
respectively	O
predicting	O
y	O
(	O
1	O
)	O
and	O
y	O
(	O
2	O
)	O
)	O
while	O
some	O
intermediate-level	O
representation	O
h	O
(	O
shared	O
)	O
is	O
shared	O
across	O
all	O
tasks	O
.	O
in	O
the	O
unsupervised	O
learning	O
context	O
,	O
it	O
makes	O
sense	O
for	O
some	O
of	O
the	O
top-level	O
factors	O
to	O
be	O
associated	O
with	O
none	O
of	O
the	O
output	O
tasks	O
(	O
h	O
(	O
3	O
)	O
)	O
:	O
these	O
are	O
the	O
factors	O
that	O
explain	O
some	O
of	O
the	O
input	O
variations	O
but	O
are	O
not	O
relevant	O
for	O
predicting	O
y	O
(	O
1	O
)	O
or	O
y	O
(	O
2	O
)	O
.	O
improved	O
generalization	O
and	O
generalization	O
error	O
bounds	O
(	O
)	O
can	O
be	O
achieved	O
because	O
of	O
the	O
shared	O
parameters	O
,	O
for	O
which	O
statistical	O
strength	O
can	O
be	O
baxter	O
1995	O
,	O
245	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
)	O
d	O
o	O
o	O
h	O
i	O
l	O
e	O
k	O
i	O
l	O
-	O
g	O
o	O
l	O
e	O
v	O
i	O
t	O
a	O
g	O
e	O
n	O
(	O
s	O
s	O
o	O
l	O
0	O
20	O
.	O
0	O
15	O
.	O
0	O
10	O
.	O
0	O
05	O
.	O
0	O
00	O
.	O
0	O
training	O
set	O
loss	O
validation	O
set	O
loss	O
50	O
100	O
150	O
200	O
250	O
time	O
(	O
epochs	O
)	O
figure	O
7.3	O
:	O
learning	O
curves	O
showing	O
how	O
the	O
negative	O
log-likelihood	O
loss	O
changes	O
over	O
time	O
(	O
indicated	O
as	O
number	O
of	O
training	O
iterations	O
over	O
the	O
dataset	O
,	O
or	O
epochs	O
)	O
.	O
in	O
this	O
example	O
,	O
we	O
train	O
a	O
maxout	O
network	O
on	O
mnist	O
.	O
observe	O
that	O
the	O
training	O
objective	O
decreases	O
consistently	O
over	O
time	O
,	O
but	O
the	O
validation	O
set	O
average	O
loss	O
eventually	O
begins	O
to	O
increase	O
again	O
,	O
forming	O
an	O
asymmetric	O
u-shaped	O
curve	O
.	O
greatly	O
improved	O
(	O
in	O
proportion	O
with	O
the	O
increased	O
number	O
of	O
examples	O
for	O
the	O
shared	O
parameters	O
,	O
compared	O
to	O
the	O
scenario	O
of	O
single-task	O
models	O
)	O
.	O
of	O
course	O
this	O
will	O
happen	O
only	O
if	O
some	O
assumptions	O
about	O
the	O
statistical	O
relationship	O
between	O
the	O
diﬀerent	O
tasks	O
are	O
valid	O
,	O
meaning	O
that	O
there	O
is	O
something	O
shared	O
across	O
some	O
of	O
the	O
tasks	O
.	O
from	O
the	O
point	O
of	O
view	O
of	O
deep	O
learning	O
,	O
the	O
underlying	O
prior	O
belief	O
is	O
the	O
following	O
:	O
among	O
the	O
factors	O
that	O
explain	O
the	O
variations	O
observed	O
in	O
the	O
data	O
associated	O
with	O
the	O
diﬀerent	O
tasks	O
,	O
some	O
are	O
shared	O
across	O
two	O
or	O
more	O
tasks	O
.	O
7.8	O
early	O
stopping	O
when	O
training	O
large	O
models	O
with	O
suﬃcient	O
representational	O
capacity	O
to	O
overﬁt	O
the	O
task	O
,	O
we	O
often	O
observe	O
that	O
training	O
error	O
decreases	O
steadily	O
over	O
time	O
,	O
but	O
validation	O
set	O
error	O
begins	O
to	O
rise	O
again	O
.	O
see	O
ﬁgure	O
for	O
an	O
example	O
of	O
this	O
behavior	O
.	O
this	O
behavior	O
occurs	O
very	O
reliably	O
.	O
7.3	O
this	O
means	O
we	O
can	O
obtain	O
a	O
model	B
with	O
better	O
validation	O
set	O
error	O
(	O
and	O
thus	O
,	O
hopefully	O
better	O
test	O
set	O
error	O
)	O
by	O
returning	O
to	O
the	O
parameter	O
setting	O
at	O
the	O
point	O
in	O
time	O
with	O
the	O
lowest	O
validation	O
set	O
error	O
.	O
every	O
time	O
the	O
error	O
on	O
the	O
validation	O
set	O
improves	O
,	O
we	O
store	O
a	O
copy	O
of	O
the	O
model	B
parameters	O
.	O
when	O
the	O
training	O
algorithm	O
terminates	O
,	O
we	O
return	O
these	O
parameters	O
,	O
rather	O
than	O
the	O
latest	O
parameters	O
.	O
the	O
246	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
algorithm	O
terminates	O
when	O
no	O
parameters	O
have	O
improved	O
over	O
the	O
best	O
recorded	O
validation	O
error	O
for	O
some	O
pre-speciﬁed	O
number	O
of	O
iterations	O
.	O
this	O
procedure	O
is	O
speciﬁed	O
more	O
formally	O
in	O
algorithm	O
.7.1	O
algorithm	O
7.1	O
the	O
early	O
stopping	O
meta-algorithm	O
for	O
determining	O
the	O
best	O
amount	O
of	O
time	O
to	O
train	O
.	O
this	O
meta-algorithm	O
is	O
a	O
general	O
strategy	O
that	O
works	O
well	O
with	O
a	O
variety	O
of	O
training	O
algorithms	O
and	O
ways	O
of	O
quantifying	O
error	O
on	O
the	O
validation	O
set	O
.	O
n	O
o	O
be	O
the	O
number	O
of	O
steps	O
between	O
evaluations	O
.	O
let	O
let	O
p	O
be	O
the	O
“	O
patience	O
,	O
”	O
the	O
number	O
of	O
times	O
to	O
observe	O
worsening	O
validation	O
set	O
error	O
before	O
giving	O
up	O
.	O
←	O
let	O
θo	O
be	O
the	O
initial	O
parameters	O
.	O
←	O
θ	O
θ	O
←	O
i	O
0	O
←	O
∞	O
j	O
0	O
∗	O
←	O
v	O
∗	O
←	O
θ	O
θ	O
i	O
i	O
while	O
←	O
update	O
	O
←	O
+	O
i	O
i	O
validationseterror	O
(	O
)	O
θ	O
v	O
	O
←	O
if	O
v	O
<	O
v	O
then	O
∗	O
←	O
j	O
0	O
∗	O
←	O
θ	O
θ	O
←	O
	O
i	O
i	O
v	O
v	O
←	O
else	O
j	O
do	O
by	O
running	O
the	O
training	O
algorithm	O
for	O
j	O
<	O
p	O
θ	O
n	O
n	O
steps	O
.	O
j	O
+	O
1	O
end	O
if	O
end	O
while	O
best	O
parameters	O
are	O
θ	O
∗	O
∗	O
,	O
best	O
number	O
of	O
training	O
steps	O
is	O
i	O
this	O
strategy	O
is	O
known	O
as	O
early	O
stopping	O
.	O
it	O
is	O
probably	O
the	O
most	O
commonly	O
used	O
form	O
of	O
regularization	O
in	O
deep	O
learning	O
.	O
its	O
popularity	O
is	O
due	O
both	O
to	O
its	O
eﬀectiveness	O
and	O
its	O
simplicity	O
.	O
one	O
way	O
to	O
think	O
of	O
early	O
stopping	O
is	O
as	O
a	O
very	O
eﬃcient	O
hyperparameter	O
selection	O
algorithm	O
.	O
in	O
this	O
view	O
,	O
the	O
number	O
of	O
training	O
steps	O
is	O
just	O
another	O
hyperparameter	O
.	O
that	O
this	O
hyperparameter	O
has	O
a	O
u-shaped	O
validation	O
set	O
we	O
can	O
see	O
in	O
ﬁgure	O
7.3	O
247	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
5.3	O
performance	O
curve	O
.	O
most	O
hyperparameters	O
that	O
control	O
model	B
capacity	O
have	O
such	O
a	O
u-shaped	O
validation	O
set	O
performance	O
curve	O
,	O
as	O
illustrated	O
in	O
ﬁgure	O
.	O
in	O
the	O
case	O
of	O
early	O
stopping	O
,	O
we	O
are	O
controlling	O
the	O
eﬀective	O
capacity	O
of	O
the	O
model	B
by	O
determining	O
how	O
many	O
steps	O
it	O
can	O
take	O
to	O
ﬁt	O
the	O
training	O
set	O
.	O
most	O
hyperparameters	O
must	O
be	O
chosen	O
using	O
an	O
expensive	O
guess	O
and	O
check	O
process	O
,	O
where	O
we	O
set	O
a	O
hyperparameter	O
at	O
the	O
start	O
of	O
training	O
,	O
then	O
run	O
training	O
for	O
several	O
steps	O
to	O
see	O
its	O
eﬀect	O
.	O
the	O
“	O
training	O
time	O
”	O
hyperparameter	O
is	O
unique	O
in	O
that	O
by	O
deﬁnition	O
a	O
single	O
run	O
of	O
training	O
tries	O
out	O
many	O
values	O
of	O
the	O
hyperparameter	O
.	O
the	O
only	O
signiﬁcant	O
cost	O
to	O
choosing	O
this	O
hyperparameter	O
automatically	O
via	O
early	O
stopping	O
is	O
running	O
the	O
validation	O
set	O
evaluation	O
periodically	O
during	O
training	O
.	O
ideally	O
,	O
this	O
is	O
done	O
in	O
parallel	O
to	O
the	O
training	O
process	O
on	O
a	O
separate	O
machine	O
,	O
separate	O
cpu	O
,	O
or	O
separate	O
gpu	O
from	O
the	O
main	O
training	O
process	O
.	O
if	O
such	O
resources	O
are	O
not	O
available	O
,	O
then	O
the	O
cost	O
of	O
these	O
periodic	O
evaluations	O
may	O
be	O
reduced	O
by	O
using	O
a	O
validation	O
set	O
that	O
is	O
small	O
compared	O
to	O
the	O
training	O
set	O
or	O
by	O
evaluating	O
the	O
validation	O
set	O
error	O
less	O
frequently	O
and	O
obtaining	O
a	O
lower	O
resolution	O
estimate	O
of	O
the	O
optimal	O
training	O
time	O
.	O
an	O
additional	O
cost	O
to	O
early	O
stopping	O
is	O
the	O
need	O
to	O
maintain	O
a	O
copy	O
of	O
the	O
best	O
parameters	O
.	O
this	O
cost	O
is	O
generally	O
negligible	O
,	O
because	O
it	O
is	O
acceptable	O
to	O
store	O
these	O
parameters	O
in	O
a	O
slower	O
and	O
larger	O
form	O
of	O
memory	O
(	O
for	O
example	O
,	O
training	O
in	O
gpu	O
memory	O
,	O
but	O
storing	O
the	O
optimal	O
parameters	O
in	O
host	O
memory	O
or	O
on	O
a	O
disk	O
drive	O
)	O
.	O
since	O
the	O
best	O
parameters	O
are	O
written	O
to	O
infrequently	O
and	O
never	O
read	O
during	O
training	O
,	O
these	O
occasional	O
slow	O
writes	O
have	O
little	O
eﬀect	O
on	O
the	O
total	O
training	O
time	O
.	O
early	O
stopping	O
is	O
a	O
very	O
unobtrusive	O
form	O
of	O
regularization	O
,	O
in	O
that	O
it	O
requires	O
almost	O
no	O
change	O
in	O
the	O
underlying	O
training	O
procedure	O
,	O
the	O
objective	O
function	O
,	O
or	O
the	O
set	O
of	O
allowable	O
parameter	O
values	O
.	O
this	O
means	O
that	O
it	O
is	O
easy	O
to	O
use	O
early	O
stopping	O
without	O
damaging	O
the	O
learning	O
dynamics	O
.	O
this	O
is	O
in	O
contrast	O
to	O
weight	O
decay	O
,	O
where	O
one	O
must	O
be	O
careful	O
not	O
to	O
use	O
too	O
much	O
weight	O
decay	O
and	O
trap	O
the	O
network	O
in	O
a	O
bad	O
local	O
minimum	O
corresponding	O
to	O
a	O
solution	O
with	O
pathologically	O
small	O
weights	O
.	O
early	O
stopping	O
may	O
be	O
used	O
either	O
alone	O
or	O
in	O
conjunction	O
with	O
other	O
regulariza-	O
tion	B
strategies	O
.	O
even	O
when	O
using	O
regularization	O
strategies	O
that	O
modify	O
the	O
objective	O
function	O
to	O
encourage	O
better	O
generalization	O
,	O
it	O
is	O
rare	O
for	O
the	O
best	O
generalization	O
to	O
occur	O
at	O
a	O
local	O
minimum	O
of	O
the	O
training	O
objective	O
.	O
early	O
stopping	O
requires	O
a	O
validation	O
set	O
,	O
which	O
means	O
some	O
training	O
data	O
is	O
not	O
fed	O
to	O
the	O
model	B
.	O
to	O
best	O
exploit	O
this	O
extra	O
data	O
,	O
one	O
can	O
perform	O
extra	O
training	O
after	O
the	O
initial	O
training	O
with	O
early	O
stopping	O
has	O
completed	O
.	O
in	O
the	O
second	O
,	O
extra	O
training	O
step	O
,	O
all	O
of	O
the	O
training	O
data	O
is	O
included	O
.	O
there	O
are	O
two	O
basic	O
strategies	O
one	O
can	O
use	O
for	O
this	O
second	O
training	O
procedure	O
.	O
one	O
strategy	O
(	O
algorithm	O
)	O
is	O
to	O
initialize	O
the	O
model	B
again	O
and	O
retrain	O
on	O
all	O
7.2	O
248	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
of	O
the	O
data	O
.	O
in	O
this	O
second	O
training	O
pass	O
,	O
we	O
train	O
for	O
the	O
same	O
number	O
of	O
steps	O
as	O
the	O
early	O
stopping	O
procedure	O
determined	O
was	O
optimal	O
in	O
the	O
ﬁrst	O
pass	O
.	O
there	O
are	O
some	O
subtleties	O
associated	O
with	O
this	O
procedure	O
.	O
for	O
example	O
,	O
there	O
is	O
not	O
a	O
good	O
way	O
of	O
knowing	O
whether	O
to	O
retrain	O
for	O
the	O
same	O
number	O
of	O
parameter	O
updates	O
or	O
the	O
same	O
number	O
of	O
passes	O
through	O
the	O
dataset	O
.	O
on	O
the	O
second	O
round	O
of	O
training	O
,	O
each	O
pass	O
through	O
the	O
dataset	O
will	O
require	O
more	O
parameter	O
updates	O
because	O
the	O
training	O
set	O
is	O
bigger	O
.	O
algorithm	O
7.2	O
a	O
meta-algorithm	O
for	O
using	O
early	O
stopping	O
to	O
determine	O
how	O
long	O
to	O
train	O
,	O
then	O
retraining	O
on	O
all	O
the	O
data.	O
)	O
)	O
train	O
)	O
and	O
y	O
(	O
(	O
subtrain	O
,	O
y	O
(	O
valid	O
)	O
)	O
)	O
into	O
(	O
x	O
(	O
train	O
and	O
y	O
(	O
)	O
train	O
and	O
y	O
(	O
subtrain	O
,	O
x	O
(	O
valid	O
)	O
)	O
train	O
be	O
the	O
training	O
set.	O
)	O
let	O
x	O
(	O
split	O
x	O
(	O
respectively	O
.	O
run	O
early	O
stopping	O
(	O
algorithm	O
)	O
starting	O
from	O
random	O
)	O
y	O
(	O
∗	O
returns	O
i	O
set	O
train	O
on	O
x	O
(	O
,	O
the	O
optimal	O
number	O
of	O
steps	O
.	O
to	O
random	O
values	O
again	O
.	O
∗	O
train	O
for	O
i	O
train	O
and	O
y	O
(	O
)	O
steps	O
.	O
θ	O
)	O
subtrain	O
and	O
subtrain	O
for	O
training	O
data	O
and	O
x	O
(	O
valid	O
)	O
and	O
y	O
(	O
valid	O
)	O
for	O
validation	O
data	O
.	O
this	O
θ	O
using	O
x	O
(	O
7.1	O
)	O
another	O
strategy	O
for	O
using	O
all	O
of	O
the	O
data	O
is	O
to	O
keep	O
the	O
parameters	O
obtained	O
from	O
the	O
ﬁrst	O
round	O
of	O
training	O
and	O
then	O
continue	O
training	O
but	O
now	O
using	O
all	O
of	O
the	O
data	O
.	O
at	O
this	O
stage	O
,	O
we	O
now	O
no	O
longer	O
have	O
a	O
guide	O
for	O
when	O
to	O
stop	O
in	O
terms	O
of	O
a	O
number	O
of	O
steps	O
.	O
instead	O
,	O
we	O
can	O
monitor	O
the	O
average	O
loss	O
function	O
on	O
the	O
validation	O
set	O
,	O
and	O
continue	O
training	O
until	O
it	O
falls	O
below	O
the	O
value	O
of	O
the	O
training	O
set	O
objective	O
at	O
which	O
the	O
early	O
stopping	O
procedure	O
halted	O
.	O
this	O
strategy	O
avoids	O
the	O
high	O
cost	O
of	O
retraining	O
the	O
model	B
from	O
scratch	O
,	O
but	O
is	O
not	O
as	O
well-behaved	O
.	O
for	O
example	O
,	O
there	O
is	O
not	O
any	O
guarantee	O
that	O
the	O
objective	O
on	O
the	O
validation	O
set	O
will	O
ever	O
reach	O
the	O
target	O
value	O
,	O
so	O
this	O
strategy	O
is	O
not	O
even	O
guaranteed	O
to	O
terminate	O
.	O
this	O
procedure	O
is	O
presented	O
more	O
formally	O
in	O
algorithm	O
.7.3	O
early	O
stopping	O
is	O
also	O
useful	O
because	O
it	O
reduces	O
the	O
computational	O
cost	O
of	O
the	O
training	O
procedure	O
.	O
besides	O
the	O
obvious	O
reduction	O
in	O
cost	O
due	O
to	O
limiting	O
the	O
number	O
of	O
training	O
iterations	O
,	O
it	O
also	O
has	O
the	O
beneﬁt	O
of	O
providing	O
regularization	O
without	O
requiring	O
the	O
addition	O
of	O
penalty	O
terms	O
to	O
the	O
cost	O
function	O
or	O
the	O
computation	O
of	O
the	O
gradients	O
of	O
such	O
additional	O
terms	O
.	O
how	O
early	O
stopping	O
acts	O
as	O
a	O
regularizer	O
:	O
so	O
far	O
we	O
have	O
stated	O
that	O
early	O
stopping	O
a	O
regularization	O
strategy	O
,	O
but	O
we	O
have	O
supported	O
this	O
claim	O
only	O
by	O
showing	O
learning	O
curves	O
where	O
the	O
validation	O
set	O
error	O
has	O
a	O
u-shaped	O
curve	O
.	O
what	O
is	O
249	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
algorithm	O
7.3	O
meta-algorithm	O
using	O
early	O
stopping	O
to	O
determine	O
at	O
what	O
objec-	O
tive	O
value	O
we	O
start	O
to	O
overﬁt	O
,	O
then	O
continue	O
training	O
until	O
that	O
value	O
is	O
reached.	O
)	O
)	O
)	O
train	O
)	O
into	O
(	O
x	O
(	O
train	O
and	O
y	O
(	O
train	O
and	O
y	O
(	O
subtrain	O
,	O
x	O
(	O
valid	O
)	O
)	O
train	O
be	O
the	O
training	O
set.	O
)	O
let	O
x	O
(	O
split	O
x	O
(	O
respectively	O
.	O
run	O
early	O
stopping	O
(	O
algorithm	O
)	O
starting	O
from	O
random	O
subtrain	O
and	O
subtrain	O
for	O
training	O
data	O
and	O
x	O
(	O
valid	O
)	O
and	O
y	O
(	O
valid	O
)	O
for	O
validation	O
data	O
.	O
this	O
y	O
(	O
←	O
updates	O
	O
j	O
,	O
while	O
j	O
,	O
(	O
θ	O
x	O
(	O
valid	O
)	O
,	O
y	O
(	O
valid	O
)	O
)	O
>	O
	O
do	O
train	O
for	O
subtrain	O
,	O
y	O
(	O
valid	O
)	O
)	O
train	O
on	O
x	O
(	O
train	O
and	O
y	O
(	O
θ	O
using	O
x	O
(	O
subtrain	O
,	O
y	O
(	O
subtrain	O
)	O
)	O
and	O
y	O
(	O
(	O
θ	O
x	O
(	O
steps	O
.	O
7.1	O
.θ	O
n	O
(	O
)	O
)	O
)	O
)	O
)	O
)	O
end	O
while	O
(	O
7.4	O
)	O
and	O
sjöberg	O
and	O
ljung	O
1995	O
is	O
the	O
actual	O
mechanism	O
by	O
which	O
early	O
stopping	O
regularizes	O
the	O
model	B
?	O
bishop	O
(	O
1995a	O
)	O
argued	O
that	O
early	O
stopping	O
has	O
the	O
eﬀect	O
of	O
restricting	O
the	O
optimization	O
procedure	O
to	O
a	O
relatively	O
small	O
volume	O
of	O
parameter	O
space	O
in	O
the	O
neighborhood	O
of	O
the	O
initial	O
parameter	O
value	O
θo	O
,	O
as	O
illustrated	O
in	O
τ	O
optimization	O
steps	O
(	O
corresponding	O
ﬁgure	O
to	O
τ	O
training	O
iterations	O
)	O
and	O
with	O
learning	O
rate	O
	O
.	O
we	O
can	O
view	O
the	O
product	O
τ	O
as	O
a	O
measure	O
of	O
eﬀective	O
capacity	O
.	O
assuming	O
the	O
gradient	O
is	O
bounded	O
,	O
restricting	O
both	O
the	O
number	O
of	O
iterations	O
and	O
the	O
learning	O
rate	O
limits	O
the	O
volume	O
of	O
parameter	O
space	O
reachable	O
from	O
θo	O
.	O
in	O
this	O
sense	O
,	O
τ	O
behaves	O
as	O
if	O
it	O
were	O
the	O
reciprocal	O
of	O
the	O
coeﬃcient	O
used	O
for	O
weight	O
decay	O
.	O
.	O
more	O
speciﬁcally	O
,	O
imagine	O
taking	O
indeed	O
,	O
we	O
can	O
show	O
how—in	O
the	O
case	O
of	O
a	O
simple	O
linear	O
model	B
with	O
a	O
quadratic	O
error	O
function	O
and	O
simple	O
gradient	O
descent—early	O
stopping	O
is	O
equivalent	O
to	O
l	O
2	O
regularization	O
.	O
in	O
order	O
to	O
compare	O
with	O
classical	O
l2	O
regularization	O
,	O
we	O
examine	O
a	O
simple	O
setting	O
where	O
the	O
only	O
parameters	O
are	O
linear	O
weights	O
(	O
θ	O
=	O
w	O
)	O
.	O
we	O
can	O
model	B
the	O
cost	O
function	O
j	O
with	O
a	O
quadratic	O
approximation	O
in	O
the	O
neighborhood	O
of	O
the	O
∗	O
empirically	O
optimal	O
value	O
of	O
the	O
weights	O
w	O
−	O
∗	O
(	O
w	O
w	O
−	O
∗	O
h	O
w	O
w	O
(	O
)	O
=	O
θ	O
(	O
7.33	O
)	O
	O
)	O
)	O
+	O
(	O
w	O
ˆj	O
)	O
,	O
j	O
∗	O
(	O
:	O
∗	O
where	O
h	O
is	O
the	O
hessian	O
matrix	O
of	O
j	O
with	O
respect	O
to	O
w	O
evaluated	O
at	O
w	O
∗	O
assumption	O
that	O
w	O
under	O
a	O
local	O
taylor	O
series	O
approximation	O
,	O
the	O
gradient	O
is	O
given	O
by	O
:	O
.	O
given	O
the	O
is	O
a	O
minimum	O
of	O
j	O
(	O
w	O
)	O
,	O
we	O
know	O
that	O
h	O
is	O
positive	O
semideﬁnite	O
.	O
1	O
2	O
∇	O
ˆj	O
(	O
w	O
−	O
∗	O
)	O
.	O
w	O
h	O
w	O
w	O
)	O
=	O
(	O
250	O
(	O
7.34	O
)	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
∗	O
w	O
∗	O
w	O
2	O
w	O
˜w	O
2	O
w	O
˜w	O
w1	O
w1	O
figure	O
7.4	O
:	O
an	O
illustration	O
of	O
the	O
eﬀect	O
of	O
early	O
stopping	O
.	O
(	O
left	O
)	O
the	O
solid	O
contour	O
lines	O
indicate	O
the	O
contours	O
of	O
the	O
negative	O
log-likelihood	O
.	O
the	O
dashed	O
line	O
indicates	O
the	O
trajectory	O
∗	O
taken	O
by	O
sgd	O
beginning	O
from	O
the	O
origin	O
.	O
rather	O
than	O
stopping	O
at	O
the	O
point	O
w	O
that	O
minimizes	O
the	O
cost	O
,	O
early	O
stopping	O
results	O
in	O
the	O
trajectory	O
stopping	O
at	O
an	O
earlier	O
point	O
˜w	O
.	O
(	O
right	O
)	O
an	O
illustration	O
of	O
the	O
eﬀect	O
of	O
l2	O
regularization	O
for	O
comparison	O
.	O
the	O
dashed	O
circles	O
indicate	O
the	O
contours	O
of	O
the	O
l2	O
penalty	O
,	O
which	O
causes	O
the	O
minimum	O
of	O
the	O
total	O
cost	O
to	O
lie	O
nearer	O
the	O
origin	O
than	O
the	O
minimum	O
of	O
the	O
unregularized	O
cost	O
.	O
we	O
are	O
going	O
to	O
study	O
the	O
trajectory	O
followed	O
by	O
the	O
parameter	O
vector	O
during	O
training	O
.	O
for	O
simplicity	O
,	O
let	O
us	O
set	O
the	O
initial	O
parameter	O
vector	O
to	O
the	O
origin,3	O
that	O
is	O
w	O
(	O
0	O
)	O
=	O
0.	O
let	O
us	O
study	O
the	O
approximate	O
behavior	O
of	O
gradient	O
descent	B
on	O
j	O
by	O
analyzing	O
gradient	O
descent	B
on	O
ˆj	O
:	O
w	O
(	O
)	O
τ	O
=	O
w	O
(	O
τ	O
1	O
)	O
−	O
−	O
∇	O
	O
w	O
−	O
−	O
=	O
w	O
(	O
τ	O
h	O
w	O
(	O
(	O
τ	O
−	O
i	O
h	O
w	O
(	O
τ	O
=	O
(	O
)	O
(	O
1	O
)	O
1	O
)	O
	O
−	O
1	O
)	O
−	O
−	O
∗	O
w	O
1	O
)	O
−	O
−	O
ˆj	O
(	O
w	O
(	O
τ	O
)	O
.	O
)	O
w	O
∗	O
)	O
−	O
∗	O
w	O
w	O
(	O
)	O
τ	O
(	O
7.35	O
)	O
(	O
7.36	O
)	O
(	O
7.37	O
)	O
let	O
us	O
now	O
rewrite	O
this	O
expression	O
in	O
the	O
space	O
of	O
the	O
eigenvectors	O
of	O
h	O
,	O
exploiting	O
the	O
eigendecomposition	O
of	O
h	O
:	O
h	O
=	O
q	O
qλ	O
,	O
where	O
λ	O
is	O
a	O
diagonal	O
matrix	O
and	O
q	O
is	O
an	O
orthonormal	O
basis	O
of	O
eigenvectors	O
.	O
−	O
−	O
w	O
(	O
)	O
τ	O
−	O
−	O
−	O
−	O
−	O
−	O
)	O
(	O
w	O
(	O
τ	O
(	O
7.38	O
)	O
	O
=	O
(	O
i	O
q	O
q	O
	O
λ	O
q	O
	O
λ	O
w	O
∗	O
w	O
w	O
∗	O
w	O
)	O
=	O
(	O
(	O
7.39	O
)	O
(	O
w	O
(	O
)	O
τ	O
(	O
w	O
(	O
τ	O
q	O
∗	O
)	O
	O
	O
1	O
)	O
i	O
)	O
1	O
)	O
∗	O
)	O
3for	O
neural	O
networks	O
,	O
to	O
obtain	O
symmetry	O
breaking	O
between	O
hidden	O
units	O
,	O
we	O
can	O
not	O
initialize	O
.	O
however	O
,	O
the	O
argument	O
holds	O
for	O
any	O
other	O
6.2	O
all	O
the	O
parameters	O
to	O
0	O
,	O
as	O
discussed	O
in	O
section	O
initial	O
value	O
w	O
(	O
0	O
)	O
.	O
251	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
−	O
|	O
λ	O
i	O
|	O
assuming	O
that	O
w	O
(	O
0	O
)	O
=	O
0	O
and	O
that	O
	O
is	O
chosen	O
to	O
be	O
small	O
enough	O
to	O
guarantee	O
<	O
1	O
,	O
the	O
parameter	O
trajectory	O
during	O
training	O
after	O
τ	O
parameter	O
updates	O
1	O
is	O
as	O
follows	O
:	O
	O
q	O
w	O
(	O
)	O
τ	O
=	O
[	O
i	O
	O
−	O
(	O
i	O
−	O
	O
λ	O
τ	O
]	O
q	O
)	O
∗	O
w	O
.	O
(	O
7.40	O
)	O
now	O
,	O
the	O
expression	O
for	O
q	O
ranged	O
as	O
:	O
˜w	O
in	O
equation	O
7.13	O
for	O
l2	O
regularization	O
can	O
be	O
rear-	O
	O
	O
q	O
q	O
˜w	O
−	O
1λq	O
=	O
(	O
+λ	O
α	O
)	O
i	O
−	O
1α	O
]	O
q	O
(	O
+λ	O
α	O
)	O
i	O
	O
w	O
−	O
∗	O
˜w	O
i	O
=	O
[	O
	O
∗	O
w	O
(	O
7.41	O
)	O
(	O
7.42	O
)	O
7.40	O
and	O
equation	O
7.42	O
,	O
we	O
see	O
that	O
if	O
the	O
hyperparameters	O
	O
,	O
λ	O
τ	O
=	O
(	O
+	O
)	O
λ	O
αi	O
)	O
−	O
1	O
α	O
,	O
(	O
7.43	O
)	O
(	O
i	O
comparing	O
equation	O
α	O
,	O
and	O
τ	O
are	O
chosen	O
such	O
that	O
−	O
then	O
l2	O
regularization	O
and	O
early	O
stopping	O
can	O
be	O
seen	O
to	O
be	O
equivalent	O
(	O
at	O
least	O
under	O
the	O
quadratic	O
approximation	O
of	O
the	O
objective	O
function	O
)	O
.	O
going	O
even	O
further	O
,	O
by	O
taking	O
logarithms	O
and	O
using	O
the	O
series	O
expansion	O
for	O
log	O
(	O
1	O
+	O
x	O
)	O
,	O
we	O
can	O
conclude	O
that	O
if	O
all	O
λi	O
are	O
small	O
(	O
that	O
is	O
,	O
λi	O
1	O
)	O
then	O
	O
	O
1	O
and	O
λi/α	O
≈	O
1	O
α	O
≈	O
1	O
τ	O
,	O
.	O
τ	O
α	O
(	O
7.44	O
)	O
(	O
7.45	O
)	O
that	O
is	O
,	O
under	O
these	O
assumptions	O
,	O
the	O
number	O
of	O
training	O
iterations	O
τ	O
plays	O
a	O
role	O
inversely	O
proportional	O
to	O
the	O
l2	O
regularization	O
parameter	O
,	O
and	O
the	O
inverse	O
of	O
τ	O
plays	O
the	O
role	O
of	O
the	O
weight	O
decay	O
coeﬃcient	O
.	O
parameter	O
values	O
corresponding	O
to	O
directions	O
of	O
signiﬁcant	O
curvature	O
(	O
of	O
the	O
objective	O
function	O
)	O
are	O
regularized	O
less	O
than	O
directions	O
of	O
less	O
curvature	O
.	O
of	O
course	O
,	O
in	O
the	O
context	O
of	O
early	O
stopping	O
,	O
this	O
really	O
means	O
that	O
parameters	O
that	O
correspond	O
to	O
directions	O
of	O
signiﬁcant	O
curvature	O
tend	O
to	O
learn	O
early	O
relative	O
to	O
parameters	O
corresponding	O
to	O
directions	O
of	O
less	O
curvature	O
.	O
the	O
derivations	O
in	O
this	O
section	O
have	O
shown	O
that	O
a	O
trajectory	O
of	O
length	O
τ	O
ends	O
at	O
a	O
point	O
that	O
corresponds	O
to	O
a	O
minimum	O
of	O
the	O
l2-regularized	O
objective	O
.	O
early	O
stopping	O
is	O
of	O
course	O
more	O
than	O
the	O
mere	O
restriction	O
of	O
the	O
trajectory	O
length	O
;	O
instead	O
,	O
early	O
stopping	O
typically	O
involves	O
monitoring	O
the	O
validation	O
set	O
error	O
in	O
order	O
to	O
stop	O
the	O
trajectory	O
at	O
a	O
particularly	O
good	O
point	O
in	O
space	O
.	O
early	O
stopping	O
therefore	O
has	O
the	O
advantage	O
over	O
weight	O
decay	O
that	O
early	O
stopping	O
automatically	O
determines	O
the	O
correct	O
amount	O
of	O
regularization	O
while	O
weight	O
decay	O
requires	O
many	O
training	O
experiments	O
with	O
diﬀerent	O
values	O
of	O
its	O
hyperparameter	O
.	O
252	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
7.9	O
parameter	O
tying	O
and	O
parameter	O
sharing	O
thus	O
far	O
,	O
in	O
this	O
chapter	O
,	O
when	O
we	O
have	O
discussed	O
adding	O
constraints	O
or	O
penalties	O
to	O
the	O
parameters	O
,	O
we	O
have	O
always	O
done	O
so	O
with	O
respect	O
to	O
a	O
ﬁxed	O
region	O
or	O
point	O
.	O
for	O
example	O
,	O
l2	O
regularization	O
(	O
or	O
weight	O
decay	O
)	O
penalizes	O
model	B
parameters	O
for	O
deviating	O
from	O
the	O
ﬁxed	O
value	O
of	O
zero	O
.	O
however	O
,	O
sometimes	O
we	O
may	O
need	O
other	O
ways	O
to	O
express	O
our	O
prior	O
knowledge	O
about	O
suitable	O
values	O
of	O
the	O
model	B
parameters	O
.	O
sometimes	O
we	O
might	O
not	O
know	O
precisely	O
what	O
values	O
the	O
parameters	O
should	O
take	O
but	O
we	O
know	O
,	O
from	O
knowledge	O
of	O
the	O
domain	O
and	O
model	B
architecture	O
,	O
that	O
there	O
should	O
be	O
some	O
dependencies	O
between	O
the	O
model	B
parameters	O
.	O
a	O
common	O
type	O
of	O
dependency	O
that	O
we	O
often	O
want	O
to	O
express	O
is	O
that	O
certain	O
parameters	O
should	O
be	O
close	O
to	O
one	O
another	O
.	O
consider	O
the	O
following	O
scenario	O
:	O
we	O
have	O
two	O
models	O
performing	O
the	O
same	O
classiﬁcation	O
task	O
(	O
with	O
the	O
same	O
set	O
of	O
classes	O
)	O
but	O
with	O
somewhat	O
diﬀerent	O
input	O
distributions	O
.	O
formally	O
,	O
we	O
have	O
model	B
a	O
with	O
parameters	O
w	O
(	O
)	O
a	O
and	O
model	B
b	O
with	O
parameters	O
w	O
(	O
)	O
b	O
.	O
the	O
two	O
models	O
map	O
the	O
input	O
to	O
two	O
diﬀerent	O
,	O
but	O
related	O
outputs	O
:	O
ˆy	O
(	O
)	O
a	O
=	O
f	O
(	O
w	O
(	O
)	O
a	O
,	O
x	O
)	O
and	O
ˆy	O
(	O
)	O
b	O
=	O
(	O
g	O
w	O
(	O
)	O
b	O
,	O
x	O
)	O
.	O
∀	O
i	O
,	O
w	O
(	O
)	O
a	O
let	O
us	O
imagine	O
that	O
the	O
tasks	O
are	O
similar	O
enough	O
(	O
perhaps	O
with	O
similar	O
input	O
and	O
output	O
distributions	O
)	O
that	O
we	O
believe	O
the	O
model	B
parameters	O
should	O
be	O
close	O
to	O
each	O
other	O
:	O
.	O
we	O
can	O
leverage	O
this	O
information	O
through	O
regularization	O
.	O
speciﬁcally	O
,	O
we	O
can	O
use	O
a	O
parameter	O
norm	O
penalty	O
of	O
the	O
form	O
:	O
ω	O
(	O
w	O
(	O
)	O
a	O
,	O
w	O
(	O
2.	O
here	O
we	O
used	O
an	O
l2	O
penalty	O
,	O
but	O
other	O
2	O
choices	O
are	O
also	O
possible	O
.	O
should	O
be	O
close	O
to	O
w	O
(	O
i	O
	O
w	O
(	O
)	O
a	O
)	O
b	O
)	O
=	O
−	O
)	O
b	O
i	O
	O
w	O
(	O
)	O
b	O
this	O
kind	O
of	O
approach	O
was	O
proposed	O
by	O
)	O
,	O
who	O
regularized	O
the	O
parameters	O
of	O
one	O
model	B
,	O
trained	O
as	O
a	O
classiﬁer	O
in	O
a	O
supervised	O
paradigm	O
,	O
to	O
be	O
close	O
to	O
the	O
parameters	O
of	O
another	O
model	B
,	O
trained	O
in	O
an	O
unsupervised	O
paradigm	O
(	O
to	O
capture	O
the	O
distribution	O
of	O
the	O
observed	O
input	O
data	O
)	O
.	O
the	O
architectures	O
were	O
constructed	O
such	O
that	O
many	O
of	O
the	O
parameters	O
in	O
the	O
classiﬁer	O
model	B
could	O
be	O
paired	O
to	O
corresponding	O
parameters	O
in	O
the	O
unsupervised	O
model	B
.	O
lasserre	O
et	O
al	O
.	O
2006	O
(	O
while	O
a	O
parameter	O
norm	O
penalty	O
is	O
one	O
way	O
to	O
regularize	O
parameters	O
to	O
be	O
close	O
to	O
one	O
another	O
,	O
the	O
more	O
popular	O
way	O
is	O
to	O
use	O
constraints	O
:	O
to	O
force	O
sets	O
of	O
parameters	O
to	O
be	O
equal	O
.	O
this	O
method	O
of	O
regularization	O
is	O
often	O
referred	O
to	O
as	O
parameter	O
sharing	O
,	O
because	O
we	O
interpret	O
the	O
various	O
models	O
or	O
model	B
components	O
as	O
sharing	O
a	O
unique	O
set	O
of	O
parameters	O
.	O
a	O
signiﬁcant	O
advantage	O
of	O
parameter	O
sharing	O
over	O
regularizing	O
the	O
parameters	O
to	O
be	O
close	O
(	O
via	O
a	O
norm	O
penalty	O
)	O
is	O
that	O
only	O
a	O
subset	O
of	O
the	O
parameters	O
(	O
the	O
unique	O
set	O
)	O
need	O
to	O
be	O
stored	O
in	O
memory	O
.	O
in	O
certain	O
models—such	O
as	O
the	O
convolutional	O
neural	O
network—this	O
can	O
lead	O
to	O
signiﬁcant	O
reduction	O
in	O
the	O
memory	O
footprint	O
of	O
the	O
model	B
.	O
253	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
convolutional	O
neural	O
networks	O
by	O
far	O
the	O
most	O
popular	O
and	O
extensive	O
use	O
of	O
parameter	O
sharing	O
occurs	O
in	O
convolutional	O
neural	O
networks	O
(	O
cnns	O
)	O
applied	O
to	O
computer	O
vision	O
.	O
natural	O
images	O
have	O
many	O
statistical	O
properties	O
that	O
are	O
invariant	O
to	O
translation	O
.	O
for	O
example	O
,	O
a	O
photo	O
of	O
a	O
cat	O
remains	O
a	O
photo	O
of	O
a	O
cat	O
if	O
it	O
is	O
translated	O
one	O
pixel	O
to	O
the	O
right	O
.	O
cnns	O
take	O
this	O
property	O
into	O
account	O
by	O
sharing	O
parameters	O
across	O
multiple	O
image	O
locations	O
.	O
the	O
same	O
feature	O
(	O
a	O
hidden	O
unit	O
with	O
the	O
same	O
weights	O
)	O
is	O
computed	O
over	O
diﬀerent	O
locations	O
in	O
the	O
input	O
.	O
this	O
means	O
that	O
we	O
can	O
ﬁnd	O
a	O
cat	O
with	O
the	O
same	O
cat	O
detector	O
whether	O
the	O
cat	O
appears	O
at	O
column	O
i	O
or	O
column	O
i	O
+	O
1	O
in	O
the	O
image	O
.	O
parameter	O
sharing	O
has	O
allowed	O
cnns	O
to	O
dramatically	O
lower	O
the	O
number	O
of	O
unique	O
model	B
parameters	O
and	O
to	O
signiﬁcantly	O
increase	O
network	O
sizes	O
without	O
requiring	O
a	O
corresponding	O
increase	O
in	O
training	O
data	O
.	O
it	O
remains	O
one	O
of	O
the	O
best	O
examples	O
of	O
how	O
to	O
eﬀectively	O
incorporate	O
domain	O
knowledge	O
into	O
the	O
network	O
architecture	O
.	O
cnns	O
will	O
be	O
discussed	O
in	O
more	O
detail	O
in	O
chapter	O
.9	O
7.10	O
sparse	O
representations	O
weight	O
decay	O
acts	O
by	O
placing	O
a	O
penalty	O
directly	O
on	O
the	O
model	B
parameters	O
.	O
another	O
strategy	O
is	O
to	O
place	O
a	O
penalty	O
on	O
the	O
activations	O
of	O
the	O
units	O
in	O
a	O
neural	O
network	O
,	O
encouraging	O
their	O
activations	O
to	O
be	O
sparse	O
.	O
this	O
indirectly	O
imposes	O
a	O
complicated	O
penalty	O
on	O
the	O
model	B
parameters	O
.	O
we	O
have	O
already	O
discussed	O
(	O
in	O
section	O
l1	O
penalization	O
induces	O
a	O
sparse	O
parametrization—meaning	O
that	O
many	O
of	O
the	O
parameters	O
become	O
zero	O
(	O
or	O
close	O
to	O
zero	O
)	O
.	O
representational	O
sparsity	O
,	O
on	O
the	O
other	O
hand	O
,	O
describes	O
a	O
representation	O
where	O
many	O
of	O
the	O
elements	O
of	O
the	O
representation	O
are	O
zero	O
(	O
or	O
close	O
to	O
zero	O
)	O
.	O
a	O
simpliﬁed	O
view	O
of	O
this	O
distinction	O
can	O
be	O
illustrated	O
in	O
the	O
context	O
of	O
linear	O
regression	O
:	O
7.1.2	O
)	O
how	O
	O
	O
	O
2	O
3−	O
2−	O
5	O
1	O
∈	O
4	O
x	O
r	O
(	O
7.46	O
)	O
n	O
	O
4	O
0	O
0	O
0	O
0	O
5	O
1	O
0	O
1	O
0	O
	O
18	O
	O
5	O
15−	O
9−	O
3	O
∈	O
=	O
m	O
y	O
r	O
−	O
2	O
0	O
−	O
0	O
1	O
0	O
×	O
m	O
n	O
r	O
−	O
0	O
1	O
0	O
0	O
0	O
∈	O
a	O
0	O
3	O
0	O
−	O
0	O
5	O
0	O
0	O
−	O
0	O
4	O
0	O
254	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
	O
3	O
−	O
4	O
1	O
−	O
3	O
5	O
	O
	O
−	O
14	O
1	O
19	O
2	O
23	O
∈	O
r	O
y	O
=	O
m	O
	O
	O
0	O
2	O
0	O
0−	O
3	O
∈	O
0	O
h	O
r	O
	O
n	O
(	O
7.47	O
)	O
−	O
1	O
2	O
5	O
1	O
4	O
b	O
−	O
−	O
−	O
2	O
5	O
1	O
3	O
−	O
2	O
4	O
−	O
2	O
3	O
2	O
2	O
∈	O
×	O
m	O
n	O
r	O
4	O
1	O
−	O
−	O
3	O
1	O
−	O
2	O
3	O
−	O
−	O
0	O
3	O
1	O
5	O
in	O
the	O
ﬁrst	O
expression	O
,	O
we	O
have	O
an	O
example	O
of	O
a	O
sparsely	O
parametrized	O
linear	O
regression	O
model	B
.	O
in	O
the	O
second	O
,	O
we	O
have	O
linear	O
regression	O
with	O
a	O
sparse	O
representa-	O
tion	B
h	O
of	O
the	O
data	O
x.	O
that	O
is	O
,	O
h	O
is	O
a	O
function	O
of	O
x	O
that	O
,	O
in	O
some	O
sense	O
,	O
represents	O
the	O
information	O
present	O
in	O
,	O
but	O
does	O
so	O
with	O
a	O
sparse	O
vector	O
.	O
x	O
representational	O
regularization	O
is	O
accomplished	O
by	O
the	O
same	O
sorts	O
of	O
mechanisms	O
that	O
we	O
have	O
used	O
in	O
parameter	O
regularization	O
.	O
norm	O
penalty	O
regularization	O
of	O
representations	O
is	O
performed	O
by	O
adding	O
to	O
the	O
loss	O
function	O
j	O
a	O
norm	O
penalty	O
on	O
the	O
representation	O
.	O
this	O
penalty	O
is	O
denoted	O
ω	O
(	O
)	O
h	O
.	O
as	O
before	O
,	O
we	O
denote	O
the	O
regularized	O
loss	O
function	O
by	O
˜j	O
:	O
˜j	O
(	O
;	O
θ	O
x	O
y	O
)	O
=	O
(	O
;	O
θ	O
x	O
y	O
)	O
+	O
ω	O
(	O
)	O
h	O
α	O
j	O
,	O
,	O
(	O
7.48	O
)	O
∈	O
∞	O
	O
i	O
where	O
α	O
larger	O
values	O
of	O
[	O
0	O
,	O
)	O
weights	O
the	O
relative	O
contribution	O
of	O
the	O
norm	O
penalty	O
term	O
,	O
with	O
α	O
corresponding	O
to	O
more	O
regularization	O
.	O
|	O
hi	O
||	O
||	O
h	O
1	O
=	O
just	O
as	O
an	O
l1	O
penalty	O
on	O
the	O
parameters	O
induces	O
parameter	O
sparsity	O
,	O
an	O
l	O
1	O
|	O
penalty	O
on	O
the	O
elements	O
of	O
the	O
representation	O
induces	O
representational	O
sparsity	O
:	O
.	O
of	O
course	O
,	O
the	O
l1	O
penalty	O
is	O
only	O
one	O
choice	O
of	O
penalty	O
ω	O
(	O
h	O
)	O
=	O
that	O
can	O
result	O
in	O
a	O
sparse	O
representation	O
.	O
others	O
include	O
the	O
penalty	O
derived	O
from	O
a	O
student-t	O
prior	O
on	O
the	O
representation	O
(	O
olshausen	O
and	O
field	O
1996	O
bergstra	O
2011	O
)	O
and	O
kl	O
divergence	O
penalties	O
(	O
)	O
that	O
are	O
especially	O
useful	O
for	O
representations	O
with	O
elements	O
constrained	O
to	O
lie	O
on	O
the	O
unit	O
interval	O
.	O
lee	O
)	O
both	O
provide	O
examples	O
of	O
strategies	O
based	O
on	O
regularizing	O
the	O
average	O
activation	O
across	O
several	O
examples	O
,	O
1	O
i	O
h	O
(	O
)	O
i	O
,	O
to	O
m	O
be	O
near	O
some	O
target	O
value	O
,	O
such	O
as	O
a	O
vector	O
with	O
.01	O
for	O
each	O
entry	O
.	O
,	O
larochelle	O
and	O
bengio	O
2008	O
goodfellow	O
	O
et	O
al	O
.	O
(	O
et	O
al	O
.	O
(	O
2009	O
2008	O
)	O
and	O
,	O
;	O
,	O
other	O
approaches	O
obtain	O
representational	O
sparsity	O
with	O
a	O
hard	O
constraint	O
on	O
the	O
activation	O
values	O
.	O
for	O
example	O
,	O
orthogonal	O
matching	O
pursuit	O
(	O
pati	O
et	O
al.	O
,	O
1993	O
)	O
encodes	O
an	O
input	O
x	O
with	O
the	O
representation	O
h	O
that	O
solves	O
the	O
constrained	O
optimization	O
problem	O
	O
	O
h	O
0	O
is	O
the	O
number	O
of	O
non-zero	O
entries	O
of	O
h	O
.	O
this	O
problem	O
can	O
be	O
solved	O
where	O
eﬃciently	O
when	O
w	O
is	O
constrained	O
to	O
be	O
orthogonal	O
.	O
this	O
method	O
is	O
often	O
called	O
(	O
7.49	O
)	O
	O
−	O
	O
x	O
w	O
h	O
2	O
,	O
arg	O
min	O
	O
	O
h	O
h	O
,	O
0	O
<	O
k	O
255	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
omp-k	O
with	O
the	O
value	O
of	O
k	O
speciﬁed	O
to	O
indicate	O
the	O
number	O
of	O
non-zero	O
features	O
allowed.	O
)	O
demonstrated	O
that	O
omp-	O
can	O
be	O
a	O
very	O
eﬀective	O
feature	O
extractor	O
for	O
deep	O
architectures	O
.	O
coates	O
and	O
ng	O
2011	O
1	O
(	O
essentially	O
any	O
model	B
that	O
has	O
hidden	O
units	O
can	O
be	O
made	O
sparse	O
.	O
throughout	O
this	O
book	O
,	O
we	O
will	O
see	O
many	O
examples	O
of	O
sparsity	O
regularization	O
used	O
in	O
a	O
variety	O
of	O
contexts	O
.	O
7.11	O
bagging	O
and	O
other	O
ensemble	O
methods	O
bagging	O
(	O
short	O
for	O
bootstrap	O
aggregating	O
)	O
is	O
a	O
technique	O
for	O
reducing	O
gen-	O
eralization	O
error	O
by	O
combining	O
several	O
models	O
(	O
)	O
.	O
the	O
idea	O
is	O
to	O
train	O
several	O
diﬀerent	O
models	O
separately	O
,	O
then	O
have	O
all	O
of	O
the	O
models	O
vote	O
on	O
the	O
output	O
for	O
test	O
examples	O
.	O
this	O
is	O
an	O
example	O
of	O
a	O
general	O
strategy	O
in	O
machine	O
learning	O
called	O
model	B
averaging	O
.	O
techniques	O
employing	O
this	O
strategy	O
are	O
known	O
as	O
ensemble	O
methods	O
.	O
breiman	O
1994	O
,	O
the	O
reason	O
that	O
model	B
averaging	O
works	O
is	O
that	O
diﬀerent	O
models	O
will	O
usually	O
not	O
make	O
all	O
the	O
same	O
errors	O
on	O
the	O
test	O
set	O
.	O
	O
consider	O
for	O
example	O
a	O
set	O
of	O
k	O
regression	O
models	O
.	O
suppose	O
that	O
each	O
model	B
makes	O
an	O
error	O
i	O
on	O
each	O
example	O
,	O
with	O
the	O
errors	O
drawn	O
from	O
a	O
zero-mean	O
multivariate	O
normal	O
distribution	O
with	O
variances	O
e	O
[	O
2	O
i	O
]	O
=	O
v	O
and	O
covariances	O
e	O
[	O
ij	O
]	O
=	O
c.	O
then	O
the	O
error	O
made	O
by	O
the	O
average	O
prediction	O
of	O
all	O
the	O
ensemble	O
models	O
is	O
1	O
k	O
i	O
i	O
.	O
the	O
expected	O
squared	O
error	O
of	O
the	O
ensemble	O
predictor	O
is	O
	O
	O
	O
	O
	O
	O
	O
	O
e	O
2	O
1	O
k	O
i	O
i	O
=	O
=	O
1	O
k2	O
e	O
k	O
v	O
+	O
1	O
k	O
i	O
−	O
k	O
2	O
i	O
+	O
ij	O
j	O
i	O
=	O
1	O
c.	O
(	O
7.50	O
)	O
(	O
7.51	O
)	O
in	O
the	O
case	O
where	O
the	O
errors	O
are	O
perfectly	O
correlated	O
and	O
c	O
=	O
v	O
,	O
the	O
mean	O
squared	O
error	O
reduces	O
to	O
v	O
,	O
so	O
the	O
model	B
averaging	O
does	O
not	O
help	O
at	O
all	O
.	O
in	O
the	O
case	O
where	O
the	O
errors	O
are	O
perfectly	O
uncorrelated	O
and	O
c	O
=	O
0	O
,	O
the	O
expected	O
squared	O
error	O
of	O
the	O
ensemble	O
is	O
only	O
1	O
v.	O
this	O
means	O
that	O
the	O
expected	O
squared	O
error	O
of	O
the	O
ensemble	O
k	O
decreases	O
linearly	O
with	O
the	O
ensemble	O
size	O
.	O
in	O
other	O
words	O
,	O
on	O
average	O
,	O
the	O
ensemble	O
will	O
perform	O
at	O
least	O
as	O
well	O
as	O
any	O
of	O
its	O
members	O
,	O
and	O
if	O
the	O
members	O
make	O
independent	O
errors	O
,	O
the	O
ensemble	O
will	O
perform	O
signiﬁcantly	O
better	O
than	O
its	O
members	O
.	O
diﬀerent	O
ensemble	O
methods	O
construct	O
the	O
ensemble	O
of	O
models	O
in	O
diﬀerent	O
ways	O
.	O
for	O
example	O
,	O
each	O
member	O
of	O
the	O
ensemble	O
could	O
be	O
formed	O
by	O
training	O
a	O
completely	O
256	O
	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
original	O
dataset	O
first	O
resampled	O
dataset	O
first	O
ensemble	O
member	O
second	O
resampled	O
dataset	O
second	O
ensemble	O
member	O
8	O
8	O
figure	O
7.5	O
:	O
a	O
cartoon	O
depiction	O
of	O
how	O
bagging	O
works	O
.	O
suppose	O
we	O
train	O
an	O
8	O
detector	O
on	O
the	O
dataset	O
depicted	O
above	O
,	O
containing	O
an	O
8	O
,	O
a	O
6	O
and	O
a	O
9.	O
suppose	O
we	O
make	O
two	O
diﬀerent	O
resampled	O
datasets	O
.	O
the	O
bagging	O
training	O
procedure	O
is	O
to	O
construct	O
each	O
of	O
these	O
datasets	O
by	O
sampling	O
with	O
replacement	O
.	O
the	O
ﬁrst	O
dataset	O
omits	O
the	O
9	O
and	O
repeats	O
the	O
8.	O
on	O
this	O
dataset	O
,	O
the	O
detector	O
learns	O
that	O
a	O
loop	O
on	O
top	O
of	O
the	O
digit	O
corresponds	O
to	O
an	O
8.	O
on	O
the	O
second	O
dataset	O
,	O
we	O
repeat	O
the	O
9	O
and	O
omit	O
the	O
6.	O
in	O
this	O
case	O
,	O
the	O
detector	O
learns	O
that	O
a	O
loop	O
on	O
the	O
bottom	O
of	O
the	O
digit	O
corresponds	O
to	O
an	O
8.	O
each	O
of	O
these	O
individual	O
classiﬁcation	O
rules	O
is	O
brittle	O
,	O
but	O
if	O
we	O
average	O
their	O
output	O
then	O
the	O
detector	O
is	O
robust	O
,	O
achieving	O
maximal	O
conﬁdence	O
only	O
when	O
both	O
loops	O
of	O
the	O
8	O
are	O
present	O
.	O
diﬀerent	O
kind	O
of	O
model	B
using	O
a	O
diﬀerent	O
algorithm	O
or	O
objective	O
function	O
.	O
bagging	O
is	O
a	O
method	O
that	O
allows	O
the	O
same	O
kind	O
of	O
model	B
,	O
training	O
algorithm	O
and	O
objective	O
function	O
to	O
be	O
reused	O
several	O
times	O
.	O
speciﬁcally	O
,	O
bagging	O
involves	O
constructing	O
k	O
diﬀerent	O
datasets	O
.	O
each	O
dataset	O
has	O
the	O
same	O
number	O
of	O
examples	O
as	O
the	O
original	O
dataset	O
,	O
but	O
each	O
dataset	O
is	O
constructed	O
by	O
sampling	O
with	O
replacement	O
from	O
the	O
original	O
dataset	O
.	O
this	O
means	O
that	O
,	O
with	O
high	O
probability	O
,	O
each	O
dataset	O
is	O
missing	O
some	O
of	O
the	O
examples	O
from	O
the	O
original	O
dataset	O
and	O
also	O
contains	O
several	O
duplicate	O
examples	O
(	O
on	O
average	O
around	O
2/3	O
of	O
the	O
examples	O
from	O
the	O
original	O
dataset	O
are	O
found	O
in	O
the	O
resulting	O
training	O
set	O
,	O
if	O
it	O
has	O
the	O
same	O
size	O
as	O
the	O
original	O
)	O
.	O
model	B
i	O
is	O
then	O
trained	O
on	O
dataset	O
i.	O
the	O
diﬀerences	O
between	O
which	O
examples	O
are	O
included	O
in	O
each	O
dataset	O
result	O
in	O
diﬀerences	O
between	O
the	O
trained	O
models	O
.	O
see	O
ﬁgure	O
for	O
an	O
example	O
.	O
7.5	O
neural	O
networks	O
reach	O
a	O
wide	O
enough	O
variety	O
of	O
solution	O
points	O
that	O
they	O
can	O
often	O
beneﬁt	O
from	O
model	B
averaging	O
even	O
if	O
all	O
of	O
the	O
models	O
are	O
trained	O
on	O
the	O
same	O
dataset	O
.	O
diﬀerences	O
in	O
random	O
initialization	O
,	O
random	O
selection	O
of	O
minibatches	O
,	O
diﬀerences	O
in	O
hyperparameters	O
,	O
or	O
diﬀerent	O
outcomes	O
of	O
non-deterministic	O
imple-	O
mentations	O
of	O
neural	O
networks	O
are	O
often	O
enough	O
to	O
cause	O
diﬀerent	O
members	O
of	O
the	O
257	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
ensemble	O
to	O
make	O
partially	O
independent	O
errors	O
.	O
model	B
averaging	O
is	O
an	O
extremely	O
powerful	O
and	O
reliable	O
method	O
for	O
reducing	O
generalization	O
error	O
.	O
its	O
use	O
is	O
usually	O
discouraged	O
when	O
benchmarking	O
algorithms	O
for	O
scientiﬁc	O
papers	O
,	O
because	O
any	O
machine	O
learning	O
algorithm	O
can	O
beneﬁt	O
substan-	O
tially	O
from	O
model	B
averaging	O
at	O
the	O
price	O
of	O
increased	O
computation	O
and	O
memory	O
.	O
for	O
this	O
reason	O
,	O
benchmark	O
comparisons	O
are	O
usually	O
made	O
using	O
a	O
single	O
model	B
.	O
machine	O
learning	O
contests	O
are	O
usually	O
won	O
by	O
methods	O
using	O
model	B
averag-	O
ing	O
over	O
dozens	O
of	O
models	O
.	O
a	O
recent	O
prominent	O
example	O
is	O
the	O
netﬂix	O
grand	O
prize	O
(	O
koren	O
2009	O
)	O
.	O
,	O
not	O
all	O
techniques	O
for	O
constructing	O
ensembles	O
are	O
designed	O
to	O
make	O
the	O
ensemble	O
more	O
regularized	O
than	O
the	O
individual	O
models	O
.	O
for	O
example	O
,	O
a	O
technique	O
called	O
boosting	O
(	O
freund	O
and	O
schapire	O
1996b	O
a	O
,	O
)	O
constructs	O
an	O
ensemble	O
with	O
higher	O
capacity	O
than	O
the	O
individual	O
models	O
.	O
boosting	O
has	O
been	O
applied	O
to	O
build	O
ensembles	O
of	O
neural	O
networks	O
(	O
schwenk	O
and	O
bengio	O
1998	O
)	O
by	O
incrementally	O
adding	O
neural	O
networks	O
to	O
the	O
ensemble	O
.	O
boosting	O
has	O
also	O
been	O
applied	O
interpreting	O
an	O
individual	O
neural	O
network	O
as	O
an	O
ensemble	O
(	O
)	O
,	O
incrementally	O
adding	O
hidden	O
units	O
to	O
the	O
neural	O
network	O
.	O
bengio	O
et	O
al	O
.	O
2006a	O
,	O
,	O
,	O
7.12	O
dropout	O
2014	O
et	O
al.	O
,	O
)	O
provides	O
a	O
computationally	O
inexpensive	O
but	O
dropout	O
(	O
srivastava	O
powerful	O
method	O
of	O
regularizing	O
a	O
broad	O
family	O
of	O
models	O
.	O
to	O
a	O
ﬁrst	O
approximation	O
,	O
dropout	O
can	O
be	O
thought	O
of	O
as	O
a	O
method	O
of	O
making	O
bagging	O
practical	O
for	O
ensembles	O
of	O
very	O
many	O
large	O
neural	O
networks	O
.	O
bagging	O
involves	O
training	O
multiple	O
models	O
,	O
and	O
evaluating	O
multiple	O
models	O
on	O
each	O
test	O
example	O
.	O
this	O
seems	O
impractical	O
when	O
each	O
model	B
is	O
a	O
large	O
neural	O
network	O
,	O
since	O
training	O
and	O
evaluating	O
such	O
networks	O
is	O
costly	O
in	O
terms	O
of	O
runtime	O
and	O
memory	O
.	O
it	O
is	O
common	O
to	O
use	O
ensembles	O
of	O
ﬁve	O
to	O
ten	O
neural	O
networks—	O
)	O
used	O
six	O
to	O
win	O
the	O
ilsvrc—	O
but	O
more	O
than	O
this	O
rapidly	O
becomes	O
unwieldy	O
.	O
dropout	O
provides	O
an	O
inexpensive	O
approximation	O
to	O
training	O
and	O
evaluating	O
a	O
bagged	O
ensemble	O
of	O
exponentially	O
many	O
neural	O
networks	O
.	O
szegedy	O
et	O
al	O
.	O
2014a	O
(	O
speciﬁcally	O
,	O
dropout	O
trains	O
the	O
ensemble	O
consisting	O
of	O
all	O
sub-networks	O
that	O
can	O
be	O
formed	O
by	O
removing	O
non-output	O
units	O
from	O
an	O
underlying	O
base	O
network	O
,	O
as	O
illustrated	O
in	O
ﬁgure	O
.	O
in	O
most	O
modern	O
neural	O
networks	O
,	O
based	O
on	O
a	O
series	O
of	O
aﬃne	O
transformations	O
and	O
nonlinearities	O
,	O
we	O
can	O
eﬀectively	O
remove	O
a	O
unit	O
from	O
a	O
network	O
by	O
multiplying	O
its	O
output	O
value	O
by	O
zero	O
.	O
this	O
procedure	O
requires	O
some	O
slight	O
modiﬁcation	O
for	O
models	O
such	O
as	O
radial	O
basis	O
function	O
networks	O
,	O
which	O
take	O
7.6	O
258	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
the	O
diﬀerence	O
between	O
the	O
unit	O
’	O
s	O
state	O
and	O
some	O
reference	O
value	O
.	O
here	O
,	O
we	O
present	O
the	O
dropout	O
algorithm	O
in	O
terms	O
of	O
multiplication	O
by	O
zero	O
for	O
simplicity	O
,	O
but	O
it	O
can	O
be	O
trivially	O
modiﬁed	O
to	O
work	B
with	O
other	O
operations	O
that	O
remove	O
a	O
unit	O
from	O
the	O
network	O
.	O
recall	O
that	O
to	O
learn	O
with	O
bagging	O
,	O
we	O
deﬁne	O
k	O
diﬀerent	O
models	O
,	O
construct	O
k	O
diﬀerent	O
datasets	O
by	O
sampling	O
from	O
the	O
training	O
set	O
with	O
replacement	O
,	O
and	O
then	O
train	O
model	B
i	O
on	O
dataset	O
i.	O
dropout	O
aims	O
to	O
approximate	O
this	O
process	O
,	O
but	O
with	O
an	O
exponentially	O
large	O
number	O
of	O
neural	O
networks	O
.	O
speciﬁcally	O
,	O
to	O
train	O
with	O
dropout	O
,	O
we	O
use	O
a	O
minibatch-based	O
learning	O
algorithm	O
that	O
makes	O
small	O
steps	O
,	O
such	O
as	O
stochastic	O
gradient	O
descent	B
.	O
each	O
time	O
we	O
load	O
an	O
example	O
into	O
a	O
minibatch	O
,	O
we	O
randomly	O
sample	O
a	O
diﬀerent	O
binary	O
mask	O
to	O
apply	O
to	O
all	O
of	O
the	O
input	O
and	O
hidden	O
units	O
in	O
the	O
network	O
.	O
the	O
mask	O
for	O
each	O
unit	O
is	O
sampled	O
independently	O
from	O
all	O
of	O
the	O
others	O
.	O
the	O
probability	O
of	O
sampling	O
a	O
mask	O
value	O
of	O
one	O
(	O
causing	O
a	O
unit	O
to	O
be	O
included	O
)	O
is	O
a	O
hyperparameter	O
ﬁxed	O
before	O
training	O
begins	O
.	O
it	O
is	O
not	O
a	O
function	O
of	O
the	O
current	O
value	O
of	O
the	O
model	B
parameters	O
or	O
the	O
input	O
example	O
.	O
typically	O
,	O
an	O
input	O
unit	O
is	O
included	O
with	O
probability	O
0.8	O
and	O
a	O
hidden	O
unit	O
is	O
included	O
with	O
probability	O
0.5.	O
we	O
then	O
run	O
forward	O
propagation	O
,	O
back-propagation	O
,	O
and	O
the	O
learning	O
update	O
as	O
usual	O
.	O
figure	O
illustrates	O
how	O
to	O
run	O
forward	O
propagation	O
with	O
dropout	O
.	O
7.7	O
more	O
formally	O
,	O
suppose	O
that	O
a	O
mask	O
vector	O
µ	O
speciﬁes	O
which	O
units	O
to	O
include	O
,	O
)	O
deﬁnes	O
the	O
cost	O
of	O
the	O
model	B
deﬁned	O
by	O
parameters	O
θ	O
and	O
mask	O
µ.	O
and	O
j	O
(	O
θ	O
µ	O
,	O
then	O
dropout	O
training	O
consists	O
in	O
minimizing	O
eµj	O
(	O
θ	O
µ	O
,	O
)	O
.	O
the	O
expectation	O
contains	O
exponentially	O
many	O
terms	O
but	O
we	O
can	O
obtain	O
an	O
unbiased	O
estimate	O
of	O
its	O
gradient	O
by	O
sampling	O
values	O
of	O
.µ	O
dropout	O
training	O
is	O
not	O
quite	O
the	O
same	O
as	O
bagging	O
training	O
.	O
in	O
the	O
case	O
of	O
bagging	O
,	O
the	O
models	O
are	O
all	O
independent	O
.	O
in	O
the	O
case	O
of	O
dropout	O
,	O
the	O
models	O
share	O
parameters	O
,	O
with	O
each	O
model	B
inheriting	O
a	O
diﬀerent	O
subset	O
of	O
parameters	O
from	O
the	O
parent	O
neural	O
network	O
.	O
this	O
parameter	O
sharing	O
makes	O
it	O
possible	O
to	O
represent	O
an	O
exponential	O
number	O
of	O
models	O
with	O
a	O
tractable	O
amount	O
of	O
memory	O
.	O
in	O
the	O
case	O
of	O
bagging	O
,	O
each	O
model	B
is	O
trained	O
to	O
convergence	O
on	O
its	O
respective	O
training	O
set	O
.	O
in	O
the	O
case	O
of	O
dropout	O
,	O
typically	O
most	O
models	O
are	O
not	O
explicitly	O
trained	O
at	O
all—usually	O
,	O
the	O
model	B
is	O
large	O
enough	O
that	O
it	O
would	O
be	O
infeasible	O
to	O
sample	O
all	O
possible	O
sub-	O
networks	O
within	O
the	O
lifetime	O
of	O
the	O
universe	O
.	O
instead	O
,	O
a	O
tiny	O
fraction	O
of	O
the	O
possible	O
sub-networks	O
are	O
each	O
trained	O
for	O
a	O
single	O
step	O
,	O
and	O
the	O
parameter	O
sharing	O
causes	O
the	O
remaining	O
sub-networks	O
to	O
arrive	O
at	O
good	O
settings	O
of	O
the	O
parameters	O
.	O
these	O
are	O
the	O
only	O
diﬀerences	O
.	O
beyond	O
these	O
,	O
dropout	O
follows	O
the	O
bagging	O
algorithm	O
.	O
for	O
example	O
,	O
the	O
training	O
set	O
encountered	O
by	O
each	O
sub-network	O
is	O
indeed	O
a	O
subset	O
of	O
the	O
original	O
training	O
set	O
sampled	O
with	O
replacement	O
.	O
259	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
yy	O
h	O
1h	O
1	O
x	O
1x	O
1	O
h	O
2h	O
2	O
x	O
2x	O
2	O
base	O
network	O
h	O
1h	O
1	O
yy	O
yy	O
h	O
2h	O
2	O
x	O
2x	O
2	O
h	O
1h	O
1	O
x	O
1x	O
1	O
yy	O
yy	O
h	O
1h	O
1	O
h	O
2h	O
2	O
h	O
2h	O
2	O
x	O
1x	O
1	O
x	O
1x	O
1	O
x	O
2x	O
2	O
h	O
2h	O
2	O
x	O
1x	O
1	O
h	O
1h	O
1	O
yy	O
yy	O
yy	O
yy	O
x	O
2x	O
2	O
x	O
1x	O
1	O
h	O
2h	O
2	O
h	O
1h	O
1	O
h	O
2h	O
2	O
x	O
2x	O
2	O
h	O
2h	O
2	O
x	O
2x	O
2	O
yy	O
yy	O
yy	O
yy	O
h	O
1h	O
1	O
x	O
1x	O
1	O
h	O
1h	O
1	O
x	O
1x	O
1	O
h	O
1h	O
1	O
x	O
1x	O
1	O
yy	O
yy	O
yy	O
yy	O
h	O
2h	O
2	O
x	O
2x	O
2	O
x	O
2x	O
2	O
x	O
2x	O
2	O
ensemble	O
of	O
subnetworks	O
figure	O
7.6	O
:	O
dropout	O
trains	O
an	O
ensemble	O
consisting	O
of	O
all	O
sub-networks	O
that	O
can	O
be	O
constructed	O
by	O
removing	O
non-output	O
units	O
from	O
an	O
underlying	O
base	O
network	O
.	O
here	O
,	O
we	O
begin	O
with	O
a	O
base	O
network	O
with	O
two	O
visible	O
units	O
and	O
two	O
hidden	O
units	O
.	O
there	O
are	O
sixteen	O
possible	O
subsets	O
of	O
these	O
four	O
units	O
.	O
we	O
show	O
all	O
sixteen	O
subnetworks	O
that	O
may	O
be	O
formed	O
by	O
dropping	O
out	O
diﬀerent	O
subsets	O
of	O
units	O
from	O
the	O
original	O
network	O
.	O
in	O
this	O
small	O
example	O
,	O
a	O
large	O
proportion	O
of	O
the	O
resulting	O
networks	O
have	O
no	O
input	O
units	O
or	O
no	O
path	O
connecting	O
the	O
input	O
to	O
the	O
output	O
.	O
this	O
problem	O
becomes	O
insigniﬁcant	O
for	O
networks	O
with	O
wider	O
layers	O
,	O
where	O
the	O
probability	O
of	O
dropping	O
all	O
possible	O
paths	O
from	O
inputs	O
to	O
outputs	O
becomes	O
smaller	O
.	O
260	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
h	O
2h	O
2	O
x	O
2x	O
2	O
h	O
1h	O
1	O
x	O
1x	O
1	O
yy	O
yy	O
ˆh	O
1	O
ˆh	O
1	O
ˆh	O
2	O
ˆh	O
2	O
µh1	O
µh1	O
h	O
1h	O
1	O
h	O
2h	O
2	O
µh2	O
µh2	O
ˆx1ˆx1	O
ˆx2ˆx2	O
µx1	O
µx1	O
x	O
1x	O
1	O
x	O
2x	O
2	O
µx2	O
µx2	O
figure	O
7.7	O
:	O
an	O
example	O
of	O
forward	O
propagation	O
through	O
a	O
feedforward	O
network	O
using	O
dropout	O
.	O
(	O
top	O
)	O
in	O
this	O
example	O
,	O
we	O
use	O
a	O
feedforward	O
network	O
with	O
two	O
input	O
units	O
,	O
one	O
hidden	O
layer	O
with	O
two	O
hidden	O
units	O
,	O
and	O
one	O
output	O
unit	O
.	O
to	O
perform	O
forward	O
propagation	O
with	O
dropout	O
,	O
we	O
randomly	O
sample	O
a	O
vector	O
µ	O
with	O
one	O
entry	O
for	O
each	O
input	O
or	O
hidden	O
unit	O
in	O
the	O
network	O
.	O
the	O
entries	O
of	O
µ	O
are	O
binary	O
and	O
are	O
sampled	O
independently	O
from	O
each	O
other	O
.	O
the	O
probability	O
of	O
each	O
entry	O
being	O
0.5	O
for	O
the	O
hidden	O
layers	O
and	O
0.8	O
for	O
the	O
input	O
.	O
each	O
unit	O
in	O
the	O
network	O
is	O
multiplied	O
by	O
the	O
corresponding	O
mask	O
,	O
and	O
then	O
forward	O
propagation	O
continues	O
through	O
the	O
rest	O
of	O
the	O
network	O
as	O
usual	O
.	O
this	O
is	O
equivalent	O
to	O
randomly	O
selecting	O
one	O
of	O
the	O
sub-networks	O
from	O
ﬁgure	O
and	O
running	O
forward	O
propagation	O
through	O
it	O
.	O
(	O
bottom	O
)	O
1	O
is	O
a	O
hyperparameter	O
,	O
usually	O
7.6	O
261	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
to	O
make	O
a	O
prediction	O
,	O
a	O
bagged	O
ensemble	O
must	O
accumulate	O
votes	O
from	O
all	O
of	O
its	O
members	O
.	O
we	O
refer	O
to	O
this	O
process	O
as	O
inference	O
in	O
this	O
context	O
.	O
so	O
far	O
,	O
our	O
description	O
of	O
bagging	O
and	O
dropout	O
has	O
not	O
required	O
that	O
the	O
model	B
be	O
explicitly	O
probabilistic	O
.	O
now	O
,	O
we	O
assume	O
that	O
the	O
model	B
’	O
s	O
role	O
is	O
to	O
output	O
a	O
probability	O
distribution	O
.	O
in	O
the	O
case	O
of	O
bagging	O
,	O
each	O
model	B
i	O
produces	O
a	O
probability	O
distribution	O
p	O
(	O
)	O
i	O
(	O
y	O
x	O
)	O
.	O
the	O
prediction	O
of	O
the	O
ensemble	O
is	O
given	O
by	O
the	O
arithmetic	O
mean	O
of	O
all	O
of	O
these	O
distributions	O
,	O
	O
|	O
)	O
x	O
.	O
(	O
7.52	O
)	O
|	O
k	O
p	O
(	O
)	O
i	O
(	O
y	O
1	O
k	O
	O
i=1	O
|	O
p	O
p	O
y	O
(	O
)	O
µ	O
(	O
µ	O
in	O
the	O
case	O
of	O
dropout	O
,	O
each	O
sub-model	O
deﬁned	O
by	O
mask	O
vector	O
µ	O
deﬁnes	O
a	O
prob-	O
ability	O
distribution	O
p	O
(	O
y	O
x	O
µ	O
)	O
.	O
the	O
arithmetic	O
mean	O
over	O
all	O
masks	O
is	O
given	O
by	O
,	O
|	O
x	O
µ	O
)	O
,	O
(	O
7.53	O
)	O
where	O
p	O
(	O
µ	O
)	O
is	O
the	O
probability	O
distribution	O
that	O
was	O
used	O
to	O
sample	O
µ	O
at	O
training	O
time	O
.	O
because	O
this	O
sum	O
includes	O
an	O
exponential	O
number	O
of	O
terms	O
,	O
it	O
is	O
intractable	O
to	O
evaluate	O
except	O
in	O
cases	O
where	O
the	O
structure	O
of	O
the	O
model	B
permits	O
some	O
form	O
of	O
simpliﬁcation	O
.	O
so	O
far	O
,	O
deep	O
neural	O
nets	O
are	O
not	O
known	O
to	O
permit	O
any	O
tractable	O
simpliﬁcation	O
.	O
instead	O
,	O
we	O
can	O
approximate	O
the	O
inference	O
with	O
sampling	O
,	O
by	O
averaging	O
together	O
the	O
output	O
from	O
many	O
masks	O
.	O
even	O
10-20	O
masks	O
are	O
often	O
suﬃcient	O
to	O
obtain	O
good	O
performance	O
.	O
however	O
,	O
there	O
is	O
an	O
even	O
better	O
approach	O
,	O
that	O
allows	O
us	O
to	O
obtain	O
a	O
good	O
approximation	O
to	O
the	O
predictions	O
of	O
the	O
entire	O
ensemble	O
,	O
at	O
the	O
cost	O
of	O
only	O
one	O
forward	O
propagation	O
.	O
to	O
do	O
so	O
,	O
we	O
change	O
to	O
using	O
the	O
geometric	O
mean	O
rather	O
than	O
the	O
arithmetic	O
mean	O
of	O
the	O
ensemble	O
members	O
’	O
predicted	O
distributions	O
.	O
warde-	O
farley	O
)	O
present	O
arguments	O
and	O
empirical	O
evidence	O
that	O
the	O
geometric	O
mean	O
performs	O
comparably	O
to	O
the	O
arithmetic	O
mean	O
in	O
this	O
context	O
.	O
et	O
al	O
.	O
(	O
2014	O
the	O
geometric	O
mean	O
of	O
multiple	O
probability	O
distributions	O
is	O
not	O
guaranteed	O
to	O
be	O
a	O
probability	O
distribution	O
.	O
to	O
guarantee	O
that	O
the	O
result	O
is	O
a	O
probability	O
distribution	O
,	O
we	O
impose	O
the	O
requirement	O
that	O
none	O
of	O
the	O
sub-models	O
assigns	O
probability	O
0	O
to	O
any	O
event	O
,	O
and	O
we	O
renormalize	O
the	O
resulting	O
distribution	O
.	O
the	O
unnormalized	O
probability	O
distribution	O
deﬁned	O
directly	O
by	O
the	O
geometric	O
mean	O
is	O
given	O
by	O
	O
|	O
|	O
˜pensemble	O
(	O
y	O
x	O
)	O
=	O
2d	O
p	O
y	O
(	O
µ	O
x	O
µ	O
)	O
,	O
(	O
7.54	O
)	O
where	O
d	O
is	O
the	O
number	O
of	O
units	O
that	O
may	O
be	O
dropped	O
.	O
here	O
we	O
use	O
a	O
uniform	O
distribution	O
over	O
µ	O
to	O
simplify	O
the	O
presentation	O
,	O
but	O
non-uniform	O
distributions	O
are	O
262	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
	O
also	O
possible	O
.	O
to	O
make	O
predictions	O
we	O
must	O
re-normalize	O
the	O
ensemble	O
:	O
|	O
x	O
)	O
=	O
pensemble	O
(	O
y	O
|	O
˜pensemble	O
(	O
y	O
	O
˜pensemble	O
(	O
y	O
y	O
	O
|	O
)	O
x	O
.	O
x	O
)	O
(	O
7.55	O
)	O
|	O
a	O
key	O
insight	O
(	O
hinton	O
et	O
al	O
.	O
2012c	O
)	O
involved	O
in	O
dropout	O
is	O
that	O
we	O
can	O
approxi-	O
,	O
mate	O
pensemble	O
by	O
evaluating	O
p	O
(	O
y	O
x	O
)	O
in	O
one	O
model	B
:	O
the	O
model	B
with	O
all	O
units	O
,	O
but	O
with	O
the	O
weights	O
going	O
out	O
of	O
unit	O
i	O
multiplied	O
by	O
the	O
probability	O
of	O
including	O
unit	O
i.	O
the	O
motivation	O
for	O
this	O
modiﬁcation	O
is	O
to	O
capture	O
the	O
right	O
expected	O
value	O
of	O
the	O
output	O
from	O
that	O
unit	O
.	O
we	O
call	O
this	O
approach	O
the	O
weight	O
scaling	O
inference	O
rule	O
.	O
there	O
is	O
not	O
yet	O
any	O
theoretical	O
argument	O
for	O
the	O
accuracy	O
of	O
this	O
approximate	O
inference	O
rule	O
in	O
deep	O
nonlinear	O
networks	O
,	O
but	O
empirically	O
it	O
performs	O
very	O
well	O
.	O
because	O
we	O
usually	O
use	O
an	O
inclusion	O
probability	O
of	O
1	O
2	O
,	O
the	O
weight	O
scaling	O
rule	O
usually	O
amounts	O
to	O
dividing	O
the	O
weights	O
by	O
at	O
the	O
end	O
of	O
training	O
,	O
and	O
then	O
using	O
the	O
model	B
as	O
usual	O
.	O
another	O
way	O
to	O
achieve	O
the	O
same	O
result	O
is	O
to	O
multiply	O
the	O
states	O
of	O
the	O
units	O
by	O
during	O
training	O
.	O
either	O
way	O
,	O
the	O
goal	O
is	O
to	O
make	O
sure	O
that	O
the	O
expected	O
total	O
input	O
to	O
a	O
unit	O
at	O
test	O
time	O
is	O
roughly	O
the	O
same	O
as	O
the	O
expected	O
total	O
input	O
to	O
that	O
unit	O
at	O
train	O
time	O
,	O
even	O
though	O
half	O
the	O
units	O
at	O
train	O
time	O
are	O
missing	O
on	O
average	O
.	O
2	O
2	O
for	O
many	O
classes	O
of	O
models	O
that	O
do	O
not	O
have	O
nonlinear	O
hidden	O
units	O
,	O
the	O
weight	O
scaling	O
inference	O
rule	O
is	O
exact	O
.	O
for	O
a	O
simple	O
example	O
,	O
consider	O
a	O
softmax	O
regression	O
classiﬁer	O
with	O
input	O
variables	O
represented	O
by	O
the	O
vector	O
v	O
n	O
:	O
	O
p	O
(	O
=	O
y	O
y	O
v	O
)	O
=	O
softmax	O
(	O
7.56	O
)	O
|	O
	O
w	O
v	O
+	O
b	O
.	O
y	O
we	O
can	O
index	O
into	O
the	O
family	O
of	O
sub-models	O
by	O
element-wise	O
multiplication	O
of	O
the	O
input	O
with	O
a	O
binary	O
vector	O
|	O
	O
:	O
d	O
	O
p	O
(	O
=	O
y	O
y	O
v	O
;	O
d	O
)	O
=	O
softmax	O
w	O
(	O
d	O
v	O
)	O
+	O
b	O
(	O
7.57	O
)	O
	O
	O
	O
.	O
y	O
the	O
ensemble	O
predictor	O
is	O
deﬁned	O
by	O
re-normalizing	O
the	O
geometric	O
mean	O
over	O
all	O
ensemble	O
members	O
’	O
predictions	O
:	O
pensemble	O
(	O
=	O
y	O
y	O
|	O
v	O
)	O
=	O
	O
	O
	O
|	O
y	O
˜pensemble	O
(	O
=	O
y	O
	O
˜pensemble	O
(	O
=	O
y	O
y	O
y	O
)	O
v	O
	O
|	O
v	O
)	O
where	O
˜pensemble	O
(	O
=	O
y	O
y	O
|	O
v	O
)	O
=	O
2n	O
|	O
v	O
;	O
.	O
)	O
d	O
p	O
(	O
=	O
y	O
y	O
∈	O
{	O
d	O
}	O
0	O
1	O
,	O
n	O
263	O
(	O
7.58	O
)	O
(	O
7.59	O
)	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
	O
	O
to	O
see	O
that	O
the	O
weight	O
scaling	O
rule	O
is	O
exact	O
,	O
we	O
can	O
simplify	O
˜pensemble	O
:	O
˜pensemble	O
(	O
=	O
y	O
y	O
p	O
(	O
=	O
y	O
y	O
v	O
;	O
)	O
d	O
(	O
7.60	O
)	O
=	O
2n	O
softmax	O
(	O
w	O
|	O
	O
	O
	O
	O
	O
	O
∈	O
{	O
d	O
∈	O
{	O
d	O
}	O
0	O
1	O
,	O
n	O
}	O
0	O
1	O
,	O
n	O
=	O
2	O
n	O
∈	O
{	O
d	O
0	O
1	O
,	O
n	O
exp	O
2n	O
=	O
2n	O
∈	O
{	O
d	O
}	O
0	O
1	O
,	O
n	O
|	O
	O
	O
	O
v	O
)	O
=	O
2n	O
y	O
}	O
	O
	O
	O
	O
w	O
exp	O
	O
exp	O
	O
	O
	O
	O
	O
	O
∈	O
{	O
d	O
	O
exp	O
2n	O
y	O
w	O
∈	O
{	O
d	O
}	O
0	O
1	O
,	O
n	O
∈	O
{	O
d	O
}	O
0	O
1	O
,	O
n	O
	O
	O
	O
	O
(	O
d	O
v	O
)	O
+	O
)	O
b	O
y	O
	O
by	O
	O
	O
by	O
)	O
+	O
v	O
	O
)	O
+	O
v	O
y	O
,	O
:	O
(	O
d	O
	O
	O
	O
d	O
(	O
w	O
,	O
:	O
y	O
	O
	O
y	O
,	O
:	O
(	O
d	O
	O
	O
,	O
:	O
y	O
)	O
+	O
v	O
	O
w	O
(	O
d	O
v	O
w	O
b	O
y	O
)	O
+	O
	O
by	O
	O
}	O
0	O
1	O
,	O
n	O
	O
d	O
y	O
,	O
:	O
(	O
w	O
exp	O
	O
	O
v	O
)	O
+	O
by	O
	O
d	O
y	O
,	O
:	O
(	O
˜pensemble	O
(	O
=	O
y	O
y	O
∝	O
)	O
|	O
	O
v	O
=	O
exp	O
1	O
2	O
n	O
=	O
exp	O
	O
y	O
,	O
:v	O
+	O
by	O
w	O
1	O
2	O
.	O
(	O
7.61	O
)	O
(	O
7.62	O
)	O
(	O
7.63	O
)	O
	O
v	O
)	O
+	O
by	O
(	O
7.64	O
)	O
(	O
7.65	O
)	O
(	O
7.66	O
)	O
because	O
˜p	O
will	O
be	O
normalized	O
,	O
we	O
can	O
safely	O
ignore	O
multiplication	O
by	O
factors	O
that	O
are	O
constant	O
with	O
respect	O
to	O
:	O
y	O
substituting	O
this	O
back	O
into	O
equation	O
1	O
2w	O
.	O
7.58	O
we	O
obtain	O
a	O
softmax	O
classiﬁer	O
with	O
weights	O
the	O
weight	O
scaling	O
rule	O
is	O
also	O
exact	O
in	O
other	O
settings	O
,	O
including	O
regression	O
networks	O
with	O
conditionally	O
normal	O
outputs	O
,	O
and	O
deep	O
networks	O
that	O
have	O
hidden	O
layers	O
without	O
nonlinearities	O
.	O
however	O
,	O
the	O
weight	O
scaling	O
rule	O
is	O
only	O
an	O
approxi-	O
mation	O
for	O
deep	O
models	O
that	O
have	O
nonlinearities	O
.	O
though	O
the	O
approximation	O
has	O
not	O
been	O
theoretically	O
characterized	O
,	O
it	O
often	O
works	O
well	O
,	O
empirically	O
.	O
goodfellow	O
et	O
al	O
.	O
(	O
)	O
found	O
experimentally	O
that	O
the	O
weight	O
scaling	O
approximation	O
can	O
work	B
better	O
(	O
in	O
terms	O
of	O
classiﬁcation	O
accuracy	O
)	O
than	O
monte	O
carlo	O
approximations	O
to	O
the	O
ensemble	O
predictor	O
.	O
this	O
held	O
true	O
even	O
when	O
the	O
monte	O
carlo	O
approximation	O
was	O
allowed	O
to	O
sample	O
up	O
to	O
1,000	O
sub-networks.	O
)	O
found	O
that	O
some	O
models	O
obtain	O
better	O
classiﬁcation	O
accuracy	O
using	O
twenty	O
samples	O
and	O
gal	O
and	O
ghahramani	O
2015	O
2013a	O
(	O
264	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
the	O
monte	O
carlo	O
approximation	O
.	O
it	O
appears	O
that	O
the	O
optimal	O
choice	O
of	O
inference	O
approximation	O
is	O
problem-dependent	O
.	O
2014	O
et	O
al	O
.	O
(	O
srivastava	O
)	O
showed	O
that	O
dropout	O
is	O
more	O
eﬀective	O
than	O
other	O
standard	O
computationally	O
inexpensive	O
regularizers	O
,	O
such	O
as	O
weight	O
decay	O
,	O
ﬁlter	O
norm	O
constraints	O
and	O
sparse	O
activity	O
regularization	O
.	O
dropout	O
may	O
also	O
be	O
combined	O
with	O
other	O
forms	O
of	O
regularization	O
to	O
yield	O
a	O
further	O
improvement	O
.	O
one	O
advantage	O
of	O
dropout	O
is	O
that	O
it	O
is	O
very	O
computationally	O
cheap	O
.	O
using	O
dropout	O
during	O
training	O
requires	O
only	O
o	O
(	O
n	O
)	O
computation	O
per	O
example	O
per	O
update	O
,	O
to	O
generate	O
n	O
random	O
binary	O
numbers	O
and	O
multiply	O
them	O
by	O
the	O
state	O
.	O
depending	O
on	O
the	O
implementation	O
,	O
it	O
may	O
also	O
require	O
o	O
(	O
n	O
)	O
memory	O
to	O
store	O
these	O
binary	O
numbers	O
until	O
the	O
back-propagation	O
stage	O
.	O
running	O
inference	O
in	O
the	O
trained	O
model	B
has	O
the	O
same	O
cost	O
per-example	O
as	O
if	O
dropout	O
were	O
not	O
used	O
,	O
though	O
we	O
must	O
pay	O
the	O
cost	O
of	O
dividing	O
the	O
weights	O
by	O
2	O
once	O
before	O
beginning	O
to	O
run	O
inference	O
on	O
examples	O
.	O
another	O
signiﬁcant	O
advantage	O
of	O
dropout	O
is	O
that	O
it	O
does	O
not	O
signiﬁcantly	O
limit	O
the	O
type	O
of	O
model	B
or	O
training	O
procedure	O
that	O
can	O
be	O
used	O
.	O
it	O
works	O
well	O
with	O
nearly	O
any	O
model	B
that	O
uses	O
a	O
distributed	O
representation	O
and	O
can	O
be	O
trained	O
with	O
stochastic	O
gradient	O
descent	B
.	O
this	O
includes	O
feedforward	O
neural	O
networks	O
,	O
probabilistic	O
models	O
such	O
as	O
restricted	O
boltzmann	O
machines	O
(	O
srivastava	O
)	O
,	O
and	O
recurrent	O
neural	O
networks	O
(	O
bayer	O
and	O
osendorfer	O
2014	O
pascanu	O
)	O
.	O
many	O
other	O
regularization	O
strategies	O
of	O
comparable	O
power	O
impose	O
more	O
severe	O
restrictions	O
on	O
the	O
architecture	O
of	O
the	O
model	B
.	O
et	O
al.	O
,	O
et	O
al.	O
,	O
2014a	O
2014	O
,	O
;	O
though	O
the	O
cost	O
per-step	O
of	O
applying	O
dropout	O
to	O
a	O
speciﬁc	O
model	B
is	O
negligible	O
,	O
the	O
cost	O
of	O
using	O
dropout	O
in	O
a	O
complete	O
system	O
can	O
be	O
signiﬁcant	O
.	O
because	O
dropout	O
is	O
a	O
regularization	O
technique	O
,	O
it	O
reduces	O
the	O
eﬀective	O
capacity	O
of	O
a	O
model	B
.	O
to	O
oﬀset	O
this	O
eﬀect	O
,	O
we	O
must	O
increase	O
the	O
size	O
of	O
the	O
model	B
.	O
typically	O
the	O
optimal	O
validation	O
set	O
error	O
is	O
much	O
lower	O
when	O
using	O
dropout	O
,	O
but	O
this	O
comes	O
at	O
the	O
cost	O
of	O
a	O
much	O
larger	O
model	B
and	O
many	O
more	O
iterations	O
of	O
the	O
training	O
algorithm	O
.	O
for	O
very	O
large	O
datasets	O
,	O
regularization	O
confers	O
little	O
reduction	O
in	O
generalization	O
error	O
.	O
in	O
these	O
cases	O
,	O
the	O
computational	O
cost	O
of	O
using	O
dropout	O
and	O
larger	O
models	O
may	O
outweigh	O
the	O
beneﬁt	O
of	O
regularization	O
.	O
when	O
extremely	O
few	O
labeled	O
training	O
examples	O
are	O
available	O
,	O
dropout	O
is	O
less	O
)	O
outperform	O
dropout	O
on	O
the	O
)	O
where	O
fewer	O
than	O
5,000	O
examples	O
)	O
.	O
when	O
additional	O
unlabeled	O
data	O
is	O
available	O
,	O
eﬀective	O
.	O
bayesian	O
neural	O
networks	O
(	O
alternative	O
splicing	O
dataset	O
(	O
are	O
available	O
(	O
srivastava	O
et	O
al.	O
,	O
unsupervised	O
feature	O
learning	O
can	O
gain	O
an	O
advantage	O
over	O
dropout	O
.	O
,	O
xiong	O
et	O
al	O
.	O
2011	O
neal	O
1996	O
2014	O
,	O
wager	O
)	O
showed	O
that	O
,	O
when	O
applied	O
to	O
linear	O
regression	O
,	O
dropout	O
is	O
equivalent	O
to	O
l2	O
weight	O
decay	O
,	O
with	O
a	O
diﬀerent	O
weight	O
decay	O
coeﬃcient	O
for	O
et	O
al	O
.	O
(	O
2013	O
265	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
each	O
input	O
feature	O
.	O
the	O
magnitude	O
of	O
each	O
feature	O
’	O
s	O
weight	O
decay	O
coeﬃcient	O
is	O
determined	O
by	O
its	O
variance	O
.	O
similar	O
results	O
hold	O
for	O
other	O
linear	O
models	O
.	O
for	O
deep	O
models	O
,	O
dropout	O
is	O
not	O
equivalent	O
to	O
weight	O
decay	O
.	O
(	O
the	O
stochasticity	O
used	O
while	O
training	O
with	O
dropout	O
is	O
not	O
necessary	O
for	O
the	O
approach	O
’	O
s	O
success	O
.	O
it	O
is	O
just	O
a	O
means	O
of	O
approximating	O
the	O
sum	O
over	O
all	O
sub-	O
models	O
.	O
wang	O
and	O
manning	O
2013	O
)	O
derived	O
analytical	O
approximations	O
to	O
this	O
marginalization	O
.	O
their	O
approximation	O
,	O
known	O
as	O
fast	O
dropout	O
resulted	O
in	O
faster	O
convergence	O
time	O
due	O
to	O
the	O
reduced	O
stochasticity	O
in	O
the	O
computation	O
of	O
the	O
gradient	O
.	O
this	O
method	O
can	O
also	O
be	O
applied	O
at	O
test	O
time	O
,	O
as	O
a	O
more	O
principled	O
(	O
but	O
also	O
more	O
computationally	O
expensive	O
)	O
approximation	O
to	O
the	O
average	O
over	O
all	O
sub-networks	O
than	O
the	O
weight	O
scaling	O
approximation	O
.	O
fast	O
dropout	O
has	O
been	O
used	O
to	O
nearly	O
match	O
the	O
performance	O
of	O
standard	O
dropout	O
on	O
small	O
neural	O
network	O
problems	O
,	O
but	O
has	O
not	O
yet	O
yielded	O
a	O
signiﬁcant	O
improvement	O
or	O
been	O
applied	O
to	O
a	O
large	O
problem	O
.	O
et	O
al	O
.	O
(	O
just	O
as	O
stochasticity	O
is	O
not	O
necessary	O
to	O
achieve	O
the	O
regularizing	O
eﬀect	O
of	O
2014	O
)	O
dropout	O
,	O
it	O
is	O
also	O
not	O
suﬃcient	O
.	O
to	O
demonstrate	O
this	O
,	O
warde-farley	O
designed	O
control	O
experiments	O
using	O
a	O
method	O
called	O
dropout	O
boosting	O
that	O
they	O
designed	O
to	O
use	O
exactly	O
the	O
same	O
mask	O
noise	O
as	O
traditional	O
dropout	O
but	O
lack	O
its	O
regularizing	O
eﬀect	O
.	O
dropout	O
boosting	O
trains	O
the	O
entire	O
ensemble	O
to	O
jointly	O
maximize	O
the	O
log-likelihood	O
on	O
the	O
training	O
set	O
.	O
in	O
the	O
same	O
sense	O
that	O
traditional	O
dropout	O
is	O
analogous	O
to	O
bagging	O
,	O
this	O
approach	O
is	O
analogous	O
to	O
boosting	O
.	O
as	O
intended	O
,	O
experiments	O
with	O
dropout	O
boosting	O
show	O
almost	O
no	O
regularization	O
eﬀect	O
compared	O
to	O
training	O
the	O
entire	O
network	O
as	O
a	O
single	O
model	B
.	O
this	O
demonstrates	O
that	O
the	O
interpretation	O
of	O
dropout	O
as	O
bagging	O
has	O
value	O
beyond	O
the	O
interpretation	O
of	O
dropout	O
as	O
robustness	O
to	O
noise	O
.	O
the	O
regularization	O
eﬀect	O
of	O
the	O
bagged	O
ensemble	O
is	O
only	O
achieved	O
when	O
the	O
stochastically	O
sampled	O
ensemble	O
members	O
are	O
trained	O
to	O
perform	O
well	O
independently	O
of	O
each	O
other	O
.	O
dropout	O
has	O
inspired	O
other	O
stochastic	O
approaches	O
to	O
training	O
exponentially	O
large	O
ensembles	O
of	O
models	O
that	O
share	O
weights	O
.	O
dropconnect	O
is	O
a	O
special	O
case	O
of	O
dropout	O
where	O
each	O
product	O
between	O
a	O
single	O
scalar	O
weight	O
and	O
a	O
single	O
hidden	O
unit	O
state	O
is	O
considered	O
a	O
unit	O
that	O
can	O
be	O
dropped	O
(	O
wan	O
)	O
.	O
stochastic	O
et	O
al.	O
,	O
pooling	O
is	O
a	O
form	O
of	O
randomized	O
pooling	O
(	O
see	O
section	O
9.3	O
)	O
for	O
building	O
ensembles	O
of	O
convolutional	O
networks	O
with	O
each	O
convolutional	O
network	O
attending	O
to	O
diﬀerent	O
spatial	O
locations	O
of	O
each	O
feature	O
map	O
.	O
so	O
far	O
,	O
dropout	O
remains	O
the	O
most	O
widely	O
used	O
implicit	O
ensemble	O
method	O
.	O
2013	O
one	O
of	O
the	O
key	O
insights	O
of	O
dropout	O
is	O
that	O
training	O
a	O
network	O
with	O
stochastic	O
behavior	O
and	O
making	O
predictions	O
by	O
averaging	O
over	O
multiple	O
stochastic	O
decisions	O
implements	O
a	O
form	O
of	O
bagging	O
with	O
parameter	O
sharing	O
.	O
earlier	O
,	O
we	O
described	O
266	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
dropout	O
as	O
bagging	O
an	O
ensemble	O
of	O
models	O
formed	O
by	O
including	O
or	O
excluding	O
units	O
.	O
however	O
,	O
there	O
is	O
no	O
need	O
for	O
this	O
model	B
averaging	O
strategy	O
to	O
be	O
based	O
on	O
inclusion	O
and	O
exclusion	O
.	O
in	O
principle	O
,	O
any	O
kind	O
of	O
random	O
modiﬁcation	O
is	O
admissible	O
.	O
in	O
practice	O
,	O
we	O
must	O
choose	O
modiﬁcation	O
families	O
that	O
neural	O
networks	O
are	O
able	O
to	O
learn	O
to	O
resist	O
.	O
ideally	O
,	O
we	O
should	O
also	O
use	O
model	B
families	O
that	O
allow	O
a	O
fast	O
approximate	O
inference	O
rule	O
.	O
we	O
can	O
think	O
of	O
any	O
form	O
of	O
modiﬁcation	O
parametrized	O
by	O
a	O
vector	O
µ	O
as	O
training	O
an	O
ensemble	O
consisting	O
of	O
p	O
(	O
y	O
x	O
µ	O
)	O
for	O
all	O
possible	O
values	O
of	O
µ.	O
there	O
is	O
no	O
requirement	O
that	O
µ	O
have	O
a	O
ﬁnite	O
number	O
of	O
values	O
.	O
for	O
example	O
,	O
µ	O
can	O
be	O
real-valued	O
.	O
srivastava	O
)	O
showed	O
that	O
multiplying	O
the	O
weights	O
by	O
µ	O
(	O
1	O
,	O
i	O
)	O
can	O
outperform	O
dropout	O
based	O
on	O
binary	O
masks	O
.	O
because	O
e	O
[	O
µ	O
]	O
=	O
1	O
the	O
standard	O
network	O
automatically	O
implements	O
approximate	O
inference	O
in	O
the	O
ensemble	O
,	O
without	O
needing	O
any	O
weight	O
scaling	O
.	O
∼	O
n	O
et	O
al	O
.	O
(	O
2014	O
|	O
,	O
so	O
far	O
we	O
have	O
described	O
dropout	O
purely	O
as	O
a	O
means	O
of	O
performing	O
eﬃcient	O
,	O
approximate	O
bagging	O
.	O
however	O
,	O
there	O
is	O
another	O
view	O
of	O
dropout	O
that	O
goes	O
further	O
than	O
this	O
.	O
dropout	O
trains	O
not	O
just	O
a	O
bagged	O
ensemble	O
of	O
models	O
,	O
but	O
an	O
ensemble	O
of	O
models	O
that	O
share	O
hidden	O
units	O
.	O
this	O
means	O
each	O
hidden	O
unit	O
must	O
be	O
able	O
to	O
perform	O
well	O
regardless	O
of	O
which	O
other	O
hidden	O
units	O
are	O
in	O
the	O
model	B
.	O
hidden	O
units	O
must	O
be	O
prepared	O
to	O
be	O
swapped	O
and	O
interchanged	O
between	O
models	O
.	O
hinton	O
et	O
al	O
.	O
(	O
2012c	O
)	O
were	O
inspired	O
by	O
an	O
idea	O
from	O
biology	O
:	O
sexual	O
reproduction	O
,	O
which	O
involves	O
swapping	O
genes	O
between	O
two	O
diﬀerent	O
organisms	O
,	O
creates	O
evolutionary	O
pressure	O
for	O
genes	O
to	O
become	O
not	O
just	O
good	O
,	O
but	O
to	O
become	O
readily	O
swapped	O
between	O
diﬀerent	O
organisms	O
.	O
such	O
genes	O
and	O
such	O
features	O
are	O
very	O
robust	O
to	O
changes	O
in	O
their	O
environment	O
because	O
they	O
are	O
not	O
able	O
to	O
incorrectly	O
adapt	O
to	O
unusual	O
features	O
of	O
any	O
one	O
organism	O
or	O
model	B
.	O
dropout	O
thus	O
regularizes	O
each	O
hidden	O
unit	O
to	O
be	O
not	O
merely	O
a	O
good	O
feature	O
but	O
a	O
feature	O
that	O
is	O
good	O
in	O
many	O
contexts	O
.	O
warde-	O
farley	O
)	O
compared	O
dropout	O
training	O
to	O
training	O
of	O
large	O
ensembles	O
and	O
concluded	O
that	O
dropout	O
oﬀers	O
additional	O
improvements	O
to	O
generalization	O
error	O
beyond	O
those	O
obtained	O
by	O
ensembles	O
of	O
independent	O
models	O
.	O
et	O
al	O
.	O
(	O
2014	O
it	O
is	O
important	O
to	O
understand	O
that	O
a	O
large	O
portion	O
of	O
the	O
power	O
of	O
dropout	O
arises	O
from	O
the	O
fact	O
that	O
the	O
masking	O
noise	O
is	O
applied	O
to	O
the	O
hidden	O
units	O
.	O
this	O
can	O
be	O
seen	O
as	O
a	O
form	O
of	O
highly	O
intelligent	O
,	O
adaptive	O
destruction	O
of	O
the	O
information	O
content	O
of	O
the	O
input	O
rather	O
than	O
destruction	O
of	O
the	O
raw	O
values	O
of	O
the	O
input	O
.	O
for	O
example	O
,	O
if	O
the	O
model	B
learns	O
a	O
hidden	O
unit	O
hi	O
that	O
detects	O
a	O
face	O
by	O
ﬁnding	O
the	O
nose	O
,	O
then	O
dropping	O
h	O
i	O
corresponds	O
to	O
erasing	O
the	O
information	O
that	O
there	O
is	O
a	O
nose	O
in	O
the	O
image	O
.	O
the	O
model	B
must	O
learn	O
another	O
h	O
i	O
,	O
either	O
that	O
redundantly	O
encodes	O
the	O
presence	O
of	O
a	O
nose	O
,	O
or	O
that	O
detects	O
the	O
face	O
by	O
another	O
feature	O
,	O
such	O
as	O
the	O
mouth	O
.	O
traditional	O
noise	O
injection	O
techniques	O
that	O
add	O
unstructured	O
noise	O
at	O
the	O
input	O
are	O
not	O
able	O
to	O
randomly	O
erase	O
the	O
information	O
about	O
a	O
nose	O
from	O
an	O
image	O
of	O
a	O
face	O
unless	O
the	O
magnitude	O
of	O
the	O
noise	O
is	O
so	O
great	O
that	O
nearly	O
all	O
of	O
the	O
information	O
in	O
267	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
the	O
image	O
is	O
removed	O
.	O
destroying	O
extracted	O
features	O
rather	O
than	O
original	O
values	O
allows	O
the	O
destruction	O
process	O
to	O
make	O
use	O
of	O
all	O
of	O
the	O
knowledge	O
about	O
the	O
input	O
distribution	O
that	O
the	O
model	B
has	O
acquired	O
so	O
far	O
.	O
another	O
important	O
aspect	O
of	O
dropout	O
is	O
that	O
the	O
noise	O
is	O
multiplicative	O
.	O
if	O
the	O
noise	O
were	O
additive	O
with	O
ﬁxed	O
scale	O
,	O
then	O
a	O
rectiﬁed	O
linear	O
hidden	O
unit	O
hi	O
with	O
added	O
noise	O
	O
could	O
simply	O
learn	O
to	O
have	O
h	O
i	O
become	O
very	O
large	O
in	O
order	O
to	O
make	O
the	O
added	O
noise	O
	O
insigniﬁcant	O
by	O
comparison	O
.	O
multiplicative	O
noise	O
does	O
not	O
allow	O
such	O
a	O
pathological	O
solution	O
to	O
the	O
noise	O
robustness	O
problem	O
.	O
another	O
deep	O
learning	O
algorithm	O
,	O
batch	O
normalization	O
,	O
reparametrizes	O
the	O
model	B
in	O
a	O
way	O
that	O
introduces	O
both	O
additive	O
and	O
multiplicative	O
noise	O
on	O
the	O
hidden	O
units	O
at	O
training	O
time	O
.	O
the	O
primary	O
purpose	O
of	O
batch	O
normalization	O
is	O
to	O
improve	O
optimization	O
,	O
but	O
the	O
noise	O
can	O
have	O
a	O
regularizing	O
eﬀect	O
,	O
and	O
sometimes	O
makes	O
dropout	O
unnecessary	O
.	O
batch	O
normalization	O
is	O
described	O
further	O
in	O
section	O
8.7.1	O
.	O
7.13	O
adversarial	O
training	O
in	O
many	O
cases	O
,	O
neural	O
networks	O
have	O
begun	O
to	O
reach	O
human	O
performance	O
when	O
evaluated	O
on	O
an	O
i.i.d	O
.	O
test	O
set	O
.	O
it	O
is	O
natural	O
therefore	O
to	O
wonder	O
whether	O
these	O
models	O
have	O
obtained	O
a	O
true	O
human-level	O
understanding	O
of	O
these	O
tasks	O
.	O
in	O
order	O
to	O
probe	O
the	O
level	O
of	O
understanding	O
a	O
network	O
has	O
of	O
the	O
underlying	O
task	O
,	O
we	O
can	O
search	O
for	O
examples	O
that	O
the	O
model	B
misclassiﬁes.	O
)	O
found	O
that	O
even	O
neural	O
networks	O
that	O
perform	O
at	O
human	O
level	O
accuracy	O
have	O
a	O
nearly	O
100	O
%	O
error	O
rate	O
on	O
examples	O
that	O
are	O
intentionally	O
constructed	O
by	O
using	O
an	O
optimization	O
	O
procedure	O
to	O
search	O
for	O
an	O
input	O
x	O
near	O
a	O
data	O
point	O
x	O
such	O
that	O
the	O
model	B
output	O
is	O
very	O
diﬀerent	O
at	O
x	O
can	O
be	O
so	O
similar	O
to	O
x	O
that	O
a	O
human	O
observer	O
can	O
not	O
tell	O
the	O
diﬀerence	O
between	O
the	O
original	O
example	O
and	O
the	O
adversarial	O
example	O
,	O
but	O
the	O
network	O
can	O
make	O
highly	O
diﬀerent	O
predictions	O
.	O
see	O
ﬁgure	O
	O
.	O
in	O
many	O
cases	O
,	O
x	O
szegedy	O
et	O
al	O
.	O
2014b	O
for	O
an	O
example	O
.	O
7.8	O
(	O
	O
adversarial	O
examples	O
have	O
many	O
implications	O
,	O
for	O
example	O
,	O
in	O
computer	O
security	O
,	O
that	O
are	O
beyond	O
the	O
scope	O
of	O
this	O
chapter	O
.	O
however	O
,	O
they	O
are	O
interesting	O
in	O
the	O
context	O
of	O
regularization	O
because	O
one	O
can	O
reduce	O
the	O
error	O
rate	O
on	O
the	O
original	O
i.i.d	O
.	O
test	O
set	O
via	O
adversarial	O
training—training	O
on	O
adversarially	O
perturbed	O
examples	O
from	O
the	O
training	O
set	O
(	O
szegedy	O
et	O
al	O
.	O
2014b	O
goodfellow	O
2014b	O
et	O
al.	O
,	O
)	O
.	O
,	O
;	O
2014b	O
et	O
al	O
.	O
(	O
goodfellow	O
)	O
showed	O
that	O
one	O
of	O
the	O
primary	O
causes	O
of	O
these	O
adversarial	O
examples	O
is	O
excessive	O
linearity	O
.	O
neural	O
networks	O
are	O
built	O
out	O
of	O
primarily	O
linear	O
building	O
blocks	O
.	O
in	O
some	O
experiments	O
the	O
overall	O
function	O
they	O
implement	O
proves	O
to	O
be	O
highly	O
linear	O
as	O
a	O
result	O
.	O
these	O
linear	O
functions	O
are	O
easy	O
268	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
×	O
+	O
.007	O
=	O
∇	O
sign	O
(	O
xj	O
(	O
θ	O
x	O
,	O
,	O
y	O
)	O
)	O
∇	O
x	O
+	O
xj	O
(	O
θ	O
x	O
,	O
,	O
y	O
)	O
)	O
	O
sign	O
(	O
“	O
nematode	O
”	O
w/	O
8.2	O
%	O
conﬁdence	O
“	O
gibbon	O
”	O
w/	O
99.3	O
%	O
conﬁdence	O
x	O
y	O
=	O
“	O
panda	O
”	O
w/	O
57.7	O
%	O
conﬁdence	O
,	O
figure	O
7.8	O
:	O
a	O
demonstration	O
of	O
adversarial	O
example	O
generation	O
applied	O
to	O
googlenet	O
(	O
szegedy	O
et	O
al	O
.	O
2014a	O
)	O
on	O
imagenet	O
.	O
by	O
adding	O
an	O
imperceptibly	O
small	O
vector	O
whose	O
elements	O
are	O
equal	O
to	O
the	O
sign	O
of	O
the	O
elements	O
of	O
the	O
gradient	O
of	O
the	O
cost	O
function	O
with	O
respect	O
to	O
the	O
input	O
,	O
we	O
can	O
change	O
googlenet	O
’	O
s	O
classiﬁcation	O
of	O
the	O
image	O
.	O
reproduced	O
with	O
permission	O
from	O
goodfellow	O
et	O
al	O
.	O
2014b	O
)	O
.	O
(	O
to	O
optimize	O
.	O
unfortunately	O
,	O
the	O
value	O
of	O
a	O
linear	O
function	O
can	O
change	O
very	O
rapidly	O
||	O
||	O
if	O
it	O
has	O
numerous	O
inputs	O
.	O
if	O
we	O
change	O
each	O
input	O
by	O
	O
,	O
then	O
a	O
linear	O
function	O
w	O
1	O
,	O
which	O
can	O
be	O
a	O
very	O
large	O
with	O
weights	O
w	O
can	O
change	O
by	O
as	O
much	O
as	O
	O
amount	O
if	O
w	O
is	O
high-dimensional	O
.	O
adversarial	O
training	O
discourages	O
this	O
highly	O
sensitive	O
locally	O
linear	O
behavior	O
by	O
encouraging	O
the	O
network	O
to	O
be	O
locally	O
constant	O
in	O
the	O
neighborhood	O
of	O
the	O
training	O
data	O
.	O
this	O
can	O
be	O
seen	O
as	O
a	O
way	O
of	O
explicitly	O
introducing	O
a	O
local	O
constancy	O
prior	O
into	O
supervised	O
neural	O
nets	O
.	O
adversarial	O
training	O
helps	O
to	O
illustrate	O
the	O
power	O
of	O
using	O
a	O
large	O
function	O
family	O
in	O
combination	O
with	O
aggressive	O
regularization	O
.	O
purely	O
linear	O
models	O
,	O
like	O
logistic	O
regression	O
,	O
are	O
not	O
able	O
to	O
resist	O
adversarial	O
examples	O
because	O
they	O
are	O
forced	O
to	O
be	O
linear	O
.	O
neural	O
networks	O
are	O
able	O
to	O
represent	O
functions	O
that	O
can	O
range	O
from	O
nearly	O
linear	O
to	O
nearly	O
locally	O
constant	O
and	O
thus	O
have	O
the	O
ﬂexibility	O
to	O
capture	O
linear	O
trends	O
in	O
the	O
training	O
data	O
while	O
still	O
learning	O
to	O
resist	O
local	O
perturbation	O
.	O
adversarial	O
examples	O
also	O
provide	O
a	O
means	O
of	O
accomplishing	O
semi-supervised	O
learning	O
.	O
at	O
a	O
point	O
x	O
that	O
is	O
not	O
associated	O
with	O
a	O
label	O
in	O
the	O
dataset	O
,	O
the	O
model	B
itself	O
assigns	O
some	O
label	O
ˆy	O
.	O
the	O
model	B
’	O
s	O
label	O
ˆy	O
may	O
not	O
be	O
the	O
true	O
label	O
,	O
but	O
if	O
the	O
model	B
is	O
high	O
quality	O
,	O
then	O
ˆy	O
has	O
a	O
high	O
probability	O
of	O
providing	O
the	O
	O
true	O
label	O
.	O
we	O
can	O
seek	O
an	O
adversarial	O
example	O
x	O
that	O
causes	O
the	O
classiﬁer	O
to	O
	O
output	O
a	O
label	O
y	O
=	O
ˆy	O
.	O
adversarial	O
examples	O
generated	O
using	O
not	O
the	O
true	O
label	O
but	O
a	O
label	O
provided	O
by	O
a	O
trained	O
model	B
are	O
called	O
virtual	O
adversarial	O
examples	O
(	O
miyato	O
)	O
.	O
the	O
classiﬁer	O
may	O
then	O
be	O
trained	O
to	O
assign	O
the	O
same	O
label	O
to	O
x	O
and	O
x	O
.	O
this	O
encourages	O
the	O
classiﬁer	O
to	O
learn	O
a	O
function	O
that	O
is	O
with	O
y	O
et	O
al.	O
,	O
2015	O
	O
	O
	O
269	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
robust	O
to	O
small	O
changes	O
anywhere	O
along	O
the	O
manifold	O
where	O
the	O
unlabeled	O
data	O
lies	O
.	O
the	O
assumption	O
motivating	O
this	O
approach	O
is	O
that	O
diﬀerent	O
classes	O
usually	O
lie	O
on	O
disconnected	O
manifolds	O
,	O
and	O
a	O
small	O
perturbation	O
should	O
not	O
be	O
able	O
to	O
jump	O
from	O
one	O
class	O
manifold	O
to	O
another	O
class	O
manifold	O
.	O
7.14	O
tangent	O
distance	O
,	O
tangent	O
prop	O
,	O
and	O
manifold	O
tangent	O
classiﬁer	O
many	O
machine	O
learning	O
algorithms	O
aim	O
to	O
overcome	O
the	O
curse	O
of	O
dimensionality	O
by	O
assuming	O
that	O
the	O
data	O
lies	O
near	O
a	O
low-dimensional	O
manifold	O
,	O
as	O
described	O
in	O
section	O
.	O
5.11.3	O
,	O
,	O
simard	O
et	O
al	O
.	O
1993	O
1998	O
one	O
of	O
the	O
early	O
attempts	O
to	O
take	O
advantage	O
of	O
the	O
manifold	O
hypothesis	O
is	O
the	O
tangent	O
distance	O
algorithm	O
(	O
)	O
.	O
it	O
is	O
a	O
non-parametric	O
nearest-neighbor	O
algorithm	O
in	O
which	O
the	O
metric	O
used	O
is	O
not	O
the	O
generic	O
euclidean	O
distance	O
but	O
one	O
that	O
is	O
derived	O
from	O
knowledge	O
of	O
the	O
manifolds	O
near	O
which	O
probability	O
concentrates	O
.	O
it	O
is	O
assumed	O
that	O
we	O
are	O
trying	O
to	O
classify	O
examples	O
and	O
that	O
examples	O
on	O
the	O
same	O
manifold	O
share	O
the	O
same	O
category	O
.	O
since	O
the	O
classiﬁer	O
should	O
be	O
invariant	O
to	O
the	O
local	O
factors	O
of	O
variation	O
that	O
correspond	O
to	O
movement	O
on	O
the	O
manifold	O
,	O
it	O
would	O
make	O
sense	O
to	O
use	O
as	O
nearest-neighbor	O
distance	O
between	O
points	O
x1	O
and	O
x2	O
the	O
distance	O
between	O
the	O
manifolds	O
m1	O
and	O
m2	O
to	O
which	O
they	O
respectively	O
belong	O
.	O
although	O
that	O
may	O
be	O
computationally	O
diﬃcult	O
(	O
it	O
would	O
require	O
solving	O
an	O
optimization	O
problem	O
,	O
to	O
ﬁnd	O
the	O
nearest	O
pair	O
of	O
points	O
on	O
m1	O
and	O
m2	O
)	O
,	O
a	O
cheap	O
alternative	O
that	O
makes	O
sense	O
locally	O
is	O
to	O
approximate	O
mi	O
by	O
its	O
tangent	O
plane	O
at	O
xi	O
and	O
measure	O
the	O
distance	O
between	O
the	O
two	O
tangents	O
,	O
or	O
between	O
a	O
tangent	O
plane	O
and	O
a	O
point	O
.	O
that	O
can	O
be	O
achieved	O
by	O
solving	O
a	O
low-dimensional	O
linear	O
system	O
(	O
in	O
the	O
dimension	O
of	O
the	O
manifolds	O
)	O
.	O
of	O
course	O
,	O
this	O
algorithm	O
requires	O
one	O
to	O
specify	O
the	O
tangent	O
vectors	O
.	O
simard	O
et	O
al	O
.	O
1992	O
in	O
a	O
related	O
spirit	O
,	O
the	O
tangent	O
prop	O
algorithm	O
(	O
)	O
7.9	O
trains	O
a	O
neural	O
net	O
classiﬁer	O
with	O
an	O
extra	O
penalty	O
to	O
make	O
each	O
output	O
f	O
(	O
x	O
)	O
of	O
the	O
neural	O
net	O
locally	O
invariant	O
to	O
known	O
factors	O
of	O
variation	O
.	O
these	O
factors	O
of	O
∇	O
variation	O
correspond	O
to	O
movement	O
along	O
the	O
manifold	O
near	O
which	O
examples	O
of	O
the	O
same	O
class	O
concentrate	O
.	O
local	O
invariance	O
is	O
achieved	O
by	O
requiring	O
xf	O
(	O
x	O
)	O
to	O
be	O
orthogonal	O
to	O
the	O
known	O
manifold	O
tangent	O
vectors	O
v	O
(	O
)	O
i	O
at	O
x	O
,	O
or	O
equivalently	O
that	O
the	O
directional	O
derivative	O
of	O
f	O
at	O
x	O
in	O
the	O
directions	O
v	O
(	O
)	O
i	O
be	O
small	O
by	O
adding	O
a	O
regularization	O
penalty	O
	O
	O
,	O
)	O
(	O
ﬁgure	O
	O
:	O
ω	O
ω	O
(	O
)	O
=f	O
i	O
∇	O
(	O
	O
v	O
(	O
)	O
i	O
2	O
.	O
xf	O
(	O
)	O
)	O
x	O
270	O
(	O
7.67	O
)	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
this	O
regularizer	O
can	O
of	O
course	O
be	O
scaled	O
by	O
an	O
appropriate	O
hyperparameter	O
,	O
and	O
,	O
for	O
most	O
neural	O
networks	O
,	O
we	O
would	O
need	O
to	O
sum	O
over	O
many	O
outputs	O
rather	O
than	O
the	O
lone	O
output	O
f	O
(	O
x	O
)	O
described	O
here	O
for	O
simplicity	O
.	O
as	O
with	O
the	O
tangent	O
distance	O
algorithm	O
,	O
the	O
tangent	O
vectors	O
are	O
derived	O
a	O
priori	O
,	O
usually	O
from	O
the	O
formal	O
knowledge	O
of	O
the	O
eﬀect	O
of	O
transformations	O
such	O
as	O
translation	O
,	O
rotation	O
,	O
and	O
scaling	O
in	O
images	O
.	O
tangent	O
prop	O
has	O
been	O
used	O
not	O
just	O
for	O
supervised	O
learning	O
(	O
simard	O
et	O
al	O
.	O
1992	O
)	O
but	O
also	O
in	O
the	O
context	O
of	O
reinforcement	O
learning	O
(	O
thrun	O
1995	O
)	O
.	O
,	O
,	O
tangent	O
propagation	O
is	O
closely	O
related	O
to	O
dataset	O
augmentation	O
.	O
in	O
both	O
cases	O
,	O
the	O
user	O
of	O
the	O
algorithm	O
encodes	O
his	O
or	O
her	O
prior	O
knowledge	O
of	O
the	O
task	O
by	O
specifying	O
a	O
set	O
of	O
transformations	O
that	O
should	O
not	O
alter	O
the	O
output	O
of	O
the	O
network	O
.	O
the	O
diﬀerence	O
is	O
that	O
in	O
the	O
case	O
of	O
dataset	O
augmentation	O
,	O
the	O
network	O
is	O
explicitly	O
trained	O
to	O
correctly	O
classify	O
distinct	O
inputs	O
that	O
were	O
created	O
by	O
applying	O
more	O
than	O
an	O
inﬁnitesimal	O
amount	O
of	O
these	O
transformations	O
.	O
tangent	O
propagation	O
does	O
not	O
require	O
explicitly	O
visiting	O
a	O
new	O
input	O
point	O
.	O
instead	O
,	O
it	O
analytically	O
regularizes	O
the	O
model	B
to	O
resist	O
perturbation	O
in	O
the	O
directions	O
corresponding	O
to	O
the	O
speciﬁed	O
transformation	O
.	O
while	O
this	O
analytical	O
approach	O
is	O
intellectually	O
elegant	O
,	O
it	O
has	O
two	O
major	O
drawbacks	O
.	O
first	O
,	O
it	O
only	O
regularizes	O
the	O
model	B
to	O
resist	O
inﬁnitesimal	O
perturbation	O
.	O
explicit	O
dataset	O
augmentation	O
confers	O
resistance	O
to	O
larger	O
perturbations	O
.	O
second	O
,	O
the	O
inﬁnitesimal	O
approach	O
poses	O
diﬃculties	O
for	O
models	O
based	O
on	O
rectiﬁed	O
linear	O
units	O
.	O
these	O
models	O
can	O
only	O
shrink	O
their	O
derivatives	O
by	O
turning	O
units	O
oﬀ	O
or	O
shrinking	O
their	O
weights	O
.	O
they	O
are	O
not	O
able	O
to	O
shrink	O
their	O
derivatives	O
by	O
saturating	O
at	O
a	O
high	O
value	O
with	O
large	O
weights	O
,	O
as	O
sigmoid	O
or	O
tanh	B
units	O
can	O
.	O
dataset	O
augmentation	O
works	O
well	O
with	O
rectiﬁed	O
linear	O
units	O
because	O
diﬀerent	O
subsets	O
of	O
rectiﬁed	O
units	O
can	O
activate	O
for	O
diﬀerent	O
transformed	O
versions	O
of	O
each	O
original	O
input	O
.	O
;	O
,	O
szegedy	O
et	O
al	O
.	O
2014b	O
goodfellow	O
et	O
al	O
.	O
2014b	O
tangent	O
propagation	O
is	O
also	O
related	O
to	O
double	O
backprop	O
(	O
drucker	O
and	O
lecun	O
,	O
1992	O
)	O
and	O
adversarial	O
training	O
(	O
)	O
.	O
double	O
backprop	O
regularizes	O
the	O
jacobian	O
to	O
be	O
small	O
,	O
while	O
adversarial	O
training	O
ﬁnds	O
inputs	O
near	O
the	O
original	O
inputs	O
and	O
trains	O
the	O
model	B
to	O
produce	O
the	O
same	O
output	O
on	O
these	O
as	O
on	O
the	O
original	O
inputs	O
.	O
tangent	O
propagation	O
and	O
dataset	O
augmentation	O
using	O
manually	O
speciﬁed	O
transformations	O
both	O
require	O
that	O
the	O
model	B
should	O
be	O
invariant	O
to	O
certain	O
speciﬁed	O
directions	O
of	O
change	O
in	O
the	O
input	O
.	O
double	O
backprop	O
and	O
adversarial	O
training	O
both	O
require	O
that	O
the	O
model	B
should	O
be	O
invariant	O
to	O
directions	O
of	O
change	O
in	O
the	O
input	O
so	O
long	O
as	O
the	O
change	O
is	O
small	O
.	O
just	O
as	O
dataset	O
augmentation	O
is	O
the	O
non-inﬁnitesimal	O
version	O
of	O
tangent	O
propagation	O
,	O
adversarial	O
training	O
is	O
the	O
non-inﬁnitesimal	O
version	O
of	O
double	O
backprop	O
.	O
all	O
,	O
the	O
manifold	O
tangent	O
classiﬁer	O
(	O
rifai	O
et	O
al	O
.	O
2011c	O
,	O
know	O
the	O
tangent	O
vectors	O
a	O
priori	O
.	O
as	O
we	O
will	O
see	O
in	O
chapter	O
)	O
,	O
eliminates	O
the	O
need	O
to	O
,	O
autoencoders	O
can	O
14	O
271	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
2	O
x	O
normal	O
tangent	O
x1	O
2011c	O
et	O
al.	O
,	O
)	O
and	O
manifold	O
tangent	O
classiﬁer	O
(	O
figure	O
7.9	O
:	O
illustration	O
of	O
the	O
main	O
idea	O
of	O
the	O
tangent	O
prop	O
algorithm	O
(	O
simard	O
et	O
al	O
.	O
,	O
1992	O
)	O
,	O
which	O
both	O
regularize	O
the	O
classiﬁer	O
output	O
function	O
f	O
(	O
x	O
)	O
.	O
each	O
curve	O
represents	O
the	O
manifold	O
for	O
a	O
diﬀerent	O
class	O
,	O
illustrated	O
here	O
as	O
a	O
one-dimensional	O
manifold	O
embedded	O
in	O
a	O
two-dimensional	O
space	O
.	O
on	O
one	O
curve	O
,	O
we	O
have	O
chosen	O
a	O
single	O
point	O
and	O
drawn	O
a	O
vector	O
that	O
is	O
tangent	O
to	O
the	O
class	O
manifold	O
(	O
parallel	O
to	O
and	O
touching	O
the	O
manifold	O
)	O
and	O
a	O
vector	O
that	O
is	O
normal	O
to	O
the	O
class	O
manifold	O
(	O
orthogonal	O
to	O
the	O
manifold	O
)	O
.	O
in	O
multiple	O
dimensions	O
there	O
may	O
be	O
many	O
tangent	O
directions	O
and	O
many	O
normal	O
directions	O
.	O
we	O
expect	O
the	O
classiﬁcation	O
function	O
to	O
change	O
rapidly	O
as	O
it	O
moves	O
in	O
the	O
direction	O
normal	O
to	O
the	O
manifold	O
,	O
and	O
not	O
to	O
change	O
as	O
it	O
moves	O
along	O
the	O
class	O
manifold	O
.	O
both	O
tangent	O
propagation	O
and	O
the	O
manifold	O
tangent	O
classiﬁer	O
regularize	O
f	O
(	O
x	O
)	O
to	O
not	O
change	O
very	O
much	O
as	O
x	O
moves	O
along	O
the	O
manifold	O
.	O
tangent	O
propagation	O
requires	O
the	O
user	O
to	O
manually	O
specify	O
functions	O
that	O
compute	O
the	O
tangent	O
directions	O
(	O
such	O
as	O
specifying	O
that	O
small	O
translations	O
of	O
images	O
remain	O
in	O
the	O
same	O
class	O
manifold	O
)	O
while	O
the	O
manifold	O
tangent	O
classiﬁer	O
estimates	O
the	O
manifold	O
tangent	O
directions	O
by	O
training	O
an	O
autoencoder	O
to	O
ﬁt	O
the	O
training	O
data	O
.	O
the	O
use	O
of	O
autoencoders	O
to	O
estimate	O
manifolds	O
will	O
be	O
described	O
in	O
chapter	O
rifai	O
.14	O
14.10	O
estimate	O
the	O
manifold	O
tangent	O
vectors	O
.	O
the	O
manifold	O
tangent	O
classiﬁer	O
makes	O
use	O
of	O
this	O
technique	O
to	O
avoid	O
needing	O
user-speciﬁed	O
tangent	O
vectors	O
.	O
as	O
illustrated	O
in	O
ﬁgure	O
,	O
these	O
estimated	O
tangent	O
vectors	O
go	O
beyond	O
the	O
classical	O
invariants	O
that	O
arise	O
out	O
of	O
the	O
geometry	O
of	O
images	O
(	O
such	O
as	O
translation	O
,	O
rotation	O
and	O
scaling	O
)	O
and	O
include	O
factors	O
that	O
must	O
be	O
learned	O
because	O
they	O
are	O
object-speciﬁc	O
(	O
such	O
as	O
moving	O
body	O
parts	O
)	O
.	O
the	O
algorithm	O
proposed	O
with	O
the	O
manifold	O
tangent	O
classiﬁer	O
is	O
therefore	O
simple	O
:	O
(	O
1	O
)	O
use	O
an	O
autoencoder	O
to	O
learn	O
the	O
manifold	O
structure	O
by	O
unsupervised	O
learning	O
,	O
and	O
(	O
2	O
)	O
use	O
these	O
tangents	O
to	O
regularize	O
a	O
neural	O
net	O
classiﬁer	O
as	O
in	O
tangent	O
prop	O
(	O
equation	O
7.67	O
)	O
.	O
this	O
chapter	O
has	O
described	O
most	O
of	O
the	O
general	O
strategies	O
used	O
to	O
regularize	O
neural	O
networks	O
.	O
regularization	O
is	O
a	O
central	O
theme	O
of	O
machine	O
learning	O
and	O
as	O
such	O
272	O
chapter	O
7.	O
regularization	O
for	O
deep	O
learning	O
will	O
be	O
revisited	O
periodically	O
by	O
most	O
of	O
the	O
remaining	O
chapters	O
.	O
another	O
central	O
theme	O
of	O
machine	O
learning	O
is	O
optimization	O
,	O
described	O
next	O
.	O
273	O
chapter	O
8	O
optimization	O
for	O
training	O
deep	O
models	O
deep	O
learning	O
algorithms	O
involve	O
optimization	O
in	O
many	O
contexts	O
.	O
for	O
example	O
,	O
performing	O
inference	O
in	O
models	O
such	O
as	O
pca	O
involves	O
solving	O
an	O
optimization	O
problem	O
.	O
we	O
often	O
use	O
analytical	O
optimization	O
to	O
write	O
proofs	O
or	O
design	O
algorithms	O
.	O
of	O
all	O
of	O
the	O
many	O
optimization	O
problems	O
involved	O
in	O
deep	O
learning	O
,	O
the	O
most	O
diﬃcult	O
is	O
neural	O
network	O
training	O
.	O
it	O
is	O
quite	O
common	O
to	O
invest	O
days	O
to	O
months	O
of	O
time	O
on	O
hundreds	O
of	O
machines	O
in	O
order	O
to	O
solve	O
even	O
a	O
single	O
instance	O
of	O
the	O
neural	O
network	O
training	O
problem	O
.	O
because	O
this	O
problem	O
is	O
so	O
important	O
and	O
so	O
expensive	O
,	O
a	O
specialized	O
set	O
of	O
optimization	O
techniques	O
have	O
been	O
developed	O
for	O
solving	O
it	O
.	O
this	O
chapter	O
presents	O
these	O
optimization	O
techniques	O
for	O
neural	O
network	O
training	O
.	O
if	O
you	O
are	O
unfamiliar	O
with	O
the	O
basic	O
principles	O
of	O
gradient-based	O
optimization	O
,	O
we	O
suggest	O
reviewing	O
chapter	O
.	O
that	O
chapter	O
includes	O
a	O
brief	O
overview	O
of	O
numerical	O
optimization	O
in	O
general	O
.	O
4	O
this	O
chapter	O
focuses	O
on	O
one	O
particular	O
case	O
of	O
optimization	O
:	O
ﬁnding	O
the	O
param-	O
eters	O
θ	O
of	O
a	O
neural	O
network	O
that	O
signiﬁcantly	O
reduce	O
a	O
cost	O
function	O
j	O
(	O
θ	O
)	O
,	O
which	O
typically	O
includes	O
a	O
performance	O
measure	O
evaluated	O
on	O
the	O
entire	O
training	O
set	O
as	O
well	O
as	O
additional	O
regularization	O
terms	O
.	O
we	O
begin	O
with	O
a	O
description	O
of	O
how	O
optimization	O
used	O
as	O
a	O
training	O
algorithm	O
for	O
a	O
machine	O
learning	O
task	O
diﬀers	O
from	O
pure	O
optimization	O
.	O
next	O
,	O
we	O
present	O
several	O
of	O
the	O
concrete	O
challenges	O
that	O
make	O
optimization	O
of	O
neural	O
networks	O
diﬃcult	O
.	O
we	O
then	O
deﬁne	O
several	O
practical	O
algorithms	O
,	O
including	O
both	O
optimization	O
algorithms	O
themselves	O
and	O
strategies	O
for	O
initializing	O
the	O
parameters	O
.	O
more	O
advanced	O
algorithms	O
adapt	O
their	O
learning	O
rates	O
during	O
training	O
or	O
leverage	O
information	O
contained	O
in	O
274	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
the	O
second	O
derivatives	O
of	O
the	O
cost	O
function	O
.	O
finally	O
,	O
we	O
conclude	O
with	O
a	O
review	O
of	O
several	O
optimization	O
strategies	O
that	O
are	O
formed	O
by	O
combining	O
simple	O
optimization	O
algorithms	O
into	O
higher-level	O
procedures	O
.	O
8.1	O
how	O
learning	O
diﬀers	O
from	O
pure	O
optimization	O
optimization	O
algorithms	O
used	O
for	O
training	O
of	O
deep	O
models	O
diﬀer	O
from	O
traditional	O
optimization	O
algorithms	O
in	O
several	O
ways	O
.	O
machine	O
learning	O
usually	O
acts	O
indirectly	O
.	O
in	O
most	O
machine	O
learning	O
scenarios	O
,	O
we	O
care	O
about	O
some	O
performance	O
measure	O
p	O
,	O
that	O
is	O
deﬁned	O
with	O
respect	O
to	O
the	O
test	O
set	O
and	O
may	O
also	O
be	O
intractable	O
.	O
we	O
therefore	O
optimize	O
p	O
only	O
indirectly	O
.	O
we	O
reduce	O
a	O
diﬀerent	O
cost	O
function	O
j	O
(	O
θ	O
)	O
in	O
the	O
hope	O
that	O
doing	O
so	O
will	O
improve	O
p	O
.	O
this	O
is	O
in	O
contrast	O
to	O
pure	O
optimization	O
,	O
where	O
minimizing	O
j	O
is	O
a	O
goal	O
in	O
and	O
of	O
itself	O
.	O
optimization	O
algorithms	O
for	O
training	O
deep	O
models	O
also	O
typically	O
include	O
some	O
specialization	O
on	O
the	O
speciﬁc	O
structure	O
of	O
machine	O
learning	O
objective	O
functions	O
.	O
typically	O
,	O
the	O
cost	O
function	O
can	O
be	O
written	O
as	O
an	O
average	O
over	O
the	O
training	O
set	O
,	O
such	O
as	O
j	O
(	O
)	O
=	O
θ	O
e	O
(	O
x	O
,	O
y	O
∼	O
pdata	O
)	O
ˆ	O
l	O
f	O
(	O
(	O
;	O
)	O
x	O
θ	O
,	O
y	O
,	O
)	O
(	O
8.1	O
)	O
where	O
l	O
is	O
the	O
per-example	O
loss	O
function	O
,	O
f	O
(	O
x	O
;	O
θ	O
)	O
is	O
the	O
predicted	O
output	O
when	O
the	O
input	O
is	O
x	O
,	O
ˆpdata	O
is	O
the	O
empirical	O
distribution	O
.	O
in	O
the	O
supervised	O
learning	O
case	O
,	O
y	O
is	O
the	O
target	O
output	O
.	O
throughout	O
this	O
chapter	O
,	O
we	O
develop	O
the	O
unregularized	O
supervised	O
case	O
,	O
where	O
the	O
arguments	O
to	O
l	O
are	O
f	O
(	O
x	O
;	O
θ	O
)	O
and	O
y.	O
however	O
,	O
it	O
is	O
trivial	O
to	O
extend	O
this	O
development	O
,	O
for	O
example	O
,	O
to	O
include	O
θ	O
or	O
x	O
as	O
arguments	O
,	O
or	O
to	O
exclude	O
y	O
as	O
arguments	O
,	O
in	O
order	O
to	O
develop	O
various	O
forms	O
of	O
regularization	O
or	O
unsupervised	O
learning	O
.	O
8.1	O
equation	O
deﬁnes	O
an	O
objective	O
function	O
with	O
respect	O
to	O
the	O
training	O
set	O
.	O
we	O
would	O
usually	O
prefer	O
to	O
minimize	O
the	O
corresponding	O
objective	O
function	O
where	O
the	O
expectation	O
is	O
taken	O
across	O
the	O
data	O
generating	O
distribution	O
pdata	O
rather	O
than	O
just	O
over	O
the	O
ﬁnite	O
training	O
set	O
:	O
∗	O
j	O
(	O
)	O
=	O
θ	O
∼	O
)	O
x	O
,	O
y	O
pdata	O
e	O
(	O
l	O
f	O
(	O
(	O
;	O
)	O
x	O
θ	O
,	O
y	O
.	O
)	O
(	O
8.2	O
)	O
8.1.1	O
empirical	O
risk	O
minimization	O
the	O
goal	O
of	O
a	O
machine	O
learning	O
algorithm	O
is	O
to	O
reduce	O
the	O
expected	O
generalization	O
error	O
given	O
by	O
equation	O
risk	O
.	O
we	O
emphasize	O
here	O
that	O
the	O
expectation	O
is	O
taken	O
over	O
the	O
true	O
underlying	O
distribution	O
pdata	O
.	O
if	O
we	O
knew	O
the	O
true	O
distribution	O
pdata	O
(	O
x	O
,	O
y	O
)	O
,	O
risk	O
minimization	O
would	O
be	O
an	O
optimization	O
task	O
.	O
this	O
quantity	O
is	O
known	O
as	O
the	O
8.2	O
275	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
solvable	O
by	O
an	O
optimization	O
algorithm	O
.	O
however	O
,	O
when	O
we	O
do	O
not	O
know	O
pdata	O
(	O
x	O
,	O
y	O
)	O
but	O
only	O
have	O
a	O
training	O
set	O
of	O
samples	O
,	O
we	O
have	O
a	O
machine	O
learning	O
problem	O
.	O
the	O
simplest	O
way	O
to	O
convert	O
a	O
machine	O
learning	O
problem	O
back	O
into	O
an	O
op-	O
timization	O
problem	O
is	O
to	O
minimize	O
the	O
expected	O
loss	O
on	O
the	O
training	O
set	O
.	O
this	O
means	O
replacing	O
the	O
true	O
distribution	O
p	O
(	O
x	O
,	O
y	O
)	O
with	O
the	O
empirical	O
distribution	O
ˆp	O
(	O
x	O
,	O
y	O
)	O
deﬁned	O
by	O
the	O
training	O
set	O
.	O
we	O
now	O
minimize	O
the	O
empirical	O
risk	O
	O
∼	O
ex	O
,	O
y	O
ˆpdata	O
(	O
)	O
x	O
,	O
y	O
[	O
(	O
(	O
;	O
)	O
l	O
f	O
x	O
θ	O
,	O
y	O
)	O
]	O
=	O
1	O
m	O
m	O
i=1	O
l	O
f	O
(	O
(	O
x	O
(	O
)	O
i	O
;	O
)	O
θ	O
,	O
y	O
(	O
)	O
i	O
)	O
(	O
8.3	O
)	O
where	O
m	O
is	O
the	O
number	O
of	O
training	O
examples	O
.	O
the	O
training	O
process	O
based	O
on	O
minimizing	O
this	O
average	O
training	O
error	O
is	O
known	O
as	O
empirical	O
risk	O
minimization	O
.	O
in	O
this	O
setting	O
,	O
machine	O
learning	O
is	O
still	O
very	O
similar	O
to	O
straightforward	O
optimization	O
.	O
rather	O
than	O
optimizing	O
the	O
risk	O
directly	O
,	O
we	O
optimize	O
the	O
empirical	O
risk	O
,	O
and	O
hope	O
that	O
the	O
risk	O
decreases	O
signiﬁcantly	O
as	O
well	O
.	O
a	O
variety	O
of	O
theoretical	O
results	O
establish	O
conditions	B
under	O
which	O
the	O
true	O
risk	O
can	O
be	O
expected	O
to	O
decrease	O
by	O
various	O
amounts	O
.	O
however	O
,	O
empirical	O
risk	O
minimization	O
is	O
prone	O
to	O
overﬁtting	O
.	O
models	O
with	O
high	O
capacity	O
can	O
simply	O
memorize	O
the	O
training	O
set	O
.	O
in	O
many	O
cases	O
,	O
empirical	O
risk	O
minimization	O
is	O
not	O
really	O
feasible	O
.	O
the	O
most	O
eﬀective	O
modern	O
optimization	O
algorithms	O
are	O
based	O
on	O
gradient	O
descent	B
,	O
but	O
many	O
useful	O
loss	O
functions	O
,	O
such	O
as	O
0-1	B
loss	I
,	O
have	O
no	O
useful	O
derivatives	O
(	O
the	O
derivative	O
is	O
either	O
zero	O
or	O
undeﬁned	O
everywhere	O
)	O
.	O
these	O
two	O
problems	O
mean	O
that	O
,	O
in	O
the	O
context	O
of	O
deep	O
learning	O
,	O
we	O
rarely	O
use	O
empirical	O
risk	O
minimization	O
.	O
instead	O
,	O
we	O
must	O
use	O
a	O
slightly	O
diﬀerent	O
approach	O
,	O
in	O
which	O
the	O
quantity	O
that	O
we	O
actually	O
optimize	O
is	O
even	O
more	O
diﬀerent	O
from	O
the	O
quantity	O
that	O
we	O
truly	O
want	O
to	O
optimize	O
.	O
8.1.2	O
surrogate	O
loss	O
functions	O
and	O
early	O
stopping	O
sometimes	O
,	O
the	O
loss	O
function	O
we	O
actually	O
care	O
about	O
(	O
say	O
classiﬁcation	O
error	O
)	O
is	O
not	O
one	O
that	O
can	O
be	O
optimized	O
eﬃciently	O
.	O
for	O
example	O
,	O
exactly	O
minimizing	O
expected	O
0-1	B
loss	I
is	O
typically	O
intractable	O
(	O
exponential	O
in	O
the	O
input	O
dimension	O
)	O
,	O
even	O
for	O
a	O
linear	O
classiﬁer	O
(	O
marcotte	O
and	O
savard	O
1992	O
)	O
.	O
in	O
such	O
situations	O
,	O
one	O
typically	O
optimizes	O
a	O
surrogate	O
loss	O
function	O
instead	O
,	O
which	O
acts	O
as	O
a	O
proxy	O
but	O
has	O
advantages	O
.	O
for	O
example	O
,	O
the	O
negative	O
log-likelihood	O
of	O
the	O
correct	O
class	O
is	O
typically	O
used	O
as	O
a	O
surrogate	O
for	O
the	O
0-1	B
loss	I
.	O
the	O
negative	O
log-likelihood	O
allows	O
the	O
model	B
to	O
estimate	O
the	O
conditional	O
probability	O
of	O
the	O
classes	O
,	O
given	O
the	O
input	O
,	O
and	O
if	O
the	O
model	B
can	O
do	O
that	O
well	O
,	O
then	O
it	O
can	O
pick	O
the	O
classes	O
that	O
yield	O
the	O
least	O
classiﬁcation	O
error	O
in	O
expectation	O
.	O
,	O
276	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
in	O
some	O
cases	O
,	O
a	O
surrogate	O
loss	O
function	O
actually	O
results	O
in	O
being	O
able	O
to	O
learn	O
more	O
.	O
for	O
example	O
,	O
the	O
test	O
set	O
0-1	B
loss	I
often	O
continues	O
to	O
decrease	O
for	O
a	O
long	O
time	O
after	O
the	O
training	O
set	O
0-1	B
loss	I
has	O
reached	O
zero	O
,	O
when	O
training	O
using	O
the	O
log-likelihood	O
surrogate	O
.	O
this	O
is	O
because	O
even	O
when	O
the	O
expected	O
0-1	B
loss	I
is	O
zero	O
,	O
one	O
can	O
improve	O
the	O
robustness	O
of	O
the	O
classiﬁer	O
by	O
further	O
pushing	O
the	O
classes	O
apart	O
from	O
each	O
other	O
,	O
obtaining	O
a	O
more	O
conﬁdent	O
and	O
reliable	O
classiﬁer	O
,	O
thus	O
extracting	O
more	O
information	O
from	O
the	O
training	O
data	O
than	O
would	O
have	O
been	O
possible	O
by	O
simply	O
minimizing	O
the	O
average	O
0-1	B
loss	I
on	O
the	O
training	O
set	O
.	O
a	O
very	O
important	O
diﬀerence	O
between	O
optimization	O
in	O
general	O
and	O
optimization	O
as	O
we	O
use	O
it	O
for	O
training	O
algorithms	O
is	O
that	O
training	O
algorithms	O
do	O
not	O
usually	O
halt	O
at	O
a	O
local	O
minimum	O
.	O
instead	O
,	O
a	O
machine	O
learning	O
algorithm	O
usually	O
minimizes	O
a	O
surrogate	O
loss	O
function	O
but	O
halts	O
when	O
a	O
convergence	O
criterion	O
based	O
on	O
early	O
stopping	O
(	O
section	O
)	O
is	O
satisﬁed	O
.	O
typically	O
the	O
early	O
stopping	O
criterion	O
is	O
based	O
on	O
the	O
true	O
underlying	O
loss	O
function	O
,	O
such	O
as	O
0-1	B
loss	I
measured	O
on	O
a	O
validation	O
set	O
,	O
and	O
is	O
designed	O
to	O
cause	O
the	O
algorithm	O
to	O
halt	O
whenever	O
overﬁtting	O
begins	O
to	O
occur	O
.	O
training	O
often	O
halts	O
while	O
the	O
surrogate	O
loss	O
function	O
still	O
has	O
large	O
derivatives	O
,	O
which	O
is	O
very	O
diﬀerent	O
from	O
the	O
pure	O
optimization	O
setting	O
,	O
where	O
an	O
optimization	O
algorithm	O
is	O
considered	O
to	O
have	O
converged	O
when	O
the	O
gradient	O
becomes	O
very	O
small	O
.	O
7.8	O
8.1.3	O
batch	O
and	O
minibatch	O
algorithms	O
one	O
aspect	O
of	O
machine	O
learning	O
algorithms	O
that	O
separates	O
them	O
from	O
general	O
optimization	O
algorithms	O
is	O
that	O
the	O
objective	O
function	O
usually	O
decomposes	O
as	O
a	O
sum	O
over	O
the	O
training	O
examples	O
.	O
optimization	O
algorithms	O
for	O
machine	O
learning	O
typically	O
compute	O
each	O
update	O
to	O
the	O
parameters	O
based	O
on	O
an	O
expected	O
value	O
of	O
the	O
cost	O
function	O
estimated	O
using	O
only	O
a	O
subset	O
of	O
the	O
terms	O
of	O
the	O
full	O
cost	O
function	O
.	O
for	O
example	O
,	O
maximum	O
likelihood	O
estimation	O
problems	O
,	O
when	O
viewed	O
in	O
log	O
	O
space	O
,	O
decompose	O
into	O
a	O
sum	O
over	O
each	O
example	O
:	O
θml	O
=	O
arg	O
max	O
log	O
pmodel	O
(	O
x	O
(	O
)	O
i	O
,	O
y	O
(	O
)	O
i	O
;	O
)	O
θ	O
.	O
(	O
8.4	O
)	O
m	O
θ	O
i=1	O
maximizing	O
this	O
sum	O
is	O
equivalent	O
to	O
maximizing	O
the	O
expectation	O
over	O
the	O
empirical	O
distribution	O
deﬁned	O
by	O
the	O
training	O
set	O
:	O
j	O
(	O
)	O
=	O
θ	O
∼	O
ex	O
,	O
y	O
ˆpdata	O
log	O
pmodel	O
(	O
;	O
)	O
x	O
,	O
y	O
θ	O
.	O
(	O
8.5	O
)	O
most	O
of	O
the	O
properties	O
of	O
the	O
objective	O
function	O
j	O
used	O
by	O
most	O
of	O
our	O
opti-	O
mization	O
algorithms	O
are	O
also	O
expectations	O
over	O
the	O
training	O
set	O
.	O
for	O
example	O
,	O
the	O
277	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
most	O
commonly	O
used	O
property	O
is	O
the	O
gradient	O
:	O
∇	O
θj	O
(	O
)	O
=	O
θ	O
∼	O
ex	O
,	O
y	O
ˆpdata	O
∇	O
θ	O
log	O
pmodel	O
(	O
;	O
)	O
x	O
,	O
y	O
θ	O
.	O
(	O
8.6	O
)	O
computing	O
this	O
expectation	O
exactly	O
is	O
very	O
expensive	O
because	O
it	O
requires	O
evaluating	O
the	O
model	B
on	O
every	O
example	O
in	O
the	O
entire	O
dataset	O
.	O
in	O
practice	O
,	O
we	O
can	O
compute	O
these	O
expectations	O
by	O
randomly	O
sampling	O
a	O
small	O
number	O
of	O
examples	O
from	O
the	O
dataset	O
,	O
then	O
taking	O
the	O
average	O
over	O
only	O
those	O
examples	O
.	O
recall	O
that	O
the	O
standard	O
error	O
of	O
the	O
mean	O
(	O
equation	O
√	O
n	O
√	O
n	O
,	O
where	O
σ	O
is	O
the	O
true	O
standard	O
deviation	O
of	O
the	O
value	O
of	O
samples	O
is	O
given	O
by	O
σ/	O
n	O
shows	O
that	O
there	O
are	O
less	O
than	O
linear	O
returns	O
the	O
samples	O
.	O
the	O
denominator	O
of	O
to	O
using	O
more	O
examples	O
to	O
estimate	O
the	O
gradient	O
.	O
compare	O
two	O
hypothetical	O
estimates	O
of	O
the	O
gradient	O
,	O
one	O
based	O
on	O
100	O
examples	O
and	O
another	O
based	O
on	O
10,000	O
examples	O
.	O
the	O
latter	O
requires	O
100	O
times	O
more	O
computation	O
than	O
the	O
former	O
,	O
but	O
reduces	O
the	O
standard	O
error	O
of	O
the	O
mean	O
only	O
by	O
a	O
factor	O
of	O
10.	O
most	O
optimization	O
algorithms	O
converge	O
much	O
faster	O
(	O
in	O
terms	O
of	O
total	O
computation	O
,	O
not	O
in	O
terms	O
of	O
number	O
of	O
updates	O
)	O
if	O
they	O
are	O
allowed	O
to	O
rapidly	O
compute	O
approximate	O
estimates	O
of	O
the	O
gradient	O
rather	O
than	O
slowly	O
computing	O
the	O
exact	O
gradient	O
.	O
5.46	O
)	O
estimated	O
from	O
another	O
consideration	O
motivating	O
statistical	O
estimation	O
of	O
the	O
gradient	O
from	O
a	O
small	O
number	O
of	O
samples	O
is	O
redundancy	O
in	O
the	O
training	O
set	O
.	O
in	O
the	O
worst	O
case	O
,	O
all	O
m	O
samples	O
in	O
the	O
training	O
set	O
could	O
be	O
identical	O
copies	O
of	O
each	O
other	O
.	O
a	O
sampling-	O
based	O
estimate	O
of	O
the	O
gradient	O
could	O
compute	O
the	O
correct	O
gradient	O
with	O
a	O
single	O
sample	O
,	O
using	O
m	O
times	O
less	O
computation	O
than	O
the	O
naive	O
approach	O
.	O
in	O
practice	O
,	O
we	O
are	O
unlikely	O
to	O
truly	O
encounter	O
this	O
worst-case	O
situation	O
,	O
but	O
we	O
may	O
ﬁnd	O
large	O
numbers	O
of	O
examples	O
that	O
all	O
make	O
very	O
similar	O
contributions	O
to	O
the	O
gradient	O
.	O
optimization	O
algorithms	O
that	O
use	O
the	O
entire	O
training	O
set	O
are	O
called	O
batch	O
or	O
deterministic	O
gradient	O
methods	O
,	O
because	O
they	O
process	O
all	O
of	O
the	O
training	O
examples	O
simultaneously	O
in	O
a	O
large	O
batch	O
.	O
this	O
terminology	O
can	O
be	O
somewhat	O
confusing	O
because	O
the	O
word	O
“	O
batch	O
”	O
is	O
also	O
often	O
used	O
to	O
describe	O
the	O
minibatch	O
used	O
by	O
minibatch	O
stochastic	O
gradient	O
descent	B
.	O
typically	O
the	O
term	O
“	O
batch	O
gradient	O
descent	B
”	O
implies	O
the	O
use	O
of	O
the	O
full	O
training	O
set	O
,	O
while	O
the	O
use	O
of	O
the	O
term	O
“	O
batch	O
”	O
to	O
describe	O
a	O
group	O
of	O
examples	O
does	O
not	O
.	O
for	O
example	O
,	O
it	O
is	O
very	O
common	O
to	O
use	O
the	O
term	O
“	O
batch	O
size	O
”	O
to	O
describe	O
the	O
size	O
of	O
a	O
minibatch	O
.	O
optimization	O
algorithms	O
that	O
use	O
only	O
a	O
single	O
example	O
at	O
a	O
time	O
are	O
sometimes	O
called	O
stochastic	O
or	O
sometimes	O
online	O
methods	O
.	O
the	O
term	O
online	O
is	O
usually	O
reserved	O
for	O
the	O
case	O
where	O
the	O
examples	O
are	O
drawn	O
from	O
a	O
stream	O
of	O
continually	O
created	O
examples	O
rather	O
than	O
from	O
a	O
ﬁxed-size	O
training	O
set	O
over	O
which	O
several	O
passes	O
are	O
made	O
.	O
most	O
algorithms	O
used	O
for	O
deep	O
learning	O
fall	O
somewhere	O
in	O
between	O
,	O
using	O
more	O
278	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
than	O
one	O
but	O
less	O
than	O
all	O
of	O
the	O
training	O
examples	O
.	O
these	O
were	O
traditionally	O
called	O
minibatch	O
or	O
minibatch	O
stochastic	O
methods	O
and	O
it	O
is	O
now	O
common	O
to	O
simply	O
call	O
them	O
stochastic	O
methods	O
.	O
the	O
canonical	O
example	O
of	O
a	O
stochastic	O
method	O
is	O
stochastic	O
gradient	O
descent	B
,	O
presented	O
in	O
detail	O
in	O
section	O
8.3.1	O
.	O
minibatch	O
sizes	O
are	O
generally	O
driven	O
by	O
the	O
following	O
factors	O
:	O
•	O
larger	O
batches	O
provide	O
a	O
more	O
accurate	O
estimate	O
of	O
the	O
gradient	O
,	O
but	O
with	O
less	O
than	O
linear	O
returns	O
.	O
•	O
•	O
•	O
•	O
multicore	O
architectures	O
are	O
usually	O
underutilized	O
by	O
extremely	O
small	O
batches	O
.	O
this	O
motivates	O
using	O
some	O
absolute	O
minimum	O
batch	O
size	O
,	O
below	O
which	O
there	O
is	O
no	O
reduction	O
in	O
the	O
time	O
to	O
process	O
a	O
minibatch	O
.	O
if	O
all	O
examples	O
in	O
the	O
batch	O
are	O
to	O
be	O
processed	O
in	O
parallel	O
(	O
as	O
is	O
typically	O
the	O
case	O
)	O
,	O
then	O
the	O
amount	O
of	O
memory	O
scales	O
with	O
the	O
batch	O
size	O
.	O
for	O
many	O
hardware	O
setups	O
this	O
is	O
the	O
limiting	O
factor	O
in	O
batch	O
size	O
.	O
some	O
kinds	O
of	O
hardware	O
achieve	O
better	O
runtime	O
with	O
speciﬁc	O
sizes	O
of	O
arrays	O
.	O
especially	O
when	O
using	O
gpus	O
,	O
it	O
is	O
common	O
for	O
power	O
of	O
2	O
batch	O
sizes	O
to	O
oﬀer	O
better	O
runtime	O
.	O
typical	O
power	O
of	O
2	O
batch	O
sizes	O
range	O
from	O
32	O
to	O
256	O
,	O
with	O
16	O
sometimes	O
being	O
attempted	O
for	O
large	O
models	O
.	O
wilson	O
and	O
martinez	O
2003	O
)	O
,	O
small	O
batches	O
can	O
oﬀer	O
a	O
regularizing	O
eﬀect	O
(	O
perhaps	O
due	O
to	O
the	O
noise	O
they	O
add	O
to	O
the	O
learning	O
process	O
.	O
generalization	O
error	O
is	O
often	O
best	O
for	O
a	O
batch	O
size	O
of	O
1.	O
training	O
with	O
such	O
a	O
small	O
batch	O
size	O
might	O
require	O
a	O
small	O
learning	O
rate	O
to	O
maintain	O
stability	O
due	O
to	O
the	O
high	O
variance	O
in	O
the	O
estimate	O
of	O
the	O
gradient	O
.	O
the	O
total	O
runtime	O
can	O
be	O
very	O
high	O
due	O
to	O
the	O
need	O
to	O
make	O
more	O
steps	O
,	O
both	O
because	O
of	O
the	O
reduced	O
learning	O
rate	O
and	O
because	O
it	O
takes	O
more	O
steps	O
to	O
observe	O
the	O
entire	O
training	O
set	O
.	O
,	O
diﬀerent	O
kinds	O
of	O
algorithms	O
use	O
diﬀerent	O
kinds	O
of	O
information	O
from	O
the	O
mini-	O
batch	O
in	O
diﬀerent	O
ways	O
.	O
some	O
algorithms	O
are	O
more	O
sensitive	O
to	O
sampling	O
error	O
than	O
others	O
,	O
either	O
because	O
they	O
use	O
information	O
that	O
is	O
diﬃcult	O
to	O
estimate	O
accurately	O
with	O
few	O
samples	O
,	O
or	O
because	O
they	O
use	O
information	O
in	O
ways	O
that	O
amplify	O
sampling	O
errors	O
more	O
.	O
methods	O
that	O
compute	O
updates	O
based	O
only	O
on	O
the	O
gradient	O
g	O
are	O
usually	O
relatively	O
robust	O
and	O
can	O
handle	O
smaller	O
batch	O
sizes	O
like	O
100.	O
second-order	O
methods	O
,	O
which	O
use	O
also	O
the	O
hessian	O
matrix	O
h	O
and	O
compute	O
updates	O
such	O
as	O
−	O
1g	O
,	O
typically	O
require	O
much	O
larger	O
batch	O
sizes	O
like	O
10,000.	O
these	O
large	O
batch	O
h	O
−	O
1g	O
.	O
suppose	O
sizes	O
are	O
required	O
to	O
minimize	O
ﬂuctuations	O
in	O
the	O
estimates	O
of	O
h	O
that	O
h	O
is	O
estimated	O
perfectly	O
but	O
has	O
a	O
poor	O
condition	O
number	O
.	O
multiplication	O
by	O
279	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
h	O
or	O
its	O
inverse	O
ampliﬁes	O
pre-existing	O
errors	O
,	O
in	O
this	O
case	O
,	O
estimation	O
errors	O
in	O
g.	O
very	O
small	O
changes	O
in	O
the	O
estimate	O
of	O
g	O
can	O
thus	O
cause	O
large	O
changes	O
in	O
the	O
update	O
−	O
1g	O
,	O
even	O
if	O
h	O
were	O
estimated	O
perfectly	O
.	O
of	O
course	O
,	O
h	O
will	O
be	O
estimated	O
only	O
h	O
−	O
1g	O
will	O
contain	O
even	O
more	O
error	O
than	O
we	O
would	O
approximately	O
,	O
so	O
the	O
update	O
h	O
predict	O
from	O
applying	O
a	O
poorly	O
conditioned	O
operation	O
to	O
the	O
estimate	O
of	O
.g	O
it	O
is	O
also	O
crucial	O
that	O
the	O
minibatches	O
be	O
selected	O
randomly	O
.	O
computing	O
an	O
unbiased	O
estimate	O
of	O
the	O
expected	O
gradient	O
from	O
a	O
set	O
of	O
samples	O
requires	O
that	O
those	O
samples	O
be	O
independent	O
.	O
we	O
also	O
wish	O
for	O
two	O
subsequent	O
gradient	O
estimates	O
to	O
be	O
independent	O
from	O
each	O
other	O
,	O
so	O
two	O
subsequent	O
minibatches	O
of	O
examples	O
should	O
also	O
be	O
independent	O
from	O
each	O
other	O
.	O
many	O
datasets	O
are	O
most	O
naturally	O
arranged	O
in	O
a	O
way	O
where	O
successive	O
examples	O
are	O
highly	O
correlated	O
.	O
for	O
example	O
,	O
we	O
might	O
have	O
a	O
dataset	O
of	O
medical	O
data	O
with	O
a	O
long	O
list	O
of	O
blood	O
sample	O
test	O
results	O
.	O
this	O
list	O
might	O
be	O
arranged	O
so	O
that	O
ﬁrst	O
we	O
have	O
ﬁve	O
blood	O
samples	O
taken	O
at	O
diﬀerent	O
times	O
from	O
the	O
ﬁrst	O
patient	O
,	O
then	O
we	O
have	O
three	O
blood	O
samples	O
taken	O
from	O
the	O
second	O
patient	O
,	O
then	O
the	O
blood	O
samples	O
from	O
the	O
third	O
patient	O
,	O
and	O
so	O
on	O
.	O
if	O
we	O
were	O
to	O
draw	O
examples	O
in	O
order	O
from	O
this	O
list	O
,	O
then	O
each	O
of	O
our	O
minibatches	O
would	O
be	O
extremely	O
biased	O
,	O
because	O
it	O
would	O
represent	O
primarily	O
one	O
patient	O
out	O
of	O
the	O
many	O
patients	O
in	O
the	O
dataset	O
.	O
in	O
cases	O
such	O
as	O
these	O
where	O
the	O
order	O
of	O
the	O
dataset	O
holds	O
some	O
signiﬁcance	O
,	O
it	O
is	O
necessary	O
to	O
shuﬄe	O
the	O
examples	O
before	O
selecting	O
minibatches	O
.	O
for	O
very	O
large	O
datasets	O
,	O
for	O
example	O
datasets	O
containing	O
billions	O
of	O
examples	O
in	O
a	O
data	O
center	O
,	O
it	O
can	O
be	O
impractical	O
to	O
sample	O
examples	O
truly	O
uniformly	O
at	O
random	O
every	O
time	O
we	O
want	O
to	O
construct	O
a	O
minibatch	O
.	O
fortunately	O
,	O
in	O
practice	O
it	O
is	O
usually	O
suﬃcient	O
to	O
shuﬄe	O
the	O
order	O
of	O
the	O
dataset	O
once	O
and	O
then	O
store	O
it	O
in	O
shuﬄed	O
fashion	O
.	O
this	O
will	O
impose	O
a	O
ﬁxed	O
set	O
of	O
possible	O
minibatches	O
of	O
consecutive	O
examples	O
that	O
all	O
models	O
trained	O
thereafter	O
will	O
use	O
,	O
and	O
each	O
individual	O
model	B
will	O
be	O
forced	O
to	O
reuse	O
this	O
ordering	O
every	O
time	O
it	O
passes	O
through	O
the	O
training	O
data	O
.	O
however	O
,	O
this	O
deviation	O
from	O
true	O
random	O
selection	O
does	O
not	O
seem	O
to	O
have	O
a	O
signiﬁcant	O
detrimental	O
eﬀect	O
.	O
failing	O
to	O
ever	O
shuﬄe	O
the	O
examples	O
in	O
any	O
way	O
can	O
seriously	O
reduce	O
the	O
eﬀectiveness	O
of	O
the	O
algorithm	O
.	O
many	O
optimization	O
problems	O
in	O
machine	O
learning	O
decompose	O
over	O
examples	O
well	O
enough	O
that	O
we	O
can	O
compute	O
entire	O
separate	O
updates	O
over	O
diﬀerent	O
examples	O
in	O
parallel	O
.	O
in	O
other	O
words	O
,	O
we	O
can	O
compute	O
the	O
update	O
that	O
minimizes	O
j	O
(	O
x	O
)	O
for	O
one	O
minibatch	O
of	O
examples	O
x	O
at	O
the	O
same	O
time	O
that	O
we	O
compute	O
the	O
update	O
for	O
several	O
other	O
minibatches	O
.	O
such	O
asynchronous	O
parallel	O
distributed	O
approaches	O
are	O
discussed	O
further	O
in	O
section	O
12.1.3	O
.	O
an	O
interesting	O
motivation	O
for	O
minibatch	O
stochastic	O
gradient	O
descent	B
is	O
that	O
it	O
follows	O
the	O
gradient	O
of	O
the	O
true	O
generalization	O
error	O
(	O
equation	O
)	O
so	O
long	O
as	O
no	O
examples	O
are	O
repeated	O
.	O
most	O
implementations	O
of	O
minibatch	O
stochastic	O
gradient	O
8.2	O
280	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
descent	B
shuﬄe	O
the	O
dataset	O
once	O
and	O
then	O
pass	O
through	O
it	O
multiple	O
times	O
.	O
on	O
the	O
ﬁrst	O
pass	O
,	O
each	O
minibatch	O
is	O
used	O
to	O
compute	O
an	O
unbiased	O
estimate	O
of	O
the	O
true	O
generalization	O
error	O
.	O
on	O
the	O
second	O
pass	O
,	O
the	O
estimate	O
becomes	O
biased	O
because	O
it	O
is	O
formed	O
by	O
re-sampling	O
values	O
that	O
have	O
already	O
been	O
used	O
,	O
rather	O
than	O
obtaining	O
new	O
fair	O
samples	O
from	O
the	O
data	O
generating	O
distribution	O
.	O
the	O
fact	O
that	O
stochastic	O
gradient	O
descent	B
minimizes	O
generalization	O
error	O
is	O
easiest	O
to	O
see	O
in	O
the	O
online	O
learning	O
case	O
,	O
where	O
examples	O
or	O
minibatches	O
are	O
drawn	O
from	O
a	O
stream	O
of	O
data	O
.	O
in	O
other	O
words	O
,	O
instead	O
of	O
receiving	O
a	O
ﬁxed-size	O
training	O
set	O
,	O
the	O
learner	O
is	O
similar	O
to	O
a	O
living	O
being	O
who	O
sees	O
a	O
new	O
example	O
at	O
each	O
instant	O
,	O
with	O
every	O
example	O
(	O
x	O
,	O
y	O
)	O
coming	O
from	O
the	O
data	O
generating	O
distribution	O
pdata	O
(	O
x	O
,	O
y	O
)	O
.	O
in	O
this	O
scenario	O
,	O
examples	O
are	O
never	O
repeated	O
;	O
every	O
experience	O
is	O
a	O
fair	O
sample	O
from	O
p	O
data	O
.	O
the	O
equivalence	O
is	O
easiest	O
to	O
derive	O
when	O
both	O
x	O
and	O
y	O
are	O
discrete	O
.	O
in	O
this	O
case	O
,	O
the	O
generalization	O
error	O
(	O
equation	O
8.2	O
)	O
can	O
be	O
written	O
as	O
a	O
sum	O
	O
	O
	O
	O
x	O
y	O
∗	O
(	O
)	O
=θ	O
j	O
pdata	O
(	O
x	O
,	O
y	O
l	O
f	O
x	O
θ	O
,	O
y	O
,	O
)	O
)	O
(	O
(	O
;	O
)	O
(	O
8.7	O
)	O
(	O
8.8	O
)	O
with	O
the	O
exact	O
gradient	O
∇	O
g	O
=	O
θj	O
∗	O
(	O
)	O
=θ	O
∇	O
)	O
x	O
,	O
y	O
pdata	O
(	O
x	O
y	O
θl	O
f	O
(	O
(	O
;	O
)	O
x	O
θ	O
,	O
y	O
.	O
)	O
8.5	O
and	O
equation	O
we	O
have	O
already	O
seen	O
the	O
same	O
fact	O
demonstrated	O
for	O
the	O
log-likelihood	O
in	O
equa-	O
l	O
tion	B
besides	O
the	O
likelihood	O
.	O
a	O
similar	O
result	O
can	O
be	O
derived	O
when	O
x	O
and	O
y	O
are	O
continuous	O
,	O
under	O
mild	O
assumptions	O
regarding	O
pdata	O
and	O
.l	O
;	O
we	O
observe	O
now	O
that	O
this	O
holds	O
for	O
other	O
functions	O
8.6	O
hence	O
,	O
we	O
can	O
obtain	O
an	O
unbiased	O
estimator	O
of	O
the	O
exact	O
gradient	O
of	O
the	O
with	O
cor-	O
from	O
the	O
data	O
generating	O
distribution	O
pdata	O
,	O
and	O
computing	O
generalization	O
error	O
by	O
sampling	O
a	O
minibatch	O
of	O
examples	O
responding	O
targets	O
y	O
(	O
)	O
i	O
the	O
gradient	O
of	O
the	O
loss	O
with	O
respect	O
to	O
the	O
parameters	O
for	O
that	O
minibatch	O
:	O
{	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
x	O
(	O
	O
}	O
)	O
m	O
∇	O
θ	O
1	O
m	O
ˆg	O
=	O
i	O
l	O
f	O
(	O
(	O
x	O
(	O
)	O
i	O
;	O
)	O
θ	O
,	O
y	O
(	O
)	O
i	O
)	O
.	O
(	O
8.9	O
)	O
updating	O
θ	O
in	O
the	O
direction	O
of	O
ˆg	O
performs	O
sgd	O
on	O
the	O
generalization	O
error	O
.	O
of	O
course	O
,	O
this	O
interpretation	O
only	O
applies	O
when	O
examples	O
are	O
not	O
reused	O
.	O
nonetheless	O
,	O
it	O
is	O
usually	O
best	O
to	O
make	O
several	O
passes	O
through	O
the	O
training	O
set	O
,	O
unless	O
the	O
training	O
set	O
is	O
extremely	O
large	O
.	O
when	O
multiple	O
such	O
epochs	O
are	O
used	O
,	O
only	O
the	O
ﬁrst	O
epoch	O
follows	O
the	O
unbiased	O
gradient	O
of	O
the	O
generalization	O
error	O
,	O
but	O
281	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
of	O
course	O
,	O
the	O
additional	O
epochs	O
usually	O
provide	O
enough	O
beneﬁt	O
due	O
to	O
decreased	O
training	O
error	O
to	O
oﬀset	O
the	O
harm	O
they	O
cause	O
by	O
increasing	O
the	O
gap	O
between	O
training	O
error	O
and	O
test	O
error	O
.	O
with	O
some	O
datasets	O
growing	O
rapidly	O
in	O
size	O
,	O
faster	O
than	O
computing	O
power	O
,	O
it	O
is	O
becoming	O
more	O
common	O
for	O
machine	O
learning	O
applications	O
to	O
use	O
each	O
training	O
example	O
only	O
once	O
or	O
even	O
to	O
make	O
an	O
incomplete	O
pass	O
through	O
the	O
training	O
set	O
.	O
when	O
using	O
an	O
extremely	O
large	O
training	O
set	O
,	O
overﬁtting	O
is	O
not	O
an	O
issue	O
,	O
so	O
underﬁtting	O
and	O
computational	O
eﬃciency	O
become	O
the	O
predominant	O
concerns	O
.	O
see	O
also	O
)	O
for	O
a	O
discussion	O
of	O
the	O
eﬀect	O
of	O
computational	O
bottlenecks	O
on	O
generalization	O
error	O
,	O
as	O
the	O
number	O
of	O
training	O
examples	O
grows	O
.	O
bottou	O
and	O
bousquet	O
2008	O
(	O
8.2	O
challenges	O
in	O
neural	O
network	O
optimization	O
optimization	O
in	O
general	O
is	O
an	O
extremely	O
diﬃcult	O
task	O
.	O
traditionally	O
,	O
machine	O
learning	O
has	O
avoided	O
the	O
diﬃculty	O
of	O
general	O
optimization	O
by	O
carefully	O
designing	O
the	O
objective	O
function	O
and	O
constraints	O
to	O
ensure	O
that	O
the	O
optimization	O
problem	O
is	O
convex	O
.	O
when	O
training	O
neural	O
networks	O
,	O
we	O
must	O
confront	O
the	O
general	O
non-convex	O
case	O
.	O
even	O
convex	O
optimization	O
is	O
not	O
without	O
its	O
complications	O
.	O
in	O
this	O
section	O
,	O
we	O
summarize	O
several	O
of	O
the	O
most	O
prominent	O
challenges	O
involved	O
in	O
optimization	O
for	O
training	O
deep	O
models	O
.	O
8.2.1	O
ill-conditioning	O
some	O
challenges	O
arise	O
even	O
when	O
optimizing	O
convex	O
functions	O
.	O
of	O
these	O
,	O
the	O
most	O
prominent	O
is	O
ill-conditioning	O
of	O
the	O
hessian	O
matrix	O
h.	O
this	O
is	O
a	O
very	O
general	O
problem	O
in	O
most	O
numerical	O
optimization	O
,	O
convex	O
or	O
otherwise	O
,	O
and	O
is	O
described	O
in	O
more	O
detail	O
in	O
section	O
4.3.1	O
.	O
the	O
ill-conditioning	O
problem	O
is	O
generally	O
believed	O
to	O
be	O
present	O
in	O
neural	O
network	O
training	O
problems	O
.	O
ill-conditioning	O
can	O
manifest	O
by	O
causing	O
sgd	O
to	O
get	O
“	O
stuck	O
”	O
in	O
the	O
sense	O
that	O
even	O
very	O
small	O
steps	O
increase	O
the	O
cost	O
function	O
.	O
recall	O
from	O
equation	O
4.9	O
that	O
a	O
second-order	O
taylor	O
series	O
expansion	O
of	O
the	O
cost	O
function	O
predicts	O
that	O
a	O
gradient	O
descent	B
step	O
of	O
will	O
add	O
−	O
g	O
	O
2g	O
1	O
2	O
hg	O
−	O
	O
g	O
g	O
	O
to	O
the	O
cost	O
.	O
ill-conditioning	O
of	O
the	O
gradient	O
becomes	O
a	O
problem	O
when	O
1	O
	O
2	O
exceeds	O
g	O
network	O
training	O
task	O
,	O
one	O
can	O
monitor	O
the	O
squared	O
gradient	O
norm	O
g	O
hg	O
g.	O
to	O
determine	O
whether	O
ill-conditioning	O
is	O
detrimental	O
to	O
a	O
neural	O
g	O
and	O
2g	O
	O
282	O
(	O
8.10	O
)	O
	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
m	O
r	O
o	O
n	O
t	O
n	O
e	O
i	O
d	O
a	O
r	O
g	O
16	O
14	O
12	O
10	O
8	O
6	O
4	O
2	O
0	O
−	O
−	O
2	O
50	O
1	O
0	O
.	O
0	O
9	O
.	O
0	O
8	O
.	O
0	O
7	O
.	O
0	O
6	O
.	O
0	O
5	O
.	O
0	O
4	O
.	O
0	O
3	O
.	O
0	O
2	O
.	O
0	O
1.	O
e	O
t	O
a	O
r	O
r	O
o	O
r	O
r	O
e	O
n	O
o	O
i	O
t	O
a	O
c	O
ﬁ	O
i	O
s	O
s	O
a	O
l	O
c	O
0	O
50	O
100	O
150	O
200	O
250	O
0	O
50	O
100	O
150	O
200	O
250	O
training	O
time	O
(	O
epochs	O
)	O
training	O
time	O
(	O
epochs	O
)	O
figure	O
8.1	O
:	O
gradient	O
descent	B
often	O
does	O
not	O
arrive	O
at	O
a	O
critical	O
point	O
of	O
any	O
kind	O
.	O
in	O
this	O
example	O
,	O
the	O
gradient	O
norm	O
increases	O
throughout	O
training	O
of	O
a	O
convolutional	O
network	O
used	O
for	O
object	O
detection	O
.	O
(	O
left	O
)	O
a	O
scatterplot	O
showing	O
how	O
the	O
norms	O
of	O
individual	O
gradient	O
evaluations	O
are	O
distributed	O
over	O
time	O
.	O
to	O
improve	O
legibility	O
,	O
only	O
one	O
gradient	O
norm	O
is	O
plotted	O
per	O
epoch	O
.	O
the	O
running	O
average	O
of	O
all	O
gradient	O
norms	O
is	O
plotted	O
as	O
a	O
solid	O
curve	O
.	O
the	O
gradient	O
norm	O
clearly	O
increases	O
over	O
time	O
,	O
rather	O
than	O
decreasing	O
as	O
we	O
would	O
expect	O
if	O
the	O
training	O
process	O
converged	O
to	O
a	O
critical	O
point	O
.	O
despite	O
the	O
increasing	O
gradient	O
,	O
the	O
training	O
process	O
is	O
reasonably	O
successful	O
.	O
the	O
validation	O
set	O
classiﬁcation	O
error	O
decreases	O
to	O
a	O
low	O
level	O
.	O
(	O
right	O
)	O
	O
	O
hg	O
term	O
.	O
in	O
many	O
cases	O
,	O
the	O
gradient	O
norm	O
does	O
not	O
shrink	O
signiﬁcantly	O
the	O
g	O
throughout	O
learning	O
,	O
but	O
the	O
g	O
hg	O
term	O
grows	O
by	O
more	O
than	O
an	O
order	O
of	O
magnitude	O
.	O
the	O
result	O
is	O
that	O
learning	O
becomes	O
very	O
slow	O
despite	O
the	O
presence	O
of	O
a	O
strong	O
gradient	O
because	O
the	O
learning	O
rate	O
must	O
be	O
shrunk	O
to	O
compensate	O
for	O
even	O
stronger	O
curvature	O
.	O
figure	O
shows	O
an	O
example	O
of	O
the	O
gradient	O
increasing	O
signiﬁcantly	O
during	O
the	O
successful	O
training	O
of	O
a	O
neural	O
network	O
.	O
8.1	O
though	O
ill-conditioning	O
is	O
present	O
in	O
other	O
settings	O
besides	O
neural	O
network	O
training	O
,	O
some	O
of	O
the	O
techniques	O
used	O
to	O
combat	O
it	O
in	O
other	O
contexts	O
are	O
less	O
applicable	O
to	O
neural	O
networks	O
.	O
for	O
example	O
,	O
newton	O
’	O
s	O
method	O
is	O
an	O
excellent	O
tool	O
for	O
minimizing	O
convex	O
functions	O
with	O
poorly	O
conditioned	O
hessian	O
matrices	O
,	O
but	O
in	O
the	O
subsequent	O
sections	O
we	O
will	O
argue	O
that	O
newton	O
’	O
s	O
method	O
requires	O
signiﬁcant	O
modiﬁcation	O
before	O
it	O
can	O
be	O
applied	O
to	O
neural	O
networks	O
.	O
8.2.2	O
local	O
minima	O
one	O
of	O
the	O
most	O
prominent	O
features	O
of	O
a	O
convex	O
optimization	O
problem	O
is	O
that	O
it	O
can	O
be	O
reduced	O
to	O
the	O
problem	O
of	O
ﬁnding	O
a	O
local	O
minimum	O
.	O
any	O
local	O
minimum	O
is	O
283	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
guaranteed	O
to	O
be	O
a	O
global	O
minimum	O
.	O
some	O
convex	O
functions	O
have	O
a	O
ﬂat	O
region	O
at	O
the	O
bottom	O
rather	O
than	O
a	O
single	O
global	O
minimum	O
point	O
,	O
but	O
any	O
point	O
within	O
such	O
a	O
ﬂat	O
region	O
is	O
an	O
acceptable	O
solution	O
.	O
when	O
optimizing	O
a	O
convex	O
function	O
,	O
we	O
know	O
that	O
we	O
have	O
reached	O
a	O
good	O
solution	O
if	O
we	O
ﬁnd	O
a	O
critical	O
point	O
of	O
any	O
kind	O
.	O
with	O
non-convex	O
functions	O
,	O
such	O
as	O
neural	O
nets	O
,	O
it	O
is	O
possible	O
to	O
have	O
many	O
local	O
minima	O
.	O
indeed	O
,	O
nearly	O
any	O
deep	O
model	B
is	O
essentially	O
guaranteed	O
to	O
have	O
an	O
extremely	O
large	O
number	O
of	O
local	O
minima	O
.	O
however	O
,	O
as	O
we	O
will	O
see	O
,	O
this	O
is	O
not	O
necessarily	O
a	O
major	O
problem	O
.	O
neural	O
networks	O
and	O
any	O
models	O
with	O
multiple	O
equivalently	O
parametrized	O
latent	O
variables	O
all	O
have	O
multiple	O
local	O
minima	O
because	O
of	O
the	O
model	B
identiﬁability	O
problem	O
.	O
a	O
model	B
is	O
said	O
to	O
be	O
identiﬁable	O
if	O
a	O
suﬃciently	O
large	O
training	O
set	O
can	O
rule	O
out	O
all	O
but	O
one	O
setting	O
of	O
the	O
model	B
’	O
s	O
parameters	O
.	O
models	O
with	O
latent	O
variables	O
are	O
often	O
not	O
identiﬁable	O
because	O
we	O
can	O
obtain	O
equivalent	O
models	O
by	O
exchanging	O
latent	O
variables	O
with	O
each	O
other	O
.	O
for	O
example	O
,	O
we	O
could	O
take	O
a	O
neural	O
network	O
and	O
modify	O
layer	O
1	O
by	O
swapping	O
the	O
incoming	O
weight	O
vector	O
for	O
unit	O
i	O
with	O
the	O
incoming	O
weight	O
vector	O
for	O
unit	O
j	O
,	O
then	O
doing	O
the	O
same	O
for	O
the	O
outgoing	O
weight	O
vectors	O
.	O
if	O
we	O
have	O
m	O
layers	O
with	O
n	O
units	O
each	O
,	O
then	O
there	O
are	O
n	O
!	O
m	O
ways	O
of	O
arranging	O
the	O
hidden	O
units	O
.	O
this	O
kind	O
of	O
non-identiﬁability	O
is	O
known	O
as	O
weight	O
space	O
symmetry	O
.	O
in	O
addition	O
to	O
weight	O
space	O
symmetry	O
,	O
many	O
kinds	O
of	O
neural	O
networks	O
have	O
additional	O
causes	O
of	O
non-identiﬁability	O
.	O
for	O
example	O
,	O
in	O
any	O
rectiﬁed	O
linear	O
or	O
maxout	O
network	O
,	O
we	O
can	O
scale	O
all	O
of	O
the	O
incoming	O
weights	O
and	O
biases	O
of	O
a	O
unit	O
by	O
α	O
if	O
we	O
also	O
scale	O
all	O
of	O
its	O
outgoing	O
weights	O
by	O
1	O
.	O
this	O
means	O
that—if	O
the	O
cost	O
α	O
function	O
does	O
not	O
include	O
terms	O
such	O
as	O
weight	O
decay	O
that	O
depend	O
directly	O
on	O
the	O
×	O
weights	O
rather	O
than	O
the	O
models	O
’	O
outputs—every	O
local	O
minimum	O
of	O
a	O
rectiﬁed	O
linear	O
or	O
maxout	O
network	O
lies	O
on	O
an	O
(	O
m	O
n	O
)	O
-dimensional	O
hyperbola	O
of	O
equivalent	O
local	O
minima	O
.	O
these	O
model	B
identiﬁability	O
issues	O
mean	O
that	O
there	O
can	O
be	O
an	O
extremely	O
large	O
or	O
even	O
uncountably	O
inﬁnite	O
amount	O
of	O
local	O
minima	O
in	O
a	O
neural	O
network	O
cost	O
function	O
.	O
however	O
,	O
all	O
of	O
these	O
local	O
minima	O
arising	O
from	O
non-identiﬁability	O
are	O
equivalent	O
to	O
each	O
other	O
in	O
cost	O
function	O
value	O
.	O
as	O
a	O
result	O
,	O
these	O
local	O
minima	O
are	O
not	O
a	O
problematic	O
form	O
of	O
non-convexity	O
.	O
local	O
minima	O
can	O
be	O
problematic	O
if	O
they	O
have	O
high	O
cost	O
in	O
comparison	O
to	O
the	O
global	O
minimum	O
.	O
one	O
can	O
construct	O
small	O
neural	O
networks	O
,	O
even	O
without	O
hidden	O
units	O
,	O
that	O
have	O
local	O
minima	O
with	O
higher	O
cost	O
than	O
the	O
global	O
minimum	O
(	O
sontag	O
and	O
sussman	O
1989	O
brady	O
)	O
.	O
if	O
local	O
minima	O
with	O
high	O
cost	O
are	O
common	O
,	O
this	O
could	O
pose	O
a	O
serious	O
problem	O
for	O
gradient-based	O
optimization	O
algorithms	O
.	O
1989	O
gori	O
and	O
tesi	O
1992	O
et	O
al.	O
,	O
,	O
;	O
;	O
,	O
it	O
remains	O
an	O
open	O
question	O
whether	O
there	O
are	O
many	O
local	O
minima	O
of	O
high	O
cost	O
284	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
for	O
networks	O
of	O
practical	O
interest	O
and	O
whether	O
optimization	O
algorithms	O
encounter	O
them	O
.	O
for	O
many	O
years	O
,	O
most	O
practitioners	O
believed	O
that	O
local	O
minima	O
were	O
a	O
common	O
problem	O
plaguing	O
neural	O
network	O
optimization	O
.	O
today	O
,	O
that	O
does	O
not	O
appear	O
to	O
be	O
the	O
case	O
.	O
the	O
problem	O
remains	O
an	O
active	O
area	O
of	O
research	O
,	O
but	O
experts	O
now	O
suspect	O
that	O
,	O
for	O
suﬃciently	O
large	O
neural	O
networks	O
,	O
most	O
local	O
minima	O
have	O
a	O
low	O
cost	O
function	O
value	O
,	O
and	O
that	O
it	O
is	O
not	O
important	O
to	O
ﬁnd	O
a	O
true	O
global	O
minimum	O
rather	O
than	O
to	O
ﬁnd	O
a	O
point	O
in	O
parameter	O
space	O
that	O
has	O
low	O
but	O
not	O
minimal	O
cost	O
(	O
saxe	O
et	O
al	O
.	O
2013	O
dauphin	O
et	O
al	O
.	O
2014	O
goodfellow	O
et	O
al	O
.	O
2015	O
choromanska	O
et	O
al.	O
,	O
2014	O
,	O
)	O
.	O
,	O
;	O
;	O
,	O
;	O
many	O
practitioners	O
attribute	O
nearly	O
all	O
diﬃculty	O
with	O
neural	O
network	O
optimiza-	O
tion	B
to	O
local	O
minima	O
.	O
we	O
encourage	O
practitioners	O
to	O
carefully	O
test	O
for	O
speciﬁc	O
problems	O
.	O
a	O
test	O
that	O
can	O
rule	O
out	O
local	O
minima	O
as	O
the	O
problem	O
is	O
to	O
plot	O
the	O
norm	O
of	O
the	O
gradient	O
over	O
time	O
.	O
if	O
the	O
norm	O
of	O
the	O
gradient	O
does	O
not	O
shrink	O
to	O
insigniﬁcant	O
size	O
,	O
the	O
problem	O
is	O
neither	O
local	O
minima	O
nor	O
any	O
other	O
kind	O
of	O
critical	O
point	O
.	O
this	O
kind	O
of	O
negative	O
test	O
can	O
rule	O
out	O
local	O
minima	O
.	O
in	O
high	O
dimensional	O
spaces	O
,	O
it	O
can	O
be	O
very	O
diﬃcult	O
to	O
positively	O
establish	O
that	O
local	O
minima	O
are	O
the	O
problem	O
.	O
many	O
structures	O
other	O
than	O
local	O
minima	O
also	O
have	O
small	O
gradients	O
.	O
8.2.3	O
plateaus	O
,	O
saddle	O
points	O
and	O
other	O
flat	O
regions	O
for	O
many	O
high-dimensional	O
non-convex	O
functions	O
,	O
local	O
minima	O
(	O
and	O
maxima	O
)	O
are	O
in	O
fact	O
rare	O
compared	O
to	O
another	O
kind	O
of	O
point	O
with	O
zero	O
gradient	O
:	O
a	O
saddle	O
point	O
.	O
some	O
points	O
around	O
a	O
saddle	O
point	O
have	O
greater	O
cost	O
than	O
the	O
saddle	O
point	O
,	O
while	O
others	O
have	O
a	O
lower	O
cost	O
.	O
at	O
a	O
saddle	O
point	O
,	O
the	O
hessian	O
matrix	O
has	O
both	O
positive	O
and	O
negative	O
eigenvalues	O
.	O
points	O
lying	O
along	O
eigenvectors	O
associated	O
with	O
positive	O
eigenvalues	O
have	O
greater	O
cost	O
than	O
the	O
saddle	O
point	O
,	O
while	O
points	O
lying	O
along	O
negative	O
eigenvalues	O
have	O
lower	O
value	O
.	O
we	O
can	O
think	O
of	O
a	O
saddle	O
point	O
as	O
being	O
a	O
local	O
minimum	O
along	O
one	O
cross-section	O
of	O
the	O
cost	O
function	O
and	O
a	O
local	O
maximum	O
along	O
another	O
cross-section	O
.	O
see	O
ﬁgure	O
for	O
an	O
illustration	O
.	O
4.5	O
many	O
classes	O
of	O
random	O
functions	O
exhibit	O
the	O
following	O
behavior	O
:	O
in	O
low-	O
→	O
dimensional	O
spaces	O
,	O
local	O
minima	O
are	O
common	O
.	O
in	O
higher	O
dimensional	O
spaces	O
,	O
local	O
n	O
minima	O
are	O
rare	O
and	O
saddle	O
points	O
are	O
more	O
common	O
.	O
for	O
a	O
function	O
f	O
:	O
r	O
r	O
of	O
this	O
type	O
,	O
the	O
expected	O
ratio	O
of	O
the	O
number	O
of	O
saddle	O
points	O
to	O
local	O
minima	O
grows	O
exponentially	O
with	O
n.	O
to	O
understand	O
the	O
intuition	O
behind	O
this	O
behavior	O
,	O
observe	O
that	O
the	O
hessian	O
matrix	O
at	O
a	O
local	O
minimum	O
has	O
only	O
positive	O
eigenvalues	O
.	O
the	O
hessian	O
matrix	O
at	O
a	O
saddle	O
point	O
has	O
a	O
mixture	O
of	O
positive	O
and	O
negative	O
eigenvalues	O
.	O
imagine	O
that	O
the	O
sign	O
of	O
each	O
eigenvalue	O
is	O
generated	O
by	O
ﬂipping	O
a	O
coin	O
.	O
in	O
a	O
single	O
dimension	O
,	O
it	O
is	O
easy	O
to	O
obtain	O
a	O
local	O
minimum	O
by	O
tossing	O
a	O
coin	O
and	O
getting	O
heads	O
once	O
.	O
in	O
n-dimensional	O
space	O
,	O
it	O
is	O
exponentially	O
unlikely	O
that	O
all	O
n	O
coin	O
tosses	O
will	O
285	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
be	O
heads	O
.	O
see	O
dauphin	O
et	O
al	O
.	O
2014	O
(	O
)	O
for	O
a	O
review	O
of	O
the	O
relevant	O
theoretical	O
work	B
.	O
an	O
amazing	O
property	O
of	O
many	O
random	O
functions	O
is	O
that	O
the	O
eigenvalues	O
of	O
the	O
hessian	O
become	O
more	O
likely	O
to	O
be	O
positive	O
as	O
we	O
reach	O
regions	O
of	O
lower	O
cost	O
.	O
in	O
our	O
coin	O
tossing	O
analogy	O
,	O
this	O
means	O
we	O
are	O
more	O
likely	O
to	O
have	O
our	O
coin	O
come	O
up	O
heads	O
n	O
times	O
if	O
we	O
are	O
at	O
a	O
critical	O
point	O
with	O
low	O
cost	O
.	O
this	O
means	O
that	O
local	O
minima	O
are	O
much	O
more	O
likely	O
to	O
have	O
low	O
cost	O
than	O
high	O
cost	O
.	O
critical	O
points	O
with	O
high	O
cost	O
are	O
far	O
more	O
likely	O
to	O
be	O
saddle	O
points	O
.	O
critical	O
points	O
with	O
extremely	O
high	O
cost	O
are	O
more	O
likely	O
to	O
be	O
local	O
maxima	O
.	O
(	O
14	O
baldi	O
and	O
hornik	O
1989	O
this	O
happens	O
for	O
many	O
classes	O
of	O
random	O
functions	O
.	O
does	O
it	O
happen	O
for	O
neural	O
)	O
showed	O
theoretically	O
that	O
shallow	O
autoencoders	O
networks	O
?	O
(	O
feedforward	O
networks	O
trained	O
to	O
copy	O
their	O
input	O
to	O
their	O
output	O
,	O
described	O
in	O
chapter	O
)	O
with	O
no	O
nonlinearities	O
have	O
global	O
minima	O
and	O
saddle	O
points	O
but	O
no	O
local	O
minima	O
with	O
higher	O
cost	O
than	O
the	O
global	O
minimum	O
.	O
they	O
observed	O
without	O
proof	O
that	O
these	O
results	O
extend	O
to	O
deeper	O
networks	O
without	O
nonlinearities	O
.	O
the	O
output	O
of	O
such	O
networks	O
is	O
a	O
linear	O
function	O
of	O
their	O
input	O
,	O
but	O
they	O
are	O
useful	O
to	O
study	O
as	O
a	O
model	B
of	O
nonlinear	O
neural	O
networks	O
because	O
their	O
loss	O
function	O
is	O
a	O
non-convex	O
function	O
of	O
their	O
parameters	O
.	O
such	O
networks	O
are	O
essentially	O
just	O
multiple	O
matrices	O
composed	O
together.	O
)	O
provided	O
exact	O
solutions	O
to	O
the	O
complete	O
learning	O
dynamics	O
in	O
such	O
networks	O
and	O
showed	O
that	O
learning	O
in	O
these	O
models	O
captures	O
many	O
of	O
the	O
qualitative	O
features	O
observed	O
in	O
the	O
training	O
of	O
deep	O
models	O
with	O
nonlinear	O
activation	O
functions.	O
)	O
showed	O
experimentally	O
that	O
real	O
neural	O
networks	O
also	O
have	O
loss	O
functions	O
that	O
contain	O
very	O
many	O
high-cost	O
saddle	O
points	O
.	O
choromanska	O
)	O
provided	O
additional	O
theoretical	O
arguments	O
,	O
showing	O
that	O
another	O
class	O
of	O
high-dimensional	O
random	O
functions	O
related	O
to	O
neural	O
networks	O
does	O
so	O
as	O
well	O
.	O
dauphin	O
et	O
al	O
.	O
2014	O
saxe	O
et	O
al	O
.	O
2013	O
et	O
al	O
.	O
(	O
(	O
(	O
2014	O
what	O
are	O
the	O
implications	O
of	O
the	O
proliferation	O
of	O
saddle	O
points	O
for	O
training	O
algo-	O
rithms	O
?	O
for	O
ﬁrst-order	O
optimization	O
algorithms	O
that	O
use	O
only	O
gradient	O
information	O
,	O
the	O
situation	O
is	O
unclear	O
.	O
the	O
gradient	O
can	O
often	O
become	O
very	O
small	O
near	O
a	O
saddle	O
point	O
.	O
on	O
the	O
other	O
hand	O
,	O
gradient	O
descent	B
empirically	O
seems	O
to	O
be	O
able	O
to	O
escape	O
saddle	O
points	O
in	O
many	O
cases.	O
)	O
provided	O
visualizations	O
of	O
several	O
learning	O
trajectories	O
of	O
state-of-the-art	O
neural	O
networks	O
,	O
with	O
an	O
example	O
given	O
in	O
ﬁgure	O
.	O
these	O
visualizations	O
show	O
a	O
ﬂattening	O
of	O
the	O
cost	O
function	O
near	O
a	O
prominent	O
saddle	O
point	O
where	O
the	O
weights	O
are	O
all	O
zero	O
,	O
but	O
they	O
also	O
show	O
the	O
gradient	O
descent	B
trajectory	O
rapidly	O
escaping	O
this	O
region	O
.	O
goodfellow	O
et	O
al	O
.	O
2015	O
)	O
also	O
argue	O
that	O
continuous-time	O
gradient	O
descent	B
may	O
be	O
shown	O
analytically	O
to	O
be	O
repelled	O
from	O
,	O
rather	O
than	O
attracted	O
to	O
,	O
a	O
nearby	O
saddle	O
point	O
,	O
but	O
the	O
situation	O
may	O
be	O
diﬀerent	O
for	O
more	O
realistic	O
uses	O
of	O
gradient	O
descent	B
.	O
goodfellow	O
et	O
al	O
.	O
2015	O
8.2	O
(	O
(	O
for	O
newton	O
’	O
s	O
method	O
,	O
it	O
is	O
clear	O
that	O
saddle	O
points	O
constitute	O
a	O
problem	O
.	O
286	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
j	O
(	O
θ	O
)	O
projection	O
1	O
of	O
θ	O
θ	O
f	O
o	O
2	O
n	O
t	O
i	O
o	O
c	O
e	O
j	O
o	O
p	O
r	O
2015	O
et	O
al	O
.	O
(	O
figure	O
8.2	O
:	O
a	O
visualization	O
of	O
the	O
cost	O
function	O
of	O
a	O
neural	O
network	O
.	O
image	O
adapted	O
with	O
permission	O
from	O
goodfellow	O
)	O
.	O
these	O
visualizations	O
appear	O
similar	O
for	O
feedforward	O
neural	O
networks	O
,	O
convolutional	O
networks	O
,	O
and	O
recurrent	O
networks	O
applied	O
to	O
real	O
object	O
recognition	B
and	O
natural	O
language	O
processing	O
tasks	O
.	O
surprisingly	O
,	O
these	O
visualizations	O
usually	O
do	O
not	O
show	O
many	O
conspicuous	O
obstacles	O
.	O
prior	O
to	O
the	O
success	O
of	O
stochastic	O
gradient	O
descent	B
for	O
training	O
very	O
large	O
models	O
beginning	O
in	O
roughly	O
2012	O
,	O
neural	O
net	O
cost	O
function	O
surfaces	O
were	O
generally	O
believed	O
to	O
have	O
much	O
more	O
non-convex	O
structure	O
than	O
is	O
revealed	O
by	O
these	O
projections	O
.	O
the	O
primary	O
obstacle	O
revealed	O
by	O
this	O
projection	O
is	O
a	O
saddle	O
point	O
of	O
high	O
cost	O
near	O
where	O
the	O
parameters	O
are	O
initialized	O
,	O
but	O
,	O
as	O
indicated	O
by	O
the	O
blue	O
path	O
,	O
the	O
sgd	O
training	O
trajectory	O
escapes	O
this	O
saddle	O
point	O
readily	O
.	O
most	O
of	O
training	O
time	O
is	O
spent	O
traversing	O
the	O
relatively	O
ﬂat	O
valley	O
of	O
the	O
cost	O
function	O
,	O
which	O
may	O
be	O
due	O
to	O
high	O
noise	O
in	O
the	O
gradient	O
,	O
poor	O
conditioning	O
of	O
the	O
hessian	O
matrix	O
in	O
this	O
region	O
,	O
or	O
simply	O
the	O
need	O
to	O
circumnavigate	O
the	O
tall	O
“	O
mountain	O
”	O
visible	O
in	O
the	O
ﬁgure	O
via	O
an	O
indirect	O
arcing	O
path	O
.	O
287	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
gradient	O
descent	B
is	O
designed	O
to	O
move	O
“	O
downhill	O
”	O
and	O
is	O
not	O
explicitly	O
designed	O
to	O
seek	O
a	O
critical	O
point	O
.	O
newton	O
’	O
s	O
method	O
,	O
however	O
,	O
is	O
designed	O
to	O
solve	O
for	O
a	O
point	O
where	O
the	O
gradient	O
is	O
zero	O
.	O
without	O
appropriate	O
modiﬁcation	O
,	O
it	O
can	O
jump	O
to	O
a	O
saddle	O
point	O
.	O
the	O
proliferation	O
of	O
saddle	O
points	O
in	O
high	O
dimensional	O
spaces	O
presumably	O
explains	O
why	O
second-order	O
methods	O
have	O
not	O
succeeded	O
in	O
replacing	O
gradient	O
descent	B
for	O
neural	O
network	O
training.	O
)	O
introduced	O
a	O
saddle-free	O
newton	O
method	O
for	O
second-order	O
optimization	O
and	O
showed	O
that	O
it	O
improves	O
signiﬁcantly	O
over	O
the	O
traditional	O
version	O
.	O
second-order	O
methods	O
remain	O
diﬃcult	O
to	O
scale	O
to	O
large	O
neural	O
networks	O
,	O
but	O
this	O
saddle-free	O
approach	O
holds	O
promise	O
if	O
it	O
could	O
be	O
scaled	O
.	O
dauphin	O
et	O
al	O
.	O
2014	O
(	O
there	O
are	O
other	O
kinds	O
of	O
points	O
with	O
zero	O
gradient	O
besides	O
minima	O
and	O
saddle	O
points	O
.	O
there	O
are	O
also	O
maxima	O
,	O
which	O
are	O
much	O
like	O
saddle	O
points	O
from	O
the	O
perspective	O
of	O
optimization—many	O
algorithms	O
are	O
not	O
attracted	O
to	O
them	O
,	O
but	O
unmodiﬁed	O
newton	O
’	O
s	O
method	O
is	O
.	O
maxima	O
of	O
many	O
classes	O
of	O
random	O
functions	O
become	O
exponentially	O
rare	O
in	O
high	O
dimensional	O
space	O
,	O
just	O
like	O
minima	O
do	O
.	O
there	O
may	O
also	O
be	O
wide	O
,	O
ﬂat	O
regions	O
of	O
constant	O
value	O
.	O
in	O
these	O
locations	O
,	O
the	O
gradient	O
and	O
also	O
the	O
hessian	O
are	O
all	O
zero	O
.	O
such	O
degenerate	O
locations	O
pose	O
major	O
problems	O
for	O
all	O
numerical	O
optimization	O
algorithms	O
.	O
in	O
a	O
convex	O
problem	O
,	O
a	O
wide	O
,	O
ﬂat	O
region	O
must	O
consist	O
entirely	O
of	O
global	O
minima	O
,	O
but	O
in	O
a	O
general	O
optimization	O
problem	O
,	O
such	O
a	O
region	O
could	O
correspond	O
to	O
a	O
high	O
value	O
of	O
the	O
objective	O
function	O
.	O
8.2.4	O
cliﬀs	O
and	O
exploding	O
gradients	O
neural	O
networks	O
with	O
many	O
layers	O
often	O
have	O
extremely	O
steep	O
regions	O
resembling	O
cliﬀs	O
,	O
as	O
illustrated	O
in	O
ﬁgure	O
.	O
these	O
result	O
from	O
the	O
multiplication	O
of	O
several	O
large	O
weights	O
together	O
.	O
on	O
the	O
face	O
of	O
an	O
extremely	O
steep	O
cliﬀ	O
structure	O
,	O
the	O
gradient	O
update	O
step	O
can	O
move	O
the	O
parameters	O
extremely	O
far	O
,	O
usually	O
jumping	O
oﬀ	O
of	O
the	O
cliﬀ	O
structure	O
altogether	O
.	O
8.3	O
288	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
	O
	O
	O
	O
	O
	O
	O
	O
figure	O
8.3	O
:	O
the	O
objective	O
function	O
for	O
highly	O
nonlinear	O
deep	O
neural	O
networks	O
or	O
for	O
recurrent	O
neural	O
networks	O
often	O
contains	O
sharp	O
nonlinearities	O
in	O
parameter	O
space	O
resulting	O
from	O
the	O
multiplication	O
of	O
several	O
parameters	O
.	O
these	O
nonlinearities	O
give	O
rise	O
to	O
very	O
high	O
derivatives	O
in	O
some	O
places	O
.	O
when	O
the	O
parameters	O
get	O
close	O
to	O
such	O
a	O
cliﬀ	O
region	O
,	O
a	O
gradient	O
descent	B
update	O
can	O
catapult	O
the	O
parameters	O
very	O
far	O
,	O
possibly	O
losing	O
most	O
of	O
the	O
optimization	O
work	B
that	O
had	O
been	O
done	O
.	O
figure	O
adapted	O
with	O
permission	O
from	O
pascanu	O
et	O
al	O
.	O
(	O
2013	O
)	O
.	O
10.11.1	O
the	O
cliﬀ	O
can	O
be	O
dangerous	O
whether	O
we	O
approach	O
it	O
from	O
above	O
or	O
from	O
below	O
,	O
but	O
fortunately	O
its	O
most	O
serious	O
consequences	O
can	O
be	O
avoided	O
using	O
the	O
gradient	O
clipping	O
heuristic	O
described	O
in	O
section	O
.	O
the	O
basic	O
idea	O
is	O
to	O
recall	O
that	O
the	O
gradient	O
does	O
not	O
specify	O
the	O
optimal	O
step	O
size	O
,	O
but	O
only	O
the	O
optimal	O
direction	O
within	O
an	O
inﬁnitesimal	O
region	O
.	O
when	O
the	O
traditional	O
gradient	O
descent	B
algorithm	O
proposes	O
to	O
make	O
a	O
very	O
large	O
step	O
,	O
the	O
gradient	O
clipping	O
heuristic	O
intervenes	O
to	O
reduce	O
the	O
step	O
size	O
to	O
be	O
small	O
enough	O
that	O
it	O
is	O
less	O
likely	O
to	O
go	O
outside	O
the	O
region	O
where	O
the	O
gradient	O
indicates	O
the	O
direction	O
of	O
approximately	O
steepest	O
descent	B
.	O
cliﬀ	O
structures	O
are	O
most	O
common	O
in	O
the	O
cost	O
functions	O
for	O
recurrent	O
neural	O
networks	O
,	O
because	O
such	O
models	O
involve	O
a	O
multiplication	O
of	O
many	O
factors	O
,	O
with	O
one	O
factor	O
for	O
each	O
time	O
step	O
.	O
long	O
temporal	O
sequences	O
thus	O
incur	O
an	O
extreme	O
amount	O
of	O
multiplication	O
.	O
8.2.5	O
long-term	O
dependencies	O
another	O
diﬃculty	O
that	O
neural	O
network	O
optimization	O
algorithms	O
must	O
overcome	O
arises	O
when	O
the	O
computational	O
graph	O
becomes	O
extremely	O
deep	O
.	O
feedforward	O
networks	O
with	O
many	O
layers	O
have	O
such	O
deep	O
computational	O
graphs	O
.	O
so	O
do	O
recurrent	O
networks	O
,	O
described	O
in	O
chapter	O
,	O
which	O
construct	O
very	O
deep	O
computational	O
graphs	O
10	O
289	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
by	O
repeatedly	O
applying	O
the	O
same	O
operation	O
at	O
each	O
time	O
step	O
of	O
a	O
long	O
temporal	O
sequence	O
.	O
repeated	O
application	O
of	O
the	O
same	O
parameters	O
gives	O
rise	O
to	O
especially	O
pronounced	O
diﬃculties	O
.	O
for	O
example	O
,	O
suppose	O
that	O
a	O
computational	O
graph	O
contains	O
a	O
path	O
that	O
consists	O
of	O
repeatedly	O
multiplying	O
by	O
a	O
matrix	O
w	O
.	O
after	O
t	O
steps	O
,	O
this	O
is	O
equivalent	O
to	O
mul-	O
−	O
tiplying	O
by	O
w	O
t.	O
suppose	O
that	O
w	O
has	O
an	O
eigendecomposition	O
w	O
=	O
v	O
diag	O
(	O
λ	O
)	O
v	O
1.	O
in	O
this	O
simple	O
case	O
,	O
it	O
is	O
straightforward	O
to	O
see	O
that	O
	O
	O
−	O
1	O
w	O
t	O
=	O
v	O
diag	O
(	O
)	O
λ	O
v	O
t	O
=	O
v	O
diag	O
λ	O
tv	O
(	O
)	O
(	O
8.11	O
)	O
−	O
1	O
.	O
1	O
1	O
any	O
eigenvalues	O
λi	O
that	O
are	O
not	O
near	O
an	O
absolute	O
value	O
of	O
will	O
either	O
explode	O
if	O
they	O
are	O
greater	O
than	O
in	O
magnitude	O
or	O
vanish	O
if	O
they	O
are	O
less	O
than	O
in	O
magnitude	O
.	O
the	O
vanishing	O
and	O
exploding	O
gradient	O
problem	O
refers	O
to	O
the	O
fact	O
that	O
gradients	O
through	O
such	O
a	O
graph	O
are	O
also	O
scaled	O
according	O
to	O
diag	O
(	O
λ	O
)	O
t.	O
vanishing	O
gradients	O
make	O
it	O
diﬃcult	O
to	O
know	O
which	O
direction	O
the	O
parameters	O
should	O
move	O
to	O
improve	O
the	O
cost	O
function	O
,	O
while	O
exploding	O
gradients	O
can	O
make	O
learning	O
unstable	O
.	O
the	O
cliﬀ	O
structures	O
described	O
earlier	O
that	O
motivate	O
gradient	O
clipping	O
are	O
an	O
example	O
of	O
the	O
exploding	O
gradient	O
phenomenon	O
.	O
1	O
the	O
repeated	O
multiplication	O
by	O
w	O
at	O
each	O
time	O
step	O
described	O
here	O
is	O
very	O
similar	O
to	O
the	O
power	O
method	O
algorithm	O
used	O
to	O
ﬁnd	O
the	O
largest	O
eigenvalue	O
of	O
a	O
matrix	O
w	O
and	O
the	O
corresponding	O
eigenvector	O
.	O
from	O
this	O
point	O
of	O
view	O
it	O
is	O
	O
not	O
surprising	O
that	O
x	O
w	O
t	O
will	O
eventually	O
discard	O
all	O
components	O
of	O
x	O
that	O
are	O
orthogonal	O
to	O
the	O
principal	O
eigenvector	O
of	O
.w	O
recurrent	O
networks	O
use	O
the	O
same	O
matrix	O
w	O
at	O
each	O
time	O
step	O
,	O
but	O
feedforward	O
networks	O
do	O
not	O
,	O
so	O
even	O
very	O
deep	O
feedforward	O
networks	O
can	O
largely	O
avoid	O
the	O
vanishing	O
and	O
exploding	O
gradient	O
problem	O
(	O
sussillo	O
2014	O
)	O
.	O
,	O
we	O
defer	O
a	O
further	O
discussion	O
of	O
the	O
challenges	O
of	O
training	O
recurrent	O
networks	O
until	O
section	O
10.7	O
,	O
after	O
recurrent	O
networks	O
have	O
been	O
described	O
in	O
more	O
detail	O
.	O
8.2.6	O
inexact	O
gradients	O
most	O
optimization	O
algorithms	O
are	O
designed	O
with	O
the	O
assumption	O
that	O
we	O
have	O
access	O
to	O
the	O
exact	O
gradient	O
or	O
hessian	O
matrix	O
.	O
in	O
practice	O
,	O
we	O
usually	O
only	O
have	O
a	O
noisy	O
or	O
even	O
biased	O
estimate	O
of	O
these	O
quantities	O
.	O
nearly	O
every	O
deep	O
learning	O
algorithm	O
relies	O
on	O
sampling-based	O
estimates	O
at	O
least	O
insofar	O
as	O
using	O
a	O
minibatch	O
of	O
training	O
examples	O
to	O
compute	O
the	O
gradient	O
.	O
in	O
other	O
cases	O
,	O
the	O
objective	O
function	O
we	O
want	O
to	O
minimize	O
is	O
actually	O
intractable	O
.	O
when	O
the	O
objective	O
function	O
is	O
intractable	O
,	O
typically	O
its	O
gradient	O
is	O
intractable	O
as	O
well	O
.	O
in	O
such	O
cases	O
we	O
can	O
only	O
approximate	O
the	O
gradient	O
.	O
these	O
issues	O
mostly	O
arise	O
290	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
.	O
for	O
example	O
,	O
contrastive	O
divergence	O
with	O
the	O
more	O
advanced	O
models	O
in	O
part	O
gives	O
a	O
technique	O
for	O
approximating	O
the	O
gradient	O
of	O
the	O
intractable	O
log-likelihood	O
of	O
a	O
boltzmann	O
machine	O
.	O
iii	O
various	O
neural	O
network	O
optimization	O
algorithms	O
are	O
designed	O
to	O
account	O
for	O
imperfections	O
in	O
the	O
gradient	O
estimate	O
.	O
one	O
can	O
also	O
avoid	O
the	O
problem	O
by	O
choosing	O
a	O
surrogate	O
loss	O
function	O
that	O
is	O
easier	O
to	O
approximate	O
than	O
the	O
true	O
loss	O
.	O
8.2.7	O
poor	O
correspondence	O
between	O
local	O
and	O
global	O
structure	O
many	O
of	O
the	O
problems	O
we	O
have	O
discussed	O
so	O
far	O
correspond	O
to	O
properties	O
of	O
the	O
loss	O
function	O
at	O
a	O
single	O
point—it	O
can	O
be	O
diﬃcult	O
to	O
make	O
a	O
single	O
step	O
if	O
j	O
(	O
θ	O
)	O
is	O
poorly	O
conditioned	O
at	O
the	O
current	O
point	O
θ	O
,	O
or	O
if	O
θ	O
lies	O
on	O
a	O
cliﬀ	O
,	O
or	O
if	O
θ	O
is	O
a	O
saddle	O
point	O
hiding	O
the	O
opportunity	O
to	O
make	O
progress	O
downhill	O
from	O
the	O
gradient	O
.	O
it	O
is	O
possible	O
to	O
overcome	O
all	O
of	O
these	O
problems	O
at	O
a	O
single	O
point	O
and	O
still	O
perform	O
poorly	O
if	O
the	O
direction	O
that	O
results	O
in	O
the	O
most	O
improvement	O
locally	O
does	O
not	O
point	O
toward	O
distant	O
regions	O
of	O
much	O
lower	O
cost	O
.	O
2015	O
et	O
al	O
.	O
(	O
goodfellow	O
)	O
argue	O
that	O
much	O
of	O
the	O
runtime	O
of	O
training	O
is	O
due	O
to	O
the	O
length	O
of	O
the	O
trajectory	O
needed	O
to	O
arrive	O
at	O
the	O
solution	O
.	O
figure	O
shows	O
that	O
the	O
learning	O
trajectory	O
spends	O
most	O
of	O
its	O
time	O
tracing	O
out	O
a	O
wide	O
arc	O
around	O
a	O
mountain-shaped	O
structure	O
.	O
8.2	O
|	O
|	O
log	O
p	O
(	O
y	O
much	O
of	O
research	O
into	O
the	O
diﬃculties	O
of	O
optimization	O
has	O
focused	O
on	O
whether	O
training	O
arrives	O
at	O
a	O
global	O
minimum	O
,	O
a	O
local	O
minimum	O
,	O
or	O
a	O
saddle	O
point	O
,	O
but	O
in	O
practice	O
neural	O
networks	O
do	O
not	O
arrive	O
at	O
a	O
critical	O
point	O
of	O
any	O
kind	O
.	O
figure	O
8.1	O
shows	O
that	O
neural	O
networks	O
often	O
do	O
not	O
arrive	O
at	O
a	O
region	O
of	O
small	O
gradient	O
.	O
indeed	O
,	O
−	O
such	O
critical	O
points	O
do	O
not	O
even	O
necessarily	O
exist	O
.	O
for	O
example	O
,	O
the	O
loss	O
function	O
x	O
;	O
θ	O
)	O
can	O
lack	O
a	O
global	O
minimum	O
point	O
and	O
instead	O
asymptotically	O
approach	O
some	O
value	O
as	O
the	O
model	B
becomes	O
more	O
conﬁdent	O
.	O
for	O
a	O
classiﬁer	O
with	O
discrete	O
y	O
and	O
p	O
(	O
y	O
x	O
)	O
provided	O
by	O
a	O
softmax	O
,	O
the	O
negative	O
log-likelihood	O
can	O
become	O
arbitrarily	O
close	O
to	O
zero	O
if	O
the	O
model	B
is	O
able	O
to	O
correctly	O
classify	O
every	O
example	O
in	O
the	O
training	O
set	O
,	O
but	O
it	O
is	O
impossible	O
to	O
actually	O
reach	O
the	O
value	O
of	O
−	O
zero	O
.	O
likewise	O
,	O
a	O
model	B
of	O
real	O
values	O
p	O
(	O
y	O
1	O
)	O
can	O
have	O
negative	O
log-likelihood	O
that	O
asymptotes	O
to	O
negative	O
inﬁnity—if	O
f	O
(	O
θ	O
)	O
is	O
able	O
to	O
correctly	O
predict	O
the	O
value	O
of	O
all	O
training	O
set	O
y	O
targets	O
,	O
the	O
learning	O
algorithm	O
will	O
increase	O
β	O
without	O
bound	B
.	O
see	O
ﬁgure	O
for	O
an	O
example	O
of	O
a	O
failure	O
of	O
local	O
optimization	O
to	O
ﬁnd	O
a	O
good	O
cost	O
function	O
value	O
even	O
in	O
the	O
absence	O
of	O
any	O
local	O
minima	O
or	O
saddle	O
points	O
.	O
(	O
y	O
;	O
f	O
(	O
θ	O
)	O
,	O
β	O
x	O
)	O
=	O
8.4	O
|	O
n	O
future	O
research	O
will	O
need	O
to	O
develop	O
further	O
understanding	O
of	O
the	O
factors	O
that	O
inﬂuence	O
the	O
length	O
of	O
the	O
learning	O
trajectory	O
and	O
better	O
characterize	O
the	O
outcome	O
291	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
)	O
θ	O
(	O
j	O
θ	O
figure	O
8.4	O
:	O
optimization	O
based	O
on	O
local	O
downhill	O
moves	O
can	O
fail	O
if	O
the	O
local	O
surface	O
does	O
not	O
point	O
toward	O
the	O
global	O
solution	O
.	O
here	O
we	O
provide	O
an	O
example	O
of	O
how	O
this	O
can	O
occur	O
,	O
even	O
if	O
there	O
are	O
no	O
saddle	O
points	O
and	O
no	O
local	O
minima	O
.	O
this	O
example	O
cost	O
function	O
contains	O
only	O
asymptotes	O
toward	O
low	O
values	O
,	O
not	O
minima	O
.	O
the	O
main	O
cause	O
of	O
diﬃculty	O
in	O
this	O
case	O
is	O
being	O
initialized	O
on	O
the	O
wrong	O
side	O
of	O
the	O
“	O
mountain	O
”	O
and	O
not	O
being	O
able	O
to	O
traverse	O
it	O
.	O
in	O
higher	O
dimensional	O
space	O
,	O
learning	O
algorithms	O
can	O
often	O
circumnavigate	O
such	O
mountains	O
but	O
the	O
trajectory	O
associated	O
with	O
doing	O
so	O
may	O
be	O
long	O
and	O
result	O
in	O
excessive	O
training	O
time	O
,	O
as	O
illustrated	O
in	O
ﬁgure	O
.8.2	O
of	O
the	O
process	O
.	O
many	O
existing	O
research	O
directions	O
are	O
aimed	O
at	O
ﬁnding	O
good	O
initial	O
points	O
for	O
problems	O
that	O
have	O
diﬃcult	O
global	O
structure	O
,	O
rather	O
than	O
developing	O
algorithms	O
that	O
use	O
non-local	O
moves	O
.	O
gradient	O
descent	B
and	O
essentially	O
all	O
learning	O
algorithms	O
that	O
are	O
eﬀective	O
for	O
training	O
neural	O
networks	O
are	O
based	O
on	O
making	O
small	O
,	O
local	O
moves	O
.	O
the	O
previous	O
sections	O
have	O
primarily	O
focused	O
on	O
how	O
the	O
correct	O
direction	O
of	O
these	O
local	O
moves	O
can	O
be	O
diﬃcult	O
to	O
compute	O
.	O
we	O
may	O
be	O
able	O
to	O
compute	O
some	O
properties	O
of	O
the	O
objective	O
function	O
,	O
such	O
as	O
its	O
gradient	O
,	O
only	O
approximately	O
,	O
with	O
bias	O
or	O
variance	O
in	O
our	O
estimate	O
of	O
the	O
correct	O
direction	O
.	O
in	O
these	O
cases	O
,	O
local	O
descent	B
may	O
or	O
may	O
not	O
deﬁne	O
a	O
reasonably	O
short	O
path	O
to	O
a	O
valid	O
solution	O
,	O
but	O
we	O
are	O
not	O
actually	O
able	O
to	O
follow	O
the	O
local	O
descent	B
path	O
.	O
the	O
objective	O
function	O
may	O
have	O
issues	O
such	O
as	O
poor	O
conditioning	O
or	O
discontinuous	O
gradients	O
,	O
causing	O
the	O
region	O
where	O
the	O
gradient	O
provides	O
a	O
good	O
model	B
of	O
the	O
objective	O
function	O
to	O
be	O
very	O
small	O
.	O
in	O
these	O
cases	O
,	O
local	O
descent	B
with	O
steps	O
of	O
size	O
	O
may	O
deﬁne	O
a	O
reasonably	O
short	O
path	O
to	O
the	O
solution	O
,	O
but	O
we	O
are	O
only	O
able	O
to	O
compute	O
the	O
local	O
descent	B
direction	O
with	O
steps	O
of	O
size	O
δ	O
.	O
in	O
these	O
cases	O
,	O
local	O
descent	B
may	O
or	O
may	O
not	O
deﬁne	O
a	O
path	O
to	O
the	O
solution	O
,	O
but	O
the	O
path	O
contains	O
many	O
steps	O
,	O
so	O
following	O
the	O
path	O
incurs	O
a	O
	O
	O
292	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
high	O
computational	O
cost	O
.	O
sometimes	O
local	O
information	O
provides	O
us	O
no	O
guide	O
,	O
when	O
the	O
function	O
has	O
a	O
wide	O
ﬂat	O
region	O
,	O
or	O
if	O
we	O
manage	O
to	O
land	O
exactly	O
on	O
a	O
critical	O
point	O
(	O
usually	O
this	O
latter	O
scenario	O
only	O
happens	O
to	O
methods	O
that	O
solve	O
explicitly	O
for	O
critical	O
points	O
,	O
such	O
as	O
newton	O
’	O
s	O
method	O
)	O
.	O
in	O
these	O
cases	O
,	O
local	O
descent	B
does	O
not	O
deﬁne	O
a	O
path	O
to	O
a	O
solution	O
at	O
all	O
.	O
in	O
other	O
cases	O
,	O
local	O
moves	O
can	O
be	O
too	O
greedy	O
and	O
lead	O
us	O
along	O
a	O
path	O
that	O
moves	O
downhill	O
but	O
away	O
from	O
any	O
solution	O
,	O
as	O
in	O
ﬁgure	O
.	O
8.2	O
currently	O
,	O
we	O
do	O
not	O
understand	O
which	O
of	O
these	O
problems	O
are	O
most	O
relevant	O
to	O
making	O
neural	O
network	O
optimization	O
diﬃcult	O
,	O
and	O
this	O
is	O
an	O
active	O
area	O
of	O
research	O
.	O
,	O
or	O
along	O
an	O
unnecessarily	O
long	O
trajectory	O
to	O
the	O
solution	O
,	O
as	O
in	O
ﬁgure	O
8.4	O
regardless	O
of	O
which	O
of	O
these	O
problems	O
are	O
most	O
signiﬁcant	O
,	O
all	O
of	O
them	O
might	O
be	O
avoided	O
if	O
there	O
exists	O
a	O
region	O
of	O
space	O
connected	O
reasonably	O
directly	O
to	O
a	O
solution	O
by	O
a	O
path	O
that	O
local	O
descent	B
can	O
follow	O
,	O
and	O
if	O
we	O
are	O
able	O
to	O
initialize	O
learning	O
within	O
that	O
well-behaved	O
region	O
.	O
this	O
last	O
view	O
suggests	O
research	O
into	O
choosing	O
good	O
initial	O
points	O
for	O
traditional	O
optimization	O
algorithms	O
to	O
use	O
.	O
8.2.8	O
theoretical	O
limits	O
of	O
optimization	O
several	O
theoretical	O
results	O
show	O
that	O
there	O
are	O
limits	O
on	O
the	O
performance	O
of	O
any	O
optimization	O
algorithm	O
we	O
might	O
design	O
for	O
neural	O
networks	O
(	O
blum	O
and	O
rivest	O
,	O
1992	O
judd	O
1989	O
wolpert	O
and	O
macready	O
1997	O
)	O
.	O
typically	O
these	O
results	O
have	O
little	O
bearing	O
on	O
the	O
use	O
of	O
neural	O
networks	O
in	O
practice	O
.	O
;	O
,	O
,	O
;	O
some	O
theoretical	O
results	O
apply	O
only	O
to	O
the	O
case	O
where	O
the	O
units	O
of	O
a	O
neural	O
network	O
output	O
discrete	O
values	O
.	O
however	O
,	O
most	O
neural	O
network	O
units	O
output	O
smoothly	O
increasing	O
values	O
that	O
make	O
optimization	O
via	O
local	O
search	O
feasible	O
.	O
some	O
theoretical	O
results	O
show	O
that	O
there	O
exist	O
problem	O
classes	O
that	O
are	O
intractable	O
,	O
but	O
it	O
can	O
be	O
diﬃcult	O
to	O
tell	O
whether	O
a	O
particular	O
problem	O
falls	O
into	O
that	O
class	O
.	O
other	O
results	O
show	O
that	O
ﬁnding	O
a	O
solution	O
for	O
a	O
network	O
of	O
a	O
given	O
size	O
is	O
intractable	O
,	O
but	O
in	O
practice	O
we	O
can	O
ﬁnd	O
a	O
solution	O
easily	O
by	O
using	O
a	O
larger	O
network	O
for	O
which	O
many	O
more	O
parameter	O
settings	O
correspond	O
to	O
an	O
acceptable	O
solution	O
.	O
moreover	O
,	O
in	O
the	O
context	O
of	O
neural	O
network	O
training	O
,	O
we	O
usually	O
do	O
not	O
care	O
about	O
ﬁnding	O
the	O
exact	O
minimum	O
of	O
a	O
function	O
,	O
but	O
seek	O
only	O
to	O
reduce	O
its	O
value	O
suﬃciently	O
to	O
obtain	O
good	O
generalization	O
error	O
.	O
theoretical	O
analysis	O
of	O
whether	O
an	O
optimization	O
algorithm	O
can	O
accomplish	O
this	O
goal	O
is	O
extremely	O
diﬃcult	O
.	O
developing	O
more	O
realistic	O
bounds	O
on	O
the	O
performance	O
of	O
optimization	O
algorithms	O
therefore	O
remains	O
an	O
important	O
goal	O
for	O
machine	O
learning	O
research	O
.	O
293	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
8.3	O
basic	O
algorithms	O
we	O
have	O
previously	O
introduced	O
the	O
gradient	O
descent	B
(	O
section	O
)	O
algorithm	O
that	O
follows	O
the	O
gradient	O
of	O
an	O
entire	O
training	O
set	O
downhill	O
.	O
this	O
may	O
be	O
accelerated	O
considerably	O
by	O
using	O
stochastic	O
gradient	O
descent	B
to	O
follow	O
the	O
gradient	O
of	O
randomly	O
selected	O
minibatches	O
downhill	O
,	O
as	O
discussed	O
in	O
section	O
and	O
section	O
.	O
8.1.3	O
4.3	O
5.9	O
8.3.1	O
stochastic	O
gradient	O
descent	B
stochastic	O
gradient	O
descent	B
(	O
sgd	O
)	O
and	O
its	O
variants	O
are	O
probably	O
the	O
most	O
used	O
optimization	O
algorithms	O
for	O
machine	O
learning	O
in	O
general	O
and	O
for	O
deep	O
learning	O
in	O
particular	O
.	O
as	O
discussed	O
in	O
section	O
,	O
it	O
is	O
possible	O
to	O
obtain	O
an	O
unbiased	O
estimate	O
of	O
the	O
gradient	O
by	O
taking	O
the	O
average	O
gradient	O
on	O
a	O
minibatch	O
of	O
m	O
examples	O
drawn	O
i.i.d	O
from	O
the	O
data	O
generating	O
distribution	O
.	O
8.1.3	O
algorithm	O
8.1	O
shows	O
how	O
to	O
follow	O
this	O
estimate	O
of	O
the	O
gradient	O
downhill	O
.	O
algorithm	O
8.1	O
stochastic	O
gradient	O
descent	B
(	O
sgd	O
)	O
update	O
at	O
training	O
iteration	O
k	O
require	O
:	O
learning	O
rate	O
k	O
.	O
require	O
:	O
initial	O
parameter	O
θ	O
	O
while	O
stopping	O
criterion	O
not	O
met	O
do	O
sample	O
a	O
minibatch	O
of	O
m	O
examples	O
from	O
the	O
training	O
set	O
corresponding	O
targets	O
y	O
(	O
)	O
i	O
.	O
compute	O
gradient	O
estimate	O
:	O
ˆg	O
apply	O
update	O
:	O
θ	O
←	O
−	O
+	O
1	O
m	O
←	O
∇	O
θ	O
ˆg	O
θ	O
i	O
l	O
f	O
(	O
(	O
x	O
(	O
)	O
i	O
;	O
)	O
θ	O
,	O
y	O
(	O
)	O
i	O
)	O
{	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
}	O
)	O
m	O
with	O
end	O
while	O
a	O
crucial	O
parameter	O
for	O
the	O
sgd	O
algorithm	O
is	O
the	O
learning	O
rate	O
.	O
previously	O
,	O
we	O
have	O
described	O
sgd	O
as	O
using	O
a	O
ﬁxed	O
learning	O
rate	O
	O
.	O
in	O
practice	O
,	O
it	O
is	O
necessary	O
to	O
gradually	O
decrease	O
the	O
learning	O
rate	O
over	O
time	O
,	O
so	O
we	O
now	O
denote	O
the	O
learning	O
rate	O
at	O
iteration	O
ask	O
	O
k.	O
this	O
is	O
because	O
the	O
sgd	O
gradient	O
estimator	O
introduces	O
a	O
source	O
of	O
noise	O
(	O
the	O
random	O
sampling	O
of	O
m	O
training	O
examples	O
)	O
that	O
does	O
not	O
vanish	O
even	O
when	O
we	O
arrive	O
at	O
a	O
minimum	O
.	O
by	O
comparison	O
,	O
the	O
true	O
gradient	O
of	O
the	O
total	O
cost	O
function	O
becomes	O
small	O
and	O
then	O
0	O
when	O
we	O
approach	O
and	O
reach	O
a	O
minimum	O
using	O
batch	O
gradient	O
descent	B
,	O
so	O
batch	O
gradient	O
descent	B
can	O
use	O
a	O
ﬁxed	O
learning	O
rate	O
.	O
a	O
suﬃcient	O
condition	O
to	O
guarantee	O
convergence	O
of	O
sgd	O
is	O
that	O
	O
∞	O
∞	O
,	O
k	O
=	O
and	O
(	O
8.12	O
)	O
k=1	O
294	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
	O
∞	O
k=1	O
∞	O
2	O
k	O
<	O
.	O
(	O
8.13	O
)	O
in	O
practice	O
,	O
it	O
is	O
common	O
to	O
decay	O
the	O
learning	O
rate	O
linearly	O
until	O
iteration	O
:	O
τ	O
−	O
k	O
=	O
(	O
1	O
)	O
α	O
0	O
+	O
α	O
τ	O
(	O
8.14	O
)	O
with	O
α	O
=	O
k	O
τ	O
.	O
after	O
iteration	O
,	O
it	O
is	O
common	O
to	O
leave	O
τ	O
	O
constant	O
.	O
1	O
%	O
the	O
value	O
of	O
the	O
learning	O
rate	O
may	O
be	O
chosen	O
by	O
trial	O
and	O
error	O
,	O
but	O
it	O
is	O
usually	O
best	O
to	O
choose	O
it	O
by	O
monitoring	O
learning	O
curves	O
that	O
plot	O
the	O
objective	O
function	O
as	O
a	O
function	O
of	O
time	O
.	O
this	O
is	O
more	O
of	O
an	O
art	O
than	O
a	O
science	O
,	O
and	O
most	O
guidance	O
on	O
this	O
subject	O
should	O
be	O
regarded	O
with	O
some	O
skepticism	O
.	O
when	O
using	O
the	O
linear	O
schedule	O
,	O
the	O
parameters	O
to	O
choose	O
are	O
0	O
,	O
τ	O
,	O
and	O
τ	O
.	O
usually	O
τ	O
may	O
be	O
set	O
to	O
the	O
number	O
of	O
iterations	O
required	O
to	O
make	O
a	O
few	O
hundred	O
passes	O
through	O
the	O
training	O
set	O
.	O
usually	O
τ	O
should	O
be	O
set	O
to	O
roughly	O
0	O
.	O
the	O
main	O
question	O
is	O
how	O
to	O
set	O
0	O
.	O
if	O
it	O
is	O
too	O
large	O
,	O
the	O
learning	O
curve	O
will	O
show	O
violent	O
oscillations	O
,	O
with	O
the	O
cost	O
function	O
often	O
increasing	O
signiﬁcantly	O
.	O
gentle	O
oscillations	O
are	O
ﬁne	O
,	O
especially	O
if	O
training	O
with	O
a	O
stochastic	O
cost	O
function	O
such	O
as	O
the	O
cost	O
function	O
arising	O
from	O
the	O
use	O
of	O
dropout	O
.	O
if	O
the	O
learning	O
rate	O
is	O
too	O
low	O
,	O
learning	O
proceeds	O
slowly	O
,	O
and	O
if	O
the	O
initial	O
learning	O
rate	O
is	O
too	O
low	O
,	O
learning	O
may	O
become	O
stuck	O
with	O
a	O
high	O
cost	O
value	O
.	O
typically	O
,	O
the	O
optimal	O
initial	O
learning	O
rate	O
,	O
in	O
terms	O
of	O
total	O
training	O
time	O
and	O
the	O
ﬁnal	O
cost	O
value	O
,	O
is	O
higher	O
than	O
the	O
learning	O
rate	O
that	O
yields	O
the	O
best	O
performance	O
after	O
the	O
ﬁrst	O
100	O
iterations	O
or	O
so	O
.	O
therefore	O
,	O
it	O
is	O
usually	O
best	O
to	O
monitor	O
the	O
ﬁrst	O
several	O
iterations	O
and	O
use	O
a	O
learning	O
rate	O
that	O
is	O
higher	O
than	O
the	O
best-performing	O
learning	O
rate	O
at	O
this	O
time	O
,	O
but	O
not	O
so	O
high	O
that	O
it	O
causes	O
severe	O
instability	O
.	O
the	O
most	O
important	O
property	O
of	O
sgd	O
and	O
related	O
minibatch	O
or	O
online	O
gradient-	O
based	O
optimization	O
is	O
that	O
computation	O
time	O
per	O
update	O
does	O
not	O
grow	O
with	O
the	O
number	O
of	O
training	O
examples	O
.	O
this	O
allows	O
convergence	O
even	O
when	O
the	O
number	O
of	O
training	O
examples	O
becomes	O
very	O
large	O
.	O
for	O
a	O
large	O
enough	O
dataset	O
,	O
sgd	O
may	O
converge	O
to	O
within	O
some	O
ﬁxed	O
tolerance	O
of	O
its	O
ﬁnal	O
test	O
set	O
error	O
before	O
it	O
has	O
processed	O
the	O
entire	O
training	O
set	O
.	O
−	O
to	O
study	O
the	O
convergence	O
rate	O
of	O
an	O
optimization	O
algorithm	O
it	O
is	O
common	O
to	O
measure	O
the	O
excess	O
error	O
j	O
(	O
θ	O
)	O
minθ	O
j	O
(	O
θ	O
)	O
,	O
which	O
is	O
the	O
amount	O
that	O
the	O
current	O
cost	O
function	O
exceeds	O
the	O
minimum	O
possible	O
cost	O
.	O
when	O
sgd	O
is	O
applied	O
to	O
a	O
convex	O
problem	O
,	O
the	O
excess	O
error	O
is	O
o	O
(	O
1√	O
)	O
after	O
k	O
iterations	O
,	O
while	O
in	O
the	O
strongly	O
convex	O
case	O
it	O
is	O
o	O
(	O
1	O
)	O
.	O
these	O
bounds	O
can	O
not	O
be	O
improved	O
unless	O
extra	O
conditions	B
are	O
k	O
assumed	O
.	O
batch	O
gradient	O
descent	B
enjoys	O
better	O
convergence	O
rates	O
than	O
stochastic	O
,	O
gradient	O
descent	B
in	O
theory	O
.	O
however	O
,	O
the	O
cramér-rao	O
bound	B
(	O
cramér	O
1946	O
rao	O
,	O
1945	O
)	O
states	O
that	O
generalization	O
error	O
can	O
not	O
decrease	O
faster	O
than	O
o	O
(	O
1	O
)	O
.	O
bottou	O
k	O
k	O
;	O
295	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
(	O
)	O
argue	O
that	O
it	O
therefore	O
may	O
not	O
be	O
worthwhile	O
to	O
pursue	O
and	O
bousquet	O
2008	O
an	O
optimization	O
algorithm	O
that	O
converges	O
faster	O
than	O
o	O
(	O
1	O
)	O
for	O
machine	O
learning	O
k	O
tasks—faster	O
convergence	O
presumably	O
corresponds	O
to	O
overﬁtting	O
.	O
moreover	O
,	O
the	O
asymptotic	O
analysis	O
obscures	O
many	O
advantages	O
that	O
stochastic	O
gradient	O
descent	B
has	O
after	O
a	O
small	O
number	O
of	O
steps	O
.	O
with	O
large	O
datasets	O
,	O
the	O
ability	O
of	O
sgd	O
to	O
make	O
rapid	O
initial	O
progress	O
while	O
evaluating	O
the	O
gradient	O
for	O
only	O
very	O
few	O
examples	O
outweighs	O
its	O
slow	O
asymptotic	O
convergence	O
.	O
most	O
of	O
the	O
algorithms	O
described	O
in	O
the	O
remainder	O
of	O
this	O
chapter	O
achieve	O
beneﬁts	O
that	O
matter	O
in	O
practice	O
but	O
are	O
lost	O
in	O
the	O
constant	O
factors	O
obscured	O
by	O
the	O
o	O
(	O
1	O
k	O
)	O
asymptotic	O
analysis	O
.	O
one	O
can	O
also	O
trade	O
oﬀ	O
the	O
beneﬁts	O
of	O
both	O
batch	O
and	O
stochastic	O
gradient	O
descent	B
by	O
gradually	O
increasing	O
the	O
minibatch	O
size	O
during	O
the	O
course	O
of	O
learning	O
.	O
for	O
more	O
information	O
on	O
sgd	O
,	O
see	O
8.3.2	O
momentum	O
bottou	O
1998	O
(	O
)	O
.	O
while	O
stochastic	O
gradient	O
descent	B
remains	O
a	O
very	O
popular	O
optimization	O
strategy	O
,	O
learning	O
with	O
it	O
can	O
sometimes	O
be	O
slow	O
.	O
the	O
method	O
of	O
momentum	O
(	O
polyak	O
1964	O
)	O
is	O
designed	O
to	O
accelerate	O
learning	O
,	O
especially	O
in	O
the	O
face	O
of	O
high	O
curvature	O
,	O
small	O
but	O
consistent	O
gradients	O
,	O
or	O
noisy	O
gradients	O
.	O
the	O
momentum	O
algorithm	O
accumulates	O
an	O
exponentially	O
decaying	O
moving	O
average	O
of	O
past	O
gradients	O
and	O
continues	O
to	O
move	O
in	O
their	O
direction	O
.	O
the	O
eﬀect	O
of	O
momentum	O
is	O
illustrated	O
in	O
ﬁgure	O
.8.5	O
,	O
formally	O
,	O
the	O
momentum	O
algorithm	O
introduces	O
a	O
variable	O
v	O
that	O
plays	O
the	O
role	O
of	O
velocity—it	O
is	O
the	O
direction	O
and	O
speed	O
at	O
which	O
the	O
parameters	O
move	O
through	O
parameter	O
space	O
.	O
the	O
velocity	O
is	O
set	O
to	O
an	O
exponentially	O
decaying	O
average	O
of	O
the	O
negative	O
gradient	O
.	O
the	O
name	O
momentum	O
derives	O
from	O
a	O
physical	O
analogy	O
,	O
in	O
which	O
the	O
negative	O
gradient	O
is	O
a	O
force	O
moving	O
a	O
particle	O
through	O
parameter	O
space	O
,	O
according	O
to	O
newton	O
’	O
s	O
laws	O
of	O
motion	O
.	O
momentum	O
in	O
physics	O
is	O
mass	O
times	O
velocity	O
.	O
in	O
the	O
momentum	O
learning	O
algorithm	O
,	O
we	O
assume	O
unit	O
mass	O
,	O
so	O
the	O
velocity	O
vector	O
v	O
may	O
also	O
be	O
regarded	O
as	O
the	O
momentum	O
of	O
the	O
particle	O
.	O
a	O
hyperparameter	O
α	O
[	O
0	O
,	O
1	O
)	O
determines	O
how	O
quickly	O
the	O
contributions	O
of	O
previous	O
gradients	O
exponentially	O
decay	O
.	O
the	O
update	O
rule	O
is	O
given	O
by	O
:	O
	O
	O
	O
∈	O
←	O
←	O
v	O
θ	O
−	O
∇	O
	O
θ	O
α	O
v	O
θ	O
v	O
+	O
.	O
1	O
m	O
m	O
i=1	O
l	O
(	O
(	O
f	O
x	O
(	O
)	O
i	O
;	O
)	O
θ	O
,	O
y	O
(	O
)	O
i	O
)	O
	O
	O
,	O
(	O
8.15	O
)	O
	O
(	O
8.16	O
)	O
m	O
i=1	O
l	O
(	O
(	O
f	O
x	O
(	O
)	O
i	O
;	O
)	O
θ	O
,	O
y	O
(	O
)	O
i	O
)	O
the	O
velocity	O
v	O
accumulates	O
the	O
gradient	O
elements	O
.	O
the	O
larger	O
α	O
is	O
relative	O
to	O
	O
,	O
the	O
more	O
previous	O
gradients	O
aﬀect	O
the	O
current	O
direction	O
.	O
the	O
sgd	O
algorithm	O
with	O
momentum	O
is	O
given	O
in	O
algorithm	O
.8.2	O
1	O
m	O
θ	O
∇	O
296	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
20	O
10	O
0	O
−	O
10	O
−	O
20	O
−	O
−	O
30	O
30	O
−	O
20	O
−	O
10	O
0	O
10	O
20	O
figure	O
8.5	O
:	O
momentum	O
aims	O
primarily	O
to	O
solve	O
two	O
problems	O
:	O
poor	O
conditioning	O
of	O
the	O
hessian	O
matrix	O
and	O
variance	O
in	O
the	O
stochastic	O
gradient	O
.	O
here	O
,	O
we	O
illustrate	O
how	O
momentum	O
overcomes	O
the	O
ﬁrst	O
of	O
these	O
two	O
problems	O
.	O
the	O
contour	O
lines	O
depict	O
a	O
quadratic	O
loss	O
function	O
with	O
a	O
poorly	O
conditioned	O
hessian	O
matrix	O
.	O
the	O
red	O
path	O
cutting	O
across	O
the	O
contours	O
indicates	O
the	O
path	O
followed	O
by	O
the	O
momentum	O
learning	O
rule	O
as	O
it	O
minimizes	O
this	O
function	O
.	O
at	O
each	O
step	O
along	O
the	O
way	O
,	O
we	O
draw	O
an	O
arrow	O
indicating	O
the	O
step	O
that	O
gradient	O
descent	B
would	O
take	O
at	O
that	O
point	O
.	O
we	O
can	O
see	O
that	O
a	O
poorly	O
conditioned	O
quadratic	O
objective	O
looks	O
like	O
a	O
long	O
,	O
narrow	O
valley	O
or	O
canyon	O
with	O
steep	O
sides	O
.	O
momentum	O
correctly	O
traverses	O
the	O
canyon	O
lengthwise	O
,	O
while	O
gradient	O
steps	O
waste	O
time	O
moving	O
back	O
and	O
forth	O
across	O
the	O
narrow	O
axis	O
of	O
the	O
canyon	O
.	O
compare	O
also	O
ﬁgure	O
,	O
which	O
shows	O
the	O
behavior	O
of	O
gradient	O
descent	B
without	O
momentum	O
.	O
4.6	O
297	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
previously	O
,	O
the	O
size	O
of	O
the	O
step	O
was	O
simply	O
the	O
norm	O
of	O
the	O
gradient	O
multiplied	O
by	O
the	O
learning	O
rate	O
.	O
now	O
,	O
the	O
size	O
of	O
the	O
step	O
depends	O
on	O
how	O
large	O
and	O
how	O
aligned	O
a	O
sequence	O
of	O
gradients	O
are	O
.	O
the	O
step	O
size	O
is	O
largest	O
when	O
many	O
successive	O
−	O
gradients	O
point	O
in	O
exactly	O
the	O
same	O
direction	O
.	O
if	O
the	O
momentum	O
algorithm	O
always	O
observes	O
gradient	O
g	O
,	O
then	O
it	O
will	O
accelerate	O
in	O
the	O
direction	O
of	O
g	O
,	O
until	O
reaching	O
a	O
terminal	O
velocity	O
where	O
the	O
size	O
of	O
each	O
step	O
is	O
||	O
||	O
−	O
g	O
	O
1	O
.	O
α	O
(	O
8.17	O
)	O
it	O
is	O
thus	O
helpful	O
to	O
think	O
of	O
the	O
momentum	O
hyperparameter	O
in	O
terms	O
of	O
example	O
,	O
α	O
=	O
.9	O
corresponds	O
to	O
multiplying	O
the	O
maximum	O
speed	O
by	O
10	O
the	O
gradient	O
descent	B
algorithm	O
.	O
−	O
1	O
α	O
1	O
.	O
for	O
relative	O
to	O
common	O
values	O
of	O
α	O
used	O
in	O
practice	O
include	O
.5	O
,	O
.9	O
,	O
and	O
.99.	O
like	O
the	O
learning	O
rate	O
,	O
α	O
may	O
also	O
be	O
adapted	O
over	O
time	O
.	O
typically	O
it	O
begins	O
with	O
a	O
small	O
value	O
and	O
is	O
later	O
raised	O
.	O
it	O
is	O
less	O
important	O
to	O
adapt	O
α	O
over	O
time	O
than	O
to	O
shrink	O
	O
over	O
time	O
.	O
algorithm	O
8.2	O
stochastic	O
gradient	O
descent	B
(	O
sgd	O
)	O
with	O
momentum	O
require	O
:	O
learning	O
rate	O
,	O
momentum	O
parameter	O
require	O
:	O
initial	O
parameter	O
,	O
initial	O
velocity	O
.	O
v	O
θ	O
	O
.	O
α	O
while	O
stopping	O
criterion	O
not	O
met	O
do	O
	O
{	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
m	O
}	O
with	O
sample	O
a	O
minibatch	O
of	O
m	O
examples	O
from	O
the	O
training	O
set	O
←	O
corresponding	O
targets	O
y	O
(	O
)	O
i	O
.	O
←	O
compute	O
gradient	O
estimate	O
:	O
g	O
compute	O
velocity	O
update	O
:	O
v	O
apply	O
update	O
:	O
θ	O
∇	O
−	O
1	O
m	O
v	O
←	O
θ	O
g	O
	O
α	O
θ	O
+	O
v	O
i	O
l	O
f	O
(	O
(	O
x	O
(	O
)	O
i	O
;	O
)	O
θ	O
,	O
y	O
(	O
)	O
i	O
)	O
end	O
while	O
we	O
can	O
view	O
the	O
momentum	O
algorithm	O
as	O
simulating	O
a	O
particle	O
subject	O
to	O
continuous-time	O
newtonian	O
dynamics	O
.	O
the	O
physical	O
analogy	O
can	O
help	O
to	O
build	O
intuition	O
for	O
how	O
the	O
momentum	O
and	O
gradient	O
descent	B
algorithms	O
behave	O
.	O
the	O
position	B
of	O
the	O
particle	O
at	O
any	O
point	O
in	O
time	O
is	O
given	O
by	O
θ	O
(	O
t	O
)	O
.	O
the	O
particle	O
experiences	O
net	O
force	O
f	O
(	O
)	O
t	O
.	O
this	O
force	O
causes	O
the	O
particle	O
to	O
accelerate	O
:	O
f	O
(	O
)	O
=t	O
∂	O
2	O
∂t2	O
θ	O
(	O
)	O
t	O
.	O
(	O
8.18	O
)	O
rather	O
than	O
viewing	O
this	O
as	O
a	O
second-order	O
diﬀerential	O
equation	O
of	O
the	O
position	B
,	O
we	O
can	O
introduce	O
the	O
variable	O
v	O
(	O
t	O
)	O
representing	O
the	O
velocity	O
of	O
the	O
particle	O
at	O
time	O
t	O
and	O
rewrite	O
the	O
newtonian	O
dynamics	O
as	O
a	O
ﬁrst-order	O
diﬀerential	O
equation	O
:	O
v	O
(	O
)	O
=t	O
∂	O
∂t	O
298	O
θ	O
(	O
)	O
t	O
,	O
(	O
8.19	O
)	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
f	O
(	O
)	O
=t	O
∂	O
∂t	O
v	O
(	O
)	O
t	O
.	O
(	O
8.20	O
)	O
the	O
momentum	O
algorithm	O
then	O
consists	O
of	O
solving	O
the	O
diﬀerential	O
equations	O
via	O
numerical	O
simulation	O
.	O
a	O
simple	O
numerical	O
method	O
for	O
solving	O
diﬀerential	O
equations	O
is	O
euler	O
’	O
s	O
method	O
,	O
which	O
simply	O
consists	O
of	O
simulating	O
the	O
dynamics	O
deﬁned	O
by	O
the	O
equation	O
by	O
taking	O
small	O
,	O
ﬁnite	O
steps	O
in	O
the	O
direction	O
of	O
each	O
gradient	O
.	O
this	O
explains	O
the	O
basic	O
form	O
of	O
the	O
momentum	O
update	O
,	O
but	O
what	O
speciﬁcally	O
are	O
−∇	O
the	O
forces	O
?	O
one	O
force	O
is	O
proportional	O
to	O
the	O
negative	O
gradient	O
of	O
the	O
cost	O
function	O
:	O
θ	O
j	O
(	O
θ	O
)	O
.	O
this	O
force	O
pushes	O
the	O
particle	O
downhill	O
along	O
the	O
cost	O
function	O
surface	O
.	O
the	O
gradient	O
descent	B
algorithm	O
would	O
simply	O
take	O
a	O
single	O
step	O
based	O
on	O
each	O
gradient	O
,	O
but	O
the	O
newtonian	O
scenario	O
used	O
by	O
the	O
momentum	O
algorithm	O
instead	O
uses	O
this	O
force	O
to	O
alter	O
the	O
velocity	O
of	O
the	O
particle	O
.	O
we	O
can	O
think	O
of	O
the	O
particle	O
as	O
being	O
like	O
a	O
hockey	O
puck	O
sliding	O
down	O
an	O
icy	O
surface	O
.	O
whenever	O
it	O
descends	O
a	O
steep	O
part	O
of	O
the	O
surface	O
,	O
it	O
gathers	O
speed	O
and	O
continues	O
sliding	O
in	O
that	O
direction	O
until	O
it	O
begins	O
to	O
go	O
uphill	O
again	O
.	O
one	O
other	O
force	O
is	O
necessary	O
.	O
if	O
the	O
only	O
force	O
is	O
the	O
gradient	O
of	O
the	O
cost	O
function	O
,	O
then	O
the	O
particle	O
might	O
never	O
come	O
to	O
rest	O
.	O
imagine	O
a	O
hockey	O
puck	O
sliding	O
down	O
one	O
side	O
of	O
a	O
valley	O
and	O
straight	O
up	O
the	O
other	O
side	O
,	O
oscillating	O
back	O
and	O
forth	O
forever	O
,	O
−	O
assuming	O
the	O
ice	O
is	O
perfectly	O
frictionless	O
.	O
to	O
resolve	O
this	O
problem	O
,	O
we	O
add	O
one	O
v	O
(	O
t	O
)	O
.	O
in	O
physics	O
terminology	O
,	O
this	O
force	O
corresponds	O
other	O
force	O
,	O
proportional	O
to	O
to	O
viscous	O
drag	O
,	O
as	O
if	O
the	O
particle	O
must	O
push	O
through	O
a	O
resistant	O
medium	O
such	O
as	O
syrup	O
.	O
this	O
causes	O
the	O
particle	O
to	O
gradually	O
lose	O
energy	O
over	O
time	O
and	O
eventually	O
converge	O
to	O
a	O
local	O
minimum	O
.	O
−	O
−	O
v	O
(	O
t	O
)	O
and	O
viscous	O
drag	O
in	O
particular	O
?	O
part	O
of	O
the	O
reason	O
to	O
why	O
do	O
we	O
use	O
v	O
(	O
t	O
)	O
is	O
mathematical	O
convenience—an	O
integer	O
power	O
of	O
the	O
velocity	O
is	O
easy	O
use	O
to	O
work	B
with	O
.	O
however	O
,	O
other	O
physical	O
systems	O
have	O
other	O
kinds	O
of	O
drag	O
based	O
on	O
other	O
integer	O
powers	O
of	O
the	O
velocity	O
.	O
for	O
example	O
,	O
a	O
particle	O
traveling	O
through	O
the	O
air	O
experiences	O
turbulent	O
drag	O
,	O
with	O
force	O
proportional	O
to	O
the	O
square	O
of	O
the	O
velocity	O
,	O
while	O
a	O
particle	O
moving	O
along	O
the	O
ground	O
experiences	O
dry	O
friction	O
,	O
with	O
a	O
force	O
of	O
constant	O
magnitude	O
.	O
we	O
can	O
reject	O
each	O
of	O
these	O
options	O
.	O
turbulent	O
drag	O
,	O
proportional	O
to	O
the	O
square	O
of	O
the	O
velocity	O
,	O
becomes	O
very	O
weak	O
when	O
the	O
velocity	O
is	O
small	O
.	O
it	O
is	O
not	O
powerful	O
enough	O
to	O
force	O
the	O
particle	O
to	O
come	O
to	O
rest	O
.	O
a	O
particle	O
with	O
a	O
non-zero	O
initial	O
velocity	O
that	O
experiences	O
only	O
the	O
force	O
of	O
turbulent	O
drag	O
will	O
move	O
away	O
from	O
its	O
initial	O
position	B
forever	O
,	O
with	O
the	O
distance	O
from	O
the	O
starting	O
point	O
growing	O
like	O
o	O
(	O
log	O
t	O
)	O
.	O
we	O
must	O
therefore	O
use	O
a	O
lower	O
power	O
of	O
the	O
velocity	O
.	O
if	O
we	O
use	O
a	O
power	O
of	O
zero	O
,	O
representing	O
dry	O
friction	O
,	O
then	O
the	O
force	O
is	O
too	O
strong	O
.	O
when	O
the	O
force	O
due	O
to	O
the	O
gradient	O
of	O
the	O
cost	O
function	O
is	O
small	O
but	O
non-zero	O
,	O
the	O
constant	O
force	O
due	O
to	O
friction	O
can	O
cause	O
the	O
particle	O
to	O
come	O
to	O
rest	O
before	O
reaching	O
a	O
local	O
minimum	O
.	O
viscous	O
drag	O
avoids	O
both	O
of	O
these	O
problems—it	O
is	O
weak	O
enough	O
299	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
that	O
the	O
gradient	O
can	O
continue	O
to	O
cause	O
motion	O
until	O
a	O
minimum	O
is	O
reached	O
,	O
but	O
strong	O
enough	O
to	O
prevent	O
motion	O
if	O
the	O
gradient	O
does	O
not	O
justify	O
moving	O
.	O
8.3.3	O
nesterov	O
momentum	O
	O
,	O
	O
	O
	O
2013	O
et	O
al	O
.	O
(	O
sutskever	O
inspired	O
by	O
nesterov	O
’	O
s	O
accelerated	O
gradient	O
method	O
(	O
update	O
rules	O
in	O
this	O
case	O
are	O
given	O
by	O
:	O
)	O
introduced	O
a	O
variant	O
of	O
the	O
momentum	O
algorithm	O
that	O
was	O
)	O
.	O
the	O
nesterov	O
1983	O
2004	O
,	O
←	O
←	O
v	O
θ	O
−	O
∇	O
	O
θ	O
α	O
v	O
θ	O
+	O
,	O
v	O
1	O
m	O
m	O
i=1	O
l	O
f	O
x	O
(	O
(	O
)	O
i	O
;	O
+	O
)	O
θ	O
αv	O
,	O
y	O
(	O
)	O
i	O
,	O
(	O
8.21	O
)	O
(	O
8.22	O
)	O
where	O
the	O
parameters	O
α	O
and	O
	O
play	O
a	O
similar	O
role	O
as	O
in	O
the	O
standard	O
momentum	O
method	O
.	O
the	O
diﬀerence	O
between	O
nesterov	O
momentum	O
and	O
standard	O
momentum	O
is	O
where	O
the	O
gradient	O
is	O
evaluated	O
.	O
with	O
nesterov	O
momentum	O
the	O
gradient	O
is	O
evaluated	O
after	O
the	O
current	O
velocity	O
is	O
applied	O
.	O
thus	O
one	O
can	O
interpret	O
nesterov	O
momentum	O
as	O
attempting	O
to	O
add	O
a	O
correction	O
factor	O
to	O
the	O
standard	O
method	O
of	O
momentum	O
.	O
the	O
complete	O
nesterov	O
momentum	O
algorithm	O
is	O
presented	O
in	O
algorithm	O
.8.3	O
in	O
the	O
convex	O
batch	O
gradient	O
case	O
,	O
nesterov	O
momentum	O
brings	O
the	O
rate	O
of	O
convergence	O
of	O
the	O
excess	O
error	O
from	O
o	O
(	O
1/k	O
)	O
(	O
after	O
k	O
steps	O
)	O
to	O
o	O
(	O
1/k2	O
)	O
as	O
shown	O
by	O
nesterov	O
1983	O
)	O
.	O
unfortunately	O
,	O
in	O
the	O
stochastic	O
gradient	O
case	O
,	O
nesterov	O
momentum	O
does	O
not	O
improve	O
the	O
rate	O
of	O
convergence	O
.	O
(	O
algorithm	O
8.3	O
stochastic	O
gradient	O
descent	B
(	O
sgd	O
)	O
with	O
nesterov	O
momentum	O
require	O
:	O
learning	O
rate	O
,	O
momentum	O
parameter	O
require	O
:	O
initial	O
parameter	O
,	O
initial	O
velocity	O
.	O
v	O
θ	O
	O
.	O
α	O
while	O
stopping	O
criterion	O
not	O
met	O
do	O
sample	O
a	O
minibatch	O
of	O
m	O
examples	O
from	O
the	O
training	O
set	O
corresponding	O
labels	O
y	O
(	O
)	O
i	O
.	O
apply	O
interim	O
update	O
:	O
˜θ	O
compute	O
gradient	O
(	O
at	O
interim	O
point	O
)	O
:	O
g	O
compute	O
velocity	O
update	O
:	O
v	O
g	O
	O
apply	O
update	O
:	O
θ	O
θ	O
+	O
α	O
←	O
∇	O
˜θ	O
v	O
−	O
←	O
←	O
←	O
1	O
m	O
α	O
v	O
θ	O
+	O
v	O
	O
{	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
m	O
}	O
with	O
i	O
l	O
f	O
(	O
(	O
x	O
(	O
)	O
i	O
;	O
˜θ	O
y	O
)	O
,	O
(	O
)	O
i	O
)	O
end	O
while	O
300	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
8.4	O
parameter	O
initialization	O
strategies	O
some	O
optimization	O
algorithms	O
are	O
not	O
iterative	O
by	O
nature	O
and	O
simply	O
solve	O
for	O
a	O
solution	O
point	O
.	O
other	O
optimization	O
algorithms	O
are	O
iterative	O
by	O
nature	O
but	O
,	O
when	O
applied	O
to	O
the	O
right	O
class	O
of	O
optimization	O
problems	O
,	O
converge	O
to	O
acceptable	O
solutions	O
in	O
an	O
acceptable	O
amount	O
of	O
time	O
regardless	O
of	O
initialization	O
.	O
deep	O
learning	O
training	O
algorithms	O
usually	O
do	O
not	O
have	O
either	O
of	O
these	O
luxuries	O
.	O
training	O
algorithms	O
for	O
deep	O
learning	O
models	O
are	O
usually	O
iterative	O
in	O
nature	O
and	O
thus	O
require	O
the	O
user	O
to	O
specify	O
some	O
initial	O
point	O
from	O
which	O
to	O
begin	O
the	O
iterations	O
.	O
moreover	O
,	O
training	O
deep	O
models	O
is	O
a	O
suﬃciently	O
diﬃcult	O
task	O
that	O
most	O
algorithms	O
are	O
strongly	O
aﬀected	O
by	O
the	O
choice	O
of	O
initialization	O
.	O
the	O
initial	O
point	O
can	O
determine	O
whether	O
the	O
algorithm	O
converges	O
at	O
all	O
,	O
with	O
some	O
initial	O
points	O
being	O
so	O
unstable	O
that	O
the	O
algorithm	O
encounters	O
numerical	O
diﬃculties	O
and	O
fails	O
altogether	O
.	O
when	O
learning	O
does	O
converge	O
,	O
the	O
initial	O
point	O
can	O
determine	O
how	O
quickly	O
learning	O
converges	O
and	O
whether	O
it	O
converges	O
to	O
a	O
point	O
with	O
high	O
or	O
low	O
cost	O
.	O
also	O
,	O
points	O
of	O
comparable	O
cost	O
can	O
have	O
wildly	O
varying	O
generalization	O
error	O
,	O
and	O
the	O
initial	O
point	O
can	O
aﬀect	O
the	O
generalization	O
as	O
well	O
.	O
modern	O
initialization	O
strategies	O
are	O
simple	O
and	O
heuristic	O
.	O
designing	O
improved	O
initialization	O
strategies	O
is	O
a	O
diﬃcult	O
task	O
because	O
neural	O
network	O
optimization	O
is	O
not	O
yet	O
well	O
understood	O
.	O
most	O
initialization	O
strategies	O
are	O
based	O
on	O
achieving	O
some	O
nice	O
properties	O
when	O
the	O
network	O
is	O
initialized	O
.	O
however	O
,	O
we	O
do	O
not	O
have	O
a	O
good	O
understanding	O
of	O
which	O
of	O
these	O
properties	O
are	O
preserved	O
under	O
which	O
circumstances	O
after	O
learning	O
begins	O
to	O
proceed	O
.	O
a	O
further	O
diﬃculty	O
is	O
that	O
some	O
initial	O
points	O
may	O
be	O
beneﬁcial	O
from	O
the	O
viewpoint	O
of	O
optimization	O
but	O
detrimental	O
from	O
the	O
viewpoint	O
of	O
generalization	O
.	O
our	O
understanding	O
of	O
how	O
the	O
initial	O
point	O
aﬀects	O
generalization	O
is	O
especially	O
primitive	O
,	O
oﬀering	O
little	O
to	O
no	O
guidance	O
for	O
how	O
to	O
select	O
the	O
initial	O
point	O
.	O
perhaps	O
the	O
only	O
property	O
known	O
with	O
complete	O
certainty	O
is	O
that	O
the	O
initial	O
parameters	O
need	O
to	O
“	O
break	O
symmetry	O
”	O
between	O
diﬀerent	O
units	O
.	O
if	O
two	O
hidden	O
units	O
with	O
the	O
same	O
activation	O
function	O
are	O
connected	O
to	O
the	O
same	O
inputs	O
,	O
then	O
these	O
units	O
must	O
have	O
diﬀerent	O
initial	O
parameters	O
.	O
if	O
they	O
have	O
the	O
same	O
initial	O
parameters	O
,	O
then	O
a	O
deterministic	O
learning	O
algorithm	O
applied	O
to	O
a	O
deterministic	O
cost	O
and	O
model	B
will	O
constantly	O
update	O
both	O
of	O
these	O
units	O
in	O
the	O
same	O
way	O
.	O
even	O
if	O
the	O
model	B
or	O
training	O
algorithm	O
is	O
capable	O
of	O
using	O
stochasticity	O
to	O
compute	O
diﬀerent	O
updates	O
for	O
diﬀerent	O
units	O
(	O
for	O
example	O
,	O
if	O
one	O
trains	O
with	O
dropout	O
)	O
,	O
it	O
is	O
usually	O
best	O
to	O
initialize	O
each	O
unit	O
to	O
compute	O
a	O
diﬀerent	O
function	O
from	O
all	O
of	O
the	O
other	O
units	O
.	O
this	O
may	O
help	O
to	O
make	O
sure	O
that	O
no	O
input	O
patterns	O
are	O
lost	O
in	O
the	O
null	O
space	O
of	O
forward	O
propagation	O
and	O
no	O
gradient	O
patterns	O
are	O
lost	O
in	O
the	O
null	O
space	O
of	O
back-propagation	O
.	O
the	O
goal	O
of	O
having	O
each	O
unit	O
compute	O
a	O
diﬀerent	O
function	O
301	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
motivates	O
random	O
initialization	O
of	O
the	O
parameters	O
.	O
we	O
could	O
explicitly	O
search	O
for	O
a	O
large	O
set	O
of	O
basis	O
functions	O
that	O
are	O
all	O
mutually	O
diﬀerent	O
from	O
each	O
other	O
,	O
but	O
this	O
often	O
incurs	O
a	O
noticeable	O
computational	O
cost	O
.	O
for	O
example	O
,	O
if	O
we	O
have	O
at	O
most	O
as	O
many	O
outputs	O
as	O
inputs	O
,	O
we	O
could	O
use	O
gram-schmidt	O
orthogonalization	O
on	O
an	O
initial	O
weight	O
matrix	O
,	O
and	O
be	O
guaranteed	O
that	O
each	O
unit	O
computes	O
a	O
very	O
diﬀerent	O
function	O
from	O
each	O
other	O
unit	O
.	O
random	O
initialization	O
from	O
a	O
high-entropy	O
distribution	O
over	O
a	O
high-dimensional	O
space	O
is	O
computationally	O
cheaper	O
and	O
unlikely	O
to	O
assign	O
any	O
units	O
to	O
compute	O
the	O
same	O
function	O
as	O
each	O
other	O
.	O
typically	O
,	O
we	O
set	O
the	O
biases	O
for	O
each	O
unit	O
to	O
heuristically	O
chosen	O
constants	O
,	O
and	O
initialize	O
only	O
the	O
weights	O
randomly	O
.	O
extra	O
parameters	O
,	O
for	O
example	O
,	O
parameters	O
encoding	O
the	O
conditional	O
variance	O
of	O
a	O
prediction	O
,	O
are	O
usually	O
set	O
to	O
heuristically	O
chosen	O
constants	O
much	O
like	O
the	O
biases	O
are	O
.	O
we	O
almost	O
always	O
initialize	O
all	O
the	O
weights	O
in	O
the	O
model	B
to	O
values	O
drawn	O
randomly	O
from	O
a	O
gaussian	O
or	O
uniform	O
distribution	O
.	O
the	O
choice	O
of	O
gaussian	O
or	O
uniform	O
distribution	O
does	O
not	O
seem	O
to	O
matter	O
very	O
much	O
,	O
but	O
has	O
not	O
been	O
exhaustively	O
studied	O
.	O
the	O
scale	O
of	O
the	O
initial	O
distribution	O
,	O
however	O
,	O
does	O
have	O
a	O
large	O
eﬀect	O
on	O
both	O
the	O
outcome	O
of	O
the	O
optimization	O
procedure	O
and	O
on	O
the	O
ability	O
of	O
the	O
network	O
to	O
generalize	O
.	O
larger	O
initial	O
weights	O
will	O
yield	O
a	O
stronger	O
symmetry	O
breaking	O
eﬀect	O
,	O
helping	O
to	O
avoid	O
redundant	O
units	O
.	O
they	O
also	O
help	O
to	O
avoid	O
losing	O
signal	O
during	O
forward	O
or	O
back-propagation	O
through	O
the	O
linear	O
component	O
of	O
each	O
layer—larger	O
values	O
in	O
the	O
matrix	O
result	O
in	O
larger	O
outputs	O
of	O
matrix	O
multiplication	O
.	O
initial	O
weights	O
that	O
are	O
too	O
large	O
may	O
,	O
however	O
,	O
result	O
in	O
exploding	O
values	O
during	O
forward	O
propagation	O
or	O
back-propagation	O
.	O
in	O
recurrent	O
networks	O
,	O
large	O
weights	O
can	O
also	O
result	O
in	O
chaos	O
(	O
such	O
extreme	O
sensitivity	O
to	O
small	O
perturbations	O
of	O
the	O
input	O
that	O
the	O
behavior	O
of	O
the	O
deterministic	O
forward	O
propagation	O
procedure	O
appears	O
random	O
)	O
.	O
to	O
some	O
extent	O
,	O
the	O
exploding	O
gradient	O
problem	O
can	O
be	O
mitigated	O
by	O
gradient	O
clipping	O
(	O
thresholding	O
the	O
values	O
of	O
the	O
gradients	O
before	O
performing	O
a	O
gradient	O
descent	B
step	O
)	O
.	O
large	O
weights	O
may	O
also	O
result	O
in	O
extreme	O
values	O
that	O
cause	O
the	O
activation	O
function	O
to	O
saturate	O
,	O
causing	O
complete	O
loss	O
of	O
gradient	O
through	O
saturated	O
units	O
.	O
these	O
competing	O
factors	O
determine	O
the	O
ideal	O
initial	O
scale	O
of	O
the	O
weights	O
.	O
the	O
perspectives	O
of	O
regularization	O
and	O
optimization	O
can	O
give	O
very	O
diﬀerent	O
insights	O
into	O
how	O
we	O
should	O
initialize	O
a	O
network	O
.	O
the	O
optimization	O
perspective	O
suggests	O
that	O
the	O
weights	O
should	O
be	O
large	O
enough	O
to	O
propagate	O
information	O
success-	O
fully	O
,	O
but	O
some	O
regularization	O
concerns	O
encourage	O
making	O
them	O
smaller	O
.	O
the	O
use	O
of	O
an	O
optimization	O
algorithm	O
such	O
as	O
stochastic	O
gradient	O
descent	B
that	O
makes	O
small	O
incremental	O
changes	O
to	O
the	O
weights	O
and	O
tends	O
to	O
halt	O
in	O
areas	O
that	O
are	O
nearer	O
to	O
the	O
initial	O
parameters	O
(	O
whether	O
due	O
to	O
getting	O
stuck	O
in	O
a	O
region	O
of	O
low	O
gradient	O
,	O
or	O
302	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
7.8	O
due	O
to	O
triggering	O
some	O
early	O
stopping	O
criterion	O
based	O
on	O
overﬁtting	O
)	O
expresses	O
a	O
prior	O
that	O
the	O
ﬁnal	O
parameters	O
should	O
be	O
close	O
to	O
the	O
initial	O
parameters	O
.	O
recall	O
from	O
section	O
that	O
gradient	O
descent	B
with	O
early	O
stopping	O
is	O
equivalent	O
to	O
weight	O
decay	O
for	O
some	O
models	O
.	O
in	O
the	O
general	O
case	O
,	O
gradient	O
descent	B
with	O
early	O
stopping	O
is	O
not	O
the	O
same	O
as	O
weight	O
decay	O
,	O
but	O
does	O
provide	O
a	O
loose	O
analogy	O
for	O
thinking	O
about	O
the	O
eﬀect	O
of	O
initialization	O
.	O
we	O
can	O
think	O
of	O
initializing	O
the	O
parameters	O
θ	O
to	O
θ0	O
as	O
being	O
similar	O
to	O
imposing	O
a	O
gaussian	O
prior	O
p	O
(	O
θ	O
)	O
with	O
mean	O
θ0	O
.	O
from	O
this	O
point	O
of	O
view	O
,	O
it	O
makes	O
sense	O
to	O
choose	O
θ0	O
to	O
be	O
near	O
0.	O
this	O
prior	O
says	O
that	O
it	O
is	O
more	O
likely	O
that	O
units	O
do	O
not	O
interact	O
with	O
each	O
other	O
than	O
that	O
they	O
do	O
interact	O
.	O
units	O
interact	O
only	O
if	O
the	O
likelihood	O
term	O
of	O
the	O
objective	O
function	O
expresses	O
a	O
strong	O
preference	O
for	O
them	O
to	O
interact	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
we	O
initialize	O
θ0	O
to	O
large	O
values	O
,	O
then	O
our	O
prior	O
speciﬁes	O
which	O
units	O
should	O
interact	O
with	O
each	O
other	O
,	O
and	O
how	O
they	O
should	O
interact	O
.	O
some	O
heuristics	O
are	O
available	O
for	O
choosing	O
the	O
initial	O
scale	O
of	O
the	O
weights	O
.	O
one	O
−	O
heuristic	O
is	O
to	O
initialize	O
the	O
weights	O
of	O
a	O
fully	O
connected	O
layer	O
with	O
m	O
inputs	O
and	O
n	O
outputs	O
by	O
sampling	O
each	O
weight	O
from	O
u	O
(	O
)	O
,	O
while	O
glorot	O
and	O
bengio	O
(	O
2010	O
normalized	O
initialization	O
)	O
suggest	O
using	O
the	O
1√	O
m	O
	O
	O
	O
1√	O
,	O
m	O
	O
∼	O
u	O
−	O
wi	O
,	O
j	O
6	O
m	O
n+	O
,	O
6	O
m	O
n+	O
.	O
(	O
8.23	O
)	O
this	O
latter	O
heuristic	O
is	O
designed	O
to	O
compromise	O
between	O
the	O
goal	O
of	O
initializing	O
all	O
layers	O
to	O
have	O
the	O
same	O
activation	O
variance	O
and	O
the	O
goal	O
of	O
initializing	O
all	O
layers	O
to	O
have	O
the	O
same	O
gradient	O
variance	O
.	O
the	O
formula	O
is	O
derived	O
using	O
the	O
assumption	O
that	O
the	O
network	O
consists	O
only	O
of	O
a	O
chain	O
of	O
matrix	O
multiplications	O
,	O
with	O
no	O
nonlinearities	O
.	O
real	O
neural	O
networks	O
obviously	O
violate	O
this	O
assumption	O
,	O
but	O
many	O
strategies	O
designed	O
for	O
the	O
linear	O
model	B
perform	O
reasonably	O
well	O
on	O
its	O
nonlinear	O
counterparts	O
.	O
2013	O
saxe	O
et	O
al	O
.	O
(	O
)	O
recommend	O
initializing	O
to	O
random	O
orthogonal	O
matrices	O
,	O
with	O
a	O
carefully	O
chosen	O
scaling	O
or	O
gain	O
factor	O
g	O
that	O
accounts	O
for	O
the	O
nonlinearity	O
applied	O
at	O
each	O
layer	O
.	O
they	O
derive	O
speciﬁc	O
values	O
of	O
the	O
scaling	O
factor	O
for	O
diﬀerent	O
types	O
of	O
nonlinear	O
activation	O
functions	O
.	O
this	O
initialization	O
scheme	O
is	O
also	O
motivated	O
by	O
a	O
model	B
of	O
a	O
deep	O
network	O
as	O
a	O
sequence	O
of	O
matrix	O
multiplies	O
without	O
nonlinearities	O
.	O
under	O
such	O
a	O
model	B
,	O
this	O
initialization	O
scheme	O
guarantees	O
that	O
the	O
total	O
number	O
of	O
training	O
iterations	O
required	O
to	O
reach	O
convergence	O
is	O
independent	O
of	O
depth	O
.	O
increasing	O
the	O
scaling	O
factor	O
g	O
pushes	O
the	O
network	O
toward	O
the	O
regime	O
where	O
activations	O
increase	O
in	O
norm	O
as	O
they	O
propagate	O
forward	O
through	O
the	O
network	O
and	O
gradients	O
increase	O
in	O
norm	O
as	O
they	O
propagate	O
backward.	O
)	O
showed	O
that	O
setting	O
the	O
gain	O
factor	O
correctly	O
is	O
suﬃcient	O
to	O
train	O
networks	O
as	O
deep	O
as	O
sussillo	O
2014	O
(	O
303	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
1,000	O
layers	O
,	O
without	O
needing	O
to	O
use	O
orthogonal	O
initializations	O
.	O
a	O
key	O
insight	O
of	O
this	O
approach	O
is	O
that	O
in	O
feedforward	O
networks	O
,	O
activations	O
and	O
gradients	O
can	O
grow	O
or	O
shrink	O
on	O
each	O
step	O
of	O
forward	O
or	O
back-propagation	O
,	O
following	O
a	O
random	O
walk	O
behavior	O
.	O
this	O
is	O
because	O
feedforward	O
networks	O
use	O
a	O
diﬀerent	O
weight	O
matrix	O
at	O
each	O
layer	O
.	O
if	O
this	O
random	O
walk	O
is	O
tuned	O
to	O
preserve	O
norms	O
,	O
then	O
feedforward	O
networks	O
can	O
mostly	O
avoid	O
the	O
vanishing	O
and	O
exploding	O
gradients	O
problem	O
that	O
arises	O
when	O
the	O
same	O
weight	O
matrix	O
is	O
used	O
at	O
each	O
step	O
,	O
described	O
in	O
section	O
8.2.5	O
.	O
unfortunately	O
,	O
these	O
optimal	O
criteria	O
for	O
initial	O
weights	O
often	O
do	O
not	O
lead	O
to	O
optimal	O
performance	O
.	O
this	O
may	O
be	O
for	O
three	O
diﬀerent	O
reasons	O
.	O
first	O
,	O
we	O
may	O
be	O
using	O
the	O
wrong	O
criteria—it	O
may	O
not	O
actually	O
be	O
beneﬁcial	O
to	O
preserve	O
the	O
norm	O
of	O
a	O
signal	O
throughout	O
the	O
entire	O
network	O
.	O
second	O
,	O
the	O
properties	O
imposed	O
at	O
initialization	O
may	O
not	O
persist	O
after	O
learning	O
has	O
begun	O
to	O
proceed	O
.	O
third	O
,	O
the	O
criteria	O
might	O
succeed	O
at	O
improving	O
the	O
speed	O
of	O
optimization	O
but	O
inadvertently	O
increase	O
generalization	O
error	O
.	O
in	O
practice	O
,	O
we	O
usually	O
need	O
to	O
treat	O
the	O
scale	O
of	O
the	O
weights	O
as	O
a	O
hyperparameter	O
whose	O
optimal	O
value	O
lies	O
somewhere	O
roughly	O
near	O
but	O
not	O
exactly	O
equal	O
to	O
the	O
theoretical	O
predictions	O
.	O
1√	O
m	O
martens	O
2010	O
one	O
drawback	O
to	O
scaling	O
rules	O
that	O
set	O
all	O
of	O
the	O
initial	O
weights	O
to	O
have	O
the	O
,	O
is	O
that	O
every	O
individual	O
weight	O
becomes	O
same	O
standard	O
deviation	O
,	O
such	O
as	O
)	O
introduced	O
an	O
extremely	O
small	O
when	O
the	O
layers	O
become	O
large	O
.	O
alternative	O
initialization	O
scheme	O
called	O
sparse	O
initialization	O
in	O
which	O
each	O
unit	O
is	O
initialized	O
to	O
have	O
exactly	O
k	O
non-zero	O
weights	O
.	O
the	O
idea	O
is	O
to	O
keep	O
the	O
total	O
amount	O
of	O
input	O
to	O
the	O
unit	O
independent	O
from	O
the	O
number	O
of	O
inputs	O
m	O
without	O
making	O
the	O
magnitude	O
of	O
individual	O
weight	O
elements	O
shrink	O
with	O
m.	O
sparse	O
initialization	O
helps	O
to	O
achieve	O
more	O
diversity	O
among	O
the	O
units	O
at	O
initialization	O
time	O
.	O
however	O
,	O
it	O
also	O
imposes	O
a	O
very	O
strong	O
prior	O
on	O
the	O
weights	O
that	O
are	O
chosen	O
to	O
have	O
large	O
gaussian	O
values	O
.	O
because	O
it	O
takes	O
a	O
long	O
time	O
for	O
gradient	O
descent	B
to	O
shrink	O
“	O
incorrect	O
”	O
large	O
values	O
,	O
this	O
initialization	O
scheme	O
can	O
cause	O
problems	O
for	O
units	O
such	O
as	O
maxout	O
units	O
that	O
have	O
several	O
ﬁlters	O
that	O
must	O
be	O
carefully	O
coordinated	O
with	O
each	O
other	O
.	O
(	O
when	O
computational	O
resources	O
allow	O
it	O
,	O
it	O
is	O
usually	O
a	O
good	O
idea	O
to	O
treat	O
the	O
initial	O
scale	O
of	O
the	O
weights	O
for	O
each	O
layer	O
as	O
a	O
hyperparameter	O
,	O
and	O
to	O
choose	O
these	O
scales	O
using	O
a	O
hyperparameter	O
search	O
algorithm	O
described	O
in	O
section	O
,	O
such	O
as	O
random	O
search	O
.	O
the	O
choice	O
of	O
whether	O
to	O
use	O
dense	O
or	O
sparse	O
initialization	O
can	O
also	O
be	O
made	O
a	O
hyperparameter	O
.	O
alternately	O
,	O
one	O
can	O
manually	O
search	O
for	O
the	O
best	O
initial	O
scales	O
.	O
a	O
good	O
rule	O
of	O
thumb	O
for	O
choosing	O
the	O
initial	O
scales	O
is	O
to	O
look	O
at	O
the	O
range	O
or	O
standard	O
deviation	O
of	O
activations	O
or	O
gradients	O
on	O
a	O
single	O
minibatch	O
of	O
data	O
.	O
if	O
the	O
weights	O
are	O
too	O
small	O
,	O
the	O
range	O
of	O
activations	O
across	O
the	O
minibatch	O
will	O
shrink	O
as	O
the	O
activations	O
propagate	O
forward	O
through	O
the	O
network	O
.	O
by	O
repeatedly	O
identifying	O
the	O
ﬁrst	O
layer	O
with	O
unacceptably	O
small	O
activations	O
and	O
11.4.2	O
304	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
increasing	O
its	O
weights	O
,	O
it	O
is	O
possible	O
to	O
eventually	O
obtain	O
a	O
network	O
with	O
reasonable	O
initial	O
activations	O
throughout	O
.	O
if	O
learning	O
is	O
still	O
too	O
slow	O
at	O
this	O
point	O
,	O
it	O
can	O
be	O
useful	O
to	O
look	O
at	O
the	O
range	O
or	O
standard	O
deviation	O
of	O
the	O
gradients	O
as	O
well	O
as	O
the	O
activations	O
.	O
this	O
procedure	O
can	O
in	O
principle	O
be	O
automated	O
and	O
is	O
generally	O
less	O
computationally	O
costly	O
than	O
hyperparameter	O
optimization	O
based	O
on	O
validation	O
set	O
error	O
because	O
it	O
is	O
based	O
on	O
feedback	O
from	O
the	O
behavior	O
of	O
the	O
initial	O
model	B
on	O
a	O
single	O
batch	O
of	O
data	O
,	O
rather	O
than	O
on	O
feedback	O
from	O
a	O
trained	O
model	B
on	O
the	O
validation	O
set	O
.	O
while	O
long	O
used	O
heuristically	O
,	O
this	O
protocol	O
has	O
recently	O
been	O
speciﬁed	O
more	O
formally	O
and	O
studied	O
by	O
mishkin	O
and	O
matas	O
2015	O
)	O
.	O
(	O
so	O
far	O
we	O
have	O
focused	O
on	O
the	O
initialization	O
of	O
the	O
weights	O
.	O
fortunately	O
,	O
initialization	O
of	O
other	O
parameters	O
is	O
typically	O
easier	O
.	O
the	O
approach	O
for	O
setting	O
the	O
biases	O
must	O
be	O
coordinated	O
with	O
the	O
approach	O
for	O
settings	O
the	O
weights	O
.	O
setting	O
the	O
biases	O
to	O
zero	O
is	O
compatible	O
with	O
most	O
weight	O
initialization	O
schemes	O
.	O
there	O
are	O
a	O
few	O
situations	O
where	O
we	O
may	O
set	O
some	O
biases	O
to	O
non-zero	O
values	O
:	O
•	O
•	O
•	O
if	O
a	O
bias	O
is	O
for	O
an	O
output	O
unit	O
,	O
then	O
it	O
is	O
often	O
beneﬁcial	O
to	O
initialize	O
the	O
bias	O
to	O
obtain	O
the	O
right	O
marginal	O
statistics	O
of	O
the	O
output	O
.	O
to	O
do	O
this	O
,	O
we	O
assume	O
that	O
the	O
initial	O
weights	O
are	O
small	O
enough	O
that	O
the	O
output	O
of	O
the	O
unit	O
is	O
determined	O
only	O
by	O
the	O
bias	O
.	O
this	O
justiﬁes	O
setting	O
the	O
bias	O
to	O
the	O
inverse	O
of	O
the	O
activation	O
function	O
applied	O
to	O
the	O
marginal	O
statistics	O
of	O
the	O
output	O
in	O
the	O
training	O
set	O
.	O
for	O
example	O
,	O
if	O
the	O
output	O
is	O
a	O
distribution	O
over	O
classes	O
and	O
this	O
distribution	O
is	O
a	O
highly	O
skewed	O
distribution	O
with	O
the	O
marginal	O
probability	O
of	O
class	O
i	O
given	O
by	O
element	O
ci	O
of	O
some	O
vector	O
c	O
,	O
then	O
we	O
can	O
set	O
the	O
bias	O
vector	O
b	O
by	O
solving	O
the	O
equation	O
softmax	O
(	O
b	O
)	O
=	O
c.	O
this	O
applies	O
not	O
only	O
to	O
classiﬁers	O
but	O
also	O
to	O
models	O
we	O
will	O
encounter	O
in	O
part	O
,	O
such	O
as	O
autoencoders	O
and	O
boltzmann	O
machines	O
.	O
these	O
models	O
have	O
layers	O
whose	O
output	O
should	O
resemble	O
the	O
input	O
data	O
x	O
,	O
and	O
it	O
can	O
be	O
very	O
helpful	O
to	O
initialize	O
the	O
biases	O
of	O
such	O
layers	O
to	O
match	O
the	O
marginal	O
distribution	O
over	O
iii	O
.x	O
sometimes	O
we	O
may	O
want	O
to	O
choose	O
the	O
bias	O
to	O
avoid	O
causing	O
too	O
much	O
saturation	O
at	O
initialization	O
.	O
for	O
example	O
,	O
we	O
may	O
set	O
the	O
bias	O
of	O
a	O
relu	O
hidden	O
unit	O
to	O
0.1	O
rather	O
than	O
0	O
to	O
avoid	O
saturating	O
the	O
relu	O
at	O
initialization	O
.	O
this	O
approach	O
is	O
not	O
compatible	O
with	O
weight	O
initialization	O
schemes	O
that	O
do	O
not	O
expect	O
strong	O
input	O
from	O
the	O
biases	O
though	O
.	O
for	O
example	O
,	O
it	O
is	O
not	O
recommended	O
for	O
use	O
with	O
random	O
walk	O
initialization	O
(	O
sussillo	O
2014	O
)	O
.	O
,	O
sometimes	O
a	O
unit	O
controls	O
whether	O
other	O
units	O
are	O
able	O
to	O
participate	O
in	O
a	O
function	O
.	O
in	O
such	O
situations	O
,	O
we	O
have	O
a	O
unit	O
with	O
output	O
u	O
and	O
another	O
unit	O
[	O
0	O
,	O
1	O
]	O
,	O
and	O
they	O
are	O
multiplied	O
together	O
to	O
produce	O
an	O
output	O
uh	O
.	O
we	O
h	O
∈	O
305	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
u	O
can	O
view	O
h	O
as	O
a	O
gate	O
that	O
determines	O
whether	O
uh	O
0.	O
in	O
these	O
situations	O
,	O
we	O
want	O
to	O
set	O
the	O
bias	O
for	O
h	O
so	O
that	O
h	O
1	O
most	O
of	O
the	O
time	O
at	O
initialization	O
.	O
otherwise	O
u	O
does	O
not	O
have	O
a	O
chance	O
to	O
learn	O
.	O
for	O
example	O
,	O
jozefowicz	O
for	O
the	O
forget	O
gate	O
of	O
the	O
lstm	O
model	B
,	O
described	O
in	O
section	O
)	O
advocate	O
setting	O
the	O
bias	O
to	O
et	O
al	O
.	O
(	O
or	O
uh	O
10.10	O
.	O
2015	O
1	O
≈	O
≈	O
≈	O
another	O
common	O
type	O
of	O
parameter	O
is	O
a	O
variance	O
or	O
precision	O
parameter	O
.	O
for	O
example	O
,	O
we	O
can	O
perform	O
linear	O
regression	O
with	O
a	O
conditional	O
variance	O
estimate	O
using	O
the	O
model	B
|	O
n	O
x	O
)	O
=	O
p	O
y	O
(	O
|	O
y	O
(	O
wt	O
x	O
+	O
1	O
b	O
,	O
/β	O
)	O
(	O
8.24	O
)	O
where	O
β	O
is	O
a	O
precision	O
parameter	O
.	O
we	O
can	O
usually	O
initialize	O
variance	O
or	O
precision	O
parameters	O
to	O
1	O
safely	O
.	O
another	O
approach	O
is	O
to	O
assume	O
the	O
initial	O
weights	O
are	O
close	O
enough	O
to	O
zero	O
that	O
the	O
biases	O
may	O
be	O
set	O
while	O
ignoring	O
the	O
eﬀect	O
of	O
the	O
weights	O
,	O
then	O
set	O
the	O
biases	O
to	O
produce	O
the	O
correct	O
marginal	O
mean	O
of	O
the	O
output	O
,	O
and	O
set	O
the	O
variance	O
parameters	O
to	O
the	O
marginal	O
variance	O
of	O
the	O
output	O
in	O
the	O
training	O
set	O
.	O
iii	O
besides	O
these	O
simple	O
constant	O
or	O
random	O
methods	O
of	O
initializing	O
model	B
parame-	O
ters	O
,	O
it	O
is	O
possible	O
to	O
initialize	O
model	B
parameters	O
using	O
machine	O
learning	O
.	O
a	O
common	O
strategy	O
discussed	O
in	O
part	O
of	O
this	O
book	O
is	O
to	O
initialize	O
a	O
supervised	O
model	B
with	O
the	O
parameters	O
learned	O
by	O
an	O
unsupervised	O
model	B
trained	O
on	O
the	O
same	O
inputs	O
.	O
one	O
can	O
also	O
perform	O
supervised	O
training	O
on	O
a	O
related	O
task	O
.	O
even	O
performing	O
supervised	O
training	O
on	O
an	O
unrelated	O
task	O
can	O
sometimes	O
yield	O
an	O
initialization	O
that	O
oﬀers	O
faster	O
convergence	O
than	O
a	O
random	O
initialization	O
.	O
some	O
of	O
these	O
initialization	O
strategies	O
may	O
yield	O
faster	O
convergence	O
and	O
better	O
generalization	O
because	O
they	O
encode	O
information	O
about	O
the	O
distribution	O
in	O
the	O
initial	O
parameters	O
of	O
the	O
model	B
.	O
others	O
apparently	O
perform	O
well	O
primarily	O
because	O
they	O
set	O
the	O
parameters	O
to	O
have	O
the	O
right	O
scale	O
or	O
set	O
diﬀerent	O
units	O
to	O
compute	O
diﬀerent	O
functions	O
from	O
each	O
other	O
.	O
8.5	O
algorithms	O
with	O
adaptive	O
learning	O
rates	O
neural	O
network	O
researchers	O
have	O
long	O
realized	O
that	O
the	O
learning	O
rate	O
was	O
reliably	O
one	O
of	O
the	O
hyperparameters	O
that	O
is	O
the	O
most	O
diﬃcult	O
to	O
set	O
because	O
it	O
has	O
a	O
signiﬁcant	O
impact	O
on	O
model	B
performance	O
.	O
as	O
we	O
have	O
discussed	O
in	O
sections	O
,	O
the	O
cost	O
is	O
often	O
highly	O
sensitive	O
to	O
some	O
directions	O
in	O
parameter	O
space	O
and	O
insensitive	O
to	O
others	O
.	O
the	O
momentum	O
algorithm	O
can	O
mitigate	O
these	O
issues	O
somewhat	O
,	O
but	O
does	O
so	O
at	O
the	O
expense	O
of	O
introducing	O
another	O
hyperparameter	O
.	O
in	O
the	O
face	O
of	O
this	O
,	O
it	O
is	O
natural	O
to	O
ask	O
if	O
there	O
is	O
another	O
way	O
.	O
if	O
we	O
believe	O
that	O
the	O
directions	O
of	O
sensitivity	O
are	O
somewhat	O
axis-aligned	O
,	O
it	O
can	O
make	O
sense	O
to	O
use	O
a	O
separate	O
learning	O
and	O
4.3	O
8.2	O
306	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
rate	O
for	O
each	O
parameter	O
,	O
and	O
automatically	O
adapt	O
these	O
learning	O
rates	O
throughout	O
the	O
course	O
of	O
learning	O
.	O
the	O
,	O
algorithm	O
(	O
jacobs	O
1988	O
delta-bar-delta	O
)	O
is	O
an	O
early	O
heuristic	O
approach	O
to	O
adapting	O
individual	O
learning	O
rates	O
for	O
model	B
parameters	O
during	O
training	O
.	O
the	O
approach	O
is	O
based	O
on	O
a	O
simple	O
idea	O
:	O
if	O
the	O
partial	O
derivative	O
of	O
the	O
loss	O
,	O
with	O
respect	O
to	O
a	O
given	O
model	B
parameter	O
,	O
remains	O
the	O
same	O
sign	O
,	O
then	O
the	O
learning	O
rate	O
should	O
increase	O
.	O
if	O
the	O
partial	O
derivative	O
with	O
respect	O
to	O
that	O
parameter	O
changes	O
sign	O
,	O
then	O
the	O
learning	O
rate	O
should	O
decrease	O
.	O
of	O
course	O
,	O
this	O
kind	O
of	O
rule	O
can	O
only	O
be	O
applied	O
to	O
full	O
batch	O
optimization	O
.	O
more	O
recently	O
,	O
a	O
number	O
of	O
incremental	O
(	O
or	O
mini-batch-based	O
)	O
methods	O
have	O
been	O
introduced	O
that	O
adapt	O
the	O
learning	O
rates	O
of	O
model	B
parameters	O
.	O
this	O
section	O
will	O
brieﬂy	O
review	O
a	O
few	O
of	O
these	O
algorithms	O
.	O
8.5.1	O
adagrad	O
8.4	O
the	O
adagrad	O
algorithm	O
,	O
shown	O
in	O
algorithm	O
,	O
individually	O
adapts	O
the	O
learning	O
rates	O
of	O
all	O
model	B
parameters	O
by	O
scaling	O
them	O
inversely	O
proportional	O
to	O
the	O
square	O
root	O
of	O
the	O
sum	O
of	O
all	O
of	O
their	O
historical	O
squared	O
values	O
(	O
)	O
.	O
the	O
parameters	O
with	O
the	O
largest	O
partial	O
derivative	O
of	O
the	O
loss	O
have	O
a	O
correspondingly	O
rapid	O
decrease	O
in	O
their	O
learning	O
rate	O
,	O
while	O
parameters	O
with	O
small	O
partial	O
derivatives	O
have	O
a	O
relatively	O
small	O
decrease	O
in	O
their	O
learning	O
rate	O
.	O
the	O
net	O
eﬀect	O
is	O
greater	O
progress	O
in	O
the	O
more	O
gently	O
sloped	O
directions	O
of	O
parameter	O
space	O
.	O
duchi	O
et	O
al	O
.	O
2011	O
,	O
in	O
the	O
context	O
of	O
convex	O
optimization	O
,	O
the	O
adagrad	O
algorithm	O
enjoys	O
some	O
desirable	O
theoretical	O
properties	O
.	O
however	O
,	O
empirically	O
it	O
has	O
been	O
found	O
that—for	O
training	O
deep	O
neural	O
network	O
models—the	O
accumulation	O
of	O
squared	O
gradients	O
from	O
the	O
beginning	O
of	O
training	O
can	O
result	O
in	O
a	O
premature	O
and	O
excessive	O
decrease	O
in	O
the	O
eﬀective	O
learning	O
rate	O
.	O
adagrad	O
performs	O
well	O
for	O
some	O
but	O
not	O
all	O
deep	O
learning	O
models	O
.	O
8.5.2	O
rmsprop	O
,	O
hinton	O
2012	O
the	O
rmsprop	O
algorithm	O
(	O
)	O
modiﬁes	O
adagrad	O
to	O
perform	O
better	O
in	O
the	O
non-convex	O
setting	O
by	O
changing	O
the	O
gradient	O
accumulation	O
into	O
an	O
exponentially	O
weighted	O
moving	O
average	O
.	O
adagrad	O
is	O
designed	O
to	O
converge	O
rapidly	O
when	O
applied	O
to	O
a	O
convex	O
function	O
.	O
when	O
applied	O
to	O
a	O
non-convex	O
function	O
to	O
train	O
a	O
neural	O
network	O
,	O
the	O
learning	O
trajectory	O
may	O
pass	O
through	O
many	O
diﬀerent	O
structures	O
and	O
eventually	O
arrive	O
at	O
a	O
region	O
that	O
is	O
a	O
locally	O
convex	O
bowl	O
.	O
adagrad	O
shrinks	O
the	O
learning	O
rate	O
according	O
to	O
the	O
entire	O
history	O
of	O
the	O
squared	O
gradient	O
and	O
may	O
307	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
algorithm	O
8.4	O
the	O
adagrad	O
algorithm	O
require	O
:	O
global	O
learning	O
rate	O
	O
require	O
:	O
initial	O
parameter	O
θ	O
require	O
:	O
small	O
constant	O
,	O
perhaps	O
10	O
δ	O
initialize	O
gradient	O
accumulation	O
variable	O
r	O
=	O
0	O
while	O
stopping	O
criterion	O
not	O
met	O
do	O
−	O
7	O
,	O
for	O
numerical	O
stability	O
	O
{	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
m	O
}	O
with	O
sample	O
a	O
minibatch	O
of	O
m	O
examples	O
from	O
the	O
training	O
set	O
corresponding	O
targets	O
y	O
(	O
)	O
i	O
.	O
1	O
compute	O
gradient	O
:	O
g	O
m	O
accumulate	O
squared	O
gradient	O
:	O
r	O
√	O
	O
compute	O
update	O
:	O
∆θ	O
δ+	O
r	O
element-wise	O
)	O
apply	O
update	O
:	O
θ	O
←	O
i	O
l	O
f	O
(	O
(	O
x	O
(	O
)	O
i	O
;	O
)	O
θ	O
,	O
y	O
(	O
)	O
i	O
)	O
	O
r	O
+	O
g.	O
←	O
∇	O
←	O
−	O
+	O
∆	O
←	O
	O
θ	O
θ	O
θ	O
g	O
g	O
(	O
division	O
and	O
square	O
root	O
applied	O
end	O
while	O
have	O
made	O
the	O
learning	O
rate	O
too	O
small	O
before	O
arriving	O
at	O
such	O
a	O
convex	O
structure	O
.	O
rmsprop	O
uses	O
an	O
exponentially	O
decaying	O
average	O
to	O
discard	O
history	O
from	O
the	O
extreme	O
past	O
so	O
that	O
it	O
can	O
converge	O
rapidly	O
after	O
ﬁnding	O
a	O
convex	O
bowl	O
,	O
as	O
if	O
it	O
were	O
an	O
instance	O
of	O
the	O
adagrad	O
algorithm	O
initialized	O
within	O
that	O
bowl	O
.	O
rmsprop	O
is	O
shown	O
in	O
its	O
standard	O
form	O
in	O
algorithm	O
and	O
combined	O
with	O
nesterov	O
momentum	O
in	O
algorithm	O
.	O
compared	O
to	O
adagrad	O
,	O
the	O
use	O
of	O
the	O
moving	O
average	O
introduces	O
a	O
new	O
hyperparameter	O
,	O
ρ	O
,	O
that	O
controls	O
the	O
length	O
scale	O
of	O
the	O
moving	O
average	O
.	O
8.6	O
8.5	O
empirically	O
,	O
rmsprop	O
has	O
been	O
shown	O
to	O
be	O
an	O
eﬀective	O
and	O
practical	O
op-	O
timization	O
algorithm	O
for	O
deep	O
neural	O
networks	O
.	O
it	O
is	O
currently	O
one	O
of	O
the	O
go-to	O
optimization	O
methods	O
being	O
employed	O
routinely	O
by	O
deep	O
learning	O
practitioners	O
.	O
8.5.3	O
adam	O
,	O
8.7	O
kingma	O
and	O
ba	O
2014	O
)	O
is	O
yet	O
another	O
adaptive	O
learning	O
rate	O
optimization	O
adam	O
(	O
.	O
the	O
name	O
“	O
adam	O
”	O
derives	O
from	O
algorithm	O
and	O
is	O
presented	O
in	O
algorithm	O
the	O
phrase	O
“	O
adaptive	O
moments.	O
”	O
in	O
the	O
context	O
of	O
the	O
earlier	O
algorithms	O
,	O
it	O
is	O
perhaps	O
best	O
seen	O
as	O
a	O
variant	O
on	O
the	O
combination	O
of	O
rmsprop	O
and	O
momentum	O
with	O
a	O
few	O
important	O
distinctions	O
.	O
first	O
,	O
in	O
adam	O
,	O
momentum	O
is	O
incorporated	O
directly	O
as	O
an	O
estimate	O
of	O
the	O
ﬁrst	O
order	O
moment	O
(	O
with	O
exponential	O
weighting	O
)	O
of	O
the	O
gradient	O
.	O
the	O
most	O
straightforward	O
way	O
to	O
add	O
momentum	O
to	O
rmsprop	O
is	O
to	O
apply	O
momentum	O
to	O
the	O
rescaled	O
gradients	O
.	O
the	O
use	O
of	O
momentum	O
in	O
combination	O
with	O
rescaling	O
does	O
not	O
have	O
a	O
clear	O
theoretical	O
motivation	O
.	O
second	O
,	O
adam	O
includes	O
308	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
algorithm	O
8.5	O
the	O
rmsprop	O
algorithm	O
require	O
:	O
global	O
learning	O
rate	O
,	O
decay	O
rate	O
.	O
ρ	O
require	O
:	O
initial	O
parameter	O
θ	O
require	O
:	O
small	O
constant	O
δ	O
,	O
usually	O
10	O
	O
−	O
6	O
,	O
used	O
to	O
stabilize	O
division	O
by	O
small	O
numbers	O
.	O
initialize	O
accumulation	O
variables	O
r	O
=	O
0	O
while	O
stopping	O
criterion	O
not	O
met	O
do	O
	O
←	O
sample	O
a	O
minibatch	O
of	O
m	O
examples	O
from	O
the	O
training	O
set	O
corresponding	O
targets	O
y	O
(	O
)	O
i	O
.	O
1	O
compute	O
gradient	O
:	O
g	O
m	O
accumulate	O
squared	O
gradient	O
:	O
r	O
compute	O
parameter	O
update	O
:	O
∆θ	O
=	O
apply	O
update	O
:	O
θ	O
←	O
	O
−	O
i	O
l	O
f	O
(	O
(	O
x	O
(	O
)	O
i	O
;	O
)	O
θ	O
,	O
y	O
(	O
)	O
i	O
)	O
−	O
	O
g	O
ρ	O
+	O
(	O
1	O
r	O
√	O
1√	O
δ+r	O
δ+r	O
g	O
)	O
ρ	O
g	O
.	O
(	O
+	O
∆	O
←	O
θ	O
∇	O
θ	O
θ	O
{	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
m	O
}	O
with	O
applied	O
element-wise	O
)	O
end	O
while	O
bias	O
corrections	O
to	O
the	O
estimates	O
of	O
both	O
the	O
ﬁrst-order	O
moments	O
(	O
the	O
momentum	O
term	O
)	O
and	O
the	O
(	O
uncentered	O
)	O
second-order	O
moments	O
to	O
account	O
for	O
their	O
initialization	O
at	O
the	O
origin	O
(	O
see	O
algorithm	O
)	O
.	O
rmsprop	O
also	O
incorporates	O
an	O
estimate	O
of	O
the	O
(	O
uncentered	O
)	O
second-order	O
moment	O
,	O
however	O
it	O
lacks	O
the	O
correction	O
factor	O
.	O
thus	O
,	O
unlike	O
in	O
adam	O
,	O
the	O
rmsprop	O
second-order	O
moment	O
estimate	O
may	O
have	O
high	O
bias	O
early	O
in	O
training	O
.	O
adam	O
is	O
generally	O
regarded	O
as	O
being	O
fairly	O
robust	O
to	O
the	O
choice	O
of	O
hyperparameters	O
,	O
though	O
the	O
learning	O
rate	O
sometimes	O
needs	O
to	O
be	O
changed	O
from	O
the	O
suggested	O
default	O
.	O
8.7	O
8.5.4	O
choosing	O
the	O
right	O
optimization	O
algorithm	O
in	O
this	O
section	O
,	O
we	O
discussed	O
a	O
series	O
of	O
related	O
algorithms	O
that	O
each	O
seek	O
to	O
address	O
the	O
challenge	O
of	O
optimizing	O
deep	O
models	O
by	O
adapting	O
the	O
learning	O
rate	O
for	O
each	O
model	B
parameter	O
.	O
at	O
this	O
point	O
,	O
a	O
natural	O
question	O
is	O
:	O
which	O
algorithm	O
should	O
one	O
choose	O
?	O
unfortunately	O
,	O
there	O
is	O
currently	O
no	O
consensus	O
on	O
this	O
point	O
.	O
schaul	O
et	O
al	O
.	O
2014	O
)	O
presented	O
a	O
valuable	O
comparison	O
of	O
a	O
large	O
number	O
of	O
optimization	O
algorithms	O
across	O
a	O
wide	O
range	O
of	O
learning	O
tasks	O
.	O
while	O
the	O
results	O
suggest	O
that	O
the	O
family	O
of	O
algorithms	O
with	O
adaptive	O
learning	O
rates	O
(	O
represented	O
by	O
rmsprop	O
and	O
adadelta	O
)	O
performed	O
fairly	O
robustly	O
,	O
no	O
single	O
best	O
algorithm	O
has	O
emerged	O
.	O
(	O
currently	O
,	O
the	O
most	O
popular	O
optimization	O
algorithms	O
actively	O
in	O
use	O
include	O
sgd	O
,	O
sgd	O
with	O
momentum	O
,	O
rmsprop	O
,	O
rmsprop	O
with	O
momentum	O
,	O
adadelta	O
and	O
adam	O
.	O
the	O
choice	O
of	O
which	O
algorithm	O
to	O
use	O
,	O
at	O
this	O
point	O
,	O
seems	O
to	O
depend	O
309	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
algorithm	O
8.6	O
rmsprop	O
algorithm	O
with	O
nesterov	O
momentum	O
require	O
:	O
global	O
learning	O
rate	O
,	O
decay	O
rate	O
,	O
momentum	O
coeﬃcient	O
require	O
:	O
initial	O
parameter	O
,	O
initial	O
velocity	O
.	O
v	O
θ	O
ρ	O
	O
.	O
α	O
initialize	O
accumulation	O
variable	O
r	O
=	O
0	O
while	O
stopping	O
criterion	O
not	O
met	O
do	O
	O
sample	O
a	O
minibatch	O
of	O
m	O
examples	O
from	O
the	O
training	O
set	O
←	O
corresponding	O
targets	O
y	O
(	O
)	O
i	O
.	O
compute	O
interim	O
update	O
:	O
˜θ	O
∇	O
←	O
compute	O
gradient	O
:	O
g	O
˜θ	O
←	O
accumulate	O
gradient	O
:	O
r	O
r	O
ρ	O
+	O
(	O
1	O
compute	O
velocity	O
update	O
:	O
v	O
v	O
apply	O
update	O
:	O
θ	O
+	O
α	O
v	O
i	O
l	O
f	O
(	O
(	O
x	O
(	O
)	O
i	O
;	O
˜θ	O
y	O
)	O
,	O
−	O
−	O
(	O
)	O
i	O
)	O
(	O
1√	O
)	O
g	O
ρ	O
√	O
	O
	O
g	O
g.	O
←	O
←	O
1	O
m	O
α	O
θ	O
θ	O
+	O
v	O
r	O
r	O
{	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
m	O
}	O
with	O
applied	O
element-wise	O
)	O
end	O
while	O
largely	O
on	O
the	O
user	O
’	O
s	O
familiarity	O
with	O
the	O
algorithm	O
(	O
for	O
ease	O
of	O
hyperparameter	O
tuning	O
)	O
.	O
8.6	O
approximate	O
second-order	O
methods	O
in	O
this	O
section	O
we	O
discuss	O
the	O
application	O
of	O
second-order	O
methods	O
to	O
the	O
training	O
of	O
deep	O
networks	O
.	O
see	O
)	O
for	O
an	O
earlier	O
treatment	O
of	O
this	O
subject	O
.	O
for	O
simplicity	O
of	O
exposition	O
,	O
the	O
only	O
objective	O
function	O
we	O
examine	O
is	O
the	O
empirical	O
risk	O
:	O
lecun	O
et	O
al	O
.	O
1998a	O
	O
(	O
j	O
(	O
)	O
=	O
θ	O
∼	O
ex	O
,	O
y	O
ˆpdata	O
(	O
)	O
x	O
,	O
y	O
[	O
(	O
(	O
;	O
l	O
f	O
x	O
θ	O
,	O
y	O
)	O
)	O
]	O
=	O
1	O
m	O
m	O
i=1	O
l	O
f	O
(	O
(	O
x	O
(	O
)	O
i	O
;	O
)	O
θ	O
,	O
y	O
(	O
)	O
i	O
)	O
.	O
(	O
8.25	O
)	O
however	O
the	O
methods	O
we	O
discuss	O
here	O
extend	O
readily	O
to	O
more	O
general	O
objective	O
functions	O
that	O
,	O
for	O
instance	O
,	O
include	O
parameter	O
regularization	O
terms	O
such	O
as	O
those	O
discussed	O
in	O
chapter	O
.7	O
8.6.1	O
newton	O
’	O
s	O
method	O
4.3	O
,	O
we	O
introduced	O
second-order	O
gradient	O
methods	O
.	O
in	O
contrast	O
to	O
ﬁrst-	O
in	O
section	O
order	O
methods	O
,	O
second-order	O
methods	O
make	O
use	O
of	O
second	O
derivatives	O
to	O
improve	O
optimization	O
.	O
the	O
most	O
widely	O
used	O
second-order	O
method	O
is	O
newton	O
’	O
s	O
method	O
.	O
we	O
now	O
describe	O
newton	O
’	O
s	O
method	O
in	O
more	O
detail	O
,	O
with	O
emphasis	O
on	O
its	O
application	O
to	O
neural	O
network	O
training	O
.	O
310	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
algorithm	O
8.7	O
the	O
adam	O
algorithm	O
require	O
:	O
step	O
size	O
require	O
:	O
exponential	O
decay	O
rates	O
for	O
moment	O
estimates	O
,	O
ρ1	O
and	O
ρ2	O
in	O
[	O
0	O
,	O
1	O
)	O
.	O
(	O
suggested	O
default	O
:	O
)	O
0	O
001	O
.	O
	O
(	O
suggested	O
defaults	O
:	O
0	O
9.	O
and	O
0	O
999.	O
respectively	O
)	O
require	O
:	O
small	O
constant	O
δ	O
used	O
for	O
numerical	O
stabilization	O
.	O
(	O
suggested	O
default	O
:	O
−	O
8	O
)	O
10	O
require	O
:	O
initial	O
parameters	O
θ	O
initialize	O
1st	O
and	O
2nd	O
moment	O
variables	O
initialize	O
time	O
step	O
t	O
=	O
0	O
while	O
stopping	O
criterion	O
not	O
met	O
do	O
	O
s	O
=	O
0	O
r	O
=	O
0	O
,	O
t	O
∇	O
+	O
1	O
←	O
sample	O
a	O
minibatch	O
of	O
m	O
examples	O
from	O
the	O
training	O
set	O
corresponding	O
targets	O
y	O
(	O
)	O
i	O
.	O
←	O
1	O
compute	O
gradient	O
:	O
g	O
m	O
t	O
update	O
biased	O
ﬁrst	O
moment	O
estimate	O
:	O
s	O
update	O
biased	O
second	O
moment	O
estimate	O
:	O
r	O
correct	O
bias	O
in	O
ﬁrst	O
moment	O
:	O
ˆs	O
correct	O
bias	O
in	O
second	O
moment	O
:	O
ˆr	O
i	O
l	O
f	O
(	O
(	O
x	O
(	O
)	O
i	O
;	O
)	O
θ	O
,	O
y	O
(	O
)	O
i	O
)	O
←	O
ρ1s	O
+	O
(	O
1	O
ρ2r	O
+	O
(	O
1	O
←	O
←	O
−	O
θ	O
−	O
←	O
s	O
ρ	O
t	O
1	O
−	O
1	O
r	O
ρt	O
1	O
2	O
−	O
compute	O
update	O
:	O
∆	O
=	O
θ	O
	O
apply	O
update	O
:	O
θ	O
θ	O
+	O
∆	O
←	O
θ	O
ˆs√	O
ˆr+δ	O
{	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
m	O
}	O
with	O
−	O
ρ1	O
)	O
g	O
ρ	O
2	O
)	O
g	O
	O
g	O
(	O
operations	O
applied	O
element-wise	O
)	O
end	O
while	O
newton	O
’	O
s	O
method	O
is	O
an	O
optimization	O
scheme	O
based	O
on	O
using	O
a	O
second-order	O
tay-	O
lor	O
series	O
expansion	O
to	O
approximate	O
j	O
(	O
θ	O
)	O
near	O
some	O
point	O
θ	O
0	O
,	O
ignoring	O
derivatives	O
of	O
higher	O
order	O
:	O
≈	O
(	O
)	O
θ	O
h	O
θ	O
(	O
8.26	O
)	O
−	O
−	O
−	O
(	O
θ	O
	O
θ	O
θ	O
j	O
j	O
(	O
∇	O
0	O
)	O
(	O
θ0	O
)	O
+	O
(	O
θ	O
θj	O
(	O
θ0	O
)	O
+	O
θ	O
0	O
)	O
,	O
0	O
)	O
1	O
2	O
where	O
h	O
is	O
the	O
hessian	O
of	O
j	O
with	O
respect	O
to	O
θ	O
evaluated	O
at	O
θ0	O
.	O
if	O
we	O
then	O
solve	O
for	O
the	O
critical	O
point	O
of	O
this	O
function	O
,	O
we	O
obtain	O
the	O
newton	O
parameter	O
update	O
rule	O
:	O
∗	O
θ	O
=	O
θ0	O
−	O
∇	O
−	O
1	O
h	O
θj	O
(	O
θ	O
0	O
)	O
(	O
8.27	O
)	O
thus	O
for	O
a	O
locally	O
quadratic	O
function	O
(	O
with	O
positive	O
deﬁnite	O
h	O
)	O
,	O
by	O
rescaling	O
−	O
1	O
,	O
newton	O
’	O
s	O
method	O
jumps	O
directly	O
to	O
the	O
minimum	O
.	O
if	O
the	O
the	O
gradient	O
by	O
h	O
objective	O
function	O
is	O
convex	O
but	O
not	O
quadratic	O
(	O
there	O
are	O
higher-order	O
terms	O
)	O
,	O
this	O
update	O
can	O
be	O
iterated	O
,	O
yielding	O
the	O
training	O
algorithm	O
associated	O
with	O
newton	O
’	O
s	O
method	O
,	O
given	O
in	O
algorithm	O
.8.8	O
311	O
	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
newton	O
’	O
s	O
method	O
with	O
objective	O
j	O
(	O
θ	O
)	O
=	O
m	O
i=1	O
l	O
f	O
(	O
(	O
x	O
(	O
)	O
i	O
;	O
)	O
θ	O
,	O
y	O
(	O
)	O
i	O
)	O
.	O
algorithm	O
8.8	O
1	O
m	O
require	O
:	O
initial	O
parameter	O
θ0	O
require	O
:	O
training	O
set	O
of	O
	O
	O
while	O
∇	O
stopping	O
criterion	O
not	O
met	O
∇	O
compute	O
gradient	O
:	O
g	O
θ	O
2	O
compute	O
hessian	O
:	O
h	O
−	O
θ	O
−	O
1	O
compute	O
hessian	O
inverse	O
:	O
h	O
−	O
1	O
g	O
compute	O
update	O
:	O
∆	O
=	O
θ	O
h	O
apply	O
update	O
:	O
θ	O
=	O
+	O
∆	O
θ	O
1	O
m	O
1	O
m	O
θ	O
m	O
←	O
←	O
examples	O
do	O
i	O
l	O
f	O
(	O
(	O
x	O
(	O
)	O
i	O
;	O
)	O
θ	O
,	O
y	O
(	O
)	O
i	O
)	O
i	O
l	O
f	O
(	O
(	O
x	O
(	O
)	O
i	O
;	O
)	O
θ	O
,	O
y	O
(	O
)	O
i	O
)	O
end	O
while	O
for	O
surfaces	O
that	O
are	O
not	O
quadratic	O
,	O
as	O
long	O
as	O
the	O
hessian	O
remains	O
positive	O
deﬁnite	O
,	O
newton	O
’	O
s	O
method	O
can	O
be	O
applied	O
iteratively	O
.	O
this	O
implies	O
a	O
two-step	O
iterative	O
procedure	O
.	O
first	O
,	O
update	O
or	O
compute	O
the	O
inverse	O
hessian	O
(	O
i.e	O
.	O
by	O
updat-	O
ing	O
the	O
quadratic	O
approximation	O
)	O
.	O
second	O
,	O
update	O
the	O
parameters	O
according	O
to	O
equation	O
.	O
8.27	O
8.2.3	O
in	O
section	O
,	O
we	O
discussed	O
how	O
newton	O
’	O
s	O
method	O
is	O
appropriate	O
only	O
when	O
the	O
hessian	O
is	O
positive	O
deﬁnite	O
.	O
in	O
deep	O
learning	O
,	O
the	O
surface	O
of	O
the	O
objective	O
function	O
is	O
typically	O
non-convex	O
with	O
many	O
features	O
,	O
such	O
as	O
saddle	O
points	O
,	O
that	O
are	O
problematic	O
for	O
newton	O
’	O
s	O
method	O
.	O
if	O
the	O
eigenvalues	O
of	O
the	O
hessian	O
are	O
not	O
all	O
positive	O
,	O
for	O
example	O
,	O
near	O
a	O
saddle	O
point	O
,	O
then	O
newton	O
’	O
s	O
method	O
can	O
actually	O
cause	O
updates	O
to	O
move	O
in	O
the	O
wrong	O
direction	O
.	O
this	O
situation	O
can	O
be	O
avoided	O
by	O
regularizing	O
the	O
hessian	O
.	O
common	O
regularization	O
strategies	O
include	O
adding	O
a	O
constant	O
,	O
,	O
along	O
the	O
diagonal	O
of	O
the	O
hessian	O
.	O
the	O
regularized	O
update	O
becomes	O
α	O
∗	O
−	O
1	O
∇	O
−	O
θ	O
=	O
θ0	O
[	O
(	O
(	O
h	O
f	O
θ0	O
)	O
)	O
+	O
]	O
αi	O
θ	O
f	O
(	O
θ0	O
)	O
.	O
(	O
8.28	O
)	O
;	O
,	O
this	O
regularization	O
strategy	O
is	O
used	O
in	O
approximations	O
to	O
newton	O
’	O
s	O
method	O
,	O
such	O
as	O
the	O
levenberg–marquardt	O
algorithm	O
(	O
levenberg	O
1944	O
marquardt	O
1963	O
)	O
,	O
and	O
works	O
fairly	O
well	O
as	O
long	O
as	O
the	O
negative	O
eigenvalues	O
of	O
the	O
hessian	O
are	O
still	O
relatively	O
close	O
to	O
zero	O
.	O
in	O
cases	O
where	O
there	O
are	O
more	O
extreme	O
directions	O
of	O
curvature	O
,	O
the	O
value	O
of	O
α	O
would	O
have	O
to	O
be	O
suﬃciently	O
large	O
to	O
oﬀset	O
the	O
negative	O
eigenvalues	O
.	O
however	O
,	O
as	O
α	O
increases	O
in	O
size	O
,	O
the	O
hessian	O
becomes	O
dominated	O
by	O
the	O
αi	O
diagonal	O
and	O
the	O
direction	O
chosen	O
by	O
newton	O
’	O
s	O
method	O
converges	O
to	O
the	O
standard	O
gradient	O
divided	O
by	O
α.	O
when	O
strong	O
negative	O
curvature	O
is	O
present	O
,	O
α	O
may	O
need	O
to	O
be	O
so	O
large	O
that	O
newton	O
’	O
s	O
method	O
would	O
make	O
smaller	O
steps	O
than	O
gradient	O
descent	B
with	O
a	O
properly	O
chosen	O
learning	O
rate	O
.	O
,	O
beyond	O
the	O
challenges	O
created	O
by	O
certain	O
features	O
of	O
the	O
objective	O
function	O
,	O
312	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
such	O
as	O
saddle	O
points	O
,	O
the	O
application	O
of	O
newton	O
’	O
s	O
method	O
for	O
training	O
large	O
neural	O
networks	O
is	O
limited	O
by	O
the	O
signiﬁcant	O
computational	O
burden	O
it	O
imposes	O
.	O
the	O
number	O
of	O
elements	O
in	O
the	O
hessian	O
is	O
squared	O
in	O
the	O
number	O
of	O
parameters	O
,	O
so	O
with	O
×	O
k	O
parameters	O
(	O
and	O
for	O
even	O
very	O
small	O
neural	O
networks	O
the	O
number	O
of	O
parameters	O
k	O
can	O
be	O
in	O
the	O
millions	O
)	O
,	O
newton	O
’	O
s	O
method	O
would	O
require	O
the	O
inversion	O
of	O
a	O
k	O
k	O
matrix—with	O
computational	O
complexity	O
of	O
o	O
(	O
k3	O
)	O
.	O
also	O
,	O
since	O
the	O
parameters	O
will	O
change	O
with	O
every	O
update	O
,	O
the	O
inverse	O
hessian	O
has	O
to	O
be	O
computed	O
at	O
every	O
training	O
iteration	O
.	O
as	O
a	O
consequence	O
,	O
only	O
networks	O
with	O
a	O
very	O
small	O
number	O
of	O
parameters	O
can	O
be	O
practically	O
trained	O
via	O
newton	O
’	O
s	O
method	O
.	O
in	O
the	O
remainder	O
of	O
this	O
section	O
,	O
we	O
will	O
discuss	O
alternatives	O
that	O
attempt	O
to	O
gain	O
some	O
of	O
the	O
advantages	O
of	O
newton	O
’	O
s	O
method	O
while	O
side-stepping	O
the	O
computational	O
hurdles	O
.	O
8.6.2	O
conjugate	O
gradients	O
conjugate	O
gradients	O
is	O
a	O
method	O
to	O
eﬃciently	O
avoid	O
the	O
calculation	O
of	O
the	O
inverse	O
hessian	O
by	O
iteratively	O
descending	O
conjugate	O
directions	O
.	O
the	O
inspiration	O
for	O
this	O
approach	O
follows	O
from	O
a	O
careful	O
study	O
of	O
the	O
weakness	O
of	O
the	O
method	O
of	O
steepest	O
for	O
details	O
)	O
,	O
where	O
line	O
searches	O
are	O
applied	O
iteratively	O
in	O
descent	B
(	O
see	O
section	O
the	O
direction	O
associated	O
with	O
the	O
gradient	O
.	O
figure	O
illustrates	O
how	O
the	O
method	O
of	O
steepest	O
descent	B
,	O
when	O
applied	O
in	O
a	O
quadratic	O
bowl	O
,	O
progresses	O
in	O
a	O
rather	O
ineﬀective	O
back-and-forth	O
,	O
zig-zag	O
pattern	O
.	O
this	O
happens	O
because	O
each	O
line	O
search	O
direction	O
,	O
when	O
given	O
by	O
the	O
gradient	O
,	O
is	O
guaranteed	O
to	O
be	O
orthogonal	O
to	O
the	O
previous	O
line	O
search	O
direction	O
.	O
4.3	O
8.6	O
∇	O
−	O
·	O
let	O
the	O
previous	O
search	O
direction	O
be	O
dt	O
1.	O
at	O
the	O
minimum	O
,	O
where	O
the	O
line	O
θj	O
(	O
θ	O
)	O
−	O
search	O
terminates	O
,	O
the	O
directional	O
derivative	O
is	O
zero	O
in	O
direction	O
dt	O
1	O
:	O
−	O
∇	O
dt	O
1	O
=	O
0.	O
since	O
the	O
gradient	O
at	O
this	O
point	O
deﬁnes	O
the	O
current	O
search	O
direction	O
,	O
−	O
1.	O
thus	O
dt	O
is	O
orthogonal	O
θj	O
(	O
θ	O
)	O
will	O
have	O
no	O
contribution	O
in	O
the	O
direction	O
dt	O
dt	O
=	O
−	O
−	O
to	O
dt	O
1.	O
this	O
relationship	O
between	O
dt	O
for	O
1	O
and	O
d	O
t	O
is	O
illustrated	O
in	O
ﬁgure	O
multiple	O
iterations	O
of	O
steepest	O
descent	B
.	O
as	O
demonstrated	O
in	O
the	O
ﬁgure	O
,	O
the	O
choice	O
of	O
orthogonal	O
directions	O
of	O
descent	B
do	O
not	O
preserve	O
the	O
minimum	O
along	O
the	O
previous	O
search	O
directions	O
.	O
this	O
gives	O
rise	O
to	O
the	O
zig-zag	O
pattern	O
of	O
progress	O
,	O
where	O
by	O
descending	O
to	O
the	O
minimum	O
in	O
the	O
current	O
gradient	O
direction	O
,	O
we	O
must	O
re-minimize	O
the	O
objective	O
in	O
the	O
previous	O
gradient	O
direction	O
.	O
thus	O
,	O
by	O
following	O
the	O
gradient	O
at	O
the	O
end	O
of	O
each	O
line	O
search	O
we	O
are	O
,	O
in	O
a	O
sense	O
,	O
undoing	O
progress	O
we	O
have	O
already	O
made	O
in	O
the	O
direction	O
of	O
the	O
previous	O
line	O
search	O
.	O
the	O
method	O
of	O
conjugate	O
gradients	O
seeks	O
to	O
address	O
this	O
problem	O
.	O
8.6	O
in	O
the	O
method	O
of	O
conjugate	O
gradients	O
,	O
we	O
seek	O
to	O
ﬁnd	O
a	O
search	O
direction	O
that	O
is	O
conjugate	O
to	O
the	O
previous	O
line	O
search	O
direction	O
,	O
i.e	O
.	O
it	O
will	O
not	O
undo	O
progress	O
made	O
in	O
that	O
direction	O
.	O
at	O
training	O
iteration	O
t	O
,	O
the	O
next	O
search	O
direction	O
dt	O
takes	O
313	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
	O
	O
	O
 	O
 	O
 	O
 	O
 	O
 	O
	O
	O
	O
figure	O
8.6	O
:	O
the	O
method	O
of	O
steepest	O
descent	B
applied	O
to	O
a	O
quadratic	O
cost	O
surface	O
.	O
the	O
method	O
of	O
steepest	O
descent	B
involves	O
jumping	O
to	O
the	O
point	O
of	O
lowest	O
cost	O
along	O
the	O
line	O
deﬁned	O
by	O
the	O
gradient	O
at	O
the	O
initial	O
point	O
on	O
each	O
step	O
.	O
this	O
resolves	O
some	O
of	O
the	O
problems	O
seen	O
with	O
using	O
a	O
ﬁxed	O
learning	O
rate	O
in	O
ﬁgure	O
,	O
but	O
even	O
with	O
the	O
optimal	O
step	O
size	O
the	O
algorithm	O
still	O
makes	O
back-and-forth	O
progress	O
toward	O
the	O
optimum	O
.	O
by	O
deﬁnition	O
,	O
at	O
the	O
minimum	O
of	O
the	O
objective	O
along	O
a	O
given	O
direction	O
,	O
the	O
gradient	O
at	O
the	O
ﬁnal	O
point	O
is	O
orthogonal	O
to	O
that	O
direction	O
.	O
4.6	O
the	O
form	O
:	O
∇	O
θj	O
dt	O
=	O
−	O
tdt	O
1	O
β	O
(	O
8.29	O
)	O
−	O
where	O
β	O
t	O
is	O
a	O
coeﬃcient	O
whose	O
magnitude	O
controls	O
how	O
much	O
of	O
the	O
direction	O
,	O
dt	O
1	O
,	O
we	O
should	O
add	O
back	O
to	O
the	O
current	O
search	O
direction	O
.	O
(	O
)	O
+θ	O
−	O
two	O
directions	O
,	O
dt	O
and	O
dt	O
1	O
,	O
are	O
deﬁned	O
as	O
conjugate	O
if	O
d	O
	O
−	O
t	O
hdt	O
1	O
=	O
0	O
,	O
where	O
h	O
is	O
the	O
hessian	O
matrix	O
.	O
the	O
straightforward	O
way	O
to	O
impose	O
conjugacy	O
would	O
involve	O
calculation	O
of	O
the	O
eigenvectors	O
of	O
h	O
to	O
choose	O
βt	O
,	O
which	O
would	O
not	O
satisfy	O
our	O
goal	O
of	O
developing	O
a	O
method	O
that	O
is	O
more	O
computationally	O
viable	O
than	O
newton	O
’	O
s	O
method	O
for	O
large	O
problems	O
.	O
can	O
we	O
calculate	O
the	O
conjugate	O
directions	O
without	O
resorting	O
to	O
these	O
calculations	O
?	O
fortunately	O
the	O
answer	O
to	O
that	O
is	O
yes	O
.	O
two	O
popular	O
methods	O
for	O
computing	O
the	O
βt	O
are	O
:	O
1.	O
fletcher-reeves	O
:	O
∇	O
∇	O
∇	O
∇	O
θj	O
(	O
θt	O
)	O
−	O
1	O
)	O
θj	O
(	O
θt	O
θj	O
(	O
θt	O
)	O
−	O
θj	O
(	O
θt	O
1	O
)	O
βt	O
=	O
(	O
8.30	O
)	O
314	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
2.	O
polak-ribière	O
:	O
∇	O
(	O
∇	O
−	O
∇	O
−	O
∇	O
∇	O
θj	O
(	O
θt	O
)	O
θj	O
(	O
θt	O
)	O
θj	O
(	O
θt	O
1	O
)	O
)	O
−	O
−	O
θj	O
(	O
θt	O
1	O
)	O
θj	O
(	O
θt	O
1	O
)	O
βt	O
=	O
(	O
8.31	O
)	O
for	O
a	O
quadratic	O
surface	O
,	O
the	O
conjugate	O
directions	O
ensure	O
that	O
the	O
gradient	O
along	O
the	O
previous	O
direction	O
does	O
not	O
increase	O
in	O
magnitude	O
.	O
we	O
therefore	O
stay	O
at	O
the	O
minimum	O
along	O
the	O
previous	O
directions	O
.	O
as	O
a	O
consequence	O
,	O
in	O
a	O
k-dimensional	O
parameter	O
space	O
,	O
the	O
conjugate	O
gradient	O
method	O
requires	O
at	O
most	O
k	O
line	O
searches	O
to	O
achieve	O
the	O
minimum	O
.	O
the	O
conjugate	O
gradient	O
algorithm	O
is	O
given	O
in	O
algorithm	O
.8.9	O
algorithm	O
8.9	O
the	O
conjugate	O
gradient	O
method	O
require	O
:	O
initial	O
parameters	O
θ0	O
require	O
:	O
training	O
set	O
of	O
examples	O
m	O
	O
do	O
θ	O
∗	O
ρt	O
g	O
←	O
	O
gt	O
1	O
m	O
(	O
polak-ribière	O
)	O
i	O
l	O
f	O
(	O
(	O
x	O
(	O
)	O
i	O
;	O
)	O
θ	O
,	O
y	O
(	O
)	O
i	O
)	O
stopping	O
criterion	O
not	O
met	O
∇	O
initialize	O
the	O
gradient	O
gt	O
=	O
0	O
compute	O
gradient	O
:	O
gt	O
−	O
−	O
compute	O
βt	O
=	O
(	O
g	O
t	O
1	O
)	O
gt	O
	O
−	O
−	O
1gt	O
1	O
t	O
(	O
nonlinear	O
conjugate	O
gradient	O
:	O
optionally	O
reset	O
βt	O
to	O
zero	O
,	O
for	O
example	O
if	O
t	O
is	O
−	O
a	O
multiple	O
of	O
some	O
constant	O
)	O
,	O
such	O
as	O
k	O
=	O
5	O
k	O
−	O
gt	O
+	O
βtρt	O
compute	O
search	O
direction	O
:	O
ρt	O
=	O
∗	O
1	O
1	O
perform	O
line	O
search	O
to	O
ﬁnd	O
:	O
	O
∗	O
m	O
(	O
on	O
a	O
truly	O
quadratic	O
cost	O
function	O
,	O
analytically	O
solve	O
for	O
	O
explicitly	O
searching	O
for	O
it	O
)	O
←	O
apply	O
update	O
:	O
θt+1	O
=	O
θt	O
+	O
	O
t	O
m	O
i=1	O
l	O
f	O
(	O
(	O
x	O
(	O
)	O
i	O
;	O
θt	O
+	O
ρt	O
)	O
,	O
y	O
(	O
)	O
i	O
)	O
=	O
argmin	O
rather	O
than	O
	O
initialize	O
ρ0	O
=	O
0	O
initialize	O
g0	O
=	O
0	O
initialize	O
t	O
=	O
1	O
while	O
t	O
+	O
1	O
end	O
while	O
nonlinear	O
conjugate	O
gradients	O
:	O
so	O
far	O
we	O
have	O
discussed	O
the	O
method	O
of	O
conjugate	O
gradients	O
as	O
it	O
is	O
applied	O
to	O
quadratic	O
objective	O
functions	O
.	O
of	O
course	O
,	O
our	O
primary	O
interest	O
in	O
this	O
chapter	O
is	O
to	O
explore	O
optimization	O
methods	O
for	O
training	O
neural	O
networks	O
and	O
other	O
related	O
deep	O
learning	O
models	O
where	O
the	O
corresponding	O
objective	O
function	O
is	O
far	O
from	O
quadratic	O
.	O
perhaps	O
surprisingly	O
,	O
the	O
method	O
of	O
conjugate	O
gradients	O
is	O
still	O
applicable	O
in	O
this	O
setting	O
,	O
though	O
with	O
some	O
modiﬁcation	O
.	O
without	O
any	O
assurance	O
that	O
the	O
objective	O
is	O
quadratic	O
,	O
the	O
conjugate	O
directions	O
315	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
are	O
no	O
longer	O
assured	O
to	O
remain	O
at	O
the	O
minimum	O
of	O
the	O
objective	O
for	O
previous	O
directions	O
.	O
as	O
a	O
result	O
,	O
the	O
nonlinear	O
conjugate	O
gradients	O
algorithm	O
includes	O
occasional	O
resets	O
where	O
the	O
method	O
of	O
conjugate	O
gradients	O
is	O
restarted	O
with	O
line	O
search	O
along	O
the	O
unaltered	O
gradient	O
.	O
practitioners	O
report	O
reasonable	O
results	O
in	O
applications	O
of	O
the	O
nonlinear	O
conjugate	O
gradients	O
algorithm	O
to	O
training	O
neural	O
networks	O
,	O
though	O
it	O
is	O
often	O
beneﬁcial	O
to	O
initialize	O
the	O
optimization	O
with	O
a	O
few	O
iterations	O
of	O
stochastic	O
gradient	O
descent	B
before	O
commencing	O
nonlinear	O
conjugate	O
gradients	O
.	O
also	O
,	O
while	O
the	O
(	O
nonlinear	O
)	O
conjugate	O
gradients	O
algorithm	O
has	O
traditionally	O
been	O
cast	O
as	O
a	O
batch	O
method	O
,	O
minibatch	O
versions	O
have	O
been	O
used	O
successfully	O
for	O
the	O
training	O
of	O
neural	O
networks	O
(	O
le	O
et	O
al	O
.	O
,	O
2011	O
)	O
.	O
adaptations	O
of	O
conjugate	O
gradients	O
speciﬁcally	O
for	O
neural	O
networks	O
have	O
been	O
proposed	O
earlier	O
,	O
such	O
as	O
the	O
scaled	O
conjugate	O
gradients	O
algorithm	O
(	O
moller	O
,	O
1993	O
)	O
.	O
8.6.3	O
bfgs	O
the	O
broyden–fletcher–goldfarb–shanno	O
(	O
bfgs	O
)	O
algorithm	O
attempts	O
to	O
bring	O
some	O
of	O
the	O
advantages	O
of	O
newton	O
’	O
s	O
method	O
without	O
the	O
computational	O
burden	O
.	O
in	O
that	O
respect	O
,	O
bfgs	O
is	O
similar	O
to	O
the	O
conjugate	O
gradient	O
method	O
.	O
however	O
,	O
bfgs	O
takes	O
a	O
more	O
direct	O
approach	O
to	O
the	O
approximation	O
of	O
newton	O
’	O
s	O
update	O
.	O
recall	O
that	O
newton	O
’	O
s	O
update	O
is	O
given	O
by	O
∗	O
θ	O
=	O
θ0	O
h	O
−	O
−	O
1	O
∇	O
θ	O
j	O
(	O
θ0	O
)	O
,	O
(	O
8.32	O
)	O
where	O
h	O
is	O
the	O
hessian	O
of	O
j	O
with	O
respect	O
to	O
θ	O
evaluated	O
at	O
θ0	O
.	O
the	O
primary	O
computational	O
diﬃculty	O
in	O
applying	O
newton	O
’	O
s	O
update	O
is	O
the	O
calculation	O
of	O
the	O
−	O
1.	O
the	O
approach	O
adopted	O
by	O
quasi-newton	O
methods	O
(	O
of	O
which	O
inverse	O
hessian	O
h	O
the	O
bfgs	O
algorithm	O
is	O
the	O
most	O
prominent	O
)	O
is	O
to	O
approximate	O
the	O
inverse	O
with	O
a	O
matrix	O
mt	O
that	O
is	O
iteratively	O
reﬁned	O
by	O
low	O
rank	O
updates	O
to	O
become	O
a	O
better	O
approximation	O
of	O
h	O
−	O
1.	O
the	O
speciﬁcation	O
and	O
derivation	O
of	O
the	O
bfgs	O
approximation	O
is	O
given	O
in	O
many	O
textbooks	O
on	O
optimization	O
,	O
including	O
luenberger	O
1984	O
(	O
)	O
.	O
once	O
the	O
inverse	O
hessian	O
approximation	O
mt	O
is	O
updated	O
,	O
the	O
direction	O
of	O
descent	B
ρt	O
is	O
determined	O
by	O
ρt	O
=	O
mtgt	O
.	O
a	O
line	O
search	O
is	O
performed	O
in	O
this	O
direction	O
to	O
∗	O
determine	O
the	O
size	O
of	O
the	O
step	O
,	O
	O
,	O
taken	O
in	O
this	O
direction	O
.	O
the	O
ﬁnal	O
update	O
to	O
the	O
parameters	O
is	O
given	O
by	O
:	O
∗	O
θ	O
t+1	O
=	O
θt	O
+	O
	O
ρt	O
.	O
(	O
8.33	O
)	O
like	O
the	O
method	O
of	O
conjugate	O
gradients	O
,	O
the	O
bfgs	O
algorithm	O
iterates	O
a	O
series	O
of	O
line	O
searches	O
with	O
the	O
direction	O
incorporating	O
second-order	O
information	O
.	O
however	O
316	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
unlike	O
conjugate	O
gradients	O
,	O
the	O
success	O
of	O
the	O
approach	O
is	O
not	O
heavily	O
dependent	O
on	O
the	O
line	O
search	O
ﬁnding	O
a	O
point	O
very	O
close	O
to	O
the	O
true	O
minimum	O
along	O
the	O
line	O
.	O
thus	O
,	O
relative	O
to	O
conjugate	O
gradients	O
,	O
bfgs	O
has	O
the	O
advantage	O
that	O
it	O
can	O
spend	O
less	O
time	O
reﬁning	O
each	O
line	O
search	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
bfgs	O
algorithm	O
must	O
store	O
the	O
inverse	O
hessian	O
matrix	O
,	O
m	O
,	O
that	O
requires	O
o	O
(	O
n2	O
)	O
memory	O
,	O
making	O
bfgs	O
impractical	O
for	O
most	O
modern	O
deep	O
learning	O
models	O
that	O
typically	O
have	O
millions	O
of	O
parameters	O
.	O
limited	O
memory	O
bfgs	O
(	O
or	O
l-bfgs	O
)	O
the	O
memory	O
costs	O
of	O
the	O
bfgs	O
algorithm	O
can	O
be	O
signiﬁcantly	O
decreased	O
by	O
avoiding	O
storing	O
the	O
complete	O
inverse	O
hessian	O
approximation	O
m	O
.	O
the	O
l-bfgs	O
algorithm	O
computes	O
the	O
approximation	O
m	O
using	O
the	O
same	O
method	O
as	O
the	O
bfgs	O
algorithm	O
,	O
but	O
beginning	O
with	O
the	O
assumption	O
−	O
1	O
)	O
t	O
that	O
m	O
(	O
is	O
the	O
identity	O
matrix	O
,	O
rather	O
than	O
storing	O
the	O
approximation	O
from	O
one	O
step	O
to	O
the	O
next	O
.	O
if	O
used	O
with	O
exact	O
line	O
searches	O
,	O
the	O
directions	O
deﬁned	O
by	O
l-bfgs	O
are	O
mutually	O
conjugate	O
.	O
however	O
,	O
unlike	O
the	O
method	O
of	O
conjugate	O
gradients	O
,	O
this	O
procedure	O
remains	O
well	O
behaved	O
when	O
the	O
minimum	O
of	O
the	O
line	O
search	O
is	O
reached	O
only	O
approximately	O
.	O
the	O
l-bfgs	O
strategy	O
with	O
no	O
storage	O
described	O
here	O
can	O
be	O
generalized	O
to	O
include	O
more	O
information	O
about	O
the	O
hessian	O
by	O
storing	O
some	O
of	O
the	O
vectors	O
used	O
to	O
update	O
at	O
each	O
time	O
step	O
,	O
which	O
costs	O
only	O
per	O
step	O
.	O
m	O
o	O
n	O
(	O
)	O
8.7	O
optimization	O
strategies	O
and	O
meta-algorithms	O
many	O
optimization	O
techniques	O
are	O
not	O
exactly	O
algorithms	O
,	O
but	O
rather	O
general	O
templates	O
that	O
can	O
be	O
specialized	O
to	O
yield	O
algorithms	O
,	O
or	O
subroutines	O
that	O
can	O
be	O
incorporated	O
into	O
many	O
diﬀerent	O
algorithms	O
.	O
8.7.1	O
batch	O
normalization	O
ioﬀe	O
and	O
szegedy	O
2015	O
batch	O
normalization	O
(	O
)	O
is	O
one	O
of	O
the	O
most	O
exciting	O
recent	O
innovations	O
in	O
optimizing	O
deep	O
neural	O
networks	O
and	O
it	O
is	O
actually	O
not	O
an	O
optimization	O
algorithm	O
at	O
all	O
.	O
instead	O
,	O
it	O
is	O
a	O
method	O
of	O
adaptive	O
reparametrization	O
,	O
motivated	O
by	O
the	O
diﬃculty	O
of	O
training	O
very	O
deep	O
models	O
.	O
,	O
very	O
deep	O
models	O
involve	O
the	O
composition	O
of	O
several	O
functions	O
or	O
layers	O
.	O
the	O
gradient	O
tells	O
how	O
to	O
update	O
each	O
parameter	O
,	O
under	O
the	O
assumption	O
that	O
the	O
other	O
layers	O
do	O
not	O
change	O
.	O
in	O
practice	O
,	O
we	O
update	O
all	O
of	O
the	O
layers	O
simultaneously	O
.	O
when	O
we	O
make	O
the	O
update	O
,	O
unexpected	O
results	O
can	O
happen	O
because	O
many	O
functions	O
composed	O
together	O
are	O
changed	O
simultaneously	O
,	O
using	O
updates	O
that	O
were	O
computed	O
under	O
the	O
assumption	O
that	O
the	O
other	O
functions	O
remain	O
constant	O
.	O
as	O
a	O
simple	O
317	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
example	O
,	O
suppose	O
we	O
have	O
a	O
deep	O
neural	O
network	O
that	O
has	O
only	O
one	O
unit	O
per	O
layer	O
and	O
does	O
not	O
use	O
an	O
activation	O
function	O
at	O
each	O
hidden	O
layer	O
:	O
ˆy	O
=	O
xw1	O
w2w3	O
.	O
.	O
.	O
wl	O
.	O
−	O
here	O
,	O
wi	O
provides	O
the	O
weight	O
used	O
by	O
layer	O
i.	O
the	O
output	O
of	O
layer	O
i	O
is	O
h	O
i	O
=	O
hi	O
1wi	O
.	O
the	O
output	O
ˆy	O
is	O
a	O
linear	O
function	O
of	O
the	O
input	O
x	O
,	O
but	O
a	O
nonlinear	O
function	O
of	O
the	O
ˆy	O
,	O
so	O
we	O
wish	O
to	O
weights	O
wi	O
.	O
suppose	O
our	O
cost	O
function	O
has	O
put	O
a	O
gradient	O
of	O
←	O
−	O
∇	O
decrease	O
ˆy	O
slightly	O
.	O
the	O
back-propagation	O
algorithm	O
can	O
then	O
compute	O
a	O
gradient	O
g	O
=	O
wˆy	O
.	O
consider	O
what	O
happens	O
when	O
we	O
make	O
an	O
update	O
w	O
w	O
	O
.	O
the	O
g	O
ﬁrst-order	O
taylor	O
series	O
approximation	O
of	O
ˆy	O
predicts	O
that	O
the	O
value	O
of	O
ˆy	O
will	O
decrease	O
	O
g.	O
if	O
we	O
wanted	O
to	O
decrease	O
ˆy	O
by	O
.1	O
,	O
this	O
ﬁrst-order	O
information	O
available	O
in	O
by	O
g	O
the	O
gradient	O
suggests	O
we	O
could	O
set	O
the	O
learning	O
rate	O
	O
to	O
.1	O
	O
.	O
however	O
,	O
the	O
actual	O
g	O
update	O
will	O
include	O
second-order	O
and	O
third-order	O
eﬀects	O
,	O
on	O
up	O
to	O
eﬀects	O
of	O
order	O
l.	O
the	O
new	O
value	O
of	O
ˆy	O
is	O
given	O
by	O
−	O
	O
on1	O
−	O
−	O
g	O
x	O
w	O
(	O
1	O
g1	O
)	O
(	O
w2	O
g	O
2	O
)	O
.	O
.	O
.	O
wl	O
(	O
gl	O
)	O
.	O
(	O
8.34	O
)	O
	O
1	O
3	O
through	O
l	O
an	O
example	O
of	O
one	O
second-order	O
term	O
arising	O
from	O
this	O
update	O
is	O
2g1	O
g2	O
i=3	O
wi	O
.	O
l	O
i=3w	O
i	O
is	O
small	O
,	O
or	O
might	O
be	O
exponentially	O
large	O
this	O
term	O
might	O
be	O
negligible	O
if	O
l	O
are	O
greater	O
than	O
.	O
this	O
makes	O
it	O
very	O
hard	O
if	O
the	O
weights	O
on	O
layers	O
to	O
choose	O
an	O
appropriate	O
learning	O
rate	O
,	O
because	O
the	O
eﬀects	O
of	O
an	O
update	O
to	O
the	O
parameters	O
for	O
one	O
layer	O
depends	O
so	O
strongly	O
on	O
all	O
of	O
the	O
other	O
layers	O
.	O
second-order	O
optimization	O
algorithms	O
address	O
this	O
issue	O
by	O
computing	O
an	O
update	O
that	O
takes	O
these	O
second-order	O
interactions	O
into	O
account	O
,	O
but	O
we	O
can	O
see	O
that	O
in	O
very	O
deep	O
networks	O
,	O
even	O
higher-order	O
interactions	O
can	O
be	O
signiﬁcant	O
.	O
even	O
second-order	O
optimization	O
algorithms	O
are	O
expensive	O
and	O
usually	O
require	O
numerous	O
approximations	O
that	O
prevent	O
them	O
from	O
truly	O
accounting	O
for	O
all	O
signiﬁcant	O
second-order	O
interactions	O
.	O
building	O
an	O
n-th	O
order	O
optimization	O
algorithm	O
for	O
n	O
>	O
2	O
thus	O
seems	O
hopeless	O
.	O
what	O
can	O
we	O
do	O
instead	O
?	O
batch	O
normalization	O
provides	O
an	O
elegant	O
way	O
of	O
reparametrizing	O
almost	O
any	O
deep	O
network	O
.	O
the	O
reparametrization	O
signiﬁcantly	O
reduces	O
the	O
problem	O
of	O
coordinating	O
updates	O
across	O
many	O
layers	O
.	O
batch	O
normalization	O
can	O
be	O
applied	O
to	O
any	O
input	O
or	O
hidden	O
layer	O
in	O
a	O
network	O
.	O
let	O
h	O
be	O
a	O
minibatch	O
of	O
activations	O
of	O
the	O
layer	O
to	O
normalize	O
,	O
arranged	O
as	O
a	O
design	O
matrix	O
,	O
with	O
the	O
activations	O
for	O
each	O
example	O
appearing	O
in	O
a	O
row	O
of	O
the	O
matrix	O
.	O
to	O
normalize	O
,	O
we	O
replace	O
it	O
with	O
h	O
	O
h	O
=	O
h	O
µ	O
−	O
σ	O
,	O
(	O
8.35	O
)	O
where	O
µ	O
is	O
a	O
vector	O
containing	O
the	O
mean	O
of	O
each	O
unit	O
and	O
σ	O
is	O
a	O
vector	O
containing	O
the	O
standard	O
deviation	O
of	O
each	O
unit	O
.	O
the	O
arithmetic	O
here	O
is	O
based	O
on	O
broadcasting	O
the	O
vector	O
µ	O
and	O
the	O
vector	O
σ	O
to	O
be	O
applied	O
to	O
every	O
row	O
of	O
the	O
matrix	O
h	O
.	O
within	O
each	O
row	O
,	O
the	O
arithmetic	O
is	O
element-wise	O
,	O
so	O
hi	O
,	O
j	O
is	O
normalized	O
by	O
subtracting	O
µj	O
318	O
at	O
training	O
time	O
,	O
and	O
	O
µ	O
=	O
	O
	O
i	O
1	O
m	O
1	O
m	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
and	O
dividing	O
by	O
σj	O
.	O
the	O
rest	O
of	O
the	O
network	O
then	O
operates	O
on	O
h	O
same	O
way	O
that	O
the	O
original	O
network	O
operated	O
on	O
.h	O
	O
in	O
exactly	O
the	O
hi	O
,	O
:	O
(	O
8.36	O
)	O
−	O
2	O
(	O
)	O
h	O
µ	O
i	O
,	O
i	O
δ	O
+	O
(	O
8.37	O
)	O
σ	O
=	O
−	O
√	O
8	O
imposed	O
to	O
avoid	O
encountering	O
where	O
δ	O
is	O
a	O
small	O
positive	O
value	O
such	O
as	O
10	O
z	O
at	O
z	O
=	O
0.	O
crucially	O
,	O
we	O
back-propagate	O
through	O
the	O
undeﬁned	O
gradient	O
of	O
these	O
operations	O
for	O
computing	O
the	O
mean	O
and	O
the	O
standard	O
deviation	O
,	O
and	O
for	O
applying	O
them	O
to	O
normalize	O
h.	O
this	O
means	O
that	O
the	O
gradient	O
will	O
never	O
propose	O
an	O
operation	O
that	O
acts	O
simply	O
to	O
increase	O
the	O
standard	O
deviation	O
or	O
mean	O
of	O
hi	O
;	O
the	O
normalization	O
operations	O
remove	O
the	O
eﬀect	O
of	O
such	O
an	O
action	O
and	O
zero	O
out	O
its	O
component	O
in	O
the	O
gradient	O
.	O
this	O
was	O
a	O
major	O
innovation	O
of	O
the	O
batch	O
normalization	O
approach	O
.	O
previous	O
approaches	O
had	O
involved	O
adding	O
penalties	O
to	O
the	O
cost	O
function	O
to	O
encourage	O
units	O
to	O
have	O
normalized	O
activation	O
statistics	O
or	O
involved	O
intervening	O
to	O
renormalize	O
unit	O
statistics	O
after	O
each	O
gradient	O
descent	B
step	O
.	O
the	O
former	O
approach	O
usually	O
resulted	O
in	O
imperfect	O
normalization	O
and	O
the	O
latter	O
usually	O
resulted	O
in	O
signiﬁcant	O
wasted	O
time	O
as	O
the	O
learning	O
algorithm	O
repeatedly	O
proposed	O
changing	O
the	O
mean	O
and	O
variance	O
and	O
the	O
normalization	O
step	O
repeatedly	O
undid	O
this	O
change	O
.	O
batch	O
normalization	O
reparametrizes	O
the	O
model	B
to	O
make	O
some	O
units	O
always	O
be	O
standardized	O
by	O
deﬁnition	O
,	O
deftly	O
sidestepping	O
both	O
problems	O
.	O
at	O
test	O
time	O
,	O
µ	O
and	O
σ	O
may	O
be	O
replaced	O
by	O
running	O
averages	O
that	O
were	O
collected	O
during	O
training	O
time	O
.	O
this	O
allows	O
the	O
model	B
to	O
be	O
evaluated	O
on	O
a	O
single	O
example	O
,	O
without	O
needing	O
to	O
use	O
deﬁnitions	O
of	O
µ	O
and	O
σ	O
that	O
depend	O
on	O
an	O
entire	O
minibatch	O
.	O
revisiting	O
the	O
ˆy	O
=	O
xw1w2	O
.	O
.	O
.	O
wl	O
example	O
,	O
we	O
see	O
that	O
we	O
can	O
mostly	O
resolve	O
the	O
−	O
diﬃculties	O
in	O
learning	O
this	O
model	B
by	O
normalizing	O
hl	O
1.	O
suppose	O
that	O
x	O
is	O
drawn	O
−	O
from	O
a	O
unit	O
gaussian	O
.	O
then	O
hl	O
1	O
will	O
also	O
come	O
from	O
a	O
gaussian	O
,	O
because	O
the	O
−	O
transformation	O
from	O
x	O
to	O
hl	O
is	O
linear	O
.	O
however	O
,	O
h	O
l	O
1	O
will	O
no	O
longer	O
have	O
zero	O
mean	O
and	O
unit	O
variance	O
.	O
after	O
applying	O
batch	O
normalization	O
,	O
we	O
obtain	O
the	O
normalized	O
−	O
ˆhl	O
1	O
that	O
restores	O
the	O
zero	O
mean	O
and	O
unit	O
variance	O
properties	O
.	O
for	O
almost	O
any	O
−	O
update	O
to	O
the	O
lower	O
layers	O
,	O
ˆhl	O
1	O
will	O
remain	O
a	O
unit	O
gaussian	O
.	O
the	O
output	O
ˆy	O
may	O
−	O
ˆh	O
l	O
then	O
be	O
learned	O
as	O
a	O
simple	O
linear	O
function	O
ˆy	O
=	O
wl	O
1.	O
learning	O
in	O
this	O
model	B
is	O
now	O
very	O
simple	O
because	O
the	O
parameters	O
at	O
the	O
lower	O
layers	O
simply	O
do	O
not	O
have	O
an	O
eﬀect	O
in	O
most	O
cases	O
;	O
their	O
output	O
is	O
always	O
renormalized	O
to	O
a	O
unit	O
gaussian	O
.	O
in	O
some	O
corner	O
cases	O
,	O
the	O
lower	O
layers	O
can	O
have	O
an	O
eﬀect	O
.	O
changing	O
one	O
of	O
the	O
lower	O
can	O
make	O
the	O
output	O
become	O
degenerate	O
,	O
and	O
changing	O
the	O
sign	O
layer	O
weights	O
to	O
0	O
319	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
−	O
of	O
one	O
of	O
the	O
lower	O
weights	O
can	O
ﬂip	O
the	O
relationship	O
between	O
ˆhl	O
1	O
and	O
y.	O
these	O
situations	O
are	O
very	O
rare	O
.	O
without	O
normalization	O
,	O
nearly	O
every	O
update	O
would	O
have	O
−	O
an	O
extreme	O
eﬀect	O
on	O
the	O
statistics	O
of	O
hl	O
1	O
.	O
batch	O
normalization	O
has	O
thus	O
made	O
this	O
model	B
signiﬁcantly	O
easier	O
to	O
learn	O
.	O
in	O
this	O
example	O
,	O
the	O
ease	O
of	O
learning	O
of	O
course	O
came	O
at	O
the	O
cost	O
of	O
making	O
the	O
lower	O
layers	O
useless	O
.	O
in	O
our	O
linear	O
example	O
,	O
the	O
lower	O
layers	O
no	O
longer	O
have	O
any	O
harmful	O
eﬀect	O
,	O
but	O
they	O
also	O
no	O
longer	O
have	O
any	O
beneﬁcial	O
eﬀect	O
.	O
this	O
is	O
because	O
we	O
have	O
normalized	O
out	O
the	O
ﬁrst	O
and	O
second	O
order	O
statistics	O
,	O
which	O
is	O
all	O
that	O
a	O
linear	O
network	O
can	O
inﬂuence	O
.	O
in	O
a	O
deep	O
neural	O
network	O
with	O
nonlinear	O
activation	O
functions	O
,	O
the	O
lower	O
layers	O
can	O
perform	O
nonlinear	O
transformations	O
of	O
the	O
data	O
,	O
so	O
they	O
remain	O
useful	O
.	O
batch	O
normalization	O
acts	O
to	O
standardize	O
only	O
the	O
mean	O
and	O
variance	O
of	O
each	O
unit	O
in	O
order	O
to	O
stabilize	O
learning	O
,	O
but	O
allows	O
the	O
relationships	O
between	O
units	O
and	O
the	O
nonlinear	O
statistics	O
of	O
a	O
single	O
unit	O
to	O
change	O
.	O
because	O
the	O
ﬁnal	O
layer	O
of	O
the	O
network	O
is	O
able	O
to	O
learn	O
a	O
linear	O
transformation	O
,	O
we	O
may	O
actually	O
wish	O
to	O
remove	O
all	O
linear	O
relationships	O
between	O
units	O
within	O
a	O
layer	O
.	O
indeed	O
,	O
this	O
is	O
the	O
approach	O
taken	O
by	O
)	O
,	O
who	O
provided	O
the	O
inspiration	O
for	O
batch	O
normalization	O
.	O
unfortunately	O
,	O
eliminating	O
all	O
linear	O
interactions	O
is	O
much	O
more	O
expensive	O
than	O
standardizing	O
the	O
mean	O
and	O
standard	O
deviation	O
of	O
each	O
individual	O
unit	O
,	O
and	O
so	O
far	O
batch	O
normalization	O
remains	O
the	O
most	O
practical	O
approach	O
.	O
desjardins	O
et	O
al	O
.	O
2015	O
(	O
	O
+	O
β	O
rather	O
than	O
simply	O
the	O
normalized	O
h	O
normalizing	O
the	O
mean	O
and	O
standard	O
deviation	O
of	O
a	O
unit	O
can	O
reduce	O
the	O
expressive	O
power	O
of	O
the	O
neural	O
network	O
containing	O
that	O
unit	O
.	O
in	O
order	O
to	O
maintain	O
the	O
expressive	O
power	O
of	O
the	O
network	O
,	O
it	O
is	O
common	O
to	O
replace	O
the	O
batch	O
of	O
hidden	O
unit	O
	O
activations	O
h	O
with	O
γh	O
.	O
the	O
variables	O
γ	O
and	O
β	O
are	O
learned	O
parameters	O
that	O
allow	O
the	O
new	O
variable	O
to	O
have	O
any	O
mean	O
and	O
standard	O
deviation	O
.	O
at	O
ﬁrst	O
glance	O
,	O
this	O
may	O
seem	O
useless—why	O
did	O
we	O
set	O
the	O
mean	O
to	O
0	O
,	O
and	O
then	O
introduce	O
a	O
parameter	O
that	O
allows	O
it	O
to	O
be	O
set	O
back	O
to	O
any	O
arbitrary	O
value	O
β	O
?	O
the	O
answer	O
is	O
that	O
the	O
new	O
parametrization	O
can	O
represent	O
the	O
same	O
family	O
of	O
functions	O
of	O
the	O
input	O
as	O
the	O
old	O
parametrization	O
,	O
but	O
the	O
new	O
parametrization	O
has	O
diﬀerent	O
learning	O
dynamics	O
.	O
in	O
the	O
old	O
parametrization	O
,	O
the	O
mean	O
of	O
h	O
was	O
determined	O
by	O
a	O
complicated	O
interaction	O
between	O
the	O
parameters	O
in	O
the	O
layers	O
below	O
h.	O
in	O
the	O
new	O
parametrization	O
,	O
the	O
mean	O
of	O
γh	O
+	O
β	O
is	O
determined	O
solely	O
by	O
β.	O
the	O
new	O
parametrization	O
is	O
much	O
easier	O
to	O
learn	O
with	O
gradient	O
descent	B
.	O
	O
most	O
neural	O
network	O
layers	O
take	O
the	O
form	O
of	O
φ	O
(	O
xw	O
+	O
b	O
)	O
where	O
φ	O
is	O
some	O
ﬁxed	O
nonlinear	O
activation	O
function	O
such	O
as	O
the	O
rectiﬁed	O
linear	O
transformation	O
.	O
it	O
is	O
natural	O
to	O
wonder	O
whether	O
we	O
should	O
apply	O
batch	O
normalization	O
to	O
the	O
input	O
x	O
,	O
or	O
to	O
the	O
transformed	O
value	O
xw	O
+	O
b.	O
)	O
recommend	O
ioﬀe	O
and	O
szegedy	O
2015	O
(	O
320	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
the	O
latter	O
.	O
more	O
speciﬁcally	O
,	O
xw	O
+	O
b	O
should	O
be	O
replaced	O
by	O
a	O
normalized	O
version	O
of	O
xw	O
.	O
the	O
bias	O
term	O
should	O
be	O
omitted	O
because	O
it	O
becomes	O
redundant	O
with	O
the	O
β	O
parameter	O
applied	O
by	O
the	O
batch	O
normalization	O
reparametrization	O
.	O
the	O
input	O
to	O
a	O
layer	O
is	O
usually	O
the	O
output	O
of	O
a	O
nonlinear	O
activation	O
function	O
such	O
as	O
the	O
rectiﬁed	O
linear	O
function	O
in	O
a	O
previous	O
layer	O
.	O
the	O
statistics	O
of	O
the	O
input	O
are	O
thus	O
more	O
non-gaussian	O
and	O
less	O
amenable	O
to	O
standardization	O
by	O
linear	O
operations	O
.	O
in	O
convolutional	O
networks	O
,	O
described	O
in	O
chapter	O
,	O
it	O
is	O
important	O
to	O
apply	O
the	O
9	O
same	O
normalizing	O
µ	O
and	O
σ	O
at	O
every	O
spatial	O
location	O
within	O
a	O
feature	O
map	O
,	O
so	O
that	O
the	O
statistics	O
of	O
the	O
feature	O
map	O
remain	O
the	O
same	O
regardless	O
of	O
spatial	O
location	O
.	O
8.7.2	O
coordinate	O
descent	B
in	O
some	O
cases	O
,	O
it	O
may	O
be	O
possible	O
to	O
solve	O
an	O
optimization	O
problem	O
quickly	O
by	O
breaking	O
it	O
into	O
separate	O
pieces	O
.	O
if	O
we	O
minimize	O
f	O
(	O
x	O
)	O
with	O
respect	O
to	O
a	O
single	O
variable	O
xi	O
,	O
then	O
minimize	O
it	O
with	O
respect	O
to	O
another	O
variable	O
xj	O
and	O
so	O
on	O
,	O
repeatedly	O
cycling	O
through	O
all	O
variables	O
,	O
we	O
are	O
guaranteed	O
to	O
arrive	O
at	O
a	O
(	O
local	O
)	O
minimum	O
.	O
this	O
practice	O
is	O
known	O
as	O
coordinate	O
descent	B
,	O
because	O
we	O
optimize	O
one	O
coordinate	O
at	O
a	O
time	O
.	O
more	O
generally	O
,	O
block	O
coordinate	O
descent	B
refers	O
to	O
minimizing	O
with	O
respect	O
to	O
a	O
subset	O
of	O
the	O
variables	O
simultaneously	O
.	O
the	O
term	O
“	O
coordinate	O
descent	B
”	O
is	O
often	O
used	O
to	O
refer	O
to	O
block	O
coordinate	O
descent	B
as	O
well	O
as	O
the	O
strictly	O
individual	O
coordinate	O
descent	B
.	O
coordinate	O
descent	B
makes	O
the	O
most	O
sense	O
when	O
the	O
diﬀerent	O
variables	O
in	O
the	O
optimization	O
problem	O
can	O
be	O
clearly	O
separated	O
into	O
groups	O
that	O
play	O
relatively	O
isolated	O
roles	O
,	O
or	O
when	O
optimization	O
with	O
respect	O
to	O
one	O
group	O
of	O
variables	O
is	O
signiﬁcantly	O
more	O
eﬃcient	O
than	O
optimization	O
with	O
respect	O
to	O
all	O
of	O
the	O
variables	O
.	O
for	O
example	O
,	O
consider	O
the	O
cost	O
function	O
|	O
	O
	O
	O
	O
2	O
j	O
(	O
h	O
w	O
)	O
=	O
,	O
(	O
8.38	O
)	O
|	O
hi	O
,	O
j	O
+	O
i	O
,	O
j	O
i	O
,	O
j	O
−	O
	O
x	O
w	O
h	O
.	O
i	O
,	O
j	O
this	O
function	O
describes	O
a	O
learning	O
problem	O
called	O
sparse	O
coding	O
,	O
where	O
the	O
goal	O
is	O
to	O
ﬁnd	O
a	O
weight	O
matrix	O
w	O
that	O
can	O
linearly	O
decode	O
a	O
matrix	O
of	O
activation	O
values	O
h	O
to	O
reconstruct	O
the	O
training	O
set	O
x.	O
most	O
applications	O
of	O
sparse	O
coding	O
also	O
involve	O
weight	O
decay	O
or	O
a	O
constraint	O
on	O
the	O
norms	O
of	O
the	O
columns	O
of	O
w	O
,	O
in	O
order	O
to	O
prevent	O
the	O
pathological	O
solution	O
with	O
extremely	O
small	O
and	O
large	O
w	O
h	O
.	O
the	O
function	O
j	O
is	O
not	O
convex	O
.	O
however	O
,	O
we	O
can	O
divide	O
the	O
inputs	O
to	O
the	O
training	O
algorithm	O
into	O
two	O
sets	O
:	O
the	O
dictionary	O
parameters	O
w	O
and	O
the	O
code	O
representations	O
h	O
.	O
minimizing	O
the	O
objective	O
function	O
with	O
respect	O
to	O
either	O
one	O
of	O
these	O
sets	O
of	O
variables	O
is	O
a	O
convex	O
problem	O
.	O
block	O
coordinate	O
descent	B
thus	O
gives	O
321	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
us	O
an	O
optimization	O
strategy	O
that	O
allows	O
us	O
to	O
use	O
eﬃcient	O
convex	O
optimization	O
algorithms	O
,	O
by	O
alternating	O
between	O
optimizing	O
w	O
with	O
h	O
ﬁxed	O
,	O
then	O
optimizing	O
h	O
wwith	O
ﬁxed	O
.	O
	O
	O
x2	O
)	O
2	O
+	O
α	O
x2	O
1	O
+	O
x	O
2	O
2	O
coordinate	O
descent	B
is	O
not	O
a	O
very	O
good	O
strategy	O
when	O
the	O
value	O
of	O
one	O
variable	O
−	O
strongly	O
inﬂuences	O
the	O
optimal	O
value	O
of	O
another	O
variable	O
,	O
as	O
in	O
the	O
function	O
f	O
(	O
x	O
)	O
=	O
(	O
x1	O
where	O
α	O
is	O
a	O
positive	O
constant	O
.	O
the	O
ﬁrst	O
term	O
encourages	O
the	O
two	O
variables	O
to	O
have	O
similar	O
value	O
,	O
while	O
the	O
second	O
term	O
encourages	O
them	O
to	O
be	O
near	O
zero	O
.	O
the	O
solution	O
is	O
to	O
set	O
both	O
to	O
zero	O
.	O
newton	O
’	O
s	O
method	O
can	O
solve	O
the	O
problem	O
in	O
a	O
single	O
step	O
because	O
it	O
is	O
a	O
positive	O
deﬁnite	O
quadratic	O
problem	O
.	O
however	O
,	O
for	O
small	O
α	O
,	O
coordinate	O
descent	B
will	O
make	O
very	O
slow	O
progress	O
because	O
the	O
ﬁrst	O
term	O
does	O
not	O
allow	O
a	O
single	O
variable	O
to	O
be	O
changed	O
to	O
a	O
value	O
that	O
diﬀers	O
signiﬁcantly	O
from	O
the	O
current	O
value	O
of	O
the	O
other	O
variable	O
.	O
8.7.3	O
polyak	O
averaging	O
	O
,	O
)	O
consists	O
of	O
averaging	O
together	O
several	O
polyak	O
averaging	O
(	O
polyak	O
and	O
juditsky	O
1992	O
points	O
in	O
the	O
trajectory	O
through	O
parameter	O
space	O
visited	O
by	O
an	O
optimization	O
algorithm	O
.	O
if	O
t	O
iterations	O
of	O
gradient	O
descent	B
visit	O
points	O
θ	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
θ	O
(	O
)	O
t	O
,	O
then	O
the	O
output	O
of	O
the	O
polyak	O
averaging	O
algorithm	O
is	O
ˆθ	O
(	O
)	O
t	O
=	O
1	O
i	O
θ	O
(	O
)	O
i	O
.	O
on	O
some	O
problem	O
t	O
classes	O
,	O
such	O
as	O
gradient	O
descent	B
applied	O
to	O
convex	O
problems	O
,	O
this	O
approach	O
has	O
strong	O
convergence	O
guarantees	O
.	O
when	O
applied	O
to	O
neural	O
networks	O
,	O
its	O
justiﬁcation	O
is	O
more	O
heuristic	O
,	O
but	O
it	O
performs	O
well	O
in	O
practice	O
.	O
the	O
basic	O
idea	O
is	O
that	O
the	O
optimization	O
algorithm	O
may	O
leap	O
back	O
and	O
forth	O
across	O
a	O
valley	O
several	O
times	O
without	O
ever	O
visiting	O
a	O
point	O
near	O
the	O
bottom	O
of	O
the	O
valley	O
.	O
the	O
average	O
of	O
all	O
of	O
the	O
locations	O
on	O
either	O
side	O
should	O
be	O
close	O
to	O
the	O
bottom	O
of	O
the	O
valley	O
though	O
.	O
in	O
non-convex	O
problems	O
,	O
the	O
path	O
taken	O
by	O
the	O
optimization	O
trajectory	O
can	O
be	O
very	O
complicated	O
and	O
visit	O
many	O
diﬀerent	O
regions	O
.	O
including	O
points	O
in	O
parameter	O
space	O
from	O
the	O
distant	O
past	O
that	O
may	O
be	O
separated	O
from	O
the	O
current	O
point	O
by	O
large	O
barriers	O
in	O
the	O
cost	O
function	O
does	O
not	O
seem	O
like	O
a	O
useful	O
behavior	O
.	O
as	O
a	O
result	O
,	O
when	O
applying	O
polyak	O
averaging	O
to	O
non-convex	O
problems	O
,	O
it	O
is	O
typical	O
to	O
use	O
an	O
exponentially	O
decaying	O
running	O
average	O
:	O
−	O
ˆθ	O
(	O
)	O
t	O
=	O
α	O
ˆθ	O
(	O
1	O
)	O
t	O
(	O
8.39	O
)	O
α	O
θ	O
(	O
)	O
t	O
.	O
+	O
(	O
1	O
−	O
)	O
the	O
running	O
average	O
approach	O
is	O
used	O
in	O
numerous	O
applications	O
.	O
see	O
szegedy	O
et	O
al	O
.	O
(	O
2015	O
)	O
for	O
a	O
recent	O
example	O
.	O
322	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
8.7.4	O
supervised	O
pretraining	O
sometimes	O
,	O
directly	O
training	O
a	O
model	B
to	O
solve	O
a	O
speciﬁc	O
task	O
can	O
be	O
too	O
ambitious	O
if	O
the	O
model	B
is	O
complex	O
and	O
hard	O
to	O
optimize	O
or	O
if	O
the	O
task	O
is	O
very	O
diﬃcult	O
.	O
it	O
is	O
sometimes	O
more	O
eﬀective	O
to	O
train	O
a	O
simpler	O
model	B
to	O
solve	O
the	O
task	O
,	O
then	O
make	O
the	O
model	B
more	O
complex	O
.	O
it	O
can	O
also	O
be	O
more	O
eﬀective	O
to	O
train	O
the	O
model	B
to	O
solve	O
a	O
simpler	O
task	O
,	O
then	O
move	O
on	O
to	O
confront	O
the	O
ﬁnal	O
task	O
.	O
these	O
strategies	O
that	O
involve	O
training	O
simple	O
models	O
on	O
simple	O
tasks	O
before	O
confronting	O
the	O
challenge	O
of	O
training	O
the	O
desired	O
model	B
to	O
perform	O
the	O
desired	O
task	O
are	O
collectively	O
known	O
as	O
pretraining	O
.	O
greedy	O
algorithms	O
break	O
a	O
problem	O
into	O
many	O
components	O
,	O
then	O
solve	O
for	O
the	O
optimal	O
version	O
of	O
each	O
component	O
in	O
isolation	O
.	O
unfortunately	O
,	O
combining	O
the	O
individually	O
optimal	O
components	O
is	O
not	O
guaranteed	O
to	O
yield	O
an	O
optimal	O
complete	O
solution	O
.	O
however	O
,	O
greedy	O
algorithms	O
can	O
be	O
computationally	O
much	O
cheaper	O
than	O
algorithms	O
that	O
solve	O
for	O
the	O
best	O
joint	O
solution	O
,	O
and	O
the	O
quality	O
of	O
a	O
greedy	O
solution	O
is	O
often	O
acceptable	O
if	O
not	O
optimal	O
.	O
greedy	O
algorithms	O
may	O
also	O
be	O
followed	O
by	O
a	O
ﬁne-tuning	B
stage	O
in	O
which	O
a	O
joint	O
optimization	O
algorithm	O
searches	O
for	O
an	O
optimal	O
solution	O
to	O
the	O
full	O
problem	O
.	O
initializing	O
the	O
joint	O
optimization	O
algorithm	O
with	O
a	O
greedy	O
solution	O
can	O
greatly	O
speed	O
it	O
up	O
and	O
improve	O
the	O
quality	O
of	O
the	O
solution	O
it	O
ﬁnds	O
.	O
pretraining	O
,	O
and	O
especially	O
greedy	O
pretraining	O
,	O
algorithms	O
are	O
ubiquitous	O
in	O
deep	O
learning	O
.	O
in	O
this	O
section	O
,	O
we	O
describe	O
speciﬁcally	O
those	O
pretraining	O
algorithms	O
that	O
break	O
supervised	O
learning	O
problems	O
into	O
other	O
simpler	O
supervised	O
learning	O
problems	O
.	O
this	O
approach	O
is	O
known	O
as	O
.	O
greedy	O
supervised	O
pretraining	O
,	O
8.7	O
in	O
the	O
original	O
(	O
bengio	O
et	O
al	O
.	O
2007	O
)	O
version	O
of	O
greedy	O
supervised	O
pretraining	O
,	O
each	O
stage	O
consists	O
of	O
a	O
supervised	O
learning	O
training	O
task	O
involving	O
only	O
a	O
subset	O
of	O
the	O
layers	O
in	O
the	O
ﬁnal	O
neural	O
network	O
.	O
an	O
example	O
of	O
greedy	O
supervised	O
pretraining	O
is	O
illustrated	O
in	O
ﬁgure	O
,	O
in	O
which	O
each	O
added	O
hidden	O
layer	O
is	O
pretrained	O
as	O
part	O
of	O
a	O
shallow	O
supervised	O
mlp	O
,	O
taking	O
as	O
input	O
the	O
output	O
of	O
the	O
previously	O
trained	O
hidden	O
layer	O
.	O
instead	O
of	O
pretraining	O
one	O
layer	O
at	O
a	O
time	O
,	O
simonyan	O
and	O
zisserman	O
(	O
)	O
pretrain	O
a	O
deep	O
convolutional	O
network	O
(	O
eleven	O
weight	O
layers	O
)	O
and	O
then	O
use	O
2015	O
the	O
ﬁrst	O
four	O
and	O
last	O
three	O
layers	O
from	O
this	O
network	O
to	O
initialize	O
even	O
deeper	O
networks	O
(	O
with	O
up	O
to	O
nineteen	O
layers	O
of	O
weights	O
)	O
.	O
the	O
middle	O
layers	O
of	O
the	O
new	O
,	O
very	O
deep	O
network	O
are	O
initialized	O
randomly	O
.	O
the	O
new	O
network	O
is	O
then	O
jointly	O
trained	O
.	O
another	O
option	O
,	O
explored	O
by	O
yu	O
of	O
the	O
previously	O
trained	O
mlps	O
,	O
as	O
well	O
as	O
the	O
raw	O
input	O
,	O
as	O
inputs	O
for	O
each	O
added	O
stage.	O
)	O
is	O
to	O
use	O
the	O
outputs	O
et	O
al	O
.	O
(	O
2010	O
why	O
would	O
greedy	O
supervised	O
pretraining	O
help	O
?	O
the	O
hypothesis	O
initially	O
)	O
is	O
that	O
it	O
helps	O
to	O
provide	O
better	O
guidance	O
to	O
the	O
bengio	O
et	O
al	O
.	O
2007	O
discussed	O
by	O
(	O
323	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
u	O
(	O
1	O
)	O
u	O
(	O
1	O
)	O
w	O
(	O
1	O
)	O
w	O
(	O
1	O
)	O
yy	O
h	O
(	O
1	O
)	O
h	O
(	O
1	O
)	O
xx	O
(	O
a	O
)	O
h	O
(	O
2	O
)	O
h	O
(	O
2	O
)	O
w	O
(	O
2	O
)	O
w	O
(	O
2	O
)	O
u	O
(	O
2	O
)	O
u	O
(	O
2	O
)	O
h	O
(	O
1	O
)	O
h	O
(	O
1	O
)	O
w	O
(	O
1	O
)	O
w	O
(	O
1	O
)	O
u	O
(	O
1	O
)	O
u	O
(	O
1	O
)	O
yy	O
yy	O
xx	O
(	O
c	O
)	O
h	O
(	O
1	O
)	O
h	O
(	O
1	O
)	O
w	O
(	O
1	O
)	O
w	O
(	O
1	O
)	O
u	O
(	O
1	O
)	O
u	O
(	O
1	O
)	O
yy	O
xx	O
(	O
b	O
)	O
y	O
h	O
(	O
2	O
)	O
h	O
(	O
2	O
)	O
h	O
(	O
1	O
)	O
h	O
(	O
1	O
)	O
u	O
(	O
2	O
)	O
u	O
(	O
2	O
)	O
w	O
(	O
2	O
)	O
w	O
(	O
2	O
)	O
w	O
(	O
1	O
)	O
w	O
(	O
1	O
)	O
u	O
(	O
1	O
)	O
u	O
(	O
1	O
)	O
yy	O
xx	O
(	O
d	O
)	O
bengio	O
et	O
al	O
.	O
2007	O
)	O
.	O
figure	O
8.7	O
:	O
illustration	O
of	O
one	O
form	O
of	O
greedy	O
supervised	O
pretraining	O
(	O
(	O
a	O
)	O
we	O
start	O
by	O
training	O
a	O
suﬃciently	O
shallow	O
architecture	O
.	O
another	O
drawing	O
of	O
the	O
same	O
architecture	O
.	O
we	O
keep	O
only	O
the	O
input-to-hidden	O
layer	O
of	O
the	O
original	O
network	O
and	O
discard	O
the	O
hidden-to-output	O
layer	O
.	O
we	O
send	O
the	O
output	O
of	O
the	O
ﬁrst	O
hidden	O
layer	O
as	O
input	O
to	O
another	O
supervised	O
single	O
hidden	O
layer	O
mlp	O
that	O
is	O
trained	O
with	O
the	O
same	O
objective	O
as	O
the	O
ﬁrst	O
network	O
was	O
,	O
thus	O
adding	O
a	O
second	O
hidden	O
layer	O
.	O
this	O
can	O
be	O
repeated	O
for	O
as	O
many	O
layers	O
as	O
desired	O
.	O
another	O
drawing	O
of	O
the	O
result	O
,	O
viewed	O
as	O
a	O
feedforward	O
network	O
.	O
to	O
further	O
improve	O
the	O
optimization	O
,	O
we	O
can	O
jointly	O
ﬁne-tune	O
all	O
the	O
layers	O
,	O
either	O
only	O
at	O
the	O
end	O
or	O
at	O
each	O
stage	O
of	O
this	O
process	O
.	O
(	O
b	O
)	O
,	O
(	O
c	O
)	O
(	O
d	O
)	O
324	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
intermediate	O
levels	O
of	O
a	O
deep	O
hierarchy	O
.	O
in	O
general	O
,	O
pretraining	O
may	O
help	O
both	O
in	O
terms	O
of	O
optimization	O
and	O
in	O
terms	O
of	O
generalization	O
.	O
2014	O
et	O
al	O
.	O
(	O
an	O
approach	O
related	O
to	O
supervised	O
pretraining	O
extends	O
the	O
idea	O
to	O
the	O
context	O
)	O
pretrain	O
a	O
deep	O
convolutional	O
net	O
with	O
8	O
of	O
transfer	O
learning	O
:	O
yosinski	O
layers	O
of	O
weights	O
on	O
a	O
set	O
of	O
tasks	O
(	O
a	O
subset	O
of	O
the	O
1000	O
imagenet	O
object	O
categories	O
)	O
and	O
then	O
initialize	O
a	O
same-size	O
network	O
with	O
the	O
ﬁrst	O
k	O
layers	O
of	O
the	O
ﬁrst	O
net	O
.	O
all	O
the	O
layers	O
of	O
the	O
second	O
network	O
(	O
with	O
the	O
upper	O
layers	O
initialized	O
randomly	O
)	O
are	O
then	O
jointly	O
trained	O
to	O
perform	O
a	O
diﬀerent	O
set	O
of	O
tasks	O
(	O
another	O
subset	O
of	O
the	O
1000	O
imagenet	O
object	O
categories	O
)	O
,	O
with	O
fewer	O
training	O
examples	O
than	O
for	O
the	O
ﬁrst	O
set	O
of	O
tasks	O
.	O
other	O
approaches	O
to	O
transfer	O
learning	O
with	O
neural	O
networks	O
are	O
discussed	O
in	O
section	O
.	O
15.2	O
,	O
romero	O
et	O
al	O
.	O
2015	O
another	O
related	O
line	O
of	O
work	B
is	O
the	O
fitnets	O
(	O
)	O
approach	O
.	O
this	O
approach	O
begins	O
by	O
training	O
a	O
network	O
that	O
has	O
low	O
enough	O
depth	O
and	O
great	O
enough	O
width	O
(	O
number	O
of	O
units	O
per	O
layer	O
)	O
to	O
be	O
easy	O
to	O
train	O
.	O
this	O
network	O
then	O
becomes	O
a	O
teacher	O
for	O
a	O
second	O
network	O
,	O
designated	O
the	O
student	O
.	O
the	O
student	O
network	O
is	O
much	O
deeper	O
and	O
thinner	O
(	O
eleven	O
to	O
nineteen	O
layers	O
)	O
and	O
would	O
be	O
diﬃcult	O
to	O
train	O
with	O
sgd	O
under	O
normal	O
circumstances	O
.	O
the	O
training	O
of	O
the	O
student	O
network	O
is	O
made	O
easier	O
by	O
training	O
the	O
student	O
network	O
not	O
only	O
to	O
predict	O
the	O
output	O
for	O
the	O
original	O
task	O
,	O
but	O
also	O
to	O
predict	O
the	O
value	O
of	O
the	O
middle	O
layer	O
of	O
the	O
teacher	O
network	O
.	O
this	O
extra	O
task	O
provides	O
a	O
set	O
of	O
hints	O
about	O
how	O
the	O
hidden	O
layers	O
should	O
be	O
used	O
and	O
can	O
simplify	O
the	O
optimization	O
problem	O
.	O
additional	O
parameters	O
are	O
introduced	O
to	O
regress	O
the	O
middle	O
layer	O
of	O
the	O
5-layer	O
teacher	O
network	O
from	O
the	O
middle	O
layer	O
of	O
the	O
deeper	O
student	O
network	O
.	O
however	O
,	O
instead	O
of	O
predicting	O
the	O
ﬁnal	O
classiﬁcation	O
target	O
,	O
the	O
objective	O
is	O
to	O
predict	O
the	O
middle	O
hidden	O
layer	O
of	O
the	O
teacher	O
network	O
.	O
the	O
lower	O
layers	O
of	O
the	O
student	O
networks	O
thus	O
have	O
two	O
objectives	O
:	O
to	O
help	O
the	O
outputs	O
of	O
the	O
student	O
network	O
accomplish	O
their	O
task	O
,	O
as	O
well	O
as	O
to	O
predict	O
the	O
intermediate	O
layer	O
of	O
the	O
teacher	O
network	O
.	O
although	O
a	O
thin	O
and	O
deep	O
network	O
appears	O
to	O
be	O
more	O
diﬃcult	O
to	O
train	O
than	O
a	O
wide	O
and	O
shallow	O
network	O
,	O
the	O
thin	O
and	O
deep	O
network	O
may	O
generalize	O
better	O
and	O
certainly	O
has	O
lower	O
computational	O
cost	O
if	O
it	O
is	O
thin	O
enough	O
to	O
have	O
far	O
fewer	O
parameters	O
.	O
without	O
the	O
hints	O
on	O
the	O
hidden	O
layer	O
,	O
the	O
student	O
network	O
performs	O
very	O
poorly	O
in	O
the	O
experiments	O
,	O
both	O
on	O
the	O
training	O
and	O
test	O
set	O
.	O
hints	O
on	O
middle	O
layers	O
may	O
thus	O
be	O
one	O
of	O
the	O
tools	O
to	O
help	O
train	O
neural	O
networks	O
that	O
otherwise	O
seem	O
diﬃcult	O
to	O
train	O
,	O
but	O
other	O
optimization	O
techniques	O
or	O
changes	O
in	O
the	O
architecture	O
may	O
also	O
solve	O
the	O
problem	O
.	O
325	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
8.7.5	O
designing	O
models	O
to	O
aid	O
optimization	O
to	O
improve	O
optimization	O
,	O
the	O
best	O
strategy	O
is	O
not	O
always	O
to	O
improve	O
the	O
optimization	O
algorithm	O
.	O
instead	O
,	O
many	O
improvements	O
in	O
the	O
optimization	O
of	O
deep	O
models	O
have	O
come	O
from	O
designing	O
the	O
models	O
to	O
be	O
easier	O
to	O
optimize	O
.	O
in	O
principle	O
,	O
we	O
could	O
use	O
activation	O
functions	O
that	O
increase	O
and	O
decrease	O
in	O
jagged	O
non-monotonic	O
patterns	O
.	O
however	O
,	O
this	O
would	O
make	O
optimization	O
extremely	O
diﬃcult	O
.	O
in	O
practice	O
,	O
it	O
is	O
more	O
important	O
to	O
choose	O
a	O
model	B
family	O
that	O
is	O
easy	O
to	O
optimize	O
than	O
to	O
use	O
a	O
powerful	O
optimization	O
algorithm	O
.	O
most	O
of	O
the	O
advances	O
in	O
neural	O
network	O
learning	O
over	O
the	O
past	O
30	O
years	O
have	O
been	O
obtained	O
by	O
changing	O
the	O
model	B
family	O
rather	O
than	O
changing	O
the	O
optimization	O
procedure	O
.	O
stochastic	O
gradient	O
descent	B
with	O
momentum	O
,	O
which	O
was	O
used	O
to	O
train	O
neural	O
networks	O
in	O
the	O
1980s	O
,	O
remains	O
in	O
use	O
in	O
modern	O
state	O
of	O
the	O
art	O
neural	O
network	O
applications	O
.	O
speciﬁcally	O
,	O
modern	O
neural	O
networks	O
reﬂect	O
a	O
design	O
choice	O
to	O
use	O
linear	O
trans-	O
formations	O
between	O
layers	O
and	O
activation	O
functions	O
that	O
are	O
diﬀerentiable	O
almost	O
everywhere	O
and	O
have	O
signiﬁcant	O
slope	O
in	O
large	O
portions	O
of	O
their	O
domain	O
.	O
in	O
par-	O
ticular	O
,	O
model	B
innovations	O
like	O
the	O
lstm	O
,	O
rectiﬁed	O
linear	O
units	O
and	O
maxout	O
units	O
have	O
all	O
moved	O
toward	O
using	O
more	O
linear	O
functions	O
than	O
previous	O
models	O
like	O
deep	O
networks	O
based	O
on	O
sigmoidal	O
units	O
.	O
these	O
models	O
have	O
nice	O
properties	O
that	O
make	O
optimization	O
easier	O
.	O
the	O
gradient	O
ﬂows	O
through	O
many	O
layers	O
provided	O
that	O
the	O
jacobian	O
of	O
the	O
linear	O
transformation	O
has	O
reasonable	O
singular	O
values	O
.	O
moreover	O
,	O
linear	O
functions	O
consistently	O
increase	O
in	O
a	O
single	O
direction	O
,	O
so	O
even	O
if	O
the	O
model	B
’	O
s	O
output	O
is	O
very	O
far	O
from	O
correct	O
,	O
it	O
is	O
clear	O
simply	O
from	O
computing	O
the	O
gradient	O
which	O
direction	O
its	O
output	O
should	O
move	O
to	O
reduce	O
the	O
loss	O
function	O
.	O
in	O
other	O
words	O
,	O
modern	O
neural	O
nets	O
have	O
been	O
designed	O
so	O
that	O
their	O
local	O
gradient	O
information	O
corresponds	O
reasonably	O
well	O
to	O
moving	O
toward	O
a	O
distant	O
solution	O
.	O
other	O
model	B
design	O
strategies	O
can	O
help	O
to	O
make	O
optimization	O
easier	O
.	O
for	O
example	O
,	O
linear	O
paths	O
or	O
skip	O
connections	O
between	O
layers	O
reduce	O
the	O
length	O
of	O
the	O
shortest	O
path	O
from	O
the	O
lower	O
layer	O
’	O
s	O
parameters	O
to	O
the	O
output	O
,	O
and	O
thus	O
mitigate	O
the	O
vanishing	O
gradient	O
problem	O
(	O
srivastava	O
)	O
.	O
a	O
related	O
idea	O
to	O
skip	O
connections	O
is	O
adding	O
extra	O
copies	O
of	O
the	O
output	O
that	O
are	O
attached	O
to	O
the	O
intermediate	O
hidden	O
layers	O
of	O
the	O
network	O
,	O
as	O
in	O
googlenet	O
(	O
szegedy	O
et	O
al	O
.	O
2014a	O
)	O
and	O
deeply-supervised	O
nets	O
(	O
)	O
.	O
these	O
“	O
auxiliary	O
heads	O
”	O
are	O
trained	O
to	O
perform	O
the	O
same	O
task	O
as	O
the	O
primary	O
output	O
at	O
the	O
top	O
of	O
the	O
network	O
in	O
order	O
to	O
ensure	O
that	O
the	O
lower	O
layers	O
receive	O
a	O
large	O
gradient	O
.	O
when	O
training	O
is	O
complete	O
the	O
auxiliary	O
heads	O
may	O
be	O
discarded	O
.	O
this	O
is	O
an	O
alternative	O
to	O
the	O
pretraining	O
strategies	O
,	O
which	O
were	O
introduced	O
in	O
the	O
previous	O
section	O
.	O
in	O
this	O
way	O
,	O
one	O
can	O
train	O
jointly	O
all	O
the	O
layers	O
in	O
a	O
single	O
phase	O
but	O
change	O
the	O
architecture	O
,	O
so	O
that	O
intermediate	O
layers	O
(	O
especially	O
the	O
lower	O
ones	O
)	O
can	O
get	O
some	O
hints	O
about	O
what	O
they	O
lee	O
et	O
al	O
.	O
2014	O
,	O
et	O
al.	O
,	O
2015	O
,	O
326	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
should	O
do	O
,	O
via	O
a	O
shorter	O
path	O
.	O
these	O
hints	O
provide	O
an	O
error	O
signal	O
to	O
lower	O
layers	O
.	O
8.7.6	O
continuation	O
methods	O
and	O
curriculum	O
learning	O
8.2.7	O
as	O
argued	O
in	O
section	O
,	O
many	O
of	O
the	O
challenges	O
in	O
optimization	O
arise	O
from	O
the	O
global	O
structure	O
of	O
the	O
cost	O
function	O
and	O
can	O
not	O
be	O
resolved	O
merely	O
by	O
making	O
better	O
estimates	O
of	O
local	O
update	O
directions	O
.	O
the	O
predominant	O
strategy	O
for	O
overcoming	O
this	O
problem	O
is	O
to	O
attempt	O
to	O
initialize	O
the	O
parameters	O
in	O
a	O
region	O
that	O
is	O
connected	O
to	O
the	O
solution	O
by	O
a	O
short	O
path	O
through	O
parameter	O
space	O
that	O
local	O
descent	B
can	O
discover	O
.	O
continuation	O
methods	O
are	O
a	O
family	O
of	O
strategies	O
that	O
can	O
make	O
optimization	O
easier	O
by	O
choosing	O
initial	O
points	O
to	O
ensure	O
that	O
local	O
optimization	O
spends	O
most	O
of	O
its	O
time	O
in	O
well-behaved	O
regions	O
of	O
space	O
.	O
the	O
idea	O
behind	O
continuation	O
methods	O
is	O
{	O
}	O
to	O
construct	O
a	O
series	O
of	O
objective	O
functions	O
over	O
the	O
same	O
parameters	O
.	O
in	O
order	O
to	O
j	O
(	O
0	O
)	O
,	O
.	O
.	O
.	O
,	O
j	O
(	O
)	O
n	O
minimize	O
a	O
cost	O
function	O
j	O
(	O
θ	O
)	O
,	O
we	O
will	O
construct	O
new	O
cost	O
functions	O
.	O
these	O
cost	O
functions	O
are	O
designed	O
to	O
be	O
increasingly	O
diﬃcult	O
,	O
with	O
j	O
(	O
0	O
)	O
being	O
fairly	O
easy	O
to	O
minimize	O
,	O
and	O
j	O
(	O
)	O
n	O
,	O
the	O
most	O
diﬃcult	O
,	O
being	O
j	O
(	O
θ	O
)	O
,	O
the	O
true	O
cost	O
function	O
motivating	O
the	O
entire	O
process	O
.	O
when	O
we	O
say	O
that	O
j	O
(	O
)	O
i	O
,	O
we	O
mean	O
that	O
it	O
is	O
well	O
behaved	O
over	O
more	O
of	O
θ	O
space	O
.	O
a	O
random	O
initialization	O
is	O
more	O
likely	O
to	O
land	O
in	O
the	O
region	O
where	O
local	O
descent	B
can	O
minimize	O
the	O
cost	O
function	O
successfully	O
because	O
this	O
region	O
is	O
larger	O
.	O
the	O
series	O
of	O
cost	O
functions	O
are	O
designed	O
so	O
that	O
a	O
solution	O
to	O
one	O
is	O
a	O
good	O
initial	O
point	O
of	O
the	O
next	O
.	O
we	O
thus	O
begin	O
by	O
solving	O
an	O
easy	O
problem	O
then	O
reﬁne	O
the	O
solution	O
to	O
solve	O
incrementally	O
harder	O
problems	O
until	O
we	O
arrive	O
at	O
a	O
solution	O
to	O
the	O
true	O
underlying	O
problem	O
.	O
is	O
easier	O
than	O
j	O
(	O
+1	O
)	O
i	O
(	O
traditional	O
continuation	O
methods	O
(	O
predating	O
the	O
use	O
of	O
continuation	O
methods	O
for	O
neural	O
network	O
training	O
)	O
are	O
usually	O
based	O
on	O
smoothing	O
the	O
objective	O
function	O
.	O
see	O
wu	O
1997	O
)	O
for	O
an	O
example	O
of	O
such	O
a	O
method	O
and	O
a	O
review	O
of	O
some	O
related	O
methods	O
.	O
continuation	O
methods	O
are	O
also	O
closely	O
related	O
to	O
simulated	O
annealing	O
,	O
which	O
adds	O
noise	O
to	O
the	O
parameters	O
(	O
kirkpatrick	O
)	O
.	O
continuation	O
methods	O
have	O
been	O
extremely	O
successful	O
in	O
recent	O
years	O
.	O
see	O
mobahi	O
and	O
fisher	O
(	O
2015	O
)	O
for	O
an	O
overview	O
of	O
recent	O
literature	O
,	O
especially	O
for	O
ai	O
applications	O
.	O
et	O
al.	O
,	O
1983	O
continuation	O
methods	O
traditionally	O
were	O
mostly	O
designed	O
with	O
the	O
goal	O
of	O
overcoming	O
the	O
challenge	O
of	O
local	O
minima	O
.	O
speciﬁcally	O
,	O
they	O
were	O
designed	O
to	O
reach	O
a	O
global	O
minimum	O
despite	O
the	O
presence	O
of	O
many	O
local	O
minima	O
.	O
to	O
do	O
so	O
,	O
these	O
continuation	O
methods	O
would	O
construct	O
easier	O
cost	O
functions	O
by	O
“	O
blurring	O
”	O
the	O
original	O
cost	O
function	O
.	O
this	O
blurring	O
operation	O
can	O
be	O
done	O
by	O
approximating	O
j	O
(	O
)	O
i	O
(	O
)	O
=	O
θ	O
eθ	O
∼n	O
	O
(	O
θ	O
;	O
θ	O
,	O
σ	O
(	O
)	O
2i	O
)	O
j	O
(	O
θ	O
	O
)	O
(	O
8.40	O
)	O
via	O
sampling	O
.	O
the	O
intuition	O
for	O
this	O
approach	O
is	O
that	O
some	O
non-convex	O
functions	O
327	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
become	O
approximately	O
convex	O
when	O
blurred	O
.	O
in	O
many	O
cases	O
,	O
this	O
blurring	O
preserves	O
enough	O
information	O
about	O
the	O
location	O
of	O
a	O
global	O
minimum	O
that	O
we	O
can	O
ﬁnd	O
the	O
global	O
minimum	O
by	O
solving	O
progressively	O
less	O
blurred	O
versions	O
of	O
the	O
problem	O
.	O
this	O
approach	O
can	O
break	O
down	O
in	O
three	O
diﬀerent	O
ways	O
.	O
first	O
,	O
it	O
might	O
successfully	O
deﬁne	O
a	O
series	O
of	O
cost	O
functions	O
where	O
the	O
ﬁrst	O
is	O
convex	O
and	O
the	O
optimum	O
tracks	O
from	O
one	O
function	O
to	O
the	O
next	O
arriving	O
at	O
the	O
global	O
minimum	O
,	O
but	O
it	O
might	O
require	O
so	O
many	O
incremental	O
cost	O
functions	O
that	O
the	O
cost	O
of	O
the	O
entire	O
procedure	O
remains	O
high	O
.	O
np-hard	O
optimization	O
problems	O
remain	O
np-hard	O
,	O
even	O
when	O
continuation	O
methods	O
are	O
applicable	O
.	O
the	O
other	O
two	O
ways	O
that	O
continuation	O
methods	O
fail	O
both	O
correspond	O
to	O
the	O
method	O
not	O
being	O
applicable	O
.	O
first	O
,	O
the	O
function	O
might	O
not	O
become	O
convex	O
,	O
no	O
matter	O
how	O
much	O
it	O
is	O
blurred	O
.	O
consider	O
for	O
example	O
the	O
function	O
j	O
(	O
θ	O
)	O
=	O
θ.	O
second	O
,	O
the	O
function	O
may	O
become	O
convex	O
as	O
a	O
result	O
of	O
blurring	O
,	O
but	O
the	O
minimum	O
of	O
this	O
blurred	O
function	O
may	O
track	O
to	O
a	O
local	O
rather	O
than	O
a	O
global	O
minimum	O
of	O
the	O
original	O
cost	O
function	O
.	O
−	O
θ	O
	O
though	O
continuation	O
methods	O
were	O
mostly	O
originally	O
designed	O
to	O
deal	O
with	O
the	O
problem	O
of	O
local	O
minima	O
,	O
local	O
minima	O
are	O
no	O
longer	O
believed	O
to	O
be	O
the	O
primary	O
problem	O
for	O
neural	O
network	O
optimization	O
.	O
fortunately	O
,	O
continuation	O
methods	O
can	O
still	O
help	O
.	O
the	O
easier	O
objective	O
functions	O
introduced	O
by	O
the	O
continuation	O
method	O
can	O
eliminate	O
ﬂat	O
regions	O
,	O
decrease	O
variance	O
in	O
gradient	O
estimates	O
,	O
improve	O
conditioning	O
of	O
the	O
hessian	O
matrix	O
,	O
or	O
do	O
anything	O
else	O
that	O
will	O
either	O
make	O
local	O
updates	O
easier	O
to	O
compute	O
or	O
improve	O
the	O
correspondence	O
between	O
local	O
update	O
directions	O
and	O
progress	O
toward	O
a	O
global	O
solution	O
.	O
,	O
;	O
,	O
;	O
;	O
,	O
bengio	O
et	O
al	O
.	O
(	O
2009	O
skinner	O
1958	O
peterson	O
2004	O
krueger	O
and	O
dayan	O
2009	O
solomonoﬀ	O
1989	O
elman	O
1993	O
sanger	O
1994	O
)	O
observed	O
that	O
an	O
approach	O
called	O
curriculum	O
learning	O
or	O
shaping	O
can	O
be	O
interpreted	O
as	O
a	O
continuation	O
method	O
.	O
curriculum	O
learning	O
is	O
based	O
on	O
the	O
idea	O
of	O
planning	O
a	O
learning	O
process	O
to	O
begin	O
by	O
learning	O
simple	O
concepts	O
and	O
progress	O
to	O
learning	O
more	O
complex	O
concepts	O
that	O
depend	O
on	O
these	O
simpler	O
concepts	O
.	O
this	O
basic	O
strategy	O
was	O
previously	O
known	O
to	O
accelerate	O
progress	O
in	O
animal	O
training	O
(	O
)	O
and	O
machine	O
,	O
learning	O
(	O
bengio	O
et	O
al	O
.	O
2009	O
)	O
justiﬁed	O
this	O
strategy	O
as	O
a	O
continuation	O
method	O
,	O
where	O
earlier	O
j	O
(	O
)	O
i	O
are	O
made	O
easier	O
by	O
increasing	O
the	O
inﬂuence	O
of	O
simpler	O
examples	O
(	O
either	O
by	O
assigning	O
their	O
contributions	O
to	O
the	O
cost	O
function	O
larger	O
coeﬃcients	O
,	O
or	O
by	O
sampling	O
them	O
more	O
frequently	O
)	O
,	O
and	O
experimentally	O
demonstrated	O
that	O
better	O
results	O
could	O
be	O
obtained	O
by	O
following	O
a	O
curriculum	O
on	O
a	O
large-scale	O
neural	O
language	O
modeling	O
task	O
.	O
curriculum	O
learning	O
has	O
been	O
successful	O
on	O
a	O
wide	O
range	O
of	O
natural	O
language	O
(	O
spitkovsky	O
2010	O
;	O
)	O
and	O
computer	O
collobert	O
vision	O
(	O
kumar	O
et	O
al	O
.	O
2010	O
lee	O
and	O
grauman	O
2011	O
supancic	O
and	O
ramanan	O
2013	O
)	O
tasks	O
.	O
curriculum	O
learning	O
was	O
also	O
veriﬁed	O
as	O
being	O
consistent	O
with	O
the	O
way	O
in	O
)	O
:	O
teachers	O
start	O
by	O
showing	O
easier	O
and	O
which	O
humans	O
teach	O
(	O
2011b	O
tu	O
and	O
honavar	O
2011	O
2011a	O
mikolov	O
khan	O
et	O
al	O
.	O
2011	O
,	O
et	O
al.	O
,	O
et	O
al.	O
,	O
;	O
,	O
,	O
;	O
,	O
)	O
.	O
;	O
,	O
;	O
(	O
,	O
et	O
al.	O
,	O
;	O
,	O
328	O
chapter	O
8.	O
optimization	O
for	O
training	O
deep	O
models	O
more	O
prototypical	O
examples	O
and	O
then	O
help	O
the	O
learner	O
reﬁne	O
the	O
decision	O
surface	O
with	O
the	O
less	O
obvious	O
cases	O
.	O
curriculum-based	O
strategies	O
are	O
more	O
eﬀective	O
for	O
teaching	O
humans	O
than	O
strategies	O
based	O
on	O
uniform	O
sampling	O
of	O
examples	O
,	O
and	O
can	O
also	O
increase	O
the	O
eﬀectiveness	O
of	O
other	O
teaching	O
strategies	O
(	O
basu	O
and	O
christensen	O
,	O
2013	O
)	O
.	O
(	O
another	O
important	O
contribution	O
to	O
research	O
on	O
curriculum	O
learning	O
arose	O
in	O
the	O
context	O
of	O
training	O
recurrent	O
neural	O
networks	O
to	O
capture	O
long-term	O
dependencies	O
:	O
zaremba	O
and	O
sutskever	O
2014	O
)	O
found	O
that	O
much	O
better	O
results	O
were	O
obtained	O
with	O
a	O
stochastic	O
curriculum	O
,	O
in	O
which	O
a	O
random	O
mix	O
of	O
easy	O
and	O
diﬃcult	O
examples	O
is	O
always	O
presented	O
to	O
the	O
learner	O
,	O
but	O
where	O
the	O
average	O
proportion	O
of	O
the	O
more	O
diﬃcult	O
examples	O
(	O
here	O
,	O
those	O
with	O
longer-term	O
dependencies	O
)	O
is	O
gradually	O
increased	O
.	O
with	O
a	O
deterministic	O
curriculum	O
,	O
no	O
improvement	O
over	O
the	O
baseline	O
(	O
ordinary	O
training	O
from	O
the	O
full	O
training	O
set	O
)	O
was	O
observed	O
.	O
we	O
have	O
now	O
described	O
the	O
basic	O
family	O
of	O
neural	O
network	O
models	O
and	O
how	O
to	O
regularize	O
and	O
optimize	O
them	O
.	O
in	O
the	O
chapters	O
ahead	O
,	O
we	O
turn	O
to	O
specializations	O
of	O
the	O
neural	O
network	O
family	O
,	O
that	O
allow	O
neural	O
networks	O
to	O
scale	O
to	O
very	O
large	O
sizes	O
and	O
process	O
input	O
data	O
that	O
has	O
special	O
structure	O
.	O
the	O
optimization	O
methods	O
discussed	O
in	O
this	O
chapter	O
are	O
often	O
directly	O
applicable	O
to	O
these	O
specialized	O
architectures	O
with	O
little	O
or	O
no	O
modiﬁcation	O
.	O
329	O
chapter	O
9	O
convolutional	O
networks	O
lecun	O
1989	O
,	O
)	O
,	O
also	O
known	O
as	O
convolutional	O
networks	O
(	O
convolutional	O
neural	O
networks	O
or	O
cnns	O
,	O
are	O
a	O
specialized	O
kind	O
of	O
neural	O
network	O
for	O
processing	O
data	O
that	O
has	O
a	O
known	O
,	O
grid-like	O
topology	O
.	O
examples	O
include	O
time-series	O
data	O
,	O
which	O
can	O
be	O
thought	O
of	O
as	O
a	O
1d	O
grid	O
taking	O
samples	O
at	O
regular	O
time	O
intervals	O
,	O
and	O
image	O
data	O
,	O
which	O
can	O
be	O
thought	O
of	O
as	O
a	O
2d	O
grid	O
of	O
pixels	O
.	O
convolutional	O
networks	O
have	O
been	O
tremendously	O
successful	O
in	O
practical	O
applications	O
.	O
the	O
name	O
“	O
convolutional	O
neural	O
network	O
”	O
indicates	O
that	O
the	O
network	O
employs	O
a	O
mathematical	O
operation	O
called	O
convolution	O
.	O
convolution	O
is	O
a	O
specialized	O
kind	O
of	O
linear	O
operation	O
.	O
convolutional	O
networks	O
are	O
simply	O
neural	O
networks	O
that	O
use	O
convolution	O
in	O
place	O
of	O
general	O
matrix	O
multiplication	O
in	O
at	O
least	O
one	O
of	O
their	O
layers	O
.	O
in	O
this	O
chapter	O
,	O
we	O
will	O
ﬁrst	O
describe	O
what	O
convolution	O
is	O
.	O
next	O
,	O
we	O
will	O
explain	O
the	O
motivation	O
behind	O
using	O
convolution	O
in	O
a	O
neural	O
network	O
.	O
we	O
will	O
then	O
describe	O
an	O
operation	O
called	O
pooling	O
,	O
which	O
almost	O
all	O
convolutional	O
networks	O
employ	O
.	O
usually	O
,	O
the	O
operation	O
used	O
in	O
a	O
convolutional	O
neural	O
network	O
does	O
not	O
correspond	O
precisely	O
to	O
the	O
deﬁnition	O
of	O
convolution	O
as	O
used	O
in	O
other	O
ﬁelds	O
such	O
as	O
engineering	O
or	O
pure	O
mathematics	O
.	O
we	O
will	O
describe	O
several	O
variants	O
on	O
the	O
convolution	O
function	O
that	O
are	O
widely	O
used	O
in	O
practice	O
for	O
neural	O
networks	O
.	O
we	O
will	O
also	O
show	O
how	O
convolution	O
may	O
be	O
applied	O
to	O
many	O
kinds	O
of	O
data	O
,	O
with	O
diﬀerent	O
numbers	O
of	O
dimensions	O
.	O
we	O
then	O
discuss	O
means	O
of	O
making	O
convolution	O
more	O
eﬃcient	O
.	O
convolutional	O
networks	O
stand	O
out	O
as	O
an	O
example	O
of	O
neuroscientiﬁc	O
principles	O
inﬂuencing	O
deep	O
learning	O
.	O
we	O
will	O
discuss	O
these	O
neuroscientiﬁc	O
principles	O
,	O
then	O
conclude	O
with	O
comments	O
about	O
the	O
role	O
convolutional	O
networks	O
have	O
played	O
in	O
the	O
history	O
of	O
deep	O
learning	O
.	O
one	O
topic	O
this	O
chapter	O
does	O
not	O
address	O
is	O
how	O
to	O
choose	O
the	O
architecture	O
of	O
your	O
convolutional	O
network	O
.	O
the	O
goal	O
of	O
this	O
chapter	O
is	O
to	O
describe	O
the	O
kinds	O
of	O
tools	O
that	O
convolutional	O
networks	O
provide	O
,	O
while	O
chapter	O
11	O
330	O
chapter	O
9.	O
convolutional	O
networks	O
describes	O
general	O
guidelines	O
for	O
choosing	O
which	O
tools	O
to	O
use	O
in	O
which	O
circumstances	O
.	O
research	O
into	O
convolutional	O
network	O
architectures	O
proceeds	O
so	O
rapidly	O
that	O
a	O
new	O
best	O
architecture	O
for	O
a	O
given	O
benchmark	O
is	O
announced	O
every	O
few	O
weeks	O
to	O
months	O
,	O
rendering	O
it	O
impractical	O
to	O
describe	O
the	O
best	O
architecture	O
in	O
print	O
.	O
however	O
,	O
the	O
best	O
architectures	O
have	O
consistently	O
been	O
composed	O
of	O
the	O
building	O
blocks	O
described	O
here	O
.	O
9.1	O
the	O
convolution	O
operation	O
in	O
its	O
most	O
general	O
form	O
,	O
convolution	O
is	O
an	O
operation	O
on	O
two	O
functions	O
of	O
a	O
real-	O
valued	O
argument	O
.	O
to	O
motivate	O
the	O
deﬁnition	O
of	O
convolution	O
,	O
we	O
start	O
with	O
examples	O
of	O
two	O
functions	O
we	O
might	O
use	O
.	O
suppose	O
we	O
are	O
tracking	O
the	O
location	O
of	O
a	O
spaceship	O
with	O
a	O
laser	O
sensor	O
.	O
our	O
laser	O
sensor	O
provides	O
a	O
single	O
output	O
x	O
(	O
t	O
)	O
,	O
the	O
position	B
of	O
the	O
spaceship	O
at	O
time	O
t.	O
both	O
x	O
and	O
t	O
are	O
real-valued	O
,	O
i.e.	O
,	O
we	O
can	O
get	O
a	O
diﬀerent	O
reading	O
from	O
the	O
laser	O
sensor	O
at	O
any	O
instant	O
in	O
time	O
.	O
now	O
suppose	O
that	O
our	O
laser	O
sensor	O
is	O
somewhat	O
noisy	O
.	O
to	O
obtain	O
a	O
less	O
noisy	O
estimate	O
of	O
the	O
spaceship	O
’	O
s	O
position	B
,	O
we	O
would	O
like	O
to	O
average	O
together	O
several	O
measurements	O
.	O
of	O
course	O
,	O
more	O
recent	O
measurements	O
are	O
more	O
relevant	O
,	O
so	O
we	O
will	O
want	O
this	O
to	O
be	O
a	O
weighted	O
average	O
that	O
gives	O
more	O
weight	O
to	O
recent	O
measurements	O
.	O
we	O
can	O
do	O
this	O
with	O
a	O
weighting	O
function	O
w	O
(	O
a	O
)	O
,	O
where	O
a	O
is	O
the	O
age	O
of	O
a	O
measurement	O
.	O
if	O
we	O
apply	O
such	O
a	O
weighted	O
average	O
operation	O
at	O
every	O
moment	O
,	O
we	O
obtain	O
a	O
new	O
function	O
providing	O
a	O
smoothed	O
estimate	O
of	O
the	O
position	B
of	O
the	O
spaceship	O
:	O
	O
s	O
s	O
t	O
(	O
)	O
=	O
x	O
a	O
w	O
t	O
(	O
)	O
(	O
−	O
a	O
da	O
)	O
(	O
9.1	O
)	O
this	O
operation	O
is	O
called	O
convolution	O
.	O
the	O
convolution	O
operation	O
is	O
typically	O
denoted	O
with	O
an	O
asterisk	O
:	O
∗	O
x	O
w	O
t	O
)	O
(	O
)	O
s	O
t	O
(	O
)	O
=	O
(	O
(	O
9.2	O
)	O
in	O
our	O
example	O
,	O
w	O
needs	O
to	O
be	O
a	O
valid	O
probability	O
density	O
function	O
,	O
or	O
the	O
output	O
is	O
not	O
a	O
weighted	O
average	O
.	O
also	O
,	O
w	O
needs	O
to	O
be	O
for	O
all	O
negative	O
arguments	O
,	O
or	O
it	O
will	O
look	O
into	O
the	O
future	O
,	O
which	O
is	O
presumably	O
beyond	O
our	O
capabilities	O
.	O
these	O
limitations	O
are	O
particular	O
to	O
our	O
example	O
though	O
.	O
in	O
general	O
,	O
convolution	O
is	O
deﬁned	O
for	O
any	O
functions	O
for	O
which	O
the	O
above	O
integral	O
is	O
deﬁned	O
,	O
and	O
may	O
be	O
used	O
for	O
other	O
purposes	O
besides	O
taking	O
weighted	O
averages	O
.	O
0	O
in	O
convolutional	O
network	O
terminology	O
,	O
the	O
ﬁrst	O
argument	O
(	O
in	O
this	O
example	O
,	O
the	O
function	O
x	O
)	O
to	O
the	O
convolution	O
is	O
often	O
referred	O
to	O
as	O
the	O
input	O
and	O
the	O
second	O
331	O
chapter	O
9.	O
convolutional	O
networks	O
argument	O
(	O
in	O
this	O
example	O
,	O
the	O
function	O
w	O
)	O
as	O
the	O
kernel	O
.	O
the	O
output	O
is	O
sometimes	O
referred	O
to	O
as	O
the	O
.	O
feature	O
map	O
in	O
our	O
example	O
,	O
the	O
idea	O
of	O
a	O
laser	O
sensor	O
that	O
can	O
provide	O
measurements	O
at	O
every	O
instant	O
in	O
time	O
is	O
not	O
realistic	O
.	O
usually	O
,	O
when	O
we	O
work	B
with	O
data	O
on	O
a	O
computer	O
,	O
time	O
will	O
be	O
discretized	O
,	O
and	O
our	O
sensor	O
will	O
provide	O
data	O
at	O
regular	O
intervals	O
.	O
in	O
our	O
example	O
,	O
it	O
might	O
be	O
more	O
realistic	O
to	O
assume	O
that	O
our	O
laser	O
provides	O
a	O
measurement	O
once	O
per	O
second	O
.	O
the	O
time	O
index	O
t	O
can	O
then	O
take	O
on	O
only	O
integer	O
values	O
.	O
if	O
we	O
now	O
assume	O
that	O
x	O
and	O
w	O
are	O
deﬁned	O
only	O
on	O
integer	O
t	O
,	O
we	O
can	O
deﬁne	O
the	O
discrete	O
convolution	O
:	O
	O
∗	O
x	O
w	O
t	O
)	O
(	O
)	O
=	O
s	O
t	O
(	O
)	O
=	O
(	O
∞	O
(	O
)	O
(	O
−∞	O
x	O
a	O
w	O
t	O
a=	O
−	O
a	O
)	O
(	O
9.3	O
)	O
in	O
machine	O
learning	O
applications	O
,	O
the	O
input	O
is	O
usually	O
a	O
multidimensional	O
array	O
of	O
data	O
and	O
the	O
kernel	O
is	O
usually	O
a	O
multidimensional	O
array	O
of	O
parameters	O
that	O
are	O
adapted	O
by	O
the	O
learning	O
algorithm	O
.	O
we	O
will	O
refer	O
to	O
these	O
multidimensional	O
arrays	O
as	O
tensors	O
.	O
because	O
each	O
element	O
of	O
the	O
input	O
and	O
kernel	O
must	O
be	O
explicitly	O
stored	O
separately	O
,	O
we	O
usually	O
assume	O
that	O
these	O
functions	O
are	O
zero	O
everywhere	O
but	O
the	O
ﬁnite	O
set	O
of	O
points	O
for	O
which	O
we	O
store	O
the	O
values	O
.	O
this	O
means	O
that	O
in	O
practice	O
we	O
can	O
implement	O
the	O
inﬁnite	O
summation	O
as	O
a	O
summation	O
over	O
a	O
ﬁnite	O
number	O
of	O
array	O
elements	O
.	O
finally	O
,	O
we	O
often	O
use	O
convolutions	O
over	O
more	O
than	O
one	O
axis	O
at	O
a	O
time	O
.	O
for	O
example	O
,	O
if	O
we	O
use	O
a	O
two-dimensional	O
image	O
i	O
as	O
our	O
input	O
,	O
we	O
probably	O
also	O
want	O
to	O
use	O
a	O
two-dimensional	O
kernel	O
:	O
k	O
s	O
i	O
,	O
j	O
(	O
)	O
=	O
(	O
∗	O
i	O
k	O
i	O
,	O
j	O
)	O
(	O
−	O
−	O
i	O
m	O
,	O
n	O
k	O
i	O
m	O
,	O
j	O
(	O
)	O
(	O
n	O
.	O
)	O
(	O
9.4	O
)	O
	O
	O
)	O
=	O
	O
m	O
	O
n	O
)	O
=	O
m	O
n	O
convolution	O
is	O
commutative	O
,	O
meaning	O
we	O
can	O
equivalently	O
write	O
:	O
∗	O
−	O
−	O
s	O
i	O
,	O
j	O
(	O
)	O
=	O
(	O
k	O
i	O
i	O
,	O
j	O
)	O
(	O
i	O
i	O
m	O
,	O
j	O
(	O
n	O
k	O
m	O
,	O
n	O
.	O
)	O
)	O
(	O
(	O
9.5	O
)	O
usually	O
the	O
latter	O
formula	O
is	O
more	O
straightforward	O
to	O
implement	O
in	O
a	O
machine	O
learning	O
library	O
,	O
because	O
there	O
is	O
less	O
variation	O
in	O
the	O
range	O
of	O
valid	O
values	O
of	O
m	O
and	O
.n	O
the	O
commutative	O
property	O
of	O
convolution	O
arises	O
because	O
we	O
have	O
ﬂipped	O
the	O
kernel	O
relative	O
to	O
the	O
input	O
,	O
in	O
the	O
sense	O
that	O
as	O
m	O
increases	O
,	O
the	O
index	O
into	O
the	O
input	O
increases	O
,	O
but	O
the	O
index	O
into	O
the	O
kernel	O
decreases	O
.	O
the	O
only	O
reason	O
to	O
ﬂip	O
the	O
kernel	O
is	O
to	O
obtain	O
the	O
commutative	O
property	O
.	O
while	O
the	O
commutative	O
property	O
332	O
chapter	O
9.	O
convolutional	O
networks	O
is	O
useful	O
for	O
writing	O
proofs	O
,	O
it	O
is	O
not	O
usually	O
an	O
important	O
property	O
of	O
a	O
neural	O
network	O
implementation	O
.	O
instead	O
,	O
many	O
neural	O
network	O
libraries	O
implement	O
a	O
related	O
function	O
called	O
the	O
cross-correlation	O
,	O
which	O
is	O
the	O
same	O
as	O
convolution	O
but	O
without	O
ﬂipping	O
the	O
kernel	O
:	O
	O
	O
s	O
i	O
,	O
j	O
(	O
)	O
=	O
(	O
∗	O
i	O
k	O
i	O
,	O
j	O
)	O
(	O
)	O
=	O
i	O
i	O
m	O
,	O
j	O
(	O
+	O
m	O
n	O
+	O
)	O
n	O
k	O
m	O
,	O
n	O
.	O
)	O
(	O
(	O
9.6	O
)	O
many	O
machine	O
learning	O
libraries	O
implement	O
cross-correlation	O
but	O
call	O
it	O
convolution	O
.	O
in	O
this	O
text	O
we	O
will	O
follow	O
this	O
convention	O
of	O
calling	O
both	O
operations	O
convolution	O
,	O
and	O
specify	O
whether	O
we	O
mean	O
to	O
ﬂip	O
the	O
kernel	O
or	O
not	O
in	O
contexts	O
where	O
kernel	O
ﬂipping	O
is	O
relevant	O
.	O
in	O
the	O
context	O
of	O
machine	O
learning	O
,	O
the	O
learning	O
algorithm	O
will	O
learn	O
the	O
appropriate	O
values	O
of	O
the	O
kernel	O
in	O
the	O
appropriate	O
place	O
,	O
so	O
an	O
algorithm	O
based	O
on	O
convolution	O
with	O
kernel	O
ﬂipping	O
will	O
learn	O
a	O
kernel	O
that	O
is	O
ﬂipped	O
relative	O
to	O
the	O
kernel	O
learned	O
by	O
an	O
algorithm	O
without	O
the	O
ﬂipping	O
.	O
it	O
is	O
also	O
rare	O
for	O
convolution	O
to	O
be	O
used	O
alone	O
in	O
machine	O
learning	O
;	O
instead	O
convolution	O
is	O
used	O
simultaneously	O
with	O
other	O
functions	O
,	O
and	O
the	O
combination	O
of	O
these	O
functions	O
does	O
not	O
commute	O
regardless	O
of	O
whether	O
the	O
convolution	O
operation	O
ﬂips	O
its	O
kernel	O
or	O
not	O
.	O
see	O
ﬁgure	O
9.1	O
to	O
a	O
2-d	O
tensor	O
.	O
for	O
an	O
example	O
of	O
convolution	O
(	O
without	O
kernel	O
ﬂipping	O
)	O
applied	O
discrete	O
convolution	O
can	O
be	O
viewed	O
as	O
multiplication	O
by	O
a	O
matrix	O
.	O
however	O
,	O
the	O
matrix	O
has	O
several	O
entries	O
constrained	O
to	O
be	O
equal	O
to	O
other	O
entries	O
.	O
for	O
example	O
,	O
for	O
univariate	O
discrete	O
convolution	O
,	O
each	O
row	O
of	O
the	O
matrix	O
is	O
constrained	O
to	O
be	O
equal	O
to	O
the	O
row	O
above	O
shifted	O
by	O
one	O
element	O
.	O
this	O
is	O
known	O
as	O
a	O
toeplitz	O
matrix	O
.	O
in	O
two	O
dimensions	O
,	O
a	O
doubly	O
block	O
circulant	O
matrix	O
corresponds	O
to	O
convolution	O
.	O
in	O
addition	O
to	O
these	O
constraints	O
that	O
several	O
elements	O
be	O
equal	O
to	O
each	O
other	O
,	O
convolution	O
usually	O
corresponds	O
to	O
a	O
very	O
sparse	O
matrix	O
(	O
a	O
matrix	O
whose	O
entries	O
are	O
mostly	O
equal	O
to	O
zero	O
)	O
.	O
this	O
is	O
because	O
the	O
kernel	O
is	O
usually	O
much	O
smaller	O
than	O
the	O
input	O
image	O
.	O
any	O
neural	O
network	O
algorithm	O
that	O
works	O
with	O
matrix	O
multiplication	O
and	O
does	O
not	O
depend	O
on	O
speciﬁc	O
properties	O
of	O
the	O
matrix	O
structure	O
should	O
work	B
with	O
convolution	O
,	O
without	O
requiring	O
any	O
further	O
changes	O
to	O
the	O
neural	O
network	O
.	O
typical	O
convolutional	O
neural	O
networks	O
do	O
make	O
use	O
of	O
further	O
specializations	O
in	O
order	O
to	O
deal	O
with	O
large	O
inputs	O
eﬃciently	O
,	O
but	O
these	O
are	O
not	O
strictly	O
necessary	O
from	O
a	O
theoretical	O
perspective	O
.	O
333	O
chapter	O
9.	O
convolutional	O
networks	O
input	O
a	O
e	O
i	O
b	O
f	O
j	O
c	O
g	O
k	O
d	O
h	O
l	O
output	O
kernel	O
w	O
y	O
x	O
z	O
aw	O
+	O
bx	O
+	O
aw	O
+	O
bx	O
+	O
ey	O
+	O
f	O
z	O
ey	O
+	O
f	O
z	O
bw	O
+	O
cx	O
+	O
bw	O
+	O
cx	O
+	O
f	O
y	O
+	O
gz	O
f	O
y	O
+	O
gz	O
cw	O
+	O
dx	O
+	O
cw	O
+	O
dx	O
+	O
gy	O
+	O
hz	O
gy	O
+	O
hz	O
ew	O
+	O
f	O
x	O
+	O
ew	O
+	O
f	O
x	O
+	O
iy	O
+	O
jz	O
iy	O
+	O
jz	O
f	O
w	O
+	O
gx	O
+	O
f	O
w	O
+	O
gx	O
+	O
jy	O
+	O
kz	O
jy	O
+	O
kz	O
gw	O
+	O
hx	O
+	O
gw	O
+	O
hx	O
+	O
ky	O
+	O
lz	O
ky	O
+	O
lz	O
figure	O
9.1	O
:	O
an	O
example	O
of	O
2-d	O
convolution	O
without	O
kernel-ﬂipping	O
.	O
in	O
this	O
case	O
we	O
restrict	O
the	O
output	O
to	O
only	O
positions	O
where	O
the	O
kernel	O
lies	O
entirely	O
within	O
the	O
image	O
,	O
called	O
“	O
valid	O
”	O
convolution	O
in	O
some	O
contexts	O
.	O
we	O
draw	O
boxes	O
with	O
arrows	O
to	O
indicate	O
how	O
the	O
upper-left	O
element	O
of	O
the	O
output	O
tensor	O
is	O
formed	O
by	O
applying	O
the	O
kernel	O
to	O
the	O
corresponding	O
upper-left	O
region	O
of	O
the	O
input	O
tensor	O
.	O
334	O
chapter	O
9.	O
convolutional	O
networks	O
9.2	O
motivation	O
convolution	O
leverages	O
three	O
important	O
ideas	O
that	O
can	O
help	O
improve	O
a	O
machine	O
learning	O
system	O
:	O
sparse	O
interactions	O
,	O
parameter	O
sharing	O
and	O
equivariant	O
representations	O
.	O
moreover	O
,	O
convolution	O
provides	O
a	O
means	O
for	O
working	O
with	O
inputs	O
of	O
variable	O
size	O
.	O
we	O
now	O
describe	O
each	O
of	O
these	O
ideas	O
in	O
turn	O
.	O
traditional	O
neural	O
network	O
layers	O
use	O
matrix	O
multiplication	O
by	O
a	O
matrix	O
of	O
parameters	O
with	O
a	O
separate	O
parameter	O
describing	O
the	O
interaction	O
between	O
each	O
input	O
unit	O
and	O
each	O
output	O
unit	O
.	O
this	O
means	O
every	O
output	O
unit	O
interacts	O
with	O
every	O
input	O
unit	O
.	O
convolutional	O
networks	O
,	O
however	O
,	O
typically	O
have	O
sparse	O
interactions	O
(	O
also	O
referred	O
to	O
as	O
sparse	O
connectivity	O
or	O
sparse	O
weights	O
)	O
.	O
this	O
is	O
accomplished	O
by	O
making	O
the	O
kernel	O
smaller	O
than	O
the	O
input	O
.	O
for	O
example	O
,	O
when	O
processing	O
an	O
image	O
,	O
the	O
input	O
image	O
might	O
have	O
thousands	O
or	O
millions	O
of	O
pixels	O
,	O
but	O
we	O
can	O
detect	O
small	O
,	O
meaningful	O
features	O
such	O
as	O
edges	O
with	O
kernels	O
that	O
occupy	O
only	O
tens	O
or	O
hundreds	O
of	O
pixels	O
.	O
this	O
means	O
that	O
we	O
need	O
to	O
store	O
fewer	O
parameters	O
,	O
which	O
both	O
reduces	O
the	O
memory	O
requirements	O
of	O
the	O
model	B
and	O
improves	O
its	O
statistical	O
eﬃciency	O
.	O
it	O
also	O
means	O
that	O
computing	O
the	O
output	O
requires	O
fewer	O
operations	O
.	O
these	O
improvements	O
×	O
in	O
eﬃciency	O
are	O
usually	O
quite	O
large	O
.	O
if	O
there	O
are	O
m	O
inputs	O
and	O
n	O
outputs	O
,	O
then	O
×	O
matrix	O
multiplication	O
requires	O
m	O
n	O
parameters	O
and	O
the	O
algorithms	O
used	O
in	O
practice	O
have	O
o	O
(	O
m	O
n	O
)	O
runtime	O
(	O
per	O
example	O
)	O
.	O
if	O
we	O
limit	O
the	O
number	O
of	O
connections	O
×	O
each	O
output	O
may	O
have	O
to	O
k	O
,	O
then	O
the	O
sparsely	O
connected	O
approach	O
requires	O
only	O
k	O
)	O
runtime	O
.	O
for	O
many	O
practical	O
applications	O
,	O
it	O
is	O
possible	O
to	O
obtain	O
good	O
performance	O
on	O
the	O
machine	O
learning	O
task	O
while	O
keeping	O
k	O
several	O
orders	O
of	O
magnitude	O
smaller	O
than	O
m.	O
for	O
graphical	O
demonstrations	O
of	O
sparse	O
connectivity	O
,	O
see	O
ﬁgure	O
.	O
in	O
a	O
deep	O
convolutional	O
network	O
,	O
units	O
in	O
the	O
deeper	O
layers	O
may	O
indirectly	O
interact	O
with	O
a	O
larger	O
portion	O
of	O
the	O
input	O
,	O
as	O
shown	O
in	O
ﬁgure	O
.	O
this	O
allows	O
the	O
network	O
to	O
eﬃciently	O
describe	O
complicated	O
interactions	O
between	O
many	O
variables	O
by	O
constructing	O
such	O
interactions	O
from	O
simple	O
building	O
blocks	O
that	O
each	O
describe	O
only	O
sparse	O
interactions	O
.	O
parameters	O
and	O
o	O
(	O
k	O
×	O
9.4	O
9.2	O
and	O
ﬁgure	O
9.3	O
n	O
n	O
parameter	O
sharing	O
refers	O
to	O
using	O
the	O
same	O
parameter	O
for	O
more	O
than	O
one	O
function	O
in	O
a	O
model	B
.	O
in	O
a	O
traditional	O
neural	O
net	O
,	O
each	O
element	O
of	O
the	O
weight	O
matrix	O
is	O
used	O
exactly	O
once	O
when	O
computing	O
the	O
output	O
of	O
a	O
layer	O
.	O
it	O
is	O
multiplied	O
by	O
one	O
element	O
of	O
the	O
input	O
and	O
then	O
never	O
revisited	O
.	O
as	O
a	O
synonym	O
for	O
parameter	O
sharing	O
,	O
one	O
can	O
say	O
that	O
a	O
network	O
has	O
tied	O
weights	O
,	O
because	O
the	O
value	O
of	O
the	O
weight	O
applied	O
to	O
one	O
input	O
is	O
tied	O
to	O
the	O
value	O
of	O
a	O
weight	O
applied	O
elsewhere	O
.	O
in	O
a	O
convolutional	O
neural	O
net	O
,	O
each	O
member	O
of	O
the	O
kernel	O
is	O
used	O
at	O
every	O
position	B
of	O
the	O
input	O
(	O
except	O
perhaps	O
some	O
of	O
the	O
boundary	O
pixels	O
,	O
depending	O
on	O
the	O
design	O
decisions	O
regarding	O
the	O
boundary	O
)	O
.	O
the	O
parameter	O
sharing	O
used	O
by	O
the	O
convolution	O
operation	O
means	O
that	O
rather	O
than	O
learning	O
a	O
separate	O
set	O
of	O
parameters	O
335	O
chapter	O
9.	O
convolutional	O
networks	O
s1s1	O
s2s2	O
s3s3	O
s4s4	O
s5s5	O
x1x1	O
s1s1	O
x2x2	O
s2s2	O
x3x3	O
s3s3	O
x4x4	O
s4s4	O
x5x5	O
s5s5	O
x1x1	O
x2x2	O
x3x3	O
x4x4	O
x5x5	O
figure	O
9.2	O
:	O
sparse	O
connectivity	O
,	O
viewed	O
from	O
below	O
:	O
we	O
highlight	O
one	O
input	O
unit	O
,	O
x3	O
,	O
and	O
also	O
highlight	O
the	O
output	O
units	O
in	O
s	O
that	O
are	O
aﬀected	O
by	O
this	O
unit	O
.	O
(	O
top	O
)	O
when	O
s	O
is	O
formed	O
by	O
convolution	O
with	O
a	O
kernel	O
of	O
width	O
,	O
only	O
three	O
outputs	O
are	O
aﬀected	O
by	O
x	O
.	O
(	O
bottom	O
)	O
when	O
is	O
formed	O
by	O
matrix	O
multiplication	O
,	O
connectivity	O
is	O
no	O
longer	O
sparse	O
,	O
so	O
all	O
of	O
the	O
outputs	O
are	O
aﬀected	O
by	O
x3	O
.	O
s	O
3	O
336	O
chapter	O
9.	O
convolutional	O
networks	O
s1s1	O
s2s2	O
s3s3	O
s4s4	O
s5s5	O
x1x1	O
s1s1	O
x2x2	O
s2s2	O
x3x3	O
s3s3	O
x4x4	O
s4s4	O
x5x5	O
s5s5	O
x1x1	O
x2x2	O
x3x3	O
x4x4	O
x5x5	O
figure	O
9.3	O
:	O
sparse	O
connectivity	O
,	O
viewed	O
from	O
above	O
:	O
we	O
highlight	O
one	O
output	O
unit	O
,	O
s3	O
,	O
and	O
also	O
highlight	O
the	O
input	O
units	O
in	O
x	O
that	O
aﬀect	O
this	O
unit	O
.	O
these	O
units	O
are	O
known	O
as	O
the	O
receptive	O
ﬁeld	O
of	O
s3	O
.	O
(	O
top	O
)	O
when	O
s	O
is	O
formed	O
by	O
convolution	O
with	O
a	O
kernel	O
of	O
width	O
,	O
only	O
three	O
inputs	O
aﬀect	O
s	O
is	O
formed	O
by	O
matrix	O
multiplication	O
,	O
connectivity	O
is	O
no	O
longer	O
sparse	O
,	O
so	O
all	O
of	O
the	O
inputs	O
aﬀect	O
s3	O
.	O
(	O
bottom	O
)	O
when	O
3	O
s	O
3.	O
g1g1	O
h1h1	O
g2g2	O
h2h2	O
x1x1	O
x2x2	O
g3g3	O
h3h3	O
x3x3	O
g4g4	O
h4h4	O
x4x4	O
g5g5	O
h5h5	O
x5x5	O
figure	O
9.4	O
:	O
the	O
receptive	O
ﬁeld	O
of	O
the	O
units	O
in	O
the	O
deeper	O
layers	O
of	O
a	O
convolutional	O
network	O
is	O
larger	O
than	O
the	O
receptive	O
ﬁeld	O
of	O
the	O
units	O
in	O
the	O
shallow	O
layers	O
.	O
this	O
eﬀect	O
increases	O
if	O
the	O
network	O
includes	O
architectural	O
features	O
like	O
strided	O
convolution	O
(	O
ﬁgure	O
)	O
or	O
pooling	O
(	O
section	O
direct	O
connections	O
in	O
a	O
convolutional	O
net	O
are	O
very	O
sparse	O
,	O
units	O
in	O
the	O
deeper	O
layers	O
can	O
be	O
indirectly	O
connected	O
to	O
all	O
or	O
most	O
of	O
the	O
input	O
image	O
.	O
)	O
.	O
this	O
means	O
that	O
even	O
though	O
9.12	O
9.3	O
337	O
chapter	O
9.	O
convolutional	O
networks	O
s1s1	O
x1x1	O
s1s1	O
x1x1	O
s2s2	O
x2x2	O
s2s2	O
x2x2	O
s3s3	O
x3x3	O
s3s3	O
x3x3	O
s4s4	O
x4x4	O
s4s4	O
x4x4	O
s5s5	O
x5x5	O
s5s5	O
x5x5	O
figure	O
9.5	O
:	O
parameter	O
sharing	O
:	O
black	O
arrows	O
indicate	O
the	O
connections	O
that	O
use	O
a	O
particular	O
parameter	O
in	O
two	O
diﬀerent	O
models	O
.	O
(	O
top	O
)	O
the	O
black	O
arrows	O
indicate	O
uses	O
of	O
the	O
central	O
element	O
of	O
a	O
3-element	O
kernel	O
in	O
a	O
convolutional	O
model	B
.	O
due	O
to	O
parameter	O
sharing	O
,	O
this	O
single	O
parameter	O
is	O
used	O
at	O
all	O
input	O
locations	O
.	O
the	O
single	O
black	O
arrow	O
indicates	O
the	O
use	O
of	O
the	O
central	O
element	O
of	O
the	O
weight	O
matrix	O
in	O
a	O
fully	O
connected	O
model	B
.	O
this	O
model	B
has	O
no	O
parameter	O
sharing	O
so	O
the	O
parameter	O
is	O
used	O
only	O
once	O
.	O
(	O
bottom	O
)	O
×	O
n	O
for	O
every	O
location	O
,	O
we	O
learn	O
only	O
one	O
set	O
.	O
this	O
does	O
not	O
aﬀect	O
the	O
runtime	O
of	O
forward	O
propagation—it	O
is	O
still	O
o	O
(	O
k	O
)	O
—but	O
it	O
does	O
further	O
reduce	O
the	O
storage	O
requirements	O
of	O
the	O
model	B
to	O
k	O
parameters	O
.	O
recall	O
that	O
k	O
is	O
usually	O
several	O
orders	O
×	O
of	O
magnitude	O
less	O
than	O
m.	O
since	O
m	O
and	O
n	O
are	O
usually	O
roughly	O
the	O
same	O
size	O
,	O
k	O
is	O
practically	O
insigniﬁcant	O
compared	O
to	O
m	O
n	O
.	O
convolution	O
is	O
thus	O
dramatically	O
more	O
eﬃcient	O
than	O
dense	O
matrix	O
multiplication	O
in	O
terms	O
of	O
the	O
memory	O
requirements	O
and	O
statistical	O
eﬃciency	O
.	O
for	O
a	O
graphical	O
depiction	O
of	O
how	O
parameter	O
sharing	O
works	O
,	O
see	O
ﬁgure	O
.9.5	O
as	O
an	O
example	O
of	O
both	O
of	O
these	O
ﬁrst	O
two	O
principles	O
in	O
action	O
,	O
ﬁgure	O
shows	O
how	O
sparse	O
connectivity	O
and	O
parameter	O
sharing	O
can	O
dramatically	O
improve	O
the	O
eﬃciency	O
of	O
a	O
linear	O
function	O
for	O
detecting	O
edges	O
in	O
an	O
image	O
.	O
9.6	O
in	O
the	O
case	O
of	O
convolution	O
,	O
the	O
particular	O
form	O
of	O
parameter	O
sharing	O
causes	O
the	O
layer	O
to	O
have	O
a	O
property	O
called	O
equivariance	O
to	O
translation	O
.	O
to	O
say	O
a	O
function	O
is	O
equivariant	O
means	O
that	O
if	O
the	O
input	O
changes	O
,	O
the	O
output	O
changes	O
in	O
the	O
same	O
way	O
.	O
speciﬁcally	O
,	O
a	O
function	O
f	O
(	O
x	O
)	O
is	O
equivariant	O
to	O
a	O
function	O
g	O
if	O
f	O
(	O
g	O
(	O
x	O
)	O
)	O
=	O
g	O
(	O
f	O
(	O
x	O
)	O
)	O
.	O
in	O
the	O
case	O
of	O
convolution	O
,	O
if	O
we	O
let	O
g	O
be	O
any	O
function	O
that	O
translates	O
the	O
input	O
,	O
i.e.	O
,	O
shifts	O
it	O
,	O
then	O
the	O
convolution	O
function	O
is	O
equivariant	O
to	O
g.	O
for	O
example	O
,	O
let	O
i	O
be	O
a	O
function	O
giving	O
image	O
brightness	O
at	O
integer	O
coordinates	O
.	O
let	O
g	O
be	O
a	O
function	O
338	O
chapter	O
9.	O
convolutional	O
networks	O
	O
−	O
	O
	O
(	O
x	O
,	O
y	O
)	O
=	O
i	O
(	O
x	O
=	O
g	O
(	O
i	O
)	O
is	O
mapping	O
one	O
image	O
function	O
to	O
another	O
image	O
function	O
,	O
such	O
that	O
i	O
the	O
image	O
function	O
with	O
i	O
1	O
,	O
y	O
)	O
.	O
this	O
shifts	O
every	O
pixel	O
of	O
i	O
one	O
unit	O
to	O
the	O
right	O
.	O
if	O
we	O
apply	O
this	O
transformation	O
to	O
i	O
,	O
then	O
apply	O
convolution	O
,	O
the	O
result	O
will	O
be	O
the	O
same	O
as	O
if	O
we	O
applied	O
convolution	O
to	O
i	O
,	O
then	O
applied	O
the	O
transformation	O
g	O
to	O
the	O
output	O
.	O
when	O
processing	O
time	O
series	O
data	O
,	O
this	O
means	O
that	O
convolution	O
produces	O
a	O
sort	O
of	O
timeline	O
that	O
shows	O
when	O
diﬀerent	O
features	O
appear	O
in	O
the	O
input	O
.	O
if	O
we	O
move	O
an	O
event	O
later	O
in	O
time	O
in	O
the	O
input	O
,	O
the	O
exact	O
same	O
representation	O
of	O
it	O
will	O
appear	O
in	O
the	O
output	O
,	O
just	O
later	O
in	O
time	O
.	O
similarly	O
with	O
images	O
,	O
convolution	O
creates	O
a	O
2-d	O
map	O
of	O
where	O
certain	O
features	O
appear	O
in	O
the	O
input	O
.	O
if	O
we	O
move	O
the	O
object	O
in	O
the	O
input	O
,	O
its	O
representation	O
will	O
move	O
the	O
same	O
amount	O
in	O
the	O
output	O
.	O
this	O
is	O
useful	O
for	O
when	O
we	O
know	O
that	O
some	O
function	O
of	O
a	O
small	O
number	O
of	O
neighboring	O
pixels	O
is	O
useful	O
when	O
applied	O
to	O
multiple	O
input	O
locations	O
.	O
for	O
example	O
,	O
when	O
processing	O
images	O
,	O
it	O
is	O
useful	O
to	O
detect	O
edges	O
in	O
the	O
ﬁrst	O
layer	O
of	O
a	O
convolutional	O
network	O
.	O
the	O
same	O
edges	O
appear	O
more	O
or	O
less	O
everywhere	O
in	O
the	O
image	O
,	O
so	O
it	O
is	O
practical	O
to	O
share	O
parameters	O
across	O
the	O
entire	O
image	O
.	O
in	O
some	O
cases	O
,	O
we	O
may	O
not	O
wish	O
to	O
share	O
parameters	O
across	O
the	O
entire	O
image	O
.	O
for	O
example	O
,	O
if	O
we	O
are	O
processing	O
images	O
that	O
are	O
cropped	O
to	O
be	O
centered	O
on	O
an	O
individual	O
’	O
s	O
face	O
,	O
we	O
probably	O
want	O
to	O
extract	O
diﬀerent	O
features	O
at	O
diﬀerent	O
locations—the	O
part	O
of	O
the	O
network	O
processing	O
the	O
top	O
of	O
the	O
face	O
needs	O
to	O
look	O
for	O
eyebrows	O
,	O
while	O
the	O
part	O
of	O
the	O
network	O
processing	O
the	O
bottom	O
of	O
the	O
face	O
needs	O
to	O
look	O
for	O
a	O
chin	O
.	O
convolution	O
is	O
not	O
naturally	O
equivariant	O
to	O
some	O
other	O
transformations	O
,	O
such	O
as	O
changes	O
in	O
the	O
scale	O
or	O
rotation	O
of	O
an	O
image	O
.	O
other	O
mechanisms	O
are	O
necessary	O
for	O
handling	O
these	O
kinds	O
of	O
transformations	O
.	O
finally	O
,	O
some	O
kinds	O
of	O
data	O
can	O
not	O
be	O
processed	O
by	O
neural	O
networks	O
deﬁned	O
by	O
matrix	O
multiplication	O
with	O
a	O
ﬁxed-shape	O
matrix	O
.	O
convolution	O
enables	O
processing	O
of	O
some	O
of	O
these	O
kinds	O
of	O
data	O
.	O
we	O
discuss	O
this	O
further	O
in	O
section	O
.9.7	O
9.3	O
pooling	O
)	O
.	O
a	O
typical	O
layer	O
of	O
a	O
convolutional	O
network	O
consists	O
of	O
three	O
stages	O
(	O
see	O
ﬁgure	O
in	O
the	O
ﬁrst	O
stage	O
,	O
the	O
layer	O
performs	O
several	O
convolutions	O
in	O
parallel	O
to	O
produce	O
a	O
set	O
of	O
linear	O
activations	O
.	O
in	O
the	O
second	O
stage	O
,	O
each	O
linear	O
activation	O
is	O
run	O
through	O
a	O
nonlinear	O
activation	O
function	O
,	O
such	O
as	O
the	O
rectiﬁed	O
linear	O
activation	O
function	O
.	O
this	O
stage	O
is	O
sometimes	O
called	O
the	O
detector	O
stage	O
.	O
in	O
the	O
third	O
stage	O
,	O
we	O
use	O
a	O
pooling	O
function	O
to	O
modify	O
the	O
output	O
of	O
the	O
layer	O
further	O
.	O
9.7	O
a	O
pooling	O
function	O
replaces	O
the	O
output	O
of	O
the	O
net	O
at	O
a	O
certain	O
location	O
with	O
a	O
summary	O
statistic	O
of	O
the	O
nearby	O
outputs	O
.	O
for	O
example	O
,	O
the	O
max	O
pooling	O
(	O
zhou	O
339	O
chapter	O
9.	O
convolutional	O
networks	O
×	O
×	O
×	O
280	O
figure	O
9.6	O
:	O
eﬃciency	O
of	O
edge	O
detection	O
.	O
the	O
image	O
on	O
the	O
right	O
was	O
formed	O
by	O
taking	O
each	O
pixel	O
in	O
the	O
original	O
image	O
and	O
subtracting	O
the	O
value	O
of	O
its	O
neighboring	O
pixel	O
on	O
the	O
left	O
.	O
this	O
shows	O
the	O
strength	O
of	O
all	O
of	O
the	O
vertically	O
oriented	O
edges	O
in	O
the	O
input	O
image	O
,	O
which	O
can	O
be	O
a	O
useful	O
operation	O
for	O
object	O
detection	O
.	O
both	O
images	O
are	O
280	O
pixels	O
tall	O
.	O
the	O
input	O
image	O
is	O
320	O
pixels	O
wide	O
while	O
the	O
output	O
image	O
is	O
319	O
pixels	O
wide	O
.	O
this	O
transformation	O
can	O
be	O
described	O
by	O
a	O
convolution	O
kernel	O
containing	O
two	O
elements	O
,	O
and	O
requires	O
319	O
3	O
=	O
267	O
,	O
960	O
ﬂoating	O
point	O
operations	O
(	O
two	O
multiplications	O
and	O
one	O
addition	O
per	O
output	O
pixel	O
)	O
to	O
compute	O
using	O
convolution	O
.	O
to	O
describe	O
the	O
same	O
transformation	O
with	O
a	O
matrix	O
multiplication	O
would	O
take	O
320	O
280	O
,	O
or	O
over	O
eight	O
billion	O
,	O
entries	O
in	O
the	O
matrix	O
,	O
making	O
convolution	O
four	O
billion	O
times	O
more	O
eﬃcient	O
for	O
representing	O
this	O
transformation	O
.	O
the	O
straightforward	O
matrix	O
multiplication	O
algorithm	O
performs	O
over	O
sixteen	O
billion	O
ﬂoating	O
point	O
operations	O
,	O
making	O
convolution	O
roughly	O
60,000	O
times	O
more	O
eﬃcient	O
computationally	O
.	O
of	O
course	O
,	O
most	O
of	O
the	O
entries	O
of	O
the	O
matrix	O
would	O
be	O
zero	O
.	O
if	O
we	O
stored	O
only	O
the	O
nonzero	O
entries	O
of	O
the	O
matrix	O
,	O
then	O
both	O
matrix	O
multiplication	O
and	O
convolution	O
would	O
require	O
the	O
same	O
number	O
of	O
ﬂoating	O
point	O
operations	O
to	O
compute	O
.	O
the	O
matrix	O
would	O
still	O
need	O
to	O
contain	O
2	O
280	O
=	O
178	O
,	O
640	O
entries	O
.	O
convolution	O
is	O
an	O
extremely	O
eﬃcient	O
way	O
of	O
describing	O
transformations	O
that	O
apply	O
the	O
same	O
linear	O
transformation	O
of	O
a	O
small	O
,	O
local	O
region	O
across	O
the	O
entire	O
input	O
.	O
(	O
photo	O
credit	O
:	O
paula	O
goodfellow	O
)	O
319	O
280	O
319	O
×	O
×	O
×	O
×	O
340	O
chapter	O
9.	O
convolutional	O
networks	O
complex	O
layer	O
terminology	O
simple	O
layer	O
terminology	O
next	O
layer	O
next	O
layer	O
convolutional	O
layer	O
pooling	O
stage	O
pooling	O
layer	O
detector	O
stage	O
:	O
nonlinearity	O
e.g.	O
,	O
rectiﬁed	O
linear	O
convolution	O
stage	O
:	O
a	O
ne	O
transform	O
ﬃ	O
detector	O
layer	O
:	O
nonlinearity	O
e.g.	O
,	O
rectiﬁed	O
linear	O
convolution	O
layer	O
:	O
a	O
ne	O
transform	O
ﬃ	O
input	O
to	O
layer	O
input	O
to	O
layers	O
figure	O
9.7	O
:	O
the	O
components	O
of	O
a	O
typical	O
convolutional	O
neural	O
network	O
layer	O
.	O
there	O
are	O
two	O
commonly	O
used	O
sets	O
of	O
terminology	O
for	O
describing	O
these	O
layers	O
.	O
(	O
left	O
)	O
in	O
this	O
terminology	O
,	O
the	O
convolutional	O
net	O
is	O
viewed	O
as	O
a	O
small	O
number	O
of	O
relatively	O
complex	O
layers	O
,	O
with	O
each	O
layer	O
having	O
many	O
“	O
stages.	O
”	O
in	O
this	O
terminology	O
,	O
there	O
is	O
a	O
one-to-one	O
mapping	O
between	O
kernel	O
tensors	O
and	O
network	O
layers	O
.	O
in	O
this	O
book	O
we	O
generally	O
use	O
this	O
terminology	O
.	O
(	O
right	O
)	O
in	O
this	O
terminology	O
,	O
the	O
convolutional	O
net	O
is	O
viewed	O
as	O
a	O
larger	O
number	O
of	O
simple	O
layers	O
;	O
every	O
step	O
of	O
processing	O
is	O
regarded	O
as	O
a	O
layer	O
in	O
its	O
own	O
right	O
.	O
this	O
means	O
that	O
not	O
every	O
“	O
layer	O
”	O
has	O
parameters	O
.	O
341	O
chapter	O
9.	O
convolutional	O
networks	O
,	O
)	O
operation	O
reports	O
the	O
maximum	O
output	O
within	O
a	O
rectangular	O
and	O
chellappa	O
1988	O
neighborhood	O
.	O
other	O
popular	O
pooling	O
functions	O
include	O
the	O
average	O
of	O
a	O
rectangular	O
neighborhood	O
,	O
the	O
l2	O
norm	O
of	O
a	O
rectangular	O
neighborhood	O
,	O
or	O
a	O
weighted	O
average	O
based	O
on	O
the	O
distance	O
from	O
the	O
central	O
pixel	O
.	O
9.8	O
for	O
an	O
example	O
of	O
how	O
this	O
works	O
.	O
in	O
all	O
cases	O
,	O
pooling	O
helps	O
to	O
make	O
the	O
representation	O
become	O
approximately	O
invariant	O
to	O
small	O
translations	O
of	O
the	O
input	O
.	O
invariance	O
to	O
translation	O
means	O
that	O
if	O
we	O
translate	O
the	O
input	O
by	O
a	O
small	O
amount	O
,	O
the	O
values	O
of	O
most	O
of	O
the	O
pooled	O
outputs	O
do	O
not	O
change	O
.	O
see	O
ﬁgure	O
invariance	O
to	O
local	O
translation	O
can	O
be	O
a	O
very	O
useful	O
property	O
if	O
we	O
care	O
more	O
about	O
whether	O
some	O
feature	O
is	O
present	O
than	O
exactly	O
where	O
it	O
is	O
.	O
for	O
example	O
,	O
when	O
determining	O
whether	O
an	O
image	O
contains	O
a	O
face	O
,	O
we	O
need	O
not	O
know	O
the	O
location	O
of	O
the	O
eyes	O
with	O
pixel-perfect	O
accuracy	O
,	O
we	O
just	O
need	O
to	O
know	O
that	O
there	O
is	O
an	O
eye	O
on	O
the	O
left	O
side	O
of	O
the	O
face	O
and	O
an	O
eye	O
on	O
the	O
right	O
side	O
of	O
the	O
face	O
.	O
in	O
other	O
contexts	O
,	O
it	O
is	O
more	O
important	O
to	O
preserve	O
the	O
location	O
of	O
a	O
feature	O
.	O
for	O
example	O
,	O
if	O
we	O
want	O
to	O
ﬁnd	O
a	O
corner	O
deﬁned	O
by	O
two	O
edges	O
meeting	O
at	O
a	O
speciﬁc	O
orientation	O
,	O
we	O
need	O
to	O
preserve	O
the	O
location	O
of	O
the	O
edges	O
well	O
enough	O
to	O
test	O
whether	O
they	O
meet	O
.	O
the	O
use	O
of	O
pooling	O
can	O
be	O
viewed	O
as	O
adding	O
an	O
inﬁnitely	O
strong	O
prior	O
that	O
the	O
function	O
the	O
layer	O
learns	O
must	O
be	O
invariant	O
to	O
small	O
translations	O
.	O
when	O
this	O
assumption	O
is	O
correct	O
,	O
it	O
can	O
greatly	O
improve	O
the	O
statistical	O
eﬃciency	O
of	O
the	O
network	O
.	O
pooling	O
over	O
spatial	O
regions	O
produces	O
invariance	O
to	O
translation	O
,	O
but	O
if	O
we	O
pool	O
over	O
the	O
outputs	O
of	O
separately	O
parametrized	O
convolutions	O
,	O
the	O
features	O
can	O
learn	O
which	O
transformations	O
to	O
become	O
invariant	O
to	O
(	O
see	O
ﬁgure	O
9.9	O
)	O
.	O
9.10	O
because	O
pooling	O
summarizes	O
the	O
responses	O
over	O
a	O
whole	O
neighborhood	O
,	O
it	O
is	O
possible	O
to	O
use	O
fewer	O
pooling	O
units	O
than	O
detector	O
units	O
,	O
by	O
reporting	O
summary	O
statistics	O
for	O
pooling	O
regions	O
spaced	O
k	O
pixels	O
apart	O
rather	O
than	O
1	O
pixel	O
apart	O
.	O
see	O
ﬁgure	O
for	O
an	O
example	O
.	O
this	O
improves	O
the	O
computational	O
eﬃciency	O
of	O
the	O
network	O
because	O
the	O
next	O
layer	O
has	O
roughly	O
k	O
times	O
fewer	O
inputs	O
to	O
process	O
.	O
when	O
the	O
number	O
of	O
parameters	O
in	O
the	O
next	O
layer	O
is	O
a	O
function	O
of	O
its	O
input	O
size	O
(	O
such	O
as	O
when	O
the	O
next	O
layer	O
is	O
fully	O
connected	O
and	O
based	O
on	O
matrix	O
multiplication	O
)	O
this	O
reduction	O
in	O
the	O
input	O
size	O
can	O
also	O
result	O
in	O
improved	O
statistical	O
eﬃciency	O
and	O
reduced	O
memory	O
requirements	O
for	O
storing	O
the	O
parameters	O
.	O
for	O
many	O
tasks	O
,	O
pooling	O
is	O
essential	O
for	O
handling	O
inputs	O
of	O
varying	O
size	O
.	O
for	O
example	O
,	O
if	O
we	O
want	O
to	O
classify	O
images	O
of	O
variable	O
size	O
,	O
the	O
input	O
to	O
the	O
classiﬁcation	O
layer	O
must	O
have	O
a	O
ﬁxed	O
size	O
.	O
this	O
is	O
usually	O
accomplished	O
by	O
varying	O
the	O
size	O
of	O
an	O
oﬀset	O
between	O
pooling	O
regions	O
so	O
that	O
the	O
classiﬁcation	O
layer	O
always	O
receives	O
the	O
same	O
number	O
of	O
summary	O
statistics	O
regardless	O
of	O
the	O
input	O
size	O
.	O
for	O
example	O
,	O
the	O
ﬁnal	O
pooling	O
layer	O
of	O
the	O
network	O
may	O
be	O
deﬁned	O
to	O
output	O
four	O
sets	O
of	O
summary	O
statistics	O
,	O
one	O
for	O
each	O
quadrant	O
of	O
an	O
image	O
,	O
regardless	O
of	O
the	O
image	O
size	O
.	O
342	O
chapter	O
9.	O
convolutional	O
networks	O
...	O
...	O
...	O
...	O
pooling	O
stage	O
1	O
.	O
1	O
.	O
1	O
.	O
0.2	O
0.1	O
1	O
.	O
0.2	O
0.1	O
detector	O
stage	O
pooling	O
stage	O
0.3	O
1	O
.	O
1	O
.	O
1	O
.	O
0.3	O
0.1	O
1	O
.	O
0.2	O
detector	O
stage	O
...	O
...	O
...	O
...	O
figure	O
9.8	O
:	O
max	O
pooling	O
introduces	O
invariance	O
.	O
(	O
top	O
)	O
a	O
view	O
of	O
the	O
middle	O
of	O
the	O
output	O
of	O
a	O
convolutional	O
layer	O
.	O
the	O
bottom	O
row	O
shows	O
outputs	O
of	O
the	O
nonlinearity	O
.	O
the	O
top	O
row	O
shows	O
the	O
outputs	O
of	O
max	O
pooling	O
,	O
with	O
a	O
stride	O
of	O
one	O
pixel	O
between	O
pooling	O
regions	O
and	O
a	O
pooling	O
region	O
width	O
of	O
three	O
pixels	O
.	O
a	O
view	O
of	O
the	O
same	O
network	O
,	O
after	O
the	O
input	O
has	O
been	O
shifted	O
to	O
the	O
right	O
by	O
one	O
pixel	O
.	O
every	O
value	O
in	O
the	O
bottom	O
row	O
has	O
changed	O
,	O
but	O
only	O
half	O
of	O
the	O
values	O
in	O
the	O
top	O
row	O
have	O
changed	O
,	O
because	O
the	O
max	O
pooling	O
units	O
are	O
only	O
sensitive	O
to	O
the	O
maximum	O
value	O
in	O
the	O
neighborhood	O
,	O
not	O
its	O
exact	O
location	O
.	O
(	O
bottom	O
)	O
343	O
chapter	O
9.	O
convolutional	O
networks	O
large	O
response	O
in	O
pooling	O
unit	O
large	O
response	O
in	O
detector	O
unit	O
1	O
large	O
response	O
in	O
pooling	O
unit	O
large	O
response	O
in	O
detector	O
unit	O
3	O
figure	O
9.9	O
:	O
example	O
of	O
learned	O
invariances	O
:	O
a	O
pooling	O
unit	O
that	O
pools	O
over	O
multiple	O
features	O
that	O
are	O
learned	O
with	O
separate	O
parameters	O
can	O
learn	O
to	O
be	O
invariant	O
to	O
transformations	O
of	O
the	O
input	O
.	O
here	O
we	O
show	O
how	O
a	O
set	O
of	O
three	O
learned	O
ﬁlters	O
and	O
a	O
max	O
pooling	O
unit	O
can	O
learn	O
to	O
become	O
invariant	O
to	O
rotation	O
.	O
all	O
three	O
ﬁlters	O
are	O
intended	O
to	O
detect	O
a	O
hand-written	O
5.	O
each	O
ﬁlter	O
attempts	O
to	O
match	O
a	O
slightly	O
diﬀerent	O
orientation	O
of	O
the	O
5.	O
when	O
a	O
5	O
appears	O
in	O
the	O
input	O
,	O
the	O
corresponding	O
ﬁlter	O
will	O
match	O
it	O
and	O
cause	O
a	O
large	O
activation	O
in	O
a	O
detector	O
unit	O
.	O
the	O
max	O
pooling	O
unit	O
then	O
has	O
a	O
large	O
activation	O
regardless	O
of	O
which	O
detector	O
unit	O
was	O
activated	O
.	O
we	O
show	O
here	O
how	O
the	O
network	O
processes	O
two	O
diﬀerent	O
inputs	O
,	O
resulting	O
in	O
two	O
diﬀerent	O
detector	O
units	O
being	O
activated	O
.	O
the	O
eﬀect	O
on	O
the	O
pooling	O
unit	O
is	O
roughly	O
the	O
same	O
either	O
way	O
.	O
this	O
principle	O
is	O
leveraged	O
by	O
maxout	O
networks	O
(	O
goodfellow	O
et	O
al.	O
,	O
2013a	O
)	O
and	O
other	O
convolutional	O
networks	O
.	O
max	O
pooling	O
over	O
spatial	O
positions	O
is	O
naturally	O
invariant	O
to	O
translation	O
;	O
this	O
multi-channel	O
approach	O
is	O
only	O
necessary	O
for	O
learning	O
other	O
transformations	O
.	O
1	O
.	O
0.2	O
0.1	O
0.1	O
1	O
.	O
0.2	O
0.1	O
0.0	O
0.1	O
figure	O
9.10	O
:	O
pooling	O
with	O
downsampling	O
.	O
here	O
we	O
use	O
max-pooling	O
with	O
a	O
pool	O
width	O
of	O
three	O
and	O
a	O
stride	O
between	O
pools	O
of	O
two	O
.	O
this	O
reduces	O
the	O
representation	O
size	O
by	O
a	O
factor	O
of	O
two	O
,	O
which	O
reduces	O
the	O
computational	O
and	O
statistical	O
burden	O
on	O
the	O
next	O
layer	O
.	O
note	O
that	O
the	O
rightmost	O
pooling	O
region	O
has	O
a	O
smaller	O
size	O
,	O
but	O
must	O
be	O
included	O
if	O
we	O
do	O
not	O
want	O
to	O
ignore	O
some	O
of	O
the	O
detector	O
units	O
.	O
344	O
chapter	O
9.	O
convolutional	O
networks	O
some	O
theoretical	O
work	B
gives	O
guidance	O
as	O
to	O
which	O
kinds	O
of	O
pooling	O
one	O
should	O
use	O
in	O
various	O
situations	O
(	O
)	O
.	O
it	O
is	O
also	O
possible	O
to	O
dynamically	O
pool	O
features	O
together	O
,	O
for	O
example	O
,	O
by	O
running	O
a	O
clustering	O
algorithm	O
on	O
the	O
locations	O
of	O
interesting	O
features	O
(	O
)	O
.	O
this	O
approach	O
yields	O
a	O
diﬀerent	O
set	O
of	O
pooling	O
regions	O
for	O
each	O
image	O
.	O
another	O
approach	O
is	O
to	O
learn	O
a	O
single	O
pooling	O
structure	O
that	O
is	O
then	O
applied	O
to	O
all	O
images	O
(	O
boureau	O
et	O
al	O
.	O
2011	O
boureau	O
et	O
al	O
.	O
2010	O
)	O
.	O
jia	O
et	O
al	O
.	O
2012	O
,	O
,	O
,	O
pooling	O
can	O
complicate	O
some	O
kinds	O
of	O
neural	O
network	O
architectures	O
that	O
use	O
top-down	O
information	O
,	O
such	O
as	O
boltzmann	O
machines	O
and	O
autoencoders	O
.	O
these	O
issues	O
will	O
be	O
discussed	O
further	O
when	O
we	O
present	O
these	O
types	O
of	O
networks	O
in	O
part	O
.	O
iii	O
.	O
the	O
pooling	O
in	O
convolutional	O
boltzmann	O
machines	O
is	O
presented	O
in	O
section	O
inverse-like	O
operations	O
on	O
pooling	O
units	O
needed	O
in	O
some	O
diﬀerentiable	O
networks	O
will	O
be	O
covered	O
in	O
section	O
20.10.6	O
.	O
20.6	O
some	O
examples	O
of	O
complete	O
convolutional	O
network	O
architectures	O
for	O
classiﬁcation	O
using	O
convolution	O
and	O
pooling	O
are	O
shown	O
in	O
ﬁgure	O
.	O
9.11	O
9.4	O
convolution	O
and	O
pooling	O
as	O
an	O
inﬁnitely	O
strong	O
prior	O
recall	O
the	O
concept	O
of	O
a	O
prior	O
probability	O
distribution	O
from	O
section	O
.	O
this	O
is	O
a	O
probability	O
distribution	O
over	O
the	O
parameters	O
of	O
a	O
model	B
that	O
encodes	O
our	O
beliefs	O
about	O
what	O
models	O
are	O
reasonable	O
,	O
before	O
we	O
have	O
seen	O
any	O
data	O
.	O
5.2	O
priors	O
can	O
be	O
considered	O
weak	O
or	O
strong	O
depending	O
on	O
how	O
concentrated	O
the	O
probability	O
density	O
in	O
the	O
prior	O
is	O
.	O
a	O
weak	O
prior	O
is	O
a	O
prior	O
distribution	O
with	O
high	O
entropy	O
,	O
such	O
as	O
a	O
gaussian	O
distribution	O
with	O
high	O
variance	O
.	O
such	O
a	O
prior	O
allows	O
the	O
data	O
to	O
move	O
the	O
parameters	O
more	O
or	O
less	O
freely	O
.	O
a	O
strong	O
prior	O
has	O
very	O
low	O
entropy	O
,	O
such	O
as	O
a	O
gaussian	O
distribution	O
with	O
low	O
variance	O
.	O
such	O
a	O
prior	O
plays	O
a	O
more	O
active	O
role	O
in	O
determining	O
where	O
the	O
parameters	O
end	O
up	O
.	O
an	O
inﬁnitely	O
strong	O
prior	O
places	O
zero	O
probability	O
on	O
some	O
parameters	O
and	O
says	O
that	O
these	O
parameter	O
values	O
are	O
completely	O
forbidden	O
,	O
regardless	O
of	O
how	O
much	O
support	O
the	O
data	O
gives	O
to	O
those	O
values	O
.	O
we	O
can	O
imagine	O
a	O
convolutional	O
net	O
as	O
being	O
similar	O
to	O
a	O
fully	O
connected	O
net	O
,	O
but	O
with	O
an	O
inﬁnitely	O
strong	O
prior	O
over	O
its	O
weights	O
.	O
this	O
inﬁnitely	O
strong	O
prior	O
says	O
that	O
the	O
weights	O
for	O
one	O
hidden	O
unit	O
must	O
be	O
identical	O
to	O
the	O
weights	O
of	O
its	O
neighbor	O
,	O
but	O
shifted	O
in	O
space	O
.	O
the	O
prior	O
also	O
says	O
that	O
the	O
weights	O
must	O
be	O
zero	O
,	O
except	O
for	O
in	O
the	O
small	O
,	O
spatially	O
contiguous	O
receptive	O
ﬁeld	O
assigned	O
to	O
that	O
hidden	O
unit	O
.	O
overall	O
,	O
we	O
can	O
think	O
of	O
the	O
use	O
of	O
convolution	O
as	O
introducing	O
an	O
inﬁnitely	O
strong	O
prior	O
probability	O
distribution	O
over	O
the	O
parameters	O
of	O
a	O
layer	O
.	O
this	O
prior	O
345	O
chapter	O
9.	O
convolutional	O
networks	O
output	O
of	O
softmax	O
:	O
output	O
of	O
softmax	O
:	O
output	O
of	O
softmax	O
:	O
1,000	O
class	O
probabilities	O
1,000	O
class	O
probabilities	O
1,000	O
class	O
probabilities	O
output	O
of	O
matrix	O
multiply	O
:	O
1,000	O
units	O
output	O
of	O
matrix	O
multiply	O
:	O
1,000	O
units	O
output	O
of	O
average	O
pooling	O
:	O
1x1x1,000	O
output	O
of	O
reshape	O
to	O
output	O
of	O
reshape	O
to	O
vector	O
:	O
16,384	O
units	O
vector	O
:	O
576	O
units	O
output	O
of	O
convolution	O
:	O
16x16x1,000	O
output	O
of	O
pooling	O
with	O
stride	O
4	O
:	O
16x16x64	O
output	O
of	O
convolution	O
+	O
relu	O
:	O
64x64x64	O
output	O
of	O
pooling	O
to	O
3x3	O
grid	O
:	O
3x3x64	O
output	O
of	O
convolution	O
+	O
relu	O
:	O
64x64x64	O
output	O
of	O
pooling	O
with	O
stride	O
4	O
:	O
16x16x64	O
output	O
of	O
convolution	O
+	O
relu	O
:	O
64x64x64	O
output	O
of	O
pooling	O
output	O
of	O
pooling	O
output	O
of	O
pooling	O
with	O
stride	O
4	O
:	O
64x64x64	O
output	O
of	O
convolution	O
+	O
with	O
stride	O
4	O
:	O
64x64x64	O
output	O
of	O
convolution	O
+	O
with	O
stride	O
4	O
:	O
64x64x64	O
output	O
of	O
convolution	O
+	O
relu	O
:	O
256x256x64	O
relu	O
:	O
256x256x64	O
relu	O
:	O
256x256x64	O
input	O
image	O
:	O
256x256x3	O
input	O
image	O
:	O
256x256x3	O
input	O
image	O
:	O
256x256x3	O
figure	O
9.11	O
:	O
examples	O
of	O
architectures	O
for	O
classiﬁcation	O
with	O
convolutional	O
networks	O
.	O
the	O
speciﬁc	O
strides	O
and	O
depths	O
used	O
in	O
this	O
ﬁgure	O
are	O
not	O
advisable	O
for	O
real	O
use	O
;	O
they	O
are	O
designed	O
to	O
be	O
very	O
shallow	O
in	O
order	O
to	O
ﬁt	O
onto	O
the	O
page	O
.	O
real	O
convolutional	O
networks	O
also	O
often	O
involve	O
signiﬁcant	O
amounts	O
of	O
branching	O
,	O
unlike	O
the	O
chain	O
structures	O
used	O
here	O
for	O
simplicity	O
.	O
(	O
left	O
)	O
a	O
convolutional	O
network	O
that	O
processes	O
a	O
ﬁxed	O
image	O
size	O
.	O
after	O
alternating	O
between	O
convolution	O
and	O
pooling	O
for	O
a	O
few	O
layers	O
,	O
the	O
tensor	O
for	O
the	O
convolutional	O
feature	O
map	O
is	O
reshaped	O
to	O
ﬂatten	O
out	O
the	O
spatial	O
dimensions	O
.	O
the	O
rest	O
of	O
the	O
network	O
is	O
an	O
ordinary	O
feedforward	O
network	O
classiﬁer	O
,	O
as	O
described	O
in	O
chapter	O
.6	O
(	O
center	O
)	O
a	O
convolutional	O
network	O
that	O
processes	O
a	O
variable-sized	O
image	O
,	O
but	O
still	O
maintains	O
a	O
fully	O
connected	O
section	O
.	O
this	O
network	O
uses	O
a	O
pooling	O
operation	O
with	O
variably-sized	O
pools	O
but	O
a	O
ﬁxed	O
number	O
of	O
pools	O
,	O
in	O
order	O
to	O
provide	O
a	O
ﬁxed-size	O
vector	O
of	O
576	O
units	O
to	O
the	O
fully	O
connected	O
portion	O
of	O
the	O
network	O
.	O
a	O
convolutional	O
network	O
that	O
does	O
not	O
have	O
any	O
fully	O
connected	O
weight	O
layer	O
.	O
instead	O
,	O
the	O
last	O
convolutional	O
layer	O
outputs	O
one	O
feature	O
map	O
per	O
class	O
.	O
the	O
model	B
presumably	O
learns	O
a	O
map	O
of	O
how	O
likely	O
each	O
class	O
is	O
to	O
occur	O
at	O
each	O
spatial	O
location	O
.	O
averaging	O
a	O
feature	O
map	O
down	O
to	O
a	O
single	O
value	O
provides	O
the	O
argument	O
to	O
the	O
softmax	O
classiﬁer	O
at	O
the	O
top	O
.	O
(	O
right	O
)	O
346	O
chapter	O
9.	O
convolutional	O
networks	O
says	O
that	O
the	O
function	O
the	O
layer	O
should	O
learn	O
contains	O
only	O
local	O
interactions	O
and	O
is	O
equivariant	O
to	O
translation	O
.	O
likewise	O
,	O
the	O
use	O
of	O
pooling	O
is	O
an	O
inﬁnitely	O
strong	O
prior	O
that	O
each	O
unit	O
should	O
be	O
invariant	O
to	O
small	O
translations	O
.	O
of	O
course	O
,	O
implementing	O
a	O
convolutional	O
net	O
as	O
a	O
fully	O
connected	O
net	O
with	O
an	O
inﬁnitely	O
strong	O
prior	O
would	O
be	O
extremely	O
computationally	O
wasteful	O
.	O
but	O
thinking	O
of	O
a	O
convolutional	O
net	O
as	O
a	O
fully	O
connected	O
net	O
with	O
an	O
inﬁnitely	O
strong	O
prior	O
can	O
give	O
us	O
some	O
insights	O
into	O
how	O
convolutional	O
nets	O
work	B
.	O
one	O
key	O
insight	O
is	O
that	O
convolution	O
and	O
pooling	O
can	O
cause	O
underﬁtting	O
.	O
like	O
any	O
prior	O
,	O
convolution	O
and	O
pooling	O
are	O
only	O
useful	O
when	O
the	O
assumptions	O
made	O
by	O
the	O
prior	O
are	O
reasonably	O
accurate	O
.	O
if	O
a	O
task	O
relies	O
on	O
preserving	O
precise	O
spatial	O
information	O
,	O
then	O
using	O
pooling	O
on	O
all	O
features	O
can	O
increase	O
the	O
training	O
error	O
.	O
some	O
convolutional	O
network	O
architectures	O
(	O
)	O
are	O
designed	O
to	O
use	O
pooling	O
on	O
some	O
channels	O
but	O
not	O
on	O
other	O
channels	O
,	O
in	O
order	O
to	O
get	O
both	O
highly	O
invariant	O
features	O
and	O
features	O
that	O
will	O
not	O
underﬁt	O
when	O
the	O
translation	O
invariance	O
prior	O
is	O
incorrect	O
.	O
when	O
a	O
task	O
involves	O
incorporating	O
information	O
from	O
very	O
distant	O
locations	O
in	O
the	O
input	O
,	O
then	O
the	O
prior	O
imposed	O
by	O
convolution	O
may	O
be	O
inappropriate	O
.	O
szegedy	O
et	O
al	O
.	O
2014a	O
,	O
another	O
key	O
insight	O
from	O
this	O
view	O
is	O
that	O
we	O
should	O
only	O
compare	O
convolu-	O
tional	O
models	O
to	O
other	O
convolutional	O
models	O
in	O
benchmarks	O
of	O
statistical	O
learning	O
performance	O
.	O
models	O
that	O
do	O
not	O
use	O
convolution	O
would	O
be	O
able	O
to	O
learn	O
even	O
if	O
we	O
permuted	O
all	O
of	O
the	O
pixels	O
in	O
the	O
image	O
.	O
for	O
many	O
image	O
datasets	O
,	O
there	O
are	O
separate	O
benchmarks	O
for	O
models	O
that	O
are	O
permutation	O
invariant	O
and	O
must	O
discover	O
the	O
concept	O
of	O
topology	O
via	O
learning	O
,	O
and	O
models	O
that	O
have	O
the	O
knowledge	O
of	O
spatial	O
relationships	O
hard-coded	O
into	O
them	O
by	O
their	O
designer	O
.	O
9.5	O
variants	O
of	O
the	O
basic	O
convolution	O
function	O
when	O
discussing	O
convolution	O
in	O
the	O
context	O
of	O
neural	O
networks	O
,	O
we	O
usually	O
do	O
not	O
refer	O
exactly	O
to	O
the	O
standard	O
discrete	O
convolution	O
operation	O
as	O
it	O
is	O
usually	O
understood	O
in	O
the	O
mathematical	O
literature	O
.	O
the	O
functions	O
used	O
in	O
practice	O
diﬀer	O
slightly	O
.	O
here	O
we	O
describe	O
these	O
diﬀerences	O
in	O
detail	O
,	O
and	O
highlight	O
some	O
useful	O
properties	O
of	O
the	O
functions	O
used	O
in	O
neural	O
networks	O
.	O
first	O
,	O
when	O
we	O
refer	O
to	O
convolution	O
in	O
the	O
context	O
of	O
neural	O
networks	O
,	O
we	O
usually	O
actually	O
mean	O
an	O
operation	O
that	O
consists	O
of	O
many	O
applications	O
of	O
convolution	O
in	O
parallel	O
.	O
this	O
is	O
because	O
convolution	O
with	O
a	O
single	O
kernel	O
can	O
only	O
extract	O
one	O
kind	O
of	O
feature	O
,	O
albeit	O
at	O
many	O
spatial	O
locations	O
.	O
usually	O
we	O
want	O
each	O
layer	O
of	O
our	O
network	O
to	O
extract	O
many	O
kinds	O
of	O
features	O
,	O
at	O
many	O
locations	O
.	O
347	O
chapter	O
9.	O
convolutional	O
networks	O
additionally	O
,	O
the	O
input	O
is	O
usually	O
not	O
just	O
a	O
grid	O
of	O
real	O
values	O
.	O
rather	O
,	O
it	O
is	O
a	O
grid	O
of	O
vector-valued	O
observations	O
.	O
for	O
example	O
,	O
a	O
color	O
image	O
has	O
a	O
red	O
,	O
green	O
and	O
blue	O
intensity	O
at	O
each	O
pixel	O
.	O
in	O
a	O
multilayer	O
convolutional	O
network	O
,	O
the	O
input	O
to	O
the	O
second	O
layer	O
is	O
the	O
output	O
of	O
the	O
ﬁrst	O
layer	O
,	O
which	O
usually	O
has	O
the	O
output	O
of	O
many	O
diﬀerent	O
convolutions	O
at	O
each	O
position	B
.	O
when	O
working	O
with	O
images	O
,	O
we	O
usually	O
think	O
of	O
the	O
input	O
and	O
output	O
of	O
the	O
convolution	O
as	O
being	O
3-d	O
tensors	O
,	O
with	O
one	O
index	O
into	O
the	O
diﬀerent	O
channels	O
and	O
two	O
indices	O
into	O
the	O
spatial	O
coordinates	O
of	O
each	O
channel	O
.	O
software	O
implementations	O
usually	O
work	B
in	O
batch	O
mode	O
,	O
so	O
they	O
will	O
actually	O
use	O
4-d	O
tensors	O
,	O
with	O
the	O
fourth	O
axis	O
indexing	O
diﬀerent	O
examples	O
in	O
the	O
batch	O
,	O
but	O
we	O
will	O
omit	O
the	O
batch	O
axis	O
in	O
our	O
description	O
here	O
for	O
simplicity	O
.	O
because	O
convolutional	O
networks	O
usually	O
use	O
multi-channel	O
convolution	O
,	O
the	O
linear	O
operations	O
they	O
are	O
based	O
on	O
are	O
not	O
guaranteed	O
to	O
be	O
commutative	O
,	O
even	O
if	O
kernel-ﬂipping	O
is	O
used	O
.	O
these	O
multi-channel	O
operations	O
are	O
only	O
commutative	O
if	O
each	O
operation	O
has	O
the	O
same	O
number	O
of	O
output	O
channels	O
as	O
input	O
channels	O
.	O
assume	O
we	O
have	O
a	O
4-d	O
kernel	O
tensor	O
k	O
with	O
element	O
ki	O
,	O
j	O
,	O
k	O
,	O
l	O
giving	O
the	O
connection	O
strength	O
between	O
a	O
unit	O
in	O
channel	O
i	O
of	O
the	O
output	O
and	O
a	O
unit	O
in	O
channel	O
j	O
of	O
the	O
input	O
,	O
with	O
an	O
oﬀset	O
of	O
k	O
rows	O
and	O
l	O
columns	O
between	O
the	O
output	O
unit	O
and	O
the	O
input	O
unit	O
.	O
assume	O
our	O
input	O
consists	O
of	O
observed	O
data	O
v	O
with	O
element	O
vi	O
,	O
j	O
,	O
k	O
giving	O
the	O
value	O
of	O
the	O
input	O
unit	O
within	O
channel	O
i	O
at	O
row	O
j	O
and	O
column	O
k.	O
assume	O
our	O
output	O
consists	O
of	O
z	O
with	O
the	O
same	O
format	O
as	O
v.	O
if	O
z	O
is	O
produced	O
by	O
convolving	O
k	O
across	O
without	O
ﬂipping	O
	O
,	O
then	O
v	O
k	O
zi	O
,	O
j	O
,	O
k	O
=	O
l	O
,	O
m	O
,	O
n	O
−	O
1ki	O
,	O
l	O
,	O
m	O
,	O
n	O
vl	O
,	O
j	O
m	O
,	O
k	O
n	O
−	O
1	O
+	O
+	O
(	O
9.7	O
)	O
where	O
the	O
summation	O
over	O
l	O
,	O
m	O
and	O
n	O
is	O
over	O
all	O
values	O
for	O
which	O
the	O
tensor	O
indexing	O
operations	O
inside	O
the	O
summation	O
is	O
valid	O
.	O
in	O
linear	O
algebra	O
notation	O
,	O
we	O
index	O
into	O
arrays	O
using	O
a	O
1	O
in	O
the	O
above	O
formula	O
.	O
programming	O
languages	O
such	O
as	O
c	O
and	O
python	O
index	O
starting	O
from	O
,	O
rendering	O
the	O
above	O
expression	O
even	O
simpler	O
.	O
for	O
the	O
ﬁrst	O
entry	O
.	O
this	O
necessitates	O
the	O
−	O
0	O
1	O
we	O
may	O
want	O
to	O
skip	O
over	O
some	O
positions	O
of	O
the	O
kernel	O
in	O
order	O
to	O
reduce	O
the	O
computational	O
cost	O
(	O
at	O
the	O
expense	O
of	O
not	O
extracting	O
our	O
features	O
as	O
ﬁnely	O
)	O
.	O
we	O
can	O
think	O
of	O
this	O
as	O
downsampling	O
the	O
output	O
of	O
the	O
full	O
convolution	O
function	O
.	O
if	O
we	O
want	O
to	O
sample	O
only	O
every	O
s	O
pixels	O
in	O
each	O
direction	O
in	O
the	O
output	O
,	O
then	O
we	O
can	O
deﬁne	O
a	O
downsampled	O
convolution	O
function	O
such	O
that	O
	O
	O
	O
c	O
zi	O
,	O
j	O
,	O
k	O
=	O
(	O
c	O
k	O
v	O
,	O
,	O
s	O
i	O
,	O
j	O
,	O
k	O
=	O
)	O
−	O
×	O
1	O
)	O
+	O
(	O
s	O
m	O
,	O
k	O
−	O
×	O
1	O
)	O
+	O
ki	O
,	O
l	O
,	O
m	O
,	O
n	O
s	O
n	O
.	O
vl	O
,	O
j	O
(	O
(	O
9.8	O
)	O
l	O
,	O
m	O
,	O
n	O
we	O
refer	O
to	O
s	O
as	O
the	O
stride	O
of	O
this	O
downsampled	O
convolution	O
.	O
it	O
is	O
also	O
possible	O
348	O
chapter	O
9.	O
convolutional	O
networks	O
to	O
deﬁne	O
a	O
separate	O
stride	O
for	O
each	O
direction	O
of	O
motion	O
.	O
see	O
ﬁgure	O
illustration	O
.	O
9.12	O
for	O
an	O
one	O
essential	O
feature	O
of	O
any	O
convolutional	O
network	O
implementation	O
is	O
the	O
ability	O
to	O
implicitly	O
zero-pad	O
the	O
input	O
v	O
in	O
order	O
to	O
make	O
it	O
wider	O
.	O
without	O
this	O
feature	O
,	O
the	O
width	O
of	O
the	O
representation	O
shrinks	O
by	O
one	O
pixel	O
less	O
than	O
the	O
kernel	O
width	O
at	O
each	O
layer	O
.	O
zero	O
padding	O
the	O
input	O
allows	O
us	O
to	O
control	O
the	O
kernel	O
width	O
and	O
the	O
size	O
of	O
the	O
output	O
independently	O
.	O
without	O
zero	O
padding	O
,	O
we	O
are	O
forced	O
to	O
choose	O
between	O
shrinking	O
the	O
spatial	O
extent	O
of	O
the	O
network	O
rapidly	O
and	O
using	O
small	O
kernels—both	O
scenarios	O
that	O
signiﬁcantly	O
limit	O
the	O
expressive	O
power	O
of	O
the	O
network	O
.	O
see	O
ﬁgure	O
for	O
an	O
example	O
.	O
9.13	O
×	O
three	O
special	O
cases	O
of	O
the	O
zero-padding	O
setting	O
are	O
worth	O
mentioning	O
.	O
one	O
is	O
the	O
extreme	O
case	O
in	O
which	O
no	O
zero-padding	O
is	O
used	O
whatsoever	O
,	O
and	O
the	O
convolution	O
kernel	O
is	O
only	O
allowed	O
to	O
visit	O
positions	O
where	O
the	O
entire	O
kernel	O
is	O
contained	O
entirely	O
within	O
the	O
image	O
.	O
in	O
matlab	O
terminology	O
,	O
this	O
is	O
called	O
valid	O
convolution	O
.	O
in	O
this	O
case	O
,	O
all	O
pixels	O
in	O
the	O
output	O
are	O
a	O
function	O
of	O
the	O
same	O
number	O
of	O
pixels	O
in	O
the	O
input	O
,	O
so	O
the	O
behavior	O
of	O
an	O
output	O
pixel	O
is	O
somewhat	O
more	O
regular	O
.	O
however	O
,	O
−	O
the	O
size	O
of	O
the	O
output	O
shrinks	O
at	O
each	O
layer	O
.	O
if	O
the	O
input	O
image	O
has	O
width	O
m	O
and	O
the	O
kernel	O
has	O
width	O
k	O
,	O
the	O
output	O
will	O
be	O
of	O
width	O
m	O
k	O
+	O
1.	O
the	O
rate	O
of	O
this	O
shrinkage	O
can	O
be	O
dramatic	O
if	O
the	O
kernels	O
used	O
are	O
large	O
.	O
since	O
the	O
shrinkage	O
is	O
greater	O
than	O
0	O
,	O
it	O
limits	O
the	O
number	O
of	O
convolutional	O
layers	O
that	O
can	O
be	O
included	O
in	O
the	O
network	O
.	O
as	O
layers	O
are	O
added	O
,	O
the	O
spatial	O
dimension	O
of	O
the	O
network	O
will	O
eventually	O
drop	O
to	O
1	O
1	O
,	O
at	O
which	O
point	O
additional	O
layers	O
can	O
not	O
meaningfully	O
be	O
considered	O
convolutional	O
.	O
another	O
special	O
case	O
of	O
the	O
zero-padding	O
setting	O
is	O
when	O
just	O
enough	O
zero-padding	O
is	O
added	O
to	O
keep	O
the	O
size	O
of	O
the	O
output	O
equal	O
to	O
the	O
size	O
of	O
the	O
input	O
.	O
matlab	O
calls	O
this	O
same	O
convolution	O
.	O
in	O
this	O
case	O
,	O
the	O
network	O
can	O
contain	O
as	O
many	O
convolutional	O
layers	O
as	O
the	O
available	O
hardware	O
can	O
support	O
,	O
since	O
the	O
operation	O
of	O
convolution	O
does	O
not	O
modify	O
the	O
architectural	O
possibilities	O
available	O
to	O
the	O
next	O
layer	O
.	O
however	O
,	O
the	O
input	O
pixels	O
near	O
the	O
border	O
inﬂuence	O
fewer	O
output	O
pixels	O
than	O
the	O
input	O
pixels	O
near	O
the	O
center	O
.	O
this	O
can	O
make	O
the	O
border	O
pixels	O
somewhat	O
underrepresented	O
in	O
the	O
model	B
.	O
this	O
motivates	O
the	O
other	O
extreme	O
case	O
,	O
which	O
matlab	O
refers	O
to	O
as	O
full	O
convolution	O
,	O
in	O
which	O
enough	O
zeroes	O
are	O
added	O
for	O
every	O
pixel	O
to	O
be	O
visited	O
k	O
times	O
in	O
each	O
direction	O
,	O
resulting	O
in	O
an	O
output	O
image	O
of	O
width	O
m	O
+	O
k	O
1.	O
in	O
this	O
case	O
,	O
the	O
output	O
pixels	O
near	O
the	O
border	O
are	O
a	O
function	O
of	O
fewer	O
pixels	O
than	O
the	O
output	O
pixels	O
near	O
the	O
center	O
.	O
this	O
can	O
make	O
it	O
diﬃcult	O
to	O
learn	O
a	O
single	O
kernel	O
that	O
performs	O
well	O
at	O
all	O
positions	O
in	O
the	O
convolutional	O
feature	O
map	O
.	O
usually	O
the	O
optimal	O
amount	O
of	O
zero	O
padding	O
(	O
in	O
terms	O
of	O
test	O
set	O
classiﬁcation	O
accuracy	O
)	O
lies	O
somewhere	O
between	O
“	O
valid	O
”	O
and	O
“	O
same	O
”	O
convolution	O
.	O
−	O
349	O
chapter	O
9.	O
convolutional	O
networks	O
s1s1	O
s2s2	O
s3s3	O
x1x1	O
x2x2	O
x3x3	O
x4x4	O
x5x5	O
s1s1	O
s2s2	O
s3s3	O
z1z1	O
z2z2	O
z3z3	O
z4z4	O
z5z5	O
strided	O
convolution	O
downsampling	O
convolution	O
x1x1	O
x2x2	O
x3x3	O
x4x4	O
x5x5	O
figure	O
9.12	O
:	O
convolution	O
with	O
a	O
stride	O
.	O
in	O
this	O
example	O
,	O
we	O
use	O
a	O
stride	O
of	O
two	O
.	O
(	O
top	O
)	O
convolution	O
with	O
a	O
stride	O
length	O
of	O
two	O
implemented	O
in	O
a	O
single	O
operation	O
.	O
(	O
bot-	O
tom	O
)	O
convolution	O
with	O
a	O
stride	O
greater	O
than	O
one	O
pixel	O
is	O
mathematically	O
equivalent	O
to	O
convolution	O
with	O
unit	O
stride	O
followed	O
by	O
downsampling	O
.	O
obviously	O
,	O
the	O
two-step	O
approach	O
involving	O
downsampling	O
is	O
computationally	O
wasteful	O
,	O
because	O
it	O
computes	O
many	O
values	O
that	O
are	O
then	O
discarded	O
.	O
350	O
chapter	O
9.	O
convolutional	O
networks	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
figure	O
9.13	O
:	O
the	O
eﬀect	O
of	O
zero	O
padding	O
on	O
network	O
size	O
:	O
consider	O
a	O
convolutional	O
network	O
with	O
a	O
kernel	O
of	O
width	O
six	O
at	O
every	O
layer	O
.	O
in	O
this	O
example	O
,	O
we	O
do	O
not	O
use	O
any	O
pooling	O
,	O
so	O
only	O
the	O
convolution	O
operation	O
itself	O
shrinks	O
the	O
network	O
size	O
.	O
(	O
top	O
)	O
in	O
this	O
convolutional	O
network	O
,	O
we	O
do	O
not	O
use	O
any	O
implicit	O
zero	O
padding	O
.	O
this	O
causes	O
the	O
representation	O
to	O
shrink	O
by	O
ﬁve	O
pixels	O
at	O
each	O
layer	O
.	O
starting	O
from	O
an	O
input	O
of	O
sixteen	O
pixels	O
,	O
we	O
are	O
only	O
able	O
to	O
have	O
three	O
convolutional	O
layers	O
,	O
and	O
the	O
last	O
layer	O
does	O
not	O
ever	O
move	O
the	O
kernel	O
,	O
so	O
arguably	O
only	O
two	O
of	O
the	O
layers	O
are	O
truly	O
convolutional	O
.	O
the	O
rate	O
of	O
shrinking	O
can	O
be	O
mitigated	O
by	O
using	O
smaller	O
kernels	O
,	O
but	O
smaller	O
kernels	O
are	O
less	O
expressive	O
and	O
some	O
shrinking	O
is	O
inevitable	O
in	O
this	O
kind	O
of	O
architecture	O
.	O
by	O
adding	O
ﬁve	O
implicit	O
zeroes	O
to	O
each	O
layer	O
,	O
we	O
prevent	O
the	O
representation	O
from	O
shrinking	O
with	O
depth	O
.	O
this	O
allows	O
us	O
to	O
make	O
an	O
arbitrarily	O
deep	O
convolutional	O
network	O
.	O
(	O
bottom	O
)	O
351	O
chapter	O
9.	O
convolutional	O
networks	O
,	O
,	O
lecun	O
1986	O
1989	O
in	O
some	O
cases	O
,	O
we	O
do	O
not	O
actually	O
want	O
to	O
use	O
convolution	O
,	O
but	O
rather	O
locally	O
connected	O
layers	O
(	O
)	O
.	O
in	O
this	O
case	O
,	O
the	O
adjacency	O
matrix	O
in	O
the	O
graph	O
of	O
our	O
mlp	O
is	O
the	O
same	O
,	O
but	O
every	O
connection	O
has	O
its	O
own	O
weight	O
,	O
speciﬁed	O
by	O
a	O
6-d	O
tensor	O
w.	O
the	O
indices	O
into	O
w	O
are	O
respectively	O
:	O
i	O
,	O
the	O
output	O
channel	O
,	O
j	O
,	O
the	O
output	O
row	O
,	O
k	O
,	O
the	O
output	O
column	O
,	O
l	O
,	O
the	O
input	O
channel	O
,	O
m	O
,	O
the	O
row	O
oﬀset	O
within	O
the	O
input	O
,	O
and	O
n	O
,	O
the	O
column	O
oﬀset	O
within	O
the	O
input	O
.	O
the	O
linear	O
part	O
of	O
a	O
locally	O
connected	O
layer	O
is	O
then	O
given	O
by	O
	O
zi	O
,	O
j	O
,	O
k	O
=	O
−	O
−	O
[	O
vl	O
,	O
j	O
m	O
,	O
k	O
n	O
1wi	O
,	O
j	O
,	O
k	O
,	O
l	O
,	O
m	O
,	O
n	O
]	O
.	O
1	O
+	O
+	O
(	O
9.9	O
)	O
l	O
,	O
m	O
,	O
n	O
this	O
is	O
sometimes	O
also	O
called	O
unshared	O
convolution	O
,	O
because	O
it	O
is	O
a	O
similar	O
oper-	O
ation	O
to	O
discrete	O
convolution	O
with	O
a	O
small	O
kernel	O
,	O
but	O
without	O
sharing	O
parameters	O
across	O
locations	O
.	O
figure	O
compares	O
local	O
connections	O
,	O
convolution	O
,	O
and	O
full	O
connections	O
.	O
9.14	O
locally	O
connected	O
layers	O
are	O
useful	O
when	O
we	O
know	O
that	O
each	O
feature	O
should	O
be	O
a	O
function	O
of	O
a	O
small	O
part	O
of	O
space	O
,	O
but	O
there	O
is	O
no	O
reason	O
to	O
think	O
that	O
the	O
same	O
feature	O
should	O
occur	O
across	O
all	O
of	O
space	O
.	O
for	O
example	O
,	O
if	O
we	O
want	O
to	O
tell	O
if	O
an	O
image	O
is	O
a	O
picture	O
of	O
a	O
face	O
,	O
we	O
only	O
need	O
to	O
look	O
for	O
the	O
mouth	O
in	O
the	O
bottom	O
half	O
of	O
the	O
image	O
.	O
it	O
can	O
also	O
be	O
useful	O
to	O
make	O
versions	O
of	O
convolution	O
or	O
locally	O
connected	O
layers	O
in	O
which	O
the	O
connectivity	O
is	O
further	O
restricted	O
,	O
for	O
example	O
to	O
constrain	O
each	O
output	O
channel	O
i	O
to	O
be	O
a	O
function	O
of	O
only	O
a	O
subset	O
of	O
the	O
input	O
channels	O
l.	O
a	O
common	O
way	O
to	O
do	O
this	O
is	O
to	O
make	O
the	O
ﬁrst	O
m	O
output	O
channels	O
connect	O
to	O
only	O
the	O
ﬁrst	O
n	O
input	O
channels	O
,	O
the	O
second	O
m	O
output	O
channels	O
connect	O
to	O
only	O
the	O
second	O
n	O
input	O
channels	O
,	O
and	O
so	O
on	O
.	O
see	O
ﬁgure	O
for	O
an	O
example	O
.	O
modeling	O
interactions	O
between	O
few	O
channels	O
allows	O
the	O
network	O
to	O
have	O
fewer	O
parameters	O
in	O
order	O
to	O
reduce	O
memory	O
consumption	O
and	O
increase	O
statistical	O
eﬃciency	O
,	O
and	O
also	O
reduces	O
the	O
amount	O
of	O
computation	O
needed	O
to	O
perform	O
forward	O
and	O
back-propagation	O
.	O
it	O
accomplishes	O
these	O
goals	O
without	O
reducing	O
the	O
number	O
of	O
hidden	O
units	O
.	O
9.15	O
,	O
;	O
,	O
tiled	O
convolution	O
(	O
gregor	O
and	O
lecun	O
2010a	O
le	O
et	O
al	O
.	O
2010	O
)	O
oﬀers	O
a	O
com-	O
promise	O
between	O
a	O
convolutional	O
layer	O
and	O
a	O
locally	O
connected	O
layer	O
.	O
rather	O
than	O
learning	O
a	O
separate	O
set	O
of	O
weights	O
at	O
spatial	O
location	O
,	O
we	O
learn	O
a	O
set	O
of	O
kernels	O
that	O
we	O
rotate	O
through	O
as	O
we	O
move	O
through	O
space	O
.	O
this	O
means	O
that	O
immediately	O
neighboring	O
locations	O
will	O
have	O
diﬀerent	O
ﬁlters	O
,	O
like	O
in	O
a	O
locally	O
connected	O
layer	O
,	O
but	O
the	O
memory	O
requirements	O
for	O
storing	O
the	O
parameters	O
will	O
increase	O
only	O
by	O
a	O
factor	O
of	O
the	O
size	O
of	O
this	O
set	O
of	O
kernels	O
,	O
rather	O
than	O
the	O
size	O
of	O
the	O
entire	O
output	O
feature	O
map	O
.	O
see	O
ﬁgure	O
for	O
a	O
comparison	O
of	O
locally	O
connected	O
layers	O
,	O
tiled	O
convolution	O
,	O
and	O
standard	O
convolution	O
.	O
every	O
9.16	O
352	O
chapter	O
9.	O
convolutional	O
networks	O
s1s1	O
s2s2	O
s3s3	O
s4s4	O
s5s5	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
x1x1	O
s1s1	O
x2x2	O
s2s2	O
x3x3	O
s3s3	O
x4x4	O
s4s4	O
x5x5	O
s5s5	O
a	O
b	O
a	O
b	O
a	O
b	O
a	O
b	O
a	O
x1x1	O
s1s1	O
x1x1	O
x2x2	O
s2s2	O
x2x2	O
x3x3	O
s3s3	O
x3x3	O
x4x4	O
s4s4	O
x4x4	O
x5x5	O
s5s5	O
x5x5	O
figure	O
9.14	O
:	O
comparison	O
of	O
local	O
connections	O
,	O
convolution	O
,	O
and	O
full	O
connections	O
.	O
(	O
top	O
)	O
a	O
locally	O
connected	O
layer	O
with	O
a	O
patch	O
size	O
of	O
two	O
pixels	O
.	O
each	O
edge	O
is	O
labeled	O
with	O
a	O
unique	O
letter	O
to	O
show	O
that	O
each	O
edge	O
is	O
associated	O
with	O
its	O
own	O
weight	O
parameter	O
.	O
(	O
center	O
)	O
a	O
convolutional	O
layer	O
with	O
a	O
kernel	O
width	O
of	O
two	O
pixels	O
.	O
this	O
model	B
has	O
exactly	O
the	O
same	O
connectivity	O
as	O
the	O
locally	O
connected	O
layer	O
.	O
the	O
diﬀerence	O
lies	O
not	O
in	O
which	O
units	O
interact	O
with	O
each	O
other	O
,	O
but	O
in	O
how	O
the	O
parameters	O
are	O
shared	O
.	O
the	O
locally	O
connected	O
layer	O
has	O
no	O
parameter	O
sharing	O
.	O
the	O
convolutional	O
layer	O
uses	O
the	O
same	O
two	O
weights	O
repeatedly	O
across	O
the	O
entire	O
input	O
,	O
as	O
indicated	O
by	O
the	O
repetition	O
of	O
the	O
letters	O
labeling	O
each	O
edge	O
.	O
(	O
bottom	O
)	O
a	O
fully	O
connected	O
layer	O
resembles	O
a	O
locally	O
connected	O
layer	O
in	O
the	O
sense	O
that	O
each	O
edge	O
has	O
its	O
own	O
parameter	O
(	O
there	O
are	O
too	O
many	O
to	O
label	O
explicitly	O
with	O
letters	O
in	O
this	O
diagram	O
)	O
.	O
however	O
,	O
it	O
does	O
not	O
have	O
the	O
restricted	O
connectivity	O
of	O
the	O
locally	O
connected	O
layer	O
.	O
353	O
chapter	O
9.	O
convolutional	O
networks	O
output	O
tensor	O
input	O
tensor	O
s	O
e	O
t	O
a	O
n	O
i	O
d	O
r	O
o	O
o	O
c	O
l	O
e	O
n	O
n	O
a	O
h	O
c	O
spatial	O
coordinates	O
figure	O
9.15	O
:	O
a	O
convolutional	O
network	O
with	O
the	O
ﬁrst	O
two	O
output	O
channels	O
connected	O
to	O
only	O
the	O
ﬁrst	O
two	O
input	O
channels	O
,	O
and	O
the	O
second	O
two	O
output	O
channels	O
connected	O
to	O
only	O
the	O
second	O
two	O
input	O
channels	O
.	O
354	O
chapter	O
9.	O
convolutional	O
networks	O
s1s1	O
s2s2	O
s3s3	O
s4s4	O
s5s5	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
x1x1	O
s1s1	O
x2x2	O
s2s2	O
x3x3	O
s3s3	O
x4x4	O
s4s4	O
x5x5	O
s5s5	O
a	O
b	O
c	O
d	O
a	O
b	O
c	O
d	O
a	O
x1x1	O
s1s1	O
x2x2	O
s2s2	O
x3x3	O
s3s3	O
x4x4	O
s4s4	O
x5x5	O
s5s5	O
a	O
b	O
a	O
b	O
a	O
b	O
a	O
b	O
a	O
x1x1	O
x2x2	O
x3x3	O
x4x4	O
x5x5	O
figure	O
9.16	O
:	O
a	O
comparison	O
of	O
locally	O
connected	O
layers	O
,	O
tiled	O
convolution	O
,	O
and	O
standard	O
convolution	O
.	O
all	O
three	O
have	O
the	O
same	O
sets	O
of	O
connections	O
between	O
units	O
,	O
when	O
the	O
same	O
size	O
of	O
kernel	O
is	O
used	O
.	O
this	O
diagram	O
illustrates	O
the	O
use	O
of	O
a	O
kernel	O
that	O
is	O
two	O
pixels	O
wide	O
.	O
the	O
diﬀerences	O
between	O
the	O
methods	O
lies	O
in	O
how	O
they	O
share	O
parameters	O
.	O
(	O
top	O
)	O
a	O
locally	O
connected	O
layer	O
has	O
no	O
sharing	O
at	O
all	O
.	O
we	O
indicate	O
that	O
each	O
connection	O
has	O
its	O
own	O
weight	O
by	O
labeling	O
each	O
connection	O
with	O
a	O
unique	O
letter	O
.	O
tiled	O
convolution	O
has	O
a	O
set	O
of	O
t	O
diﬀerent	O
kernels	O
.	O
here	O
we	O
illustrate	O
the	O
case	O
of	O
t	O
=	O
2.	O
one	O
of	O
these	O
kernels	O
has	O
edges	O
labeled	O
“	O
a	O
”	O
and	O
“	O
b	O
,	O
”	O
while	O
the	O
other	O
has	O
edges	O
labeled	O
“	O
c	O
”	O
and	O
“	O
d.	O
”	O
each	O
time	O
we	O
move	O
one	O
pixel	O
to	O
the	O
right	O
in	O
the	O
output	O
,	O
we	O
move	O
on	O
to	O
using	O
a	O
diﬀerent	O
kernel	O
.	O
this	O
means	O
that	O
,	O
like	O
the	O
locally	O
connected	O
layer	O
,	O
neighboring	O
units	O
in	O
the	O
output	O
have	O
diﬀerent	O
parameters	O
.	O
unlike	O
the	O
locally	O
connected	O
layer	O
,	O
after	O
we	O
have	O
gone	O
through	O
all	O
t	O
available	O
kernels	O
,	O
we	O
cycle	O
back	O
to	O
the	O
ﬁrst	O
kernel	O
.	O
if	O
two	O
output	O
units	O
are	O
separated	O
by	O
a	O
multiple	O
of	O
t	O
steps	O
,	O
then	O
they	O
share	O
parameters	O
.	O
traditional	O
convolution	O
is	O
equivalent	O
to	O
tiled	O
convolution	O
with	O
t	O
=	O
1.	O
there	O
is	O
only	O
one	O
kernel	O
and	O
it	O
is	O
applied	O
everywhere	O
,	O
as	O
indicated	O
in	O
the	O
diagram	O
by	O
using	O
the	O
kernel	O
with	O
weights	O
labeled	O
“	O
a	O
”	O
and	O
“	O
b	O
”	O
everywhere	O
.	O
(	O
bottom	O
)	O
(	O
center	O
)	O
355	O
chapter	O
9.	O
convolutional	O
networks	O
to	O
deﬁne	O
tiled	O
convolution	O
algebraically	O
,	O
let	O
k	O
be	O
a	O
6-d	O
tensor	O
,	O
where	O
two	O
of	O
the	O
dimensions	O
correspond	O
to	O
diﬀerent	O
locations	O
in	O
the	O
output	O
map	O
.	O
rather	O
than	O
having	O
a	O
separate	O
index	O
for	O
each	O
location	O
in	O
the	O
output	O
map	O
,	O
output	O
locations	O
cycle	O
through	O
a	O
set	O
of	O
t	O
diﬀerent	O
choices	O
of	O
kernel	O
stack	O
in	O
each	O
direction	O
.	O
if	O
t	O
is	O
equal	O
to	O
the	O
output	O
width	O
,	O
this	O
is	O
the	O
same	O
as	O
a	O
locally	O
connected	O
layer	O
.	O
	O
zi	O
,	O
j	O
,	O
k	O
=	O
l	O
,	O
m	O
,	O
n	O
−	O
1ki	O
,	O
l	O
,	O
m	O
,	O
n	O
,	O
j	O
vl	O
,	O
j	O
m	O
,	O
k	O
n	O
−	O
1	O
+	O
+	O
%	O
+1	O
%	O
+1	O
,	O
,	O
k	O
t	O
t	O
(	O
9.10	O
)	O
%	O
is	O
the	O
modulo	O
operation	O
,	O
with	O
where	O
it	O
is	O
straightforward	O
to	O
generalize	O
this	O
equation	O
to	O
use	O
a	O
diﬀerent	O
tiling	O
range	O
for	O
each	O
dimension	O
.	O
t	O
+	O
1	O
)	O
%	O
t	O
=	O
1	O
,	O
etc	O
.	O
t	O
%	O
t	O
=	O
0	O
(	O
,	O
both	O
locally	O
connected	O
layers	O
and	O
tiled	O
convolutional	O
layers	O
have	O
an	O
interesting	O
interaction	O
with	O
max-pooling	O
:	O
the	O
detector	O
units	O
of	O
these	O
layers	O
are	O
driven	O
by	O
diﬀerent	O
ﬁlters	O
.	O
if	O
these	O
ﬁlters	O
learn	O
to	O
detect	O
diﬀerent	O
transformed	O
versions	O
of	O
the	O
same	O
underlying	O
features	O
,	O
then	O
the	O
max-pooled	O
units	O
become	O
invariant	O
to	O
the	O
learned	O
transformation	O
(	O
see	O
ﬁgure	O
)	O
.	O
convolutional	O
layers	O
are	O
hard-coded	O
to	O
be	O
invariant	O
speciﬁcally	O
to	O
translation	O
.	O
9.9	O
other	O
operations	O
besides	O
convolution	O
are	O
usually	O
necessary	O
to	O
implement	O
a	O
convolutional	O
network	O
.	O
to	O
perform	O
learning	O
,	O
one	O
must	O
be	O
able	O
to	O
compute	O
the	O
gradient	O
with	O
respect	O
to	O
the	O
kernel	O
,	O
given	O
the	O
gradient	O
with	O
respect	O
to	O
the	O
outputs	O
.	O
in	O
some	O
simple	O
cases	O
,	O
this	O
operation	O
can	O
be	O
performed	O
using	O
the	O
convolution	O
operation	O
,	O
but	O
many	O
cases	O
of	O
interest	O
,	O
including	O
the	O
case	O
of	O
stride	O
greater	O
than	O
1	O
,	O
do	O
not	O
have	O
this	O
property	O
.	O
recall	O
that	O
convolution	O
is	O
a	O
linear	O
operation	O
and	O
can	O
thus	O
be	O
described	O
as	O
a	O
matrix	O
multiplication	O
(	O
if	O
we	O
ﬁrst	O
reshape	O
the	O
input	O
tensor	O
into	O
a	O
ﬂat	O
vector	O
)	O
.	O
the	O
matrix	O
involved	O
is	O
a	O
function	O
of	O
the	O
convolution	O
kernel	O
.	O
the	O
matrix	O
is	O
sparse	O
and	O
each	O
element	O
of	O
the	O
kernel	O
is	O
copied	O
to	O
several	O
elements	O
of	O
the	O
matrix	O
.	O
this	O
view	O
helps	O
us	O
to	O
derive	O
some	O
of	O
the	O
other	O
operations	O
needed	O
to	O
implement	O
a	O
convolutional	O
network	O
.	O
multiplication	O
by	O
the	O
transpose	O
of	O
the	O
matrix	O
deﬁned	O
by	O
convolution	O
is	O
one	O
such	O
operation	O
.	O
this	O
is	O
the	O
operation	O
needed	O
to	O
back-propagate	O
error	O
derivatives	O
through	O
a	O
convolutional	O
layer	O
,	O
so	O
it	O
is	O
needed	O
to	O
train	O
convolutional	O
networks	O
that	O
have	O
more	O
than	O
one	O
hidden	O
layer	O
.	O
this	O
same	O
operation	O
is	O
also	O
needed	O
if	O
we	O
wish	O
to	O
reconstruct	O
the	O
visible	O
units	O
from	O
the	O
hidden	O
units	O
(	O
)	O
.	O
reconstructing	O
the	O
visible	O
units	O
is	O
an	O
operation	O
commonly	O
used	O
in	O
the	O
models	O
described	O
in	O
part	O
of	O
this	O
book	O
,	O
such	O
as	O
autoencoders	O
,	O
rbms	O
,	O
and	O
sparse	O
coding	O
.	O
transpose	O
convolution	O
is	O
necessary	O
to	O
construct	O
convolutional	O
versions	O
of	O
those	O
models	O
.	O
like	O
the	O
kernel	O
gradient	O
operation	O
,	O
this	O
input	O
gradient	O
operation	O
can	O
be	O
simard	O
et	O
al	O
.	O
1992	O
iii	O
,	O
356	O
chapter	O
9.	O
convolutional	O
networks	O
implemented	O
using	O
a	O
convolution	O
in	O
some	O
cases	O
,	O
but	O
in	O
the	O
general	O
case	O
requires	O
a	O
third	O
operation	O
to	O
be	O
implemented	O
.	O
care	O
must	O
be	O
taken	O
to	O
coordinate	O
this	O
transpose	O
operation	O
with	O
the	O
forward	O
propagation	O
.	O
the	O
size	O
of	O
the	O
output	O
that	O
the	O
transpose	O
operation	O
should	O
return	O
depends	O
on	O
the	O
zero	O
padding	O
policy	O
and	O
stride	O
of	O
the	O
forward	O
propagation	O
operation	O
,	O
as	O
well	O
as	O
the	O
size	O
of	O
the	O
forward	O
propagation	O
’	O
s	O
output	O
map	O
.	O
in	O
some	O
cases	O
,	O
multiple	O
sizes	O
of	O
input	O
to	O
forward	O
propagation	O
can	O
result	O
in	O
the	O
same	O
size	O
of	O
output	O
map	O
,	O
so	O
the	O
transpose	O
operation	O
must	O
be	O
explicitly	O
told	O
what	O
the	O
size	O
of	O
the	O
original	O
input	O
was	O
.	O
these	O
three	O
operations—convolution	O
,	O
backprop	O
from	O
output	O
to	O
weights	O
,	O
and	O
backprop	O
from	O
output	O
to	O
inputs—are	O
suﬃcient	O
to	O
compute	O
all	O
of	O
the	O
gradients	O
needed	O
to	O
train	O
any	O
depth	O
of	O
feedforward	O
convolutional	O
network	O
,	O
as	O
well	O
as	O
to	O
train	O
convolutional	O
networks	O
with	O
reconstruction	O
functions	O
based	O
on	O
the	O
transpose	O
of	O
convolution	O
.	O
see	O
)	O
for	O
a	O
full	O
derivation	O
of	O
the	O
equations	O
in	O
the	O
fully	O
general	O
multi-dimensional	O
,	O
multi-example	O
case	O
.	O
to	O
give	O
a	O
sense	O
of	O
how	O
these	O
equations	O
work	B
,	O
we	O
present	O
the	O
two	O
dimensional	O
,	O
single	O
example	O
version	O
here	O
.	O
goodfellow	O
2010	O
(	O
suppose	O
we	O
want	O
to	O
train	O
a	O
convolutional	O
network	O
that	O
incorporates	O
strided	O
convolution	O
of	O
kernel	O
stack	O
k	O
applied	O
to	O
multi-channel	O
image	O
v	O
with	O
stride	O
s	O
as	O
,	O
s	O
)	O
as	O
in	O
equation	O
deﬁned	O
by	O
c	O
(	O
k	O
v	O
,	O
.	O
suppose	O
we	O
want	O
to	O
minimize	O
some	O
loss	O
)	O
.	O
during	O
forward	O
propagation	O
,	O
we	O
will	O
need	O
to	O
use	O
c	O
itself	O
to	O
function	O
j	O
(	O
v	O
k	O
,	O
output	O
z	O
,	O
which	O
is	O
then	O
propagated	O
through	O
the	O
rest	O
of	O
the	O
network	O
and	O
used	O
to	O
compute	O
the	O
cost	O
function	O
j	O
.	O
during	O
back-propagation	O
,	O
we	O
will	O
receive	O
a	O
tensor	O
g	O
such	O
that	O
gi	O
,	O
j	O
,	O
k	O
=	O
∂	O
.	O
(	O
v	O
k	O
)	O
9.8	O
j	O
,	O
∂zi	O
,	O
j	O
,	O
k	O
to	O
train	O
the	O
network	O
,	O
we	O
need	O
to	O
compute	O
the	O
derivatives	O
with	O
respect	O
to	O
the	O
weights	O
in	O
the	O
kernel	O
.	O
to	O
do	O
so	O
,	O
we	O
can	O
use	O
a	O
function	O
g	O
(	O
g	O
v	O
)	O
i	O
,	O
j	O
,	O
k	O
,	O
l	O
=	O
,	O
s	O
,	O
∂	O
∂ki	O
,	O
j	O
,	O
k	O
,	O
l	O
j	O
,	O
(	O
v	O
k	O
)	O
=	O
m	O
,	O
n	O
gi	O
,	O
m	O
,	O
nvj	O
,	O
m	O
(	O
−	O
×	O
1	O
)	O
+	O
(	O
s	O
k	O
,	O
n	O
−	O
×	O
1	O
)	O
+	O
.	O
s	O
l	O
(	O
9.11	O
)	O
	O
if	O
this	O
layer	O
is	O
not	O
the	O
bottom	O
layer	O
of	O
the	O
network	O
,	O
we	O
will	O
need	O
to	O
compute	O
the	O
gradient	O
with	O
respect	O
to	O
v	O
in	O
order	O
to	O
back-propagate	O
the	O
error	O
farther	O
down	O
.	O
to	O
do	O
so	O
,	O
we	O
can	O
use	O
a	O
function	O
	O
	O
	O
∂	O
∂vi	O
,	O
j	O
,	O
k	O
h	O
,	O
(	O
k	O
g	O
)	O
i	O
,	O
j	O
,	O
k	O
=	O
,	O
s	O
=	O
j	O
,	O
(	O
v	O
k	O
)	O
kq	O
,	O
i	O
,	O
m	O
,	O
pgq	O
,	O
l	O
,	O
n	O
.	O
(	O
9.12	O
)	O
(	O
9.13	O
)	O
l	O
,	O
m	O
−	O
×	O
s.t	O
.	O
1	O
)	O
+	O
=	O
s	O
m	O
j	O
l	O
(	O
n	O
,	O
p	O
−	O
×	O
s.t	O
.	O
1	O
)	O
+	O
=	O
n	O
(	O
s	O
p	O
k	O
q	O
autoencoder	O
networks	O
,	O
described	O
in	O
chapter	O
,	O
are	O
feedforward	O
networks	O
trained	O
to	O
copy	O
their	O
input	O
to	O
their	O
output	O
.	O
a	O
simple	O
example	O
is	O
the	O
pca	O
algorithm	O
,	O
14	O
357	O
chapter	O
9.	O
convolutional	O
networks	O
	O
w	O
x.	O
that	O
copies	O
its	O
input	O
x	O
to	O
an	O
approximate	O
reconstruction	O
r	O
using	O
the	O
function	O
w	O
it	O
is	O
common	O
for	O
more	O
general	O
autoencoders	O
to	O
use	O
multiplication	O
by	O
the	O
transpose	O
of	O
the	O
weight	O
matrix	O
just	O
as	O
pca	O
does	O
.	O
to	O
make	O
such	O
models	O
convolutional	O
,	O
we	O
can	O
use	O
the	O
function	O
h	O
to	O
perform	O
the	O
transpose	O
of	O
the	O
convolution	O
operation	O
.	O
suppose	O
we	O
have	O
hidden	O
units	O
h	O
in	O
the	O
same	O
format	O
as	O
z	O
and	O
we	O
deﬁne	O
a	O
reconstruction	O
=	O
(	O
h	O
,	O
k	O
h	O
,	O
s	O
.	O
)	O
r	O
(	O
9.14	O
)	O
in	O
order	O
to	O
train	O
the	O
autoencoder	O
,	O
we	O
will	O
receive	O
the	O
gradient	O
with	O
respect	O
to	O
r	O
as	O
a	O
tensor	O
e.	O
to	O
train	O
the	O
decoder	O
,	O
we	O
need	O
to	O
obtain	O
the	O
gradient	O
with	O
respect	O
to	O
k.	O
this	O
is	O
given	O
by	O
g	O
(	O
h	O
e	O
,	O
,	O
s	O
)	O
.	O
to	O
train	O
the	O
encoder	O
,	O
we	O
need	O
to	O
obtain	O
the	O
gradient	O
with	O
respect	O
to	O
h.	O
this	O
is	O
given	O
by	O
c	O
(	O
k	O
e	O
,	O
,	O
s	O
)	O
.	O
it	O
is	O
also	O
possible	O
to	O
diﬀerentiate	O
through	O
g	O
using	O
c	O
and	O
h	O
,	O
but	O
these	O
operations	O
are	O
not	O
needed	O
for	O
the	O
back-propagation	O
algorithm	O
on	O
any	O
standard	O
network	O
architectures	O
.	O
generally	O
,	O
we	O
do	O
not	O
use	O
only	O
a	O
linear	O
operation	O
in	O
order	O
to	O
transform	O
from	O
the	O
inputs	O
to	O
the	O
outputs	O
in	O
a	O
convolutional	O
layer	O
.	O
we	O
generally	O
also	O
add	O
some	O
bias	O
term	O
to	O
each	O
output	O
before	O
applying	O
the	O
nonlinearity	O
.	O
this	O
raises	O
the	O
question	O
of	O
how	O
to	O
share	O
parameters	O
among	O
the	O
biases	O
.	O
for	O
locally	O
connected	O
layers	O
it	O
is	O
natural	O
to	O
give	O
each	O
unit	O
its	O
own	O
bias	O
,	O
and	O
for	O
tiled	O
convolution	O
,	O
it	O
is	O
natural	O
to	O
share	O
the	O
biases	O
with	O
the	O
same	O
tiling	O
pattern	O
as	O
the	O
kernels	O
.	O
for	O
convolutional	O
layers	O
,	O
it	O
is	O
typical	O
to	O
have	O
one	O
bias	O
per	O
channel	O
of	O
the	O
output	O
and	O
share	O
it	O
across	O
all	O
locations	O
within	O
each	O
convolution	O
map	O
.	O
however	O
,	O
if	O
the	O
input	O
is	O
of	O
known	O
,	O
ﬁxed	O
size	O
,	O
it	O
is	O
also	O
possible	O
to	O
learn	O
a	O
separate	O
bias	O
at	O
each	O
location	O
of	O
the	O
output	O
map	O
.	O
separating	O
the	O
biases	O
may	O
slightly	O
reduce	O
the	O
statistical	O
eﬃciency	O
of	O
the	O
model	B
,	O
but	O
also	O
allows	O
the	O
model	B
to	O
correct	O
for	O
diﬀerences	O
in	O
the	O
image	O
statistics	O
at	O
diﬀerent	O
locations	O
.	O
for	O
example	O
,	O
when	O
using	O
implicit	O
zero	O
padding	O
,	O
detector	O
units	O
at	O
the	O
edge	O
of	O
the	O
image	O
receive	O
less	O
total	O
input	O
and	O
may	O
need	O
larger	O
biases	O
.	O
9.6	O
structured	O
outputs	O
convolutional	O
networks	O
can	O
be	O
used	O
to	O
output	O
a	O
high-dimensional	O
,	O
structured	O
object	O
,	O
rather	O
than	O
just	O
predicting	O
a	O
class	O
label	O
for	O
a	O
classiﬁcation	O
task	O
or	O
a	O
real	O
value	O
for	O
a	O
regression	O
task	O
.	O
typically	O
this	O
object	O
is	O
just	O
a	O
tensor	O
,	O
emitted	O
by	O
a	O
standard	O
convolutional	O
layer	O
.	O
for	O
example	O
,	O
the	O
model	B
might	O
emit	O
a	O
tensor	O
s	O
,	O
where	O
si	O
,	O
j	O
,	O
k	O
is	O
the	O
probability	O
that	O
pixel	O
(	O
j	O
,	O
k	O
)	O
of	O
the	O
input	O
to	O
the	O
network	O
belongs	O
to	O
class	O
i.	O
this	O
allows	O
the	O
model	B
to	O
label	O
every	O
pixel	O
in	O
an	O
image	O
and	O
draw	O
precise	O
masks	O
that	O
follow	O
the	O
outlines	O
of	O
individual	O
objects	O
.	O
one	O
issue	O
that	O
often	O
comes	O
up	O
is	O
that	O
the	O
output	O
plane	O
can	O
be	O
smaller	O
than	O
the	O
358	O
chapter	O
9.	O
convolutional	O
networks	O
ˆy	O
(	O
1	O
)	O
ˆy	O
(	O
1	O
)	O
ˆy	O
(	O
2	O
)	O
ˆy	O
(	O
2	O
)	O
ˆy	O
(	O
3	O
)	O
ˆy	O
(	O
3	O
)	O
v	O
w	O
v	O
w	O
v	O
h	O
(	O
1	O
)	O
h	O
(	O
1	O
)	O
h	O
(	O
2	O
)	O
h	O
(	O
2	O
)	O
h	O
(	O
3	O
)	O
h	O
(	O
3	O
)	O
u	O
u	O
u	O
xx	O
x	O
figure	O
9.17	O
:	O
an	O
example	O
of	O
a	O
recurrent	O
convolutional	O
network	O
for	O
pixel	O
labeling	O
.	O
the	O
input	O
is	O
an	O
image	O
tensor	O
,	O
with	O
axes	O
corresponding	O
to	O
image	O
rows	O
,	O
image	O
columns	O
,	O
and	O
channels	O
(	O
red	O
,	O
green	O
,	O
blue	O
)	O
.	O
the	O
goal	O
is	O
to	O
output	O
a	O
tensor	O
of	O
labels	O
ˆy	O
,	O
with	O
a	O
probability	O
distribution	O
over	O
labels	O
for	O
each	O
pixel	O
.	O
this	O
tensor	O
has	O
axes	O
corresponding	O
to	O
image	O
rows	O
,	O
image	O
columns	O
,	O
and	O
the	O
diﬀerent	O
classes	O
.	O
rather	O
than	O
outputting	O
ˆy	O
in	O
a	O
single	O
shot	O
,	O
the	O
recurrent	O
network	O
iteratively	O
reﬁnes	O
its	O
estimate	O
ˆy	O
by	O
using	O
a	O
previous	O
estimate	O
of	O
ˆy	O
as	O
input	O
for	O
creating	O
a	O
new	O
estimate	O
.	O
the	O
same	O
parameters	O
are	O
used	O
for	O
each	O
updated	O
estimate	O
,	O
and	O
the	O
estimate	O
can	O
be	O
reﬁned	O
as	O
many	O
times	O
as	O
we	O
wish	O
.	O
the	O
tensor	O
of	O
convolution	O
kernels	O
u	O
is	O
used	O
on	O
each	O
step	O
to	O
compute	O
the	O
hidden	O
representation	O
given	O
the	O
input	O
image	O
.	O
the	O
kernel	O
tensor	O
v	O
is	O
used	O
to	O
produce	O
an	O
estimate	O
of	O
the	O
labels	O
given	O
the	O
hidden	O
values	O
.	O
on	O
all	O
but	O
the	O
ﬁrst	O
step	O
,	O
the	O
kernels	O
w	O
are	O
convolved	O
over	O
ˆy	O
to	O
provide	O
input	O
to	O
the	O
hidden	O
layer	O
.	O
on	O
the	O
ﬁrst	O
time	O
step	O
,	O
this	O
term	O
is	O
replaced	O
by	O
zero	O
.	O
because	O
the	O
same	O
parameters	O
are	O
used	O
on	O
each	O
step	O
,	O
this	O
is	O
an	O
example	O
of	O
a	O
recurrent	O
network	O
,	O
as	O
described	O
in	O
chapter	O
.10	O
9.13	O
input	O
plane	O
,	O
as	O
shown	O
in	O
ﬁgure	O
.	O
in	O
the	O
kinds	O
of	O
architectures	O
typically	O
used	O
for	O
classiﬁcation	O
of	O
a	O
single	O
object	O
in	O
an	O
image	O
,	O
the	O
greatest	O
reduction	O
in	O
the	O
spatial	O
dimensions	O
of	O
the	O
network	O
comes	O
from	O
using	O
pooling	O
layers	O
with	O
large	O
stride	O
.	O
in	O
order	O
to	O
produce	O
an	O
output	O
map	O
of	O
similar	O
size	O
as	O
the	O
input	O
,	O
one	O
can	O
avoid	O
pooling	O
)	O
.	O
another	O
strategy	O
is	O
to	O
simply	O
emit	O
a	O
lower-resolution	O
altogether	O
(	O
grid	O
of	O
labels	O
(	O
)	O
.	O
finally	O
,	O
in	O
principle	O
,	O
one	O
could	O
use	O
a	O
pooling	O
operator	O
with	O
unit	O
stride	O
.	O
pinheiro	O
and	O
collobert	O
2014	O
2015	O
jain	O
et	O
al	O
.	O
2007	O
,	O
,	O
,	O
one	O
strategy	O
for	O
pixel-wise	O
labeling	O
of	O
images	O
is	O
to	O
produce	O
an	O
initial	O
guess	O
of	O
the	O
image	O
labels	O
,	O
then	O
reﬁne	O
this	O
initial	O
guess	O
using	O
the	O
interactions	O
between	O
neighboring	O
pixels	O
.	O
repeating	O
this	O
reﬁnement	O
step	O
several	O
times	O
corresponds	O
to	O
using	O
the	O
same	O
convolutions	O
at	O
each	O
stage	O
,	O
sharing	O
weights	O
between	O
the	O
last	O
layers	O
of	O
the	O
deep	O
net	O
(	O
)	O
.	O
this	O
makes	O
the	O
sequence	O
of	O
computations	O
performed	O
by	O
the	O
successive	O
convolutional	O
layers	O
with	O
weights	O
shared	O
across	O
layers	O
a	O
particular	O
kind	O
of	O
recurrent	O
network	O
(	O
shows	O
the	O
architecture	O
of	O
such	O
a	O
recurrent	O
convolutional	O
network	O
.	O
pinheiro	O
and	O
collobert	O
2014	O
2015	O
jain	O
et	O
al	O
.	O
2007	O
)	O
.	O
figure	O
9.17	O
,	O
,	O
,	O
359	O
chapter	O
9.	O
convolutional	O
networks	O
briggman	O
et	O
al	O
.	O
2009	O
turaga	O
,	O
;	O
once	O
a	O
prediction	O
for	O
each	O
pixel	O
is	O
made	O
,	O
various	O
methods	O
can	O
be	O
used	O
to	O
further	O
process	O
these	O
predictions	O
in	O
order	O
to	O
obtain	O
a	O
segmentation	O
of	O
the	O
image	O
into	O
regions	O
(	O
2013	O
)	O
.	O
the	O
general	O
idea	O
is	O
to	O
assume	O
that	O
large	O
groups	O
of	O
contiguous	O
pixels	O
tend	O
to	O
be	O
associated	O
with	O
the	O
same	O
label	O
.	O
graphical	O
models	O
can	O
describe	O
the	O
probabilistic	O
relationships	O
between	O
neighboring	O
pixels	O
.	O
alternatively	O
,	O
the	O
convolutional	O
network	O
can	O
be	O
trained	O
to	O
maximize	O
an	O
approximation	O
of	O
the	O
graphical	O
model	B
training	O
objective	O
(	O
ning	O
et	O
al	O
.	O
2005	O
thompson	O
et	O
al	O
.	O
2014	O
2010	O
farabet	O
et	O
al.	O
,	O
,	O
;	O
,	O
)	O
.	O
;	O
et	O
al.	O
,	O
9.7	O
data	O
types	O
the	O
data	O
used	O
with	O
a	O
convolutional	O
network	O
usually	O
consists	O
of	O
several	O
channels	O
,	O
each	O
channel	O
being	O
the	O
observation	O
of	O
a	O
diﬀerent	O
quantity	O
at	O
some	O
point	O
in	O
space	O
or	O
time	O
.	O
see	O
table	O
for	O
examples	O
of	O
data	O
types	O
with	O
diﬀerent	O
dimensionalities	O
and	O
number	O
of	O
channels	O
.	O
9.1	O
for	O
an	O
example	O
of	O
convolutional	O
networks	O
applied	O
to	O
video	O
,	O
see	O
chen	O
et	O
al	O
.	O
(	O
2010	O
)	O
.	O
so	O
far	O
we	O
have	O
discussed	O
only	O
the	O
case	O
where	O
every	O
example	O
in	O
the	O
train	O
and	O
test	O
data	O
has	O
the	O
same	O
spatial	O
dimensions	O
.	O
one	O
advantage	O
to	O
convolutional	O
networks	O
is	O
that	O
they	O
can	O
also	O
process	O
inputs	O
with	O
varying	O
spatial	O
extents	O
.	O
these	O
kinds	O
of	O
input	O
simply	O
can	O
not	O
be	O
represented	O
by	O
traditional	O
,	O
matrix	O
multiplication-based	O
neural	O
networks	O
.	O
this	O
provides	O
a	O
compelling	O
reason	O
to	O
use	O
convolutional	O
networks	O
even	O
when	O
computational	O
cost	O
and	O
overﬁtting	O
are	O
not	O
signiﬁcant	O
issues	O
.	O
for	O
example	O
,	O
consider	O
a	O
collection	O
of	O
images	O
,	O
where	O
each	O
image	O
has	O
a	O
diﬀerent	O
width	O
and	O
height	O
.	O
it	O
is	O
unclear	O
how	O
to	O
model	B
such	O
inputs	O
with	O
a	O
weight	O
matrix	O
of	O
ﬁxed	O
size	O
.	O
convolution	O
is	O
straightforward	O
to	O
apply	O
;	O
the	O
kernel	O
is	O
simply	O
applied	O
a	O
diﬀerent	O
number	O
of	O
times	O
depending	O
on	O
the	O
size	O
of	O
the	O
input	O
,	O
and	O
the	O
output	O
of	O
the	O
convolution	O
operation	O
scales	O
accordingly	O
.	O
convolution	O
may	O
be	O
viewed	O
as	O
matrix	O
multiplication	O
;	O
the	O
same	O
convolution	O
kernel	O
induces	O
a	O
diﬀerent	O
size	O
of	O
doubly	O
block	O
circulant	O
matrix	O
for	O
each	O
size	O
of	O
input	O
.	O
sometimes	O
the	O
output	O
of	O
the	O
network	O
is	O
allowed	O
to	O
have	O
variable	O
size	O
as	O
well	O
as	O
the	O
input	O
,	O
for	O
example	O
if	O
we	O
want	O
to	O
assign	O
a	O
class	O
label	O
to	O
each	O
pixel	O
of	O
the	O
input	O
.	O
in	O
this	O
case	O
,	O
no	O
further	O
design	O
work	B
is	O
necessary	O
.	O
in	O
other	O
cases	O
,	O
the	O
network	O
must	O
produce	O
some	O
ﬁxed-size	O
output	O
,	O
for	O
example	O
if	O
we	O
want	O
to	O
assign	O
a	O
single	O
class	O
label	O
to	O
the	O
entire	O
image	O
.	O
in	O
this	O
case	O
we	O
must	O
make	O
some	O
additional	O
design	O
steps	O
,	O
like	O
inserting	O
a	O
pooling	O
layer	O
whose	O
pooling	O
regions	O
scale	O
in	O
size	O
proportional	O
to	O
the	O
size	O
of	O
the	O
input	O
,	O
in	O
order	O
to	O
maintain	O
a	O
ﬁxed	O
number	O
of	O
pooled	O
outputs	O
.	O
some	O
examples	O
of	O
this	O
kind	O
of	O
strategy	O
are	O
shown	O
in	O
ﬁgure	O
9.11	O
.	O
360	O
chapter	O
9.	O
convolutional	O
networks	O
single	O
channel	O
1-d	O
audio	O
waveform	O
:	O
the	O
axis	O
we	O
convolve	O
over	O
corresponds	O
to	O
time	O
.	O
we	O
discretize	O
time	O
and	O
measure	O
the	O
amplitude	O
of	O
the	O
waveform	O
once	O
per	O
time	O
step	O
.	O
2-d	O
audio	O
data	O
that	O
has	O
been	O
prepro-	O
cessed	O
with	O
a	O
fourier	O
transform	O
:	O
we	O
can	O
transform	O
the	O
audio	O
wave-	O
form	O
into	O
a	O
2d	O
tensor	O
with	O
dif-	O
ferent	O
rows	O
corresponding	O
to	O
dif-	O
ferent	O
frequencies	O
and	O
diﬀerent	O
columns	O
corresponding	O
to	O
diﬀer-	O
ent	O
points	O
in	O
time	O
.	O
using	O
convolu-	O
tion	B
in	O
the	O
time	O
makes	O
the	O
model	B
equivariant	O
to	O
shifts	O
in	O
time	O
.	O
us-	O
ing	O
convolution	O
across	O
the	O
fre-	O
quency	O
axis	O
makes	O
the	O
model	B
equivariant	O
to	O
frequency	O
,	O
so	O
that	O
the	O
same	O
melody	O
played	O
in	O
a	O
dif-	O
ferent	O
octave	O
produces	O
the	O
same	O
representation	O
but	O
at	O
a	O
diﬀerent	O
height	O
in	O
the	O
network	O
’	O
s	O
output	O
.	O
3-d	O
volumetric	O
data	O
:	O
a	O
common	O
source	O
of	O
this	O
kind	O
of	O
data	O
is	O
med-	O
ical	O
imaging	O
technology	O
,	O
such	O
as	O
ct	O
scans	O
.	O
multi-channel	O
skeleton	O
animation	O
data	O
:	O
anima-	O
tions	O
of	O
3-d	O
computer-rendered	O
characters	O
are	O
generated	O
by	O
alter-	O
ing	O
the	O
pose	O
of	O
a	O
“	O
skeleton	O
”	O
over	O
time	O
.	O
at	O
each	O
point	O
in	O
time	O
,	O
the	O
pose	O
of	O
the	O
character	O
is	O
described	O
by	O
a	O
speciﬁcation	O
of	O
the	O
angles	O
of	O
each	O
of	O
the	O
joints	O
in	O
the	O
charac-	O
ter	O
’	O
s	O
skeleton	O
.	O
each	O
channel	O
in	O
the	O
data	O
we	O
feed	O
to	O
the	O
convolu-	O
tional	O
model	B
represents	O
the	O
angle	O
about	O
one	O
axis	O
of	O
one	O
joint	O
.	O
color	O
image	O
data	O
:	O
one	O
channel	O
contains	O
the	O
red	O
pixels	O
,	O
one	O
the	O
green	O
pixels	O
,	O
and	O
one	O
the	O
blue	O
pixels	O
.	O
the	O
convolution	O
kernel	O
moves	O
over	O
both	O
the	O
horizontal	O
and	O
vertical	O
axes	O
of	O
the	O
image	O
,	O
conferring	O
translation	O
equivari-	O
ance	O
in	O
both	O
directions	O
.	O
color	O
video	O
data	O
:	O
one	O
axis	O
corre-	O
sponds	O
to	O
time	O
,	O
one	O
to	O
the	O
height	O
of	O
the	O
video	O
frame	O
,	O
and	O
one	O
to	O
the	O
width	O
of	O
the	O
video	O
frame	O
.	O
table	O
9.1	O
:	O
examples	O
of	O
diﬀerent	O
formats	O
of	O
data	O
that	O
can	O
be	O
used	O
with	O
convolutional	O
networks	O
.	O
361	O
chapter	O
9.	O
convolutional	O
networks	O
note	O
that	O
the	O
use	O
of	O
convolution	O
for	O
processing	O
variable	O
sized	O
inputs	O
only	O
makes	O
sense	O
for	O
inputs	O
that	O
have	O
variable	O
size	O
because	O
they	O
contain	O
varying	O
amounts	O
of	O
observation	O
of	O
the	O
same	O
kind	O
of	O
thing—diﬀerent	O
lengths	O
of	O
recordings	O
over	O
time	O
,	O
diﬀerent	O
widths	O
of	O
observations	O
over	O
space	O
,	O
etc	O
.	O
convolution	O
does	O
not	O
make	O
sense	O
if	O
the	O
input	O
has	O
variable	O
size	O
because	O
it	O
can	O
optionally	O
include	O
diﬀerent	O
kinds	O
of	O
observations	O
.	O
for	O
example	O
,	O
if	O
we	O
are	O
processing	O
college	O
applications	O
,	O
and	O
our	O
features	O
consist	O
of	O
both	O
grades	O
and	O
standardized	O
test	O
scores	O
,	O
but	O
not	O
every	O
applicant	O
took	O
the	O
standardized	O
test	O
,	O
then	O
it	O
does	O
not	O
make	O
sense	O
to	O
convolve	O
the	O
same	O
weights	O
over	O
both	O
the	O
features	O
corresponding	O
to	O
the	O
grades	O
and	O
the	O
features	O
corresponding	O
to	O
the	O
test	O
scores	O
.	O
9.8	O
eﬃcient	O
convolution	O
algorithms	O
modern	O
convolutional	O
network	O
applications	O
often	O
involve	O
networks	O
containing	O
more	O
than	O
one	O
million	O
units	O
.	O
powerful	O
implementations	O
exploiting	O
parallel	O
computation	O
resources	O
,	O
as	O
discussed	O
in	O
section	O
,	O
are	O
essential	O
.	O
however	O
,	O
in	O
many	O
cases	O
it	O
is	O
also	O
possible	O
to	O
speed	O
up	O
convolution	O
by	O
selecting	O
an	O
appropriate	O
convolution	O
algorithm	O
.	O
12.1	O
convolution	O
is	O
equivalent	O
to	O
converting	O
both	O
the	O
input	O
and	O
the	O
kernel	O
to	O
the	O
frequency	O
domain	O
using	O
a	O
fourier	O
transform	O
,	O
performing	O
point-wise	O
multiplication	O
of	O
the	O
two	O
signals	O
,	O
and	O
converting	O
back	O
to	O
the	O
time	O
domain	O
using	O
an	O
inverse	O
fourier	O
transform	O
.	O
for	O
some	O
problem	O
sizes	O
,	O
this	O
can	O
be	O
faster	O
than	O
the	O
naive	O
implementation	O
of	O
discrete	O
convolution	O
.	O
when	O
a	O
d-dimensional	O
kernel	O
can	O
be	O
expressed	O
as	O
the	O
outer	O
product	O
of	O
d	O
vectors	O
,	O
one	O
vector	O
per	O
dimension	O
,	O
the	O
kernel	O
is	O
called	O
separable	O
.	O
when	O
the	O
kernel	O
is	O
separable	O
,	O
naive	O
convolution	O
is	O
ineﬃcient	O
.	O
it	O
is	O
equivalent	O
to	O
compose	O
d	O
one-dimensional	O
convolutions	O
with	O
each	O
of	O
these	O
vectors	O
.	O
the	O
composed	O
approach	O
is	O
signiﬁcantly	O
faster	O
than	O
performing	O
one	O
d-dimensional	O
convolution	O
with	O
their	O
outer	O
product	O
.	O
the	O
kernel	O
also	O
takes	O
fewer	O
parameters	O
to	O
represent	O
as	O
vectors	O
.	O
if	O
the	O
kernel	O
is	O
w	O
elements	O
wide	O
in	O
each	O
dimension	O
,	O
then	O
naive	O
multidimensional	O
×	O
convolution	O
requires	O
o	O
(	O
wd	O
)	O
runtime	O
and	O
parameter	O
storage	O
space	O
,	O
while	O
separable	O
convolution	O
requires	O
o	O
(	O
w	O
d	O
)	O
runtime	O
and	O
parameter	O
storage	O
space	O
.	O
of	O
course	O
,	O
not	O
every	O
convolution	O
can	O
be	O
represented	O
in	O
this	O
way	O
.	O
devising	O
faster	O
ways	O
of	O
performing	O
convolution	O
or	O
approximate	O
convolution	O
without	O
harming	O
the	O
accuracy	O
of	O
the	O
model	B
is	O
an	O
active	O
area	O
of	O
research	O
.	O
even	O
tech-	O
niques	O
that	O
improve	O
the	O
eﬃciency	O
of	O
only	O
forward	O
propagation	O
are	O
useful	O
because	O
in	O
the	O
commercial	O
setting	O
,	O
it	O
is	O
typical	O
to	O
devote	O
more	O
resources	O
to	O
deployment	O
of	O
a	O
network	O
than	O
to	O
its	O
training	O
.	O
362	O
chapter	O
9.	O
convolutional	O
networks	O
9.9	O
random	O
or	O
unsupervised	O
features	O
typically	O
,	O
the	O
most	O
expensive	O
part	O
of	O
convolutional	O
network	O
training	O
is	O
learning	O
the	O
features	O
.	O
the	O
output	O
layer	O
is	O
usually	O
relatively	O
inexpensive	O
due	O
to	O
the	O
small	O
number	O
of	O
features	O
provided	O
as	O
input	O
to	O
this	O
layer	O
after	O
passing	O
through	O
several	O
layers	O
of	O
pooling	O
.	O
when	O
performing	O
supervised	O
training	O
with	O
gradient	O
descent	B
,	O
every	O
gradient	O
step	O
requires	O
a	O
complete	O
run	O
of	O
forward	O
propagation	O
and	O
backward	O
propagation	O
through	O
the	O
entire	O
network	O
.	O
one	O
way	O
to	O
reduce	O
the	O
cost	O
of	O
convolutional	O
network	O
training	O
is	O
to	O
use	O
features	O
that	O
are	O
not	O
trained	O
in	O
a	O
supervised	O
fashion	O
.	O
coates	O
et	O
al	O
.	O
2011	O
there	O
are	O
three	O
basic	O
strategies	O
for	O
obtaining	O
convolution	O
kernels	O
without	O
supervised	O
training	O
.	O
one	O
is	O
to	O
simply	O
initialize	O
them	O
randomly	O
.	O
another	O
is	O
to	O
design	O
them	O
by	O
hand	O
,	O
for	O
example	O
by	O
setting	O
each	O
kernel	O
to	O
detect	O
edges	O
at	O
a	O
certain	O
orientation	O
or	O
scale	O
.	O
finally	O
,	O
one	O
can	O
learn	O
the	O
kernels	O
with	O
an	O
unsupervised	O
k-means	B
clustering	O
to	O
small	O
criterion	O
.	O
for	O
example	O
,	O
image	O
patches	O
,	O
then	O
use	O
each	O
learned	O
centroid	O
as	O
a	O
convolution	O
kernel	O
.	O
part	O
iii	O
describes	O
many	O
more	O
unsupervised	O
learning	O
approaches	O
.	O
learning	O
the	O
features	O
with	O
an	O
unsupervised	O
criterion	O
allows	O
them	O
to	O
be	O
determined	O
separately	O
from	O
the	O
classiﬁer	O
layer	O
at	O
the	O
top	O
of	O
the	O
architecture	O
.	O
one	O
can	O
then	O
extract	O
the	O
features	O
for	O
the	O
entire	O
training	O
set	O
just	O
once	O
,	O
essentially	O
constructing	O
a	O
new	O
training	O
set	O
for	O
the	O
last	O
layer	O
.	O
learning	O
the	O
last	O
layer	O
is	O
then	O
typically	O
a	O
convex	O
optimization	O
problem	O
,	O
assuming	O
the	O
last	O
layer	O
is	O
something	O
like	O
logistic	O
regression	O
or	O
an	O
svm.	O
)	O
apply	O
(	O
;	O
;	O
et	O
al	O
.	O
,	O
et	O
al	O
.	O
,	O
2011	O
pinto	O
2011	O
cox	O
and	O
pinto	O
2011	O
random	O
ﬁlters	O
often	O
work	B
surprisingly	O
well	O
in	O
convolutional	O
networks	O
(	O
jarrett	O
et	O
al	O
.	O
,	O
et	O
al	O
.	O
2009	O
saxe	O
(	O
)	O
showed	O
that	O
layers	O
consisting	O
of	O
convolution	O
following	O
by	O
pooling	O
naturally	O
2011	O
become	O
frequency	O
selective	O
and	O
translation	O
invariant	O
when	O
assigned	O
random	O
weights	O
.	O
they	O
argue	O
that	O
this	O
provides	O
an	O
inexpensive	O
way	O
to	O
choose	O
the	O
architecture	O
of	O
a	O
convolutional	O
network	O
:	O
ﬁrst	O
evaluate	O
the	O
performance	O
of	O
several	O
convolutional	O
network	O
architectures	O
by	O
training	O
only	O
the	O
last	O
layer	O
,	O
then	O
take	O
the	O
best	O
of	O
these	O
architectures	O
and	O
train	O
the	O
entire	O
architecture	O
using	O
a	O
more	O
expensive	O
approach	O
.	O
;	O
,	O
)	O
.	O
saxe	O
an	O
intermediate	O
approach	O
is	O
to	O
learn	O
the	O
features	O
,	O
but	O
using	O
methods	O
that	O
do	O
not	O
require	O
full	O
forward	O
and	O
back-propagation	O
at	O
every	O
gradient	O
step	O
.	O
as	O
with	O
multilayer	O
perceptrons	O
,	O
we	O
use	O
greedy	O
layer-wise	O
pretraining	O
,	O
to	O
train	O
the	O
ﬁrst	O
layer	O
in	O
isolation	O
,	O
then	O
extract	O
all	O
features	O
from	O
the	O
ﬁrst	O
layer	O
only	O
once	O
,	O
then	O
train	O
the	O
second	O
layer	O
in	O
isolation	O
given	O
those	O
features	O
,	O
and	O
so	O
on	O
.	O
chapter	O
has	O
described	O
how	O
to	O
perform	O
supervised	O
greedy	O
layer-wise	O
pretraining	O
,	O
and	O
part	O
extends	O
this	O
to	O
greedy	O
layer-wise	O
pretraining	O
using	O
an	O
unsupervised	O
criterion	O
at	O
each	O
layer	O
.	O
the	O
canonical	O
example	O
of	O
greedy	O
layer-wise	O
pretraining	O
of	O
a	O
convolutional	O
model	B
is	O
the	O
convolutional	O
deep	O
belief	O
network	O
(	O
)	O
.	O
convolutional	O
networks	O
oﬀer	O
lee	O
et	O
al	O
.	O
2009	O
8	O
iii	O
,	O
363	O
chapter	O
9.	O
convolutional	O
networks	O
(	O
)	O
do	O
with	O
coates	O
et	O
al	O
.	O
2011	O
us	O
the	O
opportunity	O
to	O
take	O
the	O
pretraining	O
strategy	O
one	O
step	O
further	O
than	O
is	O
possible	O
with	O
multilayer	O
perceptrons	O
.	O
instead	O
of	O
training	O
an	O
entire	O
convolutional	O
layer	O
at	O
a	O
k-means	B
.	O
time	O
,	O
we	O
can	O
train	O
a	O
model	B
of	O
a	O
small	O
patch	O
,	O
as	O
we	O
can	O
then	O
use	O
the	O
parameters	O
from	O
this	O
patch-based	O
model	B
to	O
deﬁne	O
the	O
kernels	O
of	O
a	O
convolutional	O
layer	O
.	O
this	O
means	O
that	O
it	O
is	O
possible	O
to	O
use	O
unsupervised	O
learning	O
to	O
train	O
a	O
convolutional	O
network	O
without	O
ever	O
using	O
convolution	O
during	O
the	O
training	O
process	O
.	O
using	O
this	O
approach	O
,	O
we	O
can	O
train	O
very	O
large	O
models	O
and	O
incur	O
a	O
high	O
ranzato	O
et	O
al	O
.	O
2007b	O
jarrett	O
et	O
al	O
.	O
computational	O
cost	O
only	O
at	O
inference	O
time	O
(	O
,	O
2009	O
kavukcuoglu	O
)	O
.	O
this	O
approach	O
was	O
popular	O
from	O
roughly	O
2007–2013	O
,	O
when	O
labeled	O
datasets	O
were	O
small	O
and	O
computational	O
power	O
was	O
more	O
limited	O
.	O
today	O
,	O
most	O
convolutional	O
networks	O
are	O
trained	O
in	O
a	O
purely	O
supervised	O
fashion	O
,	O
using	O
full	O
forward	O
and	O
back-propagation	O
through	O
the	O
entire	O
network	O
on	O
each	O
training	O
iteration	O
.	O
2010	O
coates	O
;	O
et	O
al.	O
,	O
et	O
al.	O
,	O
;	O
,	O
;	O
2013	O
as	O
with	O
other	O
approaches	O
to	O
unsupervised	O
pretraining	O
,	O
it	O
remains	O
diﬃcult	O
to	O
tease	O
apart	O
the	O
cause	O
of	O
some	O
of	O
the	O
beneﬁts	O
seen	O
with	O
this	O
approach	O
.	O
unsupervised	O
pretraining	O
may	O
oﬀer	O
some	O
regularization	O
relative	O
to	O
supervised	O
training	O
,	O
or	O
it	O
may	O
simply	O
allow	O
us	O
to	O
train	O
much	O
larger	O
architectures	O
due	O
to	O
the	O
reduced	O
computational	O
cost	O
of	O
the	O
learning	O
rule	O
.	O
9.10	O
the	O
neuroscientiﬁc	O
basis	O
for	O
convolutional	O
net-	O
works	O
convolutional	O
networks	O
are	O
perhaps	O
the	O
greatest	O
success	O
story	O
of	O
biologically	O
inspired	O
artiﬁcial	O
intelligence	O
.	O
though	O
convolutional	O
networks	O
have	O
been	O
guided	O
by	O
many	O
other	O
ﬁelds	O
,	O
some	O
of	O
the	O
key	O
design	O
principles	O
of	O
neural	O
networks	O
were	O
drawn	O
from	O
neuroscience	O
.	O
,	O
,	O
,	O
the	O
history	O
of	O
convolutional	O
networks	O
begins	O
with	O
neuroscientiﬁc	O
experiments	O
long	O
before	O
the	O
relevant	O
computational	O
models	O
were	O
developed	O
.	O
neurophysiologists	O
david	O
hubel	O
and	O
torsten	O
wiesel	O
collaborated	O
for	O
several	O
years	O
to	O
determine	O
many	O
of	O
the	O
most	O
basic	O
facts	O
about	O
how	O
the	O
mammalian	O
vision	O
system	O
works	O
(	O
hubel	O
and	O
wiesel	O
1959	O
1962	O
1968	O
)	O
.	O
their	O
accomplishments	O
were	O
eventually	O
recognized	O
with	O
a	O
nobel	O
prize	O
.	O
their	O
ﬁndings	O
that	O
have	O
had	O
the	O
greatest	O
inﬂuence	O
on	O
contemporary	O
deep	O
learning	O
models	O
were	O
based	O
on	O
recording	O
the	O
activity	O
of	O
individual	O
neurons	O
in	O
cats	O
.	O
they	O
observed	O
how	O
neurons	O
in	O
the	O
cat	O
’	O
s	O
brain	O
responded	O
to	O
images	O
projected	O
in	O
precise	O
locations	O
on	O
a	O
screen	O
in	O
front	O
of	O
the	O
cat	O
.	O
their	O
great	O
discovery	O
was	O
that	O
neurons	O
in	O
the	O
early	O
visual	O
system	O
responded	O
most	O
strongly	O
to	O
very	O
speciﬁc	O
patterns	O
of	O
light	O
,	O
such	O
as	O
precisely	O
oriented	O
bars	O
,	O
but	O
responded	O
hardly	O
at	O
all	O
to	O
other	O
patterns	O
.	O
364	O
chapter	O
9.	O
convolutional	O
networks	O
their	O
work	B
helped	O
to	O
characterize	O
many	O
aspects	O
of	O
brain	O
function	O
that	O
are	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
.	O
from	O
the	O
point	O
of	O
view	O
of	O
deep	O
learning	O
,	O
we	O
can	O
focus	O
on	O
a	O
simpliﬁed	O
,	O
cartoon	O
view	O
of	O
brain	O
function	O
.	O
in	O
this	O
simpliﬁed	O
view	O
,	O
we	O
focus	O
on	O
a	O
part	O
of	O
the	O
brain	O
called	O
v1	O
,	O
also	O
known	O
as	O
the	O
primary	O
visual	O
cortex	O
.	O
v1	O
is	O
the	O
ﬁrst	O
area	O
of	O
the	O
brain	O
that	O
begins	O
to	O
perform	O
signiﬁcantly	O
advanced	O
processing	O
of	O
visual	O
input	O
.	O
in	O
this	O
cartoon	O
view	O
,	O
images	O
are	O
formed	O
by	O
light	O
arriving	O
in	O
the	O
eye	O
and	O
stimulating	O
the	O
retina	O
,	O
the	O
light-sensitive	O
tissue	O
in	O
the	O
back	O
of	O
the	O
eye	O
.	O
the	O
neurons	O
in	O
the	O
retina	O
perform	O
some	O
simple	O
preprocessing	O
of	O
the	O
image	O
but	O
do	O
not	O
substantially	O
alter	O
the	O
way	O
it	O
is	O
represented	O
.	O
the	O
image	O
then	O
passes	O
through	O
the	O
optic	O
nerve	O
and	O
a	O
brain	O
region	O
called	O
the	O
lateral	O
geniculate	O
nucleus	O
.	O
the	O
main	O
role	O
,	O
as	O
far	O
as	O
we	O
are	O
concerned	O
here	O
,	O
of	O
both	O
of	O
these	O
anatomical	O
regions	O
is	O
primarily	O
just	O
to	O
carry	O
the	O
signal	O
from	O
the	O
eye	O
to	O
v1	O
,	O
which	O
is	O
located	O
at	O
the	O
back	O
of	O
the	O
head	O
.	O
a	O
convolutional	O
network	O
layer	O
is	O
designed	O
to	O
capture	O
three	O
properties	O
of	O
v1	O
:	O
1.	O
v1	O
is	O
arranged	O
in	O
a	O
spatial	O
map	O
.	O
it	O
actually	O
has	O
a	O
two-dimensional	O
structure	O
mirroring	O
the	O
structure	O
of	O
the	O
image	O
in	O
the	O
retina	O
.	O
for	O
example	O
,	O
light	O
arriving	O
at	O
the	O
lower	O
half	O
of	O
the	O
retina	O
aﬀects	O
only	O
the	O
corresponding	O
half	O
of	O
v1	O
.	O
convolutional	O
networks	O
capture	O
this	O
property	O
by	O
having	O
their	O
features	O
deﬁned	O
in	O
terms	O
of	O
two	O
dimensional	O
maps	O
.	O
2.	O
v1	O
contains	O
many	O
simple	O
cells	O
.	O
a	O
simple	O
cell	O
’	O
s	O
activity	O
can	O
to	O
some	O
extent	O
be	O
characterized	O
by	O
a	O
linear	O
function	O
of	O
the	O
image	O
in	O
a	O
small	O
,	O
spatially	O
localized	O
receptive	O
ﬁeld	O
.	O
the	O
detector	O
units	O
of	O
a	O
convolutional	O
network	O
are	O
designed	O
to	O
emulate	O
these	O
properties	O
of	O
simple	O
cells	O
.	O
3.	O
v1	O
also	O
contains	O
many	O
complex	O
cells	O
.	O
these	O
cells	O
respond	O
to	O
features	O
that	O
are	O
similar	O
to	O
those	O
detected	O
by	O
simple	O
cells	O
,	O
but	O
complex	O
cells	O
are	O
invariant	O
to	O
small	O
shifts	O
in	O
the	O
position	B
of	O
the	O
feature	O
.	O
this	O
inspires	O
the	O
pooling	O
units	O
of	O
convolutional	O
networks	O
.	O
complex	O
cells	O
are	O
also	O
invariant	O
to	O
some	O
changes	O
in	O
lighting	O
that	O
can	O
not	O
be	O
captured	O
simply	O
by	O
pooling	O
over	O
spatial	O
locations	O
.	O
these	O
invariances	O
have	O
inspired	O
some	O
of	O
the	O
cross-channel	O
pooling	O
strategies	O
in	O
convolutional	O
networks	O
,	O
such	O
as	O
maxout	O
units	O
(	O
goodfellow	O
et	O
al	O
.	O
2013a	O
)	O
.	O
,	O
though	O
we	O
know	O
the	O
most	O
about	O
v1	O
,	O
it	O
is	O
generally	O
believed	O
that	O
the	O
same	O
basic	O
principles	O
apply	O
to	O
other	O
areas	O
of	O
the	O
visual	O
system	O
.	O
in	O
our	O
cartoon	O
view	O
of	O
the	O
visual	O
system	O
,	O
the	O
basic	O
strategy	O
of	O
detection	O
followed	O
by	O
pooling	O
is	O
repeatedly	O
applied	O
as	O
we	O
move	O
deeper	O
into	O
the	O
brain	O
.	O
as	O
we	O
pass	O
through	O
multiple	O
anatomical	O
layers	O
of	O
the	O
brain	O
,	O
we	O
eventually	O
ﬁnd	O
cells	O
that	O
respond	O
to	O
some	O
speciﬁc	O
concept	O
and	O
are	O
invariant	O
to	O
many	O
transformations	O
of	O
the	O
input	O
.	O
these	O
cells	O
have	O
been	O
365	O
chapter	O
9.	O
convolutional	O
networks	O
nicknamed	O
“	O
grandmother	O
cells	O
”	O
—the	O
idea	O
is	O
that	O
a	O
person	O
could	O
have	O
a	O
neuron	O
that	O
activates	O
when	O
seeing	O
an	O
image	O
of	O
their	O
grandmother	O
,	O
regardless	O
of	O
whether	O
she	O
appears	O
in	O
the	O
left	O
or	O
right	O
side	O
of	O
the	O
image	O
,	O
whether	O
the	O
image	O
is	O
a	O
close-up	O
of	O
her	O
face	O
or	O
zoomed	O
out	O
shot	O
of	O
her	O
entire	O
body	O
,	O
whether	O
she	O
is	O
brightly	O
lit	O
,	O
or	O
in	O
shadow	O
,	O
etc	O
.	O
these	O
grandmother	O
cells	O
have	O
been	O
shown	O
to	O
actually	O
exist	O
in	O
the	O
human	O
brain	O
,	O
)	O
.	O
researchers	O
in	O
a	O
region	O
called	O
the	O
medial	O
temporal	O
lobe	O
(	O
tested	O
whether	O
individual	O
neurons	O
would	O
respond	O
to	O
photos	O
of	O
famous	O
individuals	O
.	O
they	O
found	O
what	O
has	O
come	O
to	O
be	O
called	O
the	O
“	O
halle	O
berry	O
neuron	O
”	O
:	O
an	O
individual	O
neuron	O
that	O
is	O
activated	O
by	O
the	O
concept	O
of	O
halle	O
berry	O
.	O
this	O
neuron	O
ﬁres	O
when	O
a	O
person	O
sees	O
a	O
photo	O
of	O
halle	O
berry	O
,	O
a	O
drawing	O
of	O
halle	O
berry	O
,	O
or	O
even	O
text	O
containing	O
the	O
words	O
“	O
halle	O
berry.	O
”	O
of	O
course	O
,	O
this	O
has	O
nothing	O
to	O
do	O
with	O
halle	O
berry	O
herself	O
;	O
other	O
neurons	O
responded	O
to	O
the	O
presence	O
of	O
bill	O
clinton	O
,	O
jennifer	O
aniston	O
,	O
etc	O
.	O
quiroga	O
et	O
al	O
.	O
2005	O
,	O
these	O
medial	O
temporal	O
lobe	O
neurons	O
are	O
somewhat	O
more	O
general	O
than	O
modern	O
convolutional	O
networks	O
,	O
which	O
would	O
not	O
automatically	O
generalize	O
to	O
identifying	O
a	O
person	O
or	O
object	O
when	O
reading	O
its	O
name	O
.	O
the	O
closest	O
analog	O
to	O
a	O
convolutional	O
network	O
’	O
s	O
last	O
layer	O
of	O
features	O
is	O
a	O
brain	O
area	O
called	O
the	O
inferotemporal	O
cortex	O
(	O
it	O
)	O
.	O
when	O
viewing	O
an	O
object	O
,	O
information	O
ﬂows	O
from	O
the	O
retina	O
,	O
through	O
the	O
lgn	O
,	O
to	O
v1	O
,	O
then	O
onward	O
to	O
v2	O
,	O
then	O
v4	O
,	O
then	O
it	O
.	O
this	O
happens	O
within	O
the	O
ﬁrst	O
100ms	O
of	O
glimpsing	O
an	O
object	O
.	O
if	O
a	O
person	O
is	O
allowed	O
to	O
continue	O
looking	O
at	O
the	O
object	O
for	O
more	O
time	O
,	O
then	O
information	O
will	O
begin	O
to	O
ﬂow	O
backwards	O
as	O
the	O
brain	O
uses	O
top-down	O
feedback	O
to	O
update	O
the	O
activations	O
in	O
the	O
lower	O
level	O
brain	O
areas	O
.	O
however	O
,	O
if	O
we	O
interrupt	O
the	O
person	O
’	O
s	O
gaze	O
,	O
and	O
observe	O
only	O
the	O
ﬁring	O
rates	O
that	O
result	O
from	O
the	O
ﬁrst	O
100ms	O
of	O
mostly	O
feedforward	O
activation	O
,	O
then	O
it	O
proves	O
to	O
be	O
very	O
similar	O
to	O
a	O
convolutional	O
network	O
.	O
convolutional	O
networks	O
can	O
predict	O
it	O
ﬁring	O
rates	O
,	O
and	O
also	O
perform	O
very	O
similarly	O
to	O
(	O
time	O
limited	O
)	O
humans	O
on	O
object	O
recognition	B
tasks	O
(	O
dicarlo	O
2013	O
,	O
)	O
.	O
that	O
being	O
said	O
,	O
there	O
are	O
many	O
diﬀerences	O
between	O
convolutional	O
networks	O
and	O
the	O
mammalian	O
vision	O
system	O
.	O
some	O
of	O
these	O
diﬀerences	O
are	O
well	O
known	O
to	O
computational	O
neuroscientists	O
,	O
but	O
outside	O
the	O
scope	O
of	O
this	O
book	O
.	O
some	O
of	O
these	O
diﬀerences	O
are	O
not	O
yet	O
known	O
,	O
because	O
many	O
basic	O
questions	O
about	O
how	O
the	O
mammalian	O
vision	O
system	O
works	O
remain	O
unanswered	O
.	O
as	O
a	O
brief	O
list	O
:	O
•	O
the	O
human	O
eye	O
is	O
mostly	O
very	O
low	O
resolution	O
,	O
except	O
for	O
a	O
tiny	O
patch	O
called	O
the	O
fovea	O
.	O
the	O
fovea	O
only	O
observes	O
an	O
area	O
about	O
the	O
size	O
of	O
a	O
thumbnail	O
held	O
at	O
arms	O
length	O
.	O
though	O
we	O
feel	O
as	O
if	O
we	O
can	O
see	O
an	O
entire	O
scene	O
in	O
high	O
resolution	O
,	O
this	O
is	O
an	O
illusion	O
created	O
by	O
the	O
subconscious	O
part	O
of	O
our	O
brain	O
,	O
as	O
it	O
stitches	O
together	O
several	O
glimpses	O
of	O
small	O
areas	O
.	O
most	O
convolutional	O
networks	O
actually	O
receive	O
large	O
full	O
resolution	O
photographs	O
as	O
input	O
.	O
the	O
human	O
brain	O
makes	O
366	O
chapter	O
9.	O
convolutional	O
networks	O
•	O
•	O
•	O
•	O
several	O
eye	O
movements	O
called	O
saccades	O
to	O
glimpse	O
the	O
most	O
visually	O
salient	O
or	O
task-relevant	O
parts	O
of	O
a	O
scene	O
.	O
incorporating	O
similar	O
attention	O
mechanisms	O
into	O
deep	O
learning	O
models	O
is	O
an	O
active	O
research	O
direction	O
.	O
in	O
the	O
context	O
of	O
deep	O
learning	O
,	O
attention	O
mechanisms	O
have	O
been	O
most	O
successful	O
for	O
natural	O
language	O
processing	O
,	O
as	O
described	O
in	O
section	O
.	O
several	O
visual	O
models	O
with	O
foveation	O
mechanisms	O
have	O
been	O
developed	O
but	O
so	O
far	O
have	O
not	O
become	O
the	O
dominant	O
approach	O
(	O
larochelle	O
and	O
hinton	O
2010	O
denil	O
12.4.5.1	O
et	O
al.	O
,	O
,	O
;	O
2012	O
)	O
.	O
the	O
human	O
visual	O
system	O
is	O
integrated	O
with	O
many	O
other	O
senses	O
,	O
such	O
as	O
hearing	O
,	O
and	O
factors	O
like	O
our	O
moods	O
and	O
thoughts	O
.	O
convolutional	O
networks	O
so	O
far	O
are	O
purely	O
visual	O
.	O
the	O
human	O
visual	O
system	O
does	O
much	O
more	O
than	O
just	O
recognize	O
objects	O
.	O
it	O
is	O
able	O
to	O
understand	O
entire	O
scenes	O
including	O
many	O
objects	O
and	O
relationships	O
between	O
objects	O
,	O
and	O
processes	O
rich	O
3-d	O
geometric	O
information	O
needed	O
for	O
our	O
bodies	O
to	O
interface	O
with	O
the	O
world	O
.	O
convolutional	O
networks	O
have	O
been	O
applied	O
to	O
some	O
of	O
these	O
problems	O
but	O
these	O
applications	O
are	O
in	O
their	O
infancy	O
.	O
even	O
simple	O
brain	O
areas	O
like	O
v1	O
are	O
heavily	O
impacted	O
by	O
feedback	O
from	O
higher	O
levels	O
.	O
feedback	O
has	O
been	O
explored	O
extensively	O
in	O
neural	O
network	O
models	O
but	O
has	O
not	O
yet	O
been	O
shown	O
to	O
oﬀer	O
a	O
compelling	O
improvement	O
.	O
while	O
feedforward	O
it	O
ﬁring	O
rates	O
capture	O
much	O
of	O
the	O
same	O
information	O
as	O
convolutional	O
network	O
features	O
,	O
it	O
is	O
not	O
clear	O
how	O
similar	O
the	O
intermediate	O
computations	O
are	O
.	O
the	O
brain	O
probably	O
uses	O
very	O
diﬀerent	O
activation	O
and	O
pooling	O
functions	O
.	O
an	O
individual	O
neuron	O
’	O
s	O
activation	O
probably	O
is	O
not	O
well-	O
characterized	O
by	O
a	O
single	O
linear	O
ﬁlter	O
response	O
.	O
a	O
recent	O
model	B
of	O
v1	O
involves	O
multiple	O
quadratic	O
ﬁlters	O
for	O
each	O
neuron	O
(	O
)	O
.	O
indeed	O
our	O
cartoon	O
picture	O
of	O
“	O
simple	O
cells	O
”	O
and	O
“	O
complex	O
cells	O
”	O
might	O
create	O
a	O
non-	O
existent	O
distinction	O
;	O
simple	O
cells	O
and	O
complex	O
cells	O
might	O
both	O
be	O
the	O
same	O
kind	O
of	O
cell	O
but	O
with	O
their	O
“	O
parameters	O
”	O
enabling	O
a	O
continuum	O
of	O
behaviors	O
ranging	O
from	O
what	O
we	O
call	O
“	O
simple	O
”	O
to	O
what	O
we	O
call	O
“	O
complex.	O
”	O
rust	O
et	O
al	O
.	O
2005	O
,	O
it	O
is	O
also	O
worth	O
mentioning	O
that	O
neuroscience	O
has	O
told	O
us	O
relatively	O
little	O
about	O
how	O
to	O
train	O
convolutional	O
networks	O
.	O
model	B
structures	O
with	O
parameter	O
sharing	O
across	O
multiple	O
spatial	O
locations	O
date	O
back	O
to	O
early	O
connectionist	O
models	O
of	O
vision	O
(	O
)	O
,	O
but	O
these	O
models	O
did	O
not	O
use	O
the	O
modern	O
back-propagation	O
algorithm	O
and	O
gradient	O
descent	B
.	O
for	O
example	O
,	O
the	O
neocognitron	O
(	O
fukushima	O
1980	O
)	O
incorporated	O
most	O
of	O
the	O
model	B
architecture	O
design	O
elements	O
of	O
the	O
modern	O
convolutional	O
network	O
but	O
relied	O
on	O
a	O
layer-wise	O
unsupervised	O
clustering	O
algorithm	O
.	O
marr	O
and	O
poggio	O
1976	O
,	O
,	O
367	O
chapter	O
9.	O
convolutional	O
networks	O
(	O
lang	O
and	O
hinton	O
1988	O
)	O
introduced	O
the	O
use	O
of	O
back-propagation	O
to	O
train	O
time-delay	O
neural	O
networks	O
(	O
tdnns	O
)	O
.	O
to	O
use	O
contemporary	O
terminology	O
,	O
tdnns	O
are	O
one-dimensional	O
convolutional	O
networks	O
applied	O
to	O
time	O
series	O
.	O
back-	O
propagation	O
applied	O
to	O
these	O
models	O
was	O
not	O
inspired	O
by	O
any	O
neuroscientiﬁc	O
observa-	O
tion	B
and	O
is	O
considered	O
by	O
some	O
to	O
be	O
biologically	O
implausible	O
.	O
following	O
the	O
success	O
of	O
back-propagation-based	O
training	O
of	O
tdnns	O
,	O
(	O
)	O
developed	O
the	O
modern	O
convolutional	O
network	O
by	O
applying	O
the	O
same	O
training	O
algorithm	O
to	O
2-d	O
convolution	O
applied	O
to	O
images	O
.	O
lecun	O
et	O
al	O
.	O
1989	O
,	O
so	O
far	O
we	O
have	O
described	O
how	O
simple	O
cells	O
are	O
roughly	O
linear	O
and	O
selective	O
for	O
certain	O
features	O
,	O
complex	O
cells	O
are	O
more	O
nonlinear	O
and	O
become	O
invariant	O
to	O
some	O
transformations	O
of	O
these	O
simple	O
cell	O
features	O
,	O
and	O
stacks	O
of	O
layers	O
that	O
alternate	O
between	O
selectivity	O
and	O
invariance	O
can	O
yield	O
grandmother	O
cells	O
for	O
very	O
speciﬁc	O
phenomena	O
.	O
we	O
have	O
not	O
yet	O
described	O
precisely	O
what	O
these	O
individual	O
cells	O
detect	O
.	O
in	O
a	O
deep	O
,	O
nonlinear	O
network	O
,	O
it	O
can	O
be	O
diﬃcult	O
to	O
understand	O
the	O
function	O
of	O
individual	O
cells	O
.	O
simple	O
cells	O
in	O
the	O
ﬁrst	O
layer	O
are	O
easier	O
to	O
analyze	O
,	O
because	O
their	O
responses	O
are	O
driven	O
by	O
a	O
linear	O
function	O
.	O
in	O
an	O
artiﬁcial	O
neural	O
network	O
,	O
we	O
can	O
just	O
display	O
an	O
image	O
of	O
the	O
convolution	O
kernel	O
to	O
see	O
what	O
the	O
corresponding	O
channel	O
of	O
a	O
convolutional	O
layer	O
responds	O
to	O
.	O
in	O
a	O
biological	O
neural	O
network	O
,	O
we	O
do	O
not	O
have	O
access	O
to	O
the	O
weights	O
themselves	O
.	O
instead	O
,	O
we	O
put	O
an	O
electrode	O
in	O
the	O
neuron	O
itself	O
,	O
display	O
several	O
samples	O
of	O
white	O
noise	O
images	O
in	O
front	O
of	O
the	O
animal	O
’	O
s	O
retina	O
,	O
and	O
record	O
how	O
each	O
of	O
these	O
samples	O
causes	O
the	O
neuron	O
to	O
activate	O
.	O
we	O
can	O
then	O
ﬁt	O
a	O
linear	O
model	B
to	O
these	O
responses	O
in	O
order	O
to	O
obtain	O
an	O
approximation	O
of	O
the	O
neuron	O
’	O
s	O
weights	O
.	O
this	O
approach	O
is	O
known	O
as	O
reverse	O
correlation	O
(	O
ringach	O
and	O
shapley	O
2004	O
)	O
.	O
,	O
reverse	O
correlation	O
shows	O
us	O
that	O
most	O
v1	O
cells	O
have	O
weights	O
that	O
are	O
described	O
by	O
gabor	O
functions	O
.	O
the	O
gabor	O
function	O
describes	O
the	O
weight	O
at	O
a	O
2-d	O
point	O
in	O
the	O
image	O
.	O
we	O
can	O
think	O
of	O
an	O
image	O
as	O
being	O
a	O
function	O
of	O
2-d	O
coordinates	O
,	O
i	O
(	O
x	O
,	O
y	O
)	O
.	O
likewise	O
,	O
we	O
can	O
think	O
of	O
a	O
simple	O
cell	O
as	O
sampling	O
the	O
image	O
at	O
a	O
set	O
of	O
locations	O
,	O
deﬁned	O
by	O
a	O
set	O
of	O
x	O
coordinates	O
x	O
and	O
a	O
set	O
of	O
y	O
coordinates	O
,	O
y	O
,	O
and	O
applying	O
weights	O
that	O
are	O
also	O
a	O
function	O
of	O
the	O
location	O
,	O
w	O
(	O
x	O
,	O
y	O
)	O
.	O
from	O
this	O
point	O
of	O
view	O
,	O
the	O
response	O
of	O
a	O
simple	O
cell	O
to	O
an	O
image	O
is	O
given	O
by	O
	O
	O
s	O
i	O
(	O
)	O
=	O
∈	O
x	O
x	O
y	O
∈	O
y	O
	O
	O
w	O
x	O
,	O
y	O
i	O
x	O
,	O
y	O
.	O
)	O
)	O
(	O
(	O
(	O
9.15	O
)	O
speciﬁcally	O
,	O
w	O
x	O
,	O
y	O
(	O
)	O
takes	O
the	O
form	O
of	O
a	O
gabor	O
function	O
:	O
w	O
x	O
,	O
y	O
α	O
,	O
β	O
(	O
;	O
)	O
=	O
exp	O
α	O
x	O
,	O
βy	O
,	O
f	O
,	O
φ	O
,	O
x0	O
,	O
y	O
0	O
,	O
τ	O
−	O
	O
x	O
=	O
(	O
x	O
where	O
	O
2	O
−	O
βxx	O
−	O
368	O
−	O
βy	O
y	O
	O
2	O
	O
cos	O
(	O
fx	O
+	O
)	O
φ	O
,	O
(	O
9.16	O
)	O
0	O
)	O
cos	O
(	O
)	O
+	O
(	O
x	O
y	O
τ	O
0	O
)	O
sin	O
(	O
)	O
τ	O
y	O
(	O
9.17	O
)	O
chapter	O
9.	O
convolutional	O
networks	O
and	O
−	O
=	O
(	O
x	O
−	O
	O
y	O
x	O
0	O
)	O
sin	O
(	O
)	O
+	O
(	O
y	O
τ	O
−	O
y	O
0	O
)	O
cos	O
(	O
)	O
τ	O
.	O
(	O
9.18	O
)	O
here	O
,	O
α	O
,	O
βx	O
,	O
βy	O
,	O
f	O
,	O
φ	O
,	O
x0	O
,	O
y0	O
,	O
and	O
τ	O
are	O
parameters	O
that	O
control	O
the	O
properties	O
shows	O
some	O
examples	O
of	O
gabor	O
functions	O
with	O
9.18	O
of	O
the	O
gabor	O
function	O
.	O
figure	O
diﬀerent	O
settings	O
of	O
these	O
parameters	O
.	O
the	O
parameters	O
x0	O
,	O
y0	O
,	O
and	O
τ	O
deﬁne	O
a	O
coordinate	O
system	O
.	O
we	O
translate	O
and	O
	O
rotate	O
x	O
and	O
y	O
to	O
form	O
x	O
.	O
speciﬁcally	O
,	O
the	O
simple	O
cell	O
will	O
respond	O
to	O
image	O
features	O
centered	O
at	O
the	O
point	O
(	O
x0	O
,	O
y	O
0	O
)	O
,	O
and	O
it	O
will	O
respond	O
to	O
changes	O
in	O
brightness	O
as	O
we	O
move	O
along	O
a	O
line	O
rotated	O
radians	O
from	O
the	O
horizontal	O
.	O
and	O
y	O
τ	O
	O
	O
viewed	O
as	O
a	O
function	O
of	O
x	O
	O
and	O
y	O
	O
brightness	O
as	O
we	O
move	O
along	O
the	O
x	O
gaussian	O
function	O
and	O
the	O
other	O
is	O
a	O
cosine	O
function	O
.	O
,	O
the	O
function	O
w	O
then	O
responds	O
to	O
changes	O
in	O
axis	O
.	O
it	O
has	O
two	O
important	O
factors	O
:	O
one	O
is	O
a	O
the	O
gaussian	O
factor	O
α	O
exp	O
can	O
be	O
seen	O
as	O
a	O
gating	O
term	O
that	O
ensures	O
the	O
simple	O
cell	O
will	O
only	O
respond	O
to	O
values	O
near	O
where	O
x	O
are	O
both	O
zero	O
,	O
in	O
other	O
words	O
,	O
near	O
the	O
center	O
of	O
the	O
cell	O
’	O
s	O
receptive	O
ﬁeld	O
.	O
the	O
scaling	O
factor	O
α	O
adjusts	O
the	O
total	O
magnitude	O
of	O
the	O
simple	O
cell	O
’	O
s	O
response	O
,	O
while	O
βx	O
and	O
β	O
y	O
control	O
how	O
quickly	O
its	O
receptive	O
ﬁeld	O
falls	O
oﬀ	O
.	O
	O
and	O
y	O
βyy	O
	O
	O
−	O
βxx	O
	O
2	O
−	O
	O
	O
2	O
	O
the	O
cosine	O
factor	O
cos	O
(	O
fx	O
brightness	O
along	O
the	O
x	O
and	O
controls	O
its	O
phase	O
oﬀset	O
.	O
φ	O
	O
+	O
φ	O
)	O
controls	O
how	O
the	O
simple	O
cell	O
responds	O
to	O
changing	O
axis	O
.	O
the	O
parameter	O
f	O
controls	O
the	O
frequency	O
of	O
the	O
cosine	O
altogether	O
,	O
this	O
cartoon	O
view	O
of	O
simple	O
cells	O
means	O
that	O
a	O
simple	O
cell	O
responds	O
to	O
a	O
speciﬁc	O
spatial	O
frequency	O
of	O
brightness	O
in	O
a	O
speciﬁc	O
direction	O
at	O
a	O
speciﬁc	O
location	O
.	O
simple	O
cells	O
are	O
most	O
excited	O
when	O
the	O
wave	O
of	O
brightness	O
in	O
the	O
image	O
has	O
the	O
same	O
phase	O
as	O
the	O
weights	O
.	O
this	O
occurs	O
when	O
the	O
image	O
is	O
bright	O
where	O
the	O
weights	O
are	O
positive	O
and	O
dark	O
where	O
the	O
weights	O
are	O
negative	O
.	O
simple	O
cells	O
are	O
most	O
inhibited	O
when	O
the	O
wave	O
of	O
brightness	O
is	O
fully	O
out	O
of	O
phase	O
with	O
the	O
weights—when	O
the	O
image	O
is	O
dark	O
where	O
the	O
weights	O
are	O
positive	O
and	O
bright	O
where	O
the	O
weights	O
are	O
negative	O
.	O
	O
the	O
cartoon	O
view	O
of	O
a	O
complex	O
cell	O
is	O
that	O
it	O
computes	O
the	O
l2	O
norm	O
of	O
the	O
s0	O
(	O
)	O
i	O
2	O
+	O
s1	O
(	O
)	O
i	O
2.	O
an	O
2-d	O
vector	O
containing	O
two	O
simple	O
cells	O
’	O
responses	O
:	O
c	O
(	O
i	O
)	O
=	O
important	O
special	O
case	O
occurs	O
when	O
s1	O
has	O
all	O
of	O
the	O
same	O
parameters	O
as	O
s0	O
except	O
for	O
φ	O
,	O
and	O
φ	O
is	O
set	O
such	O
that	O
s1	O
is	O
one	O
quarter	O
cycle	O
out	O
of	O
phase	O
with	O
s0	O
.	O
in	O
this	O
−	O
case	O
,	O
s0	O
and	O
s1	O
form	O
a	O
quadrature	O
pair	O
.	O
a	O
complex	O
cell	O
deﬁned	O
in	O
this	O
way	O
	O
	O
2	O
)	O
contains	O
2	O
responds	O
when	O
the	O
gaussian	O
reweighted	O
image	O
i	O
(	O
x	O
,	O
y	O
)	O
exp	O
(	O
βx	O
x	O
a	O
high	O
amplitude	O
sinusoidal	O
wave	O
with	O
frequency	O
f	O
in	O
direction	O
τ	O
near	O
(	O
x0	O
,	O
y0	O
)	O
,	O
regardless	O
of	O
the	O
phase	O
oﬀset	O
of	O
this	O
wave	O
.	O
in	O
other	O
words	O
,	O
the	O
complex	O
cell	O
is	O
invariant	O
to	O
small	O
translations	O
of	O
the	O
image	O
in	O
direction	O
τ	O
,	O
or	O
to	O
negating	O
the	O
image	O
βyy	O
−	O
369	O
chapter	O
9.	O
convolutional	O
networks	O
figure	O
9.18	O
:	O
gabor	O
functions	O
with	O
a	O
variety	O
of	O
parameter	O
settings	O
.	O
white	O
indicates	O
large	O
positive	O
weight	O
,	O
black	O
indicates	O
large	O
negative	O
weight	O
,	O
and	O
the	O
background	O
gray	O
corresponds	O
to	O
zero	O
weight	O
.	O
(	O
left	O
)	O
gabor	O
functions	O
with	O
diﬀerent	O
values	O
of	O
the	O
parameters	O
that	O
control	O
the	O
coordinate	O
system	O
:	O
x	O
0	O
,	O
y0	O
,	O
and	O
τ	O
.	O
each	O
gabor	O
function	O
in	O
this	O
grid	O
is	O
assigned	O
a	O
value	O
of	O
x0	O
and	O
y	O
0	O
proportional	O
to	O
its	O
position	B
in	O
its	O
grid	O
,	O
and	O
τ	O
is	O
chosen	O
so	O
that	O
each	O
gabor	O
ﬁlter	O
is	O
sensitive	O
to	O
the	O
direction	O
radiating	O
out	O
from	O
the	O
center	O
of	O
the	O
grid	O
.	O
for	O
the	O
other	O
two	O
plots	O
,	O
x0	O
,	O
y	O
0	O
,	O
and	O
τ	O
are	O
ﬁxed	O
to	O
zero	O
.	O
gabor	O
functions	O
with	O
diﬀerent	O
gaussian	O
scale	O
parameters	O
βx	O
and	O
βy	O
.	O
gabor	O
functions	O
are	O
arranged	O
in	O
increasing	O
width	O
(	O
decreasing	O
βx	O
)	O
as	O
we	O
move	O
left	O
to	O
right	O
through	O
the	O
grid	O
,	O
and	O
increasing	O
height	O
×	O
(	O
decreasing	O
βy	O
)	O
as	O
we	O
move	O
top	O
to	O
bottom	O
.	O
for	O
the	O
other	O
two	O
plots	O
,	O
the	O
β	O
values	O
are	O
ﬁxed	O
to	O
1.5	O
f	O
and	O
φ.	O
as	O
we	O
move	O
top	O
to	O
bottom	O
,	O
f	O
increases	O
,	O
and	O
as	O
we	O
move	O
left	O
to	O
right	O
,	O
φ	O
increases	O
.	O
for	O
the	O
other	O
two	O
plots	O
,	O
gabor	O
functions	O
with	O
diﬀerent	O
sinusoid	O
parameters	O
the	O
image	O
width	O
.	O
the	O
image	O
width	O
.	O
is	O
ﬁxed	O
to	O
0	O
and	O
is	O
ﬁxed	O
to	O
5	O
(	O
center	O
)	O
×	O
(	O
right	O
)	O
φ	O
f	O
(	O
replacing	O
black	O
with	O
white	O
and	O
vice	O
versa	O
)	O
.	O
(	O
olshausen	O
and	O
field	O
1996	O
some	O
of	O
the	O
most	O
striking	O
correspondences	O
between	O
neuroscience	O
and	O
machine	O
learning	O
come	O
from	O
visually	O
comparing	O
the	O
features	O
learned	O
by	O
machine	O
learning	O
models	O
with	O
those	O
employed	O
by	O
v1.	O
)	O
showed	O
that	O
a	O
simple	O
unsupervised	O
learning	O
algorithm	O
,	O
sparse	O
coding	O
,	O
learns	O
features	O
with	O
receptive	O
ﬁelds	O
similar	O
to	O
those	O
of	O
simple	O
cells	O
.	O
since	O
then	O
,	O
we	O
have	O
found	O
that	O
an	O
extremely	O
wide	O
variety	O
of	O
statistical	O
learning	O
algorithms	O
learn	O
features	O
with	O
gabor-like	O
functions	O
when	O
applied	O
to	O
natural	O
images	O
.	O
this	O
includes	O
most	O
deep	O
learning	O
algorithms	O
,	O
which	O
learn	O
these	O
features	O
in	O
their	O
ﬁrst	O
layer	O
.	O
figure	O
9.19	O
shows	O
some	O
examples	O
.	O
because	O
so	O
many	O
diﬀerent	O
learning	O
algorithms	O
learn	O
edge	O
detectors	O
,	O
it	O
is	O
diﬃcult	O
to	O
conclude	O
that	O
any	O
speciﬁc	O
learning	O
algorithm	O
is	O
the	O
“	O
right	O
”	O
model	B
of	O
the	O
brain	O
just	O
based	O
on	O
the	O
features	O
that	O
it	O
learns	O
(	O
though	O
it	O
can	O
certainly	O
be	O
a	O
bad	O
sign	O
if	O
an	O
algorithm	O
does	O
learn	O
some	O
sort	O
of	O
edge	O
detector	O
when	O
applied	O
to	O
natural	O
images	O
)	O
.	O
these	O
features	O
are	O
an	O
important	O
part	O
of	O
the	O
statistical	O
structure	O
of	O
natural	O
images	O
and	O
can	O
be	O
recovered	O
by	O
many	O
diﬀerent	O
approaches	O
to	O
statistical	O
modeling	O
.	O
see	O
hyvärinen	O
)	O
for	O
a	O
review	O
of	O
the	O
ﬁeld	O
of	O
natural	O
image	O
statistics	O
.	O
et	O
al	O
.	O
(	O
2009	O
not	O
370	O
chapter	O
9.	O
convolutional	O
networks	O
figure	O
9.19	O
:	O
many	O
machine	O
learning	O
algorithms	O
learn	O
features	O
that	O
detect	O
edges	O
or	O
speciﬁc	O
colors	O
of	O
edges	O
when	O
applied	O
to	O
natural	O
images	O
.	O
these	O
feature	O
detectors	O
are	O
reminiscent	O
of	O
the	O
gabor	O
functions	O
known	O
to	O
be	O
present	O
in	O
primary	O
visual	O
cortex	O
.	O
(	O
left	O
)	O
weights	O
learned	O
by	O
an	O
unsupervised	O
learning	O
algorithm	O
(	O
spike	O
and	O
slab	O
sparse	O
coding	O
)	O
applied	O
to	O
small	O
image	O
patches	O
.	O
(	O
right	O
)	O
convolution	O
kernels	O
learned	O
by	O
the	O
ﬁrst	O
layer	O
of	O
a	O
fully	O
supervised	O
convolutional	O
maxout	O
network	O
.	O
neighboring	O
pairs	O
of	O
ﬁlters	O
drive	O
the	O
same	O
maxout	O
unit	O
.	O
9.11	O
convolutional	O
networks	O
and	O
the	O
history	O
of	O
deep	O
learning	O
convolutional	O
networks	O
have	O
played	O
an	O
important	O
role	O
in	O
the	O
history	O
of	O
deep	O
learning	O
.	O
they	O
are	O
a	O
key	O
example	O
of	O
a	O
successful	O
application	O
of	O
insights	O
obtained	O
by	O
studying	O
the	O
brain	O
to	O
machine	O
learning	O
applications	O
.	O
they	O
were	O
also	O
some	O
of	O
the	O
ﬁrst	O
deep	O
models	O
to	O
perform	O
well	O
,	O
long	O
before	O
arbitrary	O
deep	O
models	O
were	O
considered	O
viable	O
.	O
convolutional	O
networks	O
were	O
also	O
some	O
of	O
the	O
ﬁrst	O
neural	O
networks	O
to	O
solve	O
important	O
commercial	O
applications	O
and	O
remain	O
at	O
the	O
forefront	O
of	O
commercial	O
applications	O
of	O
deep	O
learning	O
today	O
.	O
for	O
example	O
,	O
in	O
the	O
1990s	O
,	O
the	O
neural	O
network	O
research	O
group	O
at	O
at	O
&	O
t	O
developed	O
a	O
convolutional	O
network	O
for	O
reading	O
checks	O
(	O
)	O
.	O
by	O
the	O
end	O
of	O
the	O
1990s	O
,	O
this	O
system	O
deployed	O
by	O
nec	O
was	O
reading	O
over	O
10	O
%	O
of	O
all	O
the	O
checks	O
in	O
the	O
us	O
.	O
later	O
,	O
several	O
ocr	O
and	O
handwriting	O
recognition	B
systems	O
based	O
on	O
convolutional	O
nets	O
were	O
deployed	O
by	O
for	O
more	O
details	O
on	O
such	O
applications	O
microsoft	O
(	O
and	O
more	O
modern	O
applications	O
of	O
convolutional	O
networks	O
.	O
see	O
lecun	O
et	O
al	O
.	O
2010	O
)	O
for	O
a	O
more	O
in-depth	O
history	O
of	O
convolutional	O
networks	O
up	O
to	O
2010.	O
lecun	O
et	O
al	O
.	O
1998b	O
)	O
.	O
see	O
chapter	O
(	O
,	O
simard	O
et	O
al	O
.	O
2003	O
,	O
12	O
convolutional	O
networks	O
were	O
also	O
used	O
to	O
win	O
many	O
contests	O
.	O
the	O
current	O
intensity	O
of	O
commercial	O
interest	O
in	O
deep	O
learning	O
began	O
when	O
krizhevsky	O
et	O
al	O
.	O
(	O
)	O
won	O
the	O
imagenet	O
object	O
recognition	B
challenge	O
,	O
but	O
convolutional	O
networks	O
2012	O
371	O
chapter	O
9.	O
convolutional	O
networks	O
had	O
been	O
used	O
to	O
win	O
other	O
machine	O
learning	O
and	O
computer	O
vision	O
contests	O
with	O
less	O
impact	O
for	O
years	O
earlier	O
.	O
convolutional	O
nets	O
were	O
some	O
of	O
the	O
ﬁrst	O
working	O
deep	O
networks	O
trained	O
with	O
back-propagation	O
.	O
it	O
is	O
not	O
entirely	O
clear	O
why	O
convolutional	O
networks	O
succeeded	O
when	O
general	O
back-propagation	O
networks	O
were	O
considered	O
to	O
have	O
failed	O
.	O
it	O
may	O
simply	O
be	O
that	O
convolutional	O
networks	O
were	O
more	O
computationally	O
eﬃcient	O
than	O
fully	O
connected	O
networks	O
,	O
so	O
it	O
was	O
easier	O
to	O
run	O
multiple	O
experiments	O
with	O
them	O
and	O
tune	O
their	O
implementation	O
and	O
hyperparameters	O
.	O
larger	O
networks	O
also	O
seem	O
to	O
be	O
easier	O
to	O
train	O
.	O
with	O
modern	O
hardware	O
,	O
large	O
fully	O
connected	O
networks	O
appear	O
to	O
perform	O
reasonably	O
on	O
many	O
tasks	O
,	O
even	O
when	O
using	O
datasets	O
that	O
were	O
available	O
and	O
activation	O
functions	O
that	O
were	O
popular	O
during	O
the	O
times	O
when	O
fully	O
connected	O
networks	O
were	O
believed	O
not	O
to	O
work	B
well	O
.	O
it	O
may	O
be	O
that	O
the	O
primary	O
barriers	O
to	O
the	O
success	O
of	O
neural	O
networks	O
were	O
psychological	O
(	O
practitioners	O
did	O
not	O
expect	O
neural	O
networks	O
to	O
work	B
,	O
so	O
they	O
did	O
not	O
make	O
a	O
serious	O
eﬀort	O
to	O
use	O
neural	O
networks	O
)	O
.	O
whatever	O
the	O
case	O
,	O
it	O
is	O
fortunate	O
that	O
convolutional	O
networks	O
performed	O
well	O
decades	O
ago	O
.	O
in	O
many	O
ways	O
,	O
they	O
carried	O
the	O
torch	O
for	O
the	O
rest	O
of	O
deep	O
learning	O
and	O
paved	O
the	O
way	O
to	O
the	O
acceptance	O
of	O
neural	O
networks	O
in	O
general	O
.	O
convolutional	O
networks	O
provide	O
a	O
way	O
to	O
specialize	O
neural	O
networks	O
to	O
work	B
with	O
data	O
that	O
has	O
a	O
clear	O
grid-structured	O
topology	O
and	O
to	O
scale	O
such	O
models	O
to	O
very	O
large	O
size	O
.	O
this	O
approach	O
has	O
been	O
the	O
most	O
successful	O
on	O
a	O
two-dimensional	O
,	O
image	O
topology	O
.	O
to	O
process	O
one-dimensional	O
,	O
sequential	O
data	O
,	O
we	O
turn	O
next	O
to	O
another	O
powerful	O
specialization	O
of	O
the	O
neural	O
networks	O
framework	O
:	O
recurrent	O
neural	O
networks	O
.	O
372	O
chapter	O
10	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
,	O
rumelhart	O
et	O
al	O
.	O
1986a	O
recurrent	O
neural	O
networks	O
or	O
rnns	O
(	O
)	O
are	O
a	O
family	O
of	O
neural	O
networks	O
for	O
processing	O
sequential	O
data	O
.	O
much	O
as	O
a	O
convolutional	O
network	O
is	O
a	O
neural	O
network	O
that	O
is	O
specialized	O
for	O
processing	O
a	O
grid	O
of	O
values	O
x	O
such	O
as	O
an	O
image	O
,	O
a	O
recurrent	O
neural	O
network	O
is	O
a	O
neural	O
network	O
that	O
is	O
specialized	O
for	O
processing	O
a	O
sequence	O
of	O
values	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
τ	O
.	O
just	O
as	O
convolutional	O
networks	O
can	O
readily	O
scale	O
to	O
images	O
with	O
large	O
width	O
and	O
height	O
,	O
and	O
some	O
convolutional	O
networks	O
can	O
process	O
images	O
of	O
variable	O
size	O
,	O
recurrent	O
networks	O
can	O
scale	O
to	O
much	O
longer	O
sequences	O
than	O
would	O
be	O
practical	O
for	O
networks	O
without	O
sequence-based	O
specialization	O
.	O
most	O
recurrent	O
networks	O
can	O
also	O
process	O
sequences	O
of	O
variable	O
length	O
.	O
to	O
go	O
from	O
multi-layer	O
networks	O
to	O
recurrent	O
networks	O
,	O
we	O
need	O
to	O
take	O
advan-	O
tage	O
of	O
one	O
of	O
the	O
early	O
ideas	O
found	O
in	O
machine	O
learning	O
and	O
statistical	O
models	O
of	O
the	O
1980s	O
:	O
sharing	O
parameters	O
across	O
diﬀerent	O
parts	O
of	O
a	O
model	B
.	O
parameter	O
sharing	O
makes	O
it	O
possible	O
to	O
extend	O
and	O
apply	O
the	O
model	B
to	O
examples	O
of	O
diﬀerent	O
forms	O
(	O
diﬀerent	O
lengths	O
,	O
here	O
)	O
and	O
generalize	O
across	O
them	O
.	O
if	O
we	O
had	O
separate	O
parameters	O
for	O
each	O
value	O
of	O
the	O
time	O
index	O
,	O
we	O
could	O
not	O
generalize	O
to	O
sequence	O
lengths	O
not	O
seen	O
during	O
training	O
,	O
nor	O
share	O
statistical	O
strength	O
across	O
diﬀerent	O
sequence	O
lengths	O
and	O
across	O
diﬀerent	O
positions	O
in	O
time	O
.	O
such	O
sharing	O
is	O
particularly	O
important	O
when	O
a	O
speciﬁc	O
piece	O
of	O
information	O
can	O
occur	O
at	O
multiple	O
positions	O
within	O
the	O
sequence	O
.	O
for	O
example	O
,	O
consider	O
the	O
two	O
sentences	O
“	O
i	O
went	O
to	O
nepal	O
in	O
2009	O
”	O
and	O
“	O
in	O
2009	O
,	O
i	O
went	O
to	O
nepal.	O
”	O
if	O
we	O
ask	O
a	O
machine	O
learning	O
model	B
to	O
read	O
each	O
sentence	O
and	O
extract	O
the	O
year	O
in	O
which	O
the	O
narrator	O
went	O
to	O
nepal	O
,	O
we	O
would	O
like	O
it	O
to	O
recognize	O
the	O
year	O
2009	O
as	O
the	O
relevant	O
piece	O
of	O
information	O
,	O
whether	O
it	O
appears	O
in	O
the	O
sixth	O
373	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
word	O
or	O
the	O
second	O
word	O
of	O
the	O
sentence	O
.	O
suppose	O
that	O
we	O
trained	O
a	O
feedforward	O
network	O
that	O
processes	O
sentences	O
of	O
ﬁxed	O
length	O
.	O
a	O
traditional	O
fully	O
connected	O
feedforward	O
network	O
would	O
have	O
separate	O
parameters	O
for	O
each	O
input	O
feature	O
,	O
so	O
it	O
would	O
need	O
to	O
learn	O
all	O
of	O
the	O
rules	O
of	O
the	O
language	O
separately	O
at	O
each	O
position	B
in	O
the	O
sentence	O
.	O
by	O
comparison	O
,	O
a	O
recurrent	O
neural	O
network	O
shares	O
the	O
same	O
weights	O
across	O
several	O
time	O
steps	O
.	O
;	O
et	O
al.	O
,	O
1990	O
;	O
,	O
et	O
al.	O
,	O
1989	O
lang	O
a	O
related	O
idea	O
is	O
the	O
use	O
of	O
convolution	O
across	O
a	O
1-d	O
temporal	O
sequence	O
.	O
this	O
convolutional	O
approach	O
is	O
the	O
basis	O
for	O
time-delay	O
neural	O
networks	O
(	O
lang	O
and	O
hinton	O
1988	O
waibel	O
)	O
.	O
the	O
convolution	O
operation	O
allows	O
a	O
network	O
to	O
share	O
parameters	O
across	O
time	O
,	O
but	O
is	O
shallow	O
.	O
the	O
output	O
of	O
convolution	O
is	O
a	O
sequence	O
where	O
each	O
member	O
of	O
the	O
output	O
is	O
a	O
function	O
of	O
a	O
small	O
number	O
of	O
neighboring	O
members	O
of	O
the	O
input	O
.	O
the	O
idea	O
of	O
parameter	O
sharing	O
manifests	O
in	O
the	O
application	O
of	O
the	O
same	O
convolution	O
kernel	O
at	O
each	O
time	O
step	O
.	O
recurrent	O
networks	O
share	O
parameters	O
in	O
a	O
diﬀerent	O
way	O
.	O
each	O
member	O
of	O
the	O
output	O
is	O
a	O
function	O
of	O
the	O
previous	O
members	O
of	O
the	O
output	O
.	O
each	O
member	O
of	O
the	O
output	O
is	O
produced	O
using	O
the	O
same	O
update	O
rule	O
applied	O
to	O
the	O
previous	O
outputs	O
.	O
this	O
recurrent	O
formulation	O
results	O
in	O
the	O
sharing	O
of	O
parameters	O
through	O
a	O
very	O
deep	O
computational	O
graph	O
.	O
for	O
the	O
simplicity	O
of	O
exposition	O
,	O
we	O
refer	O
to	O
rnns	O
as	O
operating	O
on	O
a	O
sequence	O
that	O
contains	O
vectors	O
x	O
(	O
)	O
t	O
with	O
the	O
time	O
step	O
index	O
t	O
ranging	O
from	O
to1	O
τ	O
.	O
in	O
practice	O
,	O
recurrent	O
networks	O
usually	O
operate	O
on	O
minibatches	O
of	O
such	O
sequences	O
,	O
with	O
a	O
diﬀerent	O
sequence	O
length	O
τ	O
for	O
each	O
member	O
of	O
the	O
minibatch	O
.	O
we	O
have	O
omitted	O
the	O
minibatch	O
indices	O
to	O
simplify	O
notation	O
.	O
moreover	O
,	O
the	O
time	O
step	O
index	O
need	O
not	O
literally	O
refer	O
to	O
the	O
passage	O
of	O
time	O
in	O
the	O
real	O
world	O
.	O
sometimes	O
it	O
refers	O
only	O
to	O
the	O
position	B
in	O
the	O
sequence	O
.	O
rnns	O
may	O
also	O
be	O
applied	O
in	O
two	O
dimensions	O
across	O
spatial	O
data	O
such	O
as	O
images	O
,	O
and	O
even	O
when	O
applied	O
to	O
data	O
involving	O
time	O
,	O
the	O
network	O
may	O
have	O
connections	O
that	O
go	O
backwards	O
in	O
time	O
,	O
provided	O
that	O
the	O
entire	O
sequence	O
is	O
observed	O
before	O
it	O
is	O
provided	O
to	O
the	O
network	O
.	O
this	O
chapter	O
extends	O
the	O
idea	O
of	O
a	O
computational	O
graph	O
to	O
include	O
cycles	O
.	O
these	O
cycles	O
represent	O
the	O
inﬂuence	O
of	O
the	O
present	O
value	O
of	O
a	O
variable	O
on	O
its	O
own	O
value	O
at	O
a	O
future	O
time	O
step	O
.	O
such	O
computational	O
graphs	O
allow	O
us	O
to	O
deﬁne	O
recurrent	O
neural	O
networks	O
.	O
we	O
then	O
describe	O
many	O
diﬀerent	O
ways	O
to	O
construct	O
,	O
train	O
,	O
and	O
use	O
recurrent	O
neural	O
networks	O
.	O
for	O
more	O
information	O
on	O
recurrent	O
neural	O
networks	O
than	O
is	O
available	O
in	O
this	O
chapter	O
,	O
we	O
refer	O
the	O
reader	O
to	O
the	O
textbook	O
of	O
graves	O
2012	O
(	O
)	O
.	O
374	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
10.1	O
unfolding	O
computational	O
graphs	O
a	O
computational	O
graph	O
is	O
a	O
way	O
to	O
formalize	O
the	O
structure	O
of	O
a	O
set	O
of	O
computations	O
,	O
such	O
as	O
those	O
involved	O
in	O
mapping	O
inputs	O
and	O
parameters	O
to	O
outputs	O
and	O
loss	O
.	O
please	O
refer	O
to	O
section	O
for	O
a	O
general	O
introduction	O
.	O
in	O
this	O
section	O
we	O
explain	O
the	O
idea	O
of	O
unfolding	O
a	O
recursive	O
or	O
recurrent	O
computation	O
into	O
a	O
computational	O
graph	O
that	O
has	O
a	O
repetitive	O
structure	O
,	O
typically	O
corresponding	O
to	O
a	O
chain	O
of	O
events	O
.	O
unfolding	O
this	O
graph	O
results	O
in	O
the	O
sharing	O
of	O
parameters	O
across	O
a	O
deep	O
network	O
structure	O
.	O
6.5.1	O
for	O
example	O
,	O
consider	O
the	O
classical	O
form	O
of	O
a	O
dynamical	O
system	O
:	O
−	O
s	O
(	O
)	O
t	O
=	O
(	O
f	O
s	O
(	O
1	O
)	O
t	O
;	O
)	O
θ	O
,	O
(	O
10.1	O
)	O
where	O
s	O
(	O
)	O
t	O
is	O
called	O
the	O
state	O
of	O
the	O
system	O
.	O
equation	O
10.1	O
is	O
recurrent	O
because	O
the	O
deﬁnition	O
of	O
s	O
at	O
time	O
t	O
refers	O
back	O
to	O
the	O
same	O
deﬁnition	O
at	O
time	O
t	O
.	O
1	O
−	O
for	O
a	O
ﬁnite	O
number	O
of	O
time	O
steps	O
τ	O
,	O
the	O
graph	O
can	O
be	O
unfolded	O
by	O
applying	O
τ	O
=	O
3	O
time	O
1	O
times	O
.	O
for	O
example	O
,	O
if	O
we	O
unfold	O
equation	O
10.1	O
for	O
−	O
the	O
deﬁnition	O
τ	O
steps	O
,	O
we	O
obtain	O
s	O
(	O
3	O
)	O
=	O
(	O
f	O
s	O
(	O
2	O
)	O
;	O
)	O
θ	O
=	O
(	O
(	O
f	O
f	O
s	O
(	O
1	O
)	O
;	O
)	O
;	O
)	O
θ	O
θ	O
(	O
10.2	O
)	O
(	O
10.3	O
)	O
unfolding	O
the	O
equation	O
by	O
repeatedly	O
applying	O
the	O
deﬁnition	O
in	O
this	O
way	O
has	O
yielded	O
an	O
expression	O
that	O
does	O
not	O
involve	O
recurrence	O
.	O
such	O
an	O
expression	O
can	O
now	O
be	O
represented	O
by	O
a	O
traditional	O
directed	O
acyclic	O
computational	O
graph	O
.	O
the	O
unfolded	O
computational	O
graph	O
of	O
equation	O
is	O
illustrated	O
in	O
ﬁgure	O
and	O
equation	O
.	O
10.1	O
10.1	O
10.3	O
)	O
...	O
s	O
(	O
s	O
(	O
)	O
...	O
ff	O
−	O
−	O
s	O
(	O
t	O
s	O
(	O
t	O
1	O
)	O
1	O
)	O
ff	O
s	O
(	O
)	O
ts	O
(	O
)	O
t	O
ff	O
s	O
(	O
+1	O
)	O
ts	O
(	O
+1	O
)	O
t	O
ff	O
)	O
...	O
s	O
(	O
s	O
(	O
)	O
...	O
figure	O
10.1	O
:	O
the	O
classical	O
dynamical	O
system	O
described	O
by	O
equation	O
,	O
illustrated	O
as	O
an	O
unfolded	O
computational	O
graph	O
.	O
each	O
node	O
represents	O
the	O
state	O
at	O
some	O
time	O
t	O
and	O
the	O
function	O
f	O
maps	O
the	O
state	O
at	O
t	O
to	O
the	O
state	O
at	O
t	O
+	O
1.	O
the	O
same	O
parameters	O
(	O
the	O
same	O
value	O
of	O
)	O
are	O
used	O
for	O
all	O
time	O
steps	O
.	O
used	O
to	O
parametrize	O
10.1	O
θ	O
f	O
as	O
another	O
example	O
,	O
let	O
us	O
consider	O
a	O
dynamical	O
system	O
driven	O
by	O
an	O
external	O
signal	O
x	O
(	O
)	O
t	O
,	O
−	O
t	O
s	O
(	O
)	O
t	O
=	O
(	O
f	O
s	O
(	O
1	O
)	O
,	O
x	O
(	O
)	O
t	O
;	O
)	O
θ	O
,	O
375	O
(	O
10.4	O
)	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
where	O
we	O
see	O
that	O
the	O
state	O
now	O
contains	O
information	O
about	O
the	O
whole	O
past	O
sequence	O
.	O
recurrent	O
neural	O
networks	O
can	O
be	O
built	O
in	O
many	O
diﬀerent	O
ways	O
.	O
much	O
as	O
almost	O
any	O
function	O
can	O
be	O
considered	O
a	O
feedforward	O
neural	O
network	O
,	O
essentially	O
any	O
function	O
involving	O
recurrence	O
can	O
be	O
considered	O
a	O
recurrent	O
neural	O
network	O
.	O
many	O
recurrent	O
neural	O
networks	O
use	O
equation	O
or	O
a	O
similar	O
equation	O
to	O
deﬁne	O
the	O
values	O
of	O
their	O
hidden	O
units	O
.	O
to	O
indicate	O
that	O
the	O
state	O
is	O
the	O
hidden	O
h	O
to	O
represent	O
units	O
of	O
the	O
network	O
,	O
we	O
now	O
rewrite	O
equation	O
the	O
state	O
:	O
using	O
the	O
variable	O
10.5	O
10.4	O
−	O
t	O
h	O
(	O
)	O
t	O
=	O
(	O
f	O
h	O
(	O
1	O
)	O
,	O
x	O
(	O
)	O
t	O
;	O
)	O
θ	O
,	O
(	O
10.5	O
)	O
illustrated	O
in	O
ﬁgure	O
as	O
output	O
layers	O
that	O
read	O
information	O
out	O
of	O
the	O
state	O
10.2	O
,	O
typical	O
rnns	O
will	O
add	O
extra	O
architectural	O
features	O
such	O
h	O
to	O
make	O
predictions	O
.	O
−	O
2	O
)	O
t	O
,	O
x	O
(	O
when	O
the	O
recurrent	O
network	O
is	O
trained	O
to	O
perform	O
a	O
task	O
that	O
requires	O
predicting	O
the	O
future	O
from	O
the	O
past	O
,	O
the	O
network	O
typically	O
learns	O
to	O
use	O
h	O
(	O
)	O
t	O
as	O
a	O
kind	O
of	O
lossy	O
summary	O
of	O
the	O
task-relevant	O
aspects	O
of	O
the	O
past	O
sequence	O
of	O
inputs	O
up	O
to	O
t.	O
this	O
summary	O
is	O
in	O
general	O
necessarily	O
lossy	O
,	O
since	O
it	O
maps	O
an	O
arbitrary	O
length	O
sequence	O
−	O
1	O
)	O
t	O
(	O
x	O
(	O
)	O
t	O
,	O
x	O
(	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
2	O
)	O
,	O
x	O
(	O
1	O
)	O
)	O
to	O
a	O
ﬁxed	O
length	O
vector	O
h	O
(	O
)	O
t	O
.	O
depending	O
on	O
the	O
training	O
criterion	O
,	O
this	O
summary	O
might	O
selectively	O
keep	O
some	O
aspects	O
of	O
the	O
past	O
sequence	O
with	O
more	O
precision	O
than	O
other	O
aspects	O
.	O
for	O
example	O
,	O
if	O
the	O
rnn	O
is	O
used	O
in	O
statistical	O
language	O
modeling	O
,	O
typically	O
to	O
predict	O
the	O
next	O
word	O
given	O
previous	O
words	O
,	O
it	O
may	O
not	O
be	O
necessary	O
to	O
store	O
all	O
of	O
the	O
information	O
in	O
the	O
input	O
sequence	O
up	O
to	O
time	O
t	O
,	O
but	O
rather	O
only	O
enough	O
information	O
to	O
predict	O
the	O
rest	O
of	O
the	O
sentence	O
.	O
the	O
most	O
demanding	O
situation	O
is	O
when	O
we	O
ask	O
h	O
(	O
)	O
t	O
to	O
be	O
rich	O
enough	O
to	O
allow	O
one	O
to	O
approximately	O
recover	O
the	O
input	O
sequence	O
,	O
as	O
in	O
autoencoder	O
frameworks	O
(	O
chapter	O
)	O
.14	O
)	O
...	O
h	O
(	O
h	O
(	O
)	O
...	O
ff	O
unfold	O
hh	O
xx	O
−	O
−	O
h	O
(	O
t	O
h	O
(	O
t	O
1	O
)	O
1	O
)	O
h	O
(	O
)	O
th	O
(	O
)	O
t	O
h	O
(	O
+1	O
)	O
th	O
(	O
+1	O
)	O
t	O
)	O
...	O
h	O
(	O
h	O
(	O
)	O
...	O
ff	O
ff	O
ff	O
f	O
−	O
−	O
x	O
(	O
t	O
x	O
(	O
t	O
1	O
)	O
1	O
)	O
x	O
(	O
)	O
tx	O
(	O
)	O
t	O
t	O
x	O
(	O
+1	O
)	O
tx	O
(	O
+1	O
)	O
figure	O
10.2	O
:	O
a	O
recurrent	O
network	O
with	O
no	O
outputs	O
.	O
this	O
recurrent	O
network	O
just	O
processes	O
information	O
from	O
the	O
input	O
x	O
by	O
incorporating	O
it	O
into	O
the	O
state	O
h	O
that	O
is	O
passed	O
forward	O
through	O
time	O
.	O
(	O
left	O
)	O
circuit	O
diagram	O
.	O
the	O
black	O
square	O
indicates	O
a	O
delay	O
of	O
a	O
single	O
time	O
step	O
.	O
the	O
same	O
network	O
seen	O
as	O
an	O
unfolded	O
computational	O
graph	O
,	O
where	O
each	O
node	O
is	O
now	O
associated	O
with	O
one	O
particular	O
time	O
instance	O
.	O
(	O
right	O
)	O
equation	O
can	O
be	O
drawn	O
in	O
two	O
diﬀerent	O
ways	O
.	O
one	O
way	O
to	O
draw	O
the	O
rnn	O
is	O
with	O
a	O
diagram	O
containing	O
one	O
node	O
for	O
every	O
component	O
that	O
might	O
exist	O
in	O
a	O
10.5	O
376	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
physical	O
implementation	O
of	O
the	O
model	B
,	O
such	O
as	O
a	O
biological	O
neural	O
network	O
.	O
in	O
this	O
view	O
,	O
the	O
network	O
deﬁnes	O
a	O
circuit	O
that	O
operates	O
in	O
real	O
time	O
,	O
with	O
physical	O
parts	O
whose	O
current	O
state	O
can	O
inﬂuence	O
their	O
future	O
state	O
,	O
as	O
in	O
the	O
left	O
of	O
ﬁgure	O
.	O
10.2	O
throughout	O
this	O
chapter	O
,	O
we	O
use	O
a	O
black	O
square	O
in	O
a	O
circuit	O
diagram	O
to	O
indicate	O
that	O
an	O
interaction	O
takes	O
place	O
with	O
a	O
delay	O
of	O
a	O
single	O
time	O
step	O
,	O
from	O
the	O
state	O
at	O
time	O
t	O
to	O
the	O
state	O
at	O
time	O
t	O
+	O
1.	O
the	O
other	O
way	O
to	O
draw	O
the	O
rnn	O
is	O
as	O
an	O
unfolded	O
computational	O
graph	O
,	O
in	O
which	O
each	O
component	O
is	O
represented	O
by	O
many	O
diﬀerent	O
variables	O
,	O
with	O
one	O
variable	O
per	O
time	O
step	O
,	O
representing	O
the	O
state	O
of	O
the	O
component	O
at	O
that	O
point	O
in	O
time	O
.	O
each	O
variable	O
for	O
each	O
time	O
step	O
is	O
drawn	O
as	O
a	O
separate	O
node	O
of	O
the	O
computational	O
graph	O
,	O
as	O
in	O
the	O
right	O
of	O
ﬁgure	O
.	O
what	O
we	O
call	O
unfolding	O
is	O
the	O
operation	O
that	O
maps	O
a	O
circuit	O
as	O
in	O
the	O
left	O
side	O
of	O
the	O
ﬁgure	O
to	O
a	O
computational	O
graph	O
with	O
repeated	O
pieces	O
as	O
in	O
the	O
right	O
side	O
.	O
the	O
unfolded	O
graph	O
now	O
has	O
a	O
size	O
that	O
depends	O
on	O
the	O
sequence	O
length	O
.	O
10.2	O
we	O
can	O
represent	O
the	O
unfolded	O
recurrence	O
after	O
t	O
steps	O
with	O
a	O
function	O
g	O
(	O
)	O
t	O
:	O
−	O
h	O
(	O
)	O
t	O
=g	O
(	O
)	O
t	O
(	O
x	O
(	O
)	O
t	O
,	O
x	O
(	O
1	O
)	O
t	O
−	O
2	O
)	O
t	O
,	O
x	O
(	O
−	O
1	O
)	O
t	O
=	O
(	O
f	O
h	O
(	O
,	O
x	O
(	O
)	O
t	O
;	O
)	O
θ	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
2	O
)	O
,	O
x	O
(	O
1	O
)	O
)	O
(	O
10.6	O
)	O
(	O
10.7	O
)	O
−	O
the	O
function	O
g	O
(	O
)	O
t	O
takes	O
the	O
whole	O
past	O
sequence	O
(	O
x	O
(	O
)	O
t	O
,	O
x	O
(	O
1	O
)	O
t	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
2	O
)	O
,	O
x	O
(	O
1	O
)	O
)	O
as	O
input	O
and	O
produces	O
the	O
current	O
state	O
,	O
but	O
the	O
unfolded	O
recurrent	O
structure	O
allows	O
us	O
to	O
factorize	O
g	O
(	O
)	O
t	O
into	O
repeated	O
application	O
of	O
a	O
function	O
f.	O
the	O
unfolding	O
process	O
thus	O
introduces	O
two	O
major	O
advantages	O
:	O
−	O
2	O
)	O
t	O
,	O
x	O
(	O
1.	O
regardless	O
of	O
the	O
sequence	O
length	O
,	O
the	O
learned	O
model	B
always	O
has	O
the	O
same	O
input	O
size	O
,	O
because	O
it	O
is	O
speciﬁed	O
in	O
terms	O
of	O
transition	O
from	O
one	O
state	O
to	O
another	O
state	O
,	O
rather	O
than	O
speciﬁed	O
in	O
terms	O
of	O
a	O
variable-length	O
history	O
of	O
states	O
.	O
2.	O
it	O
is	O
possible	O
to	O
use	O
the	O
same	O
transition	O
function	O
f	O
with	O
the	O
same	O
parameters	O
at	O
every	O
time	O
step	O
.	O
these	O
two	O
factors	O
make	O
it	O
possible	O
to	O
learn	O
a	O
single	O
model	B
f	O
that	O
operates	O
on	O
all	O
time	O
steps	O
and	O
all	O
sequence	O
lengths	O
,	O
rather	O
than	O
needing	O
to	O
learn	O
a	O
separate	O
model	B
g	O
(	O
)	O
t	O
for	O
all	O
possible	O
time	O
steps	O
.	O
learning	O
a	O
single	O
,	O
shared	O
model	B
allows	O
generalization	O
to	O
sequence	O
lengths	O
that	O
did	O
not	O
appear	O
in	O
the	O
training	O
set	O
,	O
and	O
allows	O
the	O
model	B
to	O
be	O
estimated	O
with	O
far	O
fewer	O
training	O
examples	O
than	O
would	O
be	O
required	O
without	O
parameter	O
sharing	O
.	O
both	O
the	O
recurrent	O
graph	O
and	O
the	O
unrolled	O
graph	O
have	O
their	O
uses	O
.	O
the	O
recurrent	O
graph	O
is	O
succinct	O
.	O
the	O
unfolded	O
graph	O
provides	O
an	O
explicit	O
description	O
of	O
which	O
computations	O
to	O
perform	O
.	O
the	O
unfolded	O
graph	O
also	O
helps	O
to	O
illustrate	O
the	O
idea	O
of	O
377	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
information	O
ﬂow	O
forward	O
in	O
time	O
(	O
computing	O
outputs	O
and	O
losses	O
)	O
and	O
backward	O
in	O
time	O
(	O
computing	O
gradients	O
)	O
by	O
explicitly	O
showing	O
the	O
path	O
along	O
which	O
this	O
information	O
ﬂows	O
.	O
10.2	O
recurrent	O
neural	O
networks	O
armed	O
with	O
the	O
graph	O
unrolling	O
and	O
parameter	O
sharing	O
ideas	O
of	O
section	O
can	O
design	O
a	O
wide	O
variety	O
of	O
recurrent	O
neural	O
networks	O
.	O
10.1	O
,	O
we	O
yy	O
ll	O
oo	O
vv	O
hh	O
uu	O
xx	O
unfold	O
ww	O
ww	O
)	O
...	O
h	O
(	O
h	O
(	O
)	O
...	O
−	O
−	O
y	O
(	O
t	O
y	O
(	O
t	O
1	O
)	O
1	O
)	O
−	O
−	O
l	O
(	O
t	O
l	O
(	O
t	O
1	O
)	O
1	O
)	O
−	O
−	O
o	O
(	O
t	O
o	O
(	O
t	O
1	O
)	O
1	O
)	O
vv	O
−	O
−	O
h	O
(	O
t	O
h	O
(	O
t	O
1	O
)	O
1	O
)	O
y	O
(	O
)	O
ty	O
(	O
)	O
t	O
y	O
(	O
+1	O
)	O
ty	O
(	O
+1	O
)	O
t	O
l	O
(	O
)	O
tl	O
(	O
)	O
t	O
l	O
(	O
+1	O
)	O
tl	O
(	O
+1	O
)	O
t	O
o	O
(	O
)	O
to	O
(	O
)	O
t	O
t	O
o	O
(	O
+1	O
)	O
to	O
(	O
+1	O
)	O
ww	O
vv	O
ww	O
vv	O
ww	O
h	O
(	O
)	O
th	O
(	O
)	O
t	O
h	O
(	O
+1	O
)	O
th	O
(	O
+1	O
)	O
t	O
)	O
...	O
h	O
(	O
h	O
(	O
)	O
...	O
uu	O
uu	O
uu	O
−	O
−	O
x	O
(	O
t	O
x	O
(	O
t	O
1	O
)	O
1	O
)	O
x	O
(	O
)	O
tx	O
(	O
)	O
t	O
x	O
(	O
+1	O
)	O
tx	O
(	O
+1	O
)	O
t	O
figure	O
10.3	O
:	O
the	O
computational	O
graph	O
to	O
compute	O
the	O
training	O
loss	O
of	O
a	O
recurrent	O
network	O
that	O
maps	O
an	O
input	O
sequence	O
of	O
x	O
values	O
to	O
a	O
corresponding	O
sequence	O
of	O
output	O
o	O
values	O
.	O
a	O
loss	O
l	O
measures	O
how	O
far	O
each	O
o	O
is	O
from	O
the	O
corresponding	O
training	O
target	O
y	O
.	O
when	O
using	O
softmax	O
outputs	O
,	O
we	O
assume	O
o	O
is	O
the	O
unnormalized	O
log	O
probabilities	O
.	O
the	O
loss	O
l	O
internally	O
computes	O
ˆy	O
=	O
softmax	O
(	O
o	O
)	O
and	O
compares	O
this	O
to	O
the	O
target	O
y.	O
the	O
rnn	O
has	O
input	O
to	O
hidden	O
connections	O
parametrized	O
by	O
a	O
weight	O
matrix	O
u	O
,	O
hidden-to-hidden	O
recurrent	O
connections	O
parametrized	O
by	O
a	O
weight	O
matrix	O
w	O
,	O
and	O
hidden-to-output	O
connections	O
parametrized	O
by	O
a	O
weight	O
matrix	O
v	O
.	O
equation	O
(	O
left	O
)	O
the	O
rnn	O
and	O
its	O
loss	O
drawn	O
with	O
recurrent	O
connections	O
.	O
(	O
right	O
)	O
the	O
same	O
seen	O
as	O
an	O
time-	O
unfolded	O
computational	O
graph	O
,	O
where	O
each	O
node	O
is	O
now	O
associated	O
with	O
one	O
particular	O
time	O
instance	O
.	O
deﬁnes	O
forward	O
propagation	O
in	O
this	O
model	B
.	O
10.8	O
some	O
examples	O
of	O
important	O
design	O
patterns	O
for	O
recurrent	O
neural	O
networks	O
include	O
the	O
following	O
:	O
378	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
•	O
•	O
•	O
recurrent	O
networks	O
that	O
produce	O
an	O
output	O
at	O
each	O
time	O
step	O
and	O
have	O
recurrent	O
connections	O
between	O
hidden	O
units	O
,	O
illustrated	O
in	O
ﬁgure	O
.	O
10.3	O
recurrent	O
networks	O
that	O
produce	O
an	O
output	O
at	O
each	O
time	O
step	O
and	O
have	O
recurrent	O
connections	O
only	O
from	O
the	O
output	O
at	O
one	O
time	O
step	O
to	O
the	O
hidden	O
units	O
at	O
the	O
next	O
time	O
step	O
,	O
illustrated	O
in	O
ﬁgure	O
10.4	O
recurrent	O
networks	O
with	O
recurrent	O
connections	O
between	O
hidden	O
units	O
,	O
that	O
read	O
an	O
entire	O
sequence	O
and	O
then	O
produce	O
a	O
single	O
output	O
,	O
illustrated	O
in	O
ﬁgure	O
10.5	O
.	O
10.3	O
ﬁgure	O
most	O
of	O
the	O
chapter	O
.	O
is	O
a	O
reasonably	O
representative	O
example	O
that	O
we	O
return	O
to	O
throughout	O
10.8	O
,	O
,	O
;	O
10.3	O
and	O
equation	O
the	O
recurrent	O
neural	O
network	O
of	O
ﬁgure	O
is	O
universal	O
in	O
the	O
sense	O
that	O
any	O
function	O
computable	O
by	O
a	O
turing	O
machine	O
can	O
be	O
computed	O
by	O
such	O
a	O
recurrent	O
network	O
of	O
a	O
ﬁnite	O
size	O
.	O
the	O
output	O
can	O
be	O
read	O
from	O
the	O
rnn	O
after	O
a	O
number	O
of	O
time	O
steps	O
that	O
is	O
asymptotically	O
linear	O
in	O
the	O
number	O
of	O
time	O
steps	O
used	O
by	O
the	O
turing	O
machine	O
and	O
asymptotically	O
linear	O
in	O
the	O
length	O
of	O
the	O
input	O
(	O
siegelmann	O
and	O
sontag	O
1991	O
siegelmann	O
1995	O
siegelmann	O
and	O
sontag	O
1995	O
;	O
hyotyniemi	O
1996	O
)	O
.	O
the	O
functions	O
computable	O
by	O
a	O
turing	O
machine	O
are	O
discrete	O
,	O
so	O
these	O
results	O
regard	O
exact	O
implementation	O
of	O
the	O
function	O
,	O
not	O
approximations	O
.	O
the	O
rnn	O
,	O
when	O
used	O
as	O
a	O
turing	O
machine	O
,	O
takes	O
a	O
binary	O
sequence	O
as	O
input	O
and	O
its	O
outputs	O
must	O
be	O
discretized	O
to	O
provide	O
a	O
binary	O
output	O
.	O
it	O
is	O
possible	O
to	O
compute	O
all	O
functions	O
in	O
this	O
setting	O
using	O
a	O
single	O
speciﬁc	O
rnn	O
of	O
ﬁnite	O
size	O
(	O
siegelmann	O
and	O
sontag	O
1995	O
)	O
use	O
886	O
units	O
)	O
.	O
the	O
“	O
input	O
”	O
of	O
the	O
turing	O
machine	O
is	O
a	O
speciﬁcation	O
of	O
the	O
function	O
to	O
be	O
computed	O
,	O
so	O
the	O
same	O
network	O
that	O
simulates	O
this	O
turing	O
machine	O
is	O
suﬃcient	O
for	O
all	O
problems	O
.	O
the	O
theoretical	O
rnn	O
used	O
for	O
the	O
proof	O
can	O
simulate	O
an	O
unbounded	O
stack	O
by	O
representing	O
its	O
activations	O
and	O
weights	O
with	O
rational	O
numbers	O
of	O
unbounded	O
precision	O
.	O
,	O
;	O
,	O
(	O
10.3	O
we	O
now	O
develop	O
the	O
forward	O
propagation	O
equations	O
for	O
the	O
rnn	O
depicted	O
in	O
ﬁgure	O
.	O
the	O
ﬁgure	O
does	O
not	O
specify	O
the	O
choice	O
of	O
activation	O
function	O
for	O
the	O
hidden	O
units	O
.	O
here	O
we	O
assume	O
the	O
hyperbolic	O
tangent	O
activation	O
function	O
.	O
also	O
,	O
the	O
ﬁgure	O
does	O
not	O
specify	O
exactly	O
what	O
form	O
the	O
output	O
and	O
loss	O
function	O
take	O
.	O
here	O
we	O
assume	O
that	O
the	O
output	O
is	O
discrete	O
,	O
as	O
if	O
the	O
rnn	O
is	O
used	O
to	O
predict	O
words	O
or	O
characters	O
.	O
a	O
natural	O
way	O
to	O
represent	O
discrete	O
variables	O
is	O
to	O
regard	O
the	O
output	O
o	O
as	O
giving	O
the	O
unnormalized	O
log	O
probabilities	O
of	O
each	O
possible	O
value	O
of	O
the	O
discrete	O
variable	O
.	O
we	O
can	O
then	O
apply	O
the	O
softmax	O
operation	O
as	O
a	O
post-processing	O
step	O
to	O
obtain	O
a	O
vector	O
ˆy	O
of	O
normalized	O
probabilities	O
over	O
the	O
output	O
.	O
forward	O
propagation	O
begins	O
with	O
a	O
speciﬁcation	O
of	O
the	O
initial	O
state	O
h	O
(	O
0	O
)	O
.	O
then	O
,	O
for	O
each	O
time	O
step	O
from	O
379	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
yy	O
ll	O
oo	O
hh	O
v	O
u	O
xx	O
)	O
...	O
o	O
(	O
o	O
(	O
)	O
...	O
w	O
unfold	O
−	O
−	O
y	O
(	O
t	O
y	O
(	O
t	O
1	O
)	O
1	O
)	O
−	O
−	O
l	O
(	O
t	O
l	O
(	O
t	O
1	O
)	O
1	O
)	O
−	O
−	O
o	O
(	O
t	O
o	O
(	O
t	O
1	O
)	O
1	O
)	O
y	O
(	O
)	O
ty	O
(	O
)	O
t	O
y	O
(	O
+1	O
)	O
ty	O
(	O
+1	O
)	O
t	O
l	O
(	O
)	O
tl	O
(	O
)	O
t	O
t	O
l	O
(	O
+1	O
)	O
tl	O
(	O
+1	O
)	O
o	O
(	O
)	O
to	O
(	O
)	O
t	O
t	O
o	O
(	O
+1	O
)	O
to	O
(	O
+1	O
)	O
w	O
w	O
w	O
w	O
v	O
v	O
v	O
−	O
−	O
h	O
(	O
t	O
h	O
(	O
t	O
1	O
)	O
1	O
)	O
u	O
−	O
−	O
1	O
)	O
1	O
)	O
x	O
(	O
t	O
x	O
(	O
t	O
h	O
(	O
)	O
th	O
(	O
)	O
t	O
t	O
h	O
(	O
+1	O
)	O
th	O
(	O
+1	O
)	O
)	O
...	O
h	O
(	O
h	O
(	O
)	O
...	O
u	O
u	O
x	O
(	O
)	O
tx	O
(	O
)	O
t	O
t	O
x	O
(	O
+1	O
)	O
tx	O
(	O
+1	O
)	O
10.3	O
figure	O
10.4	O
:	O
an	O
rnn	O
whose	O
only	O
recurrence	O
is	O
the	O
feedback	O
connection	O
from	O
the	O
output	O
to	O
the	O
hidden	O
layer	O
.	O
at	O
each	O
time	O
step	O
t	O
,	O
the	O
input	O
is	O
x	O
t	O
,	O
the	O
hidden	O
layer	O
activations	O
are	O
h	O
(	O
)	O
t	O
,	O
the	O
outputs	O
are	O
o	O
(	O
)	O
t	O
,	O
the	O
targets	O
are	O
y	O
(	O
)	O
t	O
and	O
the	O
loss	O
is	O
l	O
(	O
)	O
t	O
.	O
(	O
left	O
)	O
circuit	O
diagram	O
.	O
(	O
right	O
)	O
unfolded	O
computational	O
graph	O
.	O
such	O
an	O
rnn	O
is	O
less	O
powerful	O
(	O
can	O
express	O
a	O
smaller	O
set	O
of	O
functions	O
)	O
than	O
those	O
in	O
the	O
family	O
represented	O
by	O
ﬁgure	O
.	O
the	O
rnn	O
in	O
ﬁgure	O
can	O
choose	O
to	O
put	O
any	O
information	O
it	O
wants	O
about	O
the	O
past	O
into	O
its	O
hidden	O
representation	O
h	O
and	O
transmit	O
h	O
to	O
the	O
future	O
.	O
the	O
rnn	O
in	O
this	O
ﬁgure	O
is	O
trained	O
to	O
put	O
a	O
speciﬁc	O
output	O
value	O
into	O
o	O
,	O
and	O
o	O
is	O
the	O
only	O
information	O
it	O
is	O
allowed	O
to	O
send	O
to	O
the	O
future	O
.	O
there	O
are	O
no	O
direct	O
connections	O
from	O
h	O
going	O
forward	O
.	O
the	O
previous	O
h	O
is	O
connected	O
to	O
the	O
present	O
only	O
indirectly	O
,	O
via	O
the	O
predictions	O
it	O
was	O
used	O
to	O
produce	O
.	O
unless	O
o	O
is	O
very	O
high-dimensional	O
and	O
rich	O
,	O
it	O
will	O
usually	O
lack	O
important	O
information	O
from	O
the	O
past	O
.	O
this	O
makes	O
the	O
rnn	O
in	O
this	O
ﬁgure	O
less	O
powerful	O
,	O
but	O
it	O
may	O
be	O
easier	O
to	O
train	O
because	O
each	O
time	O
step	O
can	O
be	O
trained	O
in	O
isolation	O
from	O
the	O
others	O
,	O
allowing	O
greater	O
parallelization	O
during	O
training	O
,	O
as	O
described	O
in	O
section	O
.	O
10.2.1	O
10.3	O
380	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
t	O
=	O
1	O
to	O
=	O
τ	O
t	O
,	O
we	O
apply	O
the	O
following	O
update	O
equations	O
:	O
−	O
a	O
(	O
)	O
t	O
=	O
1	O
)	O
t	O
+b	O
w	O
h	O
(	O
h	O
(	O
)	O
t	O
=	O
tanh	B
(	O
a	O
(	O
)	O
t	O
)	O
o	O
(	O
)	O
t	O
=	O
+c	O
v	O
h	O
(	O
)	O
t	O
ˆy	O
(	O
)	O
t	O
=	O
softmax	O
(	O
o	O
(	O
)	O
t	O
)	O
+	O
u	O
x	O
(	O
)	O
t	O
(	O
10.8	O
)	O
(	O
10.9	O
)	O
(	O
10.10	O
)	O
(	O
10.11	O
)	O
where	O
the	O
parameters	O
are	O
the	O
bias	O
vectors	O
b	O
and	O
c	O
along	O
with	O
the	O
weight	O
matrices	O
u	O
,	O
v	O
and	O
w	O
,	O
respectively	O
for	O
input-to-hidden	O
,	O
hidden-to-output	O
and	O
hidden-to-	O
hidden	O
connections	O
.	O
this	O
is	O
an	O
example	O
of	O
a	O
recurrent	O
network	O
that	O
maps	O
an	O
input	O
sequence	O
to	O
an	O
output	O
sequence	O
of	O
the	O
same	O
length	O
.	O
the	O
total	O
loss	O
for	O
a	O
values	O
would	O
then	O
be	O
just	O
given	O
sequence	O
of	O
the	O
sum	O
of	O
the	O
losses	O
over	O
all	O
the	O
time	O
steps	O
.	O
for	O
example	O
,	O
if	O
l	O
(	O
)	O
t	O
is	O
the	O
negative	O
log-likelihood	O
of	O
y	O
(	O
)	O
t	O
given	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
t	O
,	O
then	O
values	O
paired	O
with	O
a	O
sequence	O
of	O
	O
x	O
y	O
	O
	O
	O
l	O
{	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
τ	O
}	O
{	O
,	O
y	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
y	O
(	O
)	O
τ	O
}	O
	O
	O
|	O
{	O
}	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
t	O
,	O
y	O
(	O
)	O
t	O
log	O
pmodel	O
	O
=	O
l	O
(	O
)	O
t	O
t	O
−	O
t	O
=	O
|	O
{	O
	O
}	O
(	O
10.12	O
)	O
(	O
10.13	O
)	O
(	O
10.14	O
)	O
y	O
(	O
)	O
t	O
10.3	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
t	O
is	O
given	O
by	O
reading	O
the	O
entry	O
for	O
y	O
(	O
)	O
t	O
where	O
pmodel	O
from	O
the	O
model	B
’	O
s	O
output	O
vector	O
ˆy	O
(	O
)	O
t	O
.	O
computing	O
the	O
gradient	O
of	O
this	O
loss	O
function	O
with	O
respect	O
to	O
the	O
parameters	O
is	O
an	O
expensive	O
operation	O
.	O
the	O
gradient	O
computation	O
involves	O
performing	O
a	O
forward	O
propagation	O
pass	O
moving	O
left	O
to	O
right	O
through	O
our	O
illustration	O
of	O
the	O
unrolled	O
graph	O
in	O
ﬁgure	O
,	O
followed	O
by	O
a	O
backward	O
propagation	O
pass	O
moving	O
right	O
to	O
left	O
through	O
the	O
graph	O
.	O
the	O
runtime	O
is	O
o	O
(	O
τ	O
)	O
and	O
can	O
not	O
be	O
reduced	O
by	O
parallelization	O
because	O
the	O
forward	O
propagation	O
graph	O
is	O
inherently	O
sequential	O
;	O
each	O
time	O
step	O
may	O
only	O
be	O
computed	O
after	O
the	O
previous	O
one	O
.	O
states	O
computed	O
in	O
the	O
forward	O
pass	O
must	O
be	O
stored	O
until	O
they	O
are	O
reused	O
during	O
the	O
backward	O
pass	O
,	O
so	O
the	O
memory	O
cost	O
is	O
also	O
o	O
(	O
τ	O
)	O
.	O
the	O
back-propagation	O
algorithm	O
applied	O
to	O
the	O
unrolled	O
graph	O
with	O
o	O
(	O
τ	O
)	O
cost	O
is	O
called	O
back-propagation	O
through	O
time	O
or	O
bptt	O
and	O
is	O
discussed	O
further	O
in	O
section	O
.	O
the	O
network	O
with	O
recurrence	O
between	O
hidden	O
units	O
is	O
thus	O
very	O
powerful	O
but	O
also	O
expensive	O
to	O
train	O
.	O
is	O
there	O
an	O
alternative	O
?	O
10.2.2	O
10.2.1	O
teacher	O
forcing	O
and	O
networks	O
with	O
output	O
recurrence	O
the	O
network	O
with	O
recurrent	O
connections	O
only	O
from	O
the	O
output	O
at	O
one	O
time	O
step	O
to	O
)	O
is	O
strictly	O
less	O
powerful	O
the	O
hidden	O
units	O
at	O
the	O
next	O
time	O
step	O
(	O
shown	O
in	O
ﬁgure	O
10.4	O
381	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
because	O
it	O
lacks	O
hidden-to-hidden	O
recurrent	O
connections	O
.	O
for	O
example	O
,	O
it	O
can	O
not	O
simulate	O
a	O
universal	O
turing	O
machine	O
.	O
because	O
this	O
network	O
lacks	O
hidden-to-hidden	O
recurrence	O
,	O
it	O
requires	O
that	O
the	O
output	O
units	O
capture	O
all	O
of	O
the	O
information	O
about	O
the	O
past	O
that	O
the	O
network	O
will	O
use	O
to	O
predict	O
the	O
future	O
.	O
because	O
the	O
output	O
units	O
are	O
explicitly	O
trained	O
to	O
match	O
the	O
training	O
set	O
targets	O
,	O
they	O
are	O
unlikely	O
to	O
capture	O
the	O
necessary	O
information	O
about	O
the	O
past	O
history	O
of	O
the	O
input	O
,	O
unless	O
the	O
user	O
knows	O
how	O
to	O
describe	O
the	O
full	O
state	O
of	O
the	O
system	O
and	O
provides	O
it	O
as	O
part	O
of	O
the	O
training	O
set	O
targets	O
.	O
the	O
advantage	O
of	O
eliminating	O
hidden-to-hidden	O
recurrence	O
is	O
that	O
,	O
for	O
any	O
loss	O
function	O
based	O
on	O
comparing	O
the	O
prediction	O
at	O
time	O
t	O
to	O
the	O
training	O
target	O
at	O
time	O
t	O
,	O
all	O
the	O
time	O
steps	O
are	O
decoupled	O
.	O
training	O
can	O
thus	O
be	O
parallelized	O
,	O
with	O
the	O
gradient	O
for	O
each	O
step	O
t	O
computed	O
in	O
isolation	O
.	O
there	O
is	O
no	O
need	O
to	O
compute	O
the	O
output	O
for	O
the	O
previous	O
time	O
step	O
ﬁrst	O
,	O
because	O
the	O
training	O
set	O
provides	O
the	O
ideal	O
value	O
of	O
that	O
output	O
.	O
l	O
(	O
)	O
τl	O
(	O
)	O
τ	O
y	O
(	O
)	O
τy	O
(	O
)	O
τ	O
o	O
(	O
)	O
τo	O
(	O
)	O
τ	O
v	O
h	O
(	O
)	O
th	O
(	O
)	O
t	O
.	O
.	O
.	O
.	O
.	O
.	O
h	O
(	O
)	O
τh	O
(	O
)	O
τ	O
.	O
.	O
.	O
.	O
.	O
.	O
−	O
−	O
h	O
(	O
t	O
h	O
(	O
t	O
1	O
)	O
1	O
)	O
w	O
w	O
w	O
w	O
u	O
u	O
u	O
u	O
−	O
−	O
x	O
(	O
t	O
x	O
(	O
t	O
1	O
)	O
1	O
)	O
x	O
(	O
)	O
tx	O
(	O
)	O
t	O
)	O
...	O
x	O
(	O
x	O
(	O
)	O
...	O
x	O
(	O
)	O
τx	O
(	O
)	O
τ	O
figure	O
10.5	O
:	O
time-unfolded	O
recurrent	O
neural	O
network	O
with	O
a	O
single	O
output	O
at	O
the	O
end	O
of	O
the	O
sequence	O
.	O
such	O
a	O
network	O
can	O
be	O
used	O
to	O
summarize	O
a	O
sequence	O
and	O
produce	O
a	O
ﬁxed-size	O
representation	O
used	O
as	O
input	O
for	O
further	O
processing	O
.	O
there	O
might	O
be	O
a	O
target	O
right	O
at	O
the	O
end	O
(	O
as	O
depicted	O
here	O
)	O
or	O
the	O
gradient	O
on	O
the	O
output	O
o	O
(	O
)	O
t	O
can	O
be	O
obtained	O
by	O
back-propagating	O
from	O
further	O
downstream	O
modules	O
.	O
models	O
that	O
have	O
recurrent	O
connections	O
from	O
their	O
outputs	O
leading	O
back	O
into	O
the	O
model	B
may	O
be	O
trained	O
with	O
teacher	O
forcing	O
.	O
teacher	O
forcing	O
is	O
a	O
procedure	O
that	O
emerges	O
from	O
the	O
maximum	O
likelihood	O
criterion	O
,	O
in	O
which	O
during	O
training	O
the	O
model	B
receives	O
the	O
ground	O
truth	O
output	O
y	O
(	O
)	O
t	O
as	O
input	O
at	O
time	O
t	O
+	O
1.	O
we	O
can	O
see	O
this	O
by	O
examining	O
a	O
sequence	O
with	O
two	O
time	O
steps	O
.	O
the	O
conditional	O
maximum	O
382	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
−	O
−	O
y	O
(	O
t	O
y	O
(	O
t	O
1	O
)	O
1	O
)	O
−	O
−	O
l	O
(	O
t	O
l	O
(	O
t	O
1	O
)	O
1	O
)	O
−	O
−	O
o	O
(	O
t	O
o	O
(	O
t	O
1	O
)	O
1	O
)	O
v	O
−	O
−	O
h	O
(	O
t	O
h	O
(	O
t	O
1	O
)	O
1	O
)	O
u	O
−	O
−	O
1	O
)	O
1	O
)	O
x	O
(	O
t	O
x	O
(	O
t	O
w	O
y	O
(	O
)	O
ty	O
(	O
)	O
t	O
l	O
(	O
)	O
tl	O
(	O
)	O
t	O
o	O
(	O
)	O
to	O
(	O
)	O
t	O
v	O
h	O
(	O
)	O
th	O
(	O
)	O
t	O
u	O
x	O
(	O
)	O
tx	O
(	O
)	O
t	O
−	O
−	O
o	O
(	O
t	O
o	O
(	O
t	O
1	O
)	O
1	O
)	O
w	O
v	O
−	O
−	O
h	O
(	O
t	O
h	O
(	O
t	O
1	O
)	O
1	O
)	O
u	O
−	O
−	O
1	O
)	O
1	O
)	O
x	O
(	O
t	O
x	O
(	O
t	O
o	O
(	O
)	O
to	O
(	O
)	O
t	O
v	O
h	O
(	O
)	O
th	O
(	O
)	O
t	O
u	O
x	O
(	O
)	O
tx	O
(	O
)	O
t	O
train	O
time	O
test	O
time	O
figure	O
10.6	O
:	O
illustration	O
of	O
teacher	O
forcing	O
.	O
teacher	O
forcing	O
is	O
a	O
training	O
technique	O
that	O
is	O
applicable	O
to	O
rnns	O
that	O
have	O
connections	O
from	O
their	O
output	O
to	O
their	O
hidden	O
states	O
at	O
the	O
next	O
time	O
step	O
.	O
(	O
left	O
)	O
at	O
train	O
time	O
,	O
we	O
feed	O
the	O
correct	O
output	O
y	O
(	O
)	O
t	O
drawn	O
from	O
the	O
train	O
set	O
as	O
input	O
to	O
h	O
(	O
+1	O
)	O
when	O
the	O
model	B
is	O
deployed	O
,	O
the	O
true	O
output	O
is	O
generally	O
not	O
known	O
.	O
in	O
this	O
case	O
,	O
we	O
approximate	O
the	O
correct	O
output	O
y	O
(	O
)	O
t	O
with	O
the	O
model	B
’	O
s	O
output	O
o	O
(	O
)	O
t	O
,	O
and	O
feed	O
the	O
output	O
back	O
into	O
the	O
model	B
.	O
(	O
right	O
)	O
.	O
t	O
383	O
likelihood	O
criterion	O
is	O
	O
	O
	O
	O
|	O
x	O
(	O
1	O
)	O
,	O
x	O
(	O
2	O
)	O
	O
	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
log	O
p	O
y	O
(	O
1	O
)	O
,	O
y	O
(	O
2	O
)	O
|	O
=	O
log	O
p	O
y	O
(	O
2	O
)	O
y	O
(	O
1	O
)	O
,	O
x	O
(	O
1	O
)	O
,	O
x	O
(	O
2	O
)	O
+	O
log	O
p	O
y	O
(	O
1	O
)	O
|	O
x	O
(	O
1	O
)	O
,	O
x	O
(	O
2	O
)	O
(	O
10.15	O
)	O
(	O
10.16	O
)	O
in	O
this	O
example	O
,	O
we	O
see	O
that	O
at	O
time	O
t	O
=	O
2	O
,	O
the	O
model	B
is	O
trained	O
to	O
maximize	O
the	O
conditional	O
probability	O
of	O
y	O
(	O
2	O
)	O
given	O
both	O
the	O
x	O
sequence	O
so	O
far	O
and	O
the	O
previous	O
y	O
value	O
from	O
the	O
training	O
set	O
.	O
maximum	O
likelihood	O
thus	O
speciﬁes	O
that	O
during	O
training	O
,	O
rather	O
than	O
feeding	O
the	O
model	B
’	O
s	O
own	O
output	O
back	O
into	O
itself	O
,	O
these	O
connections	O
should	O
be	O
fed	O
with	O
the	O
target	O
values	O
specifying	O
what	O
the	O
correct	O
output	O
should	O
be	O
.	O
this	O
is	O
illustrated	O
in	O
ﬁgure	O
.	O
10.6	O
we	O
originally	O
motivated	O
teacher	O
forcing	O
as	O
allowing	O
us	O
to	O
avoid	O
back-propagation	O
through	O
time	O
in	O
models	O
that	O
lack	O
hidden-to-hidden	O
connections	O
.	O
teacher	O
forcing	O
may	O
still	O
be	O
applied	O
to	O
models	O
that	O
have	O
hidden-to-hidden	O
connections	O
so	O
long	O
as	O
they	O
have	O
connections	O
from	O
the	O
output	O
at	O
one	O
time	O
step	O
to	O
values	O
computed	O
in	O
the	O
next	O
time	O
step	O
.	O
however	O
,	O
as	O
soon	O
as	O
the	O
hidden	O
units	O
become	O
a	O
function	O
of	O
earlier	O
time	O
steps	O
,	O
the	O
bptt	O
algorithm	O
is	O
necessary	O
.	O
some	O
models	O
may	O
thus	O
be	O
trained	O
with	O
both	O
teacher	O
forcing	O
and	O
bptt	O
.	O
the	O
disadvantage	O
of	O
strict	O
teacher	O
forcing	O
arises	O
if	O
the	O
network	O
is	O
going	O
to	O
be	O
later	O
used	O
in	O
an	O
open-loop	O
mode	O
,	O
with	O
the	O
network	O
outputs	O
(	O
or	O
samples	O
from	O
the	O
output	O
distribution	O
)	O
fed	O
back	O
as	O
input	O
.	O
in	O
this	O
case	O
,	O
the	O
kind	O
of	O
inputs	O
that	O
the	O
network	O
sees	O
during	O
training	O
could	O
be	O
quite	O
diﬀerent	O
from	O
the	O
kind	O
of	O
inputs	O
that	O
it	O
will	O
see	O
at	O
test	O
time	O
.	O
one	O
way	O
to	O
mitigate	O
this	O
problem	O
is	O
to	O
train	O
with	O
both	O
teacher-forced	O
inputs	O
and	O
with	O
free-running	O
inputs	O
,	O
for	O
example	O
by	O
predicting	O
the	O
correct	O
target	O
a	O
number	O
of	O
steps	O
in	O
the	O
future	O
through	O
the	O
unfolded	O
recurrent	O
output-to-input	O
paths	O
.	O
in	O
this	O
way	O
,	O
the	O
network	O
can	O
learn	O
to	O
take	O
into	O
account	O
input	O
conditions	B
(	O
such	O
as	O
those	O
it	O
generates	O
itself	O
in	O
the	O
free-running	O
mode	O
)	O
not	O
seen	O
during	O
training	O
and	O
how	O
to	O
map	O
the	O
state	O
back	O
towards	O
one	O
that	O
will	O
make	O
the	O
network	O
generate	O
proper	O
outputs	O
after	O
a	O
few	O
steps	O
.	O
another	O
approach	O
(	O
bengio	O
et	O
al.	O
,	O
)	O
to	O
mitigate	O
the	O
gap	O
between	O
the	O
inputs	O
seen	O
at	O
train	O
time	O
and	O
the	O
inputs	O
seen	O
at	O
test	O
time	O
randomly	O
chooses	O
to	O
use	O
generated	O
values	O
or	O
actual	O
data	O
values	O
as	O
input	O
.	O
this	O
approach	O
exploits	O
a	O
curriculum	O
learning	O
strategy	O
to	O
gradually	O
use	O
more	O
of	O
the	O
generated	O
values	O
as	O
input	O
.	O
2015b	O
10.2.2	O
computing	O
the	O
gradient	O
in	O
a	O
recurrent	O
neural	O
network	O
computing	O
the	O
gradient	O
through	O
a	O
recurrent	O
neural	O
network	O
is	O
straightforward	O
.	O
one	O
simply	O
applies	O
the	O
generalized	O
back-propagation	O
algorithm	O
of	O
section	O
6.5.6	O
384	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
to	O
the	O
unrolled	O
computational	O
graph	O
.	O
no	O
specialized	O
algorithms	O
are	O
necessary	O
.	O
gradients	O
obtained	O
by	O
back-propagation	O
may	O
then	O
be	O
used	O
with	O
any	O
general-purpose	O
gradient-based	O
techniques	O
to	O
train	O
an	O
rnn	O
.	O
10.8	O
and	O
equation	O
to	O
gain	O
some	O
intuition	O
for	O
how	O
the	O
bptt	O
algorithm	O
behaves	O
,	O
we	O
provide	O
an	O
example	O
of	O
how	O
to	O
compute	O
gradients	O
by	O
bptt	O
for	O
the	O
rnn	O
equations	O
above	O
(	O
equation	O
)	O
.	O
the	O
nodes	O
of	O
our	O
computational	O
graph	O
include	O
the	O
parameters	O
u	O
,	O
v	O
,	O
w	O
,	O
b	O
and	O
c	O
as	O
well	O
as	O
the	O
sequence	O
of	O
nodes	O
indexed	O
by	O
∇	O
t	O
for	O
x	O
(	O
)	O
t	O
,	O
h	O
(	O
)	O
t	O
,	O
o	O
(	O
)	O
t	O
and	O
l	O
(	O
)	O
t	O
.	O
for	O
each	O
node	O
n	O
we	O
need	O
to	O
compute	O
the	O
gradient	O
l	O
recursively	O
,	O
based	O
on	O
the	O
gradient	O
computed	O
at	O
nodes	O
that	O
follow	O
it	O
in	O
the	O
graph	O
.	O
we	O
start	O
the	O
recursion	O
with	O
the	O
nodes	O
immediately	O
preceding	O
the	O
ﬁnal	O
loss	O
10.12	O
n	O
∂l	O
∂l	O
(	O
)	O
t	O
=	O
1	O
.	O
(	O
10.17	O
)	O
in	O
this	O
derivation	O
we	O
assume	O
that	O
the	O
outputs	O
o	O
(	O
)	O
t	O
are	O
used	O
as	O
the	O
argument	O
to	O
the	O
softmax	O
function	O
to	O
obtain	O
the	O
vector	O
ˆy	O
of	O
probabilities	O
over	O
the	O
output	O
.	O
we	O
also	O
assume	O
that	O
the	O
loss	O
is	O
the	O
negative	O
log-likelihood	O
of	O
the	O
true	O
target	O
y	O
(	O
)	O
t	O
given	O
the	O
o	O
(	O
)	O
t	O
l	O
on	O
the	O
outputs	O
at	O
time	O
step	O
t	O
,	O
for	O
all	O
i	O
,	O
t	O
,	O
is	O
as	O
input	O
so	O
far	O
.	O
the	O
gradient	O
follows	O
:	O
∇	O
∇	O
(	O
o	O
(	O
)	O
t	O
l	O
)	O
i	O
=	O
∂l	O
∂o	O
(	O
)	O
t	O
i	O
=	O
∂l	O
∂l	O
(	O
)	O
t	O
∂l	O
(	O
)	O
t	O
∂o	O
(	O
)	O
t	O
i	O
−	O
=	O
ˆy	O
(	O
)	O
t	O
i	O
1i	O
,	O
y	O
(	O
)	O
t	O
.	O
(	O
10.18	O
)	O
we	O
work	B
our	O
way	O
backwards	O
,	O
starting	O
from	O
the	O
end	O
of	O
the	O
sequence	O
.	O
at	O
the	O
ﬁnal	O
time	O
step	O
,	O
τ	O
h	O
(	O
)	O
τ	O
only	O
has	O
o	O
(	O
)	O
τ	O
as	O
a	O
descendent	O
,	O
so	O
its	O
gradient	O
is	O
simple	O
:	O
∇	O
h	O
(	O
)	O
τ	O
l	O
=	O
v	O
∇	O
o	O
(	O
)	O
τ	O
l.	O
(	O
10.19	O
)	O
we	O
can	O
then	O
iterate	O
backwards	O
in	O
time	O
to	O
back-propagate	O
gradients	O
through	O
time	O
,	O
1	O
down	O
to	O
t	O
=	O
1	O
,	O
noting	O
that	O
h	O
(	O
)	O
t	O
(	O
for	O
t	O
<	O
τ	O
)	O
has	O
as	O
descendents	O
both	O
from	O
t	O
=	O
τ	O
o	O
(	O
)	O
t	O
and	O
h	O
(	O
+1	O
)	O
.	O
its	O
gradient	O
is	O
thus	O
given	O
by	O
t	O
−	O
	O
∇	O
h	O
(	O
)	O
t	O
l	O
=	O
	O
	O
t	O
∂h	O
(	O
+1	O
)	O
∂h	O
(	O
)	O
t	O
∇	O
	O
(	O
	O
	O
	O
=	O
w	O
−	O
	O
	O
	O
	O
	O
	O
∇	O
(	O
∂o	O
(	O
)	O
t	O
∂h	O
(	O
)	O
t	O
	O
	O
∇	O
(	O
h	O
(	O
+1	O
)	O
t	O
l	O
)	O
+	O
−	O
t	O
l	O
)	O
diag	O
h	O
(	O
+1	O
)	O
1	O
2	O
t	O
h	O
(	O
+1	O
)	O
+	O
v	O
o	O
(	O
)	O
t	O
l	O
)	O
∇	O
(	O
	O
(	O
10.20	O
)	O
o	O
(	O
)	O
t	O
l	O
)	O
(	O
10.21	O
)	O
where	O
diag	O
−	O
1	O
t	O
(	O
h	O
(	O
+1	O
)	O
1	O
hidden	O
unit	O
at	O
time	O
i	O
i	O
h	O
(	O
+1	O
)	O
t	O
2	O
indicates	O
the	O
diagonal	O
matrix	O
containing	O
the	O
elements	O
)	O
2	O
.	O
this	O
is	O
the	O
jacobian	O
of	O
the	O
hyperbolic	O
tangent	O
associated	O
with	O
the	O
.	O
t	O
+	O
1	O
385	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
once	O
the	O
gradients	O
on	O
the	O
internal	O
nodes	O
of	O
the	O
computational	O
graph	O
are	O
obtained	O
,	O
we	O
can	O
obtain	O
the	O
gradients	O
on	O
the	O
parameter	O
nodes	O
.	O
because	O
the	O
parameters	O
are	O
shared	O
across	O
many	O
time	O
steps	O
,	O
we	O
must	O
take	O
some	O
care	O
when	O
denoting	O
calculus	O
operations	O
involving	O
these	O
variables	O
.	O
the	O
equations	O
we	O
wish	O
to	O
∇	O
,	O
that	O
computes	O
the	O
contribution	O
implement	O
use	O
the	O
bprop	O
method	O
of	O
section	O
w	O
f	O
of	O
a	O
single	O
edge	O
in	O
the	O
computational	O
graph	O
to	O
the	O
gradient	O
.	O
however	O
,	O
the	O
operator	O
used	O
in	O
calculus	O
takes	O
into	O
account	O
the	O
contribution	O
of	O
w	O
to	O
the	O
value	O
of	O
f	O
due	O
to	O
edges	O
in	O
the	O
computational	O
graph	O
.	O
to	O
resolve	O
this	O
ambiguity	O
,	O
we	O
introduce	O
dummy	O
variables	O
w	O
(	O
)	O
t	O
that	O
are	O
deﬁned	O
to	O
be	O
copies	O
of	O
w	O
but	O
with	O
each	O
w	O
(	O
)	O
t	O
used	O
only	O
at	O
time	O
step	O
t.	O
we	O
may	O
then	O
use	O
w	O
(	O
)	O
t	O
to	O
denote	O
the	O
contribution	O
of	O
the	O
weights	O
at	O
time	O
step	O
to	O
the	O
gradient	O
.	O
6.5.6	O
∇	O
all	O
t	O
using	O
this	O
notation	O
,	O
the	O
gradient	O
on	O
the	O
remaining	O
parameters	O
is	O
given	O
by	O
:	O
	O
	O
	O
∇	O
t	O
t	O
	O
	O
	O
	O
	O
	O
	O
	O
i	O
	O
i	O
diag	O
∂o	O
(	O
)	O
t	O
∂c	O
∂h	O
(	O
)	O
t	O
∂b	O
(	O
)	O
t	O
	O
	O
	O
	O
	O
t	O
	O
	O
	O
	O
	O
	O
	O
t	O
t	O
t	O
t	O
	O
	O
	O
	O
	O
∇	O
o	O
(	O
)	O
t	O
l	O
=	O
∇	O
h	O
(	O
)	O
t	O
l	O
=	O
∇	O
∂l	O
∂o	O
(	O
)	O
t	O
i	O
∂l	O
∂h	O
(	O
)	O
t	O
−	O
i	O
1	O
∂l	O
∂h	O
(	O
)	O
t	O
−	O
i	O
1	O
i	O
=	O
v	O
o	O
(	O
)	O
t	O
∇	O
	O
	O
i	O
w	O
(	O
)	O
t	O
h	O
(	O
)	O
t	O
∇	O
(	O
2	O
	O
	O
i	O
u	O
(	O
)	O
t	O
h	O
(	O
)	O
t	O
∇	O
(	O
2	O
h	O
(	O
)	O
t	O
∇	O
h	O
(	O
)	O
t	O
=	O
i	O
diag	O
t	O
t	O
∇	O
∇	O
cl	O
=	O
bl	O
=	O
∇	O
v	O
l	O
=	O
∇	O
wl	O
=	O
=	O
∇	O
ul	O
=	O
−	O
	O
1	O
)	O
t	O
h	O
(	O
)	O
t	O
l	O
)	O
h	O
(	O
	O
h	O
(	O
)	O
t	O
l	O
)	O
x	O
(	O
)	O
t	O
	O
o	O
(	O
)	O
t	O
l	O
	O
	O
	O
−	O
1	O
2	O
h	O
(	O
)	O
t	O
∇	O
h	O
(	O
)	O
t	O
l	O
(	O
10.23	O
)	O
diag	O
∇	O
o	O
(	O
)	O
t	O
l	O
)	O
h	O
(	O
)	O
t	O
(	O
	O
t	O
(	O
10.22	O
)	O
(	O
10.24	O
)	O
(	O
10.25	O
)	O
(	O
10.26	O
)	O
(	O
10.27	O
)	O
(	O
10.28	O
)	O
we	O
do	O
not	O
need	O
to	O
compute	O
the	O
gradient	O
with	O
respect	O
to	O
x	O
(	O
)	O
t	O
for	O
training	O
because	O
it	O
does	O
not	O
have	O
any	O
parameters	O
as	O
ancestors	O
in	O
the	O
computational	O
graph	O
deﬁning	O
the	O
loss	O
.	O
386	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
10.2.3	O
recurrent	O
networks	O
as	O
directed	O
graphical	O
models	O
in	O
the	O
example	O
recurrent	O
network	O
we	O
have	O
developed	O
so	O
far	O
,	O
the	O
losses	O
l	O
(	O
)	O
t	O
were	O
cross-entropies	O
between	O
training	O
targets	O
y	O
(	O
)	O
t	O
and	O
outputs	O
o	O
(	O
)	O
t	O
.	O
as	O
with	O
a	O
feedforward	O
network	O
,	O
it	O
is	O
in	O
principle	O
possible	O
to	O
use	O
almost	O
any	O
loss	O
with	O
a	O
recurrent	O
network	O
.	O
the	O
loss	O
should	O
be	O
chosen	O
based	O
on	O
the	O
task	O
.	O
as	O
with	O
a	O
feedforward	O
network	O
,	O
we	O
usually	O
wish	O
to	O
interpret	O
the	O
output	O
of	O
the	O
rnn	O
as	O
a	O
probability	O
distribution	O
,	O
and	O
we	O
usually	O
use	O
the	O
cross-entropy	O
associated	O
with	O
that	O
distribution	O
to	O
deﬁne	O
the	O
loss	O
.	O
mean	O
squared	O
error	O
is	O
the	O
cross-entropy	O
loss	O
associated	O
with	O
an	O
output	O
distribution	O
that	O
is	O
a	O
unit	O
gaussian	O
,	O
for	O
example	O
,	O
just	O
as	O
with	O
a	O
feedforward	O
network	O
.	O
when	O
we	O
use	O
a	O
predictive	O
log-likelihood	O
training	O
objective	O
,	O
such	O
as	O
equa-	O
tion	B
10.12	O
,	O
we	O
train	O
the	O
rnn	O
to	O
estimate	O
the	O
conditional	O
distribution	O
of	O
the	O
next	O
sequence	O
element	O
y	O
(	O
)	O
t	O
given	O
the	O
past	O
inputs	O
.	O
this	O
may	O
mean	O
that	O
we	O
maximize	O
the	O
log-likelihood	O
|	O
log	O
(	O
p	O
y	O
(	O
)	O
t	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
t	O
)	O
,	O
(	O
10.29	O
)	O
or	O
,	O
if	O
the	O
model	B
includes	O
connections	O
from	O
the	O
output	O
at	O
one	O
time	O
step	O
to	O
the	O
next	O
time	O
step	O
,	O
|	O
−	O
1	O
)	O
t	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
t	O
,	O
y	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
y	O
(	O
log	O
(	O
p	O
y	O
(	O
)	O
t	O
)	O
.	O
(	O
10.30	O
)	O
in	O
the	O
past	O
to	O
the	O
current	O
y	O
(	O
)	O
t	O
.	O
decomposing	O
the	O
joint	O
probability	O
over	O
the	O
sequence	O
of	O
y	O
values	O
as	O
a	O
series	O
of	O
one-step	O
probabilistic	O
predictions	O
is	O
one	O
way	O
to	O
capture	O
the	O
full	O
joint	O
distribution	O
across	O
the	O
whole	O
sequence	O
.	O
when	O
we	O
do	O
not	O
feed	O
past	O
y	O
values	O
as	O
inputs	O
that	O
condition	O
the	O
next	O
step	O
prediction	O
,	O
the	O
directed	O
graphical	O
model	B
contains	O
no	O
edges	O
from	O
any	O
y	O
(	O
)	O
i	O
in	O
this	O
case	O
,	O
the	O
outputs	O
y	O
are	O
conditionally	O
independent	O
given	O
the	O
sequence	O
of	O
x	O
values	O
.	O
when	O
we	O
do	O
feed	O
the	O
actual	O
y	O
values	O
(	O
not	O
their	O
prediction	O
,	O
but	O
the	O
actual	O
observed	O
or	O
generated	O
values	O
)	O
back	O
into	O
the	O
network	O
,	O
the	O
directed	O
graphical	O
model	B
contains	O
edges	O
from	O
all	O
y	O
(	O
)	O
i	O
values	O
in	O
the	O
past	O
to	O
the	O
current	O
y	O
(	O
)	O
t	O
value	O
.	O
{	O
y	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
y	O
(	O
)	O
τ	O
as	O
a	O
simple	O
example	O
,	O
let	O
us	O
consider	O
the	O
case	O
where	O
the	O
rnn	O
models	O
only	O
a	O
sequence	O
of	O
scalar	O
random	O
variables	O
y	O
=	O
,	O
with	O
no	O
additional	O
inputs	O
x.	O
the	O
input	O
at	O
time	O
step	O
t	O
is	O
simply	O
the	O
output	O
at	O
time	O
step	O
t	O
1.	O
the	O
rnn	O
then	O
deﬁnes	O
a	O
directed	O
graphical	O
model	B
over	O
the	O
y	O
variables	O
.	O
we	O
parametrize	O
the	O
joint	O
distribution	O
of	O
these	O
observations	O
using	O
the	O
chain	O
rule	O
(	O
equation	O
)	O
for	O
conditional	O
probabilities	O
:	O
	O
−	O
3.6	O
}	O
p	O
(	O
)	O
=	O
y	O
p	O
(	O
y	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
y	O
(	O
)	O
τ	O
)	O
=	O
p	O
(	O
y	O
(	O
)	O
t	O
τ	O
|	O
−	O
1	O
)	O
t	O
y	O
(	O
−	O
2	O
)	O
t	O
,	O
y	O
(	O
,	O
.	O
.	O
.	O
,	O
y	O
(	O
1	O
)	O
)	O
(	O
10.31	O
)	O
t=1	O
where	O
the	O
right-hand	O
side	O
of	O
the	O
bar	O
is	O
empty	O
for	O
t	O
=	O
1	O
,	O
of	O
course	O
.	O
hence	O
the	O
according	O
to	O
such	O
a	O
model	B
negative	O
log-likelihood	O
of	O
a	O
set	O
of	O
values	O
{	O
y	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
y	O
(	O
)	O
τ	O
}	O
387	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
y	O
(	O
1	O
)	O
y	O
(	O
1	O
)	O
y	O
(	O
2	O
)	O
y	O
(	O
2	O
)	O
y	O
(	O
3	O
)	O
y	O
(	O
3	O
)	O
y	O
(	O
4	O
)	O
y	O
(	O
4	O
)	O
y	O
(	O
5	O
)	O
y	O
(	O
5	O
)	O
)	O
...	O
y	O
(	O
y	O
(	O
)	O
...	O
figure	O
10.7	O
:	O
fully	O
connected	O
graphical	O
model	B
for	O
a	O
sequence	O
y	O
(	O
1	O
)	O
,	O
y	O
(	O
2	O
)	O
,	O
.	O
.	O
.	O
,	O
y	O
(	O
)	O
t	O
,	O
.	O
.	O
.	O
:	O
every	O
past	O
observation	O
y	O
(	O
)	O
i	O
may	O
inﬂuence	O
the	O
conditional	O
distribution	O
of	O
some	O
y	O
(	O
)	O
t	O
(	O
for	O
t	O
>	O
i	O
)	O
,	O
given	O
the	O
previous	O
values	O
.	O
parametrizing	O
the	O
graphical	O
model	B
directly	O
according	O
to	O
this	O
graph	O
(	O
as	O
in	O
equation	O
)	O
might	O
be	O
very	O
ineﬃcient	O
,	O
with	O
an	O
ever	O
growing	O
number	O
of	O
inputs	O
and	O
parameters	O
for	O
each	O
element	O
of	O
the	O
sequence	O
.	O
rnns	O
obtain	O
the	O
same	O
full	O
connectivity	O
but	O
eﬃcient	O
parametrization	O
,	O
as	O
illustrated	O
in	O
ﬁgure	O
.	O
10.8	O
10.6	O
	O
is	O
where	O
l	O
=	O
l	O
(	O
)	O
t	O
t	O
(	O
10.32	O
)	O
−	O
l	O
(	O
)	O
t	O
=	O
log	O
(	O
p	O
y	O
(	O
)	O
t	O
=	O
y	O
(	O
)	O
t	O
|	O
−	O
1	O
)	O
t	O
y	O
(	O
−	O
2	O
)	O
t	O
,	O
y	O
(	O
,	O
.	O
.	O
.	O
,	O
y	O
(	O
1	O
)	O
)	O
.	O
(	O
10.33	O
)	O
h	O
(	O
1	O
)	O
h	O
(	O
1	O
)	O
h	O
(	O
2	O
)	O
h	O
(	O
2	O
)	O
h	O
(	O
3	O
)	O
h	O
(	O
3	O
)	O
h	O
(	O
4	O
)	O
h	O
(	O
4	O
)	O
h	O
(	O
5	O
)	O
h	O
(	O
5	O
)	O
)	O
...	O
h	O
(	O
h	O
(	O
)	O
...	O
y	O
(	O
1	O
)	O
y	O
(	O
1	O
)	O
y	O
(	O
2	O
)	O
y	O
(	O
2	O
)	O
y	O
(	O
3	O
)	O
y	O
(	O
3	O
)	O
y	O
(	O
4	O
)	O
y	O
(	O
4	O
)	O
y	O
(	O
5	O
)	O
y	O
(	O
5	O
)	O
)	O
...	O
y	O
(	O
y	O
(	O
)	O
...	O
figure	O
10.8	O
:	O
introducing	O
the	O
state	O
variable	O
in	O
the	O
graphical	O
model	B
of	O
the	O
rnn	O
,	O
even	O
though	O
it	O
is	O
a	O
deterministic	O
function	O
of	O
its	O
inputs	O
,	O
helps	O
to	O
see	O
how	O
we	O
can	O
obtain	O
a	O
very	O
eﬃcient	O
parametrization	O
,	O
based	O
on	O
equation	O
h	O
(	O
)	O
t	O
and	O
y	O
(	O
)	O
t	O
)	O
involves	O
the	O
same	O
structure	O
(	O
the	O
same	O
number	O
of	O
inputs	O
for	O
each	O
node	O
)	O
and	O
can	O
share	O
the	O
same	O
parameters	O
with	O
the	O
other	O
stages	O
.	O
.	O
every	O
stage	O
in	O
the	O
sequence	O
(	O
for	O
10.5	O
the	O
edges	O
in	O
a	O
graphical	O
model	B
indicate	O
which	O
variables	O
depend	O
directly	O
on	O
other	O
variables	O
.	O
many	O
graphical	O
models	O
aim	O
to	O
achieve	O
statistical	O
and	O
computational	O
eﬃciency	O
by	O
omitting	O
edges	O
that	O
do	O
not	O
correspond	O
to	O
strong	O
interactions	O
.	O
for	O
388	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
{	O
y	O
(	O
−	O
}	O
1	O
)	O
−	O
t	O
k	O
)	O
example	O
,	O
it	O
is	O
common	O
to	O
make	O
the	O
markov	O
assumption	O
that	O
the	O
graphical	O
model	B
to	O
y	O
(	O
)	O
t	O
,	O
rather	O
than	O
containing	O
should	O
only	O
contain	O
edges	O
from	O
edges	O
from	O
the	O
entire	O
past	O
history	O
.	O
however	O
,	O
in	O
some	O
cases	O
,	O
we	O
believe	O
that	O
all	O
past	O
inputs	O
should	O
have	O
an	O
inﬂuence	O
on	O
the	O
next	O
element	O
of	O
the	O
sequence	O
.	O
rnns	O
are	O
useful	O
when	O
we	O
believe	O
that	O
the	O
distribution	O
over	O
y	O
(	O
)	O
t	O
may	O
depend	O
on	O
a	O
value	O
of	O
y	O
(	O
)	O
i	O
−	O
from	O
the	O
distant	O
past	O
in	O
a	O
way	O
that	O
is	O
not	O
captured	O
by	O
the	O
eﬀect	O
of	O
y	O
(	O
)	O
i	O
on	O
y	O
(	O
1	O
)	O
t	O
.	O
,	O
.	O
.	O
.	O
,	O
y	O
(	O
t	O
one	O
way	O
to	O
interpret	O
an	O
rnn	O
as	O
a	O
graphical	O
model	B
is	O
to	O
view	O
the	O
rnn	O
as	O
deﬁning	O
a	O
graphical	O
model	B
whose	O
structure	O
is	O
the	O
complete	O
graph	O
,	O
able	O
to	O
represent	O
direct	O
dependencies	O
between	O
any	O
pair	O
of	O
y	O
values	O
.	O
the	O
graphical	O
model	B
over	O
the	O
y	O
values	O
with	O
the	O
complete	O
graph	O
structure	O
is	O
shown	O
in	O
ﬁgure	O
.	O
the	O
complete	O
graph	O
interpretation	O
of	O
the	O
rnn	O
is	O
based	O
on	O
ignoring	O
the	O
hidden	O
units	O
h	O
(	O
)	O
t	O
by	O
marginalizing	O
them	O
out	O
of	O
the	O
model	B
.	O
10.7	O
it	O
is	O
more	O
interesting	O
to	O
consider	O
the	O
graphical	O
model	B
structure	O
of	O
rnns	O
that	O
results	O
from	O
regarding	O
the	O
hidden	O
units	O
h	O
(	O
)	O
t	O
as	O
random	O
variables.1	O
including	O
the	O
hidden	O
units	O
in	O
the	O
graphical	O
model	B
reveals	O
that	O
the	O
rnn	O
provides	O
a	O
very	O
eﬃcient	O
parametrization	O
of	O
the	O
joint	O
distribution	O
over	O
the	O
observations	O
.	O
suppose	O
that	O
we	O
represented	O
an	O
arbitrary	O
joint	O
distribution	O
over	O
discrete	O
values	O
with	O
a	O
tabular	O
representation—an	O
array	O
containing	O
a	O
separate	O
entry	O
for	O
each	O
possible	O
assignment	O
of	O
values	O
,	O
with	O
the	O
value	O
of	O
that	O
entry	O
giving	O
the	O
probability	O
of	O
that	O
assignment	O
occurring	O
.	O
if	O
y	O
can	O
take	O
on	O
k	O
diﬀerent	O
values	O
,	O
the	O
tabular	O
representation	O
would	O
have	O
o	O
(	O
k	O
τ	O
)	O
parameters	O
.	O
by	O
comparison	O
,	O
due	O
to	O
parameter	O
sharing	O
,	O
the	O
number	O
of	O
parameters	O
in	O
the	O
rnn	O
is	O
o	O
(	O
1	O
)	O
as	O
a	O
function	O
of	O
sequence	O
length	O
.	O
the	O
number	O
of	O
parameters	O
in	O
the	O
rnn	O
may	O
be	O
adjusted	O
to	O
control	O
model	B
capacity	O
but	O
is	O
not	O
forced	O
to	O
scale	O
with	O
sequence	O
length	O
.	O
equation	O
shows	O
that	O
the	O
rnn	O
parametrizes	O
long-term	O
relationships	O
between	O
variables	O
eﬃciently	O
,	O
using	O
recurrent	O
applications	O
of	O
the	O
same	O
function	O
f	O
and	O
same	O
parameters	O
θ	O
at	O
each	O
time	O
step	O
.	O
figure	O
10.8	O
illustrates	O
the	O
graphical	O
model	B
interpretation	O
.	O
incorporating	O
the	O
h	O
(	O
)	O
t	O
nodes	O
in	O
the	O
graphical	O
model	B
decouples	O
the	O
past	O
and	O
the	O
future	O
,	O
acting	O
as	O
an	O
intermediate	O
quantity	O
between	O
them	O
.	O
a	O
variable	O
y	O
(	O
)	O
i	O
in	O
the	O
distant	O
past	O
may	O
inﬂuence	O
a	O
variable	O
y	O
(	O
)	O
t	O
via	O
its	O
eﬀect	O
on	O
h.	O
the	O
structure	O
of	O
this	O
graph	O
shows	O
that	O
the	O
model	B
can	O
be	O
eﬃciently	O
parametrized	O
by	O
using	O
the	O
same	O
conditional	O
probability	O
distributions	O
at	O
each	O
time	O
step	O
,	O
and	O
that	O
when	O
the	O
variables	O
are	O
all	O
observed	O
,	O
the	O
probability	O
of	O
the	O
joint	O
assignment	O
of	O
all	O
variables	O
can	O
be	O
evaluated	O
eﬃciently	O
.	O
10.5	O
even	O
with	O
the	O
eﬃcient	O
parametrization	O
of	O
the	O
graphical	O
model	B
,	O
some	O
operations	O
remain	O
computationally	O
challenging	O
.	O
for	O
example	O
,	O
it	O
is	O
diﬃcult	O
to	O
predict	O
missing	O
1the	O
conditional	O
distribution	O
over	O
these	O
variables	O
given	O
their	O
parents	O
is	O
deterministic	O
.	O
this	O
is	O
perfectly	O
legitimate	O
,	O
though	O
it	O
is	O
somewhat	O
rare	O
to	O
design	O
a	O
graphical	O
model	B
with	O
such	O
deterministic	O
hidden	O
units	O
.	O
389	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
values	O
in	O
the	O
middle	O
of	O
the	O
sequence	O
.	O
the	O
price	O
recurrent	O
networks	O
pay	O
for	O
their	O
reduced	O
number	O
of	O
parameters	O
is	O
that	O
optimizing	O
the	O
parameters	O
may	O
be	O
diﬃcult	O
.	O
the	O
parameter	O
sharing	O
used	O
in	O
recurrent	O
networks	O
relies	O
on	O
the	O
assumption	O
that	O
the	O
same	O
parameters	O
can	O
be	O
used	O
for	O
diﬀerent	O
time	O
steps	O
.	O
equivalently	O
,	O
the	O
assumption	O
is	O
that	O
the	O
conditional	O
probability	O
distribution	O
over	O
the	O
variables	O
at	O
time	O
t+	O
1	O
given	O
the	O
variables	O
at	O
time	O
t	O
is	O
stationary	O
,	O
meaning	O
that	O
the	O
relationship	O
between	O
the	O
previous	O
time	O
step	O
and	O
the	O
next	O
time	O
step	O
does	O
not	O
depend	O
on	O
t.	O
in	O
principle	O
,	O
it	O
would	O
be	O
possible	O
to	O
use	O
t	O
as	O
an	O
extra	O
input	O
at	O
each	O
time	O
step	O
and	O
let	O
the	O
learner	O
discover	O
any	O
time-dependence	O
while	O
sharing	O
as	O
much	O
as	O
it	O
can	O
between	O
diﬀerent	O
time	O
steps	O
.	O
this	O
would	O
already	O
be	O
much	O
better	O
than	O
using	O
a	O
diﬀerent	O
conditional	O
probability	O
distribution	O
for	O
each	O
t	O
,	O
but	O
the	O
network	O
would	O
then	O
have	O
to	O
extrapolate	O
when	O
faced	O
with	O
new	O
values	O
of	O
.t	O
to	O
complete	O
our	O
view	O
of	O
an	O
rnn	O
as	O
a	O
graphical	O
model	B
,	O
we	O
must	O
describe	O
how	O
to	O
draw	O
samples	O
from	O
the	O
model	B
.	O
the	O
main	O
operation	O
that	O
we	O
need	O
to	O
perform	O
is	O
simply	O
to	O
sample	O
from	O
the	O
conditional	O
distribution	O
at	O
each	O
time	O
step	O
.	O
however	O
,	O
there	O
is	O
one	O
additional	O
complication	O
.	O
the	O
rnn	O
must	O
have	O
some	O
mechanism	O
for	O
determining	O
the	O
length	O
of	O
the	O
sequence	O
.	O
this	O
can	O
be	O
achieved	O
in	O
various	O
ways	O
.	O
in	O
the	O
case	O
when	O
the	O
output	O
is	O
a	O
symbol	O
taken	O
from	O
a	O
vocabulary	O
,	O
one	O
can	O
add	O
a	O
special	O
symbol	O
corresponding	O
to	O
the	O
end	O
of	O
a	O
sequence	O
(	O
schmidhuber	O
2012	O
)	O
.	O
when	O
that	O
symbol	O
is	O
generated	O
,	O
the	O
sampling	O
process	O
stops	O
.	O
in	O
the	O
training	O
set	O
,	O
we	O
insert	O
this	O
symbol	O
as	O
an	O
extra	O
member	O
of	O
the	O
sequence	O
,	O
immediately	O
after	O
x	O
(	O
)	O
τ	O
in	O
each	O
training	O
example	O
.	O
,	O
another	O
option	O
is	O
to	O
introduce	O
an	O
extra	O
bernoulli	O
output	O
to	O
the	O
model	B
that	O
represents	O
the	O
decision	O
to	O
either	O
continue	O
generation	O
or	O
halt	O
generation	O
at	O
each	O
time	O
step	O
.	O
this	O
approach	O
is	O
more	O
general	O
than	O
the	O
approach	O
of	O
adding	O
an	O
extra	O
symbol	O
to	O
the	O
vocabulary	O
,	O
because	O
it	O
may	O
be	O
applied	O
to	O
any	O
rnn	O
,	O
rather	O
than	O
only	O
rnns	O
that	O
output	O
a	O
sequence	O
of	O
symbols	O
.	O
for	O
example	O
,	O
it	O
may	O
be	O
applied	O
to	O
an	O
rnn	O
that	O
emits	O
a	O
sequence	O
of	O
real	O
numbers	O
.	O
the	O
new	O
output	O
unit	O
is	O
usually	O
a	O
sigmoid	O
unit	O
trained	O
with	O
the	O
cross-entropy	O
loss	O
.	O
in	O
this	O
approach	O
the	O
sigmoid	O
is	O
trained	O
to	O
maximize	O
the	O
log-probability	O
of	O
the	O
correct	O
prediction	O
as	O
to	O
whether	O
the	O
sequence	O
ends	O
or	O
continues	O
at	O
each	O
time	O
step	O
.	O
another	O
way	O
to	O
determine	O
the	O
sequence	O
length	O
τ	O
is	O
to	O
add	O
an	O
extra	O
output	O
to	O
the	O
model	B
that	O
predicts	O
the	O
integer	O
τ	O
itself	O
.	O
the	O
model	B
can	O
sample	O
a	O
value	O
of	O
τ	O
and	O
then	O
sample	O
τ	O
steps	O
worth	O
of	O
data	O
.	O
this	O
approach	O
requires	O
adding	O
an	O
extra	O
input	O
to	O
the	O
recurrent	O
update	O
at	O
each	O
time	O
step	O
so	O
that	O
the	O
recurrent	O
update	O
is	O
aware	O
of	O
whether	O
it	O
is	O
near	O
the	O
end	O
of	O
the	O
generated	O
sequence	O
.	O
this	O
extra	O
input	O
can	O
either	O
consist	O
of	O
the	O
value	O
of	O
τ	O
or	O
can	O
consist	O
of	O
τ	O
,	O
the	O
number	O
of	O
remaining	O
t	O
−	O
390	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
time	O
steps	O
.	O
without	O
this	O
extra	O
input	O
,	O
the	O
rnn	O
might	O
generate	O
sequences	O
that	O
end	O
abruptly	O
,	O
such	O
as	O
a	O
sentence	O
that	O
ends	O
before	O
it	O
is	O
complete	O
.	O
this	O
approach	O
is	O
based	O
on	O
the	O
decomposition	O
p	O
(	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
τ	O
)	O
=	O
(	O
)	O
(	O
p	O
τ	O
p	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
τ	O
τ	O
.	O
)	O
(	O
10.34	O
)	O
|	O
the	O
strategy	O
of	O
predicting	O
τ	O
directly	O
is	O
used	O
for	O
example	O
by	O
goodfellow	O
et	O
al	O
.	O
(	O
2014d	O
)	O
.	O
10.2.4	O
modeling	O
sequences	O
conditioned	O
on	O
context	O
with	O
rnns	O
10.8	O
in	O
the	O
previous	O
section	O
we	O
described	O
how	O
an	O
rnn	O
could	O
correspond	O
to	O
a	O
directed	O
graphical	O
model	B
over	O
a	O
sequence	O
of	O
random	O
variables	O
y	O
(	O
)	O
t	O
with	O
no	O
inputs	O
x.	O
of	O
course	O
,	O
our	O
development	O
of	O
rnns	O
as	O
in	O
equation	O
included	O
a	O
sequence	O
of	O
inputs	O
x	O
(	O
1	O
)	O
,	O
x	O
(	O
2	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
τ	O
.	O
in	O
general	O
,	O
rnns	O
allow	O
the	O
extension	O
of	O
the	O
graphical	O
model	B
view	O
to	O
represent	O
not	O
only	O
a	O
joint	O
distribution	O
over	O
the	O
y	O
variables	O
but	O
also	O
a	O
conditional	O
distribution	O
over	O
y	O
given	O
x.	O
as	O
discussed	O
in	O
the	O
context	O
of	O
|	O
p	O
(	O
y	O
;	O
θ	O
)	O
feedforward	O
networks	O
in	O
section	O
|	O
can	O
be	O
reinterpreted	O
as	O
a	O
model	B
representing	O
a	O
conditional	O
distribution	O
p	O
(	O
y	O
ω	O
)	O
with	O
ω	O
=	O
θ.	O
we	O
can	O
extend	O
such	O
a	O
model	B
to	O
represent	O
a	O
distribution	O
p	O
(	O
y	O
x	O
)	O
by	O
using	O
the	O
same	O
p	O
(	O
y	O
ω	O
)	O
as	O
before	O
,	O
but	O
making	O
ω	O
a	O
function	O
of	O
x.	O
in	O
the	O
case	O
of	O
an	O
rnn	O
,	O
this	O
can	O
be	O
achieved	O
in	O
diﬀerent	O
ways	O
.	O
we	O
review	O
here	O
the	O
most	O
common	O
and	O
obvious	O
choices	O
.	O
,	O
any	O
model	B
representing	O
a	O
variable	O
6.2.1.1	O
|	O
previously	O
,	O
we	O
have	O
discussed	O
rnns	O
that	O
take	O
a	O
sequence	O
of	O
vectors	O
x	O
(	O
)	O
t	O
for	O
t	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
τ	O
as	O
input	O
.	O
another	O
option	O
is	O
to	O
take	O
only	O
a	O
single	O
vector	O
x	O
as	O
input	O
.	O
when	O
x	O
is	O
a	O
ﬁxed-size	O
vector	O
,	O
we	O
can	O
simply	O
make	O
it	O
an	O
extra	O
input	O
of	O
the	O
rnn	O
that	O
generates	O
the	O
y	O
sequence	O
.	O
some	O
common	O
ways	O
of	O
providing	O
an	O
extra	O
input	O
to	O
an	O
rnn	O
are	O
:	O
1.	O
as	O
an	O
extra	O
input	O
at	O
each	O
time	O
step	O
,	O
or	O
2.	O
as	O
the	O
initial	O
state	O
h	O
(	O
0	O
)	O
,	O
or	O
3.	O
both	O
.	O
.	O
the	O
interaction	O
the	O
ﬁrst	O
and	O
most	O
common	O
approach	O
is	O
illustrated	O
in	O
ﬁgure	O
between	O
the	O
input	O
x	O
and	O
each	O
hidden	O
unit	O
vector	O
h	O
(	O
)	O
t	O
is	O
parametrized	O
by	O
a	O
newly	O
introduced	O
weight	O
matrix	O
r	O
that	O
was	O
absent	O
from	O
the	O
model	B
of	O
only	O
the	O
sequence	O
	O
r	O
is	O
added	O
as	O
additional	O
input	O
to	O
the	O
hidden	O
of	O
y	O
values	O
.	O
the	O
same	O
product	O
x	O
units	O
at	O
every	O
time	O
step	O
.	O
we	O
can	O
think	O
of	O
the	O
choice	O
of	O
x	O
as	O
determining	O
the	O
value	O
10.9	O
391	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
	O
r	O
that	O
is	O
eﬀectively	O
a	O
new	O
bias	O
parameter	O
used	O
for	O
each	O
of	O
the	O
hidden	O
units	O
.	O
of	O
x	O
the	O
weights	O
remain	O
independent	O
of	O
the	O
input	O
.	O
we	O
can	O
think	O
of	O
this	O
model	B
as	O
taking	O
the	O
parameters	O
θ	O
of	O
the	O
non-conditional	O
model	B
and	O
turning	O
them	O
into	O
ω	O
,	O
where	O
the	O
bias	O
parameters	O
within	O
are	O
now	O
a	O
function	O
of	O
the	O
input	O
.	O
ω	O
−	O
−	O
y	O
(	O
t	O
y	O
(	O
t	O
1	O
)	O
1	O
)	O
−	O
−	O
l	O
(	O
t	O
l	O
(	O
t	O
1	O
)	O
1	O
)	O
−	O
−	O
o	O
(	O
t	O
o	O
(	O
t	O
1	O
)	O
1	O
)	O
y	O
(	O
)	O
ty	O
(	O
)	O
t	O
t	O
y	O
(	O
+1	O
)	O
ty	O
(	O
+1	O
)	O
)	O
...	O
y	O
(	O
)	O
...	O
y	O
(	O
u	O
l	O
(	O
)	O
tl	O
(	O
)	O
t	O
u	O
l	O
(	O
+1	O
)	O
tl	O
(	O
+1	O
)	O
t	O
o	O
(	O
)	O
to	O
(	O
)	O
t	O
t	O
o	O
(	O
+1	O
)	O
to	O
(	O
+1	O
)	O
v	O
v	O
v	O
−	O
−	O
h	O
(	O
t	O
h	O
(	O
t	O
1	O
)	O
1	O
)	O
w	O
w	O
h	O
(	O
)	O
th	O
(	O
)	O
t	O
w	O
h	O
(	O
+1	O
)	O
th	O
(	O
+1	O
)	O
t	O
)	O
...	O
h	O
(	O
h	O
(	O
)	O
...	O
u	O
w	O
)	O
...	O
s	O
(	O
s	O
(	O
)	O
...	O
r	O
r	O
r	O
r	O
r	O
xx	O
figure	O
10.9	O
:	O
an	O
rnn	O
that	O
maps	O
a	O
ﬁxed-length	O
vector	O
x	O
into	O
a	O
distribution	O
over	O
sequences	O
y.	O
this	O
rnn	O
is	O
appropriate	O
for	O
tasks	O
such	O
as	O
image	O
captioning	O
,	O
where	O
a	O
single	O
image	O
is	O
used	O
as	O
input	O
to	O
a	O
model	B
that	O
then	O
produces	O
a	O
sequence	O
of	O
words	O
describing	O
the	O
image	O
.	O
each	O
element	O
y	O
(	O
)	O
t	O
of	O
the	O
observed	O
output	O
sequence	O
serves	O
both	O
as	O
input	O
(	O
for	O
the	O
current	O
time	O
step	O
)	O
and	O
,	O
during	O
training	O
,	O
as	O
target	O
(	O
for	O
the	O
previous	O
time	O
step	O
)	O
.	O
rather	O
than	O
receiving	O
only	O
a	O
single	O
vector	O
x	O
as	O
input	O
,	O
the	O
rnn	O
may	O
receive	O
corre-	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
τ	O
)	O
that	O
makes	O
a	O
a	O
sequence	O
of	O
vectors	O
x	O
(	O
)	O
t	O
as	O
input	O
.	O
the	O
rnn	O
described	O
in	O
equation	O
sponds	O
to	O
a	O
conditional	O
distribution	O
p	O
(	O
y	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
y	O
(	O
)	O
τ	O
conditional	O
independence	O
assumption	O
that	O
this	O
distribution	O
factorizes	O
as	O
10.8	O
|	O
	O
|	O
p	O
(	O
y	O
(	O
)	O
t	O
t	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
t	O
)	O
.	O
(	O
10.35	O
)	O
to	O
remove	O
the	O
conditional	O
independence	O
assumption	O
,	O
we	O
can	O
add	O
connections	O
from	O
the	O
output	O
at	O
time	O
t	O
to	O
the	O
hidden	O
unit	O
at	O
time	O
t	O
+	O
1	O
,	O
as	O
shown	O
in	O
ﬁgure	O
.	O
the	O
model	B
can	O
then	O
represent	O
arbitrary	O
probability	O
distributions	O
over	O
the	O
y	O
sequence	O
.	O
this	O
kind	O
of	O
model	B
representing	O
a	O
distribution	O
over	O
a	O
sequence	O
given	O
another	O
10.10	O
392	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
−	O
−	O
y	O
(	O
t	O
y	O
(	O
t	O
1	O
)	O
1	O
)	O
−	O
−	O
l	O
(	O
t	O
l	O
(	O
t	O
1	O
)	O
1	O
)	O
−	O
−	O
o	O
(	O
t	O
o	O
(	O
t	O
1	O
)	O
1	O
)	O
v	O
w	O
)	O
...	O
h	O
(	O
h	O
(	O
)	O
...	O
−	O
−	O
h	O
(	O
t	O
h	O
(	O
t	O
1	O
)	O
1	O
)	O
u	O
−	O
−	O
1	O
)	O
1	O
)	O
x	O
(	O
t	O
x	O
(	O
t	O
y	O
(	O
)	O
ty	O
(	O
)	O
t	O
y	O
(	O
+1	O
)	O
ty	O
(	O
+1	O
)	O
t	O
l	O
(	O
)	O
tl	O
(	O
)	O
t	O
t	O
l	O
(	O
+1	O
)	O
tl	O
(	O
+1	O
)	O
r	O
r	O
r	O
w	O
o	O
(	O
)	O
to	O
(	O
)	O
t	O
v	O
h	O
(	O
)	O
th	O
(	O
)	O
t	O
u	O
x	O
(	O
)	O
tx	O
(	O
)	O
t	O
t	O
o	O
(	O
+1	O
)	O
to	O
(	O
+1	O
)	O
v	O
w	O
w	O
t	O
h	O
(	O
+1	O
)	O
th	O
(	O
+1	O
)	O
)	O
...	O
h	O
(	O
h	O
(	O
)	O
...	O
u	O
x	O
(	O
+1	O
)	O
tx	O
(	O
+1	O
)	O
t	O
10.3	O
figure	O
10.10	O
:	O
a	O
conditional	O
recurrent	O
neural	O
network	O
mapping	O
a	O
variable-length	O
sequence	O
of	O
x	O
values	O
into	O
a	O
distribution	O
over	O
sequences	O
of	O
y	O
values	O
of	O
the	O
same	O
length	O
.	O
compared	O
to	O
ﬁgure	O
,	O
this	O
rnn	O
contains	O
connections	O
from	O
the	O
previous	O
output	O
to	O
the	O
current	O
state	O
.	O
these	O
connections	O
allow	O
this	O
rnn	O
to	O
model	B
an	O
arbitrary	O
distribution	O
over	O
sequences	O
of	O
y	O
given	O
sequences	O
of	O
x	O
of	O
the	O
same	O
length	O
.	O
the	O
rnn	O
of	O
ﬁgure	O
is	O
only	O
able	O
to	O
represent	O
distributions	O
in	O
which	O
the	O
y	O
values	O
are	O
conditionally	O
independent	O
from	O
each	O
other	O
given	O
the	O
values	O
.	O
10.3	O
x	O
393	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
sequence	O
still	O
has	O
one	O
restriction	O
,	O
which	O
is	O
that	O
the	O
length	O
of	O
both	O
sequences	O
must	O
be	O
the	O
same	O
.	O
we	O
describe	O
how	O
to	O
remove	O
this	O
restriction	O
in	O
section	O
10.4	O
.	O
−	O
−	O
y	O
(	O
t	O
y	O
(	O
t	O
1	O
)	O
1	O
)	O
−	O
−	O
l	O
(	O
t	O
l	O
(	O
t	O
1	O
)	O
1	O
)	O
−	O
−	O
o	O
(	O
t	O
o	O
(	O
t	O
1	O
)	O
1	O
)	O
−	O
−	O
g	O
(	O
t	O
g	O
(	O
t	O
1	O
)	O
1	O
)	O
−	O
−	O
h	O
(	O
t	O
h	O
(	O
t	O
1	O
)	O
1	O
)	O
−	O
−	O
x	O
(	O
t	O
x	O
(	O
t	O
1	O
)	O
1	O
)	O
y	O
(	O
)	O
ty	O
(	O
)	O
t	O
y	O
(	O
+1	O
)	O
ty	O
(	O
+1	O
)	O
t	O
l	O
(	O
)	O
tl	O
(	O
)	O
t	O
l	O
(	O
+1	O
)	O
tl	O
(	O
+1	O
)	O
t	O
o	O
(	O
)	O
to	O
(	O
)	O
t	O
t	O
o	O
(	O
+1	O
)	O
to	O
(	O
+1	O
)	O
g	O
(	O
)	O
tg	O
(	O
)	O
t	O
g	O
(	O
+1	O
)	O
tg	O
(	O
+1	O
)	O
t	O
h	O
(	O
)	O
th	O
(	O
)	O
t	O
h	O
(	O
+1	O
)	O
th	O
(	O
+1	O
)	O
t	O
x	O
(	O
)	O
tx	O
(	O
)	O
t	O
x	O
(	O
+1	O
)	O
tx	O
(	O
+1	O
)	O
t	O
figure	O
10.11	O
:	O
computation	O
of	O
a	O
typical	O
bidirectional	O
recurrent	O
neural	O
network	O
,	O
meant	O
to	O
learn	O
to	O
map	O
input	O
sequences	O
x	O
to	O
target	O
sequences	O
y	O
,	O
with	O
loss	O
l	O
(	O
)	O
t	O
at	O
each	O
step	O
t.	O
the	O
h	O
recurrence	O
propagates	O
information	O
forward	O
in	O
time	O
(	O
towards	O
the	O
right	O
)	O
while	O
the	O
g	O
recurrence	O
propagates	O
information	O
backward	O
in	O
time	O
(	O
towards	O
the	O
left	O
)	O
.	O
thus	O
at	O
each	O
point	O
t	O
,	O
the	O
output	O
units	O
o	O
(	O
)	O
t	O
can	O
beneﬁt	O
from	O
a	O
relevant	O
summary	O
of	O
the	O
past	O
in	O
its	O
h	O
(	O
)	O
t	O
input	O
and	O
from	O
a	O
relevant	O
summary	O
of	O
the	O
future	O
in	O
its	O
g	O
(	O
)	O
t	O
input	O
.	O
10.3	O
bidirectional	O
rnns	O
all	O
of	O
the	O
recurrent	O
networks	O
we	O
have	O
considered	O
up	O
to	O
now	O
have	O
a	O
“	O
causal	O
”	O
struc-	O
ture	B
,	O
meaning	O
that	O
the	O
state	O
at	O
time	O
t	O
only	O
captures	O
information	O
from	O
the	O
past	O
,	O
−	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
,	O
and	O
the	O
present	O
input	O
x	O
(	O
)	O
t	O
.	O
some	O
of	O
the	O
models	O
we	O
have	O
discussed	O
1	O
)	O
t	O
also	O
allow	O
information	O
from	O
past	O
y	O
values	O
to	O
aﬀect	O
the	O
current	O
state	O
when	O
the	O
y	O
values	O
are	O
available	O
.	O
however	O
,	O
in	O
many	O
applications	O
we	O
want	O
to	O
output	O
a	O
prediction	O
of	O
y	O
(	O
)	O
t	O
which	O
may	O
394	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
depend	O
on	O
the	O
whole	O
input	O
sequence	O
.	O
for	O
example	O
,	O
in	O
speech	O
recognition	B
,	O
the	O
correct	O
interpretation	O
of	O
the	O
current	O
sound	O
as	O
a	O
phoneme	O
may	O
depend	O
on	O
the	O
next	O
few	O
phonemes	O
because	O
of	O
co-articulation	O
and	O
potentially	O
may	O
even	O
depend	O
on	O
the	O
next	O
few	O
words	O
because	O
of	O
the	O
linguistic	O
dependencies	O
between	O
nearby	O
words	O
:	O
if	O
there	O
are	O
two	O
interpretations	O
of	O
the	O
current	O
word	O
that	O
are	O
both	O
acoustically	O
plausible	O
,	O
we	O
may	O
have	O
to	O
look	O
far	O
into	O
the	O
future	O
(	O
and	O
the	O
past	O
)	O
to	O
disambiguate	O
them	O
.	O
this	O
is	O
also	O
true	O
of	O
handwriting	O
recognition	B
and	O
many	O
other	O
sequence-to-sequence	O
learning	O
tasks	O
,	O
described	O
in	O
the	O
next	O
section	O
.	O
bidirectional	O
recurrent	O
neural	O
networks	O
(	O
or	O
bidirectional	O
rnns	O
)	O
were	O
invented	O
)	O
.	O
they	O
have	O
been	O
extremely	O
suc-	O
)	O
in	O
applications	O
where	O
that	O
need	O
arises	O
,	O
such	O
as	O
handwriting	O
)	O
,	O
speech	O
recogni-	O
et	O
al.	O
,	O
baldi	O
to	O
address	O
that	O
need	O
(	O
schuster	O
and	O
paliwal	O
1997	O
cessful	O
(	O
graves	O
2012	O
recognition	B
(	O
graves	O
tion	B
(	O
graves	O
and	O
schmidhuber	O
2005	O
graves	O
et	O
al.	O
,	O
2008	O
graves	O
and	O
schmidhuber	O
2009	O
)	O
and	O
bioinformatics	O
(	O
et	O
al.	O
,	O
1999	O
2013	O
)	O
.	O
,	O
,	O
,	O
;	O
,	O
;	O
as	O
the	O
name	O
suggests	O
,	O
bidirectional	O
rnns	O
combine	O
an	O
rnn	O
that	O
moves	O
forward	O
through	O
time	O
beginning	O
from	O
the	O
start	O
of	O
the	O
sequence	O
with	O
another	O
rnn	O
that	O
moves	O
backward	O
through	O
time	O
beginning	O
from	O
the	O
end	O
of	O
the	O
sequence	O
.	O
figure	O
10.11	O
illustrates	O
the	O
typical	O
bidirectional	O
rnn	O
,	O
with	O
h	O
(	O
)	O
t	O
standing	O
for	O
the	O
state	O
of	O
the	O
sub-rnn	O
that	O
moves	O
forward	O
through	O
time	O
and	O
g	O
(	O
)	O
t	O
standing	O
for	O
the	O
state	O
of	O
the	O
sub-rnn	O
that	O
moves	O
backward	O
through	O
time	O
.	O
this	O
allows	O
the	O
output	O
units	O
o	O
(	O
)	O
t	O
to	O
compute	O
a	O
representation	O
that	O
depends	O
on	O
both	O
the	O
past	O
and	O
the	O
future	O
but	O
is	O
most	O
sensitive	O
to	O
the	O
input	O
values	O
around	O
time	O
t	O
,	O
without	O
having	O
to	O
specify	O
a	O
ﬁxed-size	O
window	O
around	O
t	O
(	O
as	O
one	O
would	O
have	O
to	O
do	O
with	O
a	O
feedforward	O
network	O
,	O
a	O
convolutional	O
network	O
,	O
or	O
a	O
regular	O
rnn	O
with	O
a	O
ﬁxed-size	O
look-ahead	O
buﬀer	O
)	O
.	O
four	O
this	O
idea	O
can	O
be	O
naturally	O
extended	O
to	O
2-dimensional	O
input	O
,	O
such	O
as	O
images	O
,	O
by	O
having	O
rnns	O
,	O
each	O
one	O
going	O
in	O
one	O
of	O
the	O
four	O
directions	O
:	O
up	O
,	O
down	O
,	O
left	O
,	O
right	O
.	O
at	O
each	O
point	O
(	O
i	O
,	O
j	O
)	O
of	O
a	O
2-d	O
grid	O
,	O
an	O
output	O
oi	O
,	O
j	O
could	O
then	O
compute	O
a	O
representation	O
that	O
would	O
capture	O
mostly	O
local	O
information	O
but	O
could	O
also	O
depend	O
on	O
long-range	O
inputs	O
,	O
if	O
the	O
rnn	O
is	O
able	O
to	O
learn	O
to	O
carry	O
that	O
information	O
.	O
compared	O
to	O
a	O
convolutional	O
network	O
,	O
rnns	O
applied	O
to	O
images	O
are	O
typically	O
more	O
expensive	O
but	O
allow	O
for	O
long-range	O
lateral	O
interactions	O
between	O
features	O
in	O
the	O
same	O
feature	O
map	O
(	O
)	O
.	O
indeed	O
,	O
the	O
forward	O
propagation	O
equations	O
for	O
such	O
rnns	O
may	O
be	O
written	O
in	O
a	O
form	O
that	O
shows	O
they	O
use	O
a	O
convolution	O
that	O
computes	O
the	O
bottom-up	O
input	O
to	O
each	O
layer	O
,	O
prior	O
to	O
the	O
recurrent	O
propagation	O
across	O
the	O
feature	O
map	O
that	O
incorporates	O
the	O
lateral	O
interactions	O
.	O
visin	O
et	O
al	O
.	O
2015	O
kalchbrenner	O
et	O
al.	O
,	O
2015	O
,	O
;	O
395	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
10.4	O
encoder-decoder	O
sequence-to-sequence	O
architec-	O
tures	O
10.5	O
we	O
have	O
seen	O
in	O
ﬁgure	O
vector	O
.	O
we	O
have	O
seen	O
in	O
ﬁgure	O
10.9	O
sequence	O
.	O
we	O
have	O
seen	O
in	O
ﬁgures	O
map	O
an	O
input	O
sequence	O
to	O
an	O
output	O
sequence	O
of	O
the	O
same	O
length	O
.	O
how	O
an	O
rnn	O
can	O
map	O
an	O
input	O
sequence	O
to	O
a	O
ﬁxed-size	O
how	O
an	O
rnn	O
can	O
map	O
a	O
ﬁxed-size	O
vector	O
to	O
a	O
10.3	O
10.4	O
10.10	O
how	O
an	O
rnn	O
can	O
and	O
10.11	O
,	O
,	O
encoder	O
…	O
x	O
(	O
1	O
)	O
x	O
(	O
1	O
)	O
x	O
(	O
2	O
)	O
x	O
(	O
2	O
)	O
)	O
...	O
x	O
(	O
x	O
(	O
)	O
...	O
x	O
(	O
n	O
x	O
)	O
x	O
(	O
n	O
x	O
)	O
cc	O
decoder	O
…	O
y	O
(	O
1	O
)	O
y	O
(	O
1	O
)	O
y	O
(	O
2	O
)	O
y	O
(	O
2	O
)	O
)	O
...	O
y	O
(	O
)	O
...	O
y	O
(	O
y	O
(	O
n	O
y	O
)	O
y	O
(	O
n	O
y	O
)	O
figure	O
10.12	O
:	O
example	O
of	O
an	O
encoder-decoder	O
or	O
sequence-to-sequence	O
rnn	O
architecture	O
,	O
for	O
learning	O
to	O
generate	O
an	O
output	O
sequence	O
(	O
y	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
y	O
(	O
n	O
y	O
)	O
)	O
given	O
an	O
input	O
sequence	O
(	O
x	O
(	O
1	O
)	O
,	O
x	O
(	O
2	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
nx	O
)	O
)	O
.	O
it	O
is	O
composed	O
of	O
an	O
encoder	O
rnn	O
that	O
reads	O
the	O
input	O
sequence	O
and	O
a	O
decoder	O
rnn	O
that	O
generates	O
the	O
output	O
sequence	O
(	O
or	O
computes	O
the	O
probability	O
of	O
a	O
given	O
output	O
sequence	O
)	O
.	O
the	O
ﬁnal	O
hidden	O
state	O
of	O
the	O
encoder	O
rnn	O
is	O
used	O
to	O
compute	O
a	O
generally	O
ﬁxed-size	O
context	O
variable	O
c	O
which	O
represents	O
a	O
semantic	O
summary	O
of	O
the	O
input	O
sequence	O
and	O
is	O
given	O
as	O
input	O
to	O
the	O
decoder	O
rnn	O
.	O
here	O
we	O
discuss	O
how	O
an	O
rnn	O
can	O
be	O
trained	O
to	O
map	O
an	O
input	O
sequence	O
to	O
an	O
output	O
sequence	O
which	O
is	O
not	O
necessarily	O
of	O
the	O
same	O
length	O
.	O
this	O
comes	O
up	O
in	O
many	O
applications	O
,	O
such	O
as	O
speech	O
recognition	B
,	O
machine	O
translation	O
or	O
question	O
396	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
answering	O
,	O
where	O
the	O
input	O
and	O
output	O
sequences	O
in	O
the	O
training	O
set	O
are	O
generally	O
not	O
of	O
the	O
same	O
length	O
(	O
although	O
their	O
lengths	O
might	O
be	O
related	O
)	O
.	O
we	O
often	O
call	O
the	O
input	O
to	O
the	O
rnn	O
the	O
“	O
context.	O
”	O
we	O
want	O
to	O
produce	O
a	O
representation	O
of	O
this	O
context	O
,	O
c	O
.	O
the	O
context	O
c	O
might	O
be	O
a	O
vector	O
or	O
sequence	O
of	O
vectors	O
that	O
summarize	O
the	O
input	O
sequence	O
x	O
x=	O
(	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
nx	O
)	O
)	O
.	O
(	O
et	O
al	O
.	O
(	O
2014	O
10.12	O
cho	O
et	O
al	O
.	O
2014a	O
the	O
simplest	O
rnn	O
architecture	O
for	O
mapping	O
a	O
variable-length	O
sequence	O
to	O
)	O
and	O
another	O
variable-length	O
sequence	O
was	O
ﬁrst	O
proposed	O
by	O
shortly	O
after	O
by	O
sutskever	O
)	O
,	O
who	O
independently	O
developed	O
that	O
archi-	O
tecture	O
and	O
were	O
the	O
ﬁrst	O
to	O
obtain	O
state-of-the-art	O
translation	O
using	O
this	O
approach	O
.	O
the	O
former	O
system	O
is	O
based	O
on	O
scoring	O
proposals	O
generated	O
by	O
another	O
machine	O
translation	O
system	O
,	O
while	O
the	O
latter	O
uses	O
a	O
standalone	O
recurrent	O
network	O
to	O
generate	O
the	O
translations	O
.	O
these	O
authors	O
respectively	O
called	O
this	O
architecture	O
,	O
illustrated	O
in	O
ﬁgure	O
,	O
the	O
encoder-decoder	O
or	O
sequence-to-sequence	O
architecture	O
.	O
the	O
idea	O
is	O
very	O
simple	O
:	O
(	O
1	O
)	O
an	O
encoder	O
or	O
reader	O
or	O
input	O
rnn	O
processes	O
the	O
input	O
sequence	O
.	O
the	O
encoder	O
emits	O
the	O
context	O
c	O
,	O
usually	O
as	O
a	O
simple	O
function	O
of	O
its	O
ﬁnal	O
hidden	O
state	O
.	O
(	O
2	O
)	O
a	O
decoder	O
or	O
writer	O
or	O
output	O
rnn	O
is	O
conditioned	O
on	O
that	O
ﬁxed-length	O
vector	O
(	O
just	O
like	O
in	O
ﬁgure	O
)	O
to	O
generate	O
the	O
output	O
sequence	O
y	O
=	O
(	O
y	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
y	O
(	O
ny	O
)	O
)	O
.	O
the	O
innovation	O
of	O
this	O
kind	O
of	O
architecture	O
over	O
those	O
presented	O
in	O
earlier	O
sections	O
of	O
this	O
chapter	O
is	O
that	O
the	O
lengths	O
n	O
x	O
and	O
ny	O
can	O
vary	O
from	O
each	O
other	O
,	O
while	O
previous	O
architectures	O
constrained	O
nx	O
=	O
ny	O
=	O
τ.	O
in	O
a	O
sequence-to-sequence	O
architecture	O
,	O
the	O
two	O
rnns	O
are	O
trained	O
jointly	O
to	O
maximize	O
the	O
average	O
of	O
log	O
p	O
(	O
y	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
y	O
(	O
ny	O
)	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
nx	O
)	O
)	O
over	O
all	O
the	O
pairs	O
of	O
x	O
and	O
y	O
sequences	O
in	O
the	O
training	O
set	O
.	O
the	O
last	O
state	O
hnx	O
of	O
the	O
encoder	O
rnn	O
is	O
typically	O
used	O
as	O
a	O
representation	O
c	O
of	O
the	O
input	O
sequence	O
that	O
is	O
provided	O
as	O
input	O
to	O
the	O
decoder	O
rnn	O
.	O
10.9	O
|	O
if	O
the	O
context	O
c	O
is	O
a	O
vector	O
,	O
then	O
the	O
decoder	O
rnn	O
is	O
simply	O
a	O
vector-to-	O
sequence	O
rnn	O
as	O
described	O
in	O
section	O
.	O
as	O
we	O
have	O
seen	O
,	O
there	O
are	O
at	O
least	O
two	O
ways	O
for	O
a	O
vector-to-sequence	O
rnn	O
to	O
receive	O
input	O
.	O
the	O
input	O
can	O
be	O
provided	O
as	O
the	O
initial	O
state	O
of	O
the	O
rnn	O
,	O
or	O
the	O
input	O
can	O
be	O
connected	O
to	O
the	O
hidden	O
units	O
at	O
each	O
time	O
step	O
.	O
these	O
two	O
ways	O
can	O
also	O
be	O
combined	O
.	O
10.2.4	O
there	O
is	O
no	O
constraint	O
that	O
the	O
encoder	O
must	O
have	O
the	O
same	O
size	O
of	O
hidden	O
layer	O
as	O
the	O
decoder	O
.	O
one	O
clear	O
limitation	O
of	O
this	O
architecture	O
is	O
when	O
the	O
context	O
c	O
output	O
by	O
the	O
encoder	O
rnn	O
has	O
a	O
dimension	O
that	O
is	O
too	O
small	O
to	O
properly	O
summarize	O
a	O
long	O
sequence	O
.	O
this	O
phenomenon	O
was	O
observed	O
by	O
)	O
in	O
the	O
context	O
of	O
machine	O
translation	O
.	O
they	O
proposed	O
to	O
make	O
c	O
a	O
variable-length	O
sequence	O
rather	O
than	O
a	O
ﬁxed-size	O
vector	O
.	O
additionally	O
,	O
they	O
introduced	O
an	O
attention	O
mechanism	O
that	O
learns	O
to	O
associate	O
elements	O
of	O
the	O
sequence	O
c	O
to	O
elements	O
of	O
the	O
output	O
bahdanau	O
et	O
al	O
.	O
2015	O
(	O
397	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
sequence	O
.	O
see	O
section	O
12.4.5.1	O
for	O
more	O
details	O
.	O
10.5	O
deep	O
recurrent	O
networks	O
the	O
computation	O
in	O
most	O
rnns	O
can	O
be	O
decomposed	O
into	O
three	O
blocks	O
of	O
parameters	O
and	O
associated	O
transformations	O
:	O
1.	O
from	O
the	O
input	O
to	O
the	O
hidden	O
state	O
,	O
2.	O
from	O
the	O
previous	O
hidden	O
state	O
to	O
the	O
next	O
hidden	O
state	O
,	O
and	O
3.	O
from	O
the	O
hidden	O
state	O
to	O
the	O
output	O
.	O
10.3	O
with	O
the	O
rnn	O
architecture	O
of	O
ﬁgure	O
,	O
each	O
of	O
these	O
three	O
blocks	O
is	O
associated	O
with	O
a	O
single	O
weight	O
matrix	O
.	O
in	O
other	O
words	O
,	O
when	O
the	O
network	O
is	O
unfolded	O
,	O
each	O
of	O
these	O
corresponds	O
to	O
a	O
shallow	O
transformation	O
.	O
by	O
a	O
shallow	O
transformation	O
,	O
we	O
mean	O
a	O
transformation	O
that	O
would	O
be	O
represented	O
by	O
a	O
single	O
layer	O
within	O
a	O
deep	O
mlp	O
.	O
typically	O
this	O
is	O
a	O
transformation	O
represented	O
by	O
a	O
learned	O
aﬃne	O
transformation	O
followed	O
by	O
a	O
ﬁxed	O
nonlinearity	O
.	O
would	O
it	O
be	O
advantageous	O
to	O
introduce	O
depth	O
in	O
each	O
of	O
these	O
operations	O
?	O
experimental	O
evidence	O
(	O
graves	O
)	O
strongly	O
suggests	O
so	O
.	O
the	O
experimental	O
evidence	O
is	O
in	O
agreement	O
with	O
the	O
idea	O
that	O
we	O
need	O
enough	O
depth	O
in	O
order	O
to	O
perform	O
the	O
required	O
mappings	O
.	O
see	O
also	O
schmidhuber	O
1992	O
)	O
,	O
el	O
hihi	O
and	O
bengio	O
1996	O
)	O
for	O
earlier	O
work	B
on	O
deep	O
rnns	O
.	O
2013	O
pascanu	O
jaeger	O
2007a	O
2014a	O
et	O
al.	O
,	O
et	O
al.	O
,	O
)	O
,	O
or	O
(	O
(	O
(	O
;	O
2013	O
10.3	O
398	O
10.13	O
10.13	O
et	O
al	O
.	O
(	O
graves	O
et	O
al	O
.	O
(	O
)	O
were	O
the	O
ﬁrst	O
to	O
show	O
a	O
signiﬁcant	O
beneﬁt	O
of	O
decomposing	O
(	O
left	O
)	O
.	O
we	O
can	O
think	O
the	O
state	O
of	O
an	O
rnn	O
into	O
multiple	O
layers	O
as	O
in	O
ﬁgure	O
of	O
the	O
lower	O
layers	O
in	O
the	O
hierarchy	O
depicted	O
in	O
ﬁgure	O
a	O
as	O
playing	O
a	O
role	O
in	O
transforming	O
the	O
raw	O
input	O
into	O
a	O
representation	O
that	O
is	O
more	O
appropriate	O
,	O
at	O
the	O
higher	O
levels	O
of	O
the	O
hidden	O
state	O
.	O
pascanu	O
)	O
go	O
a	O
step	O
further	O
and	O
propose	O
to	O
have	O
a	O
separate	O
mlp	O
(	O
possibly	O
deep	O
)	O
for	O
each	O
of	O
the	O
three	O
blocks	O
enumerated	O
above	O
,	O
as	O
illustrated	O
in	O
ﬁgure	O
b.	O
considerations	O
of	O
representational	O
capacity	O
suggest	O
to	O
allocate	O
enough	O
capacity	O
in	O
each	O
of	O
these	O
three	O
steps	O
,	O
but	O
doing	O
so	O
by	O
adding	O
depth	O
may	O
hurt	O
learning	O
by	O
making	O
optimization	O
diﬃcult	O
.	O
in	O
general	O
,	O
it	O
is	O
easier	O
to	O
optimize	O
shallower	O
architectures	O
,	O
and	O
adding	O
the	O
extra	O
depth	O
of	O
t	O
to	O
a	O
variable	O
ﬁgure	O
in	O
time	O
step	O
t	O
+	O
1	O
become	O
longer	O
.	O
for	O
example	O
,	O
if	O
an	O
mlp	O
with	O
a	O
single	O
hidden	O
layer	O
is	O
used	O
for	O
the	O
state-to-state	O
transition	O
,	O
we	O
have	O
doubled	O
the	O
length	O
of	O
the	O
shortest	O
path	O
between	O
variables	O
in	O
any	O
two	O
diﬀerent	O
time	O
steps	O
,	O
compared	O
with	O
the	O
)	O
,	O
this	O
ordinary	O
rnn	O
of	O
ﬁgure	O
b	O
makes	O
the	O
shortest	O
path	O
from	O
a	O
variable	O
in	O
time	O
step	O
.	O
however	O
,	O
as	O
argued	O
by	O
pascanu	O
et	O
al	O
.	O
(	O
2014a	O
2014a	O
10.13	O
10.13	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
y	O
h	O
x	O
(	O
b	O
)	O
y	O
h	O
x	O
(	O
c	O
)	O
y	O
z	O
h	O
x	O
(	O
a	O
)	O
2014a	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
10.13	O
:	O
a	O
recurrent	O
neural	O
network	O
can	O
be	O
made	O
deep	O
in	O
many	O
ways	O
(	O
pascanu	O
et	O
al.	O
,	O
)	O
.	O
the	O
hidden	O
recurrent	O
state	O
can	O
be	O
broken	O
down	O
into	O
groups	O
organized	O
hierarchically	O
.	O
deeper	O
computation	O
(	O
e.g.	O
,	O
an	O
mlp	O
)	O
can	O
be	O
introduced	O
in	O
the	O
input-to-	O
hidden	O
,	O
hidden-to-hidden	O
and	O
hidden-to-output	O
parts	O
.	O
this	O
may	O
lengthen	O
the	O
shortest	O
path	O
linking	O
diﬀerent	O
time	O
steps	O
.	O
the	O
path-lengthening	O
eﬀect	O
can	O
be	O
mitigated	O
by	O
introducing	O
skip	O
connections	O
.	O
(	O
c	O
)	O
399	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
can	O
be	O
mitigated	O
by	O
introducing	O
skip	O
connections	O
in	O
the	O
hidden-to-hidden	O
path	O
,	O
as	O
illustrated	O
in	O
ﬁgure	O
10.13	O
c.	O
10.6	O
recursive	O
neural	O
networks	O
ll	O
oo	O
yy	O
u	O
w	O
u	O
w	O
u	O
w	O
v	O
v	O
v	O
v	O
x	O
(	O
1	O
)	O
x	O
(	O
1	O
)	O
x	O
(	O
2	O
)	O
x	O
(	O
2	O
)	O
x	O
(	O
3	O
)	O
x	O
(	O
3	O
)	O
x	O
(	O
4	O
)	O
x	O
(	O
4	O
)	O
figure	O
10.14	O
:	O
a	O
recursive	O
network	O
has	O
a	O
computational	O
graph	O
that	O
generalizes	O
that	O
of	O
the	O
recurrent	O
network	O
from	O
a	O
chain	O
to	O
a	O
tree	O
.	O
a	O
variable-size	O
sequence	O
x	O
(	O
1	O
)	O
,	O
x	O
(	O
2	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
t	O
can	O
be	O
mapped	O
to	O
a	O
ﬁxed-size	O
representation	O
(	O
the	O
output	O
o	O
)	O
,	O
with	O
a	O
ﬁxed	O
set	O
of	O
parameters	O
(	O
the	O
weight	O
matrices	O
u	O
,	O
v	O
,	O
w	O
)	O
.	O
the	O
ﬁgure	O
illustrates	O
a	O
supervised	O
learning	O
case	O
in	O
which	O
some	O
target	O
is	O
provided	O
which	O
is	O
associated	O
with	O
the	O
whole	O
sequence	O
.	O
y	O
recursive	O
neural	O
networks2	O
represent	O
yet	O
another	O
generalization	O
of	O
recurrent	O
networks	O
,	O
with	O
a	O
diﬀerent	O
kind	O
of	O
computational	O
graph	O
,	O
which	O
is	O
structured	O
as	O
a	O
deep	O
tree	O
,	O
rather	O
than	O
the	O
chain-like	O
structure	O
of	O
rnns	O
.	O
the	O
typical	O
computational	O
.	O
recursive	O
neural	O
graph	O
for	O
a	O
recursive	O
network	O
is	O
illustrated	O
in	O
ﬁgure	O
10.14	O
2we	O
suggest	O
to	O
not	O
abbreviate	O
“	O
recursive	O
neural	O
network	O
”	O
as	O
“	O
rnn	O
”	O
to	O
avoid	O
confusion	O
with	O
“	O
recurrent	O
neural	O
network.	O
”	O
400	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
networks	O
were	O
introduced	O
by	O
pollack	O
1990	O
reason	O
was	O
described	O
by	O
applied	O
to	O
processing	O
data	O
structures	O
as	O
input	O
to	O
neural	O
nets	O
(	O
frasconi	O
1998	O
computer	O
vision	O
(	O
)	O
and	O
their	O
potential	O
use	O
for	O
learning	O
to	O
(	O
)	O
.	O
recursive	O
networks	O
have	O
been	O
successfully	O
bottou	O
2011	O
1997	O
,	O
)	O
as	O
well	O
as	O
in	O
)	O
,	O
in	O
natural	O
language	O
processing	O
(	O
)	O
.	O
socher	O
et	O
al	O
.	O
2011b	O
2011a	O
c	O
2013a	O
socher	O
et	O
al.	O
,	O
et	O
al.	O
,	O
,	O
,	O
(	O
,	O
one	O
clear	O
advantage	O
of	O
recursive	O
nets	O
over	O
recurrent	O
nets	O
is	O
that	O
for	O
a	O
sequence	O
of	O
the	O
same	O
length	O
τ	O
,	O
the	O
depth	O
(	O
measured	O
as	O
the	O
number	O
of	O
compositions	O
of	O
nonlinear	O
operations	O
)	O
can	O
be	O
drastically	O
reduced	O
from	O
τ	O
to	O
o	O
(	O
log	O
τ	O
)	O
,	O
which	O
might	O
help	O
deal	O
with	O
long-term	O
dependencies	O
.	O
an	O
open	O
question	O
is	O
how	O
to	O
best	O
structure	O
the	O
tree	O
.	O
one	O
option	O
is	O
to	O
have	O
a	O
tree	O
structure	O
which	O
does	O
not	O
depend	O
on	O
the	O
data	O
,	O
such	O
as	O
a	O
balanced	O
binary	O
tree	O
.	O
in	O
some	O
application	O
domains	O
,	O
external	O
methods	O
can	O
suggest	O
the	O
appropriate	O
tree	O
structure	O
.	O
for	O
example	O
,	O
when	O
processing	O
natural	O
language	O
sentences	O
,	O
the	O
tree	O
structure	O
for	O
the	O
recursive	O
network	O
can	O
be	O
ﬁxed	O
to	O
the	O
structure	O
of	O
the	O
parse	O
tree	O
of	O
the	O
sentence	O
provided	O
by	O
a	O
natural	O
language	O
parser	O
(	O
)	O
.	O
ideally	O
,	O
one	O
would	O
like	O
the	O
learner	O
itself	O
to	O
discover	O
and	O
infer	O
the	O
tree	O
structure	O
that	O
is	O
appropriate	O
for	O
any	O
given	O
input	O
,	O
as	O
suggested	O
by	O
socher	O
et	O
al	O
.	O
2011a	O
2013a	O
bottou	O
2011	O
)	O
.	O
(	O
,	O
,	O
1998	O
1997	O
)	O
and	O
et	O
al	O
.	O
(	O
frasconi	O
many	O
variants	O
of	O
the	O
recursive	O
net	O
idea	O
are	O
possible	O
.	O
for	O
example	O
,	O
frasconi	O
)	O
associate	O
the	O
data	O
with	O
a	O
tree	O
structure	O
,	O
et	O
al	O
.	O
(	O
and	O
associate	O
the	O
inputs	O
and	O
targets	O
with	O
individual	O
nodes	O
of	O
the	O
tree	O
.	O
the	O
computation	O
performed	O
by	O
each	O
node	O
does	O
not	O
have	O
to	O
be	O
the	O
traditional	O
artiﬁcial	O
neuron	O
computation	O
(	O
aﬃne	O
transformation	O
of	O
all	O
inputs	O
followed	O
by	O
a	O
monotone	O
nonlinearity	O
)	O
.	O
for	O
example	O
,	O
)	O
propose	O
using	O
tensor	O
operations	O
and	O
bilinear	O
forms	O
,	O
which	O
have	O
previously	O
been	O
found	O
useful	O
to	O
model	B
relationships	O
between	O
concepts	O
(	O
weston	O
)	O
when	O
the	O
concepts	O
are	O
represented	O
by	O
continuous	O
vectors	O
(	O
embeddings	O
)	O
.	O
socher	O
et	O
al	O
.	O
2013a	O
2010	O
bordes	O
et	O
al.	O
,	O
et	O
al.	O
,	O
2012	O
(	O
;	O
10.7	O
the	O
challenge	O
of	O
long-term	O
dependencies	O
8.2.5	O
the	O
mathematical	O
challenge	O
of	O
learning	O
long-term	O
dependencies	O
in	O
recurrent	O
net-	O
works	O
was	O
introduced	O
in	O
section	O
.	O
the	O
basic	O
problem	O
is	O
that	O
gradients	O
prop-	O
agated	O
over	O
many	O
stages	O
tend	O
to	O
either	O
vanish	O
(	O
most	O
of	O
the	O
time	O
)	O
or	O
explode	O
(	O
rarely	O
,	O
but	O
with	O
much	O
damage	O
to	O
the	O
optimization	O
)	O
.	O
even	O
if	O
we	O
assume	O
that	O
the	O
parameters	O
are	O
such	O
that	O
the	O
recurrent	O
network	O
is	O
stable	O
(	O
can	O
store	O
memories	O
,	O
with	O
gradients	O
not	O
exploding	O
)	O
,	O
the	O
diﬃculty	O
with	O
long-term	O
dependencies	O
arises	O
from	O
the	O
exponentially	O
smaller	O
weights	O
given	O
to	O
long-term	O
interactions	O
(	O
involving	O
the	O
multiplication	O
of	O
many	O
jacobians	O
)	O
compared	O
to	O
short-term	O
ones	O
.	O
many	O
other	O
et	O
al.	O
,	O
sources	O
provide	O
a	O
deeper	O
treatment	O
(	O
hochreiter	O
1991	O
doya	O
1993	O
bengio	O
,	O
;	O
,	O
;	O
401	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
t	O
u	O
p	O
t	O
u	O
o	O
f	O
o	O
n	O
o	O
i	O
t	O
c	O
e	O
j	O
o	O
r	O
p	O
4	O
3	O
2	O
1	O
0	O
−	O
1	O
−	O
2	O
−	O
3	O
−	O
−	O
4	O
60	O
0	O
1	O
2	O
3	O
4	O
5	O
−	O
40	O
−	O
20	O
0	O
20	O
40	O
60	O
input	O
coordinate	O
figure	O
10.15	O
:	O
when	O
composing	O
many	O
nonlinear	O
functions	O
(	O
like	O
the	O
linear-tanh	O
layer	O
shown	O
here	O
)	O
,	O
the	O
result	O
is	O
highly	O
nonlinear	O
,	O
typically	O
with	O
most	O
of	O
the	O
values	O
associated	O
with	O
a	O
tiny	O
derivative	O
,	O
some	O
values	O
with	O
a	O
large	O
derivative	O
,	O
and	O
many	O
alternations	O
between	O
increasing	O
and	O
decreasing	O
.	O
in	O
this	O
plot	O
,	O
we	O
plot	O
a	O
linear	O
projection	O
of	O
a	O
100-dimensional	O
hidden	O
state	O
down	O
to	O
a	O
single	O
dimension	O
,	O
plotted	O
on	O
the	O
y-axis	O
.	O
the	O
x-axis	O
is	O
the	O
coordinate	O
of	O
the	O
initial	O
state	O
along	O
a	O
random	O
direction	O
in	O
the	O
100-dimensional	O
space	O
.	O
we	O
can	O
thus	O
view	O
this	O
plot	O
as	O
a	O
linear	O
cross-section	O
of	O
a	O
high-dimensional	O
function	O
.	O
the	O
plots	O
show	O
the	O
function	O
after	O
each	O
time	O
step	O
,	O
or	O
equivalently	O
,	O
after	O
each	O
number	O
of	O
times	O
the	O
transition	O
function	O
has	O
been	O
composed	O
.	O
;	O
1994	O
pascanu	O
detail	O
.	O
the	O
remaining	O
sections	O
describe	O
approaches	O
to	O
overcoming	O
the	O
problem.	O
)	O
.	O
in	O
this	O
section	O
,	O
we	O
describe	O
the	O
problem	O
in	O
more	O
et	O
al.	O
,	O
2013	O
recurrent	O
networks	O
involve	O
the	O
composition	O
of	O
the	O
same	O
function	O
multiple	O
times	O
,	O
once	O
per	O
time	O
step	O
.	O
these	O
compositions	O
can	O
result	O
in	O
extremely	O
nonlinear	O
behavior	O
,	O
as	O
illustrated	O
in	O
ﬁgure	O
.	O
10.15	O
in	O
particular	O
,	O
the	O
function	O
composition	O
employed	O
by	O
recurrent	O
neural	O
networks	O
somewhat	O
resembles	O
matrix	O
multiplication	O
.	O
we	O
can	O
think	O
of	O
the	O
recurrence	O
relation	O
as	O
a	O
very	O
simple	O
recurrent	O
neural	O
network	O
lacking	O
a	O
nonlinear	O
activation	O
function	O
,	O
and	O
lacking	O
inputs	O
x.	O
as	O
described	O
in	O
section	O
,	O
this	O
recurrence	O
relation	O
essentially	O
describes	O
the	O
power	O
method	O
.	O
it	O
may	O
be	O
simpliﬁed	O
to	O
8.2.5	O
h	O
(	O
)	O
t	O
=	O
w	O
	O
−	O
1	O
)	O
t	O
h	O
(	O
	O
	O
	O
h	O
(	O
)	O
t	O
=	O
w	O
t	O
h	O
(	O
0	O
)	O
,	O
and	O
if	O
w	O
admits	O
an	O
eigendecomposition	O
of	O
the	O
form	O
	O
w	O
q	O
q	O
=	O
λ	O
,	O
402	O
(	O
10.36	O
)	O
(	O
10.37	O
)	O
(	O
10.38	O
)	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
with	O
orthogonal	O
q	O
,	O
the	O
recurrence	O
may	O
be	O
simpliﬁed	O
further	O
to	O
	O
h	O
(	O
)	O
t	O
=	O
q	O
λtqh	O
(	O
0	O
)	O
.	O
(	O
10.39	O
)	O
the	O
eigenvalues	O
are	O
raised	O
to	O
the	O
power	O
of	O
t	O
causing	O
eigenvalues	O
with	O
magnitude	O
less	O
than	O
one	O
to	O
decay	O
to	O
zero	O
and	O
eigenvalues	O
with	O
magnitude	O
greater	O
than	O
one	O
to	O
explode	O
.	O
any	O
component	O
of	O
h	O
(	O
0	O
)	O
that	O
is	O
not	O
aligned	O
with	O
the	O
largest	O
eigenvector	O
will	O
eventually	O
be	O
discarded	O
.	O
this	O
problem	O
is	O
particular	O
to	O
recurrent	O
networks	O
.	O
in	O
the	O
scalar	O
case	O
,	O
imagine	O
multiplying	O
a	O
weight	O
w	O
by	O
itself	O
many	O
times	O
.	O
the	O
product	O
wt	O
will	O
either	O
vanish	O
or	O
explode	O
depending	O
on	O
the	O
magnitude	O
of	O
w.	O
however	O
,	O
if	O
we	O
make	O
a	O
non-recurrent	O
network	O
that	O
has	O
a	O
diﬀerent	O
weight	O
w	O
(	O
)	O
t	O
at	O
each	O
time	O
step	O
,	O
the	O
situation	O
is	O
diﬀerent	O
.	O
t	O
w	O
(	O
)	O
t	O
.	O
suppose	O
if	O
the	O
initial	O
state	O
is	O
given	O
by	O
,	O
then	O
the	O
state	O
at	O
time	O
that	O
the	O
w	O
(	O
)	O
t	O
values	O
are	O
generated	O
randomly	O
,	O
independently	O
from	O
one	O
another	O
,	O
with	O
zero	O
mean	O
and	O
variance	O
v.	O
the	O
variance	O
of	O
the	O
product	O
is	O
o	O
(	O
v	O
n	O
)	O
.	O
to	O
obtain	O
some	O
∗	O
desired	O
variance	O
v	O
.	O
very	O
deep	O
feedforward	O
networks	O
with	O
carefully	O
chosen	O
scaling	O
can	O
thus	O
avoid	O
the	O
vanishing	O
and	O
exploding	O
gradient	O
problem	O
,	O
as	O
argued	O
by	O
√	O
we	O
may	O
choose	O
the	O
individual	O
weights	O
with	O
variance	O
v	O
=	O
n	O
sussillo	O
2014	O
t	O
is	O
given	O
by	O
)	O
.	O
∗	O
1	O
v	O
(	O
	O
,	O
,	O
;	O
,	O
the	O
vanishing	O
and	O
exploding	O
gradient	O
problem	O
for	O
rnns	O
was	O
independently	O
discovered	O
by	O
separate	O
researchers	O
(	O
hochreiter	O
1991	O
bengio	O
et	O
al	O
.	O
1993	O
1994	O
)	O
.	O
one	O
may	O
hope	O
that	O
the	O
problem	O
can	O
be	O
avoided	O
simply	O
by	O
staying	O
in	O
a	O
region	O
of	O
parameter	O
space	O
where	O
the	O
gradients	O
do	O
not	O
vanish	O
or	O
explode	O
.	O
unfortunately	O
,	O
in	O
order	O
to	O
store	O
memories	O
in	O
a	O
way	O
that	O
is	O
robust	O
to	O
small	O
perturbations	O
,	O
the	O
rnn	O
must	O
enter	O
a	O
region	O
of	O
parameter	O
space	O
where	O
gradients	O
vanish	O
(	O
bengio	O
et	O
al	O
.	O
1993	O
,	O
1994	O
)	O
.	O
speciﬁcally	O
,	O
whenever	O
the	O
model	B
is	O
able	O
to	O
represent	O
long	O
term	O
dependencies	O
,	O
the	O
gradient	O
of	O
a	O
long	O
term	O
interaction	O
has	O
exponentially	O
smaller	O
magnitude	O
than	O
the	O
gradient	O
of	O
a	O
short	O
term	O
interaction	O
.	O
it	O
does	O
not	O
mean	O
that	O
it	O
is	O
impossible	O
to	O
learn	O
,	O
but	O
that	O
it	O
might	O
take	O
a	O
very	O
long	O
time	O
to	O
learn	O
long-term	O
dependencies	O
,	O
because	O
the	O
signal	O
about	O
these	O
dependencies	O
will	O
tend	O
to	O
be	O
hidden	O
by	O
the	O
smallest	O
ﬂuctuations	O
arising	O
from	O
short-term	O
dependencies	O
.	O
in	O
practice	O
,	O
the	O
experiments	O
in	O
)	O
show	O
that	O
as	O
we	O
increase	O
the	O
span	O
of	O
the	O
dependencies	O
that	O
need	O
to	O
be	O
captured	O
,	O
gradient-based	O
optimization	O
becomes	O
increasingly	O
diﬃcult	O
,	O
with	O
the	O
probability	O
of	O
successful	O
training	O
of	O
a	O
traditional	O
rnn	O
via	O
sgd	O
rapidly	O
reaching	O
0	O
for	O
sequences	O
of	O
only	O
length	O
10	O
or	O
20.	O
bengio	O
et	O
al	O
.	O
1994	O
(	O
,	O
)	O
,	O
for	O
a	O
deeper	O
treatment	O
of	O
recurrent	O
networks	O
as	O
dynamical	O
systems	O
,	O
see	O
doya	O
)	O
,	O
with	O
a	O
review	O
(	O
1993	O
bengio	O
et	O
al	O
.	O
1994	O
in	O
pascanu	O
)	O
.	O
the	O
remaining	O
sections	O
of	O
this	O
chapter	O
discuss	O
various	O
approaches	O
that	O
have	O
been	O
proposed	O
to	O
reduce	O
the	O
diﬃculty	O
of	O
learning	O
long-	O
term	O
dependencies	O
(	O
in	O
some	O
cases	O
allowing	O
an	O
rnn	O
to	O
learn	O
dependencies	O
across	O
siegelmann	O
and	O
sontag	O
1995	O
(	O
2013	O
et	O
al	O
.	O
(	O
)	O
and	O
(	O
403	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
hundreds	O
of	O
steps	O
)	O
,	O
but	O
the	O
problem	O
of	O
learning	O
long-term	O
dependencies	O
remains	O
one	O
of	O
the	O
main	O
challenges	O
in	O
deep	O
learning	O
.	O
,	O
,	O
,	O
,	O
;	O
10.8	O
echo	O
state	O
networks	O
−	O
the	O
recurrent	O
weights	O
mapping	O
from	O
h	O
(	O
1	O
)	O
t	O
to	O
h	O
(	O
)	O
t	O
and	O
the	O
input	O
weights	O
mapping	O
from	O
x	O
(	O
)	O
t	O
to	O
h	O
(	O
)	O
t	O
are	O
some	O
of	O
the	O
most	O
diﬃcult	O
parameters	O
to	O
learn	O
in	O
a	O
recurrent	O
network	O
.	O
one	O
proposed	O
(	O
jaeger	O
2003	O
maass	O
et	O
al	O
.	O
2002	O
jaeger	O
and	O
haas	O
2004	O
;	O
jaeger	O
2007b	O
)	O
approach	O
to	O
avoiding	O
this	O
diﬃculty	O
is	O
to	O
set	O
the	O
recurrent	O
weights	O
such	O
that	O
the	O
recurrent	O
hidden	O
units	O
do	O
a	O
good	O
job	O
of	O
capturing	O
the	O
history	O
of	O
past	O
inputs	O
,	O
and	O
learn	O
only	O
the	O
output	O
weights	O
.	O
this	O
is	O
the	O
idea	O
that	O
was	O
independently	O
proposed	O
for	O
echo	O
state	O
networks	O
or	O
esns	O
(	O
jaeger	O
and	O
haas	O
2004	O
jaeger	O
2007b	O
)	O
and	O
liquid	O
state	O
machines	O
(	O
)	O
.	O
the	O
latter	O
is	O
similar	O
,	O
except	O
that	O
it	O
uses	O
spiking	O
neurons	O
(	O
with	O
binary	O
outputs	O
)	O
instead	O
of	O
the	O
continuous-valued	O
hidden	O
units	O
used	O
for	O
esns	O
.	O
both	O
esns	O
and	O
liquid	O
state	O
machines	O
are	O
termed	O
reservoir	O
computing	O
(	O
lukoševičius	O
and	O
jaeger	O
2009	O
)	O
to	O
denote	O
the	O
fact	O
that	O
the	O
hidden	O
units	O
form	O
of	O
reservoir	O
of	O
temporal	O
features	O
which	O
may	O
capture	O
diﬀerent	O
aspects	O
of	O
the	O
history	O
of	O
inputs	O
.	O
maass	O
et	O
al	O
.	O
2002	O
,	O
,	O
,	O
;	O
,	O
;	O
one	O
way	O
to	O
think	O
about	O
these	O
reservoir	O
computing	O
recurrent	O
networks	O
is	O
that	O
they	O
are	O
similar	O
to	O
kernel	O
machines	O
:	O
they	O
map	O
an	O
arbitrary	O
length	O
sequence	O
(	O
the	O
history	O
of	O
inputs	O
up	O
to	O
time	O
t	O
)	O
into	O
a	O
ﬁxed-length	O
vector	O
(	O
the	O
recurrent	O
state	O
h	O
(	O
)	O
t	O
)	O
,	O
on	O
which	O
a	O
linear	O
predictor	O
(	O
typically	O
a	O
linear	O
regression	O
)	O
can	O
be	O
applied	O
to	O
solve	O
the	O
problem	O
of	O
interest	O
.	O
the	O
training	O
criterion	O
may	O
then	O
be	O
easily	O
designed	O
to	O
be	O
convex	O
as	O
a	O
function	O
of	O
the	O
output	O
weights	O
.	O
for	O
example	O
,	O
if	O
the	O
output	O
consists	O
of	O
linear	O
regression	O
from	O
the	O
hidden	O
units	O
to	O
the	O
output	O
targets	O
,	O
and	O
the	O
training	O
criterion	O
is	O
mean	O
squared	O
error	O
,	O
then	O
it	O
is	O
convex	O
and	O
may	O
be	O
solved	O
reliably	O
with	O
simple	O
learning	O
algorithms	O
(	O
jaeger	O
2003	O
)	O
.	O
,	O
the	O
important	O
question	O
is	O
therefore	O
:	O
how	O
do	O
we	O
set	O
the	O
input	O
and	O
recurrent	O
weights	O
so	O
that	O
a	O
rich	O
set	O
of	O
histories	O
can	O
be	O
represented	O
in	O
the	O
recurrent	O
neural	O
network	O
state	O
?	O
the	O
answer	O
proposed	O
in	O
the	O
reservoir	O
computing	O
literature	O
is	O
to	O
view	O
the	O
recurrent	O
net	O
as	O
a	O
dynamical	O
system	O
,	O
and	O
set	O
the	O
input	O
and	O
recurrent	O
weights	O
such	O
that	O
the	O
dynamical	O
system	O
is	O
near	O
the	O
edge	O
of	O
stability	O
.	O
the	O
original	O
idea	O
was	O
to	O
make	O
the	O
eigenvalues	O
of	O
the	O
jacobian	O
of	O
the	O
state-to-	O
,	O
an	O
important	O
state	O
transition	O
function	O
be	O
close	O
to	O
.	O
as	O
explained	O
in	O
section	O
characteristic	O
of	O
a	O
recurrent	O
network	O
is	O
the	O
eigenvalue	O
spectrum	O
of	O
the	O
jacobians	O
j	O
(	O
)	O
t	O
=	O
∂s	O
(	O
)	O
t	O
−	O
.	O
of	O
particular	O
importance	O
is	O
the	O
spectral	O
radius	O
of	O
j	O
(	O
)	O
t	O
,	O
deﬁned	O
to	O
1	O
)	O
t	O
∂s	O
(	O
be	O
the	O
maximum	O
of	O
the	O
absolute	O
values	O
of	O
its	O
eigenvalues	O
.	O
8.2.5	O
1	O
404	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
to	O
understand	O
the	O
eﬀect	O
of	O
the	O
spectral	O
radius	O
,	O
consider	O
the	O
simple	O
case	O
of	O
back-propagation	O
with	O
a	O
jacobian	O
matrix	O
j	O
that	O
does	O
not	O
change	O
with	O
t.	O
this	O
case	O
happens	O
,	O
for	O
example	O
,	O
when	O
the	O
network	O
is	O
purely	O
linear	O
.	O
suppose	O
that	O
j	O
has	O
an	O
eigenvector	O
v	O
with	O
corresponding	O
eigenvalue	O
λ.	O
consider	O
what	O
happens	O
as	O
we	O
propagate	O
a	O
gradient	O
vector	O
backwards	O
through	O
time	O
.	O
if	O
we	O
begin	O
with	O
a	O
gradient	O
vector	O
g	O
,	O
then	O
after	O
one	O
step	O
of	O
back-propagation	O
,	O
we	O
will	O
have	O
j	O
g	O
,	O
and	O
after	O
n	O
steps	O
we	O
will	O
have	O
j	O
n	O
g.	O
now	O
consider	O
what	O
happens	O
if	O
we	O
instead	O
back-propagate	O
a	O
perturbed	O
version	O
of	O
g.	O
if	O
we	O
begin	O
with	O
g	O
+	O
δv	O
,	O
then	O
after	O
one	O
step	O
,	O
we	O
will	O
have	O
j	O
(	O
g	O
+	O
δv	O
)	O
.	O
after	O
n	O
steps	O
,	O
we	O
will	O
have	O
j	O
n	O
(	O
g	O
+	O
δv	O
)	O
.	O
from	O
this	O
we	O
can	O
see	O
that	O
back-propagation	O
starting	O
from	O
g	O
and	O
back-propagation	O
starting	O
from	O
g	O
+	O
δv	O
diverge	O
by	O
δj	O
n	O
v	O
after	O
n	O
steps	O
of	O
back-propagation	O
.	O
if	O
v	O
is	O
chosen	O
to	O
be	O
a	O
unit	O
eigenvector	O
of	O
j	O
with	O
eigenvalue	O
λ	O
,	O
then	O
multiplication	O
by	O
the	O
jacobian	O
simply	O
|	O
|	O
|	O
|	O
scales	O
the	O
diﬀerence	O
at	O
each	O
step	O
.	O
the	O
two	O
executions	O
of	O
back-propagation	O
are	O
n.	O
when	O
v	O
corresponds	O
to	O
the	O
largest	O
value	O
of	O
λ	O
,	O
separated	O
by	O
a	O
distance	O
of	O
δ	O
λ	O
this	O
perturbation	O
achieves	O
the	O
widest	O
possible	O
separation	O
of	O
an	O
initial	O
perturbation	O
of	O
size	O
.δ	O
|	O
|	O
|	O
|	O
n	O
grows	O
exponentially	O
large	O
.	O
when	O
λ	O
>	O
1	O
,	O
the	O
deviation	O
size	O
δ	O
λ	O
|	O
|	O
λ	O
<	O
1	O
,	O
when	O
the	O
deviation	O
size	O
becomes	O
exponentially	O
small	O
.	O
of	O
course	O
,	O
this	O
example	O
assumed	O
that	O
the	O
jacobian	O
was	O
the	O
same	O
at	O
every	O
time	O
step	O
,	O
corresponding	O
to	O
a	O
recurrent	O
network	O
with	O
no	O
nonlinearity	O
.	O
when	O
a	O
nonlinearity	O
is	O
present	O
,	O
the	O
derivative	O
of	O
the	O
nonlinearity	O
will	O
approach	O
zero	O
on	O
many	O
time	O
steps	O
,	O
and	O
help	O
to	O
prevent	O
the	O
explosion	O
resulting	O
from	O
a	O
large	O
spectral	O
radius	O
.	O
indeed	O
,	O
the	O
most	O
recent	O
work	B
on	O
echo	O
state	O
networks	O
advocates	O
using	O
a	O
spectral	O
radius	O
much	O
larger	O
than	O
unity	O
(	O
yildiz	O
et	O
al	O
.	O
2012	O
jaeger	O
2012	O
)	O
.	O
,	O
;	O
,	O
everything	O
we	O
have	O
said	O
about	O
back-propagation	O
via	O
repeated	O
matrix	O
multipli-	O
cation	O
applies	O
equally	O
to	O
forward	O
propagation	O
in	O
a	O
network	O
with	O
no	O
nonlinearity	O
,	O
where	O
the	O
state	O
h	O
(	O
+1	O
)	O
w	O
.	O
t	O
	O
=	O
h	O
(	O
)	O
t	O
	O
when	O
a	O
linear	O
map	O
w	O
always	O
shrinks	O
h	O
as	O
measured	O
by	O
the	O
l2	O
norm	O
,	O
then	O
we	O
say	O
that	O
the	O
map	O
is	O
contractive	O
.	O
when	O
the	O
spectral	O
radius	O
is	O
less	O
than	O
one	O
,	O
the	O
mapping	O
from	O
h	O
(	O
)	O
t	O
to	O
h	O
(	O
+1	O
)	O
is	O
contractive	O
,	O
so	O
a	O
small	O
change	O
becomes	O
smaller	O
after	O
each	O
time	O
step	O
.	O
this	O
necessarily	O
makes	O
the	O
network	O
forget	O
information	O
about	O
the	O
past	O
when	O
we	O
use	O
a	O
ﬁnite	O
level	O
of	O
precision	O
(	O
such	O
as	O
32	O
bit	O
integers	O
)	O
to	O
store	O
the	O
state	O
vector	O
.	O
t	O
the	O
jacobian	O
matrix	O
tells	O
us	O
how	O
a	O
small	O
change	O
of	O
h	O
(	O
)	O
t	O
propagates	O
one	O
step	O
forward	O
,	O
or	O
equivalently	O
,	O
how	O
the	O
gradient	O
on	O
h	O
(	O
+1	O
)	O
propagates	O
one	O
step	O
backward	O
,	O
during	O
back-propagation	O
.	O
note	O
that	O
neither	O
w	O
nor	O
j	O
need	O
to	O
be	O
symmetric	O
(	O
al-	O
though	O
they	O
are	O
square	O
and	O
real	O
)	O
,	O
so	O
they	O
can	O
have	O
complex-valued	O
eigenvalues	O
and	O
eigenvectors	O
,	O
with	O
imaginary	O
components	O
corresponding	O
to	O
potentially	O
oscillatory	O
t	O
405	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
behavior	O
(	O
if	O
the	O
same	O
jacobian	O
was	O
applied	O
iteratively	O
)	O
.	O
even	O
though	O
h	O
(	O
)	O
t	O
or	O
a	O
small	O
variation	O
of	O
h	O
(	O
)	O
t	O
of	O
interest	O
in	O
back-propagation	O
are	O
real-valued	O
,	O
they	O
can	O
be	O
expressed	O
in	O
such	O
a	O
complex-valued	O
basis	O
.	O
what	O
matters	O
is	O
what	O
happens	O
to	O
the	O
magnitude	O
(	O
complex	O
absolute	O
value	O
)	O
of	O
these	O
possibly	O
complex-valued	O
basis	O
coeﬃcients	O
,	O
when	O
we	O
multiply	O
the	O
matrix	O
by	O
the	O
vector	O
.	O
an	O
eigenvalue	O
with	O
magnitude	O
greater	O
than	O
one	O
corresponds	O
to	O
magniﬁcation	O
(	O
exponential	O
growth	O
,	O
if	O
applied	O
iteratively	O
)	O
or	O
shrinking	O
(	O
exponential	O
decay	O
,	O
if	O
applied	O
iteratively	O
)	O
.	O
with	O
a	O
nonlinear	O
map	O
,	O
the	O
jacobian	O
is	O
free	O
to	O
change	O
at	O
each	O
step	O
.	O
the	O
dynamics	O
therefore	O
become	O
more	O
complicated	O
.	O
however	O
,	O
it	O
remains	O
true	O
that	O
a	O
small	O
initial	O
variation	O
can	O
turn	O
into	O
a	O
large	O
variation	O
after	O
several	O
steps	O
.	O
one	O
diﬀerence	O
between	O
the	O
purely	O
linear	O
case	O
and	O
the	O
nonlinear	O
case	O
is	O
that	O
the	O
use	O
of	O
a	O
squashing	O
nonlinearity	O
such	O
as	O
tanh	B
can	O
cause	O
the	O
recurrent	O
dynamics	O
to	O
become	O
bounded	O
.	O
note	O
that	O
it	O
is	O
possible	O
for	O
back-propagation	O
to	O
retain	O
unbounded	O
dynamics	O
even	O
when	O
forward	O
propagation	O
has	O
bounded	O
dynamics	O
,	O
for	O
example	O
,	O
when	O
a	O
sequence	O
of	O
tanh	B
units	O
are	O
all	O
in	O
the	O
middle	O
of	O
their	O
linear	O
regime	O
and	O
are	O
connected	O
by	O
weight	O
matrices	O
with	O
spectral	O
radius	O
greater	O
than	O
.	O
however	O
,	O
it	O
is	O
rare	O
for	O
all	O
of	O
the	O
units	O
to	O
simultaneously	O
lie	O
at	O
their	O
linear	O
activation	O
point	O
.	O
tanh	B
1	O
the	O
strategy	O
of	O
echo	O
state	O
networks	O
is	O
simply	O
to	O
ﬁx	O
the	O
weights	O
to	O
have	O
some	O
,	O
where	O
information	O
is	O
carried	O
forward	O
through	O
time	O
but	O
spectral	O
radius	O
such	O
as	O
does	O
not	O
explode	O
due	O
to	O
the	O
stabilizing	O
eﬀect	O
of	O
saturating	O
nonlinearities	O
like	O
tanh	B
.	O
3	O
more	O
recently	O
,	O
it	O
has	O
been	O
shown	O
that	O
the	O
techniques	O
used	O
to	O
set	O
the	O
weights	O
the	O
weights	O
in	O
a	O
fully	O
trainable	O
recurrent	O
net-	O
in	O
esns	O
could	O
be	O
used	O
to	O
work	B
(	O
with	O
the	O
hidden-to-hidden	O
recurrent	O
weights	O
trained	O
using	O
back-propagation	O
through	O
time	O
)	O
,	O
helping	O
to	O
learn	O
long-term	O
dependencies	O
(	O
sutskever	O
2012	O
sutskever	O
et	O
al.	O
,	O
)	O
.	O
in	O
this	O
setting	O
,	O
an	O
initial	O
spectral	O
radius	O
of	O
1.2	O
performs	O
well	O
,	O
combined	O
with	O
the	O
sparse	O
initialization	O
scheme	O
described	O
in	O
section	O
initialize	O
2013	O
.8.4	O
,	O
;	O
10.9	O
leaky	O
units	O
and	O
other	O
strategies	O
for	O
multiple	O
time	O
scales	O
one	O
way	O
to	O
deal	O
with	O
long-term	O
dependencies	O
is	O
to	O
design	O
a	O
model	B
that	O
operates	O
at	O
multiple	O
time	O
scales	O
,	O
so	O
that	O
some	O
parts	O
of	O
the	O
model	B
operate	O
at	O
ﬁne-grained	O
time	O
scales	O
and	O
can	O
handle	O
small	O
details	O
,	O
while	O
other	O
parts	O
operate	O
at	O
coarse	O
time	O
scales	O
and	O
transfer	O
information	O
from	O
the	O
distant	O
past	O
to	O
the	O
present	O
more	O
eﬃciently	O
.	O
various	O
strategies	O
for	O
building	O
both	O
ﬁne	O
and	O
coarse	O
time	O
scales	O
are	O
possible	O
.	O
these	O
include	O
the	O
addition	O
of	O
skip	O
connections	O
across	O
time	O
,	O
“	O
leaky	O
units	O
”	O
that	O
integrate	O
signals	O
with	O
diﬀerent	O
time	O
constants	O
,	O
and	O
the	O
removal	O
of	O
some	O
of	O
the	O
connections	O
406	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
used	O
to	O
model	B
ﬁne-grained	O
time	O
scales	O
.	O
10.9.1	O
adding	O
skip	O
connections	O
through	O
time	O
one	O
way	O
to	O
obtain	O
coarse	O
time	O
scales	O
is	O
to	O
add	O
direct	O
connections	O
from	O
variables	O
in	O
the	O
distant	O
past	O
to	O
variables	O
in	O
the	O
present	O
.	O
the	O
idea	O
of	O
using	O
such	O
skip	O
connections	O
dates	O
back	O
to	O
)	O
and	O
follows	O
from	O
the	O
idea	O
of	O
incorporating	O
delays	O
in	O
feedforward	O
neural	O
networks	O
(	O
)	O
.	O
in	O
an	O
ordinary	O
recurrent	O
network	O
,	O
a	O
recurrent	O
connection	O
goes	O
from	O
a	O
unit	O
at	O
time	O
t	O
to	O
a	O
unit	O
at	O
time	O
t	O
+	O
1.	O
it	O
is	O
possible	O
to	O
construct	O
recurrent	O
networks	O
with	O
longer	O
delays	O
(	O
lang	O
and	O
hinton	O
1988	O
lin	O
et	O
al	O
.	O
1996	O
bengio	O
1991	O
)	O
.	O
(	O
,	O
,	O
8.2.5	O
as	O
we	O
have	O
seen	O
in	O
section	O
,	O
gradients	O
may	O
vanish	O
or	O
explode	O
exponentially	O
with	O
respect	O
to	O
the	O
number	O
of	O
time	O
steps.	O
)	O
introduced	O
recurrent	O
connections	O
with	O
a	O
time-delay	O
of	O
d	O
to	O
mitigate	O
this	O
problem	O
.	O
gradients	O
now	O
diminish	O
exponentially	O
as	O
a	O
function	O
of	O
τ	O
rather	O
than	O
τ.	O
since	O
there	O
are	O
both	O
d	O
delayed	O
and	O
single	O
step	O
connections	O
,	O
gradients	O
may	O
still	O
explode	O
exponentially	O
in	O
τ.	O
this	O
allows	O
the	O
learning	O
algorithm	O
to	O
capture	O
longer	O
dependencies	O
although	O
not	O
all	O
long-term	O
dependencies	O
may	O
be	O
represented	O
well	O
in	O
this	O
way	O
.	O
lin	O
et	O
al	O
.	O
1996	O
(	O
10.9.2	O
leaky	O
units	O
and	O
a	O
spectrum	O
of	O
diﬀerent	O
time	O
scales	O
−	O
←	O
another	O
way	O
to	O
obtain	O
paths	O
on	O
which	O
the	O
product	O
of	O
derivatives	O
is	O
close	O
to	O
one	O
is	O
to	O
have	O
units	O
with	O
linear	O
self-connections	O
and	O
a	O
weight	O
near	O
one	O
on	O
these	O
connections	O
.	O
when	O
we	O
accumulate	O
a	O
running	O
average	O
µ	O
(	O
)	O
t	O
of	O
some	O
value	O
v	O
(	O
)	O
t	O
by	O
applying	O
the	O
−	O
1	O
)	O
αµ	O
(	O
t	O
+	O
(	O
1	O
α	O
)	O
v	O
(	O
)	O
t	O
the	O
α	O
parameter	O
is	O
an	O
example	O
of	O
a	O
linear	O
self-	O
update	O
µ	O
(	O
)	O
t	O
−	O
connection	O
from	O
µ	O
(	O
t	O
1	O
)	O
to	O
µ	O
(	O
)	O
t	O
.	O
when	O
α	O
is	O
near	O
one	O
,	O
the	O
running	O
average	O
remembers	O
information	O
about	O
the	O
past	O
for	O
a	O
long	O
time	O
,	O
and	O
when	O
α	O
is	O
near	O
zero	O
,	O
information	O
about	O
the	O
past	O
is	O
rapidly	O
discarded	O
.	O
hidden	O
units	O
with	O
linear	O
self-connections	O
can	O
behave	O
similarly	O
to	O
such	O
running	O
averages	O
.	O
such	O
hidden	O
units	O
are	O
called	O
leaky	O
units	O
.	O
skip	O
connections	O
through	O
d	O
time	O
steps	O
are	O
a	O
way	O
of	O
ensuring	O
that	O
a	O
unit	O
can	O
always	O
learn	O
to	O
be	O
inﬂuenced	O
by	O
a	O
value	O
from	O
d	O
time	O
steps	O
earlier	O
.	O
the	O
use	O
of	O
a	O
linear	O
self-connection	O
with	O
a	O
weight	O
near	O
one	O
is	O
a	O
diﬀerent	O
way	O
of	O
ensuring	O
that	O
the	O
unit	O
can	O
access	O
values	O
from	O
the	O
past	O
.	O
the	O
linear	O
self-connection	O
approach	O
allows	O
this	O
eﬀect	O
to	O
be	O
adapted	O
more	O
smoothly	O
and	O
ﬂexibly	O
by	O
adjusting	O
the	O
real-valued	O
α	O
rather	O
than	O
by	O
adjusting	O
the	O
integer-valued	O
skip	O
length	O
.	O
these	O
ideas	O
were	O
proposed	O
by	O
el	O
hihi	O
and	O
bengio	O
1996	O
)	O
.	O
leaky	O
units	O
were	O
also	O
found	O
to	O
be	O
useful	O
in	O
the	O
context	O
of	O
echo	O
state	O
networks	O
(	O
jaeger	O
et	O
al	O
.	O
2007	O
mozer	O
1992	O
)	O
and	O
by	O
)	O
.	O
(	O
(	O
,	O
407	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
there	O
are	O
two	O
basic	O
strategies	O
for	O
setting	O
the	O
time	O
constants	O
used	O
by	O
leaky	O
units	O
.	O
one	O
strategy	O
is	O
to	O
manually	O
ﬁx	O
them	O
to	O
values	O
that	O
remain	O
constant	O
,	O
for	O
example	O
by	O
sampling	O
their	O
values	O
from	O
some	O
distribution	O
once	O
at	O
initialization	O
time	O
.	O
another	O
strategy	O
is	O
to	O
make	O
the	O
time	O
constants	O
free	O
parameters	O
and	O
learn	O
them	O
.	O
having	O
such	O
leaky	O
units	O
at	O
diﬀerent	O
time	O
scales	O
appears	O
to	O
help	O
with	O
long-term	O
dependencies	O
(	O
mozer	O
1992	O
pascanu	O
et	O
al.	O
,	O
2013	O
)	O
.	O
,	O
;	O
10.9.3	O
removing	O
connections	O
another	O
approach	O
to	O
handle	O
long-term	O
dependencies	O
is	O
the	O
idea	O
of	O
organizing	O
the	O
state	O
of	O
the	O
rnn	O
at	O
multiple	O
time-scales	O
(	O
)	O
,	O
with	O
information	O
ﬂowing	O
more	O
easily	O
through	O
long	O
distances	O
at	O
the	O
slower	O
time	O
scales	O
.	O
el	O
hihi	O
and	O
bengio	O
1996	O
,	O
this	O
idea	O
diﬀers	O
from	O
the	O
skip	O
connections	O
through	O
time	O
discussed	O
earlier	O
because	O
it	O
involves	O
actively	O
removing	O
length-one	O
connections	O
and	O
replacing	O
them	O
with	O
longer	O
connections	O
.	O
units	O
modiﬁed	O
in	O
such	O
a	O
way	O
are	O
forced	O
to	O
operate	O
on	O
a	O
long	O
time	O
scale	O
.	O
skip	O
connections	O
through	O
time	O
edges	O
.	O
units	O
receiving	O
such	O
new	O
connections	O
may	O
learn	O
to	O
operate	O
on	O
a	O
long	O
time	O
scale	O
but	O
may	O
also	O
choose	O
to	O
focus	O
on	O
their	O
other	O
short-term	O
connections	O
.	O
add	O
there	O
are	O
diﬀerent	O
ways	O
in	O
which	O
a	O
group	O
of	O
recurrent	O
units	O
can	O
be	O
forced	O
to	O
operate	O
at	O
diﬀerent	O
time	O
scales	O
.	O
one	O
option	O
is	O
to	O
make	O
the	O
recurrent	O
units	O
leaky	O
,	O
but	O
to	O
have	O
diﬀerent	O
groups	O
of	O
units	O
associated	O
with	O
diﬀerent	O
ﬁxed	O
time	O
scales	O
.	O
this	O
was	O
the	O
proposal	O
in	O
pascanu	O
et	O
al	O
.	O
(	O
)	O
.	O
another	O
option	O
is	O
to	O
have	O
explicit	O
and	O
discrete	O
updates	O
taking	O
place	O
at	O
diﬀerent	O
times	O
,	O
with	O
a	O
diﬀerent	O
frequency	O
for	O
diﬀerent	O
groups	O
of	O
units	O
.	O
this	O
is	O
the	O
approach	O
of	O
el	O
hihi	O
and	O
bengio	O
1996	O
)	O
.	O
it	O
worked	O
well	O
on	O
a	O
number	O
of	O
benchmark	O
datasets.	O
)	O
and	O
has	O
been	O
successfully	O
used	O
in	O
mozer	O
1992	O
koutnik	O
et	O
al	O
.	O
(	O
)	O
and	O
2014	O
2013	O
(	O
(	O
10.10	O
the	O
long	O
short-term	O
memory	O
and	O
other	O
gated	O
rnns	O
as	O
of	O
this	O
writing	O
,	O
the	O
most	O
eﬀective	O
sequence	O
models	O
used	O
in	O
practical	O
applications	O
are	O
called	O
gated	O
rnns	O
.	O
these	O
include	O
the	O
long	O
short-term	O
memory	O
and	O
networks	O
based	O
on	O
the	O
.	O
gated	O
recurrent	O
unit	O
like	O
leaky	O
units	O
,	O
gated	O
rnns	O
are	O
based	O
on	O
the	O
idea	O
of	O
creating	O
paths	O
through	O
time	O
that	O
have	O
derivatives	O
that	O
neither	O
vanish	O
nor	O
explode	O
.	O
leaky	O
units	O
did	O
this	O
with	O
connection	O
weights	O
that	O
were	O
either	O
manually	O
chosen	O
constants	O
or	O
were	O
parameters	O
.	O
gated	O
rnns	O
generalize	O
this	O
to	O
connection	O
weights	O
that	O
may	O
change	O
408	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
at	O
each	O
time	O
step	O
.	O
output	O
self-loop	O
×	O
state	O
×	O
+	O
×	O
input	O
input	O
gate	O
forget	O
gate	O
output	O
gate	O
figure	O
10.16	O
:	O
block	O
diagram	O
of	O
the	O
lstm	O
recurrent	O
network	O
“	O
cell.	O
”	O
cells	O
are	O
connected	O
recurrently	O
to	O
each	O
other	O
,	O
replacing	O
the	O
usual	O
hidden	O
units	O
of	O
ordinary	O
recurrent	O
networks	O
.	O
an	O
input	O
feature	O
is	O
computed	O
with	O
a	O
regular	O
artiﬁcial	O
neuron	O
unit	O
.	O
its	O
value	O
can	O
be	O
accumulated	O
into	O
the	O
state	O
if	O
the	O
sigmoidal	O
input	O
gate	O
allows	O
it	O
.	O
the	O
state	O
unit	O
has	O
a	O
linear	O
self-loop	O
whose	O
weight	O
is	O
controlled	O
by	O
the	O
forget	O
gate	O
.	O
the	O
output	O
of	O
the	O
cell	O
can	O
be	O
shut	O
oﬀ	O
by	O
the	O
output	O
gate	O
.	O
all	O
the	O
gating	O
units	O
have	O
a	O
sigmoid	O
nonlinearity	O
,	O
while	O
the	O
input	O
unit	O
can	O
have	O
any	O
squashing	O
nonlinearity	O
.	O
the	O
state	O
unit	O
can	O
also	O
be	O
used	O
as	O
an	O
extra	O
input	O
to	O
the	O
gating	O
units	O
.	O
the	O
black	O
square	O
indicates	O
a	O
delay	O
of	O
a	O
single	O
time	O
step	O
.	O
leaky	O
units	O
allow	O
the	O
network	O
to	O
accumulate	O
information	O
(	O
such	O
as	O
evidence	O
for	O
a	O
particular	O
feature	O
or	O
category	O
)	O
over	O
a	O
long	O
duration	O
.	O
however	O
,	O
once	O
that	O
information	O
has	O
been	O
used	O
,	O
it	O
might	O
be	O
useful	O
for	O
the	O
neural	O
network	O
to	O
forget	O
the	O
old	O
state	O
.	O
for	O
example	O
,	O
if	O
a	O
sequence	O
is	O
made	O
of	O
sub-sequences	O
and	O
we	O
want	O
a	O
leaky	O
unit	O
to	O
accumulate	O
evidence	O
inside	O
each	O
sub-subsequence	O
,	O
we	O
need	O
a	O
mechanism	O
to	O
forget	O
the	O
old	O
state	O
by	O
setting	O
it	O
to	O
zero	O
.	O
instead	O
of	O
manually	O
deciding	O
when	O
to	O
clear	O
the	O
state	O
,	O
we	O
want	O
the	O
neural	O
network	O
to	O
learn	O
to	O
decide	O
when	O
to	O
do	O
it	O
.	O
this	O
409	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
is	O
what	O
gated	O
rnns	O
do	O
.	O
10.10.1	O
lstm	O
,	O
gers	O
et	O
al	O
.	O
2000	O
the	O
clever	O
idea	O
of	O
introducing	O
self-loops	O
to	O
produce	O
paths	O
where	O
the	O
gradient	O
can	O
ﬂow	O
for	O
long	O
durations	O
is	O
a	O
core	O
contribution	O
of	O
the	O
initial	O
long	O
short-term	O
memory	O
(	O
lstm	O
)	O
model	B
(	O
hochreiter	O
and	O
schmidhuber	O
1997	O
)	O
.	O
a	O
crucial	O
addition	O
has	O
been	O
to	O
make	O
the	O
weight	O
on	O
this	O
self-loop	O
conditioned	O
on	O
the	O
context	O
,	O
rather	O
than	O
ﬁxed	O
(	O
)	O
.	O
by	O
making	O
the	O
weight	O
of	O
this	O
self-loop	O
gated	O
(	O
controlled	O
by	O
another	O
hidden	O
unit	O
)	O
,	O
the	O
time	O
scale	O
of	O
integration	O
can	O
be	O
changed	O
dynamically	O
.	O
in	O
this	O
case	O
,	O
we	O
mean	O
that	O
even	O
for	O
an	O
lstm	O
with	O
ﬁxed	O
parameters	O
,	O
the	O
time	O
scale	O
of	O
integration	O
can	O
change	O
based	O
on	O
the	O
input	O
sequence	O
,	O
because	O
the	O
time	O
constants	O
are	O
output	O
by	O
the	O
model	B
itself	O
.	O
the	O
lstm	O
has	O
been	O
found	O
extremely	O
successful	O
in	O
many	O
applications	O
,	O
such	O
as	O
unconstrained	O
handwriting	O
recognition	B
(	O
graves	O
)	O
,	O
et	O
al.	O
,	O
2013	O
graves	O
and	O
jaitly	O
2014	O
2014	O
handwriting	O
generation	O
(	O
graves	O
2013	O
)	O
,	O
image	O
captioning	O
(	O
)	O
and	O
parsing	O
(	O
vinyals	O
)	O
,	O
machine	O
translation	O
(	O
sutskever	O
kiros	O
et	O
al	O
.	O
2014b	O
vinyals	O
)	O
,	O
speech	O
recognition	B
(	O
,	O
et	O
al.	O
,	O
2015	O
2014b	O
xu	O
,	O
,	O
)	O
.	O
graves	O
et	O
al.	O
,	O
et	O
al.	O
,	O
et	O
al.	O
,	O
2014a	O
et	O
al.	O
,	O
;	O
;	O
;	O
,	O
2009	O
;	O
10.16	O
et	O
al.	O
,	O
2014a	O
2013	O
pascanu	O
the	O
lstm	O
block	O
diagram	O
is	O
illustrated	O
in	O
ﬁgure	O
.	O
the	O
corresponding	O
forward	O
propagation	O
equations	O
are	O
given	O
below	O
,	O
in	O
the	O
case	O
of	O
a	O
shallow	O
recurrent	O
network	O
architecture	O
.	O
deeper	O
architectures	O
have	O
also	O
been	O
successfully	O
used	O
(	O
graves	O
et	O
al.	O
,	O
)	O
.	O
instead	O
of	O
a	O
unit	O
that	O
simply	O
applies	O
an	O
element-	O
wise	O
nonlinearity	O
to	O
the	O
aﬃne	O
transformation	O
of	O
inputs	O
and	O
recurrent	O
units	O
,	O
lstm	O
recurrent	O
networks	O
have	O
“	O
lstm	O
cells	O
”	O
that	O
have	O
an	O
internal	O
recurrence	O
(	O
a	O
self-loop	O
)	O
,	O
in	O
addition	O
to	O
the	O
outer	O
recurrence	O
of	O
the	O
rnn	O
.	O
each	O
cell	O
has	O
the	O
same	O
inputs	O
and	O
outputs	O
as	O
an	O
ordinary	O
recurrent	O
network	O
,	O
but	O
has	O
more	O
parameters	O
and	O
a	O
system	O
of	O
gating	O
units	O
that	O
controls	O
the	O
ﬂow	O
of	O
information	O
.	O
the	O
most	O
important	O
component	O
is	O
the	O
state	O
unit	O
s	O
(	O
)	O
t	O
that	O
has	O
a	O
linear	O
self-loop	O
similar	O
to	O
the	O
leaky	O
units	O
described	O
in	O
the	O
previous	O
section	O
.	O
however	O
,	O
here	O
,	O
the	O
self-loop	O
weight	O
(	O
or	O
the	O
associated	O
time	O
constant	O
)	O
is	O
controlled	O
by	O
a	O
forget	O
gate	O
unit	O
f	O
(	O
)	O
t	O
(	O
for	O
time	O
step	O
t	O
and	O
cell	O
)	O
,	O
that	O
sets	O
this	O
weight	O
to	O
a	O
value	O
between	O
0	O
and	O
1	O
via	O
a	O
sigmoid	O
unit	O
:	O
	O
	O
i	O
i	O
i	O
	O
	O
f	O
(	O
)	O
t	O
i	O
=	O
σ	O
bf	O
i	O
+	O
i	O
,	O
j	O
x	O
(	O
)	O
t	O
u	O
f	O
j	O
+	O
j	O
j	O
−	O
t	O
1	O
)	O
j	O
wf	O
i	O
,	O
j	O
h	O
(	O
,	O
(	O
10.40	O
)	O
is	O
the	O
current	O
input	O
vector	O
and	O
h	O
(	O
)	O
t	O
where	O
x	O
(	O
)	O
t	O
is	O
the	O
current	O
hidden	O
layer	O
vector	O
,	O
containing	O
the	O
outputs	O
of	O
all	O
the	O
lstm	O
cells	O
,	O
and	O
bf	O
,	O
uf	O
,	O
w	O
f	O
are	O
respectively	O
biases	O
,	O
input	O
weights	O
and	O
recurrent	O
weights	O
for	O
the	O
forget	O
gates	O
.	O
the	O
lstm	O
cell	O
410	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
	O
	O
	O
	O
internal	O
state	O
is	O
thus	O
updated	O
as	O
follows	O
,	O
but	O
with	O
a	O
conditional	O
self-loop	O
weight	O
f	O
(	O
)	O
t	O
i	O
:	O
−	O
1	O
)	O
t	O
i	O
=	O
f	O
(	O
)	O
t	O
s	O
(	O
)	O
t	O
i	O
s	O
(	O
i	O
+	O
g	O
(	O
)	O
t	O
i	O
σ	O
b	O
i	O
+	O
ui	O
,	O
jx	O
(	O
)	O
t	O
j	O
+	O
j	O
j	O
−	O
1	O
)	O
t	O
wi	O
,	O
jh	O
(	O
j	O
,	O
(	O
10.41	O
)	O
where	O
b	O
,	O
u	O
and	O
w	O
respectively	O
denote	O
the	O
biases	O
,	O
input	O
weights	O
and	O
recurrent	O
weights	O
into	O
the	O
lstm	O
cell	O
.	O
the	O
external	O
input	O
gate	O
unit	O
g	O
(	O
)	O
t	O
is	O
computed	O
similarly	O
to	O
the	O
forget	O
gate	O
(	O
with	O
a	O
sigmoid	O
unit	O
to	O
obtain	O
a	O
gating	O
value	O
between	O
0	O
and	O
1	O
)	O
,	O
but	O
with	O
its	O
own	O
parameters	O
:	O
	O
	O
i	O
g	O
(	O
)	O
t	O
i	O
=	O
σ	O
bg	O
i	O
+	O
i	O
,	O
jx	O
(	O
)	O
t	O
u	O
g	O
j	O
+	O
−	O
t	O
1	O
)	O
j	O
w	O
g	O
i	O
,	O
jh	O
(	O
j	O
.	O
(	O
10.42	O
)	O
the	O
output	O
h	O
(	O
)	O
t	O
which	O
also	O
uses	O
a	O
sigmoid	O
unit	O
for	O
gating	O
:	O
i	O
of	O
the	O
lstm	O
cell	O
can	O
also	O
be	O
shut	O
oﬀ	O
,	O
via	O
the	O
output	O
gate	O
q	O
(	O
)	O
t	O
,	O
	O
	O
	O
	O
j	O
	O
	O
	O
	O
h	O
(	O
)	O
t	O
i	O
=	O
tanh	B
s	O
(	O
)	O
t	O
i	O
q	O
(	O
)	O
t	O
i	O
q	O
(	O
)	O
t	O
i	O
=	O
σ	O
bo	O
i	O
+	O
i	O
,	O
jx	O
(	O
)	O
t	O
u	O
o	O
j	O
+	O
j	O
j	O
−	O
t	O
1	O
)	O
j	O
w	O
o	O
i	O
,	O
j	O
h	O
(	O
i	O
(	O
10.43	O
)	O
(	O
10.44	O
)	O
which	O
has	O
parameters	O
bo	O
,	O
u	O
o	O
,	O
w	O
o	O
for	O
its	O
biases	O
,	O
input	O
weights	O
and	O
recurrent	O
weights	O
,	O
respectively	O
.	O
among	O
the	O
variants	O
,	O
one	O
can	O
choose	O
to	O
use	O
the	O
cell	O
state	O
s	O
(	O
)	O
t	O
as	O
an	O
extra	O
input	O
(	O
with	O
its	O
weight	O
)	O
into	O
the	O
three	O
gates	O
of	O
the	O
i-th	O
unit	O
,	O
as	O
shown	O
in	O
ﬁgure	O
.	O
this	O
would	O
require	O
three	O
additional	O
parameters	O
.	O
10.16	O
i	O
lstm	O
networks	O
have	O
been	O
shown	O
to	O
learn	O
long-term	O
dependencies	O
more	O
easily	O
than	O
the	O
simple	O
recurrent	O
architectures	O
,	O
ﬁrst	O
on	O
artiﬁcial	O
data	O
sets	O
designed	O
for	O
testing	O
the	O
ability	O
to	O
learn	O
long-term	O
dependencies	O
(	O
bengio	O
et	O
al	O
.	O
1994	O
hochreiter	O
)	O
,	O
then	O
on	O
challenging	O
sequence	O
and	O
schmidhuber	O
1997	O
hochreiter	O
processing	O
tasks	O
where	O
state-of-the-art	O
performance	O
was	O
obtained	O
(	O
graves	O
2012	O
;	O
graves	O
)	O
.	O
variants	O
and	O
alternatives	O
to	O
the	O
lstm	O
have	O
been	O
studied	O
and	O
used	O
and	O
are	O
discussed	O
next	O
.	O
2013	O
sutskever	O
et	O
al.	O
,	O
et	O
al.	O
,	O
et	O
al.	O
,	O
2001	O
2014	O
,	O
;	O
,	O
;	O
;	O
,	O
10.10.2	O
other	O
gated	O
rnns	O
which	O
pieces	O
of	O
the	O
lstm	O
architecture	O
are	O
actually	O
necessary	O
?	O
what	O
other	O
successful	O
architectures	O
could	O
be	O
designed	O
that	O
allow	O
the	O
network	O
to	O
dynamically	O
control	O
the	O
time	O
scale	O
and	O
forgetting	O
behavior	O
of	O
diﬀerent	O
units	O
?	O
411	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
et	O
al.	O
,	O
some	O
answers	O
to	O
these	O
questions	O
are	O
given	O
with	O
the	O
recent	O
work	B
on	O
gated	O
rnns	O
,	O
cho	O
et	O
al	O
.	O
2014b	O
;	O
whose	O
units	O
are	O
also	O
known	O
as	O
gated	O
recurrent	O
units	O
or	O
grus	O
(	O
chung	O
)	O
.	O
the	O
main	O
2015	O
et	O
al.	O
,	O
diﬀerence	O
with	O
the	O
lstm	O
is	O
that	O
a	O
single	O
gating	O
unit	O
simultaneously	O
controls	O
the	O
forgetting	O
factor	O
and	O
the	O
decision	O
to	O
update	O
the	O
state	O
unit	O
.	O
the	O
update	O
equations	O
are	O
the	O
following	O
:	O
2014	O
2015a	O
jozefowicz	O
2015	O
chrupala	O
	O
et	O
al.	O
,	O
,	O
;	O
,	O
;	O
	O
−	O
1	O
)	O
t	O
h	O
(	O
)	O
t	O
i	O
=	O
u	O
(	O
i	O
−	O
1	O
)	O
t	O
h	O
(	O
i	O
−	O
+	O
(	O
1	O
−	O
1	O
)	O
t	O
u	O
(	O
i	O
−	O
t	O
1	O
)	O
ui	O
,	O
jx	O
(	O
j	O
+	O
−	O
t	O
1	O
)	O
wi	O
,	O
jr	O
(	O
j	O
−	O
t	O
1	O
)	O
h	O
(	O
j	O
,	O
(	O
10.45	O
)	O
where	O
u	O
stands	O
for	O
“	O
update	O
”	O
gate	O
and	O
r	O
for	O
“	O
reset	O
”	O
gate	O
.	O
their	O
value	O
is	O
deﬁned	O
as	O
usual	O
:	O
bu	O
i	O
+	O
i	O
,	O
jx	O
(	O
)	O
t	O
uu	O
j	O
+	O
w	O
u	O
i	O
,	O
j	O
h	O
(	O
)	O
t	O
j	O
(	O
10.46	O
)	O
u	O
(	O
)	O
t	O
i	O
=	O
σ	O
and	O
	O
	O
	O
j	O
)	O
σ	O
bi	O
+	O
j	O
	O
	O
	O
	O
j	O
	O
	O
	O
j	O
r	O
(	O
)	O
t	O
i	O
=	O
σ	O
b	O
r	O
i	O
+	O
i	O
,	O
j	O
x	O
(	O
)	O
t	O
u	O
r	O
j	O
+	O
j	O
j	O
w	O
r	O
i	O
,	O
jh	O
(	O
)	O
t	O
j	O
.	O
(	O
10.47	O
)	O
the	O
reset	O
and	O
updates	O
gates	O
can	O
individually	O
“	O
ignore	O
”	O
parts	O
of	O
the	O
state	O
vector	O
.	O
the	O
update	O
gates	O
act	O
like	O
conditional	O
leaky	O
integrators	O
that	O
can	O
linearly	O
gate	O
any	O
dimension	O
,	O
thus	O
choosing	O
to	O
copy	O
it	O
(	O
at	O
one	O
extreme	O
of	O
the	O
sigmoid	O
)	O
or	O
completely	O
ignore	O
it	O
(	O
at	O
the	O
other	O
extreme	O
)	O
by	O
replacing	O
it	O
by	O
the	O
new	O
“	O
target	O
state	O
”	O
value	O
(	O
towards	O
which	O
the	O
leaky	O
integrator	O
wants	O
to	O
converge	O
)	O
.	O
the	O
reset	O
gates	O
control	O
which	O
parts	O
of	O
the	O
state	O
get	O
used	O
to	O
compute	O
the	O
next	O
target	O
state	O
,	O
introducing	O
an	O
additional	O
nonlinear	O
eﬀect	O
in	O
the	O
relationship	O
between	O
past	O
state	O
and	O
future	O
state	O
.	O
many	O
more	O
variants	O
around	O
this	O
theme	O
can	O
be	O
designed	O
.	O
for	O
example	O
the	O
reset	O
gate	O
(	O
or	O
forget	O
gate	O
)	O
output	O
could	O
be	O
shared	O
across	O
multiple	O
hidden	O
units	O
.	O
alternately	O
,	O
the	O
product	O
of	O
a	O
global	O
gate	O
(	O
covering	O
a	O
whole	O
group	O
of	O
units	O
,	O
such	O
as	O
an	O
entire	O
layer	O
)	O
and	O
a	O
local	O
gate	O
(	O
per	O
unit	O
)	O
could	O
be	O
used	O
to	O
combine	O
global	O
control	O
and	O
local	O
control	O
.	O
however	O
,	O
several	O
investigations	O
over	O
architectural	O
variations	O
of	O
the	O
lstm	O
and	O
gru	O
found	O
no	O
variant	O
that	O
would	O
clearly	O
beat	O
both	O
of	O
these	O
2015	O
greﬀ	O
across	O
a	O
wide	O
range	O
of	O
tasks	O
(	O
)	O
found	O
that	O
a	O
crucial	O
ingredient	O
is	O
the	O
forget	O
gate	O
,	O
while	O
et	O
al	O
.	O
(	O
jozefowicz	O
et	O
al	O
.	O
(	O
)	O
found	O
that	O
adding	O
a	O
bias	O
of	O
1	O
to	O
the	O
lstm	O
forget	O
gate	O
,	O
a	O
practice	O
advocated	O
by	O
gers	O
et	O
al	O
.	O
2000	O
)	O
,	O
makes	O
the	O
lstm	O
as	O
strong	O
as	O
the	O
best	O
of	O
the	O
explored	O
architectural	O
variants	O
.	O
greﬀ	O
et	O
al	O
.	O
2015	O
jozefowicz	O
2015	O
2015	O
et	O
al.	O
,	O
)	O
.	O
(	O
,	O
;	O
412	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
10.11	O
optimization	O
for	O
long-term	O
dependencies	O
8.2.5	O
section	O
problems	O
that	O
occur	O
when	O
optimizing	O
rnns	O
over	O
many	O
time	O
steps	O
.	O
have	O
described	O
the	O
vanishing	O
and	O
exploding	O
gradient	O
and	O
section	O
10.7	O
(	O
(	O
an	O
interesting	O
idea	O
proposed	O
by	O
martens	O
and	O
sutskever	O
2011	O
)	O
is	O
that	O
second	O
derivatives	O
may	O
vanish	O
at	O
the	O
same	O
time	O
that	O
ﬁrst	O
derivatives	O
vanish	O
.	O
second-order	O
optimization	O
algorithms	O
may	O
roughly	O
be	O
understood	O
as	O
dividing	O
the	O
ﬁrst	O
derivative	O
by	O
the	O
second	O
derivative	O
(	O
in	O
higher	O
dimension	O
,	O
multiplying	O
the	O
gradient	O
by	O
the	O
inverse	O
hessian	O
)	O
.	O
if	O
the	O
second	O
derivative	O
shrinks	O
at	O
a	O
similar	O
rate	O
to	O
the	O
ﬁrst	O
derivative	O
,	O
then	O
the	O
ratio	O
of	O
ﬁrst	O
and	O
second	O
derivatives	O
may	O
remain	O
relatively	O
constant	O
.	O
unfortunately	O
,	O
second-order	O
methods	O
have	O
many	O
drawbacks	O
,	O
including	O
high	O
computational	O
cost	O
,	O
the	O
need	O
for	O
a	O
large	O
minibatch	O
,	O
and	O
a	O
tendency	O
to	O
be	O
attracted	O
to	O
saddle	O
points	O
.	O
martens	O
and	O
sutskever	O
2011	O
)	O
found	O
promising	O
results	O
using	O
second-order	O
methods	O
.	O
later	O
,	O
sutskever	O
)	O
found	O
that	O
simpler	O
et	O
al	O
.	O
(	O
2013	O
methods	O
such	O
as	O
nesterov	O
momentum	O
with	O
careful	O
initialization	O
could	O
achieve	O
similar	O
results	O
.	O
see	O
sutskever	O
2012	O
)	O
for	O
more	O
detail	O
.	O
both	O
of	O
these	O
approaches	O
have	O
largely	O
been	O
replaced	O
by	O
simply	O
using	O
sgd	O
(	O
even	O
without	O
momentum	O
)	O
applied	O
to	O
lstms	O
.	O
this	O
is	O
part	O
of	O
a	O
continuing	O
theme	O
in	O
machine	O
learning	O
that	O
it	O
is	O
often	O
much	O
easier	O
to	O
design	O
a	O
model	B
that	O
is	O
easy	O
to	O
optimize	O
than	O
it	O
is	O
to	O
design	O
a	O
more	O
powerful	O
optimization	O
algorithm	O
.	O
(	O
10.11.1	O
clipping	O
gradients	O
8.2.4	O
as	O
discussed	O
in	O
section	O
,	O
strongly	O
nonlinear	O
functions	O
such	O
as	O
those	O
computed	O
by	O
a	O
recurrent	O
net	O
over	O
many	O
time	O
steps	O
tend	O
to	O
have	O
derivatives	O
that	O
can	O
be	O
either	O
very	O
large	O
or	O
very	O
small	O
in	O
magnitude	O
.	O
this	O
is	O
illustrated	O
in	O
ﬁgure	O
and	O
ﬁgure	O
,	O
in	O
which	O
we	O
see	O
that	O
the	O
objective	O
function	O
(	O
as	O
a	O
function	O
of	O
the	O
parameters	O
)	O
has	O
a	O
“	O
landscape	O
”	O
in	O
which	O
one	O
ﬁnds	O
“	O
cliﬀs	O
”	O
:	O
wide	O
and	O
rather	O
ﬂat	O
regions	O
separated	O
by	O
tiny	O
regions	O
where	O
the	O
objective	O
function	O
changes	O
quickly	O
,	O
forming	O
a	O
kind	O
of	O
cliﬀ	O
.	O
10.17	O
8.3	O
the	O
diﬃculty	O
that	O
arises	O
is	O
that	O
when	O
the	O
parameter	O
gradient	O
is	O
very	O
large	O
,	O
a	O
gradient	O
descent	B
parameter	O
update	O
could	O
throw	O
the	O
parameters	O
very	O
far	O
,	O
into	O
a	O
region	O
where	O
the	O
objective	O
function	O
is	O
larger	O
,	O
undoing	O
much	O
of	O
the	O
work	B
that	O
had	O
been	O
done	O
to	O
reach	O
the	O
current	O
solution	O
.	O
the	O
gradient	O
tells	O
us	O
the	O
direction	O
that	O
corresponds	O
to	O
the	O
steepest	O
descent	B
within	O
an	O
inﬁnitesimal	O
region	O
surrounding	O
the	O
current	O
parameters	O
.	O
outside	O
of	O
this	O
inﬁnitesimal	O
region	O
,	O
the	O
cost	O
function	O
may	O
begin	O
to	O
curve	O
back	O
upwards	O
.	O
the	O
update	O
must	O
be	O
chosen	O
to	O
be	O
small	O
enough	O
to	O
avoid	O
traversing	O
too	O
much	O
upward	O
curvature	O
.	O
we	O
typically	O
use	O
learning	O
rates	O
that	O
413	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
decay	O
slowly	O
enough	O
that	O
consecutive	O
steps	O
have	O
approximately	O
the	O
same	O
learning	O
rate	O
.	O
a	O
step	O
size	O
that	O
is	O
appropriate	O
for	O
a	O
relatively	O
linear	O
part	O
of	O
the	O
landscape	O
is	O
often	O
inappropriate	O
and	O
causes	O
uphill	O
motion	O
if	O
we	O
enter	O
a	O
more	O
curved	O
part	O
of	O
the	O
landscape	O
on	O
the	O
next	O
step	O
.	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
figure	O
10.17	O
:	O
example	O
of	O
the	O
eﬀect	O
of	O
gradient	O
clipping	O
in	O
a	O
recurrent	O
network	O
with	O
two	O
parameters	O
w	O
and	O
b.	O
gradient	O
clipping	O
can	O
make	O
gradient	O
descent	B
perform	O
more	O
reasonably	O
in	O
the	O
vicinity	O
of	O
extremely	O
steep	O
cliﬀs	O
.	O
these	O
steep	O
cliﬀs	O
commonly	O
occur	O
in	O
recurrent	O
networks	O
near	O
where	O
a	O
recurrent	O
network	O
behaves	O
approximately	O
linearly	O
.	O
the	O
cliﬀ	O
is	O
exponentially	O
steep	O
in	O
the	O
number	O
of	O
time	O
steps	O
because	O
the	O
weight	O
matrix	O
is	O
multiplied	O
by	O
itself	O
once	O
for	O
each	O
time	O
step	O
.	O
(	O
left	O
)	O
gradient	O
descent	B
without	O
gradient	O
clipping	O
overshoots	O
the	O
bottom	O
of	O
this	O
small	O
ravine	O
,	O
then	O
receives	O
a	O
very	O
large	O
gradient	O
from	O
the	O
cliﬀ	O
face	O
.	O
the	O
large	O
gradient	O
catastrophically	O
propels	O
the	O
parameters	O
outside	O
the	O
axes	O
of	O
the	O
plot	O
.	O
gradient	O
descent	B
with	O
gradient	O
clipping	O
has	O
a	O
more	O
moderate	O
reaction	O
to	O
the	O
cliﬀ	O
.	O
while	O
it	O
does	O
ascend	O
the	O
cliﬀ	O
face	O
,	O
the	O
step	O
size	O
is	O
restricted	O
so	O
that	O
it	O
can	O
not	O
be	O
propelled	O
away	O
from	O
steep	O
region	O
near	O
the	O
solution	O
.	O
figure	O
adapted	O
with	O
permission	O
from	O
pascanu	O
et	O
al	O
.	O
(	O
(	O
right	O
)	O
2013	O
)	O
.	O
a	O
simple	O
type	O
of	O
solution	O
has	O
been	O
in	O
use	O
by	O
practitioners	O
for	O
many	O
years	O
:	O
clipping	O
the	O
gradient	O
.	O
there	O
are	O
diﬀerent	O
instances	O
of	O
this	O
idea	O
(	O
mikolov	O
2012	O
;	O
pascanu	O
)	O
.	O
one	O
option	O
is	O
to	O
clip	O
the	O
parameter	O
gradient	O
from	O
a	O
minibatch	O
)	O
just	O
before	O
the	O
parameter	O
update	O
.	O
another	O
is	O
to	O
clip	O
element-wise	O
(	O
mikolov	O
2012	O
the	O
norm	O
)	O
just	O
before	O
the	O
parameter	O
update	O
:	O
et	O
al.	O
,	O
||	O
||	O
g	O
of	O
the	O
gradient	O
g	O
(	O
pascanu	O
et	O
al.	O
,	O
2013	O
2013	O
,	O
,	O
←	O
gv||	O
||	O
g	O
g	O
(	O
10.48	O
)	O
(	O
10.49	O
)	O
||	O
||	O
g	O
>	O
v	O
if	O
414	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
where	O
v	O
is	O
the	O
norm	O
threshold	O
and	O
g	O
is	O
used	O
to	O
update	O
parameters	O
.	O
because	O
the	O
gradient	O
of	O
all	O
the	O
parameters	O
(	O
including	O
diﬀerent	O
groups	O
of	O
parameters	O
,	O
such	O
as	O
weights	O
and	O
biases	O
)	O
is	O
renormalized	O
jointly	O
with	O
a	O
single	O
scaling	O
factor	O
,	O
the	O
latter	O
method	O
has	O
the	O
advantage	O
that	O
it	O
guarantees	O
that	O
each	O
step	O
is	O
still	O
in	O
the	O
gradient	O
direction	O
,	O
but	O
experiments	O
suggest	O
that	O
both	O
forms	O
work	B
similarly	O
.	O
although	O
the	O
parameter	O
update	O
has	O
the	O
same	O
direction	O
as	O
the	O
true	O
gradient	O
,	O
with	O
gradient	O
norm	O
clipping	O
,	O
the	O
parameter	O
update	O
vector	O
norm	O
is	O
now	O
bounded	O
.	O
this	O
bounded	O
gradient	O
avoids	O
performing	O
a	O
detrimental	O
step	O
when	O
the	O
gradient	O
explodes	O
.	O
in	O
fact	O
,	O
even	O
simply	O
taking	O
a	O
random	O
step	O
when	O
the	O
gradient	O
magnitude	O
is	O
above	O
a	O
threshold	O
tends	O
to	O
work	B
almost	O
as	O
well	O
.	O
if	O
the	O
explosion	O
is	O
so	O
severe	O
that	O
the	O
gradient	O
is	O
numerically	O
inf	O
or	O
nan	O
(	O
considered	O
inﬁnite	O
or	O
not-a-number	O
)	O
,	O
then	O
a	O
random	O
step	O
of	O
size	O
v	O
can	O
be	O
taken	O
and	O
will	O
typically	O
move	O
away	O
from	O
the	O
numerically	O
unstable	O
conﬁguration	O
.	O
clipping	O
the	O
gradient	O
norm	O
per-minibatch	O
will	O
not	O
change	O
the	O
direction	O
of	O
the	O
gradient	O
for	O
an	O
individual	O
minibatch	O
.	O
however	O
,	O
taking	O
the	O
average	O
of	O
the	O
norm-clipped	O
gradient	O
from	O
many	O
minibatches	O
is	O
not	O
equivalent	O
to	O
clipping	O
the	O
norm	O
of	O
the	O
true	O
gradient	O
(	O
the	O
gradient	O
formed	O
from	O
using	O
all	O
examples	O
)	O
.	O
examples	O
that	O
have	O
large	O
gradient	O
norm	O
,	O
as	O
well	O
as	O
examples	O
that	O
appear	O
in	O
the	O
same	O
minibatch	O
as	O
such	O
examples	O
,	O
will	O
have	O
their	O
contribution	O
to	O
the	O
ﬁnal	O
direction	O
diminished	O
.	O
this	O
stands	O
in	O
contrast	O
to	O
traditional	O
minibatch	O
gradient	O
descent	B
,	O
where	O
the	O
true	O
gradient	O
direction	O
is	O
equal	O
to	O
the	O
average	O
over	O
all	O
minibatch	O
gradients	O
.	O
put	O
another	O
way	O
,	O
traditional	O
stochastic	O
gradient	O
descent	B
uses	O
an	O
unbiased	O
estimate	O
of	O
the	O
gradient	O
,	O
while	O
gradient	O
descent	B
with	O
norm	O
clipping	O
introduces	O
a	O
heuristic	O
bias	O
that	O
we	O
know	O
empirically	O
to	O
be	O
useful	O
.	O
with	O
element-	O
wise	O
clipping	O
,	O
the	O
direction	O
of	O
the	O
update	O
is	O
not	O
aligned	O
with	O
the	O
true	O
gradient	O
or	O
the	O
minibatch	O
gradient	O
,	O
but	O
it	O
is	O
still	O
a	O
descent	B
direction	O
.	O
it	O
has	O
also	O
been	O
proposed	O
(	O
graves	O
2013	O
)	O
to	O
clip	O
the	O
back-propagated	O
gradient	O
(	O
with	O
respect	O
to	O
hidden	O
units	O
)	O
but	O
no	O
comparison	O
has	O
been	O
published	O
between	O
these	O
variants	O
;	O
we	O
conjecture	O
that	O
all	O
these	O
methods	O
behave	O
similarly	O
.	O
,	O
10.11.2	O
regularizing	O
to	O
encourage	O
information	O
flow	O
gradient	O
clipping	O
helps	O
to	O
deal	O
with	O
exploding	O
gradients	O
,	O
but	O
it	O
does	O
not	O
help	O
with	O
vanishing	O
gradients	O
.	O
to	O
address	O
vanishing	O
gradients	O
and	O
better	O
capture	O
long-term	O
dependencies	O
,	O
we	O
discussed	O
the	O
idea	O
of	O
creating	O
paths	O
in	O
the	O
computational	O
graph	O
of	O
the	O
unfolded	O
recurrent	O
architecture	O
along	O
which	O
the	O
product	O
of	O
gradients	O
associated	O
with	O
arcs	O
is	O
near	O
1.	O
one	O
approach	O
to	O
achieve	O
this	O
is	O
with	O
lstms	O
and	O
other	O
self-	O
loops	O
and	O
gating	O
mechanisms	O
,	O
described	O
above	O
in	O
section	O
.	O
another	O
idea	O
is	O
∇	O
to	O
regularize	O
or	O
constrain	O
the	O
parameters	O
so	O
as	O
to	O
encourage	O
“	O
information	O
ﬂow.	O
”	O
h	O
(	O
)	O
t	O
l	O
being	O
back-propagated	O
to	O
in	O
particular	O
,	O
we	O
would	O
like	O
the	O
gradient	O
vector	O
10.10	O
415	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
maintain	O
its	O
magnitude	O
,	O
even	O
if	O
the	O
loss	O
function	O
only	O
penalizes	O
the	O
output	O
at	O
the	O
end	O
of	O
the	O
sequence	O
.	O
formally	O
,	O
we	O
want	O
to	O
be	O
as	O
large	O
as	O
∇	O
(	O
∂h	O
(	O
)	O
t	O
−	O
∂h	O
(	O
1	O
)	O
t	O
h	O
(	O
)	O
t	O
l	O
)	O
∇	O
h	O
(	O
)	O
t	O
l.	O
	O
	O
	O
et	O
al	O
.	O
(	O
|	O
∇	O
(	O
	O
	O
(	O
10.50	O
)	O
(	O
10.51	O
)	O
with	O
this	O
objective	O
,	O
pascanu	O
2013	O
)	O
propose	O
the	O
following	O
regularizer	O
:	O
|	O
−	O
h	O
(	O
)	O
t	O
l	O
)	O
∂h	O
(	O
)	O
t	O
−	O
||∇	O
||	O
∂h	O
(	O
t	O
h	O
(	O
)	O
t	O
l	O
1	O
)	O
2	O
1	O
.	O
(	O
10.52	O
)	O
ω	O
=	O
t	O
computing	O
the	O
gradient	O
of	O
this	O
regularizer	O
may	O
appear	O
diﬃcult	O
,	O
but	O
pascanu	O
∇	O
)	O
propose	O
an	O
approximation	O
in	O
which	O
we	O
consider	O
the	O
back-propagated	O
2013	O
et	O
al	O
.	O
(	O
h	O
(	O
)	O
t	O
l	O
as	O
if	O
they	O
were	O
constants	O
(	O
for	O
the	O
purpose	O
of	O
this	O
regularizer	O
,	O
so	O
vectors	O
that	O
there	O
is	O
no	O
need	O
to	O
back-propagate	O
through	O
them	O
)	O
.	O
the	O
experiments	O
with	O
this	O
regularizer	O
suggest	O
that	O
,	O
if	O
combined	O
with	O
the	O
norm	O
clipping	O
heuristic	O
(	O
which	O
handles	O
gradient	O
explosion	O
)	O
,	O
the	O
regularizer	O
can	O
considerably	O
increase	O
the	O
span	O
of	O
the	O
dependencies	O
that	O
an	O
rnn	O
can	O
learn	O
.	O
because	O
it	O
keeps	O
the	O
rnn	O
dynamics	O
on	O
the	O
edge	O
of	O
explosive	O
gradients	O
,	O
the	O
gradient	O
clipping	O
is	O
particularly	O
important	O
.	O
without	O
gradient	O
clipping	O
,	O
gradient	O
explosion	O
prevents	O
learning	O
from	O
succeeding	O
.	O
a	O
key	O
weakness	O
of	O
this	O
approach	O
is	O
that	O
it	O
is	O
not	O
as	O
eﬀective	O
as	O
the	O
lstm	O
for	O
tasks	O
where	O
data	O
is	O
abundant	O
,	O
such	O
as	O
language	O
modeling	O
.	O
10.12	O
explicit	O
memory	O
intelligence	O
requires	O
knowledge	O
and	O
acquiring	O
knowledge	O
can	O
be	O
done	O
via	O
learning	O
,	O
which	O
has	O
motivated	O
the	O
development	O
of	O
large-scale	O
deep	O
architectures	O
.	O
however	O
,	O
there	O
are	O
diﬀerent	O
kinds	O
of	O
knowledge	O
.	O
some	O
knowledge	O
can	O
be	O
implicit	O
,	O
sub-	O
conscious	O
,	O
and	O
diﬃcult	O
to	O
verbalize—such	O
as	O
how	O
to	O
walk	O
,	O
or	O
how	O
a	O
dog	O
looks	O
diﬀerent	O
from	O
a	O
cat	O
.	O
other	O
knowledge	O
can	O
be	O
explicit	O
,	O
declarative	O
,	O
and	O
relatively	O
straightforward	O
to	O
put	O
into	O
words—every	O
day	O
commonsense	O
knowledge	O
,	O
like	O
“	O
a	O
cat	O
is	O
a	O
kind	O
of	O
animal	O
,	O
”	O
or	O
very	O
speciﬁc	O
facts	O
that	O
you	O
need	O
to	O
know	O
to	O
accomplish	O
your	O
current	O
goals	O
,	O
like	O
“	O
the	O
meeting	O
with	O
the	O
sales	O
team	O
is	O
at	O
3:00	O
pm	O
in	O
room	O
141.	O
”	O
neural	O
networks	O
excel	O
at	O
storing	O
implicit	O
knowledge	O
.	O
however	O
,	O
they	O
struggle	O
to	O
memorize	O
facts	O
.	O
stochastic	O
gradient	O
descent	B
requires	O
many	O
presentations	O
of	O
the	O
416	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
memory	O
cells	O
writing	O
mechanism	O
reading	O
mechanism	O
task	O
network	O
,	O
controlling	O
the	O
memory	O
figure	O
10.18	O
:	O
a	O
schematic	O
of	O
an	O
example	O
of	O
a	O
network	O
with	O
an	O
explicit	O
memory	O
,	O
capturing	O
some	O
of	O
the	O
key	O
design	O
elements	O
of	O
the	O
neural	O
turing	O
machine	O
.	O
in	O
this	O
diagram	O
we	O
distinguish	O
the	O
“	O
representation	O
”	O
part	O
of	O
the	O
model	B
(	O
the	O
“	O
task	O
network	O
,	O
”	O
here	O
a	O
recurrent	O
net	O
in	O
the	O
bottom	O
)	O
from	O
the	O
“	O
memory	O
”	O
part	O
of	O
the	O
model	B
(	O
the	O
set	O
of	O
cells	O
)	O
,	O
which	O
can	O
store	O
facts	O
.	O
the	O
task	O
network	O
learns	O
to	O
“	O
control	O
”	O
the	O
memory	O
,	O
deciding	O
where	O
to	O
read	O
from	O
and	O
where	O
to	O
write	O
to	O
within	O
the	O
memory	O
(	O
through	O
the	O
reading	O
and	O
writing	O
mechanisms	O
,	O
indicated	O
by	O
bold	O
arrows	O
pointing	O
at	O
the	O
reading	O
and	O
writing	O
addresses	O
)	O
.	O
417	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
2014b	O
et	O
al	O
.	O
(	O
same	O
input	O
before	O
it	O
can	O
be	O
stored	O
in	O
a	O
neural	O
network	O
parameters	O
,	O
and	O
even	O
then	O
,	O
that	O
input	O
will	O
not	O
be	O
stored	O
especially	O
precisely	O
.	O
graves	O
)	O
hypothesized	O
that	O
this	O
is	O
because	O
neural	O
networks	O
lack	O
the	O
equivalent	O
of	O
the	O
working	O
memory	O
system	O
that	O
allows	O
human	O
beings	O
to	O
explicitly	O
hold	O
and	O
manipulate	O
pieces	O
of	O
information	O
that	O
are	O
relevant	O
to	O
achieving	O
some	O
goal	O
.	O
such	O
explicit	O
memory	O
components	O
would	O
allow	O
our	O
systems	O
not	O
only	O
to	O
rapidly	O
and	O
“	O
intentionally	O
”	O
store	O
and	O
retrieve	O
speciﬁc	O
facts	O
but	O
also	O
to	O
sequentially	O
reason	O
with	O
them	O
.	O
the	O
need	O
for	O
neural	O
networks	O
that	O
can	O
process	O
information	O
in	O
a	O
sequence	O
of	O
steps	O
,	O
changing	O
the	O
way	O
the	O
input	O
is	O
fed	O
into	O
the	O
network	O
at	O
each	O
step	O
,	O
has	O
long	O
been	O
recognized	O
as	O
important	O
for	O
the	O
ability	O
to	O
reason	O
rather	O
than	O
to	O
make	O
automatic	O
,	O
intuitive	O
responses	O
to	O
the	O
input	O
(	O
hinton	O
1990	O
)	O
.	O
,	O
2014	O
et	O
al	O
.	O
(	O
2014b	O
et	O
al	O
.	O
(	O
to	O
resolve	O
this	O
diﬃculty	O
,	O
weston	O
)	O
introduced	O
memory	O
networks	O
that	O
include	O
a	O
set	O
of	O
memory	O
cells	O
that	O
can	O
be	O
accessed	O
via	O
an	O
addressing	O
mecha-	O
nism	O
.	O
memory	O
networks	O
originally	O
required	O
a	O
supervision	O
signal	O
instructing	O
them	O
how	O
to	O
use	O
their	O
memory	O
cells	O
.	O
graves	O
)	O
introduced	O
the	O
neural	O
turing	O
machine	O
,	O
which	O
is	O
able	O
to	O
learn	O
to	O
read	O
from	O
and	O
write	O
arbitrary	O
content	O
to	O
memory	O
cells	O
without	O
explicit	O
supervision	O
about	O
which	O
actions	O
to	O
undertake	O
,	O
and	O
allowed	O
end-to-end	O
training	O
without	O
this	O
supervision	O
signal	O
,	O
via	O
the	O
use	O
of	O
a	O
content-based	O
soft	O
attention	O
mechanism	O
(	O
see	O
)	O
and	O
sec-	O
tion	B
)	O
.	O
this	O
soft	O
addressing	O
mechanism	O
has	O
become	O
standard	O
with	O
other	O
related	O
architectures	O
emulating	O
algorithmic	O
mechanisms	O
in	O
a	O
way	O
that	O
still	O
allows	O
gradient-based	O
optimization	O
(	O
sukhbaatar	O
et	O
al	O
.	O
2015	O
joulin	O
and	O
mikolov	O
2015	O
;	O
kumar	O
;	O
2015a	O
grefenstette	O
bahdanau	O
et	O
al	O
.	O
2015	O
2015	O
vinyals	O
12.4.5.1	O
et	O
al.	O
,	O
et	O
al.	O
,	O
2015	O
)	O
.	O
;	O
et	O
al.	O
,	O
;	O
(	O
,	O
,	O
each	O
memory	O
cell	O
can	O
be	O
thought	O
of	O
as	O
an	O
extension	O
of	O
the	O
memory	O
cells	O
in	O
lstms	O
and	O
grus	O
.	O
the	O
diﬀerence	O
is	O
that	O
the	O
network	O
outputs	O
an	O
internal	O
state	O
that	O
chooses	O
which	O
cell	O
to	O
read	O
from	O
or	O
write	O
to	O
,	O
just	O
as	O
memory	O
accesses	O
in	O
a	O
digital	O
computer	O
read	O
from	O
or	O
write	O
to	O
a	O
speciﬁc	O
address	O
.	O
it	O
is	O
diﬃcult	O
to	O
optimize	O
functions	O
that	O
produce	O
exact	O
,	O
integer	O
addresses	O
.	O
to	O
alleviate	O
this	O
problem	O
,	O
ntms	O
actually	O
read	O
to	O
or	O
write	O
from	O
many	O
memory	O
cells	O
simultaneously	O
.	O
to	O
read	O
,	O
they	O
take	O
a	O
weighted	O
average	O
of	O
many	O
cells	O
.	O
to	O
write	O
,	O
they	O
modify	O
multiple	O
cells	O
by	O
diﬀerent	O
amounts	O
.	O
the	O
coeﬃcients	O
for	O
these	O
operations	O
are	O
chosen	O
to	O
be	O
focused	O
on	O
a	O
small	O
number	O
of	O
cells	O
,	O
for	O
example	O
,	O
by	O
producing	O
them	O
via	O
a	O
softmax	O
function	O
.	O
using	O
these	O
weights	O
with	O
non-zero	O
derivatives	O
allows	O
the	O
functions	O
controlling	O
access	O
to	O
the	O
memory	O
to	O
be	O
optimized	O
using	O
gradient	O
descent	B
.	O
the	O
gradient	O
on	O
these	O
coeﬃcients	O
indicates	O
whether	O
each	O
of	O
them	O
should	O
be	O
increased	O
or	O
decreased	O
,	O
but	O
the	O
gradient	O
will	O
typically	O
be	O
large	O
only	O
for	O
those	O
memory	O
addresses	O
receiving	O
a	O
large	O
coeﬃcient	O
.	O
these	O
memory	O
cells	O
are	O
typically	O
augmented	O
to	O
contain	O
a	O
vector	O
,	O
rather	O
than	O
418	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
the	O
single	O
scalar	O
stored	O
by	O
an	O
lstm	O
or	O
gru	O
memory	O
cell	O
.	O
there	O
are	O
two	O
reasons	O
to	O
increase	O
the	O
size	O
of	O
the	O
memory	O
cell	O
.	O
one	O
reason	O
is	O
that	O
we	O
have	O
increased	O
the	O
cost	O
of	O
accessing	O
a	O
memory	O
cell	O
.	O
we	O
pay	O
the	O
computational	O
cost	O
of	O
producing	O
a	O
coeﬃcient	O
for	O
many	O
cells	O
,	O
but	O
we	O
expect	O
these	O
coeﬃcients	O
to	O
cluster	O
around	O
a	O
small	O
number	O
of	O
cells	O
.	O
by	O
reading	O
a	O
vector	O
value	O
,	O
rather	O
than	O
a	O
scalar	O
value	O
,	O
we	O
can	O
oﬀset	O
some	O
of	O
this	O
cost	O
.	O
another	O
reason	O
to	O
use	O
vector-valued	O
memory	O
cells	O
is	O
that	O
they	O
allow	O
for	O
content-based	O
addressing	O
,	O
where	O
the	O
weight	O
used	O
to	O
read	O
to	O
or	O
write	O
from	O
a	O
cell	O
is	O
a	O
function	O
of	O
that	O
cell	O
.	O
vector-valued	O
cells	O
allow	O
us	O
to	O
retrieve	O
a	O
complete	O
vector-valued	O
memory	O
if	O
we	O
are	O
able	O
to	O
produce	O
a	O
pattern	O
that	O
matches	O
some	O
but	O
not	O
all	O
of	O
its	O
elements	O
.	O
this	O
is	O
analogous	O
to	O
the	O
way	O
that	O
people	O
can	O
recall	O
the	O
lyrics	O
of	O
a	O
song	O
based	O
on	O
a	O
few	O
words	O
.	O
we	O
can	O
think	O
of	O
a	O
content-based	O
read	O
instruction	O
as	O
saying	O
,	O
“	O
retrieve	O
the	O
lyrics	O
of	O
the	O
song	O
that	O
has	O
the	O
chorus	O
‘	O
we	O
all	O
live	O
in	O
a	O
yellow	O
submarine.	O
’	O
”	O
content-based	O
addressing	O
is	O
more	O
useful	O
when	O
we	O
make	O
the	O
objects	O
to	O
be	O
retrieved	O
large—if	O
every	O
letter	O
of	O
the	O
song	O
was	O
stored	O
in	O
a	O
separate	O
memory	O
cell	O
,	O
we	O
would	O
not	O
be	O
able	O
to	O
ﬁnd	O
them	O
this	O
way	O
.	O
by	O
comparison	O
,	O
location-based	O
addressing	O
is	O
not	O
allowed	O
to	O
refer	O
to	O
the	O
content	O
of	O
the	O
memory	O
.	O
we	O
can	O
think	O
of	O
a	O
location-based	O
read	O
instruction	O
as	O
saying	O
“	O
retrieve	O
the	O
lyrics	O
of	O
the	O
song	O
in	O
slot	O
347.	O
”	O
location-based	O
addressing	O
can	O
often	O
be	O
a	O
perfectly	O
sensible	O
mechanism	O
even	O
when	O
the	O
memory	O
cells	O
are	O
small	O
.	O
if	O
the	O
content	O
of	O
a	O
memory	O
cell	O
is	O
copied	O
(	O
not	O
forgotten	O
)	O
at	O
most	O
time	O
steps	O
,	O
then	O
the	O
information	O
it	O
contains	O
can	O
be	O
propagated	O
forward	O
in	O
time	O
and	O
the	O
gradients	O
propagated	O
backward	O
in	O
time	O
without	O
either	O
vanishing	O
or	O
exploding	O
.	O
10.18	O
the	O
explicit	O
memory	O
approach	O
is	O
illustrated	O
in	O
ﬁgure	O
,	O
where	O
we	O
see	O
that	O
a	O
“	O
task	O
neural	O
network	O
”	O
is	O
coupled	O
with	O
a	O
memory	O
.	O
although	O
that	O
task	O
neural	O
network	O
could	O
be	O
feedforward	O
or	O
recurrent	O
,	O
the	O
overall	O
system	O
is	O
a	O
recurrent	O
network	O
.	O
the	O
task	O
network	O
can	O
choose	O
to	O
read	O
from	O
or	O
write	O
to	O
speciﬁc	O
memory	O
addresses	O
.	O
explicit	O
memory	O
seems	O
to	O
allow	O
models	O
to	O
learn	O
tasks	O
that	O
ordinary	O
rnns	O
or	O
lstm	O
rnns	O
can	O
not	O
learn	O
.	O
one	O
reason	O
for	O
this	O
advantage	O
may	O
be	O
because	O
information	O
and	O
gradients	O
can	O
be	O
propagated	O
(	O
forward	O
in	O
time	O
or	O
backwards	O
in	O
time	O
,	O
respectively	O
)	O
for	O
very	O
long	O
durations	O
.	O
as	O
an	O
alternative	O
to	O
back-propagation	O
through	O
weighted	O
averages	O
of	O
memory	O
cells	O
,	O
we	O
can	O
interpret	O
the	O
memory	O
addressing	O
coeﬃcients	O
as	O
probabilities	O
and	O
stochastically	O
read	O
just	O
one	O
cell	O
(	O
zaremba	O
and	O
sutskever	O
2015	O
)	O
.	O
optimizing	O
models	O
that	O
make	O
discrete	O
decisions	O
requires	O
specialized	O
optimization	O
algorithms	O
,	O
described	O
in	O
section	O
.	O
so	O
far	O
,	O
training	O
these	O
stochastic	O
architectures	O
that	O
make	O
discrete	O
decisions	O
remains	O
harder	O
than	O
training	O
deterministic	O
algorithms	O
that	O
make	O
soft	O
decisions	O
.	O
20.9.1	O
,	O
whether	O
it	O
is	O
soft	O
(	O
allowing	O
back-propagation	O
)	O
or	O
stochastic	O
and	O
hard	O
,	O
the	O
419	O
chapter	O
10.	O
sequence	O
modeling	O
:	O
recurrent	O
and	O
recursive	O
nets	O
mechanism	O
for	O
choosing	O
an	O
address	O
is	O
in	O
its	O
form	O
identical	O
to	O
the	O
attention	O
mechanism	O
which	O
had	O
been	O
previously	O
introduced	O
in	O
the	O
context	O
of	O
machine	O
translation	O
(	O
.	O
the	O
idea	O
of	O
attention	O
mechanisms	O
for	O
neural	O
networks	O
was	O
introduced	O
even	O
earlier	O
,	O
in	O
the	O
context	O
of	O
handwriting	O
generation	O
(	O
graves	O
2013	O
)	O
,	O
with	O
an	O
attention	O
mechanism	O
that	O
was	O
constrained	O
to	O
move	O
only	O
forward	O
in	O
time	O
through	O
the	O
sequence	O
.	O
in	O
the	O
case	O
of	O
machine	O
translation	O
and	O
memory	O
networks	O
,	O
at	O
each	O
step	O
,	O
the	O
focus	O
of	O
attention	O
can	O
move	O
to	O
a	O
completely	O
diﬀerent	O
place	O
,	O
compared	O
to	O
the	O
previous	O
step.	O
)	O
and	O
discussed	O
in	O
section	O
12.4.5.1	O
bahdanau	O
et	O
al	O
.	O
2015	O
,	O
,	O
recurrent	O
neural	O
networks	O
provide	O
a	O
way	O
to	O
extend	O
deep	O
learning	O
to	O
sequential	O
data	O
.	O
they	O
are	O
the	O
last	O
major	O
tool	O
in	O
our	O
deep	O
learning	O
toolbox	O
.	O
our	O
discussion	O
now	O
moves	O
to	O
how	O
to	O
choose	O
and	O
use	O
these	O
tools	O
and	O
how	O
to	O
apply	O
them	O
to	O
real-world	O
tasks	O
.	O
420	O
chapter	O
11	O
practical	O
methodology	O
successfully	O
applying	O
deep	O
learning	O
techniques	O
requires	O
more	O
than	O
just	O
a	O
good	O
knowledge	O
of	O
what	O
algorithms	O
exist	O
and	O
the	O
principles	O
that	O
explain	O
how	O
they	O
work	B
.	O
a	O
good	O
machine	O
learning	O
practitioner	O
also	O
needs	O
to	O
know	O
how	O
to	O
choose	O
an	O
algorithm	O
for	O
a	O
particular	O
application	O
and	O
how	O
to	O
monitor	O
and	O
respond	O
to	O
feedback	O
obtained	O
from	O
experiments	O
in	O
order	O
to	O
improve	O
a	O
machine	O
learning	O
system	O
.	O
during	O
day	O
to	O
day	O
development	O
of	O
machine	O
learning	O
systems	O
,	O
practitioners	O
need	O
to	O
decide	O
whether	O
to	O
gather	O
more	O
data	O
,	O
increase	O
or	O
decrease	O
model	B
capacity	O
,	O
add	O
or	O
remove	O
regularizing	O
features	O
,	O
improve	O
the	O
optimization	O
of	O
a	O
model	B
,	O
improve	O
approximate	O
inference	O
in	O
a	O
model	B
,	O
or	O
debug	O
the	O
software	O
implementation	O
of	O
the	O
model	B
.	O
all	O
of	O
these	O
operations	O
are	O
at	O
the	O
very	O
least	O
time-consuming	O
to	O
try	O
out	O
,	O
so	O
it	O
is	O
important	O
to	O
be	O
able	O
to	O
determine	O
the	O
right	O
course	O
of	O
action	O
rather	O
than	O
blindly	O
guessing	O
.	O
most	O
of	O
this	O
book	O
is	O
about	O
diﬀerent	O
machine	O
learning	O
models	O
,	O
training	O
algo-	O
rithms	O
,	O
and	O
objective	O
functions	O
.	O
this	O
may	O
give	O
the	O
impression	O
that	O
the	O
most	O
important	O
ingredient	O
to	O
being	O
a	O
machine	O
learning	O
expert	O
is	O
knowing	O
a	O
wide	O
variety	O
of	O
machine	O
learning	O
techniques	O
and	O
being	O
good	O
at	O
diﬀerent	O
kinds	O
of	O
math	O
.	O
in	O
prac-	O
tice	O
,	O
one	O
can	O
usually	O
do	O
much	O
better	O
with	O
a	O
correct	O
application	O
of	O
a	O
commonplace	O
algorithm	O
than	O
by	O
sloppily	O
applying	O
an	O
obscure	O
algorithm	O
.	O
correct	O
application	O
of	O
an	O
algorithm	O
depends	O
on	O
mastering	O
some	O
fairly	O
simple	O
methodology	O
.	O
many	O
of	O
the	O
recommendations	O
in	O
this	O
chapter	O
are	O
adapted	O
from	O
ng	O
2015	O
(	O
)	O
.	O
we	O
recommend	O
the	O
following	O
practical	O
design	O
process	O
:	O
•	O
determine	O
your	O
goals—what	O
error	O
metric	O
to	O
use	O
,	O
and	O
your	O
target	O
value	O
for	O
this	O
error	O
metric	O
.	O
these	O
goals	O
and	O
error	O
metrics	O
should	O
be	O
driven	O
by	O
the	O
problem	O
that	O
the	O
application	O
is	O
intended	O
to	O
solve	O
.	O
•	O
establish	O
a	O
working	O
end-to-end	O
pipeline	O
as	O
soon	O
as	O
possible	O
,	O
including	O
the	O
421	O
chapter	O
11.	O
practical	O
methodology	O
•	O
•	O
estimation	O
of	O
the	O
appropriate	O
performance	O
metrics	O
.	O
instrument	O
the	O
system	O
well	O
to	O
determine	O
bottlenecks	O
in	O
performance	O
.	O
diag-	O
nose	O
which	O
components	O
are	O
performing	O
worse	O
than	O
expected	O
and	O
whether	O
it	O
is	O
due	O
to	O
overﬁtting	O
,	O
underﬁtting	O
,	O
or	O
a	O
defect	O
in	O
the	O
data	O
or	O
software	O
.	O
repeatedly	O
make	O
incremental	O
changes	O
such	O
as	O
gathering	O
new	O
data	O
,	O
adjusting	O
hyperparameters	O
,	O
or	O
changing	O
algorithms	O
,	O
based	O
on	O
speciﬁc	O
ﬁndings	O
from	O
your	O
instrumentation	O
.	O
,	O
goodfellow	O
et	O
al	O
.	O
2014d	O
as	O
a	O
running	O
example	O
,	O
we	O
will	O
use	O
street	O
view	O
address	O
number	O
transcription	O
system	O
(	O
)	O
.	O
the	O
purpose	O
of	O
this	O
application	O
is	O
to	O
add	O
buildings	O
to	O
google	O
maps	O
.	O
street	O
view	O
cars	O
photograph	O
the	O
buildings	O
and	O
record	O
the	O
gps	O
coordinates	O
associated	O
with	O
each	O
photograph	O
.	O
a	O
convolutional	O
network	O
recognizes	O
the	O
address	O
number	O
in	O
each	O
photograph	O
,	O
allowing	O
the	O
google	O
maps	O
database	O
to	O
add	O
that	O
address	O
in	O
the	O
correct	O
location	O
.	O
the	O
story	O
of	O
how	O
this	O
commercial	O
application	O
was	O
developed	O
gives	O
an	O
example	O
of	O
how	O
to	O
follow	O
the	O
design	O
methodology	O
we	O
advocate	O
.	O
we	O
now	O
describe	O
each	O
of	O
the	O
steps	O
in	O
this	O
process	O
.	O
11.1	O
performance	O
metrics	O
determining	O
your	O
goals	O
,	O
in	O
terms	O
of	O
which	O
error	O
metric	O
to	O
use	O
,	O
is	O
a	O
necessary	O
ﬁrst	O
step	O
because	O
your	O
error	O
metric	O
will	O
guide	O
all	O
of	O
your	O
future	O
actions	O
.	O
you	O
should	O
also	O
have	O
an	O
idea	O
of	O
what	O
level	O
of	O
performance	O
you	O
desire	O
.	O
keep	O
in	O
mind	O
that	O
for	O
most	O
applications	O
,	O
it	O
is	O
impossible	O
to	O
achieve	O
absolute	O
zero	O
error	O
.	O
the	O
bayes	O
error	O
deﬁnes	O
the	O
minimum	O
error	O
rate	O
that	O
you	O
can	O
hope	O
to	O
achieve	O
,	O
even	O
if	O
you	O
have	O
inﬁnite	O
training	O
data	O
and	O
can	O
recover	O
the	O
true	O
probability	O
distribution	O
.	O
this	O
is	O
because	O
your	O
input	O
features	O
may	O
not	O
contain	O
complete	O
information	O
about	O
the	O
output	O
variable	O
,	O
or	O
because	O
the	O
system	O
might	O
be	O
intrinsically	O
stochastic	O
.	O
you	O
will	O
also	O
be	O
limited	O
by	O
having	O
a	O
ﬁnite	O
amount	O
of	O
training	O
data	O
.	O
the	O
amount	O
of	O
training	O
data	O
can	O
be	O
limited	O
for	O
a	O
variety	O
of	O
reasons	O
.	O
when	O
your	O
goal	O
is	O
to	O
build	O
the	O
best	O
possible	O
real-world	O
product	O
or	O
service	O
,	O
you	O
can	O
typically	O
collect	O
more	O
data	O
but	O
must	O
determine	O
the	O
value	O
of	O
reducing	O
error	O
further	O
and	O
weigh	O
this	O
against	O
the	O
cost	O
of	O
collecting	O
more	O
data	O
.	O
data	O
collection	O
can	O
require	O
time	O
,	O
money	O
,	O
or	O
human	O
suﬀering	O
(	O
for	O
example	O
,	O
if	O
your	O
data	O
collection	O
process	O
involves	O
performing	O
invasive	O
medical	O
tests	O
)	O
.	O
when	O
your	O
goal	O
is	O
to	O
answer	O
a	O
scientiﬁc	O
question	O
about	O
which	O
algorithm	O
performs	O
better	O
on	O
a	O
ﬁxed	O
benchmark	O
,	O
the	O
benchmark	O
422	O
chapter	O
11.	O
practical	O
methodology	O
speciﬁcation	O
usually	O
determines	O
the	O
training	O
set	O
and	O
you	O
are	O
not	O
allowed	O
to	O
collect	O
more	O
data	O
.	O
how	O
can	O
one	O
determine	O
a	O
reasonable	O
level	O
of	O
performance	O
to	O
expect	O
?	O
typically	O
,	O
in	O
the	O
academic	O
setting	O
,	O
we	O
have	O
some	O
estimate	O
of	O
the	O
error	O
rate	O
that	O
is	O
attainable	O
based	O
on	O
previously	O
published	O
benchmark	O
results	O
.	O
in	O
the	O
real-word	O
setting	O
,	O
we	O
have	O
some	O
idea	O
of	O
the	O
error	O
rate	O
that	O
is	O
necessary	O
for	O
an	O
application	O
to	O
be	O
safe	O
,	O
cost-eﬀective	O
,	O
or	O
appealing	O
to	O
consumers	O
.	O
once	O
you	O
have	O
determined	O
your	O
realistic	O
desired	O
error	O
rate	O
,	O
your	O
design	O
decisions	O
will	O
be	O
guided	O
by	O
reaching	O
this	O
error	O
rate	O
.	O
another	O
important	O
consideration	O
besides	O
the	O
target	O
value	O
of	O
the	O
performance	O
metric	O
is	O
the	O
choice	O
of	O
which	O
metric	O
to	O
use	O
.	O
several	O
diﬀerent	O
performance	O
metrics	O
may	O
be	O
used	O
to	O
measure	O
the	O
eﬀectiveness	O
of	O
a	O
complete	O
application	O
that	O
includes	O
machine	O
learning	O
components	O
.	O
these	O
performance	O
metrics	O
are	O
usually	O
diﬀerent	O
from	O
the	O
cost	O
function	O
used	O
to	O
train	O
the	O
model	B
.	O
as	O
described	O
in	O
section	O
,	O
it	O
is	O
common	O
to	O
measure	O
the	O
accuracy	O
,	O
or	O
equivalently	O
,	O
the	O
error	O
rate	O
,	O
of	O
a	O
system	O
.	O
5.1.2	O
however	O
,	O
many	O
applications	O
require	O
more	O
advanced	O
metrics	O
.	O
sometimes	O
it	O
is	O
much	O
more	O
costly	O
to	O
make	O
one	O
kind	O
of	O
a	O
mistake	O
than	O
another	O
.	O
for	O
example	O
,	O
an	O
e-mail	O
spam	O
detection	O
system	O
can	O
make	O
two	O
kinds	O
of	O
mistakes	O
:	O
incorrectly	O
classifying	O
a	O
legitimate	O
message	O
as	O
spam	O
,	O
and	O
incorrectly	O
allowing	O
a	O
spam	O
message	O
to	O
appear	O
in	O
the	O
inbox	O
.	O
it	O
is	O
much	O
worse	O
to	O
block	O
a	O
legitimate	O
message	O
than	O
to	O
allow	O
a	O
questionable	O
message	O
to	O
pass	O
through	O
.	O
rather	O
than	O
measuring	O
the	O
error	O
rate	O
of	O
a	O
spam	O
classiﬁer	O
,	O
we	O
may	O
wish	O
to	O
measure	O
some	O
form	O
of	O
total	O
cost	O
,	O
where	O
the	O
cost	O
of	O
blocking	O
legitimate	O
messages	O
is	O
higher	O
than	O
the	O
cost	O
of	O
allowing	O
spam	O
messages	O
.	O
sometimes	O
we	O
wish	O
to	O
train	O
a	O
binary	O
classiﬁer	O
that	O
is	O
intended	O
to	O
detect	O
some	O
rare	O
event	O
.	O
for	O
example	O
,	O
we	O
might	O
design	O
a	O
medical	O
test	O
for	O
a	O
rare	O
disease	O
.	O
suppose	O
that	O
only	O
one	O
in	O
every	O
million	O
people	O
has	O
this	O
disease	O
.	O
we	O
can	O
easily	O
achieve	O
99.9999	O
%	O
accuracy	O
on	O
the	O
detection	O
task	O
,	O
by	O
simply	O
hard-coding	O
the	O
classiﬁer	O
to	O
always	O
report	O
that	O
the	O
disease	O
is	O
absent	O
.	O
clearly	O
,	O
accuracy	O
is	O
a	O
poor	O
way	O
to	O
characterize	O
the	O
performance	O
of	O
such	O
a	O
system	O
.	O
one	O
way	O
to	O
solve	O
this	O
problem	O
is	O
to	O
instead	O
measure	O
precision	O
and	O
recall	O
.	O
precision	O
is	O
the	O
fraction	O
of	O
detections	O
reported	O
by	O
the	O
model	B
that	O
were	O
correct	O
,	O
while	O
recall	O
is	O
the	O
fraction	O
of	O
true	O
events	O
that	O
were	O
detected	O
.	O
a	O
detector	O
that	O
says	O
no	O
one	O
has	O
the	O
disease	O
would	O
achieve	O
perfect	O
precision	O
,	O
but	O
zero	O
recall	O
.	O
a	O
detector	O
that	O
says	O
everyone	O
has	O
the	O
disease	O
would	O
achieve	O
perfect	O
recall	O
,	O
but	O
precision	O
equal	O
to	O
the	O
percentage	O
of	O
people	O
who	O
have	O
the	O
disease	O
(	O
0.0001	O
%	O
in	O
our	O
example	O
of	O
a	O
disease	O
that	O
only	O
one	O
people	O
in	O
a	O
million	O
have	O
)	O
.	O
when	O
using	O
precision	O
and	O
recall	O
,	O
it	O
is	O
common	O
to	O
plot	O
a	O
pr	O
curve	O
,	O
with	O
precision	O
on	O
the	O
y-axis	O
and	O
recall	O
on	O
the	O
x-axis	O
.	O
the	O
classiﬁer	O
generates	O
a	O
score	O
that	O
is	O
higher	O
if	O
the	O
event	O
to	O
be	O
detected	O
occurred	O
.	O
for	O
example	O
,	O
a	O
feedforward	O
423	O
chapter	O
11.	O
practical	O
methodology	O
|	O
x	O
)	O
,	O
estimating	O
the	O
network	O
designed	O
to	O
detect	O
a	O
disease	O
outputs	O
ˆy	O
=	O
p	O
(	O
y	O
=	O
1	O
probability	O
that	O
a	O
person	O
whose	O
medical	O
results	O
are	O
described	O
by	O
features	O
x	O
has	O
the	O
disease	O
.	O
we	O
choose	O
to	O
report	O
a	O
detection	O
whenever	O
this	O
score	O
exceeds	O
some	O
threshold	O
.	O
by	O
varying	O
the	O
threshold	O
,	O
we	O
can	O
trade	O
precision	O
for	O
recall	O
.	O
in	O
many	O
cases	O
,	O
we	O
wish	O
to	O
summarize	O
the	O
performance	O
of	O
the	O
classiﬁer	O
with	O
a	O
single	O
number	O
rather	O
than	O
a	O
curve	O
.	O
to	O
do	O
so	O
,	O
we	O
can	O
convert	O
precision	O
p	O
and	O
recall	O
r	O
into	O
an	O
f-score	O
given	O
by	O
f	O
=	O
.	O
(	O
11.1	O
)	O
2pr	O
r+	O
p	O
another	O
option	O
is	O
to	O
report	O
the	O
total	O
area	O
lying	O
beneath	O
the	O
pr	O
curve	O
.	O
in	O
some	O
applications	O
,	O
it	O
is	O
possible	O
for	O
the	O
machine	O
learning	O
system	O
to	O
refuse	O
to	O
make	O
a	O
decision	O
.	O
this	O
is	O
useful	O
when	O
the	O
machine	O
learning	O
algorithm	O
can	O
estimate	O
how	O
conﬁdent	O
it	O
should	O
be	O
about	O
a	O
decision	O
,	O
especially	O
if	O
a	O
wrong	O
decision	O
can	O
be	O
harmful	O
and	O
if	O
a	O
human	O
operator	O
is	O
able	O
to	O
occasionally	O
take	O
over	O
.	O
the	O
street	O
view	O
transcription	O
system	O
provides	O
an	O
example	O
of	O
this	O
situation	O
.	O
the	O
task	O
is	O
to	O
transcribe	O
the	O
address	O
number	O
from	O
a	O
photograph	O
in	O
order	O
to	O
associate	O
the	O
location	O
where	O
the	O
photo	O
was	O
taken	O
with	O
the	O
correct	O
address	O
in	O
a	O
map	O
.	O
because	O
the	O
value	O
of	O
the	O
map	O
degrades	O
considerably	O
if	O
the	O
map	O
is	O
inaccurate	O
,	O
it	O
is	O
important	O
to	O
add	O
an	O
address	O
only	O
if	O
the	O
transcription	O
is	O
correct	O
.	O
if	O
the	O
machine	O
learning	O
system	O
thinks	O
that	O
it	O
is	O
less	O
likely	O
than	O
a	O
human	O
being	O
to	O
obtain	O
the	O
correct	O
transcription	O
,	O
then	O
the	O
best	O
course	O
of	O
action	O
is	O
to	O
allow	O
a	O
human	O
to	O
transcribe	O
the	O
photo	O
instead	O
.	O
of	O
course	O
,	O
the	O
machine	O
learning	O
system	O
is	O
only	O
useful	O
if	O
it	O
is	O
able	O
to	O
dramatically	O
reduce	O
the	O
amount	O
of	O
photos	O
that	O
the	O
human	O
operators	O
must	O
process	O
.	O
a	O
natural	O
performance	O
metric	O
to	O
use	O
in	O
this	O
situation	O
is	O
coverage	O
.	O
coverage	O
is	O
the	O
fraction	O
of	O
examples	O
for	O
which	O
the	O
machine	O
learning	O
system	O
is	O
able	O
to	O
produce	O
a	O
response	O
.	O
it	O
is	O
possible	O
to	O
trade	O
coverage	O
for	O
accuracy	O
.	O
one	O
can	O
always	O
obtain	O
100	O
%	O
accuracy	O
by	O
refusing	O
to	O
process	O
any	O
example	O
,	O
but	O
this	O
reduces	O
the	O
coverage	O
to	O
0	O
%	O
.	O
for	O
the	O
street	O
view	O
task	O
,	O
the	O
goal	O
for	O
the	O
project	O
was	O
to	O
reach	O
human-level	O
transcription	O
accuracy	O
while	O
maintaining	O
95	O
%	O
coverage	O
.	O
human-level	O
performance	O
on	O
this	O
task	O
is	O
98	O
%	O
accuracy	O
.	O
many	O
other	O
metrics	O
are	O
possible	O
.	O
we	O
can	O
for	O
example	O
,	O
measure	O
click-through	O
rates	O
,	O
collect	O
user	O
satisfaction	O
surveys	O
,	O
and	O
so	O
on	O
.	O
many	O
specialized	O
application	O
areas	O
have	O
application-speciﬁc	O
criteria	O
as	O
well	O
.	O
what	O
is	O
important	O
is	O
to	O
determine	O
which	O
performance	O
metric	O
to	O
improve	O
ahead	O
of	O
time	O
,	O
then	O
concentrate	O
on	O
improving	O
this	O
metric	O
.	O
without	O
clearly	O
deﬁned	O
goals	O
,	O
it	O
can	O
be	O
diﬃcult	O
to	O
tell	O
whether	O
changes	O
to	O
a	O
machine	O
learning	O
system	O
make	O
progress	O
or	O
not	O
.	O
424	O
chapter	O
11.	O
practical	O
methodology	O
11.2	O
default	O
baseline	O
models	O
after	O
choosing	O
performance	O
metrics	O
and	O
goals	O
,	O
the	O
next	O
step	O
in	O
any	O
practical	O
application	O
is	O
to	O
establish	O
a	O
reasonable	O
end-to-end	O
system	O
as	O
soon	O
as	O
possible	O
.	O
in	O
this	O
section	O
,	O
we	O
provide	O
recommendations	O
for	O
which	O
algorithms	O
to	O
use	O
as	O
the	O
ﬁrst	O
baseline	O
approach	O
in	O
various	O
situations	O
.	O
keep	O
in	O
mind	O
that	O
deep	O
learning	O
research	O
progresses	O
quickly	O
,	O
so	O
better	O
default	O
algorithms	O
are	O
likely	O
to	O
become	O
available	O
soon	O
after	O
this	O
writing	O
.	O
depending	O
on	O
the	O
complexity	O
of	O
your	O
problem	O
,	O
you	O
may	O
even	O
want	O
to	O
begin	O
without	O
using	O
deep	O
learning	O
.	O
if	O
your	O
problem	O
has	O
a	O
chance	O
of	O
being	O
solved	O
by	O
just	O
choosing	O
a	O
few	O
linear	O
weights	O
correctly	O
,	O
you	O
may	O
want	O
to	O
begin	O
with	O
a	O
simple	O
statistical	O
model	B
like	O
logistic	O
regression	O
.	O
if	O
you	O
know	O
that	O
your	O
problem	O
falls	O
into	O
an	O
“	O
ai-complete	O
”	O
category	O
like	O
object	O
recognition	B
,	O
speech	O
recognition	B
,	O
machine	O
translation	O
,	O
and	O
so	O
on	O
,	O
then	O
you	O
are	O
likely	O
to	O
do	O
well	O
by	O
beginning	O
with	O
an	O
appropriate	O
deep	O
learning	O
model	B
.	O
first	O
,	O
choose	O
the	O
general	O
category	O
of	O
model	B
based	O
on	O
the	O
structure	O
of	O
your	O
data	O
.	O
if	O
you	O
want	O
to	O
perform	O
supervised	O
learning	O
with	O
ﬁxed-size	O
vectors	O
as	O
input	O
,	O
use	O
a	O
feedforward	O
network	O
with	O
fully	O
connected	O
layers	O
.	O
if	O
the	O
input	O
has	O
known	O
topological	O
structure	O
(	O
for	O
example	O
,	O
if	O
the	O
input	O
is	O
an	O
image	O
)	O
,	O
use	O
a	O
convolutional	O
network	O
.	O
in	O
these	O
cases	O
,	O
you	O
should	O
begin	O
by	O
using	O
some	O
kind	O
of	O
piecewise	O
linear	O
unit	O
(	O
relus	O
or	O
their	O
generalizations	O
like	O
leaky	O
relus	O
,	O
prelus	O
and	O
maxout	O
)	O
.	O
if	O
your	O
input	O
or	O
output	O
is	O
a	O
sequence	O
,	O
use	O
a	O
gated	O
recurrent	O
net	O
(	O
lstm	O
or	O
gru	O
)	O
.	O
a	O
reasonable	O
choice	O
of	O
optimization	O
algorithm	O
is	O
sgd	O
with	O
momentum	O
with	O
a	O
decaying	O
learning	O
rate	O
(	O
popular	O
decay	O
schemes	O
that	O
perform	O
better	O
or	O
worse	O
on	O
diﬀerent	O
problems	O
include	O
decaying	O
linearly	O
until	O
reaching	O
a	O
ﬁxed	O
minimum	O
learning	O
rate	O
,	O
decaying	O
exponentially	O
,	O
or	O
decreasing	O
the	O
learning	O
rate	O
by	O
a	O
factor	O
of	O
2-10	O
each	O
time	O
validation	O
error	O
plateaus	O
)	O
.	O
another	O
very	O
reasonable	O
alternative	O
is	O
adam	O
.	O
batch	O
normalization	O
can	O
have	O
a	O
dramatic	O
eﬀect	O
on	O
optimization	O
performance	O
,	O
especially	O
for	O
convolutional	O
networks	O
and	O
networks	O
with	O
sigmoidal	O
nonlinearities	O
.	O
while	O
it	O
is	O
reasonable	O
to	O
omit	O
batch	O
normalization	O
from	O
the	O
very	O
ﬁrst	O
baseline	O
,	O
it	O
should	O
be	O
introduced	O
quickly	O
if	O
optimization	O
appears	O
to	O
be	O
problematic	O
.	O
unless	O
your	O
training	O
set	O
contains	O
tens	O
of	O
millions	O
of	O
examples	O
or	O
more	O
,	O
you	O
should	O
include	O
some	O
mild	O
forms	O
of	O
regularization	O
from	O
the	O
start	O
.	O
early	O
stopping	O
should	O
be	O
used	O
almost	O
universally	O
.	O
dropout	O
is	O
an	O
excellent	O
regularizer	O
that	O
is	O
easy	O
to	O
implement	O
and	O
compatible	O
with	O
many	O
models	O
and	O
training	O
algorithms	O
.	O
batch	O
normalization	O
also	O
sometimes	O
reduces	O
generalization	O
error	O
and	O
allows	O
dropout	O
to	O
be	O
omitted	O
,	O
due	O
to	O
the	O
noise	O
in	O
the	O
estimate	O
of	O
the	O
statistics	O
used	O
to	O
normalize	O
each	O
variable	O
.	O
425	O
chapter	O
11.	O
practical	O
methodology	O
if	O
your	O
task	O
is	O
similar	O
to	O
another	O
task	O
that	O
has	O
been	O
studied	O
extensively	O
,	O
you	O
will	O
probably	O
do	O
well	O
by	O
ﬁrst	O
copying	O
the	O
model	B
and	O
algorithm	O
that	O
is	O
already	O
known	O
to	O
perform	O
best	O
on	O
the	O
previously	O
studied	O
task	O
.	O
you	O
may	O
even	O
want	O
to	O
copy	O
a	O
trained	O
model	B
from	O
that	O
task	O
.	O
for	O
example	O
,	O
it	O
is	O
common	O
to	O
use	O
the	O
features	O
from	O
a	O
convolutional	O
network	O
trained	O
on	O
imagenet	O
to	O
solve	O
other	O
computer	O
vision	O
tasks	O
(	O
girshick	O
et	O
al	O
.	O
2015	O
)	O
.	O
,	O
iii	O
a	O
common	O
question	O
is	O
whether	O
to	O
begin	O
by	O
using	O
unsupervised	O
learning	O
,	O
de-	O
scribed	O
further	O
in	O
part	O
.	O
this	O
is	O
somewhat	O
domain	O
speciﬁc	O
.	O
some	O
domains	O
,	O
such	O
as	O
natural	O
language	O
processing	O
,	O
are	O
known	O
to	O
beneﬁt	O
tremendously	O
from	O
unsuper-	O
vised	O
learning	O
techniques	O
such	O
as	O
learning	O
unsupervised	O
word	O
embeddings	O
.	O
in	O
other	O
domains	O
,	O
such	O
as	O
computer	O
vision	O
,	O
current	O
unsupervised	O
learning	O
techniques	O
do	O
not	O
bring	O
a	O
beneﬁt	O
,	O
except	O
in	O
the	O
semi-supervised	O
setting	O
,	O
when	O
the	O
number	O
of	O
labeled	O
examples	O
is	O
very	O
small	O
(	O
)	O
.	O
if	O
your	O
application	O
is	O
in	O
a	O
context	O
where	O
unsupervised	O
learning	O
is	O
known	O
to	O
be	O
important	O
,	O
then	O
include	O
it	O
in	O
your	O
ﬁrst	O
end-to-end	O
baseline	O
.	O
otherwise	O
,	O
only	O
use	O
unsupervised	O
learning	O
in	O
your	O
ﬁrst	O
attempt	O
if	O
the	O
task	O
you	O
want	O
to	O
solve	O
is	O
unsupervised	O
.	O
you	O
can	O
always	O
try	O
adding	O
unsupervised	O
learning	O
later	O
if	O
you	O
observe	O
that	O
your	O
initial	O
baseline	O
overﬁts	O
.	O
kingma	O
et	O
al	O
.	O
2014	O
rasmus	O
et	O
al.	O
,	O
2015	O
,	O
;	O
11.3	O
determining	O
whether	O
to	O
gather	O
more	O
data	O
after	O
the	O
ﬁrst	O
end-to-end	O
system	O
is	O
established	O
,	O
it	O
is	O
time	O
to	O
measure	O
the	O
perfor-	O
mance	O
of	O
the	O
algorithm	O
and	O
determine	O
how	O
to	O
improve	O
it	O
.	O
many	O
machine	O
learning	O
novices	O
are	O
tempted	O
to	O
make	O
improvements	O
by	O
trying	O
out	O
many	O
diﬀerent	O
algorithms	O
.	O
however	O
,	O
it	O
is	O
often	O
much	O
better	O
to	O
gather	O
more	O
data	O
than	O
to	O
improve	O
the	O
learning	O
algorithm	O
.	O
how	O
does	O
one	O
decide	O
whether	O
to	O
gather	O
more	O
data	O
?	O
first	O
,	O
determine	O
whether	O
the	O
performance	O
on	O
the	O
training	O
set	O
is	O
acceptable	O
.	O
if	O
performance	O
on	O
the	O
training	O
set	O
is	O
poor	O
,	O
the	O
learning	O
algorithm	O
is	O
not	O
using	O
the	O
training	O
data	O
that	O
is	O
already	O
available	O
,	O
so	O
there	O
is	O
no	O
reason	O
to	O
gather	O
more	O
data	O
.	O
instead	O
,	O
try	O
increasing	O
the	O
size	O
of	O
the	O
model	B
by	O
adding	O
more	O
layers	O
or	O
adding	O
more	O
hidden	O
units	O
to	O
each	O
layer	O
.	O
also	O
,	O
try	O
improving	O
the	O
learning	O
algorithm	O
,	O
for	O
example	O
by	O
tuning	O
the	O
learning	O
rate	O
hyperparameter	O
.	O
if	O
large	O
models	O
and	O
carefully	O
tuned	O
optimization	O
algorithms	O
do	O
not	O
work	B
well	O
,	O
then	O
the	O
problem	O
might	O
be	O
the	O
of	O
the	O
training	O
data	O
.	O
the	O
data	O
may	O
be	O
too	O
noisy	O
or	O
may	O
not	O
include	O
the	O
right	O
inputs	O
needed	O
to	O
predict	O
the	O
desired	O
outputs	O
.	O
this	O
suggests	O
starting	O
over	O
,	O
collecting	O
cleaner	O
data	O
or	O
collecting	O
a	O
richer	O
set	O
of	O
features	O
.	O
quality	O
if	O
the	O
performance	O
on	O
the	O
training	O
set	O
is	O
acceptable	O
,	O
then	O
measure	O
the	O
per-	O
426	O
chapter	O
11.	O
practical	O
methodology	O
if	O
the	O
performance	O
on	O
the	O
test	O
set	O
is	O
also	O
acceptable	O
,	O
formance	O
on	O
a	O
test	O
set	O
.	O
then	O
there	O
is	O
nothing	O
left	O
to	O
be	O
done	O
.	O
if	O
test	O
set	O
performance	O
is	O
much	O
worse	O
than	O
training	O
set	O
performance	O
,	O
then	O
gathering	O
more	O
data	O
is	O
one	O
of	O
the	O
most	O
eﬀective	O
solutions	O
.	O
the	O
key	O
considerations	O
are	O
the	O
cost	O
and	O
feasibility	O
of	O
gathering	O
more	O
data	O
,	O
the	O
cost	O
and	O
feasibility	O
of	O
reducing	O
the	O
test	O
error	O
by	O
other	O
means	O
,	O
and	O
the	O
amount	O
of	O
data	O
that	O
is	O
expected	O
to	O
be	O
necessary	O
to	O
improve	O
test	O
set	O
performance	O
signiﬁcantly	O
.	O
at	O
large	O
internet	O
companies	O
with	O
millions	O
or	O
billions	O
of	O
users	O
,	O
it	O
is	O
feasible	O
to	O
gather	O
large	O
datasets	O
,	O
and	O
the	O
expense	O
of	O
doing	O
so	O
can	O
be	O
considerably	O
less	O
than	O
the	O
other	O
alternatives	O
,	O
so	O
the	O
answer	O
is	O
almost	O
always	O
to	O
gather	O
more	O
training	O
data	O
.	O
for	O
example	O
,	O
the	O
development	O
of	O
large	O
labeled	O
datasets	O
was	O
one	O
of	O
the	O
most	O
important	O
factors	O
in	O
solving	O
object	O
recognition	B
.	O
in	O
other	O
contexts	O
,	O
such	O
as	O
medical	O
applications	O
,	O
it	O
may	O
be	O
costly	O
or	O
infeasible	O
to	O
gather	O
more	O
data	O
.	O
a	O
simple	O
alternative	O
to	O
gathering	O
more	O
data	O
is	O
to	O
reduce	O
the	O
size	O
of	O
the	O
model	B
or	O
improve	O
regularization	O
,	O
by	O
adjusting	O
hyperparameters	O
such	O
as	O
weight	O
decay	O
coeﬃcients	O
,	O
or	O
by	O
adding	O
regularization	O
strategies	O
such	O
as	O
dropout	O
.	O
if	O
you	O
ﬁnd	O
that	O
the	O
gap	O
between	O
train	O
and	O
test	O
performance	O
is	O
still	O
unacceptable	O
even	O
after	O
tuning	O
the	O
regularization	O
hyperparameters	O
,	O
then	O
gathering	O
more	O
data	O
is	O
advisable	O
.	O
when	O
deciding	O
whether	O
to	O
gather	O
more	O
data	O
,	O
it	O
is	O
also	O
necessary	O
to	O
decide	O
how	O
much	O
to	O
gather	O
.	O
it	O
is	O
helpful	O
to	O
plot	O
curves	O
showing	O
the	O
relationship	O
between	O
training	O
set	O
size	O
and	O
generalization	O
error	O
,	O
like	O
in	O
ﬁgure	O
.	O
by	O
extrapolating	O
such	O
curves	O
,	O
one	O
can	O
predict	O
how	O
much	O
additional	O
training	O
data	O
would	O
be	O
needed	O
to	O
achieve	O
a	O
certain	O
level	O
of	O
performance	O
.	O
usually	O
,	O
adding	O
a	O
small	O
fraction	O
of	O
the	O
total	O
number	O
of	O
examples	O
will	O
not	O
have	O
a	O
noticeable	O
impact	O
on	O
generalization	O
error	O
.	O
it	O
is	O
therefore	O
recommended	O
to	O
experiment	O
with	O
training	O
set	O
sizes	O
on	O
a	O
logarithmic	O
scale	O
,	O
for	O
example	O
doubling	O
the	O
number	O
of	O
examples	O
between	O
consecutive	O
experiments	O
.	O
5.4	O
if	O
gathering	O
much	O
more	O
data	O
is	O
not	O
feasible	O
,	O
the	O
only	O
other	O
way	O
to	O
improve	O
generalization	O
error	O
is	O
to	O
improve	O
the	O
learning	O
algorithm	O
itself	O
.	O
this	O
becomes	O
the	O
domain	O
of	O
research	O
and	O
not	O
the	O
domain	O
of	O
advice	O
for	O
applied	O
practitioners	O
.	O
11.4	O
selecting	O
hyperparameters	O
most	O
deep	O
learning	O
algorithms	O
come	O
with	O
many	O
hyperparameters	O
that	O
control	O
many	O
aspects	O
of	O
the	O
algorithm	O
’	O
s	O
behavior	O
.	O
some	O
of	O
these	O
hyperparameters	O
aﬀect	O
the	O
time	O
and	O
memory	O
cost	O
of	O
running	O
the	O
algorithm	O
.	O
some	O
of	O
these	O
hyperparameters	O
aﬀect	O
the	O
quality	O
of	O
the	O
model	B
recovered	O
by	O
the	O
training	O
process	O
and	O
its	O
ability	O
to	O
infer	O
correct	O
results	O
when	O
deployed	O
on	O
new	O
inputs	O
.	O
there	O
are	O
two	O
basic	O
approaches	O
to	O
choosing	O
these	O
hyperparameters	O
:	O
choosing	O
them	O
manually	O
and	O
choosing	O
them	O
automatically	O
.	O
choosing	O
the	O
hyperparameters	O
427	O
chapter	O
11.	O
practical	O
methodology	O
manually	O
requires	O
understanding	O
what	O
the	O
hyperparameters	O
do	O
and	O
how	O
machine	O
learning	O
models	O
achieve	O
good	O
generalization	O
.	O
automatic	O
hyperparameter	O
selection	O
algorithms	O
greatly	O
reduce	O
the	O
need	O
to	O
understand	O
these	O
ideas	O
,	O
but	O
they	O
are	O
often	O
much	O
more	O
computationally	O
costly	O
.	O
11.4.1	O
manual	O
hyperparameter	O
tuning	O
to	O
set	O
hyperparameters	O
manually	O
,	O
one	O
must	O
understand	O
the	O
relationship	O
between	O
hyperparameters	O
,	O
training	O
error	O
,	O
generalization	O
error	O
and	O
computational	O
resources	O
(	O
memory	O
and	O
runtime	O
)	O
.	O
this	O
means	O
establishing	O
a	O
solid	O
foundation	O
on	O
the	O
fun-	O
damental	O
ideas	O
concerning	O
the	O
eﬀective	O
capacity	O
of	O
a	O
learning	O
algorithm	O
from	O
chapter	O
.5	O
the	O
goal	O
of	O
manual	O
hyperparameter	O
search	O
is	O
usually	O
to	O
ﬁnd	O
the	O
lowest	O
general-	O
ization	O
error	O
subject	O
to	O
some	O
runtime	O
and	O
memory	O
budget	O
.	O
we	O
do	O
not	O
discuss	O
how	O
to	O
determine	O
the	O
runtime	O
and	O
memory	O
impact	O
of	O
various	O
hyperparameters	O
here	O
because	O
this	O
is	O
highly	O
platform-dependent	O
.	O
the	O
primary	O
goal	O
of	O
manual	O
hyperparameter	O
search	O
is	O
to	O
adjust	O
the	O
eﬀective	O
capacity	O
of	O
the	O
model	B
to	O
match	O
the	O
complexity	O
of	O
the	O
task	O
.	O
eﬀective	O
capacity	O
is	O
constrained	O
by	O
three	O
factors	O
:	O
the	O
representational	O
capacity	O
of	O
the	O
model	B
,	O
the	O
ability	O
of	O
the	O
learning	O
algorithm	O
to	O
successfully	O
minimize	O
the	O
cost	O
function	O
used	O
to	O
train	O
the	O
model	B
,	O
and	O
the	O
degree	O
to	O
which	O
the	O
cost	O
function	O
and	O
training	O
procedure	O
regularize	O
the	O
model	B
.	O
a	O
model	B
with	O
more	O
layers	O
and	O
more	O
hidden	O
units	O
per	O
layer	O
has	O
higher	O
representational	O
capacity—it	O
is	O
capable	O
of	O
representing	O
more	O
complicated	O
functions	O
.	O
it	O
can	O
not	O
necessarily	O
actually	O
learn	O
all	O
of	O
these	O
functions	O
though	O
,	O
if	O
the	O
training	O
algorithm	O
can	O
not	O
discover	O
that	O
certain	O
functions	O
do	O
a	O
good	O
job	O
of	O
minimizing	O
the	O
training	O
cost	O
,	O
or	O
if	O
regularization	O
terms	O
such	O
as	O
weight	O
decay	O
forbid	O
some	O
of	O
these	O
functions	O
.	O
the	O
generalization	O
error	O
typically	O
follows	O
a	O
u-shaped	O
curve	O
when	O
plotted	O
as	O
a	O
function	O
of	O
one	O
of	O
the	O
hyperparameters	O
,	O
as	O
in	O
ﬁgure	O
.	O
at	O
one	O
extreme	O
,	O
the	O
hyperparameter	O
value	O
corresponds	O
to	O
low	O
capacity	O
,	O
and	O
generalization	O
error	O
is	O
high	O
because	O
training	O
error	O
is	O
high	O
.	O
this	O
is	O
the	O
underﬁtting	O
regime	O
.	O
at	O
the	O
other	O
extreme	O
,	O
the	O
hyperparameter	O
value	O
corresponds	O
to	O
high	O
capacity	O
,	O
and	O
the	O
generalization	O
error	O
is	O
high	O
because	O
the	O
gap	O
between	O
training	O
and	O
test	O
error	O
is	O
high	O
.	O
somewhere	O
in	O
the	O
middle	O
lies	O
the	O
optimal	O
model	B
capacity	O
,	O
which	O
achieves	O
the	O
lowest	O
possible	O
generalization	O
error	O
,	O
by	O
adding	O
a	O
medium	O
generalization	O
gap	O
to	O
a	O
medium	O
amount	O
of	O
training	O
error	O
.	O
5.3	O
for	O
some	O
hyperparameters	O
,	O
overﬁtting	O
occurs	O
when	O
the	O
value	O
of	O
the	O
hyper-	O
parameter	O
is	O
large	O
.	O
the	O
number	O
of	O
hidden	O
units	O
in	O
a	O
layer	O
is	O
one	O
such	O
example	O
,	O
428	O
chapter	O
11.	O
practical	O
methodology	O
because	O
increasing	O
the	O
number	O
of	O
hidden	O
units	O
increases	O
the	O
capacity	O
of	O
the	O
model	B
.	O
for	O
some	O
hyperparameters	O
,	O
overﬁtting	O
occurs	O
when	O
the	O
value	O
of	O
the	O
hyperparame-	O
ter	O
is	O
small	O
.	O
for	O
example	O
,	O
the	O
smallest	O
allowable	O
weight	O
decay	O
coeﬃcient	O
of	O
zero	O
corresponds	O
to	O
the	O
greatest	O
eﬀective	O
capacity	O
of	O
the	O
learning	O
algorithm	O
.	O
not	O
every	O
hyperparameter	O
will	O
be	O
able	O
to	O
explore	O
the	O
entire	O
u-shaped	O
curve	O
.	O
many	O
hyperparameters	O
are	O
discrete	O
,	O
such	O
as	O
the	O
number	O
of	O
units	O
in	O
a	O
layer	O
or	O
the	O
number	O
of	O
linear	O
pieces	O
in	O
a	O
maxout	O
unit	O
,	O
so	O
it	O
is	O
only	O
possible	O
to	O
visit	O
a	O
few	O
points	O
along	O
the	O
curve	O
.	O
some	O
hyperparameters	O
are	O
binary	O
.	O
usually	O
these	O
hyperparameters	O
are	O
switches	O
that	O
specify	O
whether	O
or	O
not	O
to	O
use	O
some	O
optional	O
component	O
of	O
the	O
learning	O
algorithm	O
,	O
such	O
as	O
a	O
preprocessing	O
step	O
that	O
normalizes	O
the	O
input	O
features	O
by	O
subtracting	O
their	O
mean	O
and	O
dividing	O
by	O
their	O
standard	O
deviation	O
.	O
these	O
hyperparameters	O
can	O
only	O
explore	O
two	O
points	O
on	O
the	O
curve	O
.	O
other	O
hyperparameters	O
have	O
some	O
minimum	O
or	O
maximum	O
value	O
that	O
prevents	O
them	O
from	O
exploring	O
some	O
part	O
of	O
the	O
curve	O
.	O
for	O
example	O
,	O
the	O
minimum	O
weight	O
decay	O
coeﬃcient	O
is	O
zero	O
.	O
this	O
means	O
that	O
if	O
the	O
model	B
is	O
underﬁtting	O
when	O
weight	O
decay	O
is	O
zero	O
,	O
we	O
can	O
not	O
enter	O
the	O
overﬁtting	O
region	O
by	O
modifying	O
the	O
weight	O
decay	O
coeﬃcient	O
.	O
in	O
other	O
words	O
,	O
some	O
hyperparameters	O
can	O
only	O
subtract	O
capacity	O
.	O
if	O
you	O
the	O
learning	O
rate	O
is	O
perhaps	O
the	O
most	O
important	O
hyperparameter	O
.	O
it	O
con-	O
have	O
time	O
to	O
tune	O
only	O
one	O
hyperparameter	O
,	O
tune	O
the	O
learning	O
rate	O
.	O
trols	O
the	O
eﬀective	O
capacity	O
of	O
the	O
model	B
in	O
a	O
more	O
complicated	O
way	O
than	O
other	O
hyperparameters—the	O
eﬀective	O
capacity	O
of	O
the	O
model	B
is	O
highest	O
when	O
the	O
learning	O
rate	O
is	O
correct	O
for	O
the	O
optimization	O
problem	O
,	O
not	O
when	O
the	O
learning	O
rate	O
is	O
especially	O
large	O
or	O
especially	O
small	O
.	O
the	O
learning	O
rate	O
has	O
a	O
u-shaped	O
curve	O
for	O
training	O
error	O
,	O
illustrated	O
in	O
ﬁgure	O
.	O
when	O
the	O
learning	O
rate	O
is	O
too	O
large	O
,	O
gradient	O
descent	B
can	O
inadvertently	O
increase	O
rather	O
than	O
decrease	O
the	O
training	O
error	O
.	O
in	O
the	O
idealized	O
quadratic	O
case	O
,	O
this	O
occurs	O
if	O
the	O
learning	O
rate	O
is	O
at	O
least	O
twice	O
as	O
large	O
as	O
its	O
optimal	O
value	O
(	O
)	O
.	O
when	O
the	O
learning	O
rate	O
is	O
too	O
small	O
,	O
training	O
is	O
not	O
only	O
slower	O
,	O
but	O
may	O
become	O
permanently	O
stuck	O
with	O
a	O
high	O
training	O
error	O
.	O
this	O
eﬀect	O
is	O
poorly	O
understood	O
(	O
it	O
would	O
not	O
happen	O
for	O
a	O
convex	O
loss	O
function	O
)	O
.	O
lecun	O
et	O
al	O
.	O
1998a	O
11.1	O
,	O
tuning	O
the	O
parameters	O
other	O
than	O
the	O
learning	O
rate	O
requires	O
monitoring	O
both	O
training	O
and	O
test	O
error	O
to	O
diagnose	O
whether	O
your	O
model	B
is	O
overﬁtting	O
or	O
underﬁtting	O
,	O
then	O
adjusting	O
its	O
capacity	O
appropriately	O
.	O
if	O
your	O
error	O
on	O
the	O
training	O
set	O
is	O
higher	O
than	O
your	O
target	O
error	O
rate	O
,	O
you	O
have	O
no	O
choice	O
but	O
to	O
increase	O
capacity	O
.	O
if	O
you	O
are	O
not	O
using	O
regularization	O
and	O
you	O
are	O
conﬁdent	O
that	O
your	O
optimization	O
algorithm	O
is	O
performing	O
correctly	O
,	O
then	O
you	O
must	O
add	O
more	O
layers	O
to	O
your	O
network	O
or	O
add	O
more	O
hidden	O
units	O
.	O
unfortunately	O
,	O
this	O
increases	O
the	O
computational	O
costs	O
associated	O
with	O
the	O
model	B
.	O
if	O
your	O
error	O
on	O
the	O
test	O
set	O
is	O
higher	O
than	O
than	O
your	O
target	O
error	O
rate	O
,	O
you	O
can	O
429	O
chapter	O
11.	O
practical	O
methodology	O
8	O
7	O
6	O
5	O
4	O
3	O
2	O
1	O
r	O
o	O
r	O
r	O
e	O
i	O
g	O
n	O
n	O
i	O
a	O
r	O
t	O
−	O
2	O
0	O
10	O
−	O
1	O
10	O
100	O
learning	O
rate	O
(	O
logarithmic	O
scale	O
)	O
figure	O
11.1	O
:	O
typical	O
relationship	O
between	O
the	O
learning	O
rate	O
and	O
the	O
training	O
error	O
.	O
notice	O
the	O
sharp	O
rise	O
in	O
error	O
when	O
the	O
learning	O
is	O
above	O
an	O
optimal	O
value	O
.	O
this	O
is	O
for	O
a	O
ﬁxed	O
training	O
time	O
,	O
as	O
a	O
smaller	O
learning	O
rate	O
may	O
sometimes	O
only	O
slow	O
down	O
training	O
by	O
a	O
factor	O
proportional	O
to	O
the	O
learning	O
rate	O
reduction	O
.	O
generalization	O
error	O
can	O
follow	O
this	O
curve	O
or	O
be	O
complicated	O
by	O
regularization	O
eﬀects	O
arising	O
out	O
of	O
having	O
a	O
too	O
large	O
or	O
too	O
small	O
learning	O
rates	O
,	O
since	O
poor	O
optimization	O
can	O
,	O
to	O
some	O
degree	O
,	O
reduce	O
or	O
prevent	O
overﬁtting	O
,	O
and	O
even	O
points	O
with	O
equivalent	O
training	O
error	O
can	O
have	O
diﬀerent	O
generalization	O
error	O
.	O
now	O
take	O
two	O
kinds	O
of	O
actions	O
.	O
the	O
test	O
error	O
is	O
the	O
sum	O
of	O
the	O
training	O
error	O
and	O
the	O
gap	O
between	O
training	O
and	O
test	O
error	O
.	O
the	O
optimal	O
test	O
error	O
is	O
found	O
by	O
trading	O
oﬀ	O
these	O
quantities	O
.	O
neural	O
networks	O
typically	O
perform	O
best	O
when	O
the	O
training	O
error	O
is	O
very	O
low	O
(	O
and	O
thus	O
,	O
when	O
capacity	O
is	O
high	O
)	O
and	O
the	O
test	O
error	O
is	O
primarily	O
driven	O
by	O
the	O
gap	O
between	O
train	O
and	O
test	O
error	O
.	O
your	O
goal	O
is	O
to	O
reduce	O
this	O
gap	O
without	O
increasing	O
training	O
error	O
faster	O
than	O
the	O
gap	O
decreases	O
.	O
to	O
reduce	O
the	O
gap	O
,	O
change	O
regularization	O
hyperparameters	O
to	O
reduce	O
eﬀective	O
model	B
capacity	O
,	O
such	O
as	O
by	O
adding	O
dropout	O
or	O
weight	O
decay	O
.	O
usually	O
the	O
best	O
performance	O
comes	O
from	O
a	O
large	O
model	B
that	O
is	O
regularized	O
well	O
,	O
for	O
example	O
by	O
using	O
dropout	O
.	O
most	O
hyperparameters	O
can	O
be	O
set	O
by	O
reasoning	O
about	O
whether	O
they	O
increase	O
or	O
decrease	O
model	B
capacity	O
.	O
some	O
examples	O
are	O
included	O
in	O
table	O
.	O
11.1	O
while	O
manually	O
tuning	O
hyperparameters	O
,	O
do	O
not	O
lose	O
sight	O
of	O
your	O
end	O
goal	O
:	O
good	O
performance	O
on	O
the	O
test	O
set	O
.	O
adding	O
regularization	O
is	O
only	O
one	O
way	O
to	O
achieve	O
this	O
goal	O
.	O
as	O
long	O
as	O
you	O
have	O
low	O
training	O
error	O
,	O
you	O
can	O
always	O
reduce	O
general-	O
ization	O
error	O
by	O
collecting	O
more	O
training	O
data	O
.	O
the	O
brute	O
force	O
way	O
to	O
practically	O
guarantee	O
success	O
is	O
to	O
continually	O
increase	O
model	B
capacity	O
and	O
training	O
set	O
size	O
until	O
the	O
task	O
is	O
solved	O
.	O
this	O
approach	O
does	O
of	O
course	O
increase	O
the	O
computational	O
cost	O
of	O
training	O
and	O
inference	O
,	O
so	O
it	O
is	O
only	O
feasible	O
given	O
appropriate	O
resources	O
.	O
in	O
430	O
chapter	O
11.	O
practical	O
methodology	O
reason	O
caveats	O
hyperparameter	O
number	O
of	O
hid-	O
den	O
units	O
increases	O
capacity	O
when	O
.	O
.	O
.	O
increased	O
learning	O
rate	O
tuned	O
op-	O
timally	O
convolution	O
ker-	O
nel	O
width	O
increased	O
increasing	O
the	O
number	O
of	O
hidden	O
units	O
increases	O
the	O
representational	O
capacity	O
of	O
the	O
model	B
.	O
an	O
improper	O
learning	O
rate	O
,	O
whether	O
too	O
high	O
or	O
too	O
low	O
,	O
results	O
in	O
a	O
model	B
with	O
low	O
eﬀective	O
capacity	O
due	O
to	O
optimization	O
failure	O
increasing	O
the	O
kernel	O
width	O
increases	O
the	O
number	O
of	O
pa-	O
rameters	O
in	O
the	O
model	B
increasing	O
the	O
number	O
of	O
hidden	O
units	O
increases	O
both	O
the	O
time	O
and	O
memory	O
cost	O
of	O
essentially	O
every	O
op-	O
eration	O
on	O
the	O
model	B
.	O
a	O
wider	O
kernel	O
results	O
in	O
a	O
narrower	O
output	O
dimen-	O
sion	O
,	O
reducing	O
model	B
ca-	O
pacity	O
unless	O
you	O
use	O
im-	O
plicit	O
zero	O
padding	O
to	O
re-	O
duce	O
this	O
eﬀect	O
.	O
wider	O
kernels	O
require	O
more	O
mem-	O
ory	O
for	O
parameter	O
storage	O
and	O
increase	O
runtime	O
,	O
but	O
a	O
narrower	O
output	O
reduces	O
memory	O
cost	O
.	O
increased	O
time	O
and	O
mem-	O
ory	O
cost	O
of	O
most	O
opera-	O
tions	O
.	O
implicit	O
padding	O
zero	O
weight	O
decay	O
co-	O
eﬃcient	O
dropout	O
rate	O
increased	O
adding	O
implicit	O
zeros	O
be-	O
fore	O
convolution	O
keeps	O
the	O
representation	O
size	O
large	O
decreased	O
decreasing	O
the	O
weight	O
de-	O
cay	O
coeﬃcient	O
frees	O
the	O
model	B
parameters	O
to	O
be-	O
come	O
larger	O
decreased	O
dropping	O
units	O
less	O
often	O
gives	O
the	O
units	O
more	O
oppor-	O
tunities	O
to	O
“	O
conspire	O
”	O
with	O
each	O
other	O
to	O
ﬁt	O
the	O
train-	O
ing	O
set	O
table	O
11.1	O
:	O
the	O
eﬀect	O
of	O
various	O
hyperparameters	O
on	O
model	B
capacity	O
.	O
431	O
chapter	O
11.	O
practical	O
methodology	O
principle	O
,	O
this	O
approach	O
could	O
fail	O
due	O
to	O
optimization	O
diﬃculties	O
,	O
but	O
for	O
many	O
problems	O
optimization	O
does	O
not	O
seem	O
to	O
be	O
a	O
signiﬁcant	O
barrier	O
,	O
provided	O
that	O
the	O
model	B
is	O
chosen	O
appropriately	O
.	O
11.4.2	O
automatic	O
hyperparameter	O
optimization	O
algorithms	O
the	O
ideal	O
learning	O
algorithm	O
just	O
takes	O
a	O
dataset	O
and	O
outputs	O
a	O
function	O
,	O
without	O
requiring	O
hand-tuning	O
of	O
hyperparameters	O
.	O
the	O
popularity	O
of	O
several	O
learning	O
algorithms	O
such	O
as	O
logistic	O
regression	O
and	O
svms	O
stems	O
in	O
part	O
from	O
their	O
ability	O
to	O
perform	O
well	O
with	O
only	O
one	O
or	O
two	O
tuned	O
hyperparameters	O
.	O
neural	O
networks	O
can	O
sometimes	O
perform	O
well	O
with	O
only	O
a	O
small	O
number	O
of	O
tuned	O
hyperparameters	O
,	O
but	O
often	O
beneﬁt	O
signiﬁcantly	O
from	O
tuning	O
of	O
forty	O
or	O
more	O
hyperparameters	O
.	O
manual	O
hyperparameter	O
tuning	O
can	O
work	B
very	O
well	O
when	O
the	O
user	O
has	O
a	O
good	O
starting	O
point	O
,	O
such	O
as	O
one	O
determined	O
by	O
others	O
having	O
worked	O
on	O
the	O
same	O
type	O
of	O
application	O
and	O
architecture	O
,	O
or	O
when	O
the	O
user	O
has	O
months	O
or	O
years	O
of	O
experience	O
in	O
exploring	O
hyperparameter	O
values	O
for	O
neural	O
networks	O
applied	O
to	O
similar	O
tasks	O
.	O
however	O
,	O
for	O
many	O
applications	O
,	O
these	O
starting	O
points	O
are	O
not	O
available	O
.	O
in	O
these	O
cases	O
,	O
automated	O
algorithms	O
can	O
ﬁnd	O
useful	O
values	O
of	O
the	O
hyperparameters	O
.	O
if	O
we	O
think	O
about	O
the	O
way	O
in	O
which	O
the	O
user	O
of	O
a	O
learning	O
algorithm	O
searches	O
for	O
good	O
values	O
of	O
the	O
hyperparameters	O
,	O
we	O
realize	O
that	O
an	O
optimization	O
is	O
taking	O
place	O
:	O
we	O
are	O
trying	O
to	O
ﬁnd	O
a	O
value	O
of	O
the	O
hyperparameters	O
that	O
optimizes	O
an	O
objective	O
function	O
,	O
such	O
as	O
validation	O
error	O
,	O
sometimes	O
under	O
constraints	O
(	O
such	O
as	O
a	O
budget	O
for	O
training	O
time	O
,	O
memory	O
or	O
recognition	B
time	O
)	O
.	O
it	O
is	O
therefore	O
possible	O
,	O
in	O
principle	O
,	O
to	O
develop	O
hyperparameter	O
optimization	O
algorithms	O
that	O
wrap	O
a	O
learning	O
algorithm	O
and	O
choose	O
its	O
hyperparameters	O
,	O
thus	O
hiding	O
the	O
hyperparameters	O
of	O
the	O
learning	O
algorithm	O
from	O
the	O
user	O
.	O
unfortunately	O
,	O
hyperparameter	O
optimization	O
algorithms	O
often	O
have	O
their	O
own	O
hyperparameters	O
,	O
such	O
as	O
the	O
range	O
of	O
values	O
that	O
should	O
be	O
explored	O
for	O
each	O
of	O
the	O
learning	O
algorithm	O
’	O
s	O
hyperparameters	O
.	O
however	O
,	O
these	O
secondary	O
hyperparameters	O
are	O
usually	O
easier	O
to	O
choose	O
,	O
in	O
the	O
sense	O
that	O
acceptable	O
performance	O
may	O
be	O
achieved	O
on	O
a	O
wide	O
range	O
of	O
tasks	O
using	O
the	O
same	O
secondary	O
hyperparameters	O
for	O
all	O
tasks	O
.	O
11.4.3	O
grid	O
search	O
when	O
there	O
are	O
three	O
or	O
fewer	O
hyperparameters	O
,	O
the	O
common	O
practice	O
is	O
to	O
perform	O
grid	O
search	O
.	O
for	O
each	O
hyperparameter	O
,	O
the	O
user	O
selects	O
a	O
small	O
ﬁnite	O
set	O
of	O
values	O
to	O
explore	O
.	O
the	O
grid	O
search	O
algorithm	O
then	O
trains	O
a	O
model	B
for	O
every	O
joint	O
speciﬁcation	O
of	O
hyperparameter	O
values	O
in	O
the	O
cartesian	O
product	O
of	O
the	O
set	O
of	O
values	O
for	O
each	O
individual	O
hyperparameter	O
.	O
the	O
experiment	O
that	O
yields	O
the	O
best	O
validation	O
432	O
chapter	O
11.	O
practical	O
methodology	O
grid	O
random	O
(	O
right	O
)	O
figure	O
11.2	O
:	O
comparison	O
of	O
grid	O
search	O
and	O
random	O
search	O
.	O
for	O
illustration	O
purposes	O
we	O
display	O
two	O
hyperparameters	O
but	O
we	O
are	O
typically	O
interested	O
in	O
having	O
many	O
more	O
.	O
(	O
left	O
)	O
to	O
perform	O
grid	O
search	O
,	O
we	O
provide	O
a	O
set	O
of	O
values	O
for	O
each	O
hyperparameter	O
.	O
the	O
search	O
algorithm	O
runs	O
training	O
for	O
every	O
joint	O
hyperparameter	O
setting	O
in	O
the	O
cross	O
product	O
of	O
these	O
sets	O
.	O
to	O
perform	O
random	O
search	O
,	O
we	O
provide	O
a	O
probability	O
distribution	O
over	O
joint	O
hyperparameter	O
conﬁgurations	O
.	O
usually	O
most	O
of	O
these	O
hyperparameters	O
are	O
independent	O
from	O
each	O
other	O
.	O
common	O
choices	O
for	O
the	O
distribution	O
over	O
a	O
single	O
hyperparameter	O
include	O
uniform	O
and	O
log-uniform	O
(	O
to	O
sample	O
from	O
a	O
log-uniform	O
distribution	O
,	O
take	O
the	O
exp	O
of	O
a	O
sample	O
from	O
a	O
uniform	O
distribution	O
)	O
.	O
the	O
search	O
algorithm	O
then	O
randomly	O
samples	O
joint	O
hyperparameter	O
conﬁgurations	O
and	O
runs	O
training	O
with	O
each	O
of	O
them	O
.	O
both	O
grid	O
search	O
and	O
random	O
search	O
evaluate	O
the	O
validation	O
set	O
error	O
and	O
return	O
the	O
best	O
conﬁguration	O
.	O
the	O
ﬁgure	O
illustrates	O
the	O
typical	O
case	O
where	O
only	O
some	O
hyperparameters	O
have	O
a	O
signiﬁcant	O
inﬂuence	O
on	O
the	O
result	O
.	O
in	O
this	O
illustration	O
,	O
only	O
the	O
hyperparameter	O
on	O
the	O
horizontal	O
axis	O
has	O
a	O
signiﬁcant	O
eﬀect	O
.	O
grid	O
search	O
wastes	O
an	O
amount	O
of	O
computation	O
that	O
is	O
exponential	O
in	O
the	O
number	O
of	O
non-inﬂuential	O
hyperparameters	O
,	O
while	O
random	O
search	O
tests	O
a	O
unique	O
value	O
of	O
every	O
inﬂuential	O
hyperparameter	O
on	O
nearly	O
every	O
trial	O
.	O
figure	O
reproduced	O
with	O
permission	O
from	O
bergstra	O
and	O
bengio	O
2012	O
(	O
)	O
.	O
433	O
chapter	O
11.	O
practical	O
methodology	O
set	O
error	O
is	O
then	O
chosen	O
as	O
having	O
found	O
the	O
best	O
hyperparameters	O
.	O
see	O
the	O
left	O
of	O
ﬁgure	O
for	O
an	O
illustration	O
of	O
a	O
grid	O
of	O
hyperparameter	O
values	O
.	O
11.2	O
how	O
should	O
the	O
lists	O
of	O
values	O
to	O
search	O
over	O
be	O
chosen	O
?	O
in	O
the	O
case	O
of	O
numerical	O
(	O
ordered	O
)	O
hyperparameters	O
,	O
the	O
smallest	O
and	O
largest	O
element	O
of	O
each	O
list	O
is	O
chosen	O
conservatively	O
,	O
based	O
on	O
prior	O
experience	O
with	O
similar	O
experiments	O
,	O
to	O
make	O
sure	O
that	O
the	O
optimal	O
value	O
is	O
very	O
likely	O
to	O
be	O
in	O
the	O
selected	O
range	O
.	O
typically	O
,	O
a	O
grid	O
search	O
involves	O
picking	O
values	O
approximately	O
on	O
a	O
logarithmic	O
scale	O
,	O
e.g.	O
,	O
a	O
learning	O
rate	O
taken	O
within	O
the	O
set	O
,	O
or	O
a	O
number	O
of	O
hidden	O
units	O
taken	O
with	O
the	O
set	O
{	O
−	O
−	O
−	O
{	O
}	O
3	O
,	O
10	O
4	O
,	O
10	O
5	O
.1	O
,	O
.01	O
,	O
10	O
.	O
50	O
100	O
200	O
500	O
1000	O
2000	O
,	O
,	O
,	O
}	O
,	O
,	O
}	O
grid	O
search	O
usually	O
performs	O
best	O
when	O
it	O
is	O
performed	O
repeatedly	O
.	O
for	O
example	O
,	O
suppose	O
that	O
we	O
ran	O
a	O
grid	O
search	O
over	O
a	O
hyperparameter	O
α	O
using	O
values	O
of	O
.	O
α	O
if	O
the	O
best	O
value	O
found	O
is	O
{	O
lies	O
and	O
we	O
should	O
shift	O
the	O
grid	O
and	O
run	O
another	O
search	O
with	O
α	O
in	O
,	O
for	O
example	O
,	O
,	O
then	O
we	O
may	O
wish	O
to	O
reﬁne	O
our	O
1	O
,	O
0	O
,	O
1	O
,	O
then	O
we	O
underestimated	O
the	O
range	O
in	O
which	O
the	O
best	O
1	O
}	O
.	O
if	O
we	O
ﬁnd	O
that	O
the	O
best	O
value	O
of	O
α	O
is	O
1	O
,	O
2	O
,	O
3	O
{	O
−	O
0	O
estimate	O
by	O
zooming	O
in	O
and	O
running	O
a	O
grid	O
search	O
over	O
{	O
−	O
}	O
.	O
.	O
,	O
,	O
.	O
1	O
0	O
1	O
the	O
obvious	O
problem	O
with	O
grid	O
search	O
is	O
that	O
its	O
computational	O
cost	O
grows	O
exponentially	O
with	O
the	O
number	O
of	O
hyperparameters	O
.	O
if	O
there	O
are	O
m	O
hyperparameters	O
,	O
each	O
taking	O
at	O
most	O
n	O
values	O
,	O
then	O
the	O
number	O
of	O
training	O
and	O
evaluation	O
trials	O
required	O
grows	O
as	O
o	O
(	O
nm	O
)	O
.	O
the	O
trials	O
may	O
be	O
run	O
in	O
parallel	O
and	O
exploit	O
loose	O
parallelism	O
(	O
with	O
almost	O
no	O
need	O
for	O
communication	O
between	O
diﬀerent	O
machines	O
carrying	O
out	O
the	O
search	O
)	O
unfortunately	O
,	O
due	O
to	O
the	O
exponential	O
cost	O
of	O
grid	O
search	O
,	O
even	O
parallelization	O
may	O
not	O
provide	O
a	O
satisfactory	O
size	O
of	O
search	O
.	O
11.4.4	O
random	O
search	O
fortunately	O
,	O
there	O
is	O
an	O
alternative	O
to	O
grid	O
search	O
that	O
is	O
as	O
simple	O
to	O
program	O
,	O
more	O
convenient	O
to	O
use	O
,	O
and	O
converges	O
much	O
faster	O
to	O
good	O
values	O
of	O
the	O
hyperparameters	O
:	O
random	O
search	O
(	O
bergstra	O
and	O
bengio	O
2012	O
)	O
.	O
,	O
a	O
random	O
search	O
proceeds	O
as	O
follows	O
.	O
first	O
we	O
deﬁne	O
a	O
marginal	O
distribution	O
for	O
each	O
hyperparameter	O
,	O
e.g.	O
,	O
a	O
bernoulli	O
or	O
multinoulli	O
for	O
binary	O
or	O
discrete	O
hyperparameters	O
,	O
or	O
a	O
uniform	O
distribution	O
on	O
a	O
log-scale	O
for	O
positive	O
real-valued	O
hyperparameters	O
.	O
for	O
example	O
,	O
log	O
learning	O
rate	O
_	O
_	O
∼	O
−	O
−	O
u	O
(	O
1	O
,	O
5	O
)	O
learning	O
rate	O
_	O
=	O
10log	O
learning	O
rate	O
_	O
_	O
.	O
(	O
11.2	O
)	O
(	O
11.3	O
)	O
where	O
u	O
(	O
a	O
,	O
b	O
)	O
indicates	O
a	O
sample	O
of	O
the	O
uniform	O
distribution	O
in	O
the	O
interval	O
(	O
a	O
,	O
b	O
)	O
.	O
may	O
be	O
sampled	O
from	O
u	O
(	O
log	O
(	O
50	O
)	O
,	O
similarly	O
the	O
log	O
number	O
of	O
hidden	O
units	O
log	O
(	O
2000	O
)	O
)	O
.	O
_	O
_	O
_	O
_	O
434	O
chapter	O
11.	O
practical	O
methodology	O
unlike	O
in	O
the	O
case	O
of	O
a	O
grid	O
search	O
,	O
one	O
should	O
not	O
discretize	O
or	O
bin	O
the	O
values	O
of	O
the	O
hyperparameters	O
.	O
this	O
allows	O
one	O
to	O
explore	O
a	O
larger	O
set	O
of	O
values	O
,	O
and	O
does	O
not	O
incur	O
additional	O
computational	O
cost	O
.	O
in	O
fact	O
,	O
as	O
illustrated	O
in	O
ﬁgure	O
,	O
a	O
random	O
search	O
can	O
be	O
exponentially	O
more	O
eﬃcient	O
than	O
a	O
grid	O
search	O
,	O
when	O
there	O
are	O
several	O
hyperparameters	O
that	O
do	O
not	O
strongly	O
aﬀect	O
the	O
performance	O
measure	O
.	O
this	O
is	O
studied	O
at	O
length	O
in	O
)	O
,	O
who	O
found	O
that	O
random	O
search	O
reduces	O
the	O
validation	O
set	O
error	O
much	O
faster	O
than	O
grid	O
search	O
,	O
in	O
terms	O
of	O
the	O
number	O
of	O
trials	O
run	O
by	O
each	O
method	O
.	O
bergstra	O
and	O
bengio	O
2012	O
11.2	O
(	O
as	O
with	O
grid	O
search	O
,	O
one	O
may	O
often	O
want	O
to	O
run	O
repeated	O
versions	O
of	O
random	O
search	O
,	O
to	O
reﬁne	O
the	O
search	O
based	O
on	O
the	O
results	O
of	O
the	O
ﬁrst	O
run	O
.	O
the	O
main	O
reason	O
why	O
random	O
search	O
ﬁnds	O
good	O
solutions	O
faster	O
than	O
grid	O
search	O
is	O
that	O
there	O
are	O
no	O
wasted	O
experimental	O
runs	O
,	O
unlike	O
in	O
the	O
case	O
of	O
grid	O
search	O
,	O
when	O
two	O
values	O
of	O
a	O
hyperparameter	O
(	O
given	O
values	O
of	O
the	O
other	O
hyperparameters	O
)	O
would	O
give	O
the	O
same	O
result	O
.	O
in	O
the	O
case	O
of	O
grid	O
search	O
,	O
the	O
other	O
hyperparameters	O
would	O
have	O
the	O
same	O
values	O
for	O
these	O
two	O
runs	O
,	O
whereas	O
with	O
random	O
search	O
,	O
they	O
would	O
usually	O
have	O
diﬀerent	O
values	O
.	O
hence	O
if	O
the	O
change	O
between	O
these	O
two	O
values	O
does	O
not	O
marginally	O
make	O
much	O
diﬀerence	O
in	O
terms	O
of	O
validation	O
set	O
error	O
,	O
grid	O
search	O
will	O
unnecessarily	O
repeat	O
two	O
equivalent	O
experiments	O
while	O
random	O
search	O
will	O
still	O
give	O
two	O
independent	O
explorations	O
of	O
the	O
other	O
hyperparameters	O
.	O
11.4.5	O
model-based	O
hyperparameter	O
optimization	O
the	O
search	O
for	O
good	O
hyperparameters	O
can	O
be	O
cast	O
as	O
an	O
optimization	O
problem	O
.	O
the	O
decision	O
variables	O
are	O
the	O
hyperparameters	O
.	O
the	O
cost	O
to	O
be	O
optimized	O
is	O
the	O
validation	O
set	O
error	O
that	O
results	O
from	O
training	O
using	O
these	O
hyperparameters	O
.	O
in	O
simpliﬁed	O
settings	O
where	O
it	O
is	O
feasible	O
to	O
compute	O
the	O
gradient	O
of	O
some	O
diﬀerentiable	O
error	O
measure	O
on	O
the	O
validation	O
set	O
with	O
respect	O
to	O
the	O
hyperparameters	O
,	O
we	O
can	O
simply	O
follow	O
this	O
gradient	O
(	O
bengio	O
et	O
al	O
.	O
1999	O
bengio	O
2000	O
maclaurin	O
et	O
al	O
.	O
,	O
2015	O
)	O
.	O
unfortunately	O
,	O
in	O
most	O
practical	O
settings	O
,	O
this	O
gradient	O
is	O
unavailable	O
,	O
either	O
due	O
to	O
its	O
high	O
computation	O
and	O
memory	O
cost	O
,	O
or	O
due	O
to	O
hyperparameters	O
having	O
intrinsically	O
non-diﬀerentiable	O
interactions	O
with	O
the	O
validation	O
set	O
error	O
,	O
as	O
in	O
the	O
case	O
of	O
discrete-valued	O
hyperparameters	O
.	O
,	O
;	O
,	O
;	O
to	O
compensate	O
for	O
this	O
lack	O
of	O
a	O
gradient	O
,	O
we	O
can	O
build	O
a	O
model	B
of	O
the	O
validation	O
set	O
error	O
,	O
then	O
propose	O
new	O
hyperparameter	O
guesses	O
by	O
performing	O
optimization	O
within	O
this	O
model	B
.	O
most	O
model-based	O
algorithms	O
for	O
hyperparameter	O
search	O
use	O
a	O
bayesian	O
regression	O
model	B
to	O
estimate	O
both	O
the	O
expected	O
value	O
of	O
the	O
validation	O
set	O
error	O
for	O
each	O
hyperparameter	O
and	O
the	O
uncertainty	O
around	O
this	O
expectation	O
.	O
opti-	O
mization	O
thus	O
involves	O
a	O
tradeoﬀ	O
between	O
exploration	O
(	O
proposing	O
hyperparameters	O
435	O
chapter	O
11.	O
practical	O
methodology	O
for	O
which	O
there	O
is	O
high	O
uncertainty	O
,	O
which	O
may	O
lead	O
to	O
a	O
large	O
improvement	O
but	O
may	O
also	O
perform	O
poorly	O
)	O
and	O
exploitation	O
(	O
proposing	O
hyperparameters	O
which	O
the	O
model	B
is	O
conﬁdent	O
will	O
perform	O
as	O
well	O
as	O
any	O
hyperparameters	O
it	O
has	O
seen	O
so	O
far—usually	O
hyperparameters	O
that	O
are	O
very	O
similar	O
to	O
ones	O
it	O
has	O
seen	O
before	O
)	O
.	O
contemporary	O
approaches	O
to	O
hyperparameter	O
optimization	O
include	O
spearmint	O
(	O
)	O
,	O
tpe	O
(	O
bergstra	O
et	O
al	O
.	O
2011	O
snoek	O
et	O
al	O
.	O
2012	O
)	O
.	O
,	O
,	O
)	O
and	O
smac	O
(	O
hutter	O
et	O
al	O
.	O
2011	O
,	O
currently	O
,	O
we	O
can	O
not	O
unambiguously	O
recommend	O
bayesian	O
hyperparameter	O
optimization	O
as	O
an	O
established	O
tool	O
for	O
achieving	O
better	O
deep	O
learning	O
results	O
or	O
for	O
obtaining	O
those	O
results	O
with	O
less	O
eﬀort	O
.	O
bayesian	O
hyperparameter	O
optimization	O
sometimes	O
performs	O
comparably	O
to	O
human	O
experts	O
,	O
sometimes	O
better	O
,	O
but	O
fails	O
catastrophically	O
on	O
other	O
problems	O
.	O
it	O
may	O
be	O
worth	O
trying	O
to	O
see	O
if	O
it	O
works	O
on	O
a	O
particular	O
problem	O
but	O
is	O
not	O
yet	O
suﬃciently	O
mature	O
or	O
reliable	O
.	O
that	O
being	O
said	O
,	O
hyperparameter	O
optimization	O
is	O
an	O
important	O
ﬁeld	O
of	O
research	O
that	O
,	O
while	O
often	O
driven	O
primarily	O
by	O
the	O
needs	O
of	O
deep	O
learning	O
,	O
holds	O
the	O
potential	O
to	O
beneﬁt	O
not	O
only	O
the	O
entire	O
ﬁeld	O
of	O
machine	O
learning	O
but	O
the	O
discipline	O
of	O
engineering	O
in	O
general	O
.	O
one	O
drawback	O
common	O
to	O
most	O
hyperparameter	O
optimization	O
algorithms	O
with	O
more	O
sophistication	O
than	O
random	O
search	O
is	O
that	O
they	O
require	O
for	O
a	O
training	O
ex-	O
periment	O
to	O
run	O
to	O
completion	O
before	O
they	O
are	O
able	O
to	O
extract	O
any	O
information	O
from	O
the	O
experiment	O
.	O
this	O
is	O
much	O
less	O
eﬃcient	O
,	O
in	O
the	O
sense	O
of	O
how	O
much	O
infor-	O
mation	O
can	O
be	O
gleaned	O
early	O
in	O
an	O
experiment	O
,	O
than	O
manual	O
search	O
by	O
a	O
human	O
practitioner	O
,	O
since	O
one	O
can	O
usually	O
tell	O
early	O
on	O
if	O
some	O
set	O
of	O
hyperparameters	O
is	O
completely	O
pathological.	O
)	O
have	O
introduced	O
an	O
early	O
version	O
of	O
an	O
algorithm	O
that	O
maintains	O
a	O
set	O
of	O
multiple	O
experiments	O
.	O
at	O
various	O
time	O
points	O
,	O
the	O
hyperparameter	O
optimization	O
algorithm	O
can	O
choose	O
to	O
begin	O
a	O
new	O
experiment	O
,	O
to	O
“	O
freeze	O
”	O
a	O
running	O
experiment	O
that	O
is	O
not	O
promising	O
,	O
or	O
to	O
“	O
thaw	O
”	O
and	O
resume	O
an	O
experiment	O
that	O
was	O
earlier	O
frozen	O
but	O
now	O
appears	O
promising	O
given	O
more	O
information	O
.	O
swersky	O
et	O
al	O
.	O
2014	O
(	O
11.5	O
debugging	O
strategies	O
when	O
a	O
machine	O
learning	O
system	O
performs	O
poorly	O
,	O
it	O
is	O
usually	O
diﬃcult	O
to	O
tell	O
whether	O
the	O
poor	O
performance	O
is	O
intrinsic	O
to	O
the	O
algorithm	O
itself	O
or	O
whether	O
there	O
is	O
a	O
bug	O
in	O
the	O
implementation	O
of	O
the	O
algorithm	O
.	O
machine	O
learning	O
systems	O
are	O
diﬃcult	O
to	O
debug	O
for	O
a	O
variety	O
of	O
reasons	O
.	O
in	O
most	O
cases	O
,	O
we	O
do	O
not	O
know	O
a	O
priori	O
what	O
the	O
intended	O
behavior	O
of	O
the	O
algorithm	O
is	O
.	O
in	O
fact	O
,	O
the	O
entire	O
point	O
of	O
using	O
machine	O
learning	O
is	O
that	O
it	O
will	O
discover	O
useful	O
behavior	O
that	O
we	O
were	O
not	O
able	O
to	O
specify	O
ourselves	O
.	O
if	O
we	O
train	O
a	O
436	O
chapter	O
11.	O
practical	O
methodology	O
classiﬁcation	O
task	O
and	O
it	O
achieves	O
5	O
%	O
test	O
error	O
,	O
we	O
have	O
neural	O
network	O
on	O
a	O
no	O
straightforward	O
way	O
of	O
knowing	O
if	O
this	O
is	O
the	O
expected	O
behavior	O
or	O
sub-optimal	O
behavior	O
.	O
new	O
a	O
further	O
diﬃculty	O
is	O
that	O
most	O
machine	O
learning	O
models	O
have	O
multiple	O
parts	O
that	O
are	O
each	O
adaptive	O
.	O
if	O
one	O
part	O
is	O
broken	O
,	O
the	O
other	O
parts	O
can	O
adapt	O
and	O
still	O
achieve	O
roughly	O
acceptable	O
performance	O
.	O
for	O
example	O
,	O
suppose	O
that	O
we	O
are	O
training	O
a	O
neural	O
net	O
with	O
several	O
layers	O
parametrized	O
by	O
weights	O
w	O
and	O
biases	O
b.	O
suppose	O
further	O
that	O
we	O
have	O
manually	O
implemented	O
the	O
gradient	O
descent	B
rule	O
for	O
each	O
parameter	O
separately	O
,	O
and	O
we	O
made	O
an	O
error	O
in	O
the	O
update	O
for	O
the	O
biases	O
:	O
←	O
−	O
b	O
b	O
α	O
(	O
11.4	O
)	O
where	O
α	O
is	O
the	O
learning	O
rate	O
.	O
this	O
erroneous	O
update	O
does	O
not	O
use	O
the	O
gradient	O
at	O
all	O
.	O
it	O
causes	O
the	O
biases	O
to	O
constantly	O
become	O
negative	O
throughout	O
learning	O
,	O
which	O
is	O
clearly	O
not	O
a	O
correct	O
implementation	O
of	O
any	O
reasonable	O
learning	O
algorithm	O
.	O
the	O
bug	O
may	O
not	O
be	O
apparent	O
just	O
from	O
examining	O
the	O
output	O
of	O
the	O
model	B
though	O
.	O
depending	O
on	O
the	O
distribution	O
of	O
the	O
input	O
,	O
the	O
weights	O
may	O
be	O
able	O
to	O
adapt	O
to	O
compensate	O
for	O
the	O
negative	O
biases	O
.	O
most	O
debugging	O
strategies	O
for	O
neural	O
nets	O
are	O
designed	O
to	O
get	O
around	O
one	O
or	O
both	O
of	O
these	O
two	O
diﬃculties	O
.	O
either	O
we	O
design	O
a	O
case	O
that	O
is	O
so	O
simple	O
that	O
the	O
correct	O
behavior	O
actually	O
can	O
be	O
predicted	O
,	O
or	O
we	O
design	O
a	O
test	O
that	O
exercises	O
one	O
part	O
of	O
the	O
neural	O
net	O
implementation	O
in	O
isolation	O
.	O
some	O
important	O
debugging	O
tests	O
include	O
:	O
visualize	O
the	O
model	B
in	O
action	O
:	O
when	O
training	O
a	O
model	B
to	O
detect	O
objects	O
in	O
images	O
,	O
view	O
some	O
images	O
with	O
the	O
detections	O
proposed	O
by	O
the	O
model	B
displayed	O
superimposed	O
on	O
the	O
image	O
.	O
when	O
training	O
a	O
generative	O
model	B
of	O
speech	O
,	O
listen	O
to	O
some	O
of	O
the	O
speech	O
samples	O
it	O
produces	O
.	O
this	O
may	O
seem	O
obvious	O
,	O
but	O
it	O
is	O
easy	O
to	O
fall	O
into	O
the	O
practice	O
of	O
only	O
looking	O
at	O
quantitative	O
performance	O
measurements	O
like	O
accuracy	O
or	O
log-likelihood	O
.	O
directly	O
observing	O
the	O
machine	O
learning	O
model	B
performing	O
its	O
task	O
will	O
help	O
to	O
determine	O
whether	O
the	O
quantitative	O
performance	O
numbers	O
it	O
achieves	O
seem	O
reasonable	O
.	O
evaluation	O
bugs	O
can	O
be	O
some	O
of	O
the	O
most	O
devastating	O
bugs	O
because	O
they	O
can	O
mislead	O
you	O
into	O
believing	O
your	O
system	O
is	O
performing	O
well	O
when	O
it	O
is	O
not	O
.	O
visualize	O
the	O
worst	O
mistakes	O
:	O
most	O
models	O
are	O
able	O
to	O
output	O
some	O
sort	O
of	O
conﬁdence	O
measure	O
for	O
the	O
task	O
they	O
perform	O
.	O
for	O
example	O
,	O
classiﬁers	O
based	O
on	O
a	O
softmax	O
output	O
layer	O
assign	O
a	O
probability	O
to	O
each	O
class	O
.	O
the	O
probability	O
assigned	O
to	O
the	O
most	O
likely	O
class	O
thus	O
gives	O
an	O
estimate	O
of	O
the	O
conﬁdence	O
the	O
model	B
has	O
in	O
its	O
classiﬁcation	O
decision	O
.	O
typically	O
,	O
maximum	O
likelihood	O
training	O
results	O
in	O
these	O
values	O
being	O
overestimates	O
rather	O
than	O
accurate	O
probabilities	O
of	O
correct	O
prediction	O
,	O
437	O
chapter	O
11.	O
practical	O
methodology	O
but	O
they	O
are	O
somewhat	O
useful	O
in	O
the	O
sense	O
that	O
examples	O
that	O
are	O
actually	O
less	O
likely	O
to	O
be	O
correctly	O
labeled	O
receive	O
smaller	O
probabilities	O
under	O
the	O
model	B
.	O
by	O
viewing	O
the	O
training	O
set	O
examples	O
that	O
are	O
the	O
hardest	O
to	O
model	B
correctly	O
,	O
one	O
can	O
often	O
discover	O
problems	O
with	O
the	O
way	O
the	O
data	O
has	O
been	O
preprocessed	O
or	O
labeled	O
.	O
for	O
example	O
,	O
the	O
street	O
view	O
transcription	O
system	O
originally	O
had	O
a	O
problem	O
where	O
the	O
address	O
number	O
detection	O
system	O
would	O
crop	O
the	O
image	O
too	O
tightly	O
and	O
omit	O
some	O
of	O
the	O
digits	O
.	O
the	O
transcription	O
network	O
then	O
assigned	O
very	O
low	O
probability	O
to	O
the	O
correct	O
answer	O
on	O
these	O
images	O
.	O
sorting	O
the	O
images	O
to	O
identify	O
the	O
most	O
conﬁdent	O
mistakes	O
showed	O
that	O
there	O
was	O
a	O
systematic	O
problem	O
with	O
the	O
cropping	O
.	O
modifying	O
the	O
detection	O
system	O
to	O
crop	O
much	O
wider	O
images	O
resulted	O
in	O
much	O
better	O
performance	O
of	O
the	O
overall	O
system	O
,	O
even	O
though	O
the	O
transcription	O
network	O
needed	O
to	O
be	O
able	O
to	O
process	O
greater	O
variation	O
in	O
the	O
position	B
and	O
scale	O
of	O
the	O
address	O
numbers	O
.	O
reasoning	O
about	O
software	O
using	O
train	O
and	O
test	O
error	O
:	O
it	O
is	O
often	O
diﬃcult	O
to	O
determine	O
whether	O
the	O
underlying	O
software	O
is	O
correctly	O
implemented	O
.	O
some	O
clues	O
can	O
be	O
obtained	O
from	O
the	O
train	O
and	O
test	O
error	O
.	O
if	O
training	O
error	O
is	O
low	O
but	O
test	O
error	O
is	O
high	O
,	O
then	O
it	O
is	O
likely	O
that	O
that	O
the	O
training	O
procedure	O
works	O
correctly	O
,	O
and	O
the	O
model	B
is	O
overﬁtting	O
for	O
fundamental	O
algorithmic	O
reasons	O
.	O
an	O
alternative	O
possibility	O
is	O
that	O
the	O
test	O
error	O
is	O
measured	O
incorrectly	O
due	O
to	O
a	O
problem	O
with	O
saving	O
the	O
model	B
after	O
training	O
then	O
reloading	O
it	O
for	O
test	O
set	O
evaluation	O
,	O
or	O
if	O
the	O
test	O
data	O
was	O
prepared	O
diﬀerently	O
from	O
the	O
training	O
data	O
.	O
if	O
both	O
train	O
and	O
test	O
error	O
are	O
high	O
,	O
then	O
it	O
is	O
diﬃcult	O
to	O
determine	O
whether	O
there	O
is	O
a	O
software	O
defect	O
or	O
whether	O
the	O
model	B
is	O
underﬁtting	O
due	O
to	O
fundamental	O
algorithmic	O
reasons	O
.	O
this	O
scenario	O
requires	O
further	O
tests	O
,	O
described	O
next	O
.	O
fit	O
a	O
tiny	O
dataset	O
:	O
if	O
you	O
have	O
high	O
error	O
on	O
the	O
training	O
set	O
,	O
determine	O
whether	O
it	O
is	O
due	O
to	O
genuine	O
underﬁtting	O
or	O
due	O
to	O
a	O
software	O
defect	O
.	O
usually	O
even	O
small	O
models	O
can	O
be	O
guaranteed	O
to	O
be	O
able	O
ﬁt	O
a	O
suﬃciently	O
small	O
dataset	O
.	O
for	O
example	O
,	O
a	O
classiﬁcation	O
dataset	O
with	O
only	O
one	O
example	O
can	O
be	O
ﬁt	O
just	O
by	O
setting	O
the	O
biases	O
of	O
the	O
output	O
layer	O
correctly	O
.	O
usually	O
if	O
you	O
can	O
not	O
train	O
a	O
classiﬁer	O
to	O
correctly	O
label	O
a	O
single	O
example	O
,	O
an	O
autoencoder	O
to	O
successfully	O
reproduce	O
a	O
single	O
example	O
with	O
high	O
ﬁdelity	O
,	O
or	O
a	O
generative	O
model	B
to	O
consistently	O
emit	O
samples	O
resembling	O
a	O
single	O
example	O
,	O
there	O
is	O
a	O
software	O
defect	O
preventing	O
successful	O
optimization	O
on	O
the	O
training	O
set	O
.	O
this	O
test	O
can	O
be	O
extended	O
to	O
a	O
small	O
dataset	O
with	O
few	O
examples	O
.	O
compare	O
back-propagated	O
derivatives	O
to	O
numerical	O
derivatives	O
:	O
if	O
you	O
are	O
using	O
a	O
software	O
framework	O
that	O
requires	O
you	O
to	O
implement	O
your	O
own	O
gradient	O
com-	O
putations	O
,	O
or	O
if	O
you	O
are	O
adding	O
a	O
new	O
operation	O
to	O
a	O
diﬀerentiation	O
library	O
and	O
must	O
deﬁne	O
its	O
bprop	O
method	O
,	O
then	O
a	O
common	O
source	O
of	O
error	O
is	O
implementing	O
this	O
gradient	O
expression	O
incorrectly	O
.	O
one	O
way	O
to	O
verify	O
that	O
these	O
derivatives	O
are	O
correct	O
438	O
chapter	O
11.	O
practical	O
methodology	O
is	O
to	O
compare	O
the	O
derivatives	O
computed	O
by	O
your	O
implementation	O
of	O
automatic	O
diﬀerentiation	O
to	O
the	O
derivatives	O
computed	O
by	O
a	O
ﬁnite	O
diﬀerences	O
−	O
.	O
because	O
	O
f	O
→	O
(	O
)	O
=	O
lim	O
x	O
0	O
	O
f	O
x	O
(	O
+	O
)	O
	O
	O
f	O
x	O
(	O
)	O
,	O
(	O
11.5	O
)	O
we	O
can	O
approximate	O
the	O
derivative	O
by	O
using	O
a	O
small	O
,	O
ﬁnite	O
:	O
	O
−	O
≈	O
f	O
x	O
	O
f	O
(	O
)	O
x	O
(	O
+	O
)	O
	O
	O
f	O
x	O
(	O
)	O
.	O
(	O
11.6	O
)	O
we	O
can	O
improve	O
the	O
accuracy	O
of	O
the	O
approximation	O
by	O
using	O
the	O
centered	O
diﬀer-	O
ence	O
:	O
	O
f	O
(	O
)	O
x	O
≈	O
f	O
x	O
(	O
+	O
1	O
	O
)	O
2	O
−	O
f	O
x	O
(	O
1	O
2	O
	O
)	O
−	O
	O
.	O
(	O
11.7	O
)	O
the	O
perturbation	O
size	O
	O
must	O
chosen	O
to	O
be	O
large	O
enough	O
to	O
ensure	O
that	O
the	O
pertur-	O
bation	O
is	O
not	O
rounded	O
down	O
too	O
much	O
by	O
ﬁnite-precision	O
numerical	O
computations	O
.	O
→	O
m	O
r	O
usually	O
,	O
we	O
will	O
want	O
to	O
test	O
the	O
gradient	O
or	O
jacobian	O
of	O
a	O
vector-valued	O
function	O
n.	O
unfortunately	O
,	O
ﬁnite	O
diﬀerencing	O
only	O
allows	O
us	O
to	O
take	O
a	O
single	O
g	O
:	O
r	O
derivative	O
at	O
a	O
time	O
.	O
we	O
can	O
either	O
run	O
ﬁnite	O
diﬀerencing	O
mn	O
times	O
to	O
evaluate	O
all	O
of	O
the	O
partial	O
derivatives	O
of	O
g	O
,	O
or	O
we	O
can	O
apply	O
the	O
test	O
to	O
a	O
new	O
function	O
that	O
uses	O
random	O
projections	O
at	O
both	O
the	O
input	O
and	O
output	O
of	O
g.	O
for	O
example	O
,	O
we	O
can	O
apply	O
our	O
test	O
of	O
the	O
implementation	O
of	O
the	O
derivatives	O
to	O
f	O
(	O
x	O
)	O
where	O
f	O
(	O
x	O
)	O
=	O
ut	O
g	O
(	O
vx	O
)	O
,	O
where	O
u	O
and	O
v	O
are	O
randomly	O
chosen	O
vectors	O
.	O
computing	O
f	O
(	O
x	O
)	O
correctly	O
requires	O
being	O
able	O
to	O
back-propagate	O
through	O
g	O
correctly	O
,	O
yet	O
is	O
eﬃcient	O
to	O
do	O
with	O
ﬁnite	O
diﬀerences	O
because	O
f	O
has	O
only	O
a	O
single	O
input	O
and	O
a	O
single	O
output	O
.	O
it	O
is	O
usually	O
a	O
good	O
idea	O
to	O
repeat	O
this	O
test	O
for	O
more	O
than	O
one	O
value	O
of	O
u	O
and	O
v	O
to	O
reduce	O
the	O
chance	O
that	O
the	O
test	O
overlooks	O
mistakes	O
that	O
are	O
orthogonal	O
to	O
the	O
random	O
projection	O
.	O
	O
if	O
one	O
has	O
access	O
to	O
numerical	O
computation	O
on	O
complex	O
numbers	O
,	O
then	O
there	O
is	O
a	O
very	O
eﬃcient	O
way	O
to	O
numerically	O
estimate	O
the	O
gradient	O
by	O
using	O
complex	O
numbers	O
as	O
input	O
to	O
the	O
function	O
(	O
squire	O
and	O
trapp	O
1998	O
)	O
.	O
the	O
method	O
is	O
based	O
on	O
the	O
observation	O
that	O
,	O
f	O
x	O
(	O
+	O
)	O
=	O
(	O
)	O
+	O
f	O
x	O
i	O
if	O
(	O
)	O
+	O
(	O
x	O
o	O
2	O
)	O
	O
real	O
(	O
(	O
+	O
)	O
)	O
=	O
(	O
)	O
+	O
(	O
f	O
x	O
f	O
x	O
i	O
o	O
2	O
)	O
,	O
√−	O
	O
)	O
=	O
f	O
(	O
)	O
+	O
(	O
x	O
o	O
2	O
)	O
,	O
(	O
11.9	O
)	O
(	O
11.8	O
)	O
imag	O
(	O
f	O
x	O
(	O
+	O
)	O
i	O
	O
where	O
i	O
=	O
1.	O
unlike	O
in	O
the	O
real-valued	O
case	O
above	O
,	O
there	O
is	O
no	O
cancellation	O
eﬀect	O
due	O
to	O
taking	O
the	O
diﬀerence	O
between	O
the	O
value	O
of	O
f	O
at	O
diﬀerent	O
points	O
.	O
this	O
allows	O
−	O
150	O
,	O
which	O
make	O
the	O
o	O
(	O
2	O
)	O
error	O
insigniﬁcant	O
the	O
use	O
of	O
tiny	O
values	O
of	O
	O
like	O
	O
=	O
10	O
for	O
all	O
practical	O
purposes	O
.	O
439	O
chapter	O
11.	O
practical	O
methodology	O
monitor	O
histograms	O
of	O
activations	O
and	O
gradient	O
:	O
it	O
is	O
often	O
useful	O
to	O
visualize	O
statistics	O
of	O
neural	O
network	O
activations	O
and	O
gradients	O
,	O
collected	O
over	O
a	O
large	O
amount	O
of	O
training	O
iterations	O
(	O
maybe	O
one	O
epoch	O
)	O
.	O
the	O
pre-activation	O
value	O
of	O
hidden	O
units	O
can	O
tell	O
us	O
if	O
the	O
units	O
saturate	O
,	O
or	O
how	O
often	O
they	O
do	O
.	O
for	O
example	O
,	O
for	O
rectiﬁers	O
,	O
how	O
often	O
are	O
they	O
oﬀ	O
?	O
are	O
there	O
units	O
that	O
are	O
always	O
oﬀ	O
?	O
for	O
tanh	B
units	O
,	O
the	O
average	O
of	O
the	O
absolute	O
value	O
of	O
the	O
pre-activations	O
tells	O
us	O
how	O
saturated	O
the	O
unit	O
is	O
.	O
in	O
a	O
deep	O
network	O
where	O
the	O
propagated	O
gradients	O
quickly	O
grow	O
or	O
quickly	O
vanish	O
,	O
optimization	O
may	O
be	O
hampered	O
.	O
finally	O
,	O
it	O
is	O
useful	O
to	O
compare	O
the	O
magnitude	O
of	O
parameter	O
gradients	O
to	O
the	O
magnitude	O
of	O
the	O
parameters	O
themselves	O
.	O
as	O
suggested	O
by	O
)	O
,	O
we	O
would	O
like	O
the	O
magnitude	O
of	O
parameter	O
updates	O
over	O
a	O
minibatch	O
to	O
represent	O
something	O
like	O
1	O
%	O
of	O
the	O
magnitude	O
of	O
the	O
parameter	O
,	O
not	O
50	O
%	O
or	O
0.001	O
%	O
(	O
which	O
would	O
make	O
the	O
parameters	O
move	O
too	O
slowly	O
)	O
.	O
it	O
may	O
be	O
that	O
some	O
groups	O
of	O
parameters	O
are	O
moving	O
at	O
a	O
good	O
pace	O
while	O
others	O
are	O
stalled	O
.	O
when	O
the	O
data	O
is	O
sparse	O
(	O
like	O
in	O
natural	O
language	O
)	O
,	O
some	O
parameters	O
may	O
be	O
very	O
rarely	O
updated	O
,	O
and	O
this	O
should	O
be	O
kept	O
in	O
mind	O
when	O
monitoring	O
their	O
evolution	O
.	O
bottou	O
2015	O
(	O
iii	O
finally	O
,	O
many	O
deep	O
learning	O
algorithms	O
provide	O
some	O
sort	O
of	O
guarantee	O
about	O
the	O
results	O
produced	O
at	O
each	O
step	O
.	O
for	O
example	O
,	O
in	O
part	O
,	O
we	O
will	O
see	O
some	O
approx-	O
imate	O
inference	O
algorithms	O
that	O
work	B
by	O
using	O
algebraic	O
solutions	O
to	O
optimization	O
problems	O
.	O
typically	O
these	O
can	O
be	O
debugged	O
by	O
testing	O
each	O
of	O
their	O
guarantees	O
.	O
some	O
guarantees	O
that	O
some	O
optimization	O
algorithms	O
oﬀer	O
include	O
that	O
the	O
objective	O
function	O
will	O
never	O
increase	O
after	O
one	O
step	O
of	O
the	O
algorithm	O
,	O
that	O
the	O
gradient	O
with	O
respect	O
to	O
some	O
subset	O
of	O
variables	O
will	O
be	O
zero	O
after	O
each	O
step	O
of	O
the	O
algorithm	O
,	O
and	O
that	O
the	O
gradient	O
with	O
respect	O
to	O
all	O
variables	O
will	O
be	O
zero	O
at	O
convergence	O
.	O
usually	O
due	O
to	O
rounding	O
error	O
,	O
these	O
conditions	B
will	O
not	O
hold	O
exactly	O
in	O
a	O
digital	O
computer	O
,	O
so	O
the	O
debugging	O
test	O
should	O
include	O
some	O
tolerance	O
parameter	O
.	O
11.6	O
example	O
:	O
multi-digit	O
number	O
recognition	B
to	O
provide	O
an	O
end-to-end	O
description	O
of	O
how	O
to	O
apply	O
our	O
design	O
methodology	O
in	O
practice	O
,	O
we	O
present	O
a	O
brief	O
account	O
of	O
the	O
street	O
view	O
transcription	O
system	O
,	O
from	O
the	O
point	O
of	O
view	O
of	O
designing	O
the	O
deep	O
learning	O
components	O
.	O
obviously	O
,	O
many	O
other	O
components	O
of	O
the	O
complete	O
system	O
,	O
such	O
as	O
the	O
street	O
view	O
cars	O
,	O
the	O
database	O
infrastructure	O
,	O
and	O
so	O
on	O
,	O
were	O
of	O
paramount	O
importance	O
.	O
from	O
the	O
point	O
of	O
view	O
of	O
the	O
machine	O
learning	O
task	O
,	O
the	O
process	O
began	O
with	O
data	O
collection	O
.	O
the	O
cars	O
collected	O
the	O
raw	O
data	O
and	O
human	O
operators	O
provided	O
labels	O
.	O
the	O
transcription	O
task	O
was	O
preceded	O
by	O
a	O
signiﬁcant	O
amount	O
of	O
dataset	O
curation	O
,	O
including	O
using	O
other	O
machine	O
learning	O
techniques	O
to	O
detect	O
the	O
house	O
440	O
chapter	O
11.	O
practical	O
methodology	O
numbers	O
prior	O
to	O
transcribing	O
them	O
.	O
the	O
transcription	O
project	O
began	O
with	O
a	O
choice	O
of	O
performance	O
metrics	O
and	O
desired	O
values	O
for	O
these	O
metrics	O
.	O
an	O
important	O
general	O
principle	O
is	O
to	O
tailor	O
the	O
choice	O
of	O
metric	O
to	O
the	O
business	O
goals	O
for	O
the	O
project	O
.	O
because	O
maps	O
are	O
only	O
useful	O
if	O
they	O
have	O
high	O
accuracy	O
,	O
it	O
was	O
important	O
to	O
set	O
a	O
high	O
accuracy	O
requirement	O
for	O
this	O
project	O
.	O
speciﬁcally	O
,	O
the	O
goal	O
was	O
to	O
obtain	O
human-level	O
,	O
98	O
%	O
accuracy	O
.	O
this	O
level	O
of	O
accuracy	O
may	O
not	O
always	O
be	O
feasible	O
to	O
obtain	O
.	O
in	O
order	O
to	O
reach	O
this	O
level	O
of	O
accuracy	O
,	O
the	O
street	O
view	O
transcription	O
system	O
sacriﬁces	O
coverage	O
.	O
coverage	O
thus	O
became	O
the	O
main	O
performance	O
metric	O
optimized	O
during	O
the	O
project	O
,	O
with	O
accuracy	O
held	O
at	O
98	O
%	O
.	O
as	O
the	O
convolutional	O
network	O
improved	O
,	O
it	O
became	O
possible	O
to	O
reduce	O
the	O
conﬁdence	O
threshold	O
below	O
which	O
the	O
network	O
refuses	O
to	O
transcribe	O
the	O
input	O
,	O
eventually	O
exceeding	O
the	O
goal	O
of	O
95	O
%	O
coverage	O
.	O
after	O
choosing	O
quantitative	O
goals	O
,	O
the	O
next	O
step	O
in	O
our	O
recommended	O
methodol-	O
ogy	O
is	O
to	O
rapidly	O
establish	O
a	O
sensible	O
baseline	O
system	O
.	O
for	O
vision	O
tasks	O
,	O
this	O
means	O
a	O
convolutional	O
network	O
with	O
rectiﬁed	O
linear	O
units	O
.	O
the	O
transcription	O
project	O
began	O
with	O
such	O
a	O
model	B
.	O
at	O
the	O
time	O
,	O
it	O
was	O
not	O
common	O
for	O
a	O
convolutional	O
network	O
to	O
output	O
a	O
sequence	O
of	O
predictions	O
.	O
in	O
order	O
to	O
begin	O
with	O
the	O
simplest	O
possible	O
baseline	O
,	O
the	O
ﬁrst	O
implementation	O
of	O
the	O
output	O
layer	O
of	O
the	O
model	B
consisted	O
of	O
n	O
diﬀerent	O
softmax	O
units	O
to	O
predict	O
a	O
sequence	O
of	O
n	O
characters	O
.	O
these	O
softmax	O
units	O
were	O
trained	O
exactly	O
the	O
same	O
as	O
if	O
the	O
task	O
were	O
classiﬁcation	O
,	O
with	O
each	O
softmax	O
unit	O
trained	O
independently	O
.	O
our	O
recommended	O
methodology	O
is	O
to	O
iteratively	O
reﬁne	O
the	O
baseline	O
and	O
test	O
whether	O
each	O
change	O
makes	O
an	O
improvement	O
.	O
the	O
ﬁrst	O
change	O
to	O
the	O
street	O
view	O
transcription	O
system	O
was	O
motivated	O
by	O
a	O
theoretical	O
understanding	O
of	O
the	O
coverage	O
metric	O
and	O
the	O
structure	O
of	O
the	O
data	O
.	O
speciﬁcally	O
,	O
the	O
network	O
refuses	O
to	O
classify	O
an	O
input	O
x	O
whenever	O
the	O
probability	O
of	O
the	O
output	O
sequence	O
p	O
(	O
y	O
x	O
)	O
<	O
t	O
for	O
some	O
threshold	O
t.	O
initially	O
,	O
the	O
deﬁnition	O
of	O
p	O
(	O
y	O
x	O
)	O
was	O
ad-hoc	O
,	O
based	O
on	O
simply	O
multiplying	O
all	O
of	O
the	O
softmax	O
outputs	O
together	O
.	O
this	O
motivated	O
the	O
development	O
of	O
a	O
specialized	O
output	O
layer	O
and	O
cost	O
function	O
that	O
actually	O
computed	O
a	O
principled	O
log-likelihood	O
.	O
this	O
approach	O
allowed	O
the	O
example	O
rejection	O
mechanism	O
to	O
function	O
much	O
more	O
eﬀectively	O
.	O
|	O
|	O
at	O
this	O
point	O
,	O
coverage	O
was	O
still	O
below	O
90	O
%	O
,	O
yet	O
there	O
were	O
no	O
obvious	O
theoretical	O
problems	O
with	O
the	O
approach	O
.	O
our	O
methodology	O
therefore	O
suggests	O
to	O
instrument	O
the	O
train	O
and	O
test	O
set	O
performance	O
in	O
order	O
to	O
determine	O
whether	O
the	O
problem	O
is	O
underﬁtting	O
or	O
overﬁtting	O
.	O
in	O
this	O
case	O
,	O
train	O
and	O
test	O
set	O
error	O
were	O
nearly	O
identical	O
.	O
indeed	O
,	O
the	O
main	O
reason	O
this	O
project	O
proceeded	O
so	O
smoothly	O
was	O
the	O
availability	O
of	O
a	O
dataset	O
with	O
tens	O
of	O
millions	O
of	O
labeled	O
examples	O
.	O
because	O
train	O
and	O
test	O
set	O
error	O
were	O
so	O
similar	O
,	O
this	O
suggested	O
that	O
the	O
problem	O
was	O
either	O
due	O
441	O
chapter	O
11.	O
practical	O
methodology	O
to	O
underﬁtting	O
or	O
due	O
to	O
a	O
problem	O
with	O
the	O
training	O
data	O
.	O
one	O
of	O
the	O
debugging	O
strategies	O
we	O
recommend	O
is	O
to	O
visualize	O
the	O
model	B
’	O
s	O
worst	O
errors	O
.	O
in	O
this	O
case	O
,	O
that	O
meant	O
visualizing	O
the	O
incorrect	O
training	O
set	O
transcriptions	O
that	O
the	O
model	B
gave	O
the	O
highest	O
conﬁdence	O
.	O
these	O
proved	O
to	O
mostly	O
consist	O
of	O
examples	O
where	O
the	O
input	O
image	O
had	O
been	O
cropped	O
too	O
tightly	O
,	O
with	O
some	O
of	O
the	O
digits	O
of	O
the	O
address	O
being	O
removed	O
by	O
the	O
cropping	O
operation	O
.	O
for	O
example	O
,	O
a	O
photo	O
of	O
an	O
address	O
“	O
1849	O
”	O
might	O
be	O
cropped	O
too	O
tightly	O
,	O
with	O
only	O
the	O
“	O
849	O
”	O
remaining	O
visible	O
.	O
this	O
problem	O
could	O
have	O
been	O
resolved	O
by	O
spending	O
weeks	O
improving	O
the	O
accuracy	O
of	O
the	O
address	O
number	O
detection	O
system	O
responsible	O
for	O
determining	O
the	O
cropping	O
regions	O
.	O
instead	O
,	O
the	O
team	O
took	O
a	O
much	O
more	O
practical	O
decision	O
,	O
to	O
simply	O
expand	O
the	O
width	O
of	O
the	O
crop	O
region	O
to	O
be	O
systematically	O
wider	O
than	O
the	O
address	O
number	O
detection	O
system	O
predicted	O
.	O
this	O
single	O
change	O
added	O
ten	O
percentage	O
points	O
to	O
the	O
transcription	O
system	O
’	O
s	O
coverage	O
.	O
finally	O
,	O
the	O
last	O
few	O
percentage	O
points	O
of	O
performance	O
came	O
from	O
adjusting	O
hyperparameters	O
.	O
this	O
mostly	O
consisted	O
of	O
making	O
the	O
model	B
larger	O
while	O
main-	O
taining	O
some	O
restrictions	O
on	O
its	O
computational	O
cost	O
.	O
because	O
train	O
and	O
test	O
error	O
remained	O
roughly	O
equal	O
,	O
it	O
was	O
always	O
clear	O
that	O
any	O
performance	O
deﬁcits	O
were	O
due	O
to	O
underﬁtting	O
,	O
as	O
well	O
as	O
due	O
to	O
a	O
few	O
remaining	O
problems	O
with	O
the	O
dataset	O
itself	O
.	O
overall	O
,	O
the	O
transcription	O
project	O
was	O
a	O
great	O
success	O
,	O
and	O
allowed	O
hundreds	O
of	O
millions	O
of	O
addresses	O
to	O
be	O
transcribed	O
both	O
faster	O
and	O
at	O
lower	O
cost	O
than	O
would	O
have	O
been	O
possible	O
via	O
human	O
eﬀort	O
.	O
we	O
hope	O
that	O
the	O
design	O
principles	O
described	O
in	O
this	O
chapter	O
will	O
lead	O
to	O
many	O
other	O
similar	O
successes	O
.	O
442	O
chapter	O
12	O
applications	O
in	O
this	O
chapter	O
,	O
we	O
describe	O
how	O
to	O
use	O
deep	O
learning	O
to	O
solve	O
applications	O
in	O
com-	O
puter	O
vision	O
,	O
speech	O
recognition	B
,	O
natural	O
language	O
processing	O
,	O
and	O
other	O
application	O
areas	O
of	O
commercial	O
interest	O
.	O
we	O
begin	O
by	O
discussing	O
the	O
large	O
scale	O
neural	O
network	O
implementations	O
required	O
for	O
most	O
serious	O
ai	O
applications	O
.	O
next	O
,	O
we	O
review	O
several	O
speciﬁc	O
application	O
areas	O
that	O
deep	O
learning	O
has	O
been	O
used	O
to	O
solve	O
.	O
while	O
one	O
goal	O
of	O
deep	O
learning	O
is	O
to	O
design	O
algorithms	O
that	O
are	O
capable	O
of	O
solving	O
a	O
broad	O
variety	O
of	O
tasks	O
,	O
so	O
far	O
some	O
degree	O
of	O
specialization	O
is	O
needed	O
.	O
for	O
example	O
,	O
vision	O
tasks	O
require	O
processing	O
a	O
large	O
number	O
of	O
input	O
features	O
(	O
pixels	O
)	O
per	O
example	O
.	O
language	O
tasks	O
require	O
modeling	O
a	O
large	O
number	O
of	O
possible	O
values	O
(	O
words	O
in	O
the	O
vocabulary	O
)	O
per	O
input	O
feature	O
.	O
12.1	O
large-scale	O
deep	O
learning	O
deep	O
learning	O
is	O
based	O
on	O
the	O
philosophy	O
of	O
connectionism	O
:	O
while	O
an	O
individual	O
biological	O
neuron	O
or	O
an	O
individual	O
feature	O
in	O
a	O
machine	O
learning	O
model	B
is	O
not	O
intelligent	O
,	O
a	O
large	O
population	O
of	O
these	O
neurons	O
or	O
features	O
acting	O
together	O
can	O
exhibit	O
intelligent	O
behavior	O
.	O
it	O
truly	O
is	O
important	O
to	O
emphasize	O
the	O
fact	O
that	O
the	O
number	O
of	O
neurons	O
must	O
be	O
large	O
.	O
one	O
of	O
the	O
key	O
factors	O
responsible	O
for	O
the	O
improvement	O
in	O
neural	O
network	O
’	O
s	O
accuracy	O
and	O
the	O
improvement	O
of	O
the	O
complexity	O
of	O
tasks	O
they	O
can	O
solve	O
between	O
the	O
1980s	O
and	O
today	O
is	O
the	O
dramatic	O
increase	O
in	O
the	O
size	O
of	O
the	O
networks	O
we	O
use	O
.	O
as	O
we	O
saw	O
in	O
section	O
,	O
network	O
sizes	O
have	O
grown	O
exponentially	O
for	O
the	O
past	O
three	O
decades	O
,	O
yet	O
artiﬁcial	O
neural	O
networks	O
are	O
only	O
as	O
large	O
as	O
the	O
nervous	O
systems	O
of	O
insects	O
.	O
1.2.3	O
because	O
the	O
size	O
of	O
neural	O
networks	O
is	O
of	O
paramount	O
importance	O
,	O
deep	O
learning	O
443	O
chapter	O
12.	O
applications	O
requires	O
high	O
performance	O
hardware	O
and	O
software	O
infrastructure	O
.	O
12.1.1	O
fast	O
cpu	O
implementations	O
traditionally	O
,	O
neural	O
networks	O
were	O
trained	O
using	O
the	O
cpu	O
of	O
a	O
single	O
machine	O
.	O
today	O
,	O
this	O
approach	O
is	O
generally	O
considered	O
insuﬃcient	O
.	O
we	O
now	O
mostly	O
use	O
gpu	O
computing	O
or	O
the	O
cpus	O
of	O
many	O
machines	O
networked	O
together	O
.	O
before	O
moving	O
to	O
these	O
expensive	O
setups	O
,	O
researchers	O
worked	O
hard	O
to	O
demonstrate	O
that	O
cpus	O
could	O
not	O
manage	O
the	O
high	O
computational	O
workload	O
required	O
by	O
neural	O
networks	O
.	O
a	O
description	O
of	O
how	O
to	O
implement	O
eﬃcient	O
numerical	O
cpu	O
code	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
,	O
but	O
we	O
emphasize	O
here	O
that	O
careful	O
implementation	O
for	O
speciﬁc	O
cpu	O
families	O
can	O
yield	O
large	O
improvements	O
.	O
for	O
example	O
,	O
in	O
2011	O
,	O
the	O
best	O
cpus	O
available	O
could	O
run	O
neural	O
network	O
workloads	O
faster	O
when	O
using	O
ﬁxed-point	O
arithmetic	O
rather	O
than	O
ﬂoating-point	O
arithmetic	O
.	O
by	O
creating	O
a	O
carefully	O
tuned	O
ﬁxed-	O
point	O
implementation	O
,	O
vanhoucke	O
)	O
obtained	O
a	O
threefold	O
speedup	O
over	O
a	O
strong	O
ﬂoating-point	O
system	O
.	O
each	O
new	O
model	B
of	O
cpu	O
has	O
diﬀerent	O
performance	O
characteristics	O
,	O
so	O
sometimes	O
ﬂoating-point	O
implementations	O
can	O
be	O
faster	O
too	O
.	O
the	O
important	O
principle	O
is	O
that	O
careful	O
specialization	O
of	O
numerical	O
computation	O
routines	O
can	O
yield	O
a	O
large	O
payoﬀ	O
.	O
other	O
strategies	O
,	O
besides	O
choosing	O
whether	O
to	O
use	O
ﬁxed	O
or	O
ﬂoating	O
point	O
,	O
include	O
optimizing	O
data	O
structures	O
to	O
avoid	O
cache	O
misses	O
and	O
using	O
vector	O
instructions	O
.	O
many	O
machine	O
learning	O
researchers	O
neglect	O
these	O
implementation	O
details	O
,	O
but	O
when	O
the	O
performance	O
of	O
an	O
implementation	O
restricts	O
the	O
size	O
of	O
the	O
model	B
,	O
the	O
accuracy	O
of	O
the	O
model	B
suﬀers	O
.	O
et	O
al	O
.	O
(	O
2011	O
12.1.2	O
gpu	O
implementations	O
most	O
modern	O
neural	O
network	O
implementations	O
are	O
based	O
on	O
graphics	O
processing	O
units	O
.	O
graphics	O
processing	O
units	O
(	O
gpus	O
)	O
are	O
specialized	O
hardware	O
components	O
that	O
were	O
originally	O
developed	O
for	O
graphics	O
applications	O
.	O
the	O
consumer	O
market	O
for	O
video	O
gaming	O
systems	O
spurred	O
development	O
of	O
graphics	O
processing	O
hardware	O
.	O
the	O
performance	O
characteristics	O
needed	O
for	O
good	O
video	O
gaming	O
systems	O
turn	O
out	O
to	O
be	O
beneﬁcial	O
for	O
neural	O
networks	O
as	O
well	O
.	O
video	O
game	O
rendering	O
requires	O
performing	O
many	O
operations	O
in	O
parallel	O
quickly	O
.	O
models	O
of	O
characters	O
and	O
environments	O
are	O
speciﬁed	O
in	O
terms	O
of	O
lists	O
of	O
3-d	O
coordinates	O
of	O
vertices	O
.	O
graphics	O
cards	O
must	O
perform	O
matrix	O
multiplication	O
and	O
division	O
on	O
many	O
vertices	O
in	O
parallel	O
to	O
convert	O
these	O
3-d	O
coordinates	O
into	O
2-d	O
on-screen	O
coordinates	O
.	O
the	O
graphics	O
card	O
must	O
then	O
perform	O
many	O
computations	O
at	O
each	O
pixel	O
in	O
parallel	O
to	O
determine	O
the	O
color	O
of	O
each	O
pixel	O
.	O
in	O
both	O
cases	O
,	O
the	O
444	O
chapter	O
12.	O
applications	O
computations	O
are	O
fairly	O
simple	O
and	O
do	O
not	O
involve	O
much	O
branching	O
compared	O
to	O
the	O
computational	O
workload	O
that	O
a	O
cpu	O
usually	O
encounters	O
.	O
for	O
example	O
,	O
each	O
vertex	O
in	O
the	O
same	O
rigid	O
object	O
will	O
be	O
multiplied	O
by	O
the	O
same	O
matrix	O
;	O
there	O
is	O
no	O
need	O
to	O
evaluate	O
an	O
if	O
statement	O
per-vertex	O
to	O
determine	O
which	O
matrix	O
to	O
multiply	O
by	O
.	O
the	O
computations	O
are	O
also	O
entirely	O
independent	O
of	O
each	O
other	O
,	O
and	O
thus	O
may	O
be	O
parallelized	O
easily	O
.	O
the	O
computations	O
also	O
involve	O
processing	O
massive	O
buﬀers	O
of	O
memory	O
,	O
containing	O
bitmaps	O
describing	O
the	O
texture	O
(	O
color	O
pattern	O
)	O
of	O
each	O
object	O
to	O
be	O
rendered	O
.	O
together	O
,	O
this	O
results	O
in	O
graphics	O
cards	O
having	O
been	O
designed	O
to	O
have	O
a	O
high	O
degree	O
of	O
parallelism	O
and	O
high	O
memory	O
bandwidth	O
,	O
at	O
the	O
cost	O
of	O
having	O
a	O
lower	O
clock	O
speed	O
and	O
less	O
branching	O
capability	O
relative	O
to	O
traditional	O
cpus	O
.	O
neural	O
network	O
algorithms	O
require	O
the	O
same	O
performance	O
characteristics	O
as	O
the	O
real-time	O
graphics	O
algorithms	O
described	O
above	O
.	O
neural	O
networks	O
usually	O
involve	O
large	O
and	O
numerous	O
buﬀers	O
of	O
parameters	O
,	O
activation	O
values	O
,	O
and	O
gradient	O
values	O
,	O
each	O
of	O
which	O
must	O
be	O
completely	O
updated	O
during	O
every	O
step	O
of	O
training	O
.	O
these	O
buﬀers	O
are	O
large	O
enough	O
to	O
fall	O
outside	O
the	O
cache	O
of	O
a	O
traditional	O
desktop	O
computer	O
so	O
the	O
memory	O
bandwidth	O
of	O
the	O
system	O
often	O
becomes	O
the	O
rate	O
limiting	O
factor	O
.	O
gpus	O
oﬀer	O
a	O
compelling	O
advantage	O
over	O
cpus	O
due	O
to	O
their	O
high	O
memory	O
bandwidth	O
.	O
neural	O
network	O
training	O
algorithms	O
typically	O
do	O
not	O
involve	O
much	O
branching	O
or	O
sophisticated	O
control	O
,	O
so	O
they	O
are	O
appropriate	O
for	O
gpu	O
hardware	O
.	O
since	O
neural	O
networks	O
can	O
be	O
divided	O
into	O
multiple	O
individual	O
“	O
neurons	O
”	O
that	O
can	O
be	O
processed	O
independently	O
from	O
the	O
other	O
neurons	O
in	O
the	O
same	O
layer	O
,	O
neural	O
networks	O
easily	O
beneﬁt	O
from	O
the	O
parallelism	O
of	O
gpu	O
computing	O
.	O
gpu	O
hardware	O
was	O
originally	O
so	O
specialized	O
that	O
it	O
could	O
only	O
be	O
used	O
for	O
graphics	O
tasks	O
.	O
over	O
time	O
,	O
gpu	O
hardware	O
became	O
more	O
ﬂexible	O
,	O
allowing	O
custom	O
subroutines	O
to	O
be	O
used	O
to	O
transform	O
the	O
coordinates	O
of	O
vertices	O
or	O
assign	O
colors	O
to	O
pixels	O
.	O
in	O
principle	O
,	O
there	O
was	O
no	O
requirement	O
that	O
these	O
pixel	O
values	O
actually	O
be	O
based	O
on	O
a	O
rendering	O
task	O
.	O
these	O
gpus	O
could	O
be	O
used	O
for	O
scientiﬁc	O
computing	O
by	O
writing	O
the	O
output	O
of	O
a	O
computation	O
to	O
a	O
buﬀer	O
of	O
pixel	O
values	O
.	O
steinkrau	O
et	O
al	O
.	O
(	O
)	O
implemented	O
a	O
two-layer	O
fully	O
connected	O
neural	O
network	O
on	O
a	O
gpu	O
and	O
2005	O
reported	O
a	O
threefold	O
speedup	O
over	O
their	O
cpu-based	O
baseline	O
.	O
shortly	O
thereafter	O
,	O
chellapilla	O
)	O
demonstrated	O
that	O
the	O
same	O
technique	O
could	O
be	O
used	O
to	O
accelerate	O
supervised	O
convolutional	O
networks	O
.	O
et	O
al	O
.	O
(	O
2006	O
the	O
popularity	O
of	O
graphics	O
cards	O
for	O
neural	O
network	O
training	O
exploded	O
after	O
the	O
advent	O
of	O
general	O
purpose	O
gpus	O
.	O
these	O
gp-gpus	O
could	O
execute	O
arbitrary	O
code	O
,	O
not	O
just	O
rendering	O
subroutines	O
.	O
nvidia	O
’	O
s	O
cuda	O
programming	O
language	O
provided	O
a	O
way	O
to	O
write	O
this	O
arbitrary	O
code	O
in	O
a	O
c-like	O
language	O
.	O
with	O
their	O
relatively	O
convenient	O
programming	O
model	B
,	O
massive	O
parallelism	O
,	O
and	O
high	O
memory	O
445	O
chapter	O
12.	O
applications	O
bandwidth	O
,	O
gp-gpus	O
now	O
oﬀer	O
an	O
ideal	O
platform	O
for	O
neural	O
network	O
programming	O
.	O
this	O
platform	O
was	O
rapidly	O
adopted	O
by	O
deep	O
learning	O
researchers	O
soon	O
after	O
it	O
became	O
available	O
(	O
raina	O
et	O
al	O
.	O
2009	O
ciresan	O
et	O
al	O
.	O
2010	O
)	O
.	O
,	O
;	O
,	O
writing	O
eﬃcient	O
code	O
for	O
gp-gpus	O
remains	O
a	O
diﬃcult	O
task	O
best	O
left	O
to	O
spe-	O
cialists	O
.	O
the	O
techniques	O
required	O
to	O
obtain	O
good	O
performance	O
on	O
gpu	O
are	O
very	O
diﬀerent	O
from	O
those	O
used	O
on	O
cpu	O
.	O
for	O
example	O
,	O
good	O
cpu-based	O
code	O
is	O
usually	O
designed	O
to	O
read	O
information	O
from	O
the	O
cache	O
as	O
much	O
as	O
possible	O
.	O
on	O
gpu	O
,	O
most	O
writable	O
memory	O
locations	O
are	O
not	O
cached	O
,	O
so	O
it	O
can	O
actually	O
be	O
faster	O
to	O
compute	O
the	O
same	O
value	O
twice	O
,	O
rather	O
than	O
compute	O
it	O
once	O
and	O
read	O
it	O
back	O
from	O
memory	O
.	O
gpu	O
code	O
is	O
also	O
inherently	O
multi-threaded	O
and	O
the	O
diﬀerent	O
threads	O
must	O
be	O
coordinated	O
with	O
each	O
other	O
carefully	O
.	O
for	O
example	O
,	O
memory	O
operations	O
are	O
faster	O
if	O
they	O
can	O
be	O
coalesced	O
.	O
coalesced	O
reads	O
or	O
writes	O
occur	O
when	O
several	O
threads	O
can	O
each	O
read	O
or	O
write	O
a	O
value	O
that	O
they	O
need	O
simultaneously	O
,	O
as	O
part	O
of	O
a	O
single	O
memory	O
transaction	O
.	O
diﬀerent	O
models	O
of	O
gpus	O
are	O
able	O
to	O
coalesce	O
diﬀerent	O
kinds	O
of	O
read	O
or	O
write	O
patterns	O
.	O
typically	O
,	O
memory	O
operations	O
are	O
easier	O
to	O
coalesce	O
if	O
among	O
n	O
threads	O
,	O
thread	O
i	O
accesses	O
byte	O
i	O
+	O
j	O
of	O
memory	O
,	O
and	O
j	O
is	O
a	O
multiple	O
of	O
some	O
power	O
of	O
2.	O
the	O
exact	O
speciﬁcations	O
diﬀer	O
between	O
models	O
of	O
gpu	O
.	O
another	O
common	O
consideration	O
for	O
gpus	O
is	O
making	O
sure	O
that	O
each	O
thread	O
in	O
a	O
group	O
executes	O
the	O
same	O
instruction	O
simultaneously	O
.	O
this	O
means	O
that	O
branching	O
can	O
be	O
diﬃcult	O
on	O
gpu	O
.	O
threads	O
are	O
divided	O
into	O
small	O
groups	O
called	O
warps	O
.	O
each	O
thread	O
in	O
a	O
warp	O
executes	O
the	O
same	O
instruction	O
during	O
each	O
cycle	O
,	O
so	O
if	O
diﬀerent	O
threads	O
within	O
the	O
same	O
warp	O
need	O
to	O
execute	O
diﬀerent	O
code	O
paths	O
,	O
these	O
diﬀerent	O
code	O
paths	O
must	O
be	O
traversed	O
sequentially	O
rather	O
than	O
in	O
parallel	O
.	O
due	O
to	O
the	O
diﬃculty	O
of	O
writing	O
high	O
performance	O
gpu	O
code	O
,	O
researchers	O
should	O
structure	O
their	O
workﬂow	O
to	O
avoid	O
needing	O
to	O
write	O
new	O
gpu	O
code	O
in	O
order	O
to	O
test	O
new	O
models	O
or	O
algorithms	O
.	O
typically	O
,	O
one	O
can	O
do	O
this	O
by	O
building	O
a	O
software	O
library	O
of	O
high	O
performance	O
operations	O
like	O
convolution	O
and	O
matrix	O
multiplication	O
,	O
then	O
specifying	O
models	O
in	O
terms	O
of	O
calls	O
to	O
this	O
library	O
of	O
operations	O
.	O
for	O
example	O
,	O
the	O
machine	O
learning	O
library	O
pylearn2	O
(	O
goodfellow	O
)	O
speciﬁes	O
all	O
of	O
its	O
machine	O
learning	O
algorithms	O
in	O
terms	O
of	O
calls	O
to	O
theano	O
(	O
bergstra	O
et	O
al	O
.	O
2010	O
;	O
bastien	O
)	O
,	O
which	O
provide	O
these	O
high-performance	O
operations	O
.	O
this	O
factored	O
approach	O
can	O
also	O
ease	O
support	O
for	O
multiple	O
kinds	O
of	O
hardware	O
.	O
for	O
example	O
,	O
the	O
same	O
theano	O
program	O
can	O
run	O
on	O
either	O
cpu	O
or	O
gpu	O
,	O
without	O
needing	O
to	O
change	O
any	O
of	O
the	O
calls	O
to	O
theano	O
itself	O
.	O
other	O
libraries	O
like	O
tensorflow	O
(	O
collobert	O
et	O
al	O
.	O
,	O
2011b	O
)	O
provide	O
similar	O
features.	O
)	O
and	O
cuda-convnet	O
(	O
abadi	O
et	O
al	O
.	O
2015	O
)	O
and	O
torch	O
(	O
krizhevsky	O
2010	O
,	O
et	O
al.	O
,	O
2013c	O
et	O
al.	O
,	O
2012	O
,	O
,	O
446	O
chapter	O
12.	O
applications	O
12.1.3	O
large-scale	O
distributed	O
implementations	O
in	O
many	O
cases	O
,	O
the	O
computational	O
resources	O
available	O
on	O
a	O
single	O
machine	O
are	O
insuﬃcient	O
.	O
we	O
therefore	O
want	O
to	O
distribute	O
the	O
workload	O
of	O
training	O
and	O
inference	O
across	O
many	O
machines	O
.	O
distributing	O
inference	O
is	O
simple	O
,	O
because	O
each	O
input	O
example	O
we	O
want	O
to	O
process	O
can	O
be	O
run	O
by	O
a	O
separate	O
machine	O
.	O
this	O
is	O
known	O
as	O
.	O
data	O
parallelism	O
it	O
is	O
also	O
possible	O
to	O
get	O
model	B
parallelism	O
,	O
where	O
multiple	O
machines	O
work	B
together	O
on	O
a	O
single	O
datapoint	O
,	O
with	O
each	O
machine	O
running	O
a	O
diﬀerent	O
part	O
of	O
the	O
model	B
.	O
this	O
is	O
feasible	O
for	O
both	O
inference	O
and	O
training	O
.	O
data	O
parallelism	O
during	O
training	O
is	O
somewhat	O
harder	O
.	O
we	O
can	O
increase	O
the	O
size	O
of	O
the	O
minibatch	O
used	O
for	O
a	O
single	O
sgd	O
step	O
,	O
but	O
usually	O
we	O
get	O
less	O
than	O
linear	O
returns	O
in	O
terms	O
of	O
optimization	O
performance	O
.	O
it	O
would	O
be	O
better	O
to	O
allow	O
multiple	O
machines	O
to	O
compute	O
multiple	O
gradient	O
descent	B
steps	O
in	O
parallel	O
.	O
unfortunately	O
,	O
the	O
standard	O
deﬁnition	O
of	O
gradient	O
descent	B
is	O
as	O
a	O
completely	O
sequential	O
algorithm	O
:	O
the	O
gradient	O
at	O
step	O
is	O
a	O
function	O
of	O
the	O
parameters	O
produced	O
by	O
step	O
−	O
.	O
1	O
t	O
t	O
;	O
2012	O
2011	O
et	O
al.	O
,	O
2001	O
recht	O
this	O
can	O
be	O
solved	O
using	O
asynchronous	O
stochastic	O
gradient	O
descent	B
(	O
ben-	O
gio	O
et	O
al.	O
,	O
)	O
.	O
in	O
this	O
approach	O
,	O
several	O
processor	O
cores	O
share	O
the	O
memory	O
representing	O
the	O
parameters	O
.	O
each	O
core	O
reads	O
parameters	O
without	O
a	O
lock	O
,	O
then	O
computes	O
a	O
gradient	O
,	O
then	O
increments	O
the	O
parameters	O
without	O
a	O
lock	O
.	O
this	O
reduces	O
the	O
average	O
amount	O
of	O
improvement	O
that	O
each	O
gradient	O
descent	B
step	O
yields	O
,	O
because	O
some	O
of	O
the	O
cores	O
overwrite	O
each	O
other	O
’	O
s	O
progress	O
,	O
but	O
the	O
increased	O
rate	O
of	O
production	O
of	O
steps	O
causes	O
the	O
learning	O
process	O
to	O
be	O
faster	O
overall	O
.	O
dean	O
et	O
al	O
.	O
(	O
)	O
pioneered	O
the	O
multi-machine	O
implementation	O
of	O
this	O
lock-free	O
approach	O
to	O
gradient	O
descent	B
,	O
where	O
the	O
parameters	O
are	O
managed	O
by	O
a	O
parameter	O
server	O
rather	O
than	O
stored	O
in	O
shared	O
memory	O
.	O
distributed	O
asynchronous	O
gradient	O
descent	B
remains	O
the	O
primary	O
strategy	O
for	O
training	O
large	O
deep	O
networks	O
and	O
is	O
used	O
by	O
most	O
major	O
deep	O
learning	O
groups	O
in	O
industry	O
(	O
chilimbi	O
et	O
al	O
.	O
2014	O
wu	O
et	O
al.	O
,	O
2015	O
)	O
.	O
academic	O
deep	O
learning	O
researchers	O
typically	O
can	O
not	O
aﬀord	O
the	O
same	O
scale	O
of	O
distributed	O
learning	O
systems	O
but	O
some	O
research	O
has	O
focused	O
on	O
how	O
to	O
build	O
distributed	O
networks	O
with	O
relatively	O
low-cost	O
hardware	O
available	O
in	O
the	O
university	O
setting	O
(	O
coates	O
et	O
al	O
.	O
2013	O
,	O
)	O
.	O
,	O
;	O
12.1.4	O
model	B
compression	O
in	O
many	O
commercial	O
applications	O
,	O
it	O
is	O
much	O
more	O
important	O
that	O
the	O
time	O
and	O
memory	O
cost	O
of	O
running	O
inference	O
in	O
a	O
machine	O
learning	O
model	B
be	O
low	O
than	O
that	O
the	O
time	O
and	O
memory	O
cost	O
of	O
training	O
be	O
low	O
.	O
for	O
applications	O
that	O
do	O
not	O
require	O
447	O
chapter	O
12.	O
applications	O
personalization	O
,	O
it	O
is	O
possible	O
to	O
train	O
a	O
model	B
once	O
,	O
then	O
deploy	O
it	O
to	O
be	O
used	O
by	O
billions	O
of	O
users	O
.	O
in	O
many	O
cases	O
,	O
the	O
end	O
user	O
is	O
more	O
resource-constrained	O
than	O
the	O
developer	O
.	O
for	O
example	O
,	O
one	O
might	O
train	O
a	O
speech	O
recognition	B
network	O
with	O
a	O
powerful	O
computer	O
cluster	O
,	O
then	O
deploy	O
it	O
on	O
mobile	O
phones	O
.	O
et	O
al.	O
,	O
a	O
key	O
strategy	O
for	O
reducing	O
the	O
cost	O
of	O
inference	O
is	O
model	B
compression	O
(	O
bu-	O
ciluˇa	O
)	O
.	O
the	O
basic	O
idea	O
of	O
model	B
compression	O
is	O
to	O
replace	O
the	O
original	O
,	O
expensive	O
model	B
with	O
a	O
smaller	O
model	B
that	O
requires	O
less	O
memory	O
and	O
runtime	O
to	O
store	O
and	O
evaluate	O
.	O
2006	O
model	B
compression	O
is	O
applicable	O
when	O
the	O
size	O
of	O
the	O
original	O
model	B
is	O
driven	O
primarily	O
by	O
a	O
need	O
to	O
prevent	O
overﬁtting	O
.	O
in	O
most	O
cases	O
,	O
the	O
model	B
with	O
the	O
lowest	O
generalization	O
error	O
is	O
an	O
ensemble	O
of	O
several	O
independently	O
trained	O
models	O
.	O
evaluating	O
all	O
n	O
ensemble	O
members	O
is	O
expensive	O
.	O
sometimes	O
,	O
even	O
a	O
single	O
model	B
generalizes	O
better	O
if	O
it	O
is	O
large	O
(	O
for	O
example	O
,	O
if	O
it	O
is	O
regularized	O
with	O
dropout	O
)	O
.	O
these	O
large	O
models	O
learn	O
some	O
function	O
f	O
(	O
x	O
)	O
,	O
but	O
do	O
so	O
using	O
many	O
more	O
parameters	O
than	O
are	O
necessary	O
for	O
the	O
task	O
.	O
their	O
size	O
is	O
necessary	O
only	O
due	O
to	O
the	O
limited	O
number	O
of	O
training	O
examples	O
.	O
as	O
soon	O
as	O
we	O
have	O
ﬁt	O
this	O
function	O
f	O
(	O
x	O
)	O
,	O
we	O
can	O
generate	O
a	O
training	O
set	O
containing	O
inﬁnitely	O
many	O
examples	O
,	O
simply	O
by	O
applying	O
f	O
to	O
randomly	O
sampled	O
points	O
x.	O
we	O
then	O
train	O
the	O
new	O
,	O
smaller	O
,	O
model	B
to	O
match	O
f	O
(	O
x	O
)	O
on	O
these	O
points	O
.	O
in	O
order	O
to	O
most	O
eﬃciently	O
use	O
the	O
capacity	O
of	O
the	O
new	O
,	O
small	O
model	B
,	O
it	O
is	O
best	O
to	O
sample	O
the	O
new	O
x	O
points	O
from	O
a	O
distribution	O
resembling	O
the	O
actual	O
test	O
inputs	O
that	O
will	O
be	O
supplied	O
to	O
the	O
model	B
later	O
.	O
this	O
can	O
be	O
done	O
by	O
corrupting	O
training	O
examples	O
or	O
by	O
drawing	O
points	O
from	O
a	O
generative	O
model	B
trained	O
on	O
the	O
original	O
training	O
set	O
.	O
alternatively	O
,	O
one	O
can	O
train	O
the	O
smaller	O
model	B
only	O
on	O
the	O
original	O
training	O
points	O
,	O
but	O
train	O
it	O
to	O
copy	O
other	O
features	O
of	O
the	O
model	B
,	O
such	O
as	O
its	O
posterior	O
distribution	O
over	O
the	O
incorrect	O
classes	O
(	O
hinton	O
2014	O
2015	O
et	O
al.	O
,	O
)	O
.	O
,	O
12.1.5	O
dynamic	O
structure	O
one	O
strategy	O
for	O
accelerating	O
data	O
processing	O
systems	O
in	O
general	O
is	O
to	O
build	O
systems	O
that	O
have	O
dynamic	O
structure	O
in	O
the	O
graph	O
describing	O
the	O
computation	O
needed	O
to	O
process	O
an	O
input	O
.	O
data	O
processing	O
systems	O
can	O
dynamically	O
determine	O
which	O
subset	O
of	O
many	O
neural	O
networks	O
should	O
be	O
run	O
on	O
a	O
given	O
input	O
.	O
individual	O
neural	O
networks	O
can	O
also	O
exhibit	O
dynamic	O
structure	O
internally	O
by	O
determining	O
which	O
subset	O
of	O
features	O
(	O
hidden	O
units	O
)	O
to	O
compute	O
given	O
information	O
from	O
the	O
input	O
.	O
this	O
form	O
of	O
dynamic	O
structure	O
inside	O
neural	O
networks	O
is	O
sometimes	O
called	O
conditional	O
computation	O
(	O
)	O
.	O
since	O
many	O
components	O
of	O
the	O
architecture	O
may	O
be	O
relevant	O
only	O
for	O
a	O
small	O
amount	O
of	O
possible	O
inputs	O
,	O
the	O
bengio	O
2013	O
bengio	O
et	O
al	O
.	O
2013b	O
,	O
;	O
,	O
448	O
chapter	O
12.	O
applications	O
system	O
can	O
run	O
faster	O
by	O
computing	O
these	O
features	O
only	O
when	O
they	O
are	O
needed	O
.	O
dynamic	O
structure	O
of	O
computations	O
is	O
a	O
basic	O
computer	O
science	O
principle	O
applied	O
generally	O
throughout	O
the	O
software	O
engineering	O
discipline	O
.	O
the	O
simplest	O
versions	O
of	O
dynamic	O
structure	O
applied	O
to	O
neural	O
networks	O
are	O
based	O
on	O
determining	O
which	O
subset	O
of	O
some	O
group	O
of	O
neural	O
networks	O
(	O
or	O
other	O
machine	O
learning	O
models	O
)	O
should	O
be	O
applied	O
to	O
a	O
particular	O
input	O
.	O
a	O
venerable	O
strategy	O
for	O
accelerating	O
inference	O
in	O
a	O
classiﬁer	O
is	O
to	O
use	O
a	O
cascade	O
of	O
classiﬁers	O
.	O
the	O
cascade	O
strategy	O
may	O
be	O
applied	O
when	O
the	O
goal	O
is	O
to	O
detect	O
the	O
presence	O
of	O
a	O
rare	O
object	O
(	O
or	O
event	O
)	O
.	O
to	O
know	O
for	O
sure	O
that	O
the	O
object	O
is	O
present	O
,	O
we	O
must	O
use	O
a	O
sophisticated	O
classiﬁer	O
with	O
high	O
capacity	O
,	O
that	O
is	O
expensive	O
to	O
run	O
.	O
however	O
,	O
because	O
the	O
object	O
is	O
rare	O
,	O
we	O
can	O
usually	O
use	O
much	O
less	O
computation	O
to	O
reject	O
inputs	O
as	O
not	O
containing	O
the	O
object	O
.	O
in	O
these	O
situations	O
,	O
we	O
can	O
train	O
a	O
sequence	O
of	O
classiﬁers	O
.	O
the	O
ﬁrst	O
classiﬁers	O
in	O
the	O
sequence	O
have	O
low	O
capacity	O
,	O
and	O
are	O
trained	O
to	O
have	O
high	O
recall	O
.	O
in	O
other	O
words	O
,	O
they	O
are	O
trained	O
to	O
make	O
sure	O
we	O
do	O
not	O
wrongly	O
reject	O
an	O
input	O
when	O
the	O
object	O
is	O
present	O
.	O
the	O
ﬁnal	O
classiﬁer	O
is	O
trained	O
to	O
have	O
high	O
precision	O
.	O
at	O
test	O
time	O
,	O
we	O
run	O
inference	O
by	O
running	O
the	O
classiﬁers	O
in	O
a	O
sequence	O
,	O
abandoning	O
any	O
example	O
as	O
soon	O
as	O
any	O
one	O
element	O
in	O
the	O
cascade	O
rejects	O
it	O
.	O
overall	O
,	O
this	O
allows	O
us	O
to	O
verify	O
the	O
presence	O
of	O
objects	O
with	O
high	O
conﬁdence	O
,	O
using	O
a	O
high	O
capacity	O
model	B
,	O
but	O
does	O
not	O
force	O
us	O
to	O
pay	O
the	O
cost	O
of	O
full	O
inference	O
for	O
every	O
example	O
.	O
there	O
are	O
two	O
diﬀerent	O
ways	O
that	O
the	O
cascade	O
can	O
achieve	O
high	O
capacity	O
.	O
one	O
way	O
is	O
to	O
make	O
the	O
later	O
members	O
of	O
the	O
cascade	O
individually	O
have	O
high	O
capacity	O
.	O
in	O
this	O
case	O
,	O
the	O
system	O
as	O
a	O
whole	O
obviously	O
has	O
high	O
capacity	O
,	O
because	O
some	O
of	O
its	O
individual	O
members	O
do	O
.	O
it	O
is	O
also	O
possible	O
to	O
make	O
a	O
cascade	O
in	O
which	O
every	O
individual	O
model	B
has	O
low	O
capacity	O
but	O
the	O
system	O
as	O
a	O
whole	O
has	O
high	O
capacity	O
due	O
to	O
the	O
combination	O
of	O
many	O
small	O
models	O
.	O
viola	O
and	O
jones	O
2001	O
)	O
used	O
a	O
cascade	O
of	O
boosted	O
decision	O
trees	O
to	O
implement	O
a	O
fast	O
and	O
robust	O
face	O
detector	O
suitable	O
for	O
use	O
in	O
handheld	O
digital	O
cameras	O
.	O
their	O
classiﬁer	O
localizes	O
a	O
face	O
using	O
essentially	O
a	O
sliding	O
window	O
approach	O
in	O
which	O
many	O
windows	O
are	O
examined	O
and	O
rejected	O
if	O
they	O
do	O
not	O
contain	O
faces	O
.	O
another	O
version	O
of	O
cascades	O
uses	O
the	O
earlier	O
models	O
to	O
implement	O
a	O
sort	O
of	O
hard	O
attention	O
mechanism	O
:	O
the	O
early	O
members	O
of	O
the	O
cascade	O
localize	O
an	O
object	O
and	O
later	O
members	O
of	O
the	O
cascade	O
perform	O
further	O
processing	O
given	O
the	O
location	O
of	O
the	O
object	O
.	O
for	O
example	O
,	O
google	O
transcribes	O
address	O
numbers	O
from	O
street	O
view	O
imagery	O
using	O
a	O
two-step	O
cascade	O
that	O
ﬁrst	O
locates	O
the	O
address	O
number	O
with	O
one	O
machine	O
learning	O
model	B
and	O
then	O
transcribes	O
it	O
with	O
another	O
(	O
goodfellow	O
2014d	O
(	O
et	O
al.	O
,	O
)	O
.	O
decision	O
trees	O
themselves	O
are	O
an	O
example	O
of	O
dynamic	O
structure	O
,	O
because	O
each	O
node	O
in	O
the	O
tree	O
determines	O
which	O
of	O
its	O
subtrees	O
should	O
be	O
evaluated	O
for	O
each	O
input	O
.	O
a	O
simple	O
way	O
to	O
accomplish	O
the	O
union	O
of	O
deep	O
learning	O
and	O
dynamic	O
structure	O
449	O
chapter	O
12.	O
applications	O
is	O
to	O
train	O
a	O
decision	O
tree	O
in	O
which	O
each	O
node	O
uses	O
a	O
neural	O
network	O
to	O
make	O
the	O
splitting	O
decision	O
(	O
)	O
,	O
though	O
this	O
has	O
typically	O
not	O
been	O
done	O
with	O
the	O
primary	O
goal	O
of	O
accelerating	O
inference	O
computations	O
.	O
guo	O
and	O
gelfand	O
1992	O
,	O
1991	O
,	O
;	O
et	O
al.	O
,	O
in	O
the	O
same	O
spirit	O
,	O
one	O
can	O
use	O
a	O
neural	O
network	O
,	O
called	O
the	O
gater	O
to	O
select	O
which	O
one	O
out	O
of	O
several	O
expert	O
networks	O
will	O
be	O
used	O
to	O
compute	O
the	O
output	O
,	O
given	O
the	O
current	O
input	O
.	O
the	O
ﬁrst	O
version	O
of	O
this	O
idea	O
is	O
called	O
the	O
mixture	O
of	O
experts	O
(	O
nowlan	O
1990	O
jacobs	O
)	O
,	O
in	O
which	O
the	O
gater	O
outputs	O
a	O
set	O
of	O
probabilities	O
or	O
weights	O
(	O
obtained	O
via	O
a	O
softmax	O
nonlinearity	O
)	O
,	O
one	O
per	O
expert	O
,	O
and	O
the	O
ﬁnal	O
output	O
is	O
obtained	O
by	O
the	O
weighted	O
combination	O
of	O
the	O
output	O
of	O
the	O
experts	O
.	O
in	O
that	O
case	O
,	O
the	O
use	O
of	O
the	O
gater	O
does	O
not	O
oﬀer	O
a	O
reduction	O
in	O
computational	O
cost	O
,	O
but	O
if	O
a	O
single	O
expert	O
is	O
chosen	O
by	O
the	O
gater	O
for	O
each	O
example	O
,	O
we	O
obtain	O
the	O
hard	O
mixture	O
of	O
experts	O
(	O
)	O
,	O
which	O
can	O
considerably	O
accelerate	O
training	O
and	O
inference	O
time	O
.	O
this	O
strategy	O
works	O
well	O
when	O
the	O
number	O
of	O
gating	O
decisions	O
is	O
small	O
because	O
it	O
is	O
not	O
combinatorial	O
.	O
but	O
when	O
we	O
want	O
to	O
select	O
diﬀerent	O
subsets	O
of	O
units	O
or	O
parameters	O
,	O
it	O
is	O
not	O
possible	O
to	O
use	O
a	O
“	O
soft	O
switch	O
”	O
because	O
it	O
requires	O
enumerating	O
(	O
and	O
computing	O
outputs	O
for	O
)	O
all	O
the	O
gater	O
conﬁgurations	O
.	O
to	O
deal	O
with	O
this	O
problem	O
,	O
several	O
approaches	O
have	O
been	O
explored	O
to	O
train	O
combinatorial	O
gaters.	O
)	O
experiment	O
with	O
several	O
estimators	O
of	O
the	O
gradient	O
on	O
the	O
gating	O
probabilities	O
,	O
while	O
bacon	O
et	O
al	O
.	O
(	O
2015	O
)	O
use	O
reinforcement	O
learning	O
techniques	O
(	O
policy	O
gradient	O
)	O
to	O
learn	O
a	O
form	O
of	O
conditional	O
dropout	O
on	O
blocks	O
of	O
hidden	O
units	O
and	O
get	O
an	O
actual	O
reduction	O
in	O
computational	O
cost	O
without	O
impacting	O
negatively	O
on	O
the	O
quality	O
of	O
the	O
approximation	O
.	O
collobert	O
et	O
al	O
.	O
2001	O
2002	O
bengio	O
et	O
al	O
.	O
2015a	O
bengio	O
et	O
al	O
.	O
2013b	O
(	O
,	O
,	O
)	O
and	O
(	O
another	O
kind	O
of	O
dynamic	O
structure	O
is	O
a	O
switch	O
,	O
where	O
a	O
hidden	O
unit	O
can	O
receive	O
input	O
from	O
diﬀerent	O
units	O
depending	O
on	O
the	O
context	O
.	O
this	O
dynamic	O
routing	O
approach	O
can	O
be	O
interpreted	O
as	O
an	O
attention	O
mechanism	O
(	O
olshausen	O
et	O
al	O
.	O
1993	O
)	O
.	O
so	O
far	O
,	O
the	O
use	O
of	O
a	O
hard	O
switch	O
has	O
not	O
proven	O
eﬀective	O
on	O
large-scale	O
applications	O
.	O
contemporary	O
approaches	O
instead	O
use	O
a	O
weighted	O
average	O
over	O
many	O
possible	O
inputs	O
,	O
and	O
thus	O
do	O
not	O
achieve	O
all	O
of	O
the	O
possible	O
computational	O
beneﬁts	O
of	O
dynamic	O
12.4.5.1	O
.	O
structure	O
.	O
contemporary	O
attention	O
mechanisms	O
are	O
described	O
in	O
section	O
,	O
one	O
major	O
obstacle	O
to	O
using	O
dynamically	O
structured	O
systems	O
is	O
the	O
decreased	O
degree	O
of	O
parallelism	O
that	O
results	O
from	O
the	O
system	O
following	O
diﬀerent	O
code	O
branches	O
for	O
diﬀerent	O
inputs	O
.	O
this	O
means	O
that	O
few	O
operations	O
in	O
the	O
network	O
can	O
be	O
described	O
as	O
matrix	O
multiplication	O
or	O
batch	O
convolution	O
on	O
a	O
minibatch	O
of	O
examples	O
.	O
we	O
can	O
write	O
more	O
specialized	O
sub-routines	O
that	O
convolve	O
each	O
example	O
with	O
diﬀerent	O
kernels	O
or	O
multiply	O
each	O
row	O
of	O
a	O
design	O
matrix	O
by	O
a	O
diﬀerent	O
set	O
of	O
columns	O
of	O
weights	O
.	O
unfortunately	O
,	O
these	O
more	O
specialized	O
subroutines	O
are	O
diﬃcult	O
to	O
implement	O
eﬃciently	O
.	O
cpu	O
implementations	O
will	O
be	O
slow	O
due	O
to	O
the	O
lack	O
of	O
cache	O
450	O
chapter	O
12.	O
applications	O
coherence	O
and	O
gpu	O
implementations	O
will	O
be	O
slow	O
due	O
to	O
the	O
lack	O
of	O
coalesced	O
memory	O
transactions	O
and	O
the	O
need	O
to	O
serialize	O
warps	O
when	O
members	O
of	O
a	O
warp	O
take	O
diﬀerent	O
branches	O
.	O
in	O
some	O
cases	O
,	O
these	O
issues	O
can	O
be	O
mitigated	O
by	O
partitioning	O
the	O
examples	O
into	O
groups	O
that	O
all	O
take	O
the	O
same	O
branch	O
,	O
and	O
processing	O
these	O
groups	O
of	O
examples	O
simultaneously	O
.	O
this	O
can	O
be	O
an	O
acceptable	O
strategy	O
for	O
minimizing	O
the	O
time	O
required	O
to	O
process	O
a	O
ﬁxed	O
amount	O
of	O
examples	O
in	O
an	O
oﬄine	O
setting	O
.	O
in	O
a	O
real-time	O
setting	O
where	O
examples	O
must	O
be	O
processed	O
continuously	O
,	O
partitioning	O
the	O
workload	O
can	O
result	O
in	O
load-balancing	O
issues	O
.	O
for	O
example	O
,	O
if	O
we	O
assign	O
one	O
machine	O
to	O
process	O
the	O
ﬁrst	O
step	O
in	O
a	O
cascade	O
and	O
another	O
machine	O
to	O
process	O
the	O
last	O
step	O
in	O
a	O
cascade	O
,	O
then	O
the	O
ﬁrst	O
will	O
tend	O
to	O
be	O
overloaded	O
and	O
the	O
last	O
will	O
tend	O
to	O
be	O
underloaded	O
.	O
similar	O
issues	O
arise	O
if	O
each	O
machine	O
is	O
assigned	O
to	O
implement	O
diﬀerent	O
nodes	O
of	O
a	O
neural	O
decision	O
tree	O
.	O
12.1.6	O
specialized	O
hardware	O
implementations	O
of	O
deep	O
networks	O
since	O
the	O
early	O
days	O
of	O
neural	O
networks	O
research	O
,	O
hardware	O
designers	O
have	O
worked	O
on	O
specialized	O
hardware	O
implementations	O
that	O
could	O
speed	O
up	O
training	O
and/or	O
inference	O
of	O
neural	O
network	O
algorithms	O
.	O
see	O
early	O
and	O
more	O
recent	O
reviews	O
of	O
specialized	O
hardware	O
for	O
deep	O
networks	O
(	O
lindsey	O
and	O
lindblad	O
1994	O
beiu	O
et	O
al	O
.	O
,	O
2003	O
misra	O
and	O
saha	O
2010	O
)	O
.	O
,	O
;	O
,	O
;	O
,	O
;	O
;	O
,	O
et	O
al.	O
,	O
et	O
al.	O
,	O
2012	O
chen	O
2009	O
pham	O
diﬀerent	O
forms	O
of	O
specialized	O
hardware	O
(	O
graf	O
and	O
jackel	O
1989	O
mead	O
and	O
ismail	O
2012	O
kim	O
,	O
)	O
have	O
been	O
developed	O
over	O
the	O
last	O
decades	O
,	O
either	O
with	O
asics	O
(	O
application-speciﬁc	O
inte-	O
grated	O
circuit	O
)	O
,	O
either	O
with	O
digital	O
(	O
based	O
on	O
binary	O
representations	O
of	O
numbers	O
)	O
,	O
analog	O
(	O
graf	O
and	O
jackel	O
1989	O
mead	O
and	O
ismail	O
2012	O
)	O
(	O
based	O
on	O
physical	O
imple-	O
mentations	O
of	O
continuous	O
values	O
as	O
voltages	O
or	O
currents	O
)	O
or	O
hybrid	O
implementations	O
(	O
combining	O
digital	O
and	O
analog	O
components	O
)	O
.	O
in	O
recent	O
years	O
more	O
ﬂexible	O
fpga	O
(	O
ﬁeld	O
programmable	O
gated	O
array	O
)	O
implementations	O
(	O
where	O
the	O
particulars	O
of	O
the	O
circuit	O
can	O
be	O
written	O
on	O
the	O
chip	O
after	O
it	O
has	O
been	O
built	O
)	O
have	O
been	O
developed	O
.	O
2014a	O
b	O
et	O
al.	O
,	O
;	O
;	O
,	O
,	O
;	O
though	O
software	O
implementations	O
on	O
general-purpose	O
processing	O
units	O
(	O
cpus	O
and	O
gpus	O
)	O
typically	O
use	O
32	O
or	O
64	O
bits	O
of	O
precision	O
to	O
represent	O
ﬂoating	O
point	O
numbers	O
,	O
it	O
has	O
long	O
been	O
known	O
that	O
it	O
was	O
possible	O
to	O
use	O
less	O
precision	O
,	O
at	O
least	O
at	O
inference	O
time	O
(	O
holt	O
and	O
baker	O
1991	O
holi	O
and	O
hwang	O
1993	O
presley	O
and	O
haggard	O
1994	O
simard	O
and	O
graf	O
1994	O
wawrzynek	O
et	O
al.	O
,	O
2007	O
)	O
.	O
this	O
has	O
become	O
a	O
more	O
pressing	O
issue	O
in	O
recent	O
years	O
as	O
deep	O
learning	O
has	O
gained	O
in	O
popularity	O
in	O
industrial	O
products	O
,	O
and	O
as	O
the	O
great	O
impact	O
of	O
faster	O
hardware	O
was	O
demonstrated	O
with	O
gpus	O
.	O
another	O
factor	O
that	O
motivates	O
current	O
research	O
on	O
specialized	O
hardware	O
for	O
deep	O
networks	O
is	O
that	O
the	O
rate	O
of	O
progress	O
of	O
a	O
single	O
cpu	O
or	O
gpu	O
core	O
has	O
slowed	O
down	O
,	O
and	O
most	O
recent	O
improvements	O
in	O
1996	O
savich	O
et	O
al.	O
,	O
,	O
;	O
,	O
;	O
,	O
;	O
,	O
;	O
;	O
451	O
chapter	O
12.	O
applications	O
computing	O
speed	O
have	O
come	O
from	O
parallelization	O
across	O
cores	O
(	O
either	O
in	O
cpus	O
or	O
gpus	O
)	O
.	O
this	O
is	O
very	O
diﬀerent	O
from	O
the	O
situation	O
of	O
the	O
1990s	O
(	O
the	O
previous	O
neural	O
network	O
era	O
)	O
where	O
the	O
hardware	O
implementations	O
of	O
neural	O
networks	O
(	O
which	O
might	O
take	O
two	O
years	O
from	O
inception	O
to	O
availability	O
of	O
a	O
chip	O
)	O
could	O
not	O
keep	O
up	O
with	O
the	O
rapid	O
progress	O
and	O
low	O
prices	O
of	O
general-purpose	O
cpus	O
.	O
building	O
specialized	O
hardware	O
is	O
thus	O
a	O
way	O
to	O
push	O
the	O
envelope	O
further	O
,	O
at	O
a	O
time	O
when	O
new	O
hardware	O
designs	O
are	O
being	O
developed	O
for	O
low-power	O
devices	O
such	O
as	O
phones	O
,	O
aiming	O
for	O
general-public	O
applications	O
of	O
deep	O
learning	O
(	O
e.g.	O
,	O
with	O
speech	O
,	O
computer	O
vision	O
or	O
natural	O
language	O
)	O
.	O
et	O
al.	O
,	O
2015	O
gupta	O
;	O
2015	O
;	O
et	O
al.	O
,	O
et	O
al.	O
,	O
2011	O
courbariaux	O
recent	O
work	B
on	O
low-precision	O
implementations	O
of	O
backprop-based	O
neural	O
nets	O
(	O
vanhoucke	O
)	O
suggests	O
that	O
between	O
8	O
and	O
16	O
bits	O
of	O
precision	O
can	O
suﬃce	O
for	O
using	O
or	O
training	O
deep	O
neural	O
networks	O
with	O
back-propagation	O
.	O
what	O
is	O
clear	O
is	O
that	O
more	O
precision	O
is	O
required	O
during	O
training	O
than	O
at	O
inference	O
time	O
,	O
and	O
that	O
some	O
forms	O
of	O
dynamic	O
ﬁxed	O
point	O
representation	O
of	O
numbers	O
can	O
be	O
used	O
to	O
reduce	O
how	O
many	O
bits	O
are	O
required	O
per	O
number	O
.	O
traditional	O
ﬁxed	O
point	O
numbers	O
are	O
restricted	O
to	O
a	O
ﬁxed	O
range	O
(	O
which	O
corresponds	O
to	O
a	O
given	O
exponent	O
in	O
a	O
ﬂoating	O
point	O
representation	O
)	O
.	O
dynamic	O
ﬁxed	O
point	O
representations	O
share	O
that	O
range	O
among	O
a	O
set	O
of	O
numbers	O
(	O
such	O
as	O
all	O
the	O
weights	O
in	O
one	O
layer	O
)	O
.	O
using	O
ﬁxed	O
point	O
rather	O
than	O
ﬂoating	O
point	O
representations	O
and	O
using	O
less	O
bits	O
per	O
number	O
reduces	O
the	O
hardware	O
surface	O
area	O
,	O
power	O
requirements	O
and	O
computing	O
time	O
needed	O
for	O
performing	O
multiplications	O
,	O
and	O
multiplications	O
are	O
the	O
most	O
demanding	O
of	O
the	O
operations	O
needed	O
to	O
use	O
or	O
train	O
a	O
modern	O
deep	O
network	O
with	O
backprop	O
.	O
12.2	O
computer	O
vision	O
computer	O
vision	O
has	O
traditionally	O
been	O
one	O
of	O
the	O
most	O
active	O
research	O
areas	O
for	O
deep	O
learning	O
applications	O
,	O
because	O
vision	O
is	O
a	O
task	O
that	O
is	O
eﬀortless	O
for	O
humans	O
and	O
many	O
animals	O
but	O
challenging	O
for	O
computers	O
(	O
)	O
.	O
many	O
of	O
the	O
most	O
popular	O
standard	O
benchmark	O
tasks	O
for	O
deep	O
learning	O
algorithms	O
are	O
forms	O
of	O
object	O
recognition	B
or	O
optical	O
character	O
recognition	B
.	O
ballard	O
et	O
al	O
.	O
1983	O
,	O
computer	O
vision	O
is	O
a	O
very	O
broad	O
ﬁeld	O
encompassing	O
a	O
wide	O
variety	O
of	O
ways	O
of	O
processing	O
images	O
,	O
and	O
an	O
amazing	O
diversity	O
of	O
applications	O
.	O
applications	O
of	O
computer	O
vision	O
range	O
from	O
reproducing	O
human	O
visual	O
abilities	O
,	O
such	O
as	O
recognizing	O
faces	O
,	O
to	O
creating	O
entirely	O
new	O
categories	O
of	O
visual	O
abilities	O
.	O
as	O
an	O
example	O
of	O
the	O
latter	O
category	O
,	O
one	O
recent	O
computer	O
vision	O
application	O
is	O
to	O
recognize	O
sound	O
waves	O
from	O
the	O
vibrations	O
they	O
induce	O
in	O
objects	O
visible	O
in	O
a	O
video	O
(	O
davis	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O
most	O
deep	O
learning	O
research	O
on	O
computer	O
vision	O
has	O
not	O
focused	O
on	O
such	O
452	O
chapter	O
12.	O
applications	O
exotic	O
applications	O
that	O
expand	O
the	O
realm	O
of	O
what	O
is	O
possible	O
with	O
imagery	O
but	O
rather	O
a	O
small	O
core	O
of	O
ai	O
goals	O
aimed	O
at	O
replicating	O
human	O
abilities	O
.	O
most	O
deep	O
learning	O
for	O
computer	O
vision	O
is	O
used	O
for	O
object	O
recognition	B
or	O
detection	O
of	O
some	O
form	O
,	O
whether	O
this	O
means	O
reporting	O
which	O
object	O
is	O
present	O
in	O
an	O
image	O
,	O
annotating	O
an	O
image	O
with	O
bounding	O
boxes	O
around	O
each	O
object	O
,	O
transcribing	O
a	O
sequence	O
of	O
symbols	O
from	O
an	O
image	O
,	O
or	O
labeling	O
each	O
pixel	O
in	O
an	O
image	O
with	O
the	O
identity	O
of	O
the	O
object	O
it	O
belongs	O
to	O
.	O
because	O
generative	O
modeling	O
has	O
been	O
a	O
guiding	O
principle	O
of	O
deep	O
learning	O
research	O
,	O
there	O
is	O
also	O
a	O
large	O
body	O
of	O
work	B
on	O
image	O
synthesis	O
using	O
deep	O
models	O
.	O
while	O
image	O
synthesis	O
is	O
usually	O
not	O
considered	O
a	O
computer	O
vision	O
endeavor	O
,	O
models	O
capable	O
of	O
image	O
synthesis	O
are	O
usually	O
useful	O
for	O
image	O
restoration	O
,	O
a	O
computer	O
vision	O
task	O
involving	O
repairing	O
defects	O
in	O
images	O
or	O
removing	O
objects	O
from	O
images	O
.	O
ex	O
nihilo	O
12.2.1	O
preprocessing	O
many	O
application	O
areas	O
require	O
sophisticated	O
preprocessing	O
because	O
the	O
original	O
input	O
comes	O
in	O
a	O
form	O
that	O
is	O
diﬃcult	O
for	O
many	O
deep	O
learning	O
architectures	O
to	O
represent	O
.	O
computer	O
vision	O
usually	O
requires	O
relatively	O
little	O
of	O
this	O
kind	O
of	O
pre-	O
processing	O
.	O
the	O
images	O
should	O
be	O
standardized	O
so	O
that	O
their	O
pixels	O
all	O
lie	O
in	O
the	O
same	O
,	O
reasonable	O
range	O
,	O
like	O
[	O
0,1	O
]	O
or	O
[	O
-1	O
,	O
1	O
]	O
.	O
mixing	O
images	O
that	O
lie	O
in	O
[	O
0,1	O
]	O
with	O
images	O
that	O
lie	O
in	O
[	O
0	O
,	O
255	O
]	O
will	O
usually	O
result	O
in	O
failure	O
.	O
formatting	O
images	O
to	O
have	O
the	O
same	O
scale	O
is	O
the	O
only	O
kind	O
of	O
preprocessing	O
that	O
is	O
strictly	O
necessary	O
.	O
many	O
computer	O
vision	O
architectures	O
require	O
images	O
of	O
a	O
standard	O
size	O
,	O
so	O
images	O
must	O
be	O
cropped	O
or	O
scaled	O
to	O
ﬁt	O
that	O
size	O
.	O
even	O
this	O
rescaling	O
is	O
not	O
always	O
strictly	O
necessary	O
.	O
some	O
convolutional	O
models	O
accept	O
variably-sized	O
inputs	O
and	O
dynamically	O
adjust	O
the	O
size	O
of	O
their	O
pooling	O
regions	O
to	O
keep	O
the	O
output	O
size	O
constant	O
(	O
waibel	O
et	O
al.	O
,	O
1989	O
)	O
.	O
other	O
convolutional	O
models	O
have	O
variable-sized	O
output	O
that	O
automatically	O
scales	O
in	O
size	O
with	O
the	O
input	O
,	O
such	O
as	O
models	O
that	O
denoise	O
or	O
label	O
each	O
pixel	O
in	O
an	O
image	O
(	O
hadsell	O
et	O
al	O
.	O
2007	O
,	O
)	O
.	O
dataset	O
augmentation	O
may	O
be	O
seen	O
as	O
a	O
way	O
of	O
preprocessing	O
the	O
training	O
set	O
only	O
.	O
dataset	O
augmentation	O
is	O
an	O
excellent	O
way	O
to	O
reduce	O
the	O
generalization	O
error	O
of	O
most	O
computer	O
vision	O
models	O
.	O
a	O
related	O
idea	O
applicable	O
at	O
test	O
time	O
is	O
to	O
show	O
the	O
model	B
many	O
diﬀerent	O
versions	O
of	O
the	O
same	O
input	O
(	O
for	O
example	O
,	O
the	O
same	O
image	O
cropped	O
at	O
slightly	O
diﬀerent	O
locations	O
)	O
and	O
have	O
the	O
diﬀerent	O
instantiations	O
of	O
the	O
model	B
vote	O
to	O
determine	O
the	O
output	O
.	O
this	O
latter	O
idea	O
can	O
be	O
interpreted	O
as	O
an	O
ensemble	O
approach	O
,	O
and	O
helps	O
to	O
reduce	O
generalization	O
error	O
.	O
other	O
kinds	O
of	O
preprocessing	O
are	O
applied	O
to	O
both	O
the	O
train	O
and	O
the	O
test	O
set	O
with	O
the	O
goal	O
of	O
putting	O
each	O
example	O
into	O
a	O
more	O
canonical	O
form	O
in	O
order	O
to	O
reduce	O
the	O
amount	O
of	O
variation	O
that	O
the	O
model	B
needs	O
to	O
account	O
for	O
.	O
reducing	O
the	O
amount	O
of	O
453	O
chapter	O
12.	O
applications	O
variation	O
in	O
the	O
data	O
can	O
both	O
reduce	O
generalization	O
error	O
and	O
reduce	O
the	O
size	O
of	O
the	O
model	B
needed	O
to	O
ﬁt	O
the	O
training	O
set	O
.	O
simpler	O
tasks	O
may	O
be	O
solved	O
by	O
smaller	O
models	O
,	O
and	O
simpler	O
solutions	O
are	O
more	O
likely	O
to	O
generalize	O
well	O
.	O
preprocessing	O
of	O
this	O
kind	O
is	O
usually	O
designed	O
to	O
remove	O
some	O
kind	O
of	O
variability	O
in	O
the	O
input	O
data	O
that	O
is	O
easy	O
for	O
a	O
human	O
designer	O
to	O
describe	O
and	O
that	O
the	O
human	O
designer	O
is	O
conﬁdent	O
has	O
no	O
relevance	O
to	O
the	O
task	O
.	O
when	O
training	O
with	O
large	O
datasets	O
and	O
large	O
models	O
,	O
this	O
kind	O
of	O
preprocessing	O
is	O
often	O
unnecessary	O
,	O
and	O
it	O
is	O
best	O
to	O
just	O
let	O
the	O
model	B
learn	O
which	O
kinds	O
of	O
variability	O
it	O
should	O
become	O
invariant	O
to	O
.	O
for	O
example	O
,	O
the	O
alexnet	O
system	O
for	O
classifying	O
imagenet	O
only	O
has	O
one	O
preprocessing	O
step	O
:	O
subtracting	O
the	O
mean	O
across	O
training	O
examples	O
of	O
each	O
pixel	O
(	O
krizhevsky	O
et	O
al.	O
,	O
2012	O
)	O
.	O
12.2.1.1	O
contrast	O
normalization	O
one	O
of	O
the	O
most	O
obvious	O
sources	O
of	O
variation	O
that	O
can	O
be	O
safely	O
removed	O
for	O
many	O
tasks	O
is	O
the	O
amount	O
of	O
contrast	O
in	O
the	O
image	O
.	O
contrast	O
simply	O
refers	O
to	O
the	O
magnitude	O
of	O
the	O
diﬀerence	O
between	O
the	O
bright	O
and	O
the	O
dark	O
pixels	O
in	O
an	O
image	O
.	O
there	O
are	O
many	O
ways	O
of	O
quantifying	O
the	O
contrast	O
of	O
an	O
image	O
.	O
in	O
the	O
context	O
of	O
deep	O
learning	O
,	O
contrast	O
usually	O
refers	O
to	O
the	O
standard	O
deviation	O
of	O
the	O
pixels	O
in	O
an	O
image	O
or	O
region	O
of	O
an	O
image	O
.	O
suppose	O
we	O
have	O
an	O
image	O
represented	O
by	O
a	O
tensor	O
3	O
,	O
with	O
xi	O
,	O
j,1	O
being	O
the	O
red	O
intensity	O
at	O
row	O
i	O
and	O
column	O
j	O
,	O
xi	O
,	O
j,2	O
giving	O
x	O
the	O
green	O
intensity	O
and	O
xi	O
,	O
j,3	O
giving	O
the	O
blue	O
intensity	O
.	O
then	O
the	O
contrast	O
of	O
the	O
entire	O
image	O
is	O
given	O
by	O
	O
	O
	O
×	O
×	O
r	O
c	O
	O
	O
∈	O
r	O
	O
−	O
¯	O
x	O
2	O
i=1	O
j=1	O
xi	O
,	O
j	O
,	O
k	O
	O
	O
k=1	O
	O
where	O
¯	O
x	O
is	O
the	O
mean	O
intensity	O
of	O
the	O
entire	O
image	O
:	O
r	O
c	O
3	O
1	O
3rc	O
¯	O
x	O
=	O
1	O
3rc	O
r	O
c	O
3	O
i=1	O
j=1	O
k=1	O
xi	O
,	O
j	O
,	O
k	O
.	O
(	O
12.1	O
)	O
(	O
12.2	O
)	O
global	O
contrast	O
normalization	O
(	O
gcn	O
)	O
aims	O
to	O
prevent	O
images	O
from	O
having	O
varying	O
amounts	O
of	O
contrast	O
by	O
subtracting	O
the	O
mean	O
from	O
each	O
image	O
,	O
then	O
rescaling	O
it	O
so	O
that	O
the	O
standard	O
deviation	O
across	O
its	O
pixels	O
is	O
equal	O
to	O
some	O
constant	O
s.	O
this	O
approach	O
is	O
complicated	O
by	O
the	O
fact	O
that	O
no	O
scaling	O
factor	O
can	O
change	O
the	O
contrast	O
of	O
a	O
zero-contrast	O
image	O
(	O
one	O
whose	O
pixels	O
all	O
have	O
equal	O
intensity	O
)	O
.	O
images	O
with	O
very	O
low	O
but	O
non-zero	O
contrast	O
often	O
have	O
little	O
information	O
content	O
.	O
dividing	O
by	O
the	O
true	O
standard	O
deviation	O
usually	O
accomplishes	O
nothing	O
454	O
chapter	O
12.	O
applications	O
more	O
than	O
amplifying	O
sensor	O
noise	O
or	O
compression	O
artifacts	O
in	O
such	O
cases	O
.	O
this	O
motivates	O
introducing	O
a	O
small	O
,	O
positive	O
regularization	O
parameter	O
λ	O
to	O
bias	O
the	O
estimate	O
of	O
the	O
standard	O
deviation	O
.	O
alternately	O
,	O
one	O
can	O
constrain	O
the	O
denominator	O
	O
to	O
be	O
at	O
least	O
	O
.	O
given	O
an	O
input	O
image	O
x	O
,	O
gcn	O
produces	O
an	O
output	O
image	O
x	O
,	O
deﬁned	O
such	O
that	O
	O
	O
	O
	O
	O
	O
	O
	O
−	O
¯	O
x	O
xi	O
,	O
j	O
,	O
k	O
	O
i	O
,	O
j	O
,	O
k	O
=	O
s	O
x	O
max	O
	O
,	O
λ	O
+	O
1	O
3rc	O
r	O
i=1	O
c	O
j=1	O
3	O
k=1	O
xi	O
,	O
j	O
,	O
k	O
.	O
(	O
12.3	O
)	O
−	O
¯	O
x	O
2	O
datasets	O
consisting	O
of	O
large	O
images	O
cropped	O
to	O
interesting	O
objects	O
are	O
unlikely	O
to	O
contain	O
any	O
images	O
with	O
nearly	O
constant	O
intensity	O
.	O
in	O
these	O
cases	O
,	O
it	O
is	O
safe	O
to	O
practically	O
ignore	O
the	O
small	O
denominator	O
problem	O
by	O
setting	O
λ	O
=	O
0	O
and	O
avoid	O
division	O
by	O
0	O
in	O
extremely	O
rare	O
cases	O
by	O
setting	O
	O
to	O
an	O
extremely	O
low	O
value	O
like	O
)	O
on	O
the	O
cifar-10	O
10	O
dataset	O
.	O
small	O
images	O
cropped	O
randomly	O
are	O
more	O
likely	O
to	O
have	O
nearly	O
constant	O
intensity	O
,	O
making	O
aggressive	O
regularization	O
more	O
useful.	O
)	O
used	O
	O
=	O
0	O
and	O
=	O
10	O
on	O
small	O
,	O
randomly	O
selected	O
patches	O
drawn	O
from	O
cifar-10	O
.	O
−	O
8.	O
this	O
is	O
the	O
approach	O
used	O
by	O
goodfellow	O
et	O
al	O
.	O
2013a	O
coates	O
et	O
al	O
.	O
2011	O
λ	O
(	O
(	O
the	O
scale	O
parameter	O
s	O
can	O
usually	O
be	O
set	O
to	O
,	O
as	O
done	O
by	O
)	O
,	O
or	O
chosen	O
to	O
make	O
each	O
individual	O
pixel	O
have	O
standard	O
deviation	O
across	O
examples	O
close	O
to	O
1	O
,	O
as	O
done	O
by	O
goodfellow	O
et	O
al	O
.	O
2013a	O
coates	O
et	O
al	O
.	O
2011	O
)	O
.	O
1	O
(	O
(	O
12.3	O
is	O
just	O
a	O
rescaling	O
of	O
the	O
the	O
standard	O
deviation	O
in	O
equation	O
l2	O
norm	O
of	O
the	O
image	O
(	O
assuming	O
the	O
mean	O
of	O
the	O
image	O
has	O
already	O
been	O
removed	O
)	O
.	O
it	O
is	O
preferable	O
to	O
deﬁne	O
gcn	O
in	O
terms	O
of	O
standard	O
deviation	O
rather	O
than	O
l2	O
norm	O
because	O
the	O
standard	O
deviation	O
includes	O
division	O
by	O
the	O
number	O
of	O
pixels	O
,	O
so	O
gcn	O
based	O
on	O
standard	O
deviation	O
allows	O
the	O
same	O
s	O
to	O
be	O
used	O
regardless	O
of	O
image	O
size	O
.	O
however	O
,	O
the	O
observation	O
that	O
the	O
l2	O
norm	O
is	O
proportional	O
to	O
the	O
standard	O
deviation	O
can	O
help	O
build	O
a	O
useful	O
intuition	O
.	O
one	O
can	O
understand	O
gcn	O
as	O
mapping	O
examples	O
to	O
a	O
spherical	O
shell	O
.	O
see	O
ﬁgure	O
for	O
an	O
illustration	O
.	O
this	O
can	O
be	O
a	O
useful	O
property	O
because	O
neural	O
networks	O
are	O
often	O
better	O
at	O
responding	O
to	O
directions	O
in	O
space	O
rather	O
than	O
exact	O
locations	O
.	O
responding	O
to	O
multiple	O
distances	O
in	O
the	O
same	O
direction	O
requires	O
hidden	O
units	O
with	O
collinear	O
weight	O
vectors	O
but	O
diﬀerent	O
biases	O
.	O
such	O
coordination	O
can	O
be	O
diﬃcult	O
for	O
the	O
learning	O
algorithm	O
to	O
discover	O
.	O
additionally	O
,	O
many	O
shallow	O
graphical	O
models	O
have	O
problems	O
with	O
representing	O
multiple	O
separated	O
modes	O
along	O
the	O
same	O
line	O
.	O
gcn	O
avoids	O
these	O
problems	O
by	O
reducing	O
each	O
example	O
to	O
a	O
direction	O
rather	O
than	O
a	O
direction	O
and	O
a	O
distance	O
.	O
12.1	O
counterintuitively	O
,	O
there	O
is	O
a	O
preprocessing	O
operation	O
known	O
as	O
sphering	O
and	O
it	O
is	O
not	O
the	O
same	O
operation	O
as	O
gcn	O
.	O
sphering	O
does	O
not	O
refer	O
to	O
making	O
the	O
data	O
lie	O
on	O
a	O
spherical	O
shell	O
,	O
but	O
rather	O
to	O
rescaling	O
the	O
principal	O
components	O
to	O
have	O
455	O
chapter	O
12.	O
applications	O
raw	O
input	O
gcn	O
,	O
=	O
0	O
λ	O
−	O
gcn	O
,	O
=	O
10	O
2	O
λ	O
1	O
5	O
.	O
0	O
0	O
.	O
−	O
1	O
5	O
.	O
1	O
x	O
−	O
1	O
5	O
.	O
0	O
0	O
.	O
x	O
0	O
−	O
1	O
5	O
.	O
1	O
5	O
.	O
0	O
0	O
.	O
x0	O
−	O
1	O
5	O
.	O
1	O
5	O
.	O
1	O
5	O
.	O
0	O
0	O
.	O
x0	O
−	O
figure	O
12.1	O
:	O
gcn	O
maps	O
examples	O
onto	O
a	O
sphere	O
.	O
(	O
left	O
)	O
raw	O
input	O
data	O
may	O
have	O
any	O
norm	O
.	O
(	O
center	O
)	O
gcn	O
with	O
λ	O
=	O
0	O
maps	O
all	O
non-zero	O
examples	O
perfectly	O
onto	O
a	O
sphere	O
.	O
here	O
we	O
use	O
s	O
=	O
1	O
and	O
	O
=	O
10	O
8.	O
because	O
we	O
use	O
gcn	O
based	O
on	O
normalizing	O
the	O
standard	O
deviation	O
rather	O
than	O
the	O
l2	O
norm	O
,	O
the	O
resulting	O
sphere	O
is	O
not	O
the	O
unit	O
sphere	O
.	O
(	O
right	O
)	O
regularized	O
gcn	O
,	O
with	O
λ	O
>	O
0	O
,	O
draws	O
examples	O
toward	O
the	O
sphere	O
but	O
does	O
not	O
completely	O
discard	O
the	O
variation	O
in	O
their	O
norm	O
.	O
we	O
leave	O
and	O
the	O
same	O
as	O
before	O
.	O
s	O
	O
equal	O
variance	O
,	O
so	O
that	O
the	O
multivariate	O
normal	O
distribution	O
used	O
by	O
pca	O
has	O
spherical	O
contours	O
.	O
sphering	O
is	O
more	O
commonly	O
known	O
as	O
.	O
whitening	O
global	O
contrast	O
normalization	O
will	O
often	O
fail	O
to	O
highlight	O
image	O
features	O
we	O
would	O
like	O
to	O
stand	O
out	O
,	O
such	O
as	O
edges	O
and	O
corners	O
.	O
if	O
we	O
have	O
a	O
scene	O
with	O
a	O
large	O
dark	O
area	O
and	O
a	O
large	O
bright	O
area	O
(	O
such	O
as	O
a	O
city	O
square	O
with	O
half	O
the	O
image	O
in	O
the	O
shadow	O
of	O
a	O
building	O
)	O
then	O
global	O
contrast	O
normalization	O
will	O
ensure	O
there	O
is	O
a	O
large	O
diﬀerence	O
between	O
the	O
brightness	O
of	O
the	O
dark	O
area	O
and	O
the	O
brightness	O
of	O
the	O
light	O
area	O
.	O
it	O
will	O
not	O
,	O
however	O
,	O
ensure	O
that	O
edges	O
within	O
the	O
dark	O
region	O
stand	O
out	O
.	O
this	O
motivates	O
local	O
contrast	O
normalization	O
.	O
local	O
contrast	O
normalization	O
ensures	O
that	O
the	O
contrast	O
is	O
normalized	O
across	O
each	O
small	O
window	O
,	O
rather	O
than	O
over	O
the	O
image	O
as	O
a	O
whole	O
.	O
see	O
ﬁgure	O
for	O
a	O
comparison	O
of	O
global	O
and	O
local	O
contrast	O
normalization	O
.	O
12.2	O
various	O
deﬁnitions	O
of	O
local	O
contrast	O
normalization	O
are	O
possible	O
.	O
in	O
all	O
cases	O
,	O
one	O
modiﬁes	O
each	O
pixel	O
by	O
subtracting	O
a	O
mean	O
of	O
nearby	O
pixels	O
and	O
dividing	O
by	O
a	O
standard	O
deviation	O
of	O
nearby	O
pixels	O
.	O
in	O
some	O
cases	O
,	O
this	O
is	O
literally	O
the	O
mean	O
and	O
standard	O
deviation	O
of	O
all	O
pixels	O
in	O
a	O
rectangular	O
window	O
centered	O
on	O
the	O
pixel	O
to	O
be	O
modiﬁed	O
(	O
)	O
.	O
in	O
other	O
cases	O
,	O
this	O
is	O
a	O
weighted	O
mean	O
and	O
weighted	O
standard	O
deviation	O
using	O
gaussian	O
weights	O
centered	O
on	O
the	O
pixel	O
to	O
be	O
modiﬁed	O
.	O
in	O
the	O
case	O
of	O
color	O
images	O
,	O
some	O
strategies	O
process	O
diﬀerent	O
color	O
pinto	O
et	O
al	O
.	O
2008	O
,	O
456	O
chapter	O
12.	O
applications	O
input	O
image	O
gcn	O
lcn	O
figure	O
12.2	O
:	O
a	O
comparison	O
of	O
global	O
and	O
local	O
contrast	O
normalization	O
.	O
visually	O
,	O
the	O
eﬀects	O
of	O
global	O
contrast	O
normalization	O
are	O
subtle	O
.	O
it	O
places	O
all	O
images	O
on	O
roughly	O
the	O
same	O
scale	O
,	O
which	O
reduces	O
the	O
burden	O
on	O
the	O
learning	O
algorithm	O
to	O
handle	O
multiple	O
scales	O
.	O
local	O
contrast	O
normalization	O
modiﬁes	O
the	O
image	O
much	O
more	O
,	O
discarding	O
all	O
regions	O
of	O
constant	O
intensity	O
.	O
this	O
allows	O
the	O
model	B
to	O
focus	O
on	O
just	O
the	O
edges	O
.	O
regions	O
of	O
ﬁne	O
texture	O
,	O
such	O
as	O
the	O
houses	O
in	O
the	O
second	O
row	O
,	O
may	O
lose	O
some	O
detail	O
due	O
to	O
the	O
bandwidth	O
of	O
the	O
normalization	O
kernel	O
being	O
too	O
high	O
.	O
channels	O
separately	O
while	O
others	O
combine	O
information	O
from	O
diﬀerent	O
channels	O
to	O
normalize	O
each	O
pixel	O
(	O
sermanet	O
et	O
al	O
.	O
2012	O
,	O
)	O
.	O
local	O
contrast	O
normalization	O
can	O
usually	O
be	O
implemented	O
eﬃciently	O
by	O
using	O
separable	O
convolution	O
(	O
see	O
section	O
)	O
to	O
compute	O
feature	O
maps	O
of	O
local	O
means	O
and	O
local	O
standard	O
deviations	O
,	O
then	O
using	O
element-wise	O
subtraction	O
and	O
element-wise	O
division	O
on	O
diﬀerent	O
feature	O
maps	O
.	O
9.8	O
local	O
contrast	O
normalization	O
is	O
a	O
diﬀerentiable	O
operation	O
and	O
can	O
also	O
be	O
used	O
as	O
a	O
nonlinearity	O
applied	O
to	O
the	O
hidden	O
layers	O
of	O
a	O
network	O
,	O
as	O
well	O
as	O
a	O
preprocessing	O
operation	O
applied	O
to	O
the	O
input	O
.	O
as	O
with	O
global	O
contrast	O
normalization	O
,	O
we	O
typically	O
need	O
to	O
regularize	O
local	O
contrast	O
normalization	O
to	O
avoid	O
division	O
by	O
zero	O
.	O
in	O
fact	O
,	O
because	O
local	O
contrast	O
normalization	O
typically	O
acts	O
on	O
smaller	O
windows	O
,	O
it	O
is	O
even	O
more	O
important	O
to	O
regularize	O
.	O
smaller	O
windows	O
are	O
more	O
likely	O
to	O
contain	O
values	O
that	O
are	O
all	O
nearly	O
the	O
same	O
as	O
each	O
other	O
,	O
and	O
thus	O
more	O
likely	O
to	O
have	O
zero	O
standard	O
deviation	O
.	O
457	O
chapter	O
12.	O
applications	O
12.2.1.2	O
dataset	O
augmentation	O
7.4	O
as	O
described	O
in	O
section	O
,	O
it	O
is	O
easy	O
to	O
improve	O
the	O
generalization	O
of	O
a	O
classiﬁer	O
by	O
increasing	O
the	O
size	O
of	O
the	O
training	O
set	O
by	O
adding	O
extra	O
copies	O
of	O
the	O
training	O
examples	O
that	O
have	O
been	O
modiﬁed	O
with	O
transformations	O
that	O
do	O
not	O
change	O
the	O
class	O
.	O
object	O
recognition	B
is	O
a	O
classiﬁcation	O
task	O
that	O
is	O
especially	O
amenable	O
to	O
this	O
form	O
of	O
dataset	O
augmentation	O
because	O
the	O
class	O
is	O
invariant	O
to	O
so	O
many	O
transformations	O
and	O
the	O
input	O
can	O
be	O
easily	O
transformed	O
with	O
many	O
geometric	O
operations	O
.	O
as	O
described	O
before	O
,	O
classiﬁers	O
can	O
beneﬁt	O
from	O
random	O
translations	O
,	O
rotations	O
,	O
and	O
in	O
some	O
cases	O
,	O
ﬂips	O
of	O
the	O
input	O
to	O
augment	O
the	O
dataset	O
.	O
in	O
specialized	O
computer	O
vision	O
applications	O
,	O
more	O
advanced	O
transformations	O
are	O
commonly	O
used	O
for	O
dataset	O
augmentation	O
.	O
these	O
schemes	O
include	O
random	O
perturbation	O
of	O
the	O
colors	O
in	O
an	O
image	O
(	O
)	O
and	O
nonlinear	O
geometric	O
distortions	O
of	O
the	O
input	O
(	O
krizhevsky	O
et	O
al	O
.	O
2012	O
lecun	O
et	O
al	O
.	O
1998b	O
)	O
.	O
,	O
,	O
12.3	O
speech	O
recognition	B
the	O
task	O
of	O
speech	O
recognition	B
is	O
to	O
map	O
an	O
acoustic	O
signal	O
containing	O
a	O
spoken	O
natural	O
language	O
utterance	O
into	O
the	O
corresponding	O
sequence	O
of	O
words	O
intended	O
by	O
the	O
speaker	O
.	O
let	O
x	O
=	O
(	O
x	O
(	O
1	O
)	O
,	O
x	O
(	O
2	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
t	O
)	O
denote	O
the	O
sequence	O
of	O
acoustic	O
input	O
vectors	O
(	O
traditionally	O
produced	O
by	O
splitting	O
the	O
audio	O
into	O
20ms	O
frames	O
)	O
.	O
most	O
speech	O
recognition	B
systems	O
preprocess	O
the	O
input	O
using	O
specialized	O
hand-designed	O
features	O
,	O
but	O
some	O
(	O
)	O
deep	O
learning	O
systems	O
learn	O
features	O
from	O
raw	O
input	O
.	O
let	O
y	O
=	O
(	O
y1	O
,	O
y2	O
,	O
.	O
.	O
.	O
,	O
yn	O
)	O
denote	O
the	O
target	O
output	O
sequence	O
(	O
usually	O
a	O
sequence	O
of	O
words	O
or	O
characters	O
)	O
.	O
the	O
automatic	O
speech	O
recognition	B
(	O
asr	O
)	O
∗	O
task	O
consists	O
of	O
creating	O
a	O
function	O
f	O
asr	O
that	O
computes	O
the	O
most	O
probable	O
linguistic	O
sequence	O
given	O
the	O
acoustic	O
sequence	O
jaitly	O
and	O
hinton	O
2011	O
x	O
y	O
:	O
,	O
∗	O
asr	O
(	O
f	O
x	O
)	O
=	O
arg	O
max	O
p	O
y	O
∗	O
|	O
(	O
y	O
x	O
=	O
x	O
)	O
(	O
12.4	O
)	O
∗	O
where	O
p	O
y.	O
is	O
the	O
true	O
conditional	O
distribution	O
relating	O
the	O
inputs	O
x	O
to	O
the	O
targets	O
since	O
the	O
1980s	O
and	O
until	O
about	O
2009–2012	O
,	O
state-of-the	O
art	O
speech	O
recognition	B
systems	O
primarily	O
combined	O
hidden	O
markov	O
models	O
(	O
hmms	O
)	O
and	O
gaussian	O
mixture	O
models	O
(	O
gmms	O
)	O
.	O
gmms	O
modeled	O
the	O
association	O
between	O
acoustic	O
features	O
and	O
phonemes	O
(	O
)	O
,	O
while	O
hmms	O
modeled	O
the	O
sequence	O
of	O
phonemes	O
.	O
the	O
gmm-hmm	O
model	B
family	O
treats	O
acoustic	O
waveforms	O
as	O
being	O
generated	O
by	O
the	O
following	O
process	O
:	O
ﬁrst	O
an	O
hmm	O
generates	O
a	O
sequence	O
of	O
phonemes	O
and	O
discrete	O
sub-phonemic	O
states	O
(	O
such	O
as	O
the	O
beginning	O
,	O
middle	O
,	O
and	O
end	O
of	O
each	O
bahl	O
et	O
al	O
.	O
1987	O
,	O
458	O
chapter	O
12.	O
applications	O
;	O
,	O
;	O
,	O
;	O
,	O
;	O
,	O
1996	O
et	O
al.	O
,	O
et	O
al.	O
,	O
et	O
al.	O
,	O
1991	O
1992	O
konig	O
phoneme	O
)	O
,	O
then	O
a	O
gmm	O
transforms	O
each	O
discrete	O
symbol	O
into	O
a	O
brief	O
segment	O
of	O
audio	O
waveform	O
.	O
although	O
gmm-hmm	O
systems	O
dominated	O
asr	O
until	O
recently	O
,	O
speech	O
recognition	B
was	O
actually	O
one	O
of	O
the	O
ﬁrst	O
areas	O
where	O
neural	O
networks	O
were	O
applied	O
,	O
and	O
numerous	O
asr	O
systems	O
from	O
the	O
late	O
1980s	O
and	O
early	O
1990s	O
used	O
1989	O
robinson	O
and	O
neural	O
nets	O
(	O
bourlard	O
and	O
wellekens	O
1989	O
waibel	O
fallside	O
1991	O
bengio	O
)	O
.	O
at	O
the	O
time	O
,	O
the	O
performance	O
of	O
asr	O
based	O
on	O
neural	O
nets	O
approximately	O
matched	O
the	O
performance	O
of	O
gmm-hmm	O
systems	O
.	O
for	O
example	O
,	O
robinson	O
and	O
fallside	O
1991	O
)	O
achieved	O
26	O
%	O
phoneme	O
error	O
rate	O
on	O
the	O
timit	O
(	O
)	O
corpus	O
(	O
with	O
39	O
phonemes	O
to	O
discriminate	O
between	O
)	O
,	O
which	O
was	O
better	O
than	O
or	O
comparable	O
to	O
hmm-based	O
systems	O
.	O
since	O
then	O
,	O
timit	O
has	O
been	O
a	O
benchmark	O
for	O
phoneme	O
recognition	B
,	O
playing	O
a	O
role	O
similar	O
to	O
the	O
role	O
mnist	O
plays	O
for	O
object	O
recognition	B
.	O
however	O
,	O
because	O
of	O
the	O
complex	O
engineering	O
involved	O
in	O
software	O
systems	O
for	O
speech	O
recognition	B
and	O
the	O
eﬀort	O
that	O
had	O
been	O
invested	O
in	O
building	O
these	O
systems	O
on	O
the	O
basis	O
of	O
gmm-hmms	O
,	O
the	O
industry	O
did	O
not	O
see	O
a	O
compelling	O
argument	O
for	O
switching	O
to	O
neural	O
networks	O
.	O
as	O
a	O
consequence	O
,	O
until	O
the	O
late	O
2000s	O
,	O
both	O
academic	O
and	O
industrial	O
research	O
in	O
using	O
neural	O
nets	O
for	O
speech	O
recognition	B
mostly	O
focused	O
on	O
using	O
neural	O
nets	O
to	O
learn	O
extra	O
features	O
for	O
gmm-hmm	O
systems	O
.	O
garofolo	O
et	O
al	O
.	O
1993	O
(	O
later	O
,	O
with	O
much	O
larger	O
and	O
deeper	O
models	O
and	O
much	O
larger	O
datasets	O
,	O
recognition	B
accuracy	O
was	O
dramatically	O
improved	O
by	O
using	O
neural	O
networks	O
to	O
replace	O
gmms	O
for	O
the	O
task	O
of	O
associating	O
acoustic	O
features	O
to	O
phonemes	O
(	O
or	O
sub-phonemic	O
states	O
)	O
.	O
starting	O
in	O
2009	O
,	O
speech	O
researchers	O
applied	O
a	O
form	O
of	O
deep	O
learning	O
based	O
on	O
unsupervised	O
learning	O
to	O
speech	O
recognition	B
.	O
this	O
approach	O
to	O
deep	O
learning	O
was	O
based	O
on	O
training	O
undirected	O
probabilistic	O
models	O
called	O
restricted	O
boltzmann	O
machines	O
(	O
rbms	O
)	O
to	O
model	B
the	O
input	O
data	O
.	O
rbms	O
will	O
be	O
described	O
in	O
part	O
.	O
iii	O
to	O
solve	O
speech	O
recognition	B
tasks	O
,	O
unsupervised	O
pretraining	O
was	O
used	O
to	O
build	O
deep	O
feedforward	O
networks	O
whose	O
layers	O
were	O
each	O
initialized	O
by	O
training	O
an	O
rbm	O
.	O
these	O
networks	O
take	O
spectral	O
acoustic	O
representations	O
in	O
a	O
ﬁxed-size	O
input	O
window	O
(	O
around	O
a	O
center	O
frame	O
)	O
and	O
predict	O
the	O
conditional	O
probabilities	O
of	O
hmm	O
states	O
for	O
that	O
center	O
frame	O
.	O
training	O
such	O
deep	O
networks	O
helped	O
to	O
signiﬁcantly	O
improve	O
)	O
,	O
bringing	O
down	O
the	O
the	O
recognition	B
rate	O
on	O
timit	O
(	O
phoneme	O
error	O
rate	O
from	O
about	O
26	O
%	O
to	O
20.7	O
%	O
.	O
see	O
)	O
for	O
an	O
analysis	O
of	O
reasons	O
for	O
the	O
success	O
of	O
these	O
models	O
.	O
extensions	O
to	O
the	O
basic	O
phone	O
recognition	B
pipeline	O
included	O
the	O
addition	O
of	O
speaker-adaptive	O
features	O
(	O
mohamed	O
et	O
al.	O
,	O
)	O
that	O
further	O
reduced	O
the	O
error	O
rate	O
.	O
this	O
was	O
quickly	O
followed	O
up	O
by	O
work	B
to	O
expand	O
the	O
architecture	O
from	O
phoneme	O
recognition	B
(	O
which	O
is	O
what	O
timit	O
is	O
focused	O
on	O
)	O
to	O
large-vocabulary	O
speech	O
recognition	B
(	O
)	O
,	O
which	O
involves	O
not	O
just	O
recognizing	O
phonemes	O
but	O
also	O
recognizing	O
sequences	O
of	O
words	O
from	O
a	O
large	O
vocabulary	O
.	O
deep	O
networks	O
for	O
speech	O
recognition	B
eventually	O
mohamed	O
et	O
al	O
.	O
2009	O
2012a	O
mohamed	O
et	O
al	O
.	O
2012b	O
dahl	O
et	O
al	O
.	O
2012	O
2011	O
(	O
,	O
,	O
,	O
459	O
chapter	O
12.	O
applications	O
2013	O
shifted	O
from	O
being	O
based	O
on	O
pretraining	O
and	O
boltzmann	O
machines	O
to	O
being	O
based	O
on	O
techniques	O
such	O
as	O
rectiﬁed	O
linear	O
units	O
and	O
dropout	O
(	O
zeiler	O
et	O
al	O
.	O
2013	O
dahl	O
et	O
al.	O
,	O
)	O
.	O
by	O
that	O
time	O
,	O
several	O
of	O
the	O
major	O
speech	O
groups	O
in	O
industry	O
had	O
started	O
exploring	O
deep	O
learning	O
in	O
collaboration	O
with	O
academic	O
researchers	O
.	O
hinton	O
et	O
al	O
.	O
(	O
)	O
describe	O
the	O
breakthroughs	O
achieved	O
by	O
these	O
collaborators	O
,	O
which	O
are	O
now	O
deployed	O
in	O
products	O
such	O
as	O
mobile	O
phones	O
.	O
2012a	O
,	O
;	O
later	O
,	O
as	O
these	O
groups	O
explored	O
larger	O
and	O
larger	O
labeled	O
datasets	O
and	O
incorpo-	O
rated	O
some	O
of	O
the	O
methods	O
for	O
initializing	O
,	O
training	O
,	O
and	O
setting	O
up	O
the	O
architecture	O
of	O
deep	O
nets	O
,	O
they	O
realized	O
that	O
the	O
unsupervised	O
pretraining	O
phase	O
was	O
either	O
unnecessary	O
or	O
did	O
not	O
bring	O
any	O
signiﬁcant	O
improvement	O
.	O
these	O
breakthroughs	O
in	O
recognition	B
performance	O
for	O
word	O
error	O
rate	O
in	O
speech	O
recognition	B
were	O
unprecedented	O
(	O
around	O
30	O
%	O
improvement	O
)	O
and	O
were	O
following	O
a	O
long	O
period	O
of	O
about	O
ten	O
years	O
during	O
which	O
error	O
rates	O
did	O
not	O
improve	O
much	O
with	O
the	O
traditional	O
gmm-hmm	O
technology	O
,	O
in	O
spite	O
of	O
the	O
continuously	O
growing	O
size	O
of	O
training	O
sets	O
(	O
see	O
ﬁgure	O
2.4	O
of	O
deng	O
and	O
yu	O
2014	O
)	O
)	O
.	O
this	O
created	O
a	O
rapid	O
shift	O
in	O
the	O
speech	O
recognition	B
community	O
towards	O
deep	O
learning	O
.	O
in	O
a	O
matter	O
of	O
roughly	O
two	O
years	O
,	O
most	O
of	O
the	O
industrial	O
products	O
for	O
speech	O
recognition	B
incorporated	O
deep	O
neural	O
networks	O
and	O
this	O
success	O
spurred	O
a	O
new	O
wave	O
of	O
research	O
into	O
deep	O
learning	O
algorithms	O
and	O
architectures	O
for	O
asr	O
,	O
which	O
is	O
still	O
ongoing	O
today	O
.	O
(	O
one	O
of	O
these	O
innovations	O
was	O
the	O
use	O
of	O
convolutional	O
networks	O
(	O
sainath	O
et	O
al	O
.	O
,	O
2013	O
)	O
that	O
replicate	O
weights	O
across	O
time	O
and	O
frequency	O
,	O
improving	O
over	O
the	O
earlier	O
time-delay	O
neural	O
networks	O
that	O
replicated	O
weights	O
only	O
across	O
time	O
.	O
the	O
new	O
two-dimensional	O
convolutional	O
models	O
regard	O
the	O
input	O
spectrogram	O
not	O
as	O
one	O
long	O
vector	O
but	O
as	O
an	O
image	O
,	O
with	O
one	O
axis	O
corresponding	O
to	O
time	O
and	O
the	O
other	O
to	O
frequency	O
of	O
spectral	O
components	O
.	O
another	O
important	O
push	O
,	O
still	O
ongoing	O
,	O
has	O
been	O
towards	O
end-to-end	O
deep	O
learning	O
speech	O
recognition	B
systems	O
that	O
completely	O
remove	O
the	O
hmm	O
.	O
the	O
ﬁrst	O
major	O
breakthrough	O
in	O
this	O
direction	O
came	O
from	O
graves	O
)	O
who	O
trained	O
)	O
,	O
using	O
map	O
inference	O
over	O
the	O
frame-to-	O
a	O
deep	O
lstm	O
rnn	O
(	O
see	O
section	O
10.10	O
phoneme	O
alignment	O
,	O
as	O
in	O
(	O
graves	O
)	O
has	O
state	O
variables	O
et	O
al.	O
,	O
2006	O
graves	O
2012	O
from	O
several	O
layers	O
at	O
each	O
time	O
step	O
,	O
giving	O
the	O
unfolded	O
graph	O
two	O
kinds	O
of	O
depth	O
:	O
ordinary	O
depth	O
due	O
to	O
a	O
stack	O
of	O
layers	O
,	O
and	O
depth	O
due	O
to	O
time	O
unfolding	O
.	O
this	O
work	B
brought	O
the	O
phoneme	O
error	O
rate	O
on	O
timit	O
to	O
a	O
record	O
low	O
of	O
17.7	O
%	O
.	O
see	O
pascanu	O
)	O
for	O
other	O
variants	O
of	O
deep	O
rnns	O
,	O
applied	O
in	O
other	O
settings	O
.	O
lecun	O
et	O
al	O
.	O
1998b	O
)	O
.	O
a	O
deep	O
rnn	O
(	O
)	O
and	O
in	O
the	O
ctc	O
framework	O
(	O
et	O
al	O
.	O
(	O
et	O
al	O
.	O
(	O
graves	O
et	O
al	O
.	O
(	O
2013	O
2014a	O
)	O
and	O
et	O
al.	O
,	O
2013	O
;	O
,	O
chung	O
2014	O
another	O
contemporary	O
step	O
toward	O
end-to-end	O
deep	O
learning	O
asr	O
is	O
to	O
let	O
the	O
system	O
learn	O
how	O
to	O
“	O
align	O
”	O
the	O
acoustic-level	O
information	O
with	O
the	O
phonetic-level	O
460	O
chapter	O
12.	O
applications	O
information	O
(	O
chorowski	O
et	O
al	O
.	O
2014	O
lu	O
et	O
al	O
.	O
2015	O
,	O
;	O
,	O
)	O
.	O
12.4	O
natural	O
language	O
processing	O
natural	O
language	O
processing	O
(	O
nlp	O
)	O
is	O
the	O
use	O
of	O
human	O
languages	O
,	O
such	O
as	O
english	O
or	O
french	O
,	O
by	O
a	O
computer	O
.	O
computer	O
programs	O
typically	O
read	O
and	O
emit	O
specialized	O
languages	O
designed	O
to	O
allow	O
eﬃcient	O
and	O
unambiguous	O
parsing	O
by	O
simple	O
programs	O
.	O
more	O
naturally	O
occurring	O
languages	O
are	O
often	O
ambiguous	O
and	O
defy	O
formal	O
description	O
.	O
natural	O
language	O
processing	O
includes	O
applications	O
such	O
as	O
machine	O
translation	O
,	O
in	O
which	O
the	O
learner	O
must	O
read	O
a	O
sentence	O
in	O
one	O
human	O
language	O
and	O
emit	O
an	O
equivalent	O
sentence	O
in	O
another	O
human	O
language	O
.	O
many	O
nlp	O
applications	O
are	O
based	O
on	O
language	O
models	O
that	O
deﬁne	O
a	O
probability	O
distribution	O
over	O
sequences	O
of	O
words	O
,	O
characters	O
or	O
bytes	O
in	O
a	O
natural	O
language	O
.	O
as	O
with	O
the	O
other	O
applications	O
discussed	O
in	O
this	O
chapter	O
,	O
very	O
generic	O
neural	O
network	O
techniques	O
can	O
be	O
successfully	O
applied	O
to	O
natural	O
language	O
processing	O
.	O
however	O
,	O
to	O
achieve	O
excellent	O
performance	O
and	O
to	O
scale	O
well	O
to	O
large	O
applications	O
,	O
some	O
domain-speciﬁc	O
strategies	O
become	O
important	O
.	O
to	O
build	O
an	O
eﬃcient	O
model	B
of	O
natural	O
language	O
,	O
we	O
must	O
usually	O
use	O
techniques	O
that	O
are	O
specialized	O
for	O
processing	O
sequential	O
data	O
.	O
in	O
many	O
cases	O
,	O
we	O
choose	O
to	O
regard	O
natural	O
language	O
as	O
a	O
sequence	O
of	O
words	O
,	O
rather	O
than	O
a	O
sequence	O
of	O
individual	O
characters	O
or	O
bytes	O
.	O
because	O
the	O
total	O
number	O
of	O
possible	O
words	O
is	O
so	O
large	O
,	O
word-based	O
language	O
models	O
must	O
operate	O
on	O
an	O
extremely	O
high-dimensional	O
and	O
sparse	O
discrete	O
space	O
.	O
several	O
strategies	O
have	O
been	O
developed	O
to	O
make	O
models	O
of	O
such	O
a	O
space	O
eﬃcient	O
,	O
both	O
in	O
a	O
computational	O
and	O
in	O
a	O
statistical	O
sense	O
.	O
12.4.1	O
-grams	O
n	O
a	O
language	O
model	B
deﬁnes	O
a	O
probability	O
distribution	O
over	O
sequences	O
of	O
tokens	O
in	O
a	O
natural	O
language	O
.	O
depending	O
on	O
how	O
the	O
model	B
is	O
designed	O
,	O
a	O
token	O
may	O
be	O
a	O
word	O
,	O
a	O
character	O
,	O
or	O
even	O
a	O
byte	O
.	O
tokens	O
are	O
always	O
discrete	O
entities	O
.	O
the	O
earliest	O
successful	O
language	O
models	O
were	O
based	O
on	O
models	O
of	O
ﬁxed-length	O
sequences	O
of	O
tokens	O
called	O
-grams	O
.	O
an	O
-gram	O
is	O
a	O
sequence	O
of	O
tokens	O
.	O
n	O
n	O
n	O
models	O
based	O
on	O
n-grams	O
deﬁne	O
the	O
conditional	O
probability	O
of	O
the	O
n-th	O
token	O
1	O
tokens	O
.	O
the	O
model	B
uses	O
products	O
of	O
these	O
conditional	O
given	O
the	O
preceding	O
n	O
distributions	O
to	O
deﬁne	O
the	O
probability	O
distribution	O
over	O
longer	O
sequences	O
:	O
−	O
	O
p	O
x	O
(	O
1	O
,	O
.	O
.	O
.	O
,	O
xτ	O
)	O
=	O
−	O
1	O
)	O
(	O
p	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
|	O
τ	O
p	O
x	O
(	O
t	O
−	O
xt	O
n	O
−	O
1	O
)	O
.	O
+1	O
,	O
.	O
.	O
.	O
,	O
xt	O
(	O
12.5	O
)	O
t	O
n=	O
461	O
chapter	O
12.	O
applications	O
this	O
decomposition	O
is	O
justiﬁed	O
by	O
the	O
chain	O
rule	O
of	O
probability	O
.	O
the	O
probability	O
−	O
distribution	O
over	O
the	O
initial	O
sequence	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
1	O
)	O
may	O
be	O
modeled	O
by	O
a	O
diﬀerent	O
model	B
with	O
a	O
smaller	O
value	O
of	O
.n	O
training	O
n-gram	B
models	O
is	O
straightforward	O
because	O
the	O
maximum	O
likelihood	O
estimate	O
can	O
be	O
computed	O
simply	O
by	O
counting	O
how	O
many	O
times	O
each	O
possible	O
n	O
gram	O
occurs	O
in	O
the	O
training	O
set	O
.	O
models	O
based	O
on	O
n-grams	O
have	O
been	O
the	O
core	O
building	O
block	O
of	O
statistical	O
language	O
modeling	O
for	O
many	O
decades	O
(	O
jelinek	O
and	O
mercer	O
1980	O
katz	O
1987	O
chen	O
and	O
goodman	O
1999	O
)	O
.	O
;	O
,	O
;	O
,	O
,	O
for	O
small	O
values	O
of	O
n	O
,	O
models	O
have	O
particular	O
names	O
:	O
unigram	O
for	O
n=1	O
,	O
bigram	O
for	O
n=2	O
,	O
and	O
trigram	O
for	O
n=3	O
.	O
these	O
names	O
derive	O
from	O
the	O
latin	O
preﬁxes	O
for	O
the	O
corresponding	O
numbers	O
and	O
the	O
greek	O
suﬃx	O
“	O
-gram	O
”	O
denoting	O
something	O
that	O
is	O
written	O
.	O
−	O
usually	O
we	O
train	O
both	O
an	O
n-gram	B
model	O
and	O
an	O
n	O
this	O
makes	O
it	O
easy	O
to	O
compute	O
1	O
gram	O
model	B
simultaneously	O
.	O
|	O
p	O
x	O
(	O
t	O
−	O
xt	O
n	O
−	O
1	O
)	O
=	O
+1	O
,	O
.	O
.	O
.	O
,	O
xt	O
−	O
pn	O
(	O
xt	O
n	O
−	O
−	O
1	O
(	O
xt	O
n	O
pn	O
+1	O
,	O
.	O
.	O
.	O
,	O
xt	O
)	O
−	O
1	O
)	O
+1	O
,	O
.	O
.	O
.	O
,	O
xt	O
(	O
12.6	O
)	O
simply	O
by	O
looking	O
up	O
two	O
stored	O
probabilities	O
.	O
for	O
this	O
to	O
exactly	O
reproduce	O
inference	O
in	O
pn	O
,	O
we	O
must	O
omit	O
the	O
ﬁnal	O
character	O
from	O
each	O
sequence	O
when	O
we	O
−	O
1.	O
train	O
p	O
n	O
as	O
an	O
example	O
,	O
we	O
demonstrate	O
how	O
a	O
trigram	O
model	B
computes	O
the	O
probability	O
of	O
the	O
sentence	O
“	O
the	O
dog	O
ran	O
away.	O
”	O
the	O
ﬁrst	O
words	O
of	O
the	O
sentence	O
can	O
not	O
be	O
handled	O
by	O
the	O
default	O
formula	O
based	O
on	O
conditional	O
probability	O
because	O
there	O
is	O
no	O
context	O
at	O
the	O
beginning	O
of	O
the	O
sentence	O
.	O
instead	O
,	O
we	O
must	O
use	O
the	O
marginal	O
prob-	O
ability	O
over	O
words	O
at	O
the	O
start	O
of	O
the	O
sentence	O
.	O
we	O
thus	O
evaluate	O
p3	O
(	O
the	O
dog	O
ran	O
)	O
.	O
finally	O
,	O
the	O
last	O
word	O
may	O
be	O
predicted	O
using	O
the	O
typical	O
case	O
,	O
of	O
using	O
the	O
condi-	O
tional	O
distribution	O
p	O
(	O
away	O
dog	O
ran	O
,	O
12.6	O
we	O
obtain	O
:	O
)	O
.	O
putting	O
this	O
together	O
with	O
equation	O
|	O
p	O
)	O
)	O
p	O
3	O
(	O
)	O
=	O
(	O
the	O
dog	O
ran	O
away	O
the	O
dog	O
ran	O
p3	O
(	O
dog	O
ran	O
away	O
/p	O
2	O
(	O
)	O
dog	O
ran	O
.	O
(	O
12.7	O
)	O
a	O
fundamental	O
limitation	O
of	O
maximum	O
likelihood	O
for	O
n-gram	B
models	O
is	O
that	O
pn	O
as	O
estimated	O
from	O
training	O
set	O
counts	O
is	O
very	O
likely	O
to	O
be	O
zero	O
in	O
many	O
cases	O
,	O
even	O
−	O
though	O
the	O
tuple	O
(	O
x	O
t	O
n	O
+1	O
,	O
.	O
.	O
.	O
,	O
xt	O
)	O
may	O
appear	O
in	O
the	O
test	O
set	O
.	O
this	O
can	O
cause	O
two	O
−	O
diﬀerent	O
kinds	O
of	O
catastrophic	O
outcomes	O
.	O
when	O
pn	O
1	O
is	O
zero	O
,	O
the	O
ratio	O
is	O
undeﬁned	O
,	O
−	O
so	O
the	O
model	B
does	O
not	O
even	O
produce	O
a	O
sensible	O
output	O
.	O
when	O
pn	O
1	O
is	O
non-zero	O
but	O
pn	O
is	O
zero	O
,	O
the	O
test	O
log-likelihood	O
is	O
.	O
to	O
avoid	O
such	O
catastrophic	O
outcomes	O
,	O
most	O
n-gram	B
models	O
employ	O
some	O
form	O
of	O
smoothing	O
.	O
smoothing	O
techniques	O
−∞	O
462	O
chapter	O
12.	O
applications	O
(	O
chen	O
and	O
goodman	O
1999	O
shift	O
probability	O
mass	O
from	O
the	O
observed	O
tuples	O
to	O
unobserved	O
ones	O
that	O
are	O
similar	O
.	O
see	O
)	O
for	O
a	O
review	O
and	O
empirical	O
comparisons	O
.	O
one	O
basic	O
technique	O
consists	O
of	O
adding	O
non-zero	O
probability	O
mass	O
to	O
all	O
of	O
the	O
possible	O
next	O
symbol	O
values	O
.	O
this	O
method	O
can	O
be	O
justiﬁed	O
as	O
bayesian	O
inference	O
with	O
a	O
uniform	O
or	O
dirichlet	O
prior	O
over	O
the	O
count	O
parameters	O
.	O
another	O
very	O
popular	O
idea	O
is	O
to	O
form	O
a	O
mixture	O
model	B
containing	O
higher-order	O
and	O
lower-order	O
n-gram	B
models	O
,	O
with	O
the	O
higher-order	O
models	O
providing	O
more	O
capacity	O
and	O
the	O
lower-order	O
models	O
being	O
more	O
likely	O
to	O
avoid	O
counts	O
of	O
zero	O
.	O
back-oﬀ	O
methods	O
look-up	O
the	O
lower-order	O
−	O
−	O
n-grams	O
if	O
the	O
frequency	O
of	O
the	O
context	O
xt	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
t	O
n	O
+1	O
is	O
too	O
small	O
to	O
use	O
the	O
higher-order	O
model	B
.	O
more	O
formally	O
,	O
they	O
estimate	O
the	O
distribution	O
over	O
xt	O
by	O
using	O
−	O
contexts	O
xt	O
n	O
k	O
1	O
,	O
for	O
increasing	O
k	O
,	O
until	O
a	O
suﬃciently	O
reliable	O
estimate	O
is	O
+	O
,	O
.	O
.	O
.	O
,	O
xt	O
found	O
.	O
−	O
|	O
v	O
|	O
n	O
possible	O
n-grams	O
and	O
|	O
|	O
classical	O
n-gram	B
models	O
are	O
particularly	O
vulnerable	O
to	O
the	O
curse	O
of	O
dimension-	O
ality	O
.	O
there	O
are	O
v	O
is	O
often	O
very	O
large	O
.	O
even	O
with	O
a	O
massive	O
training	O
set	O
and	O
modest	O
n	O
,	O
most	O
n-grams	O
will	O
not	O
occur	O
in	O
the	O
training	O
set	O
.	O
one	O
way	O
to	O
view	O
a	O
classical	O
n-gram	B
model	O
is	O
that	O
it	O
is	O
performing	O
nearest-neighbor	O
lookup	O
.	O
in	O
other	O
words	O
,	O
it	O
can	O
be	O
viewed	O
as	O
a	O
local	O
non-parametric	O
predictor	O
,	O
similar	O
to	O
k-nearest	B
neighbors	I
.	O
the	O
statistical	O
problems	O
facing	O
these	O
extremely	O
local	O
predictors	O
are	O
described	O
in	O
section	O
.	O
the	O
problem	O
for	O
a	O
language	O
model	B
is	O
even	O
more	O
severe	O
than	O
usual	O
,	O
because	O
any	O
two	O
diﬀerent	O
words	O
have	O
the	O
same	O
dis-	O
tance	O
from	O
each	O
other	O
in	O
one-hot	O
vector	O
space	O
.	O
it	O
is	O
thus	O
diﬃcult	O
to	O
leverage	O
much	O
information	O
from	O
any	O
“	O
neighbors	O
”	O
—only	O
training	O
examples	O
that	O
repeat	O
literally	O
the	O
same	O
context	O
are	O
useful	O
for	O
local	O
generalization	O
.	O
to	O
overcome	O
these	O
problems	O
,	O
a	O
language	O
model	B
must	O
be	O
able	O
to	O
share	O
knowledge	O
between	O
one	O
word	O
and	O
other	O
semantically	O
similar	O
words	O
.	O
5.11.2	O
,	O
;	O
;	O
1998	O
et	O
al.	O
,	O
et	O
al.	O
,	O
1992	O
ney	O
and	O
kneser	O
1993	O
niesler	O
to	O
improve	O
the	O
statistical	O
eﬃciency	O
of	O
n-gram	B
models	O
,	O
class-based	O
language	O
models	O
(	O
brown	O
)	O
introduce	O
the	O
notion	O
of	O
word	O
categories	O
and	O
then	O
share	O
statistical	O
strength	O
between	O
words	O
that	O
are	O
in	O
the	O
same	O
category	O
.	O
the	O
idea	O
is	O
to	O
use	O
a	O
clustering	O
algorithm	O
to	O
partition	O
the	O
set	O
of	O
words	O
into	O
clusters	O
or	O
classes	O
,	O
based	O
on	O
their	O
co-occurrence	O
frequencies	O
with	O
other	O
words	O
.	O
the	O
model	B
can	O
then	O
use	O
word	O
class	O
ids	O
rather	O
than	O
individual	O
word	O
ids	O
to	O
represent	O
the	O
context	O
on	O
the	O
right	O
side	O
of	O
the	O
conditioning	O
bar	O
.	O
composite	O
models	O
combining	O
word-based	O
and	O
class-based	O
models	O
via	O
mixing	O
or	O
back-oﬀ	O
are	O
also	O
possible	O
.	O
although	O
word	O
classes	O
provide	O
a	O
way	O
to	O
generalize	O
between	O
sequences	O
in	O
which	O
some	O
word	O
is	O
replaced	O
by	O
another	O
of	O
the	O
same	O
class	O
,	O
much	O
information	O
is	O
lost	O
in	O
this	O
representation	O
.	O
463	O
chapter	O
12.	O
applications	O
12.4.2	O
neural	O
language	O
models	O
,	O
neural	O
language	O
models	O
or	O
nlms	O
are	O
a	O
class	O
of	O
language	O
model	B
designed	O
to	O
overcome	O
the	O
curse	O
of	O
dimensionality	O
problem	O
for	O
modeling	O
natural	O
language	O
sequences	O
by	O
using	O
a	O
distributed	O
representation	O
of	O
words	O
(	O
bengio	O
et	O
al	O
.	O
2001	O
)	O
.	O
unlike	O
class-based	O
n-gram	B
models	O
,	O
neural	O
language	O
models	O
are	O
able	O
to	O
recognize	O
that	O
two	O
words	O
are	O
similar	O
without	O
losing	O
the	O
ability	O
to	O
encode	O
each	O
word	O
as	O
distinct	O
from	O
the	O
other	O
.	O
neural	O
language	O
models	O
share	O
statistical	O
strength	O
between	O
one	O
word	O
(	O
and	O
its	O
context	O
)	O
and	O
other	O
similar	O
words	O
and	O
contexts	O
.	O
the	O
distributed	O
representation	O
the	O
model	B
learns	O
for	O
each	O
word	O
enables	O
this	O
sharing	O
by	O
allowing	O
the	O
model	B
to	O
treat	O
words	O
that	O
have	O
features	O
in	O
common	O
similarly	O
.	O
for	O
example	O
,	O
if	O
the	O
word	O
dog	O
and	O
the	O
word	O
cat	O
map	O
to	O
representations	O
that	O
share	O
many	O
attributes	O
,	O
then	O
sentences	O
that	O
contain	O
the	O
word	O
cat	O
can	O
inform	O
the	O
predictions	O
that	O
will	O
be	O
made	O
by	O
the	O
model	B
for	O
sentences	O
that	O
contain	O
the	O
word	O
dog	O
,	O
and	O
vice-versa	O
.	O
because	O
there	O
are	O
many	O
such	O
attributes	O
,	O
there	O
are	O
many	O
ways	O
in	O
which	O
generalization	O
can	O
happen	O
,	O
transferring	O
information	O
from	O
each	O
training	O
sentence	O
to	O
an	O
exponentially	O
large	O
number	O
of	O
semantically	O
related	O
sentences	O
.	O
the	O
curse	O
of	O
dimensionality	O
requires	O
the	O
model	B
to	O
generalize	O
to	O
a	O
number	O
of	O
sentences	O
that	O
is	O
exponential	O
in	O
the	O
sentence	O
length	O
.	O
the	O
model	B
counters	O
this	O
curse	O
by	O
relating	O
each	O
training	O
sentence	O
to	O
an	O
exponential	O
number	O
of	O
similar	O
sentences	O
.	O
we	O
sometimes	O
call	O
these	O
word	O
representations	O
word	O
embeddings	O
.	O
in	O
this	O
interpretation	O
,	O
we	O
view	O
the	O
raw	O
symbols	O
as	O
points	O
in	O
a	O
space	O
of	O
dimension	O
equal	O
to	O
the	O
vocabulary	O
size	O
.	O
the	O
word	O
representations	O
embed	O
those	O
points	O
in	O
a	O
feature	O
√	O
space	O
of	O
lower	O
dimension	O
.	O
in	O
the	O
original	O
space	O
,	O
every	O
word	O
is	O
represented	O
by	O
2	O
from	O
each	O
a	O
one-hot	O
vector	O
,	O
so	O
every	O
pair	O
of	O
words	O
is	O
at	O
euclidean	O
distance	O
other	O
.	O
in	O
the	O
embedding	O
space	O
,	O
words	O
that	O
frequently	O
appear	O
in	O
similar	O
contexts	O
(	O
or	O
any	O
pair	O
of	O
words	O
sharing	O
some	O
“	O
features	O
”	O
learned	O
by	O
the	O
model	B
)	O
are	O
close	O
to	O
each	O
other	O
.	O
this	O
often	O
results	O
in	O
words	O
with	O
similar	O
meanings	O
being	O
neighbors	O
.	O
figure	O
zooms	O
in	O
on	O
speciﬁc	O
areas	O
of	O
a	O
learned	O
word	O
embedding	O
space	O
to	O
show	O
how	O
semantically	O
similar	O
words	O
map	O
to	O
representations	O
that	O
are	O
close	O
to	O
each	O
other	O
.	O
12.3	O
neural	O
networks	O
in	O
other	O
domains	O
also	O
deﬁne	O
embeddings	O
.	O
for	O
example	O
,	O
a	O
hidden	O
layer	O
of	O
a	O
convolutional	O
network	O
provides	O
an	O
“	O
image	O
embedding.	O
”	O
usually	O
nlp	O
practitioners	O
are	O
much	O
more	O
interested	O
in	O
this	O
idea	O
of	O
embeddings	O
because	O
natural	O
language	O
does	O
not	O
originally	O
lie	O
in	O
a	O
real-valued	O
vector	O
space	O
.	O
the	O
hidden	O
layer	O
has	O
provided	O
a	O
more	O
qualitatively	O
dramatic	O
change	O
in	O
the	O
way	O
the	O
data	O
is	O
represented	O
.	O
the	O
basic	O
idea	O
of	O
using	O
distributed	O
representations	O
to	O
improve	O
models	O
for	O
natural	O
language	O
processing	O
is	O
not	O
restricted	O
to	O
neural	O
networks	O
.	O
it	O
may	O
also	O
be	O
used	O
with	O
graphical	O
models	O
that	O
have	O
distributed	O
representations	O
in	O
the	O
form	O
of	O
464	O
chapter	O
12.	O
applications	O
multiple	O
latent	O
variables	O
(	O
mnih	O
and	O
hinton	O
2007	O
,	O
)	O
.	O
−	O
6	O
−	O
7	O
−	O
8	O
−	O
9	O
−	O
10	O
−	O
11	O
−	O
12	O
−	O
13	O
−	O
14	O
france	O
china	O
russian	O
french	O
english	O
germany	O
ontario	O
iraq	O
japan	O
europe	O
eu	O
unionafrican	O
africa	O
assembly	O
european	O
british	O
−	O
34	O
−	O
32	O
−	O
30	O
canada	O
canadian	O
north	O
−	O
south	O
28	O
−	O
26	O
22	O
21	O
20	O
19	O
18	O
2009	O
2008	O
2004	O
2003	O
2007	O
2001	O
2006	O
2005	O
2000	O
1999	O
1995	O
2002	O
1996	O
19971998	O
17	O
35	O
0	O
35	O
5	O
36	O
0	O
36	O
5	O
37	O
0	O
37	O
5	O
38	O
0	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
figure	O
12.3	O
:	O
two-dimensional	O
visualizations	O
of	O
word	O
embeddings	O
obtained	O
from	O
a	O
neural	O
machine	O
translation	O
model	B
(	O
)	O
,	O
zooming	O
in	O
on	O
speciﬁc	O
areas	O
where	O
semantically	O
related	O
words	O
have	O
embedding	O
vectors	O
that	O
are	O
close	O
to	O
each	O
other	O
.	O
countries	O
appear	O
on	O
the	O
left	O
and	O
numbers	O
on	O
the	O
right	O
.	O
keep	O
in	O
mind	O
that	O
these	O
embeddings	O
are	O
2-d	O
for	O
the	O
purpose	O
of	O
visualization	O
.	O
in	O
real	O
applications	O
,	O
embeddings	O
typically	O
have	O
higher	O
dimensionality	O
and	O
can	O
simultaneously	O
capture	O
many	O
kinds	O
of	O
similarity	O
between	O
words	O
.	O
bahdanau	O
et	O
al	O
.	O
2015	O
,	O
12.4.3	O
high-dimensional	O
outputs	O
in	O
many	O
natural	O
language	O
applications	O
,	O
we	O
often	O
want	O
our	O
models	O
to	O
produce	O
words	O
(	O
rather	O
than	O
characters	O
)	O
as	O
the	O
fundamental	O
unit	O
of	O
the	O
output	O
.	O
for	O
large	O
vocabularies	O
,	O
it	O
can	O
be	O
very	O
computationally	O
expensive	O
to	O
represent	O
an	O
output	O
distribution	O
over	O
the	O
choice	O
of	O
a	O
word	O
,	O
because	O
the	O
vocabulary	O
size	O
is	O
large	O
.	O
in	O
many	O
applications	O
,	O
v	O
contains	O
hundreds	O
of	O
thousands	O
of	O
words	O
.	O
the	O
naive	O
approach	O
to	O
representing	O
such	O
a	O
distribution	O
is	O
to	O
apply	O
an	O
aﬃne	O
transformation	O
from	O
a	O
hidden	O
|	O
|	O
representation	O
to	O
the	O
output	O
space	O
,	O
then	O
apply	O
the	O
softmax	O
function	O
.	O
suppose	O
we	O
have	O
a	O
vocabulary	O
v	O
with	O
size	O
v	O
.	O
the	O
weight	O
matrix	O
describing	O
the	O
linear	O
|	O
|	O
component	O
of	O
this	O
aﬃne	O
transformation	O
is	O
very	O
large	O
,	O
because	O
its	O
output	O
dimension	O
is	O
v	O
.	O
this	O
imposes	O
a	O
high	O
memory	O
cost	O
to	O
represent	O
the	O
matrix	O
,	O
and	O
a	O
high	O
|	O
|	O
computational	O
cost	O
to	O
multiply	O
by	O
it	O
.	O
because	O
the	O
softmax	O
is	O
normalized	O
across	O
all	O
v	O
outputs	O
,	O
it	O
is	O
necessary	O
to	O
perform	O
the	O
full	O
matrix	O
multiplication	O
at	O
training	O
time	O
as	O
well	O
as	O
test	O
time—we	O
can	O
not	O
calculate	O
only	O
the	O
dot	O
product	O
with	O
the	O
weight	O
vector	O
for	O
the	O
correct	O
output	O
.	O
the	O
high	O
computational	O
costs	O
of	O
the	O
output	O
layer	O
thus	O
arise	O
both	O
at	O
training	O
time	O
(	O
to	O
compute	O
the	O
likelihood	O
and	O
its	O
gradient	O
)	O
and	O
at	O
test	O
time	O
(	O
to	O
compute	O
probabilities	O
for	O
all	O
or	O
selected	O
words	O
)	O
.	O
for	O
specialized	O
465	O
chapter	O
12.	O
applications	O
)	O
,	O
but	O
loss	O
functions	O
,	O
the	O
gradient	O
can	O
be	O
computed	O
eﬃciently	O
(	O
the	O
standard	O
cross-entropy	O
loss	O
applied	O
to	O
a	O
traditional	O
softmax	O
output	O
layer	O
poses	O
many	O
diﬃculties	O
.	O
vincent	O
et	O
al	O
.	O
2015	O
,	O
suppose	O
that	O
h	O
is	O
the	O
top	O
hidden	O
layer	O
used	O
to	O
predict	O
the	O
output	O
probabilities	O
ˆy	O
.	O
if	O
we	O
parametrize	O
the	O
transformation	O
from	O
h	O
to	O
ˆy	O
with	O
learned	O
weights	O
w	O
and	O
learned	O
biases	O
b	O
,	O
then	O
the	O
aﬃne-softmax	O
output	O
layer	O
performs	O
the	O
following	O
computations	O
:	O
	O
ai	O
=	O
bi	O
+	O
wijhj	O
∀	O
∈	O
{	O
i	O
|	O
|	O
}	O
1	O
,	O
.	O
.	O
.	O
,	O
v	O
,	O
	O
ˆyi	O
=	O
j	O
eai	O
|	O
|	O
	O
.	O
	O
=1	O
ea	O
i	O
v	O
i	O
(	O
12.8	O
)	O
(	O
12.9	O
)	O
|	O
|	O
|	O
|	O
v	O
n	O
h	O
)	O
.	O
with	O
nh	O
in	O
the	O
v	O
in	O
the	O
hundreds	O
of	O
thousands	O
,	O
this	O
operation	O
dominates	O
the	O
if	O
h	O
contains	O
nh	O
elements	O
then	O
the	O
above	O
operation	O
is	O
o	O
(	O
thousands	O
and	O
computation	O
of	O
most	O
neural	O
language	O
models	O
.	O
12.4.3.1	O
use	O
of	O
a	O
short	O
list	O
,	O
,	O
bengio	O
et	O
al	O
.	O
2001	O
2003	O
the	O
ﬁrst	O
neural	O
language	O
models	O
(	O
)	O
dealt	O
with	O
the	O
high	O
cost	O
of	O
using	O
a	O
softmax	O
over	O
a	O
large	O
number	O
of	O
output	O
words	O
by	O
limiting	O
the	O
vocabulary	O
size	O
to	O
10,000	O
or	O
20,000	O
words	O
.	O
schwenk	O
and	O
gauvain	O
2002	O
schwenk	O
2007	O
)	O
)	O
and	O
\	O
built	O
upon	O
this	O
approach	O
by	O
splitting	O
the	O
vocabulary	O
v	O
into	O
a	O
shortlist	O
l	O
of	O
most	O
frequent	O
words	O
(	O
handled	O
by	O
the	O
neural	O
net	O
)	O
and	O
a	O
tail	O
t	O
=	O
v	O
l	O
of	O
more	O
rare	O
words	O
(	O
handled	O
by	O
an	O
n-gram	B
model	O
)	O
.	O
to	O
be	O
able	O
to	O
combine	O
the	O
two	O
predictions	O
,	O
the	O
neural	O
net	O
also	O
has	O
to	O
predict	O
the	O
probability	O
that	O
a	O
word	O
appearing	O
after	O
context	O
c	O
belongs	O
to	O
the	O
tail	O
list	O
.	O
this	O
may	O
be	O
achieved	O
by	O
adding	O
an	O
extra	O
sigmoid	O
output	O
unit	O
to	O
provide	O
an	O
estimate	O
of	O
p	O
(	O
i	O
)	O
.	O
the	O
extra	O
output	O
can	O
then	O
be	O
used	O
to	O
achieve	O
an	O
estimate	O
of	O
the	O
probability	O
distribution	O
over	O
all	O
words	O
in	O
as	O
follows	O
:	O
∈	O
|	O
c	O
t	O
(	O
(	O
v	O
|	O
i	O
c	O
∈	O
)	O
=1i	O
p	O
y	O
(	O
=	O
∈	O
|	O
i	O
c	O
,	O
i	O
|	O
i	O
c	O
,	O
i	O
l	O
(	O
=	O
p	O
y	O
∈	O
p	O
y	O
+	O
1i	O
t	O
(	O
=	O
−	O
∈	O
|	O
l	O
)	O
(	O
1	O
∈	O
t	O
)	O
(	O
p	O
i	O
∈	O
|	O
p	O
i	O
(	O
t	O
t	O
c	O
)	O
)	O
c	O
)	O
|	O
∈	O
(	O
12.10	O
)	O
|	O
∈	O
l	O
)	O
is	O
provided	O
by	O
the	O
neural	O
language	O
model	B
and	O
p	O
(	O
y	O
=	O
i	O
where	O
p	O
(	O
y	O
=	O
i	O
c	O
,	O
i	O
t	O
)	O
is	O
provided	O
by	O
the	O
n-gram	B
model	O
.	O
with	O
slight	O
modiﬁcation	O
,	O
this	O
approach	O
c	O
,	O
i	O
can	O
also	O
work	B
using	O
an	O
extra	O
output	O
value	O
in	O
the	O
neural	O
language	O
model	B
’	O
s	O
softmax	O
layer	O
,	O
rather	O
than	O
a	O
separate	O
sigmoid	O
unit	O
.	O
an	O
obvious	O
disadvantage	O
of	O
the	O
short	O
list	O
approach	O
is	O
that	O
the	O
potential	O
gener-	O
alization	O
advantage	O
of	O
the	O
neural	O
language	O
models	O
is	O
limited	O
to	O
the	O
most	O
frequent	O
466	O
chapter	O
12.	O
applications	O
words	O
,	O
where	O
,	O
arguably	O
,	O
it	O
is	O
the	O
least	O
useful	O
.	O
this	O
disadvantage	O
has	O
stimulated	O
the	O
exploration	O
of	O
alternative	O
methods	O
to	O
deal	O
with	O
high-dimensional	O
outputs	O
,	O
described	O
below	O
.	O
12.4.3.2	O
hierarchical	O
softmax	O
,	O
goodman	O
2001	O
)	O
to	O
reducing	O
the	O
computational	O
burden	O
a	O
classical	O
approach	O
(	O
of	O
high-dimensional	O
output	O
layers	O
over	O
large	O
vocabulary	O
sets	O
v	O
is	O
to	O
decompose	O
|	O
|	O
probabilities	O
hierarchically	O
.	O
instead	O
of	O
necessitating	O
a	O
number	O
of	O
computations	O
v	O
(	O
and	O
also	O
proportional	O
to	O
the	O
number	O
of	O
hidden	O
units	O
,	O
nh	O
)	O
,	O
proportional	O
to	O
the	O
morin	O
and	O
bengio	O
2005	O
)	O
introduced	O
this	O
factorized	O
approach	O
to	O
the	O
context	O
of	O
neural	O
language	O
(	O
models	O
.	O
|	O
|	O
v	O
factor	O
can	O
be	O
reduced	O
to	O
as	O
low	O
as	O
log	O
bengio	O
2002	O
|	O
|	O
v	O
.	O
(	O
)	O
and	O
one	O
can	O
think	O
of	O
this	O
hierarchy	O
as	O
building	O
categories	O
of	O
words	O
,	O
then	O
categories	O
of	O
categories	O
of	O
words	O
,	O
then	O
categories	O
of	O
categories	O
of	O
categories	O
of	O
words	O
,	O
etc	O
.	O
|	O
|	O
these	O
nested	O
categories	O
form	O
a	O
tree	O
,	O
with	O
words	O
at	O
the	O
leaves	O
.	O
in	O
a	O
balanced	O
tree	O
,	O
the	O
tree	O
has	O
depth	O
o	O
(	O
log	O
v	O
)	O
.	O
the	O
probability	O
of	O
a	O
choosing	O
a	O
word	O
is	O
given	O
by	O
the	O
product	O
of	O
the	O
probabilities	O
of	O
choosing	O
the	O
branch	O
leading	O
to	O
that	O
word	O
at	O
every	O
node	O
on	O
a	O
path	O
from	O
the	O
root	O
of	O
the	O
tree	O
to	O
the	O
leaf	O
containing	O
the	O
word	O
.	O
figure	O
)	O
also	O
describe	O
how	O
to	O
use	O
multiple	O
paths	O
to	O
identify	O
a	O
single	O
word	O
in	O
order	O
to	O
better	O
model	B
words	O
that	O
have	O
multiple	O
meanings	O
.	O
computing	O
the	O
probability	O
of	O
a	O
word	O
then	O
involves	O
summation	O
over	O
all	O
of	O
the	O
paths	O
that	O
lead	O
to	O
that	O
word	O
.	O
illustrates	O
a	O
simple	O
example	O
.	O
mnih	O
and	O
hinton	O
2009	O
12.4	O
(	O
to	O
predict	O
the	O
conditional	O
probabilities	O
required	O
at	O
each	O
node	O
of	O
the	O
tree	O
,	O
we	O
typically	O
use	O
a	O
logistic	O
regression	O
model	B
at	O
each	O
node	O
of	O
the	O
tree	O
,	O
and	O
provide	O
the	O
same	O
context	O
c	O
as	O
input	O
to	O
all	O
of	O
these	O
models	O
.	O
because	O
the	O
correct	O
output	O
is	O
encoded	O
in	O
the	O
training	O
set	O
,	O
we	O
can	O
use	O
supervised	O
learning	O
to	O
train	O
the	O
logistic	O
regression	O
models	O
.	O
this	O
is	O
typically	O
done	O
using	O
a	O
standard	O
cross-entropy	O
loss	O
,	O
corresponding	O
to	O
maximizing	O
the	O
log-likelihood	O
of	O
the	O
correct	O
sequence	O
of	O
decisions	O
.	O
|	O
because	O
the	O
output	O
log-likelihood	O
can	O
be	O
computed	O
eﬃciently	O
(	O
as	O
low	O
as	O
log	O
|	O
|	O
v	O
rather	O
than	O
v	O
)	O
,	O
its	O
gradients	O
may	O
also	O
be	O
computed	O
eﬃciently	O
.	O
this	O
includes	O
not	O
only	O
the	O
gradient	O
with	O
respect	O
to	O
the	O
output	O
parameters	O
but	O
also	O
the	O
gradients	O
with	O
respect	O
to	O
the	O
hidden	O
layer	O
activations	O
.	O
|	O
it	O
is	O
possible	O
but	O
usually	O
not	O
practical	O
to	O
optimize	O
the	O
tree	O
structure	O
to	O
minimize	O
the	O
expected	O
number	O
of	O
computations	O
.	O
tools	O
from	O
information	O
theory	O
specify	O
how	O
to	O
choose	O
the	O
optimal	O
binary	O
code	O
given	O
the	O
relative	O
frequencies	O
of	O
the	O
words	O
.	O
to	O
do	O
so	O
,	O
we	O
could	O
structure	O
the	O
tree	O
so	O
that	O
the	O
number	O
of	O
bits	O
associated	O
with	O
a	O
word	O
is	O
approximately	O
equal	O
to	O
the	O
logarithm	O
of	O
the	O
frequency	O
of	O
that	O
word	O
.	O
however	O
,	O
in	O
467	O
chapter	O
12.	O
applications	O
(	O
0	O
)	O
(	O
1	O
)	O
(	O
0,0	O
)	O
(	O
0,1	O
)	O
(	O
1,0	O
)	O
(	O
1,1	O
)	O
w0w0	O
w1w1	O
w2w2	O
w3w3	O
w4w4	O
w5w5	O
w6w6	O
w7w7	O
(	O
0,0,0	O
)	O
(	O
0,0,1	O
)	O
(	O
0,1,0	O
)	O
(	O
0,1,1	O
)	O
(	O
1,0,0	O
)	O
(	O
1,0,1	O
)	O
(	O
1,1,0	O
)	O
(	O
1,1,1	O
)	O
{	O
}	O
and	O
(	O
0	O
w2	O
,	O
w3	O
,	O
1	O
)	O
,	O
which	O
respectively	O
contain	O
the	O
sets	O
of	O
words	O
figure	O
12.4	O
:	O
illustration	O
of	O
a	O
simple	O
hierarchy	O
of	O
word	O
categories	O
,	O
with	O
8	O
words	O
w0	O
,	O
.	O
.	O
.	O
,	O
w7	O
organized	O
into	O
a	O
three	O
level	O
hierarchy	O
.	O
the	O
leaves	O
of	O
the	O
tree	O
represent	O
actual	O
speciﬁc	O
words	O
.	O
internal	O
nodes	O
represent	O
groups	O
of	O
words	O
.	O
any	O
node	O
can	O
be	O
indexed	O
by	O
the	O
sequence	O
}	O
{	O
of	O
binary	O
decisions	O
(	O
0=left	O
,	O
1=right	O
)	O
to	O
reach	O
the	O
node	O
from	O
the	O
root	O
.	O
super-class	O
(	O
0	O
)	O
contains	O
the	O
classes	O
(	O
0	O
,	O
0	O
)	O
w	O
0	O
,	O
w1	O
and	O
,	O
1	O
)	O
,	O
which	O
,	O
and	O
similarly	O
super-class	O
(	O
1	O
)	O
respectively	O
contain	O
the	O
words	O
(	O
w4	O
,	O
w	O
5	O
)	O
and	O
w	O
6	O
,	O
w7	O
)	O
.	O
if	O
the	O
tree	O
is	O
suﬃciently	O
balanced	O
,	O
|	O
|	O
the	O
maximum	O
depth	O
(	O
number	O
of	O
binary	O
decisions	O
)	O
is	O
on	O
the	O
order	O
of	O
the	O
logarithm	O
of	O
|	O
|	O
the	O
number	O
of	O
words	O
v	O
words	O
can	O
be	O
obtained	O
by	O
doing	O
v	O
)	O
operations	O
(	O
one	O
for	O
each	O
of	O
the	O
nodes	O
on	O
the	O
path	O
from	O
the	O
root	O
)	O
.	O
in	O
this	O
example	O
,	O
o	O
(	O
log	O
computing	O
the	O
probability	O
of	O
a	O
word	O
y	O
can	O
be	O
done	O
by	O
multiplying	O
three	O
probabilities	O
,	O
associated	O
with	O
the	O
binary	O
decisions	O
to	O
move	O
left	O
or	O
right	O
at	O
each	O
node	O
on	O
the	O
path	O
from	O
the	O
root	O
to	O
a	O
node	O
y.	O
let	O
bi	O
(	O
y	O
)	O
be	O
the	O
i-th	O
binary	O
decision	O
when	O
traversing	O
the	O
tree	O
towards	O
the	O
value	O
y.	O
the	O
probability	O
of	O
sampling	O
an	O
output	O
y	O
decomposes	O
into	O
a	O
product	O
of	O
conditional	O
probabilities	O
,	O
using	O
the	O
chain	O
rule	O
for	O
conditional	O
probabilities	O
,	O
with	O
each	O
node	O
indexed	O
by	O
the	O
preﬁx	O
of	O
these	O
bits	O
.	O
for	O
example	O
,	O
node	O
(	O
1	O
,	O
0	O
)	O
corresponds	O
to	O
the	O
preﬁx	O
(	O
b0	O
(	O
w4	O
)	O
=	O
1	O
,	O
b1	O
(	O
w4	O
)	O
=	O
0	O
)	O
,	O
and	O
the	O
probability	O
of	O
w	O
4	O
can	O
be	O
decomposed	O
as	O
follows	O
:	O
|	O
|	O
v	O
:	O
the	O
choice	O
of	O
one	O
out	O
of	O
contains	O
the	O
classes	O
(	O
(	O
1	O
,	O
0	O
)	O
and	O
(	O
1	O
p	O
y	O
(	O
=	O
w	O
4	O
)	O
=	O
(	O
p	O
b0	O
=	O
1	O
,	O
b1	O
=	O
0	O
,	O
b2	O
=	O
0	O
)	O
|	O
=	O
(	O
p	O
b0	O
=	O
1	O
)	O
(	O
p	O
b1	O
=	O
0	O
|	O
b0	O
=	O
1	O
)	O
(	O
p	O
b	O
2	O
=	O
0	O
b0	O
=	O
1	O
,	O
b1	O
=	O
0	O
)	O
.	O
(	O
12.11	O
)	O
(	O
12.12	O
)	O
468	O
chapter	O
12.	O
applications	O
≤	O
practice	O
,	O
the	O
computational	O
savings	O
are	O
typically	O
not	O
worth	O
the	O
eﬀort	O
because	O
the	O
computation	O
of	O
the	O
output	O
probabilities	O
is	O
only	O
one	O
part	O
of	O
the	O
total	O
computation	O
in	O
the	O
neural	O
language	O
model	B
.	O
for	O
example	O
,	O
suppose	O
there	O
are	O
l	O
fully	O
connected	O
hidden	O
layers	O
of	O
width	O
nh	O
.	O
let	O
nb	O
be	O
the	O
weighted	O
average	O
of	O
the	O
number	O
of	O
bits	O
required	O
to	O
identify	O
a	O
word	O
,	O
with	O
the	O
weighting	O
given	O
by	O
the	O
frequency	O
of	O
these	O
words	O
.	O
in	O
this	O
example	O
,	O
the	O
number	O
of	O
operations	O
needed	O
to	O
compute	O
the	O
hidden	O
activations	O
grows	O
as	O
as	O
o	O
(	O
ln2	O
h	O
)	O
while	O
the	O
output	O
computations	O
grow	O
as	O
o	O
(	O
nhnb	O
)	O
.	O
as	O
long	O
as	O
nb	O
lnh	O
,	O
we	O
can	O
reduce	O
computation	O
more	O
by	O
shrinking	O
nh	O
than	O
by	O
shrinking	O
nb	O
.	O
indeed	O
,	O
nb	O
is	O
often	O
small	O
.	O
because	O
the	O
size	O
of	O
the	O
vocabulary	O
rarely	O
exceeds	O
a	O
million	O
words	O
and	O
log2	O
(	O
106	O
)	O
,20	O
but	O
nh	O
is	O
often	O
much	O
larger	O
,	O
around	O
103	O
or	O
more	O
.	O
rather	O
than	O
carefully	O
optimizing	O
|	O
|	O
a	O
tree	O
with	O
a	O
branching	O
factor	O
of	O
,	O
one	O
can	O
instead	O
deﬁne	O
a	O
tree	O
with	O
depth	O
two	O
and	O
a	O
branching	O
factor	O
of	O
v	O
.	O
such	O
a	O
tree	O
corresponds	O
to	O
simply	O
deﬁning	O
a	O
set	O
of	O
mutually	O
exclusive	O
word	O
classes	O
.	O
the	O
simple	O
approach	O
based	O
on	O
a	O
tree	O
of	O
depth	O
two	O
captures	O
most	O
of	O
the	O
computational	O
beneﬁt	O
of	O
the	O
hierarchical	O
strategy	O
.	O
20	O
,	O
it	O
is	O
possible	O
to	O
reduce	O
nb	O
to	O
about	O
	O
≈	O
2	O
morin	O
and	O
bengio	O
2005	O
one	O
question	O
that	O
remains	O
somewhat	O
open	O
is	O
how	O
to	O
best	O
deﬁne	O
these	O
word	O
classes	O
,	O
or	O
how	O
to	O
deﬁne	O
the	O
word	O
hierarchy	O
in	O
general	O
.	O
early	O
work	B
used	O
existing	O
hierarchies	O
(	O
)	O
but	O
the	O
hierarchy	O
can	O
also	O
be	O
learned	O
,	O
ideally	O
jointly	O
with	O
the	O
neural	O
language	O
model	B
.	O
learning	O
the	O
hierarchy	O
is	O
diﬃcult	O
.	O
an	O
exact	O
optimization	O
of	O
the	O
log-likelihood	O
appears	O
intractable	O
because	O
the	O
choice	O
of	O
a	O
word	O
hierarchy	O
is	O
a	O
discrete	O
one	O
,	O
not	O
amenable	O
to	O
gradient-based	O
optimization	O
.	O
however	O
,	O
one	O
could	O
use	O
discrete	O
optimization	O
to	O
approximately	O
optimize	O
the	O
partition	O
of	O
words	O
into	O
word	O
classes	O
.	O
,	O
an	O
important	O
advantage	O
of	O
the	O
hierarchical	O
softmax	O
is	O
that	O
it	O
brings	O
computa-	O
tional	O
beneﬁts	O
both	O
at	O
training	O
time	O
and	O
at	O
test	O
time	O
,	O
if	O
at	O
test	O
time	O
we	O
want	O
to	O
compute	O
the	O
probability	O
of	O
speciﬁc	O
words	O
.	O
|	O
|	O
v	O
words	O
will	O
remain	O
expensive	O
even	O
with	O
the	O
hierarchical	O
softmax	O
.	O
another	O
important	O
operation	O
is	O
selecting	O
the	O
most	O
likely	O
word	O
in	O
a	O
given	O
context	O
.	O
unfortunately	O
the	O
tree	O
structure	O
does	O
not	O
provide	O
an	O
eﬃcient	O
and	O
exact	O
solution	O
to	O
this	O
problem	O
.	O
of	O
course	O
,	O
computing	O
the	O
probability	O
of	O
all	O
a	O
disadvantage	O
is	O
that	O
in	O
practice	O
the	O
hierarchical	O
softmax	O
tends	O
to	O
give	O
worse	O
test	O
results	O
than	O
sampling-based	O
methods	O
we	O
will	O
describe	O
next	O
.	O
this	O
may	O
be	O
due	O
to	O
a	O
poor	O
choice	O
of	O
word	O
classes	O
.	O
12.4.3.3	O
importance	O
sampling	O
one	O
way	O
to	O
speed	O
up	O
the	O
training	O
of	O
neural	O
language	O
models	O
is	O
to	O
avoid	O
explicitly	O
computing	O
the	O
contribution	O
of	O
the	O
gradient	O
from	O
all	O
of	O
the	O
words	O
that	O
do	O
not	O
appear	O
469	O
chapter	O
12.	O
applications	O
in	O
the	O
next	O
position	B
.	O
every	O
incorrect	O
word	O
should	O
have	O
low	O
probability	O
under	O
the	O
model	B
.	O
it	O
can	O
be	O
computationally	O
costly	O
to	O
enumerate	O
all	O
of	O
these	O
words	O
.	O
instead	O
,	O
it	O
is	O
possible	O
to	O
sample	O
only	O
a	O
subset	O
of	O
the	O
words	O
.	O
using	O
the	O
notation	O
introduced	O
in	O
equation	O
,	O
the	O
gradient	O
can	O
be	O
written	O
as	O
follows	O
:	O
12.8	O
|	O
∂	O
p	O
y	O
c	O
log	O
(	O
∂θ	O
)	O
=	O
=	O
=	O
=	O
∂	O
∂θ	O
∂	O
∂θ	O
log	O
(	O
a	O
y	O
−	O
∂ay	O
∂θ	O
i	O
∂	O
log	O
softmaxy	O
(	O
)	O
a	O
	O
	O
	O
−	O
∂θ	O
eay	O
i	O
eai	O
log	O
i	O
ea	O
i	O
)	O
|	O
i	O
c	O
p	O
y	O
(	O
=	O
(	O
12.13	O
)	O
(	O
12.14	O
)	O
(	O
12.15	O
)	O
(	O
12.16	O
)	O
)	O
∂ai	O
∂θ	O
|	O
where	O
a	O
is	O
the	O
vector	O
of	O
pre-softmax	O
activations	O
(	O
or	O
scores	O
)	O
,	O
with	O
one	O
element	O
per	O
word	O
.	O
the	O
ﬁrst	O
term	O
is	O
the	O
positive	O
phase	O
term	O
(	O
pushing	O
ay	O
up	O
)	O
while	O
the	O
second	O
term	O
is	O
the	O
negative	O
phase	O
term	O
(	O
pushing	O
ai	O
down	O
for	O
all	O
i	O
,	O
with	O
weight	O
p	O
(	O
i	O
c	O
)	O
.	O
since	O
the	O
negative	O
phase	O
term	O
is	O
an	O
expectation	O
,	O
we	O
can	O
estimate	O
it	O
with	O
a	O
monte	O
carlo	O
sample	O
.	O
however	O
,	O
that	O
would	O
require	O
sampling	O
from	O
the	O
model	B
itself	O
.	O
sampling	O
from	O
the	O
model	B
requires	O
computing	O
p	O
(	O
i	O
c	O
)	O
for	O
all	O
i	O
in	O
the	O
vocabulary	O
,	O
which	O
is	O
precisely	O
what	O
we	O
are	O
trying	O
to	O
avoid	O
.	O
|	O
;	O
,	O
,	O
instead	O
of	O
sampling	O
from	O
the	O
model	B
,	O
one	O
can	O
sample	O
from	O
another	O
distribution	O
,	O
called	O
the	O
proposal	O
distribution	O
(	O
denoted	O
q	O
)	O
,	O
and	O
use	O
appropriate	O
weights	O
to	O
correct	O
for	O
the	O
bias	O
introduced	O
by	O
sampling	O
from	O
the	O
wrong	O
distribution	O
(	O
bengio	O
and	O
sénécal	O
2003	O
bengio	O
and	O
sénécal	O
2008	O
)	O
.	O
this	O
is	O
an	O
application	O
of	O
a	O
more	O
general	O
technique	O
called	O
importance	O
sampling	O
,	O
which	O
will	O
be	O
described	O
in	O
more	O
detail	O
.	O
unfortunately	O
,	O
even	O
exact	O
importance	O
sampling	O
is	O
not	O
eﬃcient	O
in	O
section	O
because	O
it	O
requires	O
computing	O
weights	O
pi/qi	O
,	O
where	O
pi	O
=	O
p	O
(	O
i	O
c	O
)	O
,	O
which	O
can	O
only	O
be	O
computed	O
if	O
all	O
the	O
scores	O
ai	O
are	O
computed	O
.	O
the	O
solution	O
adopted	O
for	O
this	O
application	O
is	O
called	O
biased	O
importance	O
sampling	O
,	O
where	O
the	O
importance	O
weights	O
are	O
normalized	O
to	O
sum	O
to	O
1.	O
when	O
negative	O
word	O
ni	O
is	O
sampled	O
,	O
the	O
associated	O
gradient	O
is	O
weighted	O
by	O
17.2	O
	O
|	O
wi	O
=	O
pni	O
/qn	O
i	O
n	O
j=1	O
pnj	O
/qnj	O
.	O
(	O
12.17	O
)	O
these	O
weights	O
are	O
used	O
to	O
give	O
the	O
appropriate	O
importance	O
to	O
the	O
m	O
negative	O
samples	O
from	O
q	O
used	O
to	O
form	O
the	O
estimated	O
negative	O
phase	O
contribution	O
to	O
the	O
470	O
chapter	O
12.	O
applications	O
	O
gradient	O
:	O
|	O
|	O
v	O
i=1	O
|	O
p	O
i	O
c	O
(	O
≈	O
1	O
m	O
)	O
∂a	O
i	O
∂θ	O
	O
m	O
i=1	O
wi	O
∂ani	O
∂θ	O
.	O
(	O
12.18	O
)	O
a	O
unigram	O
or	O
a	O
bigram	O
distribution	O
works	O
well	O
as	O
the	O
proposal	O
distribution	O
q	O
.	O
it	O
is	O
easy	O
to	O
estimate	O
the	O
parameters	O
of	O
such	O
a	O
distribution	O
from	O
data	O
.	O
after	O
estimating	O
the	O
parameters	O
,	O
it	O
is	O
also	O
possible	O
to	O
sample	O
from	O
such	O
a	O
distribution	O
very	O
eﬃciently	O
.	O
1	O
importance	O
sampling	O
is	O
not	O
only	O
useful	O
for	O
speeding	O
up	O
models	O
with	O
large	O
softmax	O
outputs	O
.	O
more	O
generally	O
,	O
it	O
is	O
useful	O
for	O
accelerating	O
training	O
with	O
large	O
n	O
sparse	O
output	O
layers	O
,	O
where	O
the	O
output	O
is	O
a	O
sparse	O
vector	O
rather	O
than	O
a	O
-of-	O
choice	O
.	O
an	O
example	O
is	O
a	O
bag	O
of	O
words	O
.	O
a	O
bag	O
of	O
words	O
is	O
a	O
sparse	O
vector	O
v	O
where	O
vi	O
indicates	O
the	O
presence	O
or	O
absence	O
of	O
word	O
i	O
from	O
the	O
vocabulary	O
in	O
the	O
document	O
.	O
alternately	O
,	O
vi	O
can	O
indicate	O
the	O
number	O
of	O
times	O
that	O
word	O
i	O
appears	O
.	O
machine	O
learning	O
models	O
that	O
emit	O
such	O
sparse	O
vectors	O
can	O
be	O
expensive	O
to	O
train	O
for	O
a	O
variety	O
of	O
reasons	O
.	O
early	O
in	O
learning	O
,	O
the	O
model	B
may	O
not	O
actually	O
choose	O
to	O
make	O
the	O
output	O
truly	O
sparse	O
.	O
moreover	O
,	O
the	O
loss	O
function	O
we	O
use	O
for	O
training	O
might	O
most	O
naturally	O
be	O
described	O
in	O
terms	O
of	O
comparing	O
every	O
element	O
of	O
the	O
output	O
to	O
every	O
element	O
of	O
the	O
target	O
.	O
this	O
means	O
that	O
it	O
is	O
not	O
always	O
clear	O
that	O
there	O
is	O
a	O
computational	O
beneﬁt	O
to	O
using	O
sparse	O
outputs	O
,	O
because	O
the	O
model	B
may	O
choose	O
to	O
make	O
the	O
majority	O
of	O
the	O
output	O
non-zero	O
and	O
all	O
of	O
these	O
non-zero	O
values	O
need	O
to	O
be	O
compared	O
to	O
the	O
corresponding	O
training	O
target	O
,	O
even	O
if	O
the	O
training	O
target	O
is	O
zero	O
.	O
dauphin	O
)	O
demonstrated	O
that	O
such	O
models	O
can	O
be	O
accelerated	O
using	O
importance	O
sampling	O
.	O
the	O
eﬃcient	O
algorithm	O
minimizes	O
the	O
loss	O
reconstruction	O
for	O
the	O
“	O
positive	O
words	O
”	O
(	O
those	O
that	O
are	O
non-zero	O
in	O
the	O
target	O
)	O
and	O
an	O
equal	O
number	O
of	O
“	O
negative	O
words.	O
”	O
the	O
negative	O
words	O
are	O
chosen	O
randomly	O
,	O
using	O
a	O
heuristic	O
to	O
sample	O
words	O
that	O
are	O
more	O
likely	O
to	O
be	O
mistaken	O
.	O
the	O
bias	O
introduced	O
by	O
this	O
heuristic	O
oversampling	O
can	O
then	O
be	O
corrected	O
using	O
importance	O
weights	O
.	O
et	O
al	O
.	O
(	O
2011	O
in	O
all	O
of	O
these	O
cases	O
,	O
the	O
computational	O
complexity	O
of	O
gradient	O
estimation	O
for	O
the	O
output	O
layer	O
is	O
reduced	O
to	O
be	O
proportional	O
to	O
the	O
number	O
of	O
negative	O
samples	O
rather	O
than	O
proportional	O
to	O
the	O
size	O
of	O
the	O
output	O
vector	O
.	O
12.4.3.4	O
noise-contrastive	O
estimation	O
and	O
ranking	O
loss	O
other	O
approaches	O
based	O
on	O
sampling	O
have	O
been	O
proposed	O
to	O
reduce	O
the	O
computa-	O
tional	O
cost	O
of	O
training	O
neural	O
language	O
models	O
with	O
large	O
vocabularies	O
.	O
an	O
early	O
example	O
is	O
the	O
ranking	O
loss	O
proposed	O
by	O
collobert	O
and	O
weston	O
2008a	O
)	O
,	O
which	O
views	O
the	O
output	O
of	O
the	O
neural	O
language	O
model	B
for	O
each	O
word	O
as	O
a	O
score	O
and	O
tries	O
to	O
make	O
the	O
score	O
of	O
the	O
correct	O
word	O
ay	O
be	O
ranked	O
high	O
in	O
comparison	O
to	O
the	O
other	O
(	O
471	O
chapter	O
12.	O
applications	O
	O
scores	O
ai	O
.	O
the	O
ranking	O
loss	O
proposed	O
then	O
is	O
−	O
l	O
=	O
max	O
(	O
0	O
1	O
,	O
ay	O
+	O
ai	O
)	O
.	O
(	O
12.19	O
)	O
i	O
the	O
gradient	O
is	O
zero	O
for	O
the	O
i-th	O
term	O
if	O
the	O
score	O
of	O
the	O
observed	O
word	O
,	O
a	O
y	O
,	O
is	O
greater	O
than	O
the	O
score	O
of	O
the	O
negative	O
word	O
ai	O
by	O
a	O
margin	O
of	O
1.	O
one	O
issue	O
with	O
this	O
criterion	O
is	O
that	O
it	O
does	O
not	O
provide	O
estimated	O
conditional	O
probabilities	O
,	O
which	O
are	O
useful	O
in	O
some	O
applications	O
,	O
including	O
speech	O
recognition	B
and	O
text	O
generation	O
(	O
including	O
conditional	O
text	O
generation	O
tasks	O
such	O
as	O
translation	O
)	O
.	O
a	O
more	O
recently	O
used	O
training	O
objective	O
for	O
neural	O
language	O
model	B
is	O
noise-	O
contrastive	O
estimation	O
,	O
which	O
is	O
introduced	O
in	O
section	O
.	O
this	O
approach	O
has	O
been	O
successfully	O
applied	O
to	O
neural	O
language	O
models	O
(	O
mnih	O
and	O
teh	O
2012	O
mnih	O
and	O
kavukcuoglu	O
2013	O
18.6	O
)	O
.	O
,	O
;	O
,	O
12.4.4	O
combining	O
neural	O
language	O
models	O
with	O
-grams	O
n	O
a	O
major	O
advantage	O
of	O
n-gram	B
models	O
over	O
neural	O
networks	O
is	O
that	O
n-gram	B
models	O
achieve	O
high	O
model	B
capacity	O
(	O
by	O
storing	O
the	O
frequencies	O
of	O
very	O
many	O
tuples	O
)	O
while	O
requiring	O
very	O
little	O
computation	O
to	O
process	O
an	O
example	O
(	O
by	O
looking	O
up	O
only	O
a	O
few	O
tuples	O
that	O
match	O
the	O
current	O
context	O
)	O
.	O
if	O
we	O
use	O
hash	O
tables	O
or	O
trees	O
to	O
access	O
the	O
counts	O
,	O
the	O
computation	O
used	O
for	O
n-grams	O
is	O
almost	O
independent	O
of	O
capacity	O
.	O
in	O
comparison	O
,	O
doubling	O
a	O
neural	O
network	O
’	O
s	O
number	O
of	O
parameters	O
typically	O
also	O
roughly	O
doubles	O
its	O
computation	O
time	O
.	O
exceptions	O
include	O
models	O
that	O
avoid	O
using	O
all	O
parameters	O
on	O
each	O
pass	O
.	O
embedding	O
layers	O
index	O
only	O
a	O
single	O
embedding	O
in	O
each	O
pass	O
,	O
so	O
we	O
can	O
increase	O
the	O
vocabulary	O
size	O
without	O
increasing	O
the	O
computation	O
time	O
per	O
example	O
.	O
some	O
other	O
models	O
,	O
such	O
as	O
tiled	O
convolutional	O
networks	O
,	O
can	O
add	O
parameters	O
while	O
reducing	O
the	O
degree	O
of	O
parameter	O
sharing	O
in	O
order	O
to	O
maintain	O
the	O
same	O
amount	O
of	O
computation	O
.	O
however	O
,	O
typical	O
neural	O
network	O
layers	O
based	O
on	O
matrix	O
multiplication	O
use	O
an	O
amount	O
of	O
computation	O
proportional	O
to	O
the	O
number	O
of	O
parameters	O
.	O
,	O
2001	O
2003	O
one	O
easy	O
way	O
to	O
add	O
capacity	O
is	O
thus	O
to	O
combine	O
both	O
approaches	O
in	O
an	O
ensemble	O
consisting	O
of	O
a	O
neural	O
language	O
model	B
and	O
an	O
n-gram	B
language	O
model	B
(	O
bengio	O
et	O
al.	O
,	O
)	O
.	O
as	O
with	O
any	O
ensemble	O
,	O
this	O
technique	O
can	O
reduce	O
test	O
error	O
if	O
the	O
ensemble	O
members	O
make	O
independent	O
mistakes	O
.	O
the	O
ﬁeld	O
of	O
ensemble	O
learning	O
provides	O
many	O
ways	O
of	O
combining	O
the	O
ensemble	O
members	O
’	O
predictions	O
,	O
including	O
uniform	O
weighting	O
and	O
weights	O
chosen	O
on	O
a	O
validation	O
set	O
.	O
mikolov	O
2011a	O
)	O
extended	O
the	O
ensemble	O
to	O
include	O
not	O
just	O
two	O
models	O
but	O
a	O
large	O
array	O
of	O
models	O
.	O
it	O
is	O
also	O
possible	O
to	O
pair	O
a	O
neural	O
network	O
with	O
a	O
maximum	O
entropy	O
model	B
and	O
)	O
.	O
this	O
approach	O
can	O
be	O
viewed	O
as	O
training	O
train	O
both	O
jointly	O
(	O
mikolov	O
et	O
al	O
.	O
(	O
2011b	O
et	O
al.	O
,	O
472	O
chapter	O
12.	O
applications	O
a	O
neural	O
network	O
with	O
an	O
extra	O
set	O
of	O
inputs	O
that	O
are	O
connected	O
directly	O
to	O
the	O
output	O
,	O
and	O
not	O
connected	O
to	O
any	O
other	O
part	O
of	O
the	O
model	B
.	O
the	O
extra	O
inputs	O
are	O
indicators	O
for	O
the	O
presence	O
of	O
particular	O
n-grams	O
in	O
the	O
input	O
context	O
,	O
so	O
these	O
|	O
variables	O
are	O
very	O
high-dimensional	O
and	O
very	O
sparse	O
.	O
the	O
increase	O
in	O
model	B
capacity	O
sv	O
n	O
parameters—but	O
is	O
huge—the	O
new	O
portion	O
of	O
the	O
architecture	O
contains	O
up	O
to	O
the	O
amount	O
of	O
added	O
computation	O
needed	O
to	O
process	O
an	O
input	O
is	O
minimal	O
because	O
the	O
extra	O
inputs	O
are	O
very	O
sparse	O
.	O
|	O
12.4.5	O
neural	O
machine	O
translation	O
machine	O
translation	O
is	O
the	O
task	O
of	O
reading	O
a	O
sentence	O
in	O
one	O
natural	O
language	O
and	O
emitting	O
a	O
sentence	O
with	O
the	O
equivalent	O
meaning	O
in	O
another	O
language	O
.	O
machine	O
translation	O
systems	O
often	O
involve	O
many	O
components	O
.	O
at	O
a	O
high	O
level	O
,	O
there	O
is	O
often	O
one	O
component	O
that	O
proposes	O
many	O
candidate	O
translations	O
.	O
many	O
of	O
these	O
translations	O
will	O
not	O
be	O
grammatical	O
due	O
to	O
diﬀerences	O
between	O
the	O
languages	O
.	O
for	O
example	O
,	O
many	O
languages	O
put	O
adjectives	O
after	O
nouns	O
,	O
so	O
when	O
translated	O
to	O
english	O
directly	O
they	O
yield	O
phrases	O
such	O
as	O
“	O
apple	O
red.	O
”	O
the	O
proposal	O
mechanism	O
suggests	O
many	O
variants	O
of	O
the	O
suggested	O
translation	O
,	O
ideally	O
including	O
“	O
red	O
apple.	O
”	O
a	O
second	O
component	O
of	O
the	O
translation	O
system	O
,	O
a	O
language	O
model	B
,	O
evaluates	O
the	O
proposed	O
translations	O
,	O
and	O
can	O
score	O
“	O
red	O
apple	O
”	O
as	O
better	O
than	O
“	O
apple	O
red.	O
”	O
,	O
;	O
2006	O
schwenk	O
2010	O
the	O
earliest	O
use	O
of	O
neural	O
networks	O
for	O
machine	O
translation	O
was	O
to	O
upgrade	O
the	O
language	O
model	B
of	O
a	O
translation	O
system	O
by	O
using	O
a	O
neural	O
language	O
model	B
(	O
schwenk	O
et	O
al.	O
,	O
)	O
.	O
previously	O
,	O
most	O
machine	O
translation	O
systems	O
had	O
used	O
an	O
n-gram	B
model	O
for	O
this	O
component	O
.	O
the	O
n-gram	B
based	O
models	O
used	O
for	O
machine	O
translation	O
include	O
not	O
just	O
traditional	O
back-oﬀ	O
n-gram	B
models	O
(	O
jelinek	O
and	O
mercer	O
1980	O
katz	O
1987	O
chen	O
and	O
goodman	O
1999	O
)	O
but	O
also	O
maximum	O
entropy	O
language	O
models	O
(	O
)	O
,	O
in	O
which	O
an	O
aﬃne-softmax	O
layer	O
predicts	O
the	O
next	O
word	O
given	O
the	O
presence	O
of	O
frequent	O
;	O
berger	O
et	O
al	O
.	O
1996	O
-grams	O
in	O
the	O
context	O
.	O
n	O
,	O
,	O
;	O
,	O
,	O
traditional	O
language	O
models	O
simply	O
report	O
the	O
probability	O
of	O
a	O
natural	O
language	O
sentence	O
.	O
because	O
machine	O
translation	O
involves	O
producing	O
an	O
output	O
sentence	O
given	O
an	O
input	O
sentence	O
,	O
it	O
makes	O
sense	O
to	O
extend	O
the	O
natural	O
language	O
model	B
to	O
be	O
conditional	O
.	O
as	O
described	O
in	O
section	O
,	O
it	O
is	O
straightforward	O
to	O
extend	O
a	O
model	B
that	O
deﬁnes	O
a	O
marginal	O
distribution	O
over	O
some	O
variable	O
to	O
deﬁne	O
a	O
conditional	O
distribution	O
over	O
that	O
variable	O
given	O
a	O
context	O
c	O
,	O
where	O
c	O
might	O
be	O
a	O
single	O
variable	O
or	O
a	O
list	O
of	O
variables.	O
)	O
beat	O
the	O
state-of-the-art	O
in	O
some	O
statistical	O
machine	O
translation	O
benchmarks	O
by	O
using	O
an	O
mlp	O
to	O
score	O
a	O
phrase	O
t1	O
,	O
t2	O
,	O
.	O
.	O
.	O
,	O
tk	O
in	O
the	O
target	O
language	O
given	O
a	O
phrase	O
s1	O
,	O
s2	O
,	O
.	O
.	O
.	O
,	O
s	O
n	O
in	O
the	O
source	O
language	O
.	O
the	O
mlp	O
estimates	O
p	O
(	O
t1	O
,	O
t2	O
,	O
.	O
.	O
.	O
,	O
tk	O
s1	O
,	O
s2	O
,	O
.	O
.	O
.	O
,	O
sn	O
)	O
.	O
the	O
estimate	O
formed	O
by	O
this	O
mlp	O
replaces	O
the	O
estimate	O
provided	O
by	O
conditional	O
devlin	O
et	O
al	O
.	O
2014	O
-gram	O
models	O
.	O
6.2.1.1	O
n	O
|	O
(	O
473	O
chapter	O
12.	O
applications	O
output	O
object	O
(	O
english	O
sentence	O
)	O
decoder	O
intermediate	O
,	O
semantic	O
representation	O
encoder	O
source	O
object	O
(	O
french	O
sentence	O
or	O
image	O
)	O
figure	O
12.5	O
:	O
the	O
encoder-decoder	O
architecture	O
to	O
map	O
back	O
and	O
forth	O
between	O
a	O
surface	O
representation	O
(	O
such	O
as	O
a	O
sequence	O
of	O
words	O
or	O
an	O
image	O
)	O
and	O
a	O
semantic	O
representation	O
.	O
by	O
using	O
the	O
output	O
of	O
an	O
encoder	O
of	O
data	O
from	O
one	O
modality	O
(	O
such	O
as	O
the	O
encoder	O
mapping	O
from	O
french	O
sentences	O
to	O
hidden	O
representations	O
capturing	O
the	O
meaning	O
of	O
sentences	O
)	O
as	O
the	O
input	O
to	O
a	O
decoder	O
for	O
another	O
modality	O
(	O
such	O
as	O
the	O
decoder	O
mapping	O
from	O
hidden	O
representations	O
capturing	O
the	O
meaning	O
of	O
sentences	O
to	O
english	O
)	O
,	O
we	O
can	O
train	O
systems	O
that	O
translate	O
from	O
one	O
modality	O
to	O
another	O
.	O
this	O
idea	O
has	O
been	O
applied	O
successfully	O
not	O
just	O
to	O
machine	O
translation	O
but	O
also	O
to	O
caption	O
generation	O
from	O
images	O
.	O
10.2.4	O
a	O
drawback	O
of	O
the	O
mlp-based	O
approach	O
is	O
that	O
it	O
requires	O
the	O
sequences	O
to	O
be	O
preprocessed	O
to	O
be	O
of	O
ﬁxed	O
length	O
.	O
to	O
make	O
the	O
translation	O
more	O
ﬂexible	O
,	O
we	O
would	O
like	O
to	O
use	O
a	O
model	B
that	O
can	O
accommodate	O
variable	O
length	O
inputs	O
and	O
variable	O
length	O
outputs	O
.	O
an	O
rnn	O
provides	O
this	O
ability	O
.	O
section	O
describes	O
several	O
ways	O
of	O
constructing	O
an	O
rnn	O
that	O
represents	O
a	O
conditional	O
distribution	O
over	O
a	O
sequence	O
given	O
some	O
input	O
,	O
and	O
section	O
describes	O
how	O
to	O
accomplish	O
this	O
conditioning	O
when	O
the	O
input	O
is	O
a	O
sequence	O
.	O
in	O
all	O
cases	O
,	O
one	O
model	B
ﬁrst	O
reads	O
the	O
input	O
sequence	O
and	O
emits	O
a	O
data	O
structure	O
that	O
summarizes	O
the	O
input	O
sequence	O
.	O
we	O
call	O
this	O
summary	O
the	O
“	O
context	O
”	O
c	O
.	O
the	O
context	O
c	O
may	O
be	O
a	O
list	O
of	O
vectors	O
,	O
or	O
it	O
may	O
be	O
a	O
vector	O
or	O
tensor	O
.	O
the	O
model	B
that	O
reads	O
the	O
input	O
to	O
produce	O
c	O
may	O
be	O
an	O
rnn	O
(	O
;	O
cho	O
et	O
al	O
.	O
2014a	O
sutskever	O
)	O
or	O
a	O
convolutional	O
network	O
(	O
kalchbrenner	O
and	O
blunsom	O
2013	O
)	O
.	O
a	O
second	O
model	B
,	O
usually	O
an	O
rnn	O
,	O
then	O
reads	O
the	O
context	O
c	O
and	O
generates	O
a	O
sentence	O
in	O
the	O
target	O
language	O
.	O
this	O
general	O
idea	O
of	O
an	O
encoder-decoder	O
framework	O
for	O
machine	O
translation	O
is	O
illustrated	O
in	O
ﬁgure	O
2014	O
jean	O
,	O
et	O
al.	O
,	O
et	O
al.	O
,	O
2014	O
10.4	O
,	O
;	O
.	O
12.5	O
in	O
order	O
to	O
generate	O
an	O
entire	O
sentence	O
conditioned	O
on	O
the	O
source	O
sentence	O
,	O
the	O
model	B
must	O
have	O
a	O
way	O
to	O
represent	O
the	O
entire	O
source	O
sentence	O
.	O
earlier	O
models	O
were	O
only	O
able	O
to	O
represent	O
individual	O
words	O
or	O
phrases	O
.	O
from	O
a	O
representation	O
474	O
chapter	O
12.	O
applications	O
learning	O
point	O
of	O
view	O
,	O
it	O
can	O
be	O
useful	O
to	O
learn	O
a	O
representation	O
in	O
which	O
sentences	O
that	O
have	O
the	O
same	O
meaning	O
have	O
similar	O
representations	O
regardless	O
of	O
whether	O
they	O
were	O
written	O
in	O
the	O
source	O
language	O
or	O
the	O
target	O
language	O
.	O
this	O
strategy	O
was	O
explored	O
ﬁrst	O
using	O
a	O
combination	O
of	O
convolutions	O
and	O
rnns	O
(	O
kalchbrenner	O
and	O
)	O
.	O
later	O
work	B
introduced	O
the	O
use	O
of	O
an	O
rnn	O
for	O
scoring	O
proposed	O
blunsom	O
2013	O
translations	O
(	O
sutskever	O
et	O
al	O
.	O
)	O
.	O
,	O
)	O
and	O
for	O
generating	O
translated	O
sentences	O
(	O
)	O
scaled	O
these	O
models	O
to	O
larger	O
vocabularies	O
.	O
cho	O
et	O
al	O
.	O
2014a	O
(	O
2014	O
jean	O
,	O
et	O
al	O
.	O
2014	O
,	O
12.4.5.1	O
using	O
an	O
attention	O
mechanism	O
and	O
aligning	O
pieces	O
of	O
data	O
cc	O
+	O
α	O
(	O
)	O
tα	O
(	O
)	O
t	O
××	O
t	O
α	O
(	O
+1	O
)	O
tα	O
(	O
+1	O
)	O
××	O
h	O
(	O
)	O
th	O
(	O
)	O
t	O
t	O
h	O
(	O
+1	O
)	O
th	O
(	O
+1	O
)	O
−	O
−	O
α	O
(	O
t	O
α	O
(	O
t	O
1	O
)	O
1	O
)	O
××	O
−	O
−	O
h	O
(	O
t	O
h	O
(	O
t	O
1	O
)	O
1	O
)	O
(	O
bahdanau	O
et	O
al	O
.	O
2015	O
figure	O
12.6	O
:	O
a	O
modern	O
attention	O
mechanism	O
,	O
as	O
introduced	O
by	O
)	O
,	O
is	O
essentially	O
a	O
weighted	O
average	O
.	O
a	O
context	O
vector	O
c	O
is	O
formed	O
by	O
taking	O
a	O
weighted	O
average	O
of	O
feature	O
vectors	O
h	O
(	O
)	O
t	O
with	O
weights	O
α	O
(	O
)	O
t	O
.	O
in	O
some	O
applications	O
,	O
the	O
feature	O
vectors	O
h	O
are	O
hidden	O
units	O
of	O
a	O
neural	O
network	O
,	O
but	O
they	O
may	O
also	O
be	O
raw	O
input	O
to	O
the	O
model	B
.	O
the	O
weights	O
α	O
(	O
)	O
t	O
are	O
produced	O
by	O
the	O
model	B
itself	O
.	O
they	O
are	O
usually	O
values	O
in	O
the	O
interval	O
[	O
0	O
,	O
1	O
]	O
and	O
are	O
intended	O
to	O
concentrate	O
around	O
just	O
one	O
h	O
(	O
)	O
t	O
so	O
that	O
the	O
weighted	O
average	O
approximates	O
reading	O
that	O
one	O
speciﬁc	O
time	O
step	O
precisely	O
.	O
the	O
weights	O
α	O
(	O
)	O
t	O
are	O
usually	O
produced	O
by	O
applying	O
a	O
softmax	O
function	O
to	O
relevance	O
scores	O
emitted	O
by	O
another	O
portion	O
of	O
the	O
model	B
.	O
the	O
attention	O
mechanism	O
is	O
more	O
expensive	O
computationally	O
than	O
directly	O
indexing	O
the	O
desired	O
h	O
(	O
)	O
t	O
,	O
but	O
direct	O
indexing	O
can	O
not	O
be	O
trained	O
with	O
gradient	O
descent	B
.	O
the	O
attention	O
mechanism	O
based	O
on	O
weighted	O
averages	O
is	O
a	O
smooth	O
,	O
diﬀerentiable	O
approximation	O
that	O
can	O
be	O
trained	O
with	O
existing	O
optimization	O
algorithms	O
.	O
using	O
a	O
ﬁxed-size	O
representation	O
to	O
capture	O
all	O
the	O
semantic	O
details	O
of	O
a	O
very	O
long	O
sentence	O
of	O
say	O
60	O
words	O
is	O
very	O
diﬃcult	O
.	O
it	O
can	O
be	O
achieved	O
by	O
training	O
a	O
suﬃciently	O
large	O
rnn	O
well	O
enough	O
and	O
for	O
long	O
enough	O
,	O
as	O
demonstrated	O
by	O
cho	O
et	O
al	O
.	O
(	O
)	O
.	O
however	O
,	O
a	O
more	O
eﬃcient	O
approach	O
is	O
to	O
read	O
the	O
whole	O
sentence	O
or	O
paragraph	O
(	O
to	O
get	O
the	O
context	O
and	O
the	O
gist	O
of	O
what	O
sutskever	O
et	O
al	O
.	O
(	O
2014a	O
)	O
and	O
2014	O
475	O
chapter	O
12.	O
applications	O
is	O
being	O
expressed	O
)	O
,	O
then	O
produce	O
the	O
translated	O
words	O
one	O
at	O
a	O
time	O
,	O
each	O
time	O
focusing	O
on	O
a	O
diﬀerent	O
part	O
of	O
the	O
input	O
sentence	O
in	O
order	O
to	O
gather	O
the	O
semantic	O
details	O
that	O
are	O
required	O
to	O
produce	O
the	O
next	O
output	O
word	O
.	O
that	O
is	O
exactly	O
the	O
idea	O
that	O
)	O
ﬁrst	O
introduced	O
.	O
the	O
attention	O
mechanism	O
used	O
to	O
focus	O
on	O
speciﬁc	O
parts	O
of	O
the	O
input	O
sequence	O
at	O
each	O
time	O
step	O
is	O
illustrated	O
in	O
ﬁgure	O
bahdanau	O
et	O
al	O
.	O
2015	O
.	O
12.6	O
(	O
we	O
can	O
think	O
of	O
an	O
attention-based	O
system	O
as	O
having	O
three	O
components	O
:	O
1.	O
a	O
process	O
that	O
“	O
reads	O
”	O
raw	O
data	O
(	O
such	O
as	O
source	O
words	O
in	O
a	O
source	O
sentence	O
)	O
,	O
and	O
converts	O
them	O
into	O
distributed	O
representations	O
,	O
with	O
one	O
feature	O
vector	O
associated	O
with	O
each	O
word	O
position	B
.	O
2.	O
a	O
list	O
of	O
feature	O
vectors	O
storing	O
the	O
output	O
of	O
the	O
reader	O
.	O
this	O
can	O
be	O
”	O
containing	O
a	O
sequence	O
of	O
facts	O
,	O
which	O
can	O
be	O
understood	O
as	O
a	O
“	O
retrieved	O
later	O
,	O
not	O
necessarily	O
in	O
the	O
same	O
order	O
,	O
without	O
having	O
to	O
visit	O
all	O
of	O
them	O
.	O
memory	O
3.	O
a	O
process	O
that	O
“	O
”	O
the	O
content	O
of	O
the	O
memory	O
to	O
sequentially	O
perform	O
a	O
task	O
,	O
at	O
each	O
time	O
step	O
having	O
the	O
ability	O
put	O
attention	O
on	O
the	O
content	O
of	O
one	O
memory	O
element	O
(	O
or	O
a	O
few	O
,	O
with	O
a	O
diﬀerent	O
weight	O
)	O
.	O
exploits	O
the	O
third	O
component	O
generates	O
the	O
translated	O
sentence	O
.	O
when	O
words	O
in	O
a	O
sentence	O
written	O
in	O
one	O
language	O
are	O
aligned	O
with	O
correspond-	O
ing	O
words	O
in	O
a	O
translated	O
sentence	O
in	O
another	O
language	O
,	O
it	O
becomes	O
possible	O
to	O
relate	O
the	O
corresponding	O
word	O
embeddings	O
.	O
earlier	O
work	B
showed	O
that	O
one	O
could	O
learn	O
a	O
kind	O
of	O
translation	O
matrix	O
relating	O
the	O
word	O
embeddings	O
in	O
one	O
language	O
with	O
the	O
word	O
embeddings	O
in	O
another	O
(	O
kočiský	O
)	O
,	O
yielding	O
lower	O
alignment	O
error	O
rates	O
than	O
traditional	O
approaches	O
based	O
on	O
the	O
frequency	O
counts	O
in	O
the	O
phrase	O
table	O
.	O
there	O
is	O
even	O
earlier	O
work	B
on	O
learning	O
cross-lingual	O
word	O
vectors	O
(	O
klementiev	O
et	O
al.	O
,	O
2012	O
)	O
.	O
many	O
extensions	O
to	O
this	O
approach	O
are	O
possible	O
.	O
for	O
example	O
,	O
more	O
eﬃcient	O
cross-lingual	O
alignment	O
(	O
)	O
allows	O
training	O
on	O
larger	O
datasets	O
.	O
gouws	O
et	O
al	O
.	O
2014	O
et	O
al.	O
,	O
2014	O
,	O
12.4.6	O
historical	O
perspective	O
1986a	O
the	O
idea	O
of	O
distributed	O
representations	O
for	O
symbols	O
was	O
introduced	O
by	O
rumelhart	O
et	O
al	O
.	O
(	O
)	O
in	O
one	O
of	O
the	O
ﬁrst	O
explorations	O
of	O
back-propagation	O
,	O
with	O
symbols	O
corresponding	O
to	O
the	O
identity	O
of	O
family	O
members	O
and	O
the	O
neural	O
network	O
capturing	O
the	O
relationships	O
between	O
family	O
members	O
,	O
with	O
training	O
examples	O
forming	O
triplets	O
such	O
as	O
(	O
colin	O
,	O
mother	O
,	O
victoria	O
)	O
.	O
the	O
ﬁrst	O
layer	O
of	O
the	O
neural	O
network	O
learned	O
a	O
representation	O
of	O
each	O
family	O
member	O
.	O
for	O
example	O
,	O
the	O
features	O
for	O
colin	O
476	O
chapter	O
12.	O
applications	O
might	O
represent	O
which	O
family	O
tree	O
colin	O
was	O
in	O
,	O
what	O
branch	O
of	O
that	O
tree	O
he	O
was	O
in	O
,	O
what	O
generation	O
he	O
was	O
from	O
,	O
etc	O
.	O
one	O
can	O
think	O
of	O
the	O
neural	O
network	O
as	O
computing	O
learned	O
rules	O
relating	O
these	O
attributes	O
together	O
in	O
order	O
to	O
obtain	O
the	O
desired	O
predictions	O
.	O
the	O
model	B
can	O
then	O
make	O
predictions	O
such	O
as	O
inferring	O
who	O
is	O
the	O
mother	O
of	O
colin	O
.	O
the	O
idea	O
of	O
forming	O
an	O
embedding	O
for	O
a	O
symbol	O
was	O
extended	O
to	O
the	O
idea	O
of	O
an	O
)	O
.	O
these	O
embeddings	O
were	O
learned	O
embedding	O
for	O
a	O
word	O
by	O
deerwester	O
using	O
the	O
svd	O
.	O
later	O
,	O
embeddings	O
would	O
be	O
learned	O
by	O
neural	O
networks	O
.	O
et	O
al	O
.	O
(	O
1990	O
the	O
history	O
of	O
natural	O
language	O
processing	O
is	O
marked	O
by	O
transitions	O
in	O
the	O
popularity	O
of	O
diﬀerent	O
ways	O
of	O
representing	O
the	O
input	O
to	O
the	O
model	B
.	O
following	O
this	O
early	O
work	B
on	O
symbols	O
or	O
words	O
,	O
some	O
of	O
the	O
earliest	O
applications	O
of	O
neural	O
networks	O
to	O
nlp	O
(	O
)	O
represented	O
the	O
input	O
as	O
a	O
sequence	O
of	O
characters	O
.	O
miikkulainen	O
and	O
dyer	O
1991	O
schmidhuber	O
1996	O
,	O
;	O
,	O
2001	O
bengio	O
et	O
al	O
.	O
(	O
)	O
returned	O
the	O
focus	O
to	O
modeling	O
words	O
and	O
introduced	O
neural	O
language	O
models	O
,	O
which	O
produce	O
interpretable	O
word	O
embeddings	O
.	O
these	O
neural	O
models	O
have	O
scaled	O
up	O
from	O
deﬁning	O
representations	O
of	O
a	O
small	O
set	O
of	O
symbols	O
in	O
the	O
1980s	O
to	O
millions	O
of	O
words	O
(	O
including	O
proper	O
nouns	O
and	O
misspellings	O
)	O
in	O
modern	O
applications	O
.	O
this	O
computational	O
scaling	O
eﬀort	O
led	O
to	O
the	O
invention	O
of	O
the	O
techniques	O
described	O
above	O
in	O
section	O
12.4.3	O
.	O
initially	O
,	O
the	O
use	O
of	O
words	O
as	O
the	O
fundamental	O
units	O
of	O
language	O
models	O
yielded	O
improved	O
language	O
modeling	O
performance	O
(	O
)	O
.	O
to	O
this	O
day	O
,	O
new	O
techniques	O
continually	O
push	O
both	O
character-based	O
models	O
(	O
sutskever	O
et	O
al.	O
,	O
2011	O
)	O
and	O
word-based	O
models	O
forward	O
,	O
with	O
recent	O
work	B
(	O
)	O
even	O
modeling	O
individual	O
bytes	O
of	O
unicode	O
characters	O
.	O
bengio	O
et	O
al	O
.	O
2001	O
gillick	O
et	O
al	O
.	O
2015	O
,	O
,	O
the	O
ideas	O
behind	O
neural	O
language	O
models	O
have	O
been	O
extended	O
into	O
several	O
henderson	O
2003	O
2004	O
;	O
natural	O
language	O
processing	O
applications	O
,	O
such	O
as	O
parsing	O
(	O
collobert	O
2011	O
)	O
,	O
part-of-speech	O
tagging	O
,	O
semantic	O
role	O
labeling	O
,	O
chunking	O
,	O
etc	O
,	O
sometimes	O
using	O
a	O
single	O
multi-task	O
learning	O
architecture	O
(	O
collobert	O
and	O
weston	O
,	O
2008a	O
collobert	O
)	O
in	O
which	O
the	O
word	O
embeddings	O
are	O
shared	O
across	O
tasks	O
.	O
et	O
al.	O
,	O
2011a	O
,	O
;	O
,	O
,	O
two-dimensional	O
visualizations	O
of	O
embeddings	O
became	O
a	O
popular	O
tool	O
for	O
an-	O
alyzing	O
language	O
models	O
following	O
the	O
development	O
of	O
the	O
t-sne	O
dimensionality	O
reduction	O
algorithm	O
(	O
van	O
der	O
maaten	O
and	O
hinton	O
2008	O
)	O
and	O
its	O
high-proﬁle	O
appli-	O
cation	O
to	O
visualization	O
word	O
embeddings	O
by	O
joseph	O
turian	O
in	O
2009.	O
,	O
477	O
chapter	O
12.	O
applications	O
12.5	O
other	O
applications	O
in	O
this	O
section	O
we	O
cover	O
a	O
few	O
other	O
types	O
of	O
applications	O
of	O
deep	O
learning	O
that	O
are	O
diﬀerent	O
from	O
the	O
standard	O
object	O
recognition	B
,	O
speech	O
recognition	B
and	O
natural	O
language	O
processing	O
tasks	O
discussed	O
above	O
.	O
part	O
of	O
this	O
book	O
will	O
expand	O
that	O
scope	O
even	O
further	O
to	O
tasks	O
that	O
remain	O
primarily	O
research	O
areas	O
.	O
iii	O
12.5.1	O
recommender	O
systems	O
one	O
of	O
the	O
major	O
families	O
of	O
applications	O
of	O
machine	O
learning	O
in	O
the	O
information	O
technology	O
sector	O
is	O
the	O
ability	O
to	O
make	O
recommendations	O
of	O
items	O
to	O
potential	O
users	O
or	O
customers	O
.	O
two	O
major	O
types	O
of	O
applications	O
can	O
be	O
distinguished	O
:	O
online	O
advertising	O
and	O
item	O
recommendations	O
(	O
often	O
these	O
recommendations	O
are	O
still	O
for	O
the	O
purpose	O
of	O
selling	O
a	O
product	O
)	O
.	O
both	O
rely	O
on	O
predicting	O
the	O
association	O
between	O
a	O
user	O
and	O
an	O
item	O
,	O
either	O
to	O
predict	O
the	O
probability	O
of	O
some	O
action	O
(	O
the	O
user	O
buying	O
the	O
product	O
,	O
or	O
some	O
proxy	O
for	O
this	O
action	O
)	O
or	O
the	O
expected	O
gain	O
(	O
which	O
may	O
depend	O
on	O
the	O
value	O
of	O
the	O
product	O
)	O
if	O
an	O
ad	O
is	O
shown	O
or	O
a	O
recommendation	O
is	O
made	O
regarding	O
that	O
product	O
to	O
that	O
user	O
.	O
the	O
internet	O
is	O
currently	O
ﬁnanced	O
in	O
great	O
part	O
by	O
various	O
forms	O
of	O
online	O
advertising	O
.	O
there	O
are	O
major	O
parts	O
of	O
the	O
economy	O
that	O
rely	O
on	O
online	O
shopping	O
.	O
companies	O
including	O
amazon	O
and	O
ebay	O
use	O
machine	O
learning	O
,	O
including	O
deep	O
learning	O
,	O
for	O
their	O
product	O
recommendations	O
.	O
sometimes	O
,	O
the	O
items	O
are	O
not	O
products	O
that	O
are	O
actually	O
for	O
sale	O
.	O
examples	O
include	O
selecting	O
posts	O
to	O
display	O
on	O
social	O
network	O
news	O
feeds	O
,	O
recommending	O
movies	O
to	O
watch	O
,	O
recommending	O
jokes	O
,	O
recommending	O
advice	O
from	O
experts	O
,	O
matching	O
players	O
for	O
video	O
games	O
,	O
or	O
matching	O
people	O
in	O
dating	O
services	O
.	O
often	O
,	O
this	O
association	O
problem	O
is	O
handled	O
like	O
a	O
supervised	O
learning	O
problem	O
:	O
given	O
some	O
information	O
about	O
the	O
item	O
and	O
about	O
the	O
user	O
,	O
predict	O
the	O
proxy	O
of	O
interest	O
(	O
user	O
clicks	O
on	O
ad	O
,	O
user	O
enters	O
a	O
rating	O
,	O
user	O
clicks	O
on	O
a	O
“	O
like	O
”	O
button	O
,	O
user	O
buys	O
product	O
,	O
user	O
spends	O
some	O
amount	O
of	O
money	O
on	O
the	O
product	O
,	O
user	O
spends	O
time	O
visiting	O
a	O
page	O
for	O
the	O
product	O
,	O
etc	O
)	O
.	O
this	O
often	O
ends	O
up	O
being	O
either	O
a	O
regression	O
problem	O
(	O
predicting	O
some	O
conditional	O
expected	O
value	O
)	O
or	O
a	O
probabilistic	O
classiﬁcation	O
problem	O
(	O
predicting	O
the	O
conditional	O
probability	O
of	O
some	O
discrete	O
event	O
)	O
.	O
the	O
early	O
work	B
on	O
recommender	O
systems	O
relied	O
on	O
minimal	O
information	O
as	O
inputs	O
for	O
these	O
predictions	O
:	O
the	O
user	O
id	O
and	O
the	O
item	O
id	O
.	O
in	O
this	O
context	O
,	O
the	O
only	O
way	O
to	O
generalize	O
is	O
to	O
rely	O
on	O
the	O
similarity	O
between	O
the	O
patterns	O
of	O
values	O
of	O
the	O
target	O
variable	O
for	O
diﬀerent	O
users	O
or	O
for	O
diﬀerent	O
items	O
.	O
suppose	O
that	O
user	O
1	O
and	O
user	O
2	O
both	O
like	O
items	O
a	O
,	O
b	O
and	O
c.	O
from	O
this	O
,	O
we	O
may	O
infer	O
that	O
user	O
1	O
and	O
478	O
chapter	O
12.	O
applications	O
user	O
2	O
have	O
similar	O
tastes	O
.	O
if	O
user	O
1	O
likes	O
item	O
d	O
,	O
then	O
this	O
should	O
be	O
a	O
strong	O
cue	O
that	O
user	O
2	O
will	O
also	O
like	O
d.	O
algorithms	O
based	O
on	O
this	O
principle	O
come	O
under	O
the	O
name	O
of	O
collaborative	O
ﬁltering	O
.	O
both	O
non-parametric	O
approaches	O
(	O
such	O
as	O
nearest-neighbor	O
methods	O
based	O
on	O
the	O
estimated	O
similarity	O
between	O
patterns	O
of	O
preferences	O
)	O
and	O
parametric	O
methods	O
are	O
possible	O
.	O
parametric	O
methods	O
often	O
rely	O
on	O
learning	O
a	O
distributed	O
representation	O
(	O
also	O
called	O
an	O
embedding	O
)	O
for	O
each	O
user	O
and	O
for	O
each	O
item	O
.	O
bilinear	O
prediction	O
of	O
the	O
target	O
variable	O
(	O
such	O
as	O
a	O
rating	O
)	O
is	O
a	O
simple	O
parametric	O
method	O
that	O
is	O
highly	O
successful	O
and	O
often	O
found	O
as	O
a	O
component	O
of	O
state-of-the-art	O
systems	O
.	O
the	O
prediction	O
is	O
obtained	O
by	O
the	O
dot	O
product	O
between	O
the	O
user	O
embedding	O
and	O
the	O
item	O
embedding	O
(	O
possibly	O
corrected	O
by	O
constants	O
that	O
depend	O
only	O
on	O
either	O
the	O
user	O
id	O
or	O
the	O
item	O
id	O
)	O
.	O
let	O
ˆr	O
be	O
the	O
matrix	O
containing	O
our	O
predictions	O
,	O
a	O
a	O
matrix	O
with	O
user	O
embeddings	O
in	O
its	O
rows	O
and	O
b	O
a	O
matrix	O
with	O
item	O
embeddings	O
in	O
its	O
columns	O
.	O
let	O
b	O
and	O
c	O
be	O
vectors	O
that	O
contain	O
respectively	O
a	O
kind	O
of	O
bias	O
for	O
each	O
user	O
(	O
representing	O
how	O
grumpy	O
or	O
positive	O
that	O
user	O
is	O
in	O
general	O
)	O
and	O
for	O
each	O
item	O
(	O
representing	O
its	O
general	O
popularity	O
)	O
.	O
the	O
bilinear	O
prediction	O
is	O
thus	O
obtained	O
as	O
follows	O
:	O
	O
ˆru	O
,	O
i	O
=	O
bu	O
+	O
ci	O
+	O
a	O
u	O
,	O
jbj	O
,	O
i	O
.	O
j	O
(	O
12.20	O
)	O
	O
	O
typically	O
one	O
wants	O
to	O
minimize	O
the	O
squared	O
error	O
between	O
predicted	O
ratings	O
ˆru	O
,	O
i	O
and	O
actual	O
ratings	O
ru	O
,	O
i	O
.	O
user	O
embeddings	O
and	O
item	O
embeddings	O
can	O
then	O
be	O
conveniently	O
visualized	O
when	O
they	O
are	O
ﬁrst	O
reduced	O
to	O
a	O
low	O
dimension	O
(	O
two	O
or	O
three	O
)	O
,	O
or	O
they	O
can	O
be	O
used	O
to	O
compare	O
users	O
or	O
items	O
against	O
each	O
other	O
,	O
just	O
like	O
word	O
embeddings	O
.	O
one	O
way	O
to	O
obtain	O
these	O
embeddings	O
is	O
by	O
performing	O
a	O
singular	O
value	O
decomposition	O
of	O
the	O
matrix	O
r	O
of	O
actual	O
targets	O
(	O
such	O
as	O
ratings	O
)	O
.	O
this	O
corresponds	O
to	O
factorizing	O
r	O
=	O
u	O
dv	O
(	O
or	O
a	O
normalized	O
variant	O
)	O
into	O
the	O
product	O
of	O
two	O
factors	O
,	O
the	O
lower	O
rank	O
matrices	O
a	O
=	O
u	O
d	O
and	O
b	O
=	O
v	O
.	O
one	O
problem	O
with	O
the	O
svd	O
is	O
that	O
it	O
treats	O
the	O
missing	O
entries	O
in	O
an	O
arbitrary	O
way	O
,	O
as	O
if	O
they	O
corresponded	O
to	O
a	O
target	O
value	O
of	O
0.	O
instead	O
we	O
would	O
like	O
to	O
avoid	O
paying	O
any	O
cost	O
for	O
the	O
predictions	O
made	O
on	O
missing	O
entries	O
.	O
fortunately	O
,	O
the	O
sum	O
of	O
squared	O
errors	O
on	O
the	O
observed	O
ratings	O
can	O
also	O
be	O
easily	O
minimized	O
by	O
gradient-	O
based	O
optimization	O
.	O
the	O
svd	O
and	O
the	O
bilinear	O
prediction	O
of	O
equation	O
both	O
performed	O
very	O
well	O
in	O
the	O
competition	O
for	O
the	O
netﬂix	O
prize	O
(	O
bennett	O
and	O
lanning	O
,	O
2007	O
)	O
,	O
aiming	O
at	O
predicting	O
ratings	O
for	O
ﬁlms	O
,	O
based	O
only	O
on	O
previous	O
ratings	O
by	O
a	O
large	O
set	O
of	O
anonymous	O
users	O
.	O
many	O
machine	O
learning	O
experts	O
participated	O
in	O
this	O
competition	O
,	O
which	O
took	O
place	O
between	O
2006	O
and	O
2009.	O
it	O
raised	O
the	O
level	O
of	O
research	O
in	O
recommender	O
systems	O
using	O
advanced	O
machine	O
learning	O
and	O
yielded	O
improvements	O
in	O
recommender	O
systems	O
.	O
even	O
though	O
it	O
did	O
not	O
win	O
by	O
itself	O
,	O
the	O
simple	O
bilinear	O
prediction	O
or	O
svd	O
was	O
a	O
component	O
of	O
the	O
ensemble	O
models	O
12.20	O
479	O
chapter	O
12.	O
applications	O
presented	O
by	O
most	O
of	O
the	O
competitors	O
,	O
including	O
the	O
winners	O
(	O
koren	O
2009	O
)	O
.	O
,	O
töscher	O
et	O
al	O
.	O
2009	O
;	O
,	O
beyond	O
these	O
bilinear	O
models	O
with	O
distributed	O
representations	O
,	O
one	O
of	O
the	O
ﬁrst	O
uses	O
of	O
neural	O
networks	O
for	O
collaborative	O
ﬁltering	O
is	O
based	O
on	O
the	O
rbm	O
undirected	O
)	O
.	O
rbms	O
were	O
an	O
important	O
element	O
probabilistic	O
model	B
(	O
salakhutdinov	O
of	O
the	O
ensemble	O
of	O
methods	O
that	O
won	O
the	O
netﬂix	O
competition	O
(	O
töscher	O
2009	O
;	O
)	O
.	O
more	O
advanced	O
variants	O
on	O
the	O
idea	O
of	O
factorizing	O
the	O
ratings	O
matrix	O
koren	O
2009	O
have	O
also	O
been	O
explored	O
in	O
the	O
neural	O
networks	O
community	O
(	O
salakhutdinov	O
and	O
mnih	O
2008	O
et	O
al.	O
,	O
et	O
al.	O
,	O
2007	O
)	O
.	O
,	O
,	O
however	O
,	O
there	O
is	O
a	O
basic	O
limitation	O
of	O
collaborative	O
ﬁltering	O
systems	O
:	O
when	O
a	O
new	O
item	O
or	O
a	O
new	O
user	O
is	O
introduced	O
,	O
its	O
lack	O
of	O
rating	O
history	O
means	O
that	O
there	O
is	O
no	O
way	O
to	O
evaluate	O
its	O
similarity	O
with	O
other	O
items	O
or	O
users	O
(	O
respectively	O
)	O
,	O
or	O
the	O
degree	O
of	O
association	O
between	O
,	O
say	O
,	O
that	O
new	O
user	O
and	O
existing	O
items	O
.	O
this	O
is	O
called	O
the	O
problem	O
of	O
cold-start	O
recommendations	O
.	O
a	O
general	O
way	O
of	O
solving	O
the	O
cold-start	O
recommendation	O
problem	O
is	O
to	O
introduce	O
extra	O
information	O
about	O
the	O
individual	O
users	O
and	O
items	O
.	O
for	O
example	O
,	O
this	O
extra	O
information	O
could	O
be	O
user	O
proﬁle	O
information	O
or	O
features	O
of	O
each	O
item	O
.	O
systems	O
that	O
use	O
such	O
information	O
are	O
called	O
content-based	O
recommender	O
systems	O
.	O
the	O
mapping	O
from	O
a	O
rich	O
set	O
of	O
user	O
features	O
or	O
item	O
features	O
to	O
an	O
embedding	O
can	O
be	O
learned	O
through	O
a	O
deep	O
learning	O
architecture	O
(	O
huang	O
et	O
al	O
.	O
2013	O
elkahky	O
et	O
al.	O
,	O
2015	O
)	O
.	O
,	O
;	O
specialized	O
deep	O
learning	O
architectures	O
such	O
as	O
convolutional	O
networks	O
have	O
also	O
been	O
applied	O
to	O
learn	O
to	O
extract	O
features	O
from	O
rich	O
content	O
such	O
as	O
from	O
musical	O
audio	O
tracks	O
,	O
for	O
music	O
recommendation	O
(	O
van	O
den	O
oörd	O
)	O
.	O
in	O
that	O
work	B
,	O
the	O
convolutional	O
net	O
takes	O
acoustic	O
features	O
as	O
input	O
and	O
computes	O
an	O
embedding	O
for	O
the	O
associated	O
song	O
.	O
the	O
dot	O
product	O
between	O
this	O
song	O
embedding	O
and	O
the	O
embedding	O
for	O
a	O
user	O
is	O
then	O
used	O
to	O
predict	O
whether	O
a	O
user	O
will	O
listen	O
to	O
the	O
song	O
.	O
et	O
al.	O
,	O
2013	O
12.5.1.1	O
exploration	O
versus	O
exploitation	O
langford	O
and	O
zhang	O
2008	O
lu	O
et	O
al	O
.	O
2010	O
when	O
making	O
recommendations	O
to	O
users	O
,	O
an	O
issue	O
arises	O
that	O
goes	O
beyond	O
ordinary	O
supervised	O
learning	O
and	O
into	O
the	O
realm	O
of	O
reinforcement	O
learning	O
.	O
many	O
recom-	O
mendation	O
problems	O
are	O
most	O
accurately	O
described	O
theoretically	O
as	O
contextual	O
bandits	O
(	O
)	O
.	O
the	O
issue	O
is	O
that	O
when	O
we	O
use	O
the	O
recommendation	O
system	O
to	O
collect	O
data	O
,	O
we	O
get	O
a	O
biased	O
and	O
incomplete	O
view	O
of	O
the	O
preferences	O
of	O
users	O
:	O
we	O
only	O
see	O
the	O
responses	O
of	O
users	O
to	O
the	O
items	O
they	O
were	O
recommended	O
and	O
not	O
to	O
the	O
other	O
items	O
.	O
in	O
addition	O
,	O
in	O
some	O
cases	O
we	O
may	O
not	O
get	O
any	O
information	O
on	O
users	O
for	O
whom	O
no	O
recommendation	O
has	O
been	O
made	O
(	O
for	O
example	O
,	O
with	O
ad	O
auctions	O
,	O
it	O
may	O
be	O
that	O
the	O
price	O
proposed	O
for	O
an	O
,	O
;	O
,	O
480	O
chapter	O
12.	O
applications	O
ad	O
was	O
below	O
a	O
minimum	O
price	O
threshold	O
,	O
or	O
does	O
not	O
win	O
the	O
auction	O
,	O
so	O
the	O
ad	O
is	O
not	O
shown	O
at	O
all	O
)	O
.	O
more	O
importantly	O
,	O
we	O
get	O
no	O
information	O
about	O
what	O
outcome	O
would	O
have	O
resulted	O
from	O
recommending	O
any	O
of	O
the	O
other	O
items	O
.	O
this	O
would	O
be	O
like	O
training	O
a	O
classiﬁer	O
by	O
picking	O
one	O
class	O
ˆy	O
for	O
each	O
training	O
example	O
x	O
(	O
typically	O
the	O
class	O
with	O
the	O
highest	O
probability	O
according	O
to	O
the	O
model	B
)	O
and	O
then	O
only	O
getting	O
as	O
feedback	O
whether	O
this	O
was	O
the	O
correct	O
class	O
or	O
not	O
.	O
clearly	O
,	O
each	O
example	O
conveys	O
less	O
information	O
than	O
in	O
the	O
supervised	O
case	O
where	O
the	O
true	O
label	O
y	O
is	O
directly	O
accessible	O
,	O
so	O
more	O
examples	O
are	O
necessary	O
.	O
worse	O
,	O
if	O
we	O
are	O
not	O
careful	O
,	O
we	O
could	O
end	O
up	O
with	O
a	O
system	O
that	O
continues	O
picking	O
the	O
wrong	O
decisions	O
even	O
as	O
more	O
and	O
more	O
data	O
is	O
collected	O
,	O
because	O
the	O
correct	O
decision	O
initially	O
had	O
a	O
very	O
low	O
probability	O
:	O
until	O
the	O
learner	O
picks	O
that	O
correct	O
decision	O
,	O
it	O
does	O
not	O
learn	O
about	O
the	O
correct	O
decision	O
.	O
this	O
is	O
similar	O
to	O
the	O
situation	O
in	O
reinforcement	O
learning	O
where	O
only	O
the	O
reward	O
for	O
the	O
selected	O
action	O
is	O
observed	O
.	O
in	O
general	O
,	O
reinforcement	O
learning	O
can	O
involve	O
a	O
sequence	O
of	O
many	O
actions	O
and	O
many	O
rewards	O
.	O
the	O
bandits	O
scenario	O
is	O
a	O
special	O
case	O
of	O
reinforcement	O
learning	O
,	O
in	O
which	O
the	O
learner	O
takes	O
only	O
a	O
single	O
action	O
and	O
receives	O
a	O
single	O
reward	O
.	O
the	O
bandit	O
problem	O
is	O
easier	O
in	O
the	O
sense	O
that	O
the	O
learner	O
knows	O
which	O
reward	O
is	O
associated	O
with	O
which	O
action	O
.	O
in	O
the	O
general	O
reinforcement	O
learning	O
scenario	O
,	O
a	O
high	O
reward	O
or	O
a	O
low	O
reward	O
might	O
have	O
been	O
caused	O
by	O
a	O
recent	O
action	O
or	O
by	O
an	O
action	O
in	O
the	O
distant	O
past	O
.	O
the	O
term	O
contextual	O
bandits	O
refers	O
to	O
the	O
case	O
where	O
the	O
action	O
is	O
taken	O
in	O
the	O
context	O
of	O
some	O
input	O
variable	O
that	O
can	O
inform	O
the	O
decision	O
.	O
for	O
example	O
,	O
we	O
at	O
least	O
know	O
the	O
user	O
identity	O
,	O
and	O
we	O
want	O
to	O
pick	O
an	O
item	O
.	O
the	O
mapping	O
from	O
context	O
to	O
action	O
is	O
also	O
called	O
a	O
policy	O
.	O
the	O
feedback	O
loop	O
between	O
the	O
learner	O
and	O
the	O
data	O
distribution	O
(	O
which	O
now	O
depends	O
on	O
the	O
actions	O
of	O
the	O
learner	O
)	O
is	O
a	O
central	O
research	O
issue	O
in	O
the	O
reinforcement	O
learning	O
and	O
bandits	O
literature	O
.	O
reinforcement	O
learning	O
requires	O
choosing	O
a	O
tradeoﬀ	O
between	O
exploration	O
and	O
exploitation	O
.	O
exploitation	O
refers	O
to	O
taking	O
actions	O
that	O
come	O
from	O
the	O
current	O
,	O
best	O
version	O
of	O
the	O
learned	O
policy—actions	O
that	O
we	O
know	O
will	O
achieve	O
a	O
high	O
reward	O
.	O
exploration	O
refers	O
to	O
taking	O
actions	O
speciﬁcally	O
in	O
order	O
to	O
obtain	O
more	O
training	O
data	O
.	O
if	O
we	O
know	O
that	O
given	O
context	O
x	O
,	O
action	O
a	O
gives	O
us	O
a	O
reward	O
of	O
1	O
,	O
we	O
do	O
not	O
know	O
whether	O
that	O
is	O
the	O
best	O
possible	O
reward	O
.	O
we	O
may	O
want	O
to	O
exploit	O
our	O
current	O
policy	O
and	O
continue	O
taking	O
action	O
a	O
in	O
order	O
to	O
be	O
relatively	O
sure	O
of	O
obtaining	O
a	O
	O
reward	O
of	O
1.	O
however	O
,	O
we	O
may	O
also	O
want	O
to	O
explore	O
by	O
trying	O
action	O
a	O
.	O
we	O
do	O
not	O
	O
know	O
what	O
will	O
happen	O
if	O
we	O
try	O
action	O
a	O
,	O
but	O
we	O
2	O
.	O
either	O
way	O
,	O
we	O
at	O
least	O
gain	O
some	O
knowledge	O
.	O
run	O
the	O
risk	O
of	O
getting	O
a	O
reward	O
of	O
0	O
.	O
we	O
hope	O
to	O
get	O
a	O
reward	O
of	O
exploration	O
can	O
be	O
implemented	O
in	O
many	O
ways	O
,	O
ranging	O
from	O
occasionally	O
taking	O
random	O
actions	O
intended	O
to	O
cover	O
the	O
entire	O
space	O
of	O
possible	O
actions	O
,	O
to	O
model-based	O
approaches	O
that	O
compute	O
a	O
choice	O
of	O
action	O
based	O
on	O
its	O
expected	O
reward	O
and	O
the	O
model	B
’	O
s	O
amount	O
of	O
uncertainty	O
about	O
that	O
reward	O
.	O
481	O
chapter	O
12.	O
applications	O
many	O
factors	O
determine	O
the	O
extent	O
to	O
which	O
we	O
prefer	O
exploration	O
or	O
exploitation	O
.	O
one	O
of	O
the	O
most	O
prominent	O
factors	O
is	O
the	O
time	O
scale	O
we	O
are	O
interested	O
in	O
.	O
if	O
the	O
agent	O
has	O
only	O
a	O
short	O
amount	O
of	O
time	O
to	O
accrue	O
reward	O
,	O
then	O
we	O
prefer	O
more	O
exploitation	O
.	O
if	O
the	O
agent	O
has	O
a	O
long	O
time	O
to	O
accrue	O
reward	O
,	O
then	O
we	O
begin	O
with	O
more	O
exploration	O
so	O
that	O
future	O
actions	O
can	O
be	O
planned	O
more	O
eﬀectively	O
with	O
more	O
knowledge	O
.	O
as	O
time	O
progresses	O
and	O
our	O
learned	O
policy	O
improves	O
,	O
we	O
move	O
toward	O
more	O
exploitation	O
.	O
supervised	O
learning	O
has	O
no	O
tradeoﬀ	O
between	O
exploration	O
and	O
exploitation	O
because	O
the	O
supervision	O
signal	O
always	O
speciﬁes	O
which	O
output	O
is	O
correct	O
for	O
each	O
input	O
.	O
there	O
is	O
no	O
need	O
to	O
try	O
out	O
diﬀerent	O
outputs	O
to	O
determine	O
if	O
one	O
is	O
better	O
than	O
the	O
model	B
’	O
s	O
current	O
output—we	O
always	O
know	O
that	O
the	O
label	O
is	O
the	O
best	O
output	O
.	O
another	O
diﬃculty	O
arising	O
in	O
the	O
context	O
of	O
reinforcement	O
learning	O
,	O
besides	O
the	O
exploration-exploitation	O
trade-oﬀ	O
,	O
is	O
the	O
diﬃculty	O
of	O
evaluating	O
and	O
comparing	O
diﬀerent	O
policies	O
.	O
reinforcement	O
learning	O
involves	O
interaction	O
between	O
the	O
learner	O
and	O
the	O
environment	O
.	O
this	O
feedback	O
loop	O
means	O
that	O
it	O
is	O
not	O
straightforward	O
to	O
evaluate	O
the	O
learner	O
’	O
s	O
performance	O
using	O
a	O
ﬁxed	O
set	O
of	O
test	O
set	O
input	O
values	O
.	O
the	O
policy	O
itself	O
determines	O
which	O
inputs	O
will	O
be	O
seen.	O
)	O
present	O
techniques	O
for	O
evaluating	O
contextual	O
bandits	O
.	O
dudik	O
et	O
al	O
.	O
2011	O
(	O
12.5.2	O
knowledge	O
representation	O
,	O
reasoning	O
and	O
question	O
an-	O
swering	O
,	O
rumelhart	O
et	O
al	O
.	O
1986a	O
deep	O
learning	O
approaches	O
have	O
been	O
very	O
successful	O
in	O
language	O
modeling	O
,	O
machine	O
translation	O
and	O
natural	O
language	O
processing	O
due	O
to	O
the	O
use	O
of	O
embeddings	O
for	O
symbols	O
(	O
et	O
al.	O
,	O
2001	O
)	O
.	O
these	O
embeddings	O
represent	O
semantic	O
knowledge	O
about	O
individual	O
words	O
and	O
concepts	O
.	O
a	O
research	O
frontier	O
is	O
to	O
develop	O
embeddings	O
for	O
phrases	O
and	O
for	O
relations	O
between	O
words	O
and	O
facts	O
.	O
search	O
engines	O
already	O
use	O
machine	O
learning	O
for	O
this	O
purpose	O
but	O
much	O
more	O
remains	O
to	O
be	O
done	O
to	O
improve	O
these	O
more	O
advanced	O
representations.	O
)	O
and	O
words	O
(	O
deerwester	O
et	O
al.	O
,	O
1990	O
bengio	O
;	O
12.5.2.1	O
knowledge	O
,	O
relations	O
and	O
question	O
answering	O
one	O
interesting	O
research	O
direction	O
is	O
determining	O
how	O
distributed	O
representations	O
can	O
be	O
trained	O
to	O
capture	O
the	O
relations	O
between	O
two	O
entities	O
.	O
these	O
relations	O
allow	O
us	O
to	O
formalize	O
facts	O
about	O
objects	O
and	O
how	O
objects	O
interact	O
with	O
each	O
other	O
.	O
in	O
mathematics	O
,	O
a	O
binary	O
relation	O
is	O
a	O
set	O
of	O
ordered	O
pairs	O
of	O
objects	O
.	O
pairs	O
that	O
are	O
in	O
the	O
set	O
are	O
said	O
to	O
have	O
the	O
relation	O
while	O
those	O
who	O
are	O
not	O
in	O
the	O
set	O
482	O
chapter	O
12.	O
applications	O
}	O
1	O
,	O
2,3	O
by	O
deﬁning	O
the	O
set	O
of	O
ordered	O
pairs	O
s	O
=	O
{	O
do	O
not	O
.	O
for	O
example	O
,	O
we	O
can	O
deﬁne	O
the	O
relation	O
“	O
is	O
less	O
than	O
”	O
on	O
the	O
set	O
of	O
entities	O
.	O
once	O
this	O
relation	O
is	O
deﬁned	O
,	O
we	O
can	O
use	O
it	O
like	O
a	O
verb	O
.	O
because	O
(	O
1	O
,	O
2	O
)	O
s	O
,	O
we	O
say	O
that	O
1	O
is	O
less	O
than	O
2.	O
because	O
(	O
2,1	O
)	O
s	O
,	O
we	O
can	O
not	O
say	O
that	O
2	O
is	O
less	O
than	O
1.	O
of	O
course	O
,	O
the	O
entities	O
that	O
are	O
related	O
to	O
one	O
another	O
need	O
not	O
be	O
numbers	O
.	O
we	O
could	O
deﬁne	O
a	O
relation	O
{	O
}	O
(	O
1,2	O
)	O
,	O
(	O
1	O
,	O
3	O
)	O
,	O
(	O
2	O
,	O
3	O
)	O
containing	O
tuples	O
like	O
(	O
is_a_type_of	O
∈	O
∈	O
dog	O
mammal	O
,	O
)	O
.	O
in	O
the	O
context	O
of	O
ai	O
,	O
we	O
think	O
of	O
a	O
relation	O
as	O
a	O
sentence	O
in	O
a	O
syntactically	O
simple	O
and	O
highly	O
structured	O
language	O
.	O
the	O
relation	O
plays	O
the	O
role	O
of	O
a	O
verb	O
,	O
while	O
two	O
arguments	O
to	O
the	O
relation	O
play	O
the	O
role	O
of	O
its	O
subject	O
and	O
object	O
.	O
these	O
sentences	O
take	O
the	O
form	O
of	O
a	O
triplet	O
of	O
tokens	O
(	O
subject	O
verb	O
object	O
)	O
,	O
,	O
with	O
values	O
(	O
entityi	O
,	O
relationj	O
,	O
entity	O
k	O
)	O
.	O
(	O
12.21	O
)	O
(	O
12.22	O
)	O
we	O
can	O
also	O
deﬁne	O
an	O
attribute	O
,	O
a	O
concept	O
analogous	O
to	O
a	O
relation	O
,	O
but	O
taking	O
only	O
one	O
argument	O
:	O
(	O
entityi	O
,	O
attribute	O
j	O
)	O
.	O
(	O
12.23	O
)	O
for	O
example	O
,	O
we	O
could	O
deﬁne	O
the	O
has_fur	O
attribute	O
,	O
and	O
apply	O
it	O
to	O
entities	O
like	O
dog	O
.	O
many	O
applications	O
require	O
representing	O
relations	O
and	O
reasoning	O
about	O
them	O
.	O
how	O
should	O
we	O
best	O
do	O
this	O
within	O
the	O
context	O
of	O
neural	O
networks	O
?	O
machine	O
learning	O
models	O
of	O
course	O
require	O
training	O
data	O
.	O
we	O
can	O
infer	O
relations	O
between	O
entities	O
from	O
training	O
datasets	O
consisting	O
of	O
unstructured	O
natural	O
language	O
.	O
there	O
are	O
also	O
structured	O
databases	O
that	O
identify	O
relations	O
explicitly	O
.	O
a	O
common	O
structure	O
for	O
these	O
databases	O
is	O
the	O
relational	O
database	O
,	O
which	O
stores	O
this	O
same	O
kind	O
of	O
information	O
,	O
albeit	O
not	O
formatted	O
as	O
three	O
token	O
sentences	O
.	O
when	O
a	O
database	O
is	O
intended	O
to	O
convey	O
commonsense	O
knowledge	O
about	O
everyday	O
life	O
or	O
expert	O
knowledge	O
about	O
an	O
application	O
area	O
to	O
an	O
artiﬁcial	O
intelligence	O
system	O
,	O
we	O
call	O
the	O
database	O
a	O
knowledge	O
base	O
.	O
knowledge	O
bases	O
range	O
from	O
general	O
ones	O
like	O
freebase	O
,	O
opencyc	O
,	O
wordnet	O
,	O
or	O
wikibase	O
,	O
1	O
etc	O
.	O
to	O
more	O
specialized	O
knowledge	O
bases	O
,	O
like	O
geneontology.2	O
representations	O
for	O
entities	O
and	O
relations	O
can	O
be	O
learned	O
by	O
considering	O
each	O
triplet	O
in	O
a	O
knowledge	O
base	O
as	O
a	O
training	O
example	O
and	O
maximizing	O
a	O
training	O
objective	O
that	O
captures	O
their	O
joint	O
distribution	O
(	O
bordes	O
et	O
al.	O
,	O
2013a	O
)	O
.	O
1respectively	O
available	O
from	O
these	O
web	O
sites	O
:	O
freebase.com	O
,	O
cyc.com/opencyc	O
,	O
wordnet	O
.	O
princeton.edu	O
wikiba.se	O
,	O
2geneontology.org	O
483	O
chapter	O
12.	O
applications	O
in	O
addition	O
to	O
training	O
data	O
,	O
we	O
also	O
need	O
to	O
deﬁne	O
a	O
model	B
family	O
to	O
train	O
.	O
a	O
common	O
approach	O
is	O
to	O
extend	O
neural	O
language	O
models	O
to	O
model	B
entities	O
and	O
relations	O
.	O
neural	O
language	O
models	O
learn	O
a	O
vector	O
that	O
provides	O
a	O
distributed	O
representation	O
of	O
each	O
word	O
.	O
they	O
also	O
learn	O
about	O
interactions	O
between	O
words	O
,	O
such	O
as	O
which	O
word	O
is	O
likely	O
to	O
come	O
after	O
a	O
sequence	O
of	O
words	O
,	O
by	O
learning	O
functions	O
of	O
these	O
vectors	O
.	O
we	O
can	O
extend	O
this	O
approach	O
to	O
entities	O
and	O
relations	O
by	O
learning	O
an	O
embedding	O
vector	O
for	O
each	O
relation	O
.	O
in	O
fact	O
,	O
the	O
parallel	O
between	O
modeling	O
language	O
and	O
modeling	O
knowledge	O
encoded	O
as	O
relations	O
is	O
so	O
close	O
that	O
researchers	O
and	O
have	O
trained	O
representations	O
of	O
such	O
entities	O
by	O
using	O
both	O
natural	O
language	O
sentences	O
(	O
)	O
or	O
combining	O
data	O
from	O
multiple	O
relational	O
databases	O
(	O
)	O
.	O
many	O
possibilities	O
exist	O
for	O
the	O
particular	O
parametrization	O
associated	O
with	O
such	O
a	O
model	B
.	O
early	O
work	B
on	O
learning	O
about	O
relations	O
between	O
entities	O
(	O
paccanaro	O
and	O
hinton	O
,	O
2000	O
)	O
posited	O
highly	O
constrained	O
parametric	O
forms	O
(	O
“	O
linear	O
relational	O
embeddings	O
”	O
)	O
,	O
often	O
using	O
a	O
diﬀerent	O
form	O
of	O
representation	O
for	O
the	O
relation	O
than	O
for	O
the	O
entities	O
.	O
for	O
example	O
,	O
paccanaro	O
and	O
hinton	O
2000	O
)	O
used	O
vectors	O
for	O
entities	O
and	O
matrices	O
for	O
relations	O
,	O
with	O
the	O
idea	O
that	O
a	O
relation	O
acts	O
like	O
an	O
operator	O
on	O
entities	O
.	O
alternatively	O
,	O
relations	O
can	O
be	O
considered	O
as	O
any	O
other	O
entity	O
(	O
bordes	O
et	O
al.	O
,	O
)	O
,	O
allowing	O
us	O
to	O
make	O
statements	O
about	O
relations	O
,	O
but	O
more	O
ﬂexibility	O
is	O
put	O
in	O
the	O
machinery	O
that	O
combines	O
them	O
in	O
order	O
to	O
model	B
their	O
joint	O
distribution	O
.	O
knowledge	O
bases	O
2014a	O
bordes	O
et	O
al	O
.	O
2011	O
2012	O
wang	O
bordes	O
et	O
al	O
.	O
2013b	O
bordes	O
et	O
al	O
.	O
(	O
et	O
al.	O
,	O
(	O
)	O
and	O
2012	O
2011	O
,	O
,	O
;	O
,	O
a	O
practical	O
short-term	O
application	O
of	O
such	O
models	O
is	O
link	O
prediction	O
:	O
predict-	O
ing	O
missing	O
arcs	O
in	O
the	O
knowledge	O
graph	O
.	O
this	O
is	O
a	O
form	O
of	O
generalization	O
to	O
new	O
facts	O
,	O
based	O
on	O
old	O
facts	O
.	O
most	O
of	O
the	O
knowledge	O
bases	O
that	O
currently	O
exist	O
have	O
been	O
constructed	O
through	O
manual	O
labor	O
,	O
which	O
tends	O
to	O
leave	O
many	O
and	O
probably	O
the	O
majority	O
of	O
true	O
relations	O
absent	O
from	O
the	O
knowledge	O
base	O
.	O
see	O
wang	O
et	O
al	O
.	O
(	O
2014b	O
lin	O
et	O
al	O
.	O
2015	O
)	O
for	O
examples	O
of	O
such	O
an	O
application	O
.	O
garcia-duran	O
et	O
al	O
.	O
2015	O
)	O
and	O
)	O
,	O
(	O
(	O
evaluating	O
the	O
performance	O
of	O
a	O
model	B
on	O
a	O
link	O
prediction	O
task	O
is	O
diﬃcult	O
because	O
we	O
have	O
only	O
a	O
dataset	O
of	O
positive	O
examples	O
(	O
facts	O
that	O
are	O
known	O
to	O
be	O
true	O
)	O
.	O
if	O
the	O
model	B
proposes	O
a	O
fact	O
that	O
is	O
not	O
in	O
the	O
dataset	O
,	O
we	O
are	O
unsure	O
whether	O
the	O
model	B
has	O
made	O
a	O
mistake	O
or	O
discovered	O
a	O
new	O
,	O
previously	O
unknown	O
fact	O
.	O
the	O
metrics	O
are	O
thus	O
somewhat	O
imprecise	O
and	O
are	O
based	O
on	O
testing	O
how	O
the	O
model	B
ranks	O
a	O
held-out	O
of	O
set	O
of	O
known	O
true	O
positive	O
facts	O
compared	O
to	O
other	O
facts	O
that	O
are	O
less	O
likely	O
to	O
be	O
true	O
.	O
a	O
common	O
way	O
to	O
construct	O
interesting	O
examples	O
that	O
are	O
probably	O
negative	O
(	O
facts	O
that	O
are	O
probably	O
false	O
)	O
is	O
to	O
begin	O
with	O
a	O
true	O
fact	O
and	O
create	O
corrupted	O
versions	O
of	O
that	O
fact	O
,	O
for	O
example	O
by	O
replacing	O
one	O
entity	O
in	O
the	O
relation	O
with	O
a	O
diﬀerent	O
entity	O
selected	O
at	O
random	O
.	O
the	O
popular	O
precision	O
at	O
10	O
%	O
metric	O
counts	O
how	O
many	O
times	O
the	O
model	B
ranks	O
a	O
“	O
correct	O
”	O
fact	O
among	O
the	O
top	O
10	O
%	O
of	O
all	O
corrupted	O
versions	O
of	O
that	O
fact	O
.	O
484	O
chapter	O
12.	O
applications	O
another	O
application	O
of	O
knowledge	O
bases	O
and	O
distributed	O
representations	O
for	O
them	O
is	O
word-sense	O
disambiguation	O
(	O
navigli	O
and	O
velardi	O
2005	O
bordes	O
et	O
al.	O
,	O
2012	O
)	O
,	O
which	O
is	O
the	O
task	O
of	O
deciding	O
which	O
of	O
the	O
senses	O
of	O
a	O
word	O
is	O
the	O
appropriate	O
one	O
,	O
in	O
some	O
context	O
.	O
,	O
;	O
eventually	O
,	O
knowledge	O
of	O
relations	O
combined	O
with	O
a	O
reasoning	O
process	O
and	O
understanding	O
of	O
natural	O
language	O
could	O
allow	O
us	O
to	O
build	O
a	O
general	O
question	O
answering	O
system	O
.	O
a	O
general	O
question	O
answering	O
system	O
must	O
be	O
able	O
to	O
process	O
input	O
information	O
and	O
remember	O
important	O
facts	O
,	O
organized	O
in	O
a	O
way	O
that	O
enables	O
it	O
to	O
retrieve	O
and	O
reason	O
about	O
them	O
later	O
.	O
this	O
remains	O
a	O
diﬃcult	O
open	O
problem	O
which	O
can	O
only	O
be	O
solved	O
in	O
restricted	O
“	O
toy	O
”	O
environments	O
.	O
currently	O
,	O
the	O
best	O
approach	O
to	O
remembering	O
and	O
retrieving	O
speciﬁc	O
declarative	O
facts	O
is	O
to	O
use	O
an	O
.	O
memory	O
networks	O
were	O
explicit	O
memory	O
mechanism	O
,	O
as	O
described	O
in	O
section	O
ﬁrst	O
proposed	O
to	O
solve	O
a	O
toy	O
question	O
answering	O
task	O
(	O
weston	O
2014	O
kumar	O
et	O
al	O
.	O
(	O
)	O
have	O
proposed	O
an	O
extension	O
that	O
uses	O
gru	O
recurrent	O
nets	O
to	O
read	O
the	O
input	O
into	O
the	O
memory	O
and	O
to	O
produce	O
the	O
answer	O
given	O
the	O
contents	O
of	O
the	O
memory	O
.	O
et	O
al.	O
,	O
10.12	O
2015	O
)	O
.	O
deep	O
learning	O
has	O
been	O
applied	O
to	O
many	O
other	O
applications	O
besides	O
the	O
ones	O
described	O
here	O
,	O
and	O
will	O
surely	O
be	O
applied	O
to	O
even	O
more	O
after	O
this	O
writing	O
.	O
it	O
would	O
be	O
impossible	O
to	O
describe	O
anything	O
remotely	O
resembling	O
a	O
comprehensive	O
coverage	O
of	O
such	O
a	O
topic	O
.	O
this	O
survey	O
provides	O
a	O
representative	O
sample	O
of	O
what	O
is	O
possible	O
as	O
of	O
this	O
writing	O
.	O
ii	O
this	O
concludes	O
part	O
,	O
which	O
has	O
described	O
modern	O
practices	O
involving	O
deep	O
networks	O
,	O
comprising	O
all	O
of	O
the	O
most	O
successful	O
methods	O
.	O
generally	O
speaking	O
,	O
these	O
methods	O
involve	O
using	O
the	O
gradient	O
of	O
a	O
cost	O
function	O
to	O
ﬁnd	O
the	O
parameters	O
of	O
a	O
model	B
that	O
approximates	O
some	O
desired	O
function	O
.	O
with	O
enough	O
training	O
data	O
,	O
this	O
approach	O
is	O
extremely	O
powerful	O
.	O
we	O
now	O
turn	O
to	O
part	O
,	O
in	O
which	O
we	O
step	O
into	O
the	O
territory	O
of	O
research—methods	O
that	O
are	O
designed	O
to	O
work	B
with	O
less	O
training	O
data	O
or	O
to	O
perform	O
a	O
greater	O
variety	O
of	O
tasks	O
,	O
where	O
the	O
challenges	O
are	O
more	O
diﬃcult	O
and	O
not	O
as	O
close	O
to	O
being	O
solved	O
as	O
the	O
situations	O
we	O
have	O
described	O
so	O
far	O
.	O
iii	O
485	O
part	O
iii	O
deep	O
learning	O
research	O
486	O
this	O
part	O
of	O
the	O
book	O
describes	O
the	O
more	O
ambitious	O
and	O
advanced	O
approaches	O
to	O
deep	O
learning	O
,	O
currently	O
pursued	O
by	O
the	O
research	O
community	O
.	O
in	O
the	O
previous	O
parts	O
of	O
the	O
book	O
,	O
we	O
have	O
shown	O
how	O
to	O
solve	O
supervised	O
learning	O
problems—how	O
to	O
learn	O
to	O
map	O
one	O
vector	O
to	O
another	O
,	O
given	O
enough	O
examples	O
of	O
the	O
mapping	O
.	O
not	O
all	O
problems	O
we	O
might	O
want	O
to	O
solve	O
fall	O
into	O
this	O
category	O
.	O
we	O
may	O
wish	O
to	O
generate	O
new	O
examples	O
,	O
or	O
determine	O
how	O
likely	O
some	O
point	O
is	O
,	O
or	O
handle	O
missing	O
values	O
and	O
take	O
advantage	O
of	O
a	O
large	O
set	O
of	O
unlabeled	O
examples	O
or	O
examples	O
from	O
related	O
tasks	O
.	O
a	O
shortcoming	O
of	O
the	O
current	O
state	O
of	O
the	O
art	O
for	O
industrial	O
applications	O
is	O
that	O
our	O
learning	O
algorithms	O
require	O
large	O
amounts	O
of	O
supervised	O
data	O
to	O
achieve	O
good	O
accuracy	O
.	O
in	O
this	O
part	O
of	O
the	O
book	O
,	O
we	O
discuss	O
some	O
of	O
the	O
speculative	O
approaches	O
to	O
reducing	O
the	O
amount	O
of	O
labeled	O
data	O
necessary	O
for	O
existing	O
models	O
to	O
work	B
well	O
and	O
be	O
applicable	O
across	O
a	O
broader	O
range	O
of	O
tasks	O
.	O
accomplishing	O
these	O
goals	O
usually	O
requires	O
some	O
form	O
of	O
unsupervised	O
or	O
semi-supervised	O
learning	O
.	O
many	O
deep	O
learning	O
algorithms	O
have	O
been	O
designed	O
to	O
tackle	O
unsupervised	O
learning	O
problems	O
,	O
but	O
none	O
have	O
truly	O
solved	O
the	O
problem	O
in	O
the	O
same	O
way	O
that	O
deep	O
learning	O
has	O
largely	O
solved	O
the	O
supervised	O
learning	O
problem	O
for	O
a	O
wide	O
variety	O
of	O
tasks	O
.	O
in	O
this	O
part	O
of	O
the	O
book	O
,	O
we	O
describe	O
the	O
existing	O
approaches	O
to	O
unsupervised	O
learning	O
and	O
some	O
of	O
the	O
popular	O
thought	O
about	O
how	O
we	O
can	O
make	O
progress	O
in	O
this	O
ﬁeld	O
.	O
a	O
central	O
cause	O
of	O
the	O
diﬃculties	O
with	O
unsupervised	O
learning	O
is	O
the	O
high	O
di-	O
mensionality	O
of	O
the	O
random	O
variables	O
being	O
modeled	O
.	O
this	O
brings	O
two	O
distinct	O
challenges	O
:	O
a	O
statistical	O
challenge	O
and	O
a	O
computational	O
challenge	O
.	O
the	O
statistical	O
challenge	O
regards	O
generalization	O
:	O
the	O
number	O
of	O
conﬁgurations	O
we	O
may	O
want	O
to	O
distinguish	O
can	O
grow	O
exponentially	O
with	O
the	O
number	O
of	O
dimensions	O
of	O
interest	O
,	O
and	O
this	O
quickly	O
becomes	O
much	O
larger	O
than	O
the	O
number	O
of	O
examples	O
one	O
can	O
possibly	O
have	O
(	O
or	O
use	O
with	O
bounded	O
computational	O
resources	O
)	O
.	O
the	O
computational	O
challenge	O
associated	O
with	O
high-dimensional	O
distributions	O
arises	O
because	O
many	O
algorithms	O
for	O
learning	O
or	O
using	O
a	O
trained	O
model	B
(	O
especially	O
those	O
based	O
on	O
estimating	O
an	O
explicit	O
probability	O
function	O
)	O
involve	O
intractable	O
computations	O
that	O
grow	O
exponentially	O
with	O
the	O
number	O
of	O
dimensions	O
.	O
with	O
probabilistic	O
models	O
,	O
this	O
computational	O
challenge	O
arises	O
from	O
the	O
need	O
to	O
perform	O
intractable	O
inference	O
or	O
simply	O
from	O
the	O
need	O
to	O
normalize	O
the	O
distribution	O
.	O
•	O
intractable	O
inference	O
:	O
inference	O
is	O
discussed	O
mostly	O
in	O
chapter	O
.	O
it	O
regards	O
the	O
question	O
of	O
guessing	O
the	O
probable	O
values	O
of	O
some	O
variables	O
a	O
,	O
given	O
other	O
variables	O
b	O
,	O
with	O
respect	O
to	O
a	O
model	B
that	O
captures	O
the	O
joint	O
distribution	O
over	O
19	O
487	O
a	O
,	O
b	O
and	O
c.	O
in	O
order	O
to	O
even	O
compute	O
such	O
conditional	O
probabilities	O
one	O
needs	O
to	O
sum	O
over	O
the	O
values	O
of	O
the	O
variables	O
c	O
,	O
as	O
well	O
as	O
compute	O
a	O
normalization	O
constant	O
which	O
sums	O
over	O
the	O
values	O
of	O
a	O
and	O
c.	O
•	O
18	O
intractable	O
normalization	O
constants	O
(	O
the	O
partition	O
function	O
)	O
:	O
the	O
partition	O
function	O
is	O
discussed	O
mostly	O
in	O
chapter	O
.	O
normalizing	O
constants	O
of	O
proba-	O
bility	O
functions	O
come	O
up	O
in	O
inference	O
(	O
above	O
)	O
as	O
well	O
as	O
in	O
learning	O
.	O
many	O
probabilistic	O
models	O
involve	O
such	O
a	O
normalizing	O
constant	O
.	O
unfortunately	O
,	O
learning	O
such	O
a	O
model	B
often	O
requires	O
computing	O
the	O
gradient	O
of	O
the	O
loga-	O
rithm	O
of	O
the	O
partition	O
function	O
with	O
respect	O
to	O
the	O
model	B
parameters	O
.	O
that	O
computation	O
is	O
generally	O
as	O
intractable	O
as	O
computing	O
the	O
partition	O
function	O
itself	O
.	O
monte	O
carlo	O
markov	O
chain	O
(	O
mcmc	O
)	O
methods	O
(	O
chapter	O
)	O
are	O
of-	O
ten	O
used	O
to	O
deal	O
with	O
the	O
partition	O
function	O
(	O
computing	O
it	O
or	O
its	O
gradient	O
)	O
.	O
unfortunately	O
,	O
mcmc	O
methods	O
suﬀer	O
when	O
the	O
modes	O
of	O
the	O
model	B
distribu-	O
tion	B
are	O
numerous	O
and	O
well-separated	O
,	O
especially	O
in	O
high-dimensional	O
spaces	O
(	O
section	O
17.5	O
17	O
)	O
.	O
one	O
way	O
to	O
confront	O
these	O
intractable	O
computations	O
is	O
to	O
approximate	O
them	O
,	O
and	O
many	O
approaches	O
have	O
been	O
proposed	O
as	O
discussed	O
in	O
this	O
third	O
part	O
of	O
the	O
book	O
.	O
another	O
interesting	O
way	O
,	O
also	O
discussed	O
here	O
,	O
would	O
be	O
to	O
avoid	O
these	O
intractable	O
computations	O
altogether	O
by	O
design	O
,	O
and	O
methods	O
that	O
do	O
not	O
require	O
such	O
computations	O
are	O
thus	O
very	O
appealing	O
.	O
several	O
generative	O
models	O
have	O
been	O
proposed	O
in	O
recent	O
years	O
,	O
with	O
that	O
motivation	O
.	O
a	O
wide	O
variety	O
of	O
contemporary	O
approaches	O
to	O
generative	O
modeling	O
are	O
discussed	O
in	O
chapter	O
.20	O
iii	O
part	O
is	O
the	O
most	O
important	O
for	O
a	O
researcher—someone	O
who	O
wants	O
to	O
un-	O
derstand	O
the	O
breadth	O
of	O
perspectives	O
that	O
have	O
been	O
brought	O
to	O
the	O
ﬁeld	O
of	O
deep	O
learning	O
,	O
and	O
push	O
the	O
ﬁeld	O
forward	O
towards	O
true	O
artiﬁcial	O
intelligence	O
.	O
488	O
chapter	O
13	O
linear	O
factor	O
models	O
many	O
of	O
the	O
research	O
frontiers	O
in	O
deep	O
learning	O
involve	O
building	O
a	O
probabilistic	O
model	B
of	O
the	O
input	O
,	O
pmodel	O
(	O
x	O
)	O
.	O
such	O
a	O
model	B
can	O
,	O
in	O
principle	O
,	O
use	O
probabilistic	O
inference	O
to	O
predict	O
any	O
of	O
the	O
variables	O
in	O
its	O
environment	O
given	O
any	O
of	O
the	O
other	O
variables	O
.	O
many	O
of	O
these	O
models	O
also	O
have	O
latent	O
variables	O
h	O
,	O
with	O
pmodel	O
(	O
)	O
=	O
.	O
)	O
x	O
h	O
these	O
latent	O
variables	O
provide	O
another	O
means	O
of	O
representing	O
the	O
data	O
.	O
distributed	O
representations	O
based	O
on	O
latent	O
variables	O
can	O
obtain	O
all	O
of	O
the	O
advantages	O
of	O
representation	O
learning	O
that	O
we	O
have	O
seen	O
with	O
deep	O
feedforward	O
and	O
recurrent	O
networks	O
.	O
ehpmodel	O
(	O
x	O
|	O
in	O
this	O
chapter	O
,	O
we	O
describe	O
some	O
of	O
the	O
simplest	O
probabilistic	O
models	O
with	O
latent	O
variables	O
:	O
linear	O
factor	O
models	O
.	O
these	O
models	O
are	O
sometimes	O
used	O
as	O
building	O
1995a	O
ghahramani	O
and	O
hinton	O
1996	O
blocks	O
of	O
mixture	O
models	O
(	O
hinton	O
,	O
;	O
)	O
.	O
they	O
roweis	O
also	O
show	O
many	O
of	O
the	O
basic	O
approaches	O
necessary	O
to	O
build	O
generative	O
models	O
that	O
the	O
more	O
advanced	O
deep	O
models	O
will	O
extend	O
further.	O
)	O
or	O
larger	O
,	O
deep	O
probabilistic	O
models	O
(	O
et	O
al.	O
,	O
et	O
al.	O
,	O
et	O
al.	O
,	O
2002	O
tang	O
2012	O
;	O
a	O
linear	O
factor	O
model	B
is	O
deﬁned	O
by	O
the	O
use	O
of	O
a	O
stochastic	O
,	O
linear	O
decoder	O
function	O
that	O
generates	O
x	O
by	O
adding	O
noise	O
to	O
a	O
linear	O
transformation	O
of	O
.	O
h	O
these	O
models	O
are	O
interesting	O
because	O
they	O
allow	O
us	O
to	O
discover	O
explanatory	O
factors	O
that	O
have	O
a	O
simple	O
joint	O
distribution	O
.	O
the	O
simplicity	O
of	O
using	O
a	O
linear	O
decoder	O
made	O
these	O
models	O
some	O
of	O
the	O
ﬁrst	O
latent	O
variable	O
models	O
to	O
be	O
extensively	O
studied	O
.	O
a	O
linear	O
factor	O
model	B
describes	O
the	O
data	O
generation	O
process	O
as	O
follows	O
.	O
first	O
,	O
	O
we	O
sample	O
the	O
explanatory	O
factors	O
h	O
h	O
from	O
a	O
distribution	O
∼	O
p	O
,	O
(	O
)	O
h	O
(	O
13.1	O
)	O
where	O
p	O
(	O
h	O
)	O
is	O
a	O
factorial	O
distribution	O
,	O
with	O
p	O
(	O
h	O
)	O
=	O
i	O
p	O
(	O
hi	O
)	O
,	O
so	O
that	O
it	O
is	O
easy	O
to	O
489	O
chapter	O
13.	O
linear	O
factor	O
models	O
sample	O
from	O
.	O
next	O
we	O
sample	O
the	O
real-valued	O
observable	O
variables	O
given	O
the	O
factors	O
:	O
x	O
w	O
h	O
b	O
+	O
+	O
noise	O
=	O
(	O
13.2	O
)	O
where	O
the	O
noise	O
is	O
typically	O
gaussian	O
and	O
diagonal	O
(	O
independent	O
across	O
dimensions	O
)	O
.	O
this	O
is	O
illustrated	O
in	O
ﬁgure	O
.	O
13.1	O
h1h1	O
x1x1	O
h2h2	O
x2x2	O
h3h3	O
x3x3	O
x	O
x	O
=	O
w	O
+	O
+b	O
=	O
w	O
+	O
+b	O
h	O
h	O
n	O
ois	O
e	O
n	O
ois	O
e	O
figure	O
13.1	O
:	O
the	O
directed	O
graphical	O
model	B
describing	O
the	O
linear	O
factor	O
model	B
family	O
,	O
in	O
which	O
we	O
assume	O
that	O
an	O
observed	O
data	O
vector	O
x	O
is	O
obtained	O
by	O
a	O
linear	O
combination	O
of	O
independent	O
latent	O
factors	O
h	O
,	O
plus	O
some	O
noise	O
.	O
diﬀerent	O
models	O
,	O
such	O
as	O
probabilistic	O
pca	O
,	O
factor	O
analysis	O
or	O
ica	O
,	O
make	O
diﬀerent	O
choices	O
about	O
the	O
form	O
of	O
the	O
noise	O
and	O
of	O
the	O
prior	O
.	O
p	O
(	O
)	O
h	O
13.1	O
probabilistic	O
pca	O
and	O
factor	O
analysis	O
probabilistic	O
pca	O
(	O
principal	O
components	O
analysis	O
)	O
,	O
factor	O
analysis	O
and	O
other	O
linear	O
factor	O
models	O
are	O
special	O
cases	O
of	O
the	O
above	O
equations	O
(	O
)	O
and	O
only	O
diﬀer	O
in	O
the	O
choices	O
made	O
for	O
the	O
noise	O
distribution	O
and	O
the	O
model	B
’	O
s	O
prior	O
over	O
latent	O
variables	O
before	O
observing	O
13.1	O
13.2	O
and	O
.	O
x	O
h	O
bartholomew	O
1987	O
basilevsky	O
1994	O
;	O
,	O
)	O
,	O
the	O
latent	O
variable	O
in	O
factor	O
analysis	O
(	O
,	O
prior	O
is	O
just	O
the	O
unit	O
variance	O
gaussian	O
∼	O
n	O
h	O
(	O
;	O
h	O
,	O
i	O
)	O
0	O
(	O
13.3	O
)	O
while	O
the	O
observed	O
variables	O
xi	O
are	O
assumed	O
to	O
be	O
conditionally	O
independent	O
,	O
given	O
h.	O
speciﬁcally	O
,	O
the	O
noise	O
is	O
assumed	O
to	O
be	O
drawn	O
from	O
a	O
diagonal	O
co-	O
variance	O
gaussian	O
distribution	O
,	O
with	O
covariance	O
matrix	O
ψ	O
=	O
diag	O
(	O
σ2	O
)	O
,	O
with	O
σ2	O
=	O
[	O
σ2	O
a	O
vector	O
of	O
per-variable	O
variances	O
.	O
	O
1	O
,	O
σ2	O
2	O
,	O
.	O
.	O
.	O
,	O
σ2	O
n	O
]	O
the	O
role	O
of	O
the	O
latent	O
variables	O
is	O
thus	O
to	O
capture	O
the	O
dependencies	O
between	O
the	O
diﬀerent	O
observed	O
variables	O
xi	O
.	O
indeed	O
,	O
it	O
can	O
easily	O
be	O
shown	O
that	O
x	O
is	O
just	O
a	O
multivariate	O
normal	O
random	O
variable	O
,	O
with	O
∼	O
n	O
x	O
	O
(	O
;	O
x	O
b	O
w	O
w	O
,	O
490	O
+	O
)	O
ψ	O
.	O
(	O
13.4	O
)	O
chapter	O
13.	O
linear	O
factor	O
models	O
in	O
order	O
to	O
cast	O
pca	O
in	O
a	O
probabilistic	O
framework	O
,	O
we	O
can	O
make	O
a	O
slight	O
modiﬁcation	O
to	O
the	O
factor	O
analysis	O
model	B
,	O
making	O
the	O
conditional	O
variances	O
σ2	O
i	O
+	O
σ2	O
i	O
,	O
where	O
equal	O
to	O
each	O
other	O
.	O
in	O
that	O
case	O
the	O
covariance	O
of	O
x	O
is	O
just	O
w	O
w	O
σ2	O
is	O
now	O
a	O
scalar	O
.	O
this	O
yields	O
the	O
conditional	O
distribution	O
	O
x	O
or	O
equivalently	O
∼	O
n	O
	O
(	O
;	O
x	O
b	O
w	O
w	O
,	O
+	O
σ	O
2i	O
)	O
x	O
=	O
w	O
+	O
+b	O
h	O
σ	O
z	O
(	O
13.5	O
)	O
(	O
13.6	O
)	O
∼	O
n	O
→	O
where	O
z	O
iterative	O
em	O
algorithm	O
for	O
estimating	O
the	O
parameters	O
tipping	O
and	O
bishop	O
1999	O
σ2	O
.	O
(	O
z	O
;	O
0	O
,	O
i	O
)	O
is	O
gaussian	O
noise	O
.	O
(	O
andw	O
)	O
then	O
show	O
an	O
this	O
probabilistic	O
pca	O
model	B
takes	O
advantage	O
of	O
the	O
observation	O
that	O
most	O
variations	O
in	O
the	O
data	O
can	O
be	O
captured	O
by	O
the	O
latent	O
variables	O
h	O
,	O
up	O
to	O
some	O
small	O
residual	O
reconstruction	O
error	O
σ2	O
.	O
as	O
shown	O
by	O
)	O
,	O
0.	O
in	O
that	O
case	O
,	O
the	O
conditional	O
expected	O
probabilistic	O
pca	O
becomes	O
pca	O
as	O
σ	O
value	O
of	O
h	O
given	O
x	O
becomes	O
an	O
orthogonal	O
projection	O
of	O
x	O
onto	O
the	O
space	O
spanned	O
by	O
the	O
tipping	O
and	O
bishop	O
1999	O
,	O
like	O
in	O
pca	O
.	O
columns	O
of	O
→	O
−	O
w	O
d	O
b	O
(	O
as	O
σ	O
0	O
,	O
the	O
density	O
model	B
deﬁned	O
by	O
probabilistic	O
pca	O
becomes	O
very	O
sharp	O
around	O
these	O
d	O
dimensions	O
spanned	O
by	O
the	O
columns	O
of	O
w	O
.	O
this	O
can	O
make	O
the	O
model	B
assign	O
very	O
low	O
likelihood	O
to	O
the	O
data	O
if	O
the	O
data	O
does	O
not	O
actually	O
cluster	O
near	O
a	O
hyperplane	O
.	O
13.2	O
independent	O
component	O
analysis	O
(	O
ica	O
)	O
;	O
,	O
independent	O
component	O
analysis	O
(	O
ica	O
)	O
is	O
among	O
the	O
oldest	O
representation	O
learning	O
;	O
herault	O
and	O
ans	O
1984	O
jutten	O
and	O
herault	O
1991	O
comon	O
1994	O
algorithms	O
(	O
hyvärinen	O
1999	O
hyvärinen	O
)	O
.	O
2003	O
it	O
is	O
an	O
approach	O
to	O
modeling	O
linear	O
factors	O
that	O
seeks	O
to	O
separate	O
an	O
observed	O
signal	O
into	O
many	O
underlying	O
signals	O
that	O
are	O
scaled	O
and	O
added	O
together	O
to	O
form	O
the	O
observed	O
data	O
.	O
these	O
signals	O
are	O
intended	O
to	O
be	O
fully	O
independent	O
,	O
rather	O
than	O
merely	O
decorrelated	O
from	O
each	O
other.1	O
;	O
2001a	O
hinton	O
;	O
,	O
2001	O
teh	O
,	O
et	O
al.	O
,	O
,	O
et	O
al.	O
,	O
et	O
al.	O
,	O
;	O
;	O
many	O
diﬀerent	O
speciﬁc	O
methodologies	O
are	O
referred	O
to	O
as	O
ica	O
.	O
the	O
variant	O
that	O
is	O
most	O
similar	O
to	O
the	O
other	O
generative	O
models	O
we	O
have	O
described	O
here	O
is	O
a	O
variant	O
(	O
)	O
that	O
trains	O
a	O
fully	O
parametric	O
generative	O
model	B
.	O
the	O
prior	O
distribution	O
over	O
the	O
underlying	O
factors	O
,	O
p	O
(	O
h	O
)	O
,	O
must	O
be	O
ﬁxed	O
ahead	O
of	O
time	O
by	O
the	O
user	O
.	O
the	O
model	B
then	O
deterministically	O
generates	O
x	O
=	O
w	O
h.	O
we	O
can	O
perform	O
a	O
pham	O
et	O
al	O
.	O
1992	O
,	O
1see	O
section	O
dent	O
variables	O
.	O
3.8	O
for	O
a	O
discussion	O
of	O
the	O
diﬀerence	O
between	O
uncorrelated	O
variables	O
and	O
indepen-	O
491	O
chapter	O
13.	O
linear	O
factor	O
models	O
nonlinear	O
change	O
of	O
variables	O
(	O
using	O
equation	O
the	O
model	B
then	O
proceeds	O
as	O
usual	O
,	O
using	O
maximum	O
likelihood.	O
)	O
to	O
determine	O
3.47	O
p	O
(	O
x	O
)	O
.	O
learning	O
the	O
motivation	O
for	O
this	O
approach	O
is	O
that	O
by	O
choosing	O
p	O
(	O
h	O
)	O
to	O
be	O
independent	O
,	O
we	O
can	O
recover	O
underlying	O
factors	O
that	O
are	O
as	O
close	O
as	O
possible	O
to	O
independent	O
.	O
this	O
is	O
commonly	O
used	O
,	O
not	O
to	O
capture	O
high-level	O
abstract	O
causal	O
factors	O
,	O
but	O
to	O
recover	O
low-level	O
signals	O
that	O
have	O
been	O
mixed	O
together	O
.	O
in	O
this	O
setting	O
,	O
each	O
training	O
example	O
is	O
one	O
moment	O
in	O
time	O
,	O
each	O
xi	O
is	O
one	O
sensor	O
’	O
s	O
observation	O
of	O
the	O
mixed	O
signals	O
,	O
and	O
each	O
hi	O
is	O
one	O
estimate	O
of	O
one	O
of	O
the	O
original	O
signals	O
.	O
for	O
example	O
,	O
we	O
might	O
have	O
n	O
people	O
speaking	O
simultaneously	O
.	O
if	O
we	O
have	O
n	O
diﬀerent	O
microphones	O
placed	O
in	O
diﬀerent	O
locations	O
,	O
ica	O
can	O
detect	O
the	O
changes	O
in	O
the	O
volume	O
between	O
each	O
speaker	O
as	O
heard	O
by	O
each	O
microphone	O
,	O
and	O
separate	O
the	O
signals	O
so	O
that	O
each	O
h	O
i	O
contains	O
only	O
one	O
person	O
speaking	O
clearly	O
.	O
this	O
is	O
commonly	O
used	O
in	O
neuroscience	O
for	O
electroencephalography	O
,	O
a	O
technology	O
for	O
recording	O
electrical	O
signals	O
originating	O
in	O
the	O
brain	O
.	O
many	O
electrode	O
sensors	O
placed	O
on	O
the	O
subject	O
’	O
s	O
head	O
are	O
used	O
to	O
measure	O
many	O
electrical	O
signals	O
coming	O
from	O
the	O
body	O
.	O
the	O
experimenter	O
is	O
typically	O
only	O
interested	O
in	O
signals	O
from	O
the	O
brain	O
,	O
but	O
signals	O
from	O
the	O
subject	O
’	O
s	O
heart	O
and	O
eyes	O
are	O
strong	O
enough	O
to	O
confound	O
measurements	O
taken	O
at	O
the	O
subject	O
’	O
s	O
scalp	O
.	O
the	O
signals	O
arrive	O
at	O
the	O
electrodes	O
mixed	O
together	O
,	O
so	O
ica	O
is	O
necessary	O
to	O
separate	O
the	O
electrical	O
signature	O
of	O
the	O
heart	O
from	O
the	O
signals	O
originating	O
in	O
the	O
brain	O
,	O
and	O
to	O
separate	O
signals	O
in	O
diﬀerent	O
brain	O
regions	O
from	O
each	O
other	O
.	O
as	O
mentioned	O
before	O
,	O
many	O
variants	O
of	O
ica	O
are	O
possible	O
.	O
some	O
add	O
some	O
noise	O
in	O
the	O
generation	O
of	O
x	O
rather	O
than	O
using	O
a	O
deterministic	O
decoder	O
.	O
most	O
do	O
not	O
use	O
the	O
maximum	O
likelihood	O
criterion	O
,	O
but	O
instead	O
aim	O
to	O
make	O
the	O
elements	O
of	O
−	O
1x	O
independent	O
from	O
each	O
other	O
.	O
many	O
criteria	O
that	O
accomplish	O
this	O
goal	O
h	O
=	O
w	O
w	O
,	O
which	O
can	O
be	O
are	O
possible	O
.	O
equation	O
an	O
expensive	O
and	O
numerically	O
unstable	O
operation	O
.	O
some	O
variants	O
of	O
ica	O
avoid	O
this	O
problematic	O
operation	O
by	O
constraining	O
requires	O
taking	O
the	O
determinant	O
of	O
to	O
be	O
orthogonal	O
.	O
3.47	O
w	O
all	O
variants	O
of	O
ica	O
require	O
that	O
p	O
(	O
h	O
)	O
be	O
non-gaussian	O
.	O
this	O
is	O
because	O
if	O
p	O
(	O
h	O
)	O
is	O
an	O
independent	O
prior	O
with	O
gaussian	O
components	O
,	O
then	O
w	O
is	O
not	O
identiﬁable	O
.	O
we	O
can	O
obtain	O
the	O
same	O
distribution	O
over	O
p	O
(	O
x	O
)	O
for	O
many	O
values	O
of	O
w	O
.	O
this	O
is	O
very	O
diﬀerent	O
from	O
other	O
linear	O
factor	O
models	O
like	O
probabilistic	O
pca	O
and	O
factor	O
analysis	O
,	O
that	O
often	O
require	O
p	O
(	O
h	O
)	O
to	O
be	O
gaussian	O
in	O
order	O
to	O
make	O
many	O
operations	O
on	O
the	O
model	B
have	O
closed	O
form	O
solutions	O
.	O
in	O
the	O
maximum	O
likelihood	O
approach	O
where	O
the	O
user	O
explicitly	O
speciﬁes	O
the	O
distribution	O
,	O
a	O
typical	O
choice	O
is	O
to	O
use	O
p	O
(	O
hi	O
)	O
=	O
d	O
σ	O
(	O
hi	O
)	O
.	O
dhi	O
typical	O
choices	O
of	O
these	O
non-gaussian	O
distributions	O
have	O
larger	O
peaks	O
near	O
0	O
than	O
does	O
the	O
gaussian	O
distribution	O
,	O
so	O
we	O
can	O
also	O
see	O
most	O
implementations	O
of	O
ica	O
as	O
learning	O
sparse	O
features	O
.	O
492	O
chapter	O
13.	O
linear	O
factor	O
models	O
many	O
variants	O
of	O
ica	O
are	O
not	O
generative	O
models	O
in	O
the	O
sense	O
that	O
we	O
use	O
the	O
phrase	O
.	O
in	O
this	O
book	O
,	O
a	O
generative	O
model	B
either	O
represents	O
p	O
(	O
x	O
)	O
or	O
can	O
draw	O
samples	O
from	O
it	O
.	O
many	O
variants	O
of	O
ica	O
only	O
know	O
how	O
to	O
transform	O
between	O
x	O
and	O
h	O
,	O
but	O
do	O
not	O
have	O
any	O
way	O
of	O
representing	O
p	O
(	O
h	O
)	O
,	O
and	O
thus	O
do	O
not	O
impose	O
a	O
distribution	O
over	O
p	O
(	O
x	O
)	O
.	O
for	O
example	O
,	O
many	O
ica	O
variants	O
aim	O
to	O
increase	O
the	O
sample	O
kurtosis	O
of	O
−	O
1x	O
,	O
because	O
high	O
kurtosis	O
indicates	O
that	O
p	O
(	O
h	O
)	O
is	O
non-gaussian	O
,	O
but	O
this	O
is	O
h	O
=	O
w	O
accomplished	O
without	O
explicitly	O
representing	O
p	O
(	O
h	O
)	O
.	O
this	O
is	O
because	O
ica	O
is	O
more	O
often	O
used	O
as	O
an	O
analysis	O
tool	O
for	O
separating	O
signals	O
,	O
rather	O
than	O
for	O
generating	O
data	O
or	O
estimating	O
its	O
density	O
.	O
(	O
(	O
,	O
14	O
)	O
and	O
roberts	O
and	O
everson	O
2001	O
just	O
as	O
pca	O
can	O
be	O
generalized	O
to	O
the	O
nonlinear	O
autoencoders	O
described	O
in	O
chapter	O
,	O
ica	O
can	O
be	O
generalized	O
to	O
a	O
nonlinear	O
generative	O
model	B
,	O
in	O
which	O
we	O
use	O
a	O
nonlinear	O
function	O
f	O
to	O
generate	O
the	O
observed	O
data	O
.	O
see	O
hyvärinen	O
and	O
pajunen	O
1999	O
)	O
for	O
the	O
initial	O
work	B
on	O
nonlinear	O
ica	O
and	O
its	O
successful	O
use	O
with	O
ensemble	O
learning	O
by	O
lappalainen	O
et	O
al	O
.	O
2000	O
)	O
.	O
another	O
nonlinear	O
extension	O
of	O
ica	O
is	O
the	O
approach	O
of	O
nonlinear	O
independent	O
components	O
estimation	O
,	O
or	O
nice	O
(	O
)	O
,	O
which	O
stacks	O
a	O
series	O
of	O
invertible	O
transformations	O
(	O
encoder	O
stages	O
)	O
that	O
have	O
the	O
property	O
that	O
the	O
determinant	O
of	O
the	O
jacobian	O
of	O
each	O
transformation	O
can	O
be	O
computed	O
eﬃciently	O
.	O
this	O
makes	O
it	O
possible	O
to	O
compute	O
the	O
likelihood	O
exactly	O
and	O
,	O
like	O
ica	O
,	O
attempts	O
to	O
transform	O
the	O
data	O
into	O
a	O
space	O
where	O
it	O
has	O
a	O
factorized	O
marginal	O
distribution	O
,	O
but	O
is	O
more	O
likely	O
to	O
succeed	O
thanks	O
to	O
the	O
nonlinear	O
encoder	O
.	O
because	O
the	O
encoder	O
is	O
associated	O
with	O
a	O
decoder	O
that	O
is	O
its	O
perfect	O
inverse	O
,	O
it	O
is	O
straightforward	O
to	O
generate	O
samples	O
from	O
the	O
model	B
(	O
by	O
ﬁrst	O
sampling	O
from	O
p	O
(	O
h	O
)	O
and	O
then	O
applying	O
the	O
decoder	O
)	O
.	O
dinh	O
et	O
al	O
.	O
2014	O
(	O
;	O
,	O
et	O
al.	O
,	O
2001b	O
another	O
generalization	O
of	O
ica	O
is	O
to	O
learn	O
groups	O
of	O
features	O
,	O
with	O
statistical	O
dependence	O
allowed	O
within	O
a	O
group	O
but	O
discouraged	O
between	O
groups	O
(	O
hyvärinen	O
and	O
hoyer	O
1999	O
hyvärinen	O
)	O
.	O
when	O
the	O
groups	O
of	O
related	O
units	O
are	O
chosen	O
to	O
be	O
non-overlapping	O
,	O
this	O
is	O
called	O
independent	O
subspace	O
analysis	O
.	O
it	O
is	O
also	O
possible	O
to	O
assign	O
spatial	O
coordinates	O
to	O
each	O
hidden	O
unit	O
and	O
form	O
overlapping	O
groups	O
of	O
spatially	O
neighboring	O
units	O
.	O
this	O
encourages	O
nearby	O
units	O
to	O
learn	O
similar	O
features	O
.	O
when	O
applied	O
to	O
natural	O
images	O
,	O
this	O
topographic	O
ica	O
approach	O
learns	O
gabor	O
ﬁlters	O
,	O
such	O
that	O
neighboring	O
features	O
have	O
similar	O
orientation	O
,	O
location	O
or	O
frequency	O
.	O
many	O
diﬀerent	O
phase	O
oﬀsets	O
of	O
similar	O
gabor	O
functions	O
occur	O
within	O
each	O
region	O
,	O
so	O
that	O
pooling	O
over	O
small	O
regions	O
yields	O
translation	O
invariance	O
.	O
13.3	O
slow	O
feature	O
analysis	O
slow	O
feature	O
analysis	O
(	O
sfa	O
)	O
is	O
a	O
linear	O
factor	O
model	B
that	O
uses	O
information	O
from	O
493	O
chapter	O
13.	O
linear	O
factor	O
models	O
time	O
signals	O
to	O
learn	O
invariant	O
features	O
(	O
wiskott	O
and	O
sejnowski	O
2002	O
,	O
)	O
.	O
slow	O
feature	O
analysis	O
is	O
motivated	O
by	O
a	O
general	O
principle	O
called	O
the	O
slowness	O
principle	O
.	O
the	O
idea	O
is	O
that	O
the	O
important	O
characteristics	O
of	O
scenes	O
change	O
very	O
slowly	O
compared	O
to	O
the	O
individual	O
measurements	O
that	O
make	O
up	O
a	O
description	O
of	O
a	O
scene	O
.	O
for	O
example	O
,	O
in	O
computer	O
vision	O
,	O
individual	O
pixel	O
values	O
can	O
change	O
very	O
rapidly	O
.	O
if	O
a	O
zebra	O
moves	O
from	O
left	O
to	O
right	O
across	O
the	O
image	O
,	O
an	O
individual	O
pixel	O
will	O
rapidly	O
change	O
from	O
black	O
to	O
white	O
and	O
back	O
again	O
as	O
the	O
zebra	O
’	O
s	O
stripes	O
pass	O
over	O
the	O
pixel	O
.	O
by	O
comparison	O
,	O
the	O
feature	O
indicating	O
whether	O
a	O
zebra	O
is	O
in	O
the	O
image	O
will	O
not	O
change	O
at	O
all	O
,	O
and	O
the	O
feature	O
describing	O
the	O
zebra	O
’	O
s	O
position	B
will	O
change	O
slowly	O
.	O
we	O
therefore	O
may	O
wish	O
to	O
regularize	O
our	O
model	B
to	O
learn	O
features	O
that	O
change	O
slowly	O
over	O
time	O
.	O
the	O
slowness	O
principle	O
predates	O
slow	O
feature	O
analysis	O
and	O
has	O
been	O
applied	O
hinton	O
1989	O
földiák	O
1989	O
mobahi	O
et	O
al	O
.	O
2009	O
;	O
to	O
a	O
wide	O
variety	O
of	O
models	O
(	O
bergstra	O
and	O
bengio	O
2009	O
)	O
.	O
in	O
general	O
,	O
we	O
can	O
apply	O
the	O
slowness	O
principle	O
to	O
any	O
diﬀerentiable	O
model	B
trained	O
with	O
gradient	O
descent	B
.	O
the	O
slowness	O
principle	O
may	O
be	O
introduced	O
by	O
adding	O
a	O
term	O
to	O
the	O
cost	O
function	O
of	O
the	O
form	O
	O
,	O
;	O
,	O
;	O
,	O
,	O
λ	O
l	O
f	O
(	O
(	O
x	O
(	O
+1	O
)	O
t	O
(	O
,	O
f	O
x	O
(	O
)	O
t	O
)	O
)	O
)	O
(	O
13.7	O
)	O
t	O
where	O
λ	O
is	O
a	O
hyperparameter	O
determining	O
the	O
strength	O
of	O
the	O
slowness	O
regularization	O
term	O
,	O
t	O
is	O
the	O
index	O
into	O
a	O
time	O
sequence	O
of	O
examples	O
,	O
f	O
is	O
the	O
feature	O
extractor	O
to	O
be	O
regularized	O
,	O
and	O
l	O
is	O
a	O
loss	O
function	O
measuring	O
the	O
distance	O
between	O
f	O
(	O
x	O
(	O
)	O
t	O
)	O
and	O
f	O
(	O
x	O
(	O
+1	O
)	O
is	O
the	O
mean	O
squared	O
diﬀerence	O
.	O
)	O
.	O
a	O
common	O
choice	O
for	O
l	O
t	O
slow	O
feature	O
analysis	O
is	O
a	O
particularly	O
eﬃcient	O
application	O
of	O
the	O
slowness	O
principle	O
.	O
it	O
is	O
eﬃcient	O
because	O
it	O
is	O
applied	O
to	O
a	O
linear	O
feature	O
extractor	O
,	O
and	O
can	O
thus	O
be	O
trained	O
in	O
closed	O
form	O
.	O
like	O
some	O
variants	O
of	O
ica	O
,	O
sfa	O
is	O
not	O
quite	O
a	O
generative	O
model	B
per	O
se	O
,	O
in	O
the	O
sense	O
that	O
it	O
deﬁnes	O
a	O
linear	O
map	O
between	O
input	O
space	O
and	O
feature	O
space	O
but	O
does	O
not	O
deﬁne	O
a	O
prior	O
over	O
feature	O
space	O
and	O
thus	O
does	O
not	O
impose	O
a	O
distribution	O
on	O
input	O
space	O
.	O
p	O
(	O
)	O
x	O
the	O
sfa	O
algorithm	O
(	O
wiskott	O
and	O
sejnowski	O
2002	O
,	O
)	O
consists	O
of	O
deﬁning	O
f	O
(	O
x	O
;	O
θ	O
)	O
to	O
be	O
a	O
linear	O
transformation	O
,	O
and	O
solving	O
the	O
optimization	O
problem	O
−	O
et	O
(	O
(	O
f	O
x	O
(	O
+1	O
)	O
t	O
)	O
i	O
min	O
θ	O
f	O
(	O
x	O
(	O
)	O
t	O
)	O
i	O
)	O
2	O
subject	O
to	O
the	O
constraints	O
and	O
etf	O
(	O
x	O
(	O
)	O
t	O
)	O
i	O
=	O
0	O
et	O
[	O
(	O
f	O
x	O
(	O
)	O
t	O
)	O
2	O
494	O
i	O
]	O
=	O
1	O
.	O
(	O
13.8	O
)	O
(	O
13.9	O
)	O
(	O
13.10	O
)	O
chapter	O
13.	O
linear	O
factor	O
models	O
the	O
constraint	O
that	O
the	O
learned	O
feature	O
have	O
zero	O
mean	O
is	O
necessary	O
to	O
make	O
the	O
problem	O
have	O
a	O
unique	O
solution	O
;	O
otherwise	O
we	O
could	O
add	O
a	O
constant	O
to	O
all	O
feature	O
values	O
and	O
obtain	O
a	O
diﬀerent	O
solution	O
with	O
equal	O
value	O
of	O
the	O
slowness	O
objective	O
.	O
the	O
constraint	O
that	O
the	O
features	O
have	O
unit	O
variance	O
is	O
necessary	O
to	O
prevent	O
the	O
pathological	O
solution	O
where	O
all	O
features	O
collapse	O
to	O
.	O
like	O
pca	O
,	O
the	O
sfa	O
features	O
are	O
ordered	O
,	O
with	O
the	O
ﬁrst	O
feature	O
being	O
the	O
slowest	O
.	O
to	O
learn	O
multiple	O
features	O
,	O
we	O
must	O
also	O
add	O
the	O
constraint	O
0	O
∀	O
i	O
<	O
j	O
,	O
et	O
[	O
(	O
f	O
x	O
(	O
)	O
t	O
)	O
if	O
(	O
x	O
(	O
)	O
t	O
)	O
j	O
]	O
=	O
0	O
.	O
(	O
13.11	O
)	O
this	O
speciﬁes	O
that	O
the	O
learned	O
features	O
must	O
be	O
linearly	O
decorrelated	O
from	O
each	O
other	O
.	O
without	O
this	O
constraint	O
,	O
all	O
of	O
the	O
learned	O
features	O
would	O
simply	O
capture	O
the	O
one	O
slowest	O
signal	O
.	O
one	O
could	O
imagine	O
using	O
other	O
mechanisms	O
,	O
such	O
as	O
minimizing	O
reconstruction	O
error	O
,	O
to	O
force	O
the	O
features	O
to	O
diversify	O
,	O
but	O
this	O
decorrelation	O
mechanism	O
admits	O
a	O
simple	O
solution	O
due	O
to	O
the	O
linearity	O
of	O
sfa	O
features	O
.	O
the	O
sfa	O
problem	O
may	O
be	O
solved	O
in	O
closed	O
form	O
by	O
a	O
linear	O
algebra	O
package	O
.	O
sfa	O
is	O
typically	O
used	O
to	O
learn	O
nonlinear	O
features	O
by	O
applying	O
a	O
nonlinear	O
basis	O
expansion	O
to	O
x	O
before	O
running	O
sfa	O
.	O
for	O
example	O
,	O
it	O
is	O
common	O
to	O
replace	O
x	O
by	O
the	O
quadratic	O
basis	O
expansion	O
,	O
a	O
vector	O
containing	O
elements	O
x	O
ixj	O
for	O
all	O
i	O
and	O
j.	O
linear	O
sfa	O
modules	O
may	O
then	O
be	O
composed	O
to	O
learn	O
deep	O
nonlinear	O
slow	O
feature	O
extractors	O
by	O
repeatedly	O
learning	O
a	O
linear	O
sfa	O
feature	O
extractor	O
,	O
applying	O
a	O
nonlinear	O
basis	O
expansion	O
to	O
its	O
output	O
,	O
and	O
then	O
learning	O
another	O
linear	O
sfa	O
feature	O
extractor	O
on	O
top	O
of	O
that	O
expansion	O
.	O
when	O
trained	O
on	O
small	O
spatial	O
patches	O
of	O
videos	O
of	O
natural	O
scenes	O
,	O
sfa	O
with	O
quadratic	O
basis	O
expansions	O
learns	O
features	O
that	O
share	O
many	O
characteristics	O
with	O
those	O
of	O
complex	O
cells	O
in	O
v1	O
cortex	O
(	O
berkes	O
and	O
wiskott	O
2005	O
)	O
.	O
when	O
trained	O
on	O
videos	O
of	O
random	O
motion	O
within	O
3-d	O
computer	O
rendered	O
environments	O
,	O
deep	O
sfa	O
learns	O
features	O
that	O
share	O
many	O
characteristics	O
with	O
the	O
features	O
represented	O
by	O
neurons	O
in	O
rat	O
brains	O
that	O
are	O
used	O
for	O
navigation	O
(	O
franzius	O
)	O
.	O
sfa	O
thus	O
seems	O
to	O
be	O
a	O
reasonably	O
biologically	O
plausible	O
model	B
.	O
et	O
al.	O
,	O
2007	O
,	O
a	O
major	O
advantage	O
of	O
sfa	O
is	O
that	O
it	O
is	O
possibly	O
to	O
theoretically	O
predict	O
which	O
features	O
sfa	O
will	O
learn	O
,	O
even	O
in	O
the	O
deep	O
,	O
nonlinear	O
setting	O
.	O
to	O
make	O
such	O
theoretical	O
predictions	O
,	O
one	O
must	O
know	O
about	O
the	O
dynamics	O
of	O
the	O
environment	O
in	O
terms	O
of	O
conﬁguration	O
space	O
(	O
e.g.	O
,	O
in	O
the	O
case	O
of	O
random	O
motion	O
in	O
the	O
3-d	O
rendered	O
environment	O
,	O
the	O
theoretical	O
analysis	O
proceeds	O
from	O
knowledge	O
of	O
the	O
probability	O
distribution	O
over	O
position	B
and	O
velocity	O
of	O
the	O
camera	O
)	O
.	O
given	O
the	O
knowledge	O
of	O
how	O
the	O
underlying	O
factors	O
actually	O
change	O
,	O
it	O
is	O
possible	O
to	O
analytically	O
solve	O
for	O
the	O
optimal	O
functions	O
expressing	O
these	O
factors	O
.	O
in	O
practice	O
,	O
experiments	O
with	O
deep	O
sfa	O
applied	O
to	O
simulated	O
data	O
seem	O
to	O
recover	O
the	O
theoretically	O
predicted	O
functions	O
.	O
495	O
chapter	O
13.	O
linear	O
factor	O
models	O
this	O
is	O
in	O
comparison	O
to	O
other	O
learning	O
algorithms	O
where	O
the	O
cost	O
function	O
depends	O
highly	O
on	O
speciﬁc	O
pixel	O
values	O
,	O
making	O
it	O
much	O
more	O
diﬃcult	O
to	O
determine	O
what	O
features	O
the	O
model	B
will	O
learn	O
.	O
2008	O
et	O
al.	O
,	O
deep	O
sfa	O
has	O
also	O
been	O
used	O
to	O
learn	O
features	O
for	O
object	O
recognition	B
and	O
pose	O
estimation	O
(	O
franzius	O
)	O
.	O
so	O
far	O
,	O
the	O
slowness	O
principle	O
has	O
not	O
become	O
the	O
basis	O
for	O
any	O
state	O
of	O
the	O
art	O
applications	O
.	O
it	O
is	O
unclear	O
what	O
factor	O
has	O
limited	O
its	O
performance	O
.	O
we	O
speculate	O
that	O
perhaps	O
the	O
slowness	O
prior	O
is	O
too	O
strong	O
,	O
and	O
that	O
,	O
rather	O
than	O
imposing	O
a	O
prior	O
that	O
features	O
should	O
be	O
approximately	O
constant	O
,	O
it	O
would	O
be	O
better	O
to	O
impose	O
a	O
prior	O
that	O
features	O
should	O
be	O
easy	O
to	O
predict	O
from	O
one	O
time	O
step	O
to	O
the	O
next	O
.	O
the	O
position	B
of	O
an	O
object	O
is	O
a	O
useful	O
feature	O
regardless	O
of	O
whether	O
the	O
object	O
’	O
s	O
velocity	O
is	O
high	O
or	O
low	O
,	O
but	O
the	O
slowness	O
principle	O
encourages	O
the	O
model	B
to	O
ignore	O
the	O
position	B
of	O
objects	O
that	O
have	O
high	O
velocity	O
.	O
13.4	O
sparse	O
coding	O
,	O
olshausen	O
and	O
field	O
1996	O
sparse	O
coding	O
(	O
)	O
is	O
a	O
linear	O
factor	O
model	B
that	O
has	O
been	O
heavily	O
studied	O
as	O
an	O
unsupervised	O
feature	O
learning	O
and	O
feature	O
extraction	O
mechanism	O
.	O
strictly	O
speaking	O
,	O
the	O
term	O
“	O
sparse	O
coding	O
”	O
refers	O
to	O
the	O
process	O
of	O
inferring	O
the	O
value	O
of	O
h	O
in	O
this	O
model	B
,	O
while	O
“	O
sparse	O
modeling	O
”	O
refers	O
to	O
the	O
process	O
of	O
designing	O
and	O
learning	O
the	O
model	B
,	O
but	O
the	O
term	O
“	O
sparse	O
coding	O
”	O
is	O
often	O
used	O
to	O
refer	O
to	O
both	O
.	O
like	O
most	O
other	O
linear	O
factor	O
models	O
,	O
it	O
uses	O
a	O
linear	O
decoder	O
plus	O
noise	O
to	O
obtain	O
reconstructions	O
of	O
x	O
,	O
as	O
speciﬁed	O
in	O
equation	O
.	O
more	O
speciﬁcally	O
,	O
sparse	O
coding	O
models	O
typically	O
assume	O
that	O
the	O
linear	O
factors	O
have	O
gaussian	O
noise	O
with	O
isotropic	O
precision	O
:	O
β	O
13.2	O
|	O
n	O
)	O
=	O
(	O
p	O
x	O
h	O
(	O
;	O
x	O
w	O
h	O
b	O
,	O
+	O
1	O
β	O
i	O
)	O
.	O
(	O
13.12	O
)	O
the	O
distribution	O
p	O
(	O
h	O
)	O
is	O
chosen	O
to	O
be	O
one	O
with	O
sharp	O
peaks	O
near	O
0	O
(	O
olshausen	O
and	O
field	O
1996	O
)	O
.	O
common	O
choices	O
include	O
factorized	O
laplace	O
,	O
cauchy	O
or	O
factorized	O
student-t	O
distributions	O
.	O
for	O
example	O
,	O
the	O
laplace	O
prior	O
parametrized	O
in	O
terms	O
of	O
the	O
sparsity	O
penalty	O
coeﬃcient	O
is	O
given	O
by	O
λ	O
,	O
p	O
h	O
(	O
i	O
)	O
=	O
laplace	O
(	O
hi	O
;	O
0	O
,	O
2	O
λ	O
)	O
=	O
and	O
the	O
student-	O
prior	O
by	O
t	O
∝	O
p	O
h	O
(	O
i	O
)	O
1	O
(	O
1	O
+	O
h2	O
ν	O
)	O
ν+1	O
2	O
i	O
496	O
−	O
1	O
e	O
2	O
|	O
|	O
λ	O
h	O
i	O
λ	O
4	O
.	O
(	O
13.13	O
)	O
(	O
13.14	O
)	O
chapter	O
13.	O
linear	O
factor	O
models	O
training	O
sparse	O
coding	O
with	O
maximum	O
likelihood	O
is	O
intractable	O
.	O
instead	O
,	O
the	O
training	O
alternates	O
between	O
encoding	O
the	O
data	O
and	O
training	O
the	O
decoder	O
to	O
better	O
reconstruct	O
the	O
data	O
given	O
the	O
encoding	O
.	O
this	O
approach	O
will	O
be	O
justiﬁed	O
further	O
as	O
a	O
principled	O
approximation	O
to	O
maximum	O
likelihood	O
later	O
,	O
in	O
section	O
.	O
19.3	O
for	O
models	O
such	O
as	O
pca	O
,	O
we	O
have	O
seen	O
the	O
use	O
of	O
a	O
parametric	O
encoder	O
function	O
that	O
predicts	O
h	O
and	O
consists	O
only	O
of	O
multiplication	O
by	O
a	O
weight	O
matrix	O
.	O
the	O
encoder	O
that	O
we	O
use	O
with	O
sparse	O
coding	O
is	O
not	O
a	O
parametric	O
encoder	O
.	O
instead	O
,	O
the	O
encoder	O
is	O
an	O
optimization	O
algorithm	O
,	O
that	O
solves	O
an	O
optimization	O
problem	O
in	O
which	O
we	O
seek	O
the	O
single	O
most	O
likely	O
code	O
value	O
:	O
when	O
combined	O
with	O
equation	O
optimization	O
problem	O
:	O
13.13	O
and	O
equation	O
13.12	O
,	O
this	O
yields	O
the	O
following	O
∗	O
h	O
=	O
(	O
)	O
=	O
arg	O
max	O
f	O
x	O
h	O
|	O
(	O
.	O
)	O
h	O
x	O
p	O
arg	O
max	O
h	O
p	O
(	O
|	O
)	O
h	O
x	O
|	O
=	O
arg	O
max	O
h	O
=	O
arg	O
min	O
h	O
)	O
log	O
(	O
p	O
h	O
x	O
||	O
||	O
||	O
||	O
−	O
x	O
w	O
h	O
2	O
λ	O
h	O
1	O
+	O
β	O
2	O
,	O
(	O
13.15	O
)	O
(	O
13.16	O
)	O
(	O
13.17	O
)	O
(	O
13.18	O
)	O
where	O
we	O
have	O
dropped	O
terms	O
not	O
depending	O
on	O
h	O
and	O
divided	O
by	O
positive	O
scaling	O
factors	O
to	O
simplify	O
the	O
equation	O
.	O
∗	O
h	O
due	O
to	O
the	O
imposition	O
of	O
an	O
l1	O
norm	O
on	O
h	O
,	O
this	O
procedure	O
will	O
yield	O
a	O
sparse	O
(	O
see	O
section	O
7.1.2	O
)	O
.	O
to	O
train	O
the	O
model	B
rather	O
than	O
just	O
perform	O
inference	O
,	O
we	O
alternate	O
between	O
minimization	O
with	O
respect	O
to	O
h	O
and	O
minimization	O
with	O
respect	O
to	O
w	O
.	O
in	O
this	O
presentation	O
,	O
we	O
treat	O
β	O
as	O
a	O
hyperparameter	O
.	O
typically	O
it	O
is	O
set	O
to	O
1	O
because	O
its	O
role	O
in	O
this	O
optimization	O
problem	O
is	O
shared	O
with	O
λ	O
and	O
there	O
is	O
no	O
need	O
for	O
both	O
hyperparameters	O
.	O
in	O
principle	O
,	O
we	O
could	O
also	O
treat	O
β	O
as	O
a	O
parameter	O
of	O
the	O
model	B
and	O
learn	O
it	O
.	O
our	O
presentation	O
here	O
has	O
discarded	O
some	O
terms	O
that	O
do	O
not	O
depend	O
on	O
h	O
but	O
do	O
depend	O
on	O
β.	O
to	O
learn	O
β	O
,	O
these	O
terms	O
must	O
be	O
included	O
,	O
or	O
β	O
will	O
collapse	O
to	O
.0	O
not	O
all	O
approaches	O
to	O
sparse	O
coding	O
explicitly	O
build	O
a	O
p	O
(	O
h	O
)	O
and	O
a	O
p	O
(	O
x	O
h	O
)	O
.	O
often	O
we	O
are	O
just	O
interested	O
in	O
learning	O
a	O
dictionary	O
of	O
features	O
with	O
activation	O
values	O
that	O
will	O
often	O
be	O
zero	O
when	O
extracted	O
using	O
this	O
inference	O
procedure	O
.	O
if	O
we	O
sample	O
h	O
from	O
a	O
laplace	O
prior	O
,	O
it	O
is	O
in	O
fact	O
a	O
zero	O
probability	O
event	O
for	O
an	O
element	O
of	O
h	O
to	O
actually	O
be	O
zero	O
.	O
the	O
generative	O
model	B
itself	O
is	O
not	O
especially	O
)	O
describe	O
approximate	O
sparse	O
,	O
only	O
the	O
feature	O
extractor	O
is	O
.	O
goodfellow	O
et	O
al	O
.	O
2013d	O
(	O
497	O
|	O
chapter	O
13.	O
linear	O
factor	O
models	O
inference	O
in	O
a	O
diﬀerent	O
model	B
family	O
,	O
the	O
spike	O
and	O
slab	O
sparse	O
coding	O
model	B
,	O
for	O
which	O
samples	O
from	O
the	O
prior	O
usually	O
contain	O
true	O
zeros	O
.	O
the	O
sparse	O
coding	O
approach	O
combined	O
with	O
the	O
use	O
of	O
the	O
non-parametric	O
encoder	O
can	O
in	O
principle	O
minimize	O
the	O
combination	O
of	O
reconstruction	O
error	O
and	O
log-prior	O
better	O
than	O
any	O
speciﬁc	O
parametric	O
encoder	O
.	O
another	O
advantage	O
is	O
that	O
there	O
is	O
no	O
generalization	O
error	O
to	O
the	O
encoder	O
.	O
a	O
parametric	O
encoder	O
must	O
learn	O
how	O
to	O
map	O
x	O
to	O
h	O
in	O
a	O
way	O
that	O
generalizes	O
.	O
for	O
unusual	O
x	O
that	O
do	O
not	O
resemble	O
the	O
training	O
data	O
,	O
a	O
learned	O
,	O
parametric	O
encoder	O
may	O
fail	O
to	O
ﬁnd	O
an	O
h	O
that	O
results	O
in	O
accurate	O
reconstruction	O
or	O
a	O
sparse	O
code	O
.	O
for	O
the	O
vast	O
majority	O
of	O
formulations	O
of	O
sparse	O
coding	O
models	O
,	O
where	O
the	O
inference	O
problem	O
is	O
convex	O
,	O
the	O
optimization	O
procedure	O
will	O
always	O
ﬁnd	O
the	O
optimal	O
code	O
(	O
unless	O
degenerate	O
cases	O
such	O
as	O
replicated	O
weight	O
vectors	O
occur	O
)	O
.	O
obviously	O
,	O
the	O
sparsity	O
and	O
reconstruction	O
costs	O
can	O
still	O
rise	O
on	O
unfamiliar	O
points	O
,	O
but	O
this	O
is	O
due	O
to	O
generalization	O
error	O
in	O
the	O
decoder	O
weights	O
,	O
rather	O
than	O
generalization	O
error	O
in	O
the	O
encoder	O
.	O
the	O
lack	O
of	O
generalization	O
error	O
in	O
sparse	O
coding	O
’	O
s	O
optimization-based	O
encoding	O
process	O
may	O
result	O
in	O
better	O
generalization	O
when	O
sparse	O
coding	O
is	O
used	O
as	O
a	O
feature	O
extractor	O
for	O
a	O
classiﬁer	O
than	O
when	O
a	O
parametric	O
function	O
is	O
used	O
to	O
predict	O
the	O
code	O
.	O
coates	O
and	O
ng	O
2011	O
)	O
demonstrated	O
that	O
sparse	O
coding	O
features	O
generalize	O
better	O
for	O
object	O
recognition	B
tasks	O
than	O
the	O
features	O
of	O
a	O
related	O
model	B
based	O
on	O
a	O
parametric	O
encoder	O
,	O
the	O
linear-sigmoid	O
autoencoder	O
.	O
inspired	O
by	O
their	O
work	B
,	O
goodfellow	O
et	O
al	O
.	O
(	O
)	O
showed	O
that	O
a	O
variant	O
of	O
sparse	O
coding	O
generalizes	O
better	O
than	O
other	O
feature	O
2013d	O
extractors	O
in	O
the	O
regime	O
where	O
extremely	O
few	O
labels	O
are	O
available	O
(	O
twenty	O
or	O
fewer	O
labels	O
per	O
class	O
)	O
.	O
(	O
14	O
the	O
primary	O
disadvantage	O
of	O
the	O
non-parametric	O
encoder	O
is	O
that	O
it	O
requires	O
greater	O
time	O
to	O
compute	O
h	O
given	O
x	O
because	O
the	O
non-parametric	O
approach	O
requires	O
running	O
an	O
iterative	O
algorithm	O
.	O
the	O
parametric	O
autoencoder	O
approach	O
,	O
developed	O
in	O
chapter	O
,	O
uses	O
only	O
a	O
ﬁxed	O
number	O
of	O
layers	O
,	O
often	O
only	O
one	O
.	O
another	O
disadvantage	O
is	O
that	O
it	O
is	O
not	O
straight-forward	O
to	O
back-propagate	O
through	O
the	O
non-parametric	O
encoder	O
,	O
which	O
makes	O
it	O
diﬃcult	O
to	O
pretrain	O
a	O
sparse	O
coding	O
model	B
with	O
an	O
unsupervised	O
criterion	O
and	O
then	O
ﬁne-tune	O
it	O
using	O
a	O
supervised	O
criterion	O
.	O
modiﬁed	O
versions	O
of	O
sparse	O
coding	O
that	O
permit	O
approximate	O
derivatives	O
do	O
exist	O
but	O
are	O
not	O
widely	O
used	O
(	O
bagnell	O
and	O
bradley	O
2009	O
,	O
)	O
.	O
13.2	O
sparse	O
coding	O
,	O
like	O
other	O
linear	O
factor	O
models	O
,	O
often	O
produces	O
poor	O
samples	O
,	O
as	O
shown	O
in	O
ﬁgure	O
.	O
this	O
happens	O
even	O
when	O
the	O
model	B
is	O
able	O
to	O
reconstruct	O
the	O
data	O
well	O
and	O
provide	O
useful	O
features	O
for	O
a	O
classiﬁer	O
.	O
the	O
reason	O
is	O
that	O
each	O
individual	O
feature	O
may	O
be	O
learned	O
well	O
,	O
but	O
the	O
factorial	O
prior	O
on	O
the	O
hidden	O
code	O
results	O
in	O
the	O
model	B
including	O
random	O
subsets	O
of	O
all	O
of	O
the	O
features	O
in	O
each	O
generated	O
sample	O
.	O
this	O
motivates	O
the	O
development	O
of	O
deeper	O
models	O
that	O
can	O
impose	O
a	O
non-	O
498	O
chapter	O
13.	O
linear	O
factor	O
models	O
figure	O
13.2	O
:	O
example	O
samples	O
and	O
weights	O
from	O
a	O
spike	O
and	O
slab	O
sparse	O
coding	O
model	B
trained	O
on	O
the	O
mnist	O
dataset	O
.	O
(	O
left	O
)	O
the	O
samples	O
from	O
the	O
model	B
do	O
not	O
resemble	O
the	O
training	O
examples	O
.	O
at	O
ﬁrst	O
glance	O
,	O
one	O
might	O
assume	O
the	O
model	B
is	O
poorly	O
ﬁt	O
.	O
the	O
weight	O
vectors	O
of	O
the	O
model	B
have	O
learned	O
to	O
represent	O
penstrokes	O
and	O
sometimes	O
complete	O
digits	O
.	O
the	O
model	B
has	O
thus	O
learned	O
useful	O
features	O
.	O
the	O
problem	O
is	O
that	O
the	O
factorial	O
prior	O
over	O
features	O
results	O
in	O
random	O
subsets	O
of	O
features	O
being	O
combined	O
.	O
few	O
such	O
subsets	O
are	O
appropriate	O
to	O
form	O
a	O
recognizable	O
mnist	O
digit	O
.	O
this	O
motivates	O
the	O
development	O
of	O
generative	O
models	O
that	O
have	O
more	O
powerful	O
distributions	O
over	O
their	O
latent	O
codes	O
.	O
figure	O
reproduced	O
with	O
permission	O
from	O
goodfellow	O
et	O
al	O
.	O
(	O
(	O
right	O
)	O
2013d	O
)	O
.	O
factorial	O
distribution	O
on	O
the	O
deepest	O
code	O
layer	O
,	O
as	O
well	O
as	O
the	O
development	O
of	O
more	O
sophisticated	O
shallow	O
models	O
.	O
13.5	O
manifold	O
interpretation	O
of	O
pca	O
,	O
hinton	O
et	O
al	O
.	O
1997	O
linear	O
factor	O
models	O
including	O
pca	O
and	O
factor	O
analysis	O
can	O
be	O
interpreted	O
as	O
learning	O
a	O
manifold	O
(	O
)	O
.	O
we	O
can	O
view	O
probabilistic	O
pca	O
as	O
deﬁning	O
a	O
thin	O
pancake-shaped	O
region	O
of	O
high	O
probability—a	O
gaussian	O
distribution	O
that	O
is	O
very	O
narrow	O
along	O
some	O
axes	O
,	O
just	O
as	O
a	O
pancake	O
is	O
very	O
ﬂat	O
along	O
its	O
vertical	O
axis	O
,	O
but	O
is	O
elongated	O
along	O
other	O
axes	O
,	O
just	O
as	O
a	O
pancake	O
is	O
wide	O
along	O
its	O
horizontal	O
axes	O
.	O
this	O
is	O
illustrated	O
in	O
ﬁgure	O
.	O
pca	O
can	O
be	O
interpreted	O
as	O
aligning	O
this	O
pancake	O
with	O
a	O
linear	O
manifold	O
in	O
a	O
higher-dimensional	O
space	O
.	O
this	O
interpretation	O
applies	O
not	O
just	O
to	O
traditional	O
pca	O
but	O
also	O
to	O
any	O
linear	O
autoencoder	O
that	O
learns	O
matrices	O
w	O
and	O
v	O
with	O
the	O
goal	O
of	O
making	O
the	O
reconstruction	O
of	O
x	O
lie	O
as	O
close	O
to	O
x	O
as	O
possible	O
,	O
13.3	O
let	O
the	O
encoder	O
be	O
h	O
=	O
(	O
f	O
x	O
w	O
)	O
=	O
−	O
	O
)	O
x	O
µ	O
(	O
.	O
(	O
13.19	O
)	O
499	O
chapter	O
13.	O
linear	O
factor	O
models	O
the	O
encoder	O
computes	O
a	O
low-dimensional	O
representation	O
of	O
h.	O
with	O
the	O
autoencoder	O
view	O
,	O
we	O
have	O
a	O
decoder	O
computing	O
the	O
reconstruction	O
ˆx	O
=	O
(	O
g	O
h	O
)	O
=	O
+	O
b	O
v	O
h	O
.	O
(	O
13.20	O
)	O
figure	O
13.3	O
:	O
flat	O
gaussian	O
capturing	O
probability	O
concentration	O
near	O
a	O
low-dimensional	O
manifold	O
.	O
the	O
ﬁgure	O
shows	O
the	O
upper	O
half	O
of	O
the	O
“	O
pancake	O
”	O
above	O
the	O
“	O
manifold	O
plane	O
”	O
which	O
goes	O
through	O
its	O
middle	O
.	O
the	O
variance	O
in	O
the	O
direction	O
orthogonal	O
to	O
the	O
manifold	O
is	O
very	O
small	O
(	O
arrow	O
pointing	O
out	O
of	O
plane	O
)	O
and	O
can	O
be	O
considered	O
like	O
“	O
noise	O
,	O
”	O
while	O
the	O
other	O
variances	O
are	O
large	O
(	O
arrows	O
in	O
the	O
plane	O
)	O
and	O
correspond	O
to	O
“	O
signal	O
,	O
”	O
and	O
a	O
coordinate	O
system	O
for	O
the	O
reduced-dimension	O
data	O
.	O
the	O
choices	O
of	O
linear	O
encoder	O
and	O
decoder	O
that	O
minimize	O
reconstruction	O
error	O
||	O
−	O
||	O
x	O
ˆx	O
e	O
[	O
2	O
]	O
(	O
13.21	O
)	O
correspond	O
to	O
v	O
=	O
w	O
,	O
µ	O
=	O
b	O
=	O
e	O
[	O
x	O
]	O
and	O
the	O
columns	O
of	O
w	O
form	O
an	O
orthonormal	O
basis	O
which	O
spans	O
the	O
same	O
subspace	O
as	O
the	O
principal	O
eigenvectors	O
of	O
the	O
covariance	O
matrix	O
−	O
x	O
µ	O
x	O
µ	O
)	O
−	O
)	O
(	O
	O
c	O
=	O
[	O
(	O
e	O
]	O
.	O
(	O
13.22	O
)	O
in	O
the	O
case	O
of	O
pca	O
,	O
the	O
columns	O
of	O
w	O
are	O
these	O
eigenvectors	O
,	O
ordered	O
by	O
the	O
magnitude	O
of	O
the	O
corresponding	O
eigenvalues	O
(	O
which	O
are	O
all	O
real	O
and	O
non-negative	O
)	O
.	O
one	O
can	O
also	O
show	O
that	O
eigenvalue	O
λi	O
of	O
c	O
corresponds	O
to	O
the	O
variance	O
of	O
x	O
d	O
with	O
d	O
<	O
d	O
,	O
then	O
the	O
in	O
the	O
direction	O
of	O
eigenvector	O
v	O
(	O
)	O
i	O
.	O
if	O
x	O
d	O
and	O
h	O
∈	O
∈	O
r	O
r	O
500	O
chapter	O
13.	O
linear	O
factor	O
models	O
optimal	O
reconstruction	O
error	O
(	O
choosing	O
||	O
−	O
x	O
min	O
[	O
e	O
,	O
,	O
µ	O
b	O
v	O
||	O
2	O
]	O
=	O
ˆx	O
	O
and	O
d	O
w	O
as	O
above	O
)	O
is	O
λi	O
.	O
i	O
d=	O
+1	O
(	O
13.23	O
)	O
hence	O
,	O
if	O
the	O
covariance	O
has	O
rank	O
d	O
,	O
the	O
eigenvalues	O
λd+1	O
to	O
λd	O
are	O
0	O
and	O
recon-	O
struction	O
error	O
is	O
0.	O
furthermore	O
,	O
one	O
can	O
also	O
show	O
that	O
the	O
above	O
solution	O
can	O
be	O
obtained	O
by	O
maximizing	O
the	O
variances	O
of	O
the	O
elements	O
of	O
h	O
,	O
under	O
orthogonal	O
w	O
,	O
instead	O
of	O
minimizing	O
reconstruction	O
error	O
.	O
linear	O
factor	O
models	O
are	O
some	O
of	O
the	O
simplest	O
generative	O
models	O
and	O
some	O
of	O
the	O
simplest	O
models	O
that	O
learn	O
a	O
representation	O
of	O
data	O
.	O
much	O
as	O
linear	O
classiﬁers	O
and	O
linear	O
regression	O
models	O
may	O
be	O
extended	O
to	O
deep	O
feedforward	O
networks	O
,	O
these	O
linear	O
factor	O
models	O
may	O
be	O
extended	O
to	O
autoencoder	O
networks	O
and	O
deep	O
probabilistic	O
models	O
that	O
perform	O
the	O
same	O
tasks	O
but	O
with	O
a	O
much	O
more	O
powerful	O
and	O
ﬂexible	O
model	B
family	O
.	O
501	O
chapter	O
14	O
autoencoders	O
an	O
autoencoder	O
is	O
a	O
neural	O
network	O
that	O
is	O
trained	O
to	O
attempt	O
to	O
copy	O
its	O
input	O
to	O
its	O
output	O
.	O
internally	O
,	O
it	O
has	O
a	O
hidden	O
layer	O
h	O
that	O
describes	O
a	O
code	O
used	O
to	O
represent	O
the	O
input	O
.	O
the	O
network	O
may	O
be	O
viewed	O
as	O
consisting	O
of	O
two	O
parts	O
:	O
an	O
encoder	O
function	O
h	O
=	O
f	O
(	O
x	O
)	O
and	O
a	O
decoder	O
that	O
produces	O
a	O
reconstruction	O
r	O
=	O
g	O
(	O
h	O
)	O
.	O
this	O
architecture	O
is	O
presented	O
in	O
ﬁgure	O
.	O
if	O
an	O
autoencoder	O
succeeds	O
in	O
simply	O
learning	O
to	O
set	O
g	O
(	O
f	O
(	O
x	O
)	O
)	O
=	O
x	O
everywhere	O
,	O
then	O
it	O
is	O
not	O
especially	O
useful	O
.	O
instead	O
,	O
autoencoders	O
are	O
designed	O
to	O
be	O
unable	O
to	O
learn	O
to	O
copy	O
perfectly	O
.	O
usually	O
they	O
are	O
restricted	O
in	O
ways	O
that	O
allow	O
them	O
to	O
copy	O
only	O
approximately	O
,	O
and	O
to	O
copy	O
only	O
input	O
that	O
resembles	O
the	O
training	O
data	O
.	O
because	O
the	O
model	B
is	O
forced	O
to	O
prioritize	O
which	O
aspects	O
of	O
the	O
input	O
should	O
be	O
copied	O
,	O
it	O
often	O
learns	O
useful	O
properties	O
of	O
the	O
data	O
.	O
14.1	O
modern	O
autoencoders	O
have	O
generalized	O
the	O
idea	O
of	O
an	O
encoder	O
and	O
a	O
de-	O
)	O
and	O
coder	O
beyond	O
deterministic	O
functions	O
to	O
stochastic	O
mappings	O
pencoder	O
(	O
h	O
x	O
pdecoder	O
(	O
.	O
)	O
x	O
h	O
|	O
|	O
,	O
;	O
;	O
,	O
the	O
idea	O
of	O
autoencoders	O
has	O
been	O
part	O
of	O
the	O
historical	O
landscape	O
of	O
neural	O
networks	O
for	O
decades	O
(	O
lecun	O
1987	O
bourlard	O
and	O
kamp	O
1988	O
hinton	O
and	O
zemel	O
,	O
1994	O
)	O
.	O
traditionally	O
,	O
autoencoders	O
were	O
used	O
for	O
dimensionality	O
reduction	O
or	O
feature	O
learning	O
.	O
recently	O
,	O
theoretical	O
connections	O
between	O
autoencoders	O
and	O
latent	O
variable	O
models	O
have	O
brought	O
autoencoders	O
to	O
the	O
forefront	O
of	O
generative	O
modeling	O
,	O
as	O
we	O
will	O
see	O
in	O
chapter	O
.	O
autoencoders	O
may	O
be	O
thought	O
of	O
as	O
being	O
a	O
special	O
case	O
of	O
feedforward	O
networks	O
,	O
and	O
may	O
be	O
trained	O
with	O
all	O
of	O
the	O
same	O
techniques	O
,	O
typically	O
minibatch	O
gradient	O
descent	B
following	O
gradients	O
computed	O
by	O
back-propagation	O
.	O
unlike	O
general	O
feedforward	O
networks	O
,	O
autoencoders	O
may	O
also	O
be	O
trained	O
using	O
recirculation	O
(	O
hinton	O
and	O
mcclelland	O
1988	O
)	O
,	O
a	O
learning	O
algorithm	O
based	O
on	O
comparing	O
the	O
activations	O
of	O
the	O
network	O
on	O
the	O
original	O
input	O
20	O
,	O
502	O
chapter	O
14.	O
autoencoders	O
to	O
the	O
activations	O
on	O
the	O
reconstructed	O
input	O
.	O
recirculation	O
is	O
regarded	O
as	O
more	O
biologically	O
plausible	O
than	O
back-propagation	O
,	O
but	O
is	O
rarely	O
used	O
for	O
machine	O
learning	O
applications	O
.	O
hh	O
g	O
rr	O
f	O
xx	O
figure	O
14.1	O
:	O
the	O
general	O
structure	O
of	O
an	O
autoencoder	O
,	O
mapping	O
an	O
input	O
to	O
an	O
output	O
(	O
called	O
reconstruction	O
)	O
r	O
through	O
an	O
internal	O
representation	O
or	O
code	O
h.	O
the	O
autoencoder	O
has	O
two	O
components	O
:	O
the	O
encoder	O
f	O
(	O
mapping	O
x	O
to	O
h	O
)	O
and	O
the	O
decoder	O
g	O
(	O
mapping	O
h	O
to	O
r	O
)	O
.	O
x	O
14.1	O
undercomplete	O
autoencoders	O
copying	O
the	O
input	O
to	O
the	O
output	O
may	O
sound	O
useless	O
,	O
but	O
we	O
are	O
typically	O
not	O
interested	O
in	O
the	O
output	O
of	O
the	O
decoder	O
.	O
instead	O
,	O
we	O
hope	O
that	O
training	O
the	O
autoencoder	O
to	O
perform	O
the	O
input	O
copying	O
task	O
will	O
result	O
in	O
h	O
taking	O
on	O
useful	O
properties	O
.	O
one	O
way	O
to	O
obtain	O
useful	O
features	O
from	O
the	O
autoencoder	O
is	O
to	O
constrain	O
h	O
to	O
have	O
smaller	O
dimension	O
than	O
x.	O
an	O
autoencoder	O
whose	O
code	O
dimension	O
is	O
less	O
than	O
the	O
input	O
dimension	O
is	O
called	O
undercomplete	O
.	O
learning	O
an	O
undercomplete	O
representation	O
forces	O
the	O
autoencoder	O
to	O
capture	O
the	O
most	O
salient	O
features	O
of	O
the	O
training	O
data	O
.	O
the	O
learning	O
process	O
is	O
described	O
simply	O
as	O
minimizing	O
a	O
loss	O
function	O
l	O
,	O
g	O
f	O
(	O
x	O
(	O
(	O
)	O
)	O
)	O
x	O
(	O
14.1	O
)	O
where	O
l	O
is	O
a	O
loss	O
function	O
penalizing	O
g	O
(	O
f	O
(	O
x	O
)	O
)	O
for	O
being	O
dissimilar	O
from	O
x	O
,	O
such	O
as	O
the	O
mean	O
squared	O
error	O
.	O
when	O
the	O
decoder	O
is	O
linear	O
and	O
l	O
is	O
the	O
mean	O
squared	O
error	O
,	O
an	O
undercomplete	O
autoencoder	O
learns	O
to	O
span	O
the	O
same	O
subspace	O
as	O
pca	O
.	O
in	O
this	O
case	O
,	O
an	O
autoencoder	O
trained	O
to	O
perform	O
the	O
copying	O
task	O
has	O
learned	O
the	O
principal	O
subspace	O
of	O
the	O
training	O
data	O
as	O
a	O
side-eﬀect	O
.	O
autoencoders	O
with	O
nonlinear	O
encoder	O
functions	O
f	O
and	O
nonlinear	O
decoder	O
func-	O
tions	O
g	O
can	O
thus	O
learn	O
a	O
more	O
powerful	O
nonlinear	O
generalization	O
of	O
pca	O
.	O
unfortu-	O
503	O
chapter	O
14.	O
autoencoders	O
nately	O
,	O
if	O
the	O
encoder	O
and	O
decoder	O
are	O
allowed	O
too	O
much	O
capacity	O
,	O
the	O
autoencoder	O
can	O
learn	O
to	O
perform	O
the	O
copying	O
task	O
without	O
extracting	O
useful	O
information	O
about	O
the	O
distribution	O
of	O
the	O
data	O
.	O
theoretically	O
,	O
one	O
could	O
imagine	O
that	O
an	O
autoencoder	O
with	O
a	O
one-dimensional	O
code	O
but	O
a	O
very	O
powerful	O
nonlinear	O
encoder	O
could	O
learn	O
to	O
represent	O
each	O
training	O
example	O
x	O
(	O
)	O
i	O
with	O
the	O
code	O
i.	O
the	O
decoder	O
could	O
learn	O
to	O
map	O
these	O
integer	O
indices	O
back	O
to	O
the	O
values	O
of	O
speciﬁc	O
training	O
examples	O
.	O
this	O
speciﬁc	O
scenario	O
does	O
not	O
occur	O
in	O
practice	O
,	O
but	O
it	O
illustrates	O
clearly	O
that	O
an	O
autoen-	O
coder	O
trained	O
to	O
perform	O
the	O
copying	O
task	O
can	O
fail	O
to	O
learn	O
anything	O
useful	O
about	O
the	O
dataset	O
if	O
the	O
capacity	O
of	O
the	O
autoencoder	O
is	O
allowed	O
to	O
become	O
too	O
great	O
.	O
14.2	O
regularized	O
autoencoders	O
undercomplete	O
autoencoders	O
,	O
with	O
code	O
dimension	O
less	O
than	O
the	O
input	O
dimension	O
,	O
can	O
learn	O
the	O
most	O
salient	O
features	O
of	O
the	O
data	O
distribution	O
.	O
we	O
have	O
seen	O
that	O
these	O
autoencoders	O
fail	O
to	O
learn	O
anything	O
useful	O
if	O
the	O
encoder	O
and	O
decoder	O
are	O
given	O
too	O
much	O
capacity	O
.	O
a	O
similar	O
problem	O
occurs	O
if	O
the	O
hidden	O
code	O
is	O
allowed	O
to	O
have	O
dimension	O
equal	O
to	O
the	O
input	O
,	O
and	O
in	O
the	O
overcomplete	O
case	O
in	O
which	O
the	O
hidden	O
code	O
has	O
dimension	O
greater	O
than	O
the	O
input	O
.	O
in	O
these	O
cases	O
,	O
even	O
a	O
linear	O
encoder	O
and	O
linear	O
decoder	O
can	O
learn	O
to	O
copy	O
the	O
input	O
to	O
the	O
output	O
without	O
learning	O
anything	O
useful	O
about	O
the	O
data	O
distribution	O
.	O
ideally	O
,	O
one	O
could	O
train	O
any	O
architecture	O
of	O
autoencoder	O
successfully	O
,	O
choosing	O
the	O
code	O
dimension	O
and	O
the	O
capacity	O
of	O
the	O
encoder	O
and	O
decoder	O
based	O
on	O
the	O
complexity	O
of	O
distribution	O
to	O
be	O
modeled	O
.	O
regularized	O
autoencoders	O
provide	O
the	O
ability	O
to	O
do	O
so	O
.	O
rather	O
than	O
limiting	O
the	O
model	B
capacity	O
by	O
keeping	O
the	O
encoder	O
and	O
decoder	O
shallow	O
and	O
the	O
code	O
size	O
small	O
,	O
regularized	O
autoencoders	O
use	O
a	O
loss	O
function	O
that	O
encourages	O
the	O
model	B
to	O
have	O
other	O
properties	O
besides	O
the	O
ability	O
to	O
copy	O
its	O
input	O
to	O
its	O
output	O
.	O
these	O
other	O
properties	O
include	O
sparsity	O
of	O
the	O
representation	O
,	O
smallness	O
of	O
the	O
derivative	O
of	O
the	O
representation	O
,	O
and	O
robustness	O
to	O
noise	O
or	O
to	O
missing	O
inputs	O
.	O
a	O
regularized	O
autoencoder	O
can	O
be	O
nonlinear	O
and	O
overcomplete	O
but	O
still	O
learn	O
something	O
useful	O
about	O
the	O
data	O
distribution	O
even	O
if	O
the	O
model	B
capacity	O
is	O
great	O
enough	O
to	O
learn	O
a	O
trivial	O
identity	O
function	O
.	O
in	O
addition	O
to	O
the	O
methods	O
described	O
here	O
which	O
are	O
most	O
naturally	O
interpreted	O
as	O
regularized	O
autoencoders	O
,	O
nearly	O
any	O
generative	O
model	B
with	O
latent	O
variables	O
and	O
equipped	O
with	O
an	O
inference	O
procedure	O
(	O
for	O
computing	O
latent	O
representations	O
given	O
input	O
)	O
may	O
be	O
viewed	O
as	O
a	O
particular	O
form	O
of	O
autoencoder	O
.	O
two	O
generative	O
modeling	O
approaches	O
that	O
emphasize	O
this	O
connection	O
with	O
autoencoders	O
are	O
the	O
)	O
,	O
such	O
as	O
the	O
variational	O
descendants	O
of	O
the	O
helmholtz	O
machine	O
(	O
hinton	O
et	O
al	O
.	O
1995b	O
,	O
504	O
chapter	O
14.	O
autoencoders	O
20.10.3	O
)	O
and	O
the	O
generative	O
stochastic	O
networks	O
(	O
section	O
20.12	O
)	O
.	O
autoencoder	O
(	O
section	O
these	O
models	O
naturally	O
learn	O
high-capacity	O
,	O
overcomplete	O
encodings	O
of	O
the	O
input	O
and	O
do	O
not	O
require	O
regularization	O
for	O
these	O
encodings	O
to	O
be	O
useful	O
.	O
their	O
encodings	O
are	O
naturally	O
useful	O
because	O
the	O
models	O
were	O
trained	O
to	O
approximately	O
maximize	O
the	O
probability	O
of	O
the	O
training	O
data	O
rather	O
than	O
to	O
copy	O
the	O
input	O
to	O
the	O
output	O
.	O
14.2.1	O
sparse	O
autoencoders	O
a	O
sparse	O
autoencoder	O
is	O
simply	O
an	O
autoencoder	O
whose	O
training	O
criterion	O
involves	O
a	O
sparsity	O
penalty	O
ω	O
(	O
h	O
)	O
on	O
the	O
code	O
layer	O
h	O
,	O
in	O
addition	O
to	O
the	O
reconstruction	O
error	O
:	O
l	O
,	O
g	O
f	O
(	O
x	O
(	O
(	O
)	O
)	O
)	O
+	O
ω	O
(	O
)	O
h	O
x	O
(	O
14.2	O
)	O
where	O
g	O
(	O
h	O
)	O
is	O
the	O
decoder	O
output	O
and	O
typically	O
we	O
have	O
h	O
=	O
f	O
(	O
x	O
)	O
,	O
the	O
encoder	O
output	O
.	O
sparse	O
autoencoders	O
are	O
typically	O
used	O
to	O
learn	O
features	O
for	O
another	O
task	O
such	O
as	O
classiﬁcation	O
.	O
an	O
autoencoder	O
that	O
has	O
been	O
regularized	O
to	O
be	O
sparse	O
must	O
respond	O
to	O
unique	O
statistical	O
features	O
of	O
the	O
dataset	O
it	O
has	O
been	O
trained	O
on	O
,	O
rather	O
than	O
simply	O
acting	O
as	O
an	O
identity	O
function	O
.	O
in	O
this	O
way	O
,	O
training	O
to	O
perform	O
the	O
copying	O
task	O
with	O
a	O
sparsity	O
penalty	O
can	O
yield	O
a	O
model	B
that	O
has	O
learned	O
useful	O
features	O
as	O
a	O
byproduct	O
.	O
we	O
can	O
think	O
of	O
the	O
penalty	O
ω	O
(	O
h	O
)	O
simply	O
as	O
a	O
regularizer	O
term	O
added	O
to	O
a	O
feedforward	O
network	O
whose	O
primary	O
task	O
is	O
to	O
copy	O
the	O
input	O
to	O
the	O
output	O
(	O
unsupervised	O
learning	O
objective	O
)	O
and	O
possibly	O
also	O
perform	O
some	O
supervised	O
task	O
(	O
with	O
a	O
supervised	O
learning	O
objective	O
)	O
that	O
depends	O
on	O
these	O
sparse	O
features	O
.	O
unlike	O
other	O
regularizers	O
such	O
as	O
weight	O
decay	O
,	O
there	O
is	O
not	O
a	O
straightforward	O
bayesian	O
interpretation	O
to	O
this	O
regularizer	O
.	O
as	O
described	O
in	O
section	O
,	O
training	O
with	O
weight	O
decay	O
and	O
other	O
regularization	O
penalties	O
can	O
be	O
interpreted	O
as	O
a	O
map	O
approximation	O
to	O
bayesian	O
inference	O
,	O
with	O
the	O
added	O
regularizing	O
penalty	O
corresponding	O
to	O
a	O
prior	O
probability	O
distribution	O
over	O
the	O
model	B
parameters	O
.	O
in	O
this	O
view	O
,	O
regularized	O
maximum	O
likelihood	O
corresponds	O
to	O
maximizing	O
p	O
(	O
θ	O
x	O
)	O
,	O
which	O
is	O
equivalent	O
to	O
maximizing	O
log	O
p	O
(	O
x	O
θ	O
)	O
term	O
is	O
the	O
usual	O
data	O
log-likelihood	O
term	O
and	O
the	O
log	O
p	O
(	O
θ	O
)	O
term	O
,	O
the	O
log-prior	O
over	O
parameters	O
,	O
incorporates	O
the	O
preference	O
over	O
particular	O
values	O
of	O
θ.	O
this	O
view	O
was	O
described	O
in	O
section	O
.	O
regularized	O
autoencoders	O
defy	O
such	O
an	O
interpretation	O
because	O
the	O
regularizer	O
depends	O
on	O
the	O
data	O
and	O
is	O
therefore	O
by	O
deﬁnition	O
not	O
a	O
prior	O
in	O
the	O
formal	O
sense	O
of	O
the	O
word	O
.	O
we	O
can	O
still	O
think	O
of	O
these	O
regularization	O
terms	O
as	O
implicitly	O
expressing	O
a	O
preference	O
over	O
functions.	O
)	O
+	O
log	O
p	O
(	O
θ	O
)	O
.	O
the	O
log	O
p	O
(	O
x	O
θ	O
5.6.1	O
5.6	O
|	O
|	O
|	O
rather	O
than	O
thinking	O
of	O
the	O
sparsity	O
penalty	O
as	O
a	O
regularizer	O
for	O
the	O
copying	O
task	O
,	O
we	O
can	O
think	O
of	O
the	O
entire	O
sparse	O
autoencoder	O
framework	O
as	O
approximating	O
505	O
chapter	O
14.	O
autoencoders	O
maximum	O
likelihood	O
training	O
of	O
a	O
generative	O
model	B
that	O
has	O
latent	O
variables	O
.	O
suppose	O
we	O
have	O
a	O
model	B
with	O
visible	O
variables	O
x	O
and	O
latent	O
variables	O
h	O
,	O
with	O
an	O
explicit	O
joint	O
distribution	O
pmodel	O
(	O
x	O
h	O
,	O
)	O
.	O
we	O
refer	O
to	O
pmodel	O
(	O
h	O
)	O
as	O
the	O
model	B
’	O
s	O
prior	O
distribution	O
over	O
the	O
latent	O
variables	O
,	O
representing	O
the	O
model	B
’	O
s	O
beliefs	O
prior	O
to	O
seeing	O
x.	O
this	O
is	O
diﬀerent	O
from	O
the	O
way	O
we	O
have	O
previously	O
used	O
the	O
word	O
“	O
prior	O
,	O
”	O
to	O
refer	O
to	O
the	O
distribution	O
p	O
(	O
θ	O
)	O
encoding	O
our	O
beliefs	O
about	O
the	O
model	B
’	O
s	O
parameters	O
before	O
we	O
have	O
seen	O
the	O
training	O
data	O
.	O
the	O
log-likelihood	O
can	O
be	O
decomposed	O
as	O
)	O
=	O
pmodel	O
(	O
h	O
)	O
pmodel	O
(	O
x	O
h	O
	O
|	O
log	O
pmodel	O
(	O
)	O
=	O
log	O
x	O
pmodel	O
(	O
)	O
h	O
x	O
,	O
.	O
(	O
14.3	O
)	O
h	O
we	O
can	O
think	O
of	O
the	O
autoencoder	O
as	O
approximating	O
this	O
sum	O
with	O
a	O
point	O
estimate	O
for	O
just	O
one	O
highly	O
likely	O
value	O
for	O
h.	O
this	O
is	O
similar	O
to	O
the	O
sparse	O
coding	O
generative	O
h	O
being	O
the	O
output	O
of	O
the	O
parametric	O
encoder	O
rather	O
model	B
(	O
section	O
than	O
the	O
result	O
of	O
an	O
optimization	O
that	O
infers	O
the	O
most	O
likely	O
h.	O
from	O
this	O
point	O
of	O
view	O
,	O
with	O
this	O
chosen	O
,	O
we	O
are	O
maximizing	O
)	O
,	O
but	O
with	O
13.4	O
h	O
log	O
pmodel	O
(	O
h	O
x	O
,	O
)	O
=	O
log	O
pmodel	O
(	O
)	O
+	O
log	O
h	O
pmodel	O
(	O
)	O
x	O
h	O
.	O
(	O
14.4	O
)	O
|	O
the	O
log	O
pmodel	O
(	O
)	O
h	O
term	O
can	O
be	O
sparsity-inducing	O
.	O
for	O
example	O
,	O
the	O
laplace	O
prior	O
,	O
pmodel	O
(	O
hi	O
)	O
=	O
|	O
−	O
|	O
λ	O
hi	O
e	O
,	O
λ	O
2	O
(	O
14.5	O
)	O
corresponds	O
to	O
an	O
absolute	O
value	O
sparsity	O
penalty	O
.	O
expressing	O
the	O
log-prior	O
as	O
an	O
absolute	O
value	O
penalty	O
,	O
we	O
obtain	O
	O
	O
	O
|	O
|	O
hi	O
|	O
λ	O
h	O
	O
ω	O
(	O
)	O
=	O
h	O
λ	O
−	O
log	O
pmodel	O
(	O
)	O
=h	O
i	O
i	O
(	O
14.6	O
)	O
|	O
−	O
i	O
log	O
λ	O
2	O
=	O
ω	O
(	O
)	O
+	O
const	O
h	O
(	O
14.7	O
)	O
where	O
the	O
constant	O
term	O
depends	O
only	O
on	O
λ	O
and	O
not	O
h.	O
we	O
typically	O
treat	O
λ	O
as	O
a	O
hyperparameter	O
and	O
discard	O
the	O
constant	O
term	O
since	O
it	O
does	O
not	O
aﬀect	O
the	O
parameter	O
learning	O
.	O
other	O
priors	O
such	O
as	O
the	O
student-t	O
prior	O
can	O
also	O
induce	O
sparsity	O
.	O
from	O
this	O
point	O
of	O
view	O
of	O
sparsity	O
as	O
resulting	O
from	O
the	O
eﬀect	O
of	O
pmodel	O
(	O
h	O
)	O
on	O
approximate	O
maximum	O
likelihood	O
learning	O
,	O
the	O
sparsity	O
penalty	O
is	O
not	O
a	O
regularization	O
term	O
at	O
all	O
.	O
it	O
is	O
just	O
a	O
consequence	O
of	O
the	O
model	B
’	O
s	O
distribution	O
over	O
its	O
latent	O
variables	O
.	O
this	O
view	O
provides	O
a	O
diﬀerent	O
motivation	O
for	O
training	O
an	O
autoencoder	O
:	O
it	O
is	O
a	O
way	O
of	O
approximately	O
training	O
a	O
generative	O
model	B
.	O
it	O
also	O
provides	O
a	O
diﬀerent	O
reason	O
for	O
506	O
chapter	O
14.	O
autoencoders	O
why	O
the	O
features	O
learned	O
by	O
the	O
autoencoder	O
are	O
useful	O
:	O
they	O
describe	O
the	O
latent	O
variables	O
that	O
explain	O
the	O
input	O
.	O
,	O
,	O
ranzato	O
et	O
al	O
.	O
2007a	O
2008	O
early	O
work	B
on	O
sparse	O
autoencoders	O
(	O
)	O
explored	O
various	O
forms	O
of	O
sparsity	O
and	O
proposed	O
a	O
connection	O
between	O
the	O
sparsity	O
penalty	O
and	O
the	O
log	O
z	O
term	O
that	O
arises	O
when	O
applying	O
maximum	O
likelihood	O
to	O
an	O
undirected	O
probabilistic	O
model	B
p	O
(	O
x	O
)	O
=	O
1	O
˜p	O
(	O
x	O
)	O
.	O
the	O
idea	O
is	O
that	O
minimizing	O
log	O
z	O
prevents	O
a	O
z	O
probabilistic	O
model	B
from	O
having	O
high	O
probability	O
everywhere	O
,	O
and	O
imposing	O
sparsity	O
on	O
an	O
autoencoder	O
prevents	O
the	O
autoencoder	O
from	O
having	O
low	O
reconstruction	O
error	O
everywhere	O
.	O
in	O
this	O
case	O
,	O
the	O
connection	O
is	O
on	O
the	O
level	O
of	O
an	O
intuitive	O
understanding	O
of	O
a	O
general	O
mechanism	O
rather	O
than	O
a	O
mathematical	O
correspondence	O
.	O
the	O
interpretation	O
of	O
the	O
sparsity	O
penalty	O
as	O
corresponding	O
to	O
log	O
pmodel	O
(	O
h	O
)	O
in	O
a	O
directed	O
model	B
pmodel	O
(	O
)	O
h	O
pmodel	O
(	O
is	O
more	O
mathematically	O
straightforward	O
.	O
x	O
h	O
)	O
|	O
one	O
way	O
to	O
achieve	O
actual	O
zeros	O
in	O
h	O
for	O
sparse	O
(	O
and	O
denoising	O
)	O
autoencoders	O
was	O
introduced	O
in	O
)	O
.	O
the	O
idea	O
is	O
to	O
use	O
rectiﬁed	O
linear	O
units	O
to	O
produce	O
the	O
code	O
layer	O
.	O
with	O
a	O
prior	O
that	O
actually	O
pushes	O
the	O
representations	O
to	O
zero	O
(	O
like	O
the	O
absolute	O
value	O
penalty	O
)	O
,	O
one	O
can	O
thus	O
indirectly	O
control	O
the	O
average	O
number	O
of	O
zeros	O
in	O
the	O
representation	O
.	O
glorot	O
et	O
al	O
.	O
2011b	O
(	O
14.2.2	O
denoising	O
autoencoders	O
rather	O
than	O
adding	O
a	O
penalty	O
to	O
the	O
cost	O
function	O
,	O
we	O
can	O
obtain	O
an	O
autoencoder	O
that	O
learns	O
something	O
useful	O
by	O
changing	O
the	O
reconstruction	O
error	O
term	O
of	O
the	O
cost	O
function	O
.	O
ω	O
traditionally	O
,	O
autoencoders	O
minimize	O
some	O
function	O
l	O
,	O
g	O
f	O
(	O
x	O
(	O
(	O
)	O
)	O
)	O
x	O
(	O
14.8	O
)	O
where	O
l	O
is	O
a	O
loss	O
function	O
penalizing	O
g	O
(	O
f	O
(	O
x	O
)	O
)	O
for	O
being	O
dissimilar	O
from	O
x	O
,	O
such	O
as	O
the	O
l2	O
norm	O
of	O
their	O
diﬀerence	O
.	O
this	O
encourages	O
g	O
to	O
learn	O
to	O
be	O
merely	O
an	O
identity	O
function	O
if	O
they	O
have	O
the	O
capacity	O
to	O
do	O
so	O
.	O
f	O
◦	O
a	O
denoising	O
autoencoder	O
or	O
dae	O
instead	O
minimizes	O
l	O
,	O
g	O
f	O
(	O
x	O
(	O
(	O
˜x	O
)	O
)	O
)	O
,	O
(	O
14.9	O
)	O
where	O
˜x	O
is	O
a	O
copy	O
of	O
x	O
that	O
has	O
been	O
corrupted	O
by	O
some	O
form	O
of	O
noise	O
.	O
denoising	O
autoencoders	O
must	O
therefore	O
undo	O
this	O
corruption	O
rather	O
than	O
simply	O
copying	O
their	O
input	O
.	O
denoising	O
training	O
forces	O
f	O
and	O
g	O
to	O
implicitly	O
learn	O
the	O
structure	O
of	O
pdata	O
(	O
x	O
)	O
,	O
)	O
.	O
denoising	O
alain	O
and	O
bengio	O
2013	O
bengio	O
et	O
al	O
.	O
2013c	O
as	O
shown	O
by	O
)	O
and	O
(	O
(	O
507	O
chapter	O
14.	O
autoencoders	O
autoencoders	O
thus	O
provide	O
yet	O
another	O
example	O
of	O
how	O
useful	O
properties	O
can	O
emerge	O
as	O
a	O
byproduct	O
of	O
minimizing	O
reconstruction	O
error	O
.	O
they	O
are	O
also	O
an	O
example	O
of	O
how	O
overcomplete	O
,	O
high-capacity	O
models	O
may	O
be	O
used	O
as	O
autoencoders	O
so	O
long	O
as	O
care	O
is	O
taken	O
to	O
prevent	O
them	O
from	O
learning	O
the	O
identity	O
function	O
.	O
denoising	O
autoencoders	O
are	O
presented	O
in	O
more	O
detail	O
in	O
section	O
.	O
14.5	O
14.2.3	O
regularizing	O
by	O
penalizing	O
derivatives	O
another	O
strategy	O
for	O
regularizing	O
an	O
autoencoder	O
is	O
to	O
use	O
a	O
penalty	O
autoencoders	O
,	O
	O
l	O
,	O
g	O
f	O
(	O
x	O
(	O
(	O
)	O
)	O
)	O
+	O
ω	O
(	O
x	O
h	O
x	O
)	O
,	O
,	O
but	O
with	O
a	O
diﬀerent	O
form	O
of	O
:	O
ω	O
ω	O
(	O
h	O
x	O
,	O
)	O
=	O
λ	O
||∇	O
||2	O
.	O
xhi	O
i	O
ω	O
as	O
in	O
sparse	O
(	O
14.10	O
)	O
(	O
14.11	O
)	O
this	O
forces	O
the	O
model	B
to	O
learn	O
a	O
function	O
that	O
does	O
not	O
change	O
much	O
when	O
x	O
changes	O
slightly	O
.	O
because	O
this	O
penalty	O
is	O
applied	O
only	O
at	O
training	O
examples	O
,	O
it	O
forces	O
the	O
autoencoder	O
to	O
learn	O
features	O
that	O
capture	O
information	O
about	O
the	O
training	O
distribution	O
.	O
an	O
autoencoder	O
regularized	O
in	O
this	O
way	O
is	O
called	O
a	O
contractive	O
autoencoder	O
or	O
cae	O
.	O
this	O
approach	O
has	O
theoretical	O
connections	O
to	O
denoising	O
autoencoders	O
,	O
manifold	O
learning	O
and	O
probabilistic	O
modeling	O
.	O
the	O
cae	O
is	O
described	O
in	O
more	O
detail	O
in	O
section	O
14.7	O
.	O
14.3	O
representational	O
power	O
,	O
layer	O
size	O
and	O
depth	O
autoencoders	O
are	O
often	O
trained	O
with	O
only	O
a	O
single	O
layer	O
encoder	O
and	O
a	O
single	O
layer	O
decoder	O
.	O
however	O
,	O
this	O
is	O
not	O
a	O
requirement	O
.	O
in	O
fact	O
,	O
using	O
deep	O
encoders	O
and	O
decoders	O
oﬀers	O
many	O
advantages	O
.	O
6.4.1	O
recall	O
from	O
section	O
that	O
there	O
are	O
many	O
advantages	O
to	O
depth	O
in	O
a	O
feedfor-	O
ward	O
network	O
.	O
because	O
autoencoders	O
are	O
feedforward	O
networks	O
,	O
these	O
advantages	O
also	O
apply	O
to	O
autoencoders	O
.	O
moreover	O
,	O
the	O
encoder	O
is	O
itself	O
a	O
feedforward	O
network	O
as	O
is	O
the	O
decoder	O
,	O
so	O
each	O
of	O
these	O
components	O
of	O
the	O
autoencoder	O
can	O
individually	O
beneﬁt	O
from	O
depth	O
.	O
one	O
major	O
advantage	O
of	O
non-trivial	O
depth	O
is	O
that	O
the	O
universal	O
approximator	O
theorem	O
guarantees	O
that	O
a	O
feedforward	O
neural	O
network	O
with	O
at	O
least	O
one	O
hidden	O
layer	O
can	O
represent	O
an	O
approximation	O
of	O
any	O
function	O
(	O
within	O
a	O
broad	O
class	O
)	O
to	O
an	O
508	O
chapter	O
14.	O
autoencoders	O
arbitrary	O
degree	O
of	O
accuracy	O
,	O
provided	O
that	O
it	O
has	O
enough	O
hidden	O
units	O
.	O
this	O
means	O
that	O
an	O
autoencoder	O
with	O
a	O
single	O
hidden	O
layer	O
is	O
able	O
to	O
represent	O
the	O
identity	O
function	O
along	O
the	O
domain	O
of	O
the	O
data	O
arbitrarily	O
well	O
.	O
however	O
,	O
the	O
mapping	O
from	O
input	O
to	O
code	O
is	O
shallow	O
.	O
this	O
means	O
that	O
we	O
are	O
not	O
able	O
to	O
enforce	O
arbitrary	O
constraints	O
,	O
such	O
as	O
that	O
the	O
code	O
should	O
be	O
sparse	O
.	O
a	O
deep	O
autoencoder	O
,	O
with	O
at	O
least	O
one	O
additional	O
hidden	O
layer	O
inside	O
the	O
encoder	O
itself	O
,	O
can	O
approximate	O
any	O
mapping	O
from	O
input	O
to	O
code	O
arbitrarily	O
well	O
,	O
given	O
enough	O
hidden	O
units	O
.	O
depth	O
can	O
exponentially	O
reduce	O
the	O
computational	O
cost	O
of	O
representing	O
some	O
functions	O
.	O
depth	O
can	O
also	O
exponentially	O
decrease	O
the	O
amount	O
of	O
training	O
data	O
needed	O
to	O
learn	O
some	O
functions	O
.	O
see	O
section	O
for	O
a	O
review	O
of	O
the	O
advantages	O
of	O
depth	O
in	O
feedforward	O
networks	O
.	O
6.4.1	O
experimentally	O
,	O
deep	O
autoencoders	O
yield	O
much	O
better	O
compression	O
than	O
corre-	O
sponding	O
shallow	O
or	O
linear	O
autoencoders	O
(	O
hinton	O
and	O
salakhutdinov	O
2006	O
,	O
)	O
.	O
a	O
common	O
strategy	O
for	O
training	O
a	O
deep	O
autoencoder	O
is	O
to	O
greedily	O
pretrain	O
the	O
deep	O
architecture	O
by	O
training	O
a	O
stack	O
of	O
shallow	O
autoencoders	O
,	O
so	O
we	O
often	O
encounter	O
shallow	O
autoencoders	O
,	O
even	O
when	O
the	O
ultimate	O
goal	O
is	O
to	O
train	O
a	O
deep	O
autoencoder	O
.	O
14.4	O
stochastic	O
encoders	O
and	O
decoders	O
autoencoders	O
are	O
just	O
feedforward	O
networks	O
.	O
the	O
same	O
loss	O
functions	O
and	O
output	O
unit	O
types	O
that	O
can	O
be	O
used	O
for	O
traditional	O
feedforward	O
networks	O
are	O
also	O
used	O
for	O
autoencoders	O
.	O
as	O
described	O
in	O
section	O
|	O
,	O
a	O
general	O
strategy	O
for	O
designing	O
the	O
output	O
units	O
and	O
the	O
loss	O
function	O
of	O
a	O
feedforward	O
network	O
is	O
to	O
deﬁne	O
an	O
output	O
distribution	O
p	O
(	O
y	O
x	O
)	O
.	O
in	O
that	O
setting	O
,	O
y	O
was	O
a	O
vector	O
of	O
targets	O
,	O
such	O
as	O
class	O
labels.	O
)	O
and	O
minimize	O
the	O
negative	O
log-likelihood	O
log	O
p	O
(	O
y	O
x	O
6.2.2.4	O
−	O
|	O
in	O
the	O
case	O
of	O
an	O
autoencoder	O
,	O
x	O
is	O
now	O
the	O
target	O
as	O
well	O
as	O
the	O
input	O
.	O
however	O
,	O
we	O
can	O
still	O
apply	O
the	O
same	O
machinery	O
as	O
before	O
.	O
given	O
a	O
hidden	O
code	O
h	O
,	O
we	O
may	O
think	O
of	O
the	O
decoder	O
as	O
providing	O
a	O
conditional	O
distribution	O
p	O
decoder	O
(	O
x	O
h	O
)	O
.	O
we	O
may	O
then	O
train	O
the	O
autoencoder	O
by	O
minimizing	O
.	O
the	O
exact	O
form	O
of	O
this	O
loss	O
function	O
will	O
change	O
depending	O
on	O
the	O
form	O
of	O
pdecoder	O
.	O
as	O
with	O
traditional	O
feedforward	O
networks	O
,	O
we	O
usually	O
use	O
linear	O
output	O
units	O
to	O
parametrize	O
the	O
mean	O
of	O
a	O
gaussian	O
distribution	O
if	O
x	O
is	O
real-valued	O
.	O
in	O
that	O
case	O
,	O
the	O
negative	O
log-likelihood	O
yields	O
a	O
mean	O
squared	O
error	O
criterion	O
.	O
similarly	O
,	O
binary	O
x	O
values	O
correspond	O
to	O
a	O
bernoulli	O
distribution	O
whose	O
parameters	O
are	O
given	O
by	O
a	O
sigmoid	O
output	O
unit	O
,	O
discrete	O
x	O
values	O
correspond	O
to	O
a	O
softmax	O
distribution	O
,	O
and	O
so	O
on	O
.	O
log	O
pdecoder	O
(	O
x	O
h	O
)	O
−	O
|	O
|	O
509	O
chapter	O
14.	O
autoencoders	O
typically	O
,	O
the	O
output	O
variables	O
are	O
treated	O
as	O
being	O
conditionally	O
independent	O
given	O
h	O
so	O
that	O
this	O
probability	O
distribution	O
is	O
inexpensive	O
to	O
evaluate	O
,	O
but	O
some	O
techniques	O
such	O
as	O
mixture	O
density	O
outputs	O
allow	O
tractable	O
modeling	O
of	O
outputs	O
with	O
correlations	O
.	O
hh	O
|	O
pencoder	O
(	O
)	O
h	O
x	O
|	O
pdecoder	O
(	O
)	O
x	O
h	O
xx	O
rr	O
figure	O
14.2	O
:	O
the	O
structure	O
of	O
a	O
stochastic	O
autoencoder	O
,	O
in	O
which	O
both	O
the	O
encoder	O
and	O
the	O
decoder	O
are	O
not	O
simple	O
functions	O
but	O
instead	O
involve	O
some	O
noise	O
injection	O
,	O
meaning	O
that	O
their	O
output	O
can	O
be	O
seen	O
as	O
sampled	O
from	O
a	O
distribution	O
,	O
pencoder	O
(	O
h	O
x	O
)	O
for	O
the	O
encoder	O
and	O
pdecoder	O
(	O
for	O
the	O
decoder	O
.	O
x	O
h	O
)	O
|	O
|	O
to	O
make	O
a	O
more	O
radical	O
departure	O
from	O
the	O
feedforward	O
networks	O
we	O
have	O
seen	O
previously	O
,	O
we	O
can	O
also	O
generalize	O
the	O
notion	O
of	O
an	O
encoding	O
function	O
f	O
(	O
x	O
)	O
to	O
an	O
encoding	O
distribution	O
pencoder	O
(	O
any	O
latent	O
variable	O
model	B
pmodel	O
(	O
|	O
,	O
as	O
illustrated	O
in	O
ﬁgure	O
)	O
h	O
x	O
deﬁnes	O
a	O
stochastic	O
encoder	O
)	O
h	O
x	O
,	O
14.2	O
.	O
|	O
|	O
pencoder	O
(	O
h	O
x	O
)	O
=	O
pmodel	O
(	O
h	O
x	O
)	O
(	O
14.12	O
)	O
(	O
14.13	O
)	O
and	O
a	O
stochastic	O
decoder	O
|	O
|	O
pdecoder	O
(	O
x	O
h	O
)	O
=	O
pmodel	O
(	O
)	O
x	O
h	O
.	O
in	O
general	O
,	O
the	O
encoder	O
and	O
decoder	O
distributions	O
are	O
not	O
necessarily	O
conditional	O
distributions	O
compatible	O
with	O
a	O
unique	O
joint	O
distribution	O
pmodel	O
(	O
x	O
h	O
,	O
)	O
.	O
alain	O
et	O
al	O
.	O
(	O
)	O
showed	O
that	O
training	O
the	O
encoder	O
and	O
decoder	O
as	O
a	O
denoising	O
autoencoder	O
2015	O
will	O
tend	O
to	O
make	O
them	O
compatible	O
asymptotically	O
(	O
with	O
enough	O
capacity	O
and	O
examples	O
)	O
.	O
14.5	O
denoising	O
autoencoders	O
the	O
denoising	O
autoencoder	O
(	O
dae	O
)	O
is	O
an	O
autoencoder	O
that	O
receives	O
a	O
corrupted	O
data	O
point	O
as	O
input	O
and	O
is	O
trained	O
to	O
predict	O
the	O
original	O
,	O
uncorrupted	O
data	O
point	O
as	O
its	O
output	O
.	O
the	O
dae	O
training	O
procedure	O
is	O
illustrated	O
in	O
ﬁgure	O
.	O
we	O
introduce	O
a	O
)	O
which	O
represents	O
a	O
conditional	O
distribution	O
over	O
14.3	O
|	O
corruption	O
process	O
c	O
(	O
˜x	O
x	O
510	O
chapter	O
14.	O
autoencoders	O
hh	O
g	O
ll	O
f	O
˜x˜x	O
xx	O
|	O
c	O
(	O
˜x	O
x	O
)	O
figure	O
14.3	O
:	O
the	O
computational	O
graph	O
of	O
the	O
cost	O
function	O
for	O
a	O
denoising	O
autoencoder	O
,	O
which	O
is	O
trained	O
to	O
reconstruct	O
the	O
clean	O
data	O
point	O
x	O
from	O
its	O
corrupted	O
version	O
˜x	O
.	O
this	O
is	O
accomplished	O
by	O
minimizing	O
the	O
loss	O
l	O
=	O
=	O
f	O
(	O
˜x	O
)	O
)	O
,	O
where	O
˜x	O
is	O
a	O
corrupted	O
version	O
of	O
the	O
data	O
example	O
x	O
,	O
obtained	O
through	O
a	O
given	O
corruption	O
process	O
c	O
(	O
˜x	O
x	O
)	O
.	O
typically	O
the	O
distribution	O
pdecoder	O
is	O
a	O
factorial	O
distribution	O
whose	O
mean	O
parameters	O
are	O
emitted	O
by	O
a	O
feedforward	O
network	O
.g	O
log	O
pdecoder	O
(	O
x	O
h	O
−	O
|	O
|	O
corrupted	O
samples	O
˜x	O
,	O
given	O
a	O
data	O
sample	O
x.	O
the	O
autoencoder	O
then	O
learns	O
a	O
˜x	O
)	O
estimated	O
from	O
training	O
pairs	O
reconstruction	O
distribution	O
preconstruct	O
(	O
x	O
(	O
x	O
,	O
˜x	O
)	O
,	O
as	O
follows	O
:	O
|	O
1.	O
sample	O
a	O
training	O
example	O
x	O
from	O
the	O
training	O
data	O
.	O
|	O
2.	O
sample	O
a	O
corrupted	O
version	O
˜x	O
from	O
c	O
(	O
˜x	O
x	O
|	O
|	O
3.	O
use	O
(	O
x	O
,	O
˜x	O
)	O
as	O
a	O
training	O
example	O
for	O
estimating	O
the	O
autoencoder	O
reconstruction	O
)	O
with	O
h	O
the	O
output	O
of	O
encoder	O
distribution	O
preconstruct	O
(	O
x	O
f	O
(	O
˜x	O
)	O
and	O
pdecoder	O
typically	O
deﬁned	O
by	O
a	O
decoder	O
˜x	O
)	O
=	O
pdecoder	O
(	O
x	O
h	O
.	O
g	O
(	O
)	O
h	O
=	O
)	O
x	O
.	O
typically	O
we	O
can	O
simply	O
perform	O
gradient-based	O
approximate	O
minimization	O
(	O
such	O
log	O
pdecoder	O
(	O
x	O
h	O
as	O
minibatch	O
gradient	O
descent	B
)	O
on	O
the	O
negative	O
log-likelihood	O
)	O
.	O
so	O
long	O
as	O
the	O
encoder	O
is	O
deterministic	O
,	O
the	O
denoising	O
autoencoder	O
is	O
a	O
feedforward	O
network	O
and	O
may	O
be	O
trained	O
with	O
exactly	O
the	O
same	O
techniques	O
as	O
any	O
other	O
feedforward	O
network	O
.	O
−	O
|	O
we	O
can	O
therefore	O
view	O
the	O
dae	O
as	O
performing	O
stochastic	O
gradient	O
descent	B
on	O
the	O
following	O
expectation	O
:	O
−	O
|	O
∼	O
∼	O
x	O
)	O
log	O
pdecoder	O
(	O
ˆpdata	O
(	O
)	O
x	O
e	O
˜x	O
c	O
(	O
˜x	O
ex	O
x	O
h	O
=	O
(	O
f	O
˜x	O
)	O
)	O
(	O
14.14	O
)	O
|	O
where	O
ˆpdata	O
(	O
)	O
x	O
is	O
the	O
training	O
distribution	O
.	O
511	O
chapter	O
14.	O
autoencoders	O
˜x	O
◦	O
g	O
f	O
x	O
˜x	O
|	O
c	O
(	O
˜x	O
x	O
)	O
x	O
|	O
figure	O
14.4	O
:	O
a	O
denoising	O
autoencoder	O
is	O
trained	O
to	O
map	O
a	O
corrupted	O
data	O
point	O
˜x	O
back	O
to	O
the	O
original	O
data	O
point	O
x.	O
we	O
illustrate	O
training	O
examples	O
x	O
as	O
red	O
crosses	O
lying	O
near	O
a	O
low-dimensional	O
manifold	O
illustrated	O
with	O
the	O
bold	O
black	O
line	O
.	O
we	O
illustrate	O
the	O
corruption	O
process	O
c	O
(	O
˜x	O
x	O
)	O
with	O
a	O
gray	O
circle	O
of	O
equiprobable	O
corruptions	O
.	O
a	O
gray	O
arrow	O
demonstrates	O
how	O
one	O
training	O
example	O
is	O
transformed	O
into	O
one	O
sample	O
from	O
this	O
corruption	O
process	O
.	O
||	O
−	O
||	O
when	O
the	O
denoising	O
autoencoder	O
is	O
trained	O
to	O
minimize	O
the	O
average	O
of	O
squared	O
errors	O
−	O
x	O
2	O
,	O
the	O
reconstruction	O
g	O
(	O
f	O
(	O
˜x	O
)	O
)	O
estimates	O
e	O
x	O
˜x	O
]	O
.	O
the	O
vector	O
g	O
(	O
f	O
(	O
˜x	O
)	O
)	O
˜x	O
points	O
approximately	O
towards	O
the	O
nearest	O
point	O
on	O
the	O
manifold	O
,	O
since	O
g	O
(	O
f	O
(	O
˜x	O
)	O
)	O
g	O
(	O
f	O
(	O
˜x	O
)	O
)	O
estimates	O
the	O
center	O
of	O
mass	O
of	O
the	O
clean	O
points	O
x	O
which	O
could	O
have	O
given	O
rise	O
to	O
˜x	O
.	O
the	O
autoencoder	O
thus	O
learns	O
a	O
vector	O
ﬁeld	O
g	O
(	O
f	O
(	O
x	O
)	O
)	O
x	O
indicated	O
by	O
the	O
green	O
arrows	O
.	O
this	O
vector	O
ﬁeld	O
estimates	O
the	O
score	O
xlog	O
pdata	O
(	O
x	O
)	O
up	O
to	O
a	O
multiplicative	O
factor	O
that	O
is	O
the	O
average	O
root	O
mean	O
square	O
reconstruction	O
error	O
.	O
pdata	O
(	O
)	O
(	O
)	O
[	O
x	O
∼	O
,	O
˜	O
x	O
∇	O
−	O
|	O
|	O
c	O
˜	O
x	O
x	O
x	O
512	O
chapter	O
14.	O
autoencoders	O
14.5.1	O
estimating	O
the	O
score	O
hyvärinen	O
2005	O
score	O
matching	O
(	O
)	O
is	O
an	O
alternative	O
to	O
maximum	O
likelihood	O
.	O
it	O
provides	O
a	O
consistent	O
estimator	O
of	O
probability	O
distributions	O
based	O
on	O
encouraging	O
the	O
model	B
to	O
have	O
the	O
same	O
score	O
as	O
the	O
data	O
distribution	O
at	O
every	O
training	O
point	O
x.	O
in	O
this	O
context	O
,	O
the	O
score	O
is	O
a	O
particular	O
gradient	O
ﬁeld	O
:	O
,	O
∇	O
x	O
log	O
(	O
)	O
p	O
x	O
.	O
(	O
14.15	O
)	O
score	O
matching	O
is	O
discussed	O
further	O
in	O
section	O
.	O
for	O
the	O
present	O
discussion	O
regarding	O
autoencoders	O
,	O
it	O
is	O
suﬃcient	O
to	O
understand	O
that	O
learning	O
the	O
gradient	O
ﬁeld	O
of	O
log	O
pdata	O
is	O
one	O
way	O
to	O
learn	O
the	O
structure	O
of	O
pdata	O
itself	O
.	O
18.4	O
a	O
very	O
important	O
property	O
of	O
daes	O
is	O
that	O
their	O
training	O
criterion	O
(	O
with	O
)	O
)	O
makes	O
the	O
autoencoder	O
learn	O
a	O
vector	O
ﬁeld	O
x	O
)	O
that	O
estimates	O
the	O
score	O
of	O
the	O
data	O
distribution	O
.	O
this	O
is	O
illustrated	O
conditionally	O
gaussian	O
p	O
(	O
x	O
h	O
(	O
g	O
(	O
f	O
(	O
x	O
)	O
)	O
in	O
ﬁgure	O
.	O
14.4	O
−	O
|	O
,	O
vincent	O
2011	O
denoising	O
training	O
of	O
a	O
speciﬁc	O
kind	O
of	O
autoencoder	O
(	O
sigmoidal	O
hidden	O
units	O
,	O
linear	O
reconstruction	O
units	O
)	O
using	O
gaussian	O
noise	O
and	O
mean	O
squared	O
error	O
as	O
the	O
reconstruction	O
cost	O
is	O
equivalent	O
(	O
)	O
to	O
training	O
a	O
speciﬁc	O
kind	O
of	O
undirected	O
probabilistic	O
model	B
called	O
an	O
rbm	O
with	O
gaussian	O
visible	O
units	O
.	O
this	O
kind	O
of	O
model	B
will	O
be	O
described	O
in	O
detail	O
in	O
section	O
;	O
for	O
the	O
present	O
discussion	O
it	O
suﬃces	O
to	O
know	O
that	O
it	O
is	O
a	O
model	B
that	O
provides	O
an	O
explicit	O
pmodel	O
(	O
x	O
;	O
θ	O
)	O
.	O
when	O
the	O
rbm	O
is	O
trained	O
using	O
denoising	O
score	O
matching	O
(	O
kingma	O
and	O
lecun	O
,	O
2010	O
)	O
,	O
its	O
learning	O
algorithm	O
is	O
equivalent	O
to	O
denoising	O
training	O
in	O
the	O
corresponding	O
autoencoder	O
.	O
with	O
a	O
ﬁxed	O
noise	O
level	O
,	O
regularized	O
score	O
matching	O
is	O
not	O
a	O
consistent	O
estimator	O
;	O
it	O
instead	O
recovers	O
a	O
blurred	O
version	O
of	O
the	O
distribution	O
.	O
however	O
,	O
if	O
the	O
noise	O
level	O
is	O
chosen	O
to	O
approach	O
0	O
when	O
the	O
number	O
of	O
examples	O
approaches	O
inﬁnity	O
,	O
then	O
consistency	O
is	O
recovered	O
.	O
denoising	O
score	O
matching	O
is	O
discussed	O
in	O
more	O
detail	O
in	O
section	O
20.5.1	O
18.5	O
.	O
other	O
connections	O
between	O
autoencoders	O
and	O
rbms	O
exist	O
.	O
score	O
matching	O
applied	O
to	O
rbms	O
yields	O
a	O
cost	O
function	O
that	O
is	O
identical	O
to	O
reconstruction	O
error	O
combined	O
with	O
a	O
regularization	O
term	O
similar	O
to	O
the	O
contractive	O
penalty	O
of	O
the	O
cae	O
(	O
swersky	O
)	O
showed	O
that	O
an	O
autoen-	O
coder	O
gradient	O
provides	O
an	O
approximation	O
to	O
contrastive	O
divergence	O
training	O
of	O
rbms	O
.	O
bengio	O
and	O
delalleau	O
2009	O
et	O
al.	O
,	O
2011	O
)	O
.	O
(	O
for	O
continuous-valued	O
x	O
,	O
the	O
denoising	O
criterion	O
with	O
gaussian	O
corruption	O
and	O
reconstruction	O
distribution	O
yields	O
an	O
estimator	O
of	O
the	O
score	O
that	O
is	O
applicable	O
to	O
general	O
encoder	O
and	O
decoder	O
parametrizations	O
(	O
)	O
.	O
this	O
means	O
a	O
generic	O
encoder-decoder	O
architecture	O
may	O
be	O
made	O
to	O
estimate	O
the	O
score	O
alain	O
and	O
bengio	O
2013	O
,	O
513	O
chapter	O
14.	O
autoencoders	O
by	O
training	O
with	O
the	O
squared	O
error	O
criterion	O
)	O
)	O
||	O
g	O
f	O
(	O
(	O
˜x	O
n	O
)	O
=	O
|	O
c	O
(	O
˜x	O
=	O
˜x	O
x	O
and	O
corruption	O
−	O
||	O
x	O
2	O
˜x	O
(	O
;	O
=	O
µ	O
x	O
,	O
σς	O
=	O
2	O
i	O
)	O
(	O
14.16	O
)	O
(	O
14.17	O
)	O
with	O
noise	O
variance	O
σ2	O
.	O
see	O
ﬁgure	O
14.5	O
for	O
an	O
illustration	O
of	O
how	O
this	O
works	O
.	O
figure	O
14.5	O
:	O
vector	O
ﬁeld	O
learned	O
by	O
a	O
denoising	O
autoencoder	O
around	O
a	O
1-d	O
curved	O
manifold	O
near	O
which	O
the	O
data	O
concentrates	O
in	O
a	O
2-d	O
space	O
.	O
each	O
arrow	O
is	O
proportional	O
to	O
the	O
reconstruction	O
minus	O
input	O
vector	O
of	O
the	O
autoencoder	O
and	O
points	O
towards	O
higher	O
probability	O
according	O
to	O
the	O
implicitly	O
estimated	O
probability	O
distribution	O
.	O
the	O
vector	O
ﬁeld	O
has	O
zeros	O
at	O
both	O
maxima	O
of	O
the	O
estimated	O
density	O
function	O
(	O
on	O
the	O
data	O
manifolds	O
)	O
and	O
at	O
minima	O
of	O
that	O
density	O
function	O
.	O
for	O
example	O
,	O
the	O
spiral	O
arm	O
forms	O
a	O
one-dimensional	O
manifold	O
of	O
local	O
maxima	O
that	O
are	O
connected	O
to	O
each	O
other	O
.	O
local	O
minima	O
appear	O
near	O
the	O
middle	O
of	O
the	O
gap	O
between	O
two	O
arms	O
.	O
when	O
the	O
norm	O
of	O
reconstruction	O
error	O
(	O
shown	O
by	O
the	O
length	O
of	O
the	O
arrows	O
)	O
is	O
large	O
,	O
it	O
means	O
that	O
probability	O
can	O
be	O
signiﬁcantly	O
increased	O
by	O
moving	O
in	O
the	O
direction	O
of	O
the	O
arrow	O
,	O
and	O
that	O
is	O
mostly	O
the	O
case	O
in	O
places	O
of	O
low	O
probability	O
.	O
the	O
autoencoder	O
maps	O
these	O
low	O
probability	O
points	O
to	O
higher	O
probability	O
reconstructions	O
.	O
where	O
probability	O
is	O
maximal	O
,	O
the	O
arrows	O
shrink	O
because	O
the	O
reconstruction	O
becomes	O
more	O
accurate	O
.	O
figure	O
reproduced	O
with	O
permission	O
from	O
alain	O
and	O
bengio	O
2013	O
)	O
.	O
(	O
in	O
general	O
,	O
there	O
is	O
no	O
guarantee	O
that	O
the	O
reconstruction	O
g	O
(	O
f	O
(	O
x	O
)	O
)	O
minus	O
the	O
input	O
x	O
corresponds	O
to	O
the	O
gradient	O
of	O
any	O
function	O
,	O
let	O
alone	O
to	O
the	O
score	O
.	O
that	O
is	O
514	O
chapter	O
14.	O
autoencoders	O
−	O
,	O
vincent	O
2011	O
why	O
the	O
early	O
results	O
(	O
where	O
g	O
(	O
f	O
(	O
x	O
)	O
)	O
−	O
vincent	O
2011	O
)	O
generalized	O
the	O
results	O
of	O
kamyshanska	O
and	O
memisevic	O
2015	O
identifying	O
a	O
family	O
of	O
shallow	O
autoencoders	O
such	O
that	O
g	O
(	O
f	O
(	O
x	O
)	O
)	O
a	O
score	O
for	O
all	O
members	O
of	O
the	O
family.	O
)	O
are	O
specialized	O
to	O
particular	O
parametrizations	O
x	O
may	O
be	O
obtained	O
by	O
taking	O
the	O
derivative	O
of	O
another	O
function.	O
)	O
by	O
x	O
corresponds	O
to	O
(	O
(	O
so	O
far	O
we	O
have	O
described	O
only	O
how	O
the	O
denoising	O
autoencoder	O
learns	O
to	O
represent	O
a	O
probability	O
distribution	O
.	O
more	O
generally	O
,	O
one	O
may	O
want	O
to	O
use	O
the	O
autoencoder	O
as	O
a	O
generative	O
model	B
and	O
draw	O
samples	O
from	O
this	O
distribution	O
.	O
this	O
will	O
be	O
described	O
later	O
,	O
in	O
section	O
.	O
20.11	O
14.5.1.1	O
historical	O
perspective	O
(	O
,	O
(	O
(	O
)	O
.	O
gallinari	O
et	O
al	O
.	O
1987	O
behnke	O
2001	O
lecun	O
1987	O
)	O
the	O
idea	O
of	O
using	O
mlps	O
for	O
denoising	O
dates	O
back	O
to	O
the	O
work	B
of	O
and	O
)	O
also	O
used	O
recurrent	O
networks	O
to	O
denoise	O
images	O
.	O
denoising	O
autoencoders	O
are	O
,	O
in	O
some	O
sense	O
,	O
just	O
mlps	O
trained	O
to	O
denoise	O
.	O
however	O
,	O
the	O
name	O
“	O
denoising	O
autoencoder	O
”	O
refers	O
to	O
a	O
model	B
that	O
is	O
intended	O
not	O
merely	O
to	O
learn	O
to	O
denoise	O
its	O
input	O
but	O
to	O
learn	O
a	O
good	O
internal	O
representation	O
as	O
a	O
side	O
eﬀect	O
of	O
learning	O
to	O
denoise	O
.	O
this	O
idea	O
came	O
much	O
later	O
(	O
vincent	O
et	O
al.	O
,	O
)	O
.	O
the	O
learned	O
representation	O
may	O
then	O
be	O
used	O
to	O
pretrain	O
a	O
deeper	O
unsupervised	O
network	O
or	O
a	O
supervised	O
network	O
.	O
like	O
sparse	O
autoencoders	O
,	O
sparse	O
coding	O
,	O
contractive	O
autoencoders	O
and	O
other	O
regularized	O
autoencoders	O
,	O
the	O
motivation	O
for	O
daes	O
was	O
to	O
allow	O
the	O
learning	O
of	O
a	O
very	O
high-capacity	O
encoder	O
while	O
preventing	O
the	O
encoder	O
and	O
decoder	O
from	O
learning	O
a	O
useless	O
identity	O
function	O
.	O
2008	O
2010	O
prior	O
to	O
the	O
introduction	O
of	O
the	O
modern	O
dae	O
,	O
inayoshi	O
and	O
kurita	O
2005	O
)	O
explored	O
some	O
of	O
the	O
same	O
goals	O
with	O
some	O
of	O
the	O
same	O
methods	O
.	O
their	O
approach	O
minimizes	O
reconstruction	O
error	O
in	O
addition	O
to	O
a	O
supervised	O
objective	O
while	O
injecting	O
noise	O
in	O
the	O
hidden	O
layer	O
of	O
a	O
supervised	O
mlp	O
,	O
with	O
the	O
objective	O
to	O
improve	O
generalization	O
by	O
introducing	O
the	O
reconstruction	O
error	O
and	O
the	O
injected	O
noise	O
.	O
however	O
,	O
their	O
method	O
was	O
based	O
on	O
a	O
linear	O
encoder	O
and	O
could	O
not	O
learn	O
function	O
families	O
as	O
powerful	O
as	O
can	O
the	O
modern	O
dae	O
.	O
(	O
14.6	O
learning	O
manifolds	O
with	O
autoencoders	O
like	O
many	O
other	O
machine	O
learning	O
algorithms	O
,	O
autoencoders	O
exploit	O
the	O
idea	O
that	O
data	O
concentrates	O
around	O
a	O
low-dimensional	O
manifold	O
or	O
a	O
small	O
set	O
of	O
such	O
manifolds	O
,	O
as	O
described	O
in	O
section	O
.	O
some	O
machine	O
learning	O
algorithms	O
exploit	O
this	O
idea	O
only	O
insofar	O
as	O
that	O
they	O
learn	O
a	O
function	O
that	O
behaves	O
correctly	O
on	O
the	O
manifold	O
but	O
may	O
have	O
unusual	O
behavior	O
if	O
given	O
an	O
input	O
that	O
is	O
oﬀ	O
the	O
manifold	O
.	O
5.11.3	O
515	O
chapter	O
14.	O
autoencoders	O
autoencoders	O
take	O
this	O
idea	O
further	O
and	O
aim	O
to	O
learn	O
the	O
structure	O
of	O
the	O
manifold	O
.	O
to	O
understand	O
how	O
autoencoders	O
do	O
this	O
,	O
we	O
must	O
present	O
some	O
important	O
characteristics	O
of	O
manifolds	O
.	O
an	O
important	O
characterization	O
of	O
a	O
manifold	O
is	O
the	O
set	O
of	O
its	O
tangent	O
planes	O
.	O
at	O
a	O
point	O
x	O
on	O
a	O
d-dimensional	O
manifold	O
,	O
the	O
tangent	O
plane	O
is	O
given	O
by	O
d	O
basis	O
vectors	O
that	O
span	O
the	O
local	O
directions	O
of	O
variation	O
allowed	O
on	O
the	O
manifold	O
.	O
as	O
x	O
illustrated	O
in	O
ﬁgure	O
inﬁnitesimally	O
while	O
staying	O
on	O
the	O
manifold	O
.	O
,	O
these	O
local	O
directions	O
specify	O
how	O
one	O
can	O
change	O
14.6	O
all	O
autoencoder	O
training	O
procedures	O
involve	O
a	O
compromise	O
between	O
two	O
forces	O
:	O
1.	O
learning	O
a	O
representation	O
h	O
of	O
a	O
training	O
example	O
x	O
such	O
that	O
x	O
can	O
be	O
approximately	O
recovered	O
from	O
h	O
through	O
a	O
decoder	O
.	O
the	O
fact	O
that	O
x	O
is	O
drawn	O
from	O
the	O
training	O
data	O
is	O
crucial	O
,	O
because	O
it	O
means	O
the	O
autoencoder	O
need	O
not	O
successfully	O
reconstruct	O
inputs	O
that	O
are	O
not	O
probable	O
under	O
the	O
data	O
generating	O
distribution	O
.	O
2.	O
satisfying	O
the	O
constraint	O
or	O
regularization	O
penalty	O
.	O
this	O
can	O
be	O
an	O
architec-	O
tural	O
constraint	O
that	O
limits	O
the	O
capacity	O
of	O
the	O
autoencoder	O
,	O
or	O
it	O
can	O
be	O
a	O
regularization	O
term	O
added	O
to	O
the	O
reconstruction	O
cost	O
.	O
these	O
techniques	O
generally	O
prefer	O
solutions	O
that	O
are	O
less	O
sensitive	O
to	O
the	O
input	O
.	O
clearly	O
,	O
neither	O
force	O
alone	O
would	O
be	O
useful—copying	O
the	O
input	O
to	O
the	O
output	O
is	O
not	O
useful	O
on	O
its	O
own	O
,	O
nor	O
is	O
ignoring	O
the	O
input	O
.	O
instead	O
,	O
the	O
two	O
forces	O
together	O
are	O
useful	O
because	O
they	O
force	O
the	O
hidden	O
representation	O
to	O
capture	O
information	O
about	O
the	O
structure	O
of	O
the	O
data	O
generating	O
distribution	O
.	O
the	O
important	O
principle	O
is	O
that	O
the	O
autoencoder	O
can	O
aﬀord	O
to	O
represent	O
only	O
the	O
variations	O
that	O
are	O
needed	O
to	O
reconstruct	O
training	O
examples	O
.	O
if	O
the	O
data	O
generating	O
distribution	O
concentrates	O
near	O
a	O
low-dimensional	O
manifold	O
,	O
this	O
yields	O
representations	O
that	O
implicitly	O
capture	O
a	O
local	O
coordinate	O
system	O
for	O
this	O
manifold	O
:	O
only	O
the	O
variations	O
tangent	O
to	O
the	O
manifold	O
around	O
x	O
need	O
to	O
correspond	O
to	O
changes	O
in	O
h	O
=	O
f	O
(	O
x	O
)	O
.	O
hence	O
the	O
encoder	O
learns	O
a	O
mapping	O
from	O
the	O
input	O
space	O
x	O
to	O
a	O
representation	O
space	O
,	O
a	O
mapping	O
that	O
is	O
only	O
sensitive	O
to	O
changes	O
along	O
the	O
manifold	O
directions	O
,	O
but	O
that	O
is	O
insensitive	O
to	O
changes	O
orthogonal	O
to	O
the	O
manifold	O
.	O
a	O
one-dimensional	O
example	O
is	O
illustrated	O
in	O
ﬁgure	O
,	O
showing	O
that	O
,	O
by	O
making	O
the	O
reconstruction	O
function	O
insensitive	O
to	O
perturbations	O
of	O
the	O
input	O
around	O
the	O
data	O
points	O
,	O
we	O
cause	O
the	O
autoencoder	O
to	O
recover	O
the	O
manifold	O
structure	O
.	O
14.7	O
to	O
understand	O
why	O
autoencoders	O
are	O
useful	O
for	O
manifold	O
learning	O
,	O
it	O
is	O
in-	O
structive	O
to	O
compare	O
them	O
to	O
other	O
approaches	O
.	O
what	O
is	O
most	O
commonly	O
learned	O
to	O
characterize	O
a	O
manifold	O
is	O
a	O
representation	O
of	O
the	O
data	O
points	O
on	O
(	O
or	O
near	O
)	O
516	O
chapter	O
14.	O
autoencoders	O
figure	O
14.6	O
:	O
an	O
illustration	O
of	O
the	O
concept	O
of	O
a	O
tangent	O
hyperplane	O
.	O
here	O
we	O
create	O
a	O
one-dimensional	O
manifold	O
in	O
784-dimensional	O
space	O
.	O
we	O
take	O
an	O
mnist	O
image	O
with	O
784	O
pixels	O
and	O
transform	O
it	O
by	O
translating	O
it	O
vertically	O
.	O
the	O
amount	O
of	O
vertical	O
translation	O
deﬁnes	O
a	O
coordinate	O
along	O
a	O
one-dimensional	O
manifold	O
that	O
traces	O
out	O
a	O
curved	O
path	O
through	O
image	O
space	O
.	O
this	O
plot	O
shows	O
a	O
few	O
points	O
along	O
this	O
manifold	O
.	O
for	O
visualization	O
,	O
we	O
have	O
projected	O
the	O
manifold	O
into	O
two	O
dimensional	O
space	O
using	O
pca	O
.	O
an	O
n-dimensional	O
manifold	O
has	O
an	O
n-dimensional	O
tangent	O
plane	O
at	O
every	O
point	O
.	O
this	O
tangent	O
plane	O
touches	O
the	O
manifold	O
exactly	O
at	O
that	O
point	O
and	O
is	O
oriented	O
parallel	O
to	O
the	O
surface	O
at	O
that	O
point	O
.	O
it	O
deﬁnes	O
the	O
space	O
of	O
directions	O
in	O
which	O
it	O
is	O
possible	O
to	O
move	O
while	O
remaining	O
on	O
the	O
manifold	O
.	O
this	O
one-dimensional	O
manifold	O
has	O
a	O
single	O
tangent	O
line	O
.	O
we	O
indicate	O
an	O
example	O
tangent	O
line	O
at	O
one	O
point	O
,	O
with	O
an	O
image	O
showing	O
how	O
this	O
tangent	O
direction	O
appears	O
in	O
image	O
space	O
.	O
gray	O
pixels	O
indicate	O
pixels	O
that	O
do	O
not	O
change	O
as	O
we	O
move	O
along	O
the	O
tangent	O
line	O
,	O
white	O
pixels	O
indicate	O
pixels	O
that	O
brighten	O
,	O
and	O
black	O
pixels	O
indicate	O
pixels	O
that	O
darken	O
.	O
517	O
chapter	O
14.	O
autoencoders	O
1	O
0	O
.	O
0	O
8	O
.	O
0	O
6	O
.	O
0	O
4	O
.	O
0	O
2	O
.	O
0	O
0.	O
)	O
x	O
(	O
r	O
identity	O
optimal	O
reconstruction	O
x0	O
x	O
1	O
x	O
x2	O
0	O
figure	O
14.7	O
:	O
if	O
the	O
autoencoder	O
learns	O
a	O
reconstruction	O
function	O
that	O
is	O
invariant	O
to	O
small	O
perturbations	O
near	O
the	O
data	O
points	O
,	O
it	O
captures	O
the	O
manifold	O
structure	O
of	O
the	O
data	O
.	O
here	O
the	O
manifold	O
structure	O
is	O
a	O
collection	O
of	O
-dimensional	O
manifolds	O
.	O
the	O
dashed	O
diagonal	O
line	O
indicates	O
the	O
identity	O
function	O
target	O
for	O
reconstruction	O
.	O
the	O
optimal	O
reconstruction	O
function	O
crosses	O
the	O
identity	O
function	O
wherever	O
there	O
is	O
a	O
data	O
point	O
.	O
the	O
horizontal	O
arrows	O
at	O
the	O
bottom	O
of	O
the	O
plot	O
indicate	O
the	O
r	O
(	O
x	O
)	O
x	O
reconstruction	O
direction	O
vector	O
at	O
the	O
base	O
of	O
the	O
arrow	O
,	O
in	O
input	O
space	O
,	O
always	O
pointing	O
towards	O
the	O
nearest	O
“	O
manifold	O
”	O
(	O
a	O
single	O
datapoint	O
,	O
in	O
the	O
1-d	O
case	O
)	O
.	O
the	O
denoising	O
autoencoder	O
explicitly	O
tries	O
to	O
make	O
the	O
derivative	O
of	O
the	O
reconstruction	O
function	O
r	O
(	O
x	O
)	O
small	O
around	O
the	O
data	O
points	O
.	O
the	O
contractive	O
autoencoder	O
does	O
the	O
same	O
for	O
the	O
encoder	O
.	O
although	O
the	O
derivative	O
of	O
r	O
(	O
x	O
)	O
is	O
asked	O
to	O
be	O
small	O
around	O
the	O
data	O
points	O
,	O
it	O
can	O
be	O
large	O
between	O
the	O
data	O
points	O
.	O
the	O
space	O
between	O
the	O
data	O
points	O
corresponds	O
to	O
the	O
region	O
between	O
the	O
manifolds	O
,	O
where	O
the	O
reconstruction	O
function	O
must	O
have	O
a	O
large	O
derivative	O
in	O
order	O
to	O
map	O
corrupted	O
points	O
back	O
onto	O
the	O
manifold	O
.	O
−	O
the	O
manifold	O
.	O
such	O
a	O
representation	O
for	O
a	O
particular	O
example	O
is	O
also	O
called	O
its	O
embedding	O
.	O
it	O
is	O
typically	O
given	O
by	O
a	O
low-dimensional	O
vector	O
,	O
with	O
less	O
dimensions	O
than	O
the	O
“	O
ambient	O
”	O
space	O
of	O
which	O
the	O
manifold	O
is	O
a	O
low-dimensional	O
subset	O
.	O
some	O
algorithms	O
(	O
non-parametric	O
manifold	O
learning	O
algorithms	O
,	O
discussed	O
below	O
)	O
directly	O
learn	O
an	O
embedding	O
for	O
each	O
training	O
example	O
,	O
while	O
others	O
learn	O
a	O
more	O
general	O
mapping	O
,	O
sometimes	O
called	O
an	O
encoder	O
,	O
or	O
representation	O
function	O
,	O
that	O
maps	O
any	O
point	O
in	O
the	O
ambient	O
space	O
(	O
the	O
input	O
space	O
)	O
to	O
its	O
embedding	O
.	O
manifold	O
learning	O
has	O
mostly	O
focused	O
on	O
unsupervised	O
learning	O
procedures	O
that	O
attempt	O
to	O
capture	O
these	O
manifolds	O
.	O
most	O
of	O
the	O
initial	O
machine	O
learning	O
research	O
on	O
learning	O
nonlinear	O
manifolds	O
has	O
focused	O
on	O
non-parametric	O
methods	O
based	O
on	O
the	O
nearest-neighbor	O
graph	O
.	O
this	O
graph	O
has	O
one	O
node	O
per	O
training	O
example	O
and	O
edges	O
connecting	O
near	O
neighbors	O
to	O
each	O
other	O
.	O
these	O
methods	O
(	O
schölkopf	O
2000	O
brand	O
2003	O
belkin	O
et	O
al.	O
,	O
1998	O
roweis	O
and	O
saul	O
2000	O
tenenbaum	O
et	O
al.	O
,	O
;	O
;	O
,	O
;	O
,	O
;	O
518	O
chapter	O
14.	O
autoencoders	O
figure	O
14.8	O
:	O
non-parametric	O
manifold	O
learning	O
procedures	O
build	O
a	O
nearest	O
neighbor	O
graph	O
in	O
which	O
nodes	O
represent	O
training	O
examples	O
a	O
directed	O
edges	O
indicate	O
nearest	O
neighbor	O
relationships	O
.	O
various	O
procedures	O
can	O
thus	O
obtain	O
the	O
tangent	O
plane	O
associated	O
with	O
a	O
neighborhood	O
of	O
the	O
graph	O
as	O
well	O
as	O
a	O
coordinate	O
system	O
that	O
associates	O
each	O
training	O
example	O
with	O
a	O
real-valued	O
vector	O
position	B
,	O
or	O
embedding	O
.	O
it	O
is	O
possible	O
to	O
generalize	O
such	O
a	O
representation	O
to	O
new	O
examples	O
by	O
a	O
form	O
of	O
interpolation	O
.	O
so	O
long	O
as	O
the	O
number	O
of	O
examples	O
is	O
large	O
enough	O
to	O
cover	O
the	O
curvature	O
and	O
twists	O
of	O
the	O
manifold	O
,	O
these	O
approaches	O
work	B
well	O
.	O
images	O
from	O
the	O
qmul	O
multiview	O
face	O
dataset	O
(	O
gong	O
et	O
al	O
.	O
,	O
2000	O
)	O
.	O
,	O
,	O
;	O
;	O
,	O
and	O
niyogi	O
2003	O
donoho	O
and	O
grimes	O
2003	O
weinberger	O
and	O
saul	O
2004	O
hinton	O
and	O
roweis	O
2003	O
van	O
der	O
maaten	O
and	O
hinton	O
2008	O
)	O
associate	O
each	O
of	O
nodes	O
with	O
a	O
tangent	O
plane	O
that	O
spans	O
the	O
directions	O
of	O
variations	O
associated	O
with	O
the	O
diﬀerence	O
vectors	O
between	O
the	O
example	O
and	O
its	O
neighbors	O
,	O
as	O
illustrated	O
in	O
ﬁgure	O
.	O
14.8	O
;	O
,	O
,	O
;	O
a	O
global	O
coordinate	O
system	O
can	O
then	O
be	O
obtained	O
through	O
an	O
optimization	O
or	O
solving	O
a	O
linear	O
system	O
.	O
figure	O
illustrates	O
how	O
a	O
manifold	O
can	O
be	O
tiled	O
by	O
a	O
large	O
number	O
of	O
locally	O
linear	O
gaussian-like	O
patches	O
(	O
or	O
“	O
pancakes	O
,	O
”	O
because	O
the	O
gaussians	O
are	O
ﬂat	O
in	O
the	O
tangent	O
directions	O
)	O
.	O
14.9	O
however	O
,	O
there	O
is	O
a	O
fundamental	O
diﬃculty	O
with	O
such	O
local	O
non-parametric	O
approaches	O
to	O
manifold	O
learning	O
,	O
raised	O
in	O
)	O
:	O
if	O
the	O
manifolds	O
are	O
not	O
very	O
smooth	O
(	O
they	O
have	O
many	O
peaks	O
and	O
troughs	O
and	O
twists	O
)	O
,	O
one	O
may	O
need	O
a	O
very	O
large	O
number	O
of	O
training	O
examples	O
to	O
cover	O
each	O
one	O
of	O
bengio	O
and	O
monperrus	O
2005	O
(	O
519	O
chapter	O
14.	O
autoencoders	O
figure	O
14.9	O
:	O
if	O
the	O
tangent	O
planes	O
(	O
see	O
ﬁgure	O
)	O
at	O
each	O
location	O
are	O
known	O
,	O
then	O
they	O
can	O
be	O
tiled	O
to	O
form	O
a	O
global	O
coordinate	O
system	O
or	O
a	O
density	O
function	O
.	O
each	O
local	O
patch	O
can	O
be	O
thought	O
of	O
as	O
a	O
local	O
euclidean	O
coordinate	O
system	O
or	O
as	O
a	O
locally	O
ﬂat	O
gaussian	O
,	O
or	O
“	O
pancake	O
,	O
”	O
with	O
a	O
very	O
small	O
variance	O
in	O
the	O
directions	O
orthogonal	O
to	O
the	O
pancake	O
and	O
a	O
very	O
large	O
variance	O
in	O
the	O
directions	O
deﬁning	O
the	O
coordinate	O
system	O
on	O
the	O
pancake	O
.	O
a	O
mixture	O
of	O
these	O
gaussians	O
provides	O
an	O
estimated	O
density	O
function	O
,	O
as	O
in	O
the	O
manifold	O
parzen	O
window	O
algorithm	O
(	O
)	O
or	O
its	O
non-local	O
neural-net	O
based	O
variant	O
(	O
bengio	O
et	O
al	O
.	O
2006c	O
vincent	O
and	O
bengio	O
2003	O
)	O
.	O
,	O
14.6	O
,	O
these	O
variations	O
,	O
with	O
no	O
chance	O
to	O
generalize	O
to	O
unseen	O
variations	O
.	O
indeed	O
,	O
these	O
methods	O
can	O
only	O
generalize	O
the	O
shape	O
of	O
the	O
manifold	O
by	O
interpolating	O
between	O
neighboring	O
examples	O
.	O
unfortunately	O
,	O
the	O
manifolds	O
involved	O
in	O
ai	O
problems	O
can	O
have	O
very	O
complicated	O
structure	O
that	O
can	O
be	O
diﬃcult	O
to	O
capture	O
from	O
only	O
local	O
interpolation	O
.	O
consider	O
for	O
example	O
the	O
manifold	O
resulting	O
from	O
translation	O
shown	O
in	O
ﬁgure	O
xi	O
,	O
as	O
the	O
image	O
is	O
translated	O
,	O
we	O
will	O
observe	O
that	O
one	O
coordinate	O
encounters	O
a	O
peak	O
or	O
a	O
trough	O
in	O
its	O
value	O
once	O
for	O
every	O
peak	O
or	O
trough	O
in	O
brightness	O
in	O
the	O
image	O
.	O
in	O
other	O
words	O
,	O
the	O
complexity	O
of	O
the	O
patterns	O
of	O
brightness	O
in	O
an	O
underlying	O
image	O
template	O
drives	O
the	O
complexity	O
of	O
the	O
manifolds	O
that	O
are	O
generated	O
by	O
performing	O
simple	O
image	O
transformations	O
.	O
this	O
motivates	O
the	O
use	O
of	O
distributed	O
representations	O
and	O
deep	O
learning	O
for	O
capturing	O
manifold	O
structure	O
.	O
.	O
if	O
we	O
watch	O
just	O
one	O
coordinate	O
within	O
the	O
input	O
vector	O
,	O
14.6	O
520	O
chapter	O
14.	O
autoencoders	O
14.7	O
contractive	O
autoencoders	O
	O
	O
the	O
contractive	O
autoencoder	O
(	O
,	O
)	O
introduces	O
an	O
explicit	O
regularizer	O
on	O
the	O
code	O
h	O
=	O
f	O
(	O
x	O
)	O
,	O
encouraging	O
the	O
derivatives	O
of	O
f	O
to	O
be	O
as	O
small	O
as	O
possible	O
:	O
rifai	O
et	O
al	O
.	O
2011a	O
b	O
,	O
ω	O
(	O
)	O
=	O
h	O
λ	O
∂f	O
(	O
)	O
x	O
∂x	O
2	O
f	O
.	O
(	O
14.18	O
)	O
the	O
penalty	O
ω	O
(	O
h	O
)	O
is	O
the	O
squared	O
frobenius	O
norm	O
(	O
sum	O
of	O
squared	O
elements	O
)	O
of	O
the	O
jacobian	O
matrix	O
of	O
partial	O
derivatives	O
associated	O
with	O
the	O
encoder	O
function	O
.	O
(	O
alain	O
and	O
bengio	O
2013	O
there	O
is	O
a	O
connection	O
between	O
the	O
denoising	O
autoencoder	O
and	O
the	O
contractive	O
autoencoder	O
:	O
)	O
showed	O
that	O
in	O
the	O
limit	O
of	O
small	O
gaussian	O
input	O
noise	O
,	O
the	O
denoising	O
reconstruction	O
error	O
is	O
equivalent	O
to	O
a	O
contractive	O
penalty	O
on	O
the	O
reconstruction	O
function	O
that	O
maps	O
x	O
to	O
r	O
=	O
g	O
(	O
f	O
(	O
x	O
)	O
)	O
.	O
in	O
other	O
words	O
,	O
denoising	O
autoencoders	O
make	O
the	O
reconstruction	O
function	O
resist	O
small	O
but	O
ﬁnite-sized	O
perturbations	O
of	O
the	O
input	O
,	O
while	O
contractive	O
autoencoders	O
make	O
the	O
feature	O
extraction	O
function	O
resist	O
inﬁnitesimal	O
perturbations	O
of	O
the	O
input	O
.	O
when	O
using	O
the	O
jacobian-based	O
contractive	O
penalty	O
to	O
pretrain	O
features	O
f	O
(	O
x	O
)	O
for	O
use	O
with	O
a	O
classiﬁer	O
,	O
the	O
best	O
classiﬁcation	O
accuracy	O
usually	O
results	O
from	O
applying	O
the	O
contractive	O
penalty	O
to	O
f	O
(	O
x	O
)	O
rather	O
than	O
to	O
g	O
(	O
f	O
(	O
x	O
)	O
)	O
.	O
a	O
contractive	O
penalty	O
on	O
f	O
(	O
x	O
)	O
also	O
has	O
close	O
connections	O
to	O
score	O
matching	O
,	O
as	O
discussed	O
in	O
section	O
.	O
14.5.1	O
the	O
name	O
contractive	O
arises	O
from	O
the	O
way	O
that	O
the	O
cae	O
warps	O
space	O
.	O
speciﬁ-	O
cally	O
,	O
because	O
the	O
cae	O
is	O
trained	O
to	O
resist	O
perturbations	O
of	O
its	O
input	O
,	O
it	O
is	O
encouraged	O
to	O
map	O
a	O
neighborhood	O
of	O
input	O
points	O
to	O
a	O
smaller	O
neighborhood	O
of	O
output	O
points	O
.	O
we	O
can	O
think	O
of	O
this	O
as	O
contracting	O
the	O
input	O
neighborhood	O
to	O
a	O
smaller	O
output	O
neighborhood	O
.	O
	O
to	O
clarify	O
,	O
the	O
cae	O
is	O
contractive	O
only	O
locally—all	O
perturbations	O
of	O
a	O
training	O
point	O
x	O
are	O
mapped	O
near	O
to	O
f	O
(	O
x	O
)	O
.	O
globally	O
,	O
two	O
diﬀerent	O
points	O
x	O
and	O
x	O
may	O
be	O
	O
mapped	O
to	O
f	O
(	O
x	O
)	O
and	O
f	O
(	O
x	O
)	O
points	O
that	O
are	O
farther	O
apart	O
than	O
the	O
original	O
points	O
.	O
it	O
is	O
plausible	O
that	O
f	O
be	O
expanding	O
in-between	O
or	O
far	O
from	O
the	O
data	O
manifolds	O
(	O
see	O
ω	O
(	O
h	O
)	O
for	O
example	O
what	O
happens	O
in	O
the	O
1-d	O
toy	O
example	O
of	O
ﬁgure	O
penalty	O
is	O
applied	O
to	O
sigmoidal	O
units	O
,	O
one	O
easy	O
way	O
to	O
shrink	O
the	O
jacobian	O
is	O
to	O
make	O
the	O
sigmoid	O
units	O
saturate	O
to	O
.	O
this	O
encourages	O
the	O
cae	O
to	O
encode	O
input	O
points	O
with	O
extreme	O
values	O
of	O
the	O
sigmoid	O
that	O
may	O
be	O
interpreted	O
as	O
a	O
binary	O
code	O
.	O
it	O
also	O
ensures	O
that	O
the	O
cae	O
will	O
spread	O
its	O
code	O
values	O
throughout	O
most	O
of	O
the	O
hypercube	O
that	O
its	O
sigmoidal	O
hidden	O
units	O
can	O
span	O
.	O
)	O
.	O
when	O
the	O
14.7	O
or	O
0	O
1	O
we	O
can	O
think	O
of	O
the	O
jacobian	O
matrix	O
j	O
at	O
a	O
point	O
x	O
as	O
approximating	O
the	O
nonlinear	O
encoder	O
f	O
(	O
x	O
)	O
as	O
being	O
a	O
linear	O
operator	O
.	O
this	O
allows	O
us	O
to	O
use	O
the	O
word	O
“	O
contractive	O
”	O
more	O
formally	O
.	O
in	O
the	O
theory	O
of	O
linear	O
operators	O
,	O
a	O
linear	O
operator	O
521	O
chapter	O
14.	O
autoencoders	O
is	O
said	O
to	O
be	O
contractive	O
if	O
the	O
norm	O
of	O
j	O
x	O
remains	O
less	O
than	O
or	O
equal	O
to	O
for	O
all	O
unit-norm	O
x.	O
in	O
other	O
words	O
,	O
j	O
is	O
contractive	O
if	O
it	O
shrinks	O
the	O
unit	O
sphere	O
.	O
we	O
can	O
think	O
of	O
the	O
cae	O
as	O
penalizing	O
the	O
frobenius	O
norm	O
of	O
the	O
local	O
linear	O
approximation	O
of	O
f	O
(	O
x	O
)	O
at	O
every	O
training	O
point	O
x	O
in	O
order	O
to	O
encourage	O
each	O
of	O
these	O
local	O
linear	O
operator	O
to	O
become	O
a	O
contraction	O
.	O
1	O
as	O
described	O
in	O
section	O
,	O
regularized	O
autoencoders	O
learn	O
manifolds	O
by	O
14.6	O
in	O
the	O
case	O
of	O
the	O
cae	O
,	O
these	O
two	O
forces	O
are	O
balancing	O
two	O
opposing	O
forces	O
.	O
reconstruction	O
error	O
and	O
the	O
contractive	O
penalty	O
ω	O
(	O
h	O
)	O
.	O
reconstruction	O
error	O
alone	O
would	O
encourage	O
the	O
cae	O
to	O
learn	O
an	O
identity	O
function	O
.	O
the	O
contractive	O
penalty	O
alone	O
would	O
encourage	O
the	O
cae	O
to	O
learn	O
features	O
that	O
are	O
constant	O
with	O
respect	O
to	O
x.	O
the	O
compromise	O
between	O
these	O
two	O
forces	O
yields	O
an	O
autoencoder	O
whose	O
derivatives	O
∂f	O
(	O
)	O
x	O
are	O
mostly	O
tiny	O
.	O
only	O
a	O
small	O
number	O
of	O
hidden	O
units	O
,	O
corresponding	O
to	O
a	O
small	O
number	O
of	O
directions	O
in	O
the	O
input	O
,	O
may	O
have	O
signiﬁcant	O
derivatives	O
.	O
∂x	O
(	O
1	O
(	O
rifai	O
et	O
al	O
.	O
2011b	O
1	O
the	O
goal	O
of	O
the	O
cae	O
is	O
to	O
learn	O
the	O
manifold	O
structure	O
of	O
the	O
data	O
.	O
directions	O
x	O
with	O
large	O
j	O
x	O
rapidly	O
change	O
h	O
,	O
so	O
these	O
are	O
likely	O
to	O
be	O
directions	O
which	O
approximate	O
the	O
tangent	O
planes	O
of	O
the	O
manifold	O
.	O
experiments	O
by	O
rifai	O
et	O
al	O
.	O
2011a	O
)	O
and	O
)	O
show	O
that	O
training	O
the	O
cae	O
results	O
in	O
most	O
singular	O
values	O
of	O
j	O
dropping	O
below	O
in	O
magnitude	O
and	O
therefore	O
becoming	O
contractive	O
.	O
however	O
,	O
some	O
singular	O
values	O
remain	O
above	O
,	O
because	O
the	O
reconstruction	O
error	O
penalty	O
encourages	O
the	O
cae	O
to	O
encode	O
the	O
directions	O
with	O
the	O
most	O
local	O
variance	O
.	O
the	O
directions	O
corresponding	O
to	O
the	O
largest	O
singular	O
values	O
are	O
interpreted	O
as	O
the	O
tangent	O
directions	O
that	O
the	O
contractive	O
autoencoder	O
has	O
learned	O
.	O
ideally	O
,	O
these	O
tangent	O
directions	O
should	O
correspond	O
to	O
real	O
variations	O
in	O
the	O
data	O
.	O
for	O
example	O
,	O
a	O
cae	O
applied	O
to	O
images	O
should	O
learn	O
tangent	O
vectors	O
that	O
show	O
how	O
the	O
image	O
changes	O
as	O
objects	O
in	O
the	O
image	O
gradually	O
change	O
pose	O
,	O
as	O
shown	O
in	O
ﬁgure	O
.	O
visualizations	O
of	O
the	O
experimentally	O
obtained	O
singular	O
vectors	O
do	O
seem	O
to	O
correspond	O
to	O
meaningful	O
transformations	O
of	O
the	O
input	O
image	O
,	O
as	O
shown	O
in	O
ﬁgure	O
14.10	O
.	O
14.6	O
2011a	O
et	O
al	O
.	O
(	O
one	O
practical	O
issue	O
with	O
the	O
cae	O
regularization	O
criterion	O
is	O
that	O
although	O
it	O
is	O
cheap	O
to	O
compute	O
in	O
the	O
case	O
of	O
a	O
single	O
hidden	O
layer	O
autoencoder	O
,	O
it	O
becomes	O
much	O
more	O
expensive	O
in	O
the	O
case	O
of	O
deeper	O
autoencoders	O
.	O
the	O
strategy	O
followed	O
by	O
rifai	O
)	O
is	O
to	O
separately	O
train	O
a	O
series	O
of	O
single-layer	O
autoencoders	O
,	O
each	O
trained	O
to	O
reconstruct	O
the	O
previous	O
autoencoder	O
’	O
s	O
hidden	O
layer	O
.	O
the	O
composition	O
of	O
these	O
autoencoders	O
then	O
forms	O
a	O
deep	O
autoencoder	O
.	O
because	O
each	O
layer	O
was	O
separately	O
trained	O
to	O
be	O
locally	O
contractive	O
,	O
the	O
deep	O
autoencoder	O
is	O
contractive	O
as	O
well	O
.	O
the	O
result	O
is	O
not	O
the	O
same	O
as	O
what	O
would	O
be	O
obtained	O
by	O
jointly	O
training	O
the	O
entire	O
architecture	O
with	O
a	O
penalty	O
on	O
the	O
jacobian	O
of	O
the	O
deep	O
model	B
,	O
but	O
it	O
captures	O
many	O
of	O
the	O
desirable	O
qualitative	O
characteristics	O
.	O
another	O
practical	O
issue	O
is	O
that	O
the	O
contraction	O
penalty	O
can	O
obtain	O
useless	O
results	O
522	O
chapter	O
14.	O
autoencoders	O
input	O
point	O
tangent	O
vectors	O
local	O
pca	O
(	O
no	O
sharing	O
across	O
regions	O
)	O
contractive	O
autoencoder	O
figure	O
14.10	O
:	O
illustration	O
of	O
tangent	O
vectors	O
of	O
the	O
manifold	O
estimated	O
by	O
local	O
pca	O
and	O
by	O
a	O
contractive	O
autoencoder	O
.	O
the	O
location	O
on	O
the	O
manifold	O
is	O
deﬁned	O
by	O
the	O
input	O
image	O
of	O
a	O
dog	O
drawn	O
from	O
the	O
cifar-10	O
dataset	O
.	O
the	O
tangent	O
vectors	O
are	O
estimated	O
by	O
the	O
leading	O
singular	O
vectors	O
of	O
the	O
jacobian	O
matrix	O
∂h	O
of	O
the	O
input-to-code	O
mapping	O
.	O
∂x	O
although	O
both	O
local	O
pca	O
and	O
the	O
cae	O
can	O
capture	O
local	O
tangents	O
,	O
the	O
cae	O
is	O
able	O
to	O
form	O
more	O
accurate	O
estimates	O
from	O
limited	O
training	O
data	O
because	O
it	O
exploits	O
parameter	O
sharing	O
across	O
diﬀerent	O
locations	O
that	O
share	O
a	O
subset	O
of	O
active	O
hidden	O
units	O
.	O
the	O
cae	O
tangent	O
directions	O
typically	O
correspond	O
to	O
moving	O
or	O
changing	O
parts	O
of	O
the	O
object	O
(	O
such	O
as	O
the	O
head	O
or	O
legs	O
)	O
.	O
images	O
reproduced	O
with	O
permission	O
from	O
rifai	O
et	O
al	O
.	O
2011c	O
)	O
.	O
(	O
if	O
we	O
do	O
not	O
impose	O
some	O
sort	O
of	O
scale	O
on	O
the	O
decoder	O
.	O
for	O
example	O
,	O
the	O
encoder	O
could	O
consist	O
of	O
multiplying	O
the	O
input	O
by	O
a	O
small	O
constant	O
	O
and	O
the	O
decoder	O
could	O
consist	O
of	O
dividing	O
the	O
code	O
by	O
	O
.	O
as	O
	O
approaches	O
,	O
the	O
encoder	O
drives	O
the	O
0	O
contractive	O
penalty	O
ω	O
(	O
h	O
)	O
to	O
approach	O
without	O
having	O
learned	O
anything	O
about	O
the	O
distribution	O
.	O
meanwhile	O
,	O
the	O
decoder	O
maintains	O
perfect	O
reconstruction	O
.	O
in	O
rifai	O
et	O
al	O
.	O
(	O
f	O
and	O
g.	O
both	O
f	O
and	O
g	O
are	O
standard	O
neural	O
network	O
layers	O
consisting	O
of	O
an	O
aﬃne	O
transformation	O
followed	O
by	O
an	O
element-wise	O
nonlinearity	O
,	O
so	O
it	O
is	O
straightforward	O
to	O
set	O
the	O
weight	O
matrix	O
of	O
g	O
to	O
be	O
the	O
transpose	O
of	O
the	O
weight	O
matrix	O
of	O
)	O
,	O
this	O
is	O
prevented	O
by	O
tying	O
the	O
weights	O
of	O
2011a	O
.f	O
0	O
14.8	O
predictive	O
sparse	O
decomposition	O
predictive	O
sparse	O
decomposition	O
(	O
psd	O
)	O
is	O
a	O
model	B
that	O
is	O
a	O
hybrid	O
of	O
sparse	O
coding	O
and	O
parametric	O
autoencoders	O
(	O
kavukcuoglu	O
)	O
.	O
a	O
parametric	O
encoder	O
is	O
trained	O
to	O
predict	O
the	O
output	O
of	O
iterative	O
inference	O
.	O
psd	O
has	O
been	O
applied	O
to	O
unsupervised	O
feature	O
learning	O
for	O
object	O
recognition	B
in	O
images	O
and	O
video	O
)	O
,	O
as	O
well	O
(	O
kavukcuoglu	O
f	O
(	O
x	O
)	O
and	O
a	O
as	O
for	O
audio	O
(	O
decoder	O
g	O
(	O
h	O
)	O
that	O
are	O
both	O
parametric	O
.	O
during	O
training	O
,	O
h	O
is	O
controlled	O
by	O
the	O
;	O
et	O
al.	O
,	O
henaﬀ	O
et	O
al	O
.	O
2011	O
)	O
.	O
the	O
model	B
consists	O
of	O
an	O
encoder	O
2009	O
2010	O
jarrett	O
2009	O
farabet	O
et	O
al.	O
,	O
et	O
al.	O
,	O
et	O
al.	O
,	O
2008	O
,	O
,	O
;	O
2011	O
523	O
chapter	O
14.	O
autoencoders	O
optimization	O
algorithm	O
.	O
training	O
proceeds	O
by	O
minimizing	O
||	O
(	O
)	O
x	O
|	O
|	O
||	O
−	O
x	O
g	O
(	O
)	O
h	O
2	O
+	O
λ	O
h	O
1	O
+	O
||	O
−	O
h	O
||	O
γ	O
f	O
2	O
.	O
(	O
14.19	O
)	O
like	O
in	O
sparse	O
coding	O
,	O
the	O
training	O
algorithm	O
alternates	O
between	O
minimization	O
with	O
respect	O
to	O
h	O
and	O
minimization	O
with	O
respect	O
to	O
the	O
model	B
parameters	O
.	O
minimization	O
with	O
respect	O
to	O
h	O
is	O
fast	O
because	O
f	O
(	O
x	O
)	O
provides	O
a	O
good	O
initial	O
value	O
of	O
h	O
and	O
the	O
cost	O
function	O
constrains	O
h	O
to	O
remain	O
near	O
f	O
(	O
x	O
)	O
anyway	O
.	O
simple	O
gradient	O
descent	B
can	O
obtain	O
reasonable	O
values	O
of	O
in	O
as	O
few	O
as	O
ten	O
steps	O
.	O
h	O
the	O
training	O
procedure	O
used	O
by	O
psd	O
is	O
diﬀerent	O
from	O
ﬁrst	O
training	O
a	O
sparse	O
coding	O
model	B
and	O
then	O
training	O
f	O
(	O
x	O
)	O
to	O
predict	O
the	O
values	O
of	O
the	O
sparse	O
coding	O
features	O
.	O
the	O
psd	O
training	O
procedure	O
regularizes	O
the	O
decoder	O
to	O
use	O
parameters	O
for	O
which	O
can	O
infer	O
good	O
code	O
values	O
.	O
f	O
(	O
)	O
x	O
19.5	O
,	O
this	O
topic	O
is	O
developed	O
further	O
.	O
the	O
tools	O
presented	O
in	O
chapter	O
predictive	O
sparse	O
coding	O
is	O
an	O
example	O
of	O
learned	O
approximate	O
inference	O
.	O
in	O
section	O
19	O
make	O
it	O
clear	O
that	O
psd	O
can	O
be	O
interpreted	O
as	O
training	O
a	O
directed	O
sparse	O
coding	O
probabilistic	O
model	B
by	O
maximizing	O
a	O
lower	O
bound	B
on	O
the	O
log-likelihood	O
of	O
the	O
model	B
.	O
in	O
practical	O
applications	O
of	O
psd	O
,	O
the	O
iterative	O
optimization	O
is	O
only	O
used	O
during	O
training	O
.	O
the	O
parametric	O
encoder	O
f	O
is	O
used	O
to	O
compute	O
the	O
learned	O
features	O
when	O
the	O
model	B
is	O
deployed	O
.	O
evaluating	O
f	O
is	O
computationally	O
inexpensive	O
compared	O
to	O
inferring	O
h	O
via	O
gradient	O
descent	B
.	O
because	O
f	O
is	O
a	O
diﬀerentiable	O
parametric	O
function	O
,	O
psd	O
models	O
may	O
be	O
stacked	O
and	O
used	O
to	O
initialize	O
a	O
deep	O
network	O
to	O
be	O
trained	O
with	O
another	O
criterion	O
.	O
14.9	O
applications	O
of	O
autoencoders	O
autoencoders	O
have	O
been	O
successfully	O
applied	O
to	O
dimensionality	O
reduction	O
and	O
infor-	O
mation	O
retrieval	O
tasks	O
.	O
dimensionality	O
reduction	O
was	O
one	O
of	O
the	O
ﬁrst	O
applications	O
of	O
representation	O
learning	O
and	O
deep	O
learning	O
.	O
it	O
was	O
one	O
of	O
the	O
early	O
motivations	O
for	O
studying	O
autoencoders	O
.	O
for	O
example	O
,	O
hinton	O
and	O
salakhutdinov	O
2006	O
)	O
trained	O
a	O
stack	O
of	O
rbms	O
and	O
then	O
used	O
their	O
weights	O
to	O
initialize	O
a	O
deep	O
autoencoder	O
with	O
gradually	O
smaller	O
hidden	O
layers	O
,	O
culminating	O
in	O
a	O
bottleneck	O
of	O
30	O
units	O
.	O
the	O
resulting	O
code	O
yielded	O
less	O
reconstruction	O
error	O
than	O
pca	O
into	O
30	O
dimensions	O
and	O
the	O
learned	O
representation	O
was	O
qualitatively	O
easier	O
to	O
interpret	O
and	O
relate	O
to	O
the	O
underlying	O
categories	O
,	O
with	O
these	O
categories	O
manifesting	O
as	O
well-separated	O
clusters	O
.	O
(	O
lower-dimensional	O
representations	O
can	O
improve	O
performance	O
on	O
many	O
tasks	O
,	O
such	O
as	O
classiﬁcation	O
.	O
models	O
of	O
smaller	O
spaces	O
consume	O
less	O
memory	O
and	O
runtime	O
.	O
524	O
chapter	O
14.	O
autoencoders	O
many	O
forms	O
of	O
dimensionality	O
reduction	O
place	O
semantically	O
related	O
examples	O
near	O
each	O
other	O
,	O
as	O
observed	O
by	O
salakhutdinov	O
and	O
hinton	O
2007b	O
et	O
al	O
.	O
(	O
)	O
.	O
the	O
hints	O
provided	O
by	O
the	O
mapping	O
to	O
the	O
lower-dimensional	O
space	O
aid	O
2008	O
generalization	O
.	O
torralba	O
)	O
and	O
(	O
one	O
task	O
that	O
beneﬁts	O
even	O
more	O
than	O
usual	O
from	O
dimensionality	O
reduction	O
is	O
information	O
retrieval	O
,	O
the	O
task	O
of	O
ﬁnding	O
entries	O
in	O
a	O
database	O
that	O
resemble	O
a	O
query	O
entry	O
.	O
this	O
task	O
derives	O
the	O
usual	O
beneﬁts	O
from	O
dimensionality	O
reduction	O
that	O
other	O
tasks	O
do	O
,	O
but	O
also	O
derives	O
the	O
additional	O
beneﬁt	O
that	O
search	O
can	O
become	O
extremely	O
eﬃcient	O
in	O
certain	O
kinds	O
of	O
low	O
dimensional	O
spaces	O
.	O
speciﬁcally	O
,	O
if	O
we	O
train	O
the	O
dimensionality	O
reduction	O
algorithm	O
to	O
produce	O
a	O
code	O
that	O
is	O
low-	O
dimensional	O
and	O
,	O
then	O
we	O
can	O
store	O
all	O
database	O
entries	O
in	O
a	O
hash	O
table	O
mapping	O
binary	O
code	O
vectors	O
to	O
entries	O
.	O
this	O
hash	O
table	O
allows	O
us	O
to	O
perform	O
information	O
retrieval	O
by	O
returning	O
all	O
database	O
entries	O
that	O
have	O
the	O
same	O
binary	O
code	O
as	O
the	O
query	O
.	O
we	O
can	O
also	O
search	O
over	O
slightly	O
less	O
similar	O
entries	O
very	O
eﬃciently	O
,	O
just	O
by	O
ﬂipping	O
individual	O
bits	O
from	O
the	O
encoding	O
of	O
the	O
query	O
.	O
this	O
approach	O
to	O
information	O
retrieval	O
via	O
dimensionality	O
reduction	O
and	O
binarization	O
is	O
called	O
semantic	O
hashing	O
(	O
salakhutdinov	O
and	O
hinton	O
2007b	O
2009b	O
)	O
,	O
and	O
has	O
been	O
applied	O
to	O
both	O
textual	O
input	O
(	O
salakhutdinov	O
and	O
hinton	O
2007b	O
2009b	O
)	O
and	O
images	O
(	O
torralba	O
2008	O
krizhevsky	O
and	O
hinton	O
2011	O
2008	O
weiss	O
binary	O
et	O
al.	O
,	O
et	O
al.	O
,	O
)	O
.	O
,	O
,	O
,	O
,	O
,	O
;	O
;	O
to	O
produce	O
binary	O
codes	O
for	O
semantic	O
hashing	O
,	O
one	O
typically	O
uses	O
an	O
encoding	O
function	O
with	O
sigmoids	O
on	O
the	O
ﬁnal	O
layer	O
.	O
the	O
sigmoid	O
units	O
must	O
be	O
trained	O
to	O
be	O
saturated	O
to	O
nearly	O
0	O
or	O
nearly	O
1	O
for	O
all	O
input	O
values	O
.	O
one	O
trick	B
that	O
can	O
accomplish	O
this	O
is	O
simply	O
to	O
inject	O
additive	O
noise	O
just	O
before	O
the	O
sigmoid	O
nonlinearity	O
during	O
training	O
.	O
the	O
magnitude	O
of	O
the	O
noise	O
should	O
increase	O
over	O
time	O
.	O
to	O
ﬁght	O
that	O
noise	O
and	O
preserve	O
as	O
much	O
information	O
as	O
possible	O
,	O
the	O
network	O
must	O
increase	O
the	O
magnitude	O
of	O
the	O
inputs	O
to	O
the	O
sigmoid	O
function	O
,	O
until	O
saturation	O
occurs	O
.	O
the	O
idea	O
of	O
learning	O
a	O
hashing	O
function	O
has	O
been	O
further	O
explored	O
in	O
several	O
directions	O
,	O
including	O
the	O
idea	O
of	O
training	O
the	O
representations	O
so	O
as	O
to	O
optimize	O
a	O
loss	O
more	O
directly	O
linked	O
to	O
the	O
task	O
of	O
ﬁnding	O
nearby	O
examples	O
in	O
the	O
hash	O
table	O
(	O
norouzi	O
and	O
fleet	O
2011	O
)	O
.	O
,	O
525	O
chapter	O
15	O
representation	O
learning	O
in	O
this	O
chapter	O
,	O
we	O
ﬁrst	O
discuss	O
what	O
it	O
means	O
to	O
learn	O
representations	O
and	O
how	O
the	O
notion	O
of	O
representation	O
can	O
be	O
useful	O
to	O
design	O
deep	O
architectures	O
.	O
we	O
discuss	O
how	O
learning	O
algorithms	O
share	O
statistical	O
strength	O
across	O
diﬀerent	O
tasks	O
,	O
including	O
using	O
information	O
from	O
unsupervised	O
tasks	O
to	O
perform	O
supervised	O
tasks	O
.	O
shared	O
representations	O
are	O
useful	O
to	O
handle	O
multiple	O
modalities	O
or	O
domains	O
,	O
or	O
to	O
transfer	O
learned	O
knowledge	O
to	O
tasks	O
for	O
which	O
few	O
or	O
no	O
examples	O
are	O
given	O
but	O
a	O
task	O
representation	O
exists	O
.	O
finally	O
,	O
we	O
step	O
back	O
and	O
argue	O
about	O
the	O
reasons	O
for	O
the	O
success	O
of	O
representation	O
learning	O
,	O
starting	O
with	O
the	O
theoretical	O
advantages	O
of	O
distributed	O
representations	O
(	O
hinton	O
)	O
and	O
deep	O
representations	O
and	O
ending	O
with	O
the	O
more	O
general	O
idea	O
of	O
underlying	O
assumptions	O
about	O
the	O
data	O
generating	O
process	O
,	O
in	O
particular	O
about	O
underlying	O
causes	O
of	O
the	O
observed	O
data	O
.	O
et	O
al.	O
,	O
1986	O
many	O
information	O
processing	O
tasks	O
can	O
be	O
very	O
easy	O
or	O
very	O
diﬃcult	O
depending	O
on	O
how	O
the	O
information	O
is	O
represented	O
.	O
this	O
is	O
a	O
general	O
principle	O
applicable	O
to	O
daily	O
life	O
,	O
computer	O
science	O
in	O
general	O
,	O
and	O
to	O
machine	O
learning	O
.	O
for	O
example	O
,	O
it	O
is	O
straightforward	O
for	O
a	O
person	O
to	O
divide	O
210	O
by	O
6	O
using	O
long	O
division	O
.	O
the	O
task	O
becomes	O
considerably	O
less	O
straightforward	O
if	O
it	O
is	O
instead	O
posed	O
using	O
the	O
roman	O
numeral	O
representation	O
of	O
the	O
numbers	O
.	O
most	O
modern	O
people	O
asked	O
to	O
divide	O
ccx	O
by	O
vi	O
would	O
begin	O
by	O
converting	O
the	O
numbers	O
to	O
the	O
arabic	O
numeral	O
representation	O
,	O
permitting	O
long	O
division	O
procedures	O
that	O
make	O
use	O
of	O
the	O
place	O
value	O
system	O
.	O
more	O
concretely	O
,	O
we	O
can	O
quantify	O
the	O
asymptotic	O
runtime	O
of	O
various	O
operations	O
using	O
appropriate	O
or	O
inappropriate	O
representations	O
.	O
for	O
example	O
,	O
inserting	O
a	O
number	O
into	O
the	O
correct	O
position	B
in	O
a	O
sorted	O
list	O
of	O
numbers	O
is	O
an	O
o	O
(	O
n	O
)	O
operation	O
if	O
the	O
list	O
is	O
represented	O
as	O
a	O
linked	O
list	O
,	O
but	O
only	O
o	O
(	O
log	O
n	O
)	O
if	O
the	O
list	O
is	O
represented	O
as	O
a	O
red-black	O
tree	O
.	O
in	O
the	O
context	O
of	O
machine	O
learning	O
,	O
what	O
makes	O
one	O
representation	O
better	O
than	O
526	O
chapter	O
15.	O
representation	O
learning	O
another	O
?	O
generally	O
speaking	O
,	O
a	O
good	O
representation	O
is	O
one	O
that	O
makes	O
a	O
subsequent	O
learning	O
task	O
easier	O
.	O
the	O
choice	O
of	O
representation	O
will	O
usually	O
depend	O
on	O
the	O
choice	O
of	O
the	O
subsequent	O
learning	O
task	O
.	O
we	O
can	O
think	O
of	O
feedforward	O
networks	O
trained	O
by	O
supervised	O
learning	O
as	O
per-	O
forming	O
a	O
kind	O
of	O
representation	O
learning	O
.	O
speciﬁcally	O
,	O
the	O
last	O
layer	O
of	O
the	O
network	O
is	O
typically	O
a	O
linear	O
classiﬁer	O
,	O
such	O
as	O
a	O
softmax	O
regression	O
classiﬁer	O
.	O
the	O
rest	O
of	O
the	O
network	O
learns	O
to	O
provide	O
a	O
representation	O
to	O
this	O
classiﬁer	O
.	O
training	O
with	O
a	O
supervised	O
criterion	O
naturally	O
leads	O
to	O
the	O
representation	O
at	O
every	O
hidden	O
layer	O
(	O
but	O
more	O
so	O
near	O
the	O
top	O
hidden	O
layer	O
)	O
taking	O
on	O
properties	O
that	O
make	O
the	O
classiﬁcation	O
task	O
easier	O
.	O
for	O
example	O
,	O
classes	O
that	O
were	O
not	O
linearly	O
separable	O
in	O
the	O
input	O
features	O
may	O
become	O
linearly	O
separable	O
in	O
the	O
last	O
hidden	O
layer	O
.	O
in	O
principle	O
,	O
the	O
last	O
layer	O
could	O
be	O
another	O
kind	O
of	O
model	B
,	O
such	O
as	O
a	O
nearest	O
neighbor	O
classiﬁer	O
(	O
salakhutdinov	O
and	O
hinton	O
2007a	O
)	O
.	O
the	O
features	O
in	O
the	O
penultimate	O
layer	O
should	O
learn	O
diﬀerent	O
properties	O
depending	O
on	O
the	O
type	O
of	O
the	O
last	O
layer	O
.	O
,	O
supervised	O
training	O
of	O
feedforward	O
networks	O
does	O
not	O
involve	O
explicitly	O
imposing	O
any	O
condition	O
on	O
the	O
learned	O
intermediate	O
features	O
.	O
other	O
kinds	O
of	O
representation	O
learning	O
algorithms	O
are	O
often	O
explicitly	O
designed	O
to	O
shape	O
the	O
representation	O
in	O
some	O
particular	O
way	O
.	O
for	O
example	O
,	O
suppose	O
we	O
want	O
to	O
learn	O
a	O
representation	O
that	O
makes	O
density	O
estimation	O
easier	O
.	O
distributions	O
with	O
more	O
independences	O
are	O
easier	O
to	O
model	B
,	O
so	O
we	O
could	O
design	O
an	O
objective	O
function	O
that	O
encourages	O
the	O
elements	O
of	O
the	O
representation	O
vector	O
h	O
to	O
be	O
independent	O
.	O
just	O
like	O
supervised	O
networks	O
,	O
unsupervised	O
deep	O
learning	O
algorithms	O
have	O
a	O
main	O
training	O
objective	O
but	O
also	O
learn	O
a	O
representation	O
as	O
a	O
side	O
eﬀect	O
.	O
regardless	O
of	O
how	O
a	O
representation	O
was	O
obtained	O
,	O
it	O
can	O
be	O
used	O
for	O
another	O
task	O
.	O
alternatively	O
,	O
multiple	O
tasks	O
(	O
some	O
supervised	O
,	O
some	O
unsupervised	O
)	O
can	O
be	O
learned	O
together	O
with	O
some	O
shared	O
internal	O
representation	O
.	O
most	O
representation	O
learning	O
problems	O
face	O
a	O
tradeoﬀ	O
between	O
preserving	O
as	O
much	O
information	O
about	O
the	O
input	O
as	O
possible	O
and	O
attaining	O
nice	O
properties	O
(	O
such	O
as	O
independence	O
)	O
.	O
representation	O
learning	O
is	O
particularly	O
interesting	O
because	O
it	O
provides	O
one	O
way	O
to	O
perform	O
unsupervised	O
and	O
semi-supervised	O
learning	O
.	O
we	O
often	O
have	O
very	O
large	O
amounts	O
of	O
unlabeled	O
training	O
data	O
and	O
relatively	O
little	O
labeled	O
training	O
data	O
.	O
training	O
with	O
supervised	O
learning	O
techniques	O
on	O
the	O
labeled	O
subset	O
often	O
results	O
in	O
severe	O
overﬁtting	O
.	O
semi-supervised	O
learning	O
oﬀers	O
the	O
chance	O
to	O
resolve	O
this	O
overﬁtting	O
problem	O
by	O
also	O
learning	O
from	O
the	O
unlabeled	O
data	O
.	O
speciﬁcally	O
,	O
we	O
can	O
learn	O
good	O
representations	O
for	O
the	O
unlabeled	O
data	O
,	O
and	O
then	O
use	O
these	O
representations	O
to	O
solve	O
the	O
supervised	O
learning	O
task	O
.	O
humans	O
and	O
animals	O
are	O
able	O
to	O
learn	O
from	O
very	O
few	O
labeled	O
examples	O
.	O
we	O
do	O
527	O
chapter	O
15.	O
representation	O
learning	O
not	O
yet	O
know	O
how	O
this	O
is	O
possible	O
.	O
many	O
factors	O
could	O
explain	O
improved	O
human	O
performance—for	O
example	O
,	O
the	O
brain	O
may	O
use	O
very	O
large	O
ensembles	O
of	O
classiﬁers	O
or	O
bayesian	O
inference	O
techniques	O
.	O
one	O
popular	O
hypothesis	O
is	O
that	O
the	O
brain	O
is	O
able	O
to	O
leverage	O
unsupervised	O
or	O
semi-supervised	O
learning	O
.	O
there	O
are	O
many	O
ways	O
to	O
leverage	O
unlabeled	O
data	O
.	O
in	O
this	O
chapter	O
,	O
we	O
focus	O
on	O
the	O
hypothesis	O
that	O
the	O
unlabeled	O
data	O
can	O
be	O
used	O
to	O
learn	O
a	O
good	O
representation	O
.	O
15.1	O
greedy	O
layer-wise	O
unsupervised	O
pretraining	O
unsupervised	O
learning	O
played	O
a	O
key	O
historical	O
role	O
in	O
the	O
revival	O
of	O
deep	O
neural	O
networks	O
,	O
enabling	O
researchers	O
for	O
the	O
ﬁrst	O
time	O
to	O
train	O
a	O
deep	O
supervised	O
network	O
without	O
requiring	O
architectural	O
specializations	O
like	O
convolution	O
or	O
recurrence	O
.	O
we	O
call	O
this	O
procedure	O
unsupervised	O
pretraining	O
,	O
or	O
more	O
precisely	O
,	O
greedy	O
layer-	O
wise	O
unsupervised	O
pretraining	O
.	O
this	O
procedure	O
is	O
a	O
canonical	O
example	O
of	O
how	O
a	O
representation	O
learned	O
for	O
one	O
task	O
(	O
unsupervised	O
learning	O
,	O
trying	O
to	O
capture	O
the	O
shape	O
of	O
the	O
input	O
distribution	O
)	O
can	O
sometimes	O
be	O
useful	O
for	O
another	O
task	O
(	O
supervised	O
learning	O
with	O
the	O
same	O
input	O
domain	O
)	O
.	O
greedy	O
layer-wise	O
unsupervised	O
pretraining	O
relies	O
on	O
a	O
single-layer	O
represen-	O
tation	O
learning	O
algorithm	O
such	O
as	O
an	O
rbm	O
,	O
a	O
single-layer	O
autoencoder	O
,	O
a	O
sparse	O
coding	O
model	B
,	O
or	O
another	O
model	B
that	O
learns	O
latent	O
representations	O
.	O
each	O
layer	O
is	O
pretrained	O
using	O
unsupervised	O
learning	O
,	O
taking	O
the	O
output	O
of	O
the	O
previous	O
layer	O
and	O
producing	O
as	O
output	O
a	O
new	O
representation	O
of	O
the	O
data	O
,	O
whose	O
distribution	O
(	O
or	O
its	O
relation	O
to	O
other	O
variables	O
such	O
as	O
categories	O
to	O
predict	O
)	O
is	O
hopefully	O
simpler	O
.	O
see	O
algorithm	O
for	O
a	O
formal	O
description	O
.	O
15.1	O
,	O
,	O
;	O
greedy	O
layer-wise	O
training	O
procedures	O
based	O
on	O
unsupervised	O
criteria	O
have	O
long	O
been	O
used	O
to	O
sidestep	O
the	O
diﬃculty	O
of	O
jointly	O
training	O
the	O
layers	O
of	O
a	O
deep	O
neural	O
net	O
for	O
a	O
supervised	O
task	O
.	O
this	O
approach	O
dates	O
back	O
at	O
least	O
as	O
far	O
as	O
the	O
neocognitron	O
(	O
fukushima	O
1975	O
)	O
.	O
the	O
deep	O
learning	O
renaissance	O
of	O
2006	O
began	O
with	O
the	O
discovery	O
that	O
this	O
greedy	O
learning	O
procedure	O
could	O
be	O
used	O
to	O
ﬁnd	O
a	O
good	O
initialization	O
for	O
a	O
joint	O
learning	O
procedure	O
over	O
all	O
the	O
layers	O
,	O
and	O
that	O
this	O
approach	O
could	O
be	O
used	O
;	O
to	O
successfully	O
train	O
even	O
fully	O
connected	O
architectures	O
(	O
hinton	O
2006	O
hinton	O
and	O
salakhutdinov	O
2006	O
hinton	O
2006	O
bengio	O
)	O
.	O
et	O
al.	O
,	O
2007a	O
prior	O
to	O
this	O
discovery	O
,	O
only	O
convolutional	O
deep	O
networks	O
or	O
networks	O
whose	O
depth	O
resulted	O
from	O
recurrence	O
were	O
regarded	O
as	O
feasible	O
to	O
train	O
.	O
today	O
,	O
we	O
now	O
know	O
that	O
greedy	O
layer-wise	O
pretraining	O
is	O
not	O
required	O
to	O
train	O
fully	O
connected	O
deep	O
architectures	O
,	O
but	O
the	O
unsupervised	O
pretraining	O
approach	O
was	O
the	O
ﬁrst	O
method	O
to	O
succeed	O
.	O
et	O
al.	O
,	O
2007	O
ranzato	O
et	O
al.	O
,	O
;	O
,	O
;	O
greedy	O
layer-wise	O
pretraining	O
is	O
called	O
greedy	O
because	O
it	O
is	O
a	O
greedy	O
algo-	O
528	O
chapter	O
15.	O
representation	O
learning	O
rithm	O
,	O
meaning	O
that	O
it	O
optimizes	O
each	O
piece	O
of	O
the	O
solution	O
independently	O
,	O
one	O
piece	O
at	O
a	O
time	O
,	O
rather	O
than	O
jointly	O
optimizing	O
all	O
pieces	O
.	O
it	O
is	O
called	O
layer-wise	O
because	O
these	O
independent	O
pieces	O
are	O
the	O
layers	O
of	O
the	O
network	O
.	O
speciﬁcally	O
,	O
greedy	O
layer-wise	O
pretraining	O
proceeds	O
one	O
layer	O
at	O
a	O
time	O
,	O
training	O
the	O
k-th	O
layer	O
while	O
keeping	O
the	O
previous	O
ones	O
ﬁxed	O
.	O
in	O
particular	O
,	O
the	O
lower	O
layers	O
(	O
which	O
are	O
trained	O
ﬁrst	O
)	O
are	O
not	O
adapted	O
after	O
the	O
upper	O
layers	O
are	O
introduced	O
.	O
it	O
is	O
called	O
unsuper-	O
vised	O
because	O
each	O
layer	O
is	O
trained	O
with	O
an	O
unsupervised	O
representation	O
learning	O
algorithm	O
.	O
however	O
it	O
is	O
also	O
called	O
pretraining	O
,	O
because	O
it	O
is	O
supposed	O
to	O
be	O
only	O
a	O
ﬁrst	O
step	O
before	O
a	O
joint	O
training	O
algorithm	O
is	O
applied	O
to	O
ﬁne-tune	O
all	O
the	O
layers	O
together	O
.	O
in	O
the	O
context	O
of	O
a	O
supervised	O
learning	O
task	O
,	O
it	O
can	O
be	O
viewed	O
as	O
a	O
regularizer	O
(	O
in	O
some	O
experiments	O
,	O
pretraining	O
decreases	O
test	O
error	O
without	O
decreasing	O
training	O
error	O
)	O
and	O
a	O
form	O
of	O
parameter	O
initialization	O
.	O
it	O
is	O
common	O
to	O
use	O
the	O
word	O
“	O
pretraining	O
”	O
to	O
refer	O
not	O
only	O
to	O
the	O
pretraining	O
stage	O
itself	O
but	O
to	O
the	O
entire	O
two	O
phase	O
protocol	O
that	O
combines	O
the	O
pretraining	O
phase	O
and	O
a	O
supervised	O
learning	O
phase	O
.	O
the	O
supervised	O
learning	O
phase	O
may	O
involve	O
training	O
a	O
simple	O
classiﬁer	O
on	O
top	O
of	O
the	O
features	O
learned	O
in	O
the	O
pretraining	O
phase	O
,	O
or	O
it	O
may	O
involve	O
supervised	O
ﬁne-tuning	B
of	O
the	O
entire	O
network	O
learned	O
in	O
the	O
pretraining	O
phase	O
.	O
no	O
matter	O
what	O
kind	O
of	O
unsupervised	O
learning	O
algorithm	O
or	O
what	O
model	B
type	O
is	O
employed	O
,	O
in	O
the	O
vast	O
majority	O
of	O
cases	O
,	O
the	O
overall	O
training	O
scheme	O
is	O
nearly	O
the	O
same	O
.	O
while	O
the	O
choice	O
of	O
unsupervised	O
learning	O
algorithm	O
will	O
obviously	O
impact	O
the	O
details	O
,	O
most	O
applications	O
of	O
unsupervised	O
pretraining	O
follow	O
this	O
basic	O
protocol	O
.	O
greedy	O
layer-wise	O
unsupervised	O
pretraining	O
can	O
also	O
be	O
used	O
as	O
initialization	O
for	O
other	O
unsupervised	O
learning	O
algorithms	O
,	O
such	O
as	O
deep	O
autoencoders	O
(	O
hinton	O
)	O
and	O
probabilistic	O
models	O
with	O
many	O
layers	O
of	O
latent	O
and	O
salakhutdinov	O
2006	O
variables	O
.	O
such	O
models	O
include	O
deep	O
belief	O
networks	O
(	O
)	O
and	O
deep	O
boltzmann	O
machines	O
(	O
salakhutdinov	O
and	O
hinton	O
2009a	O
)	O
.	O
these	O
deep	O
generative	O
models	O
will	O
be	O
described	O
in	O
chapter	O
hinton	O
et	O
al	O
.	O
2006	O
.20	O
,	O
,	O
,	O
as	O
discussed	O
in	O
section	O
,	O
it	O
is	O
also	O
possible	O
to	O
have	O
greedy	O
layer-wise	O
supervised	O
pretraining	O
.	O
this	O
builds	O
on	O
the	O
premise	O
that	O
training	O
a	O
shallow	O
network	O
is	O
easier	O
than	O
training	O
a	O
deep	O
one	O
,	O
which	O
seems	O
to	O
have	O
been	O
validated	O
in	O
several	O
contexts	O
(	O
erhan	O
et	O
al	O
.	O
2010	O
8.7.4	O
)	O
.	O
,	O
15.1.1	O
when	O
and	O
why	O
does	O
unsupervised	O
pretraining	O
work	B
?	O
on	O
many	O
tasks	O
,	O
greedy	O
layer-wise	O
unsupervised	O
pretraining	O
can	O
yield	O
substantial	O
improvements	O
in	O
test	O
error	O
for	O
classiﬁcation	O
tasks	O
.	O
this	O
observation	O
was	O
responsible	O
for	O
the	O
renewed	O
interested	O
in	O
deep	O
neural	O
networks	O
starting	O
in	O
2006	O
(	O
hinton	O
et	O
al.	O
,	O
529	O
chapter	O
15.	O
representation	O
learning	O
l	O
algorithm	O
15.1	O
greedy	O
layer-wise	O
unsupervised	O
pretraining	O
protocol	O
.	O
given	O
the	O
following	O
:	O
unsupervised	O
feature	O
learning	O
algorithm	O
,	O
which	O
takes	O
a	O
training	O
set	O
of	O
examples	O
and	O
returns	O
an	O
encoder	O
or	O
feature	O
function	O
f.	O
the	O
raw	O
input	O
data	O
is	O
x	O
,	O
with	O
one	O
row	O
per	O
example	O
and	O
f	O
(	O
1	O
)	O
(	O
x	O
)	O
is	O
the	O
output	O
of	O
the	O
ﬁrst	O
t	O
stage	O
encoder	O
on	O
x.	O
in	O
the	O
case	O
where	O
ﬁne-tuning	B
is	O
performed	O
,	O
we	O
use	O
a	O
learner	O
which	O
takes	O
an	O
initial	O
function	O
f	O
,	O
input	O
examples	O
x	O
(	O
and	O
in	O
the	O
supervised	O
ﬁne-tuning	B
case	O
,	O
associated	O
targets	O
y	O
)	O
,	O
and	O
returns	O
a	O
tuned	O
function	O
.	O
the	O
number	O
of	O
stages	O
is	O
.m	O
identity	O
function	O
←	O
f	O
˜x	O
x=	O
for	O
do	O
,	O
.	O
.	O
.	O
,	O
m	O
l	O
˜x	O
)	O
k	O
=	O
1	O
◦	O
←	O
f	O
(	O
)	O
k	O
=	O
(	O
←	O
(	O
)	O
k	O
f	O
f	O
˜x	O
f	O
(	O
)	O
k	O
(	O
˜x	O
)	O
f	O
end	O
for	O
←	O
t	O
if	O
ﬁne-tuning	B
then	O
f	O
,	O
(	O
x	O
y	O
)	O
f	O
,	O
end	O
if	O
return	O
f	O
;	O
;	O
(	O
et	O
al.	O
,	O
et	O
al.	O
,	O
2007a	O
2007	O
ranzato	O
ma	O
et	O
al	O
.	O
2015	O
2006	O
bengio	O
)	O
.	O
on	O
many	O
other	O
tasks	O
,	O
however	O
,	O
unsupervised	O
pretraining	O
either	O
does	O
not	O
confer	O
a	O
beneﬁt	O
or	O
even	O
causes	O
noticeable	O
harm.	O
)	O
studied	O
the	O
eﬀect	O
of	O
pretraining	O
on	O
machine	O
learning	O
models	O
for	O
chemical	O
activity	O
prediction	O
and	O
found	O
that	O
,	O
on	O
average	O
,	O
pretraining	O
was	O
slightly	O
harmful	O
,	O
but	O
for	O
many	O
tasks	O
was	O
signiﬁcantly	O
helpful	O
.	O
because	O
unsupervised	O
pretraining	O
is	O
sometimes	O
helpful	O
but	O
often	O
harmful	O
it	O
is	O
important	O
to	O
understand	O
when	O
and	O
why	O
it	O
works	O
in	O
order	O
to	O
determine	O
whether	O
it	O
is	O
applicable	O
to	O
a	O
particular	O
task	O
.	O
at	O
the	O
outset	O
,	O
it	O
is	O
important	O
to	O
clarify	O
that	O
most	O
of	O
this	O
discussion	O
is	O
restricted	O
to	O
greedy	O
unsupervised	O
pretraining	O
in	O
particular	O
.	O
there	O
are	O
other	O
,	O
completely	O
diﬀerent	O
paradigms	O
for	O
performing	O
semi-supervised	O
learning	O
with	O
neural	O
networks	O
,	O
such	O
as	O
virtual	O
adversarial	O
training	O
described	O
in	O
section	O
.	O
it	O
is	O
also	O
possible	O
to	O
train	O
an	O
autoencoder	O
or	O
generative	O
model	B
at	O
the	O
same	O
time	O
as	O
the	O
supervised	O
model	B
.	O
examples	O
of	O
this	O
single-stage	O
approach	O
include	O
the	O
discriminative	O
rbm	O
(	O
larochelle	O
and	O
bengio	O
2008	O
)	O
,	O
in	O
which	O
the	O
total	O
objective	O
is	O
an	O
explicit	O
sum	O
of	O
the	O
two	O
terms	O
(	O
one	O
using	O
the	O
labels	O
and	O
one	O
only	O
using	O
the	O
input	O
)	O
.	O
)	O
and	O
the	O
ladder	O
network	O
(	O
rasmus	O
et	O
al	O
.	O
2015	O
7.13	O
,	O
,	O
unsupervised	O
pretraining	O
combines	O
two	O
diﬀerent	O
ideas	O
.	O
first	O
,	O
it	O
makes	O
use	O
of	O
530	O
chapter	O
15.	O
representation	O
learning	O
the	O
idea	O
that	O
the	O
choice	O
of	O
initial	O
parameters	O
for	O
a	O
deep	O
neural	O
network	O
can	O
have	O
a	O
signiﬁcant	O
regularizing	O
eﬀect	O
on	O
the	O
model	B
(	O
and	O
,	O
to	O
a	O
lesser	O
extent	O
,	O
that	O
it	O
can	O
improve	O
optimization	O
)	O
.	O
second	O
,	O
it	O
makes	O
use	O
of	O
the	O
more	O
general	O
idea	O
that	O
learning	O
about	O
the	O
input	O
distribution	O
can	O
help	O
to	O
learn	O
about	O
the	O
mapping	O
from	O
inputs	O
to	O
outputs	O
.	O
both	O
of	O
these	O
ideas	O
involve	O
many	O
complicated	O
interactions	O
between	O
several	O
parts	O
of	O
the	O
machine	O
learning	O
algorithm	O
that	O
are	O
not	O
entirely	O
understood	O
.	O
the	O
ﬁrst	O
idea	O
,	O
that	O
the	O
choice	O
of	O
initial	O
parameters	O
for	O
a	O
deep	O
neural	O
network	O
can	O
have	O
a	O
strong	O
regularizing	O
eﬀect	O
on	O
its	O
performance	O
,	O
is	O
the	O
least	O
well	O
understood	O
.	O
at	O
the	O
time	O
that	O
pretraining	O
became	O
popular	O
,	O
it	O
was	O
understood	O
as	O
initializing	O
the	O
model	B
in	O
a	O
location	O
that	O
would	O
cause	O
it	O
to	O
approach	O
one	O
local	O
minimum	O
rather	O
than	O
another	O
.	O
today	O
,	O
local	O
minima	O
are	O
no	O
longer	O
considered	O
to	O
be	O
a	O
serious	O
problem	O
for	O
neural	O
network	O
optimization	O
.	O
we	O
now	O
know	O
that	O
our	O
standard	O
neural	O
network	O
training	O
procedures	O
usually	O
do	O
not	O
arrive	O
at	O
a	O
critical	O
point	O
of	O
any	O
kind	O
.	O
it	O
remains	O
possible	O
that	O
pretraining	O
initializes	O
the	O
model	B
in	O
a	O
location	O
that	O
would	O
otherwise	O
be	O
inaccessible—for	O
example	O
,	O
a	O
region	O
that	O
is	O
surrounded	O
by	O
areas	O
where	O
the	O
cost	O
function	O
varies	O
so	O
much	O
from	O
one	O
example	O
to	O
another	O
that	O
minibatches	O
give	O
only	O
a	O
very	O
noisy	O
estimate	O
of	O
the	O
gradient	O
,	O
or	O
a	O
region	O
surrounded	O
by	O
areas	O
where	O
the	O
hessian	O
matrix	O
is	O
so	O
poorly	O
conditioned	O
that	O
gradient	O
descent	B
methods	O
must	O
use	O
very	O
small	O
steps	O
.	O
however	O
,	O
our	O
ability	O
to	O
characterize	O
exactly	O
what	O
aspects	O
of	O
the	O
pretrained	O
parameters	O
are	O
retained	O
during	O
the	O
supervised	O
training	O
stage	O
is	O
limited	O
.	O
this	O
is	O
one	O
reason	O
that	O
modern	O
approaches	O
typically	O
use	O
simultaneous	O
unsupervised	O
learning	O
and	O
supervised	O
learning	O
rather	O
than	O
two	O
sequential	O
stages	O
.	O
one	O
may	O
also	O
avoid	O
struggling	O
with	O
these	O
complicated	O
ideas	O
about	O
how	O
optimization	O
in	O
the	O
supervised	O
learning	O
stage	O
preserves	O
information	O
from	O
the	O
unsupervised	O
learning	O
stage	O
by	O
simply	O
freezing	O
the	O
parameters	O
for	O
the	O
feature	O
extractors	O
and	O
using	O
supervised	O
learning	O
only	O
to	O
add	O
a	O
classiﬁer	O
on	O
top	O
of	O
the	O
learned	O
features	O
.	O
the	O
other	O
idea	O
,	O
that	O
a	O
learning	O
algorithm	O
can	O
use	O
information	O
learned	O
in	O
the	O
unsupervised	O
phase	O
to	O
perform	O
better	O
in	O
the	O
supervised	O
learning	O
stage	O
,	O
is	O
better	O
understood	O
.	O
the	O
basic	O
idea	O
is	O
that	O
some	O
features	O
that	O
are	O
useful	O
for	O
the	O
unsupervised	O
task	O
may	O
also	O
be	O
useful	O
for	O
the	O
supervised	O
learning	O
task	O
.	O
for	O
example	O
,	O
if	O
we	O
train	O
a	O
generative	O
model	B
of	O
images	O
of	O
cars	O
and	O
motorcycles	O
,	O
it	O
will	O
need	O
to	O
know	O
about	O
wheels	O
,	O
and	O
about	O
how	O
many	O
wheels	O
should	O
be	O
in	O
an	O
image	O
.	O
if	O
we	O
are	O
fortunate	O
,	O
the	O
representation	O
of	O
the	O
wheels	O
will	O
take	O
on	O
a	O
form	O
that	O
is	O
easy	O
for	O
the	O
supervised	O
learner	O
to	O
access	O
.	O
this	O
is	O
not	O
yet	O
understood	O
at	O
a	O
mathematical	O
,	O
theoretical	O
level	O
,	O
so	O
it	O
is	O
not	O
always	O
possible	O
to	O
predict	O
which	O
tasks	O
will	O
beneﬁt	O
from	O
unsupervised	O
learning	O
in	O
this	O
way	O
.	O
many	O
aspects	O
of	O
this	O
approach	O
are	O
highly	O
dependent	O
on	O
the	O
speciﬁc	O
models	O
used	O
.	O
for	O
example	O
,	O
if	O
we	O
wish	O
to	O
add	O
a	O
linear	O
classiﬁer	O
on	O
531	O
chapter	O
15.	O
representation	O
learning	O
top	O
of	O
pretrained	O
features	O
,	O
the	O
features	O
must	O
make	O
the	O
underlying	O
classes	O
linearly	O
separable	O
.	O
these	O
properties	O
often	O
occur	O
naturally	O
but	O
do	O
not	O
always	O
do	O
so	O
.	O
this	O
is	O
another	O
reason	O
that	O
simultaneous	O
supervised	O
and	O
unsupervised	O
learning	O
can	O
be	O
preferable—the	O
constraints	O
imposed	O
by	O
the	O
output	O
layer	O
are	O
naturally	O
included	O
from	O
the	O
start	O
.	O
from	O
the	O
point	O
of	O
view	O
of	O
unsupervised	O
pretraining	O
as	O
learning	O
a	O
representation	O
,	O
we	O
can	O
expect	O
unsupervised	O
pretraining	O
to	O
be	O
more	O
eﬀective	O
when	O
the	O
initial	O
representation	O
is	O
poor	O
.	O
one	O
key	O
example	O
of	O
this	O
is	O
the	O
use	O
of	O
word	O
embeddings	O
.	O
words	O
represented	O
by	O
one-hot	O
vectors	O
are	O
not	O
very	O
informative	O
because	O
every	O
two	O
distinct	O
one-hot	O
vectors	O
are	O
the	O
same	O
distance	O
from	O
each	O
other	O
(	O
squared	O
l2	O
distance	O
of	O
)	O
.	O
learned	O
word	O
embeddings	O
naturally	O
encode	O
similarity	O
between	O
words	O
by	O
their	O
distance	O
from	O
each	O
other	O
.	O
because	O
of	O
this	O
,	O
unsupervised	O
pretraining	O
is	O
especially	O
useful	O
when	O
processing	O
words	O
.	O
it	O
is	O
less	O
useful	O
when	O
processing	O
images	O
,	O
perhaps	O
because	O
images	O
already	O
lie	O
in	O
a	O
rich	O
vector	O
space	O
where	O
distances	O
provide	O
a	O
low	O
quality	O
similarity	O
metric	O
.	O
2	O
from	O
the	O
point	O
of	O
view	O
of	O
unsupervised	O
pretraining	O
as	O
a	O
regularizer	O
,	O
we	O
can	O
expect	O
unsupervised	O
pretraining	O
to	O
be	O
most	O
helpful	O
when	O
the	O
number	O
of	O
labeled	O
examples	O
is	O
very	O
small	O
.	O
because	O
the	O
source	O
of	O
information	O
added	O
by	O
unsupervised	O
pretraining	O
is	O
the	O
unlabeled	O
data	O
,	O
we	O
may	O
also	O
expect	O
unsupervised	O
pretraining	O
to	O
perform	O
best	O
when	O
the	O
number	O
of	O
unlabeled	O
examples	O
is	O
very	O
large	O
.	O
the	O
advantage	O
of	O
semi-supervised	O
learning	O
via	O
unsupervised	O
pretraining	O
with	O
many	O
unlabeled	O
examples	O
and	O
few	O
labeled	O
examples	O
was	O
made	O
particularly	O
clear	O
in	O
2011	O
with	O
unsupervised	O
pretraining	O
winning	O
two	O
international	O
transfer	O
learning	O
competitions	O
(	O
)	O
,	O
in	O
settings	O
where	O
the	O
number	O
of	O
labeled	O
examples	O
in	O
the	O
target	O
task	O
was	O
small	O
(	O
from	O
a	O
handful	O
to	O
dozens	O
of	O
examples	O
per	O
class	O
)	O
.	O
these	O
eﬀects	O
were	O
also	O
documented	O
in	O
carefully	O
controlled	O
experiments	O
by	O
paine	O
mesnil	O
et	O
al	O
.	O
2011	O
goodfellow	O
et	O
al	O
.	O
2011	O
)	O
.	O
et	O
al	O
.	O
(	O
2014	O
,	O
;	O
,	O
other	O
factors	O
are	O
likely	O
to	O
be	O
involved	O
.	O
for	O
example	O
,	O
unsupervised	O
pretraining	O
is	O
likely	O
to	O
be	O
most	O
useful	O
when	O
the	O
function	O
to	O
be	O
learned	O
is	O
extremely	O
complicated	O
.	O
unsupervised	O
learning	O
diﬀers	O
from	O
regularizers	O
like	O
weight	O
decay	O
because	O
it	O
does	O
not	O
bias	O
the	O
learner	O
toward	O
discovering	O
a	O
simple	O
function	O
but	O
rather	O
toward	O
discovering	O
feature	O
functions	O
that	O
are	O
useful	O
for	O
the	O
unsupervised	O
learning	O
task	O
.	O
if	O
the	O
true	O
underlying	O
functions	O
are	O
complicated	O
and	O
shaped	O
by	O
regularities	O
of	O
the	O
input	O
distribution	O
,	O
unsupervised	O
learning	O
can	O
be	O
a	O
more	O
appropriate	O
regularizer	O
.	O
these	O
caveats	O
aside	O
,	O
we	O
now	O
analyze	O
some	O
success	O
cases	O
where	O
unsupervised	O
pretraining	O
is	O
known	O
to	O
cause	O
an	O
improvement	O
,	O
and	O
explain	O
what	O
is	O
known	O
about	O
why	O
this	O
improvement	O
occurs	O
.	O
unsupervised	O
pretraining	O
has	O
usually	O
been	O
used	O
to	O
improve	O
classiﬁers	O
,	O
and	O
is	O
usually	O
most	O
interesting	O
from	O
the	O
point	O
of	O
view	O
of	O
532	O
chapter	O
15.	O
representation	O
learning	O
	O
	O
	O
	O
	O
	O
 	O
 	O
 	O
 	O
 	O
 	O
 	O
	O
	O
	O
	O
	O
(	O
erhan	O
et	O
al	O
.	O
2010	O
(	O
erhan	O
et	O
al	O
.	O
2010	O
figure	O
15.1	O
:	O
visualization	O
via	O
nonlinear	O
projection	O
of	O
the	O
learning	O
trajectories	O
of	O
diﬀerent	O
neural	O
networks	O
in	O
function	O
space	O
(	O
not	O
parameter	O
space	O
,	O
to	O
avoid	O
the	O
issue	O
of	O
many-to-one	O
mappings	O
from	O
parameter	O
vectors	O
to	O
functions	O
)	O
,	O
with	O
diﬀerent	O
random	O
initializations	O
and	O
with	O
or	O
without	O
unsupervised	O
pretraining	O
.	O
each	O
point	O
corresponds	O
to	O
a	O
diﬀerent	O
neural	O
network	O
,	O
at	O
a	O
particular	O
time	O
during	O
its	O
training	O
process	O
.	O
this	O
ﬁgure	O
is	O
adapted	O
)	O
.	O
a	O
coordinate	O
in	O
function	O
space	O
is	O
an	O
inﬁnite-	O
with	O
permission	O
from	O
dimensional	O
vector	O
associating	O
every	O
input	O
x	O
with	O
an	O
output	O
y.	O
)	O
made	O
a	O
linear	O
projection	O
to	O
high-dimensional	O
space	O
by	O
concatenating	O
the	O
y	O
for	O
many	O
speciﬁc	O
x	O
points	O
.	O
they	O
then	O
made	O
a	O
further	O
nonlinear	O
projection	O
to	O
2-d	O
by	O
isomap	O
(	O
tenenbaum	O
et	O
al.	O
,	O
)	O
.	O
color	O
indicates	O
time	O
.	O
all	O
networks	O
are	O
initialized	O
near	O
the	O
center	O
of	O
the	O
plot	O
(	O
corresponding	O
to	O
the	O
region	O
of	O
functions	O
that	O
produce	O
approximately	O
uniform	O
distributions	O
over	O
the	O
class	O
y	O
for	O
most	O
inputs	O
)	O
.	O
over	O
time	O
,	O
learning	O
moves	O
the	O
function	O
outward	O
,	O
to	O
points	O
that	O
make	O
strong	O
predictions	O
.	O
training	O
consistently	O
terminates	O
in	O
one	O
region	O
when	O
using	O
pretraining	O
and	O
in	O
another	O
,	O
non-overlapping	O
region	O
when	O
not	O
using	O
pretraining	O
.	O
isomap	O
tries	O
to	O
preserve	O
global	O
relative	O
distances	O
(	O
and	O
hence	O
volumes	O
)	O
so	O
the	O
small	O
region	O
corresponding	O
to	O
pretrained	O
models	O
may	O
indicate	O
that	O
the	O
pretraining-based	O
estimator	O
has	O
reduced	O
variance	O
.	O
2000	O
533	O
chapter	O
15.	O
representation	O
learning	O
reducing	O
test	O
set	O
error	O
.	O
however	O
,	O
unsupervised	O
pretraining	O
can	O
help	O
tasks	O
other	O
than	O
classiﬁcation	O
,	O
and	O
can	O
act	O
to	O
improve	O
optimization	O
rather	O
than	O
being	O
merely	O
a	O
regularizer	O
.	O
for	O
example	O
,	O
it	O
can	O
improve	O
both	O
train	O
and	O
test	O
reconstruction	O
error	O
for	O
deep	O
autoencoders	O
(	O
hinton	O
and	O
salakhutdinov	O
2006	O
)	O
.	O
,	O
2010	O
erhan	O
et	O
al	O
.	O
(	O
)	O
performed	O
many	O
experiments	O
to	O
explain	O
several	O
successes	O
of	O
unsupervised	O
pretraining	O
.	O
both	O
improvements	O
to	O
training	O
error	O
and	O
improvements	O
to	O
test	O
error	O
may	O
be	O
explained	O
in	O
terms	O
of	O
unsupervised	O
pretraining	O
taking	O
the	O
parameters	O
into	O
a	O
region	O
that	O
would	O
otherwise	O
be	O
inaccessible	O
.	O
neural	O
network	O
training	O
is	O
non-deterministic	O
,	O
and	O
converges	O
to	O
a	O
diﬀerent	O
function	O
every	O
time	O
it	O
is	O
run	O
.	O
training	O
may	O
halt	O
at	O
a	O
point	O
where	O
the	O
gradient	O
becomes	O
small	O
,	O
a	O
point	O
where	O
early	O
stopping	O
ends	O
training	O
to	O
prevent	O
overﬁtting	O
,	O
or	O
at	O
a	O
point	O
where	O
the	O
gradient	O
is	O
large	O
but	O
it	O
is	O
diﬃcult	O
to	O
ﬁnd	O
a	O
downhill	O
step	O
due	O
to	O
problems	O
such	O
as	O
stochasticity	O
or	O
poor	O
conditioning	O
of	O
the	O
hessian	O
.	O
neural	O
networks	O
that	O
receive	O
unsupervised	O
pretraining	O
consistently	O
halt	O
in	O
the	O
same	O
region	O
of	O
function	O
space	O
,	O
while	O
neural	O
networks	O
without	O
pretraining	O
consistently	O
halt	O
in	O
another	O
region	O
.	O
see	O
ﬁgure	O
for	O
a	O
visualization	O
of	O
this	O
phenomenon	O
.	O
the	O
region	O
where	O
pretrained	O
networks	O
arrive	O
is	O
smaller	O
,	O
suggesting	O
that	O
pretraining	O
reduces	O
the	O
variance	O
of	O
the	O
estimation	O
process	O
,	O
which	O
can	O
in	O
turn	O
reduce	O
the	O
risk	O
of	O
severe	O
over-ﬁtting	O
.	O
in	O
other	O
words	O
,	O
unsupervised	O
pretraining	O
initializes	O
neural	O
network	O
parameters	O
into	O
a	O
region	O
that	O
they	O
do	O
not	O
escape	O
,	O
and	O
the	O
results	O
following	O
this	O
initialization	O
are	O
more	O
consistent	O
and	O
less	O
likely	O
to	O
be	O
very	O
bad	O
than	O
without	O
this	O
initialization	O
.	O
15.1	O
2010	O
erhan	O
et	O
al	O
.	O
(	O
)	O
also	O
provide	O
some	O
answers	O
as	O
to	O
pretraining	O
works	O
best—the	O
mean	O
and	O
variance	O
of	O
the	O
test	O
error	O
were	O
most	O
reduced	O
by	O
pretraining	O
for	O
deeper	O
networks	O
.	O
keep	O
in	O
mind	O
that	O
these	O
experiments	O
were	O
performed	O
before	O
the	O
invention	O
and	O
popularization	O
of	O
modern	O
techniques	O
for	O
training	O
very	O
deep	O
networks	O
(	O
rectiﬁed	O
linear	O
units	O
,	O
dropout	O
and	O
batch	O
normalization	O
)	O
so	O
less	O
is	O
known	O
about	O
the	O
eﬀect	O
of	O
unsupervised	O
pretraining	O
in	O
conjunction	O
with	O
contemporary	O
approaches	O
.	O
when	O
an	O
important	O
question	O
is	O
how	O
unsupervised	O
pretraining	O
can	O
act	O
as	O
a	O
regularizer	O
.	O
one	O
hypothesis	O
is	O
that	O
pretraining	O
encourages	O
the	O
learning	O
algorithm	O
to	O
discover	O
features	O
that	O
relate	O
to	O
the	O
underlying	O
causes	O
that	O
generate	O
the	O
observed	O
data	O
.	O
this	O
is	O
an	O
important	O
idea	O
motivating	O
many	O
other	O
algorithms	O
besides	O
unsupervised	O
pretraining	O
,	O
and	O
is	O
described	O
further	O
in	O
section	O
15.3	O
.	O
compared	O
to	O
other	O
forms	O
of	O
unsupervised	O
learning	O
,	O
unsupervised	O
pretraining	O
has	O
the	O
disadvantage	O
that	O
it	O
operates	O
with	O
two	O
separate	O
training	O
phases	O
.	O
many	O
regularization	O
strategies	O
have	O
the	O
advantage	O
of	O
allowing	O
the	O
user	O
to	O
control	O
the	O
strength	O
of	O
the	O
regularization	O
by	O
adjusting	O
the	O
value	O
of	O
a	O
single	O
hyperparameter	O
.	O
unsupervised	O
pretraining	O
does	O
not	O
oﬀer	O
a	O
clear	O
way	O
to	O
adjust	O
the	O
the	O
strength	O
of	O
the	O
regularization	O
arising	O
from	O
the	O
unsupervised	O
stage	O
.	O
instead	O
,	O
there	O
are	O
534	O
chapter	O
15.	O
representation	O
learning	O
very	O
many	O
hyperparameters	O
,	O
whose	O
eﬀect	O
may	O
be	O
measured	O
after	O
the	O
fact	O
but	O
is	O
often	O
diﬃcult	O
to	O
predict	O
ahead	O
of	O
time	O
.	O
when	O
we	O
perform	O
unsupervised	O
and	O
supervised	O
learning	O
simultaneously	O
,	O
instead	O
of	O
using	O
the	O
pretraining	O
strategy	O
,	O
there	O
is	O
a	O
single	O
hyperparameter	O
,	O
usually	O
a	O
coeﬃcient	O
attached	O
to	O
the	O
unsupervised	O
cost	O
,	O
that	O
determines	O
how	O
strongly	O
the	O
unsupervised	O
objective	O
will	O
regularize	O
the	O
supervised	O
model	B
.	O
one	O
can	O
always	O
predictably	O
obtain	O
less	O
regularization	O
by	O
decreasing	O
this	O
coeﬃcient	O
.	O
in	O
the	O
case	O
of	O
unsupervised	O
pretraining	O
,	O
there	O
is	O
not	O
a	O
way	O
of	O
ﬂexibly	O
adapting	O
the	O
strength	O
of	O
the	O
regularization—either	O
the	O
supervised	O
model	B
is	O
initialized	O
to	O
pretrained	O
parameters	O
,	O
or	O
it	O
is	O
not	O
.	O
another	O
disadvantage	O
of	O
having	O
two	O
separate	O
training	O
phases	O
is	O
that	O
each	O
phase	O
has	O
its	O
own	O
hyperparameters	O
.	O
the	O
performance	O
of	O
the	O
second	O
phase	O
usually	O
can	O
not	O
be	O
predicted	O
during	O
the	O
ﬁrst	O
phase	O
,	O
so	O
there	O
is	O
a	O
long	O
delay	O
between	O
proposing	O
hyperparameters	O
for	O
the	O
ﬁrst	O
phase	O
and	O
being	O
able	O
to	O
update	O
them	O
using	O
feedback	O
from	O
the	O
second	O
phase	O
.	O
the	O
most	O
principled	O
approach	O
is	O
to	O
use	O
validation	O
set	O
error	O
in	O
the	O
supervised	O
phase	O
in	O
order	O
to	O
select	O
the	O
hyperparameters	O
of	O
the	O
pretraining	O
phase	O
,	O
as	O
discussed	O
in	O
)	O
.	O
in	O
practice	O
,	O
some	O
hyperparameters	O
,	O
like	O
the	O
number	O
of	O
pretraining	O
iterations	O
,	O
are	O
more	O
conveniently	O
set	O
during	O
the	O
pretraining	O
phase	O
,	O
using	O
early	O
stopping	O
on	O
the	O
unsupervised	O
objective	O
,	O
which	O
is	O
not	O
ideal	O
but	O
computationally	O
much	O
cheaper	O
than	O
using	O
the	O
supervised	O
objective	O
.	O
larochelle	O
et	O
al	O
.	O
2009	O
(	O
today	O
,	O
unsupervised	O
pretraining	O
has	O
been	O
largely	O
abandoned	O
,	O
except	O
in	O
the	O
ﬁeld	O
of	O
natural	O
language	O
processing	O
,	O
where	O
the	O
natural	O
representation	O
of	O
words	O
as	O
one-hot	O
vectors	O
conveys	O
no	O
similarity	O
information	O
and	O
where	O
very	O
large	O
unlabeled	O
sets	O
are	O
available	O
.	O
in	O
that	O
case	O
,	O
the	O
advantage	O
of	O
pretraining	O
is	O
that	O
one	O
can	O
pretrain	O
once	O
on	O
a	O
huge	O
unlabeled	O
set	O
(	O
for	O
example	O
with	O
a	O
corpus	O
containing	O
billions	O
of	O
words	O
)	O
,	O
learn	O
a	O
good	O
representation	O
(	O
typically	O
of	O
words	O
,	O
but	O
also	O
of	O
sentences	O
)	O
,	O
and	O
then	O
use	O
this	O
representation	O
or	O
ﬁne-tune	O
it	O
for	O
a	O
supervised	O
task	O
for	O
which	O
the	O
training	O
set	O
contains	O
substantially	O
fewer	O
examples	O
.	O
this	O
approach	O
was	O
pioneered	O
by	O
by	O
collobert	O
and	O
weston	O
2008b	O
turian	O
et	O
al	O
.	O
(	O
2011a	O
)	O
and	O
remains	O
in	O
common	O
use	O
today	O
.	O
collobert	O
)	O
,	O
and	O
et	O
al	O
.	O
(	O
2010	O
(	O
)	O
,	O
deep	O
learning	O
techniques	O
based	O
on	O
supervised	O
learning	O
,	O
regularized	O
with	O
dropout	O
or	O
batch	O
normalization	O
,	O
are	O
able	O
to	O
achieve	O
human-level	O
performance	O
on	O
very	O
many	O
tasks	O
,	O
but	O
only	O
with	O
extremely	O
large	O
labeled	O
datasets	O
.	O
these	O
same	O
techniques	O
out-	O
perform	O
unsupervised	O
pretraining	O
on	O
medium-sized	O
datasets	O
such	O
as	O
cifar-10	O
and	O
mnist	O
,	O
which	O
have	O
roughly	O
5,000	O
labeled	O
examples	O
per	O
class	O
.	O
on	O
extremely	O
small	O
datasets	O
,	O
such	O
as	O
the	O
alternative	O
splicing	O
dataset	O
,	O
bayesian	O
methods	O
outperform	O
methods	O
based	O
on	O
unsupervised	O
pretraining	O
(	O
srivastava	O
2013	O
)	O
.	O
for	O
these	O
reasons	O
,	O
the	O
popularity	O
of	O
unsupervised	O
pretraining	O
has	O
declined	O
.	O
nevertheless	O
,	O
unsupervised	O
pretraining	O
remains	O
an	O
important	O
milestone	O
in	O
the	O
history	O
of	O
deep	O
learning	O
research	O
,	O
535	O
chapter	O
15.	O
representation	O
learning	O
and	O
continues	O
to	O
inﬂuence	O
contemporary	O
approaches	O
.	O
the	O
idea	O
of	O
pretraining	O
has	O
been	O
generalized	O
to	O
supervised	O
pretraining	O
discussed	O
in	O
section	O
,	O
as	O
a	O
very	O
common	O
approach	O
for	O
transfer	O
learning	O
.	O
supervised	O
pretraining	O
for	O
transfer	O
learning	O
is	O
popular	O
(	O
)	O
for	O
use	O
with	O
convolutional	O
networks	O
pretrained	O
on	O
the	O
imagenet	O
dataset	O
.	O
practitioners	O
publish	O
the	O
parameters	O
of	O
these	O
trained	O
networks	O
for	O
this	O
purpose	O
,	O
just	O
like	O
pretrained	O
word	O
vectors	O
are	O
published	O
for	O
natural	O
language	O
tasks	O
(	O
2013a	O
)	O
.	O
collobert	O
et	O
al	O
.	O
2011a	O
mikolov	O
oquab	O
et	O
al	O
.	O
2014	O
yosinski	O
et	O
al.	O
,	O
et	O
al.	O
,	O
2014	O
,	O
;	O
8.7.4	O
,	O
;	O
15.2	O
transfer	O
learning	O
and	O
domain	O
adaptation	O
transfer	O
learning	O
and	O
domain	O
adaptation	O
refer	O
to	O
the	O
situation	O
where	O
what	O
has	O
been	O
learned	O
in	O
one	O
setting	O
(	O
i.e.	O
,	O
distribution	O
p1	O
)	O
is	O
exploited	O
to	O
improve	O
generalization	O
in	O
another	O
setting	O
(	O
say	O
distribution	O
p	O
2	O
)	O
.	O
this	O
generalizes	O
the	O
idea	O
presented	O
in	O
the	O
previous	O
section	O
,	O
where	O
we	O
transferred	O
representations	O
between	O
an	O
unsupervised	O
learning	O
task	O
and	O
a	O
supervised	O
learning	O
task	O
.	O
in	O
transfer	O
learning	O
,	O
the	O
learner	O
must	O
perform	O
two	O
or	O
more	O
diﬀerent	O
tasks	O
,	O
but	O
we	O
assume	O
that	O
many	O
of	O
the	O
factors	O
that	O
explain	O
the	O
variations	O
in	O
p1	O
are	O
relevant	O
to	O
the	O
variations	O
that	O
need	O
to	O
be	O
captured	O
for	O
learning	O
p2	O
.	O
this	O
is	O
typically	O
understood	O
in	O
a	O
supervised	O
learning	O
context	O
,	O
where	O
the	O
input	O
is	O
the	O
same	O
but	O
the	O
target	O
may	O
be	O
of	O
a	O
diﬀerent	O
nature	O
.	O
for	O
example	O
,	O
we	O
may	O
learn	O
about	O
one	O
set	O
of	O
visual	O
categories	O
,	O
such	O
as	O
cats	O
and	O
dogs	O
,	O
in	O
the	O
ﬁrst	O
setting	O
,	O
then	O
learn	O
about	O
a	O
diﬀerent	O
set	O
of	O
visual	O
categories	O
,	O
such	O
as	O
ants	O
and	O
wasps	O
,	O
in	O
the	O
second	O
setting	O
.	O
if	O
there	O
is	O
signiﬁcantly	O
more	O
data	O
in	O
the	O
ﬁrst	O
setting	O
(	O
sampled	O
from	O
p	O
1	O
)	O
,	O
then	O
that	O
may	O
help	O
to	O
learn	O
representations	O
that	O
are	O
useful	O
to	O
quickly	O
generalize	O
from	O
only	O
very	O
few	O
examples	O
drawn	O
from	O
p2	O
.	O
many	O
visual	O
categories	O
share	O
low-level	O
notions	O
of	O
edges	O
and	O
visual	O
shapes	O
,	O
the	O
eﬀects	O
of	O
geometric	O
changes	O
,	O
changes	O
in	O
lighting	O
,	O
etc	O
.	O
in	O
general	O
,	O
transfer	O
learning	O
,	O
multi-task	O
learning	O
(	O
section	O
)	O
,	O
and	O
domain	O
adaptation	O
can	O
be	O
achieved	O
via	O
representation	O
learning	O
when	O
there	O
exist	O
features	O
that	O
are	O
useful	O
for	O
the	O
diﬀerent	O
settings	O
or	O
tasks	O
,	O
corresponding	O
to	O
underlying	O
factors	O
that	O
appear	O
in	O
more	O
than	O
one	O
setting	O
.	O
this	O
is	O
illustrated	O
in	O
ﬁgure	O
,	O
with	O
shared	O
lower	O
layers	O
and	O
task-dependent	O
upper	O
layers	O
.	O
7.7	O
7.2	O
however	O
,	O
sometimes	O
,	O
what	O
is	O
shared	O
among	O
the	O
diﬀerent	O
tasks	O
is	O
not	O
the	O
semantics	O
of	O
the	O
input	O
but	O
the	O
semantics	O
of	O
the	O
output	O
.	O
for	O
example	O
,	O
a	O
speech	O
recognition	B
system	O
needs	O
to	O
produce	O
valid	O
sentences	O
at	O
the	O
output	O
layer	O
,	O
but	O
the	O
earlier	O
layers	O
near	O
the	O
input	O
may	O
need	O
to	O
recognize	O
very	O
diﬀerent	O
versions	O
of	O
the	O
same	O
phonemes	O
or	O
sub-phonemic	O
vocalizations	O
depending	O
on	O
which	O
person	O
is	O
speaking	O
.	O
in	O
cases	O
like	O
these	O
,	O
it	O
makes	O
more	O
sense	O
to	O
share	O
the	O
upper	O
layers	O
(	O
near	O
the	O
output	O
)	O
of	O
the	O
neural	O
network	O
,	O
and	O
have	O
a	O
task-speciﬁc	O
preprocessing	O
,	O
as	O
536	O
chapter	O
15.	O
representation	O
learning	O
illustrated	O
in	O
ﬁgure	O
15.2	O
.	O
yy	O
h	O
(	O
shared	O
)	O
h	O
(	O
shared	O
)	O
selection	O
switch	O
h	O
(	O
1	O
)	O
h	O
(	O
1	O
)	O
h	O
(	O
2	O
)	O
h	O
(	O
2	O
)	O
h	O
(	O
3	O
)	O
h	O
(	O
3	O
)	O
x	O
(	O
1	O
)	O
x	O
(	O
1	O
)	O
x	O
(	O
2	O
)	O
x	O
(	O
2	O
)	O
x	O
(	O
3	O
)	O
x	O
(	O
3	O
)	O
y	O
has	O
the	O
same	O
semantics	O
for	O
all	O
tasks	O
while	O
the	O
input	O
variable	O
figure	O
15.2	O
:	O
example	O
architecture	O
for	O
multi-task	O
or	O
transfer	O
learning	O
when	O
the	O
output	O
variable	O
has	O
a	O
diﬀerent	O
meaning	O
(	O
and	O
possibly	O
even	O
a	O
diﬀerent	O
dimension	O
)	O
for	O
each	O
task	O
(	O
or	O
,	O
for	O
example	O
,	O
each	O
user	O
)	O
,	O
called	O
x	O
(	O
1	O
)	O
,	O
x	O
(	O
2	O
)	O
and	O
x	O
(	O
3	O
)	O
for	O
three	O
tasks	O
.	O
the	O
lower	O
levels	O
(	O
up	O
to	O
the	O
selection	O
switch	O
)	O
are	O
task-speciﬁc	O
,	O
while	O
the	O
upper	O
levels	O
are	O
shared	O
.	O
the	O
lower	O
levels	O
learn	O
to	O
translate	O
their	O
task-speciﬁc	O
input	O
into	O
a	O
generic	O
set	O
of	O
features	O
.	O
x	O
in	O
the	O
related	O
case	O
of	O
domain	O
adaptation	O
,	O
the	O
task	O
(	O
and	O
the	O
optimal	O
input-to-	O
output	O
mapping	O
)	O
remains	O
the	O
same	O
between	O
each	O
setting	O
,	O
but	O
the	O
input	O
distribution	O
is	O
slightly	O
diﬀerent	O
.	O
for	O
example	O
,	O
consider	O
the	O
task	O
of	O
sentiment	O
analysis	O
,	O
which	O
consists	O
of	O
determining	O
whether	O
a	O
comment	O
expresses	O
positive	O
or	O
negative	O
sentiment	O
.	O
comments	O
posted	O
on	O
the	O
web	O
come	O
from	O
many	O
categories	O
.	O
a	O
domain	O
adaptation	O
scenario	O
can	O
arise	O
when	O
a	O
sentiment	O
predictor	O
trained	O
on	O
customer	O
reviews	O
of	O
media	O
content	O
such	O
as	O
books	O
,	O
videos	O
and	O
music	O
is	O
later	O
used	O
to	O
analyze	O
comments	O
about	O
consumer	O
electronics	O
such	O
as	O
televisions	O
or	O
smartphones	O
.	O
one	O
can	O
imagine	O
that	O
there	O
is	O
an	O
underlying	O
function	O
that	O
tells	O
whether	O
any	O
statement	O
is	O
positive	O
,	O
neutral	O
or	O
negative	O
,	O
but	O
of	O
course	O
the	O
vocabulary	O
and	O
style	O
may	O
vary	O
from	O
one	O
domain	O
to	O
another	O
,	O
making	O
it	O
more	O
diﬃcult	O
to	O
generalize	O
across	O
domains	O
.	O
simple	O
unsupervised	O
pretraining	O
(	O
with	O
denoising	O
autoencoders	O
)	O
has	O
been	O
found	O
to	O
be	O
very	O
successful	O
for	O
sentiment	O
analysis	O
with	O
domain	O
adaptation	O
(	O
glorot	O
et	O
al	O
.	O
2011b	O
)	O
.	O
,	O
a	O
related	O
problem	O
is	O
that	O
of	O
concept	O
drift	O
,	O
which	O
we	O
can	O
view	O
as	O
a	O
form	O
of	O
transfer	O
learning	O
due	O
to	O
gradual	O
changes	O
in	O
the	O
data	O
distribution	O
over	O
time	O
.	O
both	O
concept	O
drift	O
and	O
transfer	O
learning	O
can	O
be	O
viewed	O
as	O
particular	O
forms	O
of	O
537	O
chapter	O
15.	O
representation	O
learning	O
multi-task	O
learning	O
.	O
while	O
the	O
phrase	O
“	O
multi-task	O
learning	O
”	O
typically	O
refers	O
to	O
supervised	O
learning	O
tasks	O
,	O
the	O
more	O
general	O
notion	O
of	O
transfer	O
learning	O
is	O
applicable	O
to	O
unsupervised	O
learning	O
and	O
reinforcement	O
learning	O
as	O
well	O
.	O
in	O
all	O
of	O
these	O
cases	O
,	O
the	O
objective	O
is	O
to	O
take	O
advantage	O
of	O
data	O
from	O
the	O
ﬁrst	O
setting	O
to	O
extract	O
information	O
that	O
may	O
be	O
useful	O
when	O
learning	O
or	O
even	O
when	O
directly	O
making	O
predictions	O
in	O
the	O
second	O
setting	O
.	O
the	O
core	O
idea	O
of	O
representation	O
learning	O
is	O
that	O
the	O
same	O
representation	O
may	O
be	O
useful	O
in	O
both	O
settings	O
.	O
using	O
the	O
same	O
representation	O
in	O
both	O
settings	O
allows	O
the	O
representation	O
to	O
beneﬁt	O
from	O
the	O
training	O
data	O
that	O
is	O
available	O
for	O
both	O
tasks	O
.	O
;	O
,	O
2011	O
as	O
mentioned	O
before	O
,	O
unsupervised	O
deep	O
learning	O
for	O
transfer	O
learning	O
has	O
found	O
mesnil	O
et	O
al	O
.	O
2011	O
goodfellow	O
success	O
in	O
some	O
machine	O
learning	O
competitions	O
(	O
et	O
al.	O
,	O
)	O
.	O
in	O
the	O
ﬁrst	O
of	O
these	O
competitions	O
,	O
the	O
experimental	O
setup	O
is	O
the	O
following	O
.	O
each	O
participant	O
is	O
ﬁrst	O
given	O
a	O
dataset	O
from	O
the	O
ﬁrst	O
setting	O
(	O
from	O
distribution	O
p1	O
)	O
,	O
illustrating	O
examples	O
of	O
some	O
set	O
of	O
categories	O
.	O
the	O
participants	O
must	O
use	O
this	O
to	O
learn	O
a	O
good	O
feature	O
space	O
(	O
mapping	O
the	O
raw	O
input	O
to	O
some	O
representation	O
)	O
,	O
such	O
that	O
when	O
we	O
apply	O
this	O
learned	O
transformation	O
to	O
inputs	O
from	O
the	O
transfer	O
setting	O
(	O
distribution	O
p2	O
)	O
,	O
a	O
linear	O
classiﬁer	O
can	O
be	O
trained	O
and	O
generalize	O
well	O
from	O
very	O
few	O
labeled	O
examples	O
.	O
one	O
of	O
the	O
most	O
striking	O
results	O
found	O
in	O
this	O
competition	O
is	O
that	O
as	O
an	O
architecture	O
makes	O
use	O
of	O
deeper	O
and	O
deeper	O
representations	O
(	O
learned	O
in	O
a	O
purely	O
unsupervised	O
way	O
from	O
data	O
collected	O
in	O
the	O
ﬁrst	O
setting	O
,	O
p	O
1	O
)	O
,	O
the	O
learning	O
curve	O
on	O
the	O
new	O
categories	O
of	O
the	O
second	O
(	O
transfer	O
)	O
setting	O
p	O
2	O
becomes	O
much	O
better	O
.	O
for	O
deep	O
representations	O
,	O
fewer	O
labeled	O
examples	O
of	O
the	O
transfer	O
tasks	O
are	O
necessary	O
to	O
achieve	O
the	O
apparently	O
asymptotic	O
generalization	O
performance	O
.	O
two	O
extreme	O
forms	O
of	O
transfer	O
learning	O
are	O
one-shot	O
learning	O
and	O
zero-shot	O
learning	O
,	O
sometimes	O
also	O
called	O
zero-data	O
learning	O
.	O
only	O
one	O
labeled	O
example	O
of	O
the	O
transfer	O
task	O
is	O
given	O
for	O
one-shot	O
learning	O
,	O
while	O
no	O
labeled	O
examples	O
are	O
given	O
at	O
all	O
for	O
the	O
zero-shot	O
learning	O
task	O
.	O
2006	O
et	O
al.	O
,	O
one-shot	O
learning	O
(	O
fei-fei	O
)	O
is	O
possible	O
because	O
the	O
representation	O
learns	O
to	O
cleanly	O
separate	O
the	O
underlying	O
classes	O
during	O
the	O
ﬁrst	O
stage	O
.	O
during	O
the	O
transfer	O
learning	O
stage	O
,	O
only	O
one	O
labeled	O
example	O
is	O
needed	O
to	O
infer	O
the	O
label	O
of	O
many	O
possible	O
test	O
examples	O
that	O
all	O
cluster	O
around	O
the	O
same	O
point	O
in	O
representation	O
space	O
.	O
this	O
works	O
to	O
the	O
extent	O
that	O
the	O
factors	O
of	O
variation	O
corresponding	O
to	O
these	O
invariances	O
have	O
been	O
cleanly	O
separated	O
from	O
the	O
other	O
factors	O
,	O
in	O
the	O
learned	O
representation	O
space	O
,	O
and	O
we	O
have	O
somehow	O
learned	O
which	O
factors	O
do	O
and	O
do	O
not	O
matter	O
when	O
discriminating	O
objects	O
of	O
certain	O
categories	O
.	O
as	O
an	O
example	O
of	O
a	O
zero-shot	O
learning	O
setting	O
,	O
consider	O
the	O
problem	O
of	O
having	O
a	O
learner	O
read	O
a	O
large	O
collection	O
of	O
text	O
and	O
then	O
solve	O
object	O
recognition	B
problems	O
.	O
538	O
chapter	O
15.	O
representation	O
learning	O
it	O
may	O
be	O
possible	O
to	O
recognize	O
a	O
speciﬁc	O
object	O
class	O
even	O
without	O
having	O
seen	O
an	O
image	O
of	O
that	O
object	O
,	O
if	O
the	O
text	O
describes	O
the	O
object	O
well	O
enough	O
.	O
for	O
example	O
,	O
having	O
read	O
that	O
a	O
cat	O
has	O
four	O
legs	O
and	O
pointy	O
ears	O
,	O
the	O
learner	O
might	O
be	O
able	O
to	O
guess	O
that	O
an	O
image	O
is	O
a	O
cat	O
,	O
without	O
having	O
seen	O
a	O
cat	O
before	O
.	O
;	O
2008	O
et	O
al.	O
,	O
et	O
al.	O
,	O
2013b	O
2009	O
socher	O
)	O
and	O
zero-shot	O
learning	O
(	O
zero-data	O
learning	O
(	O
larochelle	O
palatucci	O
et	O
al.	O
,	O
)	O
are	O
only	O
possible	O
because	O
additional	O
information	O
has	O
been	O
exploited	O
during	O
training	O
.	O
we	O
can	O
think	O
of	O
the	O
zero-data	O
learning	O
scenario	O
as	O
including	O
three	O
random	O
variables	O
:	O
the	O
traditional	O
inputs	O
x	O
,	O
the	O
traditional	O
outputs	O
or	O
targets	O
y	O
,	O
and	O
an	O
additional	O
random	O
variable	O
describing	O
the	O
task	O
,	O
t	O
.	O
the	O
model	B
is	O
trained	O
to	O
estimate	O
the	O
conditional	O
distribution	O
p	O
(	O
y	O
x	O
,	O
t	O
)	O
where	O
t	O
is	O
a	O
description	O
of	O
the	O
task	O
we	O
wish	O
the	O
model	B
to	O
perform	O
.	O
in	O
our	O
example	O
of	O
recognizing	O
cats	O
after	O
having	O
read	O
about	O
cats	O
,	O
the	O
output	O
is	O
a	O
binary	O
variable	O
y	O
with	O
y	O
=	O
1	O
indicating	O
“	O
yes	O
”	O
and	O
y	O
=	O
0	O
indicating	O
“	O
no.	O
”	O
the	O
task	O
variable	O
t	O
then	O
represents	O
questions	O
to	O
be	O
answered	O
such	O
as	O
“	O
is	O
there	O
a	O
cat	O
in	O
this	O
image	O
?	O
”	O
if	O
we	O
have	O
a	O
training	O
set	O
containing	O
unsupervised	O
examples	O
of	O
objects	O
that	O
live	O
in	O
the	O
same	O
space	O
as	O
t	O
,	O
we	O
may	O
be	O
able	O
to	O
infer	O
the	O
meaning	O
of	O
unseen	O
instances	O
of	O
t	O
.	O
in	O
our	O
example	O
of	O
recognizing	O
cats	O
without	O
having	O
seen	O
an	O
image	O
of	O
the	O
cat	O
,	O
it	O
is	O
important	O
that	O
we	O
have	O
had	O
unlabeled	O
text	O
data	O
containing	O
sentences	O
such	O
as	O
“	O
cats	O
have	O
four	O
legs	O
”	O
or	O
“	O
cats	O
have	O
pointy	O
ears.	O
”	O
|	O
zero-shot	O
learning	O
requires	O
t	O
to	O
be	O
represented	O
in	O
a	O
way	O
that	O
allows	O
some	O
sort	O
of	O
generalization	O
.	O
for	O
example	O
,	O
t	O
can	O
not	O
be	O
just	O
a	O
one-hot	O
code	O
indicating	O
an	O
object	O
category.	O
)	O
provide	O
instead	O
a	O
distributed	O
representation	O
of	O
object	O
categories	O
by	O
using	O
a	O
learned	O
word	O
embedding	O
for	O
the	O
word	O
associated	O
with	O
each	O
category	O
.	O
socher	O
et	O
al	O
.	O
2013b	O
(	O
;	O
2014	O
et	O
al.	O
,	O
et	O
al.	O
,	O
et	O
al.	O
,	O
2013b	O
gouws	O
a	O
similar	O
phenomenon	O
happens	O
in	O
machine	O
translation	O
(	O
klementiev	O
2012	O
;	O
mikolov	O
)	O
:	O
we	O
have	O
words	O
in	O
one	O
language	O
,	O
and	O
the	O
relationships	O
between	O
words	O
can	O
be	O
learned	O
from	O
unilingual	O
corpora	O
;	O
on	O
the	O
other	O
hand	O
,	O
we	O
have	O
translated	O
sentences	O
which	O
relate	O
words	O
in	O
one	O
language	O
with	O
words	O
in	O
the	O
other	O
.	O
even	O
though	O
we	O
may	O
not	O
have	O
labeled	O
examples	O
translating	O
word	O
a	O
in	O
language	O
x	O
to	O
word	O
b	O
in	O
language	O
y	O
,	O
we	O
can	O
generalize	O
and	O
guess	O
a	O
translation	O
for	O
word	O
a	O
because	O
we	O
have	O
learned	O
a	O
distributed	O
representation	O
for	O
words	O
in	O
language	O
x	O
,	O
a	O
distributed	O
representation	O
for	O
words	O
in	O
language	O
y	O
,	O
and	O
created	O
a	O
link	O
(	O
possibly	O
two-way	O
)	O
relating	O
the	O
two	O
spaces	O
,	O
via	O
training	O
examples	O
consisting	O
of	O
matched	O
pairs	O
of	O
sentences	O
in	O
both	O
languages	O
.	O
this	O
transfer	O
will	O
be	O
most	O
successful	O
if	O
all	O
three	O
ingredients	O
(	O
the	O
two	O
representations	O
and	O
the	O
relations	O
between	O
them	O
)	O
are	O
learned	O
jointly	O
.	O
zero-shot	O
learning	O
is	O
a	O
particular	O
form	O
of	O
transfer	O
learning	O
.	O
the	O
same	O
principle	O
explains	O
how	O
one	O
can	O
perform	O
multi-modal	O
learning	O
,	O
capturing	O
a	O
representation	O
539	O
chapter	O
15.	O
representation	O
learning	O
hx	O
=	O
fx	O
(	O
)	O
x	O
hy	O
=	O
fy	O
(	O
)	O
y	O
fy	O
−	O
space	O
x	O
fx	O
xtest	O
−	O
space	O
y	O
y	O
test	O
)	O
pairs	O
in	O
the	O
training	O
set	O
(	O
x	O
y	O
,	O
fx	O
:	O
encoder	O
function	O
for	O
x	O
fy	O
:	O
encoder	O
function	O
for	O
y	O
relationship	O
between	O
embedded	O
points	O
within	O
one	O
of	O
the	O
domains	O
maps	O
between	O
representation	O
spaces	O
figure	O
15.3	O
:	O
transfer	O
learning	O
between	O
two	O
domains	O
x	O
and	O
y	O
enables	O
zero-shot	O
learning	O
.	O
labeled	O
or	O
unlabeled	O
examples	O
of	O
x	O
allow	O
one	O
to	O
learn	O
a	O
representation	O
function	O
fx	O
and	O
similarly	O
with	O
examples	O
of	O
y	O
to	O
learn	O
f	O
y.	O
each	O
application	O
of	O
the	O
fx	O
and	O
fy	O
functions	O
appears	O
as	O
an	O
upward	O
arrow	O
,	O
with	O
the	O
style	O
of	O
the	O
arrows	O
indicating	O
which	O
function	O
is	O
applied	O
.	O
distance	O
in	O
hx	O
space	O
provides	O
a	O
similarity	O
metric	O
between	O
any	O
pair	O
of	O
points	O
in	O
x	O
space	O
that	O
may	O
be	O
more	O
meaningful	O
than	O
distance	O
in	O
x	O
space	O
.	O
likewise	O
,	O
distance	O
in	O
hy	O
space	O
provides	O
a	O
similarity	O
metric	O
between	O
any	O
pair	O
of	O
points	O
in	O
y	O
space	O
.	O
both	O
of	O
these	O
similarity	O
functions	O
are	O
indicated	O
with	O
dotted	O
bidirectional	O
arrows	O
.	O
labeled	O
examples	O
(	O
dashed	O
horizontal	O
lines	O
)	O
are	O
pairs	O
(	O
x	O
y	O
,	O
)	O
which	O
allow	O
one	O
to	O
learn	O
a	O
one-way	O
or	O
two-way	O
map	O
(	O
solid	O
bidirectional	O
arrow	O
)	O
between	O
the	O
representations	O
fx	O
(	O
x	O
)	O
and	O
the	O
representations	O
fy	O
(	O
y	O
)	O
and	O
anchor	O
these	O
representations	O
to	O
each	O
other	O
.	O
zero-data	O
learning	O
is	O
then	O
enabled	O
as	O
follows	O
.	O
one	O
can	O
associate	O
an	O
image	O
xtest	O
to	O
a	O
word	O
y	O
test	O
,	O
even	O
if	O
no	O
image	O
of	O
that	O
word	O
was	O
ever	O
presented	O
,	O
simply	O
because	O
word-representations	O
f	O
y	O
(	O
ytest	O
)	O
and	O
image-representations	O
fx	O
(	O
xtest	O
)	O
can	O
be	O
related	O
to	O
each	O
other	O
via	O
the	O
maps	O
between	O
representation	O
spaces	O
.	O
it	O
works	O
because	O
,	O
although	O
that	O
image	O
and	O
that	O
word	O
were	O
never	O
paired	O
,	O
their	O
respective	O
feature	O
vectors	O
fx	O
(	O
xtest	O
)	O
and	O
fy	O
(	O
ytest	O
)	O
have	O
been	O
related	O
to	O
each	O
other	O
.	O
figure	O
inspired	O
from	O
suggestion	O
by	O
hrant	O
khachatrian	O
.	O
540	O
chapter	O
15.	O
representation	O
learning	O
in	O
one	O
modality	O
,	O
a	O
representation	O
in	O
the	O
other	O
,	O
and	O
the	O
relationship	O
(	O
in	O
general	O
a	O
joint	O
distribution	O
)	O
between	O
pairs	O
(	O
x	O
y	O
,	O
)	O
consisting	O
of	O
one	O
observation	O
x	O
in	O
one	O
modality	O
and	O
another	O
observation	O
y	O
in	O
the	O
other	O
modality	O
(	O
srivastava	O
and	O
salakhutdinov	O
,	O
2012	O
)	O
.	O
by	O
learning	O
all	O
three	O
sets	O
of	O
parameters	O
(	O
from	O
x	O
to	O
its	O
representation	O
,	O
from	O
y	O
to	O
its	O
representation	O
,	O
and	O
the	O
relationship	O
between	O
the	O
two	O
representations	O
)	O
,	O
concepts	O
in	O
one	O
representation	O
are	O
anchored	O
in	O
the	O
other	O
,	O
and	O
vice-versa	O
,	O
allowing	O
one	O
to	O
meaningfully	O
generalize	O
to	O
new	O
pairs	O
.	O
the	O
procedure	O
is	O
illustrated	O
in	O
ﬁgure	O
.	O
15.3	O
15.3	O
semi-supervised	O
disentangling	O
of	O
causal	O
factors	O
an	O
important	O
question	O
about	O
representation	O
learning	O
is	O
“	O
what	O
makes	O
one	O
repre-	O
sentation	O
better	O
than	O
another	O
?	O
”	O
one	O
hypothesis	O
is	O
that	O
an	O
ideal	O
representation	O
is	O
one	O
in	O
which	O
the	O
features	O
within	O
the	O
representation	O
correspond	O
to	O
the	O
under-	O
lying	O
causes	O
of	O
the	O
observed	O
data	O
,	O
with	O
separate	O
features	O
or	O
directions	O
in	O
feature	O
space	O
corresponding	O
to	O
diﬀerent	O
causes	O
,	O
so	O
that	O
the	O
representation	O
disentangles	O
the	O
causes	O
from	O
one	O
another	O
.	O
this	O
hypothesis	O
motivates	O
approaches	O
in	O
which	O
we	O
ﬁrst	O
seek	O
a	O
good	O
representation	O
for	O
p	O
(	O
x	O
)	O
.	O
such	O
a	O
representation	O
may	O
also	O
be	O
a	O
good	O
representation	O
for	O
computing	O
p	O
(	O
y	O
x	O
)	O
if	O
y	O
is	O
among	O
the	O
most	O
salient	O
causes	O
of	O
x.	O
this	O
idea	O
has	O
guided	O
a	O
large	O
amount	O
of	O
deep	O
learning	O
research	O
since	O
at	O
least	O
the	O
1990s	O
(	O
becker	O
and	O
hinton	O
1992	O
hinton	O
and	O
sejnowski	O
1999	O
)	O
,	O
in	O
more	O
detail	O
.	O
for	O
other	O
arguments	O
about	O
when	O
semi-supervised	O
learning	O
can	O
outperform	O
pure	O
supervised	O
learning	O
,	O
we	O
refer	O
the	O
reader	O
to	O
section	O
1.2	O
of	O
chapelle	O
et	O
al	O
.	O
2006	O
)	O
.	O
|	O
(	O
;	O
,	O
,	O
in	O
other	O
approaches	O
to	O
representation	O
learning	O
,	O
we	O
have	O
often	O
been	O
concerned	O
with	O
a	O
representation	O
that	O
is	O
easy	O
to	O
model—for	O
example	O
,	O
one	O
whose	O
entries	O
are	O
sparse	O
,	O
or	O
independent	O
from	O
each	O
other	O
.	O
a	O
representation	O
that	O
cleanly	O
separates	O
the	O
underlying	O
causal	O
factors	O
may	O
not	O
necessarily	O
be	O
one	O
that	O
is	O
easy	O
to	O
model	B
.	O
however	O
,	O
a	O
further	O
part	O
of	O
the	O
hypothesis	O
motivating	O
semi-supervised	O
learning	O
via	O
unsupervised	O
representation	O
learning	O
is	O
that	O
for	O
many	O
ai	O
tasks	O
,	O
these	O
two	O
properties	O
coincide	O
:	O
once	O
we	O
are	O
able	O
to	O
obtain	O
the	O
underlying	O
explanations	O
for	O
what	O
we	O
observe	O
,	O
it	O
generally	O
becomes	O
easy	O
to	O
isolate	O
individual	O
attributes	O
from	O
the	O
others	O
.	O
speciﬁcally	O
,	O
if	O
a	O
representation	O
h	O
represents	O
many	O
of	O
the	O
underlying	O
causes	O
of	O
the	O
observed	O
x	O
,	O
and	O
the	O
outputs	O
y	O
are	O
among	O
the	O
most	O
salient	O
causes	O
,	O
then	O
it	O
is	O
easy	O
to	O
predict	O
from	O
.	O
h	O
y	O
first	O
,	O
let	O
us	O
see	O
how	O
semi-supervised	O
learning	O
can	O
fail	O
because	O
unsupervised	O
)	O
.	O
consider	O
for	O
example	O
the	O
case	O
x	O
]	O
.	O
clearly	O
,	O
.	O
y	O
x	O
)	O
learning	O
of	O
p	O
(	O
x	O
)	O
is	O
of	O
no	O
help	O
to	O
learn	O
p	O
(	O
y	O
x	O
where	O
p	O
(	O
x	O
)	O
is	O
uniformly	O
distributed	O
and	O
we	O
want	O
to	O
learn	O
f	O
(	O
x	O
)	O
=	O
e	O
[	O
y	O
observing	O
a	O
training	O
set	O
of	O
values	O
alone	O
gives	O
us	O
no	O
information	O
about	O
p	O
(	O
x	O
|	O
|	O
|	O
541	O
chapter	O
15.	O
representation	O
learning	O
y=1	O
y=2	O
y=3	O
)	O
x	O
(	O
p	O
x	O
figure	O
15.4	O
:	O
example	O
of	O
a	O
density	O
over	O
x	O
that	O
is	O
a	O
mixture	O
over	O
three	O
components	O
.	O
the	O
component	O
identity	O
is	O
an	O
underlying	O
explanatory	O
factor	O
,	O
y.	O
because	O
the	O
mixture	O
components	O
(	O
e.g.	O
,	O
natural	O
object	O
classes	O
in	O
image	O
data	O
)	O
are	O
statistically	O
salient	O
,	O
just	O
modeling	O
p	O
(	O
x	O
)	O
in	O
an	O
unsupervised	O
way	O
with	O
no	O
labeled	O
example	O
already	O
reveals	O
the	O
factor	O
y.	O
next	O
,	O
let	O
us	O
see	O
a	O
simple	O
example	O
of	O
how	O
semi-supervised	O
learning	O
can	O
succeed	O
.	O
consider	O
the	O
situation	O
where	O
x	O
arises	O
from	O
a	O
mixture	O
,	O
with	O
one	O
mixture	O
component	O
per	O
value	O
of	O
y	O
,	O
as	O
illustrated	O
in	O
ﬁgure	O
.	O
if	O
the	O
mixture	O
components	O
are	O
well-	O
separated	O
,	O
then	O
modeling	O
p	O
(	O
x	O
)	O
reveals	O
precisely	O
where	O
each	O
component	O
is	O
,	O
and	O
a	O
single	O
labeled	O
example	O
of	O
each	O
class	O
will	O
then	O
be	O
enough	O
to	O
perfectly	O
learn	O
p	O
(	O
y	O
x	O
)	O
.	O
but	O
more	O
generally	O
,	O
what	O
could	O
make	O
be	O
tied	O
together	O
?	O
y	O
x	O
)	O
p	O
(	O
)	O
x	O
15.4	O
and	O
p	O
(	O
|	O
|	O
x	O
|	O
if	O
y	O
is	O
closely	O
associated	O
with	O
one	O
of	O
the	O
causal	O
factors	O
of	O
x	O
,	O
then	O
p	O
(	O
x	O
)	O
and	O
p	O
(	O
y	O
)	O
will	O
be	O
strongly	O
tied	O
,	O
and	O
unsupervised	O
representation	O
learning	O
that	O
tries	O
to	O
disentangle	O
the	O
underlying	O
factors	O
of	O
variation	O
is	O
likely	O
to	O
be	O
useful	O
as	O
a	O
semi-supervised	O
learning	O
strategy	O
.	O
consider	O
the	O
assumption	O
that	O
y	O
is	O
one	O
of	O
the	O
causal	O
factors	O
of	O
x	O
,	O
and	O
let	O
h	O
represent	O
all	O
those	O
factors	O
.	O
the	O
true	O
generative	O
process	O
can	O
be	O
conceived	O
as	O
structured	O
according	O
to	O
this	O
directed	O
graphical	O
model	B
,	O
with	O
:	O
x	O
as	O
the	O
parent	O
of	O
h	O
|	O
|	O
(	O
15.1	O
)	O
(	O
15.2	O
)	O
as	O
a	O
consequence	O
,	O
the	O
data	O
has	O
marginal	O
probability	O
p	O
(	O
h	O
x	O
)	O
=	O
(	O
p	O
,	O
)	O
p	O
x	O
h	O
(	O
)	O
h	O
.	O
p	O
(	O
)	O
=	O
x	O
(	O
.	O
)	O
x	O
h	O
ehp	O
from	O
this	O
straightforward	O
observation	O
,	O
we	O
conclude	O
that	O
the	O
best	O
possible	O
model	B
of	O
x	O
(	O
from	O
a	O
generalization	O
point	O
of	O
view	O
)	O
is	O
the	O
one	O
that	O
uncovers	O
the	O
above	O
“	O
true	O
”	O
542	O
chapter	O
15.	O
representation	O
learning	O
structure	O
,	O
with	O
h	O
as	O
a	O
latent	O
variable	O
that	O
explains	O
the	O
observed	O
variations	O
in	O
x.	O
the	O
“	O
ideal	O
”	O
representation	O
learning	O
discussed	O
above	O
should	O
thus	O
recover	O
these	O
latent	O
factors	O
.	O
if	O
y	O
is	O
one	O
of	O
these	O
(	O
or	O
closely	O
related	O
to	O
one	O
of	O
them	O
)	O
,	O
then	O
it	O
will	O
be	O
very	O
easy	O
to	O
learn	O
to	O
predict	O
y	O
from	O
such	O
a	O
representation	O
.	O
we	O
also	O
see	O
that	O
the	O
conditional	O
distribution	O
of	O
y	O
given	O
x	O
is	O
tied	O
by	O
bayes	O
’	O
rule	O
to	O
the	O
components	O
in	O
the	O
above	O
equation	O
:	O
|	O
p	O
(	O
y	O
x	O
)	O
=	O
|	O
(	O
p	O
x	O
y	O
(	O
)	O
y	O
p	O
)	O
p	O
(	O
)	O
x	O
.	O
(	O
15.3	O
)	O
|	O
thus	O
the	O
marginal	O
p	O
(	O
x	O
)	O
is	O
intimately	O
tied	O
to	O
the	O
conditional	O
p	O
(	O
y	O
x	O
)	O
and	O
knowledge	O
of	O
the	O
structure	O
of	O
the	O
former	O
should	O
be	O
helpful	O
to	O
learn	O
the	O
latter	O
.	O
therefore	O
,	O
in	O
situations	O
respecting	O
these	O
assumptions	O
,	O
semi-supervised	O
learning	O
should	O
improve	O
performance	O
.	O
an	O
important	O
research	O
problem	O
regards	O
the	O
fact	O
that	O
most	O
observations	O
are	O
formed	O
by	O
an	O
extremely	O
large	O
number	O
of	O
underlying	O
causes	O
.	O
suppose	O
y	O
=	O
hi	O
,	O
but	O
the	O
unsupervised	O
learner	O
does	O
not	O
know	O
which	O
hi	O
.	O
the	O
brute	O
force	O
solution	O
is	O
for	O
an	O
unsupervised	O
learner	O
to	O
learn	O
a	O
representation	O
that	O
captures	O
the	O
reasonably	O
salient	O
generative	O
factors	O
hj	O
and	O
disentangles	O
them	O
from	O
each	O
other	O
,	O
thus	O
making	O
it	O
easy	O
to	O
predict	O
from	O
,	O
regardless	O
of	O
which	O
h	O
i	O
is	O
associated	O
with	O
.y	O
all	O
h	O
y	O
in	O
practice	O
,	O
the	O
brute	O
force	O
solution	O
is	O
not	O
feasible	O
because	O
it	O
is	O
not	O
possible	O
to	O
capture	O
all	O
or	O
most	O
of	O
the	O
factors	O
of	O
variation	O
that	O
inﬂuence	O
an	O
observation	O
.	O
for	O
example	O
,	O
in	O
a	O
visual	O
scene	O
,	O
should	O
the	O
representation	O
always	O
encode	O
all	O
of	O
the	O
smallest	O
objects	O
in	O
the	O
background	O
?	O
it	O
is	O
a	O
well-documented	O
psychological	O
phenomenon	O
that	O
human	O
beings	O
fail	O
to	O
perceive	O
changes	O
in	O
their	O
environment	O
that	O
are	O
not	O
immediately	O
relevant	O
to	O
the	O
task	O
they	O
are	O
performing—see	O
,	O
e.g.	O
,	O
simons	O
(	O
and	O
levin	O
1998	O
)	O
.	O
an	O
important	O
research	O
frontier	O
in	O
semi-supervised	O
learning	O
is	O
determining	O
what	O
to	O
encode	O
in	O
each	O
situation	O
.	O
currently	O
,	O
two	O
of	O
the	O
main	O
strategies	O
for	O
dealing	O
with	O
a	O
large	O
number	O
of	O
underlying	O
causes	O
are	O
to	O
use	O
a	O
supervised	O
learning	O
signal	O
at	O
the	O
same	O
time	O
as	O
the	O
unsupervised	O
learning	O
signal	O
so	O
that	O
the	O
model	B
will	O
choose	O
to	O
capture	O
the	O
most	O
relevant	O
factors	O
of	O
variation	O
,	O
or	O
to	O
use	O
much	O
larger	O
representations	O
if	O
using	O
purely	O
unsupervised	O
learning	O
.	O
an	O
emerging	O
strategy	O
for	O
unsupervised	O
learning	O
is	O
to	O
modify	O
the	O
deﬁnition	O
of	O
which	O
underlying	O
causes	O
are	O
most	O
salient	O
.	O
historically	O
,	O
autoencoders	O
and	O
generative	O
models	O
have	O
been	O
trained	O
to	O
optimize	O
a	O
ﬁxed	O
criterion	O
,	O
often	O
similar	O
to	O
mean	O
squared	O
error	O
.	O
these	O
ﬁxed	O
criteria	O
determine	O
which	O
causes	O
are	O
considered	O
salient	O
.	O
for	O
example	O
,	O
mean	O
squared	O
error	O
applied	O
to	O
the	O
pixels	O
of	O
an	O
image	O
implicitly	O
speciﬁes	O
that	O
an	O
underlying	O
cause	O
is	O
only	O
salient	O
if	O
it	O
signiﬁcantly	O
changes	O
the	O
brightness	O
of	O
a	O
large	O
number	O
of	O
pixels	O
.	O
this	O
can	O
be	O
problematic	O
if	O
the	O
task	O
we	O
wish	O
to	O
solve	O
involves	O
interacting	O
with	O
small	O
objects	O
.	O
see	O
ﬁgure	O
for	O
an	O
example	O
15.5	O
543	O
chapter	O
15.	O
representation	O
learning	O
input	O
reconstruction	O
figure	O
15.5	O
:	O
an	O
autoencoder	O
trained	O
with	O
mean	O
squared	O
error	O
for	O
a	O
robotics	O
task	O
has	O
failed	O
to	O
reconstruct	O
a	O
ping	O
pong	O
ball	O
.	O
the	O
existence	O
of	O
the	O
ping	O
pong	O
ball	O
and	O
all	O
of	O
its	O
spatial	O
coordinates	O
are	O
important	O
underlying	O
causal	O
factors	O
that	O
generate	O
the	O
image	O
and	O
are	O
relevant	O
to	O
the	O
robotics	O
task	O
.	O
unfortunately	O
,	O
the	O
autoencoder	O
has	O
limited	O
capacity	O
,	O
and	O
the	O
training	O
with	O
mean	O
squared	O
error	O
did	O
not	O
identify	O
the	O
ping	O
pong	O
ball	O
as	O
being	O
salient	O
enough	O
to	O
encode	O
.	O
images	O
graciously	O
provided	O
by	O
chelsea	O
finn	O
.	O
of	O
a	O
robotics	O
task	O
in	O
which	O
an	O
autoencoder	O
has	O
failed	O
to	O
learn	O
to	O
encode	O
a	O
small	O
ping	O
pong	O
ball	O
.	O
this	O
same	O
robot	O
is	O
capable	O
of	O
successfully	O
interacting	O
with	O
larger	O
objects	O
,	O
such	O
as	O
baseballs	O
,	O
which	O
are	O
more	O
salient	O
according	O
to	O
mean	O
squared	O
error	O
.	O
other	O
deﬁnitions	O
of	O
salience	O
are	O
possible	O
.	O
for	O
example	O
,	O
if	O
a	O
group	O
of	O
pixels	O
follow	O
a	O
highly	O
recognizable	O
pattern	O
,	O
even	O
if	O
that	O
pattern	O
does	O
not	O
involve	O
extreme	O
brightness	O
or	O
darkness	O
,	O
then	O
that	O
pattern	O
could	O
be	O
considered	O
extremely	O
salient	O
.	O
one	O
way	O
to	O
implement	O
such	O
a	O
deﬁnition	O
of	O
salience	O
is	O
to	O
use	O
a	O
recently	O
developed	O
approach	O
called	O
generative	O
adversarial	O
networks	O
(	O
goodfellow	O
et	O
al	O
.	O
2014c	O
)	O
.	O
in	O
this	O
approach	O
,	O
a	O
generative	O
model	B
is	O
trained	O
to	O
fool	O
a	O
feedforward	O
classiﬁer	O
.	O
the	O
feedforward	O
classiﬁer	O
attempts	O
to	O
recognize	O
all	O
samples	O
from	O
the	O
generative	O
model	B
as	O
being	O
fake	O
,	O
and	O
all	O
samples	O
from	O
the	O
training	O
set	O
as	O
being	O
real	O
.	O
in	O
this	O
framework	O
,	O
any	O
structured	O
pattern	O
that	O
the	O
feedforward	O
network	O
can	O
recognize	O
is	O
highly	O
salient	O
.	O
the	O
generative	O
adversarial	O
network	O
will	O
be	O
described	O
in	O
more	O
detail	O
.	O
for	O
the	O
purposes	O
of	O
the	O
present	O
discussion	O
,	O
it	O
is	O
suﬃcient	O
to	O
in	O
section	O
understand	O
that	O
they	O
learn	O
how	O
to	O
determine	O
what	O
is	O
salient	O
.	O
lotter	O
et	O
al	O
.	O
2015	O
)	O
showed	O
that	O
models	O
trained	O
to	O
generate	O
images	O
of	O
human	O
heads	O
will	O
often	O
neglect	O
to	O
generate	O
the	O
ears	O
when	O
trained	O
with	O
mean	O
squared	O
error	O
,	O
but	O
will	O
successfully	O
generate	O
the	O
ears	O
when	O
trained	O
with	O
the	O
adversarial	O
framework	O
.	O
because	O
the	O
ears	O
are	O
not	O
extremely	O
bright	O
or	O
dark	O
compared	O
to	O
the	O
surrounding	O
skin	O
,	O
they	O
are	O
not	O
especially	O
salient	O
according	O
to	O
mean	O
squared	O
error	O
loss	O
,	O
but	O
their	O
highly	O
20.10.4	O
,	O
(	O
544	O
chapter	O
15.	O
representation	O
learning	O
ground	O
truth	O
mse	O
adversarial	O
(	O
center	O
)	O
figure	O
15.6	O
:	O
predictive	O
generative	O
networks	O
provide	O
an	O
example	O
of	O
the	O
importance	O
of	O
learning	O
which	O
features	O
are	O
salient	O
.	O
in	O
this	O
example	O
,	O
the	O
predictive	O
generative	O
network	O
has	O
been	O
trained	O
to	O
predict	O
the	O
appearance	O
of	O
a	O
3-d	O
model	B
of	O
a	O
human	O
head	O
at	O
a	O
speciﬁc	O
viewing	O
angle	O
.	O
(	O
left	O
)	O
ground	O
truth	O
.	O
this	O
is	O
the	O
correct	O
image	O
,	O
that	O
the	O
network	O
should	O
emit	O
.	O
image	O
produced	O
by	O
a	O
predictive	O
generative	O
network	O
trained	O
with	O
mean	O
squared	O
error	O
alone	O
.	O
because	O
the	O
ears	O
do	O
not	O
cause	O
an	O
extreme	O
diﬀerence	O
in	O
brightness	O
compared	O
to	O
the	O
neighboring	O
skin	O
,	O
they	O
were	O
not	O
suﬃciently	O
salient	O
for	O
the	O
model	B
to	O
learn	O
to	O
represent	O
them	O
.	O
(	O
right	O
)	O
image	O
produced	O
by	O
a	O
model	B
trained	O
with	O
a	O
combination	O
of	O
mean	O
squared	O
error	O
and	O
adversarial	O
loss	O
.	O
using	O
this	O
learned	O
cost	O
function	O
,	O
the	O
ears	O
are	O
salient	O
because	O
they	O
follow	O
a	O
predictable	O
pattern	O
.	O
learning	O
which	O
underlying	O
causes	O
are	O
important	O
and	O
relevant	O
enough	O
to	O
model	B
is	O
an	O
important	O
active	O
area	O
of	O
research	O
.	O
figures	O
graciously	O
provided	O
by	O
lotter	O
et	O
al	O
.	O
2015	O
)	O
.	O
(	O
recognizable	O
shape	O
and	O
consistent	O
position	B
means	O
that	O
a	O
feedforward	O
network	O
can	O
easily	O
learn	O
to	O
detect	O
them	O
,	O
making	O
them	O
highly	O
salient	O
under	O
the	O
generative	O
adversarial	O
framework	O
.	O
see	O
ﬁgure	O
for	O
example	O
images	O
.	O
generative	O
adversarial	O
networks	O
are	O
only	O
one	O
step	O
toward	O
determining	O
which	O
factors	O
should	O
be	O
represented	O
.	O
we	O
expect	O
that	O
future	O
research	O
will	O
discover	O
better	O
ways	O
of	O
determining	O
which	O
factors	O
to	O
represent	O
,	O
and	O
develop	O
mechanisms	O
for	O
representing	O
diﬀerent	O
factors	O
depending	O
on	O
the	O
task	O
.	O
15.6	O
|	O
2012	O
)	O
,	O
is	O
that	O
if	O
the	O
true	O
generative	O
process	O
has	O
a	O
beneﬁt	O
of	O
learning	O
the	O
underlying	O
causal	O
factors	O
,	O
as	O
pointed	O
out	O
by	O
schölkopf	O
et	O
al	O
.	O
(	O
x	O
as	O
an	O
eﬀect	O
and	O
y	O
as	O
a	O
cause	O
,	O
then	O
modeling	O
p	O
(	O
x	O
y	O
)	O
is	O
robust	O
to	O
changes	O
in	O
p	O
(	O
y	O
)	O
.	O
if	O
the	O
cause-eﬀect	O
relationship	O
was	O
reversed	O
,	O
this	O
would	O
not	O
be	O
true	O
,	O
since	O
by	O
bayes	O
’	O
rule	O
,	O
p	O
(	O
x	O
y	O
)	O
would	O
be	O
sensitive	O
to	O
changes	O
in	O
p	O
(	O
y	O
)	O
.	O
very	O
often	O
,	O
when	O
we	O
consider	O
changes	O
in	O
distribution	O
due	O
to	O
diﬀerent	O
domains	O
,	O
temporal	O
non-stationarity	O
,	O
or	O
changes	O
in	O
the	O
nature	O
of	O
the	O
task	O
,	O
the	O
causal	O
mechanisms	O
remain	O
invariant	O
(	O
the	O
laws	O
of	O
the	O
universe	O
are	O
constant	O
)	O
while	O
the	O
marginal	O
distribution	O
over	O
the	O
underlying	O
causes	O
can	O
change	O
.	O
hence	O
,	O
better	O
generalization	O
and	O
robustness	O
to	O
all	O
kinds	O
of	O
changes	O
can	O
|	O
545	O
chapter	O
15.	O
representation	O
learning	O
be	O
expected	O
via	O
learning	O
a	O
generative	O
model	B
that	O
attempts	O
to	O
recover	O
the	O
causal	O
factors	O
.	O
)	O
x	O
h	O
p	O
(	O
h	O
and	O
|	O
15.4	O
distributed	O
representation	O
distributed	O
representations	O
of	O
concepts—representations	O
composed	O
of	O
many	O
ele-	O
ments	O
that	O
can	O
be	O
set	O
separately	O
from	O
each	O
other—are	O
one	O
of	O
the	O
most	O
important	O
tools	O
for	O
representation	O
learning	O
.	O
distributed	O
representations	O
are	O
powerful	O
because	O
they	O
can	O
use	O
n	O
features	O
with	O
k	O
values	O
to	O
describe	O
k	O
n	O
diﬀerent	O
concepts	O
.	O
as	O
we	O
have	O
seen	O
throughout	O
this	O
book	O
,	O
both	O
neural	O
networks	O
with	O
multiple	O
hidden	O
units	O
and	O
probabilistic	O
models	O
with	O
multiple	O
latent	O
variables	O
make	O
use	O
of	O
the	O
strategy	O
of	O
distributed	O
representation	O
.	O
we	O
now	O
introduce	O
an	O
additional	O
observation	O
.	O
many	O
deep	O
learning	O
algorithms	O
are	O
motivated	O
by	O
the	O
assumption	O
that	O
the	O
hidden	O
units	O
can	O
learn	O
to	O
represent	O
the	O
underlying	O
causal	O
factors	O
that	O
explain	O
the	O
data	O
,	O
as	O
discussed	O
in	O
section	O
.	O
distributed	O
representations	O
are	O
natural	O
for	O
this	O
approach	O
,	O
because	O
each	O
direction	O
in	O
representation	O
space	O
can	O
correspond	O
to	O
the	O
value	O
of	O
a	O
diﬀerent	O
underlying	O
conﬁguration	O
variable	O
.	O
15.3	O
15.7	O
an	O
example	O
of	O
a	O
distributed	O
representation	O
is	O
a	O
vector	O
of	O
n	O
binary	O
features	O
,	O
which	O
can	O
take	O
2	O
n	O
conﬁgurations	O
,	O
each	O
potentially	O
corresponding	O
to	O
a	O
diﬀerent	O
region	O
in	O
input	O
space	O
,	O
as	O
illustrated	O
in	O
ﬁgure	O
.	O
this	O
can	O
be	O
compared	O
with	O
a	O
symbolic	O
representation	O
,	O
where	O
the	O
input	O
is	O
associated	O
with	O
a	O
single	O
symbol	O
or	O
category	O
.	O
if	O
there	O
are	O
n	O
symbols	O
in	O
the	O
dictionary	O
,	O
one	O
can	O
imagine	O
n	O
feature	O
detectors	O
,	O
each	O
corresponding	O
to	O
the	O
detection	O
of	O
the	O
presence	O
of	O
the	O
associated	O
category	O
.	O
in	O
that	O
case	O
only	O
n	O
diﬀerent	O
conﬁgurations	O
of	O
the	O
representation	O
space	O
are	O
possible	O
,	O
carving	O
n	O
diﬀerent	O
regions	O
in	O
input	O
space	O
,	O
as	O
illustrated	O
in	O
ﬁgure	O
.	O
15.8	O
such	O
a	O
symbolic	O
representation	O
is	O
also	O
called	O
a	O
one-hot	O
representation	O
,	O
since	O
it	O
can	O
be	O
captured	O
by	O
a	O
binary	O
vector	O
with	O
n	O
bits	O
that	O
are	O
mutually	O
exclusive	O
(	O
only	O
one	O
of	O
them	O
can	O
be	O
active	O
)	O
.	O
a	O
symbolic	O
representation	O
is	O
a	O
speciﬁc	O
example	O
of	O
the	O
broader	O
class	O
of	O
non-distributed	O
representations	O
,	O
which	O
are	O
representations	O
that	O
may	O
contain	O
many	O
entries	O
but	O
without	O
signiﬁcant	O
meaningful	O
separate	O
control	O
over	O
each	O
entry	O
.	O
examples	O
of	O
learning	O
algorithms	O
based	O
on	O
non-distributed	O
representations	O
include	O
:	O
•	O
•	O
clustering	O
methods	O
,	O
including	O
the	O
k-means	B
algorithm	O
:	O
each	O
input	O
point	O
is	O
assigned	O
to	O
exactly	O
one	O
cluster	O
.	O
k-nearest	B
neighbors	I
algorithms	O
:	O
one	O
or	O
a	O
few	O
templates	O
or	O
prototype	O
examples	O
are	O
associated	O
with	O
a	O
given	O
input	O
.	O
in	O
the	O
case	O
of	O
k	O
>	O
1	O
,	O
there	O
are	O
multiple	O
546	O
chapter	O
15.	O
representation	O
learning	O
h2	O
h3	O
	O
h	O
=	O
[	O
1	O
,	O
,0	O
0	O
]	O
	O
h	O
=	O
[	O
1	O
,	O
,1	O
0	O
]	O
	O
h	O
=	O
[	O
1	O
,	O
,0	O
1	O
]	O
h	O
=	O
[	O
1	O
,	O
,1	O
1	O
]	O
	O
h	O
=	O
[	O
0	O
,	O
,1	O
0	O
]	O
h	O
=	O
[	O
0	O
,	O
,1	O
1	O
]	O
	O
	O
h1	O
	O
h	O
=	O
[	O
0	O
,	O
,0	O
1	O
]	O
−	O
i	O
	O
∩	O
∩	O
h+	O
2	O
2	O
into	O
two	O
half-planes	O
.	O
let	O
h+	O
corresponds	O
to	O
the	O
region	O
h+	O
1	O
figure	O
15.7	O
:	O
illustration	O
of	O
how	O
a	O
learning	O
algorithm	O
based	O
on	O
a	O
distributed	O
representation	O
breaks	O
up	O
the	O
input	O
space	O
into	O
regions	O
.	O
in	O
this	O
example	O
,	O
there	O
are	O
three	O
binary	O
features	O
h1	O
,	O
h2	O
,	O
and	O
h3	O
.	O
each	O
feature	O
is	O
deﬁned	O
by	O
thresholding	O
the	O
output	O
of	O
a	O
learned	O
,	O
linear	O
transformation	O
.	O
each	O
feature	O
divides	O
r	O
i	O
be	O
the	O
set	O
of	O
input	O
be	O
the	O
set	O
of	O
input	O
points	O
for	O
which	O
hi	O
=	O
0.	O
in	O
this	O
points	O
for	O
which	O
hi	O
=	O
1	O
and	O
h	O
illustration	O
,	O
each	O
line	O
represents	O
the	O
decision	O
boundary	O
for	O
one	O
hi	O
,	O
with	O
the	O
corresponding	O
arrow	O
pointing	O
to	O
the	O
h+	O
i	O
side	O
of	O
the	O
boundary	O
.	O
the	O
representation	O
as	O
a	O
whole	O
takes	O
on	O
a	O
unique	O
value	O
at	O
each	O
possible	O
intersection	O
of	O
these	O
half-planes	O
.	O
for	O
example	O
,	O
the	O
representation	O
value	O
[	O
1	O
,	O
1	O
,	O
1	O
]	O
3	O
.	O
compare	O
this	O
to	O
the	O
h+	O
non-distributed	O
representations	O
in	O
ﬁgure	O
d	O
input	O
dimensions	O
,	O
.	O
in	O
the	O
general	O
case	O
of	O
a	O
distributed	O
representation	O
divides	O
r	O
d	O
by	O
intersecting	O
half-spaces	O
rather	O
than	O
half-planes	O
.	O
the	O
distributed	O
representation	O
with	O
n	O
features	O
assigns	O
unique	O
codes	O
to	O
o	O
(	O
nd	O
)	O
diﬀerent	O
regions	O
,	O
while	O
the	O
nearest	O
neighbor	O
algorithm	O
with	O
n	O
examples	O
assigns	O
unique	O
codes	O
to	O
only	O
n	O
regions	O
.	O
the	O
distributed	O
representation	O
is	O
thus	O
able	O
to	O
distinguish	O
exponentially	O
many	O
more	O
regions	O
than	O
the	O
non-distributed	O
one	O
.	O
keep	O
in	O
mind	O
that	O
not	O
all	O
h	O
values	O
are	O
feasible	O
(	O
there	O
is	O
no	O
h	O
=	O
0	O
in	O
this	O
example	O
)	O
and	O
that	O
a	O
linear	O
classiﬁer	O
on	O
top	O
of	O
the	O
distributed	O
representation	O
is	O
not	O
able	O
to	O
assign	O
diﬀerent	O
class	O
identities	O
to	O
every	O
neighboring	O
region	O
;	O
even	O
a	O
deep	O
linear-threshold	O
network	O
has	O
a	O
vc	O
dimension	O
of	O
only	O
o	O
(	O
w	O
wlog	O
)	O
where	O
w	O
is	O
the	O
number	O
of	O
weights	O
(	O
)	O
.	O
the	O
combination	O
of	O
a	O
powerful	O
representation	O
layer	O
and	O
a	O
weak	O
classiﬁer	O
layer	O
can	O
be	O
a	O
strong	O
regularizer	O
;	O
a	O
classiﬁer	O
trying	O
to	O
learn	O
the	O
concept	O
of	O
“	O
person	O
”	O
versus	O
“	O
not	O
a	O
person	O
”	O
does	O
not	O
need	O
to	O
assign	O
a	O
diﬀerent	O
class	O
to	O
an	O
input	O
represented	O
as	O
“	O
woman	O
with	O
glasses	O
”	O
than	O
it	O
assigns	O
to	O
an	O
input	O
represented	O
as	O
“	O
man	O
without	O
glasses.	O
”	O
this	O
capacity	O
constraint	O
encourages	O
each	O
classiﬁer	O
to	O
focus	O
on	O
few	O
hi	O
and	O
encourages	O
to	O
learn	O
to	O
represent	O
the	O
classes	O
in	O
a	O
linearly	O
separable	O
way	O
.	O
sontag	O
1998	O
,	O
15.8	O
h	O
547	O
chapter	O
15.	O
representation	O
learning	O
•	O
•	O
•	O
•	O
values	O
describing	O
each	O
input	O
,	O
but	O
they	O
can	O
not	O
be	O
controlled	O
separately	O
from	O
each	O
other	O
,	O
so	O
this	O
does	O
not	O
qualify	O
as	O
a	O
true	O
distributed	O
representation	O
.	O
decision	O
trees	O
:	O
only	O
one	O
leaf	O
(	O
and	O
the	O
nodes	O
on	O
the	O
path	O
from	O
root	O
to	O
leaf	O
)	O
is	O
activated	O
when	O
an	O
input	O
is	O
given	O
.	O
gaussian	O
mixtures	O
and	O
mixtures	O
of	O
experts	O
:	O
the	O
templates	O
(	O
cluster	O
centers	O
)	O
or	O
experts	O
are	O
now	O
associated	O
with	O
a	O
degree	O
of	O
activation	O
.	O
as	O
with	O
the	O
k-nearest	B
neighbors	I
algorithm	O
,	O
each	O
input	O
is	O
represented	O
with	O
multiple	O
values	O
,	O
but	O
those	O
values	O
can	O
not	O
readily	O
be	O
controlled	O
separately	O
from	O
each	O
other	O
.	O
kernel	O
machines	O
with	O
a	O
gaussian	O
kernel	O
(	O
or	O
other	O
similarly	O
local	O
kernel	O
)	O
:	O
although	O
the	O
degree	O
of	O
activation	O
of	O
each	O
“	O
support	O
vector	O
”	O
or	O
template	O
example	O
is	O
now	O
continuous-valued	O
,	O
the	O
same	O
issue	O
arises	O
as	O
with	O
gaussian	O
mixtures	O
.	O
language	O
or	O
translation	O
models	O
based	O
on	O
n-grams	O
.	O
the	O
set	O
of	O
contexts	O
(	O
sequences	O
of	O
symbols	O
)	O
is	O
partitioned	O
according	O
to	O
a	O
tree	O
structure	O
of	O
suﬃxes	O
.	O
a	O
leaf	O
may	O
correspond	O
to	O
the	O
last	O
two	O
words	O
being	O
w1	O
and	O
w2	O
,	O
for	O
example	O
.	O
separate	O
parameters	O
are	O
estimated	O
for	O
each	O
leaf	O
of	O
the	O
tree	O
(	O
with	O
some	O
sharing	O
being	O
possible	O
)	O
.	O
for	O
some	O
of	O
these	O
non-distributed	O
algorithms	O
,	O
the	O
output	O
is	O
not	O
constant	O
by	O
parts	O
but	O
instead	O
interpolates	O
between	O
neighboring	O
regions	O
.	O
the	O
relationship	O
between	O
the	O
number	O
of	O
parameters	O
(	O
or	O
examples	O
)	O
and	O
the	O
number	O
of	O
regions	O
they	O
can	O
deﬁne	O
remains	O
linear	O
.	O
an	O
important	O
related	O
concept	O
that	O
distinguishes	O
a	O
distributed	O
representation	O
from	O
a	O
symbolic	O
one	O
is	O
that	O
generalization	O
arises	O
due	O
to	O
shared	O
attributes	O
between	O
diﬀerent	O
concepts	O
.	O
as	O
pure	O
symbols	O
,	O
“	O
cat	O
”	O
and	O
“	O
dog	O
”	O
are	O
as	O
far	O
from	O
each	O
other	O
as	O
any	O
other	O
two	O
symbols	O
.	O
however	O
,	O
if	O
one	O
associates	O
them	O
with	O
a	O
meaningful	O
distributed	O
representation	O
,	O
then	O
many	O
of	O
the	O
things	O
that	O
can	O
be	O
said	O
about	O
cats	O
can	O
generalize	O
to	O
dogs	O
and	O
vice-versa	O
.	O
for	O
example	O
,	O
our	O
distributed	O
representation	O
may	O
contain	O
entries	O
such	O
as	O
“	O
has_fur	O
”	O
or	O
“	O
number_of_legs	O
”	O
that	O
have	O
the	O
same	O
value	O
for	O
the	O
embedding	O
of	O
both	O
“	O
cat	O
”	O
and	O
“	O
dog.	O
”	O
neural	O
language	O
models	O
that	O
operate	O
on	O
distributed	O
representations	O
of	O
words	O
generalize	O
much	O
better	O
than	O
other	O
models	O
that	O
operate	O
directly	O
on	O
one-hot	O
representations	O
of	O
words	O
,	O
as	O
discussed	O
in	O
section	O
similarity	O
space	O
,	O
in	O
which	O
semantically	O
close	O
concepts	O
(	O
or	O
inputs	O
)	O
are	O
close	O
in	O
distance	O
,	O
a	O
property	O
that	O
is	O
absent	O
from	O
purely	O
symbolic	O
representations	O
.	O
.	O
distributed	O
representations	O
induce	O
a	O
rich	O
12.4	O
when	O
and	O
why	O
can	O
there	O
be	O
a	O
statistical	O
advantage	O
from	O
using	O
a	O
distributed	O
representation	O
as	O
part	O
of	O
a	O
learning	O
algorithm	O
?	O
distributed	O
representations	O
can	O
548	O
chapter	O
15.	O
representation	O
learning	O
figure	O
15.8	O
:	O
illustration	O
of	O
how	O
the	O
nearest	O
neighbor	O
algorithm	O
breaks	O
up	O
the	O
input	O
space	O
into	O
diﬀerent	O
regions	O
.	O
the	O
nearest	O
neighbor	O
algorithm	O
provides	O
an	O
example	O
of	O
a	O
learning	O
algorithm	O
based	O
on	O
a	O
non-distributed	O
representation	O
.	O
diﬀerent	O
non-distributed	O
algorithms	O
may	O
have	O
diﬀerent	O
geometry	O
,	O
but	O
they	O
typically	O
break	O
the	O
input	O
space	O
into	O
regions	O
,	O
with	O
a	O
separate	O
set	O
of	O
parameters	O
for	O
each	O
region	O
.	O
the	O
advantage	O
of	O
a	O
non-distributed	O
approach	O
is	O
that	O
,	O
given	O
enough	O
parameters	O
,	O
it	O
can	O
ﬁt	O
the	O
training	O
set	O
without	O
solving	O
a	O
diﬃcult	O
optimization	O
algorithm	O
,	O
because	O
it	O
is	O
straightforward	O
to	O
choose	O
a	O
diﬀerent	O
output	O
independently	O
for	O
each	O
region	O
.	O
the	O
disadvantage	O
is	O
that	O
such	O
non-distributed	O
models	O
generalize	O
only	O
locally	O
via	O
the	O
smoothness	O
prior	O
,	O
making	O
it	O
diﬃcult	O
to	O
learn	O
a	O
complicated	O
function	O
with	O
more	O
peaks	O
and	O
troughs	O
than	O
the	O
available	O
number	O
of	O
examples	O
.	O
contrast	O
this	O
with	O
a	O
distributed	O
representation	O
,	O
ﬁgure	O
.	O
15.7	O
549	O
chapter	O
15.	O
representation	O
learning	O
v	O
≈	O
have	O
a	O
statistical	O
advantage	O
when	O
an	O
apparently	O
complicated	O
structure	O
can	O
be	O
compactly	O
represented	O
using	O
a	O
small	O
number	O
of	O
parameters	O
.	O
some	O
traditional	O
non-	O
distributed	O
learning	O
algorithms	O
generalize	O
only	O
due	O
to	O
the	O
smoothness	O
assumption	O
,	O
≈	O
,	O
then	O
the	O
target	O
function	O
f	O
to	O
be	O
learned	O
has	O
the	O
which	O
states	O
that	O
if	O
u	O
property	O
that	O
f	O
(	O
u	O
)	O
f	O
(	O
v	O
)	O
,	O
in	O
general	O
.	O
there	O
are	O
many	O
ways	O
of	O
formalizing	O
such	O
an	O
≈	O
assumption	O
,	O
but	O
the	O
end	O
result	O
is	O
that	O
if	O
we	O
have	O
an	O
example	O
(	O
x	O
,	O
y	O
)	O
for	O
which	O
we	O
y	O
,	O
then	O
we	O
choose	O
an	O
estimator	O
ˆf	O
that	O
approximately	O
satisﬁes	O
know	O
that	O
f	O
(	O
x	O
)	O
these	O
constraints	O
while	O
changing	O
as	O
little	O
as	O
possible	O
when	O
we	O
move	O
to	O
a	O
nearby	O
input	O
x	O
+	O
	O
.	O
this	O
assumption	O
is	O
clearly	O
very	O
useful	O
,	O
but	O
it	O
suﬀers	O
from	O
the	O
curse	O
of	O
dimensionality	O
:	O
in	O
order	O
to	O
learn	O
a	O
target	O
function	O
that	O
increases	O
and	O
decreases	O
many	O
times	O
in	O
many	O
diﬀerent	O
regions,1	O
we	O
may	O
need	O
a	O
number	O
of	O
examples	O
that	O
is	O
at	O
least	O
as	O
large	O
as	O
the	O
number	O
of	O
distinguishable	O
regions	O
.	O
one	O
can	O
think	O
of	O
each	O
of	O
these	O
regions	O
as	O
a	O
category	O
or	O
symbol	O
:	O
by	O
having	O
a	O
separate	O
degree	O
of	O
freedom	O
for	O
each	O
symbol	O
(	O
or	O
region	O
)	O
,	O
we	O
can	O
learn	O
an	O
arbitrary	O
decoder	O
mapping	O
from	O
symbol	O
to	O
value	O
.	O
however	O
,	O
this	O
does	O
not	O
allow	O
us	O
to	O
generalize	O
to	O
new	O
symbols	O
for	O
new	O
regions	O
.	O
if	O
we	O
are	O
lucky	O
,	O
there	O
may	O
be	O
some	O
regularity	O
in	O
the	O
target	O
function	O
,	O
besides	O
being	O
smooth	O
.	O
for	O
example	O
,	O
a	O
convolutional	O
network	O
with	O
max-pooling	O
can	O
recognize	O
an	O
object	O
regardless	O
of	O
its	O
location	O
in	O
the	O
image	O
,	O
even	O
though	O
spatial	O
translation	O
of	O
the	O
object	O
may	O
not	O
correspond	O
to	O
smooth	O
transformations	O
in	O
the	O
input	O
space	O
.	O
let	O
us	O
examine	O
a	O
special	O
case	O
of	O
a	O
distributed	O
representation	O
learning	O
algorithm	O
,	O
that	O
extracts	O
binary	O
features	O
by	O
thresholding	O
linear	O
functions	O
of	O
the	O
input	O
.	O
each	O
d	O
into	O
a	O
pair	O
of	O
half-spaces	O
,	O
as	O
binary	O
feature	O
in	O
this	O
representation	O
divides	O
r	O
n	O
illustrated	O
in	O
ﬁgure	O
of	O
the	O
corresponding	O
half-spaces	O
determines	O
how	O
many	O
regions	O
this	O
distributed	O
representation	O
learner	O
can	O
distinguish	O
.	O
how	O
many	O
regions	O
are	O
generated	O
by	O
an	O
d	O
?	O
by	O
applying	O
a	O
general	O
result	O
concerning	O
the	O
arrangement	O
of	O
n	O
hyperplanes	O
in	O
r	O
intersection	O
of	O
hyperplanes	O
(	O
2014b	O
)	O
that	O
the	O
number	O
of	O
regions	O
this	O
binary	O
feature	O
representation	O
can	O
distinguish	O
is	O
.	O
the	O
exponentially	O
large	O
number	O
of	O
intersections	O
of	O
)	O
,	O
one	O
can	O
show	O
(	O
zaslavsky	O
1975	O
	O
	O
pascanu	O
et	O
al.	O
,	O
15.7	O
	O
,	O
d	O
j=0	O
n	O
j	O
=	O
(	O
o	O
nd	O
)	O
.	O
(	O
15.4	O
)	O
therefore	O
,	O
we	O
see	O
a	O
growth	O
that	O
is	O
exponential	O
in	O
the	O
input	O
size	O
and	O
polynomial	O
in	O
the	O
number	O
of	O
hidden	O
units	O
.	O
1	O
potentially	O
,	O
we	O
may	O
want	O
to	O
learn	O
a	O
function	O
whose	O
behavior	O
is	O
distinct	O
in	O
exponentially	O
many	O
regions	O
:	O
in	O
a	O
d-dimensional	O
space	O
with	O
at	O
least	O
2	O
diﬀerent	O
values	O
to	O
distinguish	O
per	O
dimension	O
,	O
we	O
might	O
want	O
2d	O
diﬀerent	O
regions	O
,	O
requiring	O
o	O
(	O
2	O
d	O
)	O
training	O
examples	O
.	O
to	O
diﬀer	O
in	O
f	O
550	O
chapter	O
15.	O
representation	O
learning	O
this	O
provides	O
a	O
geometric	O
argument	O
to	O
explain	O
the	O
generalization	O
power	O
of	O
distributed	O
representation	O
:	O
with	O
o	O
(	O
nd	O
)	O
parameters	O
(	O
for	O
n	O
linear-threshold	O
features	O
d	O
)	O
we	O
can	O
distinctly	O
represent	O
o	O
(	O
nd	O
)	O
regions	O
in	O
input	O
space	O
.	O
if	O
instead	O
we	O
made	O
in	O
r	O
no	O
assumption	O
at	O
all	O
about	O
the	O
data	O
,	O
and	O
used	O
a	O
representation	O
with	O
one	O
unique	O
symbol	O
for	O
each	O
region	O
,	O
and	O
separate	O
parameters	O
for	O
each	O
symbol	O
to	O
recognize	O
its	O
d	O
,	O
then	O
specifying	O
o	O
(	O
nd	O
)	O
regions	O
would	O
require	O
o	O
(	O
nd	O
)	O
corresponding	O
portion	O
of	O
r	O
examples	O
.	O
more	O
generally	O
,	O
the	O
argument	O
in	O
favor	O
of	O
the	O
distributed	O
representation	O
could	O
be	O
extended	O
to	O
the	O
case	O
where	O
instead	O
of	O
using	O
linear	O
threshold	O
units	O
we	O
use	O
nonlinear	O
,	O
possibly	O
continuous	O
,	O
feature	O
extractors	O
for	O
each	O
of	O
the	O
attributes	O
in	O
the	O
distributed	O
representation	O
.	O
the	O
argument	O
in	O
this	O
case	O
is	O
that	O
if	O
a	O
parametric	O
	O
transformation	O
with	O
k	O
parameters	O
can	O
learn	O
about	O
r	O
regions	O
in	O
input	O
space	O
,	O
with	O
k	O
,	O
and	O
if	O
obtaining	O
such	O
a	O
representation	O
was	O
useful	O
to	O
the	O
task	O
of	O
interest	O
,	O
then	O
we	O
could	O
potentially	O
generalize	O
much	O
better	O
in	O
this	O
way	O
than	O
in	O
a	O
non-distributed	O
setting	O
where	O
we	O
would	O
need	O
o	O
(	O
r	O
)	O
examples	O
to	O
obtain	O
the	O
same	O
features	O
and	O
associated	O
partitioning	O
of	O
the	O
input	O
space	O
into	O
r	O
regions	O
.	O
using	O
fewer	O
parameters	O
to	O
represent	O
the	O
model	B
means	O
that	O
we	O
have	O
fewer	O
parameters	O
to	O
ﬁt	O
,	O
and	O
thus	O
require	O
far	O
fewer	O
training	O
examples	O
to	O
generalize	O
well	O
.	O
r	O
,	O
wlog	O
a	O
further	O
part	O
of	O
the	O
argument	O
for	O
why	O
models	O
based	O
on	O
distributed	O
represen-	O
tations	O
generalize	O
well	O
is	O
that	O
their	O
capacity	O
remains	O
limited	O
despite	O
being	O
able	O
to	O
distinctly	O
encode	O
so	O
many	O
diﬀerent	O
regions	O
.	O
for	O
example	O
,	O
the	O
vc	O
dimension	O
of	O
a	O
neural	O
network	O
of	O
linear	O
threshold	O
units	O
is	O
only	O
o	O
(	O
w	O
)	O
,	O
where	O
w	O
is	O
the	O
number	O
of	O
weights	O
(	O
sontag	O
1998	O
)	O
.	O
this	O
limitation	O
arises	O
because	O
,	O
while	O
we	O
can	O
assign	O
very	O
many	O
unique	O
codes	O
to	O
representation	O
space	O
,	O
we	O
can	O
not	O
use	O
absolutely	O
all	O
of	O
the	O
code	O
space	O
,	O
nor	O
can	O
we	O
learn	O
arbitrary	O
functions	O
mapping	O
from	O
the	O
representation	O
space	O
h	O
to	O
the	O
output	O
y	O
using	O
a	O
linear	O
classiﬁer	O
.	O
the	O
use	O
of	O
a	O
distributed	O
representation	O
combined	O
with	O
a	O
linear	O
classiﬁer	O
thus	O
expresses	O
a	O
prior	O
belief	O
that	O
the	O
classes	O
to	O
be	O
recognized	O
are	O
linearly	O
separable	O
as	O
a	O
function	O
of	O
the	O
underlying	O
causal	O
factors	O
captured	O
by	O
h.	O
we	O
will	O
typically	O
want	O
to	O
learn	O
categories	O
such	O
as	O
the	O
set	O
of	O
all	O
images	O
of	O
all	O
green	O
objects	O
or	O
the	O
set	O
of	O
all	O
images	O
of	O
cars	O
,	O
but	O
not	O
categories	O
that	O
require	O
nonlinear	O
,	O
xor	O
logic	O
.	O
for	O
example	O
,	O
we	O
typically	O
do	O
not	O
want	O
to	O
partition	O
the	O
data	O
into	O
the	O
set	O
of	O
all	O
red	O
cars	O
and	O
green	O
trucks	O
as	O
one	O
class	O
and	O
the	O
set	O
of	O
all	O
green	O
cars	O
and	O
red	O
trucks	O
as	O
another	O
class	O
.	O
(	O
zhou	O
et	O
al	O
.	O
2015	O
the	O
ideas	O
discussed	O
so	O
far	O
have	O
been	O
abstract	O
,	O
but	O
they	O
may	O
be	O
experimentally	O
validated.	O
)	O
ﬁnd	O
that	O
hidden	O
units	O
in	O
a	O
deep	O
convolutional	O
network	O
trained	O
on	O
the	O
imagenet	O
and	O
places	O
benchmark	O
datasets	O
learn	O
features	O
that	O
are	O
very	O
often	O
interpretable	O
,	O
corresponding	O
to	O
a	O
label	O
that	O
humans	O
would	O
naturally	O
assign	O
.	O
in	O
practice	O
it	O
is	O
certainly	O
not	O
always	O
the	O
case	O
that	O
hidden	O
units	O
learn	O
something	O
that	O
has	O
a	O
simple	O
linguistic	O
name	O
,	O
but	O
it	O
is	O
interesting	O
to	O
see	O
this	O
emerge	O
near	O
the	O
top	O
levels	O
of	O
the	O
best	O
computer	O
vision	O
deep	O
networks	O
.	O
what	O
such	O
features	O
have	O
in	O
551	O
chapter	O
15.	O
representation	O
learning	O
-	O
+	O
=	O
figure	O
15.9	O
:	O
a	O
generative	O
model	B
has	O
learned	O
a	O
distributed	O
representation	O
that	O
disentangles	O
the	O
concept	O
of	O
gender	O
from	O
the	O
concept	O
of	O
wearing	O
glasses	O
.	O
if	O
we	O
begin	O
with	O
the	O
repre-	O
sentation	O
of	O
the	O
concept	O
of	O
a	O
man	O
with	O
glasses	O
,	O
then	O
subtract	O
the	O
vector	O
representing	O
the	O
concept	O
of	O
a	O
man	O
without	O
glasses	O
,	O
and	O
ﬁnally	O
add	O
the	O
vector	O
representing	O
the	O
concept	O
of	O
a	O
woman	O
without	O
glasses	O
,	O
we	O
obtain	O
the	O
vector	O
representing	O
the	O
concept	O
of	O
a	O
woman	O
with	O
glasses	O
.	O
the	O
generative	O
model	B
correctly	O
decodes	O
all	O
of	O
these	O
representation	O
vectors	O
to	O
images	O
that	O
may	O
be	O
recognized	O
as	O
belonging	O
to	O
the	O
correct	O
class	O
.	O
images	O
reproduced	O
with	O
permission	O
from	O
radford	O
et	O
al	O
.	O
2015	O
)	O
.	O
(	O
(	O
15.9	O
radford	O
et	O
al	O
.	O
2015	O
common	O
is	O
that	O
one	O
could	O
imagine	O
learning	O
about	O
each	O
of	O
them	O
without	O
having	O
to	O
see	O
all	O
the	O
conﬁgurations	O
of	O
all	O
the	O
others.	O
)	O
demonstrated	O
that	O
a	O
generative	O
model	B
can	O
learn	O
a	O
representation	O
of	O
images	O
of	O
faces	O
,	O
with	O
separate	O
directions	O
in	O
representation	O
space	O
capturing	O
diﬀerent	O
underlying	O
factors	O
of	O
variation	O
.	O
figure	O
demonstrates	O
that	O
one	O
direction	O
in	O
representation	O
space	O
corresponds	O
to	O
whether	O
the	O
person	O
is	O
male	O
or	O
female	O
,	O
while	O
another	O
corresponds	O
to	O
whether	O
the	O
person	O
is	O
wearing	O
glasses	O
.	O
these	O
features	O
were	O
discovered	O
automatically	O
,	O
not	O
ﬁxed	O
a	O
priori	O
.	O
there	O
is	O
no	O
need	O
to	O
have	O
labels	O
for	O
the	O
hidden	O
unit	O
classiﬁers	O
:	O
gradient	O
descent	B
on	O
an	O
objective	O
function	O
of	O
interest	O
naturally	O
learns	O
semantically	O
interesting	O
features	O
,	O
so	O
long	O
as	O
the	O
task	O
requires	O
such	O
features	O
.	O
we	O
can	O
learn	O
about	O
the	O
distinction	O
between	O
male	O
and	O
female	O
,	O
or	O
about	O
the	O
presence	O
or	O
absence	O
of	O
glasses	O
,	O
without	O
having	O
to	O
characterize	O
all	O
of	O
the	O
conﬁgurations	O
of	O
the	O
n	O
1	O
other	O
features	O
by	O
examples	O
covering	O
all	O
of	O
these	O
combinations	O
of	O
values	O
.	O
this	O
form	O
of	O
statistical	O
separability	O
is	O
what	O
allows	O
one	O
to	O
generalize	O
to	O
new	O
conﬁgurations	O
of	O
a	O
person	O
’	O
s	O
features	O
that	O
have	O
never	O
been	O
seen	O
during	O
training	O
.	O
−	O
552	O
chapter	O
15.	O
representation	O
learning	O
15.5	O
exponential	O
gains	O
from	O
depth	O
6.4.1	O
we	O
have	O
seen	O
in	O
section	O
that	O
multilayer	O
perceptrons	O
are	O
universal	O
approxima-	O
tors	O
,	O
and	O
that	O
some	O
functions	O
can	O
be	O
represented	O
by	O
exponentially	O
smaller	O
deep	O
networks	O
compared	O
to	O
shallow	O
networks	O
.	O
this	O
decrease	O
in	O
model	B
size	O
leads	O
to	O
improved	O
statistical	O
eﬃciency	O
.	O
in	O
this	O
section	O
,	O
we	O
describe	O
how	O
similar	O
results	O
apply	O
more	O
generally	O
to	O
other	O
kinds	O
of	O
models	O
with	O
distributed	O
hidden	O
representations	O
.	O
15.4	O
in	O
section	O
,	O
we	O
saw	O
an	O
example	O
of	O
a	O
generative	O
model	B
that	O
learned	O
about	O
the	O
explanatory	O
factors	O
underlying	O
images	O
of	O
faces	O
,	O
including	O
the	O
person	O
’	O
s	O
gender	O
and	O
whether	O
they	O
are	O
wearing	O
glasses	O
.	O
the	O
generative	O
model	B
that	O
accomplished	O
this	O
task	O
was	O
based	O
on	O
a	O
deep	O
neural	O
network	O
.	O
it	O
would	O
not	O
be	O
reasonable	O
to	O
expect	O
a	O
shallow	O
network	O
,	O
such	O
as	O
a	O
linear	O
network	O
,	O
to	O
learn	O
the	O
complicated	O
relationship	O
between	O
these	O
abstract	O
explanatory	O
factors	O
and	O
the	O
pixels	O
in	O
the	O
image	O
.	O
in	O
this	O
and	O
other	O
ai	O
tasks	O
,	O
the	O
factors	O
that	O
can	O
be	O
chosen	O
almost	O
independently	O
from	O
each	O
other	O
yet	O
still	O
correspond	O
to	O
meaningful	O
inputs	O
are	O
more	O
likely	O
to	O
be	O
very	O
high-level	O
and	O
related	O
in	O
highly	O
nonlinear	O
ways	O
to	O
the	O
input	O
.	O
we	O
argue	O
that	O
this	O
demands	O
deep	O
distributed	O
representations	O
,	O
where	O
the	O
higher	O
level	O
features	O
(	O
seen	O
as	O
functions	O
of	O
the	O
input	O
)	O
or	O
factors	O
(	O
seen	O
as	O
generative	O
causes	O
)	O
are	O
obtained	O
through	O
the	O
composition	O
of	O
many	O
nonlinearities	O
.	O
it	O
has	O
been	O
proven	O
in	O
many	O
diﬀerent	O
settings	O
that	O
organizing	O
computation	O
through	O
the	O
composition	O
of	O
many	O
nonlinearities	O
and	O
a	O
hierarchy	O
of	O
reused	O
features	O
can	O
give	O
an	O
exponential	O
boost	O
to	O
statistical	O
eﬃciency	O
,	O
on	O
top	O
of	O
the	O
exponential	O
boost	O
given	O
by	O
using	O
a	O
distributed	O
representation	O
.	O
many	O
kinds	O
of	O
networks	O
(	O
e.g.	O
,	O
with	O
saturating	O
nonlinearities	O
,	O
boolean	O
gates	O
,	O
sum/products	O
,	O
or	O
rbf	O
units	O
)	O
with	O
a	O
single	O
hidden	O
layer	O
can	O
be	O
shown	O
to	O
be	O
universal	O
approximators	O
.	O
a	O
model	B
family	O
that	O
is	O
a	O
universal	O
approximator	O
can	O
approximate	O
a	O
large	O
class	O
of	O
functions	O
(	O
including	O
all	O
continuous	O
functions	O
)	O
up	O
to	O
any	O
non-zero	O
tolerance	O
level	O
,	O
given	O
enough	O
hidden	O
units	O
.	O
however	O
,	O
the	O
required	O
number	O
of	O
hidden	O
units	O
may	O
be	O
very	O
large	O
.	O
theoretical	O
results	O
concerning	O
the	O
expressive	O
power	O
of	O
deep	O
architectures	O
state	O
that	O
there	O
are	O
families	O
of	O
functions	O
that	O
can	O
be	O
represented	O
eﬃciently	O
by	O
an	O
architecture	O
of	O
depth	O
k	O
,	O
but	O
would	O
require	O
an	O
exponential	O
number	O
of	O
hidden	O
units	O
(	O
with	O
respect	O
to	O
the	O
input	O
size	O
)	O
with	O
insuﬃcient	O
depth	O
(	O
depth	O
2	O
or	O
depth	O
−	O
)	O
.	O
1	O
k	O
6.4.1	O
in	O
section	O
,	O
we	O
saw	O
that	O
deterministic	O
feedforward	O
networks	O
are	O
universal	O
approximators	O
of	O
functions	O
.	O
many	O
structured	O
probabilistic	O
models	O
with	O
a	O
single	O
hidden	O
layer	O
of	O
latent	O
variables	O
,	O
including	O
restricted	O
boltzmann	O
machines	O
and	O
deep	O
belief	O
networks	O
,	O
are	O
universal	O
approximators	O
of	O
probability	O
distributions	O
(	O
le	O
roux	O
and	O
bengio	O
2008	O
2010	O
montúfar	O
and	O
ay	O
2011	O
montúfar	O
2014	O
krause	O
et	O
al.	O
,	O
2013	O
)	O
.	O
,	O
,	O
,	O
;	O
;	O
,	O
;	O
553	O
chapter	O
15.	O
representation	O
learning	O
in	O
section	O
6.4.1	O
,	O
we	O
saw	O
that	O
a	O
suﬃciently	O
deep	O
feedforward	O
network	O
can	O
have	O
an	O
exponential	O
advantage	O
over	O
a	O
network	O
that	O
is	O
too	O
shallow	O
.	O
such	O
results	O
can	O
also	O
be	O
obtained	O
for	O
other	O
models	O
such	O
as	O
probabilistic	O
models	O
.	O
one	O
such	O
probabilistic	O
model	B
is	O
the	O
sum-product	O
network	O
or	O
spn	O
(	O
poon	O
and	O
domingos	O
2011	O
)	O
.	O
these	O
models	O
use	O
polynomial	O
circuits	O
to	O
compute	O
the	O
probability	O
distribution	O
over	O
a	O
set	O
of	O
random	O
variables.	O
)	O
showed	O
that	O
there	O
exist	O
probability	O
distributions	O
for	O
which	O
a	O
minimum	O
depth	O
of	O
spn	O
is	O
required	O
to	O
avoid	O
needing	O
an	O
exponentially	O
large	O
model	B
.	O
later	O
,	O
martens	O
and	O
medabalimi	O
2014	O
)	O
showed	O
that	O
there	O
are	O
signiﬁcant	O
diﬀerences	O
between	O
every	O
two	O
ﬁnite	O
depths	O
of	O
spn	O
,	O
and	O
that	O
some	O
of	O
the	O
constraints	O
used	O
to	O
make	O
spns	O
tractable	O
may	O
limit	O
their	O
representational	O
power	O
.	O
delalleau	O
and	O
bengio	O
2011	O
(	O
,	O
(	O
another	O
interesting	O
development	O
is	O
a	O
set	O
of	O
theoretical	O
results	O
for	O
the	O
expressive	O
power	O
of	O
families	O
of	O
deep	O
circuits	O
related	O
to	O
convolutional	O
nets	O
,	O
highlighting	O
an	O
exponential	O
advantage	O
for	O
the	O
deep	O
circuit	O
even	O
when	O
the	O
shallow	O
circuit	O
is	O
allowed	O
to	O
only	O
approximate	O
the	O
function	O
computed	O
by	O
the	O
deep	O
circuit	O
(	O
cohen	O
et	O
al	O
.	O
,	O
2015	O
)	O
.	O
by	O
comparison	O
,	O
previous	O
theoretical	O
work	B
made	O
claims	O
regarding	O
only	O
the	O
case	O
where	O
the	O
shallow	O
circuit	O
must	O
exactly	O
replicate	O
particular	O
functions	O
.	O
15.6	O
providing	O
clues	O
to	O
discover	O
underlying	O
causes	O
15.3	O
to	O
close	O
this	O
chapter	O
,	O
we	O
come	O
back	O
to	O
one	O
of	O
our	O
original	O
questions	O
:	O
what	O
makes	O
one	O
representation	O
better	O
than	O
another	O
?	O
one	O
answer	O
,	O
ﬁrst	O
introduced	O
in	O
section	O
,	O
is	O
that	O
an	O
ideal	O
representation	O
is	O
one	O
that	O
disentangles	O
the	O
underlying	O
causal	O
factors	O
of	O
variation	O
that	O
generated	O
the	O
data	O
,	O
especially	O
those	O
factors	O
that	O
are	O
relevant	O
to	O
our	O
applications	O
.	O
most	O
strategies	O
for	O
representation	O
learning	O
are	O
based	O
on	O
introducing	O
clues	O
that	O
help	O
the	O
learning	O
to	O
ﬁnd	O
these	O
underlying	O
factors	O
of	O
variations	O
.	O
the	O
clues	O
can	O
help	O
the	O
learner	O
separate	O
these	O
observed	O
factors	O
from	O
the	O
others	O
.	O
supervised	O
learning	O
provides	O
a	O
very	O
strong	O
clue	O
:	O
a	O
label	O
y	O
,	O
presented	O
with	O
each	O
x	O
,	O
that	O
usually	O
speciﬁes	O
the	O
value	O
of	O
at	O
least	O
one	O
of	O
the	O
factors	O
of	O
variation	O
directly	O
.	O
more	O
generally	O
,	O
to	O
make	O
use	O
of	O
abundant	O
unlabeled	O
data	O
,	O
representation	O
learning	O
makes	O
use	O
of	O
other	O
,	O
less	O
direct	O
,	O
hints	O
about	O
the	O
underlying	O
factors	O
.	O
these	O
hints	O
take	O
the	O
form	O
of	O
implicit	O
prior	O
beliefs	O
that	O
we	O
,	O
the	O
designers	O
of	O
the	O
learning	O
algorithm	O
,	O
impose	O
in	O
order	O
to	O
guide	O
the	O
learner	O
.	O
results	O
such	O
as	O
the	O
no	O
free	O
lunch	O
theorem	O
show	O
that	O
regularization	O
strategies	O
are	O
necessary	O
to	O
obtain	O
good	O
generalization	O
.	O
while	O
it	O
is	O
impossible	O
to	O
ﬁnd	O
a	O
universally	O
superior	O
regularization	O
strategy	O
,	O
one	O
goal	O
of	O
deep	O
learning	O
is	O
to	O
ﬁnd	O
a	O
set	O
of	O
fairly	O
generic	O
regularization	O
strategies	O
that	O
are	O
applicable	O
to	O
a	O
wide	O
variety	O
of	O
ai	O
tasks	O
,	O
similar	O
to	O
the	O
tasks	O
that	O
people	O
and	O
animals	O
are	O
able	O
to	O
solve	O
.	O
554	O
chapter	O
15.	O
representation	O
learning	O
we	O
provide	O
here	O
a	O
list	O
of	O
these	O
generic	O
regularization	O
strategies	O
.	O
the	O
list	O
is	O
clearly	O
not	O
exhaustive	O
,	O
but	O
gives	O
some	O
concrete	O
examples	O
of	O
ways	O
that	O
learning	O
algorithms	O
can	O
be	O
encouraged	O
to	O
discover	O
features	O
that	O
correspond	O
to	O
underlying	O
factors	O
.	O
this	O
list	O
was	O
introduced	O
in	O
section	O
3.1	O
of	O
)	O
and	O
has	O
been	O
partially	O
expanded	O
here	O
.	O
bengio	O
et	O
al	O
.	O
2013d	O
(	O
≈	O
•	O
•	O
•	O
•	O
•	O
smoothness	O
:	O
this	O
is	O
the	O
assumption	O
that	O
f	O
(	O
x	O
+	O
d	O
)	O
f	O
(	O
x	O
)	O
for	O
unit	O
d	O
and	O
small	O
	O
.	O
this	O
assumption	O
allows	O
the	O
learner	O
to	O
generalize	O
from	O
training	O
examples	O
to	O
nearby	O
points	O
in	O
input	O
space	O
.	O
many	O
machine	O
learning	O
algorithms	O
leverage	O
this	O
idea	O
,	O
but	O
it	O
is	O
insuﬃcient	O
to	O
overcome	O
the	O
curse	O
of	O
dimensionality	O
.	O
linearity	O
:	O
many	O
learning	O
algorithms	O
assume	O
that	O
relationships	O
between	O
some	O
variables	O
are	O
linear	O
.	O
this	O
allows	O
the	O
algorithm	O
to	O
make	O
predictions	O
even	O
very	O
far	O
from	O
the	O
observed	O
data	O
,	O
but	O
can	O
sometimes	O
lead	O
to	O
overly	O
extreme	O
predictions	O
.	O
most	O
simple	O
machine	O
learning	O
algorithms	O
that	O
do	O
not	O
make	O
the	O
smoothness	O
assumption	O
instead	O
make	O
the	O
linearity	O
assumption	O
.	O
these	O
are	O
in	O
fact	O
diﬀerent	O
assumptions—linear	O
functions	O
with	O
large	O
weights	O
applied	O
to	O
high-dimensional	O
spaces	O
may	O
not	O
be	O
very	O
smooth	O
.	O
see	O
goodfellow	O
et	O
al	O
.	O
(	O
)	O
for	O
a	O
further	O
discussion	O
of	O
the	O
limitations	O
of	O
the	O
linearity	O
assumption	O
.	O
2014b	O
multiple	O
explanatory	O
factors	O
:	O
many	O
representation	O
learning	O
algorithms	O
are	O
motivated	O
by	O
the	O
assumption	O
that	O
the	O
data	O
is	O
generated	O
by	O
multiple	O
underlying	O
explanatory	O
factors	O
,	O
and	O
that	O
most	O
tasks	O
can	O
be	O
solved	O
easily	O
given	O
the	O
state	O
of	O
each	O
of	O
these	O
factors	O
.	O
section	O
describes	O
how	O
this	O
view	O
motivates	O
semi-	O
|	O
supervised	O
learning	O
via	O
representation	O
learning	O
.	O
learning	O
the	O
structure	O
of	O
p	O
(	O
x	O
)	O
requires	O
learning	O
some	O
of	O
the	O
same	O
features	O
that	O
are	O
useful	O
for	O
modeling	O
p	O
(	O
y	O
x	O
)	O
because	O
both	O
refer	O
to	O
the	O
same	O
underlying	O
explanatory	O
factors	O
.	O
section	O
15.4	O
describes	O
how	O
this	O
view	O
motivates	O
the	O
use	O
of	O
distributed	O
representations	O
,	O
with	O
separate	O
directions	O
in	O
representation	O
space	O
corresponding	O
to	O
separate	O
factors	O
of	O
variation	O
.	O
15.3	O
causal	O
factors	O
:	O
the	O
model	B
is	O
constructed	O
in	O
such	O
a	O
way	O
that	O
it	O
treats	O
the	O
factors	O
of	O
variation	O
described	O
by	O
the	O
learned	O
representation	O
h	O
as	O
the	O
causes	O
of	O
the	O
observed	O
data	O
x	O
,	O
and	O
not	O
vice-versa	O
.	O
as	O
discussed	O
in	O
section	O
,	O
this	O
is	O
advantageous	O
for	O
semi-supervised	O
learning	O
and	O
makes	O
the	O
learned	O
model	B
more	O
robust	O
when	O
the	O
distribution	O
over	O
the	O
underlying	O
causes	O
changes	O
or	O
when	O
we	O
use	O
the	O
model	B
for	O
a	O
new	O
task	O
.	O
15.3	O
,	O
or	O
a	O
hierarchical	O
organization	O
of	O
explanatory	O
factors	O
depth	O
:	O
high-level	O
,	O
abstract	O
concepts	O
can	O
be	O
deﬁned	O
in	O
terms	O
of	O
simple	O
concepts	O
,	O
forming	O
a	O
hierarchy	O
.	O
from	O
another	O
point	O
of	O
view	O
,	O
the	O
use	O
of	O
a	O
deep	O
architecture	O
555	O
chapter	O
15.	O
representation	O
learning	O
•	O
•	O
•	O
•	O
•	O
•	O
expresses	O
our	O
belief	O
that	O
the	O
task	O
should	O
be	O
accomplished	O
via	O
a	O
multi-step	O
program	O
,	O
with	O
each	O
step	O
referring	O
back	O
to	O
the	O
output	O
of	O
the	O
processing	O
accomplished	O
via	O
previous	O
steps	O
.	O
shared	O
factors	O
across	O
tasks	O
:	O
in	O
the	O
context	O
where	O
we	O
have	O
many	O
tasks	O
,	O
corresponding	O
to	O
diﬀerent	O
yi	O
variables	O
sharing	O
the	O
same	O
input	O
x	O
or	O
where	O
each	O
task	O
is	O
associated	O
with	O
a	O
subset	O
or	O
a	O
function	O
f	O
(	O
)	O
i	O
(	O
x	O
)	O
of	O
a	O
global	O
input	O
x	O
,	O
the	O
assumption	O
is	O
that	O
each	O
y	O
i	O
is	O
associated	O
with	O
a	O
diﬀerent	O
subset	O
from	O
a	O
common	O
pool	O
of	O
relevant	O
factors	O
h.	O
because	O
these	O
subsets	O
overlap	O
,	O
learning	O
all	O
the	O
p	O
(	O
y	O
i	O
)	O
allows	O
sharing	O
of	O
statistical	O
strength	O
between	O
the	O
tasks	O
.	O
x	O
)	O
via	O
a	O
shared	O
intermediate	O
representation	O
p	O
(	O
h	O
x	O
|	O
|	O
manifolds	O
:	O
probability	O
mass	O
concentrates	O
,	O
and	O
the	O
regions	O
in	O
which	O
it	O
con-	O
centrates	O
are	O
locally	O
connected	O
and	O
occupy	O
a	O
tiny	O
volume	O
.	O
in	O
the	O
continuous	O
case	O
,	O
these	O
regions	O
can	O
be	O
approximated	O
by	O
low-dimensional	O
manifolds	O
with	O
a	O
much	O
smaller	O
dimensionality	O
than	O
the	O
original	O
space	O
where	O
the	O
data	O
lives	O
.	O
many	O
machine	O
learning	O
algorithms	O
behave	O
sensibly	O
only	O
on	O
this	O
manifold	O
(	O
)	O
.	O
some	O
machine	O
learning	O
algorithms	O
,	O
especially	O
goodfellow	O
et	O
al	O
.	O
2014b	O
autoencoders	O
,	O
attempt	O
to	O
explicitly	O
learn	O
the	O
structure	O
of	O
the	O
manifold	O
.	O
,	O
natural	O
clustering	O
:	O
many	O
machine	O
learning	O
algorithms	O
assume	O
that	O
each	O
connected	O
manifold	O
in	O
the	O
input	O
space	O
may	O
be	O
assigned	O
to	O
a	O
single	O
class	O
.	O
the	O
data	O
may	O
lie	O
on	O
many	O
disconnected	O
manifolds	O
,	O
but	O
the	O
class	O
remains	O
constant	O
within	O
each	O
one	O
of	O
these	O
.	O
this	O
assumption	O
motivates	O
a	O
variety	O
of	O
learning	O
algorithms	O
,	O
including	O
tangent	O
propagation	O
,	O
double	O
backprop	O
,	O
the	O
manifold	O
tangent	O
classiﬁer	O
and	O
adversarial	O
training	O
.	O
temporal	O
and	O
spatial	O
coherence	O
:	O
slow	O
feature	O
analysis	O
and	O
related	O
algorithms	O
make	O
the	O
assumption	O
that	O
the	O
most	O
important	O
explanatory	O
factors	O
change	O
slowly	O
over	O
time	O
,	O
or	O
at	O
least	O
that	O
it	O
is	O
easier	O
to	O
predict	O
the	O
true	O
underlying	O
explanatory	O
factors	O
than	O
to	O
predict	O
raw	O
observations	O
such	O
as	O
pixel	O
values	O
.	O
see	O
section	O
for	O
further	O
description	O
of	O
this	O
approach	O
.	O
13.3	O
sparsity	O
:	O
most	O
features	O
should	O
presumably	O
not	O
be	O
relevant	O
to	O
describing	O
most	O
inputs—there	O
is	O
no	O
need	O
to	O
use	O
a	O
feature	O
that	O
detects	O
elephant	O
trunks	O
when	O
representing	O
an	O
image	O
of	O
a	O
cat	O
.	O
it	O
is	O
therefore	O
reasonable	O
to	O
impose	O
a	O
prior	O
that	O
any	O
feature	O
that	O
can	O
be	O
interpreted	O
as	O
“	O
present	O
”	O
or	O
“	O
absent	O
”	O
should	O
be	O
absent	O
most	O
of	O
the	O
time	O
.	O
simplicity	O
of	O
factor	O
dependencies	O
:	O
in	O
good	O
high-level	O
representations	O
,	O
the	O
factors	O
are	O
related	O
to	O
each	O
other	O
through	O
simple	O
dependencies	O
.	O
the	O
simplest	O
556	O
chapter	O
15.	O
representation	O
learning	O
	O
possible	O
is	O
marginal	O
independence	O
,	O
p	O
(	O
h	O
)	O
=	O
i	O
p	O
(	O
hi	O
)	O
,	O
but	O
linear	O
dependencies	O
or	O
those	O
captured	O
by	O
a	O
shallow	O
autoencoder	O
are	O
also	O
reasonable	O
assumptions	O
.	O
this	O
can	O
be	O
seen	O
in	O
many	O
laws	O
of	O
physics	O
,	O
and	O
is	O
assumed	O
when	O
plugging	O
a	O
linear	O
predictor	O
or	O
a	O
factorized	O
prior	O
on	O
top	O
of	O
a	O
learned	O
representation	O
.	O
the	O
concept	O
of	O
representation	O
learning	O
ties	O
together	O
all	O
of	O
the	O
many	O
forms	O
of	O
deep	O
learning	O
.	O
feedforward	O
and	O
recurrent	O
networks	O
,	O
autoencoders	O
and	O
deep	O
probabilistic	O
models	O
all	O
learn	O
and	O
exploit	O
representations	O
.	O
learning	O
the	O
best	O
possible	O
representation	O
remains	O
an	O
exciting	O
avenue	O
of	O
research	O
.	O
557	O
chapter	O
16	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
deep	O
learning	O
draws	O
upon	O
many	O
modeling	O
formalisms	O
that	O
researchers	O
can	O
use	O
to	O
guide	O
their	O
design	O
eﬀorts	O
and	O
describe	O
their	O
algorithms	O
.	O
one	O
of	O
these	O
formalisms	O
is	O
the	O
idea	O
of	O
structured	O
probabilistic	O
models	O
.	O
we	O
have	O
already	O
discussed	O
structured	O
probabilistic	O
models	O
brieﬂy	O
in	O
section	O
.	O
that	O
brief	O
presentation	O
was	O
suﬃcient	O
to	O
understand	O
how	O
to	O
use	O
structured	O
probabilistic	O
models	O
as	O
a	O
language	O
to	O
describe	O
some	O
of	O
the	O
algorithms	O
in	O
part	O
,	O
structured	O
probabilistic	O
models	O
are	O
a	O
key	O
ingredient	O
of	O
many	O
of	O
the	O
most	O
important	O
research	O
topics	O
in	O
deep	O
learning	O
.	O
in	O
order	O
to	O
prepare	O
to	O
discuss	O
these	O
research	O
ideas	O
,	O
this	O
chapter	O
describes	O
structured	O
probabilistic	O
models	O
in	O
much	O
greater	O
detail	O
.	O
this	O
chapter	O
is	O
intended	O
to	O
be	O
self-contained	O
;	O
the	O
reader	O
does	O
not	O
need	O
to	O
review	O
the	O
earlier	O
introduction	O
before	O
continuing	O
with	O
this	O
chapter	O
.	O
.	O
now	O
,	O
in	O
part	O
3.14	O
ii	O
iii	O
a	O
structured	O
probabilistic	O
model	B
is	O
a	O
way	O
of	O
describing	O
a	O
probability	O
distribution	O
,	O
using	O
a	O
graph	O
to	O
describe	O
which	O
random	O
variables	O
in	O
the	O
probability	O
distribution	O
interact	O
with	O
each	O
other	O
directly	O
.	O
here	O
we	O
use	O
“	O
graph	O
”	O
in	O
the	O
graph	O
theory	O
sense—a	O
set	O
of	O
vertices	O
connected	O
to	O
one	O
another	O
by	O
a	O
set	O
of	O
edges	O
.	O
because	O
the	O
structure	O
of	O
the	O
model	B
is	O
deﬁned	O
by	O
a	O
graph	O
,	O
these	O
models	O
are	O
often	O
also	O
referred	O
to	O
as	O
graphical	O
models	O
.	O
the	O
graphical	O
models	O
research	O
community	O
is	O
large	O
and	O
has	O
developed	O
many	O
diﬀerent	O
models	O
,	O
training	O
algorithms	O
,	O
and	O
inference	O
algorithms	O
.	O
in	O
this	O
chapter	O
,	O
we	O
provide	O
basic	O
background	O
on	O
some	O
of	O
the	O
most	O
central	O
ideas	O
of	O
graphical	O
models	O
,	O
with	O
an	O
emphasis	O
on	O
the	O
concepts	O
that	O
have	O
proven	O
most	O
useful	O
to	O
the	O
deep	O
learning	O
research	O
community	O
.	O
if	O
you	O
already	O
have	O
a	O
strong	O
background	O
in	O
graphical	O
models	O
,	O
you	O
may	O
wish	O
to	O
skip	O
most	O
of	O
this	O
chapter	O
.	O
however	O
,	O
even	O
a	O
graphical	O
model	B
expert	O
558	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
,	O
in	O
which	O
we	O
may	O
beneﬁt	O
from	O
reading	O
the	O
ﬁnal	O
section	O
of	O
this	O
chapter	O
,	O
section	O
highlight	O
some	O
of	O
the	O
unique	O
ways	O
that	O
graphical	O
models	O
are	O
used	O
for	O
deep	O
learning	O
algorithms	O
.	O
deep	O
learning	O
practitioners	O
tend	O
to	O
use	O
very	O
diﬀerent	O
model	B
structures	O
,	O
learning	O
algorithms	O
and	O
inference	O
procedures	O
than	O
are	O
commonly	O
used	O
by	O
the	O
rest	O
of	O
the	O
graphical	O
models	O
research	O
community	O
.	O
in	O
this	O
chapter	O
,	O
we	O
identify	O
these	O
diﬀerences	O
in	O
preferences	O
and	O
explain	O
the	O
reasons	O
for	O
them	O
.	O
16.7	O
in	O
this	O
chapter	O
we	O
ﬁrst	O
describe	O
the	O
challenges	O
of	O
building	O
large-scale	O
proba-	O
bilistic	O
models	O
.	O
next	O
,	O
we	O
describe	O
how	O
to	O
use	O
a	O
graph	O
to	O
describe	O
the	O
structure	O
of	O
a	O
probability	O
distribution	O
.	O
while	O
this	O
approach	O
allows	O
us	O
to	O
overcome	O
many	O
challenges	O
,	O
it	O
is	O
not	O
without	O
its	O
own	O
complications	O
.	O
one	O
of	O
the	O
major	O
diﬃculties	O
in	O
graphical	O
modeling	O
is	O
understanding	O
which	O
variables	O
need	O
to	O
be	O
able	O
to	O
interact	O
directly	O
,	O
i.e.	O
,	O
which	O
graph	O
structures	O
are	O
most	O
suitable	O
for	O
a	O
given	O
problem	O
.	O
we	O
outline	O
two	O
approaches	O
to	O
resolving	O
this	O
diﬃculty	O
by	O
learning	O
about	O
the	O
dependen-	O
cies	O
in	O
section	O
.	O
finally	O
,	O
we	O
close	O
with	O
a	O
discussion	O
of	O
the	O
unique	O
emphasis	O
that	O
deep	O
learning	O
practitioners	O
place	O
on	O
speciﬁc	O
approaches	O
to	O
graphical	O
modeling	O
in	O
section	O
.	O
16.7	O
16.5	O
16.1	O
the	O
challenge	O
of	O
unstructured	O
modeling	O
the	O
goal	O
of	O
deep	O
learning	O
is	O
to	O
scale	O
machine	O
learning	O
to	O
the	O
kinds	O
of	O
challenges	O
needed	O
to	O
solve	O
artiﬁcial	O
intelligence	O
.	O
this	O
means	O
being	O
able	O
to	O
understand	O
high-	O
dimensional	O
data	O
with	O
rich	O
structure	O
.	O
for	O
example	O
,	O
we	O
would	O
like	O
ai	O
algorithms	O
to	O
be	O
able	O
to	O
understand	O
natural	O
images,1	O
audio	O
waveforms	O
representing	O
speech	O
,	O
and	O
documents	O
containing	O
multiple	O
words	O
and	O
punctuation	O
characters	O
.	O
classiﬁcation	O
algorithms	O
can	O
take	O
an	O
input	O
from	O
such	O
a	O
rich	O
high-dimensional	O
distribution	O
and	O
summarize	O
it	O
with	O
a	O
categorical	O
label—what	O
object	O
is	O
in	O
a	O
photo	O
,	O
what	O
word	O
is	O
spoken	O
in	O
a	O
recording	O
,	O
what	O
topic	O
a	O
document	O
is	O
about	O
.	O
the	O
process	O
of	O
classiﬁcation	O
discards	O
most	O
of	O
the	O
information	O
in	O
the	O
input	O
and	O
produces	O
a	O
single	O
output	O
(	O
or	O
a	O
probability	O
distribution	O
over	O
values	O
of	O
that	O
single	O
output	O
)	O
.	O
the	O
classiﬁer	O
is	O
also	O
often	O
able	O
to	O
ignore	O
many	O
parts	O
of	O
the	O
input	O
.	O
for	O
example	O
,	O
when	O
recognizing	O
an	O
object	O
in	O
a	O
photo	O
,	O
it	O
is	O
usually	O
possible	O
to	O
ignore	O
the	O
background	O
of	O
the	O
photo	O
.	O
it	O
is	O
possible	O
to	O
ask	O
probabilistic	O
models	O
to	O
do	O
many	O
other	O
tasks	O
.	O
these	O
tasks	O
are	O
often	O
more	O
expensive	O
than	O
classiﬁcation	O
.	O
some	O
of	O
them	O
require	O
producing	O
multiple	O
output	O
values	O
.	O
most	O
require	O
a	O
complete	O
understanding	O
of	O
the	O
entire	O
structure	O
of	O
1	O
a	O
natural	O
image	O
is	O
an	O
image	O
that	O
might	O
be	O
captured	O
by	O
a	O
camera	O
in	O
a	O
reasonably	O
ordinary	O
environment	O
,	O
as	O
opposed	O
to	O
a	O
synthetically	O
rendered	O
image	O
,	O
a	O
screenshot	O
of	O
a	O
web	O
page	O
,	O
etc	O
.	O
559	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
the	O
input	O
,	O
with	O
no	O
option	O
to	O
ignore	O
sections	O
of	O
it	O
.	O
these	O
tasks	O
include	O
the	O
following	O
:	O
•	O
•	O
•	O
•	O
density	O
estimation	O
:	O
given	O
an	O
input	O
x	O
,	O
the	O
machine	O
learning	O
system	O
returns	O
an	O
estimate	O
of	O
the	O
true	O
density	O
p	O
(	O
x	O
)	O
under	O
the	O
data	O
generating	O
distribution	O
.	O
this	O
requires	O
only	O
a	O
single	O
output	O
,	O
but	O
it	O
does	O
require	O
a	O
complete	O
understand-	O
ing	O
of	O
the	O
entire	O
input	O
.	O
if	O
even	O
one	O
element	O
of	O
the	O
vector	O
is	O
unusual	O
,	O
the	O
system	O
must	O
assign	O
it	O
a	O
low	O
probability	O
.	O
denoising	O
:	O
given	O
a	O
damaged	O
or	O
incorrectly	O
observed	O
input	O
˜x	O
,	O
the	O
machine	O
learning	O
system	O
returns	O
an	O
estimate	O
of	O
the	O
original	O
or	O
correct	O
x.	O
for	O
example	O
,	O
the	O
machine	O
learning	O
system	O
might	O
be	O
asked	O
to	O
remove	O
dust	O
or	O
scratches	O
from	O
an	O
old	O
photograph	O
.	O
this	O
requires	O
multiple	O
outputs	O
(	O
every	O
element	O
of	O
the	O
estimated	O
clean	O
example	O
x	O
)	O
and	O
an	O
understanding	O
of	O
the	O
entire	O
input	O
(	O
since	O
even	O
one	O
damaged	O
area	O
will	O
still	O
reveal	O
the	O
ﬁnal	O
estimate	O
as	O
being	O
damaged	O
)	O
.	O
missing	O
value	O
imputation	O
:	O
given	O
the	O
observations	O
of	O
some	O
elements	O
of	O
x	O
,	O
the	O
model	B
is	O
asked	O
to	O
return	O
estimates	O
of	O
or	O
a	O
probability	O
distribution	O
over	O
some	O
or	O
all	O
of	O
the	O
unobserved	O
elements	O
of	O
x.	O
this	O
requires	O
multiple	O
outputs	O
.	O
because	O
the	O
model	B
could	O
be	O
asked	O
to	O
restore	O
any	O
of	O
the	O
elements	O
of	O
x	O
,	O
it	O
must	O
understand	O
the	O
entire	O
input	O
.	O
sampling	O
:	O
the	O
model	B
generates	O
new	O
samples	O
from	O
the	O
distribution	O
p	O
(	O
x	O
)	O
.	O
applications	O
include	O
speech	O
synthesis	O
,	O
i.e	O
.	O
producing	O
new	O
waveforms	O
that	O
sound	O
like	O
natural	O
human	O
speech	O
.	O
this	O
requires	O
multiple	O
output	O
values	O
and	O
a	O
good	O
model	B
of	O
the	O
entire	O
input	O
.	O
if	O
the	O
samples	O
have	O
even	O
one	O
element	O
drawn	O
from	O
the	O
wrong	O
distribution	O
,	O
then	O
the	O
sampling	O
process	O
is	O
wrong	O
.	O
for	O
an	O
example	O
of	O
a	O
sampling	O
task	O
using	O
small	O
natural	O
images	O
,	O
see	O
ﬁgure	O
16.1	O
.	O
modeling	O
a	O
rich	O
distribution	O
over	O
thousands	O
or	O
millions	O
of	O
random	O
variables	O
is	O
a	O
challenging	O
task	O
,	O
both	O
computationally	O
and	O
statistically	O
.	O
suppose	O
we	O
only	O
wanted	O
to	O
model	B
binary	O
variables	O
.	O
this	O
is	O
the	O
simplest	O
possible	O
case	O
,	O
and	O
yet	O
already	O
it	O
3072	O
seems	O
overwhelming	O
.	O
for	O
a	O
small	O
,	O
32	O
possible	O
binary	O
images	O
of	O
this	O
form	O
.	O
this	O
number	O
is	O
over	O
10800	O
times	O
larger	O
than	O
the	O
estimated	O
number	O
of	O
atoms	O
in	O
the	O
universe	O
.	O
pixel	O
color	O
(	O
rgb	O
)	O
image	O
,	O
there	O
are	O
2	O
×	O
32	O
in	O
general	O
,	O
if	O
we	O
wish	O
to	O
model	B
a	O
distribution	O
over	O
a	O
random	O
vector	O
x	O
containing	O
n	O
discrete	O
variables	O
capable	O
of	O
taking	O
on	O
k	O
values	O
each	O
,	O
then	O
the	O
naive	O
approach	O
of	O
representing	O
p	O
(	O
x	O
)	O
by	O
storing	O
a	O
lookup	O
table	O
with	O
one	O
probability	O
value	O
per	O
possible	O
outcome	O
requires	O
kn	O
parameters	O
!	O
this	O
is	O
not	O
feasible	O
for	O
several	O
reasons	O
:	O
560	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
32	O
pixel	O
color	O
figure	O
16.1	O
:	O
probabilistic	O
modeling	O
of	O
natural	O
images	O
.	O
(	O
top	O
)	O
example	O
32	O
images	O
from	O
the	O
cifar-10	O
dataset	O
(	O
samples	O
drawn	O
from	O
a	O
structured	O
probabilistic	O
model	B
trained	O
on	O
this	O
dataset	O
.	O
each	O
sample	O
appears	O
at	O
the	O
same	O
position	B
in	O
the	O
grid	O
as	O
the	O
training	O
example	O
that	O
is	O
closest	O
to	O
it	O
in	O
euclidean	O
space	O
.	O
this	O
comparison	O
allows	O
us	O
to	O
see	O
that	O
the	O
model	B
is	O
truly	O
synthesizing	O
new	O
images	O
,	O
rather	O
than	O
memorizing	O
the	O
training	O
data	O
.	O
contrast	O
of	O
both	O
sets	O
of	O
images	O
has	O
been	O
adjusted	O
for	O
display	O
.	O
figure	O
reproduced	O
with	O
permission	O
from	O
krizhevsky	O
and	O
hinton	O
2009	O
(	O
bottom	O
)	O
)	O
.	O
courville	O
et	O
al	O
.	O
2011	O
(	O
)	O
.	O
×	O
,	O
561	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
•	O
•	O
•	O
•	O
memory	O
:	O
the	O
cost	O
of	O
storing	O
the	O
representation	O
:	O
for	O
all	O
but	O
very	O
small	O
values	O
of	O
n	O
and	O
k	O
,	O
representing	O
the	O
distribution	O
as	O
a	O
table	O
will	O
require	O
too	O
many	O
values	O
to	O
store	O
.	O
statistical	O
eﬃciency	O
:	O
as	O
the	O
number	O
of	O
parameters	O
in	O
a	O
model	B
increases	O
,	O
so	O
does	O
the	O
amount	O
of	O
training	O
data	O
needed	O
to	O
choose	O
the	O
values	O
of	O
those	O
parameters	O
using	O
a	O
statistical	O
estimator	O
.	O
because	O
the	O
table-based	O
model	B
has	O
an	O
astronomical	O
number	O
of	O
parameters	O
,	O
it	O
will	O
require	O
an	O
astronomically	O
large	O
training	O
set	O
to	O
ﬁt	O
accurately	O
.	O
any	O
such	O
model	B
will	O
overﬁt	O
the	O
training	O
set	O
very	O
badly	O
unless	O
additional	O
assumptions	O
are	O
made	O
linking	O
the	O
diﬀerent	O
entries	O
in	O
the	O
table	O
(	O
for	O
example	O
,	O
like	O
in	O
back-oﬀ	O
or	O
smoothed	O
n-gram	B
models	O
,	O
section	O
12.4.1	O
)	O
.	O
runtime	O
:	O
the	O
cost	O
of	O
inference	O
:	O
suppose	O
we	O
want	O
to	O
perform	O
an	O
inference	O
task	O
where	O
we	O
use	O
our	O
model	B
of	O
the	O
joint	O
distribution	O
p	O
(	O
x	O
)	O
to	O
compute	O
some	O
other	O
distribution	O
,	O
such	O
as	O
the	O
marginal	O
distribution	O
p	O
(	O
x1	O
)	O
or	O
the	O
conditional	O
x1	O
)	O
.	O
computing	O
these	O
distributions	O
will	O
require	O
summing	O
distribution	O
p	O
(	O
x2	O
across	O
the	O
entire	O
table	O
,	O
so	O
the	O
runtime	O
of	O
these	O
operations	O
is	O
as	O
high	O
as	O
the	O
intractable	O
memory	O
cost	O
of	O
storing	O
the	O
model	B
.	O
|	O
runtime	O
:	O
the	O
cost	O
of	O
sampling	O
:	O
likewise	O
,	O
suppose	O
we	O
want	O
to	O
draw	O
a	O
sample	O
u	O
(	O
0	O
,	O
1	O
)	O
,	O
from	O
the	O
model	B
.	O
the	O
naive	O
way	O
to	O
do	O
this	O
is	O
to	O
sample	O
some	O
value	O
u	O
then	O
iterate	O
through	O
the	O
table	O
,	O
adding	O
up	O
the	O
probability	O
values	O
until	O
they	O
exceed	O
u	O
and	O
return	O
the	O
outcome	O
corresponding	O
to	O
that	O
position	B
in	O
the	O
table	O
.	O
this	O
requires	O
reading	O
through	O
the	O
whole	O
table	O
in	O
the	O
worst	O
case	O
,	O
so	O
it	O
has	O
the	O
same	O
exponential	O
cost	O
as	O
the	O
other	O
operations	O
.	O
∼	O
the	O
problem	O
with	O
the	O
table-based	O
approach	O
is	O
that	O
we	O
are	O
explicitly	O
modeling	O
every	O
possible	O
kind	O
of	O
interaction	O
between	O
every	O
possible	O
subset	O
of	O
variables	O
.	O
the	O
probability	O
distributions	O
we	O
encounter	O
in	O
real	O
tasks	O
are	O
much	O
simpler	O
than	O
this	O
.	O
usually	O
,	O
most	O
variables	O
inﬂuence	O
each	O
other	O
only	O
indirectly	O
.	O
for	O
example	O
,	O
consider	O
modeling	O
the	O
ﬁnishing	O
times	O
of	O
a	O
team	O
in	O
a	O
relay	O
race	O
.	O
suppose	O
the	O
team	O
consists	O
of	O
three	O
runners	O
:	O
alice	O
,	O
bob	O
and	O
carol	O
.	O
at	O
the	O
start	O
of	O
the	O
race	O
,	O
alice	O
carries	O
a	O
baton	O
and	O
begins	O
running	O
around	O
a	O
track	O
.	O
after	O
completing	O
her	O
lap	O
around	O
the	O
track	O
,	O
she	O
hands	O
the	O
baton	O
to	O
bob	O
.	O
bob	O
then	O
runs	O
his	O
own	O
lap	O
and	O
hands	O
the	O
baton	O
to	O
carol	O
,	O
who	O
runs	O
the	O
ﬁnal	O
lap	O
.	O
we	O
can	O
model	B
each	O
of	O
their	O
ﬁnishing	O
times	O
as	O
a	O
continuous	O
random	O
variable	O
.	O
alice	O
’	O
s	O
ﬁnishing	O
time	O
does	O
not	O
depend	O
on	O
anyone	O
else	O
’	O
s	O
,	O
since	O
she	O
goes	O
ﬁrst	O
.	O
bob	O
’	O
s	O
ﬁnishing	O
time	O
depends	O
on	O
alice	O
’	O
s	O
,	O
because	O
bob	O
does	O
not	O
have	O
the	O
opportunity	O
to	O
start	O
his	O
lap	O
until	O
alice	O
has	O
completed	O
hers	O
.	O
if	O
alice	O
ﬁnishes	O
faster	O
,	O
bob	O
will	O
ﬁnish	O
faster	O
,	O
all	O
else	O
being	O
562	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
equal	O
.	O
finally	O
,	O
carol	O
’	O
s	O
ﬁnishing	O
time	O
depends	O
on	O
both	O
her	O
teammates	O
.	O
if	O
alice	O
is	O
slow	O
,	O
bob	O
will	O
probably	O
ﬁnish	O
late	O
too	O
.	O
as	O
a	O
consequence	O
,	O
carol	O
will	O
have	O
quite	O
a	O
late	O
starting	O
time	O
and	O
thus	O
is	O
likely	O
to	O
have	O
a	O
late	O
ﬁnishing	O
time	O
as	O
well	O
.	O
however	O
,	O
carol	O
’	O
s	O
ﬁnishing	O
time	O
depends	O
only	O
indirectly	O
on	O
alice	O
’	O
s	O
ﬁnishing	O
time	O
via	O
bob	O
’	O
s	O
.	O
if	O
we	O
already	O
know	O
bob	O
’	O
s	O
ﬁnishing	O
time	O
,	O
we	O
will	O
not	O
be	O
able	O
to	O
estimate	O
carol	O
’	O
s	O
ﬁnishing	O
time	O
better	O
by	O
ﬁnding	O
out	O
what	O
alice	O
’	O
s	O
ﬁnishing	O
time	O
was	O
.	O
this	O
means	O
we	O
can	O
model	B
the	O
relay	O
race	O
using	O
only	O
two	O
interactions	O
:	O
alice	O
’	O
s	O
eﬀect	O
on	O
bob	O
and	O
bob	O
’	O
s	O
eﬀect	O
on	O
carol	O
.	O
we	O
can	O
omit	O
the	O
third	O
,	O
indirect	O
interaction	O
between	O
alice	O
and	O
carol	O
from	O
our	O
model	B
.	O
structured	O
probabilistic	O
models	O
provide	O
a	O
formal	O
framework	O
for	O
modeling	O
only	O
direct	O
interactions	O
between	O
random	O
variables	O
.	O
this	O
allows	O
the	O
models	O
to	O
have	O
signiﬁcantly	O
fewer	O
parameters	O
and	O
therefore	O
be	O
estimated	O
reliably	O
from	O
less	O
data	O
.	O
these	O
smaller	O
models	O
also	O
have	O
dramatically	O
reduced	O
computational	O
cost	O
in	O
terms	O
of	O
storing	O
the	O
model	B
,	O
performing	O
inference	O
in	O
the	O
model	B
,	O
and	O
drawing	O
samples	O
from	O
the	O
model	B
.	O
16.2	O
using	O
graphs	O
to	O
describe	O
model	B
structure	O
structured	O
probabilistic	O
models	O
use	O
graphs	O
(	O
in	O
the	O
graph	O
theory	O
sense	O
of	O
“	O
nodes	O
”	O
or	O
“	O
vertices	O
”	O
connected	O
by	O
edges	O
)	O
to	O
represent	O
interactions	O
between	O
random	O
variables	O
.	O
each	O
node	O
represents	O
a	O
random	O
variable	O
.	O
each	O
edge	O
represents	O
a	O
direct	O
interaction	O
.	O
these	O
direct	O
interactions	O
imply	O
other	O
,	O
indirect	O
interactions	O
,	O
but	O
only	O
the	O
direct	O
interactions	O
need	O
to	O
be	O
explicitly	O
modeled	O
.	O
there	O
is	O
more	O
than	O
one	O
way	O
to	O
describe	O
the	O
interactions	O
in	O
a	O
probability	O
distribution	O
using	O
a	O
graph	O
.	O
in	O
the	O
following	O
sections	O
we	O
describe	O
some	O
of	O
the	O
most	O
popular	O
and	O
useful	O
approaches	O
.	O
graphical	O
models	O
can	O
be	O
largely	O
divided	O
into	O
two	O
categories	O
:	O
models	O
based	O
on	O
directed	O
acyclic	O
graphs	O
,	O
and	O
models	O
based	O
on	O
undirected	O
graphs	O
.	O
16.2.1	O
directed	O
models	O
one	O
kind	O
of	O
structured	O
probabilistic	O
model	B
is	O
the	O
directed	O
graphical	O
model	B
,	O
otherwise	O
known	O
as	O
the	O
belief	O
network	O
bayesian	O
network	O
2	O
(	O
pearl	O
1985	O
or	O
,	O
)	O
.	O
directed	O
graphical	O
models	O
are	O
called	O
“	O
directed	O
”	O
because	O
their	O
edges	O
are	O
directed	O
,	O
2	O
judea	O
pearl	O
suggested	O
using	O
the	O
term	O
“	O
bayesian	O
network	O
”	O
when	O
one	O
wishes	O
to	O
“	O
emphasize	O
the	O
judgmental	O
”	O
nature	O
of	O
the	O
values	O
computed	O
by	O
the	O
network	O
,	O
i.e	O
.	O
to	O
highlight	O
that	O
they	O
usually	O
represent	O
degrees	O
of	O
belief	O
rather	O
than	O
frequencies	O
of	O
events	O
.	O
563	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
alice	O
bob	O
carol	O
t0t0	O
t1t1	O
t2t2	O
figure	O
16.2	O
:	O
a	O
directed	O
graphical	O
model	B
depicting	O
the	O
relay	O
race	O
example	O
.	O
alice	O
’	O
s	O
ﬁnishing	O
time	O
t0	O
inﬂuences	O
bob	O
’	O
s	O
ﬁnishing	O
time	O
t1	O
,	O
because	O
bob	O
does	O
not	O
get	O
to	O
start	O
running	O
until	O
alice	O
ﬁnishes	O
.	O
likewise	O
,	O
carol	O
only	O
gets	O
to	O
start	O
running	O
after	O
bob	O
ﬁnishes	O
,	O
so	O
bob	O
’	O
s	O
ﬁnishing	O
time	O
t1	O
directly	O
inﬂuences	O
carol	O
’	O
s	O
ﬁnishing	O
time	O
t2	O
.	O
that	O
is	O
,	O
they	O
point	O
from	O
one	O
vertex	O
to	O
another	O
.	O
this	O
direction	O
is	O
represented	O
in	O
the	O
drawing	O
with	O
an	O
arrow	O
.	O
the	O
direction	O
of	O
the	O
arrow	O
indicates	O
which	O
variable	O
’	O
s	O
probability	O
distribution	O
is	O
deﬁned	O
in	O
terms	O
of	O
the	O
other	O
’	O
s	O
.	O
drawing	O
an	O
arrow	O
from	O
a	O
to	O
b	O
means	O
that	O
we	O
deﬁne	O
the	O
probability	O
distribution	O
over	O
b	O
via	O
a	O
conditional	O
distribution	O
,	O
with	O
a	O
as	O
one	O
of	O
the	O
variables	O
on	O
the	O
right	O
side	O
of	O
the	O
conditioning	O
bar	O
.	O
in	O
other	O
words	O
,	O
the	O
distribution	O
over	O
b	O
depends	O
on	O
the	O
value	O
of	O
a.	O
continuing	O
with	O
the	O
relay	O
race	O
example	O
from	O
section	O
,	O
suppose	O
we	O
name	O
alice	O
’	O
s	O
ﬁnishing	O
time	O
t0	O
,	O
bob	O
’	O
s	O
ﬁnishing	O
time	O
t1	O
,	O
and	O
carol	O
’	O
s	O
ﬁnishing	O
time	O
t2	O
.	O
as	O
we	O
saw	O
earlier	O
,	O
our	O
estimate	O
of	O
t	O
1	O
depends	O
on	O
t0	O
.	O
our	O
estimate	O
of	O
t2	O
depends	O
directly	O
on	O
t1	O
but	O
only	O
indirectly	O
on	O
t0	O
.	O
we	O
can	O
draw	O
this	O
relationship	O
in	O
a	O
directed	O
graphical	O
model	B
,	O
illustrated	O
in	O
ﬁgure	O
16.2	O
.	O
16.1	O
g	O
formally	O
,	O
a	O
directed	O
graphical	O
model	B
deﬁned	O
on	O
variables	O
x	O
is	O
deﬁned	O
by	O
a	O
whose	O
vertices	O
are	O
the	O
random	O
variables	O
in	O
the	O
model	B
,	O
p	O
ag	O
(	O
xi	O
)	O
)	O
where	O
.	O
the	O
probability	O
distribution	O
over	O
x	O
is	O
given	O
directed	O
acyclic	O
graph	O
and	O
a	O
set	O
of	O
local	O
conditional	O
probability	O
distributions	O
p	O
(	O
xi	O
p	O
ag	O
(	O
xi	O
)	O
gives	O
the	O
parents	O
of	O
xi	O
in	O
by	O
g	O
|	O
p	O
(	O
)	O
=	O
π	O
x	O
i	O
p	O
(	O
xi	O
|	O
p	O
ag	O
(	O
xi	O
)	O
)	O
.	O
(	O
16.1	O
)	O
in	O
our	O
relay	O
race	O
example	O
,	O
this	O
means	O
that	O
,	O
using	O
the	O
graph	O
drawn	O
in	O
ﬁgure	O
,	O
16.2	O
p	O
(	O
t0	O
,	O
t1	O
,	O
t2	O
)	O
=	O
(	O
p	O
t0	O
)	O
(	O
p	O
t1	O
t0	O
)	O
(	O
p	O
t2	O
t1	O
)	O
.	O
(	O
16.2	O
)	O
|	O
|	O
this	O
is	O
our	O
ﬁrst	O
time	O
seeing	O
a	O
structured	O
probabilistic	O
model	B
in	O
action	O
.	O
we	O
can	O
examine	O
the	O
cost	O
of	O
using	O
it	O
,	O
in	O
order	O
to	O
observe	O
how	O
structured	O
modeling	O
has	O
many	O
advantages	O
relative	O
to	O
unstructured	O
modeling	O
.	O
suppose	O
we	O
represented	O
time	O
by	O
discretizing	O
time	O
ranging	O
from	O
minute	O
0	O
to	O
minute	O
10	O
into	O
6	O
second	O
chunks	O
.	O
this	O
would	O
make	O
t0	O
,	O
t1	O
and	O
t2	O
each	O
be	O
a	O
discrete	O
×	O
variable	O
with	O
100	O
possible	O
values	O
.	O
if	O
we	O
attempted	O
to	O
represent	O
p	O
(	O
t0	O
,	O
t1	O
,	O
t2	O
)	O
with	O
a	O
table	O
,	O
it	O
would	O
need	O
to	O
store	O
999,999	O
values	O
(	O
100	O
values	O
of	O
t0	O
100	O
values	O
of	O
t2	O
,	O
minus	O
1	O
,	O
since	O
the	O
probability	O
of	O
one	O
of	O
the	O
conﬁgurations	O
is	O
made	O
100	O
values	O
of	O
t1	O
×	O
564	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
redundant	O
by	O
the	O
constraint	O
that	O
the	O
sum	O
of	O
the	O
probabilities	O
be	O
1	O
)	O
.	O
if	O
instead	O
,	O
we	O
only	O
make	O
a	O
table	O
for	O
each	O
of	O
the	O
conditional	O
probability	O
distributions	O
,	O
then	O
the	O
distribution	O
over	O
t0	O
requires	O
99	O
values	O
,	O
the	O
table	O
deﬁning	O
t1	O
given	O
t0	O
requires	O
9900	O
values	O
,	O
and	O
so	O
does	O
the	O
table	O
deﬁning	O
t2	O
given	O
t1	O
.	O
this	O
comes	O
to	O
a	O
total	O
of	O
19,899	O
values	O
.	O
this	O
means	O
that	O
using	O
the	O
directed	O
graphical	O
model	B
reduced	O
our	O
number	O
of	O
parameters	O
by	O
a	O
factor	O
of	O
more	O
than	O
50	O
!	O
in	O
general	O
,	O
to	O
model	B
n	O
discrete	O
variables	O
each	O
having	O
k	O
values	O
,	O
the	O
cost	O
of	O
the	O
single	O
table	O
approach	O
scales	O
like	O
o	O
(	O
k	O
n	O
)	O
,	O
as	O
we	O
have	O
observed	O
before	O
.	O
now	O
suppose	O
we	O
build	O
a	O
directed	O
graphical	O
model	B
over	O
these	O
variables	O
.	O
if	O
m	O
is	O
the	O
maximum	O
number	O
of	O
variables	O
appearing	O
(	O
on	O
either	O
side	O
of	O
the	O
conditioning	O
bar	O
)	O
in	O
a	O
single	O
conditional	O
probability	O
distribution	O
,	O
then	O
the	O
cost	O
of	O
the	O
tables	O
for	O
the	O
directed	O
model	B
scales	O
like	O
o	O
(	O
km	O
)	O
.	O
as	O
long	O
as	O
we	O
can	O
design	O
a	O
model	B
such	O
that	O
m	O
<	O
<	O
n	O
,	O
we	O
get	O
very	O
dramatic	O
savings	O
.	O
in	O
other	O
words	O
,	O
so	O
long	O
as	O
each	O
variable	O
has	O
few	O
parents	O
in	O
the	O
graph	O
,	O
the	O
distribution	O
can	O
be	O
represented	O
with	O
very	O
few	O
parameters	O
.	O
some	O
restrictions	O
on	O
the	O
graph	O
structure	O
,	O
such	O
as	O
requiring	O
it	O
to	O
be	O
a	O
tree	O
,	O
can	O
also	O
guarantee	O
that	O
operations	O
like	O
computing	O
marginal	O
or	O
conditional	O
distributions	O
over	O
subsets	O
of	O
variables	O
are	O
eﬃcient	O
.	O
it	O
is	O
important	O
to	O
realize	O
what	O
kinds	O
of	O
information	O
can	O
and	O
can	O
not	O
be	O
encoded	O
in	O
the	O
graph	O
.	O
the	O
graph	O
encodes	O
only	O
simplifying	O
assumptions	O
about	O
which	O
variables	O
are	O
conditionally	O
independent	O
from	O
each	O
other	O
.	O
it	O
is	O
also	O
possible	O
to	O
make	O
other	O
kinds	O
of	O
simplifying	O
assumptions	O
.	O
for	O
example	O
,	O
suppose	O
we	O
assume	O
bob	O
always	O
runs	O
the	O
same	O
regardless	O
of	O
how	O
alice	O
performed	O
.	O
(	O
in	O
reality	O
,	O
alice	O
’	O
s	O
performance	O
probably	O
inﬂuences	O
bob	O
’	O
s	O
performance—depending	O
on	O
bob	O
’	O
s	O
personality	O
,	O
if	O
alice	O
runs	O
especially	O
fast	O
in	O
a	O
given	O
race	O
,	O
this	O
might	O
encourage	O
bob	O
to	O
push	O
hard	O
and	O
match	O
her	O
exceptional	O
performance	O
,	O
or	O
it	O
might	O
make	O
him	O
overconﬁdent	O
and	O
lazy	O
)	O
.	O
then	O
the	O
only	O
eﬀect	O
alice	O
has	O
on	O
bob	O
’	O
s	O
ﬁnishing	O
time	O
is	O
that	O
we	O
must	O
add	O
alice	O
’	O
s	O
ﬁnishing	O
time	O
to	O
the	O
total	O
amount	O
of	O
time	O
we	O
think	O
bob	O
needs	O
to	O
run	O
.	O
this	O
observation	O
allows	O
us	O
to	O
deﬁne	O
a	O
model	B
with	O
o	O
(	O
k	O
)	O
parameters	O
instead	O
of	O
o	O
(	O
k	O
2	O
)	O
.	O
however	O
,	O
note	O
that	O
t0	O
and	O
t1	O
are	O
still	O
directly	O
dependent	O
with	O
this	O
assumption	O
,	O
because	O
t1	O
represents	O
the	O
absolute	O
time	O
at	O
which	O
bob	O
ﬁnishes	O
,	O
not	O
the	O
total	O
time	O
he	O
himself	O
spends	O
running	O
.	O
this	O
means	O
our	O
graph	O
must	O
still	O
contain	O
an	O
arrow	O
from	O
t0	O
to	O
t1	O
.	O
the	O
assumption	O
that	O
bob	O
’	O
s	O
personal	O
running	O
time	O
is	O
independent	O
from	O
all	O
other	O
factors	O
can	O
not	O
be	O
encoded	O
in	O
a	O
graph	O
over	O
t0	O
,	O
t1	O
,	O
and	O
t2	O
.	O
instead	O
,	O
we	O
encode	O
this	O
information	O
in	O
the	O
deﬁnition	O
of	O
the	O
conditional	O
distribution	O
itself	O
.	O
the	O
conditional	O
distribution	O
is	O
no	O
longer	O
a	O
k	O
1	O
element	O
table	O
indexed	O
by	O
t0	O
and	O
t1	O
but	O
is	O
now	O
a	O
slightly	O
more	O
complicated	O
formula	O
using	O
only	O
k	O
1	O
parameters	O
.	O
the	O
directed	O
graphical	O
model	B
syntax	O
does	O
not	O
place	O
any	O
constraint	O
on	O
how	O
we	O
deﬁne	O
×	O
−	O
−	O
k	O
565	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
our	O
conditional	O
distributions	O
.	O
it	O
only	O
deﬁnes	O
which	O
variables	O
they	O
are	O
allowed	O
to	O
take	O
in	O
as	O
arguments	O
.	O
16.2.2	O
undirected	O
models	O
directed	O
graphical	O
models	O
give	O
us	O
one	O
language	O
for	O
describing	O
structured	O
probabilis-	O
tic	O
models	O
.	O
another	O
popular	O
language	O
is	O
that	O
of	O
undirected	O
models	O
,	O
otherwise	O
known	O
as	O
markov	O
random	O
ﬁelds	O
(	O
mrfs	O
)	O
or	O
markov	O
networks	O
(	O
kinder-	O
mann	O
1980	O
)	O
.	O
as	O
their	O
name	O
implies	O
,	O
undirected	O
models	O
use	O
graphs	O
whose	O
edges	O
are	O
undirected	O
.	O
,	O
directed	O
models	O
are	O
most	O
naturally	O
applicable	O
to	O
situations	O
where	O
there	O
is	O
a	O
clear	O
reason	O
to	O
draw	O
each	O
arrow	O
in	O
one	O
particular	O
direction	O
.	O
often	O
these	O
are	O
situations	O
where	O
we	O
understand	O
the	O
causality	O
and	O
the	O
causality	O
only	O
ﬂows	O
in	O
one	O
direction	O
.	O
one	O
such	O
situation	O
is	O
the	O
relay	O
race	O
example	O
.	O
earlier	O
runners	O
aﬀect	O
the	O
ﬁnishing	O
times	O
of	O
later	O
runners	O
;	O
later	O
runners	O
do	O
not	O
aﬀect	O
the	O
ﬁnishing	O
times	O
of	O
earlier	O
runners	O
.	O
not	O
all	O
situations	O
we	O
might	O
want	O
to	O
model	B
have	O
such	O
a	O
clear	O
direction	O
to	O
their	O
interactions	O
.	O
when	O
the	O
interactions	O
seem	O
to	O
have	O
no	O
intrinsic	O
direction	O
,	O
or	O
to	O
operate	O
in	O
both	O
directions	O
,	O
it	O
may	O
be	O
more	O
appropriate	O
to	O
use	O
an	O
undirected	O
model	B
.	O
as	O
an	O
example	O
of	O
such	O
a	O
situation	O
,	O
suppose	O
we	O
want	O
to	O
model	B
a	O
distribution	O
over	O
three	O
binary	O
variables	O
:	O
whether	O
or	O
not	O
you	O
are	O
sick	O
,	O
whether	O
or	O
not	O
your	O
coworker	O
is	O
sick	O
,	O
and	O
whether	O
or	O
not	O
your	O
roommate	O
is	O
sick	O
.	O
as	O
in	O
the	O
relay	O
race	O
example	O
,	O
we	O
can	O
make	O
simplifying	O
assumptions	O
about	O
the	O
kinds	O
of	O
interactions	O
that	O
take	O
place	O
.	O
assuming	O
that	O
your	O
coworker	O
and	O
your	O
roommate	O
do	O
not	O
know	O
each	O
other	O
,	O
it	O
is	O
very	O
unlikely	O
that	O
one	O
of	O
them	O
will	O
give	O
the	O
other	O
an	O
infection	O
such	O
as	O
a	O
cold	O
directly	O
.	O
this	O
event	O
can	O
be	O
seen	O
as	O
so	O
rare	O
that	O
it	O
is	O
acceptable	O
not	O
to	O
model	B
it	O
.	O
however	O
,	O
it	O
is	O
reasonably	O
likely	O
that	O
either	O
of	O
them	O
could	O
give	O
you	O
a	O
cold	O
,	O
and	O
that	O
you	O
could	O
pass	O
it	O
on	O
to	O
the	O
other	O
.	O
we	O
can	O
model	B
the	O
indirect	O
transmission	O
of	O
a	O
cold	O
from	O
your	O
coworker	O
to	O
your	O
roommate	O
by	O
modeling	O
the	O
transmission	O
of	O
the	O
cold	O
from	O
your	O
coworker	O
to	O
you	O
and	O
the	O
transmission	O
of	O
the	O
cold	O
from	O
you	O
to	O
your	O
roommate	O
.	O
in	O
this	O
case	O
,	O
it	O
is	O
just	O
as	O
easy	O
for	O
you	O
to	O
cause	O
your	O
roommate	O
to	O
get	O
sick	O
as	O
it	O
is	O
for	O
your	O
roommate	O
to	O
make	O
you	O
sick	O
,	O
so	O
there	O
is	O
not	O
a	O
clean	O
,	O
uni-directional	O
narrative	O
on	O
which	O
to	O
base	O
the	O
model	B
.	O
this	O
motivates	O
using	O
an	O
undirected	O
model	B
.	O
as	O
with	O
directed	O
models	O
,	O
if	O
two	O
nodes	O
in	O
an	O
undirected	O
model	B
are	O
connected	O
by	O
an	O
edge	O
,	O
then	O
the	O
random	O
variables	O
corresponding	O
to	O
those	O
nodes	O
interact	O
with	O
each	O
other	O
directly	O
.	O
unlike	O
directed	O
models	O
,	O
the	O
edge	O
in	O
an	O
undirected	O
model	B
has	O
no	O
arrow	O
,	O
and	O
is	O
not	O
associated	O
with	O
a	O
conditional	O
probability	O
distribution	O
.	O
566	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
hrhr	O
hyhy	O
hchc	O
figure	O
16.3	O
:	O
an	O
undirected	O
graph	O
representing	O
how	O
your	O
roommate	O
’	O
s	O
health	O
hr	O
,	O
your	O
health	O
hy	O
,	O
and	O
your	O
work	B
colleague	O
’	O
s	O
health	O
hc	O
aﬀect	O
each	O
other	O
.	O
you	O
and	O
your	O
roommate	O
might	O
infect	O
each	O
other	O
with	O
a	O
cold	O
,	O
and	O
you	O
and	O
your	O
work	B
colleague	O
might	O
do	O
the	O
same	O
,	O
but	O
assuming	O
that	O
your	O
roommate	O
and	O
your	O
colleague	O
do	O
not	O
know	O
each	O
other	O
,	O
they	O
can	O
only	O
infect	O
each	O
other	O
indirectly	O
via	O
you	O
.	O
we	O
denote	O
the	O
random	O
variable	O
representing	O
your	O
health	O
as	O
hy	O
,	O
the	O
random	O
variable	O
representing	O
your	O
roommate	O
’	O
s	O
health	O
as	O
hr	O
,	O
and	O
the	O
random	O
variable	O
representing	O
your	O
colleague	O
’	O
s	O
health	O
as	O
hc	O
.	O
see	O
ﬁgure	O
for	O
a	O
drawing	O
of	O
the	O
graph	O
representing	O
this	O
scenario	O
.	O
g	O
c	O
formally	O
,	O
an	O
undirected	O
graphical	O
model	B
is	O
a	O
structured	O
probabilistic	O
model	B
in	O
the	O
graph,3	O
a	O
factor	O
φ	O
(	O
)	O
deﬁned	O
on	O
an	O
undirected	O
graph	O
(	O
also	O
called	O
a	O
clique	O
potential	O
)	O
measures	O
the	O
aﬃnity	O
of	O
the	O
variables	O
in	O
that	O
clique	O
for	O
being	O
in	O
each	O
of	O
their	O
possible	O
joint	O
states	O
.	O
the	O
factors	O
are	O
constrained	O
to	O
be	O
non-negative	O
.	O
together	O
they	O
deﬁne	O
an	O
unnormalized	O
probability	O
distribution	O
.	O
for	O
each	O
clique	O
16.3	O
c	O
c	O
(	O
)	O
.	O
c∈g	O
φ	O
˜p	O
(	O
)	O
=	O
π	O
x	O
(	O
16.3	O
)	O
the	O
unnormalized	O
probability	O
distribution	O
is	O
eﬃcient	O
to	O
work	B
with	O
so	O
long	O
as	O
all	O
the	O
cliques	O
are	O
small	O
.	O
it	O
encodes	O
the	O
idea	O
that	O
states	O
with	O
higher	O
aﬃnity	O
are	O
more	O
likely	O
.	O
however	O
,	O
unlike	O
in	O
a	O
bayesian	O
network	O
,	O
there	O
is	O
little	O
structure	O
to	O
the	O
deﬁnition	O
of	O
the	O
cliques	O
,	O
so	O
there	O
is	O
nothing	O
to	O
guarantee	O
that	O
multiplying	O
them	O
together	O
will	O
yield	O
a	O
valid	O
probability	O
distribution	O
.	O
see	O
ﬁgure	O
for	O
an	O
example	O
of	O
reading	O
factorization	O
information	O
from	O
an	O
undirected	O
graph	O
.	O
16.4	O
our	O
example	O
of	O
the	O
cold	O
spreading	O
between	O
you	O
,	O
your	O
roommate	O
,	O
and	O
your	O
colleague	O
contains	O
two	O
cliques	O
.	O
one	O
clique	O
contains	O
h	O
y	O
and	O
hc	O
.	O
the	O
factor	O
for	O
this	O
clique	O
can	O
be	O
deﬁned	O
by	O
a	O
table	O
,	O
and	O
might	O
have	O
values	O
resembling	O
these	O
:	O
hy	O
=	O
0	O
hy	O
=	O
1	O
hc	O
=	O
0	O
hc	O
=	O
1	O
2	O
1	O
1	O
10	O
3a	O
clique	O
of	O
the	O
graph	O
is	O
a	O
subset	O
of	O
nodes	O
that	O
are	O
all	O
connected	O
to	O
each	O
other	O
by	O
an	O
edge	O
of	O
the	O
graph	O
.	O
567	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
a	O
state	O
of	O
1	O
indicates	O
good	O
health	O
,	O
while	O
a	O
state	O
of	O
0	O
indicates	O
poor	O
health	O
(	O
having	O
been	O
infected	O
with	O
a	O
cold	O
)	O
.	O
both	O
of	O
you	O
are	O
usually	O
healthy	O
,	O
so	O
the	O
corresponding	O
state	O
has	O
the	O
highest	O
aﬃnity	O
.	O
the	O
state	O
where	O
only	O
one	O
of	O
you	O
is	O
sick	O
has	O
the	O
lowest	O
aﬃnity	O
,	O
because	O
this	O
is	O
a	O
rare	O
state	O
.	O
the	O
state	O
where	O
both	O
of	O
you	O
are	O
sick	O
(	O
because	O
one	O
of	O
you	O
has	O
infected	O
the	O
other	O
)	O
is	O
a	O
higher	O
aﬃnity	O
state	O
,	O
though	O
still	O
not	O
as	O
common	O
as	O
the	O
state	O
where	O
both	O
are	O
healthy	O
.	O
to	O
complete	O
the	O
model	B
,	O
we	O
would	O
need	O
to	O
also	O
deﬁne	O
a	O
similar	O
factor	O
for	O
the	O
clique	O
containing	O
hy	O
and	O
hr	O
.	O
16.2.3	O
the	O
partition	O
function	O
while	O
the	O
unnormalized	O
probability	O
distribution	O
is	O
guaranteed	O
to	O
be	O
non-negative	O
everywhere	O
,	O
it	O
is	O
not	O
guaranteed	O
to	O
sum	O
or	O
integrate	O
to	O
1.	O
to	O
obtain	O
a	O
valid	O
probability	O
distribution	O
,	O
we	O
must	O
use	O
the	O
corresponding	O
normalized	O
probability	O
distribution:4	O
p	O
(	O
)	O
=x	O
˜p	O
(	O
)	O
x	O
(	O
16.4	O
)	O
	O
1	O
z	O
where	O
z	O
is	O
the	O
value	O
that	O
results	O
in	O
the	O
probability	O
distribution	O
summing	O
or	O
integrating	O
to	O
1	O
:	O
z	O
=	O
˜p	O
d	O
.	O
(	O
)	O
x	O
x	O
(	O
16.5	O
)	O
you	O
can	O
think	O
of	O
z	O
as	O
a	O
constant	O
when	O
the	O
φ	O
functions	O
are	O
held	O
constant	O
.	O
note	O
that	O
if	O
the	O
φ	O
functions	O
have	O
parameters	O
,	O
then	O
z	O
is	O
a	O
function	O
of	O
those	O
parameters	O
.	O
it	O
is	O
common	O
in	O
the	O
literature	O
to	O
write	O
z	O
with	O
its	O
arguments	O
omitted	O
to	O
save	O
space	O
.	O
the	O
normalizing	O
constant	O
z	O
is	O
known	O
as	O
the	O
partition	O
function	O
,	O
a	O
term	O
borrowed	O
from	O
statistical	O
physics	O
.	O
since	O
z	O
is	O
an	O
integral	O
or	O
sum	O
over	O
all	O
possible	O
joint	O
assignments	O
of	O
the	O
state	O
x	O
it	O
is	O
often	O
intractable	O
to	O
compute	O
.	O
in	O
order	O
to	O
be	O
able	O
to	O
obtain	O
the	O
normalized	O
probability	O
distribution	O
of	O
an	O
undirected	O
model	B
,	O
the	O
model	B
structure	O
and	O
the	O
deﬁnitions	O
of	O
the	O
φ	O
functions	O
must	O
be	O
conducive	O
to	O
computing	O
z	O
eﬃciently	O
.	O
in	O
the	O
context	O
of	O
deep	O
learning	O
,	O
z	O
is	O
usually	O
intractable	O
.	O
due	O
to	O
the	O
intractability	O
of	O
computing	O
z	O
exactly	O
,	O
we	O
must	O
resort	O
to	O
approximations	O
.	O
such	O
approximate	O
algorithms	O
are	O
the	O
topic	O
of	O
chapter	O
.18	O
one	O
important	O
consideration	O
to	O
keep	O
in	O
mind	O
when	O
designing	O
undirected	O
models	O
is	O
that	O
it	O
is	O
possible	O
to	O
specify	O
the	O
factors	O
in	O
such	O
a	O
way	O
that	O
z	O
does	O
not	O
exist	O
.	O
this	O
happens	O
if	O
some	O
of	O
the	O
variables	O
in	O
the	O
model	B
are	O
continuous	O
and	O
the	O
integral	O
4a	O
distribution	O
deﬁned	O
by	O
normalizing	O
a	O
product	O
of	O
clique	O
potentials	O
is	O
also	O
called	O
a	O
gibbs	O
distribution	O
.	O
568	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
	O
of	O
˜p	O
over	O
their	O
domain	O
diverges	O
.	O
for	O
example	O
,	O
suppose	O
we	O
want	O
to	O
model	B
a	O
single	O
scalar	O
variable	O
x	O
with	O
a	O
single	O
clique	O
potential	O
(	O
)	O
=	O
2	O
.	O
in	O
this	O
case	O
,	O
φ	O
x	O
x	O
z	O
=	O
x2dx	O
.	O
(	O
16.6	O
)	O
∈	O
r	O
	O
	O
since	O
this	O
integral	O
diverges	O
,	O
there	O
is	O
no	O
probability	O
distribution	O
corresponding	O
to	O
this	O
choice	O
of	O
φ	O
(	O
x	O
)	O
.	O
sometimes	O
the	O
choice	O
of	O
some	O
parameter	O
of	O
the	O
φ	O
functions	O
determines	O
whether	O
the	O
probability	O
distribution	O
is	O
deﬁned	O
.	O
for	O
example	O
,	O
for	O
φ	O
(	O
x	O
;	O
β	O
)	O
=	O
exp	O
,	O
the	O
β	O
parameter	O
determines	O
whether	O
z	O
exists	O
.	O
positive	O
β	O
results	O
in	O
a	O
gaussian	O
distribution	O
over	O
x	O
but	O
all	O
other	O
values	O
of	O
β	O
make	O
φ	O
impossible	O
to	O
normalize	O
.	O
−	O
βx2	O
one	O
key	O
diﬀerence	O
between	O
directed	O
modeling	O
and	O
undirected	O
modeling	O
is	O
that	O
directed	O
models	O
are	O
deﬁned	O
directly	O
in	O
terms	O
of	O
probability	O
distributions	O
from	O
the	O
start	O
,	O
while	O
undirected	O
models	O
are	O
deﬁned	O
more	O
loosely	O
by	O
φ	O
functions	O
that	O
are	O
then	O
converted	O
into	O
probability	O
distributions	O
.	O
this	O
changes	O
the	O
intuitions	O
one	O
must	O
develop	O
in	O
order	O
to	O
work	B
with	O
these	O
models	O
.	O
one	O
key	O
idea	O
to	O
keep	O
in	O
mind	O
while	O
working	O
with	O
undirected	O
models	O
is	O
that	O
the	O
domain	O
of	O
each	O
of	O
the	O
variables	O
has	O
dramatic	O
eﬀect	O
on	O
the	O
kind	O
of	O
probability	O
distribution	O
that	O
a	O
given	O
set	O
of	O
φ	O
functions	O
corresponds	O
to	O
.	O
for	O
example	O
,	O
consider	O
an	O
n-dimensional	O
vector-valued	O
random	O
variable	O
x	O
and	O
an	O
undirected	O
model	B
parametrized	O
by	O
a	O
vector	O
of	O
biases	O
b.	O
suppose	O
we	O
have	O
one	O
clique	O
for	O
each	O
element	O
of	O
x	O
,	O
φ	O
(	O
)	O
i	O
(	O
xi	O
)	O
=	O
exp	O
(	O
bixi	O
)	O
.	O
what	O
kind	O
of	O
probability	O
distribution	O
does	O
this	O
result	O
in	O
?	O
the	O
answer	O
is	O
that	O
we	O
do	O
∈	O
not	O
have	O
enough	O
information	O
,	O
because	O
we	O
have	O
not	O
yet	O
speciﬁed	O
the	O
domain	O
of	O
x.	O
n	O
,	O
then	O
the	O
integral	O
deﬁning	O
z	O
diverges	O
and	O
no	O
probability	O
distribution	O
if	O
x	O
n	O
,	O
then	O
p	O
(	O
x	O
)	O
factorizes	O
into	O
n	O
independent	O
distributions	O
,	O
with	O
exists	O
.	O
if	O
x	O
{	O
p	O
(	O
x	O
i	O
=	O
1	O
)	O
=	O
sigmoid	O
(	O
bi	O
)	O
.	O
if	O
the	O
domain	O
of	O
x	O
is	O
the	O
set	O
of	O
elementary	O
basis	O
vectors	O
)	O
then	O
p	O
(	O
x	O
)	O
=	O
softmax	O
(	O
b	O
)	O
,	O
so	O
a	O
large	O
(	O
value	O
of	O
bi	O
actually	O
reduces	O
p	O
(	O
x	O
j	O
=	O
1	O
)	O
for	O
j	O
=	O
i.	O
often	O
,	O
it	O
is	O
possible	O
to	O
leverage	O
the	O
eﬀect	O
of	O
a	O
carefully	O
chosen	O
domain	O
of	O
a	O
variable	O
in	O
order	O
to	O
obtain	O
complicated	O
behavior	O
from	O
a	O
relatively	O
simple	O
set	O
of	O
φ	O
functions	O
.	O
we	O
will	O
explore	O
a	O
practical	O
application	O
of	O
this	O
idea	O
later	O
,	O
in	O
section	O
}	O
[	O
1	O
,	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
]	O
,	O
[	O
0	O
,	O
1	O
,	O
.	O
.	O
.	O
,0	O
]	O
,	O
.	O
.	O
.	O
,	O
[	O
0	O
,	O
0	O
,	O
.	O
.	O
.	O
,	O
1	O
]	O
∈	O
{	O
20.6	O
.	O
0	O
,	O
1	O
}	O
r	O
16.2.4	O
energy-based	O
models	O
∀	O
many	O
interesting	O
theoretical	O
results	O
about	O
undirected	O
models	O
depend	O
on	O
the	O
as-	O
x	O
,	O
˜p	O
(	O
x	O
)	O
>	O
0.	O
a	O
convenient	O
way	O
to	O
enforce	O
this	O
condition	O
is	O
to	O
use	O
sumption	O
that	O
an	O
energy-based	O
model	B
(	O
ebm	O
)	O
where	O
−	O
e	O
(	O
)	O
=	O
exp	O
(	O
x	O
˜p	O
569	O
(	O
)	O
)	O
x	O
(	O
16.7	O
)	O
	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
a	O
d	O
b	O
e	O
c	O
f	O
16.4	O
:	O
a	O
b	O
,	O
(	O
a	O
b	O
,	O
)	O
φ	O
φ	O
figure	O
1	O
z	O
tions	O
.	O
,	O
this	O
b	O
c	O
,	O
(	O
b	O
c	O
,	O
)	O
φ	O
graph	O
implies	O
b	O
e	O
,	O
(	O
b	O
e	O
,	O
)	O
φ	O
)	O
φ	O
that	O
p	O
(	O
a	O
b	O
c	O
d	O
e	O
f	O
,	O
)	O
can	O
be	O
written	O
as	O
e	O
f	O
,	O
(	O
e	O
f	O
,	O
)	O
for	O
an	O
appropriate	O
choice	O
of	O
the	O
φ	O
func-	O
a	O
d	O
,	O
(	O
a	O
d	O
,	O
,	O
,	O
,	O
and	O
e	O
(	O
x	O
)	O
is	O
known	O
as	O
the	O
energy	O
function	O
.	O
because	O
exp	O
(	O
z	O
)	O
is	O
positive	O
for	O
all	O
z	O
,	O
this	O
guarantees	O
that	O
no	O
energy	O
function	O
will	O
result	O
in	O
a	O
probability	O
of	O
zero	O
for	O
any	O
state	O
x.	O
being	O
completely	O
free	O
to	O
choose	O
the	O
energy	O
function	O
makes	O
learning	O
simpler	O
.	O
if	O
we	O
learned	O
the	O
clique	O
potentials	O
directly	O
,	O
we	O
would	O
need	O
to	O
use	O
constrained	O
optimization	O
to	O
arbitrarily	O
impose	O
some	O
speciﬁc	O
minimal	O
probability	O
value	O
.	O
by	O
learning	O
the	O
energy	O
function	O
,	O
we	O
can	O
use	O
unconstrained	O
optimization.5	O
the	O
probabilities	O
in	O
an	O
energy-based	O
model	B
can	O
approach	O
arbitrarily	O
close	O
to	O
zero	O
but	O
never	O
reach	O
it	O
.	O
,	O
;	O
;	O
16.7	O
et	O
al.	O
,	O
et	O
al.	O
,	O
1983	O
ackley	O
is	O
an	O
example	O
of	O
a	O
any	O
distribution	O
of	O
the	O
form	O
given	O
by	O
equation	O
boltz-	O
mann	O
distribution	O
.	O
for	O
this	O
reason	O
,	O
many	O
energy-based	O
models	O
are	O
called	O
boltzmann	O
machines	O
(	O
fahlman	O
et	O
al.	O
,	O
1984	O
hinton	O
and	O
sejnowski	O
1986	O
)	O
.	O
there	O
is	O
no	O
accepted	O
guideline	O
for	O
when	O
to	O
call	O
a	O
model	B
an	O
energy-based	O
model	B
and	O
when	O
to	O
call	O
it	O
a	O
boltzmann	O
machine	O
.	O
the	O
term	O
boltzmann	O
machine	O
was	O
ﬁrst	O
introduced	O
to	O
describe	O
a	O
model	B
with	O
exclusively	O
binary	O
variables	O
,	O
but	O
today	O
many	O
models	O
such	O
as	O
the	O
mean-covariance	O
restricted	O
boltzmann	O
machine	O
incorporate	O
real-valued	O
variables	O
as	O
well	O
.	O
while	O
boltzmann	O
machines	O
were	O
originally	O
deﬁned	O
to	O
encompass	O
both	O
models	O
with	O
and	O
without	O
la-	O
tent	O
variables	O
,	O
the	O
term	O
boltzmann	O
machine	O
is	O
today	O
most	O
often	O
used	O
to	O
designate	O
models	O
with	O
latent	O
variables	O
,	O
while	O
boltzmann	O
machines	O
without	O
latent	O
variables	O
are	O
more	O
often	O
called	O
markov	O
random	O
ﬁelds	O
or	O
log-linear	O
models	O
.	O
1985	O
hinton	O
;	O
cliques	O
in	O
an	O
undirected	O
graph	O
correspond	O
to	O
factors	O
of	O
the	O
unnormalized	O
probability	O
function	O
.	O
because	O
exp	O
(	O
a	O
)	O
exp	O
(	O
b	O
)	O
=	O
exp	O
(	O
a+	O
b	O
)	O
,	O
this	O
means	O
that	O
diﬀerent	O
cliques	O
in	O
the	O
undirected	O
graph	O
correspond	O
to	O
the	O
diﬀerent	O
terms	O
of	O
the	O
energy	O
function	O
.	O
in	O
other	O
words	O
,	O
an	O
energy-based	O
model	B
is	O
just	O
a	O
special	O
kind	O
of	O
markov	O
network	O
:	O
the	O
exponentiation	O
makes	O
each	O
term	O
in	O
the	O
energy	O
function	O
correspond	O
to	O
a	O
factor	O
for	O
a	O
diﬀerent	O
clique	O
.	O
see	O
ﬁgure	O
for	O
an	O
example	O
of	O
how	O
to	O
read	O
the	O
16.5	O
5for	O
some	O
models	O
,	O
we	O
may	O
still	O
need	O
to	O
use	O
constrained	O
optimization	O
to	O
make	O
sure	O
z	O
exists	O
.	O
570	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
a	O
d	O
b	O
e	O
c	O
f	O
figure	O
16.5	O
:	O
this	O
graph	O
implies	O
that	O
e	O
(	O
a	O
b	O
c	O
d	O
e	O
f	O
b	O
c	O
,	O
(	O
b	O
c	O
,	O
)	O
+	O
e	O
e	O
energy	O
functions	O
.	O
note	O
that	O
we	O
can	O
obtain	O
the	O
φ	O
functions	O
in	O
ﬁgure	O
to	O
the	O
exponential	O
of	O
the	O
corresponding	O
negative	O
energy	O
,	O
e.g.	O
,	O
φ	O
a	O
b	O
,	O
(	O
a	O
b	O
,	O
)	O
+	O
e	O
f	O
,	O
(	O
e	O
f	O
,	O
)	O
for	O
an	O
appropriate	O
choice	O
of	O
the	O
per-clique	O
−	O
by	O
setting	O
each	O
φ	O
.	O
a	O
b	O
,	O
(	O
a	O
b	O
,	O
)	O
=	O
exp	O
(	O
e	O
a	O
b	O
,	O
)	O
)	O
,	O
)	O
can	O
be	O
written	O
as	O
e	O
a	O
d	O
,	O
(	O
a	O
d	O
,	O
)	O
+	O
e	O
b	O
e	O
,	O
(	O
b	O
e	O
,	O
)	O
+	O
e	O
16.4	O
(	O
,	O
,	O
,	O
,	O
,	O
form	O
of	O
the	O
energy	O
function	O
from	O
an	O
undirected	O
graph	O
structure	O
.	O
one	O
can	O
view	O
an	O
energy-based	O
model	B
with	O
multiple	O
terms	O
in	O
its	O
energy	O
function	O
as	O
being	O
a	O
product	O
of	O
experts	O
(	O
hinton	O
1999	O
)	O
.	O
each	O
term	O
in	O
the	O
energy	O
function	O
corresponds	O
to	O
another	O
factor	O
in	O
the	O
probability	O
distribution	O
.	O
each	O
term	O
of	O
the	O
energy	O
function	O
can	O
be	O
thought	O
of	O
as	O
an	O
“	O
expert	O
”	O
that	O
determines	O
whether	O
a	O
particular	O
soft	O
constraint	O
is	O
satisﬁed	O
.	O
each	O
expert	O
may	O
enforce	O
only	O
one	O
constraint	O
that	O
concerns	O
only	O
a	O
low-dimensional	O
projection	O
of	O
the	O
random	O
variables	O
,	O
but	O
when	O
combined	O
by	O
multiplication	O
of	O
probabilities	O
,	O
the	O
experts	O
together	O
enforce	O
a	O
complicated	O
high-	O
dimensional	O
constraint	O
.	O
−	O
−	O
16.7	O
.	O
this	O
sign	O
in	O
equation	O
one	O
part	O
of	O
the	O
deﬁnition	O
of	O
an	O
energy-based	O
model	B
serves	O
no	O
functional	O
purpose	O
from	O
a	O
machine	O
learning	O
point	O
of	O
view	O
:	O
the	O
sign	O
could	O
be	O
incorporated	O
into	O
the	O
deﬁnition	O
of	O
e.	O
for	O
many	O
choices	O
of	O
the	O
function	O
−	O
e	O
,	O
the	O
learning	O
algorithm	O
is	O
free	O
to	O
determine	O
the	O
sign	O
of	O
the	O
energy	O
anyway	O
.	O
the	O
sign	O
is	O
present	O
primarily	O
to	O
preserve	O
compatibility	O
between	O
the	O
machine	O
learning	O
literature	O
and	O
the	O
physics	O
literature	O
.	O
many	O
advances	O
in	O
probabilistic	O
modeling	O
were	O
originally	O
developed	O
by	O
statistical	O
physicists	O
,	O
for	O
whom	O
e	O
refers	O
to	O
actual	O
,	O
physical	O
energy	O
and	O
does	O
not	O
have	O
arbitrary	O
sign	O
.	O
terminology	O
such	O
as	O
“	O
energy	O
”	O
and	O
“	O
partition	O
function	O
”	O
remains	O
associated	O
with	O
these	O
techniques	O
,	O
even	O
though	O
their	O
mathematical	O
applicability	O
is	O
broader	O
than	O
the	O
physics	O
context	O
in	O
which	O
they	O
were	O
developed	O
.	O
some	O
machine	O
learning	O
researchers	O
(	O
e.g.	O
,	O
)	O
,	O
who	O
referred	O
to	O
negative	O
energy	O
as	O
harmony	O
)	O
have	O
chosen	O
to	O
emit	O
the	O
negation	O
,	O
but	O
this	O
is	O
not	O
the	O
standard	O
convention	O
.	O
smolensky	O
1986	O
(	O
many	O
algorithms	O
that	O
operate	O
on	O
probabilistic	O
models	O
do	O
not	O
need	O
to	O
compute	O
pmodel	O
(	O
x	O
)	O
but	O
only	O
log	O
˜pmodel	O
(	O
x	O
)	O
.	O
for	O
energy-based	O
models	O
with	O
latent	O
variables	O
h	O
,	O
these	O
algorithms	O
are	O
sometimes	O
phrased	O
in	O
terms	O
of	O
the	O
negative	O
of	O
this	O
quantity	O
,	O
571	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
a	O
s	O
b	O
a	O
s	O
b	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
16.6	O
:	O
(	O
a	O
)	O
the	O
path	O
between	O
random	O
variable	O
a	O
and	O
random	O
variable	O
b	O
through	O
s	O
is	O
active	O
,	O
because	O
s	O
is	O
not	O
observed	O
.	O
this	O
means	O
that	O
a	O
and	O
b	O
are	O
not	O
separated	O
.	O
(	O
b	O
)	O
here	O
s	O
is	O
shaded	O
in	O
,	O
to	O
indicate	O
that	O
it	O
is	O
observed	O
.	O
because	O
the	O
only	O
path	O
between	O
a	O
and	O
b	O
is	O
through	O
s	O
,	O
and	O
that	O
path	O
is	O
inactive	O
,	O
we	O
can	O
conclude	O
that	O
a	O
and	O
b	O
are	O
separated	O
given	O
s.	O
	O
called	O
the	O
:	O
free	O
energy	O
f	O
−	O
(	O
)	O
=	O
x	O
log	O
h	O
−	O
e	O
x	O
h	O
,	O
exp	O
(	O
(	O
)	O
)	O
.	O
(	O
16.8	O
)	O
in	O
this	O
book	O
,	O
we	O
usually	O
prefer	O
the	O
more	O
general	O
log	O
˜pmodel	O
(	O
)	O
x	O
formulation	O
.	O
16.2.5	O
separation	O
and	O
d-separation	O
the	O
edges	O
in	O
a	O
graphical	O
model	B
tell	O
us	O
which	O
variables	O
directly	O
interact	O
.	O
we	O
often	O
need	O
to	O
know	O
which	O
variables	O
indirectly	O
interact	O
.	O
some	O
of	O
these	O
indirect	O
interactions	O
can	O
be	O
enabled	O
or	O
disabled	O
by	O
observing	O
other	O
variables	O
.	O
more	O
formally	O
,	O
we	O
would	O
like	O
to	O
know	O
which	O
subsets	O
of	O
variables	O
are	O
conditionally	O
independent	O
from	O
each	O
other	O
,	O
given	O
the	O
values	O
of	O
other	O
subsets	O
of	O
variables	O
.	O
identifying	O
the	O
conditional	O
independences	O
in	O
a	O
graph	O
is	O
very	O
simple	O
in	O
the	O
case	O
of	O
undirected	O
models	O
.	O
in	O
this	O
case	O
,	O
conditional	O
independence	O
implied	O
by	O
the	O
graph	O
is	O
called	O
separation	O
.	O
we	O
say	O
that	O
a	O
set	O
of	O
variables	O
a	O
is	O
separated	O
from	O
another	O
set	O
of	O
variables	O
b	O
given	O
a	O
third	O
set	O
of	O
variables	O
s	O
if	O
the	O
graph	O
structure	O
implies	O
that	O
a	O
is	O
independent	O
from	O
b	O
given	O
s.	O
if	O
two	O
variables	O
a	O
and	O
b	O
are	O
connected	O
by	O
a	O
path	O
involving	O
only	O
unobserved	O
variables	O
,	O
then	O
those	O
variables	O
are	O
not	O
separated	O
.	O
if	O
no	O
path	O
exists	O
between	O
them	O
,	O
or	O
all	O
paths	O
contain	O
an	O
observed	O
variable	O
,	O
then	O
they	O
are	O
separated	O
.	O
we	O
refer	O
to	O
paths	O
involving	O
only	O
unobserved	O
variables	O
as	O
“	O
active	O
”	O
and	O
paths	O
including	O
an	O
observed	O
variable	O
as	O
“	O
inactive.	O
”	O
when	O
we	O
draw	O
a	O
graph	O
,	O
we	O
can	O
indicate	O
observed	O
variables	O
by	O
shading	O
them	O
in	O
.	O
for	O
a	O
depiction	O
of	O
how	O
active	O
and	O
inactive	O
paths	O
in	O
an	O
undirected	O
for	O
an	O
example	O
of	O
reading	O
see	O
ﬁgure	O
model	B
look	O
when	O
drawn	O
in	O
this	O
way	O
.	O
see	O
ﬁgure	O
separation	O
from	O
an	O
undirected	O
graph	O
.	O
16.7	O
16.6	O
similar	O
concepts	O
apply	O
to	O
directed	O
models	O
,	O
except	O
that	O
in	O
the	O
context	O
of	O
directed	O
models	O
,	O
these	O
concepts	O
are	O
referred	O
to	O
as	O
d-separation	O
.	O
the	O
“	O
d	O
”	O
stands	O
for	O
“	O
dependence.	O
”	O
d-separation	O
for	O
directed	O
graphs	O
is	O
deﬁned	O
the	O
same	O
as	O
separation	O
572	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
a	O
d	O
b	O
c	O
figure	O
16.7	O
:	O
an	O
example	O
of	O
reading	O
separation	O
properties	O
from	O
an	O
undirected	O
graph	O
.	O
here	O
b	O
is	O
shaded	O
to	O
indicate	O
that	O
it	O
is	O
observed	O
.	O
because	O
observing	O
b	O
blocks	O
the	O
only	O
path	O
from	O
a	O
to	O
c	O
,	O
we	O
say	O
that	O
a	O
and	O
c	O
are	O
separated	O
from	O
each	O
other	O
given	O
b	O
.	O
the	O
observation	O
of	O
b	O
also	O
blocks	O
one	O
path	O
between	O
a	O
and	O
d	O
,	O
but	O
there	O
is	O
a	O
second	O
,	O
active	O
path	O
between	O
them	O
.	O
therefore	O
,	O
a	O
and	O
d	O
are	O
not	O
separated	O
given	O
b.	O
for	O
undirected	O
graphs	O
:	O
we	O
say	O
that	O
a	O
set	O
of	O
variables	O
a	O
is	O
d-separated	O
from	O
another	O
set	O
of	O
variables	O
b	O
given	O
a	O
third	O
set	O
of	O
variables	O
s	O
if	O
the	O
graph	O
structure	O
implies	O
that	O
is	O
independent	O
from	O
given	O
.	O
s	O
a	O
b	O
as	O
with	O
undirected	O
models	O
,	O
we	O
can	O
examine	O
the	O
independences	O
implied	O
by	O
the	O
graph	O
by	O
looking	O
at	O
what	O
active	O
paths	O
exist	O
in	O
the	O
graph	O
.	O
as	O
before	O
,	O
two	O
variables	O
are	O
dependent	O
if	O
there	O
is	O
an	O
active	O
path	O
between	O
them	O
,	O
and	O
d-separated	O
if	O
no	O
such	O
path	O
exists	O
.	O
in	O
directed	O
nets	O
,	O
determining	O
whether	O
a	O
path	O
is	O
active	O
is	O
somewhat	O
more	O
complicated	O
.	O
see	O
ﬁgure	O
16.8	O
for	O
a	O
guide	O
to	O
identifying	O
active	O
paths	O
in	O
a	O
directed	O
model	B
.	O
see	O
ﬁgure	O
for	O
an	O
example	O
of	O
reading	O
some	O
properties	O
from	O
a	O
16.9	O
graph	O
.	O
it	O
is	O
important	O
to	O
remember	O
that	O
separation	O
and	O
d-separation	O
tell	O
us	O
only	O
about	O
those	O
conditional	O
independences	O
that	O
are	O
implied	O
by	O
the	O
graph	O
.	O
there	O
is	O
no	O
requirement	O
that	O
the	O
graph	O
imply	O
all	O
independences	O
that	O
are	O
present	O
.	O
in	O
particular	O
,	O
it	O
is	O
always	O
legitimate	O
to	O
use	O
the	O
complete	O
graph	O
(	O
the	O
graph	O
with	O
all	O
possible	O
edges	O
)	O
to	O
represent	O
any	O
distribution	O
.	O
in	O
fact	O
,	O
some	O
distributions	O
contain	O
independences	O
that	O
are	O
not	O
possible	O
to	O
represent	O
with	O
existing	O
graphical	O
notation	O
.	O
context-	O
speciﬁc	O
independences	O
are	O
independences	O
that	O
are	O
present	O
dependent	O
on	O
the	O
value	O
of	O
some	O
variables	O
in	O
the	O
network	O
.	O
for	O
example	O
,	O
consider	O
a	O
model	B
of	O
three	O
binary	O
variables	O
:	O
a	O
,	O
b	O
and	O
c	O
.	O
suppose	O
that	O
when	O
a	O
is	O
0	O
,	O
b	O
and	O
c	O
are	O
independent	O
,	O
but	O
when	O
a	O
is	O
1	O
,	O
b	O
is	O
deterministically	O
equal	O
to	O
c.	O
encoding	O
the	O
behavior	O
when	O
a	O
=	O
1	O
requires	O
an	O
edge	O
connecting	O
b	O
and	O
c.	O
the	O
graph	O
then	O
fails	O
to	O
indicate	O
that	O
b	O
and	O
c	O
are	O
independent	O
when	O
a	O
.=	O
0	O
in	O
general	O
,	O
a	O
graph	O
will	O
never	O
imply	O
that	O
an	O
independence	O
exists	O
when	O
it	O
does	O
not	O
.	O
however	O
,	O
a	O
graph	O
may	O
fail	O
to	O
encode	O
an	O
independence	O
.	O
573	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
a	O
a	O
b	O
b	O
s	O
s	O
(	O
a	O
)	O
a	O
b	O
a	O
a	O
s	O
(	O
c	O
)	O
b	O
b	O
s	O
(	O
b	O
)	O
s	O
c	O
(	O
d	O
)	O
(	O
a	O
)	O
any	O
path	O
with	O
arrows	O
proceeding	O
directly	O
from	O
figure	O
16.8	O
:	O
all	O
of	O
the	O
kinds	O
of	O
active	O
paths	O
of	O
length	O
two	O
that	O
can	O
exist	O
between	O
random	O
variables	O
a	O
and	O
b	O
.	O
a	O
to	O
b	O
or	O
vice	O
versa	O
.	O
this	O
kind	O
of	O
path	O
becomes	O
blocked	O
if	O
s	O
is	O
observed	O
.	O
we	O
have	O
already	O
seen	O
this	O
kind	O
of	O
path	O
in	O
the	O
relay	O
race	O
example	O
.	O
(	O
b	O
)	O
a	O
and	O
b	O
are	O
connected	O
by	O
a	O
common	O
cause	O
s.	O
for	O
example	O
,	O
suppose	O
s	O
is	O
a	O
variable	O
indicating	O
whether	O
or	O
not	O
there	O
is	O
a	O
hurricane	O
and	O
a	O
and	O
b	O
measure	O
the	O
wind	O
speed	O
at	O
two	O
diﬀerent	O
nearby	O
weather	O
monitoring	O
outposts	O
.	O
if	O
we	O
observe	O
very	O
high	O
winds	O
at	O
station	O
a	O
,	O
we	O
might	O
expect	O
to	O
also	O
see	O
high	O
winds	O
at	O
b.	O
this	O
kind	O
of	O
path	O
can	O
be	O
blocked	O
by	O
observing	O
s.	O
if	O
we	O
already	O
know	O
there	O
is	O
a	O
hurricane	O
,	O
we	O
expect	O
to	O
see	O
high	O
winds	O
at	O
b	O
,	O
regardless	O
of	O
what	O
is	O
observed	O
at	O
a.	O
a	O
lower	O
than	O
expected	O
wind	O
at	O
a	O
(	O
for	O
a	O
hurricane	O
)	O
would	O
not	O
change	O
our	O
expectation	O
of	O
winds	O
at	O
b	O
(	O
knowing	O
there	O
is	O
a	O
hurricane	O
)	O
.	O
however	O
,	O
if	O
s	O
is	O
not	O
observed	O
,	O
then	O
a	O
and	O
b	O
are	O
dependent	O
,	O
i.e.	O
,	O
the	O
path	O
is	O
active	O
.	O
(	O
c	O
)	O
a	O
and	O
b	O
are	O
both	O
parents	O
of	O
s.	O
this	O
is	O
called	O
a	O
v-structure	O
or	O
the	O
collider	O
case	O
.	O
the	O
v-structure	O
causes	O
a	O
and	O
b	O
to	O
be	O
related	O
by	O
the	O
explaining	O
away	O
eﬀect	O
.	O
in	O
this	O
case	O
,	O
the	O
path	O
is	O
actually	O
active	O
when	O
s	O
is	O
observed	O
.	O
for	O
example	O
,	O
suppose	O
s	O
is	O
a	O
variable	O
indicating	O
that	O
your	O
colleague	O
is	O
not	O
at	O
work	B
.	O
the	O
variable	O
a	O
represents	O
her	O
being	O
sick	O
,	O
while	O
b	O
represents	O
her	O
being	O
on	O
vacation	O
.	O
if	O
you	O
observe	O
that	O
she	O
is	O
not	O
at	O
work	B
,	O
you	O
can	O
presume	O
she	O
is	O
probably	O
sick	O
or	O
on	O
vacation	O
,	O
but	O
it	O
is	O
not	O
especially	O
likely	O
that	O
both	O
have	O
happened	O
at	O
the	O
same	O
time	O
.	O
if	O
you	O
ﬁnd	O
out	O
that	O
she	O
is	O
on	O
vacation	O
,	O
her	O
absence	O
.	O
you	O
can	O
infer	O
that	O
she	O
is	O
probably	O
not	O
also	O
this	O
fact	O
is	O
suﬃcient	O
to	O
sick	O
.	O
s	O
is	O
observed	O
!	O
for	O
example	O
,	O
suppose	O
that	O
c	O
is	O
a	O
variable	O
representing	O
whether	O
you	O
have	O
received	O
a	O
report	O
from	O
your	O
colleague	O
.	O
if	O
you	O
notice	O
that	O
you	O
have	O
not	O
received	O
the	O
report	O
,	O
this	O
increases	O
your	O
estimate	O
of	O
the	O
probability	O
that	O
she	O
is	O
not	O
at	O
work	B
today	O
,	O
which	O
in	O
turn	O
makes	O
it	O
more	O
likely	O
that	O
she	O
is	O
either	O
sick	O
or	O
on	O
vacation	O
.	O
the	O
only	O
way	O
to	O
block	O
a	O
path	O
through	O
a	O
v-structure	O
is	O
to	O
observe	O
none	O
of	O
the	O
descendants	O
of	O
the	O
shared	O
child	O
.	O
the	O
explaining	O
away	O
eﬀect	O
happens	O
even	O
if	O
any	O
descendant	O
of	O
explain	O
(	O
d	O
)	O
574	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
a	O
d	O
c	O
b	O
e	O
figure	O
16.9	O
:	O
from	O
this	O
graph	O
,	O
we	O
can	O
read	O
out	O
several	O
d-separation	O
properties	O
.	O
examples	O
include	O
:	O
•	O
a	O
and	O
b	O
are	O
d-separated	O
given	O
the	O
empty	O
set	O
.	O
we	O
can	O
also	O
see	O
that	O
some	O
variables	O
are	O
no	O
longer	O
d-separated	O
when	O
we	O
observe	O
some	O
variables	O
:	O
•	O
•	O
•	O
•	O
a	O
and	O
e	O
are	O
d-separated	O
given	O
c.	O
d	O
and	O
e	O
are	O
d-separated	O
given	O
c.	O
a	O
and	O
b	O
are	O
not	O
d-separated	O
given	O
c.	O
a	O
and	O
b	O
are	O
not	O
d-separated	O
given	O
d.	O
575	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
16.2.6	O
converting	O
between	O
undirected	O
and	O
directed	O
graphs	O
we	O
often	O
refer	O
to	O
a	O
speciﬁc	O
machine	O
learning	O
model	B
as	O
being	O
undirected	O
or	O
directed	O
.	O
for	O
example	O
,	O
we	O
typically	O
refer	O
to	O
rbms	O
as	O
undirected	O
and	O
sparse	O
coding	O
as	O
directed	O
.	O
this	O
choice	O
of	O
wording	O
can	O
be	O
somewhat	O
misleading	O
,	O
because	O
no	O
probabilistic	O
model	B
is	O
inherently	O
directed	O
or	O
undirected	O
.	O
instead	O
,	O
some	O
models	O
are	O
most	O
easily	O
described	O
using	O
a	O
directed	O
graph	O
,	O
or	O
most	O
easily	O
described	O
using	O
an	O
undirected	O
graph	O
.	O
directed	O
models	O
and	O
undirected	O
models	O
both	O
have	O
their	O
advantages	O
and	O
disad-	O
vantages	O
.	O
neither	O
approach	O
is	O
clearly	O
superior	O
and	O
universally	O
preferred	O
.	O
instead	O
,	O
we	O
should	O
choose	O
which	O
language	O
to	O
use	O
for	O
each	O
task	O
.	O
this	O
choice	O
will	O
partially	O
depend	O
on	O
which	O
probability	O
distribution	O
we	O
wish	O
to	O
describe	O
.	O
we	O
may	O
choose	O
to	O
use	O
either	O
directed	O
modeling	O
or	O
undirected	O
modeling	O
based	O
on	O
which	O
approach	O
can	O
capture	O
the	O
most	O
independences	O
in	O
the	O
probability	O
distribution	O
or	O
which	O
approach	O
uses	O
the	O
fewest	O
edges	O
to	O
describe	O
the	O
distribution	O
.	O
there	O
are	O
other	O
factors	O
that	O
can	O
aﬀect	O
the	O
decision	O
of	O
which	O
language	O
to	O
use	O
.	O
even	O
while	O
working	O
with	O
a	O
single	O
probability	O
distribution	O
,	O
we	O
may	O
sometimes	O
switch	O
between	O
diﬀerent	O
modeling	O
languages	O
.	O
sometimes	O
a	O
diﬀerent	O
language	O
becomes	O
more	O
appropriate	O
if	O
we	O
observe	O
a	O
certain	O
subset	O
of	O
variables	O
,	O
or	O
if	O
we	O
wish	O
to	O
perform	O
a	O
diﬀerent	O
computational	O
task	O
.	O
for	O
example	O
,	O
the	O
directed	O
model	B
description	O
often	O
provides	O
a	O
straightforward	O
approach	O
to	O
eﬃciently	O
draw	O
samples	O
from	O
the	O
model	B
(	O
described	O
in	O
section	O
16.3	O
)	O
while	O
the	O
undirected	O
model	B
formulation	O
is	O
often	O
useful	O
for	O
deriving	O
approximate	O
inference	O
procedures	O
(	O
as	O
we	O
will	O
see	O
in	O
chapter	O
,	O
where	O
the	O
role	O
of	O
undirected	O
models	O
is	O
highlighted	O
in	O
equation	O
19.56	O
19	O
)	O
.	O
every	O
probability	O
distribution	O
can	O
be	O
represented	O
by	O
either	O
a	O
directed	O
model	B
or	O
by	O
an	O
undirected	O
model	B
.	O
in	O
the	O
worst	O
case	O
,	O
one	O
can	O
always	O
represent	O
any	O
distribution	O
by	O
using	O
a	O
“	O
complete	O
graph.	O
”	O
in	O
the	O
case	O
of	O
a	O
directed	O
model	B
,	O
the	O
complete	O
graph	O
is	O
any	O
directed	O
acyclic	O
graph	O
where	O
we	O
impose	O
some	O
ordering	O
on	O
the	O
random	O
variables	O
,	O
and	O
each	O
variable	O
has	O
all	O
other	O
variables	O
that	O
precede	O
it	O
in	O
the	O
ordering	O
as	O
its	O
ancestors	O
in	O
the	O
graph	O
.	O
for	O
an	O
undirected	O
model	B
,	O
the	O
complete	O
graph	O
is	O
simply	O
a	O
graph	O
containing	O
a	O
single	O
clique	O
encompassing	O
all	O
of	O
the	O
variables	O
.	O
see	O
ﬁgure	O
for	O
an	O
example	O
.	O
16.10	O
of	O
course	O
,	O
the	O
utility	O
of	O
a	O
graphical	O
model	B
is	O
that	O
the	O
graph	O
implies	O
that	O
some	O
variables	O
do	O
not	O
interact	O
directly	O
.	O
the	O
complete	O
graph	O
is	O
not	O
very	O
useful	O
because	O
it	O
does	O
not	O
imply	O
any	O
independences	O
.	O
when	O
we	O
represent	O
a	O
probability	O
distribution	O
with	O
a	O
graph	O
,	O
we	O
want	O
to	O
choose	O
a	O
graph	O
that	O
implies	O
as	O
many	O
independences	O
as	O
possible	O
,	O
without	O
implying	O
any	O
independences	O
that	O
do	O
not	O
actually	O
exist	O
.	O
from	O
this	O
point	O
of	O
view	O
,	O
some	O
distributions	O
can	O
be	O
represented	O
more	O
eﬃciently	O
576	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
figure	O
16.10	O
:	O
examples	O
of	O
complete	O
graphs	O
,	O
which	O
can	O
describe	O
any	O
probability	O
distribution	O
.	O
here	O
we	O
show	O
examples	O
with	O
four	O
random	O
variables	O
.	O
(	O
left	O
)	O
the	O
complete	O
undirected	O
graph	O
.	O
in	O
the	O
undirected	O
case	O
,	O
the	O
complete	O
graph	O
is	O
unique	O
.	O
a	O
complete	O
directed	O
graph	O
.	O
in	O
the	O
directed	O
case	O
,	O
there	O
is	O
not	O
a	O
unique	O
complete	O
graph	O
.	O
we	O
choose	O
an	O
ordering	O
of	O
the	O
variables	O
and	O
draw	O
an	O
arc	O
from	O
each	O
variable	O
to	O
every	O
variable	O
that	O
comes	O
after	O
it	O
in	O
the	O
ordering	O
.	O
there	O
are	O
thus	O
a	O
factorial	O
number	O
of	O
complete	O
graphs	O
for	O
every	O
set	O
of	O
random	O
variables	O
.	O
in	O
this	O
example	O
we	O
order	O
the	O
variables	O
from	O
left	O
to	O
right	O
,	O
top	O
to	O
bottom	O
.	O
(	O
right	O
)	O
using	O
directed	O
models	O
,	O
while	O
other	O
distributions	O
can	O
be	O
represented	O
more	O
eﬃciently	O
using	O
undirected	O
models	O
.	O
in	O
other	O
words	O
,	O
directed	O
models	O
can	O
encode	O
some	O
independences	O
that	O
undirected	O
models	O
can	O
not	O
encode	O
,	O
and	O
vice	O
versa	O
.	O
directed	O
models	O
are	O
able	O
to	O
use	O
one	O
speciﬁc	O
kind	O
of	O
substructure	O
that	O
undirected	O
models	O
can	O
not	O
represent	O
perfectly	O
.	O
this	O
substructure	O
is	O
called	O
an	O
immorality	O
.	O
the	O
structure	O
occurs	O
when	O
two	O
random	O
variables	O
a	O
and	O
b	O
are	O
both	O
parents	O
of	O
a	O
third	O
random	O
variable	O
c	O
,	O
and	O
there	O
is	O
no	O
edge	O
directly	O
connecting	O
a	O
and	O
b	O
in	O
either	O
direction	O
.	O
(	O
the	O
name	O
“	O
immorality	O
”	O
may	O
seem	O
strange	O
;	O
it	O
was	O
coined	O
in	O
the	O
graphical	O
u	O
models	O
literature	O
as	O
a	O
joke	O
about	O
unmarried	O
parents	O
.	O
)	O
to	O
convert	O
a	O
directed	O
model	B
with	O
graph	O
.	O
for	O
u	O
every	O
pair	O
of	O
variables	O
x	O
and	O
y	O
,	O
we	O
add	O
an	O
undirected	O
edge	O
connecting	O
x	O
and	O
y	O
to	O
or	O
if	O
x	O
is	O
known	O
as	O
a	O
for	O
examples	O
of	O
converting	O
directed	O
models	O
to	O
into	O
an	O
undirected	O
model	B
,	O
we	O
need	O
to	O
create	O
a	O
new	O
graph	O
d	O
if	O
there	O
is	O
a	O
directed	O
edge	O
(	O
in	O
either	O
direction	O
)	O
connecting	O
x	O
and	O
y	O
in	O
of	O
a	O
third	O
variable	O
z.	O
the	O
resulting	O
and	O
y	O
are	O
both	O
parents	O
in	O
moralized	O
graph	O
.	O
see	O
ﬁgure	O
16.11	O
undirected	O
models	O
via	O
moralization	O
.	O
d	O
d	O
u	O
d	O
likewise	O
,	O
undirected	O
models	O
can	O
include	O
substructures	O
that	O
no	O
directed	O
model	B
can	O
not	O
capture	O
all	O
of	O
the	O
can	O
represent	O
perfectly	O
.	O
speciﬁcally	O
,	O
a	O
directed	O
graph	O
conditional	O
independences	O
implied	O
by	O
an	O
undirected	O
graph	O
contains	O
a	O
loop	O
of	O
length	O
greater	O
than	O
three	O
,	O
unless	O
that	O
loop	O
also	O
contains	O
a	O
chord	O
.	O
a	O
loop	O
is	O
a	O
sequence	O
of	O
variables	O
connected	O
by	O
undirected	O
edges	O
,	O
with	O
the	O
last	O
variable	O
in	O
the	O
sequence	O
connected	O
back	O
to	O
the	O
ﬁrst	O
variable	O
in	O
the	O
sequence	O
.	O
a	O
chord	O
is	O
a	O
connection	O
between	O
any	O
two	O
non-consecutive	O
variables	O
in	O
the	O
sequence	O
deﬁning	O
a	O
loop	O
.	O
if	O
has	O
loops	O
of	O
length	O
four	O
or	O
greater	O
and	O
does	O
not	O
have	O
chords	O
for	O
these	O
loops	O
,	O
we	O
must	O
add	O
the	O
chords	O
before	O
we	O
can	O
convert	O
it	O
to	O
a	O
directed	O
model	B
.	O
adding	O
u	O
u	O
u	O
if	O
577	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
a	O
b	O
c	O
a	O
b	O
c	O
a	O
b	O
h1h1	O
h2h2	O
h3h3	O
c	O
v1v1	O
v2v2	O
v3v3	O
a	O
b	O
h1h1	O
h2h2	O
h3h3	O
c	O
v1v1	O
v2v2	O
v3v3	O
(	O
center	O
)	O
figure	O
16.11	O
:	O
examples	O
of	O
converting	O
directed	O
models	O
(	O
top	O
row	O
)	O
to	O
undirected	O
models	O
(	O
bottom	O
row	O
)	O
by	O
constructing	O
moralized	O
graphs	O
.	O
(	O
left	O
)	O
this	O
simple	O
chain	O
can	O
be	O
converted	O
to	O
a	O
moralized	O
graph	O
merely	O
by	O
replacing	O
its	O
directed	O
edges	O
with	O
undirected	O
edges	O
.	O
the	O
resulting	O
undirected	O
model	B
implies	O
exactly	O
the	O
same	O
set	O
of	O
independences	O
and	O
conditional	O
independences	O
.	O
this	O
graph	O
is	O
the	O
simplest	O
directed	O
model	B
that	O
can	O
not	O
be	O
converted	O
to	O
an	O
undirected	O
model	B
without	O
losing	O
some	O
independences	O
.	O
this	O
graph	O
consists	O
entirely	O
of	O
a	O
single	O
immorality	O
.	O
because	O
a	O
and	O
b	O
are	O
parents	O
of	O
c	O
,	O
they	O
are	O
connected	O
by	O
an	O
active	O
⊥	O
path	O
when	O
c	O
is	O
observed	O
.	O
to	O
capture	O
this	O
dependence	O
,	O
the	O
undirected	O
model	B
must	O
include	O
a	O
clique	O
encompassing	O
all	O
three	O
variables	O
.	O
this	O
clique	O
fails	O
to	O
encode	O
the	O
fact	O
that	O
a	O
b	O
.	O
(	O
right	O
)	O
in	O
general	O
,	O
moralization	O
may	O
add	O
many	O
edges	O
to	O
the	O
graph	O
,	O
thus	O
losing	O
many	O
implied	O
independences	O
.	O
for	O
example	O
,	O
this	O
sparse	O
coding	O
graph	O
requires	O
adding	O
moralizing	O
edges	O
between	O
every	O
pair	O
of	O
hidden	O
units	O
,	O
thus	O
introducing	O
a	O
quadratic	O
number	O
of	O
new	O
direct	O
dependences	O
.	O
578	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
a	O
d	O
b	O
c	O
a	O
d	O
b	O
c	O
a	O
d	O
b	O
c	O
⊥	O
|	O
{	O
}	O
b	O
d	O
,	O
figure	O
16.12	O
:	O
converting	O
an	O
undirected	O
model	B
to	O
a	O
directed	O
model	B
.	O
(	O
left	O
)	O
this	O
undirected	O
model	B
can	O
not	O
be	O
converted	O
directed	O
to	O
a	O
directed	O
model	B
because	O
it	O
has	O
a	O
loop	O
of	O
length	O
four	O
with	O
no	O
chords	O
.	O
speciﬁcally	O
,	O
the	O
undirected	O
model	B
encodes	O
two	O
diﬀerent	O
independences	O
that	O
no	O
directed	O
model	B
can	O
capture	O
simultaneously	O
:	O
a	O
c	O
to	O
convert	O
the	O
undirected	O
model	B
to	O
a	O
directed	O
model	B
,	O
we	O
must	O
triangulate	O
the	O
graph	O
,	O
by	O
ensuring	O
that	O
all	O
loops	O
of	O
greater	O
than	O
length	O
three	O
have	O
a	O
chord	O
.	O
to	O
do	O
so	O
,	O
we	O
can	O
either	O
add	O
an	O
edge	O
connecting	O
a	O
and	O
c	O
or	O
we	O
can	O
add	O
an	O
edge	O
connecting	O
b	O
and	O
d.	O
in	O
this	O
example	O
,	O
we	O
choose	O
to	O
add	O
the	O
edge	O
connecting	O
a	O
and	O
c.	O
to	O
ﬁnish	O
the	O
conversion	O
process	O
,	O
we	O
must	O
assign	O
a	O
direction	O
to	O
each	O
edge	O
.	O
when	O
doing	O
so	O
,	O
we	O
must	O
not	O
create	O
any	O
directed	O
cycles	O
.	O
one	O
way	O
to	O
avoid	O
directed	O
cycles	O
is	O
to	O
impose	O
an	O
ordering	O
over	O
the	O
nodes	O
,	O
and	O
always	O
point	O
each	O
edge	O
from	O
the	O
node	O
that	O
comes	O
earlier	O
in	O
the	O
ordering	O
to	O
the	O
node	O
that	O
comes	O
later	O
in	O
the	O
ordering	O
.	O
in	O
this	O
example	O
,	O
we	O
use	O
the	O
variable	O
names	O
to	O
impose	O
alphabetical	O
order.	O
}	O
a	O
c	O
,	O
⊥	O
|	O
{	O
and	O
b	O
d	O
.	O
(	O
center	O
)	O
(	O
right	O
)	O
u	O
u	O
.	O
these	O
chords	O
discards	O
some	O
of	O
the	O
independence	O
information	O
that	O
was	O
encoded	O
in	O
the	O
graph	O
formed	O
by	O
adding	O
chords	O
to	O
is	O
known	O
as	O
a	O
chordal	O
or	O
triangulated	O
graph	O
,	O
because	O
all	O
the	O
loops	O
can	O
now	O
be	O
described	O
in	O
terms	O
of	O
smaller	O
,	O
triangular	O
loops	O
.	O
to	O
build	O
a	O
directed	O
graph	O
from	O
the	O
chordal	O
graph	O
,	O
we	O
need	O
to	O
also	O
assign	O
d	O
directions	O
to	O
the	O
edges	O
.	O
when	O
doing	O
so	O
,	O
we	O
must	O
not	O
create	O
a	O
directed	O
cycle	O
in	O
,	O
or	O
the	O
result	O
does	O
not	O
deﬁne	O
a	O
valid	O
directed	O
probabilistic	O
model	B
.	O
one	O
way	O
to	O
assign	O
directions	O
to	O
the	O
edges	O
in	O
is	O
to	O
impose	O
an	O
ordering	O
on	O
the	O
random	O
variables	O
,	O
then	O
point	O
each	O
edge	O
from	O
the	O
node	O
that	O
comes	O
earlier	O
in	O
the	O
ordering	O
to	O
the	O
node	O
that	O
comes	O
later	O
in	O
the	O
ordering	O
.	O
see	O
ﬁgure	O
for	O
a	O
demonstration	O
.	O
16.12	O
d	O
d	O
16.2.7	O
factor	O
graphs	O
factor	O
graphs	O
are	O
another	O
way	O
of	O
drawing	O
undirected	O
models	O
that	O
resolve	O
an	O
ambiguity	O
in	O
the	O
graphical	O
representation	O
of	O
standard	O
undirected	O
model	B
syntax	O
.	O
in	O
an	O
undirected	O
model	B
,	O
the	O
scope	O
of	O
every	O
φ	O
function	O
must	O
be	O
a	O
of	O
some	O
clique	O
in	O
the	O
graph	O
.	O
ambiguity	O
arises	O
because	O
it	O
is	O
not	O
clear	O
if	O
each	O
clique	O
actually	O
has	O
a	O
corresponding	O
factor	O
whose	O
scope	O
encompasses	O
the	O
entire	O
clique—for	O
example	O
,	O
a	O
clique	O
containing	O
three	O
nodes	O
may	O
correspond	O
to	O
a	O
factor	O
over	O
all	O
three	O
nodes	O
,	O
or	O
may	O
correspond	O
to	O
three	O
factors	O
that	O
each	O
contain	O
only	O
a	O
pair	O
of	O
the	O
nodes	O
.	O
subset	O
579	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
factor	O
graphs	O
resolve	O
this	O
ambiguity	O
by	O
explicitly	O
representing	O
the	O
scope	O
of	O
each	O
φ	O
function	O
.	O
speciﬁcally	O
,	O
a	O
factor	O
graph	O
is	O
a	O
graphical	O
representation	O
of	O
an	O
undirected	O
model	B
that	O
consists	O
of	O
a	O
bipartite	O
undirected	O
graph	O
.	O
some	O
of	O
the	O
nodes	O
are	O
drawn	O
as	O
circles	O
.	O
these	O
nodes	O
correspond	O
to	O
random	O
variables	O
as	O
in	O
a	O
standard	O
undirected	O
model	B
.	O
the	O
rest	O
of	O
the	O
nodes	O
are	O
drawn	O
as	O
squares	O
.	O
these	O
nodes	O
correspond	O
to	O
the	O
factors	O
φ	O
of	O
the	O
unnormalized	O
probability	O
distribution	O
.	O
variables	O
and	O
factors	O
may	O
be	O
connected	O
with	O
undirected	O
edges	O
.	O
a	O
variable	O
and	O
a	O
factor	O
are	O
connected	O
in	O
the	O
graph	O
if	O
and	O
only	O
if	O
the	O
variable	O
is	O
one	O
of	O
the	O
arguments	O
to	O
the	O
factor	O
in	O
the	O
unnormalized	O
probability	O
distribution	O
.	O
no	O
factor	O
may	O
be	O
connected	O
to	O
another	O
factor	O
in	O
the	O
graph	O
,	O
nor	O
can	O
a	O
variable	O
be	O
connected	O
to	O
a	O
variable	O
.	O
see	O
ﬁgure	O
16.13	O
for	O
an	O
example	O
of	O
how	O
factor	O
graphs	O
can	O
resolve	O
ambiguity	O
in	O
the	O
interpretation	O
of	O
undirected	O
networks	O
.	O
a	O
b	O
a	O
f1f1	O
c	O
f1f1	O
b	O
f3f3	O
f2f2	O
c	O
a	O
b	O
c	O
(	O
center	O
)	O
figure	O
16.13	O
:	O
an	O
example	O
of	O
how	O
a	O
factor	O
graph	O
can	O
resolve	O
ambiguity	O
in	O
the	O
interpretation	O
of	O
undirected	O
networks	O
.	O
(	O
left	O
)	O
an	O
undirected	O
network	O
with	O
a	O
clique	O
involving	O
three	O
variables	O
:	O
a	O
,	O
b	O
and	O
c.	O
a	O
factor	O
graph	O
corresponding	O
to	O
the	O
same	O
undirected	O
model	B
.	O
this	O
factor	O
graph	O
has	O
one	O
factor	O
over	O
all	O
three	O
variables	O
.	O
another	O
valid	O
factor	O
graph	O
for	O
the	O
same	O
undirected	O
model	B
.	O
this	O
factor	O
graph	O
has	O
three	O
factors	O
,	O
each	O
over	O
only	O
two	O
variables	O
.	O
representation	O
,	O
inference	O
,	O
and	O
learning	O
are	O
all	O
asymptotically	O
cheaper	O
in	O
this	O
factor	O
graph	O
than	O
in	O
the	O
factor	O
graph	O
depicted	O
in	O
the	O
center	O
,	O
even	O
though	O
both	O
require	O
the	O
same	O
undirected	O
graph	O
to	O
represent	O
.	O
(	O
right	O
)	O
16.3	O
sampling	O
from	O
graphical	O
models	O
graphical	O
models	O
also	O
facilitate	O
the	O
task	O
of	O
drawing	O
samples	O
from	O
a	O
model	B
.	O
one	O
advantage	O
of	O
directed	O
graphical	O
models	O
is	O
that	O
a	O
simple	O
and	O
eﬃcient	O
proce-	O
dure	O
called	O
ancestral	O
sampling	O
can	O
produce	O
a	O
sample	O
from	O
the	O
joint	O
distribution	O
represented	O
by	O
the	O
model	B
.	O
the	O
basic	O
idea	O
is	O
to	O
sort	O
the	O
variables	O
xi	O
in	O
the	O
graph	O
into	O
a	O
topological	O
ordering	O
,	O
so	O
that	O
for	O
all	O
i	O
and	O
j	O
,	O
j	O
is	O
greater	O
than	O
i	O
if	O
xi	O
is	O
a	O
parent	O
of	O
xj	O
.	O
the	O
variables	O
580	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
|	O
|	O
p	O
ag	O
(	O
x2	O
)	O
)	O
,	O
and	O
so	O
on	O
,	O
until	O
ﬁnally	O
we	O
sample	O
p	O
(	O
xn	O
∼	O
|	O
p	O
(	O
x1	O
)	O
,	O
can	O
then	O
be	O
sampled	O
in	O
this	O
order	O
.	O
in	O
other	O
words	O
,	O
we	O
ﬁrst	O
sample	O
x1	O
p	O
ag	O
(	O
xn	O
)	O
)	O
.	O
then	O
sample	O
p	O
(	O
x2	O
p	O
ag	O
(	O
xi	O
)	O
)	O
is	O
easy	O
to	O
sample	O
from	O
,	O
so	O
long	O
as	O
each	O
conditional	O
distribution	O
p	O
(	O
xi	O
then	O
the	O
whole	O
model	B
is	O
easy	O
to	O
sample	O
from	O
.	O
the	O
topological	O
sorting	O
operation	O
guarantees	O
that	O
we	O
can	O
read	O
the	O
conditional	O
distributions	O
in	O
equation	O
and	O
sample	O
from	O
them	O
in	O
order	O
.	O
without	O
the	O
topological	O
sorting	O
,	O
we	O
might	O
attempt	O
to	O
sample	O
a	O
variable	O
before	O
its	O
parents	O
are	O
available	O
.	O
16.1	O
for	O
some	O
graphs	O
,	O
more	O
than	O
one	O
topological	O
ordering	O
is	O
possible	O
.	O
ancestral	O
sampling	O
may	O
be	O
used	O
with	O
any	O
of	O
these	O
topological	O
orderings	O
.	O
ancestral	O
sampling	O
is	O
generally	O
very	O
fast	O
(	O
assuming	O
sampling	O
from	O
each	O
condi-	O
tional	O
is	O
easy	O
)	O
and	O
convenient	O
.	O
one	O
drawback	O
to	O
ancestral	O
sampling	O
is	O
that	O
it	O
only	O
applies	O
to	O
directed	O
graphical	O
models	O
.	O
another	O
drawback	O
is	O
that	O
it	O
does	O
not	O
support	O
every	O
conditional	O
sampling	O
operation	O
.	O
when	O
we	O
wish	O
to	O
sample	O
from	O
a	O
subset	O
of	O
the	O
variables	O
in	O
a	O
directed	O
graphical	O
model	B
,	O
given	O
some	O
other	O
variables	O
,	O
we	O
often	O
require	O
that	O
all	O
the	O
condition-	O
ing	O
variables	O
come	O
earlier	O
than	O
the	O
variables	O
to	O
be	O
sampled	O
in	O
the	O
ordered	O
graph	O
.	O
in	O
this	O
case	O
,	O
we	O
can	O
sample	O
from	O
the	O
local	O
conditional	O
probability	O
distributions	O
speciﬁed	O
by	O
the	O
model	B
distribution	O
.	O
otherwise	O
,	O
the	O
conditional	O
distributions	O
we	O
need	O
to	O
sample	O
from	O
are	O
the	O
posterior	O
distributions	O
given	O
the	O
observed	O
variables	O
.	O
these	O
posterior	O
distributions	O
are	O
usually	O
not	O
explicitly	O
speciﬁed	O
and	O
parametrized	O
in	O
the	O
model	B
.	O
inferring	O
these	O
posterior	O
distributions	O
can	O
be	O
costly	O
.	O
in	O
models	O
where	O
this	O
is	O
the	O
case	O
,	O
ancestral	O
sampling	O
is	O
no	O
longer	O
eﬃcient	O
.	O
unfortunately	O
,	O
ancestral	O
sampling	O
is	O
applicable	O
only	O
to	O
directed	O
models	O
.	O
we	O
can	O
sample	O
from	O
undirected	O
models	O
by	O
converting	O
them	O
to	O
directed	O
models	O
,	O
but	O
this	O
often	O
requires	O
solving	O
intractable	O
inference	O
problems	O
(	O
to	O
determine	O
the	O
marginal	O
distribution	O
over	O
the	O
root	O
nodes	O
of	O
the	O
new	O
directed	O
graph	O
)	O
or	O
requires	O
introducing	O
so	O
many	O
edges	O
that	O
the	O
resulting	O
directed	O
model	B
becomes	O
intractable	O
.	O
sampling	O
from	O
an	O
undirected	O
model	B
without	O
ﬁrst	O
converting	O
it	O
to	O
a	O
directed	O
model	B
seems	O
to	O
require	O
resolving	O
cyclical	O
dependencies	O
.	O
every	O
variable	O
interacts	O
with	O
every	O
other	O
variable	O
,	O
so	O
there	O
is	O
no	O
clear	O
beginning	O
point	O
for	O
the	O
sampling	O
process	O
.	O
unfortunately	O
,	O
drawing	O
samples	O
from	O
an	O
undirected	O
graphical	O
model	B
is	O
an	O
expensive	O
,	O
multi-pass	O
process	O
.	O
the	O
conceptually	O
simplest	O
approach	O
is	O
gibbs	O
sampling	O
.	O
suppose	O
we	O
have	O
a	O
graphical	O
model	B
over	O
an	O
n-dimensional	O
vector	O
of	O
random	O
variables	O
x.	O
we	O
iteratively	O
visit	O
each	O
variable	O
xi	O
and	O
draw	O
a	O
sample	O
conditioned	O
on	O
all	O
of	O
the	O
other	O
variables	O
,	O
from	O
p	O
(	O
xi	O
i	O
)	O
.	O
due	O
to	O
the	O
separation	O
properties	O
of	O
the	O
graphical	O
model	B
,	O
we	O
can	O
equivalently	O
condition	O
on	O
only	O
the	O
neighbors	O
of	O
xi	O
.	O
unfortunately	O
,	O
after	O
we	O
have	O
made	O
one	O
pass	O
through	O
the	O
graphical	O
model	B
and	O
sampled	O
all	O
n	O
variables	O
,	O
we	O
still	O
do	O
not	O
have	O
a	O
fair	O
sample	O
from	O
p	O
(	O
x	O
)	O
.	O
instead	O
,	O
we	O
must	O
repeat	O
the	O
x−	O
|	O
581	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
process	O
and	O
resample	O
all	O
n	O
variables	O
using	O
the	O
updated	O
values	O
of	O
their	O
neighbors	O
.	O
asymptotically	O
,	O
after	O
many	O
repetitions	O
,	O
this	O
process	O
converges	O
to	O
sampling	O
from	O
the	O
correct	O
distribution	O
.	O
it	O
can	O
be	O
diﬃcult	O
to	O
determine	O
when	O
the	O
samples	O
have	O
reached	O
a	O
suﬃciently	O
accurate	O
approximation	O
of	O
the	O
desired	O
distribution	O
.	O
sampling	O
techniques	O
for	O
undirected	O
models	O
are	O
an	O
advanced	O
topic	O
,	O
covered	O
in	O
more	O
detail	O
in	O
chapter	O
.17	O
16.4	O
advantages	O
of	O
structured	O
modeling	O
the	O
primary	O
advantage	O
of	O
using	O
structured	O
probabilistic	O
models	O
is	O
that	O
they	O
allow	O
us	O
to	O
dramatically	O
reduce	O
the	O
cost	O
of	O
representing	O
probability	O
distributions	O
as	O
well	O
as	O
learning	O
and	O
inference	O
.	O
sampling	O
is	O
also	O
accelerated	O
in	O
the	O
case	O
of	O
directed	O
models	O
,	O
while	O
the	O
situation	O
can	O
be	O
complicated	O
with	O
undirected	O
models	O
.	O
the	O
primary	O
mechanism	O
that	O
allows	O
all	O
of	O
these	O
operations	O
to	O
use	O
less	O
runtime	O
and	O
memory	O
is	O
choosing	O
to	O
not	O
model	B
certain	O
interactions	O
.	O
graphical	O
models	O
convey	O
information	O
by	O
leaving	O
edges	O
out	O
.	O
anywhere	O
there	O
is	O
not	O
an	O
edge	O
,	O
the	O
model	B
speciﬁes	O
the	O
assumption	O
that	O
we	O
do	O
not	O
need	O
to	O
model	B
a	O
direct	O
interaction	O
.	O
a	O
less	O
quantiﬁable	O
beneﬁt	O
of	O
using	O
structured	O
probabilistic	O
models	O
is	O
that	O
they	O
allow	O
us	O
to	O
explicitly	O
separate	O
representation	O
of	O
knowledge	O
from	O
learning	O
of	O
knowledge	O
or	O
inference	O
given	O
existing	O
knowledge	O
.	O
this	O
makes	O
our	O
models	O
easier	O
to	O
develop	O
and	O
debug	O
.	O
we	O
can	O
design	O
,	O
analyze	O
,	O
and	O
evaluate	O
learning	O
algorithms	O
and	O
inference	O
algorithms	O
that	O
are	O
applicable	O
to	O
broad	O
classes	O
of	O
graphs	O
.	O
independently	O
,	O
we	O
can	O
design	O
models	O
that	O
capture	O
the	O
relationships	O
we	O
believe	O
are	O
important	O
in	O
our	O
data	O
.	O
we	O
can	O
then	O
combine	O
these	O
diﬀerent	O
algorithms	O
and	O
structures	O
and	O
obtain	O
a	O
cartesian	O
product	O
of	O
diﬀerent	O
possibilities	O
.	O
it	O
would	O
be	O
much	O
more	O
diﬃcult	O
to	O
design	O
end-to-end	O
algorithms	O
for	O
every	O
possible	O
situation	O
.	O
16.5	O
learning	O
about	O
dependencies	O
a	O
good	O
generative	O
model	B
needs	O
to	O
accurately	O
capture	O
the	O
distribution	O
over	O
the	O
observed	O
or	O
“	O
visible	O
”	O
variables	O
v	O
.	O
often	O
the	O
diﬀerent	O
elements	O
of	O
v	O
are	O
highly	O
dependent	O
on	O
each	O
other	O
.	O
in	O
the	O
context	O
of	O
deep	O
learning	O
,	O
the	O
approach	O
most	O
commonly	O
used	O
to	O
model	B
these	O
dependencies	O
is	O
to	O
introduce	O
several	O
latent	O
or	O
“	O
hidden	O
”	O
variables	O
,	O
h.	O
the	O
model	B
can	O
then	O
capture	O
dependencies	O
between	O
any	O
pair	O
of	O
variables	O
v	O
i	O
and	O
vj	O
indirectly	O
,	O
via	O
direct	O
dependencies	O
between	O
vi	O
and	O
h	O
,	O
and	O
direct	O
dependencies	O
between	O
and	O
v	O
h	O
j.	O
a	O
good	O
model	B
of	O
v	O
which	O
did	O
not	O
contain	O
any	O
latent	O
variables	O
would	O
need	O
to	O
582	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
have	O
very	O
large	O
numbers	O
of	O
parents	O
per	O
node	O
in	O
a	O
bayesian	O
network	O
or	O
very	O
large	O
cliques	O
in	O
a	O
markov	O
network	O
.	O
just	O
representing	O
these	O
higher	O
order	O
interactions	O
is	O
costly—both	O
in	O
a	O
computational	O
sense	O
,	O
because	O
the	O
number	O
of	O
parameters	O
that	O
must	O
be	O
stored	O
in	O
memory	O
scales	O
exponentially	O
with	O
the	O
number	O
of	O
members	O
in	O
a	O
clique	O
,	O
but	O
also	O
in	O
a	O
statistical	O
sense	O
,	O
because	O
this	O
exponential	O
number	O
of	O
parameters	O
requires	O
a	O
wealth	O
of	O
data	O
to	O
estimate	O
accurately	O
.	O
when	O
the	O
model	B
is	O
intended	O
to	O
capture	O
dependencies	O
between	O
visible	O
variables	O
with	O
direct	O
connections	O
,	O
it	O
is	O
usually	O
infeasible	O
to	O
connect	O
all	O
variables	O
,	O
so	O
the	O
graph	O
must	O
be	O
designed	O
to	O
connect	O
those	O
variables	O
that	O
are	O
tightly	O
coupled	O
and	O
omit	O
edges	O
between	O
other	O
variables	O
.	O
an	O
entire	O
ﬁeld	O
of	O
machine	O
learning	O
called	O
structure	O
learning	O
is	O
devoted	O
to	O
this	O
problem	O
for	O
a	O
good	O
reference	O
on	O
structure	O
learning	O
,	O
see	O
(	O
koller	O
and	O
friedman	O
2009	O
)	O
.	O
most	O
structure	O
learning	O
techniques	O
are	O
a	O
form	O
of	O
greedy	O
search	O
.	O
a	O
structure	O
is	O
proposed	O
,	O
a	O
model	B
with	O
that	O
structure	O
is	O
trained	O
,	O
then	O
given	O
a	O
score	O
.	O
the	O
score	O
rewards	O
high	O
training	O
set	O
accuracy	O
and	O
penalizes	O
model	B
complexity	O
.	O
candidate	O
structures	O
with	O
a	O
small	O
number	O
of	O
edges	O
added	O
or	O
removed	O
are	O
then	O
proposed	O
as	O
the	O
next	O
step	O
of	O
the	O
search	O
.	O
the	O
search	O
proceeds	O
to	O
a	O
new	O
structure	O
that	O
is	O
expected	O
to	O
increase	O
the	O
score	O
.	O
,	O
using	O
latent	O
variables	O
instead	O
of	O
adaptive	O
structure	O
avoids	O
the	O
need	O
to	O
perform	O
discrete	O
searches	O
and	O
multiple	O
rounds	O
of	O
training	O
.	O
a	O
ﬁxed	O
structure	O
over	O
visible	O
and	O
hidden	O
variables	O
can	O
use	O
direct	O
interactions	O
between	O
visible	O
and	O
hidden	O
units	O
to	O
impose	O
indirect	O
interactions	O
between	O
visible	O
units	O
.	O
using	O
simple	O
parameter	O
learning	O
techniques	O
we	O
can	O
learn	O
a	O
model	B
with	O
a	O
ﬁxed	O
structure	O
that	O
imputes	O
the	O
right	O
structure	O
on	O
the	O
marginal	O
.	O
p	O
(	O
)	O
v	O
3.9.6	O
latent	O
variables	O
have	O
advantages	O
beyond	O
their	O
role	O
in	O
eﬃciently	O
capturing	O
p	O
(	O
v	O
)	O
.	O
the	O
new	O
variables	O
h	O
also	O
provide	O
an	O
alternative	O
representation	O
for	O
v.	O
for	O
example	O
,	O
as	O
discussed	O
in	O
section	O
,	O
the	O
mixture	O
of	O
gaussians	O
model	B
learns	O
a	O
latent	O
variable	O
that	O
corresponds	O
to	O
which	O
category	O
of	O
examples	O
the	O
input	O
was	O
drawn	O
from	O
.	O
this	O
means	O
that	O
the	O
latent	O
variable	O
in	O
a	O
mixture	O
of	O
gaussians	O
model	B
can	O
be	O
used	O
to	O
do	O
classiﬁcation	O
.	O
in	O
chapter	O
we	O
saw	O
how	O
simple	O
probabilistic	O
models	O
like	O
sparse	O
coding	O
learn	O
latent	O
variables	O
that	O
can	O
be	O
used	O
as	O
input	O
features	O
for	O
a	O
classiﬁer	O
,	O
or	O
as	O
coordinates	O
along	O
a	O
manifold	O
.	O
other	O
models	O
can	O
be	O
used	O
in	O
this	O
same	O
way	O
,	O
but	O
deeper	O
models	O
and	O
models	O
with	O
diﬀerent	O
kinds	O
of	O
interactions	O
can	O
create	O
even	O
richer	O
descriptions	O
of	O
the	O
input	O
.	O
many	O
approaches	O
accomplish	O
feature	O
learning	O
by	O
learning	O
latent	O
variables	O
.	O
often	O
,	O
given	O
some	O
model	B
of	O
v	O
and	O
h	O
,	O
experimental	O
observations	O
show	O
that	O
e	O
[	O
h	O
v	O
]	O
or	O
argmaxhp	O
(	O
h	O
v	O
,	O
)	O
is	O
a	O
good	O
feature	O
mapping	O
for	O
v.	O
14	O
|	O
583	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
16.6	O
inference	O
and	O
approximate	O
inference	O
one	O
of	O
the	O
main	O
ways	O
we	O
can	O
use	O
a	O
probabilistic	O
model	B
is	O
to	O
ask	O
questions	O
about	O
how	O
variables	O
are	O
related	O
to	O
each	O
other	O
.	O
given	O
a	O
set	O
of	O
medical	O
tests	O
,	O
we	O
can	O
ask	O
what	O
disease	O
a	O
patient	O
might	O
have	O
.	O
in	O
a	O
latent	O
variable	O
model	B
,	O
we	O
might	O
want	O
to	O
extract	O
features	O
e	O
[	O
h	O
v	O
]	O
describing	O
the	O
observed	O
variables	O
v.	O
sometimes	O
we	O
need	O
to	O
solve	O
such	O
problems	O
in	O
order	O
to	O
perform	O
other	O
tasks	O
.	O
we	O
often	O
train	O
our	O
models	O
using	O
the	O
principle	O
of	O
maximum	O
likelihood	O
.	O
because	O
−	O
|	O
|	O
p	O
h	O
v	O
log	O
(	O
)	O
]	O
,	O
(	O
16.9	O
)	O
log	O
(	O
)	O
=	O
p	O
v	O
|	O
v	O
)	O
[	O
log	O
(	O
h	O
p	O
h	O
v	O
,	O
)	O
∼	O
eh	O
p	O
(	O
|	O
we	O
often	O
want	O
to	O
compute	O
p	O
(	O
h	O
v	O
)	O
in	O
order	O
to	O
implement	O
a	O
learning	O
rule	O
.	O
all	O
of	O
these	O
are	O
examples	O
of	O
inference	O
problems	O
in	O
which	O
we	O
must	O
predict	O
the	O
value	O
of	O
some	O
variables	O
given	O
other	O
variables	O
,	O
or	O
predict	O
the	O
probability	O
distribution	O
over	O
some	O
variables	O
given	O
the	O
value	O
of	O
other	O
variables	O
.	O
unfortunately	O
,	O
for	O
most	O
interesting	O
deep	O
models	O
,	O
these	O
inference	O
problems	O
are	O
intractable	O
,	O
even	O
when	O
we	O
use	O
a	O
structured	O
graphical	O
model	B
to	O
simplify	O
them	O
.	O
the	O
graph	O
structure	O
allows	O
us	O
to	O
represent	O
complicated	O
,	O
high-dimensional	O
distributions	O
with	O
a	O
reasonable	O
number	O
of	O
parameters	O
,	O
but	O
the	O
graphs	O
used	O
for	O
deep	O
learning	O
are	O
usually	O
not	O
restrictive	O
enough	O
to	O
also	O
allow	O
eﬃcient	O
inference	O
.	O
it	O
is	O
straightforward	O
to	O
see	O
that	O
computing	O
the	O
marginal	O
probability	O
of	O
a	O
general	O
graphical	O
model	B
is	O
#	O
p	O
hard	O
.	O
the	O
complexity	O
class	O
#	O
p	O
is	O
a	O
generalization	O
of	O
the	O
complexity	O
class	O
np	O
.	O
problems	O
in	O
np	O
require	O
determining	O
only	O
whether	O
a	O
problem	O
has	O
a	O
solution	O
and	O
ﬁnding	O
a	O
solution	O
if	O
one	O
exists	O
.	O
problems	O
in	O
#	O
p	O
require	O
counting	O
the	O
number	O
of	O
solutions	O
.	O
to	O
construct	O
a	O
worst-case	O
graphical	O
model	B
,	O
imagine	O
that	O
we	O
deﬁne	O
a	O
graphical	O
model	B
over	O
the	O
binary	O
variables	O
in	O
a	O
3-sat	O
problem	O
.	O
we	O
can	O
impose	O
a	O
uniform	O
distribution	O
over	O
these	O
variables	O
.	O
we	O
can	O
then	O
add	O
one	O
binary	O
latent	O
variable	O
per	O
clause	O
that	O
indicates	O
whether	O
each	O
clause	O
is	O
satisﬁed	O
.	O
we	O
can	O
then	O
add	O
another	O
latent	O
variable	O
indicating	O
whether	O
all	O
of	O
the	O
clauses	O
are	O
satisﬁed	O
.	O
this	O
can	O
be	O
done	O
without	O
making	O
a	O
large	O
clique	O
,	O
by	O
building	O
a	O
reduction	O
tree	O
of	O
latent	O
variables	O
,	O
with	O
each	O
node	O
in	O
the	O
tree	O
reporting	O
whether	O
two	O
other	O
variables	O
are	O
satisﬁed	O
.	O
the	O
leaves	O
of	O
this	O
tree	O
are	O
the	O
variables	O
for	O
each	O
clause	O
.	O
the	O
root	O
of	O
the	O
tree	O
reports	O
whether	O
the	O
entire	O
problem	O
is	O
satisﬁed	O
.	O
due	O
to	O
the	O
uniform	O
distribution	O
over	O
the	O
literals	O
,	O
the	O
marginal	O
distribution	O
over	O
the	O
root	O
of	O
the	O
reduction	O
tree	O
speciﬁes	O
what	O
fraction	O
of	O
assignments	O
satisfy	O
the	O
problem	O
.	O
while	O
this	O
is	O
a	O
contrived	O
worst-case	O
example	O
,	O
np	O
hard	O
graphs	O
commonly	O
arise	O
in	O
practical	O
real-world	O
scenarios	O
.	O
this	O
motivates	O
the	O
use	O
of	O
approximate	O
inference	O
.	O
in	O
the	O
context	O
of	O
deep	O
learning	O
,	O
this	O
usually	O
refers	O
to	O
variational	O
inference	O
,	O
in	O
which	O
we	O
approximate	O
the	O
584	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
|	O
|	O
v	O
)	O
by	O
seeking	O
an	O
approximate	O
distribution	O
q	O
(	O
h	O
v	O
true	O
distribution	O
p	O
(	O
h	O
)	O
that	O
is	O
as	O
close	O
to	O
the	O
true	O
one	O
as	O
possible	O
.	O
this	O
and	O
other	O
techniques	O
are	O
described	O
in	O
depth	O
in	O
chapter	O
.19	O
16.7	O
the	O
deep	O
learning	O
approach	O
to	O
structured	O
prob-	O
abilistic	O
models	O
deep	O
learning	O
practitioners	O
generally	O
use	O
the	O
same	O
basic	O
computational	O
tools	O
as	O
other	O
machine	O
learning	O
practitioners	O
who	O
work	B
with	O
structured	O
probabilistic	O
models	O
.	O
however	O
,	O
in	O
the	O
context	O
of	O
deep	O
learning	O
,	O
we	O
usually	O
make	O
diﬀerent	O
design	O
decisions	O
about	O
how	O
to	O
combine	O
these	O
tools	O
,	O
resulting	O
in	O
overall	O
algorithms	O
and	O
models	O
that	O
have	O
a	O
very	O
diﬀerent	O
ﬂavor	O
from	O
more	O
traditional	O
graphical	O
models	O
.	O
deep	O
learning	O
does	O
not	O
always	O
involve	O
especially	O
deep	O
graphical	O
models	O
.	O
in	O
the	O
context	O
of	O
graphical	O
models	O
,	O
we	O
can	O
deﬁne	O
the	O
depth	O
of	O
a	O
model	B
in	O
terms	O
of	O
the	O
graphical	O
model	B
graph	O
rather	O
than	O
the	O
computational	O
graph	O
.	O
we	O
can	O
think	O
of	O
a	O
latent	O
variable	O
hi	O
as	O
being	O
at	O
depth	O
j	O
if	O
the	O
shortest	O
path	O
from	O
h	O
i	O
to	O
an	O
observed	O
variable	O
is	O
j	O
steps	O
.	O
we	O
usually	O
describe	O
the	O
depth	O
of	O
the	O
model	B
as	O
being	O
the	O
greatest	O
depth	O
of	O
any	O
such	O
hi	O
.	O
this	O
kind	O
of	O
depth	O
is	O
diﬀerent	O
from	O
the	O
depth	O
induced	O
by	O
the	O
computational	O
graph	O
.	O
many	O
generative	O
models	O
used	O
for	O
deep	O
learning	O
have	O
no	O
latent	O
variables	O
or	O
only	O
one	O
layer	O
of	O
latent	O
variables	O
,	O
but	O
use	O
deep	O
computational	O
graphs	O
to	O
deﬁne	O
the	O
conditional	O
distributions	O
within	O
a	O
model	B
.	O
deep	O
learning	O
essentially	O
always	O
makes	O
use	O
of	O
the	O
idea	O
of	O
distributed	O
represen-	O
tations	O
.	O
even	O
shallow	O
models	O
used	O
for	O
deep	O
learning	O
purposes	O
(	O
such	O
as	O
pretraining	O
shallow	O
models	O
that	O
will	O
later	O
be	O
composed	O
to	O
form	O
deep	O
ones	O
)	O
nearly	O
always	O
have	O
a	O
single	O
,	O
large	O
layer	O
of	O
latent	O
variables	O
.	O
deep	O
learning	O
models	O
typically	O
have	O
more	O
latent	O
variables	O
than	O
observed	O
variables	O
.	O
complicated	O
nonlinear	O
interactions	O
between	O
variables	O
are	O
accomplished	O
via	O
indirect	O
connections	O
that	O
ﬂow	O
through	O
multiple	O
latent	O
variables	O
.	O
by	O
contrast	O
,	O
traditional	O
graphical	O
models	O
usually	O
contain	O
mostly	O
variables	O
that	O
are	O
at	O
least	O
occasionally	O
observed	O
,	O
even	O
if	O
many	O
of	O
the	O
variables	O
are	O
missing	O
at	O
random	O
from	O
some	O
training	O
examples	O
.	O
traditional	O
models	O
mostly	O
use	O
higher-order	O
terms	O
and	O
structure	O
learning	O
to	O
capture	O
complicated	O
nonlinear	O
interactions	O
between	O
variables	O
.	O
if	O
there	O
are	O
latent	O
variables	O
,	O
they	O
are	O
usually	O
few	O
in	O
number	O
.	O
the	O
way	O
that	O
latent	O
variables	O
are	O
designed	O
also	O
diﬀers	O
in	O
deep	O
learning	O
.	O
the	O
deep	O
learning	O
practitioner	O
typically	O
does	O
not	O
intend	O
for	O
the	O
latent	O
variables	O
to	O
take	O
on	O
any	O
speciﬁc	O
semantics	O
ahead	O
of	O
time—the	O
training	O
algorithm	O
is	O
free	O
to	O
invent	O
the	O
concepts	O
it	O
needs	O
to	O
model	B
a	O
particular	O
dataset	O
.	O
the	O
latent	O
variables	O
are	O
585	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
usually	O
not	O
very	O
easy	O
for	O
a	O
human	O
to	O
interpret	O
after	O
the	O
fact	O
,	O
though	O
visualization	O
techniques	O
may	O
allow	O
some	O
rough	O
characterization	O
of	O
what	O
they	O
represent	O
.	O
when	O
latent	O
variables	O
are	O
used	O
in	O
the	O
context	O
of	O
traditional	O
graphical	O
models	O
,	O
they	O
are	O
often	O
designed	O
with	O
some	O
speciﬁc	O
semantics	O
in	O
mind—the	O
topic	O
of	O
a	O
document	O
,	O
the	O
intelligence	O
of	O
a	O
student	O
,	O
the	O
disease	O
causing	O
a	O
patient	O
’	O
s	O
symptoms	O
,	O
etc	O
.	O
these	O
models	O
are	O
often	O
much	O
more	O
interpretable	O
by	O
human	O
practitioners	O
and	O
often	O
have	O
more	O
theoretical	O
guarantees	O
,	O
yet	O
are	O
less	O
able	O
to	O
scale	O
to	O
complex	O
problems	O
and	O
are	O
not	O
reusable	O
in	O
as	O
many	O
diﬀerent	O
contexts	O
as	O
deep	O
models	O
.	O
another	O
obvious	O
diﬀerence	O
is	O
the	O
kind	O
of	O
connectivity	O
typically	O
used	O
in	O
the	O
deep	O
learning	O
approach	O
.	O
deep	O
graphical	O
models	O
typically	O
have	O
large	O
groups	O
of	O
units	O
that	O
are	O
all	O
connected	O
to	O
other	O
groups	O
of	O
units	O
,	O
so	O
that	O
the	O
interactions	O
between	O
two	O
groups	O
may	O
be	O
described	O
by	O
a	O
single	O
matrix	O
.	O
traditional	O
graphical	O
models	O
have	O
very	O
few	O
connections	O
and	O
the	O
choice	O
of	O
connections	O
for	O
each	O
variable	O
may	O
be	O
individually	O
designed	O
.	O
the	O
design	O
of	O
the	O
model	B
structure	O
is	O
tightly	O
linked	O
with	O
the	O
choice	O
of	O
inference	O
algorithm	O
.	O
traditional	O
approaches	O
to	O
graphical	O
models	O
typically	O
aim	O
to	O
maintain	O
the	O
tractability	O
of	O
exact	O
inference	O
.	O
when	O
this	O
constraint	O
is	O
too	O
limiting	O
,	O
a	O
popular	O
approximate	O
inference	O
algorithm	O
is	O
an	O
algorithm	O
called	O
loopy	O
belief	O
propagation	O
.	O
both	O
of	O
these	O
approaches	O
often	O
work	B
well	O
with	O
very	O
sparsely	O
connected	O
graphs	O
.	O
by	O
comparison	O
,	O
models	O
used	O
in	O
deep	O
learning	O
tend	O
to	O
connect	O
each	O
visible	O
unit	O
vi	O
to	O
very	O
many	O
hidden	O
units	O
hj	O
,	O
so	O
that	O
h	O
can	O
provide	O
a	O
distributed	O
representation	O
of	O
vi	O
(	O
and	O
probably	O
several	O
other	O
observed	O
variables	O
too	O
)	O
.	O
distributed	O
representations	O
have	O
many	O
advantages	O
,	O
but	O
from	O
the	O
point	O
of	O
view	O
of	O
graphical	O
models	O
and	O
computational	O
complexity	O
,	O
distributed	O
representations	O
have	O
the	O
disadvantage	O
of	O
usually	O
yielding	O
graphs	O
that	O
are	O
not	O
sparse	O
enough	O
for	O
the	O
traditional	O
techniques	O
of	O
exact	O
inference	O
and	O
loopy	O
belief	O
propagation	O
to	O
be	O
relevant	O
.	O
as	O
a	O
consequence	O
,	O
one	O
of	O
the	O
most	O
striking	O
diﬀerences	O
between	O
the	O
larger	O
graphical	O
models	O
community	O
and	O
the	O
deep	O
graphical	O
models	O
community	O
is	O
that	O
loopy	O
belief	O
propagation	O
is	O
almost	O
never	O
used	O
for	O
deep	O
learning	O
.	O
most	O
deep	O
models	O
are	O
instead	O
designed	O
to	O
make	O
gibbs	O
sampling	O
or	O
variational	O
inference	O
algorithms	O
eﬃcient	O
.	O
another	O
consideration	O
is	O
that	O
deep	O
learning	O
models	O
contain	O
a	O
very	O
large	O
number	O
of	O
latent	O
variables	O
,	O
making	O
eﬃcient	O
numerical	O
code	O
essential	O
.	O
this	O
provides	O
an	O
additional	O
motivation	O
,	O
besides	O
the	O
choice	O
of	O
high-level	O
inference	O
algorithm	O
,	O
for	O
grouping	O
the	O
units	O
into	O
layers	O
with	O
a	O
matrix	O
describing	O
the	O
interaction	O
between	O
two	O
layers	O
.	O
this	O
allows	O
the	O
individual	O
steps	O
of	O
the	O
algorithm	O
to	O
be	O
implemented	O
with	O
eﬃcient	O
matrix	O
product	O
operations	O
,	O
or	O
sparsely	O
connected	O
generalizations	O
,	O
like	O
block	O
diagonal	O
matrix	O
products	O
or	O
convolutions	O
.	O
finally	O
,	O
the	O
deep	O
learning	O
approach	O
to	O
graphical	O
modeling	O
is	O
characterized	O
by	O
a	O
marked	O
tolerance	O
of	O
the	O
unknown	O
.	O
rather	O
than	O
simplifying	O
the	O
model	B
until	O
all	O
quantities	O
we	O
might	O
want	O
can	O
be	O
computed	O
exactly	O
,	O
we	O
increase	O
the	O
power	O
of	O
586	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
the	O
model	B
until	O
it	O
is	O
just	O
barely	O
possible	O
to	O
train	O
or	O
use	O
.	O
we	O
often	O
use	O
models	O
whose	O
marginal	O
distributions	O
can	O
not	O
be	O
computed	O
,	O
and	O
are	O
satisﬁed	O
simply	O
to	O
draw	O
approximate	O
samples	O
from	O
these	O
models	O
.	O
we	O
often	O
train	O
models	O
with	O
an	O
intractable	O
objective	O
function	O
that	O
we	O
can	O
not	O
even	O
approximate	O
in	O
a	O
reasonable	O
amount	O
of	O
time	O
,	O
but	O
we	O
are	O
still	O
able	O
to	O
approximately	O
train	O
the	O
model	B
if	O
we	O
can	O
eﬃciently	O
obtain	O
an	O
estimate	O
of	O
the	O
gradient	O
of	O
such	O
a	O
function	O
.	O
the	O
deep	O
learning	O
approach	O
is	O
often	O
to	O
ﬁgure	O
out	O
what	O
the	O
minimum	O
amount	O
of	O
information	O
we	O
absolutely	O
need	O
is	O
,	O
and	O
then	O
to	O
ﬁgure	O
out	O
how	O
to	O
get	O
a	O
reasonable	O
approximation	O
of	O
that	O
information	O
as	O
quickly	O
as	O
possible	O
.	O
16.7.1	O
example	O
:	O
the	O
restricted	O
boltzmann	O
machine	O
,	O
)	O
or	O
smolensky	O
1986	O
the	O
restricted	O
boltzmann	O
machine	O
(	O
rbm	O
)	O
(	O
harmonium	O
is	O
the	O
quintessential	O
example	O
of	O
how	O
graphical	O
models	O
are	O
used	O
for	O
deep	O
learning	O
.	O
the	O
rbm	O
is	O
not	O
itself	O
a	O
deep	O
model	B
.	O
instead	O
,	O
it	O
has	O
a	O
single	O
layer	O
of	O
latent	O
variables	O
that	O
may	O
be	O
used	O
to	O
learn	O
a	O
representation	O
for	O
the	O
input	O
.	O
in	O
chapter	O
,	O
we	O
will	O
see	O
how	O
rbms	O
can	O
be	O
used	O
to	O
build	O
many	O
deeper	O
models	O
.	O
here	O
,	O
we	O
show	O
how	O
the	O
rbm	O
exempliﬁes	O
many	O
of	O
the	O
practices	O
used	O
in	O
a	O
wide	O
variety	O
of	O
deep	O
graphical	O
models	O
:	O
its	O
units	O
are	O
organized	O
into	O
large	O
groups	O
called	O
layers	O
,	O
the	O
connectivity	O
between	O
layers	O
is	O
described	O
by	O
a	O
matrix	O
,	O
the	O
connectivity	O
is	O
relatively	O
dense	O
,	O
the	O
model	B
is	O
designed	O
to	O
allow	O
eﬃcient	O
gibbs	O
sampling	O
,	O
and	O
the	O
emphasis	O
of	O
the	O
model	B
design	O
is	O
on	O
freeing	O
the	O
training	O
algorithm	O
to	O
learn	O
latent	O
variables	O
whose	O
semantics	O
were	O
not	O
speciﬁed	O
by	O
the	O
designer	O
.	O
later	O
,	O
in	O
section	O
,	O
we	O
will	O
revisit	O
the	O
rbm	O
in	O
more	O
detail	O
.	O
20.2	O
20	O
the	O
canonical	O
rbm	O
is	O
an	O
energy-based	O
model	B
with	O
binary	O
visible	O
and	O
hidden	O
units	O
.	O
its	O
energy	O
function	O
is	O
e	O
,	O
(	O
v	O
h	O
)	O
=	O
−	O
	O
b	O
−	O
	O
c	O
−	O
	O
h	O
v	O
v	O
w	O
h	O
,	O
(	O
16.10	O
)	O
where	O
b	O
,	O
c	O
,	O
and	O
w	O
are	O
unconstrained	O
,	O
real-valued	O
,	O
learnable	O
parameters	O
.	O
we	O
can	O
see	O
that	O
the	O
model	B
is	O
divided	O
into	O
two	O
groups	O
of	O
units	O
:	O
v	O
and	O
h	O
,	O
and	O
the	O
interaction	O
between	O
them	O
is	O
described	O
by	O
a	O
matrix	O
w	O
.	O
the	O
model	B
is	O
depicted	O
graphically	O
in	O
ﬁgure	O
.	O
as	O
this	O
ﬁgure	O
makes	O
clear	O
,	O
an	O
important	O
aspect	O
of	O
this	O
model	B
is	O
that	O
there	O
are	O
no	O
direct	O
interactions	O
between	O
any	O
two	O
visible	O
units	O
or	O
between	O
any	O
two	O
hidden	O
units	O
(	O
hence	O
the	O
“	O
restricted	O
,	O
”	O
a	O
general	O
boltzmann	O
machine	O
may	O
have	O
arbitrary	O
connections	O
)	O
.	O
16.14	O
the	O
restrictions	O
on	O
the	O
rbm	O
structure	O
yield	O
the	O
nice	O
properties	O
p	O
(	O
h	O
v	O
)	O
=	O
π	O
ip	O
(	O
hi	O
v	O
)	O
(	O
16.11	O
)	O
|	O
|	O
587	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
h1h1	O
h2h2	O
h3h3	O
h4h4	O
v1v1	O
v2v2	O
v3v3	O
figure	O
16.14	O
:	O
an	O
rbm	O
drawn	O
as	O
a	O
markov	O
network	O
.	O
and	O
|	O
p	O
(	O
v	O
h	O
)	O
=	O
π	O
ip	O
(	O
v	O
i	O
	O
	O
	O
|	O
h	O
)	O
.	O
	O
	O
(	O
16.12	O
)	O
the	O
individual	O
conditionals	O
are	O
simple	O
to	O
compute	O
as	O
well	O
.	O
for	O
the	O
binary	O
rbm	O
we	O
obtain	O
:	O
|	O
p	O
(	O
hi	O
=	O
1	O
|	O
p	O
(	O
hi	O
=	O
0	O
v	O
)	O
=	O
σ	O
v	O
)	O
=	O
1	O
v	O
−	O
σ	O
v	O
w	O
:	O
,i	O
+	O
bi	O
,	O
	O
w	O
:	O
,i	O
+	O
bi	O
.	O
(	O
16.13	O
)	O
(	O
16.14	O
)	O
together	O
these	O
properties	O
allow	O
for	O
eﬃcient	O
block	O
gibbs	O
sampling	O
,	O
which	O
alter-	O
nates	O
between	O
sampling	O
all	O
of	O
h	O
simultaneously	O
and	O
sampling	O
all	O
of	O
v	O
simultane-	O
ously	O
.	O
samples	O
generated	O
by	O
gibbs	O
sampling	O
from	O
an	O
rbm	O
model	B
are	O
shown	O
in	O
ﬁgure	O
.	O
16.15	O
since	O
the	O
energy	O
function	O
itself	O
is	O
just	O
a	O
linear	O
function	O
of	O
the	O
parameters	O
,	O
it	O
is	O
easy	O
to	O
take	O
its	O
derivatives	O
.	O
for	O
example	O
,	O
−	O
vihj	O
.	O
e	O
,	O
(	O
v	O
h	O
)	O
=	O
∂	O
∂wi	O
,	O
j	O
(	O
16.15	O
)	O
these	O
two	O
properties—eﬃcient	O
gibbs	O
sampling	O
and	O
eﬃcient	O
derivatives—make	O
,	O
we	O
will	O
see	O
that	O
undirected	O
models	O
may	O
be	O
training	O
convenient	O
.	O
in	O
chapter	O
trained	O
by	O
computing	O
such	O
derivatives	O
applied	O
to	O
samples	O
from	O
the	O
model	B
.	O
18	O
|	O
v	O
)	O
[	O
h	O
]	O
h	O
as	O
a	O
set	O
of	O
features	O
to	O
describe	O
training	O
the	O
model	B
induces	O
a	O
representation	O
h	O
of	O
the	O
data	O
v.	O
we	O
can	O
often	O
use	O
∼	O
eh	O
p	O
(	O
overall	O
,	O
the	O
rbm	O
demonstrates	O
the	O
typical	O
deep	O
learning	O
approach	O
to	O
graph-	O
ical	O
models	O
:	O
representation	O
learning	O
accomplished	O
via	O
layers	O
of	O
latent	O
variables	O
,	O
combined	O
with	O
eﬃcient	O
interactions	O
between	O
layers	O
parametrized	O
by	O
matrices	O
.	O
.v	O
the	O
language	O
of	O
graphical	O
models	O
provides	O
an	O
elegant	O
,	O
ﬂexible	O
and	O
clear	O
language	O
for	O
describing	O
probabilistic	O
models	O
.	O
in	O
the	O
chapters	O
ahead	O
,	O
we	O
use	O
this	O
language	O
,	O
among	O
other	O
perspectives	O
,	O
to	O
describe	O
a	O
wide	O
variety	O
of	O
deep	O
probabilistic	O
models	O
.	O
588	O
chapter	O
16.	O
structured	O
probabilistic	O
models	O
for	O
deep	O
learning	O
lisa	O
2008	O
(	O
)	O
.	O
figure	O
16.15	O
:	O
samples	O
from	O
a	O
trained	O
rbm	O
,	O
and	O
its	O
weights	O
.	O
image	O
reproduced	O
with	O
permission	O
from	O
(	O
left	O
)	O
samples	O
from	O
a	O
model	B
trained	O
on	O
mnist	O
,	O
drawn	O
using	O
gibbs	O
sampling	O
.	O
each	O
column	O
is	O
a	O
separate	O
gibbs	O
sampling	O
process	O
.	O
each	O
row	O
represents	O
the	O
output	O
of	O
another	O
1,000	O
steps	O
of	O
gibbs	O
sampling	O
.	O
successive	O
samples	O
are	O
highly	O
correlated	O
with	O
one	O
another	O
.	O
the	O
corresponding	O
weight	O
vectors	O
.	O
compare	O
this	O
to	O
the	O
samples	O
and	O
weights	O
of	O
a	O
linear	O
factor	O
model	B
,	O
shown	O
in	O
ﬁgure	O
.	O
the	O
samples	O
here	O
are	O
much	O
better	O
because	O
the	O
rbm	O
prior	O
p	O
(	O
h	O
)	O
is	O
not	O
constrained	O
to	O
be	O
factorial	O
.	O
the	O
rbm	O
can	O
learn	O
which	O
features	O
should	O
appear	O
together	O
when	O
sampling	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
rbm	O
posterior	O
is	O
not	O
,	O
so	O
the	O
sparse	O
coding	O
model	B
may	O
be	O
better	O
for	O
feature	O
extraction	O
.	O
other	O
models	O
are	O
able	O
to	O
have	O
both	O
a	O
non-factorial	O
is	O
factorial	O
,	O
while	O
the	O
sparse	O
coding	O
posterior	O
and	O
a	O
non-factorial	O
|	O
h	O
v	O
|	O
h	O
v	O
)	O
(	O
right	O
)	O
|	O
h	O
v	O
.	O
)	O
p	O
(	O
p	O
(	O
)	O
13.2	O
p	O
(	O
p	O
(	O
)	O
h	O
589	O
chapter	O
17	O
monte	O
carlo	O
methods	O
randomized	O
algorithms	O
fall	O
into	O
two	O
rough	O
categories	O
:	O
las	O
vegas	O
algorithms	O
and	O
monte	O
carlo	O
algorithms	O
.	O
las	O
vegas	O
algorithms	O
always	O
return	O
precisely	O
the	O
correct	O
answer	O
(	O
or	O
report	O
that	O
they	O
failed	O
)	O
.	O
these	O
algorithms	O
consume	O
a	O
random	O
amount	O
of	O
resources	O
,	O
usually	O
memory	O
or	O
time	O
.	O
in	O
contrast	O
,	O
monte	O
carlo	O
algorithms	O
return	O
answers	O
with	O
a	O
random	O
amount	O
of	O
error	O
.	O
the	O
amount	O
of	O
error	O
can	O
typically	O
be	O
reduced	O
by	O
expending	O
more	O
resources	O
(	O
usually	O
running	O
time	O
and	O
memory	O
)	O
.	O
for	O
any	O
ﬁxed	O
computational	O
budget	O
,	O
a	O
monte	O
carlo	O
algorithm	O
can	O
provide	O
an	O
approximate	O
answer	O
.	O
many	O
problems	O
in	O
machine	O
learning	O
are	O
so	O
diﬃcult	O
that	O
we	O
can	O
never	O
expect	O
to	O
obtain	O
precise	O
answers	O
to	O
them	O
.	O
this	O
excludes	O
precise	O
deterministic	O
algorithms	O
and	O
las	O
vegas	O
algorithms	O
.	O
instead	O
,	O
we	O
must	O
use	O
deterministic	O
approximate	O
algorithms	O
or	O
monte	O
carlo	O
approximations	O
.	O
both	O
approaches	O
are	O
ubiquitous	O
in	O
machine	O
learning	O
.	O
in	O
this	O
chapter	O
,	O
we	O
focus	O
on	O
monte	O
carlo	O
methods	O
.	O
17.1	O
sampling	O
and	O
monte	O
carlo	O
methods	O
many	O
important	O
technologies	O
used	O
to	O
accomplish	O
machine	O
learning	O
goals	O
are	O
based	O
on	O
drawing	O
samples	O
from	O
some	O
probability	O
distribution	O
and	O
using	O
these	O
samples	O
to	O
form	O
a	O
monte	O
carlo	O
estimate	O
of	O
some	O
desired	O
quantity	O
.	O
17.1.1	O
why	O
sampling	O
?	O
there	O
are	O
many	O
reasons	O
that	O
we	O
may	O
wish	O
to	O
draw	O
samples	O
from	O
a	O
probability	O
distribution	O
.	O
sampling	O
provides	O
a	O
ﬂexible	O
way	O
to	O
approximate	O
many	O
sums	O
and	O
590	O
chapter	O
17.	O
monte	O
carlo	O
methods	O
integrals	O
at	O
reduced	O
cost	O
.	O
sometimes	O
we	O
use	O
this	O
to	O
provide	O
a	O
signiﬁcant	O
speedup	O
to	O
a	O
costly	O
but	O
tractable	O
sum	O
,	O
as	O
in	O
the	O
case	O
when	O
we	O
subsample	O
the	O
full	O
training	O
cost	O
with	O
minibatches	O
.	O
in	O
other	O
cases	O
,	O
our	O
learning	O
algorithm	O
requires	O
us	O
to	O
approximate	O
an	O
intractable	O
sum	O
or	O
integral	O
,	O
such	O
as	O
the	O
gradient	O
of	O
the	O
log	O
partition	O
function	O
of	O
an	O
undirected	O
model	B
.	O
in	O
many	O
other	O
cases	O
,	O
sampling	O
is	O
actually	O
our	O
goal	O
,	O
in	O
the	O
sense	O
that	O
we	O
want	O
to	O
train	O
a	O
model	B
that	O
can	O
sample	O
from	O
the	O
training	O
distribution	O
.	O
17.1.2	O
basics	O
of	O
monte	O
carlo	O
sampling	O
when	O
a	O
sum	O
or	O
an	O
integral	O
can	O
not	O
be	O
computed	O
exactly	O
(	O
for	O
example	O
the	O
sum	O
has	O
an	O
exponential	O
number	O
of	O
terms	O
and	O
no	O
exact	O
simpliﬁcation	O
is	O
known	O
)	O
it	O
is	O
often	O
possible	O
to	O
approximate	O
it	O
using	O
monte	O
carlo	O
sampling	O
.	O
the	O
idea	O
is	O
to	O
view	O
the	O
sum	O
or	O
integral	O
as	O
if	O
it	O
was	O
an	O
expectation	O
under	O
some	O
distribution	O
and	O
to	O
approximate	O
the	O
expectation	O
by	O
a	O
corresponding	O
average	O
.	O
let	O
	O
	O
s	O
=	O
p	O
(	O
)	O
x	O
(	O
)	O
=	O
x	O
f	O
e	O
p	O
[	O
(	O
)	O
]	O
f	O
x	O
or	O
s	O
=	O
p	O
(	O
)	O
x	O
(	O
)	O
x	O
x	O
=	O
d	O
f	O
e	O
p	O
[	O
(	O
)	O
]	O
f	O
x	O
x	O
(	O
17.1	O
)	O
(	O
17.2	O
)	O
(	O
17.3	O
)	O
be	O
the	O
sum	O
or	O
integral	O
to	O
estimate	O
,	O
rewritten	O
as	O
an	O
expectation	O
,	O
with	O
the	O
constraint	O
that	O
p	O
is	O
a	O
probability	O
distribution	O
(	O
for	O
the	O
sum	O
)	O
or	O
a	O
probability	O
density	O
(	O
for	O
the	O
integral	O
)	O
over	O
random	O
variable	O
.x	O
we	O
can	O
approximate	O
s	O
by	O
drawing	O
n	O
samples	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
n	O
from	O
p	O
and	O
then	O
	O
forming	O
the	O
empirical	O
average	O
1	O
n	O
n	O
i=1	O
f	O
(	O
x	O
(	O
)	O
i	O
)	O
.	O
	O
ˆsn	O
=	O
	O
this	O
approximation	O
is	O
justiﬁed	O
by	O
a	O
few	O
diﬀerent	O
properties	O
.	O
the	O
ﬁrst	O
trivial	O
observation	O
is	O
that	O
the	O
estimator	O
ˆs	O
is	O
unbiased	O
,	O
since	O
e	O
[	O
ˆsn	O
]	O
=	O
1	O
n	O
n	O
i=1	O
e	O
[	O
(	O
f	O
x	O
(	O
)	O
i	O
)	O
]	O
=	O
1	O
n	O
n	O
i=1	O
s	O
s.=	O
(	O
17.4	O
)	O
but	O
in	O
addition	O
,	O
the	O
law	O
of	O
large	O
numbers	O
states	O
that	O
if	O
the	O
samples	O
x	O
(	O
)	O
i	O
are	O
i.i.d.	O
,	O
then	O
the	O
average	O
converges	O
almost	O
surely	O
to	O
the	O
expected	O
value	O
:	O
→∞	O
ˆsn	O
=	O
s	O
,	O
lim	O
n	O
591	O
(	O
17.5	O
)	O
chapter	O
17.	O
monte	O
carlo	O
methods	O
provided	O
that	O
the	O
variance	O
of	O
the	O
individual	O
terms	O
,	O
var	O
[	O
f	O
(	O
x	O
(	O
)	O
i	O
)	O
]	O
,	O
is	O
bounded	O
.	O
to	O
see	O
this	O
more	O
clearly	O
,	O
consider	O
the	O
variance	O
of	O
ˆsn	O
as	O
n	O
increases	O
.	O
the	O
variance	O
var	O
[	O
ˆsn	O
]	O
decreases	O
and	O
converges	O
to	O
0	O
,	O
so	O
long	O
as	O
var	O
[	O
(	O
f	O
x	O
(	O
)	O
i	O
)	O
]	O
<	O
∞	O
:	O
	O
var	O
[	O
ˆsn	O
]	O
=	O
1	O
n2	O
n	O
var	O
[	O
(	O
)	O
]	O
f	O
x	O
var	O
[	O
(	O
)	O
]	O
i=1	O
f	O
x	O
n	O
.	O
=	O
(	O
17.6	O
)	O
(	O
17.7	O
)	O
this	O
convenient	O
result	O
also	O
tells	O
us	O
how	O
to	O
estimate	O
the	O
uncertainty	O
in	O
a	O
monte	O
carlo	O
average	O
or	O
equivalently	O
the	O
amount	O
of	O
expected	O
error	O
of	O
the	O
monte	O
carlo	O
approximation	O
.	O
we	O
compute	O
both	O
the	O
empirical	O
average	O
of	O
the	O
f	O
(	O
x	O
(	O
)	O
i	O
)	O
and	O
their	O
empirical	O
variance,1	O
and	O
then	O
divide	O
the	O
estimated	O
variance	O
by	O
the	O
number	O
of	O
samples	O
n	O
to	O
obtain	O
an	O
estimator	O
of	O
var	O
[	O
ˆs	O
n	O
]	O
.	O
the	O
central	O
limit	O
theorem	O
tells	O
us	O
that	O
the	O
distribution	O
of	O
the	O
average	O
,	O
ˆsn	O
,	O
converges	O
to	O
a	O
normal	O
distribution	O
with	O
mean	O
s	O
and	O
variance	O
var	O
[	O
(	O
)	O
]	O
f	O
x	O
.	O
this	O
allows	O
us	O
to	O
estimate	O
conﬁdence	O
intervals	O
n	O
around	O
the	O
estimate	O
ˆsn	O
,	O
using	O
the	O
cumulative	O
distribution	O
of	O
the	O
normal	O
density	O
.	O
however	O
,	O
all	O
this	O
relies	O
on	O
our	O
ability	O
to	O
easily	O
sample	O
from	O
the	O
base	O
distribution	O
p	O
(	O
x	O
)	O
,	O
but	O
doing	O
so	O
is	O
not	O
always	O
possible	O
.	O
when	O
it	O
is	O
not	O
feasible	O
to	O
sample	O
from	O
p	O
,	O
an	O
alternative	O
is	O
to	O
use	O
importance	O
sampling	O
,	O
presented	O
in	O
section	O
.	O
a	O
more	O
general	O
approach	O
is	O
to	O
form	O
a	O
sequence	O
of	O
estimators	O
that	O
converge	O
towards	O
the	O
distribution	O
of	O
interest	O
.	O
that	O
is	O
the	O
approach	O
of	O
monte	O
carlo	O
markov	O
chains	O
(	O
section	O
17.2	O
17.3	O
)	O
.	O
17.2	O
importance	O
sampling	O
an	O
important	O
step	O
in	O
the	O
decomposition	O
of	O
the	O
integrand	O
(	O
or	O
summand	O
)	O
used	O
by	O
the	O
monte	O
carlo	O
method	O
in	O
equation	O
is	O
deciding	O
which	O
part	O
of	O
the	O
integrand	O
should	O
play	O
the	O
role	O
the	O
probability	O
p	O
(	O
x	O
)	O
and	O
which	O
part	O
of	O
the	O
integrand	O
should	O
play	O
the	O
role	O
of	O
the	O
quantity	O
f	O
(	O
x	O
)	O
whose	O
expected	O
value	O
(	O
under	O
that	O
probability	O
distribution	O
)	O
is	O
to	O
be	O
estimated	O
.	O
there	O
is	O
no	O
unique	O
decomposition	O
because	O
p	O
(	O
x	O
)	O
f	O
(	O
x	O
)	O
can	O
always	O
be	O
rewritten	O
as	O
17.2	O
p	O
(	O
)	O
x	O
(	O
)	O
=	O
q	O
x	O
f	O
(	O
)	O
x	O
,	O
(	O
17.8	O
)	O
p	O
(	O
)	O
x	O
(	O
)	O
x	O
f	O
q	O
(	O
)	O
x	O
where	O
we	O
now	O
sample	O
from	O
q	O
and	O
average	O
pf	O
.	O
in	O
many	O
cases	O
,	O
we	O
wish	O
to	O
compute	O
q	O
an	O
expectation	O
for	O
a	O
given	O
p	O
and	O
an	O
f	O
,	O
and	O
the	O
fact	O
that	O
the	O
problem	O
is	O
speciﬁed	O
1the	O
unbiased	O
estimator	O
of	O
the	O
variance	O
is	O
often	O
preferred	O
,	O
in	O
which	O
the	O
sum	O
of	O
squared	O
diﬀerences	O
is	O
divided	O
by	O
n	O
−	O
1	O
instead	O
of	O
.	O
n	O
592	O
chapter	O
17.	O
monte	O
carlo	O
methods	O
from	O
the	O
start	O
as	O
an	O
expectation	O
suggests	O
that	O
this	O
p	O
and	O
f	O
would	O
be	O
a	O
natural	O
choice	O
of	O
decomposition	O
.	O
however	O
,	O
the	O
original	O
speciﬁcation	O
of	O
the	O
problem	O
may	O
not	O
be	O
the	O
the	O
optimal	O
choice	O
in	O
terms	O
of	O
the	O
number	O
of	O
samples	O
required	O
to	O
obtain	O
a	O
given	O
level	O
of	O
accuracy	O
.	O
fortunately	O
,	O
the	O
form	O
of	O
the	O
optimal	O
choice	O
q	O
can	O
be	O
derived	O
easily	O
.	O
the	O
optimal	O
q	O
corresponds	O
to	O
what	O
is	O
called	O
optimal	O
importance	O
sampling	O
.	O
∗	O
∗	O
because	O
of	O
the	O
identity	O
shown	O
in	O
equation	O
17.8	O
,	O
any	O
monte	O
carlo	O
estimator	O
	O
n	O
1	O
n	O
	O
ˆsp	O
=	O
f	O
(	O
x	O
(	O
)	O
i	O
)	O
∼	O
p	O
i	O
,	O
=1	O
x	O
(	O
)	O
i	O
can	O
be	O
transformed	O
into	O
an	O
importance	O
sampling	O
estimator	O
ˆsq	O
=	O
1	O
n	O
n	O
p	O
(	O
x	O
(	O
)	O
i	O
)	O
(	O
f	O
x	O
(	O
)	O
i	O
)	O
∼	O
q	O
i	O
,	O
=1	O
x	O
(	O
)	O
i	O
q	O
(	O
x	O
(	O
)	O
i	O
)	O
.	O
we	O
see	O
readily	O
that	O
the	O
expected	O
value	O
of	O
the	O
estimator	O
does	O
not	O
depend	O
on	O
:	O
q	O
eq	O
[	O
ˆsq	O
]	O
=	O
eq	O
[	O
ˆsp	O
]	O
=	O
s.	O
(	O
17.11	O
)	O
however	O
,	O
the	O
variance	O
of	O
an	O
importance	O
sampling	O
estimator	O
can	O
be	O
greatly	O
sensitive	O
to	O
the	O
choice	O
of	O
.	O
the	O
variance	O
is	O
given	O
by	O
q	O
(	O
17.9	O
)	O
(	O
17.10	O
)	O
(	O
17.12	O
)	O
(	O
17.13	O
)	O
var	O
[	O
ˆsq	O
]	O
=	O
var	O
[	O
p	O
(	O
)	O
x	O
(	O
)	O
x	O
f	O
q	O
(	O
)	O
x	O
]	O
/n	O
.	O
the	O
minimum	O
variance	O
occurs	O
when	O
isq	O
∗	O
q	O
(	O
)	O
=x	O
p	O
|	O
f	O
(	O
)	O
x	O
z	O
|	O
(	O
)	O
x	O
,	O
∗	O
where	O
z	O
is	O
the	O
normalization	O
constant	O
,	O
chosen	O
so	O
that	O
q	O
(	O
x	O
)	O
sums	O
or	O
integrates	O
to	O
1	O
as	O
appropriate	O
.	O
better	O
importance	O
sampling	O
distributions	O
put	O
more	O
weight	O
where	O
∗	O
]	O
=	O
0	O
,	O
the	O
integrand	O
is	O
larger	O
.	O
in	O
fact	O
,	O
when	O
f	O
(	O
x	O
)	O
does	O
not	O
change	O
sign	O
,	O
var	O
[	O
ˆsq	O
when	O
the	O
optimal	O
distribution	O
is	O
used	O
.	O
meaning	O
that	O
of	O
course	O
,	O
this	O
is	O
only	O
because	O
the	O
computation	O
of	O
q	O
has	O
essentially	O
solved	O
the	O
original	O
problem	O
,	O
so	O
it	O
is	O
usually	O
not	O
practical	O
to	O
use	O
this	O
approach	O
of	O
drawing	O
a	O
single	O
sample	O
from	O
the	O
optimal	O
distribution	O
.	O
a	O
single	O
sample	O
is	O
suﬃcient	O
∗	O
any	O
choice	O
of	O
sampling	O
distribution	O
q	O
is	O
valid	O
(	O
in	O
the	O
sense	O
of	O
yielding	O
the	O
is	O
the	O
optimal	O
one	O
(	O
in	O
the	O
sense	O
of	O
yielding	O
minimum	O
is	O
usually	O
infeasible	O
,	O
but	O
other	O
choices	O
of	O
q	O
can	O
be	O
∗	O
correct	O
expected	O
value	O
)	O
and	O
q	O
∗	O
variance	O
)	O
.	O
sampling	O
from	O
q	O
feasible	O
while	O
still	O
reducing	O
the	O
variance	O
somewhat	O
.	O
593	O
chapter	O
17.	O
monte	O
carlo	O
methods	O
another	O
approach	O
is	O
to	O
use	O
biased	O
importance	O
sampling	O
,	O
which	O
has	O
the	O
advantage	O
of	O
not	O
requiring	O
normalized	O
p	O
or	O
q.	O
in	O
the	O
case	O
of	O
discrete	O
variables	O
,	O
the	O
biased	O
importance	O
sampling	O
estimator	O
is	O
given	O
by	O
	O
	O
	O
n	O
i=1	O
	O
	O
	O
n	O
i=1	O
n	O
i=1	O
ˆsbis	O
=	O
=	O
=	O
f	O
(	O
x	O
(	O
)	O
i	O
)	O
p	O
(	O
x	O
(	O
)	O
i	O
)	O
q	O
(	O
x	O
(	O
)	O
i	O
)	O
f	O
(	O
x	O
(	O
)	O
i	O
)	O
p	O
(	O
x	O
(	O
)	O
i	O
)	O
q	O
(	O
x	O
(	O
)	O
i	O
)	O
n	O
i=1	O
p	O
(	O
x	O
(	O
)	O
i	O
)	O
˜q	O
(	O
x	O
(	O
)	O
i	O
)	O
n	O
i=1	O
˜p	O
(	O
x	O
(	O
)	O
i	O
)	O
˜q	O
(	O
x	O
(	O
)	O
i	O
)	O
n	O
i=1	O
p	O
(	O
x	O
(	O
)	O
i	O
)	O
˜q	O
(	O
x	O
(	O
)	O
i	O
)	O
˜p	O
(	O
x	O
(	O
)	O
i	O
)	O
˜q	O
(	O
x	O
(	O
)	O
i	O
)	O
f	O
(	O
x	O
(	O
)	O
i	O
)	O
(	O
17.14	O
)	O
(	O
17.15	O
)	O
,	O
(	O
17.16	O
)	O
where	O
˜p	O
and	O
˜q	O
are	O
the	O
unnormalized	O
forms	O
of	O
p	O
and	O
q	O
and	O
the	O
x	O
(	O
)	O
i	O
are	O
the	O
samples	O
→	O
∞	O
=	O
s	O
,	O
except	O
asymptotically	O
when	O
from	O
q.	O
this	O
estimator	O
is	O
biased	O
because	O
e	O
[	O
ˆs	O
bis	O
]	O
converges	O
to	O
1.	O
hence	O
this	O
estimator	O
n	O
is	O
called	O
asymptotically	O
unbiased	O
.	O
and	O
the	O
denominator	O
of	O
equation	O
17.14	O
17.12	O
q	O
for	O
which	O
p	O
|	O
(	O
)	O
x	O
|	O
(	O
)	O
x	O
f	O
q	O
(	O
)	O
x	O
,	O
we	O
see	O
that	O
if	O
there	O
are	O
samples	O
of	O
although	O
a	O
good	O
choice	O
of	O
q	O
can	O
greatly	O
improve	O
the	O
eﬃciency	O
of	O
monte	O
carlo	O
estimation	O
,	O
a	O
poor	O
choice	O
of	O
q	O
can	O
make	O
the	O
eﬃciency	O
much	O
worse	O
.	O
going	O
back	O
to	O
equation	O
is	O
large	O
,	O
then	O
the	O
variance	O
of	O
the	O
estimator	O
can	O
get	O
very	O
large	O
.	O
this	O
may	O
happen	O
when	O
q	O
(	O
x	O
)	O
is	O
tiny	O
while	O
neither	O
p	O
(	O
x	O
)	O
nor	O
f	O
(	O
x	O
)	O
are	O
small	O
enough	O
to	O
cancel	O
it	O
.	O
the	O
q	O
distribution	O
is	O
usually	O
chosen	O
to	O
be	O
a	O
very	O
simple	O
distribution	O
so	O
that	O
it	O
is	O
easy	O
|	O
|	O
to	O
sample	O
from	O
.	O
when	O
x	O
is	O
high-dimensional	O
,	O
this	O
simplicity	O
in	O
q	O
causes	O
it	O
to	O
match	O
p	O
or	O
p	O
f	O
,	O
importance	O
sampling	O
|	O
collects	O
useless	O
samples	O
(	O
summing	O
tiny	O
numbers	O
or	O
zeros	O
)	O
.	O
on	O
the	O
other	O
hand	O
,	O
when	O
,	O
which	O
will	O
happen	O
more	O
rarely	O
,	O
the	O
ratio	O
can	O
be	O
huge	O
.	O
q	O
(	O
x	O
(	O
)	O
i	O
)	O
because	O
these	O
latter	O
events	O
are	O
rare	O
,	O
they	O
may	O
not	O
show	O
up	O
in	O
a	O
typical	O
sample	O
,	O
yielding	O
typical	O
underestimation	O
of	O
s	O
,	O
compensated	O
rarely	O
by	O
gross	O
overestimation	O
.	O
such	O
very	O
large	O
or	O
very	O
small	O
numbers	O
are	O
typical	O
when	O
x	O
is	O
high	O
dimensional	O
,	O
because	O
in	O
high	O
dimension	O
the	O
dynamic	O
range	O
of	O
joint	O
probabilities	O
can	O
be	O
very	O
large	O
.	O
poorly	O
.	O
when	O
q	O
(	O
x	O
(	O
)	O
i	O
)	O
|	O
f	O
(	O
x	O
(	O
)	O
i	O
)	O
|	O
f	O
(	O
x	O
(	O
)	O
i	O
)	O
p	O
(	O
x	O
(	O
)	O
i	O
)	O
p	O
(	O
x	O
(	O
)	O
i	O
)	O
	O
	O
|	O
in	O
spite	O
of	O
this	O
danger	O
,	O
importance	O
sampling	O
and	O
its	O
variants	O
have	O
been	O
found	O
very	O
useful	O
in	O
many	O
machine	O
learning	O
algorithms	O
,	O
including	O
deep	O
learning	O
algorithms	O
.	O
for	O
example	O
,	O
see	O
the	O
use	O
of	O
importance	O
sampling	O
to	O
accelerate	O
training	O
in	O
neural	O
language	O
models	O
with	O
a	O
large	O
vocabulary	O
(	O
section	O
)	O
or	O
other	O
neural	O
nets	O
with	O
a	O
large	O
number	O
of	O
outputs	O
.	O
see	O
also	O
how	O
importance	O
sampling	O
has	O
been	O
used	O
to	O
estimate	O
a	O
partition	O
function	O
(	O
the	O
normalization	O
constant	O
of	O
a	O
probability	O
12.4.3.3	O
594	O
	O
chapter	O
17.	O
monte	O
carlo	O
methods	O
18.7	O
,	O
and	O
to	O
estimate	O
the	O
log-likelihood	O
in	O
deep	O
directed	O
distribution	O
)	O
in	O
section	O
models	O
such	O
as	O
the	O
variational	O
autoencoder	O
,	O
in	O
section	O
.	O
importance	O
sampling	O
may	O
also	O
be	O
used	O
to	O
improve	O
the	O
estimate	O
of	O
the	O
gradient	O
of	O
the	O
cost	O
function	O
used	O
to	O
train	O
model	B
parameters	O
with	O
stochastic	O
gradient	O
descent	B
,	O
particularly	O
for	O
models	O
such	O
as	O
classiﬁers	O
where	O
most	O
of	O
the	O
total	O
value	O
of	O
the	O
cost	O
function	O
comes	O
from	O
a	O
small	O
number	O
of	O
misclassiﬁed	O
examples	O
.	O
sampling	O
more	O
diﬃcult	O
examples	O
more	O
frequently	O
can	O
reduce	O
the	O
variance	O
of	O
the	O
gradient	O
in	O
such	O
cases	O
(	O
hinton	O
2006	O
20.10.3	O
)	O
.	O
,	O
17.3	O
markov	O
chain	O
monte	O
carlo	O
methods	O
in	O
many	O
cases	O
,	O
we	O
wish	O
to	O
use	O
a	O
monte	O
carlo	O
technique	O
but	O
there	O
is	O
no	O
tractable	O
method	O
for	O
drawing	O
exact	O
samples	O
from	O
the	O
distribution	O
pmodel	O
(	O
x	O
)	O
or	O
from	O
a	O
good	O
(	O
low	O
variance	O
)	O
importance	O
sampling	O
distribution	O
q	O
(	O
x	O
)	O
.	O
in	O
the	O
context	O
of	O
deep	O
learning	O
,	O
this	O
most	O
often	O
happens	O
when	O
pmodel	O
(	O
x	O
)	O
is	O
represented	O
by	O
an	O
undirected	O
model	B
.	O
in	O
these	O
cases	O
,	O
we	O
introduce	O
a	O
mathematical	O
tool	O
called	O
a	O
markov	O
chain	O
to	O
approximately	O
sample	O
from	O
pmodel	O
(	O
x	O
)	O
.	O
the	O
family	O
of	O
algorithms	O
that	O
use	O
markov	O
chains	O
to	O
perform	O
monte	O
carlo	O
estimates	O
is	O
called	O
markov	O
chain	O
monte	O
carlo	O
methods	O
(	O
mcmc	O
)	O
.	O
markov	O
chain	O
monte	O
carlo	O
methods	O
for	O
machine	O
learning	O
are	O
described	O
at	O
greater	O
length	O
in	O
koller	O
and	O
friedman	O
2009	O
)	O
.	O
the	O
most	O
standard	O
,	O
generic	O
guarantees	O
for	O
mcmc	O
techniques	O
are	O
only	O
applicable	O
when	O
the	O
model	B
does	O
not	O
assign	O
zero	O
probability	O
to	O
any	O
state	O
.	O
therefore	O
,	O
it	O
is	O
most	O
convenient	O
to	O
present	O
these	O
techniques	O
as	O
sampling	O
from	O
an	O
energy-based	O
model	B
(	O
ebm	O
)	O
p	O
(	O
x	O
)	O
.	O
in	O
the	O
ebm	O
formulation	O
,	O
every	O
state	O
is	O
guaranteed	O
to	O
have	O
non-zero	O
probability	O
.	O
mcmc	O
methods	O
are	O
in	O
fact	O
more	O
broadly	O
applicable	O
and	O
can	O
be	O
used	O
with	O
many	O
probability	O
distributions	O
that	O
contain	O
zero	O
probability	O
states	O
.	O
however	O
,	O
the	O
theoretical	O
guarantees	O
concerning	O
the	O
behavior	O
of	O
mcmc	O
methods	O
must	O
be	O
proven	O
on	O
a	O
case-by-case	O
basis	O
for	O
diﬀerent	O
families	O
of	O
such	O
distributions	O
.	O
in	O
the	O
context	O
of	O
deep	O
learning	O
,	O
it	O
is	O
most	O
common	O
to	O
rely	O
on	O
the	O
most	O
general	O
theoretical	O
guarantees	O
that	O
naturally	O
apply	O
to	O
all	O
energy-based	O
models	O
.	O
exp	O
(	O
e	O
(	O
)	O
)	O
x	O
as	O
described	O
in	O
section	O
16.2.4	O
∝	O
−	O
(	O
|	O
to	O
understand	O
why	O
drawing	O
samples	O
from	O
an	O
energy-based	O
model	B
is	O
diﬃcult	O
,	O
a	O
b	O
.	O
in	O
order	O
consider	O
an	O
ebm	O
over	O
just	O
two	O
variables	O
,	O
deﬁning	O
a	O
distribution	O
to	O
sample	O
a	O
,	O
we	O
must	O
draw	O
a	O
from	O
p	O
(	O
a	O
b	O
)	O
,	O
and	O
in	O
order	O
to	O
sample	O
b	O
,	O
we	O
must	O
draw	O
it	O
from	O
p	O
(	O
b	O
)	O
.	O
it	O
seems	O
to	O
be	O
an	O
intractable	O
chicken-and-egg	O
problem	O
.	O
directed	O
models	O
avoid	O
this	O
because	O
their	O
graph	O
is	O
directed	O
and	O
acyclic	O
.	O
to	O
perform	O
ancestral	O
sampling	O
one	O
simply	O
samples	O
each	O
of	O
the	O
variables	O
in	O
topological	O
order	O
,	O
conditioning	O
on	O
each	O
variable	O
’	O
s	O
parents	O
,	O
which	O
are	O
guaranteed	O
to	O
have	O
already	O
been	O
sampled	O
(	O
section	O
)	O
.	O
ancestral	O
sampling	O
deﬁnes	O
an	O
eﬃcient	O
,	O
single-pass	O
method	O
p	O
(	O
,	O
)	O
16.3	O
a	O
|	O
595	O
chapter	O
17.	O
monte	O
carlo	O
methods	O
of	O
obtaining	O
a	O
sample	O
.	O
in	O
an	O
ebm	O
,	O
we	O
can	O
avoid	O
this	O
chicken	O
and	O
egg	O
problem	O
by	O
sampling	O
using	O
a	O
markov	O
chain	O
.	O
the	O
core	O
idea	O
of	O
a	O
markov	O
chain	O
is	O
to	O
have	O
a	O
state	O
x	O
that	O
begins	O
as	O
an	O
arbitrary	O
value	O
.	O
over	O
time	O
,	O
we	O
randomly	O
update	O
x	O
repeatedly	O
.	O
eventually	O
x	O
becomes	O
(	O
very	O
nearly	O
)	O
a	O
fair	O
sample	O
from	O
p	O
(	O
x	O
)	O
.	O
formally	O
,	O
a	O
markov	O
chain	O
is	O
deﬁned	O
by	O
a	O
random	O
state	O
x	O
and	O
a	O
transition	O
distribution	O
t	O
(	O
x	O
x	O
)	O
specifying	O
the	O
probability	O
that	O
a	O
random	O
update	O
will	O
go	O
to	O
state	O
x	O
if	O
it	O
starts	O
in	O
state	O
x	O
.	O
	O
running	O
the	O
markov	O
chain	O
means	O
repeatedly	O
updating	O
the	O
state	O
x	O
to	O
a	O
value	O
x	O
sampled	O
from	O
t	O
(	O
x	O
	O
|	O
	O
|	O
x	O
)	O
.	O
	O
to	O
gain	O
some	O
theoretical	O
understanding	O
of	O
how	O
mcmc	O
methods	O
work	B
,	O
it	O
is	O
useful	O
to	O
reparametrize	O
the	O
problem	O
.	O
first	O
,	O
we	O
restrict	O
our	O
attention	O
to	O
the	O
case	O
where	O
the	O
random	O
variable	O
x	O
has	O
countably	O
many	O
states	O
.	O
we	O
can	O
then	O
represent	O
the	O
state	O
as	O
just	O
a	O
positive	O
integer	O
x.	O
diﬀerent	O
integer	O
values	O
of	O
x	O
map	O
back	O
to	O
diﬀerent	O
states	O
in	O
the	O
original	O
problem	O
.	O
x	O
consider	O
what	O
happens	O
when	O
we	O
run	O
inﬁnitely	O
many	O
markov	O
chains	O
in	O
parallel	O
.	O
all	O
of	O
the	O
states	O
of	O
the	O
diﬀerent	O
markov	O
chains	O
are	O
drawn	O
from	O
some	O
distribution	O
q	O
(	O
)	O
t	O
(	O
x	O
)	O
,	O
where	O
t	O
indicates	O
the	O
number	O
of	O
time	O
steps	O
that	O
have	O
elapsed	O
.	O
at	O
the	O
beginning	O
,	O
q	O
(	O
0	O
)	O
is	O
some	O
distribution	O
that	O
we	O
used	O
to	O
arbitrarily	O
initialize	O
x	O
for	O
each	O
markov	O
chain	O
.	O
later	O
,	O
q	O
(	O
)	O
t	O
is	O
inﬂuenced	O
by	O
all	O
of	O
the	O
markov	O
chain	O
steps	O
that	O
have	O
run	O
so	O
far	O
.	O
our	O
goal	O
is	O
for	O
q	O
(	O
)	O
t	O
(	O
)	O
x	O
to	O
converge	O
to	O
.	O
p	O
x	O
(	O
)	O
because	O
we	O
have	O
reparametrized	O
the	O
problem	O
in	O
terms	O
of	O
positive	O
integer	O
x	O
,	O
we	O
can	O
describe	O
the	O
probability	O
distribution	O
q	O
using	O
a	O
vector	O
v	O
,	O
with	O
q	O
(	O
=	O
x	O
i	O
)	O
=	O
i.	O
v	O
	O
(	O
17.17	O
)	O
	O
new	O
state	O
x	O
consider	O
what	O
happens	O
when	O
we	O
update	O
a	O
single	O
markov	O
chain	O
’	O
s	O
state	O
x	O
to	O
a	O
	O
.	O
the	O
probability	O
of	O
a	O
single	O
state	O
landing	O
in	O
state	O
x	O
is	O
given	O
by	O
t	O
q	O
(	O
+1	O
)	O
(	O
x	O
	O
)	O
=	O
x	O
	O
|	O
q	O
(	O
)	O
t	O
(	O
)	O
(	O
x	O
t	O
x	O
x	O
.	O
)	O
(	O
17.18	O
)	O
using	O
our	O
integer	O
parametrization	O
,	O
we	O
can	O
represent	O
the	O
eﬀect	O
of	O
the	O
transition	O
operator	O
t	O
using	O
a	O
matrix	O
a	O
.	O
we	O
deﬁne	O
	O
ai	O
,	O
j	O
=	O
(	O
t	O
x	O
a	O
=	O
i	O
so	O
that	O
|	O
x	O
=	O
)	O
j	O
.	O
(	O
17.19	O
)	O
.	O
rather	O
than	O
writing	O
it	O
in	O
using	O
this	O
deﬁnition	O
,	O
we	O
can	O
now	O
rewrite	O
equation	O
terms	O
of	O
q	O
and	O
t	O
to	O
understand	O
how	O
a	O
single	O
state	O
is	O
updated	O
,	O
we	O
may	O
now	O
use	O
v	O
and	O
a	O
to	O
describe	O
how	O
the	O
entire	O
distribution	O
over	O
all	O
the	O
diﬀerent	O
markov	O
chains	O
(	O
running	O
in	O
parallel	O
)	O
shifts	O
as	O
we	O
apply	O
an	O
update	O
:	O
17.18	O
−	O
1	O
)	O
t	O
v	O
(	O
)	O
t	O
=	O
av	O
(	O
596	O
.	O
(	O
17.20	O
)	O
chapter	O
17.	O
monte	O
carlo	O
methods	O
applying	O
the	O
markov	O
chain	O
update	O
repeatedly	O
corresponds	O
to	O
multiplying	O
by	O
the	O
matrix	O
a	O
repeatedly	O
.	O
in	O
other	O
words	O
,	O
we	O
can	O
think	O
of	O
the	O
process	O
as	O
exponentiating	O
the	O
matrix	O
:	O
a	O
v	O
(	O
)	O
t	O
=	O
atv	O
(	O
0	O
)	O
.	O
(	O
17.21	O
)	O
the	O
matrix	O
a	O
has	O
special	O
structure	O
because	O
each	O
of	O
its	O
columns	O
represents	O
a	O
probability	O
distribution	O
.	O
such	O
matrices	O
are	O
called	O
stochastic	O
matrices	O
.	O
if	O
there	O
	O
is	O
a	O
non-zero	O
probability	O
of	O
transitioning	O
from	O
any	O
state	O
x	O
to	O
any	O
other	O
state	O
x	O
for	O
some	O
power	O
t	O
,	O
then	O
the	O
perron-frobenius	O
theorem	O
(	O
perron	O
1907	O
frobenius	O
1908	O
)	O
guarantees	O
that	O
the	O
largest	O
eigenvalue	O
is	O
real	O
and	O
equal	O
to	O
.	O
over	O
time	O
,	O
we	O
can	O
see	O
that	O
all	O
of	O
the	O
eigenvalues	O
are	O
exponentiated	O
:	O
	O
	O
1	O
,	O
;	O
,	O
v	O
(	O
)	O
t	O
=	O
v	O
diag	O
(	O
)	O
λ	O
v	O
−	O
1	O
t	O
v	O
(	O
0	O
)	O
=	O
v	O
diag	O
λ	O
tv	O
(	O
)	O
−	O
1v	O
(	O
0	O
)	O
.	O
(	O
17.22	O
)	O
this	O
process	O
causes	O
all	O
of	O
the	O
eigenvalues	O
that	O
are	O
not	O
equal	O
to	O
to	O
decay	O
to	O
zero	O
.	O
under	O
some	O
additional	O
mild	O
conditions	B
,	O
a	O
is	O
guaranteed	O
to	O
have	O
only	O
one	O
eigenvector	O
stationary	O
distribution	O
,	O
with	O
eigenvalue	O
sometimes	O
also	O
called	O
the	O
.	O
at	O
convergence	O
,	O
.	O
the	O
process	O
thus	O
converges	O
to	O
a	O
equilibrium	O
distribution	O
1	O
1	O
	O
v	O
=	O
=	O
av	O
v	O
,	O
(	O
17.23	O
)	O
and	O
this	O
same	O
condition	O
holds	O
for	O
every	O
additional	O
step	O
.	O
this	O
is	O
an	O
eigenvector	O
equation	O
.	O
to	O
be	O
a	O
stationary	O
point	O
,	O
v	O
must	O
be	O
an	O
eigenvector	O
with	O
corresponding	O
eigenvalue	O
.	O
this	O
condition	O
guarantees	O
that	O
once	O
we	O
have	O
reached	O
the	O
stationary	O
distribution	O
,	O
repeated	O
applications	O
of	O
the	O
transition	O
sampling	O
procedure	O
do	O
not	O
change	O
the	O
over	O
the	O
states	O
of	O
all	O
the	O
various	O
markov	O
chains	O
(	O
although	O
transition	O
operator	O
does	O
change	O
each	O
individual	O
state	O
,	O
of	O
course	O
)	O
.	O
distribution	O
1	O
if	O
we	O
have	O
chosen	O
t	O
correctly	O
,	O
then	O
the	O
stationary	O
distribution	O
q	O
will	O
be	O
equal	O
to	O
the	O
distribution	O
p	O
we	O
wish	O
to	O
sample	O
from	O
.	O
we	O
will	O
describe	O
how	O
to	O
choose	O
t	O
shortly	O
,	O
in	O
section	O
.	O
17.4	O
most	O
properties	O
of	O
markov	O
chains	O
with	O
countable	O
states	O
can	O
be	O
generalized	O
to	O
continuous	O
variables	O
.	O
in	O
this	O
situation	O
,	O
some	O
authors	O
call	O
the	O
markov	O
chain	O
a	O
harris	O
chain	O
but	O
we	O
use	O
the	O
term	O
markov	O
chain	O
to	O
describe	O
both	O
conditions	B
.	O
in	O
general	O
,	O
a	O
markov	O
chain	O
with	O
transition	O
operator	O
t	O
will	O
converge	O
,	O
under	O
mild	O
conditions	B
,	O
to	O
a	O
ﬁxed	O
point	O
described	O
by	O
the	O
equation	O
	O
	O
(	O
x	O
q	O
∼	O
qt	O
(	O
x	O
)	O
=	O
ex	O
	O
|	O
x	O
)	O
,	O
(	O
17.24	O
)	O
x	O
is	O
discrete	O
,	O
which	O
in	O
the	O
discrete	O
case	O
is	O
just	O
rewriting	O
equation	O
the	O
expectation	O
corresponds	O
to	O
a	O
sum	O
,	O
and	O
when	O
x	O
is	O
continuous	O
,	O
the	O
expectation	O
corresponds	O
to	O
an	O
integral	O
.	O
.	O
when	O
17.23	O
597	O
chapter	O
17.	O
monte	O
carlo	O
methods	O
regardless	O
of	O
whether	O
the	O
state	O
is	O
continuous	O
or	O
discrete	O
,	O
all	O
markov	O
chain	O
methods	O
consist	O
of	O
repeatedly	O
applying	O
stochastic	O
updates	O
until	O
eventually	O
the	O
state	O
begins	O
to	O
yield	O
samples	O
from	O
the	O
equilibrium	O
distribution	O
.	O
running	O
the	O
markov	O
chain	O
until	O
it	O
reaches	O
its	O
equilibrium	O
distribution	O
is	O
called	O
“	O
burning	O
in	O
”	O
the	O
markov	O
chain	O
.	O
after	O
the	O
chain	O
has	O
reached	O
equilibrium	O
,	O
a	O
sequence	O
of	O
inﬁnitely	O
many	O
samples	O
may	O
be	O
drawn	O
from	O
from	O
the	O
equilibrium	O
distribution	O
.	O
they	O
are	O
identically	O
distributed	O
but	O
any	O
two	O
successive	O
samples	O
will	O
be	O
highly	O
correlated	O
with	O
each	O
other	O
.	O
a	O
ﬁnite	O
sequence	O
of	O
samples	O
may	O
thus	O
not	O
be	O
very	O
representative	O
of	O
the	O
equilibrium	O
distribution	O
.	O
one	O
way	O
to	O
mitigate	O
this	O
problem	O
is	O
to	O
return	O
only	O
every	O
n	O
successive	O
samples	O
,	O
so	O
that	O
our	O
estimate	O
of	O
the	O
statistics	O
of	O
the	O
equilibrium	O
distribution	O
is	O
not	O
as	O
biased	O
by	O
the	O
correlation	O
between	O
an	O
mcmc	O
sample	O
and	O
the	O
next	O
several	O
samples	O
.	O
markov	O
chains	O
are	O
thus	O
expensive	O
to	O
use	O
because	O
of	O
the	O
time	O
required	O
to	O
burn	O
in	O
to	O
the	O
equilibrium	O
distribution	O
and	O
the	O
time	O
required	O
to	O
transition	O
from	O
one	O
sample	O
to	O
another	O
reasonably	O
decorrelated	O
sample	O
after	O
reaching	O
equilibrium	O
.	O
if	O
one	O
desires	O
truly	O
independent	O
samples	O
,	O
one	O
can	O
run	O
multiple	O
markov	O
chains	O
in	O
parallel	O
.	O
this	O
approach	O
uses	O
extra	O
parallel	O
computation	O
to	O
eliminate	O
latency	O
.	O
the	O
strategy	O
of	O
using	O
only	O
a	O
single	O
markov	O
chain	O
to	O
generate	O
all	O
samples	O
and	O
the	O
strategy	O
of	O
using	O
one	O
markov	O
chain	O
for	O
each	O
desired	O
sample	O
are	O
two	O
extremes	O
;	O
deep	O
learning	O
practitioners	O
usually	O
use	O
a	O
number	O
of	O
chains	O
that	O
is	O
similar	O
to	O
the	O
number	O
of	O
examples	O
in	O
a	O
minibatch	O
and	O
then	O
draw	O
as	O
many	O
samples	O
as	O
are	O
needed	O
from	O
this	O
ﬁxed	O
set	O
of	O
markov	O
chains	O
.	O
a	O
commonly	O
used	O
number	O
of	O
markov	O
chains	O
is	O
100.	O
another	O
diﬃculty	O
is	O
that	O
we	O
do	O
not	O
know	O
in	O
advance	O
how	O
many	O
steps	O
the	O
markov	O
chain	O
must	O
run	O
before	O
reaching	O
its	O
equilibrium	O
distribution	O
.	O
this	O
length	O
of	O
time	O
is	O
called	O
the	O
mixing	O
time	O
.	O
it	O
is	O
also	O
very	O
diﬃcult	O
to	O
test	O
whether	O
a	O
markov	O
chain	O
has	O
reached	O
equilibrium	O
.	O
we	O
do	O
not	O
have	O
a	O
precise	O
enough	O
theory	O
for	O
guiding	O
us	O
in	O
answering	O
this	O
question	O
.	O
theory	O
tells	O
us	O
that	O
the	O
chain	O
will	O
converge	O
,	O
but	O
not	O
much	O
more	O
.	O
if	O
we	O
analyze	O
the	O
markov	O
chain	O
from	O
the	O
point	O
of	O
view	O
of	O
a	O
matrix	O
a	O
acting	O
on	O
a	O
vector	O
of	O
probabilities	O
v	O
,	O
then	O
we	O
know	O
that	O
the	O
chain	O
mixes	O
when	O
at	O
has	O
eﬀectively	O
lost	O
all	O
of	O
the	O
eigenvalues	O
from	O
a	O
besides	O
the	O
unique	O
eigenvalue	O
of	O
.1	O
this	O
means	O
that	O
the	O
magnitude	O
of	O
the	O
second	O
largest	O
eigenvalue	O
will	O
determine	O
the	O
mixing	O
time	O
.	O
however	O
,	O
in	O
practice	O
,	O
we	O
can	O
not	O
actually	O
represent	O
our	O
markov	O
chain	O
in	O
terms	O
of	O
a	O
matrix	O
.	O
the	O
number	O
of	O
states	O
that	O
our	O
probabilistic	O
model	B
can	O
visit	O
is	O
exponentially	O
large	O
in	O
the	O
number	O
of	O
variables	O
,	O
so	O
it	O
is	O
infeasible	O
to	O
represent	O
v	O
,	O
a	O
,	O
or	O
the	O
eigenvalues	O
of	O
a.	O
due	O
to	O
these	O
and	O
other	O
obstacles	O
,	O
we	O
usually	O
do	O
not	O
know	O
whether	O
a	O
markov	O
chain	O
has	O
mixed	O
.	O
instead	O
,	O
we	O
simply	O
run	O
the	O
markov	O
chain	O
for	O
an	O
amount	O
of	O
time	O
that	O
we	O
roughly	O
estimate	O
to	O
be	O
suﬃcient	O
,	O
and	O
use	O
heuristic	O
methods	O
to	O
determine	O
whether	O
the	O
chain	O
has	O
mixed	O
.	O
these	O
heuristic	O
methods	O
include	O
manually	O
inspecting	O
samples	O
or	O
measuring	O
correlations	O
between	O
598	O
chapter	O
17.	O
monte	O
carlo	O
methods	O
successive	O
samples	O
.	O
17.4	O
gibbs	O
sampling	O
←	O
	O
∼	O
x	O
	O
|	O
t	O
(	O
x	O
so	O
far	O
we	O
have	O
described	O
how	O
to	O
draw	O
samples	O
from	O
a	O
distribution	O
q	O
(	O
x	O
)	O
by	O
repeatedly	O
updating	O
x	O
x	O
)	O
.	O
however	O
,	O
we	O
have	O
not	O
described	O
how	O
to	O
ensure	O
that	O
q	O
(	O
x	O
)	O
is	O
a	O
useful	O
distribution	O
.	O
two	O
basic	O
approaches	O
are	O
considered	O
in	O
this	O
book	O
.	O
the	O
ﬁrst	O
one	O
is	O
to	O
derive	O
t	O
from	O
a	O
given	O
learned	O
pmodel	O
,	O
described	O
below	O
with	O
the	O
case	O
of	O
sampling	O
from	O
ebms	O
.	O
the	O
second	O
one	O
is	O
to	O
directly	O
parametrize	O
t	O
and	O
learn	O
it	O
,	O
so	O
that	O
its	O
stationary	O
distribution	O
implicitly	O
deﬁnes	O
the	O
pmodel	O
of	O
interest	O
.	O
examples	O
of	O
this	O
second	O
approach	O
are	O
discussed	O
in	O
sections	O
.	O
20.13	O
20.12	O
and	O
in	O
the	O
context	O
of	O
deep	O
learning	O
,	O
we	O
commonly	O
use	O
markov	O
chains	O
to	O
draw	O
samples	O
from	O
an	O
energy-based	O
model	B
deﬁning	O
a	O
distribution	O
pmodel	O
(	O
x	O
)	O
.	O
in	O
this	O
case	O
,	O
	O
|	O
we	O
want	O
the	O
q	O
(	O
x	O
)	O
for	O
the	O
markov	O
chain	O
to	O
be	O
pmodel	O
(	O
x	O
)	O
.	O
to	O
obtain	O
the	O
desired	O
q	O
(	O
)	O
x	O
,	O
we	O
must	O
choose	O
an	O
appropriate	O
t	O
(	O
x	O
x	O
)	O
.	O
g	O
a	O
conceptually	O
simple	O
and	O
eﬀective	O
approach	O
to	O
building	O
a	O
markov	O
chain	O
	O
|	O
that	O
samples	O
from	O
pmodel	O
(	O
x	O
)	O
is	O
to	O
use	O
gibbs	O
sampling	O
,	O
in	O
which	O
sampling	O
from	O
x	O
)	O
is	O
accomplished	O
by	O
selecting	O
one	O
variable	O
xi	O
and	O
sampling	O
it	O
from	O
pmodel	O
t	O
(	O
x	O
conditioned	O
on	O
its	O
neighbors	O
in	O
the	O
undirected	O
graph	O
deﬁning	O
the	O
structure	O
of	O
the	O
energy-based	O
model	B
.	O
it	O
is	O
also	O
possible	O
to	O
sample	O
several	O
variables	O
at	O
the	O
same	O
time	O
so	O
long	O
as	O
they	O
are	O
conditionally	O
independent	O
given	O
all	O
of	O
their	O
neighbors	O
.	O
as	O
shown	O
in	O
the	O
rbm	O
example	O
in	O
section	O
,	O
all	O
of	O
the	O
hidden	O
units	O
of	O
an	O
rbm	O
may	O
be	O
sampled	O
simultaneously	O
because	O
they	O
are	O
conditionally	O
independent	O
from	O
each	O
other	O
given	O
all	O
of	O
the	O
visible	O
units	O
.	O
likewise	O
,	O
all	O
of	O
the	O
visible	O
units	O
may	O
be	O
sampled	O
simultaneously	O
because	O
they	O
are	O
conditionally	O
independent	O
from	O
each	O
other	O
given	O
all	O
of	O
the	O
hidden	O
units	O
.	O
gibbs	O
sampling	O
approaches	O
that	O
update	O
many	O
variables	O
simultaneously	O
in	O
this	O
way	O
are	O
called	O
block	O
gibbs	O
sampling	O
.	O
16.7.1	O
alternate	O
approaches	O
to	O
designing	O
markov	O
chains	O
to	O
sample	O
from	O
pmodel	O
are	O
possible	O
.	O
for	O
example	O
,	O
the	O
metropolis-hastings	O
algorithm	O
is	O
widely	O
used	O
in	O
other	O
disciplines	O
.	O
in	O
the	O
context	O
of	O
the	O
deep	O
learning	O
approach	O
to	O
undirected	O
modeling	O
,	O
it	O
is	O
rare	O
to	O
use	O
any	O
approach	O
other	O
than	O
gibbs	O
sampling	O
.	O
improved	O
sampling	O
techniques	O
are	O
one	O
possible	O
research	O
frontier	O
.	O
17.5	O
the	O
challenge	O
of	O
mixing	O
between	O
separated	O
modes	O
the	O
primary	O
diﬃculty	O
involved	O
with	O
mcmc	O
methods	O
is	O
that	O
they	O
have	O
a	O
tendency	O
to	O
mix	O
poorly	O
.	O
ideally	O
,	O
successive	O
samples	O
from	O
a	O
markov	O
chain	O
designed	O
to	O
sample	O
599	O
chapter	O
17.	O
monte	O
carlo	O
methods	O
from	O
p	O
(	O
x	O
)	O
would	O
be	O
completely	O
independent	O
from	O
each	O
other	O
and	O
would	O
visit	O
many	O
diﬀerent	O
regions	O
in	O
x	O
space	O
proportional	O
to	O
their	O
probability	O
.	O
instead	O
,	O
especially	O
in	O
high	O
dimensional	O
cases	O
,	O
mcmc	O
samples	O
become	O
very	O
correlated	O
.	O
we	O
refer	O
to	O
such	O
behavior	O
as	O
slow	O
mixing	O
or	O
even	O
failure	O
to	O
mix	O
.	O
mcmc	O
methods	O
with	O
slow	O
mixing	O
can	O
be	O
seen	O
as	O
inadvertently	O
performing	O
something	O
resembling	O
noisy	O
gradient	O
descent	B
on	O
the	O
energy	O
function	O
,	O
or	O
equivalently	O
noisy	O
hill	O
climbing	O
on	O
the	O
probability	O
,	O
with	O
respect	O
to	O
the	O
state	O
of	O
the	O
chain	O
(	O
the	O
random	O
variables	O
being	O
sampled	O
)	O
.	O
the	O
chain	O
tends	O
to	O
take	O
small	O
steps	O
(	O
in	O
the	O
space	O
of	O
the	O
state	O
of	O
the	O
−	O
markov	O
chain	O
)	O
,	O
from	O
a	O
conﬁguration	O
x	O
(	O
1	O
)	O
t	O
to	O
a	O
conﬁguration	O
x	O
(	O
)	O
t	O
,	O
with	O
the	O
energy	O
−	O
e	O
(	O
x	O
(	O
)	O
t	O
)	O
generally	O
lower	O
or	O
approximately	O
equal	O
to	O
the	O
energy	O
e	O
(	O
x	O
(	O
1	O
)	O
t	O
)	O
,	O
with	O
a	O
preference	O
for	O
moves	O
that	O
yield	O
lower	O
energy	O
conﬁgurations	O
.	O
when	O
starting	O
from	O
a	O
rather	O
improbable	O
conﬁguration	O
(	O
higher	O
energy	O
than	O
the	O
typical	O
ones	O
from	O
p	O
(	O
x	O
)	O
)	O
,	O
the	O
chain	O
tends	O
to	O
gradually	O
reduce	O
the	O
energy	O
of	O
the	O
state	O
and	O
only	O
occasionally	O
move	O
to	O
another	O
mode	O
.	O
once	O
the	O
chain	O
has	O
found	O
a	O
region	O
of	O
low	O
energy	O
(	O
for	O
example	O
,	O
if	O
the	O
variables	O
are	O
pixels	O
in	O
an	O
image	O
,	O
a	O
region	O
of	O
low	O
energy	O
might	O
be	O
a	O
connected	O
manifold	O
of	O
images	O
of	O
the	O
same	O
object	O
)	O
,	O
which	O
we	O
call	O
a	O
mode	O
,	O
the	O
chain	O
will	O
tend	O
to	O
walk	O
around	O
that	O
mode	O
(	O
following	O
a	O
kind	O
of	O
random	O
walk	O
)	O
.	O
once	O
in	O
a	O
while	O
it	O
will	O
step	O
out	O
of	O
that	O
mode	O
and	O
generally	O
return	O
to	O
it	O
or	O
(	O
if	O
it	O
ﬁnds	O
an	O
escape	O
route	O
)	O
move	O
towards	O
another	O
mode	O
.	O
the	O
problem	O
is	O
that	O
successful	O
escape	O
routes	O
are	O
rare	O
for	O
many	O
interesting	O
distributions	O
,	O
so	O
the	O
markov	O
chain	O
will	O
continue	O
to	O
sample	O
the	O
same	O
mode	O
longer	O
than	O
it	O
should	O
.	O
this	O
is	O
very	O
clear	O
when	O
we	O
consider	O
the	O
gibbs	O
sampling	O
algorithm	O
(	O
section	O
17.4	O
)	O
.	O
in	O
this	O
context	O
,	O
consider	O
the	O
probability	O
of	O
going	O
from	O
one	O
mode	O
to	O
a	O
nearby	O
mode	O
within	O
a	O
given	O
number	O
of	O
steps	O
.	O
what	O
will	O
determine	O
that	O
probability	O
is	O
the	O
shape	O
of	O
the	O
“	O
energy	O
barrier	O
”	O
between	O
these	O
modes	O
.	O
transitions	O
between	O
two	O
modes	O
that	O
are	O
separated	O
by	O
a	O
high	O
energy	O
barrier	O
(	O
a	O
region	O
of	O
low	O
probability	O
)	O
are	O
exponentially	O
less	O
likely	O
(	O
in	O
terms	O
of	O
the	O
height	O
of	O
the	O
energy	O
barrier	O
)	O
.	O
this	O
is	O
illustrated	O
in	O
ﬁgure	O
.	O
the	O
problem	O
arises	O
when	O
there	O
are	O
multiple	O
modes	O
with	O
high	O
probability	O
that	O
are	O
separated	O
by	O
regions	O
of	O
low	O
probability	O
,	O
especially	O
when	O
each	O
gibbs	O
sampling	O
step	O
must	O
update	O
only	O
a	O
small	O
subset	O
of	O
variables	O
whose	O
values	O
are	O
largely	O
determined	O
by	O
the	O
other	O
variables	O
.	O
17.1	O
−	O
1	O
1	O
−	O
as	O
a	O
simple	O
example	O
,	O
consider	O
an	O
energy-based	O
model	B
over	O
two	O
variables	O
a	O
and	O
wab	O
b	O
,	O
which	O
are	O
both	O
binary	O
with	O
a	O
sign	O
,	O
taking	O
on	O
values	O
for	O
some	O
large	O
positive	O
number	O
w	O
,	O
then	O
the	O
model	B
expresses	O
a	O
strong	O
belief	O
that	O
a	O
and	O
b	O
have	O
the	O
same	O
sign	O
.	O
consider	O
updating	O
b	O
using	O
a	O
gibbs	O
sampling	O
step	O
with	O
a	O
=	O
1	O
)	O
=	O
σ	O
(	O
w	O
)	O
.	O
a	O
=	O
1.	O
the	O
conditional	O
distribution	O
over	O
b	O
is	O
given	O
by	O
p	O
(	O
b	O
=	O
1	O
−	O
if	O
w	O
is	O
large	O
,	O
the	O
sigmoid	O
saturates	O
,	O
and	O
the	O
probability	O
of	O
also	O
assigning	O
b	O
to	O
be	O
1	O
is	O
close	O
to	O
1.	O
likewise	O
,	O
if	O
a	O
=	O
1	O
is	O
close	O
to	O
1.	O
according	O
to	O
pmodel	O
(	O
a	O
b	O
,	O
)	O
,	O
both	O
signs	O
of	O
both	O
variables	O
are	O
equally	O
likely	O
.	O
−	O
1	O
,	O
the	O
probability	O
of	O
assigning	O
b	O
to	O
be	O
and	O
.	O
if	O
e	O
(	O
a	O
b	O
,	O
)	O
=	O
|	O
600	O
chapter	O
17.	O
monte	O
carlo	O
methods	O
(	O
center	O
)	O
figure	O
17.1	O
:	O
paths	O
followed	O
by	O
gibbs	O
sampling	O
for	O
three	O
distributions	O
,	O
with	O
the	O
markov	O
chain	O
initialized	O
at	O
the	O
mode	O
in	O
both	O
cases	O
.	O
(	O
left	O
)	O
a	O
multivariate	O
normal	O
distribution	O
with	O
two	O
independent	O
variables	O
.	O
gibbs	O
sampling	O
mixes	O
well	O
because	O
the	O
variables	O
are	O
independent	O
.	O
a	O
multivariate	O
normal	O
distribution	O
with	O
highly	O
correlated	O
variables	O
.	O
the	O
correlation	O
between	O
variables	O
makes	O
it	O
diﬃcult	O
for	O
the	O
markov	O
chain	O
to	O
mix	O
.	O
because	O
the	O
update	O
for	O
each	O
variable	O
must	O
be	O
conditioned	O
on	O
the	O
other	O
variable	O
,	O
the	O
correlation	O
reduces	O
the	O
rate	O
at	O
which	O
the	O
markov	O
chain	O
can	O
move	O
away	O
from	O
the	O
starting	O
point	O
.	O
(	O
right	O
)	O
a	O
mixture	O
of	O
gaussians	O
with	O
widely	O
separated	O
modes	O
that	O
are	O
not	O
axis-aligned	O
.	O
gibbs	O
sampling	O
mixes	O
very	O
slowly	O
because	O
it	O
is	O
diﬃcult	O
to	O
change	O
modes	O
while	O
altering	O
only	O
one	O
variable	O
at	O
a	O
time	O
.	O
|	O
according	O
to	O
pmodel	O
(	O
a	O
b	O
that	O
gibbs	O
sampling	O
will	O
only	O
very	O
rarely	O
ﬂip	O
the	O
signs	O
of	O
these	O
variables	O
.	O
)	O
,	O
both	O
variables	O
should	O
have	O
the	O
same	O
sign	O
.	O
this	O
means	O
in	O
more	O
practical	O
scenarios	O
,	O
the	O
challenge	O
is	O
even	O
greater	O
because	O
we	O
care	O
not	O
only	O
about	O
making	O
transitions	O
between	O
two	O
modes	O
but	O
more	O
generally	O
between	O
all	O
the	O
many	O
modes	O
that	O
a	O
real	O
model	B
might	O
contain	O
.	O
if	O
several	O
such	O
transitions	O
are	O
diﬃcult	O
because	O
of	O
the	O
diﬃculty	O
of	O
mixing	O
between	O
modes	O
,	O
then	O
it	O
becomes	O
very	O
expensive	O
to	O
obtain	O
a	O
reliable	O
set	O
of	O
samples	O
covering	O
most	O
of	O
the	O
modes	O
,	O
and	O
convergence	O
of	O
the	O
chain	O
to	O
its	O
stationary	O
distribution	O
is	O
very	O
slow	O
.	O
sometimes	O
this	O
problem	O
can	O
be	O
resolved	O
by	O
ﬁnding	O
groups	O
of	O
highly	O
dependent	O
units	O
and	O
updating	O
all	O
of	O
them	O
simultaneously	O
in	O
a	O
block	O
.	O
unfortunately	O
,	O
when	O
the	O
dependencies	O
are	O
complicated	O
,	O
it	O
can	O
be	O
computationally	O
intractable	O
to	O
draw	O
a	O
sample	O
from	O
the	O
group	O
.	O
after	O
all	O
,	O
the	O
problem	O
that	O
the	O
markov	O
chain	O
was	O
originally	O
introduced	O
to	O
solve	O
is	O
this	O
problem	O
of	O
sampling	O
from	O
a	O
large	O
group	O
of	O
variables	O
.	O
in	O
the	O
context	O
of	O
models	O
with	O
latent	O
variables	O
,	O
which	O
deﬁne	O
a	O
joint	O
distribution	O
)	O
,	O
we	O
often	O
draw	O
samples	O
of	O
x	O
by	O
alternating	O
between	O
sampling	O
from	O
)	O
and	O
sampling	O
from	O
p	O
model	B
(	O
h	O
x	O
)	O
.	O
from	O
the	O
point	O
of	O
view	O
of	O
mixing	O
|	O
pmodel	O
(	O
x	O
h	O
,	O
pmodel	O
(	O
x	O
h	O
|	O
601	O
chapter	O
17.	O
monte	O
carlo	O
methods	O
figure	O
17.2	O
:	O
an	O
illustration	O
of	O
the	O
slow	O
mixing	O
problem	O
in	O
deep	O
probabilistic	O
models	O
.	O
each	O
panel	O
should	O
be	O
read	O
left	O
to	O
right	O
,	O
top	O
to	O
bottom	O
.	O
(	O
left	O
)	O
consecutive	O
samples	O
from	O
gibbs	O
sampling	O
applied	O
to	O
a	O
deep	O
boltzmann	O
machine	O
trained	O
on	O
the	O
mnist	O
dataset	O
.	O
consecutive	O
samples	O
are	O
similar	O
to	O
each	O
other	O
.	O
because	O
the	O
gibbs	O
sampling	O
is	O
performed	O
in	O
a	O
deep	O
graphical	O
model	B
,	O
this	O
similarity	O
is	O
based	O
more	O
on	O
semantic	O
rather	O
than	O
raw	O
visual	O
features	O
,	O
but	O
it	O
is	O
still	O
diﬃcult	O
for	O
the	O
gibbs	O
chain	O
to	O
transition	O
from	O
one	O
mode	O
of	O
the	O
distribution	O
to	O
another	O
,	O
for	O
example	O
by	O
changing	O
the	O
digit	O
identity	O
.	O
consecutive	O
ancestral	O
samples	O
from	O
a	O
generative	O
adversarial	O
network	O
.	O
because	O
ancestral	O
sampling	O
generates	O
each	O
sample	O
independently	O
from	O
the	O
others	O
,	O
there	O
is	O
no	O
mixing	O
problem	O
.	O
(	O
right	O
)	O
|	O
rapidly	O
,	O
we	O
would	O
like	O
pmodel	O
(	O
h	O
x	O
)	O
to	O
have	O
very	O
high	O
entropy	O
.	O
however	O
,	O
from	O
the	O
point	O
of	O
view	O
of	O
learning	O
a	O
useful	O
representation	O
of	O
h	O
,	O
we	O
would	O
like	O
h	O
to	O
encode	O
enough	O
information	O
about	O
x	O
to	O
reconstruct	O
it	O
well	O
,	O
which	O
implies	O
that	O
h	O
and	O
x	O
should	O
have	O
very	O
high	O
mutual	O
information	O
.	O
these	O
two	O
goals	O
are	O
at	O
odds	O
with	O
each	O
other	O
.	O
we	O
often	O
learn	O
generative	O
models	O
that	O
very	O
precisely	O
encode	O
x	O
into	O
h	O
but	O
are	O
not	O
able	O
to	O
mix	O
very	O
well	O
.	O
this	O
situation	O
arises	O
frequently	O
with	O
boltzmann	O
machines—the	O
sharper	O
the	O
distribution	O
a	O
boltzmann	O
machine	O
learns	O
,	O
the	O
harder	O
it	O
is	O
for	O
a	O
markov	O
chain	O
sampling	O
from	O
the	O
model	B
distribution	O
to	O
mix	O
well	O
.	O
this	O
problem	O
is	O
illustrated	O
in	O
ﬁgure	O
17.2	O
.	O
all	O
this	O
could	O
make	O
mcmc	O
methods	O
less	O
useful	O
when	O
the	O
distribution	O
of	O
interest	O
has	O
a	O
manifold	O
structure	O
with	O
a	O
separate	O
manifold	O
for	O
each	O
class	O
:	O
the	O
distribution	O
is	O
concentrated	O
around	O
many	O
modes	O
and	O
these	O
modes	O
are	O
separated	O
by	O
vast	O
regions	O
of	O
high	O
energy	O
.	O
this	O
type	O
of	O
distribution	O
is	O
what	O
we	O
expect	O
in	O
many	O
classiﬁcation	O
problems	O
and	O
would	O
make	O
mcmc	O
methods	O
converge	O
very	O
slowly	O
because	O
of	O
poor	O
mixing	O
between	O
modes	O
.	O
602	O
chapter	O
17.	O
monte	O
carlo	O
methods	O
17.5.1	O
tempering	O
to	O
mix	O
between	O
modes	O
when	O
a	O
distribution	O
has	O
sharp	O
peaks	O
of	O
high	O
probability	O
surrounded	O
by	O
regions	O
of	O
low	O
probability	O
,	O
it	O
is	O
diﬃcult	O
to	O
mix	O
between	O
the	O
diﬀerent	O
modes	O
of	O
the	O
distribution	O
.	O
several	O
techniques	O
for	O
faster	O
mixing	O
are	O
based	O
on	O
constructing	O
alternative	O
versions	O
of	O
the	O
target	O
distribution	O
in	O
which	O
the	O
peaks	O
are	O
not	O
as	O
high	O
and	O
the	O
surrounding	O
valleys	O
are	O
not	O
as	O
low	O
.	O
energy-based	O
models	O
provide	O
a	O
particularly	O
simple	O
way	O
to	O
do	O
so	O
.	O
so	O
far	O
,	O
we	O
have	O
described	O
an	O
energy-based	O
model	B
as	O
deﬁning	O
a	O
probability	O
distribution	O
∝	O
(	O
)	O
x	O
−	O
e	O
exp	O
(	O
p	O
(	O
)	O
)	O
x	O
.	O
(	O
17.25	O
)	O
energy-based	O
models	O
may	O
be	O
augmented	O
with	O
an	O
extra	O
parameter	O
β	O
controlling	O
how	O
sharply	O
peaked	O
the	O
distribution	O
is	O
:	O
∝	O
pβ	O
(	O
)	O
x	O
−	O
(	O
)	O
)	O
exp	O
(	O
βe	O
x	O
.	O
(	O
17.26	O
)	O
the	O
β	O
parameter	O
is	O
often	O
described	O
as	O
being	O
the	O
reciprocal	O
of	O
the	O
temperature	O
,	O
reﬂecting	O
the	O
origin	O
of	O
energy-based	O
models	O
in	O
statistical	O
physics	O
.	O
when	O
the	O
temperature	O
falls	O
to	O
zero	O
and	O
rises	O
to	O
inﬁnity	O
,	O
the	O
energy-based	O
model	B
becomes	O
deterministic	O
.	O
when	O
the	O
temperature	O
rises	O
to	O
inﬁnity	O
and	O
β	O
falls	O
to	O
zero	O
,	O
the	O
distribution	O
(	O
for	O
discrete	O
)	O
becomes	O
uniform	O
.	O
x	O
β	O
typically	O
,	O
a	O
model	B
is	O
trained	O
to	O
be	O
evaluated	O
at	O
β	O
=	O
1.	O
however	O
,	O
we	O
can	O
make	O
use	O
of	O
other	O
temperatures	O
,	O
particularly	O
those	O
where	O
β	O
<	O
1.	O
tempering	O
is	O
a	O
general	O
strategy	O
of	O
mixing	O
between	O
modes	O
of	O
p	O
1	O
rapidly	O
by	O
drawing	O
samples	O
with	O
.	O
β	O
<	O
1	O
,	O
,	O
,	O
iba	O
2001	O
neal	O
1994	O
markov	O
chains	O
based	O
on	O
tempered	O
transitions	O
(	O
)	O
temporarily	O
sample	O
from	O
higher-temperature	O
distributions	O
in	O
order	O
to	O
mix	O
to	O
diﬀerent	O
modes	O
,	O
then	O
resume	O
sampling	O
from	O
the	O
unit	O
temperature	O
distribution	O
.	O
these	O
techniques	O
have	O
been	O
applied	O
to	O
models	O
such	O
as	O
rbms	O
(	O
salakhutdinov	O
2010	O
)	O
.	O
another	O
approach	O
is	O
to	O
use	O
parallel	O
tempering	O
(	O
)	O
,	O
in	O
which	O
the	O
markov	O
chain	O
simulates	O
many	O
diﬀerent	O
states	O
in	O
parallel	O
,	O
at	O
diﬀerent	O
temperatures	O
.	O
the	O
highest	O
temperature	O
states	O
mix	O
slowly	O
,	O
while	O
the	O
lowest	O
temperature	O
states	O
,	O
at	O
temperature	O
1	O
,	O
provide	O
accurate	O
samples	O
from	O
the	O
model	B
.	O
the	O
transition	O
operator	O
includes	O
stochastically	O
swapping	O
states	O
between	O
two	O
diﬀerent	O
temperature	O
levels	O
,	O
so	O
that	O
a	O
suﬃciently	O
high-probability	O
sample	O
from	O
a	O
high-temperature	O
slot	O
can	O
jump	O
into	O
a	O
lower	O
temperature	O
slot	O
.	O
this	O
approach	O
has	O
also	O
been	O
applied	O
to	O
rbms	O
(	O
desjardins	O
et	O
al	O
.	O
)	O
.	O
although	O
tempering	O
is	O
a	O
promising	O
approach	O
,	O
at	O
,	O
this	O
point	O
it	O
has	O
not	O
allowed	O
researchers	O
to	O
make	O
a	O
strong	O
advance	O
in	O
solving	O
the	O
challenge	O
of	O
sampling	O
from	O
complex	O
ebms	O
.	O
one	O
possible	O
reason	O
is	O
that	O
there	O
are	O
critical	O
temperatures	O
around	O
which	O
the	O
temperature	O
transition	O
must	O
be	O
very	O
slow	O
(	O
as	O
the	O
temperature	O
is	O
gradually	O
reduced	O
)	O
in	O
order	O
for	O
tempering	O
to	O
be	O
eﬀective	O
.	O
2010	O
cho	O
;	O
et	O
al	O
.	O
,	O
2010	O
603	O
chapter	O
17.	O
monte	O
carlo	O
methods	O
17.5.2	O
depth	O
may	O
help	O
mixing	O
|	O
|	O
x	O
h	O
into	O
)	O
encodes	O
x	O
too	O
well	O
,	O
then	O
sampling	O
from	O
p	O
(	O
x	O
h	O
)	O
,	O
we	O
have	O
seen	O
that	O
if	O
when	O
drawing	O
samples	O
from	O
a	O
latent	O
variable	O
model	B
p	O
(	O
h	O
x	O
,	O
p	O
(	O
h	O
x	O
)	O
will	O
not	O
change	O
x	O
very	O
much	O
and	O
mixing	O
will	O
be	O
poor	O
.	O
one	O
way	O
to	O
resolve	O
this	O
problem	O
is	O
to	O
make	O
h	O
be	O
a	O
deep	O
representation	O
,	O
that	O
encodes	O
in	O
such	O
a	O
way	O
that	O
a	O
markov	O
chain	O
in	O
the	O
space	O
of	O
h	O
can	O
mix	O
more	O
easily	O
.	O
many	O
representation	O
learning	O
algorithms	O
,	O
such	O
as	O
autoencoders	O
and	O
rbms	O
,	O
tend	O
to	O
yield	O
a	O
marginal	O
distribution	O
over	O
h	O
that	O
is	O
more	O
uniform	O
and	O
more	O
unimodal	O
than	O
the	O
original	O
data	O
distribution	O
over	O
x.	O
it	O
can	O
be	O
argued	O
that	O
this	O
arises	O
from	O
trying	O
to	O
minimize	O
reconstruction	O
error	O
while	O
using	O
all	O
of	O
the	O
available	O
representation	O
space	O
,	O
because	O
minimizing	O
reconstruction	O
error	O
over	O
the	O
training	O
examples	O
will	O
be	O
better	O
achieved	O
when	O
diﬀerent	O
training	O
examples	O
are	O
easily	O
distinguishable	O
from	O
each	O
other	O
in	O
h-space	O
,	O
and	O
thus	O
well	O
separated	O
.	O
bengio	O
)	O
observed	O
that	O
deeper	O
stacks	O
of	O
regularized	O
autoencoders	O
or	O
rbms	O
yield	O
marginal	O
distributions	O
in	O
the	O
top-level	O
h-space	O
that	O
appeared	O
more	O
spread	O
out	O
and	O
more	O
uniform	O
,	O
with	O
less	O
of	O
a	O
gap	O
between	O
the	O
regions	O
corresponding	O
to	O
diﬀerent	O
modes	O
(	O
categories	O
,	O
in	O
the	O
experiments	O
)	O
.	O
training	O
an	O
rbm	O
in	O
that	O
higher-level	O
space	O
allowed	O
gibbs	O
sampling	O
to	O
mix	O
faster	O
between	O
modes	O
.	O
it	O
remains	O
however	O
unclear	O
how	O
to	O
exploit	O
this	O
observation	O
to	O
help	O
better	O
train	O
and	O
sample	O
from	O
deep	O
generative	O
models	O
.	O
et	O
al	O
.	O
(	O
2013a	O
despite	O
the	O
diﬃculty	O
of	O
mixing	O
,	O
monte	O
carlo	O
techniques	O
are	O
useful	O
and	O
are	O
often	O
the	O
best	O
tool	O
available	O
.	O
indeed	O
,	O
they	O
are	O
the	O
primary	O
tool	O
used	O
to	O
confront	O
the	O
intractable	O
partition	O
function	O
of	O
undirected	O
models	O
,	O
discussed	O
next	O
.	O
604	O
chapter	O
18	O
confronting	O
the	O
partition	O
function	O
16.2.2	O
in	O
section	O
we	O
saw	O
that	O
many	O
probabilistic	O
models	O
(	O
commonly	O
known	O
as	O
undi-	O
rected	O
graphical	O
models	O
)	O
are	O
deﬁned	O
by	O
an	O
unnormalized	O
probability	O
distribution	O
˜p	O
(	O
x	O
;	O
θ	O
)	O
.	O
we	O
must	O
normalize	O
˜p	O
by	O
dividing	O
by	O
a	O
partition	O
function	O
z	O
(	O
θ	O
)	O
in	O
order	O
to	O
obtain	O
a	O
valid	O
probability	O
distribution	O
:	O
p	O
(	O
;	O
)	O
=	O
x	O
θ	O
˜p	O
(	O
;	O
)	O
x	O
θ	O
.	O
(	O
18.1	O
)	O
the	O
partition	O
function	O
is	O
an	O
integral	O
(	O
for	O
continuous	O
variables	O
)	O
or	O
sum	O
(	O
for	O
discrete	O
variables	O
)	O
over	O
the	O
unnormalized	O
probability	O
of	O
all	O
states	O
:	O
1	O
z	O
(	O
)	O
θ	O
	O
	O
˜p	O
(	O
)	O
x	O
x	O
d	O
or	O
˜p	O
.	O
(	O
)	O
x	O
x	O
this	O
operation	O
is	O
intractable	O
for	O
many	O
interesting	O
models	O
.	O
(	O
18.2	O
)	O
(	O
18.3	O
)	O
20	O
as	O
we	O
will	O
see	O
in	O
chapter	O
,	O
several	O
deep	O
learning	O
models	O
are	O
designed	O
to	O
have	O
a	O
tractable	O
normalizing	O
constant	O
,	O
or	O
are	O
designed	O
to	O
be	O
used	O
in	O
ways	O
that	O
do	O
not	O
involve	O
computing	O
p	O
(	O
x	O
)	O
at	O
all	O
.	O
however	O
,	O
other	O
models	O
directly	O
confront	O
the	O
challenge	O
of	O
intractable	O
partition	O
functions	O
.	O
in	O
this	O
chapter	O
,	O
we	O
describe	O
techniques	O
used	O
for	O
training	O
and	O
evaluating	O
models	O
that	O
have	O
intractable	O
partition	O
functions	O
.	O
605	O
chapter	O
18.	O
confronting	O
the	O
partition	O
function	O
18.1	O
the	O
log-likelihood	O
gradient	O
what	O
makes	O
learning	O
undirected	O
models	O
by	O
maximum	O
likelihood	O
particularly	O
diﬃcult	O
is	O
that	O
the	O
partition	O
function	O
depends	O
on	O
the	O
parameters	O
.	O
the	O
gradient	O
of	O
the	O
log-likelihood	O
with	O
respect	O
to	O
the	O
parameters	O
has	O
a	O
term	O
corresponding	O
to	O
the	O
gradient	O
of	O
the	O
partition	O
function	O
:	O
∇	O
∇	O
θ	O
log	O
˜p	O
(	O
;	O
)	O
x	O
θ	O
θ	O
log	O
(	O
;	O
)	O
=	O
p	O
x	O
θ	O
−	O
∇	O
θ	O
log	O
(	O
)	O
z	O
θ	O
.	O
(	O
18.4	O
)	O
this	O
is	O
a	O
well-known	O
decomposition	O
into	O
the	O
positive	O
phase	O
and	O
negative	O
phase	O
of	O
learning	O
.	O
for	O
most	O
undirected	O
models	O
of	O
interest	O
,	O
the	O
negative	O
phase	O
is	O
diﬃcult	O
.	O
models	O
with	O
no	O
latent	O
variables	O
or	O
with	O
few	O
interactions	O
between	O
latent	O
variables	O
typically	O
have	O
a	O
tractable	O
positive	O
phase	O
.	O
the	O
quintessential	O
example	O
of	O
a	O
model	B
with	O
a	O
straightforward	O
positive	O
phase	O
and	O
diﬃcult	O
negative	O
phase	O
is	O
the	O
rbm	O
,	O
which	O
has	O
hidden	O
units	O
that	O
are	O
conditionally	O
independent	O
from	O
each	O
other	O
given	O
the	O
visible	O
units	O
.	O
the	O
case	O
where	O
the	O
positive	O
phase	O
is	O
diﬃcult	O
,	O
with	O
complicated	O
interactions	O
between	O
latent	O
variables	O
,	O
is	O
primarily	O
covered	O
in	O
chapter	O
.	O
this	O
chapter	O
focuses	O
on	O
the	O
diﬃculties	O
of	O
the	O
negative	O
phase	O
.	O
19	O
let	O
us	O
look	O
more	O
closely	O
at	O
the	O
gradient	O
of	O
log	O
z	O
:	O
	O
∇	O
θ	O
log	O
z	O
∇	O
θz	O
z	O
	O
=	O
∇	O
=	O
θ	O
˜p	O
(	O
)	O
x	O
x	O
∇	O
z	O
θ˜p	O
(	O
)	O
x	O
z	O
.	O
x	O
(	O
18.5	O
)	O
(	O
18.6	O
)	O
(	O
18.7	O
)	O
(	O
18.8	O
)	O
(	O
18.9	O
)	O
(	O
18.10	O
)	O
(	O
18.11	O
)	O
(	O
18.12	O
)	O
for	O
models	O
that	O
guarantee	O
p	O
(	O
x	O
)	O
>	O
0	O
for	O
all	O
x	O
,	O
we	O
can	O
substitute	O
exp	O
(	O
log	O
˜p	O
(	O
)	O
)	O
x	O
for	O
˜p	O
(	O
)	O
x	O
:	O
=	O
∇	O
θ	O
exp	O
(	O
log	O
˜p	O
(	O
)	O
)	O
x	O
z	O
∇	O
θ	O
log	O
˜p	O
(	O
)	O
x	O
exp	O
(	O
log	O
˜p	O
(	O
)	O
)	O
x	O
	O
=	O
	O
	O
	O
x	O
x	O
=	O
x	O
=	O
x	O
∇	O
z	O
˜p	O
(	O
)	O
x	O
z	O
∇	O
p	O
(	O
)	O
x	O
606	O
θ	O
log	O
˜p	O
(	O
)	O
x	O
θ	O
log	O
˜p	O
(	O
)	O
x	O
chapter	O
18.	O
confronting	O
the	O
partition	O
function	O
∼	O
=	O
ex	O
p	O
(	O
)	O
x	O
∇	O
θ	O
log	O
˜p	O
(	O
)	O
x	O
.	O
	O
	O
(	O
18.13	O
)	O
∇	O
θ	O
˜p	O
∇	O
θ	O
˜p	O
this	O
derivation	O
made	O
use	O
of	O
summation	O
over	O
discrete	O
x	O
,	O
but	O
a	O
similar	O
result	O
applies	O
using	O
integration	O
over	O
continuous	O
x.	O
in	O
the	O
continuous	O
version	O
of	O
the	O
derivation	O
,	O
we	O
use	O
leibniz	O
’	O
s	O
rule	O
for	O
diﬀerentiation	O
under	O
the	O
integral	O
sign	O
to	O
obtain	O
the	O
identity	O
(	O
)	O
x	O
x	O
d	O
.	O
d	O
(	O
)	O
x	O
x	O
=	O
(	O
18.14	O
)	O
∇	O
this	O
identity	O
is	O
applicable	O
only	O
under	O
certain	O
regularity	O
conditions	B
on	O
˜p	O
and	O
θ	O
˜p	O
(	O
x	O
)	O
.	O
in	O
measure	O
theoretic	O
terms	O
,	O
the	O
conditions	B
are	O
:	O
(	O
i	O
)	O
the	O
unnormalized	O
distribution	O
˜p	O
∇	O
must	O
be	O
a	O
lebesgue-integrable	O
function	O
of	O
x	O
for	O
every	O
value	O
of	O
θ	O
;	O
(	O
ii	O
)	O
the	O
gradient	O
θ	O
˜p	O
(	O
x	O
)	O
must	O
exist	O
for	O
all	O
θ	O
and	O
almost	O
all	O
x	O
;	O
(	O
iii	O
)	O
there	O
must	O
exist	O
an	O
integrable	O
r	O
(	O
x	O
)	O
for	O
all	O
function	O
r	O
(	O
x	O
)	O
that	O
bounds	O
θ	O
and	O
almost	O
all	O
x.	O
fortunately	O
,	O
most	O
machine	O
learning	O
models	O
of	O
interest	O
have	O
these	O
properties	O
.	O
θ˜p	O
(	O
x	O
)	O
in	O
the	O
sense	O
that	O
maxi	O
˜p	O
(	O
x	O
)	O
∂	O
∂θ	O
i	O
|	O
≤	O
∇	O
|	O
this	O
identity	O
∇	O
θ	O
log	O
=	O
z	O
∼	O
ex	O
p	O
(	O
)	O
x	O
∇	O
θ	O
log	O
˜p	O
(	O
)	O
x	O
(	O
18.15	O
)	O
is	O
the	O
basis	O
for	O
a	O
variety	O
of	O
monte	O
carlo	O
methods	O
for	O
approximately	O
maximizing	O
the	O
likelihood	O
of	O
models	O
with	O
intractable	O
partition	O
functions	O
.	O
the	O
monte	O
carlo	O
approach	O
to	O
learning	O
undirected	O
models	O
provides	O
an	O
intuitive	O
framework	O
in	O
which	O
we	O
can	O
think	O
of	O
both	O
the	O
positive	O
phase	O
and	O
the	O
negative	O
phase	O
.	O
in	O
the	O
positive	O
phase	O
,	O
we	O
increase	O
log	O
˜p	O
(	O
x	O
)	O
for	O
x	O
drawn	O
from	O
the	O
data	O
.	O
in	O
the	O
negative	O
phase	O
,	O
we	O
decrease	O
the	O
partition	O
function	O
by	O
decreasing	O
log	O
˜p	O
(	O
x	O
)	O
drawn	O
from	O
the	O
model	B
distribution	O
.	O
in	O
the	O
deep	O
learning	O
literature	O
,	O
it	O
is	O
common	O
to	O
parametrize	O
log	O
˜p	O
in	O
terms	O
of	O
an	O
energy	O
function	O
(	O
equation	O
)	O
.	O
in	O
this	O
case	O
,	O
we	O
can	O
interpret	O
the	O
positive	O
phase	O
as	O
pushing	O
down	O
on	O
the	O
energy	O
of	O
training	O
examples	O
and	O
the	O
negative	O
phase	O
as	O
pushing	O
up	O
on	O
the	O
energy	O
of	O
samples	O
drawn	O
from	O
the	O
model	B
,	O
as	O
illustrated	O
in	O
ﬁgure	O
.	O
18.1	O
16.7	O
18.2	O
stochastic	O
maximum	O
likelihood	O
and	O
contrastive	O
divergence	O
the	O
naive	O
way	O
of	O
implementing	O
equation	O
is	O
to	O
compute	O
it	O
by	O
burning	O
in	O
a	O
set	O
of	O
markov	O
chains	O
from	O
a	O
random	O
initialization	O
every	O
time	O
the	O
gradient	O
is	O
needed	O
.	O
when	O
learning	O
is	O
performed	O
using	O
stochastic	O
gradient	O
descent	B
,	O
this	O
means	O
the	O
chains	O
must	O
be	O
burned	O
in	O
once	O
per	O
gradient	O
step	O
.	O
this	O
approach	O
leads	O
to	O
the	O
18.15	O
607	O
chapter	O
18.	O
confronting	O
the	O
partition	O
function	O
.	O
the	O
high	O
cost	O
of	O
burning	O
in	O
the	O
training	O
procedure	O
presented	O
in	O
algorithm	O
markov	O
chains	O
in	O
the	O
inner	O
loop	O
makes	O
this	O
procedure	O
computationally	O
infeasible	O
,	O
but	O
this	O
procedure	O
is	O
the	O
starting	O
point	O
that	O
other	O
more	O
practical	O
algorithms	O
aim	O
to	O
approximate	O
.	O
18.1	O
algorithm	O
18.1	O
a	O
naive	O
mcmc	O
algorithm	O
for	O
maximizing	O
the	O
log-likelihood	O
with	O
an	O
intractable	O
partition	O
function	O
using	O
gradient	O
ascent	O
.	O
	O
←	O
	O
set	O
,	O
the	O
step	O
size	O
,	O
to	O
a	O
small	O
positive	O
number	O
.	O
set	O
k	O
,	O
the	O
number	O
of	O
gibbs	O
steps	O
,	O
high	O
enough	O
to	O
allow	O
burn	O
in	O
.	O
perhaps	O
100	O
to	O
train	O
an	O
rbm	O
on	O
a	O
small	O
image	O
patch	O
.	O
{	O
while	O
not	O
converged	O
do	O
∇	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
sample	O
a	O
minibatch	O
of	O
θ	O
log	O
˜p	O
(	O
x	O
(	O
)	O
i	O
;	O
)	O
θ	O
.	O
g	O
initialize	O
a	O
set	O
of	O
m	O
samples	O
to	O
random	O
values	O
(	O
e.g.	O
,	O
from	O
a	O
uniform	O
or	O
normal	O
distribution	O
,	O
or	O
possibly	O
a	O
distribution	O
with	O
marginals	O
matched	O
to	O
the	O
model	B
’	O
s	O
marginals	O
)	O
.	O
for	O
from	O
the	O
training	O
set	O
.	O
˜x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
˜x	O
(	O
examples	O
m	O
i=1	O
}	O
)	O
m	O
}	O
)	O
m	O
1	O
m	O
m	O
{	O
i	O
for	O
=	O
1	O
to	O
k	O
←	O
=	O
1	O
to	O
j	O
˜x	O
(	O
)	O
j	O
	O
do	O
m	O
do	O
gibbs_update	O
(	O
˜x	O
(	O
)	O
j	O
)	O
.	O
end	O
for	O
←	O
−	O
end	O
for	O
←	O
g	O
θ	O
1	O
m	O
+	O
	O
.g	O
g	O
θ	O
∇	O
m	O
i=1	O
θ	O
log	O
˜p	O
(	O
˜x	O
(	O
)	O
i	O
;	O
)	O
θ	O
.	O
end	O
while	O
we	O
can	O
view	O
the	O
mcmc	O
approach	O
to	O
maximum	O
likelihood	O
as	O
trying	O
to	O
achieve	O
balance	O
between	O
two	O
forces	O
,	O
one	O
pushing	O
up	O
on	O
the	O
model	B
distribution	O
where	O
the	O
data	O
occurs	O
,	O
and	O
another	O
pushing	O
down	O
on	O
the	O
model	B
distribution	O
where	O
the	O
model	B
samples	O
occur	O
.	O
figure	O
illustrates	O
this	O
process	O
.	O
the	O
two	O
forces	O
correspond	O
to	O
maximizing	O
log	O
˜p	O
and	O
minimizing	O
log	O
z	O
.	O
several	O
approximations	O
to	O
the	O
negative	O
phase	O
are	O
possible	O
.	O
each	O
of	O
these	O
approximations	O
can	O
be	O
understood	O
as	O
making	O
the	O
negative	O
phase	O
computationally	O
cheaper	O
but	O
also	O
making	O
it	O
push	O
down	O
in	O
the	O
wrong	O
locations	O
.	O
18.1	O
because	O
the	O
negative	O
phase	O
involves	O
drawing	O
samples	O
from	O
the	O
model	B
’	O
s	O
distri-	O
bution	O
,	O
we	O
can	O
think	O
of	O
it	O
as	O
ﬁnding	O
points	O
that	O
the	O
model	B
believes	O
in	O
strongly	O
.	O
because	O
the	O
negative	O
phase	O
acts	O
to	O
reduce	O
the	O
probability	O
of	O
those	O
points	O
,	O
they	O
are	O
generally	O
considered	O
to	O
represent	O
the	O
model	B
’	O
s	O
incorrect	O
beliefs	O
about	O
the	O
world	O
.	O
they	O
are	O
frequently	O
referred	O
to	O
in	O
the	O
literature	O
as	O
“	O
hallucinations	O
”	O
or	O
“	O
fantasy	O
particles.	O
”	O
in	O
fact	O
,	O
the	O
negative	O
phase	O
has	O
been	O
proposed	O
as	O
a	O
possible	O
explanation	O
608	O
chapter	O
18.	O
confronting	O
the	O
partition	O
function	O
the	O
positive	O
phase	O
the	O
negative	O
phase	O
pmodel	O
(	O
)	O
x	O
pdata	O
(	O
)	O
x	O
pmodel	O
(	O
)	O
x	O
pdata	O
(	O
)	O
x	O
)	O
x	O
(	O
p	O
)	O
x	O
(	O
p	O
x	O
x	O
18.1	O
figure	O
18.1	O
:	O
the	O
view	O
of	O
algorithm	O
as	O
having	O
a	O
“	O
positive	O
phase	O
”	O
and	O
“	O
negative	O
phase.	O
”	O
(	O
left	O
)	O
in	O
the	O
positive	O
phase	O
,	O
we	O
sample	O
points	O
from	O
the	O
data	O
distribution	O
,	O
and	O
push	O
up	O
on	O
their	O
unnormalized	O
probability	O
.	O
this	O
means	O
points	O
that	O
are	O
likely	O
in	O
the	O
data	O
get	O
pushed	O
up	O
on	O
more	O
.	O
(	O
right	O
)	O
in	O
the	O
negative	O
phase	O
,	O
we	O
sample	O
points	O
from	O
the	O
model	B
distribution	O
,	O
and	O
push	O
down	O
on	O
their	O
unnormalized	O
probability	O
.	O
this	O
counteracts	O
the	O
positive	O
phase	O
’	O
s	O
tendency	O
to	O
just	O
add	O
a	O
large	O
constant	O
to	O
the	O
unnormalized	O
probability	O
everywhere	O
.	O
when	O
the	O
data	O
distribution	O
and	O
the	O
model	B
distribution	O
are	O
equal	O
,	O
the	O
positive	O
phase	O
has	O
the	O
same	O
chance	O
to	O
push	O
up	O
at	O
a	O
point	O
as	O
the	O
negative	O
phase	O
has	O
to	O
push	O
down	O
.	O
when	O
this	O
occurs	O
,	O
there	O
is	O
no	O
longer	O
any	O
gradient	O
(	O
in	O
expectation	O
)	O
and	O
training	O
must	O
terminate	O
.	O
,	O
)	O
,	O
the	O
idea	O
for	O
dreaming	O
in	O
humans	O
and	O
other	O
animals	O
(	O
crick	O
and	O
mitchison	O
1983	O
being	O
that	O
the	O
brain	O
maintains	O
a	O
probabilistic	O
model	B
of	O
the	O
world	O
and	O
follows	O
the	O
gradient	O
of	O
log	O
˜p	O
while	O
experiencing	O
real	O
events	O
while	O
awake	O
and	O
follows	O
the	O
negative	O
gradient	O
of	O
log	O
˜p	O
to	O
minimize	O
log	O
z	O
while	O
sleeping	O
and	O
experiencing	O
events	O
sampled	O
from	O
the	O
current	O
model	B
.	O
this	O
view	O
explains	O
much	O
of	O
the	O
language	O
used	O
to	O
describe	O
algorithms	O
with	O
a	O
positive	O
and	O
negative	O
phase	O
,	O
but	O
it	O
has	O
not	O
been	O
proven	O
to	O
be	O
correct	O
with	O
neuroscientiﬁc	O
experiments	O
.	O
in	O
machine	O
learning	O
models	O
,	O
it	O
is	O
usually	O
necessary	O
to	O
use	O
the	O
positive	O
and	O
negative	O
phase	O
simultaneously	O
,	O
rather	O
than	O
in	O
separate	O
time	O
periods	O
of	O
wakefulness	O
and	O
rem	O
sleep	O
.	O
as	O
we	O
will	O
see	O
in	O
section	O
,	O
other	O
machine	O
learning	O
algorithms	O
draw	O
samples	O
from	O
the	O
model	B
distribution	O
for	O
other	O
purposes	O
and	O
such	O
algorithms	O
could	O
also	O
provide	O
an	O
account	O
for	O
the	O
function	O
of	O
dream	O
sleep	O
.	O
19.5	O
given	O
this	O
understanding	O
of	O
the	O
role	O
of	O
the	O
positive	O
and	O
negative	O
phase	O
of	O
learning	O
,	O
we	O
can	O
attempt	O
to	O
design	O
a	O
less	O
expensive	O
alternative	O
to	O
algorithm	O
.	O
18.1	O
the	O
main	O
cost	O
of	O
the	O
naive	O
mcmc	O
algorithm	O
is	O
the	O
cost	O
of	O
burning	O
in	O
the	O
markov	O
chains	O
from	O
a	O
random	O
initialization	O
at	O
each	O
step	O
.	O
a	O
natural	O
solution	O
is	O
to	O
initialize	O
the	O
markov	O
chains	O
from	O
a	O
distribution	O
that	O
is	O
very	O
close	O
to	O
the	O
model	B
distribution	O
,	O
609	O
chapter	O
18.	O
confronting	O
the	O
partition	O
function	O
so	O
that	O
the	O
burn	O
in	O
operation	O
does	O
not	O
take	O
as	O
many	O
steps	O
.	O
,	O
,	O
)	O
.	O
this	O
approach	O
is	O
presented	O
as	O
algorithm	O
the	O
contrastive	O
divergence	O
(	O
cd	O
,	O
or	O
cd-k	O
to	O
indicate	O
cd	O
with	O
k	O
gibbs	O
steps	O
)	O
algorithm	O
initializes	O
the	O
markov	O
chain	O
at	O
each	O
step	O
with	O
samples	O
from	O
the	O
data	O
distribution	O
(	O
hinton	O
2000	O
2010	O
18.2	O
.	O
obtaining	O
samples	O
from	O
the	O
data	O
distribution	O
is	O
free	O
,	O
because	O
they	O
are	O
already	O
available	O
in	O
the	O
data	O
set	O
.	O
initially	O
,	O
the	O
data	O
distribution	O
is	O
not	O
close	O
to	O
the	O
model	B
distribution	O
,	O
so	O
the	O
negative	O
phase	O
is	O
not	O
very	O
accurate	O
.	O
fortunately	O
,	O
the	O
positive	O
phase	O
can	O
still	O
accurately	O
increase	O
the	O
model	B
’	O
s	O
probability	O
of	O
the	O
data	O
.	O
after	O
the	O
positive	O
phase	O
has	O
had	O
some	O
time	O
to	O
act	O
,	O
the	O
model	B
distribution	O
is	O
closer	O
to	O
the	O
data	O
distribution	O
,	O
and	O
the	O
negative	O
phase	O
starts	O
to	O
become	O
accurate	O
.	O
algorithm	O
18.2	O
the	O
contrastive	O
divergence	O
algorithm	O
,	O
using	O
gradient	O
ascent	O
as	O
the	O
optimization	O
procedure	O
.	O
	O
	O
set	O
,	O
the	O
step	O
size	O
,	O
to	O
a	O
small	O
positive	O
number	O
.	O
set	O
k	O
,	O
the	O
number	O
of	O
gibbs	O
steps	O
,	O
high	O
enough	O
to	O
allow	O
a	O
markov	O
chain	O
sampling	O
from	O
p	O
(	O
x	O
;	O
θ	O
)	O
to	O
mix	O
when	O
initialized	O
from	O
pdata	O
.	O
perhaps	O
1-20	O
to	O
train	O
an	O
rbm	O
on	O
a	O
small	O
image	O
patch	O
.	O
while	O
not	O
converged	O
do	O
sample	O
a	O
minibatch	O
of	O
g	O
for	O
∇	O
θ	O
log	O
˜p	O
(	O
x	O
(	O
)	O
i	O
;	O
)	O
θ	O
.	O
do	O
{	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
from	O
the	O
training	O
set	O
.	O
examples	O
←	O
m	O
}	O
)	O
m	O
m	O
1	O
i=1	O
m	O
←	O
=	O
1	O
to	O
m	O
x	O
(	O
)	O
i	O
.	O
i	O
˜x	O
(	O
)	O
i	O
do	O
m	O
do	O
gibbs_update	O
(	O
˜x	O
(	O
)	O
j	O
)	O
.	O
end	O
for	O
for	O
i	O
for	O
=	O
1	O
to	O
k	O
←	O
=	O
1	O
to	O
j	O
˜x	O
(	O
)	O
j	O
	O
end	O
for	O
←	O
−	O
end	O
for	O
←	O
g	O
θ	O
1	O
m	O
+	O
	O
.g	O
g	O
θ	O
∇	O
m	O
i=1	O
θ	O
log	O
˜p	O
(	O
˜x	O
(	O
)	O
i	O
;	O
)	O
θ	O
.	O
end	O
while	O
of	O
course	O
,	O
cd	O
is	O
still	O
an	O
approximation	O
to	O
the	O
correct	O
negative	O
phase	O
.	O
the	O
main	O
way	O
that	O
cd	O
qualitatively	O
fails	O
to	O
implement	O
the	O
correct	O
negative	O
phase	O
is	O
that	O
it	O
fails	O
to	O
suppress	O
regions	O
of	O
high	O
probability	O
that	O
are	O
far	O
from	O
actual	O
training	O
examples	O
.	O
these	O
regions	O
that	O
have	O
high	O
probability	O
under	O
the	O
model	B
but	O
low	O
probability	O
under	O
the	O
data	O
generating	O
distribution	O
are	O
called	O
spurious	O
modes	O
.	O
figure	O
illustrates	O
why	O
this	O
happens	O
.	O
essentially	O
,	O
it	O
is	O
because	O
modes	O
in	O
the	O
model	B
distribution	O
that	O
are	O
far	O
from	O
the	O
data	O
distribution	O
will	O
not	O
be	O
visited	O
by	O
18.2	O
610	O
chapter	O
18.	O
confronting	O
the	O
partition	O
function	O
pmodel	O
(	O
)	O
x	O
pdata	O
(	O
)	O
x	O
)	O
x	O
(	O
p	O
x	O
18.2	O
figure	O
18.2	O
:	O
an	O
illustration	O
of	O
how	O
the	O
negative	O
phase	O
of	O
contrastive	O
divergence	O
(	O
algo-	O
rithm	O
)	O
can	O
fail	O
to	O
suppress	O
spurious	O
modes	O
.	O
a	O
spurious	O
mode	O
is	O
a	O
mode	O
that	O
is	O
present	O
in	O
the	O
model	B
distribution	O
but	O
absent	O
in	O
the	O
data	O
distribution	O
.	O
because	O
contrastive	O
divergence	O
initializes	O
its	O
markov	O
chains	O
from	O
data	O
points	O
and	O
runs	O
the	O
markov	O
chain	O
for	O
only	O
a	O
few	O
steps	O
,	O
it	O
is	O
unlikely	O
to	O
visit	O
modes	O
in	O
the	O
model	B
that	O
are	O
far	O
from	O
the	O
data	O
points	O
.	O
this	O
means	O
that	O
when	O
sampling	O
from	O
the	O
model	B
,	O
we	O
will	O
sometimes	O
get	O
samples	O
that	O
do	O
not	O
resemble	O
the	O
data	O
.	O
it	O
also	O
means	O
that	O
due	O
to	O
wasting	O
some	O
of	O
its	O
probability	O
mass	O
on	O
these	O
modes	O
,	O
the	O
model	B
will	O
struggle	O
to	O
place	O
high	O
probability	O
mass	O
on	O
the	O
correct	O
modes	O
.	O
for	O
the	O
purpose	O
of	O
visualization	O
,	O
this	O
ﬁgure	O
uses	O
a	O
somewhat	O
simpliﬁed	O
concept	O
of	O
distance—the	O
spurious	O
mode	O
is	O
far	O
from	O
the	O
correct	O
mode	O
along	O
the	O
number	O
line	O
in	O
r.	O
this	O
corresponds	O
to	O
a	O
markov	O
chain	O
based	O
on	O
making	O
local	O
moves	O
with	O
a	O
single	O
x	O
variable	O
in	O
r.	O
for	O
most	O
deep	O
probabilistic	O
models	O
,	O
the	O
markov	O
chains	O
are	O
based	O
on	O
gibbs	O
sampling	O
and	O
can	O
make	O
non-local	O
moves	O
of	O
individual	O
variables	O
but	O
can	O
not	O
move	O
all	O
of	O
the	O
variables	O
simultaneously	O
.	O
for	O
these	O
problems	O
,	O
it	O
is	O
usually	O
better	O
to	O
consider	O
the	O
edit	O
distance	O
between	O
modes	O
,	O
rather	O
than	O
the	O
euclidean	O
distance	O
.	O
however	O
,	O
edit	O
distance	O
in	O
a	O
high	O
dimensional	O
space	O
is	O
diﬃcult	O
to	O
depict	O
in	O
a	O
2-d	O
plot	O
.	O
markov	O
chains	O
initialized	O
at	O
training	O
points	O
,	O
unless	O
k	O
is	O
very	O
large	O
.	O
(	O
carreira-perpiñan	O
and	O
hinton	O
2005	O
)	O
showed	O
experimentally	O
that	O
the	O
cd	O
estimator	O
is	O
biased	O
for	O
rbms	O
and	O
fully	O
visible	O
boltzmann	O
machines	O
,	O
in	O
that	O
it	O
converges	O
to	O
diﬀerent	O
points	O
than	O
the	O
maximum	O
likelihood	O
estimator	O
.	O
they	O
argue	O
that	O
because	O
the	O
bias	O
is	O
small	O
,	O
cd	O
could	O
be	O
used	O
as	O
an	O
inexpensive	O
way	O
to	O
initialize	O
a	O
model	B
that	O
could	O
later	O
be	O
ﬁne-tuned	O
via	O
more	O
expensive	O
mcmc	O
methods	O
.	O
bengio	O
and	O
delalleau	O
2009	O
)	O
showed	O
that	O
cd	O
can	O
be	O
interpreted	O
as	O
discarding	O
the	O
smallest	O
terms	O
of	O
the	O
correct	O
mcmc	O
update	O
gradient	O
,	O
which	O
explains	O
the	O
bias	O
.	O
(	O
cd	O
is	O
useful	O
for	O
training	O
shallow	O
models	O
like	O
rbms	O
.	O
these	O
can	O
in	O
turn	O
be	O
stacked	O
to	O
initialize	O
deeper	O
models	O
like	O
dbns	O
or	O
dbms	O
.	O
however	O
,	O
cd	O
does	O
not	O
provide	O
much	O
help	O
for	O
training	O
deeper	O
models	O
directly	O
.	O
this	O
is	O
because	O
it	O
is	O
diﬃcult	O
611	O
chapter	O
18.	O
confronting	O
the	O
partition	O
function	O
to	O
obtain	O
samples	O
of	O
the	O
hidden	O
units	O
given	O
samples	O
of	O
the	O
visible	O
units	O
.	O
since	O
the	O
hidden	O
units	O
are	O
not	O
included	O
in	O
the	O
data	O
,	O
initializing	O
from	O
training	O
points	O
can	O
not	O
solve	O
the	O
problem	O
.	O
even	O
if	O
we	O
initialize	O
the	O
visible	O
units	O
from	O
the	O
data	O
,	O
we	O
will	O
still	O
need	O
to	O
burn	O
in	O
a	O
markov	O
chain	O
sampling	O
from	O
the	O
distribution	O
over	O
the	O
hidden	O
units	O
conditioned	O
on	O
those	O
visible	O
samples	O
.	O
the	O
cd	O
algorithm	O
can	O
be	O
thought	O
of	O
as	O
penalizing	O
the	O
model	B
for	O
having	O
a	O
markov	O
chain	O
that	O
changes	O
the	O
input	O
rapidly	O
when	O
the	O
input	O
comes	O
from	O
the	O
data	O
.	O
this	O
means	O
training	O
with	O
cd	O
somewhat	O
resembles	O
autoencoder	O
training	O
.	O
even	O
though	O
cd	O
is	O
more	O
biased	O
than	O
some	O
of	O
the	O
other	O
training	O
methods	O
,	O
it	O
can	O
be	O
useful	O
for	O
pretraining	O
shallow	O
models	O
that	O
will	O
later	O
be	O
stacked	O
.	O
this	O
is	O
because	O
the	O
earliest	O
models	O
in	O
the	O
stack	O
are	O
encouraged	O
to	O
copy	O
more	O
information	O
up	O
to	O
their	O
latent	O
variables	O
,	O
thereby	O
making	O
it	O
available	O
to	O
the	O
later	O
models	O
.	O
this	O
should	O
be	O
thought	O
of	O
more	O
of	O
as	O
an	O
often-exploitable	O
side	O
eﬀect	O
of	O
cd	O
training	O
rather	O
than	O
a	O
principled	O
design	O
advantage	O
.	O
sutskever	O
and	O
tieleman	O
2010	O
)	O
showed	O
that	O
the	O
cd	O
update	O
direction	O
is	O
not	O
the	O
gradient	O
of	O
any	O
function	O
.	O
this	O
allows	O
for	O
situations	O
where	O
cd	O
could	O
cycle	O
forever	O
,	O
but	O
in	O
practice	O
this	O
is	O
not	O
a	O
serious	O
problem	O
.	O
(	O
,	O
a	O
diﬀerent	O
strategy	O
that	O
resolves	O
many	O
of	O
the	O
problems	O
with	O
cd	O
is	O
to	O
initial-	O
ize	O
the	O
markov	O
chains	O
at	O
each	O
gradient	O
step	O
with	O
their	O
states	O
from	O
the	O
previous	O
gradient	O
step	O
.	O
this	O
approach	O
was	O
ﬁrst	O
discovered	O
under	O
the	O
name	O
stochastic	O
max-	O
imum	O
likelihood	O
(	O
sml	O
)	O
in	O
the	O
applied	O
mathematics	O
and	O
statistics	O
community	O
(	O
younes	O
1998	O
)	O
and	O
later	O
independently	O
rediscovered	O
under	O
the	O
name	O
persistent	O
contrastive	O
divergence	O
(	O
pcd	O
,	O
or	O
pcd-k	O
to	O
indicate	O
the	O
use	O
of	O
k	O
gibbs	O
steps	O
per	O
update	O
)	O
in	O
the	O
deep	O
learning	O
community	O
(	O
.	O
18.3	O
the	O
basic	O
idea	O
of	O
this	O
approach	O
is	O
that	O
,	O
so	O
long	O
as	O
the	O
steps	O
taken	O
by	O
the	O
stochastic	O
gradient	O
algorithm	O
are	O
small	O
,	O
then	O
the	O
model	B
from	O
the	O
previous	O
step	O
will	O
be	O
similar	O
to	O
the	O
model	B
from	O
the	O
current	O
step	O
.	O
it	O
follows	O
that	O
the	O
samples	O
from	O
the	O
previous	O
model	B
’	O
s	O
distribution	O
will	O
be	O
very	O
close	O
to	O
being	O
fair	O
samples	O
from	O
the	O
current	O
model	B
’	O
s	O
distribution	O
,	O
so	O
a	O
markov	O
chain	O
initialized	O
with	O
these	O
samples	O
will	O
not	O
require	O
much	O
time	O
to	O
mix	O
.	O
)	O
.	O
see	O
algorithm	O
tieleman	O
2008	O
,	O
because	O
each	O
markov	O
chain	O
is	O
continually	O
updated	O
throughout	O
the	O
learning	O
process	O
,	O
rather	O
than	O
restarted	O
at	O
each	O
gradient	O
step	O
,	O
the	O
chains	O
are	O
free	O
to	O
wander	O
far	O
enough	O
to	O
ﬁnd	O
all	O
of	O
the	O
model	B
’	O
s	O
modes	O
.	O
sml	O
is	O
thus	O
considerably	O
more	O
resistant	O
to	O
forming	O
models	O
with	O
spurious	O
modes	O
than	O
cd	O
is	O
.	O
moreover	O
,	O
because	O
it	O
is	O
possible	O
to	O
store	O
the	O
state	O
of	O
all	O
of	O
the	O
sampled	O
variables	O
,	O
whether	O
visible	O
or	O
latent	O
,	O
sml	O
provides	O
an	O
initialization	O
point	O
for	O
both	O
the	O
hidden	O
and	O
visible	O
units	O
.	O
cd	O
is	O
only	O
able	O
to	O
provide	O
an	O
initialization	O
for	O
the	O
visible	O
units	O
,	O
and	O
therefore	O
requires	O
burn-in	O
for	O
deep	O
models	O
.	O
sml	O
is	O
able	O
to	O
train	O
deep	O
models	O
eﬃciently	O
.	O
612	O
chapter	O
18.	O
confronting	O
the	O
partition	O
function	O
2010	O
et	O
al	O
.	O
(	O
)	O
compared	O
sml	O
to	O
many	O
of	O
the	O
other	O
criteria	O
presented	O
in	O
marlin	O
this	O
chapter	O
.	O
they	O
found	O
that	O
sml	O
results	O
in	O
the	O
best	O
test	O
set	O
log-likelihood	O
for	O
an	O
rbm	O
,	O
and	O
that	O
if	O
the	O
rbm	O
’	O
s	O
hidden	O
units	O
are	O
used	O
as	O
features	O
for	O
an	O
svm	O
classiﬁer	O
,	O
sml	O
results	O
in	O
the	O
best	O
classiﬁcation	O
accuracy	O
.	O
sml	O
is	O
vulnerable	O
to	O
becoming	O
inaccurate	O
if	O
the	O
stochastic	O
gradient	O
algorithm	O
can	O
move	O
the	O
model	B
faster	O
than	O
the	O
markov	O
chain	O
can	O
mix	O
between	O
steps	O
.	O
this	O
can	O
happen	O
if	O
k	O
is	O
too	O
small	O
or	O
	O
is	O
too	O
large	O
.	O
the	O
permissible	O
range	O
of	O
values	O
is	O
unfortunately	O
highly	O
problem-dependent	O
.	O
there	O
is	O
no	O
known	O
way	O
to	O
test	O
formally	O
whether	O
the	O
chain	O
is	O
successfully	O
mixing	O
between	O
steps	O
.	O
subjectively	O
,	O
if	O
the	O
learning	O
rate	O
is	O
too	O
high	O
for	O
the	O
number	O
of	O
gibbs	O
steps	O
,	O
the	O
human	O
operator	O
will	O
be	O
able	O
to	O
observe	O
that	O
there	O
is	O
much	O
more	O
variance	O
in	O
the	O
negative	O
phase	O
samples	O
across	O
gradient	O
steps	O
rather	O
than	O
across	O
diﬀerent	O
markov	O
chains	O
.	O
for	O
example	O
,	O
a	O
model	B
trained	O
on	O
mnist	O
might	O
sample	O
exclusively	O
7s	O
on	O
one	O
step	O
.	O
the	O
learning	O
process	O
will	O
then	O
push	O
down	O
strongly	O
on	O
the	O
mode	O
corresponding	O
to	O
7s	O
,	O
and	O
the	O
model	B
might	O
sample	O
exclusively	O
9s	O
on	O
the	O
next	O
step	O
.	O
algorithm	O
18.3	O
the	O
stochastic	O
maximum	O
likelihood	O
/	O
persistent	O
contrastive	O
divergence	O
algorithm	O
using	O
gradient	O
ascent	O
as	O
the	O
optimization	O
procedure	O
.	O
	O
{	O
set	O
,	O
the	O
step	O
size	O
,	O
to	O
a	O
small	O
positive	O
number	O
.	O
set	O
k	O
,	O
the	O
number	O
of	O
gibbs	O
steps	O
,	O
high	O
enough	O
to	O
allow	O
a	O
markov	O
chain	O
sampling	O
from	O
p	O
(	O
x	O
;	O
θ	O
+	O
g	O
)	O
to	O
burn	O
in	O
,	O
starting	O
from	O
samples	O
from	O
p	O
(	O
x	O
;	O
θ	O
)	O
.	O
perhaps	O
1	O
for	O
rbm	O
on	O
a	O
small	O
image	O
patch	O
,	O
or	O
5-50	O
for	O
a	O
more	O
complicated	O
model	B
like	O
a	O
dbm	O
.	O
initialize	O
a	O
set	O
of	O
m	O
samples	O
to	O
random	O
values	O
(	O
e.g.	O
,	O
from	O
a	O
uniform	O
or	O
normal	O
distribution	O
,	O
or	O
possibly	O
a	O
distribution	O
with	O
marginals	O
matched	O
to	O
the	O
model	B
’	O
s	O
marginals	O
)	O
.	O
while	O
not	O
converged	O
do	O
sample	O
a	O
minibatch	O
of	O
g	O
for	O
{	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
from	O
the	O
training	O
set	O
.	O
˜x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
˜x	O
(	O
examples	O
	O
←	O
}	O
}	O
)	O
m	O
)	O
m	O
m	O
∇	O
θ	O
log	O
˜p	O
(	O
x	O
(	O
)	O
i	O
;	O
)	O
θ	O
.	O
do	O
m	O
do	O
gibbs_update	O
(	O
˜x	O
(	O
)	O
j	O
)	O
.	O
∇	O
m	O
i=1	O
θ	O
log	O
˜p	O
(	O
˜x	O
(	O
)	O
i	O
;	O
)	O
θ	O
.	O
i	O
for	O
m	O
1	O
i=1	O
m	O
=	O
1	O
to	O
k	O
←	O
=	O
1	O
to	O
j	O
˜x	O
(	O
)	O
j	O
	O
end	O
for	O
←	O
−	O
end	O
for	O
←	O
g	O
θ	O
1	O
m	O
+	O
	O
.g	O
g	O
θ	O
end	O
while	O
care	O
must	O
be	O
taken	O
when	O
evaluating	O
the	O
samples	O
from	O
a	O
model	B
trained	O
with	O
sml	O
.	O
it	O
is	O
necessary	O
to	O
draw	O
the	O
samples	O
starting	O
from	O
a	O
fresh	O
markov	O
chain	O
613	O
chapter	O
18.	O
confronting	O
the	O
partition	O
function	O
initialized	O
from	O
a	O
random	O
starting	O
point	O
after	O
the	O
model	B
is	O
done	O
training	O
.	O
the	O
samples	O
present	O
in	O
the	O
persistent	O
negative	O
chains	O
used	O
for	O
training	O
have	O
been	O
inﬂuenced	O
by	O
several	O
recent	O
versions	O
of	O
the	O
model	B
,	O
and	O
thus	O
can	O
make	O
the	O
model	B
appear	O
to	O
have	O
greater	O
capacity	O
than	O
it	O
actually	O
does	O
.	O
(	O
berglund	O
and	O
raiko	O
2013	O
)	O
performed	O
experiments	O
to	O
examine	O
the	O
bias	O
and	O
variance	O
in	O
the	O
estimate	O
of	O
the	O
gradient	O
provided	O
by	O
cd	O
and	O
sml	O
.	O
cd	O
proves	O
to	O
have	O
lower	O
variance	O
than	O
the	O
estimator	O
based	O
on	O
exact	O
sampling	O
.	O
sml	O
has	O
higher	O
variance	O
.	O
the	O
cause	O
of	O
cd	O
’	O
s	O
low	O
variance	O
is	O
its	O
use	O
of	O
the	O
same	O
training	O
points	O
in	O
both	O
the	O
positive	O
and	O
negative	O
phase	O
.	O
if	O
the	O
negative	O
phase	O
is	O
initialized	O
from	O
diﬀerent	O
training	O
points	O
,	O
the	O
variance	O
rises	O
above	O
that	O
of	O
the	O
estimator	O
based	O
on	O
exact	O
sampling	O
.	O
all	O
of	O
these	O
methods	O
based	O
on	O
using	O
mcmc	O
to	O
draw	O
samples	O
from	O
the	O
model	B
can	O
in	O
principle	O
be	O
used	O
with	O
almost	O
any	O
variant	O
of	O
mcmc	O
.	O
this	O
means	O
that	O
techniques	O
such	O
as	O
sml	O
can	O
be	O
improved	O
by	O
using	O
any	O
of	O
the	O
enhanced	O
mcmc	O
techniques	O
described	O
in	O
chapter	O
desjardins	O
et	O
al	O
.	O
,	O
2010	O
cho	O
,	O
such	O
as	O
parallel	O
tempering	O
(	O
et	O
al.	O
,	O
2010	O
17	O
;	O
)	O
.	O
one	O
approach	O
to	O
accelerating	O
mixing	O
during	O
learning	O
relies	O
not	O
on	O
changing	O
the	O
monte	O
carlo	O
sampling	O
technology	O
but	O
rather	O
on	O
changing	O
the	O
parametrization	O
of	O
the	O
model	B
and	O
the	O
cost	O
function	O
.	O
fast	O
pcd	O
or	O
fpcd	O
(	O
tieleman	O
and	O
hinton	O
,	O
2009	O
)	O
involves	O
replacing	O
the	O
parameters	O
θ	O
of	O
a	O
traditional	O
model	B
with	O
an	O
expression	O
θ	O
θ=	O
slow	O
+	O
θ	O
(	O
(	O
)	O
fast	O
.	O
)	O
(	O
18.16	O
)	O
there	O
are	O
now	O
twice	O
as	O
many	O
parameters	O
as	O
before	O
,	O
and	O
they	O
are	O
added	O
together	O
element-wise	O
to	O
provide	O
the	O
parameters	O
used	O
by	O
the	O
original	O
model	B
deﬁnition	O
.	O
the	O
fast	O
copy	O
of	O
the	O
parameters	O
is	O
trained	O
with	O
a	O
much	O
larger	O
learning	O
rate	O
,	O
allowing	O
it	O
to	O
adapt	O
rapidly	O
in	O
response	O
to	O
the	O
negative	O
phase	O
of	O
learning	O
and	O
push	O
the	O
markov	O
chain	O
to	O
new	O
territory	O
.	O
this	O
forces	O
the	O
markov	O
chain	O
to	O
mix	O
rapidly	O
,	O
though	O
this	O
eﬀect	O
only	O
occurs	O
during	O
learning	O
while	O
the	O
fast	O
weights	O
are	O
free	O
to	O
change	O
.	O
typically	O
one	O
also	O
applies	O
signiﬁcant	O
weight	O
decay	O
to	O
the	O
fast	O
weights	O
,	O
encouraging	O
them	O
to	O
converge	O
to	O
small	O
values	O
,	O
after	O
only	O
transiently	O
taking	O
on	O
large	O
values	O
long	O
enough	O
to	O
encourage	O
the	O
markov	O
chain	O
to	O
change	O
modes	O
.	O
one	O
key	O
beneﬁt	O
to	O
the	O
mcmc-based	O
methods	O
described	O
in	O
this	O
section	O
is	O
that	O
they	O
provide	O
an	O
estimate	O
of	O
the	O
gradient	O
of	O
log	O
z	O
,	O
and	O
thus	O
we	O
can	O
essentially	O
decompose	O
the	O
problem	O
into	O
the	O
log	O
˜p	O
contribution	O
and	O
the	O
log	O
z	O
contribution	O
.	O
we	O
can	O
then	O
use	O
any	O
other	O
method	O
to	O
tackle	O
log	O
˜p	O
(	O
x	O
)	O
,	O
and	O
just	O
add	O
our	O
negative	O
phase	O
gradient	O
onto	O
the	O
other	O
method	O
’	O
s	O
gradient	O
.	O
in	O
particular	O
,	O
this	O
means	O
that	O
our	O
positive	O
phase	O
can	O
make	O
use	O
of	O
methods	O
that	O
provide	O
only	O
a	O
lower	O
bound	B
on	O
˜p	O
.	O
most	O
of	O
the	O
other	O
methods	O
of	O
dealing	O
with	O
log	O
z	O
presented	O
in	O
this	O
chapter	O
are	O
614	O
chapter	O
18.	O
confronting	O
the	O
partition	O
function	O
incompatible	O
with	O
bound-based	O
positive	O
phase	O
methods	O
.	O
18.3	O
pseudolikelihood	O
monte	O
carlo	O
approximations	O
to	O
the	O
partition	O
function	O
and	O
its	O
gradient	O
directly	O
confront	O
the	O
partition	O
function	O
.	O
other	O
approaches	O
sidestep	O
the	O
issue	O
,	O
by	O
training	O
the	O
model	B
without	O
computing	O
the	O
partition	O
function	O
.	O
most	O
of	O
these	O
approaches	O
are	O
based	O
on	O
the	O
observation	O
that	O
it	O
is	O
easy	O
to	O
compute	O
ratios	O
of	O
probabilities	O
in	O
an	O
undirected	O
probabilistic	O
model	B
.	O
this	O
is	O
because	O
the	O
partition	O
function	O
appears	O
in	O
both	O
the	O
numerator	O
and	O
the	O
denominator	O
of	O
the	O
ratio	O
and	O
cancels	O
out	O
:	O
p	O
(	O
)	O
x	O
p	O
(	O
)	O
y	O
=	O
˜p	O
(	O
)	O
x	O
˜p	O
(	O
)	O
y	O
1	O
z	O
1	O
z	O
=	O
˜p	O
(	O
)	O
x	O
˜p	O
(	O
)	O
y	O
.	O
(	O
18.17	O
)	O
the	O
pseudolikelihood	O
is	O
based	O
on	O
the	O
observation	O
that	O
conditional	O
probabilities	O
take	O
this	O
ratio-based	O
form	O
,	O
and	O
thus	O
can	O
be	O
computed	O
without	O
knowledge	O
of	O
the	O
partition	O
function	O
.	O
suppose	O
that	O
we	O
partition	O
x	O
into	O
a	O
,	O
b	O
and	O
c	O
,	O
where	O
a	O
contains	O
the	O
variables	O
we	O
want	O
to	O
ﬁnd	O
the	O
conditional	O
distribution	O
over	O
,	O
b	O
contains	O
the	O
variables	O
we	O
want	O
to	O
condition	O
on	O
,	O
and	O
c	O
contains	O
the	O
variables	O
that	O
are	O
not	O
part	O
of	O
our	O
query	O
.	O
	O
	O
|	O
a	O
b	O
p	O
(	O
)	O
=	O
p	O
,	O
(	O
a	O
b	O
)	O
p	O
(	O
)	O
b	O
=	O
p	O
,	O
(	O
a	O
b	O
)	O
a	O
c	O
,	O
p	O
,	O
(	O
a	O
b	O
c	O
)	O
,	O
=	O
˜p	O
,	O
(	O
a	O
b	O
)	O
a	O
c	O
,	O
˜p	O
,	O
(	O
a	O
b	O
c	O
)	O
,	O
.	O
(	O
18.18	O
)	O
this	O
quantity	O
requires	O
marginalizing	O
out	O
a	O
,	O
which	O
can	O
be	O
a	O
very	O
eﬃcient	O
operation	O
provided	O
that	O
a	O
and	O
c	O
do	O
not	O
contain	O
very	O
many	O
variables	O
.	O
in	O
the	O
extreme	O
case	O
,	O
a	O
can	O
be	O
a	O
single	O
variable	O
and	O
c	O
can	O
be	O
empty	O
,	O
making	O
this	O
operation	O
require	O
only	O
as	O
many	O
evaluations	O
of	O
˜p	O
as	O
there	O
are	O
values	O
of	O
a	O
single	O
random	O
variable	O
.	O
unfortunately	O
,	O
in	O
order	O
to	O
compute	O
the	O
log-likelihood	O
,	O
we	O
need	O
to	O
marginalize	O
out	O
large	O
sets	O
of	O
variables	O
.	O
if	O
there	O
are	O
n	O
variables	O
total	O
,	O
we	O
must	O
marginalize	O
a	O
set	O
of	O
size	O
.	O
by	O
the	O
chain	O
rule	O
of	O
probability	O
,	O
1	O
−	O
n	O
|	O
···	O
|	O
+	O
(	O
p	O
xn	O
−	O
)	O
.	O
1n	O
x1	O
:	O
(	O
18.19	O
)	O
log	O
(	O
)	O
=	O
log	O
(	O
p	O
x	O
p	O
x1	O
)	O
+	O
log	O
(	O
p	O
x2	O
x1	O
)	O
+	O
in	O
this	O
case	O
,	O
we	O
have	O
made	O
a	O
maximally	O
small	O
,	O
but	O
c	O
can	O
be	O
as	O
large	O
as	O
x2	O
:	O
n	O
.	O
what	O
if	O
we	O
simply	O
move	O
c	O
into	O
b	O
to	O
reduce	O
the	O
computational	O
cost	O
?	O
this	O
yields	O
the	O
pseudolikelihood	O
(	O
)	O
objective	O
function	O
,	O
based	O
on	O
predicting	O
the	O
value	O
of	O
feature	O
xi	O
given	O
all	O
of	O
the	O
other	O
features	O
x−	O
i	O
:	O
	O
besag	O
1975	O
,	O
|	O
x	O
−	O
i	O
)	O
.	O
log	O
(	O
p	O
xi	O
n	O
i=1	O
615	O
(	O
18.20	O
)	O
chapter	O
18.	O
confronting	O
the	O
partition	O
function	O
×	O
if	O
each	O
random	O
variable	O
has	O
k	O
diﬀerent	O
values	O
,	O
this	O
requires	O
only	O
k	O
n	O
evaluations	O
of	O
˜p	O
to	O
compute	O
,	O
as	O
opposed	O
to	O
the	O
kn	O
evaluations	O
needed	O
to	O
compute	O
the	O
partition	O
function	O
.	O
this	O
may	O
look	O
like	O
an	O
unprincipled	O
hack	O
,	O
but	O
it	O
can	O
be	O
proven	O
that	O
estimation	O
by	O
maximizing	O
the	O
pseudolikelihood	O
is	O
asymptotically	O
consistent	O
(	O
mase	O
1995	O
)	O
.	O
of	O
course	O
,	O
in	O
the	O
case	O
of	O
datasets	O
that	O
do	O
not	O
approach	O
the	O
large	O
sample	O
limit	O
,	O
pseudolikelihood	O
may	O
display	O
diﬀerent	O
behavior	O
from	O
the	O
maximum	O
likelihood	O
estimator	O
.	O
,	O
,	O
it	O
is	O
possible	O
to	O
trade	O
computational	O
complexity	O
for	O
deviation	O
from	O
maximum	O
likelihood	O
behavior	O
by	O
using	O
the	O
generalized	O
pseudolikelihood	O
estimator	O
(	O
huang	O
)	O
.	O
the	O
generalized	O
pseudolikelihood	O
estimator	O
uses	O
m	O
diﬀerent	O
sets	O
and	O
ogata	O
2002	O
(	O
)	O
i	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
of	O
indices	O
of	O
variables	O
that	O
appear	O
together	O
on	O
the	O
left	O
side	O
of	O
the	O
s	O
(	O
1	O
)	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
the	O
generalized	O
conditioning	O
bar	O
.	O
in	O
the	O
extreme	O
case	O
of	O
m	O
=	O
1	O
and	O
s	O
{	O
}	O
pseudolikelihood	O
recovers	O
the	O
log-likelihood	O
.	O
in	O
the	O
extreme	O
case	O
of	O
m	O
=	O
n	O
and	O
(	O
)	O
i	O
=	O
i	O
,	O
the	O
generalized	O
pseudolikelihood	O
recovers	O
the	O
pseudolikelihood	O
.	O
the	O
s	O
generalized	O
pseudolikelihood	O
objective	O
function	O
is	O
given	O
by	O
	O
m	O
i=1	O
log	O
(	O
p	O
x	O
(	O
)	O
i	O
s	O
|	O
x−	O
(	O
)	O
i	O
)	O
.	O
s	O
(	O
18.21	O
)	O
the	O
performance	O
of	O
pseudolikelihood-based	O
approaches	O
depends	O
largely	O
on	O
how	O
the	O
model	B
will	O
be	O
used	O
.	O
pseudolikelihood	O
tends	O
to	O
perform	O
poorly	O
on	O
tasks	O
that	O
require	O
a	O
good	O
model	B
of	O
the	O
full	O
joint	O
p	O
(	O
x	O
)	O
,	O
such	O
as	O
density	O
estimation	O
and	O
sampling	O
.	O
however	O
,	O
it	O
can	O
perform	O
better	O
than	O
maximum	O
likelihood	O
for	O
tasks	O
that	O
require	O
only	O
the	O
conditional	O
distributions	O
used	O
during	O
training	O
,	O
such	O
as	O
ﬁlling	O
in	O
small	O
amounts	O
of	O
missing	O
values	O
.	O
generalized	O
pseudolikelihood	O
techniques	O
are	O
especially	O
powerful	O
if	O
the	O
data	O
has	O
regular	O
structure	O
that	O
allows	O
the	O
s	O
index	O
sets	O
to	O
be	O
designed	O
to	O
capture	O
the	O
most	O
important	O
correlations	O
while	O
leaving	O
out	O
groups	O
of	O
variables	O
that	O
only	O
have	O
negligible	O
correlation	O
.	O
for	O
example	O
,	O
in	O
natural	O
images	O
,	O
pixels	O
that	O
are	O
widely	O
separated	O
in	O
space	O
also	O
have	O
weak	O
correlation	O
,	O
so	O
the	O
generalized	O
pseudolikelihood	O
can	O
be	O
applied	O
with	O
each	O
set	O
being	O
a	O
small	O
,	O
spatially	O
localized	O
window	O
.	O
s	O
one	O
weakness	O
of	O
the	O
pseudolikelihood	O
estimator	O
is	O
that	O
it	O
can	O
not	O
be	O
used	O
with	O
other	O
approximations	O
that	O
provide	O
only	O
a	O
lower	O
bound	B
on	O
˜p	O
(	O
x	O
)	O
,	O
such	O
as	O
variational	O
˜p	O
appears	O
in	O
the	O
inference	O
,	O
which	O
will	O
be	O
covered	O
in	O
chapter	O
denominator	O
.	O
a	O
lower	O
bound	B
on	O
the	O
denominator	O
provides	O
only	O
an	O
upper	O
bound	B
on	O
the	O
expression	O
as	O
a	O
whole	O
,	O
and	O
there	O
is	O
no	O
beneﬁt	O
to	O
maximizing	O
an	O
upper	O
bound	B
.	O
this	O
makes	O
it	O
diﬃcult	O
to	O
apply	O
pseudolikelihood	O
approaches	O
to	O
deep	O
models	O
such	O
as	O
deep	O
boltzmann	O
machines	O
,	O
since	O
variational	O
methods	O
are	O
one	O
of	O
the	O
dominant	O
approaches	O
to	O
approximately	O
marginalizing	O
out	O
the	O
many	O
layers	O
of	O
hidden	O
variables	O
.	O
this	O
is	O
because	O
19	O
616	O
chapter	O
18.	O
confronting	O
the	O
partition	O
function	O
that	O
interact	O
with	O
each	O
other	O
.	O
however	O
,	O
pseudolikelihood	O
is	O
still	O
useful	O
for	O
deep	O
learning	O
,	O
because	O
it	O
can	O
be	O
used	O
to	O
train	O
single	O
layer	O
models	O
,	O
or	O
deep	O
models	O
using	O
approximate	O
inference	O
methods	O
that	O
are	O
not	O
based	O
on	O
lower	O
bounds	O
.	O
pseudolikelihood	O
has	O
a	O
much	O
greater	O
cost	O
per	O
gradient	O
step	O
than	O
sml	O
,	O
due	O
to	O
its	O
explicit	O
computation	O
of	O
all	O
of	O
the	O
conditionals	O
.	O
however	O
,	O
generalized	O
pseudo-	O
likelihood	O
and	O
similar	O
criteria	O
can	O
still	O
perform	O
well	O
if	O
only	O
one	O
randomly	O
selected	O
conditional	O
is	O
computed	O
per	O
example	O
(	O
goodfellow	O
)	O
,	O
thereby	O
bringing	O
the	O
computational	O
cost	O
down	O
to	O
match	O
that	O
of	O
sml	O
.	O
2013b	O
et	O
al.	O
,	O
though	O
the	O
pseudolikelihood	O
estimator	O
does	O
not	O
explicitly	O
minimize	O
log	O
z	O
,	O
it	O
can	O
still	O
be	O
thought	O
of	O
as	O
having	O
something	O
resembling	O
a	O
negative	O
phase	O
.	O
the	O
denominators	O
of	O
each	O
conditional	O
distribution	O
result	O
in	O
the	O
learning	O
algorithm	O
suppressing	O
the	O
probability	O
of	O
all	O
states	O
that	O
have	O
only	O
one	O
variable	O
diﬀering	O
from	O
a	O
training	O
example	O
.	O
see	O
marlin	O
and	O
de	O
freitas	O
2011	O
(	O
)	O
for	O
a	O
theoretical	O
analysis	O
of	O
the	O
asymptotic	O
eﬃciency	O
of	O
pseudolikelihood	O
.	O
18.4	O
score	O
matching	O
and	O
ratio	O
matching	O
,	O
hyvärinen	O
2005	O
score	O
matching	O
(	O
)	O
provides	O
another	O
consistent	O
means	O
of	O
training	O
a	O
model	B
without	O
estimating	O
z	O
or	O
its	O
derivatives	O
.	O
the	O
name	O
score	O
matching	O
comes	O
∇	O
from	O
terminology	O
in	O
which	O
the	O
derivatives	O
of	O
a	O
log	O
density	O
with	O
respect	O
to	O
its	O
x	O
log	O
p	O
(	O
x	O
)	O
,	O
are	O
called	O
its	O
score	O
.	O
the	O
strategy	O
used	O
by	O
score	O
matching	O
argument	O
,	O
is	O
to	O
minimize	O
the	O
expected	O
squared	O
diﬀerence	O
between	O
the	O
derivatives	O
of	O
the	O
model	B
’	O
s	O
log	O
density	O
with	O
respect	O
to	O
the	O
input	O
and	O
the	O
derivatives	O
of	O
the	O
data	O
’	O
s	O
log	O
density	O
with	O
respect	O
to	O
the	O
input	O
:	O
l	O
,	O
(	O
x	O
θ	O
)	O
=	O
j	O
(	O
)	O
=θ	O
||∇	O
x	O
log	O
pmodel	O
(	O
;	O
)	O
x	O
θ	O
1	O
2	O
1	O
2epdata	O
(	O
)	O
x	O
l	O
,	O
(	O
x	O
θ	O
)	O
−	O
∇	O
||	O
x	O
log	O
pdata	O
(	O
)	O
x	O
2	O
2	O
∗	O
θ	O
=	O
min	O
j	O
(	O
)	O
θ	O
θ	O
(	O
18.22	O
)	O
(	O
18.23	O
)	O
(	O
18.24	O
)	O
∇	O
this	O
objective	O
function	O
avoids	O
the	O
diﬃculties	O
associated	O
with	O
diﬀerentiating	O
the	O
partition	O
function	O
z	O
because	O
z	O
is	O
not	O
a	O
function	O
of	O
x	O
and	O
therefore	O
xz	O
=	O
0.	O
initially	O
,	O
score	O
matching	O
appears	O
to	O
have	O
a	O
new	O
diﬃculty	O
:	O
computing	O
the	O
score	O
of	O
the	O
data	O
distribution	O
requires	O
knowledge	O
of	O
the	O
true	O
distribution	O
generating	O
the	O
training	O
data	O
,	O
pdata	O
.	O
fortunately	O
,	O
minimizing	O
the	O
expected	O
value	O
of	O
is	O
l	O
,	O
(	O
x	O
θ	O
)	O
617	O
chapter	O
18.	O
confronting	O
the	O
partition	O
function	O
	O
	O
2	O
log	O
pmodel	O
(	O
;	O
)	O
x	O
θ	O
(	O
18.25	O
)	O
	O
	O
	O
equivalent	O
to	O
minimizing	O
the	O
expected	O
value	O
of	O
˜l	O
,	O
(	O
x	O
θ	O
)	O
=	O
n	O
j=1	O
∂2	O
∂x2	O
j	O
log	O
pmodel	O
(	O
;	O
)	O
+	O
x	O
θ	O
1	O
2	O
∂	O
∂xj	O
where	O
n	O
is	O
the	O
dimensionality	O
of	O
.	O
x	O
because	O
score	O
matching	O
requires	O
taking	O
derivatives	O
with	O
respect	O
to	O
x	O
,	O
it	O
is	O
not	O
applicable	O
to	O
models	O
of	O
discrete	O
data	O
.	O
however	O
,	O
the	O
latent	O
variables	O
in	O
the	O
model	B
may	O
be	O
discrete	O
.	O
like	O
the	O
pseudolikelihood	O
,	O
score	O
matching	O
only	O
works	O
when	O
we	O
are	O
able	O
to	O
evaluate	O
log	O
˜p	O
(	O
x	O
)	O
and	O
its	O
derivatives	O
directly	O
.	O
it	O
is	O
not	O
compatible	O
with	O
methods	O
that	O
only	O
provide	O
a	O
lower	O
bound	B
on	O
log	O
˜p	O
(	O
x	O
)	O
,	O
because	O
score	O
matching	O
requires	O
the	O
derivatives	O
and	O
second	O
derivatives	O
of	O
log	O
˜p	O
(	O
x	O
)	O
and	O
a	O
lower	O
bound	B
conveys	O
no	O
information	O
about	O
its	O
derivatives	O
.	O
this	O
means	O
that	O
score	O
matching	O
can	O
not	O
be	O
applied	O
to	O
estimating	O
models	O
with	O
complicated	O
interactions	O
between	O
the	O
hidden	O
units	O
,	O
such	O
as	O
sparse	O
coding	O
models	O
or	O
deep	O
boltzmann	O
machines	O
.	O
while	O
score	O
matching	O
can	O
be	O
used	O
to	O
pretrain	O
the	O
ﬁrst	O
hidden	O
layer	O
of	O
a	O
larger	O
model	B
,	O
it	O
has	O
not	O
been	O
applied	O
as	O
a	O
pretraining	O
strategy	O
for	O
the	O
deeper	O
layers	O
of	O
a	O
larger	O
model	B
.	O
this	O
is	O
probably	O
because	O
the	O
hidden	O
layers	O
of	O
such	O
models	O
usually	O
contain	O
some	O
discrete	O
variables	O
.	O
while	O
score	O
matching	O
does	O
not	O
explicitly	O
have	O
a	O
negative	O
phase	O
,	O
it	O
can	O
be	O
viewed	O
as	O
a	O
version	O
of	O
contrastive	O
divergence	O
using	O
a	O
speciﬁc	O
kind	O
of	O
markov	O
chain	O
(	O
)	O
.	O
the	O
markov	O
chain	O
in	O
this	O
case	O
is	O
not	O
gibbs	O
sampling	O
,	O
but	O
hyvärinen	O
2007a	O
rather	O
a	O
diﬀerent	O
approach	O
that	O
makes	O
local	O
moves	O
guided	O
by	O
the	O
gradient	O
.	O
score	O
matching	O
is	O
equivalent	O
to	O
cd	O
with	O
this	O
type	O
of	O
markov	O
chain	O
when	O
the	O
size	O
of	O
the	O
local	O
moves	O
approaches	O
zero	O
.	O
,	O
(	O
lyu	O
2009	O
)	O
generalized	O
score	O
matching	O
to	O
the	O
discrete	O
case	O
(	O
but	O
made	O
an	O
error	O
marlin	O
et	O
al	O
.	O
2010	O
marlin	O
et	O
al	O
.	O
generalized	O
score	O
matching	O
(	O
gsm	O
)	O
does	O
not	O
work	B
in	O
high	O
in	O
their	O
derivation	O
that	O
was	O
corrected	O
by	O
(	O
2010	O
dimensional	O
discrete	O
spaces	O
where	O
the	O
observed	O
probability	O
of	O
many	O
events	O
is	O
0.	O
)	O
found	O
that	O
)	O
)	O
.	O
(	O
a	O
more	O
successful	O
approach	O
to	O
extending	O
the	O
basic	O
ideas	O
of	O
score	O
matching	O
to	O
discrete	O
data	O
is	O
ratio	O
matching	O
(	O
)	O
.	O
ratio	O
matching	O
applies	O
speciﬁcally	O
to	O
binary	O
data	O
.	O
ratio	O
matching	O
consists	O
of	O
minimizing	O
the	O
average	O
over	O
examples	O
of	O
the	O
following	O
objective	O
function	O
:	O
hyvärinen	O
2007b	O
	O
,	O
	O
	O
l	O
(	O
)	O
rm	O
(	O
x	O
θ	O
,	O
)	O
=	O
n	O
j=1	O
1	O
1	O
+	O
pmodel	O
(	O
;	O
)	O
x	O
θ	O
pmodel	O
(	O
(	O
)	O
)	O
;	O
)	O
f	O
x	O
,	O
j	O
θ	O
2	O
,	O
618	O
(	O
18.26	O
)	O
chapter	O
18.	O
confronting	O
the	O
partition	O
function	O
f	O
x	O
,	O
j	O
(	O
x	O
)	O
returns	O
with	O
the	O
bit	O
at	O
position	B
ﬂipped	O
.	O
ratio	O
matching	O
avoids	O
where	O
the	O
partition	O
function	O
using	O
the	O
same	O
trick	B
as	O
the	O
pseudolikelihood	O
estimator	O
:	O
in	O
a	O
ratio	O
of	O
two	O
probabilities	O
,	O
the	O
partition	O
function	O
cancels	O
out	O
.	O
marlin	O
et	O
al	O
.	O
2010	O
)	O
found	O
that	O
ratio	O
matching	O
outperforms	O
sml	O
,	O
pseudolikelihood	O
and	O
gsm	O
in	O
terms	O
of	O
the	O
ability	O
of	O
models	O
trained	O
with	O
ratio	O
matching	O
to	O
denoise	O
test	O
set	O
images	O
.	O
j	O
(	O
like	O
the	O
pseudolikelihood	O
estimator	O
,	O
ratio	O
matching	O
requires	O
n	O
evaluations	O
of	O
˜p	O
per	O
data	O
point	O
,	O
making	O
its	O
computational	O
cost	O
per	O
update	O
roughly	O
n	O
times	O
higher	O
than	O
that	O
of	O
sml	O
.	O
as	O
with	O
the	O
pseudolikelihood	O
estimator	O
,	O
ratio	O
matching	O
can	O
be	O
thought	O
of	O
as	O
pushing	O
down	O
on	O
all	O
fantasy	O
states	O
that	O
have	O
only	O
one	O
variable	O
diﬀerent	O
from	O
a	O
training	O
example	O
.	O
since	O
ratio	O
matching	O
applies	O
speciﬁcally	O
to	O
binary	O
data	O
,	O
this	O
means	O
that	O
it	O
acts	O
on	O
all	O
fantasy	O
states	O
within	O
hamming	O
distance	O
1	O
of	O
the	O
data	O
.	O
ratio	O
matching	O
can	O
also	O
be	O
useful	O
as	O
the	O
basis	O
for	O
dealing	O
with	O
high-dimensional	O
sparse	O
data	O
,	O
such	O
as	O
word	O
count	O
vectors	O
.	O
this	O
kind	O
of	O
data	O
poses	O
a	O
challenge	O
for	O
mcmc-based	O
methods	O
because	O
the	O
data	O
is	O
extremely	O
expensive	O
to	O
represent	O
in	O
dense	O
format	O
,	O
yet	O
the	O
mcmc	O
sampler	O
does	O
not	O
yield	O
sparse	O
values	O
until	O
the	O
model	B
has	O
learned	O
to	O
represent	O
the	O
sparsity	O
in	O
the	O
data	O
distribution	O
.	O
dauphin	O
and	O
bengio	O
(	O
)	O
overcame	O
this	O
issue	O
by	O
designing	O
an	O
unbiased	O
stochastic	O
approximation	O
to	O
2013	O
ratio	O
matching	O
.	O
the	O
approximation	O
evaluates	O
only	O
a	O
randomly	O
selected	O
subset	O
of	O
the	O
terms	O
of	O
the	O
objective	O
,	O
and	O
does	O
not	O
require	O
the	O
model	B
to	O
generate	O
complete	O
fantasy	O
samples	O
.	O
see	O
marlin	O
and	O
de	O
freitas	O
2011	O
(	O
)	O
for	O
a	O
theoretical	O
analysis	O
of	O
the	O
asymptotic	O
eﬃciency	O
of	O
ratio	O
matching	O
.	O
18.5	O
denoising	O
score	O
matching	O
	O
in	O
some	O
cases	O
we	O
may	O
wish	O
to	O
regularize	O
score	O
matching	O
,	O
by	O
ﬁtting	O
a	O
distribution	O
|	O
y	O
q	O
x	O
y	O
|	O
rather	O
than	O
the	O
true	O
pdata	O
.	O
the	O
distribution	O
q	O
(	O
x	O
y	O
one	O
that	O
forms	O
by	O
adding	O
a	O
small	O
amount	O
of	O
noise	O
to	O
psmoothed	O
(	O
)	O
=x	O
p	O
data	O
(	O
)	O
(	O
x	O
y	O
.	O
)	O
dy	O
(	O
18.27	O
)	O
)	O
is	O
a	O
corruption	O
process	O
,	O
usually	O
denoising	O
score	O
matching	O
is	O
especially	O
useful	O
because	O
in	O
practice	O
we	O
usually	O
do	O
not	O
have	O
access	O
to	O
the	O
true	O
pdata	O
but	O
rather	O
only	O
an	O
empirical	O
distribution	O
deﬁned	O
by	O
samples	O
from	O
it	O
.	O
any	O
consistent	O
estimator	O
will	O
,	O
given	O
enough	O
capacity	O
,	O
make	O
pmodel	O
into	O
a	O
set	O
of	O
dirac	O
distributions	O
centered	O
on	O
the	O
training	O
points	O
.	O
smoothing	O
by	O
q	O
helps	O
to	O
reduce	O
this	O
problem	O
,	O
at	O
the	O
loss	O
of	O
the	O
asymptotic	O
consistency	O
property	O
619	O
chapter	O
18.	O
confronting	O
the	O
partition	O
function	O
)	O
introduced	O
a	O
procedure	O
for	O
described	O
in	O
section	O
performing	O
regularized	O
score	O
matching	O
with	O
the	O
smoothing	O
distribution	O
q	O
being	O
normally	O
distributed	O
noise	O
.	O
5.4.5	O
kingma	O
and	O
lecun	O
2010	O
(	O
.	O
14.5.1	O
recall	O
from	O
section	O
that	O
several	O
autoencoder	O
training	O
algorithms	O
are	O
equivalent	O
to	O
score	O
matching	O
or	O
denoising	O
score	O
matching	O
.	O
these	O
autoencoder	O
training	O
algorithms	O
are	O
therefore	O
a	O
way	O
of	O
overcoming	O
the	O
partition	O
function	O
problem	O
.	O
18.6	O
noise-contrastive	O
estimation	O
most	O
techniques	O
for	O
estimating	O
models	O
with	O
intractable	O
partition	O
functions	O
do	O
not	O
provide	O
an	O
estimate	O
of	O
the	O
partition	O
function	O
.	O
sml	O
and	O
cd	O
estimate	O
only	O
the	O
gradient	O
of	O
the	O
log	O
partition	O
function	O
,	O
rather	O
than	O
the	O
partition	O
function	O
itself	O
.	O
score	O
matching	O
and	O
pseudolikelihood	O
avoid	O
computing	O
quantities	O
related	O
to	O
the	O
partition	O
function	O
altogether	O
.	O
noise-contrastive	O
estimation	O
(	O
nce	O
)	O
(	O
gutmann	O
and	O
hyvarinen	O
2010	O
)	O
takes	O
a	O
diﬀerent	O
strategy	O
.	O
in	O
this	O
approach	O
,	O
the	O
probability	O
distribution	O
estimated	O
by	O
the	O
model	B
is	O
represented	O
explicitly	O
as	O
,	O
log	O
pmodel	O
(	O
)	O
=	O
log	O
˜	O
x	O
pmodel	O
(	O
;	O
)	O
+	O
x	O
θ	O
(	O
18.28	O
)	O
c	O
,	O
−	O
log	O
z	O
(	O
θ	O
)	O
.	O
rather	O
than	O
where	O
c	O
is	O
explicitly	O
introduced	O
as	O
an	O
approximation	O
of	O
estimating	O
only	O
θ	O
,	O
the	O
noise	O
contrastive	O
estimation	O
procedure	O
treats	O
c	O
as	O
just	O
another	O
parameter	O
and	O
estimates	O
θ	O
and	O
c	O
simultaneously	O
,	O
using	O
the	O
same	O
algorithm	O
for	O
both	O
.	O
the	O
resulting	O
log	O
p	O
model	B
(	O
x	O
)	O
thus	O
may	O
not	O
correspond	O
exactly	O
to	O
a	O
valid	O
probability	O
distribution	O
,	O
but	O
will	O
become	O
closer	O
and	O
closer	O
to	O
being	O
valid	O
as	O
the	O
estimate	O
of	O
improves	O
.	O
c	O
1	O
such	O
an	O
approach	O
would	O
not	O
be	O
possible	O
using	O
maximum	O
likelihood	O
as	O
the	O
criterion	O
for	O
the	O
estimator	O
.	O
the	O
maximum	O
likelihood	O
criterion	O
would	O
choose	O
to	O
set	O
to	O
create	O
a	O
valid	O
probability	O
distribution	O
.	O
c	O
arbitrarily	O
high	O
,	O
rather	O
than	O
setting	O
c	O
nce	O
works	O
by	O
reducing	O
the	O
unsupervised	O
learning	O
problem	O
of	O
estimating	O
p	O
(	O
x	O
)	O
to	O
that	O
of	O
learning	O
a	O
probabilistic	O
binary	O
classiﬁer	O
in	O
which	O
one	O
of	O
the	O
categories	O
corresponds	O
to	O
the	O
data	O
generated	O
by	O
the	O
model	B
.	O
this	O
supervised	O
learning	O
problem	O
is	O
constructed	O
in	O
such	O
a	O
way	O
that	O
maximum	O
likelihood	O
estimation	O
in	O
this	O
supervised	O
1nce	O
is	O
also	O
applicable	O
to	O
problems	O
with	O
a	O
tractable	O
partition	O
function	O
,	O
where	O
there	O
is	O
no	O
need	O
to	O
introduce	O
the	O
extra	O
parameter	O
c.	O
however	O
,	O
it	O
has	O
generated	O
the	O
most	O
interest	O
as	O
a	O
means	O
of	O
estimating	O
models	O
with	O
diﬃcult	O
partition	O
functions	O
.	O
620	O
chapter	O
18.	O
confronting	O
the	O
partition	O
function	O
learning	O
problem	O
deﬁnes	O
an	O
asymptotically	O
consistent	O
estimator	O
of	O
the	O
original	O
problem	O
.	O
speciﬁcally	O
,	O
we	O
introduce	O
a	O
second	O
distribution	O
,	O
the	O
noise	O
distribution	O
pnoise	O
(	O
x	O
)	O
.	O
the	O
noise	O
distribution	O
should	O
be	O
tractable	O
to	O
evaluate	O
and	O
to	O
sample	O
from	O
.	O
we	O
can	O
now	O
construct	O
a	O
model	B
over	O
both	O
x	O
and	O
a	O
new	O
,	O
binary	O
class	O
variable	O
y	O
.	O
in	O
the	O
new	O
joint	O
model	B
,	O
we	O
specify	O
that	O
pjoint	O
(	O
=	O
1	O
)	O
=	O
y	O
1	O
2	O
,	O
|	O
pjoint	O
(	O
x	O
y	O
=	O
1	O
)	O
=	O
pmodel	O
(	O
)	O
x	O
,	O
and	O
|	O
pjoint	O
(	O
x	O
y	O
=	O
0	O
)	O
=	O
pnoise	O
(	O
)	O
x	O
.	O
(	O
18.29	O
)	O
(	O
18.30	O
)	O
(	O
18.31	O
)	O
in	O
other	O
words	O
,	O
y	O
is	O
a	O
switch	O
variable	O
that	O
determines	O
whether	O
we	O
will	O
generate	O
x	O
from	O
the	O
model	B
or	O
from	O
the	O
noise	O
distribution	O
.	O
we	O
can	O
construct	O
a	O
similar	O
joint	O
model	B
of	O
training	O
data	O
.	O
in	O
this	O
case	O
,	O
the	O
switch	O
variable	O
determines	O
whether	O
we	O
draw	O
x	O
from	O
the	O
data	O
or	O
from	O
the	O
noise	O
distribution	O
.	O
formally	O
,	O
ptrain	O
(	O
y	O
=	O
1	O
)	O
=	O
1	O
y	O
=	O
1	O
)	O
=	O
p	O
data	O
(	O
x	O
)	O
,	O
and	O
2	O
ptrain	O
(	O
x	O
,	O
ptrain	O
(	O
x	O
pnoise	O
(	O
)	O
x	O
.	O
=	O
0	O
)	O
=	O
y	O
|	O
|	O
we	O
can	O
now	O
just	O
use	O
standard	O
maximum	O
likelihood	O
learning	O
on	O
the	O
supervised	O
learning	O
problem	O
of	O
ﬁtting	O
pjoint	O
to	O
ptrain	O
:	O
∼	O
py	O
θ	O
,	O
c	O
=	O
arg	O
max	O
ex	O
,	O
θ	O
,	O
c	O
log	O
pjoint	O
(	O
y	O
train	O
|	O
x	O
.	O
)	O
(	O
18.32	O
)	O
the	O
distribution	O
pjoint	O
is	O
essentially	O
a	O
logistic	O
regression	O
model	B
applied	O
to	O
the	O
diﬀerence	O
in	O
log	O
probabilities	O
of	O
the	O
model	B
and	O
the	O
noise	O
distribution	O
:	O
|	O
pjoint	O
(	O
=	O
1	O
y	O
x	O
)	O
=	O
pmodel	O
(	O
)	O
x	O
p	O
model	B
(	O
)	O
+x	O
p	O
noise	O
(	O
)	O
x	O
(	O
18.33	O
)	O
(	O
18.34	O
)	O
(	O
18.35	O
)	O
(	O
18.36	O
)	O
(	O
18.37	O
)	O
	O
1	O
1	O
1	O
+	O
pnoise	O
(	O
)	O
x	O
pmodel	O
(	O
)	O
x	O
	O
	O
=	O
	O
=	O
1	O
+	O
exp	O
−	O
=	O
σ	O
log	O
log	O
pnoise	O
(	O
)	O
x	O
pmodel	O
(	O
)	O
x	O
pnoise	O
(	O
)	O
x	O
pmodel	O
(	O
)	O
x	O
−	O
=	O
(	O
log	O
σ	O
pmodel	O
(	O
)	O
x	O
log	O
p	O
noise	O
(	O
)	O
)	O
x	O
.	O
621	O
chapter	O
18.	O
confronting	O
the	O
partition	O
function	O
nce	O
is	O
thus	O
simple	O
to	O
apply	O
so	O
long	O
as	O
log	O
˜p	O
model	B
is	O
easy	O
to	O
back-propagate	O
through	O
,	O
and	O
,	O
as	O
speciﬁed	O
above	O
,	O
pnoise	O
is	O
easy	O
to	O
evaluate	O
(	O
in	O
order	O
to	O
evaluate	O
pjoint	O
)	O
and	O
sample	O
from	O
(	O
in	O
order	O
to	O
generate	O
the	O
training	O
data	O
)	O
.	O
nce	O
is	O
most	O
successful	O
when	O
applied	O
to	O
problems	O
with	O
few	O
random	O
variables	O
,	O
but	O
can	O
work	B
well	O
even	O
if	O
those	O
random	O
variables	O
can	O
take	O
on	O
a	O
high	O
number	O
of	O
values	O
.	O
for	O
example	O
,	O
it	O
has	O
been	O
successfully	O
applied	O
to	O
modeling	O
the	O
conditional	O
distribution	O
over	O
a	O
word	O
given	O
the	O
context	O
of	O
the	O
word	O
(	O
mnih	O
and	O
kavukcuoglu	O
,	O
2013	O
)	O
.	O
though	O
the	O
word	O
may	O
be	O
drawn	O
from	O
a	O
large	O
vocabulary	O
,	O
there	O
is	O
only	O
one	O
word	O
.	O
when	O
nce	O
is	O
applied	O
to	O
problems	O
with	O
many	O
random	O
variables	O
,	O
it	O
becomes	O
less	O
eﬃcient	O
.	O
the	O
logistic	O
regression	O
classiﬁer	O
can	O
reject	O
a	O
noise	O
sample	O
by	O
identifying	O
any	O
one	O
variable	O
whose	O
value	O
is	O
unlikely	O
.	O
this	O
means	O
that	O
learning	O
slows	O
down	O
greatly	O
after	O
pmodel	O
has	O
learned	O
the	O
basic	O
marginal	O
statistics	O
.	O
imagine	O
learning	O
a	O
model	B
of	O
images	O
of	O
faces	O
,	O
using	O
unstructured	O
gaussian	O
noise	O
as	O
pnoise	O
.	O
if	O
pmodel	O
learns	O
about	O
eyes	O
,	O
it	O
can	O
reject	O
almost	O
all	O
unstructured	O
noise	O
samples	O
without	O
having	O
learned	O
anything	O
about	O
other	O
facial	O
features	O
,	O
such	O
as	O
mouths	O
.	O
the	O
constraint	O
that	O
pnoise	O
must	O
be	O
easy	O
to	O
evaluate	O
and	O
easy	O
to	O
sample	O
from	O
can	O
be	O
overly	O
restrictive	O
.	O
when	O
pnoise	O
is	O
simple	O
,	O
most	O
samples	O
are	O
likely	O
to	O
be	O
too	O
obviously	O
distinct	O
from	O
the	O
data	O
to	O
force	O
pmodel	O
to	O
improve	O
noticeably	O
.	O
|	O
like	O
score	O
matching	O
and	O
pseudolikelihood	O
,	O
nce	O
does	O
not	O
work	B
if	O
only	O
a	O
lower	O
bound	B
on	O
˜p	O
is	O
available	O
.	O
such	O
a	O
lower	O
bound	B
could	O
be	O
used	O
to	O
construct	O
a	O
lower	O
|	O
x	O
)	O
,	O
but	O
it	O
can	O
only	O
be	O
used	O
to	O
construct	O
an	O
upper	O
bound	B
on	O
bound	B
on	O
pjoint	O
(	O
y	O
=	O
1	O
x	O
)	O
,	O
which	O
appears	O
in	O
half	O
the	O
terms	O
of	O
the	O
nce	O
objective	O
.	O
likewise	O
,	O
pjoint	O
(	O
y	O
=	O
0	O
|	O
a	O
lower	O
bound	B
on	O
p	O
noise	O
is	O
not	O
useful	O
,	O
because	O
it	O
provides	O
only	O
an	O
upper	O
bound	B
on	O
pjoint	O
(	O
=	O
1	O
x	O
.	O
)	O
y	O
goodfellow	O
2014	O
when	O
the	O
model	B
distribution	O
is	O
copied	O
to	O
deﬁne	O
a	O
new	O
noise	O
distribution	O
before	O
each	O
gradient	O
step	O
,	O
nce	O
deﬁnes	O
a	O
procedure	O
called	O
self-contrastive	O
estimation	O
,	O
whose	O
expected	O
gradient	O
is	O
equivalent	O
to	O
the	O
expected	O
gradient	O
of	O
maximum	O
likelihood	O
(	O
)	O
.	O
the	O
special	O
case	O
of	O
nce	O
where	O
the	O
noise	O
samples	O
are	O
those	O
generated	O
by	O
the	O
model	B
suggests	O
that	O
maximum	O
likelihood	O
can	O
be	O
interpreted	O
as	O
a	O
procedure	O
that	O
forces	O
a	O
model	B
to	O
constantly	O
learn	O
to	O
distinguish	O
reality	O
from	O
its	O
own	O
evolving	O
beliefs	O
,	O
while	O
noise	O
contrastive	O
estimation	O
achieves	O
some	O
reduced	O
computational	O
cost	O
by	O
only	O
forcing	O
the	O
model	B
to	O
distinguish	O
reality	O
from	O
a	O
ﬁxed	O
baseline	O
(	O
the	O
noise	O
model	B
)	O
.	O
,	O
using	O
the	O
supervised	O
task	O
of	O
classifying	O
between	O
training	O
samples	O
and	O
generated	O
samples	O
(	O
with	O
the	O
model	B
energy	O
function	O
used	O
in	O
deﬁning	O
the	O
classiﬁer	O
)	O
to	O
provide	O
a	O
gradient	O
on	O
the	O
model	B
was	O
introduced	O
earlier	O
in	O
various	O
forms	O
(	O
welling	O
et	O
al.	O
,	O
2003b	O
bengio	O
2009	O
)	O
.	O
,	O
;	O
622	O
chapter	O
18.	O
confronting	O
the	O
partition	O
function	O
noise	O
contrastive	O
estimation	O
is	O
based	O
on	O
the	O
idea	O
that	O
a	O
good	O
generative	O
model	B
should	O
be	O
able	O
to	O
distinguish	O
data	O
from	O
noise	O
.	O
a	O
closely	O
related	O
idea	O
is	O
that	O
a	O
good	O
generative	O
model	B
should	O
be	O
able	O
to	O
generate	O
samples	O
that	O
no	O
classiﬁer	O
can	O
distinguish	O
from	O
data	O
.	O
this	O
idea	O
yields	O
generative	O
adversarial	O
networks	O
(	O
section	O
20.10.4	O
)	O
.	O
18.7	O
estimating	O
the	O
partition	O
function	O
while	O
much	O
of	O
this	O
chapter	O
is	O
dedicated	O
to	O
describing	O
methods	O
that	O
avoid	O
needing	O
to	O
compute	O
the	O
intractable	O
partition	O
function	O
z	O
(	O
θ	O
)	O
associated	O
with	O
an	O
undirected	O
graphical	O
model	B
,	O
in	O
this	O
section	O
we	O
discuss	O
several	O
methods	O
for	O
directly	O
estimating	O
the	O
partition	O
function	O
.	O
estimating	O
the	O
partition	O
function	O
can	O
be	O
important	O
because	O
we	O
require	O
it	O
if	O
we	O
wish	O
to	O
compute	O
the	O
normalized	O
likelihood	O
of	O
data	O
.	O
this	O
is	O
often	O
important	O
in	O
evaluating	O
the	O
model	B
,	O
monitoring	O
training	O
performance	O
,	O
and	O
comparing	O
models	O
to	O
each	O
other	O
.	O
	O
m	O
for	O
example	O
,	O
imagine	O
we	O
have	O
two	O
models	O
:	O
model	B
a	O
deﬁning	O
a	O
probabil-	O
ity	O
distribution	O
pa	O
(	O
x	O
;	O
θa	O
)	O
=	O
1	O
b	O
deﬁning	O
a	O
probability	O
za	O
distribution	O
pb	O
(	O
x	O
;	O
θb	O
)	O
=	O
1	O
˜pb	O
(	O
x	O
;	O
θb	O
)	O
.	O
a	O
common	O
way	O
to	O
compare	O
the	O
models	O
zb	O
}	O
is	O
to	O
evaluate	O
and	O
compare	O
the	O
likelihood	O
that	O
both	O
models	O
assign	O
to	O
an	O
i.i.d	O
.	O
test	O
dataset	O
.	O
suppose	O
the	O
test	O
set	O
consists	O
of	O
m	O
examples	O
.	O
if	O
{	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
˜pa	O
(	O
x	O
;	O
θa	O
)	O
and	O
model	B
	O
	O
	O
)	O
m	O
m	O
i	O
pa	O
(	O
x	O
(	O
)	O
i	O
;	O
θa	O
)	O
>	O
i	O
pb	O
(	O
x	O
(	O
)	O
i	O
;	O
θb	O
)	O
or	O
equivalently	O
if	O
a	O
is	O
a	O
better	O
model	B
than	O
then	O
we	O
say	O
that	O
b	O
(	O
or	O
,	O
at	O
least	O
,	O
it	O
is	O
a	O
better	O
model	B
of	O
the	O
test	O
set	O
)	O
,	O
in	O
the	O
sense	O
that	O
it	O
has	O
a	O
better	O
test	O
log-likelihood	O
.	O
unfortunately	O
,	O
testing	O
whether	O
this	O
condition	O
holds	O
requires	O
knowledge	O
of	O
the	O
partition	O
function	O
.	O
unfortunately	O
,	O
equation	O
seems	O
to	O
require	O
evaluating	O
the	O
log	O
probability	O
that	O
the	O
model	B
assigns	O
to	O
each	O
point	O
,	O
which	O
in	O
turn	O
requires	O
evaluating	O
the	O
partition	O
function	O
.	O
we	O
can	O
simplify	O
the	O
situation	O
slightly	O
by	O
re-arranging	O
equation	O
18.38	O
into	O
a	O
form	O
where	O
we	O
need	O
to	O
know	O
only	O
the	O
ratio	O
of	O
the	O
two	O
model	B
’	O
s	O
partition	O
functions	O
:	O
	O
	O
	O
	O
	O
18.38	O
−	O
log	O
pa	O
(	O
x	O
(	O
)	O
i	O
;	O
θa	O
)	O
log	O
pb	O
(	O
x	O
(	O
)	O
i	O
;	O
θb	O
)	O
=	O
i	O
i	O
i	O
log	O
˜pa	O
(	O
x	O
(	O
)	O
i	O
;	O
θa	O
)	O
˜pb	O
(	O
x	O
(	O
)	O
i	O
;	O
θb	O
)	O
−	O
m	O
log	O
.	O
z	O
(	O
θa	O
)	O
z	O
(	O
θb	O
)	O
(	O
18.39	O
)	O
623	O
−	O
log	O
pa	O
(	O
x	O
(	O
)	O
i	O
;	O
θa	O
)	O
i	O
m	O
log	O
pb	O
(	O
x	O
(	O
)	O
i	O
;	O
θb	O
)	O
m	O
i	O
0	O
>	O
,	O
(	O
18.38	O
)	O
chapter	O
18.	O
confronting	O
the	O
partition	O
function	O
m	O
m	O
a	O
is	O
a	O
better	O
model	B
than	O
b	O
without	O
knowing	O
we	O
can	O
thus	O
determine	O
whether	O
the	O
partition	O
function	O
of	O
either	O
model	B
but	O
only	O
their	O
ratio	O
.	O
as	O
we	O
will	O
see	O
shortly	O
,	O
we	O
can	O
estimate	O
this	O
ratio	O
using	O
importance	O
sampling	O
,	O
provided	O
that	O
the	O
two	O
models	O
are	O
similar	O
.	O
m	O
if	O
,	O
however	O
,	O
we	O
wanted	O
to	O
compute	O
the	O
actual	O
probability	O
of	O
the	O
test	O
data	O
under	O
b	O
,	O
we	O
would	O
need	O
to	O
compute	O
the	O
actual	O
value	O
of	O
the	O
partition	O
either	O
functions	O
.	O
that	O
said	O
,	O
if	O
we	O
knew	O
the	O
ratio	O
of	O
two	O
partition	O
functions	O
,	O
r	O
=	O
z	O
(	O
θ	O
b	O
)	O
,	O
z	O
(	O
θ	O
a	O
)	O
and	O
we	O
knew	O
the	O
actual	O
value	O
of	O
just	O
one	O
of	O
the	O
two	O
,	O
say	O
z	O
(	O
θa	O
)	O
,	O
we	O
could	O
compute	O
the	O
value	O
of	O
the	O
other	O
:	O
a	O
or	O
m	O
z	O
(	O
θb	O
)	O
=	O
(	O
rz	O
θa	O
)	O
=	O
z	O
(	O
θb	O
)	O
z	O
(	O
θa	O
)	O
z	O
(	O
θa	O
)	O
.	O
(	O
18.40	O
)	O
a	O
simple	O
way	O
to	O
estimate	O
the	O
partition	O
function	O
is	O
to	O
use	O
a	O
monte	O
carlo	O
method	O
such	O
as	O
simple	O
importance	O
sampling	O
.	O
we	O
present	O
the	O
approach	O
in	O
terms	O
of	O
continuous	O
variables	O
using	O
integrals	O
,	O
but	O
it	O
can	O
be	O
readily	O
applied	O
to	O
discrete	O
variables	O
by	O
replacing	O
the	O
integrals	O
with	O
summation	O
.	O
we	O
use	O
a	O
proposal	O
distribution	O
p0	O
(	O
x	O
)	O
=	O
1	O
˜p0	O
(	O
x	O
)	O
which	O
supports	O
tractable	O
sampling	O
and	O
tractable	O
evaluation	O
of	O
z0	O
both	O
the	O
partition	O
function	O
z0	O
and	O
the	O
unnormalized	O
distribution	O
˜p	O
0	O
(	O
)	O
x	O
.	O
	O
	O
˜p1	O
(	O
)	O
x	O
dx	O
	O
	O
p0	O
(	O
)	O
x	O
p0	O
(	O
)	O
x	O
˜p1	O
(	O
)	O
x	O
dx	O
p0	O
(	O
)	O
x	O
˜p1	O
(	O
)	O
x	O
˜p0	O
(	O
)	O
x	O
˜p1	O
(	O
x	O
(	O
)	O
k	O
)	O
˜p0	O
(	O
x	O
(	O
)	O
k	O
)	O
z1	O
=	O
=	O
=	O
z0	O
ˆz1	O
=	O
z0	O
k	O
k	O
k=1	O
(	O
18.41	O
)	O
(	O
18.42	O
)	O
(	O
18.43	O
)	O
(	O
18.44	O
)	O
dx	O
.	O
.	O
x	O
(	O
)	O
k	O
s	O
t	O
:	O
∼	O
p0	O
in	O
the	O
last	O
line	O
,	O
we	O
make	O
a	O
monte	O
carlo	O
estimator	O
,	O
ˆz1	O
,	O
of	O
the	O
integral	O
using	O
samples	O
drawn	O
from	O
p0	O
(	O
x	O
)	O
and	O
then	O
weight	O
each	O
sample	O
with	O
the	O
ratio	O
of	O
the	O
unnormalized	O
˜p1	O
and	O
the	O
proposal	O
p0	O
.	O
we	O
see	O
also	O
that	O
this	O
approach	O
allows	O
us	O
to	O
estimate	O
the	O
ratio	O
between	O
the	O
	O
partition	O
functions	O
as	O
1	O
k	O
k	O
k=1	O
˜p1	O
(	O
x	O
(	O
)	O
k	O
)	O
˜p0	O
(	O
x	O
(	O
)	O
k	O
)	O
.	O
.	O
x	O
(	O
)	O
k	O
s	O
t	O
:	O
∼	O
p	O
0	O
.	O
(	O
18.45	O
)	O
this	O
value	O
can	O
then	O
be	O
used	O
directly	O
to	O
compare	O
two	O
models	O
as	O
described	O
in	O
equation	O
18.39	O
.	O
624	O
chapter	O
18.	O
confronting	O
the	O
partition	O
function	O
	O
	O
	O
	O
	O
18.44	O
if	O
the	O
distribution	O
p0	O
is	O
close	O
to	O
p1	O
,	O
equation	O
can	O
be	O
an	O
eﬀective	O
way	O
of	O
estimating	O
the	O
partition	O
function	O
(	O
minka	O
2005	O
)	O
.	O
unfortunately	O
,	O
most	O
of	O
the	O
time	O
p1	O
is	O
both	O
complicated	O
(	O
usually	O
multimodal	O
)	O
and	O
deﬁned	O
over	O
a	O
high	O
dimensional	O
space	O
.	O
it	O
is	O
diﬃcult	O
to	O
ﬁnd	O
a	O
tractable	O
p0	O
that	O
is	O
simple	O
enough	O
to	O
evaluate	O
while	O
still	O
being	O
close	O
enough	O
to	O
p1	O
to	O
result	O
in	O
a	O
high	O
quality	O
approximation	O
.	O
if	O
p0	O
and	O
p1	O
are	O
not	O
close	O
,	O
most	O
samples	O
from	O
p0	O
will	O
have	O
low	O
probability	O
under	O
p1	O
and	O
18.44	O
.	O
therefore	O
make	O
(	O
relatively	O
)	O
negligible	O
contribution	O
to	O
the	O
sum	O
in	O
equation	O
,	O
having	O
few	O
samples	O
with	O
signiﬁcant	O
weights	O
in	O
this	O
sum	O
will	O
result	O
in	O
an	O
estimator	O
that	O
is	O
of	O
poor	O
quality	O
due	O
to	O
high	O
variance	O
.	O
this	O
can	O
be	O
understood	O
quantitatively	O
through	O
an	O
estimate	O
of	O
the	O
variance	O
of	O
our	O
estimate	O
ˆz1	O
:	O
ˆvar	O
ˆz1	O
=	O
z0	O
k2	O
k	O
k=1	O
˜p1	O
(	O
x	O
(	O
)	O
k	O
)	O
˜p0	O
(	O
x	O
(	O
)	O
k	O
)	O
−	O
ˆz1	O
2	O
.	O
(	O
18.46	O
)	O
this	O
quantity	O
is	O
largest	O
when	O
there	O
is	O
signiﬁcant	O
deviation	O
in	O
the	O
values	O
of	O
the	O
importance	O
weights	O
˜p1	O
(	O
x	O
(	O
)	O
k	O
)	O
˜p0	O
(	O
x	O
(	O
)	O
k	O
)	O
.	O
we	O
now	O
turn	O
to	O
two	O
related	O
strategies	O
developed	O
to	O
cope	O
with	O
the	O
challeng-	O
ing	O
task	O
of	O
estimating	O
partition	O
functions	O
for	O
complex	O
distributions	O
over	O
high-	O
dimensional	O
spaces	O
:	O
annealed	O
importance	O
sampling	O
and	O
bridge	O
sampling	O
.	O
both	O
start	O
with	O
the	O
simple	O
importance	O
sampling	O
strategy	O
introduced	O
above	O
and	O
both	O
attempt	O
to	O
overcome	O
the	O
problem	O
of	O
the	O
proposal	O
p0	O
being	O
too	O
far	O
from	O
p	O
1	O
by	O
p0	O
introducing	O
intermediate	O
distributions	O
that	O
attempt	O
to	O
and	O
p1	O
.	O
bridge	O
the	O
gap	O
between	O
18.7.1	O
annealed	O
importance	O
sampling	O
	O
in	O
situations	O
where	O
d	O
kl	O
(	O
p0	O
p1	O
)	O
is	O
large	O
(	O
i.e.	O
,	O
where	O
there	O
is	O
little	O
overlap	O
between	O
p0	O
and	O
p1	O
)	O
,	O
a	O
strategy	O
called	O
annealed	O
importance	O
sampling	O
(	O
ais	O
)	O
attempts	O
to	O
bridge	O
the	O
gap	O
by	O
introducing	O
intermediate	O
distributions	O
(	O
jarzynski	O
1997	O
neal	O
,	O
<	O
2001	O
)	O
.	O
consider	O
a	O
sequence	O
of	O
distributions	O
p	O
η0	O
−	O
ηn	O
1	O
<	O
ηn	O
=	O
1	O
so	O
that	O
the	O
ﬁrst	O
and	O
last	O
distributions	O
in	O
the	O
sequence	O
are	O
p0	O
and	O
p1	O
respectively	O
.	O
;	O
,	O
with	O
0	O
=	O
η0	O
<	O
η1	O
<	O
,	O
.	O
.	O
.	O
,	O
pηn	O
···	O
,	O
this	O
approach	O
allows	O
us	O
to	O
estimate	O
the	O
partition	O
function	O
of	O
a	O
multimodal	O
distribution	O
deﬁned	O
over	O
a	O
high-dimensional	O
space	O
(	O
such	O
as	O
the	O
distribution	O
deﬁned	O
by	O
a	O
trained	O
rbm	O
)	O
.	O
we	O
begin	O
with	O
a	O
simpler	O
model	B
with	O
a	O
known	O
partition	O
function	O
(	O
such	O
as	O
an	O
rbm	O
with	O
zeroes	O
for	O
weights	O
)	O
and	O
estimate	O
the	O
ratio	O
between	O
the	O
two	O
model	B
’	O
s	O
partition	O
functions	O
.	O
the	O
estimate	O
of	O
this	O
ratio	O
is	O
based	O
on	O
the	O
estimate	O
of	O
the	O
ratios	O
of	O
a	O
sequence	O
of	O
many	O
similar	O
distributions	O
,	O
such	O
as	O
the	O
sequence	O
of	O
rbms	O
with	O
weights	O
interpolating	O
between	O
zero	O
and	O
the	O
learned	O
weights	O
.	O
625	O
chapter	O
18.	O
confronting	O
the	O
partition	O
function	O
we	O
can	O
now	O
write	O
the	O
ratio	O
z1	O
z0	O
as	O
···	O
zηn	O
−	O
1	O
zηn	O
−	O
···	O
zηn	O
1	O
−	O
1	O
zηn	O
−	O
2	O
z1	O
zηn	O
−	O
1	O
z1	O
z0	O
=	O
=	O
=	O
	O
z1	O
z0	O
zη1	O
z	O
0	O
−	O
1	O
n	O
zη	O
1	O
zη	O
1	O
zη	O
2	O
zη	O
1	O
zηj+1	O
zηj	O
j=0	O
(	O
18.47	O
)	O
(	O
18.48	O
)	O
(	O
18.49	O
)	O
1	O
,	O
are	O
suﬃciently	O
≤	O
≤	O
−	O
n	O
provided	O
the	O
distributions	O
pηj	O
j	O
close	O
,	O
we	O
can	O
reliably	O
estimate	O
each	O
of	O
the	O
factors	O
zη	O
j+1	O
zηj	O
sampling	O
and	O
then	O
use	O
these	O
to	O
obtain	O
an	O
estimate	O
of	O
z1	O
z0	O
and	O
pηj	O
+1	O
,	O
for	O
all	O
0	O
using	O
simple	O
importance	O
.	O
.	O
.	O
.	O
pηn	O
−	O
1	O
where	O
do	O
these	O
intermediate	O
distributions	O
come	O
from	O
?	O
just	O
as	O
the	O
original	O
proposal	O
distribution	O
p0	O
is	O
a	O
design	O
choice	O
,	O
so	O
is	O
the	O
sequence	O
of	O
distributions	O
pη1	O
.	O
that	O
is	O
,	O
it	O
can	O
be	O
speciﬁcally	O
constructed	O
to	O
suit	O
the	O
problem	O
domain	O
.	O
one	O
general-purpose	O
and	O
popular	O
choice	O
for	O
the	O
intermediate	O
distributions	O
is	O
to	O
use	O
the	O
weighted	O
geometric	O
average	O
of	O
the	O
target	O
distribution	O
p1	O
and	O
the	O
starting	O
proposal	O
distribution	O
(	O
for	O
which	O
the	O
partition	O
function	O
is	O
known	O
)	O
p0	O
:	O
∝	O
−	O
ηj	O
1	O
p1	O
pηj	O
0	O
pη	O
j	O
(	O
18.50	O
)	O
in	O
order	O
to	O
sample	O
from	O
these	O
intermediate	O
distributions	O
,	O
we	O
deﬁne	O
a	O
series	O
of	O
x	O
)	O
that	O
deﬁne	O
the	O
conditional	O
probability	O
(	O
x	O
given	O
we	O
are	O
currently	O
at	O
x.	O
the	O
transition	O
	O
markov	O
chain	O
transition	O
functions	O
tη	O
j	O
distribution	O
of	O
transitioning	O
to	O
x	O
operator	O
tηj	O
	O
|	O
(	O
x	O
x	O
)	O
is	O
deﬁned	O
to	O
leave	O
pηj	O
	O
pηj	O
(	O
)	O
=x	O
pηj	O
(	O
x	O
(	O
)	O
x	O
invariant	O
:	O
|	O
(	O
x	O
x	O
	O
	O
)	O
dx	O
)	O
tηj	O
(	O
18.51	O
)	O
	O
|	O
	O
these	O
transitions	O
may	O
be	O
constructed	O
as	O
any	O
markov	O
chain	O
monte	O
carlo	O
method	O
(	O
e.g.	O
,	O
metropolis-hastings	O
,	O
gibbs	O
)	O
,	O
including	O
methods	O
involving	O
multiple	O
passes	O
through	O
all	O
of	O
the	O
random	O
variables	O
or	O
other	O
kinds	O
of	O
iterations	O
.	O
the	O
ais	O
sampling	O
strategy	O
is	O
then	O
to	O
generate	O
samples	O
from	O
p0	O
and	O
then	O
use	O
the	O
transition	O
operators	O
to	O
sequentially	O
generate	O
samples	O
from	O
the	O
intermediate	O
distributions	O
until	O
we	O
arrive	O
at	O
samples	O
from	O
the	O
target	O
distribution	O
p1	O
:	O
•	O
for	O
k	O
=	O
1	O
.	O
.	O
.	O
k	O
–	O
sample	O
x	O
(	O
)	O
k	O
η1	O
∼	O
p0	O
(	O
)	O
x	O
626	O
chapter	O
18.	O
confronting	O
the	O
partition	O
function	O
–	O
sample	O
x	O
(	O
)	O
k	O
η2	O
–	O
.	O
.	O
.	O
–	O
sample	O
x	O
(	O
)	O
k	O
−	O
ηn	O
1	O
–	O
sample	O
x	O
(	O
)	O
k	O
ηn	O
tη1	O
∼	O
∼	O
∼	O
|	O
x	O
(	O
)	O
k	O
η1	O
(	O
x	O
(	O
)	O
k	O
η2	O
)	O
|	O
tηn	O
−	O
2	O
tηn	O
−	O
1	O
(	O
x	O
(	O
)	O
k	O
−	O
|	O
ηn	O
1	O
(	O
x	O
(	O
)	O
k	O
ηn	O
)	O
x	O
(	O
)	O
k	O
−	O
ηn	O
2	O
)	O
x	O
(	O
)	O
k	O
−	O
ηn	O
1	O
•	O
end	O
	O
for	O
sample	O
k	O
,	O
we	O
can	O
derive	O
the	O
importance	O
weight	O
by	O
chaining	O
together	O
the	O
importance	O
weights	O
for	O
the	O
jumps	O
between	O
the	O
intermediate	O
distributions	O
given	O
in	O
equation	O
18.49	O
:	O
w	O
(	O
)	O
k	O
=	O
)	O
(	O
x	O
(	O
)	O
k	O
˜pη1	O
η1	O
˜p0	O
(	O
x	O
(	O
)	O
k	O
η1	O
)	O
˜pη2	O
˜pη1	O
(	O
x	O
(	O
)	O
k	O
η2	O
(	O
x	O
(	O
)	O
k	O
η2	O
)	O
)	O
.	O
.	O
.	O
˜p1	O
(	O
x	O
(	O
)	O
k	O
1	O
)	O
(	O
x	O
(	O
)	O
k	O
˜p	O
ηn	O
−	O
ηn	O
1	O
.	O
)	O
(	O
18.52	O
)	O
to	O
avoid	O
numerical	O
issues	O
such	O
as	O
overﬂow	O
,	O
it	O
is	O
probably	O
best	O
to	O
compute	O
log	O
w	O
(	O
)	O
k	O
by	O
adding	O
and	O
subtracting	O
log	O
probabilities	O
,	O
rather	O
than	O
computing	O
w	O
(	O
)	O
k	O
by	O
multiplying	O
and	O
dividing	O
probabilities	O
.	O
with	O
the	O
sampling	O
procedure	O
thus	O
deﬁned	O
and	O
the	O
importance	O
weights	O
given	O
in	O
equation	O
18.52	O
,	O
the	O
estimate	O
of	O
the	O
ratio	O
of	O
partition	O
functions	O
is	O
given	O
by	O
:	O
≈	O
1	O
k	O
z1	O
z0	O
k	O
k=1	O
w	O
(	O
)	O
k	O
(	O
18.53	O
)	O
in	O
order	O
to	O
verify	O
that	O
this	O
procedure	O
deﬁnes	O
a	O
valid	O
importance	O
sampling	O
scheme	O
,	O
we	O
can	O
show	O
(	O
)	O
that	O
the	O
ais	O
procedure	O
corresponds	O
to	O
simple	O
importance	O
sampling	O
on	O
an	O
extended	O
state	O
space	O
with	O
points	O
sampled	O
over	O
the	O
product	O
space	O
[	O
xη1	O
,	O
x1	O
]	O
.	O
to	O
do	O
this	O
,	O
we	O
deﬁne	O
the	O
distribution	O
over	O
the	O
extended	O
space	O
as	O
:	O
,	O
.	O
.	O
.	O
,	O
xη	O
n	O
−	O
1	O
neal	O
2001	O
,	O
˜p	O
(	O
xη1	O
,	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
xηn	O
−	O
|	O
1	O
(	O
x	O
ηn	O
−	O
1	O
=˜p	O
1	O
(	O
x1	O
)	O
˜tηn	O
−	O
1	O
x	O
1	O
)	O
˜tηn	O
−	O
2	O
|	O
(	O
xηn	O
−	O
2	O
xηn	O
−	O
1	O
)	O
.	O
.	O
.	O
˜tη	O
1	O
(	O
xη1	O
|	O
xη2	O
)	O
,	O
(	O
18.54	O
)	O
(	O
18.55	O
)	O
where	O
˜ta	O
is	O
the	O
reverse	O
of	O
the	O
transition	O
operator	O
deﬁned	O
by	O
t	O
a	O
(	O
via	O
an	O
application	O
of	O
bayes	O
’	O
rule	O
)	O
:	O
	O
|	O
˜ta	O
(	O
x	O
x	O
)	O
=	O
	O
pa	O
(	O
x	O
)	O
pa	O
(	O
)	O
x	O
|	O
t	O
a	O
(	O
x	O
x	O
	O
)	O
=	O
	O
˜pa	O
(	O
x	O
)	O
˜pa	O
(	O
)	O
x	O
|	O
	O
ta	O
(	O
x	O
x	O
)	O
.	O
(	O
18.56	O
)	O
plugging	O
the	O
above	O
into	O
the	O
expression	O
for	O
the	O
joint	O
distribution	O
on	O
the	O
extended	O
state	O
space	O
given	O
in	O
equation	O
,	O
we	O
get	O
:	O
18.55	O
˜p	O
(	O
x	O
η1	O
−	O
,	O
.	O
.	O
.	O
,	O
xηn	O
1	O
,	O
x	O
1	O
)	O
(	O
18.57	O
)	O
627	O
chapter	O
18.	O
confronting	O
the	O
partition	O
function	O
=	O
˜p1	O
(	O
x1	O
)	O
)	O
−	O
−	O
˜pη	O
n	O
(	O
xηn	O
1	O
1	O
(	O
x1	O
)	O
˜pηn	O
−	O
1	O
−	O
tηn	O
1	O
(	O
x	O
1	O
|	O
xηn	O
−	O
1	O
)	O
˜pηi	O
˜pηi	O
)	O
(	O
xηi	O
(	O
xηi+1	O
)	O
tη	O
i	O
(	O
xηi+1	O
|	O
xηi	O
)	O
	O
	O
−	O
2	O
n	O
i=1	O
−	O
2	O
n	O
=	O
˜p1	O
(	O
x1	O
)	O
−	O
˜pηn	O
1	O
(	O
x1	O
)	O
t	O
ηn	O
−	O
1	O
(	O
x1	O
|	O
xηn	O
−	O
1	O
)	O
˜pη1	O
(	O
xη	O
1	O
)	O
i=1	O
)	O
˜pηi+1	O
˜pηi	O
(	O
x	O
ηi+1	O
(	O
xηi+1	O
)	O
tη	O
i	O
(	O
xη	O
i+1	O
(	O
18.58	O
)	O
|	O
xηi	O
)	O
.	O
(	O
18.59	O
)	O
we	O
now	O
have	O
means	O
of	O
generating	O
samples	O
from	O
the	O
joint	O
proposal	O
distribution	O
q	O
over	O
the	O
extended	O
sample	O
via	O
a	O
sampling	O
scheme	O
given	O
above	O
,	O
with	O
the	O
joint	O
distribution	O
given	O
by	O
:	O
|	O
|	O
q	O
(	O
xη	O
1	O
−	O
,	O
.	O
.	O
.	O
,	O
xηn	O
1	O
,	O
x	O
1	O
)	O
=	O
p	O
0	O
(	O
x	O
η1	O
)	O
tη1	O
(	O
xη2	O
xη1	O
−	O
)	O
.	O
.	O
.	O
tηn	O
1	O
(	O
x1	O
x	O
ηn	O
−	O
1	O
)	O
.	O
(	O
18.60	O
)	O
we	O
have	O
a	O
joint	O
distribution	O
on	O
the	O
extended	O
space	O
given	O
by	O
equation	O
q	O
(	O
xη1	O
which	O
we	O
will	O
draw	O
samples	O
,	O
it	O
remains	O
to	O
determine	O
the	O
importance	O
weights	O
:	O
.	O
taking	O
,	O
x1	O
)	O
as	O
the	O
proposal	O
distribution	O
on	O
the	O
extended	O
state	O
space	O
from	O
,	O
.	O
.	O
.	O
,	O
xηn	O
−	O
1	O
18.59	O
w	O
(	O
)	O
k	O
=	O
˜p	O
(	O
xη1	O
q	O
(	O
xη1	O
−	O
,	O
.	O
.	O
.	O
,	O
xηn	O
1	O
−	O
,	O
.	O
.	O
.	O
,	O
xη	O
n	O
1	O
,	O
x	O
1	O
)	O
,	O
x1	O
)	O
=	O
˜p1	O
(	O
x	O
(	O
)	O
k	O
1	O
)	O
(	O
x	O
(	O
)	O
k	O
−	O
˜pηn	O
−	O
ηn	O
1	O
1	O
)	O
.	O
.	O
.	O
)	O
(	O
x	O
(	O
)	O
k	O
˜pη2	O
η2	O
˜p1	O
(	O
x	O
(	O
)	O
k	O
)	O
η1	O
(	O
x	O
(	O
)	O
k	O
˜pη1	O
)	O
η1	O
˜p0	O
(	O
x	O
(	O
)	O
k	O
0	O
)	O
.	O
(	O
18.61	O
)	O
these	O
weights	O
are	O
the	O
same	O
as	O
proposed	O
for	O
ais	O
.	O
thus	O
we	O
can	O
interpret	O
ais	O
as	O
simple	O
importance	O
sampling	O
applied	O
to	O
an	O
extended	O
state	O
and	O
its	O
validity	O
follows	O
immediately	O
from	O
the	O
validity	O
of	O
importance	O
sampling	O
.	O
(	O
neal	O
2001	O
annealed	O
importance	O
sampling	O
(	O
ais	O
)	O
was	O
ﬁrst	O
discovered	O
by	O
jarzynski	O
1997	O
)	O
)	O
.	O
it	O
is	O
currently	O
the	O
most	O
common	O
and	O
then	O
again	O
,	O
independently	O
,	O
by	O
way	O
of	O
estimating	O
the	O
partition	O
function	O
for	O
undirected	O
probabilistic	O
models	O
.	O
the	O
reasons	O
for	O
this	O
may	O
have	O
more	O
to	O
do	O
with	O
the	O
publication	O
of	O
an	O
inﬂuential	O
paper	O
(	O
salakhutdinov	O
and	O
murray	O
2008	O
)	O
describing	O
its	O
application	O
to	O
estimating	O
the	O
partition	O
function	O
of	O
restricted	O
boltzmann	O
machines	O
and	O
deep	O
belief	O
networks	O
than	O
with	O
any	O
inherent	O
advantage	O
the	O
method	O
has	O
over	O
the	O
other	O
method	O
described	O
below	O
.	O
(	O
,	O
a	O
discussion	O
of	O
the	O
properties	O
of	O
the	O
ais	O
estimator	O
(	O
e.g..	O
its	O
variance	O
and	O
eﬃciency	O
)	O
can	O
be	O
found	O
in	O
neal	O
2001	O
(	O
)	O
.	O
18.7.2	O
bridge	O
sampling	O
)	O
is	O
another	O
method	O
that	O
,	O
like	O
ais	O
,	O
addresses	O
the	O
bridge	O
sampling	O
shortcomings	O
of	O
importance	O
sampling	O
.	O
rather	O
than	O
chaining	O
together	O
a	O
series	O
of	O
bennett	O
1976	O
(	O
628	O
chapter	O
18.	O
confronting	O
the	O
partition	O
function	O
intermediate	O
distributions	O
,	O
bridge	O
sampling	O
relies	O
on	O
a	O
single	O
distribution	O
p∗	O
,	O
known	O
as	O
the	O
bridge	O
,	O
to	O
interpolate	O
between	O
a	O
distribution	O
with	O
known	O
partition	O
function	O
,	O
p0	O
,	O
and	O
a	O
distribution	O
p1	O
for	O
which	O
we	O
are	O
trying	O
to	O
estimate	O
the	O
partition	O
function	O
z1	O
.	O
bridge	O
sampling	O
estimates	O
the	O
ratio	O
z1/z0	O
as	O
the	O
ratio	O
of	O
the	O
expected	O
impor-	O
tance	O
weights	O
between	O
˜p0	O
and	O
˜p∗	O
and	O
between	O
˜p1	O
and	O
˜p∗	O
:	O
˜p∗	O
(	O
x	O
(	O
)	O
k	O
1	O
)	O
˜p1	O
(	O
x	O
(	O
)	O
k	O
1	O
)	O
˜p∗	O
(	O
x	O
(	O
)	O
k	O
0	O
)	O
˜p0	O
(	O
x	O
(	O
)	O
k	O
0	O
)	O
≈	O
k	O
z	O
1	O
z	O
0	O
k=1	O
k=1	O
k	O
	O
	O
	O
(	O
18.62	O
)	O
if	O
the	O
bridge	O
distribution	O
p∗	O
is	O
chosen	O
carefully	O
to	O
have	O
a	O
large	O
overlap	O
of	O
support	O
	O
with	O
both	O
p0	O
and	O
p1	O
,	O
then	O
bridge	O
sampling	O
can	O
allow	O
the	O
distance	O
between	O
two	O
distributions	O
(	O
or	O
more	O
formally	O
,	O
dkl	O
(	O
p0	O
p	O
1	O
)	O
)	O
to	O
be	O
much	O
larger	O
than	O
with	O
standard	O
importance	O
sampling	O
.	O
∝	O
it	O
can	O
be	O
shown	O
that	O
the	O
optimal	O
bridging	O
distribution	O
is	O
given	O
by	O
p	O
(	O
)	O
opt∗	O
(	O
x	O
)	O
˜p0	O
(	O
)	O
˜x	O
p1	O
(	O
)	O
x	O
where	O
r	O
=	O
z1/z0	O
.	O
at	O
ﬁrst	O
,	O
this	O
appears	O
to	O
be	O
an	O
unworkable	O
solution	O
r˜p0	O
(	O
)	O
+˜x	O
p1	O
(	O
)	O
x	O
as	O
it	O
would	O
seem	O
to	O
require	O
the	O
very	O
quantity	O
we	O
are	O
trying	O
to	O
estimate	O
,	O
z1/z0	O
.	O
however	O
,	O
it	O
is	O
possible	O
to	O
start	O
with	O
a	O
coarse	O
estimate	O
of	O
r	O
and	O
use	O
the	O
resulting	O
bridge	O
distribution	O
to	O
reﬁne	O
our	O
estimate	O
iteratively	O
(	O
)	O
.	O
that	O
is	O
,	O
we	O
iteratively	O
re-estimate	O
the	O
ratio	O
and	O
use	O
each	O
iteration	O
to	O
update	O
the	O
value	O
of	O
neal	O
2005	O
.r	O
,	O
	O
linked	O
importance	O
sampling	O
both	O
ais	O
and	O
bridge	O
sampling	O
have	O
their	O
ad-	O
vantages	O
.	O
if	O
dkl	O
(	O
p0	O
p1	O
)	O
is	O
not	O
too	O
large	O
(	O
because	O
p0	O
and	O
p1	O
are	O
suﬃciently	O
close	O
)	O
bridge	O
sampling	O
can	O
be	O
a	O
more	O
eﬀective	O
means	O
of	O
estimating	O
the	O
ratio	O
of	O
partition	O
functions	O
than	O
ais	O
.	O
if	O
,	O
however	O
,	O
the	O
two	O
distributions	O
are	O
too	O
far	O
apart	O
for	O
a	O
single	O
distribution	O
p∗	O
to	O
bridge	O
the	O
gap	O
then	O
one	O
can	O
at	O
least	O
use	O
ais	O
with	O
potentially	O
many	O
intermediate	O
distributions	O
to	O
span	O
the	O
distance	O
between	O
p0	O
and	O
p1	O
.	O
neal	O
(	O
)	O
showed	O
how	O
his	O
linked	O
importance	O
sampling	O
method	O
leveraged	O
the	O
power	O
of	O
2005	O
the	O
bridge	O
sampling	O
strategy	O
to	O
bridge	O
the	O
intermediate	O
distributions	O
used	O
in	O
ais	O
to	O
signiﬁcantly	O
improve	O
the	O
overall	O
partition	O
function	O
estimates	O
.	O
estimating	O
the	O
partition	O
function	O
while	O
training	O
while	O
ais	O
has	O
become	O
accepted	O
as	O
the	O
standard	O
method	O
for	O
estimating	O
the	O
partition	O
function	O
for	O
many	O
undirected	O
models	O
,	O
it	O
is	O
suﬃciently	O
computationally	O
intensive	O
that	O
it	O
remains	O
infeasible	O
to	O
use	O
during	O
training	O
.	O
however	O
,	O
alternative	O
strategies	O
that	O
have	O
been	O
explored	O
to	O
maintain	O
an	O
estimate	O
of	O
the	O
partition	O
function	O
throughout	O
training	O
using	O
a	O
combination	O
of	O
bridge	O
sampling	O
,	O
short-chain	O
ais	O
and	O
parallel	O
tempering	O
,	O
)	O
devised	O
a	O
scheme	O
to	O
track	O
the	O
partition	O
function	O
of	O
an	O
et	O
al	O
.	O
(	O
2011	O
desjardins	O
629	O
chapter	O
18.	O
confronting	O
the	O
partition	O
function	O
rbm	O
throughout	O
the	O
training	O
process	O
.	O
the	O
strategy	O
is	O
based	O
on	O
the	O
maintenance	O
of	O
independent	O
estimates	O
of	O
the	O
partition	O
functions	O
of	O
the	O
rbm	O
at	O
every	O
temperature	O
operating	O
in	O
the	O
parallel	O
tempering	O
scheme	O
.	O
the	O
authors	O
combined	O
bridge	O
sampling	O
estimates	O
of	O
the	O
ratios	O
of	O
partition	O
functions	O
of	O
neighboring	O
chains	O
(	O
i.e	O
.	O
from	O
parallel	O
tempering	O
)	O
with	O
ais	O
estimates	O
across	O
time	O
to	O
come	O
up	O
with	O
a	O
low	O
variance	O
estimate	O
of	O
the	O
partition	O
functions	O
at	O
every	O
iteration	O
of	O
learning	O
.	O
the	O
tools	O
described	O
in	O
this	O
chapter	O
provide	O
many	O
diﬀerent	O
ways	O
of	O
overcoming	O
the	O
problem	O
of	O
intractable	O
partition	O
functions	O
,	O
but	O
there	O
can	O
be	O
several	O
other	O
diﬃculties	O
involved	O
in	O
training	O
and	O
using	O
generative	O
models	O
.	O
foremost	O
among	O
these	O
is	O
the	O
problem	O
of	O
intractable	O
inference	O
,	O
which	O
we	O
confront	O
next	O
.	O
630	O
chapter	O
19	O
approximate	O
inference	O
many	O
probabilistic	O
models	O
are	O
diﬃcult	O
to	O
train	O
because	O
it	O
is	O
diﬃcult	O
to	O
perform	O
inference	O
in	O
them	O
.	O
in	O
the	O
context	O
of	O
deep	O
learning	O
,	O
we	O
usually	O
have	O
a	O
set	O
of	O
visible	O
|	O
variables	O
v	O
and	O
a	O
set	O
of	O
latent	O
variables	O
h.	O
the	O
challenge	O
of	O
inference	O
usually	O
refers	O
to	O
the	O
diﬃcult	O
problem	O
of	O
computing	O
p	O
(	O
h	O
v	O
)	O
or	O
taking	O
expectations	O
with	O
respect	O
to	O
it	O
.	O
such	O
operations	O
are	O
often	O
necessary	O
for	O
tasks	O
like	O
maximum	O
likelihood	O
learning	O
.	O
many	O
simple	O
graphical	O
models	O
with	O
only	O
one	O
hidden	O
layer	O
,	O
such	O
as	O
restricted	O
|	O
boltzmann	O
machines	O
and	O
probabilistic	O
pca	O
,	O
are	O
deﬁned	O
in	O
a	O
way	O
that	O
makes	O
inference	O
operations	O
like	O
computing	O
p	O
(	O
h	O
v	O
)	O
,	O
or	O
taking	O
expectations	O
with	O
respect	O
to	O
it	O
,	O
simple	O
.	O
unfortunately	O
,	O
most	O
graphical	O
models	O
with	O
multiple	O
layers	O
of	O
hidden	O
variables	O
have	O
intractable	O
posterior	O
distributions	O
.	O
exact	O
inference	O
requires	O
an	O
exponential	O
amount	O
of	O
time	O
in	O
these	O
models	O
.	O
even	O
some	O
models	O
with	O
only	O
a	O
single	O
layer	O
,	O
such	O
as	O
sparse	O
coding	O
,	O
have	O
this	O
problem	O
.	O
in	O
this	O
chapter	O
,	O
we	O
introduce	O
several	O
of	O
the	O
techniques	O
for	O
confronting	O
these	O
,	O
we	O
will	O
describe	O
how	O
to	O
use	O
intractable	O
inference	O
problems	O
.	O
later	O
,	O
in	O
chapter	O
these	O
techniques	O
to	O
train	O
probabilistic	O
models	O
that	O
would	O
otherwise	O
be	O
intractable	O
,	O
such	O
as	O
deep	O
belief	O
networks	O
and	O
deep	O
boltzmann	O
machines	O
.	O
20	O
intractable	O
inference	O
problems	O
in	O
deep	O
learning	O
usually	O
arise	O
from	O
interactions	O
between	O
latent	O
variables	O
in	O
a	O
structured	O
graphical	O
model	B
.	O
see	O
ﬁgure	O
for	O
some	O
examples	O
.	O
these	O
interactions	O
may	O
be	O
due	O
to	O
direct	O
interactions	O
in	O
undirected	O
models	O
or	O
“	O
explaining	O
away	O
”	O
interactions	O
between	O
mutual	O
ancestors	O
of	O
the	O
same	O
visible	O
unit	O
in	O
directed	O
models	O
.	O
19.1	O
631	O
chapter	O
19.	O
approximate	O
inference	O
,	O
(	O
center	O
)	O
osindero	O
and	O
hinton	O
2008	O
figure	O
19.1	O
:	O
intractable	O
inference	O
problems	O
in	O
deep	O
learning	O
are	O
usually	O
the	O
result	O
of	O
interactions	O
between	O
latent	O
variables	O
in	O
a	O
structured	O
graphical	O
model	B
.	O
these	O
can	O
be	O
due	O
to	O
edges	O
directly	O
connecting	O
one	O
latent	O
variable	O
to	O
another	O
,	O
or	O
due	O
to	O
longer	O
paths	O
that	O
are	O
activated	O
when	O
the	O
child	O
of	O
a	O
v-structure	O
is	O
observed	O
.	O
(	O
left	O
)	O
a	O
semi-restricted	O
boltzmann	O
machine	O
(	O
)	O
with	O
connections	O
between	O
hidden	O
units	O
.	O
these	O
direct	O
connections	O
between	O
latent	O
variables	O
make	O
the	O
posterior	O
distribution	O
intractable	O
due	O
to	O
large	O
cliques	O
of	O
latent	O
variables	O
.	O
a	O
deep	O
boltzmann	O
machine	O
,	O
organized	O
into	O
layers	O
of	O
variables	O
without	O
intra-layer	O
connections	O
,	O
still	O
has	O
an	O
intractable	O
posterior	O
distribution	O
due	O
to	O
the	O
connections	O
between	O
layers	O
.	O
this	O
directed	O
model	B
has	O
interactions	O
between	O
latent	O
variables	O
when	O
the	O
visible	O
variables	O
are	O
observed	O
,	O
because	O
every	O
two	O
latent	O
variables	O
are	O
co-parents	O
.	O
some	O
probabilistic	O
models	O
are	O
able	O
to	O
provide	O
tractable	O
inference	O
over	O
the	O
latent	O
variables	O
despite	O
having	O
one	O
of	O
the	O
graph	O
structures	O
depicted	O
above	O
.	O
this	O
is	O
possible	O
if	O
the	O
conditional	O
probability	O
distributions	O
are	O
chosen	O
to	O
introduce	O
additional	O
independences	O
beyond	O
those	O
described	O
by	O
the	O
graph	O
.	O
for	O
example	O
,	O
probabilistic	O
pca	O
has	O
the	O
graph	O
structure	O
shown	O
in	O
the	O
right	O
,	O
yet	O
still	O
has	O
simple	O
inference	O
due	O
to	O
special	O
properties	O
of	O
the	O
speciﬁc	O
conditional	O
distributions	O
it	O
uses	O
(	O
linear-gaussian	O
conditionals	O
with	O
mutually	O
orthogonal	O
basis	O
vectors	O
)	O
.	O
(	O
right	O
)	O
632	O
chapter	O
19.	O
approximate	O
inference	O
19.1	O
inference	O
as	O
optimization	O
many	O
approaches	O
to	O
confronting	O
the	O
problem	O
of	O
diﬃcult	O
inference	O
make	O
use	O
of	O
the	O
observation	O
that	O
exact	O
inference	O
can	O
be	O
described	O
as	O
an	O
optimization	O
problem	O
.	O
approximate	O
inference	O
algorithms	O
may	O
then	O
be	O
derived	O
by	O
approximating	O
the	O
underlying	O
optimization	O
problem	O
.	O
to	O
construct	O
the	O
optimization	O
problem	O
,	O
assume	O
we	O
have	O
a	O
probabilistic	O
model	B
consisting	O
of	O
observed	O
variables	O
v	O
and	O
latent	O
variables	O
h.	O
we	O
would	O
like	O
to	O
compute	O
the	O
log	O
probability	O
of	O
the	O
observed	O
data	O
,	O
log	O
p	O
(	O
v	O
;	O
θ	O
)	O
.	O
sometimes	O
it	O
is	O
too	O
diﬃcult	O
to	O
compute	O
log	O
p	O
(	O
v	O
;	O
θ	O
)	O
if	O
it	O
is	O
costly	O
to	O
marginalize	O
out	O
h.	O
instead	O
,	O
we	O
can	O
compute	O
,	O
q	O
)	O
on	O
log	O
p	O
(	O
v	O
;	O
θ	O
)	O
.	O
this	O
bound	B
is	O
called	O
the	O
evidence	O
lower	O
a	O
lower	O
bound	B
bound	O
(	O
elbo	O
)	O
.	O
another	O
commonly	O
used	O
name	O
for	O
this	O
lower	O
bound	B
is	O
the	O
negative	O
variational	O
free	O
energy	O
.	O
speciﬁcally	O
,	O
the	O
evidence	O
lower	O
bound	B
is	O
deﬁned	O
to	O
be	O
l	O
(	O
v	O
θ	O
,	O
l	O
(	O
v	O
θ	O
,	O
,	O
q	O
)	O
=	O
log	O
(	O
;	O
)	O
p	O
v	O
θ	O
dkl	O
(	O
(	O
;	O
)	O
)	O
θ	O
(	O
19.1	O
)	O
−	O
|	O
q	O
h	O
v	O
	O
|	O
p	O
h	O
v	O
)	O
(	O
where	O
q	O
is	O
an	O
arbitrary	O
probability	O
distribution	O
over	O
l	O
.	O
h	O
because	O
the	O
diﬀerence	O
between	O
log	O
p	O
(	O
v	O
)	O
and	O
,	O
q	O
)	O
is	O
given	O
by	O
the	O
kl	O
l	O
divergence	O
and	O
because	O
the	O
kl	O
divergence	O
is	O
always	O
non-negative	O
,	O
we	O
can	O
see	O
that	O
always	O
has	O
at	O
most	O
the	O
same	O
value	O
as	O
the	O
desired	O
log	O
probability	O
.	O
the	O
two	O
are	O
(	O
v	O
θ	O
,	O
q	O
is	O
the	O
same	O
distribution	O
as	O
p	O
(	O
|	O
h	O
v	O
.	O
)	O
equal	O
if	O
and	O
only	O
if	O
l	O
surprisingly	O
,	O
simple	O
algebra	O
shows	O
that	O
we	O
can	O
rearrange	O
can	O
be	O
considerably	O
easier	O
to	O
compute	O
for	O
some	O
distributions	O
q.	O
into	O
a	O
much	O
more	O
convenient	O
form	O
:	O
l	O
l	O
(	O
v	O
θ	O
,	O
,	O
q	O
)	O
=	O
log	O
(	O
;	O
)	O
p	O
v	O
θ	O
=	O
log	O
(	O
;	O
)	O
p	O
v	O
θ	O
=	O
log	O
(	O
;	O
)	O
p	O
v	O
θ	O
−	O
−	O
−	O
;	O
)	O
)	O
θ	O
dkl	O
(	O
(	O
∼	O
q	O
log	O
eh	O
∼	O
q	O
log	O
eh	O
|	O
q	O
h	O
v	O
q	O
(	O
p	O
(	O
q	O
(	O
p	O
	O
|	O
|	O
(	O
p	O
h	O
v	O
)	O
|	O
h	O
v	O
)	O
|	O
)	O
h	O
v	O
)	O
h	O
v	O
,	O
(	O
h	O
v	O
θ	O
;	O
)	O
p	O
(	O
;	O
)	O
v	O
θ	O
q	O
h	O
v	O
log	O
(	O
|	O
−	O
p	O
h	O
v	O
,	O
)	O
−	O
=	O
log	O
(	O
;	O
)	O
p	O
v	O
θ	O
=	O
∼	O
q	O
[	O
log	O
(	O
|	O
−	O
eh	O
∼	O
q	O
h	O
v	O
q	O
[	O
log	O
(	O
eh	O
−	O
)	O
p	O
h	O
v	O
,	O
;	O
)	O
+	O
log	O
(	O
;	O
)	O
]	O
θ	O
p	O
v	O
θ	O
log	O
(	O
;	O
)	O
]	O
θ	O
.	O
(	O
19.2	O
)	O
(	O
19.3	O
)	O
(	O
19.4	O
)	O
(	O
19.5	O
)	O
(	O
19.6	O
)	O
this	O
yields	O
the	O
more	O
canonical	O
deﬁnition	O
of	O
the	O
evidence	O
lower	O
bound	B
,	O
l	O
for	O
an	O
appropriate	O
choice	O
of	O
q	O
,	O
is	O
tractable	O
to	O
compute	O
.	O
for	O
any	O
choice	O
)	O
that	O
are	O
better	O
|	O
of	O
q	O
,	O
provides	O
a	O
lower	O
bound	B
on	O
the	O
likelihood	O
.	O
for	O
q	O
(	O
h	O
v	O
p	O
h	O
v	O
,	O
)	O
]	O
+	O
(	O
)	O
h	O
q	O
.	O
(	O
19.7	O
)	O
l	O
(	O
v	O
θ	O
,	O
,	O
q	O
)	O
=	O
∼	O
q	O
[	O
log	O
(	O
eh	O
l	O
633	O
chapter	O
19.	O
approximate	O
inference	O
approximations	O
of	O
p	O
(	O
h	O
v	O
l	O
closer	O
to	O
log	O
p	O
(	O
v	O
)	O
.	O
when	O
q	O
(	O
h	O
v	O
(	O
v	O
θ	O
,	O
l	O
)	O
=	O
log	O
(	O
;	O
)	O
p	O
v	O
θ	O
.	O
l	O
,	O
q	O
|	O
|	O
l	O
)	O
,	O
the	O
lower	O
bound	B
)	O
=	O
p	O
(	O
h	O
v	O
|	O
will	O
be	O
tighter	O
,	O
in	O
other	O
words	O
,	O
)	O
,	O
the	O
approximation	O
is	O
perfect	O
,	O
and	O
|	O
.	O
exact	O
inference	O
maximizes	O
we	O
can	O
thus	O
think	O
of	O
inference	O
as	O
the	O
procedure	O
for	O
ﬁnding	O
the	O
q	O
that	O
maximizes	O
perfectly	O
by	O
searching	O
over	O
a	O
family	O
of	O
functions	O
q	O
that	O
includes	O
p	O
(	O
h	O
v	O
)	O
.	O
throughout	O
this	O
chapter	O
,	O
we	O
will	O
show	O
how	O
to	O
derive	O
diﬀerent	O
forms	O
of	O
approximate	O
inference	O
by	O
using	O
approximate	O
optimization	O
to	O
ﬁnd	O
q.	O
we	O
can	O
make	O
the	O
optimization	O
procedure	O
less	O
expensive	O
but	O
approximate	O
by	O
restricting	O
the	O
family	O
of	O
distributions	O
q	O
the	O
optimization	O
is	O
allowed	O
to	O
search	O
over	O
or	O
by	O
using	O
an	O
imperfect	O
optimization	O
procedure	O
that	O
may	O
not	O
completely	O
maximize	O
but	O
merely	O
increase	O
it	O
by	O
a	O
signiﬁcant	O
amount	O
.	O
l	O
no	O
matter	O
what	O
choice	O
of	O
q	O
we	O
use	O
,	O
is	O
a	O
lower	O
bound	B
.	O
we	O
can	O
get	O
tighter	O
or	O
looser	O
bounds	O
that	O
are	O
cheaper	O
or	O
more	O
expensive	O
to	O
compute	O
depending	O
on	O
how	O
we	O
choose	O
to	O
approach	O
this	O
optimization	O
problem	O
.	O
we	O
can	O
obtain	O
a	O
poorly	O
matched	O
q	O
but	O
reduce	O
the	O
computational	O
cost	O
by	O
using	O
an	O
imperfect	O
optimization	O
procedure	O
,	O
or	O
by	O
using	O
a	O
perfect	O
optimization	O
procedure	O
over	O
a	O
restricted	O
family	O
of	O
q	O
distributions	O
.	O
l	O
19.2	O
expectation	O
maximization	O
the	O
ﬁrst	O
algorithm	O
we	O
introduce	O
based	O
on	O
maximizing	O
a	O
lower	O
bound	B
is	O
the	O
expectation	O
maximization	O
(	O
em	O
)	O
algorithm	O
,	O
a	O
popular	O
training	O
algorithm	O
for	O
models	O
with	O
latent	O
variables	O
.	O
we	O
describe	O
here	O
a	O
view	O
on	O
the	O
em	O
algorithm	O
developed	O
by	O
)	O
.	O
unlike	O
most	O
of	O
the	O
other	O
algorithms	O
we	O
describe	O
in	O
this	O
chapter	O
,	O
em	O
is	O
not	O
an	O
approach	O
to	O
approximate	O
inference	O
,	O
but	O
rather	O
an	O
approach	O
to	O
learning	O
with	O
an	O
approximate	O
posterior	O
.	O
neal	O
and	O
hinton	O
1999	O
(	O
l	O
the	O
em	O
algorithm	O
consists	O
of	O
alternating	O
between	O
two	O
steps	O
until	O
convergence	O
:	O
•	O
the	O
e-step	O
(	O
expectation	O
step	O
)	O
:	O
let	O
θ	O
(	O
0	O
)	O
denote	O
the	O
value	O
of	O
the	O
parameters	O
at	O
the	O
beginning	O
of	O
the	O
step	O
.	O
set	O
q	O
(	O
h	O
(	O
)	O
i	O
v	O
(	O
)	O
i	O
;	O
θ	O
(	O
0	O
)	O
)	O
for	O
all	O
indices	O
i	O
of	O
the	O
training	O
examples	O
v	O
(	O
)	O
i	O
we	O
want	O
to	O
train	O
on	O
(	O
both	O
batch	O
and	O
|	O
minibatch	O
variants	O
are	O
valid	O
)	O
.	O
by	O
this	O
we	O
mean	O
q	O
is	O
deﬁned	O
in	O
terms	O
of	O
the	O
current	O
parameter	O
value	O
of	O
θ	O
(	O
0	O
)	O
;	O
if	O
we	O
vary	O
θ	O
then	O
p	O
(	O
h	O
v	O
;	O
θ	O
)	O
will	O
change	O
but	O
q	O
will	O
remain	O
equal	O
to	O
(	O
p	O
|	O
h	O
v	O
(	O
v	O
)	O
=	O
p	O
(	O
h	O
(	O
)	O
i	O
θ	O
(	O
0	O
)	O
)	O
.	O
|	O
|	O
)	O
|	O
h	O
v	O
	O
;	O
the	O
m-step	O
(	O
maximization	O
step	O
)	O
:	O
completely	O
or	O
partially	O
maximize	O
•	O
l	O
(	O
v	O
(	O
)	O
i	O
,	O
i	O
634	O
,	O
qθ	O
)	O
(	O
19.8	O
)	O
chapter	O
19.	O
approximate	O
inference	O
with	O
respect	O
to	O
θ	O
using	O
your	O
optimization	O
algorithm	O
of	O
choice	O
.	O
l	O
step	O
,	O
we	O
maximize	O
respect	O
to	O
.θ	O
this	O
can	O
be	O
viewed	O
as	O
a	O
coordinate	O
ascent	O
algorithm	O
to	O
maximize	O
with	O
respect	O
to	O
q	O
,	O
and	O
on	O
the	O
other	O
,	O
we	O
maximize	O
l	O
.	O
on	O
one	O
with	O
l	O
stochastic	O
gradient	O
ascent	O
on	O
latent	O
variable	O
models	O
can	O
be	O
seen	O
as	O
a	O
special	O
case	O
of	O
the	O
em	O
algorithm	O
where	O
the	O
m	O
step	O
consists	O
of	O
taking	O
a	O
single	O
gradient	O
step	O
.	O
other	O
variants	O
of	O
the	O
em	O
algorithm	O
can	O
make	O
much	O
larger	O
steps	O
.	O
for	O
some	O
model	B
families	O
,	O
the	O
m	O
step	O
can	O
even	O
be	O
performed	O
analytically	O
,	O
jumping	O
all	O
the	O
way	O
to	O
the	O
optimal	O
solution	O
for	O
given	O
the	O
current	O
θ	O
q	O
.	O
even	O
though	O
the	O
e-step	O
involves	O
exact	O
inference	O
,	O
we	O
can	O
think	O
of	O
the	O
em	O
algorithm	O
as	O
using	O
approximate	O
inference	O
in	O
some	O
sense	O
.	O
speciﬁcally	O
,	O
the	O
m-step	O
assumes	O
that	O
the	O
same	O
value	O
of	O
q	O
can	O
be	O
used	O
for	O
all	O
values	O
of	O
θ.	O
this	O
will	O
introduce	O
and	O
the	O
true	O
log	O
p	O
(	O
v	O
)	O
as	O
the	O
m-step	O
moves	O
further	O
and	O
further	O
a	O
gap	O
between	O
away	O
from	O
the	O
value	O
θ	O
(	O
0	O
)	O
used	O
in	O
the	O
e-step	O
.	O
fortunately	O
,	O
the	O
e-step	O
reduces	O
the	O
gap	O
to	O
zero	O
again	O
as	O
we	O
enter	O
the	O
loop	O
for	O
the	O
next	O
time	O
.	O
l	O
the	O
em	O
algorithm	O
contains	O
a	O
few	O
diﬀerent	O
insights	O
.	O
first	O
,	O
there	O
is	O
the	O
basic	O
structure	O
of	O
the	O
learning	O
process	O
,	O
in	O
which	O
we	O
update	O
the	O
model	B
parameters	O
to	O
improve	O
the	O
likelihood	O
of	O
a	O
completed	O
dataset	O
,	O
where	O
all	O
missing	O
variables	O
have	O
their	O
values	O
provided	O
by	O
an	O
estimate	O
of	O
the	O
posterior	O
distribution	O
.	O
this	O
particular	O
insight	O
is	O
not	O
unique	O
to	O
the	O
em	O
algorithm	O
.	O
for	O
example	O
,	O
using	O
gradient	O
descent	B
to	O
maximize	O
the	O
log-likelihood	O
also	O
has	O
this	O
same	O
property	O
;	O
the	O
log-likelihood	O
gradient	O
computations	O
require	O
taking	O
expectations	O
with	O
respect	O
to	O
the	O
posterior	O
distribution	O
over	O
the	O
hidden	O
units	O
.	O
another	O
key	O
insight	O
in	O
the	O
em	O
algorithm	O
is	O
that	O
we	O
can	O
continue	O
to	O
use	O
one	O
value	O
of	O
q	O
even	O
after	O
we	O
have	O
moved	O
to	O
a	O
diﬀerent	O
value	O
of	O
θ.	O
this	O
particular	O
insight	O
is	O
used	O
throughout	O
classical	O
machine	O
learning	O
to	O
derive	O
large	O
m-step	O
updates	O
.	O
in	O
the	O
context	O
of	O
deep	O
learning	O
,	O
most	O
models	O
are	O
too	O
complex	O
to	O
admit	O
a	O
tractable	O
solution	O
for	O
an	O
optimal	O
large	O
m-step	O
update	O
,	O
so	O
this	O
second	O
insight	O
which	O
is	O
more	O
unique	O
to	O
the	O
em	O
algorithm	O
is	O
rarely	O
used	O
.	O
19.3	O
map	O
inference	O
and	O
sparse	O
coding	O
we	O
usually	O
use	O
the	O
term	O
inference	O
to	O
refer	O
to	O
computing	O
the	O
probability	O
distribution	O
|	O
over	O
one	O
set	O
of	O
variables	O
given	O
another	O
.	O
when	O
training	O
probabilistic	O
models	O
with	O
latent	O
variables	O
,	O
we	O
are	O
usually	O
interested	O
in	O
computing	O
p	O
(	O
h	O
v	O
)	O
.	O
an	O
alternative	O
form	O
of	O
inference	O
is	O
to	O
compute	O
the	O
single	O
most	O
likely	O
value	O
of	O
the	O
missing	O
variables	O
,	O
rather	O
than	O
to	O
infer	O
the	O
entire	O
distribution	O
over	O
their	O
possible	O
values	O
.	O
in	O
the	O
context	O
635	O
chapter	O
19.	O
approximate	O
inference	O
of	O
latent	O
variable	O
models	O
,	O
this	O
means	O
computing	O
|	O
h	O
v	O
(	O
p	O
=	O
arg	O
max	O
h	O
∗	O
.	O
)	O
(	O
19.9	O
)	O
h	O
this	O
is	O
known	O
as	O
maximum	O
a	O
posteriori	O
inference	O
,	O
abbreviated	O
map	O
inference	O
.	O
map	O
inference	O
is	O
usually	O
not	O
thought	O
of	O
as	O
approximate	O
inference—it	O
does	O
l	O
compute	O
the	O
exact	O
most	O
likely	O
value	O
of	O
h	O
.	O
however	O
,	O
if	O
we	O
wish	O
to	O
develop	O
a	O
,	O
q	O
)	O
,	O
then	O
it	O
is	O
helpful	O
to	O
think	O
of	O
map	O
(	O
v	O
h	O
,	O
learning	O
process	O
based	O
on	O
maximizing	O
inference	O
as	O
a	O
procedure	O
that	O
provides	O
a	O
value	O
of	O
q.	O
in	O
this	O
sense	O
,	O
we	O
can	O
think	O
of	O
map	O
inference	O
as	O
approximate	O
inference	O
,	O
because	O
it	O
does	O
not	O
provide	O
the	O
optimal	O
q	O
.	O
∗	O
recall	O
from	O
section	O
l	O
(	O
v	O
θ	O
,	O
19.1	O
that	O
exact	O
inference	O
consists	O
of	O
maximizing	O
,	O
q	O
)	O
=	O
∼	O
q	O
[	O
log	O
(	O
eh	O
p	O
h	O
v	O
,	O
)	O
]	O
+	O
(	O
)	O
h	O
q	O
(	O
19.10	O
)	O
with	O
respect	O
to	O
q	O
over	O
an	O
unrestricted	O
family	O
of	O
probability	O
distributions	O
,	O
using	O
an	O
exact	O
optimization	O
algorithm	O
.	O
we	O
can	O
derive	O
map	O
inference	O
as	O
a	O
form	O
of	O
approximate	O
inference	O
by	O
restricting	O
the	O
family	O
of	O
distributions	O
q	O
may	O
be	O
drawn	O
from	O
.	O
speciﬁcally	O
,	O
we	O
require	O
to	O
take	O
on	O
a	O
dirac	O
distribution	O
:	O
q	O
|	O
h	O
v	O
(	O
q	O
−	O
)	O
=	O
δ	O
(	O
)	O
.	O
h	O
µ	O
(	O
19.11	O
)	O
l	O
that	O
(	O
19.12	O
)	O
this	O
means	O
that	O
we	O
can	O
now	O
control	O
q	O
entirely	O
via	O
µ.	O
dropping	O
terms	O
of	O
do	O
not	O
vary	O
with	O
,	O
we	O
are	O
left	O
with	O
the	O
optimization	O
problem	O
µ	O
∗	O
µ	O
=	O
arg	O
max	O
log	O
(	O
=	O
)	O
p	O
h	O
µ	O
v	O
,	O
,	O
µ	O
which	O
is	O
equivalent	O
to	O
the	O
map	O
inference	O
problem	O
∗	O
h	O
=	O
arg	O
max	O
h	O
|	O
h	O
v	O
(	O
p	O
.	O
)	O
(	O
19.13	O
)	O
∗	O
we	O
can	O
thus	O
justify	O
a	O
learning	O
procedure	O
similar	O
to	O
em	O
,	O
in	O
which	O
we	O
alternate	O
and	O
then	O
update	O
θ	O
to	O
increase	O
between	O
performing	O
map	O
inference	O
to	O
infer	O
h	O
∗	O
l	O
log	O
p	O
(	O
h	O
,	O
v	O
)	O
.	O
as	O
with	O
em	O
,	O
this	O
is	O
a	O
form	O
of	O
coordinate	O
ascent	O
on	O
,	O
where	O
we	O
with	O
respect	O
to	O
q	O
and	O
using	O
alternate	O
between	O
using	O
inference	O
to	O
optimize	O
l	O
with	O
respect	O
to	O
θ.	O
the	O
procedure	O
as	O
a	O
whole	O
can	O
parameter	O
updates	O
to	O
optimize	O
is	O
a	O
lower	O
bound	B
on	O
log	O
p	O
(	O
v	O
)	O
.	O
in	O
the	O
case	O
of	O
map	O
be	O
justiﬁed	O
by	O
the	O
fact	O
that	O
inference	O
,	O
this	O
justiﬁcation	O
is	O
rather	O
vacuous	O
,	O
because	O
the	O
bound	B
is	O
inﬁnitely	O
loose	O
,	O
due	O
to	O
the	O
dirac	O
distribution	O
’	O
s	O
diﬀerential	O
entropy	O
of	O
negative	O
inﬁnity	O
.	O
however	O
,	O
adding	O
noise	O
to	O
would	O
make	O
the	O
bound	B
meaningful	O
again	O
.	O
l	O
l	O
µ	O
636	O
chapter	O
19.	O
approximate	O
inference	O
map	O
inference	O
is	O
commonly	O
used	O
in	O
deep	O
learning	O
as	O
both	O
a	O
feature	O
extractor	O
and	O
a	O
learning	O
mechanism	O
.	O
it	O
is	O
primarily	O
used	O
for	O
sparse	O
coding	O
models	O
.	O
recall	O
from	O
section	O
that	O
sparse	O
coding	O
is	O
a	O
linear	O
factor	O
model	B
that	O
imposes	O
a	O
sparsity-inducing	O
prior	O
on	O
its	O
hidden	O
units	O
.	O
a	O
common	O
choice	O
is	O
a	O
factorial	O
laplace	O
prior	O
,	O
with	O
13.4	O
p	O
h	O
(	O
i	O
)	O
=	O
|	O
−	O
|	O
λ	O
hi	O
λ	O
2	O
e	O
.	O
(	O
19.14	O
)	O
the	O
visible	O
units	O
are	O
then	O
generated	O
by	O
performing	O
a	O
linear	O
transformation	O
and	O
adding	O
noise	O
:	O
|	O
n	O
)	O
=	O
(	O
p	O
x	O
h	O
(	O
;	O
v	O
w	O
h	O
b	O
+	O
,	O
β	O
−	O
1i	O
)	O
.	O
|	O
computing	O
or	O
even	O
representing	O
p	O
(	O
h	O
v	O
)	O
is	O
diﬃcult	O
.	O
every	O
pair	O
of	O
variables	O
hi	O
and	O
hj	O
are	O
both	O
parents	O
of	O
v.	O
this	O
means	O
that	O
when	O
v	O
is	O
observed	O
,	O
the	O
graphical	O
model	B
contains	O
an	O
active	O
path	O
connecting	O
hi	O
and	O
hj	O
.	O
all	O
of	O
the	O
hidden	O
units	O
thus	O
participate	O
in	O
one	O
massive	O
clique	O
in	O
p	O
(	O
h	O
v	O
)	O
.	O
if	O
the	O
model	B
were	O
gaussian	O
then	O
these	O
interactions	O
could	O
be	O
modeled	O
eﬃciently	O
via	O
the	O
covariance	O
matrix	O
,	O
but	O
the	O
sparse	O
prior	O
makes	O
these	O
interactions	O
non-gaussian	O
.	O
|	O
(	O
19.15	O
)	O
)	O
is	O
intractable	O
,	O
so	O
is	O
the	O
computation	O
of	O
the	O
log-likelihood	O
and	O
its	O
gradient	O
.	O
we	O
thus	O
can	O
not	O
use	O
exact	O
maximum	O
likelihood	O
learning	O
.	O
instead	O
,	O
we	O
use	O
map	O
inference	O
and	O
learn	O
the	O
parameters	O
by	O
maximizing	O
the	O
elbo	O
deﬁned	O
by	O
the	O
dirac	O
distribution	O
around	O
the	O
map	O
estimate	O
of	O
.h	O
|	O
because	O
p	O
(	O
h	O
v	O
	O
	O
	O
	O
if	O
we	O
concatenate	O
all	O
of	O
the	O
h	O
vectors	O
in	O
the	O
training	O
set	O
into	O
a	O
matrix	O
h	O
,	O
and	O
,	O
then	O
the	O
sparse	O
coding	O
learning	O
vectors	O
into	O
a	O
matrix	O
v	O
concatenate	O
all	O
of	O
the	O
process	O
consists	O
of	O
minimizing	O
v	O
j	O
(	O
h	O
w	O
)	O
=	O
,	O
|	O
|	O
hi	O
,	O
j	O
+	O
i	O
,	O
j	O
i	O
,	O
j	O
−	O
v	O
hw	O
	O
2	O
i	O
,	O
j	O
.	O
(	O
19.16	O
)	O
most	O
applications	O
of	O
sparse	O
coding	O
also	O
involve	O
weight	O
decay	O
or	O
a	O
constraint	O
on	O
the	O
norms	O
of	O
the	O
columns	O
of	O
w	O
,	O
in	O
order	O
to	O
prevent	O
the	O
pathological	O
solution	O
with	O
extremely	O
small	O
and	O
large	O
w	O
h	O
.	O
we	O
can	O
minimize	O
j	O
by	O
alternating	O
between	O
minimization	O
with	O
respect	O
to	O
h	O
and	O
minimization	O
with	O
respect	O
to	O
w	O
.	O
both	O
sub-problems	O
are	O
convex	O
.	O
in	O
fact	O
,	O
the	O
minimization	O
with	O
respect	O
to	O
w	O
is	O
just	O
a	O
linear	O
regression	O
problem	O
.	O
however	O
,	O
minimization	O
of	O
j	O
with	O
respect	O
to	O
both	O
arguments	O
is	O
usually	O
not	O
a	O
convex	O
problem	O
.	O
minimization	O
with	O
respect	O
to	O
h	O
requires	O
specialized	O
algorithms	O
such	O
as	O
the	O
feature-sign	O
search	O
algorithm	O
(	O
lee	O
et	O
al	O
.	O
2007	O
,	O
)	O
.	O
637	O
chapter	O
19.	O
approximate	O
inference	O
19.4	O
variational	O
inference	O
and	O
learning	O
l	O
l	O
,	O
q	O
)	O
is	O
a	O
lower	O
bound	B
on	O
(	O
v	O
θ	O
,	O
we	O
have	O
seen	O
how	O
the	O
evidence	O
lower	O
bound	B
l	O
with	O
respect	O
to	O
q	O
,	O
and	O
log	O
p	O
(	O
v	O
;	O
θ	O
)	O
,	O
how	O
inference	O
can	O
be	O
viewed	O
as	O
maximizing	O
with	O
respect	O
to	O
θ.	O
we	O
have	O
seen	O
how	O
learning	O
can	O
be	O
viewed	O
as	O
maximizing	O
that	O
the	O
em	O
algorithm	O
allows	O
us	O
to	O
make	O
large	O
learning	O
steps	O
with	O
a	O
ﬁxed	O
q	O
and	O
|	O
that	O
learning	O
algorithms	O
based	O
on	O
map	O
inference	O
allow	O
us	O
to	O
learn	O
using	O
a	O
point	O
estimate	O
of	O
p	O
(	O
h	O
v	O
)	O
rather	O
than	O
inferring	O
the	O
entire	O
distribution	O
.	O
now	O
we	O
develop	O
the	O
more	O
general	O
approach	O
to	O
variational	O
learning	O
.	O
the	O
core	O
idea	O
behind	O
variational	O
learning	O
is	O
that	O
we	O
can	O
maximize	O
over	O
a	O
restricted	O
family	O
of	O
distributions	O
q	O
.	O
this	O
family	O
should	O
be	O
chosen	O
so	O
that	O
it	O
is	O
easy	O
to	O
compute	O
eq	O
log	O
p	O
(	O
h	O
v	O
,	O
)	O
.	O
a	O
typical	O
way	O
to	O
do	O
this	O
is	O
to	O
introduce	O
assumptions	O
about	O
how	O
factorizes	O
.	O
q	O
l	O
	O
a	O
common	O
approach	O
to	O
variational	O
learning	O
is	O
to	O
impose	O
the	O
restriction	O
that	O
q	O
is	O
a	O
factorial	O
distribution	O
:	O
|	O
h	O
v	O
q	O
(	O
)	O
=	O
|	O
v	O
)	O
.	O
q	O
h	O
(	O
i	O
i	O
(	O
19.17	O
)	O
this	O
is	O
called	O
the	O
mean	O
ﬁeld	O
approach	O
.	O
more	O
generally	O
,	O
we	O
can	O
impose	O
any	O
graphi-	O
cal	O
model	B
structure	O
we	O
choose	O
on	O
q	O
,	O
to	O
ﬂexibly	O
determine	O
how	O
many	O
interactions	O
we	O
want	O
our	O
approximation	O
to	O
capture	O
.	O
this	O
fully	O
general	O
graphical	O
model	B
approach	O
is	O
called	O
structured	O
variational	O
inference	O
(	O
saul	O
and	O
jordan	O
1996	O
)	O
.	O
,	O
the	O
beauty	O
of	O
the	O
variational	O
approach	O
is	O
that	O
we	O
do	O
not	O
need	O
to	O
specify	O
a	O
speciﬁc	O
parametric	O
form	O
for	O
q.	O
we	O
specify	O
how	O
it	O
should	O
factorize	O
,	O
but	O
then	O
the	O
optimization	O
problem	O
determines	O
the	O
optimal	O
probability	O
distribution	O
within	O
those	O
factorization	O
constraints	O
.	O
for	O
discrete	O
latent	O
variables	O
,	O
this	O
just	O
means	O
that	O
we	O
use	O
traditional	O
optimization	O
techniques	O
to	O
optimize	O
a	O
ﬁnite	O
number	O
of	O
variables	O
describing	O
the	O
q	O
distribution	O
.	O
for	O
continuous	O
latent	O
variables	O
,	O
this	O
means	O
that	O
we	O
use	O
a	O
branch	O
of	O
mathematics	O
called	O
calculus	O
of	O
variations	O
to	O
perform	O
optimization	O
over	O
a	O
space	O
of	O
functions	O
,	O
and	O
actually	O
determine	O
which	O
function	O
should	O
be	O
used	O
to	O
represent	O
q	O
.	O
calculus	O
of	O
variations	O
is	O
the	O
origin	O
of	O
the	O
names	O
“	O
variational	O
learning	O
”	O
and	O
“	O
variational	O
inference	O
,	O
”	O
though	O
these	O
names	O
apply	O
even	O
when	O
the	O
latent	O
variables	O
are	O
discrete	O
and	O
calculus	O
of	O
variations	O
is	O
not	O
needed	O
.	O
in	O
the	O
case	O
of	O
continuous	O
latent	O
variables	O
,	O
calculus	O
of	O
variations	O
is	O
a	O
powerful	O
technique	O
that	O
removes	O
much	O
of	O
the	O
responsibility	O
from	O
the	O
human	O
designer	O
of	O
the	O
model	B
,	O
who	O
now	O
must	O
specify	O
only	O
how	O
q	O
factorizes	O
,	O
rather	O
than	O
needing	O
to	O
guess	O
how	O
to	O
design	O
a	O
speciﬁc	O
that	O
can	O
accurately	O
approximate	O
the	O
posterior	O
.	O
l	O
	O
|	O
|	O
	O
|	O
;	O
θ	O
)	O
)	O
,	O
we	O
p	O
(	O
h	O
v	O
)	O
)	O
)	O
.	O
with	O
respect	O
to	O
q	O
as	O
minimizing	O
dkl	O
(	O
q	O
(	O
h	O
v	O
p	O
(	O
h	O
v	O
|	O
dkl	O
(	O
q	O
(	O
h	O
v	O
,	O
q	O
)	O
is	O
deﬁned	O
to	O
be	O
log	O
p	O
(	O
v	O
;	O
θ	O
)	O
can	O
think	O
of	O
maximizing	O
−	O
because	O
(	O
v	O
θ	O
,	O
l	O
q	O
)	O
638	O
chapter	O
19.	O
approximate	O
inference	O
3.6	O
	O
pmodel	O
)	O
.	O
as	O
illustrated	O
in	O
ﬁgure	O
in	O
this	O
sense	O
,	O
we	O
are	O
ﬁtting	O
q	O
to	O
p	O
.	O
however	O
,	O
we	O
are	O
doing	O
so	O
with	O
the	O
opposite	O
direction	O
of	O
the	O
kl	O
divergence	O
than	O
we	O
are	O
used	O
to	O
using	O
for	O
ﬁtting	O
an	O
approximation	O
.	O
when	O
we	O
use	O
maximum	O
likelihood	O
learning	O
to	O
ﬁt	O
a	O
model	B
to	O
data	O
,	O
we	O
minimize	O
d	O
kl	O
(	O
pdata	O
,	O
this	O
means	O
that	O
maximum	O
likelihood	O
encourages	O
the	O
model	B
to	O
have	O
high	O
probability	O
everywhere	O
that	O
the	O
data	O
has	O
high	O
probability	O
,	O
while	O
our	O
optimization-based	O
inference	O
procedure	O
encourages	O
q	O
to	O
have	O
low	O
probability	O
everywhere	O
the	O
true	O
posterior	O
has	O
low	O
probability	O
.	O
both	O
directions	O
of	O
the	O
kl	O
divergence	O
can	O
have	O
desirable	O
and	O
undesirable	O
properties	O
.	O
the	O
choice	O
of	O
which	O
to	O
use	O
depends	O
on	O
which	O
properties	O
are	O
the	O
highest	O
priority	O
for	O
	O
|	O
|	O
each	O
application	O
.	O
in	O
the	O
case	O
of	O
the	O
inference	O
optimization	O
problem	O
,	O
we	O
choose	O
|	O
|	O
	O
to	O
use	O
dkl	O
(	O
q	O
(	O
h	O
v	O
p	O
(	O
h	O
v	O
)	O
)	O
)	O
for	O
computational	O
reasons	O
.	O
speciﬁcally	O
,	O
computing	O
d	O
kl	O
(	O
q	O
(	O
h	O
v	O
p	O
(	O
h	O
v	O
)	O
)	O
involves	O
evaluating	O
expectations	O
with	O
respect	O
to	O
q	O
,	O
so	O
by	O
)	O
designing	O
q	O
to	O
be	O
simple	O
,	O
we	O
can	O
simplify	O
the	O
required	O
expectations	O
.	O
the	O
opposite	O
direction	O
of	O
the	O
kl	O
divergence	O
would	O
require	O
computing	O
expectations	O
with	O
respect	O
to	O
the	O
true	O
posterior	O
.	O
because	O
the	O
form	O
of	O
the	O
true	O
posterior	O
is	O
determined	O
by	O
the	O
choice	O
of	O
model	B
,	O
we	O
can	O
not	O
design	O
a	O
reduced-cost	O
approach	O
to	O
computing	O
d	O
kl	O
(	O
(	O
	O
|	O
q	O
h	O
v	O
(	O
)	O
|	O
p	O
h	O
v	O
exactly.	O
)	O
)	O
19.4.1	O
discrete	O
latent	O
variables	O
variational	O
inference	O
with	O
discrete	O
latent	O
variables	O
is	O
relatively	O
straightforward	O
.	O
we	O
deﬁne	O
a	O
distribution	O
q	O
,	O
typically	O
one	O
where	O
each	O
factor	O
of	O
q	O
is	O
just	O
deﬁned	O
by	O
a	O
lookup	O
table	O
over	O
discrete	O
states	O
.	O
in	O
the	O
simplest	O
case	O
,	O
h	O
is	O
binary	O
and	O
we	O
make	O
the	O
mean	O
ﬁeld	O
assumption	O
that	O
hi	O
.	O
in	O
this	O
case	O
we	O
can	O
parametrize	O
q	O
with	O
a	O
vector	O
ˆh	O
whose	O
entries	O
are	O
probabilities	O
.	O
then	O
|	O
q	O
h	O
(	O
i	O
=	O
1	O
factorizes	O
over	O
each	O
individual	O
ˆhi.	O
)	O
=	O
v	O
q	O
after	O
determining	O
how	O
to	O
represent	O
q	O
,	O
we	O
simply	O
optimize	O
its	O
parameters	O
.	O
in	O
the	O
case	O
of	O
discrete	O
latent	O
variables	O
,	O
this	O
is	O
just	O
a	O
standard	O
optimization	O
problem	O
.	O
in	O
principle	O
the	O
selection	O
of	O
q	O
could	O
be	O
done	O
with	O
any	O
optimization	O
algorithm	O
,	O
such	O
as	O
gradient	O
descent	B
.	O
because	O
this	O
optimization	O
must	O
occur	O
in	O
the	O
inner	O
loop	O
of	O
a	O
learning	O
algorithm	O
,	O
it	O
must	O
be	O
very	O
fast	O
.	O
to	O
achieve	O
this	O
speed	O
,	O
we	O
typically	O
use	O
special	O
optimization	O
algorithms	O
that	O
are	O
designed	O
to	O
solve	O
comparatively	O
small	O
and	O
simple	O
problems	O
in	O
very	O
few	O
iterations	O
.	O
a	O
popular	O
choice	O
is	O
to	O
iterate	O
ﬁxed	O
point	O
equations	O
,	O
in	O
other	O
words	O
,	O
to	O
solve	O
l	O
∂	O
∂ˆhi	O
=	O
0	O
(	O
19.18	O
)	O
for	O
ˆhi	O
.	O
we	O
repeatedly	O
update	O
diﬀerent	O
elements	O
of	O
ˆh	O
until	O
we	O
satisfy	O
a	O
convergence	O
639	O
chapter	O
19.	O
approximate	O
inference	O
criterion	O
.	O
2010	O
to	O
make	O
this	O
more	O
concrete	O
,	O
we	O
show	O
how	O
to	O
apply	O
variational	O
inference	O
to	O
the	O
binary	O
sparse	O
coding	O
model	B
(	O
we	O
present	O
here	O
the	O
model	B
developed	O
by	O
henniges	O
et	O
al	O
.	O
(	O
)	O
but	O
demonstrate	O
traditional	O
,	O
generic	O
mean	O
ﬁeld	O
applied	O
to	O
the	O
model	B
,	O
while	O
they	O
introduce	O
a	O
specialized	O
algorithm	O
)	O
.	O
this	O
derivation	O
goes	O
into	O
considerable	O
mathematical	O
detail	O
and	O
is	O
intended	O
for	O
the	O
reader	O
who	O
wishes	O
to	O
fully	O
resolve	O
any	O
ambiguity	O
in	O
the	O
high-level	O
conceptual	O
description	O
of	O
variational	O
inference	O
and	O
learning	O
we	O
have	O
presented	O
so	O
far	O
.	O
readers	O
who	O
do	O
not	O
plan	O
to	O
derive	O
or	O
implement	O
variational	O
learning	O
algorithms	O
may	O
safely	O
skip	O
to	O
the	O
next	O
section	O
without	O
missing	O
any	O
new	O
high-level	O
concepts	O
.	O
readers	O
who	O
proceed	O
with	O
the	O
binary	O
sparse	O
coding	O
example	O
are	O
encouraged	O
to	O
review	O
the	O
list	O
of	O
useful	O
properties	O
of	O
functions	O
that	O
commonly	O
arise	O
in	O
probabilistic	O
models	O
in	O
section	O
.	O
we	O
use	O
these	O
properties	O
liberally	O
throughout	O
the	O
following	O
derivations	O
without	O
highlighting	O
exactly	O
where	O
we	O
use	O
each	O
one	O
.	O
3.10	O
in	O
the	O
binary	O
sparse	O
coding	O
model	B
,	O
the	O
input	O
v	O
n	O
is	O
generated	O
from	O
the	O
model	B
by	O
adding	O
gaussian	O
noise	O
to	O
the	O
sum	O
of	O
m	O
diﬀerent	O
components	O
which	O
can	O
each	O
be	O
present	O
or	O
absent	O
.	O
each	O
component	O
is	O
switched	O
on	O
or	O
oﬀ	O
by	O
the	O
corresponding	O
hidden	O
unit	O
in	O
h	O
0	O
1	O
,	O
m	O
:	O
∈	O
{	O
}	O
r	O
∈	O
p	O
h	O
(	O
i	O
=	O
1	O
)	O
=	O
(	O
σ	O
bi	O
)	O
|	O
n	O
)	O
=	O
(	O
p	O
v	O
h	O
(	O
;	O
v	O
w	O
h	O
β	O
,	O
−	O
1	O
)	O
(	O
19.19	O
)	O
(	O
19.20	O
)	O
where	O
b	O
is	O
a	O
learnable	O
set	O
of	O
biases	O
,	O
w	O
is	O
a	O
learnable	O
weight	O
matrix	O
,	O
and	O
β	O
is	O
a	O
learnable	O
,	O
diagonal	O
precision	O
matrix	O
.	O
training	O
this	O
model	B
with	O
maximum	O
likelihood	O
requires	O
taking	O
the	O
derivative	O
with	O
respect	O
to	O
the	O
parameters	O
.	O
consider	O
the	O
derivative	O
with	O
respect	O
to	O
one	O
of	O
the	O
biases	O
:	O
log	O
(	O
)	O
p	O
v	O
∂	O
∂bi	O
∂	O
∂b	O
i	O
	O
	O
p	O
(	O
)	O
v	O
=	O
=	O
p	O
(	O
)	O
v	O
∂	O
∂b	O
i	O
p	O
,	O
(	O
h	O
v	O
)	O
h	O
p	O
(	O
)	O
v	O
|	O
∂	O
∂b	O
i	O
=	O
h	O
p	O
(	O
)	O
h	O
(	O
p	O
)	O
v	O
h	O
p	O
(	O
)	O
v	O
640	O
(	O
19.21	O
)	O
(	O
19.22	O
)	O
(	O
19.23	O
)	O
(	O
19.24	O
)	O
	O
	O
h	O
=	O
=	O
|	O
p	O
(	O
v	O
h	O
)	O
p	O
(	O
)	O
v	O
|	O
h	O
v	O
)	O
p	O
(	O
p	O
(	O
)	O
h	O
∂	O
∂b	O
i	O
p	O
(	O
)	O
h	O
∂	O
∂bi	O
p	O
(	O
)	O
h	O
h	O
∼	O
=eh	O
p	O
(	O
h	O
v	O
)	O
|	O
∂	O
∂b	O
i	O
log	O
(	O
)	O
p	O
h	O
.	O
(	O
19.25	O
)	O
(	O
19.26	O
)	O
(	O
19.27	O
)	O
chapter	O
19.	O
approximate	O
inference	O
h1h1	O
h2h2	O
h3h3	O
h4h4	O
v1v1	O
v2v2	O
v3v3	O
h1h1	O
h2h2	O
h3h3	O
h4h4	O
figure	O
19.2	O
:	O
the	O
graph	O
structure	O
of	O
a	O
binary	O
sparse	O
coding	O
model	B
with	O
four	O
hidden	O
units	O
.	O
|	O
(	O
left	O
)	O
the	O
graph	O
structure	O
of	O
p	O
(	O
h	O
v	O
,	O
)	O
.	O
note	O
that	O
the	O
edges	O
are	O
directed	O
,	O
and	O
that	O
every	O
two	O
hidden	O
units	O
are	O
co-parents	O
of	O
every	O
visible	O
unit	O
.	O
)	O
.	O
p	O
(	O
h	O
v	O
in	O
order	O
to	O
account	O
for	O
the	O
active	O
paths	O
between	O
co-parents	O
,	O
the	O
posterior	O
distribution	O
needs	O
an	O
edge	O
between	O
all	O
of	O
the	O
hidden	O
units	O
.	O
the	O
graph	O
structure	O
of	O
(	O
right	O
)	O
learning	O
instead	O
.	O
we	O
can	O
make	O
a	O
mean	O
ﬁeld	O
approximation	O
:	O
	O
)	O
is	O
a	O
complicated	O
distribution	O
.	O
see	O
ﬁgure	O
|	O
|	O
this	O
requires	O
computing	O
expectations	O
with	O
respect	O
to	O
p	O
(	O
h	O
v	O
)	O
.	O
unfortunately	O
,	O
|	O
p	O
(	O
h	O
v	O
for	O
the	O
graph	O
structure	O
of	O
p	O
(	O
h	O
v	O
,	O
)	O
and	O
p	O
(	O
h	O
v	O
)	O
.	O
the	O
posterior	O
distribution	O
corresponds	O
to	O
the	O
complete	O
graph	O
over	O
the	O
hidden	O
units	O
,	O
so	O
variable	O
elimination	O
algorithms	O
do	O
not	O
help	O
us	O
to	O
compute	O
the	O
required	O
expectations	O
any	O
faster	O
than	O
brute	O
force	O
.	O
19.2	O
we	O
can	O
resolve	O
this	O
diﬃculty	O
by	O
using	O
variational	O
inference	O
and	O
variational	O
|	O
h	O
v	O
q	O
(	O
)	O
=	O
|	O
v	O
)	O
.	O
q	O
h	O
(	O
i	O
i	O
(	O
19.28	O
)	O
the	O
latent	O
variables	O
of	O
the	O
binary	O
sparse	O
coding	O
model	B
are	O
binary	O
,	O
so	O
to	O
represent	O
a	O
factorial	O
q	O
we	O
simply	O
need	O
to	O
model	B
m	O
bernoulli	O
distributions	O
q	O
(	O
hi	O
v	O
)	O
.	O
a	O
natural	O
way	O
to	O
represent	O
the	O
means	O
of	O
the	O
bernoulli	O
distributions	O
is	O
with	O
a	O
vector	O
ˆh	O
of	O
v	O
)	O
=	O
ˆhi	O
.	O
we	O
impose	O
a	O
restriction	O
that	O
ˆh	O
i	O
is	O
never	O
probabilities	O
,	O
with	O
q	O
(	O
hi	O
=	O
1	O
equal	O
to	O
0	O
or	O
to	O
1	O
,	O
in	O
order	O
to	O
avoid	O
errors	O
when	O
computing	O
,	O
for	O
example	O
,	O
log	O
ˆhi	O
.	O
ˆhi	O
we	O
will	O
see	O
that	O
the	O
variational	O
inference	O
equations	O
never	O
assign	O
or	O
to	O
0	O
1	O
|	O
|	O
641	O
chapter	O
19.	O
approximate	O
inference	O
1	O
0	O
or	O
analytically	O
.	O
however	O
,	O
in	O
a	O
software	O
implementation	O
,	O
machine	O
rounding	O
error	O
could	O
result	O
in	O
values	O
.	O
in	O
software	O
,	O
we	O
may	O
wish	O
to	O
implement	O
binary	O
sparse	O
coding	O
using	O
an	O
unrestricted	O
vector	O
of	O
variational	O
parameters	O
z	O
and	O
obtain	O
ˆh	O
via	O
the	O
relation	O
ˆh	O
=	O
σ	O
(	O
z	O
)	O
.	O
we	O
can	O
thus	O
safely	O
compute	O
log	O
ˆhi	O
on	O
a	O
computer	O
by	O
using	O
−	O
the	O
identity	O
log	O
(	O
σ	O
zi	O
)	O
=	O
ζ	O
−	O
zi	O
)	O
relating	O
the	O
sigmoid	O
and	O
the	O
softplus	O
.	O
(	O
to	O
begin	O
our	O
derivation	O
of	O
variational	O
learning	O
in	O
the	O
binary	O
sparse	O
coding	O
model	B
,	O
we	O
show	O
that	O
the	O
use	O
of	O
this	O
mean	O
ﬁeld	O
approximation	O
makes	O
learning	O
tractable	O
.	O
,	O
q	O
	O
the	O
evidence	O
lower	O
bound	B
is	O
given	O
by	O
l	O
v	O
θ	O
,	O
(	O
∼	O
=eh	O
q	O
[	O
log	O
(	O
∼	O
=eh	O
q	O
[	O
log	O
(	O
)	O
+	O
log	O
(	O
	O
	O
|	O
−	O
)	O
]	O
+	O
(	O
)	O
h	O
q	O
p	O
v	O
h	O
)	O
p	O
h	O
v	O
,	O
log	O
(	O
p	O
h	O
)	O
	O
)	O
]	O
m	O
|	O
q	O
h	O
v	O
|	O
−	O
h	O
)	O
	O
|	O
v	O
)	O
log	O
(	O
p	O
hi	O
)	O
+	O
ˆhi	O
(	O
log	O
(	O
σ	O
bi	O
)	O
log	O
∼	O
+	O
eh	O
q	O
log	O
βi	O
2π	O
	O
	O
∼	O
=eh	O
q	O
=	O
m	O
i=1	O
	O
m	O
=	O
m	O
i=1	O
	O
	O
	O
i=1	O
n	O
	O
	O
i=1	O
+	O
1	O
2	O
n	O
i=1	O
	O
−	O
−	O
−	O
n	O
i=1	O
i=1	O
log	O
(	O
p	O
vi	O
	O
log	O
(	O
q	O
hi	O
−	O
−	O
−	O
ˆhi	O
)	O
(	O
log	O
(	O
σ	O
ˆhi	O
)	O
+	O
(	O
1	O
b	O
i	O
)	O
−	O
βi	O
2	O
exp	O
−	O
(	O
vi	O
wi	O
,	O
:h	O
)	O
2	O
	O
−	O
ˆhi	O
)	O
(	O
log	O
(	O
σ	O
−	O
b	O
i	O
)	O
	O
	O
−	O
	O
−	O
ˆhi	O
)	O
)	O
	O
−	O
ˆhi	O
)	O
)	O
log	O
(	O
1	O
log	O
(	O
1	O
	O
	O
(	O
19.29	O
)	O
(	O
19.30	O
)	O
(	O
19.31	O
)	O
(	O
19.32	O
)	O
(	O
19.33	O
)	O
(	O
19.34	O
)	O
	O
(	O
19.35	O
)	O
ˆhi	O
(	O
log	O
(	O
σ	O
bi	O
)	O
log	O
ˆhi	O
)	O
+	O
(	O
1	O
log	O
βi	O
2π	O
−	O
βi	O
v2	O
i	O
2viwi	O
,	O
:	O
ˆh	O
+	O
w	O
2	O
i	O
,	O
j	O
ˆhj	O
+	O
wi	O
,	O
jwi	O
,	O
k	O
ˆhj	O
ˆhk	O
.	O
j	O
k	O
j	O
=	O
while	O
these	O
equations	O
are	O
somewhat	O
unappealing	O
aesthetically	O
,	O
they	O
show	O
that	O
can	O
be	O
expressed	O
in	O
a	O
small	O
number	O
of	O
simple	O
arithmetic	O
operations	O
.	O
the	O
evidence	O
lower	O
bound	B
as	O
a	O
replacement	O
for	O
the	O
intractable	O
log-likelihood	O
.	O
is	O
therefore	O
tractable	O
.	O
we	O
can	O
use	O
l	O
l	O
(	O
19.36	O
)	O
l	O
in	O
principle	O
,	O
we	O
could	O
simply	O
run	O
gradient	O
ascent	O
on	O
both	O
v	O
and	O
h	O
and	O
this	O
would	O
make	O
a	O
perfectly	O
acceptable	O
combined	O
inference	O
and	O
training	O
algorithm	O
.	O
usually	O
,	O
however	O
,	O
we	O
do	O
not	O
do	O
this	O
,	O
for	O
two	O
reasons	O
.	O
first	O
,	O
this	O
would	O
require	O
storing	O
ˆh	O
for	O
each	O
v.	O
we	O
typically	O
prefer	O
algorithms	O
that	O
do	O
not	O
require	O
per-	O
example	O
memory	O
.	O
it	O
is	O
diﬃcult	O
to	O
scale	O
learning	O
algorithms	O
to	O
billions	O
of	O
examples	O
if	O
we	O
must	O
remember	O
a	O
dynamically	O
updated	O
vector	O
associated	O
with	O
each	O
example	O
.	O
642	O
	O
chapter	O
19.	O
approximate	O
inference	O
second	O
,	O
we	O
would	O
like	O
to	O
be	O
able	O
to	O
extract	O
the	O
features	O
ˆh	O
very	O
quickly	O
,	O
in	O
order	O
to	O
recognize	O
the	O
content	O
of	O
v	O
.	O
in	O
a	O
realistic	O
deployed	O
setting	O
,	O
we	O
would	O
need	O
to	O
be	O
able	O
to	O
compute	O
ˆh	O
in	O
real	O
time	O
.	O
for	O
both	O
these	O
reasons	O
,	O
we	O
typically	O
do	O
not	O
use	O
gradient	O
descent	B
to	O
compute	O
the	O
mean	O
ﬁeld	O
parameters	O
ˆh	O
.	O
instead	O
,	O
we	O
rapidly	O
estimate	O
them	O
with	O
ﬁxed	O
point	O
equations	O
.	O
the	O
idea	O
behind	O
ﬁxed	O
point	O
equations	O
is	O
that	O
we	O
are	O
seeking	O
a	O
local	O
maximum	O
with	O
respect	O
to	O
ˆh	O
,	O
where	O
,	O
ˆh	O
)	O
=	O
0	O
.	O
we	O
can	O
not	O
eﬃciently	O
solve	O
this	O
equation	O
with	O
respect	O
to	O
all	O
of	O
ˆh	O
simultaneously	O
.	O
however	O
,	O
we	O
can	O
solve	O
for	O
a	O
single	O
variable	O
:	O
(	O
v	O
θ	O
,	O
h	O
∇	O
l	O
,	O
ˆh	O
)	O
=	O
0	O
.	O
(	O
19.37	O
)	O
l	O
(	O
v	O
θ	O
,	O
∂	O
∂	O
ˆhi	O
we	O
can	O
then	O
iteratively	O
apply	O
the	O
solution	O
to	O
the	O
equation	O
for	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
,	O
and	O
repeat	O
the	O
cycle	O
until	O
we	O
satisfy	O
a	O
converge	O
criterion	O
.	O
common	O
convergence	O
criteria	O
include	O
stopping	O
when	O
a	O
full	O
cycle	O
of	O
updates	O
does	O
not	O
improve	O
by	O
more	O
than	O
some	O
tolerance	O
amount	O
,	O
or	O
when	O
the	O
cycle	O
does	O
not	O
change	O
ˆh	O
by	O
more	O
than	O
some	O
amount	O
.	O
l	O
iterating	O
mean	O
ﬁeld	O
ﬁxed	O
point	O
equations	O
is	O
a	O
general	O
technique	O
that	O
can	O
provide	O
fast	O
variational	O
inference	O
in	O
a	O
broad	O
variety	O
of	O
models	O
.	O
to	O
make	O
this	O
more	O
concrete	O
,	O
we	O
show	O
how	O
to	O
derive	O
the	O
updates	O
for	O
the	O
binary	O
sparse	O
coding	O
model	B
in	O
particular	O
.	O
first	O
,	O
we	O
must	O
write	O
an	O
expression	O
for	O
the	O
derivatives	O
with	O
respect	O
to	O
ˆhi	O
.	O
to	O
do	O
so	O
,	O
we	O
substitute	O
equation	O
19.36	O
into	O
the	O
left	O
side	O
of	O
equation	O
	O
−	O
	O
−	O
ˆhj	O
)	O
(	O
log	O
(	O
σ	O
−	O
bj	O
)	O
	O
log	O
ˆh	O
j	O
)	O
+	O
(	O
1	O
−	O
	O
log	O
(	O
1	O
ˆhj	O
(	O
log	O
(	O
σ	O
bj	O
)	O
∂	O
∂	O
ˆhi	O
∂	O
∂	O
ˆhi	O
=	O
+	O
1	O
2	O
,	O
ˆh	O
)	O
m	O
l	O
(	O
v	O
θ	O
,	O
	O
	O
	O
	O
	O
	O
j=1	O
j=1	O
n	O
−	O
	O
log	O
	O
=	O
log	O
(	O
σ	O
bi	O
)	O
log	O
−	O
βj	O
2π	O
−	O
β	O
j	O
v	O
2	O
j	O
−	O
ˆhi	O
1	O
+	O
log	O
(	O
1	O
−	O
−	O
1	O
2	O
w	O
2	O
j	O
,	O
i	O
n	O
+	O
j=1	O
βj	O
vjwj	O
,	O
i	O
19.37	O
:	O
	O
−	O
ˆhj	O
)	O
)	O
(	O
19.38	O
)	O
	O
(	O
19.39	O
)	O
(	O
19.40	O
)	O
(	O
19.41	O
)	O
(	O
19.42	O
)	O
ˆh	O
+	O
k	O
2v	O
jwj	O
,	O
:	O
	O
−	O
−	O
−	O
bi	O
)	O
log	O
(	O
σ	O
ˆh	O
i	O
)	O
+	O
1	O
w	O
2	O
j	O
,	O
k	O
	O
ˆhk	O
+	O
wj	O
,	O
kwj	O
,	O
l	O
ˆhk	O
ˆhl	O
l	O
k	O
=	O
wj	O
,	O
kwj	O
,	O
i	O
ˆhk	O
k	O
i	O
=	O
643	O
	O
	O
chapter	O
19.	O
approximate	O
inference	O
−	O
=bi	O
log	O
ˆhi	O
+	O
log	O
(	O
1	O
−	O
ˆhi	O
)	O
+	O
v	O
	O
βw	O
:	O
,i	O
−	O
1	O
2	O
w	O
	O
:	O
,iβw	O
:	O
,i	O
	O
:	O
,j	O
βw	O
:	O
,i	O
w	O
ˆhj	O
.	O
(	O
19.43	O
)	O
	O
	O
j	O
i	O
=	O
−	O
	O
	O
	O
	O
644	O
to	O
apply	O
the	O
ﬁxed	O
point	O
update	O
inference	O
rule	O
,	O
we	O
solve	O
for	O
the	O
ˆhi	O
that	O
sets	O
equation	O
19.43	O
to	O
0	O
:	O
ˆhi	O
=	O
σ	O
bi	O
+	O
v	O
	O
βw	O
:	O
,i	O
−	O
1	O
2	O
	O
:	O
,i	O
βw	O
:	O
,i	O
w	O
−	O
	O
:	O
,jβw	O
:	O
,i	O
w	O
ˆhj	O
.	O
j	O
i	O
=	O
(	O
19.44	O
)	O
at	O
this	O
point	O
,	O
we	O
can	O
see	O
that	O
there	O
is	O
a	O
close	O
connection	O
between	O
recurrent	O
neural	O
networks	O
and	O
inference	O
in	O
graphical	O
models	O
.	O
speciﬁcally	O
,	O
the	O
mean	O
ﬁeld	O
ﬁxed	O
point	O
equations	O
deﬁned	O
a	O
recurrent	O
neural	O
network	O
.	O
the	O
task	O
of	O
this	O
network	O
is	O
to	O
perform	O
inference	O
.	O
we	O
have	O
described	O
how	O
to	O
derive	O
this	O
network	O
from	O
a	O
model	B
description	O
,	O
but	O
it	O
is	O
also	O
possible	O
to	O
train	O
the	O
inference	O
network	O
directly	O
.	O
several	O
ideas	O
based	O
on	O
this	O
theme	O
are	O
described	O
in	O
chapter	O
.20	O
in	O
the	O
case	O
of	O
binary	O
sparse	O
coding	O
,	O
we	O
can	O
see	O
that	O
the	O
recurrent	O
network	O
connection	O
speciﬁed	O
by	O
equation	O
consists	O
of	O
repeatedly	O
updating	O
the	O
hidden	O
19.44	O
units	O
based	O
on	O
the	O
changing	O
values	O
of	O
the	O
neighboring	O
hidden	O
units	O
.	O
the	O
input	O
	O
always	O
sends	O
a	O
ﬁxed	O
message	O
of	O
v	O
βw	O
to	O
the	O
hidden	O
units	O
,	O
but	O
the	O
hidden	O
units	O
constantly	O
update	O
the	O
message	O
they	O
send	O
to	O
each	O
other	O
.	O
speciﬁcally	O
,	O
two	O
units	O
ˆhi	O
and	O
ˆhj	O
inhibit	O
each	O
other	O
when	O
their	O
weight	O
vectors	O
are	O
aligned	O
.	O
this	O
is	O
a	O
form	O
of	O
competition—between	O
two	O
hidden	O
units	O
that	O
both	O
explain	O
the	O
input	O
,	O
only	O
the	O
one	O
that	O
explains	O
the	O
input	O
best	O
will	O
be	O
allowed	O
to	O
remain	O
active	O
.	O
this	O
competition	O
is	O
the	O
mean	O
ﬁeld	O
approximation	O
’	O
s	O
attempt	O
to	O
capture	O
the	O
explaining	O
away	O
interactions	O
in	O
the	O
binary	O
sparse	O
coding	O
posterior	O
.	O
the	O
explaining	O
away	O
eﬀect	O
actually	O
should	O
cause	O
a	O
multi-modal	O
posterior	O
,	O
so	O
that	O
if	O
we	O
draw	O
samples	O
from	O
the	O
posterior	O
,	O
some	O
samples	O
will	O
have	O
one	O
unit	O
active	O
,	O
other	O
samples	O
will	O
have	O
the	O
other	O
unit	O
active	O
,	O
but	O
very	O
few	O
samples	O
have	O
both	O
active	O
.	O
unfortunately	O
,	O
explaining	O
away	O
interactions	O
can	O
not	O
be	O
modeled	O
by	O
the	O
factorial	O
q	O
used	O
for	O
mean	O
ﬁeld	O
,	O
so	O
the	O
mean	O
ﬁeld	O
approximation	O
is	O
forced	O
to	O
choose	O
one	O
mode	O
to	O
model	B
.	O
this	O
is	O
an	O
instance	O
of	O
the	O
behavior	O
illustrated	O
in	O
ﬁgure	O
.3.6	O
we	O
can	O
rewrite	O
equation	O
19.44	O
into	O
an	O
equivalent	O
form	O
that	O
reveals	O
some	O
further	O
	O
	O
	O
insights	O
:	O
ˆhi	O
=	O
σ	O
bi	O
+	O
v	O
−	O
j	O
i	O
=	O
w	O
:	O
,j	O
ˆhj	O
βw	O
:	O
,i	O
−	O
1	O
2	O
	O
:	O
,i	O
βw	O
:	O
,i	O
w	O
.	O
	O
	O
(	O
19.45	O
)	O
−	O
ˆhj	O
in	O
this	O
reformulation	O
,	O
we	O
see	O
the	O
input	O
at	O
each	O
step	O
as	O
consisting	O
of	O
v	O
rather	O
than	O
v.	O
we	O
can	O
thus	O
think	O
of	O
unit	O
i	O
as	O
attempting	O
to	O
encode	O
the	O
residual	O
w	O
:	O
,j	O
i	O
=	O
j	O
	O
	O
	O
	O
chapter	O
19.	O
approximate	O
inference	O
error	O
in	O
v	O
given	O
the	O
code	O
of	O
the	O
other	O
units	O
.	O
we	O
can	O
thus	O
think	O
of	O
sparse	O
coding	O
as	O
an	O
iterative	O
autoencoder	O
,	O
that	O
repeatedly	O
encodes	O
and	O
decodes	O
its	O
input	O
,	O
attempting	O
to	O
ﬁx	O
mistakes	O
in	O
the	O
reconstruction	O
after	O
each	O
iteration	O
.	O
in	O
this	O
example	O
,	O
we	O
have	O
derived	O
an	O
update	O
rule	O
that	O
updates	O
a	O
single	O
unit	O
at	O
a	O
time	O
.	O
it	O
would	O
be	O
advantageous	O
to	O
be	O
able	O
to	O
update	O
more	O
units	O
simultaneously	O
.	O
some	O
graphical	O
models	O
,	O
such	O
as	O
deep	O
boltzmann	O
machines	O
,	O
are	O
structured	O
in	O
such	O
a	O
way	O
that	O
we	O
can	O
solve	O
for	O
many	O
entries	O
of	O
ˆh	O
simultaneously	O
.	O
unfortunately	O
,	O
binary	O
sparse	O
coding	O
does	O
not	O
admit	O
such	O
block	O
updates	O
.	O
instead	O
,	O
we	O
can	O
use	O
a	O
heuristic	O
technique	O
called	O
damping	O
to	O
perform	O
block	O
updates	O
.	O
in	O
the	O
damping	O
approach	O
,	O
we	O
solve	O
for	O
the	O
individually	O
optimal	O
values	O
of	O
every	O
element	O
of	O
ˆh	O
,	O
then	O
move	O
all	O
of	O
the	O
values	O
in	O
a	O
small	O
step	O
in	O
that	O
direction	O
.	O
this	O
approach	O
is	O
no	O
longer	O
guaranteed	O
at	O
each	O
step	O
,	O
but	O
works	O
well	O
in	O
practice	O
for	O
many	O
models	O
.	O
see	O
koller	O
to	O
increase	O
and	O
friedman	O
2009	O
(	O
)	O
for	O
more	O
information	O
about	O
choosing	O
the	O
degree	O
of	O
synchrony	O
and	O
damping	O
strategies	O
in	O
message	O
passing	O
algorithms	O
.	O
l	O
19.4.2	O
calculus	O
of	O
variations	O
before	O
continuing	O
with	O
our	O
presentation	O
of	O
variational	O
learning	O
,	O
we	O
must	O
brieﬂy	O
introduce	O
an	O
important	O
set	O
of	O
mathematical	O
tools	O
used	O
in	O
variational	O
learning	O
:	O
calculus	O
of	O
variations	O
.	O
∈	O
many	O
machine	O
learning	O
techniques	O
are	O
based	O
on	O
minimizing	O
a	O
function	O
j	O
(	O
θ	O
)	O
by	O
n	O
for	O
which	O
it	O
takes	O
on	O
its	O
minimal	O
value	O
.	O
this	O
can	O
ﬁnding	O
the	O
input	O
vector	O
θ	O
∇	O
be	O
accomplished	O
with	O
multivariate	O
calculus	O
and	O
linear	O
algebra	O
,	O
by	O
solving	O
for	O
the	O
θj	O
(	O
θ	O
)	O
=	O
0.	O
in	O
some	O
cases	O
,	O
we	O
actually	O
want	O
to	O
solve	O
for	O
a	O
critical	O
points	O
where	O
function	O
f	O
(	O
x	O
)	O
,	O
such	O
as	O
when	O
we	O
want	O
to	O
ﬁnd	O
the	O
probability	O
density	O
function	O
over	O
some	O
random	O
variable	O
.	O
this	O
is	O
what	O
calculus	O
of	O
variations	O
enables	O
us	O
to	O
do	O
.	O
r	O
a	O
function	O
of	O
a	O
function	O
f	O
is	O
known	O
as	O
a	O
functional	O
j	O
[	O
f	O
]	O
.	O
much	O
as	O
we	O
can	O
take	O
partial	O
derivatives	O
of	O
a	O
function	O
with	O
respect	O
to	O
elements	O
of	O
its	O
vector-	O
valued	O
argument	O
,	O
we	O
can	O
take	O
functional	O
derivatives	O
,	O
also	O
known	O
as	O
variational	O
derivatives	O
,	O
of	O
a	O
functional	O
j	O
[	O
f	O
]	O
with	O
respect	O
to	O
individual	O
values	O
of	O
the	O
function	O
f	O
(	O
x	O
)	O
at	O
any	O
speciﬁc	O
value	O
of	O
x.	O
the	O
functional	O
derivative	O
of	O
the	O
functional	O
j	O
with	O
respect	O
to	O
the	O
value	O
of	O
the	O
function	O
is	O
denoted	O
at	O
point	O
j	O
.	O
x	O
f	O
δ	O
δf	O
x	O
(	O
)	O
a	O
complete	O
formal	O
development	O
of	O
functional	O
derivatives	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
.	O
for	O
our	O
purposes	O
,	O
it	O
is	O
suﬃcient	O
to	O
state	O
that	O
for	O
diﬀerentiable	O
functions	O
f	O
(	O
x	O
)	O
with	O
continuous	O
derivatives	O
,	O
that	O
(	O
)	O
x	O
and	O
diﬀerentiable	O
functions	O
g	O
y	O
,	O
	O
δ	O
δf	O
(	O
)	O
x	O
g	O
f	O
(	O
(	O
)	O
x	O
x	O
)	O
x	O
=	O
d	O
,	O
∂	O
∂y	O
g	O
f	O
(	O
(	O
)	O
x	O
x	O
)	O
.	O
,	O
(	O
19.46	O
)	O
645	O
chapter	O
19.	O
approximate	O
inference	O
to	O
gain	O
some	O
intuition	O
for	O
this	O
identity	O
,	O
one	O
can	O
think	O
of	O
f	O
(	O
x	O
)	O
as	O
being	O
a	O
vector	O
with	O
uncountably	O
many	O
elements	O
,	O
indexed	O
by	O
a	O
real	O
vector	O
x.	O
in	O
this	O
(	O
somewhat	O
incomplete	O
view	O
)	O
,	O
the	O
identity	O
providing	O
the	O
functional	O
derivatives	O
is	O
the	O
same	O
as	O
we	O
would	O
obtain	O
for	O
a	O
vector	O
θ	O
n	O
indexed	O
by	O
positive	O
integers	O
:	O
	O
∈	O
r	O
∂	O
∂θi	O
j	O
g	O
θ	O
(	O
j	O
,	O
j	O
)	O
=	O
∂	O
∂θi	O
g	O
θ	O
(	O
i	O
,	O
i	O
.	O
)	O
(	O
19.47	O
)	O
many	O
results	O
in	O
other	O
machine	O
learning	O
publications	O
are	O
presented	O
using	O
the	O
more	O
general	O
euler-lagrange	O
equation	O
which	O
allows	O
g	O
to	O
depend	O
on	O
the	O
derivatives	O
of	O
f	O
as	O
well	O
as	O
the	O
value	O
of	O
f	O
,	O
but	O
we	O
do	O
not	O
need	O
this	O
fully	O
general	O
form	O
for	O
the	O
results	O
presented	O
in	O
this	O
book	O
.	O
to	O
optimize	O
a	O
function	O
with	O
respect	O
to	O
a	O
vector	O
,	O
we	O
take	O
the	O
gradient	O
of	O
the	O
function	O
with	O
respect	O
to	O
the	O
vector	O
and	O
solve	O
for	O
the	O
point	O
where	O
every	O
element	O
of	O
the	O
gradient	O
is	O
equal	O
to	O
zero	O
.	O
likewise	O
,	O
we	O
can	O
optimize	O
a	O
functional	O
by	O
solving	O
for	O
the	O
function	O
where	O
the	O
functional	O
derivative	O
at	O
every	O
point	O
is	O
equal	O
to	O
zero	O
.	O
as	O
an	O
example	O
of	O
how	O
this	O
process	O
works	O
,	O
consider	O
the	O
problem	O
of	O
ﬁnding	O
the	O
r	O
that	O
has	O
maximal	O
diﬀerential	O
entropy	O
.	O
probability	O
distribution	O
function	O
over	O
x	O
recall	O
that	O
the	O
entropy	O
of	O
a	O
probability	O
distribution	O
is	O
deﬁned	O
as	O
p	O
x	O
(	O
)	O
∈	O
(	O
19.48	O
)	O
(	O
19.49	O
)	O
−	O
h	O
p	O
[	O
]	O
=	O
ex	O
log	O
(	O
)	O
p	O
x	O
.	O
	O
for	O
continuous	O
values	O
,	O
the	O
expectation	O
is	O
an	O
integral	O
:	O
−	O
h	O
p	O
[	O
]	O
=	O
p	O
x	O
(	O
)	O
log	O
(	O
)	O
p	O
x	O
dx	O
.	O
we	O
can	O
not	O
simply	O
maximize	O
h	O
[	O
p	O
]	O
with	O
respect	O
to	O
the	O
function	O
p	O
(	O
x	O
)	O
,	O
because	O
the	O
result	O
might	O
not	O
be	O
a	O
probability	O
distribution	O
.	O
instead	O
,	O
we	O
need	O
to	O
use	O
lagrange	O
multipliers	O
to	O
add	O
a	O
constraint	O
that	O
p	O
(	O
x	O
)	O
integrates	O
to	O
1.	O
also	O
,	O
the	O
entropy	O
increases	O
without	O
bound	B
as	O
the	O
variance	O
increases	O
.	O
this	O
makes	O
the	O
question	O
of	O
which	O
distribution	O
has	O
the	O
greatest	O
entropy	O
uninteresting	O
.	O
instead	O
,	O
we	O
ask	O
which	O
distribution	O
has	O
maximal	O
entropy	O
for	O
ﬁxed	O
variance	O
σ2	O
.	O
finally	O
,	O
the	O
problem	O
is	O
underdetermined	O
because	O
the	O
distribution	O
can	O
be	O
shifted	O
arbitrarily	O
without	O
changing	O
the	O
entropy	O
.	O
to	O
impose	O
a	O
unique	O
solution	O
,	O
we	O
add	O
a	O
constraint	O
that	O
the	O
mean	O
of	O
the	O
distribution	O
be	O
µ.	O
the	O
lagrangian	O
functional	O
for	O
this	O
optimization	O
problem	O
is	O
l	O
[	O
]	O
=	O
p	O
−	O
2	O
]	O
+	O
[	O
]	O
h	O
p	O
(	O
19.50	O
)	O
	O
p	O
x	O
dx	O
	O
	O
	O
(	O
)	O
−	O
−	O
−	O
σ2	O
1	O
+	O
λ2	O
(	O
[	O
]	O
e	O
x	O
µ	O
)	O
+	O
λ3	O
x	O
e	O
[	O
(	O
)	O
µ	O
λ1	O
646	O
	O
	O
chapter	O
19.	O
approximate	O
inference	O
	O
=	O
λ1p	O
x	O
(	O
)	O
+	O
2	O
p	O
x	O
x	O
(	O
)	O
(	O
(	O
)	O
+	O
3p	O
x	O
x	O
λ	O
λ	O
−	O
µ	O
)	O
2	O
−	O
p	O
x	O
(	O
)	O
log	O
(	O
)	O
p	O
x	O
dx	O
−	O
−	O
λ	O
1	O
µλ2	O
−	O
σ2λ3	O
.	O
(	O
19.51	O
)	O
to	O
minimize	O
the	O
lagrangian	O
with	O
respect	O
to	O
p	O
,	O
we	O
set	O
the	O
functional	O
derivatives	O
equal	O
to	O
0	O
:	O
∀	O
x	O
,	O
l	O
δ	O
δp	O
x	O
(	O
)	O
=	O
λ1	O
+	O
λ2	O
x	O
−	O
−	O
−	O
1	O
2	O
)	O
µ	O
x	O
λ+	O
3	O
(	O
log	O
(	O
)	O
=	O
0	O
.	O
p	O
x	O
(	O
19.52	O
)	O
	O
	O
this	O
condition	O
now	O
tells	O
us	O
the	O
functional	O
form	O
of	O
p	O
(	O
x	O
)	O
.	O
by	O
algebraically	O
re-arranging	O
the	O
equation	O
,	O
we	O
obtain	O
p	O
x	O
(	O
)	O
=	O
exp	O
λ1	O
+	O
λ2x	O
λ+	O
3	O
(	O
x	O
−	O
2	O
−	O
)	O
µ	O
1	O
.	O
(	O
19.53	O
)	O
we	O
never	O
assumed	O
directly	O
that	O
p	O
(	O
x	O
)	O
would	O
take	O
this	O
functional	O
form	O
;	O
we	O
obtained	O
the	O
expression	O
itself	O
by	O
analytically	O
minimizing	O
a	O
functional	O
.	O
to	O
ﬁnish	O
the	O
minimization	O
problem	O
,	O
we	O
must	O
choose	O
the	O
λ	O
values	O
to	O
ensure	O
that	O
all	O
of	O
our	O
constraints	O
are	O
satisﬁed	O
.	O
we	O
are	O
free	O
to	O
choose	O
any	O
λ	O
values	O
,	O
because	O
the	O
gradient	O
√	O
of	O
the	O
lagrangian	O
with	O
respect	O
to	O
the	O
λ	O
variables	O
is	O
zero	O
so	O
long	O
as	O
the	O
constraints	O
−	O
2π	O
,	O
are	O
satisﬁed	O
.	O
to	O
satisfy	O
all	O
of	O
the	O
constraints	O
,	O
we	O
may	O
set	O
λ1	O
=	O
1	O
λ2	O
=	O
0	O
,	O
and	O
λ3	O
=	O
to	O
obtain	O
log	O
σ	O
−	O
1	O
2σ2	O
n	O
(	O
)	O
=	O
p	O
x	O
(	O
x	O
µ	O
,	O
σ	O
;	O
2	O
)	O
.	O
(	O
19.54	O
)	O
this	O
is	O
one	O
reason	O
for	O
using	O
the	O
normal	O
distribution	O
when	O
we	O
do	O
not	O
know	O
the	O
true	O
distribution	O
.	O
because	O
the	O
normal	O
distribution	O
has	O
the	O
maximum	O
entropy	O
,	O
we	O
impose	O
the	O
least	O
possible	O
amount	O
of	O
structure	O
by	O
making	O
this	O
assumption	O
.	O
while	O
examining	O
the	O
critical	O
points	O
of	O
the	O
lagrangian	O
functional	O
for	O
the	O
entropy	O
,	O
we	O
found	O
only	O
one	O
critical	O
point	O
,	O
corresponding	O
to	O
maximizing	O
the	O
entropy	O
for	O
ﬁxed	O
variance	O
.	O
what	O
about	O
the	O
probability	O
distribution	O
function	O
that	O
minimizes	O
the	O
entropy	O
?	O
why	O
did	O
we	O
not	O
ﬁnd	O
a	O
second	O
critical	O
point	O
corresponding	O
to	O
the	O
minimum	O
?	O
the	O
reason	O
is	O
that	O
there	O
is	O
no	O
speciﬁc	O
function	O
that	O
achieves	O
minimal	O
−	O
entropy	O
.	O
as	O
functions	O
place	O
more	O
probability	O
density	O
on	O
the	O
two	O
points	O
x	O
=	O
µ	O
+	O
σ	O
and	O
x	O
=	O
µ	O
σ	O
,	O
and	O
place	O
less	O
probability	O
density	O
on	O
all	O
other	O
values	O
of	O
x	O
,	O
they	O
lose	O
entropy	O
while	O
maintaining	O
the	O
desired	O
variance	O
.	O
however	O
,	O
any	O
function	O
placing	O
exactly	O
zero	O
mass	O
on	O
all	O
but	O
two	O
points	O
does	O
not	O
integrate	O
to	O
one	O
,	O
and	O
is	O
not	O
a	O
valid	O
probability	O
distribution	O
.	O
there	O
thus	O
is	O
no	O
single	O
minimal	O
entropy	O
probability	O
distribution	O
function	O
,	O
much	O
as	O
there	O
is	O
no	O
single	O
minimal	O
positive	O
real	O
number	O
.	O
instead	O
,	O
we	O
can	O
say	O
that	O
there	O
is	O
a	O
sequence	O
of	O
probability	O
distributions	O
converging	O
toward	O
putting	O
mass	O
only	O
on	O
these	O
two	O
points	O
.	O
this	O
degenerate	O
scenario	O
may	O
be	O
647	O
chapter	O
19.	O
approximate	O
inference	O
described	O
as	O
a	O
mixture	O
of	O
dirac	O
distributions	O
.	O
because	O
dirac	O
distributions	O
are	O
not	O
described	O
by	O
a	O
single	O
probability	O
distribution	O
function	O
,	O
no	O
dirac	O
or	O
mixture	O
of	O
dirac	O
distribution	O
corresponds	O
to	O
a	O
single	O
speciﬁc	O
point	O
in	O
function	O
space	O
.	O
these	O
distributions	O
are	O
thus	O
invisible	O
to	O
our	O
method	O
of	O
solving	O
for	O
a	O
speciﬁc	O
point	O
where	O
the	O
functional	O
derivatives	O
are	O
zero	O
.	O
this	O
is	O
a	O
limitation	O
of	O
the	O
method	O
.	O
distributions	O
such	O
as	O
the	O
dirac	O
must	O
be	O
found	O
by	O
other	O
methods	O
,	O
such	O
as	O
guessing	O
the	O
solution	O
and	O
then	O
proving	O
that	O
it	O
is	O
correct	O
.	O
19.4.3	O
continuous	O
latent	O
variables	O
when	O
our	O
graphical	O
model	B
contains	O
continuous	O
latent	O
variables	O
,	O
we	O
may	O
still	O
perform	O
variational	O
inference	O
and	O
learning	O
by	O
maximizing	O
.	O
however	O
,	O
we	O
must	O
now	O
use	O
calculus	O
of	O
variations	O
when	O
maximizing	O
with	O
respect	O
to	O
|	O
h	O
v	O
l	O
q	O
(	O
.	O
)	O
in	O
most	O
cases	O
,	O
practitioners	O
need	O
not	O
solve	O
any	O
calculus	O
of	O
variations	O
problems	O
themselves	O
.	O
instead	O
,	O
there	O
is	O
a	O
general	O
equation	O
for	O
the	O
mean	O
ﬁeld	O
ﬁxed	O
point	O
updates	O
.	O
if	O
we	O
make	O
the	O
mean	O
ﬁeld	O
approximation	O
|	O
	O
(	O
19.55	O
)	O
|	O
h	O
v	O
q	O
(	O
)	O
=	O
	O
q	O
h	O
(	O
i	O
v	O
)	O
,	O
i	O
|	O
|	O
l	O
	O
v	O
)	O
for	O
all	O
j	O
and	O
ﬁx	O
q	O
(	O
hj	O
normalizing	O
the	O
unnormalized	O
distribution	O
∼	O
q	O
(	O
h−	O
v	O
)	O
=	O
exp	O
˜q	O
h	O
(	O
i	O
eh−	O
|	O
i	O
i	O
=	O
i	O
,	O
then	O
the	O
optimal	O
q	O
(	O
h	O
i	O
|	O
v	O
)	O
log	O
˜p	O
,	O
(	O
v	O
h	O
)	O
(	O
19.56	O
)	O
v	O
)	O
may	O
be	O
obtained	O
by	O
|	O
0	O
so	O
long	O
as	O
p	O
does	O
not	O
assign	O
probability	O
to	O
any	O
joint	O
conﬁguration	O
of	O
variables	O
.	O
carrying	O
out	O
the	O
expectation	O
inside	O
the	O
equation	O
will	O
yield	O
the	O
correct	O
functional	O
form	O
of	O
q	O
(	O
hi	O
v	O
)	O
.	O
it	O
is	O
only	O
necessary	O
to	O
derive	O
functional	O
forms	O
of	O
q	O
directly	O
using	O
calculus	O
of	O
variations	O
if	O
one	O
wishes	O
to	O
develop	O
a	O
new	O
form	O
of	O
variational	O
learning	O
;	O
equation	O
yields	O
the	O
mean	O
ﬁeld	O
approximation	O
for	O
any	O
probabilistic	O
model	B
.	O
19.56	O
19.56	O
equation	O
is	O
a	O
ﬁxed	O
point	O
equation	O
,	O
designed	O
to	O
be	O
iteratively	O
applied	O
for	O
each	O
value	O
of	O
i	O
repeatedly	O
until	O
convergence	O
.	O
however	O
,	O
it	O
also	O
tells	O
us	O
more	O
than	O
that	O
.	O
it	O
tells	O
us	O
the	O
functional	O
form	O
that	O
the	O
optimal	O
solution	O
will	O
take	O
,	O
whether	O
we	O
arrive	O
there	O
by	O
ﬁxed	O
point	O
equations	O
or	O
not	O
.	O
this	O
means	O
we	O
can	O
take	O
the	O
functional	O
form	O
from	O
that	O
equation	O
but	O
regard	O
some	O
of	O
the	O
values	O
that	O
appear	O
in	O
it	O
as	O
parameters	O
,	O
that	O
we	O
can	O
optimize	O
with	O
any	O
optimization	O
algorithm	O
we	O
like	O
.	O
∈	O
as	O
an	O
example	O
,	O
consider	O
a	O
very	O
simple	O
probabilistic	O
model	B
,	O
with	O
latent	O
variables	O
|	O
(	O
h	O
;	O
0	O
,	O
i	O
)	O
and	O
h	O
h	O
;	O
1	O
)	O
.	O
we	O
could	O
actually	O
simplify	O
this	O
model	B
by	O
integrating	O
p	O
(	O
v	O
out	O
h	O
;	O
the	O
result	O
is	O
just	O
a	O
gaussian	O
distribution	O
over	O
v.	O
the	O
model	B
itself	O
is	O
not	O
2	O
and	O
just	O
one	O
visible	O
variable	O
,	O
v.	O
suppose	O
that	O
p	O
(	O
h	O
)	O
=	O
r	O
h	O
)	O
=	O
(	O
v	O
;	O
w	O
n	O
n	O
	O
648	O
	O
chapter	O
19.	O
approximate	O
inference	O
interesting	O
;	O
we	O
have	O
constructed	O
it	O
only	O
to	O
provide	O
a	O
simple	O
demonstration	O
of	O
how	O
calculus	O
of	O
variations	O
may	O
be	O
applied	O
to	O
probabilistic	O
modeling	O
.	O
the	O
true	O
posterior	O
is	O
given	O
,	O
up	O
to	O
a	O
normalizing	O
constant	O
,	O
by	O
|	O
p	O
(	O
h	O
v	O
∝	O
p	O
,	O
(	O
h	O
v	O
)	O
)	O
	O
	O
	O
	O
|	O
=	O
(	O
p	O
h1	O
)	O
(	O
p	O
h2	O
)	O
(	O
∝	O
p	O
v	O
h	O
)	O
exp	O
h2	O
1	O
+	O
h2	O
2	O
+	O
(	O
v	O
−	O
−	O
h	O
1w1	O
	O
=	O
exp	O
h2	O
1	O
+	O
h2	O
2	O
+	O
v	O
2	O
+	O
h2	O
1w2	O
1	O
+	O
h2	O
2	O
w2	O
2	O
−	O
1	O
2	O
−	O
1	O
2	O
(	O
19.57	O
)	O
(	O
19.58	O
)	O
(	O
19.59	O
)	O
	O
(	O
19.60	O
)	O
−	O
2vh1w1	O
2vh	O
2w2	O
+	O
2h1w1h2w	O
2	O
.	O
(	O
19.61	O
)	O
h2w2	O
)	O
2	O
−	O
	O
	O
due	O
to	O
the	O
presence	O
of	O
the	O
terms	O
multiplying	O
h1	O
and	O
h2	O
together	O
,	O
we	O
can	O
see	O
that	O
the	O
true	O
posterior	O
does	O
not	O
factorize	O
over	O
h1	O
and	O
h2	O
.	O
applying	O
equation	O
|	O
19.56	O
,	O
we	O
ﬁnd	O
that	O
	O
	O
∼	O
|	O
v	O
)	O
log	O
˜p	O
,	O
(	O
v	O
h	O
)	O
q	O
(	O
h2	O
eh2	O
−	O
1	O
∼	O
q	O
(	O
h2	O
|	O
v	O
)	O
v	O
)	O
˜q	O
h	O
(	O
1	O
=	O
exp	O
=	O
exp	O
−	O
2vh1w1	O
2eh2	O
−	O
2vh	O
2w2	O
+	O
2h1w1h2w2	O
]	O
.	O
	O
h2	O
1	O
+	O
h2	O
2	O
+	O
v2	O
+	O
h2	O
1w2	O
1	O
+	O
h2	O
2w2	O
2	O
(	O
19.62	O
)	O
(	O
19.63	O
)	O
(	O
19.64	O
)	O
(	O
19.65	O
)	O
	O
	O
|	O
	O
from	O
this	O
,	O
we	O
can	O
see	O
that	O
there	O
are	O
eﬀectively	O
only	O
two	O
values	O
we	O
need	O
to	O
obtain	O
from	O
q	O
(	O
h	O
2	O
,	O
we	O
obtain	O
∼	O
|	O
q	O
(	O
h	O
v	O
)	O
[	O
h2	O
]	O
and	O
eh2	O
2	O
]	O
.	O
writing	O
these	O
as	O
∼	O
|	O
q	O
(	O
h	O
v	O
)	O
[	O
h2	O
	O
	O
h2	O
v	O
)	O
:	O
eh	O
2	O
	O
h2	O
2	O
and	O
|	O
˜q	O
h	O
(	O
1	O
−	O
1	O
2	O
v	O
)	O
=	O
exp	O
−	O
2vh1w1	O
	O
	O
h2	O
h	O
2	O
1	O
+	O
2	O
−	O
	O
2v	O
h2	O
1w2	O
+	O
v2	O
+	O
h	O
2	O
	O
w	O
2	O
+	O
2h	O
1w1	O
1	O
+	O
	O
h2	O
(	O
19.66	O
)	O
.	O
(	O
19.67	O
)	O
	O
	O
	O
h2	O
w2	O
2	O
2	O
	O
w2	O
]	O
|	O
n	O
from	O
this	O
,	O
we	O
can	O
see	O
that	O
˜q	O
has	O
the	O
functional	O
form	O
of	O
a	O
gaussian	O
.	O
we	O
can	O
−	O
1	O
)	O
where	O
µ	O
and	O
diagonal	O
β	O
are	O
variational	O
thus	O
conclude	O
q	O
(	O
h	O
v	O
parameters	O
that	O
we	O
can	O
optimize	O
using	O
any	O
technique	O
we	O
choose	O
.	O
it	O
is	O
important	O
to	O
recall	O
that	O
we	O
did	O
not	O
ever	O
assume	O
that	O
q	O
would	O
be	O
gaussian	O
;	O
its	O
gaussian	O
form	O
was	O
derived	O
automatically	O
by	O
using	O
calculus	O
of	O
variations	O
to	O
maximize	O
q	O
with	O
(	O
h	O
;	O
µ	O
β	O
,	O
)	O
=	O
649	O
chapter	O
19.	O
approximate	O
inference	O
l	O
respect	O
to	O
functional	O
form	O
of	O
.q	O
.	O
using	O
the	O
same	O
approach	O
on	O
a	O
diﬀerent	O
model	B
could	O
yield	O
a	O
diﬀerent	O
this	O
was	O
of	O
course	O
,	O
just	O
a	O
small	O
case	O
constructed	O
for	O
demonstration	O
purposes	O
.	O
for	O
examples	O
of	O
real	O
applications	O
of	O
variational	O
learning	O
with	O
continuous	O
variables	O
in	O
the	O
context	O
of	O
deep	O
learning	O
,	O
see	O
goodfellow	O
et	O
al	O
.	O
2013d	O
)	O
.	O
(	O
19.4.4	O
interactions	O
between	O
learning	O
and	O
inference	O
using	O
approximate	O
inference	O
as	O
part	O
of	O
a	O
learning	O
algorithm	O
aﬀects	O
the	O
learning	O
process	O
,	O
and	O
this	O
in	O
turn	O
aﬀects	O
the	O
accuracy	O
of	O
the	O
inference	O
algorithm	O
.	O
speciﬁcally	O
,	O
the	O
training	O
algorithm	O
tends	O
to	O
adapt	O
the	O
model	B
in	O
a	O
way	O
that	O
makes	O
the	O
approximating	O
assumptions	O
underlying	O
the	O
approximate	O
inference	O
algorithm	O
become	O
more	O
true	O
.	O
when	O
training	O
the	O
parameters	O
,	O
variational	O
learning	O
increases	O
∼	O
q	O
log	O
(	O
eh	O
|	O
|	O
|	O
for	O
a	O
speciﬁc	O
v	O
,	O
this	O
increases	O
p	O
(	O
h	O
v	O
|	O
under	O
q	O
(	O
h	O
v	O
)	O
and	O
decreases	O
p	O
(	O
h	O
v	O
under	O
.	O
h	O
v	O
)	O
q	O
(	O
)	O
p	O
v	O
h	O
,	O
.	O
(	O
19.68	O
)	O
)	O
for	O
values	O
of	O
h	O
that	O
have	O
high	O
probability	O
)	O
for	O
values	O
of	O
h	O
that	O
have	O
low	O
probability	O
this	O
behavior	O
causes	O
our	O
approximating	O
assumptions	O
to	O
become	O
self-fulﬁlling	O
prophecies	O
.	O
if	O
we	O
train	O
the	O
model	B
with	O
a	O
unimodal	O
approximate	O
posterior	O
,	O
we	O
will	O
obtain	O
a	O
model	B
with	O
a	O
true	O
posterior	O
that	O
is	O
far	O
closer	O
to	O
unimodal	O
than	O
we	O
would	O
have	O
obtained	O
by	O
training	O
the	O
model	B
with	O
exact	O
inference	O
.	O
l	O
(	O
v	O
θ	O
,	O
computing	O
the	O
true	O
amount	O
of	O
harm	O
imposed	O
on	O
a	O
model	B
by	O
a	O
variational	O
approximation	O
is	O
thus	O
very	O
diﬃcult	O
.	O
there	O
exist	O
several	O
methods	O
for	O
estimating	O
log	O
p	O
(	O
v	O
)	O
.	O
we	O
often	O
estimate	O
log	O
p	O
(	O
v	O
;	O
θ	O
)	O
after	O
training	O
the	O
model	B
,	O
and	O
ﬁnd	O
that	O
,	O
q	O
)	O
is	O
small	O
.	O
from	O
this	O
,	O
we	O
can	O
conclude	O
that	O
our	O
variational	O
the	O
gap	O
with	O
approximation	O
is	O
accurate	O
for	O
the	O
speciﬁc	O
value	O
of	O
θ	O
that	O
we	O
obtained	O
from	O
the	O
learning	O
process	O
.	O
we	O
should	O
not	O
conclude	O
that	O
our	O
variational	O
approximation	O
is	O
accurate	O
in	O
general	O
or	O
that	O
the	O
variational	O
approximation	O
did	O
little	O
harm	O
to	O
the	O
learning	O
process	O
.	O
to	O
measure	O
the	O
true	O
amount	O
of	O
harm	O
induced	O
by	O
the	O
variational	O
	O
l	O
≈	O
=	O
maxθ	O
log	O
p	O
(	O
v	O
;	O
θ	O
)	O
.	O
it	O
is	O
possible	O
for	O
approximation	O
,	O
we	O
would	O
need	O
to	O
know	O
θ	O
∗	O
l	O
log	O
p	O
(	O
v	O
;	O
θ	O
,	O
q	O
)	O
(	O
v	O
θ	O
,	O
)	O
to	O
hold	O
simultaneously	O
.	O
if	O
∗	O
∗	O
maxq	O
)	O
,	O
because	O
θ	O
(	O
v	O
θ	O
,	O
induces	O
too	O
complicated	O
of	O
a	O
posterior	O
distribution	O
for	O
our	O
q	O
family	O
to	O
capture	O
,	O
then	O
the	O
learning	O
process	O
will	O
never	O
∗	O
approach	O
θ	O
.	O
such	O
a	O
problem	O
is	O
very	O
diﬃcult	O
to	O
detect	O
,	O
because	O
we	O
can	O
only	O
know	O
∗	O
for	O
sure	O
that	O
it	O
happened	O
if	O
we	O
have	O
a	O
superior	O
learning	O
algorithm	O
that	O
can	O
ﬁnd	O
θ	O
for	O
comparison	O
.	O
log	O
p	O
(	O
v	O
;	O
θ	O
)	O
and	O
log	O
p	O
(	O
v	O
;	O
θ	O
)	O
,	O
q	O
)	O
log	O
p	O
(	O
v	O
;	O
θ	O
	O
∗	O
∗	O
650	O
chapter	O
19.	O
approximate	O
inference	O
19.5	O
learned	O
approximate	O
inference	O
l	O
we	O
have	O
seen	O
that	O
inference	O
can	O
be	O
thought	O
of	O
as	O
an	O
optimization	O
procedure	O
that	O
increases	O
the	O
value	O
of	O
a	O
function	O
.	O
explicitly	O
performing	O
optimization	O
via	O
iterative	O
procedures	O
such	O
as	O
ﬁxed	O
point	O
equations	O
or	O
gradient-based	O
optimization	O
is	O
often	O
very	O
expensive	O
and	O
time-consuming	O
.	O
many	O
approaches	O
to	O
inference	O
avoid	O
this	O
expense	O
by	O
learning	O
to	O
perform	O
approximate	O
inference	O
.	O
speciﬁcally	O
,	O
we	O
can	O
l	O
think	O
of	O
the	O
optimization	O
process	O
as	O
a	O
function	O
f	O
that	O
maps	O
an	O
input	O
v	O
to	O
an	O
approximate	O
distribution	O
q	O
(	O
v	O
,	O
q	O
)	O
.	O
once	O
we	O
think	O
of	O
the	O
multi-step	O
iterative	O
optimization	O
process	O
as	O
just	O
being	O
a	O
function	O
,	O
we	O
can	O
approximate	O
it	O
with	O
a	O
neural	O
network	O
that	O
implements	O
an	O
approximation	O
ˆf	O
(	O
;	O
)	O
v	O
θ	O
.	O
=	O
arg	O
maxq	O
∗	O
19.5.1	O
wake-sleep	O
et	O
al.	O
,	O
1995b	O
frey	O
one	O
of	O
the	O
main	O
diﬃculties	O
with	O
training	O
a	O
model	B
to	O
infer	O
h	O
from	O
v	O
is	O
that	O
we	O
do	O
not	O
have	O
a	O
supervised	O
training	O
set	O
with	O
which	O
to	O
train	O
the	O
model	B
.	O
given	O
a	O
v	O
,	O
we	O
do	O
not	O
know	O
the	O
appropriate	O
h.	O
the	O
mapping	O
from	O
v	O
to	O
h	O
depends	O
on	O
the	O
choice	O
of	O
model	B
family	O
,	O
and	O
evolves	O
throughout	O
the	O
learning	O
process	O
as	O
θ	O
changes	O
.	O
the	O
wake-sleep	O
algorithm	O
(	O
hinton	O
)	O
resolves	O
this	O
problem	O
by	O
drawing	O
samples	O
of	O
both	O
h	O
and	O
v	O
from	O
the	O
model	B
distribution	O
.	O
for	O
example	O
,	O
in	O
a	O
directed	O
model	B
,	O
this	O
can	O
be	O
done	O
cheaply	O
by	O
performing	O
ancestral	O
sampling	O
beginning	O
at	O
h	O
and	O
ending	O
at	O
v.	O
the	O
inference	O
network	O
can	O
then	O
be	O
trained	O
to	O
perform	O
the	O
reverse	O
mapping	O
:	O
predicting	O
which	O
h	O
caused	O
the	O
present	O
v.	O
the	O
main	O
drawback	O
to	O
this	O
approach	O
is	O
that	O
we	O
will	O
only	O
be	O
able	O
to	O
train	O
the	O
inference	O
network	O
on	O
values	O
of	O
v	O
that	O
have	O
high	O
probability	O
under	O
the	O
model	B
.	O
early	O
in	O
learning	O
,	O
the	O
model	B
distribution	O
will	O
not	O
resemble	O
the	O
data	O
distribution	O
,	O
so	O
the	O
inference	O
network	O
will	O
not	O
have	O
an	O
opportunity	O
to	O
learn	O
on	O
samples	O
that	O
resemble	O
data	O
.	O
et	O
al.	O
,	O
;	O
1996	O
18.2	O
in	O
section	O
we	O
saw	O
that	O
one	O
possible	O
explanation	O
for	O
the	O
role	O
of	O
dream	O
sleep	O
in	O
human	O
beings	O
and	O
animals	O
is	O
that	O
dreams	O
could	O
provide	O
the	O
negative	O
phase	O
samples	O
that	O
monte	O
carlo	O
training	O
algorithms	O
use	O
to	O
approximate	O
the	O
negative	O
gradient	O
of	O
the	O
log	O
partition	O
function	O
of	O
undirected	O
models	O
.	O
another	O
possible	O
explanation	O
for	O
biological	O
dreaming	O
is	O
that	O
it	O
is	O
providing	O
samples	O
from	O
p	O
(	O
h	O
v	O
,	O
)	O
which	O
can	O
be	O
used	O
to	O
train	O
an	O
inference	O
network	O
to	O
predict	O
h	O
given	O
v	O
.	O
in	O
some	O
senses	O
,	O
this	O
explanation	O
is	O
more	O
satisfying	O
than	O
the	O
partition	O
function	O
explanation	O
.	O
monte	O
carlo	O
algorithms	O
generally	O
do	O
not	O
perform	O
well	O
if	O
they	O
are	O
run	O
using	O
only	O
the	O
positive	O
phase	O
of	O
the	O
gradient	O
for	O
several	O
steps	O
then	O
with	O
only	O
the	O
negative	O
phase	O
of	O
the	O
gradient	O
for	O
several	O
steps	O
.	O
human	O
beings	O
and	O
animals	O
are	O
usually	O
awake	O
for	O
several	O
consecutive	O
hours	O
then	O
asleep	O
for	O
several	O
consecutive	O
hours	O
.	O
it	O
is	O
651	O
chapter	O
19.	O
approximate	O
inference	O
l	O
not	O
readily	O
apparent	O
how	O
this	O
schedule	O
could	O
support	O
monte	O
carlo	O
training	O
of	O
an	O
undirected	O
model	B
.	O
learning	O
algorithms	O
based	O
on	O
maximizing	O
can	O
be	O
run	O
with	O
prolonged	O
periods	O
of	O
improving	O
q	O
and	O
prolonged	O
periods	O
of	O
improving	O
θ	O
,	O
however	O
.	O
if	O
the	O
role	O
of	O
biological	O
dreaming	O
is	O
to	O
train	O
networks	O
for	O
predicting	O
q	O
,	O
then	O
this	O
explains	O
how	O
animals	O
are	O
able	O
to	O
remain	O
awake	O
for	O
several	O
hours	O
(	O
the	O
longer	O
they	O
are	O
awake	O
,	O
the	O
greater	O
the	O
gap	O
between	O
will	O
remain	O
a	O
lower	O
bound	B
)	O
and	O
to	O
remain	O
asleep	O
for	O
several	O
hours	O
(	O
the	O
generative	O
model	B
itself	O
is	O
not	O
modiﬁed	O
during	O
sleep	O
)	O
without	O
damaging	O
their	O
internal	O
models	O
.	O
of	O
course	O
,	O
these	O
ideas	O
are	O
purely	O
speculative	O
,	O
and	O
there	O
is	O
no	O
hard	O
evidence	O
to	O
suggest	O
that	O
dreaming	O
accomplishes	O
either	O
of	O
these	O
goals	O
.	O
dreaming	O
may	O
also	O
serve	O
reinforcement	O
learning	O
rather	O
than	O
probabilistic	O
modeling	O
,	O
by	O
sampling	O
synthetic	O
experiences	O
from	O
the	O
animal	O
’	O
s	O
transition	O
model	B
,	O
on	O
which	O
to	O
train	O
the	O
animal	O
’	O
s	O
policy	O
.	O
or	O
sleep	O
may	O
serve	O
some	O
other	O
purpose	O
not	O
yet	O
anticipated	O
by	O
the	O
machine	O
learning	O
community	O
.	O
and	O
log	O
p	O
(	O
v	O
)	O
,	O
but	O
l	O
l	O
19.5.2	O
other	O
forms	O
of	O
learned	O
inference	O
this	O
strategy	O
of	O
learned	O
approximate	O
inference	O
has	O
also	O
been	O
applied	O
to	O
other	O
models	O
.	O
salakhutdinov	O
and	O
larochelle	O
2010	O
)	O
showed	O
that	O
a	O
single	O
pass	O
in	O
a	O
learned	O
inference	O
network	O
could	O
yield	O
faster	O
inference	O
than	O
iterating	O
the	O
mean	O
ﬁeld	O
ﬁxed	O
point	O
equations	O
in	O
a	O
dbm	O
.	O
the	O
training	O
procedure	O
is	O
based	O
on	O
running	O
the	O
inference	O
network	O
,	O
then	O
applying	O
one	O
step	O
of	O
mean	O
ﬁeld	O
to	O
improve	O
its	O
estimates	O
,	O
and	O
training	O
the	O
inference	O
network	O
to	O
output	O
this	O
reﬁned	O
estimate	O
instead	O
of	O
its	O
original	O
estimate	O
.	O
(	O
14.8	O
we	O
have	O
already	O
seen	O
in	O
section	O
that	O
the	O
predictive	O
sparse	O
decomposition	O
model	B
trains	O
a	O
shallow	O
encoder	O
network	O
to	O
predict	O
a	O
sparse	O
code	O
for	O
the	O
input	O
.	O
this	O
can	O
be	O
seen	O
as	O
a	O
hybrid	O
between	O
an	O
autoencoder	O
and	O
sparse	O
coding	O
.	O
it	O
is	O
possible	O
to	O
devise	O
probabilistic	O
semantics	O
for	O
the	O
model	B
,	O
under	O
which	O
the	O
encoder	O
may	O
be	O
viewed	O
as	O
performing	O
learned	O
approximate	O
map	O
inference	O
.	O
due	O
to	O
its	O
shallow	O
encoder	O
,	O
psd	O
is	O
not	O
able	O
to	O
implement	O
the	O
kind	O
of	O
competition	O
between	O
units	O
that	O
we	O
have	O
seen	O
in	O
mean	O
ﬁeld	O
inference	O
.	O
however	O
,	O
that	O
problem	O
can	O
be	O
remedied	O
by	O
training	O
a	O
deep	O
encoder	O
to	O
perform	O
learned	O
approximate	O
inference	O
,	O
as	O
in	O
the	O
ista	O
technique	O
(	O
gregor	O
and	O
lecun	O
2010b	O
)	O
.	O
,	O
,	O
learned	O
approximate	O
inference	O
has	O
recently	O
become	O
one	O
of	O
the	O
dominant	O
approaches	O
to	O
generative	O
modeling	O
,	O
in	O
the	O
form	O
of	O
the	O
variational	O
autoencoder	O
(	O
kingma	O
2013	O
rezende	O
et	O
al	O
.	O
2014	O
)	O
.	O
in	O
this	O
elegant	O
approach	O
,	O
there	O
is	O
no	O
need	O
to	O
construct	O
explicit	O
targets	O
for	O
the	O
inference	O
network	O
.	O
instead	O
,	O
the	O
inference	O
network	O
l	O
,	O
and	O
then	O
the	O
parameters	O
of	O
the	O
inference	O
network	O
are	O
is	O
simply	O
used	O
to	O
deﬁne	O
adapted	O
to	O
increase	O
20.10.3	O
.	O
.	O
this	O
model	B
is	O
described	O
in	O
depth	O
later	O
,	O
in	O
section	O
l	O
;	O
,	O
652	O
chapter	O
19.	O
approximate	O
inference	O
using	O
approximate	O
inference	O
,	O
it	O
is	O
possible	O
to	O
train	O
and	O
use	O
a	O
wide	O
variety	O
of	O
models	O
.	O
many	O
of	O
these	O
models	O
are	O
described	O
in	O
the	O
next	O
chapter	O
.	O
653	O
chapter	O
20	O
deep	O
generative	O
models	O
in	O
this	O
chapter	O
,	O
we	O
present	O
several	O
of	O
the	O
speciﬁc	O
kinds	O
of	O
generative	O
models	O
that	O
can	O
be	O
built	O
and	O
trained	O
using	O
the	O
techniques	O
presented	O
in	O
chapters	O
–	O
.	O
all	O
of	O
these	O
models	O
represent	O
probability	O
distributions	O
over	O
multiple	O
variables	O
in	O
some	O
way	O
.	O
some	O
allow	O
the	O
probability	O
distribution	O
function	O
to	O
be	O
evaluated	O
explicitly	O
.	O
others	O
do	O
not	O
allow	O
the	O
evaluation	O
of	O
the	O
probability	O
distribution	O
function	O
,	O
but	O
support	O
operations	O
that	O
implicitly	O
require	O
knowledge	O
of	O
it	O
,	O
such	O
as	O
drawing	O
samples	O
from	O
the	O
distribution	O
.	O
some	O
of	O
these	O
models	O
are	O
structured	O
probabilistic	O
models	O
described	O
in	O
terms	O
of	O
graphs	O
and	O
factors	O
,	O
using	O
the	O
language	O
of	O
graphical	O
models	O
presented	O
in	O
chapter	O
.	O
others	O
can	O
not	O
easily	O
be	O
described	O
in	O
terms	O
of	O
factors	O
,	O
but	O
represent	O
probability	O
distributions	O
nonetheless	O
.	O
16	O
19	O
16	O
20.1	O
boltzmann	O
machines	O
;	O
1983	O
ackley	O
;	O
et	O
al.	O
,	O
1985	O
hinton	O
boltzmann	O
machines	O
were	O
originally	O
introduced	O
as	O
a	O
general	O
“	O
connectionist	O
”	O
ap-	O
proach	O
to	O
learning	O
arbitrary	O
probability	O
distributions	O
over	O
binary	O
vectors	O
(	O
fahlman	O
et	O
al.	O
,	O
)	O
.	O
1984	O
hinton	O
and	O
sejnowski	O
1986	O
variants	O
of	O
the	O
boltzmann	O
machine	O
that	O
include	O
other	O
kinds	O
of	O
variables	O
have	O
long	O
ago	O
surpassed	O
the	O
popularity	O
of	O
the	O
original	O
.	O
in	O
this	O
section	O
we	O
brieﬂy	O
introduce	O
the	O
binary	O
boltzmann	O
machine	O
and	O
discuss	O
the	O
issues	O
that	O
come	O
up	O
when	O
trying	O
to	O
train	O
and	O
perform	O
inference	O
in	O
the	O
model	B
.	O
∈	O
{	O
we	O
deﬁne	O
the	O
boltzmann	O
machine	O
over	O
a	O
d-dimensional	O
binary	O
random	O
vector	O
)	O
,	O
d.	O
the	O
boltzmann	O
machine	O
is	O
an	O
energy-based	O
model	B
(	O
section	O
16.2.4	O
et	O
al.	O
,	O
}	O
x	O
0	O
,	O
1	O
;	O
,	O
654	O
chapter	O
20.	O
deep	O
generative	O
models	O
meaning	O
we	O
deﬁne	O
the	O
joint	O
probability	O
distribution	O
using	O
an	O
energy	O
function	O
:	O
−	O
e	O
x	O
exp	O
(	O
(	O
)	O
)	O
z	O
p	O
(	O
)	O
=x	O
,	O
(	O
20.1	O
)	O
	O
x	O
where	O
e	O
(	O
x	O
)	O
is	O
the	O
energy	O
function	O
and	O
z	O
is	O
the	O
partition	O
function	O
that	O
ensures	O
that	O
.	O
the	O
energy	O
function	O
of	O
the	O
boltzmann	O
machine	O
is	O
given	O
by	O
p	O
(	O
)	O
=	O
1	O
x	O
−	O
	O
x	O
e	O
(	O
)	O
=	O
x	O
−	O
	O
u	O
x	O
b	O
x	O
,	O
(	O
20.2	O
)	O
where	O
u	O
is	O
the	O
“	O
weight	O
”	O
matrix	O
of	O
model	B
parameters	O
and	O
b	O
is	O
the	O
vector	O
of	O
bias	O
parameters	O
.	O
in	O
the	O
general	O
setting	O
of	O
the	O
boltzmann	O
machine	O
,	O
we	O
are	O
given	O
a	O
set	O
of	O
training	O
examples	O
,	O
each	O
of	O
which	O
are	O
n-dimensional	O
.	O
equation	O
describes	O
the	O
joint	O
probability	O
distribution	O
over	O
the	O
observed	O
variables	O
.	O
while	O
this	O
scenario	O
is	O
certainly	O
viable	O
,	O
it	O
does	O
limit	O
the	O
kinds	O
of	O
interactions	O
between	O
the	O
observed	O
variables	O
to	O
those	O
described	O
by	O
the	O
weight	O
matrix	O
.	O
speciﬁcally	O
,	O
it	O
means	O
that	O
the	O
probability	O
of	O
one	O
unit	O
being	O
on	O
is	O
given	O
by	O
a	O
linear	O
model	B
(	O
logistic	O
regression	O
)	O
from	O
the	O
values	O
of	O
the	O
other	O
units	O
.	O
20.1	O
the	O
boltzmann	O
machine	O
becomes	O
more	O
powerful	O
when	O
not	O
all	O
the	O
variables	O
are	O
observed	O
.	O
in	O
this	O
case	O
,	O
the	O
latent	O
variables	O
,	O
can	O
act	O
similarly	O
to	O
hidden	O
units	O
in	O
a	O
multi-layer	O
perceptron	O
and	O
model	B
higher-order	O
interactions	O
among	O
the	O
visible	O
units	O
.	O
just	O
as	O
the	O
addition	O
of	O
hidden	O
units	O
to	O
convert	O
logistic	O
regression	O
into	O
an	O
mlp	O
results	O
in	O
the	O
mlp	O
being	O
a	O
universal	O
approximator	O
of	O
functions	O
,	O
a	O
boltzmann	O
machine	O
with	O
hidden	O
units	O
is	O
no	O
longer	O
limited	O
to	O
modeling	O
linear	O
relationships	O
between	O
variables	O
.	O
instead	O
,	O
the	O
boltzmann	O
machine	O
becomes	O
a	O
universal	O
approximator	O
of	O
probability	O
mass	O
functions	O
over	O
discrete	O
variables	O
(	O
le	O
roux	O
and	O
bengio	O
2008	O
)	O
.	O
,	O
formally	O
,	O
we	O
decompose	O
the	O
units	O
x	O
into	O
two	O
subsets	O
:	O
the	O
visible	O
units	O
v	O
and	O
the	O
latent	O
(	O
or	O
hidden	O
)	O
units	O
h	O
−	O
	O
v	O
e	O
,	O
(	O
v	O
h	O
)	O
=	O
rv	O
.	O
the	O
energy	O
function	O
becomes	O
−	O
	O
v	O
−	O
	O
w	O
h	O
h	O
−	O
	O
sh	O
b	O
−	O
	O
c	O
v	O
h.	O
(	O
20.3	O
)	O
boltzmann	O
machine	O
learning	O
learning	O
algorithms	O
for	O
boltzmann	O
machines	O
are	O
usually	O
based	O
on	O
maximum	O
likelihood	O
.	O
all	O
boltzmann	O
machines	O
have	O
an	O
intractable	O
partition	O
function	O
,	O
so	O
the	O
maximum	O
likelihood	O
gradient	O
must	O
be	O
ap-	O
proximated	O
using	O
the	O
techniques	O
described	O
in	O
chapter	O
.18	O
one	O
interesting	O
property	O
of	O
boltzmann	O
machines	O
when	O
trained	O
with	O
learning	O
rules	O
based	O
on	O
maximum	O
likelihood	O
is	O
that	O
the	O
update	O
for	O
a	O
particular	O
weight	O
|	O
connecting	O
two	O
units	O
depends	O
only	O
the	O
statistics	O
of	O
those	O
two	O
units	O
,	O
collected	O
under	O
diﬀerent	O
distributions	O
:	O
pmodel	O
(	O
v	O
)	O
and	O
ˆpdata	O
(	O
v	O
)	O
pmodel	O
(	O
h	O
v	O
)	O
.	O
the	O
rest	O
of	O
the	O
655	O
chapter	O
20.	O
deep	O
generative	O
models	O
network	O
participates	O
in	O
shaping	O
those	O
statistics	O
,	O
but	O
the	O
weight	O
can	O
be	O
updated	O
without	O
knowing	O
anything	O
about	O
the	O
rest	O
of	O
the	O
network	O
or	O
how	O
those	O
statistics	O
were	O
produced	O
.	O
this	O
means	O
that	O
the	O
learning	O
rule	O
is	O
“	O
local	O
,	O
”	O
which	O
makes	O
boltzmann	O
machine	O
learning	O
somewhat	O
biologically	O
plausible	O
.	O
it	O
is	O
conceivable	O
that	O
if	O
each	O
neuron	O
were	O
a	O
random	O
variable	O
in	O
a	O
boltzmann	O
machine	O
,	O
then	O
the	O
axons	O
and	O
dendrites	O
connecting	O
two	O
random	O
variables	O
could	O
learn	O
only	O
by	O
observing	O
the	O
ﬁring	O
pattern	O
of	O
the	O
cells	O
that	O
they	O
actually	O
physically	O
touch	O
.	O
in	O
particular	O
,	O
in	O
the	O
positive	O
phase	O
,	O
two	O
units	O
that	O
frequently	O
activate	O
together	O
have	O
their	O
connection	O
strengthened	O
.	O
this	O
is	O
an	O
example	O
of	O
a	O
hebbian	O
learning	O
rule	O
(	O
)	O
often	O
summarized	O
with	O
the	O
mnemonic	O
“	O
ﬁre	O
together	O
,	O
wire	O
together.	O
”	O
hebbian	O
learning	O
rules	O
are	O
among	O
the	O
oldest	O
hypothesized	O
explanations	O
for	O
learning	O
in	O
biological	O
systems	O
and	O
remain	O
relevant	O
today	O
(	O
giudice	O
et	O
al	O
.	O
2009	O
hebb	O
1949	O
)	O
.	O
,	O
,	O
other	O
learning	O
algorithms	O
that	O
use	O
more	O
information	O
than	O
local	O
statistics	O
seem	O
to	O
require	O
us	O
to	O
hypothesize	O
the	O
existence	O
of	O
more	O
machinery	O
than	O
this	O
.	O
for	O
example	O
,	O
for	O
the	O
brain	O
to	O
implement	O
back-propagation	O
in	O
a	O
multilayer	O
perceptron	O
,	O
it	O
seems	O
necessary	O
for	O
the	O
brain	O
to	O
maintain	O
a	O
secondary	O
communication	O
network	O
for	O
transmitting	O
gradient	O
information	O
backwards	O
through	O
the	O
network	O
.	O
proposals	O
for	O
biologically	O
plausible	O
implementations	O
(	O
and	O
approximations	O
)	O
of	O
back-propagation	O
have	O
been	O
made	O
(	O
)	O
but	O
remain	O
to	O
be	O
validated	O
,	O
and	O
bengio	O
2015	O
)	O
links	O
back-propagation	O
of	O
gradients	O
to	O
inference	O
in	O
energy-based	O
models	O
similar	O
to	O
the	O
boltzmann	O
machine	O
(	O
but	O
with	O
continuous	O
latent	O
variables	O
)	O
.	O
hinton	O
2007a	O
bengio	O
2015	O
(	O
,	O
;	O
,	O
the	O
negative	O
phase	O
of	O
boltzmann	O
machine	O
learning	O
is	O
somewhat	O
harder	O
to	O
explain	O
from	O
a	O
biological	O
point	O
of	O
view	O
.	O
as	O
argued	O
in	O
section	O
,	O
dream	O
sleep	O
may	O
be	O
a	O
form	O
of	O
negative	O
phase	O
sampling	O
.	O
this	O
idea	O
is	O
more	O
speculative	O
though	O
.	O
18.2	O
20.2	O
restricted	O
boltzmann	O
machines	O
,	O
smolensky	O
1986	O
)	O
,	O
restricted	O
boltzmann	O
invented	O
under	O
the	O
name	O
harmonium	O
(	O
machines	O
are	O
some	O
of	O
the	O
most	O
common	O
building	O
blocks	O
of	O
deep	O
probabilistic	O
models	O
.	O
we	O
have	O
brieﬂy	O
described	O
rbms	O
previously	O
,	O
in	O
section	O
.	O
here	O
we	O
review	O
the	O
previous	O
information	O
and	O
go	O
into	O
more	O
detail	O
.	O
rbms	O
are	O
undirected	O
probabilistic	O
graphical	O
models	O
containing	O
a	O
layer	O
of	O
observable	O
variables	O
and	O
a	O
single	O
layer	O
of	O
latent	O
variables	O
.	O
rbms	O
may	O
be	O
stacked	O
(	O
one	O
on	O
top	O
of	O
the	O
other	O
)	O
to	O
form	O
deeper	O
models	O
.	O
see	O
ﬁgure	O
a	O
shows	O
the	O
graph	O
structure	O
of	O
the	O
rbm	O
itself	O
.	O
it	O
is	O
a	O
bipartite	O
graph	O
,	O
with	O
no	O
connections	O
permitted	O
between	O
any	O
variables	O
in	O
the	O
observed	O
layer	O
or	O
between	O
any	O
units	O
in	O
the	O
latent	O
layer	O
.	O
for	O
some	O
examples	O
.	O
in	O
particular	O
,	O
ﬁgure	O
16.7.1	O
20.1	O
20.1	O
656	O
chapter	O
20.	O
deep	O
generative	O
models	O
h	O
(	O
2	O
)	O
1h	O
(	O
2	O
)	O
1	O
h	O
(	O
2	O
)	O
2h	O
(	O
2	O
)	O
2	O
h	O
(	O
2	O
)	O
h	O
(	O
2	O
)	O
3	O
3	O
h1h1	O
h2h2	O
h3h3	O
h4h4	O
h	O
(	O
1	O
)	O
1h	O
(	O
1	O
)	O
1	O
h	O
(	O
1	O
)	O
2h	O
(	O
1	O
)	O
2	O
h	O
(	O
1	O
)	O
3h	O
(	O
1	O
)	O
3	O
h	O
(	O
1	O
)	O
4h	O
(	O
1	O
)	O
4	O
v1v1	O
v2v2	O
v3v3	O
v1v1	O
v2v2	O
v3v3	O
(	O
a	O
)	O
(	O
b	O
)	O
h	O
(	O
2	O
)	O
1h	O
(	O
2	O
)	O
1	O
h	O
(	O
2	O
)	O
2h	O
(	O
2	O
)	O
2	O
h	O
(	O
2	O
)	O
3h	O
(	O
2	O
)	O
3	O
h	O
(	O
1	O
)	O
h	O
(	O
1	O
)	O
1	O
1	O
h	O
(	O
1	O
)	O
h	O
(	O
1	O
)	O
2	O
2	O
h	O
(	O
1	O
)	O
3h	O
(	O
1	O
)	O
3	O
h	O
(	O
1	O
)	O
h	O
(	O
1	O
)	O
4	O
4	O
v1v1	O
v2v2	O
v3v3	O
(	O
c	O
)	O
figure	O
20.1	O
:	O
examples	O
of	O
models	O
that	O
may	O
be	O
built	O
with	O
restricted	O
boltzmann	O
machines	O
.	O
(	O
a	O
)	O
the	O
restricted	O
boltzmann	O
machine	O
itself	O
is	O
an	O
undirected	O
graphical	O
model	B
based	O
on	O
a	O
bipartite	O
graph	O
,	O
with	O
visible	O
units	O
in	O
one	O
part	O
of	O
the	O
graph	O
and	O
hidden	O
units	O
in	O
the	O
other	O
part	O
.	O
there	O
are	O
no	O
connections	O
among	O
the	O
visible	O
units	O
,	O
nor	O
any	O
connections	O
among	O
the	O
hidden	O
units	O
.	O
typically	O
every	O
visible	O
unit	O
is	O
connected	O
to	O
every	O
hidden	O
unit	O
but	O
it	O
is	O
possible	O
to	O
construct	O
sparsely	O
connected	O
rbms	O
such	O
as	O
convolutional	O
rbms	O
.	O
a	O
(	O
b	O
)	O
deep	O
belief	O
network	O
is	O
a	O
hybrid	O
graphical	O
model	B
involving	O
both	O
directed	O
and	O
undirected	O
connections	O
.	O
like	O
an	O
rbm	O
,	O
it	O
has	O
no	O
intralayer	O
connections	O
.	O
however	O
,	O
a	O
dbn	O
has	O
multiple	O
hidden	O
layers	O
,	O
and	O
thus	O
there	O
are	O
connections	O
between	O
hidden	O
units	O
that	O
are	O
in	O
separate	O
layers	O
.	O
all	O
of	O
the	O
local	O
conditional	O
probability	O
distributions	O
needed	O
by	O
the	O
deep	O
belief	O
network	O
are	O
copied	O
directly	O
from	O
the	O
local	O
conditional	O
probability	O
distributions	O
of	O
its	O
constituent	O
rbms	O
.	O
alternatively	O
,	O
we	O
could	O
also	O
represent	O
the	O
deep	O
belief	O
network	O
with	O
a	O
completely	O
undirected	O
graph	O
,	O
but	O
it	O
would	O
need	O
intralayer	O
connections	O
to	O
capture	O
the	O
dependencies	O
between	O
parents	O
.	O
a	O
deep	O
boltzmann	O
machine	O
is	O
an	O
undirected	O
graphical	O
model	B
with	O
several	O
layers	O
of	O
latent	O
variables	O
.	O
like	O
rbms	O
and	O
dbns	O
,	O
dbms	O
lack	O
intralayer	O
connections	O
.	O
dbms	O
are	O
less	O
closely	O
tied	O
to	O
rbms	O
than	O
dbns	O
are	O
.	O
when	O
initializing	O
a	O
dbm	O
from	O
a	O
stack	O
of	O
rbms	O
,	O
it	O
is	O
necessary	O
to	O
modify	O
the	O
rbm	O
parameters	O
slightly	O
.	O
some	O
kinds	O
of	O
dbms	O
may	O
be	O
trained	O
without	O
ﬁrst	O
training	O
a	O
set	O
of	O
rbms	O
.	O
(	O
c	O
)	O
657	O
chapter	O
20.	O
deep	O
generative	O
models	O
we	O
begin	O
with	O
the	O
binary	O
version	O
of	O
the	O
restricted	O
boltzmann	O
machine	O
,	O
but	O
as	O
we	O
see	O
later	O
there	O
are	O
extensions	O
to	O
other	O
types	O
of	O
visible	O
and	O
hidden	O
units	O
.	O
more	O
formally	O
,	O
let	O
the	O
observed	O
layer	O
consist	O
of	O
a	O
set	O
of	O
n	O
v	O
binary	O
random	O
variables	O
which	O
we	O
refer	O
to	O
collectively	O
with	O
the	O
vector	O
v.	O
we	O
refer	O
to	O
the	O
latent	O
or	O
hidden	O
layer	O
of	O
nh	O
binary	O
random	O
variables	O
as	O
.h	O
like	O
the	O
general	O
boltzmann	O
machine	O
,	O
the	O
restricted	O
boltzmann	O
machine	O
is	O
an	O
energy-based	O
model	B
with	O
the	O
joint	O
probability	O
distribution	O
speciﬁed	O
by	O
its	O
energy	O
function	O
:	O
p	O
(	O
=	O
v	O
v	O
h	O
=	O
)	O
=h	O
,	O
)	O
)	O
.	O
(	O
20.4	O
)	O
−	O
e	O
v	O
h	O
,	O
exp	O
(	O
(	O
1	O
z	O
the	O
energy	O
function	O
for	O
an	O
rbm	O
is	O
given	O
by	O
−	O
	O
c	O
−	O
	O
b	O
e	O
,	O
(	O
v	O
h	O
)	O
=	O
v	O
	O
	O
−	O
	O
h	O
v	O
w	O
h	O
,	O
and	O
z	O
is	O
the	O
normalizing	O
constant	O
known	O
as	O
the	O
partition	O
function	O
:	O
{	O
−	O
}	O
)	O
e	O
v	O
h	O
,	O
(	O
.	O
z	O
=	O
exp	O
v	O
h	O
(	O
20.5	O
)	O
(	O
20.6	O
)	O
it	O
is	O
apparent	O
from	O
the	O
deﬁnition	O
of	O
the	O
partition	O
function	O
z	O
that	O
the	O
naive	O
method	O
of	O
computing	O
z	O
(	O
exhaustively	O
summing	O
over	O
all	O
states	O
)	O
could	O
be	O
computationally	O
intractable	O
,	O
unless	O
a	O
cleverly	O
designed	O
algorithm	O
could	O
exploit	O
regularities	O
in	O
the	O
probability	O
distribution	O
to	O
compute	O
z	O
faster	O
.	O
in	O
the	O
case	O
of	O
restricted	O
boltzmann	O
z	O
machines	O
,	O
is	O
intractable	O
.	O
the	O
intractable	O
partition	O
function	O
z	O
implies	O
that	O
the	O
normalized	O
joint	O
probability	O
distribution	O
)	O
formally	O
proved	O
that	O
the	O
partition	O
function	O
is	O
also	O
intractable	O
to	O
evaluate	O
.	O
long	O
and	O
servedio	O
2010	O
p	O
(	O
)	O
v	O
(	O
20.2.1	O
conditional	O
distributions	O
though	O
p	O
(	O
v	O
)	O
is	O
intractable	O
,	O
the	O
bipartite	O
graph	O
structure	O
of	O
the	O
rbm	O
has	O
the	O
very	O
special	O
property	O
that	O
its	O
conditional	O
distributions	O
p	O
(	O
h	O
v	O
)	O
are	O
factorial	O
and	O
relatively	O
simple	O
to	O
compute	O
and	O
to	O
sample	O
from.	O
)	O
and	O
p	O
(	O
v	O
h	O
|	O
|	O
deriving	O
the	O
conditional	O
distributions	O
from	O
the	O
joint	O
distribution	O
is	O
straightfor-	O
	O
	O
h	O
v+	O
	O
	O
w	O
h	O
(	O
20.7	O
)	O
(	O
20.8	O
)	O
(	O
20.9	O
)	O
ward	O
:	O
|	O
h	O
v	O
p	O
(	O
)	O
=	O
=	O
=	O
	O
p	O
,	O
(	O
h	O
v	O
)	O
p	O
(	O
)	O
v	O
1	O
1	O
p	O
(	O
)	O
v	O
z	O
1	O
	O
exp	O
z	O
	O
	O
v	O
b	O
c+	O
	O
exp	O
	O
c	O
h	O
v+	O
w	O
h	O
658	O
chapter	O
20.	O
deep	O
generative	O
models	O
	O
	O
	O
nh	O
j=1	O
	O
	O
exp	O
=	O
1	O
z	O
=	O
	O
1	O
z	O
nh	O
j=1	O
exp	O
	O
	O
w	O
:	O
,j	O
hj	O
	O
j=1	O
	O
nh	O
	O
v	O
cjhj	O
+	O
c	O
jhj	O
+	O
v	O
w	O
:	O
,j	O
hj	O
(	O
20.10	O
)	O
(	O
20.11	O
)	O
|	O
|	O
since	O
we	O
are	O
conditioning	O
on	O
the	O
visible	O
units	O
v	O
,	O
we	O
can	O
treat	O
these	O
as	O
constant	O
with	O
respect	O
to	O
the	O
distribution	O
p	O
(	O
h	O
v	O
)	O
.	O
the	O
factorial	O
nature	O
of	O
the	O
conditional	O
p	O
(	O
h	O
v	O
)	O
follows	O
immediately	O
from	O
our	O
ability	O
to	O
write	O
the	O
joint	O
probability	O
over	O
the	O
vector	O
h	O
as	O
the	O
product	O
of	O
(	O
unnormalized	O
)	O
distributions	O
over	O
the	O
individual	O
elements	O
,	O
h	O
j.	O
it	O
is	O
now	O
a	O
simple	O
matter	O
of	O
normalizing	O
the	O
distributions	O
over	O
the	O
individual	O
binary	O
hj	O
.	O
|	O
p	O
h	O
(	O
j	O
=	O
1	O
v	O
)	O
=	O
=	O
	O
	O
)	O
v	O
|	O
˜p	O
h	O
(	O
j	O
=	O
1	O
|	O
|	O
˜p	O
h	O
(	O
j	O
=	O
1	O
˜p	O
h	O
(	O
j	O
=	O
0	O
)	O
+	O
v	O
	O
{	O
{	O
}	O
exp	O
w	O
:	O
,j	O
cj	O
+	O
v	O
	O
cj	O
+	O
v	O
exp	O
0	O
+	O
exp	O
	O
w	O
:	O
,j	O
	O
cj	O
+	O
v	O
w	O
:	O
,j	O
.	O
	O
	O
	O
)	O
v	O
}	O
(	O
20.12	O
)	O
(	O
20.13	O
)	O
(	O
20.14	O
)	O
=	O
σ	O
	O
nh	O
j=1	O
	O
we	O
can	O
now	O
express	O
the	O
full	O
conditional	O
over	O
the	O
hidden	O
layer	O
as	O
the	O
factorial	O
distribution	O
:	O
|	O
h	O
v	O
p	O
(	O
)	O
=	O
σ	O
(	O
2	O
h	O
(	O
+	O
c	O
w	O
−	O
	O
1	O
)	O
	O
v	O
)	O
.	O
j	O
(	O
20.15	O
)	O
|	O
a	O
similar	O
derivation	O
will	O
show	O
that	O
the	O
other	O
condition	O
of	O
interest	O
to	O
us	O
,	O
p	O
(	O
v	O
h	O
)	O
,	O
is	O
also	O
a	O
factorial	O
distribution	O
:	O
|	O
v	O
h	O
p	O
(	O
)	O
=	O
−	O
	O
1	O
)	O
σ	O
(	O
(	O
2	O
v	O
nv	O
i=1	O
(	O
+	O
b	O
w	O
h	O
)	O
)	O
i	O
.	O
(	O
20.16	O
)	O
20.2.2	O
training	O
restricted	O
boltzmann	O
machines	O
because	O
the	O
rbm	O
admits	O
eﬃcient	O
evaluation	O
and	O
diﬀerentiation	O
of	O
˜p	O
(	O
v	O
)	O
and	O
eﬃcient	O
mcmc	O
sampling	O
in	O
the	O
form	O
of	O
block	O
gibbs	O
sampling	O
,	O
it	O
can	O
readily	O
be	O
trained	O
with	O
any	O
of	O
the	O
techniques	O
described	O
in	O
chapter	O
for	O
training	O
models	O
that	O
have	O
intractable	O
partition	O
functions	O
.	O
this	O
includes	O
cd	O
,	O
sml	O
(	O
pcd	O
)	O
,	O
ratio	O
matching	O
and	O
so	O
on	O
.	O
compared	O
to	O
other	O
undirected	O
models	O
used	O
in	O
deep	O
learning	O
,	O
v	O
)	O
the	O
rbm	O
is	O
relatively	O
straightforward	O
to	O
train	O
because	O
we	O
can	O
compute	O
p	O
(	O
h	O
18	O
|	O
659	O
chapter	O
20.	O
deep	O
generative	O
models	O
exactly	O
in	O
closed	O
form	O
.	O
some	O
other	O
deep	O
models	O
,	O
such	O
as	O
the	O
deep	O
boltzmann	O
machine	O
,	O
combine	O
both	O
the	O
diﬃculty	O
of	O
an	O
intractable	O
partition	O
function	O
and	O
the	O
diﬃculty	O
of	O
intractable	O
inference	O
.	O
20.3	O
deep	O
belief	O
networks	O
et	O
al.	O
,	O
deep	O
belief	O
networks	O
(	O
dbns	O
)	O
were	O
one	O
of	O
the	O
ﬁrst	O
non-convolutional	O
models	O
to	O
successfully	O
admit	O
training	O
of	O
deep	O
architectures	O
(	O
hinton	O
2006	O
hinton	O
,	O
2007b	O
)	O
.	O
the	O
introduction	O
of	O
deep	O
belief	O
networks	O
in	O
2006	O
began	O
the	O
current	O
deep	O
learning	O
renaissance	O
.	O
prior	O
to	O
the	O
introduction	O
of	O
deep	O
belief	O
networks	O
,	O
deep	O
models	O
were	O
considered	O
too	O
diﬃcult	O
to	O
optimize	O
.	O
kernel	O
machines	O
with	O
convex	O
objective	O
functions	O
dominated	O
the	O
research	O
landscape	O
.	O
deep	O
belief	O
networks	O
demonstrated	O
that	O
deep	O
architectures	O
can	O
be	O
successful	O
,	O
by	O
outperforming	O
kernelized	O
support	O
vector	O
machines	O
on	O
the	O
mnist	O
dataset	O
(	O
)	O
.	O
today	O
,	O
deep	O
belief	O
networks	O
have	O
mostly	O
fallen	O
out	O
of	O
favor	O
and	O
are	O
rarely	O
used	O
,	O
even	O
compared	O
to	O
other	O
unsupervised	O
or	O
generative	O
learning	O
algorithms	O
,	O
but	O
they	O
are	O
still	O
deservedly	O
recognized	O
for	O
their	O
important	O
role	O
in	O
deep	O
learning	O
history	O
.	O
hinton	O
et	O
al	O
.	O
2006	O
;	O
,	O
deep	O
belief	O
networks	O
are	O
generative	O
models	O
with	O
several	O
layers	O
of	O
latent	O
variables	O
.	O
the	O
latent	O
variables	O
are	O
typically	O
binary	O
,	O
while	O
the	O
visible	O
units	O
may	O
be	O
binary	O
or	O
real	O
.	O
there	O
are	O
no	O
intralayer	O
connections	O
.	O
usually	O
,	O
every	O
unit	O
in	O
each	O
layer	O
is	O
connected	O
to	O
every	O
unit	O
in	O
each	O
neighboring	O
layer	O
,	O
though	O
it	O
is	O
possible	O
to	O
construct	O
more	O
sparsely	O
connected	O
dbns	O
.	O
the	O
connections	O
between	O
the	O
top	O
two	O
layers	O
are	O
undirected	O
.	O
the	O
connections	O
between	O
all	O
other	O
layers	O
are	O
directed	O
,	O
with	O
the	O
arrows	O
pointed	O
toward	O
the	O
layer	O
that	O
is	O
closest	O
to	O
the	O
data	O
.	O
see	O
ﬁgure	O
b	O
for	O
an	O
example	O
.	O
a	O
dbn	O
with	O
l	O
hidden	O
layers	O
contains	O
l	O
weight	O
matrices	O
:	O
w	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
w	O
(	O
)	O
l	O
.	O
it	O
also	O
contains	O
l	O
+	O
1	O
bias	O
vectors	O
:	O
b	O
(	O
0	O
)	O
,	O
.	O
.	O
.	O
,	O
b	O
(	O
)	O
l	O
,	O
with	O
b	O
(	O
0	O
)	O
providing	O
the	O
biases	O
for	O
the	O
visible	O
layer	O
.	O
the	O
probability	O
distribution	O
represented	O
by	O
the	O
dbn	O
is	O
given	O
by	O
	O
	O
20.1	O
|	O
=	O
1	O
p	O
h	O
(	O
(	O
)	O
k	O
i	O
k	O
h	O
(	O
+1	O
)	O
−	O
l	O
p	O
(	O
h	O
(	O
)	O
l	O
,	O
h	O
(	O
1	O
)	O
∝	O
)	O
exp	O
	O
b	O
(	O
)	O
l	O
−	O
	O
1	O
)	O
l	O
h	O
(	O
)	O
l	O
+	O
b	O
(	O
)	O
=	O
σ	O
	O
i	O
+	O
w	O
(	O
+1	O
)	O
b	O
(	O
)	O
k	O
|	O
p	O
v	O
(	O
i	O
=	O
1	O
h	O
(	O
1	O
)	O
)	O
=	O
σ	O
k	O
:	O
,i	O
	O
	O
−	O
1	O
)	O
l	O
h	O
(	O
w	O
(	O
)	O
l	O
h	O
(	O
)	O
l	O
−	O
−	O
	O
+	O
h	O
(	O
1	O
)	O
l	O
∀	O
∀	O
∈	O
i	O
,	O
k	O
1	O
,	O
.	O
.	O
.	O
,	O
l	O
∀	O
	O
b	O
(	O
0	O
)	O
i	O
+	O
w	O
(	O
1	O
)	O
i	O
.	O
	O
	O
:	O
,i	O
h	O
(	O
1	O
)	O
h	O
(	O
+1	O
)	O
k	O
,	O
(	O
20.17	O
)	O
2	O
,	O
(	O
20.18	O
)	O
(	O
20.19	O
)	O
(	O
20.20	O
)	O
	O
	O
in	O
the	O
case	O
of	O
real-valued	O
visible	O
units	O
,	O
substitute	O
∼	O
n	O
v	O
	O
(	O
0	O
)	O
+	O
w	O
(	O
1	O
)	O
v	O
b	O
;	O
h	O
(	O
1	O
)	O
,	O
β	O
−	O
1	O
660	O
chapter	O
20.	O
deep	O
generative	O
models	O
with	O
β	O
diagonal	O
for	O
tractability	O
.	O
generalizations	O
to	O
other	O
exponential	O
family	O
visible	O
units	O
are	O
straightforward	O
,	O
at	O
least	O
in	O
theory	O
.	O
a	O
dbn	O
with	O
only	O
one	O
hidden	O
layer	O
is	O
just	O
an	O
rbm	O
.	O
to	O
generate	O
a	O
sample	O
from	O
a	O
dbn	O
,	O
we	O
ﬁrst	O
run	O
several	O
steps	O
of	O
gibbs	O
sampling	O
on	O
the	O
top	O
two	O
hidden	O
layers	O
.	O
this	O
stage	O
is	O
essentially	O
drawing	O
a	O
sample	O
from	O
the	O
rbm	O
deﬁned	O
by	O
the	O
top	O
two	O
hidden	O
layers	O
.	O
we	O
can	O
then	O
use	O
a	O
single	O
pass	O
of	O
ancestral	O
sampling	O
through	O
the	O
rest	O
of	O
the	O
model	B
to	O
draw	O
a	O
sample	O
from	O
the	O
visible	O
units	O
.	O
deep	O
belief	O
networks	O
incur	O
many	O
of	O
the	O
problems	O
associated	O
with	O
both	O
directed	O
models	O
and	O
undirected	O
models	O
.	O
inference	O
in	O
a	O
deep	O
belief	O
network	O
is	O
intractable	O
due	O
to	O
the	O
explaining	O
away	O
eﬀect	O
within	O
each	O
directed	O
layer	O
,	O
and	O
due	O
to	O
the	O
interaction	O
between	O
the	O
two	O
hidden	O
layers	O
that	O
have	O
undirected	O
connections	O
.	O
evaluating	O
or	O
maximizing	O
the	O
standard	O
evidence	O
lower	O
bound	B
on	O
the	O
log-likelihood	O
is	O
also	O
intractable	O
,	O
because	O
the	O
evidence	O
lower	O
bound	B
takes	O
the	O
expectation	O
of	O
cliques	O
whose	O
size	O
is	O
equal	O
to	O
the	O
network	O
width	O
.	O
evaluating	O
or	O
maximizing	O
the	O
log-likelihood	O
requires	O
not	O
just	O
confronting	O
the	O
problem	O
of	O
intractable	O
inference	O
to	O
marginalize	O
out	O
the	O
latent	O
variables	O
,	O
but	O
also	O
the	O
problem	O
of	O
an	O
intractable	O
partition	O
function	O
within	O
the	O
undirected	O
model	B
of	O
the	O
top	O
two	O
layers	O
.	O
to	O
train	O
a	O
deep	O
belief	O
network	O
,	O
one	O
begins	O
by	O
training	O
an	O
rbm	O
to	O
maximize	O
∼	O
log	O
p	O
(	O
v	O
)	O
using	O
contrastive	O
divergence	O
or	O
stochastic	O
maximum	O
likelihood	O
.	O
pdata	O
ev	O
the	O
parameters	O
of	O
the	O
rbm	O
then	O
deﬁne	O
the	O
parameters	O
of	O
the	O
ﬁrst	O
layer	O
of	O
the	O
dbn	O
.	O
next	O
,	O
a	O
second	O
rbm	O
is	O
trained	O
to	O
approximately	O
maximize	O
∼	O
pdataeh	O
(	O
1	O
)	O
ev	O
∼	O
p	O
(	O
1	O
)	O
(	O
h	O
(	O
1	O
)	O
|	O
v	O
)	O
log	O
p	O
(	O
2	O
)	O
(	O
h	O
(	O
1	O
)	O
)	O
(	O
20.21	O
)	O
where	O
p	O
(	O
1	O
)	O
is	O
the	O
probability	O
distribution	O
represented	O
by	O
the	O
ﬁrst	O
rbm	O
and	O
p	O
(	O
2	O
)	O
is	O
the	O
probability	O
distribution	O
represented	O
by	O
the	O
second	O
rbm	O
.	O
in	O
other	O
words	O
,	O
the	O
second	O
rbm	O
is	O
trained	O
to	O
model	B
the	O
distribution	O
deﬁned	O
by	O
sampling	O
the	O
hidden	O
units	O
of	O
the	O
ﬁrst	O
rbm	O
,	O
when	O
the	O
ﬁrst	O
rbm	O
is	O
driven	O
by	O
the	O
data	O
.	O
this	O
procedure	O
can	O
be	O
repeated	O
indeﬁnitely	O
,	O
to	O
add	O
as	O
many	O
layers	O
to	O
the	O
dbn	O
as	O
desired	O
,	O
with	O
each	O
new	O
rbm	O
modeling	O
the	O
samples	O
of	O
the	O
previous	O
one	O
.	O
each	O
rbm	O
deﬁnes	O
another	O
layer	O
of	O
the	O
dbn	O
.	O
this	O
procedure	O
can	O
be	O
justiﬁed	O
as	O
increasing	O
a	O
variational	O
lower	O
bound	B
on	O
the	O
log-likelihood	O
of	O
the	O
data	O
under	O
the	O
dbn	O
(	O
hinton	O
et	O
al.	O
,	O
2006	O
)	O
.	O
in	O
most	O
applications	O
,	O
no	O
eﬀort	O
is	O
made	O
to	O
jointly	O
train	O
the	O
dbn	O
after	O
the	O
greedy	O
layer-wise	O
procedure	O
is	O
complete	O
.	O
however	O
,	O
it	O
is	O
possible	O
to	O
perform	O
generative	O
ﬁne-tuning	B
using	O
the	O
wake-sleep	O
algorithm	O
.	O
661	O
chapter	O
20.	O
deep	O
generative	O
models	O
	O
	O
h	O
(	O
1	O
)	O
=	O
σ	O
	O
b	O
(	O
1	O
)	O
+	O
v	O
	O
	O
the	O
trained	O
dbn	O
may	O
be	O
used	O
directly	O
as	O
a	O
generative	O
model	B
,	O
but	O
most	O
of	O
the	O
interest	O
in	O
dbns	O
arose	O
from	O
their	O
ability	O
to	O
improve	O
classiﬁcation	O
models	O
.	O
we	O
can	O
take	O
the	O
weights	O
from	O
the	O
dbn	O
and	O
use	O
them	O
to	O
deﬁne	O
an	O
mlp	O
:	O
w	O
(	O
1	O
)	O
∀	O
∈	O
l	O
.	O
(	O
20.22	O
)	O
2	O
,	O
.	O
.	O
.	O
,	O
m	O
,	O
(	O
20.23	O
)	O
−	O
	O
1	O
)	O
l	O
w	O
(	O
)	O
l	O
h	O
(	O
)	O
l	O
=	O
σ	O
b	O
(	O
)	O
l	O
i	O
+	O
h	O
(	O
after	O
initializing	O
this	O
mlp	O
with	O
the	O
weights	O
and	O
biases	O
learned	O
via	O
generative	O
training	O
of	O
the	O
dbn	O
,	O
we	O
may	O
train	O
the	O
mlp	O
to	O
perform	O
a	O
classiﬁcation	O
task	O
.	O
this	O
additional	O
training	O
of	O
the	O
mlp	O
is	O
an	O
example	O
of	O
discriminative	O
ﬁne-tuning	B
.	O
19	O
tight	O
this	O
speciﬁc	O
choice	O
of	O
mlp	O
is	O
somewhat	O
arbitrary	O
,	O
compared	O
to	O
many	O
of	O
the	O
inference	O
equations	O
in	O
chapter	O
that	O
are	O
derived	O
from	O
ﬁrst	O
principles	O
.	O
this	O
mlp	O
is	O
a	O
heuristic	O
choice	O
that	O
seems	O
to	O
work	B
well	O
in	O
practice	O
and	O
is	O
used	O
consistently	O
in	O
the	O
literature	O
.	O
many	O
approximate	O
inference	O
techniques	O
are	O
motivated	O
by	O
their	O
ability	O
to	O
ﬁnd	O
a	O
maximally	O
variational	O
lower	O
bound	B
on	O
the	O
log-likelihood	O
under	O
some	O
set	O
of	O
constraints	O
.	O
one	O
can	O
construct	O
a	O
variational	O
lower	O
bound	B
on	O
the	O
log-likelihood	O
using	O
the	O
hidden	O
unit	O
expectations	O
deﬁned	O
by	O
the	O
dbn	O
’	O
s	O
mlp	O
,	O
but	O
this	O
is	O
true	O
of	O
probability	O
distribution	O
over	O
the	O
hidden	O
units	O
,	O
and	O
there	O
is	O
no	O
reason	O
to	O
believe	O
that	O
this	O
mlp	O
provides	O
a	O
particularly	O
tight	O
bound	B
.	O
in	O
particular	O
,	O
the	O
mlp	O
ignores	O
many	O
important	O
interactions	O
in	O
the	O
dbn	O
graphical	O
model	B
.	O
the	O
mlp	O
propagates	O
information	O
upward	O
from	O
the	O
visible	O
units	O
to	O
the	O
deepest	O
hidden	O
units	O
,	O
but	O
does	O
not	O
propagate	O
any	O
information	O
downward	O
or	O
sideways	O
.	O
the	O
dbn	O
graphical	O
model	B
has	O
explaining	O
away	O
interactions	O
between	O
all	O
of	O
the	O
hidden	O
units	O
within	O
the	O
same	O
layer	O
as	O
well	O
as	O
top-down	O
interactions	O
between	O
layers	O
.	O
any	O
while	O
the	O
log-likelihood	O
of	O
a	O
dbn	O
is	O
intractable	O
,	O
it	O
may	O
be	O
approximated	O
with	O
)	O
.	O
this	O
permits	O
evaluating	O
its	O
quality	O
as	O
a	O
,	O
ais	O
(	O
salakhutdinov	O
and	O
murray	O
2008	O
generative	O
model	B
.	O
the	O
term	O
“	O
deep	O
belief	O
network	O
”	O
is	O
commonly	O
used	O
incorrectly	O
to	O
refer	O
to	O
any	O
kind	O
of	O
deep	O
neural	O
network	O
,	O
even	O
networks	O
without	O
latent	O
variable	O
semantics	O
.	O
the	O
term	O
“	O
deep	O
belief	O
network	O
”	O
should	O
refer	O
speciﬁcally	O
to	O
models	O
with	O
undirected	O
connections	O
in	O
the	O
deepest	O
layer	O
and	O
directed	O
connections	O
pointing	O
downward	O
between	O
all	O
other	O
pairs	O
of	O
consecutive	O
layers	O
.	O
the	O
term	O
“	O
deep	O
belief	O
network	O
”	O
may	O
also	O
cause	O
some	O
confusion	O
because	O
the	O
term	O
“	O
belief	O
network	O
”	O
is	O
sometimes	O
used	O
to	O
refer	O
to	O
purely	O
directed	O
models	O
,	O
while	O
deep	O
belief	O
networks	O
contain	O
an	O
undirected	O
layer	O
.	O
deep	O
belief	O
networks	O
also	O
share	O
the	O
acronym	O
dbn	O
with	O
dynamic	O
bayesian	O
networks	O
(	O
dean	O
and	O
kanazawa	O
1989	O
)	O
,	O
which	O
are	O
bayesian	O
networks	O
for	O
representing	O
markov	O
chains	O
.	O
,	O
662	O
chapter	O
20.	O
deep	O
generative	O
models	O
h	O
(	O
2	O
)	O
1h	O
(	O
2	O
)	O
1	O
h	O
(	O
2	O
)	O
2h	O
(	O
2	O
)	O
2	O
h	O
(	O
2	O
)	O
h	O
(	O
2	O
)	O
3	O
3	O
h	O
(	O
1	O
)	O
1h	O
(	O
1	O
)	O
1	O
h	O
(	O
1	O
)	O
2h	O
(	O
1	O
)	O
2	O
h	O
(	O
1	O
)	O
3h	O
(	O
1	O
)	O
3	O
h	O
(	O
1	O
)	O
4h	O
(	O
1	O
)	O
4	O
v1v1	O
v2v2	O
v3v3	O
figure	O
20.2	O
:	O
the	O
graphical	O
model	B
for	O
a	O
deep	O
boltzmann	O
machine	O
with	O
one	O
visible	O
layer	O
(	O
bottom	O
)	O
and	O
two	O
hidden	O
layers	O
.	O
connections	O
are	O
only	O
between	O
units	O
in	O
neighboring	O
layers	O
.	O
there	O
are	O
no	O
intralayer	O
layer	O
connections	O
.	O
20.4	O
deep	O
boltzmann	O
machines	O
a	O
deep	O
boltzmann	O
machine	O
or	O
dbm	O
(	O
salakhutdinov	O
and	O
hinton	O
2009a	O
)	O
is	O
another	O
kind	O
of	O
deep	O
,	O
generative	O
model	B
.	O
unlike	O
the	O
deep	O
belief	O
network	O
(	O
dbn	O
)	O
,	O
it	O
is	O
an	O
entirely	O
undirected	O
model	B
.	O
unlike	O
the	O
rbm	O
,	O
the	O
dbm	O
has	O
several	O
layers	O
of	O
latent	O
variables	O
(	O
rbms	O
have	O
just	O
one	O
)	O
.	O
but	O
like	O
the	O
rbm	O
,	O
within	O
each	O
layer	O
,	O
each	O
of	O
the	O
variables	O
are	O
mutually	O
independent	O
,	O
conditioned	O
on	O
the	O
variables	O
in	O
the	O
neighboring	O
layers	O
.	O
see	O
ﬁgure	O
for	O
the	O
graph	O
structure	O
.	O
deep	O
boltzmann	O
machines	O
have	O
been	O
applied	O
to	O
a	O
variety	O
of	O
tasks	O
including	O
document	O
modeling	O
(	O
srivastava	O
et	O
al.	O
,	O
2013	O
20.2	O
)	O
.	O
,	O
like	O
rbms	O
and	O
dbns	O
,	O
dbms	O
typically	O
contain	O
only	O
binary	O
units—as	O
we	O
assume	O
for	O
simplicity	O
of	O
our	O
presentation	O
of	O
the	O
model—but	O
it	O
is	O
straightforward	O
to	O
include	O
real-valued	O
visible	O
units	O
.	O
	O
a	O
dbm	O
is	O
an	O
energy-based	O
model	B
,	O
meaning	O
that	O
the	O
the	O
joint	O
probability	O
distribution	O
over	O
the	O
model	B
variables	O
is	O
parametrized	O
by	O
an	O
energy	O
function	O
e	O
.	O
in	O
the	O
case	O
of	O
a	O
deep	O
boltzmann	O
machine	O
with	O
one	O
visible	O
layer	O
,	O
v	O
,	O
and	O
three	O
hidden	O
layers	O
,	O
h	O
(	O
1	O
)	O
,	O
h	O
(	O
2	O
)	O
and	O
h	O
(	O
3	O
)	O
,	O
the	O
joint	O
probability	O
is	O
given	O
by	O
:	O
	O
	O
	O
p	O
v	O
h	O
,	O
(	O
1	O
)	O
,	O
h	O
(	O
2	O
)	O
,	O
h	O
(	O
3	O
)	O
=	O
1	O
z	O
(	O
)	O
θ	O
exp	O
−	O
e	O
,	O
(	O
v	O
h	O
(	O
1	O
)	O
,	O
h	O
(	O
2	O
)	O
,	O
h	O
(	O
3	O
)	O
;	O
)	O
θ	O
.	O
(	O
20.24	O
)	O
to	O
simplify	O
our	O
presentation	O
,	O
we	O
omit	O
the	O
bias	O
parameters	O
below	O
.	O
the	O
dbm	O
energy	O
function	O
is	O
then	O
deﬁned	O
as	O
follows	O
:	O
	O
−	O
e	O
,	O
(	O
v	O
h	O
(	O
1	O
)	O
,	O
h	O
(	O
2	O
)	O
,	O
h	O
(	O
3	O
)	O
;	O
)	O
=	O
v	O
θ	O
	O
h	O
(	O
1	O
)	O
	O
h	O
(	O
2	O
)	O
w	O
(	O
3	O
)	O
h	O
(	O
3	O
)	O
.	O
w	O
(	O
1	O
)	O
h	O
(	O
1	O
)	O
w	O
(	O
2	O
)	O
h	O
(	O
2	O
)	O
−	O
−	O
663	O
(	O
20.25	O
)	O
chapter	O
20.	O
deep	O
generative	O
models	O
h	O
(	O
3	O
)	O
1h	O
(	O
3	O
)	O
1	O
h	O
(	O
3	O
)	O
2h	O
(	O
3	O
)	O
2	O
h	O
(	O
2	O
)	O
h	O
(	O
2	O
)	O
1	O
1	O
h	O
(	O
2	O
)	O
h	O
(	O
2	O
)	O
2	O
2	O
h	O
(	O
2	O
)	O
3h	O
(	O
2	O
)	O
3	O
h	O
(	O
1	O
)	O
1h	O
(	O
1	O
)	O
1	O
h	O
(	O
1	O
)	O
2h	O
(	O
1	O
)	O
2	O
h	O
(	O
1	O
)	O
h	O
(	O
1	O
)	O
3	O
3	O
v1v1	O
v2v2	O
h	O
(	O
2	O
)	O
1h	O
(	O
2	O
)	O
1	O
h	O
(	O
2	O
)	O
2h	O
(	O
2	O
)	O
2	O
h	O
(	O
2	O
)	O
h	O
(	O
2	O
)	O
3	O
3	O
v	O
1	O
v	O
2	O
h	O
(	O
3	O
)	O
1h	O
(	O
3	O
)	O
1	O
h	O
(	O
3	O
)	O
2h	O
(	O
3	O
)	O
2	O
1h	O
(	O
1	O
)	O
h	O
(	O
1	O
)	O
1	O
h	O
(	O
1	O
)	O
2h	O
(	O
1	O
)	O
2	O
h	O
(	O
1	O
)	O
3h	O
(	O
1	O
)	O
3	O
figure	O
20.3	O
:	O
a	O
deep	O
boltzmann	O
machine	O
,	O
re-arranged	O
to	O
reveal	O
its	O
bipartite	O
graph	O
structure	O
.	O
in	O
comparison	O
to	O
the	O
rbm	O
energy	O
function	O
(	O
equation	O
)	O
,	O
the	O
dbm	O
energy	O
function	O
includes	O
connections	O
between	O
the	O
hidden	O
units	O
(	O
latent	O
variables	O
)	O
in	O
the	O
form	O
of	O
the	O
weight	O
matrices	O
(	O
w	O
(	O
2	O
)	O
and	O
w	O
(	O
3	O
)	O
)	O
.	O
as	O
we	O
will	O
see	O
,	O
these	O
connections	O
have	O
signiﬁcant	O
consequences	O
for	O
both	O
the	O
model	B
behavior	O
as	O
well	O
as	O
how	O
we	O
go	O
about	O
performing	O
inference	O
in	O
the	O
model	B
.	O
20.5	O
in	O
comparison	O
to	O
fully	O
connected	O
boltzmann	O
machines	O
(	O
with	O
every	O
unit	O
con-	O
nected	O
to	O
every	O
other	O
unit	O
)	O
,	O
the	O
dbm	O
oﬀers	O
some	O
advantages	O
that	O
are	O
similar	O
to	O
those	O
oﬀered	O
by	O
the	O
rbm	O
.	O
speciﬁcally	O
,	O
as	O
illustrated	O
in	O
ﬁgure	O
,	O
the	O
dbm	O
layers	O
can	O
be	O
organized	O
into	O
a	O
bipartite	O
graph	O
,	O
with	O
odd	O
layers	O
on	O
one	O
side	O
and	O
even	O
layers	O
on	O
the	O
other	O
.	O
this	O
immediately	O
implies	O
that	O
when	O
we	O
condition	O
on	O
the	O
variables	O
in	O
the	O
even	O
layer	O
,	O
the	O
variables	O
in	O
the	O
odd	O
layers	O
become	O
conditionally	O
independent	O
.	O
of	O
course	O
,	O
when	O
we	O
condition	O
on	O
the	O
variables	O
in	O
the	O
odd	O
layers	O
,	O
the	O
variables	O
in	O
the	O
even	O
layers	O
also	O
become	O
conditionally	O
independent	O
.	O
20.3	O
the	O
bipartite	O
structure	O
of	O
the	O
dbm	O
means	O
that	O
we	O
can	O
apply	O
the	O
same	O
equa-	O
tions	O
we	O
have	O
previously	O
used	O
for	O
the	O
conditional	O
distributions	O
of	O
an	O
rbm	O
to	O
determine	O
the	O
conditional	O
distributions	O
in	O
a	O
dbm	O
.	O
the	O
units	O
within	O
a	O
layer	O
are	O
conditionally	O
independent	O
from	O
each	O
other	O
given	O
the	O
values	O
of	O
the	O
neighboring	O
layers	O
,	O
so	O
the	O
distributions	O
over	O
binary	O
variables	O
can	O
be	O
fully	O
described	O
by	O
the	O
bernoulli	O
parameters	O
giving	O
the	O
probability	O
of	O
each	O
unit	O
being	O
active	O
.	O
in	O
our	O
example	O
with	O
two	O
hidden	O
layers	O
,	O
the	O
activation	O
probabilities	O
are	O
given	O
by	O
:	O
	O
	O
|	O
p	O
v	O
(	O
i	O
=	O
1	O
h	O
(	O
1	O
)	O
)	O
=	O
σ	O
w	O
(	O
1	O
)	O
i	O
,	O
:	O
h	O
(	O
1	O
)	O
,	O
(	O
20.26	O
)	O
664	O
chapter	O
20.	O
deep	O
generative	O
models	O
	O
and	O
|	O
=	O
1	O
p	O
h	O
(	O
(	O
1	O
)	O
i	O
p	O
h	O
(	O
(	O
2	O
)	O
k	O
v	O
v	O
h	O
,	O
(	O
2	O
)	O
)	O
=	O
σ	O
|	O
=	O
1	O
h	O
(	O
1	O
)	O
)	O
=	O
σ	O
	O
	O
	O
i	O
,	O
:	O
h	O
(	O
2	O
)	O
w	O
(	O
1	O
)	O
:	O
,i	O
+	O
w	O
(	O
2	O
)	O
	O
h	O
(	O
1	O
)	O
w	O
(	O
2	O
)	O
:	O
,k	O
.	O
	O
(	O
20.27	O
)	O
(	O
20.28	O
)	O
the	O
bipartite	O
structure	O
makes	O
gibbs	O
sampling	O
in	O
a	O
deep	O
boltzmann	O
machine	O
eﬃcient	O
.	O
the	O
naive	O
approach	O
to	O
gibbs	O
sampling	O
is	O
to	O
update	O
only	O
one	O
variable	O
at	O
a	O
time	O
.	O
rbms	O
allow	O
all	O
of	O
the	O
visible	O
units	O
to	O
be	O
updated	O
in	O
one	O
block	O
and	O
all	O
of	O
the	O
hidden	O
units	O
to	O
be	O
updated	O
in	O
a	O
second	O
block	O
.	O
one	O
might	O
naively	O
assume	O
that	O
a	O
dbm	O
with	O
l	O
layers	O
requires	O
l	O
+	O
1	O
updates	O
,	O
with	O
each	O
iteration	O
updating	O
a	O
block	O
consisting	O
of	O
one	O
layer	O
of	O
units	O
.	O
instead	O
,	O
it	O
is	O
possible	O
to	O
update	O
all	O
of	O
the	O
units	O
in	O
only	O
two	O
iterations	O
.	O
gibbs	O
sampling	O
can	O
be	O
divided	O
into	O
two	O
blocks	O
of	O
updates	O
,	O
one	O
including	O
all	O
even	O
layers	O
(	O
including	O
the	O
visible	O
layer	O
)	O
and	O
the	O
other	O
including	O
all	O
odd	O
layers	O
.	O
due	O
to	O
the	O
bipartite	O
dbm	O
connection	O
pattern	O
,	O
given	O
the	O
even	O
layers	O
,	O
the	O
distribution	O
over	O
the	O
odd	O
layers	O
is	O
factorial	O
and	O
thus	O
can	O
be	O
sampled	O
simultaneously	O
and	O
independently	O
as	O
a	O
block	O
.	O
likewise	O
,	O
given	O
the	O
odd	O
layers	O
,	O
the	O
even	O
layers	O
can	O
be	O
sampled	O
simultaneously	O
and	O
independently	O
as	O
a	O
block	O
.	O
eﬃcient	O
sampling	O
is	O
especially	O
important	O
for	O
training	O
with	O
the	O
stochastic	O
maximum	O
likelihood	O
algorithm	O
.	O
20.4.1	O
interesting	O
properties	O
deep	O
boltzmann	O
machines	O
have	O
many	O
interesting	O
properties	O
.	O
|	O
dbms	O
were	O
developed	O
after	O
dbns	O
.	O
compared	O
to	O
dbns	O
,	O
the	O
posterior	O
distribu-	O
)	O
is	O
simpler	O
for	O
dbms	O
.	O
somewhat	O
counterintuitively	O
,	O
the	O
simplicity	O
of	O
tion	B
p	O
(	O
h	O
v	O
this	O
posterior	O
distribution	O
allows	O
richer	O
approximations	O
of	O
the	O
posterior	O
.	O
in	O
the	O
case	O
of	O
the	O
dbn	O
,	O
we	O
perform	O
classiﬁcation	O
using	O
a	O
heuristically	O
motivated	O
approximate	O
inference	O
procedure	O
,	O
in	O
which	O
we	O
guess	O
that	O
a	O
reasonable	O
value	O
for	O
the	O
mean	O
ﬁeld	O
expectation	O
of	O
the	O
hidden	O
units	O
can	O
be	O
provided	O
by	O
an	O
upward	O
pass	O
through	O
the	O
network	O
in	O
an	O
mlp	O
that	O
uses	O
sigmoid	O
activation	O
functions	O
and	O
the	O
same	O
weights	O
as	O
q	O
(	O
h	O
)	O
may	O
be	O
used	O
to	O
obtain	O
a	O
variational	O
lower	O
the	O
original	O
dbn	O
.	O
bound	B
on	O
the	O
log-likelihood	O
.	O
this	O
heuristic	O
procedure	O
therefore	O
allows	O
us	O
to	O
obtain	O
such	O
a	O
bound	B
.	O
however	O
,	O
the	O
bound	B
is	O
not	O
explicitly	O
optimized	O
in	O
any	O
way	O
,	O
so	O
the	O
bound	B
may	O
be	O
far	O
from	O
tight	O
.	O
in	O
particular	O
,	O
the	O
heuristic	O
estimate	O
of	O
q	O
ignores	O
interactions	O
between	O
hidden	O
units	O
within	O
the	O
same	O
layer	O
as	O
well	O
as	O
the	O
top-down	O
feedback	O
inﬂuence	O
of	O
hidden	O
units	O
in	O
deeper	O
layers	O
on	O
hidden	O
units	O
that	O
are	O
closer	O
to	O
the	O
input	O
.	O
because	O
the	O
heuristic	O
mlp-based	O
inference	O
procedure	O
in	O
the	O
dbn	O
is	O
not	O
able	O
to	O
account	O
for	O
these	O
interactions	O
,	O
the	O
resulting	O
q	O
is	O
presumably	O
far	O
distribution	O
any	O
665	O
chapter	O
20.	O
deep	O
generative	O
models	O
from	O
optimal	O
.	O
in	O
dbms	O
,	O
all	O
of	O
the	O
hidden	O
units	O
within	O
a	O
layer	O
are	O
conditionally	O
independent	O
given	O
the	O
other	O
layers	O
.	O
this	O
lack	O
of	O
intralayer	O
interaction	O
makes	O
it	O
possible	O
to	O
use	O
ﬁxed	O
point	O
equations	O
to	O
actually	O
optimize	O
the	O
variational	O
lower	O
bound	B
and	O
ﬁnd	O
the	O
true	O
optimal	O
mean	O
ﬁeld	O
expectations	O
(	O
to	O
within	O
some	O
numerical	O
tolerance	O
)	O
.	O
the	O
use	O
of	O
proper	O
mean	O
ﬁeld	O
allows	O
the	O
approximate	O
inference	O
procedure	O
for	O
dbms	O
to	O
capture	O
the	O
inﬂuence	O
of	O
top-down	O
feedback	O
interactions	O
.	O
this	O
makes	O
dbms	O
interesting	O
from	O
the	O
point	O
of	O
view	O
of	O
neuroscience	O
,	O
because	O
the	O
human	O
brain	O
is	O
known	O
to	O
use	O
many	O
top-down	O
feedback	O
connections	O
.	O
because	O
of	O
this	O
property	O
,	O
dbms	O
have	O
been	O
used	O
as	O
computational	O
models	O
of	O
real	O
neuroscientiﬁc	O
phenomena	O
(	O
series	O
et	O
al	O
.	O
2010	O
reichert	O
et	O
al.	O
,	O
2011	O
)	O
.	O
,	O
;	O
one	O
unfortunate	O
property	O
of	O
dbms	O
is	O
that	O
sampling	O
from	O
them	O
is	O
relatively	O
diﬃcult	O
.	O
dbns	O
only	O
need	O
to	O
use	O
mcmc	O
sampling	O
in	O
their	O
top	O
pair	O
of	O
layers	O
.	O
the	O
other	O
layers	O
are	O
used	O
only	O
at	O
the	O
end	O
of	O
the	O
sampling	O
process	O
,	O
in	O
one	O
eﬃcient	O
ancestral	O
sampling	O
pass	O
.	O
to	O
generate	O
a	O
sample	O
from	O
a	O
dbm	O
,	O
it	O
is	O
necessary	O
to	O
use	O
mcmc	O
across	O
all	O
layers	O
,	O
with	O
every	O
layer	O
of	O
the	O
model	B
participating	O
in	O
every	O
markov	O
chain	O
transition	O
.	O
20.4.2	O
dbm	O
mean	O
field	O
inference	O
|	O
|	O
(	O
1	O
)	O
)	O
,	O
p	O
(	O
h	O
(	O
1	O
)	O
the	O
conditional	O
distribution	O
over	O
one	O
dbm	O
layer	O
given	O
the	O
neighboring	O
layers	O
is	O
factorial	O
.	O
in	O
the	O
example	O
of	O
the	O
dbm	O
with	O
two	O
hidden	O
layers	O
,	O
these	O
distributions	O
h	O
(	O
1	O
)	O
)	O
.	O
the	O
distribution	O
over	O
all	O
are	O
p	O
(	O
v	O
h	O
hidden	O
layers	O
generally	O
does	O
not	O
factorize	O
because	O
of	O
interactions	O
between	O
layers	O
.	O
in	O
the	O
example	O
with	O
two	O
hidden	O
layers	O
,	O
p	O
(	O
h	O
(	O
1	O
)	O
,	O
h	O
(	O
2	O
)	O
v	O
)	O
does	O
not	O
factorize	O
due	O
due	O
to	O
the	O
interaction	O
weights	O
w	O
(	O
2	O
)	O
between	O
h	O
(	O
1	O
)	O
and	O
h	O
(	O
2	O
)	O
which	O
render	O
these	O
variables	O
mutually	O
dependent	O
.	O
(	O
2	O
)	O
)	O
and	O
p	O
(	O
h	O
(	O
2	O
)	O
v	O
h	O
,	O
|	O
|	O
as	O
was	O
the	O
case	O
with	O
the	O
dbn	O
,	O
we	O
are	O
left	O
to	O
seek	O
out	O
methods	O
to	O
approximate	O
the	O
dbm	O
posterior	O
distribution	O
.	O
however	O
,	O
unlike	O
the	O
dbn	O
,	O
the	O
dbm	O
posterior	O
distribution	O
over	O
their	O
hidden	O
units—while	O
complicated—is	O
easy	O
to	O
approximate	O
with	O
a	O
variational	O
approximation	O
(	O
as	O
discussed	O
in	O
section	O
)	O
,	O
speciﬁcally	O
a	O
mean	O
ﬁeld	O
approximation	O
.	O
the	O
mean	O
ﬁeld	O
approximation	O
is	O
a	O
simple	O
form	O
of	O
variational	O
inference	O
,	O
where	O
we	O
restrict	O
the	O
approximating	O
distribution	O
to	O
fully	O
factorial	O
distributions	O
.	O
in	O
the	O
context	O
of	O
dbms	O
,	O
the	O
mean	O
ﬁeld	O
equations	O
capture	O
the	O
bidirectional	O
interactions	O
between	O
layers	O
.	O
in	O
this	O
section	O
we	O
derive	O
the	O
iterative	O
approximate	O
inference	O
procedure	O
originally	O
introduced	O
in	O
salakhutdinov	O
and	O
hinton	O
(	O
2009a	O
19.4	O
)	O
.	O
in	O
variational	O
approximations	O
to	O
inference	O
,	O
we	O
approach	O
the	O
task	O
of	O
approxi-	O
666	O
chapter	O
20.	O
deep	O
generative	O
models	O
mating	O
a	O
particular	O
target	O
distribution—in	O
our	O
case	O
,	O
the	O
posterior	O
distribution	O
over	O
the	O
hidden	O
units	O
given	O
the	O
visible	O
units—by	O
some	O
reasonably	O
simple	O
family	O
of	O
dis-	O
tributions	O
.	O
in	O
the	O
case	O
of	O
the	O
mean	O
ﬁeld	O
approximation	O
,	O
the	O
approximating	O
family	O
is	O
the	O
set	O
of	O
distributions	O
where	O
the	O
hidden	O
units	O
are	O
conditionally	O
independent	O
.	O
we	O
now	O
develop	O
the	O
mean	O
ﬁeld	O
approach	O
for	O
the	O
example	O
with	O
two	O
hidden	O
v	O
)	O
.	O
the	O
mean	O
v	O
)	O
be	O
the	O
approximation	O
of	O
p	O
(	O
h	O
(	O
1	O
)	O
,	O
h	O
(	O
2	O
)	O
|	O
|	O
layers	O
.	O
let	O
q	O
(	O
h	O
(	O
1	O
)	O
,	O
h	O
(	O
2	O
)	O
ﬁeld	O
assumption	O
implies	O
that	O
	O
	O
q	O
(	O
h	O
(	O
1	O
)	O
,	O
h	O
(	O
2	O
)	O
|	O
v	O
)	O
=	O
q	O
h	O
(	O
(	O
1	O
)	O
j	O
|	O
v	O
)	O
|	O
v	O
)	O
.	O
q	O
h	O
(	O
(	O
2	O
)	O
k	O
j	O
k	O
(	O
20.29	O
)	O
the	O
mean	O
ﬁeld	O
approximation	O
attempts	O
to	O
ﬁnd	O
a	O
member	O
of	O
this	O
family	O
of	O
distributions	O
that	O
best	O
ﬁts	O
the	O
true	O
posterior	O
p	O
(	O
h	O
(	O
1	O
)	O
,	O
h	O
(	O
2	O
)	O
v	O
)	O
.	O
importantly	O
,	O
the	O
inference	O
process	O
must	O
be	O
run	O
again	O
to	O
ﬁnd	O
a	O
diﬀerent	O
distribution	O
q	O
every	O
time	O
we	O
use	O
a	O
new	O
value	O
of	O
|	O
one	O
can	O
conceive	O
of	O
many	O
ways	O
of	O
measuring	O
how	O
well	O
q	O
(	O
h	O
v	O
|	O
)	O
ﬁts	O
p	O
(	O
h	O
v	O
	O
	O
.v	O
)	O
.	O
	O
|	O
the	O
mean	O
ﬁeld	O
approach	O
is	O
to	O
minimize	O
|	O
kl	O
(	O
)	O
=	O
q	O
(	O
h	O
(	O
1	O
)	O
,	O
h	O
(	O
2	O
)	O
v	O
)	O
log	O
	O
q	O
p	O
h	O
q	O
(	O
h	O
(	O
1	O
)	O
,	O
h	O
(	O
2	O
)	O
p	O
(	O
h	O
(	O
1	O
)	O
,	O
h	O
(	O
2	O
)	O
|	O
|	O
v	O
)	O
v	O
)	O
.	O
(	O
20.30	O
)	O
in	O
general	O
,	O
we	O
do	O
not	O
have	O
to	O
provide	O
a	O
parametric	O
form	O
of	O
the	O
approximating	O
distribution	O
beyond	O
enforcing	O
the	O
independence	O
assumptions	O
.	O
the	O
variational	O
approximation	O
procedure	O
is	O
generally	O
able	O
to	O
recover	O
a	O
functional	O
form	O
of	O
the	O
approximate	O
distribution	O
.	O
however	O
,	O
in	O
the	O
case	O
of	O
a	O
mean	O
ﬁeld	O
assumption	O
on	O
binary	O
hidden	O
units	O
(	O
the	O
case	O
we	O
are	O
developing	O
here	O
)	O
there	O
is	O
no	O
loss	O
of	O
generality	O
resulting	O
from	O
ﬁxing	O
a	O
parametrization	O
of	O
the	O
model	B
in	O
advance	O
.	O
|	O
we	O
parametrize	O
q	O
as	O
a	O
product	O
of	O
bernoulli	O
distributions	O
,	O
that	O
is	O
we	O
associate	O
the	O
probability	O
of	O
each	O
element	O
of	O
h	O
(	O
1	O
)	O
with	O
a	O
parameter	O
.	O
speciﬁcally	O
,	O
for	O
each	O
j	O
,	O
ˆh	O
(	O
1	O
)	O
j	O
=	O
q	O
(	O
h	O
(	O
1	O
)	O
v	O
)	O
,	O
j	O
=	O
1	O
∈	O
where	O
ˆh	O
(	O
2	O
)	O
[	O
0	O
1	O
]	O
k	O
|	O
.	O
thus	O
we	O
have	O
the	O
following	O
approximation	O
to	O
the	O
posterior	O
:	O
[	O
0	O
,	O
1	O
]	O
and	O
for	O
each	O
k	O
,	O
ˆh	O
(	O
2	O
)	O
k	O
=	O
q	O
(	O
h	O
(	O
2	O
)	O
v	O
)	O
,	O
where	O
ˆh	O
(	O
1	O
)	O
	O
k	O
=	O
1	O
	O
	O
	O
∈	O
|	O
|	O
|	O
,	O
j	O
q	O
(	O
h	O
(	O
1	O
)	O
,	O
h	O
(	O
2	O
)	O
v	O
)	O
=	O
q	O
h	O
(	O
(	O
1	O
)	O
j	O
v	O
)	O
q	O
h	O
(	O
(	O
2	O
)	O
k	O
v	O
)	O
(	O
20.31	O
)	O
=	O
j	O
j	O
(	O
ˆh	O
(	O
1	O
)	O
j	O
)	O
h	O
(	O
1	O
)	O
j	O
(	O
1	O
k	O
−	O
ˆh	O
(	O
1	O
)	O
−	O
h	O
(	O
1	O
)	O
j	O
)	O
(	O
1	O
j	O
)	O
×	O
k	O
(	O
ˆh	O
(	O
2	O
)	O
k	O
)	O
h	O
(	O
2	O
)	O
k	O
(	O
1	O
−	O
ˆh	O
(	O
2	O
)	O
k	O
−	O
h	O
(	O
2	O
)	O
)	O
(	O
1	O
)	O
.	O
k	O
(	O
20.32	O
)	O
of	O
course	O
,	O
for	O
dbms	O
with	O
more	O
layers	O
the	O
approximate	O
posterior	O
parametrization	O
can	O
be	O
extended	O
in	O
the	O
obvious	O
way	O
,	O
exploiting	O
the	O
bipartite	O
structure	O
of	O
the	O
graph	O
667	O
chapter	O
20.	O
deep	O
generative	O
models	O
to	O
update	O
all	O
of	O
the	O
even	O
layers	O
simultaneously	O
and	O
then	O
to	O
update	O
all	O
of	O
the	O
odd	O
layers	O
simultaneously	O
,	O
following	O
the	O
same	O
schedule	O
as	O
gibbs	O
sampling	O
.	O
now	O
that	O
we	O
have	O
speciﬁed	O
our	O
family	O
of	O
approximating	O
distributions	O
q	O
,	O
it	O
remains	O
to	O
specify	O
a	O
procedure	O
for	O
choosing	O
the	O
member	O
of	O
this	O
family	O
that	O
best	O
ﬁts	O
p.	O
the	O
most	O
straightforward	O
way	O
to	O
do	O
this	O
is	O
to	O
use	O
the	O
mean	O
ﬁeld	O
equations	O
speciﬁed	O
by	O
equation	O
.	O
these	O
equations	O
were	O
derived	O
by	O
solving	O
for	O
where	O
the	O
derivatives	O
of	O
the	O
variational	O
lower	O
bound	B
are	O
zero	O
.	O
they	O
describe	O
in	O
an	O
abstract	O
manner	O
how	O
to	O
optimize	O
the	O
variational	O
lower	O
bound	B
for	O
any	O
model	B
,	O
simply	O
by	O
taking	O
expectations	O
with	O
respect	O
to	O
19.56	O
.q	O
applying	O
these	O
general	O
equations	O
,	O
we	O
obtain	O
the	O
update	O
rules	O
(	O
again	O
,	O
ignoring	O
	O
bias	O
terms	O
)	O
:	O
ˆh	O
(	O
1	O
)	O
j	O
=	O
σ	O
viw	O
(	O
1	O
)	O
i	O
,	O
j	O
+	O
w	O
(	O
2	O
)	O
j	O
,	O
k	O
	O
ˆh	O
(	O
2	O
)	O
	O
k	O
,	O
∀	O
j	O
	O
	O
i	O
	O
	O
	O
k	O
ˆh	O
(	O
2	O
)	O
k	O
=	O
σ	O
w	O
(	O
2	O
)	O
	O
,	O
k	O
j	O
ˆh	O
(	O
1	O
)	O
	O
j	O
,	O
	O
j	O
∀	O
k.	O
(	O
20.33	O
)	O
(	O
20.34	O
)	O
l	O
at	O
a	O
ﬁxed	O
point	O
of	O
this	O
system	O
of	O
equations	O
,	O
we	O
have	O
a	O
local	O
maximum	O
of	O
the	O
(	O
q	O
)	O
.	O
thus	O
these	O
ﬁxed	O
point	O
update	O
equations	O
deﬁne	O
an	O
variational	O
lower	O
bound	B
iterative	O
algorithm	O
where	O
we	O
alternate	O
updates	O
of	O
ˆh	O
(	O
1	O
)	O
)	O
and	O
updates	O
of	O
ˆh	O
(	O
2	O
)	O
)	O
.	O
on	O
small	O
problems	O
such	O
as	O
mnist	O
,	O
as	O
few	O
k	O
as	O
ten	O
iterations	O
can	O
be	O
suﬃcient	O
to	O
ﬁnd	O
an	O
approximate	O
positive	O
phase	O
gradient	O
for	O
learning	O
,	O
and	O
ﬁfty	O
usually	O
suﬃce	O
to	O
obtain	O
a	O
high	O
quality	O
representation	O
of	O
a	O
single	O
speciﬁc	O
example	O
to	O
be	O
used	O
for	O
high-accuracy	O
classiﬁcation	O
.	O
extending	O
approximate	O
variational	O
inference	O
to	O
deeper	O
dbms	O
is	O
straightforward	O
.	O
(	O
using	O
equation	O
(	O
using	O
equation	O
20.33	O
20.34	O
j	O
20.4.3	O
dbm	O
parameter	O
learning	O
learning	O
in	O
the	O
dbm	O
must	O
confront	O
both	O
the	O
challenge	O
of	O
an	O
intractable	O
partition	O
function	O
,	O
using	O
the	O
techniques	O
from	O
chapter	O
,	O
and	O
the	O
challenge	O
of	O
an	O
intractable	O
posterior	O
distribution	O
,	O
using	O
the	O
techniques	O
from	O
chapter	O
.19	O
18	O
as	O
described	O
in	O
section	O
l	O
|	O
a	O
distribution	O
q	O
(	O
h	O
v	O
proceeds	O
by	O
maximizing	O
log-likelihood	O
,	O
log	O
(	O
;	O
p	O
v	O
θ	O
(	O
v	O
.	O
)	O
20.4.2	O
|	O
)	O
that	O
approximates	O
the	O
intractable	O
p	O
(	O
h	O
v	O
,	O
variational	O
inference	O
allows	O
the	O
construction	O
of	O
)	O
.	O
learning	O
then	O
)	O
,	O
the	O
variational	O
lower	O
bound	B
on	O
the	O
intractable	O
,	O
q	O
,	O
θ	O
668	O
chapter	O
20.	O
deep	O
generative	O
models	O
	O
	O
	O
	O
for	O
a	O
deep	O
boltzmann	O
machine	O
with	O
two	O
hidden	O
layers	O
,	O
is	O
given	O
by	O
l	O
−	O
h	O
l	O
(	O
q	O
,	O
θ	O
)	O
=	O
	O
j	O
i	O
vi	O
w	O
(	O
1	O
)	O
i	O
,	O
j	O
	O
ˆh	O
(	O
1	O
)	O
	O
+	O
j	O
ˆh	O
(	O
1	O
)	O
j	O
	O
ˆh	O
(	O
2	O
)	O
	O
w	O
(	O
2	O
)	O
	O
k	O
,	O
k	O
	O
j	O
	O
j	O
	O
k	O
log	O
(	O
)	O
+	O
z	O
θ	O
)	O
q	O
.	O
(	O
20.35	O
)	O
(	O
this	O
expression	O
still	O
contains	O
the	O
log	O
partition	O
function	O
,	O
log	O
z	O
(	O
θ	O
)	O
.	O
because	O
a	O
deep	O
boltzmann	O
machine	O
contains	O
restricted	O
boltzmann	O
machines	O
as	O
components	O
,	O
the	O
hardness	O
results	O
for	O
computing	O
the	O
partition	O
function	O
and	O
sampling	O
that	O
apply	O
to	O
restricted	O
boltzmann	O
machines	O
also	O
apply	O
to	O
deep	O
boltzmann	O
machines	O
.	O
this	O
means	O
that	O
evaluating	O
the	O
probability	O
mass	O
function	O
of	O
a	O
boltzmann	O
machine	O
requires	O
approximate	O
methods	O
such	O
as	O
annealed	O
importance	O
sampling	O
.	O
likewise	O
,	O
training	O
the	O
model	B
requires	O
approximations	O
to	O
the	O
gradient	O
of	O
the	O
log	O
partition	O
function	O
.	O
see	O
chapter	O
for	O
a	O
general	O
description	O
of	O
these	O
methods	O
.	O
dbms	O
are	O
typically	O
trained	O
using	O
stochastic	O
maximum	O
likelihood	O
.	O
many	O
of	O
the	O
other	O
techniques	O
described	O
in	O
chapter	O
are	O
not	O
applicable	O
.	O
techniques	O
such	O
as	O
pseudolikelihood	O
require	O
the	O
ability	O
to	O
evaluate	O
the	O
unnormalized	O
probabilities	O
,	O
rather	O
than	O
merely	O
obtain	O
a	O
variational	O
lower	O
bound	B
on	O
them	O
.	O
contrastive	O
divergence	O
is	O
slow	O
for	O
deep	O
boltzmann	O
machines	O
because	O
they	O
do	O
not	O
allow	O
eﬃcient	O
sampling	O
of	O
the	O
hidden	O
units	O
given	O
the	O
visible	O
units—instead	O
,	O
contrastive	O
divergence	O
would	O
require	O
burning	O
in	O
a	O
markov	O
chain	O
every	O
time	O
a	O
new	O
negative	O
phase	O
sample	O
is	O
needed	O
.	O
18	O
18	O
the	O
non-variational	O
version	O
of	O
stochastic	O
maximum	O
likelihood	O
algorithm	O
was	O
.	O
variational	O
stochastic	O
maximum	O
likelihood	O
as	O
.	O
recall	O
that	O
we	O
describe	O
a	O
simpliﬁed	O
discussed	O
earlier	O
,	O
in	O
section	O
applied	O
to	O
the	O
dbm	O
is	O
given	O
in	O
algorithm	O
varient	O
of	O
the	O
dbm	O
that	O
lacks	O
bias	O
parameters	O
;	O
including	O
them	O
is	O
trivial	O
.	O
18.2	O
20.1	O
20.4.4	O
layer-wise	O
pretraining	O
unfortunately	O
,	O
training	O
a	O
dbm	O
using	O
stochastic	O
maximum	O
likelihood	O
(	O
as	O
described	O
above	O
)	O
from	O
a	O
random	O
initialization	O
usually	O
results	O
in	O
failure	O
.	O
in	O
some	O
cases	O
,	O
the	O
model	B
fails	O
to	O
learn	O
to	O
represent	O
the	O
distribution	O
adequately	O
.	O
in	O
other	O
cases	O
,	O
the	O
dbm	O
may	O
represent	O
the	O
distribution	O
well	O
,	O
but	O
with	O
no	O
higher	O
likelihood	O
than	O
could	O
be	O
obtained	O
with	O
just	O
an	O
rbm	O
.	O
a	O
dbm	O
with	O
very	O
small	O
weights	O
in	O
all	O
but	O
the	O
ﬁrst	O
layer	O
represents	O
approximately	O
the	O
same	O
distribution	O
as	O
an	O
rbm	O
.	O
20.4.5	O
various	O
techniques	O
that	O
permit	O
joint	O
training	O
have	O
been	O
developed	O
and	O
are	O
.	O
however	O
,	O
the	O
original	O
and	O
most	O
popular	O
method	O
for	O
described	O
in	O
section	O
overcoming	O
the	O
joint	O
training	O
problem	O
of	O
dbms	O
is	O
greedy	O
layer-wise	O
pretraining	O
.	O
in	O
this	O
method	O
,	O
each	O
layer	O
of	O
the	O
dbm	O
is	O
trained	O
in	O
isolation	O
as	O
an	O
rbm	O
.	O
the	O
ﬁrst	O
layer	O
is	O
trained	O
to	O
model	B
the	O
input	O
data	O
.	O
each	O
subsequent	O
rbm	O
is	O
trained	O
to	O
model	B
samples	O
from	O
the	O
previous	O
rbm	O
’	O
s	O
posterior	O
distribution	O
.	O
after	O
all	O
of	O
the	O
669	O
chapter	O
20.	O
deep	O
generative	O
models	O
algorithm	O
20.1	O
the	O
variational	O
stochastic	O
maximum	O
likelihood	O
algorithm	O
for	O
training	O
a	O
dbm	O
with	O
two	O
hidden	O
layers	O
.	O
	O
(	O
1	O
)	O
,	O
h	O
(	O
2	O
)	O
;	O
θ	O
+	O
∆θ	O
)	O
to	O
burn	O
in	O
,	O
starting	O
from	O
samples	O
from	O
p	O
(	O
v	O
h	O
,	O
set	O
,	O
the	O
step	O
size	O
,	O
to	O
a	O
small	O
positive	O
number	O
set	O
k	O
,	O
the	O
number	O
of	O
gibbs	O
steps	O
,	O
high	O
enough	O
to	O
allow	O
a	O
markov	O
chain	O
of	O
(	O
1	O
)	O
,	O
h	O
(	O
2	O
)	O
;	O
θ	O
)	O
.	O
p	O
(	O
v	O
h	O
,	O
initialize	O
three	O
matrices	O
,	O
˜v	O
,	O
˜h	O
(	O
1	O
)	O
and	O
˜h	O
(	O
2	O
)	O
each	O
with	O
m	O
rows	O
set	O
to	O
random	O
values	O
(	O
e.g.	O
,	O
from	O
bernoulli	O
distributions	O
,	O
possibly	O
with	O
marginals	O
matched	O
to	O
the	O
model	B
’	O
s	O
marginals	O
)	O
.	O
while	O
not	O
converged	O
(	O
learning	O
loop	O
)	O
do	O
sample	O
a	O
minibatch	O
of	O
m	O
examples	O
from	O
the	O
training	O
data	O
and	O
arrange	O
them	O
as	O
the	O
rows	O
of	O
a	O
design	O
matrix	O
initialize	O
matrices	O
ˆh	O
(	O
1	O
)	O
and	O
ˆh	O
(	O
2	O
)	O
,	O
possibly	O
to	O
the	O
model	B
’	O
s	O
marginals	O
.	O
while	O
not	O
converged	O
(	O
mean	O
ﬁeld	O
inference	O
loop	O
)	O
do	O
	O
.v	O
	O
	O
←	O
←	O
σ	O
σ	O
ˆh	O
(	O
1	O
)	O
ˆh	O
(	O
2	O
)	O
	O
v	O
w	O
(	O
1	O
)	O
+	O
ˆh	O
(	O
2	O
)	O
w	O
(	O
2	O
)	O
ˆh	O
(	O
1	O
)	O
w	O
(	O
2	O
)	O
	O
.	O
l	O
	O
w	O
(	O
1	O
)	O
w	O
(	O
2	O
)	O
ˆh	O
(	O
2	O
)	O
ˆh	O
(	O
1	O
)	O
v	O
ˆh	O
(	O
1	O
)	O
k	O
(	O
gibbs	O
sampling	O
)	O
←	O
end	O
while	O
←	O
1	O
∆	O
m	O
1	O
∆	O
m	O
=	O
1	O
to	O
for	O
gibbs	O
block	O
1	O
:	O
∀	O
i	O
,	O
j	O
,	O
˜v	O
i	O
,	O
j	O
sampled	O
from	O
p	O
(	O
˜vi	O
,	O
j	O
=	O
1	O
)	O
=	O
σ	O
∀	O
i	O
,	O
j	O
,	O
˜h	O
(	O
2	O
)	O
∀	O
gibbs	O
block	O
2	O
:	O
i	O
,	O
j	O
,	O
˜h	O
(	O
1	O
)	O
i	O
,	O
j	O
sampled	O
from	O
p	O
(	O
˜h	O
(	O
2	O
)	O
i	O
,	O
j	O
sampled	O
from	O
p	O
(	O
˜h	O
(	O
1	O
)	O
do	O
	O
	O
	O
	O
	O
	O
	O
	O
˜h	O
(	O
1	O
)	O
i	O
,	O
:	O
.	O
w	O
(	O
1	O
)	O
j	O
,	O
:	O
˜h	O
(	O
1	O
)	O
	O
.	O
670	O
i	O
,	O
j	O
=	O
1	O
)	O
=	O
σ	O
i	O
,	O
:	O
w	O
(	O
2	O
)	O
:	O
,j	O
.	O
i	O
,	O
j	O
=	O
1	O
)	O
=	O
σ	O
˜vi	O
,	O
:w	O
(	O
1	O
)	O
:	O
,j	O
+	O
˜h	O
(	O
2	O
)	O
	O
i	O
,	O
:	O
w	O
(	O
2	O
)	O
j	O
,	O
:	O
	O
.	O
	O
w	O
(	O
1	O
)	O
w	O
(	O
1	O
)	O
−	O
−	O
←	O
end	O
for	O
←	O
∆	O
←	O
∆	O
w	O
(	O
2	O
)	O
w	O
(	O
1	O
)	O
←	O
eﬀective	O
algorithm	O
,	O
such	O
as	O
momentum	O
with	O
a	O
decaying	O
learning	O
rate	O
)	O
w	O
(	O
2	O
)	O
˜h	O
(	O
1	O
)	O
1	O
v	O
∆	O
	O
m	O
˜h	O
(	O
1	O
)	O
1	O
∆	O
m	O
w	O
(	O
1	O
)	O
+	O
∆	O
w	O
(	O
1	O
)	O
(	O
this	O
is	O
a	O
cartoon	O
illustration	O
,	O
in	O
practice	O
use	O
a	O
more	O
w	O
(	O
2	O
)	O
+	O
∆	O
˜h	O
(	O
2	O
)	O
w	O
(	O
2	O
)	O
w	O
(	O
2	O
)	O
end	O
while	O
chapter	O
20.	O
deep	O
generative	O
models	O
rbms	O
have	O
been	O
trained	O
in	O
this	O
way	O
,	O
they	O
can	O
be	O
combined	O
to	O
form	O
a	O
dbm	O
.	O
the	O
dbm	O
may	O
then	O
be	O
trained	O
with	O
pcd	O
.	O
typically	O
pcd	O
training	O
will	O
make	O
only	O
a	O
small	O
change	O
in	O
the	O
model	B
’	O
s	O
parameters	O
and	O
its	O
performance	O
as	O
measured	O
by	O
the	O
log-likelihood	O
it	O
assigns	O
to	O
the	O
data	O
,	O
or	O
its	O
ability	O
to	O
classify	O
inputs	O
.	O
see	O
ﬁgure	O
20.4	O
for	O
an	O
illustration	O
of	O
the	O
training	O
procedure	O
.	O
this	O
greedy	O
layer-wise	O
training	O
procedure	O
is	O
not	O
just	O
coordinate	O
ascent	O
.	O
it	O
bears	O
some	O
passing	O
resemblance	O
to	O
coordinate	O
ascent	O
because	O
we	O
optimize	O
one	O
subset	O
of	O
the	O
parameters	O
at	O
each	O
step	O
.	O
the	O
two	O
methods	O
diﬀer	O
because	O
the	O
greedy	O
layer-wise	O
training	O
procedure	O
uses	O
a	O
diﬀerent	O
objective	O
function	O
at	O
each	O
step	O
.	O
greedy	O
layer-wise	O
pretraining	O
of	O
a	O
dbm	O
diﬀers	O
from	O
greedy	O
layer-wise	O
pre-	O
training	O
of	O
a	O
dbn	O
.	O
the	O
parameters	O
of	O
each	O
individual	O
rbm	O
may	O
be	O
copied	O
to	O
the	O
corresponding	O
dbn	O
directly	O
.	O
in	O
the	O
case	O
of	O
the	O
dbm	O
,	O
the	O
rbm	O
parameters	O
must	O
be	O
modiﬁed	O
before	O
inclusion	O
in	O
the	O
dbm	O
.	O
a	O
layer	O
in	O
the	O
middle	O
of	O
the	O
stack	O
of	O
rbms	O
is	O
trained	O
with	O
only	O
bottom-up	O
input	O
,	O
but	O
after	O
the	O
stack	O
is	O
combined	O
to	O
form	O
the	O
dbm	O
,	O
the	O
layer	O
will	O
have	O
both	O
bottom-up	O
and	O
top-down	O
input	O
.	O
to	O
account	O
for	O
this	O
eﬀect	O
,	O
salakhutdinov	O
and	O
hinton	O
2009a	O
)	O
advocate	O
dividing	O
the	O
weights	O
of	O
all	O
but	O
the	O
top	O
and	O
bottom	O
rbm	O
in	O
half	O
before	O
inserting	O
them	O
into	O
the	O
dbm	O
.	O
additionally	O
,	O
the	O
bottom	O
rbm	O
must	O
be	O
trained	O
using	O
two	O
“	O
copies	O
”	O
of	O
each	O
visible	O
unit	O
and	O
the	O
weights	O
tied	O
to	O
be	O
equal	O
between	O
the	O
two	O
copies	O
.	O
this	O
means	O
that	O
the	O
weights	O
are	O
eﬀectively	O
doubled	O
during	O
the	O
upward	O
pass	O
.	O
similarly	O
,	O
the	O
top	O
rbm	O
should	O
be	O
trained	O
with	O
two	O
copies	O
of	O
the	O
topmost	O
layer	O
.	O
(	O
obtaining	O
the	O
state	O
of	O
the	O
art	O
results	O
with	O
the	O
deep	O
boltzmann	O
machine	O
requires	O
a	O
modiﬁcation	O
of	O
the	O
standard	O
sml	O
algorithm	O
,	O
which	O
is	O
to	O
use	O
a	O
small	O
amount	O
of	O
mean	O
ﬁeld	O
during	O
the	O
negative	O
phase	O
of	O
the	O
joint	O
pcd	O
training	O
step	O
(	O
salakhutdinov	O
and	O
hinton	O
2009a	O
)	O
.	O
speciﬁcally	O
,	O
the	O
expectation	O
of	O
the	O
energy	O
gradient	O
should	O
be	O
computed	O
with	O
respect	O
to	O
the	O
mean	O
ﬁeld	O
distribution	O
in	O
which	O
all	O
of	O
the	O
units	O
are	O
independent	O
from	O
each	O
other	O
.	O
the	O
parameters	O
of	O
this	O
mean	O
ﬁeld	O
distribution	O
should	O
be	O
obtained	O
by	O
running	O
the	O
mean	O
ﬁeld	O
ﬁxed	O
point	O
equations	O
for	O
just	O
one	O
step	O
.	O
see	O
)	O
for	O
a	O
comparison	O
of	O
the	O
performance	O
of	O
centered	O
dbms	O
with	O
and	O
without	O
the	O
use	O
of	O
partial	O
mean	O
ﬁeld	O
in	O
the	O
negative	O
phase	O
.	O
goodfellow	O
et	O
al	O
.	O
2013b	O
,	O
(	O
20.4.5	O
jointly	O
training	O
deep	O
boltzmann	O
machines	O
classic	O
dbms	O
require	O
greedy	O
unsupervised	O
pretraining	O
,	O
and	O
to	O
perform	O
classiﬁcation	O
well	O
,	O
require	O
a	O
separate	O
mlp-based	O
classiﬁer	O
on	O
top	O
of	O
the	O
hidden	O
features	O
they	O
extract	O
.	O
this	O
has	O
some	O
undesirable	O
properties	O
.	O
it	O
is	O
hard	O
to	O
track	O
performance	O
during	O
training	O
because	O
we	O
can	O
not	O
evaluate	O
properties	O
of	O
the	O
full	O
dbm	O
while	O
training	O
the	O
ﬁrst	O
rbm	O
.	O
thus	O
,	O
it	O
is	O
hard	O
to	O
tell	O
how	O
well	O
our	O
hyperparameters	O
671	O
chapter	O
20.	O
deep	O
generative	O
models	O
a	O
)	O
b	O
)	O
c	O
)	O
d	O
)	O
,	O
;	O
et	O
al.	O
,	O
2014	O
)	O
.	O
(	O
a	O
)	O
figure	O
20.4	O
:	O
the	O
deep	O
boltzmann	O
machine	O
training	O
procedure	O
used	O
to	O
classify	O
the	O
mnist	O
dataset	O
(	O
salakhutdinov	O
and	O
hinton	O
2009a	O
srivastava	O
train	O
an	O
rbm	O
by	O
using	O
cd	O
to	O
approximately	O
maximize	O
log	O
p	O
(	O
v	O
)	O
.	O
train	O
a	O
second	O
rbm	O
that	O
models	O
(	O
b	O
)	O
h	O
(	O
1	O
)	O
and	O
target	O
class	O
y	O
by	O
using	O
cd-k	O
to	O
approximately	O
maximize	O
log	O
p	O
(	O
h	O
(	O
1	O
)	O
,	O
y	O
)	O
where	O
h	O
(	O
1	O
)	O
is	O
drawn	O
from	O
the	O
ﬁrst	O
rbm	O
’	O
s	O
posterior	O
conditioned	O
on	O
the	O
data	O
.	O
increase	O
k	O
from	O
1	O
combine	O
the	O
two	O
rbms	O
into	O
a	O
dbm	O
.	O
train	O
it	O
to	O
approximately	O
to	O
20	O
during	O
learning	O
.	O
maximize	O
log	O
p	O
(	O
v	O
,	O
y	O
)	O
using	O
stochastic	O
maximum	O
likelihood	O
with	O
k	O
=	O
5.	O
y	O
from	O
the	O
model	B
.	O
deﬁne	O
a	O
new	O
set	O
of	O
features	O
h	O
(	O
1	O
)	O
and	O
h	O
(	O
2	O
)	O
that	O
are	O
obtained	O
by	O
running	O
mean	O
ﬁeld	O
inference	O
in	O
the	O
model	B
lacking	O
y.	O
use	O
these	O
features	O
as	O
input	O
to	O
an	O
mlp	O
whose	O
structure	O
is	O
the	O
same	O
as	O
an	O
additional	O
pass	O
of	O
mean	O
ﬁeld	O
,	O
with	O
an	O
additional	O
output	O
layer	O
for	O
the	O
estimate	O
of	O
y.	O
initialize	O
the	O
mlp	O
’	O
s	O
weights	O
to	O
be	O
the	O
same	O
as	O
the	O
dbm	O
’	O
s	O
weights	O
.	O
train	O
the	O
mlp	O
to	O
approximately	O
maximize	O
log	O
p	O
(	O
y	O
v	O
)	O
using	O
stochastic	O
gradient	O
descent	B
and	O
dropout	O
.	O
figure	O
reprinted	O
from	O
(	O
goodfellow	O
et	O
al	O
.	O
2013b	O
delete	O
(	O
d	O
)	O
(	O
c	O
)	O
|	O
,	O
)	O
.	O
672	O
chapter	O
20.	O
deep	O
generative	O
models	O
are	O
working	O
until	O
quite	O
late	O
in	O
the	O
training	O
process	O
.	O
software	O
implementations	O
of	O
dbms	O
need	O
to	O
have	O
many	O
diﬀerent	O
components	O
for	O
cd	O
training	O
of	O
individual	O
rbms	O
,	O
pcd	O
training	O
of	O
the	O
full	O
dbm	O
,	O
and	O
training	O
based	O
on	O
back-propagation	O
through	O
the	O
mlp	O
.	O
finally	O
,	O
the	O
mlp	O
on	O
top	O
of	O
the	O
boltzmann	O
machine	O
loses	O
many	O
of	O
the	O
advantages	O
of	O
the	O
boltzmann	O
machine	O
probabilistic	O
model	B
,	O
such	O
as	O
being	O
able	O
to	O
perform	O
inference	O
when	O
some	O
input	O
values	O
are	O
missing	O
.	O
,	O
there	O
are	O
two	O
main	O
ways	O
to	O
resolve	O
the	O
joint	O
training	O
problem	O
of	O
the	O
deep	O
boltzmann	O
machine	O
.	O
the	O
ﬁrst	O
is	O
the	O
centered	O
deep	O
boltzmann	O
machine	O
(	O
montavon	O
and	O
muller	O
2012	O
)	O
,	O
which	O
reparametrizes	O
the	O
model	B
in	O
order	O
to	O
make	O
the	O
hessian	O
of	O
the	O
cost	O
function	O
better-conditioned	O
at	O
the	O
beginning	O
of	O
the	O
learning	O
process	O
.	O
this	O
yields	O
a	O
model	B
that	O
can	O
be	O
trained	O
without	O
a	O
greedy	O
layer-wise	O
pretraining	O
stage	O
.	O
the	O
resulting	O
model	B
obtains	O
excellent	O
test	O
set	O
log-likelihood	O
and	O
produces	O
high	O
quality	O
samples	O
.	O
unfortunately	O
,	O
it	O
remains	O
unable	O
to	O
compete	O
with	O
appropriately	O
regularized	O
mlps	O
as	O
a	O
classiﬁer	O
.	O
the	O
second	O
way	O
to	O
jointly	O
train	O
a	O
deep	O
boltzmann	O
machine	O
is	O
to	O
use	O
a	O
multi-prediction	O
deep	O
boltzmann	O
machine	O
(	O
goodfellow	O
)	O
.	O
this	O
model	B
uses	O
an	O
alternative	O
training	O
criterion	O
that	O
allows	O
the	O
use	O
of	O
the	O
back-propagation	O
algorithm	O
in	O
order	O
to	O
avoid	O
the	O
problems	O
with	O
mcmc	O
estimates	O
of	O
the	O
gradient	O
.	O
unfortunately	O
,	O
the	O
new	O
criterion	O
does	O
not	O
lead	O
to	O
good	O
likelihood	O
or	O
samples	O
,	O
but	O
,	O
compared	O
to	O
the	O
mcmc	O
approach	O
,	O
it	O
does	O
lead	O
to	O
superior	O
classiﬁcation	O
performance	O
and	O
ability	O
to	O
reason	O
well	O
about	O
missing	O
inputs	O
.	O
et	O
al.	O
,	O
2013b	O
the	O
centering	O
trick	B
for	O
the	O
boltzmann	O
machine	O
is	O
easiest	O
to	O
describe	O
if	O
we	O
return	O
to	O
the	O
general	O
view	O
of	O
a	O
boltzmann	O
machine	O
as	O
consisting	O
of	O
a	O
set	O
of	O
units	O
x	O
with	O
a	O
weight	O
matrix	O
u	O
and	O
biases	O
b.	O
recall	O
from	O
equation	O
that	O
he	O
energy	O
function	O
is	O
given	O
by	O
20.2	O
−	O
	O
x	O
e	O
(	O
)	O
=	O
x	O
−	O
	O
u	O
x	O
b	O
x	O
.	O
(	O
20.36	O
)	O
using	O
diﬀerent	O
sparsity	O
patterns	O
in	O
the	O
weight	O
matrix	O
u	O
,	O
we	O
can	O
implement	O
structures	O
of	O
boltzmann	O
machines	O
,	O
such	O
as	O
rbms	O
,	O
or	O
dbms	O
with	O
diﬀerent	O
numbers	O
of	O
layers	O
.	O
this	O
is	O
accomplished	O
by	O
partitioning	O
x	O
into	O
visible	O
and	O
hidden	O
units	O
and	O
zeroing	O
out	O
elements	O
of	O
u	O
for	O
units	O
that	O
do	O
not	O
interact	O
.	O
the	O
centered	O
boltzmann	O
machine	O
introduces	O
a	O
vector	O
that	O
is	O
subtracted	O
from	O
all	O
of	O
the	O
states	O
:	O
µ	O
−	O
)	O
=	O
(	O
−	O
	O
)	O
x	O
µ	O
	O
)	O
x	O
µ	O
(	O
−	O
−	O
u	O
x	O
µ	O
)	O
−	O
(	O
b	O
.	O
(	O
20.37	O
)	O
	O
e	O
(	O
;	O
x	O
u	O
b	O
,	O
typically	O
µ	O
is	O
a	O
hyperparameter	O
ﬁxed	O
at	O
the	O
beginning	O
of	O
training	O
.	O
it	O
is	O
usu-	O
ally	O
chosen	O
to	O
make	O
sure	O
that	O
x	O
µ	O
0	O
when	O
the	O
model	B
is	O
initialized	O
.	O
this	O
reparametrization	O
does	O
not	O
change	O
the	O
set	O
of	O
probability	O
distributions	O
that	O
the	O
model	B
can	O
represent	O
,	O
but	O
it	O
does	O
change	O
the	O
dynamics	O
of	O
stochastic	O
gradient	O
descent	B
applied	O
to	O
the	O
likelihood	O
.	O
speciﬁcally	O
,	O
in	O
many	O
cases	O
,	O
this	O
reparametrization	O
results	O
−	O
≈	O
673	O
chapter	O
20.	O
deep	O
generative	O
models	O
)	O
experimentally	O
in	O
a	O
hessian	O
matrix	O
that	O
is	O
better	O
conditioned	O
.	O
conﬁrmed	O
that	O
the	O
conditioning	O
of	O
the	O
hessian	O
matrix	O
improves	O
,	O
and	O
observed	O
that	O
the	O
centering	O
trick	B
is	O
equivalent	O
to	O
another	O
boltzmann	O
machine	O
learning	O
technique	O
,	O
the	O
enhanced	O
gradient	O
(	O
)	O
.	O
the	O
improved	O
conditioning	O
of	O
the	O
hessian	O
matrix	O
allows	O
learning	O
to	O
succeed	O
,	O
even	O
in	O
diﬃcult	O
cases	O
like	O
training	O
a	O
deep	O
boltzmann	O
machine	O
with	O
multiple	O
layers	O
.	O
melchior	O
et	O
al	O
.	O
2013	O
cho	O
et	O
al	O
.	O
2011	O
(	O
,	O
the	O
other	O
approach	O
to	O
jointly	O
training	O
deep	O
boltzmann	O
machines	O
is	O
the	O
multi-	O
prediction	O
deep	O
boltzmann	O
machine	O
(	O
mp-dbm	O
)	O
which	O
works	O
by	O
viewing	O
the	O
mean	O
ﬁeld	O
equations	O
as	O
deﬁning	O
a	O
family	O
of	O
recurrent	O
networks	O
for	O
approximately	O
solving	O
every	O
possible	O
inference	O
problem	O
(	O
)	O
.	O
rather	O
than	O
training	O
the	O
model	B
to	O
maximize	O
the	O
likelihood	O
,	O
the	O
model	B
is	O
trained	O
to	O
make	O
each	O
recurrent	O
network	O
obtain	O
an	O
accurate	O
answer	O
to	O
the	O
corresponding	O
inference	O
problem	O
.	O
the	O
training	O
process	O
is	O
illustrated	O
in	O
ﬁgure	O
.	O
it	O
consists	O
of	O
randomly	O
sampling	O
a	O
training	O
example	O
,	O
randomly	O
sampling	O
a	O
subset	O
of	O
inputs	O
to	O
the	O
inference	O
network	O
,	O
and	O
then	O
training	O
the	O
inference	O
network	O
to	O
predict	O
the	O
values	O
of	O
the	O
remaining	O
units	O
.	O
goodfellow	O
et	O
al	O
.	O
2013b	O
20.5	O
,	O
2013	O
et	O
al.	O
,	O
this	O
general	O
principle	O
of	O
back-propagating	O
through	O
the	O
computational	O
graph	O
2011	O
;	O
for	O
approximate	O
inference	O
has	O
been	O
applied	O
to	O
other	O
models	O
(	O
stoyanov	O
)	O
.	O
in	O
these	O
models	O
and	O
in	O
the	O
mp-dbm	O
,	O
the	O
ﬁnal	O
loss	O
is	O
not	O
brakel	O
the	O
lower	O
bound	B
on	O
the	O
likelihood	O
.	O
instead	O
,	O
the	O
ﬁnal	O
loss	O
is	O
typically	O
based	O
on	O
the	O
approximate	O
conditional	O
distribution	O
that	O
the	O
approximate	O
inference	O
network	O
imposes	O
over	O
the	O
missing	O
values	O
.	O
this	O
means	O
that	O
the	O
training	O
of	O
these	O
models	O
is	O
somewhat	O
heuristically	O
motivated	O
.	O
if	O
we	O
inspect	O
the	O
p	O
(	O
v	O
)	O
represented	O
by	O
the	O
boltzmann	O
machine	O
learned	O
by	O
the	O
mp-dbm	O
,	O
it	O
tends	O
to	O
be	O
somewhat	O
defective	O
,	O
in	O
the	O
sense	O
that	O
gibbs	O
sampling	O
yields	O
poor	O
samples	O
.	O
et	O
al.	O
,	O
back-propagation	O
through	O
the	O
inference	O
graph	O
has	O
two	O
main	O
advantages	O
.	O
first	O
,	O
it	O
trains	O
the	O
model	B
as	O
it	O
is	O
really	O
used—with	O
approximate	O
inference	O
.	O
this	O
means	O
that	O
approximate	O
inference	O
,	O
for	O
example	O
,	O
to	O
ﬁll	O
in	O
missing	O
inputs	O
,	O
or	O
to	O
perform	O
classiﬁcation	O
despite	O
the	O
presence	O
of	O
missing	O
inputs	O
,	O
is	O
more	O
accurate	O
in	O
the	O
mp-	O
dbm	O
than	O
in	O
the	O
original	O
dbm	O
.	O
the	O
original	O
dbm	O
does	O
not	O
make	O
an	O
accurate	O
classiﬁer	O
on	O
its	O
own	O
;	O
the	O
best	O
classiﬁcation	O
results	O
with	O
the	O
original	O
dbm	O
were	O
based	O
on	O
training	O
a	O
separate	O
classiﬁer	O
to	O
use	O
features	O
extracted	O
by	O
the	O
dbm	O
,	O
rather	O
than	O
by	O
using	O
inference	O
in	O
the	O
dbm	O
to	O
compute	O
the	O
distribution	O
over	O
the	O
class	O
labels	O
.	O
mean	O
ﬁeld	O
inference	O
in	O
the	O
mp-dbm	O
performs	O
well	O
as	O
a	O
classiﬁer	O
without	O
special	O
modiﬁcations	O
.	O
the	O
other	O
advantage	O
of	O
back-propagating	O
through	O
approximate	O
inference	O
is	O
that	O
back-propagation	O
computes	O
the	O
exact	O
gradient	O
of	O
the	O
loss	O
.	O
this	O
is	O
better	O
for	O
optimization	O
than	O
the	O
approximate	O
gradients	O
of	O
sml	O
training	O
,	O
which	O
suﬀer	O
from	O
both	O
bias	O
and	O
variance	O
.	O
this	O
probably	O
explains	O
why	O
mp-	O
674	O
chapter	O
20.	O
deep	O
generative	O
models	O
figure	O
20.5	O
:	O
an	O
illustration	O
of	O
the	O
multi-prediction	O
training	O
process	O
for	O
a	O
deep	O
boltzmann	O
machine	O
.	O
each	O
row	O
indicates	O
a	O
diﬀerent	O
example	O
within	O
a	O
minibatch	O
for	O
the	O
same	O
training	O
step	O
.	O
each	O
column	O
represents	O
a	O
time	O
step	O
within	O
the	O
mean	O
ﬁeld	O
inference	O
process	O
.	O
for	O
each	O
example	O
,	O
we	O
sample	O
a	O
subset	O
of	O
the	O
data	O
variables	O
to	O
serve	O
as	O
inputs	O
to	O
the	O
inference	O
process	O
.	O
these	O
variables	O
are	O
shaded	O
black	O
to	O
indicate	O
conditioning	O
.	O
we	O
then	O
run	O
the	O
mean	O
ﬁeld	O
inference	O
process	O
,	O
with	O
arrows	O
indicating	O
which	O
variables	O
inﬂuence	O
which	O
other	O
variables	O
in	O
the	O
process	O
.	O
in	O
practical	O
applications	O
,	O
we	O
unroll	O
mean	O
ﬁeld	O
for	O
several	O
steps	O
.	O
in	O
this	O
illustration	O
,	O
we	O
unroll	O
for	O
only	O
two	O
steps	O
.	O
dashed	O
arrows	O
indicate	O
how	O
the	O
process	O
could	O
be	O
unrolled	O
for	O
more	O
steps	O
.	O
the	O
data	O
variables	O
that	O
were	O
not	O
used	O
as	O
inputs	O
to	O
the	O
inference	O
process	O
become	O
targets	O
,	O
shaded	O
in	O
gray	O
.	O
we	O
can	O
view	O
the	O
inference	O
process	O
for	O
each	O
example	O
as	O
a	O
recurrent	O
network	O
.	O
we	O
use	O
gradient	O
descent	B
and	O
back-propagation	O
to	O
train	O
these	O
recurrent	O
networks	O
to	O
produce	O
the	O
correct	O
targets	O
given	O
their	O
inputs	O
.	O
this	O
trains	O
the	O
mean	O
ﬁeld	O
process	O
for	O
the	O
mp-dbm	O
to	O
produce	O
accurate	O
estimates	O
.	O
figure	O
adapted	O
from	O
goodfellow	O
et	O
al	O
.	O
2013b	O
(	O
)	O
.	O
675	O
chapter	O
20.	O
deep	O
generative	O
models	O
dbms	O
may	O
be	O
trained	O
jointly	O
while	O
dbms	O
require	O
a	O
greedy	O
layer-wise	O
pretraining	O
.	O
the	O
disadvantage	O
of	O
back-propagating	O
through	O
the	O
approximate	O
inference	O
graph	O
is	O
that	O
it	O
does	O
not	O
provide	O
a	O
way	O
to	O
optimize	O
the	O
log-likelihood	O
,	O
but	O
rather	O
a	O
heuristic	O
approximation	O
of	O
the	O
generalized	O
pseudolikelihood	O
.	O
the	O
mp-dbm	O
inspired	O
the	O
nade-k	O
(	O
raiko	O
2014	O
)	O
extension	O
to	O
the	O
nade	O
framework	O
,	O
which	O
is	O
described	O
in	O
section	O
et	O
al.	O
,	O
20.10.10	O
.	O
the	O
mp-dbm	O
has	O
some	O
connections	O
to	O
dropout	O
.	O
dropout	O
shares	O
the	O
same	O
pa-	O
rameters	O
among	O
many	O
diﬀerent	O
computational	O
graphs	O
,	O
with	O
the	O
diﬀerence	O
between	O
each	O
graph	O
being	O
whether	O
it	O
includes	O
or	O
excludes	O
each	O
unit	O
.	O
the	O
mp-dbm	O
also	O
shares	O
parameters	O
across	O
many	O
computational	O
graphs	O
.	O
in	O
the	O
case	O
of	O
the	O
mp-dbm	O
,	O
the	O
diﬀerence	O
between	O
the	O
graphs	O
is	O
whether	O
each	O
input	O
unit	O
is	O
observed	O
or	O
not	O
.	O
when	O
a	O
unit	O
is	O
not	O
observed	O
,	O
the	O
mp-dbm	O
does	O
not	O
delete	O
it	O
entirely	O
as	O
dropout	O
does	O
.	O
instead	O
,	O
the	O
mp-dbm	O
treats	O
it	O
as	O
a	O
latent	O
variable	O
to	O
be	O
inferred	O
.	O
one	O
could	O
imagine	O
applying	O
dropout	O
to	O
the	O
mp-dbm	O
by	O
additionally	O
removing	O
some	O
units	O
rather	O
than	O
making	O
them	O
latent	O
.	O
20.5	O
boltzmann	O
machines	O
for	O
real-valued	O
data	O
while	O
boltzmann	O
machines	O
were	O
originally	O
developed	O
for	O
use	O
with	O
binary	O
data	O
,	O
many	O
applications	O
such	O
as	O
image	O
and	O
audio	O
modeling	O
seem	O
to	O
require	O
the	O
ability	O
to	O
represent	O
probability	O
distributions	O
over	O
real	O
values	O
.	O
in	O
some	O
cases	O
,	O
it	O
is	O
possible	O
to	O
treat	O
real-valued	O
data	O
in	O
the	O
interval	O
[	O
0	O
,	O
1	O
]	O
as	O
representing	O
the	O
expectation	O
of	O
a	O
binary	O
variable	O
.	O
for	O
example	O
,	O
)	O
treats	O
grayscale	O
images	O
in	O
the	O
training	O
set	O
as	O
deﬁning	O
[	O
0,1	O
]	O
probability	O
values	O
.	O
each	O
pixel	O
deﬁnes	O
the	O
probability	O
of	O
a	O
binary	O
value	O
being	O
1	O
,	O
and	O
the	O
binary	O
pixels	O
are	O
all	O
sampled	O
independently	O
from	O
each	O
other	O
.	O
this	O
is	O
a	O
common	O
procedure	O
for	O
evaluating	O
binary	O
models	O
on	O
grayscale	O
image	O
datasets	O
.	O
however	O
,	O
it	O
is	O
not	O
a	O
particularly	O
theoretically	O
satisfying	O
approach	O
,	O
and	O
binary	O
images	O
sampled	O
independently	O
in	O
this	O
way	O
have	O
a	O
noisy	O
appearance	O
.	O
in	O
this	O
section	O
,	O
we	O
present	O
boltzmann	O
machines	O
that	O
deﬁne	O
a	O
probability	O
density	O
over	O
real-valued	O
data	O
.	O
hinton	O
2000	O
(	O
20.5.1	O
gaussian-bernoulli	O
rbms	O
restricted	O
boltzmann	O
machines	O
may	O
be	O
developed	O
for	O
many	O
exponential	O
family	O
conditional	O
distributions	O
(	O
welling	O
)	O
.	O
of	O
these	O
,	O
the	O
most	O
common	O
is	O
the	O
rbm	O
with	O
binary	O
hidden	O
units	O
and	O
real-valued	O
visible	O
units	O
,	O
with	O
the	O
conditional	O
distribution	O
over	O
the	O
visible	O
units	O
being	O
a	O
gaussian	O
distribution	O
whose	O
mean	O
is	O
a	O
function	O
of	O
the	O
hidden	O
units	O
.	O
et	O
al.	O
,	O
2005	O
676	O
chapter	O
20.	O
deep	O
generative	O
models	O
there	O
are	O
many	O
ways	O
of	O
parametrizing	O
gaussian-bernoulli	O
rbms	O
.	O
one	O
choice	O
is	O
whether	O
to	O
use	O
a	O
covariance	O
matrix	O
or	O
a	O
precision	O
matrix	O
for	O
the	O
gaussian	O
distribution	O
.	O
here	O
we	O
present	O
the	O
precision	O
formulation	O
.	O
the	O
modiﬁcation	O
to	O
obtain	O
the	O
covariance	O
formulation	O
is	O
straightforward	O
.	O
we	O
wish	O
to	O
have	O
the	O
conditional	O
distribution	O
|	O
v	O
h	O
(	O
p	O
n	O
)	O
=	O
(	O
;	O
v	O
w	O
h	O
β	O
,	O
−	O
1	O
)	O
.	O
(	O
20.38	O
)	O
we	O
can	O
ﬁnd	O
the	O
terms	O
we	O
need	O
to	O
add	O
to	O
the	O
energy	O
function	O
by	O
expanding	O
the	O
unnormalized	O
log	O
conditional	O
distribution	O
:	O
n	O
log	O
(	O
;	O
v	O
w	O
h	O
β	O
,	O
−	O
1	O
)	O
=	O
−	O
1	O
2	O
−	O
	O
)	O
v	O
w	O
h	O
(	O
−	O
β	O
v	O
w	O
h	O
(	O
)	O
+	O
(	O
f	O
β	O
)	O
.	O
(	O
20.39	O
)	O
here	O
f	O
encapsulates	O
all	O
the	O
terms	O
that	O
are	O
a	O
function	O
only	O
of	O
the	O
parameters	O
and	O
not	O
the	O
random	O
variables	O
in	O
the	O
model	B
.	O
we	O
can	O
discard	O
f	O
because	O
its	O
only	O
role	O
is	O
to	O
normalize	O
the	O
distribution	O
,	O
and	O
the	O
partition	O
function	O
of	O
whatever	O
energy	O
function	O
we	O
choose	O
will	O
carry	O
out	O
that	O
role	O
.	O
if	O
we	O
include	O
all	O
of	O
the	O
terms	O
(	O
with	O
their	O
sign	O
ﬂipped	O
)	O
involving	O
v	O
from	O
equa-	O
v	O
,	O
then	O
20.39	O
in	O
our	O
energy	O
function	O
and	O
do	O
not	O
add	O
any	O
other	O
terms	O
involving	O
tion	B
our	O
energy	O
function	O
will	O
represent	O
the	O
desired	O
conditional	O
|	O
we	O
have	O
some	O
freedom	O
regarding	O
the	O
other	O
conditional	O
distribution	O
,	O
p	O
(	O
h	O
v	O
|	O
.	O
v	O
h	O
)	O
p	O
(	O
)	O
.	O
note	O
that	O
equation	O
20.39	O
contains	O
a	O
term	O
	O
	O
h	O
w	O
βw	O
h.	O
(	O
20.40	O
)	O
1	O
2	O
1	O
2	O
this	O
term	O
can	O
not	O
be	O
included	O
in	O
its	O
entirety	O
because	O
it	O
includes	O
hihj	O
terms	O
.	O
these	O
correspond	O
to	O
edges	O
between	O
the	O
hidden	O
units	O
.	O
if	O
we	O
included	O
these	O
terms	O
,	O
we	O
would	O
have	O
a	O
linear	O
factor	O
model	B
instead	O
of	O
a	O
restricted	O
boltzmann	O
machine	O
.	O
when	O
designing	O
our	O
boltzmann	O
machine	O
,	O
we	O
simply	O
omit	O
these	O
hi	O
hj	O
cross	O
terms	O
.	O
omitting	O
them	O
does	O
not	O
change	O
the	O
conditional	O
p	O
(	O
v	O
h	O
is	O
still	O
respected	O
.	O
however	O
,	O
we	O
still	O
have	O
a	O
choice	O
about	O
whether	O
to	O
include	O
the	O
terms	O
involving	O
only	O
a	O
single	O
hi	O
.	O
if	O
we	O
assume	O
a	O
diagonal	O
precision	O
matrix	O
,	O
we	O
ﬁnd	O
that	O
for	O
each	O
hidden	O
unit	O
hi	O
we	O
have	O
a	O
term	O
)	O
so	O
equation	O
	O
20.39	O
|	O
h	O
i	O
βjw	O
2	O
j	O
,	O
i	O
.	O
(	O
20.41	O
)	O
}	O
in	O
the	O
above	O
,	O
we	O
used	O
the	O
fact	O
that	O
h2	O
.	O
if	O
we	O
include	O
this	O
term	O
(	O
with	O
its	O
sign	O
ﬂipped	O
)	O
in	O
the	O
energy	O
function	O
,	O
then	O
it	O
will	O
naturally	O
bias	O
h	O
i	O
to	O
be	O
turned	O
oﬀ	O
when	O
the	O
weights	O
for	O
that	O
unit	O
are	O
large	O
and	O
connected	O
to	O
visible	O
units	O
with	O
high	O
precision	O
.	O
the	O
choice	O
of	O
whether	O
or	O
not	O
to	O
include	O
this	O
bias	O
term	O
does	O
not	O
aﬀect	O
the	O
family	O
of	O
distributions	O
the	O
model	B
can	O
represent	O
(	O
assuming	O
that	O
j	O
i	O
=	O
hi	O
because	O
hi	O
∈	O
{	O
0	O
,	O
1	O
677	O
chapter	O
20.	O
deep	O
generative	O
models	O
we	O
include	O
bias	O
parameters	O
for	O
the	O
hidden	O
units	O
)	O
but	O
it	O
does	O
aﬀect	O
the	O
learning	O
dynamics	O
of	O
the	O
model	B
.	O
including	O
the	O
term	O
may	O
help	O
the	O
hidden	O
unit	O
activations	O
remain	O
reasonable	O
even	O
when	O
the	O
weights	O
rapidly	O
increase	O
in	O
magnitude	O
.	O
one	O
way	O
to	O
deﬁne	O
the	O
energy	O
function	O
on	O
a	O
gaussian-bernoulli	O
rbm	O
is	O
thus	O
e	O
,	O
(	O
v	O
h	O
)	O
=	O
	O
1	O
2	O
v	O
	O
−	O
(	O
β	O
v	O
)	O
	O
	O
)	O
v	O
β	O
(	O
−	O
	O
w	O
h	O
b	O
h	O
(	O
20.42	O
)	O
but	O
we	O
may	O
also	O
add	O
extra	O
terms	O
or	O
parametrize	O
the	O
energy	O
in	O
terms	O
of	O
the	O
variance	O
rather	O
than	O
precision	O
if	O
we	O
choose	O
.	O
in	O
this	O
derivation	O
,	O
we	O
have	O
not	O
included	O
a	O
bias	O
term	O
on	O
the	O
visible	O
units	O
,	O
but	O
one	O
could	O
easily	O
be	O
added	O
.	O
one	O
ﬁnal	O
source	O
of	O
variability	O
in	O
the	O
parametrization	O
of	O
a	O
gaussian-bernoulli	O
rbm	O
is	O
the	O
choice	O
of	O
how	O
to	O
treat	O
the	O
precision	O
matrix	O
.	O
it	O
may	O
either	O
be	O
ﬁxed	O
to	O
a	O
constant	O
(	O
perhaps	O
estimated	O
based	O
on	O
the	O
marginal	O
precision	O
of	O
the	O
data	O
)	O
or	O
learned	O
.	O
it	O
may	O
also	O
be	O
a	O
scalar	O
times	O
the	O
identity	O
matrix	O
,	O
or	O
it	O
may	O
be	O
a	O
diagonal	O
matrix	O
.	O
typically	O
we	O
do	O
not	O
allow	O
the	O
precision	O
matrix	O
to	O
be	O
non-diagonal	O
in	O
this	O
context	O
,	O
because	O
some	O
operations	O
on	O
the	O
gaussian	O
distribution	O
require	O
inverting	O
the	O
matrix	O
,	O
and	O
a	O
diagonal	O
matrix	O
can	O
be	O
inverted	O
trivially	O
.	O
in	O
the	O
sections	O
ahead	O
,	O
we	O
will	O
see	O
that	O
other	O
forms	O
of	O
boltzmann	O
machines	O
permit	O
modeling	O
the	O
covariance	O
structure	O
,	O
using	O
various	O
techniques	O
to	O
avoid	O
inverting	O
the	O
precision	O
matrix	O
.	O
20.5.2	O
undirected	O
models	O
of	O
conditional	O
covariance	O
(	O
ranzato	O
et	O
al	O
.	O
2010a	O
while	O
the	O
gaussian	O
rbm	O
has	O
been	O
the	O
canonical	O
energy	O
model	B
for	O
real-valued	O
data	O
,	O
)	O
argue	O
that	O
the	O
gaussian	O
rbm	O
inductive	O
bias	O
is	O
not	O
well	O
suited	O
to	O
the	O
statistical	O
variations	O
present	O
in	O
some	O
types	O
of	O
real-valued	O
data	O
,	O
especially	O
natural	O
images	O
.	O
the	O
problem	O
is	O
that	O
much	O
of	O
the	O
information	O
content	O
present	O
in	O
natural	O
images	O
is	O
embedded	O
in	O
the	O
covariance	O
between	O
pixels	O
rather	O
than	O
in	O
the	O
raw	O
pixel	O
values	O
.	O
in	O
other	O
words	O
,	O
it	O
is	O
the	O
relationships	O
between	O
pixels	O
and	O
not	O
their	O
absolute	O
values	O
where	O
most	O
of	O
the	O
useful	O
information	O
in	O
images	O
resides	O
.	O
since	O
the	O
gaussian	O
rbm	O
only	O
models	O
the	O
conditional	O
mean	O
of	O
the	O
input	O
given	O
the	O
hidden	O
units	O
,	O
it	O
can	O
not	O
capture	O
conditional	O
covariance	O
information	O
.	O
in	O
response	O
to	O
these	O
criticisms	O
,	O
alternative	O
models	O
have	O
been	O
proposed	O
that	O
attempt	O
to	O
better	O
account	O
for	O
the	O
covariance	O
of	O
real-valued	O
data	O
.	O
these	O
models	O
include	O
the	O
mean	O
and	O
covariance	O
rbm	O
(	O
mcrbm1	O
)	O
,	O
the	O
mean-product	O
of	O
t-distribution	O
(	O
mpot	O
)	O
model	B
and	O
the	O
spike	O
and	O
slab	O
rbm	O
(	O
ssrbm	O
)	O
.	O
1the	O
term	O
“	O
mcrbm	O
”	O
is	O
pronounced	O
by	O
saying	O
the	O
name	O
of	O
the	O
letters	O
m-c-r-b-m	O
;	O
the	O
“	O
mc	O
”	O
is	O
not	O
pronounced	O
like	O
the	O
“	O
mc	O
”	O
in	O
“	O
mcdonald	O
’	O
s.	O
”	O
678	O
chapter	O
20.	O
deep	O
generative	O
models	O
mean	O
and	O
covariance	O
rbm	O
the	O
mcrbm	O
uses	O
its	O
hidden	O
units	O
to	O
indepen-	O
dently	O
encode	O
the	O
conditional	O
mean	O
and	O
covariance	O
of	O
all	O
observed	O
units	O
.	O
the	O
mcrbm	O
hidden	O
layer	O
is	O
divided	O
into	O
two	O
groups	O
of	O
units	O
:	O
mean	O
units	O
and	O
covariance	O
units	O
.	O
the	O
group	O
that	O
models	O
the	O
conditional	O
mean	O
is	O
simply	O
a	O
gaussian	O
rbm	O
.	O
the	O
other	O
half	O
is	O
a	O
covariance	O
rbm	O
(	O
)	O
,	O
also	O
called	O
a	O
crbm	O
,	O
whose	O
components	O
model	B
the	O
conditional	O
covariance	O
structure	O
,	O
as	O
described	O
below	O
.	O
ranzato	O
et	O
al	O
.	O
2010a	O
,	O
speciﬁcally	O
,	O
with	O
binary	O
mean	O
units	O
h	O
(	O
)	O
m	O
and	O
binary	O
covariance	O
units	O
h	O
(	O
)	O
c	O
,	O
the	O
mcrbm	O
model	B
is	O
deﬁned	O
as	O
the	O
combination	O
of	O
two	O
energy	O
functions	O
:	O
emc	O
(	O
x	O
h	O
,	O
(	O
)	O
m	O
,	O
h	O
(	O
)	O
c	O
)	O
=	O
em	O
(	O
x	O
h	O
,	O
(	O
)	O
m	O
)	O
+	O
ec	O
(	O
x	O
h	O
,	O
(	O
)	O
c	O
)	O
,	O
(	O
20.43	O
)	O
where	O
em	O
is	O
the	O
standard	O
gaussian-bernoulli	O
rbm	O
energy	O
function:2	O
(	O
)	O
m	O
)	O
=	O
em	O
(	O
x	O
h	O
,	O
	O
x	O
x	O
1	O
2	O
)	O
m	O
j	O
h	O
(	O
b	O
(	O
)	O
m	O
j	O
,	O
(	O
20.44	O
)	O
	O
−	O
j	O
	O
	O
	O
x	O
)	O
m	O
w	O
:	O
,jh	O
(	O
j	O
−	O
	O
	O
j	O
	O
and	O
ec	O
is	O
the	O
crbm	O
energy	O
function	O
that	O
models	O
the	O
conditional	O
covariance	O
information	O
:	O
ec	O
(	O
x	O
h	O
,	O
(	O
)	O
c	O
)	O
=	O
1	O
2	O
j	O
h	O
(	O
)	O
c	O
j	O
	O
x	O
r	O
(	O
)	O
j	O
2	O
−	O
j	O
b	O
(	O
)	O
c	O
j	O
h	O
(	O
)	O
c	O
j	O
.	O
(	O
20.45	O
)	O
the	O
parameter	O
r	O
(	O
)	O
j	O
corresponds	O
to	O
the	O
covariance	O
weight	O
vector	O
associated	O
with	O
h	O
(	O
)	O
c	O
and	O
b	O
(	O
)	O
c	O
is	O
a	O
vector	O
of	O
covariance	O
oﬀsets	O
.	O
the	O
combined	O
energy	O
function	O
deﬁnes	O
j	O
a	O
joint	O
distribution	O
:	O
1	O
z	O
	O
−	O
emc	O
(	O
x	O
h	O
,	O
	O
pmc	O
(	O
x	O
h	O
,	O
(	O
)	O
m	O
,	O
h	O
(	O
)	O
c	O
)	O
=	O
exp	O
(	O
)	O
m	O
,	O
h	O
(	O
)	O
c	O
)	O
(	O
20.46	O
)	O
and	O
a	O
corresponding	O
conditional	O
distribution	O
over	O
the	O
observations	O
given	O
h	O
(	O
h	O
(	O
)	O
c	O
as	O
a	O
multivariate	O
gaussian	O
distribution	O
:	O
)	O
m	O
and	O
n	O
)	O
m	O
,	O
h	O
(	O
)	O
c	O
)	O
=	O
(	O
pmc	O
(	O
x	O
h	O
|	O
|	O
x	O
c	O
;	O
mc	O
x	O
h	O
)	O
m	O
w	O
:	O
,jh	O
(	O
j	O
|	O
,	O
c	O
mc	O
x	O
h	O
.	O
(	O
20.47	O
)	O
is	O
non-diagonal	O
note	O
that	O
the	O
covariance	O
matrix	O
cmc	O
x	O
h	O
and	O
that	O
w	O
is	O
the	O
weight	O
matrix	O
associated	O
with	O
the	O
gaussian	O
rbm	O
modeling	O
the	O
+	O
i	O
|	O
=	O
	O
j	O
r	O
(	O
)	O
j	O
r	O
(	O
)	O
j	O
j	O
h	O
(	O
)	O
c	O
2this	O
version	O
of	O
the	O
gaussian-bernoulli	O
rbm	O
energy	O
function	O
assumes	O
the	O
image	O
data	O
has	O
zero	O
mean	O
,	O
per	O
pixel	O
.	O
pixel	O
oﬀsets	O
can	O
easily	O
be	O
added	O
to	O
the	O
model	B
to	O
account	O
for	O
nonzero	O
pixel	O
means	O
.	O
679	O
	O
	O
	O
j	O
	O
	O
,	O
	O
−	O
1	O
chapter	O
20.	O
deep	O
generative	O
models	O
conditional	O
means	O
.	O
it	O
is	O
diﬃcult	O
to	O
train	O
the	O
mcrbm	O
via	O
contrastive	O
divergence	O
or	O
persistent	O
contrastive	O
divergence	O
because	O
of	O
its	O
non-diagonal	O
conditional	O
covariance	O
)	O
m	O
,	O
h	O
(	O
)	O
c	O
(	O
structure	O
.	O
cd	O
and	O
pcd	O
require	O
sampling	O
from	O
the	O
joint	O
distribution	O
of	O
x	O
h	O
,	O
which	O
,	O
in	O
a	O
standard	O
rbm	O
,	O
is	O
accomplished	O
by	O
gibbs	O
sampling	O
over	O
the	O
conditionals	O
.	O
)	O
m	O
,	O
h	O
(	O
)	O
c	O
)	O
requires	O
computing	O
(	O
however	O
,	O
in	O
the	O
mcrbm	O
,	O
sampling	O
from	O
pmc	O
(	O
x	O
h	O
−	O
(	O
cmc	O
)	O
1	O
at	O
every	O
iteration	O
of	O
learning	O
.	O
this	O
can	O
be	O
an	O
impractical	O
computational	O
|	O
burden	O
for	O
larger	O
observations	O
.	O
ranzato	O
and	O
hinton	O
2010	O
)	O
avoid	O
direct	O
sampling	O
)	O
m	O
,	O
h	O
(	O
)	O
c	O
)	O
by	O
sampling	O
directly	O
from	O
the	O
marginal	O
from	O
the	O
conditional	O
pmc	O
(	O
x	O
h	O
(	O
p	O
(	O
x	O
)	O
using	O
hamiltonian	O
(	O
hybrid	O
)	O
monte	O
carlo	O
(	O
)	O
on	O
the	O
mcrbm	O
free	O
energy	O
.	O
neal	O
1993	O
|	O
(	O
,	O
,	O
t	O
2003a	O
)	O
extends	O
the	O
pot	O
model	B
(	O
the	O
mean-product	O
of	O
student	O
’	O
s	O
mean-product	O
of	O
student	O
’	O
s	O
-distributions	O
t-distribution	O
(	O
mpot	O
)	O
model	B
(	O
welling	O
ranzato	O
et	O
al	O
.	O
2010b	O
et	O
al.	O
,	O
)	O
in	O
a	O
manner	O
similar	O
to	O
how	O
the	O
mcrbm	O
extends	O
the	O
crbm	O
.	O
this	O
is	O
achieved	O
by	O
including	O
nonzero	O
gaussian	O
means	O
by	O
the	O
addition	O
of	O
gaussian	O
rbm-like	O
hidden	O
units	O
.	O
like	O
the	O
mcrbm	O
,	O
the	O
pot	O
conditional	O
distribution	O
over	O
the	O
observation	O
is	O
a	O
multivariate	O
gaussian	O
(	O
with	O
non-diagonal	O
covariance	O
)	O
distribution	O
;	O
however	O
,	O
unlike	O
the	O
mcrbm	O
,	O
the	O
complementary	O
conditional	O
distribution	O
over	O
the	O
hidden	O
variables	O
is	O
given	O
by	O
conditionally	O
independent	O
gamma	O
distributions	O
.	O
the	O
(	O
k	O
,	O
θ	O
)	O
is	O
a	O
probability	O
distribution	O
over	O
positive	O
real	O
numbers	O
,	O
gamma	O
distribution	O
with	O
mean	O
kθ	O
.	O
it	O
is	O
not	O
necessary	O
to	O
have	O
a	O
more	O
detailed	O
understanding	O
of	O
the	O
gamma	O
distribution	O
to	O
understand	O
the	O
basic	O
ideas	O
underlying	O
the	O
mpot	O
model	B
.	O
g	O
	O
	O
	O
the	O
mpot	O
energy	O
function	O
is	O
:	O
	O
	O
	O
	O
(	O
)	O
m	O
,	O
h	O
(	O
)	O
c	O
)	O
empot	O
(	O
x	O
h	O
,	O
=	O
em	O
(	O
x	O
h	O
,	O
(	O
)	O
m	O
)	O
+	O
h	O
(	O
)	O
c	O
j	O
1	O
+	O
1	O
2	O
j	O
	O
r	O
(	O
)	O
j	O
x	O
2	O
−	O
+	O
(	O
1	O
(	O
20.48	O
)	O
γj	O
)	O
log	O
h	O
(	O
)	O
c	O
j	O
(	O
20.49	O
)	O
where	O
r	O
(	O
)	O
j	O
is	O
as	O
deﬁned	O
in	O
equation	O
.	O
20.44	O
is	O
the	O
covariance	O
weight	O
vector	O
associated	O
with	O
unit	O
h	O
(	O
)	O
c	O
(	O
)	O
m	O
)	O
j	O
and	O
em	O
(	O
x	O
h	O
,	O
just	O
as	O
with	O
the	O
mcrbm	O
,	O
the	O
mpot	O
model	B
energy	O
function	O
speciﬁes	O
a	O
mul-	O
tivariate	O
gaussian	O
,	O
with	O
a	O
conditional	O
distribution	O
over	O
x	O
that	O
has	O
non-diagonal	O
covariance	O
.	O
learning	O
in	O
the	O
mpot	O
model—again	O
,	O
like	O
the	O
mcrbm—is	O
compli-	O
cated	O
by	O
the	O
inability	O
to	O
sample	O
from	O
the	O
non-diagonal	O
gaussian	O
conditional	O
(	O
)	O
m	O
,	O
h	O
(	O
)	O
c	O
)	O
,	O
so	O
pmpot	O
(	O
x	O
h	O
)	O
also	O
advocate	O
direct	O
sampling	O
of	O
p	O
(	O
)	O
x	O
via	O
hamiltonian	O
(	O
hybrid	O
)	O
monte	O
carlo	O
.	O
ranzato	O
et	O
al	O
.	O
2010b	O
|	O
(	O
680	O
chapter	O
20.	O
deep	O
generative	O
models	O
courville	O
et	O
al	O
.	O
2011	O
spike	O
and	O
slab	O
restricted	O
boltzmann	O
machines	O
spike	O
and	O
slab	O
restricted	O
boltzmann	O
machines	O
(	O
)	O
or	O
ssrbms	O
provide	O
another	O
means	O
of	O
modeling	O
the	O
covariance	O
structure	O
of	O
real-valued	O
data	O
.	O
compared	O
to	O
mcrbms	O
,	O
ssrbms	O
have	O
the	O
advantage	O
of	O
requiring	O
neither	O
matrix	O
inversion	O
nor	O
hamiltonian	O
monte	O
carlo	O
methods	O
.	O
like	O
the	O
mcrbm	O
and	O
the	O
mpot	O
model	B
,	O
the	O
ssrbm	O
’	O
s	O
binary	O
hidden	O
units	O
encode	O
the	O
conditional	O
covariance	O
across	O
pixels	O
through	O
the	O
use	O
of	O
auxiliary	O
real-valued	O
variables	O
.	O
,	O
	O
)	O
w	O
the	O
spike	O
and	O
slab	O
rbm	O
has	O
two	O
sets	O
of	O
hidden	O
units	O
:	O
binary	O
spike	O
units	O
h	O
,	O
	O
and	O
real-valued	O
slab	O
units	O
s	O
.	O
the	O
mean	O
of	O
the	O
visible	O
units	O
conditioned	O
on	O
the	O
hidden	O
units	O
is	O
given	O
by	O
(	O
h	O
s	O
.	O
in	O
other	O
words	O
,	O
each	O
column	O
w	O
:	O
,i	O
deﬁnes	O
a	O
component	O
that	O
can	O
appear	O
in	O
the	O
input	O
when	O
hi	O
=	O
1.	O
the	O
corresponding	O
spike	O
variable	O
hi	O
determines	O
whether	O
that	O
component	O
is	O
present	O
at	O
all	O
.	O
the	O
corresponding	O
slab	O
variable	O
si	O
determines	O
the	O
intensity	O
of	O
that	O
component	O
,	O
if	O
it	O
is	O
present	O
.	O
when	O
a	O
spike	O
variable	O
is	O
active	O
,	O
the	O
corresponding	O
slab	O
variable	O
adds	O
variance	O
to	O
the	O
input	O
along	O
the	O
axis	O
deﬁned	O
by	O
w	O
:	O
,i.	O
this	O
allows	O
us	O
to	O
model	B
the	O
covariance	O
of	O
the	O
inputs	O
.	O
fortunately	O
,	O
contrastive	O
divergence	O
and	O
persistent	O
contrastive	O
divergence	O
with	O
gibbs	O
sampling	O
are	O
still	O
applicable	O
.	O
there	O
is	O
no	O
need	O
to	O
invert	O
any	O
matrix	O
.	O
	O
formally	O
,	O
the	O
ssrbm	O
model	B
is	O
deﬁned	O
via	O
its	O
energy	O
function	O
:	O
e	O
ss	O
(	O
x	O
s	O
h	O
,	O
,	O
)	O
=	O
−	O
+	O
	O
	O
1	O
2	O
x	O
αi	O
µisih	O
i	O
λ	O
+	O
−	O
w	O
:	O
,isihi	O
+	O
φihi	O
(	O
20.50	O
)	O
−	O
αis2	O
i	O
i	O
1	O
2	O
i	O
i	O
i	O
i	O
bih	O
i	O
+	O
αi	O
µ2	O
i	O
hi	O
,	O
(	O
20.51	O
)	O
i	O
	O
	O
	O
x	O
	O
	O
	O
	O
x	O
where	O
bi	O
is	O
the	O
oﬀset	O
of	O
the	O
spike	O
hi	O
and	O
λ	O
is	O
a	O
diagonal	O
precision	O
matrix	O
on	O
the	O
observations	O
x.	O
the	O
parameter	O
αi	O
>	O
0	O
is	O
a	O
scalar	O
precision	O
parameter	O
for	O
the	O
real-valued	O
slab	O
variable	O
si	O
.	O
the	O
parameter	O
φi	O
is	O
a	O
non-negative	O
diagonal	O
matrix	O
that	O
deﬁnes	O
an	O
h-modulated	O
quadratic	O
penalty	O
on	O
x.	O
each	O
µi	O
is	O
a	O
mean	O
parameter	O
for	O
the	O
slab	O
variable	O
si	O
.	O
with	O
the	O
joint	O
distribution	O
deﬁned	O
via	O
the	O
energy	O
function	O
,	O
it	O
is	O
relatively	O
straightforward	O
to	O
derive	O
the	O
ssrbm	O
conditional	O
distributions	O
.	O
for	O
example	O
,	O
by	O
marginalizing	O
out	O
the	O
slab	O
variables	O
s	O
,	O
the	O
conditional	O
distribution	O
over	O
the	O
observations	O
given	O
the	O
binary	O
spike	O
variables	O
	O
	O
exp	O
is	O
given	O
by	O
:	O
h	O
{	O
−	O
}	O
)	O
e	O
x	O
s	O
h	O
,	O
(	O
,	O
	O
ds	O
(	O
20.52	O
)	O
	O
1	O
1	O
z	O
p	O
(	O
)	O
h	O
n	O
|	O
pss	O
(	O
x	O
h	O
)	O
=	O
=	O
x	O
c	O
;	O
|	O
ss	O
x	O
h	O
|	O
w	O
:	O
,iµih	O
i	O
,	O
css	O
x	O
h	O
(	O
20.53	O
)	O
i	O
681	O
chapter	O
20.	O
deep	O
generative	O
models	O
	O
	O
	O
−	O
	O
	O
	O
i	O
λ	O
+	O
−	O
1	O
α	O
i	O
hiw	O
:	O
,iw	O
	O
:	O
,i	O
is	O
positive	O
deﬁnite	O
.	O
|	O
=	O
where	O
c	O
ss	O
φihi	O
x	O
h	O
|	O
the	O
covariance	O
matrix	O
c	O
ss	O
x	O
h	O
	O
gating	O
by	O
the	O
spike	O
variables	O
means	O
that	O
the	O
true	O
marginal	O
distribution	O
over	O
is	O
sparse	O
.	O
this	O
is	O
diﬀerent	O
from	O
sparse	O
coding	O
,	O
where	O
samples	O
from	O
the	O
model	B
h	O
s	O
“	O
almost	O
never	O
”	O
(	O
in	O
the	O
measure	O
theoretic	O
sense	O
)	O
contain	O
zeros	O
in	O
the	O
code	O
,	O
and	O
map	O
inference	O
is	O
required	O
to	O
impose	O
sparsity	O
.	O
−	O
1.	O
the	O
last	O
equality	O
holds	O
only	O
if	O
i	O
as	O
+	O
i	O
−	O
1	O
j	O
h	O
(	O
)	O
c	O
	O
j	O
r	O
(	O
)	O
j	O
r	O
(	O
)	O
j	O
comparing	O
the	O
ssrbm	O
to	O
the	O
mcrbm	O
and	O
the	O
mpot	O
models	O
,	O
the	O
ssrbm	O
parametrizes	O
the	O
conditional	O
covariance	O
of	O
the	O
observation	O
in	O
a	O
signiﬁcantly	O
diﬀerent	O
way	O
.	O
the	O
mcrbm	O
and	O
mpot	O
both	O
model	B
the	O
covariance	O
structure	O
of	O
the	O
observation	O
,	O
using	O
the	O
activation	O
of	O
the	O
hidden	O
units	O
hj	O
>	O
0	O
to	O
enforce	O
constraints	O
on	O
the	O
conditional	O
covariance	O
in	O
the	O
direction	O
r	O
(	O
)	O
j	O
.	O
in	O
contrast	O
,	O
the	O
ssrbm	O
speciﬁes	O
the	O
conditional	O
covariance	O
of	O
the	O
observations	O
using	O
the	O
hidden	O
spike	O
activations	O
hi	O
=	O
1	O
to	O
pinch	O
the	O
precision	O
matrix	O
along	O
the	O
direction	O
speciﬁed	O
by	O
the	O
corresponding	O
weight	O
vector	O
.	O
the	O
ssrbm	O
conditional	O
covariance	O
is	O
very	O
similar	O
to	O
that	O
given	O
by	O
a	O
diﬀerent	O
model	B
:	O
the	O
product	O
of	O
probabilistic	O
principal	O
components	O
analysis	O
(	O
poppca	O
)	O
(	O
williams	O
and	O
agakov	O
2002	O
)	O
.	O
in	O
the	O
overcomplete	O
setting	O
,	O
sparse	O
activations	O
with	O
the	O
ssrbm	O
parametrization	O
permit	O
signiﬁcant	O
−	O
1	O
)	O
only	O
in	O
the	O
selected	O
directions	O
variance	O
(	O
above	O
the	O
nominal	O
variance	O
given	O
by	O
λ	O
of	O
the	O
sparsely	O
activated	O
hi	O
.	O
in	O
the	O
mcrbm	O
or	O
mpot	O
models	O
,	O
an	O
overcomplete	O
representation	O
would	O
mean	O
that	O
to	O
capture	O
variation	O
in	O
a	O
particular	O
direction	O
in	O
the	O
observation	O
space	O
requires	O
removing	O
potentially	O
all	O
constraints	O
with	O
positive	O
projection	O
in	O
that	O
direction	O
.	O
this	O
would	O
suggest	O
that	O
these	O
models	O
are	O
less	O
well	O
suited	O
to	O
the	O
overcomplete	O
setting	O
.	O
,	O
the	O
primary	O
disadvantage	O
of	O
the	O
spike	O
and	O
slab	O
restricted	O
boltzmann	O
machine	O
is	O
that	O
some	O
settings	O
of	O
the	O
parameters	O
can	O
correspond	O
to	O
a	O
covariance	O
matrix	O
that	O
is	O
not	O
positive	O
deﬁnite	O
.	O
such	O
a	O
covariance	O
matrix	O
places	O
more	O
unnormalized	O
probability	O
on	O
values	O
that	O
are	O
farther	O
from	O
the	O
mean	O
,	O
causing	O
the	O
integral	O
over	O
all	O
possible	O
outcomes	O
to	O
diverge	O
.	O
generally	O
this	O
issue	O
can	O
be	O
avoided	O
with	O
simple	O
heuristic	O
tricks	O
.	O
there	O
is	O
not	O
yet	O
any	O
theoretically	O
satisfying	O
solution	O
.	O
using	O
constrained	O
optimization	O
to	O
explicitly	O
avoid	O
the	O
regions	O
where	O
the	O
probability	O
is	O
undeﬁned	O
is	O
diﬃcult	O
to	O
do	O
without	O
being	O
overly	O
conservative	O
and	O
also	O
preventing	O
the	O
model	B
from	O
accessing	O
high-performing	O
regions	O
of	O
parameter	O
space	O
.	O
qualitatively	O
,	O
convolutional	O
variants	O
of	O
the	O
ssrbm	O
produce	O
excellent	O
samples	O
of	O
natural	O
images	O
.	O
some	O
examples	O
are	O
shown	O
in	O
ﬁgure	O
.	O
16.1	O
the	O
ssrbm	O
allows	O
for	O
several	O
extensions	O
.	O
including	O
higher-order	O
interactions	O
and	O
average-pooling	O
of	O
the	O
slab	O
variables	O
(	O
)	O
enables	O
the	O
model	B
to	O
learn	O
excellent	O
features	O
for	O
a	O
classiﬁer	O
when	O
labeled	O
data	O
is	O
scarce	O
.	O
adding	O
a	O
courville	O
et	O
al	O
.	O
2014	O
,	O
682	O
chapter	O
20.	O
deep	O
generative	O
models	O
term	O
to	O
the	O
energy	O
function	O
that	O
prevents	O
the	O
partition	O
function	O
from	O
becoming	O
undeﬁned	O
results	O
in	O
a	O
sparse	O
coding	O
model	B
,	O
spike	O
and	O
slab	O
sparse	O
coding	O
(	O
goodfellow	O
et	O
al.	O
,	O
)	O
,	O
also	O
known	O
as	O
s3c	O
.	O
2013d	O
20.6	O
convolutional	O
boltzmann	O
machines	O
9	O
as	O
seen	O
in	O
chapter	O
,	O
extremely	O
high	O
dimensional	O
inputs	O
such	O
as	O
images	O
place	O
great	O
strain	O
on	O
the	O
computation	O
,	O
memory	O
and	O
statistical	O
requirements	O
of	O
machine	O
learning	O
models	O
.	O
replacing	O
matrix	O
multiplication	O
by	O
discrete	O
convolution	O
with	O
a	O
small	O
kernel	O
is	O
the	O
standard	O
way	O
of	O
solving	O
these	O
problems	O
for	O
inputs	O
that	O
have	O
translation	O
invariant	O
spatial	O
or	O
temporal	O
structure	O
.	O
desjardins	O
and	O
bengio	O
2008	O
)	O
showed	O
that	O
this	O
approach	O
works	O
well	O
when	O
applied	O
to	O
rbms	O
.	O
(	O
deep	O
convolutional	O
networks	O
usually	O
require	O
a	O
pooling	O
operation	O
so	O
that	O
the	O
spatial	O
size	O
of	O
each	O
successive	O
layer	O
decreases	O
.	O
feedforward	O
convolutional	O
networks	O
often	O
use	O
a	O
pooling	O
function	O
such	O
as	O
the	O
maximum	O
of	O
the	O
elements	O
to	O
be	O
pooled	O
.	O
it	O
is	O
unclear	O
how	O
to	O
generalize	O
this	O
to	O
the	O
setting	O
of	O
energy-based	O
models	O
.	O
we	O
could	O
introduce	O
a	O
binary	O
pooling	O
unit	O
p	O
over	O
n	O
binary	O
detector	O
units	O
d	O
and	O
enforce	O
p	O
=	O
maxi	O
di	O
by	O
setting	O
the	O
energy	O
function	O
to	O
be	O
whenever	O
that	O
constraint	O
is	O
×	O
violated	O
.	O
this	O
does	O
not	O
scale	O
well	O
though	O
,	O
as	O
it	O
requires	O
evaluating	O
2	O
n	O
diﬀerent	O
3	O
energy	O
conﬁgurations	O
to	O
compute	O
the	O
normalization	O
constant	O
.	O
for	O
a	O
small	O
3	O
pooling	O
region	O
this	O
requires	O
29	O
=	O
512	O
energy	O
function	O
evaluations	O
per	O
pooling	O
unit	O
!	O
∞	O
lee	O
2009	O
et	O
al	O
.	O
(	O
)	O
developed	O
a	O
solution	O
to	O
this	O
problem	O
called	O
probabilistic	O
max	O
pooling	O
(	O
not	O
to	O
be	O
confused	O
with	O
“	O
stochastic	O
pooling	O
,	O
”	O
which	O
is	O
a	O
technique	O
for	O
implicitly	O
constructing	O
ensembles	O
of	O
convolutional	O
feedforward	O
networks	O
)	O
.	O
the	O
strategy	O
behind	O
probabilistic	O
max	O
pooling	O
is	O
to	O
constrain	O
the	O
detector	O
units	O
so	O
at	O
most	O
one	O
may	O
be	O
active	O
at	O
a	O
time	O
.	O
this	O
means	O
there	O
are	O
only	O
n	O
+	O
1	O
total	O
states	O
(	O
one	O
state	O
for	O
each	O
of	O
the	O
n	O
detector	O
units	O
being	O
on	O
,	O
and	O
an	O
additional	O
state	O
corresponding	O
to	O
all	O
of	O
the	O
detector	O
units	O
being	O
oﬀ	O
)	O
.	O
the	O
pooling	O
unit	O
is	O
on	O
if	O
and	O
only	O
if	O
one	O
of	O
the	O
detector	O
units	O
is	O
on	O
.	O
the	O
state	O
with	O
all	O
units	O
oﬀ	O
is	O
assigned	O
energy	O
zero	O
.	O
we	O
can	O
think	O
of	O
this	O
as	O
describing	O
a	O
model	B
with	O
a	O
single	O
variable	O
that	O
has	O
n	O
+	O
1	O
states	O
,	O
or	O
equivalently	O
as	O
a	O
model	B
that	O
has	O
n	O
+	O
1	O
variables	O
that	O
assigns	O
energy	O
joint	O
assignments	O
of	O
variables	O
.	O
to	O
all	O
but	O
n	O
+	O
1	O
∞	O
while	O
eﬃcient	O
,	O
probabilistic	O
max	O
pooling	O
does	O
force	O
the	O
detector	O
units	O
to	O
be	O
mutually	O
exclusive	O
,	O
which	O
may	O
be	O
a	O
useful	O
regularizing	O
constraint	O
in	O
some	O
contexts	O
or	O
a	O
harmful	O
limit	O
on	O
model	B
capacity	O
in	O
other	O
contexts	O
.	O
it	O
also	O
does	O
not	O
support	O
overlapping	O
pooling	O
regions	O
.	O
overlapping	O
pooling	O
regions	O
are	O
usually	O
required	O
to	O
obtain	O
the	O
best	O
performance	O
from	O
feedforward	O
convolutional	O
networks	O
,	O
so	O
this	O
constraint	O
probably	O
greatly	O
reduces	O
the	O
performance	O
of	O
convolutional	O
boltzmann	O
683	O
chapter	O
20.	O
deep	O
generative	O
models	O
machines	O
.	O
lee	O
2009	O
et	O
al	O
.	O
(	O
)	O
demonstrated	O
that	O
probabilistic	O
max	O
pooling	O
could	O
be	O
used	O
to	O
build	O
convolutional	O
deep	O
boltzmann	O
machines.3	O
this	O
model	B
is	O
able	O
to	O
perform	O
operations	O
such	O
as	O
ﬁlling	O
in	O
missing	O
portions	O
of	O
its	O
input	O
.	O
while	O
intellectually	O
appealing	O
,	O
this	O
model	B
is	O
challenging	O
to	O
make	O
work	B
in	O
practice	O
,	O
and	O
usually	O
does	O
not	O
perform	O
as	O
well	O
as	O
a	O
classiﬁer	O
as	O
traditional	O
convolutional	O
networks	O
trained	O
with	O
supervised	O
learning	O
.	O
many	O
convolutional	O
models	O
work	B
equally	O
well	O
with	O
inputs	O
of	O
many	O
diﬀerent	O
spatial	O
sizes	O
.	O
for	O
boltzmann	O
machines	O
,	O
it	O
is	O
diﬃcult	O
to	O
change	O
the	O
input	O
size	O
for	O
a	O
variety	O
of	O
reasons	O
.	O
the	O
partition	O
function	O
changes	O
as	O
the	O
size	O
of	O
the	O
input	O
changes	O
.	O
moreover	O
,	O
many	O
convolutional	O
networks	O
achieve	O
size	O
invariance	O
by	O
scaling	O
up	O
the	O
size	O
of	O
their	O
pooling	O
regions	O
proportional	O
to	O
the	O
size	O
of	O
the	O
input	O
,	O
but	O
scaling	O
boltzmann	O
machine	O
pooling	O
regions	O
is	O
awkward	O
.	O
traditional	O
convolutional	O
neural	O
networks	O
can	O
use	O
a	O
ﬁxed	O
number	O
of	O
pooling	O
units	O
and	O
dynamically	O
increase	O
the	O
size	O
of	O
their	O
pooling	O
regions	O
in	O
order	O
to	O
obtain	O
a	O
ﬁxed-size	O
representation	O
of	O
a	O
variable-sized	O
input	O
.	O
for	O
boltzmann	O
machines	O
,	O
large	O
pooling	O
regions	O
become	O
too	O
expensive	O
for	O
the	O
naive	O
approach	O
.	O
the	O
approach	O
of	O
)	O
of	O
making	O
each	O
of	O
the	O
detector	O
units	O
in	O
the	O
same	O
pooling	O
region	O
mutually	O
exclusive	O
solves	O
the	O
computational	O
problems	O
,	O
but	O
still	O
does	O
not	O
allow	O
variable-size	O
pooling	O
regions	O
.	O
for	O
example	O
,	O
suppose	O
we	O
learn	O
a	O
model	B
with	O
2	O
2	O
probabilistic	O
max	O
pooling	O
over	O
detector	O
units	O
that	O
learn	O
edge	O
detectors	O
.	O
this	O
enforces	O
the	O
constraint	O
that	O
only	O
one	O
of	O
these	O
edges	O
may	O
appear	O
in	O
each	O
2	O
2	O
region	O
.	O
if	O
we	O
then	O
increase	O
the	O
size	O
of	O
the	O
input	O
image	O
by	O
50	O
%	O
in	O
each	O
direction	O
,	O
we	O
would	O
expect	O
the	O
number	O
of	O
edges	O
to	O
increase	O
correspondingly	O
.	O
instead	O
,	O
if	O
we	O
increase	O
the	O
size	O
of	O
the	O
pooling	O
regions	O
by	O
50	O
%	O
in	O
each	O
direction	O
to	O
3	O
3	O
,	O
then	O
the	O
mutual	O
exclusivity	O
constraint	O
now	O
speciﬁes	O
that	O
each	O
of	O
these	O
edges	O
may	O
only	O
appear	O
once	O
in	O
a	O
3	O
3	O
region	O
.	O
as	O
we	O
grow	O
a	O
model	B
’	O
s	O
input	O
image	O
in	O
this	O
way	O
,	O
the	O
model	B
generates	O
edges	O
with	O
less	O
density	O
.	O
of	O
course	O
,	O
these	O
issues	O
only	O
arise	O
when	O
the	O
model	B
must	O
use	O
variable	O
amounts	O
of	O
pooling	O
in	O
order	O
to	O
emit	O
a	O
ﬁxed-size	O
output	O
vector	O
.	O
models	O
that	O
use	O
probabilistic	O
max	O
pooling	O
may	O
still	O
accept	O
variable-sized	O
input	O
images	O
so	O
long	O
as	O
the	O
output	O
of	O
the	O
model	B
is	O
a	O
feature	O
map	O
that	O
can	O
scale	O
in	O
size	O
proportional	O
to	O
the	O
input	O
image	O
.	O
lee	O
et	O
al	O
.	O
2009	O
×	O
×	O
(	O
×	O
×	O
pixels	O
at	O
the	O
boundary	O
of	O
the	O
image	O
also	O
pose	O
some	O
diﬃculty	O
,	O
which	O
is	O
exac-	O
erbated	O
by	O
the	O
fact	O
that	O
connections	O
in	O
a	O
boltzmann	O
machine	O
are	O
symmetric	O
.	O
if	O
we	O
do	O
not	O
implicitly	O
zero-pad	O
the	O
input	O
,	O
then	O
there	O
are	O
fewer	O
hidden	O
units	O
than	O
visible	O
units	O
,	O
and	O
the	O
visible	O
units	O
at	O
the	O
boundary	O
of	O
the	O
image	O
are	O
not	O
modeled	O
3the	O
publication	O
describes	O
the	O
model	B
as	O
a	O
“	O
deep	O
belief	O
network	O
”	O
but	O
because	O
it	O
can	O
be	O
described	O
as	O
a	O
purely	O
undirected	O
model	B
with	O
tractable	O
layer-wise	O
mean	O
ﬁeld	O
ﬁxed	O
point	O
updates	O
,	O
it	O
best	O
ﬁts	O
the	O
deﬁnition	O
of	O
a	O
deep	O
boltzmann	O
machine	O
.	O
684	O
chapter	O
20.	O
deep	O
generative	O
models	O
well	O
because	O
they	O
lie	O
in	O
the	O
receptive	O
ﬁeld	O
of	O
fewer	O
hidden	O
units	O
.	O
however	O
,	O
if	O
we	O
do	O
implicitly	O
zero-pad	O
the	O
input	O
,	O
then	O
the	O
hidden	O
units	O
at	O
the	O
boundary	O
are	O
driven	O
by	O
fewer	O
input	O
pixels	O
,	O
and	O
may	O
fail	O
to	O
activate	O
when	O
needed	O
.	O
20.7	O
boltzmann	O
machines	O
for	O
structured	O
or	O
sequential	O
outputs	O
in	O
the	O
structured	O
output	O
scenario	O
,	O
we	O
wish	O
to	O
train	O
a	O
model	B
that	O
can	O
map	O
from	O
some	O
input	O
x	O
to	O
some	O
output	O
y	O
,	O
and	O
the	O
diﬀerent	O
entries	O
of	O
y	O
are	O
related	O
to	O
each	O
other	O
and	O
must	O
obey	O
some	O
constraints	O
.	O
for	O
example	O
,	O
in	O
the	O
speech	O
synthesis	O
task	O
,	O
y	O
is	O
a	O
waveform	O
,	O
and	O
the	O
entire	O
waveform	O
must	O
sound	O
like	O
a	O
coherent	O
utterance	O
.	O
a	O
natural	O
way	O
to	O
represent	O
the	O
relationships	O
between	O
the	O
entries	O
in	O
y	O
is	O
to	O
x	O
)	O
.	O
boltzmann	O
machines	O
,	O
extended	O
to	O
model	B
use	O
a	O
probability	O
distribution	O
p	O
(	O
y	O
conditional	O
distributions	O
,	O
can	O
supply	O
this	O
probabilistic	O
model	B
.	O
|	O
the	O
same	O
tool	O
of	O
conditional	O
modeling	O
with	O
a	O
boltzmann	O
machine	O
can	O
be	O
used	O
not	O
just	O
for	O
structured	O
output	O
tasks	O
,	O
but	O
also	O
for	O
sequence	O
modeling	O
.	O
in	O
the	O
latter	O
case	O
,	O
rather	O
than	O
mapping	O
an	O
input	O
x	O
to	O
an	O
output	O
y	O
,	O
the	O
model	B
must	O
estimate	O
a	O
probability	O
distribution	O
over	O
a	O
sequence	O
of	O
variables	O
,	O
p	O
(	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
)	O
τ	O
)	O
.	O
conditional	O
boltzmann	O
machines	O
can	O
represent	O
factors	O
of	O
the	O
form	O
p	O
(	O
x	O
(	O
)	O
t	O
)	O
in	O
order	O
to	O
accomplish	O
this	O
task	O
.	O
−	O
1	O
)	O
t	O
x	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
|	O
|	O
2007	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
−	O
x	O
(	O
1	O
)	O
t	O
an	O
important	O
sequence	O
modeling	O
task	O
for	O
the	O
video	O
game	O
and	O
ﬁlm	O
industry	O
is	O
modeling	O
sequences	O
of	O
joint	O
angles	O
of	O
skeletons	O
used	O
to	O
render	O
3-d	O
characters	O
.	O
these	O
sequences	O
are	O
often	O
collected	O
using	O
motion	O
capture	O
systems	O
to	O
record	O
the	O
movements	O
of	O
actors	O
.	O
a	O
probabilistic	O
model	B
of	O
a	O
character	O
’	O
s	O
movement	O
allows	O
the	O
generation	O
of	O
new	O
,	O
previously	O
unseen	O
,	O
but	O
realistic	O
animations	O
.	O
to	O
solve	O
et	O
al	O
.	O
(	O
this	O
sequence	O
modeling	O
task	O
,	O
taylor	O
)	O
introduced	O
a	O
conditional	O
rbm	O
−	O
modeling	O
p	O
(	O
x	O
(	O
)	O
t	O
t	O
m	O
)	O
)	O
for	O
small	O
m.	O
the	O
model	B
is	O
an	O
rbm	O
over	O
p	O
(	O
x	O
(	O
)	O
t	O
)	O
whose	O
bias	O
parameters	O
are	O
a	O
linear	O
function	O
of	O
the	O
preceding	O
m	O
values	O
of	O
x	O
.	O
−	O
when	O
we	O
condition	O
on	O
diﬀerent	O
values	O
of	O
x	O
(	O
1	O
)	O
t	O
and	O
earlier	O
variables	O
,	O
we	O
get	O
a	O
new	O
rbm	O
over	O
x	O
.	O
the	O
weights	O
in	O
the	O
rbm	O
over	O
x	O
never	O
change	O
,	O
but	O
by	O
conditioning	O
on	O
diﬀerent	O
past	O
values	O
,	O
we	O
can	O
change	O
the	O
probability	O
of	O
diﬀerent	O
hidden	O
units	O
in	O
the	O
rbm	O
being	O
active	O
.	O
by	O
activating	O
and	O
deactivating	O
diﬀerent	O
subsets	O
of	O
hidden	O
units	O
,	O
we	O
can	O
make	O
large	O
changes	O
to	O
the	O
probability	O
distribution	O
induced	O
on	O
x	O
.	O
other	O
variants	O
of	O
conditional	O
rbm	O
(	O
)	O
and	O
other	O
variants	O
of	O
sequence	O
modeling	O
using	O
conditional	O
rbms	O
are	O
possible	O
(	O
taylor	O
and	O
hinton	O
2009	O
sutskever	O
et	O
al.	O
,	O
2009	O
boulanger-lewandowski	O
mnih	O
et	O
al	O
.	O
2011	O
,	O
et	O
al.	O
,	O
2012	O
)	O
.	O
,	O
;	O
;	O
another	O
sequence	O
modeling	O
task	O
is	O
to	O
model	B
the	O
distribution	O
over	O
sequences	O
685	O
chapter	O
20.	O
deep	O
generative	O
models	O
et	O
al	O
.	O
(	O
2012	O
of	O
musical	O
notes	O
used	O
to	O
compose	O
songs	O
.	O
boulanger-lewandowski	O
)	O
introduced	O
the	O
rnn-rbm	O
sequence	O
model	B
and	O
applied	O
it	O
to	O
this	O
task	O
.	O
the	O
rnn-rbm	O
is	O
a	O
generative	O
model	B
of	O
a	O
sequence	O
of	O
frames	O
x	O
(	O
)	O
t	O
consisting	O
of	O
an	O
rnn	O
that	O
emits	O
the	O
rbm	O
parameters	O
for	O
each	O
time	O
step	O
.	O
unlike	O
previous	O
approaches	O
in	O
which	O
only	O
the	O
bias	O
parameters	O
of	O
the	O
rbm	O
varied	O
from	O
one	O
time	O
step	O
to	O
the	O
next	O
,	O
the	O
rnn-rbm	O
uses	O
the	O
rnn	O
to	O
emit	O
all	O
of	O
the	O
parameters	O
of	O
the	O
rbm	O
,	O
including	O
the	O
weights	O
.	O
to	O
train	O
the	O
model	B
,	O
we	O
need	O
to	O
be	O
able	O
to	O
back-propagate	O
the	O
gradient	O
of	O
the	O
loss	O
function	O
through	O
the	O
rnn	O
.	O
the	O
loss	O
function	O
is	O
not	O
applied	O
directly	O
to	O
the	O
rnn	O
outputs	O
.	O
instead	O
,	O
it	O
is	O
applied	O
to	O
the	O
rbm	O
.	O
this	O
means	O
that	O
we	O
must	O
approximately	O
diﬀerentiate	O
the	O
loss	O
with	O
respect	O
to	O
the	O
rbm	O
parameters	O
using	O
contrastive	O
divergence	O
or	O
a	O
related	O
algorithm	O
.	O
this	O
approximate	O
gradient	O
may	O
then	O
be	O
back-propagated	O
through	O
the	O
rnn	O
using	O
the	O
usual	O
back-propagation	O
through	O
time	O
algorithm	O
.	O
20.8	O
other	O
boltzmann	O
machines	O
many	O
other	O
variants	O
of	O
boltzmann	O
machines	O
are	O
possible	O
.	O
boltzmann	O
machines	O
may	O
be	O
extended	O
with	O
diﬀerent	O
training	O
criteria	O
.	O
we	O
have	O
focused	O
on	O
boltzmann	O
machines	O
trained	O
to	O
approximately	O
maximize	O
the	O
generative	O
criterion	O
log	O
p	O
(	O
v	O
)	O
.	O
it	O
is	O
also	O
possible	O
to	O
train	O
discriminative	O
rbms	O
that	O
aim	O
to	O
maximize	O
log	O
p	O
(	O
y	O
)	O
.	O
this	O
approach	O
often	O
performs	O
the	O
best	O
when	O
using	O
a	O
linear	O
combination	O
of	O
both	O
the	O
generative	O
and	O
the	O
discriminative	O
criteria	O
.	O
unfortunately	O
,	O
rbms	O
do	O
not	O
seem	O
to	O
be	O
as	O
powerful	O
supervised	O
learners	O
as	O
mlps	O
,	O
at	O
least	O
using	O
existing	O
methodology	O
.	O
larochelle	O
and	O
bengio	O
2008	O
v	O
)	O
instead	O
(	O
|	O
,	O
sejnowski	O
1987	O
most	O
boltzmann	O
machines	O
used	O
in	O
practice	O
have	O
only	O
second-order	O
interactions	O
in	O
their	O
energy	O
functions	O
,	O
meaning	O
that	O
their	O
energy	O
functions	O
are	O
the	O
sum	O
of	O
many	O
terms	O
and	O
each	O
individual	O
term	O
only	O
includes	O
the	O
product	O
between	O
two	O
random	O
variables	O
.	O
an	O
example	O
of	O
such	O
a	O
term	O
is	O
viwi	O
,	O
jhj	O
.	O
it	O
is	O
also	O
possible	O
to	O
train	O
higher-order	O
boltzmann	O
machines	O
(	O
)	O
whose	O
energy	O
function	O
terms	O
involve	O
the	O
products	O
between	O
many	O
variables	O
.	O
three-way	O
interactions	O
between	O
a	O
hidden	O
unit	O
and	O
two	O
diﬀerent	O
images	O
can	O
model	B
spatial	O
transformations	O
from	O
one	O
frame	O
of	O
video	O
to	O
the	O
next	O
(	O
memisevic	O
and	O
hinton	O
2007	O
2010	O
)	O
.	O
multiplication	O
by	O
a	O
one-hot	O
class	O
variable	O
can	O
change	O
the	O
relationship	O
between	O
visible	O
and	O
hidden	O
units	O
depending	O
on	O
which	O
class	O
is	O
present	O
(	O
)	O
.	O
one	O
recent	O
example	O
of	O
the	O
use	O
of	O
higher-order	O
interactions	O
is	O
a	O
boltzmann	O
machine	O
with	O
two	O
groups	O
of	O
hidden	O
units	O
,	O
with	O
one	O
group	O
of	O
hidden	O
units	O
that	O
interact	O
with	O
both	O
the	O
visible	O
units	O
v	O
and	O
the	O
class	O
label	O
y	O
,	O
and	O
another	O
group	O
of	O
hidden	O
units	O
that	O
interact	O
only	O
with	O
the	O
v	O
input	O
values	O
(	O
)	O
.	O
this	O
can	O
be	O
interpreted	O
as	O
encouraging	O
nair	O
and	O
hinton	O
2009	O
luo	O
et	O
al	O
.	O
2011	O
,	O
,	O
,	O
,	O
,	O
686	O
chapter	O
20.	O
deep	O
generative	O
models	O
some	O
hidden	O
units	O
to	O
learn	O
to	O
model	B
the	O
input	O
using	O
features	O
that	O
are	O
relevant	O
to	O
the	O
class	O
but	O
also	O
to	O
learn	O
extra	O
hidden	O
units	O
that	O
explain	O
nuisance	O
details	O
that	O
are	O
necessary	O
for	O
the	O
samples	O
of	O
v	O
to	O
be	O
realistic	O
but	O
do	O
not	O
determine	O
the	O
class	O
of	O
the	O
example	O
.	O
another	O
use	O
of	O
higher-order	O
interactions	O
is	O
to	O
gate	O
some	O
features	O
.	O
sohn	O
)	O
introduced	O
a	O
boltzmann	O
machine	O
with	O
third-order	O
interactions	O
with	O
binary	O
mask	O
variables	O
associated	O
with	O
each	O
visible	O
unit	O
.	O
when	O
these	O
masking	O
variables	O
are	O
set	O
to	O
zero	O
,	O
they	O
remove	O
the	O
inﬂuence	O
of	O
a	O
visible	O
unit	O
on	O
the	O
hidden	O
units	O
.	O
this	O
allows	O
visible	O
units	O
that	O
are	O
not	O
relevant	O
to	O
the	O
classiﬁcation	O
problem	O
to	O
be	O
removed	O
from	O
the	O
inference	O
pathway	O
that	O
estimates	O
the	O
class	O
.	O
et	O
al	O
.	O
(	O
2013	O
more	O
generally	O
,	O
the	O
boltzmann	O
machine	O
framework	O
is	O
a	O
rich	O
space	O
of	O
models	O
permitting	O
many	O
more	O
model	B
structures	O
than	O
have	O
been	O
explored	O
so	O
far	O
.	O
developing	O
a	O
new	O
form	O
of	O
boltzmann	O
machine	O
requires	O
some	O
more	O
care	O
and	O
creativity	O
than	O
developing	O
a	O
new	O
neural	O
network	O
layer	O
,	O
because	O
it	O
is	O
often	O
diﬃcult	O
to	O
ﬁnd	O
an	O
energy	O
function	O
that	O
maintains	O
tractability	O
of	O
all	O
of	O
the	O
diﬀerent	O
conditional	O
distributions	O
needed	O
to	O
use	O
the	O
boltzmann	O
machine	O
,	O
but	O
despite	O
this	O
required	O
eﬀort	O
the	O
ﬁeld	O
remains	O
open	O
to	O
innovation	O
.	O
20.9	O
back-propagation	O
through	O
random	O
operations	O
traditional	O
neural	O
networks	O
implement	O
a	O
deterministic	O
transformation	O
of	O
some	O
input	O
variables	O
x.	O
when	O
developing	O
generative	O
models	O
,	O
we	O
often	O
wish	O
to	O
extend	O
neural	O
networks	O
to	O
implement	O
stochastic	O
transformations	O
of	O
x.	O
one	O
straightforward	O
way	O
to	O
do	O
this	O
is	O
to	O
augment	O
the	O
neural	O
network	O
with	O
extra	O
inputs	O
z	O
that	O
are	O
sampled	O
from	O
some	O
simple	O
probability	O
distribution	O
,	O
such	O
as	O
a	O
uniform	O
or	O
gaussian	O
distribution	O
.	O
the	O
neural	O
network	O
can	O
then	O
continue	O
to	O
perform	O
deterministic	O
computation	O
internally	O
,	O
but	O
the	O
function	O
f	O
(	O
x	O
z	O
,	O
)	O
will	O
appear	O
stochastic	O
to	O
an	O
observer	O
who	O
does	O
not	O
have	O
access	O
to	O
z.	O
provided	O
that	O
f	O
is	O
continuous	O
and	O
diﬀerentiable	O
,	O
we	O
can	O
then	O
compute	O
the	O
gradients	O
necessary	O
for	O
training	O
using	O
back-propagation	O
as	O
usual	O
.	O
as	O
an	O
example	O
,	O
let	O
us	O
consider	O
the	O
operation	O
consisting	O
of	O
drawing	O
samples	O
y	O
from	O
a	O
gaussian	O
distribution	O
with	O
mean	O
∼	O
n	O
y	O
µ	O
and	O
variance	O
σ2	O
:	O
(	O
µ	O
,	O
σ2	O
)	O
.	O
(	O
20.54	O
)	O
because	O
an	O
individual	O
sample	O
of	O
y	O
is	O
not	O
produced	O
by	O
a	O
function	O
,	O
but	O
rather	O
by	O
a	O
sampling	O
process	O
whose	O
output	O
changes	O
every	O
time	O
we	O
query	O
it	O
,	O
it	O
may	O
seem	O
counterintuitive	O
to	O
take	O
the	O
derivatives	O
of	O
y	O
with	O
respect	O
to	O
the	O
parameters	O
of	O
its	O
distribution	O
,	O
µ	O
and	O
σ2	O
.	O
however	O
,	O
we	O
can	O
rewrite	O
the	O
sampling	O
process	O
as	O
687	O
chapter	O
20.	O
deep	O
generative	O
models	O
transforming	O
an	O
underlying	O
random	O
value	O
z	O
the	O
desired	O
distribution	O
:	O
∼	O
n	O
(	O
z	O
;	O
0	O
,	O
1	O
)	O
to	O
obtain	O
a	O
sample	O
from	O
y	O
=	O
+	O
µ	O
σz	O
(	O
20.55	O
)	O
we	O
are	O
now	O
able	O
to	O
back-propagate	O
through	O
the	O
sampling	O
operation	O
,	O
by	O
regard-	O
ing	O
it	O
as	O
a	O
deterministic	O
operation	O
with	O
an	O
extra	O
input	O
z.	O
crucially	O
,	O
the	O
extra	O
input	O
is	O
a	O
random	O
variable	O
whose	O
distribution	O
is	O
not	O
a	O
function	O
of	O
any	O
of	O
the	O
variables	O
whose	O
derivatives	O
we	O
want	O
to	O
calculate	O
.	O
the	O
result	O
tells	O
us	O
how	O
an	O
inﬁnitesimal	O
change	O
in	O
µ	O
or	O
σ	O
would	O
change	O
the	O
output	O
if	O
we	O
could	O
repeat	O
the	O
sampling	O
operation	O
again	O
with	O
the	O
same	O
value	O
of	O
z.	O
being	O
able	O
to	O
back-propagate	O
through	O
this	O
sampling	O
operation	O
allows	O
us	O
to	O
incorporate	O
it	O
into	O
a	O
larger	O
graph	O
.	O
we	O
can	O
build	O
elements	O
of	O
the	O
graph	O
on	O
top	O
of	O
the	O
output	O
of	O
the	O
sampling	O
distribution	O
.	O
for	O
example	O
,	O
we	O
can	O
compute	O
the	O
derivatives	O
of	O
some	O
loss	O
function	O
j	O
(	O
y	O
)	O
.	O
we	O
can	O
also	O
build	O
elements	O
of	O
the	O
graph	O
whose	O
outputs	O
are	O
the	O
inputs	O
or	O
the	O
parameters	O
of	O
the	O
sampling	O
operation	O
.	O
for	O
example	O
,	O
we	O
could	O
build	O
a	O
larger	O
graph	O
with	O
µ	O
=	O
f	O
(	O
x	O
;	O
θ	O
)	O
and	O
σ	O
=	O
g	O
(	O
x	O
;	O
θ	O
)	O
.	O
in	O
this	O
augmented	O
graph	O
,	O
we	O
can	O
use	O
back-propagation	O
through	O
these	O
functions	O
to	O
derive	O
∇	O
θj	O
y	O
(	O
)	O
.	O
|	O
the	O
principle	O
used	O
in	O
this	O
gaussian	O
sampling	O
example	O
is	O
more	O
generally	O
appli-	O
x	O
;	O
θ	O
)	O
ω	O
)	O
,	O
where	O
ω	O
is	O
a	O
variable	O
containing	O
both	O
parameters	O
θ	O
,	O
and	O
if	O
applicable	O
,	O
ω	O
)	O
,	O
where	O
ω	O
may	O
in	O
cable	O
.	O
we	O
can	O
express	O
any	O
probability	O
distribution	O
of	O
the	O
form	O
p	O
(	O
y	O
;	O
θ	O
)	O
or	O
p	O
(	O
y	O
as	O
p	O
(	O
y	O
the	O
inputs	O
x.	O
given	O
a	O
value	O
y	O
sampled	O
from	O
distribution	O
p	O
(	O
y	O
turn	O
be	O
a	O
function	O
of	O
other	O
variables	O
,	O
we	O
can	O
rewrite	O
|	O
|	O
∼	O
|	O
y	O
p	O
(	O
ω	O
)	O
as	O
y	O
y	O
=	O
(	O
f	O
z	O
ω	O
;	O
)	O
,	O
(	O
20.56	O
)	O
(	O
20.57	O
)	O
where	O
z	O
is	O
a	O
source	O
of	O
randomness	O
.	O
we	O
may	O
then	O
compute	O
the	O
derivatives	O
of	O
y	O
with	O
respect	O
to	O
ω	O
using	O
traditional	O
tools	O
such	O
as	O
the	O
back-propagation	O
algorithm	O
applied	O
to	O
f	O
,	O
so	O
long	O
as	O
f	O
is	O
continuous	O
and	O
diﬀerentiable	O
almost	O
everywhere	O
.	O
crucially	O
,	O
ω	O
must	O
not	O
be	O
a	O
function	O
of	O
z	O
,	O
and	O
z	O
must	O
not	O
be	O
a	O
function	O
of	O
ω.	O
this	O
technique	O
is	O
often	O
called	O
the	O
reparametrization	O
trick	B
,	O
stochastic	O
back-propagation	O
or	O
perturbation	O
analysis	O
.	O
the	O
requirement	O
that	O
f	O
be	O
continuous	O
and	O
diﬀerentiable	O
of	O
course	O
requires	O
y	O
to	O
be	O
continuous	O
.	O
if	O
we	O
wish	O
to	O
back-propagate	O
through	O
a	O
sampling	O
process	O
that	O
produces	O
discrete-valued	O
samples	O
,	O
it	O
may	O
still	O
be	O
possible	O
to	O
estimate	O
a	O
gradient	O
on	O
ω	O
,	O
using	O
reinforcement	O
learning	O
algorithms	O
such	O
as	O
variants	O
of	O
the	O
reinforce	O
algorithm	O
(	O
)	O
,	O
discussed	O
in	O
section	O
williams	O
1992	O
20.9.1	O
.	O
,	O
688	O
chapter	O
20.	O
deep	O
generative	O
models	O
in	O
neural	O
network	O
applications	O
,	O
we	O
typically	O
choose	O
z	O
to	O
be	O
drawn	O
from	O
some	O
simple	O
distribution	O
,	O
such	O
as	O
a	O
unit	O
uniform	O
or	O
unit	O
gaussian	O
distribution	O
,	O
and	O
achieve	O
more	O
complex	O
distributions	O
by	O
allowing	O
the	O
deterministic	O
portion	O
of	O
the	O
network	O
to	O
reshape	O
its	O
input	O
.	O
,	O
;	O
,	O
price	O
1958	O
bonnet	O
1964	O
the	O
idea	O
of	O
propagating	O
gradients	O
or	O
optimizing	O
through	O
stochastic	O
operations	O
)	O
and	O
was	O
dates	O
back	O
to	O
the	O
mid-twentieth	O
century	O
(	O
williams	O
,	O
ﬁrst	O
used	O
for	O
machine	O
learning	O
in	O
the	O
context	O
of	O
reinforcement	O
learning	O
(	O
1992	O
)	O
.	O
more	O
recently	O
,	O
it	O
has	O
been	O
applied	O
to	O
variational	O
approximations	O
(	O
opper	O
)	O
and	O
stochastic	O
or	O
generative	O
neural	O
networks	O
(	O
bengio	O
and	O
archambeau	O
2009	O
et	O
al.	O
,	O
2014	O
;	O
goodfellow	O
)	O
.	O
many	O
networks	O
,	O
such	O
as	O
denoising	O
autoencoders	O
or	O
networks	O
regularized	O
with	O
dropout	O
,	O
are	O
also	O
naturally	O
designed	O
to	O
take	O
noise	O
as	O
an	O
input	O
without	O
requiring	O
any	O
special	O
reparametrization	O
to	O
make	O
the	O
noise	O
independent	O
from	O
the	O
model	B
.	O
2013b	O
kingma	O
2013	O
kingma	O
and	O
welling	O
2014b	O
a	O
rezende	O
;	O
et	O
al.	O
,	O
et	O
al.	O
,	O
2014c	O
,	O
;	O
,	O
,	O
;	O
,	O
20.9.1	O
back-propagating	O
through	O
discrete	O
stochastic	O
operations	O
when	O
a	O
model	B
emits	O
a	O
discrete	O
variable	O
y	O
,	O
the	O
reparametrization	O
trick	B
is	O
not	O
applicable	O
.	O
suppose	O
that	O
the	O
model	B
takes	O
inputs	O
x	O
and	O
parameters	O
θ	O
,	O
both	O
encapsulated	O
in	O
the	O
vector	O
ω	O
,	O
and	O
combines	O
them	O
with	O
random	O
noise	O
z	O
to	O
produce	O
y	O
:	O
y	O
=	O
(	O
f	O
z	O
ω	O
;	O
)	O
.	O
(	O
20.58	O
)	O
because	O
y	O
is	O
discrete	O
,	O
f	O
must	O
be	O
a	O
step	O
function	O
.	O
the	O
derivatives	O
of	O
a	O
step	O
function	O
are	O
not	O
useful	O
at	O
any	O
point	O
.	O
right	O
at	O
each	O
step	O
boundary	O
,	O
the	O
derivatives	O
are	O
undeﬁned	O
,	O
but	O
that	O
is	O
a	O
small	O
problem	O
.	O
the	O
large	O
problem	O
is	O
that	O
the	O
derivatives	O
are	O
zero	O
almost	O
everywhere	O
,	O
on	O
the	O
regions	O
between	O
step	O
boundaries	O
.	O
the	O
derivatives	O
of	O
any	O
cost	O
function	O
j	O
(	O
y	O
)	O
therefore	O
do	O
not	O
give	O
any	O
information	O
for	O
how	O
to	O
update	O
the	O
model	B
parameters	O
.θ	O
×	O
×	O
the	O
reinforce	O
algorithm	O
(	O
reward	O
increment	O
=	O
non-negative	O
factor	O
characteristic	O
eligibility	O
)	O
provides	O
a	O
framework	O
deﬁning	O
a	O
oﬀset	O
reinforcement	O
)	O
.	O
the	O
core	O
idea	O
is	O
that	O
family	O
of	O
simple	O
but	O
powerful	O
solutions	O
(	O
even	O
though	O
j	O
(	O
f	O
(	O
z	O
;	O
ω	O
)	O
)	O
is	O
a	O
step	O
function	O
with	O
useless	O
derivatives	O
,	O
the	O
expected	O
∼	O
z	O
ω	O
is	O
often	O
a	O
smooth	O
function	O
amenable	O
to	O
gradient	O
descent	B
.	O
p	O
(	O
)	O
j	O
f	O
(	O
(	O
;	O
cost	O
ez	O
although	O
that	O
expectation	O
is	O
typically	O
not	O
tractable	O
when	O
y	O
is	O
high-dimensional	O
(	O
or	O
is	O
the	O
result	O
of	O
the	O
composition	O
of	O
many	O
discrete	O
stochastic	O
decisions	O
)	O
,	O
it	O
can	O
be	O
estimated	O
without	O
bias	O
using	O
a	O
monte	O
carlo	O
average	O
.	O
the	O
stochastic	O
estimate	O
of	O
the	O
gradient	O
can	O
be	O
used	O
with	O
sgd	O
or	O
other	O
stochastic	O
gradient-based	O
optimization	O
techniques	O
.	O
williams	O
1992	O
)	O
)	O
,	O
z	O
689	O
chapter	O
20.	O
deep	O
generative	O
models	O
the	O
simplest	O
version	O
of	O
reinforce	O
can	O
be	O
derived	O
by	O
simply	O
diﬀerentiating	O
the	O
expected	O
cost	O
:	O
ez	O
[	O
(	O
)	O
]	O
=	O
j	O
y	O
∂	O
je	O
[	O
(	O
)	O
]	O
y	O
∂ω	O
=	O
=	O
j	O
p	O
(	O
)	O
y	O
(	O
)	O
y	O
j	O
(	O
)	O
y	O
∂p	O
(	O
)	O
y	O
∂ω	O
	O
p	O
(	O
)	O
y	O
(	O
)	O
y	O
∂	O
log	O
(	O
)	O
y	O
p	O
∂ω	O
	O
	O
	O
y	O
y	O
j	O
y	O
≈	O
1	O
m	O
y	O
(	O
)	O
i	O
(	O
20.59	O
)	O
(	O
20.60	O
)	O
(	O
20.61	O
)	O
(	O
20.62	O
)	O
m	O
∼	O
p	O
(	O
)	O
y	O
=1	O
,	O
i	O
∂	O
j	O
(	O
y	O
(	O
)	O
i	O
)	O
log	O
(	O
y	O
(	O
)	O
i	O
)	O
p	O
∂ω	O
.	O
20.60	O
relies	O
on	O
the	O
assumption	O
that	O
equation	O
trivial	O
to	O
extend	O
the	O
approach	O
to	O
relax	O
this	O
assumption	O
.	O
equation	O
the	O
derivative	O
rule	O
for	O
the	O
logarithm	O
,	O
∂	O
=	O
1	O
p	O
(	O
)	O
y	O
an	O
unbiased	O
monte	O
carlo	O
estimator	O
of	O
the	O
gradient	O
.	O
j	O
does	O
not	O
reference	O
ω	O
directly	O
.	O
it	O
is	O
exploits	O
gives	O
.	O
equation	O
20.61	O
log	O
(	O
)	O
y	O
p	O
∂ω	O
∂p	O
(	O
)	O
y	O
∂ω	O
anywhere	O
we	O
write	O
p	O
(	O
y	O
)	O
in	O
this	O
section	O
,	O
one	O
could	O
equally	O
write	O
p	O
(	O
y	O
x	O
)	O
.	O
this	O
is	O
because	O
p	O
(	O
y	O
)	O
is	O
parametrized	O
by	O
ω	O
,	O
and	O
ω	O
contains	O
both	O
θ	O
and	O
x	O
,	O
if	O
x	O
is	O
present	O
.	O
20.62	O
|	O
one	O
issue	O
with	O
the	O
above	O
simple	O
reinforce	O
estimator	O
is	O
that	O
it	O
has	O
a	O
very	O
high	O
variance	O
,	O
so	O
that	O
many	O
samples	O
of	O
y	O
need	O
to	O
be	O
drawn	O
to	O
obtain	O
a	O
good	O
estimator	O
of	O
the	O
gradient	O
,	O
or	O
equivalently	O
,	O
if	O
only	O
one	O
sample	O
is	O
drawn	O
,	O
sgd	O
will	O
converge	O
very	O
slowly	O
and	O
will	O
require	O
a	O
smaller	O
learning	O
rate	O
.	O
it	O
is	O
possible	O
to	O
considerably	O
reduce	O
the	O
variance	O
of	O
that	O
estimator	O
by	O
using	O
variance	O
reduction	O
methods	O
(	O
)	O
.	O
the	O
idea	O
is	O
to	O
modify	O
the	O
estimator	O
so	O
that	O
its	O
expected	O
value	O
remains	O
unchanged	O
but	O
its	O
variance	O
get	O
reduced	O
.	O
in	O
the	O
context	O
of	O
reinforce	O
,	O
the	O
proposed	O
variance	O
reduction	O
methods	O
involve	O
the	O
computation	O
of	O
a	O
baseline	O
that	O
is	O
used	O
to	O
oﬀset	O
j	O
(	O
y	O
)	O
.	O
note	O
that	O
any	O
oﬀset	O
b	O
(	O
ω	O
)	O
that	O
does	O
not	O
depend	O
on	O
y	O
would	O
not	O
change	O
the	O
expectation	O
of	O
the	O
estimated	O
gradient	O
because	O
wilson	O
1984	O
l	O
’	O
ecuyer	O
1994	O
	O
	O
,	O
,	O
;	O
ep	O
(	O
)	O
y	O
∂	O
log	O
(	O
)	O
y	O
p	O
∂ω	O
=	O
=	O
=	O
∂	O
log	O
(	O
)	O
y	O
p	O
∂ω	O
p	O
(	O
)	O
y	O
	O
∂p	O
(	O
)	O
y	O
∂ω	O
p	O
(	O
)	O
=y	O
∂	O
∂ω	O
1	O
=	O
0	O
,	O
y	O
(	O
20.63	O
)	O
(	O
20.64	O
)	O
(	O
20.65	O
)	O
	O
	O
y	O
y	O
∂	O
∂ω	O
690	O
chapter	O
20.	O
deep	O
generative	O
models	O
	O
	O
which	O
means	O
that	O
−	O
ep	O
(	O
)	O
y	O
(	O
(	O
)	O
j	O
y	O
(	O
b	O
ω	O
)	O
)	O
∂	O
log	O
(	O
)	O
y	O
p	O
∂ω	O
=	O
ep	O
(	O
)	O
y	O
j	O
(	O
)	O
y	O
∂	O
log	O
(	O
)	O
y	O
p	O
∂ω	O
	O
	O
b	O
e	O
(	O
)	O
ω	O
p	O
(	O
)	O
y	O
∂	O
log	O
(	O
)	O
y	O
p	O
∂ω	O
(	O
20.66	O
)	O
	O
	O
−	O
.	O
	O
	O
	O
	O
(	O
20.67	O
)	O
−	O
(	O
20.68	O
)	O
(	O
20.69	O
)	O
furthermore	O
,	O
we	O
can	O
obtain	O
the	O
optimal	O
b	O
(	O
ω	O
)	O
by	O
computing	O
the	O
variance	O
of	O
(	O
j	O
(	O
y	O
)	O
b	O
(	O
ω	O
)	O
)	O
∂	O
∗	O
that	O
this	O
optimal	O
baseline	O
b	O
under	O
p	O
(	O
y	O
)	O
and	O
minimizing	O
with	O
respect	O
to	O
b	O
(	O
ω	O
)	O
.	O
what	O
we	O
ﬁnd	O
is	O
:	O
ω	O
(	O
)	O
ω	O
i	O
is	O
diﬀerent	O
for	O
each	O
element	O
ωi	O
of	O
the	O
vector	O
log	O
(	O
)	O
y	O
p	O
∂ω	O
=	O
ep	O
(	O
)	O
y	O
j	O
(	O
)	O
y	O
∂	O
log	O
(	O
)	O
y	O
p	O
∂ω	O
	O
	O
∗	O
(	O
)	O
ω	O
i	O
=	O
b	O
ep	O
(	O
)	O
y	O
j	O
(	O
)	O
y	O
∂	O
2	O
log	O
(	O
)	O
y	O
p	O
∂ωi	O
ep	O
(	O
)	O
y	O
2	O
∂	O
log	O
(	O
)	O
y	O
p	O
∂ωi	O
.	O
the	O
gradient	O
estimator	O
with	O
respect	O
to	O
ωi	O
then	O
becomes	O
(	O
(	O
)	O
j	O
y	O
(	O
b	O
ω	O
i	O
)	O
)	O
−	O
	O
	O
∂	O
p	O
log	O
(	O
)	O
y	O
∂ωi	O
2	O
∂	O
log	O
(	O
)	O
y	O
p	O
∂ω	O
i	O
]	O
and	O
ep	O
(	O
)	O
y	O
∗	O
where	O
b	O
(	O
ω	O
)	O
i	O
estimates	O
the	O
above	O
b	O
(	O
ω	O
)	O
i.	O
the	O
estimate	O
b	O
is	O
usually	O
obtained	O
by	O
adding	O
extra	O
outputs	O
to	O
the	O
neural	O
network	O
and	O
training	O
the	O
new	O
outputs	O
to	O
estimate	O
ep	O
(	O
)	O
y	O
[	O
j	O
(	O
y	O
)	O
∂	O
for	O
each	O
element	O
of	O
ω.	O
these	O
extra	O
outputs	O
can	O
be	O
trained	O
with	O
the	O
mean	O
squared	O
error	O
objective	O
,	O
using	O
respectively	O
j	O
(	O
y	O
)	O
∂	O
as	O
targets	O
when	O
y	O
is	O
sampled	O
from	O
p	O
(	O
y	O
)	O
,	O
for	O
a	O
given	O
ω.	O
the	O
estimate	O
b	O
may	O
then	O
be	O
recovered	O
by	O
substituting	O
these	O
estimates	O
into	O
≈	O
)	O
preferred	O
to	O
use	O
a	O
single	O
shared	O
output	O
equation	O
(	O
across	O
all	O
elements	O
i	O
of	O
ω	O
)	O
trained	O
with	O
the	O
target	O
j	O
(	O
y	O
)	O
,	O
using	O
as	O
baseline	O
b	O
(	O
ω	O
)	O
ep	O
(	O
)	O
y	O
[	O
(	O
)	O
]	O
j	O
y	O
.	O
20.68	O
mnih	O
and	O
gregor	O
2014	O
p	O
log	O
(	O
)	O
y	O
∂ωi	O
log	O
(	O
)	O
y	O
p	O
∂ωi	O
and	O
∂	O
log	O
(	O
)	O
y	O
p	O
∂ωi	O
(	O
2	O
2	O
2	O
.	O
;	O
,	O
(	O
(	O
)	O
,	O
et	O
al	O
.	O
(	O
variance	O
reduction	O
methods	O
have	O
been	O
introduced	O
in	O
the	O
reinforcement	O
learning	O
)	O
,	O
generalizing	O
previous	O
work	B
sutton	O
et	O
al	O
.	O
2000	O
weaver	O
and	O
tao	O
2001	O
context	O
(	O
,	O
bengio	O
2013b	O
mnih	O
on	O
the	O
case	O
of	O
binary	O
reward	O
by	O
dayan	O
1990	O
)	O
.	O
see	O
and	O
gregor	O
2014	O
ba	O
)	O
for	O
)	O
,	O
or	O
et	O
al	O
.	O
(	O
examples	O
of	O
modern	O
uses	O
of	O
the	O
reinforce	O
algorithm	O
with	O
reduced	O
variance	O
in	O
the	O
context	O
of	O
deep	O
learning	O
.	O
in	O
addition	O
to	O
the	O
use	O
of	O
an	O
input-dependent	O
baseline	O
b	O
(	O
ω	O
)	O
b	O
(	O
ω	O
)	O
)	O
could	O
be	O
,	O
adjusted	O
during	O
training	O
by	O
dividing	O
it	O
by	O
its	O
standard	O
deviation	O
estimated	O
by	O
a	O
moving	O
average	O
during	O
training	O
,	O
as	O
a	O
kind	O
of	O
adaptive	O
learning	O
rate	O
,	O
to	O
counter	O
the	O
eﬀect	O
of	O
important	O
variations	O
that	O
occur	O
during	O
the	O
course	O
of	O
training	O
in	O
the	O
)	O
found	O
that	O
the	O
scale	O
of	O
mnih	O
and	O
gregor	O
2014	O
et	O
al	O
.	O
(	O
xu	O
2014	O
mnih	O
et	O
al	O
.	O
(	O
j	O
(	O
y	O
)	O
2014	O
2015	O
−	O
)	O
,	O
)	O
,	O
(	O
(	O
691	O
chapter	O
20.	O
deep	O
generative	O
models	O
magnitude	O
of	O
this	O
quantity	O
.	O
normalization	O
.	O
mnih	O
and	O
gregor	O
2014	O
(	O
)	O
called	O
this	O
heuristic	O
variance	O
reinforce-based	O
estimators	O
can	O
be	O
understood	O
as	O
estimating	O
the	O
gradient	O
by	O
correlating	O
choices	O
of	O
y	O
with	O
corresponding	O
values	O
of	O
j	O
(	O
y	O
)	O
.	O
if	O
a	O
good	O
value	O
of	O
y	O
is	O
unlikely	O
under	O
the	O
current	O
parametrization	O
,	O
it	O
might	O
take	O
a	O
long	O
time	O
to	O
obtain	O
it	O
by	O
chance	O
,	O
and	O
get	O
the	O
required	O
signal	O
that	O
this	O
conﬁguration	O
should	O
be	O
reinforced	O
.	O
20.10	O
directed	O
generative	O
nets	O
16	O
as	O
discussed	O
in	O
chapter	O
,	O
directed	O
graphical	O
models	O
make	O
up	O
a	O
prominent	O
class	O
of	O
graphical	O
models	O
.	O
while	O
directed	O
graphical	O
models	O
have	O
been	O
very	O
popular	O
within	O
the	O
greater	O
machine	O
learning	O
community	O
,	O
within	O
the	O
smaller	O
deep	O
learning	O
community	O
they	O
have	O
until	O
roughly	O
2013	O
been	O
overshadowed	O
by	O
undirected	O
models	O
such	O
as	O
the	O
rbm	O
.	O
in	O
this	O
section	O
we	O
review	O
some	O
of	O
the	O
standard	O
directed	O
graphical	O
models	O
that	O
have	O
traditionally	O
been	O
associated	O
with	O
the	O
deep	O
learning	O
community	O
.	O
we	O
have	O
already	O
described	O
deep	O
belief	O
networks	O
,	O
which	O
are	O
a	O
partially	O
directed	O
model	B
.	O
we	O
have	O
also	O
already	O
described	O
sparse	O
coding	O
models	O
,	O
which	O
can	O
be	O
thought	O
of	O
as	O
shallow	O
directed	O
generative	O
models	O
.	O
they	O
are	O
often	O
used	O
as	O
feature	O
learners	O
in	O
the	O
context	O
of	O
deep	O
learning	O
,	O
though	O
they	O
tend	O
to	O
perform	O
poorly	O
at	O
sample	O
generation	O
and	O
density	O
estimation	O
.	O
we	O
now	O
describe	O
a	O
variety	O
of	O
deep	O
,	O
fully	O
directed	O
models	O
.	O
20.10.1	O
sigmoid	O
belief	O
nets	O
neal	O
1990	O
sigmoid	O
belief	O
networks	O
(	O
)	O
are	O
a	O
simple	O
form	O
of	O
directed	O
graphical	O
model	B
with	O
a	O
speciﬁc	O
kind	O
of	O
conditional	O
probability	O
distribution	O
.	O
in	O
general	O
,	O
we	O
can	O
think	O
of	O
a	O
sigmoid	O
belief	O
network	O
as	O
having	O
a	O
vector	O
of	O
binary	O
states	O
s	O
,	O
with	O
each	O
element	O
of	O
the	O
state	O
inﬂuenced	O
by	O
its	O
ancestors	O
:	O
,	O
	O
	O
p	O
s	O
(	O
i	O
)	O
=	O
σ	O
wj	O
,	O
isj	O
+	O
bi	O
.	O
(	O
20.70	O
)	O
j	O
<	O
i	O
the	O
most	O
common	O
structure	O
of	O
sigmoid	O
belief	O
network	O
is	O
one	O
that	O
is	O
divided	O
into	O
many	O
layers	O
,	O
with	O
ancestral	O
sampling	O
proceeding	O
through	O
a	O
series	O
of	O
many	O
hidden	O
layers	O
and	O
then	O
ultimately	O
generating	O
the	O
visible	O
layer	O
.	O
this	O
structure	O
is	O
very	O
similar	O
to	O
the	O
deep	O
belief	O
network	O
,	O
except	O
that	O
the	O
units	O
at	O
the	O
beginning	O
of	O
692	O
chapter	O
20.	O
deep	O
generative	O
models	O
the	O
sampling	O
process	O
are	O
independent	O
from	O
each	O
other	O
,	O
rather	O
than	O
sampled	O
from	O
a	O
restricted	O
boltzmann	O
machine	O
.	O
such	O
a	O
structure	O
is	O
interesting	O
for	O
a	O
variety	O
of	O
reasons	O
.	O
one	O
reason	O
is	O
that	O
the	O
structure	O
is	O
a	O
universal	O
approximator	O
of	O
probability	O
distributions	O
over	O
the	O
visible	O
units	O
,	O
in	O
the	O
sense	O
that	O
it	O
can	O
approximate	O
any	O
probability	O
distribution	O
over	O
binary	O
variables	O
arbitrarily	O
well	O
,	O
given	O
enough	O
depth	O
,	O
even	O
if	O
the	O
width	O
of	O
the	O
individual	O
layers	O
is	O
restricted	O
to	O
the	O
dimensionality	O
of	O
the	O
visible	O
layer	O
(	O
sutskever	O
and	O
hinton	O
2008	O
)	O
.	O
,	O
while	O
generating	O
a	O
sample	O
of	O
the	O
visible	O
units	O
is	O
very	O
eﬃcient	O
in	O
a	O
sigmoid	O
belief	O
network	O
,	O
most	O
other	O
operations	O
are	O
not	O
.	O
inference	O
over	O
the	O
hidden	O
units	O
given	O
the	O
visible	O
units	O
is	O
intractable	O
.	O
mean	O
ﬁeld	O
inference	O
is	O
also	O
intractable	O
because	O
the	O
variational	O
lower	O
bound	B
involves	O
taking	O
expectations	O
of	O
cliques	O
that	O
encompass	O
entire	O
layers	O
.	O
this	O
problem	O
has	O
remained	O
diﬃcult	O
enough	O
to	O
restrict	O
the	O
popularity	O
of	O
directed	O
discrete	O
networks	O
.	O
et	O
al.	O
,	O
;	O
,	O
19.5	O
1995	O
dayan	O
and	O
hinton	O
1996	O
one	O
approach	O
for	O
performing	O
inference	O
in	O
a	O
sigmoid	O
belief	O
network	O
is	O
to	O
construct	O
a	O
diﬀerent	O
lower	O
bound	B
that	O
is	O
specialized	O
for	O
sigmoid	O
belief	O
networks	O
(	O
saul	O
et	O
al	O
.	O
,	O
1996	O
)	O
.	O
this	O
approach	O
has	O
only	O
been	O
applied	O
to	O
very	O
small	O
networks	O
.	O
another	O
approach	O
is	O
to	O
use	O
learned	O
inference	O
mechanisms	O
as	O
described	O
in	O
section	O
.	O
the	O
helmholtz	O
machine	O
(	O
dayan	O
)	O
is	O
a	O
sigmoid	O
belief	O
network	O
combined	O
with	O
an	O
inference	O
network	O
that	O
predicts	O
the	O
parameters	O
of	O
the	O
mean	O
ﬁeld	O
distribution	O
over	O
the	O
hidden	O
units	O
.	O
modern	O
approaches	O
(	O
gregor	O
et	O
al	O
.	O
,	O
2014	O
mnih	O
and	O
gregor	O
2014	O
)	O
to	O
sigmoid	O
belief	O
networks	O
still	O
use	O
this	O
inference	O
network	O
approach	O
.	O
these	O
techniques	O
remain	O
diﬃcult	O
due	O
to	O
the	O
discrete	O
nature	O
of	O
the	O
latent	O
variables	O
.	O
one	O
can	O
not	O
simply	O
back-propagate	O
through	O
the	O
output	O
of	O
the	O
inference	O
network	O
,	O
but	O
instead	O
must	O
use	O
the	O
relatively	O
unreliable	O
machinery	O
for	O
back-	O
propagating	O
through	O
discrete	O
sampling	O
processes	O
,	O
described	O
in	O
section	O
.	O
recent	O
approaches	O
based	O
on	O
importance	O
sampling	O
,	O
reweighted	O
wake-sleep	O
(	O
bornschein	O
and	O
bengio	O
2015	O
2015	O
)	O
make	O
it	O
possible	O
to	O
quickly	O
train	O
sigmoid	O
belief	O
networks	O
and	O
reach	O
state-of-the-art	O
performance	O
on	O
benchmark	O
tasks.	O
)	O
and	O
bidirectional	O
helmholtz	O
machines	O
(	O
20.9.1	O
bornschein	O
et	O
al.	O
,	O
;	O
,	O
,	O
a	O
special	O
case	O
of	O
sigmoid	O
belief	O
networks	O
is	O
the	O
case	O
where	O
there	O
are	O
no	O
latent	O
variables	O
.	O
learning	O
in	O
this	O
case	O
is	O
eﬃcient	O
,	O
because	O
there	O
is	O
no	O
need	O
to	O
marginalize	O
latent	O
variables	O
out	O
of	O
the	O
likelihood	O
.	O
a	O
family	O
of	O
models	O
called	O
auto-regressive	O
networks	O
generalize	O
this	O
fully	O
visible	O
belief	O
network	O
to	O
other	O
kinds	O
of	O
variables	O
besides	O
binary	O
variables	O
and	O
other	O
structures	O
of	O
conditional	O
distributions	O
besides	O
log-	O
linear	O
relationships	O
.	O
auto-regressive	O
networks	O
are	O
described	O
later	O
,	O
in	O
section	O
20.10.7	O
.	O
693	O
chapter	O
20.	O
deep	O
generative	O
models	O
20.10.2	O
diﬀerentiable	O
generator	O
nets	O
many	O
generative	O
models	O
are	O
based	O
on	O
the	O
idea	O
of	O
using	O
a	O
diﬀerentiable	O
generator	O
network	O
.	O
the	O
model	B
transforms	O
samples	O
of	O
latent	O
variables	O
z	O
to	O
samples	O
x	O
or	O
to	O
distributions	O
over	O
samples	O
x	O
using	O
a	O
diﬀerentiable	O
function	O
g	O
(	O
z	O
;	O
θ	O
(	O
)	O
g	O
)	O
which	O
is	O
typically	O
represented	O
by	O
a	O
neural	O
network	O
.	O
this	O
model	B
class	O
includes	O
variational	O
autoencoders	O
,	O
which	O
pair	O
the	O
generator	O
net	O
with	O
an	O
inference	O
net	O
,	O
generative	O
adversarial	O
networks	O
,	O
which	O
pair	O
the	O
generator	O
network	O
with	O
a	O
discriminator	O
network	O
,	O
and	O
techniques	O
that	O
train	O
generator	O
networks	O
in	O
isolation	O
.	O
generator	O
networks	O
are	O
essentially	O
just	O
parametrized	O
computational	O
procedures	O
for	O
generating	O
samples	O
,	O
where	O
the	O
architecture	O
provides	O
the	O
family	O
of	O
possible	O
distributions	O
to	O
sample	O
from	O
and	O
the	O
parameters	O
select	O
a	O
distribution	O
from	O
within	O
that	O
family	O
.	O
as	O
an	O
example	O
,	O
the	O
standard	O
procedure	O
for	O
drawing	O
samples	O
from	O
a	O
normal	O
distribution	O
with	O
mean	O
µ	O
and	O
covariance	O
σ	O
is	O
to	O
feed	O
samples	O
z	O
from	O
a	O
normal	O
distribution	O
with	O
zero	O
mean	O
and	O
identity	O
covariance	O
into	O
a	O
very	O
simple	O
generator	O
network	O
.	O
this	O
generator	O
network	O
contains	O
just	O
one	O
aﬃne	O
layer	O
:	O
x	O
=	O
(	O
g	O
z	O
)	O
=	O
+µ	O
lz	O
(	O
20.71	O
)	O
where	O
	O
l	O
is	O
given	O
by	O
the	O
cholesky	O
decomposition	O
of	O
.	O
σ	O
pseudorandom	O
number	O
generators	O
can	O
also	O
use	O
nonlinear	O
transformations	O
of	O
simple	O
distributions	O
.	O
for	O
example	O
,	O
inverse	O
transform	O
sampling	O
(	O
devroye	O
2013	O
)	O
draws	O
a	O
scalar	O
z	O
from	O
u	O
(	O
0	O
,	O
1	O
)	O
and	O
applies	O
a	O
nonlinear	O
transformation	O
to	O
a	O
scalar	O
x.	O
in	O
this	O
case	O
g	O
(	O
z	O
)	O
is	O
given	O
by	O
the	O
inverse	O
of	O
the	O
cumulative	O
distribution	O
function	O
x−∞	O
p	O
(	O
v	O
)	O
dv	O
.	O
if	O
we	O
are	O
able	O
to	O
specify	O
p	O
(	O
x	O
)	O
,	O
integrate	O
over	O
x	O
,	O
and	O
invert	O
the	O
f	O
(	O
x	O
)	O
=	O
resulting	O
function	O
,	O
we	O
can	O
sample	O
from	O
without	O
using	O
machine	O
learning	O
.	O
p	O
x	O
(	O
)	O
,	O
to	O
generate	O
samples	O
from	O
more	O
complicated	O
distributions	O
that	O
are	O
diﬃcult	O
to	O
specify	O
directly	O
,	O
diﬃcult	O
to	O
integrate	O
over	O
,	O
or	O
whose	O
resulting	O
integrals	O
are	O
diﬃcult	O
to	O
invert	O
,	O
we	O
use	O
a	O
feedforward	O
network	O
to	O
represent	O
a	O
parametric	O
family	O
of	O
nonlinear	O
functions	O
g	O
,	O
and	O
use	O
training	O
data	O
to	O
infer	O
the	O
parameters	O
selecting	O
the	O
desired	O
function	O
.	O
we	O
can	O
think	O
of	O
g	O
as	O
providing	O
a	O
nonlinear	O
change	O
of	O
variables	O
that	O
transforms	O
the	O
distribution	O
over	O
z	O
into	O
the	O
desired	O
distribution	O
over	O
.	O
x	O
recall	O
from	O
equation	O
3.47	O
that	O
,	O
for	O
invertible	O
,	O
diﬀerentiable	O
,	O
continuous	O
g	O
,	O
	O
	O
pz	O
(	O
)	O
=	O
z	O
px	O
(	O
(	O
)	O
)	O
g	O
z	O
det	O
(	O
∂g	O
∂z	O
)	O
.	O
(	O
20.72	O
)	O
694	O
chapter	O
20.	O
deep	O
generative	O
models	O
this	O
implicitly	O
imposes	O
a	O
probability	O
distribution	O
over	O
:	O
x	O
	O
	O
−	O
1	O
(	O
)	O
)	O
x	O
pz	O
(	O
g	O
det	O
(	O
∂g	O
∂z	O
)	O
px	O
(	O
)	O
=x	O
.	O
(	O
20.73	O
)	O
of	O
course	O
,	O
this	O
formula	O
may	O
be	O
diﬃcult	O
to	O
evaluate	O
,	O
depending	O
on	O
the	O
choice	O
of	O
g	O
,	O
so	O
we	O
often	O
use	O
indirect	O
means	O
of	O
learning	O
g	O
,	O
rather	O
than	O
trying	O
to	O
maximize	O
log	O
(	O
)	O
p	O
x	O
directly	O
.	O
in	O
some	O
cases	O
,	O
rather	O
than	O
using	O
g	O
to	O
provide	O
a	O
sample	O
of	O
x	O
directly	O
,	O
we	O
use	O
g	O
to	O
deﬁne	O
a	O
conditional	O
distribution	O
over	O
x.	O
for	O
example	O
,	O
we	O
could	O
use	O
a	O
generator	O
net	O
whose	O
ﬁnal	O
layer	O
consists	O
of	O
sigmoid	O
outputs	O
to	O
provide	O
the	O
mean	O
parameters	O
of	O
bernoulli	O
distributions	O
:	O
|	O
p	O
(	O
xi	O
=	O
1	O
)	O
=	O
(	O
)	O
|	O
in	O
this	O
case	O
,	O
when	O
we	O
use	O
g	O
to	O
deﬁne	O
p	O
(	O
x	O
z	O
marginalizing	O
:	O
z	O
z	O
g	O
z	O
i	O
.	O
(	O
20.74	O
)	O
)	O
,	O
we	O
impose	O
a	O
distribution	O
over	O
x	O
by	O
|	O
x	O
z	O
(	O
ezp	O
(	O
20.75	O
)	O
.	O
)	O
p	O
(	O
)	O
=	O
x	O
both	O
approaches	O
deﬁne	O
a	O
distribution	O
pg	O
(	O
x	O
)	O
and	O
allow	O
us	O
to	O
train	O
various	O
criteria	O
of	O
pg	O
using	O
the	O
reparametrization	O
trick	B
of	O
section	O
20.9	O
.	O
the	O
two	O
diﬀerent	O
approaches	O
to	O
formulating	O
generator	O
nets—emitting	O
the	O
parameters	O
of	O
a	O
conditional	O
distribution	O
versus	O
directly	O
emitting	O
samples—have	O
complementary	O
strengths	O
and	O
weaknesses	O
.	O
when	O
the	O
generator	O
net	O
deﬁnes	O
a	O
conditional	O
distribution	O
over	O
x	O
,	O
it	O
is	O
capable	O
of	O
generating	O
discrete	O
data	O
as	O
well	O
as	O
continuous	O
data	O
.	O
when	O
the	O
generator	O
net	O
provides	O
samples	O
directly	O
,	O
it	O
is	O
capable	O
of	O
generating	O
only	O
continuous	O
data	O
(	O
we	O
could	O
introduce	O
discretization	O
in	O
the	O
forward	O
propagation	O
,	O
but	O
doing	O
so	O
would	O
mean	O
the	O
model	B
could	O
no	O
longer	O
be	O
trained	O
using	O
back-propagation	O
)	O
.	O
the	O
advantage	O
to	O
direct	O
sampling	O
is	O
that	O
we	O
are	O
no	O
longer	O
forced	O
to	O
use	O
conditional	O
distributions	O
whose	O
form	O
can	O
be	O
easily	O
written	O
down	O
and	O
algebraically	O
manipulated	O
by	O
a	O
human	O
designer	O
.	O
approaches	O
based	O
on	O
diﬀerentiable	O
generator	O
networks	O
are	O
motivated	O
by	O
the	O
success	O
of	O
gradient	O
descent	B
applied	O
to	O
diﬀerentiable	O
feedforward	O
networks	O
for	O
classiﬁcation	O
.	O
in	O
the	O
context	O
of	O
supervised	O
learning	O
,	O
deep	O
feedforward	O
networks	O
trained	O
with	O
gradient-based	O
learning	O
seem	O
practically	O
guaranteed	O
to	O
succeed	O
given	O
enough	O
hidden	O
units	O
and	O
enough	O
training	O
data	O
.	O
can	O
this	O
same	O
recipe	O
for	O
success	O
transfer	O
to	O
generative	O
modeling	O
?	O
generative	O
modeling	O
seems	O
to	O
be	O
more	O
diﬃcult	O
than	O
classiﬁcation	O
or	O
regression	O
because	O
the	O
learning	O
process	O
requires	O
optimizing	O
intractable	O
criteria	O
.	O
in	O
the	O
context	O
695	O
chapter	O
20.	O
deep	O
generative	O
models	O
of	O
diﬀerentiable	O
generator	O
nets	O
,	O
the	O
criteria	O
are	O
intractable	O
because	O
the	O
data	O
does	O
not	O
specify	O
both	O
the	O
inputs	O
z	O
and	O
the	O
outputs	O
x	O
of	O
the	O
generator	O
net	O
.	O
in	O
the	O
case	O
of	O
supervised	O
learning	O
,	O
both	O
the	O
inputs	O
x	O
and	O
the	O
outputs	O
y	O
were	O
given	O
,	O
and	O
the	O
optimization	O
procedure	O
needs	O
only	O
to	O
learn	O
how	O
to	O
produce	O
the	O
speciﬁed	O
mapping	O
.	O
in	O
the	O
case	O
of	O
generative	O
modeling	O
,	O
the	O
learning	O
procedure	O
needs	O
to	O
determine	O
how	O
to	O
arrange	O
space	O
in	O
a	O
useful	O
way	O
and	O
additionally	O
how	O
to	O
map	O
from	O
to	O
.	O
x	O
z	O
z	O
2015	O
et	O
al	O
.	O
(	O
dosovitskiy	O
)	O
studied	O
a	O
simpliﬁed	O
problem	O
,	O
where	O
the	O
correspondence	O
between	O
z	O
and	O
x	O
is	O
given	O
.	O
speciﬁcally	O
,	O
the	O
training	O
data	O
is	O
computer-rendered	O
imagery	O
of	O
chairs	O
.	O
the	O
latent	O
variables	O
z	O
are	O
parameters	O
given	O
to	O
the	O
rendering	O
engine	O
describing	O
the	O
choice	O
of	O
which	O
chair	O
model	B
to	O
use	O
,	O
the	O
position	B
of	O
the	O
chair	O
,	O
and	O
other	O
conﬁguration	O
details	O
that	O
aﬀect	O
the	O
rendering	O
of	O
the	O
image	O
.	O
using	O
this	O
synthetically	O
generated	O
data	O
,	O
a	O
convolutional	O
network	O
is	O
able	O
to	O
learn	O
to	O
map	O
z	O
descriptions	O
of	O
the	O
content	O
of	O
an	O
image	O
to	O
x	O
approximations	O
of	O
rendered	O
images	O
.	O
this	O
suggests	O
that	O
contemporary	O
diﬀerentiable	O
generator	O
networks	O
have	O
suﬃcient	O
model	B
capacity	O
to	O
be	O
good	O
generative	O
models	O
,	O
and	O
that	O
contemporary	O
optimization	O
algorithms	O
have	O
the	O
ability	O
to	O
ﬁt	O
them	O
.	O
the	O
diﬃculty	O
lies	O
in	O
determining	O
how	O
to	O
train	O
generator	O
networks	O
when	O
the	O
value	O
of	O
z	O
for	O
each	O
x	O
is	O
not	O
ﬁxed	O
and	O
known	O
ahead	O
of	O
each	O
time	O
.	O
the	O
following	O
sections	O
describe	O
several	O
approaches	O
to	O
training	O
diﬀerentiable	O
generator	O
nets	O
given	O
only	O
training	O
samples	O
of	O
.x	O
20.10.3	O
variational	O
autoencoders	O
the	O
variational	O
autoencoder	O
or	O
vae	O
(	O
)	O
is	O
a	O
directed	O
model	B
that	O
uses	O
learned	O
approximate	O
inference	O
and	O
can	O
be	O
trained	O
purely	O
with	O
gradient-based	O
methods	O
.	O
kingma	O
2013	O
rezende	O
et	O
al	O
.	O
2014	O
,	O
;	O
,	O
to	O
generate	O
a	O
sample	O
from	O
the	O
model	B
,	O
the	O
vae	O
ﬁrst	O
draws	O
a	O
sample	O
z	O
from	O
the	O
code	O
distribution	O
pmodel	O
(	O
z	O
)	O
.	O
the	O
sample	O
is	O
then	O
run	O
through	O
a	O
diﬀerentiable	O
|	O
generator	O
network	O
g	O
(	O
z	O
)	O
.	O
finally	O
,	O
x	O
is	O
sampled	O
from	O
a	O
distribution	O
pmodel	O
(	O
x	O
;	O
g	O
(	O
z	O
)	O
)	O
=	O
|	O
pmodel	O
(	O
x	O
z	O
)	O
.	O
however	O
,	O
during	O
training	O
,	O
the	O
approximate	O
inference	O
network	O
(	O
or	O
encoder	O
)	O
q	O
(	O
z	O
x	O
)	O
is	O
then	O
viewed	O
as	O
a	O
decoder	O
network	O
.	O
|	O
)	O
is	O
used	O
to	O
obtain	O
z	O
and	O
pmodel	O
(	O
x	O
z	O
the	O
key	O
insight	O
behind	O
variational	O
autoencoders	O
is	O
that	O
they	O
may	O
be	O
trained	O
l	O
by	O
maximizing	O
the	O
variational	O
lower	O
bound	B
(	O
)	O
q	O
h	O
)	O
+	O
(	O
(	O
|	O
−	O
z	O
x	O
,	O
q	O
z	O
∼	O
q	O
(	O
∼	O
q	O
(	O
|	O
z	O
x	O
|	O
z	O
x	O
)	O
log	O
pmodel	O
(	O
)	O
log	O
pmodel	O
(	O
l	O
(	O
)	O
=	O
q	O
ez	O
=	O
ez	O
≤	O
x	O
q	O
z	O
dkl	O
(	O
(	O
)	O
)	O
|	O
||	O
)	O
x	O
pmodel	O
(	O
)	O
)	O
z	O
x	O
z	O
)	O
associated	O
with	O
data	O
point	O
|	O
:	O
x	O
(	O
20.76	O
)	O
(	O
20.77	O
)	O
(	O
20.78	O
)	O
log	O
p	O
model	B
(	O
)	O
x	O
.	O
696	O
chapter	O
20.	O
deep	O
generative	O
models	O
20.76	O
,	O
we	O
recognize	O
the	O
ﬁrst	O
term	O
as	O
the	O
joint	O
log-likelihood	O
of	O
the	O
visible	O
in	O
equation	O
and	O
hidden	O
variables	O
under	O
the	O
approximate	O
posterior	O
over	O
the	O
latent	O
variables	O
(	O
just	O
like	O
with	O
em	O
,	O
except	O
that	O
we	O
use	O
an	O
approximate	O
rather	O
than	O
the	O
exact	O
posterior	O
)	O
.	O
we	O
recognize	O
also	O
a	O
second	O
term	O
,	O
the	O
entropy	O
of	O
the	O
approximate	O
posterior	O
.	O
when	O
q	O
is	O
chosen	O
to	O
be	O
a	O
gaussian	O
distribution	O
,	O
with	O
noise	O
added	O
to	O
a	O
predicted	O
mean	O
value	O
,	O
maximizing	O
this	O
entropy	O
term	O
encourages	O
increasing	O
the	O
standard	O
deviation	O
of	O
this	O
noise	O
.	O
more	O
generally	O
,	O
this	O
entropy	O
term	O
encourages	O
the	O
variational	O
posterior	O
to	O
place	O
high	O
probability	O
mass	O
on	O
many	O
z	O
values	O
that	O
could	O
have	O
generated	O
x	O
,	O
rather	O
than	O
collapsing	O
to	O
a	O
single	O
point	O
estimate	O
of	O
the	O
most	O
likely	O
value	O
.	O
in	O
equation	O
,	O
we	O
recognize	O
the	O
ﬁrst	O
term	O
as	O
the	O
reconstruction	O
log-likelihood	O
found	O
in	O
other	O
autoencoders	O
.	O
the	O
second	O
term	O
tries	O
to	O
make	O
the	O
approximate	O
posterior	O
distribution	O
q	O
(	O
z	O
x	O
)	O
and	O
the	O
model	B
prior	O
pmodel	O
(	O
z	O
)	O
approach	O
each	O
other	O
.	O
20.77	O
|	O
traditional	O
approaches	O
to	O
variational	O
inference	O
and	O
learning	O
infer	O
q	O
via	O
an	O
opti-	O
)	O
.	O
these	O
mization	O
algorithm	O
,	O
typically	O
iterated	O
ﬁxed	O
point	O
equations	O
(	O
section	O
∼	O
q	O
log	O
p	O
model	B
(	O
z	O
x	O
,	O
)	O
approaches	O
are	O
slow	O
and	O
often	O
require	O
the	O
ability	O
to	O
compute	O
ez	O
in	O
closed	O
form	O
.	O
the	O
main	O
idea	O
behind	O
the	O
variational	O
autoencoder	O
is	O
to	O
train	O
a	O
parametric	O
encoder	O
(	O
also	O
sometimes	O
called	O
an	O
inference	O
network	O
or	O
recognition	B
model	O
)	O
that	O
produces	O
the	O
parameters	O
of	O
q.	O
so	O
long	O
as	O
z	O
is	O
a	O
continuous	O
variable	O
,	O
we	O
can	O
then	O
back-propagate	O
through	O
samples	O
of	O
z	O
drawn	O
from	O
q	O
(	O
z	O
x	O
)	O
=	O
q	O
(	O
z	O
;	O
f	O
(	O
x	O
;	O
θ	O
)	O
)	O
in	O
order	O
to	O
obtain	O
a	O
gradient	O
with	O
respect	O
to	O
θ.	O
learning	O
then	O
consists	O
solely	O
of	O
maximizing	O
with	O
respect	O
to	O
the	O
parameters	O
of	O
the	O
encoder	O
and	O
decoder	O
.	O
all	O
of	O
the	O
expectations	O
in	O
may	O
be	O
approximated	O
by	O
monte	O
carlo	O
sampling	O
.	O
19.4	O
l	O
|	O
l	O
3.6	O
the	O
variational	O
autoencoder	O
approach	O
is	O
elegant	O
,	O
theoretically	O
pleasing	O
,	O
and	O
simple	O
to	O
implement	O
.	O
it	O
also	O
obtains	O
excellent	O
results	O
and	O
is	O
among	O
the	O
state	O
of	O
the	O
art	O
approaches	O
to	O
generative	O
modeling	O
.	O
its	O
main	O
drawback	O
is	O
that	O
samples	O
from	O
variational	O
autoencoders	O
trained	O
on	O
images	O
tend	O
to	O
be	O
somewhat	O
blurry	O
.	O
the	O
causes	O
	O
of	O
this	O
phenomenon	O
are	O
not	O
yet	O
known	O
.	O
one	O
possibility	O
is	O
that	O
the	O
blurriness	O
is	O
an	O
intrinsic	O
eﬀect	O
of	O
maximum	O
likelihood	O
,	O
which	O
minimizes	O
dkl	O
(	O
pdata	O
pmodel	O
)	O
.	O
as	O
illustrated	O
in	O
ﬁgure	O
,	O
this	O
means	O
that	O
the	O
model	B
will	O
assign	O
high	O
probability	O
to	O
points	O
that	O
occur	O
in	O
the	O
training	O
set	O
,	O
but	O
may	O
also	O
assign	O
high	O
probability	O
to	O
other	O
points	O
.	O
these	O
other	O
points	O
may	O
include	O
blurry	O
images	O
.	O
part	O
of	O
the	O
reason	O
that	O
the	O
model	B
would	O
choose	O
to	O
put	O
probability	O
mass	O
on	O
blurry	O
images	O
rather	O
than	O
some	O
other	O
part	O
of	O
the	O
space	O
is	O
that	O
the	O
variational	O
autoencoders	O
used	O
in	O
practice	O
usually	O
have	O
a	O
gaussian	O
distribution	O
for	O
pmodel	O
(	O
x	O
;	O
g	O
(	O
z	O
)	O
)	O
.	O
maximizing	O
a	O
lower	O
bound	B
on	O
the	O
likelihood	O
of	O
such	O
a	O
distribution	O
is	O
similar	O
to	O
training	O
a	O
traditional	O
autoencoder	O
with	O
mean	O
squared	O
error	O
,	O
in	O
the	O
sense	O
that	O
it	O
has	O
a	O
tendency	O
to	O
ignore	O
features	O
of	O
the	O
input	O
that	O
occupy	O
few	O
pixels	O
or	O
that	O
cause	O
only	O
a	O
small	O
change	O
in	O
the	O
brightness	O
of	O
the	O
pixels	O
that	O
they	O
occupy	O
.	O
this	O
issue	O
is	O
not	O
speciﬁc	O
to	O
vaes	O
and	O
697	O
chapter	O
20.	O
deep	O
generative	O
models	O
	O
pmodel	O
)	O
,	O
as	O
argued	O
by	O
is	O
shared	O
with	O
generative	O
models	O
that	O
optimize	O
a	O
log-likelihood	O
,	O
or	O
equivalently	O
,	O
dkl	O
(	O
p	O
data	O
)	O
.	O
another	O
troubling	O
issue	O
with	O
contemporary	O
vae	O
models	O
is	O
that	O
they	O
tend	O
to	O
use	O
only	O
a	O
small	O
subset	O
of	O
the	O
dimensions	O
of	O
z	O
,	O
as	O
if	O
the	O
encoder	O
was	O
not	O
able	O
to	O
transform	O
enough	O
of	O
the	O
local	O
directions	O
in	O
input	O
space	O
to	O
a	O
space	O
where	O
the	O
marginal	O
distribution	O
matches	O
the	O
factorized	O
prior	O
.	O
theis	O
et	O
al	O
.	O
2015	O
huszar	O
2015	O
)	O
and	O
by	O
(	O
(	O
the	O
vae	O
framework	O
is	O
very	O
straightforward	O
to	O
extend	O
to	O
a	O
wide	O
range	O
of	O
model	B
architectures	O
.	O
this	O
is	O
a	O
key	O
advantage	O
over	O
boltzmann	O
machines	O
,	O
which	O
require	O
extremely	O
careful	O
model	B
design	O
to	O
maintain	O
tractability	O
.	O
vaes	O
work	B
very	O
well	O
with	O
a	O
diverse	O
family	O
of	O
diﬀerentiable	O
operators	O
.	O
one	O
particularly	O
sophisticated	O
vae	O
is	O
the	O
deep	O
recurrent	O
attention	O
writer	O
or	O
draw	O
model	B
(	O
)	O
.	O
draw	O
uses	O
a	O
recurrent	O
encoder	O
and	O
recurrent	O
decoder	O
combined	O
with	O
an	O
attention	O
mechanism	O
.	O
the	O
generation	O
process	O
for	O
the	O
draw	O
model	B
consists	O
of	O
sequentially	O
visiting	O
diﬀerent	O
small	O
image	O
patches	O
and	O
drawing	O
the	O
values	O
of	O
the	O
pixels	O
at	O
those	O
points	O
.	O
vaes	O
can	O
also	O
be	O
extended	O
to	O
generate	O
sequences	O
by	O
deﬁning	O
variational	O
rnns	O
(	O
)	O
by	O
using	O
a	O
recurrent	O
encoder	O
and	O
decoder	O
within	O
the	O
vae	O
framework	O
.	O
generating	O
a	O
sample	O
from	O
a	O
traditional	O
rnn	O
involves	O
only	O
non-deterministic	O
operations	O
at	O
the	O
output	O
space	O
.	O
variational	O
rnns	O
also	O
have	O
random	O
variability	O
at	O
the	O
potentially	O
more	O
abstract	O
level	O
captured	O
by	O
the	O
vae	O
latent	O
variables	O
.	O
chung	O
et	O
al	O
.	O
2015b	O
gregor	O
et	O
al	O
.	O
2015	O
,	O
,	O
the	O
vae	O
framework	O
has	O
been	O
extended	O
to	O
maximize	O
not	O
just	O
the	O
traditional	O
variational	O
lower	O
bound	B
,	O
but	O
instead	O
the	O
importance	O
weighted	O
autoencoder	O
(	O
burda	O
et	O
al	O
.	O
2015	O
)	O
objective	O
:	O
,	O
	O
	O
	O
l	O
k	O
(	O
x	O
,	O
q	O
)	O
=	O
ez	O
(	O
1	O
)	O
,	O
...	O
,	O
z	O
(	O
)	O
k	O
∼	O
|	O
q	O
(	O
z	O
x	O
)	O
log	O
1	O
k	O
k	O
i=1	O
|	O
(	O
)	O
i	O
)	O
pmodel	O
(	O
x	O
z	O
,	O
x	O
)	O
q	O
(	O
z	O
(	O
)	O
i	O
.	O
(	O
20.79	O
)	O
l	O
this	O
new	O
objective	O
is	O
equivalent	O
to	O
the	O
traditional	O
lower	O
bound	B
when	O
k	O
=	O
1.	O
however	O
,	O
it	O
may	O
also	O
be	O
interpreted	O
as	O
forming	O
an	O
estimate	O
of	O
the	O
true	O
log	O
pmodel	O
(	O
x	O
)	O
using	O
importance	O
sampling	O
of	O
z	O
from	O
proposal	O
distribution	O
q	O
(	O
z	O
x	O
)	O
.	O
the	O
importance	O
weighted	O
autoencoder	O
objective	O
is	O
also	O
a	O
lower	O
bound	B
on	O
log	O
pmodel	O
(	O
x	O
)	O
and	O
becomes	O
tighter	O
as	O
increases	O
.	O
k	O
|	O
variational	O
autoencoders	O
have	O
some	O
interesting	O
connections	O
to	O
the	O
mp-dbm	O
and	O
other	O
approaches	O
that	O
involve	O
back-propagation	O
through	O
the	O
approximate	O
inference	O
graph	O
(	O
goodfellow	O
2013	O
)	O
.	O
these	O
previous	O
approaches	O
required	O
an	O
inference	O
procedure	O
such	O
as	O
mean	O
ﬁeld	O
ﬁxed	O
point	O
equations	O
to	O
provide	O
the	O
computational	O
graph	O
.	O
the	O
variational	O
autoencoder	O
is	O
deﬁned	O
for	O
arbitrary	O
computational	O
graphs	O
,	O
which	O
makes	O
it	O
applicable	O
to	O
a	O
wider	O
range	O
of	O
probabilistic	O
model	B
families	O
because	O
there	O
is	O
no	O
need	O
to	O
restrict	O
the	O
choice	O
2013b	O
stoyanov	O
2011	O
brakel	O
et	O
al.	O
,	O
et	O
al.	O
,	O
et	O
al.	O
,	O
;	O
;	O
698	O
chapter	O
20.	O
deep	O
generative	O
models	O
of	O
models	O
to	O
those	O
with	O
tractable	O
mean	O
ﬁeld	O
ﬁxed	O
point	O
equations	O
.	O
the	O
variational	O
autoencoder	O
also	O
has	O
the	O
advantage	O
that	O
it	O
increases	O
a	O
bound	B
on	O
the	O
log-likelihood	O
of	O
the	O
model	B
,	O
while	O
the	O
criteria	O
for	O
the	O
mp-dbm	O
and	O
related	O
models	O
are	O
more	O
heuristic	O
and	O
have	O
little	O
probabilistic	O
interpretation	O
beyond	O
making	O
the	O
results	O
of	O
approximate	O
inference	O
accurate	O
.	O
one	O
disadvantage	O
of	O
the	O
variational	O
autoencoder	O
is	O
that	O
it	O
learns	O
an	O
inference	O
network	O
for	O
only	O
one	O
problem	O
,	O
inferring	O
z	O
given	O
x.	O
the	O
older	O
methods	O
are	O
able	O
to	O
perform	O
approximate	O
inference	O
over	O
any	O
subset	O
of	O
variables	O
given	O
any	O
other	O
subset	O
of	O
variables	O
,	O
because	O
the	O
mean	O
ﬁeld	O
ﬁxed	O
point	O
equations	O
specify	O
how	O
to	O
share	O
parameters	O
between	O
the	O
computational	O
graphs	O
for	O
all	O
of	O
these	O
diﬀerent	O
problems	O
.	O
one	O
very	O
nice	O
property	O
of	O
the	O
variational	O
autoencoder	O
is	O
that	O
simultaneously	O
training	O
a	O
parametric	O
encoder	O
in	O
combination	O
with	O
the	O
generator	O
network	O
forces	O
the	O
model	B
to	O
learn	O
a	O
predictable	O
coordinate	O
system	O
that	O
the	O
encoder	O
can	O
capture	O
.	O
this	O
makes	O
it	O
an	O
excellent	O
manifold	O
learning	O
algorithm	O
.	O
see	O
ﬁgure	O
for	O
examples	O
of	O
low-dimensional	O
manifolds	O
learned	O
by	O
the	O
variational	O
autoencoder	O
.	O
in	O
one	O
of	O
the	O
cases	O
demonstrated	O
in	O
the	O
ﬁgure	O
,	O
the	O
algorithm	O
discovered	O
two	O
independent	O
factors	O
of	O
variation	O
present	O
in	O
images	O
of	O
faces	O
:	O
angle	O
of	O
rotation	O
and	O
emotional	O
expression	O
.	O
20.6	O
20.10.4	O
generative	O
adversarial	O
networks	O
generative	O
adversarial	O
networks	O
or	O
gans	O
(	O
generative	O
modeling	O
approach	O
based	O
on	O
diﬀerentiable	O
generator	O
networks	O
.	O
goodfellow	O
et	O
al	O
.	O
2014c	O
,	O
)	O
are	O
another	O
generative	O
adversarial	O
networks	O
are	O
based	O
on	O
a	O
game	O
theoretic	O
scenario	O
in	O
which	O
the	O
generator	O
network	O
must	O
compete	O
against	O
an	O
adversary	O
.	O
the	O
generator	O
network	O
directly	O
produces	O
samples	O
x	O
=	O
g	O
(	O
z	O
;	O
θ	O
(	O
)	O
g	O
)	O
.	O
its	O
adversary	O
,	O
the	O
discriminator	O
network	O
,	O
attempts	O
to	O
distinguish	O
between	O
samples	O
drawn	O
from	O
the	O
training	O
data	O
and	O
samples	O
drawn	O
from	O
the	O
generator	O
.	O
the	O
discriminator	O
emits	O
a	O
probability	O
value	O
given	O
by	O
d	O
(	O
x	O
;	O
θ	O
(	O
)	O
d	O
)	O
,	O
indicating	O
the	O
probability	O
that	O
x	O
is	O
a	O
real	O
training	O
example	O
rather	O
than	O
a	O
fake	O
sample	O
drawn	O
from	O
the	O
model	B
.	O
the	O
simplest	O
way	O
to	O
formulate	O
learning	O
in	O
generative	O
adversarial	O
networks	O
is	O
−	O
as	O
a	O
zero-sum	O
game	O
,	O
in	O
which	O
a	O
function	O
v	O
(	O
θ	O
(	O
)	O
g	O
,	O
θ	O
(	O
)	O
d	O
)	O
determines	O
the	O
payoﬀ	O
of	O
the	O
v	O
(	O
θ	O
(	O
)	O
g	O
,	O
θ	O
(	O
)	O
d	O
)	O
as	O
its	O
own	O
payoﬀ	O
.	O
during	O
discriminator	O
.	O
the	O
generator	O
receives	O
learning	O
,	O
each	O
player	O
attempts	O
to	O
maximize	O
its	O
own	O
payoﬀ	O
,	O
so	O
that	O
at	O
convergence	O
∗	O
g	O
=	O
arg	O
min	O
g	O
v	O
g	O
,	O
d	O
.	O
)	O
(	O
max	O
d	O
(	O
20.80	O
)	O
the	O
default	O
choice	O
for	O
isv	O
∼	O
v	O
(	O
θ	O
(	O
)	O
g	O
,	O
θ	O
(	O
)	O
d	O
)	O
=	O
ex	O
p	O
data	O
log	O
(	O
)	O
+	O
d	O
x	O
∼	O
pmodel	O
e	O
x	O
log	O
(	O
1	O
−	O
(	O
)	O
)	O
d	O
x	O
.	O
(	O
20.81	O
)	O
699	O
chapter	O
20.	O
deep	O
generative	O
models	O
,	O
figure	O
20.6	O
:	O
examples	O
of	O
two-dimensional	O
coordinate	O
systems	O
for	O
high-dimensional	O
mani-	O
folds	O
,	O
learned	O
by	O
a	O
variational	O
autoencoder	O
(	O
kingma	O
and	O
welling	O
2014a	O
)	O
.	O
two	O
dimensions	O
may	O
be	O
plotted	O
directly	O
on	O
the	O
page	O
for	O
visualization	O
,	O
so	O
we	O
can	O
gain	O
an	O
understanding	O
of	O
how	O
the	O
model	B
works	O
by	O
training	O
a	O
model	B
with	O
a	O
2-d	O
latent	O
code	O
,	O
even	O
if	O
we	O
believe	O
the	O
|	O
intrinsic	O
dimensionality	O
of	O
the	O
data	O
manifold	O
is	O
much	O
higher	O
.	O
the	O
images	O
shown	O
are	O
not	O
examples	O
from	O
the	O
training	O
set	O
but	O
images	O
x	O
actually	O
generated	O
by	O
the	O
model	B
p	O
(	O
x	O
z	O
)	O
,	O
simply	O
by	O
changing	O
the	O
2-d	O
“	O
code	O
”	O
z	O
(	O
each	O
image	O
corresponds	O
to	O
a	O
diﬀerent	O
choice	O
of	O
“	O
code	O
”	O
z	O
on	O
a	O
2-d	O
uniform	O
grid	O
)	O
.	O
(	O
left	O
)	O
the	O
two-dimensional	O
map	O
of	O
the	O
frey	O
faces	O
manifold	O
.	O
one	O
dimension	O
that	O
has	O
been	O
discovered	O
(	O
horizontal	O
)	O
mostly	O
corresponds	O
to	O
a	O
rotation	O
of	O
the	O
face	O
,	O
while	O
the	O
other	O
(	O
vertical	O
)	O
corresponds	O
to	O
the	O
emotional	O
expression	O
.	O
the	O
two-dimensional	O
map	O
of	O
the	O
mnist	O
manifold	O
.	O
(	O
right	O
)	O
this	O
drives	O
the	O
discriminator	O
to	O
attempt	O
to	O
learn	O
to	O
correctly	O
classify	O
samples	O
as	O
real	O
or	O
fake	O
.	O
simultaneously	O
,	O
the	O
generator	O
attempts	O
to	O
fool	O
the	O
classiﬁer	O
into	O
believing	O
its	O
samples	O
are	O
real	O
.	O
at	O
convergence	O
,	O
the	O
generator	O
’	O
s	O
samples	O
are	O
indistinguishable	O
from	O
real	O
data	O
,	O
and	O
the	O
discriminator	O
outputs	O
1	O
everywhere	O
.	O
the	O
discriminator	O
2	O
may	O
then	O
be	O
discarded	O
.	O
the	O
main	O
motivation	O
for	O
the	O
design	O
of	O
gans	O
is	O
that	O
the	O
learning	O
process	O
requires	O
neither	O
approximate	O
inference	O
nor	O
approximation	O
of	O
a	O
partition	O
function	O
gradient	O
.	O
in	O
the	O
case	O
where	O
maxd	O
v	O
(	O
g	O
,	O
d	O
)	O
is	O
convex	O
in	O
θ	O
(	O
)	O
g	O
(	O
such	O
as	O
the	O
case	O
where	O
optimization	O
is	O
performed	O
directly	O
in	O
the	O
space	O
of	O
probability	O
density	O
functions	O
)	O
the	O
procedure	O
is	O
guaranteed	O
to	O
converge	O
and	O
is	O
asymptotically	O
consistent	O
.	O
unfortunately	O
,	O
learning	O
in	O
gans	O
can	O
be	O
diﬃcult	O
in	O
practice	O
when	O
g	O
and	O
d	O
are	O
represented	O
by	O
neural	O
networks	O
and	O
maxd	O
v	O
(	O
g	O
,	O
d	O
)	O
is	O
not	O
convex	O
.	O
goodfellow	O
700	O
chapter	O
20.	O
deep	O
generative	O
models	O
(	O
)	O
identiﬁed	O
non-convergence	O
as	O
an	O
issue	O
that	O
may	O
cause	O
gans	O
to	O
underﬁt	O
.	O
2014	O
in	O
general	O
,	O
simultaneous	O
gradient	O
descent	B
on	O
two	O
players	O
’	O
costs	O
is	O
not	O
guaranteed	O
to	O
reach	O
an	O
equilibrium	O
.	O
consider	O
for	O
example	O
the	O
value	O
function	O
v	O
(	O
a	O
,	O
b	O
)	O
=	O
ab	O
,	O
−	O
where	O
one	O
player	O
controls	O
a	O
and	O
incurs	O
cost	O
ab	O
,	O
while	O
the	O
other	O
player	O
controls	O
b	O
and	O
receives	O
a	O
cost	O
ab	O
.	O
if	O
we	O
model	B
each	O
player	O
as	O
making	O
inﬁnitesimally	O
small	O
gradient	O
steps	O
,	O
each	O
player	O
reducing	O
their	O
own	O
cost	O
at	O
the	O
expense	O
of	O
the	O
other	O
player	O
,	O
then	O
a	O
and	O
b	O
go	O
into	O
a	O
stable	O
,	O
circular	O
orbit	O
,	O
rather	O
than	O
arriving	O
at	O
the	O
equilibrium	O
point	O
at	O
the	O
origin	O
.	O
note	O
that	O
the	O
equilibria	O
for	O
a	O
minimax	O
game	O
are	O
not	O
local	O
minima	O
of	O
v.	O
instead	O
,	O
they	O
are	O
points	O
that	O
are	O
simultaneously	O
minima	O
for	O
both	O
players	O
’	O
costs	O
.	O
this	O
means	O
that	O
they	O
are	O
saddle	O
points	O
of	O
v	O
that	O
are	O
local	O
minima	O
with	O
respect	O
to	O
the	O
ﬁrst	O
player	O
’	O
s	O
parameters	O
and	O
local	O
maxima	O
with	O
respect	O
to	O
the	O
second	O
player	O
’	O
s	O
parameters	O
.	O
it	O
is	O
possible	O
for	O
the	O
two	O
players	O
to	O
take	O
turns	O
increasing	O
then	O
decreasing	O
v	O
forever	O
,	O
rather	O
than	O
landing	O
exactly	O
on	O
the	O
saddle	O
point	O
where	O
neither	O
player	O
is	O
capable	O
of	O
reducing	O
its	O
cost	O
.	O
it	O
is	O
not	O
known	O
to	O
what	O
extent	O
this	O
non-convergence	O
problem	O
aﬀects	O
gans	O
.	O
(	O
goodfellow	O
2014	O
)	O
identiﬁed	O
an	O
alternative	O
formulation	O
of	O
the	O
payoﬀs	O
,	O
in	O
which	O
the	O
game	O
is	O
no	O
longer	O
zero-sum	O
,	O
that	O
has	O
the	O
same	O
expected	O
gradient	O
as	O
maximum	O
likelihood	O
learning	O
whenever	O
the	O
discriminator	O
is	O
optimal	O
.	O
because	O
maximum	O
likelihood	O
training	O
converges	O
,	O
this	O
reformulation	O
of	O
the	O
gan	O
game	O
should	O
also	O
converge	O
,	O
given	O
enough	O
samples	O
.	O
unfortunately	O
,	O
this	O
alternative	O
formulation	O
does	O
not	O
seem	O
to	O
improve	O
convergence	O
in	O
practice	O
,	O
possibly	O
due	O
to	O
suboptimality	O
of	O
the	O
discriminator	O
,	O
or	O
possibly	O
due	O
to	O
high	O
variance	O
around	O
the	O
expected	O
gradient	O
.	O
goodfellow	O
et	O
al	O
.	O
2014c	O
in	O
realistic	O
experiments	O
,	O
the	O
best-performing	O
formulation	O
of	O
the	O
gan	O
game	O
is	O
a	O
diﬀerent	O
formulation	O
that	O
is	O
neither	O
zero-sum	O
nor	O
equivalent	O
to	O
maximum	O
likelihood	O
,	O
introduced	O
by	O
)	O
with	O
a	O
heuristic	O
motivation	O
.	O
in	O
this	O
best-performing	O
formulation	O
,	O
the	O
generator	O
aims	O
to	O
increase	O
the	O
log	O
probability	O
that	O
the	O
discriminator	O
makes	O
a	O
mistake	O
,	O
rather	O
than	O
aiming	O
to	O
decrease	O
the	O
log	O
probability	O
that	O
the	O
discriminator	O
makes	O
the	O
correct	O
prediction	O
.	O
this	O
reformulation	O
is	O
motivated	O
solely	O
by	O
the	O
observation	O
that	O
it	O
causes	O
the	O
derivative	O
of	O
the	O
generator	O
’	O
s	O
cost	O
function	O
with	O
respect	O
to	O
the	O
discriminator	O
’	O
s	O
logits	O
to	O
remain	O
large	O
even	O
in	O
the	O
situation	O
where	O
the	O
discriminator	O
conﬁdently	O
rejects	O
all	O
generator	O
samples	O
.	O
(	O
stabilization	O
of	O
gan	O
learning	O
remains	O
an	O
open	O
problem	O
.	O
fortunately	O
,	O
gan	O
learning	O
performs	O
well	O
when	O
the	O
model	B
architecture	O
and	O
hyperparameters	O
are	O
care-	O
fully	O
selected.	O
)	O
crafted	O
a	O
deep	O
convolutional	O
gan	O
(	O
dcgan	O
)	O
that	O
performs	O
very	O
well	O
for	O
image	O
synthesis	O
tasks	O
,	O
and	O
showed	O
that	O
its	O
latent	O
repre-	O
sentation	O
space	O
captures	O
important	O
factors	O
of	O
variation	O
,	O
as	O
shown	O
in	O
ﬁgure	O
15.9	O
.	O
see	O
ﬁgure	O
for	O
examples	O
of	O
images	O
generated	O
by	O
a	O
dcgan	O
generator	O
.	O
radford	O
et	O
al	O
.	O
2015	O
20.7	O
(	O
the	O
gan	O
learning	O
problem	O
can	O
also	O
be	O
simpliﬁed	O
by	O
breaking	O
the	O
generation	O
701	O
chapter	O
20.	O
deep	O
generative	O
models	O
figure	O
20.7	O
:	O
images	O
generated	O
by	O
gans	O
trained	O
on	O
the	O
lsun	O
dataset	O
.	O
(	O
left	O
)	O
images	O
of	O
bedrooms	O
generated	O
by	O
a	O
dcgan	O
model	B
,	O
reproduced	O
with	O
permission	O
from	O
radford	O
et	O
al	O
.	O
(	O
images	O
of	O
churches	O
generated	O
by	O
a	O
lapgan	O
model	B
,	O
reproduced	O
with	O
permission	O
from	O
denton	O
et	O
al	O
.	O
2015	O
(	O
right	O
)	O
2015	O
)	O
.	O
)	O
.	O
(	O
|	O
,	O
)	O
that	O
learn	O
to	O
sample	O
from	O
a	O
distribution	O
p	O
(	O
x	O
y	O
process	O
into	O
many	O
levels	O
of	O
detail	O
.	O
it	O
is	O
possible	O
to	O
train	O
conditional	O
gans	O
(	O
mirza	O
)	O
rather	O
and	O
osindero	O
2014	O
than	O
simply	O
sampling	O
from	O
a	O
marginal	O
distribution	O
p	O
(	O
x	O
)	O
.	O
denton	O
et	O
al	O
.	O
2015	O
)	O
showed	O
that	O
a	O
series	O
of	O
conditional	O
gans	O
can	O
be	O
trained	O
to	O
ﬁrst	O
generate	O
a	O
very	O
low-resolution	O
version	O
of	O
an	O
image	O
,	O
then	O
incrementally	O
add	O
details	O
to	O
the	O
image	O
.	O
this	O
technique	O
is	O
called	O
the	O
lapgan	O
model	B
,	O
due	O
to	O
the	O
use	O
of	O
a	O
laplacian	O
pyramid	O
to	O
generate	O
the	O
images	O
containing	O
varying	O
levels	O
of	O
detail	O
.	O
lapgan	O
generators	O
are	O
able	O
to	O
fool	O
not	O
only	O
discriminator	O
networks	O
but	O
also	O
human	O
observers	O
,	O
with	O
experimental	O
subjects	O
identifying	O
up	O
to	O
40	O
%	O
of	O
the	O
outputs	O
of	O
the	O
network	O
as	O
being	O
real	O
data	O
.	O
see	O
ﬁgure	O
for	O
examples	O
of	O
images	O
generated	O
by	O
a	O
lapgan	O
generator	O
.	O
20.7	O
(	O
one	O
unusual	O
capability	O
of	O
the	O
gan	O
training	O
procedure	O
is	O
that	O
it	O
can	O
ﬁt	O
proba-	O
bility	O
distributions	O
that	O
assign	O
zero	O
probability	O
to	O
the	O
training	O
points	O
.	O
rather	O
than	O
maximizing	O
the	O
log	O
probability	O
of	O
speciﬁc	O
points	O
,	O
the	O
generator	O
net	O
learns	O
to	O
trace	O
out	O
a	O
manifold	O
whose	O
points	O
resemble	O
training	O
points	O
in	O
some	O
way	O
.	O
somewhat	O
para-	O
doxically	O
,	O
this	O
means	O
that	O
the	O
model	B
may	O
assign	O
a	O
log-likelihood	O
of	O
negative	O
inﬁnity	O
to	O
the	O
test	O
set	O
,	O
while	O
still	O
representing	O
a	O
manifold	O
that	O
a	O
human	O
observer	O
judges	O
to	O
capture	O
the	O
essence	O
of	O
the	O
generation	O
task	O
.	O
this	O
is	O
not	O
clearly	O
an	O
advantage	O
or	O
a	O
disadvantage	O
,	O
and	O
one	O
may	O
also	O
guarantee	O
that	O
the	O
generator	O
network	O
assigns	O
non-zero	O
probability	O
to	O
all	O
points	O
simply	O
by	O
making	O
the	O
last	O
layer	O
of	O
the	O
generator	O
network	O
add	O
gaussian	O
noise	O
to	O
all	O
of	O
the	O
generated	O
values	O
.	O
generator	O
networks	O
that	O
add	O
gaussian	O
noise	O
in	O
this	O
manner	O
sample	O
from	O
the	O
same	O
distribution	O
that	O
one	O
obtains	O
by	O
using	O
the	O
generator	O
network	O
to	O
parametrize	O
the	O
mean	O
of	O
a	O
conditional	O
702	O
chapter	O
20.	O
deep	O
generative	O
models	O
gaussian	O
distribution	O
.	O
dropout	O
seems	O
to	O
be	O
important	O
in	O
the	O
discriminator	O
network	O
.	O
in	O
particular	O
,	O
units	O
should	O
be	O
stochastically	O
dropped	O
while	O
computing	O
the	O
gradient	O
for	O
the	O
generator	O
network	O
to	O
follow	O
.	O
following	O
the	O
gradient	O
of	O
the	O
deterministic	O
version	O
of	O
the	O
discriminator	O
with	O
its	O
weights	O
divided	O
by	O
two	O
does	O
not	O
seem	O
to	O
be	O
as	O
eﬀective	O
.	O
likewise	O
,	O
never	O
using	O
dropout	O
seems	O
to	O
yield	O
poor	O
results	O
.	O
while	O
the	O
gan	O
framework	O
is	O
designed	O
for	O
diﬀerentiable	O
generator	O
networks	O
,	O
similar	O
principles	O
can	O
be	O
used	O
to	O
train	O
other	O
kinds	O
of	O
models	O
.	O
for	O
example	O
,	O
self-	O
supervised	O
boosting	O
can	O
be	O
used	O
to	O
train	O
an	O
rbm	O
generator	O
to	O
fool	O
a	O
logistic	O
regression	O
discriminator	O
(	O
welling	O
et	O
al.	O
,	O
2002	O
)	O
.	O
20.10.5	O
generative	O
moment	O
matching	O
networks	O
generative	O
moment	O
matching	O
networks	O
(	O
li	O
et	O
al	O
.	O
2015	O
dziugaite	O
et	O
al	O
.	O
,	O
2015	O
)	O
are	O
another	O
form	O
of	O
generative	O
model	B
based	O
on	O
diﬀerentiable	O
generator	O
networks	O
.	O
unlike	O
vaes	O
and	O
gans	O
,	O
they	O
do	O
not	O
need	O
to	O
pair	O
the	O
generator	O
network	O
with	O
any	O
other	O
network—neither	O
an	O
inference	O
network	O
as	O
used	O
with	O
vaes	O
nor	O
a	O
discriminator	O
network	O
as	O
used	O
with	O
gans	O
.	O
,	O
;	O
these	O
networks	O
are	O
trained	O
with	O
a	O
technique	O
called	O
moment	O
matching	O
.	O
the	O
basic	O
idea	O
behind	O
moment	O
matching	O
is	O
to	O
train	O
the	O
generator	O
in	O
such	O
a	O
way	O
that	O
many	O
of	O
the	O
statistics	O
of	O
samples	O
generated	O
by	O
the	O
model	B
are	O
as	O
similar	O
as	O
possible	O
to	O
those	O
of	O
the	O
statistics	O
of	O
the	O
examples	O
in	O
the	O
training	O
set	O
.	O
in	O
this	O
context	O
,	O
a	O
moment	O
is	O
an	O
expectation	O
of	O
diﬀerent	O
powers	O
of	O
a	O
random	O
variable	O
.	O
for	O
example	O
,	O
the	O
ﬁrst	O
moment	O
is	O
the	O
mean	O
,	O
the	O
second	O
moment	O
is	O
the	O
mean	O
of	O
the	O
squared	O
values	O
,	O
and	O
so	O
on	O
.	O
in	O
multiple	O
dimensions	O
,	O
each	O
element	O
of	O
the	O
random	O
vector	O
may	O
be	O
raised	O
to	O
diﬀerent	O
powers	O
,	O
so	O
that	O
a	O
moment	O
may	O
be	O
any	O
quantity	O
of	O
the	O
form	O
where	O
n	O
=	O
[	O
n	O
1	O
,	O
n	O
2	O
,	O
.	O
.	O
.	O
,	O
nd	O
]	O
	O
exπi	O
xni	O
i	O
(	O
20.82	O
)	O
is	O
a	O
vector	O
of	O
non-negative	O
integers	O
.	O
upon	O
ﬁrst	O
examination	O
,	O
this	O
approach	O
seems	O
to	O
be	O
computationally	O
infeasible	O
.	O
for	O
example	O
,	O
if	O
we	O
want	O
to	O
match	O
all	O
the	O
moments	O
of	O
the	O
form	O
xixj	O
,	O
then	O
we	O
need	O
to	O
minimize	O
the	O
diﬀerence	O
between	O
a	O
number	O
of	O
values	O
that	O
is	O
quadratic	O
in	O
the	O
dimension	O
of	O
x.	O
moreover	O
,	O
even	O
matching	O
all	O
of	O
the	O
ﬁrst	O
and	O
second	O
moments	O
would	O
only	O
be	O
suﬃcient	O
to	O
ﬁt	O
a	O
multivariate	O
gaussian	O
distribution	O
,	O
which	O
captures	O
only	O
linear	O
relationships	O
between	O
values	O
.	O
our	O
ambitions	O
for	O
neural	O
networks	O
are	O
to	O
capture	O
complex	O
nonlinear	O
relationships	O
,	O
which	O
would	O
require	O
far	O
more	O
moments	O
.	O
gans	O
avoid	O
this	O
problem	O
of	O
exhaustively	O
enumerating	O
all	O
moments	O
by	O
using	O
a	O
703	O
chapter	O
20.	O
deep	O
generative	O
models	O
dynamically	O
updated	O
discriminator	O
that	O
automatically	O
focuses	O
its	O
attention	O
on	O
whichever	O
statistic	O
the	O
generator	O
network	O
is	O
matching	O
the	O
least	O
eﬀectively	O
.	O
;	O
et	O
al.	O
,	O
instead	O
,	O
generative	O
moment	O
matching	O
networks	O
can	O
be	O
trained	O
by	O
minimizing	O
a	O
cost	O
function	O
called	O
maximum	O
mean	O
discrepancy	O
(	O
schölkopf	O
and	O
smola	O
,	O
2002	O
gretton	O
)	O
or	O
mmd	O
.	O
this	O
cost	O
function	O
measures	O
the	O
error	O
in	O
the	O
ﬁrst	O
moments	O
in	O
an	O
inﬁnite-dimensional	O
space	O
,	O
using	O
an	O
implicit	O
mapping	O
to	O
feature	O
space	O
deﬁned	O
by	O
a	O
kernel	O
function	O
in	O
order	O
to	O
make	O
computations	O
on	O
inﬁnite-dimensional	O
vectors	O
tractable	O
.	O
the	O
mmd	O
cost	O
is	O
zero	O
if	O
and	O
only	O
if	O
the	O
two	O
distributions	O
being	O
compared	O
are	O
equal	O
.	O
2012	O
visually	O
,	O
the	O
samples	O
from	O
generative	O
moment	O
matching	O
networks	O
are	O
somewhat	O
disappointing	O
.	O
fortunately	O
,	O
they	O
can	O
be	O
improved	O
by	O
combining	O
the	O
generator	O
network	O
with	O
an	O
autoencoder	O
.	O
first	O
,	O
an	O
autoencoder	O
is	O
trained	O
to	O
reconstruct	O
the	O
training	O
set	O
.	O
next	O
,	O
the	O
encoder	O
of	O
the	O
autoencoder	O
is	O
used	O
to	O
transform	O
the	O
entire	O
training	O
set	O
into	O
code	O
space	O
.	O
the	O
generator	O
network	O
is	O
then	O
trained	O
to	O
generate	O
code	O
samples	O
,	O
which	O
may	O
be	O
mapped	O
to	O
visually	O
pleasing	O
samples	O
via	O
the	O
decoder	O
.	O
unlike	O
gans	O
,	O
the	O
cost	O
function	O
is	O
deﬁned	O
only	O
with	O
respect	O
to	O
a	O
batch	O
of	O
examples	O
from	O
both	O
the	O
training	O
set	O
and	O
the	O
generator	O
network	O
.	O
it	O
is	O
not	O
possible	O
to	O
make	O
a	O
training	O
update	O
as	O
a	O
function	O
of	O
only	O
one	O
training	O
example	O
or	O
only	O
one	O
sample	O
from	O
the	O
generator	O
network	O
.	O
this	O
is	O
because	O
the	O
moments	O
must	O
be	O
computed	O
as	O
an	O
empirical	O
average	O
across	O
many	O
samples	O
.	O
when	O
the	O
batch	O
size	O
is	O
too	O
small	O
,	O
mmd	O
can	O
underestimate	O
the	O
true	O
amount	O
of	O
variation	O
in	O
the	O
distributions	O
being	O
sampled	O
.	O
no	O
ﬁnite	O
batch	O
size	O
is	O
suﬃciently	O
large	O
to	O
eliminate	O
this	O
problem	O
entirely	O
,	O
but	O
larger	O
batches	O
reduce	O
the	O
amount	O
of	O
underestimation	O
.	O
when	O
the	O
batch	O
size	O
is	O
too	O
large	O
,	O
the	O
training	O
procedure	O
becomes	O
infeasibly	O
slow	O
,	O
because	O
many	O
examples	O
must	O
be	O
processed	O
in	O
order	O
to	O
compute	O
a	O
single	O
small	O
gradient	O
step	O
.	O
as	O
with	O
gans	O
,	O
it	O
is	O
possible	O
to	O
train	O
a	O
generator	O
net	O
using	O
mmd	O
even	O
if	O
that	O
generator	O
net	O
assigns	O
zero	O
probability	O
to	O
the	O
training	O
points	O
.	O
20.10.6	O
convolutional	O
generative	O
networks	O
2015	O
when	O
generating	O
images	O
,	O
it	O
is	O
often	O
useful	O
to	O
use	O
a	O
generator	O
network	O
that	O
includes	O
a	O
convolutional	O
structure	O
(	O
see	O
for	O
example	O
goodfellow	O
dosovitskiy	O
)	O
)	O
.	O
to	O
do	O
so	O
,	O
we	O
use	O
the	O
“	O
transpose	O
”	O
of	O
the	O
convolution	O
operator	O
,	O
et	O
al	O
.	O
(	O
described	O
in	O
section	O
.	O
this	O
approach	O
often	O
yields	O
more	O
realistic	O
images	O
and	O
does	O
so	O
using	O
fewer	O
parameters	O
than	O
using	O
fully	O
connected	O
layers	O
without	O
parameter	O
sharing	O
.	O
et	O
al	O
.	O
(	O
2014c	O
9.5	O
)	O
or	O
convolutional	O
networks	O
for	O
recognition	B
tasks	O
have	O
information	O
ﬂow	O
from	O
the	O
image	O
to	O
some	O
summarization	O
layer	O
at	O
the	O
top	O
of	O
the	O
network	O
,	O
often	O
a	O
class	O
label	O
.	O
704	O
chapter	O
20.	O
deep	O
generative	O
models	O
as	O
this	O
image	O
ﬂows	O
upward	O
through	O
the	O
network	O
,	O
information	O
is	O
discarded	O
as	O
the	O
representation	O
of	O
the	O
image	O
becomes	O
more	O
invariant	O
to	O
nuisance	O
transformations	O
.	O
in	O
a	O
generator	O
network	O
,	O
the	O
opposite	O
is	O
true	O
.	O
rich	O
details	O
must	O
be	O
added	O
as	O
the	O
representation	O
of	O
the	O
image	O
to	O
be	O
generated	O
propagates	O
through	O
the	O
network	O
,	O
culminating	O
in	O
the	O
ﬁnal	O
representation	O
of	O
the	O
image	O
,	O
which	O
is	O
of	O
course	O
the	O
image	O
itself	O
,	O
in	O
all	O
of	O
its	O
detailed	O
glory	O
,	O
with	O
object	O
positions	O
and	O
poses	O
and	O
textures	O
and	O
lighting	O
.	O
the	O
primary	O
mechanism	O
for	O
discarding	O
information	O
in	O
a	O
convolutional	O
recognition	B
network	O
is	O
the	O
pooling	O
layer	O
.	O
the	O
generator	O
network	O
seems	O
to	O
need	O
to	O
add	O
information	O
.	O
we	O
can	O
not	O
put	O
the	O
inverse	O
of	O
a	O
pooling	O
layer	O
into	O
the	O
generator	O
network	O
because	O
most	O
pooling	O
functions	O
are	O
not	O
invertible	O
.	O
a	O
simpler	O
operation	O
is	O
to	O
merely	O
increase	O
the	O
spatial	O
size	O
of	O
the	O
representation	O
.	O
an	O
approach	O
that	O
seems	O
to	O
perform	O
acceptably	O
is	O
to	O
use	O
an	O
“	O
un-pooling	O
”	O
as	O
introduced	O
by	O
dosovitskiy	O
et	O
al	O
.	O
(	O
2015	O
)	O
.	O
this	O
layer	O
corresponds	O
to	O
the	O
inverse	O
of	O
the	O
max-pooling	O
operation	O
under	O
certain	O
simplifying	O
conditions	B
.	O
first	O
,	O
the	O
stride	O
of	O
the	O
max-pooling	O
operation	O
is	O
constrained	O
to	O
be	O
equal	O
to	O
the	O
width	O
of	O
the	O
pooling	O
region	O
.	O
second	O
,	O
the	O
maximum	O
input	O
within	O
each	O
pooling	O
region	O
is	O
assumed	O
to	O
be	O
the	O
input	O
in	O
the	O
upper-left	O
corner	O
.	O
finally	O
,	O
all	O
non-maximal	O
inputs	O
within	O
each	O
pooling	O
region	O
are	O
assumed	O
to	O
be	O
zero	O
.	O
these	O
are	O
very	O
strong	O
and	O
unrealistic	O
assumptions	O
,	O
but	O
they	O
do	O
allow	O
the	O
max-pooling	O
operator	O
to	O
be	O
inverted	O
.	O
the	O
inverse	O
un-pooling	O
operation	O
allocates	O
a	O
tensor	O
of	O
zeros	O
,	O
then	O
copies	O
each	O
value	O
from	O
spatial	O
coordinate	O
i	O
of	O
the	O
input	O
to	O
spatial	O
coordinate	O
i	O
of	O
the	O
output	O
.	O
the	O
integer	O
value	O
k	O
deﬁnes	O
the	O
size	O
of	O
the	O
pooling	O
region	O
.	O
even	O
though	O
the	O
assumptions	O
motivating	O
the	O
deﬁnition	O
of	O
the	O
un-pooling	O
operator	O
are	O
unrealistic	O
,	O
the	O
subsequent	O
layers	O
are	O
able	O
to	O
learn	O
to	O
compensate	O
for	O
its	O
unusual	O
output	O
,	O
so	O
the	O
samples	O
generated	O
by	O
the	O
model	B
as	O
a	O
whole	O
are	O
visually	O
pleasing	O
.	O
×	O
k	O
20.10.7	O
auto-regressive	O
networks	O
auto-regressive	O
networks	O
are	O
directed	O
probabilistic	O
models	O
with	O
no	O
latent	O
random	O
variables	O
.	O
the	O
conditional	O
probability	O
distributions	O
in	O
these	O
models	O
are	O
represented	O
by	O
neural	O
networks	O
(	O
sometimes	O
extremely	O
simple	O
neural	O
networks	O
such	O
as	O
logistic	O
regression	O
)	O
.	O
the	O
graph	O
structure	O
of	O
these	O
models	O
is	O
the	O
complete	O
graph	O
.	O
they	O
decompose	O
a	O
joint	O
probability	O
over	O
the	O
observed	O
variables	O
using	O
the	O
chain	O
rule	O
of	O
−	O
xd	O
probability	O
to	O
obtain	O
a	O
product	O
of	O
conditionals	O
of	O
the	O
form	O
p	O
(	O
xd	O
1	O
,	O
.	O
.	O
.	O
,	O
x1	O
)	O
.	O
such	O
models	O
have	O
been	O
called	O
fully-visible	O
bayes	O
networks	O
(	O
fvbns	O
)	O
and	O
used	O
successfully	O
in	O
many	O
forms	O
,	O
ﬁrst	O
with	O
logistic	O
regression	O
for	O
each	O
conditional	O
)	O
and	O
then	O
with	O
neural	O
networks	O
with	O
hidden	O
units	O
(	O
bengio	O
distribution	O
(	O
frey	O
1998	O
in	O
some	O
forms	O
of	O
auto-	O
and	O
bengio	O
2000b	O
larochelle	O
and	O
murray	O
2011	O
regressive	O
networks	O
,	O
such	O
as	O
nade	O
(	O
)	O
,	O
described	O
larochelle	O
and	O
murray	O
2011	O
)	O
.	O
|	O
,	O
,	O
;	O
,	O
,	O
705	O
chapter	O
20.	O
deep	O
generative	O
models	O
20.10.10	O
below	O
,	O
we	O
can	O
introduce	O
a	O
form	O
of	O
parameter	O
sharing	O
that	O
in	O
section	O
brings	O
both	O
a	O
statistical	O
advantage	O
(	O
fewer	O
unique	O
parameters	O
)	O
and	O
a	O
computational	O
advantage	O
(	O
less	O
computation	O
)	O
.	O
this	O
is	O
one	O
more	O
instance	O
of	O
the	O
recurring	O
deep	O
learning	O
motif	O
of	O
reuse	O
of	O
features	O
.	O
x1x1	O
x2x2	O
x3x3	O
x4x4	O
p	O
x	O
(	O
1	O
)	O
p	O
x	O
(	O
1	O
)	O
|	O
|	O
x1	O
)	O
x1	O
)	O
p	O
x	O
(	O
2	O
p	O
x	O
(	O
2	O
|	O
|	O
p	O
x	O
(	O
3	O
p	O
x	O
(	O
3	O
x1	O
,	O
x2	O
)	O
x1	O
,	O
x2	O
)	O
|	O
|	O
p	O
x	O
(	O
4	O
p	O
x	O
(	O
4	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
x	O
1x	O
1	O
x	O
2x	O
2	O
x	O
3x	O
3	O
x	O
4x	O
4	O
−	O
figure	O
20.8	O
:	O
a	O
fully	O
visible	O
belief	O
network	O
predicts	O
the	O
i-th	O
variable	O
from	O
the	O
i	O
1	O
previous	O
ones	O
.	O
(	O
top	O
)	O
corresponding	O
computational	O
graph	O
,	O
in	O
the	O
case	O
of	O
the	O
logistic	O
fvbn	O
,	O
where	O
each	O
prediction	O
is	O
made	O
by	O
a	O
linear	O
predictor	O
.	O
the	O
directed	O
graphical	O
model	B
for	O
an	O
fvbn	O
.	O
(	O
bottom	O
)	O
20.10.8	O
linear	O
auto-regressive	O
networks	O
the	O
simplest	O
form	O
of	O
auto-regressive	O
network	O
has	O
no	O
hidden	O
units	O
and	O
no	O
sharing	O
−	O
xi	O
1	O
,	O
.	O
.	O
.	O
,	O
x1	O
)	O
is	O
parametrized	O
as	O
a	O
linear	O
of	O
parameters	O
or	O
features	O
.	O
each	O
p	O
(	O
xi	O
model	O
(	O
linear	O
regression	O
for	O
real-valued	O
data	O
,	O
logistic	O
regression	O
for	O
binary	O
data	O
,	O
softmax	O
regression	O
for	O
discrete	O
data	O
)	O
.	O
this	O
model	B
was	O
introduced	O
by	O
frey	O
1998	O
)	O
and	O
has	O
o	O
(	O
d2	O
)	O
parameters	O
when	O
there	O
are	O
d	O
variables	O
to	O
model	B
.	O
it	O
is	O
illustrated	O
in	O
ﬁgure	O
.	O
20.8	O
|	O
(	O
if	O
the	O
variables	O
are	O
continuous	O
,	O
a	O
linear	O
auto-regressive	O
model	B
is	O
merely	O
another	O
way	O
to	O
formulate	O
a	O
multivariate	O
gaussian	O
distribution	O
,	O
capturing	O
linear	O
pairwise	O
interactions	O
between	O
the	O
observed	O
variables	O
.	O
linear	O
auto-regressive	O
networks	O
are	O
essentially	O
the	O
generalization	O
of	O
linear	O
classiﬁcation	O
methods	O
to	O
generative	O
modeling	O
.	O
they	O
therefore	O
have	O
the	O
same	O
706	O
chapter	O
20.	O
deep	O
generative	O
models	O
advantages	O
and	O
disadvantages	O
as	O
linear	O
classiﬁers	O
.	O
like	O
linear	O
classiﬁers	O
,	O
they	O
may	O
be	O
trained	O
with	O
convex	O
loss	O
functions	O
,	O
and	O
sometimes	O
admit	O
closed	O
form	O
solutions	O
(	O
as	O
in	O
the	O
gaussian	O
case	O
)	O
.	O
like	O
linear	O
classiﬁers	O
,	O
the	O
model	B
itself	O
does	O
not	O
oﬀer	O
a	O
way	O
of	O
increasing	O
its	O
capacity	O
,	O
so	O
capacity	O
must	O
be	O
raised	O
using	O
techniques	O
like	O
basis	O
expansions	O
of	O
the	O
input	O
or	O
the	O
kernel	O
trick	B
.	O
p	O
x	O
(	O
1	O
)	O
p	O
x	O
(	O
1	O
)	O
|	O
|	O
x1	O
)	O
x1	O
)	O
p	O
x	O
(	O
2	O
p	O
x	O
(	O
2	O
|	O
|	O
p	O
x	O
(	O
3	O
p	O
x	O
(	O
3	O
x1	O
,	O
x2	O
)	O
x1	O
,	O
x2	O
)	O
|	O
|	O
p	O
x	O
(	O
4	O
p	O
x	O
(	O
4	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
h1h1	O
x	O
1x	O
1	O
h2h2	O
x	O
2x	O
2	O
h3h3	O
x	O
3x	O
3	O
x	O
4x	O
4	O
−	O
figure	O
20.9	O
:	O
a	O
neural	O
auto-regressive	O
network	O
predicts	O
the	O
i-th	O
variable	O
xi	O
from	O
the	O
i	O
1	O
previous	O
ones	O
,	O
but	O
is	O
parametrized	O
so	O
that	O
features	O
(	O
groups	O
of	O
hidden	O
units	O
denoted	O
hi	O
)	O
that	O
are	O
functions	O
of	O
x1	O
,	O
.	O
.	O
.	O
,	O
xi	O
can	O
be	O
reused	O
in	O
predicting	O
all	O
of	O
the	O
subsequent	O
variables	O
xi+1	O
,	O
xi+2	O
,	O
.	O
.	O
.	O
,	O
xd	O
.	O
20.10.9	O
neural	O
auto-regressive	O
networks	O
,	O
20.8	O
bengio	O
and	O
bengio	O
2000a	O
b	O
,	O
)	O
have	O
the	O
same	O
neural	O
auto-regressive	O
networks	O
(	O
left-to-right	O
graphical	O
model	B
as	O
logistic	O
auto-regressive	O
networks	O
(	O
ﬁgure	O
)	O
but	O
employ	O
a	O
diﬀerent	O
parametrization	O
of	O
the	O
conditional	O
distributions	O
within	O
that	O
graphical	O
model	B
structure	O
.	O
the	O
new	O
parametrization	O
is	O
more	O
powerful	O
in	O
the	O
sense	O
that	O
its	O
capacity	O
can	O
be	O
increased	O
as	O
much	O
as	O
needed	O
,	O
allowing	O
approximation	O
of	O
any	O
joint	O
distribution	O
.	O
the	O
new	O
parametrization	O
can	O
also	O
improve	O
generalization	O
by	O
introducing	O
a	O
parameter	O
sharing	O
and	O
feature	O
sharing	O
principle	O
common	O
to	O
deep	O
learning	O
in	O
general	O
.	O
the	O
models	O
were	O
motivated	O
by	O
the	O
objective	O
of	O
avoiding	O
the	O
curse	O
of	O
dimensionality	O
arising	O
out	O
of	O
traditional	O
tabular	O
graphical	O
models	O
,	O
sharing	O
the	O
same	O
structure	O
as	O
ﬁgure	O
.	O
in	O
tabular	O
discrete	O
probabilistic	O
models	O
,	O
each	O
conditional	O
distribution	O
is	O
represented	O
by	O
a	O
table	O
of	O
probabilities	O
,	O
with	O
one	O
entry	O
and	O
one	O
parameter	O
for	O
each	O
possible	O
conﬁguration	O
of	O
the	O
variables	O
involved	O
.	O
by	O
using	O
a	O
neural	O
network	O
instead	O
,	O
two	O
advantages	O
are	O
obtained	O
:	O
20.8	O
707	O
chapter	O
20.	O
deep	O
generative	O
models	O
|	O
1	O
)	O
×	O
−	O
1.	O
the	O
parametrization	O
of	O
each	O
p	O
(	O
xi	O
−	O
xi	O
1	O
,	O
.	O
.	O
.	O
,	O
x1	O
)	O
by	O
a	O
neural	O
network	O
with	O
(	O
i	O
k	O
inputs	O
and	O
k	O
outputs	O
(	O
if	O
the	O
variables	O
are	O
discrete	O
and	O
take	O
k	O
values	O
,	O
encoded	O
one-hot	O
)	O
allows	O
one	O
to	O
estimate	O
the	O
conditional	O
probability	O
without	O
requiring	O
an	O
exponential	O
number	O
of	O
parameters	O
(	O
and	O
examples	O
)	O
,	O
yet	O
still	O
is	O
able	O
to	O
capture	O
high-order	O
dependencies	O
between	O
the	O
random	O
variables	O
.	O
20.9	O
left-to-right	O
connectivity	O
illustrated	O
in	O
ﬁgure	O
2.	O
instead	O
of	O
having	O
a	O
diﬀerent	O
neural	O
network	O
for	O
the	O
prediction	O
of	O
each	O
xi	O
,	O
a	O
allows	O
one	O
to	O
merge	O
all	O
the	O
neural	O
networks	O
into	O
one	O
.	O
equivalently	O
,	O
it	O
means	O
that	O
the	O
hidden	O
layer	O
features	O
computed	O
for	O
predicting	O
xi	O
can	O
be	O
reused	O
for	O
predicting	O
xi	O
k+	O
(	O
k	O
>	O
0	O
)	O
.	O
the	O
hidden	O
units	O
are	O
thus	O
organized	O
in	O
groups	O
that	O
have	O
the	O
particularity	O
that	O
all	O
the	O
units	O
in	O
the	O
i-th	O
group	O
only	O
depend	O
on	O
the	O
input	O
values	O
x1	O
,	O
.	O
.	O
.	O
,	O
xi	O
.	O
the	O
parameters	O
used	O
to	O
compute	O
these	O
hidden	O
units	O
are	O
jointly	O
optimized	O
to	O
improve	O
the	O
prediction	O
of	O
all	O
the	O
variables	O
in	O
the	O
sequence	O
.	O
this	O
is	O
an	O
instance	O
of	O
the	O
reuse	O
principle	O
that	O
recurs	O
throughout	O
deep	O
learning	O
in	O
scenarios	O
ranging	O
from	O
recurrent	O
and	O
convolutional	O
network	O
architectures	O
to	O
multi-task	O
and	O
transfer	O
learning	O
.	O
|	O
each	O
p	O
(	O
xi	O
−	O
xi	O
1	O
,	O
.	O
.	O
.	O
,	O
x1	O
)	O
can	O
represent	O
a	O
conditional	O
distribution	O
by	O
having	O
outputs	O
of	O
the	O
neural	O
network	O
predict	O
parameters	O
of	O
the	O
conditional	O
distribution	O
of	O
xi	O
,	O
as	O
discussed	O
in	O
section	O
.	O
although	O
the	O
original	O
neural	O
auto-regressive	O
networks	O
were	O
initially	O
evaluated	O
in	O
the	O
context	O
of	O
purely	O
discrete	O
multivariate	O
data	O
(	O
with	O
a	O
sigmoid	O
output	O
for	O
a	O
bernoulli	O
variable	O
or	O
softmax	O
output	O
for	O
a	O
multinoulli	O
variable	O
)	O
it	O
is	O
natural	O
to	O
extend	O
such	O
models	O
to	O
continuous	O
variables	O
or	O
joint	O
distributions	O
involving	O
both	O
discrete	O
and	O
continuous	O
variables	O
.	O
6.2.1.1	O
20.10.10	O
nade	O
the	O
neural	O
autoregressive	O
density	O
estimator	O
(	O
nade	O
)	O
is	O
a	O
very	O
successful	O
recent	O
form	O
of	O
neural	O
auto-regressive	O
network	O
(	O
larochelle	O
and	O
murray	O
2011	O
)	O
.	O
the	O
connectivity	O
is	O
the	O
same	O
as	O
for	O
the	O
original	O
neural	O
auto-regressive	O
network	O
of	O
bengio	O
)	O
but	O
nade	O
introduces	O
an	O
additional	O
parameter	O
sharing	O
scheme	O
,	O
and	O
bengio	O
2000b	O
as	O
illustrated	O
in	O
ﬁgure	O
.	O
the	O
parameters	O
of	O
the	O
hidden	O
units	O
of	O
diﬀerent	O
groups	O
j	O
are	O
shared	O
.	O
20.10	O
(	O
,	O
the	O
weights	O
w	O
of	O
hidden	O
unit	O
h	O
(	O
)	O
j	O
k	O
	O
j	O
,	O
k	O
,	O
i	O
from	O
the	O
i-th	O
input	O
xi	O
to	O
the	O
k	O
-th	O
element	O
of	O
the	O
j-th	O
group	O
(	O
j	O
)	O
are	O
shared	O
among	O
the	O
groups	O
:	O
i	O
≥	O
	O
j	O
,	O
k	O
,	O
i	O
=	O
wk	O
,	O
i	O
.	O
w	O
(	O
20.83	O
)	O
the	O
remaining	O
weights	O
,	O
where	O
j	O
<	O
i	O
,	O
are	O
zero	O
.	O
708	O
chapter	O
20.	O
deep	O
generative	O
models	O
p	O
x	O
(	O
1	O
)	O
p	O
x	O
(	O
1	O
)	O
|	O
|	O
x1	O
)	O
x1	O
)	O
p	O
x	O
(	O
2	O
p	O
x	O
(	O
2	O
|	O
|	O
p	O
x	O
(	O
3	O
p	O
x	O
(	O
3	O
x1	O
,	O
x2	O
)	O
x1	O
,	O
x2	O
)	O
|	O
|	O
p	O
x	O
(	O
4	O
p	O
x	O
(	O
4	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
h1h1	O
h2h2	O
h3h3	O
w	O
:	O
1	O
,	O
w	O
:	O
1	O
,	O
w	O
:	O
1	O
,	O
w	O
:	O
2	O
,	O
w	O
:	O
2	O
,	O
w	O
:	O
3	O
,	O
x	O
1x	O
1	O
x	O
2x	O
2	O
x	O
3x	O
3	O
x	O
4x	O
4	O
figure	O
20.10	O
:	O
an	O
illustration	O
of	O
the	O
neural	O
autoregressive	O
density	O
estimator	O
(	O
nade	O
)	O
.	O
the	O
hidden	O
units	O
are	O
organized	O
in	O
groups	O
h	O
(	O
)	O
j	O
so	O
that	O
only	O
the	O
inputs	O
x1	O
,	O
.	O
.	O
.	O
,	O
x	O
i	O
participate	O
−	O
in	O
computing	O
h	O
(	O
)	O
i	O
and	O
predicting	O
p	O
(	O
xj	O
1	O
,	O
.	O
.	O
.	O
,	O
x1	O
)	O
,	O
for	O
j	O
>	O
i.	O
nade	O
is	O
diﬀerentiated	O
from	O
earlier	O
neural	O
auto-regressive	O
networks	O
by	O
the	O
use	O
of	O
a	O
particular	O
weight	O
sharing	O
	O
pattern	O
:	O
w	O
j	O
,	O
k	O
,	O
i	O
=	O
wk	O
,	O
i	O
is	O
shared	O
(	O
indicated	O
in	O
the	O
ﬁgure	O
by	O
the	O
use	O
of	O
the	O
same	O
line	O
pattern	O
for	O
every	O
instance	O
of	O
a	O
replicated	O
weight	O
)	O
for	O
all	O
the	O
weights	O
going	O
out	O
from	O
xi	O
to	O
the	O
k-th	O
unit	O
of	O
any	O
group	O
(	O
w1	O
,	O
i	O
,	O
w2	O
,	O
i	O
,	O
.	O
.	O
.	O
,	O
wn	O
,	O
i	O
)	O
is	O
denoted	O
w	O
:	O
,i.	O
.	O
recall	O
that	O
the	O
vector	O
i	O
≥	O
xj	O
|	O
j	O
(	O
larochelle	O
and	O
murray	O
2011	O
)	O
chose	O
this	O
sharing	O
scheme	O
so	O
that	O
forward	O
propagation	O
in	O
a	O
nade	O
model	B
loosely	O
resembles	O
the	O
computations	O
performed	O
in	O
mean	O
ﬁeld	O
inference	O
to	O
ﬁll	O
in	O
missing	O
inputs	O
in	O
an	O
rbm	O
.	O
this	O
mean	O
ﬁeld	O
inference	O
corresponds	O
to	O
running	O
a	O
recurrent	O
network	O
with	O
shared	O
weights	O
and	O
the	O
ﬁrst	O
step	O
of	O
that	O
inference	O
is	O
the	O
same	O
as	O
in	O
nade	O
.	O
the	O
only	O
diﬀerence	O
is	O
that	O
with	O
nade	O
,	O
the	O
output	O
weights	O
connecting	O
the	O
hidden	O
units	O
to	O
the	O
output	O
are	O
parametrized	O
independently	O
from	O
the	O
weights	O
connecting	O
the	O
input	O
units	O
to	O
the	O
hidden	O
units	O
.	O
in	O
the	O
rbm	O
,	O
the	O
hidden-to-output	O
weights	O
are	O
the	O
transpose	O
of	O
the	O
input-to-hidden	O
weights	O
.	O
the	O
nade	O
architecture	O
can	O
be	O
extended	O
to	O
mimic	O
not	O
just	O
one	O
time	O
step	O
of	O
the	O
mean	O
ﬁeld	O
recurrent	O
inference	O
but	O
to	O
mimic	O
k	O
steps	O
.	O
this	O
approach	O
is	O
called	O
nade-	O
k	O
raiko	O
et	O
al	O
.	O
2014	O
)	O
.	O
(	O
,	O
as	O
mentioned	O
previously	O
,	O
auto-regressive	O
networks	O
may	O
be	O
extend	O
to	O
process	O
continuous-valued	O
data	O
.	O
a	O
particularly	O
powerful	O
and	O
generic	O
way	O
of	O
parametrizing	O
a	O
continuous	O
density	O
is	O
as	O
a	O
gaussian	O
mixture	O
(	O
introduced	O
in	O
section	O
)	O
with	O
mixture	O
weights	O
αi	O
(	O
the	O
coeﬃcient	O
or	O
prior	O
probability	O
for	O
component	O
i	O
)	O
,	O
per-	O
component	O
conditional	O
mean	O
µi	O
and	O
per-component	O
conditional	O
variance	O
σ2	O
i	O
.	O
a	O
)	O
uses	O
this	O
parametrization	O
to	O
extend	O
nade	O
model	B
called	O
rnade	O
(	O
to	O
real	O
values	O
.	O
as	O
with	O
other	O
mixture	O
density	O
networks	O
,	O
the	O
parameters	O
of	O
this	O
uria	O
et	O
al	O
.	O
2013	O
3.9.6	O
,	O
709	O
chapter	O
20.	O
deep	O
generative	O
models	O
distribution	O
are	O
outputs	O
of	O
the	O
network	O
,	O
with	O
the	O
mixture	O
weight	O
probabilities	O
produced	O
by	O
a	O
softmax	O
unit	O
,	O
and	O
the	O
variances	O
parametrized	O
so	O
that	O
they	O
are	O
positive	O
.	O
stochastic	O
gradient	O
descent	B
can	O
be	O
numerically	O
ill-behaved	O
due	O
to	O
the	O
interactions	O
between	O
the	O
conditional	O
means	O
µi	O
and	O
the	O
conditional	O
variances	O
σ2	O
i	O
.	O
to	O
reduce	O
this	O
diﬃculty	O
,	O
)	O
use	O
a	O
pseudo-gradient	O
that	O
replaces	O
the	O
gradient	O
on	O
the	O
mean	O
,	O
in	O
the	O
back-propagation	O
phase	O
.	O
uria	O
et	O
al	O
.	O
2013	O
(	O
,	O
another	O
very	O
interesting	O
extension	O
of	O
the	O
neural	O
auto-regressive	O
architectures	O
gets	O
rid	O
of	O
the	O
need	O
to	O
choose	O
an	O
arbitrary	O
order	O
for	O
the	O
observed	O
variables	O
(	O
murray	O
and	O
larochelle	O
2014	O
)	O
.	O
in	O
auto-regressive	O
networks	O
,	O
the	O
idea	O
is	O
to	O
train	O
the	O
network	O
to	O
be	O
able	O
to	O
cope	O
with	O
any	O
order	O
by	O
randomly	O
sampling	O
orders	O
and	O
providing	O
the	O
information	O
to	O
hidden	O
units	O
specifying	O
which	O
of	O
the	O
inputs	O
are	O
observed	O
(	O
on	O
the	O
right	O
side	O
of	O
the	O
conditioning	O
bar	O
)	O
and	O
which	O
are	O
to	O
be	O
predicted	O
and	O
are	O
thus	O
considered	O
missing	O
(	O
on	O
the	O
left	O
side	O
of	O
the	O
conditioning	O
bar	O
)	O
.	O
this	O
is	O
nice	O
because	O
it	O
allows	O
one	O
to	O
use	O
a	O
trained	O
auto-regressive	O
network	O
to	O
perform	O
any	O
inference	O
problem	O
(	O
i.e	O
.	O
predict	O
or	O
sample	O
from	O
the	O
probability	O
distribution	O
over	O
any	O
subset	O
of	O
variables	O
given	O
any	O
subset	O
)	O
extremely	O
eﬃciently	O
.	O
finally	O
,	O
since	O
many	O
orders	O
of	O
variables	O
are	O
possible	O
(	O
n	O
!	O
for	O
n	O
variables	O
)	O
and	O
each	O
order	O
o	O
of	O
variables	O
yields	O
a	O
diﬀerent	O
,	O
we	O
can	O
form	O
an	O
ensemble	O
of	O
models	O
for	O
many	O
values	O
of	O
o	O
)	O
	O
(	O
x	O
:	O
o	O
p	O
|	O
pensemble	O
(	O
)	O
=x	O
|	O
p	O
(	O
x	O
o	O
(	O
)	O
i	O
)	O
.	O
1	O
k	O
k	O
i=1	O
(	O
20.84	O
)	O
this	O
ensemble	O
model	B
usually	O
generalizes	O
better	O
and	O
assigns	O
higher	O
probability	O
to	O
the	O
test	O
set	O
than	O
does	O
an	O
individual	O
model	B
deﬁned	O
by	O
a	O
single	O
ordering	O
.	O
bengio	O
and	O
bengio	O
2000b	O
in	O
the	O
same	O
paper	O
,	O
the	O
authors	O
propose	O
deep	O
versions	O
of	O
the	O
architecture	O
,	O
but	O
unfortunately	O
that	O
immediately	O
makes	O
computation	O
as	O
expensive	O
as	O
in	O
the	O
original	O
neural	O
auto-regressive	O
neural	O
network	O
(	O
)	O
.	O
the	O
ﬁrst	O
layer	O
and	O
the	O
output	O
layer	O
can	O
still	O
be	O
computed	O
in	O
o	O
(	O
nh	O
)	O
multiply-add	O
operations	O
,	O
as	O
in	O
the	O
regular	O
nade	O
,	O
where	O
h	O
is	O
the	O
number	O
of	O
hidden	O
units	O
(	O
the	O
size	O
of	O
the	O
o	O
(	O
n2h	O
)	O
in	O
bengio	O
and	O
bengio	O
groups	O
hi	O
,	O
in	O
ﬁgures	O
o	O
(	O
n2h2	O
)	O
if	O
every	O
(	O
2000b	O
“	O
previous	O
”	O
group	O
at	O
layer	O
l	O
participates	O
in	O
predicting	O
the	O
“	O
next	O
”	O
group	O
at	O
layer	O
l	O
+	O
1	O
,	O
assuming	O
n	O
groups	O
of	O
h	O
hidden	O
units	O
at	O
each	O
layer	O
.	O
making	O
the	O
i-th	O
group	O
at	O
layer	O
l	O
+	O
1	O
only	O
depend	O
on	O
the	O
i	O
-th	O
group	O
,	O
as	O
in	O
murray	O
and	O
larochelle	O
2014	O
)	O
at	O
layer	O
l	O
reduces	O
it	O
to	O
o	O
nh	O
(	O
)	O
.	O
however	O
,	O
for	O
the	O
other	O
hidden	O
layers	O
,	O
the	O
computation	O
is	O
times	O
worse	O
than	O
the	O
regular	O
nade	O
.	O
2	O
)	O
,	O
which	O
is	O
still	O
)	O
,	O
whereas	O
it	O
is	O
20.10	O
20.9	O
and	O
,	O
(	O
h	O
710	O
chapter	O
20.	O
deep	O
generative	O
models	O
20.11	O
drawing	O
samples	O
from	O
autoencoders	O
14	O
in	O
chapter	O
,	O
we	O
saw	O
that	O
many	O
kinds	O
of	O
autoencoders	O
learn	O
the	O
data	O
distribution	O
.	O
there	O
are	O
close	O
connections	O
between	O
score	O
matching	O
,	O
denoising	O
autoencoders	O
,	O
and	O
contractive	O
autoencoders	O
.	O
these	O
connections	O
demonstrate	O
that	O
some	O
kinds	O
of	O
autoencoders	O
learn	O
the	O
data	O
distribution	O
in	O
some	O
way	O
.	O
we	O
have	O
not	O
yet	O
seen	O
how	O
to	O
draw	O
samples	O
from	O
such	O
models	O
.	O
some	O
kinds	O
of	O
autoencoders	O
,	O
such	O
as	O
the	O
variational	O
autoencoder	O
,	O
explicitly	O
represent	O
a	O
probability	O
distribution	O
and	O
admit	O
straightforward	O
ancestral	O
sampling	O
.	O
most	O
other	O
kinds	O
of	O
autoencoders	O
require	O
mcmc	O
sampling	O
.	O
contractive	O
autoencoders	O
are	O
designed	O
to	O
recover	O
an	O
estimate	O
of	O
the	O
tangent	O
plane	O
of	O
the	O
data	O
manifold	O
.	O
this	O
means	O
that	O
repeated	O
encoding	O
and	O
decoding	O
with	O
injected	O
noise	O
will	O
induce	O
a	O
random	O
walk	O
along	O
the	O
surface	O
of	O
the	O
manifold	O
(	O
rifai	O
et	O
al	O
.	O
)	O
.	O
this	O
manifold	O
diﬀusion	O
technique	O
is	O
a	O
kind	O
of	O
,	O
markov	O
chain	O
.	O
2012	O
mesnil	O
et	O
al	O
.	O
,	O
2012	O
;	O
there	O
is	O
also	O
a	O
more	O
general	O
markov	O
chain	O
that	O
can	O
sample	O
from	O
any	O
denoising	O
autoencoder	O
.	O
20.11.1	O
markov	O
chain	O
associated	O
with	O
any	O
denoising	O
autoen-	O
coder	O
the	O
above	O
discussion	O
left	O
open	O
the	O
question	O
of	O
what	O
noise	O
to	O
inject	O
and	O
where	O
,	O
in	O
order	O
to	O
obtain	O
a	O
markov	O
chain	O
that	O
would	O
generate	O
from	O
the	O
distribution	O
estimated	O
by	O
the	O
autoencoder.	O
)	O
showed	O
how	O
to	O
construct	O
such	O
a	O
markov	O
chain	O
for	O
generalized	O
denoising	O
autoencoders	O
.	O
generalized	O
denoising	O
autoencoders	O
are	O
speciﬁed	O
by	O
a	O
denoising	O
distribution	O
for	O
sampling	O
an	O
estimate	O
of	O
the	O
clean	O
input	O
given	O
the	O
corrupted	O
input	O
.	O
bengio	O
et	O
al	O
.	O
2013c	O
(	O
each	O
step	O
of	O
the	O
markov	O
chain	O
that	O
generates	O
from	O
the	O
estimated	O
distribution	O
consists	O
of	O
the	O
following	O
sub-steps	O
,	O
illustrated	O
in	O
ﬁgure	O
20.11	O
:	O
1.	O
starting	O
from	O
the	O
previous	O
state	O
x	O
,	O
inject	O
corruption	O
noise	O
,	O
sampling	O
˜x	O
from	O
|	O
c	O
(	O
˜x	O
x	O
)	O
.	O
2.	O
encode	O
˜x	O
into	O
h	O
=	O
(	O
f	O
˜x	O
)	O
.	O
3.	O
decode	O
h	O
|	O
x	O
(	O
p	O
|	O
(	O
)	O
)	O
=	O
h	O
p	O
(	O
x	O
|	O
˜x	O
)	O
.	O
ω	O
g	O
=	O
˜x	O
)	O
.	O
to	O
obtain	O
the	O
parameters	O
|	O
ω	O
h=	O
(	O
g	O
)	O
of	O
4.	O
sample	O
the	O
next	O
state	O
fromx	O
(	O
p	O
x	O
ω	O
g	O
=	O
(	O
)	O
)	O
=	O
h	O
p	O
(	O
x	O
711	O
chapter	O
20.	O
deep	O
generative	O
models	O
hh	O
g	O
f	O
˜x˜x	O
|	O
c	O
(	O
˜x	O
x	O
)	O
xx	O
ωω	O
|	O
p	O
(	O
x	O
ω	O
)	O
ˆxˆx	O
figure	O
20.11	O
:	O
each	O
step	O
of	O
the	O
markov	O
chain	O
associated	O
with	O
a	O
trained	O
denoising	O
autoen-	O
coder	O
,	O
that	O
generates	O
the	O
samples	O
from	O
the	O
probabilistic	O
model	B
implicitly	O
trained	O
by	O
the	O
denoising	O
log-likelihood	O
criterion	O
.	O
each	O
step	O
consists	O
in	O
(	O
a	O
)	O
injecting	O
noise	O
via	O
corruption	O
process	O
c	O
in	O
state	O
x	O
,	O
yielding	O
˜x	O
,	O
(	O
b	O
)	O
encoding	O
it	O
with	O
function	O
f	O
,	O
yielding	O
h	O
=	O
f	O
(	O
˜x	O
)	O
,	O
(	O
c	O
)	O
decoding	O
the	O
result	O
with	O
function	O
g	O
,	O
yielding	O
parameters	O
ω	O
for	O
the	O
reconstruction	O
|	O
distribution	O
,	O
and	O
(	O
d	O
)	O
given	O
ω	O
,	O
sampling	O
a	O
new	O
state	O
from	O
the	O
reconstruction	O
distribution	O
ω	O
=	O
g	O
(	O
f	O
(	O
˜x	O
)	O
)	O
)	O
.	O
in	O
the	O
typical	O
squared	O
reconstruction	O
error	O
case	O
,	O
g	O
(	O
h	O
)	O
=	O
ˆx	O
,	O
which	O
p	O
(	O
x	O
|	O
estimates	O
e	O
[	O
x	O
˜x	O
]	O
,	O
corruption	O
consists	O
in	O
adding	O
gaussian	O
noise	O
and	O
sampling	O
from	O
ω	O
)	O
consists	O
in	O
adding	O
gaussian	O
noise	O
,	O
a	O
second	O
time	O
,	O
to	O
the	O
reconstruction	O
ˆx	O
.	O
the	O
p	O
(	O
x	O
latter	O
noise	O
level	O
should	O
correspond	O
to	O
the	O
mean	O
squared	O
error	O
of	O
reconstructions	O
,	O
whereas	O
the	O
injected	O
noise	O
is	O
a	O
hyperparameter	O
that	O
controls	O
the	O
mixing	O
speed	O
as	O
well	O
as	O
the	O
extent	O
to	O
which	O
the	O
estimator	O
smooths	O
the	O
empirical	O
distribution	O
(	O
)	O
.	O
in	O
the	O
example	O
illustrated	O
here	O
,	O
only	O
the	O
c	O
and	O
p	O
conditionals	O
are	O
stochastic	O
steps	O
(	O
f	O
and	O
g	O
are	O
deterministic	O
computations	O
)	O
,	O
although	O
noise	O
can	O
also	O
be	O
injected	O
inside	O
the	O
autoencoder	O
,	O
as	O
in	O
generative	O
stochastic	O
networks	O
(	O
bengio	O
et	O
al	O
.	O
2014	O
)	O
.	O
vincent	O
2011	O
,	O
|	O
,	O
712	O
chapter	O
20.	O
deep	O
generative	O
models	O
|	O
)	O
showed	O
that	O
if	O
the	O
autoencoder	O
p	O
(	O
x	O
2014	O
et	O
al	O
.	O
(	O
˜x	O
)	O
forms	O
a	O
consistent	O
bengio	O
estimator	O
of	O
the	O
corresponding	O
true	O
conditional	O
distribution	O
,	O
then	O
the	O
stationary	O
distribution	O
of	O
the	O
above	O
markov	O
chain	O
forms	O
a	O
consistent	O
estimator	O
(	O
albeit	O
an	O
implicit	O
one	O
)	O
of	O
the	O
data	O
generating	O
distribution	O
of	O
.x	O
20.11.2	O
clamping	O
and	O
conditional	O
sampling	O
|	O
similarly	O
to	O
boltzmann	O
machines	O
,	O
denoising	O
autoencoders	O
and	O
their	O
generalizations	O
(	O
such	O
as	O
gsns	O
,	O
described	O
below	O
)	O
can	O
be	O
used	O
to	O
sample	O
from	O
a	O
conditional	O
distri-	O
xo	O
)	O
,	O
simply	O
by	O
clamping	O
the	O
observed	O
units	O
xf	O
and	O
only	O
resampling	O
bution	O
p	O
(	O
xf	O
the	O
free	O
units	O
xo	O
given	O
xf	O
and	O
the	O
sampled	O
latent	O
variables	O
(	O
if	O
any	O
)	O
.	O
for	O
example	O
,	O
mp-dbms	O
can	O
be	O
interpreted	O
as	O
a	O
form	O
of	O
denoising	O
autoencoder	O
,	O
and	O
are	O
able	O
to	O
sample	O
missing	O
inputs	O
.	O
gsns	O
later	O
generalized	O
some	O
of	O
the	O
ideas	O
present	O
in	O
mp-dbms	O
to	O
perform	O
the	O
same	O
operation	O
(	O
bengio	O
et	O
al	O
.	O
2014	O
alain	O
et	O
al	O
.	O
2015	O
)	O
identiﬁed	O
a	O
missing	O
condition	O
from	O
proposition	O
1	O
of	O
)	O
,	O
which	O
is	O
that	O
the	O
transition	O
operator	O
(	O
deﬁned	O
by	O
the	O
stochastic	O
mapping	O
going	O
from	O
one	O
state	O
of	O
the	O
chain	O
to	O
the	O
next	O
)	O
should	O
satisfy	O
a	O
property	O
called	O
detailed	O
balance	O
,	O
which	O
speciﬁes	O
that	O
a	O
markov	O
chain	O
at	O
equilibrium	O
will	O
remain	O
in	O
equilibrium	O
whether	O
the	O
transition	O
operator	O
is	O
run	O
in	O
forward	O
or	O
reverse	O
.	O
bengio	O
et	O
al	O
.	O
2014	O
)	O
.	O
(	O
,	O
(	O
an	O
experiment	O
in	O
clamping	O
half	O
of	O
the	O
pixels	O
(	O
the	O
right	O
part	O
of	O
the	O
image	O
)	O
and	O
running	O
the	O
markov	O
chain	O
on	O
the	O
other	O
half	O
is	O
shown	O
in	O
ﬁgure	O
20.12	O
.	O
713	O
chapter	O
20.	O
deep	O
generative	O
models	O
figure	O
20.12	O
:	O
illustration	O
of	O
clamping	O
the	O
right	O
half	O
of	O
the	O
image	O
and	O
running	O
the	O
markov	O
chain	O
by	O
resampling	O
only	O
the	O
left	O
half	O
at	O
each	O
step	O
.	O
these	O
samples	O
come	O
from	O
a	O
gsn	O
trained	O
to	O
reconstruct	O
mnist	O
digits	O
at	O
each	O
time	O
step	O
using	O
the	O
walkback	O
procedure	O
.	O
20.11.3	O
walk-back	O
training	O
procedure	O
bengio	O
et	O
al	O
.	O
2013c	O
the	O
walk-back	O
training	O
procedure	O
was	O
proposed	O
by	O
)	O
as	O
a	O
way	O
to	O
accelerate	O
the	O
convergence	O
of	O
generative	O
training	O
of	O
denoising	O
autoencoders	O
.	O
instead	O
of	O
performing	O
a	O
one-step	O
encode-decode	O
reconstruction	O
,	O
this	O
procedure	O
consists	O
in	O
alternative	O
multiple	O
stochastic	O
encode-decode	O
steps	O
(	O
as	O
in	O
the	O
generative	O
markov	O
chain	O
)	O
initialized	O
at	O
a	O
training	O
example	O
(	O
just	O
like	O
with	O
the	O
contrastive	O
divergence	O
algorithm	O
,	O
described	O
in	O
section	O
)	O
and	O
penalizing	O
the	O
last	O
probabilistic	O
reconstructions	O
(	O
or	O
all	O
of	O
the	O
reconstructions	O
along	O
the	O
way	O
)	O
.	O
18.2	O
(	O
training	O
with	O
k	O
steps	O
is	O
equivalent	O
(	O
in	O
the	O
sense	O
of	O
achieving	O
the	O
same	O
stationary	O
distribution	O
)	O
as	O
training	O
with	O
one	O
step	O
,	O
but	O
practically	O
has	O
the	O
advantage	O
that	O
spurious	O
modes	O
further	O
from	O
the	O
data	O
can	O
be	O
removed	O
more	O
eﬃciently	O
.	O
20.12	O
generative	O
stochastic	O
networks	O
generative	O
stochastic	O
networks	O
or	O
gsns	O
(	O
)	O
are	O
generaliza-	O
tions	O
of	O
denoising	O
autoencoders	O
that	O
include	O
latent	O
variables	O
h	O
in	O
the	O
generative	O
bengio	O
et	O
al	O
.	O
2014	O
,	O
714	O
chapter	O
20.	O
deep	O
generative	O
models	O
markov	O
chain	O
,	O
in	O
addition	O
to	O
the	O
visible	O
variables	O
(	O
usually	O
denoted	O
)	O
.x	O
a	O
gsn	O
is	O
parametrized	O
by	O
two	O
conditional	O
probability	O
distributions	O
which	O
specify	O
one	O
step	O
of	O
the	O
markov	O
chain	O
:	O
1.	O
p	O
(	O
x	O
(	O
)	O
k	O
h	O
(	O
)	O
k	O
)	O
tells	O
how	O
to	O
generate	O
the	O
next	O
visible	O
variable	O
given	O
the	O
current	O
latent	O
state	O
.	O
such	O
a	O
“	O
reconstruction	O
distribution	O
”	O
is	O
also	O
found	O
in	O
denoising	O
autoencoders	O
,	O
rbms	O
,	O
dbns	O
and	O
dbms	O
.	O
|	O
|	O
2.	O
p	O
(	O
h	O
(	O
)	O
k	O
−	O
h	O
(	O
1	O
)	O
k	O
−	O
1	O
)	O
k	O
,	O
x	O
(	O
)	O
tells	O
how	O
to	O
update	O
the	O
latent	O
state	O
variable	O
,	O
given	O
the	O
previous	O
latent	O
state	O
and	O
visible	O
variable	O
.	O
denoising	O
autoencoders	O
and	O
gsns	O
diﬀer	O
from	O
classical	O
probabilistic	O
models	O
(	O
directed	O
or	O
undirected	O
)	O
in	O
that	O
they	O
parametrize	O
the	O
generative	O
process	O
itself	O
rather	O
than	O
the	O
mathematical	O
speciﬁcation	O
of	O
the	O
joint	O
distribution	O
of	O
visible	O
and	O
latent	O
variables	O
.	O
instead	O
,	O
the	O
latter	O
is	O
deﬁned	O
,	O
as	O
the	O
stationary	O
distribution	O
of	O
the	O
generative	O
markov	O
chain	O
.	O
the	O
conditions	B
for	O
existence	O
of	O
the	O
stationary	O
distribution	O
are	O
mild	O
and	O
are	O
the	O
same	O
conditions	B
required	O
by	O
standard	O
mcmc	O
methods	O
(	O
see	O
section	O
)	O
.	O
these	O
conditions	B
are	O
necessary	O
to	O
guarantee	O
that	O
the	O
chain	O
mixes	O
,	O
but	O
they	O
can	O
be	O
violated	O
by	O
some	O
choices	O
of	O
the	O
transition	O
distributions	O
(	O
for	O
example	O
,	O
if	O
they	O
were	O
deterministic	O
)	O
.	O
implicitly	O
if	O
it	O
exists	O
17.3	O
,	O
(	O
bengio	O
et	O
al	O
.	O
2014	O
one	O
could	O
imagine	O
diﬀerent	O
training	O
criteria	O
for	O
gsns	O
.	O
the	O
one	O
proposed	O
and	O
evaluated	O
by	O
)	O
is	O
simply	O
reconstruction	O
log-probability	O
on	O
the	O
visible	O
units	O
,	O
just	O
like	O
for	O
denoising	O
autoencoders	O
.	O
this	O
is	O
achieved	O
by	O
clamping	O
x	O
(	O
0	O
)	O
=	O
x	O
to	O
the	O
observed	O
example	O
and	O
maximizing	O
the	O
probability	O
of	O
generating	O
x	O
at	O
some	O
subsequent	O
time	O
steps	O
,	O
i.e.	O
,	O
maximizing	O
log	O
p	O
(	O
x	O
(	O
)	O
k	O
=	O
x	O
h	O
(	O
)	O
k	O
)	O
,	O
where	O
h	O
(	O
)	O
k	O
is	O
sampled	O
from	O
the	O
chain	O
,	O
given	O
x	O
(	O
0	O
)	O
=	O
x.	O
in	O
order	O
to	O
estimate	O
the	O
gradient	O
of	O
log	O
p	O
(	O
x	O
(	O
)	O
k	O
=	O
x	O
h	O
(	O
)	O
k	O
)	O
with	O
respect	O
to	O
the	O
other	O
pieces	O
of	O
the	O
model	B
,	O
bengio	O
et	O
al	O
.	O
(	O
2014	O
)	O
use	O
the	O
reparametrization	O
trick	B
,	O
introduced	O
in	O
section	O
.	O
20.9	O
|	O
|	O
the	O
walk-back	O
training	O
protocol	O
(	O
described	O
in	O
section	O
)	O
to	O
improve	O
training	O
convergence	O
of	O
gsns	O
.	O
et	O
al.	O
,	O
2014	O
gio	O
20.12.1	O
discriminant	O
gsns	O
20.11.3	O
)	O
was	O
used	O
(	O
ben-	O
the	O
original	O
formulation	O
of	O
gsns	O
(	O
)	O
was	O
meant	O
for	O
unsupervised	O
learning	O
and	O
implicitly	O
modeling	O
p	O
(	O
x	O
)	O
for	O
observed	O
data	O
x	O
,	O
but	O
it	O
is	O
possible	O
to	O
modify	O
the	O
framework	O
to	O
optimize	O
bengio	O
et	O
al	O
.	O
2014	O
.	O
x	O
)	O
p	O
(	O
y	O
|	O
,	O
for	O
example	O
,	O
zhou	O
and	O
troyanskaya	O
2014	O
)	O
generalize	O
gsns	O
in	O
this	O
way	O
,	O
by	O
only	O
back-propagating	O
the	O
reconstruction	O
log-probability	O
over	O
the	O
output	O
variables	O
,	O
keeping	O
the	O
input	O
variables	O
ﬁxed	O
.	O
they	O
applied	O
this	O
successfully	O
to	O
model	B
sequences	O
(	O
715	O
chapter	O
20.	O
deep	O
generative	O
models	O
(	O
protein	O
secondary	O
structure	O
)	O
and	O
introduced	O
a	O
(	O
one-dimensional	O
)	O
convolutional	O
structure	O
in	O
the	O
transition	O
operator	O
of	O
the	O
markov	O
chain	O
.	O
it	O
is	O
important	O
to	O
remember	O
that	O
,	O
for	O
each	O
step	O
of	O
the	O
markov	O
chain	O
,	O
one	O
generates	O
a	O
new	O
sequence	O
for	O
each	O
layer	O
,	O
and	O
that	O
sequence	O
is	O
the	O
input	O
for	O
computing	O
other	O
layer	O
values	O
(	O
say	O
the	O
one	O
below	O
and	O
the	O
one	O
above	O
)	O
at	O
the	O
next	O
time	O
step	O
.	O
hence	O
the	O
markov	O
chain	O
is	O
really	O
over	O
the	O
output	O
variable	O
(	O
and	O
associated	O
higher-	O
level	O
hidden	O
layers	O
)	O
,	O
and	O
the	O
input	O
sequence	O
only	O
serves	O
to	O
condition	O
that	O
chain	O
,	O
with	O
back-propagation	O
allowing	O
to	O
learn	O
how	O
the	O
input	O
sequence	O
can	O
condition	O
the	O
output	O
distribution	O
implicitly	O
represented	O
by	O
the	O
markov	O
chain	O
.	O
it	O
is	O
therefore	O
a	O
case	O
of	O
using	O
the	O
gsn	O
in	O
the	O
context	O
of	O
structured	O
outputs	O
.	O
(	O
zöhrer	O
and	O
pernkopf	O
2014	O
)	O
introduced	O
a	O
hybrid	O
model	B
that	O
combines	O
a	O
super-	O
vised	O
objective	O
(	O
as	O
in	O
the	O
above	O
work	B
)	O
and	O
an	O
unsupervised	O
objective	O
(	O
as	O
in	O
the	O
original	O
gsn	O
work	B
)	O
,	O
by	O
simply	O
adding	O
(	O
with	O
a	O
diﬀerent	O
weight	O
)	O
the	O
supervised	O
and	O
unsupervised	O
costs	O
i.e.	O
,	O
the	O
reconstruction	O
log-probabilities	O
of	O
y	O
and	O
x	O
respectively	O
.	O
such	O
a	O
hybrid	O
criterion	O
had	O
previously	O
been	O
introduced	O
for	O
rbms	O
by	O
larochelle	O
and	O
bengio	O
2008	O
)	O
.	O
they	O
show	O
improved	O
classiﬁcation	O
performance	O
using	O
this	O
scheme	O
.	O
(	O
20.13	O
other	O
generation	O
schemes	O
the	O
methods	O
we	O
have	O
described	O
so	O
far	O
use	O
either	O
mcmc	O
sampling	O
,	O
ancestral	O
sampling	O
,	O
or	O
some	O
mixture	O
of	O
the	O
two	O
to	O
generate	O
samples	O
.	O
while	O
these	O
are	O
the	O
most	O
popular	O
approaches	O
to	O
generative	O
modeling	O
,	O
they	O
are	O
by	O
no	O
means	O
the	O
only	O
approaches	O
.	O
2015	O
et	O
al	O
.	O
(	O
sohl-dickstein	O
)	O
developed	O
a	O
diﬀusion	O
inversion	O
training	O
scheme	O
for	O
learning	O
a	O
generative	O
model	B
,	O
based	O
on	O
non-equilibrium	O
thermodynamics	O
.	O
the	O
approach	O
is	O
based	O
on	O
the	O
idea	O
that	O
the	O
probability	O
distributions	O
we	O
wish	O
to	O
sample	O
from	O
have	O
structure	O
.	O
this	O
structure	O
can	O
gradually	O
be	O
destroyed	O
by	O
a	O
diﬀusion	O
process	O
that	O
incrementally	O
changes	O
the	O
probability	O
distribution	O
to	O
have	O
more	O
entropy	O
.	O
to	O
form	O
a	O
generative	O
model	B
,	O
we	O
can	O
run	O
the	O
process	O
in	O
reverse	O
,	O
by	O
training	O
a	O
model	B
that	O
gradually	O
restores	O
the	O
structure	O
to	O
an	O
unstructured	O
distribution	O
.	O
by	O
iteratively	O
applying	O
a	O
process	O
that	O
brings	O
a	O
distribution	O
closer	O
to	O
the	O
target	O
one	O
,	O
we	O
can	O
gradually	O
approach	O
that	O
target	O
distribution	O
.	O
this	O
approach	O
resembles	O
mcmc	O
methods	O
in	O
the	O
sense	O
that	O
it	O
involves	O
many	O
iterations	O
to	O
produce	O
a	O
sample	O
.	O
however	O
,	O
the	O
model	B
is	O
deﬁned	O
to	O
be	O
the	O
probability	O
distribution	O
produced	O
by	O
the	O
ﬁnal	O
step	O
of	O
the	O
chain	O
.	O
in	O
this	O
sense	O
,	O
there	O
is	O
no	O
approximation	O
induced	O
by	O
the	O
iterative	O
procedure	O
.	O
the	O
approach	O
introduced	O
by	O
sohl-dickstein	O
et	O
al	O
.	O
2015	O
)	O
is	O
also	O
very	O
close	O
to	O
the	O
generative	O
interpretation	O
of	O
the	O
denoising	O
autoencoder	O
(	O
716	O
chapter	O
20.	O
deep	O
generative	O
models	O
20.11.1	O
)	O
.	O
as	O
with	O
the	O
denoising	O
autoencoder	O
,	O
diﬀusion	O
inversion	O
trains	O
a	O
(	O
section	O
transition	O
operator	O
that	O
attempts	O
to	O
probabilistically	O
undo	O
the	O
eﬀect	O
of	O
adding	O
some	O
noise	O
.	O
the	O
diﬀerence	O
is	O
that	O
diﬀusion	O
inversion	O
requres	O
undoing	O
only	O
one	O
step	O
of	O
the	O
diﬀusion	O
process	O
,	O
rather	O
than	O
traveling	O
all	O
the	O
way	O
back	O
to	O
a	O
clean	O
data	O
point	O
.	O
this	O
addresses	O
the	O
following	O
dilemma	O
present	O
with	O
the	O
ordinary	O
reconstruction	O
log-likelihood	O
objective	O
of	O
denoising	O
autoencoders	O
:	O
with	O
small	O
levels	O
of	O
noise	O
the	O
learner	O
only	O
sees	O
conﬁgurations	O
near	O
the	O
data	O
points	O
,	O
while	O
with	O
large	O
levels	O
of	O
noise	O
it	O
is	O
asked	O
to	O
do	O
an	O
almost	O
impossible	O
job	O
(	O
because	O
the	O
denoising	O
distribution	O
is	O
highly	O
complex	O
and	O
multi-modal	O
)	O
.	O
with	O
the	O
diﬀusion	O
inversion	O
objective	O
,	O
the	O
learner	O
can	O
learn	O
the	O
shape	O
of	O
the	O
density	O
around	O
the	O
data	O
points	O
more	O
precisely	O
as	O
well	O
as	O
remove	O
spurious	O
modes	O
that	O
could	O
show	O
up	O
far	O
from	O
the	O
data	O
points	O
.	O
,	O
rubin	O
et	O
al	O
.	O
1984	O
another	O
approach	O
to	O
sample	O
generation	O
is	O
the	O
approximate	O
bayesian	O
com-	O
)	O
.	O
in	O
this	O
approach	O
,	O
samples	O
are	O
putation	O
(	O
abc	O
)	O
framework	O
(	O
rejected	O
or	O
modiﬁed	O
in	O
order	O
to	O
make	O
the	O
moments	O
of	O
selected	O
functions	O
of	O
the	O
samples	O
match	O
those	O
of	O
the	O
desired	O
distribution	O
.	O
while	O
this	O
idea	O
uses	O
the	O
moments	O
of	O
the	O
samples	O
like	O
in	O
moment	O
matching	O
,	O
it	O
is	O
diﬀerent	O
from	O
moment	O
matching	O
because	O
it	O
modiﬁes	O
the	O
samples	O
themselves	O
,	O
rather	O
than	O
training	O
the	O
model	B
to	O
automatically	O
emit	O
samples	O
with	O
the	O
correct	O
moments	O
.	O
bachman	O
and	O
precup	O
2015	O
)	O
showed	O
how	O
to	O
use	O
ideas	O
from	O
abc	O
in	O
the	O
context	O
of	O
deep	O
learning	O
,	O
by	O
using	O
abc	O
to	O
shape	O
the	O
mcmc	O
trajectories	O
of	O
gsns	O
.	O
(	O
we	O
expect	O
that	O
many	O
other	O
possible	O
approaches	O
to	O
generative	O
modeling	O
await	O
discovery	O
.	O
20.14	O
evaluating	O
generative	O
models	O
researchers	O
studying	O
generative	O
models	O
often	O
need	O
to	O
compare	O
one	O
generative	O
model	B
to	O
another	O
,	O
usually	O
in	O
order	O
to	O
demonstrate	O
that	O
a	O
newly	O
invented	O
generative	O
model	B
is	O
better	O
at	O
capturing	O
some	O
distribution	O
than	O
the	O
pre-existing	O
models	O
.	O
this	O
can	O
be	O
a	O
diﬃcult	O
and	O
subtle	O
task	O
.	O
in	O
many	O
cases	O
,	O
we	O
can	O
not	O
actually	O
evaluate	O
the	O
log	O
probability	O
of	O
the	O
data	O
under	O
the	O
model	B
,	O
but	O
only	O
an	O
approximation	O
.	O
in	O
these	O
cases	O
,	O
it	O
is	O
important	O
to	O
think	O
and	O
communicate	O
clearly	O
about	O
exactly	O
what	O
is	O
being	O
measured	O
.	O
for	O
example	O
,	O
suppose	O
we	O
can	O
evaluate	O
a	O
stochastic	O
estimate	O
of	O
the	O
log-likelihood	O
for	O
model	B
a	O
,	O
and	O
a	O
deterministic	O
lower	O
bound	B
on	O
the	O
log-likelihood	O
for	O
model	B
b.	O
if	O
model	B
a	O
gets	O
a	O
higher	O
score	O
than	O
model	B
b	O
,	O
which	O
is	O
better	O
?	O
if	O
we	O
care	O
about	O
determining	O
which	O
model	B
has	O
a	O
better	O
internal	O
representation	O
of	O
the	O
distribution	O
,	O
we	O
actually	O
can	O
not	O
tell	O
,	O
unless	O
we	O
have	O
some	O
way	O
of	O
determining	O
how	O
loose	O
the	O
bound	B
for	O
model	B
b	O
is	O
.	O
however	O
,	O
if	O
we	O
care	O
about	O
how	O
well	O
we	O
can	O
use	O
the	O
model	B
in	O
practice	O
,	O
for	O
example	O
to	O
perform	O
anomaly	O
detection	O
,	O
then	O
it	O
is	O
fair	O
to	O
717	O
chapter	O
20.	O
deep	O
generative	O
models	O
say	O
that	O
a	O
model	B
is	O
preferable	O
based	O
on	O
a	O
criterion	O
speciﬁc	O
to	O
the	O
practical	O
task	O
of	O
interest	O
,	O
e.g.	O
,	O
based	O
on	O
ranking	O
test	O
examples	O
and	O
ranking	O
criteria	O
such	O
as	O
precision	O
and	O
recall	O
.	O
another	O
subtlety	O
of	O
evaluating	O
generative	O
models	O
is	O
that	O
the	O
evaluation	O
metrics	O
are	O
often	O
hard	O
research	O
problems	O
in	O
and	O
of	O
themselves	O
.	O
it	O
can	O
be	O
very	O
diﬃcult	O
to	O
establish	O
that	O
models	O
are	O
being	O
compared	O
fairly	O
.	O
for	O
example	O
,	O
suppose	O
we	O
use	O
ais	O
to	O
estimate	O
log	O
z	O
in	O
order	O
to	O
compute	O
log	O
˜p	O
(	O
x	O
)	O
log	O
z	O
for	O
a	O
new	O
model	B
we	O
have	O
just	O
invented	O
.	O
a	O
computationally	O
economical	O
implementation	O
of	O
ais	O
may	O
fail	O
to	O
ﬁnd	O
several	O
modes	O
of	O
the	O
model	B
distribution	O
and	O
underestimate	O
z	O
,	O
which	O
will	O
result	O
in	O
us	O
overestimating	O
log	O
p	O
(	O
x	O
)	O
.	O
it	O
can	O
thus	O
be	O
diﬃcult	O
to	O
tell	O
whether	O
a	O
high	O
likelihood	O
estimate	O
is	O
due	O
to	O
a	O
good	O
model	B
or	O
a	O
bad	O
ais	O
implementation	O
.	O
−	O
other	O
ﬁelds	O
of	O
machine	O
learning	O
usually	O
allow	O
for	O
some	O
variation	O
in	O
the	O
pre-	O
processing	O
of	O
the	O
data	O
.	O
for	O
example	O
,	O
when	O
comparing	O
the	O
accuracy	O
of	O
object	O
recognition	B
algorithms	O
,	O
it	O
is	O
usually	O
acceptable	O
to	O
preprocess	O
the	O
input	O
images	O
slightly	O
diﬀerently	O
for	O
each	O
algorithm	O
based	O
on	O
what	O
kind	O
of	O
input	O
requirements	O
it	O
has	O
.	O
generative	O
modeling	O
is	O
diﬀerent	O
because	O
changes	O
in	O
preprocessing	O
,	O
even	O
very	O
small	O
and	O
subtle	O
ones	O
,	O
are	O
completely	O
unacceptable	O
.	O
any	O
change	O
to	O
the	O
input	O
data	O
changes	O
the	O
distribution	O
to	O
be	O
captured	O
and	O
fundamentally	O
alters	O
the	O
task	O
.	O
for	O
example	O
,	O
multiplying	O
the	O
input	O
by	O
0.1	O
will	O
artiﬁcially	O
increase	O
likelihood	O
by	O
a	O
factor	O
of	O
10.	O
issues	O
with	O
preprocessing	O
commonly	O
arise	O
when	O
benchmarking	O
generative	O
models	O
on	O
the	O
mnist	O
dataset	O
,	O
one	O
of	O
the	O
more	O
popular	O
generative	O
modeling	O
benchmarks	O
.	O
mnist	O
consists	O
of	O
grayscale	O
images	O
.	O
some	O
models	O
treat	O
mnist	O
images	O
as	O
points	O
in	O
a	O
real	O
vector	O
space	O
,	O
while	O
others	O
treat	O
them	O
as	O
binary	O
.	O
yet	O
others	O
treat	O
the	O
grayscale	O
values	O
as	O
probabilities	O
for	O
a	O
binary	O
samples	O
.	O
it	O
is	O
essential	O
to	O
compare	O
real-valued	O
models	O
only	O
to	O
other	O
real-valued	O
models	O
and	O
binary-valued	O
models	O
only	O
to	O
other	O
binary-valued	O
models	O
.	O
otherwise	O
the	O
likelihoods	O
measured	O
are	O
not	O
on	O
the	O
same	O
space	O
.	O
for	O
binary-valued	O
models	O
,	O
the	O
log-likelihood	O
can	O
be	O
at	O
most	O
zero	O
,	O
while	O
for	O
real-valued	O
models	O
it	O
can	O
be	O
arbitrarily	O
high	O
,	O
since	O
it	O
is	O
the	O
measurement	O
of	O
a	O
density	O
.	O
among	O
binary	O
models	O
,	O
it	O
is	O
important	O
to	O
compare	O
models	O
using	O
exactly	O
the	O
same	O
kind	O
of	O
binarization	O
.	O
for	O
example	O
,	O
we	O
might	O
binarize	O
a	O
gray	O
pixel	O
to	O
0	O
or	O
1	O
by	O
thresholding	O
at	O
0.5	O
,	O
or	O
by	O
drawing	O
a	O
random	O
sample	O
whose	O
probability	O
of	O
being	O
1	O
is	O
given	O
by	O
the	O
gray	O
pixel	O
intensity	O
.	O
if	O
we	O
use	O
the	O
random	O
binarization	O
,	O
we	O
might	O
binarize	O
the	O
whole	O
dataset	O
once	O
,	O
or	O
we	O
might	O
draw	O
a	O
diﬀerent	O
random	O
example	O
for	O
each	O
step	O
of	O
training	O
and	O
then	O
draw	O
multiple	O
samples	O
for	O
evaluation	O
.	O
each	O
of	O
these	O
three	O
schemes	O
yields	O
wildly	O
diﬀerent	O
likelihood	O
numbers	O
,	O
and	O
when	O
comparing	O
diﬀerent	O
models	O
it	O
is	O
important	O
that	O
both	O
models	O
use	O
the	O
same	O
binarization	O
scheme	O
for	O
training	O
and	O
for	O
evaluation	O
.	O
in	O
fact	O
,	O
researchers	O
who	O
apply	O
a	O
single	O
random	O
718	O
chapter	O
20.	O
deep	O
generative	O
models	O
binarization	O
step	O
share	O
a	O
ﬁle	O
containing	O
the	O
results	O
of	O
the	O
random	O
binarization	O
,	O
so	O
that	O
there	O
is	O
no	O
diﬀerence	O
in	O
results	O
based	O
on	O
diﬀerent	O
outcomes	O
of	O
the	O
binarization	O
step	O
.	O
2015	O
et	O
al.	O
,	O
because	O
being	O
able	O
to	O
generate	O
realistic	O
samples	O
from	O
the	O
data	O
distribution	O
is	O
one	O
of	O
the	O
goals	O
of	O
a	O
generative	O
model	B
,	O
practitioners	O
often	O
evaluate	O
generative	O
models	O
by	O
visually	O
inspecting	O
the	O
samples	O
.	O
in	O
the	O
best	O
case	O
,	O
this	O
is	O
done	O
not	O
by	O
the	O
researchers	O
themselves	O
,	O
but	O
by	O
experimental	O
subjects	O
who	O
do	O
not	O
know	O
the	O
source	O
of	O
the	O
samples	O
(	O
denton	O
)	O
.	O
unfortunately	O
,	O
it	O
is	O
possible	O
for	O
a	O
very	O
poor	O
probabilistic	O
model	B
to	O
produce	O
very	O
good	O
samples	O
.	O
a	O
common	O
practice	O
to	O
verify	O
if	O
the	O
model	B
only	O
copies	O
some	O
of	O
the	O
training	O
examples	O
is	O
illustrated	O
in	O
ﬁgure	O
16.1	O
.	O
the	O
idea	O
is	O
to	O
show	O
for	O
some	O
of	O
the	O
generated	O
samples	O
their	O
nearest	O
neighbor	O
in	O
the	O
training	O
set	O
,	O
according	O
to	O
euclidean	O
distance	O
in	O
the	O
space	O
of	O
x.	O
this	O
test	O
is	O
intended	O
to	O
detect	O
the	O
case	O
where	O
the	O
model	B
overﬁts	O
the	O
training	O
set	O
and	O
just	O
reproduces	O
training	O
instances	O
.	O
it	O
is	O
even	O
possible	O
to	O
simultaneously	O
underﬁt	O
and	O
overﬁt	O
yet	O
still	O
produce	O
samples	O
that	O
individually	O
look	O
good	O
.	O
imagine	O
a	O
generative	O
model	B
trained	O
on	O
images	O
of	O
dogs	O
and	O
cats	O
that	O
simply	O
learns	O
to	O
reproduce	O
the	O
training	O
images	O
of	O
dogs	O
.	O
such	O
a	O
model	B
has	O
clearly	O
overﬁt	O
,	O
because	O
it	O
does	O
not	O
produces	O
images	O
that	O
were	O
not	O
in	O
the	O
training	O
set	O
,	O
but	O
it	O
has	O
also	O
underﬁt	O
,	O
because	O
it	O
assigns	O
no	O
probability	O
to	O
the	O
training	O
images	O
of	O
cats	O
.	O
yet	O
a	O
human	O
observer	O
would	O
judge	O
each	O
individual	O
image	O
of	O
a	O
dog	O
to	O
be	O
high	O
quality	O
.	O
in	O
this	O
simple	O
example	O
,	O
it	O
would	O
be	O
easy	O
for	O
a	O
human	O
observer	O
who	O
can	O
inspect	O
many	O
samples	O
to	O
determine	O
that	O
the	O
cats	O
are	O
absent	O
.	O
in	O
more	O
realistic	O
settings	O
,	O
a	O
generative	O
model	B
trained	O
on	O
data	O
with	O
tens	O
of	O
thousands	O
of	O
modes	O
may	O
ignore	O
a	O
small	O
number	O
of	O
modes	O
,	O
and	O
a	O
human	O
observer	O
would	O
not	O
easily	O
be	O
able	O
to	O
inspect	O
or	O
remember	O
enough	O
images	O
to	O
detect	O
the	O
missing	O
variation	O
.	O
since	O
the	O
visual	O
quality	O
of	O
samples	O
is	O
not	O
a	O
reliable	O
guide	O
,	O
we	O
often	O
also	O
evaluate	O
the	O
log-likelihood	O
that	O
the	O
model	B
assigns	O
to	O
the	O
test	O
data	O
,	O
when	O
this	O
is	O
computationally	O
feasible	O
.	O
unfortunately	O
,	O
in	O
some	O
cases	O
the	O
likelihood	O
seems	O
not	O
to	O
measure	O
any	O
attribute	O
of	O
the	O
model	B
that	O
we	O
really	O
care	O
about	O
.	O
for	O
example	O
,	O
real-valued	O
models	O
of	O
mnist	O
can	O
obtain	O
arbitrarily	O
high	O
likelihood	O
by	O
assigning	O
arbitrarily	O
low	O
variance	O
to	O
background	O
pixels	O
that	O
never	O
change	O
.	O
models	O
and	O
algorithms	O
that	O
detect	O
these	O
constant	O
features	O
can	O
reap	O
unlimited	O
rewards	O
,	O
even	O
though	O
this	O
is	O
not	O
a	O
very	O
useful	O
thing	O
to	O
do	O
.	O
the	O
potential	O
to	O
achieve	O
a	O
cost	O
approaching	O
negative	O
inﬁnity	O
is	O
present	O
for	O
any	O
kind	O
of	O
maximum	O
likelihood	O
problem	O
with	O
real	O
values	O
,	O
but	O
it	O
is	O
especially	O
problematic	O
for	O
generative	O
models	O
of	O
mnist	O
because	O
so	O
many	O
of	O
the	O
output	O
values	O
are	O
trivial	O
to	O
predict	O
.	O
this	O
strongly	O
suggests	O
a	O
need	O
for	O
developing	O
other	O
ways	O
of	O
evaluating	O
generative	O
models	O
.	O
theis	O
et	O
al	O
.	O
(	O
2015	O
)	O
review	O
many	O
of	O
the	O
issues	O
involved	O
in	O
evaluating	O
generative	O
719	O
chapter	O
20.	O
deep	O
generative	O
models	O
models	O
,	O
including	O
many	O
of	O
the	O
ideas	O
described	O
above	O
.	O
they	O
highlight	O
the	O
fact	O
that	O
there	O
are	O
many	O
diﬀerent	O
uses	O
of	O
generative	O
models	O
and	O
that	O
the	O
choice	O
of	O
metric	O
must	O
match	O
the	O
intended	O
use	O
of	O
the	O
model	B
.	O
for	O
example	O
,	O
some	O
generative	O
models	O
are	O
better	O
at	O
assigning	O
high	O
probability	O
to	O
most	O
realistic	O
points	O
while	O
other	O
generative	O
models	O
are	O
better	O
at	O
rarely	O
assigning	O
high	O
probability	O
to	O
unrealistic	O
points	O
.	O
these	O
diﬀerences	O
can	O
result	O
from	O
whether	O
a	O
generative	O
model	B
is	O
designed	O
to	O
minimize	O
d	O
kl	O
(	O
pdata	O
.3.6	O
unfortunately	O
,	O
even	O
when	O
we	O
restrict	O
the	O
use	O
of	O
each	O
metric	O
to	O
the	O
task	O
it	O
is	O
most	O
suited	O
for	O
,	O
all	O
of	O
the	O
metrics	O
currently	O
in	O
use	O
continue	O
to	O
have	O
serious	O
weaknesses	O
.	O
one	O
of	O
the	O
most	O
important	O
research	O
topics	O
in	O
generative	O
modeling	O
is	O
therefore	O
not	O
just	O
how	O
to	O
improve	O
generative	O
models	O
,	O
but	O
in	O
fact	O
,	O
designing	O
new	O
techniques	O
to	O
measure	O
our	O
progress	O
.	O
	O
pdata	O
)	O
,	O
as	O
illustrated	O
in	O
ﬁgure	O
	O
pmodel	O
)	O
or	O
d	O
kl	O
(	O
pmodel	O
20.15	O
conclusion	O
|	O
training	O
generative	O
models	O
with	O
hidden	O
units	O
is	O
a	O
powerful	O
way	O
to	O
make	O
models	O
understand	O
the	O
world	O
represented	O
in	O
the	O
given	O
training	O
data	O
.	O
by	O
learning	O
a	O
model	B
pmodel	O
(	O
x	O
)	O
and	O
a	O
representation	O
pmodel	O
(	O
h	O
x	O
)	O
,	O
a	O
generative	O
model	B
can	O
provide	O
answers	O
to	O
many	O
inference	O
problems	O
about	O
the	O
relationships	O
between	O
input	O
variables	O
in	O
x	O
and	O
can	O
provide	O
many	O
diﬀerent	O
ways	O
of	O
representing	O
x	O
by	O
taking	O
expectations	O
of	O
h	O
at	O
diﬀerent	O
layers	O
of	O
the	O
hierarchy	O
.	O
generative	O
models	O
hold	O
the	O
promise	O
to	O
provide	O
ai	O
systems	O
with	O
a	O
framework	O
for	O
all	O
of	O
the	O
many	O
diﬀerent	O
intuitive	O
concepts	O
they	O
need	O
to	O
understand	O
,	O
and	O
the	O
ability	O
to	O
reason	O
about	O
these	O
concepts	O
in	O
the	O
face	O
of	O
uncertainty	O
.	O
we	O
hope	O
that	O
our	O
readers	O
will	O
ﬁnd	O
new	O
ways	O
to	O
make	O
these	O
approaches	O
more	O
powerful	O
and	O
continue	O
the	O
journey	O
to	O
understanding	O
the	O
principles	O
that	O
underlie	O
learning	O
and	O
intelligence	O
.	O
720	O
bibliography	O
abadi	O
,	O
m.	O
,	O
agarwal	O
,	O
a.	O
,	O
barham	O
,	O
p.	O
,	O
brevdo	O
,	O
e.	O
,	O
chen	O
,	O
z.	O
,	O
citro	O
,	O
c.	O
,	O
corrado	O
,	O
g.	O
s.	O
,	O
davis	O
,	O
a.	O
,	O
dean	O
,	O
j.	O
,	O
devin	O
,	O
m.	O
,	O
ghemawat	O
,	O
s.	O
,	O
goodfellow	O
,	O
i.	O
,	O
harp	O
,	O
a.	O
,	O
irving	O
,	O
g.	O
,	O
isard	O
,	O
m.	O
,	O
jia	O
,	O
y.	O
,	O
jozefowicz	O
,	O
r.	O
,	O
kaiser	O
,	O
l.	O
,	O
kudlur	O
,	O
m.	O
,	O
levenberg	O
,	O
j.	O
,	O
mané	O
,	O
d.	O
,	O
monga	O
,	O
r.	O
,	O
moore	O
,	O
s.	O
,	O
murray	O
,	O
d.	O
,	O
olah	O
,	O
c.	O
,	O
schuster	O
,	O
m.	O
,	O
shlens	O
,	O
j.	O
,	O
steiner	O
,	O
b.	O
,	O
sutskever	O
,	O
i.	O
,	O
talwar	O
,	O
k.	O
,	O
tucker	O
,	O
p.	O
,	O
vanhoucke	O
,	O
v.	O
,	O
vasudevan	O
,	O
v.	O
,	O
viégas	O
,	O
f.	O
,	O
vinyals	O
,	O
o.	O
,	O
warden	O
,	O
p.	O
,	O
wattenberg	O
,	O
m.	O
,	O
wicke	O
,	O
m.	O
,	O
yu	O
,	O
y.	O
,	O
and	O
zheng	O
,	O
x	O
.	O
(	O
2015	O
)	O
.	O
tensorflow	O
:	O
large-scale	O
machine	O
learning	O
on	O
heterogeneous	O
systems	O
.	O
software	O
available	O
from	O
tensorﬂow.org	O
.	O
,25	O
214	O
446	O
,	O
ackley	O
,	O
d.	O
h.	O
,	O
hinton	O
,	O
g.	O
e.	O
,	O
and	O
sejnowski	O
,	O
t.	O
j	O
.	O
(	O
1985	O
)	O
.	O
a	O
learning	O
algorithm	O
for	O
boltzmann	O
machines	O
.	O
cognitive	O
science	O
,	O
9	O
,	O
147–169	O
.	O
570	O
654	O
,	O
alain	O
,	O
g.	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2013	O
)	O
.	O
what	O
regularized	O
auto-encoders	O
learn	O
from	O
the	O
data	O
generating	O
distribution	O
.	O
in	O
iclr	O
’	O
2013	O
,	O
arxiv:1211.4246	O
507	O
513	O
514	O
521	O
.	O
,	O
,	O
,	O
alain	O
,	O
g.	O
,	O
bengio	O
,	O
y.	O
,	O
yao	O
,	O
l.	O
,	O
éric	O
thibodeau-laufer	O
,	O
yosinski	O
,	O
j.	O
,	O
and	O
vincent	O
,	O
p.	O
(	O
2015	O
)	O
.	O
gsns	O
:	O
generative	O
stochastic	O
networks	O
.	O
arxiv:1503.05571	O
.	O
510	O
713	O
,	O
anderson	O
,	O
e.	O
(	O
1935	O
)	O
.	O
the	O
irises	O
of	O
the	O
gaspé	O
peninsula	O
.	O
bulletin	O
of	O
the	O
american	O
iris	O
society	O
,	O
59	O
,	O
2–5	O
.	O
21	O
ba	O
,	O
j.	O
,	O
mnih	O
,	O
v.	O
,	O
and	O
kavukcuoglu	O
,	O
k.	O
(	O
2014	O
)	O
.	O
multiple	O
object	O
recognition	B
with	O
visual	O
attention	O
.	O
arxiv:1412.7755	O
691	O
.	O
bachman	O
,	O
p.	O
and	O
precup	O
,	O
d.	O
(	O
2015	O
)	O
.	O
variational	O
generative	O
stochastic	O
networks	O
with	O
collaborative	O
shaping	O
.	O
in	O
proceedings	O
of	O
the	O
32nd	O
international	O
conference	O
on	O
machine	O
learning	O
,	O
icml	O
2015	O
,	O
lille	O
,	O
france	O
,	O
6-11	O
july	O
2015	O
,	O
pages	O
1964–1972	O
.	O
717	O
bacon	O
,	O
p.-l.	O
,	O
bengio	O
,	O
e.	O
,	O
pineau	O
,	O
j.	O
,	O
and	O
precup	O
,	O
d.	O
(	O
2015	O
)	O
.	O
conditional	O
computation	O
in	O
neural	O
networks	O
using	O
a	O
decision-theoretic	O
approach	O
.	O
in	O
2nd	O
multidisciplinary	O
conference	O
on	O
reinforcement	O
learning	O
and	O
decision	O
making	O
(	O
rldm	O
2015	O
)	O
.	O
450	O
bagnell	O
,	O
j.	O
a.	O
and	O
bradley	O
,	O
d.	O
m.	O
(	O
2009	O
)	O
.	O
diﬀerentiable	O
sparse	O
coding	O
.	O
in	O
d.	O
koller	O
,	O
d.	O
schuurmans	O
,	O
y.	O
bengio	O
,	O
and	O
l.	O
bottou	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
21	O
(	O
nips	O
’	O
08	O
)	O
,	O
pages	O
113–120	O
.	O
498	O
721	O
bibliography	O
bahdanau	O
,	O
d.	O
,	O
cho	O
,	O
k.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2015	O
)	O
.	O
neural	O
machine	O
translation	O
by	O
jointly	O
iclr	O
’	O
2015	O
,	O
arxiv:1409.0473	O
25	O
101	O
397	O
418	O
420	O
,	O
.	O
,	O
learning	O
to	O
align	O
and	O
translate	O
.	O
in	O
465	O
475	O
476	O
,	O
,	O
,	O
,	O
,	O
bahl	O
,	O
l.	O
r.	O
,	O
brown	O
,	O
p.	O
,	O
de	O
souza	O
,	O
p.	O
v.	O
,	O
and	O
mercer	O
,	O
r.	O
l.	O
(	O
1987	O
)	O
.	O
speech	O
recognition	B
with	O
continuous-parameter	O
hidden	O
markov	O
models	O
.	O
computer	O
,	O
speech	O
and	O
language	O
,	O
2	O
,	O
219–234	O
.	O
458	O
baldi	O
,	O
p.	O
and	O
hornik	O
,	O
k.	O
(	O
1989	O
)	O
.	O
neural	O
networks	O
and	O
principal	O
component	O
analysis	O
:	O
learning	O
from	O
examples	O
without	O
local	O
minima	O
.	O
neural	O
networks	O
,	O
2	O
,	O
53–58	O
.	O
286	O
baldi	O
,	O
p.	O
,	O
brunak	O
,	O
s.	O
,	O
frasconi	O
,	O
p.	O
,	O
soda	O
,	O
g.	O
,	O
and	O
pollastri	O
,	O
g.	O
(	O
1999	O
)	O
.	O
exploiting	O
the	O
bioinformatics	O
15	O
(	O
11	O
)	O
,	O
past	O
and	O
the	O
future	O
in	O
protein	O
secondary	O
structure	O
prediction	O
.	O
937–946	O
.	O
395	O
,	O
baldi	O
,	O
p.	O
,	O
sadowski	O
,	O
p.	O
,	O
and	O
whiteson	O
,	O
d.	O
(	O
2014	O
)	O
.	O
searching	O
for	O
exotic	O
particles	O
in	O
high-energy	O
physics	O
with	O
deep	O
learning	O
.	O
nature	O
communications	O
,	O
.5	O
26	O
ballard	O
,	O
d.	O
h.	O
,	O
hinton	O
,	O
g.	O
e.	O
,	O
and	O
sejnowski	O
,	O
t.	O
j	O
.	O
(	O
1983	O
)	O
.	O
parallel	O
vision	O
computation	O
.	O
nature	O
.	O
452	O
barlow	O
,	O
h.	O
b	O
.	O
(	O
1989	O
)	O
.	O
unsupervised	O
learning	O
.	O
neural	O
computation	O
,	O
1	O
,	O
295–311	O
.	O
147	O
barron	O
,	O
a.	O
e.	O
(	O
1993	O
)	O
.	O
universal	O
approximation	O
bounds	O
for	O
superpositions	O
of	O
a	O
sigmoidal	O
function	O
.	O
ieee	O
trans	O
.	O
on	O
information	O
theory	O
,	O
39	O
,	O
930–945	O
.	O
199	O
bartholomew	O
,	O
d.	O
j	O
.	O
(	O
1987	O
)	O
.	O
latent	O
variable	O
models	O
and	O
factor	O
analysis	O
.	O
oxford	O
university	O
press	O
.	O
490	O
basilevsky	O
,	O
a	O
.	O
(	O
1994	O
)	O
.	O
statistical	O
factor	O
analysis	O
and	O
related	O
methods	O
:	O
theory	O
and	O
applications	O
.	O
wiley	O
.	O
490	O
bastien	O
,	O
f.	O
,	O
lamblin	O
,	O
p.	O
,	O
pascanu	O
,	O
r.	O
,	O
bergstra	O
,	O
j.	O
,	O
goodfellow	O
,	O
i.	O
j.	O
,	O
bergeron	O
,	O
a.	O
,	O
bouchard	O
,	O
n.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2012	O
)	O
.	O
theano	O
:	O
new	O
features	O
and	O
speed	O
improvements	O
.	O
deep	O
learning	O
and	O
unsupervised	O
feature	O
learning	O
nips	O
2012	O
workshop	O
.	O
25	O
82	O
214	O
,	O
222	O
446	O
,	O
,	O
,	O
basu	O
,	O
s.	O
and	O
christensen	O
,	O
j	O
.	O
(	O
2013	O
)	O
.	O
teaching	O
classiﬁcation	O
boundaries	O
to	O
humans	O
.	O
in	O
aaai	O
’	O
2013	O
.	O
329	O
baxter	O
,	O
j	O
.	O
(	O
1995	O
)	O
.	O
learning	O
internal	O
representations	O
.	O
in	O
proceedings	O
of	O
the	O
8th	O
international	O
conference	O
on	O
computational	O
learning	O
theory	O
(	O
colt	O
’	O
95	O
)	O
,	O
pages	O
311–320	O
,	O
santa	O
cruz	O
,	O
california	O
.	O
acm	O
press	O
.	O
245	O
bayer	O
,	O
j.	O
and	O
osendorfer	O
,	O
c.	O
(	O
2014	O
)	O
.	O
learning	O
stochastic	O
recurrent	O
networks	O
.	O
arxiv	O
e-prints	O
.	O
265	O
becker	O
,	O
s.	O
and	O
hinton	O
,	O
g.	O
(	O
1992	O
)	O
.	O
a	O
self-organizing	O
neural	O
network	O
that	O
discovers	O
surfaces	O
in	O
random-dot	O
stereograms	O
.	O
nature	O
,	O
355	O
,	O
161–163	O
.	O
541	O
722	O
bibliography	O
behnke	O
,	O
s.	O
(	O
2001	O
)	O
.	O
learning	O
iterative	O
image	O
reconstruction	O
in	O
the	O
neural	O
abstraction	O
pyramid	O
.	O
int	O
.	O
j.	O
computational	O
intelligence	O
and	O
applications	O
,	O
1	O
(	O
4	O
)	O
,	O
427–438	O
.	O
515	O
beiu	O
,	O
v.	O
,	O
quintana	O
,	O
j.	O
m.	O
,	O
and	O
avedillo	O
,	O
m.	O
j	O
.	O
(	O
2003	O
)	O
.	O
vlsi	O
implementations	O
of	O
threshold	O
logic-a	O
comprehensive	O
survey	O
.	O
neural	O
networks	O
,	O
ieee	O
transactions	O
on	O
,	O
14	O
(	O
5	O
)	O
,	O
1217–	O
1243	O
.	O
451	O
belkin	O
,	O
m.	O
and	O
niyogi	O
,	O
p.	O
(	O
2002	O
)	O
.	O
laplacian	O
eigenmaps	O
and	O
spectral	O
techniques	O
for	O
embedding	O
and	O
clustering	O
.	O
in	O
t.	O
dietterich	O
,	O
s.	O
becker	O
,	O
and	O
z.	O
ghahramani	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
14	O
(	O
nips	O
’	O
01	O
)	O
,	O
cambridge	O
,	O
ma	O
.	O
mit	O
press	O
.	O
244	O
belkin	O
,	O
m.	O
and	O
niyogi	O
,	O
p.	O
(	O
2003	O
)	O
.	O
laplacian	O
eigenmaps	O
for	O
dimensionality	O
reduction	O
and	O
data	O
representation	O
.	O
neural	O
computation	O
,	O
15	O
(	O
6	O
)	O
,	O
1373–1396	O
.	O
164	O
518	O
,	O
bengio	O
,	O
e.	O
,	O
bacon	O
,	O
p.-l.	O
,	O
pineau	O
,	O
j.	O
,	O
and	O
precup	O
,	O
d.	O
(	O
2015a	O
)	O
.	O
conditional	O
computation	O
in	O
neural	O
networks	O
for	O
faster	O
models	O
.	O
arxiv:1511.06297	O
.	O
450	O
bengio	O
,	O
s.	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2000a	O
)	O
.	O
taking	O
on	O
the	O
curse	O
of	O
dimensionality	O
in	O
joint	O
distributions	O
using	O
neural	O
networks	O
.	O
ieee	O
transactions	O
on	O
neural	O
networks	O
,	O
special	O
issue	O
on	O
data	O
mining	O
and	O
knowledge	O
discovery	O
,	O
(	O
3	O
)	O
,	O
550–557	O
.	O
707	O
11	O
bengio	O
,	O
s.	O
,	O
vinyals	O
,	O
o.	O
,	O
jaitly	O
,	O
n.	O
,	O
and	O
shazeer	O
,	O
n.	O
(	O
2015b	O
)	O
.	O
scheduled	O
sampling	O
for	O
sequence	O
prediction	O
with	O
recurrent	O
neural	O
networks	O
.	O
technical	O
report	O
,	O
arxiv:1506.03099	O
.	O
384	O
bengio	O
,	O
y	O
.	O
(	O
1991	O
)	O
.	O
artiﬁcial	O
neural	O
networks	O
and	O
their	O
application	O
to	O
sequence	O
recognition	B
.	O
ph.d.	O
thesis	O
,	O
mcgill	O
university	O
,	O
(	O
computer	O
science	O
)	O
,	O
montreal	O
,	O
canada	O
.	O
407	O
bengio	O
,	O
y	O
.	O
(	O
2000	O
)	O
.	O
gradient-based	O
optimization	O
of	O
hyperparameters	O
.	O
neural	O
computation	O
,	O
12	O
(	O
8	O
)	O
,	O
1889–1900	O
.	O
435	O
bengio	O
,	O
y	O
.	O
(	O
2002	O
)	O
.	O
new	O
distributed	O
probabilistic	O
language	O
models	O
.	O
technical	O
report	O
1215	O
,	O
dept	O
.	O
iro	O
,	O
université	O
de	O
montréal	O
.	O
467	O
bengio	O
,	O
y	O
.	O
(	O
2009	O
)	O
.	O
learning	O
deep	O
architectures	O
for	O
ai	O
.	O
now	O
publishers	O
.	O
201	O
622	O
,	O
bengio	O
,	O
y	O
.	O
(	O
2013	O
)	O
.	O
deep	O
learning	O
of	O
representations	O
:	O
looking	O
forward	O
.	O
in	O
statistical	O
language	O
and	O
speech	O
processing	O
,	O
volume	O
7978	O
of	O
lecture	O
notes	O
in	O
computer	O
science	O
,	O
pages	O
1–37	O
.	O
springer	O
,	O
also	O
in	O
arxiv	O
at	O
http	O
:	O
//arxiv.org/abs/1305.0445	O
.	O
448	O
bengio	O
,	O
y	O
.	O
(	O
2015	O
)	O
.	O
early	O
inference	O
in	O
energy-based	O
models	O
approximates	O
back-propagation	O
.	O
technical	O
report	O
arxiv:1510.02777	O
,	O
universite	O
de	O
montreal	O
.	O
656	O
bengio	O
,	O
y.	O
and	O
bengio	O
,	O
s.	O
(	O
2000b	O
)	O
.	O
modeling	O
high-dimensional	O
discrete	O
data	O
with	O
multi-	O
layer	O
neural	O
networks	O
.	O
in	O
nips	O
12	O
,	O
pages	O
400–406	O
.	O
mit	O
press	O
.	O
705	O
707	O
708	O
710	O
,	O
,	O
,	O
bengio	O
,	O
y.	O
and	O
delalleau	O
,	O
o	O
.	O
(	O
2009	O
)	O
.	O
justifying	O
and	O
generalizing	O
contrastive	O
divergence	O
.	O
neural	O
computation	O
,	O
21	O
(	O
6	O
)	O
,	O
1601–1621	O
.	O
513	O
611	O
,	O
723	O
bibliography	O
bengio	O
,	O
y.	O
and	O
grandvalet	O
,	O
y	O
.	O
(	O
2004	O
)	O
.	O
no	O
unbiased	O
estimator	O
of	O
the	O
variance	O
of	O
k-fold	O
cross-validation	O
.	O
in	O
s.	O
thrun	O
,	O
l.	O
saul	O
,	O
and	O
b.	O
schölkopf	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
16	O
(	O
nips	O
’	O
03	O
)	O
,	O
cambridge	O
,	O
ma	O
.	O
mit	O
press	O
,	O
cambridge	O
.	O
122	O
bengio	O
,	O
y.	O
and	O
lecun	O
,	O
y	O
.	O
(	O
2007	O
)	O
.	O
scaling	O
learning	O
algorithms	O
towards	O
ai	O
.	O
in	O
large	O
scale	O
kernel	O
machines	O
.	O
19	O
bengio	O
,	O
y.	O
and	O
monperrus	O
,	O
m.	O
(	O
2005	O
)	O
.	O
non-local	O
manifold	O
tangent	O
learning	O
.	O
in	O
l.	O
saul	O
,	O
y.	O
weiss	O
,	O
and	O
l.	O
bottou	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
17	O
(	O
nips	O
’	O
04	O
)	O
,	O
pages	O
129–136	O
.	O
mit	O
press	O
.	O
160	O
519	O
,	O
bengio	O
,	O
y.	O
and	O
sénécal	O
,	O
j.-s.	O
(	O
2003	O
)	O
.	O
quick	O
training	O
of	O
probabilistic	O
neural	O
nets	O
by	O
importance	O
sampling	O
.	O
in	O
proceedings	O
of	O
aistats	O
2003	O
.	O
470	O
bengio	O
,	O
y.	O
and	O
sénécal	O
,	O
j.-s.	O
(	O
2008	O
)	O
.	O
adaptive	O
importance	O
sampling	O
to	O
accelerate	O
training	O
of	O
a	O
neural	O
probabilistic	O
language	O
model	B
.	O
ieee	O
trans	O
.	O
neural	O
networks	O
,	O
19	O
(	O
4	O
)	O
,	O
713–722	O
.	O
470	O
bengio	O
,	O
y.	O
,	O
de	O
mori	O
,	O
r.	O
,	O
flammia	O
,	O
g.	O
,	O
and	O
kompe	O
,	O
r.	O
(	O
1991	O
)	O
.	O
phonetically	O
motivated	O
acoustic	O
parameters	O
for	O
continuous	O
speech	O
recognition	B
using	O
artiﬁcial	O
neural	O
networks	O
.	O
in	O
proceedings	O
of	O
eurospeech	O
’	O
91	O
.	O
,27	O
459	O
bengio	O
,	O
y.	O
,	O
de	O
mori	O
,	O
r.	O
,	O
flammia	O
,	O
g.	O
,	O
and	O
kompe	O
,	O
r.	O
(	O
1992	O
)	O
.	O
neural	O
network-gaussian	O
,	O
pages	O
175–182	O
.	O
mixture	O
hybrid	O
for	O
speech	O
recognition	B
or	O
density	O
estimation	O
.	O
in	O
morgan	O
kaufmann	O
.	O
459	O
nips	O
4	O
bengio	O
,	O
y.	O
,	O
frasconi	O
,	O
p.	O
,	O
and	O
simard	O
,	O
p.	O
(	O
1993	O
)	O
.	O
the	O
problem	O
of	O
learning	O
long-term	O
in	O
ieee	O
international	O
conference	O
on	O
neural	O
dependencies	O
in	O
recurrent	O
networks	O
.	O
networks	O
,	O
pages	O
1183–1195	O
,	O
san	O
francisco	O
.	O
ieee	O
press	O
.	O
(	O
invited	O
paper	O
)	O
.	O
403	O
bengio	O
,	O
y.	O
,	O
simard	O
,	O
p.	O
,	O
and	O
frasconi	O
,	O
p.	O
(	O
1994	O
)	O
.	O
learning	O
long-term	O
dependencies	O
with	O
gradient	O
descent	B
is	O
diﬃcult	O
.	O
ieee	O
tr	O
.	O
neural	O
nets	O
.	O
18	O
401	O
403	O
411	O
,	O
,	O
,	O
bengio	O
,	O
y.	O
,	O
latendresse	O
,	O
s.	O
,	O
and	O
dugas	O
,	O
c.	O
(	O
1999	O
)	O
.	O
gradient-based	O
learning	O
of	O
hyper-	O
parameters	O
.	O
learning	O
conference	O
,	O
snowbird	O
.	O
435	O
bengio	O
,	O
y.	O
,	O
ducharme	O
,	O
r.	O
,	O
and	O
vincent	O
,	O
p.	O
(	O
2001	O
)	O
.	O
a	O
neural	O
probabilistic	O
language	O
model	B
.	O
,	O
pages	O
932–938	O
.	O
mit	O
nips	O
’	O
2000	O
in	O
t.	O
k.	O
leen	O
,	O
t.	O
g.	O
dietterich	O
,	O
and	O
v.	O
tresp	O
,	O
editors	O
,	O
press	O
.	O
18	O
447	O
464	O
466	O
472	O
477	O
482	O
,	O
,	O
,	O
,	O
,	O
,	O
bengio	O
,	O
y.	O
,	O
ducharme	O
,	O
r.	O
,	O
vincent	O
,	O
p.	O
,	O
and	O
jauvin	O
,	O
c.	O
(	O
2003	O
)	O
.	O
a	O
neural	O
probabilistic	O
language	O
model	B
.	O
jmlr	O
3	O
,	O
,	O
1137–1155	O
.	O
466	O
472	O
,	O
bengio	O
,	O
y.	O
,	O
le	O
roux	O
,	O
n.	O
,	O
vincent	O
,	O
p.	O
,	O
delalleau	O
,	O
o.	O
,	O
and	O
marcotte	O
,	O
p.	O
(	O
2006a	O
)	O
.	O
convex	O
neural	O
networks	O
.	O
in	O
nips	O
’	O
2005	O
,	O
pages	O
123–130	O
.	O
258	O
bengio	O
,	O
y.	O
,	O
delalleau	O
,	O
o.	O
,	O
and	O
le	O
roux	O
,	O
n.	O
(	O
2006b	O
)	O
.	O
the	O
curse	O
of	O
highly	O
variable	O
functions	O
for	O
local	O
kernel	O
machines	O
.	O
in	O
nips	O
’	O
2005	O
158	O
.	O
724	O
bibliography	O
bengio	O
,	O
y.	O
,	O
larochelle	O
,	O
h.	O
,	O
and	O
vincent	O
,	O
p.	O
(	O
2006c	O
)	O
.	O
non-local	O
manifold	O
parzen	O
windows	O
.	O
in	O
nips	O
’	O
2005	O
.	O
mit	O
press	O
.	O
160	O
520	O
,	O
bengio	O
,	O
y.	O
,	O
lamblin	O
,	O
p.	O
,	O
popovici	O
,	O
d.	O
,	O
and	O
larochelle	O
,	O
h.	O
(	O
2007	O
)	O
.	O
greedy	O
layer-wise	O
training	O
of	O
deep	O
networks	O
.	O
in	O
nips	O
’	O
2006	O
14	O
19	O
201	O
323	O
324	O
528	O
530	O
.	O
,	O
,	O
,	O
,	O
,	O
,	O
bengio	O
,	O
y.	O
,	O
louradour	O
,	O
j.	O
,	O
collobert	O
,	O
r.	O
,	O
and	O
weston	O
,	O
j	O
.	O
(	O
2009	O
)	O
.	O
curriculum	O
learning	O
.	O
in	O
icml	O
’	O
09	O
.	O
328	O
bengio	O
,	O
y.	O
,	O
mesnil	O
,	O
g.	O
,	O
dauphin	O
,	O
y.	O
,	O
and	O
rifai	O
,	O
s.	O
(	O
2013a	O
)	O
.	O
better	O
mixing	O
via	O
deep	O
representations	O
.	O
in	O
icml	O
’	O
2013	O
604	O
.	O
bengio	O
,	O
y.	O
,	O
léonard	O
,	O
n.	O
,	O
and	O
courville	O
,	O
a	O
.	O
(	O
2013b	O
)	O
.	O
estimating	O
or	O
propagating	O
gradients	O
448	O
450	O
,	O
through	O
stochastic	O
neurons	O
for	O
conditional	O
computation	O
.	O
arxiv:1308.3432	O
.	O
689	O
691	O
,	O
,	O
bengio	O
,	O
y.	O
,	O
yao	O
,	O
l.	O
,	O
alain	O
,	O
g.	O
,	O
and	O
vincent	O
,	O
p.	O
(	O
2013c	O
)	O
.	O
generalized	O
denoising	O
auto-	O
encoders	O
as	O
generative	O
models	O
.	O
in	O
nips	O
’	O
2013	O
507	O
711	O
714	O
.	O
,	O
,	O
bengio	O
,	O
y.	O
,	O
courville	O
,	O
a.	O
,	O
and	O
vincent	O
,	O
p.	O
(	O
2013d	O
)	O
.	O
representation	O
learning	O
:	O
a	O
review	O
and	O
new	O
perspectives	O
.	O
ieee	O
trans	O
.	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
(	O
pami	O
)	O
,	O
35	O
(	O
8	O
)	O
,	O
1798–1828	O
.	O
555	O
bengio	O
,	O
y.	O
,	O
thibodeau-laufer	O
,	O
e.	O
,	O
alain	O
,	O
g.	O
,	O
and	O
yosinski	O
,	O
j	O
.	O
(	O
2014	O
)	O
.	O
deep	O
generative	O
stochastic	O
networks	O
trainable	O
by	O
backprop	O
.	O
in	O
icml	O
’	O
2014	O
711	O
712	O
713	O
714	O
715	O
.	O
,	O
,	O
,	O
,	O
bennett	O
,	O
c.	O
(	O
1976	O
)	O
.	O
eﬃcient	O
estimation	O
of	O
free	O
energy	O
diﬀerences	O
from	O
monte	O
carlo	O
data	O
.	O
journal	O
of	O
computational	O
physics	O
,	O
22	O
(	O
2	O
)	O
,	O
245–268	O
.	O
628	O
bennett	O
,	O
j.	O
and	O
lanning	O
,	O
s.	O
(	O
2007	O
)	O
.	O
the	O
netﬂix	O
prize	O
.	O
479	O
berger	O
,	O
a.	O
l.	O
,	O
della	O
pietra	O
,	O
v.	O
j.	O
,	O
and	O
della	O
pietra	O
,	O
s.	O
a	O
.	O
(	O
1996	O
)	O
.	O
a	O
maximum	O
entropy	O
473	O
approach	O
to	O
natural	O
language	O
processing	O
.	O
computational	O
linguistics	O
22	O
,	O
39–71	O
.	O
,	O
berglund	O
,	O
m.	O
and	O
raiko	O
,	O
t.	O
(	O
2013	O
)	O
.	O
stochastic	O
gradient	O
estimate	O
variance	O
in	O
contrastive	O
divergence	O
and	O
persistent	O
contrastive	O
divergence	O
.	O
corr	O
abs/1312.6002	O
614	O
,	O
.	O
bergstra	O
,	O
j	O
.	O
(	O
2011	O
)	O
.	O
incorporating	O
complex	O
cells	O
into	O
neural	O
networks	O
for	O
pattern	O
classiﬁcation	O
.	O
ph.d.	O
thesis	O
,	O
université	O
de	O
montréal	O
.	O
255	O
bergstra	O
,	O
j.	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2009	O
)	O
.	O
slow	O
,	O
decorrelated	O
features	O
for	O
pretraining	O
complex	O
cell-like	O
networks	O
.	O
in	O
nips	O
’	O
2009	O
494	O
.	O
bergstra	O
,	O
j.	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2012	O
)	O
.	O
random	O
search	O
for	O
hyper-parameter	O
optimization	O
.	O
j.	O
machine	O
learning	O
res.	O
,	O
13	O
,	O
281–305	O
.	O
433	O
434	O
435	O
,	O
,	O
bergstra	O
,	O
j.	O
,	O
breuleux	O
,	O
o.	O
,	O
bastien	O
,	O
f.	O
,	O
lamblin	O
,	O
p.	O
,	O
pascanu	O
,	O
r.	O
,	O
desjardins	O
,	O
g.	O
,	O
turian	O
,	O
j.	O
,	O
warde-farley	O
,	O
d.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2010	O
)	O
.	O
theano	O
:	O
a	O
cpu	O
and	O
gpu	O
math	O
expression	O
compiler	O
.	O
in	O
proc	O
.	O
scipy	O
.	O
25	O
82	O
214	O
222	O
446	O
,	O
,	O
,	O
,	O
725	O
bibliography	O
bergstra	O
,	O
j.	O
,	O
bardenet	O
,	O
r.	O
,	O
bengio	O
,	O
y.	O
,	O
and	O
kégl	O
,	O
b	O
.	O
(	O
2011	O
)	O
.	O
algorithms	O
for	O
hyper-parameter	O
optimization	O
.	O
in	O
nips	O
’	O
2011	O
436	O
.	O
berkes	O
,	O
p.	O
and	O
wiskott	O
,	O
l.	O
(	O
2005	O
)	O
.	O
slow	O
feature	O
analysis	O
yields	O
a	O
rich	O
repertoire	O
of	O
complex	O
cell	O
properties	O
.	O
journal	O
of	O
vision	O
5	O
,	O
(	O
6	O
)	O
,	O
579–602	O
.	O
495	O
bertsekas	O
,	O
d.	O
p.	O
and	O
tsitsiklis	O
,	O
j	O
.	O
(	O
1996	O
)	O
.	O
neuro-dynamic	O
programming	O
.	O
athena	O
scientiﬁc	O
.	O
106	O
besag	O
,	O
j	O
.	O
(	O
1975	O
)	O
.	O
statistical	O
analysis	O
of	O
non-lattice	O
data	O
.	O
the	O
statistician	O
24	O
(	O
3	O
)	O
,	O
179–195	O
.	O
,	O
615	O
bishop	O
,	O
c.	O
m.	O
(	O
1994	O
)	O
.	O
mixture	O
density	O
networks	O
.	O
189	O
bishop	O
,	O
c.	O
m.	O
(	O
1995a	O
)	O
.	O
regularization	O
and	O
complexity	O
control	O
in	O
feed-forward	O
networks	O
.	O
in	O
proceedings	O
international	O
conference	O
on	O
artiﬁcial	O
neural	O
networks	O
icann	O
’	O
95	O
,	O
volume	O
1	O
,	O
page	O
141–148	O
.	O
242	O
250	O
,	O
bishop	O
,	O
c.	O
m.	O
(	O
1995b	O
)	O
.	O
training	O
with	O
noise	O
is	O
equivalent	O
to	O
tikhonov	O
regularization	O
.	O
neural	O
computation	O
,	O
7	O
(	O
1	O
)	O
,	O
108–116	O
.	O
242	O
bishop	O
,	O
c.	O
m.	O
(	O
2006	O
)	O
.	O
pattern	O
recognition	B
and	O
machine	O
learning	O
.	O
springer	O
.	O
,98	O
146	O
blum	O
,	O
a.	O
l.	O
and	O
rivest	O
,	O
r.	O
l.	O
(	O
1992	O
)	O
.	O
training	O
a	O
3-node	O
neural	O
network	O
is	O
np-complete	O
.	O
293	O
blumer	O
,	O
a.	O
,	O
ehrenfeucht	O
,	O
a.	O
,	O
haussler	O
,	O
d.	O
,	O
and	O
warmuth	O
,	O
m.	O
k.	O
(	O
1989	O
)	O
.	O
learnability	O
and	O
the	O
vapnik–chervonenkis	O
dimension	O
.	O
journal	O
of	O
the	O
acm	O
36	O
,	O
(	O
4	O
)	O
,	O
929––865	O
.	O
114	O
bonnet	O
,	O
g.	O
(	O
1964	O
)	O
.	O
transformations	O
des	O
signaux	O
aléatoires	O
à	O
travers	O
les	O
systèmes	O
non	O
linéaires	O
sans	O
mémoire	O
.	O
annales	O
des	O
télécommunications	O
,	O
19	O
(	O
9–10	O
)	O
,	O
203–220	O
.	O
689	O
bordes	O
,	O
a.	O
,	O
weston	O
,	O
j.	O
,	O
collobert	O
,	O
r.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2011	O
)	O
.	O
learning	O
structured	O
embeddings	O
of	O
knowledge	O
bases	O
.	O
in	O
aaai	O
2011	O
484	O
.	O
bordes	O
,	O
a.	O
,	O
glorot	O
,	O
x.	O
,	O
weston	O
,	O
j.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2012	O
)	O
.	O
joint	O
learning	O
of	O
words	O
and	O
401	O
484	O
485	O
meaning	O
representations	O
for	O
open-text	O
semantic	O
parsing	O
.	O
aistats	O
’	O
2012	O
.	O
,	O
,	O
bordes	O
,	O
a.	O
,	O
glorot	O
,	O
x.	O
,	O
weston	O
,	O
j.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2013a	O
)	O
.	O
a	O
semantic	O
matching	O
energy	O
function	O
for	O
learning	O
with	O
multi-relational	O
data	O
.	O
machine	O
learning	O
:	O
special	O
issue	O
on	O
learning	O
semantics	O
.	O
483	O
bordes	O
,	O
a.	O
,	O
usunier	O
,	O
n.	O
,	O
garcia-duran	O
,	O
a.	O
,	O
weston	O
,	O
j.	O
,	O
and	O
yakhnenko	O
,	O
o	O
.	O
(	O
2013b	O
)	O
.	O
translating	O
embeddings	O
for	O
modeling	O
multi-relational	O
data	O
.	O
in	O
c.	O
burges	O
,	O
l.	O
bottou	O
,	O
m.	O
welling	O
,	O
z.	O
ghahramani	O
,	O
and	O
k.	O
weinberger	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
26	O
,	O
pages	O
2787–2795	O
.	O
curran	O
associates	O
,	O
inc.	O
484	O
bornschein	O
,	O
j.	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2015	O
)	O
.	O
reweighted	O
wake-sleep	O
.	O
in	O
iclr	O
’	O
2015	O
,	O
arxiv:1406.2751	O
.	O
693	O
726	O
bibliography	O
bornschein	O
,	O
j.	O
,	O
shabanian	O
,	O
s.	O
,	O
fischer	O
,	O
a.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2015	O
)	O
.	O
training	O
bidirectional	O
helmholtz	O
machines	O
.	O
technical	O
report	O
,	O
arxiv:1506.03877	O
.	O
693	O
boser	O
,	O
b.	O
e.	O
,	O
guyon	O
,	O
i.	O
m.	O
,	O
and	O
vapnik	O
,	O
v.	O
n.	O
(	O
1992	O
)	O
.	O
a	O
training	O
algorithm	O
for	O
opti-	O
mal	O
margin	O
classiﬁers	O
.	O
in	O
colt	O
’	O
92	O
:	O
proceedings	O
of	O
the	O
ﬁfth	O
annual	O
workshop	O
on	O
computational	O
learning	O
theory	O
,	O
pages	O
144–152	O
,	O
new	O
york	O
,	O
ny	O
,	O
usa	O
.	O
acm	O
.	O
,18	O
141	O
bottou	O
,	O
l.	O
(	O
1998	O
)	O
.	O
online	O
algorithms	O
and	O
stochastic	O
approximations	O
.	O
in	O
d.	O
saad	O
,	O
editor	O
,	O
online	O
learning	O
in	O
neural	O
networks	O
.	O
cambridge	O
university	O
press	O
,	O
cambridge	O
,	O
uk	O
.	O
296	O
bottou	O
,	O
l.	O
(	O
2011	O
)	O
.	O
from	O
machine	O
learning	O
to	O
machine	O
reasoning	O
.	O
technical	O
report	O
,	O
arxiv.1102.1808	O
.	O
401	O
bottou	O
,	O
l.	O
(	O
2015	O
)	O
.	O
multilayer	O
neural	O
networks	O
.	O
deep	O
learning	O
summer	O
school	O
.	O
440	O
bottou	O
,	O
l.	O
and	O
bousquet	O
,	O
o	O
.	O
(	O
2008	O
)	O
.	O
the	O
tradeoﬀs	O
of	O
large	O
scale	O
learning	O
.	O
in	O
nips	O
’	O
2008	O
.	O
282	O
295	O
,	O
boulanger-lewandowski	O
,	O
n.	O
,	O
bengio	O
,	O
y.	O
,	O
and	O
vincent	O
,	O
p.	O
(	O
2012	O
)	O
.	O
modeling	O
temporal	O
dependencies	O
in	O
high-dimensional	O
sequences	O
:	O
application	O
to	O
polyphonic	O
music	O
generation	O
and	O
transcription	O
.	O
in	O
icml	O
’	O
12	O
685	O
686	O
.	O
,	O
boureau	O
,	O
y.	O
,	O
ponce	O
,	O
j.	O
,	O
and	O
lecun	O
,	O
y	O
.	O
(	O
2010	O
)	O
.	O
a	O
theoretical	O
analysis	O
of	O
feature	O
pooling	O
in	O
vision	O
algorithms	O
.	O
in	O
proc	O
.	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
’	O
10	O
)	O
.	O
345	O
boureau	O
,	O
y.	O
,	O
le	O
roux	O
,	O
n.	O
,	O
bach	O
,	O
f.	O
,	O
ponce	O
,	O
j.	O
,	O
and	O
lecun	O
,	O
y	O
.	O
(	O
2011	O
)	O
.	O
ask	O
the	O
locals	O
:	O
multi-way	O
local	O
pooling	O
for	O
image	O
recognition	B
.	O
in	O
proc	O
.	O
international	O
conference	O
on	O
computer	O
vision	O
(	O
iccv	O
’	O
11	O
)	O
.	O
ieee	O
.	O
345	O
bourlard	O
,	O
h.	O
and	O
kamp	O
,	O
y	O
.	O
(	O
1988	O
)	O
.	O
auto-association	O
by	O
multilayer	O
perceptrons	O
and	O
singular	O
value	O
decomposition	O
.	O
biological	O
cybernetics	O
,	O
59	O
,	O
291–294	O
.	O
502	O
bourlard	O
,	O
h.	O
and	O
wellekens	O
,	O
c.	O
(	O
1989	O
)	O
.	O
speech	O
pattern	O
discrimination	O
and	O
multi-layered	O
perceptrons	O
.	O
computer	O
speech	O
and	O
language	O
,	O
3	O
,	O
1–19	O
.	O
459	O
boyd	O
,	O
s.	O
and	O
vandenberghe	O
,	O
l.	O
(	O
2004	O
)	O
.	O
convex	O
optimization	O
.	O
cambridge	O
university	O
press	O
,	O
new	O
york	O
,	O
ny	O
,	O
usa	O
.	O
93	O
brady	O
,	O
m.	O
l.	O
,	O
raghavan	O
,	O
r.	O
,	O
and	O
slawny	O
,	O
j	O
.	O
(	O
1989	O
)	O
.	O
back-propagation	O
fails	O
to	O
separate	O
where	O
perceptrons	O
succeed	O
.	O
ieee	O
transactions	O
on	O
circuits	O
and	O
systems	O
,	O
36	O
,	O
665–674	O
.	O
284	O
brakel	O
,	O
p.	O
,	O
stroobandt	O
,	O
d.	O
,	O
and	O
schrauwen	O
,	O
b	O
.	O
(	O
2013	O
)	O
.	O
training	O
energy-based	O
models	O
for	O
674	O
,	O
time-series	O
imputation	O
.	O
journal	O
of	O
machine	O
learning	O
research	O
,	O
14	O
,	O
2771–2797	O
.	O
698	O
brand	O
,	O
m.	O
(	O
2003	O
)	O
.	O
charting	O
a	O
manifold	O
.	O
in	O
nips	O
’	O
2002	O
,	O
pages	O
961–968	O
.	O
mit	O
press	O
.	O
164	O
,	O
518	O
727	O
bibliography	O
breiman	O
,	O
l.	O
(	O
1994	O
)	O
.	O
bagging	O
predictors	O
.	O
machine	O
learning	O
,	O
24	O
(	O
2	O
)	O
,	O
123–140	O
.	O
256	O
breiman	O
,	O
l.	O
,	O
friedman	O
,	O
j.	O
h.	O
,	O
olshen	O
,	O
r.	O
a.	O
,	O
and	O
stone	O
,	O
c.	O
j	O
.	O
(	O
1984	O
)	O
.	O
classiﬁcation	O
and	O
regression	O
trees	O
.	O
wadsworth	O
international	O
group	O
,	O
belmont	O
,	O
ca	O
.	O
146	O
bridle	O
,	O
j.	O
s.	O
(	O
1990	O
)	O
.	O
alphanets	O
:	O
a	O
recurrent	O
‘	O
neural	O
’	O
network	O
architecture	O
with	O
a	O
hidden	O
markov	O
model	B
interpretation	O
.	O
speech	O
communication	O
,	O
9	O
(	O
1	O
)	O
,	O
83–92	O
.	O
186	O
briggman	O
,	O
k.	O
,	O
denk	O
,	O
w.	O
,	O
seung	O
,	O
s.	O
,	O
helmstaedter	O
,	O
m.	O
n.	O
,	O
and	O
turaga	O
,	O
s.	O
c.	O
(	O
2009	O
)	O
.	O
360	O
maximin	O
aﬃnity	O
learning	O
of	O
image	O
segmentation	O
.	O
in	O
,	O
pages	O
1865–1873	O
.	O
nips	O
’	O
2009	O
brown	O
,	O
p.	O
f.	O
,	O
cocke	O
,	O
j.	O
,	O
pietra	O
,	O
s.	O
a.	O
d.	O
,	O
pietra	O
,	O
v.	O
j.	O
d.	O
,	O
jelinek	O
,	O
f.	O
,	O
laﬀerty	O
,	O
j.	O
d.	O
,	O
mercer	O
,	O
r.	O
l.	O
,	O
and	O
roossin	O
,	O
p.	O
s.	O
(	O
1990	O
)	O
.	O
a	O
statistical	O
approach	O
to	O
machine	O
translation	O
.	O
computational	O
linguistics	O
,	O
(	O
2	O
)	O
,	O
79–85	O
.	O
16	O
21	O
brown	O
,	O
p.	O
f.	O
,	O
pietra	O
,	O
v.	O
j.	O
d.	O
,	O
desouza	O
,	O
p.	O
v.	O
,	O
lai	O
,	O
j.	O
c.	O
,	O
and	O
mercer	O
,	O
r.	O
l.	O
(	O
1992	O
)	O
.	O
class-	O
computational	O
linguistics	O
18	O
,	O
467–479	O
.	O
n	O
based	O
-gram	O
models	O
of	O
natural	O
language	O
.	O
463	O
,	O
bryson	O
,	O
a.	O
and	O
ho	O
,	O
y	O
.	O
(	O
1969	O
)	O
.	O
applied	O
optimal	O
control	O
:	O
optimization	O
,	O
estimation	O
,	O
and	O
control	O
.	O
blaisdell	O
pub	O
.	O
co.	O
225	O
bryson	O
,	O
jr.	O
,	O
a.	O
e.	O
and	O
denham	O
,	O
w.	O
f.	O
(	O
1961	O
)	O
.	O
a	O
steepest-ascent	O
method	O
for	O
solving	O
optimum	O
programming	O
problems	O
.	O
technical	O
report	O
br-1303	O
,	O
raytheon	O
company	O
,	O
missle	O
and	O
space	O
division	O
.	O
225	O
buciluˇa	O
,	O
c.	O
,	O
caruana	O
,	O
r.	O
,	O
and	O
niculescu-mizil	O
,	O
a	O
.	O
(	O
2006	O
)	O
.	O
model	B
compression	O
.	O
in	O
proceedings	O
of	O
the	O
12th	O
acm	O
sigkdd	O
international	O
conference	O
on	O
knowledge	O
discovery	O
and	O
data	O
mining	O
,	O
pages	O
535–541	O
.	O
acm	O
.	O
448	O
burda	O
,	O
y.	O
,	O
grosse	O
,	O
r.	O
,	O
and	O
salakhutdinov	O
,	O
r.	O
(	O
2015	O
)	O
.	O
importance	O
weighted	O
autoencoders	O
.	O
arxiv	O
preprint	O
arxiv:1509.00519	O
.	O
698	O
cai	O
,	O
m.	O
,	O
shi	O
,	O
y.	O
,	O
and	O
liu	O
,	O
j	O
.	O
(	O
2013	O
)	O
.	O
deep	O
maxout	O
neural	O
networks	O
for	O
speech	O
recognition	B
.	O
in	O
automatic	O
speech	O
recognition	B
and	O
understanding	O
(	O
asru	O
)	O
,	O
2013	O
ieee	O
workshop	O
on	O
,	O
pages	O
291–296	O
.	O
ieee	O
.	O
194	O
carreira-perpiñan	O
,	O
m.	O
a.	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
2005	O
)	O
.	O
on	O
contrastive	O
divergence	O
learning	O
.	O
in	O
r.	O
g.	O
cowell	O
and	O
z.	O
ghahramani	O
,	O
editors	O
,	O
proceedings	O
of	O
the	O
tenth	O
international	O
workshop	O
on	O
artiﬁcial	O
intelligence	O
and	O
statistics	O
(	O
aistats	O
’	O
05	O
)	O
,	O
pages	O
33–40	O
.	O
society	O
for	O
artiﬁcial	O
intelligence	O
and	O
statistics	O
.	O
611	O
caruana	O
,	O
r.	O
(	O
1993	O
)	O
.	O
multitask	O
connectionist	O
learning	O
.	O
in	O
proc	O
.	O
1993	O
connectionist	O
models	O
summer	O
school	O
,	O
pages	O
372–379	O
.	O
244	O
cauchy	O
,	O
a	O
.	O
(	O
1847	O
)	O
.	O
méthode	O
générale	O
pour	O
la	O
résolution	O
de	O
systèmes	O
d	O
’	O
équations	O
simul-	O
,83	O
tanées	O
.	O
in	O
compte	O
rendu	O
des	O
séances	O
de	O
l	O
’	O
académie	O
des	O
sciences	O
,	O
pages	O
536–538	O
.	O
225	O
728	O
bibliography	O
cayton	O
,	O
l.	O
(	O
2005	O
)	O
.	O
algorithms	O
for	O
manifold	O
learning	O
.	O
technical	O
report	O
cs2008-0923	O
,	O
ucsd	O
.	O
164	O
chandola	O
,	O
v.	O
,	O
banerjee	O
,	O
a.	O
,	O
and	O
kumar	O
,	O
v.	O
(	O
2009	O
)	O
.	O
anomaly	O
detection	O
:	O
a	O
survey	O
.	O
acm	O
computing	O
surveys	O
(	O
csur	O
)	O
,	O
41	O
(	O
3	O
)	O
,	O
15	O
.	O
102	O
chapelle	O
,	O
o.	O
,	O
weston	O
,	O
j.	O
,	O
and	O
schölkopf	O
,	O
b	O
.	O
(	O
2003	O
)	O
.	O
cluster	O
kernels	O
for	O
semi-supervised	O
learning	O
.	O
in	O
s.	O
becker	O
,	O
s.	O
thrun	O
,	O
and	O
k.	O
obermayer	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
15	O
(	O
nips	O
’	O
02	O
)	O
,	O
pages	O
585–592	O
,	O
cambridge	O
,	O
ma	O
.	O
mit	O
press	O
.	O
244	O
chapelle	O
,	O
o.	O
,	O
schölkopf	O
,	O
b.	O
,	O
and	O
zien	O
,	O
a.	O
,	O
editors	O
(	O
2006	O
)	O
.	O
semi-supervised	O
learning	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
244	O
541	O
,	O
chellapilla	O
,	O
k.	O
,	O
puri	O
,	O
s.	O
,	O
and	O
simard	O
,	O
p.	O
(	O
2006	O
)	O
.	O
high	O
performance	O
convolutional	O
neural	O
in	O
guy	O
lorette	O
,	O
editor	O
,	O
tenth	O
international	O
networks	O
for	O
document	O
processing	O
.	O
workshop	O
on	O
frontiers	O
in	O
handwriting	O
recognition	B
,	O
la	O
baule	O
(	O
france	O
)	O
.	O
université	O
de	O
rennes	O
1	O
,	O
suvisoft	O
.	O
http	O
:	O
//www.suvisoft.com	O
.	O
24	O
27	O
445	O
,	O
,	O
chen	O
,	O
b.	O
,	O
ting	O
,	O
j.-a.	O
,	O
marlin	O
,	O
b.	O
m.	O
,	O
and	O
de	O
freitas	O
,	O
n.	O
(	O
2010	O
)	O
.	O
deep	O
learning	O
of	O
invariant	O
spatio-temporal	O
features	O
from	O
video	O
.	O
nips*2010	O
deep	O
learning	O
and	O
unsupervised	O
feature	O
learning	O
workshop	O
.	O
360	O
chen	O
,	O
s.	O
f.	O
and	O
goodman	O
,	O
j.	O
t.	O
(	O
1999	O
)	O
.	O
an	O
empirical	O
study	O
of	O
smoothing	O
techniques	O
for	O
language	O
modeling	O
.	O
computer	O
,	O
speech	O
and	O
language	O
,	O
13	O
(	O
4	O
)	O
,	O
359–393	O
.	O
462	O
463	O
473	O
,	O
,	O
chen	O
,	O
t.	O
,	O
du	O
,	O
z.	O
,	O
sun	O
,	O
n.	O
,	O
wang	O
,	O
j.	O
,	O
wu	O
,	O
c.	O
,	O
chen	O
,	O
y.	O
,	O
and	O
temam	O
,	O
o	O
.	O
(	O
2014a	O
)	O
.	O
diannao	O
:	O
a	O
small-footprint	O
high-throughput	O
accelerator	O
for	O
ubiquitous	O
machine-learning	O
.	O
in	O
pro-	O
ceedings	O
of	O
the	O
19th	O
international	O
conference	O
on	O
architectural	O
support	O
for	O
programming	O
languages	O
and	O
operating	O
systems	O
,	O
pages	O
269–284	O
.	O
acm	O
.	O
451	O
chen	O
,	O
t.	O
,	O
li	O
,	O
m.	O
,	O
li	O
,	O
y.	O
,	O
lin	O
,	O
m.	O
,	O
wang	O
,	O
n.	O
,	O
wang	O
,	O
m.	O
,	O
xiao	O
,	O
t.	O
,	O
xu	O
,	O
b.	O
,	O
zhang	O
,	O
c.	O
,	O
and	O
zhang	O
,	O
z	O
.	O
(	O
2015	O
)	O
.	O
mxnet	O
:	O
a	O
ﬂexible	O
and	O
eﬃcient	O
machine	O
learning	O
library	O
for	O
heterogeneous	O
distributed	O
systems	O
.	O
arxiv	O
preprint	O
arxiv:1512.01274	O
.	O
25	O
(	O
2014b	O
)	O
.	O
dadiannao	O
:	O
a	O
machine-learning	O
supercomputer	O
.	O
in	O
chen	O
,	O
y.	O
,	O
luo	O
,	O
t.	O
,	O
liu	O
,	O
s.	O
,	O
zhang	O
,	O
s.	O
,	O
he	O
,	O
l.	O
,	O
wang	O
,	O
j.	O
,	O
li	O
,	O
l.	O
,	O
chen	O
,	O
t.	O
,	O
xu	O
,	O
z.	O
,	O
sun	O
,	O
n.	O
,	O
et	O
al	O
.	O
microarchitecture	O
(	O
micro	O
)	O
,	O
2014	O
47th	O
annual	O
ieee/acm	O
international	O
symposium	O
on	O
,	O
pages	O
609–622	O
.	O
ieee	O
.	O
451	O
chilimbi	O
,	O
t.	O
,	O
suzue	O
,	O
y.	O
,	O
apacible	O
,	O
j.	O
,	O
and	O
kalyanaraman	O
,	O
k.	O
(	O
2014	O
)	O
.	O
project	O
adam	O
:	O
building	O
an	O
eﬃcient	O
and	O
scalable	O
deep	O
learning	O
training	O
system	O
.	O
in	O
11th	O
usenix	O
symposium	O
on	O
operating	O
systems	O
design	O
and	O
implementation	O
(	O
osdi	O
’	O
14	O
)	O
.	O
447	O
cho	O
,	O
k.	O
,	O
raiko	O
,	O
t.	O
,	O
and	O
ilin	O
,	O
a	O
.	O
(	O
2010	O
)	O
.	O
parallel	O
tempering	O
is	O
eﬃcient	O
for	O
learning	O
restricted	O
boltzmann	O
machines	O
.	O
in	O
ijcnn	O
’	O
2010	O
603	O
614	O
.	O
,	O
729	O
bibliography	O
cho	O
,	O
k.	O
,	O
raiko	O
,	O
t.	O
,	O
and	O
ilin	O
,	O
a	O
.	O
(	O
2011	O
)	O
.	O
enhanced	O
gradient	O
and	O
adaptive	O
learning	O
rate	O
for	O
training	O
restricted	O
boltzmann	O
machines	O
.	O
in	O
icml	O
’	O
2011	O
,	O
pages	O
105–112	O
.	O
674	O
cho	O
,	O
k.	O
,	O
van	O
merriënboer	O
,	O
b.	O
,	O
gulcehre	O
,	O
c.	O
,	O
bougares	O
,	O
f.	O
,	O
schwenk	O
,	O
h.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2014a	O
)	O
.	O
learning	O
phrase	O
representations	O
using	O
rnn	O
encoder-decoder	O
for	O
statistical	O
machine	O
translation	O
.	O
in	O
proceedings	O
of	O
the	O
empiricial	O
methods	O
in	O
natural	O
language	O
processing	O
(	O
emnlp	O
2014	O
)	O
.	O
397	O
474	O
475	O
,	O
,	O
cho	O
,	O
k.	O
,	O
van	O
merriënboer	O
,	O
b.	O
,	O
bahdanau	O
,	O
d.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2014b	O
)	O
.	O
on	O
the	O
prop-	O
,	O
arxiv	O
e-prints	O
erties	O
of	O
neural	O
machine	O
translation	O
:	O
encoder-decoder	O
approaches	O
.	O
abs/1409.1259	O
.	O
412	O
choromanska	O
,	O
a.	O
,	O
henaﬀ	O
,	O
m.	O
,	O
mathieu	O
,	O
m.	O
,	O
arous	O
,	O
g.	O
b.	O
,	O
and	O
lecun	O
,	O
y	O
.	O
(	O
2014	O
)	O
.	O
the	O
loss	O
surface	O
of	O
multilayer	O
networks	O
.	O
285	O
286	O
,	O
chorowski	O
,	O
j.	O
,	O
bahdanau	O
,	O
d.	O
,	O
cho	O
,	O
k.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2014	O
)	O
.	O
end-to-end	O
continuous	O
speech	O
recognition	B
using	O
attention-based	O
recurrent	O
nn	O
:	O
first	O
results	O
.	O
arxiv:1412.1602	O
.	O
461	O
christianson	O
,	O
b	O
.	O
(	O
1992	O
)	O
.	O
automatic	O
hessians	O
by	O
reverse	O
accumulation	O
.	O
ima	O
journal	O
of	O
numerical	O
analysis	O
,	O
12	O
(	O
2	O
)	O
,	O
135–150	O
.	O
224	O
chrupala	O
,	O
g.	O
,	O
kadar	O
,	O
a.	O
,	O
and	O
alishahi	O
,	O
a	O
.	O
(	O
2015	O
)	O
.	O
learning	O
language	O
through	O
pictures	O
.	O
arxiv	O
1506.03694	O
.	O
412	O
chung	O
,	O
j.	O
,	O
gulcehre	O
,	O
c.	O
,	O
cho	O
,	O
k.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2014	O
)	O
.	O
empirical	O
evaluation	O
of	O
gated	O
recurrent	O
neural	O
networks	O
on	O
sequence	O
modeling	O
.	O
nips	O
’	O
2014	O
deep	O
learning	O
workshop	O
,	O
arxiv	O
1412.3555	O
.	O
412	O
460	O
,	O
chung	O
,	O
j.	O
,	O
gülçehre	O
,	O
ç.	O
,	O
cho	O
,	O
k.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2015a	O
)	O
.	O
gated	O
feedback	O
recurrent	O
neural	O
networks	O
.	O
in	O
icml	O
’	O
15	O
412	O
.	O
chung	O
,	O
j.	O
,	O
kastner	O
,	O
k.	O
,	O
dinh	O
,	O
l.	O
,	O
goel	O
,	O
k.	O
,	O
courville	O
,	O
a.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2015b	O
)	O
.	O
a	O
recurrent	O
latent	O
variable	O
model	B
for	O
sequential	O
data	O
.	O
in	O
nips	O
’	O
2015	O
698	O
.	O
ciresan	O
,	O
d.	O
,	O
meier	O
,	O
u.	O
,	O
masci	O
,	O
j.	O
,	O
and	O
schmidhuber	O
,	O
j	O
.	O
(	O
2012	O
)	O
.	O
multi-column	O
deep	O
neural	O
network	O
for	O
traﬃc	O
sign	O
classiﬁcation	O
.	O
neural	O
networks	O
,	O
32	O
,	O
333–338	O
.	O
23	O
201	O
,	O
ciresan	O
,	O
d.	O
c.	O
,	O
meier	O
,	O
u.	O
,	O
gambardella	O
,	O
l.	O
m.	O
,	O
and	O
schmidhuber	O
,	O
j	O
.	O
(	O
2010	O
)	O
.	O
deep	O
big	O
simple	O
neural	O
nets	O
for	O
handwritten	O
digit	O
recognition	B
.	O
neural	O
computation	O
,	O
22	O
,	O
1–14	O
.	O
24	O
27	O
446	O
,	O
,	O
coates	O
,	O
a.	O
and	O
ng	O
,	O
a.	O
y	O
.	O
(	O
2011	O
)	O
.	O
the	O
importance	O
of	O
encoding	O
versus	O
training	O
with	O
sparse	O
coding	O
and	O
vector	O
quantization	O
.	O
in	O
icml	O
’	O
2011	O
27	O
256	O
498	O
.	O
,	O
,	O
coates	O
,	O
a.	O
,	O
lee	O
,	O
h.	O
,	O
and	O
ng	O
,	O
a.	O
y	O
.	O
(	O
2011	O
)	O
.	O
an	O
analysis	O
of	O
single-layer	O
networks	O
in	O
unsupervised	O
feature	O
learning	O
.	O
in	O
proceedings	O
of	O
the	O
thirteenth	O
international	O
conference	O
on	O
artiﬁcial	O
intelligence	O
and	O
statistics	O
(	O
aistats	O
2011	O
)	O
.	O
363	O
364	O
455	O
,	O
,	O
730	O
bibliography	O
coates	O
,	O
a.	O
,	O
huval	O
,	O
b.	O
,	O
wang	O
,	O
t.	O
,	O
wu	O
,	O
d.	O
,	O
catanzaro	O
,	O
b.	O
,	O
and	O
andrew	O
,	O
n.	O
(	O
2013	O
)	O
.	O
deep	O
learning	O
with	O
cots	O
hpc	O
systems	O
.	O
in	O
s.	O
dasgupta	O
and	O
d.	O
mcallester	O
,	O
editors	O
,	O
proceedings	O
of	O
the	O
30th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml-13	O
)	O
,	O
volume	O
28	O
(	O
3	O
)	O
,	O
pages	O
1337–1345	O
.	O
jmlr	O
workshop	O
and	O
conference	O
proceedings	O
.	O
24	O
27	O
,	O
364	O
447	O
,	O
,	O
cohen	O
,	O
n.	O
,	O
sharir	O
,	O
o.	O
,	O
and	O
shashua	O
,	O
a	O
.	O
(	O
2015	O
)	O
.	O
on	O
the	O
expressive	O
power	O
of	O
deep	O
learning	O
:	O
a	O
tensor	O
analysis	O
.	O
arxiv:1509.05009	O
.	O
554	O
collobert	O
,	O
r.	O
(	O
2004	O
)	O
.	O
large	O
scale	O
machine	O
learning	O
.	O
ph.d.	O
thesis	O
,	O
université	O
de	O
paris	O
vi	O
,	O
lip6	O
.	O
197	O
collobert	O
,	O
r.	O
(	O
2011	O
)	O
.	O
deep	O
learning	O
for	O
eﬃcient	O
discriminative	O
parsing	O
.	O
in	O
aistats	O
’	O
2011	O
.	O
101	O
477	O
,	O
collobert	O
,	O
r.	O
and	O
weston	O
,	O
j	O
.	O
(	O
2008a	O
)	O
.	O
a	O
uniﬁed	O
architecture	O
for	O
natural	O
language	O
processing	O
:	O
deep	O
neural	O
networks	O
with	O
multitask	O
learning	O
.	O
in	O
icml	O
’	O
2008	O
471	O
477	O
.	O
,	O
collobert	O
,	O
r.	O
and	O
weston	O
,	O
j	O
.	O
(	O
2008b	O
)	O
.	O
a	O
uniﬁed	O
architecture	O
for	O
natural	O
language	O
processing	O
:	O
deep	O
neural	O
networks	O
with	O
multitask	O
learning	O
.	O
in	O
icml	O
’	O
2008	O
535	O
.	O
collobert	O
,	O
r.	O
,	O
bengio	O
,	O
s.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2001	O
)	O
.	O
a	O
parallel	O
mixture	O
of	O
svms	O
for	O
very	O
large	O
scale	O
problems	O
.	O
technical	O
report	O
idiap-rr-01-12	O
,	O
idiap	O
.	O
450	O
collobert	O
,	O
r.	O
,	O
bengio	O
,	O
s.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2002	O
)	O
.	O
parallel	O
mixture	O
of	O
svms	O
for	O
very	O
large	O
scale	O
problems	O
.	O
neural	O
computation	O
,	O
14	O
(	O
5	O
)	O
,	O
1105–1114	O
.	O
450	O
collobert	O
,	O
r.	O
,	O
weston	O
,	O
j.	O
,	O
bottou	O
,	O
l.	O
,	O
karlen	O
,	O
m.	O
,	O
kavukcuoglu	O
,	O
k.	O
,	O
and	O
kuksa	O
,	O
p.	O
(	O
2011a	O
)	O
.	O
natural	O
language	O
processing	O
(	O
almost	O
)	O
from	O
scratch	O
.	O
the	O
journal	O
of	O
machine	O
learning	O
research	O
,	O
328	O
477	O
535	O
536	O
,	O
2493–2537	O
.	O
12	O
,	O
,	O
,	O
collobert	O
,	O
r.	O
,	O
kavukcuoglu	O
,	O
k.	O
,	O
and	O
farabet	O
,	O
c.	O
(	O
2011b	O
)	O
.	O
torch7	O
:	O
a	O
matlab-like	O
environ-	O
ment	O
for	O
machine	O
learning	O
.	O
in	O
biglearn	O
,	O
nips	O
workshop	O
.	O
25	O
214	O
446	O
,	O
,	O
comon	O
,	O
p.	O
(	O
1994	O
)	O
.	O
independent	O
component	O
analysis	O
-	O
a	O
new	O
concept	O
?	O
signal	O
processing	O
,	O
36	O
,	O
287–314	O
.	O
491	O
cortes	O
,	O
c.	O
and	O
vapnik	O
,	O
v.	O
(	O
1995	O
)	O
.	O
support	O
vector	O
networks	O
.	O
machine	O
learning	O
,	O
20	O
,	O
273–297	O
.	O
,18	O
141	O
couprie	O
,	O
c.	O
,	O
farabet	O
,	O
c.	O
,	O
najman	O
,	O
l.	O
,	O
and	O
lecun	O
,	O
y	O
.	O
(	O
2013	O
)	O
.	O
indoor	O
semantic	O
segmentation	O
using	O
depth	O
information	O
.	O
in	O
international	O
conference	O
on	O
learning	O
representations	O
(	O
iclr2013	O
)	O
.	O
,23	O
201	O
courbariaux	O
,	O
m.	O
,	O
bengio	O
,	O
y.	O
,	O
and	O
david	O
,	O
j.-p.	O
(	O
2015	O
)	O
.	O
low	O
precision	O
arithmetic	O
for	O
deep	O
learning	O
.	O
in	O
arxiv:1412.7024	O
,	O
iclr	O
’	O
2015	O
workshop	O
.	O
452	O
courville	O
,	O
a.	O
,	O
bergstra	O
,	O
j.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2011	O
)	O
.	O
unsupervised	O
models	O
of	O
images	O
by	O
spike-and-slab	O
rbms	O
.	O
in	O
icml	O
’	O
11	O
561	O
681	O
.	O
,	O
731	O
bibliography	O
courville	O
,	O
a.	O
,	O
desjardins	O
,	O
g.	O
,	O
bergstra	O
,	O
j.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2014	O
)	O
.	O
the	O
spike-and-slab	O
rbm	O
and	O
extensions	O
to	O
discrete	O
and	O
sparse	O
data	O
distributions	O
.	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
ieee	O
transactions	O
on	O
,	O
(	O
9	O
)	O
,	O
1874–1887	O
.	O
682	O
36	O
cover	O
,	O
t.	O
m.	O
and	O
thomas	O
,	O
j.	O
a	O
.	O
(	O
2006	O
)	O
.	O
elements	O
of	O
information	O
theory	O
,	O
2nd	O
edition	O
.	O
wiley-interscience	O
.	O
73	O
cox	O
,	O
d.	O
and	O
pinto	O
,	O
n.	O
(	O
2011	O
)	O
.	O
beyond	O
simple	O
features	O
:	O
a	O
large-scale	O
feature	O
search	O
approach	O
to	O
unconstrained	O
face	O
recognition	B
.	O
in	O
automatic	O
face	O
&	O
gesture	O
recognition	B
and	O
workshops	O
(	O
fg	O
2011	O
)	O
,	O
2011	O
ieee	O
international	O
conference	O
on	O
,	O
pages	O
8–15	O
.	O
ieee	O
.	O
363	O
cramér	O
,	O
h.	O
(	O
1946	O
)	O
.	O
mathematical	O
methods	O
of	O
statistics	O
.	O
princeton	O
university	O
press	O
.	O
135	O
,	O
295	O
crick	O
,	O
f.	O
h.	O
c.	O
and	O
mitchison	O
,	O
g.	O
(	O
1983	O
)	O
.	O
the	O
function	O
of	O
dream	O
sleep	O
.	O
nature	O
,	O
304	O
,	O
111–114	O
.	O
609	O
cybenko	O
,	O
g.	O
(	O
1989	O
)	O
.	O
approximation	O
by	O
superpositions	O
of	O
a	O
sigmoidal	O
function	O
.	O
mathematics	O
of	O
control	O
,	O
signals	O
,	O
and	O
systems	O
,	O
2	O
,	O
303–314	O
.	O
198	O
dahl	O
,	O
g.	O
e.	O
,	O
ranzato	O
,	O
m.	O
,	O
mohamed	O
,	O
a.	O
,	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
2010	O
)	O
.	O
phone	O
recognition	B
with	O
the	O
mean-covariance	O
restricted	O
boltzmann	O
machine	O
.	O
in	O
nips	O
’	O
2010	O
23	O
.	O
dahl	O
,	O
g.	O
e.	O
,	O
yu	O
,	O
d.	O
,	O
deng	O
,	O
l.	O
,	O
and	O
acero	O
,	O
a	O
.	O
(	O
2012	O
)	O
.	O
context-dependent	O
pre-trained	O
deep	O
neural	O
networks	O
for	O
large	O
vocabulary	O
speech	O
recognition	B
.	O
ieee	O
transactions	O
on	O
audio	O
,	O
speech	O
,	O
and	O
language	O
processing	O
,	O
(	O
1	O
)	O
,	O
33–42	O
.	O
459	O
20	O
dahl	O
,	O
g.	O
e.	O
,	O
sainath	O
,	O
t.	O
n.	O
,	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
2013	O
)	O
.	O
improving	O
deep	O
neural	O
networks	O
for	O
lvcsr	O
using	O
rectiﬁed	O
linear	O
units	O
and	O
dropout	O
.	O
in	O
icassp	O
’	O
2013	O
460	O
.	O
dahl	O
,	O
g.	O
e.	O
,	O
jaitly	O
,	O
n.	O
,	O
and	O
salakhutdinov	O
,	O
r.	O
(	O
2014	O
)	O
.	O
multi-task	O
neural	O
networks	O
for	O
qsar	O
predictions	O
.	O
arxiv:1406.1231	O
.	O
26	O
dauphin	O
,	O
y.	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2013	O
)	O
.	O
stochastic	O
ratio	O
matching	O
of	O
rbms	O
for	O
sparse	O
high-dimensional	O
inputs	O
.	O
in	O
nips26	O
.	O
nips	O
foundation	O
.	O
619	O
dauphin	O
,	O
y.	O
,	O
glorot	O
,	O
x.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2011	O
)	O
.	O
large-scale	O
learning	O
of	O
embeddings	O
with	O
reconstruction	O
sampling	O
.	O
in	O
icml	O
’	O
2011	O
471	O
.	O
dauphin	O
,	O
y.	O
,	O
pascanu	O
,	O
r.	O
,	O
gulcehre	O
,	O
c.	O
,	O
cho	O
,	O
k.	O
,	O
ganguli	O
,	O
s.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2014	O
)	O
.	O
identifying	O
and	O
attacking	O
the	O
saddle	O
point	O
problem	O
in	O
high-dimensional	O
non-convex	O
optimization	O
.	O
in	O
nips	O
’	O
2014	O
285	O
286	O
288	O
.	O
,	O
,	O
davis	O
,	O
a.	O
,	O
rubinstein	O
,	O
m.	O
,	O
wadhwa	O
,	O
n.	O
,	O
mysore	O
,	O
g.	O
,	O
durand	O
,	O
f.	O
,	O
and	O
freeman	O
,	O
w.	O
t.	O
(	O
2014	O
)	O
.	O
the	O
visual	O
microphone	O
:	O
passive	O
recovery	O
of	O
sound	O
from	O
video	O
.	O
acm	O
transactions	O
on	O
graphics	O
(	O
proc	O
.	O
siggraph	O
)	O
,	O
(	O
4	O
)	O
,	O
79:1–79:10	O
.	O
452	O
33	O
732	O
bibliography	O
dayan	O
,	O
p.	O
(	O
1990	O
)	O
.	O
reinforcement	O
comparison	O
.	O
in	O
connectionist	O
models	O
:	O
proceedings	O
of	O
the	O
1990	O
connectionist	O
summer	O
school	O
,	O
san	O
mateo	O
,	O
ca	O
.	O
691	O
dayan	O
,	O
p.	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
1996	O
)	O
.	O
varieties	O
of	O
helmholtz	O
machine	O
.	O
neural	O
networks	O
,	O
9	O
(	O
8	O
)	O
,	O
1385–1403	O
.	O
693	O
dayan	O
,	O
p.	O
,	O
hinton	O
,	O
g.	O
e.	O
,	O
neal	O
,	O
r.	O
m.	O
,	O
and	O
zemel	O
,	O
r.	O
s.	O
(	O
1995	O
)	O
.	O
the	O
helmholtz	O
machine	O
.	O
neural	O
computation	O
,	O
7	O
(	O
5	O
)	O
,	O
889–904	O
.	O
693	O
dean	O
,	O
j.	O
,	O
corrado	O
,	O
g.	O
,	O
monga	O
,	O
r.	O
,	O
chen	O
,	O
k.	O
,	O
devin	O
,	O
m.	O
,	O
le	O
,	O
q.	O
,	O
mao	O
,	O
m.	O
,	O
ranzato	O
,	O
m.	O
,	O
senior	O
,	O
a.	O
,	O
tucker	O
,	O
p.	O
,	O
yang	O
,	O
k.	O
,	O
and	O
ng	O
,	O
a.	O
y	O
.	O
(	O
2012	O
)	O
.	O
large	O
scale	O
distributed	O
deep	O
networks	O
.	O
in	O
nips	O
’	O
2012	O
25	O
447	O
.	O
,	O
dean	O
,	O
t.	O
and	O
kanazawa	O
,	O
k.	O
(	O
1989	O
)	O
.	O
a	O
model	B
for	O
reasoning	O
about	O
persistence	O
and	O
causation	O
.	O
computational	O
intelligence	O
,	O
5	O
(	O
3	O
)	O
,	O
142–150	O
.	O
662	O
deerwester	O
,	O
s.	O
,	O
dumais	O
,	O
s.	O
t.	O
,	O
furnas	O
,	O
g.	O
w.	O
,	O
landauer	O
,	O
t.	O
k.	O
,	O
and	O
harshman	O
,	O
r.	O
(	O
1990	O
)	O
.	O
indexing	O
by	O
latent	O
semantic	O
analysis	O
.	O
journal	O
of	O
the	O
american	O
society	O
for	O
information	O
science	O
,	O
(	O
6	O
)	O
,	O
391–407	O
.	O
41	O
477	O
482	O
,	O
delalleau	O
,	O
o.	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2011	O
)	O
.	O
shallow	O
vs.	O
deep	O
sum-product	O
networks	O
.	O
in	O
nips	O
.	O
19	O
554	O
,	O
deng	O
,	O
j.	O
,	O
dong	O
,	O
w.	O
,	O
socher	O
,	O
r.	O
,	O
li	O
,	O
l.-j.	O
,	O
li	O
,	O
k.	O
,	O
and	O
fei-fei	O
,	O
l.	O
(	O
2009	O
)	O
.	O
imagenet	O
:	O
a	O
large-scale	O
hierarchical	O
image	O
database	O
.	O
in	O
cvpr09	O
21	O
.	O
deng	O
,	O
j.	O
,	O
berg	O
,	O
a.	O
c.	O
,	O
li	O
,	O
k.	O
,	O
and	O
fei-fei	O
,	O
l.	O
(	O
2010a	O
)	O
.	O
what	O
does	O
classifying	O
more	O
than	O
10,000	O
image	O
categories	O
tell	O
us	O
?	O
in	O
proceedings	O
of	O
the	O
11th	O
european	O
conference	O
on	O
computer	O
vision	O
:	O
part	O
v	O
,	O
eccv	O
’	O
10	O
,	O
pages	O
71–84	O
,	O
berlin	O
,	O
heidelberg	O
.	O
springer-verlag	O
.	O
21	O
deng	O
,	O
l.	O
and	O
yu	O
,	O
d.	O
(	O
2014	O
)	O
.	O
deep	O
learning	O
–	O
methods	O
and	O
applications	O
.	O
foundations	O
and	O
trends	O
in	O
signal	O
processing	O
.	O
460	O
deng	O
,	O
l.	O
,	O
seltzer	O
,	O
m.	O
,	O
yu	O
,	O
d.	O
,	O
acero	O
,	O
a.	O
,	O
mohamed	O
,	O
a.	O
,	O
and	O
hinton	O
,	O
g.	O
(	O
2010b	O
)	O
.	O
binary	O
coding	O
of	O
speech	O
spectrograms	O
using	O
a	O
deep	O
auto-encoder	O
.	O
in	O
interspeech	O
2010	O
,	O
makuhari	O
,	O
chiba	O
,	O
japan	O
.	O
23	O
denil	O
,	O
m.	O
,	O
bazzani	O
,	O
l.	O
,	O
larochelle	O
,	O
h.	O
,	O
and	O
de	O
freitas	O
,	O
n.	O
(	O
2012	O
)	O
.	O
learning	O
where	O
to	O
attend	O
with	O
deep	O
architectures	O
for	O
image	O
tracking	O
.	O
neural	O
computation	O
,	O
24	O
(	O
8	O
)	O
,	O
2151–2184	O
.	O
367	O
denton	O
,	O
e.	O
,	O
chintala	O
,	O
s.	O
,	O
szlam	O
,	O
a.	O
,	O
and	O
fergus	O
,	O
r.	O
(	O
2015	O
)	O
.	O
deep	O
generative	O
image	O
models	O
using	O
a	O
laplacian	O
pyramid	O
of	O
adversarial	O
networks	O
.	O
nips	O
702	O
719	O
.	O
,	O
desjardins	O
,	O
g.	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2008	O
)	O
.	O
empirical	O
evaluation	O
of	O
convolutional	O
rbms	O
for	O
vision	O
.	O
technical	O
report	O
1327	O
,	O
département	O
d	O
’	O
informatique	O
et	O
de	O
recherche	O
opéra-	O
tionnelle	O
,	O
université	O
de	O
montréal	O
.	O
683	O
733	O
bibliography	O
desjardins	O
,	O
g.	O
,	O
courville	O
,	O
a.	O
c.	O
,	O
bengio	O
,	O
y.	O
,	O
vincent	O
,	O
p.	O
,	O
and	O
delalleau	O
,	O
o	O
.	O
(	O
2010	O
)	O
.	O
tempered	O
markov	O
chain	O
monte	O
carlo	O
for	O
training	O
of	O
restricted	O
boltzmann	O
machines	O
.	O
in	O
international	O
conference	O
on	O
artiﬁcial	O
intelligence	O
and	O
statistics	O
,	O
pages	O
145–152	O
.	O
,	O
603	O
614	O
desjardins	O
,	O
g.	O
,	O
courville	O
,	O
a.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2011	O
)	O
.	O
on	O
tracking	O
the	O
partition	O
function	O
.	O
in	O
nips	O
’	O
2011	O
629	O
.	O
desjardins	O
,	O
g.	O
,	O
simonyan	O
,	O
k.	O
,	O
pascanu	O
,	O
r.	O
,	O
et	O
al	O
.	O
(	O
2015	O
)	O
.	O
natural	O
neural	O
networks	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
,	O
pages	O
2062–2070	O
.	O
320	O
devlin	O
,	O
j.	O
,	O
zbib	O
,	O
r.	O
,	O
huang	O
,	O
z.	O
,	O
lamar	O
,	O
t.	O
,	O
schwartz	O
,	O
r.	O
,	O
and	O
makhoul	O
,	O
j	O
.	O
(	O
2014	O
)	O
.	O
fast	O
and	O
robust	O
neural	O
network	O
joint	O
models	O
for	O
statistical	O
machine	O
translation	O
.	O
in	O
proc	O
.	O
acl	O
’	O
2014	O
.	O
473	O
devroye	O
,	O
l.	O
(	O
2013	O
)	O
.	O
non-uniform	O
random	O
variate	O
generation	O
.	O
springerlink	O
:	O
bücher	O
.	O
springer	O
new	O
york	O
.	O
694	O
dicarlo	O
,	O
j.	O
j	O
.	O
(	O
2013	O
)	O
.	O
mechanisms	O
underlying	O
visual	O
object	O
recognition	B
:	O
humans	O
vs.	O
neurons	O
vs.	O
machines	O
.	O
nips	O
tutorial	O
.	O
,26	O
366	O
dinh	O
,	O
l.	O
,	O
krueger	O
,	O
d.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2014	O
)	O
.	O
nice	O
:	O
non-linear	O
independent	O
components	O
estimation	O
.	O
arxiv:1410.8516	O
.	O
493	O
donahue	O
,	O
j.	O
,	O
hendricks	O
,	O
l.	O
a.	O
,	O
guadarrama	O
,	O
s.	O
,	O
rohrbach	O
,	O
m.	O
,	O
venugopalan	O
,	O
s.	O
,	O
saenko	O
,	O
k.	O
,	O
and	O
darrell	O
,	O
t.	O
(	O
2014	O
)	O
.	O
long-term	O
recurrent	O
convolutional	O
networks	O
for	O
visual	O
recognition	B
and	O
description	O
.	O
arxiv:1411.4389	O
.	O
102	O
donoho	O
,	O
d.	O
l.	O
and	O
grimes	O
,	O
c.	O
(	O
2003	O
)	O
.	O
hessian	O
eigenmaps	O
:	O
new	O
locally	O
linear	O
embedding	O
techniques	O
for	O
high-dimensional	O
data	O
.	O
technical	O
report	O
2003-08	O
,	O
dept	O
.	O
statistics	O
,	O
stanford	O
university	O
.	O
164	O
519	O
,	O
dosovitskiy	O
,	O
a.	O
,	O
springenberg	O
,	O
j.	O
t.	O
,	O
and	O
brox	O
,	O
t.	O
(	O
2015	O
)	O
.	O
learning	O
to	O
generate	O
chairs	O
with	O
convolutional	O
neural	O
networks	O
.	O
in	O
proceedings	O
of	O
the	O
ieee	O
conference	O
on	O
computer	O
vision	O
and	O
pattern	O
recognition	B
,	O
pages	O
1538–1546	O
.	O
696	O
704	O
705	O
,	O
,	O
doya	O
,	O
k.	O
(	O
1993	O
)	O
.	O
bifurcations	O
of	O
recurrent	O
neural	O
networks	O
in	O
gradient	O
descent	B
learning	O
.	O
ieee	O
transactions	O
on	O
neural	O
networks	O
,	O
1	O
,	O
75–80	O
.	O
401	O
403	O
,	O
dreyfus	O
,	O
s.	O
e.	O
(	O
1962	O
)	O
.	O
the	O
numerical	O
solution	O
of	O
variational	O
problems	O
.	O
journal	O
of	O
mathematical	O
analysis	O
and	O
applications	O
,	O
5	O
(	O
1	O
)	O
,	O
30–45	O
.	O
225	O
dreyfus	O
,	O
s.	O
e.	O
(	O
1973	O
)	O
.	O
the	O
computational	O
solution	O
of	O
optimal	O
control	O
problems	O
with	O
time	O
lag	O
.	O
ieee	O
transactions	O
on	O
automatic	O
control	O
,	O
18	O
(	O
4	O
)	O
,	O
383–385	O
.	O
225	O
drucker	O
,	O
h.	O
and	O
lecun	O
,	O
y	O
.	O
(	O
1992	O
)	O
.	O
improving	O
generalisation	O
performance	O
using	O
double	O
back-propagation	O
.	O
ieee	O
transactions	O
on	O
neural	O
networks	O
,	O
3	O
(	O
6	O
)	O
,	O
991–997	O
.	O
271	O
734	O
bibliography	O
duchi	O
,	O
j.	O
,	O
hazan	O
,	O
e.	O
,	O
and	O
singer	O
,	O
y	O
.	O
(	O
2011	O
)	O
.	O
adaptive	O
subgradient	O
methods	O
for	O
online	O
learning	O
and	O
stochastic	O
optimization	O
.	O
journal	O
of	O
machine	O
learning	O
research	O
.	O
307	O
dudik	O
,	O
m.	O
,	O
langford	O
,	O
j.	O
,	O
and	O
li	O
,	O
l.	O
(	O
2011	O
)	O
.	O
doubly	O
robust	O
policy	O
evaluation	O
and	O
learning	O
.	O
in	O
proceedings	O
of	O
the	O
28th	O
international	O
conference	O
on	O
machine	O
learning	O
,	O
icml	O
’	O
11	O
.	O
482	O
dugas	O
,	O
c.	O
,	O
bengio	O
,	O
y.	O
,	O
bélisle	O
,	O
f.	O
,	O
and	O
nadeau	O
,	O
c.	O
(	O
2001	O
)	O
.	O
incorporating	O
second-order	O
functional	O
knowledge	O
for	O
better	O
option	O
pricing	O
.	O
in	O
t.	O
leen	O
,	O
t.	O
dietterich	O
,	O
and	O
v.	O
tresp	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
13	O
(	O
nips	O
’	O
00	O
)	O
,	O
pages	O
472–478	O
.	O
mit	O
press	O
.	O
,68	O
197	O
dziugaite	O
,	O
g.	O
k.	O
,	O
roy	O
,	O
d.	O
m.	O
,	O
and	O
ghahramani	O
,	O
z	O
.	O
(	O
2015	O
)	O
.	O
training	O
generative	O
neural	O
net-	O
works	O
via	O
maximum	O
mean	O
discrepancy	O
optimization	O
.	O
arxiv	O
preprint	O
arxiv:1505.03906	O
.	O
703	O
el	O
hihi	O
,	O
s.	O
and	O
bengio	O
,	O
y	O
.	O
(	O
1996	O
)	O
.	O
hierarchical	O
recurrent	O
neural	O
networks	O
for	O
long-term	O
dependencies	O
.	O
in	O
nips	O
’	O
1995	O
398	O
407	O
408	O
.	O
,	O
,	O
elkahky	O
,	O
a.	O
m.	O
,	O
song	O
,	O
y.	O
,	O
and	O
he	O
,	O
x	O
.	O
(	O
2015	O
)	O
.	O
a	O
multi-view	O
deep	O
learning	O
approach	O
for	O
cross	O
domain	O
user	O
modeling	O
in	O
recommendation	O
systems	O
.	O
in	O
proceedings	O
of	O
the	O
24th	O
international	O
conference	O
on	O
world	O
wide	O
web	O
,	O
pages	O
278–288	O
.	O
480	O
elman	O
,	O
j.	O
l.	O
(	O
1993	O
)	O
.	O
learning	O
and	O
development	O
in	O
neural	O
networks	O
:	O
the	O
importance	O
of	O
starting	O
small	O
.	O
cognition	O
,	O
48	O
,	O
781–799	O
.	O
328	O
erhan	O
,	O
d.	O
,	O
manzagol	O
,	O
p.-a.	O
,	O
bengio	O
,	O
y.	O
,	O
bengio	O
,	O
s.	O
,	O
and	O
vincent	O
,	O
p.	O
(	O
2009	O
)	O
.	O
the	O
diﬃculty	O
of	O
training	O
deep	O
architectures	O
and	O
the	O
eﬀect	O
of	O
unsupervised	O
pre-training	O
.	O
in	O
proceedings	O
of	O
aistats	O
’	O
2009	O
.	O
201	O
erhan	O
,	O
d.	O
,	O
bengio	O
,	O
y.	O
,	O
courville	O
,	O
a.	O
,	O
manzagol	O
,	O
p.	O
,	O
vincent	O
,	O
p.	O
,	O
and	O
bengio	O
,	O
s.	O
(	O
2010	O
)	O
.	O
why	O
does	O
unsupervised	O
pre-training	O
help	O
deep	O
learning	O
?	O
j.	O
machine	O
learning	O
res	O
.	O
529	O
533	O
534	O
,	O
,	O
fahlman	O
,	O
s.	O
e.	O
,	O
hinton	O
,	O
g.	O
e.	O
,	O
and	O
sejnowski	O
,	O
t.	O
j	O
.	O
(	O
1983	O
)	O
.	O
massively	O
parallel	O
architectures	O
in	O
proceedings	O
of	O
the	O
national	O
for	O
ai	O
:	O
netl	O
,	O
thistle	O
,	O
and	O
boltzmann	O
machines	O
.	O
conference	O
on	O
artiﬁcial	O
intelligence	O
aaai-83	O
.	O
,	O
570	O
654	O
fang	O
,	O
h.	O
,	O
gupta	O
,	O
s.	O
,	O
iandola	O
,	O
f.	O
,	O
srivastava	O
,	O
r.	O
,	O
deng	O
,	O
l.	O
,	O
dollár	O
,	O
p.	O
,	O
gao	O
,	O
j.	O
,	O
he	O
,	O
x.	O
,	O
mitchell	O
,	O
m.	O
,	O
platt	O
,	O
j.	O
c.	O
,	O
zitnick	O
,	O
c.	O
l.	O
,	O
and	O
zweig	O
,	O
g.	O
(	O
2015	O
)	O
.	O
from	O
captions	O
to	O
visual	O
concepts	O
and	O
back	O
.	O
arxiv:1411.4952	O
.	O
102	O
farabet	O
,	O
c.	O
,	O
lecun	O
,	O
y.	O
,	O
kavukcuoglu	O
,	O
k.	O
,	O
culurciello	O
,	O
e.	O
,	O
martini	O
,	O
b.	O
,	O
akselrod	O
,	O
p.	O
,	O
and	O
talay	O
,	O
s.	O
(	O
2011	O
)	O
.	O
large-scale	O
fpga-based	O
convolutional	O
networks	O
.	O
in	O
r.	O
bekkerman	O
,	O
m.	O
bilenko	O
,	O
and	O
j.	O
langford	O
,	O
editors	O
,	O
scaling	O
up	O
machine	O
learning	O
:	O
parallel	O
and	O
distributed	O
approaches	O
.	O
cambridge	O
university	O
press	O
.	O
523	O
735	O
bibliography	O
farabet	O
,	O
c.	O
,	O
couprie	O
,	O
c.	O
,	O
najman	O
,	O
l.	O
,	O
and	O
lecun	O
,	O
y	O
.	O
(	O
2013	O
)	O
.	O
learning	O
hierarchical	O
features	O
for	O
scene	O
labeling	O
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
35	O
(	O
8	O
)	O
,	O
1915–1929	O
.	O
23	O
201	O
360	O
,	O
,	O
fei-fei	O
,	O
l.	O
,	O
fergus	O
,	O
r.	O
,	O
and	O
perona	O
,	O
p.	O
(	O
2006	O
)	O
.	O
one-shot	O
learning	O
of	O
object	O
categories	O
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
28	O
(	O
4	O
)	O
,	O
594–611	O
.	O
538	O
finn	O
,	O
c.	O
,	O
tan	O
,	O
x.	O
y.	O
,	O
duan	O
,	O
y.	O
,	O
darrell	O
,	O
t.	O
,	O
levine	O
,	O
s.	O
,	O
and	O
abbeel	O
,	O
p.	O
(	O
2015	O
)	O
.	O
learning	O
visual	O
feature	O
spaces	O
for	O
robotic	O
manipulation	O
with	O
deep	O
spatial	O
autoencoders	O
.	O
arxiv	O
preprint	O
arxiv:1509.06113	O
.	O
25	O
fisher	O
,	O
r.	O
a	O
.	O
(	O
1936	O
)	O
.	O
the	O
use	O
of	O
multiple	O
measurements	O
in	O
taxonomic	O
problems	O
.	O
annals	O
of	O
eugenics	O
,	O
7	O
,	O
179–188	O
.	O
21	O
105	O
,	O
földiák	O
,	O
p.	O
(	O
1989	O
)	O
.	O
adaptive	O
network	O
for	O
optimal	O
linear	O
feature	O
extraction	O
.	O
in	O
international	O
joint	O
conference	O
on	O
neural	O
networks	O
(	O
ijcnn	O
)	O
,	O
volume	O
1	O
,	O
pages	O
401–405	O
,	O
washington	O
1989.	O
ieee	O
,	O
new	O
york	O
.	O
494	O
franzius	O
,	O
m.	O
,	O
sprekeler	O
,	O
h.	O
,	O
and	O
wiskott	O
,	O
l.	O
(	O
2007	O
)	O
.	O
slowness	O
and	O
sparseness	O
lead	O
to	O
place	O
,	O
head-direction	O
,	O
and	O
spatial-view	O
cells	O
.	O
495	O
franzius	O
,	O
m.	O
,	O
wilbert	O
,	O
n.	O
,	O
and	O
wiskott	O
,	O
l.	O
(	O
2008	O
)	O
.	O
invariant	O
object	O
recognition	B
with	O
slow	O
feature	O
analysis	O
.	O
in	O
artiﬁcial	O
neural	O
networks-icann	O
2008	O
,	O
pages	O
961–970	O
.	O
springer	O
.	O
496	O
frasconi	O
,	O
p.	O
,	O
gori	O
,	O
m.	O
,	O
and	O
sperduti	O
,	O
a	O
.	O
(	O
1997	O
)	O
.	O
on	O
the	O
eﬃcient	O
classiﬁcation	O
of	O
data	O
structures	O
by	O
neural	O
networks	O
.	O
in	O
proc	O
.	O
int	O
.	O
joint	O
conf	O
.	O
on	O
artiﬁcial	O
intelligence	O
.	O
401	O
frasconi	O
,	O
p.	O
,	O
gori	O
,	O
m.	O
,	O
and	O
sperduti	O
,	O
a	O
.	O
(	O
1998	O
)	O
.	O
a	O
general	O
framework	O
for	O
adaptive	O
processing	O
of	O
data	O
structures	O
.	O
ieee	O
transactions	O
on	O
neural	O
networks	O
,	O
9	O
(	O
5	O
)	O
,	O
768–786	O
.	O
401	O
freund	O
,	O
y.	O
and	O
schapire	O
,	O
r.	O
e.	O
(	O
1996a	O
)	O
.	O
experiments	O
with	O
a	O
new	O
boosting	O
algorithm	O
.	O
in	O
machine	O
learning	O
:	O
proceedings	O
of	O
thirteenth	O
international	O
conference	O
,	O
pages	O
148–156	O
,	O
usa	O
.	O
acm	O
.	O
258	O
freund	O
,	O
y.	O
and	O
schapire	O
,	O
r.	O
e.	O
(	O
1996b	O
)	O
.	O
game	O
theory	O
,	O
on-line	O
prediction	O
and	O
boosting	O
.	O
in	O
proceedings	O
of	O
the	O
ninth	O
annual	O
conference	O
on	O
computational	O
learning	O
theory	O
,	O
pages	O
325–332	O
.	O
258	O
frey	O
,	O
b.	O
j	O
.	O
(	O
1998	O
)	O
.	O
graphical	O
models	O
for	O
machine	O
learning	O
and	O
digital	O
communication	O
.	O
mit	O
press	O
.	O
705	O
706	O
,	O
frey	O
,	O
b.	O
j.	O
,	O
hinton	O
,	O
g.	O
e.	O
,	O
and	O
dayan	O
,	O
p.	O
(	O
1996	O
)	O
.	O
does	O
the	O
wake-sleep	O
algorithm	O
learn	O
good	O
density	O
estimators	O
?	O
in	O
d.	O
touretzky	O
,	O
m.	O
mozer	O
,	O
and	O
m.	O
hasselmo	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
8	O
(	O
nips	O
’	O
95	O
)	O
,	O
pages	O
661–670	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
651	O
736	O
bibliography	O
frobenius	O
,	O
g.	O
(	O
1908	O
)	O
.	O
über	O
matrizen	O
aus	O
positiven	O
elementen	O
,	O
s.	O
b.	O
preuss	O
.	O
akad	O
.	O
wiss	O
.	O
berlin	O
,	O
germany	O
.	O
597	O
fukushima	O
,	O
k.	O
(	O
1975	O
)	O
.	O
cognitron	O
:	O
a	O
self-organizing	O
multilayered	O
neural	O
network	O
.	O
biological	O
cybernetics	O
,	O
20	O
,	O
121–136	O
.	O
16	O
226	O
528	O
,	O
,	O
fukushima	O
,	O
k.	O
(	O
1980	O
)	O
.	O
neocognitron	O
:	O
a	O
self-organizing	O
neural	O
network	O
model	B
for	O
a	O
mechanism	O
of	O
pattern	O
recognition	B
unaﬀected	O
by	O
shift	O
in	O
position	B
.	O
biological	O
cybernetics	O
,	O
36	O
,	O
193–202	O
.	O
16	O
24	O
27	O
226	O
367	O
,	O
,	O
,	O
,	O
gal	O
,	O
y.	O
and	O
ghahramani	O
,	O
z	O
.	O
(	O
2015	O
)	O
.	O
bayesian	O
convolutional	O
neural	O
networks	O
with	O
bernoulli	O
approximate	O
variational	O
inference	O
.	O
arxiv	O
preprint	O
arxiv:1506.02158	O
.	O
264	O
gallinari	O
,	O
p.	O
,	O
lecun	O
,	O
y.	O
,	O
thiria	O
,	O
s.	O
,	O
and	O
fogelman-soulie	O
,	O
f.	O
(	O
1987	O
)	O
.	O
memoires	O
associatives	O
distribuees	O
.	O
in	O
proceedings	O
of	O
cognitiva	O
87	O
,	O
paris	O
,	O
la	O
villette	O
.	O
515	O
garcia-duran	O
,	O
a.	O
,	O
bordes	O
,	O
a.	O
,	O
usunier	O
,	O
n.	O
,	O
and	O
grandvalet	O
,	O
y	O
.	O
(	O
2015	O
)	O
.	O
combining	O
two	O
and	O
three-way	O
embeddings	O
models	O
for	O
link	O
prediction	O
in	O
knowledge	O
bases	O
.	O
arxiv	O
preprint	O
arxiv:1506.00999	O
.	O
484	O
garofolo	O
,	O
j.	O
s.	O
,	O
lamel	O
,	O
l.	O
f.	O
,	O
fisher	O
,	O
w.	O
m.	O
,	O
fiscus	O
,	O
j.	O
g.	O
,	O
and	O
pallett	O
,	O
d.	O
s.	O
(	O
1993	O
)	O
.	O
darpa	O
timit	O
acoustic-phonetic	O
continous	O
speech	O
corpus	O
cd-rom	O
.	O
nist	O
speech	O
disc	O
1-1.1.	O
nasa	O
sti/recon	O
technical	O
report	O
n	O
,	O
,	O
27403	O
.	O
459	O
93	O
garson	O
,	O
j	O
.	O
(	O
1900	O
)	O
.	O
the	O
metric	O
system	O
of	O
identiﬁcation	O
of	O
criminals	O
,	O
as	O
used	O
in	O
great	O
britain	O
and	O
ireland	O
.	O
the	O
journal	O
of	O
the	O
anthropological	O
institute	O
of	O
great	O
britain	O
and	O
ireland	O
,	O
(	O
2	O
)	O
,	O
177–227	O
.	O
21	O
gers	O
,	O
f.	O
a.	O
,	O
schmidhuber	O
,	O
j.	O
,	O
and	O
cummins	O
,	O
f.	O
(	O
2000	O
)	O
.	O
learning	O
to	O
forget	O
:	O
continual	O
prediction	O
with	O
lstm	O
.	O
neural	O
computation	O
,	O
12	O
(	O
10	O
)	O
,	O
2451–2471	O
.	O
410	O
412	O
,	O
ghahramani	O
,	O
z.	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
1996	O
)	O
.	O
the	O
em	O
algorithm	O
for	O
mixtures	O
of	O
factor	O
analyzers	O
.	O
technical	O
report	O
crg-tr-96-1	O
,	O
dpt	O
.	O
of	O
comp	O
.	O
sci.	O
,	O
univ	O
.	O
of	O
toronto	O
.	O
489	O
gillick	O
,	O
d.	O
,	O
brunk	O
,	O
c.	O
,	O
vinyals	O
,	O
o.	O
,	O
and	O
subramanya	O
,	O
a	O
.	O
(	O
2015	O
)	O
.	O
multilingual	O
language	O
processing	O
from	O
bytes	O
.	O
arxiv	O
preprint	O
arxiv:1512.00103	O
.	O
477	O
girshick	O
,	O
r.	O
,	O
donahue	O
,	O
j.	O
,	O
darrell	O
,	O
t.	O
,	O
and	O
malik	O
,	O
j	O
.	O
(	O
2015	O
)	O
.	O
region-based	O
convolutional	O
networks	O
for	O
accurate	O
object	O
detection	O
and	O
segmentation	O
.	O
426	O
giudice	O
,	O
m.	O
d.	O
,	O
manera	O
,	O
v.	O
,	O
and	O
keysers	O
,	O
c.	O
(	O
2009	O
)	O
.	O
programmed	O
to	O
learn	O
?	O
the	O
ontogeny	O
of	O
mirror	O
neurons	O
.	O
dev	O
.	O
sci	O
.	O
12	O
,	O
(	O
2	O
)	O
,	O
350––363	O
.	O
656	O
glorot	O
,	O
x.	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2010	O
)	O
.	O
understanding	O
the	O
diﬃculty	O
of	O
training	O
deep	O
feedforward	O
neural	O
networks	O
.	O
in	O
aistats	O
’	O
2010	O
.	O
303	O
glorot	O
,	O
x.	O
,	O
bordes	O
,	O
a.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2011a	O
)	O
.	O
deep	O
sparse	O
rectiﬁer	O
neural	O
networks	O
.	O
in	O
aistats	O
’	O
2011	O
.	O
16	O
174	O
197	O
226	O
227	O
,	O
,	O
,	O
,	O
737	O
bibliography	O
glorot	O
,	O
x.	O
,	O
bordes	O
,	O
a.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2011b	O
)	O
.	O
domain	O
adaptation	O
for	O
large-scale	O
sentiment	O
classiﬁcation	O
:	O
a	O
deep	O
learning	O
approach	O
.	O
in	O
icml	O
’	O
2011	O
507	O
537	O
.	O
,	O
goldberger	O
,	O
j.	O
,	O
roweis	O
,	O
s.	O
,	O
hinton	O
,	O
g.	O
e.	O
,	O
and	O
salakhutdinov	O
,	O
r.	O
(	O
2005	O
)	O
.	O
neighbourhood	O
components	O
analysis	O
.	O
in	O
l.	O
saul	O
,	O
y.	O
weiss	O
,	O
and	O
l.	O
bottou	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
17	O
(	O
nips	O
’	O
04	O
)	O
.	O
mit	O
press	O
.	O
115	O
gong	O
,	O
s.	O
,	O
mckenna	O
,	O
s.	O
,	O
and	O
psarrou	O
,	O
a	O
.	O
(	O
2000	O
)	O
.	O
dynamic	O
vision	O
:	O
from	O
images	O
to	O
face	O
recognition	B
.	O
imperial	O
college	O
press	O
.	O
165	O
519	O
,	O
goodfellow	O
,	O
i.	O
,	O
le	O
,	O
q.	O
,	O
saxe	O
,	O
a.	O
,	O
and	O
ng	O
,	O
a	O
.	O
(	O
2009	O
)	O
.	O
measuring	O
invariances	O
in	O
deep	O
networks	O
.	O
in	O
nips	O
’	O
2009	O
,	O
pages	O
646–654	O
.	O
255	O
goodfellow	O
,	O
i.	O
,	O
koenig	O
,	O
n.	O
,	O
muja	O
,	O
m.	O
,	O
pantofaru	O
,	O
c.	O
,	O
sorokin	O
,	O
a.	O
,	O
and	O
takayama	O
,	O
l.	O
(	O
2010	O
)	O
.	O
help	O
me	O
help	O
you	O
:	O
interfaces	O
for	O
personal	O
robots	O
.	O
in	O
proc	O
.	O
of	O
human	O
robot	O
interaction	O
(	O
hri	O
)	O
,	O
osaka	O
,	O
japan	O
.	O
acm	O
press	O
,	O
acm	O
press	O
.	O
100	O
goodfellow	O
,	O
i.	O
j	O
.	O
(	O
2010	O
)	O
.	O
technical	O
report	O
:	O
multidimensional	O
,	O
downsampled	O
convolution	O
for	O
autoencoders	O
.	O
technical	O
report	O
,	O
université	O
de	O
montréal	O
.	O
357	O
goodfellow	O
,	O
i.	O
j	O
.	O
(	O
2014	O
)	O
.	O
on	O
distinguishability	O
criteria	O
for	O
estimating	O
generative	O
models	O
.	O
622	O
700	O
,	O
in	O
international	O
conference	O
on	O
learning	O
representations	O
,	O
workshops	O
track	O
.	O
701	O
,	O
goodfellow	O
,	O
i.	O
j.	O
,	O
courville	O
,	O
a.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2011	O
)	O
.	O
spike-and-slab	O
sparse	O
coding	O
in	O
nips	O
workshop	O
on	O
challenges	O
in	O
learning	O
for	O
unsupervised	O
feature	O
discovery	O
.	O
hierarchical	O
models	O
.	O
532	O
538	O
,	O
goodfellow	O
,	O
i.	O
j.	O
,	O
warde-farley	O
,	O
d.	O
,	O
mirza	O
,	O
m.	O
,	O
courville	O
,	O
a.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2013a	O
)	O
.	O
,	O
pages	O
1319–	O
maxout	O
networks	O
.	O
in	O
s.	O
dasgupta	O
and	O
d.	O
mcallester	O
,	O
editors	O
,	O
1327	O
.	O
193	O
264	O
344	O
365	O
455	O
icml	O
’	O
13	O
,	O
,	O
,	O
,	O
goodfellow	O
,	O
i.	O
j.	O
,	O
mirza	O
,	O
m.	O
,	O
courville	O
,	O
a.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2013b	O
)	O
.	O
multi-prediction	O
deep	O
100	O
617	O
671	O
672	O
673	O
674	O
675	O
,	O
.	O
nips	O
foundation	O
.	O
nips26	O
,	O
,	O
,	O
boltzmann	O
machines	O
.	O
in	O
698	O
,	O
,	O
,	O
goodfellow	O
,	O
i.	O
j.	O
,	O
warde-farley	O
,	O
d.	O
,	O
lamblin	O
,	O
p.	O
,	O
dumoulin	O
,	O
v.	O
,	O
mirza	O
,	O
m.	O
,	O
pascanu	O
,	O
r.	O
,	O
bergstra	O
,	O
j.	O
,	O
bastien	O
,	O
f.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2013c	O
)	O
.	O
pylearn2	O
:	O
a	O
machine	O
learning	O
research	O
library	O
.	O
arxiv	O
preprint	O
arxiv:1308.4214	O
.	O
,25	O
446	O
goodfellow	O
,	O
i.	O
j.	O
,	O
courville	O
,	O
a.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2013d	O
)	O
.	O
scaling	O
up	O
spike-and-slab	O
models	O
for	O
unsupervised	O
feature	O
learning	O
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
497	O
498	O
499	O
650	O
683	O
(	O
8	O
)	O
,	O
1902–1914	O
.	O
35	O
,	O
,	O
,	O
,	O
goodfellow	O
,	O
i.	O
j.	O
,	O
mirza	O
,	O
m.	O
,	O
xiao	O
,	O
d.	O
,	O
courville	O
,	O
a.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2014a	O
)	O
.	O
an	O
empirical	O
.	O
iclr	O
’	O
2014	O
investigation	O
of	O
catastrophic	O
forgeting	O
in	O
gradient-based	O
neural	O
networks	O
.	O
in	O
194	O
738	O
bibliography	O
goodfellow	O
,	O
i.	O
j.	O
,	O
shlens	O
,	O
j.	O
,	O
and	O
szegedy	O
,	O
c.	O
(	O
2014b	O
)	O
.	O
explaining	O
and	O
harnessing	O
adver-	O
sarial	O
examples	O
.	O
corr	O
abs/1412.6572	O
268	O
269	O
271	O
555	O
556	O
,	O
.	O
,	O
,	O
,	O
,	O
goodfellow	O
,	O
i.	O
j.	O
,	O
pouget-abadie	O
,	O
j.	O
,	O
mirza	O
,	O
m.	O
,	O
xu	O
,	O
b.	O
,	O
warde-farley	O
,	O
d.	O
,	O
ozair	O
,	O
s.	O
,	O
.	O
nips	O
’	O
2014	O
courville	O
,	O
a.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2014c	O
)	O
.	O
generative	O
adversarial	O
networks	O
.	O
in	O
544	O
689	O
699	O
701	O
704	O
,	O
,	O
,	O
,	O
goodfellow	O
,	O
i.	O
j.	O
,	O
bulatov	O
,	O
y.	O
,	O
ibarz	O
,	O
j.	O
,	O
arnoud	O
,	O
s.	O
,	O
and	O
shet	O
,	O
v.	O
(	O
2014d	O
)	O
.	O
multi-digit	O
number	O
recognition	B
from	O
street	O
view	O
imagery	O
using	O
deep	O
convolutional	O
neural	O
networks	O
.	O
in	O
international	O
conference	O
on	O
learning	O
representations	O
.	O
25	O
101	O
201	O
202	O
203	O
391	O
,	O
422	O
449	O
,	O
,	O
,	O
,	O
,	O
,	O
goodfellow	O
,	O
i.	O
j.	O
,	O
vinyals	O
,	O
o.	O
,	O
and	O
saxe	O
,	O
a.	O
m.	O
(	O
2015	O
)	O
.	O
qualitatively	O
characterizing	O
neural	O
network	O
optimization	O
problems	O
.	O
in	O
international	O
conference	O
on	O
learning	O
representa-	O
tions	O
.	O
285	O
286	O
287	O
291	O
,	O
,	O
,	O
goodman	O
,	O
j	O
.	O
(	O
2001	O
)	O
.	O
classes	O
for	O
fast	O
maximum	O
entropy	O
training	O
.	O
in	O
international	O
conference	O
on	O
acoustics	O
,	O
speech	O
and	O
signal	O
processing	O
(	O
icassp	O
)	O
,	O
utah	O
.	O
467	O
gori	O
,	O
m.	O
and	O
tesi	O
,	O
a	O
.	O
(	O
1992	O
)	O
.	O
on	O
the	O
problem	O
of	O
local	O
minima	O
in	O
backpropagation	O
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
pami-14	O
(	O
1	O
)	O
,	O
76–86	O
.	O
284	O
gosset	O
,	O
w.	O
s.	O
(	O
1908	O
)	O
.	O
the	O
probable	O
error	O
of	O
a	O
mean	O
.	O
biometrika	O
6	O
(	O
1	O
)	O
,	O
1–25	O
.	O
originally	O
,	O
published	O
under	O
the	O
pseudonym	O
“	O
student	O
”	O
.	O
21	O
gouws	O
,	O
s.	O
,	O
bengio	O
,	O
y.	O
,	O
and	O
corrado	O
,	O
g.	O
(	O
2014	O
)	O
.	O
bilbowa	O
:	O
fast	O
bilingual	O
distributed	O
476	O
539	O
representations	O
without	O
word	O
alignments	O
.	O
technical	O
report	O
,	O
arxiv:1410.2455.	O
,	O
graf	O
,	O
h.	O
p.	O
and	O
jackel	O
,	O
l.	O
d.	O
(	O
1989	O
)	O
.	O
analog	O
electronic	O
neural	O
network	O
circuits	O
.	O
circuits	O
and	O
devices	O
magazine	O
,	O
ieee	O
,	O
5	O
(	O
4	O
)	O
,	O
44–49	O
.	O
451	O
graves	O
,	O
a	O
.	O
(	O
2011	O
)	O
.	O
practical	O
variational	O
inference	O
for	O
neural	O
networks	O
.	O
in	O
nips	O
’	O
2011	O
242	O
.	O
graves	O
,	O
a	O
.	O
(	O
2012	O
)	O
.	O
supervised	O
sequence	O
labelling	O
with	O
recurrent	O
neural	O
networks	O
.	O
studies	O
in	O
computational	O
intelligence	O
.	O
springer	O
.	O
374	O
395	O
411	O
460	O
,	O
,	O
,	O
graves	O
,	O
a	O
.	O
(	O
2013	O
)	O
.	O
generating	O
sequences	O
with	O
recurrent	O
neural	O
networks	O
.	O
technical	O
report	O
,	O
arxiv:1308.0850	O
.	O
190	O
410	O
415	O
420	O
,	O
,	O
,	O
graves	O
,	O
a.	O
and	O
jaitly	O
,	O
n.	O
(	O
2014	O
)	O
.	O
towards	O
end-to-end	O
speech	O
recognition	B
with	O
recurrent	O
neural	O
networks	O
.	O
in	O
icml	O
’	O
2014	O
410	O
.	O
graves	O
,	O
a.	O
and	O
schmidhuber	O
,	O
j	O
.	O
(	O
2005	O
)	O
.	O
framewise	O
phoneme	O
classiﬁcation	O
with	O
bidirec-	O
tional	O
lstm	O
and	O
other	O
neural	O
network	O
architectures	O
.	O
neural	O
networks	O
,	O
18	O
(	O
5	O
)	O
,	O
602–610	O
.	O
395	O
graves	O
,	O
a.	O
and	O
schmidhuber	O
,	O
j	O
.	O
(	O
2009	O
)	O
.	O
oﬄine	O
handwriting	O
recognition	B
with	O
multidi-	O
mensional	O
recurrent	O
neural	O
networks	O
.	O
in	O
d.	O
koller	O
,	O
d.	O
schuurmans	O
,	O
y.	O
bengio	O
,	O
and	O
l.	O
bottou	O
,	O
editors	O
,	O
,	O
pages	O
545–552	O
.	O
nips	O
’	O
2008	O
395	O
739	O
bibliography	O
graves	O
,	O
a.	O
,	O
fernández	O
,	O
s.	O
,	O
gomez	O
,	O
f.	O
,	O
and	O
schmidhuber	O
,	O
j	O
.	O
(	O
2006	O
)	O
.	O
connectionist	O
temporal	O
classiﬁcation	O
:	O
labelling	O
unsegmented	O
sequence	O
data	O
with	O
recurrent	O
neural	O
networks	O
.	O
in	O
icml	O
’	O
2006	O
,	O
pages	O
369–376	O
,	O
pittsburgh	O
,	O
usa	O
.	O
460	O
graves	O
,	O
a.	O
,	O
liwicki	O
,	O
m.	O
,	O
bunke	O
,	O
h.	O
,	O
schmidhuber	O
,	O
j.	O
,	O
and	O
fernández	O
,	O
s.	O
(	O
2008	O
)	O
.	O
uncon-	O
strained	O
on-line	O
handwriting	O
recognition	B
with	O
recurrent	O
neural	O
networks	O
.	O
in	O
j.	O
platt	O
,	O
d.	O
koller	O
,	O
y.	O
singer	O
,	O
and	O
s.	O
roweis	O
,	O
editors	O
,	O
,	O
pages	O
577–584	O
.	O
nips	O
’	O
2007	O
395	O
graves	O
,	O
a.	O
,	O
liwicki	O
,	O
m.	O
,	O
fernández	O
,	O
s.	O
,	O
bertolami	O
,	O
r.	O
,	O
bunke	O
,	O
h.	O
,	O
and	O
schmidhuber	O
,	O
j	O
.	O
(	O
2009	O
)	O
.	O
a	O
novel	O
connectionist	O
system	O
for	O
unconstrained	O
handwriting	O
recognition	B
.	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
ieee	O
transactions	O
on	O
,	O
(	O
5	O
)	O
,	O
855–868	O
.	O
410	O
31	O
graves	O
,	O
a.	O
,	O
mohamed	O
,	O
a.	O
,	O
and	O
hinton	O
,	O
g.	O
(	O
2013	O
)	O
.	O
speech	O
recognition	B
with	O
deep	O
recurrent	O
neural	O
networks	O
.	O
in	O
icassp	O
’	O
2013	O
,	O
pages	O
6645–6649	O
.	O
395	O
398	O
410	O
411	O
460	O
,	O
,	O
,	O
,	O
graves	O
,	O
a.	O
,	O
wayne	O
,	O
g.	O
,	O
and	O
danihelka	O
,	O
i	O
.	O
(	O
2014a	O
)	O
.	O
neural	O
turing	O
machines	O
.	O
arxiv:1410.5401	O
.	O
25	O
graves	O
,	O
a.	O
,	O
wayne	O
,	O
g.	O
,	O
and	O
danihelka	O
,	O
i	O
.	O
(	O
2014b	O
)	O
.	O
neural	O
turing	O
machines	O
.	O
arxiv	O
preprint	O
arxiv:1410.5401	O
.	O
418	O
grefenstette	O
,	O
e.	O
,	O
hermann	O
,	O
k.	O
m.	O
,	O
suleyman	O
,	O
m.	O
,	O
and	O
blunsom	O
,	O
p.	O
(	O
2015	O
)	O
.	O
learning	O
to	O
transduce	O
with	O
unbounded	O
memory	O
.	O
in	O
nips	O
’	O
2015	O
418	O
.	O
greﬀ	O
,	O
k.	O
,	O
srivastava	O
,	O
r.	O
k.	O
,	O
koutník	O
,	O
j.	O
,	O
steunebrink	O
,	O
b.	O
r.	O
,	O
and	O
schmidhuber	O
,	O
j	O
.	O
(	O
2015	O
)	O
.	O
lstm	O
:	O
a	O
search	O
space	O
odyssey	O
.	O
arxiv	O
preprint	O
arxiv:1503.04069	O
.	O
412	O
gregor	O
,	O
k.	O
and	O
lecun	O
,	O
y	O
.	O
(	O
2010a	O
)	O
.	O
emergence	O
of	O
complex-like	O
cells	O
in	O
a	O
temporal	O
product	O
network	O
with	O
local	O
receptive	O
ﬁelds	O
.	O
technical	O
report	O
,	O
arxiv:1006.0448	O
.	O
352	O
gregor	O
,	O
k.	O
and	O
lecun	O
,	O
y	O
.	O
(	O
2010b	O
)	O
.	O
learning	O
fast	O
approximations	O
of	O
sparse	O
coding	O
.	O
in	O
l.	O
bottou	O
and	O
m.	O
littman	O
,	O
editors	O
,	O
proceedings	O
of	O
the	O
twenty-seventh	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml-10	O
)	O
.	O
acm	O
.	O
652	O
gregor	O
,	O
k.	O
,	O
danihelka	O
,	O
i.	O
,	O
mnih	O
,	O
a.	O
,	O
blundell	O
,	O
c.	O
,	O
and	O
wierstra	O
,	O
d.	O
(	O
2014	O
)	O
.	O
deep	O
autoregressive	O
networks	O
.	O
in	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
’	O
2014	O
)	O
.	O
693	O
gregor	O
,	O
k.	O
,	O
danihelka	O
,	O
i.	O
,	O
graves	O
,	O
a.	O
,	O
and	O
wierstra	O
,	O
d.	O
(	O
2015	O
)	O
.	O
draw	O
:	O
a	O
recurrent	O
neural	O
network	O
for	O
image	O
generation	O
.	O
arxiv	O
preprint	O
arxiv:1502.04623	O
.	O
698	O
gretton	O
,	O
a.	O
,	O
borgwardt	O
,	O
k.	O
m.	O
,	O
rasch	O
,	O
m.	O
j.	O
,	O
schölkopf	O
,	O
b.	O
,	O
and	O
smola	O
,	O
a	O
.	O
(	O
2012	O
)	O
.	O
a	O
kernel	O
two-sample	O
test	O
.	O
the	O
journal	O
of	O
machine	O
learning	O
research	O
,	O
13	O
(	O
1	O
)	O
,	O
723–773	O
.	O
704	O
gülçehre	O
,	O
ç.	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2013	O
)	O
.	O
knowledge	O
matters	O
:	O
importance	O
of	O
prior	O
information	O
for	O
optimization	O
.	O
in	O
international	O
conference	O
on	O
learning	O
representations	O
(	O
iclr	O
’	O
2013	O
)	O
.	O
25	O
740	O
bibliography	O
guo	O
,	O
h.	O
and	O
gelfand	O
,	O
s.	O
b	O
.	O
(	O
1992	O
)	O
.	O
classiﬁcation	O
trees	O
with	O
neural	O
network	O
feature	O
extraction	O
.	O
neural	O
networks	O
,	O
ieee	O
transactions	O
on	O
,	O
3	O
(	O
6	O
)	O
,	O
923–933	O
.	O
450	O
gupta	O
,	O
s.	O
,	O
agrawal	O
,	O
a.	O
,	O
gopalakrishnan	O
,	O
k.	O
,	O
and	O
narayanan	O
,	O
p.	O
(	O
2015	O
)	O
.	O
deep	O
learning	O
with	O
limited	O
numerical	O
precision	O
.	O
corr	O
abs/1502.02551	O
452	O
,	O
.	O
gutmann	O
,	O
m.	O
and	O
hyvarinen	O
,	O
a	O
.	O
(	O
2010	O
)	O
.	O
noise-contrastive	O
estimation	O
:	O
a	O
new	O
estima-	O
tion	B
principle	O
for	O
unnormalized	O
statistical	O
models	O
.	O
in	O
proceedings	O
of	O
the	O
thirteenth	O
international	O
conference	O
on	O
artiﬁcial	O
intelligence	O
and	O
statistics	O
(	O
aistats	O
’	O
10	O
)	O
.	O
620	O
hadsell	O
,	O
r.	O
,	O
sermanet	O
,	O
p.	O
,	O
ben	O
,	O
j.	O
,	O
erkan	O
,	O
a.	O
,	O
han	O
,	O
j.	O
,	O
muller	O
,	O
u.	O
,	O
and	O
lecun	O
,	O
y	O
.	O
(	O
2007	O
)	O
.	O
online	O
learning	O
for	O
oﬀroad	O
robots	O
:	O
spatial	O
label	O
propagation	O
to	O
learn	O
long-range	O
traversability	O
.	O
in	O
proceedings	O
of	O
robotics	O
:	O
science	O
and	O
systems	O
,	O
atlanta	O
,	O
ga	O
,	O
usa	O
.	O
453	O
hajnal	O
,	O
a.	O
,	O
maass	O
,	O
w.	O
,	O
pudlak	O
,	O
p.	O
,	O
szegedy	O
,	O
m.	O
,	O
and	O
turan	O
,	O
g.	O
(	O
1993	O
)	O
.	O
threshold	O
circuits	O
of	O
bounded	O
depth	O
.	O
j.	O
comput	O
.	O
system	O
.	O
sci	O
.	O
46	O
,	O
,	O
129–154	O
.	O
199	O
håstad	O
,	O
j	O
.	O
(	O
1986	O
)	O
.	O
almost	O
optimal	O
lower	O
bounds	O
for	O
small	O
depth	O
circuits	O
.	O
in	O
proceedings	O
of	O
the	O
18th	O
annual	O
acm	O
symposium	O
on	O
theory	O
of	O
computing	O
,	O
pages	O
6–20	O
,	O
berkeley	O
,	O
california	O
.	O
acm	O
press	O
.	O
199	O
håstad	O
,	O
j.	O
and	O
goldmann	O
,	O
m.	O
(	O
1991	O
)	O
.	O
on	O
the	O
power	O
of	O
small-depth	O
threshold	O
circuits	O
.	O
computational	O
complexity	O
,	O
1	O
,	O
113–129	O
.	O
199	O
hastie	O
,	O
t.	O
,	O
tibshirani	O
,	O
r.	O
,	O
and	O
friedman	O
,	O
j	O
.	O
(	O
2001	O
)	O
.	O
the	O
elements	O
of	O
statistical	O
learning	O
:	O
data	O
mining	O
,	O
inference	O
and	O
prediction	O
.	O
springer	O
series	O
in	O
statistics	O
.	O
springer	O
verlag	O
.	O
146	O
he	O
,	O
k.	O
,	O
zhang	O
,	O
x.	O
,	O
ren	O
,	O
s.	O
,	O
and	O
sun	O
,	O
j	O
.	O
(	O
2015	O
)	O
.	O
delving	O
deep	O
into	O
rectiﬁers	O
:	O
surpassing	O
human-level	O
performance	O
on	O
imagenet	O
classiﬁcation	O
.	O
arxiv	O
preprint	O
arxiv:1502.01852	O
.	O
28	O
193	O
,	O
hebb	O
,	O
d.	O
o	O
.	O
(	O
1949	O
)	O
.	O
the	O
organization	O
of	O
behavior	O
.	O
wiley	O
,	O
new	O
york	O
.	O
14	O
17	O
656	O
,	O
,	O
henaﬀ	O
,	O
m.	O
,	O
jarrett	O
,	O
k.	O
,	O
kavukcuoglu	O
,	O
k.	O
,	O
and	O
lecun	O
,	O
y	O
.	O
(	O
2011	O
)	O
.	O
unsupervised	O
learning	O
of	O
sparse	O
features	O
for	O
scalable	O
audio	O
classiﬁcation	O
.	O
in	O
ismir	O
’	O
11	O
523	O
.	O
henderson	O
,	O
j	O
.	O
(	O
2003	O
)	O
.	O
inducing	O
history	O
representations	O
for	O
broad	O
coverage	O
statistical	O
parsing	O
.	O
in	O
hlt-naacl	O
,	O
pages	O
103–110	O
.	O
477	O
henderson	O
,	O
j	O
.	O
(	O
2004	O
)	O
.	O
discriminative	O
training	O
of	O
a	O
neural	O
network	O
statistical	O
parser	O
.	O
in	O
proceedings	O
of	O
the	O
42nd	O
annual	O
meeting	O
on	O
association	O
for	O
computational	O
linguistics	O
,	O
page	O
95	O
.	O
477	O
henniges	O
,	O
m.	O
,	O
puertas	O
,	O
g.	O
,	O
bornschein	O
,	O
j.	O
,	O
eggert	O
,	O
j.	O
,	O
and	O
lücke	O
,	O
j	O
.	O
(	O
2010	O
)	O
.	O
binary	O
sparse	O
coding	O
.	O
in	O
latent	O
variable	O
analysis	O
and	O
signal	O
separation	O
,	O
pages	O
450–457	O
.	O
springer	O
.	O
640	O
741	O
bibliography	O
herault	O
,	O
j.	O
and	O
ans	O
,	O
b	O
.	O
(	O
1984	O
)	O
.	O
circuits	O
neuronaux	O
à	O
synapses	O
modiﬁables	O
:	O
décodage	O
de	O
messages	O
composites	O
par	O
apprentissage	O
non	O
supervisé	O
.	O
comptes	O
rendus	O
de	O
l	O
’	O
académie	O
des	O
sciences	O
,	O
299	O
(	O
iii-13	O
)	O
,	O
525––528	O
.	O
491	O
hinton	O
,	O
g.	O
(	O
2012	O
)	O
.	O
neural	O
networks	O
for	O
machine	O
learning	O
.	O
coursera	O
,	O
video	O
lectures	O
.	O
307	O
hinton	O
,	O
g.	O
,	O
deng	O
,	O
l.	O
,	O
dahl	O
,	O
g.	O
e.	O
,	O
mohamed	O
,	O
a.	O
,	O
jaitly	O
,	O
n.	O
,	O
senior	O
,	O
a.	O
,	O
vanhoucke	O
,	O
v.	O
,	O
nguyen	O
,	O
p.	O
,	O
sainath	O
,	O
t.	O
,	O
and	O
kingsbury	O
,	O
b	O
.	O
(	O
2012a	O
)	O
.	O
deep	O
neural	O
networks	O
for	O
acoustic	O
modeling	O
in	O
speech	O
recognition	B
.	O
ieee	O
signal	O
processing	O
magazine	O
,	O
29	O
(	O
6	O
)	O
,	O
82–97	O
.	O
,23	O
460	O
hinton	O
,	O
g.	O
,	O
vinyals	O
,	O
o.	O
,	O
and	O
dean	O
,	O
j	O
.	O
(	O
2015	O
)	O
.	O
distilling	O
the	O
knowledge	O
in	O
a	O
neural	O
network	O
.	O
arxiv	O
preprint	O
arxiv:1503.02531	O
.	O
448	O
hinton	O
,	O
g.	O
e.	O
(	O
1989	O
)	O
.	O
connectionist	O
learning	O
procedures	O
.	O
artiﬁcial	O
intelligence	O
,	O
40	O
,	O
185–234	O
.	O
494	O
hinton	O
,	O
g.	O
e.	O
(	O
1990	O
)	O
.	O
mapping	O
part-whole	O
hierarchies	O
into	O
connectionist	O
networks	O
.	O
artiﬁcial	O
intelligence	O
,	O
46	O
(	O
1	O
)	O
,	O
47–75	O
.	O
418	O
hinton	O
,	O
g.	O
e.	O
(	O
1999	O
)	O
.	O
products	O
of	O
experts	O
.	O
in	O
icann	O
’	O
1999	O
571	O
.	O
hinton	O
,	O
g.	O
e.	O
(	O
2000	O
)	O
.	O
training	O
products	O
of	O
experts	O
by	O
minimizing	O
contrastive	O
divergence	O
.	O
,	O
610	O
technical	O
report	O
gcnu	O
tr	O
2000-004	O
,	O
gatsby	O
unit	O
,	O
university	O
college	O
london	O
.	O
676	O
hinton	O
,	O
g.	O
e.	O
(	O
2006	O
)	O
.	O
to	O
recognize	O
shapes	O
,	O
ﬁrst	O
learn	O
to	O
generate	O
images	O
.	O
technical	O
report	O
utml	O
tr	O
2006-003	O
,	O
university	O
of	O
toronto	O
.	O
528	O
595	O
,	O
hinton	O
,	O
g.	O
e.	O
(	O
2007a	O
)	O
.	O
how	O
to	O
do	O
backpropagation	O
in	O
a	O
brain	O
.	O
invited	O
talk	O
at	O
the	O
nips	O
’	O
2007	O
deep	O
learning	O
workshop	O
.	O
656	O
hinton	O
,	O
g.	O
e.	O
(	O
2007b	O
)	O
.	O
learning	O
multiple	O
layers	O
of	O
representation	O
.	O
trends	O
in	O
cognitive	O
sciences	O
,	O
11	O
(	O
10	O
)	O
,	O
428–434	O
.	O
660	O
hinton	O
,	O
g.	O
e.	O
(	O
2010	O
)	O
.	O
a	O
practical	O
guide	O
to	O
training	O
restricted	O
boltzmann	O
machines	O
.	O
technical	O
report	O
utml	O
tr	O
2010-003	O
,	O
department	O
of	O
computer	O
science	O
,	O
university	O
of	O
toronto	O
.	O
610	O
hinton	O
,	O
g.	O
e.	O
and	O
ghahramani	O
,	O
z	O
.	O
(	O
1997	O
)	O
.	O
generative	O
models	O
for	O
discovering	O
sparse	O
distributed	O
representations	O
.	O
philosophical	O
transactions	O
of	O
the	O
royal	O
society	O
of	O
london	O
.	O
147	O
hinton	O
,	O
g.	O
e.	O
and	O
mcclelland	O
,	O
j.	O
l.	O
(	O
1988	O
)	O
.	O
learning	O
representations	O
by	O
recirculation	O
.	O
in	O
nips	O
’	O
1987	O
,	O
pages	O
358–366	O
.	O
502	O
hinton	O
,	O
g.	O
e.	O
and	O
roweis	O
,	O
s.	O
(	O
2003	O
)	O
.	O
stochastic	O
neighbor	O
embedding	O
.	O
in	O
nips	O
’	O
2002	O
519	O
.	O
742	O
bibliography	O
hinton	O
,	O
g.	O
e.	O
and	O
salakhutdinov	O
,	O
r.	O
(	O
2006	O
)	O
.	O
reducing	O
the	O
dimensionality	O
of	O
data	O
with	O
neural	O
networks	O
.	O
science	O
,	O
313	O
(	O
5786	O
)	O
,	O
504–507	O
.	O
509	O
524	O
528	O
529	O
534	O
,	O
,	O
,	O
,	O
hinton	O
,	O
g.	O
e.	O
and	O
sejnowski	O
,	O
t.	O
j	O
.	O
(	O
1986	O
)	O
.	O
learning	O
and	O
relearning	O
in	O
boltzmann	O
machines	O
.	O
in	O
d.	O
e.	O
rumelhart	O
and	O
j.	O
l.	O
mcclelland	O
,	O
editors	O
,	O
parallel	O
distributed	O
processing	O
,	O
volume	O
1	O
,	O
chapter	O
7	O
,	O
pages	O
282–317	O
.	O
mit	O
press	O
,	O
cambridge	O
.	O
,570	O
654	O
hinton	O
,	O
g.	O
e.	O
and	O
sejnowski	O
,	O
t.	O
j	O
.	O
(	O
1999	O
)	O
.	O
unsupervised	O
learning	O
:	O
foundations	O
of	O
neural	O
computation	O
.	O
mit	O
press	O
.	O
541	O
hinton	O
,	O
g.	O
e.	O
and	O
shallice	O
,	O
t.	O
(	O
1991	O
)	O
.	O
lesioning	O
an	O
attractor	O
network	O
:	O
investigations	O
of	O
acquired	O
dyslexia	O
.	O
psychological	O
review	O
,	O
98	O
(	O
1	O
)	O
,	O
74	O
.	O
13	O
hinton	O
,	O
g.	O
e.	O
and	O
zemel	O
,	O
r.	O
s.	O
(	O
1994	O
)	O
.	O
autoencoders	O
,	O
minimum	O
description	O
length	O
,	O
and	O
helmholtz	O
free	O
energy	O
.	O
in	O
nips	O
’	O
1993	O
502	O
.	O
hinton	O
,	O
g.	O
e.	O
,	O
sejnowski	O
,	O
t.	O
j.	O
,	O
and	O
ackley	O
,	O
d.	O
h.	O
(	O
1984	O
)	O
.	O
boltzmann	O
machines	O
:	O
constraint	O
satisfaction	O
networks	O
that	O
learn	O
.	O
technical	O
report	O
tr-cmu-cs-84-119	O
,	O
carnegie-mellon	O
university	O
,	O
dept	O
.	O
of	O
computer	O
science	O
.	O
570	O
654	O
,	O
hinton	O
,	O
g.	O
e.	O
,	O
mcclelland	O
,	O
j.	O
,	O
and	O
rumelhart	O
,	O
d.	O
(	O
1986	O
)	O
.	O
distributed	O
representations	O
.	O
in	O
d.	O
e.	O
rumelhart	O
and	O
j.	O
l.	O
mcclelland	O
,	O
editors	O
,	O
parallel	O
distributed	O
processing	O
:	O
explorations	O
in	O
the	O
microstructure	O
of	O
cognition	O
,	O
volume	O
1	O
,	O
pages	O
77–109	O
.	O
mit	O
press	O
,	O
cambridge	O
.	O
17	O
225	O
526	O
,	O
,	O
hinton	O
,	O
g.	O
e.	O
,	O
revow	O
,	O
m.	O
,	O
and	O
dayan	O
,	O
p.	O
(	O
1995a	O
)	O
.	O
recognizing	O
handwritten	O
digits	O
using	O
mixtures	O
of	O
linear	O
models	O
.	O
in	O
g.	O
tesauro	O
,	O
d.	O
touretzky	O
,	O
and	O
t.	O
leen	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
7	O
(	O
nips	O
’	O
94	O
)	O
,	O
pages	O
1015–1022	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
489	O
hinton	O
,	O
g.	O
e.	O
,	O
dayan	O
,	O
p.	O
,	O
frey	O
,	O
b.	O
j.	O
,	O
and	O
neal	O
,	O
r.	O
m.	O
(	O
1995b	O
)	O
.	O
the	O
wake-sleep	O
algorithm	O
for	O
unsupervised	O
neural	O
networks	O
.	O
science	O
,	O
268	O
,	O
1558–1161	O
.	O
504	O
651	O
,	O
hinton	O
,	O
g.	O
e.	O
,	O
dayan	O
,	O
p.	O
,	O
and	O
revow	O
,	O
m.	O
(	O
1997	O
)	O
.	O
modelling	O
the	O
manifolds	O
of	O
images	O
of	O
handwritten	O
digits	O
.	O
ieee	O
transactions	O
on	O
neural	O
networks	O
,	O
8	O
,	O
65–74	O
.	O
499	O
hinton	O
,	O
g.	O
e.	O
,	O
welling	O
,	O
m.	O
,	O
teh	O
,	O
y.	O
w.	O
,	O
and	O
osindero	O
,	O
s.	O
(	O
2001	O
)	O
.	O
a	O
new	O
view	O
of	O
ica	O
.	O
in	O
proceedings	O
of	O
3rd	O
international	O
conference	O
on	O
independent	O
component	O
analysis	O
and	O
blind	O
signal	O
separation	O
(	O
ica	O
’	O
01	O
)	O
,	O
pages	O
746–751	O
,	O
san	O
diego	O
,	O
ca	O
.	O
491	O
hinton	O
,	O
g.	O
e.	O
,	O
osindero	O
,	O
s.	O
,	O
and	O
teh	O
,	O
y	O
.	O
(	O
2006	O
)	O
.	O
a	O
fast	O
learning	O
algorithm	O
for	O
deep	O
belief	O
nets	O
.	O
neural	O
computation	O
,	O
18	O
,	O
1527–1554	O
.	O
14	O
19	O
27	O
143	O
528	O
529	O
660	O
661	O
,	O
,	O
,	O
,	O
,	O
,	O
,	O
hinton	O
,	O
g.	O
e.	O
,	O
deng	O
,	O
l.	O
,	O
yu	O
,	O
d.	O
,	O
dahl	O
,	O
g.	O
e.	O
,	O
mohamed	O
,	O
a.	O
,	O
jaitly	O
,	O
n.	O
,	O
senior	O
,	O
a.	O
,	O
vanhoucke	O
,	O
v.	O
,	O
nguyen	O
,	O
p.	O
,	O
sainath	O
,	O
t.	O
n.	O
,	O
and	O
kingsbury	O
,	O
b	O
.	O
(	O
2012b	O
)	O
.	O
deep	O
neural	O
networks	O
for	O
acoustic	O
modeling	O
in	O
speech	O
recognition	B
:	O
the	O
shared	O
views	O
of	O
four	O
research	O
groups	O
.	O
ieee	O
signal	O
process	O
.	O
mag.	O
,	O
(	O
6	O
)	O
,	O
82–97	O
.	O
101	O
29	O
743	O
bibliography	O
hinton	O
,	O
g.	O
e.	O
,	O
srivastava	O
,	O
n.	O
,	O
krizhevsky	O
,	O
a.	O
,	O
sutskever	O
,	O
i.	O
,	O
and	O
salakhutdinov	O
,	O
r.	O
(	O
2012c	O
)	O
.	O
improving	O
neural	O
networks	O
by	O
preventing	O
co-adaptation	O
of	O
feature	O
detectors	O
.	O
technical	O
report	O
,	O
arxiv:1207.0580	O
.	O
238	O
263	O
267	O
,	O
,	O
hinton	O
,	O
g.	O
e.	O
,	O
vinyals	O
,	O
o.	O
,	O
and	O
dean	O
,	O
j	O
.	O
(	O
2014	O
)	O
.	O
dark	O
knowledge	O
.	O
invited	O
talk	O
at	O
the	O
baylearn	O
bay	O
area	O
machine	O
learning	O
symposium	O
.	O
448	O
hochreiter	O
,	O
s.	O
(	O
1991	O
)	O
.	O
untersuchungen	O
zu	O
dynamischen	O
neuronalen	O
netzen	O
.	O
diploma	O
thesis	O
,	O
t.u	O
.	O
münchen	O
.	O
18	O
401	O
403	O
,	O
,	O
hochreiter	O
,	O
s.	O
and	O
schmidhuber	O
,	O
j	O
.	O
(	O
1995	O
)	O
.	O
simplifying	O
neural	O
nets	O
by	O
discovering	O
ﬂat	O
minima	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
7	O
,	O
pages	O
529–536	O
.	O
mit	O
press	O
.	O
243	O
hochreiter	O
,	O
s.	O
and	O
schmidhuber	O
,	O
j	O
.	O
(	O
1997	O
)	O
.	O
long	O
short-term	O
memory	O
.	O
neural	O
computation	O
,	O
9	O
(	O
8	O
)	O
,	O
1735–1780	O
.	O
18	O
410	O
411	O
,	O
,	O
hochreiter	O
,	O
s.	O
,	O
bengio	O
,	O
y.	O
,	O
and	O
frasconi	O
,	O
p.	O
(	O
2001	O
)	O
.	O
gradient	O
ﬂow	O
in	O
recurrent	O
nets	O
:	O
the	O
diﬃculty	O
of	O
learning	O
long-term	O
dependencies	O
.	O
in	O
j.	O
kolen	O
and	O
s.	O
kremer	O
,	O
editors	O
,	O
field	O
guide	O
to	O
dynamical	O
recurrent	O
networks	O
.	O
ieee	O
press	O
.	O
411	O
holi	O
,	O
j.	O
l.	O
and	O
hwang	O
,	O
j.-n.	O
(	O
1993	O
)	O
.	O
finite	O
precision	O
error	O
analysis	O
of	O
neural	O
network	O
hardware	O
implementations	O
.	O
computers	O
,	O
ieee	O
transactions	O
on	O
,	O
42	O
(	O
3	O
)	O
,	O
281–290	O
.	O
451	O
holt	O
,	O
j.	O
l.	O
and	O
baker	O
,	O
t.	O
e.	O
(	O
1991	O
)	O
.	O
back	O
propagation	O
simulations	O
using	O
limited	O
preci-	O
sion	O
calculations	O
.	O
in	O
neural	O
networks	O
,	O
1991.	O
,	O
ijcnn-91-seattle	O
international	O
joint	O
conference	O
on	O
,	O
volume	O
2	O
,	O
pages	O
121–126	O
.	O
ieee	O
.	O
451	O
hornik	O
,	O
k.	O
,	O
stinchcombe	O
,	O
m.	O
,	O
and	O
white	O
,	O
h.	O
(	O
1989	O
)	O
.	O
multilayer	O
feedforward	O
networks	O
are	O
universal	O
approximators	O
.	O
neural	O
networks	O
,	O
2	O
,	O
359–366	O
.	O
198	O
hornik	O
,	O
k.	O
,	O
stinchcombe	O
,	O
m.	O
,	O
and	O
white	O
,	O
h.	O
(	O
1990	O
)	O
.	O
universal	O
approximation	O
of	O
an	O
unknown	O
mapping	O
and	O
its	O
derivatives	O
using	O
multilayer	O
feedforward	O
networks	O
.	O
neural	O
networks	O
,	O
(	O
5	O
)	O
,	O
551–560	O
.	O
198	O
3	O
hsu	O
,	O
f.-h.	O
(	O
2002	O
)	O
.	O
behind	O
deep	O
blue	O
:	O
building	O
the	O
computer	O
that	O
defeated	O
the	O
world	O
chess	O
champion	O
.	O
princeton	O
university	O
press	O
,	O
princeton	O
,	O
nj	O
,	O
usa	O
.	O
2	O
huang	O
,	O
f.	O
and	O
ogata	O
,	O
y	O
.	O
(	O
2002	O
)	O
.	O
generalized	O
pseudo-likelihood	O
estimates	O
for	O
markov	O
random	O
ﬁelds	O
on	O
lattice	O
.	O
annals	O
of	O
the	O
institute	O
of	O
statistical	O
mathematics	O
,	O
54	O
(	O
1	O
)	O
,	O
1–18	O
.	O
616	O
huang	O
,	O
p.-s.	O
,	O
he	O
,	O
x.	O
,	O
gao	O
,	O
j.	O
,	O
deng	O
,	O
l.	O
,	O
acero	O
,	O
a.	O
,	O
and	O
heck	O
,	O
l.	O
(	O
2013	O
)	O
.	O
learning	O
deep	O
structured	O
semantic	O
models	O
for	O
web	O
search	O
using	O
clickthrough	O
data	O
.	O
in	O
proceedings	O
of	O
the	O
22nd	O
acm	O
international	O
conference	O
on	O
conference	O
on	O
information	O
&	O
knowledge	O
management	O
,	O
pages	O
2333–2338	O
.	O
acm	O
.	O
480	O
hubel	O
,	O
d.	O
and	O
wiesel	O
,	O
t.	O
(	O
1968	O
)	O
.	O
receptive	O
ﬁelds	O
and	O
functional	O
architecture	O
of	O
monkey	O
striate	O
cortex	O
.	O
journal	O
of	O
physiology	O
(	O
london	O
)	O
,	O
195	O
,	O
215–243	O
.	O
364	O
744	O
bibliography	O
hubel	O
,	O
d.	O
h.	O
and	O
wiesel	O
,	O
t.	O
n.	O
(	O
1959	O
)	O
.	O
receptive	O
ﬁelds	O
of	O
single	O
neurons	O
in	O
the	O
cat	O
’	O
s	O
striate	O
cortex	O
.	O
journal	O
of	O
physiology	O
,	O
148	O
,	O
574–591	O
.	O
364	O
hubel	O
,	O
d.	O
h.	O
and	O
wiesel	O
,	O
t.	O
n.	O
(	O
1962	O
)	O
.	O
receptive	O
ﬁelds	O
,	O
binocular	O
interaction	O
,	O
and	O
functional	O
architecture	O
in	O
the	O
cat	O
’	O
s	O
visual	O
cortex	O
.	O
journal	O
of	O
physiology	O
(	O
london	O
)	O
,	O
160	O
,	O
106–154	O
.	O
364	O
huszar	O
,	O
f.	O
(	O
2015	O
)	O
.	O
how	O
(	O
not	O
)	O
to	O
train	O
your	O
generative	O
model	B
:	O
schedule	O
sampling	O
,	O
likelihood	O
,	O
adversary	O
?	O
arxiv:1511.05101	O
698	O
.	O
hutter	O
,	O
f.	O
,	O
hoos	O
,	O
h.	O
,	O
and	O
leyton-brown	O
,	O
k.	O
(	O
2011	O
)	O
.	O
sequential	O
model-based	O
optimization	O
.	O
extended	O
version	O
as	O
ubc	O
tech	O
report	O
lion-5	O
for	O
general	O
algorithm	O
conﬁguration	O
.	O
in	O
tr-2010-10	O
.	O
436	O
hyotyniemi	O
,	O
h.	O
(	O
1996	O
)	O
.	O
turing	O
machines	O
are	O
recurrent	O
neural	O
networks	O
.	O
in	O
step	O
’	O
96	O
,	O
pages	O
13–24	O
.	O
379	O
hyvärinen	O
,	O
a	O
.	O
(	O
1999	O
)	O
.	O
survey	O
on	O
independent	O
component	O
analysis	O
.	O
neural	O
computing	O
surveys	O
,	O
2	O
,	O
94–128	O
.	O
491	O
hyvärinen	O
,	O
a	O
.	O
(	O
2005	O
)	O
.	O
estimation	O
of	O
non-normalized	O
statistical	O
models	O
using	O
score	O
matching	O
.	O
journal	O
of	O
machine	O
learning	O
research	O
,	O
6	O
,	O
695–709	O
.	O
513	O
617	O
,	O
hyvärinen	O
,	O
a	O
.	O
(	O
2007a	O
)	O
.	O
connections	O
between	O
score	O
matching	O
,	O
contrastive	O
divergence	O
,	O
and	O
pseudolikelihood	O
for	O
continuous-valued	O
variables	O
.	O
ieee	O
transactions	O
on	O
neural	O
networks	O
,	O
,	O
1529–1531	O
.	O
618	O
18	O
hyvärinen	O
,	O
a	O
.	O
(	O
2007b	O
)	O
.	O
some	O
extensions	O
of	O
score	O
matching	O
.	O
computational	O
statistics	O
and	O
data	O
analysis	O
,	O
51	O
,	O
2499–2512	O
.	O
618	O
hyvärinen	O
,	O
a.	O
and	O
hoyer	O
,	O
p.	O
o	O
.	O
(	O
1999	O
)	O
.	O
emergence	O
of	O
topography	O
and	O
complex	O
cell	O
493	O
properties	O
from	O
natural	O
images	O
using	O
extensions	O
of	O
ica	O
.	O
in	O
,	O
pages	O
827–833	O
.	O
nips	O
hyvärinen	O
,	O
a.	O
and	O
pajunen	O
,	O
p.	O
(	O
1999	O
)	O
.	O
nonlinear	O
independent	O
component	O
analysis	O
:	O
existence	O
and	O
uniqueness	O
results	O
.	O
neural	O
networks	O
,	O
12	O
(	O
3	O
)	O
,	O
429–439	O
.	O
493	O
hyvärinen	O
,	O
a.	O
,	O
karhunen	O
,	O
j.	O
,	O
and	O
oja	O
,	O
e.	O
(	O
2001a	O
)	O
.	O
independent	O
component	O
analysis	O
.	O
wiley-interscience	O
.	O
491	O
hyvärinen	O
,	O
a.	O
,	O
hoyer	O
,	O
p.	O
o.	O
,	O
and	O
inki	O
,	O
m.	O
o	O
.	O
(	O
2001b	O
)	O
.	O
topographic	O
independent	O
component	O
analysis	O
.	O
neural	O
computation	O
,	O
13	O
(	O
7	O
)	O
,	O
1527–1558	O
.	O
493	O
hyvärinen	O
,	O
a.	O
,	O
hurri	O
,	O
j.	O
,	O
and	O
hoyer	O
,	O
p.	O
o	O
.	O
(	O
2009	O
)	O
.	O
natural	O
image	O
statistics	O
:	O
a	O
probabilistic	O
approach	O
to	O
early	O
computational	O
vision	O
.	O
springer-verlag	O
.	O
370	O
iba	O
,	O
y	O
.	O
(	O
2001	O
)	O
.	O
extended	O
ensemble	O
monte	O
carlo	O
.	O
international	O
journal	O
of	O
modern	O
physics	O
,	O
c12	O
,	O
623–656	O
.	O
603	O
745	O
bibliography	O
inayoshi	O
,	O
h.	O
and	O
kurita	O
,	O
t.	O
(	O
2005	O
)	O
.	O
improved	O
generalization	O
by	O
adding	O
both	O
auto-	O
association	O
and	O
hidden-layer	O
noise	O
to	O
neural-network-based-classiﬁers	O
.	O
ieee	O
workshop	O
on	O
machine	O
learning	O
for	O
signal	O
processing	O
,	O
pages	O
141—-146	O
.	O
515	O
ioﬀe	O
,	O
s.	O
and	O
szegedy	O
,	O
c.	O
(	O
2015	O
)	O
.	O
batch	O
normalization	O
:	O
accelerating	O
deep	O
network	O
training	O
by	O
reducing	O
internal	O
covariate	O
shift	O
.	O
100	O
317	O
320	O
,	O
,	O
jacobs	O
,	O
r.	O
a	O
.	O
(	O
1988	O
)	O
.	O
increased	O
rates	O
of	O
convergence	O
through	O
learning	O
rate	O
adaptation	O
.	O
neural	O
networks	O
,	O
1	O
(	O
4	O
)	O
,	O
295–307	O
.	O
307	O
jacobs	O
,	O
r.	O
a.	O
,	O
jordan	O
,	O
m.	O
i.	O
,	O
nowlan	O
,	O
s.	O
j.	O
,	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
1991	O
)	O
.	O
adaptive	O
mixtures	O
of	O
local	O
experts	O
.	O
neural	O
computation	O
,	O
3	O
,	O
79–87	O
.	O
189	O
450	O
,	O
jaeger	O
,	O
h.	O
(	O
2003	O
)	O
.	O
adaptive	O
nonlinear	O
system	O
identiﬁcation	O
with	O
echo	O
state	O
networks	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
15	O
.	O
404	O
jaeger	O
,	O
h.	O
(	O
2007a	O
)	O
.	O
discovering	O
multiscale	O
dynamical	O
features	O
with	O
hierarchical	O
echo	O
state	O
networks	O
.	O
technical	O
report	O
,	O
jacobs	O
university	O
.	O
398	O
jaeger	O
,	O
h.	O
(	O
2007b	O
)	O
.	O
echo	O
state	O
network	O
.	O
scholarpedia	O
,	O
2	O
(	O
9	O
)	O
,	O
2330	O
.	O
404	O
jaeger	O
,	O
h.	O
(	O
2012	O
)	O
.	O
long	O
short-term	O
memory	O
in	O
echo	O
state	O
networks	O
:	O
details	O
of	O
a	O
simulation	O
study	O
.	O
technical	O
report	O
,	O
technical	O
report	O
,	O
jacobs	O
university	O
bremen	O
.	O
405	O
jaeger	O
,	O
h.	O
and	O
haas	O
,	O
h.	O
(	O
2004	O
)	O
.	O
harnessing	O
nonlinearity	O
:	O
predicting	O
chaotic	O
systems	O
and	O
saving	O
energy	O
in	O
wireless	O
communication	O
.	O
science	O
,	O
304	O
(	O
5667	O
)	O
,	O
78–80	O
.	O
27	O
404	O
,	O
jaeger	O
,	O
h.	O
,	O
lukosevicius	O
,	O
m.	O
,	O
popovici	O
,	O
d.	O
,	O
and	O
siewert	O
,	O
u	O
.	O
(	O
2007	O
)	O
.	O
optimization	O
and	O
applications	O
of	O
echo	O
state	O
networks	O
with	O
leaky-	O
integrator	O
neurons	O
.	O
neural	O
networks	O
,	O
20	O
(	O
3	O
)	O
,	O
335–352	O
.	O
407	O
jain	O
,	O
v.	O
,	O
murray	O
,	O
j.	O
f.	O
,	O
roth	O
,	O
f.	O
,	O
turaga	O
,	O
s.	O
,	O
zhigulin	O
,	O
v.	O
,	O
briggman	O
,	O
k.	O
l.	O
,	O
helmstaedter	O
,	O
m.	O
n.	O
,	O
denk	O
,	O
w.	O
,	O
and	O
seung	O
,	O
h.	O
s.	O
(	O
2007	O
)	O
.	O
supervised	O
learning	O
of	O
image	O
restoration	O
with	O
convolutional	O
networks	O
.	O
in	O
computer	O
vision	O
,	O
2007.	O
iccv	O
2007.	O
ieee	O
11th	O
international	O
conference	O
on	O
,	O
pages	O
1–8	O
.	O
ieee	O
.	O
359	O
jaitly	O
,	O
n.	O
and	O
hinton	O
,	O
g.	O
(	O
2011	O
)	O
.	O
learning	O
a	O
better	O
representation	O
of	O
speech	O
soundwaves	O
in	O
acoustics	O
,	O
speech	O
and	O
signal	O
processing	O
using	O
restricted	O
boltzmann	O
machines	O
.	O
(	O
icassp	O
)	O
,	O
2011	O
ieee	O
international	O
conference	O
on	O
,	O
pages	O
5884–5887	O
.	O
ieee	O
.	O
458	O
jaitly	O
,	O
n.	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
2013	O
)	O
.	O
vocal	O
tract	O
length	O
perturbation	O
(	O
vtlp	O
)	O
improves	O
speech	O
recognition	B
.	O
in	O
icml	O
’	O
2013	O
241	O
.	O
jarrett	O
,	O
k.	O
,	O
kavukcuoglu	O
,	O
k.	O
,	O
ranzato	O
,	O
m.	O
,	O
and	O
lecun	O
,	O
y	O
.	O
(	O
2009	O
)	O
.	O
what	O
is	O
the	O
best	O
iccv	O
’	O
09	O
16	O
24	O
27	O
174	O
193	O
226	O
,	O
multi-stage	O
architecture	O
for	O
object	O
recognition	B
?	O
in	O
363	O
364	O
523	O
,	O
,	O
.	O
,	O
,	O
,	O
,	O
,	O
jarzynski	O
,	O
c.	O
(	O
1997	O
)	O
.	O
nonequilibrium	O
equality	O
for	O
free	O
energy	O
diﬀerences	O
.	O
phys	O
.	O
rev	O
.	O
lett.	O
,	O
78	O
,	O
2690–2693	O
.	O
625	O
628	O
,	O
746	O
bibliography	O
jaynes	O
,	O
e.	O
t.	O
(	O
2003	O
)	O
.	O
probability	O
theory	O
:	O
the	O
logic	O
of	O
science	O
.	O
cambridge	O
university	O
press	O
.	O
53	O
jean	O
,	O
s.	O
,	O
cho	O
,	O
k.	O
,	O
memisevic	O
,	O
r.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2014	O
)	O
.	O
on	O
using	O
very	O
large	O
target	O
vocabulary	O
for	O
neural	O
machine	O
translation	O
.	O
arxiv:1412.2007	O
.	O
474	O
475	O
,	O
jelinek	O
,	O
f.	O
and	O
mercer	O
,	O
r.	O
l.	O
(	O
1980	O
)	O
.	O
interpolated	O
estimation	O
of	O
markov	O
source	O
parameters	O
from	O
sparse	O
data	O
.	O
in	O
e.	O
s.	O
gelsema	O
and	O
l.	O
n.	O
kanal	O
,	O
editors	O
,	O
pattern	O
recognition	B
in	O
practice	O
.	O
north-holland	O
,	O
amsterdam	O
.	O
462	O
473	O
,	O
jia	O
,	O
y	O
.	O
(	O
2013	O
)	O
.	O
caﬀe	O
:	O
an	O
open	O
source	O
convolutional	O
architecture	O
for	O
fast	O
feature	O
embedding	O
.	O
http	O
:	O
//caffe.berkeleyvision.org/	O
.	O
,25	O
214	O
jia	O
,	O
y.	O
,	O
huang	O
,	O
c.	O
,	O
and	O
darrell	O
,	O
t.	O
(	O
2012	O
)	O
.	O
beyond	O
spatial	O
pyramids	O
:	O
receptive	O
ﬁeld	O
in	O
computer	O
vision	O
and	O
pattern	O
recognition	B
learning	O
for	O
pooled	O
image	O
features	O
.	O
(	O
cvpr	O
)	O
,	O
2012	O
ieee	O
conference	O
on	O
,	O
pages	O
3370–3377	O
.	O
ieee	O
.	O
345	O
jim	O
,	O
k.-c.	O
,	O
giles	O
,	O
c.	O
l.	O
,	O
and	O
horne	O
,	O
b.	O
g.	O
(	O
1996	O
)	O
.	O
an	O
analysis	O
of	O
noise	O
in	O
recurrent	O
neural	O
networks	O
:	O
convergence	O
and	O
generalization	O
.	O
ieee	O
transactions	O
on	O
neural	O
networks	O
,	O
7	O
(	O
6	O
)	O
,	O
1424–1438	O
.	O
242	O
jordan	O
,	O
m.	O
i	O
.	O
(	O
1998	O
)	O
.	O
learning	O
in	O
graphical	O
models	O
.	O
kluwer	O
,	O
dordrecht	O
,	O
netherlands	O
.	O
18	O
joulin	O
,	O
a.	O
and	O
mikolov	O
,	O
t.	O
(	O
2015	O
)	O
.	O
inferring	O
algorithmic	O
patterns	O
with	O
stack-augmented	O
recurrent	O
nets	O
.	O
arxiv	O
preprint	O
arxiv:1503.01007	O
.	O
418	O
jozefowicz	O
,	O
r.	O
,	O
zaremba	O
,	O
w.	O
,	O
and	O
sutskever	O
,	O
i	O
.	O
(	O
2015	O
)	O
.	O
an	O
empirical	O
evaluation	O
of	O
recurrent	O
network	O
architectures	O
.	O
in	O
icml	O
’	O
2015	O
306	O
412	O
.	O
,	O
judd	O
,	O
j.	O
s.	O
(	O
1989	O
)	O
.	O
neural	O
network	O
design	O
and	O
the	O
complexity	O
of	O
learning	O
.	O
mit	O
press	O
.	O
293	O
jutten	O
,	O
c.	O
and	O
herault	O
,	O
j	O
.	O
(	O
1991	O
)	O
.	O
blind	O
separation	O
of	O
sources	O
,	O
part	O
i	O
:	O
an	O
adaptive	O
algorithm	O
based	O
on	O
neuromimetic	O
architecture	O
.	O
signal	O
processing	O
,	O
24	O
,	O
1–10	O
.	O
491	O
kahou	O
,	O
s.	O
e.	O
,	O
pal	O
,	O
c.	O
,	O
bouthillier	O
,	O
x.	O
,	O
froumenty	O
,	O
p.	O
,	O
gülçehre	O
,	O
c.	O
,	O
memisevic	O
,	O
r.	O
,	O
vincent	O
,	O
p.	O
,	O
courville	O
,	O
a.	O
,	O
bengio	O
,	O
y.	O
,	O
ferrari	O
,	O
r.	O
c.	O
,	O
mirza	O
,	O
m.	O
,	O
jean	O
,	O
s.	O
,	O
carrier	O
,	O
p.	O
l.	O
,	O
dauphin	O
,	O
y.	O
,	O
boulanger-lewandowski	O
,	O
n.	O
,	O
aggarwal	O
,	O
a.	O
,	O
zumer	O
,	O
j.	O
,	O
lamblin	O
,	O
p.	O
,	O
raymond	O
,	O
j.-p.	O
,	O
desjardins	O
,	O
g.	O
,	O
pascanu	O
,	O
r.	O
,	O
warde-farley	O
,	O
d.	O
,	O
torabi	O
,	O
a.	O
,	O
sharma	O
,	O
a.	O
,	O
bengio	O
,	O
e.	O
,	O
côté	O
,	O
m.	O
,	O
konda	O
,	O
k.	O
r.	O
,	O
and	O
wu	O
,	O
z	O
.	O
(	O
2013	O
)	O
.	O
combining	O
modality	O
speciﬁc	O
deep	O
neural	O
networks	O
for	O
emotion	O
recognition	B
in	O
video	O
.	O
in	O
proceedings	O
of	O
the	O
15th	O
acm	O
on	O
international	O
conference	O
on	O
multimodal	O
interaction	O
.	O
201	O
kalchbrenner	O
,	O
n.	O
and	O
blunsom	O
,	O
p.	O
(	O
2013	O
)	O
.	O
recurrent	O
continuous	O
translation	O
models	O
.	O
in	O
emnlp	O
’	O
2013	O
.	O
474	O
475	O
,	O
kalchbrenner	O
,	O
n.	O
,	O
danihelka	O
,	O
i.	O
,	O
and	O
graves	O
,	O
a	O
.	O
(	O
2015	O
)	O
.	O
grid	O
long	O
short-term	O
memory	O
.	O
arxiv	O
preprint	O
arxiv:1507.01526	O
.	O
395	O
747	O
bibliography	O
kamyshanska	O
,	O
h.	O
and	O
memisevic	O
,	O
r.	O
(	O
2015	O
)	O
.	O
the	O
potential	O
energy	O
of	O
an	O
autoencoder	O
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
.	O
515	O
karpathy	O
,	O
a.	O
and	O
li	O
,	O
f.-f.	O
(	O
2015	O
)	O
.	O
deep	O
visual-semantic	O
alignments	O
for	O
generating	O
image	O
descriptions	O
.	O
in	O
cvpr	O
’	O
2015	O
.	O
arxiv:1412.2306	O
.	O
102	O
karpathy	O
,	O
a.	O
,	O
toderici	O
,	O
g.	O
,	O
shetty	O
,	O
s.	O
,	O
leung	O
,	O
t.	O
,	O
sukthankar	O
,	O
r.	O
,	O
and	O
fei-fei	O
,	O
l.	O
(	O
2014	O
)	O
.	O
large-scale	O
video	O
classiﬁcation	O
with	O
convolutional	O
neural	O
networks	O
.	O
in	O
cvpr	O
21	O
.	O
karush	O
,	O
w.	O
(	O
1939	O
)	O
.	O
minima	O
of	O
functions	O
of	O
several	O
variables	O
with	O
inequalities	O
as	O
side	O
constraints	O
.	O
master	O
’	O
s	O
thesis	O
,	O
dept	O
.	O
of	O
mathematics	O
,	O
univ	O
.	O
of	O
chicago	O
.	O
95	O
katz	O
,	O
s.	O
m.	O
(	O
1987	O
)	O
.	O
estimation	O
of	O
probabilities	O
from	O
sparse	O
data	O
for	O
the	O
language	O
model	B
component	O
of	O
a	O
speech	O
recognizer	O
.	O
ieee	O
transactions	O
on	O
acoustics	O
,	O
speech	O
,	O
and	O
signal	O
processing	O
,	O
(	O
3	O
)	O
,	O
400–401	O
.	O
assp-35	O
462	O
473	O
,	O
kavukcuoglu	O
,	O
k.	O
,	O
ranzato	O
,	O
m.	O
,	O
and	O
lecun	O
,	O
y	O
.	O
(	O
2008	O
)	O
.	O
fast	O
inference	O
in	O
sparse	O
coding	O
algorithms	O
with	O
applications	O
to	O
object	O
recognition	B
.	O
technical	O
report	O
,	O
computational	O
and	O
biological	O
learning	O
lab	O
,	O
courant	O
institute	O
,	O
nyu	O
.	O
tech	O
report	O
cbll-tr-2008-12-01	O
.	O
523	O
kavukcuoglu	O
,	O
k.	O
,	O
ranzato	O
,	O
m.-a.	O
,	O
fergus	O
,	O
r.	O
,	O
and	O
lecun	O
,	O
y	O
.	O
(	O
2009	O
)	O
.	O
learning	O
invariant	O
features	O
through	O
topographic	O
ﬁlter	O
maps	O
.	O
in	O
cvpr	O
’	O
2009	O
523	O
.	O
kavukcuoglu	O
,	O
k.	O
,	O
sermanet	O
,	O
p.	O
,	O
boureau	O
,	O
y.-l.	O
,	O
gregor	O
,	O
k.	O
,	O
mathieu	O
,	O
m.	O
,	O
and	O
lecun	O
,	O
y.	O
.	O
nips	O
’	O
2010	O
(	O
2010	O
)	O
.	O
learning	O
convolutional	O
feature	O
hierarchies	O
for	O
visual	O
recognition	B
.	O
in	O
364	O
523	O
,	O
kelley	O
,	O
h.	O
j	O
.	O
(	O
1960	O
)	O
.	O
gradient	O
theory	O
of	O
optimal	O
ﬂight	O
paths	O
.	O
ars	O
journal	O
30	O
(	O
10	O
)	O
,	O
,	O
947–954	O
.	O
225	O
khan	O
,	O
f.	O
,	O
zhu	O
,	O
x.	O
,	O
and	O
mutlu	O
,	O
b	O
.	O
(	O
2011	O
)	O
.	O
how	O
do	O
humans	O
teach	O
:	O
on	O
curriculum	O
learning	O
and	O
teaching	O
dimension	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
24	O
(	O
nips	O
’	O
11	O
)	O
,	O
pages	O
1449–1457	O
.	O
328	O
kim	O
,	O
s.	O
k.	O
,	O
mcafee	O
,	O
l.	O
c.	O
,	O
mcmahon	O
,	O
p.	O
l.	O
,	O
and	O
olukotun	O
,	O
k.	O
(	O
2009	O
)	O
.	O
a	O
highly	O
scalable	O
restricted	O
boltzmann	O
machine	O
fpga	O
implementation	O
.	O
in	O
field	O
programmable	O
logic	O
and	O
applications	O
,	O
2009.	O
fpl	O
2009.	O
international	O
conference	O
on	O
,	O
pages	O
367–372	O
.	O
ieee	O
.	O
451	O
kindermann	O
,	O
r.	O
(	O
1980	O
)	O
.	O
markov	O
random	O
fields	O
and	O
their	O
applications	O
(	O
contemporary	O
mathematics	O
;	O
v.	O
1	O
)	O
.	O
american	O
mathematical	O
society	O
.	O
566	O
kingma	O
,	O
d.	O
and	O
ba	O
,	O
j	O
.	O
(	O
2014	O
)	O
.	O
adam	O
:	O
a	O
method	O
for	O
stochastic	O
optimization	O
.	O
arxiv	O
preprint	O
arxiv:1412.6980	O
.	O
308	O
kingma	O
,	O
d.	O
and	O
lecun	O
,	O
y	O
.	O
(	O
2010	O
)	O
.	O
regularized	O
estimation	O
of	O
image	O
statistics	O
by	O
score	O
matching	O
.	O
in	O
nips	O
’	O
2010	O
513	O
620	O
.	O
,	O
748	O
bibliography	O
kingma	O
,	O
d.	O
,	O
rezende	O
,	O
d.	O
,	O
mohamed	O
,	O
s.	O
,	O
and	O
welling	O
,	O
m.	O
(	O
2014	O
)	O
.	O
semi-supervised	O
learning	O
with	O
deep	O
generative	O
models	O
.	O
in	O
nips	O
’	O
2014	O
426	O
.	O
kingma	O
,	O
d.	O
p.	O
(	O
2013	O
)	O
.	O
fast	O
gradient-based	O
inference	O
with	O
continuous	O
latent	O
variable	O
models	O
in	O
auxiliary	O
form	O
.	O
technical	O
report	O
,	O
arxiv:1306.0733	O
.	O
652	O
689	O
696	O
,	O
,	O
kingma	O
,	O
d.	O
p.	O
and	O
welling	O
,	O
m.	O
(	O
2014a	O
)	O
.	O
auto-encoding	O
variational	O
bayes	O
.	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
learning	O
representations	O
(	O
iclr	O
)	O
.	O
689	O
700	O
,	O
kingma	O
,	O
d.	O
p.	O
and	O
welling	O
,	O
m.	O
(	O
2014b	O
)	O
.	O
eﬃcient	O
gradient-based	O
inference	O
through	O
transformations	O
between	O
bayes	O
nets	O
and	O
neural	O
nets	O
.	O
technical	O
report	O
,	O
arxiv:1402.0480	O
.	O
689	O
kirkpatrick	O
,	O
s.	O
,	O
jr.	O
,	O
c.	O
d.	O
g.	O
,	O
,	O
and	O
vecchi	O
,	O
m.	O
p.	O
(	O
1983	O
)	O
.	O
optimization	O
by	O
simulated	O
annealing	O
.	O
science	O
,	O
220	O
,	O
671–680	O
.	O
327	O
kiros	O
,	O
r.	O
,	O
salakhutdinov	O
,	O
r.	O
,	O
and	O
zemel	O
,	O
r.	O
(	O
2014a	O
)	O
.	O
multimodal	O
neural	O
language	O
models	O
.	O
in	O
icml	O
’	O
2014	O
102	O
.	O
kiros	O
,	O
r.	O
,	O
salakhutdinov	O
,	O
r.	O
,	O
and	O
zemel	O
,	O
r.	O
(	O
2014b	O
)	O
.	O
unifying	O
visual-semantic	O
embeddings	O
with	O
multimodal	O
neural	O
language	O
models	O
.	O
arxiv:1411.2539	O
[	O
cs.lg	O
]	O
102	O
410	O
.	O
,	O
klementiev	O
,	O
a.	O
,	O
titov	O
,	O
i.	O
,	O
and	O
bhattarai	O
,	O
b	O
.	O
(	O
2012	O
)	O
.	O
inducing	O
crosslingual	O
distributed	O
representations	O
of	O
words	O
.	O
in	O
proceedings	O
of	O
coling	O
2012	O
.	O
476	O
539	O
,	O
knowles-barley	O
,	O
s.	O
,	O
jones	O
,	O
t.	O
r.	O
,	O
morgan	O
,	O
j.	O
,	O
lee	O
,	O
d.	O
,	O
kasthuri	O
,	O
n.	O
,	O
lichtman	O
,	O
j.	O
w.	O
,	O
and	O
pﬁster	O
,	O
h.	O
(	O
2014	O
)	O
.	O
deep	O
learning	O
for	O
the	O
connectome	O
.	O
gpu	O
technology	O
conference	O
.	O
26	O
koller	O
,	O
d.	O
and	O
friedman	O
,	O
n.	O
(	O
2009	O
)	O
.	O
probabilistic	O
graphical	O
models	O
:	O
principles	O
and	O
techniques	O
.	O
mit	O
press	O
.	O
583	O
595	O
645	O
,	O
,	O
konig	O
,	O
y.	O
,	O
bourlard	O
,	O
h.	O
,	O
and	O
morgan	O
,	O
n.	O
(	O
1996	O
)	O
.	O
remap	O
:	O
recursive	O
estimation	O
and	O
maximization	O
of	O
a	O
posteriori	O
probabilities	O
–	O
application	O
to	O
transition-based	O
connectionist	O
speech	O
recognition	B
.	O
in	O
d.	O
touretzky	O
,	O
m.	O
mozer	O
,	O
and	O
m.	O
hasselmo	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
8	O
(	O
nips	O
’	O
95	O
)	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
459	O
koren	O
,	O
y	O
.	O
(	O
2009	O
)	O
.	O
the	O
bellkor	O
solution	O
to	O
the	O
netﬂix	O
grand	O
prize	O
.	O
258	O
480	O
,	O
kotzias	O
,	O
d.	O
,	O
denil	O
,	O
m.	O
,	O
de	O
freitas	O
,	O
n.	O
,	O
and	O
smyth	O
,	O
p.	O
(	O
2015	O
)	O
.	O
from	O
group	O
to	O
individual	O
labels	O
using	O
deep	O
features	O
.	O
in	O
acm	O
sigkdd	O
106	O
.	O
koutnik	O
,	O
j.	O
,	O
greﬀ	O
,	O
k.	O
,	O
gomez	O
,	O
f.	O
,	O
and	O
schmidhuber	O
,	O
j	O
.	O
(	O
2014	O
)	O
.	O
a	O
clockwork	O
rnn	O
.	O
in	O
icml	O
’	O
2014	O
.	O
408	O
kočiský	O
,	O
t.	O
,	O
hermann	O
,	O
k.	O
m.	O
,	O
and	O
blunsom	O
,	O
p.	O
(	O
2014	O
)	O
.	O
learning	O
bilingual	O
word	O
repre-	O
sentations	O
by	O
marginalizing	O
alignments	O
.	O
in	O
proceedings	O
of	O
acl	O
.	O
476	O
krause	O
,	O
o.	O
,	O
fischer	O
,	O
a.	O
,	O
glasmachers	O
,	O
t.	O
,	O
and	O
igel	O
,	O
c.	O
(	O
2013	O
)	O
.	O
approximation	O
properties	O
of	O
dbns	O
with	O
binary	O
hidden	O
units	O
and	O
real-valued	O
visible	O
units	O
.	O
in	O
icml	O
’	O
2013	O
553	O
.	O
749	O
bibliography	O
krizhevsky	O
,	O
a	O
.	O
(	O
2010	O
)	O
.	O
convolutional	O
deep	O
belief	O
networks	O
on	O
cifar-10	O
.	O
technical	O
report	O
,	O
university	O
of	O
toronto	O
.	O
unpublished	O
manuscript	O
:	O
http	O
:	O
//www.cs.utoronto.ca/	O
kriz/conv-	O
cifar10-aug2010.pdf	O
.	O
446	O
krizhevsky	O
,	O
a.	O
and	O
hinton	O
,	O
g.	O
(	O
2009	O
)	O
.	O
learning	O
multiple	O
layers	O
of	O
features	O
from	O
tiny	O
images	O
.	O
technical	O
report	O
,	O
university	O
of	O
toronto	O
.	O
,21	O
561	O
krizhevsky	O
,	O
a.	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
2011	O
)	O
.	O
using	O
very	O
deep	O
autoencoders	O
for	O
content-based	O
image	O
retrieval	O
.	O
in	O
esann	O
525	O
.	O
krizhevsky	O
,	O
a.	O
,	O
sutskever	O
,	O
i.	O
,	O
and	O
hinton	O
,	O
g.	O
(	O
2012	O
)	O
.	O
imagenet	O
classiﬁcation	O
with	O
deep	O
convolutional	O
neural	O
networks	O
.	O
in	O
nips	O
’	O
2012	O
23	O
24	O
27	O
100	O
201	O
371	O
454	O
458	O
.	O
,	O
,	O
,	O
,	O
,	O
,	O
,	O
krueger	O
,	O
k.	O
a.	O
and	O
dayan	O
,	O
p.	O
(	O
2009	O
)	O
.	O
flexible	O
shaping	O
:	O
how	O
learning	O
in	O
small	O
steps	O
helps	O
.	O
cognition	O
,	O
110	O
,	O
380–394	O
.	O
328	O
kuhn	O
,	O
h.	O
w.	O
and	O
tucker	O
,	O
a.	O
w.	O
(	O
1951	O
)	O
.	O
nonlinear	O
programming	O
.	O
in	O
proceedings	O
of	O
the	O
second	O
berkeley	O
symposium	O
on	O
mathematical	O
statistics	O
and	O
probability	O
,	O
pages	O
481–492	O
,	O
berkeley	O
,	O
calif.	O
university	O
of	O
california	O
press	O
.	O
95	O
kumar	O
,	O
a.	O
,	O
irsoy	O
,	O
o.	O
,	O
su	O
,	O
j.	O
,	O
bradbury	O
,	O
j.	O
,	O
english	O
,	O
r.	O
,	O
pierce	O
,	O
b.	O
,	O
ondruska	O
,	O
p.	O
,	O
iyyer	O
,	O
m.	O
,	O
gulrajani	O
,	O
i.	O
,	O
and	O
socher	O
,	O
r.	O
(	O
2015	O
)	O
.	O
ask	O
me	O
anything	O
:	O
dynamic	O
memory	O
networks	O
for	O
natural	O
language	O
processing	O
.	O
arxiv:1506.07285	O
418	O
485	O
.	O
,	O
kumar	O
,	O
m.	O
p.	O
,	O
packer	O
,	O
b.	O
,	O
and	O
koller	O
,	O
d.	O
(	O
2010	O
)	O
.	O
self-paced	O
learning	O
for	O
latent	O
variable	O
models	O
.	O
in	O
nips	O
’	O
2010	O
328	O
.	O
lang	O
,	O
k.	O
j.	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
1988	O
)	O
.	O
the	O
development	O
of	O
the	O
time-delay	O
neural	O
network	O
architecture	O
for	O
speech	O
recognition	B
.	O
technical	O
report	O
cmu-cs-88-152	O
,	O
carnegie-mellon	O
university	O
.	O
367	O
374	O
407	O
,	O
,	O
lang	O
,	O
k.	O
j.	O
,	O
waibel	O
,	O
a.	O
h.	O
,	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
1990	O
)	O
.	O
a	O
time-delay	O
neural	O
network	O
architecture	O
for	O
isolated	O
word	O
recognition	B
.	O
neural	O
networks	O
,	O
3	O
(	O
1	O
)	O
,	O
23–43	O
.	O
374	O
langford	O
,	O
j.	O
and	O
zhang	O
,	O
t.	O
(	O
2008	O
)	O
.	O
the	O
epoch-greedy	O
algorithm	O
for	O
contextual	O
multi-armed	O
bandits	O
.	O
in	O
nips	O
’	O
2008	O
,	O
pages	O
1096––1103	O
.	O
480	O
lappalainen	O
,	O
h.	O
,	O
giannakopoulos	O
,	O
x.	O
,	O
honkela	O
,	O
a.	O
,	O
and	O
karhunen	O
,	O
j	O
.	O
(	O
2000	O
)	O
.	O
nonlinear	O
independent	O
component	O
analysis	O
using	O
ensemble	O
learning	O
:	O
experiments	O
and	O
discussion	O
.	O
in	O
proc	O
.	O
ica	O
.	O
citeseer	O
.	O
493	O
larochelle	O
,	O
h.	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2008	O
)	O
.	O
classiﬁcation	O
using	O
discriminative	O
restricted	O
boltzmann	O
machines	O
.	O
in	O
icml	O
’	O
2008	O
244	O
255	O
530	O
686	O
716	O
.	O
,	O
,	O
,	O
,	O
larochelle	O
,	O
h.	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
2010	O
)	O
.	O
learning	O
to	O
combine	O
foveal	O
glimpses	O
with	O
a	O
third-order	O
boltzmann	O
machine	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
23	O
,	O
pages	O
1243–1251	O
.	O
367	O
750	O
bibliography	O
larochelle	O
,	O
h.	O
and	O
murray	O
,	O
i	O
.	O
(	O
2011	O
)	O
.	O
the	O
neural	O
autoregressive	O
distribution	O
estimator	O
.	O
in	O
aistats	O
’	O
2011	O
.	O
705	O
708	O
709	O
,	O
,	O
larochelle	O
,	O
h.	O
,	O
erhan	O
,	O
d.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2008	O
)	O
.	O
zero-data	O
learning	O
of	O
new	O
tasks	O
.	O
in	O
aaai	O
conference	O
on	O
artiﬁcial	O
intelligence	O
.	O
539	O
larochelle	O
,	O
h.	O
,	O
bengio	O
,	O
y.	O
,	O
louradour	O
,	O
j.	O
,	O
and	O
lamblin	O
,	O
p.	O
(	O
2009	O
)	O
.	O
exploring	O
strategies	O
for	O
535	O
training	O
deep	O
neural	O
networks	O
.	O
journal	O
of	O
machine	O
learning	O
research	O
,	O
,	O
1–40	O
.	O
10	O
lasserre	O
,	O
j.	O
a.	O
,	O
bishop	O
,	O
c.	O
m.	O
,	O
and	O
minka	O
,	O
t.	O
p.	O
(	O
2006	O
)	O
.	O
principled	O
hybrids	O
of	O
generative	O
and	O
discriminative	O
models	O
.	O
in	O
proceedings	O
of	O
the	O
computer	O
vision	O
and	O
pattern	O
recognition	B
conference	O
(	O
cvpr	O
’	O
06	O
)	O
,	O
pages	O
87–94	O
,	O
washington	O
,	O
dc	O
,	O
usa	O
.	O
ieee	O
computer	O
society	O
.	O
244	O
253	O
,	O
le	O
,	O
q.	O
,	O
ngiam	O
,	O
j.	O
,	O
chen	O
,	O
z.	O
,	O
hao	O
chia	O
,	O
d.	O
j.	O
,	O
koh	O
,	O
p.	O
w.	O
,	O
and	O
ng	O
,	O
a	O
.	O
(	O
2010	O
)	O
.	O
tiled	O
in	O
j.	O
laﬀerty	O
,	O
c.	O
k.	O
i.	O
williams	O
,	O
j.	O
shawe-taylor	O
,	O
convolutional	O
neural	O
networks	O
.	O
r.	O
zemel	O
,	O
and	O
a.	O
culotta	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
23	O
(	O
nips	O
’	O
10	O
)	O
,	O
pages	O
1279–1287	O
.	O
352	O
le	O
,	O
q.	O
,	O
ngiam	O
,	O
j.	O
,	O
coates	O
,	O
a.	O
,	O
lahiri	O
,	O
a.	O
,	O
prochnow	O
,	O
b.	O
,	O
and	O
ng	O
,	O
a	O
.	O
(	O
2011	O
)	O
.	O
on	O
optimization	O
methods	O
for	O
deep	O
learning	O
.	O
in	O
proc	O
.	O
icml	O
’	O
2011	O
.	O
acm	O
.	O
316	O
le	O
,	O
q.	O
,	O
ranzato	O
,	O
m.	O
,	O
monga	O
,	O
r.	O
,	O
devin	O
,	O
m.	O
,	O
corrado	O
,	O
g.	O
,	O
chen	O
,	O
k.	O
,	O
dean	O
,	O
j.	O
,	O
and	O
ng	O
,	O
a	O
.	O
(	O
2012	O
)	O
.	O
building	O
high-level	O
features	O
using	O
large	O
scale	O
unsupervised	O
learning	O
.	O
in	O
icml	O
’	O
2012	O
.	O
,24	O
27	O
le	O
roux	O
,	O
n.	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2008	O
)	O
.	O
representational	O
power	O
of	O
restricted	O
boltzmann	O
553	O
655	O
machines	O
and	O
deep	O
belief	O
networks	O
.	O
neural	O
computation	O
,	O
(	O
6	O
)	O
,	O
1631–1649	O
.	O
20	O
,	O
le	O
roux	O
,	O
n.	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2010	O
)	O
.	O
deep	O
belief	O
networks	O
are	O
compact	O
universal	O
approxi-	O
mators	O
.	O
neural	O
computation	O
,	O
22	O
(	O
8	O
)	O
,	O
2192–2207	O
.	O
553	O
lecun	O
,	O
y	O
.	O
(	O
1985	O
)	O
.	O
une	O
procédure	O
d	O
’	O
apprentissage	O
pour	O
réseau	O
à	O
seuil	O
assymétrique	O
.	O
in	O
cognitiva	O
85	O
:	O
a	O
la	O
frontière	O
de	O
l	O
’	O
intelligence	O
artiﬁcielle	O
,	O
des	O
sciences	O
de	O
la	O
connaissance	O
et	O
des	O
neurosciences	O
,	O
pages	O
599–604	O
,	O
paris	O
1985.	O
cesta	O
,	O
paris	O
.	O
225	O
lecun	O
,	O
y	O
.	O
(	O
1986	O
)	O
.	O
learning	O
processes	O
in	O
an	O
asymmetric	O
threshold	O
network	O
.	O
in	O
f.	O
fogelman-	O
soulié	O
,	O
e.	O
bienenstock	O
,	O
and	O
g.	O
weisbuch	O
,	O
editors	O
,	O
disordered	O
systems	O
and	O
biological	O
organization	O
,	O
pages	O
233–240	O
.	O
springer-verlag	O
,	O
les	O
houches	O
,	O
france	O
.	O
352	O
lecun	O
,	O
y	O
.	O
(	O
1987	O
)	O
.	O
modèles	O
connexionistes	O
de	O
l	O
’	O
apprentissage	O
.	O
ph.d.	O
thesis	O
,	O
université	O
de	O
paris	O
vi	O
.	O
18	O
502	O
515	O
,	O
,	O
lecun	O
,	O
y	O
.	O
(	O
1989	O
)	O
.	O
generalization	O
and	O
network	O
design	O
strategies	O
.	O
technical	O
report	O
crg-tr-89-4	O
,	O
university	O
of	O
toronto	O
.	O
330	O
352	O
,	O
751	O
bibliography	O
lecun	O
,	O
y.	O
,	O
jackel	O
,	O
l.	O
d.	O
,	O
boser	O
,	O
b.	O
,	O
denker	O
,	O
j.	O
s.	O
,	O
graf	O
,	O
h.	O
p.	O
,	O
guyon	O
,	O
i.	O
,	O
henderson	O
,	O
d.	O
,	O
howard	O
,	O
r.	O
e.	O
,	O
and	O
hubbard	O
,	O
w.	O
(	O
1989	O
)	O
.	O
handwritten	O
digit	O
recognition	B
:	O
applications	O
of	O
neural	O
network	O
chips	O
and	O
automatic	O
learning	O
.	O
ieee	O
communications	O
magazine	O
,	O
27	O
(	O
11	O
)	O
,	O
41–46	O
.	O
368	O
lecun	O
,	O
y.	O
,	O
bottou	O
,	O
l.	O
,	O
orr	O
,	O
g.	O
b.	O
,	O
and	O
müller	O
,	O
k.-r.	O
(	O
1998a	O
)	O
.	O
eﬃcient	O
backprop	O
.	O
in	O
neural	O
networks	O
,	O
tricks	O
of	O
the	O
trade	O
,	O
lecture	O
notes	O
in	O
computer	O
science	O
lncs	O
1524.	O
springer	O
verlag	O
.	O
310	O
429	O
,	O
lecun	O
,	O
y.	O
,	O
bottou	O
,	O
l.	O
,	O
bengio	O
,	O
y.	O
,	O
and	O
haﬀner	O
,	O
p.	O
(	O
1998b	O
)	O
.	O
gradient	O
based	O
learning	O
applied	O
to	O
document	O
recognition	B
.	O
proc	O
.	O
ieee	O
.	O
16	O
18	O
21	O
27	O
371	O
458	O
460	O
,	O
,	O
,	O
,	O
,	O
,	O
lecun	O
,	O
y.	O
,	O
kavukcuoglu	O
,	O
k.	O
,	O
and	O
farabet	O
,	O
c.	O
(	O
2010	O
)	O
.	O
convolutional	O
networks	O
and	O
applications	O
in	O
vision	O
.	O
in	O
circuits	O
and	O
systems	O
(	O
iscas	O
)	O
,	O
proceedings	O
of	O
2010	O
ieee	O
international	O
symposium	O
on	O
,	O
pages	O
253–256	O
.	O
ieee	O
.	O
371	O
l	O
’	O
ecuyer	O
,	O
p.	O
(	O
1994	O
)	O
.	O
eﬃciency	O
improvement	O
and	O
variance	O
reduction	O
.	O
in	O
proceedings	O
of	O
the	O
1994	O
winter	O
simulation	O
conference	O
,	O
pages	O
122––132	O
.	O
690	O
lee	O
,	O
c.-y.	O
,	O
xie	O
,	O
s.	O
,	O
gallagher	O
,	O
p.	O
,	O
zhang	O
,	O
z.	O
,	O
and	O
tu	O
,	O
z	O
.	O
(	O
2014	O
)	O
.	O
deeply-supervised	O
nets	O
.	O
arxiv	O
preprint	O
arxiv:1409.5185	O
.	O
326	O
lee	O
,	O
h.	O
,	O
battle	O
,	O
a.	O
,	O
raina	O
,	O
r.	O
,	O
and	O
ng	O
,	O
a	O
.	O
(	O
2007	O
)	O
.	O
eﬃcient	O
sparse	O
coding	O
algorithms	O
.	O
in	O
b.	O
schölkopf	O
,	O
j.	O
platt	O
,	O
and	O
t.	O
hoﬀman	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
19	O
(	O
nips	O
’	O
06	O
)	O
,	O
pages	O
801–808	O
.	O
mit	O
press	O
.	O
637	O
lee	O
,	O
h.	O
,	O
ekanadham	O
,	O
c.	O
,	O
and	O
ng	O
,	O
a	O
.	O
(	O
2008	O
)	O
.	O
sparse	O
deep	O
belief	O
net	O
model	B
for	O
visual	O
area	O
v2	O
.	O
in	O
nips	O
’	O
07	O
255	O
.	O
lee	O
,	O
h.	O
,	O
grosse	O
,	O
r.	O
,	O
ranganath	O
,	O
r.	O
,	O
and	O
ng	O
,	O
a.	O
y	O
.	O
(	O
2009	O
)	O
.	O
convolutional	O
deep	O
belief	O
networks	O
for	O
scalable	O
unsupervised	O
learning	O
of	O
hierarchical	O
representations	O
.	O
in	O
l.	O
bottou	O
and	O
m.	O
littman	O
,	O
editors	O
,	O
proceedings	O
of	O
the	O
twenty-sixth	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
’	O
09	O
)	O
.	O
acm	O
,	O
montreal	O
,	O
canada	O
.	O
363	O
683	O
684	O
,	O
,	O
lee	O
,	O
y.	O
j.	O
and	O
grauman	O
,	O
k.	O
(	O
2011	O
)	O
.	O
learning	O
the	O
easy	O
things	O
ﬁrst	O
:	O
self-paced	O
visual	O
category	O
discovery	O
.	O
in	O
cvpr	O
’	O
2011	O
328	O
.	O
leibniz	O
,	O
g.	O
w.	O
(	O
1676	O
)	O
.	O
memoir	O
using	O
the	O
chain	O
rule	O
.	O
(	O
cited	O
in	O
tmme	O
7:2	O
&	O
3	O
p	O
321-332	O
,	O
2010	O
)	O
.	O
225	O
lenat	O
,	O
d.	O
b.	O
and	O
guha	O
,	O
r.	O
v.	O
(	O
1989	O
)	O
.	O
building	O
large	O
knowledge-based	O
systems	O
;	O
representa-	O
tion	B
and	O
inference	O
in	O
the	O
cyc	O
project	O
.	O
addison-wesley	O
longman	O
publishing	O
co.	O
,	O
inc.	O
2	O
leshno	O
,	O
m.	O
,	O
lin	O
,	O
v.	O
y.	O
,	O
pinkus	O
,	O
a.	O
,	O
and	O
schocken	O
,	O
s.	O
(	O
1993	O
)	O
.	O
multilayer	O
feedforward	O
networks	O
with	O
a	O
nonpolynomial	O
activation	O
function	O
can	O
approximate	O
any	O
function	O
.	O
neural	O
networks	O
,	O
,	O
861––867	O
.	O
198	O
199	O
6	O
,	O
752	O
bibliography	O
levenberg	O
,	O
k.	O
(	O
1944	O
)	O
.	O
a	O
method	O
for	O
the	O
solution	O
of	O
certain	O
non-linear	O
problems	O
in	O
least	O
squares	O
.	O
quarterly	O
journal	O
of	O
applied	O
mathematics	O
,	O
ii	O
(	O
2	O
)	O
,	O
164–168	O
.	O
312	O
l	O
’	O
hôpital	O
,	O
g.	O
f.	O
a	O
.	O
(	O
1696	O
)	O
.	O
analyse	O
des	O
inﬁniment	O
petits	O
,	O
pour	O
l	O
’	O
intelligence	O
des	O
lignes	O
courbes	O
.	O
paris	O
:	O
l	O
’	O
imprimerie	O
royale	O
.	O
225	O
li	O
,	O
y.	O
,	O
swersky	O
,	O
k.	O
,	O
and	O
zemel	O
,	O
r.	O
s.	O
(	O
2015	O
)	O
.	O
generative	O
moment	O
matching	O
networks	O
.	O
corr	O
,	O
abs/1502.02761	O
703	O
.	O
lin	O
,	O
t.	O
,	O
horne	O
,	O
b.	O
g.	O
,	O
tino	O
,	O
p.	O
,	O
and	O
giles	O
,	O
c.	O
l.	O
(	O
1996	O
)	O
.	O
learning	O
long-term	O
dependencies	O
is	O
not	O
as	O
diﬃcult	O
with	O
narx	O
recurrent	O
neural	O
networks	O
.	O
ieee	O
transactions	O
on	O
neural	O
networks	O
,	O
(	O
6	O
)	O
,	O
1329–1338	O
.	O
7	O
407	O
lin	O
,	O
y.	O
,	O
liu	O
,	O
z.	O
,	O
sun	O
,	O
m.	O
,	O
liu	O
,	O
y.	O
,	O
and	O
zhu	O
,	O
x	O
.	O
(	O
2015	O
)	O
.	O
learning	O
entity	O
and	O
relation	O
embeddings	O
for	O
knowledge	O
graph	O
completion	O
.	O
in	O
proc	O
.	O
aaai	O
’	O
15	O
.	O
484	O
linde	O
,	O
n.	O
(	O
1992	O
)	O
.	O
the	O
machine	O
that	O
changed	O
the	O
world	O
,	O
episode	O
3.	O
documentary	O
miniseries	O
.	O
2	O
lindsey	O
,	O
c.	O
and	O
lindblad	O
,	O
t.	O
(	O
1994	O
)	O
.	O
review	O
of	O
hardware	O
neural	O
networks	O
:	O
a	O
user	O
’	O
s	O
perspective	O
.	O
in	O
proc	O
.	O
third	O
workshop	O
on	O
neural	O
networks	O
:	O
from	O
biology	O
to	O
high	O
energy	O
physics	O
,	O
pages	O
195––202	O
,	O
isola	O
d	O
’	O
elba	O
,	O
italy	O
.	O
451	O
linnainmaa	O
,	O
s.	O
(	O
1976	O
)	O
.	O
taylor	O
expansion	O
of	O
the	O
accumulated	O
rounding	O
error	O
.	O
bit	O
numerical	O
mathematics	O
,	O
16	O
(	O
2	O
)	O
,	O
146–160	O
.	O
225	O
lisa	O
(	O
2008	O
)	O
.	O
deep	O
learning	O
tutorials	O
:	O
restricted	O
boltzmann	O
machines	O
.	O
technical	O
report	O
,	O
lisa	O
lab	O
,	O
université	O
de	O
montréal	O
.	O
589	O
long	O
,	O
p.	O
m.	O
and	O
servedio	O
,	O
r.	O
a	O
.	O
(	O
2010	O
)	O
.	O
restricted	O
boltzmann	O
machines	O
are	O
hard	O
to	O
approximately	O
evaluate	O
or	O
simulate	O
.	O
in	O
proceedings	O
of	O
the	O
27th	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
’	O
10	O
)	O
.	O
658	O
lotter	O
,	O
w.	O
,	O
kreiman	O
,	O
g.	O
,	O
and	O
cox	O
,	O
d.	O
(	O
2015	O
)	O
.	O
unsupervised	O
learning	O
of	O
visual	O
structure	O
using	O
predictive	O
generative	O
networks	O
.	O
arxiv	O
preprint	O
arxiv:1511.06380	O
.	O
544	O
545	O
,	O
lovelace	O
,	O
a	O
.	O
(	O
1842	O
)	O
.	O
notes	O
upon	O
l.	O
f.	O
menabrea	O
’	O
s	O
“	O
sketch	O
of	O
the	O
analytical	O
engine	O
invented	O
by	O
charles	O
babbage	O
”	O
.	O
1	O
lu	O
,	O
l.	O
,	O
zhang	O
,	O
x.	O
,	O
cho	O
,	O
k.	O
,	O
and	O
renals	O
,	O
s.	O
(	O
2015	O
)	O
.	O
a	O
study	O
of	O
the	O
recurrent	O
neural	O
network	O
encoder-decoder	O
for	O
large	O
vocabulary	O
speech	O
recognition	B
.	O
in	O
proc	O
.	O
interspeech	O
.	O
461	O
lu	O
,	O
t.	O
,	O
pál	O
,	O
d.	O
,	O
and	O
pál	O
,	O
m.	O
(	O
2010	O
)	O
.	O
contextual	O
multi-armed	O
bandits	O
.	O
in	O
international	O
conference	O
on	O
artiﬁcial	O
intelligence	O
and	O
statistics	O
,	O
pages	O
485–492	O
.	O
480	O
luenberger	O
,	O
d.	O
g.	O
(	O
1984	O
)	O
.	O
linear	O
and	O
nonlinear	O
programming	O
.	O
addison	O
wesley	O
.	O
316	O
lukoševičius	O
,	O
m.	O
and	O
jaeger	O
,	O
h.	O
(	O
2009	O
)	O
.	O
reservoir	O
computing	O
approaches	O
to	O
recurrent	O
neural	O
network	O
training	O
.	O
computer	O
science	O
review	O
,	O
3	O
(	O
3	O
)	O
,	O
127–149	O
.	O
404	O
753	O
bibliography	O
luo	O
,	O
h.	O
,	O
shen	O
,	O
r.	O
,	O
niu	O
,	O
c.	O
,	O
and	O
ullrich	O
,	O
c.	O
(	O
2011	O
)	O
.	O
learning	O
class-relevant	O
features	O
and	O
class-irrelevant	O
features	O
via	O
a	O
hybrid	O
third-order	O
rbm	O
.	O
in	O
international	O
conference	O
on	O
artiﬁcial	O
intelligence	O
and	O
statistics	O
,	O
pages	O
470–478	O
.	O
686	O
luo	O
,	O
h.	O
,	O
carrier	O
,	O
p.	O
l.	O
,	O
courville	O
,	O
a.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2013	O
)	O
.	O
texture	O
modeling	O
with	O
convolutional	O
spike-and-slab	O
rbms	O
and	O
deep	O
extensions	O
.	O
in	O
aistats	O
’	O
2013	O
.	O
102	O
lyu	O
,	O
s.	O
(	O
2009	O
)	O
.	O
interpretation	O
and	O
generalization	O
of	O
score	O
matching	O
.	O
in	O
proceedings	O
of	O
the	O
twenty-ﬁfth	O
conference	O
in	O
uncertainty	O
in	O
artiﬁcial	O
intelligence	O
(	O
uai	O
’	O
09	O
)	O
.	O
618	O
ma	O
,	O
j.	O
,	O
sheridan	O
,	O
r.	O
p.	O
,	O
liaw	O
,	O
a.	O
,	O
dahl	O
,	O
g.	O
e.	O
,	O
and	O
svetnik	O
,	O
v.	O
(	O
2015	O
)	O
.	O
deep	O
neural	O
nets	O
as	O
a	O
method	O
for	O
quantitative	O
structure	O
–	O
activity	O
relationships	O
.	O
j.	O
chemical	O
information	O
and	O
modeling	O
.	O
530	O
maas	O
,	O
a.	O
l.	O
,	O
hannun	O
,	O
a.	O
y.	O
,	O
and	O
ng	O
,	O
a.	O
y	O
.	O
(	O
2013	O
)	O
.	O
rectiﬁer	O
nonlinearities	O
improve	O
neural	O
network	O
acoustic	O
models	O
.	O
in	O
icml	O
workshop	O
on	O
deep	O
learning	O
for	O
audio	O
,	O
speech	O
,	O
and	O
language	O
processing	O
.	O
193	O
maass	O
,	O
w.	O
(	O
1992	O
)	O
.	O
bounds	O
for	O
the	O
computational	O
power	O
and	O
learning	O
complexity	O
of	O
analog	O
neural	O
nets	O
(	O
extended	O
abstract	O
)	O
.	O
in	O
proc	O
.	O
of	O
the	O
25th	O
acm	O
symp	O
.	O
theory	O
of	O
computing	O
,	O
pages	O
335–344	O
.	O
199	O
maass	O
,	O
w.	O
,	O
schnitger	O
,	O
g.	O
,	O
and	O
sontag	O
,	O
e.	O
d.	O
(	O
1994	O
)	O
.	O
a	O
comparison	O
of	O
the	O
computational	O
power	O
of	O
sigmoid	O
and	O
boolean	O
threshold	O
circuits	O
.	O
theoretical	O
advances	O
in	O
neural	O
computation	O
and	O
learning	O
,	O
pages	O
127–151	O
.	O
199	O
maass	O
,	O
w.	O
,	O
natschlaeger	O
,	O
t.	O
,	O
and	O
markram	O
,	O
h.	O
(	O
2002	O
)	O
.	O
real-time	O
computing	O
without	O
stable	O
states	O
:	O
a	O
new	O
framework	O
for	O
neural	O
computation	O
based	O
on	O
perturbations	O
.	O
neural	O
computation	O
,	O
(	O
11	O
)	O
,	O
2531–2560	O
.	O
14	O
404	O
mackay	O
,	O
d.	O
(	O
2003	O
)	O
.	O
information	O
theory	O
,	O
inference	O
and	O
learning	O
algorithms	O
.	O
cambridge	O
university	O
press	O
.	O
73	O
maclaurin	O
,	O
d.	O
,	O
duvenaud	O
,	O
d.	O
,	O
and	O
adams	O
,	O
r.	O
p.	O
(	O
2015	O
)	O
.	O
gradient-based	O
hyperparameter	O
optimization	O
through	O
reversible	O
learning	O
.	O
arxiv	O
preprint	O
arxiv:1502.03492	O
.	O
435	O
mao	O
,	O
j.	O
,	O
xu	O
,	O
w.	O
,	O
yang	O
,	O
y.	O
,	O
wang	O
,	O
j.	O
,	O
huang	O
,	O
z.	O
,	O
and	O
yuille	O
,	O
a.	O
l.	O
(	O
2015	O
)	O
.	O
deep	O
captioning	O
with	O
multimodal	O
recurrent	O
neural	O
networks	O
.	O
in	O
iclr	O
’	O
2015	O
.	O
arxiv:1410.1090	O
.	O
102	O
marcotte	O
,	O
p.	O
and	O
savard	O
,	O
g.	O
(	O
1992	O
)	O
.	O
novel	O
approaches	O
to	O
the	O
discrimination	O
problem	O
.	O
zeitschrift	O
für	O
operations	O
research	O
(	O
theory	O
)	O
,	O
36	O
,	O
517–545	O
.	O
276	O
marlin	O
,	O
b.	O
and	O
de	O
freitas	O
,	O
n.	O
(	O
2011	O
)	O
.	O
asymptotic	O
eﬃciency	O
of	O
deterministic	O
estimators	O
for	O
uai	O
’	O
2011	O
617	O
,	O
discrete	O
energy-based	O
models	O
:	O
ratio	O
matching	O
and	O
pseudolikelihood	O
.	O
in	O
619	O
.	O
754	O
bibliography	O
marlin	O
,	O
b.	O
,	O
swersky	O
,	O
k.	O
,	O
chen	O
,	O
b.	O
,	O
and	O
de	O
freitas	O
,	O
n.	O
(	O
2010	O
)	O
.	O
inductive	O
principles	O
for	O
restricted	O
boltzmann	O
machine	O
learning	O
.	O
in	O
proceedings	O
of	O
the	O
thirteenth	O
international	O
conference	O
on	O
artiﬁcial	O
intelligence	O
and	O
statistics	O
(	O
aistats	O
’	O
10	O
)	O
,	O
volume	O
9	O
,	O
pages	O
509–516	O
.	O
613	O
618	O
619	O
,	O
,	O
marquardt	O
,	O
d.	O
w.	O
(	O
1963	O
)	O
.	O
an	O
algorithm	O
for	O
least-squares	O
estimation	O
of	O
non-linear	O
param-	O
eters	O
.	O
journal	O
of	O
the	O
society	O
of	O
industrial	O
and	O
applied	O
mathematics	O
,	O
11	O
(	O
2	O
)	O
,	O
431–441	O
.	O
312	O
marr	O
,	O
d.	O
and	O
poggio	O
,	O
t.	O
(	O
1976	O
)	O
.	O
cooperative	O
computation	O
of	O
stereo	O
disparity	O
.	O
science	O
,	O
194	O
.	O
367	O
martens	O
,	O
j	O
.	O
(	O
2010	O
)	O
.	O
deep	O
learning	O
via	O
hessian-free	O
optimization	O
.	O
in	O
l.	O
bottou	O
and	O
m.	O
littman	O
,	O
editors	O
,	O
proceedings	O
of	O
the	O
twenty-seventh	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml-10	O
)	O
,	O
pages	O
735–742	O
.	O
acm	O
.	O
304	O
martens	O
,	O
j.	O
and	O
medabalimi	O
,	O
v.	O
(	O
2014	O
)	O
.	O
on	O
the	O
expressive	O
eﬃciency	O
of	O
sum	O
product	O
networks	O
.	O
arxiv:1411.7717	O
554	O
.	O
martens	O
,	O
j.	O
and	O
sutskever	O
,	O
i	O
.	O
(	O
2011	O
)	O
.	O
learning	O
recurrent	O
neural	O
networks	O
with	O
hessian-free	O
optimization	O
.	O
in	O
proc	O
.	O
icml	O
’	O
2011	O
.	O
acm	O
.	O
413	O
mase	O
,	O
s.	O
(	O
1995	O
)	O
.	O
consistency	O
of	O
the	O
maximum	O
pseudo-likelihood	O
estimator	O
of	O
continuous	O
state	O
space	O
gibbsian	O
processes	O
.	O
the	O
annals	O
of	O
applied	O
probability	O
,	O
5	O
(	O
3	O
)	O
,	O
pp	O
.	O
603–612	O
.	O
616	O
mcclelland	O
,	O
j.	O
,	O
rumelhart	O
,	O
d.	O
,	O
and	O
hinton	O
,	O
g.	O
(	O
1995	O
)	O
.	O
the	O
appeal	O
of	O
parallel	O
distributed	O
processing	O
.	O
in	O
computation	O
&	O
intelligence	O
,	O
pages	O
305–341	O
.	O
american	O
association	O
for	O
artiﬁcial	O
intelligence	O
.	O
17	O
mcculloch	O
,	O
w.	O
s.	O
and	O
pitts	O
,	O
w.	O
(	O
1943	O
)	O
.	O
a	O
logical	O
calculus	O
of	O
ideas	O
immanent	O
in	O
nervous	O
activity	O
.	O
bulletin	O
of	O
mathematical	O
biophysics	O
,	O
5	O
,	O
115–133	O
.	O
14	O
15	O
,	O
mead	O
,	O
c.	O
and	O
ismail	O
,	O
m.	O
(	O
2012	O
)	O
.	O
analog	O
vlsi	O
implementation	O
of	O
neural	O
systems	O
,	O
volume	O
80.	O
springer	O
science	O
&	O
business	O
media	O
.	O
451	O
melchior	O
,	O
j.	O
,	O
fischer	O
,	O
a.	O
,	O
and	O
wiskott	O
,	O
l.	O
(	O
2013	O
)	O
.	O
how	O
to	O
center	O
binary	O
deep	O
boltzmann	O
machines	O
.	O
arxiv	O
preprint	O
arxiv:1311.1354	O
.	O
674	O
memisevic	O
,	O
r.	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
2007	O
)	O
.	O
unsupervised	O
learning	O
of	O
image	O
transformations	O
.	O
in	O
proceedings	O
of	O
the	O
computer	O
vision	O
and	O
pattern	O
recognition	B
conference	O
(	O
cvpr	O
’	O
07	O
)	O
.	O
686	O
memisevic	O
,	O
r.	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
2010	O
)	O
.	O
learning	O
to	O
represent	O
spatial	O
transformations	O
with	O
factored	O
higher-order	O
boltzmann	O
machines	O
.	O
neural	O
computation	O
,	O
22	O
(	O
6	O
)	O
,	O
1473–1492	O
.	O
686	O
755	O
bibliography	O
mesnil	O
,	O
g.	O
,	O
dauphin	O
,	O
y.	O
,	O
glorot	O
,	O
x.	O
,	O
rifai	O
,	O
s.	O
,	O
bengio	O
,	O
y.	O
,	O
goodfellow	O
,	O
i.	O
,	O
lavoie	O
,	O
e.	O
,	O
muller	O
,	O
x.	O
,	O
desjardins	O
,	O
g.	O
,	O
warde-farley	O
,	O
d.	O
,	O
vincent	O
,	O
p.	O
,	O
courville	O
,	O
a.	O
,	O
and	O
bergstra	O
,	O
j	O
.	O
(	O
2011	O
)	O
.	O
unsupervised	O
and	O
transfer	O
learning	O
challenge	O
:	O
a	O
deep	O
learning	O
approach	O
.	O
in	O
jmlr	O
w	O
&	O
cp	O
:	O
proc	O
.	O
unsupervised	O
and	O
transfer	O
learning	O
,	O
volume	O
7	O
.	O
201	O
532	O
538	O
,	O
,	O
mesnil	O
,	O
g.	O
,	O
rifai	O
,	O
s.	O
,	O
dauphin	O
,	O
y.	O
,	O
bengio	O
,	O
y.	O
,	O
and	O
vincent	O
,	O
p.	O
(	O
2012	O
)	O
.	O
surﬁng	O
on	O
the	O
manifold	O
.	O
learning	O
workshop	O
,	O
snowbird	O
.	O
711	O
miikkulainen	O
,	O
r.	O
and	O
dyer	O
,	O
m.	O
g.	O
(	O
1991	O
)	O
.	O
natural	O
language	O
processing	O
with	O
modular	O
pdp	O
networks	O
and	O
distributed	O
lexicon	O
.	O
cognitive	O
science	O
,	O
15	O
,	O
343–399	O
.	O
477	O
mikolov	O
,	O
t.	O
(	O
2012	O
)	O
.	O
statistical	O
language	O
models	O
based	O
on	O
neural	O
networks	O
.	O
ph.d.	O
thesis	O
,	O
brno	O
university	O
of	O
technology	O
.	O
414	O
mikolov	O
,	O
t.	O
,	O
deoras	O
,	O
a.	O
,	O
kombrink	O
,	O
s.	O
,	O
burget	O
,	O
l.	O
,	O
and	O
cernocky	O
,	O
j	O
.	O
(	O
2011a	O
)	O
.	O
empirical	O
evaluation	O
and	O
combination	O
of	O
advanced	O
language	O
modeling	O
techniques	O
.	O
in	O
proc	O
.	O
12th	O
an-	O
nual	O
conference	O
of	O
the	O
international	O
speech	O
communication	O
association	O
(	O
interspeech	O
2011	O
)	O
.	O
472	O
mikolov	O
,	O
t.	O
,	O
deoras	O
,	O
a.	O
,	O
povey	O
,	O
d.	O
,	O
burget	O
,	O
l.	O
,	O
and	O
cernocky	O
,	O
j	O
.	O
(	O
2011b	O
)	O
.	O
strategies	O
for	O
training	O
large	O
scale	O
neural	O
network	O
language	O
models	O
.	O
in	O
proc	O
.	O
asru	O
’	O
2011	O
.	O
328	O
472	O
,	O
mikolov	O
,	O
t.	O
,	O
chen	O
,	O
k.	O
,	O
corrado	O
,	O
g.	O
,	O
and	O
dean	O
,	O
j	O
.	O
(	O
2013a	O
)	O
.	O
eﬃcient	O
estimation	O
of	O
word	O
rep-	O
resentations	O
in	O
vector	O
space	O
.	O
in	O
international	O
conference	O
on	O
learning	O
representations	O
:	O
workshops	O
track	O
.	O
536	O
mikolov	O
,	O
t.	O
,	O
le	O
,	O
q.	O
v.	O
,	O
and	O
sutskever	O
,	O
i	O
.	O
(	O
2013b	O
)	O
.	O
exploiting	O
similarities	O
among	O
languages	O
for	O
machine	O
translation	O
.	O
technical	O
report	O
,	O
arxiv:1309.4168	O
.	O
539	O
minka	O
,	O
t.	O
(	O
2005	O
)	O
.	O
divergence	O
measures	O
and	O
message	O
passing	O
.	O
microsoft	O
research	O
cambridge	O
uk	O
tech	O
rep	O
msrtr2005173	O
,	O
72	O
(	O
tr-2005-173	O
)	O
.	O
625	O
minsky	O
,	O
m.	O
l.	O
and	O
papert	O
,	O
s.	O
a	O
.	O
(	O
1969	O
)	O
.	O
perceptrons	O
.	O
mit	O
press	O
,	O
cambridge	O
.	O
15	O
mirza	O
,	O
m.	O
and	O
osindero	O
,	O
s.	O
(	O
2014	O
)	O
.	O
conditional	O
generative	O
adversarial	O
nets	O
.	O
arxiv	O
preprint	O
arxiv:1411.1784	O
.	O
702	O
mishkin	O
,	O
d.	O
and	O
matas	O
,	O
j	O
.	O
(	O
2015	O
)	O
.	O
all	O
you	O
need	O
is	O
a	O
good	O
init	O
.	O
arxiv	O
preprint	O
arxiv:1511.06422	O
.	O
305	O
misra	O
,	O
j.	O
and	O
saha	O
,	O
i	O
.	O
(	O
2010	O
)	O
.	O
artiﬁcial	O
neural	O
networks	O
in	O
hardware	O
:	O
a	O
survey	O
of	O
two	O
decades	O
of	O
progress	O
.	O
neurocomputing	O
,	O
74	O
(	O
1	O
)	O
,	O
239–255	O
.	O
451	O
mitchell	O
,	O
t.	O
m.	O
(	O
1997	O
)	O
.	O
machine	O
learning	O
.	O
mcgraw-hill	O
,	O
new	O
york	O
.	O
99	O
miyato	O
,	O
t.	O
,	O
maeda	O
,	O
s.	O
,	O
koyama	O
,	O
m.	O
,	O
nakae	O
,	O
k.	O
,	O
and	O
ishii	O
,	O
s.	O
(	O
2015	O
)	O
.	O
distributional	O
269	O
smoothing	O
with	O
virtual	O
adversarial	O
training	O
.	O
in	O
.	O
preprint	O
:	O
arxiv:1507.00677.	O
iclr	O
756	O
bibliography	O
mnih	O
,	O
a.	O
and	O
gregor	O
,	O
k.	O
(	O
2014	O
)	O
.	O
neural	O
variational	O
inference	O
and	O
learning	O
in	O
belief	O
networks	O
.	O
in	O
icml	O
’	O
2014	O
691	O
692	O
693	O
.	O
,	O
,	O
mnih	O
,	O
a.	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
2007	O
)	O
.	O
three	O
new	O
graphical	O
models	O
for	O
statistical	O
language	O
modelling	O
.	O
in	O
z.	O
ghahramani	O
,	O
editor	O
,	O
proceedings	O
of	O
the	O
twenty-fourth	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
’	O
07	O
)	O
,	O
pages	O
641–648	O
.	O
acm	O
.	O
465	O
mnih	O
,	O
a.	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
2009	O
)	O
.	O
a	O
scalable	O
hierarchical	O
distributed	O
language	O
model	B
.	O
in	O
d.	O
koller	O
,	O
d.	O
schuurmans	O
,	O
y.	O
bengio	O
,	O
and	O
l.	O
bottou	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
21	O
(	O
nips	O
’	O
08	O
)	O
,	O
pages	O
1081–1088	O
.	O
467	O
mnih	O
,	O
a.	O
and	O
kavukcuoglu	O
,	O
k.	O
(	O
2013	O
)	O
.	O
learning	O
word	O
embeddings	O
eﬃciently	O
with	O
noise-	O
contrastive	O
estimation	O
.	O
in	O
c.	O
burges	O
,	O
l.	O
bottou	O
,	O
m.	O
welling	O
,	O
z.	O
ghahramani	O
,	O
and	O
k.	O
weinberger	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
26	O
,	O
pages	O
2265–2273	O
.	O
curran	O
associates	O
,	O
inc.	O
472	O
622	O
,	O
mnih	O
,	O
a.	O
and	O
teh	O
,	O
y.	O
w.	O
(	O
2012	O
)	O
.	O
a	O
fast	O
and	O
simple	O
algorithm	O
for	O
training	O
neural	O
probabilistic	O
language	O
models	O
.	O
in	O
icml	O
’	O
2012	O
,	O
pages	O
1751–1758	O
.	O
472	O
mnih	O
,	O
v.	O
and	O
hinton	O
,	O
g.	O
(	O
2010	O
)	O
.	O
learning	O
to	O
detect	O
roads	O
in	O
high-resolution	O
aerial	O
images	O
.	O
in	O
proceedings	O
of	O
the	O
11th	O
european	O
conference	O
on	O
computer	O
vision	O
(	O
eccv	O
)	O
.	O
102	O
mnih	O
,	O
v.	O
,	O
larochelle	O
,	O
h.	O
,	O
and	O
hinton	O
,	O
g.	O
(	O
2011	O
)	O
.	O
conditional	O
restricted	O
boltzmann	O
machines	O
for	O
structure	O
output	O
prediction	O
.	O
in	O
proc	O
.	O
conf	O
.	O
on	O
uncertainty	O
in	O
artiﬁcial	O
intelligence	O
(	O
uai	O
)	O
.	O
685	O
mnih	O
,	O
v.	O
,	O
kavukcuoglo	O
,	O
k.	O
,	O
silver	O
,	O
d.	O
,	O
graves	O
,	O
a.	O
,	O
antonoglou	O
,	O
i.	O
,	O
and	O
wierstra	O
,	O
d.	O
(	O
2013	O
)	O
.	O
playing	O
atari	O
with	O
deep	O
reinforcement	O
learning	O
.	O
technical	O
report	O
,	O
arxiv:1312.5602	O
.	O
106	O
mnih	O
,	O
v.	O
,	O
heess	O
,	O
n.	O
,	O
graves	O
,	O
a.	O
,	O
and	O
kavukcuoglu	O
,	O
k.	O
(	O
2014	O
)	O
.	O
recurrent	O
models	O
of	O
visual	O
attention	O
.	O
in	O
z.	O
ghahramani	O
,	O
m.	O
welling	O
,	O
c.	O
cortes	O
,	O
n.	O
lawrence	O
,	O
and	O
k.	O
weinberger	O
,	O
editors	O
,	O
,	O
pages	O
2204–2212	O
.	O
nips	O
’	O
2014	O
691	O
mnih	O
,	O
v.	O
,	O
kavukcuoglo	O
,	O
k.	O
,	O
silver	O
,	O
d.	O
,	O
rusu	O
,	O
a.	O
a.	O
,	O
veness	O
,	O
j.	O
,	O
bellemare	O
,	O
m.	O
g.	O
,	O
graves	O
,	O
a.	O
,	O
riedmiller	O
,	O
m.	O
,	O
fidgeland	O
,	O
a.	O
k.	O
,	O
ostrovski	O
,	O
g.	O
,	O
petersen	O
,	O
s.	O
,	O
beattie	O
,	O
c.	O
,	O
sadik	O
,	O
a.	O
,	O
antonoglou	O
,	O
i.	O
,	O
king	O
,	O
h.	O
,	O
kumaran	O
,	O
d.	O
,	O
wierstra	O
,	O
d.	O
,	O
legg	O
,	O
s.	O
,	O
and	O
hassabis	O
,	O
d.	O
(	O
2015	O
)	O
.	O
human-level	O
control	O
through	O
deep	O
reinforcement	O
learning	O
.	O
nature	O
,	O
,	O
529–533	O
.	O
518	O
25	O
mobahi	O
,	O
h.	O
and	O
fisher	O
,	O
iii	O
,	O
j.	O
w.	O
(	O
2015	O
)	O
.	O
a	O
theoretical	O
analysis	O
of	O
optimization	O
by	O
gaussian	O
continuation	O
.	O
in	O
aaai	O
’	O
2015	O
327	O
.	O
mobahi	O
,	O
h.	O
,	O
collobert	O
,	O
r.	O
,	O
and	O
weston	O
,	O
j	O
.	O
(	O
2009	O
)	O
.	O
deep	O
learning	O
from	O
temporal	O
coherence	O
in	O
video	O
.	O
in	O
l.	O
bottou	O
and	O
m.	O
littman	O
,	O
editors	O
,	O
proceedings	O
of	O
the	O
26th	O
international	O
conference	O
on	O
machine	O
learning	O
,	O
pages	O
737–744	O
,	O
montreal	O
.	O
omnipress	O
.	O
494	O
mohamed	O
,	O
a.	O
,	O
dahl	O
,	O
g.	O
,	O
and	O
hinton	O
,	O
g.	O
(	O
2009	O
)	O
.	O
deep	O
belief	O
networks	O
for	O
phone	O
recognition	B
.	O
459	O
757	O
bibliography	O
mohamed	O
,	O
a.	O
,	O
sainath	O
,	O
t.	O
n.	O
,	O
dahl	O
,	O
g.	O
,	O
ramabhadran	O
,	O
b.	O
,	O
hinton	O
,	O
g.	O
e.	O
,	O
and	O
picheny	O
,	O
m.	O
a	O
.	O
(	O
2011	O
)	O
.	O
deep	O
belief	O
networks	O
using	O
discriminative	O
features	O
for	O
phone	O
recognition	B
.	O
in	O
acoustics	O
,	O
speech	O
and	O
signal	O
processing	O
(	O
icassp	O
)	O
,	O
2011	O
ieee	O
international	O
conference	O
on	O
,	O
pages	O
5060–5063	O
.	O
ieee	O
.	O
459	O
mohamed	O
,	O
a.	O
,	O
dahl	O
,	O
g.	O
,	O
and	O
hinton	O
,	O
g.	O
(	O
2012a	O
)	O
.	O
acoustic	O
modeling	O
using	O
deep	O
belief	O
networks	O
.	O
ieee	O
trans	O
.	O
on	O
audio	O
,	O
speech	O
and	O
language	O
processing	O
,	O
20	O
(	O
1	O
)	O
,	O
14–22	O
.	O
459	O
mohamed	O
,	O
a.	O
,	O
hinton	O
,	O
g.	O
,	O
and	O
penn	O
,	O
g.	O
(	O
2012b	O
)	O
.	O
understanding	O
how	O
deep	O
belief	O
networks	O
perform	O
acoustic	O
modelling	O
.	O
in	O
acoustics	O
,	O
speech	O
and	O
signal	O
processing	O
(	O
icassp	O
)	O
,	O
2012	O
ieee	O
international	O
conference	O
on	O
,	O
pages	O
4273–4276	O
.	O
ieee	O
.	O
459	O
moller	O
,	O
m.	O
f.	O
(	O
1993	O
)	O
.	O
a	O
scaled	O
conjugate	O
gradient	O
algorithm	O
for	O
fast	O
supervised	O
learning	O
.	O
neural	O
networks	O
,	O
6	O
,	O
525–533	O
.	O
316	O
montavon	O
,	O
g.	O
and	O
muller	O
,	O
k.-r.	O
(	O
2012	O
)	O
.	O
deep	O
boltzmann	O
machines	O
and	O
the	O
centering	O
trick	B
.	O
in	O
g.	O
montavon	O
,	O
g.	O
orr	O
,	O
and	O
k.-r.	O
müller	O
,	O
editors	O
,	O
neural	O
networks	O
:	O
tricks	O
of	O
the	O
trade	O
,	O
volume	O
7700	O
of	O
lecture	O
notes	O
in	O
computer	O
science	O
,	O
pages	O
621–637	O
.	O
preprint	O
:	O
http	O
:	O
//arxiv.org/abs/1203.3783	O
.	O
673	O
montúfar	O
,	O
g.	O
(	O
2014	O
)	O
.	O
universal	O
approximation	O
depth	O
and	O
errors	O
of	O
narrow	O
belief	O
networks	O
with	O
discrete	O
units	O
.	O
neural	O
computation	O
,	O
.26	O
553	O
montúfar	O
,	O
g.	O
and	O
ay	O
,	O
n.	O
(	O
2011	O
)	O
.	O
reﬁnements	O
of	O
universal	O
approximation	O
results	O
for	O
deep	O
belief	O
networks	O
and	O
restricted	O
boltzmann	O
machines	O
.	O
neural	O
computation	O
,	O
23	O
(	O
5	O
)	O
,	O
1306–1319	O
.	O
553	O
montufar	O
,	O
g.	O
f.	O
,	O
pascanu	O
,	O
r.	O
,	O
cho	O
,	O
k.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2014	O
)	O
.	O
on	O
the	O
number	O
of	O
linear	O
regions	O
of	O
deep	O
neural	O
networks	O
.	O
in	O
nips	O
’	O
2014	O
19	O
199	O
200	O
.	O
,	O
,	O
mor-yosef	O
,	O
s.	O
,	O
samueloﬀ	O
,	O
a.	O
,	O
modan	O
,	O
b.	O
,	O
navot	O
,	O
d.	O
,	O
and	O
schenker	O
,	O
j.	O
g.	O
(	O
1990	O
)	O
.	O
ranking	O
the	O
risk	O
factors	O
for	O
cesarean	O
:	O
logistic	O
regression	O
analysis	O
of	O
a	O
nationwide	O
study	O
.	O
obstet	O
gynecol	O
,	O
(	O
6	O
)	O
,	O
944–7	O
.	O
75	O
3	O
morin	O
,	O
f.	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2005	O
)	O
.	O
hierarchical	O
probabilistic	O
neural	O
network	O
language	O
model	B
.	O
in	O
aistats	O
’	O
2005	O
.	O
467	O
469	O
,	O
mozer	O
,	O
m.	O
c.	O
(	O
1992	O
)	O
.	O
the	O
induction	O
of	O
multiscale	O
temporal	O
structure	O
.	O
in	O
j.	O
m.	O
s.	O
hanson	O
and	O
r.	O
lippmann	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
4	O
(	O
nips	O
’	O
91	O
)	O
,	O
pages	O
275–282	O
,	O
san	O
mateo	O
,	O
ca	O
.	O
morgan	O
kaufmann	O
.	O
407	O
408	O
,	O
murphy	O
,	O
k.	O
p.	O
(	O
2012	O
)	O
.	O
machine	O
learning	O
:	O
a	O
probabilistic	O
perspective	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
,	O
usa	O
.	O
62	O
98	O
146	O
,	O
,	O
murray	O
,	O
b.	O
u.	O
i.	O
and	O
larochelle	O
,	O
h.	O
(	O
2014	O
)	O
.	O
a	O
deep	O
and	O
tractable	O
density	O
estimator	O
.	O
in	O
icml	O
’	O
2014	O
.	O
190	O
710	O
,	O
nair	O
,	O
v.	O
and	O
hinton	O
,	O
g.	O
(	O
2010	O
)	O
.	O
rectiﬁed	O
linear	O
units	O
improve	O
restricted	O
boltzmann	O
machines	O
.	O
in	O
icml	O
’	O
2010	O
16	O
174	O
197	O
.	O
,	O
,	O
758	O
bibliography	O
nair	O
,	O
v.	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
2009	O
)	O
.	O
3d	O
object	O
recognition	B
with	O
deep	O
belief	O
nets	O
.	O
in	O
y.	O
bengio	O
,	O
d.	O
schuurmans	O
,	O
j.	O
d.	O
laﬀerty	O
,	O
c.	O
k.	O
i.	O
williams	O
,	O
and	O
a.	O
culotta	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
22	O
,	O
pages	O
1339–1347	O
.	O
curran	O
associates	O
,	O
inc.	O
686	O
narayanan	O
,	O
h.	O
and	O
mitter	O
,	O
s.	O
(	O
2010	O
)	O
.	O
sample	O
complexity	O
of	O
testing	O
the	O
manifold	O
hypothesis	O
.	O
in	O
nips	O
’	O
2010	O
164	O
.	O
naumann	O
,	O
u	O
.	O
(	O
2008	O
)	O
.	O
optimal	O
jacobian	O
accumulation	O
is	O
np-complete	O
.	O
mathematical	O
programming	O
,	O
112	O
(	O
2	O
)	O
,	O
427–441	O
.	O
222	O
navigli	O
,	O
r.	O
and	O
velardi	O
,	O
p.	O
(	O
2005	O
)	O
.	O
structural	O
semantic	O
interconnections	O
:	O
a	O
knowledge-	O
based	O
approach	O
to	O
word	O
sense	O
disambiguation	O
.	O
ieee	O
trans	O
.	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
(	O
7	O
)	O
,	O
1075––1086	O
.	O
485	O
27	O
neal	O
,	O
r.	O
and	O
hinton	O
,	O
g.	O
(	O
1999	O
)	O
.	O
a	O
view	O
of	O
the	O
em	O
algorithm	O
that	O
justiﬁes	O
incremental	O
,	O
sparse	O
,	O
and	O
other	O
variants	O
.	O
in	O
m.	O
i.	O
jordan	O
,	O
editor	O
,	O
learning	O
in	O
graphical	O
models	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
634	O
neal	O
,	O
r.	O
m.	O
(	O
1990	O
)	O
.	O
learning	O
stochastic	O
feedforward	O
networks	O
.	O
technical	O
report	O
.	O
692	O
neal	O
,	O
r.	O
m.	O
(	O
1993	O
)	O
.	O
probabilistic	O
inference	O
using	O
markov	O
chain	O
monte-carlo	O
methods	O
.	O
technical	O
report	O
crg-tr-93-1	O
,	O
dept	O
.	O
of	O
computer	O
science	O
,	O
university	O
of	O
toronto	O
.	O
680	O
neal	O
,	O
r.	O
m.	O
(	O
1994	O
)	O
.	O
sampling	O
from	O
multimodal	O
distributions	O
using	O
tempered	O
transitions	O
.	O
technical	O
report	O
9421	O
,	O
dept	O
.	O
of	O
statistics	O
,	O
university	O
of	O
toronto	O
.	O
603	O
neal	O
,	O
r.	O
m.	O
(	O
1996	O
)	O
.	O
bayesian	O
learning	O
for	O
neural	O
networks	O
.	O
lecture	O
notes	O
in	O
statistics	O
.	O
springer	O
.	O
265	O
neal	O
,	O
r.	O
m.	O
(	O
2001	O
)	O
.	O
annealed	O
importance	O
sampling	O
.	O
statistics	O
and	O
computing	O
11	O
(	O
2	O
)	O
,	O
,	O
125–139	O
.	O
625	O
627	O
628	O
,	O
,	O
neal	O
,	O
r.	O
m.	O
(	O
2005	O
)	O
.	O
estimating	O
ratios	O
of	O
normalizing	O
constants	O
using	O
linked	O
importance	O
sampling	O
.	O
629	O
nesterov	O
,	O
y	O
.	O
(	O
1983	O
)	O
.	O
a	O
method	O
of	O
solving	O
a	O
convex	O
programming	O
problem	O
with	O
convergence	O
rate	O
o	O
/k	O
(	O
1	O
2	O
)	O
.	O
soviet	O
mathematics	O
doklady	O
27	O
,	O
,	O
372–376	O
.	O
300	O
nesterov	O
,	O
y	O
.	O
(	O
2004	O
)	O
.	O
introductory	O
lectures	O
on	O
convex	O
optimization	O
:	O
a	O
basic	O
course	O
.	O
applied	O
optimization	O
.	O
kluwer	O
academic	O
publ.	O
,	O
boston	O
,	O
dordrecht	O
,	O
london	O
.	O
300	O
netzer	O
,	O
y.	O
,	O
wang	O
,	O
t.	O
,	O
coates	O
,	O
a.	O
,	O
bissacco	O
,	O
a.	O
,	O
wu	O
,	O
b.	O
,	O
and	O
ng	O
,	O
a.	O
y	O
.	O
(	O
2011	O
)	O
.	O
reading	O
digits	O
in	O
natural	O
images	O
with	O
unsupervised	O
feature	O
learning	O
.	O
deep	O
learning	O
and	O
unsupervised	O
feature	O
learning	O
workshop	O
,	O
nips	O
.	O
21	O
ney	O
,	O
h.	O
and	O
kneser	O
,	O
r.	O
(	O
1993	O
)	O
.	O
improved	O
clustering	O
techniques	O
for	O
class-based	O
statistical	O
language	O
modelling	O
.	O
in	O
european	O
conference	O
on	O
speech	O
communication	O
and	O
technology	O
(	O
eurospeech	O
)	O
,	O
pages	O
973–976	O
,	O
berlin	O
.	O
463	O
759	O
bibliography	O
ng	O
,	O
a	O
.	O
(	O
2015	O
)	O
.	O
advice	O
for	O
applying	O
machine	O
learning	O
.	O
https	O
:	O
//see.stanford.edu/materials/aimlcs229/ml-advice.pdf	O
.	O
421	O
niesler	O
,	O
t.	O
r.	O
,	O
whittaker	O
,	O
e.	O
w.	O
d.	O
,	O
and	O
woodland	O
,	O
p.	O
c.	O
(	O
1998	O
)	O
.	O
comparison	O
of	O
part-of-	O
speech	O
and	O
automatically	O
derived	O
category-based	O
language	O
models	O
for	O
speech	O
recognition	B
.	O
in	O
international	O
conference	O
on	O
acoustics	O
,	O
speech	O
and	O
signal	O
processing	O
(	O
icassp	O
)	O
,	O
pages	O
177–180	O
.	O
463	O
ning	O
,	O
f.	O
,	O
delhomme	O
,	O
d.	O
,	O
lecun	O
,	O
y.	O
,	O
piano	O
,	O
f.	O
,	O
bottou	O
,	O
l.	O
,	O
and	O
barbano	O
,	O
p.	O
e.	O
(	O
2005	O
)	O
.	O
toward	O
automatic	O
phenotyping	O
of	O
developing	O
embryos	O
from	O
videos	O
.	O
image	O
processing	O
,	O
ieee	O
transactions	O
on	O
,	O
(	O
9	O
)	O
,	O
1360–1371	O
.	O
360	O
14	O
nocedal	O
,	O
j.	O
and	O
wright	O
,	O
s.	O
(	O
2006	O
)	O
.	O
numerical	O
optimization	O
.	O
springer	O
.	O
,92	O
96	O
norouzi	O
,	O
m.	O
and	O
fleet	O
,	O
d.	O
j	O
.	O
(	O
2011	O
)	O
.	O
minimal	O
loss	O
hashing	O
for	O
compact	O
binary	O
codes	O
.	O
in	O
icml	O
’	O
2011	O
.	O
525	O
nowlan	O
,	O
s.	O
j	O
.	O
(	O
1990	O
)	O
.	O
competing	O
experts	O
:	O
an	O
experimental	O
investigation	O
of	O
associative	O
mixture	O
models	O
.	O
technical	O
report	O
crg-tr-90-5	O
,	O
university	O
of	O
toronto	O
.	O
450	O
nowlan	O
,	O
s.	O
j.	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
1992	O
)	O
.	O
simplifying	O
neural	O
networks	O
by	O
soft	O
weight-sharing	O
.	O
neural	O
computation	O
,	O
4	O
(	O
4	O
)	O
,	O
473–493	O
.	O
139	O
olshausen	O
,	O
b.	O
and	O
field	O
,	O
d.	O
j	O
.	O
(	O
2005	O
)	O
.	O
how	O
close	O
are	O
we	O
to	O
understanding	O
v1	O
?	O
neural	O
computation	O
,	O
17	O
,	O
1665–1699	O
.	O
16	O
olshausen	O
,	O
b.	O
a.	O
and	O
field	O
,	O
d.	O
j	O
.	O
(	O
1996	O
)	O
.	O
emergence	O
of	O
simple-cell	O
receptive	O
ﬁeld	O
properties	O
147	O
255	O
370	O
496	O
by	O
learning	O
a	O
sparse	O
code	O
for	O
natural	O
images	O
.	O
nature	O
,	O
381	O
,	O
607–609	O
.	O
,	O
,	O
,	O
olshausen	O
,	O
b.	O
a.	O
,	O
anderson	O
,	O
c.	O
h.	O
,	O
and	O
van	O
essen	O
,	O
d.	O
c.	O
(	O
1993	O
)	O
.	O
a	O
neurobiological	O
model	B
of	O
visual	O
attention	O
and	O
invariant	O
pattern	O
recognition	B
based	O
on	O
dynamic	O
routing	O
of	O
information	O
.	O
j	O
.	O
neurosci.	O
,	O
(	O
11	O
)	O
,	O
4700–4719	O
.	O
450	O
13	O
opper	O
,	O
m.	O
and	O
archambeau	O
,	O
c.	O
(	O
2009	O
)	O
.	O
the	O
variational	O
gaussian	O
approximation	O
revisited	O
.	O
neural	O
computation	O
,	O
21	O
(	O
3	O
)	O
,	O
786–792	O
.	O
689	O
oquab	O
,	O
m.	O
,	O
bottou	O
,	O
l.	O
,	O
laptev	O
,	O
i.	O
,	O
and	O
sivic	O
,	O
j	O
.	O
(	O
2014	O
)	O
.	O
learning	O
and	O
transferring	O
mid-level	O
image	O
representations	O
using	O
convolutional	O
neural	O
networks	O
.	O
in	O
computer	O
vision	O
and	O
pattern	O
recognition	B
(	O
cvpr	O
)	O
,	O
2014	O
ieee	O
conference	O
on	O
,	O
pages	O
1717–1724	O
.	O
ieee	O
.	O
536	O
osindero	O
,	O
s.	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
2008	O
)	O
.	O
modeling	O
image	O
patches	O
with	O
a	O
directed	O
hierarchy	O
of	O
markov	O
random	O
ﬁelds	O
.	O
in	O
j.	O
platt	O
,	O
d.	O
koller	O
,	O
y.	O
singer	O
,	O
and	O
s.	O
roweis	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
20	O
(	O
nips	O
’	O
07	O
)	O
,	O
pages	O
1121–1128	O
,	O
cambridge	O
,	O
ma	O
.	O
mit	O
press	O
.	O
632	O
ovid	O
and	O
martin	O
,	O
c.	O
(	O
2004	O
)	O
.	O
metamorphoses	O
.	O
w.w.	O
norton	O
.	O
1	O
760	O
bibliography	O
paccanaro	O
,	O
a.	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
2000	O
)	O
.	O
extracting	O
distributed	O
representations	O
of	O
concepts	O
and	O
relations	O
from	O
positive	O
and	O
negative	O
propositions	O
.	O
in	O
international	O
joint	O
conference	O
on	O
neural	O
networks	O
(	O
ijcnn	O
)	O
,	O
como	O
,	O
italy	O
.	O
ieee	O
,	O
new	O
york	O
.	O
484	O
paine	O
,	O
t.	O
l.	O
,	O
khorrami	O
,	O
p.	O
,	O
han	O
,	O
w.	O
,	O
and	O
huang	O
,	O
t.	O
s.	O
(	O
2014	O
)	O
.	O
an	O
analysis	O
of	O
unsupervised	O
pre-training	O
in	O
light	O
of	O
recent	O
advances	O
.	O
arxiv	O
preprint	O
arxiv:1412.6597	O
.	O
532	O
palatucci	O
,	O
m.	O
,	O
pomerleau	O
,	O
d.	O
,	O
hinton	O
,	O
g.	O
e.	O
,	O
and	O
mitchell	O
,	O
t.	O
m.	O
(	O
2009	O
)	O
.	O
zero-shot	O
learning	O
with	O
semantic	O
output	O
codes	O
.	O
in	O
y.	O
bengio	O
,	O
d.	O
schuurmans	O
,	O
j.	O
d.	O
laﬀerty	O
,	O
c.	O
k.	O
i.	O
williams	O
,	O
and	O
a.	O
culotta	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
22	O
,	O
pages	O
1410–1418	O
.	O
curran	O
associates	O
,	O
inc.	O
539	O
parker	O
,	O
d.	O
b	O
.	O
(	O
1985	O
)	O
.	O
learning-logic	O
.	O
technical	O
report	O
tr-47	O
,	O
center	O
for	O
comp	O
.	O
research	O
in	O
economics	O
and	O
management	O
sci.	O
,	O
mit	O
.	O
225	O
pascanu	O
,	O
r.	O
,	O
mikolov	O
,	O
t.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2013	O
)	O
.	O
on	O
the	O
diﬃculty	O
of	O
training	O
recurrent	O
neural	O
networks	O
.	O
in	O
icml	O
’	O
2013	O
289	O
402	O
403	O
408	O
414	O
416	O
.	O
,	O
,	O
,	O
,	O
,	O
pascanu	O
,	O
r.	O
,	O
gülçehre	O
,	O
ç.	O
,	O
cho	O
,	O
k.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2014a	O
)	O
.	O
how	O
to	O
construct	O
deep	O
recurrent	O
neural	O
networks	O
.	O
in	O
iclr	O
’	O
2014	O
19	O
265	O
398	O
399	O
410	O
460	O
.	O
,	O
,	O
,	O
,	O
,	O
pascanu	O
,	O
r.	O
,	O
montufar	O
,	O
g.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2014b	O
)	O
.	O
on	O
the	O
number	O
of	O
inference	O
regions	O
iclr	O
’	O
2014	O
550	O
of	O
deep	O
feed	O
forward	O
networks	O
with	O
piece-wise	O
linear	O
activations	O
.	O
in	O
.	O
pati	O
,	O
y.	O
,	O
rezaiifar	O
,	O
r.	O
,	O
and	O
krishnaprasad	O
,	O
p.	O
(	O
1993	O
)	O
.	O
orthogonal	O
matching	O
pursuit	O
:	O
recursive	O
function	O
approximation	O
with	O
applications	O
to	O
wavelet	O
decomposition	O
.	O
in	O
pro-	O
ceedings	O
of	O
the	O
27	O
th	O
annual	O
asilomar	O
conference	O
on	O
signals	O
,	O
systems	O
,	O
and	O
computers	O
,	O
pages	O
40–44	O
.	O
255	O
pearl	O
,	O
j	O
.	O
(	O
1985	O
)	O
.	O
bayesian	O
networks	O
:	O
a	O
model	B
of	O
self-activated	O
memory	O
for	O
evidential	O
in	O
proceedings	O
of	O
the	O
7th	O
conference	O
of	O
the	O
cognitive	O
science	O
society	O
,	O
reasoning	O
.	O
university	O
of	O
california	O
,	O
irvine	O
,	O
pages	O
329–334	O
.	O
563	O
pearl	O
,	O
j	O
.	O
(	O
1988	O
)	O
.	O
probabilistic	O
reasoning	O
in	O
intelligent	O
systems	O
:	O
networks	O
of	O
plausible	O
inference	O
.	O
morgan	O
kaufmann	O
.	O
54	O
perron	O
,	O
o	O
.	O
(	O
1907	O
)	O
.	O
zur	O
theorie	O
der	O
matrices	O
.	O
mathematische	O
annalen	O
,	O
64	O
(	O
2	O
)	O
,	O
248–263	O
.	O
597	O
petersen	O
,	O
k.	O
b.	O
and	O
pedersen	O
,	O
m.	O
s.	O
(	O
2006	O
)	O
.	O
the	O
matrix	O
cookbook	O
.	O
version	O
20051003	O
.	O
31	O
peterson	O
,	O
g.	O
b	O
.	O
(	O
2004	O
)	O
.	O
a	O
day	O
of	O
great	O
illumination	O
:	O
b.	O
f.	O
skinner	O
’	O
s	O
discovery	O
of	O
shaping	O
.	O
journal	O
of	O
the	O
experimental	O
analysis	O
of	O
behavior	O
,	O
82	O
(	O
3	O
)	O
,	O
317–328	O
.	O
328	O
pham	O
,	O
d.-t.	O
,	O
garat	O
,	O
p.	O
,	O
and	O
jutten	O
,	O
c.	O
(	O
1992	O
)	O
.	O
separation	O
of	O
a	O
mixture	O
of	O
independent	O
sources	O
through	O
a	O
maximum	O
likelihood	O
approach	O
.	O
in	O
eusipco	O
,	O
pages	O
771–774	O
.	O
491	O
761	O
bibliography	O
pham	O
,	O
p.-h.	O
,	O
jelaca	O
,	O
d.	O
,	O
farabet	O
,	O
c.	O
,	O
martini	O
,	O
b.	O
,	O
lecun	O
,	O
y.	O
,	O
and	O
culurciello	O
,	O
e.	O
(	O
2012	O
)	O
.	O
neuflow	O
:	O
dataﬂow	O
vision	O
processing	O
system-on-a-chip	O
.	O
in	O
circuits	O
and	O
systems	O
(	O
mws-	O
cas	O
)	O
,	O
2012	O
ieee	O
55th	O
international	O
midwest	O
symposium	O
on	O
,	O
pages	O
1044–1047	O
.	O
ieee	O
.	O
451	O
pinheiro	O
,	O
p.	O
h.	O
o.	O
and	O
collobert	O
,	O
r.	O
(	O
2014	O
)	O
.	O
recurrent	O
convolutional	O
neural	O
networks	O
for	O
scene	O
labeling	O
.	O
in	O
icml	O
’	O
2014	O
359	O
.	O
pinheiro	O
,	O
p.	O
h.	O
o.	O
and	O
collobert	O
,	O
r.	O
(	O
2015	O
)	O
.	O
from	O
image-level	O
to	O
pixel-level	O
labeling	O
with	O
convolutional	O
networks	O
.	O
in	O
conference	O
on	O
computer	O
vision	O
and	O
pattern	O
recognition	B
(	O
cvpr	O
)	O
.	O
359	O
pinto	O
,	O
n.	O
,	O
cox	O
,	O
d.	O
d.	O
,	O
and	O
dicarlo	O
,	O
j.	O
j	O
.	O
(	O
2008	O
)	O
.	O
why	O
is	O
real-world	O
visual	O
object	O
recognition	B
hard	O
?	O
plos	O
comput	O
biol	O
,	O
.4	O
456	O
pinto	O
,	O
n.	O
,	O
stone	O
,	O
z.	O
,	O
zickler	O
,	O
t.	O
,	O
and	O
cox	O
,	O
d.	O
(	O
2011	O
)	O
.	O
scaling	O
up	O
biologically-inspired	O
computer	O
vision	O
:	O
a	O
case	O
study	O
in	O
unconstrained	O
face	O
recognition	B
on	O
facebook	O
.	O
in	O
computer	O
vision	O
and	O
pattern	O
recognition	B
workshops	O
(	O
cvprw	O
)	O
,	O
2011	O
ieee	O
computer	O
society	O
conference	O
on	O
,	O
pages	O
35–42	O
.	O
ieee	O
.	O
363	O
pollack	O
,	O
j.	O
b	O
.	O
(	O
1990	O
)	O
.	O
recursive	O
distributed	O
representations	O
.	O
artiﬁcial	O
intelligence	O
,	O
46	O
(	O
1	O
)	O
,	O
77–105	O
.	O
401	O
polyak	O
,	O
b.	O
and	O
juditsky	O
,	O
a	O
.	O
(	O
1992	O
)	O
.	O
acceleration	O
of	O
stochastic	O
approximation	O
by	O
averaging	O
.	O
siam	O
j.	O
control	O
and	O
optimization	O
,	O
30	O
(	O
4	O
)	O
,	O
838–855	O
.	O
322	O
polyak	O
,	O
b.	O
t.	O
(	O
1964	O
)	O
.	O
some	O
methods	O
of	O
speeding	O
up	O
the	O
convergence	O
of	O
iteration	O
methods	O
.	O
ussr	O
computational	O
mathematics	O
and	O
mathematical	O
physics	O
,	O
4	O
(	O
5	O
)	O
,	O
1–17	O
.	O
296	O
poole	O
,	O
b.	O
,	O
sohl-dickstein	O
,	O
j.	O
,	O
and	O
ganguli	O
,	O
s.	O
(	O
2014	O
)	O
.	O
analyzing	O
noise	O
in	O
autoencoders	O
and	O
deep	O
networks	O
.	O
corr	O
abs/1406.1831	O
241	O
,	O
.	O
poon	O
,	O
h.	O
and	O
domingos	O
,	O
p.	O
(	O
2011	O
)	O
.	O
sum-product	O
networks	O
:	O
a	O
new	O
deep	O
architecture	O
.	O
in	O
proceedings	O
of	O
the	O
twenty-seventh	O
conference	O
in	O
uncertainty	O
in	O
artiﬁcial	O
intelligence	O
(	O
uai	O
)	O
,	O
barcelona	O
,	O
spain	O
.	O
554	O
presley	O
,	O
r.	O
k.	O
and	O
haggard	O
,	O
r.	O
l.	O
(	O
1994	O
)	O
.	O
a	O
ﬁxed	O
point	O
implementation	O
of	O
the	O
backpropa-	O
gation	O
learning	O
algorithm	O
.	O
in	O
southeastcon	O
’	O
94	O
.	O
creative	O
technology	O
transfer-a	O
global	O
aﬀair.	O
,	O
proceedings	O
of	O
the	O
1994	O
ieee	O
,	O
pages	O
136–138	O
.	O
ieee	O
.	O
451	O
price	O
,	O
r.	O
(	O
1958	O
)	O
.	O
a	O
useful	O
theorem	O
for	O
nonlinear	O
devices	O
having	O
gaussian	O
inputs	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
4	O
(	O
2	O
)	O
,	O
69–72	O
.	O
689	O
quiroga	O
,	O
r.	O
q.	O
,	O
reddy	O
,	O
l.	O
,	O
kreiman	O
,	O
g.	O
,	O
koch	O
,	O
c.	O
,	O
and	O
fried	O
,	O
i	O
.	O
(	O
2005	O
)	O
.	O
invariant	O
visual	O
representation	O
by	O
single	O
neurons	O
in	O
the	O
human	O
brain	O
.	O
nature	O
,	O
435	O
(	O
7045	O
)	O
,	O
1102–1107	O
.	O
366	O
762	O
bibliography	O
radford	O
,	O
a.	O
,	O
metz	O
,	O
l.	O
,	O
and	O
chintala	O
,	O
s.	O
(	O
2015	O
)	O
.	O
unsupervised	O
representation	O
learning	O
with	O
deep	O
convolutional	O
generative	O
adversarial	O
networks	O
.	O
arxiv	O
preprint	O
arxiv:1511.06434	O
.	O
552	O
701	O
702	O
,	O
,	O
raiko	O
,	O
t.	O
,	O
yao	O
,	O
l.	O
,	O
cho	O
,	O
k.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2014	O
)	O
.	O
iterative	O
neural	O
autoregressive	O
distribution	O
estimator	O
(	O
nade-k	O
)	O
.	O
technical	O
report	O
,	O
arxiv:1406.1485	O
.	O
676	O
709	O
,	O
raina	O
,	O
r.	O
,	O
madhavan	O
,	O
a.	O
,	O
and	O
ng	O
,	O
a.	O
y	O
.	O
(	O
2009	O
)	O
.	O
large-scale	O
deep	O
unsupervised	O
learning	O
using	O
graphics	O
processors	O
.	O
in	O
l.	O
bottou	O
and	O
m.	O
littman	O
,	O
editors	O
,	O
proceedings	O
of	O
the	O
twenty-sixth	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
’	O
09	O
)	O
,	O
pages	O
873–880	O
,	O
new	O
york	O
,	O
ny	O
,	O
usa	O
.	O
acm	O
.	O
,27	O
446	O
ramsey	O
,	O
f.	O
p.	O
(	O
1926	O
)	O
.	O
truth	O
and	O
probability	O
.	O
in	O
r.	O
b.	O
braithwaite	O
,	O
editor	O
,	O
the	O
foundations	O
of	O
mathematics	O
and	O
other	O
logical	O
essays	O
,	O
chapter	O
7	O
,	O
pages	O
156–198	O
.	O
mcmaster	O
university	O
archive	O
for	O
the	O
history	O
of	O
economic	O
thought	O
.	O
56	O
ranzato	O
,	O
m.	O
and	O
hinton	O
,	O
g.	O
h.	O
(	O
2010	O
)	O
.	O
modeling	O
pixel	O
means	O
and	O
covariances	O
using	O
factorized	O
third-order	O
boltzmann	O
machines	O
.	O
in	O
cvpr	O
’	O
2010	O
,	O
pages	O
2551–2558	O
.	O
680	O
ranzato	O
,	O
m.	O
,	O
poultney	O
,	O
c.	O
,	O
chopra	O
,	O
s.	O
,	O
and	O
lecun	O
,	O
y	O
.	O
(	O
2007a	O
)	O
.	O
eﬃcient	O
learning	O
of	O
sparse	O
representations	O
with	O
an	O
energy-based	O
model	B
.	O
in	O
nips	O
’	O
2006	O
14	O
19	O
507	O
528	O
530	O
.	O
,	O
,	O
,	O
,	O
ranzato	O
,	O
m.	O
,	O
huang	O
,	O
f.	O
,	O
boureau	O
,	O
y.	O
,	O
and	O
lecun	O
,	O
y	O
.	O
(	O
2007b	O
)	O
.	O
unsupervised	O
learning	O
of	O
invariant	O
feature	O
hierarchies	O
with	O
applications	O
to	O
object	O
recognition	B
.	O
in	O
proceedings	O
of	O
the	O
computer	O
vision	O
and	O
pattern	O
recognition	B
conference	O
(	O
cvpr	O
’	O
07	O
)	O
.	O
ieee	O
press	O
.	O
364	O
ranzato	O
,	O
m.	O
,	O
boureau	O
,	O
y.	O
,	O
and	O
lecun	O
,	O
y	O
.	O
(	O
2008	O
)	O
.	O
sparse	O
feature	O
learning	O
for	O
deep	O
belief	O
networks	O
.	O
in	O
nips	O
’	O
2007	O
507	O
.	O
ranzato	O
,	O
m.	O
,	O
krizhevsky	O
,	O
a.	O
,	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
2010a	O
)	O
.	O
factored	O
3-way	O
restricted	O
boltzmann	O
machines	O
for	O
modeling	O
natural	O
images	O
.	O
in	O
proceedings	O
of	O
aistats	O
2010	O
.	O
678	O
679	O
,	O
ranzato	O
,	O
m.	O
,	O
mnih	O
,	O
v.	O
,	O
and	O
hinton	O
,	O
g.	O
(	O
2010b	O
)	O
.	O
generating	O
more	O
realistic	O
images	O
using	O
gated	O
mrfs	O
.	O
in	O
nips	O
’	O
2010	O
680	O
.	O
rao	O
,	O
c.	O
(	O
1945	O
)	O
.	O
information	O
and	O
the	O
accuracy	O
attainable	O
in	O
the	O
estimation	O
of	O
statistical	O
parameters	O
.	O
bulletin	O
of	O
the	O
calcutta	O
mathematical	O
society	O
,	O
37	O
,	O
81–89	O
.	O
135	O
295	O
,	O
rasmus	O
,	O
a.	O
,	O
valpola	O
,	O
h.	O
,	O
honkala	O
,	O
m.	O
,	O
berglund	O
,	O
m.	O
,	O
and	O
raiko	O
,	O
t.	O
(	O
2015	O
)	O
.	O
semi-supervised	O
learning	O
with	O
ladder	O
network	O
.	O
arxiv	O
preprint	O
arxiv:1507.02672	O
.	O
426	O
530	O
,	O
recht	O
,	O
b.	O
,	O
re	O
,	O
c.	O
,	O
wright	O
,	O
s.	O
,	O
and	O
niu	O
,	O
f.	O
(	O
2011	O
)	O
.	O
hogwild	O
:	O
a	O
lock-free	O
approach	O
to	O
parallelizing	O
stochastic	O
gradient	O
descent	B
.	O
in	O
nips	O
’	O
2011	O
447	O
.	O
reichert	O
,	O
d.	O
p.	O
,	O
seriès	O
,	O
p.	O
,	O
and	O
storkey	O
,	O
a.	O
j	O
.	O
(	O
2011	O
)	O
.	O
neuronal	O
adaptation	O
for	O
sampling-	O
based	O
probabilistic	O
inference	O
in	O
perceptual	O
bistability	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
,	O
pages	O
2357–2365	O
.	O
666	O
763	O
bibliography	O
rezende	O
,	O
d.	O
j.	O
,	O
mohamed	O
,	O
s.	O
,	O
and	O
wierstra	O
,	O
d.	O
(	O
2014	O
)	O
.	O
stochastic	O
backpropagation	O
.	O
preprint	O
:	O
and	O
approximate	O
inference	O
in	O
deep	O
generative	O
models	O
.	O
arxiv:1401.4082.	O
icml	O
’	O
2014	O
in	O
652	O
689	O
696	O
,	O
,	O
rifai	O
,	O
s.	O
,	O
vincent	O
,	O
p.	O
,	O
muller	O
,	O
x.	O
,	O
glorot	O
,	O
x.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2011a	O
)	O
.	O
contractive	O
icml	O
’	O
2011	O
521	O
522	O
,	O
auto-encoders	O
:	O
explicit	O
invariance	O
during	O
feature	O
extraction	O
.	O
in	O
523	O
.	O
,	O
rifai	O
,	O
s.	O
,	O
mesnil	O
,	O
g.	O
,	O
vincent	O
,	O
p.	O
,	O
muller	O
,	O
x.	O
,	O
bengio	O
,	O
y.	O
,	O
dauphin	O
,	O
y.	O
,	O
and	O
glorot	O
,	O
x	O
.	O
(	O
2011b	O
)	O
.	O
higher	O
order	O
contractive	O
auto-encoder	O
.	O
in	O
ecml	O
pkdd	O
521	O
522	O
.	O
,	O
rifai	O
,	O
s.	O
,	O
dauphin	O
,	O
y.	O
,	O
vincent	O
,	O
p.	O
,	O
bengio	O
,	O
y.	O
,	O
and	O
muller	O
,	O
x	O
.	O
(	O
2011c	O
)	O
.	O
the	O
manifold	O
tangent	O
classiﬁer	O
.	O
in	O
nips	O
’	O
2011	O
271	O
272	O
523	O
.	O
,	O
,	O
rifai	O
,	O
s.	O
,	O
bengio	O
,	O
y.	O
,	O
dauphin	O
,	O
y.	O
,	O
and	O
vincent	O
,	O
p.	O
(	O
2012	O
)	O
.	O
a	O
generative	O
process	O
for	O
sampling	O
contractive	O
auto-encoders	O
.	O
in	O
icml	O
’	O
2012	O
711	O
.	O
ringach	O
,	O
d.	O
and	O
shapley	O
,	O
r.	O
(	O
2004	O
)	O
.	O
reverse	O
correlation	O
in	O
neurophysiology	O
.	O
cognitive	O
science	O
,	O
28	O
(	O
2	O
)	O
,	O
147–166	O
.	O
368	O
roberts	O
,	O
s.	O
and	O
everson	O
,	O
r.	O
(	O
2001	O
)	O
.	O
independent	O
component	O
analysis	O
:	O
principles	O
and	O
practice	O
.	O
cambridge	O
university	O
press	O
.	O
493	O
robinson	O
,	O
a.	O
j.	O
and	O
fallside	O
,	O
f.	O
(	O
1991	O
)	O
.	O
a	O
recurrent	O
error	O
propagation	O
network	O
speech	O
recognition	B
system	O
.	O
computer	O
speech	O
and	O
language	O
,	O
5	O
(	O
3	O
)	O
,	O
259–274	O
.	O
27	O
459	O
,	O
rockafellar	O
,	O
r.	O
t.	O
(	O
1997	O
)	O
.	O
convex	O
analysis	O
.	O
princeton	O
landmarks	O
in	O
mathematics	O
.	O
93	O
romero	O
,	O
a.	O
,	O
ballas	O
,	O
n.	O
,	O
ebrahimi	O
kahou	O
,	O
s.	O
,	O
chassang	O
,	O
a.	O
,	O
gatta	O
,	O
c.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2015	O
)	O
.	O
fitnets	O
:	O
hints	O
for	O
thin	O
deep	O
nets	O
.	O
in	O
iclr	O
’	O
2015	O
,	O
arxiv:1412.6550	O
325	O
.	O
rosen	O
,	O
j.	O
b	O
.	O
(	O
1960	O
)	O
.	O
the	O
gradient	O
projection	O
method	O
for	O
nonlinear	O
programming	O
.	O
part	O
i.	O
linear	O
constraints	O
.	O
journal	O
of	O
the	O
society	O
for	O
industrial	O
and	O
applied	O
mathematics	O
,	O
8	O
(	O
1	O
)	O
,	O
pp	O
.	O
181–217	O
.	O
93	O
rosenblatt	O
,	O
f.	O
(	O
1958	O
)	O
.	O
the	O
perceptron	O
:	O
a	O
probabilistic	O
model	B
for	O
information	O
storage	O
and	O
organization	O
in	O
the	O
brain	O
.	O
psychological	O
review	O
,	O
65	O
,	O
386–408	O
.	O
14	O
15	O
27	O
,	O
,	O
rosenblatt	O
,	O
f.	O
(	O
1962	O
)	O
.	O
principles	O
of	O
neurodynamics	O
.	O
spartan	O
,	O
new	O
york	O
.	O
,15	O
27	O
roweis	O
,	O
s.	O
and	O
saul	O
,	O
l.	O
k.	O
(	O
2000	O
)	O
.	O
nonlinear	O
dimensionality	O
reduction	O
by	O
locally	O
linear	O
embedding	O
.	O
science	O
,	O
290	O
(	O
5500	O
)	O
.	O
164	O
518	O
,	O
roweis	O
,	O
s.	O
,	O
saul	O
,	O
l.	O
,	O
and	O
hinton	O
,	O
g.	O
(	O
2002	O
)	O
.	O
global	O
coordination	O
of	O
local	O
linear	O
models	O
.	O
in	O
t.	O
dietterich	O
,	O
s.	O
becker	O
,	O
and	O
z.	O
ghahramani	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
14	O
(	O
nips	O
’	O
01	O
)	O
,	O
cambridge	O
,	O
ma	O
.	O
mit	O
press	O
.	O
489	O
rubin	O
,	O
d.	O
b.	O
et	O
al	O
.	O
(	O
1984	O
)	O
.	O
bayesianly	O
justiﬁable	O
and	O
relevant	O
frequency	O
calculations	O
for	O
the	O
applied	O
statistician	O
.	O
the	O
annals	O
of	O
statistics	O
12	O
,	O
(	O
4	O
)	O
,	O
1151–1172	O
.	O
717	O
764	O
bibliography	O
rumelhart	O
,	O
d.	O
,	O
hinton	O
,	O
g.	O
,	O
and	O
williams	O
,	O
r.	O
(	O
1986a	O
)	O
.	O
learning	O
representations	O
by	O
back-propagating	O
errors	O
.	O
nature	O
,	O
323	O
,	O
533–536	O
.	O
14	O
18	O
23	O
204	O
225	O
373	O
476	O
482	O
,	O
,	O
,	O
,	O
,	O
,	O
,	O
rumelhart	O
,	O
d.	O
e.	O
,	O
hinton	O
,	O
g.	O
e.	O
,	O
and	O
williams	O
,	O
r.	O
j	O
.	O
(	O
1986b	O
)	O
.	O
learning	O
internal	O
represen-	O
tations	O
by	O
error	O
propagation	O
.	O
in	O
d.	O
e.	O
rumelhart	O
and	O
j.	O
l.	O
mcclelland	O
,	O
editors	O
,	O
parallel	O
distributed	O
processing	O
,	O
volume	O
1	O
,	O
chapter	O
8	O
,	O
pages	O
318–362	O
.	O
mit	O
press	O
,	O
cambridge	O
.	O
,21	O
27	O
225	O
,	O
rumelhart	O
,	O
d.	O
e.	O
,	O
mcclelland	O
,	O
j.	O
l.	O
,	O
and	O
the	O
pdp	O
research	O
group	O
(	O
1986c	O
)	O
.	O
parallel	O
distributed	O
processing	O
:	O
explorations	O
in	O
the	O
microstructure	O
of	O
cognition	O
.	O
mit	O
press	O
,	O
cambridge	O
.	O
17	O
russakovsky	O
,	O
o.	O
,	O
deng	O
,	O
j.	O
,	O
su	O
,	O
h.	O
,	O
krause	O
,	O
j.	O
,	O
satheesh	O
,	O
s.	O
,	O
ma	O
,	O
s.	O
,	O
huang	O
,	O
z.	O
,	O
karpathy	O
,	O
a.	O
,	O
khosla	O
,	O
a.	O
,	O
bernstein	O
,	O
m.	O
,	O
berg	O
,	O
a.	O
c.	O
,	O
and	O
fei-fei	O
,	O
l.	O
(	O
2014a	O
)	O
.	O
imagenet	O
large	O
scale	O
visual	O
recognition	B
challenge	O
.	O
21	O
russakovsky	O
,	O
o.	O
,	O
deng	O
,	O
j.	O
,	O
su	O
,	O
h.	O
,	O
krause	O
,	O
j.	O
,	O
satheesh	O
,	O
s.	O
,	O
ma	O
,	O
s.	O
,	O
huang	O
,	O
z.	O
,	O
karpathy	O
,	O
(	O
2014b	O
)	O
.	O
imagenet	O
large	O
scale	O
visual	O
recognition	B
et	O
al	O
.	O
a.	O
,	O
khosla	O
,	O
a.	O
,	O
bernstein	O
,	O
m.	O
,	O
challenge	O
.	O
arxiv	O
preprint	O
arxiv:1409.0575	O
.	O
28	O
russel	O
,	O
s.	O
j.	O
and	O
norvig	O
,	O
p.	O
(	O
2003	O
)	O
.	O
artiﬁcial	O
intelligence	O
:	O
a	O
modern	O
approach	O
.	O
prentice	O
hall	O
.	O
86	O
rust	O
,	O
n.	O
,	O
schwartz	O
,	O
o.	O
,	O
movshon	O
,	O
j.	O
a.	O
,	O
and	O
simoncelli	O
,	O
e.	O
(	O
2005	O
)	O
.	O
spatiotemporal	O
elements	O
of	O
macaque	O
v1	O
receptive	O
ﬁelds	O
.	O
neuron	O
,	O
46	O
(	O
6	O
)	O
,	O
945–956	O
.	O
367	O
sainath	O
,	O
t.	O
,	O
mohamed	O
,	O
a.	O
,	O
kingsbury	O
,	O
b.	O
,	O
and	O
ramabhadran	O
,	O
b	O
.	O
(	O
2013	O
)	O
.	O
deep	O
convolu-	O
tional	O
neural	O
networks	O
for	O
lvcsr	O
.	O
in	O
icassp	O
2013	O
460	O
.	O
salakhutdinov	O
,	O
r.	O
(	O
2010	O
)	O
.	O
learning	O
in	O
markov	O
random	O
ﬁelds	O
using	O
tempered	O
transitions	O
.	O
in	O
y.	O
bengio	O
,	O
d.	O
schuurmans	O
,	O
c.	O
williams	O
,	O
j.	O
laﬀerty	O
,	O
and	O
a.	O
culotta	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
22	O
(	O
nips	O
’	O
09	O
)	O
.	O
603	O
salakhutdinov	O
,	O
r.	O
and	O
hinton	O
,	O
g.	O
(	O
2009a	O
)	O
.	O
deep	O
boltzmann	O
machines	O
.	O
in	O
proceedings	O
of	O
the	O
international	O
conference	O
on	O
artiﬁcial	O
intelligence	O
and	O
statistics	O
,	O
volume	O
5	O
,	O
pages	O
448–455	O
.	O
24	O
27	O
529	O
663	O
666	O
671	O
672	O
,	O
,	O
,	O
,	O
,	O
,	O
salakhutdinov	O
,	O
r.	O
and	O
hinton	O
,	O
g.	O
(	O
2009b	O
)	O
.	O
semantic	O
hashing	O
.	O
in	O
international	O
journal	O
of	O
approximate	O
reasoning	O
.	O
525	O
salakhutdinov	O
,	O
r.	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
2007a	O
)	O
.	O
learning	O
a	O
nonlinear	O
embedding	O
by	O
preserving	O
class	O
neighbourhood	O
structure	O
.	O
in	O
proceedings	O
of	O
the	O
eleventh	O
international	O
conference	O
on	O
artiﬁcial	O
intelligence	O
and	O
statistics	O
(	O
aistats	O
’	O
07	O
)	O
,	O
san	O
juan	O
,	O
porto	O
rico	O
.	O
omnipress	O
.	O
527	O
salakhutdinov	O
,	O
r.	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
2007b	O
)	O
.	O
semantic	O
hashing	O
.	O
in	O
sigir	O
’	O
2007	O
525	O
.	O
765	O
bibliography	O
salakhutdinov	O
,	O
r.	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
2008	O
)	O
.	O
using	O
deep	O
belief	O
nets	O
to	O
learn	O
covariance	O
kernels	O
for	O
gaussian	O
processes	O
.	O
in	O
j.	O
platt	O
,	O
d.	O
koller	O
,	O
y.	O
singer	O
,	O
and	O
s.	O
roweis	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
20	O
(	O
nips	O
’	O
07	O
)	O
,	O
pages	O
1249–1256	O
,	O
cambridge	O
,	O
ma	O
.	O
mit	O
press	O
.	O
244	O
salakhutdinov	O
,	O
r.	O
and	O
larochelle	O
,	O
h.	O
(	O
2010	O
)	O
.	O
eﬃcient	O
learning	O
of	O
deep	O
boltzmann	O
machines	O
.	O
in	O
proceedings	O
of	O
the	O
thirteenth	O
international	O
conference	O
on	O
artiﬁcial	O
intelligence	O
and	O
statistics	O
(	O
aistats	O
2010	O
)	O
,	O
jmlr	O
w	O
&	O
cp	O
,	O
volume	O
9	O
,	O
pages	O
693–700	O
.	O
652	O
salakhutdinov	O
,	O
r.	O
and	O
mnih	O
,	O
a	O
.	O
(	O
2008	O
)	O
.	O
probabilistic	O
matrix	O
factorization	O
.	O
in	O
nips	O
’	O
2008	O
.	O
480	O
salakhutdinov	O
,	O
r.	O
and	O
murray	O
,	O
i	O
.	O
(	O
2008	O
)	O
.	O
on	O
the	O
quantitative	O
analysis	O
of	O
deep	O
belief	O
networks	O
.	O
in	O
w.	O
w.	O
cohen	O
,	O
a.	O
mccallum	O
,	O
and	O
s.	O
t.	O
roweis	O
,	O
editors	O
,	O
proceedings	O
of	O
the	O
twenty-ﬁfth	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
’	O
08	O
)	O
,	O
volume	O
25	O
,	O
pages	O
872–879	O
.	O
acm	O
.	O
628	O
662	O
,	O
salakhutdinov	O
,	O
r.	O
,	O
mnih	O
,	O
a.	O
,	O
and	O
hinton	O
,	O
g.	O
(	O
2007	O
)	O
.	O
restricted	O
boltzmann	O
machines	O
for	O
collaborative	O
ﬁltering	O
.	O
in	O
icml	O
480	O
.	O
sanger	O
,	O
t.	O
d.	O
(	O
1994	O
)	O
.	O
neural	O
network	O
learning	O
control	O
of	O
robot	O
manipulators	O
using	O
gradually	O
increasing	O
task	O
diﬃculty	O
.	O
ieee	O
transactions	O
on	O
robotics	O
and	O
automation	O
,	O
10	O
(	O
3	O
)	O
.	O
328	O
saul	O
,	O
l.	O
k.	O
and	O
jordan	O
,	O
m.	O
i	O
.	O
(	O
1996	O
)	O
.	O
exploiting	O
tractable	O
substructures	O
in	O
intractable	O
networks	O
.	O
in	O
d.	O
touretzky	O
,	O
m.	O
mozer	O
,	O
and	O
m.	O
hasselmo	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
8	O
(	O
nips	O
’	O
95	O
)	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
638	O
saul	O
,	O
l.	O
k.	O
,	O
jaakkola	O
,	O
t.	O
,	O
and	O
jordan	O
,	O
m.	O
i	O
.	O
(	O
1996	O
)	O
.	O
mean	O
ﬁeld	O
theory	O
for	O
sigmoid	O
belief	O
networks	O
.	O
journal	O
of	O
artiﬁcial	O
intelligence	O
research	O
,	O
4	O
,	O
61–76	O
.	O
27	O
693	O
,	O
savich	O
,	O
a.	O
w.	O
,	O
moussa	O
,	O
m.	O
,	O
and	O
areibi	O
,	O
s.	O
(	O
2007	O
)	O
.	O
the	O
impact	O
of	O
arithmetic	O
representation	O
on	O
implementing	O
mlp-bp	O
on	O
fpgas	O
:	O
a	O
study	O
.	O
neural	O
networks	O
,	O
ieee	O
transactions	O
on	O
,	O
18	O
(	O
1	O
)	O
,	O
240–252	O
.	O
451	O
saxe	O
,	O
a.	O
m.	O
,	O
koh	O
,	O
p.	O
w.	O
,	O
chen	O
,	O
z.	O
,	O
bhand	O
,	O
m.	O
,	O
suresh	O
,	O
b.	O
,	O
and	O
ng	O
,	O
a	O
.	O
(	O
2011	O
)	O
.	O
on	O
random	O
weights	O
and	O
unsupervised	O
feature	O
learning	O
.	O
in	O
proc	O
.	O
icml	O
’	O
2011	O
.	O
acm	O
.	O
363	O
saxe	O
,	O
a.	O
m.	O
,	O
mcclelland	O
,	O
j.	O
l.	O
,	O
and	O
ganguli	O
,	O
s.	O
(	O
2013	O
)	O
.	O
exact	O
solutions	O
to	O
the	O
nonlinear	O
dynamics	O
of	O
learning	O
in	O
deep	O
linear	O
neural	O
networks	O
.	O
in	O
iclr	O
285	O
286	O
303	O
.	O
,	O
,	O
schaul	O
,	O
t.	O
,	O
antonoglou	O
,	O
i.	O
,	O
and	O
silver	O
,	O
d.	O
(	O
2014	O
)	O
.	O
unit	O
tests	O
for	O
stochastic	O
optimization	O
.	O
in	O
international	O
conference	O
on	O
learning	O
representations	O
.	O
309	O
schmidhuber	O
,	O
j	O
.	O
(	O
1992	O
)	O
.	O
learning	O
complex	O
,	O
extended	O
sequences	O
using	O
the	O
principle	O
of	O
history	O
compression	O
.	O
neural	O
computation	O
,	O
4	O
(	O
2	O
)	O
,	O
234–242	O
.	O
398	O
schmidhuber	O
,	O
j	O
.	O
(	O
1996	O
)	O
.	O
sequential	O
neural	O
text	O
compression	O
.	O
ieee	O
transactions	O
on	O
neural	O
networks	O
,	O
7	O
(	O
1	O
)	O
,	O
142–146	O
.	O
477	O
766	O
bibliography	O
schmidhuber	O
,	O
j	O
.	O
(	O
2012	O
)	O
.	O
self-delimiting	O
neural	O
networks	O
.	O
arxiv	O
preprint	O
arxiv:1210.0118	O
.	O
390	O
schölkopf	O
,	O
b.	O
and	O
smola	O
,	O
a.	O
j	O
.	O
(	O
2002	O
)	O
.	O
learning	O
with	O
kernels	O
:	O
support	O
vector	O
machines	O
,	O
regularization	O
,	O
optimization	O
,	O
and	O
beyond	O
.	O
mit	O
press	O
.	O
704	O
schölkopf	O
,	O
b.	O
,	O
smola	O
,	O
a.	O
,	O
and	O
müller	O
,	O
k.-r.	O
(	O
1998	O
)	O
.	O
nonlinear	O
component	O
analysis	O
as	O
a	O
kernel	O
eigenvalue	O
problem	O
.	O
neural	O
computation	O
,	O
10	O
,	O
1299–1319	O
.	O
164	O
518	O
,	O
schölkopf	O
,	O
b.	O
,	O
burges	O
,	O
c.	O
j.	O
c.	O
,	O
and	O
smola	O
,	O
a.	O
j	O
.	O
(	O
1999	O
)	O
.	O
advances	O
in	O
kernel	O
methods	O
—	O
support	O
vector	O
learning	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
,18	O
142	O
schölkopf	O
,	O
b.	O
,	O
janzing	O
,	O
d.	O
,	O
peters	O
,	O
j.	O
,	O
sgouritsa	O
,	O
e.	O
,	O
zhang	O
,	O
k.	O
,	O
and	O
mooij	O
,	O
j	O
.	O
(	O
2012	O
)	O
.	O
on	O
causal	O
and	O
anticausal	O
learning	O
.	O
in	O
icml	O
’	O
2012	O
,	O
pages	O
1255–1262	O
.	O
545	O
schuster	O
,	O
m.	O
(	O
1999	O
)	O
.	O
on	O
supervised	O
learning	O
from	O
sequential	O
data	O
with	O
applications	O
for	O
speech	O
recognition	B
.	O
190	O
schuster	O
,	O
m.	O
and	O
paliwal	O
,	O
k.	O
(	O
1997	O
)	O
.	O
bidirectional	O
recurrent	O
neural	O
networks	O
.	O
ieee	O
transactions	O
on	O
signal	O
processing	O
,	O
45	O
(	O
11	O
)	O
,	O
2673–2681	O
.	O
395	O
schwenk	O
,	O
h.	O
(	O
2007	O
)	O
.	O
continuous	O
space	O
language	O
models	O
.	O
computer	O
speech	O
and	O
language	O
,	O
21	O
,	O
492–518	O
.	O
466	O
schwenk	O
,	O
h.	O
(	O
2010	O
)	O
.	O
continuous	O
space	O
language	O
models	O
for	O
statistical	O
machine	O
translation	O
.	O
the	O
prague	O
bulletin	O
of	O
mathematical	O
linguistics	O
,	O
93	O
,	O
137–146	O
.	O
473	O
schwenk	O
,	O
h.	O
(	O
2014	O
)	O
.	O
cleaned	O
subset	O
of	O
wmt	O
’	O
14	O
dataset	O
.	O
21	O
schwenk	O
,	O
h.	O
and	O
bengio	O
,	O
y	O
.	O
(	O
1998	O
)	O
.	O
training	O
methods	O
for	O
adaptive	O
boosting	O
of	O
neural	O
net-	O
works	O
.	O
in	O
m.	O
jordan	O
,	O
m.	O
kearns	O
,	O
and	O
s.	O
solla	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
10	O
(	O
nips	O
’	O
97	O
)	O
,	O
pages	O
647–653	O
.	O
mit	O
press	O
.	O
258	O
schwenk	O
,	O
h.	O
and	O
gauvain	O
,	O
j.-l.	O
(	O
2002	O
)	O
.	O
connectionist	O
language	O
modeling	O
for	O
large	O
vocabulary	O
continuous	O
speech	O
recognition	B
.	O
in	O
international	O
conference	O
on	O
acoustics	O
,	O
speech	O
and	O
signal	O
processing	O
(	O
icassp	O
)	O
,	O
pages	O
765–768	O
,	O
orlando	O
,	O
florida	O
.	O
466	O
schwenk	O
,	O
h.	O
,	O
costa-jussà	O
,	O
m.	O
r.	O
,	O
and	O
fonollosa	O
,	O
j.	O
a.	O
r.	O
(	O
2006	O
)	O
.	O
continuous	O
space	O
in	O
international	O
workshop	O
on	O
spoken	O
language	O
models	O
for	O
the	O
iwslt	O
2006	O
task	O
.	O
language	O
translation	O
,	O
pages	O
166–173	O
.	O
473	O
seide	O
,	O
f.	O
,	O
li	O
,	O
g.	O
,	O
and	O
yu	O
,	O
d.	O
(	O
2011	O
)	O
.	O
conversational	O
speech	O
transcription	O
using	O
context-	O
dependent	O
deep	O
neural	O
networks	O
.	O
in	O
interspeech	O
2011	O
,	O
pages	O
437–440	O
.	O
23	O
sejnowski	O
,	O
t.	O
(	O
1987	O
)	O
.	O
higher-order	O
boltzmann	O
machines	O
.	O
in	O
aip	O
conference	O
proceedings	O
151	O
on	O
neural	O
networks	O
for	O
computing	O
,	O
pages	O
398–403	O
.	O
american	O
institute	O
of	O
physics	O
inc.	O
686	O
767	O
bibliography	O
series	O
,	O
p.	O
,	O
reichert	O
,	O
d.	O
p.	O
,	O
and	O
storkey	O
,	O
a.	O
j	O
.	O
(	O
2010	O
)	O
.	O
hallucinations	O
in	O
charles	O
bonnet	O
syndrome	O
induced	O
by	O
homeostasis	O
:	O
a	O
deep	O
boltzmann	O
machine	O
model	B
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
,	O
pages	O
2020–2028	O
.	O
666	O
sermanet	O
,	O
p.	O
,	O
chintala	O
,	O
s.	O
,	O
and	O
lecun	O
,	O
y	O
.	O
(	O
2012	O
)	O
.	O
convolutional	O
neural	O
networks	O
applied	O
to	O
house	O
numbers	O
digit	O
classiﬁcation	O
.	O
corr	O
abs/1204.3968	O
457	O
,	O
.	O
sermanet	O
,	O
p.	O
,	O
kavukcuoglu	O
,	O
k.	O
,	O
chintala	O
,	O
s.	O
,	O
and	O
lecun	O
,	O
y	O
.	O
(	O
2013	O
)	O
.	O
pedestrian	O
detection	O
with	O
unsupervised	O
multi-stage	O
feature	O
learning	O
.	O
in	O
proc	O
.	O
international	O
conference	O
on	O
computer	O
vision	O
and	O
pattern	O
recognition	B
(	O
cvpr	O
’	O
13	O
)	O
.	O
ieee	O
.	O
,23	O
201	O
shilov	O
,	O
g.	O
(	O
1977	O
)	O
.	O
linear	O
algebra	O
.	O
dover	O
books	O
on	O
mathematics	O
series	O
.	O
dover	O
publications	O
.	O
31	O
siegelmann	O
,	O
h.	O
(	O
1995	O
)	O
.	O
computation	O
beyond	O
the	O
turing	O
limit	O
.	O
science	O
,	O
268	O
(	O
5210	O
)	O
,	O
545–548	O
.	O
379	O
siegelmann	O
,	O
h.	O
and	O
sontag	O
,	O
e.	O
(	O
1991	O
)	O
.	O
turing	O
computability	O
with	O
neural	O
nets	O
.	O
applied	O
mathematics	O
letters	O
,	O
4	O
(	O
6	O
)	O
,	O
77–80	O
.	O
379	O
siegelmann	O
,	O
h.	O
t.	O
and	O
sontag	O
,	O
e.	O
d.	O
(	O
1995	O
)	O
.	O
on	O
the	O
computational	O
power	O
of	O
neural	O
nets	O
.	O
journal	O
of	O
computer	O
and	O
systems	O
sciences	O
,	O
50	O
(	O
1	O
)	O
,	O
132–150	O
.	O
379	O
403	O
,	O
sietsma	O
,	O
j.	O
and	O
dow	O
,	O
r.	O
(	O
1991	O
)	O
.	O
creating	O
artiﬁcial	O
neural	O
networks	O
that	O
generalize	O
.	O
neural	O
networks	O
,	O
4	O
(	O
1	O
)	O
,	O
67–79	O
.	O
241	O
simard	O
,	O
d.	O
,	O
steinkraus	O
,	O
p.	O
y.	O
,	O
and	O
platt	O
,	O
j.	O
c.	O
(	O
2003	O
)	O
.	O
best	O
practices	O
for	O
convolutional	O
neural	O
networks	O
.	O
in	O
icdar	O
’	O
2003	O
371	O
.	O
simard	O
,	O
p.	O
and	O
graf	O
,	O
h.	O
p.	O
(	O
1994	O
)	O
.	O
backpropagation	O
without	O
multiplication	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
,	O
pages	O
232–239	O
.	O
451	O
simard	O
,	O
p.	O
,	O
victorri	O
,	O
b.	O
,	O
lecun	O
,	O
y.	O
,	O
and	O
denker	O
,	O
j	O
.	O
(	O
1992	O
)	O
.	O
tangent	O
prop	O
-	O
a	O
formalism	O
nips	O
’	O
1991	O
270	O
271	O
272	O
,	O
for	O
specifying	O
selected	O
invariances	O
in	O
an	O
adaptive	O
network	O
.	O
in	O
356	O
.	O
,	O
,	O
simard	O
,	O
p.	O
y.	O
,	O
lecun	O
,	O
y.	O
,	O
and	O
denker	O
,	O
j	O
.	O
(	O
1993	O
)	O
.	O
eﬃcient	O
pattern	O
recognition	B
using	O
a	O
new	O
transformation	O
distance	O
.	O
in	O
nips	O
’	O
92	O
270	O
.	O
simard	O
,	O
p.	O
y.	O
,	O
lecun	O
,	O
y.	O
a.	O
,	O
denker	O
,	O
j.	O
s.	O
,	O
and	O
victorri	O
,	O
b	O
.	O
(	O
1998	O
)	O
.	O
transformation	O
invariance	O
in	O
pattern	O
recognition	B
—	O
tangent	O
distance	O
and	O
tangent	O
propagation	O
.	O
lecture	O
notes	O
in	O
computer	O
science	O
,	O
1524	O
270	O
.	O
simons	O
,	O
d.	O
j.	O
and	O
levin	O
,	O
d.	O
t.	O
(	O
1998	O
)	O
.	O
failure	O
to	O
detect	O
changes	O
to	O
people	O
during	O
a	O
real-world	O
interaction	O
.	O
psychonomic	O
bulletin	O
&	O
review	O
,	O
5	O
(	O
4	O
)	O
,	O
644–649	O
.	O
543	O
simonyan	O
,	O
k.	O
and	O
zisserman	O
,	O
a	O
.	O
(	O
2015	O
)	O
.	O
very	O
deep	O
convolutional	O
networks	O
for	O
large-scale	O
image	O
recognition	B
.	O
in	O
iclr	O
323	O
.	O
768	O
bibliography	O
sjöberg	O
,	O
j.	O
and	O
ljung	O
,	O
l.	O
(	O
1995	O
)	O
.	O
overtraining	O
,	O
regularization	O
and	O
searching	O
for	O
a	O
minimum	O
,	O
with	O
application	O
to	O
neural	O
networks	O
.	O
international	O
journal	O
of	O
control	O
,	O
62	O
(	O
6	O
)	O
,	O
1391–1407	O
.	O
250	O
skinner	O
,	O
b.	O
f.	O
(	O
1958	O
)	O
.	O
reinforcement	O
today	O
.	O
american	O
psychologist	O
,	O
13	O
,	O
94–99	O
.	O
328	O
smolensky	O
,	O
p.	O
(	O
1986	O
)	O
.	O
information	O
processing	O
in	O
dynamical	O
systems	O
:	O
foundations	O
of	O
harmony	O
theory	O
.	O
in	O
d.	O
e.	O
rumelhart	O
and	O
j.	O
l.	O
mcclelland	O
,	O
editors	O
,	O
parallel	O
distributed	O
processing	O
,	O
volume	O
1	O
,	O
chapter	O
6	O
,	O
pages	O
194–281	O
.	O
mit	O
press	O
,	O
cambridge	O
.	O
571	O
587	O
656	O
,	O
,	O
snoek	O
,	O
j.	O
,	O
larochelle	O
,	O
h.	O
,	O
and	O
adams	O
,	O
r.	O
p.	O
(	O
2012	O
)	O
.	O
practical	O
bayesian	O
optimization	O
of	O
machine	O
learning	O
algorithms	O
.	O
in	O
nips	O
’	O
2012	O
436	O
.	O
socher	O
,	O
r.	O
,	O
huang	O
,	O
e.	O
h.	O
,	O
pennington	O
,	O
j.	O
,	O
ng	O
,	O
a.	O
y.	O
,	O
and	O
manning	O
,	O
c.	O
d.	O
(	O
2011a	O
)	O
.	O
dynamic	O
.	O
nips	O
’	O
2011	O
pooling	O
and	O
unfolding	O
recursive	O
autoencoders	O
for	O
paraphrase	O
detection	O
.	O
in	O
401	O
socher	O
,	O
r.	O
,	O
manning	O
,	O
c.	O
,	O
and	O
ng	O
,	O
a.	O
y	O
.	O
(	O
2011b	O
)	O
.	O
parsing	O
natural	O
scenes	O
and	O
natural	O
lan-	O
guage	O
with	O
recursive	O
neural	O
networks	O
.	O
in	O
proceedings	O
of	O
the	O
twenty-eighth	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
’	O
2011	O
)	O
.	O
401	O
socher	O
,	O
r.	O
,	O
pennington	O
,	O
j.	O
,	O
huang	O
,	O
e.	O
h.	O
,	O
ng	O
,	O
a.	O
y.	O
,	O
and	O
manning	O
,	O
c.	O
d.	O
(	O
2011c	O
)	O
.	O
in	O
semi-supervised	O
recursive	O
autoencoders	O
for	O
predicting	O
sentiment	O
distributions	O
.	O
emnlp	O
’	O
2011	O
.	O
401	O
socher	O
,	O
r.	O
,	O
perelygin	O
,	O
a.	O
,	O
wu	O
,	O
j.	O
y.	O
,	O
chuang	O
,	O
j.	O
,	O
manning	O
,	O
c.	O
d.	O
,	O
ng	O
,	O
a.	O
y.	O
,	O
and	O
potts	O
,	O
c.	O
(	O
2013a	O
)	O
.	O
recursive	O
deep	O
models	O
for	O
semantic	O
compositionality	O
over	O
a	O
sentiment	O
treebank	O
.	O
in	O
emnlp	O
’	O
2013	O
401	O
.	O
socher	O
,	O
r.	O
,	O
ganjoo	O
,	O
m.	O
,	O
manning	O
,	O
c.	O
d.	O
,	O
and	O
ng	O
,	O
a.	O
y	O
.	O
(	O
2013b	O
)	O
.	O
zero-shot	O
learning	O
through	O
cross-modal	O
transfer	O
.	O
in	O
27th	O
annual	O
conference	O
on	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
2013	O
)	O
.	O
539	O
sohl-dickstein	O
,	O
j.	O
,	O
weiss	O
,	O
e.	O
a.	O
,	O
maheswaranathan	O
,	O
n.	O
,	O
and	O
ganguli	O
,	O
s.	O
(	O
2015	O
)	O
.	O
deep	O
unsupervised	O
learning	O
using	O
nonequilibrium	O
thermodynamics	O
.	O
716	O
sohn	O
,	O
k.	O
,	O
zhou	O
,	O
g.	O
,	O
and	O
lee	O
,	O
h.	O
(	O
2013	O
)	O
.	O
learning	O
and	O
selecting	O
features	O
jointly	O
with	O
point-wise	O
gated	O
boltzmann	O
machines	O
.	O
in	O
icml	O
’	O
2013	O
687	O
.	O
solomonoﬀ	O
,	O
r.	O
j	O
.	O
(	O
1989	O
)	O
.	O
a	O
system	O
for	O
incremental	O
learning	O
based	O
on	O
algorithmic	O
proba-	O
bility	O
.	O
328	O
sontag	O
,	O
e.	O
d.	O
(	O
1998	O
)	O
.	O
vc	O
dimension	O
of	O
neural	O
networks	O
.	O
nato	O
asi	O
series	O
f	O
computer	O
and	O
systems	O
sciences	O
,	O
168	O
,	O
69–96	O
.	O
547	O
551	O
,	O
sontag	O
,	O
e.	O
d.	O
and	O
sussman	O
,	O
h.	O
j	O
.	O
(	O
1989	O
)	O
.	O
backpropagation	O
can	O
give	O
rise	O
to	O
spurious	O
local	O
minima	O
even	O
for	O
networks	O
without	O
hidden	O
layers	O
.	O
769	O
complex	O
systems	O
3	O
,	O
,	O
91–106	O
.	O
284	O
bibliography	O
sparkes	O
,	O
b	O
.	O
(	O
1996	O
)	O
.	O
the	O
red	O
and	O
the	O
black	O
:	O
studies	O
in	O
greek	O
pottery	O
.	O
routledge	O
.	O
1	O
spitkovsky	O
,	O
v.	O
i.	O
,	O
alshawi	O
,	O
h.	O
,	O
and	O
jurafsky	O
,	O
d.	O
(	O
2010	O
)	O
.	O
from	O
baby	O
steps	O
to	O
leapfrog	O
:	O
how	O
“	O
less	O
is	O
more	O
”	O
in	O
unsupervised	O
dependency	O
parsing	O
.	O
in	O
hlt	O
’	O
10	O
.	O
328	O
squire	O
,	O
w.	O
and	O
trapp	O
,	O
g.	O
(	O
1998	O
)	O
.	O
using	O
complex	O
variables	O
to	O
estimate	O
derivatives	O
of	O
real	O
functions	O
.	O
siam	O
rev.	O
,	O
40	O
(	O
1	O
)	O
,	O
110––112	O
.	O
439	O
srebro	O
,	O
n.	O
and	O
shraibman	O
,	O
a	O
.	O
(	O
2005	O
)	O
.	O
rank	O
,	O
trace-norm	O
and	O
max-norm	O
.	O
in	O
proceedings	O
of	O
the	O
18th	O
annual	O
conference	O
on	O
learning	O
theory	O
,	O
pages	O
545–560	O
.	O
springer-verlag	O
.	O
238	O
srivastava	O
,	O
n.	O
(	O
2013	O
)	O
.	O
improving	O
neural	O
networks	O
with	O
dropout	O
.	O
master	O
’	O
s	O
thesis	O
,	O
u.	O
toronto	O
.	O
535	O
srivastava	O
,	O
n.	O
and	O
salakhutdinov	O
,	O
r.	O
(	O
2012	O
)	O
.	O
multimodal	O
learning	O
with	O
deep	O
boltzmann	O
machines	O
.	O
in	O
nips	O
’	O
2012	O
541	O
.	O
srivastava	O
,	O
n.	O
,	O
salakhutdinov	O
,	O
r.	O
r.	O
,	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
2013	O
)	O
.	O
modeling	O
documents	O
with	O
deep	O
boltzmann	O
machines	O
.	O
arxiv	O
preprint	O
arxiv:1309.6865	O
.	O
663	O
srivastava	O
,	O
n.	O
,	O
hinton	O
,	O
g.	O
,	O
krizhevsky	O
,	O
a.	O
,	O
sutskever	O
,	O
i.	O
,	O
and	O
salakhutdinov	O
,	O
r.	O
(	O
2014	O
)	O
.	O
dropout	O
:	O
a	O
simple	O
way	O
to	O
prevent	O
neural	O
networks	O
from	O
overﬁtting	O
.	O
journal	O
of	O
machine	O
learning	O
research	O
,	O
258	O
265	O
267	O
672	O
,	O
1929–1958	O
.	O
15	O
,	O
,	O
,	O
srivastava	O
,	O
r.	O
k.	O
,	O
greﬀ	O
,	O
k.	O
,	O
and	O
schmidhuber	O
,	O
j	O
.	O
(	O
2015	O
)	O
.	O
highway	O
networks	O
.	O
arxiv:1505.00387	O
.	O
326	O
steinkrau	O
,	O
d.	O
,	O
simard	O
,	O
p.	O
y.	O
,	O
and	O
buck	O
,	O
i	O
.	O
(	O
2005	O
)	O
.	O
using	O
gpus	O
for	O
machine	O
learning	O
algorithms	O
.	O
2013	O
12th	O
international	O
conference	O
on	O
document	O
analysis	O
and	O
recognition	B
,	O
0	O
,	O
1115–1119	O
.	O
445	O
stoyanov	O
,	O
v.	O
,	O
ropson	O
,	O
a.	O
,	O
and	O
eisner	O
,	O
j	O
.	O
(	O
2011	O
)	O
.	O
empirical	O
risk	O
minimization	O
of	O
graphical	O
model	B
parameters	O
given	O
approximate	O
inference	O
,	O
decoding	O
,	O
and	O
model	B
structure	O
.	O
in	O
proceedings	O
of	O
the	O
14th	O
international	O
conference	O
on	O
artiﬁcial	O
intelligence	O
and	O
statistics	O
,	O
pages	O
725–733	O
,	O
(	O
aistats	O
)	O
fort	O
lauderdale	O
.	O
supplementary	O
material	O
(	O
4	O
pages	O
)	O
also	O
available	O
.	O
,	O
jmlr	O
workshop	O
and	O
conference	O
proceedings	O
,	O
volume	O
15	O
of	O
674	O
698	O
sukhbaatar	O
,	O
s.	O
,	O
szlam	O
,	O
a.	O
,	O
weston	O
,	O
j.	O
,	O
and	O
fergus	O
,	O
r.	O
(	O
2015	O
)	O
.	O
weakly	O
supervised	O
memory	O
networks	O
.	O
arxiv	O
preprint	O
arxiv:1503.08895	O
.	O
418	O
supancic	O
,	O
j.	O
and	O
ramanan	O
,	O
d.	O
(	O
2013	O
)	O
.	O
self-paced	O
learning	O
for	O
long-term	O
tracking	O
.	O
in	O
cvpr	O
’	O
2013	O
.	O
328	O
sussillo	O
,	O
d.	O
(	O
2014	O
)	O
.	O
random	O
walks	O
:	O
training	O
very	O
deep	O
nonlinear	O
feed-forward	O
networks	O
with	O
smart	O
initialization	O
.	O
corr	O
abs/1412.6558	O
290	O
303	O
305	O
403	O
,	O
.	O
,	O
,	O
,	O
sutskever	O
,	O
i	O
.	O
(	O
2012	O
)	O
.	O
training	O
recurrent	O
neural	O
networks	O
.	O
ph.d.	O
thesis	O
,	O
department	O
of	O
computer	O
science	O
,	O
university	O
of	O
toronto	O
.	O
406	O
413	O
,	O
770	O
bibliography	O
sutskever	O
,	O
i.	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
2008	O
)	O
.	O
deep	O
narrow	O
sigmoid	O
belief	O
networks	O
are	O
universal	O
approximators	O
.	O
neural	O
computation	O
,	O
20	O
(	O
11	O
)	O
,	O
2629–2636	O
.	O
693	O
sutskever	O
,	O
i.	O
and	O
tieleman	O
,	O
t.	O
(	O
2010	O
)	O
.	O
on	O
the	O
convergence	O
properties	O
of	O
contrastive	O
divergence	O
.	O
in	O
y.	O
w.	O
teh	O
and	O
m.	O
titterington	O
,	O
editors	O
,	O
proc	O
.	O
of	O
the	O
international	O
conference	O
on	O
artiﬁcial	O
intelligence	O
and	O
statistics	O
(	O
aistats	O
)	O
,	O
volume	O
9	O
,	O
pages	O
789–795	O
.	O
612	O
sutskever	O
,	O
i.	O
,	O
hinton	O
,	O
g.	O
,	O
and	O
taylor	O
,	O
g.	O
(	O
2009	O
)	O
.	O
the	O
recurrent	O
temporal	O
restricted	O
boltzmann	O
machine	O
.	O
in	O
nips	O
’	O
2008	O
685	O
.	O
sutskever	O
,	O
i.	O
,	O
martens	O
,	O
j.	O
,	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
2011	O
)	O
.	O
generating	O
text	O
with	O
recurrent	O
neural	O
networks	O
.	O
in	O
icml	O
’	O
2011	O
,	O
pages	O
1017–1024	O
.	O
477	O
sutskever	O
,	O
i.	O
,	O
martens	O
,	O
j.	O
,	O
dahl	O
,	O
g.	O
,	O
and	O
hinton	O
,	O
g.	O
(	O
2013	O
)	O
.	O
on	O
the	O
importance	O
of	O
initialization	O
and	O
momentum	O
in	O
deep	O
learning	O
.	O
in	O
icml	O
300	O
406	O
413	O
.	O
,	O
,	O
sutskever	O
,	O
i.	O
,	O
vinyals	O
,	O
o.	O
,	O
and	O
le	O
,	O
q.	O
v.	O
(	O
2014	O
)	O
.	O
sequence	O
to	O
sequence	O
learning	O
with	O
neural	O
networks	O
.	O
in	O
nips	O
’	O
2014	O
,	O
arxiv:1409.3215	O
25	O
101	O
397	O
410	O
411	O
474	O
475	O
.	O
,	O
,	O
,	O
,	O
,	O
,	O
sutton	O
,	O
r.	O
and	O
barto	O
,	O
a	O
.	O
(	O
1998	O
)	O
.	O
reinforcement	O
learning	O
:	O
an	O
introduction	O
.	O
mit	O
press	O
.	O
106	O
sutton	O
,	O
r.	O
s.	O
,	O
mcallester	O
,	O
d.	O
,	O
singh	O
,	O
s.	O
,	O
and	O
mansour	O
,	O
y	O
.	O
(	O
2000	O
)	O
.	O
policy	O
gradient	O
methods	O
,	O
pages	O
1057–	O
for	O
reinforcement	O
learning	O
with	O
function	O
approximation	O
.	O
in	O
–1063	O
.	O
mit	O
press	O
.	O
691	O
nips	O
’	O
1999	O
swersky	O
,	O
k.	O
,	O
ranzato	O
,	O
m.	O
,	O
buchman	O
,	O
d.	O
,	O
marlin	O
,	O
b.	O
,	O
and	O
de	O
freitas	O
,	O
n.	O
(	O
2011	O
)	O
.	O
on	O
513	O
autoencoders	O
and	O
score	O
matching	O
for	O
energy	O
based	O
models	O
.	O
in	O
icml	O
’	O
2011	O
.	O
acm	O
.	O
swersky	O
,	O
k.	O
,	O
snoek	O
,	O
j.	O
,	O
and	O
adams	O
,	O
r.	O
p.	O
(	O
2014	O
)	O
.	O
freeze-thaw	O
bayesian	O
optimization	O
.	O
arxiv	O
preprint	O
arxiv:1406.3896	O
.	O
436	O
szegedy	O
,	O
c.	O
,	O
liu	O
,	O
w.	O
,	O
jia	O
,	O
y.	O
,	O
sermanet	O
,	O
p.	O
,	O
reed	O
,	O
s.	O
,	O
anguelov	O
,	O
d.	O
,	O
erhan	O
,	O
d.	O
,	O
vanhoucke	O
,	O
v.	O
,	O
and	O
rabinovich	O
,	O
a	O
.	O
(	O
2014a	O
)	O
.	O
going	O
deeper	O
with	O
convolutions	O
.	O
technical	O
report	O
,	O
arxiv:1409.4842	O
.	O
24	O
27	O
201	O
258	O
269	O
326	O
347	O
,	O
,	O
,	O
,	O
,	O
,	O
szegedy	O
,	O
c.	O
,	O
zaremba	O
,	O
w.	O
,	O
sutskever	O
,	O
i.	O
,	O
bruna	O
,	O
j.	O
,	O
erhan	O
,	O
d.	O
,	O
goodfellow	O
,	O
i.	O
j.	O
,	O
and	O
iclr	O
abs/1312.6199	O
.	O
fergus	O
,	O
r.	O
(	O
2014b	O
)	O
.	O
intriguing	O
properties	O
of	O
neural	O
networks	O
.	O
268	O
271	O
,	O
,	O
szegedy	O
,	O
c.	O
,	O
vanhoucke	O
,	O
v.	O
,	O
ioﬀe	O
,	O
s.	O
,	O
shlens	O
,	O
j.	O
,	O
and	O
wojna	O
,	O
z	O
.	O
(	O
2015	O
)	O
.	O
rethinking	O
the	O
inception	O
architecture	O
for	O
computer	O
vision	O
.	O
arxiv	O
e-prints	O
243	O
322	O
.	O
,	O
taigman	O
,	O
y.	O
,	O
yang	O
,	O
m.	O
,	O
ranzato	O
,	O
m.	O
,	O
and	O
wolf	O
,	O
l.	O
(	O
2014	O
)	O
.	O
deepface	O
:	O
closing	O
the	O
gap	O
to	O
human-level	O
performance	O
in	O
face	O
veriﬁcation	O
.	O
in	O
cvpr	O
’	O
2014	O
100	O
.	O
tandy	O
,	O
d.	O
w.	O
(	O
1997	O
)	O
.	O
works	O
and	O
days	O
:	O
a	O
translation	O
and	O
commentary	O
for	O
the	O
social	O
sciences	O
.	O
university	O
of	O
california	O
press	O
.	O
1	O
771	O
bibliography	O
tang	O
,	O
y.	O
and	O
eliasmith	O
,	O
c.	O
(	O
2010	O
)	O
.	O
deep	O
networks	O
for	O
robust	O
visual	O
recognition	B
.	O
in	O
proceedings	O
of	O
the	O
27th	O
international	O
conference	O
on	O
machine	O
learning	O
,	O
june	O
21-24	O
,	O
2010	O
,	O
haifa	O
,	O
israel	O
.	O
241	O
tang	O
,	O
y.	O
,	O
salakhutdinov	O
,	O
r.	O
,	O
and	O
hinton	O
,	O
g.	O
(	O
2012	O
)	O
.	O
deep	O
mixtures	O
of	O
factor	O
analysers	O
.	O
arxiv	O
preprint	O
arxiv:1206.4635	O
.	O
489	O
taylor	O
,	O
g.	O
and	O
hinton	O
,	O
g.	O
(	O
2009	O
)	O
.	O
factored	O
conditional	O
restricted	O
boltzmann	O
machines	O
for	O
modeling	O
motion	O
style	O
.	O
in	O
l.	O
bottou	O
and	O
m.	O
littman	O
,	O
editors	O
,	O
proceedings	O
of	O
the	O
twenty-sixth	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
’	O
09	O
)	O
,	O
pages	O
1025–1032	O
,	O
montreal	O
,	O
quebec	O
,	O
canada	O
.	O
acm	O
.	O
685	O
taylor	O
,	O
g.	O
,	O
hinton	O
,	O
g.	O
e.	O
,	O
and	O
roweis	O
,	O
s.	O
(	O
2007	O
)	O
.	O
modeling	O
human	O
motion	O
using	O
binary	O
latent	O
variables	O
.	O
in	O
b.	O
schölkopf	O
,	O
j.	O
platt	O
,	O
and	O
t.	O
hoﬀman	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
19	O
(	O
nips	O
’	O
06	O
)	O
,	O
pages	O
1345–1352	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
685	O
teh	O
,	O
y.	O
,	O
welling	O
,	O
m.	O
,	O
osindero	O
,	O
s.	O
,	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
2003	O
)	O
.	O
energy-based	O
models	O
for	O
sparse	O
overcomplete	O
representations	O
.	O
journal	O
of	O
machine	O
learning	O
research	O
,	O
4	O
,	O
1235–1260	O
.	O
491	O
tenenbaum	O
,	O
j.	O
,	O
de	O
silva	O
,	O
v.	O
,	O
and	O
langford	O
,	O
j.	O
c.	O
(	O
2000	O
)	O
.	O
a	O
global	O
geometric	O
framework	O
164	O
518	O
533	O
for	O
nonlinear	O
dimensionality	O
reduction	O
.	O
science	O
,	O
,	O
290	O
(	O
5500	O
)	O
,	O
2319–2323	O
.	O
,	O
theis	O
,	O
l.	O
,	O
van	O
den	O
oord	O
,	O
a.	O
,	O
and	O
bethge	O
,	O
m.	O
(	O
2015	O
)	O
.	O
a	O
note	O
on	O
the	O
evaluation	O
of	O
generative	O
models	O
.	O
arxiv:1511.01844	O
.	O
698	O
719	O
,	O
thompson	O
,	O
j.	O
,	O
jain	O
,	O
a.	O
,	O
lecun	O
,	O
y.	O
,	O
and	O
bregler	O
,	O
c.	O
(	O
2014	O
)	O
.	O
joint	O
training	O
of	O
a	O
convolutional	O
network	O
and	O
a	O
graphical	O
model	B
for	O
human	O
pose	O
estimation	O
.	O
in	O
nips	O
’	O
2014	O
360	O
.	O
thrun	O
,	O
s.	O
(	O
1995	O
)	O
.	O
learning	O
to	O
play	O
the	O
game	O
of	O
chess	O
.	O
in	O
nips	O
’	O
1994	O
271	O
.	O
tibshirani	O
,	O
r.	O
j	O
.	O
(	O
1995	O
)	O
.	O
regression	O
shrinkage	O
and	O
selection	O
via	O
the	O
lasso	O
.	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
b	O
,	O
58	O
,	O
267–288	O
.	O
236	O
tieleman	O
,	O
t.	O
(	O
2008	O
)	O
.	O
training	O
restricted	O
boltzmann	O
machines	O
using	O
approximations	O
to	O
the	O
likelihood	O
gradient	O
.	O
in	O
w.	O
w.	O
cohen	O
,	O
a.	O
mccallum	O
,	O
and	O
s.	O
t.	O
roweis	O
,	O
editors	O
,	O
pro-	O
ceedings	O
of	O
the	O
twenty-ﬁfth	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
’	O
08	O
)	O
,	O
pages	O
1064–1071	O
.	O
acm	O
.	O
612	O
tieleman	O
,	O
t.	O
and	O
hinton	O
,	O
g.	O
(	O
2009	O
)	O
.	O
using	O
fast	O
weights	O
to	O
improve	O
persistent	O
contrastive	O
divergence	O
.	O
in	O
l.	O
bottou	O
and	O
m.	O
littman	O
,	O
editors	O
,	O
proceedings	O
of	O
the	O
twenty-sixth	O
international	O
conference	O
on	O
machine	O
learning	O
(	O
icml	O
’	O
09	O
)	O
,	O
pages	O
1033–1040	O
.	O
acm	O
.	O
614	O
tipping	O
,	O
m.	O
e.	O
and	O
bishop	O
,	O
c.	O
m.	O
(	O
1999	O
)	O
.	O
probabilistic	O
principal	O
components	O
analysis	O
.	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
b	O
,	O
61	O
(	O
3	O
)	O
,	O
611–622	O
.	O
491	O
772	O
bibliography	O
torralba	O
,	O
a.	O
,	O
fergus	O
,	O
r.	O
,	O
and	O
weiss	O
,	O
y	O
.	O
(	O
2008	O
)	O
.	O
small	O
codes	O
and	O
large	O
databases	O
for	O
recognition	B
.	O
in	O
proceedings	O
of	O
the	O
computer	O
vision	O
and	O
pattern	O
recognition	B
conference	O
(	O
cvpr	O
’	O
08	O
)	O
,	O
pages	O
1–8	O
.	O
525	O
touretzky	O
,	O
d.	O
s.	O
and	O
minton	O
,	O
g.	O
e.	O
(	O
1985	O
)	O
.	O
symbols	O
among	O
the	O
neurons	O
:	O
details	O
of	O
a	O
connectionist	O
inference	O
architecture	O
.	O
in	O
proceedings	O
of	O
the	O
9th	O
international	O
joint	O
conference	O
on	O
artiﬁcial	O
intelligence	O
-	O
volume	O
1	O
,	O
ijcai	O
’	O
85	O
,	O
pages	O
238–243	O
,	O
san	O
francisco	O
,	O
ca	O
,	O
usa	O
.	O
morgan	O
kaufmann	O
publishers	O
inc.	O
17	O
tu	O
,	O
k.	O
and	O
honavar	O
,	O
v.	O
(	O
2011	O
)	O
.	O
on	O
the	O
utility	O
of	O
curricula	O
in	O
unsupervised	O
learning	O
of	O
probabilistic	O
grammars	O
.	O
in	O
ijcai	O
’	O
2011	O
328	O
.	O
turaga	O
,	O
s.	O
c.	O
,	O
murray	O
,	O
j.	O
f.	O
,	O
jain	O
,	O
v.	O
,	O
roth	O
,	O
f.	O
,	O
helmstaedter	O
,	O
m.	O
,	O
briggman	O
,	O
k.	O
,	O
denk	O
,	O
w.	O
,	O
and	O
seung	O
,	O
h.	O
s.	O
(	O
2010	O
)	O
.	O
convolutional	O
networks	O
can	O
learn	O
to	O
generate	O
aﬃnity	O
graphs	O
for	O
image	O
segmentation	O
.	O
neural	O
computation	O
,	O
(	O
2	O
)	O
,	O
511–538	O
.	O
360	O
22	O
turian	O
,	O
j.	O
,	O
ratinov	O
,	O
l.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2010	O
)	O
.	O
word	O
representations	O
:	O
a	O
simple	O
and	O
general	O
method	O
for	O
semi-supervised	O
learning	O
.	O
in	O
proc	O
.	O
acl	O
’	O
2010	O
,	O
pages	O
384–394	O
.	O
535	O
töscher	O
,	O
a.	O
,	O
jahrer	O
,	O
m.	O
,	O
and	O
bell	O
,	O
r.	O
m.	O
(	O
2009	O
)	O
.	O
the	O
bigchaos	O
solution	O
to	O
the	O
netﬂix	O
grand	O
prize	O
.	O
480	O
uria	O
,	O
b.	O
,	O
murray	O
,	O
i.	O
,	O
and	O
larochelle	O
,	O
h.	O
(	O
2013	O
)	O
.	O
rnade	O
:	O
the	O
real-valued	O
neural	O
autoregres-	O
sive	O
density-estimator	O
.	O
in	O
nips	O
’	O
2013	O
709	O
710	O
.	O
,	O
van	O
den	O
oörd	O
,	O
a.	O
,	O
dieleman	O
,	O
s.	O
,	O
and	O
schrauwen	O
,	O
b	O
.	O
(	O
2013	O
)	O
.	O
deep	O
content-based	O
music	O
recommendation	O
.	O
in	O
nips	O
’	O
2013	O
480	O
.	O
van	O
der	O
maaten	O
,	O
l.	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
2008	O
)	O
.	O
visualizing	O
data	O
using	O
t-sne	O
.	O
j.	O
machine	O
learning	O
res.	O
,	O
9	O
477	O
519	O
.	O
,	O
vanhoucke	O
,	O
v.	O
,	O
senior	O
,	O
a.	O
,	O
and	O
mao	O
,	O
m.	O
z	O
.	O
(	O
2011	O
)	O
.	O
improving	O
the	O
speed	O
of	O
neural	O
networks	O
on	O
cpus	O
.	O
in	O
proc	O
.	O
deep	O
learning	O
and	O
unsupervised	O
feature	O
learning	O
nips	O
workshop	O
.	O
444	O
452	O
,	O
vapnik	O
,	O
v.	O
n.	O
(	O
1982	O
)	O
.	O
estimation	O
of	O
dependences	O
based	O
on	O
empirical	O
data	O
.	O
springer-	O
verlag	O
,	O
berlin	O
.	O
114	O
vapnik	O
,	O
v.	O
n.	O
(	O
1995	O
)	O
.	O
the	O
nature	O
of	O
statistical	O
learning	O
theory	O
.	O
springer	O
,	O
new	O
york	O
.	O
114	O
vapnik	O
,	O
v.	O
n.	O
and	O
chervonenkis	O
,	O
a.	O
y	O
.	O
(	O
1971	O
)	O
.	O
on	O
the	O
uniform	O
convergence	O
of	O
relative	O
frequencies	O
of	O
events	O
to	O
their	O
probabilities	O
.	O
theory	O
of	O
probability	O
and	O
its	O
applications	O
,	O
16	O
,	O
264–280	O
.	O
114	O
vincent	O
,	O
p.	O
(	O
2011	O
)	O
.	O
a	O
connection	O
between	O
score	O
matching	O
and	O
denoising	O
autoencoders	O
.	O
neural	O
computation	O
,	O
23	O
(	O
7	O
)	O
.	O
513	O
515	O
712	O
,	O
,	O
773	O
bibliography	O
vincent	O
,	O
p.	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2003	O
)	O
.	O
manifold	O
parzen	O
windows	O
.	O
in	O
nips	O
’	O
2002	O
.	O
mit	O
press	O
.	O
520	O
vincent	O
,	O
p.	O
,	O
larochelle	O
,	O
h.	O
,	O
bengio	O
,	O
y.	O
,	O
and	O
manzagol	O
,	O
p.-a	O
.	O
(	O
2008	O
)	O
.	O
extracting	O
and	O
composing	O
robust	O
features	O
with	O
denoising	O
autoencoders	O
.	O
in	O
icml	O
2008	O
241	O
515	O
.	O
,	O
vincent	O
,	O
p.	O
,	O
larochelle	O
,	O
h.	O
,	O
lajoie	O
,	O
i.	O
,	O
bengio	O
,	O
y.	O
,	O
and	O
manzagol	O
,	O
p.-a	O
.	O
(	O
2010	O
)	O
.	O
stacked	O
denoising	O
autoencoders	O
:	O
learning	O
useful	O
representations	O
in	O
a	O
deep	O
network	O
with	O
a	O
local	O
denoising	O
criterion	O
.	O
j.	O
machine	O
learning	O
res.	O
,	O
.11	O
515	O
vincent	O
,	O
p.	O
,	O
de	O
brébisson	O
,	O
a.	O
,	O
and	O
bouthillier	O
,	O
x	O
.	O
(	O
2015	O
)	O
.	O
eﬃcient	O
exact	O
gradient	O
update	O
for	O
training	O
deep	O
networks	O
with	O
very	O
large	O
sparse	O
targets	O
.	O
in	O
c.	O
cortes	O
,	O
n.	O
d.	O
lawrence	O
,	O
d.	O
d.	O
lee	O
,	O
m.	O
sugiyama	O
,	O
and	O
r.	O
garnett	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
28	O
,	O
pages	O
1108–1116	O
.	O
curran	O
associates	O
,	O
inc.	O
466	O
vinyals	O
,	O
o.	O
,	O
kaiser	O
,	O
l.	O
,	O
koo	O
,	O
t.	O
,	O
petrov	O
,	O
s.	O
,	O
sutskever	O
,	O
i.	O
,	O
and	O
hinton	O
,	O
g.	O
(	O
2014a	O
)	O
.	O
grammar	O
as	O
a	O
foreign	O
language	O
.	O
technical	O
report	O
,	O
arxiv:1412.7449	O
.	O
410	O
vinyals	O
,	O
o.	O
,	O
toshev	O
,	O
a.	O
,	O
bengio	O
,	O
s.	O
,	O
and	O
erhan	O
,	O
d.	O
(	O
2014b	O
)	O
.	O
show	O
and	O
tell	O
:	O
a	O
neural	O
image	O
caption	O
generator	O
.	O
arxiv	O
1411.4555	O
.	O
410	O
vinyals	O
,	O
o.	O
,	O
fortunato	O
,	O
m.	O
,	O
and	O
jaitly	O
,	O
n.	O
(	O
2015a	O
)	O
.	O
pointer	O
networks	O
.	O
arxiv	O
preprint	O
arxiv:1506.03134	O
.	O
418	O
vinyals	O
,	O
o.	O
,	O
toshev	O
,	O
a.	O
,	O
bengio	O
,	O
s.	O
,	O
and	O
erhan	O
,	O
d.	O
(	O
2015b	O
)	O
.	O
show	O
and	O
tell	O
:	O
a	O
neural	O
image	O
caption	O
generator	O
.	O
in	O
cvpr	O
’	O
2015	O
.	O
arxiv:1411.4555	O
.	O
102	O
viola	O
,	O
p.	O
and	O
jones	O
,	O
m.	O
(	O
2001	O
)	O
.	O
robust	O
real-time	O
object	O
detection	O
.	O
in	O
international	O
journal	O
of	O
computer	O
vision	O
.	O
449	O
visin	O
,	O
f.	O
,	O
kastner	O
,	O
k.	O
,	O
cho	O
,	O
k.	O
,	O
matteucci	O
,	O
m.	O
,	O
courville	O
,	O
a.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2015	O
)	O
.	O
renet	O
:	O
a	O
recurrent	O
neural	O
network	O
based	O
alternative	O
to	O
convolutional	O
networks	O
.	O
arxiv	O
preprint	O
arxiv:1505.00393	O
.	O
395	O
von	O
melchner	O
,	O
l.	O
,	O
pallas	O
,	O
s.	O
l.	O
,	O
and	O
sur	O
,	O
m.	O
(	O
2000	O
)	O
.	O
visual	O
behaviour	O
mediated	O
by	O
retinal	O
projections	O
directed	O
to	O
the	O
auditory	O
pathway	O
.	O
nature	O
,	O
404	O
(	O
6780	O
)	O
,	O
871–876	O
.	O
16	O
wager	O
,	O
s.	O
,	O
wang	O
,	O
s.	O
,	O
and	O
liang	O
,	O
p.	O
(	O
2013	O
)	O
.	O
dropout	O
training	O
as	O
adaptive	O
regularization	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
26	O
,	O
pages	O
351–359	O
.	O
265	O
waibel	O
,	O
a.	O
,	O
hanazawa	O
,	O
t.	O
,	O
hinton	O
,	O
g.	O
e.	O
,	O
shikano	O
,	O
k.	O
,	O
and	O
lang	O
,	O
k.	O
(	O
1989	O
)	O
.	O
phoneme	O
recognition	B
using	O
time-delay	O
neural	O
networks	O
.	O
ieee	O
transactions	O
on	O
acoustics	O
,	O
speech	O
,	O
and	O
signal	O
processing	O
,	O
374	O
453	O
459	O
,	O
328–339	O
.	O
37	O
,	O
,	O
wan	O
,	O
l.	O
,	O
zeiler	O
,	O
m.	O
,	O
zhang	O
,	O
s.	O
,	O
lecun	O
,	O
y.	O
,	O
and	O
fergus	O
,	O
r.	O
(	O
2013	O
)	O
.	O
regularization	O
of	O
neural	O
networks	O
using	O
dropconnect	O
.	O
in	O
icml	O
’	O
2013	O
266	O
.	O
wang	O
,	O
s.	O
and	O
manning	O
,	O
c.	O
(	O
2013	O
)	O
.	O
fast	O
dropout	O
training	O
.	O
in	O
icml	O
’	O
2013	O
266	O
.	O
774	O
bibliography	O
wang	O
,	O
z.	O
,	O
zhang	O
,	O
j.	O
,	O
feng	O
,	O
j.	O
,	O
and	O
chen	O
,	O
z	O
.	O
(	O
2014a	O
)	O
.	O
knowledge	O
graph	O
and	O
text	O
jointly	O
embedding	O
.	O
in	O
proc	O
.	O
emnlp	O
’	O
2014	O
.	O
484	O
wang	O
,	O
z.	O
,	O
zhang	O
,	O
j.	O
,	O
feng	O
,	O
j.	O
,	O
and	O
chen	O
,	O
z	O
.	O
(	O
2014b	O
)	O
.	O
knowledge	O
graph	O
embedding	O
by	O
translating	O
on	O
hyperplanes	O
.	O
in	O
proc	O
.	O
aaai	O
’	O
2014	O
.	O
484	O
warde-farley	O
,	O
d.	O
,	O
goodfellow	O
,	O
i.	O
j.	O
,	O
courville	O
,	O
a.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2014	O
)	O
.	O
an	O
empirical	O
analysis	O
of	O
dropout	O
in	O
piecewise	O
linear	O
networks	O
.	O
in	O
iclr	O
’	O
2014	O
262	O
266	O
267	O
.	O
,	O
,	O
wawrzynek	O
,	O
j.	O
,	O
asanovic	O
,	O
k.	O
,	O
kingsbury	O
,	O
b.	O
,	O
johnson	O
,	O
d.	O
,	O
beck	O
,	O
j.	O
,	O
and	O
morgan	O
,	O
n.	O
(	O
1996	O
)	O
.	O
spert-ii	O
:	O
a	O
vector	O
microprocessor	O
system	O
.	O
computer	O
29	O
,	O
(	O
3	O
)	O
,	O
79–86	O
.	O
451	O
weaver	O
,	O
l.	O
and	O
tao	O
,	O
n.	O
(	O
2001	O
)	O
.	O
the	O
optimal	O
reward	O
baseline	O
for	O
gradient-based	O
reinforce-	O
ment	O
learning	O
.	O
in	O
proc	O
.	O
uai	O
’	O
2001	O
,	O
pages	O
538–545	O
.	O
691	O
weinberger	O
,	O
k.	O
q.	O
and	O
saul	O
,	O
l.	O
k.	O
(	O
2004	O
)	O
.	O
unsupervised	O
learning	O
of	O
image	O
manifolds	O
by	O
semideﬁnite	O
programming	O
.	O
in	O
cvpr	O
’	O
2004	O
,	O
pages	O
988–995	O
.	O
164	O
519	O
,	O
weiss	O
,	O
y.	O
,	O
torralba	O
,	O
a.	O
,	O
and	O
fergus	O
,	O
r.	O
(	O
2008	O
)	O
.	O
spectral	O
hashing	O
.	O
in	O
nips	O
,	O
pages	O
1753–1760	O
.	O
525	O
welling	O
,	O
m.	O
,	O
zemel	O
,	O
r.	O
s.	O
,	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
2002	O
)	O
.	O
self	O
supervised	O
boosting	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
,	O
pages	O
665–672	O
.	O
703	O
welling	O
,	O
m.	O
,	O
hinton	O
,	O
g.	O
e.	O
,	O
and	O
osindero	O
,	O
s.	O
(	O
2003a	O
)	O
.	O
learning	O
sparse	O
topographic	O
representations	O
with	O
products	O
of	O
student-t	O
distributions	O
.	O
in	O
nips	O
’	O
2002	O
680	O
.	O
welling	O
,	O
m.	O
,	O
zemel	O
,	O
r.	O
,	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
2003b	O
)	O
.	O
self-supervised	O
boosting	O
.	O
in	O
s.	O
becker	O
,	O
s.	O
thrun	O
,	O
and	O
k.	O
obermayer	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
15	O
(	O
nips	O
’	O
02	O
)	O
,	O
pages	O
665–672	O
.	O
mit	O
press	O
.	O
622	O
welling	O
,	O
m.	O
,	O
rosen-zvi	O
,	O
m.	O
,	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
2005	O
)	O
.	O
exponential	O
family	O
harmoniums	O
with	O
an	O
application	O
to	O
information	O
retrieval	O
.	O
in	O
l.	O
saul	O
,	O
y.	O
weiss	O
,	O
and	O
l.	O
bottou	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
17	O
(	O
nips	O
’	O
04	O
)	O
,	O
volume	O
17	O
,	O
cambridge	O
,	O
ma	O
.	O
mit	O
press	O
.	O
676	O
werbos	O
,	O
p.	O
j	O
.	O
(	O
1981	O
)	O
.	O
applications	O
of	O
advances	O
in	O
nonlinear	O
sensitivity	O
analysis	O
.	O
in	O
proceedings	O
of	O
the	O
10th	O
ifip	O
conference	O
,	O
31.8	O
-	O
4.9	O
,	O
nyc	O
,	O
pages	O
762–770	O
.	O
225	O
weston	O
,	O
j.	O
,	O
bengio	O
,	O
s.	O
,	O
and	O
usunier	O
,	O
n.	O
(	O
2010	O
)	O
.	O
large	O
scale	O
image	O
annotation	O
:	O
learning	O
to	O
rank	O
with	O
joint	O
word-image	O
embeddings	O
.	O
machine	O
learning	O
,	O
81	O
(	O
1	O
)	O
,	O
21–35	O
.	O
401	O
weston	O
,	O
j.	O
,	O
chopra	O
,	O
s.	O
,	O
and	O
bordes	O
,	O
a	O
.	O
(	O
2014	O
)	O
.	O
memory	O
networks	O
.	O
arxiv	O
preprint	O
arxiv:1410.3916	O
.	O
418	O
485	O
,	O
widrow	O
,	O
b.	O
and	O
hoﬀ	O
,	O
m.	O
e.	O
(	O
1960	O
)	O
.	O
adaptive	O
switching	O
circuits	O
.	O
in	O
1960	O
ire	O
wescon	O
convention	O
record	O
,	O
volume	O
4	O
,	O
pages	O
96–104	O
.	O
ire	O
,	O
new	O
york	O
.	O
775	O
15	O
21	O
24	O
27	O
,	O
,	O
,	O
bibliography	O
wikipedia	O
(	O
2015	O
)	O
.	O
list	O
of	O
animals	O
by	O
number	O
of	O
neurons	O
—	O
wikipedia	O
,	O
the	O
free	O
encyclopedia	O
.	O
[	O
online	O
;	O
accessed	O
4-march-2015	O
]	O
.	O
,24	O
27	O
williams	O
,	O
c.	O
k.	O
i.	O
and	O
agakov	O
,	O
f.	O
v.	O
(	O
2002	O
)	O
.	O
products	O
of	O
gaussians	O
and	O
probabilistic	O
minor	O
component	O
analysis	O
.	O
neural	O
computation	O
,	O
14	O
(	O
5	O
)	O
,	O
1169–1182	O
.	O
682	O
williams	O
,	O
c.	O
k.	O
i.	O
and	O
rasmussen	O
,	O
c.	O
e.	O
(	O
1996	O
)	O
.	O
gaussian	O
processes	O
for	O
regression	O
.	O
in	O
d.	O
touretzky	O
,	O
m.	O
mozer	O
,	O
and	O
m.	O
hasselmo	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
8	O
(	O
nips	O
’	O
95	O
)	O
,	O
pages	O
514–520	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
142	O
williams	O
,	O
r.	O
j	O
.	O
(	O
1992	O
)	O
.	O
simple	O
statistical	O
gradient-following	O
algorithms	O
connectionist	O
reinforcement	O
learning	O
.	O
machine	O
learning	O
,	O
8	O
,	O
229–256	O
.	O
688	O
689	O
,	O
williams	O
,	O
r.	O
j.	O
and	O
zipser	O
,	O
d.	O
(	O
1989	O
)	O
.	O
a	O
learning	O
algorithm	O
for	O
continually	O
running	O
fully	O
recurrent	O
neural	O
networks	O
.	O
neural	O
computation	O
,	O
1	O
,	O
270–280	O
.	O
223	O
wilson	O
,	O
d.	O
r.	O
and	O
martinez	O
,	O
t.	O
r.	O
(	O
2003	O
)	O
.	O
the	O
general	O
ineﬃciency	O
of	O
batch	O
training	O
for	O
gradient	O
descent	B
learning	O
.	O
neural	O
networks	O
,	O
16	O
(	O
10	O
)	O
,	O
1429–1451	O
.	O
279	O
wilson	O
,	O
j.	O
r.	O
(	O
1984	O
)	O
.	O
variance	O
reduction	O
techniques	O
for	O
digital	O
simulation	O
.	O
american	O
journal	O
of	O
mathematical	O
and	O
management	O
sciences	O
,	O
4	O
(	O
3	O
)	O
,	O
277––312	O
.	O
690	O
wiskott	O
,	O
l.	O
and	O
sejnowski	O
,	O
t.	O
j	O
.	O
(	O
2002	O
)	O
.	O
slow	O
feature	O
analysis	O
:	O
unsupervised	O
learning	O
of	O
invariances	O
.	O
neural	O
computation	O
,	O
14	O
(	O
4	O
)	O
,	O
715–770	O
.	O
494	O
wolpert	O
,	O
d.	O
and	O
macready	O
,	O
w.	O
(	O
1997	O
)	O
.	O
no	O
free	O
lunch	O
theorems	O
for	O
optimization	O
.	O
ieee	O
transactions	O
on	O
evolutionary	O
computation	O
,	O
1	O
,	O
67–82	O
.	O
293	O
wolpert	O
,	O
d.	O
h.	O
(	O
1996	O
)	O
.	O
the	O
lack	O
of	O
a	O
priori	O
distinction	O
between	O
learning	O
algorithms	O
.	O
neural	O
computation	O
,	O
8	O
(	O
7	O
)	O
,	O
1341–1390	O
.	O
116	O
wu	O
,	O
r.	O
,	O
yan	O
,	O
s.	O
,	O
shan	O
,	O
y.	O
,	O
dang	O
,	O
q.	O
,	O
and	O
sun	O
,	O
g.	O
(	O
2015	O
)	O
.	O
deep	O
image	O
:	O
scaling	O
up	O
image	O
recognition	B
.	O
arxiv:1501.02876	O
.	O
447	O
wu	O
,	O
z	O
.	O
(	O
1997	O
)	O
.	O
global	O
continuation	O
for	O
distance	O
geometry	O
problems	O
.	O
siam	O
journal	O
of	O
optimization	O
,	O
7	O
,	O
814–836	O
.	O
327	O
xiong	O
,	O
h.	O
y.	O
,	O
barash	O
,	O
y.	O
,	O
and	O
frey	O
,	O
b.	O
j	O
.	O
(	O
2011	O
)	O
.	O
bayesian	O
prediction	O
of	O
tissue-regulated	O
bioinformatics	O
27	O
(	O
18	O
)	O
,	O
2554–2562	O
.	O
splicing	O
using	O
rna	O
sequence	O
and	O
cellular	O
context	O
.	O
265	O
,	O
xu	O
,	O
k.	O
,	O
ba	O
,	O
j.	O
l.	O
,	O
kiros	O
,	O
r.	O
,	O
cho	O
,	O
k.	O
,	O
courville	O
,	O
a.	O
,	O
salakhutdinov	O
,	O
r.	O
,	O
zemel	O
,	O
r.	O
s.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
2015	O
)	O
.	O
show	O
,	O
attend	O
and	O
tell	O
:	O
neural	O
image	O
caption	O
generation	O
with	O
visual	O
attention	O
.	O
in	O
icml	O
’	O
2015	O
,	O
arxiv:1502.03044	O
102	O
410	O
691	O
.	O
,	O
,	O
yildiz	O
,	O
i.	O
b.	O
,	O
jaeger	O
,	O
h.	O
,	O
and	O
kiebel	O
,	O
s.	O
j	O
.	O
(	O
2012	O
)	O
.	O
re-visiting	O
the	O
echo	O
state	O
property	O
.	O
neural	O
networks	O
,	O
35	O
,	O
1–9	O
.	O
405	O
776	O
bibliography	O
yosinski	O
,	O
j.	O
,	O
clune	O
,	O
j.	O
,	O
bengio	O
,	O
y.	O
,	O
and	O
lipson	O
,	O
h.	O
(	O
2014	O
)	O
.	O
how	O
transferable	O
are	O
features	O
in	O
deep	O
neural	O
networks	O
?	O
in	O
nips	O
’	O
2014	O
325	O
536	O
.	O
,	O
younes	O
,	O
l.	O
(	O
1998	O
)	O
.	O
on	O
the	O
convergence	O
of	O
markovian	O
stochastic	O
algorithms	O
with	O
rapidly	O
decreasing	O
ergodicity	O
rates	O
.	O
in	O
stochastics	O
and	O
stochastics	O
models	O
,	O
pages	O
177–228	O
.	O
612	O
yu	O
,	O
d.	O
,	O
wang	O
,	O
s.	O
,	O
and	O
deng	O
,	O
l.	O
(	O
2010	O
)	O
.	O
sequential	O
labeling	O
using	O
deep-structured	O
conditional	O
random	O
ﬁelds	O
.	O
ieee	O
journal	O
of	O
selected	O
topics	O
in	O
signal	O
processing	O
.	O
323	O
zaremba	O
,	O
w.	O
and	O
sutskever	O
,	O
i	O
.	O
(	O
2014	O
)	O
.	O
learning	O
to	O
execute	O
.	O
arxiv	O
1410.4615	O
.	O
329	O
zaremba	O
,	O
w.	O
and	O
sutskever	O
,	O
i	O
.	O
(	O
2015	O
)	O
.	O
reinforcement	O
learning	O
neural	O
turing	O
machines	O
.	O
arxiv:1505.00521	O
.	O
419	O
zaslavsky	O
,	O
t.	O
(	O
1975	O
)	O
.	O
facing	O
up	O
to	O
arrangements	O
:	O
face-count	O
formulas	O
for	O
partitions	O
of	O
space	O
by	O
hyperplanes	O
.	O
number	O
no	O
.	O
154	O
in	O
memoirs	O
of	O
the	O
american	O
mathematical	O
society	O
.	O
american	O
mathematical	O
society	O
.	O
550	O
zeiler	O
,	O
m.	O
d.	O
and	O
fergus	O
,	O
r.	O
(	O
2014	O
)	O
.	O
visualizing	O
and	O
understanding	O
convolutional	O
networks	O
.	O
in	O
eccv	O
’	O
14	O
6	O
.	O
zeiler	O
,	O
m.	O
d.	O
,	O
ranzato	O
,	O
m.	O
,	O
monga	O
,	O
r.	O
,	O
mao	O
,	O
m.	O
,	O
yang	O
,	O
k.	O
,	O
le	O
,	O
q.	O
,	O
nguyen	O
,	O
p.	O
,	O
senior	O
,	O
a.	O
,	O
vanhoucke	O
,	O
v.	O
,	O
dean	O
,	O
j.	O
,	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
2013	O
)	O
.	O
on	O
rectiﬁed	O
linear	O
units	O
for	O
speech	O
processing	O
.	O
in	O
icassp	O
2013	O
460	O
.	O
zhou	O
,	O
b.	O
,	O
khosla	O
,	O
a.	O
,	O
lapedriza	O
,	O
a.	O
,	O
oliva	O
,	O
a.	O
,	O
and	O
torralba	O
,	O
a	O
.	O
(	O
2015	O
)	O
.	O
object	O
detectors	O
emerge	O
in	O
deep	O
scene	O
cnns	O
.	O
iclr	O
’	O
2015	O
,	O
arxiv:1412.6856	O
.	O
551	O
zhou	O
,	O
j.	O
and	O
troyanskaya	O
,	O
o.	O
g.	O
(	O
2014	O
)	O
.	O
deep	O
supervised	O
and	O
convolutional	O
generative	O
stochastic	O
network	O
for	O
protein	O
secondary	O
structure	O
prediction	O
.	O
in	O
icml	O
’	O
2014	O
715	O
.	O
zhou	O
,	O
y.	O
and	O
chellappa	O
,	O
r.	O
(	O
1988	O
)	O
.	O
computation	O
of	O
optical	O
ﬂow	O
using	O
a	O
neural	O
network	O
.	O
in	O
neural	O
networks	O
,	O
1988.	O
,	O
ieee	O
international	O
conference	O
on	O
,	O
pages	O
71–78	O
.	O
ieee	O
.	O
339	O
zöhrer	O
,	O
m.	O
and	O
pernkopf	O
,	O
f.	O
(	O
2014	O
)	O
.	O
general	O
stochastic	O
networks	O
for	O
classiﬁcation	O
.	O
in	O
nips	O
’	O
2014	O
.	O
716	O