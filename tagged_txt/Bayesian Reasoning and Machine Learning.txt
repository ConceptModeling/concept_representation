bayesian	O
reasoning	O
and	O
machine	O
learning	B
david	O
barber	O
c	O
(	O
cid:13	O
)	O
2007,2008,2009,2010	O
notation	O
list	O
v	O
dom	O
(	O
x	O
)	O
x	O
=	O
x	O
p	O
(	O
x	O
=	O
tr	O
)	O
p	O
(	O
x	O
=	O
fa	O
)	O
p	O
(	O
x	O
,	O
y	O
)	O
p	O
(	O
x	O
∩	O
y	O
)	O
p	O
(	O
x	O
∪	O
y	O
)	O
(	O
cid:82	O
)	O
p	O
(	O
x|y	O
)	O
x	O
f	O
(	O
x	O
)	O
i	O
[	O
x	O
=	O
y	O
]	O
pa	O
(	O
x	O
)	O
ch	O
(	O
x	O
)	O
ne	O
(	O
x	O
)	O
x	O
⊥⊥y|z	O
x	O
(	O
cid:62	O
)	O
(	O
cid:62	O
)	O
y|z	O
dim	O
x	O
(	O
cid:104	O
)	O
f	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
)	O
δ	O
(	O
a	O
,	O
b	O
)	O
dim	O
x	O
(	O
cid:93	O
)	O
(	O
x	O
=	O
s	O
,	O
y	O
=	O
t	O
)	O
d	O
n	O
n	O
(	O
cid:93	O
)	O
x	O
y	O
s	O
σ	O
(	O
x	O
)	O
erf	O
(	O
x	O
)	O
i	O
∼	O
j	O
im	O
ii	O
a	O
calligraphic	O
symbol	O
typically	O
denotes	O
a	O
set	O
of	O
random	O
variables	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
3	O
domain	B
of	O
a	O
variable	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
3	O
the	O
variable	B
x	O
is	O
in	O
the	O
state	O
x	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
3	O
probability	B
of	O
event/variable	O
x	O
being	O
in	O
the	O
state	O
true	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
3	O
probability	B
of	O
event/variable	O
x	O
being	O
in	O
the	O
state	O
false	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
3	O
probability	B
of	O
x	O
and	O
y	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
4	O
probability	B
of	O
x	O
and	O
y	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
4	O
probability	B
of	O
x	O
or	O
y	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
4	O
the	O
probability	B
of	O
x	O
conditioned	O
on	O
y	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
4	O
for	O
continuous	B
variables	O
this	O
is	O
shorthand	O
for	O
(	O
cid:82	O
)	O
f	O
(	O
x	O
)	O
dx	O
and	O
for	O
discrete	B
vari-	O
ables	O
means	O
summation	O
over	O
the	O
states	O
of	O
x	O
,	O
(	O
cid:80	O
)	O
x	O
f	O
(	O
x	O
)	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
7	O
indicator	O
:	O
has	O
value	B
1	O
if	O
x	O
=	O
y	O
,	O
0	O
otherwise	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
11	O
the	O
parents	B
of	O
node	B
x.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
19	O
the	O
children	B
of	O
node	B
x.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
19	O
neighbours	O
of	O
node	B
x	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
20	O
variables	O
x	O
are	O
independent	O
of	O
variables	O
y	O
conditioned	O
on	O
variables	O
z	O
.	O
33	O
.	O
.	O
.	O
.	O
.33	O
variables	O
x	O
are	O
dependent	O
on	O
variables	O
y	O
conditioned	O
variables	O
z.	O
for	O
a	O
discrete	B
variable	O
x	O
,	O
this	O
denotes	O
the	O
number	O
of	O
states	O
x	O
can	O
take	O
.	O
.43	O
the	O
average	B
of	O
the	O
function	B
f	O
(	O
x	O
)	O
with	O
respect	O
to	O
the	O
distribution	B
p	O
(	O
x	O
)	O
.	O
139	O
delta	B
function	I
.	O
for	O
discrete	B
a	O
,	O
b	O
,	O
this	O
is	O
the	O
kronecker	O
delta	O
,	O
δa	O
,	O
b	O
and	O
for	O
continuous	B
a	O
,	O
b	O
the	O
dirac	O
delta	B
function	I
δ	O
(	O
a	O
−	O
b	O
)	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
142	O
the	O
dimension	O
of	O
the	O
vector/matrix	O
x.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.150	O
the	O
number	O
of	O
times	O
variable	B
x	O
is	O
in	O
state	O
s	O
and	O
y	O
in	O
state	O
t	O
simultaneously	O
.	O
172	O
dataset	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.251	O
data	B
index	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
251	O
number	O
of	O
dataset	O
training	B
points	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
251	O
the	O
number	O
of	O
times	O
variable	B
x	O
is	O
in	O
state	O
y	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
265	O
sample	B
covariance	O
matrix	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
283	O
the	O
logistic	B
sigmoid	I
1/	O
(	O
1	O
+	O
exp	O
(	O
−x	O
)	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
319	O
the	O
(	O
gaussian	O
)	O
error	B
function	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
319	O
the	O
set	O
of	O
unique	O
neighbouring	O
edges	O
on	O
a	O
graph	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.529	O
the	O
m	O
×	O
m	O
identity	B
matrix	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
546	O
draft	O
march	O
9	O
,	O
2010	O
preface	O
machine	O
learning	B
the	O
last	O
decade	O
has	O
seen	O
considerable	O
growth	O
in	O
interest	O
in	O
artiﬁcial	O
intelligence	O
and	O
machine	O
learning	B
.	O
in	O
the	O
broadest	O
sense	O
,	O
these	O
ﬁelds	O
aim	O
to	O
‘	O
learn	O
something	O
useful	O
’	O
about	O
the	O
environment	O
within	O
which	O
the	O
organism	O
operates	O
.	O
how	O
gathered	O
information	O
is	O
processed	O
leads	O
to	O
the	O
development	O
of	O
algorithms	O
–	O
how	O
to	O
process	O
high	B
dimensional	I
data	I
and	O
deal	O
with	O
uncertainty	O
.	O
in	O
the	O
early	O
stages	O
of	O
research	O
in	O
machine	O
learning	B
and	O
related	O
areas	O
,	O
similar	O
techniques	O
were	O
discovered	O
in	O
relatively	O
isolated	O
research	O
communities	O
.	O
whilst	O
not	O
all	O
techniques	O
have	O
a	O
natural	B
description	O
in	O
terms	O
of	O
probability	B
theory	O
,	O
many	O
do	O
,	O
and	O
it	O
is	O
the	O
framework	O
of	O
graphical	O
models	O
(	O
a	O
marriage	O
between	O
graph	B
and	O
probability	B
theory	O
)	O
that	O
has	O
enabled	O
the	O
understanding	O
and	O
transference	O
of	O
ideas	O
from	O
statistical	O
physics	O
,	O
statistics	O
,	O
machine	O
learning	B
and	O
informa-	O
tion	O
theory	O
.	O
to	O
this	O
extent	O
it	O
is	O
now	O
reasonable	O
to	O
expect	O
that	O
machine	O
learning	B
researchers	O
are	O
familiar	O
with	O
the	O
basics	O
of	O
statistical	O
modelling	B
techniques	O
.	O
this	O
book	O
concentrates	O
on	O
the	O
probabilistic	B
aspects	O
of	O
information	O
processing	O
and	O
machine	O
learning	B
.	O
cer-	O
tainly	O
no	O
claim	O
is	O
made	O
as	O
to	O
the	O
correctness	O
or	O
that	O
this	O
is	O
the	O
only	O
useful	O
approach	B
.	O
indeed	O
,	O
one	O
might	O
counter	O
that	O
this	O
is	O
unnecessary	O
since	O
“	O
biological	O
organisms	O
don	O
’	O
t	O
use	O
probability	B
theory	O
”	O
.	O
whether	O
this	O
is	O
the	O
case	O
or	O
not	O
,	O
it	O
is	O
undeniable	O
that	O
the	O
framework	O
of	O
graphical	O
models	O
and	O
probability	B
has	O
helped	O
with	O
the	O
explosion	O
of	O
new	O
algorithms	O
and	O
models	O
in	O
the	O
machine	O
learning	B
community	O
.	O
one	O
should	O
also	O
be	O
clear	O
that	O
bayesian	O
viewpoint	O
is	O
not	O
the	O
only	O
way	O
to	O
go	O
about	O
describing	O
machine	O
learning	B
and	O
information	O
processing	O
.	O
bayesian	O
and	O
probabilistic	B
techniques	O
really	O
come	O
into	O
their	O
own	O
in	O
domains	O
where	O
uncertainty	B
is	O
a	O
necessary	O
consideration	O
.	O
the	O
structure	B
of	O
the	O
book	O
one	O
aim	O
of	O
part	O
i	O
of	O
the	O
book	O
is	O
to	O
encourage	O
computer	O
science	O
students	O
into	O
this	O
area	O
.	O
a	O
particular	O
diﬃ-	O
culty	O
that	O
many	O
modern	O
students	O
face	O
is	O
a	O
limited	O
formal	O
training	B
in	O
calculus	B
and	O
linear	B
algebra	I
,	O
meaning	O
that	O
minutiae	O
of	O
continuous	B
and	O
high-dimensional	O
distributions	O
can	O
turn	O
them	O
away	O
.	O
in	O
beginning	O
with	O
probability	O
as	O
a	O
form	O
of	O
reasoning	O
system	B
,	O
we	O
hope	O
to	O
show	O
the	O
reader	O
how	O
ideas	O
from	O
logical	O
inference	B
and	O
dynamical	O
programming	O
that	O
they	O
may	O
be	O
more	O
familiar	O
with	O
have	O
natural	B
parallels	O
in	O
a	O
probabilistic	B
context	O
.	O
in	O
particular	O
,	O
computer	O
science	O
students	O
are	O
familiar	O
with	O
the	O
concept	O
of	O
algorithms	O
as	O
core	O
.	O
however	O
,	O
it	O
is	O
more	O
common	O
in	O
machine	O
learning	B
to	O
view	O
the	O
model	B
as	O
core	O
,	O
and	O
how	O
this	O
is	O
implemented	O
is	O
secondary	O
.	O
from	O
this	O
perspective	O
,	O
understanding	O
how	O
to	O
translate	O
a	O
mathematical	O
model	B
into	O
a	O
piece	O
of	O
computer	O
code	O
is	O
central	O
.	O
part	O
ii	O
introduces	O
the	O
statistical	O
background	O
needed	O
to	O
understand	O
continuous	B
distributions	O
and	O
how	O
learn-	O
ing	O
can	O
be	O
viewed	O
from	O
a	O
probabilistic	B
framework	O
.	O
part	O
iii	O
discusses	O
machine	O
learning	B
topics	O
.	O
certainly	O
some	O
readers	O
will	O
raise	O
an	O
eyebrow	O
to	O
see	O
their	O
favourite	O
statistical	O
topic	O
listed	O
under	O
machine	O
learning	B
.	O
a	O
diﬀerence	O
viewpoint	O
between	O
statistics	O
and	O
machine	O
learning	B
is	O
what	O
kinds	O
of	O
systems	O
we	O
would	O
ultimately	O
iii	O
like	O
to	O
construct	O
(	O
machines	O
capable	O
of	O
‘	O
human/biological	O
information	O
processing	O
tasks	O
)	O
rather	O
than	O
in	O
some	O
of	O
the	O
techniques	O
.	O
this	O
section	O
of	O
the	O
book	O
is	O
therefore	O
what	O
i	O
feel	O
would	O
be	O
useful	O
for	O
machine	O
learners	O
to	O
know	O
.	O
part	O
iv	O
discusses	O
dynamical	O
models	O
in	O
which	O
time	O
is	O
explicitly	O
considered	O
.	O
in	O
particular	O
the	O
kalman	O
filter	O
is	O
treated	O
as	O
a	O
form	O
of	O
graphical	O
model	B
,	O
which	O
helps	O
emphasise	O
what	O
the	O
model	B
is	O
,	O
rather	O
than	O
focusing	O
on	O
it	O
as	O
a	O
‘	O
ﬁlter	O
’	O
,	O
as	O
is	O
more	O
traditional	O
in	O
the	O
engineering	O
literature	O
.	O
part	O
v	O
contains	O
a	O
brief	O
introduction	O
to	O
approximate	B
inference	I
techniques	O
,	O
including	O
both	O
stochastic	O
(	O
monte	O
carlo	O
)	O
and	O
deterministic	B
(	O
variational	O
)	O
techniques	O
.	O
the	O
references	O
in	O
the	O
book	O
are	O
not	O
generally	O
intended	O
as	O
crediting	O
authors	O
with	O
ideas	O
,	O
nor	O
are	O
they	O
always	O
to	O
the	O
most	O
authoritative	O
works	O
.	O
rather	O
,	O
the	O
references	O
are	O
largely	O
to	O
works	O
which	O
are	O
at	O
a	O
level	O
reasonably	O
consistent	B
with	O
the	O
book	O
and	O
which	O
are	O
readily	O
available	O
.	O
whom	O
this	O
book	O
is	O
for	O
my	O
primary	O
aim	O
was	O
to	O
write	O
a	O
book	O
for	O
ﬁnal	O
year	O
undergraduates	O
and	O
graduates	O
without	O
signiﬁcant	O
expe-	O
rience	O
in	O
calculus	B
and	O
mathematics	O
that	O
gave	O
an	O
inroad	O
into	O
machine	O
learning	B
,	O
much	O
of	O
which	O
is	O
currently	O
phrased	O
in	O
terms	O
of	O
probabilities	O
and	O
multi-variate	B
distributions	O
.	O
the	O
aim	O
was	O
to	O
encourage	O
students	O
that	O
apparently	O
unexciting	O
statistical	O
concepts	O
are	O
actually	O
highly	O
relevant	O
for	O
research	O
in	O
making	O
intelligent	O
systems	O
that	O
interact	O
with	O
humans	O
in	O
a	O
natural	B
manner	O
.	O
such	O
a	O
research	O
programme	O
inevitably	O
requires	O
dealing	O
with	O
high-dimensional	O
data	B
,	O
time-series	O
,	O
networks	O
,	O
logical	O
reasoning	O
,	O
modelling	B
and	O
uncertainty	B
.	O
other	O
books	O
in	O
this	O
area	O
whilst	O
there	O
are	O
several	O
excellent	O
textbooks	O
in	O
this	O
area	O
,	O
none	O
currently	O
meets	O
the	O
requirements	O
that	O
i	O
per-	O
sonally	O
need	O
for	O
teaching	O
,	O
namely	O
one	O
that	O
contains	O
demonstration	O
code	O
and	O
gently	O
introduces	O
probability	B
and	O
statistics	O
before	O
leading	O
on	O
to	O
more	O
advanced	O
topics	O
in	O
machine	O
learning	B
.	O
this	O
lead	O
me	O
to	O
build	O
on	O
my	O
lecture	O
material	O
from	O
courses	O
given	O
at	O
aston	O
,	O
edinburgh	O
,	O
epfl	O
and	O
ucl	O
and	O
expand	O
the	O
demonstration	O
software	O
considerably	O
.	O
the	O
book	O
is	O
due	O
for	O
publication	O
by	O
cambridge	O
university	O
press	O
in	O
2010.	O
the	O
literature	O
on	O
machine	O
learning	B
is	O
vast	O
,	O
as	O
is	O
the	O
overlap	O
with	O
the	O
relevant	O
areas	O
of	O
statistics	O
,	O
engineering	O
and	O
other	O
physical	O
sciences	O
.	O
in	O
this	O
respect	O
,	O
it	O
is	O
diﬃcult	O
to	O
isolate	O
particular	O
areas	O
,	O
and	O
this	O
book	O
is	O
an	O
attempt	O
to	O
integrate	O
parts	O
of	O
the	O
machine	O
learning	B
and	O
statistics	O
literature	O
.	O
the	O
book	O
is	O
written	O
in	O
an	O
informal	O
style	O
at	O
the	O
expense	O
of	O
rigour	O
and	O
detailed	O
proofs	O
.	O
as	O
an	O
introductory	O
textbook	O
,	O
topics	O
are	O
naturally	O
covered	O
to	O
a	O
somewhat	O
shallow	O
level	O
and	O
the	O
reader	O
is	O
referred	O
to	O
more	O
specialised	O
books	O
for	O
deeper	O
treatments	O
.	O
amongst	O
my	O
favourites	O
are	O
:	O
•	O
graphical	O
models	O
–	O
graphical	O
models	O
by	O
s.	O
lauritzen	O
,	O
oxford	O
university	O
press	O
,	O
1996	O
.	O
–	O
bayesian	O
networks	O
and	O
decision	O
graphs	O
by	O
f.	O
jensen	O
and	O
t.	O
d.	O
nielsen	O
,	O
springer	O
verlag	O
,	O
2007	O
.	O
–	O
probabilistic	B
networks	O
and	O
expert	O
systems	O
by	O
r.	O
g.	O
cowell	O
,	O
a.	O
p.	O
dawid	O
,	O
s.	O
l.	O
lauritzen	O
and	O
d.	O
j.	O
spiegelhalter	O
,	O
springer	O
verlag	O
,	O
1999	O
.	O
–	O
probabilistic	B
reasoning	O
in	O
intelligent	O
systems	O
by	O
j.	O
pearl	O
,	O
morgan	O
kaufmann	O
,	O
1988	O
.	O
–	O
graphical	O
models	O
in	O
applied	O
multivariate	B
statistics	O
by	O
j.	O
whittaker	O
,	O
wiley	O
,	O
1990	O
.	O
–	O
probabilistic	B
graphical	O
models	O
:	O
principles	O
and	O
techniques	O
by	O
d.	O
koller	O
and	O
n.	O
friedman	O
,	O
mit	O
press	O
,	O
2009	O
.	O
•	O
machine	O
learning	B
and	O
information	O
processing	O
–	O
information	O
theory	O
,	O
inference	B
and	O
learning	B
algorithms	O
by	O
d.	O
j.	O
c.	O
mackay	O
,	O
cambridge	O
uni-	O
versity	O
press	O
,	O
2003.	O
iv	O
draft	O
march	O
9	O
,	O
2010	O
–	O
pattern	O
recognition	O
and	O
machine	O
learning	B
by	O
c.	O
m.	O
bishop	O
,	O
springer	O
verlag	O
,	O
2006	O
.	O
–	O
an	O
introduction	O
to	O
support	O
vector	O
machines	O
,	O
n.	O
cristianini	O
and	O
j.	O
shawe-taylor	O
,	O
cambridge	O
university	O
press	O
,	O
2000	O
.	O
–	O
gaussian	O
processes	O
for	O
machine	O
learning	B
by	O
c.	O
e.	O
rasmussen	O
and	O
c.	O
k.	O
i.	O
williams	O
,	O
mit	O
press	O
,	O
2006.	O
how	O
to	O
use	O
this	O
book	O
part	O
i	O
would	O
be	O
suitable	O
for	O
an	O
introductory	O
course	O
on	O
graphical	O
models	O
with	O
a	O
focus	O
on	O
inference	B
.	O
part	O
ii	O
contains	O
enough	O
material	O
for	O
a	O
short	O
lecture	O
course	O
on	O
learning	B
in	O
probabilistic	B
models	O
.	O
part	O
iii	O
is	O
reasonably	O
self-contained	O
and	O
would	O
be	O
suitable	O
for	O
a	O
course	O
on	O
machine	O
learning	B
from	O
a	O
probabilistic	B
perspective	O
,	O
particularly	O
combined	O
with	O
the	O
dynamical	O
models	O
material	O
in	O
part	O
iv	O
.	O
part	O
v	O
would	O
be	O
suitable	O
for	O
a	O
short	O
course	O
on	O
approximate	B
inference	I
.	O
accompanying	O
code	O
the	O
matlab	O
code	O
is	O
provided	O
to	O
help	O
readers	O
see	O
how	O
mathematical	O
models	O
translate	O
into	O
actual	O
code	O
.	O
the	O
code	O
is	O
not	O
meant	O
to	O
be	O
an	O
industrial	O
strength	O
research	O
tool	O
,	O
rather	O
a	O
reasonably	O
lightweight	O
toolbox	O
that	O
enables	O
the	O
reader	O
to	O
play	O
with	O
concepts	O
in	O
graph	B
theory	O
,	O
probability	B
theory	O
and	O
machine	O
learning	B
.	O
in	O
an	O
attempt	O
to	O
retain	O
readability	O
,	O
no	O
extensive	O
error	O
and/or	O
exception	O
handling	O
has	O
been	O
included	O
.	O
the	O
code	O
contains	O
at	O
the	O
moment	O
basic	O
routines	O
for	O
manipulating	O
discrete	B
variable	O
distributions	O
,	O
along	O
with	O
a	O
set	O
of	O
routines	O
that	O
are	O
more	O
concerned	O
with	O
continuous	O
variable	B
machine	O
learning	B
.	O
one	O
could	O
in	O
principle	O
extend	O
the	O
‘	O
graphical	O
models	O
’	O
part	O
of	O
the	O
code	O
considerably	O
to	O
support	O
continuous	O
variables	O
.	O
limited	O
support	O
for	O
continuous	B
variables	O
is	O
currently	O
provided	O
so	O
that	O
,	O
for	O
example	O
,	O
inference	B
in	O
the	O
linear	B
dynamical	I
system	I
may	O
be	O
written	O
in	O
conducted	O
of	O
operations	O
on	O
gaussian	O
potentials	O
.	O
however	O
,	O
in	O
general	O
,	O
potentials	O
on	O
continuous	B
variables	O
need	O
to	O
be	O
manipulated	O
with	O
care	O
and	O
often	O
specialised	O
routines	O
are	O
required	O
to	O
ensure	O
numerical	B
stability	I
.	O
acknowledgements	O
many	O
people	O
have	O
helped	O
this	O
book	O
along	O
the	O
way	O
either	O
in	O
terms	O
of	O
reading	O
,	O
feedback	O
,	O
general	O
insights	O
,	O
allowing	O
me	O
to	O
present	O
their	O
work	O
,	O
or	O
just	O
plain	O
motivation	O
.	O
amongst	O
these	O
i	O
would	O
like	O
to	O
thank	O
massim-	O
iliano	O
pontil	O
,	O
mark	O
herbster	O
,	O
john	O
shawe-taylor	O
,	O
vladimir	O
kolmogorov	O
,	O
yuri	O
boykov	O
,	O
tom	O
minka	O
,	O
simon	O
prince	O
,	O
silvia	O
chiappa	O
,	O
bertrand	O
mesot	O
,	O
robert	O
cowell	O
,	O
ali	O
taylan	O
cemgil	O
,	O
david	O
blei	O
,	O
jeﬀ	O
bilmes	O
,	O
david	O
cohn	O
,	O
david	O
page	O
,	O
peter	O
sollich	O
,	O
chris	O
williams	O
,	O
marc	O
toussaint	O
,	O
amos	O
storkey	O
,	O
zakria	O
hussain	O
,	O
seraf´ın	O
moral	O
,	O
milan	O
studen´y	O
,	O
tristan	O
fletcher	O
,	O
tom	O
furmston	O
,	O
ed	O
challis	O
and	O
chris	O
bracegirdle	O
.	O
i	O
would	O
also	O
like	O
to	O
thank	O
the	O
many	O
students	O
that	O
have	O
helped	O
improve	O
the	O
material	O
during	O
lectures	O
over	O
the	O
years	O
.	O
i	O
’	O
m	O
particularly	O
grateful	O
to	O
tom	O
minka	O
for	O
allowing	O
parts	O
of	O
his	O
lightspeed	O
toolbox	O
to	O
be	O
bundled	O
with	O
the	O
brmltoolbox	O
and	O
am	O
similarly	O
indebted	O
to	O
taylan	O
cemgil	O
for	O
his	O
graphlayout	O
package	O
.	O
a	O
ﬁnal	O
thankyou	O
to	O
my	O
family	B
and	O
friends	O
.	O
website	B
the	O
code	O
along	O
with	O
an	O
electronic	O
version	O
of	O
the	O
book	O
is	O
available	O
from	O
http	O
:	O
//www.cs.ucl.ac.uk/staff/d.barber/brml	O
instructors	O
seeking	O
solutions	O
to	O
the	O
exercises	O
can	O
ﬁnd	O
information	O
at	O
the	O
website	B
,	O
along	O
with	O
additional	O
teaching	O
material	O
.	O
the	O
website	B
also	O
contains	O
a	O
feedback	O
form	O
and	O
errata	O
list	O
.	O
draft	O
march	O
9	O
,	O
2010	O
v	O
vi	O
draft	O
march	O
9	O
,	O
2010	O
contents	O
i	O
inference	B
in	O
probabilistic	B
models	O
1	O
1	O
probabilistic	B
reasoning	O
3	O
3	O
1.1	O
probability	B
refresher	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
6	O
1.1.1	O
probability	B
tables	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
7	O
1.1.2	O
interpreting	O
conditional	B
probability	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
1.2	O
probabilistic	B
reasoning	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
8	O
1.3	O
prior	B
,	O
likelihood	B
and	O
posterior	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
10	O
1.3.1	O
two	O
dice	O
:	O
what	O
were	O
the	O
individual	O
scores	O
?	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
10	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
11	O
1.4	O
further	O
worked	O
examples	O
1.5	O
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
15	O
1.5.1	O
basic	O
probability	B
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
15	O
1.5.2	O
general	O
utilities	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
16	O
1.5.3	O
an	O
example	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
17	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
17	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
17	O
1.6	O
notes	O
1.7	O
exercises	O
2	O
basic	O
graph	B
concepts	O
2.1.1	O
19	O
2.1	O
graphs	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
19	O
spanning	B
tree	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
21	O
2.2	O
numerically	O
encoding	O
graphs	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
21	O
2.2.1	O
edge	B
list	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
21	O
2.2.2	O
adjacency	B
matrix	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
21	O
2.2.3	O
clique	B
matrix	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
22	O
2.3	O
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
23	O
2.3.1	O
utility	B
routines	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
23	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
23	O
2.4	O
exercises	O
3	O
belief	B
networks	I
25	O
3.1	O
probabilistic	B
inference	O
in	O
structured	B
distributions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
25	O
3.2	O
graphically	O
representing	O
distributions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
26	O
3.2.1	O
constructing	O
a	O
simple	O
belief	O
network	O
:	O
wet	O
grass	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
26	O
3.2.2	O
uncertain	B
evidence	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
29	O
3.3	O
belief	B
networks	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
32	O
3.3.1	O
conditional	B
independence	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
33	O
3.3.2	O
the	O
impact	O
of	O
collisions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
34	O
d-separation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
35	O
3.3.3	O
3.3.4	O
d-connection	O
and	O
dependence	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
36	O
3.3.5	O
markov	O
equivalence	O
in	O
belief	B
networks	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
37	O
3.3.6	O
belief	B
networks	I
have	O
limited	O
expressibility	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
39	O
vii	O
contents	O
contents	O
3.4.1	O
3.4.2	O
3.4.3	O
learning	B
the	O
direction	O
of	O
arrows	O
3.4	O
causality	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
39	O
simpson	O
’	O
s	O
paradox	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
40	O
inﬂuence	B
diagrams	I
and	O
the	O
do-calculus	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
42	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
43	O
3.5	O
parameterising	O
belief	B
networks	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
43	O
3.6	O
further	O
reading	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
44	O
3.7	O
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
44	O
3.7.1	O
naive	O
inference	O
demo	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
44	O
3.7.2	O
conditional	B
independence	O
demo	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
44	O
3.7.3	O
utility	B
routines	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
44	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
44	O
3.8	O
exercises	O
4	O
graphical	O
models	O
49	O
4.1	O
graphical	O
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
49	O
4.2	O
markov	O
networks	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
50	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
51	O
4.2.1	O
markov	O
properties	B
4.2.2	O
gibbs	O
networks	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
52	O
4.2.3	O
markov	O
random	O
ﬁelds	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
53	O
4.2.4	O
conditional	B
independence	O
using	O
markov	O
networks	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
53	O
4.2.5	O
lattice	O
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
54	O
4.3	O
chain	B
graphical	O
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
55	O
4.4	O
expressiveness	O
of	O
graphical	O
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
56	O
4.5	O
factor	B
graphs	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
58	O
4.5.1	O
conditional	B
independence	O
in	O
factor	B
graphs	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
59	O
4.6	O
notes	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
59	O
4.7	O
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
59	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
59	O
4.8	O
exercises	O
5	O
eﬃcient	B
inference	O
in	O
trees	O
5.2	O
other	O
forms	O
of	O
inference	B
63	O
5.1	O
marginal	B
inference	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
63	O
5.1.1	O
variable	B
elimination	I
in	O
a	O
markov	O
chain	B
and	O
message	B
passing	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
63	O
5.1.2	O
the	O
sum-product	B
algorithm	I
on	O
factor	B
graphs	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
66	O
5.1.3	O
computing	O
the	O
marginal	B
likelihood	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
69	O
5.1.4	O
the	O
problem	B
with	O
loops	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
71	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
71	O
5.2.1	O
max-product	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
71	O
5.2.2	O
finding	O
the	O
n	O
most	O
probable	O
states	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
73	O
5.2.3	O
most	B
probable	I
path	I
and	O
shortest	B
path	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
75	O
5.2.4	O
mixed	B
inference	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
77	O
inference	B
in	O
multiply-connected	B
graphs	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
78	O
5.3.1	O
bucket	B
elimination	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
78	O
5.3.2	O
loop-cut	O
conditioning	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
79	O
5.4	O
message	B
passing	I
for	O
continuous	B
distributions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
80	O
5.5	O
notes	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
80	O
5.6	O
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
81	O
5.6.1	O
factor	B
graph	I
examples	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
81	O
5.6.2	O
most	O
probable	O
and	O
shortest	B
path	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
81	O
5.6.3	O
bucket	B
elimination	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
81	O
5.6.4	O
message	B
passing	I
on	O
gaussians	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
82	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
82	O
5.3	O
5.7	O
exercises	O
viii	O
draft	O
march	O
9	O
,	O
2010	O
contents	O
contents	O
6	O
the	O
junction	B
tree	I
algorithm	O
6.2	O
clique	B
graphs	O
6.3	O
junction	O
trees	O
6.1	O
clustering	B
variables	O
85	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
85	O
6.1.1	O
reparameterisation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
85	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
86	O
6.2.1	O
absorption	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
87	O
6.2.2	O
absorption	B
schedule	O
on	O
clique	B
trees	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
88	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
88	O
6.3.1	O
the	O
running	B
intersection	I
property	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
89	O
6.4	O
constructing	O
a	O
junction	B
tree	I
for	O
singly-connected	B
distributions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
92	O
6.4.1	O
moralisation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
92	O
6.4.2	O
forming	O
the	O
clique	B
graph	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
92	O
6.4.3	O
forming	O
a	O
junction	B
tree	I
from	O
a	O
clique	B
graph	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
92	O
6.4.4	O
assigning	O
potentials	O
to	O
cliques	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
92	O
6.5	O
junction	O
trees	O
for	O
multiply-connected	B
distributions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
93	O
6.5.1	O
triangulation	B
algorithms	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
95	O
6.6	O
the	O
junction	B
tree	I
algorithm	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
97	O
6.6.1	O
remarks	O
on	O
the	O
jta	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
98	O
6.6.2	O
computing	O
the	O
normalisation	B
constant	I
of	O
a	O
distribution	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
99	O
6.6.3	O
the	O
marginal	B
likelihood	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
99	O
6.7	O
finding	O
the	O
most	B
likely	I
state	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
101	O
6.8	O
reabsorption	B
:	O
converting	O
a	O
junction	B
tree	I
to	O
a	O
directed	B
network	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
102	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
103	O
6.9	O
the	O
need	O
for	O
approximations	O
6.9.1	O
bounded	O
width	O
junction	O
trees	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
103	O
6.10	O
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
103	O
6.10.1	O
utility	B
routines	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
103	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
104	O
6.11	O
exercises	O
7	O
making	O
decisions	O
syntax	O
of	O
inﬂuence	B
diagrams	I
7.4	O
solving	B
inﬂuence	O
diagrams	O
7.6	O
temporally	B
unbounded	I
mdps	O
107	O
7.1	O
expected	O
utility	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
107	O
7.1.1	O
utility	B
of	O
money	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
107	O
7.2	O
decision	O
trees	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
108	O
7.3	O
extending	O
bayesian	O
networks	O
for	O
decisions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
111	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
111	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
115	O
7.4.1	O
eﬃcient	B
inference	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
115	O
7.4.2	O
using	O
a	O
junction	B
tree	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
116	O
7.5	O
markov	O
decision	O
processes	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
120	O
7.5.1	O
maximising	O
expected	O
utility	B
by	O
message	B
passing	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
120	O
7.5.2	O
bellman	O
’	O
s	O
equation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
121	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
122	O
7.6.1	O
value	B
iteration	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
122	O
7.6.2	O
policy	B
iteration	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
123	O
7.6.3	O
a	O
curse	B
of	I
dimensionality	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
124	O
7.7	O
probabilistic	B
inference	O
and	O
planning	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
124	O
7.7.1	O
non-stationary	B
markov	O
decision	O
process	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
124	O
7.7.2	O
non-stationary	B
probabilistic	O
inference	B
planner	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
125	O
7.7.3	O
stationary	B
planner	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
125	O
7.7.4	O
utilities	O
at	O
each	O
timestep	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
127	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
129	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
129	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
130	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
130	O
7.9	O
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
131	O
sum/max	O
under	O
a	O
partial	B
order	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
131	O
junction	O
trees	O
for	O
inﬂuence	B
diagrams	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
131	O
7.8.1	O
partially	B
observable	I
mdps	O
7.8.2	O
restricted	B
utility	O
functions	O
7.8.3	O
reinforcement	B
learning	I
7.8	O
further	O
topics	O
7.3.1	O
7.9.1	O
7.9.2	O
draft	O
march	O
9	O
,	O
2010	O
ix	O
contents	O
contents	O
7.9.3	O
party-friend	O
example	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
131	O
7.9.4	O
chest	B
clinic	I
with	O
decisions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
131	O
7.9.5	O
markov	O
decision	O
processes	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
133	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
133	O
7.10	O
exercises	O
ii	O
learning	B
in	O
probabilistic	B
models	O
137	O
8	O
statistics	O
for	O
machine	O
learning	B
:	O
8.2.1	O
estimator	B
bias	O
8.3	O
discrete	B
distributions	O
8.4	O
continuous	B
distributions	O
139	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
139	O
8.1	O
distributions	O
8.2	O
summarising	O
distributions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
139	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
142	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
143	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
144	O
8.4.1	O
bounded	O
distributions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
144	O
8.4.2	O
unbounded	O
distributions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
146	O
8.5	O
multivariate	B
distributions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
147	O
8.6	O
multivariate	B
gaussian	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
148	O
8.6.1	O
conditioning	B
as	O
system	B
reversal	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
151	O
8.6.2	O
completing	O
the	O
square	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
151	O
8.6.3	O
gaussian	O
propagation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
152	O
8.6.4	O
whitening	B
and	O
centering	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
152	O
8.6.5	O
maximum	B
likelihood	I
training	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
152	O
8.6.6	O
bayesian	O
inference	B
of	O
the	O
mean	B
and	O
variance	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
153	O
8.6.7	O
gauss-gamma	O
distribution	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
155	O
8.7	O
exponential	B
family	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
155	O
8.7.1	O
conjugate	B
priors	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
156	O
8.8	O
the	O
kullback-leibler	O
divergence	B
kl	O
(	O
q|p	O
)	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
157	O
8.8.1	O
entropy	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
157	O
8.9	O
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
158	O
8.10	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
158	O
9	O
learning	B
as	O
inference	B
165	O
9.1	O
learning	B
as	O
inference	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
165	O
9.1.1	O
learning	B
the	O
bias	B
of	O
a	O
coin	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
165	O
9.1.2	O
making	O
decisions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
167	O
9.1.3	O
a	O
continuum	O
of	O
parameters	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
167	O
9.1.4	O
decisions	O
based	O
on	O
continuous	B
intervals	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
168	O
9.2	O
maximum	O
a	O
posteriori	O
and	O
maximum	B
likelihood	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
169	O
9.2.1	O
summarising	O
the	O
posterior	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
169	O
9.2.2	O
maximum	B
likelihood	I
and	O
the	O
empirical	B
distribution	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
170	O
9.2.3	O
maximum	B
likelihood	I
training	O
of	O
belief	B
networks	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
171	O
9.3	O
bayesian	O
belief	B
network	I
training	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
174	O
9.3.1	O
global	B
and	O
local	B
parameter	O
independence	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
174	O
9.3.2	O
learning	B
binary	O
variable	B
tables	O
using	O
a	O
beta	B
prior	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
176	O
9.3.3	O
learning	B
multivariate	O
discrete	B
tables	O
using	O
a	O
dirichlet	O
prior	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
178	O
9.3.4	O
parents	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
179	O
9.3.5	O
structure	B
learning	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
180	O
9.3.6	O
empirical	B
independence	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
182	O
9.3.7	O
network	B
scoring	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
184	O
9.4	O
maximum	B
likelihood	I
for	O
undirected	B
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
185	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
186	O
9.4.1	O
the	O
likelihood	B
gradient	O
9.4.2	O
decomposable	B
markov	O
networks	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
187	O
9.4.3	O
non-decomposable	O
markov	O
networks	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
188	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
189	O
9.4.4	O
constrained	O
decomposable	O
markov	O
networks	O
x	O
draft	O
march	O
9	O
,	O
2010	O
contents	O
contents	O
9.4.5	O
iterative	B
scaling	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
192	O
9.4.6	O
conditional	O
random	O
ﬁelds	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
193	O
9.4.7	O
pseudo	B
likelihood	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
196	O
9.4.8	O
learning	B
the	O
structure	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
196	O
9.5	O
properties	B
of	O
maximum	B
likelihood	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
196	O
9.5.1	O
training	B
assuming	O
the	O
correct	O
model	B
class	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
196	O
9.5.2	O
training	B
when	O
the	O
assumed	O
model	B
is	O
incorrect	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
197	O
9.6	O
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
197	O
9.6.1	O
pc	O
algorithm	B
using	O
an	O
oracle	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
197	O
9.6.2	O
demo	O
of	O
empirical	B
conditional	O
independence	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
197	O
9.6.3	O
bayes	O
dirichlet	O
structure	B
learning	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
198	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
198	O
9.7	O
exercises	O
10	O
naive	O
bayes	O
203	O
10.1	O
naive	O
bayes	O
and	O
conditional	B
independence	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
203	O
10.2	O
estimation	O
using	O
maximum	B
likelihood	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
204	O
10.2.1	O
binary	O
attributes	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
204	O
10.2.2	O
multi-state	O
variables	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
207	O
10.2.3	O
text	O
classiﬁcation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
208	O
10.3	O
bayesian	O
naive	O
bayes	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
208	O
10.4	O
tree	B
augmented	I
naive	O
bayes	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
210	O
10.4.1	O
chow-liu	O
trees	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
210	O
10.4.2	O
learning	B
tree	O
augmented	B
naive	O
bayes	O
networks	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
212	O
10.5	O
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
213	O
10.6	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
213	O
11	O
learning	B
with	O
hidden	B
variables	I
217	O
11.1	O
hidden	B
variables	I
and	O
missing	B
data	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
217	O
11.1.1	O
why	O
hidden/missing	O
variables	O
can	O
complicate	O
proceedings	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
217	O
11.1.2	O
the	O
missing	B
at	I
random	I
assumption	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
218	O
11.1.3	O
maximum	B
likelihood	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
219	O
11.1.4	O
identiﬁability	B
issues	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
219	O
11.2	O
expectation	B
maximisation	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
220	O
11.2.1	O
variational	O
em	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
220	O
11.2.2	O
classical	O
em	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
221	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
224	O
11.2.3	O
application	O
to	O
belief	B
networks	I
11.2.4	O
application	O
to	O
markov	O
networks	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
228	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
229	O
11.2.5	O
convergence	O
11.3	O
extensions	O
of	O
em	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
229	O
11.3.1	O
partial	O
m	O
step	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
229	O
11.3.2	O
partial	O
e	O
step	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
229	O
11.4	O
a	O
failure	B
case	I
for	O
em	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
230	O
11.5	O
variational	O
bayes	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
231	O
11.5.1	O
em	O
is	O
a	O
special	O
case	O
of	O
variational	O
bayes	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
233	O
11.5.2	O
factorising	O
the	O
parameter	B
posterior	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
233	O
11.6	O
bayesian	O
methods	O
and	O
ml-ii	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
236	O
11.7	O
optimising	O
the	O
likelihood	B
by	O
gradient	B
methods	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
236	O
11.7.1	O
directed	B
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
236	O
11.7.2	O
undirected	B
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
237	O
11.8	O
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
237	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
237	O
11.9	O
exercises	O
draft	O
march	O
9	O
,	O
2010	O
xi	O
contents	O
contents	O
12	O
bayesian	O
model	B
selection	I
241	O
12.1	O
comparing	O
models	O
the	O
bayesian	O
way	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
241	O
12.2	O
illustrations	O
:	O
coin	O
tossing	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
242	O
12.2.1	O
a	O
discrete	B
parameter	O
space	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
242	O
12.2.2	O
a	O
continuous	B
parameter	O
space	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
242	O
12.3	O
occam	O
’	O
s	O
razor	O
and	O
bayesian	O
complexity	O
penalisation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
244	O
12.4	O
a	O
continuous	B
example	O
:	O
curve	O
ﬁtting	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
245	O
12.5	O
approximating	O
the	O
model	B
likelihood	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
246	O
12.5.1	O
laplace	O
’	O
s	O
method	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
246	O
12.5.2	O
bayes	O
information	O
criterion	O
(	O
bic	O
)	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
247	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
247	O
12.6	O
exercises	O
iii	O
machine	O
learning	B
249	O
13	O
machine	O
learning	B
concepts	O
13.1	O
styles	O
of	O
learning	B
251	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
251	O
13.1.1	O
supervised	B
learning	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
251	O
13.1.2	O
unsupervised	B
learning	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
252	O
13.1.3	O
anomaly	B
detection	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
253	O
13.1.4	O
online	B
(	O
sequential	B
)	O
learning	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
253	O
13.1.5	O
interacting	O
with	O
the	O
environment	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
253	O
13.1.6	O
semi-supervised	B
learning	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
254	O
13.2	O
supervised	B
learning	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
254	O
13.2.1	O
utility	B
and	O
loss	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
254	O
13.2.2	O
what	O
’	O
s	O
the	O
catch	O
?	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
255	O
13.2.3	O
using	O
the	O
empirical	B
distribution	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
255	O
13.2.4	O
bayesian	O
decision	O
approach	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
258	O
13.2.5	O
learning	B
lower-dimensional	O
representations	O
in	O
semi-supervised	B
learning	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
261	O
13.2.6	O
features	O
and	O
preprocessing	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
262	O
13.3	O
bayes	O
versus	O
empirical	B
decisions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
262	O
13.4	O
representing	O
data	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
263	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
263	O
13.4.1	O
categorical	O
13.4.2	O
ordinal	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
263	O
13.4.3	O
numerical	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
263	O
13.5	O
bayesian	O
hypothesis	B
testing	I
for	O
outcome	B
analysis	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
263	O
13.5.1	O
outcome	B
analysis	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
264	O
13.5.2	O
hdiﬀ	O
:	O
model	B
likelihood	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
265	O
13.5.3	O
hsame	O
:	O
model	B
likelihood	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
265	O
13.5.4	O
dependent	O
outcome	B
analysis	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
266	O
13.5.5	O
is	O
classiﬁer	B
a	O
better	O
than	O
b	O
?	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
268	O
13.6	O
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
269	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
270	O
13.7	O
notes	O
13.8	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
270	O
14	O
nearest	B
neighbour	I
classiﬁcation	O
273	O
14.1	O
do	O
as	O
your	O
neighbour	B
does	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
273	O
14.2	O
k-nearest	O
neighbours	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
274	O
14.3	O
a	O
probabilistic	B
interpretation	O
of	O
nearest	O
neighbours	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
275	O
14.3.1	O
when	O
your	O
nearest	B
neighbour	I
is	O
far	O
away	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
277	O
14.4	O
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
277	O
14.4.1	O
utility	B
routines	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
277	O
14.4.2	O
demonstration	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
277	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
277	O
14.5	O
exercises	O
xii	O
draft	O
march	O
9	O
,	O
2010	O
contents	O
contents	O
15	O
unsupervised	B
linear	O
dimension	B
reduction	I
15.4	O
latent	B
semantic	I
analysis	I
15.4.1	O
lsa	O
for	O
information	B
retrieval	I
279	O
15.1	O
high-dimensional	O
spaces	O
–	O
low	B
dimensional	I
manifolds	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
279	O
15.2	O
principal	O
components	O
analysis	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
279	O
15.2.1	O
deriving	O
the	O
optimal	O
linear	B
reconstruction	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
280	O
15.2.2	O
maximum	O
variance	O
criterion	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
282	O
15.2.3	O
pca	O
algorithm	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
282	O
15.2.4	O
pca	O
and	O
nearest	O
neighbours	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
284	O
15.2.5	O
comments	O
on	O
pca	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
285	O
15.3	O
high	B
dimensional	I
data	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
285	O
15.3.1	O
eigen-decomposition	O
for	O
n	O
<	O
d	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
286	O
15.3.2	O
pca	O
via	O
singular	B
value	O
decomposition	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
286	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
287	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
288	O
15.5	O
pca	O
with	O
missing	O
data	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
289	O
15.5.1	O
finding	O
the	O
principal	B
directions	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
291	O
15.5.2	O
collaborative	B
ﬁltering	I
using	O
pca	O
with	O
missing	O
data	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
291	O
15.6	O
matrix	B
decomposition	O
methods	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
292	O
15.6.1	O
probabilistic	B
latent	I
semantic	I
analysis	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
292	O
15.6.2	O
extensions	O
and	O
variations	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
295	O
15.6.3	O
applications	O
of	O
plsa/nmf	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
296	O
15.7	O
kernel	B
pca	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
298	O
15.8	O
canonical	B
correlation	I
analysis	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
300	O
15.8.1	O
svd	O
formulation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
301	O
15.9	O
notes	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
301	O
15.10code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
301	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
301	O
15.11exercises	O
16	O
supervised	B
linear	O
dimension	B
reduction	I
303	O
16.1	O
supervised	B
linear	O
projections	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
303	O
16.2	O
fisher	O
’	O
s	O
linear	O
discriminant	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
303	O
16.3	O
canonical	B
variates	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
305	O
16.3.1	O
dealing	O
with	O
the	O
nullspace	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
307	O
16.4	O
using	O
non-gaussian	O
data	B
distributions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
308	O
16.5	O
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
308	O
16.6	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
308	O
17	O
linear	B
models	O
17.3	O
the	O
dual	B
representation	I
and	O
kernels	O
311	O
17.1	O
introduction	O
:	O
fitting	O
a	O
straight	O
line	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
311	O
17.2	O
linear	O
parameter	O
models	O
for	O
regression	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
312	O
17.2.1	O
vector	O
outputs	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
314	O
17.2.2	O
regularisation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
314	O
17.2.3	O
radial	B
basis	I
functions	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
315	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
316	O
17.3.1	O
regression	B
in	O
the	O
dual-space	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
317	O
17.3.2	O
positive	B
deﬁnite	I
kernels	O
(	O
covariance	B
functions	O
)	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
318	O
17.4	O
linear	O
parameter	O
models	O
for	O
classiﬁcation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
319	O
17.4.1	O
logistic	B
regression	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
319	O
17.4.2	O
maximum	B
likelihood	I
training	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
321	O
17.4.3	O
beyond	O
ﬁrst	B
order	I
gradient	O
ascent	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
324	O
17.4.4	O
avoiding	O
overconﬁdent	O
classiﬁcation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
324	O
17.4.5	O
multiple	B
classes	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
324	O
17.5	O
the	O
kernel	B
trick	O
for	O
classiﬁcation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
324	O
17.6	O
support	O
vector	O
machines	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
325	O
17.6.1	O
maximum	O
margin	O
linear	B
classiﬁer	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
325	O
17.6.2	O
using	O
kernels	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
328	O
draft	O
march	O
9	O
,	O
2010	O
xiii	O
contents	O
contents	O
17.6.3	O
performing	O
the	O
optimisation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
329	O
17.6.4	O
probabilistic	B
interpretation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
329	O
17.7	O
soft	B
zero-one	O
loss	O
for	O
outlier	B
robustness	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
329	O
17.8	O
notes	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
330	O
17.9	O
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
330	O
17.10exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
330	O
18	O
bayesian	O
linear	B
models	O
333	O
18.1	O
regression	B
with	O
additive	O
gaussian	O
noise	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
333	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
334	O
18.1.1	O
bayesian	O
linear	O
parameter	O
models	O
18.1.2	O
determining	O
hyperparameters	O
:	O
ml-ii	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
335	O
18.1.3	O
learning	B
the	O
hyperparameters	O
using	O
em	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
336	O
18.1.4	O
hyperparameter	B
optimisation	O
:	O
using	O
the	O
gradient	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
337	O
18.1.5	O
validation	B
likelihood	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
338	O
18.1.6	O
prediction	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
339	O
18.1.7	O
the	O
relevance	B
vector	I
machine	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
339	O
18.2	O
classiﬁcation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
340	O
18.2.1	O
hyperparameter	B
optimisation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
340	O
18.2.2	O
laplace	O
approximation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
341	O
18.2.3	O
making	O
predictions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
342	O
18.2.4	O
relevance	B
vector	I
machine	I
for	O
classiﬁcation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
344	O
18.2.5	O
multi-class	O
case	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
345	O
18.3	O
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
345	O
18.4	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
345	O
19	O
gaussian	O
processes	O
347	O
19.1	O
non-parametric	B
prediction	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
347	O
19.1.1	O
from	O
parametric	O
to	O
non-parametric	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
347	O
19.1.2	O
from	O
bayesian	O
linear	B
models	O
to	O
gaussian	O
processes	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
348	O
19.1.3	O
a	O
prior	B
on	O
functions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
349	O
19.2	O
gaussian	O
process	O
prediction	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
350	O
19.2.1	O
regression	B
with	O
noisy	O
training	O
outputs	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
350	O
19.3	O
covariance	B
functions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
351	O
19.3.1	O
making	O
new	O
covariance	B
functions	O
from	O
old	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
352	O
19.3.2	O
stationary	B
covariance	O
functions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
353	O
19.3.3	O
non-stationary	B
covariance	O
functions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
355	O
19.4	O
analysis	B
of	O
covariance	B
functions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
356	O
19.4.1	O
smoothness	B
of	O
the	O
functions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
356	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
356	O
19.4.2	O
mercer	O
kernels	O
19.4.3	O
fourier	O
analysis	B
for	O
stationary	B
kernels	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
358	O
19.5	O
gaussian	O
processes	O
for	O
classiﬁcation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
358	O
19.5.1	O
binary	O
classiﬁcation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
359	O
19.5.2	O
laplace	O
’	O
s	O
approximation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
359	O
19.5.3	O
hyperparameter	B
optimisation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
362	O
19.5.4	O
multiple	B
classes	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
362	O
19.6	O
further	O
reading	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
362	O
19.7	O
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
362	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
363	O
19.8	O
exercises	O
20	O
mixture	B
models	O
365	O
20.1	O
density	B
estimation	I
using	O
mixtures	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
365	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
366	O
20.2	O
expectation	B
maximisation	I
for	O
mixture	B
models	O
20.2.1	O
unconstrained	O
discrete	B
tables	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
366	O
20.2.2	O
mixture	O
of	O
product	O
of	O
bernoulli	O
distributions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
368	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
370	O
20.3	O
the	O
gaussian	O
mixture	B
model	I
xiv	O
draft	O
march	O
9	O
,	O
2010	O
contents	O
contents	O
20.3.1	O
em	O
algorithm	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
370	O
20.3.2	O
practical	O
issues	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
373	O
20.3.3	O
classiﬁcation	B
using	O
gaussian	O
mixture	B
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
373	O
20.3.4	O
the	O
parzen	O
estimator	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
375	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
375	O
20.3.5	O
k-means	B
20.3.6	O
bayesian	O
mixture	B
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
376	O
20.3.7	O
semi-supervised	B
learning	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
376	O
20.4	O
mixture	B
of	I
experts	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
377	O
20.5	O
indicator	O
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
378	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
378	O
20.5.1	O
joint	B
indicator	O
approach	B
:	O
factorised	B
prior	O
20.5.2	O
joint	B
indicator	O
approach	B
:	O
polya	O
prior	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
378	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
380	O
20.6.1	O
latent	B
dirichlet	O
allocation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
380	O
20.6.2	O
graph	B
based	O
representations	O
of	O
data	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
381	O
20.6.3	O
dyadic	B
data	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
382	O
20.6.4	O
monadic	B
data	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
383	O
20.6.5	O
cliques	O
and	O
adjacency	B
matrices	O
for	O
monadic	B
binary	O
data	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
383	O
20.7	O
further	O
reading	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
387	O
20.8	O
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
387	O
20.9	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
387	O
20.6	O
mixed	B
membership	I
models	O
21	O
latent	O
linear	O
models	O
389	O
21.1	O
factor	B
analysis	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
389	O
21.1.1	O
finding	O
the	O
optimal	O
bias	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
390	O
21.2	O
factor	B
analysis	I
:	O
maximum	B
likelihood	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
391	O
21.2.1	O
direct	O
likelihood	B
optimisation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
391	O
21.2.2	O
expectation	B
maximisation	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
394	O
21.3	O
interlude	O
:	O
modelling	B
faces	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
395	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
397	O
21.4	O
probabilistic	B
principal	O
components	O
analysis	B
21.5	O
canonical	B
correlation	I
analysis	I
and	O
factor	B
analysis	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
398	O
21.6	O
independent	B
components	I
analysis	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
399	O
21.7	O
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
401	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
401	O
21.8	O
exercises	O
22	O
latent	O
ability	O
models	O
403	O
22.1	O
the	O
rasch	O
model	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
403	O
22.1.1	O
maximum	B
likelihood	I
training	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
403	O
22.1.2	O
bayesian	O
rasch	O
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
404	O
22.2	O
competition	B
models	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
404	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
404	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
406	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
406	O
22.3	O
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
407	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
407	O
22.4	O
exercises	O
22.2.1	O
bradly-terry-luce	O
model	B
22.2.2	O
elo	O
ranking	O
model	B
22.2.3	O
glicko	O
and	O
trueskill	O
iv	O
dynamical	O
models	O
409	O
23	O
discrete-state	O
markov	O
models	O
411	O
23.1	O
markov	O
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
411	O
23.1.1	O
equilibrium	O
and	O
stationary	B
distribution	I
of	O
a	O
markov	O
chain	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
412	O
23.1.2	O
fitting	O
markov	O
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
413	O
23.1.3	O
mixture	O
of	O
markov	O
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
414	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
416	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
416	O
23.2.1	O
the	O
classical	O
inference	B
problems	O
23.2	O
hidden	B
markov	O
models	O
draft	O
march	O
9	O
,	O
2010	O
xv	O
contents	O
contents	O
23.3	O
learning	B
hmms	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
417	O
23.2.2	O
filtering	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
418	O
23.2.3	O
parallel	B
smoothing	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
23.2.4	O
correction	O
smoothing	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
418	O
23.2.5	O
most	O
likely	O
joint	O
state	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
420	O
23.2.6	O
self	O
localisation	O
and	O
kidnapped	O
robots	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
421	O
23.2.7	O
natural	B
language	O
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
422	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
422	O
23.3.1	O
em	O
algorithm	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
423	O
23.3.2	O
mixture	B
emission	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
424	O
23.3.3	O
the	O
hmm-gmm	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
425	O
23.3.4	O
discriminative	B
training	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
425	O
23.4	O
related	O
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
426	O
23.4.1	O
explicit	O
duration	B
model	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
426	O
23.4.2	O
input-output	B
hmm	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
427	O
23.4.3	O
linear	B
chain	O
crfs	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
428	O
23.4.4	O
dynamic	B
bayesian	O
networks	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
430	O
23.5	O
applications	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
430	O
23.5.1	O
object	O
tracking	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
430	O
23.5.2	O
automatic	O
speech	O
recognition	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
430	O
23.5.3	O
bioinformatics	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
431	O
23.5.4	O
part-of-speech	B
tagging	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
431	O
23.6	O
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
432	O
23.7	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
432	O
24	O
continuous-state	O
markov	O
models	O
24.1.1	O
stationary	B
distribution	I
with	O
noise	O
437	O
24.1	O
observed	O
linear	O
dynamical	O
systems	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
437	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
438	O
24.2	O
auto-regressive	B
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
438	O
24.2.1	O
training	B
an	O
ar	O
model	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
439	O
24.2.2	O
ar	O
model	B
as	O
an	O
olds	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
440	O
24.2.3	O
time-varying	B
ar	O
model	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
440	O
24.3	O
latent	O
linear	O
dynamical	O
systems	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
442	O
24.4	O
inference	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
443	O
24.4.1	O
filtering	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
444	O
24.4.2	O
smoothing	B
:	O
rauch-tung-striebel	O
correction	O
method	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
446	O
24.4.3	O
the	O
likelihood	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
447	O
24.4.4	O
most	B
likely	I
state	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
448	O
24.4.5	O
time	O
independence	B
and	O
riccati	O
equations	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
448	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
449	O
24.5.1	O
identiﬁability	B
issues	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
449	O
24.5.2	O
em	O
algorithm	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
450	O
24.5.3	O
subspace	O
methods	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
451	O
24.5.4	O
structured	B
ldss	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
452	O
24.5.5	O
bayesian	O
ldss	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
452	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
452	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
452	O
24.6.1	O
inference	B
24.6.2	O
maximum	B
likelihood	I
learning	O
using	O
em	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
453	O
24.7	O
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
454	O
24.7.1	O
autoregressive	O
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
455	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
455	O
24.5	O
learning	B
linear	O
dynamical	O
systems	O
24.6	O
switching	B
auto-regressive	O
models	O
24.8	O
exercises	O
xvi	O
draft	O
march	O
9	O
,	O
2010	O
contents	O
contents	O
25	O
switching	O
linear	O
dynamical	O
systems	O
457	O
25.1	O
introduction	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
457	O
25.2	O
the	O
switching	B
lds	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
457	O
25.2.1	O
exact	O
inference	O
is	O
computationally	O
intractable	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
458	O
25.3	O
gaussian	O
sum	O
filtering	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
458	O
25.3.1	O
continuous	B
ﬁltering	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
459	O
25.3.2	O
discrete	B
ﬁltering	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
461	O
25.3.3	O
the	O
likelihood	B
p	O
(	O
v1	O
:	O
t	O
)	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
461	O
25.3.4	O
collapsing	O
gaussians	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
461	O
25.3.5	O
relation	O
to	O
other	O
methods	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
462	O
25.4	O
gaussian	O
sum	O
smoothing	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
462	O
25.4.1	O
continuous	B
smoothing	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
464	O
25.4.2	O
discrete	B
smoothing	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
464	O
25.4.3	O
collapsing	B
the	I
mixture	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
464	O
25.4.4	O
using	O
mixtures	O
in	O
smoothing	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
465	O
25.4.5	O
relation	O
to	O
other	O
methods	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
466	O
25.5	O
reset	O
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
468	O
25.5.1	O
a	O
poisson	O
reset	B
model	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
470	O
25.5.2	O
hmm-reset	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
471	O
25.6	O
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
472	O
25.7	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
472	O
26	O
distributed	B
computation	I
475	O
26.1	O
introduction	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
475	O
26.2	O
stochastic	O
hopﬁeld	O
networks	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
475	O
26.3	O
learning	B
sequences	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
476	O
26.3.1	O
a	O
single	O
sequence	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
476	O
26.3.2	O
multiple	O
sequences	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
481	O
26.3.3	O
boolean	O
networks	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
482	O
26.3.4	O
sequence	O
disambiguation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
482	O
26.4	O
tractable	O
continuous	B
latent	O
variable	B
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
482	O
26.4.1	O
deterministic	O
latent	O
variables	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
482	O
26.4.2	O
an	O
augmented	B
hopﬁeld	O
network	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
483	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
484	O
26.5.1	O
stochastically	O
spiking	O
neurons	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
485	O
26.5.2	O
hopﬁeld	O
membrane	O
potential	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
485	O
26.5.3	O
dynamic	B
synapses	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
486	O
26.5.4	O
leaky	B
integrate	I
and	I
ﬁre	I
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
486	O
26.6	O
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
487	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
487	O
26.7	O
exercises	O
26.5	O
neural	O
models	O
v	O
approximate	B
inference	I
489	O
27	O
sampling	B
491	O
27.1	O
introduction	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
491	O
27.1.1	O
univariate	B
sampling	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
492	O
27.1.2	O
multi-variate	B
sampling	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
493	O
27.2	O
ancestral	B
sampling	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
494	O
27.2.1	O
dealing	O
with	O
evidence	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
494	O
27.2.2	O
perfect	B
sampling	I
for	O
a	O
markov	O
network	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
495	O
27.3	O
gibbs	O
sampling	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
495	O
27.3.1	O
gibbs	O
sampling	B
as	O
a	O
markov	O
chain	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
496	O
27.3.2	O
structured	B
gibbs	O
sampling	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
497	O
27.3.3	O
remarks	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
498	O
draft	O
march	O
9	O
,	O
2010	O
xvii	O
contents	O
contents	O
27.4	O
markov	O
chain	B
monte	O
carlo	O
(	O
mcmc	O
)	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
499	O
27.4.1	O
markov	O
chains	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
499	O
27.4.2	O
metropolis-hastings	O
sampling	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
499	O
27.5	O
auxiliary	B
variable	I
methods	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
501	O
27.5.1	O
hybrid	O
monte	O
carlo	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
502	O
27.5.2	O
swendson-wang	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
504	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
505	O
27.5.3	O
slice	B
sampling	I
27.6	O
importance	B
sampling	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
506	O
27.6.1	O
sequential	B
importance	I
sampling	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
508	O
27.6.2	O
particle	O
ﬁltering	O
as	O
an	O
approximate	B
forward	O
pass	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
509	O
27.7	O
code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
512	O
27.8	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
512	O
28	O
deterministic	B
approximate	O
inference	B
515	O
28.1	O
introduction	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
515	O
28.2	O
the	O
laplace	O
approximation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
515	O
28.3	O
properties	B
of	O
kullback-leibler	O
variational	B
inference	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
516	O
28.3.1	O
bounding	O
the	O
normalisation	B
constant	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
516	O
28.3.2	O
bounding	O
the	O
marginal	B
likelihood	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
517	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
517	O
28.3.3	O
gaussian	O
approximations	O
using	O
kl	O
divergence	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
518	O
28.3.4	O
moment	O
matching	O
properties	B
of	O
minimising	O
kl	O
(	O
p|q	O
)	O
28.4	O
variational	O
bounding	O
using	O
kl	O
(	O
q|p	O
)	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
519	O
28.4.1	O
pairwise	B
markov	O
random	B
ﬁeld	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
519	O
28.4.2	O
general	O
mean	O
ﬁeld	O
equations	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
522	O
28.4.3	O
asynchronous	B
updating	I
guarantees	O
approximation	B
improvement	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
522	O
28.4.4	O
intractable	B
energy	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
523	O
28.4.5	O
structured	B
variational	O
approximation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
524	O
28.5	O
mutual	B
information	I
maximisation	O
:	O
a	O
kl	O
variational	B
approach	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
524	O
28.5.1	O
the	O
information	B
maximisation	I
algorithm	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
525	O
28.5.2	O
linear	B
gaussian	O
decoder	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
526	O
28.6	O
loopy	B
belief	O
propagation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
526	O
28.6.1	O
classical	O
bp	O
on	O
an	O
undirected	B
graph	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
527	O
28.6.2	O
loopy	B
bp	O
as	O
a	O
variational	O
procedure	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
527	O
28.7	O
expectation	B
propagation	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
530	O
28.8	O
map	B
for	O
mrfs	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
533	O
28.8.1	O
map	B
assignment	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
533	O
28.8.2	O
attractive	B
binary	I
mrfs	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
534	O
28.8.3	O
potts	O
model	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
536	O
28.9	O
further	O
reading	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
538	O
28.10code	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
538	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
538	O
28.11exercises	O
a	O
background	O
mathematics	O
543	O
a.1	O
linear	B
algebra	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
543	O
a.1.1	O
vector	B
algebra	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
543	O
a.1.2	O
the	O
scalar	B
product	I
as	O
a	O
projection	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
544	O
a.1.3	O
lines	O
in	O
space	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
544	O
a.1.4	O
planes	O
and	O
hyperplanes	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
545	O
a.1.5	O
matrices	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
545	O
a.1.6	O
linear	B
transformations	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
546	O
a.1.7	O
determinants	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
547	O
a.1.8	O
matrix	B
inversion	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
548	O
a.1.9	O
computing	O
the	O
matrix	B
inverse	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
548	O
a.1.10	O
eigenvalues	O
and	O
eigenvectors	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
548	O
a.1.11	O
matrix	B
decompositions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
550	O
xviii	O
draft	O
march	O
9	O
,	O
2010	O
contents	O
contents	O
a.5.1	O
critical	O
points	O
a.2	O
matrix	B
identities	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
551	O
a.3	O
multivariate	B
calculus	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
551	O
a.3.1	O
interpreting	O
the	O
gradient	B
vector	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
552	O
a.3.2	O
higher	O
derivatives	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
552	O
a.3.3	O
chain	B
rule	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
553	O
a.3.4	O
matrix	B
calculus	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
553	O
a.4	O
inequalities	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
554	O
a.4.1	O
convexity	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
554	O
a.4.2	O
jensen	O
’	O
s	O
inequality	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
554	O
a.5	O
optimisation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
555	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
555	O
a.6	O
gradient	B
descent	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
555	O
a.6.1	O
gradient	B
descent	I
with	O
ﬁxed	O
stepsize	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
556	O
a.6.2	O
gradient	B
descent	I
with	O
momentum	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
556	O
a.6.3	O
gradient	B
descent	I
with	O
line	O
searches	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
557	O
a.6.4	O
exact	O
line	O
search	O
condition	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
557	O
a.7	O
multivariate	B
minimization	O
:	O
quadratic	O
functions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
557	O
a.7.1	O
minimising	O
quadratic	O
functions	O
using	O
line	B
search	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
557	O
a.7.2	O
gram-schmidt	O
construction	B
of	O
conjugate	O
vectors	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
558	O
a.7.3	O
the	O
conjugate	B
vectors	I
algorithm	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
559	O
a.7.4	O
the	O
conjugate	B
gradients	I
algorithm	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
560	O
a.7.5	O
newton	O
’	O
s	O
method	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
561	O
a.7.6	O
quasi-newton	O
methods	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
561	O
a.7	O
constrained	B
optimisation	I
using	O
lagrange	O
multipliers	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
562	O
draft	O
march	O
9	O
,	O
2010	O
xix	O
contents	O
contents	O
xx	O
draft	O
march	O
9	O
,	O
2010	O
part	O
i	O
inference	B
in	O
probabilistic	B
models	O
1	O
chapter	O
1	O
probabilistic	O
reasoning	O
1.1	O
probability	B
refresher	O
variables	O
,	O
states	O
and	O
notational	O
shortcuts	O
variables	O
will	O
be	O
denoted	O
using	O
either	O
upper	O
case	O
x	O
or	O
lower	O
case	O
x	O
and	O
a	O
set	O
of	O
variables	O
will	O
typically	O
be	O
denoted	O
by	O
a	O
calligraphic	O
symbol	O
,	O
for	O
example	O
v	O
=	O
{	O
a	O
,	O
b	O
,	O
c	O
}	O
.	O
the	O
domain	B
of	O
a	O
variable	B
x	O
is	O
written	O
dom	O
(	O
x	O
)	O
,	O
and	O
denotes	O
the	O
states	O
x	O
can	O
take	O
.	O
states	O
will	O
typically	O
be	O
represented	O
using	O
sans-serif	O
font	O
.	O
for	O
example	O
,	O
for	O
a	O
coin	O
c	O
,	O
we	O
might	O
have	O
dom	O
(	O
c	O
)	O
=	O
{	O
heads	O
,	O
tails	O
}	O
and	O
p	O
(	O
c	O
=	O
heads	O
)	O
represents	O
the	O
probability	B
that	O
variable	B
c	O
is	O
in	O
state	O
heads	O
.	O
the	O
meaning	O
of	O
p	O
(	O
state	O
)	O
will	O
often	O
be	O
clear	O
,	O
without	O
speciﬁc	O
reference	O
to	O
a	O
variable	B
.	O
for	O
example	O
,	O
if	O
we	O
are	O
discussing	O
an	O
experiment	O
about	O
a	O
coin	O
c	O
,	O
the	O
meaning	O
of	O
p	O
(	O
heads	O
)	O
is	O
clear	O
from	O
the	O
context	O
,	O
being	O
short-	O
x	O
f	O
(	O
x	O
)	O
,	O
the	O
hand	O
for	O
p	O
(	O
c	O
=	O
heads	O
)	O
.	O
when	O
summing	O
(	O
or	O
performing	O
some	O
other	O
operation	O
)	O
over	O
a	O
variable	B
(	O
cid:80	O
)	O
interpretation	O
is	O
that	O
all	O
states	O
of	O
x	O
are	O
included	O
,	O
i.e	O
.	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
x	O
f	O
(	O
x	O
)	O
≡	O
s∈dom	O
(	O
x	O
)	O
f	O
(	O
x	O
=	O
s	O
)	O
.	O
for	O
our	O
purposes	O
,	O
events	O
are	O
expressions	O
about	O
random	O
variables	O
,	O
such	O
as	O
two	O
heads	O
in	O
6	O
coin	O
tosses	O
.	O
two	O
events	O
are	O
mutually	O
exclusive	O
if	O
they	O
can	O
not	O
both	O
simultaneously	O
occur	O
.	O
for	O
example	O
the	O
events	O
the	O
coin	O
is	O
heads	O
and	O
the	O
coin	O
is	O
tails	O
are	O
mutually	O
exclusive	O
.	O
one	O
can	O
think	O
of	O
deﬁning	O
a	O
new	O
variable	B
named	O
by	O
the	O
event	O
so	O
,	O
for	O
example	O
,	O
p	O
(	O
the	O
coin	O
is	O
tails	O
)	O
can	O
be	O
interpreted	O
as	O
p	O
(	O
the	O
coin	O
is	O
tails	O
=	O
true	O
)	O
.	O
we	O
use	O
p	O
(	O
x	O
=	O
tr	O
)	O
for	O
the	O
probability	B
of	O
event/variable	O
x	O
being	O
in	O
the	O
state	O
true	O
and	O
p	O
(	O
x	O
=	O
fa	O
)	O
for	O
the	O
probability	B
of	O
event/variable	O
x	O
being	O
in	O
the	O
state	O
false	O
.	O
the	O
rules	O
of	O
probability	B
deﬁnition	O
1	O
(	O
rules	O
of	O
probability	B
(	O
discrete	B
variables	O
)	O
)	O
.	O
the	O
probability	B
of	O
an	O
event	O
x	O
occurring	O
is	O
represented	O
by	O
a	O
value	B
between	O
0	O
and	O
1.	O
p	O
(	O
x	O
)	O
=	O
1	O
means	O
that	O
we	O
are	O
certain	O
that	O
the	O
event	O
does	O
occur	O
.	O
conversely	O
,	O
p	O
(	O
x	O
)	O
=	O
0	O
means	O
that	O
we	O
are	O
certain	O
that	O
the	O
event	O
does	O
not	O
occur	O
.	O
the	O
summation	O
of	O
the	O
probability	B
over	O
all	O
the	O
states	O
is	O
1	O
:	O
(	O
cid:88	O
)	O
p	O
(	O
x	O
=	O
x	O
)	O
=	O
1	O
(	O
1.1.1	O
)	O
x	O
3	O
such	O
probabilities	O
are	O
normalised	O
.	O
we	O
will	O
usually	O
more	O
conveniently	O
write	O
(	O
cid:80	O
)	O
x	O
p	O
(	O
x	O
)	O
=	O
1.	O
probability	B
refresher	O
two	O
events	O
x	O
and	O
y	O
can	O
interact	O
through	O
p	O
(	O
x	O
or	O
y	O
)	O
=	O
p	O
(	O
x	O
)	O
+	O
p	O
(	O
y	O
)	O
−	O
p	O
(	O
x	O
and	O
y	O
)	O
(	O
1.1.2	O
)	O
we	O
will	O
use	O
the	O
shorthand	O
p	O
(	O
x	O
,	O
y	O
)	O
for	O
p	O
(	O
x	O
and	O
y	O
)	O
.	O
note	O
that	O
p	O
(	O
y	O
,	O
x	O
)	O
=	O
p	O
(	O
x	O
,	O
y	O
)	O
and	O
p	O
(	O
x	O
or	O
y	O
)	O
=	O
p	O
(	O
y	O
or	O
x	O
)	O
.	O
deﬁnition	O
2	O
(	O
set	O
notation	O
)	O
.	O
an	O
alternative	O
notation	O
in	O
terms	O
of	O
set	O
theory	O
is	O
to	O
write	O
p	O
(	O
x	O
or	O
y	O
)	O
≡	O
p	O
(	O
x	O
∪	O
y	O
)	O
,	O
p	O
(	O
x	O
,	O
y	O
)	O
≡	O
p	O
(	O
x	O
∩	O
y	O
)	O
(	O
1.1.3	O
)	O
deﬁnition	O
3	O
(	O
marginals	O
)	O
.	O
given	O
a	O
joint	B
distribution	O
p	O
(	O
x	O
,	O
y	O
)	O
the	O
distribution	B
of	O
a	O
single	O
variable	B
is	O
given	O
by	O
p	O
(	O
x	O
,	O
y	O
)	O
(	O
1.1.4	O
)	O
p	O
(	O
x	O
)	O
=	O
(	O
cid:88	O
)	O
y	O
here	O
p	O
(	O
x	O
)	O
is	O
termed	O
a	O
marginal	B
of	O
the	O
joint	B
probability	O
distribution	B
p	O
(	O
x	O
,	O
y	O
)	O
.	O
the	O
process	O
of	O
computing	O
a	O
marginal	B
from	O
a	O
joint	B
distribution	O
is	O
called	O
marginalisation	B
.	O
more	O
generally	O
,	O
one	O
has	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xi−1	O
,	O
xi+1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
=	O
(	O
cid:88	O
)	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
(	O
1.1.5	O
)	O
xi	O
an	O
important	O
deﬁnition	O
that	O
will	O
play	O
a	O
central	O
role	O
in	O
this	O
book	O
is	O
conditional	B
probability	I
.	O
deﬁnition	O
4	O
(	O
conditional	B
probability	I
/	O
bayes	O
’	O
rule	O
)	O
.	O
the	O
probability	B
of	O
event	O
x	O
conditioned	O
on	O
knowing	O
event	O
y	O
(	O
or	O
more	O
shortly	O
,	O
the	O
probability	B
of	O
x	O
given	O
y	O
)	O
is	O
deﬁned	O
as	O
p	O
(	O
x|y	O
)	O
≡	O
p	O
(	O
x	O
,	O
y	O
)	O
p	O
(	O
y	O
)	O
if	O
p	O
(	O
y	O
)	O
=	O
0	O
then	O
p	O
(	O
x|y	O
)	O
is	O
not	O
deﬁned	O
.	O
probability	B
density	O
functions	O
(	O
1.1.6	O
)	O
deﬁnition	O
5	O
(	O
probability	B
density	O
functions	O
)	O
.	O
for	O
a	O
single	O
continuous	B
variable	O
x	O
,	O
the	O
probability	B
density	O
p	O
(	O
x	O
)	O
is	O
deﬁned	O
such	O
that	O
(	O
cid:90	O
)	O
∞	O
p	O
(	O
x	O
)	O
≥	O
0	O
p	O
(	O
x	O
)	O
dx	O
=	O
1	O
−∞	O
4	O
(	O
1.1.7	O
)	O
(	O
1.1.8	O
)	O
draft	O
march	O
9	O
,	O
2010	O
probability	B
refresher	O
(	O
cid:90	O
)	O
b	O
a	O
(	O
cid:90	O
)	O
1.7	O
−1	O
p	O
(	O
a	O
<	O
x	O
<	O
b	O
)	O
=	O
as	O
shorthand	O
we	O
will	O
sometimes	O
write	O
(	O
cid:82	O
)	O
b	O
p	O
(	O
x	O
)	O
dx	O
x=a	O
p	O
(	O
x	O
)	O
,	O
particularly	O
when	O
we	O
want	O
an	O
expression	O
to	O
be	O
valid	O
for	O
either	O
continuous	B
or	O
discrete	B
variables	O
.	O
the	O
multivariate	B
case	O
is	O
analogous	O
with	O
integration	O
over	O
all	O
real	O
space	O
,	O
and	O
the	O
probability	B
that	O
x	O
belongs	O
to	O
a	O
region	O
of	O
the	O
space	O
deﬁned	O
accordingly	O
.	O
(	O
1.1.9	O
)	O
for	O
continuous	B
variables	O
,	O
formally	O
speaking	O
,	O
events	O
are	O
deﬁned	O
for	O
the	O
variable	B
occurring	O
within	O
a	O
deﬁned	O
region	O
,	O
for	O
example	O
p	O
(	O
x	O
∈	O
[	O
−1	O
,	O
1.7	O
]	O
)	O
=	O
f	O
(	O
x	O
)	O
dx	O
(	O
1.1.10	O
)	O
where	O
here	O
f	O
(	O
x	O
)	O
is	O
the	O
probability	B
density	O
function	B
(	O
pdf	O
)	O
of	O
the	O
continuous	B
random	O
variable	B
x.	O
unlike	O
probabilities	O
,	O
probability	B
densities	O
can	O
take	O
positive	O
values	O
greater	O
than	O
1.	O
appear	O
strange	O
,	O
the	O
nervous	O
reader	O
may	O
simply	O
replace	O
our	O
p	O
(	O
x	O
=	O
x	O
)	O
notation	O
for	O
(	O
cid:82	O
)	O
formally	O
speaking	O
,	O
for	O
a	O
continuous	B
variable	O
,	O
one	O
should	O
not	O
speak	O
of	O
the	O
probability	B
that	O
x	O
=	O
0.2	O
since	O
the	O
probability	B
of	O
a	O
single	O
value	B
is	O
always	O
zero	O
.	O
however	O
,	O
we	O
shall	O
often	O
write	O
p	O
(	O
x	O
)	O
for	O
continuous	B
variables	O
,	O
thus	O
not	O
distinguishing	O
between	O
probabilities	O
and	O
probability	B
density	O
function	B
values	O
.	O
whilst	O
this	O
may	O
x∈∆	O
f	O
(	O
x	O
)	O
dx	O
,	O
where	O
∆	O
is	O
a	O
small	O
region	O
centred	O
on	O
x.	O
this	O
is	O
well	O
deﬁned	O
in	O
a	O
probabilistic	B
sense	O
and	O
,	O
in	O
the	O
limit	O
∆	O
being	O
very	O
small	O
,	O
this	O
would	O
give	O
approximately	O
∆f	O
(	O
x	O
)	O
.	O
if	O
we	O
consistently	O
use	O
the	O
same	O
∆	O
for	O
all	O
occurrences	O
of	O
pdfs	O
,	O
then	O
we	O
will	O
simply	O
have	O
a	O
common	O
prefactor	O
∆	O
in	O
all	O
expressions	O
.	O
our	O
strategy	O
is	O
to	O
simply	O
ignore	O
these	O
values	O
(	O
since	O
in	O
the	O
end	O
only	O
relative	O
probabilities	O
will	O
be	O
relevant	O
)	O
and	O
write	O
p	O
(	O
x	O
)	O
.	O
in	O
this	O
way	O
,	O
all	O
the	O
standard	O
rules	O
of	O
probability	B
carry	O
over	O
,	O
including	O
bayes	O
’	O
rule	O
.	O
interpreting	O
conditional	B
probability	I
imagine	O
a	O
circular	O
dart	O
board	O
,	O
split	O
into	O
20	O
equal	O
sections	O
,	O
labelled	B
from	O
1	O
to	O
20	O
and	O
randy	O
,	O
a	O
dart	O
thrower	O
who	O
hits	O
any	O
one	O
of	O
the	O
20	O
sections	O
uniformly	O
at	O
random	O
.	O
hence	O
the	O
probability	B
that	O
a	O
dart	O
thrown	O
by	O
randy	O
occurs	O
in	O
any	O
one	O
of	O
the	O
20	O
regions	O
is	O
p	O
(	O
region	O
i	O
)	O
=	O
1/20	O
.	O
a	O
friend	O
of	O
randy	O
tells	O
him	O
that	O
he	O
hasn	O
’	O
t	O
hit	O
the	O
20	O
region	O
.	O
what	O
is	O
the	O
probability	B
that	O
randy	O
has	O
hit	O
the	O
5	O
region	O
?	O
conditioned	O
on	O
this	O
information	O
,	O
only	O
regions	O
1	O
to	O
19	O
remain	O
possible	O
and	O
,	O
since	O
there	O
is	O
no	O
preference	O
for	O
randy	O
to	O
hit	O
any	O
of	O
these	O
regions	O
,	O
the	O
probability	B
is	O
1/19	O
.	O
the	O
conditioning	B
means	O
that	O
certain	O
states	O
are	O
now	O
inaccessible	O
,	O
and	O
the	O
original	O
probability	B
is	O
subsequently	O
distributed	O
over	O
the	O
remaining	O
accessible	O
states	O
.	O
from	O
the	O
rules	O
of	O
probability	B
:	O
p	O
(	O
region	O
5|not	O
region	O
20	O
)	O
=	O
p	O
(	O
region	O
5	O
,	O
not	O
region	O
20	O
)	O
p	O
(	O
not	O
region	O
20	O
)	O
=	O
p	O
(	O
region	O
5	O
)	O
p	O
(	O
not	O
region	O
20	O
)	O
=	O
1/20	O
19/20	O
=	O
1	O
19	O
giving	O
the	O
intuitive	O
result	O
.	O
p	O
(	O
region	O
5	O
)	O
.	O
in	O
the	O
above	O
p	O
(	O
region	O
5	O
,	O
not	O
region	O
20	O
)	O
=	O
p	O
(	O
region	O
{	O
5	O
∩	O
1	O
∩	O
2∩	O
,	O
.	O
.	O
.	O
,	O
∩19	O
}	O
)	O
=	O
an	O
important	O
point	O
to	O
clarify	O
is	O
that	O
p	O
(	O
a	O
=	O
a|b	O
=	O
b	O
)	O
should	O
not	O
be	O
interpreted	O
as	O
‘	O
given	O
the	O
event	O
b	O
=	O
b	O
has	O
occurred	O
,	O
p	O
(	O
a	O
=	O
a|b	O
=	O
b	O
)	O
is	O
the	O
probability	B
of	O
the	O
event	O
a	O
=	O
a	O
occurring	O
’	O
.	O
in	O
most	O
contexts	O
,	O
no	O
such	O
explicit	O
temporal	O
causality	B
is	O
implied1	O
and	O
the	O
correct	O
interpretation	O
should	O
be	O
‘	O
p	O
(	O
a	O
=	O
a|b	O
=	O
b	O
)	O
is	O
the	O
probability	B
of	O
a	O
being	O
in	O
state	O
a	O
under	O
the	O
constraint	O
that	O
b	O
is	O
in	O
state	O
b	O
’	O
.	O
constant	O
since	O
p	O
(	O
a	O
=	O
a	O
,	O
b	O
=	O
b	O
)	O
is	O
not	O
a	O
distribution	B
in	O
a	O
–	O
in	O
other	O
words	O
,	O
(	O
cid:80	O
)	O
make	O
it	O
a	O
distribution	B
we	O
need	O
to	O
divide	O
:	O
p	O
(	O
a	O
=	O
a	O
,	O
b	O
=	O
b	O
)	O
/	O
(	O
cid:80	O
)	O
the	O
relation	O
between	O
the	O
conditional	B
p	O
(	O
a	O
=	O
a|b	O
=	O
b	O
)	O
and	O
the	O
joint	B
p	O
(	O
a	O
=	O
a	O
,	O
b	O
=	O
b	O
)	O
is	O
just	O
a	O
normalisation	B
a	O
p	O
(	O
a	O
=	O
a	O
,	O
b	O
=	O
b	O
)	O
(	O
cid:54	O
)	O
=	O
1.	O
to	O
a	O
p	O
(	O
a	O
=	O
a	O
,	O
b	O
=	O
b	O
)	O
which	O
,	O
when	O
summed	O
over	O
a	O
does	O
sum	O
to	O
1.	O
indeed	O
,	O
this	O
is	O
just	O
the	O
deﬁnition	O
of	O
p	O
(	O
a	O
=	O
a|b	O
=	O
b	O
)	O
.	O
1we	O
will	O
discuss	O
issues	O
related	O
to	O
causality	B
further	O
in	O
section	O
(	O
3.4	O
)	O
.	O
draft	O
march	O
9	O
,	O
2010	O
5	O
probability	B
refresher	O
deﬁnition	O
6	O
(	O
independence	B
)	O
.	O
events	O
x	O
and	O
y	O
are	O
independent	O
if	O
knowing	O
one	O
event	O
gives	O
no	O
extra	O
information	O
about	O
the	O
other	O
event	O
.	O
mathematically	O
,	O
this	O
is	O
expressed	O
by	O
p	O
(	O
x	O
,	O
y	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
provided	O
that	O
p	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=	O
0	O
and	O
p	O
(	O
y	O
)	O
(	O
cid:54	O
)	O
=	O
0	O
independence	B
of	O
x	O
and	O
y	O
is	O
equivalent	B
to	O
p	O
(	O
x|y	O
)	O
=	O
p	O
(	O
x	O
)	O
⇔	O
p	O
(	O
y|x	O
)	O
=	O
p	O
(	O
y	O
)	O
(	O
1.1.11	O
)	O
(	O
1.1.12	O
)	O
if	O
p	O
(	O
x|y	O
)	O
=	O
p	O
(	O
x	O
)	O
for	O
all	O
states	O
of	O
x	O
and	O
y	O
,	O
then	O
the	O
variables	O
x	O
and	O
y	O
are	O
said	O
to	O
be	O
independent	O
.	O
if	O
p	O
(	O
x	O
,	O
y	O
)	O
=	O
kf	O
(	O
x	O
)	O
g	O
(	O
y	O
)	O
(	O
1.1.13	O
)	O
for	O
some	O
constant	O
k	O
,	O
and	O
positive	O
functions	O
f	O
(	O
·	O
)	O
and	O
g	O
(	O
·	O
)	O
then	O
x	O
and	O
y	O
are	O
independent	O
.	O
deterministic	B
dependencies	O
sometimes	O
the	O
concept	O
of	O
independence	B
is	O
perhaps	O
a	O
little	O
strange	O
.	O
consider	O
the	O
following	O
:	O
variables	O
x	O
and	O
y	O
are	O
both	O
binary	O
(	O
their	O
domains	O
consist	O
of	O
two	O
states	O
)	O
.	O
we	O
deﬁne	O
the	O
distribution	B
such	O
that	O
x	O
and	O
y	O
are	O
always	O
both	O
in	O
a	O
certain	O
joint	B
state	O
:	O
p	O
(	O
x	O
=	O
a	O
,	O
y	O
=	O
1	O
)	O
=	O
1	O
p	O
(	O
x	O
=	O
a	O
,	O
y	O
=	O
2	O
)	O
=	O
0	O
p	O
(	O
x	O
=	O
b	O
,	O
y	O
=	O
2	O
)	O
=	O
0	O
p	O
(	O
x	O
=	O
b	O
,	O
y	O
=	O
1	O
)	O
=	O
0	O
are	O
x	O
and	O
y	O
dependent	O
?	O
the	O
reader	O
may	O
show	O
that	O
p	O
(	O
x	O
=	O
a	O
)	O
=	O
1	O
,	O
p	O
(	O
x	O
=	O
b	O
)	O
=	O
0	O
and	O
p	O
(	O
y	O
=	O
1	O
)	O
=	O
1	O
,	O
p	O
(	O
y	O
=	O
2	O
)	O
=	O
0.	O
hence	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
=	O
p	O
(	O
x	O
,	O
y	O
)	O
for	O
all	O
states	O
of	O
x	O
and	O
y	O
,	O
and	O
x	O
and	O
y	O
are	O
therefore	O
independent	O
.	O
this	O
may	O
seem	O
strange	O
–	O
we	O
know	O
for	O
sure	O
the	O
relation	O
between	O
x	O
and	O
y	O
,	O
namely	O
that	O
they	O
are	O
always	O
in	O
the	O
same	O
joint	B
state	O
,	O
yet	O
they	O
are	O
independent	O
.	O
since	O
the	O
distribution	B
is	O
trivially	O
concentrated	O
in	O
a	O
single	O
joint	B
state	O
,	O
knowing	O
the	O
state	O
of	O
x	O
tells	O
you	O
nothing	O
that	O
you	O
didn	O
’	O
t	O
anyway	O
know	O
about	O
the	O
state	O
of	O
y	O
,	O
and	O
vice	O
versa	O
.	O
this	O
potential	B
confusion	O
comes	O
from	O
using	O
the	O
term	O
‘	O
independent	O
’	O
which	O
,	O
in	O
english	O
,	O
suggests	O
that	O
there	O
is	O
no	O
inﬂuence	O
or	O
relation	O
between	O
objects	O
discussed	O
.	O
the	O
best	O
way	O
to	O
think	O
about	O
statistical	O
independence	B
is	O
to	O
ask	O
whether	O
or	O
not	O
knowing	O
the	O
state	O
of	O
variable	B
y	O
tells	O
you	O
something	O
more	O
than	O
you	O
knew	O
before	O
about	O
variable	B
x	O
,	O
where	O
‘	O
knew	O
before	O
’	O
means	O
working	O
with	O
the	O
joint	B
distribution	O
of	O
p	O
(	O
x	O
,	O
y	O
)	O
to	O
ﬁgure	O
out	O
what	O
we	O
can	O
know	O
about	O
x	O
,	O
namely	O
p	O
(	O
x	O
)	O
.	O
1.1.1	O
probability	B
tables	O
based	O
on	O
the	O
populations	O
60776238	O
,	O
5116900	O
and	O
2980700	O
of	O
england	O
(	O
e	O
)	O
,	O
scotland	O
(	O
s	O
)	O
and	O
wales	O
(	O
w	O
)	O
,	O
the	O
a	O
priori	O
probability	B
that	O
a	O
randomly	O
selected	O
person	O
from	O
these	O
three	O
countries	O
would	O
live	O
in	O
england	O
,	O
scotland	O
or	O
wales	O
,	O
would	O
be	O
approximately	O
0.88	O
,	O
0.08	O
and	O
0.04	O
respectively	O
.	O
we	O
can	O
write	O
this	O
as	O
a	O
vector	O
(	O
or	O
probability	B
table	O
)	O
:	O
	O
p	O
(	O
cnt	O
=	O
e	O
)	O
p	O
(	O
cnt	O
=	O
s	O
)	O
p	O
(	O
cnt	O
=	O
w	O
)	O
	O
=	O
	O
0.88	O
0.08	O
0.04	O
	O
whose	O
component	O
values	O
sum	O
to	O
1.	O
the	O
ordering	O
of	O
the	O
components	O
in	O
this	O
vector	O
is	O
arbitrary	O
,	O
as	O
long	O
as	O
it	O
is	O
consistently	O
applied	O
.	O
6	O
draft	O
march	O
9	O
,	O
2010	O
(	O
1.1.14	O
)	O
probability	B
refresher	O
for	O
the	O
sake	O
of	O
simplicity	O
,	O
let	O
’	O
s	O
assume	O
that	O
only	O
three	O
mother	O
tongue	O
languages	O
exist	O
:	O
english	O
(	O
eng	O
)	O
,	O
scottish	O
(	O
scot	O
)	O
and	O
welsh	O
(	O
wel	O
)	O
,	O
with	O
conditional	O
probabilities	O
given	O
the	O
country	O
of	O
residence	O
,	O
england	O
(	O
e	O
)	O
,	O
scotland	O
(	O
s	O
)	O
and	O
wales	O
(	O
w	O
)	O
.	O
we	O
write	O
a	O
(	O
ﬁctitious	O
)	O
conditional	B
probability	I
table	O
p	O
(	O
m	O
t	O
=	O
eng|cnt	O
=	O
e	O
)	O
=	O
0.95	O
p	O
(	O
m	O
t	O
=	O
scot|cnt	O
=	O
e	O
)	O
=	O
0.04	O
p	O
(	O
m	O
t	O
=	O
wel|cnt	O
=	O
e	O
)	O
=	O
0.01	O
p	O
(	O
m	O
t	O
=	O
eng|cnt	O
=	O
s	O
)	O
=	O
0.7	O
p	O
(	O
m	O
t	O
=	O
wel|cnt	O
=	O
s	O
)	O
=	O
0.0	O
p	O
(	O
m	O
t	O
=	O
wel|cnt	O
=	O
w	O
)	O
=	O
0.4	O
p	O
(	O
m	O
t	O
=	O
eng|cnt	O
=	O
w	O
)	O
=	O
0.6	O
(	O
1.1.15	O
)	O
p	O
(	O
m	O
t	O
=	O
scot|cnt	O
=	O
s	O
)	O
=	O
0.3	O
p	O
(	O
m	O
t	O
=	O
scot|cnt	O
=	O
w	O
)	O
=	O
0.0	O
from	O
this	O
we	O
can	O
form	O
a	O
joint	B
distribution	O
p	O
(	O
cnt	O
,	O
m	O
t	O
)	O
=	O
p	O
(	O
m	O
t|cnt	O
)	O
p	O
(	O
cnt	O
)	O
.	O
this	O
could	O
be	O
written	O
as	O
a	O
3	O
×	O
3	O
matrix	B
with	O
(	O
say	O
)	O
rows	O
indexed	O
by	O
country	O
and	O
columns	O
indexed	O
by	O
mother	O
tongue	O
:	O
	O
0.95	O
×	O
0.88	O
0.7	O
×	O
0.08	O
0.6	O
×	O
0.04	O
0.04	O
×	O
0.88	O
0.3	O
×	O
0.08	O
0.0	O
×	O
0.04	O
0.01	O
×	O
0.88	O
0.0	O
×	O
0.08	O
0.4	O
×	O
0.04	O
	O
=	O
	O
0.836	O
0.056	O
0.024	O
0.0352	O
0.024	O
0.0088	O
0	O
0	O
0.016	O
	O
(	O
1.1.16	O
)	O
the	O
joint	B
distribution	O
contains	O
all	O
the	O
information	O
about	O
the	O
model	B
of	O
this	O
environment	O
.	O
by	O
summing	O
a	O
column	O
of	O
this	O
table	O
,	O
we	O
have	O
the	O
marginal	B
p	O
(	O
cnt	O
)	O
.	O
summing	O
the	O
row	O
gives	O
the	O
marginal	B
p	O
(	O
m	O
t	O
)	O
.	O
similarly	O
,	O
one	O
could	O
easily	O
infer	O
p	O
(	O
cnt|m	O
t	O
)	O
∝	O
p	O
(	O
cnt|m	O
t	O
)	O
p	O
(	O
m	O
t	O
)	O
from	O
this	O
joint	B
distribution	O
.	O
for	O
joint	B
distributions	O
over	O
a	O
larger	O
number	O
of	O
variables	O
,	O
xi	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
,	O
with	O
each	O
variable	B
xi	O
taking	O
i=1	O
ki	O
entries	O
.	O
explicitly	O
storing	O
tables	O
therefore	O
requires	O
space	O
exponential	B
in	O
the	O
number	O
of	O
variables	O
,	O
which	O
rapidly	O
becomes	O
impractical	O
for	O
a	O
large	O
number	O
of	O
variables	O
.	O
ki	O
states	O
,	O
the	O
table	O
describing	O
the	O
joint	B
distribution	O
is	O
an	O
array	O
with	O
(	O
cid:81	O
)	O
d	O
a	O
probability	B
distribution	O
assigns	O
a	O
value	B
to	O
each	O
of	O
the	O
joint	B
states	O
of	O
the	O
variables	O
.	O
for	O
this	O
reason	O
,	O
p	O
(	O
t	O
,	O
j	O
,	O
r	O
,	O
s	O
)	O
is	O
considered	O
equivalent	B
to	O
p	O
(	O
j	O
,	O
s	O
,	O
r	O
,	O
t	O
)	O
(	O
or	O
any	O
such	O
reordering	O
of	O
the	O
variables	O
)	O
,	O
since	O
in	O
each	O
case	O
the	O
joint	B
setting	O
of	O
the	O
variables	O
is	O
simply	O
a	O
diﬀerent	O
index	O
to	O
the	O
same	O
probability	B
.	O
this	O
situation	O
is	O
more	O
clear	O
in	O
the	O
set	O
theoretic	O
notation	O
p	O
(	O
j	O
∩	O
s	O
∩	O
t	O
∩	O
r	O
)	O
.	O
we	O
abbreviate	O
this	O
set	O
theoretic	O
notation	O
by	O
using	O
the	O
commas	O
–	O
however	O
,	O
one	O
should	O
be	O
careful	O
not	O
to	O
confuse	O
the	O
use	O
of	O
this	O
indexing	O
type	O
notation	O
with	O
functions	O
f	O
(	O
x	O
,	O
y	O
)	O
which	O
are	O
in	O
general	O
dependent	O
on	O
the	O
variable	B
order	O
.	O
whilst	O
the	O
variables	O
to	O
the	O
left	O
of	O
the	O
conditioning	B
bar	O
may	O
be	O
written	O
in	O
any	O
order	O
,	O
and	O
equally	O
those	O
to	O
the	O
right	O
of	O
the	O
conditioning	B
bar	O
may	O
be	O
written	O
in	O
any	O
order	O
,	O
moving	O
variables	O
across	O
the	O
bar	O
is	O
not	O
generally	O
equivalent	B
,	O
so	O
that	O
p	O
(	O
x1|x2	O
)	O
(	O
cid:54	O
)	O
=	O
p	O
(	O
x2|x1	O
)	O
.	O
1.1.2	O
interpreting	O
conditional	B
probability	I
together	O
with	O
the	O
rules	O
of	O
probability	B
,	O
conditional	B
probability	I
enables	O
one	O
to	O
reason	O
in	O
a	O
rational	O
,	O
logical	O
and	O
consistent	B
way	O
.	O
one	O
could	O
argue	O
that	O
much	O
of	O
science	O
deals	O
with	O
problems	O
of	O
the	O
form	O
:	O
tell	O
me	O
something	O
about	O
the	O
parameters	O
θ	O
given	O
that	O
i	O
have	O
observed	O
data	O
d	O
and	O
have	O
some	O
knowledge	O
of	O
the	O
underlying	O
data	B
generating	O
mechanism	O
.	O
from	O
a	O
modelling	B
perspective	O
,	O
this	O
requires	O
p	O
(	O
θ|d	O
)	O
=	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ	O
)	O
p	O
(	O
d	O
)	O
(	O
cid:82	O
)	O
=	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ	O
)	O
θ	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ	O
)	O
(	O
1.1.17	O
)	O
this	O
shows	O
how	O
from	O
a	O
forward	O
or	O
generative	B
model	O
p	O
(	O
d|θ	O
)	O
of	O
the	O
dataset	O
,	O
and	O
coupled	B
with	O
a	O
prior	B
belief	O
p	O
(	O
θ	O
)	O
about	O
which	O
parameter	B
values	O
are	O
appropriate	O
,	O
we	O
can	O
infer	O
the	O
posterior	B
distribution	O
p	O
(	O
θ|d	O
)	O
of	O
parameters	O
in	O
light	O
of	O
the	O
observed	O
data	O
.	O
this	O
use	O
of	O
a	O
generative	B
model	O
sits	O
well	O
with	O
physical	O
models	O
of	O
the	O
world	O
which	O
typically	O
postulate	O
how	O
to	O
generate	O
observed	O
phenomena	O
,	O
assuming	O
we	O
know	O
the	O
correct	O
parameters	O
of	O
the	O
model	B
.	O
for	O
example	O
,	O
one	O
might	O
postulate	O
how	O
to	O
generate	O
a	O
time-series	O
of	O
displacements	O
for	O
a	O
swinging	O
pendulum	O
but	O
with	O
unknown	O
mass	O
,	O
length	O
and	O
damping	O
constant	O
.	O
using	O
this	O
generative	B
model	O
,	O
and	O
given	O
only	O
the	O
displacements	O
,	O
we	O
could	O
infer	O
the	O
unknown	O
physical	O
properties	B
of	O
the	O
pendulum	O
,	O
such	O
as	O
its	O
mass	O
,	O
length	O
and	O
friction	O
damping	O
constant	O
.	O
draft	O
march	O
9	O
,	O
2010	O
7	O
probabilistic	B
reasoning	O
subjective	B
probability	O
probability	B
is	O
a	O
contentious	O
topic	O
and	O
we	O
do	O
not	O
wish	O
to	O
get	O
bogged	O
down	O
by	O
the	O
debate	O
here	O
,	O
apart	O
from	O
pointing	O
out	O
that	O
it	O
is	O
not	O
necessarily	O
the	O
axioms	O
of	O
probability	B
that	O
are	O
contentious	O
rather	O
what	O
interpre-	O
tation	O
we	O
should	O
place	O
on	O
them	O
.	O
in	O
some	O
cases	O
potential	B
repetitions	O
of	O
an	O
experiment	O
can	O
be	O
envisaged	O
so	O
that	O
the	O
‘	O
long	O
run	O
’	O
(	O
or	O
frequentist	B
)	O
deﬁnition	O
of	O
probability	B
in	O
which	O
probabilities	O
are	O
deﬁned	O
with	O
respect	O
to	O
a	O
potentially	O
inﬁnite	O
repetition	O
of	O
‘	O
experiments	O
’	O
makes	O
sense	O
.	O
for	O
example	O
,	O
in	O
coin	O
tossing	O
,	O
the	O
proba-	O
bility	O
of	O
heads	O
might	O
be	O
interpreted	O
as	O
‘	O
if	O
i	O
were	O
to	O
repeat	O
the	O
experiment	O
of	O
ﬂipping	O
a	O
coin	O
(	O
at	O
‘	O
random	O
’	O
)	O
,	O
the	O
limit	O
of	O
the	O
number	O
of	O
heads	O
that	O
occurred	O
over	O
the	O
number	O
of	O
tosses	O
is	O
deﬁned	O
as	O
the	O
probability	B
of	O
a	O
head	O
occurring	O
.	O
here	O
’	O
s	O
another	O
problem	B
that	O
is	O
typical	O
of	O
the	O
kind	O
of	O
scenario	O
one	O
might	O
face	O
in	O
a	O
machine	O
learning	B
situation	O
.	O
a	O
ﬁlm	O
enthusiast	O
joins	O
a	O
new	O
online	B
ﬁlm	O
service	O
.	O
based	O
on	O
expressing	O
a	O
few	O
ﬁlms	O
a	O
user	O
likes	O
and	O
dislikes	O
,	O
the	O
online	B
company	O
tries	O
to	O
estimate	O
the	O
probability	B
that	O
the	O
user	O
will	O
like	O
each	O
of	O
the	O
10000	O
ﬁlms	O
in	O
their	O
database	O
.	O
if	O
we	O
were	O
to	O
deﬁne	O
probability	B
as	O
a	O
limiting	O
case	O
of	O
inﬁnite	O
repetitions	O
of	O
the	O
same	O
experiment	O
,	O
this	O
wouldn	O
’	O
t	O
make	O
much	O
sense	O
in	O
this	O
case	O
since	O
we	O
can	O
’	O
t	O
repeat	O
the	O
experiment	O
.	O
however	O
,	O
if	O
we	O
assume	O
that	O
the	O
user	O
behaves	O
in	O
a	O
manner	O
consistent	B
with	O
other	O
users	O
,	O
we	O
should	O
be	O
able	O
to	O
exploit	O
the	O
large	O
amount	O
of	O
data	B
from	O
other	O
users	O
’	O
ratings	O
to	O
make	O
a	O
reasonable	O
‘	O
guess	O
’	O
as	O
to	O
what	O
this	O
consumer	O
likes	O
.	O
this	O
degree	B
of	I
belief	I
or	O
bayesian	O
subjective	B
interpretation	O
of	O
probability	B
sidesteps	O
non-repeatability	O
issues	O
–	O
it	O
’	O
s	O
just	O
a	O
consistent	B
framework	O
for	O
manipulating	O
real	O
values	O
consistent	B
with	O
our	O
intuition	O
about	O
probability	B
[	O
145	O
]	O
.	O
1.2	O
probabilistic	B
reasoning	O
the	O
axioms	O
of	O
probability	B
,	O
combined	O
with	O
bayes	O
’	O
rule	O
make	O
for	O
a	O
complete	O
reasoning	O
system	B
,	O
one	O
which	O
includes	O
traditional	O
deductive	O
logic	B
as	O
a	O
special	O
case	O
[	O
145	O
]	O
.	O
remark	O
1.	O
the	O
central	O
paradigm	O
of	O
probabilistic	B
reasoning	O
is	O
to	O
identify	O
all	O
relevant	O
variables	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
in	O
the	O
environment	O
,	O
and	O
make	O
a	O
probabilistic	B
model	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
of	O
their	O
interaction	O
.	O
reasoning	O
(	O
inference	B
)	O
is	O
then	O
performed	O
by	O
introducing	O
evidence	B
2	O
that	O
sets	O
variables	O
in	O
known	O
states	O
,	O
and	O
subsequently	O
computing	O
probabilities	O
of	O
interest	O
,	O
conditioned	O
on	O
this	O
evidence	B
.	O
example	O
1	O
(	O
hamburgers	O
)	O
.	O
consider	O
the	O
following	O
ﬁctitious	O
scientiﬁc	O
information	O
:	O
doctors	O
ﬁnd	O
that	O
peo-	O
ple	O
with	O
kreuzfeld-jacob	O
disease	O
(	O
kj	O
)	O
almost	O
invariably	O
ate	O
hamburgers	O
,	O
thus	O
p	O
(	O
hamburger	O
eater|kj	O
)	O
=	O
0.9.	O
the	O
probability	B
of	O
an	O
individual	O
having	O
kj	O
is	O
currently	O
rather	O
low	O
,	O
about	O
one	O
in	O
100,000	O
.	O
1.	O
assuming	O
eating	O
lots	O
of	O
hamburgers	O
is	O
rather	O
widespread	O
,	O
say	O
p	O
(	O
hamburger	O
eater	O
)	O
=	O
0.5	O
,	O
what	O
is	O
the	O
probability	B
that	O
a	O
hamburger	O
eater	O
will	O
have	O
kreuzfeld-jacob	O
disease	O
?	O
this	O
may	O
be	O
computed	O
as	O
p	O
(	O
kj	O
|hamburger	O
eater	O
)	O
=	O
p	O
(	O
hamburger	O
eater	O
,	O
kj	O
)	O
p	O
(	O
hamburger	O
eater	O
)	O
=	O
p	O
(	O
hamburger	O
eater|kj	O
)	O
p	O
(	O
kj	O
)	O
p	O
(	O
hamburger	O
eater	O
)	O
=	O
9	O
10	O
×	O
1	O
100000	O
1	O
2	O
=	O
1.8	O
×	O
10−5	O
(	O
1.2.1	O
)	O
(	O
1.2.2	O
)	O
2.	O
if	O
the	O
fraction	O
of	O
people	O
eating	O
hamburgers	O
was	O
rather	O
small	O
,	O
p	O
(	O
hamburger	O
eater	O
)	O
=	O
0.001	O
,	O
what	O
is	O
the	O
probability	B
that	O
a	O
regular	O
hamburger	O
eater	O
will	O
have	O
kreuzfeld-jacob	O
disease	O
?	O
repeating	O
the	O
above	O
calculation	O
,	O
this	O
is	O
given	O
by	O
9	O
10	O
×	O
1	O
100000	O
1	O
1000	O
8	O
≈	O
1/100	O
(	O
1.2.3	O
)	O
draft	O
march	O
9	O
,	O
2010	O
probabilistic	B
reasoning	O
intuitively	O
,	O
this	O
is	O
much	O
higher	O
than	O
in	O
scenario	O
(	O
1	O
)	O
since	O
here	O
we	O
can	O
be	O
more	O
sure	O
that	O
eating	O
hamburgers	O
is	O
related	O
to	O
the	O
illness	O
.	O
in	O
this	O
case	O
only	O
a	O
small	O
number	O
of	O
people	O
in	O
the	O
population	O
eat	O
hamburgers	O
,	O
and	O
most	O
of	O
them	O
get	O
ill.	O
example	O
2	O
(	O
inspector	O
clouseau	O
)	O
.	O
inspector	O
clouseau	O
arrives	O
at	O
the	O
scene	O
of	O
a	O
crime	O
.	O
the	O
victim	O
lies	O
dead	O
in	O
the	O
room	O
and	O
the	O
inspector	O
quickly	O
ﬁnds	O
the	O
murder	O
weapon	O
,	O
a	O
knife	O
(	O
k	O
)	O
.	O
the	O
butler	O
(	O
b	O
)	O
and	O
maid	O
(	O
m	O
)	O
are	O
his	O
main	O
suspects	O
.	O
the	O
inspector	O
has	O
a	O
prior	B
belief	O
of	O
0.8	O
that	O
the	O
butler	O
is	O
the	O
murderer	O
,	O
and	O
a	O
prior	B
belief	O
of	O
0.2	O
that	O
the	O
maid	O
is	O
the	O
murderer	O
.	O
these	O
probabilities	O
are	O
independent	O
in	O
the	O
sense	O
that	O
p	O
(	O
b	O
,	O
m	O
)	O
=	O
p	O
(	O
b	O
)	O
p	O
(	O
m	O
)	O
.	O
(	O
it	O
is	O
possible	O
that	O
both	O
the	O
butler	O
and	O
the	O
maid	O
murdered	O
the	O
victim	O
or	O
neither	O
)	O
.	O
the	O
inspector	O
’	O
s	O
prior	B
criminal	O
knowledge	O
can	O
be	O
formulated	O
mathematically	O
as	O
follows	O
:	O
dom	O
(	O
b	O
)	O
=	O
dom	O
(	O
m	O
)	O
=	O
{	O
murderer	O
,	O
not	O
murderer	O
}	O
,	O
dom	O
(	O
k	O
)	O
=	O
{	O
knife	O
used	O
,	O
knife	O
not	O
used	O
}	O
p	O
(	O
b	O
=	O
murderer	O
)	O
=	O
0.8	O
,	O
p	O
(	O
knife	O
used|b	O
=	O
not	O
murderer	O
,	O
m	O
=	O
not	O
murderer	O
)	O
=	O
0.3	O
=	O
0.2	O
p	O
(	O
knife	O
used|b	O
=	O
not	O
murderer	O
,	O
m	O
=	O
murderer	O
)	O
m	O
=	O
not	O
murderer	O
)	O
=	O
0.6	O
p	O
(	O
knife	O
used|b	O
=	O
murderer	O
,	O
p	O
(	O
knife	O
used|b	O
=	O
murderer	O
,	O
m	O
=	O
murderer	O
)	O
=	O
0.1	O
p	O
(	O
m	O
=	O
murderer	O
)	O
=	O
0.2	O
(	O
1.2.4	O
)	O
(	O
1.2.5	O
)	O
(	O
1.2.6	O
)	O
what	O
is	O
the	O
probability	B
that	O
the	O
butler	O
is	O
the	O
murderer	O
?	O
(	O
remember	O
that	O
it	O
might	O
be	O
that	O
neither	O
is	O
the	O
murderer	O
)	O
.	O
using	O
b	O
for	O
the	O
two	O
states	O
of	O
b	O
and	O
m	O
for	O
the	O
two	O
states	O
of	O
m	O
,	O
p	O
(	O
b|k	O
)	O
=	O
(	O
cid:88	O
)	O
p	O
(	O
b	O
,	O
m|k	O
)	O
=	O
(	O
cid:88	O
)	O
m	O
m	O
plugging	O
in	O
the	O
values	O
we	O
have	O
p	O
(	O
b	O
=	O
murderer|knife	O
used	O
)	O
=	O
8	O
10	O
=	O
p	O
(	O
b	O
)	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
b	O
p	O
(	O
b	O
)	O
(	O
cid:80	O
)	O
(	O
cid:0	O
)	O
2	O
(	O
cid:1	O
)	O
+	O
2	O
10	O
×	O
1	O
10	O
×	O
6	O
10	O
+	O
8	O
10	O
10	O
m	O
p	O
(	O
k|b	O
,	O
m	O
)	O
p	O
(	O
m	O
)	O
m	O
p	O
(	O
k|b	O
,	O
m	O
)	O
p	O
(	O
m	O
)	O
(	O
cid:1	O
)	O
(	O
cid:0	O
)	O
2	O
10	O
×	O
6	O
10	O
×	O
2	O
10	O
+	O
8	O
10	O
10	O
×	O
3	O
10	O
p	O
(	O
b	O
,	O
m	O
,	O
k	O
)	O
p	O
(	O
k	O
)	O
(	O
cid:0	O
)	O
2	O
10	O
×	O
1	O
8	O
10	O
10	O
+	O
8	O
(	O
1.2.7	O
)	O
(	O
cid:1	O
)	O
=	O
200	O
228	O
≈	O
0.877	O
(	O
1.2.8	O
)	O
the	O
role	O
of	O
p	O
(	O
knife	O
used	O
)	O
in	O
the	O
inspector	O
clouseau	O
example	O
can	O
cause	O
some	O
confusion	O
.	O
in	O
the	O
above	O
,	O
p	O
(	O
knife	O
used|b	O
,	O
m	O
)	O
p	O
(	O
m	O
)	O
(	O
1.2.9	O
)	O
p	O
(	O
knife	O
used	O
)	O
=	O
(	O
cid:88	O
)	O
p	O
(	O
b	O
)	O
(	O
cid:88	O
)	O
b	O
m	O
is	O
computed	O
to	O
be	O
0.456.	O
but	O
surely	O
,	O
p	O
(	O
knife	O
used	O
)	O
=	O
1	O
,	O
since	O
this	O
is	O
given	O
in	O
the	O
question	O
!	O
note	O
that	O
the	O
quantity	O
p	O
(	O
knife	O
used	O
)	O
relates	O
to	O
the	O
prior	B
probability	O
the	O
model	B
assigns	O
to	O
the	O
knife	O
being	O
used	O
(	O
in	O
the	O
absence	O
of	O
any	O
other	O
information	O
)	O
.	O
if	O
we	O
know	O
that	O
the	O
knife	O
is	O
used	O
,	O
then	O
the	O
posterior	B
p	O
(	O
knife	O
used|knife	O
used	O
)	O
=	O
p	O
(	O
knife	O
used	O
,	O
knife	O
used	O
)	O
p	O
(	O
knife	O
used	O
)	O
=	O
p	O
(	O
knife	O
used	O
)	O
p	O
(	O
knife	O
used	O
)	O
=	O
1	O
which	O
,	O
naturally	O
,	O
must	O
be	O
the	O
case	O
.	O
another	O
potential	B
confusion	O
is	O
the	O
choice	O
p	O
(	O
b	O
=	O
murderer	O
)	O
=	O
0.8	O
,	O
p	O
(	O
m	O
=	O
murderer	O
)	O
=	O
0.2	O
(	O
1.2.10	O
)	O
(	O
1.2.11	O
)	O
which	O
means	O
that	O
p	O
(	O
b	O
=	O
not	O
murderer	O
)	O
=	O
0.2	O
,	O
p	O
(	O
m	O
=	O
not	O
murderer	O
)	O
=	O
0.8.	O
these	O
events	O
are	O
not	O
exclusive	O
and	O
it	O
’	O
s	O
just	O
‘	O
coincidence	O
’	O
that	O
the	O
numerical	B
values	O
are	O
chosen	O
this	O
way	O
.	O
for	O
example	O
,	O
we	O
could	O
have	O
also	O
chosen	O
p	O
(	O
b	O
=	O
murderer	O
)	O
=	O
0.6	O
,	O
p	O
(	O
m	O
=	O
murderer	O
)	O
=	O
0.9	O
which	O
means	O
that	O
p	O
(	O
b	O
=	O
not	O
murderer	O
)	O
=	O
0.4	O
,	O
p	O
(	O
m	O
=	O
not	O
murderer	O
)	O
=	O
0.1	O
draft	O
march	O
9	O
,	O
2010	O
(	O
1.2.12	O
)	O
9	O
p	O
(	O
θ	O
)	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
p	O
(	O
d|θ	O
)	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
p	O
(	O
d	O
)	O
prior	B
likelihood	O
evidence	B
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
p	O
(	O
θ|d	O
)	O
posterior	B
=	O
(	O
1.3.1	O
)	O
the	O
evidence	B
is	O
also	O
called	O
the	O
marginal	B
likelihood	I
.	O
the	O
term	O
likelihood	B
is	O
used	O
for	O
the	O
probability	B
that	O
a	O
model	B
generates	O
observed	O
data	O
.	O
more	O
fully	O
,	O
if	O
we	O
condition	O
on	O
the	O
model	B
m	O
,	O
we	O
have	O
p	O
(	O
θ|d	O
,	O
m	O
)	O
=	O
p	O
(	O
d|θ	O
,	O
m	O
)	O
p	O
(	O
θ|m	O
)	O
p	O
(	O
d|m	O
)	O
where	O
we	O
see	O
the	O
role	O
of	O
the	O
likelihood	B
p	O
(	O
d|θ	O
,	O
m	O
)	O
and	O
marginal	B
likelihood	I
p	O
(	O
d|m	O
)	O
.	O
the	O
marginal	B
likelihood	I
is	O
also	O
called	O
the	O
model	B
likelihood	O
.	O
the	O
most	B
probable	I
a	I
posteriori	I
argmax	O
p	O
(	O
θ|d	O
,	O
m	O
)	O
.	O
θ	O
(	O
map	B
)	O
setting	O
is	O
that	O
which	O
maximises	O
the	O
posterior	B
,	O
θ∗	O
=	O
bayes	O
’	O
rule	O
tells	O
us	O
how	O
to	O
update	O
our	O
prior	B
knowledge	O
with	O
the	O
data	B
generating	O
mechanism	O
.	O
the	O
prior	B
distribution	O
p	O
(	O
θ	O
)	O
describes	O
the	O
information	O
we	O
have	O
about	O
the	O
variable	B
before	O
seeing	O
any	O
data	B
.	O
after	O
data	B
d	O
arrives	O
,	O
we	O
update	O
the	O
prior	B
distribution	O
to	O
the	O
posterior	B
p	O
(	O
θ|d	O
)	O
∝	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ	O
)	O
.	O
1.3.1	O
two	O
dice	O
:	O
what	O
were	O
the	O
individual	O
scores	O
?	O
two	O
fair	O
dice	O
are	O
rolled	O
.	O
someone	O
tells	O
you	O
that	O
the	O
sum	O
of	O
the	O
two	O
scores	O
is	O
9.	O
what	O
is	O
the	O
probability	B
distribution	O
of	O
the	O
two	O
dice	O
scores3	O
?	O
the	O
score	O
of	O
die	O
a	O
is	O
denoted	O
sa	O
with	O
dom	O
(	O
sa	O
)	O
=	O
{	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
,	O
5	O
,	O
6	O
}	O
and	O
similarly	O
for	O
sb	O
.	O
the	O
three	O
variables	O
involved	O
are	O
then	O
sa	O
,	O
sb	O
and	O
the	O
total	O
score	O
,	O
t	O
=	O
sa	O
+	O
sb	O
.	O
a	O
model	B
of	O
these	O
three	O
variables	O
naturally	O
takes	O
the	O
form	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
p	O
(	O
t	O
,	O
sa	O
,	O
sb	O
)	O
=	O
p	O
(	O
t|sa	O
,	O
sb	O
)	O
(	O
cid:124	O
)	O
likelihood	B
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
p	O
(	O
sa	O
,	O
sb	O
)	O
prior	B
1.3	O
prior	B
,	O
likelihood	B
and	O
posterior	B
the	O
prior	B
,	O
likelihood	B
and	O
posterior	B
are	O
all	O
probabilities	O
.	O
they	O
are	O
assigned	O
these	O
names	O
due	O
to	O
their	O
role	O
in	O
bayes	O
’	O
rule	O
,	O
described	O
below	O
.	O
prior	B
,	O
likelihood	B
and	O
posterior	B
deﬁnition	O
7.	O
prior	B
likelihood	O
and	O
posterior	B
for	O
data	B
d	O
and	O
variable	B
θ	O
,	O
bayes	O
’	O
rule	O
tells	O
us	O
how	O
to	O
update	O
our	O
prior	B
beliefs	O
about	O
the	O
variable	B
θ	O
in	O
light	O
of	O
the	O
data	B
to	O
a	O
posterior	B
belief	O
:	O
the	O
prior	B
p	O
(	O
sa	O
,	O
sb	O
)	O
is	O
the	O
joint	B
probability	O
of	O
score	O
sa	O
and	O
score	O
sb	O
without	O
knowing	O
anything	O
else	O
.	O
assum-	O
ing	O
no	O
dependency	O
in	O
the	O
rolling	O
mechanism	O
,	O
p	O
(	O
sa	O
,	O
sb	O
)	O
=	O
p	O
(	O
sa	O
)	O
p	O
(	O
sb	O
)	O
(	O
1.3.3	O
)	O
since	O
the	O
dice	O
are	O
fair	O
both	O
p	O
(	O
sa	O
)	O
and	O
p	O
(	O
sb	O
)	O
are	O
uni-	O
form	O
distributions	O
,	O
p	O
(	O
sa	O
=	O
s	O
)	O
=	O
1/6	O
.	O
3this	O
example	O
is	O
due	O
to	O
taylan	O
cemgil	O
.	O
sa	O
=	O
1	O
1/36	O
1/36	O
1/36	O
1/36	O
1/36	O
1/36	O
sb	O
=	O
1	O
sb	O
=	O
2	O
sb	O
=	O
3	O
sb	O
=	O
4	O
sb	O
=	O
5	O
sb	O
=	O
6	O
p	O
(	O
sa	O
)	O
p	O
(	O
sb	O
)	O
:	O
sa	O
=	O
2	O
1/36	O
1/36	O
1/36	O
1/36	O
1/36	O
1/36	O
sa	O
=	O
3	O
1/36	O
1/36	O
1/36	O
1/36	O
1/36	O
1/36	O
sa	O
=	O
4	O
1/36	O
1/36	O
1/36	O
1/36	O
1/36	O
1/36	O
10	O
draft	O
march	O
9	O
,	O
2010	O
(	O
1.3.2	O
)	O
sa	O
=	O
5	O
1/36	O
1/36	O
1/36	O
1/36	O
1/36	O
1/36	O
sa	O
=	O
6	O
1/36	O
1/36	O
1/36	O
1/36	O
1/36	O
1/36	O
further	O
worked	O
examples	O
here	O
the	O
likelihood	B
term	O
is	O
p	O
(	O
t|sa	O
,	O
sb	O
)	O
=	O
i	O
[	O
t	O
=	O
sa	O
+	O
sb	O
]	O
(	O
1.3.4	O
)	O
which	O
states	O
that	O
the	O
total	O
score	O
is	O
given	O
by	O
sa	O
+	O
sb	O
.	O
here	O
i	O
[	O
x	O
=	O
y	O
]	O
is	O
the	O
indicator	B
function	I
deﬁned	O
as	O
i	O
[	O
x	O
=	O
y	O
]	O
=	O
1	O
if	O
x	O
=	O
y	O
and	O
0	O
otherwise	O
.	O
sa	O
=	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
sb	O
=	O
1	O
sb	O
=	O
2	O
sb	O
=	O
3	O
sb	O
=	O
4	O
sb	O
=	O
5	O
sb	O
=	O
6	O
p	O
(	O
t	O
=	O
9|sa	O
,	O
sb	O
)	O
:	O
sa	O
=	O
3	O
sa	O
=	O
2	O
sa	O
=	O
4	O
sa	O
=	O
5	O
sa	O
=	O
6	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
hence	O
,	O
our	O
complete	O
model	B
is	O
p	O
(	O
t	O
,	O
sa	O
,	O
sb	O
)	O
=	O
p	O
(	O
t|sa	O
,	O
sb	O
)	O
p	O
(	O
sa	O
)	O
p	O
(	O
sb	O
)	O
(	O
1.3.5	O
)	O
where	O
the	O
terms	O
on	O
the	O
right	O
are	O
explicitly	O
deﬁned	O
.	O
our	O
interest	O
is	O
then	O
obtainable	O
using	O
bayes	O
’	O
rule	O
,	O
(	O
1.3.6	O
)	O
where	O
p	O
(	O
t	O
=	O
9	O
)	O
p	O
(	O
sa	O
,	O
sb|t	O
=	O
9	O
)	O
=	O
p	O
(	O
t	O
=	O
9|sa	O
,	O
sb	O
)	O
p	O
(	O
sa	O
)	O
p	O
(	O
sb	O
)	O
p	O
(	O
t	O
=	O
9	O
)	O
=	O
(	O
cid:88	O
)	O
the	O
term	O
p	O
(	O
t	O
=	O
9	O
)	O
=	O
(	O
cid:80	O
)	O
sa	O
,	O
sb	O
p	O
(	O
t	O
=	O
9|sa	O
,	O
sb	O
)	O
p	O
(	O
sa	O
)	O
p	O
(	O
sb	O
)	O
(	O
1.3.7	O
)	O
p	O
(	O
t	O
=	O
9|sa	O
,	O
sb	O
)	O
p	O
(	O
sa	O
)	O
p	O
(	O
sb	O
)	O
:	O
sa	O
=	O
2	O
sa	O
=	O
3	O
sa	O
=	O
1	O
sa	O
=	O
4	O
sa	O
=	O
5	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1/36	O
0	O
0	O
0	O
0	O
1/36	O
0	O
0	O
0	O
0	O
1/36	O
0	O
0	O
sa	O
=	O
6	O
0	O
0	O
1/36	O
0	O
0	O
0	O
p	O
(	O
sa	O
,	O
sb|t	O
=	O
9	O
)	O
:	O
sa	O
=	O
3	O
sa	O
=	O
2	O
sa	O
=	O
4	O
sa	O
=	O
5	O
sa	O
=	O
6	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1/4	O
0	O
0	O
0	O
0	O
1/4	O
0	O
0	O
0	O
0	O
1/4	O
0	O
0	O
0	O
0	O
1/4	O
0	O
0	O
0	O
sa	O
=	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
sb	O
=	O
1	O
sb	O
=	O
2	O
sb	O
=	O
3	O
sb	O
=	O
4	O
sb	O
=	O
5	O
sb	O
=	O
6	O
sb	O
=	O
1	O
sb	O
=	O
2	O
sb	O
=	O
3	O
sb	O
=	O
4	O
sb	O
=	O
5	O
sb	O
=	O
6	O
equal	O
mass	O
in	O
only	O
4	O
non-zero	O
elements	O
,	O
as	O
shown	O
.	O
sa	O
,	O
sb	O
p	O
(	O
t	O
=	O
9|sa	O
,	O
sb	O
)	O
p	O
(	O
sa	O
)	O
p	O
(	O
sb	O
)	O
=	O
4	O
×	O
1/36	O
=	O
1/9	O
.	O
hence	O
the	O
posterior	B
is	O
given	O
by	O
1.4	O
further	O
worked	O
examples	O
example	O
3	O
(	O
who	O
’	O
s	O
in	O
the	O
bathroom	O
?	O
)	O
.	O
consider	O
a	O
household	O
of	O
three	O
people	O
,	O
alice	O
,	O
bob	O
and	O
cecil	O
.	O
cecil	O
wants	O
to	O
go	O
to	O
the	O
bathroom	O
but	O
ﬁnds	O
it	O
occupied	O
.	O
he	O
then	O
goes	O
to	O
alice	O
’	O
s	O
room	O
and	O
sees	O
she	O
is	O
there	O
.	O
since	O
cecil	O
knows	O
that	O
only	O
either	O
alice	O
or	O
bob	O
can	O
be	O
in	O
the	O
bathroom	O
,	O
from	O
this	O
he	O
infers	O
that	O
bob	O
must	O
be	O
in	O
the	O
bathroom	O
.	O
to	O
arrive	O
at	O
the	O
same	O
conclusion	O
in	O
a	O
mathematical	O
framework	O
,	O
let	O
’	O
s	O
deﬁne	O
the	O
following	O
events	O
a	O
=	O
alice	O
is	O
in	O
her	O
bedroom	O
,	O
b	O
=	O
bob	O
is	O
in	O
his	O
bedroom	O
,	O
o	O
=	O
bathroom	O
occupied	O
(	O
1.4.1	O
)	O
we	O
can	O
encode	O
the	O
information	O
that	O
if	O
either	O
alice	O
or	O
bob	O
are	O
not	O
in	O
their	O
bedrooms	O
,	O
then	O
they	O
must	O
be	O
in	O
the	O
bathroom	O
(	O
they	O
might	O
both	O
be	O
in	O
the	O
bathroom	O
)	O
as	O
p	O
(	O
o	O
=	O
tr|a	O
=	O
fa	O
,	O
b	O
)	O
=	O
1	O
,	O
p	O
(	O
o	O
=	O
tr|a	O
,	O
b	O
=	O
fa	O
)	O
=	O
1	O
(	O
1.4.2	O
)	O
the	O
ﬁrst	O
term	O
expresses	O
that	O
the	O
bathroom	O
is	O
occupied	O
if	O
alice	O
is	O
not	O
in	O
her	O
bedroom	O
,	O
wherever	O
bob	O
is	O
.	O
similarly	O
,	O
the	O
second	O
term	O
expresses	O
bathroom	O
occupancy	O
as	O
long	O
as	O
bob	O
is	O
not	O
in	O
his	O
bedroom	O
.	O
then	O
p	O
(	O
b	O
=	O
fa|o	O
=	O
tr	O
,	O
a	O
=	O
tr	O
)	O
=	O
p	O
(	O
b	O
=	O
fa	O
,	O
o	O
=	O
tr	O
,	O
a	O
=	O
tr	O
)	O
p	O
(	O
o	O
=	O
tr	O
,	O
a	O
=	O
tr	O
)	O
where	O
=	O
p	O
(	O
o	O
=	O
tr|a	O
=	O
tr	O
,	O
b	O
=	O
fa	O
)	O
p	O
(	O
a	O
=	O
tr	O
,	O
b	O
=	O
fa	O
)	O
p	O
(	O
o	O
=	O
tr	O
,	O
a	O
=	O
tr	O
)	O
p	O
(	O
o	O
=	O
tr	O
,	O
a	O
=	O
tr	O
)	O
=	O
p	O
(	O
o	O
=	O
tr|a	O
=	O
tr	O
,	O
b	O
=	O
fa	O
)	O
p	O
(	O
a	O
=	O
tr	O
,	O
b	O
=	O
fa	O
)	O
+	O
p	O
(	O
o	O
=	O
tr|a	O
=	O
tr	O
,	O
b	O
=	O
tr	O
)	O
p	O
(	O
a	O
=	O
tr	O
,	O
b	O
=	O
tr	O
)	O
draft	O
march	O
9	O
,	O
2010	O
(	O
1.4.3	O
)	O
(	O
1.4.4	O
)	O
11	O
further	O
worked	O
examples	O
using	O
the	O
fact	O
p	O
(	O
o	O
=	O
tr|a	O
=	O
tr	O
,	O
b	O
=	O
fa	O
)	O
=	O
1	O
and	O
p	O
(	O
o	O
=	O
tr|a	O
=	O
tr	O
,	O
b	O
=	O
tr	O
)	O
=	O
0	O
,	O
which	O
encodes	O
that	O
if	O
if	O
alice	O
is	O
in	O
her	O
room	O
and	O
bob	O
is	O
not	O
,	O
the	O
bathroom	O
must	O
be	O
occupied	O
,	O
and	O
similarly	O
,	O
if	O
both	O
alice	O
and	O
bob	O
are	O
in	O
their	O
rooms	O
,	O
the	O
bathroom	O
can	O
not	O
be	O
occupied	O
,	O
p	O
(	O
b	O
=	O
fa|o	O
=	O
tr	O
,	O
a	O
=	O
tr	O
)	O
=	O
p	O
(	O
a	O
=	O
tr	O
,	O
b	O
=	O
fa	O
)	O
p	O
(	O
a	O
=	O
tr	O
,	O
b	O
=	O
fa	O
)	O
=	O
1	O
(	O
1.4.5	O
)	O
this	O
example	O
is	O
interesting	O
since	O
we	O
are	O
not	O
required	O
to	O
make	O
a	O
full	O
probabilistic	B
model	O
in	O
this	O
case	O
thanks	O
to	O
the	O
limiting	O
nature	O
of	O
the	O
probabilities	O
(	O
we	O
don	O
’	O
t	O
need	O
to	O
specify	O
p	O
(	O
a	O
,	O
b	O
)	O
)	O
.	O
the	O
situation	O
is	O
common	O
in	O
limiting	O
situations	O
of	O
probabilities	O
being	O
either	O
0	O
or	O
1	O
,	O
corresponding	O
to	O
traditional	O
logic	B
systems	O
.	O
example	O
4	O
(	O
aristotle	O
:	O
resolution	B
)	O
.	O
we	O
can	O
represent	O
the	O
statement	O
‘	O
all	O
apples	O
are	O
fruit	O
’	O
by	O
p	O
(	O
f	O
=	O
tr|a	O
=	O
tr	O
)	O
=	O
1.	O
similarly	O
,	O
‘	O
all	O
fruits	O
grow	O
on	O
trees	O
’	O
may	O
be	O
represented	O
by	O
p	O
(	O
t	O
=	O
tr|f	O
=	O
tr	O
)	O
=	O
1.	O
additionally	O
we	O
assume	O
that	O
whether	O
or	O
not	O
something	O
grows	O
on	O
a	O
tree	B
depends	O
only	O
on	O
whether	O
or	O
not	O
it	O
is	O
a	O
fruit	O
,	O
p	O
(	O
t|a	O
,	O
f	O
)	O
=	O
p	O
(	O
t|f	O
)	O
.	O
from	O
these	O
,	O
we	O
can	O
compute	O
p	O
(	O
t	O
=	O
tr|a	O
=	O
tr	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
(	O
cid:125	O
)	O
=	O
p	O
(	O
t	O
=	O
tr|f	O
=	O
fa	O
)	O
p	O
(	O
f	O
=	O
fa|a	O
=	O
tr	O
)	O
p	O
(	O
t	O
=	O
tr|f	O
,	O
a	O
=	O
tr	O
)	O
p	O
(	O
f|a	O
=	O
tr	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
(	O
cid:124	O
)	O
(	O
cid:125	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
p	O
(	O
f	O
=	O
tr|a	O
=	O
tr	O
)	O
+	O
p	O
(	O
t	O
=	O
tr|f	O
=	O
tr	O
)	O
p	O
(	O
t	O
=	O
tr|f	O
)	O
p	O
(	O
f|a	O
=	O
tr	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
1.4.6	O
)	O
(	O
1.4.7	O
)	O
=	O
1	O
f	O
=0	O
=1	O
f	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
=1	O
in	O
other	O
words	O
we	O
have	O
deduced	O
that	O
‘	O
all	O
apples	O
grow	O
on	O
trees	O
’	O
is	O
a	O
true	O
statement	O
,	O
based	O
on	O
the	O
information	O
presented	O
.	O
(	O
this	O
kind	O
of	O
reasoning	O
is	O
called	O
resolution	B
and	O
is	O
a	O
form	O
of	O
transitivity	O
:	O
from	O
the	O
statements	O
a	O
⇒	O
f	O
and	O
f	O
⇒	O
t	O
we	O
can	O
infer	O
a	O
⇒	O
t	O
)	O
.	O
example	O
5	O
(	O
aristotle	O
:	O
inverse	B
modus	I
ponens	I
)	O
.	O
according	O
to	O
logic	B
,	O
from	O
the	O
statement	O
:	O
‘	O
if	O
a	O
is	O
true	O
then	O
b	O
is	O
true	O
’	O
,	O
one	O
may	O
deduce	O
that	O
‘	O
if	O
b	O
is	O
false	O
then	O
a	O
is	O
false	O
’	O
.	O
let	O
’	O
s	O
see	O
how	O
this	O
ﬁts	O
in	O
with	O
a	O
probabilistic	B
reasoning	O
system	B
.	O
we	O
can	O
express	O
the	O
statement	O
:	O
‘	O
if	O
a	O
is	O
true	O
then	O
b	O
is	O
true	O
’	O
as	O
p	O
(	O
b	O
=	O
tr|a	O
=	O
tr	O
)	O
=	O
1.	O
then	O
we	O
may	O
infer	O
p	O
(	O
a	O
=	O
fa|b	O
=	O
fa	O
)	O
=	O
1	O
−	O
p	O
(	O
a	O
=	O
tr|b	O
=	O
fa	O
)	O
=	O
1	O
−	O
p	O
(	O
b	O
=	O
fa|a	O
=	O
tr	O
)	O
p	O
(	O
a	O
=	O
tr	O
)	O
p	O
(	O
b	O
=	O
fa|a	O
=	O
tr	O
)	O
p	O
(	O
a	O
=	O
tr	O
)	O
+	O
p	O
(	O
b	O
=	O
fa|a	O
=	O
fa	O
)	O
p	O
(	O
a	O
=	O
fa	O
)	O
=	O
1	O
(	O
1.4.8	O
)	O
this	O
follows	O
since	O
p	O
(	O
b	O
=	O
fa|a	O
=	O
tr	O
)	O
=	O
1	O
−	O
p	O
(	O
b	O
=	O
tr|a	O
=	O
tr	O
)	O
=	O
1	O
−	O
1	O
=	O
0	O
,	O
annihilating	O
the	O
second	O
term	O
.	O
both	O
the	O
above	O
examples	O
are	O
intuitive	O
expressions	O
of	O
deductive	O
logic	B
.	O
the	O
standard	O
rules	O
of	O
aristotelian	O
logic	B
are	O
therefore	O
seen	O
to	O
be	O
limiting	O
cases	O
of	O
probabilistic	B
reasoning	O
.	O
example	O
6	O
(	O
soft	B
xor	O
gate	O
)	O
.	O
a	O
standard	O
xor	O
logic	B
gate	O
is	O
given	O
by	O
the	O
table	O
on	O
the	O
right	O
.	O
if	O
we	O
observe	O
that	O
the	O
output	O
of	O
the	O
xor	O
gate	O
is	O
0	O
,	O
what	O
can	O
we	O
say	O
about	O
a	O
and	O
b	O
?	O
in	O
this	O
case	O
,	O
either	O
a	O
and	O
b	O
were	O
both	O
0	O
,	O
or	O
a	O
and	O
b	O
were	O
both	O
1.	O
this	O
means	O
we	O
don	O
’	O
t	O
know	O
which	O
state	O
a	O
was	O
in	O
–	O
it	O
could	O
equally	O
likely	O
have	O
been	O
1	O
or	O
0.	O
a	O
b	O
a	O
xor	O
b	O
0	O
0	O
1	O
1	O
0	O
1	O
0	O
1	O
0	O
1	O
1	O
0	O
12	O
draft	O
march	O
9	O
,	O
2010	O
further	O
worked	O
examples	O
consider	O
a	O
‘	O
soft	B
’	O
version	O
of	O
the	O
xor	O
gate	O
given	O
on	O
the	O
right	O
,	O
with	O
additionally	O
p	O
(	O
a	O
=	O
1	O
)	O
=	O
0.65	O
,	O
p	O
(	O
b	O
=	O
1	O
)	O
=	O
0.77.	O
what	O
is	O
p	O
(	O
a	O
=	O
1|c	O
=	O
0	O
)	O
?	O
p	O
(	O
a	O
=	O
1	O
,	O
c	O
=	O
0	O
)	O
=	O
(	O
cid:88	O
)	O
p	O
(	O
a	O
=	O
1	O
,	O
b	O
,	O
c	O
=	O
0	O
)	O
=	O
(	O
cid:88	O
)	O
b	O
b	O
p	O
(	O
c	O
=	O
0|a	O
=	O
1	O
,	O
b	O
)	O
p	O
(	O
a	O
=	O
1	O
)	O
p	O
(	O
b	O
)	O
a	O
b	O
p	O
(	O
c	O
=	O
1|a	O
,	O
b	O
)	O
0	O
0	O
1	O
1	O
0.1	O
0.99	O
0.8	O
0.25	O
0	O
1	O
0	O
1	O
=	O
p	O
(	O
a	O
=	O
1	O
)	O
(	O
p	O
(	O
c	O
=	O
0|a	O
=	O
1	O
,	O
b	O
=	O
0	O
)	O
p	O
(	O
b	O
=	O
0	O
)	O
+	O
p	O
(	O
c	O
=	O
0|a	O
=	O
1	O
,	O
b	O
=	O
1	O
)	O
p	O
(	O
b	O
=	O
1	O
)	O
)	O
=	O
0.65	O
×	O
(	O
0.2	O
×	O
0.23	O
+	O
0.75	O
×	O
0.77	O
)	O
=	O
0.4053	O
(	O
1.4.9	O
)	O
p	O
(	O
a	O
=	O
0	O
,	O
c	O
=	O
0	O
)	O
=	O
(	O
cid:88	O
)	O
p	O
(	O
a	O
=	O
0	O
,	O
b	O
,	O
c	O
=	O
0	O
)	O
=	O
(	O
cid:88	O
)	O
b	O
b	O
p	O
(	O
c	O
=	O
0|a	O
=	O
0	O
,	O
b	O
)	O
p	O
(	O
a	O
=	O
0	O
)	O
p	O
(	O
b	O
)	O
=	O
p	O
(	O
a	O
=	O
0	O
)	O
(	O
p	O
(	O
c	O
=	O
0|a	O
=	O
0	O
,	O
b	O
=	O
0	O
)	O
p	O
(	O
b	O
=	O
0	O
)	O
+	O
p	O
(	O
c	O
=	O
0|a	O
=	O
0	O
,	O
b	O
=	O
1	O
)	O
p	O
(	O
b	O
=	O
1	O
)	O
)	O
=	O
0.35	O
×	O
(	O
0.9	O
×	O
0.23	O
+	O
0.01	O
×	O
0.77	O
)	O
=	O
0.0751	O
then	O
p	O
(	O
a	O
=	O
1|c	O
=	O
0	O
)	O
=	O
p	O
(	O
a	O
=	O
1	O
,	O
c	O
=	O
0	O
)	O
p	O
(	O
a	O
=	O
1	O
,	O
c	O
=	O
0	O
)	O
+	O
p	O
(	O
a	O
=	O
0	O
,	O
c	O
=	O
0	O
)	O
=	O
0.4053	O
0.4053	O
+	O
0.0751	O
=	O
0.8437	O
(	O
1.4.10	O
)	O
example	O
7	O
(	O
larry	O
)	O
.	O
larry	O
is	O
typically	O
late	O
for	O
school	O
.	O
if	O
larry	O
is	O
late	O
,	O
we	O
denote	O
this	O
with	O
l	O
=	O
late	O
,	O
otherwise	O
,	O
l	O
=	O
not	O
late	O
.	O
when	O
his	O
mother	O
asks	O
whether	O
or	O
not	O
he	O
was	O
late	O
for	O
school	O
he	O
never	O
admits	O
to	O
being	O
late	O
.	O
the	O
response	O
larry	O
gives	O
rl	O
is	O
represented	O
as	O
follows	O
p	O
(	O
rl	O
=	O
not	O
late|l	O
=	O
not	O
late	O
)	O
=	O
1	O
,	O
p	O
(	O
rl	O
=	O
late|l	O
=	O
late	O
)	O
=	O
0	O
the	O
remaining	O
two	O
values	O
are	O
determined	O
by	O
normalisation	B
and	O
are	O
p	O
(	O
rl	O
=	O
late|l	O
=	O
not	O
late	O
)	O
=	O
0	O
,	O
p	O
(	O
rl	O
=	O
not	O
late|l	O
=	O
late	O
)	O
=	O
1	O
(	O
1.4.11	O
)	O
(	O
1.4.12	O
)	O
given	O
that	O
rl	O
=	O
not	O
late	O
,	O
what	O
is	O
the	O
probability	B
that	O
larry	O
was	O
late	O
,	O
i.e	O
.	O
p	O
(	O
l	O
=	O
late|rl	O
=	O
not	O
late	O
)	O
?	O
using	O
bayes	O
’	O
we	O
have	O
p	O
(	O
l	O
=	O
late|rl	O
=	O
not	O
late	O
)	O
=	O
p	O
(	O
l	O
=	O
late	O
,	O
rl	O
=	O
not	O
late	O
)	O
p	O
(	O
rl	O
=	O
not	O
late	O
)	O
=	O
p	O
(	O
l	O
=	O
late	O
,	O
rl	O
=	O
not	O
late	O
)	O
p	O
(	O
l	O
=	O
late	O
,	O
rl	O
=	O
not	O
late	O
)	O
+	O
p	O
(	O
l	O
=	O
not	O
late	O
,	O
rl	O
=	O
not	O
late	O
)	O
in	O
the	O
above	O
(	O
cid:125	O
)	O
p	O
(	O
l	O
=	O
late	O
,	O
rl	O
=	O
not	O
late	O
)	O
=	O
p	O
(	O
rl	O
=	O
not	O
late|l	O
=	O
late	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:124	O
)	O
=1	O
p	O
(	O
l	O
=	O
late	O
)	O
(	O
1.4.13	O
)	O
(	O
1.4.14	O
)	O
and	O
(	O
cid:125	O
)	O
p	O
(	O
l	O
=	O
not	O
late	O
,	O
rl	O
=	O
not	O
late	O
)	O
=	O
p	O
(	O
rl	O
=	O
not	O
late|l	O
=	O
not	O
late	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:124	O
)	O
=1	O
p	O
(	O
l	O
=	O
not	O
late	O
)	O
(	O
1.4.15	O
)	O
hence	O
p	O
(	O
l	O
=	O
late|rl	O
=	O
not	O
late	O
)	O
=	O
p	O
(	O
l	O
=	O
late	O
)	O
p	O
(	O
l	O
=	O
late	O
)	O
+	O
p	O
(	O
l	O
=	O
not	O
late	O
)	O
=	O
p	O
(	O
l	O
=	O
late	O
)	O
(	O
1.4.16	O
)	O
draft	O
march	O
9	O
,	O
2010	O
13	O
where	O
we	O
used	O
normalisation	B
in	O
the	O
last	O
step	O
,	O
p	O
(	O
l	O
=	O
late	O
)	O
+	O
p	O
(	O
l	O
=	O
not	O
late	O
)	O
=	O
1.	O
this	O
result	O
is	O
intuitive	O
–	O
larry	O
’	O
s	O
mother	O
knows	O
that	O
he	O
never	O
admits	O
to	O
being	O
late	O
,	O
so	O
her	O
belief	O
about	O
whether	O
or	O
not	O
he	O
really	O
was	O
late	O
is	O
unchanged	O
,	O
regardless	O
of	O
what	O
larry	O
actually	O
says	O
.	O
further	O
worked	O
examples	O
example	O
8	O
(	O
larry	O
and	O
sue	O
)	O
.	O
continuing	O
the	O
example	O
above	O
,	O
larry	O
’	O
s	O
sister	O
sue	O
always	O
tells	O
the	O
truth	O
to	O
her	O
mother	O
as	O
to	O
whether	O
or	O
not	O
larry	O
was	O
late	O
for	O
school	O
.	O
p	O
(	O
rs	O
=	O
not	O
late|l	O
=	O
not	O
late	O
)	O
=	O
1	O
,	O
p	O
(	O
rs	O
=	O
late|l	O
=	O
late	O
)	O
=	O
1	O
the	O
remaining	O
two	O
values	O
are	O
determined	O
by	O
normalisation	B
and	O
are	O
p	O
(	O
rs	O
=	O
late|l	O
=	O
not	O
late	O
)	O
=	O
0	O
,	O
p	O
(	O
rs	O
=	O
not	O
late|l	O
=	O
late	O
)	O
=	O
0	O
we	O
also	O
assume	O
p	O
(	O
rs	O
,	O
rl|l	O
)	O
=	O
p	O
(	O
rs|l	O
)	O
p	O
(	O
rl|l	O
)	O
.	O
we	O
can	O
then	O
write	O
p	O
(	O
rl	O
,	O
rs	O
,	O
l	O
)	O
=	O
p	O
(	O
rl|l	O
)	O
p	O
(	O
rs|l	O
)	O
p	O
(	O
l	O
)	O
given	O
that	O
rs	O
=	O
late	O
,	O
what	O
is	O
the	O
probability	B
that	O
larry	O
was	O
late	O
?	O
using	O
bayes	O
’	O
rule	O
,	O
we	O
have	O
(	O
1.4.17	O
)	O
(	O
1.4.18	O
)	O
(	O
1.4.19	O
)	O
p	O
(	O
l	O
=	O
late|rl	O
=	O
not	O
late	O
,	O
rs	O
=	O
late	O
)	O
=	O
1	O
z	O
p	O
(	O
rs	O
=	O
late|l	O
=	O
late	O
)	O
p	O
(	O
rl	O
=	O
not	O
late|l	O
=	O
late	O
)	O
p	O
(	O
l	O
=	O
late	O
)	O
where	O
the	O
normalisation	B
z	O
is	O
given	O
by	O
p	O
(	O
rs	O
=	O
late|l	O
=	O
late	O
)	O
p	O
(	O
rl	O
=	O
not	O
late|l	O
=	O
late	O
)	O
p	O
(	O
l	O
=	O
late	O
)	O
+	O
p	O
(	O
rs	O
=	O
late|l	O
=	O
not	O
late	O
)	O
p	O
(	O
rl	O
=	O
not	O
late|l	O
=	O
not	O
late	O
)	O
p	O
(	O
l	O
=	O
not	O
late	O
)	O
hence	O
p	O
(	O
l	O
=	O
late|rl	O
=	O
not	O
late	O
,	O
rs	O
=	O
late	O
)	O
=	O
1	O
×	O
1	O
×	O
p	O
(	O
l	O
=	O
late	O
)	O
1	O
×	O
1	O
×	O
p	O
(	O
l	O
=	O
late	O
)	O
+	O
0	O
×	O
1	O
×	O
p	O
(	O
l	O
=	O
not	O
late	O
)	O
=	O
1	O
(	O
1.4.20	O
)	O
this	O
result	O
is	O
also	O
intuitive	O
–	O
since	O
larry	O
’	O
s	O
mother	O
knows	O
that	O
sue	O
always	O
tells	O
the	O
truth	O
,	O
no	O
matter	O
what	O
larry	O
says	O
,	O
she	O
knows	O
he	O
was	O
late	O
.	O
example	O
9	O
(	O
luke	O
)	O
.	O
luke	O
has	O
been	O
told	O
he	O
’	O
s	O
lucky	O
and	O
has	O
won	O
a	O
prize	O
in	O
the	O
lottery	O
.	O
there	O
are	O
5	O
prizes	O
available	O
of	O
value	B
£10	O
,	O
£100	O
,	O
£1000	O
,	O
£10000	O
,	O
£1000000	O
.	O
the	O
prior	B
probabilities	O
of	O
winning	O
these	O
5	O
prizes	O
are	O
p1	O
,	O
p2	O
,	O
p3	O
,	O
p4	O
,	O
p5	O
,	O
with	O
p0	O
being	O
the	O
prior	B
probability	O
of	O
winning	O
no	O
prize	O
.	O
luke	O
asks	O
eagerly	O
‘	O
did	O
i	O
win	O
£1000000	O
?	O
!	O
’	O
.	O
‘	O
i	O
’	O
m	O
afraid	O
not	O
sir	O
’	O
,	O
is	O
the	O
response	O
of	O
the	O
lottery	O
phone	O
operator	O
.	O
‘	O
did	O
i	O
win	O
£10000	O
?	O
!	O
’	O
asks	O
luke	O
.	O
‘	O
again	O
,	O
i	O
’	O
m	O
afraid	O
not	O
sir	O
’	O
.	O
what	O
is	O
the	O
probability	B
that	O
luke	O
has	O
won	O
£1000	O
?	O
note	O
ﬁrst	O
that	O
p0	O
+	O
p1	O
+	O
p2	O
+	O
p3	O
+	O
p4	O
+	O
p5	O
=	O
1.	O
we	O
denote	O
w	O
=	O
1	O
for	O
the	O
ﬁrst	O
prize	O
of	O
£10	O
,	O
and	O
w	O
=	O
2	O
,	O
.	O
.	O
.	O
,	O
5	O
for	O
the	O
remaining	O
prizes	O
and	O
w	O
=	O
0	O
for	O
no	O
prize	O
.	O
we	O
need	O
to	O
compute	O
p	O
(	O
w	O
=	O
3|w	O
(	O
cid:54	O
)	O
=	O
5	O
,	O
w	O
(	O
cid:54	O
)	O
=	O
4	O
,	O
w	O
(	O
cid:54	O
)	O
=	O
0	O
)	O
=	O
p	O
(	O
w	O
=	O
3	O
,	O
w	O
(	O
cid:54	O
)	O
=	O
5	O
,	O
w	O
(	O
cid:54	O
)	O
=	O
4	O
,	O
w	O
(	O
cid:54	O
)	O
=	O
0	O
)	O
=	O
14	O
p	O
(	O
w	O
(	O
cid:54	O
)	O
=	O
5	O
,	O
w	O
(	O
cid:54	O
)	O
=	O
4	O
,	O
w	O
(	O
cid:54	O
)	O
=	O
0	O
)	O
p	O
(	O
w	O
=	O
1	O
or	O
w	O
=	O
2	O
or	O
w	O
=	O
3	O
)	O
p	O
(	O
w	O
=	O
3	O
)	O
=	O
p3	O
p1	O
+	O
p2	O
+	O
p3	O
(	O
1.4.21	O
)	O
draft	O
march	O
9	O
,	O
2010	O
code	O
where	O
the	O
term	O
in	O
the	O
denominator	O
is	O
computed	O
using	O
the	O
fact	O
that	O
the	O
events	O
w	O
are	O
mutually	O
exclusive	O
(	O
one	O
can	O
only	O
win	O
one	O
prize	O
)	O
.	O
this	O
result	O
makes	O
intuitive	O
sense	O
:	O
once	O
we	O
have	O
removed	O
the	O
impossible	O
states	O
of	O
w	O
,	O
the	O
probability	B
that	O
luke	O
wins	O
the	O
prize	O
is	O
proportional	O
to	O
the	O
prior	B
probability	O
of	O
that	O
prize	O
,	O
with	O
the	O
normalisation	B
being	O
simply	O
the	O
total	O
set	O
of	O
possible	O
probability	B
remaining	O
.	O
1.5	O
code	O
the	O
brmltoolbox	O
code	O
accompanying	O
this	O
book	O
is	O
intended	O
to	O
give	O
the	O
reader	O
some	O
insight	O
into	O
repre-	O
senting	O
discrete	B
probability	O
tables	O
and	O
performing	O
simple	O
inference	O
.	O
the	O
matlab	O
code	O
is	O
written	O
with	O
only	O
minimal	O
error	O
trapping	O
to	O
keep	O
the	O
code	O
as	O
short	O
and	O
hopefully	O
reasonably	O
readable4	O
.	O
1.5.1	O
basic	O
probability	B
code	O
at	O
the	O
simplest	O
level	O
,	O
we	O
only	O
need	O
two	O
basic	O
routines	O
.	O
one	O
for	O
multiplying	O
probability	B
tables	O
together	O
(	O
called	O
potentials	O
in	O
the	O
code	O
)	O
,	O
and	O
one	O
for	O
summing	O
a	O
probability	B
table	O
.	O
potentials	O
are	O
represented	O
using	O
a	O
structure	B
.	O
for	O
example	O
,	O
in	O
the	O
code	O
corresponding	O
to	O
the	O
inspector	O
clouseau	O
example	O
democlouseau.m	O
,	O
we	O
deﬁne	O
a	O
probability	B
table	O
as	O
>	O
>	O
pot	O
(	O
1	O
)	O
ans	O
=	O
variables	O
:	O
[	O
1	O
3	O
2	O
]	O
table	O
:	O
[	O
2x2x2	O
double	O
]	O
this	O
says	O
that	O
the	O
potential	B
depends	O
on	O
the	O
variables	O
1	O
,	O
3	O
,	O
2	O
and	O
the	O
entries	O
are	O
stored	O
in	O
the	O
array	O
given	O
by	O
the	O
table	O
ﬁeld	O
.	O
the	O
size	O
of	O
the	O
array	O
informs	O
how	O
many	O
states	O
each	O
variable	B
takes	O
in	O
the	O
order	O
given	O
by	O
variables	O
.	O
the	O
order	O
in	O
which	O
the	O
variables	O
are	O
deﬁned	O
in	O
a	O
potential	B
is	O
irrelevant	O
provided	O
that	O
one	O
indexes	O
the	O
array	O
consistently	O
.	O
a	O
routine	O
that	O
can	O
help	O
with	O
setting	O
table	O
entries	O
is	O
setstate.m	O
.	O
for	O
example	O
,	O
>	O
>	O
pot	O
(	O
1	O
)	O
=	O
setstate	O
(	O
pot	O
(	O
1	O
)	O
,	O
[	O
2	O
1	O
3	O
]	O
,	O
[	O
2	O
1	O
1	O
]	O
,0.3	O
)	O
means	O
that	O
for	O
potential	B
1	O
,	O
the	O
table	O
entry	O
for	O
variable	B
2	O
being	O
in	O
state	O
2	O
,	O
variable	B
1	O
being	O
in	O
state	O
1	O
and	O
variable	B
3	O
being	O
in	O
state	O
1	O
should	O
be	O
set	O
to	O
value	B
0.3.	O
the	O
philosophy	O
of	O
the	O
code	O
is	O
to	O
keep	O
the	O
information	O
required	O
to	O
perform	O
computations	O
to	O
a	O
minimum	O
.	O
additional	O
information	O
about	O
the	O
labels	O
of	O
variables	O
and	O
their	O
domains	O
can	O
be	O
useful	O
to	O
check	B
results	O
,	O
but	O
is	O
not	O
actually	O
required	O
to	O
carry	O
out	O
computations	O
.	O
one	O
may	O
also	O
specify	O
the	O
name	O
and	O
domain	B
of	O
each	O
variable	B
,	O
for	O
example	O
>	O
>	O
variable	B
(	O
3	O
)	O
ans	O
=	O
domain	B
:	O
{	O
’	O
murderer	O
’	O
’	O
not	O
murderer	O
’	O
}	O
name	O
:	O
’	O
butler	O
’	O
the	O
variable	B
name	O
and	O
domain	B
information	O
in	O
the	O
clouseau	O
example	O
is	O
stored	O
in	O
the	O
structure	B
variable	O
,	O
which	O
can	O
be	O
helpful	O
to	O
display	O
the	O
potential	B
table	O
:	O
>	O
>	O
disptable	O
(	O
pot	O
(	O
1	O
)	O
,	O
variable	B
)	O
;	O
knife	O
knife	O
knife	O
knife	O
knife	O
knife	O
knife	O
knife	O
used	O
not	O
used	O
used	O
not	O
used	O
used	O
not	O
used	O
used	O
not	O
used	O
=	O
=	O
=	O
=	O
=	O
=	O
=	O
=	O
maid	O
maid	O
maid	O
maid	O
maid	O
maid	O
maid	O
maid	O
=	O
murderer	O
=	O
murderer	O
=	O
not	O
murderer	O
=	O
not	O
murderer	O
=	O
murderer	O
=	O
murderer	O
=	O
not	O
murderer	O
=	O
not	O
murderer	O
butler	O
butler	O
butler	O
butler	O
butler	O
butler	O
butler	O
butler	O
=	O
=	O
=	O
=	O
=	O
=	O
=	O
=	O
murderer	O
murderer	O
murderer	O
murderer	O
not	O
murderer	O
not	O
murderer	O
not	O
murderer	O
not	O
murderer	O
0.100000	O
0.900000	O
0.600000	O
0.400000	O
0.200000	O
0.800000	O
0.300000	O
0.700000	O
4at	O
the	O
time	O
of	O
writing	O
,	O
some	O
versions	O
of	O
matlab	O
suﬀer	O
from	O
serious	O
memory	O
indexing	O
bugs	O
,	O
some	O
of	O
which	O
may	O
appear	O
in	O
the	O
array	O
structures	O
used	O
in	O
the	O
code	O
provided	O
.	O
to	O
deal	O
with	O
this	O
,	O
turn	O
oﬀ	O
the	O
jit	O
accelerator	O
by	O
typing	O
feature	O
accel	O
off	O
.	O
draft	O
march	O
9	O
,	O
2010	O
15	O
multiplying	O
potentials	O
in	O
order	O
to	O
multiply	O
potentials	O
(	O
as	O
for	O
arrays	O
)	O
the	O
tables	O
of	O
each	O
potential	B
must	O
be	O
dimensionally	O
consistent	B
–	O
that	O
is	O
the	O
number	O
of	O
states	O
of	O
variable	B
i	O
in	O
potential	B
1	O
must	O
match	O
the	O
number	O
of	O
states	O
of	O
variable	B
i	O
in	O
any	O
other	O
potential	B
.	O
this	O
can	O
be	O
checked	O
using	O
potvariables.m	O
.	O
this	O
consistency	O
is	O
also	O
required	O
for	O
other	O
basic	O
operations	O
such	O
as	O
summing	O
potentials	O
.	O
multpots.m	B
:	O
multiplying	O
two	O
or	O
more	O
potentials	O
divpots.m	O
:	O
dividing	O
a	O
potential	B
by	O
another	O
code	O
summing	O
a	O
potential	B
sumpot.m	O
:	O
sum	O
(	O
marginalise	O
)	O
a	O
potential	B
over	O
a	O
set	O
of	O
variables	O
sumpots.m	O
:	O
sum	O
a	O
set	O
of	O
potentials	O
together	O
making	O
a	O
conditional	B
potential	O
condpot.m	O
:	O
make	O
a	O
potential	B
conditioned	O
on	O
variables	O
setting	O
a	O
potential	B
setpot.m	O
:	O
set	O
variables	O
in	O
a	O
potential	B
to	O
given	O
states	O
setevpot.m	O
:	O
set	O
variables	O
in	O
a	O
potential	B
to	O
given	O
states	O
and	O
return	O
also	O
an	O
identity	O
potential	O
on	O
the	O
given	O
states	O
the	O
philosophy	O
of	O
brmltoolbox	O
is	O
that	O
all	O
information	O
about	O
variables	O
is	O
local	B
and	O
is	O
read	O
oﬀ	O
from	O
a	O
potential	B
.	O
using	O
setevpot.m	O
enables	O
one	O
to	O
set	O
variables	O
in	O
a	O
state	O
whilst	O
maintaining	O
information	O
about	O
the	O
number	O
of	O
states	O
of	O
a	O
variable	B
.	O
maximising	O
a	O
potential	B
maxpot.m	O
:	O
maximise	O
a	O
potential	B
over	O
a	O
set	O
of	O
variables	O
see	O
also	O
maxnarray.m	O
and	O
maxnpot.m	O
which	O
return	O
the	O
n-highest	O
values	O
and	O
associated	O
states	O
.	O
other	O
potential	B
utilities	O
setstate.m	O
:	O
set	O
the	O
a	O
potential	B
state	O
to	O
a	O
given	O
value	B
table.m	O
:	O
return	O
a	O
table	O
from	O
a	O
potential	B
whichpot.m	O
:	O
return	O
potentials	O
which	O
contain	O
a	O
set	O
of	O
variables	O
potvariables.m	O
:	O
variables	O
and	O
their	O
number	O
of	O
states	O
in	O
a	O
set	O
of	O
potentials	O
orderpotfields.m	O
:	O
order	O
the	O
ﬁelds	O
of	O
a	O
potential	B
structure	O
uniquepots.m	O
:	O
merge	O
redundant	O
potentials	O
and	O
return	O
only	O
unique	O
ones	O
numstates.m	O
:	O
number	O
of	O
states	O
of	O
a	O
variable	B
in	O
a	O
domain	B
squeezepots.m	O
:	O
remove	O
redundant	O
potentials	O
by	O
merging	O
normpot.m	O
:	O
normalise	O
a	O
potential	B
to	O
form	O
a	O
distribution	B
1.5.2	O
general	O
utilities	O
condp.m	O
:	O
return	O
a	O
table	O
p	O
(	O
x|y	O
)	O
from	O
p	O
(	O
x	O
,	O
y	O
)	O
condexp.m	O
:	O
form	O
a	O
conditional	B
distribution	O
from	O
a	O
log	O
value	B
logsumexp.m	O
:	O
compute	O
the	O
log	O
of	O
a	O
sum	O
of	O
exponentials	O
in	O
a	O
numerically	O
precise	O
way	O
normp.m	O
:	O
return	O
a	O
normalised	O
table	O
from	O
an	O
unnormalised	O
table	O
16	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
assign.m	O
:	O
assign	O
values	O
to	O
multiple	O
variables	O
maxarray.m	O
:	O
maximize	O
a	O
multi-dimensional	O
array	O
over	O
a	O
subset	O
1.5.3	O
an	O
example	O
the	O
following	O
code	O
highlights	O
the	O
use	O
of	O
the	O
above	O
routines	O
in	O
solving	B
the	O
inspector	O
clouseau	O
,	O
example	O
(	O
2	O
)	O
.	O
democlouseau.m	O
:	O
solving	B
the	O
inspector	O
clouseau	O
example	O
1.6	O
notes	O
the	O
interpretation	O
of	O
probability	B
is	O
contentious	O
and	O
we	O
refer	O
the	O
reader	O
to	O
[	O
145	O
,	O
183	O
,	O
179	O
]	O
for	O
detailed	O
discus-	O
sions	O
.	O
a	O
useful	O
website	B
that	O
relates	O
to	O
understanding	O
probability	B
and	O
bayesian	O
reasoning	O
is	O
understandinguncertainty.org	O
.	O
1.7	O
exercises	O
exercise	O
1.	O
prove	O
p	O
(	O
x	O
,	O
y|z	O
)	O
=	O
p	O
(	O
x|z	O
)	O
p	O
(	O
y|x	O
,	O
z	O
)	O
and	O
also	O
p	O
(	O
x|y	O
,	O
z	O
)	O
=	O
p	O
(	O
y|x	O
,	O
z	O
)	O
p	O
(	O
x|z	O
)	O
p	O
(	O
y|z	O
)	O
exercise	O
2.	O
prove	O
the	O
bonferroni	O
inequality	O
p	O
(	O
a	O
,	O
b	O
)	O
≥	O
p	O
(	O
a	O
)	O
+	O
p	O
(	O
b	O
)	O
−	O
1	O
(	O
1.7.1	O
)	O
(	O
1.7.2	O
)	O
(	O
1.7.3	O
)	O
exercise	O
3	O
(	O
adapted	O
from	O
[	O
167	O
]	O
)	O
.	O
there	O
are	O
two	O
boxes	O
.	O
box	O
1	O
contains	O
three	O
red	O
and	O
ﬁve	O
white	O
balls	O
and	O
box	O
2	O
contains	O
two	O
red	O
and	O
ﬁve	O
white	O
balls	O
.	O
a	O
box	O
is	O
chosen	O
at	O
random	O
p	O
(	O
box	O
=	O
1	O
)	O
=	O
p	O
(	O
box	O
=	O
2	O
)	O
=	O
0.5	O
and	O
a	O
ball	O
chosen	O
at	O
random	O
from	O
this	O
box	O
turns	O
out	O
to	O
be	O
red	O
.	O
what	O
is	O
the	O
posterior	B
probability	O
that	O
the	O
red	O
ball	O
came	O
from	O
box	O
1	O
?	O
exercise	O
4	O
(	O
adapted	O
from	O
[	O
167	O
]	O
)	O
.	O
two	O
balls	O
are	O
placed	O
in	O
a	O
box	O
as	O
follows	O
:	O
a	O
fair	O
coin	O
is	O
tossed	O
and	O
a	O
white	O
ball	O
is	O
placed	O
in	O
the	O
box	O
if	O
a	O
head	O
occurs	O
,	O
otherwise	O
a	O
red	O
ball	O
is	O
placed	O
in	O
the	O
box	O
.	O
the	O
coin	O
is	O
tossed	O
again	O
and	O
a	O
red	O
ball	O
is	O
placed	O
in	O
the	O
box	O
if	O
a	O
tail	O
occurs	O
,	O
otherwise	O
a	O
white	O
ball	O
is	O
placed	O
in	O
the	O
box	O
.	O
balls	O
are	O
drawn	O
from	O
the	O
box	O
three	O
times	O
in	O
succession	O
(	O
always	O
with	O
replacing	O
the	O
drawn	O
ball	O
back	O
in	O
the	O
box	O
)	O
.	O
it	O
is	O
found	O
that	O
on	O
all	O
three	O
occasions	O
a	O
red	O
ball	O
is	O
drawn	O
.	O
what	O
is	O
the	O
probability	B
that	O
both	O
balls	O
in	O
the	O
box	O
are	O
red	O
?	O
exercise	O
5	O
(	O
from	O
david	O
spiegelhalter	O
understandinguncertainty.org	O
)	O
.	O
a	O
secret	O
government	O
agency	O
has	O
developed	O
a	O
scanner	O
which	O
determines	O
whether	O
a	O
person	O
is	O
a	O
terrorist	O
.	O
the	O
scanner	O
is	O
fairly	O
reliable	O
;	O
95	O
%	O
of	O
all	O
scanned	O
terrorists	O
are	O
identiﬁed	O
as	O
terrorists	O
,	O
and	O
95	O
%	O
of	O
all	O
upstanding	O
citizens	O
are	O
identiﬁed	O
as	O
such	O
.	O
an	O
informant	O
tells	O
the	O
agency	O
that	O
exactly	O
one	O
passenger	O
of	O
100	O
aboard	O
an	O
aeroplane	O
in	O
which	O
you	O
are	O
seated	O
is	O
a	O
terrorist	O
.	O
the	O
agency	O
decide	O
to	O
scan	O
each	O
passenger	O
and	O
the	O
shifty	O
looking	O
man	O
sitting	O
next	O
to	O
you	O
is	O
the	O
ﬁrst	O
to	O
test	O
positive	O
.	O
what	O
are	O
the	O
chances	O
that	O
this	O
man	O
is	O
a	O
terrorist	O
?	O
exercise	O
6	O
(	O
the	O
monty	O
hall	O
problem	B
)	O
.	O
on	O
a	O
gameshow	O
there	O
are	O
three	O
doors	O
.	O
behind	O
one	O
door	O
is	O
a	O
prize	O
.	O
the	O
gameshow	O
host	O
asks	O
you	O
to	O
pick	O
a	O
door	O
.	O
he	O
then	O
opens	O
a	O
diﬀerent	O
door	O
to	O
the	O
one	O
you	O
chose	O
and	O
shows	O
that	O
there	O
is	O
no	O
prize	O
behind	O
it	O
.	O
is	O
is	O
better	O
to	O
stick	O
with	O
your	O
original	O
guess	O
as	O
to	O
where	O
the	O
prize	O
is	O
,	O
or	O
better	O
to	O
change	O
your	O
mind	O
?	O
exercise	O
7.	O
consider	O
three	O
variable	B
distributions	O
which	O
admit	O
the	O
factorisation	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
=	O
p	O
(	O
a|b	O
)	O
p	O
(	O
b|c	O
)	O
p	O
(	O
c	O
)	O
(	O
1.7.4	O
)	O
where	O
all	O
variables	O
are	O
binary	O
.	O
how	O
many	O
parameters	O
are	O
needed	O
to	O
specify	O
distributions	O
of	O
this	O
form	O
?	O
draft	O
march	O
9	O
,	O
2010	O
17	O
exercise	O
8.	O
repeat	O
the	O
inspector	O
clouseau	O
scenario	O
,	O
example	O
(	O
2	O
)	O
,	O
but	O
with	O
the	O
restriction	O
that	O
either	O
the	O
maid	O
or	O
the	O
butler	O
is	O
the	O
murderer	O
,	O
but	O
not	O
both	O
.	O
explicitly	O
,	O
the	O
probability	B
of	O
the	O
maid	O
being	O
the	O
murder	O
and	O
not	O
the	O
butler	O
is	O
0.04	O
,	O
the	O
probability	B
of	O
the	O
butler	O
being	O
the	O
murder	O
and	O
not	O
the	O
maid	O
is	O
0.64.	O
modify	O
democlouseau.m	O
to	O
implement	O
this	O
.	O
exercise	O
9.	O
prove	O
exercises	O
p	O
(	O
a	O
,	O
(	O
b	O
or	O
c	O
)	O
)	O
=	O
p	O
(	O
a	O
,	O
b	O
)	O
+	O
p	O
(	O
a	O
,	O
c	O
)	O
−	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
exercise	O
10.	O
prove	O
p	O
(	O
x|z	O
)	O
=	O
(	O
cid:88	O
)	O
p	O
(	O
x|y	O
,	O
z	O
)	O
p	O
(	O
y|z	O
)	O
=	O
(	O
cid:88	O
)	O
y	O
y	O
,	O
w	O
p	O
(	O
x|w	O
,	O
y	O
,	O
z	O
)	O
p	O
(	O
w|y	O
,	O
z	O
)	O
p	O
(	O
y|z	O
)	O
(	O
1.7.5	O
)	O
(	O
1.7.6	O
)	O
exercise	O
11.	O
as	O
a	O
young	O
man	O
mr	O
gott	O
visits	O
berlin	O
in	O
1969.	O
he	O
’	O
s	O
surprised	O
that	O
he	O
can	O
not	O
cross	B
into	O
east	O
berlin	O
since	O
there	O
is	O
a	O
wall	O
separating	O
the	O
two	O
halves	O
of	O
the	O
city	O
.	O
he	O
’	O
s	O
told	O
that	O
the	O
wall	O
was	O
erected	O
8	O
years	O
previously	O
.	O
he	O
reasons	O
that	O
:	O
the	O
wall	O
will	O
have	O
a	O
ﬁnite	O
lifespan	O
;	O
his	O
ignorance	O
means	O
that	O
he	O
arrives	O
uniformly	O
at	O
random	O
at	O
some	O
time	O
in	O
the	O
lifespan	O
of	O
the	O
wall	O
.	O
since	O
only	O
5	O
%	O
of	O
the	O
time	O
one	O
would	O
arrive	O
in	O
the	O
ﬁrst	O
or	O
last	O
2.5	O
%	O
of	O
the	O
lifespan	O
of	O
the	O
wall	O
he	O
asserts	O
that	O
with	O
95	O
%	O
conﬁdence	O
the	O
wall	O
will	O
survive	O
between	O
8/0.975	O
≈	O
8.2	O
and	O
8/0.025	O
=	O
320	O
years	O
.	O
in	O
1989	O
the	O
now	O
professor	O
gott	O
is	O
pleased	O
to	O
ﬁnd	O
that	O
his	O
prediction	B
was	O
correct	O
and	O
promotes	O
his	O
prediction	B
method	O
in	O
elite	O
journals	O
.	O
this	O
‘	O
delta-t	O
’	O
method	O
is	O
widely	O
adopted	O
and	O
used	O
to	O
form	O
predictions	O
in	O
a	O
range	O
of	O
scenarios	O
about	O
which	O
researchers	O
are	O
‘	O
totally	O
ignorant	O
’	O
.	O
would	O
you	O
‘	O
buy	O
’	O
a	O
prediction	B
from	O
prof.	O
gott	O
?	O
explain	O
carefully	O
your	O
reasoning	O
.	O
exercise	O
12.	O
implement	O
the	O
soft	B
xor	O
gate	O
,	O
example	O
(	O
6	O
)	O
using	O
brmltoolbox	O
.	O
you	O
may	O
ﬁnd	O
condpot.m	O
of	O
use	O
.	O
exercise	O
13.	O
implement	O
the	O
hamburgers	O
,	O
example	O
(	O
1	O
)	O
(	O
both	O
scenarios	O
)	O
using	O
brmltoolbox	O
.	O
to	O
do	O
so	O
you	O
will	O
need	O
to	O
deﬁne	O
the	O
joint	B
distribution	O
p	O
(	O
hamburgers	O
,	O
kj	O
)	O
in	O
which	O
dom	O
(	O
hamburgers	O
)	O
=	O
dom	O
(	O
kj	O
)	O
=	O
{	O
tr	O
,	O
fa	O
}	O
.	O
exercise	O
14.	O
implement	O
the	O
two-dice	O
example	O
,	O
section	O
(	O
1.3.1	O
)	O
using	O
brmltoolbox	O
.	O
exercise	O
15.	O
a	O
redistribution	O
lottery	O
involves	O
picking	O
the	O
correct	O
four	O
numbers	O
from	O
1	O
to	O
9	O
(	O
without	O
replacement	O
,	O
so	O
3,4,4,1	O
for	O
example	O
is	O
not	O
possible	O
)	O
.	O
the	O
order	O
of	O
the	O
picked	O
numbers	O
is	O
irrelevant	O
.	O
every	O
week	O
a	O
million	O
people	O
play	O
this	O
game	O
,	O
each	O
paying	O
£1	O
to	O
enter	O
,	O
with	O
the	O
numbers	O
3,5,7,9	O
being	O
the	O
most	O
popular	O
(	O
1	O
in	O
every	O
100	O
people	O
chooses	O
these	O
numbers	O
)	O
.	O
given	O
that	O
the	O
million	O
pounds	O
prize	O
money	B
is	O
split	O
equally	O
between	O
winners	O
,	O
and	O
that	O
any	O
four	O
(	O
diﬀerent	O
)	O
numbers	O
come	O
up	O
at	O
random	O
,	O
what	O
is	O
the	O
expected	O
amount	O
of	O
money	B
each	O
of	O
the	O
players	O
choosing	O
3,5,7,9	O
will	O
win	O
each	O
week	O
?	O
the	O
least	O
popular	O
set	O
of	O
numbers	O
is	O
1,2,3,4	O
with	O
only	O
1	O
in	O
10,000	O
people	O
choosing	O
this	O
.	O
how	O
much	O
do	O
they	O
proﬁt	O
each	O
week	O
,	O
on	O
average	B
?	O
do	O
you	O
think	O
there	O
is	O
any	O
‘	O
skill	O
’	O
involved	O
in	O
playing	O
this	O
lottery	O
?	O
exercise	O
16.	O
in	O
a	O
test	O
of	O
‘	O
psychometry	O
’	O
the	O
car	O
keys	O
and	O
wrist	O
watches	O
of	O
5	O
people	O
are	O
given	O
to	O
a	O
medium	O
.	O
the	O
medium	O
then	O
attempts	O
to	O
match	O
the	O
wrist	O
watch	O
with	O
the	O
car	O
key	O
of	O
each	O
person	O
.	O
what	O
is	O
the	O
expected	O
number	O
of	O
correct	O
matches	O
that	O
the	O
medium	O
will	O
make	O
(	O
by	O
chance	O
)	O
?	O
what	O
is	O
the	O
probability	B
that	O
the	O
medium	O
will	O
obtain	O
at	O
least	O
1	O
correct	O
match	O
?	O
18	O
draft	O
march	O
9	O
,	O
2010	O
chapter	O
2	O
basic	O
graph	B
concepts	O
2.1	O
graphs	O
deﬁnition	O
8	O
(	O
graph	B
)	O
.	O
a	O
graph	B
g	O
consists	O
of	O
vertices	O
(	O
nodes	O
)	O
and	O
edges	O
(	O
links	O
)	O
between	O
the	O
vertices	O
.	O
edges	O
may	O
be	O
directed	B
(	O
they	O
have	O
an	O
arrow	O
in	O
a	O
single	O
direction	O
)	O
or	O
undirected	B
.	O
a	O
graph	B
with	O
all	O
edges	O
directed	B
is	O
called	O
a	O
directed	B
graph	O
,	O
and	O
one	O
with	O
all	O
edges	O
undirected	B
is	O
called	O
an	O
undirected	B
graph	I
.	O
deﬁnition	O
9	O
(	O
path	B
,	O
ancestors	O
,	O
descendants	O
)	O
.	O
a	O
path	B
a	O
(	O
cid:55	O
)	O
→	O
b	O
from	O
node	B
a	O
to	O
node	B
b	O
is	O
a	O
sequence	O
of	O
vertices	O
a0	O
=	O
a	O
,	O
a1	O
,	O
.	O
.	O
.	O
,	O
an−1	O
,	O
an	O
=	O
b	O
,	O
with	O
(	O
an	O
,	O
an+1	O
)	O
an	O
edge	O
in	O
the	O
graph	B
,	O
thereby	O
connecting	O
a	O
to	O
b.	O
for	O
a	O
directed	B
graph	O
this	O
means	O
that	O
a	O
path	B
is	O
a	O
sequence	O
of	O
nodes	O
which	O
when	O
we	O
follow	O
the	O
direction	O
of	O
the	O
arrows	O
leads	O
us	O
from	O
a	O
to	O
b.	O
the	O
vertices	O
a	O
such	O
that	O
a	O
(	O
cid:55	O
)	O
→	O
b	O
and	O
b	O
(	O
cid:54	O
)	O
(	O
cid:55	O
)	O
→	O
a	O
are	O
the	O
descendants	O
of	O
a	O
[	O
168	O
]	O
.	O
and	O
b	O
(	O
cid:54	O
)	O
(	O
cid:55	O
)	O
→	O
a	O
are	O
the	O
ancestors	O
of	O
b.	O
the	O
vertices	O
b	O
such	O
that	O
a	O
(	O
cid:55	O
)	O
→	O
b	O
deﬁnition	O
10	O
(	O
directed	B
acyclic	I
graph	I
(	O
dag	O
)	O
)	O
.	O
a	O
dag	O
is	O
a	O
graph	B
g	O
with	O
directed	O
edges	O
(	O
arrows	O
on	O
each	O
link	O
)	O
between	O
the	O
vertices	O
(	O
nodes	O
)	O
such	O
that	O
by	O
following	O
a	O
path	B
of	O
vertices	O
from	O
one	O
node	B
to	O
another	O
along	O
the	O
direction	O
of	O
each	O
edge	O
no	O
path	B
will	O
revisit	O
a	O
vertex	B
.	O
in	O
a	O
dag	O
the	O
ancestors	O
of	O
b	O
are	O
those	O
nodes	O
who	O
have	O
a	O
directed	B
path	O
ending	O
at	O
b.	O
conversely	O
,	O
the	O
descendants	O
of	O
a	O
are	O
those	O
nodes	O
who	O
have	O
a	O
directed	B
path	O
starting	O
at	O
a.	O
deﬁnition	O
11	O
(	O
relationships	O
in	O
a	O
dag	O
)	O
.	O
x1	O
x8	O
x2	O
x4	O
x3	O
x7	O
x5	O
x6	O
the	O
children	B
of	O
the	O
parents	B
of	O
x4	O
are	O
pa	O
(	O
x4	O
)	O
=	O
{	O
x1	O
,	O
x2	O
,	O
x3	O
}	O
.	O
the	O
family	B
of	O
a	O
node	B
is	O
itself	O
and	O
its	O
x4	O
are	O
ch	O
(	O
x4	O
)	O
=	O
{	O
x5	O
,	O
x6	O
}	O
.	O
parents	B
.	O
the	O
markov	O
blanket	O
of	O
a	O
node	B
is	O
itself	O
,	O
its	O
parents	B
,	O
children	B
and	O
the	O
parents	B
of	O
its	O
children	B
.	O
in	O
this	O
case	O
,	O
the	O
markov	O
blanket	O
of	O
x4	O
is	O
x1	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
x7	O
.	O
19	O
c	O
a	O
f	O
b	O
g	O
d	O
(	O
a	O
)	O
e	O
c	O
b	O
g	O
e	O
a	O
f	O
d	O
(	O
b	O
)	O
graphs	O
figure	O
2.1	O
:	O
(	O
b	O
)	O
multiply-connected	B
graph	O
.	O
(	O
a	O
)	O
singly-connected	B
graph	O
.	O
deﬁnition	O
12	O
(	O
undirected	B
graph	I
)	O
.	O
a	O
d	O
b	O
c	O
e	O
an	O
undirected	B
graph	I
g	O
consists	O
of	O
undirected	B
edges	O
between	O
nodes	O
.	O
deﬁnition	O
13	O
(	O
neighbour	B
)	O
.	O
for	O
an	O
undirected	B
graph	I
g	O
the	O
neighbours	O
of	O
x	O
,	O
ne	O
(	O
x	O
)	O
are	O
those	O
nodes	O
directly	O
connected	B
to	O
x.	O
deﬁnition	O
14	O
(	O
connected	B
graph	I
)	O
.	O
an	O
undirected	B
graph	I
is	O
connected	B
if	O
there	O
is	O
a	O
path	B
between	O
every	O
set	O
of	O
vertices	O
(	O
i.e	O
.	O
there	O
are	O
no	O
isolated	O
islands	O
)	O
.	O
for	O
a	O
graph	B
which	O
is	O
not	O
connected	B
,	O
the	O
connected	B
components	I
are	O
those	O
subgraphs	O
which	O
are	O
connected	B
.	O
deﬁnition	O
15	O
(	O
clique	B
)	O
.	O
a	O
d	O
b	O
c	O
e	O
given	O
an	O
undirected	B
graph	I
,	O
a	O
clique	B
is	O
a	O
maximally	O
connected	B
subset	O
of	O
vertices	O
.	O
all	O
the	O
members	O
of	O
the	O
clique	B
are	O
connected	B
to	O
each	O
other	O
;	O
further-	O
more	O
there	O
is	O
no	O
larger	O
clique	B
that	O
can	O
be	O
made	O
from	O
a	O
clique	B
.	O
for	O
example	O
this	O
graph	B
has	O
two	O
cliques	O
,	O
c1	O
=	O
{	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
}	O
and	O
c2	O
=	O
{	O
b	O
,	O
c	O
,	O
e	O
}	O
.	O
whilst	O
a	O
,	O
b	O
,	O
c	O
are	O
fully	O
connected	B
,	O
this	O
is	O
a	O
non-maximal	O
clique	B
since	O
there	O
is	O
a	O
larger	O
fully	O
connected	B
set	O
,	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
that	O
contains	O
this	O
.	O
a	O
non-maximal	O
clique	B
is	O
sometimes	O
called	O
a	O
cliquo	B
.	O
deﬁnition	O
16	O
(	O
singly-connected	B
graph	O
)	O
.	O
a	O
graph	B
is	O
singly-connected	B
if	O
there	O
is	O
only	O
one	O
path	B
from	O
a	O
vertex	B
a	O
to	O
another	O
vertex	B
b.	O
otherwise	O
the	O
graph	B
is	O
multiply-connected	B
.	O
this	O
deﬁnition	O
applies	O
regardless	O
of	O
whether	O
or	O
not	O
the	O
edges	O
in	O
the	O
graph	B
are	O
directed	B
.	O
an	O
alternative	O
name	O
for	O
a	O
singly-connected	B
graph	O
is	O
a	O
tree	B
.	O
a	O
multiply-connected	B
graph	O
is	O
also	O
called	O
loopy	B
.	O
20	O
draft	O
march	O
9	O
,	O
2010	O
numerically	O
encoding	O
graphs	O
2.1.1	O
spanning	B
tree	I
deﬁnition	O
17	O
(	O
spanning	B
tree	I
)	O
.	O
a	O
spanning	B
tree	I
of	O
an	O
undirected	B
graph	I
g	O
is	O
a	O
singly-connected	B
subset	O
of	O
the	O
existing	O
edges	O
such	O
that	O
the	O
resulting	O
singly-	O
connected	B
graph	I
covers	O
all	O
vertices	O
of	O
g.	O
on	O
the	O
right	O
is	O
a	O
graph	B
and	O
an	O
associated	O
spanning	B
tree	I
.	O
a	O
maximum	O
weight	O
spanning	B
tree	I
is	O
a	O
spanning	B
tree	I
such	O
that	O
the	O
sum	O
of	O
all	O
weights	O
on	O
the	O
edges	O
of	O
the	O
tree	B
is	O
larger	O
than	O
for	O
any	O
other	O
spanning	B
tree	I
of	O
g.	O
finding	O
the	O
maximal	O
weight	B
spanning	O
tree	B
a	O
simple	O
algorithm	O
to	O
ﬁnd	O
a	O
spanning	B
tree	I
with	O
maximal	O
weight	B
is	O
as	O
follows	O
:	O
start	O
by	O
picking	O
the	O
edge	O
with	O
the	O
largest	O
weight	B
and	O
add	O
this	O
to	O
the	O
edge	O
set	O
.	O
then	O
pick	O
the	O
next	O
candidate	O
edge	O
which	O
has	O
the	O
largest	O
weight	B
and	O
add	O
this	O
to	O
the	O
edge	O
set	O
–	O
if	O
this	O
results	O
in	O
an	O
edge	O
set	O
with	O
cycles	O
,	O
then	O
reject	O
the	O
candidate	O
edge	O
and	O
ﬁnd	O
the	O
next	O
largest	O
edge	O
weight	O
.	O
note	O
that	O
there	O
may	O
be	O
more	O
than	O
one	O
maximal	O
weight	B
spanning	O
tree	B
.	O
2.2	O
numerically	O
encoding	O
graphs	O
to	O
express	O
the	O
structure	B
of	O
gms	O
we	O
need	O
to	O
numerically	O
encode	O
the	O
links	O
on	O
the	O
graphs	O
.	O
for	O
a	O
graph	B
of	O
n	O
vertices	O
,	O
we	O
can	O
describe	O
the	O
graph	B
structure	O
in	O
various	O
equivalent	B
ways	O
.	O
2.2.1	O
edge	B
list	I
as	O
the	O
name	O
suggests	O
,	O
an	O
edge	B
list	I
simply	O
lists	O
which	O
vertex-vertex	O
pairs	O
are	O
in	O
the	O
graph	B
.	O
for	O
ﬁg	O
(	O
2.2a	O
)	O
,	O
an	O
edge	B
list	I
is	O
l	O
=	O
{	O
(	O
1	O
,	O
2	O
)	O
,	O
(	O
2	O
,	O
1	O
)	O
,	O
(	O
1	O
,	O
3	O
)	O
,	O
(	O
3	O
,	O
1	O
)	O
,	O
(	O
2	O
,	O
3	O
)	O
,	O
(	O
3	O
,	O
2	O
)	O
,	O
(	O
2	O
,	O
4	O
)	O
,	O
(	O
4	O
,	O
2	O
)	O
,	O
(	O
3	O
,	O
4	O
)	O
,	O
(	O
4	O
,	O
3	O
)	O
}	O
where	O
an	O
undirected	B
edge	O
is	O
represented	O
by	O
a	O
bidirectional	O
edge	O
.	O
2.2.2	O
adjacency	B
matrix	I
an	O
alternative	O
is	O
to	O
use	O
an	O
adjacency	B
matrix	I
	O
0	O
1	O
1	O
0	O
1	O
0	O
1	O
1	O
1	O
1	O
0	O
1	O
0	O
1	O
1	O
0	O
	O
a	O
=	O
(	O
2.2.1	O
)	O
where	O
aij	O
=	O
1	O
if	O
there	O
is	O
an	O
edge	O
from	O
variable	B
i	O
to	O
j	O
in	O
the	O
graph	B
,	O
and	O
0	O
otherwise	O
.	O
some	O
authors	O
include	O
self-connections	O
in	O
this	O
deﬁnition	O
.	O
an	O
undirected	B
graph	I
has	O
a	O
symmetric	O
adjacency	B
matrix	I
.	O
provided	O
that	O
the	O
vertices	O
are	O
labelled	B
in	O
ancestral	B
order	I
(	O
parents	B
always	O
come	O
before	O
children	B
)	O
a	O
directed	B
graph	O
ﬁg	O
(	O
2.2b	O
)	O
can	O
be	O
represented	O
as	O
a	O
triangular	O
adjacency	B
matrix	I
:	O
(	O
2.2.2	O
)	O
21	O
	O
0	O
1	O
1	O
0	O
0	O
0	O
1	O
1	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
	O
t	O
=	O
draft	O
march	O
9	O
,	O
2010	O
numerically	O
encoding	O
graphs	O
1	O
2	O
3	O
(	O
a	O
)	O
4	O
1	O
2	O
3	O
(	O
b	O
)	O
4	O
figure	O
2.2	O
:	O
(	O
a	O
)	O
:	O
an	O
undirected	B
graph	I
can	O
be	O
represented	O
as	O
a	O
symmetric	O
adjacency	B
matrix	I
.	O
(	O
b	O
)	O
:	O
a	O
directed	B
graph	O
with	O
vertices	O
labelled	B
in	O
ancestral	B
order	I
corresponds	O
to	O
a	O
triangular	O
adjacency	B
matrix	I
.	O
adjacency	B
matrix	I
powers	O
for	O
an	O
n	O
×	O
n	O
adjacency	B
matrix	I
a	O
,	O
powers	O
of	O
the	O
adjacency	B
matrix	I
(	O
cid:2	O
)	O
ak	O
(	O
cid:3	O
)	O
if	O
we	O
include	O
1	O
’	O
s	O
on	O
the	O
diagonal	O
of	O
a	O
then	O
(	O
cid:2	O
)	O
an	O
(	O
cid:3	O
)	O
in	O
the	O
graph	B
.	O
if	O
a	O
corresponds	O
to	O
a	O
dag	O
the	O
non-zero	O
entries	O
of	O
the	O
jth	O
row	O
of	O
(	O
cid:2	O
)	O
an	O
(	O
cid:3	O
)	O
correspond	O
to	O
the	O
ij	O
is	O
non-zero	O
when	O
there	O
is	O
a	O
path	B
connecting	O
j	O
to	O
i	O
are	O
from	O
node	B
i	O
to	O
node	B
j	O
in	O
k	O
edge	O
hops	O
.	O
ij	O
specify	O
how	O
many	O
paths	O
there	O
descendants	O
of	O
node	B
j	O
.	O
2.2.3	O
clique	B
matrix	I
for	O
an	O
undirected	B
graph	I
with	O
n	O
vertices	O
and	O
maximal	O
cliques	O
c1	O
,	O
.	O
.	O
.	O
,	O
ck	O
a	O
clique	B
matrix	I
is	O
an	O
n	O
×	O
k	O
matrix	B
in	O
which	O
each	O
column	O
ck	O
has	O
zeros	O
expect	O
for	O
ones	O
on	O
entries	O
describing	O
the	O
clique	B
.	O
a	O
cliquo	B
matrix	O
relaxes	O
the	O
constraint	O
that	O
cliques	O
are	O
required	O
to	O
be	O
maximal1	O
.	O
for	O
example	O
1	O
1	O
1	O
1	O
0	O
1	O
	O
	O
1	O
0	O
	O
1	O
1	O
0	O
0	O
0	O
1	O
0	O
1	O
1	O
0	O
0	O
1	O
1	O
0	O
1	O
0	O
0	O
0	O
1	O
1	O
	O
c	O
=	O
cinc	O
=	O
(	O
2.2.3	O
)	O
(	O
2.2.4	O
)	O
is	O
a	O
clique	B
matrix	I
for	O
ﬁg	O
(	O
2.2a	O
)	O
.	O
a	O
cliquo	B
matrix	O
containing	O
only	O
two-dimensional	O
maximal	O
cliques	O
is	O
called	O
an	O
incidence	B
matrix	I
.	O
for	O
example	O
is	O
an	O
incidence	B
matrix	I
for	O
ﬁg	O
(	O
2.2b	O
)	O
.	O
it	O
is	O
straightforward	O
to	O
show	O
that	O
cincct	O
inc	O
is	O
equal	O
to	O
the	O
adjacency	B
matrix	I
except	O
that	O
the	O
diagonals	O
now	O
contain	O
the	O
degree	O
of	O
each	O
vertex	B
(	O
the	O
number	O
of	O
edges	O
it	O
touches	O
)	O
.	O
similarly	O
,	O
for	O
any	O
cliquo	B
matrix	O
the	O
diagonal	O
entry	O
of	O
[	O
cct	O
]	O
ii	O
expresses	O
the	O
number	O
of	O
cliquos	O
(	O
columns	O
)	O
that	O
vertex	B
i	O
occurs	O
in	O
.	O
oﬀ	O
diagonal	O
elements	O
[	O
cct	O
]	O
ij	O
contain	O
the	O
number	O
of	O
cliquos	O
that	O
vertices	O
i	O
and	O
j	O
jointly	O
inhabit	O
.	O
remark	O
2	O
(	O
graph	B
confusions	O
)	O
.	O
graphs	O
are	O
widely	O
used	O
,	O
but	O
diﬀer	O
markedly	O
in	O
what	O
they	O
represent	O
.	O
two	O
potential	B
pitfalls	O
are	O
described	O
below	O
.	O
state	O
transition	O
diagrams	O
such	O
graphical	O
representations	O
are	O
common	O
in	O
markov	O
chains	O
and	O
fi-	O
nite	O
state	O
automata	O
.	O
a	O
set	O
of	O
states	O
is	O
written	O
as	O
set	O
of	O
nodes	O
(	O
vertices	O
)	O
of	O
a	O
graph	B
,	O
and	O
a	O
directed	B
edge	O
between	O
node	B
i	O
and	O
node	B
j	O
(	O
with	O
an	O
associated	O
weight	B
pij	O
)	O
represents	O
that	O
a	O
transition	O
from	O
state	O
i	O
to	O
state	O
j	O
can	O
occur	O
with	O
probability	O
pij	O
.	O
from	O
the	O
graphical	O
models	O
perspective	O
we	O
would	O
simply	O
write	O
down	O
a	O
directed	B
graph	O
x	O
(	O
t	O
)	O
→	O
x	O
(	O
t	O
+	O
1	O
)	O
to	O
represent	O
this	O
markov	O
chain	B
.	O
the	O
state-transition	O
diagram	O
simply	O
provides	O
a	O
graphical	O
description	O
of	O
the	O
conditional	B
probability	I
table	O
p	O
(	O
x	O
(	O
t	O
+	O
1	O
)	O
|x	O
(	O
t	O
)	O
)	O
.	O
1the	O
term	O
‘	O
cliquo	B
’	O
for	O
a	O
non-maximal	O
clique	B
is	O
attributed	O
to	O
julian	O
besag	O
.	O
22	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
neural	O
networks	O
neural	O
networks	O
also	O
have	O
vertices	O
and	O
edges	O
.	O
in	O
general	O
,	O
however	O
,	O
neural	O
net-	O
works	O
are	O
graphical	O
representations	O
of	O
functions	O
,	O
whereas	O
as	O
graphical	O
models	O
are	O
representations	O
of	O
distributions	O
(	O
a	O
richer	O
formalism	O
)	O
.	O
neural	O
networks	O
(	O
or	O
any	O
other	O
parametric	O
description	O
)	O
may	O
be	O
used	O
to	O
represent	O
the	O
conditional	B
probability	I
tables	O
,	O
as	O
in	O
sigmoid	O
belief	O
networks	O
[	O
204	O
]	O
.	O
2.3	O
code	O
2.3.1	O
utility	B
routines	O
ancestors.m	O
:	O
find	O
the	O
ancestors	O
of	O
a	O
node	B
in	O
a	O
dag	O
edges.m	O
:	O
edge	B
list	I
from	O
an	O
adjacency	B
matrix	I
ancestralorder.m	O
:	O
ancestral	B
order	I
from	O
a	O
dag	O
connectedcomponents.m	O
:	O
connected	B
components	I
parents.m	O
:	O
parents	B
of	O
a	O
node	B
given	O
an	O
adjacency	B
matrix	I
children.m	O
:	O
children	B
of	O
a	O
node	B
given	O
an	O
adjacency	B
matrix	I
neigh.m	O
:	O
neighbours	O
of	O
a	O
node	B
given	O
an	O
adjacency	B
matrix	I
a	O
connected	B
graph	I
is	O
a	O
tree	B
if	O
the	O
number	O
of	O
edges	O
plus	O
1	O
is	O
equal	O
to	O
the	O
number	O
of	O
nodes	O
.	O
however	O
,	O
for	O
a	O
possibly	O
disconnected	B
graph	O
this	O
is	O
not	O
the	O
case	O
.	O
the	O
code	O
below	O
deals	O
with	O
the	O
possibly	O
disconnected	B
case	O
.	O
the	O
routine	O
is	O
based	O
on	O
the	O
observation	O
that	O
any	O
singly-connected	B
graph	O
must	O
always	O
possess	O
a	O
simplical	B
node	O
(	O
a	O
leaf	O
node	B
)	O
which	O
can	O
be	O
eliminated	O
to	O
reveal	O
a	O
smaller	O
singly-connected	B
graph	O
.	O
istree.m	O
:	O
if	O
graph	B
is	O
singly	O
connected	B
return	O
1	O
and	O
elimination	O
sequence	O
spantree.m	O
:	O
return	O
a	O
spanning	B
tree	I
from	O
an	O
ordered	O
edge	B
list	I
singleparenttree.m	O
:	O
find	O
a	O
directed	B
tree	O
with	O
at	O
most	O
one	O
parent	O
from	O
an	O
undirected	B
tree	O
additional	O
routines	O
for	O
basic	O
manipulations	O
in	O
graphs	O
are	O
given	O
at	O
the	O
end	O
of	O
chapter	O
(	O
6	O
)	O
.	O
2.4	O
exercises	O
j	O
in	O
one	O
timestep	O
,	O
and	O
0	O
otherwise	O
.	O
show	O
that	O
the	O
matrix	B
(	O
cid:2	O
)	O
ak	O
(	O
cid:3	O
)	O
exercise	O
17.	O
consider	O
an	O
adjacency	B
matrix	I
a	O
with	O
elements	O
[	O
a	O
]	O
ij	O
=	O
1	O
if	O
one	O
can	O
reach	O
state	O
i	O
from	O
state	O
ij	O
represents	O
the	O
number	O
of	O
paths	O
that	O
lead	O
from	O
state	O
j	O
to	O
i	O
in	O
k	O
timesteps	O
.	O
hence	O
derive	O
an	O
algorithm	B
that	O
will	O
ﬁnd	O
the	O
minimum	O
number	O
of	O
steps	O
to	O
get	O
from	O
state	O
j	O
to	O
state	O
i.	O
exercise	O
18.	O
for	O
an	O
n	O
×	O
n	O
symmetric	O
adjacency	B
matrix	I
a	O
,	O
describe	O
an	O
algorithm	B
to	O
ﬁnd	O
the	O
connected	B
components	I
.	O
you	O
may	O
wish	O
to	O
examine	O
connectedcomponents.m	O
.	O
exercise	O
19.	O
show	O
that	O
for	O
a	O
connected	B
graph	I
that	O
is	O
singly-connected	B
,	O
the	O
number	O
of	O
edges	O
e	O
must	O
be	O
equal	O
to	O
the	O
number	O
of	O
vertices	O
minus	O
1	O
,	O
e	O
=	O
v	O
−	O
1.	O
give	O
an	O
example	O
graph	B
with	O
e	O
=	O
v	O
−	O
1	O
that	O
is	O
not	O
singly-connected	B
.	O
draft	O
march	O
9	O
,	O
2010	O
23	O
exercises	O
24	O
draft	O
march	O
9	O
,	O
2010	O
chapter	O
3	O
belief	B
networks	I
3.1	O
probabilistic	B
inference	O
in	O
structured	B
distributions	O
consider	O
an	O
environment	O
composed	O
of	O
n	O
variables	O
,	O
with	O
a	O
corresponding	O
distribution	B
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
.	O
writ-	O
ing	O
e	O
as	O
the	O
set	O
of	O
evidential	O
variables	O
and	O
using	O
evidence	B
=	O
{	O
xe	O
=	O
xe	O
,	O
e	O
∈	O
e	O
}	O
to	O
denote	O
all	O
available	O
evidence	B
,	O
then	O
inference	B
and	O
reasoning	O
can	O
be	O
carried	O
out	O
automatically	O
by	O
the	O
‘	O
brute	O
force	O
’	O
method1	O
(	O
cid:80	O
)	O
x	O
{	O
\e	O
,	O
\i	O
}	O
p	O
(	O
{	O
xe	O
=	O
xe	O
,	O
e	O
∈	O
e	O
}	O
,	O
xi	O
=	O
xi	O
,	O
x	O
{	O
\e	O
,	O
\i	O
}	O
)	O
(	O
cid:80	O
)	O
x\e	O
p	O
(	O
{	O
xe	O
=	O
xe	O
,	O
e	O
∈	O
e	O
}	O
,	O
x\e	O
)	O
p	O
(	O
xi	O
=	O
xi|evidence	O
)	O
=	O
(	O
3.1.1	O
)	O
if	O
all	O
variables	O
are	O
binary	O
(	O
take	O
two	O
states	O
)	O
,	O
these	O
summations	O
require	O
o	O
(	O
2n−|e|	O
)	O
operations	O
.	O
such	O
expo-	O
nential	O
computation	O
is	O
impractical	O
and	O
techniques	O
that	O
reduce	O
this	O
burden	O
by	O
exploiting	O
any	O
structure	B
in	O
the	O
joint	B
probability	O
table	O
are	O
the	O
topic	O
of	O
our	O
discussions	O
on	O
eﬃcient	B
inference	O
.	O
naively	O
specifying	O
all	O
the	O
entries	O
of	O
a	O
table	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
over	O
binary	O
variables	O
xi	O
takes	O
o	O
(	O
2n	O
)	O
space	O
.	O
we	O
will	O
need	O
to	O
deal	O
with	O
large	O
numbers	O
of	O
variables	O
in	O
machine	O
learning	B
and	O
related	O
application	O
areas	O
,	O
with	O
distributions	O
on	O
potentially	O
hundreds	O
if	O
not	O
millions	O
of	O
variables	O
.	O
the	O
only	O
way	O
to	O
deal	O
with	O
such	O
large	O
distributions	O
is	O
to	O
constrain	O
the	O
nature	O
of	O
the	O
variable	B
interactions	O
in	O
some	O
manner	O
,	O
both	O
to	O
render	O
speciﬁcation	O
and	O
ultimately	O
inference	B
in	O
such	O
systems	O
tractable	O
.	O
the	O
key	O
idea	O
is	O
to	O
specify	O
which	O
variables	O
are	O
independent	O
of	O
others	O
,	O
leading	O
to	O
a	O
structured	B
factorisation	O
of	O
the	O
joint	B
probability	O
distribution	B
.	O
belief	B
networks	I
are	O
a	O
convenient	O
framework	O
for	O
representing	O
such	O
factorisations	O
into	O
local	B
conditional	O
distribu-	O
tions	O
.	O
we	O
will	O
discuss	O
belief	B
networks	I
more	O
formally	O
in	O
section	O
(	O
3.3	O
)	O
,	O
ﬁrst	O
discussing	O
their	O
natural	B
graphical	O
representations	O
of	O
distributions	O
.	O
d	O
(	O
cid:89	O
)	O
i=1	O
deﬁnition	O
18	O
(	O
belief	B
network	I
)	O
.	O
a	O
belief	B
network	I
is	O
a	O
distribution	B
of	O
the	O
form	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
)	O
=	O
p	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
(	O
3.1.2	O
)	O
where	O
pa	O
(	O
xi	O
)	O
represent	O
the	O
parental	O
variables	O
of	O
variable	B
xi	O
.	O
written	O
as	O
a	O
directed	B
graph	O
,	O
with	O
an	O
arrow	O
pointing	O
from	O
a	O
parent	O
variable	B
to	O
child	O
variable	B
,	O
a	O
belief	B
network	I
is	O
a	O
directed	B
acyclic	I
graph	I
(	O
dag	O
)	O
,	O
with	O
the	O
ith	O
vertex	B
in	O
the	O
graph	B
corresponding	O
to	O
the	O
factor	B
p	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
.	O
1the	O
extension	O
to	O
continuous	B
variables	O
is	O
straightforward	O
,	O
replacing	O
summation	O
with	O
integration	O
over	O
pdfs	O
;	O
we	O
defer	O
treatment	O
of	O
this	O
to	O
later	O
chapters	O
,	O
since	O
our	O
aim	O
is	O
to	O
here	O
outline	O
more	O
the	O
intuitions	O
without	O
needing	O
to	O
deal	O
with	O
integration	O
of	O
high	O
dimensional	O
distributions	O
.	O
25	O
graphically	O
representing	O
distributions	O
3.2	O
graphically	O
representing	O
distributions	O
belief	B
networks	I
(	O
also	O
called	O
bayes	O
’	O
networks	O
or	O
bayesian	O
belief	B
networks	I
)	O
are	O
a	O
way	O
to	O
depict	O
the	O
inde-	O
pendence	O
assumptions	O
made	O
in	O
a	O
distribution	B
[	O
148	O
,	O
168	O
]	O
.	O
their	O
application	O
domain	B
is	O
widespread	O
,	O
ranging	O
from	O
troubleshooting	O
[	O
50	O
]	O
and	O
expert	O
reasoning	O
under	O
uncertainty	B
to	O
machine	O
learning	B
.	O
before	O
we	O
more	O
formally	O
deﬁne	O
a	O
bn	O
,	O
an	O
example	O
will	O
help	O
motivate	O
the	O
development2	O
.	O
3.2.1	O
constructing	O
a	O
simple	O
belief	O
network	O
:	O
wet	O
grass	O
one	O
morning	O
tracey	O
leaves	O
her	O
house	O
and	O
realises	O
that	O
her	O
grass	O
is	O
wet	O
.	O
is	O
it	O
due	O
to	O
overnight	O
rain	O
or	O
did	O
she	O
forget	O
to	O
turn	O
oﬀ	O
the	O
sprinkler	O
last	O
night	O
?	O
next	O
she	O
notices	O
that	O
the	O
grass	O
of	O
her	O
neighbour	B
,	O
jack	O
,	O
is	O
also	O
wet	O
.	O
this	O
explains	O
away	O
to	O
some	O
extent	O
the	O
possibility	O
that	O
her	O
sprinkler	O
was	O
left	O
on	O
,	O
and	O
she	O
concludes	O
therefore	O
that	O
it	O
has	O
probably	O
been	O
raining	O
.	O
making	O
a	O
model	B
we	O
can	O
model	B
the	O
above	O
situation	O
using	O
probability	B
by	O
following	O
a	O
general	O
modelling	B
approach	O
.	O
first	O
we	O
deﬁne	O
the	O
variables	O
we	O
wish	O
to	O
include	O
in	O
our	O
model	B
.	O
in	O
the	O
above	O
situation	O
,	O
the	O
natural	B
variables	O
are	O
r	O
∈	O
{	O
0	O
,	O
1	O
}	O
(	O
r	O
=	O
1	O
means	O
that	O
it	O
has	O
been	O
raining	O
,	O
and	O
0	O
otherwise	O
)	O
.	O
s	O
∈	O
{	O
0	O
,	O
1	O
}	O
(	O
s	O
=	O
1	O
means	O
that	O
tracey	O
has	O
forgotten	O
to	O
turn	O
oﬀ	O
the	O
sprinkler	O
,	O
and	O
0	O
otherwise	O
)	O
.	O
j	O
∈	O
{	O
0	O
,	O
1	O
}	O
(	O
j	O
=	O
1	O
means	O
that	O
jack	O
’	O
s	O
grass	O
is	O
wet	O
,	O
and	O
0	O
otherwise	O
)	O
.	O
t	O
∈	O
{	O
0	O
,	O
1	O
}	O
(	O
t	O
=	O
1	O
means	O
that	O
tracey	O
’	O
s	O
grass	O
is	O
wet	O
,	O
and	O
0	O
otherwise	O
)	O
.	O
a	O
model	B
of	O
tracey	O
’	O
s	O
world	O
then	O
corresponds	O
to	O
a	O
probability	B
distribution	O
on	O
the	O
joint	B
set	O
of	O
the	O
variables	O
of	O
interest	O
p	O
(	O
t	O
,	O
j	O
,	O
r	O
,	O
s	O
)	O
(	O
the	O
order	O
of	O
the	O
variables	O
is	O
irrelevant	O
)	O
.	O
since	O
each	O
of	O
the	O
variables	O
in	O
this	O
example	O
can	O
take	O
one	O
of	O
two	O
states	O
,	O
it	O
would	O
appear	O
that	O
we	O
naively	O
have	O
to	O
specify	O
the	O
values	O
for	O
each	O
of	O
the	O
24	O
=	O
16	O
states	O
,	O
e.g	O
.	O
p	O
(	O
t	O
=	O
1	O
,	O
j	O
=	O
0	O
,	O
r	O
=	O
1	O
,	O
s	O
=	O
1	O
)	O
=	O
0.7	O
etc	O
.	O
however	O
,	O
since	O
there	O
are	O
normalisation	B
conditions	O
for	O
probabilities	O
,	O
we	O
do	O
not	O
need	O
to	O
specify	O
all	O
the	O
state	O
probabilities	O
.	O
to	O
see	O
how	O
many	O
states	O
need	O
to	O
be	O
speciﬁed	O
,	O
consider	O
the	O
following	O
decomposition	B
.	O
without	O
loss	O
of	O
generality	O
(	O
wlog	O
)	O
and	O
repeatedly	O
using	O
bayes	O
’	O
rule	O
,	O
we	O
may	O
write	O
p	O
(	O
t	O
,	O
j	O
,	O
r	O
,	O
s	O
)	O
=	O
p	O
(	O
t|j	O
,	O
r	O
,	O
s	O
)	O
p	O
(	O
j	O
,	O
r	O
,	O
s	O
)	O
=	O
p	O
(	O
t|j	O
,	O
r	O
,	O
s	O
)	O
p	O
(	O
j|r	O
,	O
s	O
)	O
p	O
(	O
r	O
,	O
s	O
)	O
=	O
p	O
(	O
t|j	O
,	O
r	O
,	O
s	O
)	O
p	O
(	O
j|r	O
,	O
s	O
)	O
p	O
(	O
r|s	O
)	O
p	O
(	O
s	O
)	O
(	O
3.2.1	O
)	O
(	O
3.2.2	O
)	O
(	O
3.2.3	O
)	O
that	O
is	O
,	O
we	O
may	O
write	O
the	O
joint	B
distribution	O
as	O
a	O
product	O
of	O
conditional	B
distributions	O
.	O
the	O
ﬁrst	O
term	O
p	O
(	O
t|j	O
,	O
r	O
,	O
s	O
)	O
requires	O
us	O
to	O
specify	O
23	O
=	O
8	O
values	O
–	O
we	O
need	O
p	O
(	O
t	O
=	O
1|j	O
,	O
r	O
,	O
s	O
)	O
for	O
the	O
8	O
joint	B
states	O
of	O
j	O
,	O
r	O
,	O
s.	O
the	O
other	O
value	B
p	O
(	O
t	O
=	O
0|j	O
,	O
r	O
,	O
s	O
)	O
is	O
given	O
by	O
normalisation	B
:	O
p	O
(	O
t	O
=	O
0|j	O
,	O
r	O
,	O
s	O
)	O
=	O
1	O
−	O
p	O
(	O
t	O
=	O
1|j	O
,	O
r	O
,	O
s	O
)	O
.	O
similarly	O
,	O
we	O
need	O
4	O
+	O
2	O
+	O
1	O
values	O
for	O
the	O
other	O
factors	O
,	O
making	O
a	O
total	O
of	O
15	O
values	O
in	O
all	O
.	O
in	O
general	O
,	O
for	O
a	O
distribution	B
on	O
n	O
binary	O
variables	O
,	O
we	O
need	O
to	O
specify	O
2n	O
−	O
1	O
values	O
in	O
the	O
range	O
[	O
0	O
,	O
1	O
]	O
.	O
the	O
important	O
point	O
here	O
is	O
that	O
the	O
number	O
of	O
values	O
that	O
need	O
to	O
be	O
speciﬁed	O
in	O
general	O
scales	O
exponentially	O
with	O
the	O
number	O
of	O
variables	O
in	O
the	O
model	B
–	O
this	O
is	O
impractical	O
in	O
general	O
and	O
motivates	O
simpliﬁcations	O
.	O
conditional	B
independence	O
the	O
modeler	O
often	O
knows	O
constraints	O
on	O
the	O
system	B
.	O
for	O
example	O
,	O
in	O
the	O
scenario	O
above	O
,	O
we	O
may	O
assume	O
that	O
tracey	O
’	O
s	O
grass	O
is	O
wet	O
depends	O
only	O
directly	O
on	O
whether	O
or	O
not	O
is	O
has	O
been	O
raining	O
and	O
whether	O
or	O
not	O
her	O
sprinkler	O
was	O
on	O
.	O
that	O
is	O
,	O
we	O
make	O
a	O
conditional	B
independence	O
assumption	O
p	O
(	O
t|j	O
,	O
r	O
,	O
s	O
)	O
=	O
p	O
(	O
t|r	O
,	O
s	O
)	O
2the	O
scenario	O
is	O
adapted	O
from	O
[	O
219	O
]	O
.	O
26	O
(	O
3.2.4	O
)	O
draft	O
march	O
9	O
,	O
2010	O
graphically	O
representing	O
distributions	O
r	O
s	O
b	O
e	O
j	O
t	O
(	O
a	O
)	O
r	O
a	O
(	O
b	O
)	O
figure	O
3.1	O
:	O
(	O
a	O
)	O
:	O
belief	B
network	I
structure	O
for	O
the	O
‘	O
wet	O
grass	O
’	O
example	O
.	O
each	O
node	B
in	O
the	O
graph	B
represents	O
a	O
variable	B
in	O
the	O
joint	B
distribution	O
,	O
and	O
the	O
variables	O
which	O
feed	O
in	O
(	O
the	O
parents	B
)	O
to	O
another	O
variable	B
represent	O
which	O
variables	O
are	O
(	O
b	O
)	O
:	O
bn	O
to	O
the	O
right	O
of	O
the	O
conditioning	B
bar	O
.	O
for	O
the	O
burglar	O
model	B
.	O
similarly	O
,	O
since	O
whether	O
or	O
not	O
jack	O
’	O
s	O
grass	O
is	O
wet	O
is	O
inﬂuenced	O
only	O
directly	O
by	O
whether	O
or	O
not	O
it	O
has	O
been	O
raining	O
,	O
we	O
write	O
p	O
(	O
j|r	O
,	O
s	O
)	O
=	O
p	O
(	O
j|r	O
)	O
and	O
since	O
the	O
rain	O
is	O
not	O
directly	O
inﬂuenced	O
by	O
the	O
sprinkler	O
,	O
p	O
(	O
r|s	O
)	O
=	O
p	O
(	O
r	O
)	O
which	O
means	O
that	O
our	O
model	B
now	O
becomes	O
:	O
p	O
(	O
t	O
,	O
j	O
,	O
r	O
,	O
s	O
)	O
=	O
p	O
(	O
t|r	O
,	O
s	O
)	O
p	O
(	O
j|r	O
)	O
p	O
(	O
r	O
)	O
p	O
(	O
s	O
)	O
(	O
3.2.5	O
)	O
(	O
3.2.6	O
)	O
(	O
3.2.7	O
)	O
we	O
can	O
represent	O
these	O
conditional	B
independencies	O
graphically	O
,	O
as	O
in	O
ﬁg	O
(	O
3.1a	O
)	O
.	O
this	O
reduces	O
the	O
number	O
of	O
values	O
that	O
we	O
need	O
to	O
specify	O
to	O
4	O
+	O
2	O
+	O
1	O
+	O
1	O
=	O
8	O
,	O
a	O
saving	O
over	O
the	O
previous	O
15	O
values	O
in	O
the	O
case	O
where	O
no	O
conditional	O
independencies	O
had	O
been	O
assumed	O
.	O
to	O
complete	O
the	O
model	B
,	O
we	O
need	O
to	O
numerically	O
specify	O
the	O
values	O
of	O
each	O
conditional	B
probability	I
table	O
(	O
cpt	O
)	O
.	O
let	O
the	O
prior	B
probabilities	O
for	O
r	O
and	O
s	O
be	O
p	O
(	O
r	O
=	O
1	O
)	O
=	O
0.2	O
and	O
p	O
(	O
s	O
=	O
1	O
)	O
=	O
0.1.	O
we	O
set	O
the	O
remaining	O
probabilities	O
to	O
p	O
(	O
j	O
=	O
1|r	O
=	O
1	O
)	O
=	O
1	O
,	O
p	O
(	O
j	O
=	O
1|r	O
=	O
0	O
)	O
=	O
0.2	O
(	O
sometimes	O
jack	O
’	O
s	O
grass	O
is	O
wet	O
due	O
to	O
unknown	O
eﬀects	O
,	O
other	O
than	O
rain	O
)	O
,	O
p	O
(	O
t	O
=	O
1|r	O
=	O
1	O
,	O
s	O
)	O
=	O
1	O
,	O
p	O
(	O
t	O
=	O
1|r	O
=	O
0	O
,	O
s	O
=	O
1	O
)	O
=	O
0.9	O
(	O
there	O
’	O
s	O
a	O
small	O
chance	O
that	O
even	O
though	O
the	O
sprinkler	O
was	O
left	O
on	O
,	O
it	O
didn	O
’	O
t	O
wet	O
the	O
grass	O
noticeably	O
)	O
,	O
p	O
(	O
t	O
=	O
1|r	O
=	O
0	O
,	O
s	O
=	O
0	O
)	O
=	O
0.	O
inference	B
now	O
that	O
we	O
’	O
ve	O
made	O
a	O
model	B
of	O
an	O
environment	O
,	O
we	O
can	O
perform	O
inference	B
.	O
let	O
’	O
s	O
calculate	O
the	O
probability	B
that	O
the	O
sprinkler	O
was	O
on	O
overnight	O
,	O
given	O
that	O
tracey	O
’	O
s	O
grass	O
is	O
wet	O
:	O
p	O
(	O
s	O
=	O
1|t	O
=	O
1	O
)	O
.	O
to	O
do	O
this	O
,	O
we	O
use	O
bayes	O
’	O
rule	O
:	O
p	O
(	O
s	O
=	O
1|t	O
=	O
1	O
)	O
=	O
p	O
(	O
s	O
=	O
1	O
,	O
t	O
=	O
1	O
)	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
=	O
j	O
,	O
r	O
p	O
(	O
t	O
=	O
1	O
,	O
j	O
,	O
r	O
,	O
s	O
=	O
1	O
)	O
(	O
cid:80	O
)	O
j	O
,	O
r	O
,	O
s	O
p	O
(	O
t	O
=	O
1	O
,	O
j	O
,	O
r	O
,	O
s	O
)	O
p	O
(	O
t	O
=	O
1	O
)	O
(	O
cid:80	O
)	O
j	O
,	O
r	O
p	O
(	O
j|r	O
)	O
p	O
(	O
t	O
=	O
1|r	O
,	O
s	O
=	O
1	O
)	O
p	O
(	O
r	O
)	O
p	O
(	O
s	O
=	O
1	O
)	O
(	O
cid:80	O
)	O
j	O
,	O
r	O
,	O
s	O
p	O
(	O
j|r	O
)	O
p	O
(	O
t	O
=	O
1|r	O
,	O
s	O
)	O
p	O
(	O
r	O
)	O
p	O
(	O
s	O
)	O
(	O
cid:80	O
)	O
r	O
p	O
(	O
t	O
=	O
1|r	O
,	O
s	O
=	O
1	O
)	O
p	O
(	O
r	O
)	O
p	O
(	O
s	O
=	O
1	O
)	O
r	O
,	O
s	O
p	O
(	O
t	O
=	O
1|r	O
,	O
s	O
)	O
p	O
(	O
r	O
)	O
p	O
(	O
s	O
)	O
=	O
=	O
=	O
0.9	O
×	O
0.8	O
×	O
0.1	O
+	O
1	O
×	O
0.2	O
×	O
0.1	O
0.9	O
×	O
0.8	O
×	O
0.1	O
+	O
1	O
×	O
0.2	O
×	O
0.1	O
+	O
0	O
×	O
0.8	O
×	O
0.9	O
+	O
1	O
×	O
0.2	O
×	O
0.9	O
(	O
3.2.8	O
)	O
(	O
3.2.9	O
)	O
(	O
3.2.10	O
)	O
=	O
0.3382	O
(	O
3.2.11	O
)	O
so	O
the	O
belief	O
that	O
the	O
sprinkler	O
is	O
on	O
increases	O
above	O
the	O
prior	B
probability	O
0.1	O
,	O
due	O
to	O
the	O
fact	O
that	O
the	O
grass	O
is	O
wet	O
.	O
let	O
us	O
now	O
calculate	O
the	O
probability	B
that	O
tracey	O
’	O
s	O
sprinkler	O
was	O
on	O
overnight	O
,	O
given	O
that	O
her	O
grass	O
is	O
wet	O
and	O
that	O
jack	O
’	O
s	O
grass	O
is	O
also	O
wet	O
,	O
p	O
(	O
s	O
=	O
1|t	O
=	O
1	O
,	O
j	O
=	O
1	O
)	O
.	O
we	O
use	O
bayes	O
’	O
rule	O
again	O
:	O
draft	O
march	O
9	O
,	O
2010	O
27	O
graphically	O
representing	O
distributions	O
p	O
(	O
s	O
=	O
1|t	O
=	O
1	O
,	O
j	O
=	O
1	O
)	O
=	O
p	O
(	O
s	O
=	O
1	O
,	O
t	O
=	O
1	O
,	O
j	O
=	O
1	O
)	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
p	O
(	O
t	O
=	O
1	O
,	O
j	O
=	O
1	O
)	O
r	O
p	O
(	O
t	O
=	O
1	O
,	O
j	O
=	O
1	O
,	O
r	O
,	O
s	O
=	O
1	O
)	O
(	O
cid:80	O
)	O
r	O
,	O
s	O
p	O
(	O
t	O
=	O
1	O
,	O
j	O
=	O
1	O
,	O
r	O
,	O
s	O
)	O
(	O
cid:80	O
)	O
r	O
p	O
(	O
j	O
=	O
1|r	O
)	O
p	O
(	O
t	O
=	O
1|r	O
,	O
s	O
=	O
1	O
)	O
p	O
(	O
r	O
)	O
p	O
(	O
s	O
=	O
1	O
)	O
r	O
,	O
s	O
p	O
(	O
j	O
=	O
1|r	O
)	O
p	O
(	O
t	O
=	O
1|r	O
,	O
s	O
)	O
p	O
(	O
r	O
)	O
p	O
(	O
s	O
)	O
=	O
0.1604	O
0.0344	O
0.2144	O
=	O
=	O
=	O
(	O
3.2.12	O
)	O
(	O
3.2.13	O
)	O
(	O
3.2.14	O
)	O
(	O
3.2.15	O
)	O
the	O
probability	B
that	O
the	O
sprinkler	O
is	O
on	O
,	O
given	O
the	O
extra	O
evidence	B
that	O
jack	O
’	O
s	O
grass	O
is	O
wet	O
,	O
is	O
lower	O
than	O
the	O
probability	B
that	O
the	O
grass	O
is	O
wet	O
given	O
only	O
that	O
tracey	O
’	O
s	O
grass	O
is	O
wet	O
.	O
that	O
is	O
,	O
that	O
the	O
grass	O
is	O
wet	O
due	O
to	O
the	O
sprinkler	O
is	O
(	O
partly	O
)	O
explained	O
away	O
by	O
the	O
fact	O
that	O
jack	O
’	O
s	O
grass	O
is	O
also	O
wet	O
–	O
this	O
increases	O
the	O
chance	O
that	O
the	O
rain	O
has	O
played	O
a	O
role	O
in	O
making	O
tracey	O
’	O
s	O
grass	O
wet	O
.	O
naturally	O
,	O
we	O
don	O
’	O
t	O
wish	O
to	O
carry	O
out	O
such	O
inference	B
calculations	O
by	O
hand	O
all	O
the	O
time	O
.	O
general	O
purpose	O
algorithms	O
exist	O
for	O
this	O
,	O
such	O
as	O
the	O
junction	B
tree	I
algorithm	O
,	O
and	O
we	O
shall	O
introduce	O
these	O
in	O
later	O
chapters	O
.	O
example	O
10	O
(	O
was	O
it	O
the	O
burglar	O
?	O
)	O
.	O
here	O
’	O
s	O
another	O
example	O
using	O
binary	O
variables	O
,	O
adapted	O
from	O
[	O
219	O
]	O
.	O
sally	O
comes	O
home	O
to	O
ﬁnd	O
that	O
the	O
burglar	O
alarm	O
is	O
sounding	O
(	O
a	O
=	O
1	O
)	O
.	O
has	O
she	O
been	O
burgled	O
(	O
b	O
=	O
1	O
)	O
,	O
or	O
was	O
the	O
alarm	O
triggered	O
by	O
an	O
earthquake	O
(	O
e	O
=	O
1	O
)	O
?	O
she	O
turns	O
the	O
car	O
radio	O
on	O
for	O
news	O
of	O
earthquakes	O
and	O
ﬁnds	O
that	O
the	O
radio	O
broadcasts	O
an	O
earthquake	O
alert	O
(	O
r	O
=	O
1	O
)	O
.	O
using	O
bayes	O
’	O
rule	O
,	O
we	O
can	O
write	O
,	O
without	O
loss	O
of	O
generality	O
,	O
p	O
(	O
b	O
,	O
e	O
,	O
a	O
,	O
r	O
)	O
=	O
p	O
(	O
a|b	O
,	O
e	O
,	O
r	O
)	O
p	O
(	O
b	O
,	O
e	O
,	O
r	O
)	O
we	O
can	O
repeat	O
this	O
for	O
p	O
(	O
b	O
,	O
e	O
,	O
r	O
)	O
,	O
and	O
continue	O
p	O
(	O
b	O
,	O
e	O
,	O
a	O
,	O
r	O
)	O
=	O
p	O
(	O
a|b	O
,	O
e	O
,	O
r	O
)	O
p	O
(	O
r|b	O
,	O
e	O
)	O
p	O
(	O
e|b	O
)	O
p	O
(	O
b	O
)	O
(	O
3.2.16	O
)	O
(	O
3.2.17	O
)	O
however	O
,	O
the	O
alarm	O
is	O
surely	O
not	O
directly	O
inﬂuenced	O
by	O
any	O
report	O
on	O
the	O
radio	O
–	O
that	O
is	O
,	O
p	O
(	O
a|b	O
,	O
e	O
,	O
r	O
)	O
=	O
p	O
(	O
a|b	O
,	O
e	O
)	O
.	O
similarly	O
,	O
we	O
can	O
make	O
other	O
conditional	B
independence	O
assumptions	O
such	O
that	O
p	O
(	O
b	O
,	O
e	O
,	O
a	O
,	O
r	O
)	O
=	O
p	O
(	O
a|b	O
,	O
e	O
)	O
p	O
(	O
r|e	O
)	O
p	O
(	O
e	O
)	O
p	O
(	O
b	O
)	O
specifying	O
conditional	B
probability	I
tables	O
alarm	O
=	O
1	O
burglar	O
earthquake	O
0.9999	O
0.99	O
0.99	O
0.0001	O
1	O
1	O
0	O
0	O
1	O
0	O
1	O
0	O
(	O
3.2.18	O
)	O
radio	O
=	O
1	O
earthquake	O
1	O
0	O
1	O
0	O
the	O
remaining	O
tables	O
are	O
p	O
(	O
b	O
=	O
1	O
)	O
=	O
0.01	O
and	O
p	O
(	O
e	O
=	O
1	O
)	O
=	O
0.000001.	O
the	O
tables	O
and	O
graphical	O
structure	B
fully	O
specify	O
the	O
distribution	B
.	O
now	O
consider	O
what	O
happens	O
as	O
we	O
observe	O
evidence	B
.	O
initial	O
evidence	B
:	O
the	O
alarm	O
is	O
sounding	O
p	O
(	O
b	O
=	O
1|a	O
=	O
1	O
)	O
=	O
=	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
e	O
,	O
r	O
p	O
(	O
b	O
=	O
1	O
,	O
e	O
,	O
a	O
=	O
1	O
,	O
r	O
)	O
(	O
cid:80	O
)	O
b	O
,	O
e	O
,	O
r	O
p	O
(	O
b	O
,	O
e	O
,	O
a	O
=	O
1	O
,	O
r	O
)	O
(	O
cid:80	O
)	O
e	O
,	O
r	O
p	O
(	O
a	O
=	O
1|b	O
=	O
1	O
,	O
e	O
)	O
p	O
(	O
b	O
=	O
1	O
)	O
p	O
(	O
e	O
)	O
p	O
(	O
r|e	O
)	O
b	O
,	O
e	O
,	O
r	O
p	O
(	O
a	O
=	O
1|b	O
,	O
e	O
)	O
p	O
(	O
b	O
)	O
p	O
(	O
e	O
)	O
p	O
(	O
r|e	O
)	O
≈	O
0.99	O
(	O
3.2.19	O
)	O
(	O
3.2.20	O
)	O
28	O
draft	O
march	O
9	O
,	O
2010	O
graphically	O
representing	O
distributions	O
additional	O
evidence	B
:	O
the	O
radio	O
broadcasts	O
an	O
earthquake	O
warning	O
:	O
a	O
similar	O
calculation	O
gives	O
p	O
(	O
b	O
=	O
1|a	O
=	O
1	O
,	O
r	O
=	O
1	O
)	O
≈	O
0.01.	O
thus	O
,	O
initially	O
,	O
because	O
the	O
alarm	O
sounds	O
,	O
sally	O
thinks	O
that	O
she	O
’	O
s	O
been	O
burgled	O
.	O
however	O
,	O
this	O
probability	B
drops	O
dramatically	O
when	O
she	O
hears	O
that	O
there	O
has	O
been	O
an	O
earthquake	O
.	O
that	O
is	O
,	O
the	O
earthquake	O
‘	O
explains	O
away	O
’	O
to	O
an	O
extent	O
the	O
fact	O
that	O
the	O
alarm	O
is	O
ringing	O
.	O
see	O
demoburglar.m	O
.	O
3.2.2	O
uncertain	B
evidence	O
in	O
soft	B
or	O
uncertain	B
evidence	O
,	O
the	O
variable	B
is	O
in	O
more	O
than	O
one	O
state	O
,	O
with	O
the	O
strength	O
of	O
our	O
belief	O
about	O
each	O
state	O
being	O
given	O
by	O
probabilities	O
.	O
for	O
example	O
,	O
if	O
x	O
has	O
the	O
states	O
dom	O
(	O
x	O
)	O
=	O
{	O
red	O
,	O
blue	O
,	O
green	O
}	O
the	O
vector	O
(	O
0.6	O
,	O
0.1	O
,	O
0.3	O
)	O
represents	O
the	O
probabilities	O
of	O
the	O
respective	O
states	O
.	O
in	O
contrast	O
,	O
for	O
hard	B
evidence	O
we	O
are	O
certain	O
that	O
a	O
variable	B
is	O
in	O
a	O
particular	O
state	O
.	O
in	O
this	O
case	O
,	O
all	O
the	O
probability	B
mass	O
is	O
in	O
one	O
of	O
the	O
vector	O
components	O
,	O
for	O
example	O
(	O
0	O
,	O
0	O
,	O
1	O
)	O
.	O
performing	O
inference	B
with	O
soft-evidence	O
is	O
straightforward	O
and	O
can	O
be	O
achieved	O
using	O
bayes	O
’	O
rule	O
.	O
writing	O
the	O
soft	B
evidence	O
as	O
˜y	O
,	O
we	O
have	O
p	O
(	O
x|y	O
)	O
p	O
(	O
y|˜y	O
)	O
(	O
3.2.21	O
)	O
p	O
(	O
x|˜y	O
)	O
=	O
(	O
cid:88	O
)	O
y	O
where	O
p	O
(	O
y	O
=	O
i|˜y	O
)	O
represents	O
the	O
probability	B
that	O
y	O
is	O
in	O
state	O
i	O
under	O
the	O
soft-evidence	O
.	O
this	O
is	O
a	O
gener-	O
alisation	O
of	O
hard-evidence	O
in	O
which	O
the	O
vector	O
p	O
(	O
y|˜y	O
)	O
has	O
all	O
zero	O
component	O
values	O
,	O
except	O
for	O
all	O
but	O
a	O
single	O
component	O
.	O
note	O
that	O
the	O
soft	B
evidence	O
p	O
(	O
y	O
=	O
i|˜y	O
)	O
does	O
not	O
correspond	O
to	O
the	O
marginal	B
p	O
(	O
y	O
=	O
i	O
)	O
in	O
the	O
original	O
joint	B
distribution	O
p	O
(	O
x	O
,	O
y	O
)	O
.	O
a	O
procedure	O
to	O
form	O
a	O
joint	B
distribution	O
,	O
known	O
as	O
jeﬀrey	O
’	O
s	O
rule	O
is	O
to	O
begin	O
with	O
an	O
original	O
distribution	B
p1	O
(	O
x	O
,	O
y	O
)	O
,	O
from	O
which	O
we	O
can	O
deﬁne	O
(	O
cid:80	O
)	O
p1	O
(	O
x|y	O
)	O
=	O
p1	O
(	O
x	O
,	O
y	O
)	O
x	O
p1	O
(	O
x	O
,	O
y	O
)	O
using	O
the	O
soft	B
evidence	O
p	O
(	O
y|˜y	O
)	O
we	O
then	O
deﬁne	O
a	O
new	O
joint	B
distribution	O
p2	O
(	O
x	O
,	O
y|˜y	O
)	O
=	O
p1	O
(	O
x|y	O
)	O
p	O
(	O
y|˜y	O
)	O
in	O
the	O
bn	O
we	O
use	O
a	O
dashed	O
circle	O
to	O
represent	O
that	O
a	O
variable	B
is	O
in	O
a	O
soft-evidence	O
state	O
.	O
(	O
3.2.22	O
)	O
(	O
3.2.23	O
)	O
example	O
11	O
(	O
soft-evidence	O
)	O
.	O
revisiting	O
the	O
earthquake	O
scenario	O
,	O
example	O
(	O
10	O
)	O
,	O
imagine	O
that	O
we	O
think	O
we	O
hear	O
the	O
burglar	O
alarm	O
sounding	O
,	O
but	O
are	O
not	O
sure	O
,	O
speciﬁcally	O
we	O
are	O
only	O
70	O
%	O
sure	O
we	O
heard	O
the	O
alarm	O
.	O
for	O
this	O
binary	O
variable	O
case	O
we	O
represent	O
this	O
soft-evidence	O
for	O
the	O
states	O
(	O
1	O
,	O
0	O
)	O
as	O
˜a	O
=	O
(	O
0.7	O
,	O
0.3	O
)	O
.	O
what	O
is	O
the	O
probability	B
of	O
a	O
burglary	O
under	O
this	O
soft-evidence	O
?	O
p	O
(	O
b	O
=	O
1|a	O
)	O
p	O
(	O
a|	O
˜a	O
)	O
=	O
p	O
(	O
b	O
=	O
1|a	O
=	O
1	O
)	O
×	O
0.7	O
+	O
p	O
(	O
b	O
=	O
1|a	O
=	O
0	O
)	O
×	O
0.3	O
(	O
3.2.24	O
)	O
p	O
(	O
b	O
=	O
1|	O
˜a	O
)	O
=	O
(	O
cid:88	O
)	O
a	O
the	O
probabilities	O
p	O
(	O
b	O
=	O
1|a	O
=	O
1	O
)	O
≈	O
0.99	O
and	O
p	O
(	O
b	O
=	O
1|a	O
=	O
0	O
)	O
≈	O
0.0001	O
are	O
calculated	O
using	O
bayes	O
’	O
rule	O
as	O
before	O
to	O
give	O
p	O
(	O
b	O
=	O
1|	O
˜a	O
)	O
≈	O
0.6930	O
uncertain	B
evidence	O
versus	O
unreliable	O
modelling	B
an	O
entertaining	O
example	O
of	O
uncertain	B
evidence	O
is	O
given	O
by	O
pearl	O
[	O
219	O
]	O
:	O
draft	O
march	O
9	O
,	O
2010	O
(	O
3.2.25	O
)	O
29	O
graphically	O
representing	O
distributions	O
b	O
a	O
g	O
w	O
b	O
a	O
h	O
w	O
b	O
a	O
g	O
w	O
j	O
n	O
b	O
a	O
g	O
w	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
(	O
d	O
)	O
figure	O
3.2	O
:	O
(	O
a	O
)	O
:	O
mr	O
holmes	O
’	O
burglary	O
worries	O
as	O
given	O
in	O
[	O
219	O
]	O
:	O
(	O
b	O
)	O
urglar	O
,	O
(	O
a	O
)	O
larm	O
,	O
(	O
w	O
)	O
atson	O
,	O
mrs	O
(	O
c	O
)	O
:	O
modiﬁed	O
problem	B
.	O
mrs	O
(	O
gibbon	O
)	O
.	O
(	O
d	O
)	O
:	O
gibbon	O
is	O
not	O
drinking	O
but	O
somewhat	O
deaf	O
;	O
we	O
represent	O
such	O
uncertain	B
(	O
soft-evidence	O
)	O
by	O
a	O
circle	O
.	O
holmes	O
gets	O
additional	O
information	O
from	O
his	O
neighbour	B
mrs	O
(	O
n	O
)	O
osy	O
and	O
informant	O
dodgy	O
(	O
j	O
)	O
oe	O
.	O
(	O
b	O
)	O
:	O
virtual	B
evidence	O
can	O
be	O
represented	O
by	O
a	O
dashed	O
line	O
.	O
mr	O
holmes	O
receives	O
a	O
telephone	O
call	O
from	O
his	O
neighbour	B
dr	O
watson	O
,	O
who	O
states	O
that	O
he	O
hears	O
the	O
sound	O
of	O
a	O
burglar	O
alarm	O
from	O
the	O
direction	O
of	O
mr	O
holmes	O
’	O
house	O
.	O
while	O
preparing	O
to	O
rush	O
home	O
,	O
mr	O
holmes	O
recalls	O
that	O
dr	O
watson	O
is	O
known	O
to	O
be	O
a	O
tasteless	O
practical	O
joker	O
,	O
and	O
he	O
decides	O
to	O
ﬁrst	O
call	O
another	O
neighbour	B
,	O
mrs	O
gibbon	O
who	O
,	O
despite	O
occasional	O
drinking	O
problems	O
,	O
is	O
far	O
more	O
reliable	O
.	O
when	O
mr	O
holmes	O
calls	O
mrs	O
gibbon	O
,	O
he	O
soon	O
realises	O
that	O
she	O
is	O
somewhat	O
tipsy	O
.	O
instead	O
of	O
answering	O
his	O
question	O
directly	O
,	O
she	O
goes	O
on	O
and	O
on	O
about	O
her	O
latest	O
back	O
operation	O
and	O
about	O
how	O
terribly	O
noisy	O
and	O
crime-ridden	O
the	O
neighbourhood	O
has	O
become	O
.	O
when	O
he	O
ﬁnally	O
hangs	O
up	O
,	O
all	O
mr	O
holmes	O
can	O
glean	O
from	O
the	O
conversation	O
is	O
that	O
there	O
is	O
probably	O
an	O
80	O
%	O
chance	O
that	O
mrs	O
gibbon	O
did	O
hear	O
an	O
alarm	O
sound	O
from	O
her	O
window	O
.	O
a	O
bn	O
for	O
this	O
scenario	O
is	O
depicted	O
in	O
ﬁg	O
(	O
3.2a	O
)	O
which	O
deals	O
with	O
four	O
binary	O
variables	O
:	O
house	O
is	O
(	O
b	O
)	O
urgled	O
,	O
(	O
a	O
)	O
larm	O
has	O
sounded	O
,	O
(	O
w	O
)	O
atson	O
hears	O
alarm	O
,	O
and	O
mrs	O
(	O
g	O
)	O
ibbon	O
hears	O
alarm3	O
:	O
p	O
(	O
b	O
,	O
a	O
,	O
g	O
,	O
w	O
)	O
=	O
p	O
(	O
a|b	O
)	O
p	O
(	O
b	O
)	O
p	O
(	O
w|a	O
)	O
p	O
(	O
g|a	O
)	O
(	O
3.2.26	O
)	O
holmes	O
is	O
interested	O
in	O
the	O
likelihood	B
that	O
his	O
house	O
has	O
been	O
burgled	O
.	O
naively	O
,	O
holmes	O
’	O
might	O
calculate4	O
p	O
(	O
b	O
=	O
tr|w	O
=	O
tr	O
,	O
g	O
=	O
tr	O
)	O
(	O
3.2.27	O
)	O
however	O
,	O
after	O
ﬁnding	O
out	O
about	O
mrs	O
gibbon	O
’	O
s	O
state	O
,	O
mr	O
holmes	O
no	O
longer	O
ﬁnds	O
the	O
above	O
model	B
reliable	O
.	O
he	O
wants	O
to	O
ignore	O
the	O
eﬀect	O
that	O
mrs	O
gibbon	O
’	O
s	O
evidence	B
has	O
on	O
the	O
inference	B
,	O
and	O
replace	O
it	O
with	O
his	O
own	O
belief	O
as	O
to	O
what	O
mrs	O
gibbon	O
observed	O
.	O
mr	O
holmes	O
can	O
achieve	O
this	O
by	O
replacing	O
the	O
term	O
p	O
(	O
g	O
=	O
tr|a	O
)	O
by	O
a	O
so-called	O
virtual	B
evidence	O
term	O
p	O
(	O
g	O
=	O
tr|a	O
)	O
→	O
p	O
(	O
h|a	O
)	O
,	O
where	O
p	O
(	O
h|a	O
)	O
=	O
(	O
cid:26	O
)	O
0.8	O
a	O
=	O
tr	O
0.2	O
a	O
=	O
fa	O
(	O
3.2.28	O
)	O
(	O
3.2.29	O
)	O
here	O
the	O
state	O
h	O
is	O
arbitrary	O
and	O
ﬁxed	O
.	O
this	O
is	O
used	O
to	O
modify	O
the	O
joint	B
distribution	O
to	O
p	O
(	O
b	O
,	O
a	O
,	O
h	O
,	O
w	O
)	O
=	O
p	O
(	O
a|b	O
)	O
p	O
(	O
b	O
)	O
p	O
(	O
w|a	O
)	O
p	O
(	O
h|a	O
)	O
,	O
see	O
ﬁg	O
(	O
3.2b	O
)	O
.	O
when	O
we	O
then	O
compute	O
p	O
(	O
b	O
=	O
tr|w	O
=	O
tr	O
,	O
h	O
)	O
the	O
eﬀect	O
of	O
mr	O
holmes	O
’	O
judgement	O
will	O
count	O
for	O
a	O
factor	B
of	O
4	O
times	O
more	O
in	O
favour	O
of	O
the	O
alarm	O
sounding	O
than	O
not	O
.	O
the	O
values	O
of	O
the	O
table	O
entries	O
are	O
irrelevant	O
up	O
to	O
normalisation	B
since	O
any	O
constants	O
can	O
be	O
absorbed	O
into	O
the	O
proportionality	O
constant	O
.	O
note	O
also	O
that	O
p	O
(	O
h|a	O
)	O
is	O
not	O
a	O
distribution	B
in	O
a	O
,	O
and	O
hence	O
no	O
normalisation	O
is	O
required	O
.	O
this	O
form	O
of	O
evidence	B
is	O
also	O
called	O
likelihood	B
evidence	O
.	O
a	O
twist	O
on	O
pearl	O
’	O
s	O
scenario	O
is	O
that	O
mrs	O
gibbon	O
has	O
not	O
been	O
drinking	O
.	O
however	O
,	O
she	O
is	O
a	O
little	O
deaf	O
and	O
can	O
not	O
be	O
sure	O
herself	O
that	O
she	O
heard	O
the	O
alarm	O
.	O
she	O
is	O
80	O
%	O
sure	O
she	O
heard	O
it	O
.	O
in	O
this	O
case	O
,	O
holmes	O
would	O
3one	O
might	O
be	O
tempted	O
to	O
include	O
an	O
additional	O
(	O
t	O
)	O
ipsy	O
variable	B
as	O
a	O
parent	O
of	O
g.	O
this	O
would	O
then	O
require	O
us	O
to	O
specify	O
the	O
joint	B
distribution	O
p	O
(	O
g|t	O
,	O
a	O
)	O
for	O
the	O
4	O
parental	O
joint	B
states	O
of	O
t	O
and	O
a.	O
here	O
we	O
assume	O
that	O
we	O
do	O
not	O
have	O
access	O
to	O
such	O
information	O
.	O
4the	O
notation	O
tr	O
is	O
equivalent	B
to	O
1	O
and	O
fa	O
to	O
0	O
from	O
example	O
(	O
10	O
)	O
.	O
30	O
draft	O
march	O
9	O
,	O
2010	O
trust	O
the	O
model	B
–	O
however	O
,	O
the	O
observation	O
itself	O
is	O
now	O
uncertain	B
,	O
ﬁg	O
(	O
3.2c	O
)	O
.	O
this	O
can	O
be	O
dealt	O
with	O
using	O
the	O
soft	B
evidence	O
technique	O
.	O
from	O
jeﬀrey	O
’	O
s	O
rule	O
,	O
one	O
uses	O
the	O
original	O
model	B
equation	O
(	O
3.2.26	O
)	O
to	O
compute	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
a	O
p	O
(	O
g|a	O
)	O
p	O
(	O
w	O
=	O
tr|a	O
)	O
p	O
(	O
a|b	O
=	O
tr	O
)	O
b	O
,	O
a	O
p	O
(	O
g|a	O
)	O
p	O
(	O
w	O
=	O
tr|a	O
)	O
p	O
(	O
a|b	O
)	O
graphically	O
representing	O
distributions	O
p	O
(	O
b	O
=	O
tr|w	O
=	O
tr	O
,	O
g	O
)	O
=	O
p	O
(	O
b	O
=	O
tr	O
,	O
w	O
=	O
tr	O
,	O
g	O
)	O
p	O
(	O
w	O
=	O
tr	O
,	O
g	O
)	O
=	O
and	O
then	O
uses	O
the	O
soft-evidence	O
(	O
cid:26	O
)	O
0.8	O
g	O
=	O
tr	O
0.2	O
g	O
=	O
fa	O
p	O
(	O
g|	O
˜g	O
)	O
=	O
to	O
compute	O
p	O
(	O
b	O
=	O
tr|w	O
=	O
tr	O
,	O
˜g	O
)	O
=	O
p	O
(	O
b	O
=	O
tr|w	O
=	O
tr	O
,	O
g	O
=	O
tr	O
)	O
p	O
(	O
g	O
=	O
tr|	O
˜g	O
)	O
+	O
p	O
(	O
b	O
=	O
tr|w	O
=	O
tr	O
,	O
g	O
=	O
fa	O
)	O
p	O
(	O
g	O
=	O
fa|	O
˜g	O
)	O
(	O
3.2.32	O
)	O
the	O
reader	O
may	O
show	O
that	O
an	O
alternative	O
way	O
to	O
represent	O
an	O
uncertain	B
observation	O
such	O
as	O
mrs	O
gibbon	O
being	O
non-tipsy	O
but	O
hard-of-hearing	O
above	O
is	O
to	O
use	O
a	O
virtual	B
evidence	O
child	O
from	O
g.	O
uncertain	B
evidence	O
within	O
an	O
unreliable	O
model	B
to	O
highlight	O
uncertain	B
evidence	O
in	O
an	O
unreliable	O
model	B
we	O
introduce	O
two	O
additional	O
characters	O
.	O
mrs	O
nosy	O
lives	O
next	O
door	O
to	O
mr	O
holmes	O
and	O
is	O
completely	B
deaf	O
,	O
but	O
nevertheless	O
an	O
incorrigible	O
curtain-peeker	O
who	O
seems	O
to	O
notice	O
most	O
things	O
.	O
unfortunately	O
,	O
she	O
’	O
s	O
also	O
rather	O
prone	O
to	O
imagining	O
things	O
.	O
based	O
on	O
his	O
conversation	O
with	O
her	O
,	O
mr	O
holmes	O
counts	O
her	O
story	O
as	O
3	O
times	O
in	O
favour	O
of	O
there	O
not	O
being	O
a	O
burglary	O
to	O
there	O
being	O
a	O
burglary	O
,	O
and	O
therefore	O
uses	O
a	O
virtual	B
evidence	O
term	O
(	O
3.2.30	O
)	O
(	O
3.2.31	O
)	O
(	O
3.2.33	O
)	O
(	O
3.2.34	O
)	O
(	O
cid:26	O
)	O
1	O
b	O
=	O
tr	O
3	O
b	O
=	O
fa	O
(	O
cid:26	O
)	O
1	O
b	O
=	O
tr	O
5	O
b	O
=	O
fa	O
p	O
(	O
nosy|b	O
)	O
=	O
p	O
(	O
joe|b	O
)	O
=	O
mr	O
holmes	O
also	O
telephones	O
dodgy	O
joe	O
,	O
his	O
contact	O
in	O
the	O
criminal	O
underworld	O
to	O
see	O
if	O
he	O
’	O
s	O
heard	O
of	O
any	O
planned	O
burglary	O
on	O
mr	O
holmes	O
’	O
home	O
.	O
he	O
summarises	O
this	O
information	O
using	O
a	O
virtual	B
evidence	O
term	O
when	O
all	O
this	O
information	O
is	O
combined	O
:	O
mrs	O
gibbon	O
is	O
not	O
tipsy	O
but	O
somewhat	O
hard	B
of	O
hearing	O
,	O
mrs	O
nosy	O
,	O
and	O
dodgy	O
joe	O
,	O
we	O
ﬁrst	O
deal	O
with	O
the	O
unreliable	O
model	B
p	O
(	O
b	O
,	O
a	O
,	O
w	O
=	O
tr	O
,	O
g	O
,	O
nosy	O
,	O
joe	O
)	O
∝	O
p	O
(	O
b	O
)	O
p	O
(	O
nosy|b	O
)	O
p	O
(	O
joe|b	O
)	O
p	O
(	O
a|b	O
)	O
p	O
(	O
w	O
=	O
tr|a	O
)	O
p	O
(	O
g|a	O
)	O
(	O
3.2.35	O
)	O
from	O
which	O
we	O
can	O
compute	O
p	O
(	O
b	O
=	O
tr|w	O
=	O
tr	O
,	O
g	O
,	O
nosy	O
,	O
joe	O
)	O
finally	O
we	O
perform	O
inference	B
with	O
the	O
soft-evidence	O
p	O
(	O
b	O
=	O
tr|w	O
=	O
tr	O
,	O
˜g	O
,	O
nosy	O
,	O
joe	O
)	O
=	O
(	O
cid:88	O
)	O
p	O
(	O
b	O
=	O
tr|w	O
=	O
tr	O
,	O
g	O
,	O
nosy	O
,	O
joe	O
)	O
p	O
(	O
g|	O
˜g	O
)	O
g	O
(	O
3.2.36	O
)	O
(	O
3.2.37	O
)	O
an	O
important	O
consideration	O
above	O
is	O
that	O
the	O
virtual	B
evidence	O
does	O
not	O
replace	O
the	O
prior	B
p	O
(	O
b	O
)	O
with	O
another	O
prior	B
distribution	O
–	O
rather	O
the	O
virtual	B
evidence	O
terms	O
modify	O
the	O
prior	B
through	O
the	O
inclusion	O
of	O
extra	O
factors	O
.	O
the	O
usual	O
assumption	O
is	O
that	O
each	O
virtual	B
evidence	O
acts	O
independently	O
,	O
although	O
one	O
can	O
consider	O
dependent	O
scenarios	O
if	O
required	O
.	O
draft	O
march	O
9	O
,	O
2010	O
31	O
belief	B
networks	I
x1	O
x2	O
x3	O
x4	O
x3	O
x4	O
x1	O
x2	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
3.3	O
:	O
two	O
bns	O
for	O
a	O
4	O
variable	B
distribution	O
.	O
both	O
graphs	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
represent	O
the	O
same	O
distribution	B
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
.	O
strictly	O
speaking	O
they	O
represent	O
the	O
same	O
(	O
lack	O
of	O
)	O
independence	B
assumptions	O
–	O
the	O
graphs	O
say	O
nothing	O
about	O
the	O
content	O
of	O
the	O
cpts	O
.	O
the	O
extension	O
of	O
this	O
‘	O
cascade	B
’	O
to	O
many	O
variables	O
is	O
clear	O
and	O
always	O
results	O
in	O
a	O
directed	B
acyclic	I
graph	I
.	O
3.3	O
belief	B
networks	I
in	O
the	O
wet	O
grass	O
and	O
burglar	O
examples	O
,	O
we	O
had	O
a	O
choice	O
as	O
to	O
how	O
we	O
recursively	O
used	O
bayes	O
’	O
rule	O
.	O
in	O
a	O
general	O
4	O
variable	B
case	O
we	O
could	O
choose	O
the	O
factorisation	O
,	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
=	O
p	O
(	O
x1|x2	O
,	O
x3	O
,	O
x4	O
)	O
p	O
(	O
x2|x3	O
,	O
x4	O
)	O
p	O
(	O
x3|x4	O
)	O
p	O
(	O
x4	O
)	O
an	O
equally	O
valid	O
choice	O
is	O
(	O
see	O
ﬁg	O
(	O
3.3	O
)	O
)	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
=	O
p	O
(	O
x3|x4	O
,	O
x1	O
,	O
x2	O
)	O
p	O
(	O
x4|x1	O
,	O
x2	O
)	O
p	O
(	O
x1|x2	O
)	O
p	O
(	O
x2	O
)	O
.	O
(	O
3.3.1	O
)	O
(	O
3.3.2	O
)	O
in	O
general	O
,	O
two	O
diﬀerent	O
graphs	O
may	O
represent	O
the	O
same	O
independence	B
assumptions	O
,	O
as	O
we	O
will	O
discuss	O
further	O
in	O
section	O
(	O
3.3.1	O
)	O
.	O
if	O
one	O
wishes	O
to	O
make	O
independence	B
assumptions	O
,	O
then	O
the	O
choice	O
of	O
factorisation	O
becomes	O
signiﬁcant	O
.	O
the	O
observation	O
that	O
any	O
distribution	B
may	O
be	O
written	O
in	O
the	O
cascade	B
form	O
,	O
ﬁg	O
(	O
3.3	O
)	O
,	O
gives	O
an	O
algorithm	B
for	O
constructing	O
a	O
bn	O
on	O
variables	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
:	O
write	O
down	O
the	O
n−variable	O
cascade	B
graph	O
;	O
assign	O
any	O
ordering	O
of	O
the	O
variables	O
to	O
the	O
nodes	O
;	O
you	O
may	O
then	O
delete	O
any	O
of	O
the	O
directed	B
connections	O
.	O
more	O
formally	O
,	O
this	O
corresponds	O
to	O
an	O
ordering	O
of	O
the	O
variables	O
which	O
,	O
without	O
loss	O
of	O
generality	O
,	O
we	O
may	O
write	O
as	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
.	O
then	O
,	O
from	O
bayes	O
’	O
rule	O
,	O
we	O
have	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
=	O
p	O
(	O
x1|x2	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
p	O
(	O
x2	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
=	O
p	O
(	O
x1|x2	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
p	O
(	O
x2|x3	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
p	O
(	O
x3	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
=	O
p	O
(	O
xn	O
)	O
p	O
(	O
xi|xi+1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
n−1	O
(	O
cid:89	O
)	O
i=1	O
(	O
3.3.3	O
)	O
(	O
3.3.4	O
)	O
(	O
3.3.5	O
)	O
the	O
representation	B
of	O
any	O
bn	O
is	O
therefore	O
a	O
directed	B
acyclic	I
graph	I
(	O
dag	O
)	O
.	O
every	O
probability	B
distribution	O
can	O
be	O
written	O
as	O
a	O
bn	O
,	O
even	O
though	O
it	O
may	O
correspond	O
to	O
a	O
fully	O
con-	O
nected	O
‘	O
cascade	B
’	O
dag	O
.	O
the	O
particular	O
role	O
of	O
a	O
bn	O
is	O
that	O
the	O
structure	B
of	O
the	O
dag	O
corresponds	O
to	O
a	O
set	O
of	O
conditional	B
independence	O
assumptions	O
,	O
namely	O
which	O
parental	O
variables	O
are	O
suﬃcient	O
to	O
specify	O
each	O
conditional	B
probability	I
table	O
.	O
note	O
that	O
this	O
does	O
not	O
mean	B
that	O
non-parental	O
variables	O
have	O
no	O
inﬂu-	O
ence	O
.	O
for	O
example	O
,	O
for	O
distribution	B
p	O
(	O
x1|x2	O
)	O
p	O
(	O
x2|x3	O
)	O
p	O
(	O
x3	O
)	O
with	O
dag	O
x1	O
←	O
x2	O
←	O
x3	O
,	O
this	O
does	O
not	O
imply	O
p	O
(	O
x2|x1	O
,	O
x3	O
)	O
=	O
p	O
(	O
x2|x3	O
)	O
.	O
the	O
dag	O
speciﬁes	O
conditional	B
independence	O
statements	O
of	O
variables	O
on	O
their	O
ancestors	O
–	O
namely	O
which	O
ancestors	O
are	O
‘	O
causes	O
’	O
for	O
the	O
variable	B
.	O
the	O
dag	O
corresponds	O
to	O
a	O
statement	O
of	O
conditional	B
independencies	O
in	O
the	O
model	B
.	O
to	O
complete	O
the	O
speci-	O
ﬁcation	O
of	O
the	O
bn	O
we	O
need	O
to	O
deﬁne	O
all	O
elements	O
of	O
the	O
conditional	B
probability	I
tables	O
p	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
.	O
once	O
the	O
graphical	O
structure	B
is	O
deﬁned	O
,	O
the	O
entries	O
of	O
the	O
conditional	B
probability	I
tables	O
(	O
cpts	O
)	O
p	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
can	O
be	O
expressed	O
.	O
for	O
every	O
possible	O
state	O
of	O
the	O
parental	O
variables	O
pa	O
(	O
xi	O
)	O
,	O
a	O
value	B
for	O
each	O
of	O
the	O
states	O
of	O
xi	O
needs	O
to	O
be	O
speciﬁed	O
(	O
except	O
one	O
,	O
since	O
this	O
is	O
determined	O
by	O
normalisation	B
)	O
.	O
for	O
a	O
large	O
number	O
of	O
parents	B
,	O
writing	O
out	O
a	O
table	O
of	O
values	O
is	O
intractable	O
,	O
and	O
the	O
tables	O
are	O
usually	O
parameterised	O
in	O
a	O
low	B
dimensional	I
manner	O
.	O
this	O
will	O
be	O
a	O
central	O
topic	O
of	O
our	O
discussion	O
on	O
the	O
application	O
of	O
bns	O
in	O
machine	O
learning	B
.	O
32	O
draft	O
march	O
9	O
,	O
2010	O
belief	B
networks	I
3.3.1	O
conditional	B
independence	O
whilst	O
a	O
bn	O
corresponds	O
to	O
a	O
set	O
of	O
conditional	B
independence	O
assumptions	O
,	O
it	O
is	O
not	O
always	O
immediately	O
clear	O
from	O
the	O
dag	O
whether	O
a	O
set	O
of	O
variables	O
is	O
conditionally	O
independent	O
of	O
a	O
set	O
of	O
other	O
variables	O
.	O
for	O
example	O
,	O
in	O
ﬁg	O
(	O
3.4	O
)	O
are	O
x1	O
and	O
x2	O
independent	O
,	O
given	O
the	O
state	O
of	O
x4	O
?	O
the	O
answer	O
is	O
yes	O
,	O
since	O
we	O
have	O
p	O
(	O
x1	O
,	O
x2|x4	O
)	O
=	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
=	O
p	O
(	O
x1|x4	O
)	O
p	O
(	O
x2|x3	O
,	O
x4	O
)	O
p	O
(	O
x3	O
)	O
p	O
(	O
x4	O
)	O
(	O
3.3.6	O
)	O
(	O
3.3.7	O
)	O
(	O
3.3.8	O
)	O
(	O
3.3.9	O
)	O
(	O
3.3.10	O
)	O
(	O
3.3.11	O
)	O
(	O
cid:88	O
)	O
x1	O
,	O
x3	O
1	O
p	O
(	O
x4	O
)	O
p	O
(	O
x1|x4	O
)	O
p	O
(	O
x2|x3	O
,	O
x4	O
)	O
p	O
(	O
x3	O
)	O
p	O
(	O
x4	O
)	O
(	O
cid:88	O
)	O
1	O
p	O
(	O
x4	O
)	O
x3	O
p	O
(	O
x2|x3	O
,	O
x4	O
)	O
p	O
(	O
x3	O
)	O
1	O
x3	O
p	O
(	O
x4	O
)	O
(	O
cid:88	O
)	O
=	O
p	O
(	O
x1|x4	O
)	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
x3	O
now	O
p	O
(	O
x2|x4	O
)	O
=	O
p	O
(	O
x4	O
)	O
1	O
=	O
(	O
cid:88	O
)	O
x3	O
x1	O
,	O
x3	O
p	O
(	O
x2|x3	O
,	O
x4	O
)	O
p	O
(	O
x3	O
)	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
=	O
combining	O
the	O
two	O
results	O
above	O
we	O
have	O
p	O
(	O
x1	O
,	O
x2|x4	O
)	O
=	O
p	O
(	O
x1|x4	O
)	O
p	O
(	O
x2|x4	O
)	O
so	O
that	O
x1	O
and	O
x2	O
are	O
indeed	O
independent	O
conditioned	O
on	O
x4	O
.	O
deﬁnition	O
19	O
(	O
conditional	B
independence	O
)	O
.	O
x	O
⊥⊥y|z	O
denotes	O
that	O
the	O
two	O
sets	O
of	O
variables	O
x	O
and	O
y	O
are	O
independent	O
of	O
each	O
other	O
provided	O
we	O
know	O
the	O
state	O
of	O
the	O
set	O
of	O
variables	O
z.	O
for	O
full	O
conditional	B
independence	O
,	O
x	O
and	O
y	O
must	O
be	O
independent	O
given	O
all	O
states	O
of	O
z.	O
formally	O
,	O
this	O
means	O
that	O
p	O
(	O
x	O
,	O
y|z	O
)	O
=	O
p	O
(	O
x|z	O
)	O
p	O
(	O
y|z	O
)	O
(	O
3.3.12	O
)	O
for	O
all	O
states	O
of	O
x	O
,	O
y	O
,	O
z	O
.	O
in	O
case	O
the	O
conditioning	B
set	O
is	O
empty	O
we	O
may	O
also	O
write	O
x	O
⊥⊥y	O
for	O
x	O
⊥⊥y|∅	O
,	O
in	O
which	O
case	O
x	O
is	O
(	O
unconditionally	O
)	O
independent	O
of	O
y.	O
if	O
x	O
and	O
y	O
are	O
not	O
conditionally	O
independent	O
,	O
they	O
are	O
conditionally	O
dependent	O
.	O
this	O
is	O
written	O
x	O
(	O
cid:62	O
)	O
(	O
cid:62	O
)	O
y|z	O
(	O
3.3.13	O
)	O
to	O
develop	O
intuition	O
about	O
conditional	B
independence	O
consider	O
the	O
three	O
variable	B
distribution	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
.	O
we	O
may	O
write	O
this	O
in	O
any	O
of	O
the	O
6	O
ways	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
=	O
p	O
(	O
xi1|xi2	O
,	O
xi3	O
)	O
p	O
(	O
xi2|xi3	O
)	O
p	O
(	O
xi3	O
)	O
(	O
3.3.14	O
)	O
where	O
(	O
i1	O
,	O
i2	O
,	O
i3	O
)	O
is	O
any	O
of	O
the	O
6	O
permutations	O
of	O
(	O
1	O
,	O
2	O
,	O
3	O
)	O
.	O
whilst	O
all	O
diﬀerent	O
dags	O
,	O
they	O
represent	O
the	O
same	O
distribution	B
,	O
namely	O
that	O
which	O
makes	O
no	O
conditional	O
independence	B
statements	O
.	O
to	O
make	O
an	O
independence	B
statement	O
,	O
we	O
need	O
to	O
drop	O
one	O
of	O
the	O
links	O
.	O
this	O
gives	O
rise	O
to	O
the	O
4	O
dags	O
in	O
ﬁg	O
(	O
3.5	O
)	O
.	O
are	O
any	O
of	O
these	O
graphs	O
equivalent	B
,	O
in	O
the	O
sense	O
that	O
they	O
represent	O
the	O
same	O
distribution	B
?	O
x1	O
x2	O
x3	O
x4	O
figure	O
3.4	O
:	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
=	O
p	O
(	O
x1|x4	O
)	O
p	O
(	O
x2|x3	O
,	O
x4	O
)	O
p	O
(	O
x3	O
)	O
p	O
(	O
x4	O
)	O
.	O
draft	O
march	O
9	O
,	O
2010	O
33	O
belief	B
networks	I
x1	O
x2	O
x1	O
x2	O
x1	O
x2	O
x1	O
x2	O
x3	O
(	O
a	O
)	O
x3	O
(	O
b	O
)	O
x3	O
(	O
c	O
)	O
x3	O
(	O
d	O
)	O
figure	O
3.5	O
:	O
by	O
dropping	O
say	O
the	O
connection	O
between	O
variables	O
x1	O
and	O
x2	O
,	O
we	O
reduce	O
the	O
6	O
possible	O
bn	O
graphs	O
amongst	O
three	O
variables	O
to	O
4	O
.	O
(	O
the	O
6	O
fully	O
connected	B
‘	O
cascade	B
’	O
graphs	O
correspond	O
to	O
(	O
a	O
)	O
with	O
x1	O
→	O
x2	O
,	O
(	O
a	O
)	O
with	O
x2	O
→	O
x1	O
,	O
(	O
b	O
)	O
with	O
x1	O
→	O
x2	O
,	O
(	O
b	O
)	O
with	O
x2	O
→	O
x1	O
,	O
(	O
c	O
)	O
with	O
x1	O
→	O
x3	O
and	O
(	O
d	O
)	O
with	O
x2	O
→	O
x1	O
.	O
any	O
other	O
graphs	O
would	O
be	O
cyclic	O
and	O
therefore	O
not	O
distributions	O
)	O
.	O
x	O
y	O
x	O
y	O
x	O
y	O
z	O
(	O
a	O
)	O
z	O
(	O
b	O
)	O
z	O
(	O
c	O
)	O
x	O
y	O
w	O
z	O
(	O
d	O
)	O
(	O
c	O
)	O
:	O
variable	B
z	O
is	O
a	O
collider	B
.	O
graphs	O
figure	O
3.6	O
:	O
in	O
graphs	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
,	O
variable	B
z	O
is	O
not	O
a	O
collider	B
.	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
represent	O
conditional	B
independence	O
x⊥⊥	O
y|	O
z.	O
in	O
graphs	O
(	O
c	O
)	O
and	O
(	O
d	O
)	O
,	O
x	O
and	O
y	O
are	O
’	O
graphically	O
’	O
conditionally	O
dependent	O
given	O
variable	B
z.	O
applying	O
bayes	O
’	O
rule	O
gives	O
:	O
(	O
cid:124	O
)	O
(	O
cid:125	O
)	O
p	O
(	O
x2|x3	O
)	O
p	O
(	O
x3|x1	O
)	O
p	O
(	O
x1	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
graph	B
(	O
c	O
)	O
=	O
p	O
(	O
x2	O
,	O
x3	O
)	O
p	O
(	O
x3	O
,	O
x1	O
)	O
/p	O
(	O
x3	O
)	O
=	O
p	O
(	O
x1|x3	O
)	O
p	O
(	O
x2	O
,	O
x3	O
)	O
(	O
cid:124	O
)	O
(	O
cid:125	O
)	O
=	O
p	O
(	O
x1|x3	O
)	O
p	O
(	O
x3|x2	O
)	O
p	O
(	O
x2	O
)	O
(	O
cid:124	O
)	O
(	O
cid:125	O
)	O
=	O
p	O
(	O
x1|x3	O
)	O
p	O
(	O
x2|x3	O
)	O
p	O
(	O
x3	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
graph	B
(	O
d	O
)	O
graph	B
(	O
b	O
)	O
(	O
3.3.15	O
)	O
(	O
3.3.16	O
)	O
so	O
that	O
dags	O
(	O
b	O
)	O
,	O
(	O
c	O
)	O
and	O
(	O
d	O
)	O
represent	O
the	O
same	O
ci	O
assumptions	O
namely	O
that	O
,	O
given	O
the	O
state	O
of	O
variable	B
x3	O
,	O
variables	O
x1	O
and	O
x2	O
are	O
independent	O
,	O
x1⊥⊥	O
x2|	O
x3	O
.	O
however	O
,	O
graph	B
(	O
a	O
)	O
represents	O
something	O
fundamentally	O
diﬀerent	O
,	O
namely	O
:	O
p	O
(	O
x1	O
,	O
x2	O
)	O
=	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2	O
)	O
.	O
there	O
is	O
no	O
way	O
to	O
transform	O
the	O
distribution	B
p	O
(	O
x3|x1	O
,	O
x2	O
)	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2	O
)	O
into	O
any	O
of	O
the	O
others	O
.	O
remark	O
3	O
(	O
graphical	O
dependence	B
)	O
.	O
belief	B
networks	I
are	O
good	O
for	O
encoding	O
conditional	B
independence	O
,	O
but	O
are	O
not	O
well	O
suited	O
to	O
describing	O
dependence	B
.	O
for	O
example	O
,	O
consider	O
the	O
trivial	O
network	O
p	O
(	O
a	O
,	O
b	O
)	O
=	O
p	O
(	O
b|a	O
)	O
p	O
(	O
a	O
)	O
which	O
has	O
the	O
dag	O
representation	B
a	O
→	O
b.	O
this	O
may	O
appear	O
to	O
encode	O
that	O
a	O
and	O
b	O
are	O
dependent	O
.	O
however	O
,	O
there	O
are	O
certainly	O
instances	O
when	O
this	O
is	O
not	O
the	O
case	O
.	O
for	O
example	O
,	O
it	O
may	O
be	O
that	O
the	O
conditional	B
is	O
such	O
that	O
p	O
(	O
b|a	O
)	O
=	O
p	O
(	O
b	O
)	O
,	O
so	O
that	O
p	O
(	O
a	O
,	O
b	O
)	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
.	O
in	O
this	O
case	O
,	O
although	O
generally	O
there	O
may	O
appear	O
to	O
be	O
a	O
‘	O
graphical	O
’	O
dependence	B
from	O
the	O
dag	O
,	O
there	O
can	O
be	O
instances	O
of	O
the	O
distributions	O
for	O
which	O
dependence	B
does	O
not	O
follow	O
.	O
the	O
same	O
holds	O
for	O
markov	O
networks	O
,	O
section	O
(	O
4.2	O
)	O
,	O
in	O
which	O
p	O
(	O
a	O
,	O
b	O
)	O
=	O
φ	O
(	O
a	O
,	O
b	O
)	O
.	O
whilst	O
this	O
suggests	O
‘	O
graphical	O
’	O
dependence	B
between	O
a	O
and	O
b	O
,	O
for	O
φ	O
(	O
a	O
,	O
b	O
)	O
=	O
φ	O
(	O
a	O
)	O
φ	O
(	O
b	O
)	O
,	O
the	O
variables	O
are	O
independent	O
.	O
3.3.2	O
the	O
impact	O
of	O
collisions	O
in	O
a	O
general	O
bn	O
,	O
how	O
could	O
we	O
check	B
if	O
x⊥⊥	O
y|	O
z	O
?	O
in	O
ﬁg	O
(	O
3.6	O
)	O
(	O
a	O
,	O
b	O
)	O
,	O
x	O
and	O
y	O
are	O
independent	O
when	O
conditioned	O
on	O
z.	O
in	O
ﬁg	O
(	O
3.6	O
)	O
(	O
c	O
)	O
they	O
are	O
dependent	O
;	O
in	O
this	O
situation	O
,	O
variable	B
z	O
is	O
called	O
a	O
collider	B
–	O
the	O
arrows	O
of	O
its	O
34	O
draft	O
march	O
9	O
,	O
2010	O
belief	B
networks	I
a	O
b	O
e	O
c	O
d	O
figure	O
3.7	O
:	O
the	O
variable	B
d	O
is	O
a	O
collider	B
along	O
the	O
path	B
a−b−d−c	O
,	O
but	O
not	O
along	O
the	O
path	B
a	O
−	O
b	O
−	O
d	O
−	O
e.	O
is	O
a	O
⊥⊥	O
e|	O
b	O
?	O
a	O
and	O
b	O
are	O
not	O
d-connected	O
since	O
there	O
are	O
no	O
colliders	O
on	O
the	O
only	O
path	B
between	O
a	O
and	O
e	O
,	O
and	O
since	O
there	O
is	O
a	O
non-collider	O
b	O
which	O
is	O
in	O
the	O
conditioning	B
set	O
.	O
hence	O
a	O
and	O
b	O
are	O
d-separated	O
,	O
i.e	O
.	O
a⊥⊥	O
e|	O
b.	O
neighbours	O
are	O
pointing	O
towards	O
it	O
.	O
what	O
about	O
ﬁg	O
(	O
3.6	O
)	O
(	O
d	O
)	O
?	O
in	O
(	O
d	O
)	O
,	O
when	O
we	O
condition	O
on	O
z	O
,	O
x	O
and	O
y	O
will	O
be	O
‘	O
graphically	O
’	O
dependent	O
,	O
since	O
(	O
cid:88	O
)	O
w	O
p	O
(	O
x	O
,	O
y|z	O
)	O
=	O
p	O
(	O
x	O
,	O
y	O
,	O
z	O
)	O
p	O
(	O
z	O
)	O
=	O
1	O
p	O
(	O
z	O
)	O
p	O
(	O
z|w	O
)	O
p	O
(	O
w|x	O
,	O
y	O
)	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
(	O
cid:54	O
)	O
=	O
p	O
(	O
x|z	O
)	O
p	O
(	O
y|z	O
)	O
(	O
3.3.17	O
)	O
–	O
intuitively	O
,	O
variable	B
w	O
becomes	O
dependent	O
on	O
the	O
value	B
of	O
z	O
,	O
and	O
since	O
x	O
and	O
y	O
are	O
conditionally	O
depen-	O
dent	O
on	O
w	O
,	O
they	O
are	O
also	O
conditionally	O
dependent	O
on	O
z.	O
roughly	O
speaking	O
,	O
if	O
there	O
is	O
a	O
non-collider	O
z	O
which	O
is	O
conditioned	O
on	O
along	O
the	O
path	B
between	O
x	O
and	O
y	O
(	O
as	O
in	O
ﬁg	O
(	O
3.6	O
)	O
(	O
a	O
,	O
b	O
)	O
)	O
,	O
then	O
this	O
path	B
does	O
not	O
make	O
x	O
and	O
y	O
dependent	O
.	O
similarly	O
,	O
if	O
there	O
is	O
a	O
path	B
between	O
x	O
and	O
y	O
which	O
contains	O
a	O
collider	B
,	O
provided	O
that	O
this	O
collider	B
is	O
not	O
in	O
the	O
conditioning	B
set	O
(	O
and	O
neither	O
are	O
any	O
of	O
its	O
descendants	O
)	O
then	O
this	O
path	B
does	O
not	O
make	O
x	O
and	O
y	O
dependent	O
.	O
if	O
there	O
is	O
a	O
path	B
between	O
x	O
and	O
y	O
which	O
contains	O
no	O
colliders	O
and	O
no	O
conditioning	O
variables	O
,	O
then	O
this	O
path	B
‘	O
d-connects	O
’	O
x	O
and	O
y.	O
note	O
that	O
a	O
collider	B
is	O
deﬁned	O
relative	O
to	O
a	O
path	B
.	O
in	O
ﬁg	O
(	O
3.7	O
)	O
,	O
the	O
variable	B
d	O
is	O
a	O
collider	B
along	O
the	O
path	B
a−	O
b−	O
d−	O
c	O
,	O
but	O
not	O
along	O
the	O
path	B
a−	O
b−	O
d−	O
e	O
(	O
since	O
,	O
relative	O
to	O
this	O
path	B
,	O
the	O
two	O
arrows	O
do	O
not	O
point	O
inwards	O
to	O
d	O
)	O
.	O
consider	O
the	O
bn	O
:	O
a	O
→	O
b	O
←	O
c.	O
here	O
a	O
and	O
c	O
are	O
(	O
unconditionally	O
)	O
independent	O
.	O
however	O
,	O
conditioning	B
of	O
b	O
makes	O
them	O
‘	O
graphically	O
’	O
dependent	O
.	O
intuitively	O
,	O
whilst	O
we	O
believe	O
the	O
root	O
causes	O
are	O
independent	O
,	O
given	O
the	O
value	B
of	O
the	O
observation	O
,	O
this	O
tells	O
us	O
something	O
about	O
the	O
state	O
of	O
both	O
the	O
causes	O
,	O
coupling	O
them	O
and	O
making	O
them	O
(	O
generally	O
)	O
dependent	O
.	O
3.3.3	O
d-separation	O
the	O
dag	O
concepts	O
of	O
d-separation	O
and	O
d-connection	O
are	O
central	O
to	O
determining	O
conditional	B
independence	O
in	O
any	O
bn	O
with	O
structure	O
given	O
by	O
the	O
dag	O
[	O
284	O
]	O
.	O
deﬁnition	O
20	O
(	O
d-connection	O
,	O
d-separation	O
)	O
.	O
if	O
g	O
is	O
a	O
directed	B
graph	O
in	O
which	O
x	O
,	O
y	O
and	O
z	O
are	O
disjoint	O
sets	O
of	O
vertices	O
,	O
then	O
x	O
and	O
y	O
are	O
d-connected	O
by	O
z	O
in	O
g	O
if	O
and	O
only	O
if	O
there	O
exists	O
an	O
undirected	B
path	O
u	O
between	O
some	O
vertex	B
in	O
x	O
and	O
some	O
vertex	B
in	O
y	O
such	O
that	O
for	O
every	O
collider	B
c	O
on	O
u	O
,	O
either	O
c	O
or	O
a	O
descendent	O
of	O
c	O
is	O
in	O
z	O
,	O
and	O
no	O
non-collider	O
on	O
u	O
is	O
in	O
z.	O
x	O
and	O
y	O
are	O
d-separated	O
by	O
z	O
in	O
g	O
if	O
and	O
only	O
if	O
they	O
are	O
not	O
d-connected	O
by	O
z	O
in	O
g.	O
one	O
may	O
also	O
phrase	O
this	O
as	O
follows	O
.	O
for	O
every	O
variable	B
x	O
∈	O
x	O
and	O
y	O
∈	O
y	O
,	O
check	B
every	O
path	B
u	O
between	O
x	O
and	O
y.	O
a	O
path	B
u	O
is	O
said	O
to	O
be	O
blocked	B
if	O
there	O
is	O
a	O
node	B
w	O
on	O
u	O
such	O
that	O
either	O
1.	O
w	O
is	O
a	O
collider	B
and	O
neither	O
w	O
nor	O
any	O
of	O
its	O
descendants	O
is	O
in	O
z	O
.	O
2.	O
w	O
is	O
not	O
a	O
collider	B
on	O
u	O
and	O
w	O
is	O
in	O
z.	O
draft	O
march	O
9	O
,	O
2010	O
35	O
belief	B
networks	I
if	O
all	O
such	O
paths	O
are	O
blocked	B
then	O
x	O
and	O
y	O
are	O
d-separated	O
by	O
z.	O
if	O
the	O
variable	B
sets	O
x	O
and	O
y	O
are	O
d-separated	O
by	O
z	O
,	O
they	O
are	O
independent	O
conditional	O
on	O
z	O
in	O
all	O
probability	B
distributions	O
such	O
a	O
graph	B
can	O
represent	O
.	O
the	O
bayes	O
ball	O
algorithm	B
[	O
241	O
]	O
provides	O
a	O
linear	B
time	O
complexity	O
algorithm	B
which	O
given	O
a	O
set	O
of	O
nodes	O
x	O
and	O
z	O
determines	O
the	O
set	O
of	O
nodes	O
y	O
such	O
that	O
x	O
⊥⊥	O
y|	O
z.	O
y	O
is	O
called	O
the	O
set	O
of	O
irrelevant	O
nodes	O
for	O
x	O
given	O
z	O
.	O
3.3.4	O
d-connection	O
and	O
dependence	B
given	O
a	O
dag	O
we	O
can	O
imply	O
with	O
certainty	O
that	O
two	O
variables	O
are	O
(	O
conditionally	O
)	O
independent	O
,	O
provided	O
they	O
are	O
d-separated	O
.	O
can	O
we	O
infer	O
that	O
they	O
are	O
dependent	O
,	O
provided	O
they	O
are	O
d-connected	O
?	O
consider	O
the	O
following	O
situation	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
=	O
p	O
(	O
c|a	O
,	O
b	O
)	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
(	O
3.3.18	O
)	O
for	O
which	O
we	O
note	O
that	O
a	O
and	O
b	O
are	O
d-connected	O
by	O
c.	O
for	O
concreteness	O
,	O
we	O
assume	O
c	O
is	O
binary	O
with	O
states	O
1,2.	O
the	O
question	O
is	O
whether	O
a	O
and	O
b	O
are	O
dependent	O
,	O
conditioned	O
on	O
c	O
,	O
a⊥⊥	O
b|	O
c.	O
to	O
answer	O
this	O
,	O
consider	O
(	O
3.3.19	O
)	O
p	O
(	O
a	O
,	O
b|c	O
=	O
1	O
)	O
=	O
p	O
(	O
c	O
=	O
1|a	O
,	O
b	O
)	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
a	O
,	O
b	O
p	O
(	O
c	O
=	O
1|a	O
,	O
b	O
)	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
(	O
cid:80	O
)	O
in	O
general	O
,	O
the	O
ﬁrst	O
term	O
p	O
(	O
c	O
=	O
1|a	O
,	O
b	O
)	O
does	O
not	O
need	O
to	O
be	O
a	O
factored	O
function	B
of	O
a	O
and	O
b	O
and	O
therefore	O
a	O
and	O
b	O
are	O
conditionally	O
‘	O
graphically	O
’	O
dependent	O
.	O
however	O
,	O
we	O
can	O
construct	O
cases	O
where	O
this	O
is	O
not	O
so	O
.	O
for	O
example	O
,	O
let	O
p	O
(	O
c	O
=	O
1|a	O
,	O
b	O
)	O
=	O
φ	O
(	O
a	O
)	O
ψ	O
(	O
b	O
)	O
,	O
and	O
p	O
(	O
c	O
=	O
2|a	O
,	O
b	O
)	O
=	O
1	O
−	O
p	O
(	O
c	O
=	O
1|a	O
,	O
b	O
)	O
where	O
φ	O
(	O
a	O
)	O
and	O
ψ	O
(	O
b	O
)	O
are	O
arbitrary	O
potentials	O
between	O
0	O
and	O
1.	O
then	O
(	O
3.3.20	O
)	O
z	O
=	O
(	O
cid:88	O
)	O
φ	O
(	O
a	O
)	O
p	O
(	O
a	O
)	O
(	O
cid:88	O
)	O
a	O
b	O
p	O
(	O
a	O
,	O
b|c	O
=	O
1	O
)	O
=	O
1	O
z	O
φ	O
(	O
a	O
)	O
p	O
(	O
a	O
)	O
ψ	O
(	O
b	O
)	O
p	O
(	O
b	O
)	O
,	O
ψ	O
(	O
b	O
)	O
p	O
(	O
b	O
)	O
(	O
3.3.21	O
)	O
which	O
shows	O
that	O
p	O
(	O
a	O
,	O
b|c	O
=	O
1	O
)	O
is	O
a	O
product	O
of	O
a	O
function	B
in	O
a	O
and	O
function	B
in	O
b	O
,	O
so	O
that	O
a	O
and	O
b	O
are	O
independent	O
,	O
conditioned	O
on	O
c	O
=	O
1.	O
a	O
second	O
example	O
is	O
given	O
by	O
the	O
distribution	B
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
=	O
p	O
(	O
c|b	O
)	O
p	O
(	O
b|a	O
)	O
p	O
(	O
a	O
)	O
(	O
3.3.22	O
)	O
in	O
which	O
a	O
and	O
c	O
are	O
d-connected	O
by	O
b.	O
the	O
question	O
is	O
,	O
are	O
a	O
and	O
c	O
dependent	O
,	O
a⊥⊥	O
c|∅	O
?	O
for	O
simplicity	O
we	O
assume	O
b	O
takes	O
the	O
two	O
states	O
1,2.	O
then	O
p	O
(	O
a	O
,	O
c	O
)	O
=	O
p	O
(	O
a	O
)	O
(	O
cid:88	O
)	O
b	O
p	O
(	O
a	O
,	O
c	O
)	O
=	O
p	O
(	O
a	O
)	O
γ	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
=	O
1|a	O
)	O
p	O
(	O
c|b	O
)	O
p	O
(	O
b|a	O
)	O
=	O
p	O
(	O
a	O
)	O
(	O
p	O
(	O
c|b	O
=	O
1	O
)	O
p	O
(	O
b	O
=	O
1|a	O
)	O
+	O
p	O
(	O
c|b	O
=	O
2	O
)	O
p	O
(	O
b	O
=	O
2|a	O
)	O
)	O
(	O
cid:18	O
)	O
(	O
cid:20	O
)	O
(	O
cid:19	O
)	O
(	O
cid:21	O
)	O
p	O
(	O
c|b	O
=	O
1	O
)	O
+	O
p	O
(	O
c|b	O
=	O
2	O
)	O
p	O
(	O
b	O
=	O
1|a	O
)	O
−	O
1	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
1	O
(	O
cid:18	O
)	O
p	O
(	O
c|b	O
=	O
1	O
)	O
+	O
p	O
(	O
c|b	O
=	O
2	O
)	O
(	O
cid:18	O
)	O
1	O
γ	O
−	O
1	O
for	O
the	O
setting	O
p	O
(	O
b	O
=	O
1|a	O
)	O
=	O
γ	O
,	O
for	O
some	O
constant	O
γ	O
for	O
all	O
states	O
of	O
a	O
,	O
then	O
(	O
3.3.23	O
)	O
(	O
3.3.24	O
)	O
(	O
3.3.25	O
)	O
which	O
is	O
a	O
product	O
of	O
a	O
function	B
of	O
a	O
and	O
a	O
function	B
of	O
c.	O
hence	O
a	O
and	O
c	O
are	O
independent	O
.	O
36	O
draft	O
march	O
9	O
,	O
2010	O
belief	B
networks	I
c	O
e	O
a	O
c	O
e	O
a	O
b	O
t	O
d	O
f	O
s	O
b	O
(	O
a	O
)	O
g	O
(	O
a	O
)	O
b	O
d	O
(	O
b	O
)	O
f	O
s	O
b	O
t	O
g	O
u	O
(	O
b	O
)	O
figure	O
3.8	O
:	O
examples	O
for	O
d-separation	O
.	O
(	O
a	O
)	O
:	O
b	O
d-separates	O
a	O
from	O
e.	O
the	O
joint	B
vari-	O
ables	O
{	O
b	O
,	O
d	O
}	O
d-connect	O
a	O
and	O
e.	O
(	O
b	O
)	O
:	O
c	O
and	O
e	O
are	O
(	O
unconditionally	O
)	O
d-connected	O
.	O
b	O
d-	O
connects	O
a	O
and	O
e.	O
figure	O
3.9	O
:	O
(	O
a	O
)	O
:	O
t	O
and	O
f	O
are	O
d-connected	O
by	O
g.	O
(	O
b	O
)	O
:	O
b	O
and	O
f	O
are	O
d-separated	O
by	O
u.	O
the	O
moral	O
of	O
the	O
story	O
is	O
that	O
d-separation	O
necessarily	O
implies	O
independence	B
.	O
however	O
,	O
d-connection	O
does	O
not	O
necessarily	O
imply	O
dependence	B
.	O
it	O
might	O
be	O
that	O
there	O
are	O
numerical	B
settings	O
for	O
which	O
variables	O
are	O
independent	O
,	O
even	O
though	O
they	O
are	O
d-connected	O
.	O
for	O
this	O
reason	O
we	O
use	O
the	O
term	O
‘	O
graphical	O
’	O
depen-	O
dence	O
when	O
the	O
graph	B
would	O
suggest	O
that	O
variables	O
are	O
dependent	O
,	O
even	O
though	O
there	O
may	O
be	O
numerical	B
instantiations	O
were	O
dependence	B
does	O
not	O
hold	O
,	O
see	O
deﬁnition	O
(	O
21	O
)	O
.	O
example	O
12.	O
consider	O
ﬁg	O
(	O
3.8a	O
)	O
.	O
is	O
a	O
⊥⊥	O
e|	O
b	O
?	O
if	O
we	O
sum	O
out	O
variable	B
d	O
,	O
then	O
we	O
see	O
that	O
a	O
and	O
e	O
are	O
independent	O
given	O
b	O
,	O
since	O
the	O
variable	B
e	O
will	O
appear	O
as	O
an	O
isolated	O
factor	B
independent	O
of	O
all	O
other	O
variables	O
,	O
hence	O
indeed	O
a⊥⊥	O
e|	O
b.	O
whilst	O
b	O
is	O
a	O
collider	B
which	O
is	O
in	O
the	O
conditioning	B
set	O
,	O
we	O
need	O
all	O
colliders	O
on	O
the	O
path	B
to	O
be	O
in	O
the	O
conditioning	B
set	O
(	O
or	O
their	O
descendants	O
)	O
for	O
d-connectedness	O
.	O
in	O
ﬁg	O
(	O
3.8b	O
)	O
,	O
if	O
we	O
sum	O
out	O
variable	B
d	O
,	O
then	O
c	O
and	O
e	O
become	O
intrinsically	O
linked	O
and	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
e	O
)	O
will	O
not	O
factorise	O
into	O
a	O
function	B
of	O
a	O
multiplied	O
by	O
a	O
function	B
of	O
e	O
–	O
hence	O
they	O
are	O
dependent	O
.	O
example	O
13.	O
consider	O
the	O
graph	B
in	O
ﬁg	O
(	O
3.9a	O
)	O
.	O
1.	O
are	O
the	O
variables	O
t	O
and	O
f	O
unconditionally	O
independent	O
,	O
i.e	O
.	O
t	O
⊥⊥	O
f|	O
∅	O
?	O
here	O
there	O
are	O
two	O
colliders	O
,	O
namely	O
g	O
and	O
s	O
–	O
however	O
,	O
these	O
are	O
not	O
in	O
the	O
conditioning	B
set	O
(	O
which	O
is	O
empty	O
)	O
,	O
and	O
hence	O
they	O
are	O
d-separated	O
and	O
therefore	O
unconditionally	O
independent	O
.	O
2.	O
what	O
about	O
t⊥⊥	O
f|	O
g	O
?	O
there	O
is	O
a	O
collider	B
on	O
the	O
path	B
between	O
t	O
and	O
f	O
which	O
is	O
in	O
the	O
conditioning	B
set	O
.	O
hence	O
t	O
and	O
f	O
are	O
d-connected	O
by	O
g	O
,	O
and	O
therefore	O
t	O
and	O
f	O
are	O
not	O
independent	O
conditioned	O
on	O
g.	O
3.	O
what	O
about	O
b⊥⊥	O
f|	O
s	O
?	O
since	O
there	O
is	O
a	O
collider	B
s	O
in	O
the	O
conditioning	B
set	O
on	O
the	O
path	B
between	O
t	O
and	O
f	O
,	O
then	O
b	O
and	O
f	O
are	O
‘	O
graphically	O
’	O
conditionally	O
dependent	O
given	O
s.	O
example	O
14.	O
is	O
{	O
b	O
,	O
f	O
}	O
⊥⊥	O
u|	O
∅	O
?	O
in	O
ﬁg	O
(	O
3.9b	O
)	O
.	O
since	O
the	O
conditioning	B
set	O
is	O
empty	O
and	O
every	O
path	B
from	O
either	O
b	O
or	O
f	O
to	O
u	O
contains	O
a	O
collider	B
,	O
b	O
,	O
f	O
are	O
unconditionally	O
independent	O
of	O
u	O
.	O
3.3.5	O
markov	O
equivalence	O
in	O
belief	B
networks	I
draft	O
march	O
9	O
,	O
2010	O
37	O
deﬁnition	O
21	O
(	O
some	O
properties	B
of	O
belief	B
networks	I
)	O
.	O
belief	B
networks	I
b	O
b	O
b	O
b	O
b	O
b	O
b	O
a	O
a	O
a	O
a	O
a	O
a	O
a	O
c	O
c	O
c	O
c	O
c	O
c	O
c	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
=	O
p	O
(	O
c|a	O
,	O
b	O
)	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
(	O
3.3.26	O
)	O
a	O
and	O
b	O
are	O
(	O
unconditionally	O
)	O
independent	O
:	O
p	O
(	O
a	O
,	O
b	O
)	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
.	O
a	O
and	O
b	O
are	O
conditionally	O
dependent	O
on	O
c	O
:	O
p	O
(	O
a	O
,	O
b|c	O
)	O
(	O
cid:54	O
)	O
=	O
p	O
(	O
a|c	O
)	O
p	O
(	O
b|c	O
)	O
.	O
→	O
a	O
b	O
marginalising	O
over	O
c	O
makes	O
a	O
and	O
b	O
independent	O
.	O
→	O
a	O
b	O
conditioning	B
on	O
c	O
makes	O
a	O
and	O
b	O
(	O
graphically	O
)	O
de-	O
pendent	O
.	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
=	O
p	O
(	O
a|c	O
)	O
p	O
(	O
b|c	O
)	O
p	O
(	O
c	O
)	O
(	O
3.3.27	O
)	O
a	O
and	O
b	O
are	O
(	O
unconditionally	O
)	O
dependent	O
:	O
p	O
(	O
a	O
,	O
b	O
)	O
(	O
cid:54	O
)	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
.	O
a	O
and	O
b	O
are	O
conditionally	O
independent	O
on	O
c	O
:	O
p	O
(	O
a	O
,	O
b|c	O
)	O
=	O
p	O
(	O
a|c	O
)	O
p	O
(	O
b|c	O
)	O
.	O
→	O
a	O
marginalising	O
over	O
c	O
makes	O
a	O
and	O
b	O
(	O
graphically	O
)	O
dependent	O
.	O
b	O
→	O
a	O
=	O
a	O
=	O
b	O
b	O
conditioning	B
on	O
c	O
makes	O
a	O
and	O
b	O
independent	O
.	O
=	O
a	O
b	O
c	O
c	O
deﬁnition	O
22	O
(	O
markov	O
equivalence	O
)	O
.	O
two	O
graphs	O
are	O
markov	O
equivalent	B
if	O
they	O
both	O
represent	O
the	O
same	O
set	O
of	O
conditional	B
independence	O
statements	O
.	O
deﬁne	O
the	O
skeleton	B
of	O
a	O
graph	B
by	O
removing	O
the	O
directions	O
on	O
the	O
arrows	O
.	O
deﬁne	O
an	O
immorality	B
in	O
a	O
dag	O
as	O
a	O
conﬁguration	O
of	O
three	O
nodes	O
,	O
a	O
,	O
b	O
,	O
c	O
such	O
that	O
c	O
is	O
a	O
child	O
of	O
both	O
a	O
and	O
b	O
,	O
with	O
a	O
and	O
b	O
not	O
directly	O
connected	B
.	O
two	O
dags	O
represent	O
the	O
same	O
set	O
of	O
independence	B
assumptions	O
(	O
they	O
are	O
markov	O
equivalent	B
)	O
if	O
and	O
only	O
if	O
they	O
have	O
the	O
same	O
skeleton	B
and	O
the	O
same	O
set	O
of	O
immoralities	O
[	O
74	O
]	O
.	O
using	O
this	O
rule	O
we	O
see	O
that	O
in	O
ﬁg	O
(	O
3.5	O
)	O
,	O
bns	O
(	O
b	O
,	O
c	O
,	O
d	O
)	O
have	O
the	O
same	O
skeleton	B
with	O
no	O
immoralities	O
and	O
are	O
therefore	O
equivalent	B
.	O
however	O
bn	O
(	O
a	O
)	O
has	O
an	O
immorality	B
and	O
is	O
therefore	O
not	O
equivalent	B
to	O
dags	O
(	O
b	O
,	O
c	O
,	O
d	O
)	O
.	O
38	O
draft	O
march	O
9	O
,	O
2010	O
causality	B
3.3.6	O
belief	B
networks	I
have	O
limited	O
expressibility	O
t1	O
y1	O
t2	O
y2	O
h	O
(	O
a	O
)	O
t2	O
y2	O
t1	O
y1	O
(	O
b	O
)	O
figure	O
3.10	O
:	O
(	O
a	O
)	O
:	O
two	O
treatments	O
t1	O
,	O
t2	O
and	O
correspond-	O
ing	O
outcomes	O
y1	O
,	O
y2	O
.	O
the	O
health	O
of	O
a	O
patient	O
is	O
represented	O
by	O
h.	O
this	O
dag	O
embodies	O
the	O
conditional	B
independence	O
statements	O
t1	O
⊥⊥	O
t2	O
,	O
y2	O
|	O
∅	O
,	O
t2	O
⊥⊥	O
t1	O
,	O
y1	O
|	O
∅	O
,	O
namely	O
that	O
the	O
treatments	O
have	O
no	O
eﬀect	O
on	O
each	O
other	O
.	O
(	O
b	O
)	O
:	O
one	O
could	O
represent	O
the	O
marginalised	O
latent	B
variable	I
using	O
a	O
bi-directional	O
edge	O
.	O
consider	O
the	O
dag	O
in	O
ﬁg	O
(	O
3.10a	O
)	O
,	O
(	O
from	O
[	O
232	O
]	O
)	O
.	O
this	O
dag	O
could	O
be	O
used	O
to	O
represent	O
two	O
successive	O
experiments	O
where	O
t1	O
and	O
t2	O
are	O
two	O
treatments	O
and	O
y1	O
and	O
y2	O
represent	O
two	O
outcomes	O
of	O
interest	O
;	O
h	O
is	O
the	O
underlying	O
health	O
status	O
of	O
the	O
patient	O
;	O
the	O
ﬁrst	O
treatment	O
has	O
no	O
eﬀect	O
on	O
the	O
second	O
outcome	O
hence	O
there	O
is	O
no	O
edge	O
from	O
y1	O
to	O
y2	O
.	O
now	O
consider	O
the	O
implied	O
independencies	O
in	O
the	O
marginal	B
distribution	O
p	O
(	O
t1	O
,	O
t2	O
,	O
y1	O
,	O
y2	O
)	O
,	O
obtained	O
by	O
marginalising	O
the	O
full	O
distribution	B
over	O
h.	O
there	O
is	O
no	O
dag	O
containing	O
only	O
the	O
vertices	O
t1	O
,	O
y1	O
,	O
t2	O
,	O
y2	O
which	O
represents	O
the	O
independence	B
relations	O
and	O
does	O
not	O
also	O
imply	O
some	O
other	O
independence	B
relation	O
that	O
is	O
not	O
implied	O
by	O
ﬁg	O
(	O
3.10a	O
)	O
.	O
consequently	O
,	O
any	O
dag	O
on	O
vertices	O
t1	O
,	O
y1	O
,	O
t2	O
,	O
y2	O
alone	O
will	O
either	O
fail	O
to	O
represent	O
an	O
independence	B
relation	O
of	O
p	O
(	O
t1	O
,	O
t2	O
,	O
y1	O
,	O
y2	O
)	O
,	O
or	O
will	O
impose	O
some	O
additional	O
independence	B
restriction	O
that	O
is	O
not	O
implied	O
by	O
the	O
dag	O
.	O
in	O
the	O
above	O
example	O
p	O
(	O
t1	O
,	O
t2	O
,	O
y1	O
,	O
y2	O
)	O
=	O
p	O
(	O
t1	O
)	O
p	O
(	O
t2	O
)	O
(	O
cid:88	O
)	O
h	O
p	O
(	O
y1|t1	O
,	O
h	O
)	O
p	O
(	O
y2|t2	O
,	O
h	O
)	O
p	O
(	O
h	O
)	O
(	O
3.3.28	O
)	O
can	O
not	O
in	O
general	O
be	O
expressed	O
as	O
a	O
product	O
of	O
functions	O
deﬁned	O
on	O
a	O
limited	O
set	O
of	O
the	O
variables	O
.	O
however	O
,	O
it	O
is	O
the	O
case	O
that	O
the	O
conditional	B
independence	O
conditions	O
t1⊥⊥	O
t2	O
,	O
y2|∅	O
,	O
t2⊥⊥	O
t1	O
,	O
y1|∅	O
hold	O
in	O
p	O
(	O
t1	O
,	O
t2	O
,	O
y1	O
,	O
y2	O
)	O
–	O
they	O
are	O
there	O
,	O
encoded	O
in	O
the	O
form	O
of	O
the	O
conditional	B
probability	I
tables	O
.	O
it	O
is	O
just	O
that	O
we	O
can	O
not	O
‘	O
see	O
’	O
this	O
independence	B
since	O
it	O
is	O
not	O
present	O
in	O
the	O
structure	B
of	O
the	O
marginalised	O
graph	B
(	O
though	O
one	O
can	O
naturally	O
infer	O
this	O
in	O
the	O
larger	O
graph	B
p	O
(	O
t1	O
,	O
t2	O
,	O
y1	O
,	O
y2	O
,	O
h	O
)	O
)	O
.	O
this	O
example	O
demonstrates	O
that	O
bns	O
can	O
not	O
express	O
all	O
the	O
conditional	B
independence	O
statements	O
that	O
could	O
be	O
made	O
on	O
that	O
set	O
of	O
variables	O
(	O
the	O
set	O
of	O
conditional	B
independence	O
statements	O
can	O
be	O
increased	O
by	O
considering	O
extra	O
latent	B
variables	O
however	O
)	O
.	O
this	O
situation	O
is	O
rather	O
general	O
in	O
the	O
sense	O
that	O
any	O
graphical	O
model	B
has	O
limited	O
expressibility	O
in	O
terms	O
of	O
independence	B
statements	O
[	O
265	O
]	O
.	O
it	O
is	O
worth	O
bearing	O
in	O
mind	O
that	O
belief	B
networks	I
may	O
not	O
always	O
be	O
the	O
most	O
appropriate	O
framework	O
to	O
express	O
one	O
’	O
s	O
independence	B
assumptions	O
and	O
intuitions	O
.	O
a	O
natural	B
consideration	O
is	O
to	O
use	O
a	O
bi-directional	O
arrow	O
when	O
a	O
latent	B
variable	I
is	O
marginalised	O
.	O
for	O
ﬁg	O
(	O
3.10a	O
)	O
,	O
one	O
could	O
depict	O
the	O
marginal	B
distribution	O
using	O
a	O
bi-directional	O
edge	O
,	O
ﬁg	O
(	O
3.10b	O
)	O
.	O
similarly	O
a	O
bn	O
with	O
a	O
latent	B
conditioned	O
variable	B
can	O
be	O
represented	O
using	O
an	O
undirected	B
edge	O
.	O
for	O
a	O
discussion	O
of	O
these	O
and	O
related	O
issues	O
,	O
see	O
[	O
232	O
]	O
.	O
3.4	O
causality	B
causality	O
is	O
a	O
contentious	O
topic	O
and	O
the	O
purpose	O
of	O
this	O
section	O
is	O
make	O
the	O
reader	O
aware	O
of	O
some	O
pitfalls	O
that	O
can	O
occur	O
and	O
which	O
may	O
give	O
rise	O
to	O
erroneous	O
inferences	O
.	O
the	O
reader	O
is	O
referred	O
to	O
[	O
220	O
]	O
and	O
[	O
74	O
]	O
for	O
further	O
details	O
.	O
the	O
word	O
‘	O
causal	B
’	O
is	O
contentious	O
particularly	O
in	O
cases	O
where	O
the	O
model	B
of	O
the	O
data	B
contains	O
no	O
explicit	O
temporal	O
information	O
,	O
so	O
that	O
formally	O
only	O
correlations	O
or	O
dependencies	O
can	O
be	O
inferred	O
.	O
for	O
a	O
distri-	O
bution	O
p	O
(	O
a	O
,	O
b	O
)	O
,	O
we	O
could	O
write	O
this	O
as	O
either	O
(	O
i	O
)	O
p	O
(	O
a|b	O
)	O
p	O
(	O
b	O
)	O
or	O
(	O
ii	O
)	O
p	O
(	O
b|a	O
)	O
p	O
(	O
a	O
)	O
.	O
in	O
(	O
i	O
)	O
we	O
might	O
think	O
that	O
b	O
‘	O
causes	O
’	O
a	O
,	O
and	O
in	O
(	O
ii	O
)	O
a	O
‘	O
causes	O
’	O
b.	O
clearly	O
,	O
this	O
is	O
not	O
very	O
meaningful	O
since	O
they	O
both	O
represent	O
exactly	O
the	O
same	O
distribution	B
.	O
formally	O
belief	B
networks	I
only	O
make	O
independence	B
statements	O
,	O
not	O
causal	B
ones	O
.	O
nevertheless	O
,	O
in	O
constructing	O
bns	O
,	O
it	O
can	O
be	O
helpful	O
to	O
think	O
about	O
dependencies	O
in	O
terms	O
of	O
causation	O
draft	O
march	O
9	O
,	O
2010	O
39	O
causality	B
a	O
b	O
a	O
b	O
r	O
w	O
r	O
w	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
(	O
d	O
)	O
(	O
c	O
)	O
:	O
figure	O
3.11	O
:	O
both	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
represent	O
the	O
same	O
distribution	B
p	O
(	O
a	O
,	O
b	O
)	O
=	O
p	O
(	O
a|b	O
)	O
p	O
(	O
b	O
)	O
=	O
p	O
(	O
b|a	O
)	O
p	O
(	O
a	O
)	O
.	O
the	O
graph	B
represents	O
p	O
(	O
rain	O
,	O
grasswet	O
)	O
=	O
p	O
(	O
grasswet|rain	O
)	O
p	O
(	O
rain	O
)	O
.	O
(	O
d	O
)	O
:	O
we	O
could	O
equally	O
have	O
written	O
p	O
(	O
rain|grasswet	O
)	O
p	O
(	O
grasswet	O
)	O
,	O
although	O
this	O
appears	O
to	O
be	O
causally	O
non-sensical	O
.	O
g	O
d	O
r	O
fd	O
(	O
a	O
)	O
g	O
d	O
(	O
b	O
)	O
(	O
a	O
)	O
:	O
a	O
dag	O
for	O
the	O
relation	O
be-	O
figure	O
3.12	O
:	O
tween	O
gender	O
(	O
g	O
)	O
,	O
drug	O
(	O
d	O
)	O
and	O
recovery	O
(	O
r	O
)	O
,	O
see	O
(	O
b	O
)	O
:	O
inﬂuence	B
diagram	I
.	O
no	O
decision	O
table	O
(	O
3.1	O
)	O
.	O
variable	B
is	O
required	O
for	O
g	O
since	O
g	O
has	O
no	O
parents	O
.	O
r	O
since	O
our	O
intuitive	O
understanding	O
is	O
usually	O
framed	O
in	O
how	O
one	O
variable	B
‘	O
inﬂuences	O
’	O
another	O
.	O
first	O
we	O
discuss	O
a	O
classic	O
conundrum	O
that	O
highlights	O
potential	B
pitfalls	O
that	O
can	O
arise	O
.	O
3.4.1	O
simpson	O
’	O
s	O
paradox	O
simpson	O
’	O
s	O
‘	O
paradox	O
’	O
is	O
a	O
cautionary	O
tale	O
in	O
causal	B
reasoning	O
in	O
bns	O
.	O
consider	O
a	O
medical	O
trial	O
in	O
which	O
patient	O
treatment	O
and	O
outcome	O
are	O
recovered	O
.	O
two	O
trials	O
were	O
conducted	O
,	O
one	O
with	O
40	O
females	O
and	O
one	O
with	O
40	O
males	O
.	O
the	O
data	B
is	O
summarised	O
in	O
table	O
(	O
3.1	O
)	O
.	O
the	O
question	O
is	O
:	O
does	O
the	O
drug	O
cause	O
increased	O
recovery	O
?	O
according	O
to	O
the	O
table	O
for	O
males	O
,	O
the	O
answer	O
is	O
no	O
,	O
since	O
more	O
males	O
recovered	O
when	O
they	O
were	O
not	O
given	O
the	O
drug	O
than	O
when	O
they	O
were	O
.	O
similarly	O
,	O
more	O
females	O
recovered	O
when	O
not	O
given	O
the	O
drug	O
than	O
recovered	O
when	O
given	O
the	O
drug	O
.	O
the	O
conclusion	O
appears	O
that	O
the	O
drug	O
can	O
not	O
be	O
beneﬁcial	O
since	O
it	O
aids	O
neither	O
subpopulation	O
.	O
however	O
,	O
ignoring	O
the	O
gender	O
information	O
,	O
and	O
collating	O
both	O
the	O
male	O
and	O
female	O
data	B
into	O
one	O
combined	O
table	O
,	O
we	O
ﬁnd	O
that	O
more	O
people	O
recovered	O
when	O
given	O
the	O
drug	O
than	O
when	O
not	O
.	O
hence	O
,	O
even	O
though	O
the	O
drug	O
doesn	O
’	O
t	O
seem	O
to	O
work	O
for	O
either	O
males	O
or	O
females	O
,	O
it	O
does	O
seem	O
to	O
work	O
overall	O
!	O
should	O
we	O
therefore	O
recommend	O
the	O
drug	O
or	O
not	O
?	O
resolution	B
of	O
the	O
paradox	O
the	O
‘	O
paradox	O
’	O
occurs	O
since	O
we	O
are	O
asking	O
a	O
causal	B
(	O
or	O
interventional	O
)	O
question	O
.	O
the	O
question	O
we	O
are	O
in-	O
tuitively	O
asking	O
is	O
,	O
if	O
we	O
give	O
someone	O
the	O
drug	O
,	O
what	O
happens	O
?	O
however	O
,	O
the	O
calculation	O
we	O
performed	O
above	O
was	O
only	O
an	O
observational	O
calculation	O
.	O
the	O
calculation	O
we	O
really	O
want	O
is	O
to	O
ﬁrst	O
intervene	O
,	O
setting	O
males	O
recovered	O
not	O
recovered	O
rec	O
.	O
rate	O
given	O
drug	O
not	O
given	O
drug	O
females	O
given	O
drug	O
not	O
given	O
drug	O
combined	O
given	O
drug	O
not	O
given	O
drug	O
18	O
7	O
12	O
3	O
60	O
%	O
70	O
%	O
recovered	O
not	O
recovered	O
rec	O
.	O
rate	O
2	O
9	O
8	O
21	O
20	O
%	O
30	O
%	O
recovered	O
not	O
recovered	O
rec	O
.	O
rate	O
20	O
16	O
20	O
24	O
50	O
%	O
40	O
%	O
table	O
3.1	O
:	O
table	O
for	O
simpson	O
’	O
s	O
paradox	O
(	O
from	O
[	O
220	O
]	O
)	O
40	O
draft	O
march	O
9	O
,	O
2010	O
causality	B
the	O
drug	O
state	O
,	O
and	O
then	O
observe	O
what	O
eﬀect	O
this	O
has	O
on	O
recovery	O
.	O
pearl	O
[	O
220	O
]	O
describes	O
this	O
as	O
the	O
diﬀerence	O
between	O
‘	O
given	O
that	O
we	O
see	O
’	O
(	O
observational	O
evidence	B
)	O
,	O
versus	O
‘	O
given	O
that	O
we	O
do	O
’	O
(	O
interventional	O
evidence	B
)	O
.	O
a	O
model	B
of	O
the	O
gender	O
,	O
drug	O
and	O
recovery	O
data	B
(	O
which	O
makes	O
no	O
conditional	O
independence	B
assumptions	O
)	O
is	O
p	O
(	O
g	O
,	O
d	O
,	O
r	O
)	O
=	O
p	O
(	O
r|g	O
,	O
d	O
)	O
p	O
(	O
d|g	O
)	O
p	O
(	O
g	O
)	O
(	O
3.4.1	O
)	O
an	O
observational	O
calculation	O
concerns	O
computing	O
p	O
(	O
r|g	O
,	O
d	O
)	O
and	O
p	O
(	O
r|d	O
)	O
.	O
interpretation	O
,	O
however	O
,	O
if	O
we	O
intervene	O
and	O
give	O
the	O
drug	O
,	O
then	O
the	O
term	O
p	O
(	O
d|g	O
)	O
in	O
equation	B
(	O
3.4.1	O
)	O
should	O
play	O
no	O
role	O
in	O
the	O
experiment	O
(	O
otherwise	O
the	O
distribution	B
models	O
that	O
given	O
the	O
gender	O
we	O
select	O
a	O
drug	O
with	O
probability	O
p	O
(	O
d|g	O
)	O
,	O
which	O
is	O
not	O
the	O
case	O
–	O
we	O
decide	O
to	O
give	O
the	O
drug	O
or	O
not	O
,	O
independent	O
of	O
gender	O
)	O
.	O
in	O
the	O
causal	B
case	O
we	O
are	O
modelling	B
the	O
causal	B
experiment	O
;	O
in	O
this	O
case	O
the	O
term	O
p	O
(	O
d|g	O
)	O
needs	O
to	O
be	O
replaced	O
by	O
a	O
term	O
that	O
reﬂects	O
the	O
setup	O
of	O
the	O
experiment	O
.	O
in	O
an	O
atomic	O
intervention	O
a	O
single	O
variable	B
is	O
set	O
in	O
a	O
particular	O
state5	O
.	O
in	O
our	O
atomic	O
causal	B
intervention	O
in	O
setting	O
d	O
,	O
we	O
are	O
dealing	O
with	O
the	O
modiﬁed	O
distribution	B
in	O
a	O
causal	B
˜p	O
(	O
g	O
,	O
r|d	O
)	O
=	O
p	O
(	O
r|g	O
,	O
d	O
)	O
p	O
(	O
g	O
)	O
(	O
3.4.2	O
)	O
where	O
the	O
terms	O
on	O
the	O
right	O
hand	O
side	O
of	O
this	O
equation	B
are	O
taken	O
from	O
the	O
original	O
bn	O
of	O
the	O
data	B
.	O
to	O
denote	O
an	O
intervention	O
we	O
use	O
||	O
:	O
(	O
cid:80	O
)	O
p	O
(	O
r||g	O
,	O
d	O
)	O
≡	O
˜p	O
(	O
r|g	O
,	O
d	O
)	O
=	O
p	O
(	O
r|g	O
,	O
d	O
)	O
p	O
(	O
g	O
)	O
r	O
p	O
(	O
r|g	O
,	O
d	O
)	O
p	O
(	O
g	O
)	O
=	O
p	O
(	O
r|g	O
,	O
d	O
)	O
(	O
3.4.3	O
)	O
(	O
one	O
can	O
also	O
consider	O
here	O
g	O
as	O
being	O
interventional	O
–	O
in	O
this	O
case	O
it	O
doesn	O
’	O
t	O
matter	O
since	O
the	O
fact	O
that	O
the	O
variable	B
g	O
has	O
no	O
parents	O
means	O
that	O
for	O
any	O
distribution	B
conditional	O
on	O
g	O
,	O
the	O
prior	B
factor	O
p	O
(	O
g	O
)	O
will	O
not	O
be	O
present	O
)	O
.	O
using	O
equation	B
(	O
3.4.3	O
)	O
,	O
for	O
the	O
males	O
given	O
the	O
drug	O
60	O
%	O
recover	O
,	O
versus	O
70	O
%	O
recovery	O
when	O
not	O
given	O
the	O
drug	O
.	O
for	O
the	O
females	O
given	O
the	O
drug	O
20	O
%	O
recover	O
,	O
versus	O
30	O
%	O
recovery	O
when	O
not	O
given	O
the	O
drug	O
.	O
similarly	O
,	O
p	O
(	O
r||d	O
)	O
≡	O
˜p	O
(	O
r|d	O
)	O
=	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
g	O
p	O
(	O
r|g	O
,	O
d	O
)	O
p	O
(	O
g	O
)	O
r	O
,	O
g	O
p	O
(	O
r|g	O
,	O
d	O
)	O
p	O
(	O
g	O
)	O
=	O
(	O
cid:88	O
)	O
g	O
p	O
(	O
r|g	O
,	O
d	O
)	O
p	O
(	O
g	O
)	O
using	O
the	O
above	O
post	B
intervention	I
distribution	I
we	O
have	O
p	O
(	O
recovery|drug	O
)	O
=	O
0.6	O
×	O
0.5	O
+	O
0.2	O
×	O
0.5	O
=	O
0.4	O
and	O
p	O
(	O
recovery|no	O
drug	O
)	O
=	O
0.7	O
×	O
0.5	O
+	O
0.3	O
×	O
0.5	O
=	O
0.5	O
(	O
3.4.4	O
)	O
(	O
3.4.5	O
)	O
(	O
3.4.6	O
)	O
hence	O
we	O
correctly	O
infer	O
that	O
the	O
drug	O
is	O
overall	O
not	O
helpful	O
,	O
as	O
we	O
intuitively	O
expect	O
,	O
and	O
is	O
consistent	B
with	O
the	O
results	O
from	O
both	O
subpopulations	O
.	O
here	O
p	O
(	O
r||d	O
)	O
means	O
that	O
we	O
ﬁrst	O
choose	O
either	O
a	O
male	O
or	O
female	O
patient	O
at	O
random	O
,	O
and	O
then	O
give	O
them	O
the	O
drug	O
,	O
or	O
not	O
depending	O
on	O
the	O
state	O
of	O
d.	O
the	O
point	O
is	O
that	O
we	O
do	O
not	O
randomly	O
decide	O
whether	O
or	O
not	O
to	O
give	O
the	O
drug	O
,	O
hence	O
the	O
absence	O
of	O
the	O
term	O
p	O
(	O
d|g	O
)	O
from	O
the	O
joint	B
distribution	O
.	O
one	O
way	O
to	O
think	O
about	O
such	O
models	O
is	O
to	O
consider	O
how	O
to	O
draw	O
a	O
sample	B
from	O
the	O
joint	B
distribution	O
of	O
the	O
random	O
variables	O
–	O
in	O
most	O
cases	O
this	O
should	O
clarify	O
the	O
role	O
of	O
causality	B
in	O
the	O
experiment	O
.	O
in	O
contrast	O
to	O
the	O
interventional	O
calculation	O
,	O
the	O
observational	O
calculation	O
makes	O
no	O
conditional	O
indepen-	O
dence	O
assumptions	O
.	O
this	O
means	O
that	O
,	O
for	O
example	O
,	O
the	O
term	O
p	O
(	O
d|g	O
)	O
plays	O
a	O
role	O
in	O
the	O
calculation	O
(	O
the	O
reader	O
might	O
wish	O
to	O
verify	O
that	O
the	O
result	O
given	O
in	O
the	O
combined	O
data	B
in	O
table	O
(	O
3.1	O
)	O
is	O
equivalent	B
to	O
inferring	O
with	O
the	O
full	O
distribution	B
equation	O
(	O
3.4.1	O
)	O
)	O
.	O
5more	O
general	O
experimental	O
conditions	O
can	O
be	O
modelled	O
by	O
replacing	O
p	O
(	O
d|g	O
)	O
by	O
an	O
intervention	O
distribution	B
π	O
(	O
d|g	O
)	O
draft	O
march	O
9	O
,	O
2010	O
41	O
deﬁnition	O
23	O
(	O
pearl	O
’	O
s	O
do	O
operator	O
)	O
.	O
causality	B
in	O
a	O
causal	B
inference	O
,	O
in	O
which	O
the	O
eﬀect	O
of	O
setting	O
variables	O
xc1	O
,	O
.	O
.	O
.	O
,	O
xck	O
,	O
ck	O
∈	O
c	O
,	O
in	O
states	O
xc1	O
,	O
.	O
.	O
.	O
,	O
xck	O
,	O
is	O
to	O
be	O
inferred	O
,	O
this	O
is	O
equivalent	B
to	O
standard	O
evidential	O
inference	B
in	O
the	O
post	B
intervention	I
distribution	I
:	O
(	O
cid:81	O
)	O
k	O
p	O
(	O
x|do	O
(	O
xc1	O
=	O
xc1	O
)	O
,	O
.	O
.	O
.	O
,	O
do	O
(	O
xck	O
=	O
xck	O
)	O
)	O
=	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn|xc1	O
,	O
.	O
.	O
.	O
,	O
xck	O
)	O
i=1	O
p	O
(	O
xci|pa	O
(	O
xci	O
)	O
)	O
p	O
(	O
xj|pa	O
(	O
xj	O
)	O
)	O
(	O
3.4.7	O
)	O
=	O
(	O
cid:89	O
)	O
j	O
(	O
cid:54	O
)	O
∈c	O
where	O
any	O
parental	O
states	O
of	O
pa	O
(	O
xj	O
)	O
of	O
xj	O
are	O
set	O
in	O
their	O
evidential	O
states	O
.	O
an	O
alternative	O
notation	O
is	O
p	O
(	O
x||xc1	O
,	O
.	O
.	O
.	O
,	O
xck	O
)	O
.	O
in	O
words	O
,	O
for	O
those	O
variables	O
for	O
which	O
we	O
causally	O
intervene	O
and	O
set	O
in	O
a	O
particular	O
state	O
,	O
the	O
corresponding	O
terms	O
p	O
(	O
xci|pa	O
(	O
xci	O
)	O
)	O
are	O
removed	O
from	O
the	O
original	O
belief	B
network	I
.	O
for	O
variables	O
which	O
are	O
evidential	O
but	O
non-causal	O
,	O
the	O
corresponding	O
factors	O
are	O
not	O
removed	O
from	O
the	O
distribution	B
.	O
the	O
interpretation	O
is	O
that	O
the	O
post	B
intervention	I
distribution	I
corresponds	O
to	O
an	O
experiment	O
in	O
which	O
the	O
causal	B
variables	O
are	O
ﬁrst	O
set	O
and	O
non-causal	O
variables	O
are	O
subsequently	O
observed	O
.	O
3.4.2	O
inﬂuence	B
diagrams	I
and	O
the	O
do-calculus	O
in	O
making	O
causal	B
inferences	O
we	O
must	O
adjust	O
the	O
model	B
to	O
reﬂect	O
any	O
causal	B
experimental	O
conditions	O
.	O
in	O
setting	O
any	O
variable	B
into	O
a	O
particular	O
state	O
we	O
need	O
to	O
surgically	O
remove	O
all	O
parental	O
links	O
of	O
that	O
vari-	O
able	O
.	O
pearl	O
calls	O
this	O
the	O
do	O
operator	O
,	O
and	O
contrasts	O
an	O
observational	O
(	O
‘	O
see	O
’	O
)	O
inference	B
p	O
(	O
x|y	O
)	O
with	O
a	O
causal	B
(	O
‘	O
make	O
’	O
or	O
‘	O
do	O
’	O
)	O
inference	B
p	O
(	O
x|do	O
(	O
y	O
)	O
)	O
.	O
a	O
useful	O
alternative	O
representation	B
is	O
to	O
append	O
variables	O
x	O
upon	O
which	O
an	O
intervention	O
can	O
possibly	O
be	O
made	O
with	O
a	O
parental	O
decision	O
variable	O
fx	O
[	O
74	O
]	O
.	O
for	O
example6	O
˜p	O
(	O
d	O
,	O
g	O
,	O
r	O
,	O
fd	O
)	O
=	O
p	O
(	O
d|fd	O
,	O
g	O
)	O
p	O
(	O
g	O
)	O
p	O
(	O
r|g	O
,	O
d	O
)	O
p	O
(	O
fd	O
)	O
(	O
3.4.8	O
)	O
where	O
p	O
(	O
d|fd	O
=	O
∅	O
,	O
g	O
)	O
≡	O
p	O
(	O
d|pa	O
(	O
d	O
)	O
)	O
p	O
(	O
d|fd	O
=	O
d	O
,	O
g	O
)	O
=	O
1	O
for	O
d	O
=	O
d	O
and	O
0	O
otherwise	O
hence	O
,	O
if	O
the	O
decision	O
variable	O
fd	O
is	O
set	O
to	O
the	O
empty	O
state	O
,	O
the	O
variable	B
d	O
is	O
determined	O
by	O
the	O
standard	O
observational	O
term	O
p	O
(	O
d|pa	O
(	O
d	O
)	O
)	O
.	O
if	O
the	O
decision	O
variable	O
is	O
set	O
to	O
a	O
state	O
of	O
d	O
,	O
then	O
the	O
variable	B
puts	O
all	O
its	O
probability	B
in	O
that	O
single	O
state	O
of	O
d	O
=	O
d.	O
this	O
has	O
the	O
eﬀect	O
of	O
replacing	O
the	O
conditional	B
probability	I
term	O
a	O
unit	O
factor	O
and	O
any	O
instances	O
of	O
d	O
set	O
to	O
the	O
variable	B
in	O
its	O
interventional	O
state7	O
.	O
a	O
potential	B
advantage	O
of	O
the	O
inﬂuence	B
diagram	I
approach	O
over	O
the	O
do-calculus	O
is	O
that	O
deriving	O
conditional	B
independence	O
statements	O
can	O
be	O
made	O
based	O
on	O
standard	O
techniques	O
for	O
the	O
augmented	B
bn	O
.	O
additionally	O
,	O
for	O
parameter	B
learning	O
,	O
standard	O
techniques	O
apply	O
in	O
which	O
the	O
decision	O
variables	O
are	O
set	O
to	O
the	O
condition	O
under	O
which	O
each	O
data	B
sample	O
was	O
collected	O
(	O
a	O
causal	B
or	O
non-causal	O
sample	B
)	O
.	O
example	O
15	O
(	O
drivers	O
and	O
accidents	O
:	O
a	O
causal	B
belief	O
network	O
)	O
.	O
6here	O
the	O
inﬂuence	B
diagram	I
is	O
a	O
distribution	B
over	O
variables	O
in	O
including	O
decision	O
variables	O
,	O
in	O
contrast	O
to	O
the	O
application	O
of	O
ids	O
in	O
chapter	O
(	O
7	O
)	O
.	O
7more	O
general	O
cases	O
can	O
be	O
considered	O
in	O
which	O
the	O
variables	O
are	O
placed	O
in	O
a	O
distribution	B
of	O
states	O
[	O
74	O
]	O
.	O
42	O
draft	O
march	O
9	O
,	O
2010	O
parameterising	O
belief	B
networks	I
x1	O
x2	O
x3	O
x4	O
x5	O
x1	O
x2	O
x3	O
x4	O
x5	O
x1	O
x2	O
x3	O
x4	O
x5	O
z1	O
z2	O
z1	O
z2	O
z3	O
z4	O
z5	O
y	O
(	O
a	O
)	O
y	O
(	O
b	O
)	O
y	O
(	O
c	O
)	O
figure	O
3.13	O
:	O
(	O
a	O
)	O
:	O
if	O
all	O
variables	O
are	O
binary	O
25	O
=	O
32	O
states	O
are	O
required	O
to	O
specify	O
p	O
(	O
y|x1	O
,	O
.	O
.	O
.	O
,	O
x5	O
)	O
.	O
here	O
only	O
16	O
states	O
are	O
required	O
.	O
(	O
c	O
)	O
:	O
noisy	O
logic	O
gates	O
.	O
(	O
b	O
)	O
:	O
fd	O
fa	O
d	O
a	O
consider	O
the	O
following	O
cpt	O
entries	O
p	O
(	O
d	O
=	O
bad	O
)	O
=	O
0.3	O
,	O
p	O
(	O
a	O
=	O
tr|d	O
=	O
bad	O
)	O
=	O
0.9.	O
if	O
we	O
intervene	O
and	O
use	O
a	O
bad	O
driver	O
,	O
what	O
is	O
the	O
probability	B
of	O
an	O
accident	O
?	O
p	O
(	O
a	O
=	O
tr|d	O
=	O
bad	O
,	O
fd	O
=	O
bad	O
,	O
fa	O
=	O
∅	O
)	O
=	O
p	O
(	O
a	O
=	O
tr|d	O
=	O
bad	O
)	O
=	O
0.9	O
(	O
3.4.9	O
)	O
on	O
the	O
other	O
hand	O
,	O
if	O
we	O
intervene	O
and	O
make	O
an	O
accident	O
,	O
what	O
is	O
the	O
probability	B
the	O
driver	O
involved	O
is	O
bad	O
?	O
this	O
is	O
p	O
(	O
d	O
=	O
bad||a	O
=	O
tr	O
,	O
fd	O
=	O
∅	O
,	O
fa	O
=	O
tr	O
)	O
=	O
p	O
(	O
d	O
=	O
bad	O
)	O
=	O
0.3	O
3.4.3	O
learning	B
the	O
direction	O
of	O
arrows	O
in	O
the	O
absence	O
of	O
data	B
from	O
causal	B
experiments	O
,	O
one	O
should	O
be	O
justiﬁably	O
sceptical	O
about	O
learning	B
‘	O
causal	B
’	O
networks	O
.	O
nevertheless	O
,	O
one	O
might	O
prefer	O
a	O
certain	O
direction	O
of	O
a	O
link	O
based	O
on	O
assumptions	O
of	O
the	O
‘	O
sim-	O
plicity	O
’	O
of	O
the	O
cpts	O
.	O
this	O
preference	O
may	O
come	O
from	O
a	O
‘	O
physical	O
intuition	O
’	O
that	O
whilst	O
root	O
‘	O
causes	O
’	O
may	O
be	O
uncertain	B
,	O
the	O
relationship	O
from	O
cause	O
to	O
eﬀect	O
is	O
clear	O
.	O
in	O
this	O
sense	O
a	O
measure	O
of	O
the	O
complexity	O
of	O
a	O
cpt	O
is	O
required	O
,	O
such	O
as	O
entropy	O
.	O
such	O
heuristics	O
can	O
be	O
numerically	O
encoded	O
and	O
the	O
‘	O
directions	O
’	O
learned	O
in	O
an	O
otherwise	O
markov	O
equivalent	B
graph	O
.	O
3.5	O
parameterising	O
belief	B
networks	I
(	O
dim	O
(	O
y	O
)	O
−	O
1	O
)	O
(	O
cid:81	O
)	O
consider	O
a	O
variable	B
y	O
with	O
many	O
parental	O
variables	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
ﬁg	O
(	O
3.13a	O
)	O
.	O
formally	O
,	O
the	O
structure	B
of	O
the	O
graph	B
implies	O
nothing	O
about	O
the	O
form	O
of	O
the	O
parameterisation	B
of	O
the	O
table	O
p	O
(	O
y|x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
.	O
if	O
each	O
parent	O
xi	O
has	O
dim	O
(	O
xi	O
)	O
states	O
,	O
and	O
there	O
is	O
no	O
constraint	O
on	O
the	O
table	O
,	O
then	O
the	O
table	O
p	O
(	O
y|x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
contains	O
i	O
dim	O
(	O
xi	O
)	O
entries	O
.	O
if	O
stored	O
explicitly	O
for	O
each	O
state	O
,	O
this	O
would	O
require	O
potentially	O
huge	O
storage	O
.	O
an	O
alternative	O
is	O
to	O
constrain	O
the	O
table	O
to	O
have	O
a	O
simpler	O
parametric	O
form	O
.	O
for	O
example	O
,	O
one	O
might	O
write	O
a	O
decomposition	B
in	O
which	O
only	O
a	O
limited	O
number	O
of	O
parental	O
interactions	O
are	O
required	O
(	O
this	O
is	O
called	O
divorcing	B
parents	I
in	O
[	O
148	O
]	O
)	O
.	O
for	O
example	O
,	O
in	O
ﬁg	O
(	O
3.13b	O
)	O
,	O
assuming	O
all	O
variables	O
are	O
binary	O
,	O
the	O
number	O
of	O
states	O
requiring	O
speciﬁcation	O
is	O
23	O
+	O
22	O
+	O
22	O
=	O
16	O
,	O
compared	O
to	O
the	O
25	O
=	O
32	O
states	O
in	O
the	O
unconstrained	O
case	O
.	O
the	O
distribution	B
p	O
(	O
y|x1	O
,	O
.	O
.	O
.	O
,	O
x5	O
)	O
=	O
(	O
cid:88	O
)	O
z1	O
,	O
z2	O
p	O
(	O
y|z1	O
,	O
z2	O
)	O
p	O
(	O
z1|x1	O
,	O
x2	O
,	O
x3	O
)	O
p	O
(	O
z2|x4	O
,	O
x5	O
)	O
can	O
be	O
stored	O
using	O
only	O
16	O
independent	O
parameters	O
.	O
draft	O
march	O
9	O
,	O
2010	O
(	O
3.5.1	O
)	O
43	O
logic	B
gates	O
exercises	O
another	O
technique	O
to	O
constrain	O
cpts	O
uses	O
simple	O
classes	O
of	O
conditional	B
tables	O
.	O
for	O
example	O
,	O
in	O
ﬁg	O
(	O
3.13c	O
)	O
,	O
one	O
could	O
use	O
a	O
logical	O
or	O
gate	O
on	O
binary	O
zi	O
,	O
say	O
(	O
cid:26	O
)	O
1	O
if	O
at	O
least	O
one	O
of	O
the	O
zi	O
is	O
in	O
state	O
1	O
p	O
(	O
y|z1	O
,	O
.	O
.	O
.	O
,	O
z5	O
)	O
=	O
0	O
otherwise	O
(	O
3.5.2	O
)	O
we	O
can	O
then	O
make	O
a	O
cpt	O
p	O
(	O
y|x1	O
,	O
.	O
.	O
.	O
,	O
x5	O
)	O
by	O
including	O
the	O
additional	O
terms	O
p	O
(	O
zi	O
=	O
1|xi	O
)	O
.	O
when	O
each	O
xi	O
is	O
binary	O
there	O
are	O
in	O
total	O
only	O
2	O
+	O
2	O
+	O
2	O
+	O
2	O
+	O
2	O
=	O
10	O
quantities	O
required	O
for	O
specifying	O
p	O
(	O
y|x	O
)	O
.	O
in	O
this	O
case	O
,	O
ﬁg	O
(	O
3.13c	O
)	O
can	O
be	O
used	O
to	O
represent	O
any	O
noisy	B
logic	I
gate	I
,	O
such	O
as	O
the	O
noisy	O
or	O
or	O
noisy	O
and	O
,	O
where	O
the	O
number	O
of	O
parameters	O
required	O
to	O
specify	O
the	O
noisy	O
gate	O
is	O
linear	B
in	O
the	O
number	O
of	O
parents	B
x.	O
the	O
noisy-or	O
is	O
particularly	O
common	O
in	O
disease-symptom	O
networks	O
in	O
which	O
many	O
diseases	O
x	O
can	O
give	O
rise	O
to	O
the	O
same	O
symptom	O
y–	O
provided	O
that	O
at	O
least	O
one	O
of	O
the	O
diseases	O
is	O
present	O
,	O
the	O
probability	B
that	O
the	O
symptom	O
will	O
be	O
present	O
is	O
high	O
.	O
3.6	O
further	O
reading	O
an	O
introduction	O
to	O
bayesian	O
networks	O
and	O
graphical	O
models	O
in	O
expert	O
systems	O
is	O
to	O
be	O
found	O
in	O
[	O
258	O
]	O
,	O
which	O
also	O
discusses	O
general	O
inference	B
techniques	O
which	O
will	O
be	O
discussed	O
during	O
later	O
chapters	O
.	O
3.7	O
code	O
3.7.1	O
naive	O
inference	O
demo	O
demoburglar.m	O
:	O
was	O
it	O
the	O
burglar	O
demo	O
demochestclinic.m	O
:	O
naive	O
inference	O
on	O
chest	B
clinic	I
3.7.2	O
conditional	B
independence	O
demo	O
the	O
following	O
demo	O
determines	O
whether	O
x	O
⊥⊥	O
y|	O
z	O
for	O
the	O
chest	B
clinic	I
network	O
,	O
and	O
checks	O
the	O
result	O
numerically8	O
.	O
the	O
independence	B
test	O
is	O
based	O
on	O
the	O
markov	O
method	O
of	O
section	O
(	O
4.2.4	O
)	O
.	O
this	O
is	O
preferred	O
over	O
the	O
d-separation	O
method	O
since	O
it	O
is	O
arguably	O
simpler	O
to	O
code	O
and	O
also	O
more	O
general	O
in	O
that	O
it	O
deals	O
also	O
with	O
conditional	O
independence	B
in	O
markov	O
networks	O
as	O
well	O
as	O
belief	O
networks	O
.	O
running	O
the	O
demo	O
code	O
below	O
,	O
it	O
may	O
happen	O
that	O
the	O
numerical	B
dependence	O
is	O
very	O
low	O
–	O
that	O
is	O
p	O
(	O
x	O
,	O
y|z	O
)	O
≈	O
p	O
(	O
x|z	O
)	O
p	O
(	O
y|z	O
)	O
(	O
3.7.1	O
)	O
even	O
though	O
x	O
(	O
cid:62	O
)	O
(	O
cid:62	O
)	O
y|z	O
.	O
this	O
highlights	O
the	O
diﬀerence	O
between	O
‘	O
structural	O
’	O
and	O
‘	O
numerical	B
’	O
independence	B
.	O
condindeppot.m	O
:	O
numerical	B
measure	O
of	O
conditional	B
independence	O
democondindep.m	O
:	O
demo	O
of	O
conditional	B
independence	O
(	O
using	O
markov	O
method	O
)	O
3.7.3	O
utility	B
routines	O
dag.m	O
:	O
find	O
the	O
dag	O
structure	B
for	O
a	O
belief	B
network	I
3.8	O
exercises	O
exercise	O
20	O
(	O
party	O
animal	O
)	O
.	O
the	O
party	O
animal	O
problem	B
corresponds	O
to	O
the	O
network	O
in	O
ﬁg	O
(	O
3.14	O
)	O
.	O
the	O
boss	O
is	O
angry	O
and	O
the	O
worker	O
has	O
a	O
headache	O
–	O
what	O
is	O
the	O
probability	B
the	O
worker	O
has	O
been	O
to	O
a	O
party	O
?	O
to	O
8the	O
code	O
for	O
(	O
structural	O
)	O
conditional	B
independence	O
is	O
given	O
in	O
chapter	O
(	O
4	O
)	O
.	O
44	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
d	O
u	O
h	O
p	O
a	O
figure	O
3.14	O
:	O
party	O
animal	O
.	O
here	O
all	O
variables	O
are	O
binary	O
.	O
when	O
set	O
to	O
1	O
the	O
statements	O
are	O
true	O
:	O
p	O
=	O
been	O
to	O
party	O
,	O
h	O
=	O
got	O
a	O
headache	O
,	O
d	O
=	O
demotivated	O
at	O
work	O
,	O
u	O
=	O
underperform	O
at	O
work	O
,	O
a	O
=boss	O
angry	O
.	O
shaded	O
variables	O
are	O
observed	O
in	O
the	O
true	O
state	O
.	O
a	O
t	O
x	O
l	O
e	O
b	O
s	O
d	O
x	O
=	O
positive	O
x-ray	O
d	O
=	O
dyspnea	O
(	O
shortness	O
of	O
breath	O
)	O
e	O
=	O
either	O
tuberculosis	O
or	O
lung	O
cancer	O
t	O
=	O
tuberculosis	O
l	O
=	O
lung	O
cancer	O
b	O
=	O
bronchitis	O
a	O
=	O
visited	O
asia	O
s	O
=	O
smoker	O
figure	O
3.15	O
:	O
belief	B
network	I
structure	O
for	O
the	O
chest	B
clinic	I
example	O
.	O
complete	O
the	O
speciﬁcations	O
,	O
the	O
probabilities	O
are	O
given	O
as	O
follows	O
:	O
p	O
(	O
u	O
=	O
tr|p	O
=	O
tr	O
,	O
d	O
=	O
tr	O
)	O
=	O
0.999	O
p	O
(	O
u	O
=	O
tr|p	O
=	O
fa	O
,	O
d	O
=	O
tr	O
)	O
=	O
0.9	O
p	O
(	O
u	O
=	O
tr|p	O
=	O
fa	O
,	O
d	O
=	O
fa	O
)	O
=	O
0.01	O
p	O
(	O
u	O
=	O
tr|p	O
=	O
tr	O
,	O
d	O
=	O
fa	O
)	O
=	O
0.9	O
exercise	O
21.	O
consider	O
the	O
distribution	B
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
=	O
p	O
(	O
c|a	O
,	O
b	O
)	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
.	O
(	O
i	O
)	O
is	O
a⊥⊥	O
b|∅	O
?	O
.	O
(	O
ii	O
)	O
is	O
a⊥⊥	O
b|	O
c	O
?	O
exercise	O
22.	O
the	O
chest	B
clinic	I
network	O
[	O
170	O
]	O
concerns	O
the	O
diagnosis	O
of	O
lung	O
disease	O
(	O
tuberculosis	O
,	O
lung	O
cancer	O
,	O
or	O
both	O
,	O
or	O
neither	O
)	O
.	O
in	O
this	O
model	B
a	O
visit	O
to	O
asia	O
is	O
assumed	O
to	O
increase	O
the	O
probability	B
of	O
tuber-	O
culosis	O
.	O
state	O
if	O
the	O
following	O
conditional	B
independence	O
relationships	O
are	O
true	O
or	O
false	O
1.	O
tuberculosis⊥⊥	O
smoking|	O
shortness	O
of	O
breath	O
,	O
2.	O
lung	O
cancer⊥⊥	O
bronchitis|	O
smoking	O
,	O
3.	O
visit	O
to	O
asia⊥⊥	O
smoking|	O
lung	O
cancer	O
4.	O
visit	O
to	O
asia⊥⊥	O
smoking|	O
lung	O
cancer	O
,	O
shortness	O
of	O
breath	O
.	O
exercise	O
23	O
(	O
[	O
128	O
]	O
)	O
.	O
consider	O
the	O
network	O
in	O
ﬁg	O
(	O
3.16	O
)	O
,	O
which	O
concerns	O
the	O
probability	B
of	O
a	O
car	O
starting	O
.	O
p	O
(	O
f	O
=	O
empty	O
)	O
=	O
0.05	O
p	O
(	O
b	O
=	O
bad	O
)	O
=	O
0.02	O
p	O
(	O
g	O
=	O
empty|b	O
=	O
good	O
,	O
f	O
=	O
not	O
empty	O
)	O
=	O
0.04	O
p	O
(	O
g	O
=	O
empty|b	O
=	O
good	O
,	O
f	O
=	O
empty	O
)	O
=	O
0.97	O
p	O
(	O
g	O
=	O
empty|b	O
=	O
bad	O
,	O
f	O
=	O
not	O
empty	O
)	O
=	O
0.1	O
p	O
(	O
g	O
=	O
empty|b	O
=	O
bad	O
,	O
f	O
=	O
empty	O
)	O
=	O
0.99	O
p	O
(	O
t	O
=	O
fa|b	O
=	O
good	O
)	O
=	O
0.03	O
p	O
(	O
t	O
=	O
fa|b	O
=	O
bad	O
)	O
=	O
0.98	O
p	O
(	O
s	O
=	O
fa|t	O
=	O
tr	O
,	O
f	O
=	O
empty	O
)	O
=	O
0.92	O
p	O
(	O
s	O
=	O
fa|t	O
=	O
tr	O
,	O
f	O
=	O
not	O
empty	O
)	O
=	O
0.01	O
p	O
(	O
s	O
=	O
fa|t	O
=	O
fa	O
,	O
f	O
=	O
not	O
empty	O
)	O
=	O
1.0	O
p	O
(	O
s	O
=	O
fa|t	O
=	O
fa	O
,	O
f	O
=	O
empty	O
)	O
=	O
0.99	O
calculate	O
p	O
(	O
f	O
=	O
empty|s	O
=	O
no	O
)	O
,	O
the	O
probability	B
of	O
the	O
fuel	O
tank	O
being	O
empty	O
conditioned	O
on	O
the	O
observation	O
that	O
the	O
car	O
does	O
not	O
start	O
.	O
exercise	O
24.	O
consider	O
the	O
chest	B
clinic	I
bayesian	O
network	O
in	O
ﬁg	O
(	O
3.15	O
)	O
[	O
170	O
]	O
.	O
calculate	O
by	O
hand	O
the	O
values	O
for	O
p	O
(	O
d	O
)	O
,	O
p	O
(	O
d|s	O
=	O
tr	O
)	O
,	O
p	O
(	O
d|s	O
=	O
fa	O
)	O
.	O
the	O
table	O
values	O
are	O
:	O
p	O
(	O
a	O
=	O
tr	O
)	O
p	O
(	O
t	O
=	O
tr|a	O
=	O
tr	O
)	O
p	O
(	O
l	O
=	O
tr|s	O
=	O
tr	O
)	O
p	O
(	O
b	O
=	O
tr|s	O
=	O
tr	O
)	O
p	O
(	O
x	O
=	O
tr|e	O
=	O
tr	O
)	O
p	O
(	O
d	O
=	O
tr|e	O
=	O
tr	O
,	O
b	O
=	O
tr	O
)	O
=	O
0.9	O
p	O
(	O
d	O
=	O
tr|e	O
=	O
fa	O
,	O
b	O
=	O
tr	O
)	O
=	O
0.8	O
=	O
0.01	O
p	O
(	O
s	O
=	O
tr	O
)	O
=	O
0.5	O
=	O
0.05	O
p	O
(	O
t	O
=	O
tr|a	O
=	O
fa	O
)	O
=	O
0.01	O
=	O
0.1	O
p	O
(	O
l	O
=	O
tr|s	O
=	O
fa	O
)	O
=	O
0.01	O
=	O
0.6	O
p	O
(	O
b	O
=	O
tr|s	O
=	O
fa	O
)	O
=	O
0.3	O
=	O
0.98	O
p	O
(	O
x	O
=	O
tr|e	O
=	O
fa	O
)	O
=	O
0.05	O
p	O
(	O
d	O
=	O
tr|e	O
=	O
tr	O
,	O
b	O
=	O
fa	O
)	O
=	O
0.7	O
p	O
(	O
d	O
=	O
tr|e	O
=	O
fa	O
,	O
b	O
=	O
fa	O
)	O
=	O
0.1	O
p	O
(	O
e	O
=	O
tr|t	O
,	O
l	O
)	O
=	O
0	O
only	O
if	O
both	O
t	O
and	O
l	O
are	O
fa	O
,	O
1	O
otherwise	O
.	O
draft	O
march	O
9	O
,	O
2010	O
45	O
exercises	O
battery	O
fuel	O
gauge	O
figure	O
3.16	O
:	O
belief	B
network	I
of	O
car	O
not	O
starting	O
[	O
128	O
]	O
,	O
see	O
exercise	O
(	O
23	O
)	O
.	O
turn	O
over	O
start	O
exercise	O
25.	O
if	O
we	O
interpret	O
the	O
chest	B
clinic	I
network	O
exercise	O
(	O
24	O
)	O
causally	O
,	O
how	O
can	O
we	O
help	O
a	O
doctor	O
answer	O
the	O
question	O
‘	O
if	O
i	O
could	O
cure	O
my	O
patients	O
of	O
bronchitis	O
,	O
how	O
would	O
this	O
aﬀect	O
my	O
patients	O
’	O
s	O
chance	O
of	O
being	O
short	O
of	O
breath	O
?	O
’	O
.	O
how	O
does	O
this	O
compare	O
with	O
p	O
(	O
d	O
=	O
tr|b	O
=	O
fa	O
)	O
in	O
a	O
non-causal	O
interpretation	O
,	O
and	O
what	O
does	O
this	O
mean	B
?	O
exercise	O
26.	O
there	O
is	O
a	O
synergistic	O
relationship	O
between	O
asbestos	O
(	O
a	O
)	O
exposure	O
,	O
smoking	O
(	O
s	O
)	O
and	O
cancer	O
(	O
c	O
)	O
.	O
a	O
model	B
describing	O
this	O
relationship	O
is	O
given	O
by	O
p	O
(	O
a	O
,	O
s	O
,	O
c	O
)	O
=	O
p	O
(	O
c|a	O
,	O
s	O
)	O
p	O
(	O
a	O
)	O
p	O
(	O
s	O
)	O
(	O
3.8.1	O
)	O
1.	O
is	O
a⊥⊥	O
s|∅	O
?	O
2.	O
is	O
a⊥⊥	O
s|	O
c	O
?	O
3.	O
how	O
could	O
you	O
adjust	O
the	O
model	B
to	O
account	O
for	O
the	O
fact	O
that	O
people	O
who	O
work	O
in	O
the	O
building	O
industry	O
have	O
a	O
higher	O
likelihood	B
to	O
also	O
be	O
smokers	O
and	O
also	O
a	O
higher	O
likelihood	B
to	O
asbestos	O
exposure	O
?	O
exercise	O
27.	O
consider	O
the	O
three	O
variable	B
distribution	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
=	O
p	O
(	O
a|b	O
)	O
p	O
(	O
b|c	O
)	O
p	O
(	O
c	O
)	O
(	O
3.8.2	O
)	O
where	O
all	O
variables	O
are	O
binary	O
.	O
how	O
many	O
parameters	O
are	O
needed	O
to	O
specify	O
distributions	O
of	O
this	O
form	O
?	O
exercise	O
28.	O
consider	O
the	O
belief	B
network	I
on	O
the	O
right	O
which	O
represents	O
mr	O
holmes	O
’	O
burglary	O
worries	O
as	O
given	O
in	O
ﬁg	O
(	O
3.2a	O
)	O
:	O
(	O
b	O
)	O
urglar	O
,	O
(	O
a	O
)	O
larm	O
,	O
(	O
w	O
)	O
atson	O
,	O
mrs	O
(	O
gibbon	O
)	O
.	O
all	O
variables	O
take	O
the	O
two	O
states	O
{	O
tr	O
,	O
fa	O
}	O
.	O
the	O
table	O
entries	O
are	O
=	O
0.01	O
p	O
(	O
b	O
=	O
tr	O
)	O
p	O
(	O
a	O
=	O
tr|b	O
=	O
tr	O
)	O
=	O
0.99	O
p	O
(	O
a	O
=	O
tr|b	O
=	O
fa	O
)	O
=	O
0.05	O
p	O
(	O
w	O
=	O
tr|a	O
=	O
tr	O
)	O
=	O
0.9	O
p	O
(	O
w	O
=	O
tr|a	O
=	O
fa	O
)	O
=	O
0.5	O
p	O
(	O
g	O
=	O
tr|a	O
=	O
tr	O
)	O
=	O
0.7	O
p	O
(	O
g	O
=	O
tr|a	O
=	O
fa	O
)	O
=	O
0.2	O
1.	O
compute	O
‘	O
by	O
hand	O
’	O
(	O
i.e	O
.	O
show	O
your	O
working	O
)	O
:	O
(	O
a	O
)	O
p	O
(	O
b	O
=	O
tr|w	O
=	O
tr	O
)	O
(	O
b	O
)	O
p	O
(	O
b	O
=	O
tr|w	O
=	O
tr	O
,	O
g	O
=	O
fa	O
)	O
b	O
a	O
g	O
w	O
(	O
3.8.3	O
)	O
2.	O
consider	O
the	O
same	O
situation	O
as	O
above	O
,	O
except	O
that	O
now	O
the	O
evidence	B
is	O
uncertain	B
.	O
mrs	O
gibbon	O
thinks	O
that	O
the	O
state	O
is	O
g	O
=	O
fa	O
with	O
probability	O
0.9.	O
similarly	O
,	O
dr	O
watson	O
believes	O
in	O
the	O
state	O
w	O
=	O
fa	O
with	O
value	O
0.7.	O
compute	O
‘	O
by	O
hand	O
’	O
the	O
posteriors	O
under	O
these	O
uncertain	B
(	O
soft	B
)	O
evidences	O
:	O
(	O
a	O
)	O
p	O
(	O
b	O
=	O
tr|	O
˜w	O
)	O
(	O
b	O
)	O
p	O
(	O
b	O
=	O
tr|	O
˜w	O
,	O
˜g	O
)	O
exercise	O
29.	O
a	O
doctor	O
gives	O
a	O
patient	O
a	O
(	O
d	O
)	O
rug	O
(	O
drug	O
or	O
no	O
drug	O
)	O
dependent	O
on	O
their	O
(	O
a	O
)	O
ge	O
(	O
old	O
or	O
young	O
)	O
and	O
(	O
g	O
)	O
ender	O
(	O
male	O
or	O
female	O
)	O
.	O
whether	O
or	O
not	O
the	O
patient	O
(	O
r	O
)	O
ecovers	O
(	O
recovers	O
or	O
doesn	O
’	O
t	O
recover	O
)	O
depends	O
on	O
all	O
d	O
,	O
a	O
,	O
g.	O
in	O
addition	O
a⊥⊥	O
g|∅	O
.	O
1.	O
write	O
down	O
the	O
belief	B
network	I
for	O
the	O
above	O
situation	O
.	O
46	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
2.	O
explain	O
how	O
to	O
compute	O
p	O
(	O
recover|drug	O
)	O
.	O
3.	O
explain	O
how	O
to	O
compute	O
p	O
(	O
recover|do	O
(	O
drug	O
)	O
,	O
young	O
)	O
.	O
exercise	O
30.	O
implement	O
the	O
wet	O
grass	O
scenario	O
numerically	O
using	O
the	O
brmltoolbox	O
.	O
exercise	O
31	O
(	O
la	O
burglar	O
)	O
.	O
consider	O
the	O
burglar	O
scenario	O
,	O
example	O
(	O
10	O
)	O
.	O
we	O
now	O
wish	O
to	O
model	B
the	O
fact	O
that	O
in	O
los	O
angeles	O
the	O
probability	B
of	O
being	O
burgled	O
increases	O
if	O
there	O
is	O
an	O
earthquake	O
.	O
explain	O
how	O
to	O
include	O
this	O
eﬀect	O
in	O
the	O
model	B
.	O
exercise	O
32.	O
given	O
two	O
belief	B
networks	I
represented	O
as	O
dags	O
with	O
associated	O
adjacency	B
matrices	O
a	O
and	O
b	O
,	O
write	O
a	O
matlab	O
function	B
markovequiv	O
(	O
a	O
,	O
b	O
)	O
.m	O
that	O
returns	O
1	O
if	O
a	O
and	O
b	O
are	O
markov	O
equivalent	B
,	O
and	O
zero	O
otherwise	O
.	O
exercise	O
33.	O
the	O
adjacency	B
matrices	O
of	O
two	O
belief	B
networks	I
are	O
given	O
below	O
(	O
see	O
abmatrices.mat	O
)	O
.	O
state	O
if	O
they	O
are	O
markov	O
equivalent	B
.	O
	O
a	O
=	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
1	O
0	O
0	O
	O
,	O
	O
b	O
=	O
	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
1	O
1	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
1	O
0	O
0	O
(	O
3.8.4	O
)	O
exercise	O
34.	O
there	O
are	O
three	O
computers	O
indexed	O
by	O
i	O
∈	O
{	O
1	O
,	O
2	O
,	O
3	O
}	O
.	O
computer	O
i	O
can	O
send	O
a	O
message	B
in	O
one	O
timestep	O
to	O
computer	O
j	O
if	O
cij	O
=	O
1	O
,	O
otherwise	O
cij	O
=	O
0.	O
there	O
is	O
a	O
fault	O
in	O
the	O
network	O
and	O
the	O
task	O
is	O
to	O
ﬁnd	O
out	O
some	O
information	O
about	O
the	O
communication	O
matrix	B
c	O
(	O
c	O
is	O
not	O
necessarily	O
symmetric	O
)	O
.	O
to	O
do	O
this	O
,	O
thomas	O
,	O
the	O
engineer	O
,	O
will	O
run	O
some	O
tests	O
that	O
reveal	O
whether	O
or	O
not	O
computer	O
i	O
can	O
send	O
a	O
message	B
to	O
computer	O
j	O
in	O
t	O
timesteps	O
,	O
t	O
∈	O
{	O
1	O
,	O
2	O
}	O
.	O
this	O
is	O
expressed	O
as	O
cij	O
(	O
t	O
)	O
,	O
with	O
cij	O
(	O
1	O
)	O
≡	O
cij	O
.	O
for	O
example	O
,	O
he	O
might	O
know	O
that	O
c13	O
(	O
2	O
)	O
=	O
1	O
,	O
meaning	O
that	O
according	O
to	O
his	O
test	O
,	O
a	O
message	B
sent	O
from	O
computer	O
1	O
will	O
arrive	O
at	O
computer	O
3	O
in	O
at	O
most	O
2	O
timesteps	O
.	O
note	O
that	O
this	O
message	B
could	O
go	O
via	O
diﬀerent	O
routes	O
–	O
it	O
might	O
go	O
directly	O
from	O
1	O
to	O
3	O
in	O
one	O
timestep	O
,	O
or	O
indirectly	O
from	O
1	O
to	O
2	O
and	O
then	O
from	O
2	O
to	O
3	O
,	O
or	O
both	O
.	O
you	O
may	O
assume	O
cii	O
=	O
1.	O
a	O
priori	O
thomas	O
thinks	O
there	O
is	O
a	O
10	O
%	O
probability	B
that	O
cij	O
=	O
1.	O
given	O
the	O
test	O
information	O
c	O
=	O
{	O
c12	O
(	O
2	O
)	O
=	O
1	O
,	O
c23	O
(	O
2	O
)	O
=	O
0	O
}	O
,	O
compute	O
the	O
a	O
posteriori	O
probability	B
vector	O
[	O
p	O
(	O
c12	O
=	O
1|c	O
)	O
,	O
p	O
(	O
c13	O
=	O
1|c	O
)	O
,	O
p	O
(	O
c23	O
=	O
1|c	O
)	O
,	O
p	O
(	O
c32	O
=	O
1|c	O
)	O
,	O
p	O
(	O
c21	O
=	O
1|c	O
)	O
,	O
p	O
(	O
c31	O
=	O
1|c	O
)	O
]	O
(	O
3.8.5	O
)	O
exercise	O
35.	O
a	O
belief	B
network	I
models	O
the	O
relation	O
between	O
the	O
variables	O
oil	O
,	O
inf	O
,	O
eh	O
,	O
bp	O
,	O
rt	O
which	O
stand	O
for	O
the	O
price	O
of	O
oil	O
,	O
inﬂation	O
rate	O
,	O
economy	O
health	O
,	O
british	O
petroleum	O
stock	O
price	O
,	O
retailer	O
stock	O
price	O
.	O
each	O
variable	B
takes	O
the	O
states	O
low	O
,	O
high	O
,	O
except	O
for	O
bp	O
which	O
has	O
states	O
low	O
,	O
high	O
,	O
normal	B
.	O
the	O
belief	B
network	I
model	O
for	O
these	O
variables	O
has	O
tables	O
p	O
(	O
eh=low	O
)	O
=0.2	O
p	O
(	O
bp=low|oil=low	O
)	O
=0.9	O
p	O
(	O
bp=low|oil=high	O
)	O
=0.1	O
p	O
(	O
oil=low|eh=low	O
)	O
=0.9	O
p	O
(	O
rt=low|inf=low	O
,	O
eh=low	O
)	O
=0.9	O
p	O
(	O
rt=low|inf=high	O
,	O
eh=low	O
)	O
=0.1	O
p	O
(	O
inf=low|oil=low	O
,	O
eh=low	O
)	O
=0.9	O
p	O
(	O
inf=low|oil=high	O
,	O
eh=low	O
)	O
=0.1	O
p	O
(	O
bp=normal|oil=low	O
)	O
=0.1	O
p	O
(	O
bp=normal|oil=high	O
)	O
=0.4	O
p	O
(	O
oil=low|eh=high	O
)	O
=0.05	O
p	O
(	O
rt=low|inf=low	O
,	O
eh=high	O
)	O
=0.1	O
p	O
(	O
rt=low|inf=high	O
,	O
eh=high	O
)	O
=0.01	O
p	O
(	O
inf=low|oil=low	O
,	O
eh=high	O
)	O
=0.1	O
p	O
(	O
inf=low|oil=high	O
,	O
eh=high	O
)	O
=0.01	O
1.	O
draw	O
a	O
belief	B
network	I
for	O
this	O
distribution	B
.	O
2.	O
given	O
that	O
bp	O
stock	O
price	O
is	O
normal	B
and	O
the	O
retailer	O
stock	O
price	O
is	O
high	O
,	O
what	O
is	O
the	O
probability	B
that	O
inﬂation	O
is	O
high	O
?	O
exercise	O
36.	O
there	O
are	O
a	O
set	O
of	O
c	O
potentials	O
with	O
potential	O
c	O
deﬁned	O
on	O
a	O
subset	O
of	O
variables	O
xc	O
.	O
if	O
xc	O
⊆	O
xd	O
then	O
can	O
merge	O
(	O
multiply	O
)	O
potentials	O
c	O
and	O
d	O
since	O
c	O
is	O
contained	O
within	O
d.	O
with	O
reference	O
to	O
suitable	O
graph	B
structures	O
,	O
describe	O
an	O
eﬃcient	B
algorithm	O
to	O
merge	O
a	O
set	O
of	O
potentials	O
so	O
that	O
for	O
the	O
new	O
set	O
of	O
potentials	O
no	O
potential	O
is	O
contained	O
within	O
the	O
other	O
.	O
draft	O
march	O
9	O
,	O
2010	O
47	O
exercises	O
48	O
draft	O
march	O
9	O
,	O
2010	O
chapter	O
4	O
graphical	O
models	O
4.1	O
graphical	O
models	O
graphical	O
models	O
(	O
gms	O
)	O
are	O
depictions	O
of	O
independence/dependence	O
relationships	O
for	O
distributions	O
.	O
each	O
form	O
of	O
gm	O
is	O
a	O
particular	O
union	O
of	O
graph	B
and	O
probability	B
constructs	O
and	O
details	O
the	O
form	O
of	O
independence	B
assumptions	O
represented	O
.	O
gms	O
are	O
useful	O
since	O
they	O
provide	O
a	O
framework	O
for	O
studying	O
a	O
wide	O
class	O
of	O
probabilistic	B
models	O
and	O
associated	O
algorithms	O
.	O
in	O
particular	O
they	O
help	O
to	O
clarify	O
modelling	B
assumptions	O
and	O
provide	O
a	O
uniﬁed	O
framework	O
under	O
which	O
inference	B
algorithms	O
in	O
diﬀerent	O
communities	O
can	O
be	O
related	O
.	O
it	O
needs	O
to	O
be	O
emphasised	O
that	O
all	O
forms	O
of	O
gm	O
have	O
a	O
limited	O
ability	O
to	O
graphically	O
expresses	O
condi-	O
tional	O
(	O
in	O
)	O
dependence	B
statements	O
[	O
265	O
]	O
.	O
as	O
we	O
’	O
ve	O
seen	O
,	O
belief	B
networks	I
are	O
useful	O
for	O
modelling	B
ancestral	O
conditional	B
independence	O
.	O
in	O
this	O
chapter	O
we	O
’	O
ll	O
introduce	O
other	O
types	O
of	O
gm	O
that	O
are	O
more	O
suited	O
to	O
representing	O
diﬀerent	O
assumptions	O
.	O
markov	O
networks	O
,	O
for	O
example	O
,	O
are	O
particularly	O
suited	O
to	O
modelling	B
marginal	O
dependence	B
and	O
conditional	B
independence	O
.	O
here	O
we	O
’	O
ll	O
focus	O
on	O
markov	O
networks	O
,	O
chain	B
graphs	O
(	O
which	O
marry	O
belief	O
and	O
markov	O
networks	O
)	O
and	O
factor	B
graphs	O
.	O
there	O
are	O
many	O
more	O
inhabitants	O
of	O
the	O
zoo	O
of	O
graphical	O
models	O
,	O
see	O
[	O
70	O
,	O
293	O
]	O
.	O
the	O
general	O
viewpoint	O
we	O
adopt	O
is	O
to	O
describe	O
the	O
problem	B
environment	O
using	O
a	O
probabilistic	B
model	O
,	O
after	O
which	O
reasoning	O
corresponds	O
to	O
performing	O
probabilistic	B
inference	O
.	O
this	O
is	O
therefore	O
a	O
two	O
part	O
process	O
:	O
modelling	B
after	O
identifying	O
all	O
potentially	O
relevant	O
variables	O
of	O
a	O
problem	B
environment	O
,	O
our	O
task	O
is	O
to	O
describe	O
how	O
these	O
variables	O
can	O
interact	O
.	O
this	O
is	O
achieved	O
using	O
structural	O
assumptions	O
as	O
to	O
the	O
form	O
of	O
the	O
joint	B
probability	O
distribution	B
of	O
all	O
the	O
variables	O
,	O
typically	O
corresponding	O
to	O
assumptions	O
of	O
independence	B
of	O
variables	O
.	O
each	O
class	O
of	O
graphical	O
model	B
corresponds	O
to	O
a	O
factorisation	O
property	O
of	O
the	O
joint	B
distribution	O
.	O
inference	B
once	O
the	O
basic	O
assumptions	O
as	O
to	O
how	O
variables	O
interact	O
with	O
each	O
other	O
is	O
formed	O
(	O
i.e	O
.	O
the	O
probabilistic	B
model	O
is	O
constructed	O
)	O
all	O
questions	O
of	O
interest	O
are	O
answered	O
by	O
performing	O
inference	B
on	O
the	O
distribution	B
.	O
this	O
can	O
be	O
a	O
computationally	O
non-trivial	O
step	O
so	O
that	O
coupling	O
gms	O
with	O
accurate	O
inference	B
algorithms	O
is	O
central	O
to	O
successful	O
graphical	O
modelling	B
.	O
whilst	O
not	O
a	O
strict	O
separation	B
,	O
gms	O
tend	O
to	O
fall	O
into	O
two	O
broad	O
classes	O
–	O
those	O
useful	O
in	O
modelling	B
,	O
and	O
those	O
useful	O
in	O
representing	O
inference	B
algorithms	O
.	O
for	O
modelling	B
,	O
belief	B
networks	I
,	O
markov	O
networks	O
,	O
chain	B
graphs	O
and	O
inﬂuence	B
diagrams	I
are	O
some	O
of	O
the	O
most	O
popular	O
.	O
for	O
inference	B
one	O
typically	O
‘	O
compiles	O
’	O
a	O
model	B
into	O
a	O
suitable	O
gm	O
for	O
which	O
an	O
algorithm	B
can	O
be	O
readily	O
applied	O
.	O
such	O
inference	B
gms	O
include	O
factor	B
graphs	O
,	O
junction	O
trees	O
and	O
region	B
graphs	I
.	O
49	O
x2	O
x2	O
x1	O
x3	O
x1	O
x3	O
x1	O
x4	O
(	O
a	O
)	O
x4	O
(	O
b	O
)	O
x5	O
x6	O
x3	O
x2	O
x4	O
(	O
c	O
)	O
4.2	O
markov	O
networks	O
markov	O
networks	O
(	O
a	O
)	O
:	O
4.1	O
:	O
figure	O
pa	O
φ	O
(	O
x1	O
,	O
x2	O
)	O
φ	O
(	O
x2	O
,	O
x3	O
)	O
φ	O
(	O
x3	O
,	O
x4	O
)	O
φ	O
(	O
x4	O
,	O
x1	O
)	O
/za	O
.	O
(	O
c	O
)	O
:	O
pb	O
=	O
φ	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
/zb	O
.	O
φ	O
(	O
x1	O
,	O
x2	O
,	O
x4	O
)	O
φ	O
(	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
φ	O
(	O
x3	O
,	O
x5	O
)	O
φ	O
(	O
x3	O
,	O
x6	O
)	O
/zc	O
.	O
=	O
(	O
b	O
)	O
:	O
pc	O
=	O
belief	B
networks	I
correspond	O
to	O
a	O
special	O
kind	O
of	O
factorisation	O
of	O
the	O
joint	B
probability	O
distribution	B
in	O
which	O
each	O
of	O
the	O
factors	O
is	O
itself	O
a	O
distribution	B
.	O
an	O
alternative	O
factorisation	O
is	O
,	O
for	O
example	O
and	O
z	O
is	O
a	O
constant	O
which	O
ensures	O
normalisation	B
,	O
called	O
the	O
(	O
4.2.1	O
)	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
=	O
1	O
z	O
φ	O
(	O
a	O
,	O
b	O
)	O
φ	O
(	O
b	O
,	O
c	O
)	O
where	O
φ	O
(	O
a	O
,	O
b	O
)	O
and	O
φ	O
(	O
b	O
,	O
c	O
)	O
are	O
potentials	O
partition	B
function	I
z	O
=	O
(	O
cid:88	O
)	O
φ	O
(	O
a	O
,	O
b	O
)	O
φ	O
(	O
b	O
,	O
c	O
)	O
(	O
4.2.2	O
)	O
a	O
,	O
b	O
,	O
c	O
we	O
will	O
typically	O
use	O
the	O
convention	O
that	O
the	O
ordering	O
of	O
the	O
variables	O
in	O
the	O
potential	B
is	O
not	O
relevant	O
(	O
as	O
for	O
a	O
distribution	B
)	O
–	O
the	O
joint	B
variables	O
simply	O
index	O
an	O
element	O
of	O
the	O
potential	B
table	O
.	O
markov	O
networks	O
are	O
deﬁned	O
as	O
products	O
of	O
non-negative	O
functions	O
deﬁned	O
on	O
maximal	O
cliques	O
of	O
an	O
undirected	B
graph	I
–	O
see	O
ﬁg	O
(	O
4.1	O
)	O
.	O
case	O
of	O
a	O
potential	B
satisfying	O
normalisation	B
,	O
(	O
cid:80	O
)	O
deﬁnition	O
24	O
(	O
potential	B
)	O
.	O
a	O
potential	B
φ	O
(	O
x	O
)	O
is	O
a	O
non-negative	O
function	O
of	O
the	O
variable	B
x	O
,	O
φ	O
(	O
x	O
)	O
≥	O
0.	O
a	O
joint	B
potential	O
φ	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
is	O
a	O
non-negative	O
function	O
of	O
the	O
set	O
of	O
variables	O
.	O
a	O
distribution	B
is	O
a	O
special	O
x	O
φ	O
(	O
x	O
)	O
=	O
1.	O
this	O
holds	O
similarly	O
for	O
continuous	B
variables	O
,	O
with	O
summation	O
replaced	O
by	O
integration	O
.	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
=	O
1	O
z	O
c	O
(	O
cid:89	O
)	O
c=1	O
deﬁnition	O
25	O
(	O
markov	O
network	O
)	O
.	O
for	O
a	O
set	O
of	O
variables	O
x	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
a	O
markov	O
network	O
is	O
deﬁned	O
as	O
a	O
product	O
of	O
potentials	O
on	O
subsets	O
of	O
the	O
variables	O
xc	O
⊆	O
x	O
:	O
φc	O
(	O
xc	O
)	O
(	O
4.2.3	O
)	O
graphically	O
this	O
is	O
represented	O
by	O
an	O
undirected	B
graph	I
g	O
with	O
xc	O
,	O
c	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
c	O
being	O
the	O
maximal	O
cliques	O
of	O
g.	O
the	O
constant	O
z	O
ensures	O
the	O
distribution	B
is	O
normalised	O
.	O
the	O
graph	B
is	O
said	O
to	O
satisfy	O
the	O
factorisation	O
property	O
.	O
in	O
the	O
special	O
case	O
that	O
the	O
graph	B
contains	O
cliques	O
of	O
only	O
size	O
2	O
,	O
the	O
distribution	B
is	O
called	O
a	O
pairwise	B
markov	O
network	O
,	O
with	O
potentials	O
deﬁned	O
on	O
each	O
link	O
between	O
two	O
variables	O
.	O
for	O
the	O
case	O
in	O
which	O
clique	B
potentials	O
are	O
strictly	O
positive	O
,	O
this	O
is	O
called	O
a	O
gibbs	O
distribution	B
.	O
remark	O
4	O
(	O
pairwise	B
markov	O
network	O
)	O
.	O
whilst	O
a	O
markov	O
network	O
is	O
formally	O
deﬁned	O
on	O
maximal	O
cliques	O
,	O
in	O
practice	O
authors	O
often	O
use	O
the	O
term	O
to	O
refer	O
to	O
non-maximal	O
cliques	O
.	O
for	O
example	O
,	O
in	O
the	O
graph	B
on	O
the	O
right	O
,	O
the	O
maximal	O
cliques	O
are	O
x1	O
,	O
x2	O
,	O
x3	O
and	O
x2	O
,	O
x3	O
,	O
x4	O
,	O
so	O
that	O
the	O
graph	B
describes	O
a	O
distribution	B
p	O
(	O
x2	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
=	O
in	O
a	O
pairwise	B
network	O
though	O
the	O
poten-	O
φ	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
φ	O
(	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
/z	O
.	O
tials	O
are	O
assumed	O
to	O
be	O
over	O
two-cliques	O
,	O
giving	O
p	O
(	O
x2	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
=	O
φ	O
(	O
x1	O
,	O
x2	O
)	O
φ	O
(	O
x1	O
,	O
x3	O
)	O
φ	O
(	O
x2	O
,	O
x3	O
)	O
φ	O
(	O
x2	O
,	O
x4	O
)	O
φ	O
(	O
x3	O
,	O
x4	O
)	O
/z	O
.	O
x1	O
x2	O
x3	O
x4	O
50	O
draft	O
march	O
9	O
,	O
2010	O
markov	O
networks	O
deﬁnition	O
26	O
(	O
properties	B
of	O
markov	O
networks	O
)	O
.	O
b	O
b	O
b	O
a	O
a	O
a	O
c	O
c	O
c	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
=	O
φac	O
(	O
a	O
,	O
c	O
)	O
φbc	O
(	O
b	O
,	O
c	O
)	O
/z	O
(	O
4.2.4	O
)	O
a	O
and	O
b	O
are	O
unconditionally	O
dependent	O
:	O
p	O
(	O
a	O
,	O
b	O
)	O
(	O
cid:54	O
)	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
.	O
a	O
and	O
b	O
are	O
conditionally	O
independent	O
on	O
c	O
:	O
p	O
(	O
a	O
,	O
b|c	O
)	O
=	O
p	O
(	O
a|c	O
)	O
p	O
(	O
b|c	O
)	O
.	O
→	O
a	O
marginalising	O
over	O
c	O
makes	O
a	O
and	O
b	O
(	O
graphically	O
)	O
dependent	O
.	O
b	O
→	O
a	O
b	O
conditioning	B
on	O
c	O
makes	O
a	O
and	O
b	O
independent	O
.	O
4.2.1	O
markov	O
properties	B
we	O
here	O
state	O
some	O
of	O
the	O
most	O
useful	O
results	O
.	O
the	O
reader	O
is	O
referred	O
to	O
[	O
168	O
]	O
for	O
proofs	O
and	O
more	O
detailed	O
discussion	O
.	O
consider	O
the	O
markov	O
network	O
in	O
ﬁg	O
(	O
4.2a	O
)	O
.	O
here	O
we	O
use	O
the	O
shorthand	O
p	O
(	O
1	O
)	O
≡	O
p	O
(	O
x1	O
)	O
,	O
φ	O
(	O
1	O
,	O
2	O
,	O
3	O
)	O
≡	O
φ	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
etc	O
.	O
we	O
will	O
use	O
this	O
undirected	B
graph	I
to	O
demonstrate	O
conditional	B
independence	O
properties	B
.	O
local	B
markov	O
property	O
deﬁnition	O
27	O
(	O
local	B
markov	O
property	O
)	O
.	O
p	O
(	O
x|x\x	O
)	O
=	O
p	O
(	O
x|ne	O
(	O
x	O
)	O
)	O
when	O
conditioned	O
on	O
its	O
neighbours	O
,	O
x	O
is	O
independent	O
of	O
the	O
remaining	O
variables	O
of	O
the	O
graph	B
.	O
(	O
4.2.5	O
)	O
the	O
conditional	B
distribution	O
p	O
(	O
4|1	O
,	O
2	O
,	O
3	O
,	O
5	O
,	O
6	O
,	O
7	O
)	O
is	O
p	O
(	O
4|1	O
,	O
2	O
,	O
3	O
,	O
5	O
,	O
6	O
,	O
7	O
)	O
=	O
φ	O
(	O
1	O
,	O
2	O
,	O
3	O
)	O
φ	O
(	O
2	O
,	O
3	O
,	O
4	O
)	O
φ	O
(	O
4	O
,	O
5	O
,	O
6	O
)	O
φ	O
(	O
5	O
,	O
6	O
,	O
7	O
)	O
4	O
φ	O
(	O
1	O
,	O
2	O
,	O
3	O
)	O
φ	O
(	O
2	O
,	O
3	O
,	O
4	O
)	O
φ	O
(	O
4	O
,	O
5	O
,	O
6	O
)	O
φ	O
(	O
5	O
,	O
6	O
,	O
7	O
)	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
=	O
φ	O
(	O
2	O
,	O
3	O
,	O
4	O
)	O
φ	O
(	O
4	O
,	O
5	O
,	O
6	O
)	O
4	O
φ	O
(	O
2	O
,	O
3	O
,	O
4	O
)	O
φ	O
(	O
4	O
,	O
5	O
,	O
6	O
)	O
=	O
p	O
(	O
4|2	O
,	O
3	O
,	O
5	O
,	O
6	O
)	O
(	O
4.2.6	O
)	O
the	O
last	O
line	O
above	O
follows	O
since	O
the	O
variable	B
x4	O
only	O
appears	O
in	O
the	O
cliques	O
that	O
border	O
x4	O
.	O
the	O
general-	O
isation	O
of	O
the	O
above	O
example	O
is	O
clear	O
:	O
a	O
mn	O
with	O
positive	O
clique	B
potentials	O
φ	O
,	O
deﬁned	O
with	O
respect	O
to	O
an	O
undirected	B
graph	I
g	O
entails1	O
p	O
(	O
xi|x\i	O
)	O
=	O
p	O
(	O
xi|ne	O
(	O
xi	O
)	O
)	O
.	O
pairwise	B
markov	O
property	O
deﬁnition	O
28	O
(	O
pairwise	B
markov	O
property	O
)	O
.	O
for	O
any	O
non-adjacent	O
vertices	O
x	O
and	O
y	O
x⊥⊥	O
y|x\	O
{	O
x	O
,	O
y	O
}	O
1the	O
notation	O
x\i	O
is	O
shorthand	O
for	O
the	O
set	O
of	O
all	O
variables	O
x	O
excluding	O
variable	B
xi	O
,	O
namely	O
x\xi	O
in	O
set	O
notation	O
.	O
draft	O
march	O
9	O
,	O
2010	O
(	O
4.2.7	O
)	O
51	O
markov	O
networks	O
1	O
2	O
3	O
5	O
6	O
4	O
(	O
a	O
)	O
7	O
1	O
5	O
6	O
2	O
3	O
4	O
(	O
b	O
)	O
7	O
4.2	O
:	O
(	O
a	O
)	O
:	O
figure	O
(	O
b	O
)	O
:	O
φ	O
(	O
1	O
,	O
2	O
,	O
3	O
)	O
φ	O
(	O
2	O
,	O
3	O
,	O
4	O
)	O
φ	O
(	O
4	O
,	O
5	O
,	O
6	O
)	O
φ	O
(	O
5	O
,	O
6	O
,	O
7	O
)	O
.	O
by	O
the	O
global	B
markov	O
property	O
,	O
since	O
every	O
path	B
from	O
1	O
to	O
7	O
passes	O
through	O
4	O
,	O
then	O
1⊥⊥7|4	O
.	O
p	O
(	O
1	O
,	O
4|2	O
,	O
3	O
,	O
5	O
,	O
6	O
,	O
7	O
)	O
=	O
φ	O
(	O
1	O
,	O
2	O
,	O
3	O
)	O
φ	O
(	O
2	O
,	O
3	O
,	O
4	O
)	O
φ	O
(	O
4	O
,	O
5	O
,	O
6	O
)	O
φ	O
(	O
5	O
,	O
6	O
,	O
7	O
)	O
1,4	O
φ	O
(	O
1	O
,	O
2	O
,	O
3	O
)	O
φ	O
(	O
2	O
,	O
3	O
,	O
4	O
)	O
φ	O
(	O
4	O
,	O
5	O
,	O
6	O
)	O
φ	O
(	O
5	O
,	O
6	O
,	O
7	O
)	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
=	O
φ	O
(	O
1	O
,	O
2	O
,	O
3	O
)	O
φ	O
(	O
2	O
,	O
3	O
,	O
4	O
)	O
φ	O
(	O
4	O
,	O
5	O
,	O
6	O
)	O
1,4	O
φ	O
(	O
1	O
,	O
2	O
,	O
3	O
)	O
φ	O
(	O
2	O
,	O
3	O
,	O
4	O
)	O
φ	O
(	O
4	O
,	O
5	O
,	O
6	O
)	O
(	O
4.2.8	O
)	O
(	O
4.2.9	O
)	O
=	O
p	O
(	O
1|2	O
,	O
3	O
,	O
4	O
,	O
5	O
,	O
6	O
,	O
7	O
)	O
p	O
(	O
4|1	O
,	O
2	O
,	O
3	O
,	O
5	O
,	O
6	O
,	O
7	O
)	O
where	O
the	O
last	O
line	O
follows	O
since	O
for	O
ﬁxed	O
2	O
,	O
3	O
,	O
5	O
,	O
6	O
,	O
7	O
,	O
the	O
function	B
φ	O
(	O
1	O
,	O
2	O
,	O
3	O
)	O
φ	O
(	O
2	O
,	O
3	O
,	O
4	O
)	O
φ	O
(	O
4	O
,	O
5	O
,	O
6	O
)	O
is	O
a	O
product	O
of	O
a	O
function	B
on	O
1	O
and	O
a	O
function	B
on	O
4	O
,	O
implying	O
independence	B
.	O
global	B
markov	O
property	O
deﬁnition	O
29	O
(	O
separation	B
)	O
.	O
a	O
subset	O
s	O
separates	O
a	O
subset	O
a	O
from	O
a	O
subset	O
b	O
if	O
every	O
path	B
from	O
any	O
member	O
of	O
a	O
to	O
any	O
member	O
of	O
b	O
passes	O
though	O
s.	O
deﬁnition	O
30	O
(	O
global	B
markov	O
property	O
)	O
.	O
for	O
a	O
disjoint	O
subset	O
of	O
variables	O
,	O
(	O
a	O
,	O
b	O
,	O
s	O
)	O
where	O
s	O
separates	O
a	O
from	O
b	O
in	O
g	O
,	O
then	O
a⊥⊥b|s	O
.	O
2,3,5,6	O
(	O
cid:88	O
)	O
=	O
(	O
cid:88	O
)	O
	O
(	O
cid:88	O
)	O
2,3,5,6	O
=	O
2,3	O
p	O
(	O
1	O
,	O
7|4	O
)	O
∝	O
p	O
(	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
,	O
5	O
,	O
6	O
,	O
7	O
)	O
φ	O
(	O
1	O
,	O
2	O
,	O
3	O
)	O
φ	O
(	O
2	O
,	O
3	O
,	O
4	O
)	O
φ	O
(	O
4	O
,	O
5	O
,	O
6	O
)	O
φ	O
(	O
5	O
,	O
6	O
,	O
7	O
)	O
	O
	O
(	O
cid:88	O
)	O
5,6	O
	O
φ	O
(	O
1	O
,	O
2	O
,	O
3	O
)	O
φ	O
(	O
2	O
,	O
3	O
,	O
4	O
)	O
φ	O
(	O
4	O
,	O
5	O
,	O
6	O
)	O
φ	O
(	O
5	O
,	O
6	O
,	O
7	O
)	O
(	O
4.2.10	O
)	O
(	O
4.2.11	O
)	O
(	O
4.2.12	O
)	O
this	O
implies	O
that	O
p	O
(	O
1	O
,	O
7|4	O
)	O
=	O
p	O
(	O
1|4	O
)	O
p	O
(	O
7|4	O
)	O
.	O
example	O
16	O
(	O
boltzmann	O
machine	O
)	O
.	O
a	O
boltzmann	O
machine	O
is	O
a	O
mn	O
on	O
binary	O
variables	O
dom	O
(	O
xi	O
)	O
=	O
{	O
0	O
,	O
1	O
}	O
of	O
the	O
form	O
i	O
bixi	O
(	O
4.2.13	O
)	O
(	O
cid:80	O
)	O
i	O
<	O
j	O
wij	O
xixj	O
+	O
(	O
cid:80	O
)	O
p	O
(	O
x	O
)	O
=	O
1	O
z	O
(	O
w	O
,	O
b	O
)	O
e	O
where	O
the	O
interactions	O
wij	O
are	O
the	O
‘	O
weights	O
’	O
and	O
the	O
bi	O
the	O
biases	O
.	O
this	O
model	B
has	O
been	O
studied	O
in	O
the	O
machine	O
learning	B
community	O
as	O
a	O
basic	O
model	B
of	O
distributed	O
memory	O
and	O
computation	O
[	O
2	O
]	O
.	O
the	O
graphical	O
model	B
of	O
the	O
bm	O
is	O
an	O
undirected	B
graph	I
with	O
a	O
link	O
been	O
nodes	O
i	O
and	O
j	O
for	O
wij	O
(	O
cid:54	O
)	O
=	O
0.	O
consequently	O
,	O
for	O
all	O
but	O
specially	O
constrained	O
w	O
,	O
the	O
graph	B
is	O
multiply-connected	B
and	O
inference	B
will	O
be	O
typically	O
intractable	O
.	O
4.2.2	O
gibbs	O
networks	O
for	O
simplicity	O
we	O
assume	O
that	O
the	O
potentials	O
are	O
strictly	O
positive	O
in	O
which	O
case	O
mns	O
are	O
also	O
termed	O
gibbs	O
networks	O
.	O
in	O
this	O
case	O
,	O
a	O
gn	O
satisﬁes	O
the	O
following	O
independence	B
relations	O
:	O
52	O
draft	O
march	O
9	O
,	O
2010	O
markov	O
networks	O
x1	O
x3	O
x2	O
x1	O
x2	O
x1	O
x4	O
x3	O
x4	O
x3	O
x4	O
x3	O
x4	O
x3	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
(	O
d	O
)	O
(	O
e	O
)	O
x2	O
x4	O
figure	O
4.3	O
:	O
(	O
a-d	O
)	O
:	O
local	B
distributions	O
.	O
(	O
e	O
)	O
:	O
the	O
markov	O
network	O
consistent	O
with	O
the	O
local	B
distributions	O
.	O
if	O
the	O
local	B
distributions	O
are	O
positive	O
,	O
by	O
the	O
hammersley-cliﬀord	O
theorem	B
,	O
the	O
only	O
joint	B
distribution	O
that	O
can	O
be	O
consistent	B
with	O
the	O
local	B
distributions	O
must	O
be	O
a	O
gibbs	O
distribution	B
with	O
structure	B
given	O
by	O
(	O
e	O
)	O
.	O
4.2.3	O
markov	O
random	O
ﬁelds	O
deﬁnition	O
31	O
(	O
markov	O
random	O
field	O
)	O
.	O
a	O
mrf	O
is	O
deﬁned	O
by	O
a	O
set	O
of	O
distributions	O
p	O
(	O
xi|ne	O
(	O
xi	O
)	O
)	O
where	O
i	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
indexes	O
the	O
distributions	O
and	O
ne	O
(	O
xi	O
)	O
are	O
the	O
neighbours	O
of	O
variable	B
xi	O
,	O
namely	O
that	O
subset	O
of	O
the	O
variables	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
that	O
the	O
distribution	B
of	O
variable	B
xi	O
depends	O
on	O
.	O
the	O
term	O
markov	O
indicates	O
that	O
this	O
is	O
a	O
proper	O
subset	O
of	O
the	O
variables	O
.	O
a	O
distribution	B
is	O
an	O
mrf	O
with	O
respect	O
to	O
an	O
undirected	B
graph	I
g	O
if	O
p	O
(	O
xi|x\i	O
)	O
=	O
p	O
(	O
xi|ne	O
(	O
xi	O
)	O
)	O
(	O
4.2.14	O
)	O
where	O
ne	O
(	O
xi	O
)	O
are	O
the	O
neighbouring	O
variables	O
of	O
variable	B
xi	O
,	O
according	O
to	O
the	O
undirected	B
graph	I
g.	O
hammersley	O
cliﬀord	O
theorem	B
the	O
hammersley-cliﬀord	O
theorem	B
helps	O
resolve	O
questions	O
as	O
to	O
when	O
a	O
set	O
of	O
positive	O
local	O
distributions	O
p	O
(	O
xi|ne	O
(	O
xi	O
)	O
)	O
could	O
ever	O
form	O
a	O
consistent	B
joint	O
distribution	B
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
.	O
local	B
distributions	O
p	O
(	O
xi|ne	O
(	O
xi	O
)	O
)	O
can	O
form	O
a	O
consistent	B
joint	O
distribution	B
if	O
and	O
only	O
if	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
factorises	O
according	O
to	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
(	O
cid:33	O
)	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
=	O
exp	O
−	O
vc	O
(	O
xc	O
)	O
1	O
z	O
(	O
4.2.15	O
)	O
indexed	O
by	O
c.	O
equation	B
(	O
4.2.15	O
)	O
is	O
equivalent	B
to	O
(	O
cid:81	O
)	O
c	O
where	O
the	O
sum	O
is	O
over	O
all	O
cliques	O
and	O
vc	O
(	O
xc	O
)	O
is	O
a	O
real	O
function	B
deﬁned	O
over	O
the	O
variables	O
in	O
the	O
clique	B
c	O
φ	O
(	O
xc	O
)	O
,	O
namely	O
a	O
mn	O
on	O
positive	O
clique	O
potentials	O
.	O
the	O
graph	B
over	O
which	O
the	O
cliques	O
are	O
deﬁned	O
is	O
an	O
undirected	B
graph	I
with	O
a	O
link	O
between	O
xi	O
and	O
xj	O
if	O
p	O
(	O
xi|x\i	O
)	O
(	O
cid:54	O
)	O
=	O
p	O
(	O
xi|x\	O
(	O
i	O
,	O
j	O
)	O
)	O
(	O
4.2.16	O
)	O
that	O
is	O
,	O
if	O
xj	O
has	O
an	O
eﬀect	O
on	O
the	O
conditional	B
distribution	O
of	O
xi	O
,	O
then	O
add	O
an	O
undirected	B
link	O
between	O
xi	O
and	O
xj	O
.	O
this	O
is	O
then	O
repeated	O
over	O
all	O
the	O
variables	O
xi	O
[	O
35	O
,	O
203	O
]	O
,	O
see	O
ﬁg	O
(	O
4.3	O
)	O
.	O
note	O
that	O
the	O
hc	O
theorem	B
does	O
not	O
mean	B
that	O
given	O
a	O
set	O
of	O
conditional	B
distributions	O
,	O
we	O
can	O
always	O
form	O
a	O
consistent	B
joint	O
distribution	B
from	O
them	O
–	O
rather	O
it	O
states	O
what	O
the	O
functional	O
form	O
of	O
a	O
joint	B
distribution	O
for	O
the	O
conditionals	O
to	O
be	O
consistent	B
with	O
the	O
joint	B
,	O
see	O
exercise	O
(	O
45	O
)	O
.	O
4.2.4	O
conditional	B
independence	O
using	O
markov	O
networks	O
for	O
x	O
,	O
y	O
,	O
z	O
each	O
being	O
collections	O
of	O
variables	O
,	O
in	O
section	O
(	O
3.3.3	O
)	O
we	O
discussed	O
an	O
algorithm	B
to	O
determine	O
x	O
⊥⊥y|z	O
.	O
an	O
alternative	O
and	O
more	O
general	O
method	O
(	O
since	O
it	O
handles	O
directed	B
and	O
undirected	B
graphs	O
)	O
uses	O
the	O
following	O
steps	O
:	O
(	O
see	O
[	O
74	O
,	O
169	O
]	O
)	O
draft	O
march	O
9	O
,	O
2010	O
53	O
a	O
c	O
e	O
h	O
i	O
j	O
(	O
a	O
)	O
b	O
d	O
f	O
k	O
g	O
b	O
d	O
f	O
a	O
c	O
e	O
i	O
(	O
b	O
)	O
markov	O
networks	O
figure	O
4.4	O
:	O
(	O
a	O
)	O
:	O
belief	B
network	I
for	O
which	O
we	O
are	O
interested	O
(	O
b	O
)	O
:	O
in	O
checking	O
conditional	B
independence	O
a	O
⊥⊥	O
b|	O
{	O
d	O
,	O
i	O
}	O
.	O
ancestral	B
moralised	O
graph	B
for	O
a	O
⊥⊥	O
b|	O
{	O
d	O
,	O
i	O
}	O
.	O
every	O
path	B
from	O
a	O
red	O
to	O
green	O
node	B
passes	O
through	O
a	O
yellow	O
node	B
,	O
so	O
a	O
and	O
b	O
are	O
independent	O
given	O
d	O
,	O
i.	O
alternatively	O
,	O
if	O
we	O
consider	O
a	O
⊥⊥	O
b|	O
i	O
,	O
the	O
variable	B
d	O
is	O
uncoloured	O
,	O
and	O
we	O
can	O
travel	O
from	O
the	O
red	O
to	O
the	O
green	O
without	O
encountering	O
in	O
this	O
case	O
a	O
is	O
a	O
yellow	O
node	B
(	O
using	O
the	O
e	O
−	O
f	O
path	B
)	O
.	O
dependent	O
on	O
b	O
,	O
conditioned	O
on	O
i.	O
ancestral	B
graph	O
remove	O
from	O
the	O
dag	O
any	O
node	B
which	O
is	O
neither	O
in	O
x	O
∪	O
y	O
∪	O
z	O
nor	O
an	O
ancestor	B
of	O
a	O
node	B
in	O
this	O
set	O
,	O
together	O
with	O
any	O
edges	O
in	O
or	O
out	O
of	O
such	O
nodes	O
.	O
moralisation	B
add	O
a	O
line	O
between	O
any	O
two	O
remaining	O
nodes	O
which	O
have	O
a	O
common	O
child	O
,	O
but	O
are	O
not	O
already	O
connected	B
by	O
an	O
arrow	O
.	O
then	O
remove	O
remaining	O
arrowheads	O
.	O
separation	B
in	O
the	O
undirected	B
graph	I
so	O
constructed	O
,	O
look	O
for	O
a	O
path	B
which	O
joins	O
a	O
node	B
in	O
x	O
to	O
one	O
in	O
y	O
but	O
does	O
not	O
intersect	O
z.	O
if	O
there	O
is	O
no	O
such	O
path	B
deduce	O
that	O
x	O
⊥⊥y|z	O
.	O
for	O
markov	O
networks	O
only	O
the	O
ﬁnal	O
separation	B
criterion	O
needs	O
to	O
be	O
applied	O
.	O
see	O
ﬁg	O
(	O
4.4	O
)	O
for	O
an	O
example	O
.	O
4.2.5	O
lattice	O
models	O
undirected	B
models	O
have	O
a	O
long	O
history	O
in	O
diﬀerent	O
branches	O
of	O
science	O
,	O
especially	O
statistical	O
mechanics	O
on	O
lattices	O
and	O
more	O
recently	O
as	O
models	O
in	O
visual	O
processing	O
in	O
which	O
the	O
models	O
encourage	O
neighbouring	O
variables	O
to	O
be	O
in	O
the	O
same	O
states	O
[	O
35	O
,	O
36	O
,	O
106	O
]	O
.	O
consider	O
a	O
model	B
in	O
which	O
our	O
desire	O
is	O
that	O
states	O
of	O
the	O
binary	O
valued	O
vari-	O
ables	O
x1	O
,	O
.	O
.	O
.	O
,	O
x9	O
,	O
arranged	O
on	O
a	O
lattice	O
(	O
right	O
)	O
should	O
prefer	O
their	O
neighbouring	O
variables	O
to	O
be	O
in	O
the	O
same	O
state	O
(	O
cid:89	O
)	O
i∼j	O
x1	O
x4	O
x7	O
x2	O
x5	O
x8	O
x3	O
x6	O
x9	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
x9	O
)	O
=	O
1	O
z	O
φij	O
(	O
xi	O
,	O
xj	O
)	O
(	O
4.2.17	O
)	O
where	O
i	O
∼	O
j	O
denotes	O
the	O
set	O
of	O
indices	O
where	O
i	O
and	O
j	O
are	O
neighbours	O
in	O
the	O
undirected	B
graph	I
.	O
the	O
ising	O
model	B
a	O
set	O
of	O
potentials	O
for	O
equation	B
(	O
4.2.17	O
)	O
that	O
encourages	O
neighbouring	O
variables	O
to	O
have	O
the	O
same	O
state	O
is	O
φij	O
(	O
xi	O
,	O
xj	O
)	O
=	O
e	O
−	O
1	O
2t	O
(	O
xi−xj	O
)	O
2	O
(	O
4.2.18	O
)	O
this	O
corresponds	O
to	O
a	O
well-known	O
model	B
of	O
the	O
physics	O
of	O
magnetic	O
systems	O
,	O
called	O
the	O
ising	O
model	B
which	O
consists	O
of	O
‘	O
mini-magnets	O
’	O
which	O
prefer	O
to	O
be	O
aligned	O
in	O
the	O
same	O
state	O
,	O
depending	O
on	O
the	O
temperature	O
figure	O
4.5	O
:	O
onsagar	O
magnetisation	O
.	O
as	O
the	O
temperature	O
t	O
decreases	O
towards	O
the	O
critical	O
temperature	O
tc	O
a	O
phase	O
transition	O
occurs	O
in	O
which	O
a	O
large	O
fraction	O
of	O
the	O
variables	O
become	O
aligned	O
in	O
the	O
same	O
state	O
.	O
54	O
draft	O
march	O
9	O
,	O
2010	O
00.511.5200.51t/tcm	O
chain	B
graphical	O
models	O
a	O
c	O
(	O
a	O
)	O
b	O
d	O
a	O
b	O
a	O
cd	O
(	O
b	O
)	O
b	O
e	O
g	O
d	O
f	O
c	O
c	O
aedf	O
bg	O
(	O
c	O
)	O
(	O
d	O
)	O
figure	O
4.6	O
:	O
chain	B
graphs	O
.	O
the	O
chain	B
components	O
are	O
identiﬁed	O
by	O
deleting	O
the	O
directed	B
edges	O
and	O
iden-	O
(	O
a	O
)	O
:	O
chain	B
components	O
are	O
(	O
a	O
)	O
,	O
(	O
b	O
)	O
,	O
(	O
c	O
,	O
d	O
)	O
,	O
which	O
can	O
be	O
tifying	O
the	O
remaining	O
connected	B
components	I
.	O
written	O
as	O
a	O
bn	O
on	O
the	O
cluster	O
variables	O
in	O
(	O
b	O
)	O
.	O
(	O
c	O
)	O
:	O
chain	B
components	O
are	O
(	O
a	O
,	O
e	O
,	O
d	O
,	O
f	O
)	O
,	O
(	O
b	O
,	O
g	O
)	O
,	O
(	O
c	O
)	O
,	O
which	O
has	O
the	O
cluster	O
bn	O
representation	B
(	O
d	O
)	O
.	O
(	O
from	O
[	O
168	O
]	O
)	O
t	O
.	O
for	O
high	O
t	O
the	O
variables	O
behave	O
independently	O
so	O
that	O
no	O
global	O
magnetisation	O
appears	O
.	O
for	O
low	O
t	O
,	O
there	O
is	O
a	O
strong	B
preference	O
for	O
neighbouring	O
mini-magnets	O
to	O
become	O
aligned	O
,	O
generating	O
a	O
strong	B
macro-	O
magnet	O
.	O
remarkably	O
,	O
one	O
can	O
show	O
that	O
,	O
in	O
a	O
very	O
large	O
two-dimensional	O
lattice	O
,	O
below	O
the	O
so-called	O
curie	O
(	O
cid:80	O
)	O
n	O
temperature	O
,	O
tc	O
≈	O
2.269	O
(	O
for	O
±1	O
variables	O
)	O
,	O
the	O
system	B
admits	O
a	O
phase	O
change	O
in	O
that	O
a	O
large	O
fraction	O
of	O
the	O
variables	O
become	O
aligned	O
–	O
above	O
tc	O
,	O
on	O
average	B
,	O
the	O
variables	O
are	O
unaligned	O
.	O
this	O
is	O
depicted	O
in	O
i=1	O
xi|/n	O
is	O
the	O
average	B
alignment	O
of	O
the	O
variables	O
.	O
that	O
this	O
phase	O
change	O
happens	O
ﬁg	O
(	O
4.5	O
)	O
where	O
m	O
=	O
|	O
for	O
non-zero	O
temperature	O
has	O
driven	O
considerable	O
research	O
in	O
this	O
and	O
related	O
areas	O
[	O
40	O
]	O
.	O
global	B
coherence	O
eﬀects	O
such	O
as	O
this	O
that	O
arise	O
from	O
weak	O
local	B
constraints	O
are	O
present	O
in	O
systems	O
that	O
admit	O
emergent	O
behaviour	O
.	O
similar	O
local	B
constraints	O
are	O
popular	O
in	O
image	O
restoration	O
algorithms	O
to	O
clean	O
up	O
noise	O
,	O
under	O
the	O
assumption	O
that	O
noise	O
will	O
not	O
show	O
any	O
local	B
spatial	O
coherence	O
,	O
whilst	O
‘	O
signal	O
’	O
will	O
.	O
an	O
example	O
is	O
given	O
in	O
section	O
(	O
28.8	O
)	O
where	O
we	O
discuss	O
algorithms	O
for	O
inference	B
under	O
special	O
constraints	O
on	O
the	O
mrf	O
.	O
4.3	O
chain	B
graphical	O
models	O
deﬁnition	O
32	O
(	O
chain	B
component	I
)	O
.	O
the	O
chain	B
components	O
of	O
a	O
graph	B
g	O
are	O
obtained	O
by	O
:	O
1.	O
forming	O
a	O
graph	B
g	O
(	O
cid:48	O
)	O
with	O
directed	O
edges	O
removed	O
from	O
g.	O
2.	O
then	O
each	O
connected	B
component	O
in	O
g	O
(	O
cid:48	O
)	O
constitutes	O
a	O
chain	B
component	I
.	O
chain	B
graphs	O
(	O
cgs	O
)	O
contain	O
both	O
directed	B
and	O
undirected	B
links	O
.	O
to	O
develop	O
the	O
intuition	O
,	O
consider	O
ﬁg	O
(	O
4.6a	O
)	O
.	O
the	O
only	O
terms	O
that	O
we	O
can	O
unambiguously	O
specify	O
from	O
this	O
depiction	O
are	O
p	O
(	O
a	O
)	O
and	O
p	O
(	O
b	O
)	O
since	O
there	O
is	O
no	O
mixed	O
interaction	O
of	O
directed	B
and	O
undirected	B
edges	O
at	O
the	O
a	O
and	O
b	O
vertices	O
.	O
by	O
probability	B
,	O
therefore	O
,	O
we	O
must	O
have	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
)	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
p	O
(	O
c	O
,	O
d|a	O
,	O
b	O
)	O
looking	O
at	O
the	O
graph	B
,	O
we	O
might	O
expect	O
the	O
interpretation	O
to	O
be	O
p	O
(	O
c	O
,	O
d|a	O
,	O
b	O
)	O
=	O
φ	O
(	O
c	O
,	O
d	O
)	O
p	O
(	O
c|a	O
)	O
p	O
(	O
d|b	O
)	O
(	O
4.3.1	O
)	O
(	O
4.3.2	O
)	O
(	O
cid:88	O
)	O
c	O
,	O
d	O
however	O
,	O
to	O
ensure	O
normalisation	B
,	O
and	O
also	O
to	O
retain	O
generality	O
,	O
we	O
interpret	O
this	O
chain	B
component	I
as	O
p	O
(	O
c	O
,	O
d|a	O
,	O
b	O
)	O
=	O
φ	O
(	O
c	O
,	O
d	O
)	O
p	O
(	O
c|a	O
)	O
p	O
(	O
d|b	O
)	O
φ	O
(	O
a	O
,	O
b	O
)	O
,	O
with	O
φ	O
(	O
a	O
,	O
b	O
)	O
≡	O
1/	O
φ	O
(	O
c	O
,	O
d	O
)	O
p	O
(	O
c|a	O
)	O
p	O
(	O
d|b	O
)	O
(	O
4.3.3	O
)	O
this	O
leads	O
to	O
the	O
interpretation	O
of	O
a	O
cg	O
as	O
a	O
dag	O
over	O
the	O
chain	B
components	O
.	O
each	O
chain	B
component	I
represents	O
a	O
distribution	B
over	O
the	O
variables	O
of	O
the	O
component	O
,	O
conditioned	O
on	O
the	O
parental	O
components	O
.	O
the	O
conditional	B
distribution	O
is	O
itself	O
a	O
product	O
over	O
the	O
cliques	O
of	O
the	O
undirected	B
component	O
and	O
moralised	O
parental	O
components	O
,	O
including	O
also	O
a	O
factor	B
to	O
ensure	O
normalisation	B
over	O
the	O
chain	B
component	I
.	O
draft	O
march	O
9	O
,	O
2010	O
55	O
expressiveness	O
of	O
graphical	O
models	O
deﬁnition	O
33	O
(	O
chain	B
graph	I
distribution	O
)	O
.	O
the	O
distribution	B
associated	O
with	O
a	O
chain	B
graph	I
g	O
is	O
found	O
by	O
ﬁrst	O
identifying	O
the	O
chain	B
components	O
,	O
τ.	O
then	O
p	O
(	O
x	O
)	O
=	O
(	O
cid:89	O
)	O
τ	O
and	O
p	O
(	O
xτ|pa	O
(	O
xτ	O
)	O
)	O
(	O
cid:89	O
)	O
p	O
(	O
xτ|pa	O
(	O
xτ	O
)	O
)	O
∝	O
φ	O
(	O
xcτ	O
)	O
c∈cτ	O
where	O
cτ	O
denotes	O
the	O
union	O
of	O
the	O
cliques	O
in	O
component	O
τ	O
together	O
with	O
the	O
moralised	O
parental	O
components	O
of	O
τ	O
,	O
with	O
φ	O
being	O
the	O
associated	O
functions	O
deﬁned	O
on	O
each	O
clique	B
.	O
the	O
proportionality	O
factor	B
is	O
determined	O
implicitly	O
by	O
the	O
constraint	O
that	O
the	O
distribution	B
sums	O
to	O
1.	O
bns	O
are	O
cgs	O
in	O
which	O
the	O
connected	B
components	I
are	O
singletons	O
.	O
mns	O
are	O
cgs	O
in	O
which	O
the	O
chain	B
com-	O
ponents	O
are	O
simply	O
the	O
connected	B
components	I
of	O
the	O
undirected	B
graph	I
.	O
cgs	O
can	O
be	O
useful	O
since	O
they	O
are	O
more	O
expressive	O
of	O
ci	O
statements	O
than	O
either	O
belief	B
networks	I
or	O
markov	O
networks	O
alone	O
.	O
the	O
reader	O
is	O
referred	O
to	O
[	O
168	O
]	O
and	O
[	O
99	O
]	O
for	O
further	O
details	O
.	O
example	O
17	O
(	O
chain	B
graphs	O
are	O
more	O
expressive	O
than	O
belief	O
or	O
markov	O
networks	O
)	O
.	O
consider	O
the	O
chain	B
graph	I
in	O
ﬁg	O
(	O
4.7a	O
)	O
,	O
which	O
has	O
chain	B
component	I
decomposition	O
(	O
4.3.4	O
)	O
(	O
4.3.5	O
)	O
(	O
4.3.6	O
)	O
(	O
4.3.7	O
)	O
(	O
4.3.8	O
)	O
(	O
4.3.9	O
)	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
,	O
e	O
,	O
f	O
)	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
p	O
(	O
c	O
,	O
d	O
,	O
e	O
,	O
f|a	O
,	O
b	O
)	O
where	O
p	O
(	O
c	O
,	O
d	O
,	O
e	O
,	O
f|a	O
,	O
b	O
)	O
=	O
φ	O
(	O
a	O
,	O
c	O
)	O
φ	O
(	O
c	O
,	O
e	O
)	O
φ	O
(	O
e	O
,	O
f	O
)	O
φ	O
(	O
d	O
,	O
f	O
)	O
φ	O
(	O
d	O
,	O
b	O
)	O
φ	O
(	O
a	O
,	O
b	O
)	O
with	O
the	O
normalisation	B
requirement	O
φ	O
(	O
a	O
,	O
b	O
)	O
≡	O
1/	O
φ	O
(	O
a	O
,	O
c	O
)	O
φ	O
(	O
c	O
,	O
e	O
)	O
φ	O
(	O
e	O
,	O
f	O
)	O
φ	O
(	O
d	O
,	O
f	O
)	O
φ	O
(	O
d	O
,	O
b	O
)	O
the	O
marginal	B
p	O
(	O
c	O
,	O
d	O
,	O
e	O
,	O
f	O
)	O
is	O
given	O
by	O
φ	O
(	O
c	O
,	O
e	O
)	O
φ	O
(	O
e	O
,	O
f	O
)	O
φ	O
(	O
d	O
,	O
f	O
)	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
a	O
,	O
b	O
φ	O
(	O
a	O
,	O
b	O
)	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
φ	O
(	O
a	O
,	O
c	O
)	O
φ	O
(	O
d	O
,	O
b	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
φ	O
(	O
c	O
,	O
d	O
)	O
(	O
cid:125	O
)	O
(	O
cid:88	O
)	O
c	O
,	O
d	O
,	O
e	O
,	O
f	O
since	O
the	O
marginal	B
distribution	O
of	O
p	O
(	O
c	O
,	O
d	O
,	O
e	O
,	O
f	O
)	O
is	O
an	O
undirected	B
4-cycle	O
,	O
no	O
dag	O
can	O
express	O
the	O
ci	O
state-	O
ments	O
contained	O
in	O
the	O
marginal	B
p	O
(	O
c	O
,	O
d	O
,	O
e	O
,	O
f	O
)	O
.	O
similarly	O
no	O
undirected	O
distribution	B
on	O
the	O
same	O
skeleton	B
as	O
ﬁg	O
(	O
4.7a	O
)	O
could	O
express	O
that	O
a	O
and	O
b	O
are	O
independent	O
(	O
unconditionally	O
)	O
,	O
i.e	O
.	O
p	O
(	O
a	O
,	O
b	O
)	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
.	O
4.4	O
expressiveness	O
of	O
graphical	O
models	O
it	O
is	O
clear	O
that	O
directed	B
distributions	O
can	O
be	O
represented	O
as	O
undirected	O
distributions	O
since	O
one	O
can	O
asso-	O
ciate	O
each	O
(	O
normalised	O
)	O
factor	B
in	O
a	O
directed	B
distribution	O
with	O
a	O
potential	B
.	O
for	O
example	O
,	O
the	O
distribution	B
p	O
(	O
a|b	O
)	O
p	O
(	O
b|c	O
)	O
p	O
(	O
c	O
)	O
can	O
be	O
factored	O
as	O
φ	O
(	O
a	O
,	O
b	O
)	O
φ	O
(	O
b	O
,	O
c	O
)	O
,	O
where	O
φ	O
(	O
a	O
,	O
b	O
)	O
=	O
p	O
(	O
a|b	O
)	O
and	O
φ	O
(	O
b	O
,	O
c	O
)	O
=	O
p	O
(	O
b|c	O
)	O
p	O
(	O
c	O
)	O
,	O
with	O
z	O
=	O
1.	O
hence	O
every	O
belief	B
network	I
can	O
be	O
represented	O
as	O
some	O
mn	O
by	O
simple	O
identiﬁcation	O
of	O
the	O
fac-	O
tors	O
in	O
the	O
distributions	O
.	O
however	O
,	O
in	O
general	O
,	O
the	O
associated	O
undirected	B
graph	I
(	O
which	O
corresponds	O
to	O
the	O
56	O
draft	O
march	O
9	O
,	O
2010	O
expressiveness	O
of	O
graphical	O
models	O
a	O
c	O
e	O
b	O
d	O
f	O
c	O
e	O
d	O
f	O
c	O
e	O
d	O
f	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
4.7	O
:	O
the	O
cg	O
(	O
a	O
)	O
expresses	O
a	O
⊥⊥	O
b	O
|	O
∅	O
and	O
d	O
⊥⊥	O
e	O
|	O
(	O
c	O
,	O
f	O
)	O
.	O
no	O
directed	O
graph	B
could	O
express	O
both	O
these	O
conditions	O
since	O
the	O
marginal	B
distribu-	O
tion	O
p	O
(	O
c	O
,	O
d	O
,	O
e	O
,	O
f	O
)	O
is	O
an	O
undirected	B
four	O
cycle	O
,	O
(	O
b	O
)	O
.	O
any	O
dag	O
on	O
a	O
4	O
cycle	O
must	O
contain	O
a	O
collider	B
,	O
as	O
in	O
(	O
c	O
)	O
and	O
therefore	O
express	O
a	O
diﬀerent	O
set	O
of	O
ci	O
statements	O
than	O
(	O
b	O
)	O
.	O
similarly	O
,	O
no	O
connected	O
markov	O
network	O
can	O
express	O
unconditional	O
independence	B
and	O
hence	O
(	O
a	O
)	O
expresses	O
ci	O
statements	O
that	O
no	O
belief	O
network	O
or	O
markov	O
network	O
alone	O
can	O
express	O
.	O
moralised	O
directed	B
graph	O
)	O
will	O
contain	O
additional	O
links	O
and	O
independence	B
information	O
can	O
be	O
lost	O
.	O
for	O
example	O
,	O
the	O
mn	O
of	O
p	O
(	O
c|a	O
,	O
b	O
)	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
if	O
a	O
single	O
clique	B
φ	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
from	O
which	O
one	O
can	O
not	O
graphically	O
infer	O
that	O
a⊥⊥	O
b|∅	O
.	O
the	O
converse	O
question	O
is	O
whether	O
every	O
undirected	B
model	I
can	O
be	O
represented	O
by	O
a	O
bn	O
with	O
a	O
readily	O
derived	O
link	O
structure	B
?	O
consider	O
the	O
example	O
in	O
ﬁg	O
(	O
4.8	O
)	O
.	O
in	O
this	O
case	O
,	O
there	O
is	O
no	O
directed	O
model	B
with	O
the	O
same	O
link	O
structure	B
that	O
can	O
express	O
the	O
(	O
in	O
)	O
dependencies	O
in	O
the	O
undirected	B
graph	I
.	O
naturally	O
,	O
every	O
probability	B
distribution	O
can	O
be	O
represented	O
by	O
some	O
bn	O
though	O
it	O
may	O
not	O
necessarily	O
have	O
a	O
simple	O
structure	O
and	O
be	O
simply	O
a	O
‘	O
fully	O
connected	B
’	O
cascade	B
style	O
graph	B
.	O
in	O
this	O
sense	O
the	O
dag	O
can	O
not	O
graphically	O
represent	O
the	O
independence/dependence	O
relations	O
true	O
in	O
the	O
distribution	B
.	O
deﬁnition	O
34	O
(	O
independence	B
maps	O
)	O
.	O
a	O
graph	B
is	O
an	O
independence	B
map	O
(	O
i-map	O
)	O
of	O
a	O
given	O
distribution	B
p	O
if	O
every	O
conditional	B
independence	O
statement	O
that	O
one	O
can	O
derive	O
from	O
the	O
graph	B
g	O
is	O
true	O
in	O
the	O
distribution	B
p	O
.	O
that	O
is	O
x	O
⊥⊥y|z	O
g	O
⇒	O
x	O
⊥⊥y|z	O
p	O
(	O
4.4.1	O
)	O
for	O
all	O
disjoint	O
sets	O
x	O
,	O
y	O
,	O
z	O
.	O
similarly	O
,	O
a	O
graph	B
is	O
a	O
dependence	B
map	O
(	O
d-map	O
)	O
of	O
a	O
given	O
distribution	B
p	O
if	O
every	O
conditional	B
independence	O
statement	O
that	O
one	O
can	O
derive	O
from	O
p	O
is	O
true	O
in	O
the	O
graph	B
g.	O
that	O
is	O
x	O
⊥⊥y|z	O
g	O
⇐	O
x	O
⊥⊥y|z	O
p	O
for	O
all	O
disjoint	O
sets	O
x	O
,	O
y	O
,	O
z	O
.	O
a	O
graph	B
g	O
which	O
is	O
both	O
an	O
i-map	O
and	O
a	O
d-map	O
for	O
p	O
is	O
called	O
a	O
perfect	B
map	I
and	O
x	O
⊥⊥y|z	O
g	O
⇔	O
x	O
⊥⊥y|z	O
p	O
(	O
4.4.2	O
)	O
(	O
4.4.3	O
)	O
for	O
all	O
disjoint	O
sets	O
x	O
,	O
y	O
,	O
z	O
.	O
in	O
this	O
case	O
,	O
the	O
set	O
of	O
all	O
conditional	B
independence	O
and	O
dependence	B
statements	O
expressible	O
in	O
the	O
graph	B
g	O
are	O
consistent	B
with	O
p	O
and	O
vice	O
versa	O
.	O
due	O
to	O
inverse	B
modus	I
ponens	I
,	O
example	O
(	O
5	O
)	O
,	O
a	O
dependence	B
map	O
is	O
equivalent	B
to	O
x	O
(	O
cid:62	O
)	O
(	O
cid:62	O
)	O
y|z	O
g	O
⇒	O
x	O
(	O
cid:62	O
)	O
(	O
cid:62	O
)	O
y|z	O
p	O
(	O
4.4.4	O
)	O
although	O
this	O
is	O
less	O
useful	O
since	O
standard	O
graphical	O
model	B
representations	O
can	O
not	O
express	O
dependence	B
.	O
note	O
that	O
the	O
above	O
deﬁnitions	O
are	O
not	O
dependent	O
on	O
the	O
graph	B
being	O
directed	B
or	O
undirected	B
.	O
indeed	O
,	O
some	O
distributions	O
may	O
have	O
a	O
perfect	O
directed	O
map	B
,	O
but	O
no	O
perfect	O
undirected	B
map	O
.	O
for	O
example	O
p	O
(	O
x	O
,	O
y	O
,	O
z	O
)	O
=	O
p	O
(	O
z|x	O
,	O
y	O
)	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
draft	O
march	O
9	O
,	O
2010	O
(	O
4.4.5	O
)	O
57	O
factor	B
graphs	O
b	O
b	O
a	O
c	O
a	O
c	O
d	O
(	O
a	O
)	O
d	O
(	O
b	O
)	O
figure	O
4.8	O
:	O
(	O
a	O
)	O
:	O
an	O
undirected	B
model	I
for	O
which	O
we	O
wish	O
to	O
ﬁnd	O
a	O
directed	B
equivalent	O
.	O
(	O
b	O
)	O
:	O
every	O
dag	O
with	O
the	O
same	O
structure	B
as	O
the	O
undirected	B
model	I
must	O
have	O
a	O
situation	O
where	O
two	O
arrows	O
will	O
point	O
to	O
a	O
node	B
,	O
such	O
as	O
node	O
d.	O
summing	O
over	O
the	O
states	O
of	O
variable	B
d	O
will	O
leave	O
a	O
dag	O
on	O
the	O
variables	O
a	O
,	O
b	O
,	O
c	O
with	O
no	O
link	O
between	O
a	O
and	O
c.	O
this	O
can	O
not	O
represent	O
the	O
undirected	B
model	I
since	O
when	O
one	O
marginals	O
over	O
d	O
in	O
the	O
undirected	B
this	O
adds	O
a	O
link	O
between	O
a	O
and	O
c.	O
has	O
a	O
directed	B
perfect	O
map	B
x	O
→	O
z	O
←	O
y	O
(	O
assuming	O
that	O
p	O
(	O
z|x	O
,	O
y	O
)	O
(	O
cid:54	O
)	O
=	O
φx	O
(	O
x	O
)	O
φy	O
(	O
y	O
)	O
)	O
,	O
but	O
no	O
perfect	O
undirected	B
map	O
.	O
example	O
18.	O
consider	O
the	O
distribution	B
deﬁned	O
on	O
variables	O
t1	O
,	O
t2	O
,	O
y1	O
,	O
y2	O
[	O
232	O
]	O
:	O
p	O
(	O
t1	O
,	O
t2	O
,	O
y1	O
,	O
y2	O
)	O
=	O
p	O
(	O
t1	O
)	O
p	O
(	O
t2	O
)	O
(	O
cid:88	O
)	O
h	O
the	O
bn	O
p	O
(	O
y2|y1	O
,	O
t2	O
)	O
p	O
(	O
y1|t1	O
)	O
p	O
(	O
t1	O
)	O
p	O
(	O
t2	O
)	O
p	O
(	O
y1|t1	O
,	O
h	O
)	O
p	O
(	O
y2|t2	O
,	O
h	O
)	O
p	O
(	O
h	O
)	O
(	O
4.4.6	O
)	O
(	O
4.4.7	O
)	O
is	O
an	O
i-map	O
for	O
distribution	B
(	O
4.4.6	O
)	O
since	O
every	O
independence	B
statement	O
in	O
the	O
bn	O
is	O
true	O
for	O
the	O
corre-	O
sponding	O
graph	B
.	O
however	O
,	O
it	O
is	O
not	O
a	O
d-map	O
since	O
t1	O
(	O
cid:62	O
)	O
(	O
cid:62	O
)	O
t2|	O
y2	O
can	O
not	O
be	O
inferred	O
from	O
the	O
bn	O
.	O
similarly	O
no	O
undirected	O
graph	B
can	O
represent	O
all	O
independence	B
statements	O
true	O
in	O
(	O
4.4.6	O
)	O
.	O
in	O
this	O
case	O
no	O
perfect	O
map	B
(	O
a	O
bn	O
or	O
a	O
mn	O
)	O
can	O
represent	O
(	O
4.4.6	O
)	O
.	O
4.5	O
factor	B
graphs	O
factor	B
graphs	O
(	O
fgs	O
)	O
are	O
mainly	O
used	O
as	O
part	O
of	O
inference	B
algorithms2	O
.	O
deﬁnition	O
35	O
(	O
factor	B
graph	I
)	O
.	O
given	O
a	O
function	B
f	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
=	O
(	O
cid:89	O
)	O
(	O
cid:0	O
)	O
x	O
i	O
(	O
cid:1	O
)	O
ψi	O
i	O
(	O
4.5.1	O
)	O
the	O
fg	O
has	O
a	O
node	B
(	O
represented	O
by	O
a	O
square	O
)	O
for	O
each	O
factor	B
ψi	O
,	O
and	O
a	O
variable	B
node	O
(	O
represented	O
by	O
a	O
circle	O
)	O
for	O
each	O
variable	B
xj	O
.	O
for	O
each	O
xj	O
∈	O
x	O
i	O
an	O
undirected	B
link	O
is	O
made	O
between	O
factor	B
ψi	O
and	O
variable	B
xj	O
.	O
for	O
a	O
factor	B
ψi	O
parents	B
to	O
the	O
factor	B
node	O
,	O
and	O
a	O
directed	B
link	O
from	O
the	O
factor	B
node	O
to	O
the	O
child	O
.	O
this	O
has	O
the	O
same	O
structure	B
as	O
an	O
(	O
undirected	B
)	O
fg	O
,	O
but	O
preserves	O
the	O
information	O
that	O
the	O
factors	O
are	O
distributions	O
.	O
x	O
i	O
(	O
cid:1	O
)	O
which	O
is	O
a	O
conditional	B
distribution	O
p	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
,	O
we	O
may	O
use	O
directed	B
links	O
from	O
the	O
(	O
cid:0	O
)	O
factor	B
graphs	O
are	O
useful	O
since	O
they	O
can	O
preserve	O
more	O
information	O
about	O
the	O
form	O
of	O
the	O
distribution	B
than	O
either	O
a	O
belief	B
network	I
or	O
a	O
markov	O
network	O
(	O
or	O
chain	B
graph	I
)	O
can	O
do	O
alone	O
.	O
consider	O
the	O
distribution	B
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
=	O
φ	O
(	O
a	O
,	O
b	O
)	O
φ	O
(	O
a	O
,	O
c	O
)	O
φ	O
(	O
b	O
,	O
c	O
)	O
(	O
4.5.2	O
)	O
2formally	O
a	O
fg	O
is	O
an	O
alternative	O
graphical	O
depiction	O
of	O
a	O
hypergraph	O
[	O
81	O
]	O
in	O
which	O
the	O
vertices	O
represent	O
variables	O
,	O
and	O
a	O
hyperedge	O
a	O
factor	B
as	O
a	O
function	B
of	O
the	O
variables	O
associated	O
with	O
the	O
hyperedge	O
.	O
a	O
fg	O
is	O
therefore	O
a	O
hypergraph	O
with	O
the	O
additional	O
interpretation	O
that	O
the	O
graph	B
represents	O
a	O
function	B
deﬁned	O
as	O
products	O
over	O
the	O
associated	O
hyperedges	O
.	O
many	O
thanks	O
to	O
robert	O
cowell	O
for	O
this	O
observation	O
.	O
58	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
a	O
a	O
a	O
a	O
a	O
c	O
b	O
c	O
b	O
c	O
b	O
c	O
b	O
c	O
b	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
(	O
d	O
)	O
(	O
e	O
)	O
a	O
c	O
b	O
d	O
(	O
f	O
)	O
(	O
b	O
)	O
:	O
φ	O
(	O
a	O
,	O
b	O
)	O
φ	O
(	O
b	O
,	O
c	O
)	O
φ	O
(	O
c	O
,	O
a	O
)	O
.	O
figure	O
4.9	O
:	O
(	O
a	O
)	O
:	O
φ	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
.	O
(	O
c	O
)	O
:	O
φ	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
.	O
both	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
have	O
the	O
same	O
undirected	B
graphical	O
model	B
,	O
(	O
c	O
)	O
.	O
(	O
e	O
)	O
:	O
directed	B
fg	O
of	O
the	O
bn	O
in	O
(	O
d	O
)	O
.	O
(	O
a	O
)	O
is	O
an	O
undirected	B
fg	O
of	O
(	O
d	O
)	O
.	O
the	O
advantage	O
of	O
(	O
e	O
)	O
over	O
(	O
a	O
)	O
is	O
that	O
information	O
regarding	O
the	O
marginal	B
independence	O
of	O
variables	O
b	O
and	O
c	O
is	O
clear	O
from	O
graph	B
(	O
e	O
)	O
,	O
whereas	O
one	O
could	O
only	O
ascertain	O
this	O
by	O
examination	O
of	O
the	O
numerical	B
entries	O
of	O
the	O
(	O
f	O
)	O
:	O
a	O
partially	O
directed	O
fg	O
of	O
p	O
(	O
a|b	O
,	O
c	O
)	O
φ	O
(	O
d	O
,	O
c	O
)	O
φ	O
(	O
b	O
,	O
d	O
)	O
.	O
no	O
directed	O
,	O
undirected	B
or	O
factors	O
in	O
graph	B
(	O
a	O
)	O
.	O
chain	B
graph	I
can	O
represent	O
both	O
the	O
conditional	B
and	O
marginal	B
independence	O
statements	O
expressed	O
by	O
this	O
graph	B
and	O
also	O
the	O
factored	O
structure	B
of	O
the	O
undirected	B
terms	O
.	O
the	O
mn	O
representation	B
is	O
given	O
in	O
ﬁg	O
(	O
4.9c	O
)	O
.	O
however	O
,	O
ﬁg	O
(	O
4.9c	O
)	O
could	O
equally	O
represent	O
some	O
unfactored	O
clique	B
potential	O
φ	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
.	O
in	O
this	O
sense	O
,	O
the	O
fg	O
representation	B
in	O
ﬁg	O
(	O
4.9b	O
)	O
more	O
precisely	O
conveys	O
the	O
form	O
of	O
distribution	B
equation	O
(	O
4.5.2	O
)	O
.	O
an	O
unfactored	O
clique	B
potential	O
φ	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
is	O
represented	O
by	O
the	O
fg	O
ﬁg	O
(	O
4.9a	O
)	O
.	O
hence	O
diﬀerent	O
fgs	O
can	O
have	O
the	O
same	O
mn	O
since	O
information	O
regarding	O
the	O
structure	B
of	O
the	O
clique	B
potential	O
is	O
lost	O
in	O
the	O
mn	O
.	O
4.5.1	O
conditional	B
independence	O
in	O
factor	B
graphs	O
a	O
rule	O
which	O
works	O
with	O
both	O
directed	B
and	O
undirected	B
(	O
and	O
partially	O
directed	O
)	O
fgs	O
is	O
as	O
follows	O
[	O
96	O
]	O
.	O
to	O
determine	O
whether	O
two	O
variables	O
are	O
independent	O
given	O
a	O
set	O
of	O
conditioned	O
variables	O
,	O
consider	O
all	O
paths	O
connecting	O
the	O
two	O
variables	O
.	O
if	O
all	O
paths	O
are	O
blocked	B
,	O
the	O
variables	O
are	O
conditionally	O
independent	O
.	O
a	O
path	B
is	O
blocked	B
if	O
any	O
one	O
or	O
more	O
of	O
the	O
following	O
conditions	O
are	O
satisﬁed	O
:	O
•	O
one	O
of	O
the	O
variables	O
in	O
the	O
path	B
is	O
in	O
the	O
conditioning	B
set	O
.	O
•	O
one	O
of	O
the	O
variables	O
or	O
factors	O
in	O
the	O
path	B
has	O
two	O
incoming	O
edges	O
that	O
are	O
part	O
of	O
the	O
path	B
,	O
and	O
neither	O
the	O
variable	B
or	O
factor	B
nor	O
any	O
of	O
its	O
descendants	O
are	O
in	O
the	O
conditioning	B
set	O
.	O
4.6	O
notes	O
a	O
detailed	O
discussion	O
of	O
the	O
axiomatic	O
and	O
logical	O
basis	O
of	O
conditional	B
independence	O
is	O
given	O
in	O
[	O
45	O
]	O
and	O
[	O
264	O
]	O
.	O
4.7	O
code	O
condindep.m	B
:	O
conditional	B
independence	O
test	O
p	O
(	O
x	O
,	O
y	O
|z	O
)	O
=	O
p	O
(	O
x|z	O
)	O
p	O
(	O
y	O
|z	O
)	O
?	O
4.8	O
exercises	O
exercise	O
37	O
.	O
1.	O
consider	O
the	O
pairwise	B
markov	O
network	O
,	O
p	O
(	O
x	O
)	O
=	O
φ	O
(	O
x1	O
,	O
x2	O
)	O
φ	O
(	O
x2	O
,	O
x3	O
)	O
φ	O
(	O
x3	O
,	O
x4	O
)	O
φ	O
(	O
x4	O
,	O
x1	O
)	O
express	O
in	O
terms	O
of	O
φ	O
the	O
following	O
:	O
p	O
(	O
x2|x1	O
,	O
x3	O
)	O
,	O
p	O
(	O
x1|x2	O
,	O
x4	O
)	O
,	O
draft	O
march	O
9	O
,	O
2010	O
p	O
(	O
x3|x2	O
,	O
x4	O
)	O
,	O
p	O
(	O
x4|x1	O
,	O
x3	O
)	O
(	O
4.8.1	O
)	O
(	O
4.8.2	O
)	O
59	O
exercises	O
2.	O
for	O
a	O
set	O
of	O
local	B
distributions	O
deﬁned	O
as	O
p1	O
(	O
x1|x2	O
,	O
x4	O
)	O
,	O
p2	O
(	O
x2|x1	O
,	O
x3	O
)	O
,	O
p3	O
(	O
x3|x2	O
,	O
x4	O
)	O
,	O
p4	O
(	O
x4|x1	O
,	O
x3	O
)	O
(	O
4.8.3	O
)	O
is	O
it	O
always	O
possible	O
to	O
ﬁnd	O
a	O
joint	B
distribution	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
consistent	B
with	O
these	O
local	B
conditional	O
distributions	O
?	O
exercise	O
38.	O
consider	O
the	O
markov	O
network	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
=	O
φab	O
(	O
a	O
,	O
b	O
)	O
φbc	O
(	O
b	O
,	O
c	O
)	O
(	O
4.8.4	O
)	O
nominally	O
,	O
by	O
summing	O
over	O
b	O
,	O
the	O
variables	O
a	O
and	O
c	O
are	O
dependent	O
.	O
for	O
binary	O
b	O
,	O
explain	O
a	O
situation	O
in	O
which	O
this	O
is	O
not	O
the	O
case	O
,	O
so	O
that	O
marginally	O
,	O
a	O
and	O
c	O
are	O
independent	O
.	O
exercise	O
39.	O
show	O
that	O
for	O
the	O
boltzmann	O
machine	O
p	O
(	O
x	O
)	O
=	O
1	O
z	O
(	O
w	O
,	O
b	O
)	O
extwx+xtb	O
one	O
may	O
assume	O
,	O
without	O
loss	O
of	O
generality	O
,	O
w	O
=	O
wt	O
.	O
exercise	O
40	O
.	O
(	O
4.8.5	O
)	O
the	O
restricted	B
boltzmann	O
machine	O
(	O
or	O
harmonium	B
[	O
253	O
]	O
)	O
is	O
a	O
specially	O
constrained	O
boltzmann	O
machine	O
on	O
a	O
bipartite	O
graph	B
,	O
consisting	O
of	O
a	O
layer	O
of	O
visible	B
variables	O
v	O
=	O
(	O
v1	O
,	O
.	O
.	O
.	O
,	O
vv	O
)	O
and	O
hidden	B
variables	I
h	O
=	O
(	O
h1	O
,	O
.	O
.	O
.	O
,	O
hh	O
)	O
:	O
h1	O
h2	O
1	O
p	O
(	O
v	O
,	O
h	O
)	O
=	O
z	O
(	O
w	O
,	O
a	O
,	O
b	O
)	O
evtwh+atv+bth	O
all	O
variables	O
are	O
binary	O
taking	O
states	O
0	O
,	O
1	O
.	O
(	O
4.8.6	O
)	O
v1	O
v2	O
v3	O
1.	O
show	O
that	O
the	O
distribution	B
of	O
hidden	B
units	O
conditional	B
on	O
the	O
visible	B
units	O
factorises	O
as	O
bi	O
+	O
(	O
cid:88	O
)	O
j	O
	O
wjivj	O
(	O
4.8.7	O
)	O
(	O
4.8.8	O
)	O
(	O
4.8.9	O
)	O
p	O
(	O
h|v	O
)	O
=	O
(	O
cid:89	O
)	O
i	O
p	O
(	O
hi|v	O
)	O
,	O
with	O
p	O
(	O
hi|v	O
)	O
=	O
σ	O
where	O
σ	O
(	O
x	O
)	O
=	O
ex/	O
(	O
1	O
+	O
ex	O
)	O
.	O
2.	O
by	O
symmetry	O
arguments	O
,	O
write	O
down	O
the	O
form	O
of	O
the	O
conditional	B
p	O
(	O
v|h	O
)	O
.	O
3.	O
is	O
p	O
(	O
h	O
)	O
factorised	B
?	O
4.	O
can	O
the	O
partition	B
function	I
z	O
(	O
w	O
,	O
a	O
,	O
b	O
)	O
be	O
computed	O
eﬃciently	O
for	O
the	O
rbm	O
?	O
exercise	O
41.	O
consider	O
p	O
(	O
x	O
)	O
=	O
φ	O
(	O
x1	O
,	O
x100	O
)	O
99	O
(	O
cid:89	O
)	O
i=1	O
φ	O
(	O
xi	O
,	O
xi+1	O
)	O
is	O
it	O
possible	O
to	O
compute	O
argmax	O
x1	O
,	O
...	O
,	O
x100	O
p	O
(	O
x	O
)	O
eﬃciently	O
?	O
exercise	O
42.	O
you	O
are	O
given	O
that	O
x⊥⊥	O
y|	O
(	O
z	O
,	O
u	O
)	O
,	O
u⊥⊥	O
z|∅	O
derive	O
the	O
most	O
general	O
form	O
of	O
probability	B
distribution	O
p	O
(	O
x	O
,	O
y	O
,	O
z	O
,	O
u	O
)	O
consistent	B
with	O
these	O
statements	O
.	O
does	O
this	O
distribution	B
have	O
a	O
simple	O
graphical	O
model	B
?	O
60	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
exercise	O
43.	O
the	O
undirected	B
graph	I
represents	O
a	O
markov	O
network	O
with	O
nodes	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
,	O
x5	O
,	O
count-	O
ing	O
clockwise	O
around	O
the	O
pentagon	O
with	O
potentials	O
φ	O
(	O
xi	O
,	O
x1+mod	O
(	O
i,5	O
)	O
)	O
.	O
show	O
that	O
the	O
joint	B
distribution	O
can	O
be	O
written	O
as	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
,	O
x5	O
)	O
=	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x5	O
)	O
p	O
(	O
x2	O
,	O
x4	O
,	O
x5	O
)	O
p	O
(	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
p	O
(	O
x2	O
,	O
x5	O
)	O
p	O
(	O
x2	O
,	O
x4	O
)	O
(	O
4.8.10	O
)	O
and	O
express	O
the	O
marginal	B
probability	O
tables	O
explicitly	O
as	O
functions	O
of	O
the	O
potentials	O
φ	O
(	O
xi	O
,	O
xj	O
)	O
.	O
exercise	O
44.	O
consider	O
the	O
belief	B
network	I
on	O
the	O
right	O
.	O
1.	O
write	O
down	O
a	O
markov	O
network	O
of	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
.	O
2.	O
is	O
your	O
markov	O
network	O
a	O
perfect	B
map	I
of	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
?	O
h1	O
h2	O
x1	O
x2	O
x3	O
exercise	O
45.	O
two	O
research	O
labs	O
work	O
independently	O
on	O
the	O
relationship	O
between	O
discrete	B
variables	O
x	O
and	O
y.	O
lab	O
a	O
proudly	O
announces	O
that	O
they	O
have	O
ascertained	O
distribution	B
pa	O
(	O
x|y	O
)	O
from	O
data	B
.	O
lab	O
b	O
proudly	O
announces	O
that	O
they	O
have	O
ascertained	O
pb	O
(	O
y|x	O
)	O
from	O
data	B
.	O
1.	O
is	O
it	O
always	O
possible	O
to	O
ﬁnd	O
a	O
joint	B
distribution	O
p	O
(	O
x	O
,	O
y	O
)	O
consistent	B
with	O
the	O
results	O
of	O
both	O
labs	O
?	O
2.	O
is	O
it	O
possible	O
to	O
deﬁne	O
consistent	B
marginals	O
p	O
(	O
x	O
)	O
and	O
p	O
(	O
y	O
)	O
,	O
in	O
the	O
sense	O
that	O
p	O
(	O
x	O
)	O
=	O
(	O
cid:80	O
)	O
and	O
p	O
(	O
y	O
)	O
=	O
(	O
cid:80	O
)	O
y	O
pa	O
(	O
x|y	O
)	O
p	O
(	O
y	O
)	O
x	O
pb	O
(	O
y|x	O
)	O
p	O
(	O
x	O
)	O
?	O
if	O
so	O
,	O
explain	O
how	O
to	O
ﬁnd	O
such	O
marginals	O
.	O
if	O
not	O
,	O
explain	O
why	O
not	O
.	O
exercise	O
46.	O
research	O
lab	O
a	O
states	O
its	O
ﬁndings	O
about	O
a	O
set	O
of	O
variables	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
as	O
a	O
list	O
la	O
of	O
conditional	B
independence	O
statements	O
.	O
lab	O
b	O
similarly	O
provides	O
a	O
list	O
of	O
conditional	B
independence	O
statements	O
lb	O
.	O
1.	O
is	O
it	O
possible	O
to	O
ﬁnd	O
a	O
distribution	B
which	O
is	O
consistent	B
with	O
la	O
and	O
lb	O
?	O
2.	O
if	O
the	O
lists	O
also	O
contain	O
dependence	B
statements	O
,	O
how	O
could	O
one	O
attempt	O
to	O
ﬁnd	O
a	O
distribution	B
that	O
is	O
consistent	B
with	O
both	O
lists	O
?	O
exercise	O
47.	O
consider	O
the	O
distribution	B
p	O
(	O
x	O
,	O
y	O
,	O
w	O
,	O
z	O
)	O
=	O
p	O
(	O
z|w	O
)	O
p	O
(	O
w|x	O
,	O
y	O
)	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
(	O
4.8.11	O
)	O
1.	O
write	O
p	O
(	O
x|z	O
)	O
using	O
a	O
formula	O
involving	O
(	O
all	O
or	O
some	O
of	O
)	O
p	O
(	O
z|w	O
)	O
,	O
p	O
(	O
w|x	O
,	O
y	O
)	O
,	O
p	O
(	O
x	O
)	O
,	O
p	O
(	O
y	O
)	O
.	O
2.	O
write	O
p	O
(	O
y|z	O
)	O
using	O
a	O
formula	O
involving	O
(	O
all	O
or	O
some	O
of	O
)	O
p	O
(	O
z|w	O
)	O
,	O
p	O
(	O
w|x	O
,	O
y	O
)	O
,	O
p	O
(	O
x	O
)	O
,	O
p	O
(	O
y	O
)	O
.	O
3.	O
using	O
the	O
above	O
results	O
,	O
derive	O
an	O
explicit	O
condition	O
for	O
x	O
⊥⊥	O
y|	O
z	O
and	O
explain	O
if	O
this	O
is	O
satisﬁed	O
for	O
this	O
distribution	B
.	O
exercise	O
48.	O
consider	O
the	O
distribution	B
p	O
(	O
t1	O
,	O
t2	O
,	O
y1	O
,	O
y2	O
,	O
h	O
)	O
=	O
p	O
(	O
y1|t1	O
,	O
h	O
)	O
p	O
(	O
y2|t2	O
,	O
h	O
)	O
p	O
(	O
t1	O
)	O
p	O
(	O
t2	O
)	O
p	O
(	O
h	O
)	O
1.	O
draw	O
a	O
belief	B
network	I
for	O
this	O
distribution	B
.	O
2.	O
can	O
the	O
distribution	B
p	O
(	O
t1	O
,	O
t2	O
,	O
y1	O
,	O
y2	O
)	O
=	O
(	O
cid:88	O
)	O
h	O
p	O
(	O
y1|t1	O
,	O
h	O
)	O
p	O
(	O
y2|t2	O
,	O
h	O
)	O
p	O
(	O
t1	O
)	O
p	O
(	O
t2	O
)	O
p	O
(	O
h	O
)	O
be	O
written	O
as	O
a	O
(	O
‘	O
non-complete	O
’	O
)	O
belief	B
network	I
?	O
3.	O
show	O
that	O
for	O
p	O
(	O
t1	O
,	O
t2	O
,	O
y1	O
,	O
y2	O
)	O
as	O
deﬁned	O
above	O
t1⊥⊥	O
y2|∅	O
.	O
draft	O
march	O
9	O
,	O
2010	O
(	O
4.8.12	O
)	O
(	O
4.8.13	O
)	O
61	O
exercise	O
49.	O
consider	O
the	O
distribution	B
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
)	O
=	O
φab	O
(	O
a	O
,	O
b	O
)	O
φbc	O
(	O
b	O
,	O
c	O
)	O
φcd	O
(	O
c	O
,	O
d	O
)	O
φda	O
(	O
d	O
,	O
a	O
)	O
where	O
the	O
φ	O
are	O
potentials	O
.	O
1.	O
draw	O
a	O
markov	O
network	O
for	O
this	O
distribution	B
.	O
exercises	O
(	O
4.8.14	O
)	O
2.	O
explain	O
if	O
the	O
distribution	B
can	O
be	O
represented	O
as	O
a	O
(	O
‘	O
non-complete	O
’	O
)	O
belief	B
network	I
.	O
3.	O
derive	O
explicitly	O
if	O
a⊥⊥	O
c|∅	O
.	O
exercise	O
50.	O
show	O
how	O
for	O
any	O
singly-connected	B
markov	O
network	O
,	O
one	O
may	O
construct	O
a	O
markov	O
equivalent	B
belief	O
network	O
.	O
exercise	O
51.	O
consider	O
a	O
pairwise	B
binary	O
markov	O
network	O
deﬁned	O
on	O
variables	O
si	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
ij∈e	O
φij	O
(	O
si	O
,	O
sj	O
)	O
,	O
where	O
e	O
is	O
a	O
given	O
edge	O
set	O
and	O
the	O
potentials	O
φij	O
are	O
arbitrary	O
.	O
explain	O
how	O
to	O
translate	O
such	O
a	O
markov	O
network	O
into	O
a	O
boltzmann	O
machine	O
.	O
with	O
p	O
(	O
s	O
)	O
=	O
(	O
cid:81	O
)	O
62	O
draft	O
march	O
9	O
,	O
2010	O
chapter	O
5	O
eﬃcient	B
inference	O
in	O
trees	O
5.1	O
marginal	B
inference	O
given	O
a	O
distribution	B
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
,	O
inference	B
is	O
the	O
process	O
of	O
computing	O
functions	O
of	O
the	O
distribution	B
.	O
for	O
example	O
,	O
computing	O
a	O
marginal	B
conditioned	O
on	O
a	O
subset	O
of	O
variables	O
being	O
in	O
a	O
particular	O
state	O
would	O
be	O
an	O
inference	B
task	O
.	O
similarly	O
,	O
computing	O
the	O
mean	B
of	O
a	O
variable	B
can	O
be	O
considered	O
an	O
inference	B
task	O
.	O
the	O
main	O
focus	O
of	O
this	O
chapter	O
is	O
on	O
eﬃcient	B
inference	O
algorithms	O
for	O
marginal	B
inference	O
in	O
singly-connected	B
structures	O
.	O
an	O
eﬃcient	B
algorithm	O
for	O
multiply-connected	B
graphs	O
will	O
be	O
considered	O
in	O
chapter	O
(	O
6	O
)	O
.	O
marginal	B
inference	O
is	O
concerned	O
with	O
the	O
computation	O
of	O
the	O
distribution	B
of	O
a	O
subset	O
of	O
variables	O
,	O
possibly	O
conditioned	O
on	O
another	O
subset	O
.	O
for	O
example	O
,	O
given	O
a	O
joint	B
distribution	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
,	O
x5	O
)	O
,	O
a	O
marginal	B
inference	O
given	O
evidence	B
calculation	O
is	O
p	O
(	O
x1	O
=	O
tr	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
,	O
x5	O
)	O
(	O
5.1.1	O
)	O
(	O
cid:88	O
)	O
p	O
(	O
x5|x1	O
=	O
tr	O
)	O
∝	O
x2	O
,	O
x3	O
,	O
x4	O
marginal	B
inference	O
for	O
discrete	B
models	O
involves	O
summation	O
and	O
will	O
be	O
the	O
focus	O
of	O
our	O
development	O
.	O
in	O
principle	O
the	O
algorithms	O
carry	O
over	O
to	O
continuous	B
variable	O
models	O
although	O
the	O
lack	O
of	O
closure	O
of	O
most	O
continuous	O
distributions	O
under	O
marginalisation	B
(	O
the	O
gaussian	O
being	O
a	O
notable	O
exception	O
)	O
can	O
make	O
the	O
direct	O
transference	O
of	O
these	O
algorithms	O
to	O
the	O
continuous	B
domain	O
problematic	O
.	O
5.1.1	O
variable	B
elimination	I
in	O
a	O
markov	O
chain	B
and	O
message	B
passing	I
a	O
key	O
concept	O
in	O
eﬃcient	B
inference	O
is	O
message	B
passing	I
in	O
which	O
information	O
from	O
the	O
graph	B
is	O
summarised	O
by	O
local	B
edge	O
information	O
.	O
to	O
develop	O
this	O
idea	O
,	O
consider	O
the	O
four	O
variable	B
markov	O
chain	B
(	O
markov	O
chains	O
are	O
discussed	O
in	O
more	O
depth	O
in	O
section	O
(	O
23.1	O
)	O
)	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
)	O
=	O
p	O
(	O
a|b	O
)	O
p	O
(	O
b|c	O
)	O
p	O
(	O
c|d	O
)	O
p	O
(	O
d	O
)	O
(	O
5.1.2	O
)	O
as	O
given	O
in	O
ﬁg	O
(	O
5.1	O
)	O
,	O
for	O
which	O
our	O
task	O
is	O
to	O
calculate	O
the	O
marginal	B
p	O
(	O
a	O
)	O
.	O
for	O
simplicity	O
,	O
we	O
assume	O
that	O
each	O
of	O
the	O
variables	O
has	O
domain	B
{	O
0	O
,	O
1	O
}	O
.	O
then	O
(	O
cid:88	O
)	O
p	O
(	O
a	O
=	O
0	O
)	O
=	O
p	O
(	O
a	O
=	O
0	O
,	O
b	O
,	O
c	O
,	O
d	O
)	O
=	O
b∈	O
{	O
0,1	O
}	O
,	O
c∈	O
{	O
0,1	O
}	O
,	O
d∈	O
{	O
0,1	O
}	O
b∈	O
{	O
0,1	O
}	O
,	O
c∈	O
{	O
0,1	O
}	O
,	O
d∈	O
{	O
0,1	O
}	O
p	O
(	O
a	O
=	O
0|b	O
)	O
p	O
(	O
b|c	O
)	O
p	O
(	O
c|d	O
)	O
p	O
(	O
d	O
)	O
(	O
5.1.3	O
)	O
(	O
cid:88	O
)	O
a	O
b	O
c	O
d	O
figure	O
5.1	O
:	O
a	O
markov	O
chain	B
is	O
of	O
the	O
form	O
p	O
(	O
xt	O
)	O
(	O
cid:81	O
)	O
t−1	O
t=1	O
p	O
(	O
xt|xt+1	O
)	O
for	O
some	O
assignment	O
of	O
the	O
variables	O
to	O
labels	O
xt	O
.	O
variable	B
elim-	O
ination	O
can	O
be	O
carried	O
out	O
in	O
time	O
linear	B
in	O
the	O
number	O
of	O
vari-	O
ables	O
in	O
the	O
chain	B
.	O
63	O
we	O
could	O
carry	O
out	O
this	O
computation	O
by	O
simply	O
summing	O
each	O
of	O
the	O
probabilities	O
for	O
the	O
2	O
×	O
2	O
×	O
2	O
=	O
8	O
states	O
of	O
the	O
variables	O
b	O
,	O
c	O
and	O
d.	O
marginal	B
inference	O
a	O
more	O
eﬃcient	B
approach	O
is	O
to	O
push	O
the	O
summation	O
over	O
d	O
as	O
far	O
to	O
the	O
right	O
as	O
possible	O
:	O
p	O
(	O
a	O
=	O
0	O
)	O
=	O
(	O
cid:88	O
)	O
b∈	O
{	O
0,1	O
}	O
,	O
c∈	O
{	O
0,1	O
}	O
p	O
(	O
a	O
=	O
0|b	O
)	O
p	O
(	O
b|c	O
)	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
d∈	O
{	O
0,1	O
}	O
(	O
cid:125	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
p	O
(	O
c|d	O
)	O
p	O
(	O
d	O
)	O
γd	O
(	O
c	O
)	O
(	O
5.1.4	O
)	O
where	O
γd	O
(	O
c	O
)	O
is	O
a	O
(	O
two	O
state	O
)	O
potential	B
.	O
similarly	O
,	O
we	O
can	O
distribute	O
the	O
summation	O
over	O
c	O
as	O
far	O
to	O
the	O
right	O
as	O
possible	O
:	O
p	O
(	O
a	O
=	O
0	O
)	O
=	O
(	O
cid:88	O
)	O
b∈	O
{	O
0,1	O
}	O
p	O
(	O
a	O
=	O
0|b	O
)	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
c∈	O
{	O
0,1	O
}	O
then	O
,	O
ﬁnally	O
,	O
p	O
(	O
a	O
=	O
0	O
)	O
=	O
(	O
cid:88	O
)	O
b∈	O
{	O
0,1	O
}	O
p	O
(	O
a	O
=	O
0|b	O
)	O
γc	O
(	O
b	O
)	O
(	O
cid:125	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
p	O
(	O
b|c	O
)	O
γd	O
(	O
c	O
)	O
γc	O
(	O
b	O
)	O
(	O
5.1.5	O
)	O
(	O
5.1.6	O
)	O
by	O
distributing	O
the	O
summations	O
we	O
have	O
made	O
2	O
+	O
2	O
+	O
2	O
=	O
6	O
additions	O
,	O
compared	O
to	O
8	O
from	O
the	O
naive	O
approach	O
.	O
whilst	O
this	O
saving	O
may	O
not	O
appear	O
much	O
,	O
the	O
important	O
point	O
is	O
that	O
the	O
number	O
of	O
computa-	O
tions	O
for	O
a	O
chain	B
of	O
length	O
t	O
would	O
scale	O
linearly	O
with	O
t	O
,	O
as	O
opposed	O
to	O
exponentially	O
for	O
the	O
naive	O
approach	O
.	O
this	O
procedure	O
is	O
naturally	O
enough	O
called	O
variable	B
elimination	I
,	O
since	O
each	O
time	O
we	O
sum	O
over	O
the	O
states	O
of	O
a	O
variable	B
,	O
we	O
eliminate	O
it	O
from	O
the	O
distribution	B
.	O
we	O
can	O
always	O
perform	O
variable	B
elimination	I
in	O
a	O
chain	B
eﬃciently	O
since	O
there	O
is	O
a	O
natural	B
way	O
to	O
distribute	O
the	O
summations	O
,	O
working	O
inwards	O
from	O
the	O
edges	O
.	O
note	O
that	O
in	O
the	O
above	O
case	O
,	O
the	O
potentials	O
are	O
in	O
fact	O
always	O
distributions	O
–	O
we	O
are	O
just	O
recursively	O
computing	O
the	O
marginal	B
distribution	O
of	O
the	O
right	O
leaf	O
of	O
the	O
chain	B
.	O
one	O
can	O
view	O
the	O
elimination	O
of	O
a	O
variable	B
as	O
passing	B
a	O
message	B
(	O
information	O
)	O
to	O
a	O
neighbouring	O
vertex	B
on	O
the	O
graph	B
.	O
we	O
can	O
calculate	O
a	O
univariate-marginal	O
of	O
any	O
singly-connected	B
graph	O
by	O
starting	O
at	O
a	O
leaf	O
of	O
the	O
tree	B
,	O
eliminating	O
the	O
variable	B
there	O
,	O
and	O
then	O
working	O
inwards	O
,	O
nibbling	O
oﬀ	O
each	O
time	O
a	O
leaf	O
of	O
the	O
remaining	O
tree	B
.	O
provided	O
we	O
perform	O
elimination	O
from	O
the	O
leaves	O
inwards	O
,	O
then	O
the	O
structure	B
of	O
the	O
remaining	O
graph	B
is	O
simply	O
a	O
subtree	O
of	O
the	O
original	O
tree	B
,	O
albeit	O
with	O
the	O
conditional	B
probability	I
table	O
entries	O
modiﬁed	O
to	O
potentials	O
which	O
update	O
under	O
recursion	O
.	O
this	O
is	O
guaranteed	O
to	O
enable	O
us	O
to	O
calculate	O
any	O
marginal	B
p	O
(	O
xi	O
)	O
using	O
a	O
number	O
of	O
summations	O
which	O
scales	O
linearly	O
with	O
the	O
number	O
of	O
variables	O
in	O
the	O
graph	B
.	O
finding	O
conditional	B
marginals	O
for	O
a	O
chain	B
consider	O
the	O
following	O
inference	B
problem	O
,	O
ﬁg	O
(	O
5.1	O
)	O
:	O
given	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
)	O
=	O
p	O
(	O
a|b	O
)	O
p	O
(	O
b|c	O
)	O
p	O
(	O
c|d	O
)	O
p	O
(	O
d	O
)	O
,	O
ﬁnd	O
p	O
(	O
d|a	O
)	O
.	O
this	O
can	O
be	O
computed	O
using	O
(	O
cid:88	O
)	O
b	O
,	O
c	O
(	O
cid:88	O
)	O
b	O
,	O
c	O
p	O
(	O
d|a	O
)	O
∝	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
)	O
∝	O
p	O
(	O
a|b	O
)	O
p	O
(	O
b|c	O
)	O
p	O
(	O
c|d	O
)	O
p	O
(	O
d	O
)	O
∝	O
(	O
5.1.7	O
)	O
p	O
(	O
c|d	O
)	O
p	O
(	O
d	O
)	O
≡	O
γc	O
(	O
d	O
)	O
(	O
5.1.8	O
)	O
(	O
cid:88	O
)	O
c	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
b	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
p	O
(	O
a|b	O
)	O
p	O
(	O
b|c	O
)	O
γb	O
(	O
c	O
)	O
the	O
fact	O
that	O
(	O
cid:80	O
)	O
the	O
missing	B
proportionality	O
constant	O
is	O
found	O
by	O
repeating	O
the	O
computation	O
for	O
all	O
states	O
of	O
variable	B
d.	O
since	O
we	O
know	O
that	O
p	O
(	O
d|a	O
)	O
=	O
kγc	O
(	O
d	O
)	O
,	O
where	O
γc	O
(	O
d	O
)	O
is	O
the	O
unnormalised	O
result	O
of	O
the	O
summation	O
,	O
we	O
can	O
use	O
d	O
p	O
(	O
d|a	O
)	O
=	O
1	O
to	O
infer	O
that	O
k	O
=	O
1/	O
(	O
cid:80	O
)	O
d	O
γc	O
(	O
d	O
)	O
.	O
64	O
draft	O
march	O
9	O
,	O
2010	O
marginal	B
inference	O
in	O
this	O
example	O
,	O
the	O
potential	B
γb	O
(	O
c	O
)	O
is	O
not	O
a	O
distribution	B
in	O
c	O
,	O
nor	O
is	O
γc	O
(	O
d	O
)	O
.	O
in	O
general	O
,	O
one	O
may	O
view	O
variable	B
elimination	I
as	O
the	O
passing	B
of	O
messages	O
in	O
the	O
form	O
of	O
potentials	O
from	O
nodes	O
to	O
their	O
neighbours	O
.	O
for	O
belief	B
networks	I
,	O
variable	B
elimination	I
passes	O
messages	O
that	O
are	O
distributions	O
when	O
following	O
the	O
direction	O
of	O
the	O
edge	O
,	O
and	O
non-normalised	O
potentials	O
when	O
passing	B
messages	O
against	O
the	O
direction	O
of	O
the	O
edge	O
.	O
remark	O
5.	O
variable	B
elimination	I
in	O
trees	O
as	O
matrix	O
multiplication	O
variable	B
elimination	I
is	O
related	O
to	O
the	O
associativity	O
of	O
matrix	B
multiplication	O
.	O
for	O
equation	B
(	O
5.1.2	O
)	O
above	O
,	O
we	O
can	O
deﬁne	O
matrices	O
[	O
mab	O
]	O
i	O
,	O
j	O
=	O
p	O
(	O
a	O
=	O
i|b	O
=	O
j	O
)	O
,	O
[	O
mcd	O
]	O
i	O
,	O
j	O
=	O
p	O
(	O
c	O
=	O
i|d	O
=	O
j	O
)	O
,	O
[	O
mbc	O
]	O
i	O
,	O
j	O
=	O
p	O
(	O
b	O
=	O
i|c	O
=	O
j	O
)	O
,	O
[	O
md	O
]	O
i	O
=	O
p	O
(	O
d	O
=	O
i	O
)	O
,	O
[	O
ma	O
]	O
i	O
=	O
p	O
(	O
a	O
=	O
i	O
)	O
then	O
the	O
marginal	B
ma	O
can	O
be	O
written	O
ma	O
=	O
mabmbcmcdmd	O
=	O
mab	O
(	O
mbc	O
(	O
mcdmd	O
)	O
)	O
(	O
5.1.9	O
)	O
(	O
5.1.10	O
)	O
since	O
matrix	B
multiplication	O
is	O
associative	O
.	O
this	O
matrix	B
formulation	O
of	O
calculating	O
marginals	O
is	O
called	O
the	O
transfer	B
matrix	I
method	O
,	O
and	O
is	O
particularly	O
popular	O
in	O
the	O
physics	O
literature	O
[	O
26	O
]	O
.	O
example	O
19	O
(	O
where	O
will	O
the	O
ﬂy	O
be	O
?	O
)	O
.	O
you	O
live	O
in	O
a	O
house	O
with	O
three	O
rooms	O
,	O
labelled	B
1	O
,	O
2	O
,	O
3.	O
there	O
is	O
a	O
door	O
between	O
rooms	O
1	O
and	O
2	O
and	O
another	O
between	O
rooms	O
2	O
and	O
3.	O
one	O
can	O
not	O
directly	O
pass	O
between	O
rooms	O
1	O
and	O
3	O
in	O
one	O
time-step	O
.	O
an	O
annoying	O
ﬂy	O
is	O
buzzing	O
from	O
one	O
room	O
to	O
another	O
and	O
there	O
is	O
some	O
smelly	O
cheese	O
in	O
room	O
1	O
which	O
seems	O
to	O
attract	O
the	O
ﬂy	O
more	O
.	O
using	O
xt	O
for	O
which	O
room	O
the	O
ﬂy	O
is	O
in	O
at	O
time	O
t	O
,	O
with	O
dom	O
(	O
xt	O
)	O
=	O
{	O
1	O
,	O
2	O
,	O
3	O
}	O
,	O
the	O
movement	O
of	O
the	O
ﬂy	O
can	O
be	O
described	O
by	O
a	O
transition	O
p	O
(	O
xt+1	O
=	O
i|xt	O
=	O
j	O
)	O
=	O
mij	O
where	O
m	O
is	O
a	O
transition	B
matrix	I
	O
0.7	O
0.5	O
0	O
0.3	O
0.3	O
0.5	O
0	O
0.2	O
0.5	O
	O
m	O
=	O
(	O
cid:80	O
)	O
3	O
t−1	O
(	O
cid:89	O
)	O
t=1	O
the	O
transition	B
matrix	I
is	O
stochastic	O
in	O
the	O
sense	O
that	O
,	O
as	O
required	O
of	O
a	O
conditional	B
probability	I
distribution	O
i=1	O
mij	O
=	O
1.	O
given	O
that	O
the	O
ﬂy	O
is	O
in	O
room	O
1	O
at	O
time	O
1	O
,	O
what	O
is	O
the	O
probability	B
of	O
room	O
occupancy	O
at	O
time	O
t	O
=	O
5	O
?	O
assume	O
a	O
markov	O
chain	B
which	O
is	O
deﬁned	O
by	O
the	O
joint	B
distribution	O
(	O
5.1.11	O
)	O
(	O
5.1.12	O
)	O
(	O
5.1.13	O
)	O
(	O
5.1.14	O
)	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xt	O
)	O
=	O
p	O
(	O
x1	O
)	O
p	O
(	O
xt+1|xt	O
)	O
we	O
are	O
asked	O
to	O
compute	O
p	O
(	O
x5|x1	O
=	O
1	O
)	O
which	O
is	O
given	O
by	O
p	O
(	O
x5|x4	O
)	O
p	O
(	O
x4|x3	O
)	O
p	O
(	O
x3|x2	O
)	O
p	O
(	O
x2|x1	O
=	O
1	O
)	O
(	O
cid:88	O
)	O
x4	O
,	O
x3	O
,	O
x2	O
since	O
the	O
graph	B
of	O
the	O
distribution	B
is	O
a	O
markov	O
chain	B
,	O
we	O
can	O
easily	O
distribute	O
the	O
summation	O
over	O
the	O
terms	O
.	O
this	O
is	O
most	O
easily	O
done	O
using	O
the	O
transfer	B
matrix	I
method	O
,	O
giving	O
p	O
(	O
x5	O
=	O
i|x1	O
=	O
1	O
)	O
=	O
[	O
m4v	O
]	O
i	O
draft	O
march	O
9	O
,	O
2010	O
(	O
5.1.15	O
)	O
65	O
where	O
v	O
is	O
a	O
vector	O
with	O
components	O
(	O
1	O
,	O
0	O
,	O
0	O
)	O
t	O
,	O
reﬂecting	O
the	O
evidence	B
that	O
at	O
time	O
1	O
the	O
ﬂy	O
is	O
in	O
room	O
1.	O
computing	O
this	O
we	O
have	O
(	O
to	O
4	O
decimal	O
places	O
of	O
accuracy	O
)	O
marginal	B
inference	O
(	O
5.1.16	O
)	O
	O
0.5746	O
0.3180	O
0.1074	O
	O
m4v	O
=	O
p	O
(	O
xt+1	O
)	O
=	O
(	O
cid:88	O
)	O
xt	O
similarly	O
,	O
after	O
5	O
time-steps	O
,	O
the	O
occupancy	O
probabilities	O
are	O
(	O
0.5612	O
,	O
0.3215	O
,	O
0.1173	O
)	O
.	O
the	O
room	O
occupancy	O
probability	B
is	O
converging	O
to	O
a	O
particular	O
distribution	B
–	O
the	O
stationary	B
distribution	I
of	O
the	O
markov	O
chain	B
.	O
one	O
might	O
ask	O
where	O
the	O
ﬂy	O
is	O
after	O
an	O
inﬁnite	O
number	O
of	O
time-steps	O
.	O
that	O
is	O
,	O
we	O
are	O
interested	O
in	O
the	O
large	O
t	O
behaviour	O
of	O
p	O
(	O
xt+1|xt	O
)	O
p	O
(	O
xt	O
)	O
(	O
5.1.17	O
)	O
at	O
convergence	O
p	O
(	O
xt+1	O
)	O
=	O
p	O
(	O
xt	O
)	O
.	O
writing	O
p	O
for	O
the	O
vector	O
describing	O
the	O
stationary	B
distribution	I
,	O
this	O
means	O
p	O
=	O
mp	O
(	O
5.1.18	O
)	O
in	O
other	O
words	O
,	O
p	O
is	O
the	O
eigenvector	O
of	O
m	O
with	O
eigenvalue	O
1	O
[	O
122	O
]	O
.	O
computing	O
this	O
numerically	O
,	O
the	O
station-	O
ary	O
distribution	B
is	O
(	O
0.5435	O
,	O
0.3261	O
,	O
0.1304	O
)	O
.	O
note	O
that	O
software	O
packages	O
usually	O
return	O
eigenvectors	O
with	O
ete	O
=	O
1	O
–	O
the	O
unit	O
eigenvector	O
therefore	O
will	O
usually	O
require	O
normalisation	B
to	O
make	O
this	O
a	O
probability	B
.	O
5.1.2	O
the	O
sum-product	B
algorithm	I
on	O
factor	B
graphs	O
both	O
markov	O
and	O
belief	B
networks	I
can	O
be	O
represented	O
using	O
factor	B
graphs	O
.	O
for	O
this	O
reason	O
it	O
is	O
convenient	O
to	O
derive	O
a	O
marginal	B
inference	O
algorithm	B
for	O
the	O
fg	O
since	O
this	O
then	O
applies	O
to	O
both	O
markov	O
and	O
belief	B
networks	I
.	O
this	O
is	O
termed	O
the	O
sum-product	B
algorithm	I
since	O
to	O
compute	O
marginals	O
we	O
need	O
to	O
distribute	O
the	O
sum	O
over	O
variable	B
states	O
over	O
the	O
product	O
of	O
factors	O
.	O
in	O
older	O
texts	O
,	O
this	O
is	O
referred	O
to	O
as	O
belief	O
propagation	B
.	O
non-branching	O
graphs	O
:	O
variable	B
to	O
variable	B
messages	O
consider	O
the	O
distribution	B
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
)	O
=	O
f1	O
(	O
a	O
,	O
b	O
)	O
f2	O
(	O
b	O
,	O
c	O
)	O
f3	O
(	O
c	O
,	O
d	O
)	O
f4	O
(	O
d	O
)	O
(	O
5.1.19	O
)	O
which	O
has	O
the	O
factor	B
graph	I
represented	O
in	O
ﬁg	O
(	O
5.2	O
)	O
with	O
factors	O
as	O
deﬁned	O
by	O
the	O
f	O
above	O
.	O
to	O
compute	O
the	O
marginal	B
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
,	O
since	O
the	O
variable	B
d	O
only	O
occurs	O
locally	O
,	O
we	O
use	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
=	O
(	O
cid:88	O
)	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
)	O
=	O
(	O
cid:88	O
)	O
d	O
d	O
f1	O
(	O
a	O
,	O
b	O
)	O
f2	O
(	O
b	O
,	O
c	O
)	O
f3	O
(	O
c	O
,	O
d	O
)	O
f4	O
(	O
d	O
)	O
=	O
f1	O
(	O
a	O
,	O
b	O
)	O
f2	O
(	O
b	O
,	O
c	O
)	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
d	O
similarly	O
,	O
p	O
(	O
a	O
,	O
b	O
)	O
=	O
(	O
cid:88	O
)	O
c	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
=	O
f1	O
(	O
a	O
,	O
b	O
)	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
c	O
hence	O
µc→b	O
(	O
b	O
)	O
=	O
(	O
cid:88	O
)	O
c	O
f2	O
(	O
b	O
,	O
c	O
)	O
µd→c	O
(	O
c	O
)	O
f2	O
(	O
b	O
,	O
c	O
)	O
µd→c	O
(	O
c	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
µc→b	O
(	O
b	O
)	O
(	O
cid:125	O
)	O
it	O
is	O
clear	O
how	O
one	O
can	O
recurse	O
this	O
deﬁnition	O
of	O
messages	O
so	O
that	O
for	O
a	O
chain	B
of	O
length	O
n	O
variables	O
the	O
marginal	B
of	O
the	O
ﬁrst	O
node	O
can	O
be	O
computed	O
in	O
time	O
linear	B
in	O
n.	O
the	O
term	O
µc→b	O
(	O
b	O
)	O
can	O
be	O
interpreted	O
as	O
66	O
draft	O
march	O
9	O
,	O
2010	O
f3	O
(	O
c	O
,	O
d	O
)	O
f4	O
(	O
d	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
µd→c	O
(	O
c	O
)	O
(	O
cid:125	O
)	O
(	O
5.1.20	O
)	O
(	O
5.1.21	O
)	O
(	O
5.1.22	O
)	O
marginal	B
inference	O
f1	O
a	O
b	O
f2	O
f3	O
c	O
d	O
f4	O
figure	O
5.2	O
:	O
for	O
singly-connected	B
structures	O
with-	O
out	O
branches	O
,	O
simple	O
messages	O
from	O
one	O
variable	B
to	O
its	O
neighbour	B
may	O
be	O
deﬁned	O
to	O
form	O
an	O
eﬃcient	B
marginal	O
inference	B
scheme	O
.	O
carrying	O
marginal	B
information	O
from	O
the	O
graph	B
beyond	O
c.	O
for	O
any	O
singly-connected	B
structure	O
the	O
factors	O
at	O
the	O
edge	O
of	O
the	O
graph	B
can	O
be	O
replaced	O
with	O
messages	O
that	O
reﬂect	O
marginal	B
information	O
from	O
the	O
graph	B
beyond	O
that	O
factor	B
.	O
for	O
simple	O
linear	O
structures	O
with	O
no	O
branching	O
,	O
messages	O
from	O
variables	O
to	O
variables	O
are	O
suﬃcient	O
.	O
however	O
,	O
as	O
we	O
will	O
see	O
below	O
,	O
it	O
is	O
useful	O
in	O
more	O
general	O
structures	O
with	O
branching	O
to	O
consider	O
two	O
types	O
of	O
messages	O
,	O
namely	O
those	O
from	O
variables	O
to	O
factors	O
and	O
vice	O
versa	O
.	O
general	O
singly-connected	B
factor	O
graphs	O
the	O
slightly	O
more	O
complex	O
example	O
,	O
p	O
(	O
a|b	O
)	O
p	O
(	O
b|c	O
,	O
d	O
)	O
p	O
(	O
c	O
)	O
p	O
(	O
d	O
)	O
p	O
(	O
e|d	O
)	O
has	O
the	O
factor	B
graph	I
,	O
ﬁg	O
(	O
5.3	O
)	O
f1	O
(	O
a	O
,	O
b	O
)	O
f2	O
(	O
b	O
,	O
c	O
,	O
d	O
)	O
f3	O
(	O
c	O
)	O
f4	O
(	O
d	O
,	O
e	O
)	O
f5	O
(	O
d	O
)	O
(	O
5.1.23	O
)	O
(	O
5.1.24	O
)	O
if	O
the	O
marginal	B
p	O
(	O
a	O
,	O
b	O
)	O
is	O
to	O
be	O
represented	O
by	O
the	O
amputated	O
graph	B
with	O
messages	O
on	O
the	O
edges	O
,	O
then	O
f4	O
(	O
d	O
,	O
e	O
)	O
(	O
cid:125	O
)	O
(	O
5.1.25	O
)	O
e	O
in	O
this	O
case	O
it	O
is	O
natural	B
to	O
consider	O
messages	O
from	O
factors	O
to	O
variables	O
.	O
similarly	O
,	O
we	O
can	O
break	O
the	O
message	B
from	O
the	O
factor	B
f2	O
into	O
messages	O
arriving	O
from	O
the	O
two	O
branches	O
through	O
c	O
and	O
d	O
,	O
namely	O
p	O
(	O
a	O
,	O
b	O
)	O
=	O
f1	O
(	O
a	O
,	O
b	O
)	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
c	O
,	O
d	O
f2	O
(	O
b	O
,	O
c	O
,	O
d	O
)	O
f3	O
(	O
c	O
)	O
f5	O
(	O
d	O
)	O
(	O
cid:88	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
µf2→b	O
(	O
b	O
)	O
µf2→b	O
(	O
b	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
f2	O
(	O
b	O
,	O
c	O
,	O
d	O
)	O
f3	O
(	O
c	O
)	O
µc→f2	O
(	O
c	O
)	O
c	O
,	O
d	O
f5	O
(	O
d	O
)	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
e	O
µd→f2	O
(	O
d	O
)	O
f4	O
(	O
d	O
,	O
e	O
)	O
(	O
cid:125	O
)	O
similarly	O
,	O
we	O
can	O
interpret	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
µd→f2	O
(	O
d	O
)	O
=	O
f5	O
(	O
d	O
)	O
µf5→d	O
(	O
d	O
)	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
e	O
f4	O
(	O
d	O
,	O
e	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
µf4→d	O
(	O
d	O
)	O
(	O
5.1.26	O
)	O
(	O
5.1.27	O
)	O
to	O
complete	O
the	O
interpretation	O
we	O
identify	O
µc→f2	O
(	O
c	O
)	O
≡	O
µf3→c	O
(	O
c	O
)	O
.	O
in	O
a	O
non-branching	O
link	O
,	O
one	O
can	O
more	O
simply	O
use	O
a	O
variable	B
to	O
variable	B
message	O
.	O
a	O
f1	O
b	O
f2	O
f3	O
e	O
f4	O
c	O
d	O
f5	O
figure	O
5.3	O
:	O
for	O
a	O
branching	O
singly-connected	B
graph	O
,	O
it	O
is	O
useful	O
to	O
deﬁne	O
messages	O
from	O
both	O
factors	O
to	O
variables	O
,	O
and	O
variables	O
to	O
factors	O
.	O
draft	O
march	O
9	O
,	O
2010	O
67	O
for	O
consistency	O
of	O
interpretation	O
,	O
one	O
also	O
can	O
view	O
the	O
above	O
as	O
a	O
convenience	O
of	O
this	O
approach	B
is	O
that	O
the	O
messages	O
can	O
be	O
reused	O
to	O
evaluate	O
other	O
marginal	B
inferences	O
.	O
for	O
example	O
,	O
it	O
is	O
clear	O
that	O
p	O
(	O
b	O
)	O
is	O
given	O
by	O
marginal	B
inference	O
(	O
5.1.28	O
)	O
(	O
5.1.29	O
)	O
(	O
5.1.30	O
)	O
(	O
5.1.31	O
)	O
to	O
compute	O
the	O
marginal	B
p	O
(	O
a	O
)	O
,	O
we	O
then	O
have	O
f1	O
(	O
a	O
,	O
b	O
)	O
µf2→b	O
(	O
b	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
µf1→a	O
(	O
a	O
)	O
(	O
cid:125	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
f1	O
(	O
a	O
,	O
b	O
)	O
µf2→b	O
(	O
b	O
)	O
µb→f1	O
(	O
b	O
)	O
b	O
(	O
cid:124	O
)	O
f1	O
(	O
a	O
,	O
b	O
)	O
µf2→b	O
(	O
b	O
)	O
p	O
(	O
a	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
b	O
µf1→a	O
(	O
a	O
)	O
=	O
(	O
cid:88	O
)	O
a	O
p	O
(	O
b	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
µf2→c	O
(	O
c	O
)	O
=	O
(	O
cid:88	O
)	O
µf1→b	O
(	O
b	O
)	O
(	O
cid:125	O
)	O
if	O
we	O
additionally	O
desire	O
p	O
(	O
c	O
)	O
,	O
we	O
need	O
to	O
deﬁne	O
the	O
message	B
from	O
f2	O
to	O
c	O
,	O
f2	O
(	O
b	O
,	O
c	O
,	O
d	O
)	O
µb→f2	O
(	O
b	O
)	O
µd→f2	O
(	O
d	O
)	O
b	O
,	O
d	O
where	O
µb→f2	O
(	O
b	O
)	O
≡	O
µf1→b	O
(	O
b	O
)	O
.	O
this	O
demonstrates	O
the	O
reuse	O
of	O
already	O
computed	O
message	B
from	O
d	O
to	O
f2	O
to	O
compute	O
the	O
marginal	B
p	O
(	O
c	O
)	O
.	O
deﬁnition	O
36	O
(	O
message	B
schedule	O
)	O
.	O
a	O
message	B
schedule	O
is	O
a	O
speciﬁed	O
sequence	O
of	O
message	B
updates	O
.	O
a	O
valid	O
schedule	B
is	O
that	O
a	O
message	B
can	O
be	O
sent	O
from	O
a	O
node	B
only	O
when	O
that	O
node	B
has	O
received	O
all	O
requisite	O
messages	O
from	O
its	O
neighbours	O
.	O
in	O
general	O
,	O
there	O
is	O
more	O
than	O
one	O
valid	O
updating	O
schedule	B
.	O
sum-product	B
algorithm	I
the	O
sum-product	B
algorithm	I
is	O
described	O
below	O
in	O
which	O
messages	O
are	O
updated	O
as	O
a	O
function	B
of	O
incoming	O
messages	O
.	O
one	O
then	O
proceeds	O
by	O
computing	O
the	O
messages	O
in	O
a	O
schedule	B
that	O
allows	O
the	O
computation	O
of	O
a	O
new	O
message	B
based	O
on	O
previously	O
computed	O
messages	O
,	O
until	O
all	O
messages	O
from	O
all	O
factors	O
to	O
variables	O
and	O
vice-versa	O
have	O
been	O
computed	O
.	O
deﬁnition	O
37	O
(	O
sum-product	B
messages	O
on	O
factor	B
graphs	O
)	O
.	O
(	O
cid:81	O
)	O
(	O
cid:0	O
)	O
x	O
f	O
(	O
cid:1	O
)	O
,	O
provided	O
the	O
given	O
a	O
distribution	B
deﬁned	O
as	O
a	O
product	O
on	O
subsets	O
of	O
the	O
variables	O
,	O
p	O
(	O
x	O
)	O
=	O
1	O
factor	O
graph	B
is	O
singly-connected	B
we	O
can	O
carry	O
out	O
summation	O
over	O
the	O
variables	O
eﬃciently	O
.	O
f	O
φf	O
z	O
initialisation	O
messages	O
from	O
extremal	B
(	O
simplical	B
)	O
node	B
factors	O
are	O
initialised	O
to	O
the	O
factor	B
.	O
messages	O
from	O
extremal	B
(	O
simplical	B
)	O
variable	B
nodes	O
are	O
set	O
to	O
unity	O
.	O
variable	B
to	O
factor	B
message	O
µx→f	O
(	O
x	O
)	O
=	O
(	O
cid:89	O
)	O
g∈	O
{	O
ne	O
(	O
x	O
)	O
\f	O
}	O
µg→x	O
(	O
x	O
)	O
f2	O
f1	O
f3	O
µ	O
f	O
1→	O
x	O
(	O
x	O
)	O
µf2→x	O
(	O
x	O
)	O
x	O
(	O
x	O
)	O
→	O
f	O
3	O
µ	O
µx→f	O
(	O
x	O
)	O
f	O
x	O
68	O
draft	O
march	O
9	O
,	O
2010	O
marginal	B
inference	O
factor	B
to	O
variable	B
message	O
µf→x	O
(	O
x	O
)	O
=	O
(	O
cid:88	O
)	O
φf	O
(	O
x	O
f	O
)	O
(	O
cid:89	O
)	O
y∈x	O
f\x	O
y∈	O
{	O
ne	O
(	O
f	O
)	O
\x	O
}	O
µy→f	O
(	O
y	O
)	O
we	O
write	O
(	O
cid:80	O
)	O
y∈x	O
f\x	O
to	O
emphasise	O
that	O
we	O
sum	O
over	O
all	O
states	O
in	O
the	O
set	O
of	O
variables	O
x	O
f\x	O
.	O
marginal	B
(	O
cid:89	O
)	O
f∈ne	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
∝	O
µf→x	O
(	O
x	O
)	O
y1	O
µ	O
y	O
1→	O
f	O
(	O
y	O
1	O
)	O
y2	O
f1	O
f2	O
µy2→f	O
(	O
y2	O
)	O
(	O
y	O
→	O
f	O
y	O
3	O
µ	O
y3	O
µf1→x	O
(	O
x	O
)	O
(	O
x	O
)	O
µ	O
f	O
2	O
→	O
x	O
f	O
µf→x	O
(	O
x	O
)	O
x	O
3	O
)	O
x	O
µx←f3	O
(	O
x	O
)	O
f3	O
for	O
marginal	B
inference	O
,	O
the	O
important	O
information	O
is	O
the	O
relative	O
size	O
of	O
the	O
message	B
states	O
so	O
that	O
we	O
may	O
renormalise	O
messages	O
as	O
we	O
wish	O
.	O
since	O
the	O
marginal	B
will	O
be	O
proportional	O
to	O
the	O
incoming	O
messages	O
for	O
that	O
variable	B
,	O
the	O
normalisation	B
constant	I
is	O
trivially	O
obtained	O
using	O
the	O
fact	O
that	O
the	O
marginal	B
must	O
sum	O
to	O
1.	O
however	O
,	O
if	O
we	O
wish	O
to	O
also	O
compute	O
any	O
normalisation	B
constant	I
using	O
these	O
messages	O
,	O
we	O
can	O
not	O
normalise	O
the	O
messages	O
since	O
this	O
global	B
information	O
will	O
then	O
be	O
lost	O
.	O
to	O
resolve	O
this	O
one	O
may	O
work	O
with	O
log	O
messages	O
to	O
avoid	O
numerical	B
under/overﬂow	O
problems	O
.	O
the	O
sum-product	B
algorithm	I
is	O
able	O
to	O
perform	O
eﬃcient	B
marginal	O
inference	B
in	O
both	O
belief	O
and	O
markov	O
networks	O
,	O
since	O
both	O
are	O
expressible	O
as	O
factor	O
graphs	O
.	O
this	O
is	O
the	O
reason	O
for	O
the	O
preferred	O
use	O
of	O
the	O
factor	B
graph	I
since	O
it	O
requires	O
only	O
a	O
single	O
algorithm	B
and	O
is	O
agnostic	O
to	O
whether	O
or	O
not	O
the	O
graph	B
is	O
a	O
locally	O
or	O
globally	O
normalised	O
distribution	O
.	O
5.1.3	O
computing	O
the	O
marginal	B
likelihood	I
for	O
a	O
distribution	B
deﬁned	O
as	O
products	O
over	O
potentials	O
φf	O
(	O
cid:89	O
)	O
f	O
φf	O
(	O
cid:16	O
)	O
φf	O
x	O
f	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
x	O
f	O
(	O
cid:17	O
)	O
p	O
(	O
x	O
)	O
=	O
1	O
z	O
z	O
=	O
(	O
cid:88	O
)	O
(	O
cid:89	O
)	O
x	O
f	O
the	O
normalisation	B
is	O
given	O
by	O
(	O
cid:0	O
)	O
x	O
f	O
(	O
cid:1	O
)	O
(	O
5.1.32	O
)	O
(	O
5.1.33	O
)	O
to	O
compute	O
this	O
summation	O
eﬃciently	O
we	O
take	O
the	O
product	O
of	O
all	O
incoming	O
messages	O
to	O
an	O
arbitrarily	O
chosen	O
variable	B
x	O
and	O
then	O
sum	O
over	O
the	O
states	O
of	O
that	O
variable	B
:	O
z	O
=	O
(	O
cid:88	O
)	O
(	O
cid:89	O
)	O
x	O
f∈ne	O
(	O
x	O
)	O
µf→x	O
(	O
x	O
)	O
if	O
the	O
factor	B
graph	I
is	O
derived	O
from	O
setting	O
a	O
subset	O
of	O
variables	O
of	O
a	O
bn	O
in	O
evidential	O
states	O
p	O
(	O
x	O
,	O
v	O
)	O
=	O
p	O
(	O
x|v	O
)	O
p	O
(	O
v	O
)	O
then	O
the	O
summation	O
over	O
all	O
non-evidential	O
variables	O
will	O
yield	O
the	O
marginal	B
on	O
the	O
visible	B
(	O
evidential	O
)	O
variables	O
,	O
p	O
(	O
v	O
)	O
.	O
for	O
this	O
method	O
to	O
work	O
,	O
the	O
absolute	O
(	O
not	O
relative	O
)	O
values	O
of	O
the	O
messages	O
are	O
required	O
,	O
which	O
prohibits	O
renormalisation	O
at	O
each	O
stage	O
of	O
the	O
message	B
passing	I
procedure	O
.	O
however	O
,	O
without	O
normalisation	B
the	O
draft	O
march	O
9	O
,	O
2010	O
69	O
(	O
5.1.34	O
)	O
(	O
5.1.35	O
)	O
marginal	B
inference	O
f4	O
a	O
d	O
f1	O
f3	O
(	O
a	O
)	O
b	O
c	O
f2	O
a	O
f1	O
f5	O
(	O
b	O
)	O
b	O
c	O
f2	O
figure	O
5.4	O
:	O
(	O
a	O
)	O
factor	B
graph	I
with	O
a	O
loop	O
.	O
(	O
b	O
)	O
eliminating	O
the	O
variable	B
d	O
adds	O
an	O
edge	O
between	O
a	O
and	O
c	O
,	O
demonstrating	O
that	O
,	O
in	O
general	O
,	O
one	O
can	O
not	O
perform	O
marginal	B
inference	O
in	O
loopy	B
graphs	O
by	O
simply	O
passing	B
messages	O
along	O
existing	O
edges	O
in	O
the	O
original	O
graph	B
.	O
numerical	B
value	O
of	O
messages	O
can	O
become	O
very	O
small	O
,	O
particularly	O
for	O
large	O
graphs	O
,	O
and	O
numerical	B
precision	O
issues	O
can	O
occur	O
.	O
a	O
remedy	O
in	O
this	O
situation	O
is	O
to	O
work	O
with	O
log	O
messages	O
,	O
for	O
this	O
,	O
the	O
variable	B
to	O
factor	B
messages	O
become	O
simply	O
λ	O
=	O
log	O
µ	O
λg→x	O
(	O
x	O
)	O
µg→x	O
(	O
x	O
)	O
g∈	O
{	O
ne	O
(	O
x	O
)	O
\f	O
}	O
g∈	O
{	O
ne	O
(	O
x	O
)	O
\f	O
}	O
µx→f	O
(	O
x	O
)	O
=	O
(	O
cid:89	O
)	O
λx→f	O
(	O
x	O
)	O
=	O
(	O
cid:88	O
)	O
µf→x	O
(	O
x	O
)	O
=	O
(	O
cid:88	O
)	O
φf	O
(	O
x	O
f	O
)	O
(	O
cid:89	O
)	O
	O
(	O
cid:88	O
)	O
λf→x	O
(	O
x	O
)	O
=	O
log	O
φf	O
(	O
x	O
f	O
)	O
e	O
y∈x	O
f\x	O
naively	O
,	O
one	O
may	O
write	O
more	O
care	O
is	O
required	O
for	O
the	O
factors	O
to	O
variable	B
messages	O
,	O
which	O
are	O
deﬁned	O
by	O
y∈x	O
f\x	O
y∈	O
{	O
ne	O
(	O
f	O
)	O
\x	O
}	O
µy→f	O
(	O
y	O
)	O
	O
y∈	O
{	O
ne	O
(	O
f	O
)	O
\x	O
}	O
λy→f	O
(	O
y	O
)	O
(	O
cid:80	O
)	O
(	O
5.1.36	O
)	O
(	O
5.1.37	O
)	O
(	O
5.1.38	O
)	O
(	O
5.1.39	O
)	O
(	O
5.1.40	O
)	O
however	O
,	O
the	O
exponentiation	O
of	O
the	O
log	O
messages	O
will	O
cause	O
potential	B
numerical	O
precision	B
problems	O
.	O
a	O
solution	O
to	O
this	O
numerical	B
diﬃculty	O
is	O
obtained	O
by	O
ﬁnding	O
the	O
largest	O
value	B
of	O
the	O
incoming	O
log	O
messages	O
,	O
λ	O
∗	O
y→f	O
=	O
max	O
y∈	O
{	O
ne	O
(	O
f	O
)	O
\x	O
}	O
λy→f	O
(	O
y	O
)	O
then	O
λf→x	O
(	O
x	O
)	O
=	O
λ	O
	O
(	O
cid:88	O
)	O
y∈x	O
f\x	O
∗	O
y→f	O
+	O
log	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
φf	O
(	O
x	O
f	O
)	O
e	O
y∈	O
{	O
ne	O
(	O
f	O
)	O
\x	O
}	O
λy→f	O
(	O
y	O
)	O
−λ∗	O
y→f	O
	O
(	O
5.1.41	O
)	O
(	O
5.1.42	O
)	O
by	O
construction	B
the	O
terms	O
e	O
contributions	O
to	O
the	O
summation	O
are	O
computed	O
accurately	O
.	O
y∈	O
{	O
ne	O
(	O
f	O
)	O
\x	O
}	O
λy→f	O
(	O
y	O
)	O
−λ∗	O
y→f	O
will	O
be	O
≤	O
1.	O
this	O
ensures	O
that	O
the	O
dominant	O
numerical	B
log	O
marginals	O
are	O
readily	O
found	O
using	O
log	O
p	O
(	O
x	O
)	O
=	O
(	O
cid:88	O
)	O
f∈	O
{	O
ne	O
(	O
x	O
)	O
}	O
λf→x	O
(	O
x	O
)	O
(	O
5.1.43	O
)	O
70	O
draft	O
march	O
9	O
,	O
2010	O
other	O
forms	O
of	O
inference	B
5.1.4	O
the	O
problem	B
with	O
loops	O
loops	O
cause	O
a	O
problem	B
with	O
variable	B
elimination	I
(	O
or	O
message	B
passing	I
)	O
techniques	O
since	O
once	O
a	O
variable	B
is	O
eliminated	O
the	O
structure	B
of	O
the	O
‘	O
amputated	O
’	O
graph	B
in	O
general	O
changes	O
.	O
for	O
example	O
,	O
consider	O
the	O
fg	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
)	O
=	O
f1	O
(	O
a	O
,	O
b	O
)	O
f2	O
(	O
b	O
,	O
c	O
)	O
f3	O
(	O
c	O
,	O
d	O
)	O
f4	O
(	O
a	O
,	O
d	O
)	O
the	O
marginal	B
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
is	O
given	O
by	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
=	O
f1	O
(	O
a	O
,	O
b	O
)	O
f2	O
(	O
b	O
,	O
c	O
)	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
d	O
f3	O
(	O
c	O
,	O
d	O
)	O
f4	O
(	O
a	O
,	O
d	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
f5	O
(	O
a	O
,	O
c	O
)	O
(	O
cid:125	O
)	O
(	O
5.1.44	O
)	O
(	O
5.1.45	O
)	O
which	O
adds	O
a	O
link	O
ac	O
in	O
the	O
amputated	O
graph	B
,	O
see	O
ﬁg	O
(	O
5.4	O
)	O
.	O
this	O
means	O
that	O
one	O
can	O
not	O
account	O
for	O
information	O
from	O
variable	B
d	O
by	O
simply	O
updating	O
potentials	O
on	O
links	O
in	O
the	O
original	O
graph	B
–	O
one	O
needs	O
to	O
account	O
for	O
the	O
fact	O
that	O
the	O
structure	B
of	O
the	O
graph	B
changes	O
.	O
the	O
junction	B
tree	I
algorithm	O
,	O
chapter	O
(	O
6	O
)	O
is	O
a	O
widely	O
used	O
technique	O
to	O
deal	O
with	O
this	O
and	O
essentially	O
combines	O
variables	O
together	O
in	O
order	O
to	O
make	O
a	O
new	O
singly-connected	B
graph	O
for	O
which	O
the	O
graph	B
structure	O
remains	O
singly-connected	B
under	O
variable	B
elimination	I
.	O
5.2	O
other	O
forms	O
of	O
inference	B
5.2.1	O
max-product	B
a	O
common	O
interest	O
is	O
the	O
most	B
likely	I
state	I
of	O
distribution	B
.	O
that	O
is	O
argmax	O
x1	O
,	O
x2	O
,	O
...	O
,	O
xn	O
p	O
(	O
x1	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
(	O
5.2.1	O
)	O
to	O
compute	O
this	O
eﬃciently	O
we	O
exploit	O
any	O
factorisation	O
structure	B
of	O
the	O
distribution	B
,	O
analogous	O
to	O
the	O
sum-product	B
algorithm	I
.	O
that	O
is	O
,	O
we	O
aim	O
to	O
distribute	O
the	O
maximization	O
so	O
that	O
only	O
local	B
computations	O
are	O
required	O
.	O
to	O
develop	O
the	O
algorithm	B
,	O
consider	O
a	O
function	B
which	O
can	O
be	O
represented	O
as	O
an	O
undirected	B
chain	O
,	O
f	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
=	O
φ	O
(	O
x1	O
,	O
x2	O
)	O
φ	O
(	O
x2	O
,	O
x3	O
)	O
φ	O
(	O
x3	O
,	O
x4	O
)	O
(	O
5.2.2	O
)	O
for	O
which	O
we	O
wish	O
to	O
ﬁnd	O
the	O
joint	B
state	O
x∗	O
which	O
maximises	O
f.	O
firstly	O
,	O
we	O
calculate	O
the	O
maximum	O
value	O
of	O
f.	O
since	O
potentials	O
are	O
non-negative	O
,	O
we	O
may	O
write	O
max	O
x	O
f	O
(	O
x	O
)	O
=	O
max	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
φ	O
(	O
x1	O
,	O
x2	O
)	O
φ	O
(	O
x2	O
,	O
x3	O
)	O
φ	O
(	O
x3	O
,	O
x4	O
)	O
=	O
max	O
x1	O
,	O
x2	O
,	O
x3	O
φ	O
(	O
x1	O
,	O
x2	O
)	O
φ	O
(	O
x2	O
,	O
x3	O
)	O
max	O
x4	O
φ	O
(	O
x3	O
,	O
x4	O
)	O
(	O
cid:124	O
)	O
(	O
cid:124	O
)	O
(	O
cid:125	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
γ	O
(	O
x1	O
)	O
γ	O
(	O
x3	O
)	O
φ	O
(	O
x1	O
,	O
x2	O
)	O
γ	O
(	O
x2	O
)	O
(	O
cid:125	O
)	O
=	O
max	O
x1	O
,	O
x2	O
φ	O
(	O
x1	O
,	O
x2	O
)	O
max	O
x3	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
γ	O
(	O
x2	O
)	O
(	O
cid:125	O
)	O
φ	O
(	O
x2	O
,	O
x3	O
)	O
γ	O
(	O
x3	O
)	O
=	O
max	O
x1	O
,	O
x2	O
φ	O
(	O
x1	O
,	O
x2	O
)	O
γ	O
(	O
x2	O
)	O
=	O
max	O
x1	O
max	O
x2	O
the	O
ﬁnal	O
equation	B
corresponds	O
to	O
solving	B
a	O
single	O
variable	B
optimisation	O
and	O
determines	O
both	O
the	O
optimal	O
value	B
of	O
the	O
function	B
f	O
and	O
also	O
the	O
optimal	O
state	O
x∗	O
γ	O
(	O
x1	O
)	O
.	O
given	O
x∗	O
1	O
,	O
the	O
optimal	O
x2	O
is	O
given	O
by	O
x∗	O
2	O
,	O
x3	O
)	O
γ	O
(	O
x3	O
)	O
,	O
and	O
so	O
on	O
.	O
this	O
procedure	O
is	O
called	O
backtracking	B
.	O
note	O
that	O
we	O
could	O
have	O
equally	O
started	O
at	O
the	O
other	O
end	O
of	O
the	O
chain	B
by	O
deﬁning	O
messages	O
γ	O
that	O
pass	O
information	O
from	O
xi	O
to	O
xi+1	O
.	O
1	O
,	O
x2	O
)	O
γ	O
(	O
x2	O
)	O
,	O
and	O
similarly	O
x∗	O
1	O
=	O
argmax	O
φ	O
(	O
x∗	O
2	O
=	O
argmax	O
3	O
=	O
argmax	O
φ	O
(	O
x∗	O
x1	O
x3	O
x2	O
the	O
chain	B
structure	I
of	O
the	O
function	B
ensures	O
that	O
the	O
maximal	O
value	B
(	O
and	O
its	O
state	O
)	O
can	O
be	O
computed	O
in	O
time	O
which	O
scales	O
linearly	O
with	O
the	O
number	O
of	O
factors	O
in	O
the	O
function	B
.	O
there	O
is	O
no	O
requirement	O
here	O
that	O
the	O
function	B
f	O
corresponds	O
to	O
a	O
probability	B
distribution	O
(	O
though	O
the	O
factors	O
must	O
be	O
non-negative	O
)	O
.	O
draft	O
march	O
9	O
,	O
2010	O
71	O
other	O
forms	O
of	O
inference	B
(	O
5.2.3	O
)	O
example	O
20.	O
consider	O
a	O
distribution	B
deﬁned	O
over	O
binary	O
variables	O
:	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
≡	O
p	O
(	O
a|b	O
)	O
p	O
(	O
b|c	O
)	O
p	O
(	O
c	O
)	O
with	O
p	O
(	O
a	O
=	O
tr|b	O
=	O
tr	O
)	O
=	O
0.3	O
,	O
p	O
(	O
a	O
=	O
tr|b	O
=	O
fa	O
)	O
=	O
0.2	O
,	O
p	O
(	O
b	O
=	O
tr|c	O
=	O
tr	O
)	O
=	O
0.75	O
p	O
(	O
b	O
=	O
tr|c	O
=	O
fa	O
)	O
=	O
0.1	O
,	O
p	O
(	O
c	O
=	O
tr	O
)	O
=	O
0.4	O
what	O
is	O
the	O
most	O
likely	O
joint	O
conﬁguration	O
,	O
argmax	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
?	O
a	O
,	O
b	O
,	O
c	O
naively	O
,	O
we	O
could	O
evaluate	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
over	O
all	O
the	O
8	O
joint	B
states	O
of	O
a	O
,	O
b	O
,	O
c	O
and	O
select	O
that	O
states	O
with	O
highest	O
probability	B
.	O
a	O
message	B
passing	I
approach	O
is	O
to	O
deﬁne	O
γ	O
(	O
b	O
)	O
≡	O
max	O
c	O
p	O
(	O
b|c	O
)	O
p	O
(	O
c	O
)	O
for	O
the	O
state	O
b	O
=	O
tr	O
,	O
p	O
(	O
b	O
=	O
tr|c	O
=	O
tr	O
)	O
p	O
(	O
c	O
=	O
tr	O
)	O
=	O
0.75	O
×	O
0.4	O
,	O
p	O
(	O
b	O
=	O
tr|c	O
=	O
fa	O
)	O
p	O
(	O
c	O
=	O
fa	O
)	O
=	O
0.1	O
×	O
0.6	O
hence	O
,	O
γ	O
(	O
b	O
=	O
tr	O
)	O
=	O
0.75	O
×	O
0.4	O
=	O
0.3.	O
similarly	O
,	O
for	O
b	O
=	O
fa	O
,	O
p	O
(	O
b	O
=	O
fa|c	O
=	O
tr	O
)	O
p	O
(	O
c	O
=	O
tr	O
)	O
=	O
0.25	O
×	O
0.4	O
p	O
(	O
b	O
=	O
fa|c	O
=	O
fa	O
)	O
p	O
(	O
c	O
=	O
fa	O
)	O
=	O
0.9	O
×	O
0.6	O
hence	O
,	O
γ	O
(	O
b	O
=	O
fa	O
)	O
=	O
0.9	O
×	O
0.6	O
=	O
0.54.	O
we	O
now	O
consider	O
γ	O
(	O
a	O
)	O
≡	O
max	O
b	O
p	O
(	O
a|b	O
)	O
γ	O
(	O
b	O
)	O
for	O
a	O
=	O
tr	O
,	O
the	O
state	O
b	O
=	O
tr	O
has	O
value	B
p	O
(	O
a	O
=	O
tr|b	O
=	O
tr	O
)	O
γ	O
(	O
b	O
=	O
tr	O
)	O
=	O
0.3	O
×	O
0.3	O
=	O
0.09	O
and	O
state	O
b	O
=	O
fa	O
has	O
value	B
p	O
(	O
a	O
=	O
tr|b	O
=	O
fa	O
)	O
γ	O
(	O
b	O
=	O
fa	O
)	O
=	O
0.2	O
×	O
0.54	O
=	O
0.108	O
hence	O
γ	O
(	O
a	O
=	O
tr	O
)	O
=	O
0.108.	O
similarly	O
,	O
for	O
a	O
=	O
fa	O
,	O
the	O
state	O
b	O
=	O
tr	O
has	O
value	B
p	O
(	O
a	O
=	O
fa|b	O
=	O
tr	O
)	O
γ	O
(	O
b	O
=	O
tr	O
)	O
=	O
0.7	O
×	O
0.3	O
=	O
0.21	O
and	O
state	O
b	O
=	O
fa	O
has	O
value	B
p	O
(	O
a	O
=	O
fa|b	O
=	O
fa	O
)	O
γ	O
(	O
b	O
=	O
fa	O
)	O
=	O
0.8	O
×	O
0.54	O
=	O
0.432	O
giving	O
γ	O
(	O
a	O
=	O
fa	O
)	O
=	O
0.432.	O
now	O
we	O
can	O
compute	O
the	O
optimal	O
state	O
∗	O
=	O
argmax	O
a	O
a	O
γ	O
(	O
a	O
)	O
=	O
fa	O
given	O
this	O
optimal	O
state	O
,	O
we	O
can	O
backtrack	O
,	O
giving	O
∗	O
=	O
argmax	O
b	O
b	O
p	O
(	O
a	O
=	O
fa|b	O
)	O
γ	O
(	O
b	O
)	O
=	O
fa	O
,	O
c	O
∗	O
=	O
argmax	O
c	O
p	O
(	O
b	O
=	O
fa|c	O
)	O
p	O
(	O
c	O
)	O
=	O
fa	O
(	O
5.2.4	O
)	O
(	O
5.2.5	O
)	O
(	O
5.2.6	O
)	O
(	O
5.2.7	O
)	O
(	O
5.2.8	O
)	O
(	O
5.2.9	O
)	O
(	O
5.2.10	O
)	O
(	O
5.2.11	O
)	O
(	O
5.2.12	O
)	O
(	O
5.2.13	O
)	O
note	O
that	O
in	O
the	O
backtracking	B
process	O
,	O
we	O
already	O
have	O
all	O
the	O
information	O
required	O
from	O
the	O
computation	O
of	O
the	O
messages	O
γ	O
.	O
72	O
draft	O
march	O
9	O
,	O
2010	O
other	O
forms	O
of	O
inference	B
using	O
a	O
factor	B
graph	I
one	O
can	O
also	O
use	O
the	O
factor	B
graph	I
to	O
compute	O
the	O
joint	B
most	O
probable	O
state	O
.	O
provided	O
that	O
a	O
full	O
schedule	B
of	O
message	B
passing	I
has	O
occurred	O
,	O
the	O
product	O
of	O
messages	O
into	O
a	O
variable	B
equals	O
the	O
maximum	O
value	O
of	O
the	O
joint	B
function	O
with	O
respect	O
to	O
all	O
other	O
variables	O
.	O
one	O
can	O
then	O
simply	O
read	O
oﬀ	O
the	O
most	B
probable	I
state	I
by	O
maximising	O
this	O
local	B
potential	O
.	O
one	O
then	O
proceeds	O
in	O
computing	O
the	O
messages	O
in	O
a	O
schedule	B
that	O
allows	O
the	O
computation	O
of	O
a	O
new	O
message	B
based	O
on	O
previously	O
computed	O
messages	O
,	O
until	O
all	O
messages	O
from	O
all	O
factors	O
to	O
variables	O
and	O
vice-versa	O
have	O
been	O
computed	O
.	O
the	O
message	B
updates	O
are	O
given	O
below	O
.	O
deﬁnition	O
38	O
(	O
max-product	B
messages	O
on	O
factor	B
graphs	O
)	O
.	O
(	O
cid:81	O
)	O
(	O
cid:0	O
)	O
x	O
f	O
(	O
cid:1	O
)	O
,	O
provided	O
the	O
given	O
a	O
distribution	B
deﬁned	O
as	O
a	O
product	O
on	O
subsets	O
of	O
the	O
variables	O
,	O
p	O
(	O
x	O
)	O
=	O
1	O
factor	O
graph	B
is	O
singly-connected	B
we	O
can	O
carry	O
out	O
maximisation	B
over	O
the	O
variables	O
eﬃciently	O
.	O
f	O
φf	O
z	O
initialisation	O
messages	O
from	O
extremal	B
(	O
simplical	B
)	O
node	B
factors	O
are	O
initialised	O
to	O
the	O
factor	B
.	O
messages	O
from	O
extremal	B
(	O
simplical	B
)	O
variable	B
nodes	O
are	O
set	O
to	O
unity	O
.	O
variable	B
to	O
factor	B
message	O
µx→f	O
(	O
x	O
)	O
=	O
(	O
cid:89	O
)	O
g∈	O
{	O
ne	O
(	O
x	O
)	O
\f	O
}	O
µg→x	O
(	O
x	O
)	O
f1	O
µ	O
f	O
1→	O
x	O
(	O
x	O
)	O
f2	O
µf2→x	O
(	O
x	O
)	O
µx→f	O
(	O
x	O
)	O
f	O
x	O
x	O
(	O
x	O
)	O
→	O
f	O
3	O
µ	O
f3	O
y1	O
factor	B
to	O
variable	B
message	O
µf→x	O
(	O
x	O
)	O
=	O
max	O
y∈x	O
f\x	O
φf	O
(	O
x	O
f	O
)	O
(	O
cid:89	O
)	O
y∈	O
{	O
ne	O
(	O
f	O
)	O
\x	O
}	O
maximal	O
state	O
x∗	O
=	O
argmax	O
x	O
(	O
cid:89	O
)	O
f∈ne	O
(	O
x	O
)	O
µf→x	O
(	O
x	O
)	O
µy→f	O
(	O
y	O
)	O
y2	O
f1	O
f2	O
µy2→f	O
(	O
y2	O
)	O
(	O
y	O
→	O
f	O
y	O
3	O
µ	O
y3	O
µf1→x	O
(	O
x	O
)	O
(	O
x	O
)	O
µ	O
f	O
2	O
→	O
x	O
µ	O
y	O
1→	O
f	O
(	O
y	O
1	O
)	O
f	O
µf→x	O
(	O
x	O
)	O
x	O
3	O
)	O
x	O
µx←f3	O
(	O
x	O
)	O
f3	O
in	O
earlier	O
literature	O
,	O
this	O
algorithm	B
is	O
called	O
belief	B
revision	I
.	O
5.2.2	O
finding	O
the	O
n	O
most	O
probable	O
states	O
it	O
is	O
often	O
of	O
interest	O
to	O
calculate	O
not	O
just	O
the	O
most	O
likely	O
joint	O
state	O
,	O
but	O
the	O
n	O
most	O
probable	O
states	O
,	O
particularly	O
in	O
cases	O
where	O
the	O
optimal	O
state	O
is	O
only	O
slightly	O
more	O
probable	O
than	O
other	O
states	O
.	O
this	O
is	O
an	O
interesting	O
problem	B
in	O
itself	O
and	O
can	O
be	O
tackled	O
with	O
a	O
variety	O
of	O
methods	O
.	O
a	O
general	O
technique	O
is	O
given	O
by	O
nilson	O
[	O
210	O
]	O
which	O
is	O
based	O
on	O
the	O
junction	B
tree	I
formalism	O
,	O
chapter	O
(	O
6	O
)	O
,	O
and	O
the	O
construction	B
of	O
candidate	O
lists	O
,	O
see	O
for	O
example	O
[	O
69	O
]	O
.	O
draft	O
march	O
9	O
,	O
2010	O
73	O
other	O
forms	O
of	O
inference	B
4	O
6	O
2	O
3	O
5	O
1	O
8	O
7	O
9	O
figure	O
5.5	O
:	O
state	O
transition	O
diagram	O
(	O
weights	O
not	O
shown	O
)	O
.	O
the	O
shortest	O
(	O
unweighted	O
)	O
path	B
from	O
state	O
1	O
to	O
state	O
7	O
is	O
1	O
−	O
2	O
−	O
7.	O
considered	O
as	O
a	O
markov	O
chain	B
(	O
random	O
walk	O
)	O
,	O
the	O
most	B
probable	I
path	I
from	O
state	O
1	O
to	O
state	O
7	O
is	O
1−	O
8−	O
9−	O
7.	O
the	O
latter	O
path	B
is	O
longer	O
but	O
more	O
probable	O
since	O
for	O
the	O
path	B
1	O
−	O
2	O
−	O
7	O
,	O
the	O
probability	B
of	O
exiting	O
from	O
state	O
2	O
into	O
state	O
7	O
is	O
1/5	O
(	O
assuming	O
each	O
transition	O
is	O
equally	O
likely	O
)	O
.	O
see	O
demomostprobablepath.m	O
for	O
singly-connected	B
structures	O
,	O
several	O
approaches	O
have	O
been	O
developed	O
.	O
for	O
the	O
hidden	B
markov	O
model	B
,	O
section	O
(	O
23.2	O
)	O
a	O
simple	O
algorithm	O
is	O
the	O
n-viterbi	O
approach	B
which	O
stores	O
the	O
n-most	O
probable	O
messages	O
at	O
each	O
stage	O
of	O
the	O
propagation	B
,	O
see	O
for	O
example	O
[	O
256	O
]	O
.	O
a	O
special	O
case	O
of	O
nilsson	O
’	O
s	O
approach	B
is	O
available	O
for	O
hidden	B
markov	O
models	O
[	O
211	O
]	O
which	O
is	O
the	O
particularly	O
eﬃcient	B
for	O
large	O
state	O
spaces	O
.	O
for	O
more	O
general	O
singly-connected	B
graphs	O
one	O
can	O
extend	O
the	O
max-product	B
algorithm	O
to	O
an	O
n-max-product	O
algorithm	B
by	O
retaining	O
at	O
each	O
stage	O
the	O
n	O
most	O
probable	O
messages	O
,	O
see	O
below	O
.	O
these	O
techniques	O
require	O
n	O
to	O
be	O
speciﬁed	O
a-priori	O
compared	O
to	O
anytime	O
alternatives	O
,	O
[	O
298	O
]	O
.	O
an	O
alternative	O
approach	B
for	O
singly-	O
connected	B
networks	O
was	O
developed	O
in	O
[	O
269	O
]	O
.	O
of	O
particular	O
interest	O
is	O
the	O
application	O
of	O
the	O
singly-connected	B
algorithms	O
as	O
an	O
approximation	B
when	O
for	O
example	O
nilsson	O
’	O
s	O
approach	B
on	O
a	O
multiply-connected	B
graph	O
is	O
intractable	O
[	O
298	O
]	O
.	O
n-max-product	O
the	O
algorithm	B
for	O
n-max-product	O
is	O
a	O
straightforward	O
modiﬁcation	O
of	O
the	O
standard	O
algorithms	O
.	O
compu-	O
tationally	O
,	O
a	O
straightforward	O
way	O
to	O
accomplish	O
this	O
is	O
to	O
introduce	O
an	O
additional	O
variable	B
for	O
each	O
message	B
that	O
is	O
used	O
to	O
index	O
the	O
most	O
likely	O
messages	O
.	O
for	O
example	O
,	O
consider	O
the	O
distribution	B
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
)	O
=	O
φ	O
(	O
a	O
,	O
b	O
)	O
φ	O
(	O
b	O
,	O
c	O
)	O
φ	O
(	O
b	O
,	O
d	O
)	O
for	O
which	O
we	O
wish	O
to	O
ﬁnd	O
the	O
two	O
most	O
probable	O
values	O
.	O
using	O
the	O
notation	O
imax	O
x	O
f	O
(	O
x	O
)	O
for	O
the	O
ith	O
highest	O
value	B
of	O
f	O
(	O
x	O
)	O
,	O
the	O
maximisation	B
over	O
d	O
can	O
be	O
expressed	O
using	O
the	O
message	B
γd	O
(	O
b	O
,	O
1	O
)	O
=	O
1max	O
d	O
φ	O
(	O
b	O
,	O
d	O
)	O
,	O
γd	O
(	O
b	O
,	O
2	O
)	O
=	O
2max	O
d	O
φ	O
(	O
b	O
,	O
d	O
)	O
(	O
5.2.14	O
)	O
(	O
5.2.15	O
)	O
(	O
5.2.16	O
)	O
using	O
a	O
similar	O
message	B
for	O
the	O
maximisation	B
over	O
c	O
,	O
the	O
2	O
most	O
likely	O
states	O
of	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
)	O
can	O
be	O
computed	O
using	O
1:2max	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
=	O
1:2max	O
a	O
,	O
b	O
,	O
mb	O
,	O
md	O
φ	O
(	O
a	O
,	O
b	O
)	O
γd	O
(	O
b	O
,	O
md	O
)	O
γc	O
(	O
b	O
,	O
mc	O
)	O
(	O
5.2.17	O
)	O
where	O
mc	O
and	O
md	O
index	O
the	O
highest	O
values	O
.	O
at	O
the	O
ﬁnal	O
stage	O
we	O
now	O
have	O
a	O
table	O
with	O
dim	O
a	O
×	O
dim	O
b	O
×	O
4	O
entries	O
,	O
from	O
which	O
we	O
compute	O
the	O
highest	O
two	O
states	O
.	O
the	O
generalisation	B
of	O
this	O
to	O
the	O
factor	B
graph	I
formalism	O
is	O
straightforward	O
and	O
contained	O
in	O
maxnprodfg.m	O
.	O
essentially	O
the	O
only	O
modiﬁcation	O
required	O
is	O
to	O
deﬁne	O
extended	O
messages	O
which	O
contain	O
the	O
n-most	O
likely	O
messages	O
computed	O
at	O
each	O
stage	O
.	O
at	O
a	O
junction	O
of	O
the	O
factor	B
graph	I
,	O
all	O
the	O
messages	O
from	O
the	O
neighbours	O
,	O
along	O
with	O
their	O
n-most	O
probable	O
tables	O
are	O
multiplied	O
together	O
into	O
a	O
large	O
table	O
.	O
for	O
a	O
factor	B
to	O
variable	B
message	O
the	O
n-most	O
probable	O
messages	O
are	O
retained	O
,	O
see	O
maxnprodfg.m	O
.	O
the	O
n-most	O
probable	O
states	O
for	O
each	O
variable	B
can	O
then	O
be	O
read	O
oﬀ	O
by	O
ﬁnding	O
the	O
variable	B
state	O
that	O
maximises	O
the	O
product	O
of	O
incoming	O
extended	O
messages	O
.	O
74	O
draft	O
march	O
9	O
,	O
2010	O
other	O
forms	O
of	O
inference	B
5.2.3	O
most	B
probable	I
path	I
and	O
shortest	B
path	I
what	O
is	O
the	O
most	O
likely	O
path	O
from	O
state	O
a	O
to	O
state	O
b	O
for	O
an	O
n	O
state	O
markov	O
chain	B
?	O
note	O
that	O
this	O
is	O
not	O
necessarily	O
the	O
same	O
as	O
the	O
shortest	B
path	I
,	O
as	O
explained	O
in	O
ﬁg	O
(	O
5.5	O
)	O
.	O
if	O
assume	O
that	O
a	O
length	O
t	O
path	B
exists	O
,	O
this	O
has	O
probability	B
p	O
(	O
s2|s1	O
=	O
a	O
)	O
p	O
(	O
s3|s2	O
)	O
.	O
.	O
.	O
p	O
(	O
st	O
=	O
b|st−1	O
)	O
(	O
5.2.18	O
)	O
finding	O
the	O
most	B
probable	I
path	I
can	O
then	O
be	O
readily	O
solved	O
using	O
the	O
max-product	B
(	O
or	O
max-sum	B
algorithm	O
for	O
the	O
log-transitions	O
)	O
on	O
a	O
simple	O
serial	O
factor	B
graph	I
.	O
to	O
deal	O
with	O
the	O
issue	O
that	O
we	O
don	O
’	O
t	O
know	O
the	O
optimal	O
t	O
,	O
one	O
approach	B
is	O
to	O
redeﬁne	O
the	O
probability	B
transitions	O
such	O
that	O
the	O
desired	O
state	O
b	O
is	O
an	O
ab-	O
sorbing	O
state	O
of	O
the	O
chain	B
(	O
that	O
is	O
,	O
one	O
can	O
enter	O
this	O
state	O
but	O
not	O
leave	O
it	O
)	O
.	O
with	O
this	O
redeﬁnition	O
,	O
the	O
most	O
probable	O
joint	O
state	O
will	O
correspond	O
to	O
the	O
most	B
probable	I
state	I
on	O
the	O
product	O
of	O
n	O
transitions	O
–	O
once	O
the	O
absorbing	B
state	I
is	O
reached	O
the	O
chain	B
will	O
stay	O
in	O
this	O
state	O
,	O
and	O
hence	O
the	O
most	B
probable	I
path	I
can	O
be	O
read	O
oﬀ	O
from	O
the	O
sequence	O
of	O
states	O
up	O
to	O
the	O
ﬁrst	O
time	O
the	O
chain	B
hits	O
the	O
absorbing	B
state	I
.	O
this	O
approach	B
is	O
demonstrated	O
in	O
demomostprobablepath.m	O
,	O
along	O
with	O
the	O
more	O
direct	O
approaches	O
described	O
below	O
.	O
an	O
alternative	O
,	O
cleaner	O
approach	B
is	O
as	O
follows	O
:	O
for	O
the	O
markov	O
chain	B
we	O
can	O
dispense	O
with	O
variable-to-factor	O
and	O
factor-to-variable	O
messages	O
and	O
use	O
only	O
variable-to-variable	O
messages	O
.	O
if	O
we	O
want	O
to	O
ﬁnd	O
the	O
most	O
likely	O
set	O
of	O
states	O
a	O
,	O
s2	O
,	O
.	O
.	O
.	O
,	O
st−1	O
,	O
b	O
to	O
get	O
us	O
there	O
,	O
then	O
this	O
can	O
be	O
computed	O
by	O
deﬁning	O
the	O
maximal	O
path	B
probability	O
e	O
(	O
a	O
→	O
b	O
,	O
t	O
)	O
to	O
get	O
from	O
a	O
to	O
b	O
in	O
t	O
-timesteps	O
:	O
e	O
(	O
a	O
→	O
b	O
,	O
t	O
)	O
=	O
max	O
=	O
max	O
s2	O
,	O
...	O
,	O
st−1	O
s3	O
,	O
...	O
,	O
st−1	O
p	O
(	O
s2|s1	O
=	O
a	O
)	O
p	O
(	O
s3|s2	O
)	O
p	O
(	O
s4|s3	O
)	O
.	O
.	O
.	O
p	O
(	O
st	O
=	O
b|st−1	O
)	O
(	O
cid:124	O
)	O
max	O
s2	O
(	O
cid:125	O
)	O
p	O
(	O
s2|s1	O
=	O
a	O
)	O
p	O
(	O
s3|s2	O
)	O
p	O
(	O
s4|s3	O
)	O
.	O
.	O
.	O
p	O
(	O
st	O
=	O
b|st−1	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
γ2→3	O
(	O
s3	O
)	O
to	O
compute	O
this	O
eﬃciently	O
we	O
deﬁne	O
messages	O
γt−1→t	O
(	O
st	O
)	O
p	O
(	O
st+1|st	O
)	O
,	O
γt→t+1	O
(	O
st+1	O
)	O
=	O
max	O
st	O
t	O
≥	O
2	O
,	O
γ1→2	O
(	O
s2	O
)	O
=	O
p	O
(	O
s2|s1	O
=	O
a	O
)	O
until	O
the	O
point	O
e	O
(	O
a	O
→	O
b	O
,	O
t	O
)	O
=	O
max	O
st−1	O
γt−2→t−1	O
(	O
st−1	O
)	O
p	O
(	O
st	O
=	O
b|st−1	O
)	O
=	O
γt−1→t	O
(	O
st	O
=	O
b	O
)	O
(	O
5.2.19	O
)	O
(	O
5.2.20	O
)	O
(	O
5.2.21	O
)	O
(	O
5.2.22	O
)	O
we	O
can	O
now	O
proceed	O
to	O
ﬁnd	O
the	O
maximal	O
path	B
probability	O
for	O
timestep	O
t	O
+	O
1.	O
since	O
the	O
messages	O
up	O
to	O
time	O
t	O
−	O
1	O
will	O
be	O
the	O
same	O
as	O
before	O
,	O
we	O
need	O
only	O
compute	O
one	O
additional	O
message	B
,	O
γt−1→t	O
(	O
st	O
)	O
,	O
from	O
which	O
e	O
(	O
a	O
→	O
b	O
,	O
t	O
+	O
1	O
)	O
=	O
max	O
st	O
γt−1→t	O
(	O
st	O
)	O
p	O
(	O
st	O
+1	O
=	O
b|st	O
)	O
=	O
γt→t	O
+1	O
(	O
st	O
+1	O
=	O
b	O
)	O
(	O
5.2.23	O
)	O
we	O
can	O
proceed	O
in	O
this	O
manner	O
until	O
we	O
reach	O
e	O
(	O
a	O
→	O
b	O
,	O
n	O
)	O
where	O
n	O
is	O
the	O
number	O
of	O
nodes	O
in	O
the	O
graph	B
.	O
we	O
don	O
’	O
t	O
need	O
to	O
go	O
beyond	O
this	O
number	O
of	O
steps	O
since	O
those	O
that	O
do	O
must	O
necessarily	O
contain	O
non-simple	O
paths	O
.	O
(	O
a	O
simple	B
path	I
is	O
one	O
that	O
does	O
not	O
include	O
the	O
same	O
state	O
more	O
than	O
once	O
.	O
)	O
the	O
optimal	O
time	O
t∗	O
is	O
then	O
given	O
by	O
which	O
of	O
e	O
(	O
a	O
→	O
b	O
,	O
2	O
)	O
,	O
.	O
.	O
.	O
,	O
e	O
(	O
a	O
→	O
b	O
,	O
n	O
)	O
is	O
maximal	O
.	O
given	O
t∗	O
one	O
can	O
begin	O
to	O
backtrack1	O
.	O
since	O
e	O
(	O
a	O
→	O
b	O
,	O
t	O
∗	O
)	O
=	O
max	O
st∗−1	O
γt∗−2→t∗−1	O
(	O
st∗−1	O
)	O
p	O
(	O
st∗	O
=	O
b|st∗−1	O
)	O
we	O
know	O
the	O
optimal	O
state	O
s∗	O
t∗−1	O
=	O
argmax	O
st∗−1	O
γt∗−2→t∗−1	O
(	O
st∗−1	O
)	O
p	O
(	O
st∗	O
=	O
b|st∗−1	O
)	O
(	O
5.2.24	O
)	O
(	O
5.2.25	O
)	O
1an	O
alternative	O
to	O
ﬁnding	O
t∗	O
is	O
to	O
deﬁne	O
self-transitions	O
with	O
probability	O
1	O
,	O
and	O
then	O
use	O
a	O
ﬁxed	O
time	O
t	O
=	O
n	O
.	O
once	O
the	O
desired	O
state	O
b	O
is	O
reached	O
,	O
the	O
self-transition	O
then	O
preserves	O
the	O
chain	B
in	O
state	O
b	O
for	O
the	O
remaining	O
timesteps	O
.	O
this	O
procedure	O
is	O
used	O
in	O
mostprobablepathmult.m	O
draft	O
march	O
9	O
,	O
2010	O
75	O
we	O
can	O
then	O
continue	O
to	O
backtrack	O
:	O
s∗	O
t∗−2	O
=	O
argmax	O
st∗−2	O
γt∗−3→t∗−2	O
(	O
st∗−2	O
)	O
p	O
(	O
s∗	O
t∗−1|st∗−2	O
)	O
and	O
so	O
on	O
.	O
see	O
mostprobablepath.m	O
.	O
other	O
forms	O
of	O
inference	B
(	O
5.2.26	O
)	O
•	O
in	O
the	O
above	O
derivation	O
we	O
do	O
not	O
use	O
any	O
properties	B
of	O
probability	B
,	O
except	O
that	O
p	O
must	O
be	O
non-	O
negative	O
(	O
otherwise	O
sign	O
changes	O
can	O
ﬂip	O
a	O
whole	O
sequence	O
‘	O
probability	B
’	O
and	O
the	O
local	B
message	O
recur-	O
sion	O
no	O
longer	O
applies	O
)	O
.	O
one	O
can	O
consider	O
the	O
algorithm	B
as	O
ﬁnding	O
the	O
optimal	O
‘	O
product	O
’	O
path	B
from	O
a	O
to	O
b	O
.	O
•	O
it	O
is	O
straightforward	O
to	O
modify	O
the	O
algorithm	B
to	O
solve	O
the	O
(	O
single-source	O
,	O
single-sink	O
)	O
shortest	B
weighted	I
path	I
problem	O
.	O
one	O
way	O
to	O
do	O
this	O
is	O
to	O
replace	O
the	O
markov	O
transition	O
probabilities	O
with	O
edge	O
weights	O
exp	O
(	O
−u	O
(	O
st|st−1	O
)	O
)	O
,	O
where	O
u	O
(	O
st|st−1	O
)	O
is	O
inﬁnite	O
if	O
there	O
is	O
no	O
edge	O
from	O
st−1	O
to	O
st.	O
this	O
approach	B
is	O
taken	O
in	O
shortestpath.m	O
which	O
is	O
able	O
to	O
deal	O
with	O
either	O
positive	O
or	O
negative	O
edge	O
weights	O
.	O
this	O
method	O
is	O
therefore	O
more	O
general	O
than	O
the	O
well-known	O
dijkstra	O
’	O
s	O
algorithm	B
[	O
111	O
]	O
which	O
requires	O
weights	O
to	O
be	O
positive	O
.	O
if	O
a	O
negative	O
edge	O
cycle	O
exists	O
,	O
the	O
code	O
returns	O
the	O
shortest	O
weighted	O
length	O
n	O
path	B
,	O
where	O
n	O
is	O
the	O
number	O
of	O
nodes	O
in	O
the	O
graph	B
.	O
see	O
demoshortestpath.m	O
.	O
•	O
the	O
above	O
algorithm	B
is	O
eﬃcient	B
for	O
the	O
single-source	O
,	O
single-sink	O
scenario	O
,	O
since	O
the	O
messages	O
contain	O
only	O
n	O
states	O
,	O
meaning	O
that	O
the	O
overall	O
storage	O
is	O
o	O
(	O
n	O
2	O
)	O
.	O
•	O
as	O
it	O
stands	O
,	O
the	O
algorithm	B
is	O
numerically	O
impractical	O
since	O
the	O
messages	O
are	O
recursively	O
multiplied	O
by	O
values	O
usually	O
less	O
than	O
1	O
(	O
at	O
least	O
for	O
the	O
case	O
of	O
probabilities	O
)	O
.	O
one	O
will	O
therefore	O
quickly	O
run	O
into	O
numerical	B
underﬂow	O
(	O
or	O
possibly	O
overﬂow	O
in	O
the	O
case	O
of	O
non-probabilities	O
)	O
with	O
this	O
method	O
.	O
to	O
ﬁx	O
the	O
ﬁnal	O
point	O
above	O
,	O
it	O
is	O
best	O
to	O
work	O
by	O
deﬁning	O
the	O
logarithm	O
of	O
e.	O
since	O
this	O
is	O
a	O
monotonic	O
transformation	O
,	O
the	O
most	B
probable	I
path	I
deﬁned	O
through	O
log	O
e	O
is	O
the	O
same	O
as	O
that	O
obtained	O
from	O
e.	O
in	O
this	O
case	O
l	O
(	O
a	O
→	O
b	O
,	O
t	O
)	O
=	O
max	O
s2	O
,	O
...	O
,	O
st−1	O
=	O
max	O
s2	O
,	O
...	O
,	O
st−1	O
(	O
cid:34	O
)	O
log	O
[	O
p	O
(	O
s2|s1	O
=	O
a	O
)	O
p	O
(	O
s3|s2	O
)	O
p	O
(	O
s4|s3	O
)	O
.	O
.	O
.	O
p	O
(	O
st	O
=	O
b|st−1	O
)	O
]	O
log	O
p	O
(	O
s2|s1	O
=	O
a	O
)	O
+	O
(	O
cid:35	O
)	O
log	O
p	O
(	O
st|st−1	O
)	O
+	O
log	O
p	O
(	O
st	O
=	O
b|st−1	O
)	O
t−1	O
(	O
cid:88	O
)	O
t=2	O
we	O
can	O
therefore	O
deﬁne	O
new	O
messages	O
λt→t+1	O
(	O
st+1	O
)	O
=	O
max	O
st	O
[	O
λt−1→t	O
(	O
st	O
)	O
+	O
log	O
p	O
(	O
st+1|st	O
)	O
]	O
one	O
then	O
proceeds	O
as	O
before	O
by	O
ﬁnding	O
the	O
most	O
probable	O
t∗	O
deﬁned	O
on	O
l	O
,	O
and	O
backtracks	O
.	O
(	O
5.2.27	O
)	O
(	O
5.2.28	O
)	O
(	O
5.2.29	O
)	O
graphical	O
model	B
corresponding	O
to	O
this	O
simple	O
markov	O
chain	B
is	O
the	O
belief	B
network	I
(	O
cid:81	O
)	O
remark	O
6.	O
a	O
possible	O
confusion	O
is	O
that	O
optimal	O
paths	O
can	O
be	O
eﬃciently	O
found	O
‘	O
when	O
the	O
graph	B
is	O
loopy	B
’	O
.	O
note	O
that	O
the	O
graph	B
in	O
ﬁg	O
(	O
5.5	O
)	O
is	O
a	O
state-transition	O
diagram	O
,	O
not	O
a	O
graphical	O
model	B
.	O
the	O
t	O
p	O
(	O
st|st−1	O
)	O
,	O
a	O
linear	B
serial	O
structure	B
.	O
hence	O
the	O
underlying	O
graphical	O
model	B
is	O
a	O
simple	O
chain	O
,	O
which	O
explains	O
why	O
computation	O
is	O
eﬃcient	B
.	O
most	B
probable	I
path	I
(	O
multiple-source	O
,	O
multiple-sink	O
)	O
if	O
we	O
need	O
the	O
most	B
probable	I
path	I
between	O
all	O
states	O
a	O
and	O
b	O
,	O
one	O
could	O
re-run	O
the	O
above	O
single-source-	O
single-sink	O
algorithm	B
for	O
all	O
a	O
and	O
b.	O
a	O
computationally	O
more	O
eﬃcient	B
approach	O
is	O
to	O
observe	O
that	O
one	O
can	O
deﬁne	O
a	O
message	B
for	O
each	O
starting	O
state	O
a	O
:	O
γt→t+1	O
(	O
st+1|a	O
)	O
=	O
max	O
st	O
γt−1→t	O
(	O
st|a	O
)	O
p	O
(	O
st+1|st	O
)	O
76	O
(	O
5.2.30	O
)	O
draft	O
march	O
9	O
,	O
2010	O
other	O
forms	O
of	O
inference	B
algorithm	O
1	O
compute	O
marginal	B
p	O
(	O
x1|evidence	O
)	O
from	O
distribution	B
p	O
(	O
x	O
)	O
=	O
(	O
cid:81	O
)	O
1	O
:	O
procedure	O
bucket	B
elimination	I
(	O
p	O
(	O
x	O
)	O
=	O
(	O
cid:81	O
)	O
evidential	O
variables	O
are	O
ordered	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
.	O
f	O
φf	O
(	O
{	O
x	O
}	O
f	O
)	O
.	O
)	O
initialize	O
all	O
bucket	O
potentials	O
to	O
unity	O
.	O
while	O
there	O
are	O
potentials	O
left	O
in	O
the	O
distribution	B
do	O
for	O
each	O
potential	B
φf	O
,	O
its	O
highest	O
variable	B
xj	O
(	O
according	O
to	O
the	O
ordering	O
)	O
.	O
multiply	O
φf	O
with	O
the	O
potential	B
in	O
bucket	O
j	O
and	O
remove	O
φf	O
the	O
distribution	B
.	O
end	O
while	O
for	O
i	O
=	O
bucket	O
n	O
to	O
1	O
do	O
for	O
bucket	O
i	O
sum	O
over	O
the	O
states	O
of	O
variable	B
xi	O
and	O
call	O
this	O
potential	B
γi	O
identify	O
the	O
highest	O
variable	B
xh	O
of	O
potential	B
γi	O
multiply	O
the	O
existing	O
potential	B
in	O
bucket	O
h	O
by	O
γi	O
end	O
for	O
the	O
marginal	B
p	O
(	O
x1|evidence	O
)	O
is	O
proportional	O
to	O
γ1	O
.	O
return	O
p	O
(	O
x1|evidence	O
)	O
2	O
:	O
3	O
:	O
4	O
:	O
5	O
:	O
6	O
:	O
7	O
:	O
8	O
:	O
9	O
:	O
10	O
:	O
11	O
:	O
12	O
:	O
13	O
:	O
14	O
:	O
end	O
procedure	O
f	O
φf	O
(	O
{	O
x	O
}	O
f	O
)	O
.	O
assumes	O
non-	O
(	O
cid:46	O
)	O
fill	O
buckets	O
(	O
cid:46	O
)	O
empty	O
buckets	O
(	O
cid:46	O
)	O
the	O
conditional	B
marginal	I
.	O
and	O
continue	O
until	O
we	O
ﬁnd	O
the	O
maximal	O
path	B
probability	O
matrix	B
for	O
getting	O
from	O
any	O
state	O
a	O
to	O
any	O
state	O
b	O
in	O
t	O
timesteps	O
:	O
e	O
(	O
a	O
→	O
b	O
,	O
t	O
)	O
=	O
max	O
st−1	O
γt−2→t−1	O
(	O
st−1|a	O
)	O
p	O
(	O
st	O
=	O
b|st−1	O
)	O
(	O
5.2.31	O
)	O
since	O
we	O
know	O
the	O
message	B
γt−2→t−1	O
(	O
st−1|a	O
)	O
for	O
all	O
states	O
a	O
,	O
we	O
can	O
readily	O
compute	O
the	O
most	B
probable	I
path	I
from	O
all	O
starting	O
states	O
a	O
to	O
all	O
states	O
b	O
after	O
t	O
steps	O
.	O
this	O
requires	O
passing	B
an	O
n	O
×	O
n	O
matrix	B
message	O
γ.	O
we	O
can	O
then	O
proceed	O
to	O
the	O
next	O
timestep	O
t	O
+	O
1.	O
since	O
the	O
messages	O
up	O
to	O
time	O
t	O
−	O
1	O
will	O
be	O
the	O
same	O
as	O
before	O
,	O
we	O
need	O
only	O
compute	O
one	O
additional	O
message	B
,	O
γt−1→t	O
(	O
st	O
)	O
,	O
from	O
which	O
e	O
(	O
a	O
→	O
b	O
,	O
t	O
+	O
1	O
)	O
=	O
max	O
st	O
γt−1→t	O
(	O
st|a	O
)	O
p	O
(	O
st	O
+1	O
=	O
b|st	O
)	O
(	O
5.2.32	O
)	O
in	O
this	O
way	O
one	O
can	O
then	O
eﬃciently	O
compute	O
the	O
optimal	O
path	B
probabilities	O
for	O
any	O
starting	O
state	O
a	O
and	O
end	O
state	O
b	O
after	O
t	O
timesteps	O
.	O
to	O
ﬁnd	O
the	O
optimal	O
corresponding	O
path	B
,	O
backtracking	B
proceeds	O
as	O
before	O
,	O
see	O
mostprobablepathmult.m	O
.	O
one	O
can	O
also	O
use	O
the	O
same	O
algorithm	B
to	O
solve	O
the	O
multiple-source	O
,	O
multiple	O
sink	O
shortest	B
path	I
problem	O
.	O
this	O
algorithm	B
is	O
a	O
variant	O
of	O
the	O
floyd-warshall-roy	O
algorithm	B
[	O
111	O
]	O
for	O
ﬁnding	O
shortest	O
weighted	O
summed	O
paths	O
on	O
a	O
directed	B
graph	O
(	O
the	O
above	O
algorithm	B
enumerates	O
through	O
time	O
,	O
whereas	O
the	O
fwr	O
algorithm	B
enumerates	O
through	O
states	O
)	O
.	O
5.2.4	O
mixed	B
inference	I
an	O
often	O
encountered	O
situation	O
is	O
to	O
infer	O
the	O
most	B
likely	I
state	I
of	O
a	O
joint	B
marginal	O
,	O
possibly	O
given	O
some	O
evidence	B
.	O
for	O
example	O
,	O
given	O
a	O
distribution	B
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
,	O
ﬁnd	O
(	O
cid:88	O
)	O
argmax	O
x1	O
,	O
x2	O
,	O
...	O
,	O
xm	O
p	O
(	O
x1	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
=	O
argmax	O
x1	O
,	O
x2	O
,	O
...	O
,	O
xm	O
xm+1	O
,	O
...	O
,	O
xn	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
(	O
5.2.33	O
)	O
in	O
general	O
,	O
even	O
for	O
tree	B
structured	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
,	O
the	O
optimal	O
marginal	B
state	O
can	O
not	O
be	O
computed	O
eﬃciently	O
.	O
one	O
way	O
to	O
see	O
this	O
is	O
that	O
due	O
to	O
the	O
summation	O
the	O
resulting	O
joint	B
marginal	O
does	O
not	O
have	O
a	O
structured	B
factored	O
form	O
as	O
products	O
of	O
simpler	O
functions	O
of	O
the	O
marginal	B
variables	O
.	O
finding	O
the	O
most	O
probable	O
joint	O
marginal	B
then	O
requires	O
a	O
search	O
over	O
all	O
the	O
joint	B
marginal	O
states	O
–	O
a	O
task	O
exponential	B
in	O
m.	O
an	O
approximate	B
solution	O
is	O
provided	O
by	O
the	O
em	O
algorithm	B
(	O
see	O
section	O
(	O
11.2	O
)	O
and	O
exercise	O
(	O
57	O
)	O
)	O
.	O
draft	O
march	O
9	O
,	O
2010	O
77	O
inference	B
in	O
multiply-connected	B
graphs	O
e	O
c	O
b	O
g	O
a	O
d	O
f	O
p	O
(	O
e	O
)	O
p	O
(	O
g|d	O
,	O
e	O
)	O
p	O
(	O
c|a	O
)	O
p	O
(	O
b	O
)	O
p	O
(	O
d|a	O
,	O
b	O
)	O
p	O
(	O
b	O
)	O
p	O
(	O
d|a	O
,	O
b	O
)	O
γe	O
(	O
d	O
,	O
g	O
)	O
γe	O
(	O
d	O
,	O
g	O
)	O
p	O
(	O
a	O
)	O
p	O
(	O
f|d	O
)	O
p	O
(	O
a	O
)	O
p	O
(	O
a	O
)	O
γb	O
(	O
d	O
,	O
a	O
)	O
p	O
(	O
a	O
)	O
γb	O
(	O
d	O
,	O
a	O
)	O
p	O
(	O
f|d	O
)	O
p	O
(	O
f|d	O
)	O
p	O
(	O
f|d	O
)	O
γg	O
(	O
d	O
)	O
p	O
(	O
f|d	O
)	O
γg	O
(	O
d	O
)	O
γa	O
(	O
d	O
)	O
γd	O
(	O
f	O
)	O
node	B
is	O
eliminated	O
from	O
the	O
graph	B
.	O
the	O
second	O
stage	O
of	O
eliminating	O
c	O
is	O
trivial	O
since	O
(	O
cid:80	O
)	O
figure	O
5.6	O
:	O
the	O
bucket	B
elimination	I
algorithm	O
applied	O
to	O
the	O
graph	B
ﬁg	O
(	O
2.1	O
)	O
.	O
at	O
each	O
stage	O
,	O
at	O
least	O
one	O
c	O
p	O
(	O
c|a	O
)	O
=	O
1	O
and	O
has	O
therefore	O
been	O
skipped	O
over	O
since	O
this	O
bucket	O
does	O
not	O
send	O
any	O
message	B
.	O
5.3	O
inference	B
in	O
multiply-connected	B
graphs	O
5.3.1	O
bucket	B
elimination	I
we	O
consider	O
here	O
a	O
general	O
conditional	B
marginal	I
variable	O
elimination	O
method	O
that	O
works	O
for	O
any	O
distribu-	O
tion	O
(	O
including	O
multiply	O
connected	B
graphs	O
)	O
.	O
the	O
algorithm	B
assumes	O
the	O
distribution	B
is	O
in	O
the	O
form	O
(	O
cid:89	O
)	O
f	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
∝	O
φ	O
(	O
xf	O
)	O
(	O
5.3.1	O
)	O
(	O
5.3.2	O
)	O
and	O
that	O
the	O
task	O
is	O
to	O
compute	O
p	O
(	O
x1|evidence	O
)	O
.	O
for	O
example	O
,	O
for	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
=	O
p	O
(	O
x1|x2	O
)	O
p	O
(	O
x2|x3	O
)	O
p	O
(	O
x3|x4	O
)	O
p	O
(	O
x4	O
)	O
we	O
could	O
use	O
φ1	O
(	O
x1	O
,	O
x2	O
)	O
=	O
p	O
(	O
x1|x2	O
)	O
,	O
φ2	O
(	O
x2	O
,	O
x3	O
)	O
=	O
p	O
(	O
x2|x3	O
)	O
,	O
φ3	O
(	O
x3	O
,	O
x4	O
)	O
=	O
p	O
(	O
x3|x4	O
)	O
p	O
(	O
x4	O
)	O
(	O
5.3.3	O
)	O
the	O
sets	O
of	O
variables	O
here	O
are	O
x1	O
=	O
(	O
x1	O
,	O
x2	O
)	O
,	O
x2	O
=	O
(	O
x2	O
,	O
x3	O
)	O
,	O
x3	O
=	O
(	O
x3	O
,	O
x4	O
)	O
.	O
in	O
general	O
,	O
the	O
construction	B
of	O
potentials	O
for	O
a	O
distribution	B
is	O
not	O
unique	O
.	O
the	O
task	O
of	O
computing	O
a	O
marginal	B
in	O
which	O
a	O
set	O
of	O
variables	O
xn+1	O
,	O
.	O
.	O
.	O
are	O
clamped	O
to	O
their	O
evidential	O
states	O
is	O
p	O
(	O
x1|evidence	O
)	O
∝	O
p	O
(	O
x1	O
,	O
evidence	B
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:89	O
)	O
x2	O
,	O
...	O
,	O
xn	O
f	O
φf	O
(	O
xf	O
)	O
(	O
5.3.4	O
)	O
the	O
algorithm	B
is	O
given	O
in	O
algorithm	B
(	O
11	O
)	O
and	O
can	O
be	O
considered	O
a	O
way	O
to	O
organise	O
the	O
distributed	O
summation	O
[	O
79	O
]	O
.	O
the	O
algorithm	B
is	O
best	O
explained	O
by	O
a	O
simple	O
example	O
,	O
as	O
given	O
below	O
.	O
example	O
21	O
(	O
bucket	B
elimination	I
)	O
.	O
consider	O
the	O
problem	B
of	O
calculating	O
the	O
marginal	B
p	O
(	O
f	O
)	O
of	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
,	O
e	O
,	O
f	O
,	O
g	O
)	O
=	O
p	O
(	O
f|d	O
)	O
p	O
(	O
g|d	O
,	O
e	O
)	O
p	O
(	O
c|a	O
)	O
p	O
(	O
d|a	O
,	O
b	O
)	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
p	O
(	O
e	O
)	O
,	O
see	O
ﬁg	O
(	O
2.1	O
)	O
.	O
p	O
(	O
f	O
)	O
=	O
(	O
cid:88	O
)	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
,	O
e	O
,	O
f	O
,	O
g	O
)	O
=	O
(	O
cid:88	O
)	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
,	O
e	O
,	O
g	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
,	O
e	O
,	O
g	O
p	O
(	O
f|d	O
)	O
p	O
(	O
g|d	O
,	O
e	O
)	O
p	O
(	O
c|a	O
)	O
p	O
(	O
d|a	O
,	O
b	O
)	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
p	O
(	O
e	O
)	O
(	O
5.3.5	O
)	O
(	O
5.3.6	O
)	O
78	O
draft	O
march	O
9	O
,	O
2010	O
inference	B
in	O
multiply-connected	B
graphs	O
p	O
(	O
f	O
)	O
=	O
(	O
cid:88	O
)	O
terms	O
,	O
we	O
can	O
write	O
p	O
(	O
f	O
)	O
=	O
(	O
cid:88	O
)	O
a	O
,	O
d	O
,	O
g	O
p	O
(	O
f	O
)	O
=	O
(	O
cid:88	O
)	O
p	O
(	O
f	O
)	O
=	O
(	O
cid:88	O
)	O
d	O
d	O
we	O
can	O
distribute	O
the	O
summation	O
over	O
the	O
various	O
terms	O
as	O
follows	O
:	O
e	O
,	O
b	O
and	O
c	O
are	O
end	O
nodes	O
,	O
so	O
that	O
we	O
can	O
sum	O
over	O
their	O
values	O
:	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
b	O
(	O
cid:33	O
)	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
(	O
cid:33	O
)	O
p	O
(	O
g|d	O
,	O
e	O
)	O
p	O
(	O
e	O
)	O
(	O
cid:33	O
)	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
b	O
p	O
(	O
d|a	O
,	O
b	O
)	O
p	O
(	O
b	O
)	O
≡	O
γb	O
(	O
a	O
,	O
d	O
)	O
,	O
(	O
cid:80	O
)	O
e	O
p	O
(	O
f|d	O
)	O
p	O
(	O
a	O
)	O
a	O
,	O
d	O
,	O
g	O
for	O
convenience	O
,	O
lets	O
write	O
the	O
terms	O
in	O
the	O
brackets	O
as	O
(	O
cid:80	O
)	O
γe	O
(	O
d	O
,	O
g	O
)	O
.	O
the	O
term	O
(	O
cid:80	O
)	O
c	O
p	O
(	O
d|a	O
,	O
b	O
)	O
p	O
(	O
b	O
)	O
p	O
(	O
c|a	O
)	O
e	O
p	O
(	O
g|d	O
,	O
e	O
)	O
p	O
(	O
e	O
)	O
≡	O
c	O
p	O
(	O
c|a	O
)	O
is	O
equal	O
to	O
unity	O
,	O
and	O
we	O
therefore	O
eliminate	O
this	O
node	B
directly	O
.	O
rearranging	O
(	O
5.3.7	O
)	O
p	O
(	O
f|d	O
)	O
p	O
(	O
a	O
)	O
γb	O
(	O
a	O
,	O
d	O
)	O
γe	O
(	O
d	O
,	O
g	O
)	O
(	O
5.3.8	O
)	O
if	O
we	O
think	O
of	O
this	O
graphically	O
,	O
the	O
eﬀect	O
of	O
summing	O
over	O
b	O
,	O
c	O
,	O
e	O
is	O
eﬀectively	O
to	O
remove	O
or	O
‘	O
eliminate	O
’	O
those	O
variables	O
.	O
we	O
can	O
now	O
carry	O
on	O
summing	O
over	O
a	O
and	O
g	O
since	O
these	O
are	O
end	O
points	O
of	O
the	O
new	O
graph	B
:	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
(	O
cid:33	O
)	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
(	O
cid:33	O
)	O
p	O
(	O
f|d	O
)	O
p	O
(	O
a	O
)	O
γb	O
(	O
a	O
,	O
d	O
)	O
γe	O
(	O
d	O
,	O
g	O
)	O
a	O
g	O
again	O
,	O
this	O
deﬁnes	O
new	O
functions	O
γa	O
(	O
d	O
)	O
,	O
γg	O
(	O
d	O
)	O
,	O
so	O
that	O
the	O
ﬁnal	O
answer	O
can	O
be	O
found	O
from	O
(	O
5.3.9	O
)	O
(	O
5.3.10	O
)	O
p	O
(	O
f|d	O
)	O
γa	O
(	O
d	O
)	O
γg	O
(	O
d	O
)	O
we	O
illustrate	O
this	O
in	O
ﬁg	O
(	O
5.6	O
)	O
.	O
initially	O
,	O
we	O
deﬁne	O
an	O
ordering	O
of	O
the	O
variables	O
,	O
beginning	O
with	O
the	O
one	O
that	O
we	O
wish	O
to	O
ﬁnd	O
the	O
marginal	B
for	O
–	O
a	O
suitable	O
ordering	O
is	O
therefore	O
,	O
f	O
,	O
d	O
,	O
a	O
,	O
g	O
,	O
b	O
,	O
c	O
,	O
e.	O
then	O
starting	O
with	O
the	O
highest	O
bucket	O
e	O
(	O
according	O
to	O
our	O
ordering	O
f	O
,	O
d	O
,	O
a	O
,	O
g	O
,	O
b	O
,	O
c	O
,	O
e	O
)	O
,	O
we	O
put	O
all	O
the	O
functions	O
that	O
mention	O
e	O
in	O
the	O
e	O
bucket	O
.	O
continuing	O
with	O
the	O
next	O
highest	O
bucket	O
,	O
c	O
,	O
we	O
put	O
all	O
the	O
remaining	O
functions	O
that	O
mention	O
c	O
in	O
this	O
c	O
bucket	O
,	O
etc	O
.	O
the	O
result	O
of	O
this	O
initialisation	O
procedure	O
is	O
that	O
terms	O
(	O
conditional	B
distributions	O
)	O
in	O
the	O
dag	O
are	O
distributed	O
over	O
the	O
buckets	O
,	O
as	O
shown	O
in	O
the	O
left	O
most	O
column	O
of	O
ﬁg	O
(	O
5.6	O
)	O
.	O
eliminating	O
then	O
the	O
highest	O
bucket	O
e	O
,	O
we	O
pass	O
a	O
message	B
to	O
node	B
g.	O
immediately	O
,	O
we	O
can	O
also	O
eliminate	O
bucket	O
c	O
since	O
this	O
sums	O
to	O
unity	O
.	O
in	O
the	O
next	O
column	O
,	O
we	O
have	O
now	O
two	O
less	O
buckets	O
,	O
and	O
we	O
eliminate	O
the	O
highest	O
remaining	O
bucket	O
,	O
this	O
time	O
b	O
,	O
passing	B
a	O
message	B
to	O
bucket	O
a.	O
there	O
are	O
some	O
important	O
observations	O
we	O
can	O
make	O
about	O
bucket	B
elimination	I
:	O
1.	O
to	O
compute	O
say	O
p	O
(	O
x2|evidence	O
)	O
we	O
need	O
to	O
re-order	O
the	O
variables	O
(	O
so	O
that	O
the	O
required	O
marginal	B
variable	O
is	O
labelled	B
x1	O
)	O
and	O
repeat	O
bucket	B
elimination	I
.	O
hence	O
each	O
query	B
(	O
calculation	O
of	O
a	O
marginal	B
in	O
this	O
case	O
)	O
requires	O
re-running	O
the	O
algorithm	B
.	O
it	O
would	O
be	O
more	O
eﬃcient	B
to	O
reuse	O
messages	O
,	O
rather	O
than	O
recalculating	O
them	O
each	O
time	O
.	O
2.	O
in	O
general	O
,	O
bucket	B
elimination	I
constructs	O
multi-variable	O
messages	O
γ	O
from	O
bucket	O
to	O
bucket	O
.	O
the	O
storage	O
requirements	O
of	O
a	O
multi-variable	O
message	B
are	O
exponential	B
in	O
the	O
number	O
of	O
variables	O
of	O
the	O
message	B
.	O
3.	O
for	O
trees	O
we	O
can	O
always	O
choose	O
a	O
variable	B
ordering	O
to	O
render	O
the	O
computational	B
complexity	I
to	O
be	O
linear	B
in	O
the	O
number	O
of	O
variables	O
.	O
such	O
an	O
ordering	O
is	O
called	O
perfect	O
,	O
deﬁnition	O
(	O
49	O
)	O
,	O
and	O
indeed	O
it	O
can	O
be	O
shown	O
that	O
a	O
perfect	O
ordering	O
can	O
always	O
easily	O
be	O
found	O
for	O
singly-connected	B
graphs	O
(	O
see	O
[	O
86	O
]	O
)	O
.	O
however	O
,	O
orderings	O
exist	O
for	O
which	O
bucket	B
elimination	I
will	O
be	O
extremely	O
ineﬃcient	O
.	O
5.3.2	O
loop-cut	O
conditioning	B
for	O
distributions	O
which	O
contain	O
a	O
loop	O
(	O
there	O
is	O
more	O
than	O
one	O
path	B
between	O
two	O
nodes	O
in	O
the	O
graph	B
when	O
the	O
directions	O
are	O
removed	O
)	O
,	O
we	O
run	O
into	O
some	O
diﬃculty	O
with	O
the	O
message	B
passing	I
routines	O
such	O
as	O
the	O
sum-product	B
algorithm	I
which	O
are	O
designed	O
to	O
work	O
on	O
singly-connected	B
graphs	O
only	O
.	O
one	O
way	O
to	O
solve	O
draft	O
march	O
9	O
,	O
2010	O
79	O
c	O
a	O
f	O
b	O
g	O
d	O
(	O
a	O
)	O
e	O
c	O
b	O
g	O
e	O
a	O
f	O
d	O
(	O
b	O
)	O
notes	O
5.7	O
:	O
figure	O
a	O
multiply-	O
connected	B
graph	I
(	O
a	O
)	O
reduced	O
to	O
a	O
singly-connected	B
graph	O
(	O
b	O
)	O
by	O
conditioning	B
on	O
the	O
variable	B
c.	O
the	O
diﬃculties	O
of	O
multiply	O
connected	B
(	O
loopy	B
)	O
graphs	O
is	O
to	O
identify	O
nodes	O
that	O
,	O
when	O
removed	O
,	O
would	O
reveal	O
a	O
singly-connected	B
subgraph	O
[	O
219	O
]	O
.	O
consider	O
the	O
example	O
if	O
ﬁg	O
(	O
5.7	O
)	O
.	O
imagine	O
that	O
we	O
wish	O
to	O
calculate	O
a	O
marginal	B
,	O
say	O
p	O
(	O
d	O
)	O
.	O
then	O
p	O
(	O
d	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
c	O
a	O
,	O
b	O
,	O
e	O
,	O
f	O
,	O
g	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
p	O
(	O
c|a	O
)	O
p	O
(	O
a	O
)	O
p∗	O
(	O
a	O
)	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
p	O
(	O
d|a	O
,	O
b	O
)	O
p	O
(	O
b	O
)	O
p	O
(	O
f|c	O
,	O
d	O
)	O
p∗	O
(	O
f|d	O
)	O
p	O
(	O
g|d	O
,	O
e	O
)	O
(	O
5.3.11	O
)	O
where	O
the	O
p∗	O
deﬁnitions	O
are	O
not	O
necessarily	O
distributions	O
.	O
for	O
each	O
state	O
of	O
c	O
,	O
the	O
form	O
of	O
the	O
products	O
of	O
factors	O
remaining	O
as	O
a	O
function	B
of	O
a	O
,	O
b	O
,	O
e	O
,	O
f	O
,	O
g	O
is	O
singly-connected	B
,	O
so	O
that	O
standard	O
singly-connected	O
message	B
passing	I
can	O
be	O
used	O
to	O
perform	O
inference	B
.	O
we	O
will	O
need	O
to	O
do	O
perform	O
inference	B
for	O
each	O
state	O
of	O
variable	B
c	O
,	O
each	O
state	O
deﬁning	O
a	O
new	O
singly-connected	B
graph	O
(	O
with	O
the	O
same	O
structure	B
)	O
but	O
with	O
modiﬁed	O
potentials	O
.	O
more	O
generally	O
,	O
we	O
can	O
deﬁne	O
a	O
set	O
of	O
variables	O
c	O
,	O
called	O
the	O
loop	B
cut	I
set	I
and	O
run	O
singly-connected	B
inference	O
for	O
each	O
joint	B
state	O
of	O
the	O
cut-set	O
variables	O
c.	O
this	O
can	O
also	O
be	O
used	O
for	O
ﬁnding	O
the	O
most	B
likely	I
state	I
of	O
a	O
multiply-connected	B
joint	O
distribution	B
as	O
well	O
.	O
hence	O
,	O
for	O
a	O
computational	O
price	O
exponential	B
in	O
the	O
loop-	O
cut	B
size	O
,	O
we	O
can	O
calculate	O
the	O
marginals	O
(	O
or	O
the	O
most	B
likely	I
state	I
)	O
for	O
a	O
multiply-connected	B
distribution	O
.	O
however	O
,	O
determining	O
a	O
small	O
cut	O
set	O
is	O
in	O
general	O
diﬃcult	O
,	O
and	O
there	O
is	O
no	O
guarantee	O
that	O
this	O
will	O
anyway	O
be	O
small	O
for	O
a	O
given	O
graph	B
.	O
whilst	O
this	O
method	O
is	O
able	O
to	O
handle	O
loops	O
in	O
a	O
general	O
manner	O
,	O
it	O
is	O
not	O
particularly	O
elegant	O
since	O
the	O
concept	O
of	O
messages	O
now	O
only	O
applies	O
conditioned	O
on	O
the	O
cut	O
set	O
variables	O
,	O
and	O
how	O
to	O
re-use	O
messages	O
for	O
inference	B
of	O
additional	O
quantities	O
of	O
interest	O
becomes	O
unclear	O
.	O
we	O
will	O
discuss	O
an	O
alternative	O
method	O
for	O
handling	O
multiply	O
connected	B
distributions	O
in	O
chapter	O
(	O
6	O
)	O
.	O
5.4	O
message	B
passing	I
for	O
continuous	B
distributions	O
for	O
parametric	O
continuous	B
distributions	O
p	O
(	O
x|θx	O
)	O
,	O
message	B
passing	I
corresponds	O
to	O
passing	B
parameters	O
θ	O
of	O
the	O
distributions	O
.	O
for	O
the	O
sum-product	B
algorithm	I
,	O
this	O
requires	O
that	O
the	O
operations	O
of	O
multiplication	O
and	O
integration	O
over	O
the	O
variables	O
are	O
closed	O
with	O
respect	O
to	O
the	O
family	B
of	O
distributions	O
.	O
this	O
is	O
the	O
case	O
,	O
for	O
example	O
,	O
for	O
the	O
gaussian	O
distribution	B
–	O
the	O
marginal	B
(	O
integral	O
)	O
of	O
a	O
gaussian	O
is	O
another	O
gaussian	O
,	O
and	O
the	O
product	O
of	O
two	O
gaussians	O
is	O
a	O
gaussian	O
,	O
see	O
section	O
(	O
8.6	O
)	O
.	O
this	O
means	O
that	O
we	O
can	O
then	O
implement	O
the	O
sum-product	B
algorithm	I
based	O
on	O
passing	B
mean	O
and	O
covariance	B
parameters	O
.	O
to	O
implement	O
this	O
requires	O
some	O
tedious	O
algebra	O
to	O
compute	O
the	O
appropriate	O
message	B
parameter	O
updates	O
.	O
at	O
this	O
stage	O
,	O
the	O
complex-	O
ities	O
from	O
performing	O
such	O
calculations	O
are	O
a	O
potential	B
distraction	O
,	O
though	O
the	O
interested	O
reader	O
may	O
refer	O
to	O
demosumprodgaussmoment.m	O
,	O
demosumprodgausscanon.m	O
and	O
demosumprodgausscanonlds.m	O
and	O
also	O
chapter	O
(	O
24	O
)	O
for	O
examples	O
of	O
message	B
passing	I
with	O
gaussians	O
.	O
for	O
more	O
general	O
exponential	B
family	I
dis-	O
tributions	O
,	O
message	B
passing	I
is	O
essentially	O
straightforward	O
,	O
though	O
again	O
the	O
speciﬁcs	O
of	O
the	O
updates	O
may	O
be	O
tedious	O
to	O
work	O
out	O
.	O
in	O
cases	O
where	O
the	O
operations	O
of	O
marginalisation	B
and	O
products	O
are	O
not	O
closed	O
within	O
the	O
family	B
,	O
the	O
distributions	O
need	O
to	O
be	O
projected	O
back	O
to	O
the	O
chosen	O
message	B
family	O
.	O
expectation	B
propagation	I
,	O
section	O
(	O
28.7	O
)	O
is	O
relevant	O
in	O
this	O
case	O
.	O
5.5	O
notes	O
a	O
take-home	O
message	B
from	O
this	O
chapter	O
is	O
that	O
(	O
non-mixed	O
)	O
inference	B
in	O
singly-connected	B
structures	O
is	O
usually	O
computationally	O
tractable	O
.	O
notable	O
exceptions	O
are	O
when	O
the	O
message	B
passing	I
operations	O
are	O
not-	O
closed	O
within	O
the	O
message	B
family	O
,	O
or	O
representing	O
messages	O
explicitly	O
requires	O
an	O
exponential	B
amount	O
of	O
space	O
.	O
this	O
happens	O
for	O
example	O
when	O
the	O
distribution	B
can	O
contain	O
both	O
discrete	B
and	O
continuous	B
variables	O
,	O
80	O
draft	O
march	O
9	O
,	O
2010	O
code	O
such	O
as	O
the	O
switching	B
linear	I
dynamical	I
system	I
,	O
which	O
we	O
discuss	O
in	O
chapter	O
(	O
25	O
)	O
.	O
broadly	O
speaking	O
,	O
inference	B
in	O
multiply-connected	B
structures	O
is	O
more	O
complex	O
and	O
may	O
be	O
intractable	O
.	O
however	O
,	O
we	O
do	O
not	O
want	O
to	O
give	O
the	O
impression	O
that	O
this	O
is	O
always	O
the	O
case	O
.	O
notable	O
exceptions	O
are	O
:	O
ﬁnding	O
the	O
map	B
state	O
in	O
an	O
attractive	O
pairwise	O
mrf	O
,	O
section	O
(	O
28.8	O
)	O
;	O
ﬁnding	O
the	O
map	B
and	O
mpm	O
state	O
in	O
a	O
binary	O
planar	O
mrf	O
with	O
pure	O
interactions	O
,	O
see	O
for	O
example	O
[	O
115	O
,	O
243	O
]	O
.	O
for	O
n	O
variables	O
in	O
the	O
graph	B
,	O
a	O
naive	O
use	O
of	O
the	O
junction	B
tree	I
algorithm	O
for	O
these	O
inferences	O
would	O
result	O
in	O
an	O
o	O
(	O
cid:0	O
)	O
2n	O
(	O
cid:1	O
)	O
computation	O
,	O
whereas	O
clever	O
algorithms	O
are	O
able	O
to	O
return	O
the	O
exact	O
results	O
in	O
o	O
(	O
cid:0	O
)	O
n	O
3	O
(	O
cid:1	O
)	O
operations	O
.	O
of	O
interest	O
is	O
bond	B
propagation	I
[	O
177	O
]	O
which	O
is	O
an	O
intuitive	O
node	B
elimination	O
method	O
to	O
arrive	O
at	O
the	O
mpm	O
inference	B
in	O
pure-	O
interaction	O
ising	O
models	O
.	O
5.6	O
code	O
the	O
code	O
below	O
implements	O
message	B
passing	I
on	O
a	O
tree	B
structured	O
factor	B
graph	I
.	O
the	O
fg	O
is	O
stored	O
as	O
an	O
adjacency	B
matrix	I
with	O
the	O
message	B
between	O
fg	O
node	B
i	O
and	O
fg	O
node	B
j	O
given	O
in	O
ai	O
,	O
j	O
.	O
factorgraph.m	O
:	O
return	O
a	O
factor	B
graph	I
adjacency	O
matrix	B
and	O
message	B
numbers	O
sumprodfg.m	O
:	O
sum-product	B
algorithm	I
on	O
a	O
factor	B
graph	I
in	O
general	O
it	O
is	O
recommended	O
to	O
work	O
in	O
log-space	O
in	O
the	O
max-product	B
case	O
,	O
particularly	O
for	O
large	O
graphs	O
since	O
the	O
produce	O
of	O
messages	O
can	O
become	O
very	O
small	O
.	O
the	O
code	O
provided	O
does	O
not	O
work	O
in	O
log	O
space	O
and	O
as	O
such	O
may	O
not	O
work	O
on	O
large	O
graphs	O
;	O
writing	O
this	O
using	O
log-messages	O
is	O
straightforward	O
but	O
leads	O
to	O
less	O
readable	O
code	O
.	O
an	O
implementation	O
based	O
on	O
log-messages	O
is	O
left	O
as	O
an	O
exercise	O
for	O
the	O
interested	O
reader	O
.	O
maxprodfg.m	O
:	O
max-product	B
algorithm	O
on	O
a	O
factor	B
graph	I
maxnprodfg.m	O
:	O
n-max-product	O
algorithm	B
on	O
a	O
factor	B
graph	I
5.6.1	O
factor	B
graph	I
examples	O
for	O
the	O
distribution	B
from	O
ﬁg	O
(	O
5.3	O
)	O
,	O
the	O
following	O
code	O
ﬁnds	O
the	O
marginals	O
and	O
most	O
likely	O
joint	O
states	O
.	O
the	O
number	O
of	O
states	O
of	O
each	O
variable	B
is	O
chosen	O
at	O
random	O
.	O
demosumprod.m	O
:	O
test	O
the	O
sum-product	B
algorithm	I
demomaxprod.m	O
:	O
test	O
the	O
max-product	B
algorithm	O
demomaxnprod.m	O
:	O
test	O
the	O
max-n-product	O
algorithm	B
5.6.2	O
most	O
probable	O
and	O
shortest	B
path	I
mostprobablepath.m	O
:	O
most	B
probable	I
path	I
demomostprobablepath.m	O
:	O
most	O
probable	O
versus	O
shortest	B
path	I
demo	O
the	O
shortest	B
path	I
demo	O
works	O
for	O
both	O
positive	O
and	O
negative	O
edge	O
weights	O
.	O
if	O
negative	O
weight	B
cycles	O
exist	O
,	O
the	O
code	O
ﬁnds	O
the	O
best	O
length	O
n	O
shortest	B
path	I
.	O
demoshortestpath.m	O
:	O
shortest	B
path	I
demo	O
mostprobablepathmult.m	O
:	O
most	B
probable	I
path	I
–	O
multi-source	O
,	O
multi-sink	O
demomostprobablepathmult.m	O
:	O
demo	O
of	O
most	B
probable	I
path	I
–	O
multi-source	O
,	O
multi-sink	O
5.6.3	O
bucket	B
elimination	I
the	O
eﬃcacy	O
of	O
bucket	B
elimination	I
depends	O
critically	O
on	O
the	O
elimination	O
sequence	O
chosen	O
.	O
in	O
the	O
demon-	O
stration	O
below	O
we	O
ﬁnd	O
the	O
marginal	B
of	O
a	O
variable	B
in	O
the	O
chest	B
clinic	I
exercise	O
using	O
a	O
randomly	O
chosen	O
elimination	O
order	O
.	O
the	O
desired	O
marginal	B
variable	O
is	O
speciﬁed	O
as	O
the	O
last	O
to	O
be	O
eliminated	O
.	O
for	O
comparison	O
we	O
use	O
an	O
elimination	O
sequence	O
based	O
on	O
decimating	O
a	O
triangulated	B
graph	O
of	O
the	O
model	B
,	O
as	O
discussed	O
in	O
section	O
(	O
6.5.1	O
)	O
,	O
again	O
under	O
the	O
constraint	O
that	O
the	O
last	O
variable	B
to	O
be	O
‘	O
decimated	O
’	O
is	O
the	O
marginal	B
variable	O
of	O
draft	O
march	O
9	O
,	O
2010	O
81	O
exercises	O
interest	O
.	O
for	O
this	O
smarter	O
choice	O
of	O
elimination	O
sequence	O
,	O
the	O
complexity	O
of	O
computing	O
this	O
single	O
marginal	B
is	O
roughly	O
the	O
same	O
as	O
that	O
for	O
the	O
junction	B
tree	I
algorithm	O
,	O
using	O
the	O
same	O
triangulation	B
.	O
bucketelim.m	O
:	O
bucket	B
elimination	I
demobucketelim.m	O
:	O
demo	O
bucket	B
elimination	I
5.6.4	O
message	B
passing	I
on	O
gaussians	O
the	O
following	O
code	O
hints	O
at	O
how	O
message	B
passing	I
may	O
be	O
implemented	O
for	O
continuous	B
distributions	O
.	O
the	O
reader	O
is	O
referred	O
to	O
the	O
brmltoolbox	O
for	O
further	O
details	O
and	O
also	O
section	O
(	O
8.6	O
)	O
for	O
the	O
algebraic	O
manipu-	O
lations	O
required	O
to	O
perform	O
marginalisation	B
and	O
products	O
of	O
gaussians	O
.	O
the	O
same	O
principal	O
holds	O
for	O
any	O
family	B
of	O
distributions	O
which	O
is	O
closed	O
under	O
products	O
and	O
marginalisation	B
,	O
and	O
the	O
reader	O
may	O
wish	O
to	O
implement	O
speciﬁc	O
families	O
following	O
the	O
method	O
outlined	O
for	O
gaussians	O
.	O
demosumprodgaussmoment.m	O
:	O
sum-product	B
message	O
passing	B
based	O
on	O
gaussian	O
moment	O
parameterisation	O
5.7	O
exercises	O
exercise	O
52.	O
given	O
a	O
pairwise	B
singly	O
connected	B
markov	O
network	O
of	O
the	O
form	O
φ	O
(	O
xi	O
,	O
xj	O
)	O
(	O
5.7.1	O
)	O
(	O
cid:89	O
)	O
i∼j	O
p	O
(	O
x	O
)	O
=	O
1	O
z	O
explain	O
how	O
to	O
eﬃciently	O
compute	O
the	O
normalisation	B
factor	O
(	O
also	O
called	O
the	O
partition	B
function	I
)	O
z	O
as	O
a	O
function	B
of	O
the	O
potentials	O
φ.	O
exercise	O
53.	O
you	O
are	O
employed	O
by	O
a	O
web	B
start	O
up	O
company	O
that	O
designs	O
virtual	B
environments	O
,	O
in	O
which	O
players	O
can	O
move	O
between	O
rooms	O
.	O
the	O
rooms	O
which	O
are	O
accessible	O
from	O
another	O
in	O
one	O
time	O
step	O
is	O
given	O
by	O
the	O
100	O
×	O
100	O
matrix	B
m	O
,	O
stored	O
in	O
virtualworlds.mat	O
,	O
where	O
mij	O
=	O
1	O
means	O
that	O
there	O
is	O
a	O
door	O
between	O
rooms	O
i	O
and	O
j	O
(	O
mij	O
=	O
mji	O
)	O
.	O
mij	O
=	O
0	O
means	O
that	O
there	O
is	O
no	O
door	O
between	O
rooms	O
i	O
and	O
j.	O
mii	O
=	O
1	O
meaning	O
that	O
in	O
one	O
time	O
step	O
,	O
one	O
can	O
stay	O
in	O
the	O
same	O
room	O
.	O
you	O
can	O
visualise	O
this	O
matrix	B
by	O
typing	O
imagesc	O
(	O
m	O
)	O
.	O
1.	O
write	O
a	O
list	O
of	O
rooms	O
which	O
can	O
not	O
be	O
reached	O
from	O
room	O
2	O
after	O
10	O
time	O
steps	O
.	O
2.	O
the	O
manager	O
complains	O
that	O
takes	O
at	O
least	O
13	O
time	O
steps	O
to	O
get	O
from	O
room	O
1	O
to	O
room	O
100.	O
is	O
this	O
true	O
?	O
3.	O
find	O
the	O
most	O
likely	O
path	O
(	O
sequence	O
of	O
rooms	O
)	O
to	O
get	O
from	O
room	O
1	O
to	O
room	O
100	O
.	O
4.	O
if	O
a	O
single	O
player	O
were	O
to	O
jump	O
randomly	O
from	O
one	O
room	O
to	O
another	O
(	O
or	O
stay	O
in	O
the	O
same	O
room	O
)	O
,	O
with	O
no	O
preference	O
between	O
rooms	O
,	O
what	O
is	O
the	O
probability	B
at	O
time	O
t	O
(	O
cid:29	O
)	O
1	O
the	O
player	O
will	O
be	O
in	O
room	O
1	O
?	O
assume	O
that	O
eﬀectively	O
an	O
inﬁnite	O
amount	O
of	O
time	O
has	O
passed	O
and	O
the	O
player	O
began	O
in	O
room	O
1	O
at	O
t	O
=	O
1	O
.	O
5.	O
if	O
two	O
players	O
are	O
jumping	O
randomly	O
between	O
rooms	O
(	O
or	O
staying	O
in	O
the	O
same	O
room	O
)	O
,	O
explain	O
how	O
to	O
compute	O
the	O
probability	B
that	O
,	O
after	O
an	O
inﬁnite	O
amount	O
of	O
time	O
,	O
at	O
least	O
one	O
of	O
them	O
will	O
be	O
in	O
room	O
1	O
?	O
assume	O
that	O
both	O
players	O
begin	O
in	O
room	O
1.	O
exercise	O
54.	O
consider	O
the	O
hidden	B
markov	O
model	B
:	O
p	O
(	O
v1	O
,	O
.	O
.	O
.	O
,	O
vt	O
,	O
h1	O
,	O
.	O
.	O
.	O
,	O
ht	O
)	O
=	O
p	O
(	O
h1	O
)	O
p	O
(	O
v1|h1	O
)	O
p	O
(	O
vt|ht	O
)	O
p	O
(	O
ht|ht−1	O
)	O
(	O
5.7.2	O
)	O
in	O
which	O
dom	O
(	O
ht	O
)	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
h	O
}	O
and	O
dom	O
(	O
vt	O
)	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
v	O
}	O
for	O
all	O
t	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
t	O
.	O
1.	O
draw	O
a	O
belief	B
network	I
representation	O
of	O
the	O
above	O
distribution	B
.	O
2.	O
draw	O
a	O
factor	B
graph	I
representation	O
of	O
the	O
above	O
distribution	B
.	O
82	O
draft	O
march	O
9	O
,	O
2010	O
t	O
(	O
cid:89	O
)	O
t=2	O
exercises	O
3.	O
use	O
the	O
factor	B
graph	I
to	O
derive	O
a	O
sum-product	B
algorithm	I
to	O
compute	O
marginals	O
p	O
(	O
ht|v1	O
,	O
.	O
.	O
.	O
,	O
vt	O
)	O
.	O
explain	O
the	O
sequence	O
order	O
of	O
messages	O
passed	O
on	O
your	O
factor	B
graph	I
.	O
4.	O
explain	O
how	O
to	O
compute	O
p	O
(	O
ht	O
,	O
ht+1|v1	O
,	O
.	O
.	O
.	O
,	O
vt	O
)	O
.	O
exercise	O
55.	O
for	O
a	O
singly	O
connected	B
markov	O
network	O
,	O
p	O
(	O
x	O
)	O
=	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
,	O
the	O
computation	O
of	O
a	O
marginal	B
p	O
(	O
xi	O
)	O
can	O
be	O
carried	O
out	O
eﬃciently	O
.	O
similarly	O
,	O
the	O
most	O
likely	O
joint	O
state	O
x∗	O
=	O
arg	O
maxx1	O
,	O
...	O
,	O
xn	O
p	O
(	O
x	O
)	O
can	O
be	O
computed	O
eﬃciently	O
.	O
explain	O
when	O
the	O
most	O
likely	O
joint	O
state	O
of	O
a	O
marginal	B
can	O
be	O
computed	O
eﬃciently	O
,	O
i.e	O
.	O
under	O
what	O
circumstances	O
could	O
one	O
eﬃciently	O
(	O
in	O
o	O
(	O
m	O
)	O
time	O
)	O
compute	O
argmax	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
for	O
x1	O
,	O
x2	O
,	O
...	O
,	O
xm	O
m	O
<	O
n	O
?	O
exercise	O
56.	O
consider	O
the	O
internet	O
with	O
webpages	O
labelled	B
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
if	O
webpage	O
j	O
has	O
a	O
link	O
to	O
webpage	O
i	O
,	O
then	O
we	O
place	O
an	O
element	O
of	O
the	O
matrix	B
lij	O
=	O
1	O
,	O
otherwise	O
lij	O
=	O
0.	O
by	O
considering	O
a	O
random	O
jump	O
from	O
webpage	O
j	O
to	O
webpage	O
i	O
to	O
be	O
given	O
by	O
the	O
transition	O
probability	O
(	O
5.7.3	O
)	O
mij	O
=	O
lij	O
(	O
cid:80	O
)	O
i	O
lij	O
t	O
(	O
cid:89	O
)	O
t=2	O
what	O
is	O
the	O
probability	B
that	O
after	O
an	O
inﬁnite	O
amount	O
of	O
random	O
surﬁng	O
,	O
one	O
ends	O
up	O
on	O
webpage	O
i	O
?	O
how	O
could	O
you	O
relate	O
this	O
to	O
the	O
potential	B
‘	O
relevance	O
’	O
of	O
a	O
webpage	O
in	O
terms	O
of	O
a	O
search	B
engine	I
?	O
exercise	O
57.	O
a	O
special	O
time-homogeneous	O
hidden	B
markov	O
model	B
is	O
given	O
by	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xt	O
,	O
y1	O
,	O
.	O
.	O
.	O
,	O
yt	O
,	O
h1	O
,	O
.	O
.	O
.	O
,	O
ht	O
)	O
=	O
p	O
(	O
x1|h1	O
)	O
p	O
(	O
y1|h1	O
)	O
p	O
(	O
h1	O
)	O
p	O
(	O
ht|ht−1	O
)	O
p	O
(	O
xt|ht	O
)	O
p	O
(	O
yt|ht	O
)	O
(	O
5.7.4	O
)	O
the	O
variable	B
xt	O
has	O
4	O
states	O
,	O
dom	O
(	O
xt	O
)	O
=	O
{	O
a	O
,	O
c	O
,	O
g	O
,	O
t	O
}	O
(	O
numerically	O
labelled	B
as	O
states	O
1,2,3,4	O
)	O
.	O
the	O
variable	B
yt	O
has	O
4	O
states	O
,	O
dom	O
(	O
yt	O
)	O
=	O
{	O
a	O
,	O
c	O
,	O
g	O
,	O
t	O
}	O
.	O
the	O
hidden	B
or	O
latent	B
variable	I
ht	O
has	O
5	O
states	O
,	O
dom	O
(	O
ht	O
)	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
5	O
}	O
.	O
the	O
hmm	O
models	O
the	O
following	O
(	O
ﬁctitious	O
)	O
process	O
:	O
in	O
humans	O
,	O
z-factor	O
proteins	O
are	O
a	O
sequence	O
on	O
states	O
of	O
the	O
variables	O
x1	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
xt	O
.	O
in	O
bananas	O
z-	O
factor	B
proteins	O
are	O
also	O
present	O
,	O
but	O
represented	O
by	O
a	O
diﬀerent	O
sequence	O
y1	O
,	O
y2	O
,	O
.	O
.	O
.	O
,	O
yt	O
.	O
given	O
a	O
sequence	O
x1	O
,	O
.	O
.	O
.	O
,	O
xt	O
from	O
a	O
human	O
,	O
the	O
task	O
is	O
to	O
ﬁnd	O
the	O
corresponding	O
sequence	O
y1	O
,	O
.	O
.	O
.	O
,	O
yt	O
in	O
the	O
banana	O
by	O
ﬁrst	O
ﬁnding	O
the	O
most	O
likely	O
joint	O
latent	B
sequence	O
,	O
and	O
then	O
the	O
most	O
likely	O
banana	O
sequence	O
given	O
this	O
optimal	O
latent	B
sequence	O
.	O
that	O
is	O
,	O
we	O
require	O
∗	O
t	O
)	O
∗	O
1	O
,	O
.	O
.	O
.	O
,	O
h	O
(	O
5.7.5	O
)	O
argmax	O
y1	O
,	O
...	O
,	O
yt	O
p	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yt|h	O
where	O
∗	O
1	O
,	O
.	O
.	O
.	O
,	O
h	O
h	O
∗	O
t	O
=	O
argmax	O
h1	O
,	O
...	O
,	O
ht	O
p	O
(	O
h1	O
,	O
.	O
.	O
.	O
,	O
ht|x1	O
,	O
.	O
.	O
.	O
,	O
xt	O
)	O
(	O
5.7.6	O
)	O
the	O
ﬁle	O
banana.mat	O
contains	O
the	O
emission	O
distributions	O
pxgh	O
(	O
p	O
(	O
x|h	O
)	O
)	O
,	O
pygh	O
(	O
p	O
(	O
y|h	O
)	O
)	O
and	O
transition	O
phtghtm	O
(	O
p	O
(	O
ht|ht−1	O
)	O
)	O
.	O
the	O
initial	O
hidden	B
distribution	O
is	O
given	O
in	O
ph1	O
(	O
p	O
(	O
h1	O
)	O
)	O
.	O
the	O
observed	O
x	O
sequence	O
is	O
given	O
in	O
x	O
.	O
1.	O
explain	O
mathematically	O
and	O
in	O
detail	O
how	O
to	O
compute	O
the	O
optimal	O
y-sequence	O
,	O
using	O
the	O
two-stage	O
procedure	O
as	O
stated	O
above	O
.	O
2.	O
write	O
a	O
matlab	O
routine	O
that	O
computes	O
and	O
displays	O
the	O
optimal	O
y-sequence	O
,	O
given	O
the	O
observed	O
x-sequence	O
.	O
your	O
routine	O
must	O
make	O
use	O
of	O
the	O
factor	B
graph	I
formalism	O
.	O
3.	O
explain	O
whether	O
or	O
not	O
it	O
is	O
computationally	O
tractable	O
to	O
compute	O
arg	O
max	O
y1	O
,	O
...	O
,	O
yt	O
p	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yt|x1	O
,	O
.	O
.	O
.	O
,	O
xt	O
)	O
(	O
5.7.7	O
)	O
4.	O
bonus	O
question	O
:	O
by	O
considering	O
y1	O
,	O
.	O
.	O
.	O
,	O
yt	O
as	O
parameters	O
,	O
explain	O
how	O
the	O
em	O
algorithm	B
may	O
be	O
used	O
to	O
ﬁnd	O
most	O
likely	O
marginal	O
states	O
.	O
implement	O
this	O
approach	B
with	O
a	O
suitable	O
initialisation	O
for	O
the	O
optimal	O
parameters	O
y1	O
,	O
.	O
.	O
.	O
,	O
yt	O
.	O
draft	O
march	O
9	O
,	O
2010	O
83	O
exercises	O
84	O
draft	O
march	O
9	O
,	O
2010	O
chapter	O
6	O
the	O
junction	B
tree	I
algorithm	O
6.1	O
clustering	B
variables	O
in	O
chapter	O
(	O
5	O
)	O
we	O
discussed	O
eﬃcient	B
inference	O
for	O
singly-connected	B
graphs	O
,	O
for	O
which	O
variable	B
elimination	I
and	O
message	B
passing	I
schemes	O
are	O
appropriate	O
.	O
in	O
the	O
multiply-connected	B
case	O
,	O
however	O
,	O
one	O
can	O
not	O
in	O
general	O
perform	O
inference	B
by	O
passing	B
messages	O
only	O
along	O
existing	O
links	O
in	O
the	O
graph	B
.	O
the	O
idea	O
behind	O
the	O
junction	B
tree	I
algorithm	O
is	O
to	O
form	O
a	O
new	O
representation	B
of	O
the	O
graph	B
in	O
which	O
variables	O
are	O
clustered	O
together	O
,	O
resulting	O
in	O
a	O
singly-connected	B
graph	O
in	O
the	O
cluster	O
variables	O
(	O
albeit	O
on	O
a	O
diﬀerent	O
graph	B
)	O
.	O
the	O
main	O
focus	O
of	O
the	O
development	O
will	O
be	O
on	O
marginal	B
inference	O
,	O
though	O
similar	O
techniques	O
apply	O
to	O
diﬀerence	O
inferences	O
,	O
such	O
as	O
ﬁnding	O
the	O
maximal	O
state	O
of	O
the	O
distribution	B
.	O
at	O
this	O
stage	O
it	O
is	O
important	O
to	O
point	O
out	O
that	O
the	O
junction	B
tree	I
algorithm	O
is	O
not	O
a	O
magic	O
method	O
to	O
deal	O
with	O
intractabilities	O
resulting	O
from	O
multiply	O
connected	B
graphs	O
;	O
it	O
is	O
simply	O
a	O
way	O
to	O
perform	O
correct	O
inference	B
on	O
a	O
multiply	O
connected	B
graph	I
by	O
transforming	O
to	O
a	O
singly	O
connected	B
structure	O
.	O
carrying	O
out	O
the	O
inference	B
on	O
the	O
resulting	O
junction	B
tree	I
may	O
still	O
be	O
computationally	O
intractable	O
.	O
for	O
example	O
,	O
the	O
junction	B
tree	I
representation	O
of	O
a	O
general	O
two-dimensional	O
ising	O
model	B
is	O
a	O
single	O
supernode	O
containing	O
all	O
the	O
variables	O
.	O
inference	B
in	O
this	O
case	O
is	O
exponentially	O
complex	O
in	O
the	O
number	O
of	O
variables	O
.	O
nevertheless	O
,	O
even	O
in	O
cases	O
where	O
implementing	O
the	O
jta	O
(	O
or	O
any	O
other	O
exact	O
inference	O
algorithm	B
)	O
may	O
be	O
intractable	O
,	O
the	O
jta	O
provides	O
useful	O
insight	O
into	O
the	O
representation	B
of	O
distributions	O
that	O
can	O
form	O
the	O
basis	O
for	O
approximate	B
inference	I
.	O
in	O
this	O
sense	O
the	O
jta	O
is	O
key	O
to	O
understanding	O
issues	O
related	O
to	O
representations	O
and	O
complexity	O
of	O
inference	B
and	O
is	O
central	O
to	O
the	O
development	O
of	O
eﬃcient	B
inference	O
algorithms	O
.	O
6.1.1	O
reparameterisation	B
consider	O
the	O
chain	B
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
)	O
=	O
p	O
(	O
a|b	O
)	O
p	O
(	O
b|c	O
)	O
p	O
(	O
c|d	O
)	O
p	O
(	O
d	O
)	O
using	O
bayes	O
’	O
rule	O
,	O
we	O
can	O
reexpress	O
this	O
as	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
)	O
=	O
p	O
(	O
a	O
,	O
b	O
)	O
p	O
(	O
b	O
)	O
p	O
(	O
b	O
,	O
c	O
)	O
p	O
(	O
c	O
)	O
p	O
(	O
c	O
,	O
d	O
)	O
p	O
(	O
d	O
)	O
p	O
(	O
d	O
)	O
=	O
p	O
(	O
a	O
,	O
b	O
)	O
p	O
(	O
b	O
,	O
c	O
)	O
p	O
(	O
c	O
,	O
d	O
)	O
p	O
(	O
b	O
)	O
p	O
(	O
c	O
)	O
(	O
6.1.1	O
)	O
(	O
6.1.2	O
)	O
a	O
useful	O
insight	O
is	O
that	O
the	O
distribution	B
can	O
therefore	O
be	O
written	O
as	O
a	O
product	O
of	O
marginal	B
distribu-	O
tions	O
,	O
divided	O
by	O
a	O
product	O
of	O
the	O
intersection	O
of	O
the	O
marginal	B
distributions	O
:	O
looking	O
at	O
the	O
numerator	O
p	O
(	O
a	O
,	O
b	O
)	O
p	O
(	O
b	O
,	O
c	O
)	O
p	O
(	O
c	O
,	O
d	O
)	O
this	O
can	O
not	O
be	O
a	O
distribution	B
over	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
since	O
we	O
are	O
overcounting	B
b	O
and	O
c	O
,	O
where	O
this	O
overcounting	B
of	O
b	O
arises	O
from	O
the	O
overlap	O
between	O
the	O
sets	O
a	O
,	O
b	O
and	O
b	O
,	O
c	O
,	O
which	O
have	O
b	O
as	O
their	O
intersection	O
.	O
similarly	O
,	O
the	O
overcounting	B
of	O
c	O
arises	O
from	O
the	O
overlap	O
between	O
the	O
sets	O
b	O
,	O
c	O
and	O
c	O
,	O
d.	O
roughly	O
speaking	O
we	O
need	O
to	O
correct	O
for	O
this	O
overcounting	B
by	O
dividing	O
by	O
the	O
distribution	B
on	O
the	O
intersections	O
.	O
given	O
the	O
transformed	O
representation	B
,	O
a	O
marginal	B
such	O
as	O
p	O
(	O
a	O
,	O
b	O
)	O
can	O
be	O
read	O
oﬀ	O
directly	O
from	O
the	O
factors	O
in	O
the	O
new	O
85	O
clique	B
graphs	O
a	O
b	O
c	O
(	O
a	O
)	O
d	O
expression	O
.	O
abc	O
bc	O
(	O
b	O
)	O
bcd	O
6.1	O
:	O
figure	O
φ	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
φ	O
(	O
b	O
,	O
c	O
,	O
d	O
)	O
.	O
graph	B
of	O
(	O
a	O
)	O
.	O
(	O
a	O
)	O
markov	O
network	O
(	O
b	O
)	O
equivalent	B
clique	O
the	O
aim	O
of	O
the	O
junction	B
tree	I
algorithm	O
is	O
to	O
form	O
a	O
representation	B
of	O
the	O
distribution	B
which	O
contains	O
the	O
marginals	O
explicitly	O
.	O
we	O
want	O
to	O
do	O
this	O
in	O
a	O
way	O
that	O
works	O
for	O
belief	O
and	O
markov	O
networks	O
,	O
and	O
also	O
deals	O
with	O
the	O
multiply-connected	B
case	O
.	O
in	O
order	O
to	O
do	O
so	O
,	O
an	O
appropriate	O
way	O
to	O
parameterise	O
the	O
distribution	B
is	O
in	O
terms	O
of	O
a	O
clique	B
graph	I
,	O
as	O
described	O
in	O
the	O
next	O
section	O
.	O
6.2	O
clique	B
graphs	O
deﬁnition	O
39	O
(	O
clique	B
graph	I
)	O
.	O
a	O
clique	B
graph	I
consists	O
of	O
a	O
set	O
of	O
potentials	O
,	O
φ1	O
(	O
x	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
φn	O
(	O
x	O
n	O
)	O
each	O
deﬁned	O
on	O
a	O
set	O
of	O
variables	O
x	O
i.	O
for	O
neighbouring	O
cliques	O
on	O
the	O
graph	B
,	O
deﬁned	O
on	O
sets	O
of	O
variables	O
x	O
i	O
and	O
x	O
j	O
,	O
the	O
intersection	O
x	O
s	O
=	O
x	O
i	O
∩	O
x	O
j	O
is	O
called	O
the	O
separator	B
and	O
has	O
a	O
corresponding	O
potential	B
φs	O
(	O
x	O
s	O
)	O
.	O
a	O
clique	B
graph	I
represents	O
the	O
function	B
(	O
6.2.1	O
)	O
(	O
cid:81	O
)	O
(	O
cid:81	O
)	O
c	O
φc	O
(	O
x	O
c	O
)	O
s	O
φs	O
(	O
x	O
s	O
)	O
for	O
notational	O
simplicity	O
we	O
will	O
usually	O
drop	O
the	O
clique	B
potential	O
index	O
c.	O
graphically	O
clique	B
potentials	O
are	O
represented	O
by	O
circles/ovals	O
,	O
and	O
separator	B
potentials	O
by	O
squares	O
.	O
x	O
1	O
x	O
1	O
∩	O
x	O
2	O
x	O
2	O
the	O
graph	B
on	O
the	O
left	O
represents	O
φ	O
(	O
x	O
1	O
)	O
φ	O
(	O
x	O
2	O
)	O
/φ	O
(	O
x	O
1	O
∩	O
x	O
2	O
)	O
.	O
clique	B
graphs	O
translate	O
markov	O
networks	O
into	O
structures	O
convenient	O
for	O
carrying	O
out	O
inference	B
.	O
consider	O
the	O
markov	O
network	O
in	O
ﬁg	O
(	O
6.1a	O
)	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
)	O
=	O
φ	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
φ	O
(	O
b	O
,	O
c	O
,	O
d	O
)	O
z	O
(	O
6.2.2	O
)	O
which	O
contains	O
two	O
clique	B
potentials	O
sharing	O
the	O
variables	O
b	O
,	O
c.	O
an	O
equivalent	B
representation	O
is	O
given	O
by	O
the	O
clique	B
graph	I
in	O
ﬁg	O
(	O
6.1b	O
)	O
,	O
deﬁned	O
as	O
the	O
product	O
of	O
the	O
numerator	O
clique	B
potentials	O
,	O
divided	O
by	O
the	O
product	O
of	O
the	O
separator	B
potentials	O
.	O
in	O
this	O
case	O
the	O
separator	B
potential	O
may	O
be	O
set	O
to	O
the	O
normalisation	B
constant	I
z.	O
by	O
summing	O
we	O
have	O
zp	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
=	O
φ	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
(	O
cid:88	O
)	O
(	O
cid:32	O
)	O
φ	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
(	O
cid:88	O
)	O
φ	O
(	O
b	O
,	O
c	O
,	O
d	O
)	O
,	O
z2p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
p	O
(	O
b	O
,	O
c	O
,	O
d	O
)	O
=	O
multiplying	O
the	O
two	O
expressions	O
,	O
we	O
have	O
d	O
zp	O
(	O
b	O
,	O
c	O
,	O
d	O
)	O
=	O
φ	O
(	O
b	O
,	O
c	O
,	O
d	O
)	O
(	O
cid:88	O
)	O
(	O
cid:33	O
)	O
(	O
cid:32	O
)	O
φ	O
(	O
b	O
,	O
c	O
,	O
d	O
)	O
(	O
cid:88	O
)	O
a	O
φ	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
φ	O
(	O
b	O
,	O
c	O
,	O
d	O
)	O
d	O
a	O
in	O
other	O
words	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
)	O
=	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
p	O
(	O
b	O
,	O
c	O
,	O
d	O
)	O
p	O
(	O
c	O
,	O
b	O
)	O
86	O
φ	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
(	O
6.2.3	O
)	O
(	O
cid:33	O
)	O
=	O
z2p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
)	O
(	O
cid:88	O
)	O
a	O
,	O
d	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
)	O
(	O
6.2.4	O
)	O
(	O
6.2.5	O
)	O
draft	O
march	O
9	O
,	O
2010	O
clique	B
graphs	O
the	O
important	O
observation	O
is	O
that	O
the	O
distribution	B
can	O
be	O
written	O
in	O
terms	O
of	O
its	O
marginals	O
on	O
the	O
variables	O
in	O
the	O
original	O
cliques	O
and	O
that	O
,	O
as	O
a	O
clique	B
graph	I
,	O
it	O
has	O
the	O
same	O
structure	B
as	O
before	O
.	O
all	O
that	O
has	O
changed	O
is	O
that	O
the	O
original	O
clique	B
potentials	O
have	O
been	O
replaced	O
by	O
the	O
marginals	O
of	O
the	O
distribution	B
and	O
the	O
separator	B
by	O
the	O
marginal	B
deﬁned	O
on	O
the	O
separator	B
variables	O
φ	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
→	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
,	O
φ	O
(	O
b	O
,	O
c	O
,	O
d	O
)	O
→	O
p	O
(	O
b	O
,	O
c	O
,	O
d	O
)	O
,	O
z	O
→	O
p	O
(	O
c	O
,	O
b	O
)	O
.	O
the	O
usefulness	O
of	O
this	O
representation	B
is	O
that	O
if	O
we	O
are	O
interested	O
in	O
the	O
marginal	B
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
,	O
this	O
can	O
be	O
read	O
oﬀ	O
from	O
the	O
transformed	O
clique	B
potential	O
.	O
to	O
make	O
use	O
of	O
this	O
representation	B
,	O
we	O
require	O
a	O
systematic	O
way	O
of	O
transforming	O
the	O
clique	B
graph	I
potentials	O
so	O
that	O
at	O
the	O
end	O
of	O
the	O
transformation	O
the	O
new	O
potentials	O
contain	O
the	O
marginals	O
of	O
the	O
distribution	B
.	O
remark	O
7.	O
note	O
that	O
,	O
whilst	O
visually	O
similar	O
,	O
a	O
factor	B
graph	I
and	O
a	O
clique	B
graph	I
are	O
diﬀerent	O
representations	O
.	O
in	O
a	O
clique	B
graph	I
the	O
nodes	O
contain	O
sets	O
of	O
variables	O
,	O
which	O
may	O
share	O
variables	O
with	O
other	O
nodes	O
.	O
6.2.1	O
absorption	B
consider	O
neighbouring	O
cliques	O
v	O
and	O
w	O
,	O
sharing	O
the	O
variables	O
s	O
in	O
common	O
.	O
in	O
this	O
case	O
,	O
the	O
distribution	B
on	O
the	O
variables	O
x	O
=	O
v	O
∪	O
w	O
is	O
p	O
(	O
x	O
)	O
=	O
ψ	O
(	O
v	O
)	O
ψ	O
(	O
w	O
)	O
ψ	O
(	O
s	O
)	O
and	O
our	O
aim	O
is	O
to	O
ﬁnd	O
a	O
new	O
representation	B
(	O
6.2.6	O
)	O
(	O
6.2.7	O
)	O
ψ	O
(	O
v	O
)	O
ψ	O
(	O
s	O
)	O
ψ	O
(	O
w	O
)	O
ψ∗	O
(	O
v	O
)	O
ψ∗	O
(	O
s	O
)	O
ψ∗	O
(	O
w	O
)	O
in	O
which	O
the	O
potentials	O
are	O
given	O
by	O
∗	O
(	O
w	O
)	O
=	O
p	O
(	O
w	O
)	O
,	O
ψ	O
∗	O
(	O
s	O
)	O
=	O
p	O
(	O
s	O
)	O
(	O
6.2.8	O
)	O
in	O
this	O
example	O
,	O
we	O
can	O
explicitly	O
work	O
out	O
the	O
new	O
potentials	O
as	O
function	O
of	O
the	O
old	O
potentials	O
by	O
computing	O
the	O
marginals	O
as	O
follows	O
:	O
(	O
cid:80	O
)	O
v\s	O
ψ	O
(	O
v	O
)	O
(	O
cid:80	O
)	O
w\s	O
ψ	O
(	O
w	O
)	O
ψ	O
(	O
s	O
)	O
ψ	O
(	O
s	O
)	O
ψ	O
(	O
v	O
)	O
ψ	O
(	O
w	O
)	O
ψ	O
(	O
s	O
)	O
=	O
ψ	O
(	O
w	O
)	O
ψ	O
(	O
v	O
)	O
ψ	O
(	O
w	O
)	O
ψ	O
(	O
s	O
)	O
=	O
ψ	O
(	O
v	O
)	O
p	O
(	O
x	O
)	O
=	O
ψ∗	O
(	O
v	O
)	O
ψ∗	O
(	O
w	O
)	O
ψ∗	O
(	O
s	O
)	O
∗	O
(	O
v	O
)	O
=	O
p	O
(	O
v	O
)	O
,	O
ψ	O
ψ	O
p	O
(	O
w	O
)	O
=	O
(	O
cid:88	O
)	O
p	O
(	O
v	O
)	O
=	O
(	O
cid:88	O
)	O
v\s	O
w\s	O
p	O
(	O
x	O
)	O
=	O
(	O
cid:88	O
)	O
p	O
(	O
x	O
)	O
=	O
(	O
cid:88	O
)	O
v\s	O
w\s	O
∗	O
(	O
s	O
)	O
=	O
(	O
cid:88	O
)	O
v\s	O
ψ	O
ψ	O
(	O
v	O
)	O
and	O
and	O
the	O
reﬁne	O
the	O
w	O
potential	B
using	O
ψ	O
∗	O
(	O
w	O
)	O
=	O
ψ	O
(	O
w	O
)	O
ψ∗	O
(	O
s	O
)	O
ψ	O
(	O
s	O
)	O
(	O
6.2.9	O
)	O
(	O
6.2.10	O
)	O
(	O
6.2.11	O
)	O
(	O
6.2.12	O
)	O
there	O
is	O
a	O
symmetry	O
present	O
in	O
the	O
two	O
equations	O
above	O
–	O
they	O
are	O
the	O
same	O
under	O
interchanging	O
v	O
and	O
w.	O
one	O
way	O
to	O
describe	O
these	O
equations	O
is	O
through	O
‘	O
absorption	B
’	O
.	O
we	O
say	O
that	O
the	O
cluster	O
w	O
‘	O
absorbs	O
’	O
information	O
from	O
cluster	O
v	O
by	O
the	O
following	O
updating	O
procedure	O
.	O
first	O
we	O
deﬁne	O
a	O
new	O
separator	B
the	O
advantage	O
of	O
this	O
interpretation	O
is	O
that	O
the	O
new	O
representation	B
is	O
still	O
a	O
valid	O
clique	B
graph	I
represen-	O
tation	O
of	O
the	O
distribution	B
since	O
ψ	O
(	O
v	O
)	O
ψ∗	O
(	O
w	O
)	O
ψ∗	O
(	O
s	O
)	O
=	O
ψ	O
(	O
v	O
)	O
ψ	O
(	O
w	O
)	O
ψ∗	O
(	O
s	O
)	O
ψ	O
(	O
s	O
)	O
ψ∗	O
(	O
s	O
)	O
draft	O
march	O
9	O
,	O
2010	O
=	O
ψ	O
(	O
v	O
)	O
ψ	O
(	O
w	O
)	O
ψ	O
(	O
s	O
)	O
=	O
p	O
(	O
x	O
)	O
(	O
6.2.13	O
)	O
87	O
a	O
3	O
4	O
→	O
←	O
c	O
6	O
←	O
1	O
→	O
2	O
→	O
←	O
1	O
0	O
e	O
d	O
8	O
9	O
→←	O
junction	O
trees	O
figure	O
6.2	O
:	O
an	O
example	O
absorption	B
schedule	O
on	O
a	O
clique	B
tree	O
.	O
many	O
valid	O
schedules	O
exist	O
under	O
the	O
con-	O
straint	O
that	O
messages	O
can	O
only	O
be	O
passed	O
to	O
a	O
neigh-	O
bour	O
when	O
all	O
other	O
messages	O
have	O
been	O
received	O
.	O
b	O
7	O
→	O
←	O
5	O
f	O
after	O
w	O
absorbs	O
information	O
from	O
v	O
then	O
ψ∗	O
(	O
w	O
)	O
contains	O
the	O
marginal	B
p	O
(	O
w	O
)	O
.	O
similarly	O
,	O
after	O
v	O
absorbs	O
information	O
from	O
w	O
then	O
ψ∗	O
(	O
v	O
)	O
contains	O
the	O
marginal	B
p	O
(	O
v	O
)	O
.	O
after	O
the	O
separator	B
s	O
has	O
participated	O
in	O
absorption	B
along	O
both	O
directions	O
,	O
then	O
the	O
separator	B
potential	O
will	O
contain	O
p	O
(	O
s	O
)	O
(	O
this	O
is	O
not	O
the	O
case	O
after	O
only	O
a	O
single	O
absorption	B
)	O
.	O
to	O
see	O
this	O
,	O
consider	O
ψ	O
∗∗	O
(	O
s	O
)	O
=	O
(	O
cid:88	O
)	O
∗	O
(	O
w	O
)	O
=	O
(	O
cid:88	O
)	O
=	O
(	O
cid:88	O
)	O
continuing	O
,	O
we	O
have	O
the	O
new	O
potential	B
ψ∗	O
(	O
v	O
)	O
given	O
by	O
ψ	O
(	O
w	O
)	O
ψ∗	O
(	O
s	O
)	O
ψ	O
(	O
v	O
)	O
(	O
cid:80	O
)	O
w\s	O
ψ	O
(	O
w	O
)	O
ψ∗	O
(	O
s	O
)	O
/ψ	O
(	O
s	O
)	O
{	O
w∪v	O
}	O
\s	O
ψ	O
(	O
s	O
)	O
w\s	O
ψ	O
w\s	O
ψ	O
∗	O
(	O
v	O
)	O
=	O
ψ	O
(	O
v	O
)	O
ψ∗∗	O
(	O
s	O
)	O
ψ∗	O
(	O
s	O
)	O
=	O
ψ∗	O
(	O
s	O
)	O
ψ	O
(	O
w	O
)	O
ψ	O
(	O
v	O
)	O
ψ	O
(	O
s	O
)	O
=	O
p	O
(	O
s	O
)	O
(	O
6.2.14	O
)	O
(	O
cid:80	O
)	O
w\s	O
ψ	O
(	O
v	O
)	O
ψ	O
(	O
w	O
)	O
=	O
ψ	O
(	O
s	O
)	O
=	O
p	O
(	O
v	O
)	O
(	O
6.2.15	O
)	O
deﬁnition	O
40	O
(	O
absorption	B
)	O
.	O
ψ	O
(	O
v	O
)	O
ψ∗	O
(	O
s	O
)	O
ψ∗	O
(	O
w	O
)	O
let	O
v	O
and	O
w	O
be	O
neighbours	O
in	O
a	O
clique	B
graph	I
,	O
let	O
s	O
be	O
their	O
separator	B
,	O
and	O
let	O
ψ	O
(	O
v	O
)	O
,	O
ψ	O
(	O
w	O
)	O
and	O
ψ	O
(	O
s	O
)	O
be	O
their	O
potentials	O
.	O
absorption	B
from	O
v	O
to	O
w	O
through	O
s	O
replaces	O
the	O
tables	O
ψ∗	O
(	O
s	O
)	O
and	O
ψ∗	O
(	O
w	O
)	O
with	O
ψ	O
(	O
v	O
)	O
ψ	O
∗	O
(	O
w	O
)	O
=	O
ψ	O
(	O
w	O
)	O
ψ∗	O
(	O
s	O
)	O
ψ	O
(	O
s	O
)	O
(	O
6.2.16	O
)	O
∗	O
(	O
s	O
)	O
=	O
(	O
cid:88	O
)	O
v\s	O
ψ	O
we	O
say	O
that	O
clique	B
w	O
absorbs	O
information	O
from	O
clique	B
v.	O
6.2.2	O
absorption	B
schedule	O
on	O
clique	B
trees	O
having	O
deﬁned	O
the	O
local	B
message	O
propagation	B
approach	O
,	O
we	O
need	O
to	O
deﬁne	O
an	O
update	O
ordering	O
for	O
absorp-	O
tion	O
.	O
in	O
general	O
,	O
a	O
node	B
v	O
can	O
send	O
exactly	O
one	O
message	B
to	O
a	O
neighbour	B
w	O
,	O
and	O
it	O
may	O
only	O
be	O
sent	O
when	O
v	O
has	O
received	O
a	O
message	B
from	O
each	O
of	O
its	O
other	O
neighbours	O
.	O
we	O
continue	O
this	O
sequence	O
of	O
absorptions	O
until	O
a	O
message	B
has	O
been	O
passed	O
in	O
both	O
directions	O
along	O
every	O
link	O
.	O
see	O
,	O
for	O
example	O
,	O
ﬁg	O
(	O
6.2	O
)	O
.	O
note	O
that	O
the	O
message	B
passing	I
scheme	O
is	O
not	O
unique	O
.	O
deﬁnition	O
41	O
(	O
absorption	B
schedule	O
)	O
.	O
a	O
clique	B
can	O
send	O
a	O
message	B
to	O
a	O
neighbour	B
,	O
provided	O
it	O
has	O
already	O
received	O
messages	O
from	O
all	O
other	O
neighbours	O
.	O
6.3	O
junction	O
trees	O
there	O
are	O
a	O
few	O
stages	O
we	O
need	O
to	O
go	O
through	O
in	O
order	O
to	O
transform	O
a	O
distribution	B
into	O
an	O
appropriate	O
structure	B
for	O
inference	B
.	O
initially	O
we	O
explain	O
how	O
to	O
do	O
this	O
for	O
singly-connected	B
structures	O
before	O
moving	O
88	O
draft	O
march	O
9	O
,	O
2010	O
junction	O
trees	O
x1	O
x4	O
x1	O
,	O
x4	O
x1	O
,	O
x4	O
x4	O
x4	O
x4	O
x4	O
x3	O
x2	O
x3	O
,	O
x4	O
(	O
a	O
)	O
x4	O
(	O
b	O
)	O
x2	O
,	O
x4	O
x3	O
,	O
x4	O
x2	O
,	O
x4	O
(	O
c	O
)	O
figure	O
6.3	O
:	O
(	O
a	O
)	O
:	O
singly-connected	B
markov	O
network	O
.	O
(	O
b	O
)	O
:	O
clique	B
graph	I
.	O
(	O
c	O
)	O
:	O
clique	B
tree	O
.	O
onto	O
the	O
multiply-connected	B
case	O
.	O
consider	O
the	O
singly-connected	B
markov	O
network	O
,	O
ﬁg	O
(	O
6.3a	O
)	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
=	O
φ	O
(	O
x1	O
,	O
x4	O
)	O
φ	O
(	O
x2	O
,	O
x4	O
)	O
φ	O
(	O
x3	O
,	O
x4	O
)	O
(	O
6.3.1	O
)	O
the	O
clique	B
graph	I
of	O
this	O
singly-connected	B
markov	O
network	O
is	O
multiply-connected	B
,	O
ﬁg	O
(	O
6.3b	O
)	O
,	O
where	O
the	O
separator	B
potentials	O
are	O
all	O
set	O
to	O
unity	O
.	O
nevertheless	O
,	O
let	O
’	O
s	O
try	O
to	O
reexpress	O
the	O
markov	O
network	O
in	O
terms	O
of	O
marginals	O
.	O
first	O
we	O
have	O
the	O
relations	O
p	O
(	O
x1	O
,	O
x4	O
)	O
=	O
(	O
cid:88	O
)	O
p	O
(	O
x2	O
,	O
x4	O
)	O
=	O
(	O
cid:88	O
)	O
p	O
(	O
x3	O
,	O
x4	O
)	O
=	O
(	O
cid:88	O
)	O
x1	O
,	O
x3	O
x2	O
,	O
x3	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
=	O
φ	O
(	O
x1	O
,	O
x4	O
)	O
(	O
cid:88	O
)	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
=	O
φ	O
(	O
x2	O
,	O
x4	O
)	O
(	O
cid:88	O
)	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
=	O
φ	O
(	O
x3	O
,	O
x4	O
)	O
(	O
cid:88	O
)	O
x2	O
x1	O
x1	O
,	O
x2	O
x1	O
taking	O
the	O
product	O
of	O
the	O
three	O
marginals	O
,	O
we	O
have	O
p	O
(	O
x1	O
,	O
x4	O
)	O
p	O
(	O
x2	O
,	O
x4	O
)	O
p	O
(	O
x3	O
,	O
x4	O
)	O
=	O
φ	O
(	O
x1	O
,	O
x4	O
)	O
φ	O
(	O
x2	O
,	O
x4	O
)	O
φ	O
(	O
x3	O
,	O
x4	O
)	O
φ	O
(	O
x3	O
,	O
x4	O
)	O
φ	O
(	O
x3	O
,	O
x4	O
)	O
φ	O
(	O
x2	O
,	O
x4	O
)	O
x3	O
x3	O
φ	O
(	O
x2	O
,	O
x4	O
)	O
(	O
cid:88	O
)	O
φ	O
(	O
x1	O
,	O
x4	O
)	O
(	O
cid:88	O
)	O
φ	O
(	O
x1	O
,	O
x4	O
)	O
(	O
cid:88	O
)	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
x1	O
x2	O
φ	O
(	O
x1	O
,	O
x4	O
)	O
(	O
cid:88	O
)	O
x2	O
φ	O
(	O
x2	O
,	O
x4	O
)	O
(	O
cid:88	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
x3	O
p	O
(	O
x4	O
)	O
2	O
(	O
6.3.2	O
)	O
(	O
6.3.3	O
)	O
(	O
6.3.4	O
)	O
(	O
cid:33	O
)	O
2	O
(	O
cid:125	O
)	O
φ	O
(	O
x3	O
,	O
x4	O
)	O
(	O
6.3.5	O
)	O
(	O
6.3.6	O
)	O
this	O
means	O
that	O
the	O
markov	O
network	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
marginals	O
as	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
=	O
p	O
(	O
x1	O
,	O
x4	O
)	O
p	O
(	O
x2	O
,	O
x4	O
)	O
p	O
(	O
x3	O
,	O
x4	O
)	O
p	O
(	O
x4	O
)	O
p	O
(	O
x4	O
)	O
hence	O
a	O
valid	O
clique	B
graph	I
is	O
also	O
given	O
by	O
the	O
representation	B
ﬁg	O
(	O
6.3c	O
)	O
.	O
indeed	O
,	O
if	O
a	O
variable	B
(	O
here	O
x4	O
)	O
occurs	O
on	O
every	O
separator	B
in	O
a	O
clique	B
graph	I
loop	O
,	O
one	O
can	O
remove	O
that	O
variable	B
from	O
an	O
arbitrarily	O
chosen	O
separator	B
in	O
the	O
loop	O
.	O
this	O
leaves	O
an	O
empty	O
separator	B
,	O
which	O
we	O
can	O
simply	O
remove	O
.	O
this	O
shows	O
that	O
in	O
such	O
cases	O
we	O
can	O
transform	O
the	O
clique	B
graph	I
into	O
a	O
clique	B
tree	O
(	O
i.e	O
.	O
a	O
singly-connected	B
clique	O
graph	B
)	O
.	O
provided	O
that	O
the	O
original	O
markov	O
network	O
is	O
singly-connected	B
,	O
one	O
can	O
always	O
form	O
a	O
clique	B
tree	O
in	O
this	O
manner	O
.	O
6.3.1	O
the	O
running	B
intersection	I
property	I
sticking	O
with	O
the	O
above	O
example	O
,	O
consider	O
the	O
clique	B
tree	O
in	O
ﬁg	O
(	O
6.3	O
)	O
φ	O
(	O
x3	O
,	O
x4	O
)	O
φ	O
(	O
x1	O
,	O
x4	O
)	O
φ	O
(	O
x2	O
,	O
x4	O
)	O
φ1	O
(	O
x4	O
)	O
φ2	O
(	O
x4	O
)	O
draft	O
march	O
9	O
,	O
2010	O
(	O
6.3.7	O
)	O
89	O
as	O
a	O
representation	B
of	O
the	O
distribution	B
(	O
6.3.1	O
)	O
where	O
we	O
set	O
φ1	O
(	O
x4	O
)	O
=	O
φ2	O
(	O
x4	O
)	O
=	O
1	O
to	O
make	O
this	O
match	O
.	O
now	O
perform	O
absorption	B
on	O
this	O
clique	B
tree	O
:	O
we	O
absorb	O
(	O
x3	O
,	O
x4	O
)	O
(	O
cid:32	O
)	O
(	O
x1	O
,	O
x4	O
)	O
.	O
the	O
new	O
separator	B
is	O
junction	O
trees	O
1	O
(	O
x4	O
)	O
=	O
(	O
cid:88	O
)	O
∗	O
φ	O
φ	O
(	O
x3	O
,	O
x4	O
)	O
x3	O
and	O
the	O
new	O
potential	B
is	O
∗	O
(	O
x1	O
,	O
x4	O
)	O
=	O
φ	O
(	O
x1	O
,	O
x4	O
)	O
φ∗	O
2	O
(	O
x4	O
)	O
=	O
(	O
cid:88	O
)	O
∗	O
(	O
x1	O
,	O
x4	O
)	O
φ	O
∗	O
φ	O
φ	O
1	O
(	O
x4	O
)	O
φ1	O
(	O
x4	O
)	O
=	O
φ	O
(	O
x1	O
,	O
x4	O
)	O
φ	O
∗	O
1	O
(	O
x4	O
)	O
now	O
(	O
x1	O
,	O
x4	O
)	O
(	O
cid:32	O
)	O
(	O
x2	O
,	O
x4	O
)	O
.	O
the	O
new	O
separator	B
is	O
x1	O
and	O
the	O
new	O
potential	B
is	O
∗	O
(	O
x2	O
,	O
x4	O
)	O
=	O
φ	O
(	O
x2	O
,	O
x4	O
)	O
φ∗	O
2	O
(	O
x4	O
)	O
φ2	O
(	O
x4	O
)	O
φ	O
=	O
φ	O
(	O
x2	O
,	O
x4	O
)	O
φ	O
∗	O
2	O
(	O
x4	O
)	O
since	O
we	O
’	O
ve	O
‘	O
hit	O
the	O
buﬀers	O
’	O
in	O
terms	O
of	O
message	B
passing	I
,	O
the	O
potential	B
φ	O
(	O
x2	O
,	O
x4	O
)	O
can	O
not	O
be	O
updated	O
further	O
.	O
let	O
’	O
s	O
examine	O
more	O
carefully	O
the	O
value	B
of	O
this	O
new	O
potential	B
,	O
∗	O
(	O
x1	O
,	O
x4	O
)	O
∗	O
(	O
x2	O
,	O
x4	O
)	O
=	O
φ	O
(	O
x2	O
,	O
x4	O
)	O
φ	O
(	O
6.3.12	O
)	O
φ	O
φ	O
∗	O
2	O
(	O
x4	O
)	O
=	O
φ	O
(	O
x2	O
,	O
x4	O
)	O
(	O
cid:88	O
)	O
φ	O
(	O
x1	O
,	O
x4	O
)	O
(	O
cid:88	O
)	O
x1	O
φ	O
(	O
x3	O
,	O
x4	O
)	O
=	O
(	O
cid:88	O
)	O
=	O
φ	O
(	O
x2	O
,	O
x4	O
)	O
(	O
cid:88	O
)	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
=	O
p	O
(	O
x2	O
,	O
x4	O
)	O
(	O
6.3.13	O
)	O
hence	O
the	O
new	O
potential	B
φ∗	O
(	O
x2	O
,	O
x4	O
)	O
contains	O
the	O
marginal	B
p	O
(	O
x2	O
,	O
x4	O
)	O
.	O
x1	O
x3	O
x1	O
,	O
x3	O
to	O
complete	O
a	O
full	O
round	O
of	O
message	B
passing	I
we	O
need	O
to	O
have	O
passed	O
messages	O
in	O
a	O
valid	O
schedule	B
along	O
both	O
directions	O
of	O
each	O
separator	B
.	O
to	O
do	O
so	O
,	O
we	O
continue	O
as	O
follows	O
:	O
we	O
absorb	O
(	O
x2	O
,	O
x4	O
)	O
(	O
cid:32	O
)	O
(	O
x1	O
,	O
x4	O
)	O
.	O
the	O
new	O
separator	B
is	O
(	O
6.3.8	O
)	O
(	O
6.3.9	O
)	O
(	O
6.3.10	O
)	O
(	O
6.3.11	O
)	O
(	O
6.3.14	O
)	O
(	O
6.3.15	O
)	O
(	O
6.3.16	O
)	O
(	O
6.3.17	O
)	O
2	O
(	O
x4	O
)	O
=	O
(	O
cid:88	O
)	O
∗∗	O
x2	O
∗	O
(	O
x2	O
,	O
x4	O
)	O
φ	O
∗∗	O
(	O
x1	O
,	O
x4	O
)	O
=	O
φ	O
2	O
(	O
x4	O
)	O
=	O
(	O
cid:80	O
)	O
1	O
(	O
x4	O
)	O
=	O
(	O
cid:88	O
)	O
∗∗	O
φ	O
x1	O
∗	O
(	O
x1	O
,	O
x4	O
)	O
φ∗∗	O
2	O
(	O
x4	O
)	O
x2	O
φ∗	O
(	O
x2	O
,	O
x4	O
)	O
=	O
(	O
cid:80	O
)	O
φ∗	O
2	O
(	O
x4	O
)	O
∗∗	O
(	O
x1	O
,	O
x4	O
)	O
=	O
p	O
(	O
x4	O
)	O
and	O
φ	O
φ	O
φ	O
and	O
note	O
that	O
φ∗∗	O
x2	O
p	O
(	O
x2	O
,	O
x4	O
)	O
=	O
p	O
(	O
x4	O
)	O
so	O
that	O
now	O
,	O
after	O
absorbing	O
through	O
both	O
directions	O
,	O
the	O
separator	B
contains	O
the	O
marginal	B
p	O
(	O
x4	O
)	O
.	O
the	O
reader	O
may	O
show	O
that	O
φ∗∗	O
(	O
x1	O
,	O
x4	O
)	O
=	O
p	O
(	O
x1	O
,	O
x4	O
)	O
.	O
finally	O
,	O
we	O
absorb	O
(	O
x1	O
,	O
x4	O
)	O
(	O
cid:32	O
)	O
(	O
x3	O
,	O
x4	O
)	O
.	O
the	O
new	O
separator	B
is	O
∗	O
(	O
x3	O
,	O
x4	O
)	O
=	O
φ	O
(	O
x3	O
,	O
x4	O
)	O
φ∗∗	O
1	O
(	O
x4	O
)	O
φ∗	O
1	O
(	O
x4	O
)	O
φ	O
=	O
p	O
(	O
x3	O
,	O
x4	O
)	O
hence	O
,	O
after	O
a	O
full	O
round	O
of	O
message	B
passing	I
,	O
the	O
new	O
potentials	O
all	O
contain	O
the	O
correct	O
marginals	O
.	O
90	O
draft	O
march	O
9	O
,	O
2010	O
junction	O
trees	O
(	O
cid:88	O
)	O
v\i	O
ψ	O
(	O
v	O
)	O
=	O
(	O
cid:88	O
)	O
w\i	O
the	O
new	O
representation	B
is	O
consistent	B
in	O
the	O
sense	O
that	O
for	O
any	O
(	O
not	O
necessarily	O
neighbouring	O
)	O
cliques	O
v	O
and	O
w	O
with	O
intersection	O
i	O
,	O
and	O
corresponding	O
potentials	O
ψ	O
(	O
v	O
)	O
and	O
ψ	O
(	O
w	O
)	O
,	O
ψ	O
(	O
w	O
)	O
(	O
6.3.18	O
)	O
note	O
that	O
bidirectional	O
absorption	B
guarantees	O
consistency	O
for	O
neighbouring	O
cliques	O
,	O
as	O
in	O
the	O
example	O
above	O
,	O
provided	O
that	O
we	O
started	O
with	O
a	O
clique	B
tree	O
which	O
is	O
a	O
correct	O
representation	B
of	O
the	O
distribution	B
.	O
in	O
general	O
,	O
the	O
only	O
possible	O
source	O
of	O
non-consistency	O
is	O
if	O
a	O
variable	B
occurs	O
in	O
two	O
non-neighbouring	O
cliques	O
and	O
is	O
not	O
present	O
in	O
all	O
cliques	O
on	O
any	O
path	B
connection	O
them	O
.	O
an	O
extreme	O
example	O
would	O
be	O
if	O
we	O
removed	O
the	O
link	O
between	O
cliques	O
(	O
x3	O
,	O
x4	O
)	O
and	O
(	O
x1	O
,	O
x4	O
)	O
.	O
in	O
this	O
case	O
this	O
is	O
still	O
a	O
clique	B
tree	O
;	O
however	O
global	B
consistency	O
could	O
not	O
be	O
guaranteed	O
since	O
the	O
information	O
required	O
to	O
make	O
clique	B
(	O
x3	O
,	O
x4	O
)	O
consis-	O
tent	O
with	O
the	O
rest	O
of	O
the	O
graph	B
can	O
not	O
reach	O
this	O
clique	B
.	O
formally	O
,	O
the	O
requirement	O
for	O
the	O
propagation	B
of	O
local	B
to	O
global	B
consistency	O
is	O
that	O
the	O
clique	B
tree	O
is	O
a	O
junction	B
tree	I
,	O
as	O
deﬁned	O
below	O
.	O
deﬁnition	O
42	O
(	O
junction	B
tree	I
)	O
.	O
a	O
clique	B
tree	O
is	O
a	O
junction	B
tree	I
if	O
,	O
for	O
each	O
pair	O
of	O
nodes	O
,	O
v	O
and	O
w	O
,	O
all	O
nodes	O
on	O
the	O
path	B
between	O
v	O
and	O
w	O
contain	O
the	O
intersection	O
v	O
∩	O
w.	O
this	O
is	O
also	O
called	O
the	O
running	B
intersection	I
property	I
.	O
from	O
this	O
deﬁnition	O
local	B
consistency	O
will	O
be	O
passed	O
on	O
to	O
any	O
neighbours	O
and	O
the	O
distribution	B
will	O
be	O
globally	O
consistent	B
.	O
proofs	O
for	O
these	O
results	O
are	O
contained	O
in	O
[	O
148	O
]	O
.	O
example	O
22	O
(	O
a	O
consistent	B
junction	O
tree	B
)	O
.	O
to	O
gain	O
some	O
intuition	O
about	O
the	O
meaning	O
of	O
consistency	O
,	O
consider	O
the	O
junction	B
tree	I
in	O
ﬁg	O
(	O
6.4d	O
)	O
.	O
after	O
a	O
full	O
round	O
of	O
message	B
passing	I
on	O
this	O
tree	B
,	O
each	O
link	O
is	O
consistent	B
,	O
and	O
the	O
product	O
of	O
the	O
potentials	O
divided	O
by	O
the	O
product	O
of	O
the	O
separator	B
potentials	O
is	O
just	O
the	O
original	O
distribution	B
itself	O
.	O
imagine	O
that	O
we	O
are	O
interested	O
in	O
calculating	O
the	O
marginal	B
for	O
the	O
node	B
abc	O
.	O
that	O
requires	O
summing	O
over	O
all	O
the	O
other	O
variables	O
,	O
def	O
gh	O
.	O
if	O
we	O
consider	O
summing	O
over	O
h	O
then	O
,	O
because	O
the	O
link	O
is	O
consistent	B
,	O
∗	O
(	O
e	O
,	O
h	O
)	O
=	O
ψ	O
(	O
cid:88	O
)	O
(	O
6.3.19	O
)	O
∗	O
(	O
e	O
)	O
ψ	O
h	O
so	O
that	O
the	O
ratio	O
(	O
cid:80	O
)	O
ψ∗	O
(	O
e	O
,	O
h	O
)	O
ψ∗	O
(	O
e	O
)	O
(	O
cid:88	O
)	O
∗	O
(	O
d	O
,	O
c	O
,	O
e	O
)	O
=	O
ψ	O
∗	O
(	O
c	O
)	O
ψ	O
h	O
is	O
unity	O
,	O
and	O
the	O
eﬀect	O
of	O
summing	O
over	O
node	B
h	O
is	O
that	O
the	O
link	O
between	O
eh	O
and	O
dce	O
can	O
be	O
removed	O
,	O
along	O
with	O
the	O
separator	B
.	O
the	O
same	O
happens	O
for	O
the	O
link	O
between	O
node	B
eg	O
and	O
dce	O
,	O
and	O
also	O
for	O
cf	O
to	O
abc	O
.	O
the	O
only	O
nodes	O
remaining	O
are	O
now	O
dce	O
and	O
abc	O
and	O
their	O
separator	B
c	O
,	O
which	O
have	O
so	O
far	O
been	O
unaﬀected	O
by	O
the	O
summations	O
.	O
we	O
still	O
need	O
to	O
sum	O
out	O
over	O
d	O
and	O
e.	O
again	O
,	O
because	O
the	O
link	O
is	O
consistent	B
,	O
de	O
so	O
that	O
the	O
ratio	O
(	O
cid:80	O
)	O
summation	O
over	O
the	O
other	O
variables	O
in	O
that	O
potential	B
,	O
for	O
example	O
p	O
(	O
f	O
)	O
=	O
(	O
cid:80	O
)	O
ψ∗	O
(	O
d	O
,	O
c	O
,	O
e	O
)	O
ψ∗	O
(	O
c	O
)	O
=	O
1.	O
the	O
result	O
of	O
the	O
summation	O
of	O
all	O
variables	O
not	O
in	O
abc	O
therefore	O
produces	O
unity	O
for	O
the	O
cliques	O
and	O
their	O
separators	O
,	O
and	O
the	O
summed	O
potential	B
representation	O
reduces	O
simply	O
to	O
the	O
potential	B
ψ∗	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
which	O
is	O
the	O
marginal	B
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
.	O
it	O
is	O
clear	O
that	O
a	O
similar	O
eﬀect	O
will	O
happen	O
for	O
other	O
nodes	O
.	O
we	O
can	O
then	O
obtain	O
the	O
marginals	O
for	O
individual	O
variables	O
by	O
simple	O
brute	O
force	O
de	O
c	O
ψ∗	O
(	O
c	O
,	O
f	O
)	O
.	O
(	O
6.3.20	O
)	O
draft	O
march	O
9	O
,	O
2010	O
91	O
constructing	O
a	O
junction	B
tree	I
for	O
singly-connected	B
distributions	O
6.4	O
constructing	O
a	O
junction	B
tree	I
for	O
singly-connected	B
distributions	O
6.4.1	O
moralisation	B
for	O
belief	B
networks	I
,	O
an	O
initial	O
step	O
is	O
required	O
,	O
which	O
is	O
not	O
required	O
in	O
the	O
case	O
of	O
undirected	B
graphs	O
.	O
deﬁnition	O
43	O
(	O
moralisation	B
)	O
.	O
for	O
each	O
variable	B
x	O
add	O
an	O
undirected	B
link	O
between	O
all	O
parents	B
of	O
x	O
and	O
replace	O
the	O
directed	B
link	O
from	O
x	O
to	O
its	O
parents	B
by	O
undirected	B
links	O
.	O
this	O
creates	O
a	O
‘	O
moralised	O
’	O
markov	O
network	O
.	O
6.4.2	O
forming	O
the	O
clique	B
graph	I
the	O
clique	B
graph	I
is	O
formed	O
by	O
identifying	O
the	O
cliques	O
in	O
the	O
markov	O
network	O
and	O
adding	O
a	O
link	O
between	O
cliques	O
that	O
have	O
a	O
non-empty	O
intersection	O
.	O
add	O
a	O
separator	B
between	O
the	O
intersecting	O
cliques	O
.	O
6.4.3	O
forming	O
a	O
junction	B
tree	I
from	O
a	O
clique	B
graph	I
for	O
a	O
singly-connected	B
distribution	O
,	O
any	O
maximal	O
weight	B
spanning	O
tree	B
of	O
a	O
clique	B
graph	I
is	O
a	O
junction	B
tree	I
.	O
deﬁnition	O
44	O
(	O
junction	B
tree	I
)	O
.	O
a	O
junction	B
tree	I
is	O
obtained	O
by	O
ﬁnding	O
a	O
maximal	O
weight	B
spanning	O
tree	B
of	O
the	O
clique	B
graph	I
.	O
the	O
weight	B
of	O
the	O
tree	B
is	O
deﬁned	O
as	O
the	O
sum	O
of	O
all	O
the	O
separator	B
weights	O
of	O
the	O
tree	B
,	O
where	O
the	O
separator	B
weight	O
is	O
the	O
number	O
of	O
variables	O
in	O
the	O
separator	B
.	O
if	O
the	O
clique	B
graph	I
contains	O
loops	O
,	O
then	O
all	O
separators	O
on	O
the	O
loop	O
contain	O
the	O
same	O
variable	B
.	O
by	O
continuing	O
to	O
remove	O
loop	O
links	O
until	O
you	O
have	O
a	O
tree	B
is	O
revealed	O
,	O
we	O
obtain	O
a	O
junction	B
tree	I
.	O
example	O
23	O
(	O
forming	O
a	O
junction	B
tree	I
)	O
.	O
consider	O
the	O
belief	B
network	I
in	O
ﬁg	O
(	O
6.4a	O
)	O
.	O
the	O
moralisation	B
procedure	O
gives	O
ﬁg	O
(	O
6.4b	O
)	O
.	O
identifying	O
the	O
cliques	O
in	O
this	O
graph	B
,	O
and	O
linking	O
them	O
together	O
gives	O
the	O
clique	B
graph	I
in	O
ﬁg	O
(	O
6.4c	O
)	O
.	O
there	O
are	O
several	O
possible	O
junction	O
trees	O
one	O
could	O
obtain	O
from	O
this	O
clique	B
graph	I
,	O
and	O
one	O
is	O
given	O
in	O
ﬁg	O
(	O
6.4d	O
)	O
.	O
6.4.4	O
assigning	O
potentials	O
to	O
cliques	O
of	O
a	O
set	O
of	O
potentials	O
ψ	O
(	O
cid:0	O
)	O
deﬁnition	O
45	O
(	O
clique	B
potential	O
assignment	O
)	O
.	O
given	O
a	O
junction	B
tree	I
and	O
a	O
function	B
deﬁned	O
as	O
the	O
product	O
x	O
1	O
(	O
cid:1	O
)	O
,	O
.	O
.	O
.	O
,	O
ψ	O
(	O
x	O
n	O
)	O
,	O
a	O
valid	O
clique	B
potential	O
assignment	O
places	O
potentials	O
in	O
jt	O
cliques	O
whose	O
variables	O
can	O
contain	O
them	O
such	O
that	O
the	O
product	O
of	O
the	O
jt	O
clique	B
potentials	O
,	O
divided	O
by	O
the	O
jt	O
separator	B
potentials	O
,	O
is	O
equal	O
to	O
the	O
function	B
.	O
a	O
simple	O
way	O
to	O
achieve	O
this	O
assignment	O
is	O
to	O
list	O
all	O
the	O
potentials	O
and	O
order	O
the	O
jt	O
cliques	O
arbitrarily	O
.	O
then	O
,	O
for	O
each	O
potential	B
,	O
search	O
through	O
the	O
jt	O
cliques	O
until	O
the	O
ﬁrst	O
is	O
encountered	O
for	O
which	O
the	O
potential	B
variables	O
are	O
a	O
subset	O
of	O
the	O
jt	O
clique	B
variables	O
.	O
subsequently	O
the	O
potential	B
on	O
each	O
jt	O
clique	B
is	O
taken	O
as	O
the	O
product	O
of	O
all	O
clique	B
potentials	O
assigned	O
to	O
the	O
jt	O
clique	B
.	O
lastly	O
,	O
we	O
assign	O
all	O
jt	O
separators	O
to	O
unity	O
.	O
this	O
approach	B
is	O
taken	O
in	O
jtassignpot.m	O
.	O
92	O
draft	O
march	O
9	O
,	O
2010	O
junction	O
trees	O
for	O
multiply-connected	B
distributions	O
c	O
h	O
c	O
a	O
e	O
(	O
a	O
)	O
abc	O
c	O
eh	O
d	O
g	O
c	O
e	O
(	O
c	O
)	O
b	O
f	O
d	O
g	O
a	O
e	O
c	O
h	O
(	O
b	O
)	O
cf	O
dce	O
e	O
eg	O
b	O
f	O
c	O
e	O
(	O
d	O
)	O
abc	O
c	O
cf	O
eh	O
dce	O
e	O
eg	O
e	O
figure	O
6.4	O
:	O
(	O
a	O
)	O
:	O
belief	B
network	I
.	O
(	O
d	O
)	O
:	O
a	O
junction	B
tree	I
.	O
this	O
satisﬁes	O
the	O
running	B
intersection	I
property	I
that	O
for	O
any	O
two	O
nodes	O
which	O
contain	O
a	O
variable	B
in	O
common	O
,	O
any	O
clique	B
on	O
the	O
path	B
linking	O
the	O
two	O
nodes	O
also	O
contains	O
that	O
variable	B
.	O
(	O
b	O
)	O
:	O
moralised	O
version	O
of	O
(	O
a	O
)	O
.	O
(	O
c	O
)	O
:	O
clique	B
graph	I
of	O
(	O
b	O
)	O
.	O
example	O
24.	O
for	O
the	O
belief	B
network	I
of	O
ﬁg	O
(	O
6.4a	O
)	O
,	O
we	O
wish	O
to	O
assign	O
its	O
potentials	O
to	O
the	O
junction	B
tree	I
ﬁg	O
(	O
6.4d	O
)	O
.	O
in	O
this	O
case	O
the	O
assignment	O
is	O
unique	O
and	O
is	O
given	O
by	O
ψ	O
(	O
abc	O
)	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
b	O
)	O
p	O
(	O
c|a	O
,	O
b	O
)	O
ψ	O
(	O
dce	O
)	O
=	O
p	O
(	O
d	O
)	O
p	O
(	O
e|d	O
,	O
c	O
)	O
ψ	O
(	O
cf	O
)	O
=	O
p	O
(	O
f|c	O
)	O
ψ	O
(	O
eg	O
)	O
=	O
p	O
(	O
g|e	O
)	O
ψ	O
(	O
eh	O
)	O
=	O
p	O
(	O
h|e	O
)	O
(	O
6.4.1	O
)	O
all	O
separator	B
potentials	O
are	O
initialised	O
to	O
unity	O
.	O
note	O
that	O
in	O
some	O
instances	O
it	O
can	O
be	O
that	O
a	O
junction	B
tree	I
clique	O
is	O
assigned	O
to	O
unity	O
.	O
6.5	O
junction	O
trees	O
for	O
multiply-connected	B
distributions	O
when	O
the	O
distribution	B
contains	O
loops	O
,	O
the	O
construction	B
outlined	O
in	O
section	O
(	O
6.4	O
)	O
does	O
not	O
result	O
in	O
a	O
junction	B
tree	I
.	O
the	O
reason	O
is	O
that	O
,	O
due	O
to	O
the	O
loops	O
,	O
variable	B
elimination	I
changes	O
the	O
structure	B
of	O
the	O
remaining	O
graph	B
.	O
to	O
see	O
this	O
,	O
consider	O
the	O
following	O
distribution	B
,	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
)	O
=	O
φ	O
(	O
a	O
,	O
b	O
)	O
φ	O
(	O
b	O
,	O
c	O
)	O
φ	O
(	O
c	O
,	O
d	O
)	O
φ	O
(	O
d	O
,	O
a	O
)	O
(	O
6.5.1	O
)	O
as	O
shown	O
in	O
ﬁg	O
(	O
6.5a	O
)	O
.	O
let	O
’	O
s	O
ﬁrst	O
try	O
to	O
make	O
a	O
clique	B
graph	I
.	O
we	O
have	O
a	O
choice	O
about	O
which	O
variable	B
ﬁrst	O
to	O
marginalise	O
over	O
.	O
let	O
’	O
s	O
choose	O
d	O
:	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
=	O
φ	O
(	O
a	O
,	O
b	O
)	O
φ	O
(	O
b	O
,	O
c	O
)	O
(	O
cid:88	O
)	O
draft	O
march	O
9	O
,	O
2010	O
d	O
φ	O
(	O
c	O
,	O
d	O
)	O
φ	O
(	O
d	O
,	O
a	O
)	O
(	O
6.5.2	O
)	O
93	O
junction	O
trees	O
for	O
multiply-connected	B
distributions	O
a	O
d	O
a	O
b	O
c	O
b	O
c	O
a	O
d	O
b	O
c	O
a	O
d	O
b	O
c	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
(	O
d	O
)	O
abc	O
ac	O
(	O
e	O
)	O
acd	O
figure	O
6.5	O
:	O
(	O
a	O
)	O
:	O
an	O
undirected	B
graph	I
with	O
a	O
loop	O
.	O
c	O
in	O
the	O
subgraph	O
.	O
representation	B
.	O
(	O
e	O
)	O
:	O
junction	B
tree	I
for	O
(	O
a	O
)	O
.	O
(	O
c	O
)	O
:	O
the	O
induced	B
representation	I
for	O
the	O
graph	B
in	O
(	O
a	O
)	O
.	O
(	O
b	O
)	O
:	O
eliminating	O
node	B
d	O
adds	O
a	O
link	O
between	O
a	O
and	O
(	O
d	O
)	O
:	O
equivalent	B
induced	O
the	O
remaining	O
subgraph	O
therefore	O
has	O
an	O
extra	O
connection	O
between	O
a	O
and	O
c	O
,	O
see	O
ﬁg	O
(	O
6.5b	O
)	O
.	O
we	O
can	O
express	O
the	O
joint	B
in	O
terms	O
of	O
the	O
marginals	O
using	O
to	O
continue	O
the	O
transformation	O
into	O
marginal	B
form	O
,	O
let	O
’	O
s	O
try	O
to	O
replace	O
the	O
numerator	O
terms	O
with	O
proba-	O
bilities	O
.	O
we	O
can	O
do	O
this	O
by	O
considering	O
(	O
6.5.3	O
)	O
(	O
6.5.4	O
)	O
(	O
6.5.5	O
)	O
(	O
6.5.6	O
)	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
)	O
=	O
(	O
cid:80	O
)	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
d	O
φ	O
(	O
c	O
,	O
d	O
)	O
φ	O
(	O
d	O
,	O
a	O
)	O
φ	O
(	O
c	O
,	O
d	O
)	O
φ	O
(	O
d	O
,	O
a	O
)	O
p	O
(	O
a	O
,	O
c	O
,	O
d	O
)	O
=	O
φ	O
(	O
c	O
,	O
d	O
)	O
φ	O
(	O
d	O
,	O
a	O
)	O
(	O
cid:88	O
)	O
φ	O
(	O
a	O
,	O
b	O
)	O
φ	O
(	O
b	O
,	O
c	O
)	O
b	O
plugging	O
this	O
into	O
the	O
above	O
equation	B
,	O
we	O
have	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
)	O
=	O
d	O
φ	O
(	O
c	O
,	O
d	O
)	O
φ	O
(	O
d	O
,	O
a	O
)	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
p	O
(	O
a	O
,	O
c	O
,	O
d	O
)	O
b	O
φ	O
(	O
a	O
,	O
b	O
)	O
φ	O
(	O
b	O
,	O
c	O
)	O
we	O
recognise	O
that	O
the	O
denominator	O
is	O
simply	O
p	O
(	O
a	O
,	O
c	O
)	O
,	O
hence	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
)	O
=	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
p	O
(	O
a	O
,	O
c	O
,	O
d	O
)	O
p	O
(	O
a	O
,	O
c	O
)	O
.	O
this	O
means	O
that	O
a	O
valid	O
clique	B
graph	I
for	O
the	O
distribution	B
ﬁg	O
(	O
6.5a	O
)	O
must	O
contain	O
cliques	O
larger	O
than	O
those	O
in	O
the	O
original	O
distribution	B
.	O
to	O
form	O
a	O
jt	O
based	O
on	O
products	O
of	O
cliques	O
divided	O
by	O
products	O
of	O
separators	O
,	O
we	O
could	O
start	O
from	O
the	O
induced	B
representation	I
ﬁg	O
(	O
6.5c	O
)	O
.	O
alternatively	O
,	O
we	O
could	O
have	O
marginalised	O
over	O
variables	O
a	O
and	O
c	O
,	O
and	O
ended	O
up	O
with	O
the	O
equivalent	B
representation	O
ﬁg	O
(	O
6.5d	O
)	O
.	O
generally	O
,	O
the	O
result	O
from	O
variable	B
elimination	I
and	O
re-representation	O
in	O
terms	O
of	O
the	O
induced	O
graph	O
is	O
that	O
a	O
link	O
is	O
added	O
between	O
any	O
two	O
variables	O
on	O
a	O
loop	O
(	O
of	O
length	O
4	O
or	O
more	O
)	O
which	O
does	O
not	O
have	O
a	O
chord	B
.	O
this	O
is	O
called	O
triangulation	B
.	O
a	O
markov	O
network	O
on	O
a	O
triangulated	B
graph	O
can	O
always	O
be	O
written	O
in	O
terms	O
of	O
the	O
product	O
of	O
marginals	O
divided	O
by	O
the	O
product	O
of	O
separators	O
.	O
armed	O
with	O
this	O
new	O
induced	B
representation	I
,	O
we	O
can	O
form	O
a	O
junction	B
tree	I
.	O
a	O
f	O
b	O
e	O
(	O
a	O
)	O
c	O
a	O
d	O
f	O
b	O
e	O
(	O
b	O
)	O
c	O
d	O
(	O
a	O
)	O
:	O
figure	O
6.6	O
:	O
markov	O
network	O
.	O
resentation	O
.	O
loopy	B
‘	O
ladder	O
’	O
(	O
b	O
)	O
:	O
induced	O
rep-	O
94	O
draft	O
march	O
9	O
,	O
2010	O
junction	O
trees	O
for	O
multiply-connected	B
distributions	O
a	O
a	O
f	O
f	O
b	O
j	O
b	O
j	O
c	O
d	O
e	O
a	O
g	O
h	O
i	O
e	O
a	O
k	O
d	O
l	O
(	O
a	O
)	O
c	O
g	O
h	O
i	O
k	O
l	O
(	O
d	O
)	O
f	O
f	O
b	O
j	O
b	O
j	O
c	O
d	O
e	O
a	O
g	O
h	O
i	O
e	O
a	O
a	O
k	O
d	O
l	O
(	O
b	O
)	O
c	O
g	O
h	O
i	O
k	O
l	O
(	O
e	O
)	O
f	O
f	O
f	O
b	O
j	O
b	O
b	O
j	O
j	O
c	O
d	O
e	O
g	O
h	O
i	O
k	O
d	O
d	O
e	O
e	O
l	O
(	O
c	O
)	O
c	O
c	O
g	O
g	O
h	O
h	O
i	O
i	O
k	O
k	O
l	O
l	O
(	O
f	O
)	O
figure	O
6.7	O
:	O
(	O
a	O
)	O
:	O
markov	O
network	O
for	O
which	O
we	O
seek	O
a	O
triangulation	B
via	O
greedy	O
variable	O
elimination	O
.	O
we	O
(	O
b	O
)	O
:	O
we	O
then	O
eliminate	O
variables	O
b	O
,	O
d	O
since	O
these	O
only	O
add	O
ﬁrst	O
eliminate	O
the	O
simplical	B
nodes	I
a	O
,	O
e	O
,	O
l.	O
(	O
d	O
)	O
:	O
(	O
c	O
)	O
:	O
f	O
and	O
i	O
are	O
now	O
simplical	B
and	O
are	O
eliminated	O
.	O
a	O
single	O
extra	O
link	O
to	O
the	O
induced	O
graph	O
.	O
(	O
e	O
)	O
:	O
the	O
remaining	O
variables	O
{	O
c	O
,	O
j	O
,	O
k	O
}	O
we	O
eliminate	O
g	O
and	O
h	O
since	O
this	O
adds	O
only	O
single	O
extra	O
links	O
.	O
(	O
f	O
)	O
:	O
final	O
triangulation	B
.	O
the	O
variable	B
elimination	I
(	O
partial	O
)	O
order	O
is	O
may	O
be	O
eliminated	O
in	O
any	O
order	O
.	O
{	O
a	O
,	O
e	O
,	O
l	O
}	O
,	O
{	O
b	O
,	O
d	O
}	O
,	O
{	O
f	O
,	O
i	O
}	O
,	O
{	O
g	O
,	O
h	O
}	O
,	O
{	O
c	O
,	O
j	O
,	O
k	O
}	O
where	O
the	O
brackets	O
indicate	O
that	O
the	O
order	O
in	O
which	O
the	O
variables	O
inside	O
the	O
bracket	O
are	O
eliminated	O
is	O
irrelevant	O
.	O
compared	O
with	O
the	O
triangulation	B
produced	O
by	O
the	O
max-	O
cardinality	O
checking	O
approach	B
in	O
ﬁg	O
(	O
6.9d	O
)	O
,	O
this	O
triangulation	B
is	O
more	O
parsimonious	O
.	O
example	O
25.	O
a	O
slightly	O
more	O
complex	O
loopy	B
distribution	O
is	O
depicted	O
in	O
ﬁg	O
(	O
6.6a	O
)	O
,	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
,	O
e	O
,	O
f	O
)	O
=	O
φ	O
(	O
a	O
,	O
b	O
)	O
φ	O
(	O
b	O
,	O
c	O
)	O
φ	O
(	O
c	O
,	O
d	O
)	O
φ	O
(	O
d	O
,	O
e	O
)	O
φ	O
(	O
e	O
,	O
f	O
)	O
φ	O
(	O
a	O
,	O
f	O
)	O
φ	O
(	O
b	O
,	O
e	O
)	O
(	O
6.5.7	O
)	O
there	O
are	O
diﬀerent	O
induced	O
representations	O
depending	O
on	O
which	O
variables	O
we	O
decide	O
to	O
eliminate	O
.	O
the	O
reader	O
may	O
convince	O
herself	O
that	O
one	O
such	O
induced	B
representation	I
is	O
given	O
by	O
ﬁg	O
(	O
6.6b	O
)	O
.	O
deﬁnition	O
46	O
(	O
chord	B
)	O
.	O
this	O
is	O
a	O
link	O
joining	O
two	O
non-consecutive	O
vertices	O
of	O
a	O
loop	O
.	O
deﬁnition	O
47	O
(	O
triangulated	B
(	O
decomposable	B
)	O
graph	B
)	O
.	O
an	O
undirected	B
graph	I
is	O
triangulated	B
if	O
every	O
loop	O
of	O
length	O
4	O
or	O
more	O
has	O
a	O
chord	B
.	O
an	O
equivalent	B
term	O
is	O
that	O
the	O
graph	B
is	O
decomposable	B
or	O
chordal	B
.	O
an	O
undirected	B
graph	I
is	O
triangulated	B
if	O
and	O
only	O
if	O
its	O
clique	B
graph	I
has	O
a	O
junction	B
tree	I
.	O
6.5.1	O
triangulation	B
algorithms	O
when	O
a	O
variable	B
is	O
eliminated	O
from	O
a	O
graph	B
,	O
links	O
are	O
added	O
between	O
all	O
the	O
neighbours	O
of	O
the	O
eliminated	O
variable	B
.	O
a	O
triangulation	B
algorithm	O
is	O
one	O
that	O
produces	O
a	O
graph	B
for	O
which	O
there	O
exists	O
a	O
variable	B
elimi-	O
nation	O
order	O
that	O
introduces	O
no	O
extra	O
links	O
in	O
the	O
graph	B
.	O
draft	O
march	O
9	O
,	O
2010	O
95	O
junction	O
trees	O
for	O
multiply-connected	B
distributions	O
figure	O
6.8	O
:	O
junction	B
tree	I
formed	O
from	O
the	O
triangulation	B
ﬁg	O
(	O
6.7	O
)	O
f.	O
one	O
verify	O
that	O
this	O
satisﬁes	O
the	O
running	B
intersection	I
property	I
.	O
abf	O
bf	O
bcf	O
g	O
cdhi	O
di	O
dei	O
cf	O
g	O
cf	O
gj	O
chi	O
chik	O
cj	O
ck	O
cjk	O
jk	O
jkl	O
for	O
discrete	B
variables	O
the	O
complexity	O
of	O
inference	B
scales	O
exponentially	O
with	O
clique	O
sizes	O
in	O
the	O
triangulated	B
graph	O
since	O
absorption	B
requires	O
computing	O
tables	O
on	O
the	O
cliques	O
.	O
it	O
is	O
therefore	O
of	O
some	O
interest	O
to	O
ﬁnd	O
a	O
triangulated	B
graph	O
with	O
small	O
clique	B
sizes	O
.	O
however	O
,	O
ﬁnding	O
the	O
triangulated	B
graph	O
with	O
the	O
smallest	O
maximal	O
clique	B
is	O
an	O
np-hard	O
problem	B
for	O
a	O
general	O
graph	B
,	O
and	O
heuristics	O
are	O
unavoidable	O
.	O
below	O
we	O
describe	O
two	O
simple	O
algorithms	O
that	O
are	O
generically	O
reasonable	O
,	O
although	O
there	O
may	O
be	O
cases	O
where	O
an	O
alternative	O
algorithm	B
may	O
be	O
considerably	O
more	O
eﬃcient	B
[	O
53	O
,	O
28	O
,	O
191	O
]	O
.	O
remark	O
8	O
(	O
triangulation	B
does	O
not	O
mean	B
putting	O
‘	O
triangles	O
’	O
on	O
the	O
original	O
graph	B
)	O
.	O
note	O
that	O
a	O
triangulated	B
graph	O
is	O
not	O
one	O
in	O
which	O
‘	O
squares	O
in	O
the	O
original	O
graph	B
have	O
triangles	O
within	O
them	O
in	O
the	O
triangulated	B
graph	O
’	O
.	O
whilst	O
this	O
is	O
the	O
case	O
for	O
ﬁg	O
(	O
6.6b	O
)	O
,	O
this	O
is	O
not	O
true	O
for	O
ﬁg	O
(	O
6.9d	O
)	O
.	O
the	O
term	O
triangulation	B
refers	O
to	O
the	O
fact	O
that	O
every	O
‘	O
square	O
’	O
(	O
i.e	O
.	O
loop	O
of	O
length	O
4	O
)	O
must	O
have	O
a	O
‘	O
triangle	O
’	O
,	O
with	O
edges	O
added	O
until	O
this	O
criterion	O
is	O
satisﬁed	O
.	O
greedy	O
variable	O
elimination	O
an	O
intuitive	O
way	O
to	O
think	O
of	O
triangulation	B
is	O
to	O
ﬁrst	O
start	O
with	O
simplical	O
nodes	O
,	O
namely	O
those	O
which	O
,	O
when	O
eliminated	O
do	O
not	O
introduce	O
any	O
extra	O
links	O
in	O
the	O
remaining	O
graph	B
.	O
next	O
consider	O
a	O
non-simplical	O
node	B
of	O
the	O
remaining	O
graph	B
that	O
has	O
the	O
minimal	O
number	O
of	O
neighbours	O
.	O
then	O
add	O
a	O
link	O
between	O
all	O
neighbours	O
of	O
this	O
node	B
and	O
ﬁnally	O
eliminate	O
this	O
node	B
from	O
the	O
graph	B
.	O
continue	O
until	O
all	O
nodes	O
have	O
been	O
eliminated	O
.	O
(	O
this	O
procedure	O
corresponds	O
to	O
rose-tarjan	O
elimination	O
[	O
233	O
]	O
with	O
a	O
particular	O
node	B
elimination	O
choice	O
)	O
.	O
by	O
labelling	O
the	O
nodes	O
eliminated	O
in	O
sequence	O
,	O
we	O
obtain	O
a	O
perfect	O
ordering	O
(	O
see	O
below	O
)	O
in	O
reverse	O
.	O
in	O
the	O
case	O
that	O
(	O
discrete	B
)	O
variables	O
have	O
diﬀerent	O
numbers	O
of	O
states	O
,	O
a	O
more	O
reﬁned	O
version	O
is	O
to	O
choose	O
the	O
non-simplical	O
node	B
i	O
which	O
,	O
when	O
eliminated	O
,	O
leaves	O
the	O
smallest	O
clique	B
table	O
size	O
(	O
the	O
product	O
of	O
the	O
size	O
of	O
all	O
the	O
state	O
dimensions	O
of	O
the	O
neighbours	O
of	O
node	B
i	O
)	O
.	O
see	O
ﬁg	O
(	O
6.7	O
)	O
for	O
an	O
example	O
.	O
deﬁnition	O
48	O
(	O
variable	B
elimination	I
)	O
.	O
in	O
variable	B
elimination	I
,	O
one	O
simply	O
picks	O
any	O
non-deleted	O
node	B
x	O
in	O
the	O
graph	B
,	O
and	O
then	O
adds	O
links	O
to	O
all	O
the	O
neighbours	O
of	O
x.	O
node	B
x	O
is	O
then	O
deleted	O
.	O
one	O
repeats	O
this	O
until	O
all	O
nodes	O
have	O
been	O
deleted	O
[	O
233	O
]	O
.	O
whilst	O
this	O
procedure	O
guarantees	O
a	O
triangulated	B
graph	O
,	O
its	O
eﬃciency	O
depends	O
heavily	O
on	O
the	O
sequence	O
of	O
nodes	O
chosen	O
to	O
be	O
eliminated	O
.	O
several	O
heuristics	O
for	O
this	O
have	O
been	O
proposed	O
,	O
including	O
the	O
one	O
below	O
,	O
which	O
corresponds	O
to	O
choosing	O
x	O
to	O
be	O
the	O
node	B
with	O
the	O
minimal	O
number	O
of	O
neighbours	O
.	O
maximum	B
cardinality	I
checking	I
algorithm	O
(	O
2	O
)	O
terminates	O
with	O
success	O
if	O
the	O
graph	B
is	O
triangulated	B
.	O
not	O
only	O
is	O
this	O
a	O
suﬃcient	O
condition	O
for	O
a	O
graph	B
to	O
be	O
triangulated	B
,	O
but	O
is	O
also	O
necessary	O
[	O
271	O
]	O
.	O
it	O
processes	O
each	O
node	B
and	O
the	O
time	O
to	O
process	O
a	O
node	B
is	O
quadratic	O
in	O
the	O
number	O
of	O
adjacent	O
nodes	O
.	O
this	O
triangulation	B
checking	O
algorithm	B
also	O
suggests	O
96	O
draft	O
march	O
9	O
,	O
2010	O
the	O
junction	B
tree	I
algorithm	O
1	O
3	O
2	O
6	O
5	O
7	O
10	O
1	O
4	O
8	O
9	O
3	O
11	O
2	O
6	O
5	O
7	O
10	O
1	O
4	O
8	O
9	O
3	O
11	O
2	O
6	O
5	O
7	O
10	O
4	O
8	O
9	O
11	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
(	O
d	O
)	O
figure	O
6.9	O
:	O
starting	O
with	O
the	O
markov	O
network	O
in	O
(	O
a	O
)	O
,	O
the	O
maximum	B
cardinality	I
check	O
algorithm	B
proceeds	O
until	O
(	O
b	O
)	O
.	O
where	O
an	O
additional	O
link	O
is	O
required	O
(	O
c	O
)	O
.	O
one	O
continues	O
the	O
algorithm	B
until	O
the	O
fully	O
triangulated	B
graph	O
(	O
d	O
)	O
is	O
found	O
.	O
a	O
triangulation	B
construction	O
algorithm	B
–	O
we	O
simply	O
add	O
a	O
link	O
between	O
the	O
two	O
neighbours	O
that	O
caused	O
the	O
algorithm	B
to	O
fail	O
,	O
and	O
then	O
restart	O
the	O
algorithm	B
.	O
the	O
algorithm	B
is	O
restarted	O
from	O
the	O
beginning	O
,	O
not	O
just	O
continued	O
from	O
the	O
current	O
node	B
.	O
this	O
is	O
important	O
since	O
the	O
new	O
link	O
may	O
change	O
the	O
connectivity	O
between	O
previously	O
labelled	B
nodes	O
.	O
see	O
ﬁg	O
(	O
6.9	O
)	O
for	O
an	O
example1	O
.	O
deﬁnition	O
49	O
(	O
perfect	B
elimination	I
order	I
)	O
.	O
let	O
the	O
n	O
variables	O
in	O
a	O
markov	O
network	O
be	O
ordered	O
from	O
1	O
to	O
n.	O
the	O
ordering	O
is	O
perfect	O
if	O
,	O
for	O
each	O
node	B
i	O
,	O
the	O
neighbours	O
of	O
i	O
that	O
are	O
later	O
in	O
the	O
ordering	O
,	O
and	O
i	O
itself	O
,	O
form	O
a	O
(	O
maximal	O
)	O
clique	B
.	O
this	O
means	O
that	O
when	O
we	O
eliminate	O
the	O
variables	O
in	O
sequence	O
from	O
1	O
to	O
n	O
,	O
no	O
additional	O
links	O
are	O
induced	O
in	O
the	O
remaining	O
marginal	B
graph	O
.	O
a	O
graph	B
which	O
admits	O
a	O
perfect	B
elimination	I
order	I
is	O
decomposable	B
,	O
and	O
vice	O
versa	O
.	O
algorithm	B
2	O
a	O
check	B
if	O
a	O
graph	B
is	O
decomposable	B
(	O
triangulated	B
)	O
.	O
the	O
graph	B
is	O
triangulated	B
if	O
,	O
after	O
cycling	O
through	O
all	O
the	O
n	O
nodes	O
in	O
the	O
graph	B
,	O
the	O
fail	O
criterion	O
is	O
not	O
encountered	O
.	O
1	O
:	O
choose	O
any	O
node	B
in	O
the	O
graph	B
and	O
label	O
it	O
1	O
.	O
2	O
:	O
for	O
i	O
=	O
2	O
to	O
n	O
do	O
3	O
:	O
4	O
:	O
5	O
:	O
end	O
for	O
where	O
there	O
is	O
more	O
than	O
one	O
node	B
with	O
the	O
most	O
labeled	O
neighbours	O
,	O
the	O
tie	O
may	O
be	O
broken	O
arbitrarily	O
.	O
choose	O
the	O
node	B
with	O
the	O
most	O
labeled	O
neighbours	O
and	O
label	O
it	O
i.	O
if	O
any	O
two	O
labeled	O
neighbours	O
of	O
i	O
are	O
not	O
adjacent	O
to	O
each	O
other	O
,	O
fail	O
.	O
6.6	O
the	O
junction	B
tree	I
algorithm	O
we	O
now	O
have	O
all	O
the	O
steps	O
required	O
for	O
inference	B
in	O
multiply-connected	B
graphs	O
:	O
moralisation	B
marry	O
the	O
parents	B
.	O
this	O
is	O
required	O
only	O
for	O
directed	B
distributions	O
.	O
triangulation	B
ensure	O
that	O
every	O
loop	O
of	O
length	O
4	O
or	O
more	O
has	O
a	O
chord	B
.	O
junction	B
tree	I
form	O
a	O
junction	B
tree	I
from	O
cliques	O
of	O
the	O
triangulated	B
graph	O
,	O
removing	O
any	O
unnecessary	O
links	O
in	O
a	O
loop	O
on	O
the	O
cluster	O
graph	B
.	O
algorithmically	O
,	O
this	O
can	O
be	O
achieved	O
by	O
ﬁnding	O
a	O
tree	B
with	O
maximal	O
spanning	O
weight	O
with	O
weight	O
wij	O
given	O
by	O
the	O
number	O
of	O
variables	O
in	O
the	O
separator	B
between	O
cliques	O
i	O
and	O
j.	O
alternatively	O
,	O
given	O
a	O
clique	B
elimination	O
order	O
(	O
with	O
the	O
lowest	O
cliques	O
eliminated	O
ﬁrst	O
)	O
,	O
one	O
may	O
connect	O
each	O
clique	B
i	O
to	O
the	O
single	O
neighbouring	O
clique	B
j	O
>	O
i	O
with	O
greatest	O
edge	O
weight	O
wij	O
.	O
1this	O
example	O
is	O
due	O
to	O
david	O
page	O
www.cs.wisc.edu/∼dpage/cs731	O
draft	O
march	O
9	O
,	O
2010	O
97	O
the	O
junction	B
tree	I
algorithm	O
a	O
c	O
f	O
b	O
e	O
d	O
g	O
b	O
e	O
a	O
c	O
f	O
d	O
g	O
h	O
i	O
h	O
i	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
a	O
)	O
:	O
original	O
figure	O
6.10	O
:	O
loopy	B
belief	O
network	O
.	O
(	O
b	O
)	O
:	O
the	O
moralisation	B
links	O
(	O
dashed	O
)	O
are	O
between	O
nodes	O
e	O
and	O
f	O
and	O
between	O
nodes	O
f	O
and	O
g.	O
the	O
other	O
additional	O
links	O
come	O
from	O
triangulation	B
.	O
the	O
clique	B
size	O
of	O
the	O
resulting	O
clique	B
tree	O
(	O
not	O
shown	O
)	O
is	O
four	O
.	O
from	O
[	O
55	O
]	O
.	O
potential	B
assignment	O
assign	O
potentials	O
to	O
junction	B
tree	I
cliques	O
and	O
set	O
the	O
separator	B
potentials	O
to	O
unity	O
.	O
message	B
propagation	O
carry	O
out	O
absorption	B
until	O
updates	O
have	O
been	O
passed	O
along	O
both	O
directions	O
of	O
every	O
link	O
on	O
the	O
jt	O
.	O
the	O
clique	B
marginals	O
can	O
then	O
be	O
read	O
oﬀ	O
from	O
the	O
jt	O
.	O
an	O
example	O
is	O
given	O
in	O
ﬁg	O
(	O
6.10	O
)	O
.	O
6.6.1	O
remarks	O
on	O
the	O
jta	O
•	O
the	O
algorithm	B
provides	O
an	O
upper	O
bound	B
on	O
the	O
computation	O
required	O
to	O
calculate	O
marginals	O
in	O
the	O
graph	B
.	O
there	O
may	O
exist	O
more	O
eﬃcient	B
algorithms	O
in	O
particular	O
cases	O
,	O
although	O
generally	O
it	O
is	O
believed	O
that	O
there	O
can	O
not	O
be	O
much	O
more	O
eﬃcient	B
approaches	O
than	O
the	O
jta	O
since	O
every	O
other	O
approach	B
must	O
perform	O
a	O
triangulation	B
[	O
147	O
,	O
173	O
]	O
.	O
one	O
particular	O
special	O
case	O
is	O
that	O
of	O
marginal	B
inference	O
for	O
a	O
binary	O
variable	O
mrf	O
on	O
a	O
two-dimensional	O
lattice	O
containing	O
only	O
pure	O
quadratic	O
interactions	O
.	O
in	O
this	O
case	O
the	O
complexity	O
of	O
computing	O
a	O
marginal	B
inference	O
is	O
o	O
(	O
cid:0	O
)	O
n3	O
(	O
cid:1	O
)	O
where	O
n	O
is	O
the	O
number	O
of	O
variables	O
in	O
the	O
distribution	B
.	O
this	O
is	O
in	O
contrast	O
to	O
the	O
pessimistic	O
exponential	B
complexity	O
suggested	O
by	O
the	O
jta	O
.	O
such	O
cases	O
are	O
highly	O
specialised	O
and	O
it	O
is	O
unlikely	O
that	O
a	O
general	O
purpose	O
algorithm	B
that	O
could	O
consistently	O
outperform	O
the	O
jta	O
exists	O
.	O
•	O
one	O
might	O
think	O
that	O
the	O
only	O
class	O
of	O
distributions	O
for	O
which	O
essentially	O
a	O
linear	B
time	O
algorithm	B
is	O
available	O
are	O
singly-connected	B
distributions	O
.	O
however	O
,	O
there	O
are	O
decomposable	B
graphs	O
for	O
which	O
the	O
cliques	O
have	O
limited	O
size	O
meaning	O
that	O
inference	B
is	O
tractable	O
.	O
for	O
example	O
an	O
extended	O
version	O
of	O
the	O
‘	O
ladder	O
’	O
in	O
ﬁg	O
(	O
6.6a	O
)	O
has	O
a	O
simple	O
induced	O
decomposable	B
representation	O
ﬁg	O
(	O
6.6b	O
)	O
,	O
for	O
which	O
marginal	B
inference	O
would	O
be	O
linear	B
in	O
the	O
number	O
of	O
rungs	O
in	O
the	O
ladder	O
.	O
eﬀectively	O
these	O
structures	O
are	O
hyper	B
trees	O
in	O
which	O
the	O
complexity	O
is	O
then	O
related	O
to	O
the	O
tree	B
width	I
of	O
the	O
graph	B
[	O
81	O
]	O
.	O
•	O
ideally	O
,	O
we	O
would	O
like	O
to	O
ﬁnd	O
a	O
triangulated	B
graph	O
which	O
has	O
minimal	O
clique	B
size	O
.	O
however	O
,	O
it	O
can	O
be	O
shown	O
to	O
be	O
a	O
hard-computation	O
problem	B
(	O
n	O
p	O
-hard	O
)	O
to	O
ﬁnd	O
the	O
most	O
eﬃcient	O
triangulation	B
.	O
in	O
practice	O
,	O
most	O
general	O
purpose	O
triangulation	B
algorithms	O
are	O
somewhat	O
heuristic	O
and	O
chosen	O
to	O
provide	O
reasonable	O
,	O
but	O
clearly	O
not	O
optimal	O
,	O
generic	O
performance	B
.	O
•	O
numerical	B
over/under	O
ﬂow	O
issues	O
can	O
occur	O
in	O
large	O
cliques	O
,	O
where	O
many	O
probability	B
values	O
are	O
mul-	O
tiplied	O
together	O
.	O
similarly	O
in	O
long	O
chains	O
since	O
absorption	B
will	O
tend	O
to	O
reduce	O
the	O
numerical	B
size	O
of	O
potential	B
entries	O
in	O
a	O
clique	B
.	O
if	O
we	O
only	O
care	O
about	O
marginals	O
we	O
can	O
avoid	O
numerical	B
diﬃculties	O
by	O
normalising	O
potentials	O
at	O
each	O
step	O
;	O
these	O
missing	B
normalisation	O
constants	O
can	O
always	O
be	O
found	O
under	O
the	O
normalisation	B
constraint	O
.	O
if	O
required	O
one	O
can	O
always	O
store	O
the	O
values	O
of	O
these	O
local	B
renor-	O
malisations	O
,	O
should	O
,	O
for	O
example	O
,	O
the	O
global	B
normalisation	O
constant	O
of	O
a	O
distribution	B
be	O
required	O
,	O
see	O
section	O
(	O
6.6.2	O
)	O
.	O
•	O
after	O
clamping	O
variables	O
in	O
evidential	O
states	O
,	O
running	O
the	O
jta	O
returns	O
the	O
joint	B
distribution	O
on	O
the	O
non-evidential	O
variables	O
in	O
a	O
clique	B
with	O
all	O
the	O
evidential	O
variables	O
clamped	O
in	O
their	O
evidential	O
states	O
.	O
from	O
this	O
conditionals	O
are	O
straightforward	O
to	O
calculate	O
.	O
•	O
imagine	O
that	O
we	O
have	O
run	O
the	O
jt	O
algorithm	B
and	O
want	O
to	O
afterwards	O
ﬁnd	O
the	O
marginal	B
p	O
(	O
x|evidence	O
)	O
.	O
we	O
could	O
do	O
so	O
by	O
clamping	O
the	O
evidential	O
variables	O
.	O
however	O
,	O
if	O
both	O
x	O
and	O
the	O
set	O
of	O
evidential	O
draft	O
march	O
9	O
,	O
2010	O
98	O
the	O
junction	B
tree	I
algorithm	O
variables	O
are	O
all	O
contained	O
within	O
a	O
single	O
clique	B
of	O
the	O
jt	O
,	O
then	O
we	O
may	O
use	O
the	O
consistent	B
jt	O
cliques	O
to	O
compute	O
p	O
(	O
x|evidence	O
)	O
.	O
the	O
reason	O
is	O
that	O
since	O
the	O
jt	O
clique	B
contains	O
the	O
marginal	B
on	O
the	O
set	O
of	O
variables	O
which	O
includes	O
x	O
and	O
the	O
evidential	O
variables	O
,	O
we	O
can	O
obtain	O
the	O
required	O
marginal	B
by	O
considering	O
the	O
single	O
jt	O
clique	B
alone	O
.	O
•	O
representing	O
the	O
marginal	B
distribution	O
of	O
a	O
set	O
of	O
variables	O
x	O
which	O
are	O
not	O
contained	O
within	O
a	O
single	O
clique	B
is	O
in	O
general	O
computationally	O
diﬃcult	O
.	O
whilst	O
the	O
probability	B
of	O
any	O
state	O
of	O
p	O
(	O
x	O
)	O
may	O
be	O
computed	O
eﬃciently	O
,	O
there	O
are	O
in	O
general	O
an	O
exponential	B
number	O
of	O
such	O
states	O
.	O
a	O
classical	O
example	O
in	O
this	O
regard	O
is	O
the	O
hmm	O
,	O
section	O
(	O
23.2	O
)	O
with	O
singly-connected	O
joint	B
distribution	O
p	O
(	O
v	O
,	O
h	O
)	O
.	O
however	O
the	O
marginal	B
distribution	O
p	O
(	O
h	O
)	O
is	O
fully	O
connected	B
.	O
this	O
means	O
that	O
for	O
example	O
whilst	O
the	O
entropy	B
of	O
p	O
(	O
v	O
,	O
h	O
)	O
is	O
straightforward	O
to	O
compute	O
,	O
the	O
entropy	B
of	O
the	O
marginal	B
p	O
(	O
h	O
)	O
is	O
intractable	O
.	O
6.6.2	O
computing	O
the	O
normalisation	B
constant	I
of	O
a	O
distribution	B
for	O
a	O
markov	O
network	O
(	O
cid:89	O
)	O
p	O
(	O
x	O
)	O
=	O
1	O
z	O
how	O
can	O
we	O
ﬁnd	O
z	O
eﬃciently	O
?	O
if	O
we	O
used	O
the	O
jta	O
on	O
the	O
unnormalised	O
distribution	B
(	O
cid:81	O
)	O
i	O
φ	O
(	O
xi	O
)	O
have	O
the	O
equivalent	B
representation	O
:	O
(	O
6.6.1	O
)	O
i	O
φ	O
(	O
xi	O
)	O
,	O
we	O
would	O
(	O
6.6.2	O
)	O
(	O
6.6.3	O
)	O
1	O
z	O
(	O
cid:81	O
)	O
(	O
cid:81	O
)	O
c	O
φ	O
(	O
xc	O
)	O
s	O
φ	O
(	O
xs	O
)	O
(	O
cid:81	O
)	O
(	O
cid:81	O
)	O
c	O
φ	O
(	O
xc	O
)	O
s	O
φ	O
(	O
xs	O
)	O
p	O
(	O
x	O
)	O
=	O
z	O
=	O
(	O
cid:88	O
)	O
x	O
z	O
=	O
(	O
cid:88	O
)	O
xc	O
since	O
the	O
distribution	B
must	O
normalise	O
,	O
we	O
can	O
obtain	O
z	O
from	O
for	O
a	O
consistent	B
jt	O
,	O
summing	O
ﬁrst	O
over	O
the	O
variables	O
of	O
a	O
simplical	B
jt	O
clique	B
(	O
not	O
including	O
the	O
separator	B
variables	O
)	O
,	O
the	O
marginal	B
clique	O
will	O
cancel	O
with	O
the	O
corresponding	O
separator	B
to	O
give	O
a	O
unity	O
term	O
so	O
that	O
the	O
clique	B
and	O
separator	B
can	O
be	O
removed	O
.	O
this	O
forms	O
a	O
new	O
jt	O
for	O
which	O
we	O
then	O
eliminate	O
another	O
simplical	B
clique	O
.	O
continuing	O
in	O
this	O
manner	O
we	O
will	O
be	O
left	O
with	O
a	O
single	O
numerator	O
potential	B
so	O
that	O
φ	O
(	O
xc	O
)	O
(	O
6.6.4	O
)	O
this	O
is	O
true	O
for	O
any	O
clique	B
c	O
,	O
so	O
it	O
makes	O
sense	O
to	O
choose	O
one	O
with	O
a	O
small	O
number	O
of	O
states	O
so	O
that	O
the	O
resulting	O
raw	O
summation	O
is	O
eﬃcient	B
.	O
hence	O
in	O
order	O
to	O
compute	O
the	O
normalisation	B
constant	I
of	O
a	O
distribution	B
one	O
runs	O
the	O
jt	O
algorithm	B
on	O
an	O
unnormalised	O
distribution	B
and	O
the	O
global	B
normalisation	O
is	O
then	O
given	O
by	O
the	O
local	B
normalisation	O
of	O
any	O
clique	B
.	O
note	O
that	O
if	O
the	O
graph	B
is	O
disconnected	B
(	O
there	O
are	O
isolated	O
cliques	O
)	O
,	O
the	O
normalisation	B
is	O
the	O
product	O
of	O
the	O
connected	B
component	O
normalisation	B
constants	O
.	O
a	O
computationally	O
convenient	O
way	O
to	O
ﬁnd	O
this	O
is	O
to	O
compute	O
the	O
product	O
of	O
all	O
clique	B
normalisations	O
divided	O
by	O
the	O
product	O
of	O
all	O
separator	B
normalisations	O
.	O
6.6.3	O
the	O
marginal	B
likelihood	I
our	O
interest	O
here	O
is	O
the	O
computation	O
of	O
p	O
(	O
v	O
)	O
where	O
v	O
is	O
a	O
subset	O
of	O
the	O
full	O
variable	B
set	O
.	O
naively	O
,	O
one	O
could	O
carry	O
out	O
this	O
computation	O
by	O
summing	O
over	O
all	O
the	O
non-evidential	O
variables	O
(	O
hidden	B
variables	I
h	O
=	O
x\v	O
)	O
explicitly	O
.	O
in	O
cases	O
where	O
this	O
is	O
computationally	O
impractical	O
an	O
alternative	O
is	O
to	O
use	O
p	O
(	O
h|v	O
)	O
=	O
p	O
(	O
v	O
,	O
h	O
)	O
p	O
(	O
v	O
)	O
(	O
6.6.5	O
)	O
one	O
can	O
view	O
this	O
as	O
a	O
product	O
of	O
clique	B
potentials	O
divided	O
by	O
the	O
normalisation	B
p	O
(	O
v	O
)	O
,	O
for	O
which	O
the	O
general	O
method	O
of	O
section	O
(	O
6.6.2	O
)	O
may	O
be	O
directly	O
applied	O
.	O
see	O
demojtree.m	O
.	O
draft	O
march	O
9	O
,	O
2010	O
99	O
the	O
junction	B
tree	I
algorithm	O
example	O
26	O
(	O
a	O
simple	O
example	O
of	O
the	O
jta	O
)	O
.	O
consider	O
running	O
the	O
jta	O
on	O
the	O
simple	O
graph	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
=	O
p	O
(	O
a|b	O
)	O
p	O
(	O
b|c	O
)	O
p	O
(	O
c	O
)	O
(	O
6.6.6	O
)	O
the	O
moralisation	B
and	O
triangulation	B
steps	O
are	O
trivial	O
,	O
and	O
the	O
jta	O
is	O
given	O
immediately	O
by	O
the	O
ﬁgure	O
on	O
the	O
right	O
.	O
a	O
valid	O
assignment	O
is	O
a	O
ab	O
b	O
b	O
c	O
bc	O
to	O
ﬁnd	O
a	O
marginal	B
p	O
(	O
b	O
)	O
we	O
ﬁrst	O
run	O
the	O
jta	O
:	O
ψ	O
(	O
a	O
,	O
b	O
)	O
=	O
p	O
(	O
a|b	O
)	O
,	O
ψ	O
(	O
b	O
)	O
=	O
1	O
,	O
ψ	O
(	O
b	O
,	O
c	O
)	O
=	O
p	O
(	O
b|c	O
)	O
p	O
(	O
c	O
)	O
•	O
absorbing	O
from	O
ab	O
through	O
b	O
,	O
the	O
new	O
separator	B
is	O
ψ∗	O
(	O
b	O
)	O
=	O
(	O
cid:80	O
)	O
(	O
6.6.7	O
)	O
a	O
ψ	O
(	O
a	O
,	O
b	O
)	O
=	O
(	O
cid:80	O
)	O
a	O
p	O
(	O
a|b	O
)	O
=	O
1	O
.	O
•	O
the	O
new	O
potential	B
on	O
(	O
b	O
,	O
c	O
)	O
is	O
given	O
by	O
=	O
p	O
(	O
b|c	O
)	O
p	O
(	O
c	O
)	O
×	O
1	O
1	O
ψ	O
(	O
b	O
)	O
∗	O
(	O
b	O
,	O
c	O
)	O
=	O
ψ	O
(	O
b	O
,	O
c	O
)	O
ψ∗	O
(	O
b	O
)	O
∗	O
(	O
b	O
,	O
c	O
)	O
=	O
(	O
cid:88	O
)	O
∗∗	O
(	O
b	O
)	O
=	O
(	O
cid:88	O
)	O
ψ	O
ψ	O
ψ	O
c	O
c	O
•	O
absorbing	O
from	O
bc	O
through	O
b	O
,	O
the	O
new	O
separator	B
is	O
p	O
(	O
b|c	O
)	O
p	O
(	O
c	O
)	O
=	O
p	O
(	O
a|b	O
)	O
(	O
cid:80	O
)	O
•	O
the	O
new	O
potential	B
on	O
(	O
a	O
,	O
b	O
)	O
is	O
given	O
by	O
∗	O
(	O
a	O
,	O
b	O
)	O
=	O
ψ	O
(	O
a	O
,	O
b	O
)	O
ψ∗∗	O
(	O
b	O
)	O
c	O
p	O
(	O
b|c	O
)	O
p	O
(	O
c	O
)	O
1	O
ψ	O
ψ∗	O
(	O
b	O
)	O
this	O
is	O
therefore	O
indeed	O
equal	O
to	O
the	O
marginal	B
since	O
(	O
cid:80	O
)	O
∗∗	O
(	O
b	O
)	O
=	O
(	O
cid:88	O
)	O
the	O
new	O
separator	B
ψ∗∗	O
(	O
b	O
)	O
contains	O
the	O
marginal	B
p	O
(	O
b	O
)	O
since	O
p	O
(	O
b|c	O
)	O
p	O
(	O
c	O
)	O
=	O
(	O
cid:88	O
)	O
p	O
(	O
b	O
,	O
c	O
)	O
=	O
p	O
(	O
b	O
)	O
ψ	O
c	O
c	O
c	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
=	O
p	O
(	O
a	O
,	O
b	O
)	O
.	O
(	O
6.6.8	O
)	O
(	O
6.6.9	O
)	O
(	O
6.6.10	O
)	O
(	O
6.6.11	O
)	O
(	O
6.6.12	O
)	O
(	O
6.6.13	O
)	O
(	O
6.6.14	O
)	O
example	O
27	O
(	O
finding	O
a	O
conditional	B
marginal	I
)	O
.	O
continuing	O
with	O
the	O
distribution	B
in	O
example	O
(	O
26	O
)	O
,	O
we	O
consider	O
how	O
to	O
compute	O
p	O
(	O
b|a	O
=	O
1	O
,	O
c	O
=	O
1	O
)	O
.	O
first	O
we	O
clamp	O
the	O
evidential	O
variables	O
in	O
their	O
states	O
.	O
then	O
we	O
claim	O
that	O
the	O
eﬀect	O
of	O
running	O
the	O
jta	O
is	O
to	O
produce	O
on	O
a	O
set	O
of	O
clique	B
variables	O
x	O
the	O
marginals	O
on	O
the	O
cliques	O
p	O
(	O
x	O
,	O
v	O
)	O
.	O
we	O
demonstrate	O
this	O
below	O
:	O
a	O
p	O
(	O
a|b	O
)	O
=	O
1.	O
however	O
,	O
since	O
a	O
is	O
clamped	O
in	O
state	O
a	O
=	O
1	O
,	O
then	O
the	O
summation	O
is	O
not	O
carried	O
out	O
over	O
a	O
,	O
and	O
we	O
have	O
instead	O
•	O
in	O
general	O
,	O
the	O
new	O
separator	B
is	O
given	O
by	O
ψ∗	O
(	O
b	O
)	O
=	O
(	O
cid:80	O
)	O
ψ∗	O
(	O
b	O
)	O
=	O
p	O
(	O
a	O
=	O
1|b	O
)	O
.	O
•	O
the	O
new	O
potential	B
on	O
the	O
(	O
b	O
,	O
c	O
)	O
clique	B
is	O
given	O
by	O
a	O
ψ	O
(	O
a	O
,	O
b	O
)	O
=	O
(	O
cid:80	O
)	O
ψ	O
(	O
b	O
)	O
∗	O
(	O
b	O
,	O
c	O
)	O
=	O
ψ	O
(	O
b	O
,	O
c	O
)	O
ψ∗	O
(	O
b	O
)	O
∗	O
(	O
b	O
,	O
c	O
)	O
=	O
(	O
cid:88	O
)	O
∗∗	O
(	O
b	O
)	O
=	O
(	O
cid:88	O
)	O
ψ	O
ψ	O
ψ	O
c	O
c	O
•	O
the	O
new	O
separator	B
is	O
normally	O
given	O
by	O
p	O
(	O
b|c	O
)	O
p	O
(	O
c	O
)	O
=	O
p	O
(	O
b|c	O
=	O
1	O
)	O
p	O
(	O
c	O
=	O
1	O
)	O
p	O
(	O
a	O
=	O
1|b	O
)	O
1	O
however	O
,	O
since	O
c	O
is	O
clamped	O
in	O
state	O
1	O
,	O
we	O
have	O
instead	O
ψ	O
∗∗	O
(	O
b	O
)	O
=	O
p	O
(	O
b|c	O
=	O
1	O
)	O
p	O
(	O
c	O
=	O
1	O
)	O
p	O
(	O
a	O
=	O
1|b	O
)	O
100	O
draft	O
march	O
9	O
,	O
2010	O
finding	O
the	O
most	B
likely	I
state	I
•	O
the	O
new	O
potential	B
on	O
(	O
a	O
,	O
b	O
)	O
is	O
given	O
by	O
∗	O
(	O
a	O
,	O
b	O
)	O
=	O
ψ	O
(	O
a	O
,	O
b	O
)	O
ψ∗∗	O
(	O
b	O
)	O
ψ	O
ψ∗	O
(	O
b	O
)	O
=	O
p	O
(	O
a	O
=	O
1|b	O
)	O
p	O
(	O
b|c	O
=	O
1	O
)	O
p	O
(	O
c	O
=	O
1	O
)	O
p	O
(	O
a	O
=	O
1|b	O
)	O
p	O
(	O
a	O
=	O
1|b	O
)	O
=	O
p	O
(	O
a	O
=	O
1|b	O
)	O
p	O
(	O
b|c	O
=	O
1	O
)	O
p	O
(	O
c	O
=	O
1	O
)	O
(	O
6.6.15	O
)	O
the	O
eﬀect	O
of	O
clamping	O
a	O
set	O
of	O
variables	O
v	O
in	O
their	O
evidential	O
states	O
and	O
running	O
the	O
jta	O
is	O
that	O
,	O
for	O
a	O
clique	B
i	O
which	O
contains	O
the	O
set	O
of	O
non-evidential	O
variables	O
hi	O
,	O
the	O
consistent	B
potential	O
from	O
the	O
jta	O
contains	O
the	O
marginal	B
p	O
(	O
hi	O
,	O
v	O
)	O
.	O
finding	O
a	O
conditional	B
marginal	I
is	O
then	O
straightforward	O
by	O
ensuring	O
normalisation	B
.	O
example	O
28	O
(	O
ﬁnding	O
the	O
likelihood	B
p	O
(	O
a	O
=	O
1	O
,	O
c	O
=	O
1	O
)	O
)	O
.	O
the	O
eﬀect	O
of	O
clamping	O
the	O
variables	O
in	O
their	O
evidential	O
states	O
and	O
running	O
the	O
jta	O
produces	O
the	O
joint	B
marginals	O
,	O
such	O
as	O
ψ∗	O
(	O
a	O
,	O
b	O
)	O
=	O
p	O
(	O
a	O
=	O
1	O
,	O
b	O
,	O
c	O
=	O
1	O
)	O
.	O
then	O
calculating	O
the	O
likelihood	B
is	O
easy	O
since	O
we	O
just	O
sum	O
out	O
over	O
the	O
non-evidential	O
variables	O
of	O
any	O
converged	O
potential	B
:	O
p	O
(	O
a	O
=	O
1	O
,	O
c	O
=	O
1	O
)	O
=	O
(	O
cid:80	O
)	O
b	O
ψ∗	O
(	O
a	O
,	O
b	O
)	O
=	O
(	O
cid:80	O
)	O
b	O
p	O
(	O
a	O
=	O
1	O
,	O
b	O
,	O
c	O
=	O
1	O
)	O
.	O
6.7	O
finding	O
the	O
most	B
likely	I
state	I
a	O
quantity	O
of	O
interest	O
is	O
the	O
most	O
likely	O
joint	O
state	O
of	O
a	O
distribution	B
:	O
argmax	O
x1	O
,	O
...	O
,	O
xn	O
p	O
(	O
x	O
)	O
(	O
6.7.1	O
)	O
and	O
it	O
is	O
natural	B
to	O
wonder	O
how	O
this	O
can	O
be	O
eﬃciently	O
computed	O
in	O
the	O
case	O
of	O
a	O
loopy	B
distribution	O
.	O
since	O
the	O
development	O
of	O
the	O
jta	O
is	O
based	O
around	O
a	O
variable	B
elimination	I
procedure	O
and	O
the	O
max	O
operator	O
dis-	O
tributes	O
over	O
the	O
distribution	B
as	O
well	O
,	O
eliminating	O
a	O
variable	B
by	O
maximising	O
over	O
that	O
variable	B
will	O
have	O
the	O
same	O
eﬀect	O
on	O
the	O
graph	B
structure	O
as	O
summation	O
did	O
.	O
this	O
means	O
that	O
a	O
junction	B
tree	I
is	O
again	O
an	O
appropriate	O
structure	B
on	O
which	O
to	O
perform	O
max	O
operations	O
.	O
once	O
a	O
jt	O
has	O
been	O
constructed	O
,	O
one	O
then	O
uses	O
the	O
max	O
absorption	B
procedure	O
(	O
see	O
below	O
)	O
,	O
to	O
perform	O
maximisation	B
over	O
the	O
variables	O
.	O
after	O
a	O
full	O
round	O
of	O
absorption	B
has	O
been	O
carried	O
out	O
,	O
the	O
cliques	O
contain	O
the	O
distribution	B
on	O
the	O
variables	O
of	O
the	O
clique	B
with	O
all	O
remaining	O
variables	O
set	O
to	O
their	O
optimal	O
states	O
.	O
the	O
optimal	O
local	B
states	O
can	O
be	O
found	O
by	O
explicit	O
optimisation	B
of	O
each	O
clique	B
potential	O
separately	O
.	O
note	O
that	O
this	O
procedure	O
holds	O
also	O
for	O
non-distributions	O
–	O
in	O
this	O
sense	O
this	O
is	O
an	O
example	O
of	O
a	O
more	O
general	O
dynamic	B
programming	O
procedure	O
applied	O
in	O
a	O
case	O
where	O
the	O
underlying	O
graph	B
is	O
multiply-connected	B
.	O
this	O
demonstrates	O
how	O
to	O
eﬃciently	O
compute	O
the	O
optimum	O
of	O
a	O
multiply-connected	B
function	O
deﬁned	O
as	O
the	O
product	O
on	O
potentials	O
.	O
deﬁnition	O
50	O
(	O
max	O
absorption	B
)	O
.	O
ψ	O
(	O
v	O
)	O
ψ∗	O
(	O
s	O
)	O
ψ∗	O
(	O
w	O
)	O
let	O
v	O
and	O
w	O
be	O
neighbours	O
in	O
a	O
clique	B
graph	I
,	O
let	O
s	O
be	O
their	O
separator	B
,	O
and	O
let	O
ψ	O
(	O
v	O
)	O
,	O
ψ	O
(	O
w	O
)	O
and	O
ψ	O
(	O
s	O
)	O
be	O
their	O
potentials	O
.	O
absorption	B
replaces	O
the	O
tables	O
ψ	O
(	O
s	O
)	O
and	O
ψ	O
(	O
w	O
)	O
with	O
∗	O
(	O
w	O
)	O
=	O
ψ	O
(	O
w	O
)	O
ψ∗	O
(	O
s	O
)	O
ψ	O
(	O
s	O
)	O
∗	O
(	O
s	O
)	O
=	O
maxv\s	O
ψ	O
(	O
v	O
)	O
ψ	O
ψ	O
once	O
messages	O
have	O
been	O
passed	O
in	O
both	O
directions	O
over	O
all	O
separators	O
,	O
according	O
to	O
a	O
valid	O
schedule	B
,	O
the	O
most-likely	O
joint	B
state	O
can	O
be	O
read	O
oﬀ	O
from	O
maximising	O
the	O
state	O
of	O
the	O
clique	B
potentials	O
.	O
this	O
is	O
implemented	O
in	O
absorb.m	O
and	O
absorption.m	O
where	O
a	O
ﬂag	O
is	O
used	O
to	O
switch	O
between	O
either	O
sum	O
or	O
max	O
absorption	B
.	O
draft	O
march	O
9	O
,	O
2010	O
101	O
reabsorption	B
:	O
converting	O
a	O
junction	B
tree	I
to	O
a	O
directed	B
network	O
abc	O
c	O
c	O
e	O
dce	O
e	O
abc	O
c	O
c	O
e	O
cf	O
dce	O
e	O
cf	O
a	O
b	O
c	O
e	O
d	O
f	O
eg	O
eh	O
eg	O
(	O
a	O
)	O
eh	O
(	O
b	O
)	O
g	O
h	O
(	O
c	O
)	O
figure	O
6.11	O
:	O
(	O
a	O
)	O
:	O
junction	B
tree	I
.	O
(	O
b	O
)	O
:	O
directed	B
junction	O
tree	B
in	O
which	O
all	O
edges	O
are	O
consistently	O
oriented	O
away	O
from	O
the	O
clique	B
(	O
abc	O
)	O
.	O
(	O
c	O
)	O
:	O
a	O
set	B
chain	I
formed	O
from	O
the	O
junction	B
tree	I
by	O
reabsorbing	O
each	O
separator	B
into	O
its	O
child	O
clique	B
.	O
6.8	O
reabsorption	B
:	O
converting	O
a	O
junction	B
tree	I
to	O
a	O
directed	B
network	O
it	O
is	O
sometimes	O
useful	O
to	O
be	O
able	O
to	O
convert	O
the	O
jt	O
back	O
to	O
a	O
bn	O
of	O
a	O
desired	O
form	O
.	O
for	O
example	O
,	O
if	O
one	O
wishes	O
to	O
draw	O
samples	O
from	O
a	O
markov	O
network	O
,	O
this	O
can	O
be	O
achieved	O
by	O
ancestral	B
sampling	I
on	O
an	O
equivalent	B
directed	O
structure	B
,	O
see	O
section	O
(	O
27.2.2	O
)	O
.	O
revisiting	O
the	O
example	O
from	O
ﬁg	O
(	O
6.4	O
)	O
,	O
we	O
have	O
the	O
jt	O
given	O
in	O
ﬁg	O
(	O
6.11a	O
)	O
.	O
to	O
ﬁnd	O
a	O
valid	O
directed	B
represen-	O
tation	O
we	O
ﬁrst	O
orient	O
the	O
jt	O
edges	O
consistently	O
away	O
from	O
a	O
chosen	O
root	O
node	B
(	O
see	O
singleparenttree.m	O
)	O
,	O
thereby	O
forming	O
a	O
directed	B
jt	O
which	O
has	O
the	O
property	O
that	O
each	O
clique	B
has	O
at	O
most	O
one	O
parent	O
clique	B
.	O
deﬁnition	O
51	O
(	O
reabsorption	B
)	O
.	O
v	O
s	O
w	O
⇒	O
v	O
w	O
let	O
v	O
and	O
w	O
be	O
neighbouring	O
cliques	O
in	O
a	O
directed	B
jt	O
in	O
which	O
each	O
clique	B
in	O
the	O
tree	B
has	O
at	O
most	O
one	O
parent	O
.	O
furthermore	O
,	O
let	O
s	O
be	O
their	O
separator	B
,	O
and	O
ψ	O
(	O
v	O
)	O
,	O
ψ	O
(	O
w	O
)	O
and	O
ψ	O
(	O
s	O
)	O
be	O
the	O
potentials	O
.	O
reabsorption	B
into	O
w	O
removes	O
the	O
separator	B
and	O
forms	O
a	O
(	O
set	O
)	O
conditional	B
distribution	O
p	O
(	O
w|v	O
)	O
=	O
ψ	O
(	O
w	O
)	O
ψ	O
(	O
s	O
)	O
we	O
say	O
that	O
clique	B
w	O
reabsorbs	O
the	O
separator	B
s.	O
(	O
6.8.1	O
)	O
in	O
ﬁg	O
(	O
6.11	O
)	O
where	O
one	O
amongst	O
many	O
possible	O
directed	B
representations	O
is	O
formed	O
from	O
the	O
jt	O
.	O
speciﬁcally	O
,	O
ﬁg	O
(	O
6.11a	O
)	O
represents	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
,	O
e	O
,	O
f	O
,	O
g	O
,	O
h	O
)	O
=	O
p	O
(	O
e	O
,	O
g	O
)	O
p	O
(	O
d	O
,	O
c	O
,	O
e	O
)	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
p	O
(	O
c	O
,	O
f	O
)	O
p	O
(	O
e	O
,	O
h	O
)	O
p	O
(	O
e	O
)	O
p	O
(	O
c	O
)	O
p	O
(	O
c	O
)	O
p	O
(	O
e	O
)	O
we	O
now	O
have	O
many	O
choices	O
as	O
to	O
which	O
clique	B
re-absorbs	O
a	O
separator	B
.	O
one	O
such	O
choice	O
would	O
give	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
,	O
e	O
,	O
f	O
,	O
g	O
,	O
h	O
)	O
=	O
p	O
(	O
g|e	O
)	O
p	O
(	O
d	O
,	O
e|c	O
)	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
)	O
p	O
(	O
f|c	O
)	O
p	O
(	O
h|e	O
)	O
(	O
6.8.2	O
)	O
(	O
6.8.3	O
)	O
this	O
can	O
be	O
represented	O
using	O
a	O
so-called	O
set	B
chain	I
[	O
170	O
]	O
in	O
ﬁg	O
(	O
6.11c	O
)	O
(	O
set	O
chains	O
generalise	O
belief	B
networks	I
to	O
a	O
product	O
of	O
clusters	O
of	O
variables	O
conditioned	O
on	O
parents	B
)	O
.	O
by	O
writing	O
each	O
of	O
the	O
set	O
conditional	O
probabilities	O
as	O
local	O
conditional	B
bns	O
,	O
one	O
may	O
also	O
write	O
full	O
bn	O
.	O
for	O
example	O
,	O
one	O
such	O
would	O
be	O
given	O
from	O
the	O
decomposition	B
p	O
(	O
c|a	O
,	O
b	O
)	O
p	O
(	O
b|a	O
)	O
p	O
(	O
a	O
)	O
p	O
(	O
g|e	O
)	O
p	O
(	O
f|c	O
)	O
p	O
(	O
h|e	O
)	O
p	O
(	O
d|e	O
,	O
c	O
)	O
p	O
(	O
e|c	O
)	O
102	O
(	O
6.8.4	O
)	O
draft	O
march	O
9	O
,	O
2010	O
code	O
d1	O
d2	O
d3	O
d4	O
d5	O
s1	O
s2	O
s3	O
figure	O
6.12	O
:	O
5	O
diseases	O
giving	O
rise	O
to	O
3	O
symptoms	O
.	O
assuming	O
the	O
symptoms	O
are	O
all	O
instantiated	O
,	O
the	O
triangulated	B
graph	O
of	O
the	O
diseases	O
is	O
a	O
5	O
clique	B
.	O
6.9	O
the	O
need	O
for	O
approximations	O
the	O
jta	O
provides	O
an	O
upper	O
bound	B
on	O
the	O
complexity	O
of	O
(	O
marginal	B
)	O
inference	B
and	O
attempts	O
to	O
exploit	O
the	O
structure	B
of	O
the	O
graph	B
to	O
reduce	O
computations	O
.	O
however	O
,	O
in	O
a	O
great	O
deal	O
of	O
interesting	O
applications	O
the	O
use	O
of	O
the	O
jta	O
algorithm	B
would	O
result	O
in	O
clique-sizes	O
in	O
the	O
triangulated	B
graph	O
that	O
are	O
prohibitively	O
large	O
.	O
a	O
classical	O
situation	O
in	O
which	O
this	O
can	O
arise	O
are	O
disease-symptom	O
networks	O
.	O
for	O
example	O
,	O
for	O
the	O
graph	B
in	O
ﬁg	O
(	O
6.12	O
)	O
,	O
the	O
triangulated	B
graph	O
of	O
the	O
diseases	O
is	O
fully	O
coupled	B
,	O
meaning	O
that	O
no	O
simpliﬁcation	O
can	O
occur	O
in	O
general	O
.	O
this	O
situation	O
is	O
common	O
in	O
such	O
bipartite	O
networks	O
,	O
even	O
when	O
the	O
children	B
only	O
have	O
a	O
small	O
number	O
of	O
parents	B
.	O
intuitively	O
,	O
as	O
one	O
eliminates	O
each	O
parent	O
,	O
links	O
are	O
added	O
between	O
other	O
parents	B
,	O
mediated	O
via	O
the	O
common	O
children	B
.	O
unless	O
the	O
graph	B
is	O
highly	O
regular	O
,	O
analogous	O
to	O
a	O
form	O
of	O
hidden	B
markov	O
model	B
,	O
this	O
ﬁll-in	O
eﬀect	O
rapidly	O
results	O
in	O
large	O
cliques	O
and	O
intractable	O
computations	O
.	O
dealing	O
with	O
large	O
clique	B
in	O
the	O
triangulated	B
graph	O
is	O
an	O
active	B
research	O
topic	O
and	O
we	O
’	O
ll	O
discuss	O
strategies	O
to	O
approximate	B
the	O
computations	O
in	O
chapter	O
(	O
28	O
)	O
.	O
6.9.1	O
bounded	O
width	O
junction	O
trees	O
in	O
some	O
applications	O
we	O
may	O
be	O
at	O
liberty	O
to	O
choose	O
the	O
structure	B
of	O
the	O
markov	O
network	O
.	O
for	O
example	O
,	O
if	O
we	O
wish	O
to	O
ﬁt	O
a	O
markov	O
network	O
to	O
data	B
,	O
we	O
may	O
wish	O
to	O
use	O
as	O
complex	O
a	O
markov	O
network	O
as	O
we	O
can	O
computationally	O
aﬀord	O
.	O
in	O
such	O
cases	O
we	O
desire	O
that	O
the	O
clique	B
sizes	O
of	O
the	O
resulting	O
triangulated	B
markov	O
network	O
are	O
smaller	O
than	O
a	O
speciﬁed	O
‘	O
tree	B
width	I
’	O
(	O
considering	O
the	O
corresponding	O
junction	B
tree	I
as	O
a	O
hypertree	O
)	O
.	O
constructing	O
such	O
bounded	O
width	O
or	O
‘	O
thin	B
’	O
junction	O
trees	O
is	O
an	O
active	B
research	O
topic	O
.	O
a	O
simple	O
way	O
to	O
do	O
this	O
is	O
to	O
start	O
with	O
a	O
graph	B
and	O
include	O
a	O
randomly	O
chosen	O
edge	O
provided	O
that	O
the	O
size	O
of	O
all	O
cliques	O
in	O
the	O
resulting	O
triangulated	B
graph	O
is	O
below	O
a	O
speciﬁed	O
maximal	O
width	O
.	O
see	O
demothinjt.m	O
and	O
makethinjt.m	O
which	O
assumes	O
an	O
initial	O
graph	B
g	O
and	O
a	O
graph	B
of	O
candidate	O
edges	O
c	O
,	O
iteratively	O
expanding	O
g	O
until	O
a	O
maximal	O
tree	B
width	I
limit	O
is	O
reached	O
.	O
see	O
also	O
[	O
11	O
]	O
for	O
a	O
discussion	O
on	O
learning	B
an	O
appropriate	O
markov	O
structure	B
based	O
on	O
data	B
.	O
6.10	O
code	O
absorb.m	O
:	O
absorption	B
update	O
v	O
→	O
s	O
→	O
w	O
absorption.m	O
:	O
full	O
absorption	B
schedule	O
over	O
tree	B
jtree.m	O
:	O
form	O
a	O
junction	B
tree	I
triangulate.m	O
:	O
triangulation	B
based	O
on	O
simple	O
node	O
elimination	O
6.10.1	O
utility	B
routines	O
knowing	O
if	O
an	O
undirected	B
graph	I
is	O
a	O
tree	B
,	O
and	O
returning	O
a	O
valid	O
elimination	O
sequence	O
is	O
useful	O
.	O
a	O
connected	B
graph	I
is	O
a	O
tree	B
if	O
the	O
number	O
of	O
edges	O
plus	O
1	O
is	O
equal	O
to	O
the	O
number	O
of	O
nodes	O
.	O
however	O
,	O
for	O
a	O
possibly	O
disconnected	B
graph	O
this	O
is	O
not	O
the	O
case	O
.	O
the	O
code	O
deals	O
with	O
the	O
possibly	O
disconnected	B
case	O
,	O
returning	O
a	O
valid	O
elimination	O
sequence	O
if	O
the	O
graph	B
is	O
singly-connected	B
.	O
the	O
routine	O
is	O
based	O
on	O
the	O
observation	O
that	O
any	O
singly-connected	B
graph	O
must	O
always	O
possess	O
a	O
simplical	B
node	O
which	O
can	O
be	O
eliminated	O
to	O
reveal	O
a	O
smaller	O
singly-connected	B
graph	O
.	O
istree.m	O
:	O
if	O
graph	B
is	O
singly	O
connected	B
return	O
1	O
and	O
elimination	O
sequence	O
elimtri.m	O
:	O
vertex/node	O
elimination	O
on	O
a	O
triangulated	B
graph	O
,	O
with	O
given	O
end	O
node	B
demojtree.m	O
:	O
junction	B
tree	I
:	O
chest	B
clinic	I
draft	O
march	O
9	O
,	O
2010	O
103	O
6.11	O
exercises	O
exercise	O
58.	O
show	O
that	O
the	O
markov	O
network	O
1	O
elimination	O
labelling	O
for	O
this	O
graph	B
.	O
4	O
2	O
exercise	O
59.	O
consider	O
the	O
following	O
distribution	B
:	O
exercises	O
3	O
is	O
not	O
perfect	O
elimination	O
ordered	O
and	O
give	O
a	O
perfect	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
=	O
φ	O
(	O
x1	O
,	O
x2	O
)	O
φ	O
(	O
x2	O
,	O
x3	O
)	O
φ	O
(	O
x3	O
,	O
x4	O
)	O
(	O
6.11.1	O
)	O
1.	O
draw	O
a	O
clique	B
graph	I
that	O
represents	O
this	O
distribution	B
and	O
indicate	O
the	O
separators	O
on	O
the	O
graph	B
.	O
2.	O
write	O
down	O
an	O
alternative	O
formula	O
for	O
the	O
distribution	B
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
in	O
terms	O
of	O
the	O
marginal	B
probabilities	O
p	O
(	O
x1	O
,	O
x2	O
)	O
,	O
p	O
(	O
x2	O
,	O
x3	O
)	O
,	O
p	O
(	O
x3	O
,	O
x4	O
)	O
,	O
p	O
(	O
x2	O
)	O
,	O
p	O
(	O
x3	O
)	O
exercise	O
60.	O
consider	O
the	O
distribution	B
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
=	O
φ	O
(	O
x1	O
,	O
x2	O
)	O
φ	O
(	O
x2	O
,	O
x3	O
)	O
φ	O
(	O
x3	O
,	O
x4	O
)	O
φ	O
(	O
x4	O
,	O
x1	O
)	O
(	O
6.11.2	O
)	O
1.	O
write	O
down	O
a	O
junction	B
tree	I
for	O
the	O
above	O
graph	B
.	O
2.	O
carry	O
out	O
the	O
absorption	B
procedure	O
and	O
demonstrate	O
that	O
this	O
gives	O
the	O
correct	O
result	O
for	O
the	O
marginal	B
p	O
(	O
x1	O
)	O
.	O
exercise	O
61.	O
consider	O
the	O
distribution	B
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
,	O
e	O
,	O
f	O
,	O
g	O
,	O
h	O
,	O
i	O
)	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
b|a	O
)	O
p	O
(	O
c|a	O
)	O
p	O
(	O
d|a	O
)	O
p	O
(	O
e|b	O
)	O
p	O
(	O
f|c	O
)	O
p	O
(	O
g|d	O
)	O
p	O
(	O
h|e	O
,	O
f	O
)	O
p	O
(	O
i|f	O
,	O
g	O
)	O
(	O
6.11.3	O
)	O
1.	O
draw	O
the	O
belief	B
network	I
for	O
this	O
distribution	B
.	O
2.	O
draw	O
the	O
moralised	O
graph	B
.	O
3.	O
draw	O
the	O
triangulated	B
graph	O
.	O
your	O
triangulated	B
graph	O
should	O
contain	O
cliques	O
of	O
the	O
smallest	O
size	O
possible	O
.	O
4.	O
draw	O
a	O
junction	B
tree	I
for	O
the	O
above	O
graph	B
and	O
verify	O
that	O
it	O
satisﬁes	O
the	O
running	B
intersection	I
property	I
.	O
5.	O
describe	O
a	O
suitable	O
initialisation	O
of	O
clique	B
potentials	O
.	O
6.	O
describe	O
the	O
absorption	B
procedure	O
and	O
write	O
down	O
an	O
appropriate	O
message	B
updating	O
schedule	B
.	O
exercise	O
62.	O
this	O
question	O
concerns	O
the	O
distribution	B
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
,	O
e	O
,	O
f	O
)	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
b|a	O
)	O
p	O
(	O
c|b	O
)	O
p	O
(	O
d|c	O
)	O
p	O
(	O
e|d	O
)	O
p	O
(	O
f|a	O
,	O
e	O
)	O
1.	O
draw	O
the	O
belief	B
network	I
for	O
this	O
distribution	B
.	O
2.	O
draw	O
the	O
moralised	O
graph	B
.	O
(	O
6.11.4	O
)	O
3.	O
draw	O
the	O
triangulated	B
graph	O
.	O
your	O
triangulated	B
graph	O
should	O
contain	O
cliques	O
of	O
the	O
smallest	O
size	O
possible	O
.	O
4.	O
draw	O
a	O
junction	B
tree	I
for	O
the	O
above	O
graph	B
and	O
verify	O
that	O
it	O
satisﬁes	O
the	O
running	B
intersection	I
property	I
.	O
5.	O
describe	O
a	O
suitable	O
initialisation	O
of	O
clique	B
potentials	O
.	O
6.	O
describe	O
the	O
absorption	B
procedure	O
and	O
an	O
appropriate	O
message	B
updating	O
schedule	B
.	O
7.	O
show	O
that	O
the	O
distribution	B
can	O
be	O
expressed	O
in	O
the	O
form	O
p	O
(	O
a|f	O
)	O
p	O
(	O
b|a	O
,	O
c	O
)	O
p	O
(	O
c|a	O
,	O
d	O
)	O
p	O
(	O
d|a	O
,	O
e	O
)	O
p	O
(	O
e|a	O
,	O
f	O
)	O
p	O
(	O
f	O
)	O
104	O
(	O
6.11.5	O
)	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
exercise	O
63.	O
for	O
the	O
undirected	B
graph	I
on	O
the	O
square	O
lattice	O
as	O
shown	O
,	O
draw	O
a	O
triangulated	B
graph	O
with	O
the	O
smallest	O
clique	B
sizes	O
possible	O
.	O
exercise	O
64.	O
consider	O
a	O
binary	O
variable	O
markov	O
random	O
field	O
p	O
(	O
x	O
)	O
=	O
z−1	O
(	O
cid:81	O
)	O
i	O
>	O
j	O
φ	O
(	O
xi	O
,	O
xj	O
)	O
,	O
deﬁned	O
i	O
[	O
xi=xj	O
]	O
for	O
i	O
a	O
neighbour	B
of	O
j	O
on	O
the	O
lattice	O
on	O
the	O
n	O
×	O
n	O
lattice	O
with	O
φ	O
(	O
xi	O
,	O
xj	O
)	O
=	O
e	O
and	O
i	O
>	O
j.	O
a	O
naive	O
way	O
to	O
perform	O
inference	B
is	O
to	O
ﬁrst	O
stack	O
all	O
the	O
variables	O
in	O
the	O
tth	O
column	O
and	O
call	O
this	O
cluster	O
variable	B
xt	O
,	O
as	O
shown	O
.	O
the	O
resulting	O
graph	B
is	O
then	O
singly-connected	B
.	O
what	O
is	O
the	O
complexity	O
of	O
computing	O
the	O
normalisation	B
constant	I
based	O
on	O
this	O
cluster	O
representation	B
?	O
compute	O
log	O
z	O
for	O
n	O
=	O
10.	O
x1	O
x2	O
x3	O
exercise	O
65.	O
given	O
a	O
consistent	B
junction	O
tree	B
on	O
which	O
a	O
full	O
round	O
of	O
message	B
passing	I
has	O
occurred	O
,	O
explain	O
how	O
to	O
form	O
a	O
belief	B
network	I
from	O
the	O
junction	B
tree	I
.	O
exercise	O
66.	O
the	O
ﬁle	O
diseasenet.mat	O
contains	O
the	O
potentials	O
for	O
a	O
disease	O
bi-partite	O
belief	B
network	I
,	O
with	O
20	O
diseases	O
d1	O
,	O
.	O
.	O
.	O
,	O
d20	O
and	O
40	O
symptoms	O
,	O
s1	O
,	O
.	O
.	O
.	O
,	O
s40	O
.	O
each	O
disease	O
and	O
symptom	O
is	O
a	O
binary	O
variable	O
,	O
and	O
each	O
symptom	O
connects	O
to	O
3	O
parent	O
diseases	O
.	O
1.	O
using	O
the	O
brmltoolbox	O
,	O
construct	O
a	O
junction	B
tree	I
for	O
this	O
distribution	B
and	O
use	O
it	O
to	O
compute	O
all	O
the	O
marginals	O
of	O
the	O
symptoms	O
,	O
p	O
(	O
si	O
=	O
1	O
)	O
.	O
2.	O
on	O
most	O
standard	O
computers	O
,	O
computing	O
the	O
marginal	B
p	O
(	O
si	O
=	O
1	O
)	O
by	O
raw	O
summation	O
of	O
the	O
joint	B
distribution	O
is	O
computationally	O
infeasible	O
.	O
explain	O
how	O
to	O
compute	O
the	O
marginals	O
p	O
(	O
si	O
=	O
1	O
)	O
in	O
a	O
tractable	O
way	O
without	O
using	O
the	O
junction	B
tree	I
formalism	O
.	O
by	O
implementing	O
this	O
method	O
,	O
compare	O
it	O
with	O
the	O
results	O
from	O
the	O
junction	B
tree	I
algorithm	O
.	O
3.	O
consider	O
the	O
(	O
unlikely	O
)	O
scenario	O
in	O
which	O
all	O
the	O
40	O
symptom	O
variables	O
are	O
instantiated	O
.	O
using	O
the	O
junction	B
tree	I
,	O
estimate	O
an	O
upper	O
bound	B
on	O
the	O
number	O
of	O
seconds	O
that	O
computing	O
a	O
marginal	B
p	O
(	O
d1|s1:40	O
)	O
takes	O
,	O
assuming	O
that	O
for	O
a	O
two	O
clique	B
table	O
containing	O
s	O
(	O
joint	B
)	O
states	O
,	O
absorption	B
takes	O
o	O
(	O
sδ	O
)	O
seconds	O
,	O
for	O
an	O
unspeciﬁed	O
δ.	O
compare	O
this	O
estimate	O
with	O
the	O
time	O
required	O
to	O
compute	O
the	O
marginal	B
by	O
raw	O
summation	O
of	O
the	O
instantiated	O
belief	B
network	I
.	O
draft	O
march	O
9	O
,	O
2010	O
105	O
exercises	O
106	O
draft	O
march	O
9	O
,	O
2010	O
chapter	O
7	O
making	O
decisions	O
7.1	O
expected	O
utility	B
this	O
chapter	O
concerns	O
situations	O
in	O
which	O
decisions	O
need	O
to	O
be	O
taken	O
under	O
uncertainty	B
.	O
consider	O
the	O
following	O
scenario	O
:	O
you	O
are	O
asked	O
if	O
you	O
wish	O
to	O
take	O
a	O
bet	O
on	O
the	O
outcome	O
of	O
tossing	O
a	O
fair	O
coin	O
.	O
if	O
you	O
bet	O
and	O
win	O
,	O
you	O
gain	O
£100	O
.	O
if	O
you	O
bet	O
and	O
lose	O
,	O
you	O
lose	O
£200	O
.	O
if	O
you	O
don	O
’	O
t	O
bet	O
,	O
the	O
cost	O
to	O
you	O
is	O
zero	O
.	O
we	O
can	O
set	O
this	O
up	O
using	O
a	O
two	O
state	O
variable	B
x	O
,	O
with	O
dom	O
(	O
x	O
)	O
=	O
{	O
win	O
,	O
lose	O
}	O
,	O
a	O
decision	O
variable	O
d	O
with	O
dom	O
(	O
d	O
)	O
=	O
{	O
bet	O
,	O
no	O
bet	O
}	O
and	O
utilities	O
as	O
follows	O
:	O
u	O
(	O
win	O
,	O
bet	O
)	O
=	O
100	O
,	O
u	O
(	O
lose	O
,	O
bet	O
)	O
=	O
−200	O
,	O
u	O
(	O
win	O
,	O
no	O
bet	O
)	O
=	O
0	O
,	O
u	O
(	O
lose	O
,	O
no	O
bet	O
)	O
=	O
0	O
(	O
7.1.1	O
)	O
since	O
we	O
don	O
’	O
t	O
know	O
the	O
state	O
of	O
x	O
,	O
in	O
order	O
to	O
make	O
a	O
decision	O
about	O
whether	O
or	O
not	O
to	O
bet	O
,	O
arguably	O
the	O
best	O
we	O
can	O
do	O
is	O
work	O
out	O
our	O
expected	O
winnings/losses	O
under	O
the	O
situations	O
of	O
betting	O
and	O
not	O
betting	O
[	O
240	O
]	O
.	O
if	O
we	O
bet	O
,	O
we	O
would	O
expect	O
to	O
gain	O
u	O
(	O
bet	O
)	O
=	O
p	O
(	O
win	O
)	O
×	O
u	O
(	O
win	O
,	O
bet	O
)	O
+	O
p	O
(	O
lose	O
)	O
×	O
u	O
(	O
lose	O
,	O
bet	O
)	O
=	O
0.5	O
×	O
100	O
−	O
0.5	O
×	O
200	O
=	O
−50	O
if	O
we	O
don	O
’	O
t	O
bet	O
,	O
the	O
expected	O
gain	O
is	O
zero	O
,	O
u	O
(	O
no	O
bet	O
)	O
=	O
0.	O
based	O
on	O
taking	O
the	O
decision	O
which	O
maximises	O
expected	O
utility	B
,	O
we	O
would	O
therefore	O
be	O
advised	O
not	O
to	O
bet	O
.	O
deﬁnition	O
52	O
(	O
subjective	B
expected	O
utility	B
)	O
.	O
the	O
utility	B
of	O
a	O
decision	O
is	O
u	O
(	O
d	O
)	O
=	O
(	O
cid:104	O
)	O
u	O
(	O
d	O
,	O
x	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
)	O
where	O
p	O
(	O
x	O
)	O
is	O
the	O
distribution	B
of	O
the	O
outcome	O
x	O
and	O
d	O
represents	O
the	O
decision	O
.	O
(	O
7.1.2	O
)	O
7.1.1	O
utility	B
of	O
money	B
you	O
are	O
a	O
wealthy	O
individual	O
,	O
with	O
£1	O
,	O
000	O
,	O
000	O
in	O
your	O
bank	O
account	O
.	O
you	O
are	O
asked	O
if	O
you	O
would	O
like	O
to	O
participate	O
in	O
a	O
fair	O
coin	O
tossing	O
bet	O
in	O
which	O
,	O
if	O
you	O
win	O
,	O
your	O
bank	O
account	O
will	O
become	O
£1	O
,	O
000	O
,	O
000	O
,	O
000.	O
however	O
,	O
if	O
you	O
lose	O
,	O
your	O
bank	O
account	O
will	O
contain	O
only	O
£1000	O
.	O
assuming	O
the	O
coin	O
is	O
fair	O
,	O
should	O
you	O
take	O
the	O
bet	O
?	O
if	O
we	O
take	O
the	O
bet	O
our	O
expected	O
bank	O
balance	O
would	O
be	O
u	O
(	O
bet	O
)	O
=	O
0.5	O
×	O
1	O
,	O
000	O
,	O
000	O
,	O
000	O
+	O
0.5	O
×	O
1000	O
=	O
500	O
,	O
000	O
,	O
500.00	O
(	O
7.1.3	O
)	O
if	O
we	O
don	O
’	O
t	O
bet	O
,	O
our	O
bank	O
balance	O
will	O
remain	O
at	O
£1	O
,	O
000	O
,	O
000.	O
based	O
on	O
expected	O
utility	B
,	O
we	O
are	O
therefore	O
advised	O
to	O
take	O
the	O
bet	O
.	O
(	O
note	O
that	O
if	O
one	O
considers	O
instead	O
the	O
amount	O
one	O
will	O
win	O
or	O
lose	O
,	O
one	O
may	O
show	O
107	O
decision	O
trees	O
that	O
the	O
diﬀerence	O
in	O
expected	O
utility	B
between	O
betting	O
and	O
not	O
betting	O
is	O
the	O
same	O
,	O
exercise	O
(	O
73	O
)	O
)	O
.	O
whilst	O
the	O
above	O
is	O
a	O
correct	O
mathematical	O
derivation	O
,	O
few	O
people	O
who	O
are	O
millionaires	O
are	O
likely	O
to	O
be	O
willing	O
to	O
risk	B
losing	O
almost	O
everything	O
in	O
order	O
to	O
become	O
a	O
billionaire	O
.	O
this	O
means	O
that	O
the	O
subjective	B
utility	O
of	O
money	B
is	O
not	O
simply	O
the	O
quantity	O
of	O
money	B
.	O
in	O
order	O
to	O
better	O
reﬂect	O
the	O
situation	O
,	O
the	O
utility	B
of	O
money	B
would	O
need	O
to	O
be	O
a	O
non-linear	B
function	O
of	O
money	B
,	O
growing	O
slowly	O
for	O
large	O
quantities	O
of	O
money	B
and	O
decreasing	O
rapidly	O
for	O
small	O
quantities	O
of	O
money	B
,	O
exercise	O
(	O
68	O
)	O
.	O
7.2	O
decision	O
trees	O
decision	O
trees	O
are	O
a	O
way	O
to	O
graphically	O
organise	O
a	O
sequential	B
decision	O
process	O
.	O
a	O
decision	B
tree	I
contains	O
decision	O
nodes	O
,	O
each	O
with	O
branches	O
for	O
each	O
of	O
the	O
alternative	O
decisions	O
.	O
chance	O
nodes	O
(	O
random	O
variables	O
)	O
also	O
appear	O
in	O
the	O
tree	B
,	O
with	O
the	O
utility	B
of	O
each	O
branch	O
computed	O
at	O
the	O
leaf	O
of	O
each	O
branch	O
.	O
the	O
expected	O
utility	B
of	O
any	O
decision	O
can	O
then	O
be	O
computed	O
on	O
the	O
basis	O
of	O
the	O
weighted	O
summation	O
of	O
all	O
branches	O
from	O
the	O
decision	O
to	O
all	O
leaves	O
from	O
that	O
branch	O
.	O
example	O
29	O
(	O
party	O
)	O
.	O
consider	O
the	O
decision	O
problem	O
as	O
to	O
whether	O
or	O
not	O
to	O
go	O
ahead	O
with	O
a	O
fund-raising	O
garden	O
party	O
.	O
if	O
we	O
go	O
ahead	O
with	O
the	O
party	O
and	O
it	O
subsequently	O
rains	O
,	O
then	O
we	O
will	O
lose	O
money	B
(	O
since	O
very	O
few	O
people	O
will	O
show	O
up	O
)	O
;	O
on	O
the	O
other	O
hand	O
,	O
if	O
we	O
don	O
’	O
t	O
go	O
ahead	O
with	O
the	O
party	O
and	O
it	O
doesn	O
’	O
t	O
rain	O
we	O
’	O
re	O
free	O
to	O
go	O
and	O
do	O
something	O
else	O
fun	O
.	O
to	O
characterise	O
this	O
numerically	O
,	O
we	O
use	O
:	O
p	O
(	O
rain	O
=	O
rain	O
)	O
=	O
0.6	O
,	O
p	O
(	O
rain	O
=	O
no	O
rain	O
)	O
=	O
0.4	O
(	O
7.2.1	O
)	O
the	O
utility	B
is	O
deﬁned	O
as	O
u	O
(	O
party	O
,	O
rain	O
)	O
=	O
−100	O
,	O
u	O
(	O
party	O
,	O
no	O
rain	O
)	O
=	O
500	O
,	O
u	O
(	O
no	O
party	O
,	O
rain	O
)	O
=	O
0	O
,	O
u	O
(	O
no	O
party	O
,	O
no	O
rain	O
)	O
=	O
50	O
(	O
7.2.2	O
)	O
we	O
represent	O
this	O
situation	O
in	O
ﬁg	O
(	O
7.1	O
)	O
.	O
the	O
question	O
is	O
,	O
should	O
we	O
go	O
ahead	O
with	O
the	O
party	O
?	O
since	O
we	O
don	O
’	O
t	O
know	O
what	O
will	O
actually	O
happen	O
to	O
the	O
weather	O
,	O
we	O
compute	O
the	O
expected	O
utility	B
of	O
each	O
decision	O
:	O
u	O
(	O
party	O
,	O
rain	O
)	O
p	O
(	O
rain	O
)	O
=	O
−100	O
×	O
0.6	O
+	O
500	O
×	O
0.4	O
=	O
140	O
u	O
(	O
no	O
party	O
,	O
rain	O
)	O
p	O
(	O
rain	O
)	O
=	O
0	O
×	O
0.6	O
+	O
50	O
×	O
0.4	O
=	O
20	O
(	O
7.2.3	O
)	O
(	O
7.2.4	O
)	O
u	O
(	O
party	O
)	O
=	O
(	O
cid:88	O
)	O
u	O
(	O
no	O
party	O
)	O
=	O
(	O
cid:88	O
)	O
rain	O
rain	O
(	O
cid:88	O
)	O
rain	O
max	O
p	O
arty	O
based	O
on	O
expected	O
utility	B
,	O
we	O
are	O
therefore	O
advised	O
to	O
go	O
ahead	O
with	O
the	O
party	O
.	O
the	O
maximal	O
expected	O
utility	B
is	O
given	O
by	O
(	O
see	O
demodecparty.m	O
)	O
p	O
(	O
rain	O
)	O
u	O
(	O
p	O
arty	O
,	O
rain	O
)	O
=	O
140	O
(	O
7.2.5	O
)	O
example	O
30	O
(	O
party-friend	O
)	O
.	O
an	O
extension	O
of	O
the	O
party	O
problem	B
is	O
that	O
if	O
we	O
decide	O
not	O
to	O
go	O
ahead	O
with	O
the	O
party	O
,	O
we	O
have	O
the	O
opportunity	O
to	O
visit	O
a	O
friend	O
.	O
however	O
,	O
we	O
’	O
re	O
not	O
sure	O
if	O
this	O
friend	O
will	O
be	O
in	O
.	O
the	O
question	O
is	O
should	O
we	O
still	O
go	O
ahead	O
with	O
the	O
party	O
?	O
we	O
need	O
to	O
quantify	O
all	O
the	O
uncertainties	O
and	O
utilities	O
.	O
if	O
we	O
go	O
ahead	O
with	O
the	O
party	O
,	O
the	O
utilities	O
are	O
as	O
before	O
:	O
uparty	O
(	O
party	O
,	O
rain	O
)	O
=	O
−100	O
,	O
uparty	O
(	O
party	O
,	O
no	O
rain	O
)	O
=	O
500	O
108	O
(	O
7.2.6	O
)	O
draft	O
march	O
9	O
,	O
2010	O
decision	O
trees	O
y	O
e	O
s	O
(	O
0	O
.	O
6	O
)	O
(	O
0.4	O
)	O
no	O
rain	O
yes	O
p	O
arty	O
n	O
o	O
y	O
e	O
s	O
(	O
0	O
.	O
6	O
)	O
(	O
0.4	O
)	O
no	O
rain	O
−100	O
500	O
0	O
50	O
with	O
figure	O
7.1	O
:	O
a	O
decision	B
tree	I
containing	O
chance	O
nodes	O
(	O
denoted	O
with	O
ovals	O
)	O
,	O
decision	O
nodes	O
(	O
denoted	O
with	O
squares	O
)	O
and	O
utility	B
nodes	O
(	O
denoted	O
with	O
diamonds	O
)	O
.	O
note	O
that	O
a	O
decision	B
tree	I
is	O
not	O
a	O
graphical	O
representation	B
of	O
a	O
belief	B
network	I
with	O
additional	O
nodes	O
.	O
rather	O
,	O
a	O
decision	B
tree	I
is	O
an	O
explicit	O
enumeration	O
of	O
the	O
possible	O
choices	O
that	O
can	O
be	O
made	O
,	O
beginning	O
with	O
the	O
leftmost	O
decision	O
node	O
,	O
with	O
probabilities	O
on	O
the	O
links	O
out	O
of	O
‘	O
chance	O
’	O
nodes	O
.	O
p	O
(	O
rain	O
=	O
rain	O
)	O
=	O
0.6	O
,	O
p	O
(	O
rain	O
=	O
no	O
rain	O
)	O
=	O
0.4	O
(	O
7.2.7	O
)	O
if	O
we	O
decide	O
not	O
to	O
go	O
ahead	O
with	O
the	O
party	O
,	O
we	O
will	O
consider	O
going	O
to	O
visit	O
a	O
friend	O
.	O
in	O
making	O
the	O
decision	O
not	O
to	O
go	O
ahead	O
with	O
the	O
party	O
we	O
have	O
utilities	O
uparty	O
(	O
no	O
party	O
,	O
rain	O
)	O
=	O
0	O
,	O
uparty	O
(	O
no	O
party	O
,	O
no	O
rain	O
)	O
=	O
50	O
the	O
probability	B
that	O
the	O
friend	O
is	O
in	O
depends	O
on	O
the	O
weather	O
according	O
to	O
p	O
(	O
f	O
riend	O
=	O
in|rain	O
)	O
=	O
0.8	O
,	O
p	O
(	O
f	O
riend	O
=	O
in|no	O
rain	O
)	O
=	O
0.1	O
,	O
the	O
other	O
probabilities	O
are	O
determined	O
by	O
normalisation	B
.	O
we	O
additionally	O
have	O
uvisit	O
(	O
friend	O
in	O
,	O
visit	O
)	O
=	O
200	O
,	O
uvisit	O
(	O
friend	O
out	O
,	O
visit	O
)	O
=	O
−100	O
(	O
7.2.8	O
)	O
(	O
7.2.9	O
)	O
(	O
7.2.10	O
)	O
with	O
the	O
remaining	O
utilities	O
zero	O
.	O
the	O
two	O
sets	O
of	O
utilities	O
add	O
up	O
so	O
that	O
the	O
overall	O
utility	B
of	O
any	O
decision	O
sequence	O
is	O
uparty	O
+	O
uvisit	O
.	O
the	O
decision	B
tree	I
for	O
the	O
party-friend	O
problem	B
is	O
shown	O
is	O
ﬁg	O
(	O
7.2	O
)	O
.	O
for	O
each	O
decision	O
sequence	O
the	O
utility	B
of	O
that	O
sequence	O
is	O
given	O
at	O
the	O
corresponding	O
leaf	O
of	O
the	O
dt	O
.	O
note	O
that	O
the	O
leaves	O
contain	O
the	O
total	O
utility	B
uparty	O
+	O
uvisit	O
.	O
solving	B
the	O
dt	O
corresponds	O
to	O
ﬁnding	O
for	O
each	O
decision	O
node	O
the	O
maximal	O
expected	O
utility	B
possible	O
(	O
by	O
optimising	O
over	O
future	O
decisions	O
)	O
.	O
at	O
any	O
point	O
in	O
the	O
tree	B
choosing	O
that	O
action	O
which	O
leads	O
to	O
the	O
child	O
with	O
highest	O
expected	O
utility	B
will	O
lead	O
to	O
the	O
optimal	O
strategy	O
.	O
using	O
this	O
,	O
we	O
ﬁnd	O
that	O
the	O
optimal	O
expected	O
utility	B
has	O
value	B
140	O
and	O
is	O
given	O
by	O
going	O
ahead	O
with	O
the	O
party	O
,	O
see	O
demodecpartyfriend.m	O
.	O
•	O
in	O
dts	O
the	O
same	O
nodes	O
are	O
often	O
repeated	O
throughout	O
the	O
tree	B
.	O
for	O
a	O
longer	O
sequence	O
of	O
decisions	O
,	O
the	O
number	O
of	O
branches	O
in	O
the	O
tree	B
can	O
grow	O
exponentially	O
with	O
the	O
number	O
of	O
decisions	O
,	O
making	O
this	O
representation	B
impractical	O
.	O
•	O
in	O
this	O
example	O
the	O
dt	O
is	O
asymmetric	O
since	O
if	O
we	O
decide	O
to	O
go	O
ahead	O
with	O
the	O
party	O
we	O
will	O
not	O
visit	O
the	O
friend	O
,	O
curtailing	O
the	O
further	O
decisions	O
present	O
in	O
the	O
lower	O
half	O
of	O
the	O
tree	B
.	O
(	O
cid:88	O
)	O
rain	O
(	O
cid:88	O
)	O
f	O
riend	O
mathematically	O
,	O
we	O
can	O
express	O
the	O
optimal	O
expected	O
utility	B
u	O
for	O
the	O
party-visit	O
example	O
by	O
summing	O
over	O
un-revealed	O
variables	O
and	O
optimising	O
over	O
future	O
decisions	O
:	O
max	O
p	O
arty	O
p	O
(	O
rain	O
)	O
max	O
v	O
isit	O
p	O
(	O
f	O
riend|rain	O
)	O
[	O
uparty	O
(	O
p	O
arty	O
,	O
rain	O
)	O
+	O
uvisit	O
(	O
v	O
isit	O
,	O
f	O
riend	O
)	O
i	O
[	O
p	O
arty	O
=	O
no	O
]	O
]	O
(	O
7.2.11	O
)	O
where	O
the	O
term	O
i	O
[	O
p	O
arty	O
=	O
no	O
]	O
has	O
the	O
eﬀect	O
of	O
curtailing	O
the	O
dt	O
if	O
the	O
party	O
goes	O
ahead	O
.	O
to	O
answer	O
the	O
question	O
as	O
to	O
whether	O
or	O
not	O
to	O
go	O
ahead	O
with	O
the	O
party	O
,	O
we	O
take	O
that	O
state	O
of	O
p	O
arty	O
that	O
corresponds	O
to	O
draft	O
march	O
9	O
,	O
2010	O
109	O
-100	O
-100	O
s	O
e	O
y	O
p	O
arty	O
s	O
e	O
y	O
(	O
0.6	O
)	O
rain	O
(	O
0	O
.	O
4	O
)	O
n	O
o	O
500	O
yes	O
v	O
isit	O
n	O
o	O
s	O
e	O
y	O
(	O
0.6	O
)	O
rain	O
200	O
(	O
0.8	O
)	O
in	O
f	O
riend	O
(	O
0.2	O
)	O
o	O
u	O
t	O
−100	O
n	O
o	O
0	O
(	O
0.8	O
)	O
in	O
f	O
riend	O
(	O
0.2	O
)	O
o	O
u	O
t	O
0	O
250	O
(	O
0.1	O
)	O
in	O
(	O
0	O
.	O
4	O
)	O
n	O
o	O
f	O
riend	O
(	O
0.9	O
)	O
o	O
u	O
t	O
−50	O
yes	O
v	O
isit	O
50	O
n	O
o	O
(	O
0.1	O
)	O
in	O
f	O
riend	O
(	O
0.9	O
)	O
o	O
u	O
t	O
50	O
s	O
e	O
y	O
rain	O
(	O
140	O
)	O
s	O
e	O
y	O
n	O
o	O
500	O
200	O
in	O
p	O
arty	O
(	O
140	O
)	O
f	O
riend	O
(	O
140	O
)	O
o	O
u	O
t	O
−100	O
yes	O
v	O
isit	O
(	O
140	O
)	O
n	O
o	O
n	O
o	O
0	O
s	O
e	O
y	O
in	O
f	O
riend	O
(	O
0	O
)	O
rain	O
(	O
104	O
)	O
o	O
u	O
t	O
0	O
250	O
in	O
n	O
o	O
f	O
riend	O
(	O
−20	O
)	O
o	O
u	O
t	O
−50	O
yes	O
v	O
isit	O
(	O
50	O
)	O
n	O
o	O
50	O
in	O
f	O
riend	O
(	O
50	O
)	O
o	O
u	O
t	O
50	O
(	O
a	O
)	O
(	O
b	O
)	O
decision	O
trees	O
figure	O
7.2	O
:	O
solving	B
a	O
decision	B
tree	I
.	O
(	O
a	O
)	O
:	O
decision	B
tree	I
for	O
the	O
party-visit	O
,	O
(	O
b	O
)	O
:	O
solving	B
the	O
dt	O
cor-	O
example	O
(	O
30	O
)	O
.	O
responds	O
to	O
making	O
the	O
decision	O
with	O
the	O
highest	O
expected	O
future	O
utility	B
.	O
this	O
can	O
be	O
achieved	O
by	O
starting	O
at	O
the	O
leaves	O
(	O
util-	O
ities	O
)	O
.	O
for	O
a	O
chance	O
parent	O
node	B
x	O
,	O
the	O
utility	B
of	O
the	O
parent	O
is	O
the	O
expected	O
util-	O
ity	O
of	O
that	O
variable	B
.	O
for	O
example	O
,	O
at	O
the	O
top	O
of	O
the	O
dt	O
we	O
have	O
the	O
rain	O
vari-	O
able	O
with	O
the	O
children	B
−100	O
(	O
probability	B
0.6	O
)	O
and	O
500	O
(	O
probability	B
0.4	O
)	O
.	O
hence	O
the	O
expected	O
utility	B
of	O
the	O
rain	O
node	B
is	O
−100	O
×	O
0.6	O
+	O
500	O
×	O
0.4	O
=	O
140.	O
for	O
a	O
decision	O
node	O
,	O
the	O
value	B
of	O
the	O
node	B
is	O
the	O
optimum	O
of	O
its	O
child	O
values	O
.	O
one	O
re-	O
curses	O
thus	O
backwards	O
from	O
the	O
leaves	O
to	O
the	O
root	O
.	O
for	O
example	O
,	O
the	O
value	B
of	O
the	O
rain	O
chance	O
node	B
in	O
the	O
lower	O
branch	O
is	O
given	O
by	O
140	O
×	O
0.6	O
+	O
50	O
×	O
0.4	O
=	O
104.	O
the	O
optimal	O
decision	O
sequence	O
is	O
then	O
given	O
at	O
each	O
decision	O
node	O
by	O
ﬁnding	O
which	O
child	O
node	B
has	O
the	O
maximal	O
value	B
.	O
hence	O
the	O
overall	O
best	O
decision	O
is	O
to	O
decide	O
to	O
go	O
ahead	O
with	O
the	O
party	O
.	O
if	O
we	O
decided	O
not	O
to	O
do	O
so	O
,	O
and	O
it	O
does	O
not	O
rain	O
,	O
then	O
the	O
best	O
decision	O
we	O
could	O
take	O
would	O
be	O
to	O
not	O
visit	O
the	O
friend	O
(	O
which	O
has	O
an	O
expected	O
utility	B
of	O
50	O
)	O
.	O
a	O
more	O
compact	O
description	O
of	O
this	O
problem	B
is	O
given	O
by	O
the	O
inﬂuence	B
diagram	I
,	O
ﬁg	O
(	O
7.4	O
)	O
.	O
see	O
also	O
demodecpartyfriend.m	O
.	O
the	O
maximal	O
expected	O
utility	B
above	O
.	O
the	O
way	O
to	O
read	O
equation	B
(	O
7.2.11	O
)	O
is	O
to	O
start	O
from	O
the	O
last	O
decision	O
that	O
needs	O
to	O
be	O
taken	O
,	O
in	O
this	O
case	O
v	O
isit	O
.	O
when	O
we	O
are	O
at	O
the	O
v	O
isit	O
stage	O
we	O
assume	O
that	O
we	O
will	O
have	O
previously	O
made	O
a	O
decision	O
about	O
p	O
arty	O
and	O
also	O
will	O
have	O
observed	O
whether	O
or	O
not	O
is	O
is	O
raining	O
.	O
however	O
,	O
we	O
don	O
’	O
t	O
know	O
whether	O
or	O
not	O
our	O
friend	O
will	O
be	O
in	O
,	O
so	O
we	O
compute	O
the	O
expected	O
utility	B
by	O
averaging	O
over	O
this	O
unknown	O
.	O
we	O
then	O
take	O
the	O
optimal	O
decision	O
by	O
maximising	O
over	O
v	O
isit	O
.	O
subsequently	O
we	O
move	O
to	O
the	O
next-to-last	O
decision	O
,	O
assuming	O
that	O
what	O
we	O
will	O
do	O
in	O
the	O
future	O
is	O
optimal	O
.	O
since	O
in	O
the	O
future	O
we	O
will	O
have	O
taken	O
a	O
decision	O
under	O
the	O
uncertain	B
f	O
riend	O
variable	B
,	O
the	O
current	O
decision	O
can	O
then	O
be	O
taken	O
under	O
uncertainty	B
about	O
rain	O
and	O
maximising	O
this	O
expected	O
optimal	O
utility	B
over	O
p	O
arty	O
.	O
note	O
that	O
the	O
sequence	O
of	O
maximisations	O
and	O
summations	O
matters	O
–	O
changing	O
the	O
order	O
will	O
in	O
general	O
result	O
in	O
a	O
diﬀerent	O
problem	B
with	O
a	O
diﬀerent	O
expected	O
utility1	O
.	O
1if	O
one	O
only	O
had	O
a	O
sequence	O
of	O
summations	O
,	O
the	O
order	O
of	O
the	O
summations	O
is	O
irrelevant	O
–	O
likewise	O
for	O
the	O
case	O
of	O
all	O
maximisations	O
.	O
however	O
,	O
summation	O
and	O
maximisation	B
operators	O
do	O
not	O
in	O
general	O
commute	B
.	O
110	O
draft	O
march	O
9	O
,	O
2010	O
extending	O
bayesian	O
networks	O
for	O
decisions	O
p	O
arty	O
rain	O
u	O
tility	O
figure	O
7.3	O
:	O
an	O
inﬂuence	B
diagram	I
which	O
contains	O
random	O
vari-	O
ables	O
(	O
denoted	O
with	O
ovals/circles	O
)	O
decision	O
nodes	O
(	O
denoted	O
with	O
squares	O
)	O
and	O
utility	B
nodes	O
(	O
denoted	O
with	O
diamonds	O
)	O
.	O
con-	O
trasted	O
with	O
ﬁg	O
(	O
7.1	O
)	O
this	O
is	O
a	O
more	O
compact	O
representation	B
of	O
the	O
structure	B
of	O
the	O
problem	B
.	O
the	O
diagram	O
represents	O
the	O
ex-	O
pression	O
p	O
(	O
rain	O
)	O
u	O
(	O
party	O
,	O
rain	O
)	O
.	O
in	O
addition	O
the	O
diagram	O
denotes	O
an	O
ordering	O
of	O
the	O
variables	O
with	O
party	O
≺	O
rain	O
(	O
according	O
to	O
the	O
convention	O
given	O
by	O
equation	B
(	O
7.3.1	O
)	O
)	O
.	O
7.3	O
extending	O
bayesian	O
networks	O
for	O
decisions	O
an	O
inﬂuence	B
diagram	I
is	O
a	O
bayesian	O
network	O
with	O
additional	O
decision	O
nodes	O
and	O
utility	B
nodes	O
[	O
137	O
,	O
148	O
,	O
162	O
]	O
.	O
the	O
decision	O
nodes	O
have	O
no	O
associated	O
distribution	B
;	O
the	O
utility	B
nodes	O
are	O
deterministic	B
functions	O
of	O
their	O
parents	B
.	O
the	O
utility	B
and	O
decision	O
nodes	O
can	O
be	O
either	O
continuous	B
or	O
discrete	B
;	O
for	O
simplicity	O
,	O
in	O
the	O
examples	O
here	O
the	O
decisions	O
will	O
be	O
discrete	B
.	O
a	O
beneﬁt	O
of	O
decision	O
trees	O
is	O
that	O
they	O
are	O
general	O
and	O
explicitly	O
encode	O
the	O
utilities	O
and	O
probabilities	O
associated	O
with	O
each	O
decision	O
and	O
event	O
.	O
in	O
addition	O
,	O
we	O
can	O
readily	O
solve	O
small	O
decision	O
problems	O
using	O
decision	O
trees	O
.	O
however	O
,	O
when	O
the	O
sequence	O
of	O
decisions	O
increases	O
,	O
the	O
number	O
of	O
leaves	O
in	O
the	O
decision	B
tree	I
grows	O
and	O
representing	O
the	O
tree	B
can	O
become	O
an	O
exponentially	O
complex	O
problem	B
.	O
in	O
such	O
cases	O
it	O
can	O
be	O
useful	O
to	O
use	O
an	O
inﬂuence	B
diagram	I
(	O
id	O
)	O
.	O
an	O
id	O
states	O
which	O
information	O
is	O
required	O
in	O
order	O
to	O
make	O
each	O
decision	O
,	O
and	O
the	O
order	O
in	O
which	O
these	O
decisions	O
are	O
to	O
be	O
made	O
.	O
the	O
details	O
of	O
the	O
probabilities	O
and	O
rewards	O
are	O
not	O
speciﬁed	O
in	O
the	O
id	O
,	O
and	O
this	O
can	O
enable	O
a	O
more	O
compact	O
description	O
of	O
the	O
decision	O
problem	O
.	O
7.3.1	O
syntax	O
of	O
inﬂuence	B
diagrams	I
information	O
links	O
an	O
information	B
link	I
from	O
a	O
random	O
variable	O
into	O
a	O
decision	O
node	O
:	O
x	O
d	O
indicates	O
that	O
the	O
state	O
of	O
the	O
variable	B
x	O
will	O
be	O
known	O
before	O
decision	O
d	O
is	O
taken	O
.	O
information	O
links	O
from	O
another	O
decision	O
node	O
d	O
in	O
to	O
d	O
similarly	O
indicate	O
that	O
decision	O
d	O
is	O
known	O
before	O
decision	O
d	O
is	O
taken	O
.	O
we	O
use	O
a	O
dashed	O
link	O
to	O
denote	O
that	O
decision	O
d	O
is	O
not	O
functionally	O
related	O
to	O
its	O
parents	B
.	O
random	O
variables	O
random	O
variables	O
may	O
depend	O
on	O
the	O
states	O
of	O
parental	O
random	O
variables	O
(	O
as	O
in	O
belief	B
networks	I
)	O
,	O
but	O
also	O
decision	O
node	O
states	O
:	O
d	O
x	O
y	O
as	O
decisions	O
are	O
taken	O
,	O
the	O
states	O
of	O
some	O
random	O
variables	O
will	O
be	O
revealed	O
.	O
to	O
emphasise	O
this	O
we	O
typically	O
shade	O
a	O
node	B
to	O
denote	O
that	O
its	O
state	O
will	O
be	O
revealed	O
during	O
the	O
sequential	B
decision	O
process	O
.	O
utilities	O
a	O
utility	B
node	O
is	O
a	O
deterministic	B
function	O
of	O
its	O
parents	B
.	O
the	O
parents	B
can	O
be	O
either	O
random	O
variables	O
or	O
decision	O
nodes	O
.	O
in	O
the	O
party	O
example	O
,	O
the	O
bn	O
trivially	O
consists	O
of	O
a	O
single	O
node	B
,	O
and	O
the	O
inﬂuence	B
diagram	I
is	O
given	O
in	O
ﬁg	O
(	O
7.3	O
)	O
.	O
the	O
more	O
complex	O
party-friend	O
problem	B
is	O
depicted	O
in	O
ﬁg	O
(	O
7.4	O
)	O
.	O
the	O
id	O
generally	O
provides	O
a	O
more	O
compact	O
representation	B
of	O
the	O
structure	B
of	O
problem	B
than	O
a	O
dt	O
,	O
although	O
details	O
about	O
the	O
speciﬁc	O
probabilities	O
and	O
utilities	O
are	O
not	O
present	O
in	O
the	O
id	O
.	O
draft	O
march	O
9	O
,	O
2010	O
111	O
extending	O
bayesian	O
networks	O
for	O
decisions	O
p	O
arty	O
v	O
isit	O
rain	O
f	O
riend	O
uparty	O
uvisit	O
figure	O
7.4	O
:	O
an	O
inﬂuence	B
diagram	I
for	O
the	O
party-visit	O
prob-	O
lem	O
,	O
example	O
(	O
30	O
)	O
.	O
the	O
partial	O
ordering	O
is	O
p	O
arty∗	O
≺	O
rain	O
≺	O
v	O
isit∗	O
≺	O
f	O
riend	O
.	O
the	O
dashed-link	O
from	O
party	O
to	O
visit	O
is	O
not	O
strictly	O
necessary	O
but	O
retained	O
in	O
order	O
to	O
satisfy	O
the	O
conven-	O
tion	O
that	O
there	O
is	O
a	O
directed	B
path	O
connecting	O
all	O
decision	O
nodes	O
.	O
partial	O
ordering	O
an	O
id	O
deﬁnes	O
a	O
partial	O
ordering	O
of	O
the	O
nodes	O
.	O
we	O
begin	O
by	O
writing	O
those	O
variables	O
x0	O
whose	O
states	O
are	O
known	O
(	O
evidential	O
variables	O
)	O
before	O
the	O
ﬁrst	O
decision	O
d1	O
.	O
we	O
then	O
ﬁnd	O
that	O
set	O
of	O
variables	O
x1	O
whose	O
states	O
are	O
revealed	O
before	O
the	O
second	O
decision	O
d2	O
.	O
subsequently	O
the	O
set	O
of	O
variables	O
xt	O
is	O
revealed	O
before	O
decision	O
dt+1	O
.	O
the	O
remaining	O
fully-unobserved	O
variables	O
are	O
placed	O
at	O
the	O
end	O
of	O
the	O
ordering	O
:	O
x0	O
≺	O
d1	O
≺	O
x1	O
≺	O
d2	O
,	O
.	O
.	O
.	O
,	O
≺	O
xn−1	O
≺	O
dn	O
≺	O
xn	O
(	O
7.3.1	O
)	O
with	O
xk	O
being	O
the	O
variables	O
revealed	O
between	O
decision	O
dk	O
and	O
dk+1	O
.	O
the	O
term	O
‘	O
partial	O
’	O
refers	O
to	O
the	O
fact	O
that	O
there	O
is	O
no	O
order	O
implied	O
amongst	O
the	O
variables	O
within	O
the	O
set	O
xn	O
.	O
for	O
notational	O
clarity	O
,	O
at	O
points	O
below	O
we	O
will	O
indicate	O
decision	O
variables	O
with	O
∗	O
to	O
reinforce	O
that	O
we	O
maximise	O
over	O
these	O
variables	O
,	O
and	O
sum	O
over	O
the	O
non-starred	O
variables	O
.	O
where	O
the	O
sets	O
are	O
empty	O
we	O
omit	O
writing	O
them	O
.	O
for	O
example	O
,	O
in	O
∗	O
ﬁg	O
(	O
7.5a	O
)	O
the	O
ordering	O
is	O
t	O
est	O
∗	O
≺	O
oil	O
.	O
the	O
optimal	O
ﬁrst	O
decision	O
d1	O
is	O
determined	O
by	O
computing	O
≺	O
seismic	O
≺	O
drill	O
uj	O
(	O
pa	O
(	O
uj	O
)	O
)	O
(	O
7.3.2	O
)	O
(	O
cid:88	O
)	O
x1	O
(	O
cid:88	O
)	O
xn−1	O
(	O
cid:88	O
)	O
(	O
cid:89	O
)	O
xn	O
i∈i	O
p	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
(	O
cid:88	O
)	O
j∈j	O
max	O
dn	O
u	O
(	O
d1|x0	O
)	O
≡	O
max	O
d2	O
.	O
.	O
.	O
for	O
each	O
state	O
of	O
the	O
decision	O
d1	O
,	O
given	O
x0	O
.	O
in	O
equation	B
(	O
7.3.2	O
)	O
above	O
i	O
denotes	O
the	O
set	O
of	O
indices	O
for	O
the	O
random	O
variables	O
,	O
and	O
j	O
the	O
indices	O
for	O
the	O
utility	B
nodes	O
.	O
for	O
each	O
state	O
of	O
the	O
conditioning	B
variables	O
,	O
the	O
optimal	O
decision	O
d1	O
is	O
found	O
using	O
argmax	O
d1	O
u	O
(	O
d1|x0	O
)	O
(	O
7.3.3	O
)	O
remark	O
9	O
(	O
reading	O
oﬀ	O
the	O
partial	O
ordering	O
)	O
.	O
sometimes	O
it	O
can	O
be	O
tricky	O
to	O
read	O
the	O
partial	O
ordering	O
from	O
the	O
id	O
.	O
a	O
method	O
is	O
to	O
identify	O
the	O
ﬁrst	O
decision	O
d1	O
and	O
then	O
any	O
variables	O
x0	O
that	O
need	O
to	O
be	O
observed	O
to	O
make	O
that	O
decision	O
.	O
then	O
identify	O
the	O
next	O
decision	O
d2	O
and	O
the	O
variables	O
x1	O
that	O
are	O
revealed	O
after	O
decision	O
d1	O
is	O
taken	O
and	O
before	O
decision	O
d2	O
is	O
taken	O
,	O
etc	O
.	O
this	O
gives	O
the	O
partial	O
ordering	O
x0	O
≺	O
d1	O
≺	O
x1	O
≺	O
d2	O
,	O
.	O
.	O
..	O
place	O
any	O
unrevealed	O
variables	O
at	O
the	O
end	O
of	O
the	O
ordering	O
.	O
implicit	O
and	O
explicit	O
information	O
links	O
the	O
information	O
links	O
are	O
a	O
potential	B
source	O
of	O
confusion	O
.	O
an	O
information	B
link	I
speciﬁes	O
explicitly	O
which	O
quantities	O
are	O
known	O
before	O
that	O
decision	O
is	O
taken2	O
.	O
we	O
also	O
implicitly	O
assume	O
the	O
no	B
forgetting	I
principle	I
that	O
all	O
past	O
decisions	O
and	O
revealed	O
variables	O
are	O
available	O
at	O
the	O
current	O
decision	O
(	O
the	O
revealed	O
variables	O
are	O
necessarily	O
the	O
parents	B
of	O
all	O
past	O
decision	O
nodes	O
)	O
.	O
if	O
we	O
were	O
to	O
include	O
all	O
such	O
information	O
links	O
,	O
ids	O
would	O
get	O
potentially	O
rather	O
messy	O
.	O
in	O
ﬁg	O
(	O
7.5	O
)	O
,	O
both	O
explicit	O
and	O
implicit	O
information	O
links	O
are	O
demon-	O
strated	O
.	O
we	O
call	O
an	O
information	B
link	I
fundamental	O
if	O
its	O
removal	O
would	O
alter	O
the	O
partial	O
ordering	O
.	O
2some	O
authors	O
prefer	O
to	O
write	O
all	O
information	O
links	O
where	O
possible	O
,	O
and	O
others	O
prefer	O
to	O
leave	O
them	O
implicit	O
.	O
here	O
we	O
largely	O
take	O
the	O
implicit	O
approach	B
.	O
for	O
the	O
purposes	O
of	O
computation	O
,	O
all	O
that	O
is	O
required	O
is	O
a	O
partial	O
ordering	O
;	O
one	O
can	O
therefore	O
view	O
this	O
as	O
‘	O
basic	O
’	O
and	O
the	O
information	O
links	O
as	O
superﬁcial	O
(	O
see	O
[	O
69	O
]	O
)	O
.	O
112	O
draft	O
march	O
9	O
,	O
2010	O
extending	O
bayesian	O
networks	O
for	O
decisions	O
t	O
est	O
oil	O
u2	O
t	O
est	O
oil	O
u2	O
u1	O
seismic	O
drill	O
u1	O
seismic	O
drill	O
(	O
a	O
)	O
(	O
b	O
)	O
∗	O
figure	O
7.5	O
:	O
(	O
a	O
)	O
:	O
the	O
partial	O
ordering	O
is	O
t	O
est	O
≺	O
oil	O
.	O
the	O
explicit	O
information	O
links	O
from	O
t	O
est	O
to	O
seismic	O
and	O
from	O
seismic	O
to	O
drill	O
are	O
both	O
fundamental	O
in	O
the	O
sense	O
that	O
removing	O
either	O
results	O
in	O
a	O
diﬀerent	O
partial	O
ordering	O
.	O
the	O
shaded	O
node	B
emphasises	O
that	O
the	O
state	O
of	O
this	O
variable	B
will	O
be	O
revealed	O
during	O
the	O
sequential	B
decision	O
process	O
.	O
conversely	O
,	O
the	O
non-shaded	O
node	B
will	O
never	O
be	O
observed	O
.	O
(	O
b	O
)	O
:	O
based	O
on	O
the	O
id	O
in	O
(	O
a	O
)	O
,	O
there	O
is	O
an	O
implicit	O
link	O
from	O
t	O
est	O
to	O
drill	O
since	O
the	O
decision	O
about	O
t	O
est	O
is	O
taken	O
before	O
seismic	O
is	O
revealed	O
.	O
≺	O
seismic	O
≺	O
drill	O
∗	O
causal	B
consistency	I
for	O
an	O
inﬂuence	B
diagram	I
to	O
be	O
consistent	B
a	O
current	O
decision	O
can	O
not	O
aﬀect	O
the	O
past	O
.	O
this	O
means	O
that	O
any	O
random	O
variable	O
descendants	O
of	O
a	O
decision	O
d	O
in	O
the	O
id	O
must	O
come	O
later	O
in	O
the	O
partial	O
ordering	O
.	O
assuming	O
the	O
no-forgetting	O
principle	O
,	O
this	O
means	O
that	O
for	O
any	O
valid	O
id	O
there	O
must	O
be	O
a	O
directed	B
path	O
connecting	O
all	O
decisions	O
.	O
this	O
can	O
be	O
a	O
useful	O
check	B
on	O
the	O
consistency	O
of	O
an	O
id	O
.	O
asymmetry	B
ids	O
are	O
most	O
convenient	O
when	O
the	O
corresponding	O
dt	O
is	O
symmetric	O
.	O
however	O
,	O
some	O
forms	O
of	O
asymmetry	B
are	O
relatively	O
straightforward	O
to	O
deal	O
with	O
in	O
the	O
id	O
framework	O
.	O
for	O
our	O
party-visit	O
example	O
,	O
the	O
dt	O
is	O
asymmetric	O
.	O
however	O
,	O
this	O
is	O
easily	O
dealt	O
with	O
in	O
the	O
id	O
by	O
using	O
a	O
link	O
from	O
p	O
arty	O
to	O
uvisit	O
which	O
removes	O
the	O
contribution	O
from	O
uvisit	O
when	O
the	O
party	O
goes	O
ahead	O
.	O
more	O
complex	O
issues	O
arise	O
when	O
the	O
set	O
of	O
variables	O
than	O
can	O
be	O
observed	O
depends	O
on	O
the	O
decision	O
sequence	O
taken	O
.	O
in	O
this	O
case	O
the	O
dt	O
is	O
asymmetric	O
.	O
in	O
general	O
,	O
inﬂuence	B
diagrams	I
are	O
not	O
well	O
suited	O
to	O
modelling	B
such	O
asymmetries	O
,	O
although	O
some	O
eﬀects	O
can	O
be	O
mediated	O
either	O
by	O
careful	O
use	O
of	O
additional	O
variables	O
,	O
or	O
extending	O
the	O
id	O
notation	O
.	O
see	O
[	O
69	O
]	O
and	O
[	O
148	O
]	O
for	O
further	O
details	O
of	O
these	O
issues	O
and	O
possible	O
resolutions	O
.	O
example	O
31	O
(	O
should	O
i	O
do	O
a	O
phd	O
?	O
)	O
.	O
consider	O
a	O
decision	O
whether	O
or	O
not	O
to	O
do	O
phd	O
as	O
part	O
of	O
our	O
education	O
(	O
e	O
)	O
.	O
taking	O
a	O
phd	O
incurs	O
costs	O
,	O
uc	O
both	O
in	O
terms	O
of	O
fees	O
,	O
but	O
also	O
in	O
terms	O
of	O
lost	O
income	O
.	O
however	O
,	O
if	O
we	O
have	O
a	O
phd	O
,	O
we	O
are	O
more	O
likely	O
to	O
win	O
a	O
nobel	O
prize	O
(	O
p	O
)	O
,	O
which	O
would	O
certainly	O
be	O
likely	O
to	O
boost	O
our	O
income	O
(	O
i	O
)	O
,	O
subsequently	O
beneﬁtting	O
our	O
ﬁnances	O
(	O
ub	O
)	O
.	O
this	O
setup	O
is	O
depicted	O
in	O
ﬁg	O
(	O
7.6a	O
)	O
.	O
the	O
ordering	O
is	O
(	O
eliding	O
empty	O
sets	O
)	O
∗	O
e	O
≺	O
{	O
i	O
,	O
p	O
}	O
and	O
(	O
7.3.4	O
)	O
dom	O
(	O
e	O
)	O
=	O
(	O
do	O
phd	O
,	O
no	O
phd	O
)	O
,	O
dom	O
(	O
i	O
)	O
=	O
(	O
low	O
,	O
average	B
,	O
high	O
)	O
,	O
dom	O
(	O
p	O
)	O
=	O
(	O
prize	O
,	O
no	O
prize	O
)	O
(	O
7.3.5	O
)	O
the	O
probabilities	O
are	O
p	O
(	O
win	O
nobel	O
prize|no	O
phd	O
)	O
=	O
0.0000001	O
p	O
(	O
win	O
nobel	O
prize|do	O
phd	O
)	O
=	O
0.001	O
(	O
7.3.6	O
)	O
p	O
(	O
low|do	O
phd	O
,	O
no	O
prize	O
)	O
=	O
0.1	O
p	O
(	O
average|do	O
phd	O
,	O
no	O
prize	O
)	O
=	O
0.5	O
p	O
(	O
high|do	O
phd	O
,	O
no	O
prize	O
)	O
=	O
0.4	O
p	O
(	O
low|no	O
phd	O
,	O
no	O
prize	O
)	O
=	O
0.2	O
p	O
(	O
average|no	O
phd	O
,	O
no	O
prize	O
)	O
=	O
0.6	O
p	O
(	O
high|no	O
phd	O
,	O
no	O
prize	O
)	O
=	O
0.2	O
p	O
(	O
low|do	O
phd	O
,	O
prize	O
)	O
=	O
0.01	O
p	O
(	O
high|do	O
phd	O
,	O
prize	O
)	O
=	O
0.95	O
p	O
(	O
low|no	O
phd	O
,	O
prize	O
)	O
=	O
0.01	O
p	O
(	O
high|no	O
phd	O
,	O
prize	O
)	O
=	O
0.95	O
p	O
(	O
average|do	O
phd	O
,	O
prize	O
)	O
=	O
0.04	O
p	O
(	O
average|no	O
phd	O
,	O
prize	O
)	O
=	O
0.04	O
draft	O
march	O
9	O
,	O
2010	O
113	O
s	O
us	O
p	O
i	O
ub	O
e	O
uc	O
(	O
a	O
)	O
e	O
uc	O
p	O
i	O
ub	O
(	O
b	O
)	O
extending	O
bayesian	O
networks	O
for	O
decisions	O
figure	O
7.6	O
:	O
(	O
a	O
)	O
:	O
education	O
e	O
incurs	O
some	O
cost	O
,	O
but	O
also	O
gives	O
a	O
chance	O
to	O
win	O
a	O
pres-	O
tigious	O
science	O
prize	O
.	O
both	O
of	O
these	O
af-	O
fect	O
our	O
likely	O
incomes	O
,	O
with	O
correspond-	O
ing	O
long	O
term	O
ﬁnancial	B
beneﬁts	O
.	O
(	O
b	O
)	O
:	O
the	O
start-up	O
scenario	O
.	O
the	O
utilities	O
are	O
uc	O
(	O
do	O
phd	O
)	O
=	O
−50000	O
,	O
uc	O
(	O
no	O
phd	O
)	O
=	O
0	O
,	O
ub	O
(	O
low	O
)	O
=	O
100000	O
,	O
ub	O
(	O
average	B
)	O
=	O
200000	O
,	O
ub	O
(	O
high	O
)	O
=	O
500000	O
the	O
expected	O
utility	B
of	O
education	O
is	O
u	O
(	O
e	O
)	O
=	O
(	O
cid:88	O
)	O
i	O
,	O
p	O
p	O
(	O
i|e	O
,	O
p	O
)	O
p	O
(	O
p|e	O
)	O
[	O
uc	O
(	O
e	O
)	O
+	O
ub	O
(	O
i	O
)	O
]	O
(	O
7.3.7	O
)	O
(	O
7.3.8	O
)	O
(	O
7.3.9	O
)	O
(	O
7.3.10	O
)	O
so	O
that	O
u	O
(	O
do	O
phd	O
)	O
=	O
260174.000	O
,	O
whilst	O
not	O
taking	O
a	O
phd	O
is	O
u	O
(	O
no	O
phd	O
)	O
=	O
240000.0244	O
,	O
making	O
it	O
on	O
average	B
beneﬁcial	O
to	O
do	O
a	O
phd	O
.	O
see	O
demodecphd.m	O
.	O
example	O
32	O
(	O
phds	O
and	O
start-up	O
companies	O
)	O
.	O
inﬂuence	B
diagrams	I
are	O
particularly	O
useful	O
when	O
a	O
sequence	O
of	O
decisions	O
is	O
taken	O
.	O
for	O
example	O
,	O
in	O
ﬁg	O
(	O
7.6b	O
)	O
we	O
model	B
a	O
new	O
situation	O
in	O
which	O
someone	O
has	O
ﬁrst	O
decided	O
whether	O
or	O
not	O
to	O
take	O
a	O
phd	O
.	O
ten	O
years	O
later	O
in	O
their	O
career	O
they	O
decide	O
whether	O
or	O
not	O
to	O
make	O
a	O
start-up	O
company	O
.	O
this	O
decision	O
is	O
based	O
on	O
whether	O
or	O
not	O
they	O
won	O
the	O
nobel	O
prize	O
.	O
the	O
start-up	O
decision	O
is	O
modelled	O
by	O
s	O
with	O
dom	O
(	O
s	O
)	O
=	O
(	O
tr	O
,	O
fa	O
)	O
.	O
if	O
we	O
make	O
a	O
start-up	O
,	O
this	O
will	O
cost	O
some	O
money	B
in	O
terms	O
of	O
investment	O
.	O
however	O
,	O
the	O
potential	B
beneﬁt	O
in	O
terms	O
of	O
our	O
income	O
could	O
be	O
high	O
.	O
we	O
model	B
this	O
with	O
(	O
the	O
other	O
required	O
table	O
entries	O
being	O
taken	O
from	O
example	O
(	O
31	O
)	O
)	O
:	O
p	O
(	O
low|start	O
up	O
,	O
no	O
prize	O
)	O
=	O
0.1	O
p	O
(	O
low|no	O
start	O
up	O
,	O
no	O
prize	O
)	O
=	O
0.2	O
p	O
(	O
average|no	O
start	O
up	O
,	O
no	O
prize	O
)	O
=	O
0.6	O
p	O
(	O
high|no	O
start	O
up	O
,	O
no	O
prize	O
)	O
=	O
0.2	O
p	O
(	O
low|start	O
up	O
,	O
prize	O
)	O
=	O
0.005	O
p	O
(	O
low|no	O
start	O
up	O
,	O
prize	O
)	O
=	O
0.05	O
p	O
(	O
average|start	O
up	O
,	O
no	O
prize	O
)	O
=	O
0.5	O
p	O
(	O
average|start	O
up	O
,	O
prize	O
)	O
=	O
0.005	O
p	O
(	O
average|no	O
start	O
up	O
,	O
prize	O
)	O
=	O
0.15	O
p	O
(	O
high|start	O
up	O
,	O
no	O
prize	O
)	O
=	O
0.4	O
p	O
(	O
high|start	O
up	O
,	O
prize	O
)	O
=	O
0.99	O
p	O
(	O
high|no	O
start	O
up	O
,	O
prize	O
)	O
=	O
0.8	O
(	O
7.3.11	O
)	O
and	O
us	O
(	O
start	O
up	O
)	O
=	O
−200000	O
,	O
us	O
(	O
no	O
start	O
up	O
)	O
=	O
0	O
(	O
7.3.12	O
)	O
our	O
interest	O
is	O
to	O
advise	O
whether	O
or	O
not	O
it	O
is	O
desirable	O
(	O
in	O
terms	O
of	O
expected	O
utility	B
)	O
to	O
take	O
a	O
phd	O
,	O
now	O
bearing	O
in	O
mind	O
that	O
later	O
one	O
may	O
or	O
may	O
not	O
win	O
the	O
nobel	O
prize	O
,	O
and	O
may	O
or	O
may	O
not	O
make	O
a	O
start-up	O
company	O
.	O
the	O
ordering	O
is	O
(	O
eliding	O
empty	O
sets	O
)	O
∗	O
e	O
≺	O
p	O
≺	O
s	O
∗	O
≺	O
i	O
114	O
(	O
7.3.13	O
)	O
draft	O
march	O
9	O
,	O
2010	O
solving	B
inﬂuence	O
diagrams	O
d1	O
x1	O
d2	O
x2	O
u2	O
d3	O
x3	O
u3	O
x4	O
u4	O
figure	O
7.7	O
:	O
markov	O
decision	O
process	O
.	O
these	O
can	O
be	O
used	O
to	O
model	B
planning	O
problems	O
of	O
the	O
form	O
‘	O
how	O
do	O
i	O
get	O
to	O
where	O
i	O
want	O
to	O
be	O
incurring	O
the	O
lowest	O
to-	O
tal	O
cost	O
?	O
’	O
.	O
they	O
are	O
readily	O
solvable	O
using	O
a	O
message	B
passing	I
algorithm	O
.	O
the	O
expected	O
optimal	O
utility	B
for	O
any	O
state	O
of	O
e	O
is	O
u	O
(	O
e	O
)	O
=	O
(	O
cid:88	O
)	O
max	O
s	O
p	O
(	O
cid:88	O
)	O
i	O
p	O
(	O
i|s	O
,	O
p	O
)	O
p	O
(	O
p|e	O
)	O
[	O
us	O
(	O
s	O
)	O
+	O
uc	O
(	O
e	O
)	O
+	O
ub	O
(	O
i	O
)	O
]	O
(	O
7.3.14	O
)	O
where	O
we	O
assume	O
that	O
the	O
optimal	O
decisions	O
are	O
taken	O
in	O
the	O
future	O
.	O
computing	O
the	O
above	O
,	O
we	O
ﬁnd	O
u	O
(	O
do	O
phd	O
)	O
=	O
190195.00	O
,	O
u	O
(	O
no	O
phd	O
)	O
=	O
240000.02	O
(	O
7.3.15	O
)	O
hence	O
,	O
we	O
are	O
better	O
oﬀ	O
not	O
doing	O
a	O
phd	O
.	O
see	O
demodecphd.m	O
.	O
7.4	O
solving	B
inﬂuence	O
diagrams	O
solving	B
an	O
inﬂuence	B
diagram	I
means	O
computing	O
the	O
optimal	O
decision	O
or	O
sequence	O
of	O
decisions	O
.	O
here	O
we	O
focus	O
on	O
ﬁnding	O
the	O
optimal	O
ﬁrst	O
decision	O
.	O
the	O
direct	O
approach	B
is	O
to	O
take	O
equation	B
(	O
7.3.2	O
)	O
and	O
perform	O
the	O
required	O
sequence	O
of	O
summations	O
and	O
maximisations	O
explicitly	O
.	O
however	O
,	O
we	O
may	O
be	O
able	O
to	O
exploit	O
the	O
structure	B
of	O
the	O
problem	B
to	O
for	O
computational	O
eﬃciency	O
.	O
to	O
develop	O
this	O
we	O
ﬁrst	O
derive	O
an	O
eﬃcient	B
algorithm	O
for	O
a	O
highly	O
structured	B
id	O
,	O
the	O
markov	O
decision	O
process	O
,	O
which	O
we	O
will	O
discuss	O
further	O
in	O
section	O
(	O
7.5	O
)	O
.	O
7.4.1	O
eﬃcient	B
inference	O
consider	O
the	O
following	O
function	B
from	O
the	O
id	O
of	O
ﬁg	O
(	O
7.7	O
)	O
φ	O
(	O
x4	O
,	O
x3	O
,	O
d3	O
)	O
φ	O
(	O
x3	O
,	O
x2	O
,	O
d2	O
)	O
φ	O
(	O
x2	O
,	O
x1	O
,	O
d1	O
)	O
(	O
u	O
(	O
x2	O
)	O
+	O
u	O
(	O
x3	O
)	O
+	O
u	O
(	O
x4	O
)	O
)	O
(	O
7.4.1	O
)	O
where	O
the	O
φ	O
represent	O
conditional	B
probabilities	O
and	O
the	O
u	O
are	O
utilities	O
.	O
we	O
write	O
this	O
in	O
terms	O
of	O
potentials	O
since	O
this	O
will	O
facilitate	O
the	O
generalisation	B
to	O
other	O
cases	O
.	O
our	O
task	O
is	O
to	O
take	O
the	O
optimal	O
ﬁrst	O
decision	O
,	O
based	O
on	O
the	O
expected	O
optimal	O
utility	B
φ	O
(	O
x4	O
,	O
x3	O
,	O
d3	O
)	O
φ	O
(	O
x3	O
,	O
x2	O
,	O
d2	O
)	O
φ	O
(	O
x2	O
,	O
x1	O
,	O
d1	O
)	O
(	O
u	O
(	O
x2	O
)	O
+	O
u	O
(	O
x3	O
)	O
+	O
u	O
(	O
x4	O
)	O
)	O
(	O
7.4.2	O
)	O
(	O
cid:88	O
)	O
x3	O
(	O
cid:88	O
)	O
x4	O
max	O
d2	O
max	O
d3	O
x2	O
whilst	O
we	O
could	O
carry	O
out	O
the	O
sequence	O
of	O
maximisations	O
and	O
summations	O
naively	O
,	O
our	O
interest	O
is	O
to	O
derive	O
a	O
computationally	O
eﬃcient	B
approach	O
.	O
let	O
’	O
s	O
see	O
how	O
to	O
distribute	O
these	O
operations	O
‘	O
by	O
hand	O
’	O
.	O
since	O
only	O
u	O
(	O
x4	O
)	O
depends	O
on	O
x4	O
explicitly	O
we	O
can	O
write	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
x3	O
x3	O
(	O
cid:88	O
)	O
x3	O
(	O
cid:88	O
)	O
x4	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
x4	O
x4	O
φ	O
(	O
x2	O
,	O
x1	O
,	O
d1	O
)	O
max	O
d2	O
φ	O
(	O
x2	O
,	O
x1	O
,	O
d1	O
)	O
max	O
d2	O
φ	O
(	O
x3	O
,	O
x2	O
,	O
d2	O
)	O
max	O
d3	O
φ	O
(	O
x4	O
,	O
x3	O
,	O
d3	O
)	O
u	O
(	O
x4	O
)	O
φ	O
(	O
x3	O
,	O
x2	O
,	O
d2	O
)	O
u	O
(	O
x3	O
)	O
max	O
d3	O
φ	O
(	O
x2	O
,	O
x1	O
,	O
d1	O
)	O
u	O
(	O
x2	O
)	O
max	O
d2	O
φ	O
(	O
x3	O
,	O
x2	O
,	O
d2	O
)	O
max	O
d3	O
φ	O
(	O
x4	O
,	O
x3	O
,	O
d3	O
)	O
φ	O
(	O
x4	O
,	O
x3	O
,	O
d3	O
)	O
(	O
7.4.3	O
)	O
115	O
u	O
(	O
d1	O
)	O
=	O
(	O
cid:88	O
)	O
u	O
(	O
d1	O
)	O
=	O
(	O
cid:88	O
)	O
+	O
(	O
cid:88	O
)	O
+	O
(	O
cid:88	O
)	O
x2	O
x2	O
x2	O
draft	O
march	O
9	O
,	O
2010	O
solving	B
inﬂuence	O
diagrams	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
x4	O
x4	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
x3	O
starting	O
with	O
the	O
ﬁrst	O
line	O
and	O
carrying	O
out	O
the	O
summation	O
over	O
x4	O
and	O
max	O
over	O
d3	O
,	O
this	O
gives	O
a	O
new	O
function	B
of	O
x3	O
,	O
u3←4	O
(	O
x3	O
)	O
≡	O
max	O
d3	O
φ	O
(	O
x4	O
,	O
x3	O
,	O
d3	O
)	O
u	O
(	O
x4	O
)	O
in	O
addition	O
we	O
deﬁne	O
the	O
message	B
(	O
which	O
in	O
our	O
particular	O
example	O
will	O
be	O
unity	O
)	O
(	O
7.4.4	O
)	O
(	O
7.4.5	O
)	O
φ3←4	O
(	O
x3	O
)	O
≡	O
max	O
using	O
this	O
we	O
can	O
write	O
d3	O
u	O
(	O
d1	O
)	O
=	O
(	O
cid:88	O
)	O
φ	O
(	O
x4	O
,	O
x3	O
,	O
d3	O
)	O
(	O
cid:88	O
)	O
x3	O
φ	O
(	O
x2	O
,	O
x1	O
,	O
d1	O
)	O
max	O
d2	O
x2	O
φ	O
(	O
x3	O
,	O
x2	O
,	O
d2	O
)	O
[	O
u	O
(	O
x3	O
)	O
φ3←4	O
(	O
x3	O
)	O
+	O
u3←4	O
(	O
x3	O
)	O
]	O
+	O
(	O
cid:88	O
)	O
x2	O
(	O
cid:88	O
)	O
x3	O
φ	O
(	O
x2	O
,	O
x1	O
,	O
d1	O
)	O
u	O
(	O
x2	O
)	O
max	O
d2	O
φ	O
(	O
x3	O
,	O
x2	O
,	O
d2	O
)	O
φ3←4	O
(	O
x3	O
)	O
(	O
7.4.6	O
)	O
now	O
we	O
carry	O
out	O
the	O
sum	O
over	O
x3	O
and	O
max	O
over	O
d2	O
for	O
the	O
ﬁrst	O
row	O
above	O
and	O
deﬁne	O
a	O
utility	B
message	O
u2←3	O
(	O
x2	O
)	O
≡	O
max	O
d2	O
x3	O
and	O
probability	B
message3	O
φ2←3	O
(	O
x2	O
)	O
≡	O
max	O
d2	O
φ	O
(	O
x3	O
,	O
x2	O
,	O
d2	O
)	O
[	O
u	O
(	O
x3	O
)	O
φ3←4	O
(	O
x3	O
)	O
+	O
u3←4	O
(	O
x3	O
)	O
]	O
φ	O
(	O
x3	O
,	O
x2	O
,	O
d2	O
)	O
φ3←4	O
(	O
x3	O
)	O
(	O
7.4.7	O
)	O
(	O
7.4.8	O
)	O
the	O
optimal	O
decision	O
for	O
d1	O
can	O
be	O
obtained	O
from	O
φ	O
(	O
x2	O
,	O
x1	O
,	O
d1	O
)	O
[	O
u	O
(	O
x2	O
)	O
φ2←3	O
(	O
x2	O
)	O
+	O
u2←3	O
(	O
x2	O
)	O
]	O
u	O
(	O
d1	O
)	O
=	O
(	O
cid:88	O
)	O
u	O
(	O
d1	O
)	O
=	O
(	O
cid:88	O
)	O
x2	O
x2	O
since	O
the	O
probability	B
message	O
φ2←3	O
(	O
x2	O
)	O
represents	O
information	O
about	O
the	O
distribution	B
passed	O
to	O
x2	O
via	O
x3	O
,	O
it	O
is	O
more	O
intuitive	O
to	O
write	O
(	O
cid:20	O
)	O
(	O
cid:21	O
)	O
φ	O
(	O
x2	O
,	O
x1	O
,	O
d1	O
)	O
φ2←3	O
(	O
x2	O
)	O
u	O
(	O
x2	O
)	O
+	O
u2←3	O
(	O
x2	O
)	O
φ2←3	O
(	O
x2	O
)	O
which	O
has	O
the	O
interpretation	O
of	O
the	O
average	B
of	O
a	O
utility	B
with	O
respect	O
to	O
a	O
distribution	B
.	O
it	O
is	O
intuitively	O
clear	O
that	O
we	O
can	O
continue	O
along	O
this	O
line	O
for	O
richer	O
structures	O
than	O
chains	O
.	O
indeed	O
,	O
provided	O
we	O
have	O
formed	O
an	O
appropriate	O
junction	B
tree	I
,	O
we	O
can	O
pass	O
potential	B
and	O
utility	B
messages	O
from	O
clique	B
to	O
neighbouring	O
clique	B
,	O
as	O
described	O
in	O
the	O
following	O
section	O
.	O
7.4.2	O
using	O
a	O
junction	B
tree	I
in	O
complex	O
ids	O
computational	O
eﬃciency	O
in	O
carrying	O
out	O
the	O
series	O
of	O
summations	O
and	O
maximisations	O
may	O
be	O
an	O
issue	O
and	O
one	O
therefore	O
seeks	O
to	O
exploit	O
structure	B
in	O
the	O
id	O
.	O
it	O
is	O
intuitive	O
that	O
some	O
form	O
of	O
junction	B
tree	I
style	O
algorithm	B
is	O
applicable	O
.	O
we	O
can	O
ﬁrst	O
represent	O
an	O
id	O
using	O
decision	O
potentials	O
which	O
consist	O
of	O
two	O
parts	O
,	O
as	O
deﬁned	O
below	O
.	O
deﬁnition	O
53	O
(	O
decision	B
potential	I
)	O
.	O
a	O
decision	B
potential	I
on	O
a	O
clique	B
c	O
contains	O
two	O
potentials	O
:	O
a	O
prob-	O
ability	O
potential	B
ρc	O
and	O
a	O
utility	B
potential	I
µc	O
.	O
the	O
joint	B
potentials	O
for	O
the	O
junction	B
tree	I
are	O
deﬁned	O
as	O
µc	O
(	O
7.4.9	O
)	O
ρ	O
=	O
(	O
cid:89	O
)	O
c∈c	O
ρc	O
,	O
µ	O
=	O
(	O
cid:88	O
)	O
c∈c	O
with	O
the	O
junction	B
tree	I
representing	O
the	O
term	O
ρµ	O
.	O
3for	O
our	O
mdp	O
example	O
all	O
these	O
probability	B
messages	O
are	O
unity	O
.	O
116	O
draft	O
march	O
9	O
,	O
2010	O
solving	B
inﬂuence	O
diagrams	O
in	O
this	O
case	O
there	O
are	O
constraints	O
on	O
the	O
triangulation	B
,	O
imposed	O
by	O
the	O
partial	O
ordering	O
which	O
restricts	O
the	O
variables	O
elimination	O
sequence	O
.	O
this	O
results	O
in	O
a	O
so-called	O
strong	B
junction	O
tree	B
.	O
the	O
treatment	O
here	O
is	O
inspired	O
by	O
[	O
146	O
]	O
;	O
a	O
related	O
approach	B
which	O
deals	O
with	O
more	O
general	O
chain	B
graphs	O
is	O
given	O
in	O
[	O
69	O
]	O
.	O
the	O
sequence	O
of	O
steps	O
required	O
to	O
construct	O
a	O
jt	O
for	O
an	O
id	O
is	O
as	O
follows	O
:	O
remove	O
information	O
edges	O
parental	O
links	O
of	O
decision	O
nodes	O
are	O
removed4	O
.	O
moralization	O
marry	O
all	O
parents	B
of	O
the	O
remaining	O
nodes	O
.	O
remove	O
utility	B
nodes	O
remove	O
the	O
utility	B
nodes	O
and	O
their	O
parental	O
links	O
.	O
strong	B
triangulation	I
form	O
a	O
triangulation	B
based	O
on	O
an	O
elimination	O
order	O
which	O
obeys	O
the	O
partial	O
or-	O
dering	O
of	O
the	O
variables	O
.	O
strong	B
junction	O
tree	B
from	O
the	O
strongly	O
triangulated	B
graph	O
,	O
form	O
a	O
junction	B
tree	I
and	O
orient	O
the	O
edges	O
towards	O
the	O
strong	B
root	O
(	O
the	O
clique	B
that	O
appears	O
last	O
in	O
the	O
elimination	O
sequence	O
)	O
.	O
the	O
cliques	O
are	O
ordered	O
according	O
to	O
the	O
sequence	O
in	O
which	O
they	O
are	O
eliminated	O
.	O
the	O
separator	B
probability	O
cliques	O
are	O
initialised	O
to	O
the	O
identity	O
,	O
with	O
the	O
separator	B
utilities	O
initialised	O
to	O
zero	O
.	O
the	O
probability	B
cliques	O
are	O
then	O
initialised	O
by	O
placing	O
conditional	B
probability	I
factors	O
into	O
the	O
lowest	O
available	O
clique	B
(	O
according	O
to	O
the	O
elimination	O
order	O
)	O
that	O
can	O
contain	O
them	O
,	O
and	O
similarly	O
for	O
the	O
utilities	O
.	O
remaining	O
probability	B
cliques	O
are	O
set	O
to	O
the	O
identity	O
and	O
utility	B
cliques	O
to	O
zero	O
.	O
example	O
33	O
(	O
junction	B
tree	I
)	O
.	O
an	O
example	O
of	O
a	O
junction	B
tree	I
for	O
an	O
id	O
is	O
given	O
in	O
ﬁg	O
(	O
7.8a	O
)	O
.	O
the	O
moralisation	B
and	O
triangulation	B
links	O
are	O
given	O
in	O
ﬁg	O
(	O
7.8b	O
)	O
.	O
the	O
orientation	O
of	O
the	O
edges	O
follows	O
the	O
partial	O
ordering	O
with	O
the	O
leaf	O
cliques	O
being	O
the	O
ﬁrst	O
to	O
disappear	O
under	O
the	O
sequence	O
of	O
summations	O
and	O
maximisations	O
.	O
a	O
by-product	O
of	O
the	O
above	O
steps	O
is	O
that	O
the	O
cliques	O
describe	O
the	O
fundamental	O
dependencies	O
on	O
previous	O
decisions	O
and	O
observations	O
.	O
in	O
ﬁg	O
(	O
7.8a	O
)	O
,	O
for	O
example	O
,	O
the	O
information	B
link	I
from	O
f	O
to	O
d2	O
is	O
not	O
present	O
in	O
the	O
moralised-triangulated	O
graph	B
ﬁg	O
(	O
7.8b	O
)	O
,	O
nor	O
in	O
the	O
associated	O
cliques	O
of	O
ﬁg	O
(	O
7.8c	O
)	O
.	O
this	O
is	O
because	O
once	O
e	O
is	O
revealed	O
,	O
the	O
utility	B
u4	O
is	O
independent	O
of	O
f	O
,	O
giving	O
rise	O
to	O
the	O
two-branch	O
structure	B
in	O
ﬁg	O
(	O
7.8b	O
)	O
.	O
nevertheless	O
,	O
the	O
information	B
link	I
from	O
f	O
to	O
d2	O
is	O
fundamental	O
since	O
it	O
speciﬁes	O
that	O
f	O
will	O
be	O
revealed	O
–	O
removing	O
this	O
link	O
would	O
therefore	O
change	O
the	O
partial	O
ordering	O
.	O
absorption	B
by	O
analogy	O
with	O
the	O
deﬁnition	O
of	O
messages	O
in	O
section	O
(	O
7.4.1	O
)	O
,	O
for	O
two	O
neighbouring	O
cliques	O
c1	O
and	O
c2	O
,	O
where	O
c1	O
is	O
closer	O
to	O
the	O
strong	B
root	O
of	O
the	O
jt	O
(	O
the	O
last	O
clique	B
deﬁned	O
through	O
the	O
elimination	O
order	O
)	O
,	O
we	O
deﬁne	O
∗	O
(	O
cid:88	O
)	O
c2\s	O
ρs	O
=	O
ρc2	O
,	O
µs	O
=	O
∗	O
(	O
cid:88	O
)	O
c2\s	O
ρc2µc2	O
(	O
7.4.10	O
)	O
(	O
7.4.11	O
)	O
ρnew	O
c1	O
=	O
ρc1ρs	O
,	O
in	O
the	O
above	O
(	O
cid:80	O
)	O
∗	O
c1	O
=	O
µc1	O
+	O
µs	O
µnew	O
ρs	O
c	O
is	O
a	O
‘	O
generalised	B
marginalisation	O
’	O
operation	O
–	O
it	O
sums	O
over	O
those	O
elements	O
of	O
clique	B
c	O
which	O
are	O
random	O
variables	O
and	O
maximises	O
over	O
the	O
decision	O
variables	O
in	O
the	O
clique	B
.	O
the	O
order	O
of	O
this	O
sequence	O
of	O
sums	O
and	O
maximisations	O
follows	O
the	O
partial	O
ordering	O
deﬁned	O
by	O
≺	O
.	O
absorption	B
is	O
then	O
computed	O
from	O
the	O
leaves	O
inwards	O
to	O
the	O
root	O
of	O
the	O
strong	B
junction	O
tree	B
.	O
the	O
optimal	O
setting	O
of	O
a	O
decision	O
d1	O
can	O
then	O
be	O
computed	O
from	O
the	O
root	O
clique	B
.	O
subsequently	O
backtracking	B
may	O
be	O
4note	O
that	O
for	O
the	O
case	O
in	O
which	O
the	O
domain	B
is	O
dependent	O
on	O
the	O
parental	O
variables	O
,	O
such	O
links	O
must	O
remain	O
.	O
draft	O
march	O
9	O
,	O
2010	O
117	O
l	O
u4	O
u3	O
a	O
b	O
d1	O
c	O
d	O
d4	O
i	O
h	O
d3	O
j	O
k	O
u2	O
g	O
d2	O
(	O
a	O
)	O
a	O
b	O
d1	O
u1	O
c	O
d	O
e	O
f	O
b	O
,	O
c	O
,	O
a	O
b	O
,	O
c	O
solving	B
inﬂuence	O
diagrams	O
d4	O
i	O
h	O
d3	O
l	O
j	O
k	O
g	O
d2	O
e	O
f	O
(	O
b	O
)	O
b	O
,	O
e	O
,	O
d	O
,	O
c	O
e	O
,	O
d2	O
,	O
g	O
d2	O
,	O
g	O
d2	O
,	O
g	O
,	O
d4	O
,	O
i	O
d4	O
,	O
i	O
d4	O
,	O
i	O
,	O
l	O
b	O
,	O
e	O
,	O
d	O
b	O
,	O
d1	O
,	O
e	O
,	O
f	O
,	O
d	O
e	O
f	O
f	O
,	O
d3	O
,	O
h	O
d3	O
,	O
h	O
d3	O
,	O
h	O
,	O
k	O
h	O
,	O
k	O
h	O
,	O
k	O
,	O
j	O
(	O
c	O
)	O
figure	O
7.8	O
:	O
(	O
a	O
)	O
:	O
inﬂuence	B
diagram	I
,	O
adapted	O
from	O
[	O
146	O
]	O
.	O
causal	B
consistency	I
is	O
satisﬁed	O
since	O
there	O
is	O
a	O
directed	B
path	O
linking	O
the	O
all	O
decisions	O
in	O
sequence	O
.	O
the	O
partial	O
ordering	O
is	O
b	O
≺	O
d1	O
≺	O
(	O
e	O
,	O
f	O
)	O
≺	O
d2	O
≺	O
(	O
·	O
)	O
≺	O
d3	O
≺	O
g	O
≺	O
d4	O
≺	O
(	O
a	O
,	O
c	O
,	O
d	O
,	O
h	O
,	O
i	O
,	O
j	O
,	O
k	O
,	O
l	O
)	O
.	O
(	O
b	O
)	O
:	O
moralised	O
and	O
strongly	O
triangulated	B
graph	O
.	O
moralisation	B
links	O
are	O
in	O
green	O
,	O
strong	B
triangulation	I
links	O
are	O
in	O
red	O
.	O
(	O
c	O
)	O
:	O
strong	B
junction	O
tree	B
.	O
absorption	B
passes	O
information	O
from	O
the	O
leaves	O
of	O
the	O
tree	B
towards	O
the	O
root	O
.	O
applied	O
to	O
infer	O
the	O
optimal	O
decision	O
trajectory	O
.	O
the	O
optimal	O
decision	O
for	O
d	O
can	O
be	O
obtained	O
by	O
working	O
with	O
the	O
clique	B
containing	O
d	O
which	O
is	O
closest	O
to	O
the	O
strong	B
root	O
and	O
setting	O
any	O
previously	O
taken	O
decisions	O
and	O
revealed	O
observations	O
into	O
their	O
evidential	O
states	O
.	O
see	O
demodecasia.m	O
for	O
an	O
example	O
.	O
example	O
34	O
(	O
absorption	B
on	O
a	O
chain	B
)	O
.	O
for	O
the	O
id	O
of	O
ﬁg	O
(	O
7.7	O
)	O
,	O
the	O
moralisation	B
and	O
triangulation	B
steps	O
are	O
trivial	O
and	O
give	O
the	O
jt	O
:	O
3	O
:	O
x1	O
,	O
x2	O
,	O
d1	O
x2	O
2	O
:	O
x2	O
,	O
x3	O
,	O
d2	O
x3	O
1	O
:	O
x3	O
,	O
x4	O
,	O
d3	O
where	O
the	O
cliques	O
are	O
indexed	O
according	O
the	O
elimination	O
order	O
.	O
the	O
probability	B
and	O
utility	B
cliques	O
are	O
initialised	O
to	O
ρ3	O
(	O
x1	O
,	O
x2	O
,	O
d1	O
)	O
=	O
p	O
(	O
x2|x1	O
,	O
d1	O
)	O
µ3	O
(	O
x1	O
,	O
x2	O
,	O
d1	O
)	O
=	O
0	O
ρ2	O
(	O
x2	O
,	O
x3	O
,	O
d2	O
)	O
=	O
p	O
(	O
x3|x2	O
,	O
d2	O
)	O
µ2	O
(	O
x2	O
,	O
x3	O
,	O
d2	O
)	O
=	O
u	O
(	O
x2	O
)	O
ρ1	O
(	O
x3	O
,	O
x4	O
,	O
d3	O
)	O
=	O
p	O
(	O
x4|x3	O
,	O
d3	O
)	O
µ1	O
(	O
x3	O
,	O
x4	O
,	O
d3	O
)	O
=	O
u	O
(	O
x3	O
)	O
+	O
u	O
(	O
x4	O
)	O
(	O
7.4.12	O
)	O
118	O
draft	O
march	O
9	O
,	O
2010	O
updating	O
the	O
separator	B
we	O
have	O
the	O
new	O
probability	B
potential	I
ρ1	O
(	O
x3	O
,	O
x4	O
,	O
d3	O
)	O
=	O
1	O
solving	O
inﬂuence	B
diagrams	I
with	O
the	O
separator	B
cliques	O
initialised	O
to	O
ρ1−2	O
(	O
x3	O
)	O
=	O
1	O
µ1−2	O
(	O
x3	O
)	O
=	O
0	O
ρ2−3	O
(	O
x2	O
)	O
=	O
1	O
µ2−3	O
(	O
x2	O
)	O
=	O
0	O
ρ1−2	O
(	O
x3	O
)	O
∗	O
=	O
max	O
d3	O
and	O
utility	B
potential	I
µ1−2	O
(	O
x3	O
)	O
∗	O
=	O
max	O
d3	O
=	O
max	O
d3	O
x4	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
(	O
cid:32	O
)	O
u	O
(	O
x3	O
)	O
+	O
(	O
cid:88	O
)	O
x4	O
x4	O
(	O
cid:88	O
)	O
x4	O
p	O
(	O
x4|x3	O
,	O
d3	O
)	O
(	O
u	O
(	O
x3	O
)	O
+	O
u	O
(	O
x4	O
)	O
)	O
(	O
7.4.15	O
)	O
ρ1	O
(	O
x3	O
,	O
x4	O
,	O
d3	O
)	O
µ1	O
(	O
x3	O
,	O
x4	O
,	O
d3	O
)	O
=	O
max	O
d3	O
(	O
cid:33	O
)	O
p	O
(	O
x4|x3	O
,	O
d3	O
)	O
u	O
(	O
x4	O
)	O
(	O
7.4.16	O
)	O
(	O
7.4.17	O
)	O
(	O
cid:33	O
)	O
p	O
(	O
x4|x3	O
,	O
d3	O
)	O
u	O
(	O
x4	O
)	O
(	O
7.4.18	O
)	O
(	O
7.4.13	O
)	O
(	O
7.4.14	O
)	O
(	O
7.4.19	O
)	O
(	O
7.4.20	O
)	O
(	O
7.4.21	O
)	O
(	O
7.4.22	O
)	O
(	O
7.4.23	O
)	O
(	O
7.4.24	O
)	O
(	O
7.4.25	O
)	O
at	O
the	O
next	O
step	O
we	O
update	O
the	O
probability	B
potential	I
ρ2	O
(	O
x2	O
,	O
x3	O
,	O
d2	O
)	O
∗	O
=	O
ρ2	O
(	O
x2	O
,	O
x3	O
,	O
d2	O
)	O
ρ1−2	O
(	O
x3	O
)	O
∗	O
=	O
1	O
and	O
utility	B
potential	I
µ2	O
(	O
x2	O
,	O
x3	O
,	O
d2	O
)	O
∗	O
=	O
µ2	O
(	O
x2	O
,	O
x3	O
,	O
d2	O
)	O
+	O
µ1−2	O
(	O
x3	O
)	O
∗	O
ρ1−2	O
(	O
x3	O
)	O
=	O
u	O
(	O
x2	O
)	O
+	O
max	O
d3	O
(	O
cid:32	O
)	O
u	O
(	O
x3	O
)	O
+	O
(	O
cid:88	O
)	O
x4	O
the	O
next	O
separator	B
decision	O
potential	B
is	O
ρ2−3	O
(	O
x2	O
)	O
∗	O
=	O
max	O
d2	O
µ2−3	O
(	O
x2	O
)	O
∗	O
=	O
max	O
d2	O
=	O
max	O
d2	O
ρ2	O
(	O
x2	O
,	O
x3	O
,	O
d2	O
)	O
∗	O
=	O
1	O
ρ2	O
(	O
x2	O
,	O
x3	O
,	O
d2	O
)	O
µ2	O
(	O
x2	O
,	O
x3	O
,	O
d2	O
)	O
∗	O
(	O
cid:32	O
)	O
u	O
(	O
x3	O
)	O
+	O
(	O
cid:88	O
)	O
(	O
cid:32	O
)	O
u	O
(	O
x2	O
)	O
+	O
max	O
d3	O
p	O
(	O
x3|x2	O
,	O
d2	O
)	O
x4	O
(	O
cid:33	O
)	O
(	O
cid:33	O
)	O
p	O
(	O
x4|x3	O
,	O
d3	O
)	O
u	O
(	O
x4	O
)	O
x3	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
x3	O
x3	O
finally	O
we	O
end	O
up	O
with	O
the	O
root	O
decision	B
potential	I
ρ3	O
(	O
x1	O
,	O
x2	O
,	O
d1	O
)	O
∗	O
=	O
ρ3	O
(	O
x1	O
,	O
x2	O
,	O
d1	O
)	O
ρ2−3	O
(	O
x2	O
)	O
∗	O
=	O
p	O
(	O
x2|x1	O
,	O
d1	O
)	O
and	O
µ3	O
(	O
x1	O
,	O
x2	O
,	O
d1	O
)	O
∗	O
=	O
µ3	O
(	O
x2	O
,	O
x1	O
,	O
d1	O
)	O
+	O
µ2−3	O
(	O
x2	O
)	O
∗	O
(	O
cid:32	O
)	O
ρ2−3	O
(	O
x2	O
)	O
∗	O
=	O
max	O
d2	O
p	O
(	O
x3|x2	O
,	O
d2	O
)	O
u	O
(	O
x2	O
)	O
+	O
max	O
d3	O
(	O
cid:88	O
)	O
x3	O
(	O
cid:32	O
)	O
u	O
(	O
x3	O
)	O
+	O
(	O
cid:88	O
)	O
x4	O
(	O
cid:33	O
)	O
(	O
cid:33	O
)	O
p	O
(	O
x4|x3	O
,	O
d3	O
)	O
u	O
(	O
x4	O
)	O
from	O
the	O
ﬁnal	O
decision	B
potential	I
we	O
have	O
the	O
expression	O
ρ3	O
(	O
x1	O
,	O
x2	O
,	O
d1	O
)	O
∗	O
µ3	O
(	O
x1	O
,	O
x2	O
,	O
d1	O
)	O
∗	O
which	O
is	O
equivalent	B
to	O
that	O
which	O
would	O
be	O
obtained	O
by	O
simply	O
distributing	O
the	O
summations	O
and	O
maximi-	O
sations	O
over	O
the	O
original	O
id	O
.	O
at	O
least	O
for	O
this	O
special	O
case	O
,	O
we	O
therefore	O
have	O
veriﬁed	O
that	O
the	O
jt	O
approach	B
yields	O
the	O
correct	O
root	O
clique	B
potentials	O
.	O
draft	O
march	O
9	O
,	O
2010	O
119	O
markov	O
decision	O
processes	O
7.5	O
markov	O
decision	O
processes	O
consider	O
a	O
markov	O
chain	B
with	O
transition	O
probabilities	O
p	O
(	O
xt+1	O
=	O
j|xt	O
=	O
i	O
)	O
.	O
at	O
each	O
time	O
t	O
we	O
consider	O
an	O
action	O
(	O
decision	O
)	O
,	O
which	O
aﬀects	O
the	O
state	O
at	O
time	O
t	O
+	O
1.	O
we	O
describe	O
this	O
by	O
p	O
(	O
xt+1	O
=	O
i|xt	O
=	O
j	O
,	O
dt	O
=	O
k	O
)	O
(	O
7.5.1	O
)	O
associated	O
with	O
each	O
state	O
xt	O
=	O
i	O
is	O
a	O
utility	B
u	O
(	O
xt	O
=	O
i	O
)	O
,	O
and	O
is	O
schematically	O
depicted	O
in	O
ﬁg	O
(	O
7.7	O
)	O
.	O
one	O
use	O
of	O
such	O
an	O
environment	O
model	B
would	O
be	O
to	O
help	O
plan	O
a	O
sequence	O
of	O
actions	O
(	O
decisions	O
)	O
required	O
to	O
reach	O
a	O
goal	O
state	O
in	O
minimal	O
total	O
summed	O
cost	O
.	O
more	O
generally	O
one	O
could	O
consider	O
utilities	O
that	O
depend	O
on	O
transitions	O
and	O
decisions	O
,	O
u	O
(	O
xt+1	O
=	O
i	O
,	O
xt	O
=	O
j	O
,	O
dt	O
=	O
k	O
)	O
and	O
also	O
time	O
dependent	O
versions	O
of	O
all	O
of	O
these	O
,	O
pt	O
(	O
xt+1	O
=	O
i|xt	O
=	O
j	O
,	O
dt	O
=	O
k	O
)	O
,	O
ut	O
(	O
xt+1	O
=	O
i	O
,	O
xt	O
=	O
j	O
,	O
dt	O
=	O
k	O
)	O
.	O
we	O
’	O
ll	O
stick	O
with	O
the	O
time-independent	O
(	O
stationary	B
)	O
case	O
here	O
since	O
the	O
generalisations	O
are	O
conceptually	O
straightforward	O
at	O
the	O
expense	O
of	O
notational	O
complexity	O
.	O
mdps	O
can	O
be	O
used	O
to	O
solve	O
planning	B
tasks	O
such	O
as	O
how	O
can	O
one	O
get	O
to	O
a	O
desired	O
goal	O
state	O
as	O
quickly	O
as	O
possible	O
.	O
by	O
deﬁning	O
the	O
utility	B
of	O
being	O
in	O
the	O
goal	O
state	O
as	O
high	O
,	O
and	O
being	O
in	O
the	O
non-goal	O
state	O
as	O
a	O
low	O
value	O
,	O
at	O
each	O
time	O
t	O
,	O
we	O
have	O
a	O
utility	B
u	O
(	O
xt	O
)	O
of	O
being	O
in	O
state	O
xt	O
.	O
for	O
positive	O
utilities	O
,	O
the	O
total	O
utility	B
of	O
any	O
state-decision	O
path	B
x1	O
:	O
t	O
,	O
d1	O
:	O
t	O
is	O
deﬁned	O
as	O
(	O
assuming	O
we	O
know	O
the	O
initial	O
state	O
x1	O
)	O
and	O
the	O
probability	B
with	O
which	O
this	O
happens	O
is	O
given	O
by	O
t	O
(	O
cid:88	O
)	O
t=2	O
u	O
(	O
x1	O
:	O
t	O
)	O
≡	O
u	O
(	O
xt	O
)	O
t−1	O
(	O
cid:89	O
)	O
p	O
(	O
x2	O
:	O
t|x1	O
,	O
d1	O
:	O
t−1	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
t=1	O
u	O
(	O
d1	O
)	O
≡	O
max	O
d2	O
max	O
d3	O
x2	O
x3	O
x4	O
p	O
(	O
xt+1|xt	O
,	O
dt	O
)	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
.	O
.	O
.	O
max	O
dt−1	O
xt	O
at	O
time	O
t	O
=	O
1	O
we	O
want	O
to	O
make	O
that	O
decision	O
d1	O
that	O
will	O
lead	O
to	O
maximal	O
expected	O
total	O
utility	B
p	O
(	O
x2	O
:	O
t|x1	O
,	O
d1	O
:	O
t−1	O
)	O
u	O
(	O
x1	O
:	O
t	O
)	O
our	O
task	O
is	O
to	O
compute	O
u	O
(	O
d1	O
)	O
for	O
each	O
state	O
of	O
d1	O
and	O
then	O
choose	O
that	O
state	O
with	O
maximal	O
expected	O
total	O
utility	B
.	O
to	O
carry	O
out	O
the	O
summations	O
and	O
maximisations	O
eﬃciently	O
,	O
we	O
could	O
use	O
the	O
junction	B
tree	I
approach	O
,	O
as	O
described	O
in	O
the	O
previous	O
section	O
.	O
however	O
,	O
in	O
this	O
case	O
,	O
the	O
id	O
is	O
suﬃciently	O
simple	O
that	O
a	O
direct	O
message	B
passing	I
approach	O
can	O
be	O
used	O
to	O
compute	O
the	O
expected	O
utility	B
.	O
7.5.1	O
maximising	O
expected	O
utility	B
by	O
message	B
passing	I
(	O
7.5.2	O
)	O
(	O
7.5.3	O
)	O
(	O
7.5.4	O
)	O
(	O
7.5.5	O
)	O
(	O
7.5.6	O
)	O
(	O
7.5.7	O
)	O
(	O
7.5.8	O
)	O
consider	O
the	O
mdp	O
t−1	O
(	O
cid:89	O
)	O
t=1	O
p	O
(	O
xt+1|xt	O
,	O
dt	O
)	O
t	O
(	O
cid:88	O
)	O
t=2	O
u	O
(	O
xt	O
)	O
for	O
the	O
speciﬁc	O
example	O
in	O
ﬁg	O
(	O
7.7	O
)	O
the	O
joint	B
model	O
of	O
the	O
bn	O
and	O
utility	B
is	O
p	O
(	O
x4|x3	O
,	O
d3	O
)	O
p	O
(	O
x3|x2	O
,	O
d2	O
)	O
p	O
(	O
x2|x1	O
,	O
d1	O
)	O
(	O
u	O
(	O
x2	O
)	O
+	O
u	O
(	O
x3	O
)	O
+	O
u	O
(	O
x4	O
)	O
)	O
to	O
decide	O
on	O
how	O
to	O
take	O
the	O
ﬁrst	O
optimal	O
decision	O
,	O
we	O
need	O
to	O
compute	O
max	O
d2	O
max	O
d3	O
p	O
(	O
x4|x3	O
,	O
d3	O
)	O
p	O
(	O
x3|x2	O
,	O
d2	O
)	O
p	O
(	O
x2|x1	O
,	O
d1	O
)	O
(	O
u	O
(	O
x2	O
)	O
+	O
u	O
(	O
x3	O
)	O
+	O
u	O
(	O
x4	O
)	O
)	O
since	O
only	O
u	O
(	O
x4	O
)	O
depends	O
on	O
x4	O
explicitly	O
,	O
we	O
can	O
write	O
max	O
d3	O
p	O
(	O
x4|x3	O
,	O
d3	O
)	O
p	O
(	O
x3|x2	O
,	O
d2	O
)	O
p	O
(	O
x2|x1	O
,	O
d1	O
)	O
u	O
(	O
x4	O
)	O
p	O
(	O
x3|x2	O
,	O
d2	O
)	O
p	O
(	O
x2|x1	O
,	O
d1	O
)	O
u	O
(	O
x3	O
)	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
x4	O
x4	O
x2	O
u	O
(	O
d1	O
)	O
=	O
(	O
cid:88	O
)	O
u	O
(	O
d1	O
)	O
=	O
(	O
cid:88	O
)	O
+	O
(	O
cid:88	O
)	O
+	O
(	O
cid:88	O
)	O
x2	O
x2	O
x3	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
x3	O
x3	O
max	O
d2	O
max	O
d2	O
p	O
(	O
x2|x1	O
,	O
d1	O
)	O
u	O
(	O
x2	O
)	O
x2	O
120	O
draft	O
march	O
9	O
,	O
2010	O
(	O
cid:88	O
)	O
x4	O
p	O
(	O
x3|x2	O
,	O
d2	O
)	O
max	O
p	O
(	O
x3|x2	O
,	O
d2	O
)	O
u	O
(	O
x3	O
)	O
d3	O
p	O
(	O
x4|x3	O
,	O
d3	O
)	O
u	O
(	O
x4	O
)	O
(	O
7.5.9	O
)	O
we	O
now	O
start	O
with	O
the	O
ﬁrst	O
line	O
and	O
carry	O
out	O
the	O
summation	O
over	O
x4	O
and	O
maximisation	B
over	O
d3	O
.	O
this	O
gives	O
a	O
new	O
function	B
of	O
x3	O
,	O
markov	O
decision	O
processes	O
for	O
each	O
line	O
we	O
distribute	O
the	O
operations	O
:	O
u	O
(	O
d1	O
)	O
=	O
(	O
cid:88	O
)	O
+	O
(	O
cid:88	O
)	O
+	O
(	O
cid:88	O
)	O
x2	O
x2	O
x2	O
d2	O
p	O
(	O
x2|x1	O
,	O
d1	O
)	O
max	O
p	O
(	O
x2|x1	O
,	O
d1	O
)	O
max	O
p	O
(	O
x2|x1	O
,	O
d1	O
)	O
u	O
(	O
x2	O
)	O
d2	O
u3←4	O
(	O
x3	O
)	O
≡	O
max	O
d3	O
p	O
(	O
x4|x3	O
,	O
d3	O
)	O
u	O
(	O
x4	O
)	O
which	O
we	O
can	O
incorporate	O
in	O
the	O
next	O
line	O
u	O
(	O
d1	O
)	O
=	O
(	O
cid:88	O
)	O
+	O
(	O
cid:88	O
)	O
x2	O
x2	O
p	O
(	O
x2|x1	O
,	O
d1	O
)	O
max	O
p	O
(	O
x2|x1	O
,	O
d1	O
)	O
u	O
(	O
x2	O
)	O
d2	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
x3	O
x3	O
(	O
cid:88	O
)	O
x3	O
(	O
cid:88	O
)	O
x4	O
(	O
cid:88	O
)	O
x3	O
(	O
7.5.10	O
)	O
(	O
7.5.11	O
)	O
(	O
7.5.12	O
)	O
(	O
7.5.13	O
)	O
(	O
7.5.14	O
)	O
(	O
7.5.15	O
)	O
(	O
7.5.16	O
)	O
(	O
7.5.17	O
)	O
(	O
7.5.18	O
)	O
(	O
7.5.19	O
)	O
121	O
p	O
(	O
x3|x2	O
,	O
d2	O
)	O
[	O
u	O
(	O
x3	O
)	O
+	O
u3←4	O
(	O
x3	O
)	O
]	O
similarly	O
,	O
we	O
can	O
now	O
carry	O
out	O
the	O
sum	O
over	O
x3	O
and	O
max	O
over	O
d2	O
to	O
deﬁne	O
a	O
new	O
function	B
u2←3	O
(	O
x2	O
)	O
≡	O
max	O
d2	O
p	O
(	O
x3|x2	O
,	O
d2	O
)	O
[	O
u	O
(	O
x3	O
)	O
+	O
u3←4	O
(	O
x3	O
)	O
]	O
to	O
give	O
u	O
(	O
d1	O
)	O
=	O
(	O
cid:88	O
)	O
x2	O
p	O
(	O
x2|x1	O
,	O
d1	O
)	O
[	O
u	O
(	O
x2	O
)	O
+	O
u2←3	O
(	O
x2	O
)	O
]	O
given	O
u	O
(	O
d1	O
)	O
above	O
,	O
we	O
can	O
then	O
ﬁnd	O
the	O
optimal	O
decision	O
d1	O
by	O
d1	O
2	O
?	O
bear	O
in	O
mind	O
that	O
when	O
we	O
come	O
to	O
make	O
decision	O
d2	O
we	O
will	O
have	O
observed	O
x1	O
,	O
x2	O
and	O
3.	O
in	O
general	O
,	O
the	O
optimal	O
decision	O
is	O
given	O
by	O
∗	O
1	O
=	O
argmax	O
d	O
u	O
(	O
d1	O
)	O
what	O
about	O
d∗	O
d2	O
.	O
we	O
can	O
then	O
ﬁnd	O
d∗	O
2	O
by	O
(	O
cid:88	O
)	O
d2	O
argmax	O
p	O
(	O
x3|x2	O
,	O
d2	O
)	O
[	O
u	O
(	O
x3	O
)	O
+	O
u3←4	O
(	O
x3	O
)	O
]	O
(	O
cid:88	O
)	O
subsequently	O
we	O
can	O
backtrack	O
further	O
to	O
ﬁnd	O
d∗	O
x3	O
∗	O
t−1	O
=	O
argmax	O
d	O
p	O
(	O
xt|xt−1	O
,	O
dt−1	O
)	O
[	O
u	O
(	O
xt	O
)	O
+	O
ut←t+1	O
(	O
xt	O
)	O
]	O
dt−1	O
xt	O
7.5.2	O
bellman	O
’	O
s	O
equation	B
(	O
cid:88	O
)	O
xt	O
ut−1←t	O
(	O
xt−1	O
)	O
≡	O
max	O
dt−1	O
p	O
(	O
xt|xt−1	O
,	O
dt−1	O
)	O
[	O
u	O
(	O
xt	O
)	O
+	O
ut←t+1	O
(	O
xt	O
)	O
]	O
it	O
is	O
more	O
common	O
to	O
deﬁne	O
the	O
value	B
of	O
being	O
in	O
state	O
xt	O
as	O
vt	O
(	O
xt	O
)	O
≡	O
u	O
(	O
xt	O
)	O
+	O
ut←t+1	O
(	O
xt	O
)	O
,	O
vt	O
(	O
xt	O
)	O
=	O
u	O
(	O
xt	O
)	O
and	O
write	O
then	O
the	O
equivalent	B
recursion	O
vt−1	O
(	O
xt−1	O
)	O
=	O
u	O
(	O
xt−1	O
)	O
+	O
max	O
dt−1	O
draft	O
march	O
9	O
,	O
2010	O
p	O
(	O
xt|xt−1	O
,	O
dt−1	O
)	O
vt	O
(	O
xt	O
)	O
(	O
cid:88	O
)	O
xt	O
in	O
a	O
markov	O
decision	O
process	O
,	O
as	O
above	O
,	O
we	O
can	O
deﬁne	O
utility	B
messages	O
recursively	O
as	O
temporally	O
unbounded	O
mdps	O
the	O
optimal	O
decision	O
d∗	O
t	O
is	O
then	O
given	O
by	O
(	O
cid:88	O
)	O
xt+1	O
∗	O
t	O
=	O
argmax	O
d	O
dt	O
p	O
(	O
xt+1|xt	O
,	O
dt	O
)	O
v	O
(	O
xt+1	O
)	O
(	O
7.5.20	O
)	O
equation	B
(	O
7.5.19	O
)	O
is	O
called	O
bellman	O
’	O
s	O
equation	B
[	O
30	O
]	O
5	O
.	O
7.6	O
temporally	B
unbounded	I
mdps	O
in	O
the	O
previous	O
discussion	O
about	O
mdps	O
we	O
assumed	O
a	O
given	O
end	O
time	O
,	O
t	O
,	O
from	O
which	O
one	O
can	O
propagate	O
messages	O
back	O
from	O
the	O
end	O
of	O
the	O
chain	B
.	O
the	O
inﬁnite	O
t	O
case	O
would	O
appear	O
to	O
be	O
ill-deﬁned	O
since	O
the	O
sum	O
of	O
utilities	O
u	O
(	O
x1	O
)	O
+	O
u	O
(	O
x2	O
)	O
+	O
.	O
.	O
.	O
+	O
u	O
(	O
xt	O
)	O
(	O
7.6.1	O
)	O
will	O
in	O
general	O
be	O
unbounded	O
.	O
there	O
is	O
a	O
simple	O
way	O
to	O
avoid	O
this	O
diﬃculty	O
.	O
if	O
we	O
let	O
u∗	O
=	O
maxs	O
u	O
(	O
s	O
)	O
be	O
the	O
largest	O
value	B
of	O
the	O
utility	B
and	O
consider	O
the	O
sum	O
of	O
modiﬁed	O
utilities	O
for	O
a	O
chosen	O
discount	B
factor	I
0	O
<	O
γ	O
<	O
1	O
t	O
(	O
cid:88	O
)	O
t=1	O
∗	O
t	O
(	O
cid:88	O
)	O
t=1	O
γtu	O
(	O
xt	O
)	O
≤	O
u	O
γt	O
=	O
γu	O
∗	O
1	O
−	O
γt	O
1	O
−	O
γ	O
(	O
7.6.2	O
)	O
where	O
we	O
used	O
the	O
result	O
for	O
a	O
geometric	O
series	O
.	O
in	O
the	O
limit	O
t	O
→	O
∞	O
this	O
means	O
that	O
the	O
summed	O
modiﬁed	O
utility	B
γtu	O
(	O
xt	O
)	O
is	O
ﬁnite	O
.	O
the	O
only	O
modiﬁcation	O
required	O
to	O
our	O
previous	O
discussion	O
is	O
to	O
include	O
a	O
factor	B
γ	O
in	O
the	O
message	B
deﬁnition	O
.	O
assuming	O
that	O
we	O
are	O
at	O
convergence	O
,	O
we	O
deﬁne	O
a	O
value	B
v	O
(	O
xt	O
=	O
s	O
)	O
dependent	O
only	O
on	O
the	O
state	O
s	O
,	O
and	O
not	O
the	O
time	O
.	O
this	O
means	O
we	O
replace	O
the	O
time-dependent	O
bellman	O
’	O
s	O
value	B
recursion	O
equation	B
(	O
7.5.19	O
)	O
with	O
the	O
stationary	B
equation	O
v	O
(	O
s	O
)	O
≡	O
u	O
(	O
s	O
)	O
+	O
γ	O
max	O
d	O
p	O
(	O
xt	O
=	O
s	O
(	O
cid:48	O
)	O
|xt−1	O
=	O
s	O
,	O
dt−1	O
=	O
d	O
)	O
v	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
7.6.3	O
)	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
we	O
then	O
need	O
to	O
solve	O
equation	B
(	O
7.6.3	O
)	O
for	O
the	O
value	B
v	O
(	O
s	O
)	O
for	O
all	O
states	O
s.	O
the	O
optimal	O
decision	O
policy	O
when	O
one	O
is	O
in	O
state	O
xt	O
=	O
s	O
is	O
then	O
given	O
by	O
p	O
(	O
xt+1	O
=	O
s	O
(	O
cid:48	O
)	O
d∗	O
(	O
s	O
)	O
=	O
argmax	O
(	O
cid:88	O
)	O
(	O
7.6.4	O
)	O
|xt	O
=	O
s	O
,	O
dt	O
=	O
d	O
)	O
v	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
d	O
s	O
(	O
cid:48	O
)	O
for	O
a	O
deterministic	B
transition	O
p	O
(	O
i.e	O
.	O
for	O
each	O
decision	O
d	O
,	O
only	O
one	O
state	O
s	O
(	O
cid:48	O
)	O
is	O
available	O
)	O
,	O
this	O
means	O
that	O
the	O
best	O
decision	O
is	O
the	O
one	O
that	O
takes	O
us	O
to	O
the	O
accessible	O
state	O
with	O
highest	O
value	B
.	O
equation	B
(	O
7.6.3	O
)	O
seems	O
straightforward	O
to	O
solve	O
.	O
however	O
,	O
the	O
max	O
operation	O
means	O
that	O
the	O
equations	O
are	O
non-linear	B
in	O
the	O
value	B
v	O
and	O
no	O
closed	O
form	O
solution	O
is	O
available	O
.	O
two	O
popular	O
techniques	O
for	O
solving	B
equation	O
(	O
7.6.3	O
)	O
,	O
are	O
value	B
and	O
policy	B
iteration	I
,	O
which	O
we	O
describe	O
below	O
.	O
when	O
the	O
number	O
of	O
states	O
s	O
is	O
very	O
large	O
,	O
approximate	B
solutions	O
are	O
required	O
.	O
sampling	B
and	O
state-dimension	O
reduction	O
techniques	O
are	O
described	O
in	O
[	O
58	O
]	O
.	O
7.6.1	O
value	B
iteration	I
a	O
naive	O
procedure	O
is	O
to	O
iterate	O
equation	B
(	O
7.6.3	O
)	O
until	O
convergence	O
,	O
assuming	O
some	O
initial	O
guess	O
for	O
the	O
values	O
(	O
say	O
uniform	B
)	O
.	O
one	O
can	O
show	O
that	O
this	O
value	B
iteration	I
procedure	O
is	O
guaranteed	O
to	O
converge	O
to	O
a	O
unique	O
optimum	O
[	O
34	O
]	O
.	O
the	O
convergence	O
rate	O
depends	O
somewhat	O
on	O
the	O
discount	O
γ	O
–	O
the	O
smaller	O
γ	O
is	O
,	O
the	O
faster	O
is	O
the	O
convergence	O
.	O
an	O
example	O
of	O
value	B
iteration	I
is	O
given	O
in	O
ﬁg	O
(	O
7.10	O
)	O
.	O
5the	O
continuous-time	O
analog	O
has	O
a	O
long	O
history	O
in	O
physics	O
and	O
is	O
called	O
the	O
hamilton-jacobi	O
equation	B
and	O
enables	O
one	O
to	O
solve	O
mdps	O
by	O
message	B
passing	I
,	O
this	O
being	O
a	O
special	O
case	O
of	O
the	O
more	O
general	O
junction	B
tree	I
approach	O
described	O
earlier	O
in	O
section	O
(	O
7.4.2	O
)	O
.	O
122	O
draft	O
march	O
9	O
,	O
2010	O
temporally	B
unbounded	I
mdps	O
figure	O
7.9	O
:	O
states	O
deﬁned	O
on	O
a	O
two	O
dimensional	O
grid	O
.	O
in	O
each	O
square	O
the	O
top	O
left	O
value	B
is	O
the	O
state	O
number	O
,	O
and	O
the	O
bottom	O
right	O
is	O
the	O
utility	B
of	O
being	O
in	O
that	O
state	O
.	O
an	O
‘	O
agent	O
’	O
can	O
move	O
from	O
a	O
state	O
to	O
a	O
neighbouring	O
state	O
,	O
as	O
indicated	O
.	O
the	O
task	O
is	O
to	O
solve	O
this	O
problem	B
such	O
that	O
for	O
any	O
position	O
(	O
state	O
)	O
one	O
knows	O
how	O
to	O
move	O
optimally	O
to	O
maximise	O
the	O
expected	O
utility	B
.	O
this	O
means	O
that	O
we	O
need	O
to	O
move	O
towards	O
the	O
goal	O
states	O
(	O
states	O
with	O
non-	O
zero	O
utility	B
)	O
.	O
see	O
demomdp	O
.	O
figure	O
7.10	O
:	O
value	B
iteration	I
on	O
a	O
set	O
of	O
225	O
states	O
,	O
corresponding	O
to	O
a	O
15×	O
15	O
two	O
dimensional	O
grid	O
.	O
de-	O
terministic	O
transitions	O
are	O
allowed	O
to	O
neighbours	O
on	O
the	O
grid	O
,	O
{	O
stay	O
,	O
left	O
,	O
right	O
,	O
up	O
,	O
down	O
}	O
.	O
there	O
are	O
three	O
goal	O
states	O
,	O
each	O
with	O
utility	O
1	O
–	O
all	O
other	O
states	O
have	O
utility	B
0.	O
plotted	O
is	O
the	O
value	B
v	O
(	O
s	O
)	O
for	O
γ	O
=	O
0.9	O
after	O
30	O
updates	O
of	O
value	B
iteration	I
,	O
where	O
the	O
states	O
index	O
a	O
point	O
on	O
the	O
x−	O
y	O
grid	O
.	O
the	O
optimal	O
decision	O
for	O
any	O
state	O
on	O
the	O
grid	O
is	O
to	O
go	O
to	O
the	O
neighbouring	O
state	O
with	O
highest	O
value	B
.	O
see	O
demomdp	O
.	O
7.6.2	O
policy	B
iteration	I
in	O
policy	B
iteration	I
we	O
ﬁrst	O
assume	O
we	O
know	O
the	O
optimal	O
decision	O
d∗	O
(	O
s	O
)	O
for	O
any	O
state	O
s.	O
we	O
may	O
use	O
this	O
in	O
equation	B
(	O
7.6.3	O
)	O
to	O
give	O
v	O
(	O
s	O
)	O
=	O
u	O
(	O
s	O
)	O
+	O
γ	O
p	O
(	O
xt	O
=	O
s	O
(	O
cid:48	O
)	O
|xt−1	O
=	O
s	O
,	O
d∗	O
(	O
s	O
)	O
)	O
v	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
(	O
7.6.5	O
)	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
the	O
maximisation	B
over	O
d	O
has	O
disappeared	O
since	O
we	O
have	O
assumed	O
we	O
already	O
know	O
the	O
optimal	O
decision	O
for	O
each	O
state	O
s.	O
for	O
ﬁxed	O
d∗	O
(	O
s	O
)	O
,	O
equation	B
(	O
7.6.5	O
)	O
is	O
now	O
linear	B
in	O
the	O
value	B
.	O
deﬁning	O
the	O
value	B
v	O
and	O
utility	B
u	O
vectors	O
and	O
transition	B
matrix	I
p	O
,	O
[	O
u	O
]	O
s	O
=	O
u	O
(	O
s	O
)	O
,	O
[	O
v	O
]	O
s	O
=	O
v	O
(	O
s	O
)	O
,	O
(	O
7.6.6	O
)	O
[	O
p	O
]	O
s	O
(	O
cid:48	O
)	O
,	O
s	O
=	O
p	O
(	O
s	O
’	O
|s	O
,	O
d∗	O
(	O
s	O
)	O
)	O
in	O
matrix	B
notation	O
,	O
equation	B
(	O
7.6.5	O
)	O
becomes	O
(	O
cid:16	O
)	O
i	O
−	O
γpt	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
i	O
−	O
γpt	O
(	O
cid:17	O
)	O
−1	O
v	O
=	O
u	O
+	O
γptv	O
⇔	O
v	O
=	O
u	O
⇔	O
v	O
=	O
u	O
(	O
7.6.7	O
)	O
these	O
linear	B
equations	O
are	O
readily	O
solved	O
with	O
gaussian	O
elimination	O
.	O
using	O
this	O
,	O
the	O
optimal	O
policy	B
is	O
recomputed	O
using	O
equation	B
(	O
7.6.4	O
)	O
.	O
the	O
two	O
steps	O
of	O
solving	B
for	O
the	O
value	B
,	O
and	O
recomputing	O
the	O
policy	B
are	O
iterated	O
until	O
convergence	O
.	O
in	O
policy	B
iteration	I
we	O
guess	O
an	O
initial	O
d∗	O
(	O
s	O
)	O
,	O
then	O
solve	O
the	O
linear	B
equations	O
(	O
7.6.5	O
)	O
for	O
the	O
value	B
,	O
and	O
then	O
recompute	O
the	O
optimal	O
decision	O
.	O
see	O
demomdp.m	O
for	O
a	O
comparison	O
of	O
value	B
and	O
policy	B
iteration	I
,	O
and	O
also	O
an	O
em	O
style	O
approach	B
which	O
we	O
discuss	O
in	O
the	O
next	O
section	O
.	O
example	O
35	O
(	O
a	O
grid-world	O
mdp	O
)	O
.	O
a	O
set	O
of	O
states	O
deﬁned	O
on	O
a	O
grid	O
,	O
utilities	O
for	O
being	O
in	O
a	O
grid	O
state	O
is	O
given	O
in	O
ﬁg	O
(	O
7.9	O
)	O
,	O
for	O
which	O
the	O
agent	O
deterministically	O
moves	O
to	O
a	O
neighbouring	O
grid	O
state	O
at	O
each	O
time	O
step	O
.	O
after	O
initialising	O
the	O
value	B
of	O
each	O
grid	O
state	O
to	O
unity	O
,	O
the	O
converged	O
value	B
for	O
each	O
state	O
is	O
given	O
in	O
ﬁg	O
(	O
7.10	O
)	O
.	O
the	O
optimal	O
policy	B
is	O
then	O
given	O
by	O
moving	O
to	O
the	O
neighbouring	O
grid	O
state	O
with	O
highest	O
value	B
.	O
draft	O
march	O
9	O
,	O
2010	O
123	O
1020314050607080901001101201301401501601701811902002102202302402502602702802903003103203303403503603703803904004104204304404504614704804905005115205305405505605705805906006106206306406506606706806907017107207317407507607707807908008108208308408508608708808909019109219309409509609709809901000123456789101112131415123456789101112131415024	O
d1	O
x1	O
d2	O
x2	O
(	O
a	O
)	O
d1	O
x3	O
x1	O
u3	O
d2	O
x2	O
(	O
b	O
)	O
x3	O
u3	O
probabilistic	B
inference	O
and	O
planning	B
figure	O
7.11	O
:	O
(	O
a	O
)	O
:	O
a	O
markov	O
decision	O
pro-	O
(	O
b	O
)	O
:	O
the	O
corresponding	O
probabilis-	O
cess	O
.	O
tic	O
inference	B
planner	O
.	O
7.6.3	O
a	O
curse	B
of	I
dimensionality	I
consider	O
the	O
following	O
tower	O
of	O
hanoi	O
problem	B
.	O
there	O
are	O
4	O
pegs	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
and	O
10	O
disks	O
numbered	O
from	O
1	O
to	O
10.	O
you	O
may	O
move	O
a	O
single	O
disk	O
from	O
one	O
peg	O
to	O
another	O
–	O
however	O
,	O
you	O
are	O
not	O
allowed	O
to	O
put	O
a	O
bigger	O
numbered	O
disk	O
on	O
top	O
of	O
a	O
smaller	O
numbered	O
disk	O
.	O
starting	O
with	O
all	O
disks	O
on	O
peg	O
a	O
,	O
how	O
can	O
you	O
move	O
them	O
all	O
to	O
peg	O
d	O
in	O
the	O
minimal	O
number	O
of	O
moves	O
?	O
this	O
would	O
appear	O
to	O
be	O
a	O
straightforward	O
markov	O
decision	O
process	O
in	O
which	O
the	O
transitions	O
are	O
allowed	O
disk	O
moves	O
.	O
if	O
we	O
use	O
x	O
to	O
represent	O
the	O
state	O
of	O
the	O
disks	O
on	O
the	O
4	O
pegs	O
,	O
this	O
has	O
410	O
=	O
1048576	O
states	O
(	O
some	O
are	O
equivalent	B
up	O
to	O
permutation	O
of	O
the	O
pegs	O
,	O
which	O
reduces	O
this	O
by	O
a	O
factor	B
of	O
2	O
)	O
.	O
this	O
large	O
number	O
of	O
states	O
renders	O
this	O
naive	O
approach	O
computationally	O
problematic	O
.	O
many	O
interesting	O
real-world	O
problems	O
suﬀer	O
from	O
this	O
large	O
number	O
of	O
states	O
issue	O
so	O
that	O
a	O
naive	O
approach	O
based	O
as	O
we	O
’	O
ve	O
described	O
is	O
computationally	O
infeasible	O
.	O
finding	O
eﬃcient	B
exact	O
and	O
also	O
approximate	B
state	O
representations	O
is	O
a	O
key	O
aspect	O
to	O
solving	B
large	O
scale	O
mdps	O
,	O
see	O
for	O
example	O
[	O
193	O
]	O
.	O
7.7	O
probabilistic	B
inference	O
and	O
planning	B
an	O
alternative	O
to	O
the	O
classical	O
mdp	O
solution	O
methods	O
is	O
to	O
make	O
use	O
of	O
the	O
standard	O
methods	O
for	O
training	B
probabilistic	O
models	O
,	O
such	O
as	O
the	O
expectation-maximisation	O
algorithm	B
.	O
in	O
order	O
to	O
do	O
so	O
we	O
ﬁrst	O
need	O
to	O
write	O
the	O
problem	B
of	O
maximising	O
expected	O
utility	B
in	O
a	O
form	O
that	O
is	O
suitable	O
.	O
to	O
do	O
this	O
we	O
ﬁrst	O
discuss	O
how	O
a	O
mdp	O
can	O
be	O
expressed	O
as	O
the	O
maximisation	B
of	O
a	O
form	O
of	O
belief	B
network	I
in	O
which	O
the	O
parameters	O
to	O
be	O
found	O
relate	O
to	O
the	O
policy	B
.	O
7.7.1	O
non-stationary	B
markov	O
decision	O
process	O
consider	O
the	O
mdp	O
in	O
ﬁg	O
(	O
7.11a	O
)	O
in	O
which	O
,	O
for	O
simplicity	O
,	O
we	O
assume	O
we	O
know	O
the	O
initial	O
state	O
x1	O
=	O
x1	O
.	O
our	O
task	O
is	O
then	O
to	O
ﬁnd	O
the	O
decisions	O
that	O
maximise	O
the	O
expected	O
utility	B
,	O
based	O
on	O
a	O
sequential	B
decision	O
process	O
.	O
the	O
ﬁrst	O
decision	O
d1	O
is	O
given	O
by	O
maximising	O
the	O
expected	O
utility	B
:	O
u	O
(	O
d1	O
)	O
=	O
(	O
cid:88	O
)	O
x2	O
(	O
cid:88	O
)	O
x3	O
p	O
(	O
x2|x1	O
,	O
d1	O
)	O
max	O
d2	O
p	O
(	O
x3|x2	O
,	O
d2	O
)	O
u3	O
(	O
x3	O
)	O
more	O
generally	O
,	O
this	O
utility	B
can	O
be	O
computed	O
eﬃciently	O
using	O
a	O
standard	O
message	O
passing	B
routine	O
:	O
(	O
cid:88	O
)	O
xt+1	O
ut←t+1	O
(	O
xt	O
)	O
≡	O
max	O
dt	O
where	O
ut←t	O
+1	O
(	O
xt	O
)	O
=	O
ut	O
(	O
xt	O
)	O
p	O
(	O
xt+1|xt	O
,	O
dt	O
)	O
ut+1←t+2	O
(	O
xt+1	O
)	O
(	O
7.7.1	O
)	O
(	O
7.7.2	O
)	O
(	O
7.7.3	O
)	O
124	O
draft	O
march	O
9	O
,	O
2010	O
probabilistic	B
inference	O
and	O
planning	B
7.7.2	O
non-stationary	B
probabilistic	O
inference	B
planner	O
as	O
an	O
alternative	O
to	O
the	O
above	O
mdp	O
description	O
,	O
consider	O
the	O
belief	B
network	I
ﬁg	O
(	O
7.11b	O
)	O
in	O
which	O
we	O
have	O
a	O
utility	B
associated	O
with	O
the	O
last	O
time-point	O
[	O
279	O
]	O
.	O
then	O
the	O
expected	O
utility	B
is	O
given	O
by	O
u	O
(	O
π1	O
,	O
π2	O
)	O
=	O
(	O
cid:88	O
)	O
=	O
(	O
cid:88	O
)	O
d1	O
,	O
d2	O
,	O
x2	O
,	O
x3	O
p	O
(	O
d1|x1	O
,	O
π1	O
)	O
p	O
(	O
x2|x1	O
,	O
d1	O
)	O
p	O
(	O
d2|x2	O
,	O
π2	O
)	O
p	O
(	O
x3|x2	O
,	O
d2	O
)	O
u3	O
(	O
x3	O
)	O
p	O
(	O
d1|x1	O
,	O
π1	O
)	O
(	O
cid:88	O
)	O
p	O
(	O
x2|x1	O
,	O
d1	O
)	O
(	O
cid:88	O
)	O
p	O
(	O
d2|x2	O
,	O
π2	O
)	O
(	O
cid:88	O
)	O
d1	O
x2	O
d2	O
x3	O
p	O
(	O
x3|x2	O
,	O
d2	O
)	O
u3	O
(	O
x3	O
)	O
(	O
7.7.4	O
)	O
(	O
7.7.5	O
)	O
here	O
the	O
terms	O
p	O
(	O
dt|xt	O
,	O
πt	O
)	O
are	O
the	O
‘	O
policy	B
distributions	O
’	O
that	O
we	O
wish	O
to	O
learn	O
and	O
πt	O
are	O
the	O
parameters	O
of	O
the	O
tth	O
policy	B
distribution	O
.	O
let	O
’	O
s	O
assume	O
that	O
we	O
have	O
one	O
per	O
time	O
so	O
that	O
πt	O
is	O
a	O
function	B
that	O
maps	O
a	O
state	O
x	O
to	O
a	O
probability	B
distribution	O
over	O
decisions	O
.	O
our	O
interest	O
is	O
to	O
ﬁnd	O
the	O
policy	B
distributions	O
π1	O
,	O
π2	O
that	O
maximise	O
the	O
expected	O
utility	B
.	O
since	O
each	O
time-step	O
has	O
its	O
own	O
πt	O
and	O
for	O
each	O
state	O
x2	O
=	O
x2	O
we	O
have	O
a	O
separate	O
unconstrained	O
distribution	B
p	O
(	O
d2|x2	O
,	O
π2	O
)	O
to	O
optimise	O
over	O
and	O
we	O
can	O
write	O
max	O
π1	O
,	O
π2	O
u	O
(	O
π1	O
,	O
π2	O
)	O
=	O
max	O
π1	O
p	O
(	O
x2|x1	O
,	O
d1	O
)	O
max	O
π2	O
p	O
(	O
x3|x2	O
,	O
d2	O
)	O
u3	O
(	O
x3	O
)	O
(	O
7.7.6	O
)	O
(	O
cid:88	O
)	O
d1	O
p	O
(	O
d1|x1	O
,	O
π1	O
)	O
(	O
cid:88	O
)	O
x2	O
(	O
cid:88	O
)	O
d2	O
p	O
(	O
d2|x2	O
,	O
π2	O
)	O
(	O
cid:88	O
)	O
x3	O
this	O
shows	O
that	O
provided	O
there	O
are	O
no	O
constraints	O
on	O
the	O
policy	B
distributions	O
(	O
there	O
is	O
a	O
separate	O
one	O
for	O
each	O
timepoint	O
)	O
,	O
we	O
are	O
allowed	O
to	O
distribute	O
the	O
maximisations	O
over	O
the	O
individual	O
policies	O
inside	O
the	O
summation	O
.	O
more	O
generally	O
,	O
for	O
a	O
ﬁnite	O
time	O
t	O
one	O
can	O
deﬁne	O
messages	O
to	O
solve	O
for	O
the	O
optimal	O
policy	B
distributions	O
ut←t+1	O
(	O
xt	O
)	O
≡	O
max	O
πt	O
p	O
(	O
xt+1|xt	O
,	O
dt	O
)	O
ut+1←t+2	O
(	O
xt+1	O
)	O
(	O
cid:88	O
)	O
dt	O
p	O
(	O
dt|xt	O
,	O
πt	O
)	O
(	O
cid:88	O
)	O
xt+1	O
with	O
ut	O
+1←t	O
(	O
xt	O
)	O
=	O
ut	O
(	O
xt	O
)	O
deterministic	B
policy	O
for	O
a	O
deterministic	B
policy	O
,	O
only	O
a	O
single	O
state	O
is	O
allowed	O
,	O
so	O
that	O
(	O
7.7.7	O
)	O
(	O
7.7.8	O
)	O
where	O
d∗	O
function	B
for	O
each	O
time	O
t	O
equation	B
(	O
7.7.7	O
)	O
reduces	O
to	O
∗	O
p	O
(	O
dt|xt	O
,	O
πt	O
)	O
=	O
δ	O
(	O
dt	O
,	O
d	O
t	O
(	O
xt	O
)	O
)	O
t	O
(	O
x	O
)	O
is	O
a	O
policy	B
function	O
that	O
maps	O
a	O
state	O
x	O
to	O
a	O
single	O
decision	O
d.	O
since	O
we	O
have	O
a	O
separate	O
policy	B
(	O
cid:88	O
)	O
(	O
7.7.9	O
)	O
ut←t+1	O
(	O
xt	O
)	O
≡	O
max	O
d∗	O
t	O
(	O
xt	O
)	O
xt+1	O
∗	O
t	O
(	O
xt	O
)	O
)	O
ut+1←t+2	O
(	O
xt+1	O
)	O
p	O
(	O
xt+1|xt	O
,	O
d	O
(	O
7.7.10	O
)	O
which	O
is	O
equivalent	B
to	O
equation	B
(	O
7.7.2	O
)	O
.	O
this	O
shows	O
that	O
solving	B
the	O
mdp	O
is	O
equivalent	B
to	O
maximising	O
a	O
standard	O
expected	O
utility	B
deﬁned	O
in	O
terms	O
of	O
a	O
belief	B
network	I
under	O
the	O
assumption	O
that	O
each	O
time	O
point	O
has	O
its	O
own	O
policy	B
distribution	O
,	O
and	O
that	O
this	O
is	O
deterministic	B
.	O
7.7.3	O
stationary	B
planner	I
if	O
we	O
reconsider	O
our	O
simple	O
example	O
,	O
ﬁg	O
(	O
7.11b	O
)	O
but	O
now	O
constrain	O
the	O
policy	B
distributions	O
to	O
be	O
the	O
same	O
for	O
all	O
time	O
,	O
p	O
(	O
dt|xt	O
,	O
πt	O
)	O
=	O
p	O
(	O
dt|xt	O
,	O
π	O
)	O
(	O
or	O
more	O
succinctly	O
πt	O
=	O
π	O
)	O
,	O
then	O
equation	B
(	O
7.7.5	O
)	O
becomes	O
p	O
(	O
d1|x1	O
,	O
π	O
)	O
(	O
cid:88	O
)	O
p	O
(	O
x2|x1	O
,	O
d1	O
)	O
(	O
cid:88	O
)	O
p	O
(	O
d2|x2	O
,	O
π	O
)	O
(	O
cid:88	O
)	O
p	O
(	O
x3|x2	O
,	O
d2	O
)	O
u3	O
(	O
x3	O
)	O
(	O
7.7.11	O
)	O
u	O
(	O
π	O
)	O
=	O
(	O
cid:88	O
)	O
d1	O
x2	O
d2	O
x3	O
in	O
this	O
case	O
we	O
can	O
not	O
distribute	O
the	O
maximisation	B
over	O
the	O
policy	B
π	O
over	O
the	O
individual	O
terms	O
of	O
the	O
product	O
.	O
however	O
,	O
computing	O
the	O
expected	O
utility	B
for	O
any	O
given	O
policy	B
π	O
is	O
straightforward	O
,	O
using	O
message	B
passing	I
.	O
one	O
may	O
thus	O
optimise	O
the	O
expected	O
utility	B
using	O
standard	O
numerical	O
optimisation	B
procedures	O
,	O
or	O
alternatively	O
an	O
em	O
style	O
approach	B
as	O
we	O
discuss	O
below	O
.	O
draft	O
march	O
9	O
,	O
2010	O
125	O
a	O
variational	O
training	O
approach	B
probabilistic	O
inference	B
and	O
planning	B
without	O
loss	O
of	O
generality	O
,	O
we	O
assume	O
that	O
the	O
utility	B
is	O
positive	O
and	O
deﬁne	O
a	O
distribution	B
(	O
cid:80	O
)	O
d1	O
,	O
d2	O
,	O
d3	O
,	O
x2	O
,	O
x3	O
p	O
(	O
d1|x1	O
,	O
π	O
)	O
p	O
(	O
x2|x1	O
,	O
d1	O
)	O
p	O
(	O
d2|x2	O
,	O
π	O
)	O
p	O
(	O
x3|x2	O
,	O
d2	O
)	O
u3	O
(	O
x3	O
)	O
p	O
(	O
d1|x1	O
,	O
π	O
)	O
p	O
(	O
x2|x1	O
,	O
d1	O
)	O
p	O
(	O
d2|x2	O
,	O
π	O
)	O
p	O
(	O
x3|x2	O
,	O
d2	O
)	O
u3	O
(	O
x3	O
)	O
(	O
7.7.12	O
)	O
˜p	O
(	O
d1	O
,	O
d2	O
,	O
d3	O
,	O
x2	O
,	O
x3	O
)	O
=	O
then	O
for	O
any	O
variational	O
distribution	O
q	O
(	O
d1	O
,	O
d2	O
,	O
d3	O
,	O
x2	O
,	O
x3	O
)	O
,	O
kl	O
(	O
q	O
(	O
d1	O
,	O
d2	O
,	O
d3	O
,	O
x2	O
,	O
x3	O
)	O
|˜p	O
(	O
d1	O
,	O
d2	O
,	O
d3	O
,	O
x2	O
,	O
x3	O
)	O
)	O
=	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
d1	O
,	O
d2	O
,	O
d3	O
,	O
x2	O
,	O
x3	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
d1	O
,	O
d2	O
,	O
d3	O
,	O
x2	O
,	O
x3	O
)	O
−	O
(	O
cid:104	O
)	O
log	O
˜p	O
(	O
d1	O
,	O
d2	O
,	O
d3	O
,	O
x2	O
,	O
x3	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
d1	O
,	O
d2	O
,	O
d3	O
,	O
x2	O
,	O
x3	O
)	O
≥	O
0	O
(	O
7.7.13	O
)	O
using	O
the	O
deﬁnition	O
of	O
˜p	O
(	O
d1	O
,	O
d2	O
,	O
d3	O
,	O
x2	O
,	O
x3	O
)	O
and	O
the	O
fact	O
that	O
the	O
denominator	O
in	O
equation	B
(	O
7.7.12	O
)	O
is	O
equal	O
to	O
u	O
(	O
π	O
)	O
we	O
obtain	O
the	O
bound	B
log	O
u	O
(	O
π	O
)	O
≥	O
−	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
d1	O
,	O
d2	O
,	O
d3	O
,	O
x2	O
,	O
x3	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
d1	O
,	O
d2	O
,	O
d3	O
,	O
x2	O
,	O
x3	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
d1|x1	O
,	O
π	O
)	O
p	O
(	O
x2|x1	O
,	O
d1	O
)	O
p	O
(	O
d2|x2	O
,	O
π	O
)	O
p	O
(	O
x3|x2	O
,	O
d2	O
)	O
u3	O
(	O
x3	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
d1	O
,	O
d2	O
,	O
d3	O
,	O
x2	O
,	O
x3	O
)	O
(	O
7.7.14	O
)	O
this	O
then	O
gives	O
a	O
two-stage	O
em	O
style	O
procedure	O
:	O
m-step	O
isolating	O
the	O
dependencies	O
on	O
π	O
,	O
for	O
a	O
given	O
variational	O
distribution	O
qold	O
,	O
maximising	O
the	O
bound	B
equation	O
(	O
7.7.14	O
)	O
is	O
equivalent	B
to	O
maximising	O
e	O
(	O
π	O
)	O
≡	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
d1|x1	O
,	O
π	O
)	O
(	O
cid:105	O
)	O
qold	O
(	O
d1	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
d2|x2	O
,	O
π	O
)	O
(	O
cid:105	O
)	O
qold	O
(	O
d2	O
,	O
x2	O
)	O
one	O
then	O
ﬁnds	O
a	O
policy	B
πnew	O
which	O
maximises	O
e	O
(	O
π	O
)	O
:	O
πnew	O
=	O
argmax	O
π	O
e	O
(	O
π	O
)	O
e-step	O
for	O
ﬁxed	O
π	O
the	O
best	O
q	O
is	O
given	O
by	O
the	O
update	O
qnew	O
∝	O
p	O
(	O
d1|x1	O
,	O
π	O
)	O
p	O
(	O
x2|x1	O
,	O
d1	O
)	O
p	O
(	O
d2|x2	O
,	O
π	O
)	O
p	O
(	O
x3|x2	O
,	O
d2	O
)	O
u3	O
(	O
x3	O
)	O
(	O
7.7.15	O
)	O
(	O
7.7.16	O
)	O
(	O
7.7.17	O
)	O
from	O
this	O
joint	B
distribution	O
,	O
in	O
order	O
to	O
determine	O
the	O
m-step	O
updates	O
,	O
we	O
only	O
require	O
the	O
marginals	O
q	O
(	O
d1	O
)	O
and	O
q	O
(	O
d2	O
,	O
x2	O
)	O
,	O
both	O
of	O
which	O
are	O
straightforward	O
to	O
obtain	O
since	O
q	O
is	O
simply	O
a	O
ﬁrst	B
order	I
markov	O
chain	B
in	O
the	O
joint	B
variables	O
xt	O
,	O
dt	O
.	O
for	O
example	O
one	O
may	O
write	O
the	O
q-distribution	O
as	O
a	O
simple	O
chain	O
factor	B
graph	I
for	O
which	O
marginal	B
inference	O
can	O
be	O
performed	O
readily	O
using	O
the	O
sum-product	B
algorithm	I
.	O
this	O
procedure	O
is	O
analogous	O
to	O
the	O
standard	O
em	O
procedure	O
,	O
section	O
(	O
11.2	O
)	O
.	O
the	O
usual	O
guarantees	O
therefore	O
carry	O
over	O
so	O
that	O
ﬁnding	O
a	O
policy	B
that	O
increases	O
e	O
(	O
π	O
)	O
is	O
guaranteed	O
to	O
improve	O
the	O
expected	O
utility	B
.	O
in	O
complex	O
situations	O
in	O
which	O
,	O
for	O
reasons	O
of	O
storage	O
,	O
the	O
optimal	O
q	O
can	O
not	O
be	O
used	O
,	O
a	O
structured	B
con-	O
strained	O
variational	B
approximation	I
may	O
be	O
applied	O
.	O
in	O
this	O
case	O
,	O
as	O
in	O
generalised	B
em	O
,	O
only	O
a	O
guaranteed	O
improvement	O
on	O
the	O
lower	O
bound	O
of	O
the	O
expected	O
utility	B
is	O
achieved	O
.	O
nevertheless	O
,	O
this	O
may	O
be	O
of	O
consid-	O
erable	O
use	O
in	O
practical	O
situations	O
,	O
for	O
which	O
general	O
techniques	O
of	O
approximate	B
inference	I
may	O
be	O
applied	O
.	O
the	O
deterministic	B
case	O
for	O
the	O
special	O
case	O
that	O
the	O
policy	B
π	O
is	O
deterministic	B
,	O
π	O
simply	O
maps	O
each	O
state	O
x	O
to	O
single	O
decision	O
d.	O
writing	O
this	O
policy	B
map	O
as	O
d∗	O
(	O
x	O
)	O
equation	B
(	O
7.7.11	O
)	O
reduces	O
to	O
∗	O
)	O
=	O
(	O
cid:88	O
)	O
x2	O
u	O
(	O
d	O
∗	O
(	O
x1	O
)	O
)	O
(	O
cid:88	O
)	O
x3	O
p	O
(	O
x2|x1	O
,	O
d	O
p	O
(	O
x3|x2	O
,	O
d	O
∗	O
(	O
x2	O
)	O
)	O
u3	O
(	O
x3	O
)	O
we	O
now	O
deﬁne	O
a	O
variational	O
distribution	O
only	O
over	O
x2	O
,	O
x3	O
,	O
q	O
(	O
x2	O
,	O
x3	O
)	O
∝	O
p	O
(	O
x2|x1	O
,	O
d	O
∗	O
(	O
x1	O
)	O
)	O
p	O
(	O
x3|x2	O
,	O
d	O
∗	O
(	O
x2	O
)	O
)	O
u3	O
(	O
x3	O
)	O
126	O
draft	O
march	O
9	O
,	O
2010	O
(	O
7.7.18	O
)	O
(	O
7.7.19	O
)	O
probabilistic	B
inference	O
and	O
planning	B
and	O
the	O
‘	O
energy	B
’	O
term	O
becomes	O
e	O
(	O
d	O
∗	O
)	O
≡	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x2|x1	O
,	O
d	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
e	O
(	O
d	O
(	O
s	O
)	O
)	O
≡	O
s	O
(	O
cid:48	O
)	O
t	O
and	O
d∗	O
(	O
s	O
)	O
=	O
argmax	O
e	O
(	O
d	O
(	O
s	O
)	O
)	O
d	O
(	O
s	O
)	O
∗	O
(	O
x1	O
)	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x2	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x3|x2	O
,	O
d	O
∗	O
(	O
x2	O
)	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x2	O
,	O
x3	O
)	O
(	O
7.7.20	O
)	O
for	O
a	O
more	O
general	O
problem	B
in	O
which	O
the	O
utility	B
is	O
at	O
the	O
last	O
time	O
point	O
t	O
and	O
no	O
starting	O
state	O
is	O
given	O
(	O
cid:33	O
)	O
we	O
have	O
(	O
for	O
a	O
stationary	B
transition	O
p	O
(	O
xt+1|xt	O
,	O
dt	O
)	O
)	O
q	O
(	O
xt	O
=	O
s	O
,	O
xt+1	O
=	O
s	O
(	O
cid:48	O
)	O
)	O
log	O
p	O
(	O
x	O
(	O
cid:48	O
)	O
=	O
s	O
(	O
cid:48	O
)	O
|x	O
=	O
s	O
,	O
d	O
(	O
s	O
)	O
=	O
d	O
)	O
(	O
7.7.21	O
)	O
(	O
7.7.22	O
)	O
this	O
shows	O
how	O
to	O
train	O
a	O
stationary	B
mdp	O
using	O
em	O
in	O
which	O
there	O
is	O
a	O
utility	B
deﬁned	O
only	O
at	O
the	O
last	O
time-point	O
.	O
below	O
we	O
generalise	O
this	O
to	O
the	O
case	O
of	O
utilities	O
at	O
each	O
time	O
for	O
both	O
the	O
stationary	B
and	O
non-stationary	B
cases	O
.	O
7.7.4	O
utilities	O
at	O
each	O
timestep	O
consider	O
a	O
generalisation	B
in	O
which	O
we	O
have	O
an	O
additive	O
utility	B
associated	O
with	O
each	O
time-point	O
.	O
non-stationary	B
policy	I
to	O
help	O
develop	O
the	O
approach	B
,	O
let	O
’	O
s	O
look	O
at	O
simply	O
including	O
utilities	O
at	O
times	O
t	O
=	O
1	O
,	O
2	O
for	O
the	O
previous	O
example	O
.	O
the	O
expected	O
utility	B
is	O
given	O
by	O
p	O
(	O
d1|x1	O
,	O
π1	O
)	O
p	O
(	O
x2|x1	O
,	O
d1	O
)	O
p	O
(	O
d2|x2	O
,	O
π2	O
)	O
p	O
(	O
x3|x2	O
,	O
d2	O
)	O
u3	O
(	O
x3	O
)	O
(	O
7.7.23	O
)	O
d1	O
,	O
x2	O
d1	O
,	O
d2	O
,	O
x1	O
,	O
x2	O
,	O
x3	O
u	O
(	O
π1	O
,	O
π2	O
)	O
=	O
(	O
cid:88	O
)	O
+	O
(	O
cid:88	O
)	O
=	O
u1	O
(	O
x1	O
)	O
+	O
(	O
cid:88	O
)	O
vπ2	O
(	O
x2	O
)	O
=	O
u2	O
(	O
x2	O
)	O
+	O
(	O
cid:88	O
)	O
vπ1	O
(	O
x1	O
)	O
=	O
u1	O
(	O
x1	O
)	O
+	O
(	O
cid:88	O
)	O
d2	O
,	O
x3	O
d1	O
,	O
x2	O
deﬁning	O
value	B
messages	O
and	O
p	O
(	O
d1|x1	O
,	O
π1	O
)	O
p	O
(	O
x2|x1	O
,	O
d1	O
)	O
u2	O
(	O
x2	O
)	O
+	O
u1	O
(	O
x1	O
)	O
u2	O
(	O
x2	O
)	O
+	O
(	O
cid:88	O
)	O
d2	O
,	O
x3	O
p	O
(	O
d1|x1	O
,	O
π1	O
)	O
p	O
(	O
x2|x1	O
)	O
p	O
(	O
d2|x2	O
,	O
π2	O
)	O
p	O
(	O
x3|x2	O
,	O
d2	O
)	O
u3	O
(	O
x3	O
)	O
p	O
(	O
d1|x1	O
,	O
π1	O
)	O
p	O
(	O
x2|x1	O
,	O
d1	O
)	O
vπ2	O
(	O
x2	O
)	O
d1	O
,	O
x2	O
	O
p	O
(	O
d2|x2	O
,	O
π2	O
)	O
p	O
(	O
x3|x2	O
,	O
d2	O
)	O
u3	O
(	O
x3	O
)	O
(	O
7.7.24	O
)	O
(	O
7.7.25	O
)	O
(	O
7.7.26	O
)	O
u	O
(	O
π1	O
,	O
π2	O
)	O
=	O
vπ1	O
(	O
x1	O
)	O
(	O
7.7.27	O
)	O
for	O
a	O
more	O
general	O
case	O
deﬁned	O
over	O
t	O
timesteps	O
,	O
we	O
have	O
analogously	O
an	O
expected	O
utility	B
u	O
(	O
π1	O
:	O
t	O
)	O
,	O
and	O
our	O
interest	O
is	O
to	O
maximise	O
this	O
expected	O
utility	B
with	O
respect	O
to	O
all	O
the	O
policies	O
max	O
π1	O
:	O
t	O
u	O
(	O
π1	O
:	O
t	O
)	O
(	O
7.7.28	O
)	O
as	O
before	O
,	O
since	O
each	O
timestep	O
has	O
its	O
own	O
policy	B
distribution	O
for	O
each	O
state	O
,	O
we	O
may	O
distribute	O
the	O
maximisation	B
using	O
the	O
recursion	O
vt	O
,	O
t+1	O
(	O
xt	O
)	O
≡	O
ut	O
(	O
xt	O
)	O
+	O
max	O
πt	O
with	O
vt	O
,	O
t	O
+1	O
(	O
xt	O
)	O
≡	O
u	O
(	O
xt	O
)	O
draft	O
march	O
9	O
,	O
2010	O
p	O
(	O
dt|xt	O
,	O
πt	O
)	O
p	O
(	O
xt+1|xt	O
,	O
dt	O
)	O
vt+1	O
,	O
t+2	O
(	O
xt+1	O
)	O
(	O
7.7.29	O
)	O
(	O
7.7.30	O
)	O
127	O
(	O
cid:88	O
)	O
dt	O
,	O
xt+1	O
stationary	B
deterministic	I
policy	I
probabilistic	O
inference	B
and	O
planning	B
for	O
an	O
mdp	O
the	O
optimal	O
policy	B
is	O
deterministic	B
[	O
267	O
]	O
,	O
so	O
that	O
methods	O
which	O
explicitly	O
seek	O
for	O
deterministic	B
policies	O
are	O
of	O
interest	O
.	O
for	O
a	O
stationary	B
deterministic	I
policy	I
π	O
we	O
have	O
the	O
expected	O
utility	B
t	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
ut	O
(	O
xt	O
)	O
(	O
cid:88	O
)	O
t	O
(	O
cid:89	O
)	O
t=1	O
xt	O
x1	O
:	O
t−1	O
τ	O
=1	O
u	O
(	O
π	O
)	O
=	O
p	O
(	O
xτ|xτ−1	O
,	O
d	O
(	O
xτ−1	O
)	O
)	O
(	O
7.7.31	O
)	O
with	O
the	O
convention	O
p	O
(	O
x1|x0	O
,	O
d	O
(	O
x0	O
)	O
)	O
=	O
p	O
(	O
x1	O
)	O
.	O
viewed	O
as	O
a	O
factor	B
graph	I
,	O
this	O
is	O
simply	O
a	O
chain	B
,	O
so	O
that	O
for	O
any	O
policy	B
d	O
,	O
the	O
expected	O
utility	B
can	O
be	O
computed	O
easily	O
.	O
in	O
principle	O
one	O
could	O
then	O
attempt	O
to	O
optimise	O
u	O
with	O
respect	O
to	O
the	O
decisions	O
directly	O
.	O
an	O
alternative	O
is	O
to	O
use	O
an	O
em	O
style	O
procedure	O
[	O
100	O
]	O
.	O
to	O
do	O
this	O
we	O
need	O
to	O
deﬁne	O
a	O
(	O
trans-dimensional	O
)	O
distribution	B
ˆp	O
(	O
x1	O
:	O
t	O
,	O
t	O
)	O
=	O
ut	O
(	O
xt	O
)	O
z	O
(	O
d	O
)	O
t	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
t	O
(	O
cid:89	O
)	O
p	O
(	O
xτ|xτ−1	O
,	O
d	O
(	O
xτ−1	O
)	O
)	O
t	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
the	O
normalisation	B
constant	I
z	O
(	O
d	O
)	O
of	O
this	O
distribution	B
is	O
ut	O
(	O
xt	O
)	O
p	O
(	O
xτ|xτ−1	O
,	O
d	O
(	O
xτ−1	O
)	O
)	O
=	O
t=1	O
x1	O
:	O
t	O
τ	O
=1	O
t=1	O
x1	O
:	O
t	O
ut	O
(	O
xt	O
)	O
(	O
7.7.32	O
)	O
t	O
(	O
cid:89	O
)	O
τ	O
=1	O
p	O
(	O
xτ|xτ−1	O
,	O
d	O
(	O
xτ−1	O
)	O
)	O
=	O
u	O
(	O
π	O
)	O
(	O
7.7.33	O
)	O
t	O
(	O
cid:89	O
)	O
τ	O
=1	O
if	O
we	O
now	O
deﬁne	O
a	O
variational	O
distribution	O
q	O
(	O
x1	O
:	O
t	O
,	O
t	O
)	O
,	O
and	O
consider	O
kl	O
(	O
q	O
(	O
x1	O
:	O
t	O
,	O
t	O
)	O
|ˆp	O
(	O
x1	O
:	O
t	O
,	O
t	O
)	O
)	O
≥	O
0	O
this	O
gives	O
the	O
lower	O
bound	O
(	O
cid:42	O
)	O
log	O
u	O
(	O
π	O
)	O
≥	O
−h	O
(	O
q	O
(	O
x1	O
:	O
t	O
,	O
t	O
)	O
)	O
+	O
log	O
ut	O
(	O
xt	O
)	O
(	O
cid:43	O
)	O
p	O
(	O
xτ|xτ−1	O
,	O
d	O
(	O
xτ−1	O
)	O
)	O
q	O
(	O
x1	O
:	O
t	O
,	O
t	O
)	O
t	O
(	O
cid:89	O
)	O
τ	O
=1	O
in	O
terms	O
of	O
an	O
em	O
algorithm	B
,	O
the	O
m-step	O
requires	O
the	O
dependency	O
on	O
d	O
alone	O
,	O
which	O
is	O
t	O
(	O
cid:88	O
)	O
t	O
(	O
cid:88	O
)	O
t=1	O
t	O
(	O
cid:88	O
)	O
t	O
(	O
cid:88	O
)	O
τ	O
=1	O
t=1	O
τ	O
=1	O
e	O
(	O
d	O
)	O
=	O
=	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
xτ|xτ−1	O
,	O
d	O
(	O
xτ−1	O
)	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
xτ	O
,	O
xτ−1	O
,	O
t	O
)	O
q	O
(	O
xτ	O
=	O
s	O
(	O
cid:48	O
)	O
,	O
xτ−1	O
=	O
s	O
,	O
t	O
)	O
log	O
p	O
(	O
xτ	O
=	O
s	O
(	O
cid:48	O
)	O
|xτ−1	O
=	O
s	O
,	O
d	O
(	O
xτ−1	O
)	O
=	O
d	O
)	O
(	O
7.7.34	O
)	O
(	O
7.7.35	O
)	O
(	O
7.7.36	O
)	O
(	O
7.7.37	O
)	O
(	O
7.7.38	O
)	O
for	O
each	O
given	O
state	O
s	O
we	O
now	O
attempt	O
to	O
ﬁnd	O
the	O
optimal	O
decision	O
d	O
,	O
which	O
corresponds	O
to	O
maximising	O
(	O
cid:41	O
)	O
q	O
(	O
xτ	O
=	O
s	O
(	O
cid:48	O
)	O
,	O
xτ−1	O
=	O
s	O
,	O
t	O
)	O
(	O
cid:40	O
)	O
t	O
(	O
cid:88	O
)	O
t	O
(	O
cid:88	O
)	O
t=1	O
τ	O
=1	O
ˆe	O
(	O
d|s	O
)	O
=	O
(	O
cid:88	O
)	O
t	O
(	O
cid:88	O
)	O
t	O
(	O
cid:88	O
)	O
s	O
(	O
cid:48	O
)	O
q	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
)	O
∝	O
t=1	O
τ	O
=1	O
deﬁning	O
q	O
(	O
xτ	O
=	O
s	O
(	O
cid:48	O
)	O
,	O
xτ−1	O
=	O
s	O
,	O
t	O
)	O
log	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
,	O
d	O
)	O
(	O
7.7.39	O
)	O
(	O
7.7.40	O
)	O
we	O
see	O
that	O
for	O
given	O
s	O
,	O
up	O
to	O
a	O
constant	O
,	O
ˆe	O
(	O
d|s	O
)	O
is	O
the	O
kullback-leibler	O
divergence	B
between	O
q	O
(	O
s	O
(	O
cid:48	O
)	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
aligned	O
with	O
q	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
,	O
d	O
)	O
so	O
that	O
the	O
optimal	O
decision	O
d	O
is	O
given	O
by	O
the	O
index	O
of	O
the	O
distribution	B
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
)	O
:	O
d∗	O
(	O
s	O
)	O
=	O
argmin	O
kl	O
(	O
cid:0	O
)	O
q	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
)	O
and	O
|s	O
,	O
d	O
)	O
most	O
closely	O
(	O
7.7.41	O
)	O
|s	O
,	O
d	O
)	O
(	O
cid:1	O
)	O
|s	O
)	O
|p	O
(	O
s	O
(	O
cid:48	O
)	O
d	O
128	O
draft	O
march	O
9	O
,	O
2010	O
further	O
topics	O
d1	O
h1	O
v1	O
d2	O
h2	O
v2	O
u2	O
d3	O
h3	O
v3	O
u3	O
h4	O
v4	O
u4	O
figure	O
7.12	O
:	O
an	O
example	O
partially	B
observable	I
markov	O
decision	O
process	O
(	O
pomdp	O
)	O
.	O
the	O
‘	O
hidden	B
’	O
variables	O
h	O
are	O
never	O
observed	O
.	O
in	O
solving	B
the	O
in-	O
ﬂuence	O
diagram	O
we	O
are	O
required	O
to	O
ﬁrst	O
sum	O
over	O
variables	O
that	O
are	O
never	O
observed	O
;	O
doing	O
so	O
will	O
cou-	O
ple	O
together	O
all	O
past	O
observed	O
variables	O
and	O
decisions	O
that	O
means	O
any	O
decision	O
at	O
time	O
t	O
will	O
depend	O
on	O
all	O
previous	O
decisions	O
.	O
note	O
that	O
the	O
no-forgetting	O
prin-	O
ciple	O
means	O
that	O
we	O
do	O
not	O
need	O
to	O
explicitly	O
write	O
that	O
each	O
decision	O
depends	O
on	O
all	O
previous	O
observa-	O
tions	O
–	O
this	O
is	O
implicitly	O
assumed	O
.	O
the	O
e-step	O
concerns	O
the	O
computation	O
of	O
the	O
marginal	B
distributions	O
required	O
in	O
the	O
m-step	O
.	O
the	O
optimal	O
q	O
distribution	B
is	O
proportional	O
to	O
ˆp	O
evaluated	O
at	O
the	O
previous	O
decision	B
function	I
d	O
:	O
t	O
(	O
cid:89	O
)	O
τ	O
=1	O
q	O
(	O
x1	O
:	O
t	O
,	O
t	O
)	O
∝	O
ut	O
(	O
xt	O
)	O
p	O
(	O
xτ|xτ−1	O
,	O
d	O
(	O
xτ−1	O
)	O
)	O
(	O
7.7.42	O
)	O
(	O
7.7.43	O
)	O
(	O
7.7.44	O
)	O
for	O
a	O
constant	O
discount	B
factor	I
γ	O
at	O
each	O
time-step	O
and	O
an	O
otherwise	O
stationary	B
utility6	O
ut	O
(	O
xt	O
)	O
=	O
γtu	O
(	O
xt	O
)	O
using	O
this	O
q	O
(	O
x1	O
:	O
t	O
,	O
t	O
)	O
∝	O
γtu	O
(	O
xt	O
)	O
t	O
(	O
cid:89	O
)	O
τ	O
=1	O
p	O
(	O
xτ|xτ−1	O
,	O
d	O
(	O
xτ−1	O
)	O
)	O
for	O
each	O
t	O
this	O
is	O
a	O
simple	O
markov	O
chain	B
for	O
which	O
the	O
pairwise	B
transition	O
marginals	O
required	O
for	O
the	O
m-step	O
,	O
equation	B
(	O
7.7.40	O
)	O
are	O
straightforward	O
.	O
this	O
requires	O
inference	B
in	O
a	O
series	O
of	O
markov	O
models	O
of	O
diﬀerent	O
lengths	O
.	O
this	O
can	O
be	O
done	O
eﬃciently	O
using	O
a	O
single	O
forward	O
and	O
backward	O
pass	O
[	O
279	O
]	O
.	O
see	O
mdpemdeterministicpolicy.m	O
which	O
also	O
deals	O
with	O
the	O
more	O
general	O
case	O
of	O
utilities	O
dependent	O
on	O
the	O
decision	O
(	O
action	O
)	O
as	O
well	O
as	O
the	O
state	O
.	O
note	O
that	O
this	O
em	O
algorithm	B
formally	O
fails	O
in	O
the	O
case	O
of	O
a	O
deterministic	B
environment	O
(	O
the	O
transition	O
p	O
(	O
xt|xt−1	O
,	O
dt−1	O
)	O
is	O
deterministic	B
)	O
–	O
see	O
exercise	O
(	O
75	O
)	O
for	O
an	O
explanation	O
and	O
exercise	O
(	O
76	O
)	O
for	O
a	O
possible	O
resolution	B
.	O
7.8	O
further	O
topics	O
7.8.1	O
partially	B
observable	I
mdps	O
in	O
a	O
pomdp	O
there	O
are	O
states	O
that	O
are	O
not	O
observed	O
.	O
this	O
seemingly	O
innocuous	O
extension	O
of	O
the	O
mdp	O
case	O
can	O
lead	O
however	O
to	O
computational	O
diﬃculties	O
.	O
let	O
’	O
s	O
consider	O
the	O
situation	O
in	O
ﬁg	O
(	O
7.12	O
)	O
,	O
and	O
attempt	O
to	O
compute	O
the	O
optimal	O
expected	O
utility	B
based	O
on	O
the	O
sequence	O
of	O
summations	O
and	O
maximisations	O
:	O
u	O
=	O
max	O
d1	O
max	O
d2	O
max	O
d3	O
p	O
(	O
h4|h3	O
,	O
d3	O
)	O
p	O
(	O
v3|h3	O
)	O
p	O
(	O
h3|h2	O
,	O
d2	O
)	O
p	O
(	O
v2|h2	O
)	O
p	O
(	O
h2|h1	O
,	O
d1	O
)	O
p	O
(	O
v1|h1	O
)	O
p	O
(	O
h1	O
)	O
(	O
cid:88	O
)	O
v2	O
(	O
cid:88	O
)	O
v3	O
(	O
cid:88	O
)	O
h1:3	O
the	O
sum	O
over	O
the	O
hidden	B
variables	I
h1:3	O
couples	O
all	O
the	O
decisions	O
and	O
observations	O
,	O
meaning	O
that	O
we	O
no	O
longer	O
have	O
a	O
simple	O
chain	O
structure	B
for	O
the	O
remaining	O
maximisations	O
.	O
for	O
a	O
pomdp	O
of	O
length	O
t	O
,	O
this	O
leads	O
to	O
intractable	O
problem	O
with	O
complexity	O
exponential	B
in	O
t.	O
an	O
alternative	O
view	O
is	O
to	O
recognise	O
that	O
all	O
past	O
decisions	O
and	O
observations	O
v1	O
:	O
t	O
,	O
d1	O
:	O
t−1	O
,	O
can	O
be	O
summarised	O
in	O
terms	O
of	O
a	O
belief	O
in	O
the	O
current	O
latent	B
state	O
,	O
p	O
(	O
ht|v1	O
:	O
t	O
,	O
d1	O
:	O
t−1	O
)	O
.	O
this	O
suggests	O
that	O
instead	O
of	O
having	O
an	O
actual	O
state	O
,	O
as	O
in	O
the	O
mdp	O
case	O
,	O
we	O
need	O
6in	O
the	O
standard	O
mdp	O
framework	O
it	O
is	O
more	O
common	O
to	O
deﬁne	O
ut	O
(	O
xt	O
)	O
=	O
γt−1u	O
(	O
xt	O
)	O
so	O
that	O
for	O
comparison	O
with	O
the	O
standard	O
policy/value	O
routines	O
one	O
needs	O
to	O
divide	O
the	O
expected	O
utility	B
by	O
γ.	O
draft	O
march	O
9	O
,	O
2010	O
129	O
further	O
topics	O
to	O
use	O
a	O
distribution	B
over	O
states	O
to	O
represent	O
our	O
current	O
knowledge	O
.	O
one	O
can	O
therefore	O
write	O
down	O
an	O
eﬀective	O
mdp	O
albeit	O
over	O
belief	O
distributions	O
,	O
as	O
opposed	O
to	O
ﬁnite	O
states	O
.	O
approximate	B
techniques	O
are	O
required	O
to	O
solve	O
the	O
resulting	O
‘	O
inﬁnite	O
’	O
state	O
mdps	O
,	O
and	O
the	O
reader	O
is	O
referred	O
to	O
more	O
specialised	O
texts	O
for	O
a	O
study	O
of	O
approximation	B
procedures	O
.	O
see	O
for	O
example	O
[	O
148	O
,	O
151	O
]	O
.	O
7.8.2	O
restricted	B
utility	O
functions	O
an	O
alternative	O
to	O
solving	B
mdps	O
is	O
to	O
consider	O
restricted	B
utilities	O
such	O
that	O
the	O
policy	B
can	O
be	O
found	O
easily	O
.	O
recently	O
eﬃcient	B
solutions	O
have	O
been	O
developed	O
for	O
classes	O
of	O
mdps	O
with	O
utilities	O
restricted	B
to	O
kullback-	O
leibler	O
divergences	O
[	O
152	O
,	O
278	O
]	O
.	O
7.8.3	O
reinforcement	B
learning	I
reinforcement	O
learning	B
deals	O
mainly	O
with	O
stationary	O
markov	O
decision	O
processes	O
.	O
the	O
added	O
twist	O
is	O
that	O
the	O
transition	O
p	O
(	O
s	O
(	O
cid:48	O
)	O
|s	O
,	O
d	O
)	O
(	O
and	O
possibly	O
the	O
utility	B
)	O
is	O
unknown	O
.	O
initially	O
an	O
‘	O
agent	O
’	O
begins	O
to	O
explore	O
the	O
set	O
of	O
states	O
and	O
utilities	O
(	O
rewards	O
)	O
associated	O
with	O
taking	O
decisions	O
.	O
the	O
set	O
of	O
accessible	O
states	O
and	O
their	O
rewards	O
populates	O
as	O
the	O
agent	O
traverses	O
its	O
environment	O
.	O
consider	O
for	O
example	O
a	O
maze	O
problem	B
with	O
a	O
given	O
start	O
and	O
goal	O
state	O
,	O
though	O
with	O
an	O
unknown	O
maze	O
structure	B
.	O
the	O
task	O
is	O
to	O
get	O
from	O
the	O
start	O
to	O
the	O
goal	O
in	O
the	O
minimum	O
number	O
of	O
moves	O
on	O
the	O
maze	O
.	O
clearly	O
there	O
is	O
a	O
balance	O
required	O
between	O
curiosity	O
and	O
acting	O
to	O
maximise	O
the	O
expected	O
reward	O
.	O
if	O
we	O
are	O
too	O
curious	O
(	O
don	O
’	O
t	O
take	O
optimal	O
decisions	O
given	O
the	O
currently	O
available	O
information	O
about	O
the	O
maze	O
structure	B
)	O
and	O
continue	O
exploring	O
the	O
possible	O
maze	O
routes	O
,	O
this	O
may	O
be	O
bad	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
we	O
don	O
’	O
t	O
explore	O
the	O
possible	O
maze	O
states	O
,	O
we	O
might	O
never	O
realise	O
that	O
there	O
is	O
a	O
much	O
more	O
optimal	O
short-cut	O
to	O
follow	O
than	O
that	O
based	O
on	O
our	O
current	O
knowl-	O
edge	O
.	O
this	O
exploration-exploitation	O
tradeoﬀ	O
is	O
central	O
to	O
the	O
diﬃculties	O
of	O
rl	O
.	O
see	O
[	O
267	O
]	O
for	O
an	O
extensive	O
discussion	O
of	O
reinforcement	B
learning	I
.	O
for	O
a	O
given	O
set	O
of	O
environment	O
data	B
x	O
(	O
observed	O
transitions	O
and	O
utilities	O
)	O
one	O
aspect	O
of	O
rl	O
problem	B
can	O
be	O
considered	O
as	O
ﬁnding	O
the	O
policy	B
that	O
maximises	O
expected	O
reward	O
,	O
given	O
only	O
a	O
prior	B
belief	O
about	O
the	O
environment	O
and	O
observed	O
decisions	O
and	O
states	O
.	O
if	O
we	O
assume	O
we	O
know	O
the	O
utility	B
function	O
but	O
not	O
the	O
transition	O
,	O
we	O
may	O
write	O
u	O
(	O
π|x	O
)	O
=	O
(	O
cid:104	O
)	O
u	O
(	O
π|θ	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
θ|x	O
)	O
where	O
θ	O
represents	O
the	O
environment	O
state	O
transition	O
,	O
θ	O
=	O
p	O
(	O
xt+1|xt	O
,	O
dt	O
)	O
given	O
a	O
set	O
of	O
observed	O
states	O
and	O
decisions	O
,	O
p	O
(	O
θ|x	O
)	O
∝	O
p	O
(	O
x|θ	O
)	O
p	O
(	O
θ	O
)	O
(	O
7.8.1	O
)	O
(	O
7.8.2	O
)	O
(	O
7.8.3	O
)	O
where	O
p	O
(	O
θ	O
)	O
is	O
a	O
prior	B
on	O
the	O
transition	O
.	O
similar	O
techniques	O
to	O
the	O
em	O
style	O
training	B
can	O
be	O
carried	O
through	O
in	O
this	O
case	O
as	O
well	O
[	O
77	O
,	O
279	O
]	O
.	O
rather	O
than	O
the	O
policy	B
being	O
a	O
function	B
of	O
the	O
state	O
and	O
the	O
environment	O
θ	O
,	O
optimally	O
one	O
needs	O
to	O
consider	O
a	O
policy	B
p	O
(	O
dt|xt	O
,	O
b	O
(	O
θ	O
)	O
)	O
as	O
a	O
function	B
of	O
the	O
state	O
and	O
the	O
belief	O
in	O
the	O
environment	O
.	O
this	O
means	O
that	O
,	O
for	O
example	O
,	O
if	O
the	O
belief	O
in	O
the	O
environment	O
has	O
high	O
entropy	O
,	O
the	O
agent	O
can	O
recognise	O
this	O
and	O
explicitly	O
carry	O
out	O
decisions/actions	O
to	O
explore	O
the	O
environment	O
.	O
a	O
further	O
complication	O
in	O
rl	O
is	O
that	O
the	O
data	B
collected	O
x	O
depends	O
on	O
the	O
policy	B
π.	O
if	O
we	O
write	O
t	O
for	O
an	O
‘	O
episode	O
’	O
in	O
which	O
policy	B
πt	O
is	O
followed	O
and	O
data	B
xt	O
collected	O
,	O
then	O
the	O
utility	B
of	O
the	O
policy	B
π	O
given	O
all	O
the	O
historical	O
information	O
is	O
u	O
(	O
π|π1	O
:	O
t	O
,	O
x1	O
:	O
t	O
)	O
=	O
(	O
cid:104	O
)	O
u	O
(	O
π|θ	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
θ|x1	O
:	O
t	O
,	O
π1	O
:	O
t	O
)	O
(	O
7.8.4	O
)	O
depending	O
on	O
the	O
priors	O
on	O
the	O
environment	O
,	O
and	O
also	O
on	O
how	O
long	O
each	O
episode	O
is	O
,	O
we	O
will	O
have	O
diﬀerent	O
posteriors	O
for	O
the	O
environment	O
parameters	O
.	O
if	O
we	O
then	O
set	O
πt+1	O
=	O
argmax	O
π	O
u	O
(	O
π|π1	O
:	O
t	O
,	O
x1	O
:	O
t	O
)	O
(	O
7.8.5	O
)	O
this	O
aﬀects	O
the	O
data	B
we	O
collect	O
at	O
the	O
next	O
episode	O
xt+1	O
.	O
in	O
this	O
way	O
,	O
the	O
trajectory	O
of	O
policies	O
π1	O
,	O
π2	O
,	O
.	O
.	O
.	O
can	O
be	O
very	O
diﬀerent	O
depending	O
on	O
these	O
episode	O
lengths	O
and	O
priors	O
.	O
130	O
draft	O
march	O
9	O
,	O
2010	O
code	O
7.9	O
code	O
7.9.1	O
sum/max	O
under	O
a	O
partial	B
order	I
maxsumpot.m	O
:	O
generalised	B
elimination	O
operation	O
according	O
to	O
a	O
partial	O
ordering	O
sumpotid.m	O
:	O
sum/max	O
an	O
id	O
with	O
probability	O
and	O
decision	O
potentials	O
demodecparty.m	O
:	O
demo	O
of	O
summing/maxing	O
an	O
id	O
7.9.2	O
junction	O
trees	O
for	O
inﬂuence	B
diagrams	I
there	O
is	O
no	O
need	O
to	O
specify	O
the	O
information	O
links	O
provided	O
that	O
a	O
partial	O
ordering	O
is	O
given	O
.	O
in	O
the	O
code	O
jtreeid.m	O
no	O
check	O
is	O
made	O
that	O
the	O
partial	O
ordering	O
is	O
consistent	B
with	O
the	O
inﬂuence	B
diagram	I
.	O
in	O
this	O
case	O
,	O
the	O
ﬁrst	O
step	O
of	O
the	O
junction	B
tree	I
formulation	O
in	O
section	O
(	O
7.4.2	O
)	O
is	O
not	O
required	O
.	O
also	O
the	O
moralisation	B
and	O
removal	O
of	O
utility	B
nodes	O
is	O
easily	O
dealt	O
with	O
by	O
deﬁning	O
utility	B
potentials	O
and	O
including	O
them	O
in	O
the	O
moralisation	B
process	O
.	O
the	O
strong	B
triangulation	I
is	O
found	O
by	O
a	O
simple	O
variable	O
elimination	O
scheme	O
which	O
seeks	O
to	O
eliminate	O
a	O
variable	B
with	O
the	O
least	O
number	O
of	O
neighbours	O
,	O
provided	O
that	O
the	O
variable	B
may	O
be	O
eliminated	O
according	O
to	O
the	O
speciﬁed	O
partial	O
ordering	O
.	O
the	O
junction	B
tree	I
is	O
constructed	O
based	O
only	O
on	O
the	O
elimination	O
clique	B
sequence	O
c1	O
,	O
.	O
.	O
.	O
,	O
cn	O
.	O
obtained	O
from	O
the	O
triangulation	B
routine	O
.	O
the	O
junction	B
tree	I
is	O
then	O
obtained	O
by	O
connecting	O
a	O
clique	B
ci	O
to	O
the	O
ﬁrst	O
clique	O
j	O
>	O
i	O
that	O
is	O
connected	B
to	O
this	O
clique	B
.	O
clique	B
ci	O
is	O
then	O
eliminated	O
from	O
the	O
graph	B
.	O
in	O
this	O
manner	O
a	O
junction	B
tree	I
of	O
connected	B
cliques	O
is	O
formed	O
.	O
we	O
do	O
not	O
require	O
the	O
separators	O
for	O
the	O
inﬂuence	B
diagram	I
absorption	O
since	O
these	O
can	O
be	O
computed	O
and	O
discarded	O
on	O
the	O
ﬂy	O
.	O
note	O
that	O
the	O
code	O
only	O
computes	O
messages	O
from	O
the	O
leaves	O
to	O
the	O
root	O
of	O
the	O
junction	B
tree	I
,	O
which	O
is	O
suﬃcient	O
for	O
taking	O
decisions	O
at	O
the	O
root	O
.	O
if	O
one	O
desires	O
an	O
optimal	O
decision	O
at	O
a	O
non-root	O
,	O
one	O
would	O
need	O
to	O
absorb	O
probabilities	O
into	O
a	O
clique	B
which	O
contains	O
the	O
decision	O
required	O
.	O
these	O
extra	O
forward	O
probability	O
absorptions	O
are	O
required	O
because	O
information	O
about	O
any	O
unobserved	O
variables	O
can	O
be	O
aﬀected	O
by	O
decisions	O
and	O
observations	O
in	O
the	O
past	O
.	O
this	O
extra	O
forward	O
probability	O
schedule	B
is	O
not	O
given	O
in	O
the	O
code	O
and	O
left	O
as	O
an	O
exercise	O
for	O
the	O
interested	O
reader	O
.	O
jtreeid.m	O
:	O
junction	B
tree	I
for	O
an	O
inﬂuence	B
diagram	I
absorptionid.m	O
:	O
absorption	B
on	O
an	O
inﬂuence	B
diagram	I
triangulateporder.m	O
:	O
triangulation	B
based	O
on	O
a	O
partial	O
ordering	O
demodecphd.m	O
:	O
demo	O
for	O
utility	B
of	O
doing	O
phd	O
and	O
startup	O
7.9.3	O
party-friend	O
example	O
the	O
code	O
below	O
implements	O
the	O
party-friend	O
example	O
in	O
the	O
text	O
.	O
to	O
deal	O
with	O
the	O
asymmetry	B
the	O
v	O
isit	O
utility	B
is	O
zero	O
if	O
p	O
arty	O
is	O
in	O
state	O
yes	O
.	O
demodecpartyfriend.m	O
:	O
demo	O
for	O
party-friend	O
7.9.4	O
chest	B
clinic	I
with	O
decisions	O
the	O
table	O
for	O
the	O
chest	B
clinic	I
decision	O
network	O
,	O
ﬁg	O
(	O
7.13	O
)	O
is	O
taken	O
from	O
exercise	O
(	O
24	O
)	O
,	O
see	O
[	O
119	O
,	O
69	O
]	O
.	O
there	O
if	O
an	O
x-ray	O
is	O
taken	O
,	O
then	O
information	O
about	O
x	O
is	O
is	O
a	O
slight	O
modiﬁcation	O
however	O
to	O
the	O
p	O
(	O
x|e	O
)	O
table	O
.	O
available	O
.	O
however	O
,	O
if	O
the	O
decision	O
is	O
not	O
to	O
take	O
an	O
x-ray	O
no	O
information	O
about	O
x	O
is	O
available	O
.	O
this	O
is	O
a	O
form	O
of	O
asymmetry	B
.	O
a	O
straightforward	O
approach	B
in	O
this	O
case	O
is	O
to	O
make	O
dx	O
a	O
parent	O
of	O
the	O
x	O
variable	B
and	O
draft	O
march	O
9	O
,	O
2010	O
131	O
a	O
t	O
x	O
dx	O
ux	O
l	O
e	O
s	O
d	O
b	O
dh	O
uh	O
code	O
s	O
=	O
smoking	O
x	O
=	O
positive	O
x-ray	O
d	O
=	O
dyspnea	O
(	O
shortness	O
of	O
breath	O
)	O
e	O
=	O
either	O
tuberculosis	O
or	O
lung	O
cancer	O
t	O
=	O
tuberculosis	O
l	O
=	O
lung	O
cancer	O
b	O
=	O
bronchitis	O
a	O
=	O
visited	O
asia	O
dh	O
=	O
hospitalise	O
?	O
dx	O
=	O
take	O
x-ray	O
?	O
figure	O
7.13	O
:	O
inﬂuence	B
diagram	I
for	O
the	O
‘	O
chest	B
clinic	I
’	O
decision	O
example	O
.	O
set	O
the	O
distribution	B
of	O
x	O
to	O
be	O
uninformative	O
if	O
dx	O
=	O
fa	O
.	O
p	O
(	O
a	O
=	O
tr	O
)	O
=	O
0.01	O
p	O
(	O
s	O
=	O
tr	O
)	O
=	O
0.5	O
p	O
(	O
t	O
=	O
tr|a	O
=	O
tr	O
)	O
=	O
0.05	O
p	O
(	O
t	O
=	O
tr|a	O
=	O
fa	O
)	O
=	O
0.01	O
p	O
(	O
l	O
=	O
tr|s	O
=	O
tr	O
)	O
=	O
0.1	O
p	O
(	O
l	O
=	O
tr|s	O
=	O
fa	O
)	O
=	O
0.01	O
p	O
(	O
b	O
=	O
tr|s	O
=	O
tr	O
)	O
=	O
0.6	O
p	O
(	O
b	O
=	O
tr|s	O
=	O
fa	O
)	O
=	O
0.3	O
p	O
(	O
x	O
=	O
tr|e	O
=	O
tr	O
,	O
dx	O
=	O
tr	O
)	O
=	O
0.98	O
p	O
(	O
x	O
=	O
tr|e	O
=	O
fa	O
,	O
dx	O
=	O
tr	O
)	O
=	O
0.05	O
p	O
(	O
x	O
=	O
tr|e	O
=	O
fa	O
,	O
dx	O
=	O
fa	O
)	O
=	O
0.5	O
p	O
(	O
x	O
=	O
tr|e	O
=	O
tr	O
,	O
dx	O
=	O
fa	O
)	O
=	O
0.5	O
p	O
(	O
d	O
=	O
tr|e	O
=	O
tr	O
,	O
b	O
=	O
tr	O
)	O
=	O
0.9	O
p	O
(	O
d	O
=	O
tr|e	O
=	O
tr	O
,	O
b	O
=	O
fa	O
)	O
=	O
0.3	O
p	O
(	O
d	O
=	O
tr|e	O
=	O
fa	O
,	O
b	O
=	O
tr	O
)	O
=	O
0.2	O
p	O
(	O
d	O
=	O
tr|e	O
=	O
fa	O
,	O
b	O
=	O
fa	O
)	O
=	O
0.1	O
(	O
7.9.1	O
)	O
the	O
two	O
utilities	O
are	O
designed	O
to	O
reﬂect	O
the	O
costs	O
and	O
beneﬁts	O
of	O
taking	O
an	O
x-ray	O
and	O
hospitalising	O
a	O
patient	O
:	O
l	O
=	O
tr	O
180	O
t	O
=	O
tr	O
dh	O
=	O
tr	O
l	O
=	O
fa	O
120	O
t	O
=	O
tr	O
dh	O
=	O
tr	O
t	O
=	O
fa	O
l	O
=	O
tr	O
160	O
dh	O
=	O
tr	O
t	O
=	O
fa	O
l	O
=	O
fa	O
15	O
dh	O
=	O
tr	O
l	O
=	O
tr	O
2	O
dh	O
=	O
fa	O
t	O
=	O
tr	O
l	O
=	O
fa	O
4	O
dh	O
=	O
fa	O
t	O
=	O
tr	O
0	O
dh	O
=	O
fa	O
t	O
=	O
fa	O
l	O
=	O
tr	O
dh	O
=	O
fa	O
t	O
=	O
fa	O
l	O
=	O
fa	O
40	O
(	O
7.9.2	O
)	O
t	O
=	O
tr	O
0	O
dx	O
=	O
tr	O
t	O
=	O
fa	O
1	O
dx	O
=	O
tr	O
dx	O
=	O
fa	O
t	O
=	O
tr	O
10	O
dx	O
=	O
fa	O
t	O
=	O
fa	O
10	O
(	O
7.9.3	O
)	O
we	O
assume	O
that	O
we	O
know	O
whether	O
or	O
not	O
the	O
patient	O
has	O
been	O
to	O
asia	O
,	O
before	O
deciding	O
on	O
taking	O
an	O
x-ray	O
.	O
the	O
partial	O
ordering	O
is	O
then	O
a	O
≺	O
dx	O
≺	O
{	O
d	O
,	O
x	O
}	O
≺	O
dh	O
≺	O
{	O
b	O
,	O
e	O
,	O
l	O
,	O
s	O
,	O
t	O
}	O
the	O
demo	O
demodecasia.m	O
produces	O
the	O
results	O
:	O
utility	B
table	O
:	O
asia	O
=	O
yes	O
takexray	O
=	O
yes	O
49.976202	O
takexray	O
=	O
yes	O
46.989441	O
asia	O
=	O
no	O
48.433043	O
asia	O
=	O
yes	O
takexray	O
=	O
no	O
asia	O
=	O
no	O
takexray	O
=	O
no	O
47.460900	O
(	O
7.9.4	O
)	O
which	O
shows	O
that	O
optimally	O
one	O
should	O
take	O
an	O
x-ray	O
only	O
if	O
the	O
patient	O
has	O
been	O
to	O
asia	O
.	O
demodecasia.m	O
:	O
junction	B
tree	I
inﬂuence	O
diagram	O
demo	O
132	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
7.9.5	O
markov	O
decision	O
processes	O
in	O
demomdp.m	O
we	O
consider	O
a	O
simple	O
two	O
dimensional	O
grid	O
in	O
which	O
an	O
‘	O
agent	O
’	O
can	O
move	O
to	O
a	O
grid	O
square	O
either	O
above	O
,	O
below	O
,	O
left	O
,	O
right	O
of	O
the	O
current	O
square	O
,	O
or	O
stay	O
in	O
the	O
current	O
square	O
.	O
we	O
deﬁned	O
goal	O
states	O
(	O
grid	O
squares	O
)	O
that	O
have	O
high	O
utility	O
,	O
with	O
others	O
having	O
zero	O
utility	B
.	O
demomdpclean.m	O
:	O
demo	O
of	O
value	B
and	O
policy	B
iteration	I
for	O
a	O
simple	O
mdp	O
mdpsolve.m	O
:	O
mdp	O
solver	O
using	O
value	B
or	O
policy	B
iteration	I
mdp	O
solver	O
using	O
em	O
and	O
assuming	O
a	O
deterministic	B
policy	O
the	O
following	O
code7	O
is	O
not	O
fully	O
documented	O
in	O
the	O
text	O
,	O
although	O
the	O
method	O
is	O
reasonably	O
straightforward	O
and	O
follows	O
that	O
described	O
in	O
section	O
(	O
7.7.3	O
)	O
.	O
the	O
inference	B
is	O
carried	O
out	O
using	O
a	O
simple	O
α−β	O
style	O
recursion	O
.	O
this	O
could	O
also	O
be	O
implemented	O
using	O
the	O
general	O
factor	B
graph	I
code	O
,	O
but	O
was	O
coded	O
explicitly	O
for	O
reasons	O
of	O
speed	O
.	O
the	O
code	O
also	O
handles	O
the	O
more	O
general	O
case	O
of	O
utilities	O
(	O
rewards	O
)	O
as	O
a	O
function	B
of	O
both	O
the	O
state	O
and	O
the	O
action	O
u	O
(	O
xt	O
,	O
dt	O
)	O
.	O
mdpemdeterministicpolicy.m	O
:	O
mdp	O
solver	O
using	O
em	O
and	O
assuming	O
a	O
deterministic	B
policy	O
emqtranmarginal.m	O
:	O
marginal	B
information	O
required	O
for	O
the	O
transition	O
term	O
of	O
the	O
energy	B
emqutilmarginal.m	O
:	O
marginal	B
information	O
required	O
for	O
the	O
utility	B
term	O
of	O
the	O
energy	B
emtotalbetamessage.m	O
:	O
backward	O
information	O
required	O
for	O
inference	B
in	O
the	O
mdp	O
emminimizekl.m	O
:	O
find	O
the	O
optimal	O
decision	O
emvaluetable.m	O
:	O
return	O
the	O
expected	O
value	B
of	O
the	O
policy	B
7.10	O
exercises	O
exercise	O
67.	O
you	O
play	O
a	O
game	O
in	O
which	O
you	O
have	O
a	O
probability	B
p	O
of	O
winning	O
.	O
if	O
you	O
win	O
the	O
game	O
you	O
gain	O
an	O
amount	O
£s	O
and	O
if	O
you	O
lose	O
the	O
game	O
you	O
lose	O
an	O
amount	O
£s	O
.	O
show	O
that	O
the	O
expected	O
gain	O
from	O
playing	O
the	O
game	O
is	O
£	O
(	O
2p	O
−	O
1	O
)	O
s.	O
exercise	O
68.	O
it	O
is	O
suggested	O
that	O
the	O
utility	B
of	O
money	B
is	O
based	O
,	O
not	O
on	O
the	O
amount	O
,	O
but	O
rather	O
how	O
much	O
we	O
have	O
relative	O
to	O
other	O
peoples	O
.	O
assume	O
a	O
distribution	B
p	O
(	O
i	O
)	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
10	O
of	O
incomes	O
using	O
a	O
histogram	O
with	O
10	O
bins	O
,	O
each	O
bin	O
representing	O
an	O
income	O
range	O
.	O
use	O
a	O
histogram	O
to	O
roughly	O
reﬂect	O
the	O
distribution	B
of	O
incomes	O
in	O
society	O
,	O
namely	O
that	O
most	O
incomes	O
are	O
around	O
the	O
average	B
with	O
few	O
very	O
wealthy	O
and	O
few	O
extremely	O
poor	O
people	O
.	O
now	O
deﬁne	O
the	O
utility	B
of	O
an	O
income	O
x	O
as	O
the	O
chance	O
that	O
income	O
x	O
will	O
be	O
higher	O
than	O
a	O
randomly	O
chosen	O
income	O
y	O
(	O
under	O
the	O
distribution	B
you	O
deﬁned	O
)	O
and	O
relate	O
this	O
to	O
the	O
cumulative	O
distribution	B
of	O
p.	O
write	O
a	O
program	O
to	O
compute	O
this	O
probability	B
and	O
plot	O
the	O
resulting	O
utility	B
as	O
a	O
function	B
of	O
income	O
.	O
now	O
repeat	O
the	O
coin	O
tossing	O
bet	O
of	O
section	O
(	O
7.1.1	O
)	O
so	O
that	O
if	O
one	O
wins	O
the	O
bet	O
one	O
’	O
s	O
new	O
income	O
will	O
be	O
placed	O
in	O
the	O
top	O
histogram	O
bin	O
,	O
whilst	O
if	O
one	O
loses	O
one	O
’	O
s	O
new	O
income	O
is	O
in	O
the	O
lowest	O
bin	O
.	O
compare	O
the	O
optimal	O
expect	O
utility	B
decisions	O
under	O
the	O
situations	O
in	O
which	O
one	O
’	O
s	O
original	O
income	O
is	O
(	O
i	O
)	O
average	B
,	O
and	O
(	O
ii	O
)	O
much	O
higher	O
than	O
average	B
.	O
exercise	O
69.	O
derive	O
a	O
partial	O
ordering	O
for	O
the	O
id	O
on	O
the	O
right	O
,	O
and	O
explain	O
how	O
this	O
id	O
diﬀers	O
from	O
that	O
of	O
ﬁg	O
(	O
7.5	O
)	O
.	O
t	O
est	O
oil	O
u2	O
u1	O
seismic	O
drill	O
exercise	O
70.	O
this	O
question	O
follows	O
closely	O
demomdp.m	O
,	O
and	O
represents	O
a	O
problem	B
in	O
which	O
a	O
pilot	O
wishes	O
to	O
land	O
an	O
airplane	O
.	O
the	O
matrix	B
u	O
(	O
x	O
,	O
y	O
)	O
in	O
the	O
ﬁle	O
airplane.mat	O
contains	O
the	O
utilities	O
of	O
being	O
in	O
position	O
x	O
,	O
y	O
and	O
is	O
a	O
very	O
crude	O
model	B
of	O
a	O
runway	O
and	O
taxiing	O
area	O
.	O
7thanks	O
to	O
tom	O
furmston	O
for	O
coding	O
this	O
.	O
draft	O
march	O
9	O
,	O
2010	O
133	O
exercises	O
the	O
airspace	O
is	O
represented	O
by	O
an	O
18×15	O
grid	O
(	O
gx	O
=	O
18	O
,	O
gy	O
=	O
15	O
in	O
the	O
notation	O
employed	O
in	O
demomdp.m	O
)	O
.	O
the	O
matrix	B
u	O
(	O
8	O
,	O
4	O
)	O
=	O
2	O
represents	O
that	O
position	O
(	O
8	O
,	O
4	O
)	O
is	O
the	O
desired	O
parking	O
bay	O
of	O
the	O
airplane	O
(	O
the	O
vertical	O
height	O
of	O
the	O
airplane	O
is	O
not	O
taken	O
in	O
to	O
account	O
)	O
.	O
the	O
positive	O
values	O
in	O
u	O
represent	O
runway	O
and	O
areas	O
where	O
the	O
airplane	O
is	O
allowed	O
.	O
zero	O
utilities	O
represent	O
neutral	O
positions	O
.	O
the	O
negative	O
values	O
represent	O
unfavourable	O
positions	O
for	O
the	O
airplane	O
.	O
by	O
examining	O
the	O
matrix	B
u	O
you	O
will	O
see	O
that	O
the	O
airplane	O
should	O
preferably	O
not	O
veer	O
oﬀ	O
the	O
runway	O
,	O
and	O
also	O
should	O
avoid	O
two	O
small	O
villages	O
close	O
to	O
the	O
airport	O
.	O
at	O
each	O
timestep	O
the	O
plane	O
can	O
perform	O
one	O
of	O
the	O
following	O
actions	O
stay	O
up	O
down	O
left	O
right	O
:	O
for	O
stay	O
,	O
the	O
airplane	O
stays	O
in	O
the	O
same	O
x	O
,	O
y	O
position	O
.	O
for	O
up	O
,	O
the	O
airplane	O
moves	O
to	O
the	O
x	O
,	O
y	O
+	O
1	O
position	O
.	O
for	O
down	O
,	O
the	O
airplane	O
moves	O
to	O
the	O
x	O
,	O
y	O
−	O
1	O
position	O
.	O
for	O
left	O
,	O
the	O
airplane	O
moves	O
to	O
the	O
x	O
−	O
1	O
,	O
y	O
position	O
.	O
for	O
right	O
,	O
the	O
airplane	O
moves	O
to	O
the	O
x	O
+	O
1	O
,	O
y	O
position	O
.	O
a	O
move	O
that	O
takes	O
the	O
airplane	O
out	O
of	O
the	O
airspace	O
is	O
not	O
allowed	O
.	O
1.	O
the	O
airplane	O
begins	O
in	O
at	O
point	O
x	O
=	O
1	O
,	O
y	O
=	O
13.	O
assuming	O
that	O
an	O
action	O
deterministically	O
results	O
in	O
the	O
intended	O
grid	O
move	O
,	O
ﬁnd	O
the	O
optimal	O
xt	O
,	O
yt	O
sequence	O
for	O
times	O
t	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
for	O
the	O
position	O
of	O
the	O
aircraft	O
.	O
2.	O
the	O
pilot	O
tells	O
you	O
that	O
there	O
is	O
a	O
fault	O
with	O
the	O
airplane	O
.	O
when	O
the	O
pilot	O
instructs	O
the	O
plane	O
to	O
go	O
right	O
with	O
probability	O
0.1	O
it	O
actually	O
goes	O
up	O
(	O
provided	O
this	O
remains	O
in	O
the	O
airspace	O
)	O
.	O
assuming	O
again	O
that	O
the	O
airplane	O
begins	O
at	O
point	O
x	O
=	O
1	O
,	O
y	O
=	O
13	O
,	O
return	O
the	O
optimal	O
xt	O
,	O
yt	O
sequence	O
for	O
times	O
t	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
for	O
the	O
position	O
of	O
the	O
aircraft	O
.	O
exercise	O
71.	O
the	O
inﬂuence	B
diagram	I
depicted	O
describes	O
the	O
ﬁrst	O
stage	O
of	O
a	O
game	O
.	O
the	O
decision	O
variable	O
dom	O
(	O
d1	O
)	O
=	O
{	O
play	O
,	O
not	O
play	O
}	O
,	O
indicates	O
the	O
decision	O
to	O
either	O
play	O
the	O
ﬁrst	O
stage	O
or	O
not	O
.	O
if	O
you	O
decide	O
to	O
play	O
,	O
there	O
is	O
a	O
cost	O
c1	O
(	O
play	O
)	O
=	O
c1	O
,	O
but	O
no	O
cost	O
otherwise	O
,	O
c1	O
(	O
no	O
play	O
)	O
=	O
0.	O
the	O
variable	B
x1	O
describes	O
if	O
you	O
win	O
or	O
lose	O
the	O
game	O
,	O
dom	O
(	O
x1	O
)	O
=	O
{	O
win	O
,	O
lose	O
}	O
,	O
with	O
probabilities	O
:	O
p	O
(	O
x1	O
=	O
win|d1	O
=	O
play	O
)	O
=	O
p1	O
,	O
the	O
utility	B
of	O
winning/losing	O
is	O
p	O
(	O
x1	O
=	O
win|d1	O
=	O
no	O
play	O
)	O
=	O
0	O
(	O
7.10.1	O
)	O
u1	O
(	O
x1	O
=	O
win	O
)	O
=	O
w1	O
,	O
u1	O
(	O
x1	O
=	O
lose	O
)	O
=	O
0	O
(	O
7.10.2	O
)	O
c1	O
d1	O
x1	O
u1	O
show	O
that	O
the	O
expected	O
utility	B
gain	O
of	O
playing	O
this	O
game	O
is	O
u	O
(	O
d1	O
=	O
play	O
)	O
=	O
p1w1	O
−	O
c1	O
(	O
7.10.3	O
)	O
if	O
you	O
win	O
the	O
exercise	O
72.	O
exercise	O
(	O
71	O
)	O
above	O
describes	O
the	O
ﬁrst	O
stage	O
of	O
a	O
new	O
two-stage	O
game	O
.	O
ﬁrst	O
stage	O
x1	O
=	O
win	O
,	O
you	O
have	O
to	O
make	O
a	O
decision	O
d2	O
as	O
to	O
whether	O
or	O
not	O
play	O
in	O
the	O
second	O
stage	O
dom	O
(	O
d2	O
)	O
=	O
{	O
play	O
,	O
not	O
play	O
}	O
.	O
if	O
you	O
do	O
not	O
win	O
the	O
ﬁrst	O
stage	O
,	O
you	O
can	O
not	O
enter	O
the	O
second	O
stage	O
.	O
if	O
you	O
decide	O
to	O
play	O
the	O
second	O
stage	O
,	O
you	O
win	O
with	O
probability	O
p2	O
:	O
p	O
(	O
x2	O
=	O
win|x1	O
=	O
win	O
,	O
d2	O
=	O
play	O
)	O
=	O
p2	O
if	O
you	O
decide	O
not	O
to	O
play	O
the	O
second	O
stage	O
there	O
is	O
no	O
chance	O
to	O
win	O
:	O
p	O
(	O
x2	O
=	O
win|x1	O
=	O
win	O
,	O
d2	O
=	O
not	O
play	O
)	O
=	O
0	O
the	O
cost	O
of	O
playing	O
the	O
second	O
stage	O
is	O
c2	O
(	O
d2	O
=	O
play	O
)	O
=	O
c2	O
,	O
c2	O
(	O
d2	O
=	O
no	O
play	O
)	O
=	O
0	O
(	O
7.10.4	O
)	O
(	O
7.10.5	O
)	O
(	O
7.10.6	O
)	O
134	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
and	O
the	O
utility	B
of	O
winning/losing	O
the	O
second	O
stage	O
is	O
u2	O
(	O
x2	O
=	O
win	O
)	O
=	O
w2	O
,	O
u2	O
(	O
x2	O
=	O
lose	O
)	O
=	O
0	O
1.	O
draw	O
an	O
inﬂuence	B
diagram	I
that	O
describes	O
this	O
two-stage	O
game	O
.	O
(	O
7.10.7	O
)	O
2.	O
a	O
gambler	O
needs	O
to	O
decide	O
if	O
he	O
should	O
even	O
enter	O
the	O
ﬁrst	O
stage	O
of	O
this	O
two-stage	O
game	O
.	O
show	O
that	O
based	O
on	O
taking	O
the	O
optimal	O
future	O
decision	O
d2	O
the	O
expected	O
utility	B
based	O
on	O
the	O
ﬁrst	O
decision	O
is	O
:	O
u	O
(	O
d1	O
=	O
play	O
)	O
=	O
(	O
cid:26	O
)	O
p1	O
(	O
p2w2	O
−	O
c2	O
)	O
+	O
p1w1	O
−	O
c1	O
p1w1	O
−	O
c1	O
if	O
p2w2	O
−	O
c2	O
≥	O
0	O
if	O
p2w2	O
−	O
c2	O
≤	O
0	O
(	O
7.10.8	O
)	O
exercise	O
73.	O
you	O
have	O
£b	O
in	O
your	O
bank	O
account	O
.	O
you	O
are	O
asked	O
if	O
you	O
would	O
like	O
to	O
participate	O
in	O
a	O
bet	O
in	O
which	O
,	O
if	O
you	O
win	O
,	O
your	O
bank	O
account	O
will	O
become	O
£w	O
.	O
however	O
,	O
if	O
you	O
lose	O
,	O
your	O
bank	O
account	O
will	O
contain	O
only	O
£l	O
.	O
you	O
win	O
the	O
bet	O
with	O
probability	O
pw	O
.	O
1.	O
assuming	O
that	O
the	O
utility	B
is	O
given	O
by	O
the	O
number	O
of	O
pounds	O
in	O
your	O
bank	O
account	O
,	O
write	O
down	O
a	O
formula	O
for	O
the	O
expected	O
utility	B
of	O
taking	O
the	O
bet	O
,	O
u	O
(	O
bet	O
)	O
and	O
also	O
the	O
expected	O
utility	B
of	O
not	O
taking	O
the	O
bet	O
,	O
u	O
(	O
no	O
bet	O
)	O
.	O
2.	O
the	O
above	O
situation	O
can	O
be	O
formulated	O
diﬀerently	O
.	O
if	O
you	O
win	O
the	O
bet	O
you	O
gain	O
£	O
(	O
w	O
−	O
b	O
)	O
.	O
if	O
you	O
lose	O
the	O
bet	O
you	O
lose	O
£	O
(	O
b	O
−	O
l	O
)	O
.	O
compute	O
the	O
expected	O
amount	O
of	O
money	B
you	O
gain	O
if	O
you	O
bet	O
ugain	O
(	O
bet	O
)	O
and	O
if	O
you	O
don	O
’	O
t	O
bet	O
ugain	O
(	O
no	O
bet	O
)	O
.	O
3.	O
show	O
that	O
u	O
(	O
bet	O
)	O
−	O
u	O
(	O
no	O
bet	O
)	O
=	O
ugain	O
(	O
bet	O
)	O
−	O
ugain	O
(	O
no	O
bet	O
)	O
.	O
exercise	O
74.	O
consider	O
the	O
party-friend	O
scenario	O
,	O
example	O
(	O
30	O
)	O
.	O
an	O
alternative	O
is	O
to	O
replace	O
the	O
link	O
from	O
p	O
arty	O
to	O
uvisit	O
by	O
an	O
information	B
link	I
from	O
p	O
arty	O
to	O
v	O
isit	O
with	O
the	O
constraint	O
that	O
v	O
isit	O
can	O
be	O
in	O
state	O
yes	O
only	O
if	O
p	O
arty	O
is	O
in	O
state	O
no	O
.	O
1.	O
explain	O
how	O
this	O
constraint	O
can	O
be	O
achieved	O
by	O
including	O
an	O
additional	O
additive	O
term	O
to	O
the	O
utilities	O
and	O
modify	O
demodecpartyfriend.m	O
accordingly	O
to	O
demonstrate	O
this	O
.	O
2.	O
for	O
the	O
case	O
in	O
which	O
utilities	O
are	O
all	O
positive	O
,	O
explain	O
how	O
the	O
same	O
constraint	O
can	O
be	O
achieved	O
using	O
a	O
multiplicative	O
factor	B
.	O
exercise	O
75.	O
consider	O
an	O
objective	O
f	O
(	O
θ	O
)	O
=	O
(	O
cid:88	O
)	O
x	O
u	O
(	O
x	O
)	O
p	O
(	O
x|θ	O
)	O
(	O
7.10.9	O
)	O
for	O
a	O
positive	O
function	O
u	O
(	O
x	O
)	O
and	O
that	O
our	O
task	O
is	O
to	O
maximise	O
f	O
with	O
respect	O
to	O
θ.	O
an	O
expectation-	O
maximisation	B
style	O
bounding	O
approach	B
(	O
see	O
section	O
(	O
11.2	O
)	O
)	O
can	O
be	O
derived	O
by	O
deﬁning	O
the	O
auxiliary	O
distri-	O
bution	O
˜p	O
(	O
x|θ	O
)	O
=	O
u	O
(	O
x	O
)	O
p	O
(	O
x|θ	O
)	O
f	O
(	O
θ	O
)	O
so	O
that	O
by	O
considering	O
kl	O
(	O
q	O
(	O
x	O
)	O
|˜p	O
(	O
x	O
)	O
)	O
for	O
some	O
variational	O
distribution	O
q	O
(	O
x	O
)	O
we	O
obtain	O
the	O
bound	B
log	O
f	O
(	O
θ	O
)	O
≥	O
−	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
u	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x|θ	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
the	O
m-step	O
states	O
that	O
the	O
optimal	O
q	O
distribution	B
is	O
given	O
by	O
(	O
7.10.10	O
)	O
(	O
7.10.11	O
)	O
(	O
7.10.12	O
)	O
q	O
(	O
x	O
)	O
=	O
˜p	O
(	O
x|θold	O
)	O
at	O
the	O
e-step	O
of	O
the	O
algorithm	B
the	O
new	O
parameters	O
θnew	O
are	O
given	O
by	O
maximising	O
the	O
‘	O
energy	B
’	O
term	O
θnew	O
=	O
argmax	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x|θ	O
)	O
(	O
cid:105	O
)	O
˜p	O
(	O
x|θold	O
)	O
show	O
that	O
for	O
a	O
deterministic	B
distribution	O
θ	O
p	O
(	O
x|θ	O
)	O
=	O
δ	O
(	O
x	O
,	O
f	O
(	O
θ	O
)	O
)	O
the	O
e-step	O
fails	O
,	O
giving	O
θnew	O
=	O
θold	O
.	O
draft	O
march	O
9	O
,	O
2010	O
(	O
7.10.13	O
)	O
(	O
7.10.14	O
)	O
135	O
exercise	O
76.	O
consider	O
an	O
objective	O
f	O
(	O
θ	O
)	O
=	O
(	O
cid:88	O
)	O
u	O
(	O
x	O
)	O
p	O
(	O
x|θ	O
)	O
x	O
for	O
a	O
positive	O
function	O
u	O
(	O
x	O
)	O
and	O
p	O
(	O
x|θ	O
)	O
=	O
(	O
1	O
−	O
	O
)	O
δ	O
(	O
x	O
,	O
f	O
(	O
θ	O
)	O
)	O
+	O
n	O
(	O
x	O
)	O
,	O
0	O
≤	O
	O
≤	O
1	O
exercises	O
(	O
7.10.15	O
)	O
(	O
7.10.16	O
)	O
and	O
an	O
arbitrary	O
distribution	B
n	O
(	O
x	O
)	O
.	O
our	O
task	O
is	O
to	O
maximise	O
f	O
with	O
respect	O
to	O
θ.	O
as	O
the	O
previous	O
exercise	O
showed	O
,	O
if	O
we	O
attempt	O
an	O
em	O
algorithm	B
in	O
the	O
limit	O
of	O
a	O
deterministic	B
model	O
	O
=	O
0	O
,	O
then	O
no-updating	O
occurs	O
and	O
the	O
em	O
algorithm	B
fails	O
to	O
ﬁnd	O
θ	O
that	O
optimises	O
f0	O
(	O
θ	O
)	O
.	O
show	O
that	O
f	O
(	O
θ	O
)	O
=	O
(	O
1	O
−	O
	O
)	O
f0	O
(	O
θ	O
)	O
+	O
	O
and	O
hence	O
(	O
cid:88	O
)	O
x	O
n	O
(	O
x	O
)	O
u	O
(	O
x	O
)	O
f	O
(	O
θnew	O
)	O
−	O
f	O
(	O
θold	O
)	O
=	O
(	O
1	O
−	O
	O
)	O
[	O
f0	O
(	O
θnew	O
)	O
−	O
f0	O
(	O
θold	O
]	O
(	O
7.10.17	O
)	O
(	O
7.10.18	O
)	O
show	O
that	O
if	O
for	O
	O
>	O
0	O
we	O
can	O
ﬁnd	O
a	O
θnew	O
such	O
that	O
f	O
(	O
θnew	O
)	O
>	O
f	O
(	O
θold	O
)	O
,	O
then	O
necessarily	O
f0	O
(	O
θnew	O
)	O
>	O
f0	O
(	O
θold	O
)	O
.	O
using	O
this	O
result	O
,	O
derive	O
an	O
em-style	O
algorithm	B
that	O
guarantees	O
to	O
increase	O
f	O
(	O
θ	O
)	O
(	O
unless	O
we	O
are	O
already	O
at	O
the	O
optimum	O
)	O
for	O
	O
>	O
0	O
and	O
therefore	O
guarantees	O
to	O
increase	O
f0	O
(	O
θ	O
)	O
.	O
hint	O
:	O
use	O
˜p	O
(	O
x|θ	O
)	O
=	O
u	O
(	O
x	O
)	O
p	O
(	O
x|θ	O
)	O
f	O
(	O
θ	O
)	O
and	O
consider	O
kl	O
(	O
q	O
(	O
x	O
)	O
|˜p	O
(	O
x	O
)	O
)	O
for	O
some	O
variational	O
distribution	O
q	O
(	O
x	O
)	O
.	O
(	O
7.10.19	O
)	O
(	O
7.10.20	O
)	O
exercise	O
77.	O
the	O
ﬁle	O
idjensen.mat	O
contains	O
probability	B
and	O
utility	B
tables	O
for	O
the	O
inﬂuence	B
diagram	I
of	O
ﬁg	O
(	O
7.8a	O
)	O
.	O
using	O
brmltoolbox	O
,	O
write	O
a	O
program	O
that	O
returns	O
the	O
maximal	O
expected	O
utility	B
for	O
this	O
id	O
using	O
a	O
strong	B
junction	O
tree	B
approach	O
,	O
and	O
check	B
the	O
result	O
by	O
explicit	O
summation	O
and	O
maximisation	B
.	O
similarly	O
,	O
your	O
program	O
should	O
output	O
the	O
maximal	O
expected	O
utility	B
for	O
both	O
states	O
of	O
d1	O
,	O
and	O
check	B
that	O
the	O
computation	O
using	O
the	O
strong	B
junction	O
tree	B
agrees	O
with	O
the	O
result	O
from	O
explicit	O
elimination	O
summation	O
and	O
maximisation	B
.	O
exercise	O
78.	O
for	O
a	O
pomdp	O
,	O
explain	O
the	O
structure	B
of	O
the	O
strong	B
junction	O
tree	B
,	O
and	O
relate	O
this	O
to	O
the	O
complexity	O
of	O
inference	B
in	O
the	O
pomdp	O
.	O
exercise	O
79	O
.	O
(	O
i	O
)	O
deﬁne	O
a	O
partial	B
order	I
for	O
the	O
id	O
diagram	O
depicted	O
.	O
(	O
ii	O
)	O
draw	O
a	O
(	O
strong	B
)	O
junction	B
tree	I
for	O
this	O
id	O
.	O
i	O
b	O
f	O
u2	O
d	O
g	O
a	O
e	O
u1	O
c	O
h	O
136	O
draft	O
march	O
9	O
,	O
2010	O
part	O
ii	O
learning	B
in	O
probabilistic	B
models	O
137	O
chapter	O
8	O
statistics	O
for	O
machine	O
learning	B
:	O
8.1	O
distributions	O
deﬁnition	O
54	O
(	O
cumulative	O
distribution	B
function	O
)	O
.	O
for	O
a	O
univariate	B
distribution	O
p	O
(	O
x	O
)	O
,	O
the	O
cdf	O
is	O
deﬁned	O
as	O
cdf	O
(	O
y	O
)	O
≡	O
p	O
(	O
x	O
≤	O
y	O
)	O
=	O
(	O
cid:104	O
)	O
i	O
[	O
x	O
≤	O
y	O
]	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
)	O
for	O
an	O
unbounded	O
domain	B
,	O
cdf	O
(	O
−∞	O
)	O
=	O
0	O
and	O
cdf	O
(	O
∞	O
)	O
=	O
1	O
.	O
8.2	O
summarising	O
distributions	O
(	O
8.1.1	O
)	O
deﬁnition	O
55	O
(	O
mode	B
)	O
.	O
the	O
mode	B
x∗	O
of	O
a	O
distribution	B
p	O
(	O
x	O
)	O
is	O
the	O
state	O
of	O
x	O
at	O
which	O
the	O
distribution	B
takes	O
its	O
highest	O
value	B
,	O
x∗	O
=	O
argmax	O
p	O
(	O
x	O
)	O
.	O
a	O
distribution	B
could	O
have	O
more	O
than	O
one	O
node	B
(	O
be	O
multi-modal	O
)	O
.	O
a	O
widespread	O
abuse	O
of	O
terminology	O
is	O
to	O
refer	O
to	O
any	O
isolated	O
local	B
maximum	O
of	O
p	O
(	O
x	O
)	O
to	O
be	O
a	O
mode	B
.	O
x	O
deﬁnition	O
56	O
(	O
averages	O
and	O
expectation	B
)	O
.	O
(	O
cid:104	O
)	O
f	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
)	O
(	O
8.2.1	O
)	O
denotes	O
the	O
average	B
or	O
expectation	B
of	O
f	O
(	O
x	O
)	O
with	O
respect	O
to	O
the	O
distribution	B
p	O
(	O
x	O
)	O
.	O
a	O
common	O
alternative	O
notation	O
is	O
e	O
(	O
x	O
)	O
(	O
8.2.2	O
)	O
when	O
the	O
context	O
is	O
clear	O
,	O
one	O
may	O
drop	O
the	O
notational	O
dependency	O
on	O
p	O
(	O
x	O
)	O
.	O
the	O
notation	O
(	O
cid:104	O
)	O
f	O
(	O
x	O
)	O
|y	O
(	O
cid:105	O
)	O
(	O
8.2.3	O
)	O
is	O
shorthand	O
for	O
the	O
average	B
of	O
f	O
(	O
x	O
)	O
conditioned	O
on	O
knowing	O
the	O
state	O
of	O
variable	B
y	O
,	O
i.e	O
.	O
the	O
average	B
of	O
f	O
(	O
x	O
)	O
with	O
respect	O
to	O
the	O
distribution	B
p	O
(	O
x|y	O
)	O
.	O
139	O
an	O
advantage	O
of	O
the	O
expectation	B
notations	O
is	O
that	O
they	O
hold	O
whether	O
the	O
distribution	B
is	O
over	O
continuous	B
or	O
discrete	B
variables	O
.	O
in	O
the	O
discrete	B
case	O
summarising	O
distributions	O
(	O
cid:88	O
)	O
(	O
cid:90	O
)	O
∞	O
x	O
(	O
cid:104	O
)	O
f	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
≡	O
f	O
(	O
x	O
=	O
x	O
)	O
p	O
(	O
x	O
=	O
x	O
)	O
and	O
for	O
continuous	B
variables	O
,	O
(	O
cid:104	O
)	O
f	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
≡	O
−∞	O
f	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
dx	O
(	O
8.2.4	O
)	O
(	O
8.2.5	O
)	O
(	O
8.2.6	O
)	O
(	O
8.2.7	O
)	O
(	O
8.2.8	O
)	O
(	O
8.2.9	O
)	O
the	O
reader	O
might	O
wonder	O
what	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
means	O
when	O
x	O
is	O
discrete	B
.	O
if	O
dom	O
(	O
x	O
)	O
=	O
{	O
apple	O
,	O
orange	O
,	O
pear	O
}	O
,	O
with	O
associated	O
probabilities	O
p	O
(	O
x	O
)	O
for	O
each	O
of	O
the	O
states	O
,	O
what	O
does	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
refer	O
to	O
?	O
clearly	O
,	O
(	O
cid:104	O
)	O
f	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
makes	O
sense	O
if	O
f	O
(	O
x	O
=	O
x	O
)	O
maps	O
the	O
state	O
x	O
to	O
a	O
numerical	B
value	O
.	O
for	O
example	O
f	O
(	O
x	O
=	O
apple	O
)	O
=	O
1	O
,	O
f	O
(	O
x	O
=	O
orange	O
)	O
=	O
2	O
,	O
f	O
(	O
x	O
=	O
pear	O
)	O
=	O
3	O
for	O
which	O
(	O
cid:104	O
)	O
f	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
is	O
meaningful	O
.	O
unless	O
the	O
states	O
of	O
the	O
discrete	B
variable	O
are	O
associated	O
with	O
a	O
numerical	B
value	O
,	O
then	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
has	O
no	O
meaning	O
.	O
for	O
example	O
,	O
deﬁnition	O
57	O
(	O
moments	O
)	O
.	O
the	O
kth	O
moment	O
of	O
a	O
distribution	B
is	O
given	O
by	O
the	O
average	B
of	O
xk	O
under	O
the	O
distribution	B
:	O
for	O
k	O
=	O
1	O
,	O
we	O
have	O
the	O
mean	B
,	O
typically	O
denoted	O
by	O
µ	O
,	O
µ	O
≡	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
deﬁnition	O
58	O
(	O
variance	B
and	O
correlation	B
)	O
.	O
(	O
cid:68	O
)	O
xk	O
(	O
cid:69	O
)	O
p	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
(	O
x	O
−	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
)	O
2	O
(	O
cid:69	O
)	O
(	O
cid:68	O
)	O
(	O
cid:10	O
)	O
x2	O
(	O
cid:11	O
)	O
−	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
2	O
σ2	O
≡	O
σ2	O
≡	O
the	O
square	O
root	O
of	O
the	O
variance	B
,	O
σ	O
is	O
called	O
the	O
standard	B
deviation	I
.	O
the	O
notation	O
var	O
(	O
x	O
)	O
is	O
also	O
used	O
to	O
emphasise	O
for	O
which	O
variable	B
the	O
variance	B
is	O
computed	O
.	O
the	O
reader	O
may	O
show	O
that	O
an	O
equivalent	B
expression	O
is	O
for	O
a	O
multivariate	B
distribution	O
the	O
matrix	B
with	O
elements	O
σij	O
=	O
(	O
cid:104	O
)	O
(	O
xi	O
−	O
µi	O
)	O
(	O
xj	O
−	O
µj	O
)	O
(	O
cid:105	O
)	O
(	O
8.2.10	O
)	O
where	O
µi	O
=	O
(	O
cid:104	O
)	O
xi	O
(	O
cid:105	O
)	O
is	O
called	O
the	O
covariance	B
matrix	O
.	O
the	O
diagonal	O
entries	O
of	O
the	O
covariance	B
matrix	O
contain	O
the	O
variance	B
of	O
each	O
variable	B
.	O
an	O
equivalent	B
expression	O
is	O
the	O
correlation	B
matrix	O
has	O
elements	O
σij	O
=	O
(	O
cid:104	O
)	O
xixj	O
(	O
cid:105	O
)	O
−	O
(	O
cid:104	O
)	O
xi	O
(	O
cid:105	O
)	O
(	O
cid:104	O
)	O
xj	O
(	O
cid:105	O
)	O
(	O
cid:28	O
)	O
(	O
xi	O
−	O
µi	O
)	O
σi	O
ρij	O
=	O
(	O
cid:29	O
)	O
(	O
xj	O
−	O
µj	O
)	O
σj	O
(	O
8.2.11	O
)	O
(	O
8.2.12	O
)	O
where	O
σi	O
is	O
the	O
deviation	O
of	O
variable	B
xi	O
.	O
the	O
correlation	B
is	O
a	O
normalised	O
form	O
of	O
the	O
covariance	B
so	O
that	O
each	O
element	O
is	O
bounded	O
−1	O
≤	O
ρij	O
≤	O
1	O
.	O
140	O
draft	O
march	O
9	O
,	O
2010	O
summarising	O
distributions	O
for	O
independent	O
variables	O
xi	O
and	O
xj	O
,	O
xi⊥⊥	O
xj|∅	O
the	O
covariance	B
σij	O
is	O
zero	O
.	O
similarly	O
independent	O
variables	O
have	O
zero	O
correlation	B
–	O
they	O
are	O
‘	O
uncorrelated	O
’	O
.	O
note	O
however	O
that	O
the	O
converse	O
is	O
not	O
generally	O
true	O
–	O
two	O
variables	O
can	O
be	O
uncorrelated	O
but	O
dependent	O
.	O
a	O
special	O
case	O
is	O
for	O
when	O
xi	O
and	O
xj	O
are	O
gaussian	O
distributed	O
then	O
independence	B
is	O
equivalent	B
to	O
being	O
uncorrelated	O
,	O
see	O
exercise	O
(	O
81	O
)	O
.	O
deﬁnition	O
59	O
(	O
skewness	B
and	O
kurtosis	B
)	O
.	O
the	O
skewness	B
is	O
a	O
measure	O
of	O
the	O
asymmetry	B
of	O
a	O
distribution	B
:	O
(	O
cid:68	O
)	O
(	O
x	O
−	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
)	O
3	O
(	O
cid:69	O
)	O
σ3	O
p	O
(	O
x	O
)	O
γ1	O
≡	O
(	O
cid:68	O
)	O
(	O
x	O
−	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
)	O
4	O
(	O
cid:69	O
)	O
σ4	O
p	O
(	O
x	O
)	O
γ2	O
≡	O
where	O
σ2	O
is	O
the	O
variance	B
of	O
x	O
with	O
respect	O
to	O
p	O
(	O
x	O
)	O
.	O
a	O
positive	O
skewness	O
means	O
the	O
distribution	B
has	O
a	O
heavy	O
tail	O
to	O
the	O
right	O
.	O
similarly	O
,	O
a	O
negative	O
skewness	B
means	O
the	O
distribution	B
has	O
a	O
heavy	O
tail	O
to	O
the	O
left	O
.	O
the	O
kurtosis	B
is	O
a	O
measure	O
of	O
how	O
peaked	O
around	O
the	O
mean	B
a	O
distribution	B
is	O
:	O
(	O
8.2.13	O
)	O
−	O
3	O
(	O
8.2.14	O
)	O
a	O
distribution	B
with	O
positive	O
kurtosis	O
has	O
more	O
mass	O
around	O
its	O
mean	B
than	O
would	O
a	O
gaussian	O
with	O
the	O
same	O
mean	B
and	O
variance	B
.	O
these	O
are	O
also	O
called	O
super	B
gaussian	O
.	O
similarly	O
a	O
negative	O
kurtosis	B
(	O
sub	B
gaussian	O
)	O
distribution	B
has	O
less	O
mass	O
around	O
its	O
mean	B
than	O
the	O
corresponding	O
gaussian	O
.	O
the	O
kurtosis	B
is	O
deﬁned	O
such	O
that	O
a	O
gaussian	O
has	O
zero	O
kurtosis	B
(	O
which	O
accounts	O
for	O
the	O
-3	O
term	O
in	O
the	O
deﬁnition	O
)	O
.	O
deﬁnition	O
60	O
(	O
empirical	B
distribution	I
)	O
.	O
for	O
a	O
set	O
of	O
datapoints	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
which	O
are	O
states	O
of	O
a	O
random	O
variable	O
x	O
,	O
the	O
empirical	B
distribution	I
has	O
probability	B
mass	O
distributed	O
evenly	O
over	O
the	O
datapoints	O
,	O
and	O
zero	O
elsewhere	O
.	O
for	O
a	O
discrete	B
variable	O
x	O
the	O
empirical	B
distribution	I
is	O
n	O
(	O
cid:88	O
)	O
n=1	O
n	O
(	O
cid:88	O
)	O
n=1	O
p	O
(	O
x	O
)	O
=	O
1	O
n	O
i	O
[	O
x	O
=	O
xn	O
]	O
where	O
n	O
is	O
the	O
number	O
of	O
datapoints	O
.	O
for	O
a	O
continuous	B
distribution	O
we	O
have	O
p	O
(	O
x	O
)	O
=	O
1	O
n	O
δ	O
(	O
x	O
−	O
xn	O
)	O
where	O
δ	O
(	O
x	O
)	O
is	O
the	O
dirac	O
delta	B
function	I
.	O
the	O
sample	B
mean	O
of	O
the	O
datapoints	O
is	O
given	O
by	O
the	O
and	O
the	O
sample	B
variance	O
is	O
given	O
by	O
the	O
ˆµ	O
=	O
1	O
n	O
xn	O
n	O
(	O
cid:88	O
)	O
n	O
(	O
cid:88	O
)	O
n=1	O
n=1	O
ˆσ2	O
=	O
1	O
n	O
(	O
xn	O
−	O
ˆµ	O
)	O
2	O
draft	O
march	O
9	O
,	O
2010	O
(	O
8.2.15	O
)	O
(	O
8.2.16	O
)	O
(	O
8.2.17	O
)	O
(	O
8.2.18	O
)	O
141	O
summarising	O
distributions	O
for	O
vectors	O
the	O
sample	B
mean	O
vector	O
has	O
elements	O
n	O
(	O
cid:88	O
)	O
n=1	O
xn	O
i	O
ˆµi	O
=	O
1	O
n	O
and	O
sample	B
covariance	O
matrix	B
has	O
elements	O
n	O
(	O
cid:88	O
)	O
n=1	O
i	O
−	O
ˆµi	O
)	O
(	O
cid:0	O
)	O
xn	O
(	O
xn	O
j	O
−	O
ˆµj	O
(	O
cid:1	O
)	O
ˆσij	O
=	O
1	O
n	O
deﬁnition	O
61	O
(	O
delta	B
function	I
)	O
.	O
for	O
continuous	B
x	O
,	O
we	O
deﬁne	O
the	O
dirac	O
delta	B
function	I
which	O
is	O
zero	O
everywhere	O
expect	O
at	O
x0	O
,	O
where	O
there	O
is	O
a	O
spike	O
.	O
(	O
cid:82	O
)	O
∞	O
δ	O
(	O
x	O
−	O
x0	O
)	O
(	O
cid:90	O
)	O
∞	O
δ	O
(	O
x	O
−	O
x0	O
)	O
f	O
(	O
x	O
)	O
dx	O
=	O
f	O
(	O
x0	O
)	O
−∞	O
−∞	O
δ	O
(	O
x	O
−	O
x0	O
)	O
dx	O
=	O
1	O
and	O
one	O
can	O
view	O
the	O
dirac	O
delta	B
function	I
as	O
an	O
inﬁnitely	O
narrow	O
gaussian	O
:	O
δ	O
(	O
x	O
−	O
x0	O
)	O
=	O
limσ→0	O
n	O
the	O
kronecker	O
delta	O
,	O
(	O
8.2.19	O
)	O
(	O
8.2.20	O
)	O
(	O
8.2.21	O
)	O
(	O
8.2.22	O
)	O
(	O
cid:0	O
)	O
x	O
x0	O
,	O
σ2	O
(	O
cid:1	O
)	O
.	O
δx	O
,	O
x0	O
(	O
8.2.23	O
)	O
is	O
similarly	O
zero	O
everywhere	O
,	O
except	O
for	O
δx0	O
,	O
x0	O
=	O
1.	O
the	O
kronecker	O
delta	O
is	O
equivalent	B
to	O
δx	O
,	O
x0	O
=	O
i	O
[	O
x	O
=	O
x0	O
]	O
.	O
we	O
use	O
the	O
expression	O
δ	O
(	O
x	O
,	O
x0	O
)	O
to	O
denote	O
either	O
the	O
dirac	O
or	O
kronecker	O
delta	O
,	O
depending	O
on	O
the	O
context	O
.	O
8.2.1	O
estimator	B
bias	O
deﬁnition	O
62	O
(	O
unbiased	B
estimator	I
)	O
.	O
given	O
data	B
x	O
=	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
from	O
a	O
distribution	B
p	O
(	O
x|θ	O
)	O
we	O
can	O
use	O
the	O
data	B
x	O
to	O
estimate	O
the	O
parameter	B
θ	O
that	O
was	O
used	O
to	O
generate	O
the	O
data	B
.	O
the	O
estimator	B
is	O
a	O
function	B
of	O
the	O
data	B
,	O
which	O
we	O
write	O
ˆθ	O
(	O
x	O
)	O
.	O
for	O
an	O
unbiased	B
estimator	I
(	O
8.2.24	O
)	O
(	O
cid:69	O
)	O
(	O
cid:68	O
)	O
ˆθ	O
(	O
x	O
)	O
p	O
(	O
x|θ	O
)	O
=	O
θ	O
(	O
cid:69	O
)	O
(	O
cid:68	O
)	O
ˆψ	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
more	O
generally	O
,	O
one	O
can	O
consider	O
any	O
estimating	O
function	B
ˆψ	O
(	O
x	O
)	O
of	O
data	B
.	O
this	O
is	O
an	O
unbiased	B
estimator	I
of	O
a	O
quantity	O
ψ	O
if	O
=	O
ψ	O
.	O
1	O
2	O
3	O
4	O
figure	O
8.1	O
:	O
empirical	B
distribution	I
over	O
a	O
discrete	B
variable	O
with	O
4	O
states	O
.	O
the	O
empirical	B
samples	O
consist	O
of	O
n	O
samples	O
at	O
each	O
of	O
states	O
1	O
,	O
2	O
,	O
4	O
and	O
2n	O
samples	O
at	O
state	O
3	O
where	O
n	O
>	O
0.	O
on	O
normalising	O
this	O
gives	O
a	O
distribution	B
with	O
values	O
0.2	O
,	O
0.2	O
,	O
0.4	O
,	O
0.2	O
over	O
the	O
4	O
states	O
.	O
142	O
draft	O
march	O
9	O
,	O
2010	O
discrete	B
distributions	O
a	O
classical	O
example	O
for	O
estimator	B
bias	O
are	O
those	O
of	O
the	O
mean	B
and	O
variance	B
.	O
let	O
n	O
(	O
cid:88	O
)	O
n=1	O
xn	O
ˆµ	O
(	O
x	O
)	O
=	O
1	O
n	O
this	O
is	O
an	O
unbiased	B
estimator	I
of	O
the	O
mean	B
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
)	O
since	O
(	O
cid:104	O
)	O
xn	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
)	O
=	O
1	O
n	O
n	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
)	O
=	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
)	O
on	O
the	O
other	O
hand	O
,	O
consider	O
the	O
estimator	B
of	O
the	O
variance	B
,	O
n	O
(	O
cid:88	O
)	O
n=1	O
1	O
n	O
(	O
cid:104	O
)	O
ˆµ	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
)	O
=	O
n	O
(	O
cid:88	O
)	O
ˆσ2	O
(	O
x	O
)	O
=	O
1	O
n	O
n=1	O
(	O
cid:10	O
)	O
ˆσ2	O
(	O
x	O
)	O
(	O
cid:11	O
)	O
p	O
(	O
x	O
)	O
=	O
1	O
n	O
this	O
is	O
biased	O
since	O
(	O
omitting	O
a	O
few	O
lines	O
of	O
algebra	O
)	O
(	O
xn	O
−	O
ˆµ	O
(	O
x	O
)	O
)	O
2	O
n	O
(	O
cid:88	O
)	O
(	O
cid:68	O
)	O
(	O
xn	O
−	O
ˆµ	O
(	O
x	O
)	O
)	O
2	O
(	O
cid:69	O
)	O
n=1	O
=	O
n	O
−	O
1	O
n	O
σ2	O
(	O
8.2.25	O
)	O
(	O
8.2.26	O
)	O
(	O
8.2.27	O
)	O
(	O
8.2.28	O
)	O
8.3	O
discrete	B
distributions	O
deﬁnition	O
63	O
(	O
bernoulli	O
distribution	B
)	O
.	O
the	O
bernoulli	O
distribution	B
concerns	O
a	O
discrete	B
binary	O
variable	B
x	O
,	O
with	O
dom	O
(	O
x	O
)	O
=	O
{	O
0	O
,	O
1	O
}	O
.	O
the	O
states	O
are	O
not	O
merely	O
symbolic	O
,	O
but	O
real	O
values	O
0	O
and	O
1.	O
p	O
(	O
x	O
=	O
1	O
)	O
=	O
θ	O
from	O
normalisation	B
,	O
it	O
follows	O
that	O
p	O
(	O
x	O
=	O
0	O
)	O
=	O
1	O
−	O
θ.	O
from	O
this	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
=	O
0	O
×	O
p	O
(	O
x	O
=	O
0	O
)	O
+	O
1	O
×	O
p	O
(	O
x	O
=	O
1	O
)	O
=	O
θ	O
the	O
variance	B
is	O
given	O
by	O
var	O
(	O
x	O
)	O
=	O
θ	O
(	O
1	O
−	O
θ	O
)	O
.	O
(	O
8.3.1	O
)	O
(	O
8.3.2	O
)	O
deﬁnition	O
64	O
(	O
categorical	O
distribution	B
)	O
.	O
the	O
categorical	O
distribution	B
generalises	O
the	O
bernoulli	O
distri-	O
bution	O
to	O
more	O
than	O
two	O
(	O
symbolic	O
)	O
states	O
.	O
for	O
a	O
discrete	B
variable	O
x	O
,	O
with	O
symbolic	O
states	O
dom	O
(	O
x	O
)	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
c	O
}	O
,	O
p	O
(	O
x	O
=	O
c	O
)	O
=	O
θc	O
,	O
θc	O
=	O
1	O
(	O
8.3.3	O
)	O
(	O
cid:88	O
)	O
the	O
dirichlet	O
is	O
conjugate	B
to	O
the	O
categorical	O
distribution	B
.	O
c	O
deﬁnition	O
65	O
(	O
binomial	B
distribution	O
)	O
.	O
the	O
binomial	B
describes	O
the	O
distribution	B
of	O
a	O
discrete	B
two-state	O
variable	B
x	O
,	O
with	O
dom	O
(	O
x	O
)	O
=	O
{	O
1	O
,	O
0	O
}	O
where	O
the	O
states	O
are	O
symbolic	O
.	O
the	O
probability	B
that	O
in	O
n	O
bernoulli	O
trials	O
(	O
independent	O
samples	O
)	O
,	O
k	O
‘	O
success	O
’	O
states	O
1	O
will	O
be	O
observed	O
is	O
(	O
cid:18	O
)	O
n	O
k	O
(	O
cid:19	O
)	O
θk	O
(	O
1	O
−	O
θ	O
)	O
n−k	O
p	O
(	O
y	O
=	O
k|θ	O
)	O
=	O
draft	O
march	O
9	O
,	O
2010	O
(	O
8.3.4	O
)	O
143	O
where	O
(	O
cid:0	O
)	O
n	O
(	O
cid:1	O
)	O
is	O
the	O
binomial	B
coeﬃcient	O
.	O
the	O
mean	B
and	O
variance	B
are	O
k	O
(	O
cid:104	O
)	O
y	O
(	O
cid:105	O
)	O
=	O
nθ	O
,	O
var	O
(	O
x	O
)	O
=	O
nθ	O
(	O
1	O
−	O
θ	O
)	O
the	O
beta	B
distribution	O
is	O
the	O
conjugate	B
prior	O
for	O
the	O
binomial	B
distribution	O
.	O
(	O
8.3.5	O
)	O
continuous	B
distributions	O
deﬁnition	O
66	O
(	O
multinomial	B
distribution	O
)	O
.	O
consider	O
a	O
multi-state	O
variable	B
x	O
,	O
with	O
dom	O
(	O
x	O
)	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
,	O
with	O
corresponding	O
state	O
probabilities	O
θ1	O
,	O
.	O
.	O
.	O
,	O
θk	O
.	O
we	O
then	O
draw	O
n	O
samples	O
from	O
this	O
distribution	B
.	O
the	O
probability	B
of	O
observing	O
the	O
state	O
1	O
y1	O
times	O
,	O
state	O
2	O
y2	O
times	O
,	O
.	O
.	O
.	O
,	O
state	O
k	O
yk	O
times	O
in	O
the	O
n	O
samples	O
is	O
(	O
8.3.6	O
)	O
(	O
8.3.7	O
)	O
n	O
(	O
cid:89	O
)	O
θyi	O
i	O
n	O
!	O
y1	O
!	O
.	O
.	O
.	O
,	O
yk	O
!	O
i=1	O
p	O
(	O
y|θ	O
)	O
=	O
where	O
n	O
=	O
(	O
cid:80	O
)	O
n	O
i=1	O
yi	O
.	O
(	O
cid:104	O
)	O
yi	O
(	O
cid:105	O
)	O
=	O
nθi	O
,	O
var	O
(	O
yi	O
)	O
=	O
nθi	O
(	O
1	O
−	O
θi	O
)	O
,	O
(	O
cid:104	O
)	O
yiyj	O
(	O
cid:105	O
)	O
−	O
(	O
cid:104	O
)	O
yi	O
(	O
cid:105	O
)	O
(	O
cid:104	O
)	O
yj	O
(	O
cid:105	O
)	O
=	O
−nθiθj	O
(	O
i	O
(	O
cid:54	O
)	O
=	O
j	O
)	O
the	O
dirichlet	O
distribution	B
is	O
the	O
conjugate	B
prior	O
for	O
the	O
multinomial	B
distribution	O
.	O
deﬁnition	O
67	O
(	O
poisson	O
distribution	B
)	O
.	O
the	O
poisson	O
distribution	B
can	O
be	O
used	O
to	O
model	B
situations	O
in	O
which	O
the	O
expected	O
number	O
of	O
events	O
scales	O
with	O
the	O
length	O
of	O
the	O
interval	O
within	O
which	O
the	O
events	O
can	O
occur	O
.	O
if	O
λ	O
is	O
the	O
expected	O
number	O
of	O
events	O
per	O
unit	O
interval	O
,	O
then	O
the	O
distribution	B
of	O
the	O
number	O
of	O
events	O
x	O
within	O
an	O
interval	O
tλ	O
is	O
p	O
(	O
x	O
=	O
k|λ	O
)	O
=	O
−λt	O
(	O
λt	O
)	O
k	O
,	O
1	O
k	O
!	O
e	O
k	O
=	O
0	O
,	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
for	O
a	O
unit	O
length	O
interval	O
(	O
t	O
=	O
1	O
)	O
,	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
=	O
λ	O
,	O
var	O
(	O
x	O
)	O
=	O
λ	O
(	O
8.3.8	O
)	O
(	O
8.3.9	O
)	O
the	O
poisson	O
distribution	B
can	O
be	O
derived	O
as	O
a	O
limiting	O
case	O
of	O
a	O
binomial	B
distribution	O
in	O
which	O
the	O
success	O
probability	B
scales	O
as	O
θ	O
=	O
λ/n	O
,	O
in	O
the	O
limit	O
n	O
→	O
∞	O
.	O
8.4	O
continuous	B
distributions	O
8.4.1	O
bounded	O
distributions	O
deﬁnition	O
68	O
(	O
uniform	B
distribution	I
)	O
.	O
for	O
a	O
variable	B
x	O
,	O
the	O
distribution	B
is	O
uniform	B
if	O
p	O
(	O
x	O
)	O
=	O
const	O
.	O
over	O
the	O
domain	B
of	O
the	O
variable	B
.	O
deﬁnition	O
69	O
(	O
exponential	B
distribution	O
)	O
.	O
for	O
x	O
≥	O
0	O
,	O
−λx	O
p	O
(	O
x|λ	O
)	O
≡	O
λe	O
one	O
can	O
show	O
that	O
for	O
rate	O
λ	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
=	O
1	O
λ	O
,	O
var	O
(	O
x	O
)	O
=	O
1	O
λ2	O
(	O
8.4.1	O
)	O
(	O
8.4.2	O
)	O
144	O
draft	O
march	O
9	O
,	O
2010	O
continuous	B
distributions	O
(	O
a	O
)	O
:	O
exponential	B
figure	O
8.2	O
:	O
(	O
b	O
)	O
:	O
laplace	O
distribution	B
.	O
(	O
double	B
exponential	I
)	O
distribu-	O
tion	O
.	O
(	O
a	O
)	O
(	O
b	O
)	O
the	O
alternative	O
parameterisation	B
b	O
=	O
1/λ	O
is	O
called	O
the	O
scale	O
.	O
deﬁnition	O
70	O
(	O
gamma	B
distribution	O
)	O
.	O
(	O
cid:18	O
)	O
x	O
(	O
cid:19	O
)	O
α−1	O
gam	O
(	O
x|α	O
,	O
β	O
)	O
=	O
(	O
cid:90	O
)	O
∞	O
γ	O
(	O
a	O
)	O
=	O
ta−1e	O
−tdt	O
1	O
βγ	O
(	O
γ	O
)	O
β	O
−	O
x	O
β	O
,	O
e	O
x	O
≥	O
0	O
,	O
α	O
>	O
0	O
,	O
β	O
>	O
0	O
α	O
is	O
called	O
the	O
shape	O
parameter	B
,	O
β	O
is	O
the	O
scale	O
parameter	B
and	O
the	O
parameters	O
are	O
related	O
to	O
the	O
mean	B
and	O
variance	B
through	O
0	O
(	O
cid:16	O
)	O
µ	O
(	O
cid:17	O
)	O
2	O
s	O
α	O
=	O
,	O
β	O
=	O
s2	O
µ	O
(	O
8.4.3	O
)	O
(	O
8.4.4	O
)	O
(	O
8.4.5	O
)	O
where	O
µ	O
is	O
the	O
mean	B
of	O
the	O
distribution	B
and	O
s	O
is	O
the	O
standard	B
deviation	I
.	O
the	O
mode	B
is	O
given	O
by	O
(	O
α	O
−	O
1	O
)	O
β	O
,	O
for	O
α	O
≥	O
1.	O
an	O
alternative	O
parameterisation	B
uses	O
the	O
inverse	O
scale	O
gamis	O
(	O
x|α	O
,	O
b	O
)	O
=	O
gam	O
(	O
x|α	O
,	O
1/b	O
)	O
∝	O
xα−1e	O
−bx	O
deﬁnition	O
71	O
(	O
inverse	B
gamma	I
distribution	O
)	O
.	O
invgam	O
(	O
x|α	O
,	O
β	O
)	O
=	O
βα	O
γ	O
(	O
α	O
)	O
−β/x	O
1	O
xα+1	O
e	O
this	O
has	O
mean	B
β/	O
(	O
α	O
−	O
1	O
)	O
for	O
α	O
>	O
1	O
and	O
variance	B
β2	O
(	O
α−1	O
)	O
2	O
(	O
α−2	O
)	O
for	O
α	O
>	O
2.	O
deﬁnition	O
72	O
(	O
beta	B
distribution	O
)	O
.	O
1	O
p	O
(	O
x|α	O
,	O
β	O
)	O
=	O
b	O
(	O
x|α	O
,	O
β	O
)	O
=	O
b	O
(	O
α	O
,	O
β	O
)	O
xα−1	O
(	O
1	O
−	O
x	O
)	O
β−1	O
,	O
0	O
≤	O
x	O
≤	O
1	O
where	O
the	O
beta	B
function	O
is	O
deﬁned	O
as	O
b	O
(	O
α	O
,	O
β	O
)	O
=	O
γ	O
(	O
α	O
)	O
γ	O
(	O
β	O
)	O
γ	O
(	O
α	O
+	O
β	O
)	O
draft	O
march	O
9	O
,	O
2010	O
(	O
8.4.6	O
)	O
(	O
8.4.7	O
)	O
(	O
8.4.8	O
)	O
(	O
8.4.9	O
)	O
145	O
−101234500.511.5	O
λ=0.2λ=0.5λ=1λ=1.5−50500.20.40.60.81	O
λ=0.2λ=0.5λ=1λ=1.5	O
and	O
γ	O
(	O
x	O
)	O
is	O
the	O
gamma	B
function	O
.	O
note	O
that	O
the	O
distribution	B
can	O
be	O
ﬂipped	O
by	O
interchanging	O
x	O
for	O
1	O
−	O
x	O
,	O
which	O
is	O
equivalent	B
to	O
interchanging	O
α	O
and	O
β.	O
continuous	B
distributions	O
the	O
mean	B
is	O
given	O
by	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
=	O
α	O
α	O
+	O
β	O
,	O
var	O
(	O
x	O
)	O
=	O
αβ	O
(	O
α	O
+	O
β	O
)	O
2	O
(	O
α	O
+	O
β	O
+	O
1	O
)	O
8.4.2	O
unbounded	O
distributions	O
deﬁnition	O
73	O
(	O
laplace	O
(	O
double	B
exponential	I
)	O
distribution	B
)	O
.	O
p	O
(	O
x|λ	O
)	O
≡	O
λe	O
−	O
1	O
b	O
|x−µ|	O
for	O
scale	O
b	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
=	O
µ	O
,	O
var	O
(	O
x	O
)	O
=	O
2b2	O
univariate	B
gaussian	O
distribution	B
(	O
8.4.10	O
)	O
(	O
8.4.11	O
)	O
(	O
8.4.12	O
)	O
the	O
gaussian	O
distribution	B
is	O
an	O
important	O
distribution	B
in	O
science	O
.	O
it	O
’	O
s	O
technical	O
description	O
is	O
given	O
in	O
deﬁnition	O
(	O
74	O
)	O
.	O
deﬁnition	O
74	O
(	O
univariate	B
gaussian	O
distribution	B
)	O
.	O
(	O
cid:0	O
)	O
x	O
µ	O
,	O
σ2	O
(	O
cid:1	O
)	O
p	O
(	O
x|µ	O
,	O
σ2	O
)	O
=	O
n	O
1	O
√2πσ2	O
≡	O
2σ2	O
(	O
x−µ	O
)	O
2	O
−	O
1	O
e	O
(	O
8.4.13	O
)	O
where	O
µ	O
is	O
the	O
mean	B
of	O
the	O
distribution	B
,	O
and	O
σ2	O
the	O
variance	B
.	O
this	O
is	O
also	O
called	O
the	O
normal	B
distribution	I
.	O
one	O
can	O
show	O
that	O
the	O
parameters	O
indeed	O
correspond	O
to	O
µ	O
=	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
n	O
(	O
x	O
µ	O
,	O
σ2	O
)	O
,	O
σ2	O
=	O
n	O
(	O
x	O
µ	O
,	O
σ2	O
)	O
(	O
cid:68	O
)	O
(	O
x	O
−	O
µ	O
)	O
2	O
(	O
cid:69	O
)	O
for	O
µ	O
=	O
0	O
and	O
σ	O
=	O
1	O
,	O
the	O
gaussian	O
is	O
called	O
the	O
standard	B
normal	I
distribution	I
.	O
deﬁnition	O
75	O
(	O
student	O
’	O
s	O
t-distribution	O
)	O
.	O
p	O
(	O
x|µ	O
,	O
λ	O
,	O
ν	O
)	O
=	O
γ	O
(	O
ν+1	O
2	O
)	O
γ	O
(	O
ν	O
2	O
)	O
1	O
+	O
λ	O
(	O
x	O
−	O
µ	O
)	O
2	O
ν	O
(	O
cid:34	O
)	O
(	O
cid:19	O
)	O
1	O
2	O
(	O
cid:18	O
)	O
λ	O
νπ	O
(	O
cid:35	O
)	O
−	O
ν+1	O
2	O
(	O
8.4.14	O
)	O
(	O
8.4.15	O
)	O
(	O
a	O
)	O
(	O
b	O
)	O
146	O
draft	O
march	O
9	O
,	O
2010	O
figure	O
8.3	O
:	O
(	O
a	O
)	O
:	O
gamma	B
distri-	O
bution	O
with	O
varying	O
β	O
for	O
ﬁxed	O
(	O
b	O
)	O
:	O
gamma	B
distribution	O
α.	O
with	O
varying	O
α	O
for	O
ﬁxed	O
β	O
.	O
012345012345	O
α=1	O
β=0.2α=2	O
β=0.2α=5	O
β=0.2α=10	O
β=0.2012345012345	O
α=2	O
β=0.1α=2	O
β=0.5α=2	O
β=1α=2	O
β=2	O
multivariate	B
distributions	O
figure	O
8.4	O
:	O
top	O
:	O
200	O
datapoints	O
x1	O
,	O
.	O
.	O
.	O
,	O
x200	O
drawn	O
from	O
a	O
gaussian	O
distribution	B
.	O
each	O
vertical	O
line	O
denotes	O
a	O
datapoint	O
at	O
the	O
corresponding	O
x	O
value	B
on	O
the	O
horizontal	O
axis	O
.	O
middle	O
:	O
histogram	O
using	O
10	O
equally	O
spaced	O
bins	O
of	O
the	O
datapoints	O
.	O
bot-	O
tom	O
:	O
gaussian	O
distribution	B
n	O
(	O
x	O
µ	O
=	O
5	O
,	O
σ	O
=	O
3	O
)	O
from	O
which	O
the	O
in	O
the	O
limit	O
of	O
an	O
inﬁnite	O
amount	O
of	O
datapoints	O
were	O
drawn	O
.	O
data	B
,	O
and	O
limitingly	O
small	O
bin	O
size	O
,	O
the	O
normalised	O
histogram	O
tends	O
to	O
the	O
gaussian	O
probability	B
density	O
function	B
.	O
where	O
µ	O
is	O
the	O
mean	B
,	O
ν	O
the	O
degrees	O
of	O
freedom	O
,	O
and	O
λ	O
scales	O
the	O
distribution	B
.	O
the	O
variance	B
is	O
given	O
by	O
var	O
(	O
x	O
)	O
=	O
ν	O
λ	O
(	O
ν	O
−	O
2	O
)	O
,	O
for	O
ν	O
>	O
2	O
(	O
8.4.16	O
)	O
for	O
ν	O
→	O
∞	O
the	O
distribution	B
tends	O
to	O
a	O
gaussian	O
with	O
mean	O
µ	O
and	O
variance	B
1/λ	O
.	O
as	O
ν	O
decreases	O
the	O
tails	O
of	O
the	O
distribution	B
become	O
fatter	O
.	O
the	O
t-distribution	O
can	O
be	O
derived	O
from	O
a	O
scaled	B
mixture	I
p	O
(	O
x|a	O
,	O
b	O
)	O
=	O
=	O
(	O
cid:90	O
)	O
∞	O
(	O
cid:16	O
)	O
τ	O
τ	O
=0	O
n	O
(	O
cid:17	O
)	O
1	O
2	O
(	O
cid:0	O
)	O
x	O
µ	O
,	O
τ	O
(	O
cid:90	O
)	O
∞	O
2π	O
=	O
ba	O
γ	O
(	O
a	O
)	O
τ	O
=0	O
γ	O
(	O
a	O
+	O
1	O
2	O
)	O
√2π	O
2	O
(	O
x−µ	O
)	O
2	O
−	O
τ	O
e	O
−1	O
(	O
cid:1	O
)	O
gamis	O
(	O
τ|a	O
,	O
b	O
)	O
dτ	O
2	O
(	O
x	O
−	O
µ	O
)	O
2	O
(	O
cid:17	O
)	O
a+	O
1	O
(	O
cid:16	O
)	O
b	O
+	O
1	O
bae	O
−bτ	O
τ	O
a−1	O
1	O
1	O
2	O
γ	O
(	O
a	O
)	O
dτ	O
it	O
is	O
conventional	O
to	O
reparameterise	O
using	O
ν	O
=	O
2a	O
and	O
λ	O
=	O
a/b	O
.	O
8.5	O
multivariate	B
distributions	O
(	O
8.4.17	O
)	O
(	O
8.4.18	O
)	O
(	O
8.4.19	O
)	O
deﬁnition	O
76	O
(	O
dirichlet	O
distribution	B
)	O
.	O
the	O
dirichlet	O
distribution	B
is	O
a	O
distribution	B
on	O
probability	B
distri-	O
butions	O
:	O
(	O
8.5.1	O
)	O
(	O
8.5.2	O
)	O
(	O
8.5.3	O
)	O
147	O
(	O
cid:32	O
)	O
q	O
(	O
cid:88	O
)	O
(	O
cid:33	O
)	O
q	O
(	O
cid:89	O
)	O
q=1	O
uq−1	O
α	O
q	O
p	O
(	O
α	O
)	O
=	O
where	O
z	O
(	O
u	O
)	O
=	O
1	O
z	O
(	O
u	O
)	O
δ	O
i=1	O
(	O
cid:81	O
)	O
q	O
(	O
cid:16	O
)	O
(	O
cid:80	O
)	O
q	O
q=1	O
γ	O
(	O
uq	O
)	O
q=1	O
uq	O
γ	O
αi	O
−	O
1	O
(	O
cid:17	O
)	O
it	O
is	O
conventional	O
to	O
denote	O
the	O
distribution	B
as	O
dirichlet	O
(	O
α|u	O
)	O
draft	O
march	O
9	O
,	O
2010	O
−5051015−50510150102030−505101500.10.2	O
multivariate	B
gaussian	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
(	O
d	O
)	O
figure	O
8.5	O
:	O
dirichlet	O
distribution	B
with	O
parameter	B
(	O
u1	O
,	O
u2	O
,	O
u3	O
)	O
displayed	O
on	O
the	O
simplex	O
x1	O
,	O
x2	O
,	O
x3	O
≥	O
0	O
,	O
x1	O
+	O
x2	O
+	O
x3	O
=	O
1.	O
black	O
denotes	O
low	O
probability	O
and	O
white	O
high	O
probability	O
.	O
(	O
a	O
)	O
:	O
(	O
3	O
,	O
3	O
,	O
3	O
)	O
(	O
b	O
)	O
:	O
(	O
0.1	O
,	O
1	O
,	O
1	O
)	O
.	O
(	O
c	O
)	O
:	O
(	O
4	O
,	O
3	O
,	O
2	O
)	O
.	O
(	O
d	O
)	O
:	O
(	O
0.05	O
,	O
0.05	O
,	O
0.05	O
)	O
.	O
the	O
parameter	B
u	O
controls	O
how	O
strongly	O
the	O
mass	O
of	O
the	O
distribution	B
is	O
pushed	O
to	O
the	O
corners	O
of	O
the	O
simplex	O
.	O
setting	O
uq	O
=	O
1	O
for	O
all	O
q	O
corresponds	O
to	O
a	O
uniform	B
distribution	I
,	O
ﬁg	O
(	O
8.5	O
)	O
.	O
in	O
the	O
binary	O
case	O
q	O
=	O
2	O
,	O
this	O
is	O
equivalent	B
to	O
a	O
beta	B
distribution	O
.	O
the	O
product	O
of	O
two	O
dirichlet	O
distributions	O
is	O
marginal	B
of	O
a	O
dirichlet	O
:	O
dirichlet	O
(	O
θ|u1	O
)	O
dirichlet	O
(	O
θ|u2	O
)	O
=	O
dirichlet	O
(	O
θ|u1	O
+	O
u2	O
)	O
(	O
cid:90	O
)	O
(	O
cid:1	O
)	O
θj	O
dirichlet	O
(	O
θ|u	O
)	O
=	O
dirichlet	O
(	O
cid:0	O
)	O
θ\j|u\j	O
θi|ui	O
,	O
(	O
cid:88	O
)	O
	O
uj	O
j	O
(	O
cid:54	O
)	O
=i	O
p	O
(	O
θi	O
)	O
=	O
b	O
the	O
marginal	B
of	O
a	O
single	O
component	O
θi	O
is	O
a	O
beta	B
distribution	O
:	O
(	O
8.5.4	O
)	O
(	O
8.5.5	O
)	O
(	O
8.5.6	O
)	O
8.6	O
multivariate	B
gaussian	O
the	O
multivariate	B
gaussian	O
plays	O
a	O
central	O
role	O
throughout	O
this	O
book	O
and	O
as	O
such	O
we	O
discuss	O
its	O
properties	B
in	O
some	O
detail	O
.	O
deﬁnition	O
77	O
(	O
multivariate	B
gaussian	O
distribution	B
)	O
.	O
p	O
(	O
x|µ	O
,	O
σ	O
)	O
=	O
n	O
(	O
x	O
µ	O
,	O
σ	O
)	O
≡	O
1	O
(	O
cid:112	O
)	O
det	O
(	O
2πς	O
)	O
−	O
1	O
2	O
(	O
x−µ	O
)	O
tς−1	O
(	O
x−µ	O
)	O
e	O
(	O
8.6.1	O
)	O
where	O
µ	O
is	O
the	O
mean	B
vector	O
of	O
the	O
distribution	B
,	O
and	O
σ	O
the	O
covariance	B
matrix	O
.	O
the	O
inverse	O
covariance	O
σ−1	O
is	O
called	O
the	O
precision	B
.	O
one	O
may	O
show	O
µ	O
=	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
n	O
(	O
x	O
µ	O
,	O
σ	O
)	O
,	O
σ	O
=	O
(	O
cid:68	O
)	O
(	O
x	O
−	O
µ	O
)	O
(	O
x	O
−	O
µ	O
)	O
t	O
(	O
cid:69	O
)	O
n	O
(	O
x	O
µ	O
,	O
σ	O
)	O
(	O
8.6.2	O
)	O
148	O
draft	O
march	O
9	O
,	O
2010	O
00.5100.5100.51x3x2x100.5100.5100.51x3x2x100.5100.5100.51x3x2x100.5100.5100.51x3x2x1	O
multivariate	B
gaussian	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
8.6	O
:	O
(	O
a	O
)	O
:	O
bivariate	O
gaussian	O
with	O
mean	O
(	O
0	O
,	O
0	O
)	O
and	O
covariance	B
[	O
1	O
,	O
0.5	O
;	O
0.5	O
,	O
1.75	O
]	O
.	O
plotted	O
on	O
the	O
(	O
b	O
)	O
:	O
probability	B
density	O
contours	O
for	O
the	O
same	O
vertical	O
axis	O
is	O
the	O
probability	B
density	O
value	B
p	O
(	O
x	O
)	O
.	O
bivariate	O
gaussian	O
.	O
plotted	O
are	O
the	O
unit	O
eigenvectors	O
scaled	O
by	O
the	O
square	O
root	O
of	O
their	O
eigenvalues	O
,	O
√λi	O
.	O
the	O
multivariate	B
gaussian	O
is	O
given	O
in	O
deﬁnition	O
(	O
77	O
)	O
.	O
note	O
that	O
det	O
(	O
ρm	O
)	O
=	O
ρddet	O
(	O
m	O
)	O
,	O
where	O
m	O
is	O
a	O
d×d	O
matrix	B
,	O
which	O
explains	O
the	O
dimension	O
independent	O
notation	O
in	O
the	O
normalisation	B
constant	I
of	O
deﬁnition	O
(	O
77	O
)	O
.	O
the	O
moment	B
representation	I
uses	O
µ	O
and	O
σ	O
to	O
parameterise	O
the	O
gaussian	O
.	O
the	O
alternative	O
canonical	B
repre-	O
sentation	O
p	O
(	O
x|b	O
,	O
m	O
,	O
c	O
)	O
=	O
ce	O
−	O
1	O
2	O
xtmx+xtb	O
is	O
related	O
to	O
the	O
moment	B
representation	I
via	O
σ	O
=	O
m−1	O
,	O
µ	O
=	O
m−1b	O
,	O
1	O
(	O
cid:112	O
)	O
det	O
(	O
2πς	O
)	O
1	O
2	O
btm−1b	O
=	O
ce	O
(	O
8.6.3	O
)	O
(	O
8.6.4	O
)	O
the	O
multivariate	B
gaussian	O
is	O
widely	O
used	O
and	O
it	O
is	O
instructive	O
to	O
understand	O
the	O
geometric	O
picture	O
.	O
this	O
can	O
be	O
obtained	O
by	O
view	O
the	O
distribution	B
in	O
a	O
diﬀerent	O
co-ordinate	O
system	B
.	O
first	O
we	O
use	O
that	O
every	O
real	O
symmetric	O
matrix	B
d	O
×	O
d	O
has	O
an	O
eigen-decomposition	O
σ	O
=	O
eλet	O
(	O
8.6.5	O
)	O
where	O
ete	O
=	O
i	O
and	O
λ	O
=	O
diag	O
(	O
λ1	O
,	O
.	O
.	O
.	O
,	O
λd	O
)	O
.	O
in	O
the	O
case	O
of	O
a	O
covariance	B
matrix	O
,	O
all	O
the	O
eigenvalues	O
λi	O
are	O
positive	O
.	O
this	O
means	O
that	O
one	O
can	O
use	O
the	O
transformation	O
y	O
=	O
λ	O
1	O
2	O
et	O
(	O
x	O
−	O
µ	O
)	O
so	O
that	O
(	O
x	O
−	O
µ	O
)	O
t	O
σ	O
(	O
x	O
−	O
µ	O
)	O
=	O
(	O
x	O
−	O
µ	O
)	O
t	O
eλet	O
(	O
x	O
−	O
µ	O
)	O
=	O
yty	O
(	O
8.6.6	O
)	O
(	O
8.6.7	O
)	O
under	O
this	O
transformation	O
,	O
the	O
multivariate	B
gaussian	O
reduces	O
to	O
a	O
product	O
of	O
d	O
univariate	B
zero-mean	O
unit	O
variance	O
gaussians	O
(	O
since	O
the	O
jacobian	O
of	O
the	O
transformation	O
is	O
a	O
constant	O
)	O
.	O
this	O
means	O
that	O
we	O
can	O
view	O
a	O
multivariate	B
gaussian	O
as	O
a	O
shifted	O
,	O
scaled	O
and	O
rotated	O
version	O
of	O
an	O
isotropic	B
gaussian	O
in	O
which	O
the	O
centre	O
is	O
given	O
by	O
the	O
mean	B
,	O
the	O
rotation	O
by	O
the	O
eigenvectors	O
,	O
and	O
the	O
scaling	O
by	O
the	O
square	O
root	O
of	O
the	O
eigenvalues	O
,	O
as	O
depicted	O
in	O
ﬁg	O
(	O
8.6b	O
)	O
.	O
isotropic	B
means	O
‘	O
same	O
under	O
rotation	O
’	O
.	O
for	O
any	O
isotropic	B
distribution	O
,	O
contours	O
of	O
equal	O
probability	B
are	O
spherical	O
around	O
the	O
origin	O
.	O
some	O
useful	O
properties	B
of	O
the	O
gaussian	O
are	O
as	O
follows	O
:	O
draft	O
march	O
9	O
,	O
2010	O
149	O
−4−2024−4−202400.020.040.060.080.10.120.14−4−3−2−101234−4−3−2−101234	O
0.020.040.060.080.10.12	O
deﬁnition	O
78	O
(	O
partitioned	B
gaussian	O
)	O
.	O
for	O
a	O
distribution	B
n	O
(	O
z	O
µ	O
,	O
σ	O
)	O
deﬁned	O
jointly	O
over	O
two	O
vectors	O
x	O
and	O
y	O
of	O
potentially	O
diﬀering	O
dimensions	O
,	O
multivariate	B
gaussian	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
x	O
(	O
cid:18	O
)	O
µx	O
y	O
µy	O
(	O
cid:19	O
)	O
z	O
=	O
µ	O
=	O
with	O
corresponding	O
mean	B
and	O
partitioned	B
covariance	O
(	O
cid:18	O
)	O
σxx	O
σxy	O
σyx	O
σyy	O
(	O
cid:19	O
)	O
σ	O
=	O
where	O
σyx	O
≡	O
σt	O
xy	O
.	O
the	O
marginal	B
distribution	O
is	O
given	O
by	O
p	O
(	O
x	O
)	O
=	O
n	O
(	O
x	O
µx	O
,	O
σxx	O
)	O
and	O
conditional	B
p	O
(	O
x|y	O
)	O
=	O
n	O
(	O
cid:0	O
)	O
x	O
µx	O
+	O
σxyς−1	O
yy	O
(	O
cid:0	O
)	O
y	O
−	O
µy	O
(	O
cid:1	O
)	O
,	O
σxx	O
−	O
σxyς−1	O
yy	O
σyx	O
(	O
cid:1	O
)	O
(	O
8.6.8	O
)	O
(	O
8.6.9	O
)	O
(	O
8.6.10	O
)	O
(	O
8.6.11	O
)	O
(	O
8.6.12	O
)	O
(	O
8.6.13	O
)	O
(	O
8.6.14	O
)	O
(	O
8.6.15	O
)	O
deﬁnition	O
79	O
(	O
product	O
of	O
two	O
gaussians	O
)	O
.	O
the	O
product	O
of	O
two	O
gaussians	O
is	O
another	O
gaussian	O
,	O
with	O
a	O
multiplicative	O
factor	B
,	O
exercise	O
(	O
114	O
)	O
:	O
n	O
(	O
x	O
µ1	O
,	O
σ1	O
)	O
n	O
(	O
x	O
µ2	O
,	O
σ2	O
)	O
=	O
n	O
(	O
x	O
µ	O
,	O
σ	O
)	O
exp	O
(	O
cid:112	O
)	O
det	O
(	O
2πs	O
)	O
2	O
(	O
µ1	O
−	O
µ2	O
)	O
t	O
s−1	O
(	O
µ1	O
−	O
µ2	O
)	O
−	O
1	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
where	O
s	O
≡	O
σ1	O
+	O
σ2	O
and	O
the	O
mean	B
and	O
covariance	B
are	O
given	O
by	O
µ	O
=	O
σ1s−1µ2	O
+	O
σ2s−1µ1	O
σ	O
=	O
σ1s−1σ2	O
deﬁnition	O
80	O
(	O
linear	B
transform	O
of	O
a	O
gaussian	O
)	O
.	O
for	O
the	O
linear	B
transformation	I
y	O
=	O
ax	O
+	O
b	O
where	O
x	O
∼	O
n	O
(	O
x	O
µ	O
,	O
σ	O
)	O
,	O
(	O
cid:16	O
)	O
y	O
aµ	O
+	O
b	O
,	O
aσat	O
(	O
cid:17	O
)	O
y	O
∼	O
n	O
deﬁnition	O
81	O
(	O
entropy	B
of	O
a	O
gaussian	O
)	O
.	O
the	O
diﬀerential	B
entropy	O
of	O
a	O
multivariate	B
gaussian	O
p	O
(	O
x	O
)	O
=	O
n	O
(	O
x	O
µ	O
,	O
σ	O
)	O
is	O
h	O
(	O
x	O
)	O
≡	O
−	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
)	O
=	O
1	O
2	O
log	O
det	O
(	O
2πς	O
)	O
+	O
d	O
2	O
where	O
d	O
=	O
dim	O
x.	O
note	O
that	O
the	O
entropy	B
is	O
independent	O
of	O
the	O
mean	B
µ	O
.	O
(	O
8.6.16	O
)	O
150	O
draft	O
march	O
9	O
,	O
2010	O
multivariate	B
gaussian	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
8.7	O
:	O
beta	B
distribution	O
.	O
the	O
parameters	O
α	O
and	O
β	O
can	O
also	O
be	O
wit-	O
ting	O
in	O
terms	O
of	O
the	O
mean	B
and	O
vari-	O
ance	O
,	O
leading	O
to	O
an	O
alternative	O
pa-	O
rameterisation	O
,	O
see	O
exercise	O
(	O
94	O
)	O
.	O
8.6.1	O
conditioning	B
as	O
system	B
reversal	I
for	O
a	O
joint	B
distribution	O
p	O
(	O
x	O
,	O
y	O
)	O
,	O
consider	O
the	O
conditional	B
p	O
(	O
x|y	O
)	O
.	O
the	O
statistics	O
of	O
p	O
(	O
x|y	O
)	O
can	O
be	O
obtained	O
using	O
a	O
linear	B
system	O
of	O
the	O
form	O
where	O
x	O
=	O
←−ay	O
+	O
←−η	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
←−η	O
←−µ	O
,	O
←−σ	O
←−η	O
∼	O
n	O
and	O
this	O
reversed	O
noise	O
is	O
uncorrelated	O
with	O
y	O
.	O
(	O
8.6.17	O
)	O
(	O
8.6.18	O
)	O
to	O
show	O
this	O
,	O
we	O
need	O
to	O
make	O
the	O
statistics	O
of	O
x	O
under	O
this	O
linear	B
system	O
match	O
those	O
given	O
by	O
the	O
conditioning	B
operation	O
,	O
(	O
8.6.11	O
)	O
.	O
the	O
mean	B
of	O
the	O
linear	B
system	O
is	O
given	O
by	O
µx	O
=	O
←−aµy	O
+	O
←−µ	O
and	O
the	O
covariances	O
by	O
(	O
note	O
that	O
covariance	B
of	O
y	O
remains	O
unaﬀected	O
by	O
the	O
system	B
reversal	I
)	O
σxx	O
=	O
←−aσyy←−at	O
+	O
←−σ	O
σxy	O
=	O
←−aσyy	O
from	O
equation	B
(	O
8.6.21	O
)	O
we	O
have	O
←−a	O
=	O
σxyς−1	O
yy	O
which	O
,	O
using	O
in	O
equation	B
(	O
8.6.20	O
)	O
,	O
gives	O
using	O
equation	B
(	O
8.6.19	O
)	O
we	O
similarly	O
obtain	O
←−σ	O
=	O
σxx	O
−	O
←−aσyy←−at	O
=	O
σxx	O
−	O
σxyς−1	O
←−µ	O
=	O
µx	O
−	O
←−aµy	O
=	O
µx	O
−	O
σxyς−1	O
yy	O
σyx	O
(	O
8.6.24	O
)	O
this	O
means	O
that	O
we	O
can	O
write	O
an	O
explicit	O
linear	B
system	O
of	O
the	O
form	O
equation	B
(	O
8.6.17	O
)	O
where	O
the	O
parameters	O
are	O
given	O
in	O
terms	O
of	O
the	O
statistics	O
of	O
the	O
original	O
system	B
.	O
these	O
results	O
are	O
just	O
a	O
restatement	O
of	O
the	O
conditioning	B
results	O
but	O
shows	O
how	O
it	O
may	O
be	O
interpreted	O
as	O
a	O
linear	B
system	O
.	O
this	O
is	O
useful	O
in	O
deriving	O
results	O
in	O
inference	B
with	O
linear	O
dynamical	O
systems	O
.	O
yy	O
µy	O
(	O
8.6.19	O
)	O
(	O
8.6.20	O
)	O
(	O
8.6.21	O
)	O
(	O
8.6.22	O
)	O
(	O
8.6.23	O
)	O
(	O
8.6.25	O
)	O
(	O
8.6.26	O
)	O
(	O
8.6.27	O
)	O
(	O
8.6.28	O
)	O
151	O
8.6.2	O
completing	O
the	O
square	O
a	O
useful	O
technique	O
in	O
manipulating	O
gaussians	O
is	O
completing	O
the	O
square	O
.	O
for	O
example	O
,	O
the	O
expression	O
−	O
1	O
e	O
2	O
xtax+btx	O
can	O
be	O
transformed	O
as	O
follows	O
.	O
first	O
we	O
complete	O
the	O
square	O
:	O
1	O
2	O
hence	O
xtax	O
−	O
btx	O
=	O
1	O
2	O
2	O
xtax−btx	O
=	O
n	O
−	O
1	O
e	O
−	O
1	O
(	O
cid:90	O
)	O
from	O
this	O
one	O
can	O
derive	O
e	O
2	O
xtax+btxdx	O
=	O
(	O
cid:0	O
)	O
x	O
−	O
a−1b	O
(	O
cid:1	O
)	O
t	O
a	O
(	O
cid:0	O
)	O
x	O
−	O
a−1b	O
(	O
cid:1	O
)	O
(	O
cid:0	O
)	O
x	O
a−1b	O
,	O
a−1	O
(	O
cid:1	O
)	O
(	O
cid:113	O
)	O
det	O
(	O
cid:0	O
)	O
2πa−1	O
(	O
cid:1	O
)	O
e	O
(	O
cid:113	O
)	O
det	O
(	O
cid:0	O
)	O
2πa−1	O
(	O
cid:1	O
)	O
e	O
2	O
bta−1b	O
−	O
1	O
bta−1b	O
1	O
2	O
1	O
2	O
bta−1b	O
draft	O
march	O
9	O
,	O
2010	O
00.20.40.60.8100.511.522.53	O
α=0.1	O
β=0.1α=1	O
β=1α=2	O
β=2α=5	O
β=500.20.40.60.8100.511.522.53	O
α=0.1	O
β=2α=1	O
β=2α=2	O
β=2α=5	O
β=2	O
8.6.3	O
gaussian	O
propagation	B
let	O
y	O
be	O
linearly	O
related	O
to	O
x	O
through	O
y	O
=	O
mx	O
+	O
η	O
where	O
η	O
∼	O
n	O
(	O
µ	O
,	O
σ	O
)	O
,	O
and	O
x	O
∼	O
n	O
(	O
µx	O
,	O
σx	O
)	O
.	O
then	O
the	O
marginal	B
p	O
(	O
y	O
)	O
=	O
(	O
cid:82	O
)	O
x	O
p	O
(	O
y|x	O
)	O
p	O
(	O
x	O
)	O
is	O
a	O
gaussian	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
p	O
(	O
y	O
)	O
=	O
n	O
y	O
mµx	O
+	O
µ	O
,	O
mσxmt	O
+	O
σ	O
multivariate	B
gaussian	O
(	O
8.6.29	O
)	O
(	O
8.6.30	O
)	O
8.6.4	O
whitening	B
and	O
centering	B
for	O
a	O
set	O
of	O
data	B
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
with	O
dim	O
xn	O
=	O
d	O
,	O
we	O
can	O
transform	O
this	O
data	B
to	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
with	O
zero	O
mean	B
using	O
centering	B
:	O
where	O
the	O
mean	B
m	O
of	O
the	O
data	B
is	O
given	O
by	O
yn	O
=	O
xn	O
−	O
m	O
n	O
(	O
cid:88	O
)	O
m	O
=	O
1	O
n	O
xn	O
n=1	O
(	O
8.6.31	O
)	O
(	O
8.6.32	O
)	O
furthermore	O
,	O
we	O
can	O
transform	O
to	O
a	O
values	O
z1	O
,	O
.	O
.	O
.	O
,	O
zn	O
that	O
have	O
zero	O
mean	B
and	O
unit	O
covariance	O
using	O
whitening	B
where	O
the	O
covariance	B
s	O
of	O
the	O
data	B
is	O
given	O
by	O
zn	O
=	O
s−	O
1	O
2	O
(	O
xn	O
−	O
m	O
)	O
n	O
(	O
cid:88	O
)	O
s	O
=	O
1	O
n	O
n=1	O
(	O
xn	O
−	O
m	O
)	O
(	O
xn	O
−	O
m	O
)	O
t	O
y	O
=	O
(	O
cid:2	O
)	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
(	O
cid:3	O
)	O
usvt	O
=	O
y	O
,	O
then	O
an	O
equivalent	B
approach	O
is	O
to	O
compute	O
the	O
svd	O
decomposition	B
of	O
the	O
matrix	B
of	O
centered	O
datapoints	O
(	O
8.6.33	O
)	O
(	O
8.6.34	O
)	O
(	O
8.6.35	O
)	O
(	O
8.6.36	O
)	O
z	O
=	O
√ndiag	O
(	O
1/s1,1	O
,	O
.	O
.	O
.	O
,	O
1/sd	O
,	O
d	O
)	O
uty	O
has	O
zero	O
mean	B
and	O
unit	O
covariance	O
,	O
see	O
exercise	O
(	O
111	O
)	O
.	O
8.6.5	O
maximum	B
likelihood	I
training	O
given	O
a	O
set	O
of	O
training	B
data	O
x	O
=	O
(	O
cid:8	O
)	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
(	O
cid:9	O
)	O
,	O
drawn	O
from	O
a	O
gaussian	O
n	O
(	O
x	O
µ	O
,	O
σ	O
)	O
with	O
unknown	O
mean	B
µ	O
and	O
covariance	B
σ	O
,	O
how	O
can	O
we	O
ﬁnd	O
these	O
parameters	O
?	O
assuming	O
the	O
data	B
are	O
drawn	O
i.i.d	O
.	O
the	O
log	O
likelihood	B
is	O
n	O
(	O
cid:88	O
)	O
n=1	O
log	O
p	O
(	O
x|µ	O
,	O
σ	O
)	O
=	O
−	O
1	O
2	O
n	O
(	O
cid:88	O
)	O
n=1	O
l	O
(	O
µ	O
,	O
σ	O
)	O
≡	O
152	O
(	O
xn	O
−	O
µ	O
)	O
t	O
σ−1	O
(	O
xn	O
−	O
µ	O
)	O
−	O
n	O
2	O
log	O
det	O
(	O
2πς	O
)	O
(	O
8.6.37	O
)	O
draft	O
march	O
9	O
,	O
2010	O
taking	O
the	O
partial	O
derivative	O
with	O
respect	O
to	O
the	O
vector	O
µ	O
we	O
obtain	O
the	O
vector	O
derivative	O
equating	O
to	O
zero	O
gives	O
that	O
at	O
the	O
optimum	O
of	O
the	O
log	O
likelihood	B
,	O
the	O
derivative	O
of	O
l	O
with	O
respect	O
to	O
the	O
matrix	B
σ	O
requires	O
more	O
work	O
.	O
dependence	B
on	O
the	O
covariance	B
,	O
and	O
also	O
parameterise	O
using	O
the	O
inverse	O
covariance	O
,	O
σ−1	O
,	O
it	O
is	O
convenient	O
to	O
isolate	O
the	O
	O
+	O
n	O
2	O
log	O
det	O
(	O
cid:0	O
)	O
2πς−1	O
(	O
cid:1	O
)	O
(	O
cid:125	O
)	O
(	O
xn	O
−	O
µ	O
)	O
(	O
xn	O
−	O
µ	O
)	O
t	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
≡m	O
(	O
8.6.38	O
)	O
(	O
8.6.39	O
)	O
(	O
8.6.40	O
)	O
(	O
8.6.41	O
)	O
(	O
8.6.42	O
)	O
(	O
8.6.43	O
)	O
multivariate	B
gaussian	O
optimal	O
µ	O
n	O
(	O
cid:88	O
)	O
n=1	O
σ−1	O
(	O
xn	O
−	O
µ	O
)	O
∇µl	O
(	O
µ	O
,	O
σ	O
)	O
=	O
n	O
(	O
cid:88	O
)	O
n=1	O
σ−1xn	O
=	O
n	O
µς−1	O
and	O
therefore	O
,	O
optimally	O
n	O
(	O
cid:88	O
)	O
n=1	O
xn	O
µ	O
=	O
1	O
n	O
optimal	O
σ	O
using	O
m	O
=	O
mt	O
,	O
we	O
obtain	O
σ−1	O
n	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
n=1	O
1	O
2	O
m	O
+	O
n	O
2	O
σ	O
l	O
=	O
−	O
1	O
2	O
trace	O
∂	O
∂σ−1	O
l	O
=	O
−	O
n	O
(	O
cid:88	O
)	O
σ	O
=	O
1	O
n	O
n=1	O
(	O
xn	O
−	O
µ	O
)	O
(	O
xn	O
−	O
µ	O
)	O
t	O
equating	O
the	O
derivative	O
to	O
the	O
zero	O
matrix	B
and	O
solving	B
for	O
σ	O
gives	O
equations	O
(	O
8.6.40	O
)	O
and	O
(	O
8.6.43	O
)	O
deﬁne	O
the	O
maximum	B
likelihood	I
solution	O
mean	B
and	O
covariance	B
for	O
training	B
data	O
x	O
.	O
consistent	B
with	O
our	O
previous	O
results	O
,	O
in	O
fact	O
these	O
equations	O
simply	O
set	O
the	O
parameters	O
to	O
their	O
sample	B
statistics	O
of	O
the	O
empirical	B
distribution	I
.	O
that	O
is	O
,	O
the	O
mean	B
is	O
set	O
to	O
the	O
sample	B
mean	O
of	O
the	O
data	B
and	O
the	O
covariance	B
to	O
the	O
sample	B
covariance	O
.	O
8.6.6	O
bayesian	O
inference	B
of	O
the	O
mean	B
and	O
variance	B
for	O
simplicity	O
here	O
we	O
deal	O
with	O
the	O
univariate	B
case	O
.	O
assuming	O
i.i.d	O
.	O
data	B
the	O
likelihood	B
is	O
p	O
(	O
x|µ	O
,	O
σ2	O
)	O
=	O
(	O
2πσ2	O
)	O
n/2	O
e	O
1	O
−	O
1	O
2σ2	O
(	O
cid:80	O
)	O
n	O
n=1	O
(	O
xn−µ	O
)	O
2	O
for	O
a	O
bayesian	O
treatment	O
,	O
we	O
require	O
the	O
posterior	B
of	O
the	O
parameters	O
p	O
(	O
µ	O
,	O
σ2|x	O
)	O
∝	O
p	O
(	O
x|µ	O
,	O
σ2	O
)	O
p	O
(	O
µ	O
,	O
σ2	O
)	O
=	O
p	O
(	O
x|µ	O
,	O
σ2	O
)	O
p	O
(	O
µ|σ2	O
)	O
p	O
(	O
σ2	O
)	O
(	O
8.6.44	O
)	O
(	O
8.6.45	O
)	O
our	O
aim	O
is	O
to	O
ﬁnd	O
conjugate	B
priors	O
for	O
the	O
mean	B
and	O
variance	B
.	O
a	O
convenient	O
choice	O
for	O
a	O
prior	B
on	O
the	O
mean	B
µ	O
is	O
that	O
it	O
is	O
a	O
gaussian	O
centred	O
on	O
µ0	O
:	O
p	O
(	O
µ|µ0	O
,	O
σ2	O
0	O
)	O
=	O
1	O
(	O
cid:112	O
)	O
2πσ2	O
0	O
draft	O
march	O
9	O
,	O
2010	O
(	O
µ0−µ	O
)	O
2	O
−	O
1	O
2σ2	O
e	O
0	O
(	O
8.6.46	O
)	O
153	O
the	O
posterior	B
is	O
then	O
p	O
(	O
µ	O
,	O
σ2|x	O
)	O
∝	O
1	O
(	O
cid:112	O
)	O
1	O
(	O
σ2	O
)	O
n/2	O
e	O
σ2	O
0	O
(	O
cid:80	O
)	O
(	O
µ0−µ	O
)	O
2−	O
1	O
2σ2	O
n	O
(	O
xn−µ	O
)	O
2	O
p	O
(	O
σ2	O
)	O
−	O
1	O
2σ2	O
0	O
it	O
is	O
convenient	O
to	O
write	O
this	O
in	O
the	O
form	O
p	O
(	O
µ	O
,	O
σ2|x	O
)	O
=	O
p	O
(	O
µ|σ2	O
,	O
x	O
)	O
p	O
(	O
σ2|x	O
)	O
multivariate	B
gaussian	O
(	O
8.6.47	O
)	O
(	O
8.6.48	O
)	O
since	O
equation	B
(	O
8.6.47	O
)	O
has	O
quadratic	O
contributions	O
in	O
µ	O
in	O
the	O
exponent	O
,	O
the	O
conditional	B
posterior	O
p	O
(	O
µ|σ2	O
,	O
x	O
)	O
is	O
gaussian	O
.	O
to	O
identify	O
this	O
gaussian	O
we	O
multiply	O
out	O
the	O
terms	O
in	O
the	O
exponent	O
to	O
arrive	O
at	O
we	O
encounter	O
a	O
diﬃculty	O
in	O
attempting	O
to	O
ﬁnd	O
a	O
conjugate	B
prior	O
for	O
σ2	O
because	O
the	O
term	O
b2/a	O
is	O
not	O
a	O
simple	O
expression	O
of	O
σ2	O
.	O
for	O
this	O
reason	O
we	O
constrain	O
if	O
we	O
therefore	O
use	O
an	O
inverse	B
gamma	I
distribution	O
we	O
will	O
have	O
a	O
conjugate	B
prior	O
for	O
σ2	O
.	O
for	O
a	O
gauss-	O
inverse-gamma	O
prior	B
:	O
(	O
cid:33	O
)	O
(	O
cid:33	O
)	O
(	O
cid:32	O
)	O
1	O
2	O
˜b2	O
˜a	O
˜c	O
−	O
(	O
8.6.57	O
)	O
(	O
8.6.58	O
)	O
2	O
,	O
β	O
+	O
154	O
draft	O
march	O
9	O
,	O
2010	O
(	O
cid:0	O
)	O
aµ2	O
−	O
2bµ	O
+	O
c	O
(	O
cid:1	O
)	O
σ2	O
,	O
b	O
=	O
µ0	O
+	O
n	O
σ2	O
0	O
(	O
cid:18	O
)	O
(	O
cid:80	O
)	O
+	O
n	O
xn	O
σ2	O
(	O
cid:19	O
)	O
2	O
b	O
a	O
+	O
µ	O
−	O
with	O
exp−	O
1	O
2	O
a	O
=	O
1	O
σ2	O
0	O
using	O
the	O
identity	O
we	O
can	O
write	O
aµ2	O
−	O
2bµ	O
+	O
c	O
=	O
a	O
(	O
cid:124	O
)	O
p	O
(	O
µ	O
,	O
σ2|x	O
)	O
∝	O
√ae	O
,	O
c	O
=	O
µ2	O
0	O
σ2	O
0	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
b2	O
a	O
c	O
−	O
(	O
cid:16	O
)	O
−	O
1	O
2	O
+	O
(	O
cid:88	O
)	O
n	O
(	O
xn	O
)	O
2	O
σ2	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
−	O
1	O
2	O
a	O
(	O
µ−	O
b	O
p	O
(	O
µ|x	O
,	O
σ2	O
)	O
a	O
)	O
2	O
(	O
cid:125	O
)	O
e	O
1	O
√a	O
(	O
cid:124	O
)	O
c−	O
b2	O
a	O
(	O
cid:17	O
)	O
1	O
(	O
cid:112	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
σ2	O
0	O
p	O
(	O
σ2|x	O
)	O
1	O
(	O
cid:125	O
)	O
(	O
σ2	O
)	O
n/2	O
p	O
(	O
σ2	O
)	O
for	O
some	O
ﬁxed	O
hyperparameter	B
γ.	O
deﬁning	O
the	O
constants	O
+	O
(	O
cid:88	O
)	O
n	O
(	O
xn	O
)	O
2	O
xn	O
,	O
˜c	O
=	O
µ2	O
0	O
γ	O
σ2	O
0	O
≡	O
γσ2	O
+	O
(	O
cid:88	O
)	O
(	O
cid:33	O
)	O
n	O
˜a	O
=	O
1	O
γ	O
+	O
n	O
,	O
˜b	O
=	O
µ0	O
γ	O
we	O
have	O
b2	O
a	O
=	O
1	O
σ2	O
c	O
−	O
(	O
cid:32	O
)	O
˜b2	O
˜a	O
˜c	O
−	O
using	O
this	O
expression	O
in	O
equation	B
(	O
8.6.52	O
)	O
we	O
obtain	O
e	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
˜c−	O
˜b2	O
˜a	O
−	O
1	O
2σ2	O
p	O
(	O
σ2	O
)	O
(	O
cid:0	O
)	O
σ2	O
(	O
cid:1	O
)	O
−n/2	O
(	O
cid:0	O
)	O
µ	O
µ0	O
,	O
γσ2	O
(	O
cid:1	O
)	O
invgam	O
(	O
cid:0	O
)	O
σ2|α	O
,	O
β	O
(	O
cid:1	O
)	O
(	O
cid:32	O
)	O
(	O
cid:32	O
)	O
σ2|α	O
+	O
n	O
invgam	O
(	O
cid:33	O
)	O
σ2	O
˜a	O
˜b	O
˜a	O
µ	O
,	O
p	O
(	O
σ2|x	O
)	O
∝	O
p	O
(	O
µ	O
,	O
σ2	O
)	O
=	O
n	O
p	O
(	O
µ	O
,	O
σ2|x	O
)	O
=	O
n	O
the	O
posterior	B
is	O
also	O
gauss-inverse-gamma	O
with	O
(	O
8.6.49	O
)	O
(	O
8.6.50	O
)	O
(	O
8.6.51	O
)	O
(	O
8.6.52	O
)	O
(	O
8.6.53	O
)	O
(	O
8.6.54	O
)	O
(	O
8.6.55	O
)	O
(	O
8.6.56	O
)	O
exponential	B
family	I
8.6.7	O
gauss-gamma	O
distribution	B
it	O
is	O
common	O
to	O
to	O
use	O
a	O
prior	B
on	O
the	O
precision	B
,	O
deﬁned	O
as	O
the	O
inverse	O
variance	O
1	O
σ2	O
λ	O
≡	O
if	O
we	O
then	O
use	O
a	O
gamma	B
prior	O
p	O
(	O
λ|α	O
,	O
β	O
)	O
=	O
gam	O
(	O
λ|α	O
,	O
β	O
)	O
=	O
1	O
βαγ	O
(	O
α	O
)	O
λα−1e	O
−λ/β	O
(	O
cid:17	O
)	O
λ|α	O
+	O
n/2	O
,	O
˜β	O
the	O
posterior	B
will	O
be	O
(	O
cid:16	O
)	O
p	O
(	O
λ|x	O
,	O
α	O
,	O
β	O
)	O
=	O
gam	O
(	O
cid:33	O
)	O
(	O
cid:32	O
)	O
˜c	O
−	O
1	O
2	O
1	O
˜β	O
=	O
+	O
1	O
β	O
˜b2	O
˜a	O
where	O
the	O
gauss-gamma	O
prior	B
distribution	O
p	O
(	O
µ	O
,	O
λ|µ0	O
,	O
α	O
,	O
β	O
,	O
γ	O
)	O
=	O
n	O
p	O
(	O
µ	O
,	O
λ|x	O
,	O
µ0	O
,	O
α	O
,	O
β	O
,	O
γ	O
)	O
=	O
n	O
(	O
8.6.59	O
)	O
(	O
8.6.60	O
)	O
(	O
8.6.61	O
)	O
(	O
8.6.62	O
)	O
(	O
8.6.63	O
)	O
(	O
8.6.64	O
)	O
is	O
the	O
conjugate	B
prior	O
for	O
a	O
gaussian	O
with	O
unknown	O
mean	B
µ	O
and	O
precision	B
λ.	O
the	O
posterior	B
for	O
this	O
prior	B
is	O
a	O
gauss-gamma	O
distribution	B
with	O
parameters	O
(	O
cid:0	O
)	O
µ	O
µ0	O
,	O
γλ	O
−1	O
(	O
cid:1	O
)	O
gam	O
(	O
λ|α	O
,	O
β	O
)	O
(	O
cid:33	O
)	O
(	O
cid:16	O
)	O
gam	O
(	O
cid:32	O
)	O
˜b	O
˜a	O
µ	O
,	O
1	O
˜aλ	O
(	O
cid:17	O
)	O
λ|α	O
+	O
n/2	O
,	O
˜β	O
the	O
marginal	B
p	O
(	O
µ|x	O
,	O
µ0	O
,	O
α	O
,	O
β	O
,	O
γ	O
)	O
is	O
a	O
student	O
’	O
s	O
t-distribution	O
.	O
an	O
example	O
of	O
a	O
gauss-gamma	O
prior/posterior	O
is	O
given	O
in	O
ﬁg	O
(	O
8.8	O
)	O
.	O
the	O
maximum	B
likelihood	I
solution	O
is	O
recovered	O
in	O
the	O
limit	O
of	O
a	O
‘	O
ﬂat	O
’	O
(	O
improper	O
)	O
prior	B
µ0	O
=	O
0	O
,	O
γ	O
→	O
∞	O
,	O
α	O
=	O
1/2	O
,	O
β	O
→	O
∞	O
,	O
see	O
exercise	O
(	O
102	O
)	O
.	O
the	O
unbiased	O
estimators	O
for	O
the	O
mean	B
and	O
variance	B
are	O
given	O
using	O
the	O
proper	O
prior	B
µ0	O
=	O
0	O
,	O
γ	O
→	O
∞	O
,	O
α	O
=	O
1	O
,	O
β	O
→	O
∞	O
,	O
exercise	O
(	O
103	O
)	O
.	O
for	O
the	O
multivariate	B
case	O
,	O
the	O
extension	O
of	O
these	O
techniques	O
uses	O
a	O
multivariate	B
gaussian	O
distribution	B
for	O
the	O
conjugate	B
prior	O
on	O
the	O
mean	B
,	O
and	O
an	O
inverse	O
wishart	O
distribution	B
for	O
the	O
conjugate	B
prior	O
on	O
the	O
covariance	B
[	O
124	O
]	O
.	O
8.7	O
exponential	B
family	I
a	O
theoretically	O
convenient	O
class	O
of	O
distributions	O
are	O
the	O
exponential	B
family	I
,	O
which	O
contains	O
many	O
standard	O
distributions	O
,	O
including	O
the	O
gaussian	O
,	O
gamma	B
,	O
poisson	O
,	O
dirichlet	O
,	O
wishart	O
,	O
multinomial	B
,	O
markov	O
random	O
field	O
.	O
deﬁnition	O
82	O
(	O
exponential	B
family	I
)	O
.	O
for	O
a	O
distribution	B
on	O
a	O
(	O
possibly	O
multidimensional	O
)	O
variable	B
x	O
(	O
continuous	B
or	O
discrete	B
)	O
an	O
exponential	B
family	I
model	O
is	O
of	O
the	O
form	O
i	O
ηi	O
(	O
θ	O
)	O
ti	O
(	O
x	O
)	O
−ψ	O
(	O
θ	O
)	O
(	O
8.7.1	O
)	O
θ	O
are	O
the	O
parameters	O
,	O
ti	O
(	O
x	O
)	O
the	O
test	O
statistics	O
,	O
and	O
ψ	O
(	O
θ	O
)	O
is	O
the	O
log	O
partition	B
function	I
that	O
ensure	O
normal-	O
isation	O
(	O
cid:80	O
)	O
p	O
(	O
x|θ	O
)	O
=	O
h	O
(	O
x	O
)	O
e	O
(	O
cid:90	O
)	O
ψ	O
(	O
θ	O
)	O
=	O
log	O
(	O
cid:80	O
)	O
h	O
(	O
x	O
)	O
e	O
x	O
draft	O
march	O
9	O
,	O
2010	O
i	O
ηi	O
(	O
θ	O
)	O
ti	O
(	O
x	O
)	O
(	O
8.7.2	O
)	O
155	O
exponential	B
family	I
(	O
a	O
)	O
prior	B
(	O
b	O
)	O
posterior	B
figure	O
8.8	O
:	O
bayesian	O
approach	B
to	O
inferring	O
the	O
mean	B
and	O
precision	B
(	O
inverse	O
variance	O
)	O
of	O
a	O
gaussian	O
based	O
(	O
a	O
)	O
:	O
a	O
gauss-gamma	O
prior	B
with	O
µ0	O
=	O
0	O
,	O
α	O
=	O
2	O
,	O
β	O
=	O
1	O
,	O
γ	O
=	O
1.	O
on	O
n	O
=	O
10	O
randomly	O
drawn	O
datapoints	O
.	O
(	O
b	O
)	O
:	O
gauss-gamma	O
posterior	B
conditional	O
on	O
the	O
data	B
.	O
for	O
comparison	O
,	O
the	O
sample	B
mean	O
of	O
the	O
data	B
is	O
1.87	O
and	O
maximum	B
likelihood	I
optimal	O
variance	B
is	O
1.16	O
(	O
computed	O
using	O
the	O
n	O
normalisation	B
)	O
.	O
the	O
10	O
datapoints	O
were	O
drawn	O
from	O
a	O
gaussian	O
with	O
mean	O
2	O
and	O
variance	B
1.	O
see	O
demogaussbayes.m	O
.	O
one	O
can	O
always	O
transform	O
the	O
parameters	O
to	O
the	O
form	O
η	O
(	O
θ	O
)	O
=	O
θ	O
in	O
which	O
case	O
the	O
distribution	B
is	O
in	O
canonical	B
form	I
:	O
p	O
(	O
x|θ	O
)	O
=	O
h	O
(	O
x	O
)	O
eθtt	O
(	O
x	O
)	O
−ψ	O
(	O
θ	O
)	O
for	O
example	O
the	O
univariate	B
gaussian	O
can	O
be	O
written	O
1	O
√2πσ2	O
2σ2	O
(	O
x−µ	O
)	O
2	O
−	O
1	O
e	O
=	O
e	O
−	O
1	O
2σ2	O
x2+	O
µ	O
σ2	O
x−	O
µ2	O
2σ2	O
−	O
1	O
2	O
log	O
πσ2	O
deﬁning	O
t1	O
(	O
x	O
)	O
=	O
x	O
,	O
t2	O
(	O
x	O
)	O
=	O
−x2/2	O
and	O
,	O
θ1	O
=	O
µ	O
,	O
θ2	O
=	O
σ2	O
,	O
h	O
(	O
x	O
)	O
=	O
1	O
,	O
then	O
η1	O
(	O
θ	O
)	O
=	O
θ1	O
θ2	O
,	O
η2	O
(	O
θ	O
)	O
=	O
1	O
θ2	O
,	O
ψ	O
(	O
θ	O
)	O
=	O
1	O
2	O
+	O
log	O
πθ2	O
(	O
cid:18	O
)	O
θ2	O
1	O
θ2	O
(	O
cid:19	O
)	O
(	O
8.7.3	O
)	O
(	O
8.7.4	O
)	O
(	O
8.7.5	O
)	O
note	O
that	O
the	O
parameterisation	B
is	O
not	O
necessarily	O
unique	O
–	O
we	O
can	O
for	O
example	O
rescale	O
the	O
functions	O
ti	O
(	O
x	O
)	O
and	O
inversely	O
scale	O
ηi	O
by	O
the	O
same	O
amount	O
to	O
arrive	O
at	O
an	O
equivalent	B
representation	O
.	O
8.7.1	O
conjugate	B
priors	O
in	O
principle	O
,	O
bayesian	O
learning	B
for	O
the	O
exponential	B
family	I
is	O
straightforward	O
.	O
in	O
canonical	B
form	I
p	O
(	O
x|θ	O
)	O
=	O
h	O
(	O
x	O
)	O
eθtt	O
(	O
x	O
)	O
−ψ	O
(	O
θ	O
)	O
for	O
a	O
prior	B
with	O
hyperparameters	O
α	O
,	O
γ	O
,	O
p	O
(	O
θ|α	O
,	O
γ	O
)	O
∝	O
eθtα−γψ	O
(	O
θ	O
)	O
the	O
posterior	B
is	O
p	O
(	O
θ|x	O
)	O
∝	O
p	O
(	O
x|θ	O
)	O
p	O
(	O
θ	O
)	O
∝	O
h	O
(	O
x	O
)	O
eθtt	O
(	O
x	O
)	O
−ψ	O
(	O
θ	O
)	O
eθtα−γψ	O
(	O
θ	O
)	O
∝	O
eθt	O
[	O
t	O
(	O
x	O
)	O
+α	O
]	O
−	O
[	O
γ+1	O
]	O
ψ	O
(	O
θ	O
)	O
(	O
8.7.6	O
)	O
(	O
8.7.7	O
)	O
(	O
8.7.8	O
)	O
(	O
8.7.9	O
)	O
(	O
8.7.10	O
)	O
so	O
that	O
the	O
prior	B
equation	O
(	O
8.7.7	O
)	O
is	O
conjugate	B
for	O
the	O
exponential	B
family	I
likelihood	O
equation	B
(	O
8.7.6	O
)	O
.	O
whilst	O
the	O
likelihood	B
is	O
in	O
the	O
exponential	B
family	I
,	O
the	O
conjugate	B
prior	O
is	O
not	O
necessarily	O
in	O
the	O
exponential	B
family	I
.	O
156	O
draft	O
march	O
9	O
,	O
2010	O
the	O
kullback-leibler	O
divergence	B
kl	O
(	O
q|p	O
)	O
8.8	O
the	O
kullback-leibler	O
divergence	B
kl	O
(	O
q|p	O
)	O
the	O
kullback-leibler	O
divergence	B
kl	O
(	O
q|p	O
)	O
measures	O
the	O
‘	O
diﬀerence	O
’	O
between	O
distributions	O
q	O
and	O
p	O
[	O
68	O
]	O
.	O
deﬁnition	O
83.	O
kl	O
divergence	B
for	O
two	O
distributions	O
q	O
(	O
x	O
)	O
and	O
p	O
(	O
x	O
)	O
kl	O
(	O
q|p	O
)	O
≡	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
x	O
)	O
−	O
log	O
p	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
≥	O
0	O
(	O
8.8.1	O
)	O
where	O
(	O
cid:104	O
)	O
f	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
r	O
(	O
x	O
)	O
denotes	O
average	B
of	O
the	O
function	B
f	O
(	O
x	O
)	O
with	O
respect	O
to	O
the	O
distribution	B
r	O
(	O
x	O
)	O
.	O
the	O
kl	O
divergence	B
is	O
≥	O
0	O
the	O
kl	O
divergence	B
is	O
widely	O
used	O
and	O
it	O
is	O
therefore	O
important	O
to	O
understand	O
why	O
the	O
divergence	B
is	O
positive	O
.	O
to	O
see	O
this	O
,	O
consider	O
the	O
following	O
linear	B
bound	O
on	O
the	O
function	B
log	O
(	O
x	O
)	O
log	O
(	O
x	O
)	O
≤	O
x	O
−	O
1	O
(	O
8.8.2	O
)	O
as	O
plotted	O
in	O
the	O
ﬁgure	O
on	O
the	O
right	O
.	O
replacing	O
x	O
by	O
p	O
(	O
x	O
)	O
/q	O
(	O
x	O
)	O
in	O
the	O
above	O
bound	B
p	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
−	O
1	O
≥	O
log	O
p	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
(	O
8.8.3	O
)	O
since	O
probabilities	O
are	O
non-negative	O
,	O
we	O
can	O
multiply	O
both	O
sides	O
by	O
q	O
(	O
x	O
)	O
to	O
obtain	O
we	O
now	O
integrate	O
(	O
or	O
sum	O
in	O
the	O
case	O
of	O
discrete	B
variables	O
)	O
both	O
sides	O
.	O
using	O
(	O
cid:82	O
)	O
p	O
(	O
x	O
)	O
dx	O
=	O
1	O
,	O
(	O
cid:82	O
)	O
q	O
(	O
x	O
)	O
dx	O
=	O
1	O
,	O
p	O
(	O
x	O
)	O
−	O
q	O
(	O
x	O
)	O
≥	O
q	O
(	O
x	O
)	O
log	O
p	O
(	O
x	O
)	O
−	O
q	O
(	O
x	O
)	O
log	O
q	O
(	O
x	O
)	O
(	O
8.8.4	O
)	O
1	O
−	O
1	O
≥	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x	O
)	O
−	O
log	O
q	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
rearranging	O
gives	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
x	O
)	O
−	O
log	O
p	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
≡	O
kl	O
(	O
q|p	O
)	O
≥	O
0	O
the	O
kl	O
divergence	B
is	O
zero	O
if	O
and	O
only	O
if	O
the	O
two	O
distributions	O
are	O
exactly	O
the	O
same	O
.	O
8.8.1	O
entropy	B
for	O
both	O
discrete	B
and	O
continuous	B
variables	O
,	O
the	O
entropy	B
is	O
deﬁned	O
as	O
h	O
(	O
p	O
)	O
≡	O
−	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
)	O
(	O
8.8.5	O
)	O
(	O
8.8.6	O
)	O
(	O
8.8.7	O
)	O
for	O
continuous	B
variables	O
,	O
this	O
is	O
also	O
called	O
the	O
diﬀerential	B
entropy	O
,	O
see	O
also	O
exercise	O
(	O
113	O
)	O
.	O
the	O
entropy	B
is	O
a	O
measure	O
of	O
the	O
uncertainty	B
in	O
a	O
distribution	B
.	O
one	O
way	O
to	O
see	O
this	O
is	O
that	O
h	O
(	O
p	O
)	O
=	O
−kl	O
(	O
p|u	O
)	O
+	O
const	O
.	O
(	O
8.8.8	O
)	O
where	O
u	O
is	O
a	O
uniform	B
distribution	I
.	O
since	O
the	O
kl	O
(	O
p|u	O
)	O
≥	O
0	O
,	O
the	O
less	O
like	O
a	O
uniform	B
distribution	I
p	O
is	O
,	O
the	O
smaller	O
will	O
be	O
the	O
entropy	B
.	O
or	O
,	O
vice	O
versa	O
,	O
the	O
more	O
similar	O
p	O
is	O
to	O
a	O
uniform	B
distribution	I
,	O
the	O
greater	O
will	O
be	O
the	O
entropy	B
.	O
since	O
the	O
uniform	B
distribution	I
contains	O
the	O
least	O
information	O
a	O
prior	B
about	O
which	O
state	O
p	O
(	O
x	O
)	O
is	O
in	O
,	O
the	O
entropy	B
is	O
therefore	O
a	O
measure	O
of	O
the	O
a	O
priori	O
uncertainty	B
in	O
the	O
state	O
occupancy	O
.	O
for	O
a	O
discrete	B
distribution	O
we	O
can	O
permute	O
the	O
state	O
labels	O
without	O
changing	O
the	O
entropy	B
.	O
for	O
a	O
discrete	B
distribution	O
the	O
entropy	B
is	O
positive	O
,	O
whereas	O
the	O
diﬀerential	B
entropy	O
can	O
be	O
negative	O
.	O
draft	O
march	O
9	O
,	O
2010	O
157	O
1234−4−3−2−10123	O
8.9	O
code	O
demogaussbayes.m	O
:	O
bayesian	O
ﬁtting	O
of	O
a	O
univariate	B
gaussian	O
loggaussgamma.m	O
:	O
plotting	O
routine	O
for	O
a	O
gauss-gamma	O
distribution	B
exercises	O
8.10	O
exercises	O
exercise	O
80.	O
in	O
a	O
public	O
lecture	O
,	O
the	O
following	O
phrase	O
was	O
uttered	O
by	O
a	O
professor	O
of	O
experimental	O
psy-	O
chology	O
:	O
‘	O
in	O
a	O
recent	O
data	B
survey	O
,	O
90	O
%	O
of	O
people	O
claim	O
to	O
have	O
above	O
average	B
intelligence	O
,	O
which	O
is	O
clearly	O
nonsense	O
!	O
’	O
[	O
audience	O
laughs	O
]	O
.	O
is	O
it	O
theoretically	O
possible	O
for	O
90	O
%	O
of	O
people	O
to	O
have	O
above	O
average	B
intelli-	O
gence	O
?	O
if	O
so	O
,	O
give	O
an	O
example	O
,	O
otherwise	O
explain	O
why	O
not	O
.	O
what	O
about	O
above	O
median	O
intelligence	O
?	O
exercise	O
81.	O
consider	O
the	O
distribution	B
deﬁned	O
on	O
real	O
variables	O
x	O
,	O
y	O
:	O
−x2−y2	O
,	O
p	O
(	O
x	O
,	O
y	O
)	O
∝	O
(	O
x2	O
+	O
y2	O
)	O
2e	O
dom	O
(	O
x	O
)	O
=	O
dom	O
(	O
y	O
)	O
=	O
{	O
−∞	O
.	O
.	O
.∞	O
}	O
(	O
8.10.1	O
)	O
show	O
that	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
=	O
(	O
cid:104	O
)	O
y	O
(	O
cid:105	O
)	O
=	O
0.	O
furthermore	O
show	O
that	O
x	O
and	O
y	O
are	O
uncorrelated	O
,	O
(	O
cid:104	O
)	O
xy	O
(	O
cid:105	O
)	O
=	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
(	O
cid:104	O
)	O
y	O
(	O
cid:105	O
)	O
.	O
whilst	O
x	O
and	O
y	O
are	O
uncorrelated	O
,	O
show	O
that	O
they	O
are	O
nevertheless	O
dependent	O
.	O
exercise	O
82.	O
for	O
a	O
variable	B
x	O
with	O
dom	O
(	O
x	O
)	O
=	O
{	O
0	O
,	O
1	O
}	O
,	O
and	O
p	O
(	O
x	O
=	O
1	O
)	O
=	O
θ	O
,	O
show	O
that	O
in	O
n	O
independent	O
draws	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
from	O
this	O
distribution	B
,	O
the	O
probability	B
of	O
observing	O
k	O
states	O
1	O
is	O
the	O
binomial	B
distribution	O
(	O
cid:18	O
)	O
n	O
(	O
cid:19	O
)	O
k	O
i	O
=	O
θk	O
(	O
1	O
−	O
θ	O
)	O
n−k	O
(	O
cid:90	O
)	O
∞	O
(	O
cid:90	O
)	O
∞	O
−	O
1	O
e	O
−∞	O
2	O
x2	O
dx	O
−	O
1	O
2	O
x2	O
e	O
−∞	O
i	O
2	O
=	O
dx	O
by	O
considering	O
exercise	O
83	O
(	O
normalisation	B
constant	I
of	O
a	O
gaussian	O
)	O
.	O
the	O
normalisation	B
constant	I
of	O
a	O
gaussian	O
distri-	O
bution	O
is	O
related	O
to	O
the	O
integral	O
(	O
cid:90	O
)	O
∞	O
−∞	O
−	O
1	O
e	O
2	O
y2	O
dy	O
=	O
(	O
cid:90	O
)	O
∞	O
(	O
cid:90	O
)	O
∞	O
−∞	O
−∞	O
−	O
1	O
e	O
2	O
x2+y2	O
dxdy	O
(	O
8.10.3	O
)	O
(	O
8.10.4	O
)	O
(	O
8.10.2	O
)	O
(	O
8.10.5	O
)	O
(	O
8.10.6	O
)	O
and	O
transforming	O
to	O
polar	O
coordinates	O
,	O
show	O
that	O
1.	O
i	O
=	O
√2π	O
−	O
1	O
2σ2	O
(	O
x−µ	O
)	O
2	O
2	O
.	O
(	O
cid:82	O
)	O
∞	O
−∞	O
e	O
dx	O
=	O
√2πσ2	O
exercise	O
84.	O
for	O
a	O
univariate	B
gaussian	O
distribution	B
,	O
show	O
that	O
(	O
cid:90	O
)	O
1.	O
µ	O
=	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
n	O
(	O
x	O
µ	O
,	O
σ2	O
)	O
2.	O
σ2	O
=	O
n	O
(	O
x	O
µ	O
,	O
σ2	O
)	O
(	O
cid:68	O
)	O
(	O
x	O
−	O
µ	O
)	O
2	O
(	O
cid:69	O
)	O
dirichlet	O
(	O
θ|u	O
)	O
=	O
dirichlet	O
(	O
cid:0	O
)	O
θ\j|u\j	O
xk	O
(	O
cid:69	O
)	O
=	O
b	O
(	O
α	O
+	O
k	O
,	O
β	O
)	O
b	O
(	O
α	O
,	O
β	O
)	O
(	O
cid:68	O
)	O
θj	O
(	O
cid:1	O
)	O
exercise	O
86.	O
for	O
a	O
beta	B
distribution	O
,	O
show	O
that	O
exercise	O
85.	O
show	O
that	O
the	O
marginal	B
of	O
a	O
dirichlet	O
distribution	B
is	O
another	O
dirichlet	O
distribution	B
:	O
and	O
,	O
using	O
γ	O
(	O
x	O
+	O
1	O
)	O
=	O
xγ	O
(	O
x	O
)	O
,	O
derive	O
an	O
explicit	O
expression	O
for	O
the	O
kth	O
moment	O
of	O
a	O
beta	B
distribution	O
.	O
158	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
exercise	O
87.	O
deﬁne	O
the	O
moment	B
generating	I
function	I
as	O
(	O
cid:10	O
)	O
etx	O
(	O
cid:11	O
)	O
p	O
(	O
x	O
)	O
g	O
(	O
t	O
)	O
≡	O
show	O
that	O
(	O
cid:68	O
)	O
xk	O
(	O
cid:69	O
)	O
p	O
(	O
x	O
)	O
lim	O
t→0	O
dk	O
dtk	O
g	O
(	O
t	O
)	O
=	O
(	O
8.10.7	O
)	O
(	O
8.10.8	O
)	O
exercise	O
88	O
(	O
change	B
of	I
variables	I
)	O
.	O
consider	O
a	O
one	O
dimensional	O
continuous	B
random	O
variable	B
x	O
with	O
corresponding	O
p	O
(	O
x	O
)	O
.	O
for	O
a	O
variable	B
y	O
=	O
f	O
(	O
x	O
)	O
,	O
where	O
f	O
(	O
x	O
)	O
is	O
a	O
monotonic	O
function	B
,	O
show	O
that	O
the	O
distribution	B
of	O
y	O
is	O
(	O
cid:18	O
)	O
df	O
(	O
cid:19	O
)	O
−1	O
dx	O
p	O
(	O
y	O
)	O
=	O
p	O
(	O
x	O
)	O
,	O
x	O
=	O
f	O
−1	O
(	O
y	O
)	O
exercise	O
89	O
(	O
normalisation	B
of	O
a	O
multivariate	B
gaussian	O
)	O
.	O
consider	O
(	O
cid:90	O
)	O
∞	O
−∞	O
i	O
=	O
2	O
(	O
x−µ	O
)	O
tς−1	O
(	O
x−µ	O
)	O
dx	O
−	O
1	O
e	O
by	O
using	O
the	O
transformation	O
show	O
that	O
z	O
=	O
σ−	O
1	O
2	O
(	O
x	O
−	O
µ	O
)	O
i	O
=	O
(	O
cid:112	O
)	O
det	O
(	O
2πς	O
)	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
a	O
b	O
m	O
=	O
c	O
d	O
exercise	O
90.	O
consider	O
the	O
partitioned	B
matrix	I
(	O
8.10.9	O
)	O
(	O
8.10.10	O
)	O
(	O
8.10.11	O
)	O
(	O
8.10.12	O
)	O
(	O
8.10.13	O
)	O
for	O
which	O
we	O
wish	O
to	O
ﬁnd	O
the	O
inverse	O
m−1	O
.	O
we	O
assume	O
that	O
a	O
is	O
m	O
×	O
m	O
and	O
invertible	O
,	O
and	O
d	O
is	O
n	O
×	O
n	O
and	O
invertible	O
.	O
by	O
deﬁnition	O
,	O
the	O
partitioned	B
inverse	O
(	O
cid:19	O
)	O
(	O
8.10.14	O
)	O
(	O
8.10.15	O
)	O
(	O
cid:19	O
)	O
=	O
(	O
cid:18	O
)	O
im	O
0	O
0	O
in	O
where	O
in	O
the	O
above	O
im	O
is	O
the	O
m	O
×	O
m	O
identity	B
matrix	I
of	O
the	O
same	O
dimension	O
as	O
a	O
,	O
and	O
0	O
the	O
zero	O
matrix	B
of	O
the	O
same	O
dimension	O
as	O
d.	O
using	O
the	O
above	O
,	O
derive	O
the	O
results	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
p	O
q	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
p	O
q	O
r	O
s	O
m−1	O
=	O
(	O
cid:18	O
)	O
a	O
b	O
c	O
d	O
r	O
s	O
must	O
satisfy	O
p	O
=	O
(	O
cid:0	O
)	O
a	O
−	O
bd−1c	O
(	O
cid:1	O
)	O
−1	O
q	O
=	O
−a−1b	O
(	O
cid:0	O
)	O
d	O
−	O
ca−1b	O
(	O
cid:1	O
)	O
−1	O
r	O
=	O
−d−1c	O
(	O
cid:0	O
)	O
a	O
−	O
bd−1c	O
(	O
cid:1	O
)	O
−1	O
s	O
=	O
(	O
cid:0	O
)	O
d	O
−	O
ca−1b	O
(	O
cid:1	O
)	O
−1	O
(	O
cid:0	O
)	O
x	O
µ	O
,	O
σ2	O
(	O
cid:1	O
)	O
the	O
skewness	B
and	O
kurtosis	B
are	O
both	O
exercise	O
91.	O
show	O
that	O
for	O
gaussian	O
distribution	B
p	O
(	O
x	O
)	O
=	O
n	O
zero	O
.	O
exercise	O
92.	O
consider	O
a	O
small	O
interval	O
of	O
time	O
δt	O
and	O
let	O
the	O
probability	B
of	O
an	O
event	O
occurring	O
in	O
this	O
small	O
interval	O
be	O
θδt	O
.	O
derive	O
a	O
distribution	B
that	O
expresses	O
the	O
probability	B
of	O
at	O
least	O
one	O
event	O
in	O
an	O
interval	O
from	O
0	O
to	O
t.	O
draft	O
march	O
9	O
,	O
2010	O
159	O
exercise	O
93.	O
consider	O
a	O
vector	O
variable	O
x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
and	O
set	O
of	O
functions	O
deﬁned	O
on	O
each	O
component	O
of	O
x	O
,	O
φi	O
(	O
xi	O
)	O
.	O
for	O
example	O
for	O
x	O
=	O
(	O
x1	O
,	O
x2	O
)	O
we	O
might	O
have	O
exercises	O
(	O
8.10.16	O
)	O
(	O
8.10.17	O
)	O
(	O
8.10.18	O
)	O
φ1	O
(	O
x1	O
)	O
=	O
−|x1|	O
,	O
φ2	O
(	O
x2	O
)	O
=	O
−x2	O
2	O
consider	O
the	O
distribution	B
1	O
z	O
eθtφ	O
(	O
x	O
)	O
p	O
(	O
x|θ	O
)	O
=	O
(	O
cid:90	O
)	O
∞	O
−∞	O
eθiφi	O
(	O
xi	O
)	O
dxi	O
where	O
φ	O
(	O
x	O
)	O
is	O
a	O
vector	O
function	O
with	O
ith	O
component	O
φi	O
(	O
xi	O
)	O
,	O
and	O
θ	O
is	O
a	O
parameter	B
vector	O
.	O
each	O
component	O
is	O
tractably	O
integrable	O
in	O
the	O
sense	O
that	O
can	O
be	O
computed	O
either	O
analytically	O
or	O
to	O
an	O
acceptable	O
numerical	B
accuracy	O
.	O
show	O
that	O
1.	O
xi⊥⊥	O
xj|∅	O
.	O
2.	O
the	O
normalisation	B
constant	I
z	O
can	O
be	O
tractably	O
computed	O
.	O
3.	O
consider	O
the	O
transformation	O
x	O
=	O
my	O
(	O
8.10.19	O
)	O
for	O
an	O
invertible	O
matrix	B
m.	O
show	O
that	O
the	O
distribution	B
p	O
(	O
y|m	O
,	O
θ	O
)	O
is	O
tractable	O
(	O
its	O
normalisation	B
constant	I
is	O
known	O
)	O
,	O
and	O
that	O
,	O
in	O
general	O
,	O
yi	O
(	O
cid:62	O
)	O
(	O
cid:62	O
)	O
yj	O
|	O
∅	O
.	O
explain	O
the	O
signiﬁcance	O
of	O
this	O
is	O
deriving	O
tractable	O
multivariate	B
distributions	O
.	O
exercise	O
94.	O
show	O
that	O
we	O
may	O
reparameterise	O
the	O
beta	B
distribution	O
,	O
deﬁnition	O
(	O
72	O
)	O
by	O
writing	O
the	O
pa-	O
rameters	O
α	O
and	O
β	O
as	O
functions	O
of	O
the	O
mean	B
m	O
and	O
variance	B
s	O
using	O
α	O
=	O
βγ	O
,	O
1	O
β	O
=	O
1	O
+	O
γ	O
(	O
cid:18	O
)	O
(	O
cid:19	O
)	O
γ	O
≡	O
m/	O
(	O
1	O
−	O
m	O
)	O
s	O
(	O
1	O
+	O
γ	O
)	O
2	O
−	O
1	O
γ	O
exercise	O
95.	O
consider	O
the	O
function	B
f	O
(	O
γ	O
+	O
α	O
,	O
β	O
,	O
θ	O
)	O
≡	O
θγ+α−1	O
(	O
1	O
−	O
θ	O
)	O
β−1	O
show	O
that	O
lim	O
γ→0	O
∂	O
∂γ	O
f	O
(	O
γ	O
+	O
α	O
,	O
β	O
,	O
θ	O
)	O
=	O
θα−1	O
(	O
1	O
−	O
θ	O
)	O
β−1	O
log	O
θ	O
and	O
hence	O
that	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
θα−1	O
(	O
1	O
−	O
θ	O
)	O
β−1	O
log	O
θdθ	O
=	O
lim	O
γ→0	O
using	O
this	O
result	O
,	O
show	O
therefore	O
that	O
∂	O
∂γ	O
f	O
(	O
γ	O
+	O
α	O
,	O
β	O
,	O
θ	O
)	O
dθ	O
=	O
∂	O
∂α	O
(	O
cid:104	O
)	O
log	O
θ	O
(	O
cid:105	O
)	O
b	O
(	O
θ|α	O
,	O
β	O
)	O
=	O
∂	O
∂α	O
log	O
b	O
(	O
α	O
,	O
β	O
)	O
where	O
b	O
(	O
α	O
,	O
β	O
)	O
is	O
the	O
beta	B
function	O
.	O
show	O
additionally	O
that	O
(	O
cid:104	O
)	O
log	O
(	O
1	O
−	O
θ	O
)	O
(	O
cid:105	O
)	O
b	O
(	O
θ|α	O
,	O
β	O
)	O
=	O
∂	O
∂β	O
log	O
b	O
(	O
α	O
,	O
β	O
)	O
using	O
the	O
fact	O
that	O
b	O
(	O
α	O
,	O
β	O
)	O
=	O
γ	O
(	O
α	O
)	O
γ	O
(	O
β	O
)	O
γ	O
(	O
α	O
+	O
β	O
)	O
(	O
cid:90	O
)	O
(	O
8.10.20	O
)	O
(	O
8.10.21	O
)	O
(	O
8.10.22	O
)	O
(	O
8.10.23	O
)	O
f	O
(	O
α	O
,	O
β	O
,	O
θ	O
)	O
dθ	O
(	O
8.10.24	O
)	O
(	O
8.10.25	O
)	O
(	O
8.10.26	O
)	O
(	O
8.10.27	O
)	O
where	O
γ	O
(	O
x	O
)	O
is	O
the	O
gamma	B
function	O
,	O
relate	O
the	O
above	O
averages	O
to	O
the	O
digamma	B
function	I
,	O
deﬁned	O
as	O
ψ	O
(	O
x	O
)	O
=	O
d	O
dx	O
log	O
γ	O
(	O
x	O
)	O
160	O
(	O
8.10.28	O
)	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
exercise	O
96.	O
using	O
a	O
similar	O
‘	O
generating	O
function	B
’	O
approach	B
as	O
in	O
exercise	O
(	O
95	O
)	O
,	O
explain	O
how	O
to	O
compute	O
(	O
cid:104	O
)	O
log	O
θi	O
(	O
cid:105	O
)	O
dirichlet	O
(	O
θ|u	O
)	O
exercise	O
97.	O
consider	O
the	O
function	B
θui−1	O
(	O
cid:33	O
)	O
(	O
cid:89	O
)	O
(	O
cid:27	O
)	O
i	O
f	O
(	O
x	O
)	O
=	O
0	O
δ	O
θi	O
−	O
x	O
(	O
cid:82	O
)	O
∞	O
0	O
e−sxf	O
(	O
x	O
)	O
dx	O
is	O
show	O
that	O
the	O
laplace	O
transform	O
of	O
f	O
(	O
x	O
)	O
,	O
˜f	O
(	O
s	O
)	O
≡	O
n	O
(	O
cid:89	O
)	O
dθ1	O
.	O
.	O
.	O
dθn	O
i	O
−sθiθui−1	O
e	O
i	O
dθi	O
=	O
˜f	O
(	O
s	O
)	O
=	O
(	O
cid:80	O
)	O
1	O
i	O
ui	O
s	O
γ	O
(	O
ui	O
)	O
i=1	O
by	O
taking	O
the	O
inverse	O
laplace	O
transform	O
,	O
show	O
that	O
(	O
cid:90	O
)	O
∞	O
(	O
cid:32	O
)	O
n	O
(	O
cid:88	O
)	O
(	O
cid:26	O
)	O
(	O
cid:90	O
)	O
∞	O
i=1	O
0	O
i=1	O
n	O
(	O
cid:89	O
)	O
(	O
cid:81	O
)	O
n	O
γ	O
(	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
i	O
ui−1	O
i=1	O
γ	O
(	O
ui	O
)	O
i	O
ui	O
−	O
1	O
)	O
x	O
f	O
(	O
x	O
)	O
=	O
(	O
cid:81	O
)	O
n	O
γ	O
(	O
(	O
cid:80	O
)	O
i=1	O
γ	O
(	O
ui	O
)	O
i	O
ui	O
)	O
hence	O
show	O
that	O
the	O
normalisation	B
constant	I
of	O
a	O
dirichlet	O
distribution	B
with	O
parameters	O
u	O
is	O
given	O
by	O
exercise	O
98.	O
by	O
using	O
the	O
laplace	O
transform	O
,	O
as	O
in	O
exercise	O
(	O
97	O
)	O
,	O
show	O
that	O
the	O
marginal	B
of	O
a	O
dirichlet	O
distribution	B
is	O
a	O
dirichlet	O
distribution	B
.	O
exercise	O
99.	O
derive	O
the	O
formula	O
for	O
the	O
diﬀerential	B
entropy	O
of	O
a	O
multi-variate	B
gaussian	O
.	O
exercise	O
100.	O
show	O
that	O
for	O
a	O
gamma	B
distribution	O
gam	O
(	O
x|α	O
,	O
β	O
)	O
the	O
mode	B
is	O
given	O
by	O
x	O
∗	O
=	O
(	O
α	O
−	O
1	O
)	O
β	O
provided	O
that	O
α	O
≥	O
1.	O
exercise	O
101.	O
consider	O
a	O
distribution	B
p	O
(	O
x|θ	O
)	O
and	O
a	O
distribution	B
with	O
θ	O
changed	O
by	O
a	O
small	O
amount	O
,	O
δ.	O
take	O
the	O
taylor	O
expansion	O
of	O
kl	O
(	O
p	O
(	O
x|θ	O
)	O
|p	O
(	O
x|θ	O
+	O
δ	O
)	O
)	O
(	O
cid:29	O
)	O
for	O
small	O
δ	O
and	O
show	O
that	O
this	O
is	O
equal	O
to	O
(	O
8.10.34	O
)	O
(	O
8.10.35	O
)	O
(	O
cid:28	O
)	O
∂2	O
∂θ2	O
log	O
p	O
(	O
x|θ	O
)	O
p	O
(	O
θ	O
)	O
more	O
generally	O
for	O
a	O
distribution	B
parameterised	O
by	O
a	O
vector	O
θi	O
+	O
δi	O
,	O
show	O
that	O
a	O
small	O
change	O
in	O
the	O
parameter	B
results	O
in	O
(	O
8.10.29	O
)	O
(	O
8.10.30	O
)	O
(	O
8.10.31	O
)	O
(	O
8.10.32	O
)	O
(	O
8.10.33	O
)	O
(	O
8.10.36	O
)	O
(	O
8.10.37	O
)	O
(	O
8.10.38	O
)	O
δ2	O
2	O
−	O
(	O
cid:88	O
)	O
i	O
,	O
j	O
δiδj	O
2	O
fij	O
(	O
cid:28	O
)	O
∂2	O
(	O
cid:28	O
)	O
∂	O
where	O
the	O
fisher	O
information	O
matrix	O
is	O
deﬁned	O
as	O
fij	O
=	O
−	O
∂θi∂θj	O
log	O
p	O
(	O
x|θ	O
)	O
(	O
cid:29	O
)	O
p	O
(	O
θ	O
)	O
(	O
cid:29	O
)	O
log	O
p	O
(	O
x|θ	O
)	O
show	O
that	O
the	O
fisher	O
information	O
matrix	O
is	O
positive	O
(	O
semi	O
)	O
deﬁnite	O
by	O
expressing	O
it	O
equivalently	O
as	O
fij	O
=	O
log	O
p	O
(	O
x|θ	O
)	O
∂	O
∂θj	O
∂θi	O
draft	O
march	O
9	O
,	O
2010	O
p	O
(	O
θ	O
)	O
(	O
8.10.39	O
)	O
161	O
exercise	O
102.	O
consider	O
the	O
joint	B
prior	O
distribution	B
p	O
(	O
µ	O
,	O
λ|µ0	O
,	O
α	O
,	O
β	O
,	O
γ	O
)	O
=	O
n	O
(	O
cid:88	O
)	O
n	O
µ∗	O
=	O
1	O
n	O
xn	O
,	O
σ2∗	O
=	O
1	O
n	O
(	O
cid:0	O
)	O
µ	O
µ0	O
,	O
γλ	O
−1	O
(	O
cid:1	O
)	O
gam	O
(	O
λ|α	O
,	O
β	O
)	O
(	O
cid:88	O
)	O
(	O
xn	O
−	O
µ∗	O
)	O
2	O
n	O
show	O
that	O
for	O
µ0	O
=	O
0	O
,	O
γ	O
→	O
∞	O
,	O
β	O
→	O
∞	O
,	O
then	O
the	O
prior	B
distribution	O
becomes	O
‘	O
ﬂat	O
’	O
(	O
independent	O
of	O
µ	O
and	O
λ	O
)	O
for	O
α	O
=	O
1/2	O
.	O
show	O
that	O
for	O
these	O
settings	O
the	O
mean	B
and	O
variance	B
that	O
jointly	O
maximise	O
the	O
posterior	B
equation	O
(	O
8.6.64	O
)	O
are	O
given	O
by	O
the	O
standard	O
maximum	O
likelihood	B
settings	O
exercises	O
(	O
8.10.40	O
)	O
(	O
8.10.41	O
)	O
(	O
8.10.44	O
)	O
exercise	O
103.	O
show	O
that	O
in	O
the	O
limit	O
µ0	O
=	O
0	O
,	O
γ	O
→	O
∞	O
,	O
α	O
=	O
1	O
,	O
β	O
→	O
∞	O
,	O
the	O
jointly	O
optimal	O
mean	B
and	O
variance	B
obtained	O
from	O
argmax	O
µ	O
,	O
λ	O
is	O
given	O
by	O
µ∗	O
=	O
1	O
n	O
p	O
(	O
µ	O
,	O
λ|x	O
,	O
α	O
,	O
β	O
,	O
γ	O
)	O
(	O
cid:88	O
)	O
xn	O
,	O
σ2∗	O
=	O
n	O
(	O
cid:88	O
)	O
n	O
1	O
n	O
+	O
1	O
(	O
xn	O
−	O
µ∗	O
)	O
2	O
(	O
8.10.42	O
)	O
(	O
8.10.43	O
)	O
where	O
σ2∗	O
=	O
1/λ∗	O
.	O
note	O
that	O
these	O
correspond	O
to	O
the	O
standard	O
‘	O
unbiased	O
’	O
estimators	O
of	O
the	O
mean	B
and	O
variance	B
.	O
exercise	O
104.	O
for	O
the	O
gauss-gamma	O
posterior	B
p	O
(	O
µ	O
,	O
λ|µ0	O
,	O
α	O
,	O
β	O
,	O
x	O
)	O
given	O
in	O
equation	B
(	O
8.6.64	O
)	O
compute	O
the	O
marginal	B
posterior	O
p	O
(	O
µ|µ0	O
,	O
α	O
,	O
β	O
,	O
x	O
)	O
.	O
what	O
is	O
the	O
mean	B
of	O
this	O
distribution	B
?	O
exercise	O
105.	O
derive	O
equation	B
(	O
8.6.30	O
)	O
.	O
exercise	O
106.	O
consider	O
the	O
multivariate	B
gaussian	O
distribution	B
p	O
(	O
x	O
)	O
∼	O
n	O
(	O
x	O
µ	O
,	O
σ	O
)	O
on	O
the	O
vector	O
x	O
with	O
components	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
:	O
p	O
(	O
x	O
)	O
=	O
−	O
1	O
2	O
(	O
x−µ	O
)	O
tς−1	O
(	O
x−µ	O
)	O
e	O
1	O
(	O
cid:112	O
)	O
det	O
(	O
2πς	O
)	O
(	O
cid:1	O
)	O
and	O
p	O
(	O
yi|x	O
)	O
∼	O
n	O
(	O
cid:0	O
)	O
x	O
0	O
,	O
σ2	O
0	O
calculate	O
p	O
(	O
xi|x1	O
,	O
.	O
.	O
.	O
,	O
xi−1	O
,	O
xi+1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
.	O
exercise	O
107.	O
observations	O
y0	O
,	O
.	O
.	O
.	O
,	O
yn−1	O
are	O
noisy	O
i.i.d	O
.	O
measurements	O
of	O
an	O
underlying	O
variable	B
x	O
with	O
p	O
(	O
x	O
)	O
∼	O
n	O
with	O
mean	O
(	O
cid:0	O
)	O
yi	O
x	O
,	O
σ2	O
(	O
cid:1	O
)	O
for	O
i	O
=	O
0	O
,	O
.	O
.	O
.	O
,	O
n−1	O
.	O
show	O
that	O
p	O
(	O
x|y0	O
,	O
.	O
.	O
.	O
,	O
yn−1	O
)	O
is	O
gaussian	O
µ	O
=	O
nσ2	O
0	O
nσ2	O
0	O
+	O
σ2	O
y	O
where	O
y	O
=	O
(	O
y0	O
+	O
y1	O
+	O
.	O
.	O
.	O
+	O
yn−1	O
)	O
/n	O
and	O
variance	B
σ2	O
n	O
such	O
that	O
1	O
σ2	O
n	O
=	O
n	O
σ2	O
+	O
1	O
σ2	O
0	O
.	O
(	O
8.10.45	O
)	O
(	O
8.10.46	O
)	O
exercise	O
108.	O
consider	O
a	O
set	O
of	O
data	B
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
drawn	O
from	O
a	O
gaussian	O
with	O
know	O
mean	B
µ	O
and	O
unknown	O
variance	B
σ2	O
.	O
assume	O
a	O
gamma	B
distribution	O
prior	B
on	O
τ	O
=	O
1/σ2	O
,	O
p	O
(	O
τ	O
)	O
=	O
gamis	O
(	O
τ|a	O
,	O
b	O
)	O
1.	O
show	O
that	O
the	O
posterior	B
distribution	O
is	O
(	O
cid:32	O
)	O
τ|a	O
+	O
n	O
2	O
,	O
b	O
+	O
1	O
2	O
(	O
cid:33	O
)	O
n	O
(	O
cid:88	O
)	O
n=1	O
(	O
xn	O
−	O
µ	O
)	O
2	O
(	O
8.10.47	O
)	O
(	O
8.10.48	O
)	O
p	O
(	O
τ|x	O
)	O
=	O
gamis	O
162	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
2.	O
show	O
that	O
the	O
distribution	B
for	O
x	O
is	O
(	O
cid:90	O
)	O
p	O
(	O
x|x	O
)	O
=	O
p	O
(	O
x|τ	O
)	O
p	O
(	O
τ|x	O
)	O
dτ	O
=	O
student	O
(	O
cid:16	O
)	O
x|µ	O
,	O
λ	O
=	O
a	O
b	O
(	O
cid:17	O
)	O
,	O
ν	O
=	O
2a	O
(	O
8.10.49	O
)	O
exercise	O
109.	O
the	O
poisson	O
distribution	B
is	O
a	O
discrete	B
distribution	O
on	O
the	O
non-negative	O
integers	O
,	O
with	O
p	O
(	O
x	O
)	O
=	O
e−λλx	O
x	O
!	O
x	O
=	O
0	O
,	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
(	O
8.10.50	O
)	O
you	O
are	O
given	O
a	O
sample	B
of	O
n	O
observations	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
drawn	O
from	O
this	O
distribution	B
.	O
determine	O
the	O
maximum	B
likelihood	I
estimator	O
of	O
the	O
poisson	O
parameter	B
λ.	O
exercise	O
110.	O
for	O
a	O
gaussian	O
mixture	B
model	I
pin	O
(	O
x	O
µi	O
,	O
σi	O
)	O
,	O
pi	O
>	O
0	O
,	O
(	O
cid:88	O
)	O
i	O
show	O
that	O
p	O
(	O
x	O
)	O
has	O
mean	B
i	O
p	O
(	O
x	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:16	O
)	O
and	O
covariance	B
(	O
cid:88	O
)	O
i	O
piµi	O
pi	O
σi	O
+	O
µiµt	O
i	O
i	O
(	O
cid:17	O
)	O
(	O
cid:88	O
)	O
i	O
−	O
(	O
cid:88	O
)	O
j	O
piµi	O
pjµt	O
j	O
pi	O
=	O
1	O
(	O
8.10.51	O
)	O
(	O
8.10.52	O
)	O
(	O
8.10.53	O
)	O
exercise	O
111.	O
show	O
that	O
for	O
the	O
whitened	O
data	B
matrix	O
,	O
given	O
in	O
equation	B
(	O
8.6.36	O
)	O
,	O
zzt	O
=	O
ni	O
.	O
exercise	O
112.	O
consider	O
a	O
uniform	B
distribution	I
pi	O
=	O
1/n	O
deﬁned	O
on	O
states	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
show	O
that	O
the	O
entropy	B
of	O
this	O
distribution	B
is	O
n	O
(	O
cid:88	O
)	O
i=1	O
h	O
=	O
−	O
pi	O
log	O
pi	O
=	O
log	O
n	O
(	O
8.10.54	O
)	O
and	O
that	O
there	O
for	O
as	O
the	O
number	O
of	O
states	O
n	O
increases	O
to	O
inﬁnity	O
,	O
the	O
entropy	B
diverges	O
to	O
inﬁnity	O
.	O
exercise	O
113.	O
consider	O
a	O
continuous	B
distribution	O
p	O
(	O
x	O
)	O
,	O
x	O
∈	O
[	O
0	O
,	O
1	O
]	O
.	O
we	O
can	O
form	O
a	O
discrete	B
approximation	O
with	O
probabilities	O
pi	O
to	O
this	O
continuous	B
distribution	O
by	O
identifying	O
a	O
continuous	B
value	O
i/n	O
for	O
each	O
state	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
with	O
this	O
(	O
8.10.55	O
)	O
(	O
8.10.56	O
)	O
(	O
8.10.57	O
)	O
(	O
8.10.58	O
)	O
163	O
(	O
cid:80	O
)	O
pi	O
=	O
p	O
(	O
i/n	O
)	O
i	O
p	O
(	O
i/n	O
)	O
(	O
cid:88	O
)	O
show	O
that	O
the	O
entropy	B
h	O
=	O
−	O
1	O
(	O
cid:80	O
)	O
i	O
p	O
(	O
i/n	O
)	O
(	O
cid:80	O
)	O
p	O
(	O
i/n	O
)	O
log	O
p	O
(	O
i/n	O
)	O
+	O
log	O
(	O
cid:88	O
)	O
i	O
pi	O
log	O
pi	O
is	O
given	O
by	O
i	O
i	O
p	O
(	O
i/n	O
)	O
since	O
for	O
a	O
continuous	B
distribution	O
p	O
(	O
x	O
)	O
dx	O
=	O
1	O
h	O
=	O
−	O
(	O
cid:90	O
)	O
1	O
0	O
1	O
n	O
n	O
(	O
cid:88	O
)	O
i=1	O
p	O
(	O
i/n	O
)	O
=	O
1	O
draft	O
march	O
9	O
,	O
2010	O
a	O
discrete	B
approximation	O
of	O
this	O
integral	O
into	O
bins	O
of	O
size	O
1/n	O
gives	O
hence	O
show	O
that	O
for	O
large	O
n	O
,	O
(	O
cid:90	O
)	O
1	O
h	O
≈	O
−	O
0	O
p	O
(	O
x	O
)	O
log	O
p	O
(	O
x	O
)	O
dx	O
+	O
const	O
.	O
exercises	O
(	O
8.10.59	O
)	O
where	O
the	O
constant	O
tends	O
to	O
inﬁnity	O
as	O
n	O
→	O
∞	O
.	O
note	O
that	O
this	O
result	O
says	O
that	O
as	O
a	O
continuous	B
distribu-	O
tion	O
has	O
essentially	O
an	O
inﬁnite	O
number	O
of	O
states	O
,	O
the	O
amount	O
of	O
uncertainty	B
in	O
the	O
distribution	B
is	O
inﬁnite	O
(	O
alternatively	O
,	O
we	O
would	O
need	O
an	O
inﬁnite	O
number	O
of	O
bits	O
to	O
specify	O
a	O
continuous	B
value	O
)	O
.	O
this	O
motivates	O
the	O
deﬁnition	O
of	O
the	O
diﬀerential	B
entropy	O
,	O
which	O
neglects	O
the	O
inﬁnite	O
constant	O
of	O
the	O
limiting	O
case	O
of	O
the	O
discrete	B
entropy	O
.	O
exercise	O
114.	O
consider	O
two	O
multivariate	B
gaussians	O
n	O
(	O
x	O
µ1	O
,	O
σ1	O
)	O
and	O
n	O
(	O
x	O
µ2	O
,	O
σ2	O
)	O
.	O
1.	O
show	O
that	O
the	O
log	O
product	O
of	O
the	O
two	O
gaussians	O
is	O
given	O
by	O
1	O
µ1	O
+	O
σ−1	O
2	O
µ2	O
1	O
σ−1	O
µt	O
1	O
µ1	O
+	O
µt	O
2	O
σ−1	O
2	O
µ2	O
1	O
2	O
−	O
log	O
det	O
(	O
2πς1	O
)	O
det	O
(	O
2πς2	O
)	O
2	O
1	O
+	O
σ−1	O
(	O
cid:1	O
)	O
x+xt	O
(	O
cid:0	O
)	O
σ−1	O
xt	O
(	O
cid:0	O
)	O
σ−1	O
(	O
cid:0	O
)	O
x	O
−	O
a−1b	O
(	O
cid:1	O
)	O
t	O
a	O
(	O
cid:0	O
)	O
x	O
−	O
a−1b	O
(	O
cid:1	O
)	O
+	O
1	O
−	O
2	O
2.	O
deﬁning	O
a	O
=	O
σ−1	O
1	O
2	O
1	O
+	O
σ−1	O
−	O
(	O
cid:16	O
)	O
1	O
2	O
−	O
(	O
cid:1	O
)	O
1	O
µ1	O
+	O
σ−1	O
(	O
cid:16	O
)	O
2	O
and	O
b	O
=	O
σ−1	O
log	O
det	O
(	O
2πς1	O
)	O
det	O
(	O
2πς2	O
)	O
writing	O
σ	O
=	O
a−1	O
and	O
µ	O
=	O
a−1b	O
show	O
that	O
the	O
product	O
of	O
gaussians	O
is	O
a	O
gaussian	O
with	O
covariance	O
1	O
µ1	O
+	O
µt	O
2	O
µ2	O
−	O
1	O
2	O
bta−1b−	O
1	O
2	O
2	O
µ2	O
we	O
can	O
write	O
the	O
above	O
as	O
1	O
σ−1	O
µt	O
2	O
σ−1	O
1	O
2	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
σ	O
=	O
σ1	O
(	O
σ1	O
+	O
σ2	O
)	O
−1	O
σ2	O
mean	B
µ	O
=	O
σ1	O
(	O
σ1	O
+	O
σ2	O
)	O
−1	O
µ2	O
+	O
σ2	O
(	O
σ1	O
+	O
σ2	O
)	O
−1	O
µ1	O
and	O
log	O
prefactor	O
1	O
2	O
bta−1b	O
−	O
1	O
2	O
(	O
cid:16	O
)	O
1	O
σ−1	O
µt	O
1	O
µ1	O
+	O
µt	O
2	O
σ−1	O
2	O
µ2	O
(	O
cid:17	O
)	O
1	O
2	O
−	O
3.	O
show	O
that	O
this	O
can	O
be	O
written	O
as	O
n	O
(	O
x	O
µ1	O
,	O
σ1	O
)	O
n	O
(	O
x	O
µ2	O
,	O
σ2	O
)	O
=	O
n	O
(	O
x	O
µ	O
,	O
σ	O
)	O
exp	O
exercise	O
115.	O
show	O
that	O
∂	O
∂θ	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x|θ	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
x|θ0	O
)	O
|θ=θ0	O
=	O
0	O
1	O
2	O
log	O
det	O
(	O
2πς1	O
)	O
det	O
(	O
2πς2	O
)	O
+	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
(	O
cid:112	O
)	O
det	O
(	O
2πs	O
)	O
2	O
(	O
µ1	O
−	O
µ2	O
)	O
t	O
s−1	O
(	O
µ1	O
−	O
µ2	O
)	O
−	O
1	O
(	O
8.10.60	O
)	O
(	O
8.10.61	O
)	O
log	O
det	O
(	O
2πς	O
)	O
(	O
8.10.62	O
)	O
(	O
8.10.63	O
)	O
164	O
draft	O
march	O
9	O
,	O
2010	O
chapter	O
9	O
learning	B
as	O
inference	B
9.1	O
learning	B
as	O
inference	B
in	O
previous	O
chapters	O
we	O
largely	O
assumed	O
that	O
all	O
distributions	O
are	O
fully	O
speciﬁed	O
for	O
the	O
inference	B
tasks	O
.	O
in	O
machine	O
learning	B
and	O
related	O
ﬁelds	O
,	O
however	O
,	O
the	O
distributions	O
need	O
to	O
be	O
learned	O
on	O
the	O
basis	O
of	O
data	B
.	O
learning	B
is	O
then	O
the	O
problem	B
of	O
integrating	O
data	B
with	O
domain	B
knowledge	O
of	O
the	O
model	B
environment	O
.	O
deﬁnition	O
84	O
(	O
priors	O
and	O
posteriors	O
)	O
.	O
priors	O
and	O
posteriors	O
typically	O
refer	O
to	O
the	O
parameter	B
distributions	O
before	O
(	O
prior	B
to	O
)	O
and	O
after	O
(	O
posterior	B
to	O
)	O
seeing	O
the	O
data	B
.	O
formally	O
,	O
bayes	O
’	O
rule	O
relates	O
these	O
via	O
p	O
(	O
θ|v	O
)	O
=	O
p	O
(	O
v|θ	O
)	O
p	O
(	O
θ	O
)	O
p	O
(	O
v	O
)	O
where	O
θ	O
is	O
the	O
parameter	B
of	O
interest	O
and	O
v	O
represents	O
the	O
observed	O
(	O
visible	B
)	O
data	B
.	O
(	O
9.1.1	O
)	O
9.1.1	O
learning	B
the	O
bias	B
of	O
a	O
coin	O
consider	O
data	B
expressing	O
the	O
results	O
of	O
tossing	O
a	O
coin	O
.	O
we	O
write	O
vn	O
=	O
1	O
if	O
on	O
toss	O
n	O
the	O
coin	O
comes	O
up	O
heads	O
,	O
and	O
vn	O
=	O
0	O
if	O
it	O
is	O
tails	O
.	O
our	O
aim	O
is	O
to	O
estimate	O
the	O
probability	B
θ	O
that	O
the	O
coin	O
will	O
be	O
a	O
head	O
,	O
p	O
(	O
vn	O
=	O
1|θ	O
)	O
=	O
θ	O
–	O
called	O
the	O
‘	O
bias	B
’	O
of	O
the	O
coin	O
.	O
for	O
a	O
fair	O
coin	O
,	O
θ	O
=	O
0.5.	O
the	O
variables	O
in	O
this	O
environment	O
are	O
v1	O
,	O
.	O
.	O
.	O
,	O
vn	O
and	O
θ	O
and	O
we	O
require	O
a	O
model	B
of	O
the	O
probabilistic	B
interaction	O
of	O
the	O
variables	O
,	O
p	O
(	O
v1	O
,	O
.	O
.	O
.	O
,	O
vn	O
,	O
θ	O
)	O
.	O
assuming	O
there	O
is	O
no	O
dependence	O
between	O
the	O
observed	O
tosses	O
,	O
except	O
through	O
θ	O
,	O
we	O
have	O
the	O
belief	B
network	I
n	O
(	O
cid:89	O
)	O
n=1	O
p	O
(	O
v1	O
,	O
.	O
.	O
.	O
,	O
vn	O
,	O
θ	O
)	O
=	O
p	O
(	O
θ	O
)	O
p	O
(	O
vn|θ	O
)	O
(	O
9.1.2	O
)	O
which	O
is	O
depicted	O
in	O
ﬁg	O
(	O
9.1	O
)	O
.	O
the	O
assumption	O
that	O
each	O
observation	O
is	O
identically	O
and	O
independently	O
dis-	O
tributed	O
is	O
called	O
the	O
i.i.d	O
.	O
assumption	O
.	O
learning	B
refers	O
to	O
using	O
the	O
observations	O
v1	O
,	O
.	O
.	O
.	O
,	O
vn	O
to	O
infer	O
θ.	O
in	O
this	O
context	O
,	O
our	O
interest	O
is	O
p	O
(	O
θ|v1	O
,	O
.	O
.	O
.	O
,	O
vn	O
)	O
=	O
p	O
(	O
v1	O
,	O
.	O
.	O
.	O
,	O
vn	O
,	O
θ	O
)	O
p	O
(	O
v1	O
,	O
.	O
.	O
.	O
,	O
vn	O
)	O
=	O
p	O
(	O
v1	O
,	O
.	O
.	O
.	O
,	O
vn|θ	O
)	O
p	O
(	O
θ	O
)	O
p	O
(	O
v1	O
,	O
.	O
.	O
.	O
,	O
vn	O
)	O
(	O
9.1.3	O
)	O
we	O
still	O
need	O
to	O
fully	O
specify	O
the	O
prior	B
p	O
(	O
θ	O
)	O
.	O
to	O
avoid	O
complexities	O
resulting	O
from	O
continuous	B
variables	O
,	O
we	O
’	O
ll	O
consider	O
a	O
discrete	B
θ	O
with	O
only	O
three	O
possible	O
states	O
,	O
θ	O
∈	O
{	O
0.1	O
,	O
0.5	O
,	O
0.8	O
}	O
.	O
speciﬁcally	O
,	O
we	O
assume	O
p	O
(	O
θ	O
=	O
0.1	O
)	O
=	O
0.15	O
,	O
p	O
(	O
θ	O
=	O
0.5	O
)	O
=	O
0.8	O
,	O
p	O
(	O
θ	O
=	O
0.8	O
)	O
=	O
0.05	O
(	O
9.1.4	O
)	O
165	O
θ	O
v3	O
(	O
a	O
)	O
···	O
vn	O
v1	O
v2	O
θ	O
vn	O
(	O
b	O
)	O
n	O
learning	B
as	O
inference	B
figure	O
9.1	O
:	O
(	O
a	O
)	O
:	O
belief	B
network	I
for	O
coin	O
tossing	O
model	B
.	O
(	O
b	O
)	O
:	O
plate	B
notation	O
equiv-	O
alent	O
of	O
(	O
a	O
)	O
.	O
a	O
plate	B
replicates	O
the	O
quanti-	O
ties	O
inside	O
the	O
plate	B
a	O
number	O
of	O
times	O
as	O
speciﬁed	O
in	O
the	O
plate	B
.	O
as	O
shown	O
in	O
ﬁg	O
(	O
9.2a	O
)	O
.	O
this	O
prior	B
expresses	O
that	O
we	O
have	O
80	O
%	O
belief	O
that	O
the	O
coin	O
is	O
‘	O
fair	O
’	O
,	O
5	O
%	O
belief	O
the	O
coin	O
is	O
biased	O
to	O
land	O
heads	O
(	O
with	O
θ	O
=	O
0.8	O
)	O
,	O
and	O
15	O
%	O
belief	O
the	O
coin	O
is	O
biased	O
to	O
land	O
tails	O
(	O
with	O
θ	O
=	O
0.1	O
)	O
.	O
the	O
distribution	B
of	O
θ	O
given	O
the	O
data	B
and	O
our	O
beliefs	O
is	O
n	O
(	O
cid:89	O
)	O
n	O
(	O
cid:89	O
)	O
p	O
(	O
vn|θ	O
)	O
=	O
p	O
(	O
θ	O
)	O
(	O
cid:80	O
)	O
n	O
(	O
cid:80	O
)	O
n	O
i	O
[	O
vn=1	O
]	O
(	O
1	O
−	O
θ	O
)	O
n=1	O
n=1	O
θ	O
p	O
(	O
θ|v1	O
,	O
.	O
.	O
.	O
,	O
vn	O
)	O
∝	O
p	O
(	O
θ	O
)	O
∝	O
p	O
(	O
θ	O
)	O
θ	O
(	O
9.1.6	O
)	O
i	O
[	O
vn	O
=	O
1	O
]	O
is	O
the	O
number	O
of	O
occurrences	O
of	O
heads	O
,	O
which	O
we	O
more	O
conveniently	O
denote	O
(	O
9.1.5	O
)	O
n=1	O
i	O
[	O
vn=1	O
]	O
(	O
1	O
−	O
θ	O
)	O
i	O
[	O
vn=0	O
]	O
i	O
[	O
vn=0	O
]	O
n=1	O
in	O
the	O
above	O
(	O
cid:80	O
)	O
n	O
as	O
nh	O
.	O
likewise	O
,	O
(	O
cid:80	O
)	O
n	O
n=1	O
i	O
[	O
vn	O
=	O
0	O
]	O
is	O
the	O
number	O
of	O
tails	O
,	O
nt	O
.	O
hence	O
n=1	O
p	O
(	O
θ|v1	O
,	O
.	O
.	O
.	O
,	O
vn	O
)	O
∝	O
p	O
(	O
θ	O
)	O
θnh	O
(	O
1	O
−	O
θ	O
)	O
nt	O
for	O
an	O
experiment	O
with	O
nh	O
=	O
2	O
,	O
nt	O
=	O
8	O
,	O
the	O
posterior	B
distribution	O
is	O
p	O
(	O
θ	O
=	O
0.1|v	O
)	O
=	O
k	O
×	O
0.15	O
×	O
0.12	O
×	O
0.98	O
=	O
k	O
×	O
6.46	O
×	O
10−4	O
p	O
(	O
θ	O
=	O
0.5|v	O
)	O
=	O
k	O
×	O
0.8	O
×	O
0.52	O
×	O
0.58	O
=	O
k	O
×	O
7.81	O
×	O
10−4	O
p	O
(	O
θ	O
=	O
0.8|v	O
)	O
=	O
k	O
×	O
0.05	O
×	O
0.82	O
×	O
0.28	O
=	O
k	O
×	O
8.19	O
×	O
10−8	O
(	O
9.1.7	O
)	O
(	O
9.1.8	O
)	O
(	O
9.1.9	O
)	O
(	O
9.1.10	O
)	O
where	O
v	O
is	O
shorthand	O
for	O
v1	O
,	O
.	O
.	O
.	O
,	O
vn	O
.	O
from	O
the	O
normalisation	B
requirement	O
we	O
have	O
1/k	O
=	O
6.46	O
×	O
10−4	O
+	O
7.81	O
×	O
10−4	O
+	O
8.19	O
×	O
10−8	O
=	O
0.0014	O
,	O
so	O
that	O
p	O
(	O
θ	O
=	O
0.1|v	O
)	O
=	O
0.4525	O
,	O
p	O
(	O
θ	O
=	O
0.5|v	O
)	O
=	O
0.5475	O
,	O
p	O
(	O
θ	O
=	O
0.8|v	O
)	O
=	O
0.0001	O
(	O
9.1.11	O
)	O
as	O
shown	O
in	O
ﬁg	O
(	O
9.2b	O
)	O
.	O
these	O
are	O
the	O
‘	O
posterior	B
’	O
parameter	B
beliefs	O
.	O
in	O
this	O
case	O
,	O
if	O
we	O
were	O
asked	O
to	O
choose	O
a	O
single	O
a	O
posteriori	O
most	O
likely	O
value	O
for	O
θ	O
,	O
it	O
would	O
be	O
θ	O
=	O
0.5	O
,	O
although	O
our	O
conﬁdence	O
in	O
this	O
is	O
low	O
since	O
the	O
posterior	B
belief	O
that	O
θ	O
=	O
0.1	O
is	O
also	O
appreciable	O
.	O
this	O
result	O
is	O
intuitive	O
since	O
,	O
even	O
though	O
we	O
observed	O
more	O
tails	O
than	O
heads	O
,	O
our	O
prior	B
belief	O
was	O
that	O
it	O
was	O
more	O
likely	O
the	O
coin	O
is	O
fair	O
.	O
repeating	O
the	O
above	O
with	O
nh	O
=	O
20	O
,	O
nt	O
=	O
80	O
,	O
the	O
posterior	B
changes	O
to	O
p	O
(	O
θ	O
=	O
0.1|v	O
)	O
=	O
1−1.93×10−6	O
,	O
p	O
(	O
θ	O
=	O
0.8|v	O
)	O
=	O
2.13×10−35	O
(	O
9.1.12	O
)	O
ﬁg	O
(	O
9.1c	O
)	O
,	O
so	O
that	O
the	O
posterior	B
belief	O
in	O
θ	O
=	O
0.1	O
dominates	O
.	O
this	O
is	O
reasonable	O
since	O
in	O
this	O
situation	O
,	O
there	O
are	O
so	O
many	O
more	O
tails	O
than	O
heads	O
that	O
this	O
is	O
unlikely	O
to	O
occur	O
from	O
a	O
fair	O
coin	O
.	O
even	O
though	O
we	O
a	O
priori	O
thought	O
that	O
the	O
coin	O
was	O
fair	O
,	O
a	O
posteriori	O
we	O
have	O
enough	O
evidence	B
to	O
change	O
our	O
minds	O
.	O
p	O
(	O
θ	O
=	O
0.5|v	O
)	O
=	O
1.93×10−6	O
,	O
0.8	O
0.1	O
0.1	O
0.5	O
θ	O
(	O
a	O
)	O
0.8	O
0.1	O
0.5	O
θ	O
(	O
b	O
)	O
0.8	O
0.5	O
θ	O
(	O
c	O
)	O
figure	O
9.2	O
:	O
(	O
a	O
)	O
:	O
prior	B
encoding	O
our	O
be-	O
liefs	O
about	O
the	O
amount	O
the	O
coin	O
is	O
biased	O
(	O
b	O
)	O
:	O
posterior	B
having	O
seen	O
2	O
to	O
heads	O
.	O
(	O
c	O
)	O
:	O
posterior	B
having	O
heads	O
and	O
8	O
tails	O
.	O
seen	O
20	O
heads	O
and	O
80	O
tails	O
.	O
166	O
draft	O
march	O
9	O
,	O
2010	O
learning	B
as	O
inference	B
9.1.2	O
making	O
decisions	O
in	O
itself	O
,	O
the	O
bayesian	O
posterior	B
merely	O
represents	O
our	O
beliefs	O
and	O
says	O
nothing	O
about	O
how	O
best	O
to	O
sum-	O
marise	O
these	O
beliefs	O
.	O
in	O
situations	O
in	O
which	O
decisions	O
need	O
to	O
be	O
taken	O
under	O
uncertainty	B
we	O
need	O
to	O
additionally	O
specify	O
what	O
the	O
utility	B
of	O
any	O
decision	O
is	O
,	O
as	O
in	O
chapter	O
(	O
7	O
)	O
.	O
in	O
the	O
coin	O
tossing	O
scenario	O
where	O
θ	O
is	O
assumed	O
to	O
be	O
either	O
0.1	O
,	O
0.5	O
or	O
0.8	O
,	O
we	O
setup	O
a	O
decision	O
problem	O
as	O
follows	O
:	O
if	O
we	O
correctly	O
state	O
the	O
bias	B
of	O
the	O
coin	O
we	O
gain	O
10	O
points	O
;	O
being	O
incorrect	O
,	O
however	O
,	O
loses	O
20	O
points	O
.	O
we	O
can	O
write	O
this	O
using	O
u	O
(	O
θ	O
,	O
θ0	O
)	O
=	O
10i	O
(	O
cid:2	O
)	O
θ	O
=	O
θ0	O
(	O
cid:3	O
)	O
−	O
20i	O
(	O
cid:2	O
)	O
θ	O
(	O
cid:54	O
)	O
=	O
θ0	O
(	O
cid:3	O
)	O
(	O
9.1.13	O
)	O
where	O
θ0	O
is	O
the	O
true	O
value	B
for	O
the	O
bias	B
.	O
the	O
expected	O
utility	B
of	O
the	O
decision	O
that	O
the	O
coin	O
is	O
θ	O
=	O
0.1	O
is	O
u	O
(	O
θ	O
=	O
0.1	O
)	O
=	O
u	O
(	O
θ	O
=	O
0.1	O
,	O
θ0	O
=	O
0.1	O
)	O
p	O
(	O
θ0	O
=	O
0.1|v	O
)	O
+	O
u	O
(	O
θ	O
=	O
0.1	O
,	O
θ0	O
=	O
0.5	O
)	O
p	O
(	O
θ0	O
=	O
0.5|v	O
)	O
+	O
u	O
(	O
θ	O
=	O
0.1	O
,	O
θ0	O
=	O
0.8	O
)	O
p	O
(	O
θ0	O
=	O
0.8|v	O
)	O
plugging	O
in	O
the	O
numbers	O
from	O
equation	B
(	O
9.1.11	O
)	O
,	O
we	O
obtain	O
u	O
(	O
θ	O
=	O
0.1	O
)	O
=	O
10	O
×	O
0.4525	O
−	O
20	O
×	O
0.5475	O
−	O
20	O
×	O
0.0001	O
=	O
−6.4270	O
similarly	O
u	O
(	O
θ	O
=	O
0.5	O
)	O
=	O
10	O
×	O
0.5475	O
−	O
20	O
×	O
0.4525	O
−	O
20	O
×	O
0.0001	O
=	O
−3.5770	O
and	O
u	O
(	O
θ	O
=	O
0.8	O
)	O
=	O
10	O
×	O
0.0001	O
−	O
20	O
×	O
0.4525	O
−	O
20	O
×	O
0.5475	O
=	O
−19.999	O
so	O
that	O
the	O
best	O
decision	O
is	O
to	O
say	O
that	O
the	O
coin	O
is	O
unbiased	O
,	O
θ	O
=	O
0.5.	O
repeating	O
the	O
above	O
calculations	O
for	O
nh	O
=	O
20	O
,	O
nt	O
=	O
80	O
,	O
we	O
arrive	O
at	O
u	O
(	O
θ	O
=	O
0.1	O
)	O
=	O
10	O
×	O
(	O
1	O
−	O
1.93	O
×	O
10−6	O
)	O
−	O
20	O
(	O
cid:0	O
)	O
1.93	O
×	O
10−6	O
+	O
2.13	O
×	O
10−35	O
(	O
cid:1	O
)	O
=	O
9.9999	O
u	O
(	O
θ	O
=	O
0.5	O
)	O
=	O
10	O
×	O
1.93	O
×	O
10−6	O
−	O
20	O
(	O
cid:0	O
)	O
1	O
−	O
1.93	O
×	O
10−6	O
+	O
2.13	O
×	O
10−35	O
(	O
cid:1	O
)	O
u	O
(	O
θ	O
=	O
0.8	O
)	O
=	O
10	O
×	O
2.13	O
×	O
10−35	O
−	O
20	O
(	O
cid:0	O
)	O
1	O
−	O
1.93	O
×	O
10−6	O
+	O
1.93	O
×	O
10−6	O
(	O
cid:1	O
)	O
≈	O
−20.0	O
≈	O
−20.0	O
so	O
that	O
the	O
best	O
decision	O
in	O
this	O
case	O
is	O
to	O
choose	O
θ	O
=	O
0.1	O
.	O
(	O
9.1.14	O
)	O
(	O
9.1.15	O
)	O
(	O
9.1.16	O
)	O
(	O
9.1.17	O
)	O
(	O
9.1.18	O
)	O
(	O
9.1.19	O
)	O
(	O
9.1.20	O
)	O
as	O
more	O
information	O
about	O
the	O
distribution	B
p	O
(	O
v	O
,	O
θ	O
)	O
becomes	O
available	O
the	O
posterior	B
p	O
(	O
θ|v	O
)	O
becomes	O
in-	O
creasingly	O
peaked	O
,	O
aiding	O
our	O
decision	O
making	O
process	O
.	O
9.1.3	O
a	O
continuum	O
of	O
parameters	O
in	O
section	O
(	O
9.1.1	O
)	O
we	O
considered	O
only	O
three	O
possible	O
values	O
for	O
θ.	O
here	O
we	O
discuss	O
a	O
continuum	O
of	O
parameters	O
.	O
using	O
a	O
ﬂat	O
prior	B
we	O
ﬁrst	O
examine	O
the	O
case	O
of	O
a	O
‘	O
ﬂat	O
’	O
or	O
uniform	B
prior	O
p	O
(	O
θ	O
)	O
=	O
k	O
for	O
some	O
constant	O
k.	O
for	O
continuous	B
variables	O
,	O
normalisation	B
requires	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
1	O
p	O
(	O
θ	O
)	O
dθ	O
=	O
1	O
since	O
0	O
≤	O
θ	O
≤	O
1	O
,	O
p	O
(	O
θ	O
)	O
dθ	O
=	O
k	O
=	O
1	O
0	O
draft	O
march	O
9	O
,	O
2010	O
(	O
9.1.21	O
)	O
(	O
9.1.22	O
)	O
167	O
learning	B
as	O
inference	B
figure	O
9.3	O
:	O
posterior	B
p	O
(	O
θ|v	O
)	O
assuming	O
a	O
ﬂat	O
prior	B
on	O
θ	O
.	O
(	O
red	O
)	O
nh	O
=	O
2	O
,	O
nt	O
=	O
8	O
and	O
(	O
blue	O
)	O
nh	O
=	O
20	O
,	O
nt	O
=	O
80.	O
in	O
both	O
cases	O
,	O
the	O
most	B
probable	I
state	I
of	O
the	O
posterior	B
is	O
0.2	O
,	O
which	O
makes	O
intuitive	O
sense	O
,	O
since	O
the	O
fraction	O
of	O
heads	O
to	O
tails	O
in	O
both	O
cases	O
is	O
0.2.	O
where	O
there	O
is	O
more	O
data	B
,	O
the	O
posterior	B
is	O
more	O
certain	O
and	O
sharpens	O
around	O
the	O
most	O
probable	O
value	O
.	O
the	O
maximum	O
a	O
posteriori	O
setting	O
is	O
θ	O
=	O
0.2	O
in	O
both	O
cases	O
,	O
this	O
being	O
the	O
value	B
of	O
θ	O
for	O
which	O
the	O
posterior	B
attains	O
its	O
highest	O
value	B
.	O
repeating	O
the	O
previous	O
calculations	O
with	O
this	O
ﬂat	O
continuous	B
prior	O
,	O
we	O
have	O
1	O
c	O
θnh	O
(	O
1	O
−	O
θ	O
)	O
nt	O
p	O
(	O
θ|v	O
)	O
=	O
(	O
cid:90	O
)	O
1	O
c	O
=	O
where	O
c	O
is	O
a	O
constant	O
to	O
be	O
determined	O
by	O
normalisation	B
,	O
θnh	O
(	O
1	O
−	O
θ	O
)	O
nt	O
dθ	O
≡	O
b	O
(	O
nh	O
+	O
1	O
,	O
nt	O
+	O
1	O
)	O
0	O
(	O
9.1.23	O
)	O
(	O
9.1.24	O
)	O
where	O
b	O
(	O
α	O
,	O
β	O
)	O
is	O
the	O
beta	B
function	O
.	O
deﬁnition	O
85	O
(	O
conjugacy	O
)	O
.	O
if	O
the	O
posterior	B
is	O
of	O
the	O
same	O
parametric	O
form	O
as	O
the	O
prior	B
,	O
then	O
we	O
call	O
the	O
prior	B
the	O
conjugate	B
distribution	I
for	O
the	O
likelihood	B
distribution	O
.	O
using	O
a	O
conjugate	B
prior	O
determining	O
the	O
normalisation	B
constant	I
of	O
a	O
continuous	B
distribution	O
requires	O
that	O
the	O
integral	O
of	O
the	O
unnormalised	O
posterior	B
can	O
be	O
carried	O
out	O
.	O
for	O
the	O
coin	O
tossing	O
case	O
,	O
it	O
is	O
clear	O
that	O
if	O
the	O
prior	B
is	O
of	O
the	O
form	O
of	O
a	O
beta	B
distribution	O
,	O
then	O
the	O
posterior	B
will	O
be	O
of	O
the	O
same	O
parametric	O
form	O
:	O
p	O
(	O
θ	O
)	O
=	O
1	O
b	O
(	O
α	O
,	O
β	O
)	O
θα−1	O
(	O
1	O
−	O
θ	O
)	O
β−1	O
the	O
posterior	B
is	O
p	O
(	O
θ|v	O
)	O
∝	O
θα−1	O
(	O
1	O
−	O
θ	O
)	O
β−1	O
θnh	O
(	O
1	O
−	O
θ	O
)	O
nt	O
so	O
that	O
p	O
(	O
θ|v	O
)	O
=	O
1	O
b	O
(	O
α	O
+	O
nh	O
,	O
β	O
+	O
nt	O
)	O
θα+nh−1	O
(	O
1	O
−	O
θ	O
)	O
β+nt	O
−1	O
≡	O
b	O
(	O
θ|α	O
+	O
nh	O
,	O
β	O
+	O
nt	O
)	O
(	O
9.1.25	O
)	O
(	O
9.1.26	O
)	O
(	O
9.1.27	O
)	O
the	O
prior	B
and	O
posterior	B
are	O
of	O
the	O
same	O
form	O
(	O
both	O
beta	B
distributions	O
)	O
but	O
simply	O
with	O
diﬀerent	O
parame-	O
ters	O
.	O
hence	O
the	O
beta	B
distribution	O
is	O
‘	O
conjugate	B
’	O
to	O
the	O
binomial	B
distribution	O
.	O
9.1.4	O
decisions	O
based	O
on	O
continuous	B
intervals	O
the	O
result	O
of	O
a	O
coin	O
tossing	O
experiment	O
is	O
nh	O
=	O
2	O
heads	O
and	O
nt	O
=	O
8	O
tails	O
.	O
you	O
now	O
need	O
to	O
make	O
a	O
deci-	O
sion	O
:	O
you	O
win	O
10	O
dollars	O
if	O
your	O
guess	O
that	O
the	O
coin	O
is	O
more	O
likely	O
to	O
come	O
up	O
heads	O
than	O
tails	O
is	O
correct	O
.	O
if	O
your	O
guess	O
is	O
incorrect	O
,	O
you	O
lose	O
a	O
million	O
dollars	O
.	O
what	O
is	O
your	O
decision	O
?	O
(	O
assume	O
an	O
uninformative	O
prior	B
)	O
.	O
we	O
need	O
two	O
quantities	O
,	O
θ	O
for	O
our	O
guess	O
and	O
θ0	O
for	O
the	O
truth	O
.	O
then	O
the	O
utility	B
of	O
saying	O
heads	O
is	O
u	O
(	O
θ	O
>	O
0.5	O
,	O
θ0	O
>	O
0.5	O
)	O
p	O
(	O
θ0	O
>	O
0.5|v	O
)	O
+	O
u	O
(	O
θ	O
>	O
0.5	O
,	O
θ0	O
<	O
0.5	O
)	O
p	O
(	O
θ0	O
<	O
0.5|v	O
)	O
168	O
(	O
9.1.28	O
)	O
draft	O
march	O
9	O
,	O
2010	O
00.20.40.60.810510θ	O
maximum	O
a	O
posteriori	O
and	O
maximum	B
likelihood	I
in	O
the	O
above	O
,	O
(	O
cid:90	O
)	O
0.5	O
0	O
p	O
(	O
θ0|v	O
)	O
dθ0	O
p	O
(	O
θ0	O
<	O
0.5|v	O
)	O
=	O
=	O
b	O
(	O
α	O
+	O
nh	O
,	O
β	O
+	O
nt	O
)	O
≡	O
i0.5	O
(	O
α	O
+	O
nh	O
,	O
β	O
+	O
nt	O
)	O
1	O
(	O
cid:90	O
)	O
0.5	O
0	O
θα+nh−1	O
(	O
1	O
−	O
θ	O
)	O
β+nt	O
−1	O
dθ	O
(	O
9.1.29	O
)	O
(	O
9.1.30	O
)	O
(	O
9.1.31	O
)	O
where	O
ix	O
(	O
a	O
,	O
b	O
)	O
is	O
the	O
regularised	B
incomplete	O
beta	B
function	O
.	O
for	O
the	O
former	O
case	O
of	O
nh	O
=	O
2	O
,	O
nt	O
=	O
8	O
,	O
under	O
a	O
ﬂat	O
prior	B
,	O
p	O
(	O
θ0	O
<	O
0.5|v	O
)	O
=	O
i0.5	O
(	O
nh	O
+	O
1	O
,	O
nt	O
+	O
1	O
)	O
=	O
0.9673	O
since	O
the	O
events	O
are	O
exclusive	O
,	O
p	O
(	O
θ0	O
≥	O
0.5|v	O
)	O
=	O
1	O
−	O
0.9673	O
=	O
0.0327.	O
hence	O
the	O
expected	O
utility	B
of	O
saying	O
heads	O
is	O
more	O
likely	O
is	O
10	O
×	O
0.0327	O
−	O
1000000	O
×	O
0.9673	O
=	O
−9.673	O
×	O
105.	O
similarly	O
,	O
the	O
utility	B
of	O
saying	O
tails	O
is	O
more	O
likely	O
is	O
10	O
×	O
0.9673	O
−	O
1000000	O
×	O
0.0327	O
=	O
−3.269	O
×	O
104	O
.	O
(	O
9.1.32	O
)	O
(	O
9.1.33	O
)	O
(	O
9.1.34	O
)	O
so	O
we	O
are	O
better	O
oﬀ	O
taking	O
the	O
decision	O
that	O
the	O
coin	O
is	O
more	O
likely	O
to	O
come	O
up	O
tails	O
.	O
if	O
we	O
modify	O
the	O
above	O
so	O
that	O
we	O
lose	O
100	O
million	O
dollars	O
if	O
we	O
guess	O
tails	O
when	O
in	O
fact	O
it	O
as	O
heads	O
,	O
the	O
expected	O
utility	B
of	O
saying	O
tails	O
would	O
be	O
−3.27×	O
106	O
in	O
which	O
case	O
we	O
would	O
be	O
better	O
of	O
saying	O
heads	O
.	O
in	O
this	O
case	O
,	O
even	O
though	O
we	O
are	O
more	O
conﬁdent	O
that	O
the	O
coin	O
is	O
likely	O
to	O
come	O
up	O
tails	O
,	O
we	O
would	O
pay	O
such	O
a	O
penalty	O
of	O
making	O
a	O
mistake	O
in	O
saying	O
tails	O
,	O
that	O
it	O
is	O
fact	O
better	O
to	O
say	O
heads	O
.	O
9.2	O
maximum	O
a	O
posteriori	O
and	O
maximum	B
likelihood	I
9.2.1	O
summarising	O
the	O
posterior	B
deﬁnition	O
86	O
(	O
maximum	B
likelihood	I
and	O
maximum	O
a	O
posteriori	O
)	O
.	O
maximum	B
likelihood	I
sets	O
parameter	B
θ	O
,	O
given	O
data	B
v	O
,	O
using	O
θm	O
l	O
=	O
argmax	O
(	O
9.2.1	O
)	O
p	O
(	O
v|θ	O
)	O
θ	O
maximum	O
a	O
posteriori	O
uses	O
that	O
setting	O
θ	O
that	O
maximises	O
the	O
posterior	B
distribution	O
of	O
the	O
parameter	B
,	O
θm	O
ap	O
=	O
argmax	O
θ	O
p	O
(	O
v|θ	O
)	O
p	O
(	O
θ	O
)	O
where	O
p	O
(	O
θ	O
)	O
is	O
the	O
prior	B
distribution	O
.	O
(	O
9.2.2	O
)	O
δ	O
(	O
cid:0	O
)	O
θ	O
,	O
θm	O
ap	O
(	O
cid:1	O
)	O
.	O
in	O
making	O
such	O
an	O
approximation	B
,	O
potentially	O
useful	O
information	O
concerning	O
the	O
reliability	O
a	O
crude	O
summary	O
of	O
the	O
posterior	B
is	O
given	O
by	O
a	O
distribution	B
with	O
all	O
its	O
mass	O
in	O
a	O
single	O
most	B
likely	I
state	I
,	O
of	O
the	O
parameter	B
estimate	O
is	O
lost	O
.	O
possibilities	O
and	O
their	O
associated	O
credibilities	O
.	O
in	O
contrast	O
the	O
full	O
posterior	B
reﬂects	O
our	O
beliefs	O
about	O
the	O
range	O
of	O
one	O
can	O
motivate	O
map	B
from	O
a	O
decision	O
theoretic	O
perspective	O
.	O
if	O
we	O
assume	O
a	O
utility	B
that	O
is	O
zero	O
for	O
all	O
but	O
the	O
correct	O
θ	O
,	O
u	O
(	O
θtrue	O
,	O
θ	O
)	O
=	O
i	O
[	O
θtrue	O
=	O
θ	O
]	O
draft	O
march	O
9	O
,	O
2010	O
(	O
9.2.3	O
)	O
169	O
θa	O
an	O
θs	O
sn	O
cn	O
n	O
=	O
1	O
:	O
n	O
θc	O
(	O
b	O
)	O
a	O
s	O
c	O
(	O
a	O
)	O
maximum	O
a	O
posteriori	O
and	O
maximum	B
likelihood	I
figure	O
9.4	O
:	O
(	O
a	O
)	O
:	O
a	O
model	B
for	O
the	O
relationship	O
between	O
(	O
b	O
)	O
:	O
lung	O
cancer	O
,	O
asbestos	O
exposure	O
and	O
smoking	O
.	O
plate	B
notation	O
replicating	O
the	O
observed	O
n	O
datapoints	O
and	O
placing	O
priors	O
over	O
the	O
cpts	O
,	O
tied	O
across	O
all	O
dat-	O
apoints	O
.	O
then	O
the	O
expected	O
utility	B
of	O
θ	O
is	O
u	O
(	O
θ	O
)	O
=	O
(	O
cid:88	O
)	O
θtrue	O
i	O
[	O
θtrue	O
=	O
θ	O
]	O
p	O
(	O
θtrue|v	O
)	O
=	O
p	O
(	O
θ|v	O
)	O
(	O
9.2.4	O
)	O
this	O
means	O
that	O
the	O
maximum	O
utility	O
decision	O
is	O
to	O
return	O
that	O
θ	O
with	O
the	O
highest	O
posterior	B
value	O
.	O
when	O
a	O
‘	O
ﬂat	O
’	O
prior	B
p	O
(	O
θ	O
)	O
=	O
const	O
.	O
is	O
used	O
the	O
map	B
parameter	O
assignment	O
is	O
equivalent	B
to	O
the	O
maximum	B
likelihood	I
setting	O
θm	O
l	O
=	O
argmax	O
θ	O
p	O
(	O
v|θ	O
)	O
(	O
9.2.5	O
)	O
the	O
term	O
maximum	B
likelihood	I
refers	O
to	O
the	O
parameter	B
θ	O
for	O
which	O
the	O
observed	O
data	O
is	O
most	O
likely	O
to	O
be	O
generated	O
by	O
the	O
model	B
.	O
since	O
the	O
logarithm	O
is	O
a	O
strictly	O
increasing	O
function	B
,	O
then	O
for	O
a	O
positive	O
function	O
f	O
(	O
θ	O
)	O
θopt	O
=	O
argmax	O
θ	O
f	O
(	O
θ	O
)	O
⇔	O
θopt	O
=	O
argmax	O
θ	O
log	O
f	O
(	O
θ	O
)	O
(	O
9.2.6	O
)	O
so	O
that	O
the	O
map	B
parameters	O
can	O
be	O
found	O
either	O
by	O
optimising	O
the	O
map	B
objective	O
or	O
,	O
equivalently	O
,	O
its	O
logarithm	O
,	O
log	O
p	O
(	O
θ|v	O
)	O
=	O
log	O
p	O
(	O
v|θ	O
)	O
+	O
log	O
p	O
(	O
θ	O
)	O
−	O
log	O
p	O
(	O
v	O
)	O
where	O
the	O
normalisation	B
constant	I
,	O
p	O
(	O
v	O
)	O
,	O
is	O
not	O
a	O
function	B
of	O
θ.	O
the	O
log	O
likelihood	B
is	O
convenient	O
since	O
under	O
the	O
i.i.d	O
.	O
assumption	O
it	O
is	O
a	O
summation	O
of	O
data	B
terms	O
,	O
log	O
p	O
(	O
θ|v	O
)	O
=	O
(	O
cid:88	O
)	O
n	O
log	O
p	O
(	O
vn|θ	O
)	O
+	O
log	O
p	O
(	O
θ	O
)	O
−	O
log	O
p	O
(	O
v	O
)	O
(	O
9.2.7	O
)	O
(	O
9.2.8	O
)	O
so	O
that	O
quantities	O
such	O
as	O
derivatives	O
of	O
the	O
log-likelihood	O
w.r.t	O
.	O
θ	O
are	O
straightforward	O
to	O
compute	O
.	O
example	O
36.	O
in	O
the	O
coin-tossing	O
experiment	O
of	O
section	O
(	O
9.1.1	O
)	O
the	O
ml	O
setting	O
is	O
θ	O
=	O
0.2	O
in	O
both	O
nh	O
=	O
2	O
,	O
nt	O
=	O
8	O
and	O
nh	O
=	O
20	O
,	O
nt	O
=	O
80	O
.	O
9.2.2	O
maximum	B
likelihood	I
and	O
the	O
empirical	B
distribution	I
given	O
a	O
dataset	O
of	O
discrete	B
variables	O
x	O
=	O
(	O
cid:8	O
)	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
(	O
cid:9	O
)	O
we	O
deﬁne	O
the	O
empirical	B
distribution	I
as	O
i	O
[	O
x	O
=	O
xn	O
]	O
(	O
9.2.9	O
)	O
draft	O
march	O
9	O
,	O
2010	O
n	O
(	O
cid:88	O
)	O
n=1	O
q	O
(	O
x	O
)	O
=	O
1	O
n	O
170	O
maximum	O
a	O
posteriori	O
and	O
maximum	B
likelihood	I
a	O
1	O
1	O
0	O
0	O
1	O
0	O
1	O
s	O
1	O
0	O
1	O
1	O
1	O
0	O
0	O
c	O
1	O
0	O
1	O
0	O
1	O
0	O
1	O
figure	O
9.5	O
:	O
a	O
database	O
containing	O
information	O
about	O
the	O
asbestos	O
expo-	O
sure	O
(	O
1	O
signiﬁes	O
exposure	O
)	O
,	O
being	O
a	O
smoker	O
(	O
1	O
signiﬁes	O
the	O
individual	O
is	O
a	O
smoker	O
)	O
,	O
and	O
lung	O
cancer	O
(	O
1	O
signiﬁes	O
the	O
individual	O
has	O
lung	O
cancer	O
)	O
.	O
each	O
row	O
contains	O
the	O
information	O
for	O
an	O
individual	O
,	O
so	O
that	O
there	O
are	O
7	O
individuals	O
in	O
the	O
database	O
.	O
in	O
the	O
case	O
that	O
x	O
is	O
a	O
vector	O
of	O
variables	O
,	O
i	O
[	O
x	O
=	O
xn	O
]	O
=	O
(	O
cid:89	O
)	O
i	O
[	O
xi	O
=	O
xn	O
i	O
]	O
(	O
9.2.10	O
)	O
i	O
the	O
kullback-leibler	O
divergence	B
between	O
the	O
empirical	B
distribution	I
q	O
(	O
x	O
)	O
and	O
a	O
distribution	B
p	O
(	O
x	O
)	O
is	O
kl	O
(	O
q|p	O
)	O
=	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
−	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
(	O
9.2.11	O
)	O
our	O
interest	O
is	O
the	O
functional	O
dependence	B
of	O
kl	O
(	O
q|p	O
)	O
on	O
p.	O
since	O
the	O
entropic	O
term	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
is	O
inde-	O
pendent	O
of	O
p	O
(	O
x	O
)	O
we	O
may	O
consider	O
this	O
constant	O
and	O
focus	O
on	O
the	O
second	O
term	O
alone	O
.	O
hence	O
n	O
(	O
cid:88	O
)	O
n=1	O
1	O
n	O
log	O
p	O
(	O
xn	O
)	O
+	O
const	O
.	O
(	O
9.2.12	O
)	O
kl	O
(	O
q|p	O
)	O
=	O
−	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
+	O
const	O
.	O
=	O
−	O
we	O
recognise	O
(	O
cid:80	O
)	O
n	O
n=1	O
log	O
p	O
(	O
xn	O
)	O
as	O
the	O
log	O
likelihood	B
under	O
the	O
model	B
p	O
(	O
x	O
)	O
,	O
assuming	O
that	O
the	O
data	B
is	O
i.i.d	O
.	O
this	O
means	O
that	O
setting	O
parameters	O
by	O
maximum	B
likelihood	I
is	O
equivalent	B
to	O
setting	O
parameters	O
by	O
minimis-	O
ing	O
the	O
kullback-leibler	O
divergence	B
between	O
the	O
empirical	B
distribution	I
and	O
the	O
parameterised	O
distribution	B
.	O
in	O
the	O
case	O
that	O
p	O
(	O
x	O
)	O
is	O
unconstrained	O
,	O
the	O
optimal	O
choice	O
is	O
to	O
set	O
p	O
(	O
x	O
)	O
=	O
q	O
(	O
x	O
)	O
,	O
namely	O
the	O
maximum	B
likelihood	I
optimal	O
distribution	B
corresponds	O
to	O
the	O
empirical	B
distribution	I
.	O
9.2.3	O
maximum	B
likelihood	I
training	O
of	O
belief	B
networks	I
consider	O
the	O
following	O
model	B
of	O
the	O
relationship	O
between	O
exposure	O
to	O
asbestos	O
(	O
a	O
)	O
,	O
being	O
a	O
smoker	O
(	O
s	O
)	O
and	O
the	O
incidence	B
of	O
lung	O
cancer	O
(	O
c	O
)	O
p	O
(	O
a	O
,	O
s	O
,	O
c	O
)	O
=	O
p	O
(	O
c|a	O
,	O
s	O
)	O
p	O
(	O
a	O
)	O
p	O
(	O
s	O
)	O
(	O
9.2.13	O
)	O
which	O
is	O
depicted	O
in	O
ﬁg	O
(	O
9.4a	O
)	O
.	O
each	O
variable	B
is	O
binary	O
,	O
dom	O
(	O
a	O
)	O
=	O
{	O
0	O
,	O
1	O
}	O
,	O
dom	O
(	O
s	O
)	O
=	O
{	O
0	O
,	O
1	O
}	O
,	O
dom	O
(	O
c	O
)	O
=	O
{	O
0	O
,	O
1	O
}	O
.	O
we	O
assume	O
that	O
there	O
is	O
no	O
direct	O
relationship	O
between	O
smoking	O
and	O
exposure	O
to	O
asbestos	O
.	O
this	O
is	O
the	O
kind	O
of	O
assumption	O
that	O
we	O
may	O
be	O
able	O
to	O
elicit	O
from	O
medical	O
experts	O
.	O
furthermore	O
,	O
we	O
assume	O
that	O
we	O
have	O
a	O
list	O
of	O
patient	O
records	O
,	O
ﬁg	O
(	O
9.5	O
)	O
,	O
where	O
each	O
row	O
represents	O
a	O
patient	O
’	O
s	O
data	B
.	O
to	O
learn	O
the	O
table	O
entries	O
p	O
(	O
c|a	O
,	O
s	O
)	O
we	O
can	O
do	O
so	O
by	O
counting	B
the	O
number	O
of	O
c	O
is	O
in	O
state	O
1	O
for	O
each	O
of	O
the	O
4	O
parental	O
states	O
of	O
a	O
and	O
s	O
:	O
p	O
(	O
c	O
=	O
1|a	O
=	O
0	O
,	O
s	O
=	O
0	O
)	O
=	O
0	O
,	O
p	O
(	O
c	O
=	O
1|a	O
=	O
0	O
,	O
s	O
=	O
1	O
)	O
=	O
0.5	O
p	O
(	O
c	O
=	O
1|a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
=	O
0.5	O
p	O
(	O
c	O
=	O
1|a	O
=	O
1	O
,	O
s	O
=	O
1	O
)	O
=	O
1	O
(	O
9.2.14	O
)	O
similarly	O
,	O
based	O
on	O
counting	B
,	O
p	O
(	O
a	O
=	O
1	O
)	O
=	O
4/7	O
,	O
and	O
p	O
(	O
s	O
=	O
1	O
)	O
=	O
4/7	O
.	O
these	O
three	O
cpts	O
then	O
complete	O
the	O
full	O
distribution	B
speciﬁcation	O
.	O
setting	O
the	O
cpt	O
entries	O
in	O
this	O
way	O
by	O
counting	B
the	O
relative	O
number	O
of	O
occurrences	O
corresponds	O
mathe-	O
matically	O
to	O
maximum	B
likelihood	I
learning	O
under	O
the	O
i.i.d	O
.	O
assumption	O
,	O
as	O
we	O
show	O
below	O
.	O
maximum	B
likelihood	I
corresponds	O
to	O
counting	B
for	O
a	O
bn	O
there	O
is	O
a	O
constraint	O
on	O
the	O
form	O
of	O
p	O
(	O
x	O
)	O
,	O
namely	O
k	O
(	O
cid:89	O
)	O
i=1	O
p	O
(	O
x	O
)	O
=	O
p	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
draft	O
march	O
9	O
,	O
2010	O
(	O
9.2.15	O
)	O
171	O
maximum	O
a	O
posteriori	O
and	O
maximum	B
likelihood	I
to	O
compute	O
the	O
maximum	B
likelihood	I
setting	O
of	O
each	O
term	O
p	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
,	O
as	O
shown	O
in	O
section	O
(	O
9.2.2	O
)	O
,	O
we	O
can	O
equivalently	O
minimise	O
the	O
kullback-leibler	O
divergence	B
between	O
the	O
empirical	B
distribution	I
q	O
(	O
x	O
)	O
and	O
p	O
(	O
x	O
)	O
.	O
for	O
the	O
bn	O
p	O
(	O
x	O
)	O
,	O
and	O
empirical	B
distribution	I
q	O
(	O
x	O
)	O
we	O
have	O
(	O
cid:42	O
)	O
k	O
(	O
cid:88	O
)	O
i=1	O
(	O
cid:43	O
)	O
log	O
p	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
k	O
(	O
cid:88	O
)	O
i=1	O
+	O
const	O
.	O
=	O
−	O
q	O
(	O
x	O
)	O
kl	O
(	O
q|p	O
)	O
=	O
−	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
xi	O
,	O
pa	O
(	O
xi	O
)	O
)	O
+	O
const	O
.	O
(	O
9.2.16	O
)	O
(	O
9.2.17	O
)	O
this	O
follows	O
using	O
the	O
general	O
result	O
(	O
cid:104	O
)	O
f	O
(	O
xi	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
=	O
(	O
cid:104	O
)	O
f	O
(	O
xi	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
xi	O
)	O
which	O
says	O
that	O
if	O
the	O
function	B
f	O
only	O
depends	O
on	O
a	O
subset	O
of	O
the	O
variables	O
,	O
we	O
only	O
need	O
to	O
know	O
the	O
marginal	B
distribution	O
of	O
this	O
subset	O
of	O
variables	O
in	O
order	O
to	O
carry	O
out	O
the	O
average	B
.	O
since	O
q	O
(	O
x	O
)	O
is	O
ﬁxed	O
,	O
we	O
can	O
add	O
on	O
entropic	O
terms	O
in	O
q	O
and	O
equivalently	O
mimimize	O
(	O
cid:105	O
)	O
(	O
cid:104	O
)	O
k	O
(	O
cid:88	O
)	O
k	O
(	O
cid:88	O
)	O
i=1	O
i=1	O
kl	O
(	O
q|p	O
)	O
=	O
=	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
xi	O
,	O
pa	O
(	O
xi	O
)	O
)	O
−	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
xi	O
,	O
pa	O
(	O
xi	O
)	O
)	O
(	O
cid:104	O
)	O
kl	O
(	O
q	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
|p	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
pa	O
(	O
xi	O
)	O
)	O
(	O
9.2.18	O
)	O
(	O
9.2.19	O
)	O
the	O
ﬁnal	O
line	O
is	O
a	O
positive	O
weighted	O
sum	O
of	O
individual	O
kullback-leibler	O
divergences	O
.	O
the	O
minimal	O
kullback-	O
leibler	O
setting	O
,	O
and	O
that	O
which	O
corresponds	O
to	O
maximum	B
likelihood	I
,	O
is	O
therefore	O
in	O
terms	O
of	O
the	O
original	O
data	B
,	O
this	O
is	O
p	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
=	O
q	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
n	O
(	O
cid:88	O
)	O
p	O
(	O
xi	O
=	O
s|pa	O
(	O
xi	O
)	O
=	O
t	O
)	O
∝	O
n=1	O
i	O
=	O
s	O
]	O
(	O
cid:89	O
)	O
i	O
[	O
xn	O
xj∈pa	O
(	O
xi	O
)	O
i	O
(	O
cid:2	O
)	O
xn	O
j	O
=	O
tj	O
(	O
cid:3	O
)	O
(	O
9.2.20	O
)	O
(	O
9.2.21	O
)	O
this	O
expression	O
corresponds	O
to	O
the	O
intuition	O
that	O
the	O
table	O
entry	O
p	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
can	O
be	O
set	O
by	O
counting	B
the	O
number	O
of	O
times	O
the	O
state	O
{	O
xi	O
=	O
s	O
,	O
pa	O
(	O
xi	O
)	O
=	O
t	O
}	O
occurs	O
in	O
the	O
dataset	O
(	O
where	O
t	O
is	O
a	O
vector	O
of	O
parental	O
states	O
)	O
.	O
the	O
table	O
is	O
then	O
given	O
by	O
the	O
relative	O
number	O
of	O
counts	O
of	O
being	O
in	O
state	O
s	O
compared	O
to	O
the	O
other	O
states	O
s	O
(	O
cid:48	O
)	O
,	O
for	O
ﬁxed	O
joint	B
parental	O
state	O
t.	O
an	O
alternative	O
method	O
to	O
derive	O
this	O
intuitive	O
result	O
is	O
to	O
use	O
lagrange	O
multipliers	O
,	O
see	O
exercise	O
(	O
120	O
)	O
.	O
for	O
reader	O
less	O
comfortable	O
with	O
the	O
above	O
kullback-leibler	O
derivation	O
,	O
a	O
more	O
direct	O
example	O
is	O
given	O
below	O
which	O
makes	O
use	O
of	O
the	O
notation	O
(	O
cid:93	O
)	O
(	O
x1	O
=	O
s1	O
,	O
x2	O
=	O
s2	O
,	O
x3	O
=	O
s3	O
,	O
.	O
.	O
.	O
)	O
(	O
9.2.22	O
)	O
to	O
denote	O
the	O
number	O
of	O
times	O
that	O
states	O
x1	O
=	O
s1	O
,	O
x2	O
=	O
s2	O
,	O
x3	O
=	O
s3	O
,	O
.	O
.	O
.	O
occur	O
together	O
in	O
the	O
training	B
data	O
.	O
example	O
37.	O
we	O
wish	O
to	O
learn	O
the	O
table	O
entries	O
of	O
the	O
distribution	B
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
=	O
p	O
(	O
x1|x2	O
,	O
x3	O
)	O
p	O
(	O
x2	O
)	O
p	O
(	O
x3	O
)	O
.	O
we	O
address	O
here	O
how	O
to	O
ﬁnd	O
the	O
cpt	O
entry	O
p	O
(	O
x1	O
=	O
1|x2	O
=	O
1	O
,	O
x3	O
=	O
0	O
)	O
using	O
maximum	B
likelihood	I
.	O
for	O
i.i.d	O
.	O
data	B
,	O
the	O
contribution	O
from	O
p	O
(	O
x1|x2	O
,	O
x3	O
)	O
to	O
the	O
log	O
likelihood	B
is	O
(	O
cid:88	O
)	O
n	O
log	O
p	O
(	O
xn	O
2	O
,	O
xn	O
3	O
)	O
1|xn	O
the	O
number	O
of	O
times	O
p	O
(	O
x1	O
=	O
1|x2	O
=	O
1	O
,	O
x3	O
=	O
0	O
)	O
occurs	O
in	O
the	O
log	O
likelihood	B
is	O
(	O
cid:93	O
)	O
(	O
x1	O
=	O
1	O
,	O
x2	O
=	O
1	O
,	O
x3	O
=	O
0	O
)	O
,	O
the	O
number	O
of	O
such	O
occurrences	O
in	O
the	O
training	B
set	O
.	O
since	O
(	O
by	O
the	O
normalisation	B
constraint	O
)	O
p	O
(	O
x1	O
=	O
0|x2	O
=	O
172	O
draft	O
march	O
9	O
,	O
2010	O
maximum	O
a	O
posteriori	O
and	O
maximum	B
likelihood	I
xn−1	O
xn	O
x1	O
x2	O
···	O
y	O
figure	O
9.6	O
:	O
a	O
variable	B
y	O
with	O
a	O
large	O
number	O
of	O
parents	B
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
requires	O
the	O
speciﬁcation	O
of	O
an	O
exponentially	O
large	O
number	O
of	O
entries	O
in	O
the	O
con-	O
ditional	O
probability	B
p	O
(	O
y|x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
.	O
one	O
solution	O
to	O
this	O
diﬃculty	O
is	O
to	O
parameterise	O
the	O
conditional	B
,	O
p	O
(	O
y|x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
θ	O
)	O
.	O
1	O
,	O
x3	O
=	O
0	O
)	O
=	O
1	O
−	O
p	O
(	O
x1	O
=	O
1|x2	O
=	O
1	O
,	O
x3	O
=	O
0	O
)	O
,	O
the	O
total	O
contribution	O
of	O
p	O
(	O
x1	O
=	O
1|x2	O
=	O
1	O
,	O
x3	O
=	O
0	O
)	O
to	O
the	O
log	O
likelihood	B
is	O
(	O
cid:93	O
)	O
(	O
x1	O
=	O
1	O
,	O
x2	O
=	O
1	O
,	O
x3	O
=	O
0	O
)	O
log	O
p	O
(	O
x1	O
=	O
1|x2	O
=	O
1	O
,	O
x3	O
=	O
0	O
)	O
+	O
(	O
cid:93	O
)	O
(	O
x1	O
=	O
0	O
,	O
x2	O
=	O
1	O
,	O
x3	O
=	O
0	O
)	O
log	O
(	O
1	O
−	O
p	O
(	O
x1	O
=	O
1|x2	O
=	O
1	O
,	O
x3	O
=	O
0	O
)	O
)	O
using	O
θ	O
≡	O
p	O
(	O
x1	O
=	O
1|x2	O
=	O
1	O
,	O
x3	O
=	O
0	O
)	O
we	O
have	O
(	O
cid:93	O
)	O
(	O
x1	O
=	O
1	O
,	O
x2	O
=	O
1	O
,	O
x3	O
=	O
0	O
)	O
log	O
θ	O
+	O
(	O
cid:93	O
)	O
(	O
x1	O
=	O
0	O
,	O
x2	O
=	O
1	O
,	O
x3	O
=	O
0	O
)	O
log	O
(	O
1	O
−	O
θ	O
)	O
diﬀerentiating	O
the	O
above	O
expression	O
w.r.t	O
.	O
θ	O
and	O
equating	O
to	O
zero	O
gives	O
(	O
cid:93	O
)	O
(	O
x1	O
=	O
1	O
,	O
x2	O
=	O
1	O
,	O
x3	O
=	O
0	O
)	O
−	O
the	O
solution	O
for	O
optimal	O
θ	O
is	O
then	O
θ	O
(	O
cid:93	O
)	O
(	O
x1	O
=	O
0	O
,	O
x2	O
=	O
1	O
,	O
x3	O
=	O
0	O
)	O
=	O
0	O
1	O
−	O
θ	O
p	O
(	O
x1	O
=	O
1|x2	O
=	O
1	O
,	O
x3	O
=	O
0	O
)	O
=	O
(	O
cid:93	O
)	O
(	O
x1	O
=	O
1	O
,	O
x2	O
=	O
1	O
,	O
x3	O
=	O
0	O
)	O
(	O
cid:93	O
)	O
(	O
x1	O
=	O
1	O
,	O
x2	O
=	O
1	O
,	O
x3	O
=	O
0	O
)	O
+	O
(	O
cid:93	O
)	O
(	O
x1	O
=	O
0	O
,	O
x2	O
=	O
1	O
,	O
x3	O
=	O
0	O
)	O
,	O
corresponding	O
to	O
the	O
intuitive	O
counting	B
procedure	O
.	O
(	O
9.2.23	O
)	O
(	O
9.2.24	O
)	O
(	O
9.2.25	O
)	O
(	O
9.2.26	O
)	O
conditional	B
probability	I
functions	O
consider	O
a	O
binary	O
variable	O
y	O
with	O
n	O
binary	O
parental	O
variables	O
,	O
x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
.	O
there	O
are	O
2n	O
entries	O
in	O
the	O
cpt	O
of	O
p	O
(	O
y|x	O
)	O
so	O
that	O
it	O
is	O
infeasible	O
to	O
explicitly	O
store	O
these	O
entries	O
for	O
even	O
moderate	O
values	O
of	O
n.	O
to	O
reduce	O
the	O
complexity	O
of	O
this	O
cpt	O
we	O
may	O
constrain	O
the	O
form	O
of	O
the	O
table	O
.	O
for	O
example	O
,	O
one	O
could	O
use	O
a	O
function	B
p	O
(	O
y	O
=	O
1|x	O
,	O
w	O
)	O
=	O
1	O
1	O
+	O
e−wtx	O
where	O
we	O
only	O
need	O
to	O
specify	O
the	O
n-dimensional	O
parameter	B
vector	O
w.	O
(	O
9.2.27	O
)	O
in	O
this	O
case	O
,	O
rather	O
than	O
using	O
maximum	B
likelihood	I
to	O
learn	O
the	O
entries	O
of	O
the	O
cpts	O
directly	O
,	O
we	O
instead	O
learn	O
the	O
value	B
of	O
the	O
parameter	B
w.	O
since	O
the	O
number	O
of	O
parameters	O
in	O
w	O
is	O
small	O
(	O
n	O
,	O
compared	O
with	O
2n	O
in	O
the	O
unconstrained	O
case	O
)	O
,	O
we	O
also	O
have	O
some	O
hope	O
that	O
with	O
a	O
small	O
number	O
of	O
training	B
examples	O
we	O
can	O
learn	O
a	O
reliable	O
value	B
for	O
w.	O
example	O
38.	O
consider	O
the	O
following	O
3	O
variable	B
model	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
=	O
p	O
(	O
x1|x2	O
,	O
x3	O
)	O
p	O
(	O
x2	O
)	O
p	O
(	O
x3	O
)	O
,	O
where	O
xi	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
i	O
=	O
1	O
,	O
2	O
,	O
3.	O
we	O
assume	O
that	O
the	O
cpt	O
is	O
parameterised	O
using	O
p	O
(	O
x1	O
=	O
1|x2	O
,	O
x3	O
,	O
θ	O
)	O
≡	O
e	O
−θ2	O
1−θ2	O
2	O
(	O
x2−x3	O
)	O
2	O
(	O
9.2.28	O
)	O
one	O
may	O
verify	O
that	O
the	O
above	O
probability	B
is	O
always	O
positive	O
and	O
lies	O
between	O
0	O
and	O
1.	O
due	O
to	O
normalisation	B
,	O
we	O
must	O
have	O
p	O
(	O
x1	O
=	O
0|x2	O
,	O
x3	O
)	O
=	O
1	O
−	O
p	O
(	O
x1	O
=	O
1|x2	O
,	O
x3	O
)	O
draft	O
march	O
9	O
,	O
2010	O
(	O
9.2.29	O
)	O
173	O
bayesian	O
belief	B
network	I
training	O
for	O
unrestricted	O
p	O
(	O
x2	O
)	O
and	O
p	O
(	O
x3	O
)	O
,	O
the	O
maximum	B
likelihood	I
setting	O
is	O
p	O
(	O
x2	O
=	O
1	O
)	O
∝	O
(	O
cid:93	O
)	O
(	O
x2	O
=	O
1	O
)	O
,	O
and	O
p	O
(	O
x3	O
=	O
1	O
)	O
∝	O
(	O
cid:93	O
)	O
(	O
x3	O
=	O
1	O
)	O
.	O
the	O
contribution	O
to	O
the	O
log	O
likelihood	B
from	O
the	O
term	O
p	O
(	O
x1|x2	O
,	O
x3	O
,	O
θ	O
)	O
,	O
assuming	O
i.i.d	O
.	O
data	B
,	O
is	O
1	O
=	O
1	O
]	O
(	O
cid:0	O
)	O
i	O
[	O
xn	O
n	O
(	O
cid:88	O
)	O
n=1	O
3	O
)	O
2	O
(	O
cid:1	O
)	O
+	O
i	O
[	O
xn	O
(	O
cid:16	O
)	O
−θ2	O
1	O
−	O
θ2	O
2	O
(	O
xn	O
2	O
−	O
xn	O
1	O
=	O
0	O
]	O
log	O
1	O
−	O
e	O
3	O
)	O
2	O
(	O
cid:17	O
)	O
−θ2	O
1−θ2	O
2	O
−xn	O
2	O
(	O
xn	O
(	O
9.2.30	O
)	O
this	O
objective	O
function	B
needs	O
to	O
be	O
optimised	O
numerically	O
to	O
ﬁnd	O
the	O
best	O
θ1	O
and	O
θ2	O
.	O
the	O
gradient	B
is	O
l	O
(	O
θ1	O
,	O
θ2	O
)	O
=	O
n	O
(	O
cid:88	O
)	O
n	O
(	O
cid:88	O
)	O
n=1	O
n=1	O
dl	O
dθ1	O
=	O
dl	O
dθ2	O
=	O
−2i	O
[	O
xn	O
1	O
=	O
1	O
]	O
θ1	O
+	O
2i	O
[	O
xn	O
−2i	O
[	O
xn	O
1	O
=	O
1	O
]	O
θ2	O
(	O
xn	O
2	O
−	O
xn	O
2	O
−xn	O
3	O
)	O
2	O
2	O
−xn	O
3	O
)	O
2	O
2	O
(	O
xn	O
2	O
(	O
xn	O
1	O
=	O
0	O
]	O
θ1e−θ2	O
1−θ2	O
1−θ2	O
1	O
−	O
e−θ2	O
3	O
)	O
2	O
+	O
2θ2i	O
[	O
xn	O
1	O
=	O
0	O
]	O
(	O
xn	O
3	O
)	O
2e−θ2	O
1−θ2	O
2	O
−	O
xn	O
1	O
−	O
e−θ2	O
1−θ2	O
2	O
(	O
xn	O
2	O
−xn	O
2	O
(	O
xn	O
3	O
)	O
2	O
2	O
−xn	O
3	O
)	O
2	O
(	O
9.2.31	O
)	O
(	O
9.2.32	O
)	O
the	O
gradient	B
can	O
be	O
used	O
as	O
part	O
of	O
a	O
standard	O
optimisation	O
procedure	O
(	O
such	O
as	O
conjugate	O
gradients	O
,	O
see	O
appendix	O
(	O
a	O
)	O
)	O
to	O
aid	O
ﬁnding	O
the	O
maximum	B
likelihood	I
parameters	O
θ1	O
,	O
θ2	O
.	O
9.3	O
bayesian	O
belief	B
network	I
training	O
an	O
alternative	O
to	O
maximum	B
likelihood	I
training	O
of	O
a	O
bn	O
is	O
to	O
use	O
a	O
bayesian	O
approach	B
in	O
which	O
we	O
maintain	O
a	O
distribution	B
over	O
parameters	O
.	O
we	O
continue	O
with	O
the	O
asbestos	O
,	O
smoking	O
,	O
cancer	O
scenario	O
,	O
p	O
(	O
a	O
,	O
c	O
,	O
s	O
)	O
=	O
p	O
(	O
c|a	O
,	O
s	O
)	O
p	O
(	O
a	O
)	O
p	O
(	O
s	O
)	O
(	O
9.3.1	O
)	O
which	O
can	O
be	O
represented	O
as	O
a	O
belief	B
network	I
,	O
ﬁg	O
(	O
9.4a	O
)	O
.	O
so	O
far	O
we	O
’	O
ve	O
only	O
speciﬁed	O
the	O
independence	B
structure	O
,	O
but	O
not	O
the	O
entries	O
of	O
the	O
tables	O
p	O
(	O
c|a	O
,	O
s	O
)	O
,	O
p	O
(	O
a	O
)	O
,	O
p	O
(	O
s	O
)	O
.	O
given	O
a	O
set	O
of	O
visible	B
observations	O
,	O
v	O
=	O
{	O
(	O
an	O
,	O
sn	O
,	O
cn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
,	O
we	O
would	O
like	O
to	O
learn	O
appropriate	O
distributions	O
for	O
the	O
table	O
entries	O
.	O
to	O
begin	O
we	O
need	O
a	O
notation	O
for	O
the	O
table	O
entries	O
.	O
with	O
all	O
variables	O
binary	O
we	O
have	O
parameters	O
such	O
as	O
p	O
(	O
a	O
=	O
1|θa	O
)	O
=	O
θa	O
,	O
p	O
(	O
c	O
=	O
1|a	O
=	O
0	O
,	O
s	O
=	O
1	O
,	O
θc	O
)	O
=	O
θ0,1	O
c	O
,	O
θ1,0	O
and	O
similarly	O
for	O
the	O
remaining	O
parameters	O
θ1,1	O
,	O
θ0,0	O
c	O
c	O
c	O
θa	O
,	O
θs	O
,	O
θ0,0	O
c	O
,	O
θ0,1	O
c	O
,	O
θ1,0	O
c	O
,	O
θ1,1	O
c	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
θc	O
(	O
cid:125	O
)	O
.	O
for	O
our	O
example	O
,	O
the	O
parameters	O
are	O
(	O
9.3.2	O
)	O
(	O
9.3.3	O
)	O
9.3.1	O
global	B
and	O
local	B
parameter	O
independence	B
in	O
bayesian	O
learning	B
of	O
bns	O
,	O
we	O
need	O
to	O
specify	O
a	O
prior	B
on	O
the	O
joint	B
table	O
entries	O
.	O
since	O
in	O
general	O
dealing	O
with	O
multi-dimensional	O
continuous	B
distributions	O
is	O
computationally	O
problematic	O
,	O
it	O
is	O
useful	O
to	O
specify	O
only	O
uni-variate	O
distributions	O
in	O
the	O
prior	B
.	O
as	O
we	O
show	O
below	O
,	O
this	O
has	O
a	O
pleasing	O
consequence	O
that	O
for	O
i.i.d	O
.	O
data	B
the	O
posterior	B
also	O
factorises	O
into	O
uni-variate	O
distributions	O
.	O
global	B
parameter	O
independence	B
a	O
convenient	O
assumption	O
is	O
that	O
the	O
prior	B
factorises	O
over	O
parameters	O
.	O
for	O
our	O
asbestos	O
,	O
smoking	O
,	O
cancer	O
example	O
,	O
we	O
assume	O
assuming	O
the	O
data	B
is	O
i.i.d.	O
,	O
we	O
then	O
have	O
the	O
joint	B
model	O
p	O
(	O
θa	O
,	O
θs	O
,	O
θc	O
)	O
=	O
p	O
(	O
θa	O
)	O
p	O
(	O
θs	O
)	O
p	O
(	O
θc	O
)	O
p	O
(	O
θa	O
,	O
θs	O
,	O
θc	O
,	O
v	O
)	O
=	O
p	O
(	O
θa	O
)	O
p	O
(	O
θs	O
)	O
p	O
(	O
θc	O
)	O
(	O
cid:89	O
)	O
n	O
p	O
(	O
an|θa	O
)	O
p	O
(	O
sn|θs	O
)	O
p	O
(	O
cn|sn	O
,	O
an	O
,	O
θc	O
)	O
(	O
9.3.4	O
)	O
(	O
9.3.5	O
)	O
174	O
draft	O
march	O
9	O
,	O
2010	O
bayesian	O
belief	B
network	I
training	O
θa	O
an	O
θs	O
sn	O
cn	O
n	O
=	O
1	O
:	O
n	O
θa	O
,	O
s	O
c	O
(	O
a	O
,	O
s	O
)	O
∈	O
p	O
figure	O
9.7	O
:	O
a	O
bayesian	O
parameter	B
model	O
for	O
the	O
relationship	O
between	O
lung	O
cancer	O
,	O
asbestos	O
exposure	O
and	O
smoking	O
with	O
factorised	O
parameter	B
pri-	O
ors	O
.	O
the	O
global	B
parameter	O
independence	B
assumption	O
means	O
that	O
the	O
prior	B
over	O
tables	O
factorises	O
into	O
priors	O
over	O
each	O
conditional	B
probability	I
table	O
.	O
the	O
local	B
independence	O
assumption	O
,	O
which	O
in	O
this	O
case	O
comes	O
into	O
ef-	O
c	O
)	O
,	O
where	O
fect	O
only	O
for	O
p	O
(	O
c|a	O
,	O
s	O
)	O
,	O
means	O
that	O
p	O
(	O
θc	O
)	O
factorises	O
in	O
(	O
cid:81	O
)	O
a	O
,	O
s∈p	O
p	O
(	O
θa	O
,	O
s	O
p	O
=	O
{	O
(	O
0	O
,	O
0	O
)	O
,	O
(	O
0	O
,	O
1	O
)	O
,	O
(	O
1	O
,	O
0	O
)	O
,	O
(	O
1	O
,	O
1	O
)	O
}	O
.	O
the	O
belief	B
network	I
for	O
which	O
is	O
given	O
in	O
ﬁg	O
(	O
9.7	O
.	O
)	O
learning	B
then	O
corresponds	O
to	O
inference	B
of	O
a	O
convenience	O
of	O
the	O
factorised	B
prior	O
for	O
a	O
bn	O
is	O
that	O
the	O
posterior	B
also	O
factorises	O
,	O
since	O
=	O
p	O
(	O
v|θa	O
,	O
θs	O
,	O
θc	O
)	O
p	O
(	O
θa	O
)	O
p	O
(	O
θs	O
)	O
p	O
(	O
θc	O
)	O
p	O
(	O
v	O
)	O
p	O
(	O
θa	O
,	O
θs	O
,	O
θc|v	O
)	O
=	O
p	O
(	O
v|θa	O
,	O
θs	O
,	O
θc	O
)	O
p	O
(	O
θa	O
,	O
θs	O
,	O
θc	O
)	O
(	O
cid:41	O
)	O
(	O
cid:40	O
)	O
p	O
(	O
θs	O
)	O
(	O
cid:89	O
)	O
p	O
(	O
θa	O
,	O
θs	O
,	O
θc|v	O
)	O
∝	O
p	O
(	O
θa	O
,	O
θs	O
,	O
θc	O
,	O
v	O
)	O
(	O
cid:40	O
)	O
p	O
(	O
θa	O
)	O
(	O
cid:89	O
)	O
p	O
(	O
an|θa	O
)	O
∝	O
=	O
p	O
(	O
θa|va	O
)	O
p	O
(	O
θs|vs	O
)	O
p	O
(	O
θc|v	O
)	O
n	O
p	O
(	O
v	O
)	O
(	O
cid:41	O
)	O
(	O
cid:40	O
)	O
p	O
(	O
θc	O
)	O
(	O
cid:89	O
)	O
p	O
(	O
sn|θs	O
)	O
n	O
n	O
p	O
(	O
cn|sn	O
,	O
an	O
,	O
θc	O
)	O
(	O
cid:41	O
)	O
(	O
9.3.6	O
)	O
(	O
9.3.7	O
)	O
so	O
that	O
one	O
can	O
consider	O
each	O
parameter	B
posterior	O
separately	O
.	O
in	O
this	O
case	O
,	O
‘	O
learning	B
’	O
involves	O
computing	O
the	O
posterior	B
distributions	O
p	O
(	O
θi|vi	O
)	O
where	O
vi	O
is	O
the	O
set	O
of	O
training	B
data	O
restricted	B
to	O
the	O
family	B
of	O
variable	B
i.	O
the	O
global	B
independence	O
assumption	O
conveniently	O
results	O
in	O
a	O
posterior	B
distribution	O
that	O
factorises	O
over	O
the	O
conditional	B
tables	O
.	O
however	O
,	O
the	O
parameter	B
θc	O
is	O
itself	O
4	O
dimensional	O
.	O
to	O
simplify	O
this	O
we	O
need	O
to	O
make	O
a	O
further	O
assumption	O
as	O
to	O
the	O
structure	B
of	O
each	O
local	B
table	O
.	O
local	B
parameter	O
independence	B
if	O
we	O
further	O
assume	O
that	O
the	O
prior	B
for	O
the	O
table	O
factorises	O
over	O
all	O
states	O
a	O
,	O
c	O
:	O
p	O
(	O
θc	O
)	O
=	O
p	O
(	O
θ0,0	O
c	O
)	O
p	O
(	O
θ1,0	O
c	O
)	O
p	O
(	O
θ0,1	O
c	O
)	O
p	O
(	O
θ1,1	O
c	O
)	O
(	O
9.3.8	O
)	O
then	O
the	O
posterior	B
(	O
cid:3	O
)	O
(	O
cid:93	O
)	O
(	O
a=0	O
,	O
s=0	O
)	O
p	O
(	O
θc|v	O
)	O
∝	O
p	O
(	O
v|θc	O
)	O
p	O
(	O
θ0,0	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:2	O
)	O
θ0,0	O
(	O
cid:124	O
)	O
=	O
∝	O
c	O
(	O
cid:2	O
)	O
θ0,1	O
c	O
)	O
p	O
(	O
θ0,1	O
c	O
)	O
p	O
(	O
θ1,1	O
c	O
)	O
p	O
(	O
θ0,0	O
c	O
)	O
∝	O
(	O
cid:3	O
)	O
(	O
cid:93	O
)	O
(	O
a=0	O
,	O
s=1	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
(	O
cid:124	O
)	O
c	O
c	O
)	O
p	O
(	O
θ1,0	O
p	O
(	O
θ0,0	O
c	O
|v	O
)	O
p	O
(	O
θ0,1	O
c	O
|v	O
)	O
(	O
cid:2	O
)	O
θ1,0	O
c	O
(	O
cid:124	O
)	O
∝	O
p	O
(	O
θ0,1	O
c	O
)	O
(	O
cid:125	O
)	O
(	O
cid:3	O
)	O
(	O
cid:93	O
)	O
(	O
a=1	O
,	O
s=0	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
p	O
(	O
θ1,0	O
c	O
|v	O
)	O
(	O
cid:2	O
)	O
θ1,1	O
c	O
(	O
cid:124	O
)	O
∝	O
p	O
(	O
θ1,0	O
c	O
)	O
(	O
cid:125	O
)	O
(	O
9.3.9	O
)	O
(	O
cid:3	O
)	O
(	O
cid:93	O
)	O
(	O
a=1	O
,	O
s=1	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
|v	O
)	O
c	O
p	O
(	O
θ1,1	O
(	O
9.3.10	O
)	O
p	O
(	O
θ1,1	O
c	O
)	O
so	O
that	O
the	O
posterior	B
also	O
factorises	O
over	O
the	O
parental	O
states	O
of	O
the	O
local	B
conditional	O
table	O
.	O
posterior	B
marginal	O
table	O
a	O
marginal	B
probability	O
table	O
is	O
given	O
by	O
,	O
for	O
example	O
,	O
(	O
cid:90	O
)	O
p	O
(	O
c	O
=	O
1|a	O
=	O
1	O
,	O
s	O
=	O
0	O
,	O
v	O
)	O
=	O
draft	O
march	O
9	O
,	O
2010	O
θc	O
p	O
(	O
c	O
=	O
1|a	O
=	O
1	O
,	O
s	O
=	O
0	O
,	O
θ1,0	O
c	O
)	O
p	O
(	O
θc|v	O
)	O
(	O
9.3.11	O
)	O
175	O
bayesian	O
belief	B
network	I
training	O
(	O
cid:90	O
)	O
the	O
integral	O
over	O
all	O
the	O
other	O
tables	O
in	O
equation	B
(	O
9.3.11	O
)	O
is	O
unity	O
,	O
and	O
we	O
are	O
left	O
with	O
p	O
(	O
c	O
=	O
1|a	O
=	O
1	O
,	O
s	O
=	O
0	O
,	O
v	O
)	O
=	O
p	O
(	O
c	O
=	O
1|a	O
=	O
1	O
,	O
s	O
=	O
0	O
,	O
θ1,0	O
c	O
)	O
p	O
(	O
θ1,0	O
c	O
|v	O
)	O
(	O
9.3.12	O
)	O
θ1,0	O
c	O
9.3.2	O
learning	B
binary	O
variable	B
tables	O
using	O
a	O
beta	B
prior	O
we	O
continue	O
the	O
example	O
of	O
section	O
(	O
9.3.1	O
)	O
where	O
all	O
variables	O
are	O
binary	O
,	O
but	O
using	O
a	O
continuous	B
valued	O
table	O
prior	B
.	O
the	O
simplest	O
case	O
is	O
to	O
start	O
with	O
p	O
(	O
a|θa	O
)	O
since	O
this	O
requires	O
only	O
a	O
univariate	B
prior	O
distribution	B
p	O
(	O
θa	O
)	O
.	O
the	O
likelihood	B
depends	O
on	O
the	O
table	O
variable	B
via	O
p	O
(	O
a	O
=	O
1|θa	O
)	O
=	O
θa	O
so	O
that	O
the	O
total	O
likelihood	B
term	O
is	O
θ	O
(	O
cid:93	O
)	O
(	O
a=1	O
)	O
a	O
(	O
1	O
−	O
θa	O
)	O
(	O
cid:93	O
)	O
(	O
a=0	O
)	O
the	O
posterior	B
is	O
therefore	O
(	O
9.3.13	O
)	O
(	O
9.3.14	O
)	O
p	O
(	O
θa|va	O
)	O
∝	O
p	O
(	O
θa	O
)	O
θ	O
(	O
cid:93	O
)	O
(	O
a=1	O
)	O
(	O
9.3.15	O
)	O
a	O
(	O
1	O
−	O
θa	O
)	O
β	O
then	O
conjugacy	O
will	O
hold	O
,	O
and	O
the	O
mathematics	O
this	O
means	O
that	O
if	O
the	O
prior	B
is	O
also	O
of	O
the	O
form	O
θα	O
of	O
integration	O
will	O
be	O
straightforward	O
.	O
this	O
suggests	O
that	O
the	O
most	O
convenient	O
choice	O
is	O
a	O
beta	B
distribution	O
,	O
(	O
1	O
−	O
θa	O
)	O
(	O
cid:93	O
)	O
(	O
a=0	O
)	O
a	O
p	O
(	O
θa	O
)	O
=	O
b	O
(	O
θa|αa	O
,	O
βa	O
)	O
=	O
1	O
b	O
(	O
αa	O
,	O
βa	O
)	O
θαa−1	O
a	O
(	O
1	O
−	O
θa	O
)	O
βa−1	O
for	O
which	O
the	O
posterior	B
is	O
also	O
a	O
beta	B
distribution	O
:	O
p	O
(	O
θa|va	O
)	O
=	O
b	O
(	O
θa|αa	O
+	O
(	O
cid:93	O
)	O
(	O
a	O
=	O
1	O
)	O
,	O
βa	O
+	O
(	O
cid:93	O
)	O
(	O
a	O
=	O
0	O
)	O
)	O
the	O
marginal	B
table	O
is	O
given	O
by	O
(	O
cid:90	O
)	O
p	O
(	O
a	O
=	O
1|va	O
)	O
=	O
θa	O
p	O
(	O
θa|va	O
)	O
θa	O
=	O
αa	O
+	O
(	O
cid:93	O
)	O
(	O
a	O
=	O
1	O
)	O
αa	O
+	O
(	O
cid:93	O
)	O
(	O
a	O
=	O
1	O
)	O
+	O
βa	O
+	O
(	O
cid:93	O
)	O
(	O
a	O
=	O
0	O
)	O
(	O
9.3.16	O
)	O
(	O
9.3.17	O
)	O
(	O
9.3.18	O
)	O
using	O
the	O
result	O
for	O
the	O
mean	B
of	O
a	O
beta	B
distribution	O
,	O
deﬁnition	O
(	O
72	O
)	O
.	O
the	O
situation	O
for	O
the	O
table	O
p	O
(	O
c|a	O
,	O
s	O
)	O
is	O
slightly	O
more	O
complex	O
since	O
we	O
need	O
to	O
specify	O
a	O
prior	B
for	O
each	O
of	O
the	O
parental	O
tables	O
.	O
as	O
above	O
,	O
this	O
is	O
most	O
convenient	O
if	O
we	O
specify	O
a	O
beta	B
prior	O
,	O
one	O
for	O
each	O
of	O
the	O
(	O
four	O
)	O
parental	O
states	O
.	O
let	O
’	O
s	O
look	O
at	O
a	O
speciﬁc	O
table	O
p	O
(	O
c	O
=	O
1|a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
assuming	O
the	O
local	B
independence	O
property	O
,	O
we	O
have	O
p	O
(	O
θ1,0	O
|αc	O
(	O
a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
+	O
(	O
cid:93	O
)	O
(	O
c	O
=	O
1	O
,	O
a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
,	O
βc	O
(	O
a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
+	O
(	O
cid:93	O
)	O
(	O
c	O
=	O
0	O
,	O
a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
(	O
cid:1	O
)	O
|vc	O
)	O
given	O
by	O
b	O
(	O
cid:0	O
)	O
θ1,0	O
c	O
c	O
as	O
before	O
,	O
the	O
marginal	B
probability	O
table	O
is	O
then	O
given	O
by	O
p	O
(	O
c	O
=	O
1|a	O
=	O
1	O
,	O
s	O
=	O
0	O
,	O
vc	O
)	O
=	O
αc	O
(	O
a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
+	O
(	O
cid:93	O
)	O
(	O
c	O
=	O
1	O
,	O
a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
αc	O
(	O
a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
+	O
βc	O
(	O
a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
+	O
(	O
cid:93	O
)	O
(	O
a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
since	O
(	O
cid:93	O
)	O
(	O
a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
=	O
(	O
cid:93	O
)	O
(	O
c	O
=	O
0	O
,	O
a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
+	O
(	O
cid:93	O
)	O
(	O
c	O
=	O
1	O
,	O
a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
.	O
(	O
9.3.19	O
)	O
(	O
9.3.20	O
)	O
(	O
9.3.21	O
)	O
the	O
prior	B
parameters	O
αc	O
(	O
a	O
,	O
s	O
)	O
are	O
called	O
hyperparameters	O
.	O
if	O
one	O
had	O
no	O
preference	O
,	O
one	O
could	O
set	O
all	O
of	O
the	O
αc	O
(	O
a	O
,	O
s	O
)	O
to	O
be	O
equal	O
to	O
the	O
same	O
value	B
α	O
and	O
similarly	O
for	O
β.	O
a	O
complete	O
ignorance	O
prior	B
would	O
correspond	O
to	O
setting	O
α	O
=	O
β	O
=	O
1	O
,	O
see	O
ﬁg	O
(	O
8.7	O
)	O
.	O
176	O
draft	O
march	O
9	O
,	O
2010	O
bayesian	O
belief	B
network	I
training	O
no	O
data	O
limit	O
n	O
→	O
0	O
in	O
the	O
limit	O
of	O
no	O
data	O
,	O
the	O
marginal	B
probability	O
table	O
corresponds	O
to	O
the	O
prior	B
,	O
which	O
is	O
given	O
in	O
this	O
case	O
by	O
p	O
(	O
c	O
=	O
1|a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
=	O
αc	O
(	O
a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
αc	O
(	O
a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
+	O
βc	O
(	O
a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
(	O
9.3.22	O
)	O
for	O
a	O
ﬂat	O
prior	B
α	O
=	O
β	O
=	O
1	O
for	O
all	O
states	O
a	O
,	O
c	O
,	O
this	O
would	O
give	O
a	O
prior	B
probability	O
of	O
p	O
(	O
c	O
=	O
1|a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
=	O
0.5.	O
inﬁnite	O
data	O
limit	O
n	O
→	O
∞	O
in	O
this	O
limit	O
the	O
marginal	B
probability	O
tables	O
are	O
dominated	O
by	O
the	O
data	B
counts	O
,	O
since	O
these	O
will	O
typically	O
grow	O
in	O
proportion	O
to	O
the	O
size	O
of	O
the	O
dataset	O
.	O
this	O
means	O
that	O
in	O
the	O
inﬁnite	O
(	O
or	O
very	O
large	O
)	O
data	B
limit	O
,	O
p	O
(	O
c	O
=	O
1|a	O
=	O
1	O
,	O
s	O
=	O
0	O
,	O
v	O
)	O
→	O
(	O
cid:93	O
)	O
(	O
c	O
=	O
1	O
,	O
a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
(	O
cid:93	O
)	O
(	O
c	O
=	O
1	O
,	O
a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
+	O
(	O
cid:93	O
)	O
(	O
c	O
=	O
0	O
,	O
a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
(	O
9.3.23	O
)	O
which	O
corresponds	O
to	O
the	O
maximum	B
likelihood	I
solution	O
.	O
this	O
eﬀect	O
that	O
the	O
large	O
data	B
limit	O
of	O
a	O
bayesian	O
procedure	O
corresponds	O
to	O
the	O
maximum	B
likelihood	I
solution	O
is	O
general	O
unless	O
the	O
prior	B
has	O
a	O
pathologically	O
strong	B
eﬀect	O
.	O
example	O
39	O
(	O
asbestos-smoking-cancer	B
)	O
.	O
consider	O
the	O
binary	O
variable	O
network	O
p	O
(	O
c	O
,	O
a	O
,	O
s	O
)	O
=	O
p	O
(	O
c|a	O
,	O
s	O
)	O
p	O
(	O
a	O
)	O
p	O
(	O
s	O
)	O
(	O
9.3.24	O
)	O
the	O
data	B
v	O
is	O
given	O
in	O
ﬁg	O
(	O
9.5	O
)	O
.	O
using	O
a	O
ﬂat	O
beta	B
prior	O
α	O
=	O
β	O
=	O
1	O
for	O
all	O
conditional	B
probability	I
tables	O
,	O
the	O
marginal	B
posterior	O
tables	O
are	O
given	O
by	O
p	O
(	O
a	O
=	O
1|v	O
)	O
=	O
1	O
+	O
(	O
cid:93	O
)	O
(	O
a	O
=	O
1	O
)	O
2	O
+	O
n	O
=	O
1	O
+	O
4	O
2	O
+	O
7	O
=	O
5	O
9	O
≈	O
0.556	O
(	O
9.3.25	O
)	O
by	O
comparison	O
,	O
the	O
maximum	B
likelihood	I
setting	O
is	O
4/7	O
=	O
0.571.	O
the	O
bayesian	O
result	O
is	O
a	O
little	O
more	O
cautious	O
than	O
the	O
maximum	B
likelihood	I
,	O
which	O
squares	O
with	O
our	O
prior	B
belief	O
that	O
any	O
setting	O
of	O
the	O
probability	B
is	O
equally	O
likely	O
,	O
pulling	O
the	O
posterior	B
towards	O
0.5.	O
similarly	O
,	O
p	O
(	O
s	O
=	O
1|v	O
)	O
=	O
1	O
+	O
(	O
cid:93	O
)	O
(	O
s	O
=	O
1	O
)	O
2	O
+	O
n	O
=	O
1	O
+	O
4	O
2	O
+	O
7	O
=	O
5	O
9	O
≈	O
0.556	O
and	O
p	O
(	O
c	O
=	O
1|a	O
=	O
1	O
,	O
s	O
=	O
1	O
,	O
v	O
)	O
=	O
1	O
+	O
(	O
cid:93	O
)	O
(	O
c	O
=	O
1	O
,	O
a	O
=	O
1	O
,	O
s	O
=	O
1	O
)	O
2	O
+	O
(	O
cid:93	O
)	O
(	O
c	O
=	O
1	O
,	O
a	O
=	O
1	O
,	O
s	O
=	O
1	O
)	O
+	O
(	O
cid:93	O
)	O
(	O
c	O
=	O
0	O
,	O
a	O
=	O
1	O
,	O
s	O
=	O
1	O
)	O
p	O
(	O
c	O
=	O
1|a	O
=	O
1	O
,	O
s	O
=	O
0	O
,	O
v	O
)	O
=	O
p	O
(	O
c	O
=	O
1|a	O
=	O
0	O
,	O
s	O
=	O
1	O
,	O
v	O
)	O
=	O
p	O
(	O
c	O
=	O
1|a	O
=	O
0	O
,	O
s	O
=	O
0	O
,	O
v	O
)	O
=	O
1	O
+	O
(	O
cid:93	O
)	O
(	O
c	O
=	O
1	O
,	O
a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
2	O
+	O
(	O
cid:93	O
)	O
(	O
c	O
=	O
1	O
,	O
a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
+	O
(	O
cid:93	O
)	O
(	O
c	O
=	O
0	O
,	O
a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
1	O
+	O
(	O
cid:93	O
)	O
(	O
c	O
=	O
1	O
,	O
a	O
=	O
0	O
,	O
s	O
=	O
1	O
)	O
2	O
+	O
(	O
cid:93	O
)	O
(	O
c	O
=	O
1	O
,	O
a	O
=	O
0	O
,	O
s	O
=	O
1	O
)	O
+	O
(	O
cid:93	O
)	O
(	O
c	O
=	O
0	O
,	O
a	O
=	O
0	O
,	O
s	O
=	O
1	O
)	O
1	O
+	O
(	O
cid:93	O
)	O
(	O
c	O
=	O
1	O
,	O
a	O
=	O
0	O
,	O
s	O
=	O
0	O
)	O
2	O
+	O
(	O
cid:93	O
)	O
(	O
c	O
=	O
1	O
,	O
a	O
=	O
0	O
,	O
s	O
=	O
0	O
)	O
+	O
(	O
cid:93	O
)	O
(	O
c	O
=	O
0	O
,	O
a	O
=	O
0	O
,	O
s	O
=	O
0	O
)	O
=	O
=	O
=	O
=	O
1	O
+	O
2	O
2	O
+	O
2	O
1	O
+	O
1	O
2	O
+	O
1	O
1	O
+	O
1	O
2	O
+	O
2	O
1	O
+	O
0	O
2	O
+	O
1	O
=	O
=	O
=	O
=	O
3	O
4	O
2	O
3	O
1	O
2	O
1	O
3	O
draft	O
march	O
9	O
,	O
2010	O
(	O
9.3.26	O
)	O
(	O
9.3.27	O
)	O
(	O
9.3.28	O
)	O
(	O
9.3.29	O
)	O
(	O
9.3.30	O
)	O
177	O
bayesian	O
belief	B
network	I
training	O
9.3.3	O
learning	B
multivariate	O
discrete	B
tables	O
using	O
a	O
dirichlet	O
prior	B
the	O
natural	B
generalisation	O
to	O
more	O
than	O
two-state	O
variables	O
is	O
given	O
by	O
using	O
a	O
dirichlet	O
prior	B
,	O
again	O
assuming	O
i.i.d	O
.	O
data	B
and	O
the	O
local	B
and	O
global	B
parameter	O
prior	B
independencies	O
.	O
since	O
under	O
the	O
global	B
parameter	O
independence	B
assumption	O
the	O
posterior	B
factorises	O
over	O
variables	O
(	O
as	O
in	O
equation	B
(	O
9.3.7	O
)	O
)	O
,	O
we	O
can	O
concentrate	O
on	O
the	O
posterior	B
of	O
a	O
single	O
variable	B
.	O
no	O
parents	O
let	O
’	O
s	O
consider	O
the	O
contribution	O
of	O
a	O
variable	B
v	O
with	O
dom	O
(	O
v	O
)	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
i	O
}	O
.	O
the	O
contribution	O
to	O
the	O
posterior	B
from	O
a	O
datapoint	O
vn	O
is	O
i	O
[	O
vn=i	O
]	O
i	O
,	O
θ	O
θi	O
=	O
1	O
i	O
(	O
cid:88	O
)	O
i=1	O
i	O
(	O
cid:89	O
)	O
i=1	O
p	O
(	O
vn|θ	O
)	O
=	O
i	O
(	O
cid:89	O
)	O
n	O
(	O
cid:89	O
)	O
p	O
(	O
θ	O
)	O
so	O
that	O
the	O
posterior	B
is	O
proportional	O
to	O
i	O
(	O
cid:89	O
)	O
(	O
cid:80	O
)	O
n	O
n=1	O
θ	O
i	O
i	O
[	O
vn=i	O
]	O
i	O
[	O
vn=i	O
]	O
i	O
θ	O
=	O
p	O
(	O
θ	O
)	O
n=1	O
i=1	O
i=1	O
for	O
a	O
dirichlet	O
prior	B
distribution	O
with	O
hyperparameters	O
u	O
i	O
(	O
cid:89	O
)	O
i=1	O
p	O
(	O
θ	O
)	O
∝	O
θui−1	O
i	O
using	O
this	O
prior	B
the	O
posterior	B
becomes	O
i	O
(	O
cid:89	O
)	O
i	O
(	O
cid:89	O
)	O
(	O
cid:80	O
)	O
n	O
n=1	O
θ	O
i	O
θui−1	O
i	O
i=1	O
i=1	O
i	O
[	O
vn=i	O
]	O
=	O
ui−1+	O
(	O
cid:80	O
)	O
n	O
i	O
[	O
vn=i	O
]	O
n=1	O
i	O
(	O
cid:89	O
)	O
i=1	O
θ	O
i	O
p	O
(	O
θ|v	O
)	O
∝	O
which	O
means	O
that	O
the	O
posterior	B
is	O
given	O
by	O
p	O
(	O
θ|v	O
)	O
=	O
dirichlet	O
(	O
θ|u	O
+	O
c	O
)	O
where	O
c	O
is	O
a	O
count	O
vector	O
with	O
components	O
n	O
(	O
cid:88	O
)	O
ci	O
=	O
i	O
[	O
vn	O
=	O
i	O
]	O
n=1	O
being	O
the	O
number	O
of	O
times	O
state	O
i	O
was	O
observed	O
in	O
the	O
training	B
data	O
.	O
the	O
marginal	B
table	O
is	O
given	O
by	O
integrating	O
(	O
cid:90	O
)	O
p	O
(	O
v	O
=	O
i|v	O
)	O
=	O
p	O
(	O
v	O
=	O
i|θ	O
)	O
p	O
(	O
θ|v	O
)	O
=	O
θ	O
θip	O
(	O
θi|v	O
)	O
θi	O
(	O
cid:90	O
)	O
(	O
cid:80	O
)	O
p	O
(	O
v	O
=	O
i|v	O
)	O
=	O
ui	O
+	O
ci	O
j	O
uj	O
+	O
cj	O
(	O
9.3.31	O
)	O
(	O
9.3.32	O
)	O
(	O
9.3.33	O
)	O
(	O
9.3.34	O
)	O
(	O
9.3.35	O
)	O
(	O
9.3.36	O
)	O
(	O
9.3.37	O
)	O
(	O
9.3.38	O
)	O
since	O
the	O
single-variable	O
marginal	B
distribution	O
of	O
a	O
dirichlet	O
is	O
a	O
beta	B
distribution	O
,	O
section	O
(	O
8.5	O
)	O
,	O
the	O
marginal	B
table	O
is	O
the	O
mean	B
of	O
a	O
beta	B
distribution	O
.	O
given	O
that	O
the	O
marginal	B
p	O
(	O
θ|v	O
)	O
is	O
beta	B
distribution	O
with	O
parameters	O
α	O
=	O
ui	O
+	O
ci	O
,	O
β	O
=	O
(	O
cid:80	O
)	O
j	O
(	O
cid:54	O
)	O
=i	O
uj	O
+	O
cj	O
,	O
the	O
marginal	B
table	O
is	O
given	O
by	O
which	O
generalises	O
the	O
binary	O
state	O
formula	O
equation	B
(	O
9.3.18	O
)	O
.	O
178	O
draft	O
march	O
9	O
,	O
2010	O
bayesian	O
belief	B
network	I
training	O
9.3.4	O
parents	B
to	O
deal	O
with	O
the	O
general	O
case	O
of	O
a	O
variable	B
v	O
with	O
parents	O
pa	O
(	O
v	O
)	O
we	O
denote	O
the	O
probability	B
of	O
v	O
being	O
in	O
state	O
i	O
,	O
conditioned	O
on	O
the	O
parents	B
being	O
in	O
state	O
j	O
as	O
p	O
(	O
v	O
=	O
i|pa	O
(	O
v	O
)	O
=	O
j	O
,	O
θ	O
)	O
=	O
θi	O
(	O
v	O
;	O
j	O
)	O
where	O
(	O
cid:80	O
)	O
the	O
number	O
of	O
states	O
j	O
will	O
be	O
exponential	B
in	O
k.	O
i	O
θi	O
(	O
v	O
;	O
j	O
)	O
=	O
1.	O
this	O
forms	O
the	O
components	O
of	O
a	O
vector	O
θ	O
(	O
v	O
;	O
j	O
)	O
.	O
note	O
that	O
if	O
v	O
has	O
k	O
parents	B
then	O
(	O
9.3.39	O
)	O
(	O
9.3.40	O
)	O
(	O
9.3.41	O
)	O
local	B
(	O
state	O
)	O
independence	B
means	O
p	O
(	O
θ	O
(	O
v	O
;	O
j	O
)	O
)	O
p	O
(	O
θ	O
(	O
v	O
)	O
)	O
=	O
(	O
cid:89	O
)	O
p	O
(	O
θ	O
)	O
=	O
(	O
cid:89	O
)	O
j	O
p	O
(	O
θ	O
(	O
v	O
)	O
)	O
and	O
global	B
independence	O
means	O
v	O
where	O
θ	O
=	O
(	O
θ	O
(	O
v	O
)	O
,	O
v	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
v	O
)	O
represents	O
the	O
combined	O
table	O
of	O
all	O
the	O
variables	O
.	O
we	O
drop	O
the	O
explicit	O
sans-serif	O
font	O
on	O
the	O
states	O
from	O
here	O
on	O
in	O
.	O
parameter	B
posterior	O
thanks	O
to	O
the	O
global	B
parameter	O
independence	B
the	O
posterior	B
distribution	O
over	O
the	O
tables	O
θ	O
factorises	O
,	O
with	O
one	O
posterior	B
table	O
per	O
variable	B
.	O
each	O
posterior	B
table	O
for	O
a	O
variable	B
v	O
depends	O
only	O
on	O
the	O
information	O
local	O
to	O
the	O
family	B
of	O
each	O
variable	B
d	O
(	O
v	O
)	O
.	O
assuming	O
a	O
dirichlet	O
distribution	B
prior	O
p	O
(	O
θ	O
(	O
v	O
;	O
j	O
)	O
)	O
=	O
dirichlet	O
(	O
θ	O
(	O
v	O
;	O
j	O
)	O
|u	O
(	O
v	O
;	O
j	O
)	O
)	O
the	O
posterior	B
is	O
proportional	O
to	O
the	O
joint	B
distribution	O
(	O
cid:89	O
)	O
p	O
(	O
θ	O
(	O
v	O
)	O
,	O
d	O
(	O
v	O
)	O
)	O
=	O
p	O
(	O
θ	O
(	O
v	O
)	O
)	O
p	O
(	O
d	O
(	O
v	O
)	O
|θ	O
(	O
v	O
)	O
)	O
(	O
cid:89	O
)	O
=	O
(	O
cid:89	O
)	O
=	O
(	O
cid:89	O
)	O
z	O
(	O
u	O
(	O
v	O
;	O
j	O
)	O
)	O
1	O
1	O
i	O
z	O
(	O
u	O
(	O
v	O
;	O
j	O
)	O
)	O
i	O
j	O
j	O
θi	O
(	O
v	O
;	O
j	O
)	O
ui	O
(	O
v	O
;	O
j	O
)	O
−1	O
(	O
cid:89	O
)	O
(	O
cid:89	O
)	O
(	O
cid:89	O
)	O
θi	O
(	O
v	O
;	O
j	O
)	O
i	O
[	O
vn=i	O
,	O
pa	O
(	O
vn	O
)	O
=j	O
]	O
n	O
j	O
i	O
θi	O
(	O
v	O
;	O
j	O
)	O
ui	O
(	O
v	O
;	O
j	O
)	O
−1+	O
(	O
cid:93	O
)	O
(	O
v=i	O
,	O
pa	O
(	O
v	O
)	O
=j	O
)	O
where	O
z	O
(	O
u	O
)	O
is	O
the	O
normalisation	B
constant	I
of	O
a	O
dirichlet	O
distribution	B
.	O
hence	O
the	O
posterior	B
is	O
p	O
(	O
θ	O
(	O
v	O
)	O
|d	O
(	O
v	O
)	O
)	O
=	O
(	O
cid:89	O
)	O
dirichlet	O
(	O
cid:0	O
)	O
θ	O
(	O
v	O
;	O
j	O
)	O
|u	O
(	O
cid:48	O
)	O
(	O
v	O
;	O
j	O
)	O
(	O
cid:1	O
)	O
j	O
where	O
the	O
hyperparameter	B
prior	O
term	O
is	O
updated	O
by	O
the	O
observed	O
counts	O
,	O
(	O
cid:48	O
)	O
i	O
(	O
v	O
;	O
j	O
)	O
≡	O
ui	O
(	O
v	O
;	O
j	O
)	O
+	O
(	O
cid:93	O
)	O
(	O
v	O
=	O
i	O
,	O
pa	O
(	O
v	O
)	O
=	O
j	O
)	O
u	O
(	O
9.3.42	O
)	O
(	O
9.3.43	O
)	O
(	O
9.3.44	O
)	O
(	O
9.3.45	O
)	O
(	O
9.3.46	O
)	O
(	O
9.3.47	O
)	O
by	O
analogy	O
with	O
the	O
no-parents	O
case	O
,	O
the	O
marginal	B
table	O
is	O
given	O
by	O
(	O
writing	O
the	O
states	O
explicitly	O
)	O
(	O
cid:48	O
)	O
i	O
(	O
v	O
;	O
j	O
)	O
p	O
(	O
v	O
=	O
i|pa	O
(	O
v	O
)	O
=	O
j	O
,	O
d	O
(	O
v	O
)	O
)	O
∝	O
u	O
draft	O
march	O
9	O
,	O
2010	O
(	O
9.3.48	O
)	O
179	O
bayesian	O
belief	B
network	I
training	O
a	O
1	O
1	O
0	O
0	O
1	O
0	O
1	O
s	O
1	O
0	O
1	O
1	O
1	O
0	O
0	O
c	O
2	O
0	O
1	O
0	O
2	O
0	O
1	O
figure	O
9.8	O
:	O
a	O
database	O
of	O
patient	O
records	O
about	O
the	O
asbestos	O
exposure	O
(	O
1	O
signiﬁes	O
exposure	O
)	O
,	O
being	O
a	O
smoker	O
(	O
1	O
signiﬁes	O
the	O
individual	O
is	O
a	O
smoker	O
)	O
,	O
and	O
lung	O
cancer	O
(	O
0	O
signiﬁes	O
no	O
cancer	O
,	O
1	O
signiﬁes	O
early	O
stage	O
cancer	O
,	O
2	O
signiﬁes	O
late	O
state	O
cancer	O
)	O
.	O
each	O
row	O
contains	O
the	O
information	O
for	O
an	O
indi-	O
vidual	O
,	O
so	O
that	O
there	O
are	O
7	O
individuals	O
in	O
the	O
database	O
.	O
example	O
40.	O
consider	O
the	O
p	O
(	O
c|a	O
,	O
s	O
)	O
p	O
(	O
s	O
)	O
p	O
(	O
a	O
)	O
asbestos	O
example	O
with	O
dom	O
(	O
a	O
)	O
=	O
dom	O
(	O
s	O
)	O
=	O
{	O
0	O
,	O
1	O
}	O
,	O
except	O
now	O
with	O
the	O
variable	B
c	O
taking	O
three	O
states	O
,	O
dom	O
(	O
c	O
)	O
=	O
{	O
0	O
,	O
1	O
,	O
2	O
}	O
,	O
accounting	O
for	O
diﬀerent	O
kinds	O
of	O
cancer	O
.	O
the	O
marginal	B
table	O
under	O
a	O
dirichlet	O
prior	B
is	O
then	O
given	O
by	O
p	O
(	O
c	O
=	O
0|a	O
=	O
1	O
,	O
s	O
=	O
1	O
,	O
v	O
)	O
=	O
u0	O
(	O
a	O
=	O
1	O
,	O
s	O
=	O
1	O
)	O
+	O
(	O
cid:93	O
)	O
(	O
c	O
=	O
0	O
,	O
a	O
=	O
1	O
,	O
s	O
=	O
1	O
)	O
i∈	O
{	O
0,1,2	O
}	O
ui	O
(	O
a	O
=	O
1	O
,	O
s	O
=	O
1	O
)	O
+	O
(	O
cid:93	O
)	O
(	O
c	O
=	O
i	O
,	O
a	O
=	O
1	O
,	O
s	O
=	O
1	O
)	O
(	O
cid:80	O
)	O
(	O
9.3.49	O
)	O
(	O
9.3.50	O
)	O
(	O
9.3.51	O
)	O
(	O
9.3.52	O
)	O
(	O
9.3.53	O
)	O
(	O
9.3.54	O
)	O
assuming	O
a	O
ﬂat	O
dirichlet	O
prior	B
,	O
which	O
corresponds	O
to	O
setting	O
all	O
components	O
of	O
u	O
to	O
1	O
,	O
this	O
gives	O
p	O
(	O
c	O
=	O
0|a	O
=	O
1	O
,	O
s	O
=	O
1	O
,	O
v	O
)	O
=	O
p	O
(	O
c	O
=	O
1|a	O
=	O
1	O
,	O
s	O
=	O
1	O
,	O
v	O
)	O
=	O
p	O
(	O
c	O
=	O
2|a	O
=	O
1	O
,	O
s	O
=	O
1	O
,	O
v	O
)	O
=	O
1	O
+	O
0	O
3	O
+	O
2	O
1	O
+	O
0	O
3	O
+	O
2	O
1	O
+	O
2	O
3	O
+	O
2	O
=	O
=	O
=	O
1	O
5	O
1	O
5	O
3	O
5	O
and	O
similarly	O
for	O
the	O
other	O
three	O
tables	O
p	O
(	O
c|a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
,	O
p	O
(	O
c|a	O
=	O
0	O
,	O
s	O
=	O
1	O
)	O
,	O
p	O
(	O
c|a	O
=	O
1	O
,	O
s	O
=	O
1	O
)	O
.	O
model	B
likelihood	O
for	O
a	O
belief	B
network	I
m	O
,	O
the	O
joint	B
probability	O
of	O
all	O
variables	O
factorises	O
into	O
the	O
local	B
probabilities	O
of	O
each	O
variable	B
conditioned	O
on	O
its	O
parents	B
:	O
p	O
(	O
v|m	O
)	O
=	O
(	O
cid:89	O
)	O
p	O
(	O
d|m	O
)	O
=	O
(	O
cid:89	O
)	O
v	O
for	O
i.i.d	O
.	O
data	B
d	O
,	O
the	O
likelihood	B
under	O
the	O
network	O
m	O
is	O
p	O
(	O
v|pa	O
(	O
v	O
)	O
,	O
m	O
)	O
(	O
cid:89	O
)	O
p	O
(	O
vn|pa	O
(	O
vn	O
)	O
,	O
m	O
)	O
=	O
(	O
cid:89	O
)	O
(	O
cid:89	O
)	O
v	O
n	O
v	O
j	O
z	O
(	O
u	O
(	O
cid:48	O
)	O
(	O
v	O
;	O
j	O
)	O
)	O
z	O
(	O
u	O
(	O
v	O
;	O
j	O
)	O
)	O
where	O
u	O
are	O
the	O
dirichlet	O
hyperparameters	O
and	O
u	O
(	O
cid:48	O
)	O
is	O
given	O
by	O
equation	B
(	O
9.3.47	O
)	O
.	O
expression	O
(	O
9.3.54	O
)	O
can	O
be	O
written	O
explicitly	O
in	O
terms	O
of	O
gamma	B
functions	O
,	O
see	O
exercise	O
(	O
125	O
)	O
.	O
in	O
the	O
above	O
expression	O
in	O
general	O
the	O
number	O
of	O
parental	O
states	O
diﬀers	O
for	O
each	O
variable	B
v	O
,	O
so	O
that	O
implicit	O
in	O
the	O
above	O
formula	O
is	O
that	O
the	O
state	O
product	O
over	O
j	O
goes	O
from	O
1	O
to	O
the	O
number	O
of	O
parental	O
states	O
of	O
variable	B
v.	O
due	O
to	O
the	O
local	B
and	O
global	B
parameter	O
independence	B
assumptions	O
,	O
the	O
logarithm	O
of	O
the	O
model	B
likelihood	O
splits	O
into	O
terms	O
,	O
one	O
for	O
each	O
variable	B
v	O
and	O
parental	O
conﬁguration	O
.	O
this	O
is	O
called	O
the	O
likelihood	B
decomposable	I
property	O
.	O
9.3.5	O
structure	B
learning	I
up	O
to	O
this	O
point	O
we	O
have	O
assumed	O
that	O
we	O
are	O
given	O
both	O
the	O
structure	B
of	O
the	O
distribution	B
and	O
a	O
dataset	O
‘	O
d.	O
a	O
more	O
complex	O
task	O
is	O
when	O
we	O
need	O
to	O
learn	O
the	O
structure	B
of	O
the	O
network	O
as	O
well	O
.	O
we	O
’	O
ll	O
consider	O
the	O
case	O
in	O
which	O
the	O
data	B
is	O
complete	O
(	O
i.e	O
.	O
there	O
are	O
no	O
missing	O
observations	O
)	O
.	O
since	O
for	O
d	O
variables	O
,	O
there	O
is	O
an	O
exponentially	O
large	O
number	O
(	O
in	O
d	O
)	O
of	O
bn	O
structures	O
,	O
it	O
’	O
s	O
clear	O
that	O
we	O
can	O
not	O
search	O
over	O
all	O
possible	O
structures	O
.	O
for	O
this	O
reason	O
structure	B
learning	I
is	O
a	O
computationally	O
challenging	O
problem	B
and	O
we	O
must	O
rely	O
on	O
constraints	O
and	O
heuristics	O
to	O
help	O
guide	O
the	O
search	O
.	O
furthermore	O
,	O
for	O
all	O
but	O
the	O
sparsest	O
180	O
draft	O
march	O
9	O
,	O
2010	O
bayesian	O
belief	B
network	I
training	O
algorithm	B
3	O
pc	O
algorithm	B
for	O
skeleton	B
learning	O
.	O
1	O
:	O
start	O
with	O
a	O
complete	O
undirected	B
graph	I
g	O
on	O
the	O
set	O
v	O
of	O
all	O
vertices	O
.	O
2	O
:	O
i	O
=	O
0	O
3	O
:	O
repeat	O
4	O
:	O
5	O
:	O
for	O
x	O
∈	O
v	O
do	O
for	O
y	O
∈	O
adj	O
{	O
x	O
}	O
do	O
determine	O
if	O
there	O
a	O
subset	O
s	O
of	O
size	O
i	O
of	O
the	O
neighbours	O
of	O
x	O
(	O
not	O
including	O
y	O
)	O
for	O
which	O
x⊥⊥	O
y|s	O
.	O
if	O
this	O
set	O
exists	O
remove	O
the	O
x	O
−	O
y	O
link	O
from	O
the	O
graph	B
g	O
and	O
set	O
sxy	O
=	O
s.	O
6	O
:	O
7	O
:	O
8	O
:	O
9	O
:	O
end	O
for	O
end	O
for	O
i	O
=	O
i	O
+	O
1	O
.	O
10	O
:	O
until	O
all	O
nodes	O
have	O
≤	O
i	O
neighbours	O
.	O
networks	O
,	O
estimating	O
the	O
dependencies	O
to	O
any	O
accuracy	O
requires	O
a	O
large	O
amount	O
of	O
data	B
,	O
making	O
testing	O
of	O
dependencies	O
diﬃcult	O
.	O
indeed	O
,	O
for	O
a	O
ﬁnite	O
amount	O
of	O
data	B
,	O
two	O
variables	O
will	O
always	O
have	O
non-zero	O
mutual	B
information	I
,	O
so	O
that	O
a	O
threshold	O
needs	O
to	O
be	O
set	O
to	O
decide	O
if	O
the	O
measured	O
dependence	B
is	O
signiﬁ-	O
cant	O
under	O
the	O
ﬁnite	O
sample	O
,	O
see	O
section	O
(	O
9.3.6	O
)	O
.	O
other	O
complexities	O
arise	O
from	O
the	O
concern	O
that	O
a	O
belief	O
or	O
markov	O
network	O
on	O
the	O
visible	B
variables	O
alone	O
may	O
not	O
be	O
a	O
parsimonious	O
way	O
to	O
represent	O
the	O
ob-	O
served	O
data	B
if	O
,	O
for	O
example	O
,	O
there	O
may	O
be	O
latent	B
variables	O
which	O
are	O
driving	O
the	O
observed	O
dependencies	O
.	O
for	O
these	O
reasons	O
we	O
will	O
not	O
discuss	O
this	O
topic	O
in	O
detail	O
here	O
and	O
limit	O
the	O
discussion	O
to	O
two	O
central	O
approaches	O
.	O
a	O
special	O
case	O
that	O
is	O
computationally	O
tractable	O
is	O
when	O
the	O
network	O
is	O
constrained	O
to	O
have	O
at	O
most	O
one	O
parent	O
.	O
we	O
defer	O
discussion	O
of	O
this	O
to	O
section	O
(	O
10.4.1	O
)	O
.	O
pc	O
algorithm	B
the	O
pc	O
algorithm	B
[	O
259	O
]	O
ﬁrst	O
learns	O
the	O
skeleton	B
of	O
a	O
graph	B
,	O
after	O
which	O
edges	O
may	O
be	O
oriented	O
to	O
form	O
a	O
(	O
partially	O
oriented	O
)	O
dag	O
.	O
the	O
pc	O
algorithm	B
begins	O
at	O
the	O
ﬁrst	O
round	O
with	O
a	O
complete	O
skeleton	B
g	O
and	O
attempts	O
to	O
remove	O
as	O
many	O
links	O
as	O
possible	O
.	O
at	O
the	O
ﬁrst	O
step	O
we	O
test	O
all	O
pairs	O
x⊥⊥	O
y|∅	O
.	O
if	O
an	O
x	O
and	O
y	O
pair	O
are	O
deemed	O
independent	O
then	O
the	O
link	O
x−	O
y	O
is	O
removed	O
from	O
the	O
complete	O
graph	B
.	O
one	O
repeats	O
this	O
for	O
all	O
the	O
pairwise	B
links	O
.	O
in	O
the	O
second	O
round	O
,	O
for	O
the	O
remaining	O
graph	B
,	O
one	O
examines	O
each	O
x−	O
y	O
link	O
and	O
conditions	O
on	O
a	O
single	O
neighbour	B
z	O
of	O
x.	O
if	O
x⊥⊥	O
y|	O
z	O
then	O
remove	O
the	O
link	O
x	O
−	O
y.	O
one	O
repeats	O
in	O
this	O
way	O
through	O
all	O
the	O
variables	O
.	O
at	O
each	O
round	O
the	O
number	O
of	O
neighbours	O
in	O
the	O
conditioning	B
set	O
is	O
increased	O
by	O
one	O
.	O
see	O
algorithm	O
(	O
3	O
)	O
,	O
ﬁg	O
(	O
9.9	O
)	O
1	O
and	O
demopcoracle.m	O
.	O
a	O
reﬁnement	O
of	O
this	O
algorithm	B
,	O
known	O
as	O
npc	O
for	O
necessary	O
path	B
pc	O
[	O
261	O
]	O
attempts	O
to	O
limit	O
the	O
number	O
of	O
independence	B
checks	O
which	O
may	O
otherwise	O
result	O
in	O
inconsistencies	O
due	O
to	O
the	O
empirical	B
estimates	O
of	O
conditional	B
mutual	I
information	I
.	O
given	O
a	O
learned	O
skeleton	B
,	O
a	O
partial	O
dag	O
can	O
be	O
constructed	O
using	O
algorithm	B
(	O
4	O
)	O
.	O
note	O
that	O
this	O
is	O
necessary	O
since	O
the	O
undirected	B
graph	I
g	O
is	O
a	O
skeleton	B
–	O
not	O
a	O
belief	B
network	I
of	O
the	O
independence	B
assumptions	O
discovered	O
.	O
for	O
example	O
,	O
we	O
may	O
have	O
a	O
graph	B
g	O
with	O
x−	O
z	O
−	O
y	O
in	O
which	O
the	O
x	O
−	O
y	O
link	O
was	O
removed	O
on	O
the	O
basis	O
x	O
⊥⊥	O
y|	O
∅	O
→	O
sxy	O
=	O
∅	O
.	O
as	O
a	O
mn	O
the	O
graph	B
x	O
−	O
z	O
−	O
y	O
implies	O
x	O
(	O
cid:62	O
)	O
(	O
cid:62	O
)	O
y	O
,	O
although	O
this	O
is	O
inconsistent	O
with	O
the	O
discovery	O
in	O
the	O
ﬁrst	O
round	O
x⊥⊥	O
y.	O
this	O
is	O
the	O
reason	O
for	O
the	O
orientation	O
part	O
:	O
for	O
consistency	O
,	O
we	O
must	O
have	O
x	O
→	O
z	O
←	O
y	O
,	O
for	O
which	O
x	O
⊥⊥	O
y	O
and	O
x	O
(	O
cid:62	O
)	O
(	O
cid:62	O
)	O
y|	O
z.	O
note	O
that	O
in	O
algorithm	B
(	O
4	O
)	O
we	O
have	O
for	O
the	O
‘	O
unmarried	O
collider	B
’	O
test	O
,	O
z	O
(	O
cid:54	O
)	O
∈	O
∅	O
,	O
which	O
in	O
this	O
case	O
is	O
true	O
,	O
resulting	O
in	O
a	O
collider	B
forming	O
.	O
see	O
also	O
ﬁg	O
(	O
9.10	O
)	O
.	O
example	O
41	O
(	O
skeleton	B
orienting	O
)	O
.	O
z	O
x	O
y	O
x⊥⊥	O
y|∅	O
⇒	O
z	O
x	O
y	O
if	O
x	O
is	O
(	O
unconditionally	O
)	O
independent	O
of	O
y	O
,	O
it	O
must	O
be	O
that	O
z	O
is	O
a	O
collider	B
since	O
otherwise	O
marginalising	O
over	O
z	O
would	O
introduce	O
a	O
depen-	O
dence	O
between	O
x	O
and	O
y	O
.	O
1this	O
example	O
appears	O
in	O
[	O
148	O
]	O
and	O
[	O
208	O
]	O
–	O
thanks	O
also	O
to	O
seraf´ın	O
moral	O
for	O
his	O
online	B
notes	O
.	O
draft	O
march	O
9	O
,	O
2010	O
181	O
bayesian	O
belief	B
network	I
training	O
x	O
z	O
x	O
z	O
t	O
(	O
a	O
)	O
t	O
(	O
j	O
)	O
y	O
x	O
y	O
x	O
y	O
x	O
y	O
x	O
y	O
x	O
y	O
x	O
y	O
x	O
y	O
x	O
w	O
z	O
w	O
z	O
w	O
z	O
w	O
z	O
w	O
z	O
w	O
z	O
w	O
z	O
w	O
z	O
t	O
(	O
b	O
)	O
t	O
(	O
c	O
)	O
t	O
(	O
d	O
)	O
t	O
(	O
e	O
)	O
t	O
(	O
f	O
)	O
t	O
(	O
g	O
)	O
t	O
(	O
h	O
)	O
y	O
x	O
y	O
x	O
y	O
x	O
y	O
x	O
y	O
x	O
y	O
x	O
y	O
x	O
y	O
x	O
w	O
z	O
w	O
z	O
w	O
z	O
w	O
z	O
w	O
z	O
w	O
z	O
w	O
z	O
w	O
z	O
t	O
(	O
k	O
)	O
t	O
(	O
l	O
)	O
t	O
(	O
m	O
)	O
t	O
(	O
n	O
)	O
t	O
(	O
o	O
)	O
t	O
(	O
p	O
)	O
t	O
(	O
q	O
)	O
y	O
w	O
y	O
w	O
t	O
(	O
i	O
)	O
t	O
(	O
r	O
)	O
(	O
a	O
)	O
:	O
the	O
bn	O
from	O
which	O
data	B
is	O
assumed	O
generated	O
and	O
against	O
which	O
figure	O
9.9	O
:	O
pc	O
algorithm	B
.	O
conditional	B
independence	O
tests	O
will	O
be	O
performed	O
.	O
(	O
b	O
)	O
:	O
the	O
initial	O
skeleton	B
is	O
fully	O
connected	B
.	O
(	O
c-l	O
)	O
:	O
in	O
the	O
ﬁrst	O
round	O
(	O
i	O
=	O
0	O
)	O
all	O
the	O
pairwise	B
mutual	O
informations	O
x⊥⊥	O
y|∅	O
are	O
checked	O
,	O
and	O
the	O
link	O
between	O
x	O
(	O
m-o	O
)	O
:	O
i	O
=	O
1.	O
we	O
now	O
look	O
at	O
connected	B
subsets	O
on	O
and	O
y	O
removed	O
if	O
deemed	O
independent	O
(	O
green	O
line	O
)	O
.	O
the	O
three	O
variables	O
x	O
,	O
y	O
,	O
z	O
of	O
the	O
remaining	O
graph	B
,	O
removing	O
the	O
link	O
x	O
−	O
y	O
if	O
x⊥⊥	O
y|	O
z	O
is	O
true	O
.	O
not	O
all	O
steps	O
(	O
p	O
,	O
q	O
)	O
:	O
i	O
=	O
2.	O
we	O
now	O
examine	O
all	O
x⊥⊥	O
y|	O
{	O
a	O
,	O
b	O
}	O
.	O
the	O
algorithm	B
terminates	O
after	O
this	O
round	O
are	O
shown	O
.	O
(	O
r	O
)	O
:	O
final	O
skeleton	B
.	O
(	O
when	O
i	O
gets	O
incremented	O
to	O
3	O
)	O
since	O
there	O
are	O
no	O
nodes	O
with	O
3	O
or	O
more	O
neighbours	O
.	O
during	O
this	O
process	O
the	O
sets	O
sx	O
,	O
y	O
=	O
∅	O
,	O
sx	O
,	O
w	O
=	O
∅	O
,	O
sz	O
,	O
w	O
=	O
y	O
,	O
sx	O
,	O
t	O
=	O
{	O
z	O
,	O
w	O
}	O
,	O
sy	O
,	O
t	O
=	O
{	O
z	O
,	O
w	O
}	O
were	O
found	O
.	O
see	O
also	O
demopcoracle.m	O
z	O
x	O
y	O
x⊥⊥	O
y|	O
z	O
⇒	O
z	O
x	O
y	O
if	O
x	O
is	O
independent	O
of	O
y	O
conditioned	O
on	O
z	O
,	O
z	O
must	O
not	O
be	O
a	O
collider	B
.	O
any	O
other	O
orientation	O
is	O
ap-	O
propriate	O
.	O
9.3.6	O
empirical	B
independence	O
given	O
a	O
data	B
set	O
d	O
,	O
containing	O
variables	O
x	O
,	O
y	O
,	O
z	O
,	O
our	O
interest	O
is	O
to	O
measure	O
if	O
x⊥⊥	O
y|	O
z.	O
one	O
approach	B
is	O
to	O
use	O
the	O
conditional	B
mutual	I
information	I
which	O
is	O
the	O
average	B
of	O
conditional	B
kullback-leibler	O
divergences	O
.	O
deﬁnition	O
87	O
(	O
mutual	B
information	I
)	O
.	O
mi	O
(	O
x	O
;	O
y|z	O
)	O
≡	O
(	O
cid:104	O
)	O
kl	O
(	O
p	O
(	O
x	O
,	O
y|z	O
)	O
|p	O
(	O
x|z	O
)	O
p	O
(	O
y|z	O
)	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
z	O
)	O
≥	O
0	O
(	O
9.3.55	O
)	O
where	O
this	O
expression	O
is	O
equally	O
valid	O
for	O
sets	O
of	O
variables	O
.	O
if	O
x⊥⊥	O
y|	O
z	O
is	O
true	O
,	O
then	O
mi	O
(	O
x	O
;	O
y|z	O
)	O
is	O
zero	O
,	O
and	O
vice	O
versa	O
.	O
when	O
z	O
=	O
∅	O
,	O
the	O
average	B
over	O
p	O
(	O
z	O
)	O
is	O
absent	O
and	O
one	O
writes	O
mi	O
(	O
x	O
;	O
y	O
)	O
.	O
given	O
data	B
we	O
can	O
obtain	O
an	O
estimate	O
of	O
the	O
conditional	B
mutual	I
information	I
by	O
using	O
the	O
empirical	B
distribution	I
p	O
(	O
x	O
,	O
y	O
,	O
z	O
)	O
estimated	O
by	O
simply	O
counting	B
occurrences	O
in	O
the	O
data	B
.	O
in	O
practice	O
,	O
however	O
,	O
we	O
only	O
have	O
a	O
ﬁnite	O
amount	O
of	O
data	B
to	O
estimate	O
the	O
empirical	B
distribution	I
so	O
that	O
for	O
data	B
sampled	O
from	O
distribution	B
for	O
which	O
the	O
variables	O
truly	O
are	O
independent	O
,	O
the	O
empirical	B
mutual	O
information	O
will	O
typically	O
be	O
greater	O
than	O
zero	O
.	O
an	O
issue	O
therefore	O
is	O
what	O
threshold	O
to	O
use	O
for	O
the	O
empirical	B
conditional	O
mutual	B
information	I
to	O
decide	O
if	O
this	O
is	O
suﬃciently	O
far	O
from	O
zero	O
to	O
be	O
caused	O
by	O
dependence	B
.	O
a	O
frequentist	B
approach	O
is	O
to	O
compute	O
the	O
distribution	B
of	O
the	O
conditional	B
mutual	I
information	I
and	O
then	O
see	O
where	O
the	O
sample	B
value	O
is	O
compared	O
to	O
the	O
distribution	B
.	O
according	O
to	O
[	O
164	O
]	O
2nmi	O
(	O
x	O
;	O
y|z	O
)	O
is	O
chi-square	O
distributed	O
182	O
draft	O
march	O
9	O
,	O
2010	O
bayesian	O
belief	B
network	I
training	O
algorithm	B
4	O
skeleton	B
orientation	O
algorithm	B
(	O
returns	O
a	O
dag	O
)	O
.	O
1	O
:	O
unmarried	O
collider	B
:	O
examine	O
all	O
undirected	B
links	O
x	O
−	O
z	O
−	O
y.	O
if	O
z	O
(	O
cid:54	O
)	O
∈	O
sxy	O
set	O
x	O
→	O
z	O
←	O
y	O
.	O
2	O
:	O
repeat	O
3	O
:	O
4	O
:	O
5	O
:	O
6	O
:	O
until	O
no	O
more	O
edges	O
can	O
be	O
oriented	O
.	O
7	O
:	O
the	O
remaining	O
edges	O
can	O
be	O
arbitrarily	O
oriented	O
provided	O
that	O
the	O
graph	B
remains	O
a	O
dag	O
and	O
no	O
additional	O
x	O
→	O
z	O
−	O
y	O
⇒	O
x	O
→	O
z	O
→	O
y	O
for	O
x	O
−	O
y	O
,	O
if	O
there	O
is	O
a	O
directed	B
path	O
from	O
x	O
to	O
y	O
orient	O
x	O
→	O
y	O
if	O
for	O
x	O
−	O
z	O
−	O
y	O
there	O
is	O
a	O
w	O
such	O
that	O
x	O
→	O
w	O
,	O
y	O
→	O
w	O
,	O
z	O
−	O
w	O
then	O
orient	O
z	O
→	O
w	O
colliders	O
are	O
introduced	O
.	O
with	O
(	O
x−1	O
)	O
(	O
y	O
−1	O
)	O
z	O
degrees	O
of	O
freedom	O
,	O
although	O
this	O
test	O
does	O
not	O
work	O
well	O
in	O
the	O
case	O
of	O
small	O
amounts	O
of	O
data	B
.	O
an	O
alternative	O
pragmatic	O
approach	B
is	O
to	O
estimate	O
the	O
threshold	O
based	O
on	O
empirical	B
samples	O
of	O
the	O
mi	O
under	O
controlled	O
independent/dependent	O
conditions	O
–	O
see	O
democondindepemp.m	O
for	O
a	O
comparison	O
of	O
these	O
approaches	O
.	O
bayesian	O
conditional	B
independence	O
test	O
a	O
bayesian	O
approach	B
to	O
testing	O
for	O
independence	B
can	O
be	O
made	O
by	O
comparing	O
the	O
likelihood	B
of	O
the	O
data	B
under	O
the	O
independence	B
hypothesis	O
,	O
versus	O
the	O
likelihood	B
under	O
the	O
dependent	O
hypothesis	O
.	O
for	O
the	O
independence	B
hypothesis	O
we	O
have	O
a	O
joint	B
distribution	O
over	O
variables	O
and	O
parameters	O
:	O
p	O
(	O
x	O
,	O
y	O
,	O
z	O
,	O
θ|hindep	O
)	O
=	O
p	O
(	O
x|z	O
,	O
θx|z	O
)	O
p	O
(	O
y|z	O
,	O
θy|z	O
)	O
p	O
(	O
z|θz	O
)	O
p	O
(	O
θx|z	O
)	O
p	O
(	O
θy|z	O
)	O
p	O
(	O
θz	O
)	O
(	O
9.3.56	O
)	O
for	O
categorical	O
distributions	O
,	O
it	O
is	O
convenient	O
to	O
use	O
a	O
prior	B
dirichlet	O
(	O
θ|u	O
)	O
on	O
the	O
parameters	O
θ	O
,	O
assum-	O
ing	O
also	O
local	B
as	O
well	O
as	O
global	O
parameter	B
independence	O
.	O
for	O
a	O
set	O
of	O
assumed	O
i.i.d	O
.	O
data	B
(	O
x	O
,	O
y	O
,	O
z	O
)	O
=	O
(	O
xn	O
,	O
yn	O
,	O
zn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
the	O
likelihood	B
is	O
then	O
given	O
by	O
integratingover	O
the	O
parameters	O
θ	O
:	O
p	O
(	O
x	O
,	O
y	O
,	O
z|hindep	O
)	O
=	O
z	O
(	O
uz	O
+	O
(	O
cid:93	O
)	O
(	O
z	O
)	O
)	O
z	O
(	O
uz	O
)	O
z	O
(	O
ux|z	O
+	O
(	O
cid:93	O
)	O
(	O
x	O
,	O
z	O
)	O
)	O
z	O
(	O
uy|z	O
+	O
(	O
cid:93	O
)	O
(	O
y	O
,	O
z	O
)	O
)	O
z	O
(	O
ux|z	O
)	O
z	O
z	O
(	O
uy|z	O
)	O
(	O
9.3.57	O
)	O
where	O
ux|z	O
is	O
a	O
hyperparameter	B
matrix	O
of	O
pseudo	B
counts	O
for	O
each	O
state	O
of	O
x	O
given	O
each	O
state	O
of	O
z.	O
z	O
(	O
v	O
)	O
is	O
the	O
normalisation	B
constant	I
of	O
a	O
dirichlet	O
distribution	B
with	O
vector	O
parameter	O
v.	O
for	O
the	O
dependent	O
hypothesis	O
we	O
have	O
p	O
(	O
x	O
,	O
y	O
,	O
z	O
,	O
θ|hdep	O
)	O
=	O
p	O
(	O
x	O
,	O
y	O
,	O
z|θx	O
,	O
y	O
,	O
z	O
)	O
p	O
(	O
θx	O
,	O
y	O
,	O
z	O
)	O
the	O
likelihood	B
is	O
then	O
p	O
(	O
x	O
,	O
y	O
,	O
z|hdep	O
)	O
=	O
z	O
(	O
ux	O
,	O
y	O
,	O
z	O
+	O
(	O
cid:93	O
)	O
(	O
x	O
,	O
y	O
,	O
z	O
)	O
)	O
z	O
(	O
ux	O
,	O
y	O
,	O
z	O
)	O
(	O
9.3.58	O
)	O
(	O
9.3.59	O
)	O
x	O
z	O
y	O
x	O
y	O
x	O
y	O
x	O
w	O
z	O
w	O
z	O
w	O
z	O
y	O
w	O
t	O
(	O
a	O
)	O
t	O
(	O
b	O
)	O
t	O
(	O
c	O
)	O
t	O
(	O
d	O
)	O
(	O
a	O
)	O
:	O
figure	O
9.10	O
:	O
skeleton	B
orientation	O
algorithm	B
.	O
the	O
skeleton	B
along	O
with	O
sx	O
,	O
y	O
=	O
∅	O
,	O
sx	O
,	O
w	O
=	O
∅	O
,	O
sz	O
,	O
w	O
=	O
(	O
b	O
)	O
:	O
z	O
(	O
cid:54	O
)	O
∈	O
sx	O
,	O
y	O
,	O
y	O
,	O
sx	O
,	O
t	O
=	O
{	O
z	O
,	O
w	O
}	O
,	O
sy	O
,	O
t	O
=	O
{	O
z	O
,	O
w	O
}	O
.	O
(	O
c	O
)	O
:	O
t	O
(	O
cid:54	O
)	O
∈	O
sz	O
,	O
w	O
,	O
so	O
form	O
collider	B
.	O
so	O
form	O
collider	B
.	O
(	O
d	O
)	O
:	O
final	O
partially	O
oriented	O
dag	O
.	O
the	O
remaining	O
edge	O
may	O
be	O
oriented	O
as	O
desired	O
,	O
without	O
violating	O
the	O
dag	O
condition	O
.	O
see	O
also	O
demopcoracle.m	O
.	O
draft	O
march	O
9	O
,	O
2010	O
183	O
p	O
(	O
x	O
,	O
y	O
,	O
z|hindep	O
)	O
=	O
p	O
(	O
xn	O
,	O
yn	O
,	O
zn	O
,	O
θ|hindep	O
)	O
thanks	O
to	O
conjugacy	O
,	O
this	O
is	O
straightforward	O
and	O
gives	O
the	O
expression	O
(	O
cid:90	O
)	O
(	O
cid:89	O
)	O
θ	O
n	O
(	O
cid:89	O
)	O
bayesian	O
belief	B
network	I
training	O
θz	O
zn	O
xn	O
yn	O
n	O
θx|z	O
θy|z	O
θx	O
,	O
y	O
,	O
z	O
xn	O
,	O
yn	O
,	O
zn	O
n	O
figure	O
9.11	O
:	O
bayesian	O
conditional	B
independence	O
test	O
(	O
a	O
)	O
:	O
a	O
using	O
dirichlet	O
priors	O
on	O
the	O
tables	O
.	O
model	B
hindep	O
for	O
conditional	B
independence	O
x	O
⊥⊥	O
y	O
|	O
(	O
b	O
)	O
:	O
a	O
model	B
hdep	O
for	O
conditional	B
depen-	O
z.	O
dence	O
x	O
(	O
cid:62	O
)	O
(	O
cid:62	O
)	O
y|	O
z.	O
by	O
computing	O
the	O
likelihood	B
of	O
the	O
data	B
under	O
each	O
model	B
,	O
a	O
numerical	B
score	O
for	O
the	O
whether	O
the	O
data	B
is	O
more	O
consistent	B
with	O
the	O
condi-	O
tional	O
independence	B
assumption	O
can	O
be	O
formed	O
.	O
see	O
democondindepemp.m	O
.	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
9.12	O
:	O
conditional	B
independence	O
test	O
of	O
x	O
⊥⊥	O
y	O
|	O
z1	O
,	O
z2	O
with	O
x	O
,	O
y	O
,	O
z1	O
,	O
z2	O
having	O
3	O
,	O
2	O
,	O
4	O
,	O
2	O
states	O
respectively	O
.	O
from	O
the	O
oracle	O
belief	B
network	I
shown	O
,	O
in	O
each	O
experiment	O
the	O
tables	O
are	O
drawn	O
at	O
random	O
and	O
20	O
examples	O
are	O
sampled	O
to	O
form	O
a	O
dataset	O
.	O
for	O
each	O
dataset	O
a	O
test	O
is	O
carried	O
out	O
to	O
determine	O
if	O
x	O
and	O
y	O
are	O
independent	O
conditioned	O
on	O
z1	O
,	O
z2	O
(	O
the	O
correct	O
answer	O
being	O
that	O
they	O
are	O
independent	O
)	O
.	O
over	O
500	O
experiments	O
,	O
the	O
bayesian	O
conditional	B
independence	O
test	O
correctly	O
states	O
that	O
the	O
variables	O
are	O
conditionally	O
independent	O
74	O
%	O
of	O
the	O
time	O
,	O
compared	O
with	O
only	O
50	O
%	O
accuracy	O
using	O
the	O
chi-square	O
mutual	B
information	I
test	O
.	O
see	O
democondindepemp.m	O
.	O
assuming	O
each	O
hypothesis	O
is	O
equally	O
likely	O
,	O
for	O
a	O
bayes	O
’	O
factor	B
p	O
(	O
x	O
,	O
y	O
,	O
z|hindep	O
)	O
p	O
(	O
x	O
,	O
y	O
,	O
z|hdep	O
)	O
(	O
9.3.60	O
)	O
greater	O
than	O
1	O
,	O
we	O
assume	O
that	O
conditional	B
independence	O
holds	O
,	O
otherwise	O
we	O
assume	O
the	O
variables	O
are	O
conditionally	O
dependent	O
.	O
democondindepemp.m	O
suggests	O
that	O
the	O
bayesisan	O
hypothesis	O
test	O
tends	O
to	O
out-	O
perform	O
the	O
conditional	B
mutual	I
information	I
approach	O
,	O
particularly	O
in	O
the	O
small	O
sample	B
size	O
case	O
,	O
see	O
ﬁg	O
(	O
9.12	O
)	O
.	O
9.3.7	O
network	B
scoring	I
an	O
alternative	O
to	O
local	B
methods	O
such	O
as	O
the	O
pc	O
algorithm	B
is	O
to	O
evaluate	O
the	O
whole	O
network	O
structure	O
.	O
in	O
a	O
probabilistic	B
context	O
,	O
given	O
a	O
model	B
structure	O
m	O
,	O
we	O
wish	O
to	O
compute	O
p	O
(	O
m|d	O
)	O
∝	O
p	O
(	O
d|m	O
)	O
p	O
(	O
m	O
)	O
.	O
some	O
care	O
is	O
needed	O
here	O
since	O
we	O
have	O
to	O
ﬁrst	O
‘	O
ﬁt	O
’	O
each	O
model	B
with	O
parameters	O
θ	O
,	O
p	O
(	O
v|θ	O
,	O
m	O
)	O
to	O
the	O
data	B
d.	O
if	O
we	O
do	O
this	O
using	O
maximum	B
likelihood	I
alone	O
,	O
with	O
no	O
constraints	O
on	O
θ	O
,	O
we	O
will	O
always	O
end	O
up	O
favouring	O
that	O
model	B
m	O
with	O
the	O
most	O
complex	O
structure	B
(	O
assuming	O
p	O
(	O
m	O
)	O
=	O
const.	O
)	O
.	O
this	O
can	O
be	O
remedied	O
by	O
using	O
the	O
bayesian	O
technique	O
(	O
cid:90	O
)	O
p	O
(	O
d|m	O
)	O
=	O
θ	O
p	O
(	O
d|θ	O
,	O
m	O
)	O
p	O
(	O
θ|m	O
)	O
(	O
9.3.61	O
)	O
in	O
the	O
case	O
of	O
directed	B
networks	O
,	O
however	O
,	O
as	O
we	O
saw	O
in	O
section	O
(	O
9.3	O
)	O
,	O
the	O
assumptions	O
of	O
local	B
and	O
global	B
parameter	O
independence	B
make	O
the	O
integrals	O
tractable	O
.	O
for	O
a	O
discrete	B
state	O
network	O
and	O
dirichlet	O
priors	O
,	O
we	O
have	O
p	O
(	O
d|m	O
)	O
given	O
explicitly	O
by	O
the	O
bayesian	O
dirichlet	O
score	O
equation	B
(	O
9.3.54	O
)	O
.	O
first	O
we	O
specify	O
the	O
hyperparameters	O
u	O
(	O
v	O
;	O
j	O
)	O
,	O
and	O
then	O
search	O
over	O
structures	O
m	O
,	O
to	O
ﬁnd	O
the	O
one	O
with	O
the	O
best	O
score	O
p	O
(	O
d|m	O
)	O
.	O
the	O
simplest	O
setting	O
for	O
the	O
hyperparameters	O
is	O
set	O
them	O
all	O
to	O
unity	O
[	O
66	O
]	O
.	O
another	O
setting	O
is	O
the	O
‘	O
uninformative	O
prior	B
’	O
[	O
52	O
]	O
ui	O
(	O
v	O
;	O
j	O
)	O
=	O
α	O
dim	O
v	O
dim	O
pa	O
(	O
v	O
)	O
184	O
(	O
9.3.62	O
)	O
draft	O
march	O
9	O
,	O
2010	O
xyz1z2	O
maximum	B
likelihood	I
for	O
undirected	B
models	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
(	O
a	O
)	O
:	O
the	O
correct	O
structure	B
in	O
which	O
all	O
figure	O
9.13	O
:	O
learning	B
the	O
structure	B
of	O
a	O
bayesian	O
network	O
.	O
variables	O
are	O
binary	O
.	O
the	O
ancestral	B
order	I
is	O
2	O
,	O
1	O
,	O
5	O
,	O
4	O
,	O
3	O
,	O
8	O
,	O
7	O
,	O
6.	O
the	O
dataset	O
is	O
formed	O
from	O
1000	O
samples	O
(	O
b	O
)	O
:	O
the	O
learned	O
structure	B
based	O
on	O
the	O
pc	O
algorithm	B
using	O
the	O
bayesian	O
empirical	B
from	O
this	O
network	O
.	O
conditional	B
independence	O
test	O
.	O
undirected	B
edges	O
may	O
be	O
oriented	O
arbitrarily	O
.	O
(	O
c	O
)	O
:	O
the	O
learned	O
structure	B
based	O
on	O
the	O
bayes	O
dirichlet	O
network	B
scoring	I
method	O
.	O
see	O
demopcdata.m	O
and	O
demobdscore.m	O
.	O
where	O
dim	O
x	O
is	O
the	O
number	O
of	O
states	O
of	O
the	O
variable	B
(	O
s	O
)	O
x	O
,	O
giving	O
rise	O
to	O
the	O
bdeu	O
score	O
,	O
for	O
an	O
‘	O
equivalent	B
sample	O
size	O
’	O
parameter	B
α.	O
a	O
discussion	O
of	O
these	O
settings	O
is	O
given	O
in	O
[	O
129	O
]	O
under	O
the	O
concept	O
of	O
likelihood	B
equivalence	O
,	O
namely	O
that	O
two	O
networks	O
which	O
are	O
markov	O
equivalent	B
should	O
have	O
the	O
same	O
score	O
.	O
how	O
dense	O
the	O
resulting	O
network	O
is	O
can	O
be	O
sensitive	O
to	O
α	O
[	O
263	O
,	O
250	O
,	O
262	O
]	O
.	O
including	O
an	O
explicit	O
prior	B
p	O
(	O
m	O
)	O
on	O
the	O
networks	O
to	O
favour	O
those	O
with	O
sparse	O
connections	O
is	O
also	O
a	O
sensible	O
idea	O
,	O
for	O
which	O
one	O
modiﬁed	O
the	O
score	O
to	O
p	O
(	O
d|m	O
)	O
p	O
(	O
m	O
)	O
.	O
searching	O
over	O
structures	O
is	O
a	O
computationally	O
demanding	O
task	O
.	O
however	O
,	O
since	O
the	O
log-score	O
decomposes	O
into	O
terms	O
involving	O
each	O
family	B
of	O
v	O
,	O
we	O
can	O
compare	O
two	O
networks	O
diﬀering	O
in	O
a	O
single	O
arc	O
eﬃciently	O
.	O
search	O
heuristics	O
based	O
on	O
local	B
addition/removal/reversal	O
of	O
links	O
[	O
66	O
,	O
129	O
]	O
that	O
increase	O
the	O
score	O
are	O
popular	O
[	O
129	O
]	O
.	O
in	O
learnbayesnet.m	O
we	O
simplify	O
the	O
problem	B
for	O
demonstration	O
purposes	O
in	O
which	O
we	O
assume	O
we	O
know	O
the	O
ancestral	B
order	I
of	O
the	O
variables	O
,	O
and	O
also	O
the	O
maximal	O
number	O
of	O
parents	B
of	O
each	O
variable	B
.	O
example	O
42	O
(	O
pc	O
algorithm	B
versus	O
network	B
scoring	I
)	O
.	O
in	O
ﬁg	O
(	O
9.13	O
)	O
we	O
compare	O
the	O
pc	O
algorithm	B
with	O
bd	O
network	B
scoring	I
based	O
(	O
with	O
dirichlet	O
hyperparameters	O
set	O
to	O
unity	O
)	O
on	O
1000	O
samples	O
from	O
a	O
known	O
belief	B
network	I
.	O
the	O
pc	O
algorithm	B
conditional	O
independence	B
test	O
is	O
based	O
on	O
the	O
bayesian	O
factor	B
(	O
9.3.60	O
)	O
in	O
which	O
dirichlet	O
priors	O
with	O
α	O
=	O
0.1	O
were	O
used	O
throughout	O
.	O
in	O
ﬁg	O
(	O
9.13	O
)	O
the	O
network	B
scoring	I
technique	O
outperforms	O
the	O
pc	O
algorithm	B
.	O
this	O
is	O
partly	O
explained	O
by	O
the	O
network	B
scoring	I
technique	O
being	O
provided	O
with	O
the	O
correct	O
ancestral	B
order	I
and	O
the	O
constraint	O
that	O
each	O
variable	B
has	O
maximally	O
two	O
parents	B
.	O
9.4	O
maximum	B
likelihood	I
for	O
undirected	B
models	O
consider	O
a	O
markov	O
network	O
distribution	O
p	O
(	O
x	O
)	O
deﬁned	O
on	O
(	O
not	O
necessarily	O
maximal	O
)	O
cliques	O
xc	O
⊆	O
x	O
p	O
(	O
x|θ	O
)	O
=	O
1	O
z	O
(	O
θ	O
)	O
φc	O
(	O
xc|θc	O
)	O
(	O
cid:89	O
)	O
c	O
where	O
z	O
(	O
θ	O
)	O
=	O
(	O
cid:88	O
)	O
x	O
(	O
cid:89	O
)	O
c	O
φc	O
(	O
xc|θc	O
)	O
draft	O
march	O
9	O
,	O
2010	O
(	O
9.4.1	O
)	O
(	O
9.4.2	O
)	O
185	O
123456781234567812345678	O
maximum	B
likelihood	I
for	O
undirected	B
models	O
ensures	O
normalisation	B
.	O
given	O
a	O
set	O
of	O
data	B
,	O
x	O
n	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
and	O
assuming	O
i.i.d	O
.	O
data	B
,	O
the	O
log	O
likelihood	B
is	O
log	O
φc	O
(	O
x	O
n	O
c	O
|θc	O
)	O
−	O
n	O
log	O
z	O
(	O
θ	O
)	O
(	O
9.4.3	O
)	O
l	O
(	O
θ	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
n	O
c	O
in	O
general	O
learning	B
the	O
optimal	O
parameters	O
θc	O
,	O
c	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
c	O
is	O
awkward	O
since	O
they	O
are	O
coupled	B
via	O
z	O
(	O
θ	O
)	O
.	O
unlike	O
the	O
bn	O
,	O
the	O
objective	O
function	B
does	O
not	O
split	O
into	O
a	O
set	O
of	O
isolated	O
parameter	B
terms	O
and	O
in	O
general	O
we	O
need	O
to	O
resort	O
to	O
numerical	B
methods	O
.	O
in	O
special	O
cases	O
,	O
however	O
,	O
exact	O
results	O
still	O
apply	O
,	O
in	O
particular	O
when	O
the	O
mn	O
is	O
decomposable	B
and	O
no	O
constraints	O
are	O
placed	O
on	O
the	O
form	O
of	O
the	O
clique	B
potentials	O
,	O
as	O
we	O
discuss	O
in	O
section	O
(	O
9.4.2	O
)	O
.	O
more	O
generally	O
,	O
however	O
,	O
gradient	B
based	O
techniques	O
may	O
be	O
used	O
and	O
also	O
give	O
insight	O
into	O
properties	B
of	O
the	O
maximum	B
likelihood	I
solution	O
.	O
9.4.1	O
the	O
likelihood	B
gradient	O
l	O
(	O
θ	O
)	O
=	O
(	O
cid:88	O
)	O
∂	O
∂θc	O
∂	O
∂θc	O
log	O
z	O
(	O
θ	O
)	O
=	O
where	O
we	O
used	O
the	O
result	O
c	O
|θc	O
)	O
−	O
n	O
(	O
cid:28	O
)	O
∂	O
φc	O
(	O
xc|θc	O
)	O
(	O
cid:89	O
)	O
c	O
(	O
cid:54	O
)	O
=c	O
(	O
cid:48	O
)	O
∂θc	O
∂	O
∂θc	O
n	O
log	O
φc	O
(	O
x	O
n	O
(	O
cid:88	O
)	O
∂	O
∂θc	O
1	O
z	O
(	O
θ	O
)	O
x	O
(	O
cid:29	O
)	O
log	O
φc	O
(	O
xc|θc	O
)	O
(	O
cid:28	O
)	O
∂	O
p	O
(	O
xc|θ	O
)	O
φc	O
(	O
cid:48	O
)	O
(	O
xc	O
(	O
cid:48	O
)	O
|θc	O
(	O
cid:48	O
)	O
)	O
=	O
log	O
φc	O
(	O
xc|θc	O
)	O
∂θc	O
(	O
cid:29	O
)	O
p	O
(	O
xc|θ	O
)	O
the	O
gradient	B
can	O
then	O
be	O
used	O
as	O
part	O
of	O
a	O
standard	O
numerical	O
optimisation	B
package	O
.	O
exponential	B
form	O
potentials	O
a	O
common	O
form	O
of	O
parameterisation	B
is	O
to	O
use	O
an	O
exponential	B
form	O
φc	O
(	O
xc	O
)	O
=	O
eθtψc	O
(	O
xc	O
)	O
(	O
9.4.4	O
)	O
(	O
9.4.5	O
)	O
(	O
9.4.6	O
)	O
where	O
θ	O
are	O
the	O
parameters	O
and	O
ψc	O
(	O
xc	O
)	O
is	O
a	O
ﬁxed	O
‘	O
feature	O
function	O
’	O
deﬁned	O
on	O
the	O
variables	O
of	O
clique	B
c.	O
diﬀerentiating	O
with	O
respect	O
to	O
θ	O
and	O
equating	O
to	O
zero	O
,	O
we	O
obtain	O
that	O
the	O
maximum	B
likelihood	I
solution	O
satisﬁes	O
that	O
the	O
empirical	B
average	O
of	O
a	O
feature	O
function	O
matches	O
the	O
average	B
of	O
the	O
feature	O
function	O
with	O
respect	O
to	O
the	O
model	B
:	O
(	O
cid:104	O
)	O
ψc	O
(	O
xc	O
)	O
(	O
cid:105	O
)	O
	O
(	O
xc	O
)	O
=	O
(	O
cid:104	O
)	O
ψc	O
(	O
xc	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
xc	O
)	O
(	O
9.4.7	O
)	O
1	O
n	O
(	O
cid:93	O
)	O
(	O
xc	O
)	O
	O
(	O
xc	O
)	O
=	O
(	O
9.4.8	O
)	O
where	O
(	O
cid:93	O
)	O
(	O
xc	O
)	O
is	O
the	O
number	O
of	O
times	O
the	O
clique	B
state	O
xc	O
is	O
observed	O
in	O
the	O
dataset	O
.	O
an	O
intuitive	O
interpretation	O
is	O
to	O
sample	B
states	O
x	O
from	O
the	O
trained	O
model	B
p	O
(	O
x	O
)	O
and	O
use	O
these	O
to	O
compute	O
the	O
average	B
of	O
each	O
feature	O
function	O
.	O
in	O
the	O
limit	O
of	O
an	O
inﬁnite	O
number	O
of	O
samples	O
,	O
for	O
a	O
maximum	B
likelihood	I
optimal	O
model	B
,	O
these	O
sample	B
averages	O
will	O
match	O
those	O
based	O
on	O
the	O
empirical	B
average	O
.	O
unconstrained	O
potentials	O
for	O
unconstrained	O
potentials	O
we	O
have	O
a	O
separate	O
table	O
for	O
each	O
of	O
the	O
states	O
deﬁned	O
on	O
the	O
clique	B
.	O
this	O
means	O
we	O
may	O
write	O
φ	O
(	O
yc	O
)	O
i	O
[	O
yc=x	O
n	O
c	O
]	O
(	O
9.4.9	O
)	O
c	O
)	O
=	O
(	O
cid:89	O
)	O
yc	O
φc	O
(	O
x	O
n	O
where	O
the	O
product	O
is	O
over	O
all	O
states	O
of	O
potential	B
c.	O
this	O
expression	O
follows	O
since	O
the	O
indicator	O
is	O
zero	O
for	O
all	O
but	O
the	O
single	O
observed	O
state	O
x	O
n	O
i	O
[	O
yc	O
=	O
x	O
n	O
c	O
]	O
log	O
φc	O
(	O
yc	O
)	O
−	O
n	O
log	O
z	O
(	O
φ	O
)	O
l	O
(	O
θ	O
)	O
=	O
(	O
cid:88	O
)	O
c	O
.	O
the	O
log	O
likelihood	B
is	O
then	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
(	O
9.4.10	O
)	O
c	O
yc	O
n	O
186	O
draft	O
march	O
9	O
,	O
2010	O
maximum	B
likelihood	I
for	O
undirected	B
models	O
ψ	O
(	O
x1	O
,	O
x2	O
)	O
ψ	O
(	O
x2	O
)	O
ψ	O
(	O
x2	O
,	O
x3	O
,	O
x5	O
)	O
ψ	O
(	O
x5	O
)	O
ψ	O
(	O
x5	O
,	O
x6	O
)	O
x1	O
x2	O
x3	O
x4	O
(	O
a	O
)	O
x5	O
x6	O
ψ	O
(	O
x2	O
,	O
x5	O
)	O
ψ	O
(	O
x2	O
,	O
x4	O
,	O
x5	O
)	O
(	O
b	O
)	O
x1	O
x2	O
,	O
x3	O
,	O
x5	O
x6	O
x4	O
(	O
c	O
)	O
figure	O
9.14	O
:	O
(	O
a	O
)	O
:	O
a	O
decomposable	B
markov	O
network	O
.	O
(	O
c	O
)	O
:	O
set	B
chain	I
for	O
(	O
a	O
)	O
formed	O
by	O
choosing	O
clique	B
x2	O
,	O
x3	O
,	O
x5	O
as	O
root	O
and	O
orienting	O
edges	O
consistently	O
away	O
from	O
the	O
root	O
.	O
each	O
separator	B
is	O
absorbed	O
into	O
its	O
child	O
clique	B
to	O
form	O
the	O
set	B
chain	I
.	O
(	O
b	O
)	O
:	O
a	O
junction	B
tree	I
for	O
(	O
a	O
)	O
.	O
where	O
(	O
cid:89	O
)	O
c	O
φc	O
(	O
yc	O
)	O
z	O
(	O
φ	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
y	O
i	O
[	O
yc	O
=	O
x	O
n	O
c	O
]	O
n	O
p	O
(	O
yc	O
)	O
φc	O
(	O
yc	O
)	O
1	O
φc	O
(	O
yc	O
)	O
−	O
n	O
(	O
cid:88	O
)	O
1	O
n	O
n	O
p	O
(	O
yc	O
)	O
=	O
	O
(	O
yc	O
)	O
≡	O
i	O
[	O
yc	O
=	O
x	O
n	O
c	O
]	O
diﬀerentiating	O
the	O
log	O
likelihood	B
with	O
respect	O
to	O
a	O
speciﬁc	O
table	O
entry	O
φ	O
(	O
yc	O
)	O
we	O
obtain	O
equating	O
to	O
zero	O
,	O
the	O
maximum	B
likelihood	I
solution	O
is	O
obtained	O
when	O
that	O
is	O
,	O
the	O
unconstrained	O
optimal	O
maximum	B
likelihood	I
solution	O
is	O
given	O
by	O
setting	O
the	O
clique	B
potentials	O
such	O
that	O
the	O
marginal	B
distribution	O
on	O
each	O
clique	B
p	O
(	O
yc	O
)	O
matches	O
the	O
empirical	B
distribution	I
on	O
each	O
clique	B
	O
(	O
yc	O
)	O
.	O
9.4.2	O
decomposable	B
markov	O
networks	O
in	O
the	O
case	O
that	O
there	O
is	O
no	O
constraint	O
placed	O
on	O
the	O
form	O
of	O
the	O
factors	O
φc	O
and	O
if	O
the	O
mn	O
corresponding	O
to	O
these	O
potentials	O
is	O
decomposable	B
,	O
then	O
we	O
know	O
(	O
from	O
the	O
junction	B
tree	I
representation	O
)	O
that	O
we	O
can	O
express	O
the	O
distribution	B
in	O
the	O
form	O
of	O
a	O
product	O
of	O
local	B
marginals	O
divided	O
by	O
the	O
separator	B
distributions	O
(	O
9.4.11	O
)	O
(	O
9.4.12	O
)	O
(	O
9.4.13	O
)	O
(	O
9.4.14	O
)	O
(	O
9.4.15	O
)	O
p	O
(	O
x	O
)	O
=	O
(	O
cid:81	O
)	O
(	O
cid:81	O
)	O
c	O
p	O
(	O
xc	O
)	O
s	O
p	O
(	O
xs	O
)	O
p	O
(	O
x	O
)	O
=	O
(	O
cid:89	O
)	O
c	O
p	O
(	O
xc|xs	O
)	O
by	O
reabsorbing	O
the	O
separators	O
into	O
the	O
numerator	O
terms	O
,	O
we	O
can	O
form	O
a	O
set	B
chain	I
distribution	O
,	O
section	O
(	O
6.8	O
)	O
since	O
this	O
is	O
directed	B
,	O
the	O
maximum	B
likelihood	I
solution	O
to	O
learning	B
the	O
tables	O
is	O
trivial	O
since	O
we	O
assign	O
each	O
set	B
chain	I
factor	O
p	O
(	O
xc|xs	O
)	O
by	O
counting	B
the	O
instances	O
in	O
the	O
dataset	O
[	O
168	O
]	O
,	O
see	O
learnmarkovdecom.m	O
.	O
the	O
procedure	O
is	O
perhaps	O
best	O
explained	O
by	O
an	O
example	O
,	O
as	O
given	O
below	O
.	O
see	O
algorithm	O
(	O
5	O
)	O
for	O
a	O
general	O
description	O
.	O
example	O
43.	O
given	O
a	O
dataset	O
v	O
=	O
{	O
x	O
n	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
,	O
we	O
wish	O
to	O
ﬁt	O
by	O
maximum	B
likelihood	I
a	O
mn	O
of	O
the	O
form	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
x6	O
)	O
=	O
1	O
z	O
φ	O
(	O
x1	O
,	O
x2	O
)	O
φ	O
(	O
x2	O
,	O
x3	O
,	O
x5	O
)	O
φ	O
(	O
x2	O
,	O
x4	O
,	O
x5	O
)	O
φ	O
(	O
x5	O
,	O
x6	O
)	O
draft	O
march	O
9	O
,	O
2010	O
(	O
9.4.16	O
)	O
187	O
maximum	B
likelihood	I
for	O
undirected	B
models	O
algorithm	B
5	O
learning	B
of	O
an	O
unconstrained	O
decomposable	B
markov	O
network	O
using	O
maximum	B
likelihood	I
.	O
we	O
have	O
a	O
triangulated	B
(	O
decomposable	B
)	O
markov	O
network	O
on	O
cliques	O
φc	O
(	O
xc	O
)	O
,	O
c	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
c	O
and	O
the	O
empirical	B
marginal	O
distributions	O
on	O
all	O
cliques	O
and	O
separators	O
,	O
	O
(	O
xc	O
)	O
,	O
	O
(	O
xs	O
)	O
1	O
:	O
form	O
a	O
junction	B
tree	I
from	O
the	O
cliques	O
.	O
2	O
:	O
initialise	O
each	O
clique	B
ψc	O
(	O
xc	O
)	O
to	O
	O
(	O
xc	O
)	O
and	O
each	O
separator	B
ψs	O
(	O
xs	O
)	O
to	O
	O
(	O
xs	O
)	O
.	O
3	O
:	O
choose	O
a	O
root	O
clique	B
on	O
the	O
junction	B
tree	I
and	O
orient	O
edges	O
consistently	O
away	O
from	O
this	O
root	O
.	O
4	O
:	O
for	O
this	O
oriented	O
junction	B
tree	I
,	O
divide	O
each	O
clique	B
by	O
its	O
parent	O
separator	B
.	O
5	O
:	O
return	O
the	O
new	O
potentials	O
on	O
each	O
clique	B
as	O
the	O
maximum	B
likelihood	I
solution	O
.	O
where	O
the	O
potentials	O
are	O
unconstrained	O
tables	O
,	O
see	O
ﬁg	O
(	O
9.14a	O
)	O
.	O
since	O
the	O
graph	B
is	O
decomposable	B
,	O
we	O
know	O
it	O
admits	O
a	O
factorisation	O
of	O
clique	B
potentials	O
divided	O
by	O
the	O
separators	O
:	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
x6	O
)	O
=	O
p	O
(	O
x1	O
,	O
x2	O
)	O
p	O
(	O
x2	O
,	O
x3	O
,	O
x5	O
)	O
p	O
(	O
x2	O
,	O
x4	O
,	O
x5	O
)	O
p	O
(	O
x5	O
,	O
x6	O
)	O
p	O
(	O
x2	O
)	O
p	O
(	O
x2	O
,	O
x5	O
)	O
p	O
(	O
x5	O
)	O
(	O
9.4.17	O
)	O
we	O
can	O
convert	O
this	O
to	O
a	O
set	B
chain	I
by	O
reabsorbing	O
the	O
denominators	O
into	O
numerator	O
terms	O
,	O
see	O
section	O
(	O
6.8	O
)	O
.	O
for	O
example	O
,	O
by	O
choosing	O
the	O
clique	B
x2	O
,	O
x3	O
,	O
x5	O
as	O
root	O
,	O
we	O
can	O
write	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
x6	O
)	O
=	O
p	O
(	O
x1|x2	O
)	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
p	O
(	O
x2	O
,	O
x3	O
,	O
x5	O
)	O
ψ	O
(	O
x1	O
,	O
x2	O
)	O
ψ	O
(	O
x2	O
,	O
x3	O
,	O
x5	O
)	O
(	O
cid:125	O
)	O
(	O
cid:124	O
)	O
(	O
cid:125	O
)	O
p	O
(	O
x4|x2	O
,	O
x5	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
ψ	O
(	O
x2	O
,	O
x4	O
,	O
x5	O
)	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
p	O
(	O
x6|x5	O
)	O
ψ	O
(	O
x5	O
,	O
x6	O
)	O
(	O
9.4.18	O
)	O
where	O
we	O
identiﬁed	O
the	O
factors	O
with	O
clique	O
potentials	O
,	O
and	O
the	O
normalisation	B
constant	I
z	O
is	O
unity	O
,	O
see	O
ﬁg	O
(	O
9.14b	O
)	O
.	O
the	O
advantage	O
is	O
that	O
in	O
this	O
representation	B
,	O
the	O
clique	B
potentials	O
are	O
independent	O
since	O
the	O
distribution	B
is	O
a	O
bn	O
on	O
cluster	O
variables	O
.	O
the	O
log	O
likelihood	B
for	O
an	O
i.i.d	O
.	O
dataset	O
x	O
=	O
{	O
xn	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
is	O
log	O
p	O
(	O
xn	O
1|xn	O
2	O
)	O
+	O
log	O
p	O
(	O
xn	O
2	O
,	O
xn	O
3	O
,	O
xn	O
5	O
)	O
+	O
log	O
p	O
(	O
xn	O
2	O
,	O
xn	O
5	O
)	O
+	O
log	O
p	O
(	O
xn	O
4|xn	O
6|xn	O
5	O
)	O
(	O
9.4.19	O
)	O
l	O
=	O
(	O
cid:88	O
)	O
n	O
where	O
each	O
of	O
the	O
terms	O
is	O
an	O
independent	O
parameter	O
of	O
the	O
model	B
.	O
the	O
maximum	B
likelihood	I
solution	O
then	O
corresponds	O
(	O
as	O
for	O
the	O
bn	O
case	O
)	O
to	O
simply	O
setting	O
each	O
factor	B
to	O
the	O
datacounts	O
.	O
for	O
example	O
φ	O
(	O
x2	O
,	O
x4	O
,	O
x5	O
)	O
=	O
p	O
(	O
x4|x2	O
,	O
x5	O
)	O
=	O
(	O
cid:93	O
)	O
(	O
x2	O
,	O
x4	O
,	O
x5	O
)	O
(	O
cid:93	O
)	O
(	O
x2	O
,	O
x5	O
)	O
,	O
φ	O
(	O
x2	O
,	O
x3	O
,	O
x5	O
)	O
=	O
p	O
(	O
x2	O
,	O
x3	O
,	O
x5	O
)	O
=	O
(	O
cid:93	O
)	O
(	O
x2	O
,	O
x3	O
,	O
x5	O
)	O
n	O
(	O
9.4.20	O
)	O
9.4.3	O
non-decomposable	O
markov	O
networks	O
in	O
the	O
non-decomposable	O
or	O
constrained	O
case	O
,	O
no	O
closed	O
form	O
maximum	B
likelihood	I
solution	O
generally	O
exists	O
and	O
one	O
needs	O
to	O
resort	O
to	O
numerical	B
methods	O
.	O
according	O
to	O
equation	B
(	O
9.4.13	O
)	O
the	O
maximum	B
likelihood	I
solution	O
is	O
such	O
that	O
the	O
clique	B
marginals	O
match	O
the	O
empirical	B
marginals	O
.	O
assuming	O
that	O
we	O
can	O
absorb	O
the	O
normalisation	B
constant	I
into	O
an	O
arbitrarily	O
chosen	O
clique	B
,	O
we	O
can	O
drop	O
explicitly	O
representing	O
the	O
normalisation	B
constant	I
.	O
for	O
a	O
clique	B
c	O
,	O
the	O
requirement	O
that	O
the	O
marginal	B
of	O
p	O
matches	O
the	O
empirical	B
marginal	O
on	O
the	O
variables	O
in	O
the	O
clique	B
is	O
φ	O
(	O
xd	O
)	O
=	O
	O
(	O
xc	O
)	O
(	O
9.4.21	O
)	O
φ	O
(	O
xc	O
)	O
(	O
cid:88	O
)	O
x\c	O
(	O
cid:89	O
)	O
d	O
(	O
cid:54	O
)	O
=c	O
given	O
an	O
initial	O
setting	O
for	O
the	O
potentials	O
we	O
can	O
then	O
update	O
φ	O
(	O
xc	O
)	O
to	O
satisfy	O
the	O
above	O
marginal	B
require-	O
ment	O
,	O
(	O
cid:80	O
)	O
x\c	O
(	O
cid:81	O
)	O
	O
(	O
xc	O
)	O
d	O
(	O
cid:54	O
)	O
=c	O
φ	O
(	O
xd	O
)	O
φnew	O
(	O
xc	O
)	O
=	O
188	O
(	O
9.4.22	O
)	O
draft	O
march	O
9	O
,	O
2010	O
maximum	B
likelihood	I
for	O
undirected	B
models	O
which	O
is	O
required	O
for	O
each	O
of	O
the	O
states	O
of	O
xc	O
.	O
by	O
multiplying	O
and	O
dividing	O
the	O
right	O
hand	O
side	O
by	O
φ	O
(	O
xc	O
)	O
this	O
is	O
equivalent	B
to	O
φnew	O
(	O
xc	O
)	O
=	O
φ	O
(	O
xc	O
)	O
	O
(	O
xc	O
)	O
p	O
(	O
xc	O
)	O
one	O
can	O
view	O
this	O
ipf	O
update	O
as	O
coordinate-wise	O
optimisation	B
of	O
the	O
log	O
likelihood	B
in	O
which	O
the	O
coor-	O
dinate	O
corresponds	O
to	O
φc	O
(	O
xc	O
)	O
,	O
with	O
all	O
other	O
parameters	O
ﬁxed	O
.	O
in	O
this	O
case	O
this	O
conditional	B
optimum	O
is	O
analytically	O
given	O
by	O
the	O
above	O
setting	O
.	O
one	O
proceeds	O
by	O
selecting	O
another	O
potential	B
to	O
update	O
.	O
note	O
that	O
in	O
general	O
,	O
with	O
each	O
update	O
,	O
the	O
marginal	B
p	O
(	O
xc	O
)	O
need	O
to	O
be	O
recomputed	O
.	O
computing	O
these	O
marginals	O
may	O
be	O
expensive	O
unless	O
the	O
width	O
of	O
the	O
junction	B
tree	I
formed	O
from	O
the	O
graph	B
is	O
suitably	O
limited	O
.	O
example	O
44	O
(	O
boltzmann	O
machine	O
learning	B
)	O
.	O
we	O
deﬁne	O
the	O
bm	O
as	O
1	O
1	O
2	O
vtwv	O
,	O
p	O
(	O
v|w	O
)	O
=	O
z	O
(	O
w	O
)	O
e	O
for	O
symmetric	O
w	O
and	O
binary	O
variables	O
dom	O
(	O
vi	O
)	O
=	O
{	O
0	O
,	O
1	O
}	O
.	O
given	O
a	O
set	O
of	O
training	B
data	O
,	O
d	O
=	O
(	O
cid:8	O
)	O
v1	O
,	O
.	O
.	O
.	O
,	O
vn	O
(	O
cid:9	O
)	O
,	O
e	O
v	O
(	O
9.4.24	O
)	O
1	O
2	O
vtwv	O
the	O
log	O
likelihood	B
is	O
z	O
(	O
w	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
9.4.23	O
)	O
(	O
9.4.25	O
)	O
(	O
9.4.26	O
)	O
(	O
9.4.27	O
)	O
n	O
(	O
cid:88	O
)	O
n=1	O
n	O
(	O
cid:88	O
)	O
(	O
cid:16	O
)	O
n=1	O
l	O
(	O
w	O
)	O
=	O
1	O
2	O
(	O
vn	O
)	O
t	O
wvn	O
−	O
n	O
log	O
z	O
(	O
w	O
)	O
diﬀerentiating	O
w.r.t	O
.	O
wij	O
,	O
i	O
(	O
cid:54	O
)	O
=	O
j	O
we	O
have	O
the	O
gradient	B
∂l	O
∂wij	O
=	O
vn	O
i	O
vn	O
j	O
−	O
(	O
cid:104	O
)	O
vivj	O
(	O
cid:105	O
)	O
p	O
(	O
v|w	O
)	O
(	O
cid:17	O
)	O
a	O
simple	O
algorithm	O
to	O
optimise	O
the	O
weight	B
matrix	O
w	O
is	O
to	O
use	O
gradient	B
ascent	O
,	O
n	O
(	O
cid:88	O
)	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
wnew	O
ij	O
=	O
wold	O
ij	O
+	O
η	O
vn	O
i	O
vn	O
j	O
−	O
(	O
cid:104	O
)	O
vivj	O
(	O
cid:105	O
)	O
p	O
(	O
v|w	O
)	O
n=1	O
the	O
second	O
order	O
statistics	O
of	O
the	O
model	B
(	O
cid:104	O
)	O
vivj	O
(	O
cid:105	O
)	O
p	O
(	O
v|w	O
)	O
match	O
those	O
of	O
the	O
empirical	B
distribution	I
,	O
(	O
cid:80	O
)	O
for	O
a	O
learning	B
rate	I
η	O
>	O
0.	O
the	O
intuitive	O
interpretation	O
is	O
that	O
learning	B
will	O
stop	O
(	O
the	O
gradient	B
is	O
zero	O
)	O
when	O
j	O
/n	O
.	O
bm	O
learning	B
however	O
is	O
diﬃcult	O
since	O
(	O
cid:104	O
)	O
vivj	O
(	O
cid:105	O
)	O
p	O
(	O
v|w	O
)	O
is	O
typically	O
computationally	O
intractable	O
for	O
an	O
arbitrary	O
interaction	O
matrix	B
w	O
and	O
therefore	O
needs	O
to	O
be	O
approximated	O
.	O
indeed	O
,	O
one	O
can	O
not	O
compute	O
the	O
likelihood	B
l	O
(	O
w	O
)	O
exactly	O
so	O
that	O
monitoring	O
performance	B
is	O
also	O
diﬃcult	O
.	O
n	O
vn	O
i	O
vn	O
9.4.4	O
constrained	O
decomposable	O
markov	O
networks	O
if	O
there	O
are	O
no	O
constraints	O
on	O
the	O
forms	O
of	O
the	O
maximal	O
clique	B
potentials	O
of	O
the	O
markov	O
network	O
,	O
as	O
we	O
’	O
ve	O
seen	O
,	O
learning	B
is	O
straightforward	O
.	O
here	O
our	O
interest	O
is	O
when	O
the	O
functional	O
form	O
of	O
the	O
maximal	O
clique	B
is	O
constrained	O
to	O
be	O
a	O
product	O
of	O
potentials	O
on	O
smaller	O
cliques2	O
:	O
c	O
(	O
x	O
i	O
φi	O
c	O
)	O
(	O
9.4.28	O
)	O
φc	O
(	O
xc	O
)	O
=	O
(	O
cid:89	O
)	O
i	O
with	O
no	O
constraint	O
being	O
placed	O
on	O
the	O
non-maximal	O
clique	B
potentials	O
φi	O
in	O
general	O
,	O
in	O
this	O
case	O
one	O
can	O
not	O
write	O
down	O
directly	O
the	O
maximum	B
likelihood	I
solution	O
for	O
the	O
non-maximal	O
clique	B
potentials	O
c	O
)	O
.	O
c	O
(	O
x	O
i	O
φi	O
c	O
)	O
.	O
c	O
(	O
x	O
i	O
2a	O
boltzmann	O
machine	O
is	O
of	O
this	O
form	O
since	O
any	O
unconstrained	O
binary	O
pairwise	O
potentials	O
can	O
be	O
converted	O
into	O
a	O
bm	O
.	O
for	O
other	O
cases	O
in	O
which	O
the	O
φi	O
c	O
are	O
constrained	O
,	O
then	O
iterative	B
scaling	I
may	O
be	O
used	O
in	O
place	O
of	O
ipf	O
.	O
draft	O
march	O
9	O
,	O
2010	O
189	O
maximum	B
likelihood	I
for	O
undirected	B
models	O
x2	O
x4	O
x1	O
(	O
a	O
)	O
x5	O
x3	O
φ1,4φ1,5	O
x1,4	O
φ1,2φ1,4	O
x2,4	O
φ2,3φ2,4φ3,4	O
(	O
b	O
)	O
(	O
a	O
)	O
:	O
interpreted	O
as	O
9.15	O
:	O
φ	O
(	O
x1	O
,	O
x4	O
,	O
x5	O
)	O
φ	O
(	O
x1	O
,	O
x2	O
,	O
x4	O
)	O
φ	O
(	O
x2	O
,	O
x4	O
,	O
x3	O
)	O
.	O
the	O
distri-	O
figure	O
represents	O
bution	O
(	O
b	O
)	O
:	O
a	O
junction	B
tree	I
for	O
the	O
pair-	O
φ	O
(	O
x4	O
,	O
x5	O
)	O
φ	O
(	O
x1	O
,	O
x4	O
)	O
φ	O
(	O
x4	O
,	O
x5	O
)	O
φ	O
(	O
x1	O
,	O
x2	O
)	O
φ	O
(	O
x2	O
,	O
x4	O
)	O
φ	O
(	O
x2	O
,	O
x3	O
)	O
φ	O
(	O
x3	O
,	O
x4	O
)	O
.	O
wise	O
mn	O
in	O
(	O
a	O
)	O
.	O
we	O
have	O
a	O
choice	O
were	O
to	O
place	O
the	O
pairwise	B
cliques	O
,	O
and	O
this	O
is	O
one	O
valid	O
choice	O
,	O
using	O
the	O
shorthand	O
φa	O
,	O
b	O
=	O
φa	O
,	O
b	O
(	O
xa	O
,	O
xb	O
)	O
and	O
xa	O
,	O
b	O
=	O
{	O
xa	O
,	O
xb	O
}	O
.	O
graph	B
represents	O
graph	B
a	O
markov	O
network	O
,	O
pairwise	B
mn	O
,	O
the	O
the	O
as	O
a	O
consider	O
the	O
graph	B
in	O
ﬁg	O
(	O
9.15	O
)	O
.	O
in	O
the	O
constrained	O
case	O
,	O
in	O
which	O
we	O
interpret	O
the	O
graph	B
as	O
a	O
pairwise	B
mn	O
,	O
ipf	O
may	O
be	O
used	O
to	O
learn	O
the	O
pairwise	B
tables	O
.	O
since	O
the	O
graph	B
is	O
decomposable	B
,	O
there	O
are	O
however	O
,	O
computational	O
savings	O
that	O
can	O
be	O
made	O
in	O
this	O
case	O
[	O
11	O
]	O
.	O
for	O
an	O
empirical	B
distribution	I
	O
,	O
maximum	B
likelihood	I
requires	O
that	O
all	O
the	O
pairwise	B
marginals	O
of	O
the	O
mn	O
match	O
the	O
corresponding	O
marginals	O
obtained	O
from	O
	O
.	O
as	O
explained	O
in	O
ﬁg	O
(	O
9.15	O
)	O
we	O
have	O
a	O
choice	O
as	O
to	O
which	O
junction	B
tree	I
clique	O
each	O
potential	B
is	O
assigned	O
to	O
,	O
with	O
one	O
valid	O
choice	O
being	O
given	O
in	O
ﬁg	O
(	O
9.15b	O
)	O
.	O
keeping	O
the	O
potentials	O
of	O
the	O
cliques	O
φ1,4φ1,5	O
and	O
φ2,3φ2,4φ3,4	O
ﬁxed	O
we	O
can	O
update	O
the	O
potentials	O
of	O
clique	B
φ1,2φ1,4	O
.	O
using	O
a	O
bar	O
to	O
denote	O
ﬁxed	O
potentials	O
,	O
we	O
the	O
marginal	B
requirement	O
that	O
the	O
mn	O
matches	O
the	O
empirical	B
marginal	O
	O
(	O
x1	O
,	O
x2	O
)	O
can	O
be	O
written	O
in	O
shorthand	O
as	O
which	O
can	O
be	O
expressed	O
as	O
x3	O
,	O
x4	O
,	O
x5	O
p	O
(	O
x1	O
,	O
x2	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
¯φ1,5	O
¯φ4,5	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
x4	O
x5	O
γ1,4	O
(	O
cid:33	O
)	O
(	O
cid:125	O
)	O
¯φ1,5	O
¯φ4,5φ1,4φ1,2	O
¯φ2,4	O
¯φ2,3	O
¯φ3,4	O
=	O
	O
(	O
x1	O
,	O
x2	O
)	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
x3	O
¯φ2,4	O
¯φ2,3	O
¯φ3,4	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
γ2,4	O
(	O
cid:33	O
)	O
(	O
cid:125	O
)	O
=	O
	O
(	O
x1	O
,	O
x2	O
)	O
φ1,4φ1,2	O
the	O
‘	O
messages	O
’	O
γ1,4	O
and	O
γ1,2	O
are	O
the	O
boundary	B
separator	O
tables	O
when	O
we	O
choose	O
the	O
central	O
clique	B
as	O
root	O
and	O
carry	O
out	O
absorption	B
towards	O
the	O
root	O
.	O
given	O
these	O
ﬁxed	O
messages	O
we	O
can	O
then	O
perform	O
updates	O
of	O
the	O
root	O
clique	B
using	O
(	O
9.4.29	O
)	O
(	O
9.4.30	O
)	O
(	O
9.4.31	O
)	O
(	O
9.4.32	O
)	O
(	O
9.4.33	O
)	O
after	O
making	O
this	O
update	O
,	O
we	O
can	O
subsequently	O
update	O
φ1,4	O
similarly	O
using	O
the	O
constraint	O
¯φ1,5	O
¯φ4,5	O
φ1,4φ1,2	O
=	O
	O
(	O
x1	O
,	O
x4	O
)	O
(	O
cid:33	O
)	O
(	O
cid:125	O
)	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
x3	O
(	O
cid:33	O
)	O
(	O
cid:125	O
)	O
¯φ2,4	O
¯φ2,3	O
¯φ3,4	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
γ2,4	O
φnew	O
1,2	O
=	O
	O
(	O
x1	O
,	O
x2	O
)	O
x4	O
γ1,4φ1,4γ2,4	O
(	O
cid:80	O
)	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
x5	O
(	O
cid:88	O
)	O
γ1,4	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:80	O
)	O
x2	O
so	O
that	O
φnew	O
1,4	O
=	O
	O
(	O
x1	O
,	O
x4	O
)	O
x2	O
γ1,4φ1,2γ2,4	O
given	O
converged	O
updates	O
for	O
this	O
clique	B
,	O
we	O
can	O
choose	O
another	O
clique	B
as	O
root	O
,	O
propagate	O
towards	O
the	O
root	O
and	O
compute	O
the	O
separator	B
cliques	O
on	O
the	O
boundary	B
of	O
the	O
root	O
.	O
given	O
these	O
ﬁxed	O
boundary	B
clique	O
potentials	O
we	O
perform	O
ipf	O
within	O
the	O
clique	B
.	O
190	O
draft	O
march	O
9	O
,	O
2010	O
maximum	B
likelihood	I
for	O
undirected	B
models	O
algorithm	B
6	O
eﬃcient	B
iterative	O
proportional	O
fitting	O
.	O
given	O
a	O
set	O
of	O
φi	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
i	O
and	O
a	O
corresponding	O
set	O
of	O
reference	O
(	O
empirical	B
)	O
marginal	B
distributions	O
on	O
the	O
variables	O
of	O
each	O
potential	B
,	O
i	O
,	O
we	O
aim	O
to	O
set	O
all	O
φ	O
such	O
that	O
all	O
marginals	O
of	O
the	O
markov	O
network	O
match	O
the	O
given	O
empirical	B
marginals	O
.	O
1	O
:	O
given	O
a	O
markov	O
network	O
on	O
potentials	O
φi	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
i	O
,	O
triangulate	O
the	O
graph	B
and	O
form	O
the	O
cliques	O
c1	O
,	O
.	O
.	O
.	O
,	O
cc	O
.	O
choose	O
a	O
clique	B
c	O
as	O
root	O
.	O
propagate	O
messages	O
towards	O
the	O
root	O
and	O
compute	O
the	O
separators	O
on	O
the	O
boundary	B
of	O
the	O
root	O
.	O
repeat	O
2	O
:	O
assign	O
potentials	O
to	O
cliques	O
.	O
thus	O
each	O
clique	B
has	O
a	O
set	O
of	O
associated	O
potentials	O
fc	O
3	O
:	O
initialise	O
all	O
potentials	O
(	O
for	O
example	O
to	O
unity	O
)	O
.	O
4	O
:	O
repeat	O
5	O
:	O
6	O
:	O
7	O
:	O
8	O
:	O
9	O
:	O
10	O
:	O
11	O
:	O
until	O
all	O
markov	O
network	O
marginals	O
converge	O
to	O
the	O
reference	O
marginals	O
.	O
choose	O
a	O
potential	B
φi	O
in	O
clique	B
c	O
,	O
i	O
∈	O
fc	O
.	O
perform	O
an	O
ipf	O
update	O
for	O
φi	O
,	O
given	O
ﬁxed	O
boundary	B
separators	O
and	O
other	O
potentials	O
in	O
c.	O
until	O
potentials	O
in	O
clique	B
c	O
converge	O
.	O
this	O
‘	O
eﬃcient	B
’	O
ipf	O
procedure	O
is	O
described	O
more	O
generally	O
in	O
algorithm	B
(	O
6	O
)	O
for	O
an	O
empirical	B
distribution	I
	O
.	O
more	O
generally	O
,	O
ipf	O
minimises	O
the	O
kullback-leibler	O
divergence	B
between	O
a	O
given	O
reference	O
distribution	B
	O
and	O
the	O
markov	O
network	O
.	O
see	O
demoipfeff.m	O
and	O
ipf.m	O
.	O
example	O
45.	O
in	O
ﬁg	O
(	O
9.17	O
)	O
36	O
examples	O
of	O
18	O
×	O
14	O
=	O
252	O
binary	O
pixel	O
handwritten	O
twos	O
are	O
presented	O
,	O
forming	O
the	O
training	B
set	O
from	O
which	O
we	O
wish	O
to	O
ﬁt	O
a	O
markov	O
network	O
.	O
first	O
all	O
pairwise	B
empirical	O
entropies	O
h	O
(	O
xi	O
,	O
xj	O
)	O
,	O
i	O
,	O
j	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
252	O
were	O
computed	O
and	O
used	O
to	O
rank	B
edges	O
,	O
with	O
highest	O
entropy	B
edges	O
ranked	O
ﬁrst	O
.	O
edges	O
were	O
included	O
in	O
a	O
graph	B
g	O
,	O
highest	O
ranked	O
ﬁrst	O
,	O
provided	O
the	O
triangulated	B
g	O
had	O
all	O
cliques	O
less	O
than	O
size	O
15.	O
this	O
resulted	O
in	O
238	O
unique	O
cliques	O
and	O
an	O
adjacency	B
matrix	I
for	O
the	O
triangulated	B
g	O
as	O
presented	O
in	O
ﬁg	O
(	O
9.16a	O
)	O
.	O
in	O
ﬁg	O
(	O
9.16b	O
)	O
the	O
number	O
of	O
times	O
that	O
a	O
pixel	O
appears	O
in	O
the	O
238	O
cliques	O
is	O
shown	O
,	O
and	O
indicates	O
the	O
degree	O
of	O
importance	O
of	O
each	O
pixel	O
in	O
distinguishing	O
between	O
the	O
36	O
examples	O
.	O
two	O
models	O
were	O
then	O
trained	O
and	O
used	O
to	O
compute	O
the	O
most	O
likely	O
reconstruction	O
based	O
on	O
missing	B
data	I
p	O
(	O
xmissing|xvisible	O
)	O
.	O
the	O
ﬁrst	O
model	O
was	O
a	O
markov	O
network	O
on	O
the	O
maximal	O
cliques	O
of	O
the	O
graph	B
,	O
for	O
which	O
essentially	O
no	O
training	O
is	O
required	O
,	O
and	O
the	O
settings	O
for	O
each	O
clique	B
potential	O
can	O
be	O
obtained	O
as	O
explained	O
in	O
algorithm	B
(	O
5	O
)	O
.	O
the	O
model	B
makes	O
3.8	O
%	O
errors	O
in	O
reconstruction	O
of	O
the	O
missing	B
pixels	O
.	O
note	O
that	O
the	O
unfortunate	O
eﬀect	O
of	O
reconstructing	O
a	O
white	O
pixel	O
surrounded	O
by	O
black	O
pixels	O
is	O
an	O
eﬀect	O
of	O
the	O
limited	O
training	B
data	O
.	O
with	O
larger	O
amounts	O
of	O
data	B
the	O
model	B
would	O
recognise	O
that	O
such	O
eﬀects	O
do	O
not	O
occur	O
.	O
in	O
the	O
second	O
model	B
,	O
the	O
same	O
maximal	O
cliques	O
were	O
used	O
,	O
but	O
the	O
maximal	O
clique	B
potentials	O
restricted	B
to	O
be	O
the	O
product	O
of	O
all	O
pairwise	B
two-cliques	O
within	O
the	O
maximal	O
clique	B
.	O
this	O
is	O
equivalent	B
to	O
using	O
a	O
boltzmann	O
machine	O
,	O
and	O
was	O
trained	O
using	O
the	O
eﬃcient	B
ipf	O
approach	B
of	O
algorithm	B
(	O
6	O
)	O
.	O
the	O
corresponding	O
reconstruction	O
error	O
is	O
20	O
%	O
.	O
this	O
performance	B
is	O
worse	O
than	O
the	O
unconstrained	O
network	O
since	O
the	O
boltzmann	O
machine	O
is	O
a	O
highly	O
constrained	O
markov	O
network	O
.	O
see	O
demolearndecmn.m	O
.	O
figure	O
9.16	O
:	O
(	O
a	O
)	O
:	O
based	O
on	O
the	O
pairwise	B
empirical	O
entropies	O
h	O
(	O
xi	O
,	O
xj	O
)	O
edges	O
are	O
or-	O
dered	O
,	O
high	O
entropy	O
edges	O
ﬁrst	O
.	O
shown	O
is	O
the	O
adjacency	B
matrix	I
of	O
the	O
resulting	O
markov	O
network	O
whose	O
junction	B
tree	I
has	O
cliques	O
≤	O
15	O
in	O
size	O
(	O
white	O
represents	O
an	O
(	O
b	O
)	O
:	O
indicated	O
are	O
the	O
number	O
of	O
edge	O
)	O
.	O
cliques	O
that	O
each	O
pixel	O
is	O
a	O
member	O
of	O
,	O
in-	O
dicating	O
a	O
degree	O
of	O
importance	O
.	O
note	O
that	O
the	O
lowest	O
clique	B
membership	O
value	B
is	O
1	O
,	O
so	O
that	O
each	O
pixel	O
is	O
a	O
member	O
of	O
at	O
least	O
one	O
clique	B
.	O
(	O
a	O
)	O
(	O
b	O
)	O
draft	O
march	O
9	O
,	O
2010	O
191	O
5010015020025050100150200250	O
24681012142468101214161820406080100120140160180200220	O
maximum	B
likelihood	I
for	O
undirected	B
models	O
figure	O
9.17	O
:	O
learning	B
digits	O
(	O
from	O
simon	O
lucas	O
’	O
algoval	O
system	B
)	O
using	O
a	O
markov	O
network	O
.	O
top	O
row	O
:	O
the	O
36	O
training	B
examples	O
.	O
each	O
example	O
is	O
a	O
binary	O
image	O
on	O
18	O
×	O
14	O
pixels	O
.	O
second	O
row	O
:	O
the	O
training	B
data	O
with	O
50	O
%	O
missing	B
pixels	O
(	O
grey	O
represents	O
a	O
missing	B
pixel	O
)	O
.	O
third	O
row	O
:	O
reconstructions	O
from	O
the	O
missing	B
data	I
using	O
a	O
thin-junction-tree	O
mn	O
with	O
maximum	O
clique	B
size	O
15.	O
bottom	O
row	O
:	O
reconstructions	O
using	O
a	O
thin-junction-tree	O
boltzmann	O
machine	O
with	O
maximum	O
clique	B
size	O
15	O
,	O
trained	O
using	O
eﬃcient	B
ipf	O
.	O
9.4.5	O
iterative	B
scaling	I
we	O
consider	O
markov	O
networks	O
of	O
the	O
exponential	B
form	O
eθcfc	O
(	O
vc	O
)	O
(	O
9.4.34	O
)	O
where	O
fc	O
(	O
vc	O
)	O
≥	O
0	O
and	O
c	O
ranges	O
of	O
the	O
non-maximal	O
cliques	O
vc	O
⊂	O
v.	O
the	O
normalisation	B
requirement	O
is	O
(	O
cid:89	O
)	O
c	O
eθcfc	O
(	O
vc	O
)	O
p	O
(	O
v|θ	O
)	O
=	O
1	O
z	O
(	O
θ	O
)	O
z	O
(	O
θ	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:89	O
)	O
v	O
c	O
a	O
maximum	B
likelihood	I
training	O
algorithm	B
for	O
a	O
markov	O
network	O
,	O
somewhat	O
analogous	O
to	O
the	O
em	O
ap-	O
proach	O
of	O
section	O
(	O
11.2	O
)	O
can	O
be	O
derived	O
as	O
follows	O
[	O
32	O
]	O
:	O
consider	O
the	O
bound	B
,	O
for	O
positive	O
x	O
:	O
log	O
x	O
≤	O
x	O
−	O
1	O
⇒	O
−	O
log	O
x	O
≥	O
1	O
−	O
x	O
hence	O
z	O
(	O
θ	O
)	O
z	O
(	O
θold	O
)	O
⇒	O
−	O
log	O
z	O
(	O
θ	O
)	O
≥	O
−	O
log	O
z	O
(	O
θold	O
)	O
+	O
1	O
−	O
z	O
(	O
θ	O
)	O
z	O
(	O
θold	O
)	O
−	O
log	O
z	O
(	O
θ	O
)	O
z	O
(	O
θold	O
)	O
≥	O
1	O
−	O
(	O
cid:88	O
)	O
1	O
n	O
l	O
(	O
θ	O
)	O
≥	O
1	O
n	O
c	O
,	O
n	O
then	O
we	O
can	O
write	O
a	O
bound	B
on	O
the	O
log	O
likelihood	B
θcfc	O
(	O
v	O
n	O
c	O
)	O
−	O
log	O
z	O
(	O
θold	O
)	O
+	O
1	O
−	O
z	O
(	O
θ	O
)	O
z	O
(	O
θold	O
)	O
(	O
9.4.35	O
)	O
(	O
9.4.36	O
)	O
(	O
9.4.37	O
)	O
(	O
9.4.38	O
)	O
(	O
9.4.39	O
)	O
(	O
9.4.40	O
)	O
(	O
9.4.41	O
)	O
as	O
it	O
stands	O
,	O
the	O
bound	B
(	O
9.4.38	O
)	O
is	O
in	O
general	O
not	O
straightforward	O
to	O
optimise	O
since	O
the	O
parameters	O
of	O
each	O
potential	B
are	O
coupled	B
through	O
the	O
z	O
(	O
θ	O
)	O
term	O
.	O
for	O
convenience	O
it	O
is	O
useful	O
to	O
ﬁrst	O
reparmameterise	O
and	O
write	O
then	O
αc	O
c	O
c	O
(	O
cid:124	O
)	O
+θold	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
θc	O
=	O
θc	O
−	O
θold	O
z	O
(	O
θ	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
c	O
fc	O
(	O
vc	O
)	O
θc	O
=	O
(	O
cid:88	O
)	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
c	O
αcfc	O
(	O
vc	O
)	O
=	O
e	O
c	O
pc	O
[	O
αc	O
v	O
e	O
v	O
e	O
d	O
fd	O
(	O
vd	O
)	O
]	O
(	O
cid:80	O
)	O
e	O
(	O
cid:80	O
)	O
c	O
fc	O
(	O
vc	O
)	O
θold	O
c	O
e	O
c	O
fc	O
(	O
vc	O
)	O
αc	O
one	O
can	O
decouple	O
this	O
using	O
an	O
additional	O
bound	B
derived	O
by	O
ﬁrst	O
considering	O
:	O
192	O
draft	O
march	O
9	O
,	O
2010	O
c	O
pc	O
=	O
1	O
we	O
may	O
apply	O
jensen	O
’	O
s	O
inequality	O
to	O
give	O
maximum	B
likelihood	I
for	O
undirected	B
models	O
e	O
where	O
pc	O
≡	O
(	O
cid:80	O
)	O
fc	O
(	O
vc	O
)	O
(	O
cid:80	O
)	O
since	O
pc	O
≥	O
0	O
and	O
(	O
cid:80	O
)	O
d	O
fd	O
(	O
vd	O
)	O
(	O
cid:88	O
)	O
c	O
αcfc	O
(	O
vc	O
)	O
≤	O
(	O
cid:88	O
)	O
(	O
cid:80	O
)	O
(	O
cid:40	O
)	O
(	O
cid:124	O
)	O
z	O
(	O
θ	O
)	O
≤	O
l	O
(	O
θ	O
)	O
≥	O
(	O
cid:88	O
)	O
hence	O
1	O
n	O
v	O
c	O
1	O
n	O
d	O
fd	O
(	O
vd	O
)	O
αc	O
pce	O
(	O
cid:80	O
)	O
c	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
c	O
fc	O
(	O
v	O
n	O
c	O
)	O
θc	O
−	O
c	O
n	O
plugging	O
this	O
bound	B
into	O
(	O
9.4.38	O
)	O
we	O
have	O
c	O
fc	O
(	O
vc	O
)	O
θold	O
e	O
(	O
cid:80	O
)	O
f	O
fd	O
(	O
vc	O
)	O
pceαc	O
(	O
cid:80	O
)	O
d	O
fd	O
(	O
vc	O
)	O
(	O
cid:69	O
)	O
p	O
(	O
v|θold	O
)	O
(	O
cid:41	O
)	O
(	O
cid:125	O
)	O
+1	O
−	O
log	O
z	O
(	O
θold	O
)	O
(	O
cid:68	O
)	O
pceαc	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
lb	O
(	O
θc	O
)	O
(	O
9.4.42	O
)	O
(	O
9.4.43	O
)	O
(	O
9.4.44	O
)	O
(	O
9.4.45	O
)	O
the	O
term	O
in	O
curly	O
brackets	O
contains	O
the	O
potential	B
parameters	O
θc	O
in	O
an	O
uncoupled	O
fashion	O
.	O
diﬀerentiating	O
with	O
respect	O
to	O
θc	O
the	O
gradient	B
of	O
each	O
lower	O
bound	O
is	O
given	O
by	O
∂lb	O
(	O
θc	O
)	O
∂θc	O
=	O
1	O
n	O
fc	O
(	O
v	O
n	O
c	O
)	O
−	O
(	O
cid:88	O
)	O
n	O
(	O
cid:68	O
)	O
c	O
)	O
(	O
cid:80	O
)	O
fc	O
(	O
vc	O
)	O
e	O
(	O
θc−θold	O
d	O
fd	O
(	O
vd	O
)	O
(	O
cid:69	O
)	O
p	O
(	O
v|θold	O
)	O
(	O
9.4.46	O
)	O
this	O
can	O
be	O
used	O
as	O
part	O
of	O
a	O
gradient	B
based	O
optimisation	B
procedure	O
to	O
learn	O
the	O
parameters	O
θc	O
.	O
a	O
potential	B
advantage	O
over	O
ipf	O
is	O
that	O
all	O
the	O
parameters	O
may	O
be	O
updated	O
simultaneously	O
,	O
whereas	O
in	O
ipf	O
they	O
must	O
be	O
updated	O
sequentially	O
.	O
intuitively	O
,	O
the	O
parameters	O
converge	O
when	O
the	O
empirical	B
average	O
of	O
the	O
functions	O
f	O
match	O
the	O
average	B
of	O
the	O
functions	O
with	O
respect	O
to	O
samples	O
drawn	O
from	O
the	O
distribution	B
,	O
in	O
line	O
with	O
our	O
general	O
condition	O
for	O
maximum	B
likelihood	I
optimal	O
solution	O
.	O
c	O
fc	O
(	O
vc	O
)	O
=	O
1	O
,	O
the	O
zero	O
of	O
the	O
gradient	B
can	O
be	O
found	O
in	O
the	O
special	O
case	O
that	O
the	O
functions	O
sum	O
to	O
1	O
,	O
(	O
cid:80	O
)	O
analytically	O
,	O
giving	O
the	O
update	O
θc	O
=	O
θold	O
c	O
+	O
log	O
1	O
n	O
(	O
cid:88	O
)	O
n	O
fc	O
(	O
v	O
n	O
c	O
)	O
−	O
log	O
(	O
cid:104	O
)	O
fc	O
(	O
vc	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
vc|θold	O
)	O
(	O
9.4.47	O
)	O
the	O
constraint	O
that	O
the	O
features	O
fc	O
need	O
to	O
be	O
non-negative	O
can	O
be	O
relaxed	O
at	O
the	O
expense	O
of	O
additional	O
variational	O
parameters	O
,	O
see	O
exercise	O
(	O
129	O
)	O
.	O
in	O
cases	O
where	O
the	O
zero	O
of	O
the	O
gradient	B
can	O
not	O
be	O
computed	O
analytically	O
,	O
there	O
may	O
be	O
little	O
advantage	O
in	O
general	O
in	O
using	O
is	O
over	O
standard	O
gradient	O
based	O
procedures	O
on	O
the	O
log	O
likelihood	B
directly	O
[	O
196	O
]	O
.	O
if	O
the	O
junction	B
tree	I
formed	O
from	O
this	O
exponential	B
form	O
markov	O
network	O
has	O
limited	O
tree	B
width	I
,	O
compu-	O
tational	O
savings	O
can	O
be	O
made	O
by	O
performing	O
ipf	O
over	O
the	O
cliques	O
of	O
the	O
junction	B
tree	I
and	O
updating	O
the	O
parameters	O
θ	O
within	O
each	O
clique	B
using	O
is	O
[	O
11	O
]	O
.	O
this	O
is	O
a	O
modiﬁed	O
version	O
of	O
the	O
constrained	O
decomposable	O
case	O
.	O
see	O
also	O
[	O
274	O
]	O
for	O
a	O
uniﬁed	O
treatment	O
of	O
propagation	B
and	O
scaling	O
on	O
junction	O
trees	O
.	O
9.4.6	O
conditional	O
random	O
ﬁelds	O
for	O
an	O
input	O
x	O
and	O
output	O
y	O
,	O
a	O
crf	O
is	O
deﬁned	O
by	O
a	O
conditional	B
distribution	O
[	O
266	O
,	O
166	O
]	O
φk	O
(	O
y	O
,	O
x	O
)	O
(	O
9.4.48	O
)	O
for	O
(	O
positive	O
)	O
potentials	O
φk	O
(	O
y	O
,	O
x	O
)	O
.to	O
make	O
learning	B
more	O
straightforward	O
,	O
the	O
potentials	O
are	O
usually	O
deﬁned	O
as	O
eλkfk	O
(	O
y	O
,	O
x	O
)	O
for	O
ﬁxed	O
functions	O
f	O
(	O
y	O
,	O
x	O
)	O
and	O
parameters	O
λk	O
.	O
in	O
this	O
case	O
the	O
distribution	B
of	O
the	O
output	O
conditioned	O
on	O
the	O
input	O
is	O
p	O
(	O
y|x	O
)	O
=	O
1	O
z	O
(	O
x	O
)	O
(	O
cid:89	O
)	O
k	O
(	O
cid:89	O
)	O
p	O
(	O
y|x	O
,	O
λ	O
)	O
=	O
1	O
z	O
(	O
x	O
,	O
λ	O
)	O
k	O
draft	O
march	O
9	O
,	O
2010	O
eλkfk	O
(	O
y	O
,	O
x	O
)	O
(	O
9.4.49	O
)	O
193	O
maximum	B
likelihood	I
for	O
undirected	B
models	O
for	O
an	O
i.i.d	O
.	O
dataset	O
of	O
input-outputs	O
,	O
d	O
=	O
{	O
(	O
xn	O
,	O
yn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
,	O
training	B
based	O
on	O
conditional	B
maxi-	O
mum	O
likelihood	B
requires	O
the	O
maximisation	B
of	O
n	O
(	O
cid:88	O
)	O
n=1	O
n	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
n=1	O
k	O
l	O
(	O
λ	O
)	O
≡	O
log	O
p	O
(	O
yn|xn	O
,	O
λ	O
)	O
=	O
λkfk	O
(	O
yn	O
,	O
xn	O
)	O
−	O
log	O
z	O
(	O
xn	O
,	O
λ	O
)	O
(	O
9.4.50	O
)	O
in	O
general	O
no	O
closed	O
form	O
solution	O
for	O
the	O
optimal	O
λ	O
exists	O
and	O
this	O
needs	O
to	O
be	O
determined	O
numerically	O
.	O
first	O
we	O
note	O
that	O
equation	B
(	O
9.4.49	O
)	O
is	O
equivalent	B
to	O
equation	B
(	O
9.4.34	O
)	O
where	O
the	O
parameters	O
θ	O
are	O
here	O
denoted	O
by	O
λ	O
and	O
the	O
variables	O
v	O
are	O
here	O
denoted	O
by	O
y.	O
in	O
the	O
crf	O
case	O
the	O
inputs	O
simply	O
have	O
the	O
eﬀect	O
of	O
determining	O
the	O
feature	O
fk	O
(	O
y	O
,	O
x	O
)	O
.	O
in	O
this	O
sense	O
iterative	B
scaling	I
,	O
or	O
any	O
related	O
method	O
for	O
maximum	B
likelihood	I
training	O
of	O
constrained	O
markov	O
networks	O
,	O
may	O
be	O
readily	O
adapted	O
,	O
taking	O
advantage	O
also	O
of	O
any	O
computational	O
savings	O
from	O
limited	O
width	O
junction	O
trees	O
.	O
as	O
an	O
alternative	O
here	O
we	O
brieﬂy	O
describe	O
gradient	B
based	O
training	B
.	O
the	O
gradient	B
has	O
components	O
l	O
=	O
(	O
cid:88	O
)	O
(	O
cid:16	O
)	O
n	O
∂	O
∂λi	O
(	O
cid:17	O
)	O
fi	O
(	O
yn	O
,	O
xn	O
)	O
−	O
(	O
cid:104	O
)	O
fi	O
(	O
y	O
,	O
xn	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
y|xn	O
,	O
λ	O
)	O
(	O
9.4.51	O
)	O
the	O
terms	O
(	O
cid:104	O
)	O
fi	O
(	O
y	O
,	O
xn	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
y|xn	O
,	O
λ	O
)	O
can	O
be	O
problematic	O
and	O
their	O
tractability	O
depends	O
on	O
the	O
structure	B
of	O
the	O
potentials	O
.	O
for	O
a	O
multivariate	B
y	O
,	O
provided	O
the	O
structure	B
of	O
the	O
cliques	O
deﬁned	O
on	O
subsets	O
of	O
y	O
is	O
singly-	O
connected	B
,	O
then	O
computing	O
the	O
average	B
is	O
generally	O
tractable	O
.	O
more	O
generally	O
,	O
provided	O
the	O
cliques	O
of	O
the	O
resulting	O
junction	B
tree	I
have	O
limited	O
width	O
,	O
then	O
exact	O
marginals	O
are	O
avaiable	O
.	O
an	O
example	O
of	O
this	O
is	O
given	O
for	O
a	O
linear-chain	O
crf	O
in	O
section	O
(	O
23.4.3	O
)	O
–	O
see	O
also	O
example	O
(	O
46	O
)	O
below	O
.	O
another	O
quantity	O
often	O
useful	O
for	O
numerical	B
optimisation	O
is	O
the	O
hessian	O
which	O
has	O
components	O
l	O
=	O
(	O
cid:88	O
)	O
n	O
∂2	O
∂λi∂λj	O
(	O
(	O
cid:104	O
)	O
fi	O
(	O
y	O
,	O
xn	O
)	O
(	O
cid:105	O
)	O
(	O
cid:104	O
)	O
fj	O
(	O
y	O
,	O
xn	O
)	O
(	O
cid:105	O
)	O
−	O
(	O
cid:104	O
)	O
fi	O
(	O
y	O
,	O
xn	O
)	O
fj	O
(	O
y	O
,	O
xn	O
)	O
(	O
cid:105	O
)	O
)	O
(	O
9.4.52	O
)	O
where	O
the	O
averages	O
above	O
are	O
with	O
respect	O
to	O
p	O
(	O
y|xn	O
,	O
λ	O
)	O
.	O
this	O
expression	O
is	O
a	O
(	O
negated	O
)	O
sum	O
of	O
covariance	B
elements	O
,	O
and	O
is	O
therefore	O
negative	O
(	O
semi	O
)	O
deﬁnite	O
.	O
hence	O
the	O
function	B
l	O
(	O
λ	O
)	O
is	O
concave	O
and	O
has	O
only	O
a	O
single	O
global	B
optimum	O
.	O
whilst	O
no	O
closed	O
form	O
solution	O
for	O
the	O
optimal	O
λ	O
exists	O
,	O
the	O
optimal	O
solutions	O
can	O
be	O
found	O
easily	O
using	O
a	O
numerical	B
technique	O
such	O
as	O
conjugate	O
gradients	O
.	O
in	O
practice	O
regularisation	B
terms	O
are	O
often	O
added	O
to	O
prevent	O
overﬁtting	B
(	O
see	O
section	O
(	O
13.2.3	O
)	O
for	O
a	O
discussion	O
of	O
regularisation	B
)	O
.	O
using	O
a	O
term	O
kλ2	O
c2	O
k	O
(	O
9.4.53	O
)	O
(	O
cid:88	O
)	O
k	O
−	O
for	O
positive	O
regularisation	O
constants	O
c2	O
k	O
discourages	O
the	O
weights	O
λ	O
from	O
being	O
too	O
large	O
.	O
this	O
term	O
is	O
also	O
negative	O
deﬁnite	O
and	O
hence	O
the	O
overall	O
objective	O
function	B
remains	O
concave	O
.	O
iterative	B
scaling	I
may	O
also	O
be	O
used	O
to	O
train	O
a	O
crf	O
though	O
in	O
practice	O
gradient	B
based	O
techniques	O
are	O
to	O
be	O
preferred	O
[	O
196	O
]	O
.	O
once	O
trained	O
a	O
crf	O
can	O
be	O
used	O
for	O
predicting	O
the	O
output	O
distribution	B
for	O
a	O
novel	O
input	O
x∗	O
.	O
the	O
most	O
likely	O
output	O
y∗	O
is	O
equivalently	O
given	O
by	O
∗	O
)	O
=	O
argmax	O
∗	O
=	O
argmax	O
(	O
cid:88	O
)	O
(	O
9.4.54	O
)	O
λkfk	O
(	O
y	O
,	O
x	O
,	O
λ	O
)	O
y	O
∗	O
)	O
−	O
log	O
z	O
(	O
x	O
∗	O
since	O
the	O
normalisation	B
term	O
is	O
independent	O
of	O
y	O
,	O
ﬁnding	O
the	O
most	O
likely	O
output	O
is	O
equivalent	B
to	O
y	O
y	O
k	O
log	O
p	O
(	O
y|x	O
(	O
cid:88	O
)	O
k	O
∗	O
)	O
λkfk	O
(	O
y	O
,	O
x	O
y	O
∗	O
=	O
argmax	O
y	O
194	O
(	O
9.4.55	O
)	O
draft	O
march	O
9	O
,	O
2010	O
maximum	B
likelihood	I
for	O
undirected	B
models	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
9.18	O
:	O
(	O
a	O
)	O
:	O
training	B
results	O
for	O
a	O
linear	B
chain	O
crf	O
.	O
there	O
are	O
5	O
training	B
sequences	O
,	O
one	O
per	O
subpanel	O
.	O
in	O
each	O
the	O
top	O
row	O
corresponds	O
to	O
the	O
input	O
sequence	O
x1:20	O
,	O
xt	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
5	O
}	O
(	O
each	O
state	O
represented	O
by	O
a	O
diﬀerent	O
colour	O
)	O
the	O
middle	O
row	O
,	O
the	O
correct	O
output	O
sequence	O
y1:20	O
,	O
yt	O
∈	O
{	O
1	O
,	O
2	O
,	O
3	O
}	O
(	O
each	O
state	O
represented	O
by	O
a	O
diﬀerent	O
colour	O
)	O
.	O
together	O
the	O
input	O
and	O
output	O
sequences	B
make	O
the	O
training	B
data	O
d.	O
the	O
bottom	O
(	O
b	O
)	O
:	O
row	O
contains	O
the	O
most	O
likely	O
output	O
sequence	O
given	O
the	O
trained	O
crf	O
,	O
arg	O
maxy1:20	O
p	O
(	O
y1:20|x1:20	O
,	O
d	O
)	O
.	O
five	O
additional	O
test	O
sequences	O
along	O
with	O
the	O
correct	O
output	O
and	O
predicted	O
output	O
sequence	O
.	O
natural	B
language	O
processing	O
in	O
a	O
natural	B
language	O
processing	O
application	O
,	O
xt	O
might	O
represent	O
a	O
word	O
and	O
yt	O
a	O
corresponding	O
linguistic	O
tag	O
(	O
‘	O
noun	O
’	O
,	O
‘	O
verb	O
’	O
,	O
etc	O
.	O
)	O
.	O
a	O
more	O
suitable	O
form	O
in	O
this	O
case	O
is	O
to	O
constrain	O
the	O
crf	O
to	O
be	O
of	O
the	O
form	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
exp	O
µkgk	O
(	O
yt	O
,	O
yt−1	O
)	O
+	O
(	O
cid:88	O
)	O
(	O
cid:33	O
)	O
ρlhl	O
(	O
yt	O
,	O
xt	O
)	O
(	O
9.4.56	O
)	O
k	O
l	O
for	O
binary	O
functions	O
gk	O
and	O
hl	O
and	O
parameters	O
µk	O
and	O
ρl	O
.	O
the	O
grammatical	O
structure	B
of	O
tag-tag	O
transitions	O
is	O
encoded	O
in	O
gk	O
(	O
yt	O
,	O
yt−1	O
)	O
and	O
linguistic	O
tag	O
information	O
in	O
hk	O
(	O
yt	O
,	O
xt	O
)	O
,	O
with	O
the	O
importance	B
of	O
these	O
being	O
determined	O
by	O
the	O
corresponding	O
parameters	O
[	O
166	O
]	O
.	O
in	O
this	O
case	O
inference	B
of	O
the	O
marginals	O
(	O
cid:104	O
)	O
ytyt−1|x1	O
:	O
t	O
(	O
cid:105	O
)	O
is	O
straightforward	O
since	O
the	O
factor	B
graph	I
corresponding	O
to	O
the	O
inference	B
problem	O
is	O
a	O
linear	B
chain	O
.	O
variants	O
of	O
the	O
linear	B
chain	O
crf	O
are	O
used	O
heavily	O
in	O
natural	B
language	O
processing	O
,	O
including	O
part-of-speech	B
tagging	I
and	O
machine	O
translation	O
(	O
in	O
which	O
the	O
input	O
sequence	O
x	O
represents	O
a	O
sentence	O
say	O
in	O
english	O
and	O
the	O
output	O
sequence	O
y	O
the	O
corresponding	O
translation	O
into	O
french	O
)	O
.	O
see	O
,	O
for	O
example	O
,	O
[	O
213	O
]	O
.	O
example	O
46	O
(	O
linear	B
chain	O
crf	O
)	O
.	O
we	O
consider	O
a	O
crf	O
with	O
x	O
=	O
5	O
input	O
states	O
and	O
y	O
=	O
3	O
output	O
states	O
of	O
the	O
form	O
t	O
(	O
cid:89	O
)	O
t=2	O
k	O
µkgk	O
(	O
yt	O
,	O
yt−1	O
)	O
+	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
e	O
p	O
(	O
y1	O
:	O
t|x1	O
:	O
t	O
)	O
=	O
l	O
ρlhl	O
(	O
yt	O
,	O
xt	O
)	O
(	O
9.4.57	O
)	O
here	O
the	O
binary	O
functions	O
gk	O
(	O
yt	O
,	O
yt−1	O
)	O
=	O
i	O
[	O
yt	O
=	O
ak	O
]	O
i	O
[	O
yt−1	O
=	O
bk	O
]	O
,	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
9	O
model	B
the	O
transitions	O
between	O
two	O
consecutive	O
outputs	O
.	O
the	O
binary	O
functions	O
hl	O
(	O
yt	O
,	O
xt	O
)	O
=	O
i	O
[	O
yt	O
=	O
al	O
]	O
i	O
[	O
xt	O
=	O
cl	O
]	O
,	O
l	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
15	O
model	B
the	O
translation	O
of	O
the	O
input	O
to	O
the	O
output	O
.	O
there	O
are	O
therefore	O
9	O
+	O
15	O
=	O
24	O
parameters	O
in	O
total	O
.	O
in	O
ﬁg	O
(	O
9.18	O
)	O
we	O
plot	O
the	O
training	B
and	O
test	O
results	O
based	O
on	O
a	O
small	O
set	O
of	O
data	B
.	O
the	O
training	B
of	O
the	O
crf	O
is	O
obtained	O
using	O
50	O
iterations	O
of	O
gradient	B
ascent	O
with	O
a	O
learning	B
rate	I
of	O
0.1.	O
see	O
demolinearcrf.m	O
.	O
draft	O
march	O
9	O
,	O
2010	O
195	O
2468101214161820123246810121416182012324681012141618201232468101214161820123246810121416182012324681012141618201232468101214161820123246810121416182012324681012141618201232468101214161820123	O
properties	B
of	O
maximum	B
likelihood	I
9.4.7	O
pseudo	B
likelihood	I
consider	O
a	O
mn	O
on	O
variables	O
x	O
with	O
dim	O
x	O
=	O
d	O
of	O
the	O
form	O
φc	O
(	O
xc|θc	O
)	O
(	O
9.4.58	O
)	O
for	O
all	O
but	O
specially	O
constrained	O
φc	O
,	O
the	O
partition	B
function	I
z	O
will	O
be	O
intractable	O
and	O
the	O
likelihood	B
of	O
a	O
set	O
of	O
i.i.d	O
.	O
data	B
intractable	O
as	O
well	O
.	O
a	O
surrogate	O
is	O
to	O
use	O
the	O
pseudo	B
likelihood	I
of	O
the	O
probability	B
of	O
each	O
variable	B
conditioned	O
on	O
all	O
other	O
variables	O
(	O
which	O
is	O
equivalent	B
to	O
conditioning	B
on	O
only	O
the	O
variable	B
’	O
s	O
neighbours	O
for	O
a	O
mn	O
)	O
(	O
cid:89	O
)	O
c	O
p	O
(	O
x|θ	O
)	O
=	O
1	O
z	O
n	O
(	O
cid:88	O
)	O
d	O
(	O
cid:88	O
)	O
n=1	O
i=1	O
(	O
cid:48	O
)	O
(	O
θ	O
)	O
=	O
l	O
log	O
p	O
(	O
xn	O
i	O
|xn\i|θ	O
)	O
(	O
9.4.59	O
)	O
i	O
|xn\i|θ	O
)	O
are	O
usually	O
straightforward	O
to	O
work	O
out	O
since	O
they	O
require	O
ﬁnding	O
the	O
normalisation	B
the	O
terms	O
p	O
(	O
xn	O
of	O
a	O
univariate	B
distribution	O
only	O
.	O
in	O
this	O
case	O
the	O
gradient	B
can	O
be	O
computed	O
exactly	O
,	O
and	O
learning	B
of	O
the	O
parameters	O
θ	O
carried	O
out	O
.	O
at	O
least	O
for	O
the	O
case	O
of	O
the	O
boltzmann	O
machine	O
,	O
this	O
forms	O
a	O
consistent	B
estimator	I
[	O
139	O
]	O
.	O
9.4.8	O
learning	B
the	O
structure	B
learning	I
the	O
structure	B
of	O
a	O
markov	O
network	O
can	O
also	O
be	O
based	O
on	O
independence	B
tests	O
,	O
as	O
for	O
belief	O
net-	O
works	O
.	O
a	O
criterion	O
for	O
ﬁnding	O
a	O
mn	O
on	O
a	O
set	O
of	O
nodes	O
v	O
is	O
to	O
use	O
the	O
fact	O
that	O
no	O
edge	O
exits	O
between	O
x	O
and	O
y	O
if	O
,	O
conditioned	O
on	O
all	O
other	O
nodes	O
,	O
x	O
and	O
y	O
are	O
deemed	O
independent	O
.	O
this	O
is	O
the	O
pairwise	B
markov	O
property	O
described	O
in	O
section	O
(	O
4.2.1	O
)	O
.	O
by	O
checking	O
x⊥⊥	O
y|v\	O
{	O
x	O
,	O
y	O
}	O
for	O
every	O
pair	O
of	O
variables	O
x	O
and	O
y	O
,	O
this	O
edge	O
deletion	O
approach	B
in	O
principle	O
reveals	O
the	O
structure	B
of	O
the	O
network	O
[	O
219	O
]	O
.	O
for	O
learning	B
the	O
structure	B
from	O
an	O
oracle	O
,	O
this	O
method	O
is	O
sound	O
.	O
however	O
,	O
a	O
practical	O
diﬃculty	O
in	O
the	O
case	O
where	O
the	O
independencies	O
are	O
determined	O
from	O
data	B
is	O
that	O
checking	O
if	O
x	O
⊥⊥	O
y|	O
v\	O
{	O
x	O
,	O
y	O
}	O
requires	O
in	O
principle	O
enormous	O
amounts	O
of	O
data	B
.	O
the	O
reason	O
for	O
this	O
is	O
that	O
the	O
conditioning	B
selects	O
only	O
those	O
parts	O
of	O
the	O
dataset	O
consistent	B
with	O
the	O
conditioning	B
.	O
in	O
practice	O
this	O
will	O
result	O
in	O
very	O
small	O
numbers	O
of	O
remaining	O
datapoints	O
,	O
and	O
estimating	O
independencies	O
on	O
this	O
basis	O
is	O
unreliable	O
.	O
the	O
markov	O
boundary	B
criterion	O
[	O
219	O
]	O
uses	O
the	O
local	B
markov	O
property	O
,	O
section	O
(	O
4.2.1	O
)	O
,	O
namely	O
that	O
condi-	O
tioned	O
on	O
its	O
neighbours	O
,	O
a	O
variable	B
is	O
independent	O
of	O
all	O
other	O
variables	O
in	O
the	O
graph	B
.	O
by	O
starting	O
with	O
a	O
variable	B
x	O
and	O
an	O
empty	O
neighbourhood	O
set	O
,	O
one	O
can	O
progressively	O
include	O
neighbours	O
,	O
testing	O
if	O
their	O
inclusion	O
renders	O
the	O
remaining	O
non-neighbours	O
independent	O
of	O
x.	O
a	O
diﬃcultly	O
with	O
this	O
is	O
that	O
,	O
if	O
one	O
doesn	O
’	O
t	O
have	O
the	O
correct	O
markov	O
boundary	B
,	O
then	O
including	O
a	O
variable	B
in	O
the	O
neighbourhood	O
set	O
may	O
be	O
deemed	O
necessary	O
.	O
to	O
see	O
this	O
,	O
consider	O
a	O
network	O
which	O
corresponds	O
to	O
a	O
linear	B
chain	O
and	O
that	O
x	O
is	O
at	O
the	O
edge	O
of	O
the	O
chain	B
.	O
in	O
this	O
case	O
,	O
only	O
the	O
nearest	B
neighbour	I
of	O
x	O
is	O
in	O
the	O
markov	O
boundary	B
of	O
x.	O
however	O
,	O
if	O
this	O
nearest	B
neighbour	I
were	O
not	O
currently	O
in	O
the	O
set	O
,	O
then	O
any	O
other	O
non-nearest	O
neighbour	B
would	O
be	O
included	O
,	O
even	O
though	O
this	O
is	O
not	O
strictly	O
required	O
.	O
to	O
counter	O
this	O
,	O
the	O
neighbourhood	O
variables	O
included	O
in	O
the	O
neighbourhood	O
of	O
x	O
may	O
be	O
later	O
removed	O
if	O
they	O
are	O
deemed	O
superﬂuous	O
to	O
the	O
boundary	B
[	O
102	O
]	O
.	O
in	O
cases	O
where	O
speciﬁc	O
constraints	O
are	O
imposed	O
,	O
such	O
as	O
learning	O
structures	O
whose	O
resulting	O
triangulation	B
has	O
a	O
bounded	O
tree-width	O
,	O
whilst	O
still	O
formally	O
diﬃcult	O
,	O
approximate	B
procedures	O
are	O
available	O
[	O
260	O
]	O
.	O
in	O
terms	O
of	O
network	B
scoring	I
methods	O
for	O
undirected	B
networks	O
,	O
computing	O
a	O
score	O
is	O
hampered	O
by	O
the	O
fact	O
that	O
the	O
parameters	O
of	O
each	O
clique	B
become	O
coupled	B
in	O
the	O
normalisation	B
constant	I
of	O
the	O
distribution	B
.	O
this	O
issue	O
can	O
be	O
addressed	O
using	O
hyper	B
markov	O
priors	O
[	O
75	O
]	O
.	O
9.5	O
properties	B
of	O
maximum	B
likelihood	I
9.5.1	O
training	B
assuming	O
the	O
correct	O
model	B
class	O
consider	O
a	O
dataset	O
x	O
=	O
{	O
xn	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
generated	O
from	O
an	O
underlying	O
parametric	O
model	B
p	O
(	O
x|θ0	O
)	O
.	O
our	O
interest	O
is	O
to	O
ﬁt	O
a	O
model	B
p	O
(	O
x|θ	O
)	O
of	O
the	O
same	O
form	O
as	O
the	O
correct	O
underlying	O
model	B
p	O
(	O
x|θ0	O
)	O
and	O
examine	O
196	O
draft	O
march	O
9	O
,	O
2010	O
code	O
whether	O
if	O
,	O
in	O
the	O
limit	O
of	O
a	O
large	O
amount	O
of	O
data	B
,	O
the	O
parameter	B
θ	O
learned	O
by	O
maximum	B
likelihood	I
matches	O
the	O
correct	O
parameter	B
θ0	O
.	O
our	O
derivation	O
below	O
is	O
non-rigorous	O
,	O
but	O
highlights	O
the	O
essence	O
of	O
the	O
argument	O
.	O
assuming	O
the	O
data	B
is	O
i.i.d.	O
,	O
the	O
log	O
likelihood	B
l	O
(	O
θ	O
)	O
≡	O
log	O
p	O
(	O
x|θ	O
)	O
is	O
n	O
(	O
cid:88	O
)	O
n=1	O
l	O
(	O
θ	O
)	O
=	O
1	O
n	O
log	O
p	O
(	O
xn|θ	O
)	O
(	O
9.5.1	O
)	O
in	O
the	O
limit	O
n	O
→	O
∞	O
,	O
the	O
sample	B
average	O
can	O
be	O
replaced	O
by	O
an	O
average	B
with	O
respect	O
to	O
the	O
distribution	B
generating	O
the	O
data	B
(	O
cid:104	O
)	O
log	O
p	O
(	O
x|θ	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
x|θ0	O
)	O
=	O
−kl	O
(	O
cid:0	O
)	O
p	O
(	O
x|θ0	O
)	O
|p	O
(	O
x|θ	O
)	O
(	O
cid:1	O
)	O
+	O
(	O
cid:10	O
)	O
log	O
p	O
(	O
x|θ0	O
)	O
(	O
cid:11	O
)	O
l	O
(	O
θ	O
)	O
=n→∞	O
p	O
(	O
x|θ0	O
)	O
(	O
9.5.2	O
)	O
up	O
to	O
a	O
negligible	O
constant	O
,	O
this	O
is	O
the	O
kullback-leibler	O
divergence	B
between	O
two	O
distributions	O
in	O
x	O
,	O
just	O
with	O
diﬀerent	O
parameter	B
settings	O
.	O
the	O
θ	O
that	O
maximises	O
l	O
(	O
θ	O
)	O
is	O
that	O
which	O
minimises	O
the	O
kullback-	O
leibler	O
divergence	B
,	O
namely	O
θ	O
=	O
θ0	O
.	O
in	O
the	O
limit	O
of	O
a	O
large	O
amount	O
of	O
data	B
we	O
can	O
,	O
in	O
principle	O
,	O
learn	O
the	O
correct	O
parameters	O
(	O
assuming	O
we	O
know	O
the	O
correct	O
model	B
class	O
)	O
.	O
the	O
property	O
of	O
an	O
estimator	B
such	O
that	O
the	O
parameter	B
θ	O
converges	O
to	O
the	O
true	O
model	B
parameter	O
θ0	O
as	O
the	O
sequence	O
of	O
data	B
increase	O
is	O
termed	O
a	O
consistency	O
.	O
9.5.2	O
training	B
when	O
the	O
assumed	O
model	B
is	O
incorrect	O
we	O
write	O
q	O
(	O
x|θ	O
)	O
for	O
the	O
assumed	O
model	B
,	O
and	O
p	O
(	O
x|φ	O
)	O
for	O
the	O
correct	O
generating	O
model	B
.	O
repeating	O
the	O
above	O
calculations	O
in	O
the	O
case	O
of	O
the	O
assumed	O
model	B
being	O
correct	O
,	O
we	O
have	O
that	O
,	O
in	O
the	O
limit	O
of	O
a	O
large	O
amount	O
of	O
data	B
,	O
the	O
likelihood	B
is	O
l	O
(	O
θ	O
)	O
=	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
x|θ	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
x|φ	O
)	O
=	O
−kl	O
(	O
p	O
(	O
x|φ	O
)	O
|q	O
(	O
x|θ	O
)	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x|φ	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
x|φ	O
)	O
(	O
9.5.3	O
)	O
since	O
q	O
and	O
p	O
are	O
not	O
of	O
the	O
same	O
form	O
,	O
setting	O
θ	O
to	O
φ	O
does	O
not	O
necessarily	O
minimise	O
kl	O
(	O
p	O
(	O
x|φ	O
)	O
|q	O
(	O
x|θ	O
)	O
)	O
,	O
and	O
therefore	O
does	O
not	O
necessarily	O
optimize	O
l	O
(	O
θ	O
)	O
.	O
9.6	O
code	O
condindepemp.m	O
:	O
bayes	O
test	O
and	O
mutual	B
information	I
for	O
empirical	B
conditional	O
independence	B
condmi.m	O
:	O
conditional	B
mutual	I
information	I
condmiemp.m	O
:	O
conditional	B
mutual	I
information	I
of	O
empirical	B
distribution	I
miemp.m	O
:	O
mutual	B
information	I
of	O
empirical	B
distribution	I
9.6.1	O
pc	O
algorithm	B
using	O
an	O
oracle	O
this	O
demo	O
uses	O
an	O
oracle	O
to	O
determine	O
x	O
⊥⊥	O
y	O
|	O
z	O
,	O
rather	O
than	O
using	O
data	B
to	O
determine	O
the	O
empirical	B
dependence	O
.	O
the	O
oracle	O
is	O
itself	O
a	O
belief	B
network	I
.	O
for	O
the	O
partial	O
orientation	O
only	O
the	O
ﬁrst	O
‘	O
unmarried	O
collider	B
’	O
rule	O
is	O
implemented	O
.	O
demopcoracle.m	O
:	O
demo	O
of	O
pc	O
algorithm	B
with	O
an	O
oracle	O
pcskeletonoracle.m	O
:	O
pc	O
algorithm	B
using	O
an	O
oracle	O
pcorient.m	O
:	O
orient	O
a	O
skeleton	B
9.6.2	O
demo	O
of	O
empirical	B
conditional	O
independence	B
for	O
half	O
of	O
the	O
experiments	O
,	O
the	O
data	B
is	O
drawn	O
from	O
a	O
distribution	B
for	O
which	O
x⊥⊥	O
y|	O
z	O
is	O
true	O
.	O
for	O
the	O
other	O
half	O
of	O
the	O
experiments	O
,	O
the	O
data	B
is	O
drawn	O
from	O
a	O
random	O
distribution	O
for	O
which	O
x⊥⊥	O
y|	O
z	O
is	O
false	O
.	O
we	O
then	O
measure	O
the	O
fraction	O
of	O
experiments	O
for	O
which	O
the	O
bayes	O
test	O
correctly	O
decides	O
x⊥⊥	O
y|	O
z.	O
we	O
also	O
measure	O
the	O
fraction	O
of	O
experiments	O
for	O
which	O
the	O
mutual	B
information	I
test	O
correctly	O
decides	O
x	O
⊥⊥	O
y|	O
z	O
,	O
based	O
on	O
197	O
draft	O
march	O
9	O
,	O
2010	O
fuse	O
drum	O
toner	O
paper	O
roller	O
burning	O
quality	O
wrinkled	O
mult	O
.	O
pages	O
paper	O
jam	O
exercises	O
figure	O
9.19	O
:	O
printer	B
nightmare	I
belief	O
network	O
.	O
all	O
variables	O
are	O
binary	O
.	O
the	O
upper	O
variables	O
without	O
parents	B
are	O
possible	O
problems	O
(	O
diagnoses	O
)	O
,	O
and	O
the	O
lower	O
variables	O
consequences	O
of	O
problems	O
(	O
faults	O
)	O
.	O
setting	O
the	O
threshold	O
equal	O
to	O
the	O
median	O
of	O
all	O
the	O
empirical	B
conditional	O
mutual	B
information	I
values	O
.	O
a	O
similar	O
empirical	B
threshold	O
can	O
also	O
be	O
obtained	O
for	O
the	O
bayes	O
’	O
factor	B
(	O
although	O
this	O
is	O
not	O
strictly	O
kosher	O
in	O
the	O
pure	O
bayesian	O
spirit	O
since	O
one	O
should	O
in	O
principle	O
set	O
the	O
threshold	O
to	O
zero	O
)	O
.	O
the	O
test	O
based	O
on	O
the	O
assumed	O
chi-squared	O
distributed	O
mi	O
is	O
included	O
for	O
comparison	O
,	O
although	O
it	O
seems	O
to	O
be	O
impractical	O
in	O
these	O
small	O
data	B
cases	O
.	O
democondindepemp.m	O
:	O
demo	O
of	O
empirical	B
conditional	O
independence	B
based	O
on	O
data	B
9.6.3	O
bayes	O
dirichlet	O
structure	B
learning	I
it	O
is	O
interesting	O
to	O
compare	O
the	O
result	O
of	O
demopcdata.m	O
with	O
demobdscore.m	O
.	O
pcskeletondata.m	O
:	O
pc	O
algorithm	B
using	O
empirical	B
conditional	O
independence	B
demopcdata.m	O
:	O
demo	O
of	O
pc	O
algorithm	B
with	O
data	B
bdscore.m	O
:	O
bayes	O
dirichlet	O
(	O
bd	O
)	O
score	O
for	O
a	O
node	B
given	O
parents	B
learnbayesnet.m	O
:	O
given	O
an	O
ancestral	B
order	I
and	O
maximal	O
parents	B
,	O
learn	O
the	O
network	O
demobdscore.m	O
:	O
demo	O
of	O
structure	B
learning	I
9.7	O
exercises	O
exercise	O
116	O
(	O
printer	B
nightmare	I
)	O
.	O
cheapco	O
is	O
,	O
quite	O
honestly	O
,	O
a	O
pain	O
in	O
the	O
neck	O
.	O
not	O
only	O
did	O
they	O
buy	O
a	O
dodgy	O
old	O
laser	O
printer	O
from	O
stoppress	O
and	O
use	O
it	O
mercilessly	O
,	O
but	O
try	O
to	O
get	O
away	O
with	O
using	O
substandard	O
components	O
and	O
materials	O
.	O
unfortunately	O
for	O
stoppress	O
,	O
they	O
have	O
a	O
contract	O
to	O
maintain	O
cheapco	O
’	O
s	O
old	O
warhorse	O
,	O
and	O
end	O
up	O
frequently	O
sending	O
the	O
mechanic	O
out	O
to	O
repair	O
the	O
printer	O
.	O
after	O
the	O
10th	O
visit	O
,	O
they	O
decide	O
to	O
make	O
a	O
statistical	O
model	B
of	O
cheapco	O
’	O
s	O
printer	O
,	O
so	O
that	O
they	O
will	O
have	O
a	O
reasonable	O
idea	O
of	O
the	O
fault	O
based	O
only	O
on	O
the	O
information	O
that	O
cheapco	O
’	O
s	O
secretary	O
tells	O
them	O
on	O
the	O
phone	O
.	O
in	O
that	O
way	O
,	O
stoppress	O
hopes	O
to	O
be	O
able	O
to	O
send	O
out	O
to	O
cheapco	O
only	O
a	O
junior	O
repair	O
mechanic	O
,	O
having	O
most	O
likely	O
diagnosed	O
the	O
fault	O
over	O
the	O
phone	O
.	O
based	O
on	O
the	O
manufacturer	O
’	O
s	O
information	O
,	O
stoppress	O
has	O
a	O
good	O
idea	O
of	O
the	O
dependencies	O
in	O
the	O
printer	O
,	O
and	O
what	O
is	O
likely	O
to	O
directly	O
aﬀect	O
other	O
printer	O
components	O
.	O
the	O
belief	B
network	I
in	O
ﬁg	O
(	O
9.19	O
)	O
represents	O
these	O
assumptions	O
.	O
however	O
,	O
the	O
speciﬁc	O
way	O
that	O
cheapco	O
abuse	O
their	O
printer	O
is	O
a	O
mystery	O
,	O
so	O
that	O
the	O
exact	O
probabilistic	O
relationships	O
between	O
the	O
faults	O
and	O
problems	O
is	O
idiosyncratic	O
to	O
cheapco	O
.	O
stoppress	O
has	O
the	O
following	O
table	O
of	O
faults	O
for	O
each	O
of	O
the	O
10	O
visits	O
.	O
each	O
column	O
represents	O
a	O
visit	O
.	O
fuse	O
assembly	O
malfunction	O
drum	O
unit	O
toner	O
out	O
poor	O
paper	O
quality	O
worn	O
roller	O
burning	O
smell	O
poor	O
print	O
quality	O
wrinkled	O
pages	O
multiple	O
pages	O
fed	O
paper	O
jam	O
0	O
0	O
1	O
1	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
1	O
1	O
1	O
1	O
1	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
1	O
0	O
1	O
0	O
1	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
0	O
0	O
0	O
1	O
1	O
0	O
1	O
1	O
0	O
0	O
0	O
1	O
0	O
0	O
1	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
1	O
1	O
1	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
1	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
0	O
0	O
1	O
0	O
0	O
0	O
1	O
0	O
0	O
1	O
0	O
1	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
1	O
0	O
1	O
1	O
0	O
0	O
0	O
1	O
0	O
0	O
1	O
1	O
0	O
198	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
1.	O
the	O
above	O
table	O
is	O
contained	O
in	O
printer.mat	O
.	O
learn	O
all	O
table	O
entries	O
on	O
the	O
basis	O
of	O
maximum	B
likelihood	I
.	O
2.	O
program	O
the	O
belief	B
network	I
using	O
the	O
tables	O
maximum	B
likelihood	I
tables	O
and	O
brmltoolbox	O
.	O
com-	O
pute	O
the	O
probability	B
that	O
there	O
is	O
a	O
fuse	O
assembly	O
malfunction	O
given	O
that	O
the	O
secretary	O
complains	O
there	O
is	O
a	O
burning	O
smell	O
and	O
that	O
the	O
paper	O
is	O
jammed	O
,	O
and	O
that	O
there	O
are	O
no	O
other	O
problems	O
.	O
3.	O
repeat	O
the	O
above	O
calculation	O
using	O
a	O
bayesian	O
method	O
in	O
which	O
a	O
ﬂat	O
beta	B
prior	O
is	O
used	O
on	O
all	O
tables	O
.	O
4.	O
given	O
the	O
above	O
information	O
from	O
the	O
secretary	O
,	O
what	O
is	O
the	O
most	O
likely	O
joint	O
diagnosis	O
over	O
the	O
diagnostic	O
variables	O
–	O
that	O
is	O
the	O
joint	B
most	O
likely	O
p	O
(	O
f	O
use	O
,	O
drum	O
,	O
t	O
oner	O
,	O
p	O
aper	O
,	O
roller|evidence	O
)	O
?	O
use	O
the	O
max-absorption	O
method	O
on	O
the	O
associated	O
junction	B
tree	I
.	O
5.	O
compute	O
the	O
joint	B
most	O
likely	O
state	O
of	O
the	O
distribution	B
p	O
(	O
f	O
use	O
,	O
drum	O
,	O
t	O
oner	O
,	O
p	O
aper	O
,	O
roller|burning	O
smell	O
,	O
paper	O
jammed	O
)	O
explain	O
how	O
to	O
compute	O
this	O
eﬃciently	O
using	O
the	O
max-absorption	O
method	O
.	O
(	O
cid:80	O
)	O
n	O
exercise	O
117.	O
explain	O
how	O
to	O
use	O
a	O
factorised	B
beta	O
prior	B
in	O
the	O
case	O
of	O
learning	B
table	O
entries	O
in	O
belief	B
networks	I
in	O
which	O
each	O
variable	B
has	O
maximally	O
a	O
single	O
parent	O
.	O
consider	O
the	O
issues	O
around	O
bayesian	O
learning	B
of	O
binary	O
table	O
entries	O
when	O
the	O
number	O
of	O
parental	O
variables	O
is	O
not	O
restricted	B
.	O
exercise	O
118.	O
consider	O
data	B
xn	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
show	O
that	O
for	O
a	O
gaussian	O
distribution	B
,	O
the	O
maximum	B
likelihood	I
estimator	O
of	O
the	O
mean	B
is	O
ˆm	O
=	O
1	O
n	O
n=1	O
xn	O
and	O
variance	B
is	O
ˆσ2	O
=	O
1	O
n	O
exercise	O
119.	O
a	O
training	B
set	O
consists	O
of	O
one	O
dimensional	O
examples	O
from	O
two	O
classes	O
.	O
the	O
training	B
exam-	O
ples	O
from	O
class	O
1	O
are	O
0.5	O
,	O
0.1	O
,	O
0.2	O
,	O
0.4	O
,	O
0.3	O
,	O
0.2	O
,	O
0.2	O
,	O
0.1	O
,	O
0.35	O
,	O
0.25	O
and	O
from	O
class	O
2	O
are	O
0.9	O
,	O
0.8	O
,	O
0.75	O
,	O
1.0	O
(	O
9.7.1	O
)	O
(	O
9.7.2	O
)	O
(	O
cid:80	O
)	O
n	O
n=1	O
(	O
xn	O
−	O
ˆm	O
)	O
2.	O
fit	O
a	O
(	O
one	O
dimensional	O
)	O
gaussian	O
using	O
maximum	B
likelihood	I
to	O
each	O
of	O
these	O
two	O
classes	O
.	O
also	O
estimate	O
the	O
class	O
probabilities	O
p1	O
and	O
p2	O
using	O
maximum	B
likelihood	I
.	O
what	O
is	O
the	O
probability	B
that	O
the	O
test	O
point	O
x	O
=	O
0.6	O
belongs	O
to	O
class	O
1	O
?	O
exercise	O
120.	O
for	O
a	O
set	O
of	O
n	O
observations	O
(	O
training	B
data	O
)	O
,	O
x	O
=	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
and	O
independently	O
gathered	O
observations	O
,	O
the	O
log	O
likelihood	B
for	O
a	O
belief	B
network	I
to	O
generate	O
x	O
is	O
n	O
(	O
cid:88	O
)	O
k	O
(	O
cid:88	O
)	O
n=1	O
i=1	O
log	O
p	O
(	O
x	O
)	O
=	O
log	O
p	O
(	O
xn	O
i	O
|pa	O
(	O
xn	O
i	O
)	O
)	O
we	O
deﬁne	O
the	O
notation	O
(	O
9.7.3	O
)	O
(	O
9.7.4	O
)	O
(	O
9.7.5	O
)	O
(	O
9.7.6	O
)	O
199	O
meaning	O
variable	B
xi	O
is	O
in	O
state	O
s	O
,	O
and	O
the	O
parents	B
of	O
variable	B
xi	O
are	O
in	O
the	O
vector	O
of	O
states	O
t.	O
using	O
a	O
lagrangian	O
θi	O
s	O
(	O
t	O
)	O
=	O
p	O
(	O
xi	O
=	O
s|pa	O
(	O
xi	O
)	O
=	O
t	O
)	O
n	O
(	O
cid:88	O
)	O
k	O
(	O
cid:88	O
)	O
l	O
≡	O
n=1	O
i=1	O
log	O
p	O
(	O
xn	O
i	O
|pa	O
(	O
xn	O
i	O
)	O
)	O
+	O
(	O
cid:80	O
)	O
n	O
(	O
cid:80	O
)	O
n	O
n=1	O
i	O
(	O
cid:104	O
)	O
i	O
(	O
cid:104	O
)	O
(	O
cid:80	O
)	O
s	O
n=1	O
j	O
=	O
s	O
xn	O
(	O
cid:105	O
)	O
i	O
(	O
cid:104	O
)	O
(	O
cid:105	O
)	O
i	O
(	O
cid:104	O
)	O
pa	O
j	O
=	O
s	O
xn	O
pa	O
s	O
(	O
tj	O
)	O
=	O
θj	O
k	O
(	O
cid:88	O
)	O
(	O
cid:16	O
)	O
i=1	O
(	O
cid:16	O
)	O
xn	O
j	O
(	O
cid:32	O
)	O
ti	O
λi	O
ti	O
(	O
cid:88	O
)	O
=	O
tj	O
(	O
cid:105	O
)	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
=	O
tj	O
(	O
cid:105	O
)	O
xn	O
j	O
show	O
that	O
the	O
maximum	B
likelihood	I
setting	O
of	O
θi	O
s	O
(	O
t	O
)	O
is	O
(	O
cid:0	O
)	O
ti	O
(	O
cid:1	O
)	O
(	O
cid:33	O
)	O
(	O
cid:88	O
)	O
s	O
θi	O
s	O
1	O
−	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
n	O
(	O
cid:88	O
)	O
n=1	O
cl	O
(	O
θ	O
)	O
=	O
1	O
n	O
exercise	O
121	O
(	O
conditional	B
likelihood	I
training	O
)	O
.	O
consider	O
a	O
situation	O
in	O
which	O
we	O
partition	O
observable	O
variables	O
into	O
disjoint	O
sets	O
x	O
and	O
y	O
and	O
that	O
we	O
want	O
to	O
ﬁnd	O
the	O
parameters	O
that	O
maximize	O
the	O
conditional	B
likelihood	I
,	O
p	O
(	O
yn|xn	O
,	O
θ	O
)	O
,	O
(	O
9.7.7	O
)	O
for	O
a	O
set	O
of	O
training	B
data	O
{	O
(	O
xn	O
,	O
yn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
.	O
all	O
data	B
is	O
assumed	O
generated	O
from	O
the	O
same	O
distribu-	O
tion	O
p	O
(	O
x	O
,	O
y|θ0	O
)	O
=	O
p	O
(	O
y|x	O
,	O
θ0	O
)	O
p	O
(	O
x|θ0	O
)	O
for	O
some	O
unknown	O
parameter	B
θ0	O
.	O
in	O
the	O
limit	O
of	O
a	O
large	O
amount	O
of	O
i.i.d	O
.	O
training	B
data	O
,	O
does	O
cl	O
(	O
θ	O
)	O
have	O
an	O
optimum	O
at	O
θ0	O
?	O
exercise	O
122	O
(	O
moment	O
matching	O
)	O
.	O
one	O
way	O
to	O
set	O
parameters	O
of	O
a	O
distribution	B
is	O
to	O
match	O
the	O
moments	O
of	O
the	O
distribution	B
to	O
the	O
empirical	B
moments	O
.	O
this	O
sometimes	O
corresponds	O
to	O
maximum	B
likelihood	I
(	O
for	O
the	O
gaussian	O
distribution	B
for	O
example	O
)	O
,	O
though	O
generally	O
this	O
is	O
not	O
consistent	B
with	O
maximum	B
likelihood	I
.	O
for	O
data	B
with	O
mean	B
m	O
and	O
variance	B
s	O
,	O
show	O
that	O
to	O
ﬁt	O
a	O
beta	B
distribution	O
by	O
moment	O
matching	O
,	O
we	O
use	O
α	O
=	O
m2	O
(	O
1	O
−	O
m	O
)	O
s	O
1	O
−	O
m	O
m	O
(	O
9.7.8	O
)	O
exercise	O
123.	O
for	O
data	B
0	O
≤	O
xn	O
≤	O
1	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
generated	O
from	O
a	O
beta	B
distribution	O
b	O
(	O
x|a	O
,	O
b	O
)	O
,	O
show	O
that	O
the	O
log	O
likelihood	B
is	O
given	O
by	O
−	O
m	O
,	O
β	O
=	O
α	O
n	O
(	O
cid:88	O
)	O
n=1	O
n	O
(	O
cid:88	O
)	O
n=1	O
n	O
(	O
cid:88	O
)	O
n=1	O
l	O
(	O
a	O
,	O
b	O
)	O
≡	O
(	O
a	O
−	O
1	O
)	O
log	O
xn	O
+	O
(	O
b	O
−	O
1	O
)	O
log	O
(	O
1	O
−	O
xn	O
)	O
−	O
n	O
log	O
b	O
(	O
a	O
,	O
b	O
)	O
(	O
9.7.9	O
)	O
where	O
b	O
(	O
a	O
,	O
b	O
)	O
is	O
the	O
beta	B
function	O
.	O
show	O
that	O
the	O
derivatives	O
are	O
∂	O
∂a	O
l	O
=	O
log	O
xn	O
−	O
ψ	O
(	O
a	O
)	O
−	O
ψ	O
(	O
a	O
+	O
b	O
)	O
,	O
∂	O
∂b	O
l	O
=	O
log	O
(	O
1	O
−	O
xn	O
)	O
−	O
ψ	O
(	O
b	O
)	O
−	O
ψ	O
(	O
a	O
+	O
b	O
)	O
(	O
9.7.10	O
)	O
where	O
ψ	O
(	O
x	O
)	O
≡	O
d	O
log	O
γ	O
(	O
x	O
)	O
/dx	O
is	O
the	O
digamma	B
function	I
,	O
and	O
suggest	O
a	O
method	O
to	O
learn	O
the	O
parameters	O
a	O
,	O
b	O
.	O
exercise	O
124.	O
consider	O
the	O
boltzmann	O
machine	O
as	O
deﬁned	O
in	O
example	O
(	O
44	O
)	O
.	O
1.	O
derive	O
the	O
gradient	B
with	O
respect	O
to	O
the	O
‘	O
biases	O
’	O
wii	O
.	O
2.	O
write	O
down	O
the	O
pseudo	B
likelihood	I
for	O
a	O
set	O
of	O
i.i.d	O
.	O
data	B
v1	O
,	O
.	O
.	O
.	O
,	O
vn	O
and	O
derive	O
the	O
gradient	B
of	O
this	O
n	O
(	O
cid:88	O
)	O
n=1	O
exercise	O
125.	O
show	O
that	O
the	O
model	B
likelihood	O
equation	B
(	O
9.3.54	O
)	O
can	O
be	O
written	O
explicitly	O
as	O
with	O
respect	O
to	O
wij	O
,	O
i	O
(	O
cid:54	O
)	O
=	O
j.	O
p	O
(	O
d|m	O
)	O
=	O
(	O
cid:89	O
)	O
(	O
cid:89	O
)	O
v	O
j	O
γ	O
(	O
(	O
cid:80	O
)	O
γ	O
(	O
(	O
cid:80	O
)	O
(	O
cid:20	O
)	O
γ	O
(	O
u	O
(	O
cid:48	O
)	O
(	O
cid:89	O
)	O
(	O
cid:21	O
)	O
i	O
ui	O
(	O
v	O
;	O
j	O
)	O
)	O
i	O
u	O
(	O
cid:48	O
)	O
i	O
(	O
v	O
;	O
j	O
)	O
)	O
i	O
i	O
(	O
v	O
;	O
j	O
)	O
)	O
γ	O
(	O
ui	O
(	O
v	O
;	O
j	O
)	O
)	O
(	O
9.7.11	O
)	O
exercise	O
126.	O
deﬁne	O
the	O
set	O
n	O
as	O
consisting	O
of	O
8	O
node	B
belief	O
networks	O
in	O
which	O
each	O
node	B
has	O
at	O
most	O
2	O
parents	B
.	O
for	O
a	O
given	O
ancestral	B
order	I
a	O
,	O
the	O
restricted	B
set	O
is	O
written	O
na	O
1.	O
how	O
many	O
belief	B
networks	I
are	O
in	O
na	O
?	O
2.	O
what	O
is	O
the	O
computational	O
time	O
to	O
ﬁnd	O
the	O
optimal	O
member	O
of	O
na	O
using	O
the	O
bayesian	O
dirichlet	O
score	O
,	O
assuming	O
that	O
computing	O
the	O
bd	O
score	O
of	O
any	O
member	O
of	O
na	O
takes	O
1	O
second	O
and	O
bearing	O
in	O
mind	O
the	O
decomposability	O
of	O
the	O
bd	O
score	O
.	O
3.	O
what	O
is	O
the	O
time	O
to	O
ﬁnd	O
the	O
optimal	O
member	O
of	O
n	O
?	O
exercise	O
127.	O
for	O
the	O
markov	O
network	O
p	O
(	O
x	O
,	O
y	O
,	O
z	O
)	O
=	O
1	O
z	O
φ1	O
(	O
x	O
,	O
y	O
)	O
φ2	O
(	O
y	O
,	O
z	O
)	O
(	O
9.7.12	O
)	O
derive	O
an	O
iterative	B
scaling	I
algorithm	O
to	O
learn	O
the	O
unconstrained	O
tables	O
φ1	O
(	O
x	O
,	O
y	O
)	O
and	O
φ2	O
(	O
x	O
,	O
y	O
)	O
based	O
on	O
a	O
set	O
of	O
i.i.d	O
.	O
data	B
x	O
,	O
y	O
,	O
z	O
.	O
200	O
draft	O
march	O
9	O
,	O
2010	O
exercise	O
128.	O
given	O
training	B
data	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
derive	O
an	O
iterative	B
scaling	I
algorithm	O
for	O
maximum	O
likeli-	O
hood	O
training	B
of	O
crfs	O
of	O
the	O
form	O
1	O
p	O
(	O
x|λ	O
)	O
=	O
eλcfc	O
(	O
x	O
)	O
z	O
(	O
λ	O
)	O
where	O
z	O
(	O
λ	O
)	O
=	O
(	O
cid:80	O
)	O
can	O
not	O
all	O
be	O
zero	O
for	O
any	O
given	O
x	O
)	O
.	O
(	O
cid:81	O
)	O
exercise	O
129.	O
for	O
data	B
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
n	O
,	O
consider	O
maximum	B
likelihood	I
learning	O
of	O
a	O
markov	O
network	O
p	O
(	O
x	O
)	O
=	O
c	O
φc	O
(	O
xc	O
)	O
with	O
potentials	O
of	O
the	O
form	O
c	O
eλcfc	O
(	O
x	O
)	O
and	O
non-negative	O
features	O
,	O
fc	O
(	O
x	O
)	O
≥	O
0	O
(	O
you	O
may	O
assume	O
that	O
the	O
features	O
x	O
with	O
fc	O
(	O
xc	O
)	O
being	O
general	O
real	O
valued	O
functions	O
and	O
θc	O
real	O
valued	O
parameters	O
.	O
by	O
considering	O
(	O
9.7.13	O
)	O
(	O
9.7.14	O
)	O
exercises	O
(	O
cid:89	O
)	O
(	O
cid:81	O
)	O
c	O
φc	O
(	O
xc	O
)	O
=	O
eθcfc	O
(	O
xc	O
)	O
(	O
cid:88	O
)	O
θcfc	O
(	O
xc	O
)	O
=	O
(	O
cid:88	O
)	O
c	O
θcfc	O
(	O
xc	O
)	O
for	O
auxiliary	O
variables	O
pc	O
>	O
0	O
such	O
that	O
(	O
cid:80	O
)	O
pc	O
pc	O
c	O
algorithm	B
in	O
which	O
each	O
parameter	B
θc	O
can	O
be	O
learned	O
separately	O
.	O
c	O
pc	O
=	O
1	O
,	O
explain	O
how	O
to	O
derive	O
a	O
form	O
of	O
iterative	B
scaling	I
training	O
draft	O
march	O
9	O
,	O
2010	O
201	O
exercises	O
202	O
draft	O
march	O
9	O
,	O
2010	O
chapter	O
10	O
naive	O
bayes	O
10.1	O
naive	O
bayes	O
and	O
conditional	B
independence	O
naive	O
bayes	O
(	O
nb	O
)	O
is	O
a	O
popular	O
classiﬁcation	B
method	O
and	O
aids	O
our	O
discussion	O
of	O
conditional	B
independence	O
,	O
overﬁtting	B
and	O
bayesian	O
methods	O
.	O
in	O
nb	O
,	O
we	O
form	O
a	O
joint	B
model	O
of	O
observations	O
x	O
and	O
the	O
corresponding	O
class	O
label	O
c	O
using	O
a	O
belief	B
network	I
of	O
the	O
form	O
p	O
(	O
x	O
,	O
c	O
)	O
=	O
p	O
(	O
c	O
)	O
p	O
(	O
xi|c	O
)	O
(	O
10.1.1	O
)	O
whose	O
belief	B
network	I
is	O
depicted	O
in	O
ﬁg	O
(	O
10.1a	O
)	O
.	O
coupled	B
with	O
a	O
suitable	O
choice	O
for	O
each	O
conditional	B
distri-	O
bution	O
p	O
(	O
xi|c	O
)	O
,	O
we	O
can	O
then	O
use	O
bayes	O
’	O
rule	O
to	O
form	O
a	O
classiﬁer	B
for	O
a	O
novel	O
attribute	O
vector	O
x∗	O
:	O
d	O
(	O
cid:89	O
)	O
i=1	O
p	O
(	O
c|x∗	O
)	O
=	O
p	O
(	O
x∗	O
|c	O
)	O
p	O
(	O
c	O
)	O
p	O
(	O
x∗	O
)	O
(	O
cid:80	O
)	O
=	O
p	O
(	O
x∗	O
c	O
p	O
(	O
x∗	O
|c	O
)	O
p	O
(	O
c	O
)	O
|c	O
)	O
p	O
(	O
c	O
)	O
(	O
10.1.2	O
)	O
in	O
practice	O
it	O
is	O
common	O
to	O
consider	O
only	O
two	O
classes	O
dom	O
(	O
c	O
)	O
=	O
{	O
0	O
,	O
1	O
}	O
.	O
the	O
theory	O
we	O
describe	O
below	O
is	O
valid	O
for	O
any	O
number	O
of	O
classes	O
c	O
,	O
though	O
our	O
examples	O
are	O
restricted	B
to	O
the	O
binary	O
class	O
case	O
.	O
also	O
,	O
the	O
attributes	O
xi	O
are	O
often	O
taken	O
to	O
be	O
binary	O
,	O
as	O
we	O
shall	O
do	O
initially	O
below	O
as	O
well	O
.	O
the	O
extension	O
to	O
more	O
than	O
two	O
attribute	O
states	O
,	O
or	O
continuous	B
attributes	O
is	O
straightforward	O
.	O
example	O
47.	O
ezsurvey.org	O
considers	O
radio	O
station	O
listeners	O
conveniently	O
fall	O
into	O
two	O
groups	O
–	O
the	O
‘	O
young	O
’	O
and	O
‘	O
old	O
’	O
.	O
they	O
assume	O
that	O
,	O
given	O
the	O
knowledge	O
that	O
a	O
customer	O
is	O
either	O
‘	O
young	O
’	O
or	O
‘	O
old	O
’	O
,	O
this	O
is	O
suﬃcient	O
to	O
determine	O
whether	O
or	O
not	O
a	O
customer	O
will	O
like	O
a	O
particular	O
radio	O
station	O
,	O
independent	O
of	O
their	O
likes	O
or	O
dislikes	O
for	O
any	O
other	O
stations	O
:	O
p	O
(	O
r1	O
,	O
r2	O
,	O
r3	O
,	O
r4|age	O
)	O
=	O
p	O
(	O
r1|age	O
)	O
p	O
(	O
r2|age	O
)	O
p	O
(	O
r3|age	O
)	O
p	O
(	O
r4|age	O
)	O
(	O
10.1.3	O
)	O
where	O
each	O
of	O
the	O
variables	O
r1	O
,	O
r2	O
,	O
r3	O
,	O
r4	O
can	O
take	O
the	O
states	O
either	O
like	O
or	O
dislike	O
,	O
and	O
the	O
‘	O
age	O
’	O
variable	B
can	O
take	O
the	O
value	B
either	O
young	O
or	O
old	O
.	O
thus	O
the	O
information	O
about	O
the	O
age	O
of	O
the	O
customer	O
determines	O
the	O
individual	O
product	O
preferences	O
without	O
needing	O
to	O
know	O
anything	O
else	O
.	O
to	O
complete	O
the	O
speciﬁcation	O
,	O
given	O
that	O
a	O
customer	O
is	O
young	O
,	O
she	O
has	O
a	O
95	O
%	O
chance	O
to	O
like	O
radio1	O
,	O
a	O
5	O
%	O
chance	O
to	O
like	O
radio2	O
,	O
a	O
2	O
%	O
chance	O
to	O
like	O
radio3	O
and	O
a	O
20	O
%	O
chance	O
to	O
like	O
radio4	O
.	O
similarly	O
,	O
an	O
old	O
listener	O
has	O
a	O
3	O
%	O
chance	O
to	O
like	O
radio1	O
,	O
an	O
82	O
%	O
chance	O
to	O
like	O
radio2	O
,	O
a	O
34	O
%	O
chance	O
to	O
like	O
radio3	O
and	O
a	O
92	O
%	O
chance	O
to	O
like	O
radio4	O
.	O
they	O
know	O
that	O
90	O
%	O
of	O
the	O
listeners	O
are	O
old	O
.	O
203	O
estimation	O
using	O
maximum	B
likelihood	I
c	O
cn	O
xn	O
i	O
θc	O
θi	O
,	O
c	O
i	O
=	O
1	O
:	O
d	O
x1	O
x2	O
x3	O
n	O
=	O
1	O
:	O
n	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
10.1	O
:	O
naive	O
bayes	O
classifer	O
.	O
(	O
a	O
)	O
:	O
the	O
central	O
assumption	O
is	O
that	O
given	O
the	O
class	O
c	O
,	O
the	O
attributes	O
xi	O
(	O
b	O
)	O
:	O
assuming	O
the	O
data	B
is	O
i.i.d.	O
,	O
are	O
independent	O
.	O
maximum	B
likelihood	I
learns	O
the	O
optimal	O
parameters	O
of	O
the	O
distribution	B
p	O
(	O
c	O
)	O
and	O
the	O
class-dependent	O
at-	O
tribute	O
distributions	O
p	O
(	O
xi|c	O
)	O
.	O
given	O
this	O
model	B
,	O
and	O
a	O
new	O
customer	O
that	O
likes	O
radio1	O
,	O
and	O
radio3	O
,	O
but	O
dislikes	O
radio2	O
and	O
radio4	O
,	O
what	O
is	O
the	O
probability	B
that	O
they	O
are	O
young	O
?	O
this	O
is	O
given	O
by	O
p	O
(	O
age	O
=	O
young|r1	O
=	O
like	O
,	O
r2	O
=	O
dislike	O
,	O
r3	O
=	O
like	O
,	O
r4	O
=	O
dislike	O
)	O
(	O
cid:80	O
)	O
=	O
p	O
(	O
r1	O
=	O
like	O
,	O
r2	O
=	O
dislike	O
,	O
r3	O
=	O
like	O
,	O
r4	O
=	O
dislike|age	O
=	O
young	O
)	O
p	O
(	O
age	O
=	O
young	O
)	O
age	O
p	O
(	O
r1	O
=	O
like	O
,	O
r2	O
=	O
dislike	O
,	O
r3	O
=	O
like	O
,	O
r4	O
=	O
dislike|age	O
)	O
p	O
(	O
age	O
)	O
using	O
the	O
naive	O
bayes	O
structure	B
,	O
the	O
numerator	O
above	O
is	O
given	O
by	O
p	O
(	O
r1	O
=	O
like|age	O
=	O
young	O
)	O
p	O
(	O
r2	O
=	O
dislike|age	O
=	O
young	O
)	O
×	O
p	O
(	O
r3	O
=	O
like|age	O
=	O
young	O
)	O
p	O
(	O
r4	O
=	O
dislike|age	O
=	O
young	O
)	O
p	O
(	O
age	O
=	O
young	O
)	O
plugging	O
in	O
the	O
values	O
we	O
obtain	O
0.95	O
×	O
0.95	O
×	O
0.02	O
×	O
0.8	O
×	O
0.1	O
=	O
0.0014	O
(	O
10.1.4	O
)	O
(	O
10.1.5	O
)	O
the	O
denominator	O
is	O
given	O
by	O
this	O
value	B
plus	O
the	O
corresponding	O
term	O
evaluated	O
under	O
assuming	O
the	O
customer	O
is	O
old	O
,	O
0.03	O
×	O
0.18	O
×	O
0.34	O
×	O
0.08	O
×	O
0.9	O
=	O
1.3219	O
×	O
10−4	O
which	O
gives	O
p	O
(	O
age	O
=	O
young|r1	O
=	O
like	O
,	O
r2	O
=	O
dislike	O
,	O
r3	O
=	O
like	O
,	O
r4	O
=	O
dislike	O
)	O
=	O
0.0014	O
0.0014	O
+	O
1.3219	O
×	O
10−4	O
=	O
0.9161	O
(	O
10.1.6	O
)	O
10.2	O
estimation	O
using	O
maximum	B
likelihood	I
learning	O
the	O
table	O
entries	O
for	O
nb	O
is	O
a	O
straightforward	O
application	O
of	O
the	O
more	O
general	O
bn	O
learning	B
dis-	O
cussed	O
in	O
section	O
(	O
9.2.3	O
)	O
.	O
for	O
a	O
fully	O
observed	O
dataset	O
,	O
maximum	B
likelihood	I
learning	O
of	O
the	O
table	O
entries	O
corresponds	O
to	O
counting	B
the	O
number	O
of	O
occurrences	O
in	O
the	O
training	B
data	O
,	O
as	O
we	O
show	O
below	O
.	O
10.2.1	O
binary	O
attributes	O
consider	O
a	O
dataset	O
{	O
xn	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
of	O
binary	O
attributes	O
,	O
xn	O
i	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
d.	O
each	O
datapoint	O
xn	O
has	O
an	O
associated	O
class	O
label	O
cn	O
.	O
the	O
number	O
of	O
datapoints	O
from	O
class	O
c	O
=	O
0	O
is	O
n0	O
and	O
the	O
num-	O
ber	O
from	O
class	O
c	O
=	O
1	O
denoted	O
is	O
n1	O
.	O
for	O
each	O
attribute	O
of	O
the	O
two	O
classes	O
,	O
we	O
need	O
to	O
estimate	O
the	O
values	O
p	O
(	O
xi	O
=	O
1|c	O
)	O
≡	O
θc	O
i	O
.	O
the	O
other	O
probability	B
,	O
p	O
(	O
xi	O
=	O
0|c	O
)	O
is	O
given	O
by	O
the	O
normalisation	B
requirement	O
,	O
p	O
(	O
xi	O
=	O
0|c	O
)	O
=	O
1	O
−	O
p	O
(	O
xi	O
=	O
1|c	O
)	O
=	O
1	O
−	O
θc	O
i	O
.	O
204	O
draft	O
march	O
9	O
,	O
2010	O
based	O
on	O
the	O
nb	O
conditional	B
independence	O
assumption	O
the	O
probability	B
of	O
observing	O
a	O
vector	O
x	O
can	O
be	O
compactly	O
written	O
estimation	O
using	O
maximum	B
likelihood	I
d	O
(	O
cid:89	O
)	O
i=1	O
d	O
(	O
cid:89	O
)	O
i=1	O
p	O
(	O
x|c	O
)	O
=	O
p	O
(	O
xi|c	O
)	O
=	O
(	O
θc	O
i	O
)	O
xi	O
(	O
1	O
−	O
θc	O
i	O
)	O
1−xi	O
in	O
the	O
above	O
expression	O
,	O
xi	O
is	O
either	O
0	O
or	O
1	O
and	O
hence	O
each	O
i	O
term	O
contributes	O
a	O
factor	B
θc	O
i	O
if	O
xi	O
=	O
1	O
or	O
1−	O
θc	O
i	O
if	O
xi	O
=	O
0.	O
together	O
with	O
the	O
assumption	O
that	O
the	O
training	B
data	O
is	O
i.i.d	O
.	O
generated	O
,	O
the	O
log	O
likelihood	B
of	O
the	O
attributes	O
and	O
class	O
labels	O
is	O
log	O
p	O
(	O
xn	O
,	O
cn	O
)	O
=	O
(	O
cid:88	O
)	O
log	O
p	O
(	O
cn	O
)	O
(	O
cid:89	O
)	O
n	O
i	O
p	O
(	O
xn	O
i	O
|cn	O
)	O
i	O
log	O
θcn	O
xn	O
i	O
+	O
(	O
1	O
−	O
xn	O
i	O
)	O
log	O
(	O
1	O
−	O
θcn	O
i	O
)	O
+	O
n0	O
log	O
p	O
(	O
c	O
=	O
0	O
)	O
+	O
n1	O
log	O
p	O
(	O
c	O
=	O
1	O
)	O
this	O
can	O
be	O
written	O
more	O
explicitly	O
in	O
terms	O
of	O
the	O
parameters	O
as	O
n	O
l	O
=	O
(	O
cid:88	O
)	O
=	O
(	O
cid:88	O
)	O
l	O
=	O
(	O
cid:88	O
)	O
i	O
,	O
n	O
i	O
,	O
n	O
(	O
cid:8	O
)	O
i	O
[	O
xn	O
i	O
=	O
1	O
,	O
cn	O
=	O
0	O
]	O
log	O
θ0	O
i	O
+	O
i	O
[	O
xn	O
i	O
=	O
0	O
,	O
cn	O
=	O
0	O
]	O
log	O
(	O
1	O
−	O
θ0	O
i	O
)	O
+	O
i	O
[	O
xn	O
i	O
=	O
1	O
,	O
cn	O
=	O
1	O
]	O
log	O
θ1	O
i	O
i	O
)	O
(	O
cid:9	O
)	O
+	O
n0	O
log	O
p	O
(	O
c	O
=	O
0	O
)	O
+	O
n1	O
log	O
p	O
(	O
c	O
=	O
1	O
)	O
we	O
can	O
ﬁnd	O
the	O
maximum	B
likelihood	I
optimal	O
θc	O
i	O
by	O
diﬀerentiating	O
w.r.t	O
.	O
θc	O
i	O
and	O
equating	O
to	O
zero	O
,	O
giving	O
+	O
i	O
[	O
xn	O
i	O
=	O
0	O
,	O
cn	O
=	O
1	O
]	O
log	O
(	O
1	O
−	O
θ1	O
(	O
cid:80	O
)	O
i	O
=	O
1	O
,	O
cn	O
=	O
c	O
]	O
i	O
[	O
xn	O
n	O
(	O
cid:80	O
)	O
θc	O
i	O
=	O
p	O
(	O
xi	O
=	O
1|c	O
)	O
=	O
=	O
i	O
[	O
xn	O
i	O
=	O
0	O
,	O
cn	O
=	O
c	O
]	O
+	O
i	O
[	O
xn	O
n	O
i	O
=	O
1	O
,	O
cn	O
=	O
c	O
]	O
number	O
of	O
times	O
xi	O
=	O
1	O
for	O
class	O
c	O
number	O
of	O
datapoints	O
in	O
class	O
c	O
(	O
10.2.1	O
)	O
(	O
10.2.2	O
)	O
(	O
10.2.3	O
)	O
(	O
10.2.4	O
)	O
(	O
10.2.5	O
)	O
(	O
10.2.6	O
)	O
(	O
10.2.7	O
)	O
(	O
10.2.8	O
)	O
(	O
10.2.9	O
)	O
similarly	O
,	O
optimising	O
equation	B
(	O
10.2.3	O
)	O
with	O
respect	O
to	O
p	O
(	O
c	O
)	O
gives	O
p	O
(	O
c	O
)	O
=	O
number	O
of	O
times	O
class	O
c	O
occurs	O
total	O
number	O
of	O
data	B
points	O
classiﬁcation	B
boundary	O
we	O
classify	O
a	O
novel	O
input	O
x∗	O
as	O
class	O
1	O
if	O
p	O
(	O
c	O
=	O
1|x∗	O
)	O
>	O
p	O
(	O
c	O
=	O
0|x∗	O
)	O
using	O
bayes	O
’	O
rule	O
and	O
writing	O
the	O
log	O
of	O
the	O
above	O
expression	O
,	O
this	O
is	O
equivalent	B
to	O
(	O
cid:88	O
)	O
(	O
cid:8	O
)	O
x	O
i	O
log	O
p	O
(	O
x∗	O
|c	O
=	O
1	O
)	O
+	O
log	O
p	O
(	O
c	O
=	O
1	O
)	O
−	O
log	O
p	O
(	O
x∗	O
)	O
>	O
log	O
p	O
(	O
x∗	O
|c	O
=	O
0	O
)	O
+	O
log	O
p	O
(	O
c	O
=	O
0	O
)	O
−	O
log	O
p	O
(	O
x∗	O
)	O
from	O
the	O
deﬁnition	O
of	O
the	O
classiﬁer	B
,	O
this	O
is	O
equivalent	B
to	O
(	O
the	O
normalisation	B
constant	I
−	O
log	O
p	O
(	O
x∗	O
)	O
can	O
be	O
dropped	O
from	O
both	O
sides	O
)	O
∗	O
i|c	O
=	O
1	O
)	O
+	O
log	O
p	O
(	O
c	O
=	O
1	O
)	O
>	O
log	O
p	O
(	O
x	O
log	O
p	O
(	O
x	O
(	O
cid:88	O
)	O
i	O
∗	O
i|c	O
=	O
0	O
)	O
+	O
log	O
p	O
(	O
c	O
=	O
0	O
)	O
(	O
cid:88	O
)	O
∗	O
i	O
log	O
θ1	O
(	O
cid:88	O
)	O
using	O
the	O
binary	O
encoding	O
xi	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
we	O
classify	O
x∗	O
as	O
class	O
1	O
if	O
(	O
cid:8	O
)	O
x	O
∗	O
i	O
log	O
θ0	O
this	O
decision	O
rule	O
can	O
be	O
expressed	O
in	O
the	O
form	O
:	O
classify	O
x∗	O
as	O
class	O
1	O
if	O
(	O
cid:80	O
)	O
i	O
)	O
(	O
cid:9	O
)	O
+log	O
p	O
(	O
c	O
=	O
1	O
)	O
>	O
∗	O
i	O
)	O
log	O
(	O
1	O
−	O
θ1	O
i	O
+	O
(	O
1	O
−	O
x	O
i	O
i	O
i	O
+	O
(	O
1	O
−	O
x	O
i	O
wix∗	O
i	O
+	O
a	O
>	O
0	O
for	O
some	O
suitable	O
choice	O
of	O
weights	O
wi	O
and	O
constant	O
a	O
,	O
see	O
exercise	O
(	O
133	O
)	O
.	O
the	O
interpretation	O
is	O
that	O
w	O
speciﬁes	O
a	O
hyperplane	B
in	O
the	O
attribute	O
space	O
and	O
x∗	O
is	O
classiﬁed	O
as	O
1	O
if	O
it	O
lies	O
on	O
the	O
positive	O
side	O
of	O
the	O
hyperplane	B
.	O
(	O
10.2.10	O
)	O
i	O
)	O
(	O
cid:9	O
)	O
+log	O
p	O
(	O
c	O
=	O
0	O
)	O
∗	O
i	O
)	O
log	O
(	O
1	O
−	O
θ0	O
(	O
10.2.11	O
)	O
draft	O
march	O
9	O
,	O
2010	O
205	O
estimation	O
using	O
maximum	B
likelihood	I
0	O
0	O
1	O
1	O
1	O
1	O
0	O
1	O
1	O
0	O
1	O
1	O
0	O
0	O
0	O
1	O
1	O
0	O
0	O
1	O
(	O
a	O
)	O
0	O
1	O
0	O
0	O
1	O
0	O
0	O
0	O
1	O
0	O
1	O
0	O
0	O
1	O
1	O
1	O
1	O
0	O
0	O
1	O
1	O
1	O
1	O
1	O
0	O
1	O
1	O
0	O
1	O
0	O
(	O
b	O
)	O
1	O
1	O
0	O
1	O
1	O
1	O
0	O
1	O
1	O
0	O
1	O
0	O
1	O
0	O
0	O
figure	O
10.2	O
:	O
(	O
a	O
)	O
:	O
english	O
tastes	O
over	O
attributes	O
(	O
shortbread	O
,	O
lager	O
,	O
whiskey	O
,	O
porridge	O
,	O
f	O
ootball	O
)	O
.	O
each	O
column	O
represents	O
the	O
tastes	O
of	O
an	O
indi-	O
vidual	O
.	O
(	O
b	O
)	O
:	O
scottish	O
tastes	O
.	O
example	O
48	O
(	O
are	O
they	O
scottish	O
?	O
)	O
.	O
consider	O
the	O
following	O
vector	O
of	O
attributes	O
:	O
(	O
likes	O
shortbread	O
,	O
likes	O
lager	O
,	O
drinks	O
whiskey	O
,	O
eats	O
porridge	O
,	O
watched	O
england	O
play	O
football	O
)	O
(	O
10.2.12	O
)	O
a	O
vector	O
x	O
=	O
(	O
1	O
,	O
0	O
,	O
1	O
,	O
1	O
,	O
0	O
)	O
t	O
would	O
describe	O
that	O
a	O
person	O
likes	O
shortbread	O
,	O
does	O
not	O
like	O
lager	O
,	O
drinks	O
whiskey	O
,	O
eats	O
porridge	O
,	O
and	O
has	O
not	O
watched	O
england	O
play	O
football	O
.	O
together	O
with	O
each	O
vector	O
x	O
,	O
there	O
is	O
a	O
label	O
nat	O
describing	O
the	O
nationality	O
of	O
the	O
person	O
,	O
dom	O
(	O
nat	O
)	O
=	O
{	O
scottish	O
,	O
english	O
}	O
,	O
see	O
ﬁg	O
(	O
10.2	O
)	O
.	O
we	O
wish	O
to	O
classify	O
the	O
vector	O
x	O
=	O
(	O
1	O
,	O
0	O
,	O
1	O
,	O
1	O
,	O
0	O
)	O
t	O
as	O
either	O
scottish	O
or	O
english	O
.	O
we	O
can	O
use	O
bayes	O
’	O
rule	O
to	O
calculate	O
the	O
probability	B
that	O
x	O
is	O
scottish	O
or	O
english	O
:	O
p	O
(	O
scottish|x	O
)	O
=	O
p	O
(	O
x|scottish	O
)	O
p	O
(	O
scottish	O
)	O
p	O
(	O
x	O
)	O
=	O
p	O
(	O
x|scottish	O
)	O
p	O
(	O
scottish	O
)	O
p	O
(	O
x|scottish	O
)	O
p	O
(	O
scottish	O
)	O
+	O
p	O
(	O
x|english	O
)	O
p	O
(	O
english	O
)	O
(	O
10.2.13	O
)	O
by	O
maximum	B
likelihood	I
the	O
‘	O
prior	B
’	O
class	O
probability	B
p	O
(	O
scottish	O
)	O
is	O
given	O
by	O
the	O
fraction	O
of	O
people	O
in	O
the	O
database	O
that	O
are	O
scottish	O
,	O
and	O
similarly	O
p	O
(	O
english	O
)	O
is	O
given	O
as	O
the	O
fraction	O
of	O
people	O
in	O
the	O
database	O
that	O
are	O
english	O
.	O
this	O
gives	O
p	O
(	O
scottish	O
)	O
=	O
7/13	O
and	O
p	O
(	O
english	O
)	O
=	O
6/13	O
.	O
for	O
p	O
(	O
x|nat	O
)	O
under	O
the	O
naive	O
bayes	O
assumption	O
:	O
p	O
(	O
x|nat	O
)	O
=	O
p	O
(	O
x1|nat	O
)	O
p	O
(	O
x2|nat	O
)	O
p	O
(	O
x3|nat	O
)	O
p	O
(	O
x4|nat	O
)	O
p	O
(	O
x5|nat	O
)	O
(	O
10.2.14	O
)	O
so	O
that	O
knowing	O
whether	O
not	O
someone	O
is	O
scottish	O
,	O
we	O
don	O
’	O
t	O
need	O
to	O
know	O
anything	O
else	O
to	O
calculate	O
the	O
probability	B
of	O
their	O
likes	O
and	O
dislikes	O
.	O
based	O
on	O
the	O
table	O
in	O
ﬁg	O
(	O
10.2	O
)	O
and	O
using	O
maximum	B
likelihood	I
we	O
have	O
:	O
p	O
(	O
x1	O
=	O
1|english	O
)	O
=	O
1/2	O
p	O
(	O
x2	O
=	O
1|english	O
)	O
=	O
1/2	O
p	O
(	O
x3	O
=	O
1|english	O
)	O
=	O
1/3	O
p	O
(	O
x4	O
=	O
1|english	O
)	O
=	O
1/2	O
p	O
(	O
x5	O
=	O
1|english	O
)	O
=	O
1/2	O
for	O
x	O
=	O
(	O
1	O
,	O
0	O
,	O
1	O
,	O
1	O
,	O
0	O
)	O
t	O
,	O
we	O
get	O
p	O
(	O
x1	O
=	O
1|scottish	O
)	O
=	O
1	O
p	O
(	O
x2	O
=	O
1|scottish	O
)	O
=	O
4/7	O
p	O
(	O
x3	O
=	O
1|scottish	O
)	O
=	O
3/7	O
p	O
(	O
x4	O
=	O
1|scottish	O
)	O
=	O
5/7	O
p	O
(	O
x5	O
=	O
1|scottish	O
)	O
=	O
3/7	O
p	O
(	O
scottish|x	O
)	O
=	O
1	O
×	O
3	O
7	O
×	O
3	O
7	O
×	O
5	O
1	O
×	O
3	O
7	O
×	O
4	O
7	O
×	O
3	O
7	O
×	O
7	O
7	O
×	O
5	O
13	O
+	O
1	O
7	O
×	O
4	O
2	O
×	O
1	O
7	O
×	O
7	O
2	O
×	O
1	O
13	O
3	O
×	O
1	O
2	O
×	O
1	O
2	O
×	O
6	O
13	O
(	O
10.2.15	O
)	O
=	O
0.8076	O
(	O
10.2.16	O
)	O
since	O
this	O
is	O
greater	O
than	O
0.5	O
,	O
we	O
would	O
classify	O
this	O
person	O
as	O
being	O
scottish	O
.	O
small	O
data	B
counts	O
in	O
example	O
(	O
48	O
)	O
,	O
consider	O
trying	O
to	O
classify	O
the	O
vector	O
x	O
=	O
(	O
0	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
)	O
t.	O
in	O
the	O
training	B
data	O
,	O
all	O
scottish	O
people	O
say	O
they	O
like	O
shortbread	O
.	O
this	O
means	O
that	O
for	O
this	O
particular	O
x	O
,	O
p	O
(	O
x	O
,	O
scottish	O
)	O
=	O
0	O
,	O
and	O
therefore	O
that	O
we	O
make	O
the	O
extremely	O
conﬁdent	O
classiﬁcation	B
p	O
(	O
scottish|x	O
)	O
=	O
0.	O
this	O
demonstrates	O
a	O
diﬃculty	O
using	O
maximum	B
likelihood	I
with	O
sparse	B
data	O
.	O
one	O
way	O
to	O
ameliorate	O
this	O
is	O
to	O
smooth	O
the	O
probabilities	O
,	O
for	O
example	O
by	O
adding	O
a	O
certain	O
small	O
number	O
to	O
the	O
frequency	O
counts	O
of	O
each	O
attribute	O
.	O
this	O
ensures	O
that	O
206	O
draft	O
march	O
9	O
,	O
2010	O
estimation	O
using	O
maximum	B
likelihood	I
there	O
are	O
no	O
zero	O
probabilities	O
in	O
the	O
model	B
.	O
an	O
alternative	O
is	O
to	O
use	O
a	O
bayesian	O
approach	B
that	O
discourages	O
extreme	O
probabilities	O
,	O
as	O
discussed	O
in	O
section	O
(	O
10.3	O
)	O
.	O
potential	B
pitfalls	O
with	O
encoding	O
in	O
many	O
oﬀ-the-shelf	O
packages	O
implementing	O
naive	O
bayes	O
,	O
binary	O
attributes	O
are	O
assumed	O
.	O
in	O
practice	O
,	O
however	O
,	O
the	O
case	O
of	O
non-binary	O
attributes	O
often	O
occurs	O
.	O
consider	O
the	O
following	O
attribute	O
:	O
age	O
.	O
in	O
a	O
survey	O
,	O
a	O
person	O
’	O
s	O
age	O
is	O
marked	O
down	O
using	O
the	O
variable	B
a	O
∈	O
1	O
,	O
2	O
,	O
3.	O
a	O
=	O
1	O
means	O
the	O
person	O
is	O
between	O
0	O
and	O
10	O
years	O
old	O
,	O
a	O
=	O
2	O
means	O
the	O
person	O
is	O
between	O
10	O
and	O
20	O
years	O
old	O
,	O
a	O
=	O
3	O
means	O
the	O
person	O
is	O
older	O
than	O
20.	O
one	O
way	O
to	O
transform	O
the	O
variable	B
a	O
into	O
a	O
binary	O
representation	O
would	O
be	O
to	O
use	O
three	O
binary	O
variables	O
(	O
a1	O
,	O
a2	O
,	O
a3	O
)	O
with	O
(	O
1	O
,	O
0	O
,	O
0	O
)	O
,	O
(	O
0	O
,	O
1	O
,	O
0	O
)	O
,	O
(	O
0	O
,	O
0	O
,	O
1	O
)	O
representing	O
a	O
=	O
1	O
,	O
a	O
=	O
2	O
,	O
a	O
=	O
3	O
respectively	O
.	O
this	O
is	O
called	O
1	O
−	O
of	O
−	O
m	O
coding	O
since	O
only	O
1	O
of	O
the	O
binary	O
variables	O
is	O
active	B
in	O
encoding	O
the	O
m	O
states	O
.	O
by	O
construction	B
,	O
means	O
that	O
the	O
variables	O
a1	O
,	O
a2	O
,	O
a3	O
are	O
dependent	O
–	O
for	O
example	O
,	O
if	O
we	O
know	O
that	O
a1	O
=	O
1	O
,	O
we	O
know	O
that	O
a2	O
=	O
0	O
and	O
a3	O
=	O
0.	O
regardless	O
of	O
any	O
class	O
conditioning	B
,	O
these	O
variables	O
will	O
always	O
be	O
dependent	O
,	O
contrary	O
to	O
the	O
assumption	O
of	O
naive	O
bayes	O
.	O
a	O
correct	O
approach	B
is	O
to	O
use	O
variables	O
with	O
more	O
than	O
two	O
states	O
,	O
as	O
explained	O
in	O
section	O
(	O
10.2.2	O
)	O
.	O
10.2.2	O
multi-state	O
variables	O
for	O
a	O
variable	B
xi	O
with	O
more	O
than	O
two	O
states	O
,	O
dom	O
(	O
xi	O
)	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
s	O
}	O
,	O
the	O
likelihood	B
of	O
observing	O
a	O
state	O
xi	O
=	O
s	O
is	O
denoted	O
s	O
(	O
c	O
)	O
with	O
(	O
cid:80	O
)	O
p	O
(	O
xi	O
=	O
s|c	O
)	O
=	O
θi	O
s	O
p	O
(	O
xi	O
=	O
s|c	O
)	O
=	O
1.	O
for	O
a	O
set	O
of	O
data	B
vectors	O
xn	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
n	O
,	O
belonging	O
to	O
class	O
c	O
,	O
under	O
the	O
i.i.d	O
.	O
n	O
(	O
cid:89	O
)	O
assumption	O
,	O
the	O
likelihood	B
of	O
the	O
nb	O
model	B
generating	O
data	B
from	O
class	O
c	O
is	O
n	O
(	O
cid:89	O
)	O
d	O
(	O
cid:89	O
)	O
s	O
(	O
cid:89	O
)	O
c	O
(	O
cid:89	O
)	O
(	O
10.2.17	O
)	O
i	O
[	O
xn	O
i	O
=s	O
]	O
i	O
[	O
cn=c	O
]	O
θi	O
s	O
(	O
c	O
)	O
(	O
10.2.18	O
)	O
n=1	O
n=1	O
i=1	O
s=1	O
c=1	O
which	O
gives	O
the	O
class	O
conditional	B
log-likelihood	O
l	O
=	O
i	O
[	O
xn	O
i	O
=	O
s	O
]	O
i	O
[	O
cn	O
=	O
c	O
]	O
log	O
θi	O
s	O
(	O
c	O
)	O
(	O
10.2.19	O
)	O
p	O
(	O
xn|cn	O
)	O
=	O
s	O
(	O
cid:88	O
)	O
d	O
(	O
cid:88	O
)	O
n	O
(	O
cid:88	O
)	O
c	O
(	O
cid:88	O
)	O
n=1	O
i=1	O
s=1	O
c=1	O
n	O
(	O
cid:88	O
)	O
d	O
(	O
cid:88	O
)	O
s	O
(	O
cid:88	O
)	O
c	O
(	O
cid:88	O
)	O
we	O
can	O
optimize	O
with	O
respect	O
to	O
the	O
parameters	O
θ	O
using	O
a	O
lagrange	O
multiplier	B
(	O
one	O
for	O
each	O
of	O
the	O
attributes	O
i	O
and	O
classes	O
c	O
)	O
to	O
ensure	O
normalisation	B
:	O
l	O
(	O
θ	O
)	O
=	O
i	O
[	O
xn	O
i	O
=	O
s	O
]	O
i	O
[	O
cn	O
=	O
c	O
]	O
log	O
θi	O
s	O
(	O
c	O
)	O
+	O
n=1	O
i=1	O
s=1	O
c=1	O
c=1	O
i=1	O
θi	O
s	O
(	O
c	O
)	O
(	O
10.2.20	O
)	O
to	O
ﬁnd	O
the	O
optimum	O
of	O
this	O
function	B
we	O
may	O
diﬀerentiate	O
with	O
respect	O
to	O
θi	O
the	O
resulting	O
equation	B
we	O
obtain	O
s	O
(	O
c	O
)	O
and	O
equate	O
to	O
zero	O
.	O
solving	B
c	O
(	O
cid:88	O
)	O
d	O
(	O
cid:88	O
)	O
(	O
cid:32	O
)	O
1	O
−	O
s	O
(	O
cid:88	O
)	O
s=1	O
λc	O
i	O
(	O
cid:33	O
)	O
n	O
(	O
cid:88	O
)	O
n=1	O
i	O
[	O
xn	O
i	O
=	O
s	O
]	O
i	O
[	O
cn	O
=	O
c	O
]	O
s	O
(	O
c	O
)	O
θi	O
=	O
λc	O
i	O
hence	O
,	O
by	O
normalisation	B
,	O
θi	O
s	O
(	O
c	O
)	O
=	O
p	O
(	O
xi	O
=	O
s|c	O
)	O
=	O
(	O
cid:80	O
)	O
s	O
(	O
cid:48	O
)	O
,	O
n	O
(	O
cid:48	O
)	O
i	O
(	O
cid:2	O
)	O
xn	O
(	O
cid:48	O
)	O
(	O
cid:80	O
)	O
i	O
[	O
xn	O
n	O
i	O
=	O
s	O
]	O
i	O
[	O
cn	O
=	O
c	O
]	O
i	O
=	O
s	O
(	O
cid:48	O
)	O
(	O
cid:3	O
)	O
i	O
[	O
cn	O
(	O
cid:48	O
)	O
=	O
c	O
]	O
(	O
10.2.21	O
)	O
(	O
10.2.22	O
)	O
the	O
maximum	B
likelihood	I
setting	O
for	O
the	O
parameter	B
p	O
(	O
xi	O
=	O
s|c	O
)	O
equals	O
the	O
relative	O
number	O
of	O
times	O
that	O
attribute	O
i	O
is	O
in	O
state	O
s	O
for	O
class	O
c.	O
draft	O
march	O
9	O
,	O
2010	O
207	O
bayesian	O
naive	O
bayes	O
figure	O
10.3	O
:	O
bayesian	O
naive	O
bayes	O
with	O
a	O
factorised	B
prior	O
on	O
the	O
class	O
conditional	B
attribute	O
probabilities	O
p	O
(	O
xi	O
=	O
s|c	O
)	O
.	O
for	O
simplicity	O
we	O
assume	O
that	O
the	O
class	O
probability	B
θc	O
≡	O
p	O
(	O
c	O
)	O
is	O
learned	O
with	O
maximum	O
likelihood	B
,	O
so	O
that	O
no	O
distribution	O
is	O
placed	O
over	O
this	O
parameter	B
.	O
θc	O
n	O
=	O
1	O
:	O
n	O
cn	O
xn	O
i	O
θi	O
,	O
c	O
c	O
=	O
1	O
:	O
c	O
i	O
=	O
1	O
:	O
d	O
10.2.3	O
text	O
classiﬁcation	O
consider	O
a	O
set	O
of	O
documents	O
about	O
politics	O
,	O
and	O
another	O
set	O
about	O
sport	O
.	O
our	O
interest	O
is	O
to	O
make	O
a	O
method	O
that	O
can	O
automatically	O
classify	O
a	O
new	O
document	O
as	O
pertaining	O
to	O
either	O
sport	O
or	O
politics	O
.	O
we	O
search	O
through	O
both	O
sets	O
of	O
documents	O
to	O
ﬁnd	O
the	O
100	O
most	O
commonly	O
occurring	O
words	O
.	O
each	O
document	O
is	O
then	O
represented	O
by	O
a	O
100	O
dimensional	O
vector	O
representing	O
the	O
number	O
of	O
times	O
that	O
each	O
of	O
the	O
words	O
occurs	O
in	O
that	O
document	O
–	O
the	O
so	O
called	O
bag	B
of	I
words	I
representation	O
(	O
this	O
is	O
a	O
crude	O
representation	B
of	O
the	O
document	O
since	O
it	O
discards	O
word	O
order	O
)	O
.	O
a	O
naive	O
bayes	O
model	B
speciﬁes	O
a	O
distribution	B
of	O
these	O
number	O
of	O
occurrences	O
p	O
(	O
xi|c	O
)	O
,	O
where	O
xi	O
is	O
the	O
count	O
of	O
the	O
number	O
of	O
times	O
word	O
i	O
appears	O
in	O
documents	O
of	O
type	O
c.	O
one	O
can	O
achieve	O
this	O
using	O
either	O
a	O
multistate	O
representation	B
(	O
as	O
discussed	O
in	O
section	O
(	O
10.2.2	O
)	O
)	O
or	O
using	O
a	O
continuous	B
xi	O
to	O
represent	O
the	O
frequency	O
of	O
word	O
i	O
in	O
the	O
document	O
.	O
in	O
this	O
case	O
p	O
(	O
xi|c	O
)	O
could	O
be	O
conve-	O
niently	O
modelled	O
using	O
for	O
example	O
a	O
beta	B
distribution	O
.	O
intuitively	O
a	O
despite	O
the	O
simplicity	O
of	O
naive	O
bayes	O
,	O
it	O
can	O
classify	O
documents	O
surprisingly	O
well	O
[	O
125	O
]	O
.	O
potential	B
justiﬁcation	O
for	O
the	O
conditional	B
independence	O
assumption	O
is	O
that	O
if	O
we	O
know	O
a	O
document	O
is	O
about	O
politics	O
,	O
this	O
is	O
a	O
good	O
indication	O
of	O
the	O
kinds	O
of	O
other	O
words	O
we	O
will	O
ﬁnd	O
in	O
the	O
document	O
.	O
because	O
naive	O
bayes	O
is	O
a	O
reasonable	O
classiﬁer	B
in	O
this	O
sense	O
,	O
and	O
has	O
minimal	O
storage	O
and	O
fast	O
training	B
,	O
it	O
has	O
been	O
applied	O
to	O
time-storage	O
critical	O
applications	O
,	O
such	O
as	O
automatically	O
classifying	O
webpages	O
into	O
types	O
[	O
289	O
]	O
,	O
and	O
spam	B
ﬁltering	I
[	O
9	O
]	O
.	O
10.3	O
bayesian	O
naive	O
bayes	O
to	O
predict	O
the	O
class	O
c	O
of	O
an	O
input	O
x	O
we	O
use	O
p	O
(	O
c|x	O
,	O
d	O
)	O
∝	O
p	O
(	O
x	O
,	O
d	O
,	O
c	O
)	O
p	O
(	O
c|d	O
)	O
∝	O
p	O
(	O
x|d	O
,	O
c	O
)	O
p	O
(	O
c|d	O
)	O
for	O
convenience	O
we	O
will	O
simply	O
set	O
p	O
(	O
c|d	O
)	O
using	O
maximum	B
likelihood	I
(	O
cid:88	O
)	O
n	O
p	O
(	O
c|d	O
)	O
=	O
1	O
n	O
i	O
[	O
cn	O
=	O
c	O
]	O
(	O
10.3.1	O
)	O
(	O
10.3.2	O
)	O
however	O
,	O
as	O
we	O
’	O
ve	O
seen	O
,	O
setting	O
the	O
parameters	O
of	O
p	O
(	O
x|d	O
,	O
c	O
)	O
using	O
maximum	B
likelihood	I
training	O
can	O
yield	O
over-conﬁdent	O
predictions	O
in	O
the	O
case	O
of	O
sparse	B
data	O
.	O
a	O
bayesian	O
approach	B
that	O
addresses	O
this	O
diﬃculty	O
s	O
(	O
c	O
)	O
that	O
discourage	O
extreme	O
values	O
.	O
the	O
model	B
is	O
is	O
to	O
use	O
priors	O
on	O
the	O
probabilities	O
p	O
(	O
xi	O
=	O
s|c	O
)	O
≡	O
θi	O
depicted	O
in	O
ﬁg	O
(	O
10.3	O
)	O
.	O
the	O
prior	B
p	O
(	O
θ	O
)	O
=	O
(	O
cid:89	O
)	O
i	O
,	O
c	O
208	O
we	O
will	O
use	O
a	O
prior	B
on	O
the	O
table	O
entries	O
and	O
make	O
the	O
global	B
factorisation	O
assumption	O
(	O
see	O
section	O
(	O
9.3	O
)	O
)	O
p	O
(	O
θi	O
(	O
c	O
)	O
)	O
(	O
10.3.3	O
)	O
draft	O
march	O
9	O
,	O
2010	O
bayesian	O
naive	O
bayes	O
we	O
consider	O
discrete	B
xi	O
each	O
of	O
which	O
take	O
states	O
from	O
1	O
,	O
.	O
.	O
.	O
,	O
s.	O
in	O
this	O
case	O
p	O
(	O
xi	O
=	O
s|c	O
)	O
corresponds	O
to	O
a	O
multinomial	B
distribution	O
,	O
for	O
which	O
the	O
conjugate	B
prior	O
is	O
a	O
dirichlet	O
distribution	B
.	O
under	O
the	O
factorised	B
prior	O
assumption	O
(	O
10.3.3	O
)	O
we	O
deﬁne	O
a	O
prior	B
for	O
each	O
attribute	O
i	O
and	O
class	O
c	O
,	O
p	O
(	O
θi	O
(	O
c	O
)	O
)	O
=	O
dirichlet	O
(	O
cid:0	O
)	O
θi	O
(	O
c	O
)	O
|ui	O
(	O
c	O
)	O
(	O
cid:1	O
)	O
where	O
ui	O
(	O
c	O
)	O
is	O
the	O
hyperparameter	B
vector	O
of	O
the	O
dirichlet	O
distribution	B
for	O
table	O
p	O
(	O
xi|c	O
)	O
.	O
the	O
posterior	B
first	O
let	O
’	O
s	O
see	O
how	O
the	O
bayesian	O
approach	B
is	O
used	O
to	O
classify	O
a	O
novel	O
point	O
x∗	O
.	O
let	O
d	O
denote	O
the	O
training	B
data	O
(	O
xn	O
,	O
cn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
from	O
equation	B
(	O
10.3.14	O
)	O
,	O
the	O
term	O
p	O
(	O
x∗	O
,	O
d|c∗	O
)	O
is	O
computed	O
using	O
the	O
following	O
decomposition	B
:	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
p	O
(	O
x∗	O
∗	O
)	O
=	O
,	O
d|c	O
p	O
(	O
x∗	O
∗	O
)	O
=	O
,	O
d	O
,	O
θ|c	O
θ	O
p	O
(	O
x∗	O
|θ	O
,	O
  d	O
,	O
c	O
∗	O
)	O
p	O
(	O
θ	O
,	O
d|  c	O
∗	O
)	O
∝	O
θ	O
p	O
(	O
x∗	O
|θ	O
(	O
c	O
∗	O
)	O
)	O
p	O
(	O
θ	O
(	O
c	O
∗	O
)	O
|d	O
)	O
(	O
10.3.5	O
)	O
θ	O
(	O
c∗	O
)	O
(	O
cid:90	O
)	O
hence	O
in	O
order	O
to	O
make	O
a	O
prediction	B
,	O
we	O
require	O
the	O
parameter	B
posterior	O
.	O
consistent	B
with	O
our	O
general	O
bayesian	O
bn	O
training	B
result	O
in	O
section	O
(	O
9.3	O
)	O
,	O
the	O
parameter	B
posterior	O
factorises	O
for	O
dirichlet	O
hyperparameters	O
ui	O
(	O
c∗	O
)	O
the	O
above	O
equation	B
updates	O
the	O
hyperparameter	B
by	O
the	O
number	O
of	O
times	O
variable	B
i	O
is	O
in	O
state	O
s	O
for	O
class	O
c∗	O
data	B
.	O
a	O
common	O
default	O
setting	O
is	O
to	O
take	O
all	O
components	O
of	O
u	O
to	O
be	O
1.	O
p	O
(	O
x	O
∗	O
∗	O
)	O
i|d	O
,	O
c	O
(	O
10.3.10	O
)	O
p	O
(	O
x	O
∗	O
i	O
=	O
s|θ	O
(	O
c	O
∗	O
)	O
)	O
p	O
(	O
θ	O
(	O
c	O
∗	O
)	O
|d	O
)	O
=	O
(	O
cid:90	O
)	O
θ	O
(	O
c∗	O
)	O
∗	O
)	O
p	O
(	O
θ	O
(	O
c	O
θi	O
s	O
(	O
c	O
∗	O
)	O
|d	O
)	O
θ	O
(	O
c∗	O
)	O
(	O
10.3.4	O
)	O
(	O
10.3.6	O
)	O
(	O
10.3.7	O
)	O
(	O
10.3.8	O
)	O
(	O
10.3.9	O
)	O
(	O
10.3.11	O
)	O
(	O
10.3.12	O
)	O
(	O
10.3.13	O
)	O
209	O
∗	O
)	O
|d	O
)	O
=	O
(	O
cid:89	O
)	O
i	O
p	O
(	O
θ	O
(	O
c	O
where	O
p	O
(	O
θi	O
(	O
c	O
p	O
(	O
θi	O
(	O
c	O
∗	O
)	O
|d	O
)	O
∗	O
)	O
)	O
(	O
cid:89	O
)	O
∗	O
)	O
|d	O
)	O
∝	O
p	O
(	O
θi	O
(	O
c	O
∗	O
)	O
|d	O
)	O
=	O
dirichlet	O
(	O
cid:0	O
)	O
θi	O
(	O
c	O
∗	O
)	O
+	O
(	O
cid:88	O
)	O
(	O
cid:2	O
)	O
ˆui	O
(	O
c	O
∗	O
)	O
(	O
cid:3	O
)	O
∗	O
)	O
(	O
cid:1	O
)	O
∗	O
)	O
|ˆui	O
(	O
c	O
where	O
the	O
vector	O
ˆui	O
(	O
c∗	O
)	O
has	O
components	O
n	O
:	O
cn=c∗	O
p	O
(	O
θi	O
(	O
c	O
i	O
[	O
xn	O
i	O
=	O
s	O
]	O
s	O
=	O
ui	O
s	O
(	O
c	O
p	O
(	O
xn	O
i	O
|θi	O
(	O
c	O
n	O
:	O
cn=c∗	O
∗	O
)	O
)	O
by	O
conjugacy	O
,	O
the	O
posterior	B
for	O
class	O
c∗	O
is	O
a	O
dirichlet	O
distribution	B
,	O
classiﬁcation	B
the	O
class	O
distribution	B
is	O
given	O
by	O
∗	O
p	O
(	O
c	O
|x∗	O
to	O
compute	O
p	O
(	O
x∗	O
|d	O
,	O
c	O
∗	O
|d	O
)	O
p	O
(	O
x∗	O
,	O
d	O
)	O
∝	O
p	O
(	O
c	O
(	O
cid:90	O
)	O
|d	O
,	O
c∗	O
)	O
we	O
use	O
∗	O
)	O
=	O
∗	O
i	O
=	O
s	O
,	O
θ	O
(	O
c	O
p	O
(	O
x	O
θ	O
(	O
c∗	O
)	O
p	O
(	O
x	O
∗	O
i	O
=	O
s|d	O
,	O
c	O
(	O
cid:90	O
)	O
using	O
the	O
general	O
identity	O
|d	O
)	O
(	O
cid:89	O
)	O
(	O
cid:90	O
)	O
i	O
∗	O
)	O
=	O
∗	O
)	O
=	O
p	O
(	O
c	O
∗	O
∗	O
)	O
|d	O
,	O
c	O
(	O
cid:90	O
)	O
(	O
cid:89	O
)	O
s	O
(	O
cid:48	O
)	O
θsdirichlet	O
(	O
θ|u	O
)	O
dθ	O
=	O
1	O
z	O
(	O
u	O
)	O
θus	O
(	O
cid:48	O
)	O
−1+i	O
[	O
s	O
(	O
cid:48	O
)	O
=s	O
]	O
dθ	O
=	O
z	O
(	O
u	O
(	O
cid:48	O
)	O
)	O
z	O
(	O
u	O
)	O
(	O
cid:26	O
)	O
us	O
(	O
cid:48	O
)	O
s	O
=	O
u	O
s	O
(	O
cid:54	O
)	O
=	O
s	O
(	O
cid:48	O
)	O
us	O
+	O
1	O
s	O
=	O
s	O
(	O
cid:48	O
)	O
draft	O
march	O
9	O
,	O
2010	O
where	O
z	O
(	O
u	O
)	O
is	O
the	O
normalisation	B
constant	I
of	O
the	O
dirichlet	O
distribution	B
dirichlet	O
(	O
·|u	O
)	O
and	O
x1	O
x2	O
x3	O
x4	O
figure	O
10.4	O
:	O
a	O
chow-liu	O
tree	B
in	O
which	O
each	O
variable	B
xi	O
has	O
at	O
most	O
one	O
parent	O
.	O
the	O
variables	O
may	O
be	O
indexed	O
such	O
that	O
1	O
≤	O
i	O
≤	O
d.	O
tree	B
augmented	I
naive	O
bayes	O
we	O
obtain	O
∗	O
p	O
(	O
c	O
|x∗	O
∗	O
,	O
d	O
)	O
∝	O
p	O
(	O
c	O
|d	O
)	O
(	O
cid:89	O
)	O
i	O
z	O
(	O
u∗i	O
(	O
c∗	O
)	O
)	O
z	O
(	O
ˆui	O
(	O
c∗	O
)	O
)	O
where	O
∗i	O
s	O
(	O
c	O
u	O
∗	O
)	O
=	O
ˆui	O
s	O
(	O
c	O
∗	O
)	O
+	O
i	O
[	O
x	O
∗	O
i	O
=	O
s	O
]	O
(	O
10.3.14	O
)	O
(	O
10.3.15	O
)	O
example	O
49	O
(	O
bayesian	O
naive	O
bayes	O
)	O
.	O
repeating	O
the	O
previous	O
analysis	B
for	O
the	O
‘	O
are	O
they	O
scottish	O
?	O
’	O
data	B
from	O
example	O
(	O
48	O
)	O
,	O
the	O
probability	B
under	O
a	O
uniform	B
dirichlet	O
prior	B
for	O
all	O
the	O
tables	O
,	O
gives	O
a	O
value	B
of	O
0.236	O
for	O
the	O
probability	B
that	O
(	O
1	O
,	O
0	O
,	O
1	O
,	O
1	O
,	O
0	O
)	O
is	O
scottish	O
,	O
compared	O
with	O
a	O
value	B
of	O
0.192	O
under	O
the	O
standard	O
naive	O
bayes	O
assumption	O
.	O
10.4	O
tree	B
augmented	I
naive	O
bayes	O
a	O
natural	B
extension	O
of	O
naive	O
bayes	O
is	O
to	O
relax	O
the	O
assumption	O
that	O
the	O
attributes	O
are	O
independent	O
given	O
the	O
class	O
:	O
p	O
(	O
x|c	O
)	O
(	O
cid:54	O
)	O
=	O
p	O
(	O
xi|c	O
)	O
(	O
10.4.1	O
)	O
the	O
question	O
then	O
arises	O
–	O
which	O
structure	B
should	O
we	O
choose	O
for	O
p	O
(	O
x|c	O
)	O
?	O
as	O
we	O
saw	O
in	O
section	O
(	O
9.3.5	O
)	O
,	O
learning	B
a	O
structure	B
is	O
computationally	O
infeasible	O
for	O
all	O
but	O
very	O
small	O
numbers	O
of	O
attributes	O
.	O
a	O
practical	O
algorithm	B
requires	O
a	O
speciﬁc	O
form	O
of	O
constraint	O
on	O
the	O
structure	B
.	O
to	O
do	O
this	O
we	O
ﬁrst	O
make	O
a	O
digression	O
into	O
the	O
maximum	B
likelihood	I
learning	O
of	O
trees	O
constrained	O
to	O
have	O
at	O
most	O
a	O
single	O
parent	O
.	O
10.4.1	O
chow-liu	O
trees	O
consider	O
a	O
multivariate	B
distribution	O
p	O
(	O
x	O
)	O
that	O
we	O
wish	O
to	O
approximate	B
with	O
a	O
distribution	B
q	O
(	O
x	O
)	O
.	O
further-	O
more	O
,	O
we	O
constrain	O
the	O
approximation	B
q	O
(	O
x	O
)	O
to	O
be	O
a	O
belief	B
network	I
in	O
which	O
each	O
node	B
has	O
at	O
most	O
one	O
parent	O
.	O
first	O
we	O
assume	O
that	O
we	O
have	O
chosen	O
a	O
particular	O
labelling	O
of	O
the	O
variables	O
1	O
≤	O
i	O
≤	O
d	O
,	O
for	O
which	O
the	O
dag	O
single	O
parent	O
constraint	O
means	O
d	O
(	O
cid:89	O
)	O
i=1	O
d	O
(	O
cid:89	O
)	O
i=1	O
d	O
(	O
cid:88	O
)	O
(	O
cid:10	O
)	O
log	O
q	O
(	O
xi|xpa	O
(	O
i	O
)	O
)	O
(	O
cid:11	O
)	O
q	O
(	O
x	O
)	O
=	O
q	O
(	O
xi|xpa	O
(	O
i	O
)	O
)	O
,	O
pa	O
(	O
i	O
)	O
<	O
i	O
,	O
or	O
pa	O
(	O
i	O
)	O
=	O
∅	O
(	O
10.4.2	O
)	O
where	O
pa	O
(	O
i	O
)	O
is	O
the	O
single	O
parent	O
index	O
of	O
node	B
i.	O
to	O
ﬁnd	O
the	O
best	O
approximating	O
distribution	B
q	O
in	O
this	O
constrained	O
class	O
,	O
we	O
may	O
minimise	O
the	O
kullback-leibler	O
divergence	B
kl	O
(	O
p|q	O
)	O
=	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
)	O
−	O
since	O
p	O
(	O
x	O
)	O
is	O
ﬁxed	O
,	O
the	O
ﬁrst	O
term	O
is	O
constant	O
.	O
by	O
adding	O
a	O
term	O
(	O
cid:10	O
)	O
log	O
p	O
(	O
xi|xpa	O
(	O
i	O
)	O
)	O
(	O
cid:11	O
)	O
p	O
(	O
xi	O
,	O
xpa	O
(	O
i	O
)	O
)	O
i=1	O
on	O
p	O
(	O
x	O
)	O
alone	O
,	O
we	O
can	O
write	O
(	O
10.4.3	O
)	O
p	O
(	O
xi	O
,	O
xpa	O
(	O
i	O
)	O
)	O
that	O
depends	O
(	O
cid:68	O
)	O
(	O
cid:10	O
)	O
log	O
q	O
(	O
xi|xpa	O
(	O
i	O
)	O
)	O
(	O
cid:11	O
)	O
d	O
(	O
cid:88	O
)	O
i=1	O
(	O
cid:10	O
)	O
log	O
p	O
(	O
xi|xpa	O
(	O
i	O
)	O
)	O
(	O
cid:11	O
)	O
(	O
cid:69	O
)	O
p	O
(	O
xi|xpa	O
(	O
i	O
)	O
)	O
−	O
p	O
(	O
xi|xpa	O
(	O
i	O
)	O
)	O
(	O
10.4.4	O
)	O
p	O
(	O
xpa	O
(	O
i	O
)	O
)	O
kl	O
(	O
p|q	O
)	O
=	O
const	O
.	O
−	O
210	O
draft	O
march	O
9	O
,	O
2010	O
tree	B
augmented	I
naive	O
bayes	O
algorithm	B
7	O
chow-liu	O
trees	O
end	O
for	O
for	O
j	O
=	O
1	O
to	O
d	O
do	O
compute	O
the	O
mutual	B
information	I
for	O
the	O
pair	O
of	O
variables	O
xi	O
,	O
xj	O
:	O
wij	O
=	O
mi	O
(	O
xi	O
;	O
xj	O
)	O
1	O
:	O
for	O
i	O
=	O
1	O
to	O
d	O
do	O
2	O
:	O
3	O
:	O
4	O
:	O
5	O
:	O
end	O
for	O
6	O
:	O
for	O
the	O
undirected	B
graph	I
g	O
with	O
edge	O
weights	O
w	O
,	O
ﬁnd	O
a	O
maximum	O
weight	O
undirected	B
spanning	O
tree	B
t	O
7	O
:	O
choose	O
an	O
arbitrary	O
variable	B
as	O
the	O
root	O
node	B
of	O
the	O
tree	B
t	O
.	O
8	O
:	O
form	O
a	O
directed	B
tree	O
by	O
orienting	O
all	O
edges	O
away	O
from	O
the	O
root	O
node	B
.	O
this	O
enables	O
us	O
to	O
recognise	O
that	O
,	O
up	O
to	O
a	O
negligible	O
constant	O
,	O
the	O
overall	O
kullback-leibler	O
divergence	B
is	O
a	O
positive	O
sum	O
of	O
individual	O
kullback-leibler	O
divergences	O
so	O
that	O
the	O
optimal	O
setting	O
is	O
therefore	O
(	O
10.4.5	O
)	O
plugging	O
this	O
solution	O
into	O
equation	B
(	O
10.4.3	O
)	O
and	O
using	O
log	O
p	O
(	O
xi|xpa	O
(	O
i	O
)	O
)	O
=	O
log	O
p	O
(	O
xi	O
,	O
xpa	O
(	O
i	O
)	O
)	O
−	O
log	O
p	O
(	O
xpa	O
(	O
i	O
)	O
)	O
we	O
obtain	O
q	O
(	O
xi|xpa	O
(	O
i	O
)	O
)	O
=	O
p	O
(	O
xi|xpa	O
(	O
i	O
)	O
)	O
d	O
(	O
cid:88	O
)	O
kl	O
(	O
p|q	O
)	O
=	O
const	O
.	O
−	O
i=1	O
(	O
cid:10	O
)	O
log	O
p	O
(	O
xi	O
,	O
xpa	O
(	O
i	O
)	O
)	O
(	O
cid:11	O
)	O
p	O
(	O
xi	O
,	O
xpa	O
(	O
i	O
)	O
)	O
+	O
p	O
(	O
xpa	O
(	O
i	O
)	O
)	O
(	O
10.4.6	O
)	O
(	O
cid:10	O
)	O
log	O
p	O
(	O
xpa	O
(	O
i	O
)	O
)	O
(	O
cid:11	O
)	O
d	O
(	O
cid:88	O
)	O
i=1	O
we	O
still	O
need	O
to	O
ﬁnd	O
the	O
optimal	O
parental	O
structure	B
pa	O
(	O
i	O
)	O
that	O
minimises	O
the	O
above	O
expression	O
.	O
if	O
we	O
add	O
and	O
subtract	O
an	O
entropy	B
term	O
we	O
can	O
write	O
(	O
cid:10	O
)	O
log	O
p	O
(	O
xi	O
,	O
xpa	O
(	O
i	O
)	O
)	O
(	O
cid:11	O
)	O
(	O
cid:10	O
)	O
log	O
p	O
(	O
xpa	O
(	O
i	O
)	O
)	O
(	O
cid:11	O
)	O
d	O
(	O
cid:88	O
)	O
i=1	O
d	O
(	O
cid:88	O
)	O
i=1	O
kl	O
(	O
p|q	O
)	O
=	O
−	O
p	O
(	O
xi	O
,	O
xpa	O
(	O
i	O
)	O
)	O
+	O
p	O
(	O
xpa	O
(	O
i	O
)	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
xi	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
xi	O
)	O
d	O
(	O
cid:88	O
)	O
i=1	O
−	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
xi	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
xi	O
)	O
+	O
const	O
.	O
(	O
10.4.7	O
)	O
for	O
two	O
variables	O
xi	O
and	O
xj	O
and	O
distribution	B
p	O
(	O
xi	O
,	O
xj	O
)	O
,	O
the	O
mutual	B
information	I
deﬁnition	O
(	O
87	O
)	O
can	O
be	O
written	O
as	O
mi	O
(	O
xi	O
;	O
xj	O
)	O
=	O
log	O
p	O
(	O
xi	O
,	O
xj	O
)	O
p	O
(	O
xi	O
)	O
p	O
(	O
xj	O
)	O
p	O
(	O
xi	O
,	O
xj	O
)	O
(	O
10.4.8	O
)	O
(	O
cid:29	O
)	O
which	O
can	O
be	O
seen	O
as	O
the	O
kullback-leibler	O
divergence	B
kl	O
(	O
p	O
(	O
xi	O
,	O
xj	O
)	O
|p	O
(	O
xi	O
)	O
p	O
(	O
xj	O
)	O
)	O
and	O
is	O
therefore	O
non-	O
negative	O
.	O
using	O
this	O
,	O
equation	B
(	O
10.4.7	O
)	O
is	O
d	O
(	O
cid:88	O
)	O
i=1	O
(	O
cid:28	O
)	O
d	O
(	O
cid:88	O
)	O
mi	O
(	O
cid:0	O
)	O
xi	O
;	O
xpa	O
(	O
i	O
)	O
(	O
cid:1	O
)	O
d	O
(	O
cid:88	O
)	O
d	O
(	O
cid:88	O
)	O
mi	O
(	O
cid:0	O
)	O
xi	O
;	O
xpa	O
(	O
i	O
)	O
(	O
cid:1	O
)	O
kl	O
(	O
p|q	O
)	O
=	O
−	O
since	O
our	O
task	O
is	O
to	O
ﬁnd	O
the	O
parental	O
indices	O
pa	O
(	O
i	O
)	O
,	O
and	O
the	O
entropic	O
term	O
(	O
cid:80	O
)	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
xi	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
xi	O
)	O
+	O
const	O
.	O
−	O
i=1	O
i=1	O
i	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
xi	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
xi	O
)	O
is	O
independent	O
of	O
this	O
mapping	O
,	O
ﬁnding	O
the	O
optimal	O
mapping	O
is	O
equivalent	B
to	O
maximising	O
the	O
summed	O
mutual	O
informations	O
(	O
10.4.9	O
)	O
(	O
10.4.10	O
)	O
i=1	O
under	O
the	O
constraint	O
that	O
pa	O
(	O
i	O
)	O
≤	O
i.	O
since	O
we	O
also	O
need	O
to	O
choose	O
the	O
optimal	O
initial	O
labelling	O
of	O
the	O
variables	O
as	O
well	O
,	O
the	O
problem	B
is	O
equivalent	B
to	O
computing	O
all	O
the	O
pairwise	B
mutual	O
informations	O
wij	O
=	O
mi	O
(	O
xi	O
;	O
xj	O
)	O
(	O
10.4.11	O
)	O
and	O
then	O
ﬁnding	O
a	O
maximal	O
spanning	B
tree	I
for	O
the	O
graph	B
with	O
edge	O
weights	O
w	O
(	O
see	O
spantree.m	O
)	O
.	O
this	O
can	O
be	O
thought	O
of	O
as	O
a	O
form	O
of	O
breadth-ﬁrst-search	O
[	O
61	O
]	O
.	O
once	O
found	O
,	O
we	O
need	O
to	O
identify	O
a	O
directed	B
tree	O
with	O
at	O
most	O
one	O
parent	O
.	O
this	O
is	O
achieved	O
by	O
choosing	O
an	O
arbitrary	O
node	B
and	O
then	O
orienting	O
edges	O
consistently	O
away	O
from	O
this	O
node	B
.	O
draft	O
march	O
9	O
,	O
2010	O
211	O
tree	B
augmented	I
naive	O
bayes	O
c	O
x1	O
x2	O
x3	O
x4	O
figure	O
10.5	O
:	O
tree	B
augmented	I
naive	O
(	O
tan	O
)	O
bayes	O
.	O
each	O
variable	B
xi	O
has	O
at	O
most	O
one	O
parent	O
.	O
the	O
maximum	B
likelihood	I
optimal	O
tan	O
structure	B
is	O
computed	O
using	O
a	O
modiﬁed	O
chow-liu	O
algorithm	B
in	O
which	O
the	O
conditional	B
mutual	I
information	I
mi	O
(	O
xi	O
;	O
xj|c	O
)	O
is	O
computed	O
for	O
all	O
i	O
,	O
j.	O
a	O
maximum	O
weight	O
spanning	B
tree	I
is	O
then	O
found	O
and	O
turned	O
into	O
a	O
directed	B
graph	O
by	O
orienting	O
the	O
edges	O
outwards	O
from	O
a	O
chosen	O
root	O
node	B
.	O
the	O
table	O
entries	O
can	O
then	O
be	O
read	O
oﬀ	O
using	O
the	O
usual	O
maximum	B
likelihood	I
counting	O
argu-	O
ment	O
.	O
maximum	B
likelihood	I
chow-liu	O
trees	O
n	O
(	O
cid:88	O
)	O
n=1	O
if	O
p	O
(	O
x	O
)	O
is	O
the	O
empirical	B
distribution	I
p	O
(	O
x	O
)	O
=	O
1	O
n	O
then	O
δ	O
(	O
x	O
,	O
xn	O
)	O
kl	O
(	O
p|q	O
)	O
=	O
const	O
.	O
−	O
(	O
cid:88	O
)	O
n	O
1	O
n	O
log	O
q	O
(	O
xn	O
)	O
(	O
10.4.12	O
)	O
(	O
10.4.13	O
)	O
(	O
10.4.14	O
)	O
hence	O
the	O
approximation	B
q	O
that	O
minimises	O
the	O
kullback-leibler	O
divergence	B
between	O
the	O
empirical	B
distri-	O
bution	O
and	O
p	O
is	O
equivalent	B
to	O
that	O
which	O
maximises	O
the	O
likelihood	B
of	O
the	O
data	B
.	O
this	O
means	O
that	O
if	O
we	O
use	O
the	O
mutual	B
information	I
found	O
from	O
the	O
empirical	B
distribution	I
,	O
with	O
p	O
(	O
xi	O
=	O
a	O
,	O
xj	O
=	O
b	O
)	O
∝	O
(	O
cid:93	O
)	O
(	O
xi	O
=	O
a	O
,	O
xj	O
=	O
b	O
)	O
then	O
the	O
chow-liu	O
tree	B
produced	O
corresponds	O
to	O
the	O
maximum	B
likelihood	I
solution	O
amongst	O
all	O
single-	O
parent	O
trees	O
.	O
an	O
outline	O
of	O
the	O
procedure	O
is	O
given	O
in	O
algorithm	B
(	O
7	O
)	O
.	O
an	O
eﬃcient	B
algorithm	O
for	O
sparse	B
data	O
is	O
also	O
available	O
[	O
190	O
]	O
.	O
remark	O
10	O
(	O
learning	B
tree	O
structured	B
belief	O
networks	O
)	O
.	O
the	O
chow-liu	O
algorithm	B
pertains	O
to	O
the	O
discussion	O
in	O
section	O
(	O
9.3.5	O
)	O
on	O
learning	B
the	O
structure	B
of	O
belief	B
networks	I
from	O
data	B
.	O
under	O
the	O
special	O
constraint	O
that	O
each	O
variable	B
has	O
at	O
most	O
one	O
parent	O
,	O
the	O
chow-liu	O
algorithm	B
returns	O
the	O
maximum	B
likelihood	I
structure	O
to	O
ﬁt	O
the	O
data	B
.	O
10.4.2	O
learning	B
tree	O
augmented	B
naive	O
bayes	O
networks	O
for	O
a	O
distribution	B
p	O
(	O
x|c	O
)	O
of	O
the	O
form	O
of	O
a	O
tree	B
structure	O
with	O
a	O
single-parent	O
constraint	O
we	O
can	O
readily	O
ﬁnd	O
the	O
class	O
conditional	B
maximum	O
likelihood	B
solution	O
by	O
computing	O
the	O
chow-liu	O
tree	B
for	O
each	O
class	O
.	O
one	O
then	O
adds	O
links	O
from	O
the	O
class	O
node	B
c	O
to	O
each	O
variable	B
and	O
learns	O
the	O
class	O
conditional	B
probabilities	O
from	O
c	O
to	O
x	O
,	O
which	O
can	O
be	O
read	O
oﬀ	O
for	O
maximum	B
likelihood	I
using	O
the	O
usual	O
counting	B
argument	O
.	O
note	O
that	O
this	O
would	O
generally	O
result	O
in	O
a	O
diﬀerent	O
chow-liu	O
tree	B
for	O
each	O
class	O
.	O
practitioners	O
typically	O
constrain	O
the	O
network	O
to	O
have	O
the	O
same	O
structure	B
for	O
all	O
classes	O
.	O
the	O
maximum	B
likelihood	I
objective	O
under	O
the	O
tan	O
constraint	O
then	O
corresponds	O
to	O
maximising	O
the	O
conditional	B
mutual	I
information	I
[	O
97	O
]	O
mi	O
(	O
xi	O
;	O
xj|c	O
)	O
=	O
(	O
cid:104	O
)	O
kl	O
(	O
p	O
(	O
xi	O
,	O
xj|c	O
)	O
|p	O
(	O
xi|c	O
)	O
p	O
(	O
xj|c	O
)	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
c	O
)	O
(	O
10.4.15	O
)	O
see	O
exercise	O
(	O
136	O
)	O
.	O
once	O
the	O
structure	B
is	O
learned	O
one	O
subsequently	O
sets	O
parameters	O
by	O
maximum	B
likelihood	I
counting	O
.	O
techniques	O
to	O
prevent	O
overﬁtting	B
are	O
discussed	O
in	O
[	O
97	O
]	O
and	O
can	O
be	O
addressed	O
using	O
dirichlet	O
priors	O
,	O
as	O
for	O
the	O
simpler	O
naive	O
bayes	O
structure	B
.	O
one	O
can	O
readily	O
consider	O
less	O
restrictive	O
structures	O
than	O
single-parent	O
belief	B
networks	I
.	O
however	O
,	O
the	O
complexity	O
of	O
ﬁnding	O
optimal	O
bn	O
structures	O
is	O
generally	O
computationally	O
infeasible	O
and	O
heuristics	O
are	O
required	O
to	O
limit	O
the	O
search	O
space	O
.	O
212	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
10.5	O
code	O
naivebayestrain.m	O
:	O
naive	O
bayes	O
trained	O
with	O
maximum	O
likelihood	B
naivebayestest.m	O
:	O
naive	O
bayes	O
test	O
naivebayesdirichlettrain.m	O
:	O
naive	O
bayes	O
trained	O
with	O
bayesian	O
dirichlet	O
naivebayesdirichlettest.m	O
:	O
naive	O
bayes	O
testing	O
with	O
bayesian	O
dirichlet	O
demonaivebayes.m	O
:	O
demo	O
of	O
naive	O
bayes	O
10.6	O
exercises	O
exercise	O
130.	O
a	O
local	B
supermarket	O
specializing	O
in	O
breakfast	O
cereals	O
decides	O
to	O
analyze	O
the	O
buying	O
patterns	O
of	O
its	O
customers	O
.	O
they	O
make	O
a	O
small	O
survey	O
asking	O
6	O
randomly	O
chosen	O
people	O
their	O
age	O
(	O
older	O
or	O
younger	O
than	O
60	O
years	O
)	O
and	O
which	O
of	O
the	O
breakfast	O
cereals	O
(	O
cornﬂakes	O
,	O
frosties	O
,	O
sugar	O
puﬀs	O
,	O
branﬂakes	O
)	O
they	O
like	O
.	O
each	O
respondent	O
provides	O
a	O
vector	O
with	O
entries	O
1	O
or	O
0	O
corresponding	O
to	O
whether	O
they	O
like	O
or	O
dislike	O
the	O
cereal	O
.	O
thus	O
a	O
respondent	O
with	O
(	O
1101	O
)	O
would	O
like	O
cornﬂakes	O
,	O
frosties	O
and	O
branﬂakes	O
,	O
but	O
not	O
sugar	O
puﬀs	O
.	O
the	O
older	O
than	O
60	O
years	O
respondents	O
provide	O
the	O
following	O
data	B
(	O
1000	O
)	O
,	O
(	O
1001	O
)	O
,	O
(	O
1111	O
)	O
,	O
(	O
0001	O
)	O
.	O
the	O
younger	O
than	O
60	O
years	O
old	O
respondents	O
responded	O
(	O
0110	O
)	O
,	O
(	O
1110	O
)	O
.	O
a	O
novel	O
customer	O
comes	O
into	O
the	O
supermarket	O
and	O
says	O
she	O
only	O
likes	O
frosties	O
and	O
sugar	O
puﬀs	O
.	O
using	O
naive	O
bayes	O
trained	O
with	O
maximum	O
likelihood	B
,	O
what	O
is	O
the	O
probability	B
that	O
she	O
is	O
younger	O
than	O
60	O
?	O
exercise	O
131.	O
a	O
psychologist	O
does	O
a	O
small	O
survey	O
on	O
‘	O
happiness	O
’	O
.	O
each	O
respondent	O
provides	O
a	O
vector	O
with	O
entries	O
1	O
or	O
0	O
corresponding	O
to	O
whether	O
they	O
answer	O
‘	O
yes	O
’	O
to	O
a	O
question	O
or	O
‘	O
no	O
’	O
,	O
respectively	O
.	O
the	O
question	O
vector	O
has	O
attributes	O
x	O
=	O
(	O
rich	O
,	O
married	O
,	O
healthy	O
)	O
(	O
10.6.1	O
)	O
thus	O
,	O
a	O
response	O
(	O
1	O
,	O
0	O
,	O
1	O
)	O
would	O
indicate	O
that	O
the	O
respondent	O
was	O
‘	O
rich	O
’	O
,	O
‘	O
unmarried	O
’	O
,	O
‘	O
healthy	O
’	O
.	O
in	O
addition	O
,	O
each	O
respondent	O
gives	O
a	O
value	B
c	O
=	O
1	O
if	O
they	O
are	O
content	O
with	O
their	O
lifestyle	O
,	O
and	O
c	O
=	O
0	O
if	O
they	O
are	O
not	O
.	O
the	O
fol-	O
lowing	O
responses	O
were	O
obtained	O
from	O
people	O
who	O
claimed	O
also	O
to	O
be	O
‘	O
content	O
’	O
:	O
(	O
1	O
,	O
1	O
,	O
1	O
)	O
,	O
(	O
0	O
,	O
0	O
,	O
1	O
)	O
,	O
(	O
1	O
,	O
1	O
,	O
0	O
)	O
,	O
(	O
1	O
,	O
0	O
,	O
1	O
)	O
and	O
for	O
‘	O
not	O
content	O
’	O
:	O
(	O
0	O
,	O
0	O
,	O
0	O
)	O
,	O
(	O
1	O
,	O
0	O
,	O
0	O
)	O
,	O
(	O
0	O
,	O
0	O
,	O
1	O
)	O
,	O
(	O
0	O
,	O
1	O
,	O
0	O
)	O
,	O
(	O
0	O
,	O
0	O
,	O
0	O
)	O
.	O
1.	O
using	O
naive	O
bayes	O
,	O
what	O
is	O
the	O
probability	B
that	O
a	O
person	O
who	O
is	O
‘	O
not	O
rich	O
’	O
,	O
‘	O
married	O
’	O
and	O
‘	O
healthy	O
’	O
is	O
‘	O
content	O
’	O
?	O
2.	O
what	O
is	O
the	O
probability	B
that	O
a	O
person	O
who	O
is	O
‘	O
not	O
rich	O
’	O
and	O
‘	O
married	O
’	O
is	O
‘	O
content	O
’	O
?	O
(	O
that	O
is	O
,	O
we	O
do	O
not	O
know	O
whether	O
or	O
not	O
they	O
are	O
‘	O
healthy	O
’	O
)	O
.	O
3.	O
consider	O
the	O
following	O
vector	O
of	O
attributes	O
:	O
x1	O
=	O
1	O
if	O
customer	O
is	O
younger	O
than	O
20	O
;	O
x1	O
=	O
0	O
otherwise	O
x2	O
=	O
1	O
if	O
customer	O
is	O
between	O
20	O
and	O
30	O
years	O
old	O
;	O
x2	O
=	O
0	O
otherwise	O
x3	O
=	O
1	O
if	O
customer	O
is	O
older	O
than	O
30	O
;	O
x3	O
=	O
0	O
otherwise	O
x4	O
=	O
1	O
if	O
customer	O
walks	O
to	O
work	O
;	O
x4	O
=	O
0	O
otherwise	O
(	O
10.6.2	O
)	O
(	O
10.6.3	O
)	O
(	O
10.6.4	O
)	O
(	O
10.6.5	O
)	O
each	O
vector	O
of	O
attributes	O
has	O
an	O
associated	O
class	O
label	O
‘	O
rich	O
’	O
or	O
‘	O
poor	O
’	O
.	O
point	O
out	O
any	O
potential	B
diﬃculties	O
with	O
using	O
your	O
previously	O
described	O
approach	B
to	O
training	B
using	O
naive	O
bayes	O
.	O
hence	O
describe	O
how	O
to	O
extend	O
your	O
previous	O
naive	O
bayes	O
method	O
to	O
deal	O
with	O
this	O
dataset	O
.	O
exercise	O
132.	O
whizzco	O
decide	O
to	O
make	O
a	O
text	O
classiﬁer	O
.	O
to	O
begin	O
with	O
they	O
attempt	O
to	O
classify	O
documents	O
as	O
either	O
sport	O
or	O
politics	O
.	O
they	O
decide	O
to	O
represent	O
each	O
document	O
as	O
a	O
(	O
row	O
)	O
vector	O
of	O
attributes	O
describing	O
the	O
presence	O
or	O
absence	O
of	O
words	O
.	O
x	O
=	O
(	O
goal	O
,	O
football	O
,	O
golf	O
,	O
defence	O
,	O
oﬀence	O
,	O
wicket	O
,	O
oﬃce	O
,	O
strategy	O
)	O
draft	O
march	O
9	O
,	O
2010	O
(	O
10.6.6	O
)	O
213	O
exercises	O
training	B
data	O
from	O
sport	O
documents	O
and	O
from	O
politics	O
documents	O
is	O
represented	O
below	O
in	O
matlab	O
using	O
a	O
matrix	B
in	O
which	O
each	O
row	O
represents	O
the	O
8	O
attributes	O
.	O
xp=	O
[	O
1	O
0	O
1	O
1	O
1	O
0	O
1	O
1	O
;	O
%	O
politics	O
0	O
0	O
0	O
1	O
0	O
0	O
1	O
1	O
;	O
1	O
0	O
0	O
1	O
1	O
0	O
1	O
0	O
;	O
0	O
1	O
0	O
0	O
1	O
1	O
0	O
1	O
;	O
0	O
0	O
0	O
1	O
1	O
0	O
1	O
1	O
;	O
0	O
0	O
0	O
1	O
1	O
0	O
0	O
1	O
]	O
xs=	O
[	O
1	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
;	O
%	O
sport	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
;	O
1	O
1	O
0	O
1	O
0	O
0	O
0	O
0	O
;	O
1	O
1	O
0	O
1	O
0	O
0	O
0	O
1	O
;	O
1	O
1	O
0	O
1	O
1	O
0	O
0	O
0	O
;	O
0	O
0	O
0	O
1	O
0	O
1	O
0	O
0	O
;	O
1	O
1	O
1	O
1	O
1	O
0	O
1	O
0	O
]	O
using	O
a	O
naive	O
bayes	O
classiﬁer	B
,	O
what	O
is	O
the	O
probability	B
that	O
the	O
document	O
x	O
=	O
(	O
1	O
,	O
0	O
,	O
0	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
0	O
)	O
is	O
about	O
politics	O
?	O
exercise	O
133.	O
a	O
naive	O
bayes	O
classiﬁer	B
for	O
binary	O
attributes	O
xi	O
∈	O
{	O
0	O
,	O
1	O
}	O
is	O
parameterised	O
by	O
θ1	O
i	O
=	O
p	O
(	O
xi	O
=	O
1|class	O
=	O
1	O
)	O
,	O
θ0	O
i	O
=	O
p	O
(	O
xi	O
=	O
1|class	O
=	O
0	O
)	O
,	O
and	O
p1	O
=	O
p	O
(	O
class	O
=	O
1	O
)	O
and	O
p0	O
=	O
p	O
(	O
class	O
=	O
0	O
)	O
.	O
show	O
that	O
the	O
decision	B
boundary	I
to	O
classify	O
a	O
datapoint	O
x	O
can	O
be	O
written	O
as	O
wtx	O
+	O
b	O
>	O
0	O
,	O
and	O
state	O
explicitly	O
w	O
and	O
b	O
as	O
a	O
function	B
of	O
θ1	O
,	O
θ0	O
,	O
p1	O
,	O
p0	O
.	O
exercise	O
134.	O
this	O
question	O
concerns	O
spam	B
ﬁltering	I
.	O
each	O
email	O
is	O
represented	O
by	O
a	O
vector	O
x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
)	O
(	O
10.6.7	O
)	O
where	O
xi	O
∈	O
{	O
0	O
,	O
1	O
}	O
.	O
each	O
entry	O
of	O
the	O
vector	O
indicates	O
if	O
a	O
particular	O
symbol	O
or	O
word	O
appears	O
in	O
the	O
email	O
.	O
the	O
symbols/words	O
are	O
money	B
,	O
cash	O
,	O
!	O
!	O
!	O
,	O
viagra	O
,	O
.	O
.	O
.	O
,	O
etc	O
.	O
(	O
10.6.8	O
)	O
so	O
that	O
x2	O
=	O
1	O
if	O
the	O
word	O
‘	O
cash	O
’	O
appears	O
in	O
the	O
email	O
.	O
the	O
training	B
dataset	O
consists	O
of	O
a	O
set	O
of	O
vectors	O
along	O
with	O
the	O
class	O
label	O
c	O
,	O
where	O
c	O
=	O
1	O
indicates	O
the	O
email	O
is	O
spam	O
,	O
and	O
c	O
=	O
0	O
not	O
spam	O
.	O
hence	O
,	O
the	O
training	B
set	O
consists	O
of	O
a	O
set	O
of	O
pairs	O
(	O
xn	O
,	O
cn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
the	O
naive	O
bayes	O
model	B
is	O
given	O
by	O
p	O
(	O
c	O
,	O
x	O
)	O
=	O
p	O
(	O
c	O
)	O
p	O
(	O
xi|c	O
)	O
(	O
10.6.9	O
)	O
1.	O
draw	O
a	O
belief	B
network	I
for	O
this	O
distribution	B
.	O
2.	O
derive	O
expressions	O
for	O
the	O
parameters	O
of	O
this	O
model	B
in	O
terms	O
of	O
the	O
training	B
data	O
using	O
maximum	B
likelihood	I
.	O
assume	O
that	O
the	O
data	B
is	O
independent	O
and	O
identically	O
distributed	O
d	O
(	O
cid:89	O
)	O
i=1	O
n	O
(	O
cid:89	O
)	O
p	O
(	O
c1	O
,	O
.	O
.	O
.	O
,	O
cn	O
,	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
=	O
p	O
(	O
cn	O
,	O
xn	O
)	O
explicitly	O
,	O
the	O
parameters	O
are	O
n=1	O
p	O
(	O
c	O
=	O
1	O
)	O
,	O
p	O
(	O
xi	O
=	O
1|c	O
=	O
1	O
)	O
,	O
p	O
(	O
xi	O
=	O
1|c	O
=	O
0	O
)	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
(	O
10.6.10	O
)	O
(	O
10.6.11	O
)	O
3.	O
given	O
a	O
trained	O
model	B
p	O
(	O
x	O
,	O
c	O
)	O
,	O
explain	O
how	O
to	O
form	O
a	O
classiﬁer	B
p	O
(	O
c|x	O
)	O
.	O
4.	O
if	O
‘	O
viagra	O
’	O
never	O
appears	O
in	O
the	O
spam	O
training	O
data	B
,	O
discuss	O
what	O
eﬀect	O
this	O
will	O
have	O
on	O
the	O
classiﬁ-	O
cation	O
for	O
a	O
new	O
email	O
that	O
contains	O
the	O
word	O
‘	O
viagra	O
’	O
.	O
5.	O
write	O
down	O
an	O
expression	O
for	O
the	O
decision	B
boundary	I
and	O
show	O
that	O
it	O
can	O
be	O
written	O
in	O
the	O
form	O
p	O
(	O
c	O
=	O
1|x	O
)	O
=	O
p	O
(	O
c	O
=	O
0|x	O
)	O
d	O
(	O
cid:88	O
)	O
udxd	O
−	O
b	O
=	O
0	O
d=1	O
for	O
suitably	O
deﬁned	O
u	O
and	O
b	O
.	O
214	O
(	O
10.6.12	O
)	O
(	O
10.6.13	O
)	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
exercise	O
135.	O
for	O
a	O
distribution	B
p	O
(	O
x	O
,	O
c	O
)	O
and	O
an	O
approximation	B
q	O
(	O
x	O
,	O
c	O
)	O
,	O
show	O
that	O
when	O
p	O
(	O
x	O
,	O
c	O
)	O
corresponds	O
to	O
the	O
empirical	B
distribution	I
,	O
ﬁnding	O
q	O
(	O
x	O
,	O
c	O
)	O
that	O
minimises	O
the	O
kullback-leibler	O
divergence	B
kl	O
(	O
p	O
(	O
x	O
,	O
c	O
)	O
|q	O
(	O
x	O
,	O
c	O
)	O
)	O
corresponds	O
to	O
maximum	B
likelihood	I
training	O
of	O
q	O
(	O
x	O
,	O
c	O
)	O
.	O
exercise	O
136.	O
consider	O
a	O
distribution	B
p	O
(	O
x	O
,	O
c	O
)	O
and	O
a	O
tree	B
augmented	I
approximation	O
q	O
(	O
x	O
,	O
c	O
)	O
=	O
q	O
(	O
c	O
)	O
(	O
cid:89	O
)	O
(	O
10.6.14	O
)	O
(	O
10.6.15	O
)	O
i	O
q	O
(	O
xi|xpa	O
(	O
i	O
)	O
,	O
c	O
)	O
,	O
(	O
cid:28	O
)	O
(	O
cid:88	O
)	O
pa	O
(	O
i	O
)	O
<	O
i	O
or	O
pa	O
(	O
i	O
)	O
=	O
∅	O
(	O
cid:29	O
)	O
show	O
that	O
for	O
the	O
optimal	O
q	O
(	O
x	O
,	O
c	O
)	O
constrained	O
as	O
above	O
,	O
the	O
solution	O
q	O
(	O
x	O
,	O
c	O
)	O
that	O
minimises	O
kl	O
(	O
p	O
(	O
x	O
,	O
c	O
)	O
|q	O
(	O
x	O
,	O
c	O
)	O
)	O
when	O
plugged	O
back	O
into	O
the	O
kullback-leibler	O
expression	O
gives	O
,	O
as	O
a	O
function	B
of	O
the	O
parental	O
structure	B
,	O
kl	O
(	O
p	O
(	O
x	O
,	O
c	O
)	O
|q	O
(	O
x	O
,	O
c	O
)	O
)	O
=	O
−	O
i	O
log	O
p	O
(	O
xi	O
,	O
xpa	O
(	O
i	O
)	O
|c	O
)	O
p	O
(	O
xpa	O
(	O
i	O
)	O
|c	O
)	O
p	O
(	O
xi|c	O
)	O
+	O
const	O
.	O
p	O
(	O
xi	O
,	O
xpa	O
(	O
i	O
)	O
,	O
c	O
)	O
(	O
10.6.16	O
)	O
this	O
shows	O
that	O
under	O
the	O
single-parent	O
constraint	O
and	O
that	O
each	O
tree	B
q	O
(	O
x|c	O
)	O
has	O
the	O
same	O
structure	B
,	O
min-	O
imising	O
the	O
kullback-leibler	O
divergence	B
is	O
equivalent	B
to	O
maximising	O
the	O
sum	O
of	O
conditional	O
mutual	O
infor-	O
mation	O
terms	O
.	O
exercise	O
137.	O
write	O
a	O
matlab	O
routine	O
a	O
=	O
chowliu	O
(	O
x	O
)	O
where	O
x	O
is	O
a	O
d	O
×	O
n	O
data	B
matrix	O
containing	O
a	O
multivariate	B
datapoint	O
on	O
each	O
column	O
that	O
returns	O
a	O
chow-liu	O
maximum	B
likelihood	I
tree	O
for	O
x.	O
the	O
tree	B
structure	O
is	O
to	O
be	O
returned	O
in	O
the	O
sparse	B
matrix	O
a.	O
you	O
may	O
ﬁnd	O
the	O
routine	O
spantree.m	O
useful	O
.	O
the	O
ﬁle	O
chowliudata.mat	O
contains	O
a	O
data	B
matrix	O
for	O
10	O
variables	O
.	O
use	O
your	O
routine	O
to	O
ﬁnd	O
the	O
maximum	B
likelihood	I
chow	O
liu	O
tree	B
,	O
and	O
draw	O
a	O
picture	O
of	O
the	O
resulting	O
dag	O
with	O
edges	O
oriented	O
away	O
from	O
variable	B
1.	O
draft	O
march	O
9	O
,	O
2010	O
215	O
exercises	O
216	O
draft	O
march	O
9	O
,	O
2010	O
chapter	O
11	O
learning	B
with	O
hidden	B
variables	I
11.1	O
hidden	B
variables	I
and	O
missing	B
data	I
in	O
practice	O
data	B
entries	O
are	O
often	O
missing	B
resulting	O
in	O
incomplete	O
information	O
to	O
specify	O
a	O
likelihood	B
.	O
ob-	O
servational	O
variables	O
may	O
be	O
split	O
into	O
visible	B
(	O
those	O
for	O
which	O
we	O
actually	O
know	O
the	O
state	O
)	O
and	O
missing	B
(	O
those	O
whose	O
states	O
would	O
nominally	O
be	O
known	O
but	O
are	O
missing	B
for	O
a	O
particular	O
datapoint	O
)	O
.	O
another	O
scenario	O
in	O
which	O
not	O
all	O
variables	O
in	O
the	O
model	B
are	O
observed	O
are	O
the	O
so-called	O
hidden	B
or	O
latent	B
variable	I
models	O
.	O
in	O
this	O
case	O
there	O
are	O
variables	O
which	O
are	O
essential	O
for	O
the	O
model	B
description	O
but	O
never	O
observed	O
.	O
for	O
example	O
,	O
the	O
underlying	O
physics	O
of	O
a	O
model	B
may	O
contain	O
latent	B
processes	O
which	O
are	O
essential	O
to	O
describe	O
the	O
model	B
,	O
but	O
can	O
not	O
be	O
directly	O
measured	O
.	O
11.1.1	O
why	O
hidden/missing	O
variables	O
can	O
complicate	O
proceedings	O
in	O
learning	B
the	O
parameters	O
of	O
models	O
as	O
previously	O
described	O
in	O
chapter	O
(	O
9	O
)	O
,	O
we	O
assumed	O
we	O
have	O
complete	O
information	O
to	O
deﬁne	O
all	O
variables	O
of	O
the	O
joint	B
model	O
of	O
the	O
data	B
p	O
(	O
v|θ	O
)	O
.	O
consider	O
the	O
asbestos-smoking-	O
cancer	O
network	O
of	O
section	O
(	O
9.2.3	O
)	O
.	O
in	O
the	O
case	O
of	O
complete	O
data	B
,	O
the	O
likelihood	B
is	O
p	O
(	O
vn|θ	O
)	O
=	O
p	O
(	O
an	O
,	O
sn	O
,	O
cn|θ	O
)	O
=	O
p	O
(	O
cn|an	O
,	O
sn	O
,	O
θc	O
)	O
p	O
(	O
an|θa	O
)	O
p	O
(	O
sn|θs	O
)	O
(	O
11.1.1	O
)	O
which	O
is	O
factorised	B
in	O
terms	O
of	O
the	O
table	O
entry	O
parameters	O
.	O
we	O
exploited	O
this	O
property	O
to	O
show	O
that	O
table	O
entries	O
θ	O
can	O
be	O
learned	O
by	O
considering	O
only	O
local	B
information	O
,	O
both	O
in	O
the	O
maximum	B
likelihood	I
and	O
bayesian	O
frameworks	O
.	O
now	O
consider	O
the	O
case	O
that	O
for	O
some	O
of	O
the	O
patients	O
,	O
only	O
partial	O
information	O
is	O
available	O
.	O
for	O
example	O
,	O
for	O
patient	O
n	O
with	O
record	O
vn	O
=	O
{	O
c	O
=	O
1	O
,	O
s	O
=	O
1	O
}	O
it	O
is	O
known	O
that	O
the	O
patient	O
has	O
cancer	O
and	O
is	O
a	O
smoker	O
,	O
but	O
whether	O
or	O
not	O
they	O
had	O
exposure	O
to	O
asbestos	O
is	O
unknown	O
.	O
since	O
we	O
can	O
only	O
use	O
the	O
‘	O
visible	B
’	O
available	O
information	O
is	O
it	O
would	O
seem	O
reasonable	O
to	O
assess	O
parameters	O
using	O
the	O
marginal	B
likelihood	I
p	O
(	O
cn|a	O
,	O
sn	O
,	O
θc	O
)	O
p	O
(	O
a|θa	O
)	O
p	O
(	O
sn|θs	O
)	O
(	O
11.1.2	O
)	O
p	O
(	O
vn|θ	O
)	O
=	O
(	O
cid:88	O
)	O
p	O
(	O
a	O
,	O
sn	O
,	O
cn|θ	O
)	O
=	O
(	O
cid:88	O
)	O
a	O
a	O
we	O
will	O
discuss	O
when	O
this	O
approach	B
is	O
valid	O
in	O
section	O
(	O
11.1.2	O
)	O
.	O
using	O
the	O
marginal	B
likelihood	I
may	O
result	O
in	O
computational	O
diﬃculties	O
since	O
equation	B
(	O
11.1.2	O
)	O
is	O
not	O
factorised	B
over	O
the	O
tables	O
.	O
this	O
means	O
that	O
the	O
likelihood	B
function	O
can	O
not	O
be	O
written	O
as	O
a	O
product	O
of	O
functions	O
,	O
one	O
for	O
each	O
separate	O
parameter	B
.	O
in	O
this	O
case	O
the	O
maximisation	B
of	O
the	O
likelihood	B
is	O
more	O
complex	O
since	O
the	O
parameters	O
of	O
diﬀerent	O
tables	O
are	O
coupled	B
.	O
a	O
similar	O
complication	O
holds	O
for	O
bayesian	O
learning	B
.	O
as	O
we	O
saw	O
in	O
section	O
(	O
13	O
)	O
,	O
under	O
a	O
prior	B
factorised	O
over	O
each	O
cpt	O
θ	O
,	O
the	O
posterior	B
is	O
also	O
factorised	B
.	O
however	O
,	O
in	O
the	O
case	O
of	O
unknown	O
asbestos	O
exposure	O
,	O
a	O
term	O
217	O
hidden	B
variables	I
and	O
missing	B
data	I
θ	O
θ	O
xvis	O
xinv	O
xvis	O
xinv	O
minv	O
(	O
a	O
)	O
minv	O
(	O
b	O
)	O
figure	O
11.1	O
:	O
(	O
a	O
)	O
:	O
missing	B
at	I
random	I
assumption	O
.	O
missing	B
completely	O
at	O
random	O
assumption	O
.	O
(	O
b	O
)	O
:	O
is	O
introduced	O
of	O
the	O
form	O
p	O
(	O
vn|θ	O
)	O
=	O
(	O
cid:88	O
)	O
p	O
(	O
cn|a	O
,	O
sn	O
,	O
θc	O
)	O
p	O
(	O
a|θa	O
)	O
p	O
(	O
sn|θs	O
)	O
=	O
p	O
(	O
sn|θs	O
)	O
(	O
cid:88	O
)	O
a	O
p	O
(	O
cn|a	O
,	O
sn	O
,	O
θc	O
)	O
p	O
(	O
a|θa	O
)	O
(	O
11.1.3	O
)	O
a	O
which	O
can	O
not	O
be	O
written	O
as	O
a	O
product	O
of	O
a	O
functions	O
of	O
fs	O
(	O
θs	O
)	O
fa	O
(	O
θa	O
)	O
fc	O
(	O
θc	O
)	O
.	O
the	O
missing	B
variable	O
therefore	O
introduces	O
dependencies	O
in	O
the	O
posterior	B
parameter	O
distribution	B
,	O
making	O
the	O
posterior	B
more	O
complex	O
.	O
in	O
both	O
the	O
maximum	B
likelihood	I
and	O
bayesian	O
cases	O
,	O
one	O
has	O
a	O
well	O
deﬁned	O
likelihood	B
function	O
of	O
the	O
table	O
parameters/posterior	O
.	O
the	O
diﬃculty	O
is	O
therefore	O
not	O
conceptual	O
,	O
but	O
rather	O
computational	O
–	O
how	O
are	O
we	O
to	O
ﬁnd	O
the	O
optimum	O
of	O
the	O
likelihood/summarise	O
the	O
posterior	B
?	O
note	O
that	O
missing	B
data	I
does	O
not	O
always	O
make	O
the	O
parameter	B
posterior	O
non-factorised	O
.	O
for	O
example	O
,	O
if	O
the	O
cancer	O
state	O
is	O
unobserved	O
above	O
,	O
because	O
cancer	O
is	O
a	O
collider	B
with	O
no	O
descendants	O
,	O
the	O
conditional	B
distribution	O
simply	O
sums	O
to	O
1	O
,	O
and	O
one	O
is	O
left	O
with	O
a	O
factor	B
dependent	O
on	O
a	O
and	O
another	O
on	O
s.	O
11.1.2	O
the	O
missing	B
at	I
random	I
assumption	O
under	O
what	O
circumstances	O
is	O
it	O
valid	O
to	O
use	O
the	O
marginal	B
likelihood	I
to	O
assess	O
parameters	O
?	O
we	O
partition	O
the	O
variables	O
x	O
into	O
those	O
that	O
are	O
‘	O
visible	B
’	O
,	O
xvis	O
and	O
‘	O
invisible	O
’	O
,	O
xinv	O
,	O
so	O
that	O
the	O
set	O
of	O
all	O
variables	O
can	O
be	O
written	O
x	O
=	O
[	O
xvis	O
,	O
xinv	O
]	O
.	O
for	O
the	O
visible	B
variables	O
we	O
have	O
an	O
observed	O
state	O
xvis	O
=	O
v	O
,	O
whereas	O
the	O
state	O
of	O
the	O
invisible	O
variables	O
is	O
unknown	O
.	O
we	O
use	O
an	O
indicator	O
minv	O
=	O
1	O
to	O
denote	O
that	O
the	O
state	O
of	O
the	O
invisible	O
variables	O
is	O
unknown	O
.	O
then	O
for	O
a	O
datapoint	O
which	O
contains	O
both	O
visible	B
and	O
invisible	O
information	O
,	O
p	O
(	O
xvis	O
=	O
v	O
,	O
minv	O
=	O
1|θ	O
)	O
=	O
(	O
cid:88	O
)	O
=	O
(	O
cid:88	O
)	O
xinv	O
xinv	O
p	O
(	O
xvis	O
=	O
v	O
,	O
xinv	O
,	O
minv	O
=	O
1|θ	O
)	O
p	O
(	O
minv	O
=	O
1|xvis	O
=	O
v	O
,	O
xinv	O
,	O
θ	O
)	O
p	O
(	O
xvis	O
=	O
v	O
,	O
xinv|θ	O
)	O
(	O
11.1.4	O
)	O
(	O
11.1.5	O
)	O
(	O
11.1.6	O
)	O
(	O
11.1.7	O
)	O
(	O
11.1.8	O
)	O
if	O
we	O
assume	O
that	O
the	O
mechanism	O
which	O
generates	O
invisible	O
data	B
has	O
the	O
form	O
then	O
p	O
(	O
minv	O
=	O
1|xvis	O
=	O
v	O
,	O
xinv	O
,	O
θ	O
)	O
=	O
p	O
(	O
minv	O
=	O
1|xvis	O
=	O
v	O
)	O
p	O
(	O
xvis	O
=	O
v	O
,	O
minv	O
=	O
1|θ	O
)	O
=	O
p	O
(	O
minv	O
=	O
1|xvis	O
=	O
v	O
)	O
(	O
cid:88	O
)	O
p	O
(	O
xvis	O
=	O
v	O
,	O
xinv|θ	O
)	O
xinv	O
=	O
p	O
(	O
minv	O
=	O
1|xvis	O
=	O
v	O
)	O
p	O
(	O
xvis	O
=	O
v|θ	O
)	O
only	O
the	O
term	O
p	O
(	O
xvis	O
=	O
v|θ	O
)	O
conveys	O
information	O
about	O
the	O
model	B
.	O
therefore	O
,	O
provided	O
the	O
mechanism	O
by	O
which	O
the	O
data	B
is	O
missing	B
depends	O
only	O
on	O
the	O
visible	B
states	O
,	O
we	O
may	O
simply	O
use	O
the	O
marginal	B
likelihood	I
to	O
assess	O
parameters	O
.	O
this	O
is	O
called	O
the	O
missing	B
at	I
random	I
assumption	O
.	O
example	O
50	O
(	O
not	O
missing	B
at	I
random	I
)	O
.	O
ezsurvey.org	O
stop	O
men	O
on	O
the	O
street	O
and	O
ask	O
them	O
their	O
favourite	O
colour	O
.	O
all	O
men	O
whose	O
favourite	O
colour	O
is	O
pink	O
decline	O
to	O
respond	O
to	O
the	O
question	O
–	O
for	O
any	O
other	O
colour	O
,	O
all	O
men	O
respond	O
to	O
the	O
question	O
.	O
based	O
on	O
the	O
data	B
,	O
ezsurvey.org	O
produce	O
a	O
histogram	O
of	O
men	O
’	O
s	O
favourite	O
colour	O
,	O
based	O
on	O
the	O
likelihood	B
of	O
the	O
visible	B
data	O
alone	O
,	O
conﬁdently	O
stating	O
that	O
none	O
of	O
them	O
likes	O
pink	O
.	O
218	O
draft	O
march	O
9	O
,	O
2010	O
hidden	B
variables	I
and	O
missing	B
data	I
for	O
simplicity	O
,	O
assume	O
there	O
are	O
only	O
three	O
colours	O
,	O
blue	O
,	O
green	O
and	O
pink	O
.	O
ezsurvey.org	O
attempts	O
to	O
ﬁnd	O
the	O
histogram	O
with	O
probabilities	O
θb	O
,	O
θg	O
,	O
θp	O
with	O
θb	O
+	O
θg	O
+	O
θp	O
=	O
1.	O
each	O
respondent	O
produces	O
a	O
visible	B
response	O
xc	O
with	O
dom	O
(	O
xc	O
)	O
=	O
{	O
blue	O
,	O
green	O
,	O
pink	O
}	O
,	O
otherwise	O
mc	O
=	O
1	O
if	O
there	O
is	O
no	O
response	O
.	O
three	O
men	O
are	O
asked	O
their	O
favourite	O
colour	O
,	O
giving	O
data	B
(	O
cid:8	O
)	O
x1	O
c	O
,	O
x2	O
c	O
,	O
x3	O
c	O
(	O
cid:9	O
)	O
=	O
{	O
blue	O
,	O
missing	B
,	O
green	O
}	O
(	O
11.1.9	O
)	O
(	O
11.1.10	O
)	O
based	O
on	O
the	O
likelihood	B
of	O
the	O
visible	B
data	O
alone	O
we	O
have	O
the	O
log	O
likelihood	B
for	O
i.i.d	O
.	O
data	B
l	O
(	O
θb	O
,	O
θg	O
,	O
θp	O
)	O
=	O
log	O
θb	O
+	O
log	O
θg	O
+	O
λ	O
(	O
1	O
−	O
θb	O
−	O
θg	O
−	O
θp	O
)	O
where	O
the	O
last	O
lagrange	O
term	O
ensures	O
normalisation	B
.	O
maximising	O
the	O
expression	O
we	O
arrive	O
at	O
(	O
see	O
exercise	O
(	O
145	O
)	O
)	O
θb	O
=	O
1	O
2	O
,	O
θg	O
=	O
1	O
2	O
,	O
θp	O
=	O
0	O
(	O
11.1.11	O
)	O
the	O
unreasonable	O
result	O
that	O
ezsurvey.org	O
produce	O
is	O
due	O
to	O
not	O
accounting	O
correctly	O
for	O
the	O
mechanism	O
which	O
produces	O
the	O
data	B
.	O
the	O
correct	O
mechanism	O
that	O
generates	O
the	O
data	B
(	O
including	O
the	O
missing	B
data	I
is	O
)	O
p	O
(	O
c1	O
=	O
blue|θ	O
)	O
p	O
(	O
m2	O
c	O
=	O
1|θ	O
)	O
p	O
(	O
c3	O
=	O
green|θ	O
)	O
=	O
θbθpθg	O
=	O
θb	O
(	O
1	O
−	O
θb	O
−	O
θg	O
)	O
θg	O
(	O
11.1.12	O
)	O
where	O
we	O
used	O
p	O
(	O
m2	O
probability	B
that	O
the	O
favourite	O
colour	O
is	O
pink	O
.	O
maximising	O
the	O
likelihood	B
,	O
we	O
arrive	O
at	O
c	O
=	O
1|θ	O
)	O
=	O
θp	O
since	O
the	O
probability	B
that	O
a	O
datapoint	O
is	O
missing	B
is	O
the	O
same	O
as	O
the	O
θb	O
=	O
1	O
3	O
,	O
θg	O
=	O
1	O
3	O
,	O
θp	O
=	O
1	O
3	O
(	O
11.1.13	O
)	O
as	O
we	O
would	O
expect	O
.	O
on	O
the	O
other	O
hand	O
if	O
there	O
is	O
another	O
visible	B
variable	O
,	O
t	O
,	O
denoting	O
the	O
time	O
of	O
day	O
,	O
and	O
the	O
probability	B
that	O
men	O
respond	O
to	O
the	O
question	O
depends	O
only	O
on	O
the	O
time	O
t	O
alone	O
(	O
for	O
example	O
the	O
missing	B
probability	O
is	O
high	O
during	O
rush	O
hour	O
)	O
,	O
then	O
we	O
may	O
indeed	O
treat	O
the	O
missing	B
data	I
as	O
missing	B
at	I
random	I
.	O
a	O
stronger	O
assumption	O
than	O
mar	O
is	O
p	O
(	O
minv	O
=	O
1|xvis	O
=	O
v	O
,	O
xinv	O
,	O
θ	O
)	O
=	O
p	O
(	O
minv	O
=	O
1	O
)	O
(	O
11.1.14	O
)	O
which	O
is	O
called	O
missing	B
completely	O
at	O
random	O
.	O
this	O
applies	O
for	O
example	O
to	O
latent	B
variable	I
models	O
in	O
which	O
the	O
variable	B
state	O
is	O
always	O
missing	B
,	O
independent	O
of	O
anything	O
else	O
.	O
11.1.3	O
maximum	B
likelihood	I
throughout	O
the	O
remaining	O
discussion	O
we	O
will	O
assume	O
any	O
missing	B
data	I
is	O
mar	O
or	O
missing	B
completely	O
at	O
random	O
.	O
this	O
means	O
that	O
we	O
can	O
treat	O
any	O
unobserved	O
variables	O
by	O
summing	O
(	O
or	O
integrating	O
)	O
over	O
their	O
states	O
.	O
for	O
maximum	B
likelihood	I
we	O
learn	O
model	B
parameters	O
θ	O
by	O
optimising	O
the	O
marginal	B
likelihood	I
p	O
(	O
v	O
,	O
h|θ	O
)	O
(	O
11.1.15	O
)	O
p	O
(	O
v|θ	O
)	O
=	O
(	O
cid:88	O
)	O
h	O
with	O
respect	O
to	O
θ	O
.	O
11.1.4	O
identiﬁability	B
issues	O
the	O
marginal	B
likelihood	I
objective	O
function	B
depends	O
on	O
the	O
parameters	O
only	O
through	O
p	O
(	O
v|θ	O
)	O
,	O
so	O
that	O
equiv-	O
alent	O
parameter	B
solutions	O
may	O
exist	O
.	O
for	O
example	O
,	O
consider	O
a	O
latent	B
variable	I
problem	O
with	O
distribution	O
p	O
(	O
x1	O
,	O
x2|θ	O
)	O
=	O
θx1	O
,	O
x2	O
draft	O
march	O
9	O
,	O
2010	O
(	O
11.1.16	O
)	O
219	O
expectation	B
maximisation	I
p	O
(	O
x1|θ	O
)	O
=	O
(	O
cid:80	O
)	O
(	O
cid:88	O
)	O
θ	O
x1	O
,	O
x2	O
=	O
(	O
cid:88	O
)	O
(	O
cid:48	O
)	O
in	O
which	O
variable	B
x2	O
is	O
never	O
observed	O
.	O
this	O
means	O
that	O
the	O
marginal	B
likelihood	I
only	O
depends	O
on	O
the	O
entry	O
x2	O
θx1	O
,	O
x2	O
.	O
given	O
a	O
maximum	B
likelihood	I
solution	O
θ∗	O
,	O
we	O
can	O
then	O
always	O
ﬁnd	O
an	O
equivalent	B
maximum	O
likelihood	B
solution	O
θ	O
(	O
cid:48	O
)	O
provided	O
(	O
see	O
exercise	O
(	O
146	O
)	O
)	O
∗	O
x1	O
,	O
x2	O
θ	O
(	O
11.1.17	O
)	O
x2	O
x2	O
in	O
other	O
cases	O
there	O
is	O
an	O
inherent	O
symmetry	O
in	O
the	O
parameter	B
space	O
of	O
the	O
marginal	B
likelihood	I
.	O
for	O
example	O
,	O
consider	O
the	O
network	O
over	O
binary	O
variables	O
p	O
(	O
c	O
,	O
a	O
,	O
s	O
)	O
=	O
p	O
(	O
c|a	O
,	O
s	O
)	O
p	O
(	O
a	O
)	O
p	O
(	O
s	O
)	O
our	O
aim	O
is	O
to	O
learn	O
the	O
table	O
ˆp	O
(	O
a	O
=	O
1	O
)	O
and	O
the	O
four	O
tables	O
(	O
11.1.18	O
)	O
(	O
11.1.19	O
)	O
ˆp	O
(	O
c	O
=	O
1|a	O
=	O
1	O
,	O
s	O
=	O
1	O
)	O
,	O
ˆp	O
(	O
c	O
=	O
1|a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
,	O
ˆp	O
(	O
c	O
=	O
1|a	O
=	O
0	O
,	O
s	O
=	O
1	O
)	O
,	O
ˆp	O
(	O
c	O
=	O
1|a	O
=	O
0	O
,	O
s	O
=	O
0	O
)	O
(	O
11.1.20	O
)	O
where	O
we	O
used	O
a	O
‘	O
ˆ	O
’	O
to	O
denote	O
that	O
these	O
are	O
parameter	B
estimates	O
.	O
we	O
assume	O
that	O
we	O
have	O
missing	B
data	I
such	O
that	O
the	O
states	O
of	O
variable	B
a	O
are	O
never	O
observed	O
.	O
in	O
this	O
case	O
an	O
equivalent	B
solution	O
(	O
in	O
the	O
sense	O
that	O
it	O
has	O
the	O
same	O
marginal	B
likelihood	I
)	O
is	O
given	O
by	O
interchanging	O
the	O
states	O
of	O
a	O
:	O
(	O
cid:48	O
)	O
(	O
a	O
=	O
0	O
)	O
=	O
ˆp	O
(	O
a	O
=	O
1	O
)	O
ˆp	O
and	O
the	O
four	O
tables	O
(	O
11.1.21	O
)	O
ˆp	O
ˆp	O
(	O
cid:48	O
)	O
(	O
c	O
=	O
1|a	O
=	O
0	O
,	O
s	O
=	O
1	O
)	O
=	O
ˆp	O
(	O
c	O
=	O
1|a	O
=	O
1	O
,	O
s	O
=	O
1	O
)	O
,	O
(	O
cid:48	O
)	O
(	O
c	O
=	O
1|a	O
=	O
1	O
,	O
s	O
=	O
1	O
)	O
=	O
ˆp	O
(	O
c	O
=	O
1|a	O
=	O
0	O
,	O
s	O
=	O
1	O
)	O
,	O
ˆp	O
ˆp	O
(	O
cid:48	O
)	O
(	O
c	O
=	O
1|a	O
=	O
0	O
,	O
s	O
=	O
0	O
)	O
=	O
ˆp	O
(	O
c	O
=	O
1|a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
(	O
cid:48	O
)	O
(	O
c	O
=	O
1|a	O
=	O
1	O
,	O
s	O
=	O
0	O
)	O
=	O
ˆp	O
(	O
c	O
=	O
1|a	O
=	O
0	O
,	O
s	O
=	O
0	O
)	O
a	O
similar	O
situation	O
occurs	O
in	O
a	O
more	O
general	O
setting	O
in	O
which	O
the	O
state	O
of	O
a	O
variable	B
is	O
consistently	O
unob-	O
served	O
(	O
mixture	B
models	O
are	O
a	O
case	O
in	O
point	O
)	O
yielding	O
an	O
inherent	O
symmetry	O
in	O
the	O
solution	O
space	O
.	O
a	O
well	O
known	O
characteristic	O
of	O
maximum	B
likelihood	I
algorithms	O
is	O
that	O
‘	O
jostling	O
’	O
occurs	O
in	O
the	O
initial	O
stages	O
of	O
training	B
in	O
which	O
these	O
symmetric	O
solutions	O
compete	O
.	O
11.2	O
expectation	B
maximisation	I
the	O
em	O
algorithm	B
is	O
a	O
convenient	O
and	O
general	O
purpose	O
iterative	O
approach	O
to	O
maximising	O
the	O
likelihood	B
under	O
missing	B
data/hidden	O
variables	O
[	O
187	O
]	O
.	O
it	O
is	O
generally	O
straightforward	O
to	O
implement	O
and	O
can	O
achieve	O
large	O
jumps	O
in	O
parameter	B
space	O
,	O
particularly	O
in	O
the	O
initial	O
iterations	O
.	O
11.2.1	O
variational	O
em	O
the	O
key	O
feature	O
of	O
the	O
em	O
algorithm	B
is	O
to	O
form	O
an	O
alternative	O
objective	O
function	B
for	O
which	O
the	O
parameter	B
coupling	O
eﬀect	O
discussed	O
in	O
section	O
(	O
11.1.1	O
)	O
is	O
removed	O
,	O
meaning	O
that	O
individual	O
parameter	B
updates	O
can	O
be	O
achieved	O
,	O
akin	O
to	O
the	O
case	O
of	O
fully	O
observed	O
data	O
.	O
the	O
way	O
this	O
works	O
is	O
to	O
replace	O
the	O
marginal	B
likelihood	I
with	O
a	O
lower	O
bound	O
–	O
it	O
is	O
this	O
lower	O
bound	O
that	O
has	O
the	O
decoupled	O
form	O
.	O
we	O
ﬁrst	O
consider	O
a	O
single	O
variable	B
pair	O
(	O
v	O
,	O
h	O
)	O
,	O
where	O
v	O
stands	O
for	O
‘	O
visible	B
’	O
and	O
h	O
for	O
‘	O
hidden	B
’	O
.	O
to	O
derive	O
the	O
bound	B
on	O
the	O
marginal	B
likelihood	I
,	O
consider	O
the	O
kullback-leibler	O
divergence	B
between	O
a	O
‘	O
variational	O
’	O
distribution	B
q	O
(	O
h|v	O
)	O
and	O
the	O
parametric	O
model	B
p	O
(	O
h|v	O
,	O
θ	O
)	O
:	O
kl	O
(	O
q	O
(	O
h|v	O
)	O
|p	O
(	O
h|v	O
,	O
θ	O
)	O
)	O
≡	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
h|v	O
)	O
−	O
log	O
p	O
(	O
h|v	O
,	O
θ	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
h|v	O
)	O
≥	O
0	O
220	O
(	O
11.2.1	O
)	O
draft	O
march	O
9	O
,	O
2010	O
expectation	B
maximisation	I
the	O
term	O
‘	O
variational	O
’	O
refers	O
to	O
the	O
fact	O
that	O
this	O
distribution	B
will	O
be	O
a	O
parameter	B
of	O
an	O
optimisation	B
problem	O
.	O
using	O
bayes	O
’	O
rule	O
,	O
p	O
(	O
h|v	O
,	O
θ	O
)	O
=	O
p	O
(	O
h	O
,	O
v|θ	O
)	O
/p	O
(	O
v|θ	O
)	O
and	O
the	O
fact	O
that	O
p	O
(	O
v|θ	O
)	O
does	O
not	O
depend	O
on	O
h	O
,	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
h|v	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
h|v	O
)	O
−	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
h	O
,	O
v|θ	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
h|v	O
)	O
+	O
log	O
p	O
(	O
v|θ	O
)	O
=	O
kl	O
(	O
q	O
(	O
h|v	O
)	O
|p	O
(	O
h|v	O
,	O
θ	O
)	O
)	O
≥	O
0	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
rearranging	O
,	O
we	O
obtain	O
a	O
bound	B
on	O
the	O
marginal	B
likelihood1	O
(	O
cid:125	O
)	O
log	O
p	O
(	O
v|θ	O
)	O
≥	O
−	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
h|v	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
h|v	O
)	O
(	O
cid:125	O
)	O
(	O
cid:124	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
h	O
,	O
v|θ	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
h|v	O
)	O
v	O
=	O
(	O
cid:8	O
)	O
v1	O
,	O
.	O
.	O
.	O
,	O
vn	O
(	O
cid:9	O
)	O
is	O
the	O
sum	O
of	O
the	O
individual	O
log	O
likelihoods	O
:	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
entropy	B
energy	O
the	O
bound	B
is	O
potentially	O
useful	O
since	O
it	O
is	O
similar	O
in	O
form	O
to	O
the	O
fully	O
observed	O
case	O
,	O
except	O
that	O
terms	O
with	O
missing	O
data	B
have	O
their	O
log	O
likelihood	B
weighted	O
by	O
a	O
prefactor	O
.	O
equation	B
(	O
11.2.3	O
)	O
is	O
a	O
marginal	B
likelihood	I
bound	O
for	O
a	O
single	O
training	B
example	O
.	O
under	O
the	O
i.i.d	O
.	O
assumption	O
,	O
the	O
log	O
likelihood	B
of	O
all	O
training	B
data	O
log	O
p	O
(	O
v|θ	O
)	O
=	O
n	O
(	O
cid:88	O
)	O
n=1	O
log	O
p	O
(	O
vn|θ	O
)	O
n	O
(	O
cid:88	O
)	O
n=1	O
n	O
(	O
cid:88	O
)	O
n=1	O
(	O
11.2.2	O
)	O
(	O
11.2.3	O
)	O
(	O
11.2.4	O
)	O
(	O
11.2.5	O
)	O
summing	O
over	O
the	O
training	B
data	O
,	O
we	O
obtain	O
a	O
bound	B
on	O
the	O
log	O
(	O
marginal	B
)	O
likelihood	B
log	O
p	O
(	O
v|θ	O
)	O
≥	O
−	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
hn|vn	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
hn|vn	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
hn	O
,	O
vn|θ	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
hn|vn	O
)	O
note	O
that	O
the	O
bound	B
is	O
exact	O
(	O
that	O
is	O
,	O
the	O
right	O
hand	O
side	O
is	O
equal	O
to	O
the	O
log	O
likelihood	B
)	O
when	O
we	O
set	O
q	O
(	O
hn|vn	O
)	O
=	O
p	O
(	O
hn|vn	O
,	O
θ	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
the	O
bound	B
suggests	O
an	O
iterative	O
procedure	O
to	O
optimise	O
θ	O
:	O
e-step	O
for	O
ﬁxed	O
θ	O
,	O
ﬁnd	O
the	O
distributions	O
q	O
(	O
hn|vn	O
)	O
that	O
maximise	O
equation	B
(	O
11.2.5	O
)	O
.	O
m-step	O
for	O
ﬁxed	O
{	O
q	O
(	O
hn|vn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
,	O
ﬁnd	O
the	O
parameters	O
θ	O
that	O
maximise	O
equation	B
(	O
11.2.5	O
)	O
.	O
11.2.2	O
classical	O
em	O
in	O
the	O
variational	O
e-step	O
above	O
,	O
the	O
fully	O
optimal	O
setting	O
is	O
q	O
(	O
hn|vn	O
)	O
=	O
p	O
(	O
hn|vn	O
,	O
θ	O
)	O
(	O
11.2.6	O
)	O
since	O
q	O
does	O
not	O
depend	O
on	O
θnew	O
the	O
m-step	O
is	O
equivalent	B
to	O
maximising	O
the	O
energy	B
term	O
alone	O
,	O
see	O
algorithm	O
(	O
8	O
)	O
.	O
example	O
51	O
(	O
a	O
one-parameter	O
one-state	O
example	O
)	O
.	O
we	O
consider	O
a	O
model	B
small	O
enough	O
that	O
we	O
can	O
plot	O
fully	O
the	O
evolution	O
of	O
the	O
em	O
algorithm	B
.	O
the	O
model	B
is	O
on	O
a	O
single	O
visible	B
variable	O
v	O
and	O
single	O
two-state	O
hidden	B
variable	I
h	O
∈	O
{	O
1	O
,	O
2	O
}	O
.	O
we	O
deﬁne	O
a	O
model	B
p	O
(	O
v	O
,	O
h	O
)	O
=	O
p	O
(	O
v|h	O
)	O
p	O
(	O
h	O
)	O
with	O
p	O
(	O
v|h	O
,	O
θ	O
)	O
=	O
1	O
√2πσ2	O
−	O
1	O
2σ2	O
(	O
v−θh	O
)	O
2	O
e	O
(	O
11.2.7	O
)	O
and	O
p	O
(	O
h	O
=	O
1	O
)	O
=	O
p	O
(	O
h	O
=	O
2	O
)	O
=	O
0.5.	O
for	O
an	O
observation	O
v	O
=	O
2.75	O
and	O
σ2	O
=	O
0.5	O
our	O
interest	O
is	O
to	O
ﬁnd	O
the	O
parameter	B
θ	O
that	O
optimises	O
the	O
likelihood	B
−	O
(	O
2.75−θh	O
)	O
2	O
e	O
(	O
11.2.8	O
)	O
p	O
(	O
v	O
=	O
2.75|θ	O
)	O
=	O
1	O
2√π	O
(	O
cid:88	O
)	O
h=1,2	O
1this	O
is	O
analogous	O
to	O
a	O
standard	O
partition	O
function	B
bound	O
in	O
statistical	O
physics	O
,	O
from	O
where	O
the	O
terminology	O
‘	O
energy	B
’	O
and	O
‘	O
entropy	B
’	O
hails	O
.	O
draft	O
march	O
9	O
,	O
2010	O
221	O
expectation	B
maximisation	I
algorithm	O
8	O
expectation	B
maximisation	I
.	O
compute	O
maximum	B
likelihood	I
value	O
for	O
data	B
with	O
hidden	B
variables	I
.	O
input	O
:	O
a	O
distribution	B
p	O
(	O
x|θ	O
)	O
and	O
dataset	O
v.	O
returns	O
ml	O
setting	O
of	O
θ	O
.	O
1	O
:	O
t	O
=	O
0	O
2	O
:	O
choose	O
an	O
initial	O
setting	O
for	O
the	O
parameters	O
θ0	O
.	O
3	O
:	O
while	O
θ	O
not	O
converged	O
(	O
or	O
likelihood	B
not	O
converged	O
)	O
do	O
4	O
:	O
5	O
:	O
6	O
:	O
7	O
:	O
8	O
:	O
9	O
:	O
end	O
while	O
10	O
:	O
return	O
θt	O
(	O
cid:80	O
)	O
n	O
n=1	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
hn	O
,	O
vn|θ	O
)	O
(	O
cid:105	O
)	O
qn	O
t	O
(	O
hn|vn	O
)	O
=	O
p	O
(	O
hn|vn	O
,	O
θt−1	O
)	O
(	O
cid:46	O
)	O
run	O
over	O
all	O
datapoints	O
(	O
cid:46	O
)	O
e	O
step	O
t	O
←	O
t	O
+	O
1	O
for	O
n	O
=	O
1	O
to	O
n	O
do	O
qn	O
end	O
for	O
θt	O
=	O
arg	O
maxθ	O
(	O
cid:46	O
)	O
the	O
max	O
likelihood	B
parameter	O
estimate	O
.	O
t	O
(	O
hn|vn	O
)	O
(	O
cid:46	O
)	O
m	O
step	O
(	O
cid:46	O
)	O
iteration	B
counter	O
(	O
cid:46	O
)	O
initialisation	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
11.2	O
:	O
(	O
a	O
)	O
:	O
the	O
log	O
likelihood	B
for	O
the	O
model	B
described	O
in	O
example	O
(	O
51	O
)	O
.	O
(	O
b	O
)	O
:	O
contours	O
of	O
the	O
lower	O
bound	O
lb	O
(	O
q	O
(	O
h	O
=	O
2	O
)	O
,	O
θ	O
)	O
.	O
for	O
an	O
initial	O
choice	O
q	O
(	O
h	O
=	O
2	O
)	O
=	O
0.5	O
and	O
θ	O
=	O
1.9	O
,	O
successive	O
updates	O
of	O
the	O
e	O
(	O
c	O
)	O
:	O
starting	O
at	O
θ	O
=	O
1.95	O
,	O
the	O
em	O
algorithm	B
converges	O
(	O
vertical	O
)	O
and	O
m	O
(	O
horizontal	O
)	O
steps	O
are	O
plotted	O
.	O
to	O
a	O
local	B
optimum	O
.	O
the	O
log	O
likelihood	B
is	O
plotted	O
in	O
ﬁg	O
(	O
11.2a	O
)	O
with	O
optimum	O
at	O
θ	O
=	O
1.325.	O
the	O
em	O
procedure	O
iteratively	O
optimises	O
the	O
lower	O
bound	O
log	O
p	O
(	O
v	O
=	O
2.75|θ	O
)	O
≥	O
lb	O
(	O
q	O
(	O
h	O
=	O
2	O
)	O
,	O
θ	O
)	O
≡	O
−q	O
(	O
h	O
=	O
1	O
)	O
log	O
q	O
(	O
h	O
=	O
1	O
)	O
−	O
q	O
(	O
h	O
=	O
2	O
)	O
log	O
q	O
(	O
h	O
=	O
2	O
)	O
−	O
q	O
(	O
h	O
)	O
(	O
2.75	O
−	O
θh	O
)	O
2	O
+	O
log	O
2	O
(	O
11.2.9	O
)	O
(	O
cid:88	O
)	O
h=1,2	O
where	O
q	O
(	O
h	O
=	O
1	O
)	O
=	O
1	O
−	O
q	O
(	O
h	O
=	O
2	O
)	O
.	O
from	O
an	O
initial	O
starting	O
θ	O
,	O
the	O
em	O
algorithm	B
ﬁnds	O
the	O
q	O
distribution	B
that	O
optimises	O
l	O
(	O
q	O
,	O
θ	O
)	O
(	O
e-step	O
)	O
and	O
then	O
updates	O
θ	O
(	O
m-step	O
)	O
.	O
depending	O
on	O
the	O
initial	O
θ	O
,	O
the	O
solution	O
found	O
is	O
either	O
a	O
global	B
or	O
local	B
optimum	O
of	O
the	O
likelihood	B
,	O
see	O
ﬁg	O
(	O
11.2	O
)	O
.	O
the	O
m-step	O
is	O
easy	O
to	O
work	O
out	O
analytically	O
in	O
this	O
case	O
with	O
θnew	O
=	O
v	O
(	O
cid:104	O
)	O
h	O
(	O
cid:105	O
)	O
q	O
(	O
h	O
)	O
/	O
(	O
cid:10	O
)	O
h2	O
(	O
cid:11	O
)	O
e-step	O
sets	O
qnew	O
(	O
h	O
)	O
=	O
p	O
(	O
h|v	O
,	O
θ	O
)	O
so	O
that	O
qnew	O
(	O
h	O
=	O
2	O
)	O
=	O
p	O
(	O
v	O
=	O
2.75|h	O
=	O
2	O
,	O
θ	O
)	O
p	O
(	O
h	O
=	O
2	O
)	O
p	O
(	O
v	O
=	O
2.75	O
)	O
=	O
where	O
we	O
used	O
e−	O
(	O
2.75−2θ	O
)	O
2	O
e−	O
(	O
2.75−2θ	O
)	O
2	O
+	O
e−	O
(	O
2.75−θ	O
)	O
2	O
p	O
(	O
v	O
=	O
2.75	O
)	O
=	O
p	O
(	O
v	O
=	O
2.75|h	O
=	O
1	O
,	O
θ	O
)	O
p	O
(	O
h	O
=	O
1	O
)	O
+	O
p	O
(	O
v	O
=	O
2.75|h	O
=	O
2	O
,	O
θ	O
)	O
p	O
(	O
h	O
=	O
2	O
)	O
q	O
(	O
h	O
)	O
.	O
similarly	O
,	O
the	O
(	O
11.2.10	O
)	O
(	O
11.2.11	O
)	O
222	O
draft	O
march	O
9	O
,	O
2010	O
11.522.53−1.55−1.5−1.45−1.4−1.35−1.3−1.25−1.2−1.15−1.1−1.05θlogp	O
(	O
v|θ	O
)	O
θq	O
(	O
h=2	O
)	O
11.522.530.10.20.30.40.50.60.70.80.9θq	O
(	O
h=2	O
)	O
11.522.530.10.20.30.40.50.60.70.80.9	O
expectation	B
maximisation	I
example	O
52.	O
consider	O
a	O
simple	O
model	O
p	O
(	O
x1	O
,	O
x2|θ	O
)	O
where	O
dom	O
(	O
x1	O
)	O
=	O
dom	O
(	O
x2	O
)	O
=	O
{	O
1	O
,	O
2	O
}	O
.	O
assuming	O
an	O
unconstrained	O
distribution	B
p	O
(	O
x1	O
,	O
x2|θ	O
)	O
=	O
θx1	O
,	O
x2	O
,	O
θ1,1	O
+	O
θ1,2	O
+	O
θ2,1	O
+	O
θ2,2	O
=	O
1	O
(	O
11.2.12	O
)	O
(	O
11.2.13	O
)	O
our	O
aim	O
is	O
to	O
learn	O
θ	O
from	O
the	O
data	B
x1	O
=	O
(	O
1	O
,	O
1	O
)	O
,	O
x2	O
=	O
(	O
1	O
,	O
?	O
)	O
,	O
x3	O
=	O
(	O
?	O
,	O
2	O
)	O
.	O
the	O
energy	B
term	O
for	O
the	O
classical	O
em	O
is	O
log	O
p	O
(	O
x1	O
=	O
1	O
,	O
x2	O
=	O
1|θ	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x1	O
=	O
1	O
,	O
x2|θ	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
x2|x1=1	O
,	O
θold	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x1	O
,	O
x2	O
=	O
2|θ	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
x1|x2=2	O
,	O
θold	O
)	O
writing	O
out	O
fully	O
each	O
of	O
the	O
above	O
terms	O
on	O
a	O
separate	O
line	O
gives	O
the	O
energy	B
log	O
θ1,1	O
+p	O
(	O
x2	O
=	O
1|x1	O
=	O
1	O
,	O
θold	O
)	O
log	O
θ1,1	O
+	O
p	O
(	O
x2	O
=	O
2|x1	O
=	O
1	O
,	O
θold	O
)	O
log	O
θ1,2	O
+p	O
(	O
x1	O
=	O
1|x2	O
=	O
2	O
,	O
θold	O
)	O
log	O
θ1,2	O
+	O
p	O
(	O
x1	O
=	O
2|x2	O
=	O
2	O
,	O
θold	O
)	O
log	O
θ2,2	O
(	O
11.2.14	O
)	O
(	O
11.2.15	O
)	O
(	O
11.2.16	O
)	O
(	O
11.2.17	O
)	O
this	O
expression	O
resembles	O
the	O
standard	O
log	O
likelihood	B
of	O
fully	O
observed	O
data	O
except	O
that	O
terms	O
with	O
missing	O
data	B
have	O
their	O
weighted	O
log	O
parameters	O
.	O
the	O
parameters	O
are	O
conveniently	O
decoupled	O
in	O
this	O
bound	B
(	O
apart	O
from	O
the	O
trivial	O
normalisation	B
constraint	O
)	O
so	O
that	O
ﬁnding	O
the	O
optimal	O
parameters	O
is	O
straightforward	O
.	O
this	O
is	O
achieved	O
by	O
the	O
m-step	O
update	O
which	O
gives	O
θ1,1	O
∝	O
1	O
+	O
p	O
(	O
x2	O
=	O
1|x1	O
=	O
1	O
,	O
θold	O
)	O
θ1,2	O
∝	O
p	O
(	O
x2	O
=	O
2|x1	O
=	O
1	O
,	O
θold	O
)	O
+	O
p	O
(	O
x1	O
=	O
1|x2	O
=	O
2	O
,	O
θold	O
)	O
θ2,1	O
=	O
0	O
θ2,2	O
∝	O
p	O
(	O
x1	O
=	O
2|x2	O
=	O
2	O
,	O
θold	O
)	O
(	O
11.2.18	O
)	O
where	O
p	O
(	O
x2|x1	O
,	O
θold	O
)	O
∝	O
θold	O
x1	O
,	O
x2	O
(	O
e-step	O
)	O
etc	O
.	O
the	O
e	O
and	O
m-steps	O
are	O
iterated	O
till	O
convergence	O
.	O
the	O
em	O
algorithm	B
increases	O
the	O
likelihood	B
whilst	O
,	O
by	O
construction	B
,	O
the	O
em	O
algorithm	B
can	O
not	O
decrease	O
the	O
bound	B
on	O
the	O
likelihood	B
,	O
an	O
important	O
question	O
is	O
whether	O
or	O
not	O
the	O
log	O
likelihood	B
itself	O
is	O
necessarily	O
increased	O
by	O
this	O
procedure	O
.	O
we	O
use	O
θ	O
(	O
cid:48	O
)	O
for	O
the	O
new	O
parameters	O
,	O
and	O
θ	O
for	O
the	O
previous	O
parameters	O
in	O
two	O
consecutive	O
iterations	O
.	O
using	O
q	O
(	O
hn|vn	O
)	O
=	O
p	O
(	O
hn|vn	O
,	O
θ	O
)	O
we	O
see	O
that	O
as	O
a	O
function	B
of	O
the	O
parameters	O
,	O
the	O
lower	O
bound	O
for	O
a	O
single	O
variable	B
pair	O
(	O
v	O
,	O
h	O
)	O
depends	O
on	O
θ	O
and	O
θ	O
(	O
cid:48	O
)	O
:	O
|θ	O
)	O
≡	O
−	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
h|v	O
,	O
θ	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
h|v	O
,	O
θ	O
)	O
+	O
(	O
cid:10	O
)	O
log	O
p	O
(	O
h	O
,	O
v|θ	O
(	O
cid:48	O
)	O
)	O
(	O
cid:11	O
)	O
p	O
(	O
h|v	O
,	O
θ	O
)	O
(	O
11.2.19	O
)	O
(	O
11.2.20	O
)	O
(	O
cid:48	O
)	O
lb	O
(	O
θ	O
and	O
that	O
is	O
,	O
the	O
kullback-leibler	O
divergence	B
is	O
the	O
diﬀerence	O
between	O
the	O
lower	O
bound	O
and	O
the	O
true	O
likelihood	B
.	O
we	O
may	O
write	O
(	O
cid:48	O
)	O
)	O
=	O
lb	O
(	O
θ	O
(	O
cid:48	O
)	O
log	O
p	O
(	O
v|θ	O
|θ	O
)	O
+	O
kl	O
(	O
p	O
(	O
h|v	O
,	O
θ	O
)	O
|p	O
(	O
h|v	O
,	O
θ	O
(	O
cid:48	O
)	O
)	O
)	O
(	O
cid:125	O
)	O
log	O
p	O
(	O
v|θ	O
)	O
=	O
lb	O
(	O
θ|θ	O
)	O
+	O
kl	O
(	O
p	O
(	O
h|v	O
,	O
θ	O
)	O
|p	O
(	O
h|v	O
,	O
θ	O
)	O
)	O
(	O
cid:124	O
)	O
hence	O
log	O
p	O
(	O
v|θ	O
(	O
cid:48	O
)	O
)	O
−	O
log	O
p	O
(	O
v|θ	O
)	O
=	O
lb	O
(	O
θ	O
(	O
cid:124	O
)	O
(	O
cid:48	O
)	O
draft	O
march	O
9	O
,	O
2010	O
0	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
|θ	O
)	O
−	O
lb	O
(	O
θ|θ	O
)	O
≥0	O
(	O
cid:124	O
)	O
+	O
kl	O
(	O
p	O
(	O
h|v	O
,	O
θ	O
)	O
|p	O
(	O
h|v	O
,	O
θ	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
≥0	O
(	O
cid:48	O
)	O
)	O
)	O
(	O
cid:125	O
)	O
(	O
11.2.21	O
)	O
(	O
11.2.22	O
)	O
223	O
expectation	B
maximisation	I
s	O
1	O
0	O
1	O
1	O
1	O
0	O
0	O
c	O
1	O
0	O
1	O
0	O
1	O
0	O
1	O
figure	O
11.3	O
:	O
a	O
database	O
containing	O
information	O
about	O
being	O
a	O
smoker	O
(	O
1	O
signiﬁes	O
the	O
individual	O
is	O
a	O
smoker	O
)	O
,	O
and	O
lung	O
cancer	O
(	O
1	O
signiﬁes	O
the	O
individual	O
has	O
lung	O
cancer	O
)	O
.	O
each	O
row	O
contains	O
the	O
information	O
for	O
an	O
individual	O
,	O
so	O
that	O
there	O
are	O
7	O
individuals	O
in	O
the	O
database	O
.	O
the	O
ﬁrst	O
assertion	O
is	O
true	O
since	O
,	O
by	O
deﬁnition	O
of	O
the	O
m-step	O
,	O
we	O
search	O
for	O
a	O
θ	O
(	O
cid:48	O
)	O
which	O
has	O
a	O
higher	O
value	B
for	O
the	O
bound	B
than	O
our	O
starting	O
value	B
θ.	O
the	O
second	O
assertion	O
is	O
true	O
by	O
the	O
property	O
of	O
the	O
kullback-leibler	O
divergence	B
.	O
for	O
more	O
than	O
a	O
single	O
datapoint	O
,	O
we	O
simply	O
sum	O
each	O
individual	O
bound	B
for	O
log	O
p	O
(	O
vn|θ	O
)	O
.	O
hence	O
we	O
reach	O
the	O
important	O
conclusion	O
that	O
the	O
em	O
algorithm	B
increases	O
,	O
not	O
only	O
the	O
lower	O
bound	O
on	O
the	O
marginal	B
likelihood	I
,	O
but	O
the	O
marginal	B
likelihood	I
itself	O
(	O
more	O
correctly	O
,	O
the	O
em	O
can	O
not	O
decrease	O
these	O
quantities	O
)	O
.	O
shared	O
parameters	O
and	O
tables	O
the	O
case	O
of	O
tables	O
sharing	O
parameters	O
is	O
essentially	O
straightforward	O
.	O
according	O
to	O
the	O
energy	B
term	O
,	O
we	O
need	O
to	O
identify	O
all	O
those	O
terms	O
in	O
which	O
the	O
shared	O
parameter	B
occurs	O
.	O
the	O
objective	O
for	O
the	O
shared	O
parameter	B
is	O
then	O
the	O
sum	O
over	O
all	O
energy	B
terms	O
containing	O
the	O
shared	O
parameter	B
.	O
11.2.3	O
application	O
to	O
belief	B
networks	I
conceptually	O
,	O
the	O
application	O
of	O
em	O
to	O
training	B
belief	O
networks	O
with	O
missing	O
data	B
is	O
straightforward	O
.	O
the	O
battle	O
is	O
more	O
notational	O
than	O
conceptual	O
.	O
we	O
begin	O
the	O
development	O
with	O
an	O
example	O
,	O
from	O
which	O
intuition	O
about	O
the	O
general	O
case	O
can	O
be	O
gleaned	O
.	O
example	O
53.	O
consider	O
the	O
network	O
.	O
p	O
(	O
a	O
,	O
c	O
,	O
s	O
)	O
=	O
p	O
(	O
c|a	O
,	O
s	O
)	O
p	O
(	O
a	O
)	O
p	O
(	O
s	O
)	O
(	O
11.2.23	O
)	O
for	O
which	O
we	O
have	O
a	O
set	O
of	O
data	B
,	O
but	O
that	O
the	O
states	O
of	O
variable	B
a	O
are	O
never	O
observed	O
,	O
see	O
ﬁg	O
(	O
11.3	O
)	O
.	O
our	O
goal	O
is	O
to	O
learn	O
the	O
cpts	O
p	O
(	O
c|a	O
,	O
s	O
)	O
and	O
p	O
(	O
a	O
)	O
and	O
p	O
(	O
s	O
)	O
.	O
to	O
apply	O
em	O
,	O
algorithm	B
(	O
8	O
)	O
to	O
this	O
case	O
,	O
we	O
ﬁrst	O
assume	O
initial	O
parameters	O
θ0	O
a	O
,	O
θ0	O
s	O
,	O
θ0	O
c	O
.	O
the	O
ﬁrst	O
e-step	O
,	O
for	O
iteration	B
t	O
=	O
1	O
then	O
deﬁnes	O
a	O
set	O
of	O
distributions	O
on	O
the	O
hidden	B
variables	I
(	O
here	O
the	O
hidden	B
variable	I
is	O
a	O
)	O
qn=1	O
t=1	O
(	O
a	O
)	O
=	O
p	O
(	O
a|c	O
=	O
1	O
,	O
s	O
=	O
1	O
,	O
θ0	O
)	O
,	O
qn=2	O
t=1	O
(	O
a	O
)	O
=	O
p	O
(	O
a|c	O
=	O
0	O
,	O
s	O
=	O
0	O
,	O
θ0	O
)	O
(	O
11.2.24	O
)	O
and	O
so	O
on	O
for	O
the	O
7	O
training	B
examples	O
,	O
n	O
=	O
2	O
,	O
.	O
.	O
.	O
,	O
7.	O
for	O
notational	O
convenience	O
,	O
we	O
write	O
qn	O
of	O
qn	O
t	O
(	O
a	O
)	O
in	O
place	O
t	O
(	O
a|vn	O
)	O
.	O
we	O
now	O
move	O
to	O
the	O
ﬁrst	O
m-step	O
.	O
the	O
energy	B
term	O
for	O
any	O
iteration	B
t	O
is	O
:	O
7	O
(	O
cid:88	O
)	O
7	O
(	O
cid:88	O
)	O
n=1	O
n=1	O
e	O
(	O
θ	O
)	O
=	O
=	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
cn|an	O
,	O
sn	O
)	O
+	O
log	O
p	O
(	O
an	O
)	O
+	O
log	O
p	O
(	O
sn	O
)	O
(	O
cid:105	O
)	O
qn	O
(	O
cid:110	O
)	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
cn|an	O
,	O
sn	O
)	O
(	O
cid:105	O
)	O
qn	O
t	O
(	O
a	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
an	O
)	O
(	O
cid:105	O
)	O
qn	O
t	O
(	O
a	O
)	O
t	O
(	O
a	O
)	O
+	O
log	O
p	O
(	O
sn	O
)	O
(	O
cid:111	O
)	O
(	O
11.2.25	O
)	O
(	O
11.2.26	O
)	O
the	O
ﬁnal	O
term	O
is	O
the	O
log	O
likelihood	B
of	O
the	O
variable	B
s	O
,	O
and	O
p	O
(	O
s	O
)	O
appears	O
explicitly	O
only	O
in	O
this	O
term	O
.	O
hence	O
,	O
the	O
usual	O
maximum	B
likelihood	I
rule	O
applies	O
,	O
and	O
p	O
(	O
s	O
=	O
1	O
)	O
is	O
simply	O
given	O
by	O
the	O
relative	O
number	O
of	O
times	O
224	O
draft	O
march	O
9	O
,	O
2010	O
expectation	B
maximisation	I
that	O
s	O
=	O
1	O
occurs	O
in	O
the	O
database	O
,	O
giving	O
p	O
(	O
s	O
=	O
1	O
)	O
=	O
4/7	O
,	O
p	O
(	O
s	O
=	O
0	O
)	O
=	O
3/7	O
.	O
(	O
cid:88	O
)	O
n	O
{	O
qn	O
log	O
p	O
(	O
a	O
=	O
0	O
)	O
(	O
cid:88	O
)	O
the	O
parameter	B
p	O
(	O
a	O
=	O
1	O
)	O
occurs	O
in	O
the	O
terms	O
t	O
(	O
a	O
=	O
0	O
)	O
log	O
p	O
(	O
a	O
=	O
0	O
)	O
+	O
qn	O
t	O
(	O
a	O
=	O
1	O
)	O
log	O
p	O
(	O
a	O
=	O
1	O
)	O
}	O
which	O
,	O
using	O
the	O
normalisation	B
constraint	O
is	O
qn	O
t	O
(	O
a	O
=	O
0	O
)	O
+	O
log	O
(	O
1	O
−	O
p	O
(	O
a	O
=	O
0	O
)	O
)	O
(	O
cid:88	O
)	O
(	O
cid:80	O
)	O
(	O
cid:88	O
)	O
t	O
(	O
a	O
=	O
0	O
)	O
+	O
(	O
cid:80	O
)	O
t	O
(	O
a	O
=	O
0	O
)	O
n	O
qn	O
t	O
(	O
a	O
=	O
1	O
)	O
n	O
qn	O
1	O
n	O
=	O
n	O
n	O
n	O
qn	O
n	O
(	O
cid:80	O
)	O
qn	O
t	O
(	O
a	O
=	O
1	O
)	O
p	O
(	O
a	O
=	O
0	O
)	O
=	O
qn	O
t	O
(	O
a	O
=	O
0	O
)	O
diﬀerentiating	O
with	O
respect	O
to	O
p	O
(	O
a	O
=	O
0	O
)	O
and	O
solving	B
for	O
the	O
zero	O
derivative	O
we	O
get	O
(	O
11.2.27	O
)	O
(	O
11.2.28	O
)	O
(	O
11.2.29	O
)	O
that	O
is	O
,	O
whereas	O
in	O
the	O
standard	O
maximum	O
likelihood	B
estimate	O
,	O
we	O
would	O
have	O
the	O
real	O
counts	O
of	O
the	O
t	O
(	O
a	O
=	O
1	O
)	O
.	O
data	B
in	O
the	O
above	O
formula	O
,	O
here	O
they	O
have	O
been	O
replaced	O
with	O
our	O
guessed	O
values	O
qn	O
t	O
(	O
a	O
=	O
0	O
)	O
and	O
qn	O
a	O
similar	O
story	O
holds	O
for	O
p	O
(	O
c	O
=	O
1|a	O
=	O
0	O
,	O
s	O
=	O
1	O
)	O
.	O
the	O
contribution	O
of	O
this	O
term	O
to	O
the	O
energy	B
is	O
(	O
cid:88	O
)	O
t	O
(	O
a	O
=	O
0	O
)	O
log	O
p	O
(	O
c	O
=	O
1|a	O
=	O
0	O
,	O
s	O
=	O
1	O
)	O
+	O
(	O
cid:88	O
)	O
qn	O
qn	O
t	O
(	O
a	O
=	O
0	O
)	O
log	O
(	O
1	O
−	O
p	O
(	O
c	O
=	O
1|a	O
=	O
0	O
,	O
s	O
=	O
1	O
)	O
)	O
n	O
:	O
cn=0	O
,	O
sn=1	O
n	O
:	O
cn=1	O
,	O
sn=1	O
which	O
is	O
log	O
p	O
(	O
c	O
=	O
1|a	O
=	O
0	O
,	O
s	O
=	O
1	O
)	O
(	O
cid:88	O
)	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
p	O
(	O
c	O
=	O
1|a	O
=	O
0	O
,	O
s	O
=	O
1	O
)	O
=	O
p	O
(	O
c	O
=	O
1|a	O
=	O
0	O
,	O
s	O
=	O
1	O
)	O
=	O
n	O
t	O
(	O
a	O
=	O
0	O
)	O
+log	O
(	O
1−p	O
(	O
c	O
=	O
1|a	O
=	O
0	O
,	O
s	O
=	O
1	O
)	O
)	O
(	O
cid:88	O
)	O
qn	O
n	O
:	O
cn=0	O
,	O
sn=1	O
qn	O
t	O
(	O
a	O
=	O
0	O
)	O
(	O
11.2.30	O
)	O
n	O
:	O
cn=1	O
,	O
sn=1	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
n	O
i	O
[	O
cn	O
=	O
1	O
]	O
i	O
[	O
sn	O
=	O
1	O
]	O
qn	O
t	O
(	O
a	O
=	O
0	O
)	O
+	O
(	O
cid:80	O
)	O
i	O
[	O
cn	O
=	O
1	O
]	O
i	O
[	O
sn	O
=	O
1	O
]	O
i	O
[	O
an	O
=	O
0	O
]	O
+	O
(	O
cid:80	O
)	O
n	O
n	O
i	O
[	O
cn	O
=	O
1	O
]	O
i	O
[	O
sn	O
=	O
1	O
]	O
qn	O
t	O
(	O
a	O
=	O
0	O
)	O
i	O
[	O
cn	O
=	O
0	O
]	O
i	O
[	O
sn	O
=	O
1	O
]	O
qn	O
t	O
(	O
a	O
=	O
0	O
)	O
n	O
i	O
[	O
cn	O
=	O
1	O
]	O
i	O
[	O
sn	O
=	O
1	O
]	O
i	O
[	O
an	O
=	O
0	O
]	O
i	O
[	O
cn	O
=	O
0	O
]	O
i	O
[	O
sn	O
=	O
1	O
]	O
i	O
[	O
an	O
=	O
0	O
]	O
n	O
(	O
11.2.31	O
)	O
(	O
11.2.32	O
)	O
optimising	O
with	O
respect	O
to	O
p	O
(	O
c	O
=	O
1|a	O
=	O
0	O
,	O
s	O
=	O
1	O
)	O
gives	O
for	O
comparison	O
,	O
the	O
setting	O
in	O
the	O
complete	O
data	B
case	O
is	O
there	O
is	O
an	O
intuitive	O
relationship	O
between	O
these	O
updates	O
:	O
in	O
the	O
missing	B
data	I
case	O
we	O
replace	O
the	O
indicators	O
by	O
the	O
assumed	O
distributions	O
q.	O
iterating	O
the	O
e	O
and	O
m	O
steps	O
,	O
these	O
equations	O
will	O
converge	O
to	O
a	O
local	B
likelihood	O
optimum	O
.	O
to	O
minimise	O
the	O
notational	O
burden	O
,	O
we	O
assume	O
that	O
the	O
structure	B
of	O
the	O
missing	B
variables	O
is	O
ﬁxed	O
through-	O
out	O
,	O
this	O
being	O
equivalent	B
therefore	O
to	O
a	O
latent	B
variable	I
model	O
.	O
the	O
form	O
of	O
the	O
energy	B
term	O
for	O
belief	B
networks	I
is	O
(	O
cid:88	O
)	O
n	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
xn	O
)	O
(	O
cid:105	O
)	O
qt	O
(	O
hn|vn	O
)	O
=	O
(	O
cid:88	O
)	O
n	O
(	O
cid:88	O
)	O
i	O
it	O
is	O
useful	O
to	O
deﬁne	O
the	O
following	O
notation	O
:	O
t	O
(	O
x	O
)	O
=	O
qt	O
(	O
h|vn	O
)	O
δ	O
(	O
v	O
,	O
vn	O
)	O
qn	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
xn	O
i	O
|pa	O
(	O
xn	O
i	O
)	O
)	O
(	O
cid:105	O
)	O
qt	O
(	O
hn|vn	O
)	O
(	O
11.2.33	O
)	O
(	O
11.2.34	O
)	O
t	O
(	O
x	O
)	O
sets	O
the	O
visible	B
where	O
x	O
=	O
(	O
v	O
,	O
h	O
)	O
represents	O
all	O
the	O
variables	O
in	O
the	O
distribution	B
.	O
this	O
means	O
that	O
qn	O
variables	O
in	O
the	O
observed	O
state	O
,	O
and	O
deﬁnes	O
a	O
conditional	B
distribution	O
on	O
the	O
unobserved	O
variables	O
.	O
we	O
draft	O
march	O
9	O
,	O
2010	O
225	O
expectation	B
maximisation	I
algorithm	O
9	O
em	O
for	O
belief	B
networks	I
.	O
tables	O
,	O
and	O
dataset	O
on	O
the	O
visible	B
variables	O
v.	O
returns	O
the	O
maximum	B
likelihood	I
setting	O
of	O
tables	O
.	O
1	O
:	O
t	O
=	O
1	O
2	O
:	O
set	O
pt	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
to	O
initial	O
values	O
.	O
3	O
:	O
while	O
p	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
not	O
converged	O
(	O
or	O
likelihood	B
not	O
converged	O
)	O
do	O
input	O
:	O
a	O
bn	O
structure	B
p	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
with	O
empty	O
(	O
cid:46	O
)	O
iteration	B
counter	O
(	O
cid:46	O
)	O
initialisation	O
t	O
(	O
x	O
)	O
=	O
pt	O
(	O
hn|vn	O
)	O
δ	O
(	O
v	O
,	O
vn	O
)	O
qn	O
(	O
cid:80	O
)	O
n	O
pt+1	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
=	O
1	O
n	O
n=1	O
qn	O
t	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
t	O
←	O
t	O
+	O
1	O
for	O
n	O
=	O
1	O
to	O
n	O
do	O
end	O
for	O
for	O
i	O
=	O
1	O
to	O
k	O
do	O
4	O
:	O
5	O
:	O
6	O
:	O
7	O
:	O
8	O
:	O
9	O
:	O
end	O
for	O
10	O
:	O
11	O
:	O
end	O
while	O
12	O
:	O
return	O
pt	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
n	O
(	O
cid:88	O
)	O
qt	O
(	O
x	O
)	O
=	O
qn	O
t	O
(	O
x	O
)	O
1	O
n	O
n=1	O
then	O
deﬁne	O
the	O
mixture	B
distribution	O
(	O
cid:46	O
)	O
run	O
over	O
all	O
datapoints	O
(	O
cid:46	O
)	O
e	O
step	O
(	O
cid:46	O
)	O
run	O
over	O
all	O
variables	O
(	O
cid:46	O
)	O
m	O
step	O
(	O
cid:46	O
)	O
the	O
max	O
likelihood	B
parameter	O
estimate	O
.	O
the	O
energy	B
term	O
in	O
equation	B
(	O
11.2.5	O
)	O
can	O
be	O
written	O
more	O
compactly	O
as	O
(	O
cid:88	O
)	O
n	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
xn	O
)	O
(	O
cid:105	O
)	O
qt	O
(	O
h|vn	O
)	O
=	O
n	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
qt	O
(	O
x	O
)	O
(	O
cid:88	O
)	O
n	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
qt	O
(	O
x	O
)	O
=	O
n	O
to	O
see	O
this	O
consider	O
the	O
right	O
hand	O
side	O
of	O
the	O
above	O
qt	O
(	O
h|vn	O
)	O
δ	O
(	O
v	O
,	O
vn	O
)	O
=	O
(	O
cid:88	O
)	O
x	O
1	O
n	O
[	O
log	O
p	O
(	O
x	O
)	O
]	O
(	O
cid:88	O
)	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
(	O
cid:105	O
)	O
qt	O
(	O
x	O
)	O
=	O
(	O
cid:88	O
)	O
n	O
using	O
the	O
structure	B
of	O
the	O
belief	B
network	I
,	O
we	O
have	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
qt	O
(	O
x	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:68	O
)	O
(	O
cid:88	O
)	O
(	O
cid:104	O
)	O
log	O
qt	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
(	O
cid:105	O
)	O
qt	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
−	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
(	O
cid:105	O
)	O
qt	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
this	O
means	O
that	O
maximising	O
the	O
energy	B
is	O
equivalent	B
to	O
minimising	O
i	O
i	O
(	O
cid:68	O
)	O
i	O
(	O
cid:69	O
)	O
qt	O
(	O
pa	O
(	O
xi	O
)	O
)	O
n	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
xn	O
)	O
(	O
cid:105	O
)	O
qt	O
(	O
h|vn	O
)	O
(	O
cid:69	O
)	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
(	O
cid:105	O
)	O
qt	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
qt	O
(	O
pa	O
(	O
xi	O
)	O
)	O
where	O
we	O
added	O
the	O
constant	O
ﬁrst	O
term	O
to	O
make	O
this	O
into	O
the	O
form	O
of	O
a	O
kullback-leibler	O
divergence	B
.	O
since	O
this	O
is	O
a	O
sum	O
of	O
independent	O
kullback-leibler	O
divergences	O
,	O
optimally	O
the	O
m-step	O
is	O
given	O
by	O
setting	O
p	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
=	O
qt	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
(	O
11.2.40	O
)	O
in	O
practice	O
,	O
storing	O
the	O
qt	O
(	O
x	O
)	O
over	O
the	O
states	O
of	O
all	O
variables	O
x	O
is	O
prohibitively	O
expense	O
.	O
fortunately	O
,	O
since	O
the	O
m-step	O
only	O
requires	O
the	O
distribution	B
on	O
the	O
family	B
of	O
each	O
variable	B
xi	O
,	O
one	O
only	O
requires	O
the	O
local	B
distributions	O
qn	O
old	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
.	O
we	O
may	O
therefore	O
dispense	O
with	O
the	O
global	B
qold	O
(	O
x	O
)	O
and	O
equivalently	O
use	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
pnew	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
=	O
old	O
(	O
xi	O
,	O
pa	O
(	O
xi	O
)	O
)	O
n	O
qn	O
n	O
(	O
cid:48	O
)	O
qn	O
(	O
cid:48	O
)	O
old	O
(	O
pa	O
(	O
xi	O
)	O
)	O
using	O
the	O
em	O
algorithm	B
,	O
the	O
optimal	O
setting	O
for	O
the	O
e-step	O
is	O
to	O
use	O
qt	O
(	O
hn|vn	O
)	O
=	O
pold	O
(	O
hn|vn	O
)	O
.	O
with	O
this	O
notation	O
,	O
the	O
em	O
algorithm	B
can	O
be	O
compactly	O
stated	O
as	O
in	O
algorithm	B
(	O
9	O
)	O
.	O
see	O
also	O
embeliefnet.m	O
.	O
226	O
draft	O
march	O
9	O
,	O
2010	O
(	O
11.2.35	O
)	O
(	O
11.2.36	O
)	O
(	O
11.2.37	O
)	O
(	O
11.2.38	O
)	O
(	O
11.2.39	O
)	O
(	O
11.2.41	O
)	O
expectation	B
maximisation	I
example	O
54	O
(	O
more	O
general	O
belief	B
networks	I
)	O
.	O
consider	O
a	O
ﬁve	O
variable	B
distribution	O
with	O
discrete	O
variables	O
,	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
,	O
x5	O
)	O
=	O
p	O
(	O
x1|x2	O
)	O
p	O
(	O
x2|x3	O
)	O
p	O
(	O
x3|x4	O
)	O
p	O
(	O
x4|x5	O
)	O
p	O
(	O
x5	O
)	O
(	O
11.2.42	O
)	O
in	O
which	O
the	O
variables	O
x2	O
and	O
x4	O
are	O
consistently	O
hidden	B
in	O
the	O
training	B
data	O
,	O
and	O
training	B
data	O
for	O
x1	O
,	O
x3	O
,	O
x5	O
are	O
always	O
present	O
.	O
the	O
distribution	B
can	O
be	O
represented	O
as	O
a	O
belief	B
network	I
x1	O
x2	O
x3	O
x4	O
x5	O
in	O
this	O
case	O
,	O
the	O
contributions	O
to	O
the	O
energy	B
have	O
the	O
form	O
which	O
may	O
be	O
written	O
as	O
(	O
cid:88	O
)	O
n	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
xn	O
(	O
cid:88	O
)	O
+	O
(	O
cid:88	O
)	O
n	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
xn	O
n	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
xn	O
1|x2	O
)	O
p	O
(	O
x2|xn	O
3	O
)	O
p	O
(	O
xn	O
3|x4	O
)	O
p	O
(	O
x4|xn	O
5	O
)	O
p	O
(	O
xn	O
5	O
(	O
cid:105	O
)	O
qn	O
(	O
x2	O
,	O
x4|x1	O
,	O
x3	O
,	O
x5	O
)	O
1|x2	O
)	O
(	O
cid:105	O
)	O
qn	O
(	O
x2	O
,	O
x4|x1	O
,	O
x3	O
,	O
x5	O
)	O
+	O
(	O
cid:88	O
)	O
3|x4	O
)	O
(	O
cid:105	O
)	O
qn	O
(	O
x2	O
,	O
x4|x1	O
,	O
x3	O
,	O
x5	O
)	O
+	O
(	O
cid:88	O
)	O
n	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x2|xn	O
3	O
)	O
(	O
cid:105	O
)	O
qn	O
(	O
x2	O
,	O
x4|x1	O
,	O
x3	O
,	O
x5	O
)	O
5	O
)	O
(	O
cid:105	O
)	O
qn	O
(	O
x2	O
,	O
x4|x1	O
,	O
x3	O
,	O
x5	O
)	O
+	O
(	O
cid:88	O
)	O
n	O
n	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x4|xn	O
(	O
11.2.43	O
)	O
log	O
p	O
(	O
xn	O
5	O
)	O
(	O
11.2.44	O
)	O
a	O
useful	O
property	O
can	O
now	O
be	O
exploited	O
,	O
namely	O
that	O
each	O
term	O
depends	O
on	O
only	O
those	O
hidden	B
variables	I
in	O
the	O
family	B
that	O
that	O
term	O
represents	O
.	O
thus	O
we	O
may	O
write	O
(	O
cid:88	O
)	O
n	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
xn	O
1|x2	O
)	O
(	O
cid:105	O
)	O
qn	O
(	O
x2|x1	O
,	O
x3	O
,	O
x5	O
)	O
+	O
(	O
cid:88	O
)	O
+	O
(	O
cid:88	O
)	O
n	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
xn	O
n	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x2|xn	O
3|x4	O
)	O
(	O
cid:105	O
)	O
qn	O
(	O
x4|x1	O
,	O
x3	O
,	O
x5	O
)	O
+	O
(	O
cid:88	O
)	O
3	O
)	O
(	O
cid:105	O
)	O
qn	O
(	O
x2|x1	O
,	O
x3	O
,	O
x5	O
)	O
n	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x4|xn	O
5	O
)	O
(	O
cid:105	O
)	O
qn	O
(	O
x4|x1	O
,	O
x3	O
,	O
x5	O
)	O
+	O
(	O
cid:88	O
)	O
n	O
log	O
p	O
(	O
xn	O
5	O
)	O
the	O
ﬁnal	O
term	O
can	O
be	O
set	O
using	O
maximum	B
likelihood	I
.	O
let	O
us	O
consider	O
therefore	O
a	O
more	O
diﬃcult	O
table	O
,	O
p	O
(	O
x1|x2	O
)	O
.	O
when	O
will	O
the	O
table	O
entry	O
p	O
(	O
x1	O
=	O
i|x2	O
=	O
j	O
)	O
occur	O
in	O
the	O
energy	B
?	O
this	O
happens	O
whenever	O
xn	O
1	O
is	O
in	O
state	O
i.	O
since	O
there	O
is	O
a	O
summation	O
over	O
all	O
the	O
states	O
of	O
variables	O
x2	O
(	O
due	O
to	O
the	O
average	B
)	O
,	O
there	O
is	O
also	O
a	O
term	O
with	O
variable	O
x2	O
in	O
state	O
j.	O
hence	O
the	O
contribution	O
to	O
the	O
energy	B
from	O
terms	O
of	O
the	O
form	O
p	O
(	O
x1	O
=	O
i|x2	O
=	O
j	O
)	O
is	O
i	O
[	O
xn	O
1	O
=	O
i	O
]	O
qn	O
(	O
x2	O
=	O
j|x1	O
,	O
x3	O
,	O
x5	O
)	O
log	O
p	O
(	O
x1	O
=	O
i|x2	O
=	O
j	O
)	O
where	O
the	O
indicator	B
function	I
i	O
[	O
xn	O
isation	O
of	O
the	O
table	O
,	O
we	O
add	O
a	O
lagrange	O
term	O
:	O
1	O
=	O
i	O
]	O
equals	O
1	O
if	O
xn	O
1	O
is	O
in	O
state	O
i	O
and	O
is	O
zero	O
otherwise	O
.	O
to	O
ensure	O
normal-	O
(	O
cid:40	O
)	O
1	O
−	O
(	O
cid:88	O
)	O
k	O
(	O
11.2.45	O
)	O
(	O
cid:41	O
)	O
p	O
(	O
x1	O
=	O
k|x2	O
=	O
j	O
)	O
(	O
11.2.46	O
)	O
(	O
11.2.47	O
)	O
(	O
11.2.48	O
)	O
(	O
11.2.49	O
)	O
227	O
(	O
cid:88	O
)	O
n	O
(	O
cid:88	O
)	O
n	O
(	O
cid:88	O
)	O
i	O
[	O
xn	O
1	O
=	O
i	O
]	O
qn	O
(	O
x2	O
=	O
j|x1	O
,	O
x3	O
,	O
x5	O
)	O
log	O
p	O
(	O
x1	O
=	O
i|x2	O
=	O
j	O
)	O
+	O
λ	O
diﬀerentiating	O
with	O
respect	O
to	O
p	O
(	O
x1	O
=	O
i|x2	O
=	O
j	O
)	O
we	O
get	O
=	O
λ	O
n	O
or	O
hence	O
i	O
[	O
xn	O
1	O
=	O
i	O
]	O
qn	O
(	O
x2	O
=	O
j|x1	O
,	O
x3	O
,	O
x5	O
)	O
p	O
(	O
x1	O
=	O
i|x2	O
=	O
j	O
)	O
(	O
cid:88	O
)	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
i	O
[	O
xn	O
i	O
[	O
xn	O
i	O
[	O
xn	O
n	O
n	O
n	O
,	O
k	O
p	O
(	O
x1	O
=	O
i|x2	O
=	O
j	O
)	O
=	O
p	O
(	O
x1	O
=	O
i|x2	O
=	O
j	O
)	O
∝	O
1	O
=	O
i	O
]	O
qn	O
(	O
x2	O
=	O
j|x1	O
,	O
x3	O
,	O
x5	O
)	O
.	O
1	O
=	O
i	O
]	O
qn	O
(	O
x2	O
=	O
j|x1	O
,	O
x3	O
,	O
x5	O
)	O
1	O
=	O
k	O
]	O
qn	O
(	O
x2	O
=	O
j|x1	O
,	O
x3	O
,	O
x5	O
)	O
draft	O
march	O
9	O
,	O
2010	O
expectation	B
maximisation	I
figure	O
11.4	O
:	O
evolution	O
of	O
the	O
log-likelihood	O
versus	O
iterations	O
un-	O
der	O
the	O
em	O
training	B
procedure	O
(	O
from	O
solving	B
the	O
printer	O
night-	O
mare	O
with	O
missing	O
data	B
,	O
exercise	O
(	O
138	O
)	O
.	O
note	O
how	O
rapid	O
progress	O
is	O
made	O
at	O
the	O
beginning	O
,	O
but	O
convergence	O
can	O
be	O
slow	O
.	O
using	O
the	O
em	O
algorithm	B
,	O
we	O
have	O
qn	O
(	O
x2	O
=	O
j|x1	O
,	O
x3	O
,	O
x5	O
)	O
=	O
p	O
(	O
x2	O
=	O
j|xn	O
1	O
,	O
xn	O
3	O
,	O
xn	O
5	O
)	O
(	O
11.2.50	O
)	O
this	O
optimal	O
distribution	B
is	O
easy	O
to	O
compute	O
since	O
this	O
is	O
the	O
marginal	B
on	O
the	O
family	B
,	O
given	O
some	O
evidential	O
variables	O
.	O
hence	O
,	O
the	O
m-step	O
update	O
for	O
the	O
table	O
is	O
what	O
about	O
the	O
table	O
p	O
(	O
x2	O
=	O
i|x3	O
=	O
j	O
)	O
?	O
to	O
ensure	O
normalisation	B
of	O
the	O
table	O
,	O
we	O
add	O
a	O
lagrange	O
term	O
:	O
i	O
[	O
xn	O
i	O
[	O
xn	O
n	O
n	O
,	O
k	O
1	O
=	O
i	O
]	O
pold	O
(	O
x2	O
=	O
j|xn	O
1	O
=	O
k	O
]	O
pold	O
(	O
x2	O
=	O
j|xn	O
1	O
,	O
xn	O
1	O
,	O
xn	O
5	O
)	O
3	O
,	O
xn	O
5	O
)	O
3	O
,	O
xn	O
(	O
cid:40	O
)	O
1	O
−	O
(	O
cid:88	O
)	O
k	O
(	O
cid:41	O
)	O
p	O
(	O
x2	O
=	O
k|x3	O
=	O
j	O
)	O
(	O
11.2.51	O
)	O
(	O
11.2.52	O
)	O
(	O
11.2.53	O
)	O
(	O
11.2.54	O
)	O
(	O
11.2.55	O
)	O
pnew	O
(	O
x1	O
=	O
i|x2	O
=	O
j	O
)	O
=	O
(	O
cid:88	O
)	O
i	O
[	O
xn	O
n	O
3	O
=	O
j	O
]	O
qn	O
(	O
x2	O
=	O
i|x1	O
,	O
x3	O
,	O
x5	O
)	O
log	O
p	O
(	O
x2	O
=	O
i|x3	O
=	O
j	O
)	O
+	O
λ	O
as	O
before	O
,	O
diﬀerentiating	O
,	O
and	O
using	O
the	O
em	O
settings	O
,	O
we	O
have	O
pnew	O
(	O
x2	O
=	O
i|x3	O
=	O
j	O
)	O
=	O
i	O
[	O
xn	O
i	O
[	O
xn	O
3	O
=	O
j	O
]	O
pold	O
(	O
x2	O
=	O
i|xn	O
3	O
=	O
j	O
]	O
pold	O
(	O
x2	O
=	O
k|xn	O
1	O
,	O
xn	O
1	O
,	O
xn	O
5	O
)	O
3	O
,	O
xn	O
5	O
)	O
3	O
,	O
xn	O
n	O
n	O
,	O
k	O
pnew	O
(	O
x1	O
=	O
i|x2	O
=	O
j	O
)	O
∝	O
i	O
[	O
xn	O
1	O
=	O
i	O
]	O
i	O
[	O
xn	O
2	O
=	O
j	O
]	O
and	O
equation	B
(	O
11.2.53	O
)	O
would	O
be	O
pnew	O
(	O
x2	O
=	O
i|x3	O
=	O
j	O
)	O
∝	O
i	O
[	O
xn	O
3	O
=	O
j	O
]	O
i	O
[	O
xn	O
2	O
=	O
i	O
]	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
n	O
n	O
there	O
is	O
a	O
simple	O
intuitive	O
pattern	O
to	O
equation	B
(	O
11.9.2	O
)	O
and	O
equation	B
(	O
11.2.53	O
)	O
:	O
if	O
there	O
were	O
no	O
hidden	O
data	B
,	O
equation	B
(	O
11.9.2	O
)	O
would	O
read	O
all	O
that	O
we	O
do	O
,	O
therefore	O
,	O
in	O
the	O
general	O
em	O
case	O
,	O
is	O
to	O
replace	O
those	O
deterministic	B
functions	O
such	O
as	O
i	O
[	O
xn	O
5	O
)	O
.	O
this	O
is	O
merely	O
a	O
restatement	O
of	O
the	O
general	O
update	O
given	O
in	O
equation	B
(	O
11.2.41	O
)	O
under	O
the	O
deﬁnition	O
(	O
11.2.34	O
)	O
.	O
2	O
=	O
i	O
]	O
by	O
their	O
missing	B
variable	O
equivalents	O
pold	O
(	O
x2	O
=	O
i|xn	O
3	O
,	O
xn	O
1	O
,	O
xn	O
11.2.4	O
application	O
to	O
markov	O
networks	O
for	O
a	O
mn	O
deﬁned	O
over	O
visible	B
and	O
hidden	B
variables	I
p	O
(	O
v	O
,	O
h|θ	O
)	O
=	O
1	O
is	O
z	O
(	O
θ	O
)	O
log	O
p	O
(	O
v|θ	O
)	O
≥	O
−h	O
(	O
q	O
(	O
h	O
)	O
)	O
+	O
(	O
cid:88	O
)	O
c	O
(	O
cid:104	O
)	O
log	O
φc	O
(	O
h	O
,	O
v|θ	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
h	O
)	O
−	O
log	O
z	O
(	O
θ	O
)	O
(	O
cid:81	O
)	O
c	O
φc	O
(	O
h	O
,	O
v	O
)	O
the	O
em	O
variational	O
bound	O
(	O
11.2.56	O
)	O
whilst	O
the	O
bound	B
decouples	O
the	O
parameters	O
in	O
the	O
second	O
term	O
,	O
the	O
parameters	O
are	O
nevertheless	O
coupled	B
in	O
the	O
normalisation	B
z	O
(	O
θ	O
)	O
.	O
because	O
of	O
this	O
we	O
can	O
not	O
optimise	O
the	O
above	O
bound	B
on	O
a	O
parameter	B
by	O
parameter	B
basis	O
.	O
one	O
approach	B
is	O
to	O
use	O
an	O
additional	O
bound	B
log	O
z	O
(	O
θ	O
)	O
from	O
above	O
,	O
as	O
for	O
iterative	B
scaling	I
.	O
228	O
draft	O
march	O
9	O
,	O
2010	O
024681012−120−110−100−90−80−70−60−50log	O
likelihood	B
extensions	O
of	O
em	O
11.2.5	O
convergence	O
convergence	O
of	O
em	O
can	O
be	O
slow	O
,	O
particularly	O
when	O
the	O
number	O
of	O
missing	B
observations	O
is	O
greater	O
than	O
the	O
number	O
of	O
visible	B
observations	O
.	O
in	O
practice	O
,	O
one	O
often	O
combines	O
the	O
em	O
with	O
gradient	O
based	O
procedures	O
to	O
improve	O
convergence	O
,	O
see	O
section	O
(	O
11.7	O
)	O
.	O
note	O
also	O
that	O
the	O
log	O
likelihood	B
is	O
typically	O
a	O
non-convex	O
function	B
of	O
the	O
parameters	O
.	O
this	O
means	O
that	O
there	O
may	O
be	O
multiple	O
local	O
optima	O
and	O
the	O
solution	O
found	O
often	O
depends	O
on	O
the	O
initialisation	O
.	O
11.3	O
extensions	O
of	O
em	O
11.3.1	O
partial	O
m	O
step	O
it	O
is	O
not	O
necessary	O
to	O
ﬁnd	O
the	O
full	O
optimum	O
of	O
the	O
energy	B
term	O
at	O
each	O
iteration	B
.	O
as	O
long	O
as	O
one	O
ﬁnds	O
a	O
parameter	B
θ	O
(	O
cid:48	O
)	O
which	O
has	O
a	O
higher	O
energy	B
than	O
that	O
of	O
the	O
current	O
parameter	B
θ	O
,	O
then	O
the	O
conditions	O
required	O
in	O
section	O
(	O
11.2.2	O
)	O
still	O
hold	O
,	O
and	O
the	O
likelihood	B
can	O
not	O
decrease	O
at	O
each	O
iteration	B
.	O
11.3.2	O
partial	O
e	O
step	O
n	O
(	O
cid:88	O
)	O
n=1	O
n	O
(	O
cid:88	O
)	O
n=1	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
hn	O
,	O
vn|θ	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
hn|vn	O
)	O
the	O
e-step	O
requires	O
us	O
to	O
ﬁnd	O
the	O
optimum	O
of	O
log	O
p	O
(	O
v|θ	O
)	O
≥	O
−	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
hn|vn	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
hn|vn	O
)	O
+	O
with	O
respect	O
to	O
q	O
(	O
hn|vn	O
)	O
.	O
the	O
fully	O
optimal	O
setting	O
is	O
q	O
(	O
hn|vn	O
)	O
=	O
p	O
(	O
hn|vn	O
)	O
(	O
11.3.1	O
)	O
(	O
11.3.2	O
)	O
(	O
11.3.3	O
)	O
for	O
a	O
guaranteed	O
increase	O
in	O
likelihood	B
at	O
each	O
iteration	B
,	O
from	O
section	O
(	O
11.2.2	O
)	O
we	O
required	O
that	O
the	O
fully	O
optimal	O
setting	O
of	O
q	O
is	O
used	O
.	O
unfortunately	O
,	O
therefore	O
,	O
one	O
can	O
not	O
in	O
general	O
guarantee	O
that	O
such	O
a	O
partial	O
e	O
step	O
would	O
always	O
increase	O
the	O
likelihood	B
.	O
of	O
course	O
,	O
it	O
is	O
guaranteed	O
to	O
increase	O
the	O
lower	O
bound	O
on	O
the	O
likelihood	B
,	O
though	O
not	O
the	O
likelihood	B
itself	O
.	O
intractable	B
energy	I
the	O
em	O
algorithm	B
assumes	O
that	O
we	O
can	O
calculate	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
h	O
,	O
v|θ	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
h|v	O
)	O
of	O
distributions	O
-	O
for	O
example	O
,	O
factorised	B
distributions	O
q	O
(	O
h|v	O
)	O
=	O
(	O
cid:81	O
)	O
simpler	O
class	O
of	O
distributions	O
,	O
q	O
,	O
e.g	O
.	O
q	O
=	O
factorised	B
q	O
(	O
h|v	O
)	O
=	O
(	O
cid:81	O
)	O
however	O
,	O
in	O
general	O
,	O
it	O
may	O
be	O
that	O
we	O
can	O
only	O
carry	O
out	O
the	O
average	B
over	O
q	O
for	O
a	O
very	O
restricted	B
class	O
j	O
q	O
(	O
hj|v	O
)	O
.	O
in	O
such	O
cases	O
one	O
may	O
use	O
a	O
i	O
q	O
(	O
hi|v	O
)	O
,	O
for	O
which	O
the	O
averaging	O
required	O
for	O
the	O
energy	B
may	O
be	O
simpler	O
.	O
we	O
can	O
ﬁnd	O
the	O
best	O
distribution	B
in	O
class	O
q	O
by	O
minimising	O
the	O
kl	O
divergence	B
between	O
q	O
(	O
h|v	O
,	O
θq	O
)	O
and	O
p	O
(	O
h|v	O
,	O
θ	O
)	O
numerically	O
using	O
a	O
non-linear	B
optimisation	O
routine	O
:	O
qopt	O
=	O
argmin	O
q∈q	O
kl	O
(	O
q	O
(	O
h	O
)	O
|p	O
(	O
h|v	O
,	O
θ	O
)	O
)	O
(	O
11.3.4	O
)	O
alternatively	O
,	O
one	O
can	O
assume	O
a	O
certain	O
structured	B
form	O
for	O
the	O
q	O
distribution	B
,	O
and	O
learn	O
the	O
optimal	O
factors	O
of	O
the	O
distribution	B
by	O
free	O
form	O
functional	O
calculus	B
.	O
viterbi	B
training	O
an	O
extreme	O
case	O
is	O
to	O
restrict	O
q	O
(	O
hn|vn	O
)	O
to	O
a	O
delta-function	O
.	O
in	O
this	O
case	O
,	O
the	O
entropic	O
term	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
hn|vn	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
hn|vn	O
)	O
is	O
constant	O
,	O
so	O
that	O
the	O
optimal	O
delta	B
function	I
q	O
is	O
to	O
set	O
q	O
(	O
hn|vn	O
)	O
=	O
δ	O
(	O
hn	O
,	O
hn∗	O
)	O
draft	O
march	O
9	O
,	O
2010	O
(	O
11.3.5	O
)	O
229	O
n	O
(	O
cid:88	O
)	O
n=1	O
where	O
hn∗	O
=	O
argmax	O
h	O
p	O
(	O
h	O
,	O
vn|θ	O
)	O
a	O
failure	B
case	I
for	O
em	O
(	O
11.3.6	O
)	O
this	O
is	O
called	O
viterbi	B
training	O
and	O
is	O
common	O
in	O
training	B
hmms	O
,	O
see	O
section	O
(	O
23.2	O
)	O
.	O
em	O
training	B
with	O
this	O
restricted	B
class	O
of	O
q	O
distribution	B
is	O
therefore	O
guaranteed	O
to	O
increase	O
the	O
lower	O
bound	O
on	O
the	O
log	O
likelihood	B
,	O
though	O
not	O
the	O
likelihood	B
itself	O
.	O
a	O
practical	O
advantage	O
of	O
viterbi	B
training	O
is	O
that	O
the	O
energy	B
is	O
always	O
tractable	O
to	O
compute	O
,	O
becoming	O
simply	O
log	O
p	O
(	O
hn∗	O
,	O
vn|θ	O
)	O
(	O
11.3.7	O
)	O
which	O
is	O
amenable	O
to	O
optimisation	B
.	O
provided	O
there	O
is	O
suﬃcient	O
data	B
,	O
one	O
might	O
hope	O
that	O
the	O
likelihood	B
as	O
a	O
function	B
of	O
the	O
parameter	B
θ	O
will	O
be	O
sharply	O
peaked	O
around	O
the	O
optimum	O
value	B
.	O
this	O
means	O
that	O
at	O
convergence	O
the	O
approximation	B
of	O
the	O
posterior	B
p	O
(	O
h|v	O
,	O
θopt	O
)	O
by	O
a	O
delta	B
function	I
will	O
be	O
reasonable	O
,	O
and	O
an	O
update	O
of	O
em	O
using	O
viterbi	B
training	O
will	O
produce	O
a	O
new	O
θ	O
approximately	O
the	O
same	O
as	O
θopt	O
.	O
for	O
any	O
highly	O
suboptimal	O
θ	O
,	O
however	O
,	O
p	O
(	O
h|v	O
,	O
θ	O
)	O
will	O
be	O
far	O
from	O
a	O
delta	B
function	I
,	O
and	O
therefore	O
a	O
viterbi	B
update	O
is	O
less	O
reliable	O
in	O
terms	O
of	O
leading	O
to	O
an	O
increase	O
in	O
the	O
likelihood	B
itself	O
.	O
this	O
suggests	O
that	O
the	O
initialisation	O
of	O
θ	O
for	O
viterbi	B
training	O
is	O
more	O
critical	O
than	O
for	O
the	O
standard	O
em	O
.	O
stochastic	O
training	B
another	O
approximate	B
q	O
(	O
hn|vn	O
)	O
distribution	B
would	O
be	O
to	O
use	O
an	O
empirical	B
distribution	I
formed	O
by	O
samples	O
from	O
the	O
fully	O
optimal	O
distribution	B
p	O
(	O
hn|vn	O
,	O
θ	O
)	O
.	O
that	O
is	O
one	O
draws	O
samples	O
(	O
see	O
chapter	O
(	O
27	O
)	O
for	O
a	O
discussion	O
on	O
sampling	B
)	O
hn	O
1	O
,	O
.	O
.	O
.	O
,	O
hn	O
l	O
from	O
p	O
(	O
hn|vn	O
,	O
θ	O
)	O
and	O
forms	O
a	O
q	O
distribution	B
l	O
(	O
cid:88	O
)	O
1	O
l	O
l=1	O
δ	O
(	O
hn	O
,	O
hn	O
l	O
)	O
the	O
energy	B
becomes	O
then	O
proportional	O
to	O
q	O
(	O
hn|vn	O
)	O
=	O
l	O
(	O
cid:88	O
)	O
n	O
(	O
cid:88	O
)	O
n=1	O
l=1	O
log	O
p	O
(	O
hn	O
l	O
,	O
vn|θ	O
)	O
(	O
11.3.8	O
)	O
(	O
11.3.9	O
)	O
so	O
that	O
,	O
as	O
in	O
viterbi	B
training	O
,	O
the	O
energy	B
is	O
always	O
computationally	O
tractable	O
for	O
this	O
restricted	B
q	O
class	O
.	O
provided	O
that	O
the	O
samples	O
from	O
p	O
(	O
hn|vn	O
)	O
are	O
reliable	O
,	O
stochastic	O
training	B
will	O
produce	O
an	O
energy	B
function	O
with	O
(	O
on	O
average	B
)	O
the	O
same	O
characteristics	O
as	O
the	O
true	O
energy	B
under	O
the	O
classical	O
em	O
algorithm	B
.	O
this	O
means	O
that	O
the	O
solution	O
obtained	O
from	O
stochastic	O
training	B
should	O
tend	O
to	O
that	O
from	O
the	O
classical	O
em	O
as	O
the	O
number	O
of	O
samples	O
increases	O
.	O
p	O
(	O
v|θ	O
)	O
=	O
h	O
δ	O
(	O
v	O
,	O
f	O
(	O
h|θ	O
)	O
)	O
p	O
(	O
h	O
)	O
(	O
11.4.1	O
)	O
if	O
we	O
attempt	O
an	O
em	O
approach	B
for	O
this	O
,	O
this	O
will	O
fail	O
(	O
see	O
also	O
exercise	O
(	O
75	O
)	O
)	O
.	O
for	O
a	O
more	O
general	O
model	B
of	O
the	O
form	O
11.4	O
a	O
failure	B
case	I
for	O
em	O
consider	O
a	O
likelihood	B
of	O
the	O
form	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
p	O
(	O
v|θ	O
)	O
=	O
the	O
e-step	O
is	O
p	O
(	O
v|h	O
,	O
θ	O
)	O
p	O
(	O
h	O
)	O
h	O
q	O
(	O
h|θold	O
)	O
∝	O
p	O
(	O
v|h	O
,	O
θold	O
)	O
p	O
(	O
h	O
)	O
230	O
(	O
11.4.2	O
)	O
(	O
11.4.3	O
)	O
draft	O
march	O
9	O
,	O
2010	O
variational	O
bayes	O
and	O
the	O
m-step	O
sets	O
θnew	O
=	O
argmax	O
θ	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
v	O
,	O
h|θ	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
h|θold	O
)	O
=	O
argmax	O
θ	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
v|h	O
,	O
θ	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
h|θold	O
)	O
(	O
11.4.4	O
)	O
where	O
we	O
used	O
the	O
fact	O
that	O
for	O
this	O
model	B
p	O
(	O
h	O
)	O
is	O
independent	O
of	O
θ.	O
in	O
the	O
case	O
that	O
p	O
(	O
v|h	O
,	O
θ	O
)	O
=	O
δ	O
(	O
v	O
,	O
f	O
(	O
h|θ	O
)	O
)	O
then	O
p	O
(	O
h|θold	O
)	O
∝	O
δ	O
(	O
v	O
,	O
f	O
(	O
h|θ	O
)	O
)	O
p	O
(	O
h	O
)	O
so	O
that	O
optimising	O
the	O
energy	B
requires	O
θnew	O
=	O
argmax	O
θ	O
(	O
cid:104	O
)	O
log	O
δ	O
(	O
v	O
,	O
f	O
(	O
h|θ	O
)	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
h|θold	O
)	O
(	O
11.4.5	O
)	O
(	O
11.4.6	O
)	O
since	O
p	O
(	O
h|θold	O
)	O
is	O
zero	O
everywhere	O
expect	O
that	O
h	O
for	O
which	O
v	O
=	O
f	O
(	O
h|θ	O
)	O
,	O
then	O
the	O
energy	B
is	O
eﬀectively	O
negative	O
inﬁnity	O
if	O
θ	O
(	O
cid:54	O
)	O
=	O
θold	O
.	O
however	O
,	O
when	O
θ	O
=	O
θold	O
the	O
energy	B
is	O
maximal2	O
.	O
this	O
is	O
therefore	O
the	O
optimum	O
of	O
the	O
energy	B
,	O
and	O
represents	O
therefore	O
a	O
failure	O
in	O
updating	O
for	O
em	O
.	O
this	O
situation	O
occurs	O
in	O
practice	O
,	O
and	O
has	O
been	O
noted	O
in	O
particular	O
in	O
the	O
context	O
of	O
independent	O
component	O
analysis	B
[	O
222	O
]	O
.	O
one	O
can	O
attempt	O
to	O
heal	O
this	O
behaviour	O
by	O
deriving	O
an	O
em	O
algorithm	B
based	O
on	O
the	O
distribution	B
p	O
(	O
v|h	O
,	O
θ	O
)	O
=	O
(	O
1	O
−	O
	O
)	O
δ	O
(	O
v	O
,	O
f	O
(	O
h|θ	O
)	O
)	O
+	O
n	O
(	O
h	O
)	O
,	O
0	O
≤	O
	O
≤	O
1	O
where	O
n	O
(	O
h	O
)	O
is	O
an	O
arbitrary	O
distribution	B
on	O
the	O
hidden	B
variable	I
h.	O
the	O
original	O
deterministic	B
model	O
corre-	O
sponds	O
to	O
p0	O
(	O
v|h	O
,	O
θ	O
)	O
.	O
deﬁning	O
p	O
(	O
v|h	O
,	O
θ	O
)	O
p	O
(	O
h	O
)	O
p	O
(	O
v|θ	O
)	O
=	O
(	O
11.4.8	O
)	O
(	O
cid:90	O
)	O
h	O
we	O
have	O
p	O
(	O
v|θ	O
)	O
=	O
(	O
1	O
−	O
	O
)	O
p0	O
(	O
v|θ	O
)	O
+	O
	O
(	O
cid:104	O
)	O
n	O
(	O
h	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
h	O
)	O
an	O
em	O
algorithm	B
for	O
p	O
(	O
v|θ	O
)	O
,	O
0	O
<	O
	O
<	O
1	O
satisﬁes	O
p	O
(	O
v|θnew	O
)	O
−	O
p	O
(	O
v|θold	O
)	O
=	O
(	O
1	O
−	O
	O
)	O
(	O
p0	O
(	O
v|θnew	O
)	O
−	O
p0	O
(	O
v|θold	O
)	O
)	O
>	O
0	O
which	O
implies	O
p0	O
(	O
v|θnew	O
)	O
−	O
p0	O
(	O
v|θold	O
)	O
>	O
0	O
(	O
11.4.7	O
)	O
(	O
11.4.9	O
)	O
(	O
11.4.10	O
)	O
(	O
11.4.11	O
)	O
this	O
means	O
that	O
the	O
em	O
algorithm	B
for	O
the	O
non-deterministic	O
case	O
0	O
<	O
	O
<	O
1	O
is	O
guaranteed	O
to	O
increase	O
the	O
likelihood	B
under	O
the	O
deterministic	B
model	O
p0	O
(	O
v|θ	O
)	O
at	O
each	O
iteration	B
(	O
unless	O
we	O
are	O
at	O
convergence	O
)	O
.	O
see	O
[	O
100	O
]	O
for	O
an	O
application	O
of	O
this	O
‘	O
antifreeze	B
’	O
technique	O
to	O
learning	B
markov	O
decision	O
processes	O
with	O
em	O
.	O
11.5	O
variational	O
bayes	O
as	O
discussed	O
in	O
section	O
(	O
9.2	O
)	O
maximum	B
likelihood	I
corresponds	O
to	O
a	O
form	O
of	O
bayesian	O
approach	B
in	O
which	O
the	O
parameter	B
posterior	O
distribution	B
(	O
under	O
a	O
ﬂat	O
prior	B
)	O
is	O
approximated	O
with	O
a	O
delta	B
function	I
p	O
(	O
θ|v	O
)	O
≈	O
δ	O
(	O
θ	O
,	O
θopt	O
)	O
.	O
variational	O
bayes	O
is	O
analogous	O
to	O
em	O
in	O
that	O
it	O
attempts	O
to	O
deal	O
with	O
hidden	O
variables	O
but	O
using	O
a	O
distribution	B
that	O
better	O
represents	O
the	O
posterior	B
distribution	O
than	O
given	O
by	O
maximum	B
likelihood	I
.	O
to	O
keep	O
the	O
notation	O
simple	O
,	O
we	O
’	O
ll	O
initially	O
assume	O
only	O
a	O
single	O
datapoint	O
with	O
observation	O
v.	O
our	O
interest	O
is	O
then	O
the	O
parameter	B
posterior	O
p	O
(	O
θ|v	O
)	O
∝	O
p	O
(	O
v|θ	O
)	O
p	O
(	O
θ	O
)	O
∝	O
p	O
(	O
v	O
,	O
h|θ	O
)	O
p	O
(	O
θ	O
)	O
(	O
11.5.1	O
)	O
(	O
cid:88	O
)	O
h	O
the	O
vb	O
approach	B
assumes	O
a	O
factorised	B
approximation	O
of	O
the	O
joint	B
hidden	O
and	O
parameter	B
posterior	O
:	O
p	O
(	O
h	O
,	O
θ|v	O
)	O
≈	O
q	O
(	O
h	O
)	O
q	O
(	O
θ	O
)	O
(	O
11.5.2	O
)	O
2for	O
discrete	B
variables	O
and	O
the	O
kronecker	O
delta	O
,	O
the	O
energy	B
attains	O
the	O
maximal	O
value	B
of	O
zero	O
when	O
θ	O
=	O
θold	O
.	O
in	O
the	O
case	O
of	O
continuous	B
variables	O
,	O
however	O
,	O
the	O
log	O
of	O
the	O
dirac	O
delta	B
function	I
is	O
not	O
well	O
deﬁned	O
.	O
considering	O
the	O
delta	B
function	I
as	O
the	O
limit	O
of	O
a	O
narrow	O
width	O
gaussian	O
,	O
for	O
any	O
small	O
but	O
ﬁnite	O
width	O
,	O
the	O
energy	B
is	O
largest	O
when	O
θ	O
=	O
θold	O
.	O
draft	O
march	O
9	O
,	O
2010	O
231	O
a	O
bound	B
on	O
the	O
marginal	B
likelihood	I
by	O
minimising	O
the	O
kl	O
divergence	B
,	O
kl	O
(	O
q	O
(	O
h	O
)	O
q	O
(	O
θ	O
)	O
|p	O
(	O
h	O
,	O
θ|v	O
)	O
)	O
=	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
h	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
h	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
θ	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
θ	O
)	O
−	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
h	O
,	O
θ|v	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
h	O
)	O
q	O
(	O
θ	O
)	O
≥	O
0	O
we	O
arrive	O
at	O
the	O
bound	B
log	O
p	O
(	O
v	O
)	O
≥	O
−	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
h	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
h	O
)	O
−	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
θ	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
θ	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
v	O
,	O
h	O
,	O
θ	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
h	O
)	O
q	O
(	O
θ	O
)	O
for	O
ﬁxed	O
q	O
(	O
θ	O
)	O
if	O
we	O
minimize	O
the	O
kullback-leibler	O
divergence	B
,	O
we	O
get	O
the	O
tightest	O
lower	O
bound	O
on	O
log	O
p	O
(	O
v	O
)	O
.	O
if	O
then	O
for	O
ﬁxed	O
q	O
(	O
h	O
)	O
we	O
minimise	O
the	O
kullback-leibler	O
divergence	B
w.r.t	O
.	O
q	O
(	O
θ	O
)	O
we	O
are	O
maximising	O
the	O
term	O
−	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
θ	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
θ	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
v	O
,	O
h	O
,	O
θ	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
h	O
)	O
q	O
(	O
θ	O
)	O
and	O
hence	O
pushing	O
up	O
the	O
bound	B
on	O
the	O
marginal	B
likelihood	I
.	O
this	O
simple	O
co-ordinate	O
wise	O
procedure	O
in	O
which	O
we	O
ﬁrst	O
ﬁx	O
the	O
q	O
(	O
θ	O
)	O
and	O
solve	O
for	O
q	O
(	O
h	O
)	O
and	O
then	O
vice	O
versa	O
is	O
analogous	O
to	O
the	O
e	O
and	O
m	O
step	O
of	O
the	O
em	O
algorithm	B
:	O
e-step	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
q	O
(	O
h	O
)	O
qold	O
(	O
θ	O
)	O
|p	O
(	O
h	O
,	O
θ|v	O
)	O
qnew	O
(	O
h	O
)	O
=	O
argmin	O
q	O
(	O
h	O
)	O
m-step	O
kl	O
qnew	O
(	O
θ	O
)	O
=	O
argmin	O
q	O
(	O
θ	O
)	O
kl	O
(	O
qnew	O
(	O
h	O
)	O
q	O
(	O
θ	O
)	O
|p	O
(	O
h	O
,	O
θ|v	O
)	O
)	O
variational	O
bayes	O
(	O
11.5.3	O
)	O
(	O
11.5.4	O
)	O
(	O
11.5.5	O
)	O
(	O
11.5.6	O
)	O
in	O
full	O
generality	O
for	O
a	O
set	O
of	O
observations	O
v	O
and	O
hidden	B
variables	I
h	O
,	O
algorithm	B
(	O
7	O
)	O
.	O
for	O
distributions	O
q	O
(	O
h	O
)	O
and	O
q	O
(	O
θ	O
)	O
which	O
are	O
parametersised/constrained	O
,	O
the	O
best	O
distributions	O
in	O
the	O
minimal	O
kl	O
sense	O
are	O
returned	O
.	O
in	O
general	O
,	O
each	O
iteration	B
of	O
vb	O
is	O
guaranteed	O
to	O
increase	O
the	O
bound	B
on	O
the	O
marginal	B
likelihood	I
,	O
but	O
not	O
the	O
marginal	B
likelihood	I
itself	O
.	O
like	O
the	O
em	O
algorithm	B
,	O
vb	O
can	O
(	O
and	O
often	O
does	O
)	O
suﬀer	O
from	O
local	B
maxima	O
issues	O
.	O
this	O
means	O
that	O
the	O
converged	O
solution	O
can	O
be	O
dependent	O
on	O
the	O
initialisation	O
.	O
unconstrained	O
approximations	O
for	O
ﬁxed	O
q	O
(	O
θ	O
)	O
the	O
contribution	O
to	O
the	O
kl	O
divergence	B
is	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
h	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
h	O
)	O
−	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
v	O
,	O
h	O
,	O
θ	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
h	O
)	O
q	O
(	O
θ	O
)	O
=	O
kl	O
(	O
q	O
(	O
h	O
)	O
|˜p	O
(	O
h	O
)	O
)	O
+	O
const	O
.	O
where	O
˜p	O
(	O
h	O
)	O
≡	O
1	O
˜z	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
v	O
,	O
h	O
,	O
θ	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
θ	O
)	O
e	O
where	O
˜z	O
is	O
a	O
normalising	O
constant	O
.	O
hence	O
,	O
for	O
ﬁxed	O
q	O
(	O
θ	O
)	O
,	O
the	O
optimal	O
q	O
(	O
h	O
)	O
is	O
given	O
by	O
˜p	O
,	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
v	O
,	O
h	O
,	O
θ	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
θ	O
)	O
q	O
(	O
h	O
)	O
∝	O
e	O
similarly	O
,	O
for	O
ﬁxed	O
q	O
(	O
h	O
)	O
,	O
optimally	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
v	O
,	O
h	O
,	O
θ	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
h	O
)	O
q	O
(	O
θ	O
)	O
∝	O
e	O
(	O
11.5.7	O
)	O
(	O
11.5.8	O
)	O
(	O
11.5.9	O
)	O
(	O
11.5.10	O
)	O
log	O
p	O
(	O
v|θ	O
)	O
≥	O
q	O
(	O
h1	O
,	O
.	O
.	O
.	O
,	O
hn	O
)	O
=	O
(	O
cid:89	O
)	O
i.i.d	O
.	O
data	B
under	O
the	O
i.i.d	O
.	O
assumption	O
,	O
we	O
obtain	O
a	O
bound	B
on	O
the	O
marginal	B
likelihood	I
for	O
the	O
whole	O
dataset	O
:	O
(	O
cid:88	O
)	O
n	O
(	O
cid:110	O
)	O
−	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
hn	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
hn	O
)	O
−	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
θ	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
θ	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
vn	O
,	O
hn	O
,	O
θ	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
hn	O
)	O
q	O
(	O
θ	O
)	O
(	O
cid:111	O
)	O
(	O
11.5.11	O
)	O
the	O
bound	B
holds	O
for	O
any	O
q	O
(	O
hn	O
)	O
and	O
q	O
(	O
θ	O
)	O
but	O
is	O
tightest	O
for	O
the	O
converged	O
estimates	O
from	O
the	O
vb	O
procedure	O
.	O
for	O
an	O
i.i.d	O
.	O
dataset	O
,	O
it	O
is	O
straightforward	O
to	O
show	O
that	O
without	O
loss	O
of	O
generality	O
we	O
may	O
assume	O
q	O
(	O
hn	O
)	O
(	O
11.5.12	O
)	O
under	O
this	O
we	O
arrive	O
at	O
algorithm	B
(	O
11	O
)	O
.	O
n	O
232	O
draft	O
march	O
9	O
,	O
2010	O
variational	O
bayes	O
algorithm	B
10	O
variational	O
bayes	O
.	O
1	O
:	O
t	O
=	O
0	O
2	O
:	O
choose	O
an	O
initial	O
distribution	B
q0	O
(	O
θ	O
)	O
.	O
3	O
:	O
while	O
θ	O
not	O
converged	O
(	O
or	O
likelihood	B
bound	O
not	O
converged	O
)	O
do	O
4	O
:	O
5	O
:	O
6	O
:	O
7	O
:	O
end	O
while	O
8	O
:	O
return	O
qn	O
t	O
←	O
t	O
+	O
1	O
t	O
(	O
h	O
)	O
=	O
arg	O
minq	O
(	O
h	O
)	O
kl	O
(	O
q	O
(	O
h	O
)	O
qt−1	O
(	O
θ	O
)	O
|p	O
(	O
h	O
,	O
θ|v	O
)	O
)	O
qn	O
t	O
(	O
θ	O
)	O
=	O
arg	O
minq	O
(	O
θ	O
)	O
kl	O
(	O
qn	O
qn	O
t	O
(	O
h	O
)	O
q	O
(	O
θ	O
)	O
|p	O
(	O
h	O
,	O
θ|v	O
)	O
)	O
t	O
(	O
θ	O
)	O
(	O
cid:46	O
)	O
iteration	B
counter	O
(	O
cid:46	O
)	O
initialisation	O
(	O
cid:46	O
)	O
e	O
step	O
(	O
cid:46	O
)	O
m	O
step	O
(	O
cid:46	O
)	O
the	O
posterior	B
parameter	O
approximation	B
.	O
hn	O
vn	O
n	O
(	O
a	O
)	O
θ	O
hn	O
n	O
θ	O
(	O
b	O
)	O
figure	O
11.5	O
:	O
(	O
a	O
)	O
:	O
generic	O
form	O
of	O
a	O
model	B
(	O
b	O
)	O
:	O
a	O
fac-	O
with	O
hidden	O
variables	O
.	O
torised	O
posterior	B
approximation	O
uses	O
in	O
variational	O
bayes	O
.	O
11.5.1	O
em	O
is	O
a	O
special	O
case	O
of	O
variational	O
bayes	O
if	O
we	O
wish	O
to	O
ﬁnd	O
a	O
summary	O
of	O
the	O
parameter	B
distribution	O
corresponding	O
to	O
only	O
the	O
most	O
likely	O
point	O
θ	O
,	O
then	O
q	O
(	O
θ	O
)	O
=	O
δ	O
(	O
θ	O
,	O
θ∗	O
)	O
(	O
11.5.13	O
)	O
where	O
θ∗	O
is	O
the	O
single	O
optimal	O
value	B
of	O
the	O
parameter	B
.	O
if	O
we	O
plug	O
this	O
assumption	O
into	O
equation	B
(	O
11.5.4	O
)	O
we	O
obtain	O
the	O
bound	B
the	O
m-step	O
is	O
then	O
given	O
by	O
log	O
p	O
(	O
v|θ∗	O
)	O
≥	O
−	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
h	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
h	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
v	O
,	O
h	O
,	O
θ∗	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
h	O
)	O
+	O
const	O
.	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
v|h	O
,	O
θ	O
)	O
p	O
(	O
h|θ	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
h	O
)	O
+	O
log	O
p	O
(	O
θ	O
)	O
θ∗	O
=	O
argmax	O
θ	O
(	O
11.5.14	O
)	O
(	O
11.5.15	O
)	O
for	O
a	O
ﬂat	O
prior	B
p	O
(	O
θ	O
)	O
=	O
const.	O
,	O
this	O
is	O
therefore	O
equivalent	B
to	O
energy	B
maximisation	O
in	O
the	O
em	O
algorithm	B
.	O
using	O
this	O
single	O
optimal	O
value	B
in	O
the	O
vb	O
update	O
for	O
q	O
(	O
hn	O
)	O
we	O
have	O
qn	O
t	O
(	O
h	O
)	O
∝	O
p	O
(	O
v	O
,	O
h|θ∗	O
)	O
∝	O
p	O
(	O
h|v	O
,	O
θ∗	O
)	O
(	O
11.5.16	O
)	O
which	O
is	O
the	O
standard	O
e-step	O
of	O
em	O
.	O
hence	O
em	O
is	O
a	O
special	O
case	O
of	O
vb	O
,	O
under	O
a	O
ﬂat	O
prior	B
p	O
(	O
θ	O
)	O
=	O
const	O
.	O
and	O
a	O
delta	B
function	I
approximation	O
of	O
the	O
parameter	B
posterior	O
.	O
11.5.2	O
factorising	O
the	O
parameter	B
posterior	O
let	O
’	O
s	O
reconsider	O
bayesian	O
learning	B
in	O
the	O
binary	O
variable	O
network	O
p	O
(	O
a	O
,	O
c	O
,	O
s	O
)	O
=	O
p	O
(	O
c|a	O
,	O
s	O
)	O
p	O
(	O
a	O
)	O
p	O
(	O
s	O
)	O
in	O
which	O
we	O
use	O
a	O
factorised	B
parameter	O
prior	B
(	O
11.5.17	O
)	O
p	O
(	O
θc	O
)	O
(	O
θa	O
)	O
p	O
(	O
θs	O
)	O
(	O
11.5.18	O
)	O
when	O
all	O
the	O
data	B
is	O
observed	O
,	O
the	O
parameter	B
posterior	O
factorises	O
.	O
as	O
we	O
discussed	O
in	O
section	O
(	O
11.1.1	O
)	O
if	O
the	O
state	O
of	O
a	O
is	O
not	O
observed	O
,	O
the	O
parameter	B
posterior	O
no	O
longer	O
factorises	O
:	O
p	O
(	O
θa	O
,	O
θs	O
,	O
θc|v	O
)	O
∝	O
p	O
(	O
θa	O
)	O
p	O
(	O
θs	O
)	O
p	O
(	O
θc	O
)	O
p	O
(	O
v|θa	O
,	O
θs	O
,	O
θc	O
)	O
∝	O
p	O
(	O
θa	O
)	O
p	O
(	O
θs	O
)	O
p	O
(	O
θc	O
)	O
(	O
cid:89	O
)	O
∝	O
p	O
(	O
θa	O
)	O
p	O
(	O
θs	O
)	O
p	O
(	O
θc	O
)	O
(	O
cid:89	O
)	O
n	O
n	O
p	O
(	O
vn|θa	O
,	O
θs	O
,	O
θc	O
)	O
p	O
(	O
sn|θs	O
)	O
(	O
cid:88	O
)	O
an	O
p	O
(	O
cn|sn	O
,	O
an	O
,	O
θc	O
)	O
p	O
(	O
an|θa	O
)	O
draft	O
march	O
9	O
,	O
2010	O
(	O
11.5.19	O
)	O
(	O
11.5.20	O
)	O
(	O
11.5.21	O
)	O
233	O
algorithm	B
11	O
variational	O
bayes	O
(	O
i.i.d	O
.	O
data	B
)	O
.	O
1	O
:	O
t	O
=	O
0	O
2	O
:	O
choose	O
an	O
initial	O
distribution	B
q0	O
(	O
θ	O
)	O
.	O
3	O
:	O
while	O
θ	O
not	O
converged	O
(	O
or	O
likelihood	B
bound	O
not	O
converged	O
)	O
do	O
4	O
:	O
5	O
:	O
t	O
←	O
t	O
+	O
1	O
for	O
n	O
=	O
1	O
to	O
n	O
do	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
vn	O
,	O
hn	O
,	O
θ	O
)	O
(	O
cid:105	O
)	O
qt−1	O
(	O
θ	O
)	O
t	O
(	O
hn	O
)	O
∝	O
e	O
qn	O
(	O
cid:80	O
)	O
end	O
for	O
qt	O
(	O
θ	O
)	O
∝	O
p	O
(	O
θ	O
)	O
e	O
n	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
vn	O
,	O
hn|θ	O
)	O
(	O
cid:105	O
)	O
qn	O
t	O
(	O
hn	O
)	O
6	O
:	O
7	O
:	O
8	O
:	O
9	O
:	O
end	O
while	O
10	O
:	O
return	O
qn	O
t	O
(	O
θ	O
)	O
θs	O
s	O
n	O
θa	O
a	O
c	O
θc	O
(	O
a	O
)	O
θs	O
θa	O
a	O
n	O
θc	O
(	O
b	O
)	O
variational	O
bayes	O
(	O
cid:46	O
)	O
iteration	B
counter	O
(	O
cid:46	O
)	O
initialisation	O
(	O
cid:46	O
)	O
run	O
over	O
all	O
datapoints	O
(	O
cid:46	O
)	O
e	O
step	O
(	O
cid:46	O
)	O
m	O
step	O
(	O
cid:46	O
)	O
the	O
posterior	B
parameter	O
approximation	B
.	O
figure	O
11.6	O
:	O
(	O
a	O
)	O
:	O
a	O
model	B
for	O
the	O
rela-	O
tionship	O
between	O
lung	O
cancer	O
,	O
asbestos	O
exposure	O
and	O
smoking	O
with	O
factorised	O
pa-	O
rameter	O
priors	O
.	O
variables	O
c	O
and	O
s	O
are	O
ob-	O
served	O
,	O
but	O
variable	B
a	O
is	O
consistently	O
miss-	O
(	O
b	O
)	O
:	O
a	O
factorised	B
parameter	O
poste-	O
ing	O
.	O
rior	O
approximation	B
.	O
where	O
the	O
summation	O
over	O
a	O
prevents	O
the	O
factorisation	O
into	O
a	O
product	O
of	O
the	O
individual	O
table	O
parameters	O
.	O
since	O
it	O
is	O
convenient	O
in	O
terms	O
of	O
representations	O
to	O
work	O
with	O
factorised	O
posteriors	O
,	O
we	O
can	O
apply	O
vb	O
but	O
with	O
a	O
factorised	B
constraint	O
on	O
the	O
form	O
of	O
the	O
q.	O
in	O
vb	O
we	O
deﬁne	O
a	O
distribution	B
over	O
the	O
visible	B
and	O
hidden	B
variables	I
.	O
in	O
this	O
case	O
the	O
hidden	B
variables	I
are	O
the	O
an	O
and	O
the	O
visible	B
are	O
sn	O
,	O
cn	O
.	O
the	O
joint	B
posterior	O
over	O
all	O
unobserved	O
variables	O
(	O
parameters	O
and	O
missing	B
observations	O
)	O
is	O
p	O
(	O
θa	O
,	O
θs	O
,	O
θc	O
,	O
a1	O
,	O
.	O
.	O
.	O
,	O
an|v	O
)	O
∝	O
p	O
(	O
θa	O
)	O
p	O
(	O
θs	O
)	O
p	O
(	O
θc	O
)	O
(	O
cid:89	O
)	O
p	O
(	O
θa	O
,	O
θs	O
,	O
θc	O
,	O
a1	O
,	O
.	O
.	O
.	O
,	O
an|v	O
)	O
≈	O
q	O
(	O
θa	O
)	O
q	O
(	O
θc	O
)	O
q	O
(	O
θs	O
)	O
(	O
cid:89	O
)	O
n	O
to	O
make	O
a	O
factorised	B
posterior	O
approximation	B
we	O
use	O
n	O
p	O
(	O
cn|sn	O
,	O
an	O
,	O
θc	O
)	O
p	O
(	O
sn|θs	O
)	O
p	O
(	O
an|θa	O
)	O
(	O
11.5.22	O
)	O
q	O
(	O
an	O
)	O
(	O
11.5.23	O
)	O
and	O
minimise	O
the	O
kullback-leibler	O
divergence	B
between	O
the	O
left	O
and	O
right	O
of	O
the	O
above	O
.	O
m-step	O
hence	O
q	O
(	O
θa	O
)	O
∝	O
p	O
(	O
θa	O
)	O
(	O
cid:89	O
)	O
n	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
an|θa	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
an	O
)	O
e	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
an|θa	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
an	O
)	O
=	O
q	O
(	O
an	O
=	O
1	O
)	O
log	O
θa	O
+	O
q	O
(	O
an	O
=	O
0	O
)	O
log	O
(	O
1	O
−	O
θa	O
)	O
hence	O
e	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
an|θa	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
an	O
)	O
=	O
θq	O
(	O
an=1	O
)	O
(	O
1	O
−	O
θa	O
)	O
q	O
(	O
an=0	O
)	O
it	O
is	O
convenient	O
to	O
use	O
a	O
beta	B
distribution	O
prior	B
,	O
a	O
p	O
(	O
θa	O
)	O
∝	O
θα−1	O
a	O
(	O
1	O
−	O
θa	O
)	O
β−1	O
234	O
(	O
11.5.24	O
)	O
(	O
11.5.25	O
)	O
(	O
11.5.26	O
)	O
(	O
11.5.27	O
)	O
draft	O
march	O
9	O
,	O
2010	O
variational	O
bayes	O
(	O
cid:32	O
)	O
θa|α	O
+	O
(	O
cid:88	O
)	O
(	O
cid:32	O
)	O
θs|α	O
+	O
(	O
cid:88	O
)	O
n	O
q	O
(	O
θa	O
)	O
=	O
b	O
n	O
a	O
similar	O
calculation	O
gives	O
q	O
(	O
θs	O
)	O
=	O
b	O
q	O
(	O
θc	O
(	O
a	O
=	O
0	O
,	O
s	O
=	O
1	O
)	O
)	O
=	O
b	O
(	O
cid:33	O
)	O
q	O
(	O
an	O
=	O
0	O
(	O
cid:33	O
)	O
i	O
[	O
sn	O
=	O
0	O
]	O
n	O
q	O
(	O
an	O
=	O
1	O
)	O
,	O
β	O
+	O
(	O
cid:88	O
)	O
i	O
[	O
sn	O
=	O
1	O
]	O
,	O
β	O
+	O
(	O
cid:88	O
)	O
(	O
cid:32	O
)	O
θc|α	O
+	O
(	O
cid:88	O
)	O
n	O
n	O
i	O
[	O
sn	O
=	O
1	O
]	O
q	O
(	O
an	O
=	O
0	O
)	O
,	O
β	O
+	O
(	O
cid:88	O
)	O
n	O
(	O
11.5.28	O
)	O
(	O
11.5.29	O
)	O
(	O
11.5.30	O
)	O
(	O
cid:33	O
)	O
i	O
[	O
sn	O
=	O
0	O
]	O
q	O
(	O
an	O
=	O
1	O
)	O
since	O
the	O
posterior	B
approximation	O
is	O
then	O
also	O
a	O
beta	B
distribution	O
:	O
and	O
four	O
tables	O
,	O
one	O
for	O
each	O
of	O
the	O
parental	O
states	O
of	O
c.	O
for	O
example	O
these	O
are	O
reminiscent	O
of	O
the	O
standard	O
bayesian	O
equations	O
,	O
equation	B
(	O
9.3.17	O
)	O
except	O
that	O
the	O
counts	O
have	O
been	O
replaced	O
by	O
q	O
’	O
s	O
.	O
e-step	O
we	O
still	O
need	O
to	O
determine	O
q	O
(	O
an	O
)	O
.	O
the	O
optimal	O
value	B
is	O
given	O
by	O
minimising	O
the	O
kullback-leibler	O
divergence	B
with	O
respect	O
to	O
q	O
(	O
an	O
)	O
.	O
this	O
gives	O
the	O
solution	O
that	O
optimally	O
,	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
cn|sn	O
,	O
an	O
,	O
θc	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
θc	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
an|θa	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
θa	O
)	O
q	O
(	O
an	O
)	O
∝	O
e	O
for	O
example	O
,	O
if	O
assume	O
that	O
for	O
datapoint	O
n	O
,	O
s	O
is	O
in	O
state	O
1	O
and	O
c	O
in	O
state	O
0	O
,	O
then	O
(	O
cid:104	O
)	O
log	O
(	O
1−θc	O
(	O
s=1	O
,	O
a=1	O
)	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
θc	O
(	O
s=1	O
,	O
a=1	O
)	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
θa	O
(	O
cid:105	O
)	O
q	O
(	O
θa	O
)	O
q	O
(	O
an	O
=	O
1	O
)	O
∝	O
e	O
and	O
(	O
11.5.31	O
)	O
(	O
11.5.32	O
)	O
(	O
cid:104	O
)	O
log	O
(	O
1−θc	O
(	O
s=1	O
,	O
a=0	O
)	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
θc	O
(	O
s=1	O
,	O
a=1	O
)	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
(	O
1−θa	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
θa	O
)	O
q	O
(	O
an	O
=	O
0	O
)	O
∝	O
e	O
(	O
11.5.33	O
)	O
to	O
compute	O
such	O
quantities	O
explicitly	O
,	O
we	O
need	O
the	O
values	O
(	O
cid:104	O
)	O
log	O
θ	O
(	O
cid:105	O
)	O
b	O
(	O
θ|α	O
,	O
β	O
)	O
and	O
(	O
cid:104	O
)	O
log	O
(	O
1	O
−	O
θ	O
)	O
(	O
cid:105	O
)	O
b	O
(	O
θ|α	O
,	O
β	O
)	O
.	O
for	O
a	O
beta	B
distribution	O
,	O
these	O
are	O
straightforward	O
to	O
compute	O
,	O
see	O
exercise	O
(	O
95	O
)	O
.	O
the	O
complete	O
vb	O
procedure	O
is	O
then	O
given	O
by	O
iterating	O
equations	O
(	O
11.5.28,11.5.29,11.5.30	O
)	O
and	O
(	O
11.5.32,11.5.33	O
)	O
until	O
convergence	O
.	O
given	O
a	O
converged	O
factorised	B
approximation	O
,	O
computing	O
a	O
marginal	B
table	O
p	O
(	O
a	O
=	O
1|v	O
)	O
is	O
then	O
straightforward	O
under	O
the	O
approximation	B
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
p	O
(	O
a	O
=	O
1|v	O
)	O
≈	O
q	O
(	O
a	O
=	O
1|θa	O
)	O
q	O
(	O
θa|v	O
)	O
=	O
θa	O
θa	O
θaq	O
(	O
θa|v	O
)	O
(	O
11.5.34	O
)	O
since	O
q	O
(	O
θa|v	O
)	O
is	O
a	O
beta	B
distribution	O
b	O
(	O
θa|α	O
,	O
β	O
)	O
,	O
the	O
mean	B
is	O
straightforward	O
.	O
using	O
this	O
for	O
both	O
states	O
of	O
a	O
leads	O
to	O
α	O
+	O
(	O
cid:80	O
)	O
n	O
q	O
(	O
an	O
=	O
0	O
)	O
+	O
β	O
+	O
(	O
cid:80	O
)	O
n	O
q	O
(	O
an	O
=	O
1	O
)	O
α	O
+	O
(	O
cid:80	O
)	O
n	O
q	O
(	O
an	O
=	O
1	O
)	O
p	O
(	O
a	O
=	O
1|v	O
)	O
=	O
(	O
11.5.35	O
)	O
the	O
application	O
of	O
vb	O
to	O
learning	B
the	O
tables	O
in	O
arbitrarily	O
structured	B
bns	O
is	O
a	O
straightforward	O
extension	O
of	O
the	O
technique	O
outlined	O
here	O
.	O
under	O
the	O
factorised	B
approximation	O
,	O
q	O
(	O
h	O
,	O
θ	O
)	O
=	O
q	O
(	O
h	O
)	O
q	O
(	O
θ	O
)	O
,	O
one	O
will	O
always	O
obtain	O
a	O
simple	O
updating	O
equation	B
analogous	O
to	O
the	O
full	O
data	B
case	O
,	O
but	O
with	O
the	O
missing	B
data	I
replaced	O
by	O
variational	O
approximations	O
.	O
nevertheless	O
,	O
if	O
a	O
variable	B
has	O
many	O
missing	B
parents	O
,	O
the	O
number	O
of	O
states	O
in	O
the	O
average	B
with	O
respect	O
to	O
the	O
q	O
distribution	B
can	O
become	O
intractable	O
,	O
and	O
further	O
constraints	O
on	O
the	O
form	O
of	O
the	O
approximation	B
,	O
or	O
additional	O
bounds	O
are	O
required	O
.	O
one	O
may	O
readily	O
extend	O
the	O
above	O
to	O
the	O
case	O
of	O
dirichlet	O
distributions	O
on	O
multinomial	B
variables	O
,	O
see	O
exercise	O
(	O
142	O
)	O
.	O
indeed	O
,	O
the	O
extension	O
to	O
the	O
exponential	B
family	I
is	O
straightforward	O
.	O
draft	O
march	O
9	O
,	O
2010	O
235	O
optimising	O
the	O
likelihood	B
by	O
gradient	B
methods	O
figure	O
11.7	O
:	O
(	O
a	O
)	O
:	O
standard	O
ml	O
learning	B
.	O
the	O
best	O
parameter	B
θ	O
is	O
found	O
by	O
maximising	O
the	O
probability	B
that	O
the	O
model	B
generates	O
the	O
observed	O
data	O
(	O
b	O
)	O
:	O
ml-ii	O
learning	B
.	O
in	O
cases	O
where	O
we	O
have	O
a	O
θopt	O
=	O
arg	O
maxθ	O
p	O
(	O
v|θ	O
)	O
.	O
prior	B
preference	O
for	O
the	O
parameters	O
θ	O
,	O
but	O
with	O
unspeciﬁed	O
hyperparameter	B
θ	O
(	O
cid:48	O
)	O
,	O
we	O
can	O
ﬁnd	O
θ	O
(	O
cid:48	O
)	O
by	O
θ	O
(	O
cid:48	O
)	O
opt	O
=	O
arg	O
maxθ	O
(	O
cid:48	O
)	O
p	O
(	O
v|θ	O
(	O
cid:48	O
)	O
)	O
=	O
arg	O
maxθ	O
(	O
cid:48	O
)	O
(	O
cid:104	O
)	O
p	O
(	O
v|θ	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
θ|θ	O
(	O
cid:48	O
)	O
)	O
.	O
θ	O
(	O
cid:48	O
)	O
θ	O
v	O
(	O
b	O
)	O
θ	O
v	O
(	O
a	O
)	O
11.6	O
bayesian	O
methods	O
and	O
ml-ii	O
consider	O
a	O
parameterised	O
distribution	B
p	O
(	O
v|θ	O
)	O
,	O
for	O
which	O
we	O
wish	O
to	O
the	O
learn	O
the	O
optimal	O
parameters	O
θ	O
given	O
some	O
data	B
.	O
the	O
model	B
p	O
(	O
v|θ	O
)	O
is	O
depicted	O
in	O
ﬁg	O
(	O
11.7a	O
)	O
,	O
where	O
a	O
dot	O
indicates	O
that	O
no	O
distribution	O
is	O
present	O
on	O
that	O
variable	B
.	O
for	O
a	O
single	O
observed	O
datapoint	O
v	O
,	O
setting	O
θ	O
by	O
maximum	B
likelihood	I
corresponds	O
to	O
ﬁnding	O
the	O
parameter	B
θ	O
that	O
maximises	O
p	O
(	O
v|θ	O
)	O
.	O
in	O
some	O
cases	O
we	O
may	O
have	O
an	O
idea	O
about	O
which	O
parameters	O
θ	O
are	O
more	O
appropriate	O
and	O
can	O
express	O
this	O
prior	B
preference	O
using	O
a	O
distribution	B
p	O
(	O
θ	O
)	O
.	O
if	O
the	O
prior	B
were	O
fully	O
speciﬁed	O
,	O
then	O
there	O
is	O
nothing	O
to	O
‘	O
learn	O
’	O
since	O
p	O
(	O
θ|v	O
)	O
is	O
now	O
fully	O
known	O
.	O
however	O
,	O
in	O
many	O
cases	O
in	O
practice	O
,	O
we	O
are	O
unsure	O
of	O
the	O
exact	O
parameter	O
settings	O
of	O
the	O
prior	B
,	O
and	O
hence	O
specify	O
a	O
parametersised	O
prior	B
using	O
a	O
distribution	B
p	O
(	O
θ|θ	O
(	O
cid:48	O
)	O
)	O
with	O
hyperparameter	O
θ	O
(	O
cid:48	O
)	O
.	O
this	O
is	O
depicted	O
in	O
ﬁg	O
(	O
11.7b	O
)	O
.	O
the	O
learning	B
corresponds	O
to	O
ﬁnding	O
the	O
optimal	O
θ	O
(	O
cid:48	O
)	O
θ	O
p	O
(	O
v|θ	O
)	O
p	O
(	O
θ|θ	O
(	O
cid:48	O
)	O
)	O
.	O
this	O
is	O
known	O
as	O
an	O
ml-ii	O
procedure	O
since	O
it	O
corresponds	O
to	O
maximum	B
likelihood	I
,	O
but	O
at	O
the	O
higher	O
,	O
hyperparameter	B
level	O
[	O
33	O
,	O
183	O
]	O
.	O
this	O
is	O
a	O
form	O
of	O
approximate	B
bayesian	O
analysis	B
since	O
,	O
although	O
θ	O
(	O
cid:48	O
)	O
is	O
set	O
using	O
maximum	B
likelihood	I
,	O
after	O
training	B
,	O
we	O
have	O
a	O
distribution	B
over	O
parameters	O
,	O
p	O
(	O
θ|v	O
,	O
θ	O
(	O
cid:48	O
)	O
)	O
.	O
11.7	O
optimising	O
the	O
likelihood	B
by	O
gradient	B
methods	O
that	O
maximises	O
the	O
likelihood	B
p	O
(	O
v|θ	O
(	O
cid:48	O
)	O
)	O
=	O
(	O
cid:82	O
)	O
11.7.1	O
directed	B
models	O
the	O
em	O
algorithm	B
typically	O
works	O
well	O
when	O
the	O
amount	O
of	O
missing	B
information	O
is	O
small	O
compared	O
to	O
the	O
complete	O
information	O
.	O
in	O
this	O
case	O
em	O
exhibits	O
approximately	O
the	O
same	O
convergence	O
as	O
newton	O
based	O
gradient	B
method	O
[	O
237	O
]	O
.	O
however	O
,	O
if	O
the	O
fraction	O
of	O
missing	B
information	O
approaches	O
unity	O
,	O
em	O
can	O
converge	O
very	O
slowly	O
.	O
in	O
the	O
case	O
of	O
continuous	B
parameters	O
θ	O
,	O
an	O
alternative	O
is	O
to	O
compute	O
the	O
gradient	B
of	O
the	O
likelihood	B
directly	O
and	O
use	O
this	O
as	O
part	O
of	O
a	O
standard	O
continuous	O
variable	B
optimisation	O
routine	O
.	O
the	O
gradient	B
is	O
straightforward	O
to	O
compute	O
using	O
the	O
following	O
identity	O
.	O
consider	O
the	O
log	O
likelihood	B
l	O
(	O
θ	O
)	O
=	O
log	O
p	O
(	O
v|θ	O
)	O
the	O
derivative	O
can	O
be	O
written	O
(	O
11.7.1	O
)	O
(	O
11.7.2	O
)	O
(	O
11.7.3	O
)	O
∂θl	O
(	O
θ	O
)	O
=	O
p	O
(	O
v	O
,	O
h|θ	O
)	O
at	O
this	O
point	O
,	O
we	O
take	O
the	O
derivative	O
inside	O
the	O
integral	O
p	O
(	O
v|θ	O
)	O
∂θp	O
(	O
v|θ	O
)	O
=	O
h	O
1	O
1	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
1	O
p	O
(	O
v|θ	O
)	O
∂θ	O
(	O
cid:90	O
)	O
∂θl	O
(	O
θ	O
)	O
=	O
p	O
(	O
v|θ	O
)	O
h	O
∂θp	O
(	O
v	O
,	O
h|θ	O
)	O
=	O
h	O
p	O
(	O
h|v	O
,	O
θ	O
)	O
∂θ	O
log	O
p	O
(	O
v	O
,	O
h|θ	O
)	O
=	O
(	O
cid:104	O
)	O
∂θ	O
log	O
p	O
(	O
v	O
,	O
h|θ	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
h|v	O
,	O
θ	O
)	O
where	O
we	O
used	O
∂	O
log	O
f	O
(	O
x	O
)	O
=	O
(	O
1/f	O
(	O
x	O
)	O
)	O
∂f	O
(	O
x	O
)	O
.	O
the	O
right	O
hand	O
side	O
is	O
the	O
average	B
of	O
the	O
derivative	O
of	O
the	O
log	O
complete	O
likelihood	B
.	O
this	O
is	O
closely	O
related	O
to	O
the	O
derivative	O
of	O
the	O
energy	B
term	O
in	O
the	O
em	O
algorithm	B
,	O
though	O
note	O
that	O
the	O
average	B
here	O
is	O
performed	O
with	O
respect	O
the	O
current	O
distribution	B
parameters	O
θ	O
and	O
not	O
θold	O
as	O
in	O
the	O
em	O
case	O
.	O
used	O
in	O
this	O
way	O
,	O
computing	O
the	O
derivatives	O
of	O
latent	B
variable	I
models	O
is	O
relatively	O
straightforward	O
.	O
these	O
derivatives	O
may	O
then	O
be	O
used	O
as	O
part	O
of	O
a	O
standard	O
optimisation	O
routine	O
such	O
as	O
conjugate	O
gradients	O
[	O
237	O
]	O
.	O
236	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
11.7.2	O
undirected	B
models	O
consider	O
an	O
undirected	B
model	I
which	O
contains	O
both	O
hidden	B
and	O
visible	B
variables	O
p	O
(	O
v	O
,	O
h|θ	O
)	O
=	O
1	O
z	O
(	O
θ	O
)	O
eφ	O
(	O
v	O
,	O
h	O
)	O
for	O
i.i.d	O
.	O
data	B
,	O
the	O
log	O
likelihood	B
on	O
the	O
visible	B
variables	O
is	O
(	O
assuming	O
discrete	B
v	O
and	O
h	O
)	O
which	O
has	O
gradient	B
l	O
(	O
θ	O
)	O
=	O
(	O
cid:88	O
)	O
n	O
l	O
=	O
(	O
cid:88	O
)	O
n	O
∂	O
∂θ	O
h	O
log	O
(	O
cid:88	O
)	O
	O
(	O
cid:28	O
)	O
∂	O
(	O
cid:124	O
)	O
∂θ	O
eφ	O
(	O
vn	O
,	O
h|θ	O
)	O
−	O
log	O
(	O
cid:88	O
)	O
h	O
,	O
v	O
	O
eφ	O
(	O
v	O
,	O
h|θ	O
)	O
(	O
cid:29	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
φ	O
(	O
vn	O
,	O
h|θ	O
)	O
clamped	O
average	B
p	O
(	O
h|vn	O
)	O
(	O
cid:28	O
)	O
∂	O
(	O
cid:124	O
)	O
∂θ	O
−	O
(	O
cid:29	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
φ	O
(	O
v	O
,	O
h|θ	O
)	O
free	O
average	B
	O
(	O
cid:125	O
)	O
p	O
(	O
h	O
,	O
v	O
)	O
(	O
cid:125	O
)	O
(	O
11.7.4	O
)	O
(	O
11.7.5	O
)	O
(	O
11.7.6	O
)	O
for	O
a	O
markov	O
network	O
that	O
is	O
intractable	O
(	O
the	O
partition	B
function	I
z	O
can	O
not	O
be	O
computed	O
eﬃciently	O
)	O
,	O
the	O
gradient	B
is	O
particularly	O
diﬃcult	O
to	O
estimate	O
since	O
it	O
is	O
the	O
diﬀerence	O
of	O
two	O
quantities	O
,	O
each	O
of	O
which	O
needs	O
to	O
be	O
estimated	O
.	O
even	O
getting	O
the	O
sign	O
of	O
the	O
gradient	B
correct	O
can	O
therefore	O
be	O
computationally	O
diﬃcult	O
.	O
for	O
this	O
reason	O
learning	B
in	O
models	O
,	O
such	O
as	O
the	O
boltzmann	O
machine	O
with	O
hidden	O
units	O
,	O
is	O
particularly	O
diﬃcult	O
.	O
11.8	O
code	O
in	O
the	O
demo	O
code	O
we	O
take	O
the	O
original	O
chest	B
clinic	I
network	O
[	O
170	O
]	O
and	O
draw	O
data	B
samples	O
from	O
this	O
network	O
.	O
our	O
interest	O
is	O
then	O
to	O
see	O
if	O
we	O
can	O
use	O
the	O
em	O
algorithm	B
to	O
estimate	O
the	O
tables	O
based	O
on	O
the	O
data	B
(	O
with	O
some	O
parts	O
of	O
the	O
data	B
missing	O
at	O
random	O
)	O
.	O
we	O
assume	O
that	O
we	O
know	O
the	O
correct	O
bn	O
structure	B
,	O
only	O
that	O
the	O
cpts	O
are	O
unknown	O
.	O
we	O
assume	O
the	O
logic	B
gate	O
table	O
is	O
known	O
,	O
so	O
we	O
do	O
not	O
need	O
to	O
learn	O
this	O
.	O
demoemchestclinic.m	O
:	O
demo	O
of	O
em	O
in	O
learning	B
the	O
chest	B
clinic	I
tables	O
the	O
following	O
code	O
implements	O
maximum	B
likelihood	I
learning	O
of	O
bn	O
tables	O
based	O
on	O
data	B
with	O
possibly	O
missing	B
values	O
.	O
embeliefnet.m	O
:	O
em	O
training	B
of	O
a	O
belief	B
network	I
11.9	O
exercises	O
exercise	O
138	O
(	O
printer	B
nightmare	I
continued	O
)	O
.	O
continuing	O
with	O
the	O
bn	O
given	O
in	O
ﬁg	O
(	O
9.19	O
)	O
,	O
the	O
following	O
table	O
represents	O
data	B
gathered	O
on	O
the	O
printer	O
,	O
where	O
?	O
indicates	O
that	O
the	O
entry	O
is	O
missing	B
.	O
each	O
column	O
represents	O
a	O
datapoint	O
.	O
use	O
the	O
em	O
algorithm	B
to	O
learn	O
all	O
cpts	O
of	O
the	O
network	O
.	O
fuse	O
assembly	O
malfunction	O
drum	O
unit	O
toner	O
out	O
poor	O
paper	O
quality	O
worn	O
roller	O
burning	O
smell	O
poor	O
print	O
quality	O
wrinkled	O
pages	O
multiple	O
pages	O
fed	O
paper	O
jam	O
?	O
?	O
1	O
1	O
0	O
0	O
1	O
0	O
0	O
?	O
?	O
0	O
1	O
0	O
0	O
?	O
1	O
0	O
?	O
0	O
?	O
?	O
0	O
1	O
?	O
?	O
1	O
1	O
1	O
1	O
1	O
0	O
?	O
0	O
?	O
1	O
0	O
0	O
0	O
1	O
0	O
1	O
?	O
1	O
?	O
0	O
1	O
0	O
?	O
?	O
0	O
0	O
1	O
?	O
0	O
0	O
1	O
0	O
0	O
0	O
?	O
0	O
0	O
1	O
1	O
0	O
0	O
?	O
1	O
1	O
0	O
1	O
1	O
0	O
?	O
0	O
1	O
0	O
0	O
1	O
?	O
?	O
0	O
1	O
0	O
0	O
0	O
1	O
1	O
1	O
0	O
?	O
?	O
1	O
0	O
?	O
0	O
?	O
?	O
1	O
0	O
1	O
0	O
?	O
?	O
0	O
1	O
0	O
0	O
0	O
?	O
1	O
1	O
1	O
0	O
?	O
1	O
0	O
0	O
?	O
1	O
?	O
?	O
1	O
?	O
1	O
?	O
1	O
?	O
0	O
?	O
0	O
0	O
?	O
1	O
0	O
?	O
1	O
0	O
1	O
1	O
0	O
?	O
0	O
1	O
?	O
0	O
1	O
1	O
?	O
draft	O
march	O
9	O
,	O
2010	O
237	O
the	O
table	O
is	O
contained	O
in	O
emprinter.mat	O
,	O
using	O
states	O
1	O
,	O
2	O
,	O
nan	O
in	O
place	O
of	O
0	O
,	O
1	O
,	O
?	O
(	O
since	O
brmltoolbox	O
requires	O
states	O
to	O
be	O
numbered	O
1,2	O
,	O
...	O
.	O
)	O
.	O
given	O
no	O
wrinkled	O
pages	O
,	O
no	O
burning	O
smell	O
and	O
poor	O
print	O
quality	O
,	O
what	O
is	O
the	O
probability	B
there	O
is	O
a	O
drum	O
unit	O
problem	O
?	O
exercise	O
139.	O
consider	O
the	O
following	O
distribution	B
over	O
discrete	B
variables	O
,	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
,	O
x5	O
)	O
=	O
p	O
(	O
x1|x2	O
)	O
p	O
(	O
x2|x3	O
)	O
p	O
(	O
x3|x4	O
)	O
p	O
(	O
x4|x5	O
)	O
p	O
(	O
x5	O
)	O
,	O
(	O
11.9.1	O
)	O
exercises	O
in	O
which	O
the	O
variables	O
x2	O
and	O
x4	O
are	O
consistently	O
hidden	B
in	O
the	O
training	B
data	O
,	O
and	O
training	B
data	O
for	O
x1	O
,	O
x3	O
,	O
x5	O
are	O
always	O
present	O
.	O
show	O
that	O
the	O
em	O
update	O
for	O
the	O
table	O
p	O
(	O
x1|x2	O
)	O
is	O
given	O
by	O
pnew	O
(	O
x1	O
=	O
i|x2	O
=	O
j	O
)	O
=	O
i	O
[	O
xn	O
i	O
[	O
xn	O
n	O
n	O
,	O
k	O
1	O
=	O
i	O
]	O
pold	O
(	O
x2	O
=	O
j|xn	O
1	O
=	O
k	O
]	O
pold	O
(	O
x2	O
=	O
j|xn	O
1	O
,	O
xn	O
1	O
,	O
xn	O
5	O
)	O
3	O
,	O
xn	O
5	O
)	O
3	O
,	O
xn	O
exercise	O
140.	O
consider	O
a	O
simple	O
two	O
variable	B
bn	O
p	O
(	O
y	O
,	O
x	O
)	O
=	O
p	O
(	O
y|x	O
)	O
p	O
(	O
x	O
)	O
(	O
11.9.3	O
)	O
where	O
both	O
y	O
and	O
x	O
are	O
binary	O
variables	O
,	O
dom	O
(	O
x	O
)	O
=	O
{	O
1	O
,	O
2	O
}	O
,	O
dom	O
(	O
y	O
)	O
=	O
{	O
1	O
,	O
2	O
}	O
.	O
you	O
have	O
a	O
set	O
of	O
training	B
data	O
{	O
(	O
yn	O
,	O
xn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
,	O
in	O
which	O
for	O
some	O
cases	O
xn	O
may	O
be	O
missing	B
.	O
we	O
are	O
speciﬁcally	O
interested	O
in	O
learning	B
the	O
table	O
p	O
(	O
x	O
)	O
from	O
this	O
data	B
.	O
a	O
colleague	O
suggests	O
that	O
one	O
can	O
set	O
p	O
(	O
x	O
)	O
by	O
simply	O
looking	O
at	O
datapoints	O
where	O
x	O
is	O
observed	O
,	O
and	O
then	O
setting	O
p	O
(	O
x	O
=	O
1	O
)	O
to	O
be	O
the	O
fraction	O
of	O
observed	O
x	O
that	O
is	O
in	O
state	O
1.	O
explain	O
how	O
this	O
suggested	O
procedure	O
relates	O
to	O
maximum	B
likelihood	I
and	O
em	O
.	O
exercise	O
141.	O
assume	O
that	O
a	O
sequence	O
is	O
generated	O
by	O
a	O
markov	O
chain	B
.	O
for	O
a	O
single	O
chain	B
of	O
length	O
t	O
,	O
we	O
have	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
t−1	O
(	O
cid:89	O
)	O
t=1	O
(	O
11.9.2	O
)	O
(	O
11.9.4	O
)	O
(	O
11.9.5	O
)	O
(	O
11.9.6	O
)	O
p	O
(	O
v1	O
,	O
.	O
.	O
.	O
,	O
vt	O
)	O
=	O
p	O
(	O
v1	O
)	O
p	O
(	O
vt+1|vt	O
)	O
for	O
simplicity	O
,	O
we	O
denote	O
the	O
sequence	O
of	O
visible	B
variables	O
as	O
v	O
=	O
(	O
v1	O
,	O
.	O
.	O
.	O
,	O
vt	O
)	O
for	O
a	O
single	O
markov	O
chain	B
labelled	O
by	O
h	O
,	O
t−1	O
(	O
cid:89	O
)	O
t=1	O
p	O
(	O
v|h	O
)	O
=	O
p	O
(	O
v1|h	O
)	O
p	O
(	O
vt+1|vt	O
,	O
h	O
)	O
h	O
(	O
cid:88	O
)	O
h=1	O
in	O
total	O
there	O
are	O
a	O
set	O
of	O
h	O
such	O
markov	O
chains	O
(	O
h	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
h	O
)	O
.	O
the	O
distribution	B
on	O
the	O
visible	B
variables	O
is	O
therefore	O
p	O
(	O
v	O
)	O
=	O
p	O
(	O
v|h	O
)	O
p	O
(	O
h	O
)	O
(	O
11.9.7	O
)	O
1.	O
there	O
are	O
a	O
set	O
of	O
training	B
sequences	O
,	O
vn	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
assuming	O
that	O
each	O
sequence	O
vn	O
is	O
inde-	O
pendently	O
and	O
identically	O
drawn	O
from	O
a	O
markov	O
chain	B
mixture	O
model	B
with	O
h	O
components	O
,	O
derive	O
the	O
expectation	B
maximisation	I
algorithm	O
for	O
training	B
this	O
model	B
.	O
2.	O
write	O
a	O
general	O
matlab	O
function	B
in	O
the	O
form	O
function	B
[	O
q	O
,	O
ph	O
,	O
pv	O
,	O
a	O
]	O
=mchain_mix	O
(	O
v	O
,	O
v	O
,	O
h	O
,	O
num_em_loops	O
)	O
to	O
perform	O
em	O
learning	B
for	O
any	O
set	O
of	O
(	O
the	O
same	O
length	O
)	O
sequences	B
of	O
integers	O
vn	O
t	O
∈	O
[	O
1	O
:	O
v	O
]	O
,	O
t	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
t	O
.	O
v	O
is	O
a	O
cell	O
array	O
of	O
the	O
training	B
data	O
:	O
v	O
{	O
2	O
}	O
(	O
4	O
)	O
is	O
the	O
4th	O
time	O
element	O
of	O
the	O
sec-	O
ond	O
training	B
sequence	O
.	O
each	O
element	O
,	O
say	O
v	O
{	O
2	O
}	O
(	O
4	O
)	O
must	O
be	O
an	O
integer	O
from	O
1	O
to	O
v	O
.	O
v	O
is	O
the	O
number	O
of	O
states	O
of	O
the	O
visible	B
variables	O
(	O
in	O
the	O
bio-sequence	O
case	O
below	O
,	O
this	O
will	O
be	O
4	O
)	O
.	O
h	O
is	O
the	O
238	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
number	O
of	O
mixture	B
components	O
.	O
num_em_loops	O
is	O
the	O
number	O
of	O
em	O
iterations	O
.	O
a	O
is	O
the	O
transi-	O
tion	O
matrix	B
a	O
{	O
h	O
}	O
(	O
i	O
,	O
j	O
)	O
=p	O
(	O
v	O
(	O
t+1	O
)	O
=i|v	O
(	O
t	O
)	O
=j	O
,	O
h	O
)	O
.	O
pv	O
is	O
the	O
prior	B
state	O
of	O
the	O
ﬁrst	O
visible	O
variable	B
,	O
pv	O
{	O
h	O
}	O
(	O
i	O
)	O
=p	O
(	O
v	O
(	O
t=1	O
)	O
=i|h	O
)	O
.	O
ph	O
is	O
a	O
vector	O
of	O
prior	B
probabilities	O
for	O
the	O
mixture	B
state	O
ph	O
(	O
h	O
)	O
=p	O
(	O
h	O
)	O
.	O
q	O
is	O
the	O
cell	O
array	O
of	O
posterior	B
probabilities	O
q	O
{	O
mu	O
}	O
(	O
h	O
)	O
=p	O
(	O
h|v	O
{	O
mu	O
}	O
)	O
.	O
your	O
routine	O
must	O
also	O
display	O
,	O
for	O
each	O
em	O
iteration	B
,	O
the	O
value	B
of	O
the	O
log	O
likelihood	B
.	O
as	O
a	O
check	B
on	O
your	O
routine	O
,	O
the	O
log	O
likelihood	B
must	O
increase	O
at	O
each	O
iteration	B
.	O
3.	O
the	O
ﬁle	O
sequences.mat	O
contains	O
a	O
set	O
of	O
ﬁctitious	O
bio-sequence	O
in	O
a	O
cell	O
array	O
sequences	B
{	O
mu	O
}	O
(	O
t	O
)	O
.	O
thus	O
sequences	B
{	O
3	O
}	O
(	O
:	O
)	O
is	O
the	O
third	O
sequence	O
,	O
gtctcctgccctctctgaac	O
which	O
consists	O
of	O
20	O
timesteps	O
.	O
there	O
are	O
20	O
such	O
sequences	B
in	O
total	O
.	O
your	O
task	O
is	O
to	O
cluster	O
these	O
sequences	B
into	O
two	O
clusters	O
,	O
assuming	O
that	O
each	O
cluster	O
is	O
modelled	O
by	O
a	O
markov	O
chain	B
.	O
state	O
which	O
of	O
the	O
sequences	B
belong	O
together	O
by	O
assigning	O
a	O
sequence	O
vn	O
to	O
that	O
state	O
for	O
which	O
p	O
(	O
h|vn	O
)	O
is	O
highest	O
.	O
exercise	O
142.	O
write	O
a	O
general	O
purpose	O
routine	O
vbbeliefnet	O
(	O
pot	O
,	O
x	O
,	O
pars	O
)	O
along	O
the	O
lines	O
of	O
embeliefnet.m	O
that	O
performs	O
variational	O
bayes	O
under	O
a	O
dirichlet	O
prior	B
,	O
using	O
a	O
factorised	B
parameter	O
approximation	B
.	O
as-	O
sume	O
both	O
global	B
and	O
local	B
parameter	O
independence	B
for	O
the	O
prior	B
and	O
the	O
approximation	B
q	O
,	O
section	O
(	O
9.3.1	O
)	O
.	O
exercise	O
143.	O
consider	O
a	O
3	O
‘	O
layered	O
’	O
boltzmann	O
machine	O
which	O
has	O
the	O
form	O
p	O
(	O
v	O
,	O
h1	O
,	O
h2	O
,	O
h3|θ	O
)	O
=	O
1	O
z	O
φ	O
(	O
v	O
,	O
h1|θ1	O
)	O
φ	O
(	O
h1	O
,	O
h2|θ2	O
)	O
φ	O
(	O
h2	O
,	O
h3|θ3	O
)	O
(	O
11.9.8	O
)	O
where	O
dim	O
v	O
=	O
dim	O
h1	O
=	O
dim	O
h2	O
=	O
dim	O
h3	O
=	O
v	O
(	O
cid:80	O
)	O
v	O
φ	O
(	O
x	O
,	O
y|θ	O
)	O
=	O
e	O
all	O
variables	O
are	O
binary	O
with	O
states	O
0	O
,	O
1	O
and	O
the	O
parameters	O
for	O
each	O
layer	O
l	O
are	O
θl	O
=	O
(	O
cid:8	O
)	O
wl	O
,	O
al	O
,	O
bl	O
(	O
cid:9	O
)	O
.	O
i	O
,	O
j=1	O
wij	O
xiyj	O
+aij	O
xixj	O
+bij	O
yiyj	O
(	O
11.9.9	O
)	O
1.	O
in	O
terms	O
of	O
ﬁtting	O
the	O
model	B
to	O
visible	B
data	O
v1	O
,	O
.	O
.	O
.	O
,	O
vn	O
,	O
is	O
the	O
3	O
layered	O
model	B
above	O
any	O
more	O
powerful	O
than	O
ﬁtting	O
a	O
two-layered	O
model	B
(	O
the	O
factor	B
φ	O
(	O
h2	O
,	O
h3|θ3	O
)	O
is	O
not	O
present	O
in	O
the	O
two-layer	O
case	O
)	O
?	O
2.	O
if	O
we	O
use	O
a	O
restricted	B
potential	O
(	O
cid:80	O
)	O
φ	O
(	O
x	O
,	O
y|θ	O
)	O
=	O
e	O
i	O
,	O
j	O
wij	O
xiyj	O
(	O
11.9.10	O
)	O
is	O
the	O
three	O
layered	O
model	B
more	O
powerful	O
in	O
being	O
able	O
to	O
ﬁt	O
the	O
visible	B
data	O
than	O
the	O
two-layered	O
model	B
?	O
exercise	O
144.	O
the	O
sigmoid	B
belief	I
network	I
is	O
deﬁned	O
by	O
the	O
layered	O
network	O
p	O
(	O
xl	O
)	O
p	O
(	O
xl−1|xl	O
)	O
(	O
11.9.11	O
)	O
where	O
vector	O
variables	O
have	O
binary	O
components	O
xl	O
∈	O
{	O
0	O
,	O
1	O
}	O
wl	O
and	O
the	O
width	O
of	O
layer	O
l	O
is	O
given	O
by	O
wl	O
.	O
in	O
addition	O
l	O
(	O
cid:89	O
)	O
l=1	O
wl	O
(	O
cid:89	O
)	O
i=1	O
p	O
(	O
xl−1|xl	O
)	O
=	O
p	O
(	O
xl−1	O
i	O
|xl	O
)	O
and	O
p	O
(	O
xl−1	O
i	O
=	O
1|xl	O
)	O
=	O
σ	O
(	O
cid:16	O
)	O
i	O
,	O
lxl	O
(	O
cid:17	O
)	O
wt	O
,	O
σ	O
(	O
x	O
)	O
=	O
1/	O
(	O
1	O
+	O
e	O
−x	O
)	O
(	O
11.9.12	O
)	O
(	O
11.9.13	O
)	O
for	O
a	O
weight	B
vector	O
wi	O
,	O
l	O
describing	O
the	O
interaction	O
from	O
the	O
parental	O
layer	O
.	O
the	O
top	O
layer	O
,	O
p	O
(	O
xl	O
)	O
describes	O
a	O
factorised	B
distribution	O
p	O
(	O
xl	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
p	O
(	O
xl	O
wl	O
)	O
.	O
1.	O
draw	O
the	O
belief	B
network	I
structure	O
of	O
this	O
distribution	B
.	O
2.	O
for	O
the	O
layer	O
x0	O
,	O
what	O
is	O
the	O
computational	B
complexity	I
of	O
computing	O
the	O
likelihood	B
p	O
(	O
x0	O
)	O
,	O
assuming	O
that	O
all	O
layers	O
have	O
equal	O
width	O
w	O
?	O
draft	O
march	O
9	O
,	O
2010	O
239	O
3.	O
assuming	O
a	O
fully	O
factorised	B
approximation	O
for	O
an	O
equal	O
width	O
network	O
,	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xl|x0	O
)	O
≈	O
q	O
(	O
xl	O
i	O
)	O
l	O
(	O
cid:89	O
)	O
w	O
(	O
cid:89	O
)	O
l=1	O
i=1	O
exercises	O
(	O
11.9.14	O
)	O
write	O
down	O
the	O
energy	B
term	O
of	O
the	O
variational	O
em	O
procedure	O
for	O
a	O
single	O
data	B
observation	O
x0	O
,	O
and	O
discuss	O
the	O
tractability	O
of	O
computing	O
the	O
energy	B
.	O
exercise	O
146.	O
a	O
2	O
×	O
2	O
probability	B
table	O
,	O
p	O
(	O
x1	O
=	O
i	O
,	O
x2	O
=	O
j	O
)	O
=	O
θi	O
,	O
j	O
,	O
with	O
0	O
≤	O
θi	O
,	O
j	O
≤	O
1	O
,	O
(	O
cid:80	O
)	O
2	O
exercise	O
145.	O
show	O
how	O
to	O
ﬁnd	O
the	O
components	O
0	O
≤	O
(	O
θb	O
,	O
θg	O
,	O
θp	O
)	O
≤	O
1	O
that	O
maximise	O
equation	B
(	O
11.1.10	O
)	O
.	O
j=1	O
θi	O
,	O
j	O
=	O
1	O
is	O
learned	O
using	O
maximal	O
marginal	B
likelihood	I
in	O
which	O
x2	O
is	O
never	O
observed	O
.	O
show	O
that	O
if	O
(	O
cid:80	O
)	O
2	O
i=1	O
(	O
cid:18	O
)	O
0.3	O
0.3	O
(	O
cid:18	O
)	O
0.2	O
0.4	O
0.2	O
0.2	O
0.4	O
0	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
θ	O
=	O
θ	O
=	O
is	O
given	O
as	O
a	O
maximal	O
marginal	B
likelihood	I
solution	O
,	O
then	O
(	O
11.9.15	O
)	O
(	O
11.9.16	O
)	O
has	O
the	O
same	O
marginal	B
likelihood	I
score	O
.	O
240	O
draft	O
march	O
9	O
,	O
2010	O
chapter	O
12	O
bayesian	O
model	B
selection	I
12.1	O
comparing	O
models	O
the	O
bayesian	O
way	O
given	O
two	O
models	O
m1	O
and	O
m2	O
with	O
parameters	O
θ1	O
,	O
θ2	O
and	O
associated	O
parameter	B
priors	O
,	O
p	O
(	O
x	O
,	O
θ1|m1	O
)	O
=	O
p	O
(	O
x|θ1	O
,	O
m1	O
)	O
p	O
(	O
θ1|m1	O
)	O
,	O
p	O
(	O
x	O
,	O
θ2|m2	O
)	O
=	O
p	O
(	O
x|θ2	O
,	O
m2	O
)	O
p	O
(	O
θ2|m2	O
)	O
(	O
12.1.1	O
)	O
how	O
can	O
we	O
compare	O
the	O
performance	B
of	O
the	O
models	O
in	O
ﬁtting	O
a	O
set	O
of	O
data	B
d	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
?	O
the	O
application	O
of	O
bayes	O
’	O
rule	O
to	O
models	O
gives	O
a	O
framework	O
for	O
answering	O
questions	O
like	O
this	O
–	O
a	O
form	O
of	O
bayesian	O
hypothesis	B
testing	I
,	O
applied	O
at	O
the	O
model	B
level	O
.	O
more	O
generally	O
,	O
given	O
an	O
indexed	O
set	O
of	O
models	O
m1	O
,	O
.	O
.	O
.	O
,	O
mm	O
,	O
and	O
associated	O
prior	B
beliefs	O
in	O
the	O
appropriateness	O
of	O
each	O
model	B
p	O
(	O
mi	O
)	O
,	O
our	O
interest	O
is	O
the	O
model	B
posterior	O
probability	B
p	O
(	O
mi|d	O
)	O
=	O
p	O
(	O
d|mi	O
)	O
p	O
(	O
mi	O
)	O
p	O
(	O
d	O
)	O
where	O
p	O
(	O
d	O
)	O
=	O
m	O
(	O
cid:88	O
)	O
i=1	O
p	O
(	O
d|mi	O
)	O
p	O
(	O
mi	O
)	O
(	O
cid:90	O
)	O
model	B
mi	O
is	O
parameterised	O
by	O
θi	O
,	O
and	O
the	O
model	B
likelihood	O
is	O
given	O
by	O
p	O
(	O
d|mi	O
)	O
=	O
p	O
(	O
d|θi	O
,	O
mi	O
)	O
p	O
(	O
θi|mi	O
)	O
dθi	O
(	O
12.1.2	O
)	O
(	O
12.1.3	O
)	O
(	O
12.1.4	O
)	O
in	O
discrete	B
parameter	O
spaces	O
,	O
the	O
integral	O
is	O
replaced	O
with	O
summation	O
.	O
note	O
that	O
the	O
number	O
of	O
parameters	O
dim	O
(	O
θi	O
)	O
need	O
not	O
be	O
the	O
same	O
for	O
each	O
model	B
.	O
a	O
point	O
of	O
caution	O
here	O
is	O
that	O
p	O
(	O
mi|d	O
)	O
only	O
refers	O
to	O
the	O
probability	B
relative	O
to	O
the	O
set	O
of	O
models	O
speciﬁed	O
m1	O
,	O
.	O
.	O
.	O
,	O
mm	O
.	O
this	O
is	O
not	O
the	O
absolute	O
probability	B
that	O
model	B
m	O
ﬁts	O
‘	O
well	O
’	O
.	O
to	O
compute	O
such	O
a	O
quantity	O
would	O
require	O
one	O
to	O
specify	O
all	O
possible	O
models	O
.	O
whilst	O
interpreting	O
the	O
posterior	B
p	O
(	O
mi|d	O
)	O
requires	O
some	O
care	O
,	O
comparing	O
two	O
competing	O
model	B
hypotheses	O
mi	O
and	O
mj	O
is	O
straightforward	O
and	O
only	O
requires	O
the	O
bayes	O
’	O
factor	B
p	O
(	O
mi|d	O
)	O
p	O
(	O
mj|d	O
)	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
=	O
p	O
(	O
d|mi	O
)	O
p	O
(	O
d|mj	O
)	O
bayes	O
’	O
factor	B
p	O
(	O
mi	O
)	O
p	O
(	O
mj	O
)	O
which	O
does	O
not	O
require	O
integration/summation	O
over	O
all	O
possible	O
models	O
.	O
241	O
(	O
12.1.5	O
)	O
illustrations	O
:	O
coin	O
tossing	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
12.1	O
:	O
(	O
a	O
)	O
:	O
discrete	B
prior	O
model	B
of	O
a	O
‘	O
fair	O
’	O
coin	O
.	O
(	O
b	O
)	O
:	O
prior	B
for	O
a	O
biased	O
‘	O
unfair	O
’	O
coin	O
.	O
in	O
both	O
cases	O
we	O
are	O
making	O
explicit	O
choices	O
here	O
about	O
what	O
we	O
consider	O
to	O
be	O
a	O
‘	O
fair	O
’	O
and	O
and	O
‘	O
unfair	O
’	O
.	O
12.2	O
illustrations	O
:	O
coin	O
tossing	O
we	O
’	O
ll	O
consider	O
two	O
illustrations	O
.	O
the	O
ﬁrst	O
uses	O
a	O
discrete	B
parameter	O
space	O
to	O
keep	O
the	O
mathematics	O
simple	O
.	O
in	O
the	O
second	O
we	O
use	O
a	O
continuous	B
parameter	O
space	O
.	O
12.2.1	O
a	O
discrete	B
parameter	O
space	O
a	O
simple	O
choice	O
would	O
be	O
to	O
consider	O
two	O
competing	O
models	O
,	O
one	O
corresponding	O
to	O
a	O
fair	O
coin	O
,	O
and	O
the	O
other	O
a	O
biased	O
coin	O
.	O
the	O
bias	B
of	O
the	O
coin	O
,	O
namely	O
the	O
probability	B
that	O
the	O
coin	O
will	O
land	O
heads	O
,	O
is	O
speciﬁed	O
by	O
θ	O
,	O
so	O
that	O
a	O
truly	O
fair	O
coin	O
has	O
θ	O
=	O
0.5.	O
for	O
simplicity	O
we	O
assume	O
dom	O
(	O
θ	O
)	O
=	O
{	O
0.1	O
,	O
0.2	O
,	O
.	O
.	O
.	O
,	O
0.9	O
}	O
.	O
for	O
the	O
fair	O
coin	O
we	O
use	O
the	O
distribution	B
p	O
(	O
θ|mf	O
air	O
)	O
in	O
ﬁg	O
(	O
12.1a	O
)	O
and	O
for	O
the	O
biased	O
coin	O
the	O
distribution	B
p	O
(	O
θ|mbiased	O
)	O
in	O
ﬁg	O
(	O
12.1b	O
)	O
.	O
for	O
each	O
model	B
m	O
,	O
the	O
likelihood	B
is	O
given	O
by	O
p	O
(	O
d|m	O
)	O
=	O
(	O
cid:88	O
)	O
p	O
(	O
d|θ	O
,	O
m	O
)	O
p	O
(	O
θ|m	O
)	O
=	O
(	O
cid:88	O
)	O
θ	O
θnh	O
(	O
1	O
−	O
θ	O
)	O
nt	O
p	O
(	O
θ|m	O
)	O
θ	O
=	O
0.1nh	O
(	O
1	O
−	O
0.1	O
)	O
nt	O
p	O
(	O
θ	O
=	O
0.1|m	O
)	O
+	O
.	O
.	O
.	O
+	O
0.9nh	O
(	O
1	O
−	O
0.9	O
)	O
nt	O
p	O
(	O
θ	O
=	O
0.9|m	O
)	O
(	O
12.2.1	O
)	O
(	O
12.2.2	O
)	O
assuming	O
that	O
p	O
(	O
mf	O
air	O
)	O
=	O
p	O
(	O
mbiased	O
)	O
the	O
bayes	O
’	O
factor	B
is	O
given	O
by	O
the	O
ratio	O
of	O
the	O
two	O
model	B
likelihoods	O
.	O
example	O
55	O
(	O
discrete	B
parameter	O
space	O
)	O
.	O
5	O
heads	O
and	O
2	O
tails	O
here	O
p	O
(	O
d|mf	O
air	O
)	O
=	O
0.00786	O
and	O
p	O
(	O
d|mbiased	O
)	O
=	O
0.0072.	O
the	O
bayes	O
’	O
factor	B
is	O
p	O
(	O
mf	O
air|d	O
)	O
p	O
(	O
mbiased|d	O
)	O
=	O
1.09	O
(	O
12.2.3	O
)	O
indicating	O
that	O
there	O
is	O
little	O
to	O
choose	O
between	O
the	O
two	O
models	O
.	O
50	O
heads	O
and	O
20	O
tails	O
here	O
p	O
(	O
d|mf	O
air	O
)	O
=	O
1.5	O
×	O
10−20	O
and	O
p	O
(	O
d|mbiased	O
)	O
=	O
1.4	O
×	O
10−19	O
.	O
the	O
bayes	O
’	O
factor	B
is	O
p	O
(	O
mf	O
air|d	O
)	O
p	O
(	O
mbiased|d	O
)	O
=	O
0.109	O
(	O
12.2.4	O
)	O
indicating	O
that	O
have	O
around	O
10	O
times	O
the	O
belief	O
in	O
the	O
biased	O
model	B
as	O
opposed	O
to	O
the	O
fair	O
model	B
.	O
12.2.2	O
a	O
continuous	B
parameter	O
space	O
here	O
we	O
repeat	O
the	O
above	O
calculation	O
but	O
for	O
continuous	B
parameter	O
spaces	O
.	O
242	O
draft	O
march	O
9	O
,	O
2010	O
00.10.20.30.40.50.60.70.80.9100.20.40.60.800.10.20.30.40.50.60.70.80.9100.050.10.15	O
illustrations	O
:	O
coin	O
tossing	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
12.2	O
:	O
probability	B
density	O
priors	O
on	O
the	O
probability	B
of	O
a	O
head	O
p	O
(	O
θ	O
)	O
.	O
(	O
a	O
)	O
:	O
for	O
a	O
fair	O
coin	O
,	O
p	O
(	O
θ|mf	O
air	O
)	O
=	O
(	O
b	O
)	O
:	O
for	O
an	O
biased	O
coin	O
,	O
p	O
(	O
θ|mbiased	O
)	O
=	O
0.5	O
(	O
b	O
(	O
θ|3	O
,	O
10	O
)	O
+	O
b	O
(	O
θ|10	O
,	O
3	O
)	O
)	O
.	O
note	O
the	O
diﬀerent	O
b	O
(	O
θ|50	O
,	O
50	O
)	O
.	O
vertical	O
scales	O
in	O
the	O
two	O
cases	O
.	O
fair	O
coin	O
for	O
the	O
fair	O
coin	O
,	O
a	O
uni-modal	O
prior	B
is	O
appropriate	O
.	O
we	O
use	O
beta	B
distribution	O
p	O
(	O
θ	O
)	O
=	O
b	O
(	O
θ|a	O
,	O
b	O
)	O
,	O
b	O
(	O
θ|a	O
,	O
b	O
)	O
≡	O
1	O
b	O
(	O
a	O
,	O
b	O
)	O
θa−1	O
(	O
1	O
−	O
θ	O
)	O
b−1	O
(	O
12.2.5	O
)	O
for	O
convenience	O
since	O
as	O
this	O
is	O
conjugate	B
to	O
the	O
binomial	B
distribution	O
the	O
required	O
integrations	O
are	O
trivial	O
.	O
a	O
reasonable	O
choice	O
for	O
a	O
fair	O
coin	O
is	O
a	O
=	O
50	O
,	O
b	O
=	O
50	O
,	O
as	O
shown	O
in	O
ﬁg	O
(	O
12.2a	O
)	O
.	O
(	O
cid:90	O
)	O
1	O
b	O
(	O
a	O
,	O
b	O
)	O
θ	O
θa−1	O
(	O
1	O
−	O
θ	O
)	O
b−1	O
θnh	O
(	O
1	O
−	O
θ	O
)	O
nt	O
(	O
cid:90	O
)	O
p	O
(	O
θ	O
)	O
θnh	O
(	O
1	O
−	O
θ	O
)	O
nt	O
=	O
1	O
b	O
(	O
a	O
,	O
b	O
)	O
θ	O
θnh	O
+a−1	O
(	O
1	O
−	O
θ	O
)	O
nt	O
+b−1	O
=	O
b	O
(	O
nh	O
+	O
a	O
,	O
nt	O
+	O
b	O
)	O
b	O
(	O
a	O
,	O
b	O
)	O
(	O
cid:90	O
)	O
θ	O
in	O
general	O
,	O
p	O
(	O
d|mf	O
air	O
)	O
=	O
=	O
biased	O
coin	O
(	O
12.2.6	O
)	O
(	O
12.2.7	O
)	O
for	O
the	O
biased	O
coin	O
,	O
we	O
use	O
a	O
bimodal	O
distribution	B
formed	O
,	O
for	O
convenience	O
,	O
as	O
a	O
mixture	O
of	O
two	O
beta	B
distributions	O
:	O
1	O
2	O
[	O
b	O
(	O
θ|a1	O
,	O
b1	O
)	O
+	O
b	O
(	O
θ|a2	O
,	O
b2	O
)	O
]	O
(	O
12.2.8	O
)	O
as	O
shown	O
in	O
ﬁg	O
(	O
12.2b	O
)	O
.	O
the	O
model	B
likelihood	O
p	O
(	O
d|mbiased	O
)	O
is	O
given	O
by	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
θ	O
p	O
(	O
θ|mbiased	O
)	O
=	O
(	O
cid:90	O
)	O
1	O
(	O
cid:26	O
)	O
p	O
(	O
θ|mbiased	O
)	O
θnh	O
(	O
1	O
−	O
θ	O
)	O
nt	O
1	O
(	O
cid:26	O
)	O
b	O
(	O
nh	O
+	O
a1	O
,	O
nt	O
+	O
b1	O
)	O
2	O
b	O
(	O
a1	O
,	O
b1	O
)	O
θ	O
b	O
(	O
a1	O
,	O
b1	O
)	O
=	O
=	O
1	O
2	O
θa1−1	O
(	O
1θ	O
)	O
b1−1	O
θnh	O
(	O
1	O
−	O
θ	O
)	O
nt	O
+	O
b	O
(	O
a2	O
,	O
b2	O
)	O
θ	O
θa2−1	O
(	O
1	O
−	O
θ	O
)	O
b2−1	O
θnh	O
(	O
1	O
−	O
θ	O
)	O
nt	O
1	O
(	O
cid:27	O
)	O
+	O
b	O
(	O
nh	O
+	O
a2	O
,	O
nt	O
+	O
b2	O
)	O
b	O
(	O
a2	O
,	O
b2	O
)	O
(	O
12.2.9	O
)	O
(	O
cid:27	O
)	O
(	O
12.2.10	O
)	O
(	O
12.2.11	O
)	O
assuming	O
no	O
prior	O
preference	O
for	O
either	O
a	O
fair	O
or	O
biased	O
coin	O
p	O
(	O
m	O
)	O
=	O
const.	O
,	O
and	O
repeating	O
the	O
above	O
scenario	O
in	O
the	O
discrete	B
parameter	O
case	O
:	O
example	O
56	O
(	O
continuous	B
parameter	O
space	O
)	O
.	O
draft	O
march	O
9	O
,	O
2010	O
243	O
00.20.40.60.810246800.20.40.60.8100.511.5	O
occam	O
’	O
s	O
razor	O
and	O
bayesian	O
complexity	O
penalisation	O
figure	O
12.3	O
:	O
the	O
likelihood	B
of	O
the	O
total	O
dice	O
score	O
,	O
p	O
(	O
t|n	O
)	O
for	O
n	O
=	O
1	O
(	O
top	O
)	O
to	O
n	O
=	O
5	O
(	O
bottom	O
)	O
die	O
.	O
plotted	O
along	O
the	O
horizontal	O
axis	O
is	O
the	O
total	O
score	O
t.	O
the	O
vertical	O
line	O
marks	O
the	O
comparison	O
for	O
p	O
(	O
t	O
=	O
9|n	O
)	O
for	O
the	O
diﬀerent	O
number	O
of	O
die	O
.	O
the	O
more	O
complex	O
models	O
,	O
which	O
can	O
reach	O
more	O
states	O
,	O
have	O
lower	O
likelihood	O
,	O
due	O
to	O
normalisation	B
over	O
t.	O
5	O
heads	O
and	O
2	O
tails	O
here	O
p	O
(	O
d|mf	O
air	O
)	O
=	O
0.0079	O
and	O
p	O
(	O
d|mbiased	O
)	O
=	O
0.00622.	O
the	O
bayes	O
’	O
factor	B
is	O
p	O
(	O
mf	O
air|d	O
)	O
p	O
(	O
mbiased|d	O
)	O
=	O
1.27	O
(	O
12.2.12	O
)	O
indicating	O
that	O
there	O
is	O
little	O
to	O
choose	O
between	O
the	O
two	O
models	O
.	O
50	O
heads	O
and	O
20	O
tails	O
here	O
p	O
(	O
d|mf	O
air	O
)	O
=	O
9.4	O
×	O
10−21	O
and	O
p	O
(	O
d|mbiased	O
)	O
=	O
1.09	O
×	O
10−19	O
.	O
the	O
bayes	O
’	O
factor	B
is	O
p	O
(	O
mf	O
air|d	O
)	O
p	O
(	O
mbiased|d	O
)	O
=	O
0.087	O
(	O
12.2.13	O
)	O
indicating	O
that	O
have	O
around	O
11	O
times	O
the	O
belief	O
in	O
the	O
biased	O
model	B
as	O
opposed	O
to	O
the	O
fair	O
model	B
.	O
12.3	O
occam	O
’	O
s	O
razor	O
and	O
bayesian	O
complexity	O
penalisation	O
we	O
return	O
to	O
the	O
dice	O
scenario	O
of	O
section	O
(	O
1.3.1	O
)	O
.	O
there	O
we	O
assumed	O
there	O
are	O
two	O
die	O
whose	O
scores	O
s1	O
and	O
s2	O
are	O
not	O
known	O
.	O
only	O
the	O
sum	O
of	O
the	O
two	O
scores	O
t	O
=	O
s1	O
+	O
s2	O
is	O
known	O
.	O
we	O
then	O
computed	O
the	O
posterior	B
joint	O
score	O
distribution	B
p	O
(	O
s1	O
,	O
s2|t	O
=	O
9	O
)	O
for	O
the	O
two	O
die	O
.	O
we	O
repeat	O
the	O
calculation	O
but	O
now	O
for	O
multiple	O
dice	O
and	O
with	O
the	O
twist	O
that	O
we	O
don	O
’	O
t	O
know	O
how	O
many	O
dice	O
there	O
are1	O
,	O
only	O
that	O
the	O
sum	O
of	O
the	O
scores	O
is	O
9.	O
i=1	O
si	O
and	O
are	O
given	O
the	O
value	B
t	O
=	O
9.	O
however	O
,	O
we	O
are	O
not	O
told	O
the	O
number	O
of	O
die	O
that	O
is	O
,	O
we	O
know	O
t	O
=	O
(	O
cid:80	O
)	O
n	O
involved	O
n.	O
assuming	O
that	O
any	O
number	O
n	O
is	O
equally	O
likely	O
,	O
what	O
is	O
the	O
posterior	B
distribution	O
over	O
n	O
?	O
from	O
bayes	O
’	O
rule	O
,	O
we	O
need	O
to	O
compute	O
the	O
posterior	B
distribution	O
over	O
models	O
in	O
the	O
above	O
p	O
(	O
t	O
)	O
p	O
(	O
n|t	O
)	O
=	O
p	O
(	O
t|n	O
)	O
p	O
(	O
n	O
)	O
p	O
(	O
t|n	O
)	O
=	O
(	O
cid:88	O
)	O
s1	O
,	O
...	O
,	O
sn	O
p	O
(	O
t	O
,	O
s1	O
,	O
.	O
.	O
.	O
,	O
sn|n	O
)	O
=	O
(	O
cid:88	O
)	O
s1	O
,	O
...	O
,	O
sn	O
p	O
(	O
t|s1	O
,	O
.	O
.	O
.	O
,	O
sn	O
)	O
(	O
cid:89	O
)	O
i	O
p	O
(	O
si	O
)	O
=	O
(	O
cid:88	O
)	O
s1	O
,	O
...	O
,	O
sn	O
(	O
cid:34	O
)	O
i	O
t	O
=	O
(	O
cid:35	O
)	O
(	O
cid:89	O
)	O
i	O
si	O
n	O
(	O
cid:88	O
)	O
i=1	O
(	O
12.3.1	O
)	O
p	O
(	O
si	O
)	O
(	O
12.3.2	O
)	O
1this	O
description	O
of	O
occam	O
’	O
s	O
razor	O
is	O
due	O
to	O
taylan	O
cemgil	O
.	O
244	O
draft	O
march	O
9	O
,	O
2010	O
000.10.2000.10.2000.10.2000.10.212345678910111213141516171819202122232425262728293000.10.2	O
a	O
continuous	B
example	O
:	O
curve	O
ﬁtting	O
0.5	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
n	O
figure	O
12.4	O
:	O
the	O
posterior	B
distribution	O
p	O
(	O
n|t	O
=	O
9	O
)	O
of	O
the	O
number	O
of	O
die	O
given	O
the	O
observed	O
summed	O
score	O
of	O
9.	O
where	O
p	O
(	O
si	O
)	O
=	O
1/6	O
for	O
all	O
scores	O
si	O
.	O
by	O
enumerating	O
all	O
6n	O
states	O
,	O
we	O
can	O
explicitly	O
compute	O
p	O
(	O
t|n	O
)	O
,	O
as	O
displayed	O
in	O
ﬁg	O
(	O
12.3	O
)	O
.	O
the	O
important	O
observation	O
is	O
that	O
as	O
the	O
models	O
explaining	O
the	O
data	B
become	O
more	O
‘	O
complex	O
’	O
(	O
n	O
increases	O
)	O
,	O
more	O
states	O
become	O
accessible	O
and	O
the	O
probability	B
mass	O
typically	O
reduces	O
.	O
we	O
see	O
this	O
eﬀect	O
at	O
p	O
(	O
t	O
=	O
9|n	O
)	O
where	O
,	O
apart	O
from	O
n	O
=	O
1	O
,	O
the	O
value	B
of	O
p	O
(	O
t	O
=	O
9|n	O
)	O
decreases	O
with	O
increasing	O
n	O
since	O
the	O
higher	O
n	O
have	O
mass	O
in	O
more	O
states	O
,	O
becoming	O
more	O
spread	O
out	O
.	O
assuming	O
p	O
(	O
n	O
)	O
=	O
const.	O
,	O
the	O
posterior	B
p	O
(	O
n|t	O
=	O
9	O
)	O
is	O
plotted	O
in	O
ﬁg	O
(	O
12.4	O
)	O
.	O
a	O
posteriori	O
,	O
there	O
are	O
only	O
3	O
plausible	O
models	O
,	O
namely	O
n	O
=	O
2	O
,	O
3	O
,	O
4	O
since	O
the	O
rest	O
are	O
either	O
too	O
complex	O
,	O
or	O
impossible	O
.	O
this	O
demonstrates	O
the	O
occam	O
’	O
s	O
razor	O
eﬀect	O
which	O
penalises	O
models	O
which	O
are	O
over	O
complex	O
.	O
12.4	O
a	O
continuous	B
example	O
:	O
curve	O
ﬁtting	O
consider	O
an	O
additive	O
set	O
of	O
periodic	B
functions	O
y0	O
=	O
w0	O
+	O
w1	O
cos	O
(	O
x	O
)	O
+	O
w2	O
cos	O
(	O
2x	O
)	O
+	O
.	O
.	O
.	O
+	O
wk	O
cos	O
(	O
kx	O
)	O
(	O
12.4.1	O
)	O
this	O
can	O
be	O
conveniently	O
written	O
in	O
vector	O
form	O
y0	O
=	O
wtφ	O
(	O
x	O
)	O
(	O
12.4.2	O
)	O
where	O
φ	O
(	O
x	O
)	O
is	O
a	O
k	O
+	O
1	O
dimensional	O
vector	O
with	O
elements	O
(	O
1	O
,	O
cos	O
(	O
x	O
)	O
,	O
cos	O
(	O
2x	O
)	O
,	O
.	O
.	O
.	O
,	O
cos	O
(	O
kx	O
)	O
)	O
t	O
and	O
the	O
vector	O
w	O
contains	O
the	O
weights	O
of	O
the	O
additive	O
function	B
.	O
we	O
are	O
given	O
a	O
set	O
of	O
data	B
d	O
=	O
{	O
(	O
xn	O
,	O
yn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
drawn	O
from	O
this	O
distribution	B
,	O
where	O
y	O
is	O
the	O
clean	O
y0	O
(	O
x	O
)	O
corrupted	O
with	O
additive	O
zero	O
mean	B
gaussian	O
noise	O
with	O
variance	O
σ2	O
,	O
yn	O
=	O
y0	O
(	O
xn	O
)	O
+	O
n	O
,	O
n	O
∼	O
n	O
see	O
ﬁg	O
(	O
12.5	O
)	O
.	O
assuming	O
i.i.d	O
.	O
data	B
,	O
we	O
are	O
interested	O
in	O
the	O
posterior	B
probability	O
of	O
the	O
number	O
of	O
coeﬃ-	O
cients	O
,	O
given	O
the	O
observed	O
data	O
:	O
(	O
cid:0	O
)	O
n	O
0	O
,	O
σ2	O
(	O
cid:1	O
)	O
=	O
p	O
(	O
k	O
)	O
(	O
cid:81	O
)	O
(	O
cid:90	O
)	O
p	O
(	O
d	O
)	O
(	O
12.4.3	O
)	O
(	O
12.4.4	O
)	O
(	O
12.4.5	O
)	O
we	O
will	O
assume	O
p	O
(	O
k	O
)	O
=	O
const	O
.	O
the	O
likelihood	B
term	O
above	O
is	O
given	O
by	O
the	O
integral	O
p	O
(	O
k|d	O
)	O
=	O
p	O
(	O
d|k	O
)	O
p	O
(	O
k	O
)	O
p	O
(	O
d	O
)	O
n	O
p	O
(	O
xn	O
)	O
p	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn|x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
k	O
)	O
=	O
p	O
(	O
w|k	O
)	O
w	O
p	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn|x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
k	O
)	O
n	O
(	O
cid:89	O
)	O
p	O
(	O
yn|xn	O
,	O
w	O
,	O
k	O
)	O
n=1	O
for	O
p	O
(	O
w|k	O
)	O
=	O
n	O
(	O
w	O
0	O
,	O
ik/α	O
)	O
,	O
the	O
integrand	O
is	O
a	O
gaussian	O
in	O
w	O
for	O
which	O
it	O
is	O
straightforward	O
to	O
evaluate	O
the	O
integral	O
,	O
(	O
see	O
section	O
(	O
8.6	O
)	O
and	O
exercise	O
(	O
149	O
)	O
)	O
2	O
log	O
p	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn|x1	O
,	O
.	O
.	O
.	O
,	O
yn	O
,	O
k	O
)	O
=	O
n	O
log	O
(	O
cid:0	O
)	O
2πσ2	O
(	O
cid:1	O
)	O
−	O
n	O
(	O
cid:88	O
)	O
n=1	O
(	O
yn	O
)	O
2	O
σ2	O
+	O
bta−1b−	O
log	O
det	O
(	O
2πa	O
)	O
+	O
k	O
log	O
(	O
2πα	O
)	O
k	O
w	O
xn	O
yn	O
0	O
yn	O
n	O
(	O
cid:82	O
)	O
for	O
regression	B
under	O
the	O
i.i.d	O
.	O
figure	O
12.5	O
:	O
belief	B
network	I
representation	O
of	O
a	O
hierarchical	O
bayesian	O
model	B
data	O
assumption	O
.	O
note	O
that	O
0	O
are	O
included	O
to	O
highlight	O
the	O
role	O
of	O
the	O
the	O
intermediate	O
nodes	O
on	O
yn	O
p	O
(	O
y|y0	O
)	O
p	O
(	O
y0|w	O
,	O
x	O
)	O
=	O
‘	O
clean	O
’	O
underlying	O
model	B
.	O
y0	O
n	O
with	O
the	O
intermediate	O
node	B
y0	O
and	O
place	O
directly	O
arrows	O
from	O
w	O
and	O
xn	O
to	O
yn	O
.	O
since	O
p	O
(	O
y|w	O
,	O
x	O
)	O
=	O
(	O
cid:82	O
)	O
(	O
cid:0	O
)	O
y	O
wtx	O
,	O
σ2	O
(	O
cid:1	O
)	O
,	O
we	O
can	O
if	O
desired	O
do	O
away	O
(	O
cid:0	O
)	O
y	O
y0	O
,	O
σ2	O
(	O
cid:1	O
)	O
δ	O
(	O
cid:0	O
)	O
y0	O
−	O
wtx	O
(	O
cid:1	O
)	O
=	O
n	O
y0	O
draft	O
march	O
9	O
,	O
2010	O
245	O
approximating	O
the	O
model	B
likelihood	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
12.6	O
:	O
(	O
a	O
)	O
the	O
data	B
generated	O
with	O
additive	O
gaussian	O
noise	O
σ	O
=	O
0.5	O
from	O
a	O
k	O
=	O
5	O
component	O
model	B
.	O
(	O
b	O
)	O
the	O
posterior	B
p	O
(	O
k|d	O
)	O
.	O
(	O
c	O
)	O
the	O
reconstruction	O
of	O
the	O
data	B
using	O
(	O
cid:104	O
)	O
w	O
(	O
cid:105	O
)	O
t	O
φ	O
(	O
x	O
)	O
where	O
(	O
cid:104	O
)	O
w	O
(	O
cid:105	O
)	O
is	O
the	O
mean	B
posterior	O
vector	O
of	O
the	O
optimal	O
dimensional	O
model	B
p	O
(	O
w|d	O
,	O
k	O
=	O
5	O
)	O
.	O
plotted	O
in	O
the	O
continuous	B
line	O
is	O
the	O
reconstruction	O
.	O
plotted	O
in	O
dots	O
is	O
the	O
true	O
underlying	O
clean	O
data	B
.	O
where	O
a	O
≡	O
αi	O
+	O
1	O
σ2	O
n	O
(	O
cid:88	O
)	O
n=1	O
φ	O
(	O
xn	O
)	O
φt	O
(	O
xn	O
)	O
,	O
n	O
(	O
cid:88	O
)	O
n=1	O
b	O
≡	O
1	O
σ2	O
ynφ	O
(	O
xn	O
)	O
(	O
12.4.6	O
)	O
(	O
12.4.7	O
)	O
assuming	O
α	O
=	O
1	O
and	O
σ	O
=	O
0.5	O
,	O
we	O
sampled	O
some	O
data	B
from	O
a	O
model	B
with	O
k	O
=	O
5	O
components	O
,	O
ﬁg	O
(	O
12.6a	O
)	O
.	O
we	O
assume	O
that	O
we	O
know	O
the	O
correct	O
noise	O
level	O
σ.	O
the	O
posterior	B
p	O
(	O
k|d	O
)	O
plotted	O
in	O
ﬁg	O
(	O
12.6b	O
)	O
is	O
sharply	O
peaked	O
at	O
k	O
=	O
5	O
,	O
which	O
is	O
the	O
‘	O
correct	O
’	O
value	B
used	O
to	O
generate	O
the	O
data	B
.	O
the	O
clean	O
reconstructions	O
for	O
k	O
=	O
5	O
are	O
plotted	O
in	O
ﬁg	O
(	O
12.6c	O
)	O
.	O
12.5	O
approximating	O
the	O
model	B
likelihood	O
for	O
a	O
model	B
with	O
continuous	B
parameter	O
vector	O
θ	O
,	O
dim	O
(	O
θ	O
)	O
=	O
k	O
and	O
data	B
d	O
,	O
the	O
model	B
likelihood	O
is	O
(	O
cid:90	O
)	O
p	O
(	O
d|m	O
)	O
=	O
p	O
(	O
d|θ	O
,	O
m	O
)	O
p	O
(	O
θ|m	O
)	O
dθ	O
θ	O
for	O
a	O
generic	O
expression	O
p	O
(	O
d|θ	O
,	O
m	O
)	O
p	O
(	O
θ|m	O
)	O
=	O
e	O
−f	O
(	O
θ	O
)	O
(	O
12.5.1	O
)	O
(	O
12.5.2	O
)	O
(	O
12.5.3	O
)	O
(	O
12.5.4	O
)	O
unless	O
f	O
is	O
of	O
a	O
particularly	O
simple	O
form	O
(	O
quadratic	O
in	O
θ	O
for	O
example	O
)	O
,	O
one	O
can	O
not	O
compute	O
the	O
integral	O
in	O
(	O
12.5.1	O
)	O
and	O
approximations	O
are	O
required	O
.	O
12.5.1	O
laplace	O
’	O
s	O
method	O
a	O
simple	O
approximation	O
of	O
(	O
12.5.1	O
)	O
is	O
given	O
by	O
laplace	O
’	O
s	O
method	O
,	O
section	O
(	O
28.2	O
)	O
,	O
log	O
det	O
(	O
cid:0	O
)	O
2πh−1	O
(	O
cid:1	O
)	O
log	O
p	O
(	O
d|m	O
)	O
≈	O
−f	O
(	O
θ∗	O
)	O
+	O
where	O
θ∗	O
is	O
the	O
map	B
solution	O
1	O
2	O
θ∗	O
=	O
argmax	O
p	O
(	O
d|θ	O
,	O
m	O
)	O
p	O
(	O
θ|m	O
)	O
and	O
h	O
is	O
the	O
hessian	O
of	O
f	O
(	O
θ	O
)	O
at	O
θ∗	O
.	O
θ	O
246	O
draft	O
march	O
9	O
,	O
2010	O
−10−50510−3−2−101231234567891000.20.40.60.81−10−50510−3−2−10123	O
exercises	O
for	O
data	B
d	O
=	O
(	O
cid:8	O
)	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
(	O
cid:9	O
)	O
that	O
is	O
i.i.d	O
.	O
generated	O
the	O
above	O
specialises	O
to	O
(	O
cid:90	O
)	O
p	O
(	O
d|m	O
)	O
=	O
p	O
(	O
θ|m	O
)	O
θ	O
n	O
(	O
cid:89	O
)	O
n=1	O
p	O
(	O
xn|θ	O
,	O
m	O
)	O
dθ	O
n	O
(	O
cid:88	O
)	O
−f	O
(	O
θ	O
)	O
=	O
log	O
p	O
(	O
θ|m	O
)	O
+	O
log	O
p	O
(	O
xn|θ	O
,	O
m	O
)	O
n=1	O
in	O
this	O
case	O
laplace	O
’	O
s	O
method	O
computes	O
the	O
optimum	O
of	O
the	O
function	B
(	O
12.5.5	O
)	O
(	O
12.5.6	O
)	O
12.5.2	O
bayes	O
information	O
criterion	O
(	O
bic	O
)	O
for	O
i.i.d	O
.	O
data	B
the	O
hessian	O
scales	O
with	O
the	O
number	O
of	O
training	B
examples	O
,	O
n	O
,	O
and	O
a	O
crude	O
approximation	B
is	O
to	O
set	O
h	O
≈	O
nik	O
where	O
k	O
=	O
dim	O
θ.	O
in	O
this	O
case	O
one	O
may	O
take	O
as	O
a	O
model	B
comparison	O
procedure	O
the	O
function	B
log	O
p	O
(	O
d|m	O
)	O
≈	O
log	O
p	O
(	O
d|θ∗	O
,	O
m	O
)	O
+	O
log	O
p	O
(	O
θ∗	O
|m	O
)	O
+	O
k	O
2	O
log	O
2π	O
−	O
k	O
2	O
log	O
n	O
(	O
12.5.7	O
)	O
for	O
a	O
simple	O
prior	O
that	O
penalises	O
the	O
length	O
of	O
the	O
parameter	B
vector	O
,	O
p	O
(	O
θ|m	O
)	O
=	O
n	O
(	O
θ	O
0	O
,	O
i	O
)	O
,	O
the	O
above	O
reduces	O
to	O
log	O
p	O
(	O
d|m	O
)	O
≈	O
log	O
p	O
(	O
d|θ∗	O
,	O
m	O
)	O
−	O
1	O
2	O
(	O
θ∗	O
)	O
t	O
θ∗	O
k	O
2	O
−	O
log	O
n	O
the	O
bayes	O
information	O
criterion	O
[	O
244	O
]	O
approximates	O
(	O
12.5.7	O
)	O
by	O
ignoring	O
the	O
penalty	O
term	O
,	O
giving	O
bic	O
=	O
log	O
p	O
(	O
d|θ∗	O
,	O
m	O
)	O
−	O
k	O
2	O
log	O
n	O
(	O
12.5.8	O
)	O
(	O
12.5.9	O
)	O
the	O
bic	O
criterion	O
may	O
be	O
used	O
as	O
an	O
approximate	B
way	O
to	O
compare	O
models	O
,	O
where	O
the	O
term	O
−	O
k	O
2	O
log	O
n	O
penalises	O
model	B
complexity	O
.	O
in	O
general	O
,	O
the	O
laplace	O
approximation	B
,	O
equation	B
(	O
12.5.3	O
)	O
,	O
is	O
to	O
be	O
preferred	O
to	O
the	O
bic	O
criterion	O
since	O
it	O
more	O
correctly	O
accounts	O
for	O
the	O
uncertainty	B
in	O
the	O
posterior	B
parameter	O
esti-	O
mate	O
.	O
other	O
techniques	O
that	O
aim	O
to	O
improve	O
on	O
the	O
laplace	O
method	O
are	O
discussed	O
in	O
section	O
(	O
28.3	O
)	O
and	O
section	O
(	O
28.7	O
)	O
.	O
12.6	O
exercises	O
exercise	O
147.	O
write	O
a	O
program	O
to	O
implement	O
the	O
fair/biased	O
coin	O
tossing	O
model	B
selection	I
example	O
of	O
section	O
(	O
12.2.1	O
)	O
using	O
a	O
discrete	B
domain	O
for	O
θ.	O
explain	O
how	O
to	O
overcome	O
potential	B
numerical	O
issues	O
in	O
dealing	O
with	O
large	O
nh	O
and	O
nt	O
(	O
of	O
the	O
order	O
of	O
1000	O
)	O
.	O
exercise	O
148.	O
you	O
work	O
at	O
dodder	O
’	O
s	O
hedge	B
fund	I
and	O
the	O
manager	O
wants	O
to	O
model	B
next	O
day	O
‘	O
returns	O
’	O
yt+1	O
based	O
on	O
current	O
day	O
information	O
xt	O
.	O
the	O
vector	O
of	O
‘	O
factors	O
’	O
each	O
day	O
,	O
xt	O
captures	O
essential	O
aspects	O
of	O
the	O
market	O
.	O
he	O
argues	O
that	O
a	O
simple	O
linear	O
model	B
k	O
(	O
cid:88	O
)	O
k=1	O
yt+1	O
=	O
wkxkt	O
(	O
12.6.1	O
)	O
should	O
be	O
reasonable	O
and	O
asks	O
you	O
to	O
ﬁnd	O
the	O
weight	B
vector	O
w	O
,	O
based	O
on	O
historical	O
information	O
d	O
=	O
{	O
(	O
xt	O
,	O
yt+1	O
)	O
,	O
t	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
t	O
−	O
1	O
}	O
.	O
in	O
addition	O
he	O
also	O
gives	O
you	O
a	O
measure	O
of	O
the	O
‘	O
volatility	O
’	O
σ2	O
t	O
for	O
each	O
day	O
.	O
1.	O
under	O
the	O
assumption	O
that	O
the	O
returns	O
are	O
i.i.d	O
.	O
gaussian	O
distributed	O
t	O
(	O
cid:89	O
)	O
t=2	O
(	O
cid:16	O
)	O
t	O
(	O
cid:89	O
)	O
t=2	O
n	O
(	O
cid:17	O
)	O
p	O
(	O
y1	O
:	O
t|x1	O
:	O
t	O
,	O
w	O
)	O
=	O
p	O
(	O
yt|xt−1	O
,	O
w	O
)	O
=	O
yt	O
wtxt−1	O
,	O
σ2	O
t	O
explain	O
how	O
to	O
set	O
the	O
weight	B
vector	O
w	O
by	O
maximum	B
likelihood	I
.	O
draft	O
march	O
9	O
,	O
2010	O
(	O
12.6.2	O
)	O
247	O
exercises	O
2.	O
your	O
hedge	B
fund	I
manager	O
is	O
however	O
convinced	O
that	O
some	O
of	O
the	O
factors	O
are	O
useless	O
for	O
prediction	B
and	O
wishes	O
to	O
remove	O
as	O
many	O
as	O
possible	O
.	O
to	O
do	O
this	O
you	O
decide	O
to	O
use	O
a	O
bayesian	O
model	B
selection	I
method	O
in	O
which	O
you	O
use	O
a	O
prior	B
p	O
(	O
w|m	O
)	O
=	O
n	O
(	O
w	O
0	O
,	O
i	O
)	O
(	O
12.6.3	O
)	O
where	O
m	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
2k	O
−	O
1	O
indexes	O
the	O
model	B
.	O
each	O
model	B
uses	O
only	O
a	O
subset	O
of	O
the	O
factors	O
.	O
by	O
translating	O
the	O
integer	O
m	O
into	O
a	O
binary	O
vector	O
representation	B
,	O
the	O
model	B
describes	O
which	O
factors	O
are	O
to	O
be	O
used	O
.	O
for	O
example	O
if	O
k	O
=	O
3	O
,	O
there	O
would	O
be	O
7	O
models	O
{	O
0	O
,	O
0	O
,	O
1	O
}	O
,	O
{	O
0	O
,	O
1	O
,	O
0	O
}	O
,	O
{	O
1	O
,	O
0	O
,	O
0	O
}	O
,	O
{	O
0	O
,	O
1	O
,	O
1	O
}	O
,	O
{	O
1	O
,	O
0	O
,	O
1	O
}	O
,	O
{	O
1	O
,	O
1	O
,	O
0	O
}	O
,	O
{	O
1	O
,	O
1	O
,	O
1	O
}	O
(	O
12.6.4	O
)	O
where	O
the	O
ﬁrst	O
model	O
is	O
yt	O
=	O
w3x3	O
with	O
weight	O
prior	B
p	O
(	O
w3	O
)	O
=	O
n	O
(	O
w3	O
0	O
,	O
1	O
)	O
.	O
similarly	O
model	B
7	O
would	O
be	O
yt	O
=	O
w1x1	O
+	O
w2x2	O
+	O
w3x3	O
with	O
p	O
(	O
w1	O
,	O
w2	O
,	O
w3	O
)	O
=	O
n	O
(	O
(	O
w1	O
,	O
w2	O
,	O
w3	O
)	O
(	O
0	O
,	O
0	O
,	O
0	O
)	O
,	O
i3	O
)	O
.	O
you	O
decide	O
to	O
use	O
a	O
ﬂat	O
prior	B
p	O
(	O
m	O
)	O
=	O
const	O
.	O
draw	O
the	O
hierarchical	O
bayesian	O
network	O
for	O
this	O
model	B
and	O
explain	O
how	O
to	O
ﬁnd	O
the	O
best	O
model	B
for	O
the	O
data	B
using	O
bayesian	O
model	B
selection	I
by	O
suitably	O
adapting	O
equation	B
(	O
12.4.6	O
)	O
.	O
3.	O
using	O
the	O
data	B
dodder.mat	O
,	O
perform	O
bayesian	O
model	B
selection	I
as	O
above	O
for	O
k	O
=	O
6	O
and	O
ﬁnd	O
which	O
of	O
the	O
factors	O
x1	O
,	O
.	O
.	O
.	O
,	O
x6	O
are	O
most	O
likely	O
to	O
explain	O
the	O
data	B
.	O
exercise	O
149.	O
here	O
we	O
will	O
derive	O
the	O
expression	O
(	O
12.4.6	O
)	O
and	O
also	O
an	O
alternative	O
form	O
.	O
1.	O
starting	O
from	O
n	O
(	O
cid:89	O
)	O
n=1	O
p	O
(	O
yn|w	O
,	O
xn	O
,	O
k	O
)	O
=	O
n	O
(	O
w	O
0	O
,	O
i/α	O
)	O
(	O
cid:89	O
)	O
p	O
(	O
w	O
)	O
(	O
cid:16	O
)	O
yn	O
wtφ	O
(	O
xn	O
)	O
,	O
σ2	O
(	O
cid:17	O
)	O
(	O
cid:80	O
)	O
n	O
(	O
yn−wtφ	O
(	O
xn	O
)	O
)	O
2	O
−	O
1	O
2σ2	O
1	O
(	O
2πσ2	O
)	O
n/2	O
e	O
n	O
n	O
2	O
wtw	O
−	O
α	O
e	O
show	O
that	O
this	O
can	O
be	O
expressed	O
as	O
1	O
=	O
√2πα−1	O
(	O
cid:80	O
)	O
n	O
(	O
yn	O
)	O
2	O
−	O
1	O
e	O
−	O
1	O
2σ2	O
1	O
√2πα−1	O
where	O
1	O
(	O
2πσ2	O
)	O
n/2	O
e	O
(	O
cid:88	O
)	O
1	O
σ2	O
n	O
a	O
=	O
αi	O
+	O
φ	O
(	O
xn	O
)	O
φt	O
(	O
xn	O
)	O
2	O
wtaw+btw	O
(	O
cid:88	O
)	O
n	O
b	O
=	O
1	O
σ2	O
ynφ	O
(	O
xn	O
)	O
(	O
12.6.5	O
)	O
(	O
12.6.6	O
)	O
(	O
12.6.7	O
)	O
(	O
12.6.8	O
)	O
2.	O
by	O
completing	O
the	O
square	O
(	O
see	O
section	O
(	O
8.6.2	O
)	O
)	O
,	O
derive	O
(	O
12.4.6	O
)	O
.	O
3.	O
since	O
each	O
yn	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
is	O
linearly	O
related	O
through	O
w	O
and	O
w	O
is	O
gaussian	O
distributed	O
,	O
the	O
joint	B
vector	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
is	O
gaussian	O
distributed	O
.	O
using	O
the	O
gaussian	O
propagation	B
results	O
,	O
section	O
(	O
8.6.3	O
)	O
,	O
derive	O
an	O
alternative	O
expression	O
for	O
log	O
p	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn|x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
.	O
248	O
draft	O
march	O
9	O
,	O
2010	O
part	O
iii	O
machine	O
learning	B
249	O
chapter	O
13	O
machine	O
learning	B
concepts	O
13.1	O
styles	O
of	O
learning	B
broadly	O
speaking	O
the	O
main	O
two	O
subﬁelds	O
of	O
machine	O
learning	B
are	O
supervised	B
learning	I
and	O
unsupervised	B
learning	I
.	O
in	O
supervised	B
learning	I
the	O
focus	O
is	O
on	O
accurate	O
prediction	B
,	O
whereas	O
in	O
unsupervised	B
learning	I
the	O
aim	O
is	O
to	O
ﬁnd	O
accurate	O
compact	O
descriptions	O
of	O
the	O
data	B
.	O
particularly	O
in	O
supervised	B
learning	I
,	O
one	O
is	O
interested	O
in	O
methods	O
that	O
perform	O
well	O
on	O
previously	O
unseen	O
data	B
.	O
that	O
is	O
,	O
the	O
method	O
‘	O
generalises	O
’	O
to	O
unseen	O
data	B
.	O
in	O
this	O
sense	O
,	O
one	O
distinguishes	O
between	O
data	B
that	O
is	O
used	O
to	O
train	O
a	O
model	B
,	O
and	O
data	B
that	O
is	O
used	O
to	O
test	O
the	O
performance	B
of	O
the	O
trained	O
model	B
,	O
see	O
ﬁg	O
(	O
13.1	O
)	O
.	O
13.1.1	O
supervised	B
learning	I
consider	O
a	O
database	O
of	O
face	O
images	O
,	O
each	O
represented	O
by	O
a	O
vector1	O
x.	O
along	O
with	O
each	O
image	O
x	O
is	O
an	O
output	O
class	O
y	O
∈	O
{	O
male	O
,	O
female	O
}	O
that	O
states	O
if	O
the	O
image	O
is	O
of	O
a	O
male	O
or	O
female	O
.	O
a	O
database	O
of	O
10000	O
such	O
image-class	O
pairs	O
is	O
available	O
,	O
d	O
=	O
{	O
(	O
xn	O
,	O
yn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
10000	O
}	O
.	O
the	O
task	O
is	O
to	O
make	O
an	O
accurate	O
predictor	O
y	O
(	O
x∗	O
)	O
of	O
the	O
sex	O
of	O
a	O
novel	O
image	O
x∗	O
.	O
this	O
is	O
an	O
example	O
application	O
that	O
would	O
be	O
hard	B
to	O
program	O
in	O
a	O
traditional	O
‘	O
programming	O
’	O
manner	O
since	O
formally	O
specifying	O
how	O
male	O
faces	B
diﬀer	O
from	O
female	O
faces	B
is	O
diﬃcult	O
.	O
an	O
alternative	O
is	O
to	O
give	O
examples	O
faces	B
and	O
their	O
gender	O
labels	O
and	O
let	O
a	O
machine	O
automatically	O
‘	O
learn	O
’	O
a	O
rule	O
to	O
diﬀerentiate	O
male	O
from	O
female	O
faces	B
.	O
deﬁnition	O
88	O
(	O
supervised	B
learning	I
)	O
.	O
given	O
a	O
set	O
of	O
data	B
d	O
=	O
{	O
(	O
xn	O
,	O
yn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
the	O
task	O
is	O
to	O
‘	O
learn	O
’	O
the	O
relationship	O
between	O
the	O
input	O
x	O
and	O
output	O
y	O
such	O
that	O
,	O
when	O
given	O
a	O
new	O
input	O
x∗	O
the	O
predicted	O
output	O
y∗	O
is	O
accurate	O
.	O
to	O
specify	O
explicitly	O
what	O
accuracy	O
means	O
one	O
deﬁnes	O
a	O
loss	B
function	I
l	O
(	O
ypred	O
,	O
ytrue	O
)	O
or	O
,	O
conversely	O
,	O
a	O
utility	B
function	O
u	O
=	O
−l	O
.	O
in	O
supervised	B
learning	I
our	O
interest	O
is	O
describing	O
y	O
conditioned	O
on	O
knowing	O
x.	O
from	O
a	O
probabilistic	B
modelling	O
perspective	O
,	O
we	O
are	O
therefore	O
concerned	O
primarily	O
with	O
the	O
conditional	B
distribution	O
p	O
(	O
y|x	O
,	O
d	O
)	O
.	O
the	O
term	O
‘	O
supervised	B
’	O
indicates	O
that	O
there	O
is	O
a	O
‘	O
supervisor	O
’	O
specifying	O
the	O
output	O
y	O
for	O
each	O
input	O
x	O
in	O
the	O
available	O
data	B
d.	O
the	O
output	O
is	O
also	O
called	O
a	O
‘	O
label	O
’	O
,	O
particularly	O
when	O
discussing	O
classiﬁcation	B
.	O
predicting	O
tomorrow	O
’	O
s	O
stock	O
price	O
y	O
(	O
t	O
+1	O
)	O
based	O
on	O
past	O
observations	O
y	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
y	O
(	O
t	O
)	O
is	O
a	O
form	O
of	O
supervised	B
learning	I
.	O
we	O
have	O
a	O
collection	O
of	O
times	O
and	O
prices	O
d	O
=	O
{	O
(	O
t	O
,	O
y	O
(	O
t	O
)	O
)	O
,	O
t	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
t	O
}	O
where	O
time	O
t	O
is	O
the	O
‘	O
input	O
’	O
and	O
the	O
price	O
y	O
(	O
t	O
)	O
is	O
the	O
output	O
.	O
1for	O
an	O
m	O
×	O
n	O
face	O
image	O
with	O
elements	O
fmn	O
we	O
can	O
form	O
a	O
vector	O
by	O
stacking	O
the	O
entries	O
of	O
the	O
matrix	B
.	O
in	O
matlab	O
one	O
may	O
achieve	O
this	O
using	O
x=f	O
(	O
:	O
)	O
.	O
251	O
train	O
test	O
styles	O
of	O
learning	B
figure	O
13.1	O
:	O
in	O
training	B
and	O
evaluating	O
a	O
model	B
,	O
conceptually	O
there	O
are	O
two	O
sources	O
of	O
data	B
.	O
the	O
parameters	O
of	O
the	O
model	B
are	O
set	O
on	O
the	O
basis	O
of	O
the	O
train	O
data	O
only	O
.	O
if	O
the	O
test	O
data	O
is	O
gener-	O
ated	O
from	O
the	O
same	O
underlying	O
process	O
that	O
generated	O
the	O
train	O
data	O
,	O
an	O
unbiased	O
estimate	O
of	O
the	O
generalisation	B
performance	O
can	O
be	O
obtained	O
by	O
measuring	O
the	O
test	O
data	O
performance	B
of	O
the	O
trained	O
model	B
.	O
importantly	O
,	O
the	O
test	O
performance	O
should	O
not	O
be	O
used	O
to	O
adjust	O
the	O
model	B
parameters	O
since	O
we	O
would	O
then	O
no	O
longer	O
have	O
an	O
independent	O
measure	O
of	O
the	O
performance	B
of	O
the	O
model	B
.	O
example	O
57.	O
a	O
father	O
decides	O
to	O
teach	O
his	O
young	O
son	O
what	O
a	O
sports	O
car	O
is	O
.	O
finding	O
it	O
diﬃcult	O
to	O
explain	O
in	O
words	O
,	O
he	O
decides	O
to	O
give	O
some	O
examples	O
.	O
they	O
stand	O
on	O
a	O
motorway	O
bridge	O
and	O
,	O
as	O
each	O
car	O
passes	O
underneath	O
,	O
the	O
father	O
cries	O
out	O
‘	O
that	O
’	O
s	O
a	O
sports	O
car	O
!	O
’	O
when	O
a	O
sports	O
car	O
passes	O
by	O
.	O
after	O
ten	O
minutes	O
,	O
the	O
father	O
asks	O
his	O
son	O
if	O
he	O
’	O
s	O
understood	O
what	O
a	O
sports	O
car	O
is	O
.	O
the	O
son	O
says	O
,	O
‘	O
sure	O
,	O
it	O
’	O
s	O
easy	O
’	O
.	O
an	O
old	O
red	O
vw	O
beetle	O
passes	O
by	O
,	O
and	O
the	O
son	O
shouts	O
–	O
‘	O
that	O
’	O
s	O
a	O
sports	O
car	O
!	O
’	O
.	O
dejected	O
,	O
the	O
father	O
asks	O
–	O
‘	O
why	O
do	O
you	O
say	O
that	O
?	O
’	O
.	O
‘	O
because	O
all	O
sports	O
cars	O
are	O
red	O
!	O
’	O
,	O
replies	O
the	O
son	O
.	O
this	O
is	O
an	O
example	O
scenario	O
for	O
supervised	B
learning	I
.	O
here	O
the	O
father	O
plays	O
the	O
role	O
of	O
the	O
supervisor	O
,	O
and	O
his	O
son	O
is	O
the	O
‘	O
student	O
’	O
(	O
or	O
‘	O
learner	O
’	O
)	O
.	O
it	O
’	O
s	O
indicative	O
of	O
the	O
kinds	O
of	O
problems	O
encountered	O
in	O
machine	O
learning	B
in	O
that	O
it	O
is	O
not	O
really	O
clear	O
anyway	O
what	O
a	O
sports	O
car	O
is	O
–	O
if	O
we	O
knew	O
that	O
,	O
then	O
we	O
wouldn	O
’	O
t	O
need	O
to	O
go	O
through	O
the	O
process	O
of	O
learning	B
.	O
this	O
example	O
also	O
highlights	O
the	O
issue	O
that	O
there	O
is	O
a	O
diﬀerence	O
between	O
performing	O
well	O
on	O
training	B
data	O
and	O
performing	O
well	O
on	O
novel	O
test	O
data	O
.	O
the	O
main	O
interest	O
in	O
supervised	B
learning	I
is	O
to	O
discover	O
an	O
underlying	O
rule	O
that	O
will	O
generalise	O
well	O
,	O
leading	O
to	O
accurate	O
prediction	B
on	O
new	O
inputs	O
.	O
for	O
an	O
input	O
x	O
,	O
if	O
the	O
output	O
is	O
one	O
of	O
a	O
discrete	B
number	O
of	O
possible	O
‘	O
classes	O
’	O
,	O
this	O
is	O
called	O
a	O
classiﬁcation	B
problem	O
.	O
in	O
classiﬁcation	B
problems	O
we	O
will	O
generally	O
use	O
c	O
for	O
the	O
output	O
.	O
for	O
an	O
input	O
x	O
,	O
if	O
the	O
output	O
is	O
continuous	B
,	O
this	O
is	O
called	O
a	O
regression	B
problem	O
.	O
for	O
example	O
,	O
based	O
on	O
historical	O
information	O
of	O
demand	O
for	O
sun-cream	O
in	O
your	O
supermarket	O
,	O
you	O
are	O
asked	O
to	O
predict	O
the	O
demand	O
for	O
the	O
next	O
month	O
.	O
in	O
some	O
cases	O
it	O
is	O
possible	O
to	O
discretise	O
a	O
continuous	B
output	O
and	O
then	O
consider	O
a	O
corresponding	O
classiﬁcation	B
problem	O
.	O
however	O
,	O
in	O
other	O
cases	O
it	O
is	O
impractical	O
or	O
unnatural	O
to	O
do	O
this	O
;	O
for	O
example	O
if	O
the	O
output	O
y	O
is	O
a	O
high	O
dimensional	O
continuous	O
valued	O
vector	O
,	O
or	O
if	O
the	O
ordering	O
of	O
states	O
of	O
the	O
variable	B
is	O
meaningful	O
.	O
13.1.2	O
unsupervised	B
learning	I
deﬁnition	O
89	O
(	O
unsupervised	B
learning	I
)	O
.	O
given	O
a	O
set	O
of	O
data	B
d	O
=	O
{	O
xn	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
in	O
unsupervised	B
learning	I
we	O
aim	O
to	O
to	O
‘	O
learn	O
’	O
a	O
plausible	O
compact	O
description	O
of	O
the	O
data	B
.	O
an	O
objective	O
is	O
used	O
to	O
quantify	O
the	O
accuracy	O
of	O
the	O
description	O
.	O
in	O
unsupervised	B
learning	I
there	O
is	O
no	O
special	O
‘	O
prediction	B
’	O
variable	B
.	O
from	O
a	O
probabilistic	B
perspective	O
we	O
are	O
interested	O
in	O
modelling	B
the	O
distribution	B
p	O
(	O
x	O
)	O
.	O
the	O
likelihood	B
of	O
the	O
data	B
under	O
the	O
i.i.d	O
.	O
assumption	O
,	O
for	O
example	O
,	O
would	O
be	O
one	O
objective	O
measure	O
of	O
the	O
accuracy	O
of	O
the	O
description	O
.	O
252	O
draft	O
march	O
9	O
,	O
2010	O
styles	O
of	O
learning	B
example	O
58.	O
a	O
supermarket	O
chain	B
wishes	O
to	O
discover	O
how	O
many	O
diﬀerent	O
basic	O
consumer	O
buying	O
behaviours	O
there	O
are	O
based	O
on	O
a	O
large	O
database	O
of	O
supermarket	O
checkout	O
data	B
.	O
items	O
brought	O
by	O
a	O
customer	O
on	O
a	O
visit	O
to	O
a	O
checkout	O
are	O
represented	O
by	O
a	O
(	O
very	O
sparse	B
)	O
10,000	O
dimensional	O
vector	O
x	O
which	O
contains	O
a	O
1	O
in	O
the	O
ith	O
element	O
if	O
the	O
customer	O
bought	O
product	O
i	O
and	O
0	O
otherwise	O
.	O
based	O
on	O
10	O
million	O
such	O
checkout	O
vectors	O
from	O
stores	O
across	O
the	O
country	O
,	O
d	O
=	O
(	O
cid:8	O
)	O
xn	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
107	O
(	O
cid:9	O
)	O
the	O
supermarket	O
chain	B
wishes	O
to	O
discover	O
patterns	O
of	O
buying	O
behaviour	O
.	O
in	O
the	O
table	O
each	O
column	O
represents	O
the	O
buying	O
patterns	O
of	O
a	O
customer	O
(	O
7	O
customer	O
records	O
and	O
just	O
the	O
ﬁrst	O
6	O
of	O
the	O
10,000	O
products	O
are	O
shown	O
)	O
.	O
a	O
1	O
indicates	O
that	O
the	O
customer	O
bought	O
that	O
item	O
.	O
we	O
wish	O
to	O
ﬁnd	O
common	O
patterns	O
in	O
the	O
data	B
,	O
such	O
as	O
if	O
someone	O
buys	O
coﬀee	O
they	O
are	O
also	O
likely	O
to	O
buy	O
milk	O
.	O
coﬀee	O
tea	O
milk	O
beer	O
diapers	O
aspirin	O
1	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
1	O
1	O
0	O
1	O
0	O
1	O
0	O
1	O
1	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
1	O
·	O
·	O
·	O
·	O
·	O
·	O
·	O
·	O
·	O
·	O
·	O
·	O
example	O
59	O
(	O
clustering	B
)	O
.	O
the	O
table	O
on	O
the	O
right	O
represents	O
a	O
collection	O
of	O
unla-	O
belled	O
two-dimensional	O
points	O
.	O
we	O
can	O
visualise	O
this	O
data	B
by	O
plotting	O
it	O
in	O
2	O
dimensions	O
.	O
x1	O
x2	O
-2	O
7	O
-6	O
22	O
-1	O
1	O
11	O
1	O
-1	O
-8	O
46	O
52	O
33	O
40	O
42	O
33	O
32	O
54	O
45	O
39	O
by	O
simply	O
eye-balling	O
the	O
data	B
,	O
we	O
can	O
see	O
that	O
there	O
are	O
two	O
apparent	O
clusters	O
here	O
,	O
one	O
centred	O
around	O
(	O
0,5	O
)	O
and	O
the	O
other	O
around	O
(	O
35,45	O
)	O
.	O
a	O
reasonable	O
model	B
to	O
describe	O
this	O
data	B
might	O
therefore	O
be	O
to	O
describe	O
it	O
as	O
two	O
clusters	O
,	O
centred	O
at	O
(	O
0,0	O
)	O
and	O
(	O
35,35	O
)	O
,	O
each	O
with	O
a	O
standard	B
deviation	I
of	O
around	O
10	O
.	O
13.1.3	O
anomaly	B
detection	I
a	O
baby	O
processes	O
a	O
mass	O
of	O
initially	O
confusing	O
sensory	O
data	B
.	O
after	O
a	O
while	O
the	O
baby	O
begins	O
to	O
understand	O
her	O
environment	O
in	O
the	O
sense	O
that	O
novel	O
sensory	O
data	B
from	O
the	O
same	O
environment	O
is	O
familiar	O
or	O
expected	O
.	O
when	O
a	O
strange	O
face	O
presents	O
itself	O
,	O
the	O
baby	O
recognises	O
that	O
this	O
is	O
not	O
familiar	O
and	O
may	O
be	O
upset	O
.	O
the	O
baby	O
has	O
learned	O
a	O
representation	B
of	O
the	O
familiar	O
and	O
can	O
distinguish	O
the	O
expected	O
from	O
the	O
unexpected	O
;	O
this	O
is	O
an	O
example	O
of	O
unsupervised	B
learning	I
.	O
models	O
that	O
can	O
detect	O
irregular	O
events	O
are	O
used	O
in	O
plant	B
monitoring	I
and	O
require	O
a	O
model	B
of	O
normality	O
which	O
will	O
in	O
most	O
cases	O
be	O
based	O
on	O
unlabelled	B
data	I
.	O
13.1.4	O
online	B
(	O
sequential	B
)	O
learning	B
in	O
the	O
above	O
situations	O
,	O
we	O
assumed	O
that	O
the	O
data	B
d	O
was	O
given	O
beforehand	O
.	O
in	O
online	B
learning	I
data	O
arrives	O
sequentially	O
and	O
we	O
want	O
to	O
continually	O
update	O
our	O
model	B
as	O
new	O
data	B
becomes	O
available	O
.	O
online	B
learning	I
may	O
occur	O
in	O
either	O
a	O
supervised	B
or	O
unsupervised	B
context	O
.	O
13.1.5	O
interacting	O
with	O
the	O
environment	O
in	O
many	O
real-world	O
situations	O
,	O
an	O
agent	O
is	O
able	O
to	O
interact	O
in	O
some	O
manner	O
with	O
its	O
environment	O
.	O
query	B
(	O
active	B
)	O
learning	B
here	O
the	O
agent	O
has	O
the	O
ability	O
to	O
request	O
data	B
from	O
the	O
environment	O
.	O
for	O
example	O
,	O
a	O
predictor	O
might	O
recognise	O
that	O
it	O
is	O
less	O
conﬁdently	O
able	O
to	O
predict	O
in	O
certain	O
regions	O
of	O
the	O
space	O
x	O
and	O
therefore	O
requests	O
more	O
training	B
data	O
in	O
this	O
region	O
.	O
active	B
learning	I
can	O
also	O
be	O
considered	O
in	O
an	O
unsupervised	B
context	O
in	O
which	O
the	O
agent	O
might	O
request	O
information	O
in	O
regions	O
where	O
p	O
(	O
x	O
)	O
looks	O
uninformative	O
or	O
‘	O
ﬂat	O
’	O
.	O
draft	O
march	O
9	O
,	O
2010	O
253	O
−100102030405001020304050	O
supervised	B
learning	I
reinforcement	O
learning	B
one	O
might	O
term	O
this	O
also	O
‘	O
survival	O
learning	B
’	O
.	O
one	O
has	O
in	O
mind	O
scenarios	O
such	O
as	O
encountered	O
in	O
real-life	O
where	O
an	O
organism	O
needs	O
to	O
learn	O
the	O
best	O
actions	O
to	O
take	O
in	O
its	O
environment	O
in	O
order	O
to	O
survive	O
as	O
long	O
as	O
possible	O
.	O
in	O
each	O
situation	O
in	O
which	O
the	O
agent	O
ﬁnds	O
itself	O
it	O
needs	O
to	O
take	O
an	O
action	O
.	O
some	O
actions	O
may	O
eventually	O
be	O
beneﬁcial	O
(	O
lead	O
to	O
food	O
for	O
example	O
)	O
,	O
whilst	O
others	O
may	O
be	O
disastrous	O
(	O
lead	O
to	O
being	O
eaten	O
for	O
example	O
)	O
.	O
based	O
on	O
accumulated	O
experience	O
,	O
the	O
agent	O
needs	O
to	O
learn	O
which	O
action	O
to	O
take	O
in	O
a	O
given	O
situation	O
in	O
order	O
to	O
obtain	O
a	O
desired	O
long	O
term	O
goal	O
.	O
essentially	O
actions	O
that	O
lead	O
to	O
long	O
term	O
rewards	O
need	O
to	O
reinforced	O
.	O
reinforcement	B
learning	I
has	O
connections	O
with	O
control	O
theory	O
,	O
markov	O
decision	O
processes	O
and	O
game	O
theory	O
.	O
whilst	O
we	O
discussed	O
mdps	O
and	O
brieﬂy	O
mentioned	O
how	O
an	O
environment	O
can	O
be	O
learned	O
based	O
on	O
delayed	O
rewards	O
in	O
section	O
(	O
7.8.3	O
)	O
,	O
we	O
will	O
not	O
discuss	O
this	O
topic	O
further	O
in	O
this	O
book	O
.	O
13.1.6	O
semi-supervised	B
learning	I
in	O
machine	O
learning	B
,	O
a	O
common	O
scenario	O
is	O
to	O
have	O
a	O
small	O
amount	O
of	O
labelled	B
and	O
a	O
large	O
amount	O
of	O
unlabelled	B
data	I
.	O
for	O
example	O
,	O
it	O
may	O
be	O
that	O
we	O
have	O
access	O
to	O
many	O
images	O
of	O
faces	B
;	O
however	O
,	O
only	O
a	O
small	O
number	O
of	O
them	O
may	O
have	O
been	O
labelled	B
as	O
instances	O
of	O
known	O
faces	B
.	O
in	O
semi-supervised	B
learning	I
,	O
one	O
tries	O
to	O
use	O
the	O
unlabelled	B
data	I
to	O
make	O
a	O
better	O
classiﬁer	B
than	O
that	O
based	O
on	O
the	O
labelled	B
data	I
alone	O
.	O
13.2	O
supervised	B
learning	I
supervised	O
and	O
unsupervised	B
learning	I
are	O
mature	O
ﬁelds	O
with	O
a	O
wide	O
range	O
of	O
practical	O
tools	O
and	O
associated	O
theoretical	O
analyses	O
.	O
our	O
aim	O
here	O
is	O
to	O
give	O
a	O
brief	O
introduction	O
to	O
the	O
issues	O
and	O
‘	O
philosophies	O
’	O
behind	O
the	O
approaches	O
.	O
we	O
focus	O
here	O
mainly	O
on	O
supervised	B
learning	I
and	O
classiﬁcation	B
in	O
particular	O
.	O
13.2.1	O
utility	B
and	O
loss	O
to	O
more	O
fully	O
specify	O
a	O
supervised	B
problem	O
we	O
need	O
to	O
be	O
clear	O
what	O
‘	O
cost	O
’	O
is	O
involved	O
in	O
making	O
a	O
correct	O
or	O
incorrect	O
prediction	B
.	O
in	O
a	O
two	O
class	O
problem	B
dom	O
(	O
c	O
)	O
=	O
{	O
1	O
,	O
2	O
}	O
,	O
we	O
assume	O
here	O
that	O
everything	O
we	O
know	O
about	O
the	O
environment	O
is	O
contained	O
in	O
a	O
model	B
p	O
(	O
x	O
,	O
c	O
)	O
.	O
given	O
a	O
new	O
input	O
x∗	O
,	O
the	O
optimal	O
prediction	B
also	O
depends	O
on	O
how	O
costly	O
making	O
an	O
error	O
is	O
.	O
this	O
can	O
be	O
quantiﬁed	O
using	O
a	O
loss	B
function	I
(	O
or	O
conversely	O
a	O
utility	B
)	O
.	O
in	O
forming	O
a	O
decision	B
function	I
c	O
(	O
x∗	O
)	O
that	O
will	O
produce	O
a	O
class	O
label	O
for	O
the	O
new	O
input	O
x∗	O
,	O
we	O
don	O
’	O
t	O
know	O
the	O
true	O
class	O
,	O
only	O
our	O
presumed	O
distribution	B
p	O
(	O
c|x∗	O
)	O
.	O
the	O
expected	O
utility	B
for	O
the	O
decision	B
function	I
is	O
∗	O
)	O
)	O
=	O
(	O
cid:88	O
)	O
ctrue	O
u	O
(	O
c	O
(	O
x	O
u	O
(	O
ctrue	O
,	O
c	O
(	O
x	O
∗	O
)	O
)	O
p	O
(	O
ctrue|x	O
∗	O
)	O
(	O
13.2.1	O
)	O
and	O
the	O
optimal	O
decision	O
is	O
that	O
which	O
maximises	O
the	O
expected	O
utility	B
.	O
zero-one	B
loss	I
0	O
if	O
c∗	O
u	O
(	O
ctrue	O
,	O
c	O
∗	O
)	O
=	O
(	O
cid:54	O
)	O
=	O
ctrue	O
for	O
the	O
two	O
class	O
case	O
,	O
we	O
then	O
have	O
a	O
‘	O
count	O
the	O
correct	O
predictions	O
’	O
measure	O
of	O
prediction	B
performance	O
is	O
based	O
on	O
the	O
‘	O
zero-one	B
’	O
utility	B
(	O
or	O
conversely	O
the	O
zero-one	B
loss	I
)	O
:	O
(	O
cid:26	O
)	O
1	O
if	O
c∗	O
=	O
ctrue	O
(	O
cid:26	O
)	O
p	O
(	O
ctrue	O
=	O
1|x∗	O
)	O
for	O
c	O
(	O
x∗	O
)	O
=	O
1	O
p	O
(	O
ctrue	O
=	O
2|x∗	O
)	O
for	O
c	O
(	O
x∗	O
)	O
=	O
2	O
hence	O
,	O
in	O
order	O
to	O
have	O
the	O
highest	O
expected	O
utility	B
,	O
the	O
decision	B
function	I
c	O
(	O
x∗	O
)	O
should	O
correspond	O
to	O
(	O
cid:26	O
)	O
1	O
if	O
p	O
(	O
c	O
=	O
1|x∗	O
)	O
≥	O
0.5	O
selecting	O
the	O
highest	O
class	O
probability	B
p	O
(	O
c|x∗	O
)	O
:	O
2	O
if	O
p	O
(	O
c	O
=	O
2|x∗	O
)	O
≥	O
0.5	O
∗	O
)	O
=	O
c	O
(	O
x	O
u	O
(	O
c	O
(	O
x	O
∗	O
)	O
)	O
=	O
(	O
13.2.2	O
)	O
(	O
13.2.3	O
)	O
(	O
13.2.4	O
)	O
in	O
the	O
case	O
of	O
a	O
tie	O
,	O
either	O
class	O
is	O
selected	O
at	O
random	O
with	O
equal	O
probability	B
.	O
254	O
draft	O
march	O
9	O
,	O
2010	O
supervised	B
learning	I
general	O
loss	O
functions	O
in	O
general	O
,	O
for	O
a	O
two-class	O
problem	B
,	O
we	O
have	O
(	O
cid:26	O
)	O
u	O
(	O
ctrue	O
=	O
1	O
,	O
c∗	O
=	O
1	O
)	O
p	O
(	O
ctrue	O
=	O
1|x∗	O
)	O
+	O
u	O
(	O
ctrue	O
=	O
2	O
,	O
c∗	O
=	O
1	O
)	O
p	O
(	O
ctrue	O
=	O
2|x∗	O
)	O
for	O
c	O
(	O
x∗	O
)	O
=	O
1	O
u	O
(	O
ctrue	O
=	O
1	O
,	O
c∗	O
=	O
2	O
)	O
p	O
(	O
ctrue	O
=	O
1|x∗	O
)	O
+	O
u	O
(	O
ctrue	O
=	O
2	O
,	O
c∗	O
=	O
2	O
)	O
p	O
(	O
ctrue	O
=	O
2|x∗	O
)	O
for	O
c	O
(	O
x∗	O
)	O
=	O
2	O
u	O
(	O
c	O
(	O
x	O
∗	O
)	O
)	O
=	O
(	O
13.2.5	O
)	O
and	O
the	O
optimal	O
decision	B
function	I
c	O
(	O
x∗	O
)	O
chooses	O
that	O
class	O
with	O
highest	O
expected	O
utility	B
.	O
one	O
can	O
readily	O
generalise	O
this	O
to	O
multiple-class	O
situations	O
using	O
a	O
utility	B
matrix	O
with	O
elements	O
ui	O
,	O
j	O
=	O
u	O
(	O
ctrue	O
=	O
i	O
,	O
cpred	O
=	O
j	O
)	O
(	O
13.2.6	O
)	O
where	O
the	O
i	O
,	O
j	O
element	O
of	O
the	O
matrix	B
contains	O
the	O
utility	B
of	O
predicting	O
class	O
j	O
when	O
the	O
true	O
class	O
is	O
i.	O
conversely	O
one	O
could	O
think	O
of	O
a	O
loss-matrix	O
with	O
entries	O
lij	O
=	O
−uij	O
.	O
the	O
expected	O
loss	O
with	O
respect	O
to	O
p	O
(	O
c|x	O
)	O
is	O
then	O
termed	O
the	O
risk	B
.	O
in	O
some	O
applications	O
the	O
utility	B
matrix	O
is	O
highly	O
non-symmetric	O
.	O
consider	O
a	O
medical	O
scenario	O
in	O
which	O
we	O
are	O
asked	O
to	O
predict	O
whether	O
or	O
not	O
the	O
patient	O
has	O
cancer	O
dom	O
(	O
c	O
)	O
=	O
{	O
cancer	O
,	O
benign	O
}	O
.	O
if	O
the	O
true	O
class	O
is	O
cancer	O
yet	O
we	O
predict	O
benign	O
,	O
this	O
could	O
have	O
terrible	O
consequences	O
for	O
the	O
patient	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
the	O
class	O
is	O
benign	O
yet	O
we	O
predict	O
cancer	O
,	O
this	O
may	O
be	O
less	O
disastrous	O
for	O
the	O
patient	O
.	O
such	O
asymmetric	O
utilities	O
can	O
bias	B
the	O
predictions	O
in	O
favour	O
of	O
conservative	O
decisions	O
–	O
in	O
the	O
cancer	O
case	O
,	O
we	O
would	O
be	O
more	O
inclined	O
to	O
decide	O
the	O
sample	B
is	O
cancerous	O
than	O
benign	O
,	O
even	O
if	O
the	O
predictive	O
probability	O
of	O
the	O
two	O
classes	O
is	O
equal	O
.	O
13.2.2	O
what	O
’	O
s	O
the	O
catch	O
?	O
in	O
solving	B
for	O
the	O
optimal	O
decision	B
function	I
c	O
(	O
x∗	O
)	O
above	O
we	O
are	O
assuming	O
that	O
the	O
model	B
p	O
(	O
c|x	O
)	O
is	O
‘	O
correct	O
’	O
.	O
the	O
catch	O
is	O
therefore	O
that	O
in	O
practice	O
:	O
•	O
we	O
typically	O
don	O
’	O
t	O
know	O
the	O
correct	O
model	B
underlying	O
the	O
data	B
–	O
all	O
we	O
have	O
is	O
a	O
dataset	O
of	O
examples	O
d	O
=	O
{	O
(	O
xn	O
,	O
yn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
and	O
our	O
domain	B
knowledge	O
.	O
•	O
we	O
want	O
our	O
method	O
to	O
perform	O
well	O
not	O
just	O
on	O
a	O
speciﬁcally	O
chosen	O
x∗	O
,	O
but	O
any	O
new	O
input	O
that	O
could	O
come	O
along	O
–	O
that	O
is	O
we	O
want	O
it	O
to	O
generalise	O
to	O
novel	O
inputs	O
.	O
this	O
means	O
we	O
also	O
need	O
a	O
model	B
for	O
p	O
(	O
x	O
)	O
in	O
order	O
to	O
measure	O
what	O
the	O
expected	O
performance	B
of	O
our	O
decision	B
function	I
would	O
be	O
.	O
hence	O
we	O
require	O
knowledge	O
of	O
the	O
joint	B
distribution	O
p	O
(	O
c	O
,	O
x	O
)	O
=	O
p	O
(	O
c|x	O
)	O
p	O
(	O
x	O
)	O
.	O
we	O
therefore	O
need	O
to	O
form	O
a	O
distribution	B
p	O
(	O
x	O
,	O
c|d	O
)	O
which	O
should	O
ideally	O
be	O
close	O
to	O
the	O
true	O
but	O
unknown	O
joint	B
data	O
distribution	B
.	O
communities	O
of	O
researchers	O
in	O
machine	O
learning	B
form	O
around	O
diﬀerent	O
strategies	O
to	O
address	O
the	O
lack	O
of	O
knowledge	O
about	O
the	O
true	O
p	O
(	O
c	O
,	O
x	O
)	O
.	O
13.2.3	O
using	O
the	O
empirical	B
distribution	I
a	O
direct	O
approach	B
to	O
not	O
knowing	O
the	O
correct	O
model	B
ptrue	O
(	O
c	O
,	O
x	O
)	O
is	O
to	O
replace	O
it	O
with	O
the	O
empirical	B
distribution	I
p	O
(	O
x	O
,	O
c|d	O
)	O
=	O
1	O
n	O
n	O
(	O
cid:88	O
)	O
n=1	O
δ	O
(	O
x	O
,	O
xn	O
)	O
δ	O
(	O
c	O
,	O
cn	O
)	O
(	O
13.2.7	O
)	O
(	O
cid:88	O
)	O
1	O
n	O
that	O
is	O
,	O
we	O
assume	O
that	O
the	O
underlying	O
distribution	B
is	O
approximated	O
by	O
placing	O
equal	O
mass	O
on	O
each	O
of	O
the	O
points	O
(	O
xn	O
,	O
cn	O
)	O
in	O
the	O
dataset	O
.	O
using	O
this	O
gives	O
the	O
empirical	B
utility	O
(	O
cid:104	O
)	O
u	O
(	O
c	O
,	O
c	O
(	O
x	O
)	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
c	O
,	O
x|d	O
)	O
=	O
n	O
or	O
conversely	O
the	O
empirical	B
risk	I
(	O
cid:88	O
)	O
n	O
r	O
=	O
1	O
n	O
l	O
(	O
cn	O
,	O
c	O
(	O
xn	O
)	O
)	O
draft	O
march	O
9	O
,	O
2010	O
u	O
(	O
cn	O
,	O
c	O
(	O
xn	O
)	O
)	O
(	O
13.2.8	O
)	O
(	O
13.2.9	O
)	O
255	O
supervised	B
learning	I
train	O
validate	O
test	O
figure	O
13.2	O
:	O
models	O
can	O
be	O
trained	O
using	O
the	O
train	O
data	O
based	O
on	O
diﬀerent	O
regularisation	B
parameters	O
.	O
the	O
optimal	O
regularisation	B
parameter	O
is	O
determined	O
by	O
the	O
empirical	B
performance	O
on	O
the	O
validation	B
data	O
.	O
an	O
independent	O
measure	O
of	O
the	O
generalisation	B
performance	O
is	O
obtained	O
by	O
using	O
a	O
separate	O
test	B
set	I
.	O
assuming	O
the	O
loss	O
is	O
minimal	O
when	O
the	O
correct	O
class	O
is	O
predicted	O
,	O
the	O
optimal	O
decision	O
c	O
(	O
x	O
)	O
for	O
any	O
input	O
in	O
the	O
train	B
set	I
is	O
trivially	O
given	O
by	O
c	O
(	O
xn	O
)	O
=	O
cn	O
.	O
however	O
,	O
for	O
any	O
new	O
x∗	O
not	O
contained	O
in	O
d	O
then	O
c	O
(	O
x∗	O
)	O
is	O
undeﬁned	O
.	O
in	O
order	O
to	O
deﬁne	O
the	O
class	O
of	O
a	O
novel	O
input	O
,	O
one	O
may	O
use	O
a	O
parametric	O
function	B
for	O
example	O
for	O
a	O
two	O
class	O
problem	B
dom	O
(	O
c	O
)	O
=	O
{	O
1	O
,	O
2	O
}	O
,	O
a	O
linear	B
decision	O
function	B
is	O
given	O
by	O
c	O
(	O
x	O
)	O
=	O
f	O
(	O
x|θ	O
)	O
(	O
cid:26	O
)	O
1	O
if	O
θtx	O
+	O
θ0	O
≥	O
0	O
2	O
if	O
θtx	O
+	O
θ0	O
<	O
0	O
f	O
(	O
x|θ	O
)	O
=	O
if	O
the	O
vector	O
input	O
x	O
is	O
on	O
the	O
positive	O
side	O
of	O
a	O
hyperplane	B
deﬁned	O
by	O
the	O
vector	O
θ	O
and	O
bias	B
θ0	O
,	O
we	O
assign	O
it	O
to	O
class	O
1	O
,	O
otherwise	O
to	O
class	O
2	O
.	O
(	O
we	O
return	O
to	O
the	O
geometric	O
interpretation	O
of	O
this	O
in	O
chapter	O
(	O
17	O
)	O
)	O
.	O
the	O
empirical	B
risk	I
then	O
becomes	O
a	O
function	B
of	O
the	O
parameters	O
θ	O
=	O
{	O
θ	O
,	O
θ0	O
}	O
,	O
(	O
cid:88	O
)	O
n	O
r	O
(	O
θ|d	O
)	O
=	O
1	O
n	O
l	O
(	O
cn	O
,	O
f	O
(	O
xn|θ	O
)	O
)	O
the	O
optimal	O
parameters	O
θ	O
are	O
given	O
by	O
minimising	O
the	O
empirical	B
risk	I
with	O
respect	O
to	O
θ	O
,	O
θopt	O
=	O
argmin	O
θ	O
r	O
(	O
θ|d	O
)	O
the	O
decision	O
for	O
a	O
new	O
datapoint	O
x∗	O
is	O
then	O
given	O
by	O
f	O
(	O
x∗	O
|θopt	O
)	O
.	O
in	O
this	O
empirical	B
risk	I
minimisation	I
approach	O
,	O
as	O
we	O
make	O
the	O
decision	B
function	I
f	O
(	O
x|θ	O
)	O
more	O
complex	O
,	O
the	O
empirical	B
risk	I
goes	O
down	O
.	O
if	O
we	O
make	O
f	O
(	O
x|θ	O
)	O
too	O
complex	O
we	O
will	O
have	O
no	O
conﬁdence	O
f	O
(	O
x|θ	O
)	O
will	O
perform	O
well	O
on	O
a	O
novel	O
input	O
x∗	O
.	O
to	O
constrain	O
the	O
complexity	O
of	O
f	O
(	O
x|θ	O
)	O
we	O
may	O
minimise	O
the	O
penalised	B
empirical	O
risk	B
r	O
(	O
cid:48	O
)	O
(	O
θ|d	O
)	O
=	O
r	O
(	O
θ|d	O
)	O
+	O
λp	O
(	O
θ	O
)	O
for	O
the	O
linear	B
decision	O
function	B
above	O
,	O
it	O
is	O
reasonable	O
to	O
penalise	O
wildly	O
changing	O
classiﬁcations	O
in	O
the	O
sense	O
that	O
if	O
we	O
change	O
the	O
input	O
x	O
by	O
only	O
a	O
small	O
amount	O
we	O
expect	O
(	O
on	O
average	B
)	O
minimal	O
change	O
in	O
the	O
class	O
label	O
.	O
the	O
squared	O
diﬀerence	O
in	O
θtx	O
+	O
θ0	O
for	O
two	O
inputs	O
x1	O
and	O
x2	O
is	O
(	O
cid:0	O
)	O
θt∆x	O
(	O
cid:1	O
)	O
2	O
where	O
∆x	O
≡	O
x2	O
−	O
x1	O
.	O
by	O
constraining	O
the	O
length	O
of	O
θ	O
to	O
be	O
small	O
we	O
would	O
then	O
limit	O
the	O
ability	O
of	O
the	O
classiﬁer	B
to	O
change	O
class	O
for	O
only	O
a	O
small	O
change	O
in	O
input	O
space2	O
.	O
this	O
motivates	O
a	O
penalised	B
risk	O
of	O
the	O
form	O
(	O
13.2.10	O
)	O
(	O
13.2.11	O
)	O
(	O
13.2.12	O
)	O
(	O
13.2.13	O
)	O
(	O
13.2.14	O
)	O
(	O
13.2.15	O
)	O
r	O
(	O
cid:48	O
)	O
(	O
θ	O
,	O
θ0|d	O
)	O
=	O
r	O
(	O
θ	O
,	O
θ0|d	O
)	O
+	O
λθtθ	O
where	O
λ	O
is	O
a	O
regularising	O
constant	O
,	O
.	O
we	O
subsequently	O
minimise	O
this	O
penalised	B
empirical	O
risk	B
with	O
respect	O
to	O
θ	O
,	O
θ0	O
.	O
we	O
discuss	O
how	O
to	O
ﬁnd	O
an	O
appropriate	O
setting	O
for	O
the	O
regularisation	B
constant	O
λ	O
below	O
.	O
validation	B
in	O
penalised	B
empirical	O
risk	B
minimisation	O
we	O
need	O
to	O
set	O
the	O
regularisation	B
parameter	O
λ.	O
this	O
can	O
be	O
achieved	O
by	O
evaluating	O
the	O
performance	B
of	O
the	O
learned	O
classiﬁer	B
f	O
(	O
x|θ	O
)	O
on	O
validation	B
data	O
dvalidate	O
for	O
sev-	O
eral	O
diﬀerent	O
λ	O
values	O
,	O
and	O
choosing	O
the	O
one	O
with	O
the	O
best	O
performance	B
.	O
it	O
’	O
s	O
important	O
that	O
the	O
validation	B
data	O
is	O
not	O
the	O
data	B
on	O
which	O
the	O
model	B
was	O
trained	O
since	O
we	O
know	O
that	O
the	O
optimal	O
setting	O
for	O
λ	O
in	O
that	O
case	O
is	O
zero	O
,	O
and	O
again	O
we	O
will	O
have	O
no	O
conﬁdence	O
in	O
the	O
generalisation	B
ability	O
.	O
2assuming	O
the	O
distance	O
between	O
two	O
datapoints	O
is	O
distributed	O
according	O
to	O
an	O
isotropic	B
multivariate	O
gaussian	O
with	O
zero	O
=	O
σ2θtθ	O
,	O
motivating	O
the	O
choice	O
of	O
the	O
euclidean	O
squared	O
mean	O
and	O
covariance	B
σ2i	O
,	O
the	O
average	B
squared	O
change	O
is	O
length	O
of	O
the	O
parameter	B
θ	O
as	O
the	O
penalty	O
term	O
.	O
(	O
cid:68	O
)	O
(	O
cid:0	O
)	O
θt∆x	O
(	O
cid:1	O
)	O
2	O
(	O
cid:69	O
)	O
256	O
draft	O
march	O
9	O
,	O
2010	O
supervised	B
learning	I
algorithm	O
12	O
setting	O
regularisation	B
parameters	O
using	O
cross-validation	B
.	O
2	O
:	O
choose	O
a	O
set	O
of	O
training	B
and	O
validation	B
set	O
splits	O
(	O
cid:8	O
)	O
1	O
:	O
choose	O
a	O
set	O
of	O
regularisation	B
parameters	O
λ1	O
,	O
.	O
.	O
.	O
,	O
λa	O
di	O
train	O
,	O
di	O
(	O
cid:9	O
)	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
i	O
validate	O
3	O
:	O
for	O
a	O
=	O
1	O
to	O
a	O
do	O
4	O
:	O
5	O
:	O
for	O
i	O
=	O
1	O
to	O
i	O
do	O
a	O
=	O
argmin	O
θi	O
(	O
cid:2	O
)	O
r	O
(	O
θ|di	O
train	O
)	O
+	O
λap	O
(	O
θ	O
)	O
(	O
cid:3	O
)	O
θ	O
(	O
cid:80	O
)	O
i	O
l	O
(	O
λa	O
)	O
end	O
for	O
l	O
(	O
λa	O
)	O
=	O
1	O
i	O
6	O
:	O
7	O
:	O
8	O
:	O
end	O
for	O
9	O
:	O
λopt	O
=	O
argmin	O
λa	O
i=1	O
r	O
(	O
θi	O
validate	O
)	O
a|di	O
train	O
and	O
validation	B
di	O
given	O
an	O
original	O
dataset	O
d	O
we	O
split	O
this	O
into	O
disjoint	O
parts	O
,	O
dtrain	O
,	O
dvalidate	O
,	O
where	O
the	O
size	O
of	O
the	O
vali-	O
dation	O
set	O
is	O
usually	O
chosen	O
to	O
be	O
smaller	O
than	O
the	O
train	B
set	I
.	O
for	O
each	O
parameter	B
λa	O
one	O
then	O
ﬁnds	O
the	O
minimal	O
empirical	B
risk	I
parameter	O
θa	O
.	O
this	O
splitting	O
procedure	O
is	O
repeated	O
,	O
each	O
time	O
producing	O
a	O
separate	O
training	B
di	O
validation	B
set	O
,	O
along	O
with	O
an	O
optimal	O
penalised	B
empirical	O
risk	B
parameter	O
θi	O
a	O
and	O
associated	O
(	O
unregularised	O
)	O
validation	B
performance	O
r	O
(	O
θi	O
validate	O
)	O
.	O
the	O
performance	B
of	O
regularisation	B
parameter	O
λa	O
is	O
taken	O
as	O
the	O
average	B
of	O
the	O
validation	B
performances	O
over	O
i.	O
the	O
best	O
regularisation	B
param-	O
eter	O
is	O
then	O
given	O
as	O
that	O
with	O
the	O
minimal	O
average	B
validation	O
error	O
,	O
see	O
algorithm	O
(	O
12	O
)	O
and	O
ﬁg	O
(	O
13.2	O
)	O
.	O
using	O
the	O
optimal	O
regularisation	B
parameter	O
λ	O
,	O
many	O
practitioners	O
retrain	O
θ	O
on	O
the	O
basis	O
of	O
the	O
whole	O
dataset	O
d.	O
in	O
cross-validation	B
a	O
dataset	O
is	O
partitioned	B
into	O
training	B
and	O
validation	B
sets	O
multiple	O
times	O
with	O
validation	O
results	O
obtained	O
for	O
each	O
partition	O
.	O
more	O
speciﬁcally	O
,	O
in	O
k-fold	O
cross	B
validation	O
the	O
data	B
d	O
is	O
split	O
into	O
k	O
validate	O
.	O
this	O
gives	O
a	O
total	O
of	O
equal	O
sized	O
disjoint	O
parts	O
d1	O
,	O
.	O
.	O
.	O
,	O
dk	O
.	O
then	O
di	O
k	O
diﬀerent	O
training-validation	O
sets	O
over	O
which	O
performance	B
is	O
averaged	O
,	O
see	O
ﬁg	O
(	O
13.3	O
)	O
.	O
in	O
practice	O
10-fold	O
cross	B
validation	O
is	O
popular	O
,	O
as	O
is	O
leave-one-out	O
cross	B
validation	O
in	O
which	O
the	O
validation	B
sets	O
consist	O
of	O
only	O
a	O
single	O
example	O
.	O
validate	O
=	O
di	O
and	O
di	O
train	O
=	O
d\di	O
a|di	O
beneﬁts	O
of	O
the	O
empirical	B
risk	I
approach	O
for	O
a	O
utility	B
u	O
(	O
ctrue	O
,	O
cpred	O
)	O
and	O
penalty	O
p	O
(	O
θ	O
)	O
,	O
the	O
empirical	B
risk	I
approach	O
is	O
summarised	O
in	O
ﬁg	O
(	O
13.4	O
)	O
.	O
•	O
in	O
the	O
limit	O
of	O
a	O
large	O
amount	O
of	O
training	B
data	O
the	O
empirical	B
distribution	I
will	O
tend	O
towards	O
the	O
correct	O
distribution	B
.	O
•	O
the	O
discriminant	O
function	B
is	O
chosen	O
on	O
the	O
basis	O
of	O
minimal	O
risk	B
,	O
which	O
is	O
the	O
quantity	O
we	O
are	O
ultimately	O
interested	O
in	O
.	O
•	O
the	O
procedure	O
is	O
conceptually	O
straightforward	O
.	O
train	O
validate	O
train	O
validate	O
train	O
validate	O
train	O
test	O
draft	O
march	O
9	O
,	O
2010	O
figure	O
13.3	O
:	O
in	O
cross-validation	B
the	O
original	O
dataset	O
is	O
split	O
into	O
several	O
train-validation	O
sets	O
.	O
depicted	O
is	O
3-fold	O
cross-validation	B
.	O
for	O
a	O
range	O
of	O
regularisation	B
parameters	O
,	O
the	O
optimal	O
regular-	O
isation	O
parameter	B
is	O
found	O
based	O
on	O
the	O
empirical	B
validation	O
performance	B
averaged	O
across	O
the	O
diﬀerent	O
splits	O
.	O
257	O
x	O
,	O
c	O
p	O
(	O
x	O
,	O
c	O
)	O
x∗	O
θ	O
c∗	O
=	O
f	O
(	O
x∗	O
|θ	O
)	O
u	O
(	O
c	O
,	O
f	O
(	O
x|θ	O
)	O
)	O
−	O
λp	O
(	O
θ	O
)	O
supervised	B
learning	I
figure	O
13.4	O
:	O
empirical	B
risk	I
approach	O
.	O
given	O
the	O
dataset	O
x	O
,	O
c	O
,	O
a	O
model	B
of	O
the	O
data	B
p	O
(	O
x	O
,	O
c	O
)	O
is	O
made	O
,	O
usually	O
using	O
the	O
empirical	B
distribu-	O
tion	O
.	O
for	O
a	O
classiﬁer	B
f	O
(	O
x|θ	O
)	O
,	O
the	O
parameter	B
θ	O
is	O
learned	O
by	O
maximising	O
the	O
penalised	B
empir-	O
ical	O
utility	O
(	O
or	O
minimising	O
empirical	B
risk	I
)	O
with	O
respect	O
to	O
θ.	O
the	O
penalty	O
parameter	B
λ	O
is	O
set	O
by	O
validation	B
.	O
a	O
novel	O
input	O
x∗	O
is	O
then	O
assigned	O
to	O
class	O
f	O
(	O
x∗	O
|θ	O
)	O
,	O
given	O
this	O
optimal	O
θ	O
.	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
13.5	O
:	O
(	O
a	O
)	O
:	O
the	O
unregularised	O
ﬁt	O
(	O
λ	O
=	O
0	O
)	O
to	O
training	B
given	O
by	O
×	O
.	O
whilst	O
the	O
training	B
data	O
is	O
well	O
(	O
b	O
)	O
:	O
the	O
regularised	B
ﬁt	O
(	O
λ	O
=	O
0.5	O
)	O
.	O
whilst	O
the	O
ﬁtted	O
,	O
the	O
error	O
on	O
the	O
validation	B
examples	O
,	O
+	O
is	O
high	O
.	O
train	O
error	O
is	O
high	O
,	O
the	O
validation	B
error	O
(	O
which	O
is	O
all	O
important	O
)	O
is	O
low	O
.	O
the	O
true	O
function	B
which	O
generated	O
this	O
noisy	O
data	O
is	O
the	O
dashed	O
line	O
;	O
the	O
function	B
learned	O
from	O
the	O
data	B
is	O
given	O
by	O
the	O
solid	O
line	O
.	O
drawbacks	O
of	O
the	O
empirical	B
risk	I
approach	O
•	O
it	O
seems	O
extreme	O
to	O
assume	O
that	O
the	O
data	B
follows	O
the	O
empirical	B
distribution	I
,	O
particularly	O
for	O
small	O
amounts	O
of	O
training	B
data	O
.	O
to	O
generalise	O
well	O
,	O
we	O
need	O
to	O
make	O
sensible	O
assumptions	O
as	O
to	O
p	O
(	O
x	O
)	O
–	O
that	O
is	O
the	O
distribution	B
for	O
all	O
x	O
that	O
could	O
arise	O
.	O
•	O
if	O
the	O
utility	B
(	O
or	O
loss	O
)	O
function	B
changes	O
,	O
the	O
discriminant	O
function	B
needs	O
to	O
be	O
retrained	O
.	O
•	O
some	O
problems	O
require	O
an	O
estimate	O
of	O
the	O
conﬁdence	O
of	O
the	O
prediction	B
.	O
whilst	O
there	O
may	O
be	O
heuristic	O
ways	O
to	O
evaluating	O
conﬁdence	O
in	O
the	O
prediction	B
,	O
this	O
is	O
not	O
inherent	O
in	O
the	O
framework	O
.	O
•	O
when	O
there	O
are	O
multiple	O
penalty	O
parameters	O
,	O
performing	O
cross	B
validation	O
in	O
a	O
discretised	O
grid	O
of	O
the	O
parameters	O
becomes	O
infeasible	O
.	O
•	O
it	O
seems	O
a	O
shame	O
to	O
discard	O
all	O
those	O
trained	O
models	O
in	O
cross-validation	B
–	O
can	O
’	O
t	O
they	O
be	O
combined	O
in	O
some	O
manner	O
and	O
used	O
to	O
make	O
a	O
better	O
predictor	O
?	O
example	O
60	O
(	O
finding	O
a	O
good	O
regularisation	B
parameter	O
)	O
.	O
in	O
ﬁg	O
(	O
13.5	O
)	O
,	O
we	O
ﬁt	O
the	O
function	B
a	O
sin	O
(	O
wx	O
)	O
to	O
data	B
,	O
learning	B
the	O
parameters	O
a	O
and	O
w.	O
the	O
unregularised	O
solution	O
ﬁg	O
(	O
13.5a	O
)	O
badly	O
overﬁts	O
the	O
data	B
,	O
and	O
has	O
a	O
high	O
validation	O
error	O
.	O
to	O
encourage	O
a	O
smoother	O
solution	O
,	O
a	O
regularisation	B
term	O
ereg	O
=	O
w2	O
is	O
used	O
.	O
the	O
validation	B
error	O
based	O
on	O
several	O
diﬀerent	O
values	O
of	O
the	O
regularisation	B
parameter	O
λ	O
was	O
computed	O
,	O
ﬁnding	O
that	O
λ	O
=	O
0.5	O
gave	O
a	O
low	O
validation	O
error	O
.	O
the	O
resulting	O
ﬁt	O
to	O
novel	O
data	B
,	O
ﬁg	O
(	O
13.5b	O
)	O
is	O
reasonable	O
.	O
13.2.4	O
bayesian	O
decision	O
approach	O
an	O
alternative	O
to	O
using	O
the	O
empirical	B
distribution	I
is	O
to	O
ﬁt	O
a	O
model	B
p	O
(	O
c	O
,	O
x|θ	O
)	O
to	O
the	O
train	O
data	O
d.	O
given	O
this	O
model	B
,	O
the	O
decision	B
function	I
c	O
(	O
x	O
)	O
is	O
automatically	O
determined	O
from	O
the	O
maximal	O
expected	O
utility	B
(	O
or	O
258	O
draft	O
march	O
9	O
,	O
2010	O
−3−2−10123−1.5−1−0.500.511.5−3−2−10123−1.5−1−0.500.511.5	O
supervised	B
learning	I
x	O
,	O
c	O
θ	O
p	O
(	O
x	O
,	O
c|θ	O
)	O
x∗	O
p	O
(	O
c|x∗	O
,	O
θ	O
)	O
u	O
(	O
c	O
,	O
c∗	O
)	O
c∗	O
figure	O
13.6	O
:	O
bayesian	O
decision	O
approach	O
.	O
a	O
model	B
p	O
(	O
x	O
,	O
c|θ	O
)	O
is	O
ﬁtted	O
to	O
the	O
data	B
.	O
after	O
lean-	O
ing	O
,	O
this	O
model	B
is	O
used	O
to	O
compute	O
p	O
(	O
c|x	O
,	O
θ	O
)	O
.	O
for	O
a	O
novel	O
x∗	O
,	O
we	O
then	O
ﬁnd	O
the	O
distribution	B
of	O
the	O
assumed	O
‘	O
truth	O
’	O
,	O
p	O
(	O
c|x∗	O
,	O
θ	O
)	O
.	O
the	O
prediction	B
(	O
de-	O
cision	O
)	O
is	O
then	O
given	O
by	O
that	O
c∗	O
which	O
maximises	O
the	O
expected	O
utility	B
(	O
cid:104	O
)	O
u	O
(	O
c	O
,	O
c∗	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
c|x∗	O
,	O
θ	O
)	O
.	O
minimal	O
risk	B
)	O
,	O
with	O
respect	O
to	O
this	O
model	B
,	O
as	O
in	O
equation	B
(	O
13.2.5	O
)	O
,	O
in	O
which	O
the	O
unknown	O
p	O
(	O
ctrue|x	O
)	O
is	O
replaced	O
with	O
p	O
(	O
c|x	O
,	O
θ	O
)	O
.	O
this	O
approach	B
therefore	O
divorces	O
learning	B
the	O
parameters	O
θ	O
of	O
p	O
(	O
c	O
,	O
x|θ	O
)	O
from	O
the	O
utility	B
(	O
or	O
loss	O
)	O
.	O
beneﬁts	O
of	O
the	O
bayesian	O
decision	O
approach	O
•	O
this	O
is	O
a	O
conceptually	O
‘	O
clean	O
’	O
approach	B
,	O
in	O
which	O
one	O
tries	O
ones	O
best	O
to	O
model	B
the	O
environment	O
,	O
independent	O
of	O
the	O
subsequent	O
decision	O
process	O
.	O
in	O
this	O
case	O
learning	B
the	O
environment	O
is	O
separated	O
from	O
the	O
ultimate	O
eﬀect	O
this	O
will	O
have	O
on	O
the	O
expected	O
utility	B
.	O
•	O
the	O
ultimate	O
decision	O
c∗	O
for	O
a	O
novel	O
input	O
x∗	O
can	O
be	O
a	O
highly	O
complex	O
function	B
of	O
x∗	O
due	O
to	O
the	O
maximisation	B
operation	O
.	O
drawbacks	O
of	O
the	O
bayesian	O
decision	O
approach	O
•	O
if	O
the	O
environment	O
model	B
p	O
(	O
c	O
,	O
x|θ	O
)	O
is	O
poor	O
,	O
the	O
prediction	B
c∗	O
could	O
be	O
highly	O
inaccurate	O
since	O
mod-	O
elling	O
the	O
environment	O
is	O
divorced	O
from	O
prediction	B
.	O
•	O
to	O
avoid	O
fully	O
divorcing	O
the	O
learning	B
of	O
the	O
model	B
p	O
(	O
c	O
,	O
x|θ	O
)	O
from	O
its	O
eﬀect	O
on	O
decisions	O
,	O
in	O
practice	O
one	O
often	O
includes	O
regularisation	B
terms	O
in	O
the	O
environment	O
model	B
p	O
(	O
c	O
,	O
x|θ	O
)	O
which	O
are	O
set	O
by	O
validation	B
based	O
on	O
an	O
empirical	B
utility	O
.	O
there	O
are	O
two	O
main	O
approaches	O
to	O
ﬁtting	O
p	O
(	O
c	O
,	O
x|θ	O
)	O
to	O
data	B
d.	O
we	O
could	O
parameterise	O
the	O
joint	B
distribution	O
using	O
p	O
(	O
c	O
,	O
x|θ	O
)	O
=	O
p	O
(	O
c|x	O
,	O
θc|x	O
)	O
p	O
(	O
x|θx	O
)	O
discriminative	B
approach	I
or	O
p	O
(	O
c	O
,	O
x|θ	O
)	O
=	O
p	O
(	O
x|c	O
,	O
θx|c	O
)	O
p	O
(	O
c|θc	O
)	O
generative	B
approach	I
(	O
13.2.16	O
)	O
(	O
13.2.17	O
)	O
we	O
’	O
ll	O
consider	O
these	O
two	O
approaches	O
below	O
in	O
the	O
context	O
of	O
trying	O
to	O
make	O
a	O
system	B
that	O
can	O
distinguish	O
between	O
a	O
male	O
and	O
female	O
face	O
.	O
we	O
have	O
a	O
database	O
of	O
face	O
images	O
in	O
which	O
each	O
image	O
is	O
represented	O
as	O
a	O
real-valued	O
vector	O
xn	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
)	O
,	O
along	O
with	O
a	O
label	O
cn	O
∈	O
{	O
0	O
,	O
1	O
}	O
stating	O
if	O
the	O
image	O
is	O
male	O
or	O
female	O
.	O
generative	B
approach	I
p	O
(	O
x	O
,	O
c|θ	O
)	O
=	O
p	O
(	O
x|c	O
,	O
θx|c	O
)	O
p	O
(	O
c|θc	O
)	O
for	O
simplicity	O
we	O
use	O
maximum	B
likelihood	I
training	O
for	O
the	O
parameters	O
θ.	O
assuming	O
the	O
data	B
d	O
is	O
i.i.d.	O
,	O
we	O
have	O
a	O
log	O
likelihood	B
log	O
p	O
(	O
d|θ	O
)	O
=	O
(	O
cid:88	O
)	O
log	O
p	O
(	O
xn|cn	O
,	O
θx|c	O
)	O
+	O
(	O
cid:88	O
)	O
n	O
n	O
log	O
p	O
(	O
cn|θc	O
)	O
(	O
13.2.18	O
)	O
as	O
we	O
see	O
the	O
dependence	B
on	O
θx|c	O
occurs	O
only	O
in	O
the	O
ﬁrst	O
term	O
,	O
and	O
θc	O
only	O
occurs	O
in	O
the	O
second	O
.	O
this	O
means	O
that	O
learning	B
the	O
optimal	O
parameters	O
is	O
equivalent	B
to	O
isolating	O
the	O
data	B
for	O
the	O
male-class	O
and	O
draft	O
march	O
9	O
,	O
2010	O
259	O
supervised	B
learning	I
θc	O
cn	O
θx|c	O
θc|x	O
θx	O
xn	O
n	O
cn	O
xn	O
n	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
13.7	O
:	O
two	O
generic	O
strategies	O
for	O
probabilis-	O
(	O
a	O
)	O
:	O
class	O
dependent	O
generative	B
tic	O
classiﬁcation	B
.	O
model	B
of	O
x.	O
after	O
learning	B
parameters	O
,	O
classiﬁca-	O
tion	O
is	O
obtained	O
by	O
making	O
x	O
evidential	O
and	O
inferring	O
(	O
b	O
)	O
:	O
a	O
discriminative	B
classiﬁcation	O
method	O
p	O
(	O
c|x	O
)	O
.	O
p	O
(	O
c|x	O
)	O
.	O
ﬁtting	O
a	O
model	B
p	O
(	O
x|c	O
=	O
male	O
,	O
θx|male	O
)	O
.	O
we	O
similarly	O
isolate	O
the	O
female	O
data	B
and	O
ﬁt	O
a	O
separate	O
model	B
p	O
(	O
x|c	O
=	O
female	O
,	O
θx|female	O
)	O
.	O
the	O
class	O
distribution	B
p	O
(	O
c|θc	O
)	O
can	O
be	O
easily	O
set	O
according	O
to	O
the	O
ratio	O
of	O
males/females	O
in	O
the	O
set	O
of	O
training	B
data	O
.	O
to	O
make	O
a	O
classiﬁcation	B
of	O
a	O
new	O
image	O
x∗	O
as	O
either	O
male	O
or	O
female	O
,	O
we	O
may	O
use	O
p	O
(	O
c	O
=	O
male|x∗	O
)	O
=	O
p	O
(	O
x∗	O
,	O
c	O
=	O
male|θx|male	O
)	O
p	O
(	O
x∗	O
,	O
c	O
=	O
male|θx|male	O
)	O
+	O
p	O
(	O
x∗	O
,	O
c	O
=	O
female|θx|female	O
)	O
(	O
13.2.19	O
)	O
based	O
on	O
zero-one	B
loss	I
,	O
if	O
this	O
probability	B
is	O
greater	O
than	O
0.5	O
we	O
classify	O
x∗	O
as	O
male	O
,	O
otherwise	O
female	O
.	O
more	O
generally	O
,	O
we	O
may	O
use	O
this	O
probability	B
as	O
part	O
of	O
a	O
decision	O
process	O
,	O
as	O
in	O
equation	B
(	O
13.2.5	O
)	O
.	O
advantages	O
prior	B
information	O
about	O
the	O
structure	B
of	O
the	O
data	B
is	O
often	O
most	O
naturally	O
speciﬁed	O
through	O
a	O
generative	B
model	O
p	O
(	O
x|c	O
)	O
.	O
for	O
example	O
,	O
for	O
male	O
faces	B
,	O
we	O
would	O
expect	O
to	O
see	O
heavier	O
eyebrows	O
,	O
a	O
squarer	O
jaw	O
,	O
etc	O
.	O
disadvantages	O
the	O
generative	B
approach	I
does	O
not	O
directly	O
target	O
the	O
classiﬁcation	B
model	O
p	O
(	O
c|x	O
)	O
since	O
the	O
goal	O
of	O
generative	B
training	O
is	O
rather	O
to	O
model	B
p	O
(	O
x|c	O
)	O
.	O
if	O
the	O
data	B
x	O
is	O
complex	O
,	O
ﬁnding	O
a	O
suitable	O
generative	B
data	O
model	B
p	O
(	O
x|c	O
)	O
is	O
a	O
diﬃcult	O
task	O
.	O
on	O
the	O
other	O
hand	O
it	O
might	O
be	O
that	O
making	O
a	O
model	B
of	O
p	O
(	O
c|x	O
)	O
is	O
simpler	O
,	O
particularly	O
if	O
the	O
decision	B
boundary	I
between	O
the	O
classes	O
has	O
a	O
simple	O
form	O
,	O
even	O
if	O
the	O
data	B
distribution	O
of	O
each	O
class	O
is	O
complex	O
,	O
see	O
ﬁg	O
(	O
13.8	O
)	O
.	O
furthermore	O
,	O
since	O
each	O
generative	B
model	O
is	O
separately	O
trained	O
for	O
each	O
class	O
,	O
there	O
is	O
no	O
competition	O
amongst	O
the	O
models	O
to	O
explain	O
the	O
data	B
.	O
discriminative	B
approach	I
p	O
(	O
c	O
,	O
x	O
)	O
=	O
p	O
(	O
c|x	O
,	O
θc|x	O
)	O
p	O
(	O
x|θx	O
)	O
assuming	O
i.i.d	O
.	O
data	B
,	O
the	O
log	O
likelihood	B
is	O
log	O
p	O
(	O
d|θ	O
)	O
=	O
(	O
cid:88	O
)	O
log	O
p	O
(	O
cn|xn	O
,	O
θc|x	O
)	O
+	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
n	O
n	O
n	O
log	O
p	O
(	O
xn|θx	O
)	O
(	O
13.2.20	O
)	O
the	O
parameters	O
are	O
isolated	O
in	O
the	O
two	O
terms	O
so	O
that	O
maximum	B
likelihood	I
training	O
is	O
equivalent	B
to	O
ﬁnding	O
the	O
parameters	O
of	O
θc|x	O
that	O
will	O
best	O
predict	O
the	O
class	O
c	O
for	O
a	O
given	O
training	B
input	O
x.	O
the	O
parameters	O
θx	O
for	O
modelling	B
the	O
data	B
occur	O
separately	O
in	O
the	O
second	O
term	O
above	O
,	O
and	O
setting	O
them	O
can	O
therefore	O
be	O
treated	O
as	O
a	O
separate	O
unsupervised	B
learning	I
problem	O
.	O
this	O
approach	B
therefore	O
isolates	O
modelling	B
the	O
decision	B
boundary	I
from	O
modelling	B
the	O
input	O
distribution	B
,	O
see	O
ﬁg	O
(	O
13.8	O
)	O
.	O
classiﬁcation	B
of	O
a	O
new	O
point	O
x∗	O
is	O
based	O
on	O
p	O
(	O
c|x	O
,	O
θopt	O
c|x	O
)	O
(	O
13.2.21	O
)	O
as	O
for	O
the	O
generative	B
case	O
,	O
this	O
approach	B
still	O
learns	O
a	O
joint	B
distribution	O
p	O
(	O
c	O
,	O
x	O
)	O
=	O
p	O
(	O
c|x	O
)	O
p	O
(	O
x	O
)	O
which	O
can	O
be	O
used	O
as	O
part	O
of	O
a	O
decision	O
process	O
if	O
required	O
.	O
advantages	O
the	O
discriminative	B
approach	I
directly	O
addresses	O
making	O
an	O
accurate	O
classiﬁer	B
based	O
on	O
p	O
(	O
c|x	O
)	O
,	O
modelling	B
the	O
decision	B
boundary	I
,	O
as	O
opposed	O
to	O
the	O
class	O
conditional	B
data	O
distribution	B
in	O
the	O
gener-	O
ative	O
approach	B
.	O
whilst	O
the	O
data	B
from	O
each	O
class	O
may	O
be	O
distributed	O
in	O
a	O
complex	O
way	O
,	O
it	O
could	O
be	O
that	O
the	O
decision	B
boundary	I
between	O
them	O
is	O
relatively	O
easy	O
to	O
model	B
.	O
260	O
draft	O
march	O
9	O
,	O
2010	O
supervised	B
learning	I
figure	O
13.8	O
:	O
each	O
point	O
represents	O
a	O
high	O
dimensional	O
vector	O
with	O
an	O
associated	O
class	O
label	O
,	O
either	O
male	O
or	O
female	O
.	O
the	O
point	O
x∗	O
is	O
a	O
new	O
point	O
for	O
which	O
we	O
would	O
like	O
to	O
predict	O
whether	O
this	O
should	O
be	O
male	O
or	O
female	O
.	O
in	O
the	O
generative	B
approach	I
,	O
a	O
male	O
model	B
p	O
(	O
x|male	O
)	O
generates	O
data	B
similar	O
to	O
the	O
‘	O
m	O
’	O
points	O
.	O
similarly	O
,	O
the	O
female	O
model	B
p	O
(	O
x|female	O
)	O
generates	O
points	O
that	O
are	O
similar	O
to	O
the	O
‘	O
f	O
’	O
points	O
above	O
.	O
we	O
then	O
use	O
bayes	O
’	O
rule	O
to	O
calculate	O
the	O
probability	B
p	O
(	O
male|x∗	O
)	O
using	O
the	O
two	O
we	O
directly	O
make	O
a	O
model	B
of	O
p	O
(	O
male|x∗	O
)	O
,	O
which	O
cares	O
less	O
about	O
how	O
the	O
points	O
‘	O
m	O
’	O
or	O
‘	O
f	O
’	O
are	O
distributed	O
,	O
but	O
more	O
about	O
describing	O
the	O
boundary	B
which	O
can	O
separate	O
the	O
two	O
classes	O
,	O
as	O
given	O
by	O
the	O
line	O
.	O
ﬁtted	O
models	O
,	O
as	O
given	O
in	O
the	O
text	O
.	O
in	O
the	O
discriminative	B
approach	I
,	O
θc|h	O
θh	O
θx|h	O
cn	O
hn	O
xn	O
n	O
figure	O
13.9	O
:	O
a	O
strategy	O
for	O
semi-supervised	B
learning	I
.	O
when	O
cn	O
is	O
missing	B
,	O
the	O
term	O
p	O
(	O
cn|hn	O
)	O
is	O
absent	O
.	O
the	O
large	O
amount	O
of	O
training	B
data	O
helps	O
the	O
model	B
learn	O
a	O
good	O
lower	O
dimension/compressed	O
rep-	O
resentation	O
h	O
of	O
the	O
data	B
x.	O
fitting	O
then	O
a	O
classiﬁcation	B
model	O
p	O
(	O
c|h	O
)	O
using	O
this	O
lower	O
dimensional	O
representation	O
may	O
be	O
much	O
easier	O
than	O
ﬁtting	O
a	O
model	B
directly	O
from	O
the	O
complex	O
data	B
to	O
the	O
class	O
,	O
p	O
(	O
c|x	O
)	O
.	O
disadvantages	O
discriminative	B
approaches	O
are	O
usually	O
trained	O
as	O
‘	O
black-box	B
’	O
classiﬁers	O
,	O
with	O
little	O
prior	B
knowledge	O
built	O
in	O
as	O
to	O
how	O
data	B
for	O
a	O
given	O
class	O
might	O
look	O
.	O
domain	B
knowledge	O
is	O
often	O
more	O
easily	O
expressed	O
using	O
the	O
generative	B
framework	O
.	O
hybrid	O
generative-discriminative	O
approaches	O
one	O
could	O
use	O
a	O
generative	B
description	O
,	O
p	O
(	O
x|c	O
)	O
,	O
building	O
in	O
prior	B
information	O
,	O
and	O
use	O
this	O
to	O
form	O
a	O
joint	B
distribution	O
p	O
(	O
x	O
,	O
c	O
)	O
,	O
from	O
which	O
a	O
discriminative	B
model	O
p	O
(	O
c|x	O
)	O
may	O
be	O
formed	O
,	O
using	O
bayes	O
’	O
rule	O
.	O
speciﬁcally	O
,	O
we	O
can	O
use	O
(	O
cid:80	O
)	O
p	O
(	O
x|c	O
,	O
θx|c	O
)	O
p	O
(	O
c|θc	O
)	O
c	O
p	O
(	O
x|c	O
,	O
θx|c	O
)	O
p	O
(	O
c|θc	O
)	O
p	O
(	O
c|x	O
,	O
θ	O
)	O
=	O
and	O
use	O
a	O
separate	O
model	B
for	O
p	O
(	O
x|θx	O
)	O
.	O
subsequently	O
the	O
parameters	O
θ	O
=	O
(	O
cid:0	O
)	O
θx|c	O
,	O
θc	O
(	O
13.2.22	O
)	O
(	O
cid:1	O
)	O
,	O
for	O
this	O
hybrid	O
model	O
can	O
be	O
found	O
by	O
maximising	O
the	O
probability	B
of	O
being	O
in	O
the	O
correct	O
class	O
.	O
this	O
approach	B
would	O
appear	O
to	O
leverage	O
the	O
advantages	O
of	O
both	O
the	O
discriminative	B
and	O
generative	B
frameworks	O
since	O
we	O
can	O
more	O
readily	O
incorporate	O
domain	B
knowledge	O
in	O
the	O
generative	B
model	O
p	O
(	O
x|c	O
,	O
θx|c	O
)	O
yet	O
train	O
this	O
in	O
a	O
discriminative	B
way	O
.	O
this	O
approach	B
is	O
rarely	O
taken	O
in	O
practice	O
since	O
the	O
resulting	O
functional	O
form	O
of	O
the	O
likelihood	B
depends	O
in	O
a	O
complex	O
manner	O
on	O
the	O
parameters	O
.	O
in	O
this	O
case	O
no	O
separation	O
occurs	O
(	O
as	O
was	O
previously	O
the	O
case	O
for	O
the	O
generative	B
and	O
discriminative	B
approaches	O
)	O
.	O
13.2.5	O
learning	B
lower-dimensional	O
representations	O
in	O
semi-supervised	B
learning	I
one	O
way	O
to	O
exploit	O
a	O
large	O
amount	O
of	O
unlabelled	B
training	O
data	B
to	O
improve	O
classiﬁcation	B
modelling	O
is	O
to	O
try	O
to	O
ﬁnd	O
a	O
lower	O
dimensional	O
representation	O
h	O
of	O
the	O
data	B
x.	O
based	O
on	O
this	O
,	O
the	O
mapping	O
from	O
h	O
to	O
c	O
may	O
be	O
rather	O
simpler	O
to	O
learn	O
than	O
a	O
mapping	O
from	O
x	O
to	O
c	O
directly	O
.	O
to	O
do	O
so	O
we	O
can	O
form	O
the	O
likelihood	B
using	O
,	O
see	O
ﬁg	O
(	O
13.9	O
)	O
,	O
p	O
(	O
c	O
,	O
x	O
,	O
h|θ	O
)	O
=	O
(	O
cid:89	O
)	O
(	O
cid:8	O
)	O
p	O
(	O
cn|hn	O
,	O
θc|h	O
)	O
(	O
cid:9	O
)	O
i	O
[	O
cn	O
(	O
cid:54	O
)	O
=∅	O
]	O
(	O
cid:88	O
)	O
n	O
θopt	O
=	O
argmax	O
p	O
(	O
c	O
,	O
x	O
,	O
h|θ	O
)	O
θ	O
h	O
draft	O
march	O
9	O
,	O
2010	O
and	O
then	O
set	O
any	O
parameters	O
for	O
example	O
by	O
using	O
maximum	B
likelihood	I
p	O
(	O
xn|hn	O
,	O
θx|h	O
)	O
p	O
(	O
h|θh	O
)	O
(	O
13.2.23	O
)	O
(	O
13.2.24	O
)	O
261	O
*mmmmmmmffffffffffx	O
bayes	O
versus	O
empirical	B
decisions	O
13.2.6	O
features	O
and	O
preprocessing	O
it	O
is	O
often	O
the	O
case	O
that	O
when	O
attempting	O
to	O
make	O
a	O
predictive	O
model	O
,	O
transforming	O
the	O
raw	O
input	O
x	O
into	O
a	O
form	O
that	O
more	O
directly	O
captures	O
the	O
relevant	O
label	O
information	O
can	O
greatly	O
improve	O
performance	B
.	O
for	O
example	O
,	O
in	O
the	O
male-female	O
classiﬁcation	B
case	O
,	O
it	O
might	O
be	O
that	O
building	O
a	O
classiﬁer	B
directly	O
in	O
terms	O
of	O
the	O
elements	O
of	O
the	O
face	O
vector	O
x	O
is	O
diﬃcult	O
.	O
however	O
,	O
using	O
‘	O
features	O
’	O
which	O
contain	O
geometric	O
information	O
such	O
as	O
the	O
distance	O
between	O
eyes	O
,	O
width	O
of	O
mouth	O
,	O
etc	O
.	O
may	O
make	O
ﬁnding	O
a	O
classiﬁer	B
easier	O
.	O
in	O
practice	O
data	B
is	O
often	O
preprocessed	O
to	O
remove	O
noise	O
,	O
centre	O
an	O
image	O
etc	O
.	O
13.3	O
bayes	O
versus	O
empirical	B
decisions	O
the	O
empirical	B
risk	I
and	O
bayesian	O
approaches	O
are	O
at	O
the	O
extremes	O
of	O
the	O
philosophical	O
spectrum	B
.	O
in	O
the	O
empirical	B
risk	I
approach	O
one	O
makes	O
a	O
seemingly	O
over-simplistic	O
data	B
generating	O
assumption	O
.	O
however	O
decision	B
function	I
parameters	O
are	O
set	O
based	O
on	O
the	O
task	O
of	O
making	O
decisions	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
bayesian	O
approach	B
attempts	O
to	O
learn	O
p	O
(	O
c	O
,	O
x	O
)	O
without	O
regard	O
to	O
its	O
ultimate	O
use	O
as	O
part	O
of	O
a	O
larger	O
decision	O
process	O
.	O
what	O
‘	O
objective	O
’	O
criterion	O
can	O
we	O
use	O
to	O
learn	O
p	O
(	O
c	O
,	O
x	O
)	O
,	O
particularly	O
if	O
we	O
are	O
only	O
interested	O
in	O
classiﬁcation	B
with	O
a	O
low	O
test-risk	O
?	O
the	O
following	O
example	O
is	O
intended	O
to	O
recapitulate	O
the	O
two	O
generic	O
bayes	O
and	O
empirical	B
risk	I
approaches	O
we	O
’	O
ve	O
been	O
considering	O
.	O
example	O
61	O
(	O
the	O
two	O
generic	O
decision	O
strategies	O
)	O
.	O
consider	O
a	O
situation	O
in	O
which	O
,	O
based	O
on	O
patient	O
information	O
x	O
,	O
we	O
need	O
to	O
take	O
a	O
decision	O
d	O
as	O
whether	O
or	O
not	O
to	O
operate	O
.	O
the	O
utility	B
of	O
operating	O
u	O
(	O
d	O
,	O
c	O
)	O
depends	O
on	O
whether	O
or	O
not	O
the	O
patient	O
has	O
cancer	O
.	O
for	O
example	O
u	O
(	O
operate	O
,	O
cancer	O
)	O
=	O
100	O
u	O
(	O
don	O
’	O
t	O
operate	O
,	O
cancer	O
)	O
=	O
0	O
u	O
(	O
don	O
’	O
t	O
operate	O
,	O
benign	O
)	O
=	O
70	O
u	O
(	O
operate	O
,	O
benign	O
)	O
=	O
30	O
(	O
13.3.1	O
)	O
we	O
have	O
independent	O
true	O
assessments	O
of	O
whether	O
or	O
not	O
a	O
patient	O
had	O
cancer	O
,	O
giving	O
rise	O
to	O
a	O
set	O
of	O
historical	O
records	O
d	O
=	O
{	O
(	O
xn	O
,	O
cn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
.	O
faced	O
with	O
a	O
new	O
patient	O
with	O
information	O
x	O
,	O
we	O
need	O
to	O
make	O
a	O
decision	O
whether	O
or	O
not	O
to	O
operate	O
.	O
in	O
the	O
bayesian	O
decision	O
approach	O
one	O
would	O
ﬁrst	O
make	O
a	O
model	B
p	O
(	O
c|x	O
,	O
d	O
)	O
(	O
for	O
example	O
logistic	B
regression	I
)	O
.	O
using	O
this	O
model	B
the	O
decision	O
is	O
given	O
by	O
that	O
which	O
maximises	O
the	O
expected	O
utility	B
d	O
=	O
argmax	O
d	O
[	O
p	O
(	O
cancer|x	O
,	O
d	O
)	O
u	O
(	O
d	O
,	O
cancer	O
)	O
+	O
p	O
(	O
benign|x	O
,	O
d	O
)	O
u	O
(	O
d	O
,	O
benign	O
)	O
]	O
(	O
13.3.2	O
)	O
in	O
this	O
approach	B
learning	O
the	O
model	B
p	O
(	O
c|x	O
,	O
d	O
)	O
is	O
divorced	O
from	O
the	O
ultimate	O
use	O
of	O
the	O
model	B
in	O
the	O
decision	O
making	O
process	O
.	O
an	O
advantage	O
of	O
this	O
approach	B
is	O
that	O
,	O
from	O
the	O
viewpoint	O
of	O
expected	O
utility	B
,	O
it	O
is	O
optimal	O
–	O
provided	O
the	O
model	B
p	O
(	O
c|x	O
,	O
d	O
)	O
is	O
‘	O
correct	O
’	O
.	O
unfortunately	O
,	O
this	O
is	O
rarely	O
the	O
case	O
.	O
given	O
the	O
limited	O
model	B
resources	O
,	O
it	O
might	O
make	O
sense	O
to	O
focus	O
on	O
ensuring	O
the	O
prediction	B
of	O
cancer	O
is	O
correct	O
since	O
this	O
has	O
a	O
more	O
signiﬁcant	O
eﬀect	O
on	O
the	O
utility	B
.	O
however	O
,	O
formally	O
,	O
this	O
is	O
not	O
possible	O
in	O
this	O
framework	O
.	O
the	O
alternative	O
empirical	B
utility	O
approach	B
recognises	O
that	O
the	O
task	O
can	O
be	O
stated	O
as	O
to	O
translate	O
patient	O
information	O
x	O
into	O
an	O
operation	O
decision	O
d.	O
to	O
do	O
so	O
one	O
could	O
parameterise	O
this	O
as	O
d	O
(	O
x	O
)	O
=	O
f	O
(	O
x|θ	O
)	O
and	O
then	O
learn	O
θ	O
under	O
maximising	O
the	O
empirical	B
utility	O
u	O
(	O
f	O
(	O
xn|θ	O
)	O
,	O
cn	O
)	O
(	O
13.3.3	O
)	O
for	O
example	O
,	O
if	O
x	O
is	O
a	O
vector	O
representing	O
the	O
patient	O
information	O
and	O
θ	O
the	O
parameter	B
,	O
we	O
might	O
use	O
a	O
linear	B
decision	O
function	B
such	O
as	O
(	O
cid:26	O
)	O
θtx	O
≥	O
0	O
d	O
=	O
operate	O
θtx	O
<	O
0	O
d	O
=	O
don	O
’	O
t	O
operate	O
u	O
(	O
θ	O
)	O
=	O
(	O
cid:88	O
)	O
n	O
f	O
(	O
x|θ	O
)	O
=	O
the	O
advantage	O
of	O
this	O
approach	B
is	O
that	O
the	O
parameters	O
of	O
the	O
decision	O
are	O
directly	O
related	O
to	O
the	O
utility	B
of	O
making	O
the	O
decision	O
.	O
a	O
disadvantage	O
is	O
that	O
we	O
can	O
not	O
easily	O
incorporate	O
domain	B
knowledge	O
into	O
the	O
decision	B
function	I
.	O
it	O
may	O
be	O
that	O
we	O
have	O
a	O
good	O
model	B
of	O
p	O
(	O
c|x	O
)	O
and	O
would	O
wish	O
to	O
make	O
use	O
of	O
this	O
.	O
262	O
draft	O
march	O
9	O
,	O
2010	O
(	O
13.3.4	O
)	O
bayesian	O
hypothesis	B
testing	I
for	O
outcome	B
analysis	I
both	O
approaches	O
are	O
heavily	O
used	O
in	O
practice	O
and	O
which	O
is	O
to	O
be	O
preferred	O
depends	O
very	O
much	O
on	O
the	O
problem	B
.	O
whilst	O
the	O
bayesian	O
approach	B
appears	O
formally	O
optimal	O
,	O
it	O
is	O
prone	O
to	O
model	B
mis-speciﬁcation	O
.	O
a	O
pragmatic	O
alternative	O
bayesian	O
approach	B
is	O
to	O
ﬁt	O
a	O
parameterised	O
distribution	B
p	O
(	O
c	O
,	O
x|λ	O
)	O
to	O
the	O
data	B
d	O
,	O
where	O
λ	O
penalises	O
‘	O
complexity	O
’	O
of	O
the	O
ﬁtted	O
distribution	B
,	O
setting	O
λ	O
using	O
validation	B
on	O
the	O
risk	B
.	O
this	O
has	O
the	O
potential	B
advantage	O
of	O
allowing	O
one	O
to	O
incorporate	O
sensible	O
prior	B
information	O
about	O
p	O
(	O
c	O
,	O
x	O
)	O
whilst	O
assessing	O
competing	O
models	O
in	O
the	O
light	O
of	O
their	O
actual	O
predictive	O
risk	O
.	O
similarly	O
,	O
for	O
the	O
empirical	B
risk	I
approach	O
,	O
one	O
can	O
modify	O
the	O
extreme	O
empirical	B
distribution	I
assumption	O
by	O
using	O
a	O
more	O
plausible	O
model	B
p	O
(	O
x	O
,	O
c	O
)	O
of	O
the	O
data	B
.	O
13.4	O
representing	O
data	B
the	O
numeric	O
encoding	O
of	O
data	B
can	O
have	O
a	O
signiﬁcant	O
eﬀect	O
on	O
performance	B
and	O
an	O
understanding	O
of	O
the	O
options	O
for	O
representing	O
data	B
is	O
therefore	O
of	O
considerable	O
importance	B
.	O
13.4.1	O
categorical	O
for	O
categorical	O
(	O
or	O
nominal	O
)	O
data	B
,	O
the	O
observed	O
value	O
belongs	O
to	O
one	O
of	O
a	O
number	O
of	O
classes	O
,	O
with	O
no	O
intrinsic	O
ordering	O
of	O
the	O
classes	O
.	O
an	O
example	O
of	O
a	O
categorical	O
variable	B
would	O
be	O
the	O
description	O
of	O
the	O
type	O
of	O
job	O
that	O
someone	O
does	O
,	O
e.g	O
.	O
healthcare	O
,	O
education	O
,	O
ﬁnancial	B
services	O
,	O
transport	O
,	O
homeworker	O
,	O
unemployed	O
,	O
engineering	O
etc	O
.	O
one	O
way	O
to	O
transform	O
this	O
data	B
into	O
numerical	B
values	O
would	O
be	O
to	O
use	O
1-of-m	O
encoding	O
.	O
here	O
’	O
s	O
an	O
example	O
:	O
there	O
are	O
4	O
kinds	O
of	O
jobs	O
:	O
soldier	O
,	O
sailor	O
,	O
tinker	O
,	O
spy	O
.	O
a	O
soldier	O
is	O
represented	O
as	O
(	O
1,0,0,0	O
)	O
,	O
a	O
sailer	O
as	O
(	O
0,1,0,0	O
)	O
,	O
a	O
tinker	O
as	O
(	O
0,0,1,0	O
)	O
and	O
a	O
spy	O
as	O
(	O
0,0,0,1	O
)	O
.	O
in	O
this	O
encoding	O
the	O
distance	O
between	O
the	O
vectors	O
representing	O
two	O
diﬀerent	O
professions	O
is	O
constant	O
.	O
it	O
is	O
clear	O
that	O
1-of-m	O
encoding	O
induces	O
dependencies	O
in	O
the	O
profession	O
attributes	O
since	O
if	O
one	O
of	O
the	O
profession	O
attributes	O
is	O
1	O
,	O
the	O
others	O
must	O
be	O
zero	O
.	O
13.4.2	O
ordinal	B
an	O
ordinal	B
variable	O
consists	O
of	O
categories	O
with	O
an	O
ordering	O
or	O
ranking	O
of	O
the	O
categories	O
,	O
e.g	O
.	O
cold	O
,	O
cool	O
,	O
warm	O
,	O
hot	O
.	O
in	O
this	O
case	O
,	O
to	O
preserve	O
the	O
ordering	O
we	O
could	O
perhaps	O
use	O
-1	O
for	O
cold	O
,	O
0	O
for	O
cool	O
,	O
+1	O
for	O
warm	O
and	O
+2	O
for	O
hot	O
.	O
this	O
choice	O
is	O
somewhat	O
arbitrary	O
,	O
and	O
one	O
should	O
bear	O
in	O
mind	O
that	O
results	O
will	O
generally	O
be	O
dependent	O
on	O
the	O
numerical	B
coding	O
used	O
.	O
13.4.3	O
numerical	B
numerical	O
data	B
takes	O
on	O
values	O
that	O
are	O
real	O
numbers	O
,	O
e.g	O
.	O
a	O
temperature	O
measured	O
by	O
a	O
thermometer	O
,	O
or	O
the	O
salary	O
that	O
someone	O
earns	O
.	O
13.5	O
bayesian	O
hypothesis	B
testing	I
for	O
outcome	B
analysis	I
how	O
can	O
we	O
assess	O
whether	O
two	O
classiﬁers	O
are	O
performing	O
diﬀerently	O
?	O
for	O
techniques	O
which	O
are	O
based	O
on	O
bayesian	O
classiﬁers	O
p	O
(	O
c	O
,	O
θ|d	O
,	O
m	O
)	O
there	O
will	O
always	O
be	O
,	O
in	O
principle	O
,	O
a	O
direct	O
way	O
to	O
estimate	O
the	O
suitability	O
of	O
the	O
model	B
m	O
by	O
computing	O
p	O
(	O
m|d	O
)	O
.	O
we	O
consider	O
here	O
the	O
less	O
fortunate	O
situation	O
where	O
the	O
only	O
information	O
presumed	O
available	O
is	O
the	O
test	O
performance	O
of	O
the	O
two	O
classiﬁers	O
.	O
to	O
outline	O
the	O
basic	O
issue	O
,	O
let	O
’	O
s	O
consider	O
two	O
classiﬁers	O
a	O
and	O
b	O
which	O
predict	O
the	O
class	O
of	O
55	O
test	O
exam-	O
ples	O
.	O
classiﬁer	B
a	O
makes	O
20	O
errors	O
,	O
and	O
35	O
correct	O
classiﬁcations	O
,	O
whereas	O
classiﬁer	B
b	O
makes	O
23	O
errors	O
and	O
32	O
correct	O
classiﬁcations	O
.	O
is	O
classiﬁer	B
a	O
better	O
than	O
classiﬁer	B
b	O
?	O
our	O
lack	O
of	O
conﬁdence	O
in	O
pronouncing	O
that	O
a	O
is	O
better	O
than	O
b	O
results	O
from	O
the	O
small	O
number	O
of	O
test	O
examples	O
.	O
on	O
the	O
other	O
hand	O
if	O
classiﬁer	B
a	O
makes	O
200	O
errors	O
and	O
350	O
correct	O
classiﬁcations	O
,	O
whilst	O
classiﬁer	B
b	O
makes	O
230	O
errors	O
and	O
320	O
correct	O
classiﬁcations	O
,	O
intuitively	O
,	O
we	O
would	O
be	O
more	O
conﬁdent	O
that	O
classiﬁer	B
a	O
is	O
better	O
than	O
classiﬁer	B
b.	O
perhaps	O
the	O
most	O
practically	O
relevant	O
question	O
from	O
a	O
machine	O
learning	B
perspective	O
is	O
the	O
probability	B
that	O
classiﬁer	B
a	O
outperforms	O
classiﬁer	B
b	O
,	O
given	O
the	O
available	O
test	O
information	O
.	O
whilst	O
this	O
question	O
can	O
draft	O
march	O
9	O
,	O
2010	O
263	O
bayesian	O
hypothesis	B
testing	I
for	O
outcome	B
analysis	I
be	O
addressed	O
using	O
a	O
bayesian	O
procedure	O
,	O
section	O
(	O
13.5.5	O
)	O
,	O
we	O
ﬁrst	O
focus	O
on	O
a	O
simpler	O
question	O
,	O
namely	O
whether	O
classiﬁer	B
a	O
and	O
b	O
are	O
the	O
same	O
[	O
16	O
]	O
.	O
13.5.1	O
outcome	B
analysis	I
the	O
treatment	O
in	O
this	O
section	O
refers	O
to	O
outcomes	O
and	O
quantiﬁes	O
if	O
data	B
is	O
likely	O
to	O
come	O
from	O
the	O
same	O
multinomial	B
distribution	O
.	O
in	O
the	O
main	O
we	O
will	O
apply	O
this	O
to	O
assessing	O
if	O
two	O
classiﬁers	O
are	O
essentially	O
per-	O
forming	O
the	O
same	O
,	O
although	O
one	O
should	O
bear	O
in	O
mind	O
that	O
the	O
method	O
applies	O
more	O
generally	O
to	O
assessing	O
if	O
outcomes	O
are	O
likely	O
to	O
have	O
been	O
generated	O
from	O
the	O
same	O
or	O
diﬀerent	O
underlying	O
processes	O
.	O
consider	O
a	O
situation	O
where	O
two	O
classiﬁers	O
a	O
and	O
b	O
have	O
been	O
tested	O
on	O
some	O
data	B
,	O
so	O
that	O
we	O
have	O
,	O
for	O
each	O
example	O
in	O
the	O
test	B
set	I
,	O
an	O
outcome	O
pair	O
(	O
oa	O
(	O
n	O
)	O
,	O
ob	O
(	O
n	O
)	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
(	O
13.5.1	O
)	O
where	O
n	O
is	O
the	O
number	O
of	O
test	O
data	O
points	O
,	O
and	O
oa	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
q	O
}	O
(	O
and	O
similarly	O
for	O
ob	O
)	O
.	O
that	O
is	O
,	O
there	O
are	O
q	O
possible	O
types	O
of	O
outcomes	O
that	O
can	O
occur	O
.	O
for	O
example	O
,	O
for	O
binary	O
classiﬁcation	O
we	O
will	O
typically	O
have	O
the	O
four	O
cases	O
dom	O
(	O
o	O
)	O
=	O
{	O
truepositive	O
,	O
falsepositive	O
,	O
truenegative	O
,	O
falsenegative	O
}	O
(	O
13.5.2	O
)	O
if	O
the	O
classiﬁer	B
predicts	O
class	O
c	O
∈	O
{	O
true	O
,	O
false	O
}	O
and	O
the	O
truth	O
is	O
class	O
t	O
∈	O
{	O
true	O
,	O
false	O
}	O
these	O
are	O
deﬁned	O
as	O
truepositive	O
falsepositive	O
truenegative	O
falsenegative	O
c	O
=	O
true	O
c	O
=	O
true	O
c	O
=	O
false	O
c	O
=	O
false	O
t	O
=	O
true	O
t	O
=	O
false	O
t	O
=	O
false	O
t	O
=	O
true	O
(	O
13.5.3	O
)	O
we	O
call	O
oa	O
=	O
{	O
oa	O
(	O
n	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
,	O
the	O
outcomes	O
for	O
classiﬁer	B
a	O
,	O
and	O
similarly	O
for	O
ob	O
=	O
{	O
ob	O
(	O
n	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
for	O
classiﬁer	B
b.	O
to	O
be	O
speciﬁc	O
we	O
have	O
two	O
hypotheses	O
we	O
wish	O
to	O
test	O
:	O
1.	O
hdiﬀ	O
:	O
oa	O
and	O
ob	O
are	O
from	O
diﬀerent	O
categorical	O
distributions	O
.	O
2.	O
hsame	O
:	O
oa	O
and	O
ob	O
are	O
from	O
the	O
same	O
categorical	O
distribution	B
.	O
in	O
both	O
cases	O
we	O
will	O
use	O
categorical	O
models	O
p	O
(	O
oc	O
=	O
q|γ	O
,	O
h	O
)	O
=	O
γc	O
q	O
,	O
with	O
unknown	O
parameters	O
α	O
(	O
hypothesis	O
2	O
will	O
correspond	O
to	O
using	O
the	O
same	O
parameters	O
γa	O
=	O
γb	O
for	O
both	O
classiﬁers	O
,	O
and	O
hypothesis	O
1	O
to	O
using	O
diﬀerent	O
parameters	O
,	O
as	O
we	O
will	O
discuss	O
below	O
)	O
.	O
in	O
the	O
bayesian	O
framework	O
,	O
we	O
want	O
to	O
ﬁnd	O
how	O
likely	O
it	O
is	O
that	O
a	O
model/hypothesis	O
is	O
responsible	O
for	O
generating	O
the	O
data	B
.	O
for	O
any	O
hypothesis	O
h	O
calculate	O
p	O
(	O
h|oa	O
,	O
ob	O
)	O
=	O
p	O
(	O
oa	O
,	O
ob|h	O
)	O
p	O
(	O
h	O
)	O
p	O
(	O
oa	O
,	O
ob	O
)	O
(	O
13.5.4	O
)	O
where	O
p	O
(	O
h	O
)	O
is	O
our	O
prior	B
belief	O
that	O
h	O
is	O
the	O
correct	O
hypothesis	O
.	O
note	O
that	O
the	O
normalising	O
constant	O
p	O
(	O
oa	O
,	O
ob	O
)	O
does	O
not	O
depend	O
on	O
the	O
hypothesis	O
.	O
under	O
all	O
hypotheses	O
we	O
will	O
make	O
the	O
independence	B
of	O
trials	O
assumption	O
n	O
(	O
cid:89	O
)	O
n=1	O
p	O
(	O
oa	O
,	O
ob|h	O
)	O
=	O
p	O
(	O
oa	O
(	O
n	O
)	O
,	O
ob	O
(	O
n	O
)	O
|h	O
)	O
.	O
(	O
13.5.5	O
)	O
to	O
make	O
further	O
progress	O
we	O
need	O
to	O
state	O
what	O
the	O
speciﬁc	O
hypotheses	O
mean	B
.	O
264	O
draft	O
march	O
9	O
,	O
2010	O
bayesian	O
hypothesis	B
testing	I
for	O
outcome	B
analysis	I
α	O
oa	O
β	O
α	O
ob	O
oa	O
ob	O
(	O
a	O
)	O
(	O
b	O
)	O
p	O
oa	O
,	O
ob	O
(	O
c	O
)	O
figure	O
13.10	O
:	O
(	O
a	O
)	O
:	O
hdiﬀ	O
:	O
corresponds	O
to	O
the	O
out-	O
comes	O
for	O
the	O
two	O
classiﬁers	O
being	O
independently	O
gen-	O
(	O
b	O
)	O
:	O
hsame	O
:	O
both	O
outcomes	O
are	O
generated	O
erated	O
.	O
(	O
c	O
)	O
:	O
hdep	O
:	O
the	O
out-	O
from	O
the	O
same	O
distribution	B
.	O
comes	O
are	O
dependent	O
(	O
‘	O
correlated	O
’	O
)	O
.	O
13.5.2	O
hdiﬀ	O
:	O
model	B
likelihood	O
we	O
now	O
use	O
the	O
above	O
assumptions	O
to	O
compute	O
the	O
hypothesis	O
likelihood	O
:	O
p	O
(	O
hdiﬀ|oa	O
,	O
ob	O
)	O
=	O
p	O
(	O
oa	O
,	O
ob|hdiﬀ	O
)	O
p	O
(	O
hdiﬀ	O
)	O
p	O
(	O
oa	O
,	O
ob	O
)	O
(	O
13.5.6	O
)	O
the	O
outcome	O
model	O
for	O
classiﬁer	B
a	O
is	O
speciﬁed	O
using	O
parameters	O
,	O
α	O
,	O
giving	O
p	O
(	O
oa|α	O
,	O
hdiﬀ	O
)	O
,	O
and	O
similarly	O
we	O
use	O
β	O
for	O
classiﬁer	B
b.	O
the	O
ﬁnite	O
amount	O
of	O
data	B
means	O
that	O
we	O
are	O
uncertain	B
as	O
to	O
these	O
parameter	B
values	O
,	O
and	O
therefore	O
the	O
joint	B
term	O
in	O
the	O
numerator	O
above	O
is	O
p	O
(	O
oa	O
,	O
ob	O
)	O
p	O
(	O
hdiﬀ|oa	O
,	O
ob	O
)	O
=	O
p	O
(	O
oa	O
,	O
ob|α	O
,	O
β	O
,	O
hdiﬀ	O
)	O
p	O
(	O
α	O
,	O
β|hdiﬀ	O
)	O
p	O
(	O
hdiﬀ	O
)	O
dαdβ	O
(	O
13.5.7	O
)	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
=	O
p	O
(	O
hdiﬀ	O
)	O
where	O
we	O
assumed	O
p	O
(	O
oa|α	O
,	O
hdiﬀ	O
)	O
p	O
(	O
α|hdiﬀ	O
)	O
dα	O
p	O
(	O
ob|β	O
,	O
hdiﬀ	O
)	O
p	O
(	O
β|hdiﬀ	O
)	O
dβ	O
(	O
13.5.8	O
)	O
p	O
(	O
α	O
,	O
β|hdiﬀ	O
)	O
=	O
p	O
(	O
α|hdiﬀ	O
)	O
p	O
(	O
β|hdiﬀ	O
)	O
and	O
p	O
(	O
oa	O
,	O
ob|α	O
,	O
β	O
,	O
hdiﬀ	O
)	O
=	O
p	O
(	O
oa|α	O
,	O
hdiﬀ	O
)	O
p	O
(	O
ob|β	O
,	O
hdiﬀ	O
)	O
(	O
13.5.9	O
)	O
note	O
that	O
one	O
might	O
expect	O
there	O
to	O
be	O
a	O
speciﬁc	O
constraint	O
that	O
the	O
two	O
models	O
a	O
and	O
b	O
are	O
diﬀerent	O
.	O
however	O
since	O
the	O
models	O
are	O
assumed	O
independent	O
and	O
each	O
has	O
parameters	O
sampled	O
from	O
an	O
eﬀectively	O
inﬁnite	O
set	O
(	O
α	O
and	O
β	O
are	O
continuous	B
)	O
,	O
the	O
probability	B
that	O
sampled	O
parameters	O
α	O
and	O
β	O
of	O
the	O
two	O
models	O
are	O
the	O
same	O
is	O
zero	O
.	O
since	O
we	O
are	O
dealing	O
with	O
categorical	O
distributions	O
,	O
it	O
is	O
convenient	O
to	O
use	O
the	O
dirichlet	O
prior	B
,	O
which	O
is	O
conjugate	B
to	O
the	O
categorical	O
distribution	B
:	O
(	O
cid:89	O
)	O
1	O
z	O
(	O
u	O
)	O
q	O
p	O
(	O
α|hdiﬀ	O
)	O
=	O
uq−1	O
α	O
q	O
,	O
z	O
(	O
u	O
)	O
=	O
(	O
cid:81	O
)	O
q	O
(	O
cid:16	O
)	O
(	O
cid:80	O
)	O
q	O
q=1	O
γ	O
(	O
uq	O
)	O
q=1	O
uq	O
(	O
cid:17	O
)	O
γ	O
(	O
13.5.10	O
)	O
the	O
prior	B
hyperparameter	O
u	O
controls	O
how	O
strongly	O
the	O
mass	O
of	O
the	O
distribution	B
is	O
pushed	O
to	O
the	O
corners	O
of	O
the	O
simplex	O
,	O
see	O
ﬁg	O
(	O
8.5	O
)	O
.	O
setting	O
uq	O
=	O
1	O
for	O
all	O
q	O
corresponds	O
to	O
a	O
uniform	B
prior	O
.	O
the	O
likelihood	B
of	O
oa	O
(	O
cid:90	O
)	O
is	O
given	O
by	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
(	O
cid:89	O
)	O
(	O
cid:89	O
)	O
p	O
(	O
oa|α	O
,	O
hdiﬀ	O
)	O
p	O
(	O
α|hdiﬀ	O
)	O
dα	O
=	O
1	O
(	O
cid:93	O
)	O
a	O
q	O
q	O
α	O
z	O
(	O
u	O
)	O
q	O
q	O
uq−1	O
q	O
α	O
dα	O
=	O
z	O
(	O
u	O
+	O
(	O
cid:93	O
)	O
a	O
)	O
z	O
(	O
u	O
)	O
(	O
13.5.11	O
)	O
where	O
(	O
cid:93	O
)	O
a	O
is	O
a	O
vector	O
with	O
components	O
(	O
cid:93	O
)	O
a	O
hence	O
q	O
being	O
the	O
number	O
of	O
times	O
that	O
variable	B
a	O
is	O
is	O
state	O
q	O
in	O
the	O
data	B
.	O
p	O
(	O
hdiﬀ|oa	O
,	O
ob	O
)	O
=	O
p	O
(	O
hdiﬀ	O
)	O
z	O
(	O
u	O
+	O
(	O
cid:93	O
)	O
a	O
)	O
z	O
(	O
u	O
)	O
z	O
(	O
u	O
+	O
(	O
cid:93	O
)	O
b	O
)	O
z	O
(	O
u	O
)	O
where	O
z	O
(	O
u	O
)	O
is	O
given	O
by	O
equation	B
(	O
13.5.10	O
)	O
.	O
(	O
13.5.12	O
)	O
13.5.3	O
hsame	O
:	O
model	B
likelihood	O
in	O
hsame	O
,	O
the	O
hypothesis	O
is	O
that	O
the	O
outcomes	O
for	O
the	O
two	O
classiﬁers	O
are	O
generated	O
from	O
the	O
same	O
categorical	O
distribution	B
.	O
hence	O
(	O
cid:90	O
)	O
p	O
(	O
oa	O
,	O
ob	O
)	O
p	O
(	O
hsame|oa	O
,	O
ob	O
)	O
=	O
p	O
(	O
hsame	O
)	O
p	O
(	O
oa|α	O
,	O
hsame	O
)	O
p	O
(	O
ob|α	O
,	O
hsame	O
)	O
p	O
(	O
α|hsame	O
)	O
dα	O
=	O
p	O
(	O
hsame	O
)	O
z	O
(	O
u	O
+	O
(	O
cid:93	O
)	O
a	O
+	O
(	O
cid:93	O
)	O
b	O
)	O
z	O
(	O
u	O
)	O
draft	O
march	O
9	O
,	O
2010	O
(	O
13.5.13	O
)	O
(	O
13.5.14	O
)	O
265	O
bayesian	O
hypothesis	B
testing	I
for	O
outcome	B
analysis	I
bayes	O
’	O
factor	B
if	O
we	O
assume	O
that	O
we	O
have	O
no	O
prior	O
preference	O
for	O
either	O
hypothesis	O
(	O
p	O
(	O
hdiﬀ	O
)	O
=	O
p	O
(	O
hsame	O
)	O
)	O
,	O
then	O
p	O
(	O
hdiﬀ|oa	O
,	O
ob	O
)	O
p	O
(	O
hsame|oa	O
,	O
ob	O
)	O
=	O
z	O
(	O
u	O
+	O
(	O
cid:93	O
)	O
a	O
)	O
z	O
(	O
u	O
+	O
(	O
cid:93	O
)	O
b	O
)	O
z	O
(	O
u	O
)	O
z	O
(	O
u	O
+	O
(	O
cid:93	O
)	O
a	O
+	O
(	O
cid:93	O
)	O
b	O
)	O
(	O
13.5.15	O
)	O
this	O
is	O
the	O
evidence	B
to	O
suggest	O
that	O
the	O
data	B
were	O
generated	O
by	O
two	O
diﬀerent	O
categorical	O
distributions	O
.	O
example	O
62.	O
two	O
people	O
classify	O
the	O
expression	O
of	O
each	O
image	O
into	O
happy	O
,	O
sad	O
or	O
normal	B
,	O
using	O
states	O
1	O
,	O
2	O
,	O
3	O
respectively	O
.	O
each	O
column	O
of	O
the	O
data	B
below	O
represents	O
an	O
image	O
classed	O
by	O
the	O
two	O
people	O
(	O
person	O
1	O
is	O
the	O
top	O
row	O
and	O
person	O
2	O
the	O
second	O
row	O
)	O
.	O
are	O
the	O
two	O
people	O
essentially	O
in	O
agreement	O
?	O
1	O
1	O
3	O
3	O
1	O
1	O
3	O
2	O
1	O
2	O
1	O
3	O
3	O
3	O
2	O
3	O
2	O
2	O
3	O
3	O
1	O
3	O
1	O
2	O
1	O
2	O
1	O
2	O
1	O
2	O
1	O
1	O
1	O
2	O
1	O
1	O
1	O
3	O
2	O
2	O
to	O
help	O
answer	O
this	O
question	O
,	O
we	O
perform	O
a	O
hdiﬀ	O
versus	O
hsame	O
test	O
.	O
from	O
this	O
data	B
,	O
the	O
count	O
vector	O
for	O
person	O
1	O
is	O
[	O
13	O
,	O
3	O
,	O
4	O
]	O
and	O
for	O
person	O
2	O
,	O
[	O
4	O
,	O
9	O
,	O
7	O
]	O
.	O
based	O
on	O
a	O
ﬂat	O
prior	B
for	O
the	O
categorical	O
distribution	B
and	O
assuming	O
no	O
prior	O
preference	O
for	O
either	O
hypothesis	O
,	O
we	O
have	O
the	O
bayes	O
’	O
factor	B
p	O
(	O
persons	O
1	O
and	O
2	O
classify	O
diﬀerently	O
)	O
p	O
(	O
persons	O
1	O
and	O
2	O
classify	O
the	O
same	O
)	O
=	O
z	O
(	O
[	O
14	O
,	O
4	O
,	O
5	O
]	O
)	O
z	O
(	O
[	O
5	O
,	O
10	O
,	O
8	O
]	O
)	O
z	O
(	O
[	O
1	O
,	O
1	O
,	O
1	O
]	O
)	O
z	O
(	O
[	O
18	O
,	O
13	O
,	O
12	O
]	O
)	O
=	O
12.87	O
(	O
13.5.16	O
)	O
where	O
the	O
z	O
function	B
is	O
given	O
in	O
equation	B
(	O
13.5.10	O
)	O
.	O
this	O
is	O
strong	B
evidence	O
the	O
two	O
people	O
are	O
classifying	O
the	O
images	O
diﬀerently	O
.	O
below	O
we	O
discuss	O
some	O
further	O
examples	O
for	O
the	O
hdiﬀ	O
versus	O
hsame	O
test	O
.	O
as	O
above	O
,	O
the	O
only	O
quantities	O
we	O
need	O
for	O
this	O
test	O
are	O
the	O
vector	O
counts	O
from	O
the	O
data	B
.	O
let	O
’	O
s	O
assume	O
that	O
there	O
are	O
three	O
kinds	O
of	O
outcomes	O
,	O
q	O
=	O
3.	O
for	O
example	O
dom	O
(	O
o	O
)	O
=	O
{	O
good	O
,	O
bad	O
,	O
ugly	O
}	O
are	O
our	O
set	O
of	O
outcomes	O
and	O
we	O
want	O
to	O
test	O
if	O
two	O
classiﬁers	O
are	O
essentially	O
producing	O
the	O
same	O
outcome	O
distributions	O
,	O
or	O
diﬀerent	O
.	O
example	O
63	O
(	O
hdiﬀ	O
versus	O
hsame	O
)	O
.	O
•	O
we	O
have	O
the	O
two	O
outcome	O
counts	O
(	O
cid:93	O
)	O
a	O
=	O
[	O
39	O
,	O
26	O
,	O
35	O
]	O
and	O
(	O
cid:93	O
)	O
b	O
=	O
[	O
63	O
,	O
12	O
,	O
25	O
]	O
.	O
then	O
,	O
the	O
bayes	O
’	O
factor	B
equation	O
(	O
13.5.15	O
)	O
is	O
20.7	O
–	O
strong	B
evidence	O
in	O
favour	O
of	O
the	O
two	O
classiﬁers	O
being	O
diﬀerent	O
.	O
•	O
alternatively	O
,	O
consider	O
the	O
two	O
outcome	O
counts	O
(	O
cid:93	O
)	O
a	O
=	O
[	O
52	O
,	O
20	O
,	O
28	O
]	O
and	O
(	O
cid:93	O
)	O
b	O
=	O
[	O
44	O
,	O
14	O
,	O
42	O
]	O
.	O
then	O
,	O
the	O
bayes	O
’	O
factor	B
equation	O
(	O
13.5.15	O
)	O
is	O
0.38	O
–	O
weak	O
evidence	B
against	O
the	O
two	O
classiﬁers	O
being	O
diﬀerent	O
.	O
•	O
as	O
a	O
ﬁnal	O
example	O
,	O
consider	O
counts	O
(	O
cid:93	O
)	O
a	O
=	O
[	O
459	O
,	O
191	O
,	O
350	O
]	O
and	O
(	O
cid:93	O
)	O
b	O
=	O
[	O
465	O
,	O
206	O
,	O
329	O
]	O
.	O
this	O
gives	O
a	O
bayes	O
’	O
factor	B
equation	O
(	O
13.5.15	O
)	O
of	O
0.008	O
–	O
strong	B
evidence	O
that	O
the	O
two	O
classiﬁers	O
are	O
statistically	O
the	O
same	O
.	O
in	O
all	O
cases	O
the	O
results	O
are	O
consistent	B
with	O
the	O
model	B
in	O
fact	O
used	O
to	O
generate	O
the	O
count	O
data	B
–	O
the	O
two	O
outcomes	O
for	O
a	O
and	O
b	O
were	O
indeed	O
from	O
diﬀerent	O
categorical	O
distributions	O
.	O
the	O
more	O
test	O
data	O
we	O
have	O
,	O
the	O
more	O
conﬁdent	O
we	O
are	O
in	O
our	O
statements	O
.	O
13.5.4	O
dependent	O
outcome	B
analysis	I
here	O
we	O
consider	O
the	O
(	O
perhaps	O
more	O
common	O
)	O
case	O
that	O
outcomes	O
are	O
dependent	O
.	O
for	O
example	O
,	O
it	O
is	O
often	O
the	O
case	O
that	O
if	O
classiﬁer	B
a	O
works	O
well	O
,	O
then	O
classiﬁer	B
b	O
will	O
also	O
work	O
well	O
.	O
thus	O
we	O
want	O
to	O
evaluate	O
the	O
266	O
draft	O
march	O
9	O
,	O
2010	O
bayesian	O
hypothesis	B
testing	I
for	O
outcome	B
analysis	I
hypothesis	O
:	O
hdep	O
:	O
the	O
outcomes	O
that	O
the	O
two	O
classiﬁers	O
make	O
are	O
dependent	O
to	O
do	O
so	O
we	O
assume	O
a	O
categorical	O
distribution	B
over	O
the	O
joint	B
states	O
:	O
p	O
(	O
oa	O
(	O
n	O
)	O
,	O
ob	O
(	O
n	O
)	O
|p	O
,	O
hdep	O
)	O
[	O
p	O
]	O
ij	O
=	O
p	O
(	O
oa	O
=	O
i	O
,	O
ob	O
=	O
j	O
)	O
here	O
p	O
is	O
a	O
q	O
×	O
q	O
matrix	B
of	O
probabilities	O
:	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
so	O
[	O
p	O
]	O
ij	O
is	O
the	O
probability	B
that	O
a	O
makes	O
outcome	O
i	O
,	O
and	O
b	O
makes	O
outcome	O
j.	O
then	O
,	O
p	O
(	O
o|hdep	O
)	O
=	O
p	O
(	O
o	O
,	O
p|hdep	O
)	O
dp	O
=	O
p	O
(	O
o|p	O
,	O
hdep	O
)	O
p	O
(	O
p|hdep	O
)	O
dp	O
where	O
,	O
for	O
convenience	O
,	O
we	O
write	O
o	O
=	O
(	O
oa	O
,	O
ob	O
)	O
.	O
assuming	O
a	O
dirichlet	O
prior	B
on	O
p	O
,	O
with	O
hyperparameters	O
u	O
,	O
we	O
have	O
p	O
(	O
o	O
)	O
p	O
(	O
hdep|o	O
)	O
=	O
p	O
(	O
hdep	O
)	O
z	O
(	O
vec	O
(	O
u	O
+	O
(	O
cid:93	O
)	O
)	O
)	O
z	O
(	O
vec	O
(	O
u	O
)	O
)	O
(	O
13.5.20	O
)	O
where	O
vec	O
(	O
d	O
)	O
is	O
a	O
vector	O
formed	O
from	O
concatenating	O
the	O
rows	O
of	O
the	O
matrix	B
d.	O
here	O
(	O
cid:93	O
)	O
is	O
the	O
count	O
matrix	B
,	O
with	O
[	O
(	O
cid:93	O
)	O
]	O
ij	O
equal	O
to	O
the	O
number	O
of	O
times	O
that	O
joint	B
outcome	O
(	O
oa	O
=	O
i	O
,	O
ob	O
=	O
j	O
)	O
occurred	O
in	O
the	O
n	O
datapoints	O
.	O
as	O
before	O
,	O
we	O
can	O
then	O
use	O
this	O
in	O
a	O
bayes	O
factor	B
calculation	O
.	O
for	O
the	O
uniform	B
prior	O
,	O
[	O
u	O
]	O
ij	O
=	O
1	O
,	O
∀i	O
,	O
j.	O
testing	O
for	O
dependencies	O
in	O
the	O
outcomes	O
:	O
hdep	O
versus	O
hdiﬀ	O
to	O
test	O
whether	O
or	O
not	O
the	O
outcomes	O
of	O
the	O
classiﬁers	O
are	O
dependent	O
hdep	O
against	O
the	O
hypothesis	O
that	O
they	O
are	O
independent	O
hdiﬀ	O
we	O
may	O
use	O
,	O
assuming	O
p	O
(	O
hdiﬀ	O
)	O
=	O
p	O
(	O
hdep	O
)	O
,	O
(	O
13.5.17	O
)	O
(	O
13.5.18	O
)	O
(	O
13.5.19	O
)	O
(	O
13.5.21	O
)	O
(	O
13.5.22	O
)	O
(	O
13.5.23	O
)	O
(	O
13.5.24	O
)	O
(	O
13.5.25	O
)	O
267	O
p	O
(	O
hdiﬀ|o	O
)	O
p	O
(	O
hdep|o	O
)	O
=	O
z	O
(	O
u	O
+	O
(	O
cid:93	O
)	O
a	O
)	O
z	O
(	O
u	O
)	O
z	O
(	O
u	O
+	O
(	O
cid:93	O
)	O
b	O
)	O
z	O
(	O
vec	O
(	O
u	O
)	O
)	O
z	O
(	O
u	O
)	O
z	O
(	O
vec	O
(	O
u	O
+	O
(	O
cid:93	O
)	O
)	O
)	O
example	O
64	O
(	O
hdep	O
versus	O
hdiﬀ	O
)	O
.	O
•	O
consider	O
the	O
outcome	O
count	O
matrix	B
(	O
cid:93	O
)	O
	O
98	O
7	O
93	O
168	O
13	O
163	O
245	O
12	O
201	O
so	O
that	O
(	O
cid:93	O
)	O
a	O
=	O
[	O
511	O
,	O
32	O
,	O
457	O
]	O
,	O
and	O
(	O
cid:93	O
)	O
b	O
=	O
[	O
198	O
,	O
344	O
,	O
458	O
]	O
.	O
then	O
	O
	O
p	O
(	O
hdiﬀ|o	O
)	O
p	O
(	O
hdep|o	O
)	O
=	O
3020	O
	O
82	O
120	O
83	O
107	O
162	O
4	O
170	O
203	O
70	O
-	O
strong	B
evidence	O
that	O
the	O
classiﬁers	O
perform	O
independently	O
.	O
•	O
consider	O
the	O
outcome	O
count	O
matrix	B
(	O
cid:93	O
)	O
so	O
that	O
(	O
cid:93	O
)	O
a	O
=	O
[	O
359	O
,	O
485	O
,	O
156	O
]	O
,	O
and	O
(	O
cid:93	O
)	O
b	O
=	O
[	O
284	O
,	O
273	O
,	O
443	O
]	O
.	O
then	O
p	O
(	O
hdiﬀ|o	O
)	O
p	O
(	O
hdep|o	O
)	O
=	O
2	O
×	O
10−18	O
-	O
strong	B
evidence	O
that	O
the	O
classiﬁers	O
perform	O
dependently	O
.	O
these	O
results	O
are	O
in	O
fact	O
consistent	B
with	O
the	O
way	O
the	O
data	B
was	O
generated	O
in	O
each	O
case	O
.	O
draft	O
march	O
9	O
,	O
2010	O
bayesian	O
hypothesis	B
testing	I
for	O
outcome	B
analysis	I
testing	O
for	O
dependencies	O
in	O
the	O
outcomes	O
:	O
hdep	O
versus	O
hsame	O
in	O
practice	O
,	O
it	O
is	O
reasonable	O
to	O
believe	O
that	O
dependencies	O
are	O
quite	O
likely	O
in	O
the	O
outcomes	O
that	O
classiﬁers	O
.	O
for	O
example	O
two	O
classiﬁers	O
will	O
often	O
do	O
well	O
on	O
‘	O
easy	O
’	O
test	O
examples	O
,	O
and	O
badly	O
on	O
‘	O
diﬃcult	O
’	O
examples	O
.	O
are	O
these	O
dependencies	O
strong	B
enough	O
to	O
make	O
us	O
believe	O
that	O
the	O
outcomes	O
are	O
coming	O
from	O
the	O
same	O
process	O
?	O
in	O
this	O
sense	O
,	O
we	O
want	O
to	O
test	O
p	O
(	O
hsame|o	O
)	O
p	O
(	O
hdep|o	O
)	O
=	O
z	O
(	O
u	O
+	O
(	O
cid:93	O
)	O
a	O
+	O
(	O
cid:93	O
)	O
b	O
)	O
z	O
(	O
u	O
)	O
z	O
(	O
vec	O
(	O
u	O
)	O
)	O
z	O
(	O
vec	O
(	O
u	O
+	O
(	O
cid:93	O
)	O
)	O
)	O
example	O
65	O
(	O
hdep	O
versus	O
hsame	O
)	O
.	O
•	O
consider	O
an	O
experiment	O
which	O
gives	O
the	O
test	O
outcome	O
count	O
matrix	B
(	O
cid:93	O
)	O
	O
	O
105	O
42	O
45	O
172	O
42	O
29	O
192	O
203	O
170	O
so	O
that	O
(	O
cid:93	O
)	O
a	O
=	O
[	O
339	O
,	O
290	O
,	O
371	O
]	O
,	O
and	O
(	O
cid:93	O
)	O
b	O
=	O
[	O
319	O
,	O
116	O
,	O
565	O
]	O
.	O
then	O
p	O
(	O
hsame|o	O
)	O
p	O
(	O
hdep|o	O
)	O
=	O
4.5	O
×	O
10−38	O
–	O
strong	B
evidence	O
that	O
the	O
classiﬁers	O
are	O
performing	O
diﬀerently	O
.	O
•	O
consider	O
an	O
experiment	O
which	O
gives	O
the	O
test	O
outcome	O
count	O
matrix	B
(	O
cid:93	O
)	O
	O
15	O
8	O
4	O
10	O
8	O
5	O
13	O
12	O
25	O
	O
so	O
that	O
(	O
cid:93	O
)	O
a	O
=	O
[	O
33	O
,	O
24	O
,	O
43	O
]	O
,	O
and	O
(	O
cid:93	O
)	O
b	O
=	O
[	O
33	O
,	O
17	O
,	O
50	O
]	O
.	O
then	O
p	O
(	O
hsame|o	O
)	O
p	O
(	O
hdep|o	O
)	O
=	O
42	O
–	O
strong	B
evidence	O
that	O
the	O
classiﬁers	O
are	O
performing	O
the	O
same	O
.	O
these	O
results	O
are	O
in	O
fact	O
consistent	B
with	O
the	O
way	O
the	O
data	B
was	O
generated	O
.	O
(	O
13.5.26	O
)	O
(	O
13.5.27	O
)	O
(	O
13.5.28	O
)	O
(	O
13.5.29	O
)	O
(	O
13.5.30	O
)	O
13.5.5	O
is	O
classiﬁer	B
a	O
better	O
than	O
b	O
?	O
we	O
return	O
to	O
the	O
question	O
with	O
which	O
we	O
began	O
this	O
outcome	B
analysis	I
.	O
given	O
the	O
common	O
scenario	O
of	O
observing	O
a	O
number	O
of	O
errors	O
for	O
classiﬁer	B
a	O
on	O
a	O
test	B
set	I
and	O
a	O
number	O
for	O
b	O
,	O
can	O
we	O
say	O
which	O
classiﬁer	B
is	O
better	O
?	O
this	O
corresponds	O
to	O
the	O
special	O
case	O
of	O
binary	O
classes	O
q	O
=	O
2	O
with	O
dom	O
(	O
e	O
)	O
=	O
{	O
correct	O
,	O
incorrect	O
}	O
.	O
under	O
the	O
hdiﬀ	O
for	O
this	O
special	O
case	O
it	O
makes	O
sense	O
to	O
use	O
a	O
beta	B
distribution	O
(	O
which	O
corresponds	O
to	O
the	O
dirichlet	O
when	O
q	O
=	O
2	O
)	O
.	O
then	O
for	O
θa	O
being	O
the	O
probability	B
that	O
classiﬁer	B
a	O
generates	O
a	O
correct	O
label	O
we	O
have	O
p	O
(	O
oa|θa	O
)	O
=	O
θ	O
similarly	O
(	O
cid:93	O
)	O
a	O
correct	O
a	O
(	O
1	O
−	O
θa	O
)	O
(	O
cid:93	O
)	O
a	O
incorrect	O
p	O
(	O
ob|θb	O
)	O
=	O
θ	O
(	O
cid:93	O
)	O
b	O
correct	O
b	O
(	O
1	O
−	O
θb	O
)	O
(	O
cid:93	O
)	O
b	O
incorrect	O
we	O
assume	O
independent	O
identical	O
beta	B
distribution	O
priors	O
p	O
(	O
θa	O
)	O
=	O
b	O
(	O
θa|u1	O
,	O
u2	O
)	O
,	O
p	O
(	O
θb	O
)	O
=	O
b	O
(	O
θb|u1	O
,	O
u2	O
)	O
(	O
13.5.31	O
)	O
(	O
13.5.32	O
)	O
(	O
13.5.33	O
)	O
268	O
draft	O
march	O
9	O
,	O
2010	O
code	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
13.11	O
:	O
two	O
classiﬁers	O
a	O
and	O
b	O
and	O
their	O
posterior	B
distributions	O
of	O
the	O
probability	B
that	O
they	O
(	O
a	O
)	O
:	O
for	O
a	O
with	O
35	O
correct	O
and	O
20	O
incorrect	O
labels	O
,	O
classify	O
correctly	O
(	O
using	O
a	O
uniform	B
beta	O
prior	B
)	O
.	O
b	O
(	O
x|1	O
+	O
35	O
,	O
1	O
+	O
20	O
)	O
(	O
solid	O
curve	O
)	O
;	O
b	O
with	O
32	O
correct	O
23	O
incorrect	O
b	O
(	O
y|1	O
+	O
32	O
,	O
1	O
+	O
23	O
)	O
(	O
dashed	O
curve	O
)	O
.	O
p	O
(	O
x	O
>	O
y	O
)	O
=	O
0.719	O
(	O
b	O
)	O
:	O
for	O
a	O
with	O
350	O
correct	O
and	O
200	O
incorrect	O
labels	O
(	O
solid	O
curve	O
)	O
,	O
b	O
(	O
x|1	O
+	O
350	O
,	O
1	O
+	O
200	O
)	O
;	O
b	O
with	O
320	O
correct	O
230	O
incorrect	O
b	O
(	O
y|1	O
+	O
320	O
,	O
1	O
+	O
230	O
)	O
(	O
dashed	O
curve	O
)	O
,	O
p	O
(	O
x	O
>	O
y	O
)	O
=	O
0.968.	O
as	O
the	O
amount	O
of	O
data	B
increases	O
the	O
overlap	O
between	O
the	O
distributions	O
decreases	O
and	O
the	O
certainty	O
that	O
one	O
classiﬁer	B
is	O
better	O
than	O
the	O
other	O
correspondingly	O
increases	O
.	O
where	O
a	O
ﬂat	O
prior	B
corresponds	O
to	O
using	O
the	O
hyperparameter	B
setting	O
u1	O
=	O
u2	O
=	O
1.	O
the	O
posterior	B
distribu-	O
tions	O
for	O
θa	O
and	O
θb	O
are	O
independent	O
:	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
(	O
13.5.34	O
)	O
(	O
13.5.35	O
)	O
p	O
(	O
θa|oa	O
)	O
=	O
b	O
(	O
θa|	O
(	O
cid:93	O
)	O
a	O
correct	O
+	O
u1	O
,	O
(	O
cid:93	O
)	O
a	O
correct	O
+	O
u1	O
,	O
(	O
cid:93	O
)	O
b	O
incorrect	O
+	O
u2	O
θb|	O
(	O
cid:93	O
)	O
b	O
the	O
question	O
of	O
whether	O
a	O
is	O
better	O
than	O
b	O
can	O
then	O
be	O
addressed	O
by	O
computing	O
incorrect	O
+	O
u2	O
)	O
,	O
p	O
(	O
θb|ob	O
)	O
=	O
b	O
(	O
cid:90	O
)	O
1	O
(	O
cid:90	O
)	O
1	O
xa−1	O
(	O
1	O
−	O
x	O
)	O
b−1	O
1	O
b	O
(	O
a	O
,	O
b	O
)	O
b	O
(	O
c	O
,	O
d	O
)	O
0	O
yc−1	O
(	O
1	O
−	O
y	O
)	O
d−1	O
dydx	O
x	O
p	O
(	O
θa	O
>	O
θb|oa	O
,	O
ob	O
)	O
=	O
where	O
a	O
=	O
(	O
cid:93	O
)	O
a	O
correct	O
+	O
u1	O
,	O
b	O
=	O
(	O
cid:93	O
)	O
a	O
incorrect	O
+	O
u2	O
,	O
c	O
=	O
(	O
cid:93	O
)	O
b	O
correct	O
+	O
u1	O
,	O
d	O
=	O
(	O
cid:93	O
)	O
b	O
incorrect	O
+	O
u2	O
(	O
13.5.36	O
)	O
example	O
66.	O
classiﬁer	B
a	O
makes	O
20	O
errors	O
,	O
and	O
35	O
correct	O
classiﬁcations	O
,	O
whereas	O
classiﬁer	B
b	O
makes	O
23	O
errors	O
and	O
32	O
correct	O
classiﬁcations	O
.	O
using	O
a	O
ﬂat	O
prior	B
this	O
gives	O
p	O
(	O
θa	O
>	O
θb|oa	O
,	O
ob	O
)	O
=	O
betaxbiggery	O
(	O
1+35,1+20,1+32,1+23	O
)	O
=	O
0.719	O
(	O
13.5.37	O
)	O
on	O
the	O
other	O
hand	O
if	O
classiﬁer	B
a	O
makes	O
200	O
errors	O
and	O
350	O
correct	O
classiﬁcations	O
,	O
whilst	O
classiﬁer	B
b	O
makes	O
230	O
errors	O
and	O
320	O
correct	O
classiﬁcations	O
,	O
we	O
have	O
p	O
(	O
θa	O
>	O
θb|oa	O
,	O
ob	O
)	O
=	O
betaxbiggery	O
(	O
1+350,1+200,1+320,1+230	O
)	O
=	O
0.968	O
(	O
13.5.38	O
)	O
this	O
demonstrates	O
the	O
intuitive	O
eﬀect	O
that	O
even	O
though	O
the	O
proportion	O
of	O
correct/incorrect	O
classiﬁcations	O
doesn	O
’	O
t	O
change	O
for	O
the	O
two	O
scenarios	O
,	O
as	O
we	O
have	O
more	O
data	B
our	O
conﬁdence	O
in	O
determining	O
the	O
better	O
classiﬁer	B
increases	O
.	O
13.6	O
code	O
demobayeserroranalysis.m	O
:	O
demo	O
for	O
bayesian	O
error	B
analysis	I
betaxbiggery.m	O
:	O
p	O
(	O
x	O
>	O
y	O
)	O
for	O
x	O
∼	O
b	O
(	O
x|a	O
,	O
b	O
)	O
,	O
y	O
∼	O
b	O
(	O
y|c	O
,	O
d	O
)	O
draft	O
march	O
9	O
,	O
2010	O
269	O
00.10.20.30.40.50.60.70.80.910123456700.10.20.30.40.50.60.70.80.9105101520	O
13.7	O
notes	O
a	O
general	O
introduction	O
to	O
machine	O
learning	B
is	O
given	O
in	O
[	O
200	O
]	O
.	O
an	O
excellent	O
reference	O
for	O
bayesian	O
decision	B
theory	I
is	O
[	O
33	O
]	O
.	O
approaches	O
based	O
on	O
empirical	B
risk	I
are	O
discussed	O
in	O
[	O
282	O
]	O
.	O
exercises	O
(	O
cid:0	O
)	O
x	O
µ2	O
,	O
σ2	O
2	O
(	O
cid:1	O
)	O
,	O
with	O
1	O
1	O
,	O
σ2	O
13.8	O
exercises	O
exercise	O
150.	O
given	O
the	O
distributions	O
p	O
(	O
x|class1	O
)	O
=	O
n	O
corresponding	O
prior	B
occurrence	O
of	O
classes	O
p1	O
and	O
p2	O
(	O
p1	O
+	O
p2	O
=	O
1	O
)	O
,	O
calculate	O
the	O
decision	B
boundary	I
explicitly	O
as	O
a	O
function	B
of	O
µ1	O
,	O
µ2	O
,	O
σ2	O
2	O
,	O
p1	O
,	O
p2	O
.	O
how	O
many	O
solutions	O
are	O
there	O
to	O
the	O
decision	B
boundary	I
,	O
and	O
are	O
they	O
all	O
reasonable	O
?	O
exercise	O
151.	O
suppose	O
that	O
instead	O
of	O
using	O
the	O
bayes	O
’	O
decision	O
rule	O
to	O
choose	O
class	O
k	O
if	O
p	O
(	O
ck|x	O
)	O
>	O
p	O
(	O
cj|x	O
)	O
for	O
all	O
j	O
(	O
cid:54	O
)	O
=	O
k	O
,	O
we	O
use	O
a	O
randomized	O
decision	O
rule	O
,	O
choosing	O
class	O
j	O
with	O
probability	O
q	O
(	O
cj|x	O
)	O
.	O
calculate	O
the	O
error	O
for	O
this	O
decision	O
rule	O
,	O
and	O
show	O
that	O
the	O
error	O
is	O
minimized	O
by	O
using	O
bayes	O
’	O
decision	O
rule	O
.	O
(	O
cid:1	O
)	O
and	O
p	O
(	O
x|class2	O
)	O
=	O
n	O
(	O
cid:0	O
)	O
x	O
µ1	O
,	O
σ2	O
(	O
cid:0	O
)	O
x	O
m1	O
,	O
σ2	O
(	O
cid:1	O
)	O
and	O
class	O
2	O
has	O
the	O
distribution	B
p	O
(	O
x|c	O
=	O
2	O
)	O
∼	O
n	O
exercise	O
152.	O
consider	O
datapoints	O
generated	O
from	O
two	O
diﬀerent	O
classes	O
.	O
class	O
1	O
has	O
the	O
distribution	B
p	O
(	O
x|c	O
=	O
1	O
)	O
∼	O
n	O
probabilities	O
of	O
each	O
class	O
are	O
p	O
(	O
c	O
=	O
1	O
)	O
=	O
p	O
(	O
c	O
=	O
2	O
)	O
=	O
1/2	O
.	O
show	O
that	O
the	O
posterior	B
probability	O
p	O
(	O
c	O
=	O
1|x	O
)	O
is	O
of	O
the	O
form	O
(	O
cid:0	O
)	O
x	O
m2	O
,	O
σ2	O
(	O
cid:1	O
)	O
.	O
the	O
prior	B
p	O
(	O
c	O
=	O
1|x	O
)	O
=	O
1	O
1	O
+	O
exp−	O
(	O
ax	O
+	O
b	O
)	O
and	O
determine	O
a	O
and	O
b	O
in	O
terms	O
of	O
m1	O
,	O
m2	O
and	O
σ2	O
.	O
(	O
13.8.1	O
)	O
exercise	O
153.	O
wowco.com	O
is	O
a	O
new	O
startup	O
prediction	B
company	O
.	O
after	O
years	O
of	O
failures	O
,	O
they	O
eventually	O
ﬁnd	O
a	O
neural	B
network	I
with	O
a	O
trillion	O
hidden	B
units	O
that	O
achieves	O
zero	O
test	O
error	O
on	O
every	O
learning	B
problem	O
posted	O
on	O
the	O
internet	O
up	O
last	O
week	O
.	O
each	O
learning	B
problem	O
included	O
a	O
training	B
and	O
test	B
set	I
.	O
proud	O
of	O
their	O
achievement	O
,	O
they	O
market	O
their	O
product	O
aggressively	O
with	O
the	O
claim	O
that	O
it	O
‘	O
predicts	O
perfectly	O
on	O
all	O
known	O
problems	O
’	O
.	O
would	O
you	O
buy	O
this	O
product	O
?	O
justify	O
your	O
answer	O
.	O
exercise	O
154.	O
three	O
people	O
classify	O
images	O
into	O
1	O
of	O
three	O
categories	O
.	O
each	O
column	O
in	O
the	O
table	O
below	O
represents	O
the	O
classiﬁcations	O
of	O
each	O
image	O
,	O
with	O
the	O
top	O
row	O
being	O
the	O
class	O
from	O
person	O
1	O
,	O
the	O
middle	O
from	O
person	O
2	O
and	O
the	O
bottom	O
from	O
person	O
3	O
.	O
1	O
1	O
1	O
3	O
3	O
2	O
1	O
1	O
1	O
3	O
2	O
1	O
1	O
2	O
1	O
1	O
3	O
3	O
3	O
3	O
2	O
2	O
3	O
2	O
2	O
2	O
2	O
3	O
3	O
3	O
1	O
3	O
1	O
1	O
2	O
2	O
1	O
2	O
1	O
1	O
2	O
2	O
1	O
2	O
1	O
1	O
1	O
1	O
1	O
2	O
2	O
1	O
1	O
3	O
1	O
3	O
3	O
2	O
2	O
2	O
assuming	O
no	O
prior	O
preference	O
amongst	O
hypotheses	O
and	O
a	O
uniform	B
prior	O
on	O
counts	O
,	O
compute	O
p	O
(	O
persons	O
1	O
,	O
2	O
and	O
3	O
classify	O
diﬀerently	O
)	O
p	O
(	O
persons	O
1	O
,	O
2	O
and	O
3	O
classify	O
the	O
same	O
)	O
(	O
13.8.2	O
)	O
exercise	O
155	O
(	O
better	O
than	O
random	B
guessing	I
?	O
)	O
.	O
consider	O
a	O
classiﬁer	B
that	O
makes	O
r	O
correct	O
classiﬁcations	O
and	O
w	O
wrong	O
classiﬁcations	O
.	O
is	O
the	O
classiﬁer	B
better	O
than	O
random	B
guessing	I
?	O
let	O
d	O
be	O
the	O
fact	O
that	O
there	O
are	O
r	O
right	O
and	O
w	O
wrong	O
answers	O
.	O
assume	O
also	O
that	O
the	O
classiﬁcations	O
are	O
i.i.d	O
.	O
1.	O
show	O
that	O
under	O
the	O
hypothesis	O
the	O
data	B
is	O
generated	O
purely	O
at	O
random	O
,	O
p	O
(	O
d|hrandom	O
)	O
=	O
0.5r+w	O
2.	O
deﬁne	O
θ	O
to	O
be	O
the	O
probability	B
that	O
the	O
classiﬁer	B
makes	O
an	O
error	O
.	O
then	O
p	O
(	O
d|θ	O
)	O
=	O
θr	O
(	O
1	O
−	O
θ	O
)	O
w	O
270	O
(	O
13.8.3	O
)	O
(	O
13.8.4	O
)	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
then	O
consider	O
p	O
(	O
d|hnon	O
random	O
)	O
=	O
(	O
cid:90	O
)	O
θ	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ	O
)	O
show	O
that	O
for	O
a	O
beta	B
prior	O
,	O
p	O
(	O
θ	O
)	O
=	O
b	O
(	O
θ|a	O
,	O
b	O
)	O
p	O
(	O
d|hnon	O
random	O
)	O
=	O
b	O
(	O
r	O
+	O
a	O
,	O
w	O
+	O
b	O
)	O
b	O
(	O
a	O
,	O
b	O
)	O
where	O
b	O
(	O
a	O
,	O
b	O
)	O
is	O
the	O
beta-function	O
.	O
3.	O
considering	O
the	O
random	O
and	O
non-random	O
hypotheses	O
as	O
a	O
priori	O
equally	O
likely	O
,	O
show	O
that	O
p	O
(	O
hrandom|d	O
)	O
=	O
0.5r+w	O
0.5r+w	O
+	O
b	O
(	O
r+a	O
,	O
w	O
+b	O
)	O
b	O
(	O
a	O
,	O
b	O
)	O
(	O
13.8.5	O
)	O
(	O
13.8.6	O
)	O
(	O
13.8.7	O
)	O
4.	O
for	O
a	O
ﬂat	O
prior	B
a	O
=	O
b	O
=	O
1	O
compute	O
the	O
probability	B
that	O
for	O
10	O
correct	O
and	O
12	O
incorrect	O
classiﬁcations	O
,	O
the	O
data	B
is	O
from	O
a	O
purely	O
random	O
distribution	O
(	O
according	O
to	O
equation	B
(	O
13.8.7	O
)	O
)	O
.	O
repeat	O
this	O
for	O
100	O
correct	O
and	O
120	O
incorrect	O
classiﬁcations	O
.	O
5.	O
show	O
that	O
the	O
standard	B
deviation	I
in	O
the	O
number	O
of	O
errors	O
of	O
a	O
random	O
classiﬁer	O
is	O
0.5√r	O
+	O
w	O
and	O
relate	O
this	O
to	O
the	O
above	O
computation	O
.	O
exercise	O
156.	O
for	O
a	O
prediction	B
model	O
˜p	O
(	O
y|x	O
)	O
and	O
true	O
data	B
generating	O
distribution	B
p	O
(	O
x	O
,	O
y	O
)	O
,	O
we	O
deﬁne	O
the	O
accuracy	O
as	O
(	O
cid:90	O
)	O
a	O
=	O
p	O
(	O
x	O
,	O
y	O
)	O
˜p	O
(	O
y|x	O
)	O
x	O
,	O
y	O
2.	O
you	O
are	O
given	O
a	O
set	O
of	O
training	B
data	O
d	O
=	O
{	O
xn	O
,	O
yn	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
.	O
by	O
taking	O
q	O
(	O
x	O
,	O
y	O
)	O
to	O
be	O
the	O
empirical	B
this	O
shows	O
that	O
the	O
prediction	B
accuracy	O
is	O
lower	O
bounded	O
by	O
the	O
training	B
accuracy	O
and	O
the	O
‘	O
gap	O
’	O
be-	O
tween	O
the	O
empirical	B
distribution	I
and	O
the	O
unknown	O
true	O
data	B
generating	O
mechanism	O
.	O
in	O
theories	O
such	O
as	O
pac	O
bayes	O
,	O
one	O
may	O
bound	B
this	O
gap	O
,	O
resulting	O
in	O
a	O
bound	B
on	O
the	O
predictive	O
accuracy	O
.	O
according	O
to	O
this	O
naive	O
bound	O
,	O
the	O
best	O
thing	O
to	O
do	O
to	O
increase	O
the	O
prediction	B
accuracy	O
is	O
to	O
increase	O
the	O
training	B
accuracy	O
(	O
since	O
the	O
ﬁrst	O
kullback-leibler	O
term	O
is	O
independent	O
of	O
the	O
predictor	O
)	O
.	O
as	O
n	O
increases	O
,	O
the	O
ﬁrst	O
term	O
kullback-leibler	O
term	O
becomes	O
small	O
,	O
and	O
minimising	O
the	O
training	B
error	O
is	O
justiﬁable	O
.	O
draft	O
march	O
9	O
,	O
2010	O
271	O
1.	O
by	O
deﬁning	O
p	O
(	O
x	O
,	O
y	O
)	O
˜p	O
(	O
y|x	O
)	O
a	O
ˆp	O
(	O
x	O
,	O
y	O
)	O
≡	O
and	O
considering	O
kl	O
(	O
q	O
(	O
x	O
,	O
y	O
)	O
|ˆp	O
(	O
x	O
,	O
y	O
)	O
)	O
≥	O
0	O
show	O
that	O
for	O
any	O
distribution	B
q	O
(	O
x	O
,	O
y	O
)	O
,	O
log	O
a	O
≥	O
−kl	O
(	O
q	O
(	O
x	O
,	O
y	O
)	O
|p	O
(	O
x	O
,	O
y	O
)	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
˜p	O
(	O
y|x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
,	O
y	O
)	O
distribution	B
q	O
(	O
x	O
,	O
y	O
)	O
=	O
1	O
n	O
show	O
that	O
n	O
(	O
cid:88	O
)	O
n=1	O
δ	O
(	O
x	O
,	O
xn	O
)	O
δ	O
(	O
y	O
,	O
yn	O
)	O
log	O
a	O
≥	O
−kl	O
(	O
q	O
(	O
x	O
,	O
y	O
)	O
|p	O
(	O
x	O
,	O
y	O
)	O
)	O
+	O
n	O
(	O
cid:88	O
)	O
n=1	O
1	O
n	O
log	O
˜p	O
(	O
yn|xn	O
)	O
(	O
13.8.8	O
)	O
(	O
13.8.9	O
)	O
(	O
13.8.10	O
)	O
(	O
13.8.11	O
)	O
(	O
13.8.12	O
)	O
(	O
13.8.13	O
)	O
assuming	O
that	O
the	O
training	B
data	O
are	O
drawn	O
from	O
a	O
distribution	B
p	O
(	O
y|x	O
)	O
which	O
is	O
deterministic	B
,	O
show	O
that	O
exercises	O
n	O
(	O
cid:88	O
)	O
n=1	O
log	O
a	O
≥	O
−kl	O
(	O
q	O
(	O
x	O
)	O
|p	O
(	O
x	O
)	O
)	O
+	O
1	O
n	O
log	O
˜p	O
(	O
yn|xn	O
)	O
(	O
13.8.14	O
)	O
and	O
hence	O
that	O
,	O
provided	O
the	O
training	B
data	O
is	O
correctly	O
predicted	O
,	O
(	O
q	O
(	O
yn|xn	O
)	O
=	O
p	O
(	O
yn|xn	O
)	O
,	O
the	O
accuracy	O
can	O
be	O
related	O
to	O
the	O
empirical	B
input	O
distribution	B
and	O
true	O
input	O
distribution	B
by	O
−kl	O
(	O
q	O
(	O
x	O
)	O
|p	O
(	O
x	O
)	O
)	O
a	O
≥	O
e	O
(	O
13.8.15	O
)	O
272	O
draft	O
march	O
9	O
,	O
2010	O
chapter	O
14	O
nearest	B
neighbour	I
classiﬁcation	O
14.1	O
do	O
as	O
your	O
neighbour	B
does	O
successful	O
prediction	B
typically	O
relies	O
on	O
smoothness	B
in	O
the	O
data	B
–	O
if	O
the	O
class	O
label	O
can	O
change	O
arbitrarily	O
as	O
we	O
move	O
a	O
small	O
amount	O
in	O
the	O
input	O
space	O
,	O
the	O
problem	B
is	O
essentially	O
random	O
and	O
no	O
algorithm	O
will	O
generalise	O
well	O
.	O
machine	O
learning	B
researchers	O
therefore	O
construct	O
appropriate	O
measures	O
of	O
smoothness	B
for	O
the	O
problem	B
they	O
have	O
at	O
hand	O
.	O
nearest	B
neighbour	I
methods	O
are	O
a	O
good	O
starting	O
point	O
since	O
they	O
readily	O
encode	O
basic	O
smoothness	B
intuitions	O
and	O
are	O
easy	O
to	O
program	O
,	O
forming	O
a	O
useful	O
baseline	O
method	O
.	O
in	O
a	O
classiﬁcation	B
problem	O
each	O
input	O
vector	O
x	O
has	O
a	O
corresponding	O
class	O
label	O
,	O
cn	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
c	O
}	O
.	O
given	O
a	O
dataset	O
of	O
n	O
such	O
training	B
examples	O
,	O
d	O
=	O
{	O
xn	O
,	O
cn	O
}	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
and	O
a	O
novel	O
x	O
,	O
we	O
aim	O
to	O
return	O
the	O
correct	O
class	O
c	O
(	O
x	O
)	O
.	O
a	O
simple	O
,	O
but	O
often	O
eﬀective	O
,	O
strategy	O
for	O
this	O
supervised	B
learning	I
problem	O
can	O
be	O
stated	O
as	O
:	O
for	O
novel	O
x	O
,	O
ﬁnd	O
the	O
nearest	O
input	O
in	O
the	O
training	B
set	O
and	O
use	O
the	O
class	O
of	O
this	O
nearest	O
input	O
,	O
algorithm	B
(	O
13	O
)	O
.	O
for	O
vectors	O
x	O
and	O
x	O
(	O
cid:48	O
)	O
representing	O
two	O
diﬀerent	O
datapoints	O
,	O
we	O
measure	O
‘	O
nearness	O
’	O
using	O
a	O
dissimilarity	B
function	I
d	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
.	O
a	O
common	O
dissimilarity	O
is	O
the	O
squared	O
euclidean	O
distance	O
d	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
(	O
x	O
−	O
x	O
(	O
cid:48	O
)	O
)	O
t	O
(	O
x	O
−	O
x	O
(	O
cid:48	O
)	O
)	O
which	O
can	O
be	O
more	O
conveniently	O
written	O
(	O
x	O
−	O
x	O
(	O
cid:48	O
)	O
)	O
2.	O
based	O
on	O
the	O
squared	O
euclidean	O
distance	O
,	O
the	O
decision	B
boundary	I
is	O
determined	O
by	O
the	O
lines	O
which	O
are	O
the	O
perpendicular	O
bisectors	O
of	O
the	O
closest	O
training	B
points	O
with	O
diﬀerent	O
training	B
labels	O
,	O
see	O
ﬁg	O
(	O
14.1	O
)	O
.	O
this	O
is	O
called	O
a	O
voronoi	O
tessellation	O
.	O
the	O
nearest	B
neighbour	I
algorithm	O
is	O
simple	O
and	O
intuitive	O
.	O
there	O
are	O
,	O
however	O
,	O
some	O
issues	O
:	O
(	O
14.1.1	O
)	O
(	O
14.1.2	O
)	O
•	O
how	O
should	O
we	O
measure	O
the	O
distance	O
between	O
points	O
?	O
whilst	O
the	O
euclidean	O
square	O
distance	O
is	O
popular	O
,	O
this	O
may	O
not	O
always	O
be	O
appropriate	O
.	O
a	O
fundamental	O
limitation	O
of	O
the	O
euclidean	O
distance	O
is	O
that	O
it	O
does	O
not	O
take	O
into	O
account	O
how	O
the	O
data	B
is	O
distributed	O
.	O
for	O
example	O
if	O
the	O
length	O
scales	O
of	O
x	O
vary	O
greatly	O
the	O
largest	O
length	O
scale	O
will	O
dominate	O
the	O
squared	O
distance	O
,	O
with	O
potentially	O
useful	O
class-speciﬁc	O
information	O
in	O
other	O
components	O
of	O
x	O
lost	O
.	O
the	O
mahalanobis	O
distance	O
d	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
(	O
cid:0	O
)	O
x	O
−	O
x	O
(	O
cid:48	O
)	O
(	O
cid:1	O
)	O
t	O
σ−1	O
(	O
cid:0	O
)	O
x	O
−	O
x	O
(	O
cid:48	O
)	O
(	O
cid:1	O
)	O
where	O
σ	O
is	O
the	O
covariance	B
matrix	O
of	O
the	O
inputs	O
(	O
from	O
all	O
classes	O
)	O
can	O
overcome	O
some	O
of	O
these	O
problems	O
since	O
it	O
rescales	O
all	O
length	O
scales	O
to	O
be	O
essentially	O
equal	O
.	O
•	O
the	O
whole	O
dataset	O
needs	O
to	O
be	O
stored	O
to	O
make	O
a	O
classiﬁcation	B
.	O
this	O
can	O
be	O
addressed	O
by	O
a	O
method	O
called	O
data	B
editing	O
in	O
which	O
datapoints	O
which	O
have	O
little	O
or	O
no	O
eﬀect	O
on	O
the	O
decision	B
boundary	I
are	O
removed	O
from	O
the	O
training	B
dataset	O
.	O
273	O
k-nearest	O
neighbours	O
figure	O
14.1	O
:	O
in	O
nearest	B
neighbour	I
classiﬁcation	O
a	O
new	O
vector	O
is	O
assigned	O
the	O
label	O
of	O
the	O
nearest	O
vector	O
in	O
the	O
training	B
set	O
.	O
here	O
there	O
are	O
three	O
classes	O
,	O
with	O
training	O
points	O
given	O
by	O
the	O
circles	O
,	O
along	O
with	O
their	O
class	O
.	O
the	O
dots	O
indicate	O
the	O
class	O
of	O
the	O
nearest	O
train-	O
ing	O
vector	O
.	O
the	O
decision	B
boundary	I
is	O
piecewise	O
linear	B
with	O
each	O
segment	O
corresponding	O
to	O
the	O
perpendicu-	O
lar	O
bisector	O
between	O
two	O
datapoints	O
belonging	O
to	O
dif-	O
ferent	O
classes	O
,	O
giving	O
rise	O
to	O
a	O
voronoi	O
tessellation	O
of	O
the	O
input	O
space	O
.	O
algorithm	B
13	O
nearest	B
neighbour	I
algorithm	O
to	O
classify	O
a	O
new	O
vector	O
x	O
,	O
given	O
a	O
set	O
of	O
training	B
data	O
d	O
=	O
{	O
(	O
xn	O
,	O
cn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
:	O
1	O
:	O
calculate	O
the	O
dissimilarity	O
of	O
the	O
test	O
point	O
x	O
to	O
each	O
of	O
the	O
stored	O
points	O
,	O
dn	O
=	O
d	O
(	O
x	O
,	O
xn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
.	O
2	O
:	O
find	O
the	O
training	B
point	O
xn∗	O
which	O
is	O
nearest	O
to	O
x	O
:	O
n	O
∗	O
=	O
argmin	O
n	O
d	O
(	O
x	O
,	O
xn	O
)	O
3	O
:	O
assign	O
the	O
class	O
label	O
c	O
(	O
x	O
)	O
=	O
cn∗	O
4	O
:	O
in	O
the	O
case	O
that	O
there	O
are	O
two	O
or	O
more	O
‘	O
equidistant	O
’	O
neighbours	O
with	O
diﬀerent	O
class	O
labels	O
,	O
the	O
most	O
numerous	O
if	O
there	O
is	O
no	O
one	O
single	O
most	O
numerous	O
class	O
,	O
we	O
use	O
the	O
k-nearest-neighbours	O
case	O
.	O
class	O
is	O
chosen	O
.	O
described	O
in	O
the	O
next	O
section	O
.	O
•	O
each	O
distance	O
calculation	O
can	O
be	O
expensive	O
if	O
the	O
datapoints	O
are	O
high	O
dimensional	O
.	O
principal	O
com-	O
ponents	O
analysis	B
,	O
see	O
chapter	O
(	O
15	O
)	O
,	O
is	O
one	O
way	O
to	O
address	O
this	O
and	O
replaces	O
xn	O
with	O
a	O
low	B
dimensional	I
projection	O
p.	O
the	O
euclidean	O
distance	O
of	O
two	O
datapoints	O
(	O
cid:0	O
)	O
xa	O
−	O
xb	O
(	O
cid:1	O
)	O
2	O
is	O
then	O
approximately	O
given	O
by	O
(	O
cid:0	O
)	O
pa	O
−	O
pb	O
(	O
cid:1	O
)	O
2	O
,	O
see	O
section	O
(	O
15.2.4	O
)	O
.	O
this	O
is	O
both	O
faster	O
to	O
compute	O
and	O
can	O
also	O
improve	O
classiﬁcation	B
accuracy	O
since	O
only	O
the	O
large	O
scale	O
characteristics	O
of	O
the	O
data	B
are	O
retained	O
in	O
the	O
pca	O
projections	O
.	O
•	O
it	O
is	O
not	O
clear	O
how	O
to	O
deal	O
with	O
missing	O
data	B
or	O
incorporate	O
prior	B
beliefs	O
and	O
domain	B
knowledge	O
.	O
•	O
for	O
large	O
databases	O
,	O
computing	O
the	O
nearest	B
neighbour	I
of	O
a	O
novel	O
point	O
x∗	O
can	O
be	O
very	O
time-consuming	O
since	O
x∗	O
needs	O
to	O
be	O
compared	O
to	O
each	O
of	O
the	O
training	B
points	O
.	O
depending	O
on	O
the	O
geometry	O
of	O
the	O
training	B
points	O
,	O
ﬁnding	O
the	O
nearest	B
neighbour	I
can	O
accelerated	O
by	O
examining	O
the	O
values	O
of	O
each	O
of	O
the	O
components	O
xi	O
of	O
x	O
in	O
turn	O
.	O
such	O
an	O
axis-aligned	O
space-split	O
is	O
called	O
a	O
kd-tree	O
[	O
202	O
]	O
and	O
can	O
reduce	O
the	O
possible	O
set	O
of	O
candidate	O
nearest	O
neighbours	O
in	O
the	O
training	B
set	O
to	O
the	O
novel	O
x∗	O
,	O
particularly	O
in	O
low	O
dimensions	O
.	O
14.2	O
k-nearest	O
neighbours	O
basing	O
the	O
classiﬁcation	B
on	O
only	O
the	O
single	O
nearest	B
neighbour	I
can	O
lead	O
to	O
inaccuracies	O
.	O
if	O
your	O
neighbour	B
is	O
simply	O
mistaken	O
(	O
has	O
an	O
incorrect	O
training	B
class	O
label	O
)	O
,	O
or	O
is	O
not	O
a	O
particularly	O
representative	O
example	O
of	O
his	O
class	O
,	O
then	O
these	O
situations	O
will	O
typically	O
result	O
in	O
an	O
incorrect	O
classiﬁcation	B
.	O
by	O
including	O
more	O
than	O
the	O
single	O
nearest	B
neighbour	I
,	O
we	O
hope	O
to	O
make	O
a	O
more	O
robust	O
classiﬁer	O
with	O
a	O
smoother	O
decision	B
boundary	I
(	O
less	O
swayed	O
by	O
single	O
neighbour	B
opinions	O
)	O
.	O
for	O
datapoints	O
which	O
are	O
somewhat	O
anomalous	O
compared	O
with	O
274	O
draft	O
march	O
9	O
,	O
2010	O
a	O
probabilistic	B
interpretation	O
of	O
nearest	O
neighbours	O
figure	O
14.2	O
:	O
in	O
k-nearest	O
neighbours	O
,	O
we	O
centre	O
a	O
hypersphere	O
around	O
the	O
point	O
we	O
wish	O
to	O
classify	O
(	O
here	O
the	O
central	O
dot	O
)	O
.	O
the	O
inner	O
circle	O
corre-	O
sponds	O
to	O
the	O
nearest	B
neighbour	I
.	O
however	O
,	O
using	O
the	O
3	O
nearest	O
neighbours	O
,	O
we	O
ﬁnd	O
that	O
there	O
are	O
two	O
‘	O
blue	O
’	O
classes	O
and	O
one	O
‘	O
red	O
’	O
–	O
and	O
we	O
would	O
there-	O
fore	O
class	O
the	O
point	O
as	O
‘	O
blue	O
’	O
class	O
.	O
in	O
the	O
case	O
of	O
a	O
tie	O
,	O
one	O
can	O
extend	O
k	O
until	O
the	O
tie	O
is	O
broken	O
.	O
neighbours	O
from	O
the	O
same	O
class	O
,	O
their	O
inﬂuence	O
will	O
be	O
outvoted	O
.	O
if	O
we	O
assume	O
the	O
euclidean	O
distance	O
as	O
the	O
dissimilarity	O
measure	O
,	O
the	O
k-nearest	O
neighbour	B
algorithm	O
considers	O
a	O
hypersphere	O
centred	O
on	O
the	O
test	O
point	O
x.	O
we	O
increase	O
the	O
radius	O
r	O
until	O
the	O
hypersphere	O
contains	O
exactly	O
k	O
points	O
in	O
the	O
training	B
data	O
.	O
the	O
class	O
label	O
c	O
(	O
x	O
)	O
is	O
then	O
given	O
by	O
the	O
most	O
numerous	O
class	O
within	O
the	O
hypersphere	O
.	O
choosing	O
k	O
whilst	O
there	O
is	O
some	O
sense	O
in	O
making	O
k	O
>	O
1	O
,	O
there	O
is	O
certainly	O
little	O
sense	O
in	O
making	O
k	O
=	O
n	O
(	O
n	O
being	O
the	O
number	O
of	O
training	B
points	O
)	O
.	O
for	O
k	O
very	O
large	O
,	O
all	O
classiﬁcations	O
will	O
become	O
the	O
same	O
–	O
simply	O
assign	O
each	O
novel	O
x	O
to	O
the	O
most	O
numerous	O
class	O
in	O
the	O
training	B
data	O
.	O
this	O
suggests	O
that	O
there	O
is	O
an	O
‘	O
optimal	O
’	O
intermediate	O
setting	O
of	O
k	O
which	O
gives	O
the	O
best	O
generalisation	B
performance	O
.	O
this	O
can	O
be	O
determined	O
using	O
cross-validation	B
,	O
as	O
described	O
in	O
section	O
(	O
13.2.3	O
)	O
.	O
example	O
67	O
(	O
handwritten	O
digit	O
example	O
)	O
.	O
consider	O
two	O
classes	O
of	O
handwritten	B
digits	I
,	O
zeros	O
and	O
ones	O
.	O
each	O
digit	O
contains	O
28	O
×	O
28	O
=	O
784	O
pixels	O
.	O
the	O
training	B
data	O
consists	O
of	O
300	O
zeros	O
,	O
and	O
300	O
ones	O
,	O
a	O
subset	O
of	O
which	O
are	O
plotted	O
in	O
ﬁg	O
(	O
14.3a	O
,	O
b	O
)	O
.	O
to	O
test	O
the	O
performance	B
of	O
the	O
nearest	B
neighbour	I
method	O
(	O
based	O
on	O
euclidean	O
distance	O
)	O
we	O
use	O
an	O
independent	O
test	O
set	O
containing	O
a	O
further	O
600	O
digits	O
.	O
the	O
nearest	B
neighbour	I
method	O
,	O
applied	O
to	O
this	O
data	B
,	O
correctly	O
predicts	O
the	O
class	O
label	O
of	O
all	O
600	O
test	O
points	O
.	O
the	O
reason	O
for	O
the	O
high	O
success	O
rate	O
is	O
that	O
examples	O
of	O
zeros	O
and	O
ones	O
are	O
suﬃciently	O
diﬀerent	O
that	O
they	O
can	O
be	O
easily	O
distinguished	O
.	O
a	O
more	O
diﬃcult	O
task	O
is	O
to	O
distinguish	O
between	O
ones	O
and	O
sevens	O
.	O
we	O
repeat	O
the	O
above	O
experiment	O
,	O
now	O
using	O
300	O
training	B
examples	O
of	O
ones	O
,	O
and	O
300	O
training	B
examples	O
of	O
sevens	O
,	O
ﬁg	O
(	O
14.3b	O
,	O
c	O
)	O
.	O
again	O
,	O
600	O
new	O
test	O
examples	O
(	O
containing	O
300	O
ones	O
and	O
300	O
sevens	O
)	O
were	O
used	O
to	O
assess	O
the	O
performance	B
.	O
this	O
time	O
,	O
18	O
errors	O
are	O
found	O
using	O
nearest	B
neighbour	I
classiﬁcation	O
–	O
a	O
3	O
%	O
error	O
rate	O
for	O
this	O
two	O
class	O
problem	B
.	O
the	O
18	O
test	O
points	O
on	O
which	O
the	O
nearest	B
neighbour	I
method	O
makes	O
errors	O
are	O
plotted	O
in	O
ﬁg	O
(	O
14.4	O
)	O
.	O
if	O
we	O
use	O
k	O
=	O
3	O
nearest	O
neighbours	O
,	O
the	O
classiﬁcation	B
error	O
reduces	O
to	O
14	O
–	O
a	O
slight	O
improvement	O
.	O
as	O
an	O
aside	O
,	O
the	O
best	O
machine	O
learning	B
methods	O
classify	O
real	O
world	O
digits	O
(	O
over	O
all	O
10	O
classes	O
)	O
to	O
an	O
error	O
of	O
less	O
than	O
1	O
%	O
(	O
yann.lecun.com/exdb/mnist	O
)	O
–	O
better	O
than	O
the	O
performance	B
of	O
an	O
‘	O
average	B
’	O
human	O
.	O
14.3	O
a	O
probabilistic	B
interpretation	O
of	O
nearest	O
neighbours	O
consider	O
the	O
situation	O
where	O
we	O
have	O
(	O
for	O
simplicity	O
)	O
data	B
from	O
two	O
classes	O
–	O
class	O
0	O
and	O
class	O
1.	O
we	O
make	O
the	O
following	O
mixture	B
model	I
for	O
data	B
from	O
class	O
0	O
:	O
−	O
(	O
x−xn	O
)	O
2/	O
(	O
2σ2	O
)	O
e	O
(	O
14.3.1	O
)	O
(	O
cid:88	O
)	O
(	O
cid:0	O
)	O
x	O
xn	O
,	O
σ2i	O
(	O
cid:1	O
)	O
=	O
p	O
(	O
x|c	O
=	O
0	O
)	O
=	O
1	O
n0	O
n	O
n∈	O
class	O
0	O
(	O
cid:88	O
)	O
1	O
1	O
n0	O
(	O
2πσ2	O
)	O
d/2	O
n∈	O
class	O
0	O
where	O
d	O
is	O
the	O
dimension	O
of	O
a	O
datapoint	O
x	O
and	O
n0	O
are	O
the	O
number	O
of	O
training	B
datapoints	O
of	O
class	O
0	O
,	O
and	O
σ2	O
is	O
the	O
variance	B
.	O
this	O
is	O
a	O
parzen	O
estimator	B
,	O
which	O
models	O
the	O
data	B
distribution	O
as	O
a	O
uniform	B
weighted	O
sum	O
of	O
distributions	O
centred	O
on	O
the	O
training	B
points	O
,	O
ﬁg	O
(	O
14.5	O
)	O
.	O
draft	O
march	O
9	O
,	O
2010	O
275	O
a	O
probabilistic	B
interpretation	O
of	O
nearest	O
neighbours	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
14.3	O
:	O
some	O
of	O
the	O
training	B
examples	O
of	O
the	O
digit	O
zero	O
and	O
(	O
a	O
)	O
,	O
one	O
(	O
b	O
)	O
and	O
seven	O
(	O
c	O
)	O
.	O
there	O
are	O
300	O
training	B
examples	O
of	O
each	O
of	O
these	O
three	O
digit	O
classes	O
.	O
figure	O
14.4	O
:	O
‘	O
1	O
’	O
versus	O
‘	O
7	O
’	O
classiﬁcation	B
us-	O
ing	O
the	O
nn	O
method	O
.	O
(	O
top	O
)	O
the	O
18	O
out	O
of	O
600	O
test	O
examples	O
that	O
are	O
incorrectly	O
clas-	O
siﬁed	O
;	O
(	O
bottom	O
)	O
the	O
nearest	O
neighbours	O
in	O
the	O
training	B
set	O
corresponding	O
to	O
each	O
test-	O
point	O
above	O
.	O
(	O
cid:88	O
)	O
1	O
(	O
2πσ2	O
)	O
d/2	O
n∈	O
class	O
1	O
−	O
(	O
x−xn	O
)	O
2/	O
(	O
2σ2	O
)	O
e	O
(	O
14.3.2	O
)	O
similarly	O
,	O
for	O
data	B
from	O
class	O
1	O
:	O
(	O
cid:88	O
)	O
(	O
cid:0	O
)	O
x	O
xn	O
,	O
σ2i	O
(	O
cid:1	O
)	O
=	O
1	O
n1	O
p	O
(	O
x|c	O
=	O
1	O
)	O
=	O
1	O
n1	O
to	O
classify	O
a	O
new	O
datapoint	O
x∗	O
,	O
we	O
use	O
bayes	O
’	O
rule	O
|c	O
=	O
0	O
)	O
p	O
(	O
c	O
=	O
0	O
)	O
n∈	O
class	O
1	O
p	O
(	O
x∗	O
n	O
p	O
(	O
c	O
=	O
0|x∗	O
)	O
=	O
p	O
(	O
x∗	O
|c	O
=	O
0	O
)	O
p	O
(	O
c	O
=	O
0	O
)	O
+	O
p	O
(	O
x∗	O
|c	O
=	O
1	O
)	O
p	O
(	O
c	O
=	O
1	O
)	O
the	O
maximum	B
likelihood	I
setting	O
of	O
p	O
(	O
c	O
=	O
0	O
)	O
is	O
n0/	O
(	O
n0	O
+	O
n1	O
)	O
,	O
and	O
p	O
(	O
c	O
=	O
1	O
)	O
=	O
n1/	O
(	O
n0	O
+	O
n1	O
)	O
.	O
an	O
analogous	O
expression	O
to	O
equation	B
(	O
14.3.3	O
)	O
holds	O
for	O
p	O
(	O
c	O
=	O
1|x∗	O
)	O
.	O
to	O
see	O
which	O
class	O
is	O
most	O
likely	O
we	O
may	O
use	O
the	O
ratio	O
(	O
14.3.3	O
)	O
(	O
14.3.4	O
)	O
p	O
(	O
c	O
=	O
0|x∗	O
)	O
p	O
(	O
c	O
=	O
1|x∗	O
)	O
=	O
p	O
(	O
x∗	O
p	O
(	O
x∗	O
|c	O
=	O
0	O
)	O
p	O
(	O
c	O
=	O
0	O
)	O
|c	O
=	O
1	O
)	O
p	O
(	O
c	O
=	O
1	O
)	O
if	O
this	O
ratio	O
is	O
greater	O
than	O
one	O
,	O
we	O
classify	O
x∗	O
as	O
0	O
,	O
otherwise	O
1.	O
equation	B
(	O
14.3.4	O
)	O
is	O
a	O
complicated	O
function	B
of	O
x∗	O
.	O
however	O
,	O
if	O
σ2	O
is	O
very	O
small	O
,	O
the	O
numerator	O
,	O
which	O
is	O
a	O
sum	O
of	O
exponential	B
terms	O
,	O
will	O
be	O
dominated	O
by	O
that	O
term	O
for	O
which	O
datapoint	O
xn0	O
in	O
class	O
0	O
is	O
closest	O
to	O
the	O
point	O
x∗	O
.	O
similarly	O
,	O
the	O
denominator	O
will	O
be	O
dominated	O
by	O
that	O
datapoint	O
xn1	O
in	O
class	O
1	O
which	O
is	O
closest	O
to	O
x∗	O
.	O
in	O
this	O
case	O
,	O
therefore	O
,	O
p	O
(	O
c	O
=	O
0|x∗	O
)	O
p	O
(	O
c	O
=	O
1|x∗	O
)	O
≈	O
e−	O
(	O
x∗−xn0	O
)	O
2/	O
(	O
2σ2	O
)	O
p	O
(	O
c	O
=	O
0	O
)	O
/n0	O
e−	O
(	O
x∗−xn1	O
)	O
2/	O
(	O
2σ2	O
)	O
p	O
(	O
c	O
=	O
1	O
)	O
/n1	O
=	O
e−	O
(	O
x∗−xn0	O
)	O
2/	O
(	O
2σ2	O
)	O
e−	O
(	O
x∗−xn1	O
)	O
2/	O
(	O
2σ2	O
)	O
(	O
14.3.5	O
)	O
taking	O
the	O
limit	O
σ2	O
→	O
0	O
,	O
with	O
certainty	O
we	O
classify	O
x∗	O
as	O
class	O
0	O
if	O
x∗	O
has	O
a	O
point	O
in	O
the	O
class	O
0	O
data	B
which	O
is	O
closer	O
than	O
the	O
closest	O
point	O
in	O
the	O
class	O
1	O
data	O
.	O
the	O
nearest	O
(	O
single	O
)	O
neighbour	B
method	O
is	O
therefore	O
recovered	O
as	O
the	O
limiting	O
case	O
of	O
a	O
probabilistic	B
generative	O
model	B
,	O
see	O
ﬁg	O
(	O
14.5	O
)	O
.	O
the	O
motivation	O
of	O
using	O
k	O
nearest	O
neighbours	O
is	O
to	O
produce	O
a	O
result	O
that	O
is	O
robust	O
against	O
unrepresentative	O
nearest	O
neighbours	O
.	O
to	O
ensure	O
a	O
similar	O
kind	O
of	O
robustness	O
in	O
the	O
probabilistic	B
interpretation	O
,	O
we	O
may	O
use	O
a	O
ﬁnite	O
value	O
σ2	O
>	O
0.	O
this	O
smoothes	O
the	O
extreme	O
probabilities	O
of	O
classiﬁcation	B
and	O
means	O
that	O
more	O
points	O
(	O
not	O
just	O
the	O
nearest	O
)	O
will	O
have	O
an	O
eﬀective	O
contribution	O
in	O
equation	B
(	O
14.3.4	O
)	O
.	O
the	O
extension	O
to	O
more	O
than	O
276	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
figure	O
14.5	O
:	O
a	O
probabilistic	B
interpretation	O
of	O
nearest	O
neighbours	O
.	O
for	O
each	O
class	O
we	O
use	O
a	O
mixture	O
of	O
gaussians	O
to	O
model	B
the	O
data	B
from	O
that	O
class	O
p	O
(	O
x|c	O
)	O
,	O
placing	O
at	O
each	O
training	B
point	O
an	O
isotropic	B
gaussian	O
of	O
width	O
σ2	O
.	O
in	O
the	O
limit	O
the	O
width	O
of	O
each	O
gaussian	O
is	O
represented	O
by	O
the	O
circle	O
.	O
σ2	O
→	O
0	O
a	O
novel	O
point	O
(	O
black	O
)	O
is	O
assigned	O
the	O
class	O
of	O
its	O
nearest	B
neighbour	I
.	O
for	O
ﬁnite	O
σ2	O
>	O
0	O
the	O
inﬂuence	O
of	O
non-nearest	O
neighbours	O
has	O
an	O
eﬀect	O
,	O
resulting	O
in	O
a	O
soft	B
version	O
of	O
nearest	O
neighbours	O
.	O
two	O
classes	O
is	O
straightforward	O
,	O
requiring	O
a	O
class	O
conditional	B
generative	O
model	B
for	O
each	O
class	O
.	O
to	O
go	O
beyond	O
nearest	B
neighbour	I
methods	O
,	O
we	O
can	O
relax	O
the	O
assumption	O
of	O
using	O
a	O
parzen	O
estimator	B
,	O
and	O
use	O
a	O
richer	O
generative	B
model	O
.	O
we	O
will	O
examine	O
such	O
cases	O
in	O
some	O
detail	O
in	O
later	O
chapters	O
,	O
in	O
particular	O
chapter	O
(	O
20	O
)	O
.	O
14.3.1	O
when	O
your	O
nearest	B
neighbour	I
is	O
far	O
away	O
for	O
a	O
novel	O
input	O
x∗	O
that	O
is	O
far	O
from	O
all	O
training	B
points	O
,	O
nearest	O
neighbours	O
,	O
and	O
its	O
soft	B
probabilistic	O
variant	O
will	O
conﬁdently	O
classify	O
x∗	O
as	O
belonging	O
to	O
the	O
class	O
of	O
the	O
nearest	O
training	O
point	O
.	O
this	O
is	O
arguably	O
opposite	O
to	O
what	O
we	O
would	O
like	O
,	O
namely	O
that	O
the	O
classiﬁcation	B
should	O
tend	O
to	O
the	O
prior	B
probabilities	O
of	O
the	O
class	O
based	O
on	O
the	O
number	O
of	O
training	B
data	O
per	O
class	O
.	O
a	O
way	O
to	O
avoid	O
this	O
problem	B
is	O
,	O
for	O
each	O
class	O
,	O
to	O
include	O
a	O
ﬁctitious	O
mixture	B
component	O
at	O
the	O
mean	B
of	O
all	O
the	O
data	B
with	O
large	O
variance	B
,	O
equal	O
for	O
each	O
class	O
.	O
for	O
novel	O
inputs	O
close	O
to	O
the	O
training	B
data	O
,	O
this	O
extra	O
ﬁctitious	O
datapoint	O
will	O
have	O
no	O
appreciable	O
eﬀect	O
.	O
however	O
,	O
as	O
we	O
move	O
away	O
from	O
the	O
high	O
density	O
regions	O
of	O
the	O
training	B
data	O
,	O
this	O
additional	O
ﬁctitious	O
component	O
will	O
dominate	O
.	O
since	O
the	O
distance	O
from	O
x∗	O
to	O
each	O
ﬁctitious	O
class	O
point	O
is	O
the	O
same	O
,	O
in	O
the	O
limit	O
that	O
x∗	O
is	O
far	O
from	O
the	O
training	B
data	O
,	O
the	O
eﬀect	O
is	O
that	O
no	O
class	O
information	O
from	O
the	O
position	O
of	O
x∗	O
occurs	O
.	O
see	O
section	O
(	O
20.3.3	O
)	O
for	O
an	O
example	O
.	O
14.4	O
code	O
nearneigh.m	O
:	O
k	O
nearest	B
neighbour	I
14.4.1	O
utility	B
routines	O
majority.m	O
:	O
find	O
the	O
majority	O
entry	O
in	O
each	O
column	O
of	O
a	O
matrix	B
14.4.2	O
demonstration	O
demonearneigh.m	O
:	O
k	O
nearest	B
neighbour	I
demo	O
14.5	O
exercises	O
exercise	O
157.	O
the	O
ﬁle	O
nndata.mat	O
contains	O
training	B
and	O
test	O
data	O
for	O
the	O
handwritten	B
digits	I
5	O
and	O
9.	O
using	O
leave	O
one	O
out	O
cross-validation	B
,	O
ﬁnd	O
the	O
optimal	O
k	O
in	O
k-nearest	O
neighours	O
,	O
and	O
use	O
this	O
to	O
compute	O
the	O
classiﬁcation	B
accuracy	O
of	O
the	O
method	O
on	O
the	O
test	O
data	O
.	O
exercise	O
158.	O
write	O
a	O
routine	O
softnearneigh	O
(	O
xtrain	O
,	O
xtest	O
,	O
trainlabels	O
,	O
sigma	O
)	O
to	O
implement	O
soft	B
nearest	O
neighbours	O
,	O
analogous	O
to	O
nearneigh.m	O
.	O
here	O
sigma	O
is	O
the	O
variance	B
σ2	O
in	O
equation	B
(	O
14.3.1	O
)	O
.	O
as	O
above	O
,	O
the	O
ﬁle	O
nndata.mat	O
contains	O
training	B
and	O
test	O
data	O
for	O
the	O
handwritten	B
digits	I
5	O
and	O
9.	O
using	O
leave	O
one	O
out	O
cross-validation	B
,	O
ﬁnd	O
the	O
optimal	O
σ2	O
and	O
use	O
this	O
to	O
compute	O
the	O
classiﬁcation	B
accuracy	O
of	O
the	O
method	O
on	O
the	O
test	O
data	O
.	O
hint	O
:	O
you	O
may	O
have	O
numerical	B
diﬃculty	O
with	O
this	O
method	O
.	O
to	O
avoid	O
this	O
,	O
consider	O
using	O
the	O
logarithm	O
,	O
and	O
how	O
to	O
numerically	O
compute	O
log	O
(	O
cid:0	O
)	O
ea	O
+	O
eb	O
(	O
cid:1	O
)	O
for	O
large	O
(	O
negative	O
)	O
a	O
and	O
b.	O
see	O
also	O
logsumexp.m	O
.	O
draft	O
march	O
9	O
,	O
2010	O
277	O
exercise	O
159.	O
in	O
the	O
text	O
we	O
suggested	O
the	O
use	O
of	O
the	O
mahalanobis	O
distance	O
d	O
(	O
x	O
,	O
y	O
)	O
=	O
(	O
x	O
−	O
y	O
)	O
t	O
σ−1	O
(	O
x	O
−	O
y	O
)	O
exercises	O
as	O
a	O
way	O
to	O
improve	O
on	O
the	O
euclidean	O
distance	O
,	O
with	O
σ	O
the	O
covariance	B
matrix	O
of	O
the	O
combined	O
data	B
from	O
both	O
classes	O
.	O
consider	O
a	O
modiﬁcation	O
based	O
on	O
using	O
a	O
mixture	B
model	I
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
n∈class	O
0	O
n∈class	O
1	O
and	O
p	O
(	O
x|c	O
=	O
0	O
)	O
=	O
p	O
(	O
x|c	O
=	O
1	O
)	O
=	O
1	O
n0	O
1	O
n1	O
n	O
(	O
x	O
xn	O
,	O
σ0	O
)	O
n	O
(	O
x	O
xn	O
,	O
σ1	O
)	O
1.	O
explain	O
how	O
the	O
soft	B
nearest	O
neighbours	O
algorithm	B
can	O
deal	O
with	O
the	O
issue	O
that	O
the	O
distribution	B
of	O
data	B
from	O
the	O
diﬀerent	O
classes	O
can	O
be	O
very	O
diﬀerent	O
.	O
2.	O
for	O
the	O
case	O
σ0	O
=	O
γ2σ	O
(	O
cid:48	O
)	O
(	O
cid:18	O
)	O
p	O
(	O
c	O
=	O
0|x∗	O
)	O
p	O
(	O
c	O
=	O
1|x∗	O
)	O
log	O
(	O
cid:19	O
)	O
0	O
and	O
σ1	O
=	O
γ2σ	O
(	O
cid:48	O
)	O
1	O
and	O
γ2	O
small	O
,	O
derive	O
a	O
simple	O
expression	O
that	O
approximates	O
exercise	O
160.	O
the	O
editor	O
at	O
yoman	O
!	O
(	O
a	O
‘	O
mens	O
’	O
magazine	O
)	O
has	O
just	O
had	O
a	O
great	O
idea	O
.	O
based	O
on	O
the	O
success	O
of	O
a	O
recent	O
national	O
poll	O
to	O
test	O
iq	O
,	O
she	O
decides	O
to	O
make	O
a	O
‘	O
beauty	O
quotient	O
’	O
(	O
bq	O
)	O
test	O
.	O
she	O
collects	O
as	O
many	O
images	O
of	O
male	O
faces	B
as	O
she	O
can	O
,	O
taking	O
care	O
to	O
make	O
sure	O
that	O
all	O
the	O
images	O
are	O
scaled	O
to	O
roughly	O
the	O
same	O
size	O
and	O
under	O
the	O
same	O
lighting	O
conditions	O
.	O
she	O
then	O
gives	O
each	O
male	O
face	O
a	O
bq	O
score	O
from	O
0	O
(	O
‘	O
severely	O
aesthetically	O
challenged	O
’	O
)	O
to	O
100	O
(	O
‘	O
generously	O
aesthetically	O
gifted	O
’	O
)	O
.	O
thus	O
,	O
for	O
each	O
image	O
x	O
,	O
there	O
is	O
an	O
associated	O
value	B
b	O
in	O
the	O
range	O
0	O
to	O
100.	O
in	O
total	O
she	O
collects	O
n	O
images	O
and	O
associated	O
scores	O
,	O
{	O
(	O
xn	O
,	O
bn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
,	O
where	O
each	O
image	O
is	O
represented	O
by	O
a	O
d-dimensional	O
real-valued	O
vector	O
x.	O
one	O
morning	O
,	O
she	O
bounces	O
into	O
your	O
oﬃce	O
and	O
tells	O
you	O
the	O
good	O
news	O
:	O
it	O
is	O
your	O
task	O
to	O
make	O
a	O
test	O
for	O
the	O
male	O
nation	O
to	O
determine	O
their	O
beauty	O
quotient	O
.	O
the	O
idea	O
,	O
she	O
explains	O
,	O
is	O
that	O
a	O
man	O
can	O
send	O
online	B
an	O
image	O
of	O
their	O
face	O
x∗	O
,	O
to	O
yoman	O
!	O
and	O
will	O
‘	O
instantly	O
’	O
receive	O
an	O
automatic	O
bq	O
response	O
b∗	O
.	O
1.	O
as	O
a	O
ﬁrst	O
step	O
,	O
you	O
decide	O
to	O
use	O
the	O
k	O
nearest	B
neighbour	I
method	O
(	O
knn	O
)	O
to	O
assign	O
a	O
bq	O
score	O
b∗	O
to	O
a	O
novel	O
test	O
image	O
x∗	O
.	O
describe	O
how	O
to	O
determine	O
the	O
optimal	O
number	O
of	O
neighbours	O
k	O
to	O
use	O
.	O
2.	O
your	O
line	O
manager	O
is	O
pleased	O
with	O
your	O
algorithm	B
but	O
is	O
disappointed	O
that	O
it	O
does	O
not	O
provide	O
any	O
simple	O
explanation	O
of	O
beauty	O
that	O
she	O
can	O
present	O
in	O
a	O
future	O
version	O
of	O
yoman	O
!	O
magazine	O
.	O
to	O
address	O
this	O
,	O
you	O
decide	O
to	O
make	O
a	O
model	B
based	O
on	O
linear	B
regression	O
.	O
that	O
is	O
b	O
=	O
wtx	O
(	O
14.5.1	O
)	O
where	O
w	O
is	O
a	O
parameter	B
(	O
weight	B
)	O
vector	O
chosen	O
to	O
minimise	O
e	O
(	O
w	O
)	O
=	O
(	O
cid:88	O
)	O
n	O
(	O
cid:16	O
)	O
bn	O
−	O
wtxn	O
(	O
cid:17	O
)	O
2	O
(	O
a	O
)	O
after	O
training	B
(	O
ﬁnding	O
a	O
suitable	O
w	O
)	O
,	O
how	O
can	O
yoman	O
!	O
explain	O
to	O
its	O
readership	O
in	O
a	O
simple	O
way	O
what	O
facial	O
features	O
are	O
important	O
for	O
determining	O
one	O
’	O
s	O
bq	O
?	O
(	O
b	O
)	O
describe	O
fully	O
and	O
mathematically	O
a	O
method	O
to	O
train	O
this	O
linear	B
regression	O
model	B
.	O
your	O
expla-	O
nation	O
must	O
be	O
detailed	O
enough	O
so	O
that	O
a	O
programmer	O
can	O
directly	O
implement	O
it	O
.	O
(	O
c	O
)	O
discuss	O
any	O
implications	O
of	O
the	O
situation	O
d	O
>	O
n.	O
(	O
d	O
)	O
discuss	O
any	O
advantages/disadvantages	O
of	O
using	O
the	O
linear	B
regression	O
model	B
compared	O
with	O
using	O
the	O
knn	O
approach	B
.	O
278	O
draft	O
march	O
9	O
,	O
2010	O
chapter	O
15	O
unsupervised	B
linear	O
dimension	B
reduction	I
15.1	O
high-dimensional	O
spaces	O
–	O
low	B
dimensional	I
manifolds	O
in	O
machine	O
learning	B
problems	O
data	B
is	O
often	O
high	O
dimensional	O
–	O
images	O
,	O
bag-of-word	O
descriptions	O
,	O
gene-	O
expresssions	O
etc	O
.	O
in	O
such	O
cases	O
we	O
can	O
not	O
expect	O
the	O
training	B
data	O
to	O
densely	O
populate	O
the	O
space	O
,	O
meaning	O
that	O
there	O
will	O
be	O
large	O
parts	O
in	O
which	O
little	O
is	O
known	O
about	O
the	O
data	B
.	O
for	O
the	O
hand-written	O
digits	O
from	O
chapter	O
(	O
14	O
)	O
,	O
the	O
data	B
is	O
784	O
dimensional	O
.	O
for	O
binary	O
valued	O
pixels	O
the	O
possible	O
number	O
of	O
images	O
that	O
could	O
ever	O
exist	O
is	O
2784	O
≈	O
10236.	O
nevertheless	O
,	O
we	O
would	O
expect	O
that	O
only	O
a	O
handful	O
of	O
examples	O
of	O
a	O
digit	O
should	O
be	O
suﬃcient	O
(	O
for	O
a	O
human	O
)	O
to	O
understand	O
how	O
to	O
recognise	O
a	O
7.	O
digit-like	O
images	O
must	O
therefore	O
occupy	O
a	O
highly	O
constrained	O
subspace	O
of	O
the	O
784	O
dimensions	O
and	O
we	O
expect	O
only	O
a	O
small	O
number	O
of	O
directions	O
to	O
be	O
relevant	O
for	O
describing	O
the	O
data	B
to	O
a	O
reasonable	O
accuracy	O
.	O
whilst	O
the	O
data	B
vectors	O
may	O
be	O
very	O
high	O
dimensional	O
,	O
they	O
will	O
therefore	O
typically	O
lie	O
close	O
to	O
a	O
much	O
lower	O
dimensional	O
‘	O
manifold	B
’	O
(	O
informally	O
,	O
a	O
two-dimensional	O
manifold	B
corresponds	O
to	O
a	O
warped	O
sheet	O
of	O
paper	O
embedded	O
in	O
a	O
high	O
dimensional	O
space	O
)	O
,	O
meaning	O
that	O
the	O
distribution	B
of	O
the	O
data	B
is	O
heavily	O
constrained	O
.	O
here	O
we	O
concentrate	O
on	O
linear	B
dimension	I
reduction	I
techniques	O
for	O
which	O
there	O
exist	O
computationally	O
eﬃ-	O
cient	O
approaches	O
.	O
in	O
this	O
approach	B
a	O
high	O
dimensional	O
datapoint	O
x	O
is	O
‘	O
projected	O
down	O
’	O
to	O
a	O
lower	O
dimen-	O
sional	O
vector	O
y	O
by	O
y	O
=	O
fx	O
+	O
const	O
.	O
(	O
15.1.1	O
)	O
where	O
the	O
non-square	O
matrix	B
f	O
has	O
dimensions	O
dim	O
(	O
y	O
)	O
×	O
dim	O
(	O
x	O
)	O
,	O
with	O
dim	O
(	O
y	O
)	O
<	O
dim	O
(	O
x	O
)	O
.	O
the	O
methods	O
in	O
this	O
chapter	O
are	O
largely	O
non-probabilistic	O
,	O
although	O
many	O
have	O
natural	B
probabilistic	O
interpretations	O
.	O
for	O
example	O
,	O
pca	O
is	O
closely	O
related	O
to	O
factor	B
analysis	I
,	O
described	O
in	O
chapter	O
(	O
21	O
)	O
.	O
15.2	O
principal	O
components	O
analysis	B
if	O
data	B
lies	O
close	O
to	O
a	O
hyperplane	B
,	O
as	O
in	O
ﬁg	O
(	O
15.1	O
)	O
we	O
can	O
accurately	O
approximate	B
each	O
data	B
point	O
by	O
using	O
vectors	O
that	O
span	O
the	O
hyperplane	B
alone	O
.	O
eﬀectively	O
,	O
we	O
are	O
trying	O
to	O
discover	O
a	O
low	B
dimensional	I
co-ordinate	O
system	B
in	O
which	O
we	O
can	O
approximately	O
represent	O
the	O
data	B
.	O
we	O
express	O
the	O
approximation	B
for	O
datapoint	O
xn	O
as	O
m	O
(	O
cid:88	O
)	O
i=1	O
xn	O
≈	O
c	O
+	O
i	O
bi	O
≡	O
˜xn	O
yn	O
(	O
15.2.1	O
)	O
here	O
the	O
vector	O
c	O
is	O
a	O
constant	O
and	O
deﬁnes	O
a	O
point	O
in	O
the	O
hyperplane	B
and	O
the	O
bi	O
deﬁne	O
vectors	O
in	O
the	O
hy-	O
perplane	O
(	O
also	O
known	O
as	O
‘	O
principal	O
component	O
coeﬃcients	O
’	O
or	O
‘	O
loadings	O
’	O
)	O
.	O
the	O
yn	O
i	O
are	O
the	O
low	B
dimensional	I
co-ordinates	O
of	O
the	O
data	B
.	O
equation	B
(	O
15.2.1	O
)	O
expresses	O
how	O
to	O
ﬁnd	O
the	O
reconstruction	O
˜xn	O
given	O
the	O
lower	O
dimensional	O
representation	O
yn	O
(	O
which	O
has	O
components	O
yn	O
i	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
)	O
.	O
for	O
a	O
data	B
space	O
of	O
dimension	O
279	O
principal	O
components	O
analysis	B
figure	O
15.1	O
:	O
in	O
linear	B
dimension	I
reduction	I
a	O
hyper-	O
plane	O
is	O
ﬁtted	O
such	O
that	O
the	O
average	B
distance	O
between	O
datapoints	O
(	O
red	O
rings	O
)	O
and	O
their	O
projections	O
onto	O
the	O
plane	O
(	O
black	O
dots	O
)	O
is	O
minimal	O
.	O
n	O
(	O
cid:88	O
)	O
d	O
(	O
cid:88	O
)	O
dim	O
(	O
x	O
)	O
=	O
d	O
,	O
we	O
hope	O
to	O
accurately	O
describe	O
the	O
data	B
using	O
only	O
a	O
small	O
number	O
m	O
(	O
cid:28	O
)	O
d	O
of	O
co-ordinates	O
y.	O
to	O
determine	O
the	O
best	O
lower	O
dimensional	O
representation	O
it	O
is	O
convenient	O
to	O
use	O
the	O
square	O
distance	O
error	O
between	O
x	O
and	O
its	O
reconstruction	O
˜x	O
:	O
e	O
(	O
b	O
,	O
y	O
,	O
c	O
)	O
=	O
i	O
]	O
2	O
i	O
−	O
˜xn	O
[	O
xn	O
it	O
is	O
straightforward	O
to	O
show	O
that	O
the	O
optimal	O
bias	B
c	O
is	O
given	O
by	O
the	O
mean	B
of	O
the	O
data	B
(	O
cid:80	O
)	O
therefore	O
assume	O
that	O
the	O
data	B
has	O
been	O
centred	O
(	O
has	O
zero	O
mean	B
(	O
cid:80	O
)	O
n	O
xn/n	O
.	O
we	O
n	O
xn	O
=	O
0	O
)	O
,	O
so	O
that	O
we	O
can	O
set	O
c	O
to	O
zero	O
,	O
n=1	O
i=1	O
(	O
15.2.2	O
)	O
and	O
concentrate	O
on	O
ﬁnding	O
the	O
optimal	O
basis	O
b	O
below	O
.	O
m	O
(	O
cid:88	O
)	O
2	O
e	O
(	O
b	O
,	O
y	O
)	O
=	O
15.2.1	O
deriving	O
the	O
optimal	O
linear	B
reconstruction	O
to	O
ﬁnd	O
the	O
best	O
basis	O
vectors	O
b	O
=	O
(	O
cid:2	O
)	O
b1	O
,	O
.	O
.	O
.	O
,	O
bm	O
(	O
cid:3	O
)	O
(	O
deﬁning	O
[	O
b	O
]	O
i	O
,	O
j	O
=	O
bj	O
coordinates	O
y	O
=	O
(	O
cid:2	O
)	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
(	O
cid:3	O
)	O
,	O
we	O
may	O
minimize	O
the	O
sum	O
of	O
squared	O
diﬀerences	O
between	O
each	O
vector	O
x	O
i	O
)	O
and	O
corresponding	O
low	B
dimensional	I
and	O
its	O
reconstruction	O
˜x	O
:	O
xn	O
d	O
(	O
cid:88	O
)	O
n	O
(	O
cid:88	O
)	O
where	O
x	O
=	O
(	O
cid:2	O
)	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
(	O
cid:3	O
)	O
.	O
consider	O
a	O
transformation	O
q	O
of	O
the	O
basis	O
b	O
so	O
that	O
˜b	O
≡	O
bq	O
is	O
an	O
orthonormal	B
matrix	O
,	O
˜bt	O
˜b	O
=	O
i.	O
since	O
q	O
is	O
invertible	O
,	O
we	O
may	O
write	O
by	O
=	O
˜bq−1y	O
≡	O
˜b	O
˜y	O
,	O
which	O
is	O
of	O
then	O
same	O
form	O
as	O
by	O
,	O
albeit	O
with	O
an	O
orthonormality	O
constraint	O
on	O
˜b	O
.	O
hence	O
,	O
without	O
loss	O
of	O
generality	O
,	O
we	O
may	O
consider	O
equation	B
(	O
15.2.3	O
)	O
under	O
the	O
orthonormality	O
constraint	O
btb	O
=	O
i	O
,	O
namely	O
that	O
the	O
basis	O
vectors	O
are	O
mutually	O
orthogonal	B
and	O
of	O
unit	O
length	O
.	O
(	O
x	O
−	O
by	O
)	O
t	O
(	O
x	O
−	O
by	O
)	O
(	O
15.2.3	O
)	O
=	O
trace	O
j	O
bj	O
yn	O
i	O
i	O
−	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
n=1	O
i=1	O
j=1	O
by	O
diﬀerentiating	O
equation	B
(	O
15.2.3	O
)	O
with	O
respect	O
to	O
yn	O
k	O
we	O
obtain	O
(	O
using	O
the	O
orthonormality	O
constraint	O
)	O
e	O
(	O
b	O
,	O
y	O
)	O
=	O
(	O
cid:88	O
)	O
xn	O
i	O
−	O
(	O
cid:88	O
)	O
	O
bk	O
i	O
=	O
(	O
cid:88	O
)	O
j	O
bj	O
yn	O
i	O
i	O
j	O
i	O
1	O
2	O
−	O
∂	O
∂yn	O
k	O
(	O
cid:88	O
)	O
j	O
=	O
(	O
cid:88	O
)	O
i	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
bj	O
i	O
bk	O
i	O
i	O
δjk	O
xn	O
i	O
bk	O
i	O
−	O
yn	O
j	O
the	O
squared	O
error	O
e	O
(	O
b	O
,	O
y	O
)	O
therefore	O
has	O
zero	O
derivative	O
when	O
k	O
=	O
(	O
cid:88	O
)	O
yn	O
bk	O
i	O
xn	O
i	O
i	O
280	O
i	O
bk	O
xn	O
i	O
−	O
yn	O
k	O
(	O
15.2.4	O
)	O
draft	O
march	O
9	O
,	O
2010	O
−2024−2024−10123	O
we	O
now	O
substitute	O
this	O
solution	O
into	O
equation	B
(	O
15.2.3	O
)	O
to	O
write	O
the	O
squared	O
error	O
only	O
as	O
a	O
function	B
of	O
b.	O
bi	O
,	O
jbk	O
,	O
jxn	O
k	O
=	O
[	O
bbtxn	O
]	O
i	O
principal	O
components	O
analysis	B
using	O
(	O
cid:88	O
)	O
j	O
j	O
bj	O
yn	O
i	O
=	O
(	O
cid:88	O
)	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
e	O
(	O
b	O
)	O
=	O
(	O
cid:88	O
)	O
j	O
,	O
k	O
the	O
objective	O
e	O
(	O
b	O
)	O
becomes	O
j	O
,	O
k	O
kxn	O
bj	O
i	O
bj	O
k	O
=	O
(	O
cid:88	O
)	O
xn	O
(	O
cid:17	O
)	O
2	O
i	O
−	O
bbt	O
(	O
cid:17	O
)	O
i	O
−	O
bbt	O
(	O
cid:17	O
)	O
(	O
xn	O
)	O
t	O
(	O
cid:16	O
)	O
(	O
cid:104	O
)	O
n	O
e	O
(	O
b	O
)	O
=	O
(	O
cid:88	O
)	O
since	O
(	O
cid:0	O
)	O
i	O
−	O
bbt	O
(	O
cid:1	O
)	O
2	O
=	O
i	O
−	O
bbt	O
,	O
(	O
using	O
btb	O
=	O
i	O
)	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
sbbt	O
(	O
cid:17	O
)	O
(	O
cid:105	O
)	O
hence	O
the	O
objective	O
becomes	O
xn	O
=	O
trace	O
(	O
cid:16	O
)	O
n	O
n	O
e	O
(	O
b	O
)	O
=	O
(	O
n	O
−	O
1	O
)	O
where	O
s	O
is	O
the	O
sample	B
covariance	O
matrix	B
of	O
the	O
data1	O
trace	O
(	O
s	O
)	O
−	O
trace	O
n	O
(	O
cid:88	O
)	O
1	O
n	O
(	O
cid:88	O
)	O
n=1	O
m	O
=	O
1	O
n	O
xn	O
,	O
s	O
=	O
(	O
xn	O
−	O
m	O
)	O
(	O
xn	O
−	O
m	O
)	O
t	O
n=1	O
n	O
−	O
1	O
(	O
cid:16	O
)	O
l	O
(	O
cid:16	O
)	O
sbbt	O
(	O
cid:17	O
)	O
−trace	O
+	O
trace	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
btb	O
−	O
i	O
(	O
xn	O
)	O
(	O
xn	O
)	O
t	O
(	O
cid:16	O
)	O
i	O
−	O
bbt	O
(	O
cid:17	O
)	O
(	O
cid:33	O
)	O
to	O
minimise	O
equation	B
(	O
15.2.8	O
)	O
under	O
the	O
constraint	O
btb	O
=	O
i	O
we	O
use	O
a	O
set	O
of	O
lagrange	O
multipliers	O
l	O
,	O
so	O
that	O
the	O
objective	O
is	O
to	O
minimize	O
(	O
15.2.5	O
)	O
(	O
15.2.6	O
)	O
(	O
15.2.7	O
)	O
(	O
15.2.8	O
)	O
(	O
15.2.9	O
)	O
(	O
15.2.10	O
)	O
(	O
15.2.11	O
)	O
(	O
neglecting	O
the	O
constant	O
prefactor	O
n	O
−	O
1	O
)	O
.	O
since	O
the	O
constraint	O
is	O
symmetric	O
,	O
we	O
can	O
assume	O
that	O
l	O
is	O
also	O
symmetric	O
.	O
diﬀerentiating	O
with	O
respect	O
to	O
b	O
and	O
equating	O
to	O
zero	O
we	O
obtain	O
that	O
at	O
the	O
optimum	O
sb	O
=	O
bl	O
whose	O
columns	O
are	O
the	O
corresponding	O
eigenvectors	O
of	O
s.	O
in	O
this	O
case	O
,	O
trace	O
(	O
cid:0	O
)	O
sbbt	O
(	O
cid:1	O
)	O
=	O
trace	O
(	O
l	O
)	O
,	O
which	O
is	O
this	O
is	O
a	O
form	O
of	O
eigen-equation	O
so	O
that	O
a	O
solution	O
is	O
given	O
by	O
taking	O
l	O
to	O
be	O
diagonal	O
and	O
b	O
as	O
the	O
matrix	B
the	O
sum	O
of	O
the	O
eigenvalues	O
corresponding	O
to	O
the	O
eigenvectors	O
forming	O
b.	O
since	O
we	O
wish	O
to	O
minimise	O
e	O
(	O
b	O
)	O
,	O
we	O
take	O
the	O
eigenvectors	O
with	O
largest	O
corresponding	O
eigenvalues	O
.	O
if	O
we	O
order	O
the	O
eigenvalues	O
λ1	O
≥	O
λ2	O
,	O
.	O
.	O
.	O
,	O
the	O
squared	O
error	O
is	O
given	O
by	O
,	O
from	O
equation	B
(	O
15.2.8	O
)	O
1	O
n	O
−	O
1	O
e	O
(	O
b	O
)	O
=	O
trace	O
(	O
s	O
)	O
−	O
trace	O
(	O
l	O
)	O
=	O
m	O
(	O
cid:88	O
)	O
d	O
(	O
cid:88	O
)	O
i=1	O
λi	O
−	O
d	O
(	O
cid:88	O
)	O
i=1	O
i=m	O
+1	O
λi	O
=	O
λi	O
(	O
15.2.12	O
)	O
whilst	O
the	O
solution	O
to	O
this	O
eigen-problem	O
is	O
unique	O
,	O
this	O
only	O
serves	O
to	O
deﬁne	O
the	O
solution	O
subspace	O
since	O
one	O
may	O
rotate	O
and	O
scale	O
b	O
and	O
y	O
such	O
that	O
the	O
value	B
of	O
the	O
squared	O
loss	O
is	O
exactly	O
the	O
same	O
.	O
the	O
justiﬁcation	O
for	O
choosing	O
the	O
non-rotated	O
eigen	B
solution	O
is	O
given	O
by	O
the	O
additional	O
requirement	O
that	O
the	O
principal	O
components	O
corresponds	O
to	O
directions	O
of	O
maximal	O
variance	B
,	O
as	O
explained	O
in	O
section	O
(	O
15.2.2	O
)	O
.	O
1here	O
we	O
use	O
the	O
unbiased	O
sample	O
covariance	B
,	O
simply	O
because	O
this	O
is	O
standard	O
in	O
the	O
literature	O
.	O
if	O
we	O
were	O
to	O
replace	O
this	O
with	O
the	O
sample	B
covariance	O
as	O
deﬁned	O
in	O
chapter	O
(	O
8	O
)	O
,	O
the	O
only	O
change	O
required	O
is	O
to	O
replace	O
n	O
−	O
1	O
by	O
n	O
throughout	O
,	O
which	O
has	O
no	O
eﬀect	O
on	O
the	O
form	O
of	O
the	O
solutions	O
found	O
by	O
pca	O
.	O
draft	O
march	O
9	O
,	O
2010	O
281	O
yn	O
=	O
(	O
cid:88	O
)	O
i	O
(	O
cid:88	O
)	O
(	O
cid:16	O
)	O
btxn	O
(	O
cid:17	O
)	O
2	O
(	O
cid:34	O
)	O
(	O
cid:88	O
)	O
(	O
cid:35	O
)	O
principal	O
components	O
analysis	B
figure	O
15.2	O
:	O
projection	B
of	O
two	O
dimensional	O
data	B
using	O
one	O
dimensional	O
pca	O
.	O
plotted	O
are	O
the	O
original	O
dat-	O
apoints	O
x	O
(	O
larger	O
rings	O
)	O
and	O
their	O
reconstructions	O
˜x	O
(	O
small	O
dots	O
)	O
using	O
1	O
dimensional	O
pca	O
.	O
the	O
lines	O
rep-	O
resent	O
the	O
orthogonal	B
projection	O
of	O
the	O
original	O
dat-	O
apoint	O
onto	O
the	O
ﬁrst	O
eigenvector	O
.	O
the	O
arrows	O
are	O
the	O
two	O
eigenvectors	O
scaled	O
by	O
the	O
square	O
root	O
of	O
their	O
corresponding	O
eigenvalues	O
.	O
the	O
data	B
has	O
been	O
cen-	O
tred	O
to	O
have	O
zero	O
mean	B
.	O
for	O
each	O
‘	O
high	O
dimensional	O
’	O
datapoint	O
x	O
,	O
the	O
‘	O
low	B
dimensional	I
’	O
representation	B
y	O
is	O
given	O
in	O
this	O
case	O
by	O
the	O
distance	O
(	O
possibly	O
negative	O
)	O
from	O
the	O
origin	O
along	O
the	O
ﬁrst	O
eigenvector	O
direction	O
to	O
the	O
corresponding	O
orthogonal	B
projection	O
point	O
.	O
15.2.2	O
maximum	O
variance	O
criterion	O
we	O
search	O
ﬁrst	O
for	O
the	O
single	O
direction	O
b	O
such	O
that	O
,	O
when	O
the	O
data	B
is	O
projected	O
onto	O
this	O
direction	O
,	O
the	O
variance	B
of	O
this	O
projection	B
is	O
maximal	O
amongst	O
all	O
possible	O
such	O
projections	O
.	O
using	O
equation	B
(	O
15.2.4	O
)	O
for	O
a	O
single	O
vector	O
b	O
we	O
have	O
bixn	O
i	O
(	O
15.2.13	O
)	O
the	O
projection	B
of	O
a	O
datapoint	O
onto	O
a	O
direction	O
b	O
is	O
btxn	O
for	O
a	O
unit	O
length	O
vector	O
b.	O
hence	O
the	O
sum	O
of	O
squared	O
projections	O
is	O
=	O
bt	O
xn	O
(	O
xn	O
)	O
t	O
b	O
=	O
(	O
n	O
−	O
1	O
)	O
btsb	O
=	O
λ	O
(	O
n	O
−	O
1	O
)	O
(	O
15.2.14	O
)	O
n	O
n	O
which	O
,	O
ignoring	O
constants	O
,	O
is	O
simply	O
the	O
negative	O
of	O
equation	B
(	O
15.2.8	O
)	O
for	O
a	O
single	O
retained	O
eigenvector	O
b	O
(	O
with	O
sb	O
=	O
λb	O
)	O
.	O
hence	O
the	O
optimal	O
single	O
b	O
which	O
maximises	O
the	O
projection	B
variance	O
is	O
given	O
by	O
the	O
eigenvector	O
corresponding	O
to	O
the	O
largest	O
eigenvalue	O
of	O
s.	O
under	O
the	O
criterion	O
that	O
the	O
next	O
optimal	O
direction	O
should	O
be	O
orthonormal	B
to	O
the	O
ﬁrst	O
,	O
one	O
can	O
readily	O
show	O
that	O
b	O
(	O
2	O
)	O
is	O
given	O
by	O
the	O
second	O
largest	O
eigenvector	O
,	O
and	O
so	O
on	O
.	O
this	O
explains	O
why	O
,	O
despite	O
the	O
squared	O
loss	O
equation	B
(	O
15.2.8	O
)	O
being	O
invariant	O
with	O
respect	O
to	O
arbitrary	O
rotation	O
(	O
and	O
scaling	O
)	O
of	O
the	O
basis	O
vectors	O
,	O
the	O
ones	O
given	O
by	O
the	O
eigen-decomposition	O
have	O
the	O
additional	O
property	O
that	O
they	O
correspond	O
to	O
directions	O
of	O
maximal	O
variance	B
.	O
these	O
maximal	O
variance	B
directions	O
found	O
by	O
pca	O
are	O
called	O
the	O
principal	B
directions	I
.	O
15.2.3	O
pca	O
algorithm	B
the	O
routine	O
for	O
pca	O
is	O
presented	O
in	O
algorithm	B
(	O
14	O
)	O
.	O
in	O
the	O
notation	O
of	O
y	O
=	O
fx	O
,	O
the	O
projection	B
matrix	O
f	O
corresponds	O
to	O
et	O
.	O
similarly	O
for	O
the	O
reconstruction	O
equation	B
(	O
15.2.1	O
)	O
,	O
the	O
coordinate	O
yn	O
corresponds	O
to	O
etxn	O
and	O
bi	O
corresponds	O
to	O
ei	O
.	O
the	O
pca	O
reconstructions	O
are	O
orthogonal	B
projections	O
of	O
the	O
data	B
onto	O
the	O
subspace	O
spanned	O
by	O
the	O
eigenvectors	O
corresponding	O
to	O
the	O
m	O
largest	O
eigenvalues	O
of	O
the	O
covariance	B
matrix	O
,	O
see	O
ﬁg	O
(	O
15.2	O
)	O
.	O
figure	O
15.3	O
:	O
top	O
row	O
:	O
a	O
selection	O
of	O
the	O
digit	O
5	O
taken	O
from	O
the	O
database	O
of	O
892	O
ex-	O
amples	O
.	O
plotted	O
beneath	O
each	O
digit	O
is	O
the	O
reconstruction	O
using	O
100	O
,	O
30	O
and	O
5	O
eigen-	O
vectors	O
(	O
from	O
top	O
to	O
bottom	O
)	O
.	O
note	O
how	O
the	O
reconstructions	O
for	O
fewer	O
eigenvectors	O
express	O
less	O
variability	O
from	O
each	O
other	O
,	O
and	O
resemble	O
more	O
a	O
mean	B
5	O
digit	O
.	O
draft	O
march	O
9	O
,	O
2010	O
282	O
−2−10123−2−1.5−1−0.500.511.52	O
principal	O
components	O
analysis	B
algorithm	O
14	O
principal	O
components	O
analysis	B
to	O
form	O
an	O
m-dimensional	O
approximation	B
of	O
a	O
dataset	O
{	O
xn	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
,	O
with	O
dim	O
xn	O
=	O
d.	O
1	O
:	O
find	O
the	O
d	O
×	O
1	O
sample	O
mean	B
vector	O
and	O
d	O
×	O
d	O
covariance	B
matrix	O
n	O
(	O
cid:88	O
)	O
n=1	O
n	O
(	O
cid:88	O
)	O
n=1	O
m	O
=	O
1	O
n	O
xn	O
,	O
s	O
=	O
1	O
n	O
−	O
1	O
(	O
xn	O
−	O
m	O
)	O
(	O
xn	O
−	O
m	O
)	O
t	O
2	O
:	O
find	O
the	O
eigenvectors	O
e1	O
,	O
.	O
.	O
.	O
,	O
em	O
of	O
the	O
covariance	B
matrix	O
s	O
,	O
sorted	O
so	O
that	O
the	O
eigenvalue	O
of	O
ei	O
is	O
larger	O
than	O
ej	O
for	O
i	O
<	O
j.	O
form	O
the	O
matrix	B
e	O
=	O
[	O
e1	O
,	O
.	O
.	O
.	O
,	O
em	O
]	O
.	O
3	O
:	O
the	O
lower	O
dimensional	O
representation	O
of	O
each	O
data	B
point	O
xn	O
is	O
given	O
by	O
yn	O
=	O
et	O
(	O
xn	O
−	O
m	O
)	O
4	O
:	O
the	O
approximate	B
reconstruction	O
of	O
the	O
original	O
datapoint	O
xn	O
is	O
5	O
:	O
the	O
total	O
squared	O
error	O
over	O
all	O
the	O
training	B
data	O
made	O
by	O
the	O
approximation	B
is	O
xn	O
≈	O
m	O
+	O
eyn	O
n	O
(	O
cid:88	O
)	O
(	O
xn	O
−	O
˜xn	O
)	O
2	O
=	O
(	O
n	O
−	O
1	O
)	O
n=1	O
d	O
(	O
cid:88	O
)	O
λj	O
j=m	O
+1	O
(	O
15.2.15	O
)	O
(	O
15.2.16	O
)	O
(	O
15.2.17	O
)	O
where	O
λm	O
+1	O
.	O
.	O
.	O
λn	O
are	O
the	O
eigenvalues	O
discarded	O
in	O
the	O
projection	B
.	O
example	O
68	O
(	O
reducing	O
the	O
dimension	O
of	O
digits	O
)	O
.	O
we	O
have	O
892	O
examples	O
of	O
handwritten	O
5	O
’	O
s	O
,	O
where	O
each	O
image	O
consists	O
of	O
28	O
×	O
28	O
real-values	O
pixels	O
,	O
see	O
ﬁg	O
(	O
15.3	O
)	O
.	O
each	O
image	O
matrix	O
is	O
stacked	O
to	O
form	O
a	O
784	O
dimensional	O
vector	O
,	O
giving	O
a	O
784	O
×	O
892	O
dimensional	O
data	B
matrix	O
x.	O
the	O
covariance	B
matrix	O
of	O
this	O
data	B
has	O
eigenvalue	O
spectrum	B
as	O
plotted	O
in	O
ﬁg	O
(	O
15.4	O
)	O
,	O
where	O
we	O
plot	O
only	O
the	O
100	O
largest	O
eigenvalues	O
.	O
note	O
how	O
after	O
around	O
40	O
components	O
,	O
the	O
mean	B
squared	O
reconstruction	O
error	O
is	O
small	O
,	O
indicating	O
that	O
the	O
data	B
indeed	O
lie	O
close	O
to	O
a	O
40	O
dimensional	O
hyperplane	B
.	O
the	O
eigenvalues	O
are	O
computed	O
using	O
pca.m	O
.	O
the	O
reconstructions	O
using	O
diﬀerent	O
numbers	O
of	O
eigenvectors	O
(	O
100	O
,	O
30	O
and	O
5	O
)	O
are	O
plotted	O
in	O
ﬁg	O
(	O
15.3	O
)	O
.	O
note	O
how	O
using	O
only	O
a	O
small	O
number	O
of	O
eigenvectors	O
,	O
the	O
reconstruction	O
more	O
closely	O
resembles	O
the	O
mean	B
image	O
.	O
example	O
69	O
(	O
eigenfaces	O
)	O
.	O
in	O
ﬁg	O
(	O
15.5	O
)	O
we	O
present	O
example	O
images	O
for	O
which	O
we	O
wish	O
to	O
ﬁnd	O
a	O
lower	O
dimensional	O
representation	O
.	O
using	O
pca	O
the	O
ﬁrst	O
49	O
‘	O
eigenfaces	O
’	O
are	O
presented	O
along	O
with	O
reconstructions	O
of	O
the	O
original	O
data	B
using	O
these	O
eigenfaces	O
,	O
see	O
ﬁg	O
(	O
15.6	O
)	O
.	O
figure	O
15.4	O
:	O
for	O
the	O
digits	O
data	B
consisting	O
of	O
892	O
examples	O
of	O
the	O
digit	O
5	O
,	O
each	O
image	O
being	O
represented	O
by	O
a	O
784	O
dimensional	O
vector	O
.	O
plotted	O
as	O
the	O
largest	O
100	O
eigenvalues	O
(	O
scaled	O
so	O
that	O
the	O
largest	O
eigenvalue	O
is	O
1	O
)	O
of	O
the	O
sample	B
covariance	O
matrix	B
.	O
draft	O
march	O
9	O
,	O
2010	O
283	O
02040608010000.20.40.60.81eigenvalue	O
numbereigenvalue	O
principal	O
components	O
analysis	B
figure	O
15.5	O
:	O
100	O
of	O
the	O
120	O
training	B
images	O
(	O
40	O
people	O
,	O
with	O
3	O
images	O
of	O
each	O
person	O
)	O
.	O
each	O
image	O
consists	O
of	O
92×112	O
=	O
10304	O
greyscale	O
pixels	O
.	O
the	O
training	B
data	O
is	O
scaled	O
so	O
that	O
,	O
represented	O
as	O
an	O
image	O
,	O
the	O
components	O
of	O
each	O
image	O
sum	O
to	O
1.	O
the	O
average	B
value	O
of	O
each	O
pixel	O
across	O
all	O
images	O
is	O
9.70×10−5	O
.	O
this	O
is	O
a	O
subset	O
of	O
the	O
400	O
images	O
in	O
the	O
full	O
olivetti	O
research	O
face	O
database	O
.	O
figure	O
15.6	O
:	O
(	O
a	O
)	O
:	O
svd	O
reconstruc-	O
tion	O
of	O
the	O
images	O
in	O
ﬁg	O
(	O
15.5	O
)	O
using	O
a	O
combination	O
of	O
the	O
49	O
eigen-images	O
.	O
(	O
b	O
)	O
:	O
the	O
eigen-images	O
are	O
found	O
us-	O
ing	O
svd	O
of	O
the	O
ﬁg	O
(	O
15.5	O
)	O
and	O
tak-	O
ing	O
the	O
49	O
eigenvectors	O
with	O
largest	O
eigenvalue	O
.	O
the	O
images	O
correspond-	O
ing	O
to	O
the	O
largest	O
eigenvalues	O
are	O
con-	O
tained	O
in	O
the	O
ﬁrst	O
row	O
,	O
and	O
the	O
next	O
7	O
in	O
the	O
row	O
below	O
,	O
etc	O
.	O
the	O
root	O
mean	B
square	O
reconstruction	O
error	O
is	O
1.121	O
×	O
10−5	O
,	O
a	O
small	O
improvement	O
over	O
plsa	O
(	O
see	O
ﬁg	O
(	O
15.14	O
)	O
)	O
.	O
(	O
a	O
)	O
(	O
b	O
)	O
15.2.4	O
pca	O
and	O
nearest	O
neighbours	O
for	O
high-dimensional	O
data	B
computing	O
the	O
squared	O
euclidean	O
distance	O
between	O
vectors	O
can	O
be	O
expensive	O
,	O
and	O
also	O
sensitive	O
to	O
noise	O
.	O
it	O
is	O
therefore	O
often	O
useful	O
to	O
project	O
the	O
data	B
to	O
form	O
a	O
lower	O
dimensional	O
representation	O
ﬁrst	O
.	O
for	O
example	O
,	O
in	O
making	O
a	O
classiﬁer	B
to	O
distinguish	O
between	O
the	O
digit	O
1	O
and	O
the	O
digit	O
7	O
,	O
example	O
(	O
67	O
)	O
,	O
we	O
can	O
form	O
a	O
lower	O
dimensional	O
representation	O
ﬁrst	O
by	O
ignoring	O
the	O
class	O
label	O
(	O
to	O
make	O
a	O
dataset	O
of	O
1200	O
training	B
points	O
)	O
.	O
each	O
of	O
the	O
training	B
points	O
xn	O
is	O
then	O
projected	O
to	O
a	O
lower	O
dimensional	O
pca	O
representation	B
yn	O
.	O
subsequently	O
,	O
any	O
distance	O
calculations	O
(	O
xa	O
−	O
xb	O
)	O
2	O
are	O
replaced	O
by	O
(	O
ya	O
−	O
yb	O
)	O
2.	O
to	O
justify	O
this	O
,	O
consider	O
(	O
xa	O
−	O
xb	O
)	O
t	O
(	O
xa	O
−	O
xb	O
)	O
≈	O
(	O
eya	O
+	O
m	O
−	O
eyb	O
−	O
m	O
)	O
t	O
(	O
eya	O
+	O
m	O
−	O
eyb	O
−	O
m	O
)	O
=	O
(	O
ya	O
−	O
yb	O
)	O
tete	O
(	O
ya	O
−	O
yb	O
)	O
=	O
(	O
ya	O
−	O
yb	O
)	O
t	O
(	O
ya	O
−	O
yb	O
)	O
where	O
the	O
last	O
equality	O
is	O
due	O
to	O
the	O
orthonormality	O
of	O
eigenvectors	O
:	O
ete	O
=	O
i	O
.	O
(	O
15.2.18	O
)	O
using	O
19	O
principal	O
components	O
(	O
see	O
example	O
(	O
70	O
)	O
why	O
this	O
number	O
was	O
chosen	O
)	O
and	O
the	O
nearest	B
neighbour	I
rule	O
to	O
classify	O
ones	O
and	O
sevens	O
gave	O
a	O
test-set	O
error	O
of	O
14	O
in	O
600	O
examples	O
,	O
compared	O
to	O
18	O
from	O
the	O
standard	O
method	O
on	O
the	O
non-projected	O
data	B
.	O
how	O
can	O
it	O
be	O
that	O
the	O
classiﬁcation	B
performance	O
has	O
improved	O
?	O
a	O
plausible	O
explanation	O
is	O
that	O
the	O
new	O
pca	O
representation	B
of	O
the	O
data	B
is	O
more	O
robust	O
since	O
only	O
the	O
large	O
scale	O
change	O
directions	O
in	O
the	O
space	O
are	O
retained	O
,	O
with	O
low	O
variance	B
directions	O
discarded	O
.	O
284	O
draft	O
march	O
9	O
,	O
2010	O
high	B
dimensional	I
data	I
figure	O
15.7	O
:	O
finding	O
the	O
optimal	O
pca	O
dimension	O
to	O
use	O
for	O
classifying	O
hand-written	O
digits	O
using	O
nearest	O
neighbours	O
.	O
400	O
training	B
examples	O
are	O
used	O
,	O
and	O
the	O
validation	B
error	O
plotted	O
on	O
200	O
further	O
examples	O
.	O
based	O
on	O
the	O
validation	B
error	O
,	O
we	O
see	O
that	O
a	O
dimension	O
of	O
19	O
is	O
reasonable	O
.	O
example	O
70	O
(	O
finding	O
the	O
best	O
pca	O
dimension	O
)	O
.	O
there	O
are	O
600	O
examples	O
of	O
the	O
digit	O
1	O
and	O
600	O
examples	O
of	O
the	O
digit	O
7.	O
we	O
will	O
use	O
half	O
the	O
data	B
for	O
training	B
and	O
the	O
other	O
half	O
for	O
testing	O
.	O
the	O
600	O
training	B
examples	O
were	O
further	O
split	O
into	O
a	O
training	B
set	O
of	O
400	O
examples	O
,	O
and	O
a	O
separate	O
validation	B
set	O
of	O
200	O
examples	O
.	O
pca	O
was	O
used	O
to	O
reduce	O
the	O
dimensionality	O
of	O
the	O
inputs	O
,	O
and	O
then	O
nearest	O
neighbours	O
used	O
to	O
classify	O
the	O
200	O
validation	B
examples	O
.	O
diﬀerent	O
reduced	O
dimensions	O
were	O
in	O
vestigated	O
and	O
,	O
based	O
on	O
the	O
validation	B
results	O
,	O
19	O
was	O
selected	O
as	O
the	O
optimal	O
number	O
of	O
pca	O
components	O
retained	O
,	O
see	O
ﬁg	O
(	O
15.7	O
)	O
.	O
the	O
independent	O
test	O
error	O
on	O
600	O
independent	O
examples	O
using	O
19	O
dimensions	O
is	O
14	O
.	O
15.2.5	O
comments	O
on	O
pca	O
the	O
‘	O
intrinsic	O
’	O
dimension	O
of	O
data	B
how	O
many	O
dimensions	O
should	O
the	O
linear	B
subspace	O
have	O
?	O
from	O
equation	B
(	O
15.2.12	O
)	O
,	O
the	O
reconstruction	O
error	O
is	O
proportional	O
to	O
the	O
sum	O
of	O
the	O
discarded	O
eigenvalues	O
.	O
if	O
we	O
plot	O
the	O
eigenvalue	O
spectrum	B
(	O
the	O
set	O
of	O
eigenvalues	O
ordered	O
by	O
decreasing	O
value	B
)	O
,	O
we	O
might	O
hope	O
to	O
see	O
a	O
few	O
large	O
values	O
and	O
many	O
small	O
values	O
.	O
if	O
the	O
data	B
does	O
lie	O
close	O
to	O
an	O
m	O
dimensional	O
hyperplane	B
,	O
we	O
would	O
see	O
m	O
large	O
eigenvalues	O
with	O
the	O
rest	O
being	O
very	O
small	O
.	O
this	O
would	O
give	O
an	O
indication	O
of	O
the	O
number	O
of	O
degrees	O
of	O
freedom	O
in	O
the	O
data	B
,	O
or	O
the	O
intrinsic	O
dimensionality	O
.	O
directions	O
corresponding	O
to	O
the	O
small	O
eigenvalues	O
are	O
then	O
interpreted	O
as	O
‘	O
noise	O
’	O
.	O
non-linear	B
dimension	O
reduction	O
in	O
pca	O
we	O
are	O
presupposing	O
that	O
the	O
data	B
lies	O
close	O
to	O
a	O
hyperplane	B
.	O
is	O
this	O
really	O
a	O
good	O
description	O
?	O
more	O
generally	O
,	O
we	O
would	O
expect	O
data	B
to	O
lie	O
on	O
low	B
dimensional	I
curved	O
manifolds	O
.	O
also	O
,	O
data	B
is	O
often	O
clustered	O
–	O
examples	O
of	O
handwritten	O
‘	O
4	O
’	O
s	O
look	O
similar	O
to	O
each	O
other	O
and	O
form	O
a	O
cluster	O
,	O
separate	O
from	O
the	O
‘	O
8	O
’	O
s	O
cluster	O
.	O
nevertheless	O
,	O
since	O
linear	B
dimension	I
reduction	I
is	O
computationally	O
relatively	O
straightforward	O
,	O
this	O
is	O
one	O
of	O
the	O
most	O
common	O
dimensionality	B
reduction	I
techniques	O
.	O
15.3	O
high	B
dimensional	I
data	I
the	O
computational	B
complexity	I
of	O
computing	O
an	O
eigen-decomposition	O
of	O
a	O
d	O
×	O
d	O
matrix	B
is	O
o	O
(	O
cid:0	O
)	O
d3	O
(	O
cid:1	O
)	O
.	O
you	O
exceed	O
500.	O
one	O
can	O
exploit	O
this	O
fact	O
to	O
bound	B
the	O
complexity	O
by	O
o	O
(	O
cid:0	O
)	O
min	O
(	O
d	O
,	O
n	O
)	O
3	O
(	O
cid:1	O
)	O
,	O
as	O
described	O
below	O
.	O
might	O
be	O
wondering	O
therefore	O
how	O
it	O
is	O
possible	O
to	O
perform	O
pca	O
on	O
high	B
dimensional	I
data	I
.	O
for	O
example	O
,	O
if	O
we	O
have	O
500	O
images	O
each	O
of	O
1000×	O
1000	O
=	O
106	O
pixels	O
,	O
the	O
covariance	B
matrix	O
will	O
be	O
106	O
×	O
106	O
dimensional	O
.	O
it	O
would	O
appear	O
a	O
signiﬁcant	O
computational	O
challenge	O
to	O
compute	O
the	O
eigen-decomposition	O
of	O
this	O
matrix	B
.	O
in	O
this	O
case	O
,	O
however	O
,	O
since	O
there	O
are	O
only	O
500	O
such	O
vectors	O
,	O
the	O
number	O
of	O
non-zero	O
eigenvalues	O
can	O
not	O
draft	O
march	O
9	O
,	O
2010	O
285	O
020406080100024681012number	O
of	O
eigenvaluesnumber	O
of	O
errors	O
15.3.1	O
eigen-decomposition	O
for	O
n	O
<	O
d	O
high	B
dimensional	I
data	I
first	O
note	O
that	O
for	O
zero	O
mean	B
data	O
,	O
the	O
sample	B
covariance	O
matrix	B
can	O
be	O
expressed	O
as	O
n	O
(	O
cid:88	O
)	O
1	O
[	O
s	O
]	O
ij	O
=	O
xn	O
i	O
xn	O
j	O
n	O
−	O
1	O
n=1	O
(	O
15.3.1	O
)	O
(	O
15.3.2	O
)	O
(	O
15.3.3	O
)	O
in	O
matrix	B
notation	O
this	O
can	O
be	O
written	O
s	O
=	O
1	O
n	O
−	O
1	O
xxt	O
where	O
the	O
d	O
×	O
n	O
matrix	B
x	O
contains	O
all	O
the	O
data	B
vectors	O
:	O
x	O
=	O
(	O
cid:2	O
)	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
(	O
cid:3	O
)	O
since	O
the	O
eigenvectors	O
of	O
a	O
matrix	B
m	O
are	O
equal	O
to	O
those	O
of	O
γm	O
for	O
scalar	O
γ	O
,	O
one	O
can	O
consider	O
more	O
simply	O
the	O
eigenvectors	O
of	O
xxt	O
.	O
writing	O
the	O
d×	O
n	O
matrix	B
of	O
eigenvectors	O
as	O
e	O
(	O
this	O
is	O
a	O
non-square	O
thin	B
matrix	O
since	O
there	O
will	O
be	O
fewer	O
eigenvalues	O
than	O
data	B
dimensions	O
)	O
and	O
the	O
eigenvalues	O
as	O
an	O
n	O
×	O
n	O
diagonal	O
matrix	B
λ	O
,	O
the	O
eigen-decomposition	O
of	O
the	O
covariance	B
s	O
is	O
xxte	O
=	O
eλ	O
⇒	O
xtxxte	O
=	O
xteλ	O
⇒	O
xtx	O
˜e	O
=	O
˜eλ	O
where	O
we	O
deﬁned	O
˜e	O
=	O
xte	O
.	O
the	O
ﬁnal	O
expression	O
above	O
represents	O
the	O
eigenvector	O
equation	B
for	O
xtx	O
.	O
this	O
is	O
a	O
matrix	B
of	O
dimensions	O
n	O
×n	O
so	O
that	O
calculating	O
the	O
eigen-decomposition	O
takes	O
o	O
(	O
cid:0	O
)	O
n	O
3	O
(	O
cid:1	O
)	O
operations	O
,	O
compared	O
with	O
o	O
(	O
cid:0	O
)	O
d3	O
(	O
cid:1	O
)	O
operations	O
in	O
the	O
original	O
high-dimensional	O
space	O
.	O
we	O
then	O
can	O
therefore	O
calculate	O
the	O
eigenvectors	O
˜e	O
and	O
eigenvalues	O
λ	O
of	O
this	O
matrix	B
more	O
easily	O
.	O
once	O
found	O
,	O
we	O
use	O
the	O
fact	O
that	O
the	O
eigenvalues	O
of	O
s	O
are	O
given	O
by	O
the	O
diagonal	O
entries	O
of	O
λ	O
and	O
the	O
eigenvectors	O
by	O
(	O
15.3.4	O
)	O
e	O
=	O
x	O
˜eλ−1	O
(	O
15.3.5	O
)	O
15.3.2	O
pca	O
via	O
singular	B
value	O
decomposition	B
an	O
alternative	O
to	O
using	O
an	O
eigen-decomposition	O
routine	O
to	O
ﬁnd	O
the	O
pca	O
solution	O
is	O
to	O
make	O
use	O
of	O
the	O
singular	B
value	O
decomposition	B
(	O
svd	O
)	O
of	O
an	O
d	O
×	O
n	O
dimensional	O
matrix	B
x.	O
this	O
is	O
given	O
by	O
x	O
=	O
udvt	O
(	O
15.3.6	O
)	O
where	O
utu	O
=	O
id	O
and	O
vtv	O
=	O
in	O
and	O
d	O
is	O
a	O
diagonal	O
matrix	B
of	O
the	O
(	O
positive	O
)	O
singular	B
values	O
.	O
we	O
assume	O
that	O
the	O
decomposition	B
has	O
ordered	O
the	O
singular	B
values	O
so	O
that	O
the	O
upper	O
left	O
diagonal	O
element	O
of	O
d	O
contains	O
the	O
largest	O
singular	B
value	O
.	O
the	O
matrix	B
xxt	O
can	O
then	O
be	O
written	O
as	O
xxt	O
=	O
udvtvdut	O
=	O
ud2ut	O
(	O
15.3.7	O
)	O
since	O
ud2ut	O
is	O
in	O
the	O
form	O
of	O
an	O
eigen-decomposition	O
,	O
the	O
pca	O
solution	O
is	O
equivalently	O
given	O
by	O
per-	O
forming	O
the	O
svd	O
decomposition	B
of	O
x	O
,	O
for	O
which	O
the	O
eigenvectors	O
are	O
then	O
given	O
by	O
u	O
,	O
and	O
corresponding	O
eigenvalues	O
by	O
the	O
square	O
of	O
the	O
singular	B
values	O
.	O
equation	B
(	O
15.3.6	O
)	O
shows	O
that	O
pca	O
is	O
a	O
form	O
of	O
matrix	B
decomposition	O
method	O
:	O
x	O
=	O
udvt	O
≈	O
um	O
dm	O
vt	O
m	O
(	O
15.3.8	O
)	O
where	O
um	O
,	O
dm	O
,	O
vm	O
correspond	O
to	O
taking	O
only	O
the	O
ﬁrst	O
m	O
singular	B
values	O
of	O
the	O
full	O
matrices	O
.	O
286	O
draft	O
march	O
9	O
,	O
2010	O
latent	B
semantic	I
analysis	I
figure	O
15.8	O
:	O
document	O
data	B
for	O
a	O
dictionary	O
contain-	O
ing	O
10	O
words	O
and	O
1200	O
documents	O
.	O
black	O
indicates	O
that	O
a	O
word	O
was	O
present	O
in	O
a	O
document	O
.	O
the	O
data	B
consists	O
of	O
two	O
‘	O
similar	O
’	O
topics	O
(	O
diﬀering	O
only	O
in	O
their	O
usage	O
of	O
the	O
ﬁrst	O
two	O
words	O
)	O
and	O
a	O
random	O
back-	O
ground	O
topic	O
.	O
15.4	O
latent	B
semantic	I
analysis	I
in	O
the	O
document	O
analysis	B
literature	O
pca	O
is	O
also	O
called	O
latent	B
semantic	I
analysis	I
and	O
is	O
concerned	O
with	O
analysing	O
a	O
set	O
of	O
n	O
documents	O
,	O
where	O
each	O
document	O
dn	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
is	O
represented	O
by	O
a	O
vector	O
xn	O
=	O
(	O
xn	O
1	O
,	O
.	O
.	O
.	O
,	O
xn	O
d	O
)	O
(	O
15.4.1	O
)	O
1	O
might	O
count	O
how	O
many	O
times	O
the	O
word	O
‘	O
cat	O
’	O
appears	O
of	O
word	O
occurrences	O
.	O
for	O
example	O
the	O
ﬁrst	O
element	O
xn	O
2	O
the	O
number	O
of	O
occurrences	O
of	O
‘	O
dog	O
’	O
,	O
etc	O
.	O
this	O
bag	B
of	I
words	I
2	O
is	O
formed	O
by	O
ﬁrst	O
choosing	O
in	O
document	O
n	O
,	O
xn	O
is	O
the	O
(	O
possibly	O
normalised	O
)	O
number	O
of	O
occurrences	O
of	O
the	O
a	O
dictionary	O
of	O
d	O
words	O
.	O
the	O
vector	O
element	O
xn	O
i	O
word	O
i	O
in	O
the	O
document	O
n.	O
typically	O
d	O
will	O
be	O
large	O
,	O
of	O
the	O
order	O
106	O
,	O
and	O
x	O
will	O
be	O
very	O
sparse	B
since	O
any	O
document	O
contains	O
only	O
a	O
small	O
fraction	O
of	O
the	O
available	O
words	O
in	O
the	O
dictionary	O
.	O
given	O
a	O
set	O
of	O
documents	O
d	O
,	O
the	O
aim	O
in	O
lsa	O
is	O
to	O
form	O
a	O
lower	O
dimensional	O
representation	O
of	O
each	O
document	O
.	O
the	O
whole	O
document	O
database	O
is	O
represented	O
by	O
the	O
so-called	O
term-document	B
matrix	I
x	O
=	O
(	O
cid:2	O
)	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
(	O
cid:3	O
)	O
(	O
15.4.2	O
)	O
which	O
has	O
dimension	O
d	O
×	O
n	O
,	O
see	O
for	O
example	O
ﬁg	O
(	O
15.8	O
)	O
.	O
in	O
this	O
small	O
example	O
the	O
term-document	B
matrix	I
is	O
‘	O
short	O
and	O
fat	O
’	O
,	O
whereas	O
in	O
practice	O
most	O
often	O
the	O
matrix	B
will	O
be	O
‘	O
tall	O
and	O
thin	B
’	O
.	O
example	O
71	O
(	O
latent	B
topic	I
)	O
.	O
we	O
have	O
a	O
small	O
dictionary	O
containing	O
the	O
words	O
inﬂuenza	O
,	O
ﬂu	O
,	O
headache	O
,	O
nose	O
,	O
temperature	O
,	O
bed	O
,	O
cat	O
,	O
tree	B
,	O
car	O
,	O
foot	O
.	O
the	O
database	O
contains	O
a	O
large	O
number	O
of	O
articles	O
that	O
discuss	O
ailments	O
,	O
and	O
articles	O
which	O
seem	O
to	O
talk	O
about	O
the	O
eﬀects	O
of	O
inﬂuenza	O
,	O
in	O
addition	O
to	O
some	O
background	O
documents	O
that	O
are	O
not	O
speciﬁc	O
to	O
ailments	O
.	O
some	O
of	O
the	O
more	O
formal	O
documents	O
exclusively	O
use	O
the	O
term	O
inﬂuenza	O
,	O
whereas	O
the	O
other	O
more	O
‘	O
tabloid	O
’	O
documents	O
use	O
the	O
informal	O
term	O
ﬂu	O
.	O
each	O
document	O
is	O
represented	O
by	O
a	O
simple	O
bag-of-words	O
style	O
description	O
,	O
namely	O
a	O
10-dimensional	O
vector	O
in	O
which	O
element	O
i	O
of	O
that	O
vector	O
is	O
set	O
to	O
1	O
if	O
word	O
i	O
occurs	O
in	O
the	O
document	O
,	O
and	O
0	O
otherwise	O
.	O
the	O
data	B
is	O
represented	O
in	O
ﬁg	O
(	O
15.8	O
)	O
.	O
the	O
data	B
is	O
generated	O
using	O
the	O
artiﬁcial	O
mechanism	O
described	O
in	O
demolsi.m	O
.	O
the	O
result	O
of	O
using	O
pca	O
on	O
this	O
data	B
is	O
represented	O
in	O
ﬁg	O
(	O
15.9	O
)	O
where	O
we	O
plot	O
the	O
eigenvectors	O
,	O
scaled	O
by	O
their	O
eigenvalue	O
.	O
the	O
ﬁrst	O
eigenvector	O
groups	O
all	O
the	O
‘	O
inﬂuenza	O
’	O
words	O
together	O
,	O
and	O
the	O
second	O
deals	O
with	O
the	O
diﬀerent	O
usage	O
of	O
the	O
terms	O
inﬂuenza	O
and	O
ﬂu	O
.	O
rescaling	O
in	O
lsa	O
it	O
is	O
common	O
to	O
scale	O
the	O
transformation	O
so	O
that	O
the	O
projected	O
vectors	O
have	O
approximately	O
unit	O
covariance	O
(	O
assuming	O
centred	O
data	B
)	O
.	O
using	O
the	O
covariance	B
of	O
the	O
projections	O
is	O
obtained	O
from	O
y	O
=	O
√n	O
−	O
1d−1	O
m	O
ut	O
m	O
x	O
(	O
cid:88	O
)	O
1	O
n	O
−	O
1	O
n	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
n	O
yn	O
(	O
yn	O
)	O
t	O
=	O
d−1	O
m	O
ut	O
m	O
um	O
d−1	O
m	O
=	O
d−1	O
m	O
ut	O
m	O
ud2utum	O
d−1	O
m	O
≈	O
i	O
xn	O
(	O
xn	O
)	O
t	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
xxt	O
(	O
cid:125	O
)	O
(	O
15.4.3	O
)	O
2more	O
generally	O
one	O
can	O
consider	O
term-counts	O
,	O
in	O
which	O
terms	O
can	O
can	O
be	O
single	O
words	O
,	O
or	O
sets	O
of	O
words	O
,	O
or	O
even	O
sub-words	O
.	O
draft	O
march	O
9	O
,	O
2010	O
287	O
20040060080010001200246810	O
latent	B
semantic	I
analysis	I
figure	O
15.9	O
:	O
hinton	O
diagram	O
of	O
the	O
eigenvector	O
matrix	B
e	O
where	O
each	O
eigen-	O
vector	O
column	O
is	O
scaled	O
by	O
the	O
corresponding	O
eigenvalue	O
.	O
red	O
indicates	O
positive	O
and	O
green	O
negative	O
(	O
the	O
area	O
of	O
each	O
square	O
corresponds	O
to	O
the	O
magnitude	O
)	O
,	O
showing	O
that	O
there	O
are	O
only	O
a	O
few	O
large	O
eigenvalues	O
.	O
note	O
that	O
the	O
overall	O
sign	O
of	O
any	O
eigenvector	O
is	O
irrelevant	O
.	O
the	O
ﬁrst	O
eigenvector	O
corresponds	O
to	O
a	O
topic	O
in	O
which	O
the	O
words	O
inﬂuenza	O
,	O
ﬂu	O
,	O
headache	O
,	O
nose	O
,	O
temperature	O
,	O
bed	O
are	O
prevalent	O
.	O
the	O
second	O
eigenvector	O
denotes	O
that	O
there	O
is	O
negative	O
correlation	B
between	O
the	O
occurrence	O
of	O
inﬂuenza	O
and	O
ﬂu	O
.	O
given	O
y	O
,	O
the	O
approximate	B
reconstruction	O
˜x	O
is	O
the	O
euclidean	O
distance	O
between	O
two	O
points	O
xa	O
and	O
xb	O
is	O
then	O
approximately	O
(	O
15.4.4	O
)	O
(	O
cid:16	O
)	O
ya	O
−	O
yb	O
(	O
cid:17	O
)	O
d2	O
m	O
(	O
cid:16	O
)	O
ya	O
−	O
yb	O
(	O
cid:17	O
)	O
t	O
(	O
cid:16	O
)	O
ya	O
−	O
yb	O
(	O
cid:17	O
)	O
≈	O
1	O
n	O
−	O
1	O
˜x	O
=	O
um	O
dm	O
y	O
1	O
√n	O
−	O
1	O
˜xa	O
,	O
˜xb	O
(	O
cid:17	O
)	O
=	O
(	O
cid:16	O
)	O
d	O
(	O
cid:16	O
)	O
ya	O
−	O
yb	O
(	O
cid:17	O
)	O
t	O
1	O
n	O
−	O
1	O
dm	O
ut	O
m	O
um	O
dm	O
it	O
is	O
common	O
to	O
ignore	O
the	O
d2	O
the	O
projected	O
space	O
just	O
to	O
be	O
the	O
euclidean	O
distance	O
between	O
the	O
y	O
vectors	O
.	O
m	O
term	O
(	O
and	O
1/	O
(	O
n	O
−	O
1	O
)	O
factor	B
)	O
,	O
and	O
consider	O
a	O
measure	O
of	O
dissimilarity	O
in	O
15.4.1	O
lsa	O
for	O
information	B
retrieval	I
consider	O
a	O
large	O
collection	O
of	O
documents	O
from	O
the	O
web	B
,	O
creating	O
a	O
database	O
d.	O
our	O
interest	O
it	O
to	O
ﬁnd	O
the	O
most	O
similar	O
document	O
to	O
a	O
speciﬁed	O
query	B
document	O
.	O
using	O
a	O
bag-of-words	O
style	O
representation	B
for	O
document	O
n	O
,	O
xn	O
,	O
and	O
similarly	O
for	O
the	O
query	B
document	O
,	O
x∗	O
we	O
address	O
this	O
task	O
by	O
ﬁrst	O
deﬁning	O
a	O
measure	O
of	O
dissimilarity	O
between	O
documents	O
,	O
for	O
example	O
d	O
(	O
xn	O
,	O
xm	O
)	O
=	O
(	O
xn	O
−	O
xm	O
)	O
t	O
(	O
xn	O
−	O
xm	O
)	O
one	O
then	O
searches	O
for	O
the	O
document	O
that	O
minimises	O
this	O
dissimilarity	O
:	O
nopt	O
=	O
argmin	O
n	O
d	O
(	O
xn	O
,	O
x∗	O
)	O
(	O
15.4.5	O
)	O
(	O
15.4.6	O
)	O
and	O
returns	O
document	O
xnopt	O
as	O
the	O
result	O
of	O
the	O
search	O
query	O
.	O
a	O
diﬃculty	O
with	O
this	O
approach	B
is	O
that	O
the	O
bag-of-words	O
representation	B
will	O
have	O
mostly	O
zeros	O
(	O
i.e	O
.	O
be	O
very	O
sparse	B
)	O
.	O
hence	O
diﬀerences	O
may	O
be	O
due	O
to	O
‘	O
noise	O
’	O
rather	O
than	O
any	O
real	O
similarity	O
between	O
the	O
query	B
and	O
database	O
document	O
.	O
lsa	O
alleviates	O
this	O
problem	B
by	O
using	O
a	O
lower	O
dimensional	O
representation	O
y	O
of	O
the	O
high-dimensional	O
x.	O
the	O
y	O
capture	O
the	O
main	O
variations	O
in	O
the	O
data	B
and	O
are	O
less	O
sensitive	O
to	O
random	O
uncorrelated	O
noise	O
.	O
using	O
the	O
dissimilarity	O
deﬁned	O
in	O
terms	O
of	O
the	O
lower	O
dimensional	O
y	O
is	O
therefore	O
more	O
robust	O
and	O
likely	O
to	O
retrieve	O
more	O
useful	O
documents	O
.	O
the	O
squared	O
diﬀerence	O
between	O
two	O
documents	O
can	O
also	O
be	O
written	O
(	O
cid:0	O
)	O
x	O
−	O
x	O
(	O
cid:48	O
)	O
(	O
cid:1	O
)	O
t	O
(	O
cid:0	O
)	O
x	O
−	O
x	O
(	O
cid:48	O
)	O
(	O
cid:1	O
)	O
=	O
xtx	O
+	O
x	O
(	O
cid:48	O
)	O
tx	O
(	O
cid:48	O
)	O
−	O
2xtx	O
(	O
cid:48	O
)	O
if	O
,	O
as	O
is	O
commonly	O
done	O
,	O
the	O
bag-of-words	O
representations	O
are	O
scaled	O
to	O
have	O
unit	O
length	O
,	O
ˆx	O
=	O
x	O
√xtx	O
so	O
that	O
ˆxtˆx	O
=	O
1	O
,	O
the	O
distance	O
is	O
(	O
cid:0	O
)	O
ˆx	O
−	O
ˆx	O
(	O
cid:48	O
)	O
(	O
cid:1	O
)	O
t	O
(	O
cid:0	O
)	O
ˆx	O
−	O
ˆx	O
(	O
cid:48	O
)	O
(	O
cid:1	O
)	O
=	O
2	O
(	O
cid:16	O
)	O
1	O
−	O
ˆxtˆx	O
(	O
cid:48	O
)	O
(	O
cid:17	O
)	O
(	O
15.4.7	O
)	O
(	O
15.4.8	O
)	O
(	O
15.4.9	O
)	O
288	O
draft	O
march	O
9	O
,	O
2010	O
1234567891010987654321	O
pca	O
with	O
missing	O
data	B
figure	O
15.10	O
:	O
(	O
a	O
)	O
:	O
two	O
bag-of-word	O
vectors	O
.	O
the	O
euclidean	O
distance	O
be-	O
tween	O
the	O
two	O
is	O
large	O
.	O
(	O
b	O
)	O
:	O
normalised	O
vectors	O
.	O
the	O
euclidean	O
distance	O
is	O
now	O
related	O
directly	O
to	O
the	O
angle	O
between	O
the	O
vectors	O
.	O
in	O
this	O
case	O
two	O
documents	O
which	O
have	O
the	O
same	O
relative	O
frequency	O
of	O
words	O
will	O
both	O
have	O
the	O
same	O
dissimilarly	O
,	O
even	O
though	O
the	O
number	O
of	O
occurrences	O
of	O
the	O
words	O
is	O
diﬀerent	O
.	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
15.11	O
:	O
top	O
:	O
original	O
data	B
matrix	O
x.	O
black	O
is	O
missing	B
,	O
white	O
present	O
.	O
the	O
data	B
is	O
constructed	O
from	O
a	O
set	O
of	O
only	O
5	O
basis	O
vectors	O
.	O
middle	O
:	O
x	O
with	O
missing	O
data	B
(	O
80	O
%	O
sparsity	O
)	O
.	O
bottom	O
:	O
reconstruction	O
found	O
using	O
svdm.m	O
,	O
svd	O
for	O
missing	B
data	I
.	O
this	O
problem	B
is	O
essentially	O
easy	O
since	O
,	O
despite	O
there	O
being	O
many	O
miss-	O
ing	O
elements	O
,	O
the	O
data	B
is	O
indeed	O
constructed	O
from	O
a	O
model	B
for	O
which	O
svd	O
is	O
appropriate	O
.	O
such	O
techniques	O
have	O
application	O
in	O
collaborative	B
ﬁltering	I
and	O
recom-	O
mender	O
systems	O
where	O
one	O
wishes	O
to	O
‘	O
ﬁll	O
in	O
’	O
missing	B
values	O
in	O
a	O
matrix	B
.	O
and	O
one	O
may	O
equivalently	O
consider	O
the	O
cosine	B
similarity	I
s	O
(	O
ˆx	O
,	O
ˆx	O
(	O
cid:48	O
)	O
)	O
=	O
ˆxtˆx	O
(	O
cid:48	O
)	O
=	O
cos	O
(	O
θ	O
)	O
where	O
θ	O
is	O
the	O
angle	O
between	O
the	O
unit	O
vectors	O
ˆx	O
and	O
ˆx	O
(	O
cid:48	O
)	O
,	O
ﬁg	O
(	O
15.10	O
)	O
.	O
(	O
15.4.10	O
)	O
pca	O
is	O
arguably	O
suboptimal	O
for	O
document	O
analysis	B
since	O
we	O
would	O
expect	O
the	O
presence	O
of	O
a	O
latent	B
topic	I
to	O
contribute	O
only	O
positive	O
counts	O
to	O
the	O
data	B
.	O
a	O
related	O
version	O
of	O
pca	O
in	O
which	O
the	O
decomposition	B
is	O
constrained	O
to	O
have	O
positive	O
elements	O
is	O
called	O
plsa	O
,	O
and	O
discussed	O
in	O
section	O
(	O
15.6	O
)	O
.	O
example	O
72.	O
continuing	O
the	O
inﬂuenza	O
example	O
,	O
someone	O
who	O
uploads	O
a	O
query	B
document	O
which	O
uses	O
the	O
term	O
‘	O
ﬂu	O
’	O
might	O
also	O
be	O
interested	O
in	O
documents	O
about	O
‘	O
inﬂuenza	O
’	O
.	O
however	O
,	O
the	O
search	O
query	O
term	O
‘	O
ﬂu	O
’	O
does	O
not	O
contain	O
the	O
word	O
‘	O
inﬂuenza	O
’	O
,	O
so	O
how	O
can	O
one	O
retrieve	O
such	O
documents	O
?	O
since	O
the	O
ﬁrst	O
component	O
using	O
pca	O
(	O
lsa	O
)	O
groups	O
all	O
‘	O
inﬂuenza	O
’	O
terms	O
together	O
,	O
if	O
we	O
use	O
only	O
the	O
ﬁrst	O
component	O
of	O
the	O
representation	B
y	O
to	O
compare	O
documents	O
,	O
this	O
will	O
retrieve	O
documents	O
independent	O
of	O
whether	O
the	O
term	O
‘	O
ﬂu	O
’	O
or	O
‘	O
inﬂuenza	O
’	O
is	O
used	O
.	O
15.5	O
pca	O
with	O
missing	O
data	B
when	O
values	O
of	O
the	O
data	B
matrix	O
x	O
are	O
missing	B
,	O
the	O
standard	O
pca	O
algorithm	B
as	O
described	O
can	O
not	O
be	O
implemented	O
.	O
unfortunately	O
,	O
there	O
is	O
no	O
‘	O
quick	O
ﬁx	O
’	O
pca	O
solution	O
when	O
some	O
of	O
the	O
xn	O
i	O
are	O
missing	B
and	O
more	O
complex	O
numerical	B
procedures	O
need	O
to	O
invoked	O
.	O
a	O
naive	O
approach	O
in	O
this	O
case	O
is	O
to	O
require	O
the	O
draft	O
march	O
9	O
,	O
2010	O
289	O
501001502002503002040608010050100150200250300204060801005010015020025030020406080100	O
squared	O
reconstruction	O
error	O
to	O
be	O
small	O
only	O
for	O
the	O
existing	O
elements	O
of	O
x.	O
that	O
is	O
n	O
(	O
cid:88	O
)	O
d	O
(	O
cid:88	O
)	O
n=1	O
i=1	O
xn	O
(	O
cid:88	O
)	O
j	O
e	O
(	O
b	O
,	O
y	O
)	O
=	O
γn	O
i	O
i	O
−	O
j	O
bj	O
yn	O
i	O
2	O
(	O
15.5.1	O
)	O
pca	O
with	O
missing	O
data	B
where	O
γn	O
we	O
ﬁnd	O
that	O
the	O
optimal	O
weights	O
satisfy	O
(	O
assuming	O
btb	O
=	O
i	O
)	O
,	O
i	O
=	O
1	O
if	O
the	O
ith	O
entry	O
of	O
the	O
nth	O
vector	O
is	O
available	O
,	O
and	O
is	O
zero	O
otherwise	O
.	O
diﬀerentiating	O
,	O
as	O
before	O
,	O
γn	O
i	O
xn	O
i	O
bk	O
i	O
(	O
15.5.2	O
)	O
n	O
=	O
(	O
cid:88	O
)	O
yk	O
i	O
one	O
then	O
substitutes	O
this	O
expression	O
into	O
the	O
squared	O
error	O
,	O
and	O
minimises	O
the	O
error	O
with	O
respect	O
to	O
b	O
under	O
the	O
orthonormality	O
constraint	O
.	O
an	O
alternative	O
iterative	O
optimisation	O
procedure	O
is	O
as	O
follows	O
:	O
first	O
select	O
a	O
random	O
d	O
×	O
m	O
matrix	B
ˆb	O
.	O
then	O
iterate	O
until	O
convergence	O
the	O
following	O
two	O
steps	O
:	O
optimize	O
y	O
for	O
ﬁxed	O
b	O
for	O
ﬁxed	O
ˆb	O
the	O
above	O
e	O
(	O
ˆb	O
,	O
y	O
)	O
is	O
a	O
quadratic	O
function	O
of	O
the	O
matrix	B
y	O
,	O
which	O
can	O
be	O
optimised	O
directly	O
.	O
by	O
diﬀerentiating	O
and	O
equating	O
to	O
zero	O
,	O
one	O
obtains	O
the	O
ﬁxed	O
point	O
condition	O
2	O
(	O
cid:88	O
)	O
j	O
i	O
−	O
yn	O
j	O
ˆbj	O
i	O
n	O
(	O
cid:88	O
)	O
d	O
(	O
cid:88	O
)	O
n=1	O
i=1	O
(	O
cid:88	O
)	O
l	O
γn	O
i	O
xn	O
(	O
cid:33	O
)	O
m	O
(	O
n	O
)	O
(	O
cid:105	O
)	O
(	O
cid:104	O
)	O
ˆbl	O
i	O
xn	O
i	O
−	O
yn	O
l	O
ˆbk	O
i	O
=	O
0	O
e	O
(	O
ˆb	O
,	O
y	O
)	O
=	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
w	O
(	O
n	O
)	O
(	O
cid:105	O
)	O
deﬁning	O
(	O
cid:104	O
)	O
γn	O
i	O
i	O
=	O
yl	O
n	O
,	O
l	O
=	O
(	O
cid:88	O
)	O
i	O
kl	O
ˆbl	O
i	O
ˆbk	O
i	O
γn	O
i	O
,	O
[	O
cn	O
]	O
k	O
=	O
(	O
cid:88	O
)	O
i	O
γn	O
i	O
xn	O
i	O
ˆbk	O
i	O
,	O
(	O
15.5.3	O
)	O
(	O
15.5.4	O
)	O
(	O
15.5.5	O
)	O
in	O
matrix	B
notation	O
,	O
we	O
then	O
have	O
a	O
set	O
of	O
linear	B
systems	O
:	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
c	O
(	O
n	O
)	O
=	O
m	O
(	O
n	O
)	O
y	O
(	O
n	O
)	O
,	O
(	O
15.5.6	O
)	O
one	O
may	O
solve	O
each	O
linear	B
system	O
using	O
gaussian	O
elimination	O
(	O
one	O
can	O
avoid	O
explicit	O
matrix	B
inversion	O
by	O
using	O
the	O
\	O
operator	O
in	O
matlab	O
)	O
.	O
it	O
can	O
be	O
that	O
one	O
or	O
more	O
of	O
the	O
above	O
linear	B
systems	O
is	O
underdetermined–	O
this	O
can	O
occur	O
when	O
there	O
are	O
less	O
observed	O
values	O
in	O
the	O
nth	O
data	B
column	O
of	O
x	O
than	O
there	O
are	O
components	O
m.	O
in	O
this	O
case	O
one	O
may	O
use	O
the	O
pseudo-inverse	O
to	O
provide	O
a	O
minimal	O
length	O
solution	O
.	O
optimize	O
b	O
for	O
ﬁxed	O
y	O
one	O
now	O
freezes	O
ˆy	O
and	O
considers	O
the	O
function	B
n	O
(	O
cid:88	O
)	O
d	O
(	O
cid:88	O
)	O
n=1	O
i=1	O
xn	O
(	O
cid:88	O
)	O
j	O
γn	O
i	O
i	O
−	O
j	O
bj	O
ˆyn	O
i	O
2	O
e	O
(	O
b	O
,	O
ˆy	O
)	O
=	O
(	O
15.5.7	O
)	O
for	O
ﬁxed	O
ˆy	O
the	O
above	O
expression	O
is	O
quadratic	O
in	O
the	O
matrix	B
b	O
,	O
which	O
can	O
again	O
be	O
optimised	O
using	O
linear	B
algebra	I
.	O
this	O
corresponds	O
to	O
solving	B
a	O
set	O
of	O
linear	B
systems	O
for	O
the	O
ith	O
row	O
of	O
b	O
:	O
m	O
(	O
i	O
)	O
=	O
f	O
(	O
i	O
)	O
b	O
(	O
i	O
)	O
m	O
(	O
i	O
)	O
(	O
cid:105	O
)	O
=	O
(	O
cid:88	O
)	O
where	O
(	O
cid:104	O
)	O
mathematically	O
,	O
this	O
is	O
b	O
(	O
i	O
)	O
=	O
f	O
(	O
i	O
)	O
−1m	O
(	O
i	O
)	O
.	O
(	O
cid:104	O
)	O
f	O
(	O
i	O
)	O
(	O
cid:105	O
)	O
γn	O
i	O
xn	O
i	O
ˆyn	O
k	O
,	O
n	O
k	O
kj	O
=	O
(	O
cid:88	O
)	O
n	O
γn	O
i	O
ˆyn	O
j	O
ˆyn	O
k	O
(	O
15.5.8	O
)	O
(	O
15.5.9	O
)	O
in	O
this	O
manner	O
one	O
is	O
guaranteed	O
to	O
iteratively	O
decrease	O
the	O
value	B
of	O
the	O
squared	O
error	O
loss	O
until	O
a	O
minimum	O
is	O
reached	O
.	O
this	O
technique	O
is	O
implemented	O
in	O
svdm.m	O
.	O
note	O
that	O
eﬃcient	B
techniques	O
based	O
on	O
updating	O
the	O
solution	O
as	O
a	O
new	O
column	O
of	O
x	O
arrives	O
one	O
at	O
a	O
time	O
(	O
‘	O
online	B
’	O
updating	O
)	O
are	O
available	O
,	O
see	O
for	O
example	O
[	O
49	O
]	O
.	O
290	O
draft	O
march	O
9	O
,	O
2010	O
pca	O
with	O
missing	O
data	B
15.5.1	O
finding	O
the	O
principal	B
directions	I
for	O
the	O
missing	B
data	I
case	O
the	O
basis	O
b	O
found	O
using	O
the	O
above	O
technique	O
is	O
based	O
only	O
on	O
minimising	O
the	O
squared	O
reconstruction	O
error	O
and	O
therefore	O
does	O
not	O
necessarily	O
satisfy	O
the	O
maximal	O
variance	B
(	O
or	O
principal	B
directions	I
)	O
criterion	O
,	O
namely	O
that	O
the	O
columns	O
of	O
b	O
point	O
along	O
the	O
eigen-directions	O
.	O
for	O
a	O
given	O
b	O
,	O
y	O
with	O
approximate	O
decomposition	B
x	O
≈	O
by	O
we	O
can	O
return	O
a	O
new	O
orthonormal	B
basis	O
u	O
by	O
performing	O
svd	O
on	O
the	O
completed	O
data	B
,	O
by	O
=	O
usvt	O
to	O
return	O
an	O
orthonormal	B
basis	O
b	O
→	O
u.	O
in	O
general	O
,	O
however	O
,	O
this	O
is	O
potentially	O
computationally	O
expensive	O
.	O
if	O
only	O
the	O
principal	B
directions	I
are	O
required	O
,	O
an	O
alternative	O
is	O
to	O
explicitly	O
transform	O
the	O
solution	O
b	O
using	O
an	O
invertible	O
matrix	B
q	O
:	O
x	O
≈	O
bqq−1y	O
(	O
15.5.10	O
)	O
calling	O
the	O
new	O
basis	O
˜b	O
=	O
bq	O
,	O
for	O
a	O
solution	O
to	O
be	O
aligned	O
with	O
the	O
principal	B
directions	I
,	O
we	O
need	O
˜bt	O
˜b	O
=	O
i	O
in	O
other	O
words	O
qtbtbq	O
=	O
i	O
forming	O
the	O
svd	O
of	O
b	O
,	O
b	O
=	O
udvt	O
and	O
substituting	O
in	O
equation	B
(	O
15.5.12	O
)	O
,	O
we	O
have	O
the	O
requirement	O
qtvdutudvtq	O
=	O
i	O
since	O
utu	O
=	O
i	O
and	O
vtv	O
=	O
vvt	O
=	O
i	O
,	O
we	O
can	O
use	O
q	O
=	O
vtd−1	O
hence	O
,	O
given	O
a	O
solution	O
b	O
,	O
we	O
can	O
ﬁnd	O
the	O
principal	B
directions	I
from	O
the	O
svd	O
of	O
b	O
using	O
˜b	O
=	O
utd−1b	O
(	O
15.5.11	O
)	O
(	O
15.5.12	O
)	O
(	O
15.5.13	O
)	O
(	O
15.5.14	O
)	O
(	O
15.5.15	O
)	O
(	O
15.5.16	O
)	O
if	O
the	O
d	O
×	O
m	O
matrix	B
b	O
is	O
non-square	O
m	O
<	O
d	O
,	O
then	O
the	O
matrix	B
d	O
will	O
be	O
non-square	O
and	O
non-invertible	O
.	O
to	O
make	O
the	O
above	O
well	O
deﬁned	O
,	O
one	O
may	O
append	O
d	O
with	O
the	O
columns	O
of	O
the	O
identity	O
:	O
d	O
(	O
cid:48	O
)	O
=	O
[	O
d	O
,	O
im	O
+1	O
,	O
.	O
.	O
.	O
,	O
id	O
]	O
(	O
15.5.17	O
)	O
where	O
ik	O
is	O
the	O
kth	O
column	O
of	O
the	O
identity	B
matrix	I
,	O
and	O
use	O
d	O
(	O
cid:48	O
)	O
in	O
place	O
of	O
d	O
above	O
.	O
15.5.2	O
collaborative	B
ﬁltering	I
using	O
pca	O
with	O
missing	O
data	B
entry	O
in	O
the	O
vector	O
x	O
speciﬁes	O
the	O
rating	O
the	O
user	O
gives	O
to	O
the	O
ith	O
ﬁlm	O
.	O
the	O
matrix	B
x	O
=	O
(	O
cid:2	O
)	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
(	O
cid:3	O
)	O
for	O
all	O
a	O
database	O
contains	O
a	O
set	O
of	O
vectors	O
,	O
each	O
describing	O
the	O
ﬁlm	O
ratings	O
for	O
a	O
user	O
in	O
the	O
database	O
.	O
the	O
ith	O
the	O
n	O
users	O
,	O
has	O
many	O
missing	B
values	O
since	O
any	O
single	O
user	O
will	O
only	O
have	O
given	O
a	O
rating	O
for	O
a	O
small	O
selection	O
of	O
the	O
possible	O
d	O
ﬁlms	O
.	O
in	O
a	O
practical	O
example	O
one	O
might	O
have	O
d	O
=	O
10	O
,	O
000	O
ﬁlms	O
and	O
n	O
=	O
1	O
,	O
000	O
,	O
000	O
users	O
.	O
for	O
any	O
user	O
n	O
the	O
task	O
is	O
to	O
predict	O
reasonable	O
values	O
for	O
the	O
missing	B
entries	O
of	O
their	O
rating	O
vector	O
xn	O
,	O
thereby	O
providing	O
a	O
suggestion	O
as	O
to	O
which	O
ﬁlms	O
they	O
might	O
like	O
to	O
view	O
.	O
viewed	O
as	O
a	O
missing	B
data	I
problem	O
,	O
one	O
can	O
ﬁt	O
b	O
and	O
y	O
using	O
svdm.m	O
as	O
above	O
.	O
given	O
b	O
and	O
y	O
we	O
can	O
form	O
a	O
reconstruction	O
on	O
all	O
the	O
entries	O
of	O
x	O
,	O
by	O
using	O
˜x	O
=	O
by	O
giving	O
therefore	O
a	O
prediction	B
for	O
the	O
missing	B
values	O
.	O
draft	O
march	O
9	O
,	O
2010	O
(	O
15.5.18	O
)	O
291	O
matrix	B
decomposition	O
methods	O
figure	O
15.12	O
:	O
(	O
a	O
)	O
:	O
under-complete	B
representation	I
.	O
there	O
are	O
too	O
few	O
basis	O
(	O
b	O
)	O
:	O
over-complete	B
representation	I
.	O
vectors	O
to	O
represent	O
the	O
datapoints	O
.	O
there	O
are	O
too	O
many	O
basis	O
vectors	O
to	O
form	O
a	O
unique	O
representation	B
of	O
a	O
datapoint	O
in	O
terms	O
of	O
a	O
linear	B
combination	O
of	O
the	O
basis	O
vectors	O
.	O
(	O
a	O
)	O
(	O
b	O
)	O
z	O
(	O
a	O
)	O
x	O
y	O
x	O
y	O
z	O
(	O
b	O
)	O
figure	O
15.13	O
:	O
(	O
a	O
)	O
:	O
joint	B
plsa	O
.	O
(	O
b	O
)	O
:	O
conditional	B
plsa	O
.	O
whilst	O
written	O
as	O
a	O
graphical	O
model	B
,	O
some	O
care	O
is	O
required	O
in	O
the	O
interpretation	O
,	O
see	O
text	O
.	O
15.6	O
matrix	B
decomposition	O
methods	O
given	O
a	O
data	B
matrix	O
x	O
for	O
which	O
each	O
column	O
represents	O
a	O
datapoint	O
,	O
an	O
approximate	B
matrix	O
decompo-	O
sition	O
is	O
of	O
the	O
form	O
x	O
≈	O
by	O
into	O
a	O
basis	O
matrix	B
b	O
and	O
weight	B
(	O
or	O
coordinate	O
)	O
matrix	B
y.	O
symbolically	O
,	O
matrix	B
decompositions	O
are	O
of	O
the	O
form	O
	O
(	O
cid:124	O
)	O
x	O
:	O
data	B
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
d×n	O
	O
(	O
cid:125	O
)	O
	O
b	O
:	O
basis	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
d×m	O
≈	O
(	O
cid:124	O
)	O
	O
(	O
cid:125	O
)	O
	O
(	O
cid:124	O
)	O
y	O
:	O
weights/components	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
m×n	O
	O
(	O
cid:125	O
)	O
(	O
15.6.1	O
)	O
in	O
this	O
section	O
we	O
will	O
consider	O
some	O
common	O
matrix	B
decomposition	O
methods	O
.	O
under-complete	B
decompositions	O
when	O
m	O
<	O
d	O
,	O
there	O
are	O
fewer	O
basis	O
vectors	O
than	O
dimensions	O
,	O
ﬁg	O
(	O
15.12a	O
)	O
.	O
the	O
matrix	B
b	O
is	O
then	O
called	O
‘	O
tall	O
’	O
or	O
‘	O
thin	B
’	O
.	O
in	O
this	O
case	O
the	O
matrix	B
y	O
forms	O
a	O
lower	O
dimensional	O
approximate	O
representation	B
of	O
the	O
data	B
x	O
,	O
pca	O
being	O
a	O
classic	O
example	O
.	O
over-complete	B
decompositions	O
for	O
m	O
>	O
d	O
the	O
basis	O
is	O
over-complete	B
,	O
there	O
being	O
more	O
basis	O
vectors	O
than	O
dimensions	O
,	O
ﬁg	O
(	O
15.12b	O
)	O
.	O
in	O
such	O
cases	O
additional	O
constraints	O
are	O
placed	O
on	O
either	O
the	O
basis	O
or	O
components	O
.	O
for	O
example	O
,	O
one	O
might	O
require	O
that	O
only	O
a	O
small	O
number	O
of	O
the	O
large	O
number	O
of	O
available	O
basis	O
vectors	O
is	O
used	O
to	O
form	O
the	O
representation	B
for	O
any	O
given	O
x.	O
such	O
sparse-representations	O
are	O
common	O
in	O
theoretical	O
neurobiology	O
where	O
issues	O
of	O
energy	B
eﬃciency	O
,	O
rapidity	O
of	O
processing	O
and	O
robustness	O
are	O
of	O
interest	O
[	O
214	O
,	O
155	O
,	O
252	O
]	O
.	O
15.6.1	O
probabilistic	B
latent	I
semantic	I
analysis	I
consider	O
two	O
objects	O
,	O
x	O
and	O
y	O
,	O
where	O
dom	O
(	O
x	O
)	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
i	O
}	O
and	O
dom	O
(	O
y	O
)	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
j	O
}	O
.	O
we	O
have	O
a	O
count	O
matrix	B
with	O
elements	O
cij	O
which	O
describes	O
the	O
number	O
of	O
times	O
that	O
x	O
=	O
i	O
,	O
y	O
=	O
j	O
was	O
observed	O
.	O
we	O
can	O
transform	O
this	O
count	O
matrix	B
into	O
a	O
‘	O
frequency	O
’	O
matrix	B
p	O
with	O
elements	O
p	O
(	O
x	O
=	O
i	O
,	O
y	O
=	O
j	O
)	O
=	O
cij	O
(	O
cid:80	O
)	O
(	O
cid:88	O
)	O
p	O
(	O
x	O
=	O
i	O
,	O
y	O
=	O
j	O
)	O
ij	O
cij	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
xij	O
our	O
interest	O
is	O
to	O
ﬁnd	O
a	O
decomposition	B
of	O
this	O
frequency	O
matrix	B
of	O
the	O
form	O
in	O
ﬁg	O
(	O
15.13a	O
)	O
(	O
cid:125	O
)	O
≈	O
k	O
(	O
cid:124	O
)	O
(	O
cid:125	O
)	O
˜p	O
(	O
x	O
=	O
i|z	O
=	O
k	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:124	O
)	O
(	O
cid:125	O
)	O
˜p	O
(	O
y	O
=	O
j|z	O
=	O
k	O
)	O
˜p	O
(	O
z	O
=	O
k	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
bik	O
ykj	O
≡	O
˜p	O
(	O
x	O
=	O
i	O
,	O
y	O
=	O
j	O
)	O
292	O
draft	O
march	O
9	O
,	O
2010	O
(	O
15.6.2	O
)	O
(	O
15.6.3	O
)	O
matrix	B
decomposition	O
methods	O
which	O
is	O
a	O
form	O
of	O
matrix	B
decomposition	O
into	O
basis	O
b	O
and	O
coordinates	O
y.	O
this	O
has	O
the	O
interpretation	O
of	O
discovering	O
latent	B
topics	O
z	O
that	O
describe	O
the	O
joint	B
behaviour	O
of	O
x	O
and	O
y.	O
an	O
em	O
style	O
training	B
algorithm	O
in	O
order	O
to	O
ﬁnd	O
the	O
approximate	B
decomposition	O
we	O
ﬁrst	O
need	O
a	O
measure	O
of	O
diﬀerence	O
between	O
the	O
matrix	B
with	O
elements	O
pij	O
and	O
the	O
approximation	B
with	O
elements	O
˜pij	O
.	O
since	O
all	O
elements	O
are	O
bounded	O
between	O
0	O
and	O
1	O
and	O
sum	O
to	O
1	O
,	O
we	O
may	O
interpret	O
p	O
as	O
a	O
joint	B
probability	O
and	O
˜p	O
as	O
an	O
approximation	B
to	O
this	O
.	O
for	O
probabilities	O
,	O
a	O
useful	O
measure	O
of	O
discrepancy	O
is	O
the	O
kullback-leibler	O
divergence	B
since	O
p	O
is	O
ﬁxed	O
,	O
minimising	O
the	O
kullback-leibler	O
divergence	B
with	O
respect	O
to	O
the	O
approximation	B
˜p	O
is	O
equiv-	O
alent	O
to	O
maximising	O
the	O
‘	O
likelihood	B
’	O
term	O
(	O
cid:104	O
)	O
log	O
˜p	O
(	O
cid:105	O
)	O
p.	O
this	O
is	O
(	O
15.6.4	O
)	O
(	O
15.6.5	O
)	O
p	O
(	O
x	O
,	O
y	O
)	O
log	O
˜p	O
(	O
x	O
,	O
y	O
)	O
kl	O
(	O
p|˜p	O
)	O
=	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
cid:105	O
)	O
p	O
−	O
(	O
cid:104	O
)	O
log	O
˜p	O
(	O
cid:105	O
)	O
p	O
(	O
cid:88	O
)	O
kl	O
(	O
q	O
(	O
z|x	O
,	O
y	O
)	O
|˜p	O
(	O
z|x	O
,	O
y	O
)	O
)	O
=	O
(	O
cid:88	O
)	O
where	O
(	O
cid:80	O
)	O
x	O
,	O
y	O
z	O
it	O
’	O
s	O
convenient	O
to	O
derive	O
an	O
em	O
style	O
algorithm	B
to	O
learn	O
˜p	O
(	O
x|z	O
)	O
,	O
˜p	O
(	O
y|z	O
)	O
and	O
˜p	O
(	O
z	O
)	O
.	O
to	O
do	O
this	O
,	O
consider	O
q	O
(	O
z|x	O
,	O
y	O
)	O
log	O
q	O
(	O
z|x	O
,	O
y	O
)	O
−	O
q	O
(	O
z|x	O
,	O
y	O
)	O
log	O
˜p	O
(	O
z|x	O
,	O
y	O
)	O
≥	O
0	O
(	O
15.6.6	O
)	O
(	O
cid:88	O
)	O
z	O
z	O
implies	O
summation	O
over	O
all	O
states	O
of	O
the	O
variable	B
z.	O
rearranging	O
,	O
this	O
gives	O
the	O
bound	B
,	O
q	O
(	O
z|x	O
,	O
y	O
)	O
log	O
˜p	O
(	O
z	O
,	O
x	O
,	O
y	O
)	O
(	O
15.6.7	O
)	O
log	O
˜p	O
(	O
x	O
,	O
y	O
)	O
≥	O
−	O
(	O
cid:88	O
)	O
plugging	O
this	O
into	O
the	O
‘	O
likelihood	B
’	O
term	O
above	O
,	O
we	O
have	O
the	O
bound	B
p	O
(	O
x	O
,	O
y	O
)	O
log	O
˜p	O
(	O
x	O
,	O
y	O
)	O
≥	O
−	O
x	O
,	O
y	O
q	O
(	O
z|x	O
,	O
y	O
)	O
log	O
q	O
(	O
z|x	O
,	O
y	O
)	O
(	O
cid:88	O
)	O
z	O
q	O
(	O
z|x	O
,	O
y	O
)	O
log	O
q	O
(	O
z|x	O
,	O
y	O
)	O
+	O
(	O
cid:88	O
)	O
p	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:88	O
)	O
+	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
p	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:88	O
)	O
x	O
,	O
y	O
z	O
z	O
x	O
,	O
y	O
z	O
m-step	O
for	O
ﬁxed	O
˜p	O
(	O
x|z	O
)	O
,	O
˜p	O
(	O
y|z	O
)	O
,	O
the	O
contribution	O
to	O
the	O
bound	B
from	O
˜p	O
(	O
z	O
)	O
is	O
q	O
(	O
z|x	O
,	O
y	O
)	O
[	O
log	O
˜p	O
(	O
x|z	O
)	O
+	O
log	O
˜p	O
(	O
y|z	O
)	O
+	O
log	O
˜p	O
(	O
z	O
)	O
]	O
(	O
15.6.8	O
)	O
(	O
15.6.9	O
)	O
(	O
15.6.10	O
)	O
.	O
similarly	O
,	O
for	O
ﬁxed	O
˜p	O
(	O
y|z	O
)	O
,	O
˜p	O
(	O
z	O
)	O
,	O
(	O
15.6.11	O
)	O
(	O
15.6.12	O
)	O
(	O
15.6.13	O
)	O
293	O
it	O
is	O
straightforward	O
to	O
see	O
that	O
the	O
optimal	O
setting	O
of	O
˜p	O
(	O
z	O
)	O
is	O
(	O
cid:16	O
)	O
(	O
cid:80	O
)	O
(	O
cid:17	O
)	O
x	O
,	O
y	O
q	O
(	O
z|x	O
,	O
y	O
)	O
p	O
(	O
x	O
,	O
y	O
)	O
|˜p	O
(	O
z	O
)	O
q	O
(	O
z|x	O
,	O
y	O
)	O
log	O
˜p	O
(	O
z	O
)	O
q	O
(	O
z|x	O
,	O
y	O
)	O
p	O
(	O
x	O
,	O
y	O
)	O
x	O
,	O
y	O
(	O
cid:88	O
)	O
p	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:88	O
)	O
˜p	O
(	O
z	O
)	O
=	O
(	O
cid:88	O
)	O
x	O
,	O
y	O
z	O
(	O
cid:88	O
)	O
x	O
,	O
y	O
z	O
p	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
y	O
since	O
equation	B
(	O
15.6.9	O
)	O
is	O
,	O
up	O
to	O
a	O
constant	O
,	O
kl	O
the	O
contribution	O
to	O
the	O
bound	B
from	O
˜p	O
(	O
x|z	O
)	O
is	O
q	O
(	O
z|x	O
,	O
y	O
)	O
log	O
˜p	O
(	O
y|z	O
)	O
therefore	O
,	O
optimally	O
˜p	O
(	O
x|z	O
)	O
∝	O
p	O
(	O
x	O
,	O
y	O
)	O
q	O
(	O
z|x	O
,	O
y	O
)	O
and	O
similarly	O
,	O
˜p	O
(	O
y|z	O
)	O
∝	O
p	O
(	O
x	O
,	O
y	O
)	O
q	O
(	O
z|x	O
,	O
y	O
)	O
x	O
draft	O
march	O
9	O
,	O
2010	O
matrix	B
decomposition	O
methods	O
the	O
images	O
figure	O
15.14	O
:	O
(	O
a	O
)	O
conditional	B
plsa	O
reconstruction	O
of	O
in	O
ﬁg	O
(	O
15.5	O
)	O
using	O
a	O
positive	O
(	O
convex	O
)	O
combination	O
of	O
the	O
49	O
positive	O
base	O
images	O
in	O
(	O
b	O
)	O
.	O
the	O
root	O
mean	B
square	O
reconstruction	O
error	O
is	O
1.391×	O
10−5	O
.	O
the	O
base	O
images	O
tend	O
to	O
be	O
more	O
‘	O
localised	O
’	O
than	O
the	O
correspond-	O
ing	O
eigen-images	O
ﬁg	O
(	O
15.6b	O
)	O
.	O
here	O
one	O
sees	O
local	B
structure	O
such	O
as	O
foreheads	O
,	O
chins	O
,	O
etc	O
.	O
(	O
15.6.14	O
)	O
(	O
a	O
)	O
(	O
b	O
)	O
e-step	O
the	O
optimal	O
setting	O
for	O
the	O
q	O
distribution	B
at	O
each	O
iteration	B
is	O
q	O
(	O
z|x	O
,	O
y	O
)	O
=	O
˜p	O
(	O
z|x	O
,	O
y	O
)	O
which	O
is	O
ﬁxed	O
throughout	O
the	O
m-step	O
.	O
the	O
procedure	O
is	O
given	O
in	O
algorithm	B
(	O
15	O
)	O
and	O
a	O
demonstration	O
is	O
in	O
demoplsa.m	O
.	O
the	O
‘	O
likelihood	B
’	O
equation	B
(	O
15.6.5	O
)	O
is	O
guaranteed	O
to	O
increase	O
(	O
and	O
the	O
kullback-leibler	O
divergence	B
equation	O
(	O
15.6.4	O
)	O
decrease	O
)	O
under	O
iterating	O
between	O
the	O
e	O
and	O
m-steps	O
,	O
since	O
the	O
method	O
is	O
analogous	O
to	O
an	O
em	O
procedure	O
generalisations	O
,	O
such	O
as	O
using	O
simpler	O
q	O
distributions	O
,	O
(	O
corresponding	O
to	O
generalised	B
em	O
procedures	O
)	O
are	O
immediate	O
based	O
on	O
modifying	O
the	O
above	O
derivation	O
.	O
a	O
related	O
probabilistic	B
model	O
for	O
variables	O
x	O
,	O
y	O
,	O
z	O
with	O
z	O
hidden	B
and	O
dom	O
(	O
x	O
)	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
i	O
}	O
,	O
dom	O
(	O
y	O
)	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
j	O
}	O
,	O
dom	O
(	O
z	O
)	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
,	O
consider	O
a	O
distribution	B
˜p	O
(	O
x	O
,	O
y	O
,	O
z	O
)	O
=	O
˜p	O
(	O
x|z	O
)	O
˜p	O
(	O
y|z	O
)	O
˜p	O
(	O
z	O
)	O
(	O
15.6.15	O
)	O
and	O
data	B
d	O
=	O
{	O
(	O
xn	O
,	O
yn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
.	O
assuming	O
the	O
data	B
are	O
i.i.d	O
.	O
draws	O
from	O
equation	B
(	O
15.6.15	O
)	O
the	O
log	O
likelihood	B
is	O
log	O
˜p	O
(	O
d	O
)	O
=	O
n	O
(	O
cid:88	O
)	O
˜p	O
(	O
xn	O
,	O
yn	O
)	O
=	O
(	O
cid:88	O
)	O
n=1	O
where	O
log	O
˜p	O
(	O
xn	O
,	O
yn	O
)	O
˜p	O
(	O
xn|zn	O
)	O
˜p	O
(	O
yn|zn	O
)	O
˜p	O
(	O
zn	O
)	O
zn	O
(	O
15.6.16	O
)	O
(	O
15.6.17	O
)	O
if	O
xn	O
,	O
yn	O
are	O
sampled	O
from	O
a	O
distribution	B
p	O
(	O
x	O
,	O
y	O
)	O
then	O
,	O
in	O
the	O
limit	O
of	O
an	O
inﬁnite	O
number	O
of	O
samples	O
,	O
n	O
→	O
∞	O
,	O
equation	B
(	O
15.6.16	O
)	O
becomes	O
log	O
˜p	O
(	O
d	O
)	O
=	O
(	O
cid:104	O
)	O
log	O
˜p	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
,	O
y	O
)	O
(	O
15.6.18	O
)	O
which	O
is	O
equation	B
(	O
15.6.5	O
)	O
.	O
from	O
this	O
viewpoint	O
,	O
even	O
though	O
we	O
started	O
out	O
with	O
a	O
set	O
of	O
samples	O
,	O
in	O
the	O
limit	O
,	O
only	O
the	O
distribution	B
of	O
the	O
observed	O
data	O
,	O
p	O
(	O
x	O
,	O
y	O
)	O
is	O
relevant	O
.	O
the	O
generic	O
code	O
for	O
the	O
ﬁnite	O
sample	O
case	O
,	O
trained	O
with	O
em	O
is	O
given	O
in	O
demomultinomialpxygz.m	O
.	O
see	O
also	O
exercise	O
(	O
168	O
)	O
.	O
a	O
fully	O
probabilistic	B
interpretation	O
of	O
plsa	O
can	O
be	O
made	O
via	O
poisson	O
processes	O
[	O
56	O
]	O
.	O
a	O
related	O
probabilistic	B
model	O
is	O
latent	B
dirichlet	O
allocation	O
,	O
which	O
is	O
described	O
in	O
section	O
(	O
20.6.1	O
)	O
.	O
294	O
draft	O
march	O
9	O
,	O
2010	O
matrix	B
decomposition	O
methods	O
algorithm	B
15	O
plsa	O
:	O
given	O
a	O
frequency	O
matrix	B
p	O
(	O
x	O
=	O
i	O
,	O
y	O
=	O
j	O
)	O
,	O
return	O
a	O
decomposition	B
(	O
cid:80	O
)	O
k	O
)	O
˜p	O
(	O
y	O
=	O
j|z	O
=	O
k	O
)	O
˜p	O
(	O
z	O
=	O
k	O
)	O
.	O
see	O
plsa.m	O
1	O
:	O
initialise	O
˜p	O
(	O
z	O
)	O
,	O
˜p	O
(	O
x|z	O
)	O
,	O
˜p	O
(	O
y|z	O
)	O
.	O
2	O
:	O
while	O
not	O
converged	O
do	O
set	O
q	O
(	O
z|x	O
,	O
y	O
)	O
=	O
˜p	O
(	O
z|x	O
,	O
y	O
)	O
3	O
:	O
set	O
˜p	O
(	O
x|z	O
)	O
∝	O
4	O
:	O
set	O
˜p	O
(	O
y|z	O
)	O
∝	O
5	O
:	O
6	O
:	O
end	O
while	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
y	O
p	O
(	O
x	O
,	O
y	O
)	O
q	O
(	O
z|x	O
,	O
y	O
)	O
x	O
p	O
(	O
x	O
,	O
y	O
)	O
q	O
(	O
z|x	O
,	O
y	O
)	O
x	O
,	O
y	O
p	O
(	O
x	O
,	O
y	O
)	O
q	O
(	O
z|x	O
,	O
y	O
)	O
k	O
˜p	O
(	O
x	O
=	O
i|z	O
=	O
(	O
cid:46	O
)	O
e-step	O
(	O
cid:46	O
)	O
m-steps	O
7	O
:	O
set	O
˜p	O
(	O
z	O
)	O
=	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
algorithm	B
16	O
conditional	B
plsa	O
:	O
given	O
a	O
frequency	O
matrix	B
p	O
(	O
x	O
=	O
i|y	O
=	O
j	O
)	O
,	O
return	O
a	O
decomposition	B
k	O
˜p	O
(	O
x	O
=	O
i|z	O
=	O
k	O
)	O
˜p	O
(	O
z	O
=	O
k|y	O
=	O
j	O
)	O
.	O
see	O
plsacond.m	O
1	O
:	O
initialise	O
˜p	O
(	O
x|z	O
)	O
,	O
˜p	O
(	O
z|y	O
)	O
.	O
(	O
cid:80	O
)	O
2	O
:	O
while	O
not	O
converged	O
do	O
(	O
cid:80	O
)	O
3	O
:	O
y	O
p	O
(	O
x|y	O
)	O
q	O
(	O
z|x	O
,	O
y	O
)	O
4	O
:	O
x	O
p	O
(	O
x|y	O
)	O
q	O
(	O
z|x	O
,	O
y	O
)	O
5	O
:	O
6	O
:	O
end	O
while	O
set	O
q	O
(	O
z|x	O
,	O
y	O
)	O
=	O
˜p	O
(	O
z|x	O
,	O
y	O
)	O
set	O
˜p	O
(	O
x|z	O
)	O
∝	O
set	O
˜p	O
(	O
z|y	O
)	O
∝	O
(	O
cid:46	O
)	O
e-step	O
(	O
cid:46	O
)	O
m-steps	O
conditional	B
plsa	O
in	O
some	O
cases	O
it	O
is	O
more	O
natural	B
to	O
consider	O
a	O
conditional	B
frequency	O
matrix	B
p	O
(	O
x	O
=	O
i|y	O
=	O
j	O
)	O
(	O
cid:124	O
)	O
(	O
cid:125	O
)	O
p	O
(	O
x	O
=	O
i|y	O
=	O
j	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
and	O
seek	O
an	O
approximate	B
decomposition	O
(	O
cid:124	O
)	O
(	O
cid:125	O
)	O
˜p	O
(	O
x	O
=	O
i|z	O
=	O
k	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
≈	O
(	O
cid:88	O
)	O
k	O
xij	O
bik	O
(	O
cid:124	O
)	O
(	O
cid:125	O
)	O
˜p	O
(	O
z	O
=	O
k|y	O
=	O
j	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
ykj	O
(	O
15.6.19	O
)	O
(	O
15.6.20	O
)	O
as	O
depicted	O
in	O
ﬁg	O
(	O
15.13b	O
)	O
.	O
deriving	O
an	O
em	O
style	O
algorithm	B
for	O
this	O
is	O
straightforward	O
(	O
see	O
exercise	O
(	O
167	O
)	O
)	O
,	O
and	O
is	O
presented	O
in	O
algorithm	B
(	O
16	O
)	O
,	O
being	O
equivalent	B
to	O
the	O
non-negative	B
matrix	I
factorisation	I
algorithm	O
of	O
[	O
171	O
]	O
.	O
is	O
unity	O
,	O
(	O
cid:80	O
)	O
example	O
73	O
(	O
discovering	O
the	O
basis	O
)	O
.	O
a	O
set	O
of	O
images	O
is	O
give	O
in	O
ﬁg	O
(	O
15.15a	O
)	O
.	O
these	O
were	O
created	O
by	O
ﬁrst	O
deﬁning	O
4	O
base	O
images	O
ﬁg	O
(	O
15.15b	O
)	O
.	O
each	O
base	O
image	O
is	O
positive	O
and	O
scaled	O
so	O
that	O
the	O
sum	O
of	O
the	O
pixels	O
i	O
p	O
(	O
x	O
=	O
i|z	O
=	O
k	O
)	O
=	O
1	O
,	O
where	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
4	O
and	O
x	O
indexes	O
the	O
pixels	O
,	O
see	O
ﬁg	O
(	O
15.15	O
)	O
.	O
we	O
then	O
sum	O
each	O
of	O
these	O
images	O
using	O
a	O
randomly	O
chosen	O
positive	O
set	O
of	O
4	O
weights	O
(	O
under	O
the	O
constraint	O
that	O
the	O
weights	O
sum	O
to	O
1	O
)	O
to	O
generate	O
a	O
training	B
image	O
with	O
elements	O
p	O
(	O
x	O
=	O
i|y	O
=	O
j	O
)	O
and	O
j	O
indexes	O
the	O
training	B
image	O
.	O
this	O
is	O
repeated	O
144	O
times	O
to	O
form	O
the	O
full	O
training	B
set	O
,	O
ﬁg	O
(	O
15.15a	O
)	O
.	O
the	O
task	O
is	O
,	O
given	O
only	O
the	O
training	B
set	O
images	O
,	O
to	O
reconstruct	O
the	O
basis	O
from	O
which	O
the	O
images	O
were	O
formed	O
.	O
we	O
assume	O
that	O
we	O
know	O
the	O
correct	O
number	O
of	O
base	O
images	O
,	O
namely	O
4.	O
the	O
results	O
of	O
using	O
conditional	B
plsa	O
on	O
this	O
task	O
are	O
presented	O
in	O
ﬁg	O
(	O
15.15c	O
)	O
and	O
using	O
svd	O
in	O
ﬁg	O
(	O
15.15d	O
)	O
.	O
in	O
this	O
case	O
plsa	O
ﬁnds	O
the	O
correct	O
‘	O
natural	B
’	O
basis	O
,	O
corresponding	O
to	O
the	O
way	O
the	O
images	O
were	O
generated	O
.	O
the	O
eigenbasis	O
is	O
just	O
as	O
good	O
in	O
terms	O
of	O
being	O
able	O
to	O
represent	O
any	O
of	O
the	O
training	B
images	O
,	O
but	O
in	O
this	O
case	O
does	O
not	O
correspond	O
to	O
the	O
constraints	O
under	O
which	O
the	O
data	B
was	O
generated	O
15.6.2	O
extensions	O
and	O
variations	O
non-negative	B
matrix	I
factorisation	I
non-negative	O
matrix	B
factorisation	I
(	O
nmf	O
)	O
considers	O
a	O
decomposition	B
in	O
which	O
both	O
the	O
basis	O
and	O
weight	B
matrices	O
have	O
non-negative	O
entries	O
.	O
an	O
early	O
example	O
of	O
this	O
work	O
is	O
as	O
a	O
form	O
of	O
constrained	O
factor	O
draft	O
march	O
9	O
,	O
2010	O
295	O
matrix	B
decomposition	O
methods	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
(	O
d	O
)	O
figure	O
15.15	O
:	O
(	O
a	O
)	O
:	O
training	B
data	O
,	O
consisting	O
of	O
a	O
positive	O
(	O
convex	O
)	O
combination	O
of	O
the	O
base	O
images	O
.	O
(	O
b	O
)	O
:	O
(	O
c	O
)	O
:	O
basis	O
learned	O
using	O
conditional	B
the	O
chosen	O
base	O
images	O
from	O
which	O
the	O
training	B
data	O
is	O
derived	O
.	O
(	O
d	O
)	O
:	O
eigenbasis	O
plsa	O
on	O
the	O
training	B
data	O
.	O
this	O
is	O
virtually	O
indistinguishable	O
from	O
the	O
true	O
basis	O
.	O
(	O
sometimes	O
called	O
‘	O
eigenfaces	O
’	O
)	O
.	O
analysis	B
[	O
217	O
]	O
.	O
closely	O
related	O
works	O
are	O
[	O
171	O
]	O
which	O
is	O
a	O
generalisation	B
of	O
plsa	O
(	O
since	O
there	O
is	O
no	O
require-	O
ment	O
that	O
the	O
basis	O
or	O
components	O
sum	O
to	O
unity	O
)	O
.	O
in	O
all	O
cases	O
em-style	O
training	B
algorithms	O
exist	O
,	O
although	O
their	O
convergence	O
can	O
be	O
slow	O
.	O
a	O
natural	B
relaxation	O
is	O
when	O
only	O
one	O
of	O
the	O
factors	O
in	O
the	O
decomposition	B
is	O
constrained	O
to	O
be	O
non-negative	O
.	O
we	O
will	O
encounter	O
similar	O
models	O
in	O
the	O
discussion	O
on	O
independent	O
component	O
analysis	B
,	O
section	O
(	O
21.6	O
)	O
.	O
gradient	B
based	O
training	B
em	O
style	O
algorithms	O
are	O
easy	O
to	O
derive	O
and	O
implement	O
but	O
can	O
exhibit	O
poor	O
convergence	O
.	O
gradient	B
based	O
methods	O
to	O
simultaneously	O
optimize	O
with	O
respect	O
to	O
the	O
basis	O
and	O
the	O
components	O
have	O
been	O
developed	O
,	O
but	O
require	O
a	O
parameterisation	B
that	O
ensures	O
positivity	O
of	O
the	O
solutions	O
[	O
217	O
]	O
.	O
array	O
decompositions	O
it	O
is	O
straightforward	O
to	O
extend	O
the	O
method	O
to	O
the	O
decomposition	B
of	O
multidimensional	O
arrays	O
,	O
based	O
also	O
on	O
more	O
than	O
one	O
basis	O
.	O
for	O
example	O
˜p	O
(	O
s	O
,	O
t|u	O
,	O
v	O
)	O
˜p	O
(	O
u|w	O
)	O
˜p	O
(	O
v	O
)	O
˜p	O
(	O
w	O
)	O
(	O
15.6.21	O
)	O
(	O
cid:88	O
)	O
v	O
,	O
w	O
p	O
(	O
s	O
,	O
t	O
,	O
u	O
)	O
≈	O
˜p	O
(	O
s	O
,	O
t	O
,	O
u|v	O
,	O
w	O
)	O
˜p	O
(	O
v	O
,	O
w	O
)	O
=	O
(	O
cid:88	O
)	O
v	O
,	O
w	O
such	O
extensions	O
require	O
only	O
additional	O
bookkeeping	O
.	O
15.6.3	O
applications	O
of	O
plsa/nmf	O
physical	O
models	O
non-negative	O
decompositions	O
can	O
arise	O
naturally	O
in	O
certain	O
physical	O
situations	O
.	O
for	O
example	O
,	O
in	O
acoustics	O
,	O
positive	O
amounts	O
of	O
energy	B
combine	O
linearly	O
from	O
diﬀerent	O
signal	O
sources	O
to	O
form	O
the	O
observed	O
signal	O
.	O
let	O
’	O
s	O
imagine	O
that	O
two	O
kinds	O
of	O
signals	O
are	O
present	O
in	O
an	O
acoustic	O
signal	O
,	O
say	O
a	O
piano	O
and	O
a	O
singer	O
.	O
using	O
nmf	O
one	O
can	O
learn	O
two	O
separate	O
bases	O
for	O
these	O
cases	O
,	O
and	O
then	O
reconstruct	O
a	O
given	O
signal	O
using	O
only	O
one	O
of	O
the	O
bases	O
.	O
this	O
means	O
that	O
one	O
could	O
potentially	O
remove	O
the	O
singer	O
from	O
a	O
recording	O
,	O
leaving	O
only	O
the	O
piano	O
.	O
see	O
also	O
[	O
285	O
]	O
for	O
a	O
more	O
standard	O
probabilistic	O
model	B
in	O
acoustics	O
.	O
this	O
would	O
be	O
analogous	O
to	O
reconstructing	O
the	O
images	O
in	O
ﬁg	O
(	O
15.15a	O
)	O
using	O
say	O
only	O
one	O
of	O
the	O
learned	O
basis	O
images	O
,	O
see	O
example	O
(	O
73	O
)	O
.	O
modelling	B
citations	O
we	O
have	O
a	O
collection	O
of	O
research	O
documents	O
which	O
cite	O
other	O
documents	O
.	O
for	O
example	O
,	O
document	O
1	O
might	O
cite	O
documents	O
3	O
,	O
2	O
,	O
10	O
,	O
etc	O
.	O
given	O
only	O
the	O
list	O
of	O
citations	O
for	O
each	O
document	O
,	O
can	O
we	O
identify	O
key	O
research	O
papers	O
and	O
the	O
communities	O
that	O
cite	O
them	O
?	O
note	O
that	O
this	O
is	O
not	O
the	O
same	O
question	O
as	O
ﬁnding	O
the	O
most	O
cited	O
documents	O
–	O
rather	O
we	O
want	O
to	O
identify	O
documents	O
with	O
communities	O
and	O
ﬁnd	O
their	O
relevance	O
for	O
a	O
296	O
draft	O
march	O
9	O
,	O
2010	O
matrix	B
decomposition	O
methods	O
(	O
reinforcement	B
learning	I
)	O
learning	B
to	O
predict	O
by	O
the	O
methods	O
of	O
temporal	O
diﬀerences	O
.	O
sutton	O
.	O
neuronlike	O
adaptive	O
elements	O
that	O
can	O
solve	O
diﬃcult	O
learning	B
control	O
problems	O
.	O
barto	O
et	O
al	O
.	O
practical	O
issues	O
in	O
temporal	O
diﬀerence	O
learning	B
.	O
tesauro	O
.	O
(	O
rule	O
learning	B
)	O
explanation-based	O
generalization	O
:	O
a	O
unifying	O
view	O
.	O
mitchell	O
et	O
al	O
.	O
learning	B
internal	O
representations	O
by	O
error	O
propagation	O
.	O
rumelhart	O
et	O
al	O
.	O
explanation-based	O
learning	B
:	O
an	O
alternative	O
view	O
.	O
dejong	O
et	O
al	O
.	O
(	O
neural	O
networks	O
)	O
learning	B
internal	O
representations	O
by	O
error	O
propagation	O
.	O
rumelhart	O
et	O
al	O
.	O
neural	O
networks	O
and	O
the	O
bias-variance	O
dilemma	O
.	O
geman	O
et	O
al	O
.	O
the	O
cascade-correlation	O
learning	B
architecture	O
.	O
fahlman	O
et	O
al	O
.	O
(	O
theory	O
)	O
classiﬁcation	B
and	O
regression	B
trees	O
.	O
breiman	O
et	O
al	O
.	O
learnability	O
and	O
the	O
vapnik-chervonenkis	O
dimension	O
.	O
blumer	O
et	O
al	O
.	O
learning	B
quickly	O
when	O
irrelevant	O
attributes	O
abound	O
.	O
littlestone	O
.	O
(	O
probabilistic	B
reasoning	O
)	O
probabilistic	B
reasoning	O
in	O
intelligent	O
systems	O
:	O
networks	O
of	O
plausible	O
inference	B
.	O
pearl	O
.	O
factor	B
1	O
0.0108	O
0.0066	O
0.0065	O
factor	B
2	O
0.0038	O
0.0037	O
0.0036	O
factor	B
3	O
0.0120	O
0.0061	O
0.0049	O
factor	B
4	O
0.0093	O
0.0066	O
0.0055	O
factor	B
5	O
0.0118	O
0.0094	O
maximum	B
likelihood	I
from	O
incomplete	O
data	B
via	O
the	O
em	O
algorithm	B
.	O
dempster	O
et	O
al	O
.	O
0.0056	O
factor	B
6	O
0.0157	O
0.0132	O
0.0096	O
factor	B
7	O
0.0063	O
0.0054	O
0.0033	O
local	B
computations	O
with	O
probabilities	O
on	O
graphical	O
structures	O
.	O
lauritzen	O
et	O
al	O
.	O
(	O
genetic	O
algorithms	O
)	O
genetic	O
algorithms	O
in	O
search	O
,	O
optimization	O
,	O
and	O
machine	O
learning	B
.	O
goldberg	O
.	O
adaptation	O
in	O
natural	B
and	O
artiﬁcial	O
systems	O
.	O
holland	O
.	O
genetic	O
programming	O
:	O
on	O
the	O
programming	O
of	O
computers	O
by	O
means	O
of	O
natural	B
selection	O
.	O
koza	O
.	O
(	O
logic	B
)	O
eﬃcient	B
induction	O
of	O
logic	B
programs	O
.	O
muggleton	O
et	O
al	O
.	O
learning	B
logical	O
deﬁnitions	O
from	O
relations	O
.	O
quinlan	O
.	O
inductive	O
logic	B
programming	O
techniques	O
and	O
applications	O
.	O
lavrac	O
et	O
al	O
.	O
table	O
15.1	O
:	O
highest	O
ranked	O
documents	O
according	O
to	O
p	O
(	O
c|z	O
)	O
.	O
the	O
factor	B
topic	O
labels	O
are	O
manual	O
assignments	O
based	O
on	O
similarities	O
to	O
the	O
cora	O
topics	O
.	O
reproduced	O
from	O
[	O
63	O
]	O
.	O
community	O
.	O
we	O
use	O
the	O
variable	B
d	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
}	O
to	O
index	O
documents	O
and	O
c	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
}	O
to	O
index	O
citations	O
(	O
both	O
d	O
and	O
c	O
have	O
the	O
same	O
domain	B
,	O
namely	O
the	O
index	O
of	O
a	O
research	O
article	O
)	O
.	O
if	O
document	O
d	O
=	O
i	O
cites	O
article	O
c	O
=	O
j	O
then	O
we	O
set	O
the	O
entry	O
of	O
the	O
matrix	B
cij	O
=	O
1.	O
if	O
there	O
is	O
no	O
citation	O
,	O
cij	O
is	O
set	O
to	O
zero	O
.	O
we	O
can	O
form	O
a	O
‘	O
distribution	B
’	O
over	O
documents	O
and	O
citations	O
using	O
p	O
(	O
d	O
=	O
i	O
,	O
c	O
=	O
j	O
)	O
=	O
cij	O
(	O
cid:80	O
)	O
ij	O
cij	O
(	O
15.6.22	O
)	O
example	O
74	O
(	O
modelling	B
citations	O
)	O
.	O
the	O
cora	O
corpus	O
(	O
www.cs.umass.edu/∼mccallum	O
)	O
contains	O
an	O
archive	O
of	O
around	O
30,000	O
computer	O
science	O
research	O
papers	O
.	O
from	O
this	O
archive	O
the	O
authors	O
in	O
[	O
63	O
]	O
extracted	O
the	O
papers	O
in	O
the	O
machine	O
learning	B
category	O
,	O
consisting	O
of	O
4220	O
documents	O
and	O
38,372	O
citations	O
.	O
using	O
these	O
the	O
distribution	B
equation	O
(	O
15.6.22	O
)	O
was	O
formed	O
.	O
the	O
documents	O
have	O
additionally	O
been	O
categorised	O
by	O
hand	O
into	O
7	O
topics	O
:	O
case-based	O
reasoning	O
,	O
genetic	O
algorithms	O
,	O
neural	O
networks	O
,	O
probabilistic	B
methods	O
,	O
reinforcement	B
learning	I
,	O
rule	O
learning	B
and	O
theory	O
.	O
in	O
[	O
63	O
]	O
the	O
joint	B
plsa	O
method	O
is	O
ﬁtted	O
to	O
the	O
data	B
using	O
z	O
=	O
7	O
topics	O
.	O
from	O
the	O
trained	O
model	B
the	O
expression	O
p	O
(	O
c	O
=	O
j|z	O
=	O
k	O
)	O
deﬁnes	O
how	O
authoritative	O
paper	O
j	O
is	O
according	O
to	O
community	O
z	O
=	O
k.	O
the	O
results	O
are	O
presented	O
in	O
table	O
(	O
15.1	O
)	O
and	O
show	O
how	O
the	O
method	O
discovers	O
intuitively	O
meaningful	O
topics	O
.	O
modelling	B
the	O
web	B
consider	O
a	O
collection	O
of	O
websites	O
,	O
indexed	O
by	O
i.	O
if	O
website	B
j	O
points	O
to	O
website	B
i	O
,	O
one	O
sets	O
cij	O
=	O
1	O
giving	O
a	O
directed	B
graph	O
of	O
website-to-website	O
links	O
.	O
since	O
a	O
website	B
will	O
discuss	O
usually	O
only	O
of	O
a	O
small	O
number	O
of	O
draft	O
march	O
9	O
,	O
2010	O
297	O
‘	O
topics	O
’	O
we	O
might	O
be	O
able	O
to	O
explain	O
why	O
there	O
is	O
a	O
link	O
between	O
two	O
websites	O
using	O
a	O
plsa	O
decomposition	B
.	O
these	O
algorithms	O
have	O
proved	O
useful	O
for	O
internet	O
search	O
for	O
example	O
to	O
determine	O
the	O
latent	B
topics	O
of	O
websites	O
and	O
identify	O
the	O
most	O
authoritative	O
websites	O
.	O
see	O
[	O
64	O
]	O
for	O
a	O
discussion	O
.	O
kernel	B
pca	O
15.7	O
kernel	B
pca	O
kernel	B
pca	O
is	O
a	O
non-linear	B
extension	O
of	O
pca	O
designed	O
to	O
discover	O
non-linear	B
manifolds	O
.	O
here	O
we	O
only	O
brieﬂy	O
describe	O
the	O
approach	B
and	O
refer	O
the	O
reader	O
to	O
[	O
242	O
]	O
for	O
details	O
.	O
in	O
kernel	B
pca	O
,	O
we	O
replace	O
each	O
x	O
by	O
a	O
‘	O
feature	O
’	O
vector	O
˜x	O
≡	O
φ	O
(	O
x	O
)	O
.	O
note	O
that	O
the	O
use	O
of	O
˜x	O
here	O
does	O
not	O
have	O
the	O
interpretation	O
we	O
used	O
before	O
as	O
the	O
approximate	B
reconstruction	O
.	O
the	O
feature	B
map	I
φ	O
takes	O
a	O
vector	O
x	O
and	O
produces	O
a	O
higher	O
dimensional	O
vector	O
˜x	O
.	O
for	O
example	O
we	O
could	O
map	B
a	O
two	O
dimensional	O
vector	O
x	O
=	O
[	O
x1	O
,	O
x2	O
]	O
t	O
using	O
φ	O
(	O
x	O
)	O
=	O
(	O
cid:2	O
)	O
x1	O
,	O
x2	O
,	O
x2	O
1	O
,	O
.	O
.	O
.	O
(	O
cid:3	O
)	O
t	O
1	O
,	O
x2	O
2	O
,	O
x1x2	O
,	O
x3	O
(	O
15.7.1	O
)	O
the	O
idea	O
is	O
then	O
to	O
perform	O
pca	O
on	O
these	O
higher	O
dimensional	O
feature	O
vectors	O
,	O
subsequently	O
mapping	O
back	O
the	O
eigenvectors	O
to	O
the	O
original	O
space	O
x.	O
the	O
main	O
challenge	O
is	O
to	O
write	O
this	O
without	O
explicitly	O
computing	O
pca	O
in	O
the	O
potentially	O
very	O
high	O
dimensional	O
feature	O
vector	O
space	O
.	O
as	O
a	O
reminder	O
,	O
in	O
standard	O
pca	O
,	O
for	O
zero	O
mean	B
data	O
,	O
one	O
forms	O
an	O
eigen-decomposition	O
of	O
the	O
sample	B
matrix3	O
s	O
=	O
1	O
n	O
˜x	O
˜xt	O
(	O
15.7.2	O
)	O
for	O
simplicity	O
,	O
we	O
concentrate	O
here	O
on	O
ﬁnding	O
the	O
ﬁrst	O
principal	O
component	O
˜e	O
which	O
satisﬁes	O
˜x	O
˜xt˜e	O
=	O
λ	O
(	O
cid:48	O
)	O
˜e	O
(	O
15.7.3	O
)	O
for	O
corresponding	O
eigenvalue	O
λ	O
(	O
writing	O
λ	O
(	O
cid:48	O
)	O
=	O
n	O
λ	O
)	O
.	O
the	O
‘	O
dual	B
’	O
representation	B
is	O
obtained	O
by	O
pre-multiplying	O
by	O
˜xt	O
,	O
so	O
that	O
in	O
terms	O
of	O
˜f	O
≡	O
˜xt˜e	O
,	O
the	O
standard	O
pca	O
eigen-problem	O
reduces	O
to	O
solving	B
:	O
(	O
15.7.4	O
)	O
(	O
15.7.5	O
)	O
(	O
15.7.6	O
)	O
˜xt	O
˜x˜f	O
=	O
λ	O
(	O
cid:48	O
)	O
˜f	O
(	O
cid:48	O
)	O
˜e	O
˜x˜f	O
=	O
λ	O
(	O
cid:104	O
)	O
˜xt	O
˜x	O
(	O
cid:105	O
)	O
(	O
cid:104	O
)	O
˜xt	O
˜x	O
(	O
cid:105	O
)	O
n	O
(	O
cid:88	O
)	O
n=1	O
yi	O
=	O
1	O
n	O
λi	O
the	O
feature	O
eigenvector	O
˜e	O
is	O
then	O
recovered	O
using	O
we	O
note	O
that	O
matrix	B
˜xt	O
˜x	O
has	O
elements	O
=	O
φ	O
(	O
xm	O
)	O
tφ	O
(	O
xn	O
)	O
mn	O
and	O
recognise	O
this	O
as	O
the	O
scalar	B
product	I
between	O
vectors	O
.	O
this	O
means	O
that	O
the	O
matrix	B
is	O
positive	O
(	O
semi	O
)	O
deﬁnite	O
and	O
we	O
may	O
equivalently	O
use	O
a	O
positive	B
deﬁnite	I
kernel	O
,	O
see	O
section	O
(	O
19.3	O
)	O
,	O
=	O
k	O
(	O
xm	O
,	O
xn	O
)	O
=	O
kmn	O
mn	O
(	O
15.7.7	O
)	O
then	O
equation	B
(	O
15.7.4	O
)	O
can	O
be	O
written	O
as	O
k˜f	O
=	O
λ	O
(	O
cid:48	O
)	O
˜f	O
(	O
15.7.8	O
)	O
one	O
then	O
solves	O
this	O
eigen-equation	O
to	O
ﬁnd	O
the	O
n	O
dimensional	O
principal	O
dual	O
feature	O
vector	O
˜f	O
.	O
the	O
projection	B
of	O
the	O
feature	O
˜x	O
is	O
given	O
by	O
y	O
=	O
˜xt˜e	O
=	O
˜xt	O
˜x˜f	O
1	O
λ	O
(	O
15.7.9	O
)	O
more	O
generally	O
,	O
for	O
a	O
larger	O
number	O
of	O
components	O
,	O
the	O
ith	O
kernel	B
pca	O
projection	B
yi	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
kernel	B
directly	O
as	O
k	O
(	O
x	O
,	O
xn	O
)	O
˜f	O
i	O
n	O
(	O
15.7.10	O
)	O
3we	O
use	O
the	O
normalisation	B
n	O
as	O
opposed	O
to	O
n	O
−	O
1	O
just	O
for	O
notational	O
convenience	O
–	O
in	O
practice	O
,	O
there	O
is	O
little	O
diﬀerence	O
.	O
298	O
draft	O
march	O
9	O
,	O
2010	O
kernel	B
pca	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
15.16	O
:	O
canonical	B
correlation	I
analysis	I
.	O
(	O
a	O
)	O
:	O
training	B
data	O
.	O
the	O
top	O
panel	O
contains	O
the	O
x	O
matrix	B
of	O
1000	O
,	O
15	O
dimensional	O
points	O
,	O
and	O
the	O
bottom	O
the	O
corresponding	O
30	O
dimensional	O
y	O
matrix	B
.	O
(	O
b	O
)	O
:	O
the	O
data	B
in	O
(	O
a	O
)	O
was	O
produced	O
using	O
x	O
=	O
ah	O
,	O
y	O
=	O
bh	O
where	O
a	O
is	O
a	O
15	O
×	O
1	O
matrix	O
,	O
and	O
b	O
is	O
a	O
30	O
×	O
1	O
matrix	O
.	O
(	O
c	O
)	O
:	O
matrices	O
a	O
and	O
b	O
learned	O
by	O
cca	O
.	O
note	O
that	O
they	O
are	O
close	O
to	O
the	O
true	O
a	O
and	O
b	O
up	O
to	O
rescaling	O
and	O
sign	O
changes	O
.	O
see	O
democca.m	O
.	O
where	O
i	O
is	O
the	O
eigenvalue	O
label	O
.	O
the	O
above	O
derivation	O
implicitly	O
assumed	O
zero	O
mean	B
features	O
˜x	O
.	O
even	O
if	O
the	O
original	O
data	B
x	O
is	O
zero	O
mean	B
,	O
due	O
to	O
the	O
non-linear	B
mapping	O
,	O
the	O
features	O
may	O
not	O
be	O
zero	O
mean	B
.	O
to	O
correct	O
for	O
this	O
one	O
may	O
show	O
that	O
the	O
only	O
modiﬁcation	O
required	O
is	O
to	O
replace	O
the	O
matrix	B
k	O
in	O
equation	B
(	O
15.7.8	O
)	O
above	O
with	O
(	O
cid:48	O
)	O
mn	O
=	O
k	O
(	O
xm	O
,	O
xn	O
)	O
−	O
k	O
1	O
n	O
k	O
(	O
xd	O
,	O
xn	O
)	O
−	O
1	O
n	O
k	O
(	O
xm	O
,	O
xd	O
)	O
+	O
1	O
n	O
2	O
k	O
(	O
xd	O
(	O
cid:48	O
)	O
,	O
xd	O
)	O
(	O
15.7.11	O
)	O
n	O
(	O
cid:88	O
)	O
d=1	O
n	O
(	O
cid:88	O
)	O
d=1	O
n	O
(	O
cid:88	O
)	O
d=1	O
,	O
d	O
(	O
cid:48	O
)	O
=1	O
finding	O
the	O
reconstructions	O
the	O
above	O
gives	O
a	O
procedure	O
for	O
ﬁnding	O
the	O
kpca	O
projection	B
y.	O
however	O
,	O
in	O
many	O
cases	O
we	O
would	O
also	O
like	O
to	O
have	O
an	O
approximate	B
reconstruction	O
using	O
the	O
lower	O
dimensional	O
y.	O
this	O
is	O
not	O
straightforward	O
since	O
the	O
mapping	O
from	O
y	O
to	O
x	O
is	O
in	O
general	O
highly	O
non-linear	B
.	O
here	O
we	O
outline	O
a	O
procedure	O
for	O
achieving	O
this	O
.	O
first	O
we	O
ﬁnd	O
the	O
reconstruction	O
˜x∗	O
of	O
the	O
feature	O
space	O
˜x	O
.	O
now	O
˜f	O
n	O
i	O
φ	O
(	O
xn	O
)	O
(	O
15.7.12	O
)	O
˜x∗	O
=	O
(	O
cid:88	O
)	O
yi˜ei	O
=	O
(	O
cid:88	O
)	O
i	O
i	O
yi	O
1	O
λi	O
(	O
cid:88	O
)	O
n	O
given	O
˜x∗	O
we	O
try	O
to	O
ﬁnd	O
that	O
point	O
x	O
(	O
cid:48	O
)	O
in	O
the	O
original	O
data	B
space	O
that	O
maps	O
to	O
˜x∗	O
.	O
this	O
can	O
be	O
found	O
by	O
minimising	O
e	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
(	O
cid:0	O
)	O
φ	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
−	O
˜x∗	O
(	O
cid:1	O
)	O
2	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
−	O
2	O
(	O
cid:88	O
)	O
e	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
k	O
(	O
x	O
(	O
cid:48	O
)	O
(	O
cid:88	O
)	O
yi	O
λi	O
i	O
n	O
up	O
to	O
negligable	O
constants	O
this	O
is	O
i	O
k	O
(	O
xn	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
˜f	O
n	O
one	O
then	O
ﬁnds	O
x	O
(	O
cid:48	O
)	O
by	O
minimising	O
e	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
numerically	O
.	O
.	O
draft	O
march	O
9	O
,	O
2010	O
(	O
15.7.13	O
)	O
(	O
15.7.14	O
)	O
299	O
training	B
data2004006008001000510152004006008001000102030051015−202true	O
model0102030−202051015−0.200.2learned	O
model0102030−0.200.2	O
canonical	B
correlation	I
analysis	I
15.8	O
canonical	B
correlation	I
analysis	I
consider	O
x	O
and	O
y	O
which	O
have	O
dimensions	O
dim	O
(	O
x	O
)	O
and	O
dim	O
(	O
y	O
)	O
respectively	O
.	O
for	O
example	O
x	O
might	O
represent	O
a	O
segment	O
of	O
video	O
and	O
y	O
the	O
corresponding	O
audio	O
.	O
given	O
then	O
a	O
collection	O
(	O
xn	O
,	O
yn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
an	O
interesting	O
challenge	O
is	O
to	O
identify	O
which	O
parts	O
of	O
the	O
audio	O
and	O
video	O
ﬁles	O
are	O
strongly	O
correlated	O
.	O
one	O
might	O
expect	O
,	O
for	O
example	O
,	O
that	O
the	O
mouth	O
region	O
of	O
the	O
video	O
is	O
strongly	O
correlated	O
with	O
the	O
audio	O
.	O
one	O
way	O
to	O
achieve	O
this	O
is	O
to	O
project	O
each	O
x	O
and	O
y	O
to	O
one	O
dimension	O
using	O
atx	O
and	O
bty	O
such	O
that	O
the	O
correlation	B
between	O
the	O
projections	O
is	O
maximal	O
.	O
the	O
unnormalised	O
correlation	B
between	O
the	O
projections	O
atx	O
and	O
bty	O
is	O
(	O
cid:88	O
)	O
(	O
cid:34	O
)	O
(	O
cid:88	O
)	O
(	O
cid:35	O
)	O
atxnbtyn	O
=	O
at	O
xnynt	O
b	O
n	O
n	O
and	O
the	O
normalised	O
correlation	O
is	O
(	O
cid:112	O
)	O
atsxxa	O
(	O
cid:112	O
)	O
btsyyb	O
atsxyb	O
(	O
15.8.1	O
)	O
(	O
15.8.2	O
)	O
(	O
15.8.4	O
)	O
(	O
15.8.5	O
)	O
(	O
15.8.6	O
)	O
where	O
sxy	O
is	O
the	O
sample	B
x	O
,	O
y	O
cross	B
correlation	O
matrix	B
.	O
when	O
the	O
joint	B
covariance	O
of	O
the	O
stacked	O
vectors	O
zn	O
=	O
[	O
xn	O
,	O
yn	O
]	O
is	O
considered	O
sxx	O
,	O
sxy	O
,	O
syx	O
,	O
syy	O
are	O
the	O
blocks	O
of	O
the	O
joint	B
covariance	O
matrix	B
.	O
since	O
equation	B
(	O
15.8.2	O
)	O
is	O
invariant	O
with	O
respect	O
to	O
length	O
scaling	O
of	O
a	O
and	O
also	O
b	O
,	O
we	O
can	O
consider	O
the	O
equivalent	B
objective	O
e	O
(	O
a	O
,	O
b	O
)	O
=	O
atsxyb	O
(	O
15.8.3	O
)	O
subject	O
to	O
atsxxa	O
=	O
1	O
and	O
btsyyb	O
=	O
1.	O
to	O
ﬁnd	O
the	O
optimal	O
projections	O
a	O
,	O
b	O
,	O
under	O
the	O
constraints	O
,	O
we	O
use	O
the	O
lagrangian	O
,	O
l	O
(	O
a	O
,	O
b	O
,	O
λa	O
,	O
λb	O
)	O
≡	O
atsxyb	O
+	O
λa	O
2	O
1	O
−	O
atsxxa	O
from	O
which	O
we	O
obtain	O
the	O
zero	O
derivative	O
criteria	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
1	O
−	O
btsyyb	O
+	O
λb	O
2	O
sxyb	O
=	O
λasxxa	O
,	O
syxa	O
=	O
λbsyyb	O
hence	O
atsxyb	O
=	O
λaatsxxa	O
=	O
λa	O
,	O
btsyxa	O
=	O
λbbtsyyb	O
=	O
λb	O
since	O
atsxyb	O
=	O
btsyxa	O
we	O
must	O
have	O
λa	O
=	O
λb	O
=	O
λ	O
at	O
the	O
optimum	O
.	O
if	O
we	O
assume	O
that	O
syy	O
is	O
invertible	O
,	O
b	O
=	O
s−1	O
yy	O
syxa	O
1	O
λ	O
using	O
this	O
to	O
eliminate	O
b	O
in	O
equation	B
(	O
15.8.5	O
)	O
we	O
have	O
sxys−1	O
yy	O
syxa	O
=	O
λ2sxxa	O
which	O
is	O
a	O
generalised	B
eigen-problem	O
.	O
assuming	O
that	O
sxx	O
is	O
invertible	O
we	O
can	O
equivalently	O
write	O
xx	O
sxys−1	O
s−1	O
yy	O
syxa	O
=	O
λ2a	O
(	O
15.8.7	O
)	O
(	O
15.8.8	O
)	O
(	O
15.8.9	O
)	O
which	O
is	O
a	O
standard	O
eigen-problem	O
(	O
albeit	O
with	O
λ2	O
as	O
the	O
eigenvalue	O
)	O
.	O
once	O
this	O
is	O
solved	O
we	O
can	O
ﬁnd	O
b	O
using	O
equation	B
(	O
15.8.7	O
)	O
.	O
300	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
15.8.1	O
svd	O
formulation	O
it	O
is	O
straightforward	O
to	O
show	O
that	O
we	O
can	O
ﬁnd	O
a	O
by	O
ﬁrst	O
computing	O
the	O
svd	O
of	O
−	O
1	O
xx	O
sxys	O
2	O
−	O
1	O
2	O
yy	O
s	O
(	O
15.8.10	O
)	O
in	O
the	O
form	O
udvt	O
and	O
extracting	O
the	O
maximal	O
singular	B
vector	O
u1	O
of	O
u	O
(	O
the	O
ﬁrst	O
column	O
on	O
u	O
)	O
.	O
then	O
a	O
−	O
1	O
yy	O
v1	O
,	O
where	O
v1	O
is	O
the	O
ﬁrst	O
column	O
of	O
v.	O
in	O
this	O
way	O
,	O
is	O
optimally	O
s	O
the	O
extension	O
to	O
ﬁnding	O
m	O
multiple	O
directions	O
a	O
=	O
(	O
cid:2	O
)	O
a1	O
,	O
.	O
.	O
.	O
,	O
am	O
(	O
cid:3	O
)	O
and	O
b	O
=	O
(	O
cid:2	O
)	O
b1	O
,	O
.	O
.	O
.	O
,	O
bm	O
(	O
cid:3	O
)	O
is	O
clear	O
–	O
one	O
−	O
1	O
xx	O
u1	O
,	O
and	O
similarly	O
,	O
b	O
is	O
optimally	O
s	O
2	O
2	O
takes	O
the	O
corresponding	O
ﬁrst	O
m	O
singular	B
values	O
accordingly	O
.	O
doing	O
so	O
maximises	O
the	O
criterion	O
trace	O
(	O
cid:0	O
)	O
atsxyb	O
(	O
cid:1	O
)	O
(	O
cid:112	O
)	O
trace	O
(	O
atsxxa	O
)	O
(	O
cid:112	O
)	O
trace	O
(	O
btsyyb	O
)	O
(	O
15.8.11	O
)	O
this	O
approach	B
is	O
taken	O
in	O
cca.m	O
–	O
see	O
ﬁg	O
(	O
15.16	O
)	O
for	O
a	O
demonstration	O
.	O
one	O
can	O
also	O
show	O
that	O
cca	O
cor-	O
responds	O
to	O
the	O
probabilistic	B
factor	O
analysis	B
model	O
under	O
a	O
block	O
restriction	O
on	O
the	O
form	O
of	O
the	O
factor	B
loadings	O
,	O
see	O
section	O
(	O
21.2.1	O
)	O
.	O
cca	O
and	O
related	O
kernel	B
extensions	O
have	O
been	O
applied	O
in	O
machine	O
learning	B
contexts	O
,	O
for	O
example	O
to	O
model	B
the	O
correlation	B
between	O
images	O
and	O
text	O
in	O
order	O
to	O
improve	O
image	O
retrieval	O
from	O
text	O
queries	O
,	O
see	O
[	O
126	O
]	O
.	O
15.9	O
notes	O
pca	O
is	O
also	O
known	O
as	O
the	O
karhunen-lo`eve	O
decomposition	B
,	O
particularly	O
in	O
the	O
engineering	O
literature	O
.	O
15.10	O
code	O
pca.m	O
:	O
principal	O
components	O
analysis	B
demolsi.m	O
:	O
demo	O
of	O
latent	O
semantic	O
indexing/analysis	O
svdm.m	O
:	O
singular	B
value	O
decomposition	B
with	O
missing	B
data	I
demosvdmissing.m	O
:	O
demo	O
svd	O
with	O
missing	O
data	B
plsa.m	O
:	O
probabilistic	B
latent	I
semantic	I
analysis	I
plsacond.m	O
:	O
conditional	B
probabilistic	O
latent	B
semantic	I
analysis	I
demoplsa.m	O
:	O
demo	O
of	O
plsa	O
demomultnomialpxygz.m	O
:	O
demo	O
of	O
‘	O
ﬁnite	O
sample	O
’	O
plsa	O
cca.m	O
:	O
canonical	B
correlation	I
analysis	I
(	O
cca	O
)	O
democca.m	O
:	O
demo	O
of	O
canonical	B
correlation	I
analysis	I
15.11	O
exercises	O
exercise	O
161.	O
consider	O
a	O
dataset	O
in	O
two	O
dimensions	O
where	O
the	O
data	B
lies	O
on	O
the	O
circumference	O
of	O
a	O
circle	O
of	O
unit	O
radius	O
.	O
what	O
would	O
be	O
the	O
eﬀect	O
of	O
using	O
pca	O
on	O
this	O
dataset	O
,	O
in	O
which	O
we	O
attempt	O
to	O
reduce	O
the	O
dimensionality	O
to	O
1	O
?	O
suggest	O
an	O
alternative	O
one	O
dimensional	O
representation	B
of	O
the	O
data	B
.	O
exercise	O
162.	O
consider	O
two	O
vectors	O
xa	O
and	O
xb	O
and	O
their	O
corresponding	O
pca	O
approximations	O
c+	O
(	O
cid:80	O
)	O
m	O
and	O
c	O
+	O
(	O
cid:80	O
)	O
m	O
i=1	O
aiei	O
i=1	O
biei	O
,	O
where	O
the	O
eigenvectors	O
ei	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
are	O
mutually	O
orthogonal	B
and	O
have	O
unit	O
length	O
.	O
the	O
eigenvector	O
ei	O
has	O
corresponding	O
eigenvalue	O
λi	O
.	O
approximate	B
(	O
xa	O
−	O
xb	O
)	O
2	O
by	O
using	O
the	O
pca	O
represen-	O
tations	O
of	O
the	O
data	B
,	O
and	O
show	O
that	O
this	O
is	O
equal	O
to	O
(	O
a	O
−	O
b	O
)	O
2.	O
exercise	O
163.	O
show	O
how	O
the	O
solution	O
for	O
a	O
to	O
the	O
cca	O
problem	B
in	O
equation	B
(	O
15.8.8	O
)	O
can	O
be	O
transformed	O
into	O
the	O
form	O
expressed	O
by	O
equation	B
(	O
15.8.10	O
)	O
,	O
as	O
claimed	O
in	O
the	O
text	O
.	O
draft	O
march	O
9	O
,	O
2010	O
301	O
exercises	O
(	O
15.11.3	O
)	O
(	O
15.11.4	O
)	O
(	O
cid:16	O
)	O
xa	O
−	O
xb	O
(	O
cid:17	O
)	O
t	O
s−1	O
(	O
cid:16	O
)	O
xa	O
−	O
xb	O
(	O
cid:17	O
)	O
exercise	O
164.	O
let	O
s	O
be	O
the	O
covariance	B
matrix	O
of	O
the	O
data	B
.	O
the	O
mahalanobis	O
distance	O
between	O
xa	O
and	O
xb	O
is	O
deﬁned	O
as	O
.	O
(	O
15.11.1	O
)	O
explain	O
how	O
to	O
approximate	B
this	O
distance	O
using	O
m-dimensional	O
pca	O
approximations	O
.	O
exercise	O
165	O
(	O
pca	O
with	O
external	O
inputs	O
)	O
.	O
in	O
some	O
applications	O
,	O
one	O
may	O
suspect	O
that	O
certain	O
external	O
variables	O
v	O
have	O
a	O
strong	B
inﬂuence	O
on	O
how	O
the	O
data	B
x	O
is	O
distributed	O
.	O
for	O
example	O
,	O
if	O
x	O
represents	O
an	O
image	O
,	O
it	O
might	O
be	O
that	O
we	O
know	O
the	O
lighting	O
condition	O
v	O
under	O
which	O
the	O
image	O
was	O
made	O
–	O
this	O
will	O
have	O
a	O
large	O
eﬀect	O
on	O
the	O
image	O
.	O
it	O
would	O
make	O
sense	O
therefore	O
to	O
include	O
the	O
known	O
lighting	O
condition	O
in	O
forming	O
a	O
lower	O
dimensional	O
representation	O
of	O
the	O
image	O
.	O
note	O
that	O
we	O
don	O
’	O
t	O
want	O
to	O
form	O
a	O
lower	O
dimensional	O
representation	O
of	O
the	O
joint	B
x	O
,	O
v	O
,	O
rather	O
we	O
want	O
to	O
form	O
a	O
lower	O
dimensional	O
representation	O
of	O
x	O
alone	O
,	O
bearing	O
in	O
mind	O
that	O
some	O
of	O
the	O
variability	O
observed	O
may	O
be	O
due	O
to	O
v.	O
we	O
therefore	O
assume	O
an	O
approximation	B
k	O
ck	O
vn	O
(	O
15.11.2	O
)	O
j	O
k	O
where	O
the	O
coeﬃcients	O
yn	O
i	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
and	O
basis	O
vectors	O
bj	O
,	O
j	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
j	O
and	O
ck	O
,	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
are	O
to	O
be	O
determined	O
.	O
the	O
external	O
inputs	O
v1	O
,	O
.	O
.	O
.	O
,	O
vn	O
are	O
given	O
.	O
the	O
sum	O
squared	O
error	O
loss	O
between	O
the	O
xn	O
and	O
their	O
linear	B
reconstruction	O
equation	B
(	O
15.11.2	O
)	O
is	O
(	O
cid:88	O
)	O
j	O
bj	O
+	O
(	O
cid:88	O
)	O
yn	O
xn	O
≈	O
xn	O
e	O
=	O
(	O
cid:88	O
)	O
n	O
,	O
i	O
(	O
cid:88	O
)	O
j	O
2	O
(	O
cid:88	O
)	O
k	O
i	O
−	O
j	O
bj	O
yn	O
i	O
−	O
k	O
ck	O
vn	O
i	O
find	O
the	O
parameters	O
that	O
minimise	O
e.	O
exercise	O
166.	O
consider	O
the	O
following	O
3-dimensional	O
datapoints	O
:	O
(	O
1.3	O
,	O
1.6	O
,	O
2.8	O
)	O
(	O
4.3	O
,	O
−1.4	O
,	O
5.8	O
)	O
(	O
−0.6	O
,	O
3.7	O
,	O
0.7	O
)	O
(	O
−0.4	O
,	O
3.2	O
,	O
5.8	O
)	O
(	O
3.3	O
,	O
−0.4	O
,	O
4.3	O
)	O
(	O
−0.4	O
,	O
3.1	O
,	O
0.9	O
)	O
perform	O
principal	O
components	O
analysis	B
by	O
:	O
1.	O
calculating	O
the	O
mean	B
,	O
c	O
,	O
of	O
the	O
data	B
.	O
2.	O
calculating	O
the	O
covariance	B
matrix	O
s	O
=	O
1	O
6	O
3.	O
finding	O
the	O
eigenvalues	O
and	O
eigenvectors	O
ei	O
of	O
the	O
covariance	B
matrix	O
.	O
(	O
cid:80	O
)	O
6	O
n=1	O
xn	O
(	O
xn	O
)	O
t	O
−	O
cct	O
of	O
the	O
data	B
.	O
you	O
should	O
ﬁnd	O
that	O
only	O
two	O
eigenvalues	O
are	O
large	O
,	O
and	O
therefore	O
that	O
the	O
data	B
can	O
be	O
well	O
represented	O
using	O
two	O
components	O
only	O
.	O
let	O
e1	O
and	O
e2	O
be	O
the	O
two	O
eigenvectors	O
with	O
largest	O
eigenvalues	O
.	O
1.	O
calculate	O
the	O
two	O
dimensional	O
representation	B
of	O
each	O
datapoint	O
(	O
e1·	O
(	O
xn−c	O
)	O
,	O
e2·	O
(	O
xn−c	O
)	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
6	O
.	O
2.	O
calculate	O
the	O
reconstruction	O
of	O
each	O
datapoint	O
c	O
+	O
(	O
e1t	O
(	O
xn	O
−	O
c	O
)	O
)	O
e1	O
+	O
(	O
e2t	O
(	O
xn	O
−	O
c	O
)	O
)	O
e2	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
6.	O
exercise	O
167.	O
consider	O
a	O
‘	O
conditional	B
frequency	O
matrix	B
’	O
p	O
(	O
x	O
=	O
i|y	O
=	O
j	O
)	O
(	O
cid:88	O
)	O
k	O
show	O
how	O
to	O
derive	O
an	O
em	O
style	O
algorithm	B
for	O
an	O
approximate	B
decomposition	O
of	O
this	O
matrix	B
in	O
the	O
form	O
p	O
(	O
x	O
=	O
i|y	O
=	O
j	O
)	O
≈	O
˜p	O
(	O
x	O
=	O
i|z	O
=	O
k	O
)	O
˜p	O
(	O
z	O
=	O
k|y	O
=	O
j	O
)	O
(	O
15.11.6	O
)	O
where	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
z	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
,	O
j	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
.	O
exercise	O
168.	O
for	O
the	O
multinomial	B
model	O
˜p	O
(	O
x	O
,	O
y	O
,	O
z	O
)	O
described	O
in	O
equation	B
(	O
15.6.15	O
)	O
,	O
derive	O
explicitly	O
the	O
em	O
algorithm	B
and	O
implement	O
this	O
in	O
matlab	O
.	O
for	O
randomly	O
chosen	O
values	O
for	O
the	O
conditional	B
proba-	O
bilities	O
,	O
draw	O
10000	O
samples	O
from	O
this	O
model	B
for	O
x	O
=	O
5	O
,	O
y	O
=	O
5	O
,	O
z	O
=	O
4	O
and	O
compute	O
from	O
this	O
the	O
matrix	B
with	O
elements	O
(	O
15.11.5	O
)	O
(	O
15.11.7	O
)	O
(	O
cid:80	O
)	O
x	O
(	O
cid:80	O
)	O
y	O
pij	O
=	O
(	O
cid:93	O
)	O
(	O
x	O
=	O
i	O
,	O
y	O
=	O
j	O
)	O
i=1	O
j=1	O
(	O
cid:93	O
)	O
(	O
x	O
=	O
i	O
,	O
y	O
=	O
j	O
)	O
now	O
run	O
plsa	O
(	O
use	O
plsa.m	O
)	O
with	O
the	O
settings	O
x	O
=	O
5	O
,	O
y	O
=	O
5	O
,	O
z	O
=	O
4	O
to	O
learn	O
and	O
compare	O
your	O
results	O
with	O
those	O
obtained	O
from	O
the	O
ﬁnite	O
sample	O
model	B
equation	O
(	O
15.6.15	O
)	O
.	O
302	O
draft	O
march	O
9	O
,	O
2010	O
chapter	O
16	O
supervised	B
linear	O
dimension	B
reduction	I
16.1	O
supervised	B
linear	O
projections	O
in	O
chapter	O
(	O
15	O
)	O
we	O
discussed	O
dimension	B
reduction	I
using	O
an	O
unsupervised	B
procedure	O
.	O
in	O
cases	O
where	O
class	O
information	O
is	O
available	O
,	O
and	O
our	O
ultimate	O
interest	O
is	O
to	O
reduce	O
dimensionality	O
for	O
improved	O
classiﬁcation	B
,	O
it	O
makes	O
sense	O
to	O
use	O
the	O
available	O
class	O
information	O
in	O
forming	O
the	O
projections	O
.	O
exploiting	O
the	O
class	O
label	O
information	O
to	O
improve	O
the	O
projection	B
is	O
a	O
form	O
of	O
supervised	B
dimension	O
reduction	O
.	O
let	O
’	O
s	O
consider	O
data	B
from	O
two	O
diﬀerent	O
classes	O
.	O
for	O
class	O
1	O
,	O
we	O
have	O
a	O
set	O
of	O
data	B
n1	O
datapoints	O
,	O
(	O
cid:110	O
)	O
(	O
cid:110	O
)	O
(	O
cid:111	O
)	O
(	O
cid:111	O
)	O
x1	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
xn1	O
x1	O
1	O
and	O
similarly	O
for	O
class	O
2	O
,	O
we	O
have	O
a	O
set	O
of	O
n2	O
datapoints	O
x2	O
=	O
2	O
,	O
.	O
.	O
.	O
,	O
xn2	O
x1	O
2	O
our	O
interest	O
is	O
then	O
to	O
ﬁnd	O
a	O
linear	B
projection	O
,	O
y	O
=	O
wtx	O
(	O
16.1.1	O
)	O
(	O
16.1.2	O
)	O
(	O
16.1.3	O
)	O
where	O
dim	O
w	O
=	O
d×l	O
,	O
l	O
<	O
d	O
,	O
such	O
that	O
for	O
datapoints	O
xi	O
,	O
xj	O
in	O
the	O
same	O
class	O
,	O
the	O
distance	O
between	O
their	O
projections	O
yi	O
,	O
yj	O
should	O
be	O
small	O
.	O
conversely	O
,	O
for	O
datapoints	O
in	O
diﬀerent	O
classes	O
,	O
the	O
distance	O
between	O
their	O
projections	O
should	O
be	O
large	O
.	O
this	O
may	O
be	O
useful	O
for	O
classiﬁcation	B
purposes	O
since	O
for	O
a	O
novel	O
point	O
x∗	O
,	O
if	O
its	O
projection	B
y∗	O
=	O
wtx∗	O
(	O
16.1.4	O
)	O
is	O
close	O
to	O
class	O
1	O
projected	O
data	B
,	O
we	O
would	O
expect	O
x∗	O
to	O
belong	O
to	O
class	O
1.	O
in	O
forming	O
the	O
supervised	B
projection	O
,	O
only	O
the	O
class	O
discriminative	B
parts	O
of	O
the	O
data	B
are	O
retained	O
,	O
so	O
that	O
the	O
procedure	O
can	O
be	O
considered	O
a	O
form	O
of	O
supervised	B
feature	O
extraction	O
.	O
16.2	O
fisher	O
’	O
s	O
linear	O
discriminant	O
we	O
restrict	O
attention	O
to	O
binary	O
class	O
data	B
.	O
also	O
,	O
for	O
simplicity	O
,	O
we	O
project	O
the	O
data	B
down	O
to	O
one	O
dimension	O
.	O
the	O
canonical	B
variates	I
algorithm	O
of	O
section	O
(	O
16.3	O
)	O
deals	O
with	O
the	O
generalisations	O
.	O
gaussian	O
assumption	O
we	O
model	B
the	O
data	B
from	O
each	O
class	O
with	O
a	O
gaussian	O
.	O
that	O
is	O
p	O
(	O
x1	O
)	O
=	O
n	O
(	O
x1	O
m1	O
,	O
s1	O
)	O
,	O
p	O
(	O
x2	O
)	O
=	O
n	O
(	O
x2	O
m2	O
,	O
s2	O
)	O
303	O
(	O
16.2.1	O
)	O
fisher	O
’	O
s	O
linear	O
discriminant	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
16.1	O
:	O
the	O
large	O
crosses	O
represent	O
data	B
from	O
class	O
1	O
,	O
and	O
the	O
large	O
circles	O
from	O
class	O
2.	O
their	O
pro-	O
jections	O
onto	O
1	O
dimension	O
are	O
represented	O
by	O
their	O
small	O
counterparts	O
.	O
(	O
a	O
)	O
:	O
fisher	O
’	O
s	O
linear	O
discriminant	O
(	O
b	O
)	O
:	O
unsupervised	B
dimension	O
reduction	O
analysis	B
.	O
here	O
there	O
is	O
little	O
class	O
overlap	O
in	O
the	O
projections	O
.	O
using	O
principal	O
components	O
analysis	B
for	O
comparison	O
.	O
there	O
is	O
considerable	O
class	O
overlap	O
in	O
the	O
projec-	O
tion	O
.	O
in	O
both	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
the	O
one	O
dimensional	O
projection	B
is	O
the	O
distance	O
along	O
the	O
line	O
,	O
measured	O
from	O
an	O
arbitrary	O
chosen	O
ﬁxed	O
point	O
on	O
the	O
line	O
.	O
where	O
m1	O
is	O
the	O
sample	B
mean	O
of	O
class	O
1	O
data	O
,	O
and	O
s1	O
the	O
sample	B
covariance	O
;	O
similarly	O
for	O
class	O
2.	O
the	O
projections	O
of	O
the	O
points	O
from	O
the	O
two	O
classes	O
are	O
then	O
given	O
by	O
because	O
the	O
projections	O
are	O
linear	B
,	O
the	O
projected	O
distributions	O
are	O
also	O
gaussian	O
,	O
1	O
=	O
wtxn	O
yn	O
1	O
,	O
2	O
=	O
wtxn	O
yn	O
2	O
(	O
cid:0	O
)	O
y1	O
µ1	O
,	O
σ2	O
(	O
cid:0	O
)	O
y2	O
µ2	O
,	O
σ2	O
1	O
2	O
(	O
cid:1	O
)	O
,	O
(	O
cid:1	O
)	O
,	O
p	O
(	O
y1	O
)	O
=	O
n	O
p	O
(	O
y2	O
)	O
=	O
n	O
µ1	O
=	O
wtm1	O
,	O
µ2	O
=	O
wtm2	O
,	O
1	O
=	O
wts1w	O
σ2	O
2	O
=	O
wts2w	O
σ2	O
(	O
16.2.2	O
)	O
(	O
16.2.3	O
)	O
(	O
16.2.4	O
)	O
we	O
search	O
for	O
a	O
projection	B
w	O
such	O
that	O
the	O
projected	O
distributions	O
have	O
minimal	O
overlap	O
.	O
this	O
can	O
be	O
achieved	O
if	O
the	O
projected	O
gaussian	O
means	O
are	O
maximally	O
separated	O
,	O
(	O
µ1	O
−	O
µ2	O
)	O
2	O
is	O
large	O
.	O
however	O
,	O
if	O
the	O
variances	O
σ2	O
2	O
are	O
also	O
large	O
,	O
there	O
could	O
be	O
a	O
large	O
overlap	O
still	O
in	O
the	O
classes	O
.	O
a	O
useful	O
objective	O
function	B
therefore	O
is	O
1	O
,	O
σ2	O
(	O
µ1	O
−	O
µ2	O
)	O
2	O
1	O
+	O
π2σ2	O
π1σ2	O
2	O
(	O
16.2.5	O
)	O
where	O
πi	O
represents	O
the	O
fraction	O
of	O
the	O
dataset	O
in	O
class	O
i.	O
equation	B
(	O
16.2.5	O
)	O
is	O
in	O
terms	O
of	O
the	O
projection	B
w	O
,	O
the	O
objective	O
f	O
(	O
w	O
)	O
=	O
wt	O
(	O
m1	O
−	O
m2	O
)	O
(	O
m1	O
−	O
m2	O
)	O
t	O
w	O
wt	O
(	O
π1s1	O
+	O
π2s2	O
)	O
w	O
=	O
wtaw	O
wtbw	O
where	O
the	O
optimal	O
w	O
can	O
be	O
found	O
by	O
diﬀerentiating	O
equation	B
(	O
16.2.6	O
)	O
with	O
respect	O
to	O
w.	O
this	O
gives	O
a	O
=	O
(	O
m1	O
−	O
m2	O
)	O
(	O
m1	O
−	O
m2	O
)	O
t	O
,	O
(	O
cid:104	O
)	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
wtaw	O
wtbw	O
∂	O
∂w	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
=	O
aw	O
−	O
and	O
therefore	O
the	O
zero	O
derivative	O
requirement	O
is	O
(	O
wbw	O
)	O
2	O
wtbw	O
(	O
cid:17	O
)	O
2	O
(	O
cid:16	O
)	O
wtbw	O
aw	O
=	O
wtaw	O
bw	O
b	O
=	O
π1s1	O
+	O
π2s2	O
(	O
cid:17	O
)	O
(	O
cid:105	O
)	O
wtaw	O
bw	O
(	O
16.2.6	O
)	O
(	O
16.2.7	O
)	O
(	O
16.2.8	O
)	O
(	O
16.2.9	O
)	O
304	O
draft	O
march	O
9	O
,	O
2010	O
−202468−5−4−3−2−1012345−202468−5−4−3−2−1012345	O
canonical	B
variates	I
multiplying	O
by	O
the	O
inverse	O
of	O
b	O
we	O
have	O
b−1	O
(	O
m1	O
−	O
m2	O
)	O
(	O
m1	O
−	O
m2	O
)	O
t	O
w	O
=	O
wtaw	O
wtbw	O
w	O
this	O
means	O
that	O
the	O
optimal	O
projection	B
is	O
explicitly	O
given	O
by	O
w	O
∝	O
b−1	O
(	O
m1	O
−	O
m2	O
)	O
(	O
16.2.10	O
)	O
(	O
16.2.11	O
)	O
although	O
the	O
proportionality	O
factor	B
depends	O
on	O
w	O
,	O
we	O
may	O
take	O
it	O
to	O
be	O
constant	O
since	O
the	O
objective	O
function	B
f	O
(	O
w	O
)	O
of	O
equation	B
(	O
16.2.6	O
)	O
is	O
invariant	O
to	O
rescaling	O
of	O
w.	O
we	O
may	O
therefore	O
take	O
w	O
=	O
kb−1	O
(	O
m1	O
−	O
m2	O
)	O
it	O
is	O
common	O
to	O
rescale	O
w	O
to	O
have	O
unit	O
length	O
,	O
wtw	O
=	O
1	O
,	O
such	O
that	O
k	O
=	O
(	O
cid:113	O
)	O
(	O
m1	O
−	O
m2	O
)	O
t	O
b−2	O
(	O
m1	O
−	O
m2	O
)	O
1	O
(	O
16.2.12	O
)	O
(	O
16.2.13	O
)	O
an	O
illustration	O
of	O
the	O
method	O
is	O
given	O
in	O
ﬁg	O
(	O
16.1	O
)	O
,	O
which	O
demonstrates	O
how	O
supervised	B
dimension	O
reduc-	O
tion	O
can	O
produce	O
lower	B
dimensional	I
representations	I
more	O
suitable	O
for	O
subsequent	O
classiﬁcation	B
than	O
an	O
unsupervised	B
method	O
such	O
as	O
pca	O
.	O
one	O
can	O
also	O
arrive	O
at	O
the	O
equation	B
(	O
16.2.12	O
)	O
from	O
a	O
diﬀerent	O
starting	O
objective	O
.	O
by	O
treating	O
the	O
projection	B
as	O
a	O
regression	B
problem	O
y	O
=	O
wtx+b	O
in	O
which	O
the	O
outputs	O
y	O
are	O
deﬁned	O
as	O
y1	O
and	O
y2	O
for	O
classes	O
1	O
and	O
class	O
2	O
respectively	O
,	O
one	O
may	O
show	O
that	O
,	O
for	O
suitably	O
chosen	O
y1	O
and	O
y2	O
,	O
the	O
solution	O
using	O
a	O
least	O
squares	O
criterion	O
is	O
given	O
by	O
equation	B
(	O
16.2.12	O
)	O
[	O
83	O
,	O
42	O
]	O
.	O
this	O
also	O
suggests	O
a	O
way	O
to	O
regularise	O
lda	O
,	O
see	O
exercise	O
(	O
171	O
)	O
.	O
kernel	B
extensions	O
of	O
lda	O
are	O
possible	O
,	O
see	O
for	O
example	O
[	O
78	O
,	O
248	O
]	O
.	O
when	O
the	O
naive	O
method	O
breaks	O
down	O
the	O
above	O
derivation	O
relied	O
on	O
the	O
existence	O
of	O
the	O
inverse	O
of	O
b.	O
in	O
practice	O
,	O
however	O
,	O
b	O
may	O
not	O
be	O
invertible	O
,	O
and	O
the	O
above	O
procedure	O
requires	O
modiﬁcation	O
.	O
a	O
case	O
where	O
b	O
is	O
not	O
invertible	O
is	O
when	O
there	O
are	O
fewer	O
datapoints	O
n1	O
+	O
n2	O
than	O
dimensions	O
d.	O
another	O
case	O
is	O
when	O
there	O
are	O
elements	O
of	O
the	O
input	O
vectors	O
that	O
never	O
vary	O
.	O
for	O
example	O
,	O
in	O
the	O
hand-written	O
digits	O
case	O
,	O
the	O
pixels	O
at	O
the	O
corner	O
edges	O
are	O
actually	O
always	O
zero	O
.	O
let	O
’	O
s	O
call	O
this	O
corner	O
pixel	O
z.	O
the	O
matrix	B
b	O
will	O
then	O
have	O
a	O
zero	O
entry	O
for	O
[	O
b	O
]	O
z	O
,	O
z	O
(	O
indeed	O
the	O
whole	O
zth	O
row	O
and	O
column	O
will	O
be	O
zero	O
)	O
so	O
that	O
for	O
any	O
vector	O
w	O
=	O
(	O
0	O
,	O
0	O
,	O
.	O
.	O
.	O
,	O
wz	O
,	O
0	O
,	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
)	O
⇒	O
wtbw	O
=	O
0	O
(	O
16.2.14	O
)	O
this	O
shows	O
that	O
the	O
denominator	O
of	O
fisher	O
’	O
s	O
objective	O
can	O
become	O
zero	O
,	O
and	O
the	O
objective	O
ill	O
deﬁned	O
.	O
we	O
will	O
deal	O
with	O
these	O
issues	O
section	O
(	O
16.3.1	O
)	O
.	O
16.3	O
canonical	B
variates	I
canonical	O
variates	O
generalises	O
fisher	O
’	O
s	O
method	O
to	O
projections	O
in	O
more	O
than	O
one	O
dimension	O
and	O
more	O
than	O
two	O
classes	O
.	O
the	O
projection	B
of	O
any	O
point	O
is	O
given	O
by	O
y	O
=	O
wtx	O
where	O
w	O
is	O
a	O
d	O
×	O
l	O
matrix	B
.	O
assuming	O
that	O
the	O
data	B
x	O
from	O
class	O
c	O
is	O
gaussian	O
distributed	O
,	O
p	O
(	O
x	O
)	O
=	O
n	O
(	O
x	O
mc	O
,	O
sc	O
)	O
the	O
projections	O
y	O
are	O
also	O
gaussian	O
p	O
(	O
y	O
)	O
=	O
n	O
y	O
wtmc	O
,	O
wtscw	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
to	O
extend	O
to	O
more	O
than	O
two	O
classes	O
,	O
we	O
deﬁne	O
the	O
following	O
matrices	O
:	O
draft	O
march	O
9	O
,	O
2010	O
(	O
16.3.1	O
)	O
(	O
16.3.2	O
)	O
(	O
16.3.3	O
)	O
305	O
between	O
class	O
scatter	O
find	O
m	O
the	O
mean	B
of	O
the	O
whole	O
dataset	O
and	O
mc	O
,	O
the	O
mean	B
of	O
the	O
each	O
class	O
c.	O
canonical	B
variates	I
where	O
nc	O
is	O
the	O
number	O
of	O
datapoints	O
in	O
class	O
c	O
,	O
c	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
c.	O
within	O
class	O
scatter	O
for	O
each	O
class	O
c	O
form	O
a	O
covariance	B
matrix	O
sc	O
and	O
mean	B
mc	O
.	O
deﬁne	O
form	O
a	O
≡	O
nc	O
(	O
mc	O
−	O
m	O
)	O
(	O
mc	O
−	O
m	O
)	O
t	O
c	O
(	O
cid:88	O
)	O
c=1	O
c	O
(	O
cid:88	O
)	O
c=1	O
b	O
≡	O
ncsc	O
this	O
naturally	O
gives	O
rise	O
to	O
a	O
raleigh	O
quotient	O
objective	O
trace	O
(	O
cid:0	O
)	O
wtaw	O
(	O
cid:1	O
)	O
trace	O
(	O
wtbw	O
)	O
f	O
(	O
w	O
)	O
≡	O
˜bt	O
˜b	O
=	O
b	O
then	O
deﬁning	O
the	O
objective	O
can	O
be	O
written	O
in	O
terms	O
of	O
˜w	O
:	O
(	O
cid:17	O
)	O
trace	O
(	O
cid:16	O
)	O
˜wt	O
˜b−ta	O
˜b−1	O
˜w	O
(	O
cid:17	O
)	O
˜w	O
=	O
˜bw	O
⇒	O
w	O
=	O
˜b−1	O
˜w	O
(	O
cid:16	O
)	O
˜wt	O
˜w	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
˜wtc	O
˜w	O
f	O
(	O
˜w	O
)	O
≡	O
trace	O
f	O
(	O
˜w	O
)	O
≡	O
trace	O
,	O
subject	O
to	O
˜wt	O
˜w	O
=	O
i	O
(	O
16.3.4	O
)	O
(	O
16.3.5	O
)	O
(	O
16.3.6	O
)	O
(	O
16.3.7	O
)	O
(	O
16.3.8	O
)	O
(	O
16.3.9	O
)	O
(	O
16.3.10	O
)	O
(	O
16.3.11	O
)	O
(	O
16.3.12	O
)	O
(	O
16.3.13	O
)	O
assuming	O
b	O
is	O
invertible	O
(	O
see	O
section	O
(	O
16.3.1	O
)	O
otherwise	O
)	O
,	O
we	O
can	O
deﬁne	O
the	O
cholesky	O
factor	B
˜b	O
,	O
with	O
if	O
we	O
assume	O
an	O
orthonormality	O
constraint	O
on	O
˜w	O
,	O
then	O
we	O
equivalently	O
require	O
the	O
maximisation	B
of	O
where	O
c	O
≡	O
˜b−ta	O
˜b−1	O
since	O
c	O
is	O
symmetric	O
and	O
positive	O
semi-deﬁnite	O
,	O
it	O
has	O
a	O
real	O
eigen-decomposition	O
c	O
=	O
eλet	O
where	O
λ	O
=	O
diag	O
(	O
λ1	O
,	O
λ2	O
,	O
.	O
.	O
.	O
,	O
λd	O
)	O
is	O
diagonal	O
with	O
non-negative	O
entries	O
containing	O
the	O
eigenvalues	O
,	O
sorted	O
by	O
decreasing	O
order	O
,	O
λ1	O
≥	O
λ2	O
≥	O
.	O
.	O
.	O
and	O
ete	O
=	O
i.	O
hence	O
(	O
cid:16	O
)	O
˜wteλet	O
˜w	O
(	O
cid:17	O
)	O
f	O
(	O
˜w	O
)	O
=	O
trace	O
by	O
setting	O
˜w	O
=	O
[	O
e1	O
,	O
.	O
.	O
.	O
,	O
el	O
]	O
,	O
where	O
el	O
is	O
the	O
lth	O
eigenvector	O
,	O
the	O
objective	O
becomes	O
the	O
sum	O
of	O
the	O
ﬁrst	O
l	O
eigenvalues	O
.	O
this	O
setting	O
maximises	O
the	O
objective	O
function	B
since	O
forming	O
˜w	O
from	O
any	O
other	O
columns	O
of	O
e	O
would	O
give	O
a	O
lower	O
sum	O
.	O
we	O
then	O
return	O
w	O
=	O
˜b−1	O
˜w	O
(	O
16.3.14	O
)	O
as	O
the	O
projection	B
matrix	O
.	O
the	O
procedure	O
is	O
outlined	O
in	O
algorithm	B
(	O
17	O
)	O
.	O
note	O
that	O
since	O
a	O
has	O
rank	B
c	O
,	O
there	O
can	O
be	O
no	O
more	O
than	O
c	O
−	O
1	O
non-zero	O
eigenvalues	O
and	O
corresponding	O
directions	O
.	O
306	O
draft	O
march	O
9	O
,	O
2010	O
canonical	B
variates	I
algorithm	O
17	O
canonical	B
variates	I
1	O
:	O
compute	O
the	O
between	O
and	O
within	O
class	O
scatter	O
matrices	O
a	O
,	O
equation	B
(	O
16.3.4	O
)	O
and	O
b	O
,	O
equation	B
(	O
16.3.5	O
)	O
.	O
2	O
:	O
compute	O
the	O
cholesky	O
factor	B
˜b	O
of	O
b	O
.	O
3	O
:	O
compute	O
the	O
l	O
principal	O
eigenvectors	O
[	O
e1	O
,	O
.	O
.	O
.	O
,	O
el	O
]	O
of	O
˜b−ta	O
˜b−1	O
.	O
4	O
:	O
˜w	O
=	O
[	O
e1	O
,	O
.	O
.	O
.	O
,	O
el	O
]	O
5	O
:	O
return	O
w	O
=	O
˜b−1	O
˜w	O
as	O
the	O
projection	B
matrix	O
.	O
16.3.1	O
dealing	O
with	O
the	O
nullspace	O
the	O
above	O
derivation	O
of	O
canonical	B
variates	I
(	O
and	O
also	O
fisher	O
’	O
s	O
lda	O
)	O
requires	O
the	O
invertibility	O
of	O
the	O
matrix	B
b.	O
however	O
,	O
as	O
we	O
discussed	O
in	O
section	O
(	O
16.2	O
)	O
one	O
may	O
encounter	O
situations	O
where	O
b	O
is	O
not	O
invertible	O
.	O
a	O
solution	O
is	O
to	O
require	O
that	O
w	O
lies	O
only	O
in	O
the	O
subspace	O
spanned	O
by	O
the	O
data	B
(	O
that	O
is	O
there	O
can	O
be	O
no	O
contribution	O
from	O
the	O
nullspace	O
)	O
.	O
to	O
do	O
this	O
we	O
ﬁrst	O
concatenate	O
the	O
training	B
data	O
from	O
all	O
classes	O
into	O
one	O
large	O
matrix	B
x.	O
a	O
basis	O
for	O
x	O
can	O
be	O
found	O
using	O
,	O
for	O
example	O
,	O
the	O
thin-svd	O
technique	O
which	O
returns	O
an	O
orthonormal	B
basis	O
q.	O
we	O
then	O
require	O
the	O
solution	O
w	O
to	O
be	O
expressed	O
in	O
this	O
basis	O
:	O
w	O
=	O
qw	O
(	O
cid:48	O
)	O
(	O
16.3.15	O
)	O
for	O
some	O
matrix	B
w	O
(	O
cid:48	O
)	O
.	O
substituting	O
this	O
in	O
the	O
canonical	B
variates	I
objective	O
equation	B
(	O
16.3.6	O
)	O
,	O
we	O
obtain	O
(	O
cid:16	O
)	O
w	O
(	O
cid:48	O
)	O
tqtaqw	O
(	O
cid:48	O
)	O
(	O
cid:17	O
)	O
trace	O
(	O
cid:0	O
)	O
w	O
(	O
cid:48	O
)	O
tqtbqw	O
(	O
cid:48	O
)	O
(	O
cid:1	O
)	O
trace	O
f	O
(	O
w	O
(	O
cid:48	O
)	O
)	O
≡	O
(	O
16.3.16	O
)	O
this	O
is	O
of	O
the	O
same	O
form	O
as	O
the	O
standard	O
quotient	O
,	O
equation	B
(	O
16.3.6	O
)	O
,	O
on	O
replacing	O
the	O
between-scatter	O
a	O
with	O
a	O
(	O
cid:48	O
)	O
≡	O
qtaq	O
and	O
the	O
within-scatter	O
b	O
with	O
(	O
16.3.17	O
)	O
b	O
(	O
cid:48	O
)	O
≡	O
qtbq	O
(	O
16.3.18	O
)	O
in	O
this	O
case	O
b	O
(	O
cid:48	O
)	O
is	O
guaranteed	O
invertible	O
,	O
and	O
one	O
may	O
carry	O
out	O
canonical	B
variates	I
,	O
as	O
in	O
section	O
(	O
16.3	O
)	O
above	O
.	O
this	O
will	O
return	O
a	O
matrix	B
w	O
(	O
cid:48	O
)	O
.	O
we	O
then	O
return	O
w	O
=	O
qw	O
(	O
cid:48	O
)	O
see	O
also	O
canonvar.m	O
.	O
(	O
16.3.19	O
)	O
example	O
75	O
(	O
using	O
canonical	B
variates	I
on	O
the	O
digits	O
data	B
)	O
.	O
we	O
apply	O
canonical	B
variates	I
to	O
project	O
the	O
digit	B
data	I
onto	O
two	O
dimensions	O
,	O
see	O
ﬁg	O
(	O
16.3	O
)	O
.	O
there	O
are	O
800	O
examples	O
of	O
a	O
three	O
,	O
800	O
examples	O
of	O
a	O
ﬁve	O
and	O
800	O
examples	O
of	O
a	O
seven	O
.	O
thus	O
,	O
overall	O
,	O
there	O
are	O
2400	O
examples	O
lying	O
in	O
a	O
784	O
(	O
28×	O
28	O
pixels	O
)	O
dimensional	O
space	O
.	O
note	O
how	O
the	O
canonical	B
variates	I
projected	O
data	B
onto	O
two	O
dimensions	O
has	O
very	O
little	O
class	O
overlap	O
,	O
see	O
ﬁg	O
(	O
16.3a	O
)	O
.	O
in	O
comparison	O
the	O
projections	O
formed	O
from	O
pca	O
,	O
which	O
discards	O
the	O
class	O
information	O
,	O
displays	O
a	O
high	O
degree	O
of	O
class	O
overlap	O
.	O
the	O
diﬀerent	O
scales	O
of	O
the	O
canonical	B
variates	I
and	O
pca	O
projections	O
q1	O
q2	O
figure	O
16.2	O
:	O
each	O
three	O
dimensional	O
datapoint	O
lies	O
in	O
a	O
two-dimensional	O
plane	O
,	O
meaning	O
that	O
the	O
matrix	B
b	O
is	O
not	O
full	O
rank	B
,	O
and	O
therefore	O
not	O
invertible	O
.	O
a	O
solution	O
is	O
given	O
by	O
ﬁnding	O
vectors	O
q1	O
,	O
q2	O
that	O
span	O
the	O
plane	O
,	O
and	O
express-	O
ing	O
the	O
canonical	B
variates	I
solution	O
in	O
terms	O
of	O
these	O
vec-	O
tors	O
alone	O
.	O
draft	O
march	O
9	O
,	O
2010	O
307	O
exercises	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
16.3	O
:	O
(	O
a	O
)	O
:	O
canonical	B
variates	I
projection	O
of	O
examples	O
of	O
handwritten	B
digits	I
3	O
(	O
‘	O
+	O
’	O
)	O
,	O
5	O
(	O
‘	O
o	O
’	O
)	O
and	O
7	O
(	O
di-	O
amond	O
)	O
.	O
there	O
are	O
800	O
examples	O
from	O
each	O
digit	O
class	O
.	O
plotted	O
are	O
the	O
projections	O
down	O
to	O
2	O
dimensions	O
.	O
(	O
b	O
)	O
:	O
pca	O
projections	O
for	O
comparison	O
.	O
in	O
pca	O
w	O
is	O
unitary	O
;	O
in	O
canonical	B
is	O
due	O
to	O
the	O
diﬀerent	O
constraints	O
on	O
the	O
projection	B
matrices	O
w.	O
variates	O
wtbw	O
=	O
i	O
,	O
meaning	O
that	O
w	O
will	O
scale	O
with	O
the	O
inverse	O
square	O
root	O
of	O
the	O
largest	O
eigenvalues	O
of	O
the	O
within	O
class	O
scatter	O
matrix	B
.	O
since	O
the	O
canonical	B
variates	I
objective	O
is	O
independent	O
of	O
linear	B
scaling	O
,	O
w	O
can	O
be	O
rescaled	O
with	O
an	O
arbitrary	O
scalar	O
prefactor	O
γw	O
,	O
as	O
desired	O
.	O
16.4	O
using	O
non-gaussian	O
data	B
distributions	O
the	O
applicability	O
of	O
canonical	B
variates	I
depends	O
on	O
our	O
assumption	O
that	O
a	O
gaussian	O
is	O
a	O
good	O
description	O
of	O
the	O
data	B
.	O
clearly	O
,	O
if	O
the	O
data	B
is	O
multimodal	O
,	O
using	O
a	O
single	O
gaussian	O
to	O
model	B
the	O
data	B
in	O
each	O
class	O
is	O
a	O
poor	O
assumption	O
.	O
this	O
may	O
result	O
in	O
projections	O
with	O
a	O
large	O
class	O
overlap	O
.	O
in	O
principle	O
,	O
there	O
is	O
no	O
conceptual	O
diﬃculty	O
in	O
using	O
more	O
complex	O
distributions	O
,	O
with	O
say	O
more	O
general	O
criteria	O
such	O
as	O
kullback-	O
leibler	O
divergence	B
between	O
projected	O
distributions	O
used	O
as	O
the	O
objective	O
.	O
however	O
,	O
such	O
criteria	O
typically	O
result	O
in	O
diﬃcult	O
optimisation	B
problems	O
.	O
canonical	B
variates	I
is	O
popular	O
due	O
to	O
its	O
simplicity	O
and	O
lack	O
of	O
local	B
optima	O
issues	O
in	O
constructing	O
the	O
projection	B
.	O
16.5	O
code	O
canonvar.m	O
:	O
canonical	B
variates	I
democanonvardigits.m	O
:	O
demo	O
for	O
canonical	B
variates	I
16.6	O
exercises	O
exercise	O
169.	O
what	O
happens	O
to	O
fisher	O
’	O
s	O
linear	O
discriminant	O
if	O
there	O
are	O
less	O
datapoints	O
than	O
dimensions	O
?	O
exercise	O
170.	O
modify	O
democanonvardigits.m	O
to	O
project	O
and	O
visualise	O
the	O
digits	O
data	B
in	O
3	O
dimensions	O
.	O
exercise	O
171.	O
consider	O
n1	O
class	O
1	O
datapoints	O
xn1	O
,	O
n1	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n1	O
and	O
class	O
2	O
datapoints	O
xn2	O
,	O
n2	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n2	O
.	O
we	O
will	O
make	O
a	O
linear	B
predictor	O
for	O
the	O
data	B
,	O
y	O
=	O
wtx	O
+	O
b	O
308	O
(	O
16.6.1	O
)	O
draft	O
march	O
9	O
,	O
2010	O
−0.1−0.0500.050.10.15−0.06−0.04−0.0200.020.040.060.080.10.120.14−3000−2500−2000−1500−1000−500−1000−50005001000	O
exercises	O
with	O
the	O
aim	O
to	O
predict	O
value	B
y1	O
for	O
data	B
from	O
class	O
1	O
and	O
y2	O
for	O
data	B
from	O
class	O
two	O
.	O
a	O
measure	O
of	O
the	O
ﬁt	O
is	O
given	O
by	O
n1	O
(	O
cid:88	O
)	O
n1=1	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
2	O
y1	O
−	O
wtxn1	O
−	O
b	O
+	O
n2	O
(	O
cid:88	O
)	O
n2=1	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
2	O
y2	O
−	O
wtxn2	O
−	O
b	O
show	O
that	O
by	O
setting	O
y1	O
=	O
(	O
n1	O
+	O
n2	O
)	O
/n1	O
and	O
y2	O
=	O
(	O
n1	O
+	O
n2	O
)	O
/n2	O
the	O
w	O
which	O
minimises	O
e	O
corresponds	O
to	O
fisher	O
’	O
s	O
lda	O
solution	O
.	O
hint	O
:	O
ﬁrst	O
show	O
that	O
the	O
two	O
zero	O
derivative	O
conditions	O
are	O
e	O
(	O
w	O
,	O
b|y1	O
,	O
y2	O
)	O
=	O
(	O
cid:16	O
)	O
(	O
cid:88	O
)	O
(	O
cid:16	O
)	O
and	O
(	O
cid:88	O
)	O
n1	O
y1	O
−	O
b	O
−	O
wtxn1	O
n1	O
y1	O
−	O
b	O
−	O
wtxn1	O
(	O
cid:18	O
)	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
+	O
(	O
cid:88	O
)	O
n1	O
+	O
(	O
cid:88	O
)	O
xt	O
n2	O
n2	O
(	O
cid:17	O
)	O
=	O
0	O
y2	O
−	O
b	O
−	O
wtxn2	O
(	O
cid:16	O
)	O
y2	O
−	O
b	O
−	O
wtxn2	O
(	O
cid:17	O
)	O
xt	O
n2	O
=	O
0	O
which	O
can	O
be	O
reduced	O
to	O
the	O
single	O
equation	B
n	O
(	O
m1	O
−	O
m2	O
)	O
=	O
(	O
m1	O
−	O
m2	O
)	O
(	O
m1	O
−	O
m2	O
)	O
t	O
where	O
b	O
is	O
as	O
deﬁned	O
for	O
lda	O
in	O
the	O
text	O
,	O
equation	B
(	O
16.2.7	O
)	O
.	O
n	O
nb	O
+	O
n1n2	O
(	O
cid:19	O
)	O
w	O
note	O
that	O
this	O
suggests	O
a	O
way	O
to	O
regularise	O
lda	O
,	O
namely	O
by	O
adding	O
on	O
a	O
term	O
λwtw	O
to	O
e	O
(	O
w	O
,	O
b|y1	O
,	O
y2	O
)	O
.	O
this	O
can	O
be	O
absorbed	O
into	O
redeﬁning	O
equation	B
(	O
16.3.5	O
)	O
as	O
b	O
(	O
cid:48	O
)	O
=	O
b	O
+	O
λi	O
(	O
16.6.6	O
)	O
in	O
other	O
words	O
,	O
one	O
can	O
increase	O
the	O
covariance	B
b	O
by	O
an	O
additive	O
amount	O
λi	O
.	O
the	O
optimal	O
regularising	O
constant	O
λ	O
may	O
be	O
set	O
by	O
cross-validation	B
.	O
more	O
generally	O
one	O
can	O
consider	O
the	O
use	O
of	O
a	O
regularising	O
matrix	B
λr	O
,	O
where	O
r	O
is	O
positive	B
deﬁnite	I
.	O
exercise	O
172.	O
consider	O
the	O
digit	B
data	I
of	O
892	O
ﬁves	O
digit5.mat	O
and	O
1028	O
sevens	O
digit7.mat	O
.	O
make	O
a	O
training	B
set	O
which	O
consists	O
of	O
the	O
ﬁrst	O
500	O
examples	O
from	O
each	O
digit	O
class	O
.	O
use	O
canonical	B
variates	I
to	O
ﬁrst	O
project	O
the	O
data	B
down	O
to	O
50	O
dimensions	O
and	O
compute	O
the	O
nearest	B
neighbour	I
performance	O
on	O
the	O
remaining	O
digits	O
.	O
compare	O
the	O
classiﬁcation	B
accuracy	O
to	O
using	O
nearest	O
neighbours	O
the	O
projections	O
from	O
pca	O
using	O
50	O
components	O
.	O
exercise	O
173.	O
consider	O
an	O
objective	O
function	B
of	O
the	O
form	O
f	O
(	O
w	O
)	O
≡	O
a	O
(	O
w	O
)	O
b	O
(	O
w	O
)	O
(	O
16.6.7	O
)	O
where	O
a	O
(	O
w	O
)	O
and	O
b	O
(	O
w	O
)	O
are	O
positive	O
functions	O
,	O
and	O
our	O
task	O
is	O
to	O
maximise	O
f	O
(	O
w	O
)	O
with	O
respect	O
to	O
w.	O
it	O
may	O
be	O
that	O
this	O
objective	O
does	O
not	O
have	O
a	O
simple	O
algebraic	O
solution	O
,	O
even	O
though	O
a	O
(	O
w	O
)	O
and	O
b	O
(	O
w	O
)	O
are	O
simple	O
functions	O
.	O
(	O
16.6.2	O
)	O
(	O
16.6.3	O
)	O
(	O
16.6.4	O
)	O
(	O
16.6.5	O
)	O
(	O
16.6.8	O
)	O
(	O
16.6.9	O
)	O
(	O
16.6.10	O
)	O
we	O
can	O
consider	O
an	O
alternative	O
objective	O
,	O
namely	O
j	O
(	O
w	O
,	O
λ	O
)	O
=	O
a	O
(	O
w	O
)	O
−	O
λb	O
(	O
w	O
)	O
where	O
λ	O
is	O
a	O
constant	O
scalar	O
.	O
choose	O
an	O
initial	O
point	O
wold	O
at	O
random	O
and	O
set	O
λold	O
≡	O
a	O
(	O
wold	O
)	O
/b	O
(	O
wold	O
)	O
in	O
that	O
case	O
j	O
(	O
wold	O
,	O
λold	O
)	O
=	O
0.	O
now	O
choose	O
a	O
w	O
such	O
that	O
j	O
(	O
w	O
,	O
λold	O
)	O
=	O
a	O
(	O
w	O
)	O
−	O
λoldb	O
(	O
w	O
)	O
≥	O
0	O
this	O
is	O
certainly	O
possible	O
since	O
j	O
(	O
wold	O
,	O
λold	O
)	O
=	O
0.	O
if	O
we	O
can	O
ﬁnd	O
a	O
w	O
such	O
that	O
j	O
(	O
w	O
,	O
λold	O
)	O
>	O
0	O
,	O
then	O
a	O
(	O
w	O
)	O
−	O
λoldb	O
(	O
w	O
)	O
>	O
0	O
(	O
16.6.11	O
)	O
show	O
that	O
for	O
such	O
a	O
w	O
,	O
f	O
(	O
w	O
)	O
>	O
f	O
(	O
wold	O
)	O
,	O
and	O
suggest	O
an	O
iterative	O
optimisation	O
procedure	O
for	O
objective	O
functions	O
of	O
the	O
form	O
f	O
(	O
w	O
)	O
.	O
draft	O
march	O
9	O
,	O
2010	O
309	O
exercises	O
310	O
draft	O
march	O
9	O
,	O
2010	O
chapter	O
17	O
linear	B
models	O
17.1	O
introduction	O
:	O
fitting	O
a	O
straight	O
line	O
given	O
training	B
data	O
{	O
(	O
xn	O
,	O
yn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
,	O
for	O
scalar	O
input	O
xn	O
and	O
scalar	O
output	O
yn	O
,	O
a	O
linear	B
regression	O
ﬁt	O
is	O
y	O
(	O
x	O
)	O
=	O
a	O
+	O
bx	O
(	O
17.1.1	O
)	O
to	O
determine	O
the	O
best	O
parameters	O
a	O
,	O
b	O
,	O
we	O
use	O
a	O
measure	O
of	O
the	O
discrepancy	O
between	O
the	O
observed	O
outputs	O
and	O
the	O
linear	B
regression	O
ﬁt	O
such	O
as	O
the	O
sum	O
squared	O
training	O
error	O
.	O
this	O
is	O
also	O
called	O
ordinary	B
least	I
squares	I
and	O
minimises	O
the	O
average	B
vertical	O
projection	B
of	O
the	O
points	O
y	O
to	O
ﬁtted	O
line	O
:	O
n	O
(	O
cid:88	O
)	O
n=1	O
e	O
(	O
a	O
,	O
b	O
)	O
=	O
n	O
(	O
cid:88	O
)	O
n=1	O
(	O
yn	O
−	O
a	O
−	O
bxn	O
)	O
2	O
[	O
yn	O
−	O
y	O
(	O
xn	O
)	O
]	O
2	O
=	O
n	O
(	O
cid:88	O
)	O
our	O
task	O
is	O
to	O
ﬁnd	O
the	O
parameters	O
a	O
and	O
b	O
that	O
minimise	O
e	O
(	O
a	O
,	O
b	O
)	O
.	O
diﬀerentiating	O
with	O
respect	O
to	O
a	O
and	O
b	O
we	O
obtain	O
∂	O
∂a	O
e	O
(	O
a	O
,	O
b	O
)	O
=	O
−2	O
(	O
yn	O
−	O
a	O
−	O
bxn	O
)	O
,	O
n=1	O
∂	O
∂b	O
e	O
(	O
a	O
,	O
b	O
)	O
=	O
−2	O
(	O
yn	O
−	O
a	O
−	O
bxn	O
)	O
xn	O
dividing	O
by	O
n	O
and	O
equating	O
to	O
zero	O
,	O
the	O
optimal	O
parameters	O
are	O
given	O
from	O
the	O
solution	O
to	O
the	O
two	O
linear	B
equations	O
n	O
(	O
cid:88	O
)	O
n=1	O
(	O
17.1.2	O
)	O
(	O
17.1.3	O
)	O
(	O
17.1.4	O
)	O
(	O
17.1.5	O
)	O
(	O
17.1.6	O
)	O
(	O
17.1.7	O
)	O
(	O
cid:104	O
)	O
y	O
(	O
cid:105	O
)	O
−	O
a	O
−	O
b	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
=	O
0	O
,	O
(	O
cid:104	O
)	O
xy	O
(	O
cid:105	O
)	O
−	O
a	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
−	O
b	O
(	O
cid:10	O
)	O
x2	O
(	O
cid:11	O
)	O
=	O
0	O
(	O
cid:80	O
)	O
n	O
−	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
2	O
(	O
cid:105	O
)	O
(	O
cid:104	O
)	O
(	O
cid:10	O
)	O
x2	O
(	O
cid:11	O
)	O
where	O
we	O
used	O
the	O
notation	O
(	O
cid:104	O
)	O
f	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:105	O
)	O
to	O
denote	O
1	O
to	O
determine	O
a	O
and	O
b	O
:	O
a	O
=	O
(	O
cid:104	O
)	O
y	O
(	O
cid:105	O
)	O
−	O
b	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
b	O
(	O
cid:10	O
)	O
x2	O
(	O
cid:11	O
)	O
=	O
(	O
cid:104	O
)	O
yx	O
(	O
cid:105	O
)	O
−	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
(	O
(	O
cid:104	O
)	O
y	O
(	O
cid:105	O
)	O
−	O
b	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
)	O
⇒	O
b	O
n	O
hence	O
=	O
(	O
cid:104	O
)	O
xy	O
(	O
cid:105	O
)	O
−	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
(	O
cid:104	O
)	O
y	O
(	O
cid:105	O
)	O
b	O
=	O
(	O
cid:104	O
)	O
xy	O
(	O
cid:105	O
)	O
−	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
(	O
cid:104	O
)	O
y	O
(	O
cid:105	O
)	O
(	O
cid:104	O
)	O
x2	O
(	O
cid:105	O
)	O
−	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
2	O
and	O
a	O
is	O
found	O
by	O
substituting	O
this	O
value	B
for	O
b	O
into	O
equation	B
(	O
17.1.5	O
)	O
.	O
n=1	O
f	O
(	O
xn	O
,	O
yn	O
)	O
.	O
we	O
can	O
readily	O
solve	O
the	O
equations	O
(	O
17.1.4	O
)	O
in	O
contrast	O
to	O
ordinary	B
least	I
squares	I
regression	O
,	O
pca	O
from	O
chapter	O
(	O
15	O
)	O
minimises	O
the	O
orthogonal	B
projection	O
of	O
y	O
to	O
the	O
line	O
and	O
is	O
known	O
as	O
orthogonal	O
least	O
squares	O
–	O
see	O
example	O
(	O
76	O
)	O
.	O
311	O
linear	O
parameter	O
models	O
for	O
regression	B
figure	O
17.1	O
:	O
data	B
from	O
crickets	O
–	O
the	O
number	O
of	O
chirps	O
per	O
second	O
,	O
versus	O
the	O
temperature	O
in	O
fahren-	O
heit	O
.	O
example	O
76.	O
consider	O
the	O
data	B
in	O
ﬁg	O
(	O
17.1	O
)	O
,	O
in	O
which	O
we	O
plot	O
the	O
number	O
of	O
chirps	O
c	O
per	O
second	O
for	O
crickets	O
,	O
versus	O
the	O
temperature	O
t	O
in	O
degrees	O
fahrenheit	O
.	O
a	O
biologist	O
believes	O
that	O
there	O
is	O
a	O
simple	O
relation	O
between	O
the	O
number	O
of	O
chirps	O
and	O
the	O
temperature	O
of	O
the	O
form	O
c	O
=	O
a	O
+	O
bt	O
(	O
17.1.8	O
)	O
where	O
she	O
needs	O
to	O
determine	O
the	O
parameters	O
a	O
and	O
b.	O
for	O
the	O
cricket	O
data	B
,	O
the	O
ﬁt	O
is	O
plotted	O
in	O
ﬁg	O
(	O
17.2a	O
)	O
.	O
for	O
comparison	O
we	O
plot	O
the	O
ﬁt	O
from	O
the	O
pca	O
,	O
ﬁg	O
(	O
17.2b	O
)	O
,	O
which	O
minimises	O
the	O
sum	O
of	O
the	O
squared	O
orthogonal	O
projections	O
from	O
the	O
data	B
to	O
the	O
line	O
.	O
in	O
this	O
case	O
there	O
is	O
little	O
numerical	B
diﬀerence	O
between	O
the	O
two	O
ﬁts	O
.	O
17.2	O
linear	O
parameter	O
models	O
for	O
regression	B
we	O
can	O
generalise	O
on	O
the	O
idea	O
of	O
ﬁtting	O
straight	O
lines	O
to	O
ﬁtting	O
linear	B
functions	O
of	O
vector	O
inputs	O
.	O
for	O
a	O
dataset	O
{	O
(	O
xn	O
,	O
yn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
,	O
a	O
linear	O
parameter	O
regression	O
model	B
(	O
lpm	O
)	O
is	O
deﬁned	O
by1	O
y	O
(	O
x	O
)	O
=	O
wtφ	O
(	O
x	O
)	O
(	O
17.2.1	O
)	O
where	O
φ	O
(	O
x	O
)	O
is	O
a	O
vector	O
valued	O
function	B
of	O
the	O
input	O
vector	O
x.	O
for	O
example	O
,	O
in	O
the	O
case	O
of	O
a	O
straight	O
line	O
ﬁt	O
,	O
with	O
a	O
scalar	O
input	O
and	O
output	O
,	O
section	O
(	O
17.1	O
)	O
,	O
we	O
have	O
φ	O
(	O
x	O
)	O
=	O
(	O
1	O
,	O
x	O
)	O
t	O
,	O
w	O
=	O
(	O
a	O
,	O
b	O
)	O
t	O
,	O
(	O
17.2.2	O
)	O
we	O
deﬁne	O
the	O
train	O
error	O
as	O
the	O
sum	O
of	O
squared	O
diﬀerences	O
between	O
the	O
observed	O
outputs	O
and	O
the	O
predictions	O
under	O
the	O
linear	B
model	I
:	O
e	O
(	O
w	O
)	O
=	O
(	O
yn	O
−	O
wtφn	O
)	O
2	O
,	O
where	O
φn	O
≡	O
φ	O
(	O
xn	O
)	O
(	O
17.2.3	O
)	O
we	O
now	O
wish	O
to	O
determine	O
the	O
parameter	B
vector	O
w	O
that	O
minimises	O
e	O
(	O
w	O
)	O
.	O
writing	O
out	O
the	O
error	O
in	O
terms	O
of	O
the	O
components	O
of	O
w	O
,	O
n	O
(	O
cid:88	O
)	O
n=1	O
n	O
(	O
cid:88	O
)	O
n=1	O
(	O
17.2.4	O
)	O
(	O
17.2.5	O
)	O
(	O
17.2.6	O
)	O
e	O
(	O
w	O
)	O
=	O
(	O
yn	O
−	O
wiφn	O
i	O
)	O
(	O
yn	O
−	O
wjφn	O
j	O
)	O
diﬀerentiating	O
with	O
respect	O
to	O
wk	O
,	O
and	O
equating	O
to	O
zero	O
gives	O
(	O
cid:88	O
)	O
j	O
(	O
cid:88	O
)	O
i	O
(	O
cid:88	O
)	O
n	O
ynφn	O
wi	O
i	O
φn	O
φn	O
k	O
or	O
,	O
in	O
matrix	B
notation	O
,	O
n	O
(	O
cid:88	O
)	O
n=1	O
n	O
(	O
cid:88	O
)	O
k	O
=	O
(	O
cid:88	O
)	O
n	O
(	O
cid:88	O
)	O
i	O
ynφn	O
=	O
φn	O
(	O
φn	O
)	O
tw	O
n=1	O
n=1	O
1note	O
that	O
the	O
model	B
is	O
linear	B
in	O
the	O
parameter	B
w	O
–	O
not	O
necessarily	O
linear	B
in	O
x	O
.	O
312	O
draft	O
march	O
9	O
,	O
2010	O
65707580859095100121416182022chirps	O
per	O
sectemperature	O
(	O
f	O
)	O
linear	O
parameter	O
models	O
for	O
regression	B
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
17.2	O
:	O
(	O
a	O
)	O
:	O
straight	O
line	O
regression	O
ﬁt	O
to	O
the	O
cricket	O
data	B
.	O
(	O
b	O
)	O
:	O
pca	O
ﬁt	O
to	O
the	O
data	B
.	O
in	O
regression	B
we	O
minimize	O
the	O
residuals	B
–	O
the	O
vertical	O
distances	O
from	O
datapoints	O
to	O
the	O
line	O
.	O
in	O
pca	O
the	O
ﬁt	O
minimizes	O
the	O
orthogonal	B
projections	O
to	O
the	O
line	O
.	O
in	O
this	O
case	O
,	O
there	O
is	O
little	O
diﬀerence	O
in	O
the	O
ﬁtted	O
lines	O
.	O
both	O
go	O
through	O
the	O
mean	B
of	O
the	O
data	B
;	O
the	O
linear	B
regression	O
ﬁt	O
has	O
slope	O
0.121	O
and	O
the	O
pca	O
ﬁt	O
has	O
slope	O
0.126.	O
these	O
are	O
called	O
the	O
normal	B
equations	I
,	O
for	O
which	O
the	O
solution	O
is	O
(	O
cid:32	O
)	O
n	O
(	O
cid:88	O
)	O
(	O
cid:33	O
)	O
−1	O
n	O
(	O
cid:88	O
)	O
w	O
=	O
φn	O
(	O
φn	O
)	O
t	O
ynφn	O
(	O
17.2.7	O
)	O
n=1	O
n=1	O
although	O
we	O
write	O
the	O
solution	O
using	O
matrix	B
inversion	O
,	O
in	O
practice	O
one	O
ﬁnds	O
the	O
numerical	B
solution	O
using	O
gaussian	O
elimination	O
[	O
117	O
]	O
since	O
this	O
is	O
faster	O
and	O
more	O
numerically	O
stable	O
.	O
example	O
77.	O
a	O
cubic	O
polynomial	O
ﬁt	O
a	O
cubic	O
polynomial	O
is	O
given	O
by	O
y	O
(	O
x	O
)	O
=	O
w1	O
+	O
w2x	O
+	O
w3x2	O
+	O
w4x3	O
as	O
a	O
lpm	O
,	O
this	O
can	O
be	O
expressed	O
using	O
φ	O
(	O
x	O
)	O
=	O
(	O
cid:0	O
)	O
1	O
,	O
x	O
,	O
x2	O
,	O
x3	O
(	O
cid:1	O
)	O
t	O
(	O
17.2.8	O
)	O
(	O
17.2.9	O
)	O
the	O
ordinary	B
least	I
squares	I
solution	O
has	O
the	O
form	O
given	O
in	O
equation	B
(	O
17.2.17	O
)	O
.	O
the	O
ﬁtted	O
cubic	O
polynomial	O
is	O
plotted	O
in	O
ﬁg	O
(	O
17.3	O
)	O
.	O
see	O
also	O
democubicpoly.m	O
.	O
5	O
(	O
cid:88	O
)	O
example	O
78	O
(	O
predicting	O
return	O
)	O
.	O
in	O
ﬁg	O
(	O
17.4	O
)	O
we	O
present	O
ﬁtting	O
an	O
lpm	O
with	O
vector	O
inputs	O
x	O
to	O
a	O
scalar	O
output	O
y.	O
the	O
vector	O
x	O
represents	O
factors	O
that	O
are	O
believed	O
to	O
aﬀect	O
the	O
stock	O
price	O
of	O
a	O
company	O
,	O
with	O
the	O
stock	O
price	O
return	O
given	O
by	O
the	O
scalar	O
y.	O
a	O
hedge	B
fund	I
manager	O
believes	O
that	O
the	O
returns	O
may	O
be	O
linearly	O
related	O
to	O
the	O
factors	O
:	O
yt	O
=	O
wixit	O
(	O
17.2.10	O
)	O
i=1	O
and	O
wishes	O
to	O
ﬁt	O
the	O
parameters	O
w	O
in	O
order	O
to	O
use	O
the	O
model	B
to	O
predict	O
future	O
stock	O
returns	O
.	O
this	O
is	O
straightforward	O
using	O
ordinary	B
least	I
squares	I
,	O
this	O
being	O
simply	O
an	O
lpm	O
with	O
a	O
linear	B
φ	O
function	B
.	O
see	O
figure	O
17.3	O
:	O
cubic	O
polynomial	O
ﬁt	O
to	O
the	O
cricket	O
data	B
.	O
draft	O
march	O
9	O
,	O
2010	O
313	O
65707580859095100121416182022chirps	O
per	O
sectemperature	O
(	O
f	O
)	O
65707580859095100121416182022chirps	O
per	O
sectemperature	O
(	O
f	O
)	O
65707580859095100121416182022chirps	O
per	O
sectemperature	O
(	O
f	O
)	O
linear	O
parameter	O
models	O
for	O
regression	B
on	O
yt	O
=	O
(	O
cid:80	O
)	O
figure	O
17.4	O
:	O
predicting	O
stock	O
return	O
using	O
a	O
linear	B
lpm	O
.	O
the	O
top	O
ﬁve	O
panels	O
present	O
the	O
inputs	O
x1	O
,	O
.	O
.	O
.	O
,	O
x5	O
for	O
20	O
train	O
days	O
(	O
blue	O
)	O
and	O
5	O
test	O
days	O
(	O
red	O
)	O
.	O
the	O
corresponding	O
train	O
out-	O
put	O
(	O
stock	O
return	O
)	O
y	O
for	O
each	O
day	O
is	O
given	O
in	O
the	O
bottom	O
panel	O
.	O
the	O
predictions	O
y21	O
,	O
.	O
.	O
.	O
,	O
y25	O
are	O
the	O
predictions	O
based	O
i	O
wixit	O
with	O
w	O
trained	O
using	O
ordinary	B
least	I
squares	I
.	O
with	O
a	O
regularisation	B
term	O
0.01wtw	O
,	O
the	O
ols	O
learned	O
w	O
is	O
[	O
1.42	O
,	O
0.62	O
,	O
0.27	O
,	O
−0.26	O
,	O
1.54	O
]	O
.	O
despite	O
the	O
simplicity	O
of	O
these	O
models	O
,	O
their	O
application	O
in	O
the	O
ﬁnance	O
industry	O
is	O
widespread	O
,	O
with	O
signiﬁcant	O
investment	O
made	O
on	O
collating	O
factors	O
x	O
that	O
may	O
be	O
indicative	O
of	O
future	O
return	O
.	O
see	O
demolpmhedge.m	O
.	O
ﬁg	O
(	O
17.4	O
)	O
for	O
an	O
example	O
.	O
such	O
models	O
also	O
form	O
the	O
basis	O
for	O
more	O
complex	O
models	O
in	O
ﬁnance	O
,	O
see	O
for	O
example	O
[	O
194	O
]	O
.	O
17.2.1	O
vector	O
outputs	O
it	O
is	O
straightforward	O
to	O
generalise	O
the	O
above	O
framework	O
to	O
vector	O
outputs	O
y.	O
using	O
a	O
separate	O
weight	B
vector	O
wi	O
for	O
each	O
output	O
component	O
yi	O
,	O
we	O
have	O
the	O
mathematics	O
follows	O
similarly	O
to	O
before	O
,	O
and	O
we	O
may	O
deﬁne	O
a	O
training	B
error	O
per	O
output	O
as	O
yi	O
(	O
x	O
)	O
=	O
wt	O
i	O
φ	O
(	O
x	O
)	O
e	O
(	O
w	O
)	O
=	O
(	O
cid:88	O
)	O
e	O
(	O
wi	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
(	O
cid:16	O
)	O
i	O
i	O
n	O
i	O
φn	O
(	O
cid:17	O
)	O
2	O
i	O
−	O
wt	O
yn	O
(	O
17.2.11	O
)	O
(	O
17.2.12	O
)	O
since	O
the	O
training	B
error	O
decomposes	O
into	O
individual	O
terms	O
,	O
one	O
for	O
each	O
output	O
,	O
the	O
weights	O
for	O
each	O
output	O
can	O
be	O
trained	O
separately	O
.	O
in	O
other	O
words	O
,	O
the	O
problem	B
decomposes	O
into	O
a	O
set	O
of	O
independent	O
scalar	O
output	O
problems	O
.	O
in	O
case	O
the	O
parameters	O
w	O
are	O
tied	O
or	O
shared	O
amongst	O
the	O
outputs	O
,	O
the	O
training	B
is	O
still	O
straightforward	O
since	O
the	O
objective	O
function	B
remains	O
linear	B
in	O
the	O
parameters	O
,	O
and	O
this	O
is	O
left	O
as	O
an	O
exercise	O
for	O
the	O
interested	O
reader	O
.	O
17.2.2	O
regularisation	B
for	O
most	O
purposes	O
,	O
our	O
interest	O
is	O
not	O
just	O
to	O
ﬁnd	O
the	O
function	B
that	O
best	O
ﬁts	O
the	O
training	B
data	O
but	O
one	O
that	O
that	O
will	O
generalise	O
well	O
.	O
to	O
control	O
the	O
complexity	O
of	O
the	O
ﬁtted	O
function	B
we	O
may	O
add	O
an	O
extra	O
‘	O
regularising	O
’	O
(	O
or	O
‘	O
penalty	O
’	O
)	O
term	O
to	O
the	O
training	B
error	O
to	O
penalise	O
rapid	O
changes	O
in	O
the	O
output	O
.	O
for	O
example	O
a	O
regularising	O
term	O
that	O
can	O
be	O
added	O
to	O
equation	B
(	O
17.2.3	O
)	O
is	O
(	O
17.2.13	O
)	O
(	O
cid:105	O
)	O
2	O
y	O
(	O
xn	O
)	O
−	O
y	O
(	O
xn	O
(	O
cid:48	O
)	O
)	O
n	O
(	O
cid:88	O
)	O
n=1	O
λ	O
e	O
(	O
cid:16	O
)	O
−γ	O
xn−xn	O
(	O
cid:48	O
)	O
(	O
cid:17	O
)	O
2	O
(	O
cid:104	O
)	O
(	O
cid:105	O
)	O
2	O
n	O
(	O
cid:88	O
)	O
(	O
cid:104	O
)	O
y	O
(	O
xn	O
)	O
−	O
y	O
(	O
xn	O
(	O
cid:48	O
)	O
xn−xn	O
(	O
cid:48	O
)	O
(	O
cid:17	O
)	O
2	O
n	O
(	O
cid:48	O
)	O
=1	O
)	O
the	O
factor	B
−γ	O
(	O
cid:16	O
)	O
factor	B
e	O
xn	O
(	O
cid:48	O
)	O
regularising	O
term	O
.	O
penalises	O
large	O
diﬀerences	O
in	O
the	O
outputs	O
corresponding	O
to	O
two	O
inputs	O
.	O
the	O
has	O
the	O
eﬀect	O
of	O
weighting	O
more	O
heavily	O
terms	O
for	O
which	O
two	O
input	O
vectors	O
xn	O
and	O
are	O
close	O
together	O
;	O
γ	O
is	O
a	O
ﬁxed	O
length-scale	O
parameter	B
and	O
λ	O
determines	O
the	O
overall	O
strength	O
of	O
the	O
314	O
draft	O
march	O
9	O
,	O
2010	O
051015202500.510510152025−0.500.50510152025−0.500.50510152025−2.5−2−1.50510152025−1010510152025024	O
linear	O
parameter	O
models	O
for	O
regression	B
figure	O
17.5	O
:	O
a	O
set	O
of	O
ﬁxed-width	O
(	O
α	O
=	O
1	O
)	O
radial	B
basis	I
functions	I
,	O
2	O
(	O
x−mi	O
)	O
2	O
−	O
1	O
,	O
with	O
the	O
centres	O
mi	O
evenly	O
spaced	O
.	O
by	O
taking	O
a	O
linear	B
combi-	O
e	O
nation	O
of	O
these	O
functions	O
we	O
can	O
form	O
a	O
ﬂexible	O
function	B
class	O
.	O
since	O
y	O
=	O
wtφ	O
(	O
x	O
)	O
,	O
expression	O
(	O
17.2.13	O
)	O
can	O
be	O
written	O
as	O
wtrw	O
where	O
r	O
≡	O
λ	O
xn−xn	O
(	O
cid:48	O
)	O
(	O
cid:17	O
)	O
2	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
φn	O
−	O
φn	O
(	O
cid:48	O
)	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
φn	O
−	O
φn	O
(	O
cid:48	O
)	O
(	O
cid:17	O
)	O
t	O
−γ	O
e	O
the	O
regularised	B
train	O
error	O
is	O
then	O
(	O
cid:48	O
)	O
(	O
w	O
)	O
=	O
e	O
(	O
yn	O
−	O
wtφn	O
)	O
2	O
+	O
wtrw	O
n	O
(	O
cid:48	O
)	O
=1	O
n=1	O
n	O
(	O
cid:88	O
)	O
n	O
(	O
cid:88	O
)	O
n	O
(	O
cid:88	O
)	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
n=1	O
n	O
(	O
cid:88	O
)	O
(	O
cid:33	O
)	O
−1	O
n	O
(	O
cid:88	O
)	O
n=1	O
(	O
17.2.14	O
)	O
(	O
17.2.15	O
)	O
(	O
17.2.16	O
)	O
by	O
diﬀerentiating	O
the	O
regularised	B
training	O
error	O
and	O
equating	O
to	O
zero	O
,	O
we	O
ﬁnd	O
the	O
optimal	O
w	O
is	O
given	O
by	O
w	O
=	O
φn	O
(	O
φn	O
)	O
t	O
+	O
r	O
ynφn	O
(	O
17.2.17	O
)	O
in	O
practice	O
it	O
is	O
common	O
to	O
use	O
a	O
regulariser	O
that	O
penalises	O
the	O
sum	O
square	O
length	O
of	O
the	O
weights	O
λwtw	O
=	O
λ	O
w2	O
i	O
(	O
17.2.18	O
)	O
i	O
which	O
corresponds	O
to	O
setting	O
r	O
=	O
λi	O
.	O
regularising	O
pararameters	O
such	O
as	O
λ	O
,	O
γ	O
may	O
be	O
determined	O
using	O
a	O
validation	B
set	O
,	O
section	O
(	O
13.2.3	O
)	O
.	O
a	O
popular	O
lpm	O
is	O
given	O
by	O
the	O
non-linear	B
function	O
φ	O
(	O
x	O
)	O
with	O
components	O
17.2.3	O
radial	B
basis	I
functions	I
(	O
cid:18	O
)	O
1	O
2α2	O
−	O
(	O
cid:0	O
)	O
x	O
−	O
mi	O
(	O
cid:1	O
)	O
2	O
(	O
cid:19	O
)	O
φi	O
(	O
x	O
)	O
=	O
exp	O
(	O
17.2.19	O
)	O
these	O
basis	O
functions	O
are	O
bump	O
shaped	O
,	O
with	O
the	O
center	O
of	O
the	O
bump	O
i	O
being	O
given	O
by	O
mi	O
and	O
the	O
width	O
by	O
α.	O
an	O
example	O
is	O
given	O
in	O
ﬁg	O
(	O
17.5	O
)	O
in	O
which	O
several	O
rbfs	O
are	O
plotted	O
with	O
diﬀerent	O
centres	O
.	O
in	O
lpm	O
regression	B
,	O
we	O
can	O
then	O
use	O
a	O
linear	B
combination	O
of	O
these	O
‘	O
bumps	O
’	O
to	O
ﬁt	O
the	O
data	B
.	O
one	O
can	O
apply	O
the	O
same	O
approach	B
using	O
vector	O
inputs	O
.	O
for	O
vector	O
x	O
and	O
centre	O
m	O
,	O
the	O
radial	O
basis	O
function	O
depends	O
on	O
the	O
distance	O
between	O
x	O
and	O
the	O
centre	O
m	O
,	O
giving	O
a	O
‘	O
bump	O
’	O
in	O
input	O
space	O
,	O
ﬁg	O
(	O
17.8	O
)	O
.	O
example	O
79	O
(	O
setting	O
α	O
)	O
.	O
consider	O
ﬁtting	O
the	O
data	B
in	O
ﬁg	O
(	O
17.6	O
)	O
using	O
16	O
radial	B
basis	I
functions	I
uniformly	O
spread	O
over	O
the	O
input	O
space	O
,	O
with	O
width	O
parameter	B
α	O
and	O
regularising	O
term	O
λwtw	O
.	O
the	O
generalisation	B
performance	O
on	O
the	O
test	O
data	O
depends	O
heavily	O
on	O
the	O
width	O
and	O
regularising	O
parameter	B
λ.	O
in	O
order	O
to	O
ﬁnd	O
reasonable	O
values	O
for	O
these	O
parameters	O
we	O
may	O
use	O
a	O
validation	B
set	O
.	O
for	O
simplicity	O
we	O
set	O
the	O
regularisation	B
parameter	O
to	O
λ	O
=	O
0.0001	O
and	O
use	O
the	O
validation	B
set	O
to	O
determine	O
a	O
suitable	O
α.	O
in	O
ﬁg	O
(	O
17.7	O
)	O
we	O
plot	O
the	O
validation	B
error	O
as	O
a	O
function	B
of	O
α.	O
based	O
on	O
this	O
graph	B
,	O
we	O
can	O
ﬁnd	O
the	O
best	O
value	B
of	O
α	O
;	O
that	O
which	O
minimises	O
the	O
validation	B
error	O
.	O
the	O
predictions	O
are	O
also	O
given	O
in	O
ﬁg	O
(	O
17.6	O
)	O
.	O
draft	O
march	O
9	O
,	O
2010	O
315	O
00.20.40.60.8100.10.20.30.40.50.60.70.80.91x	O
the	O
dual	B
representation	I
and	O
kernels	O
figure	O
17.6	O
:	O
the	O
×	O
are	O
the	O
training	B
points	O
,	O
and	O
the	O
+	O
are	O
the	O
validation	B
points	O
.	O
the	O
solid	O
line	O
is	O
the	O
correct	O
underlying	O
function	B
sin	O
(	O
10x	O
)	O
which	O
is	O
corrupted	O
with	O
a	O
small	O
amount	O
of	O
additive	O
noise	O
to	O
form	O
the	O
train	O
data	O
.	O
the	O
dashed	O
line	O
is	O
the	O
best	O
predictor	O
based	O
on	O
the	O
validation	B
set	O
.	O
a	O
curse	B
of	I
dimensionality	I
if	O
the	O
data	B
has	O
non-trivial	O
behaviour	O
over	O
some	O
region	O
in	O
x	O
,	O
then	O
we	O
need	O
to	O
cover	O
the	O
region	O
of	O
x	O
space	O
fairly	O
densely	O
with	O
‘	O
bump	O
’	O
type	O
functions	O
.	O
in	O
the	O
above	O
case	O
,	O
we	O
used	O
16	O
basis	O
functions	O
for	O
this	O
one	O
dimensional	O
space	O
.	O
in	O
2	O
dimensions	O
if	O
we	O
wish	O
to	O
cover	O
each	O
dimension	O
to	O
the	O
same	O
discretisation	O
level	O
,	O
we	O
would	O
need	O
162	O
=	O
256	O
basis	O
functions	O
.	O
similarly	O
,	O
for	O
10	O
dimensions	O
we	O
would	O
need	O
1610	O
≈	O
1012	O
functions	O
.	O
to	O
ﬁt	O
such	O
an	O
lpm	O
would	O
require	O
solving	B
a	O
linear	B
system	O
in	O
more	O
than	O
1012	O
variables	O
.	O
this	O
explosion	O
in	O
the	O
number	O
of	O
basis	O
functions	O
with	O
the	O
input	O
dimension	O
is	O
a	O
‘	O
curse	B
of	I
dimensionality	I
’	O
.	O
a	O
possible	O
remedy	O
is	O
to	O
make	O
the	O
basis	O
functions	O
very	O
broad	O
so	O
that	O
each	O
covers	O
more	O
of	O
the	O
high	O
dimen-	O
sional	O
space	O
.	O
however	O
,	O
this	O
will	O
mean	B
a	O
lack	O
of	O
ﬂexibility	O
of	O
the	O
ﬁtted	O
function	B
since	O
it	O
is	O
constrained	O
to	O
be	O
smooth	O
.	O
another	O
approach	B
is	O
to	O
place	O
basis	O
functions	O
centred	O
on	O
the	O
training	B
input	O
points	O
and	O
add	O
some	O
more	O
basis	O
functions	O
randomly	O
placed	O
close	O
to	O
the	O
training	B
inputs	O
.	O
the	O
rational	O
behind	O
this	O
is	O
that	O
when	O
we	O
come	O
to	O
do	O
prediction	O
,	O
we	O
will	O
most	O
likely	O
see	O
novel	O
x	O
that	O
are	O
close	O
to	O
the	O
training	B
points	O
–	O
we	O
do	O
not	O
need	O
to	O
make	O
‘	O
accurate	O
’	O
predictions	O
over	O
all	O
the	O
space	O
.	O
a	O
further	O
approach	B
is	O
to	O
make	O
the	O
positions	O
of	O
the	O
basis	O
functions	O
adaptive	O
,	O
allowing	O
them	O
to	O
be	O
moved	O
around	O
in	O
the	O
space	O
to	O
minimise	O
the	O
error	O
.	O
this	O
approach	B
motivates	O
the	O
neural	B
network	I
models	O
[	O
41	O
]	O
.	O
an	O
alternative	O
is	O
to	O
reexpress	O
the	O
problem	B
of	O
ﬁtting	O
an	O
lpm	O
by	O
reparameterising	O
the	O
problem	B
,	O
as	O
discussed	O
below	O
.	O
17.3	O
the	O
dual	B
representation	I
and	O
kernels	O
consider	O
a	O
set	O
of	O
training	B
data	O
with	O
inputs	O
,	O
x	O
=	O
{	O
xn	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
and	O
corresponding	O
outputs	O
yn	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
for	O
an	O
lpm	O
of	O
the	O
form	O
f	O
(	O
x	O
)	O
=	O
wtx	O
(	O
17.3.1	O
)	O
our	O
interest	O
is	O
to	O
ﬁnd	O
the	O
‘	O
best	O
ﬁt	O
’	O
parameters	O
w.	O
we	O
assume	O
that	O
we	O
have	O
found	O
an	O
optimal	O
parameter	B
w∗	O
.	O
the	O
nullspace	O
of	O
x	O
are	O
those	O
x⊥	O
which	O
are	O
orthogonal	B
to	O
all	O
the	O
inputs	O
in	O
x	O
.	O
that	O
is	O
,	O
(	O
17.3.2	O
)	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
xn	O
=	O
0	O
,	O
x⊥	O
(	O
cid:17	O
)	O
t	O
w∗	O
+	O
x⊥	O
(	O
cid:17	O
)	O
t	O
for	O
all	O
n.	O
if	O
we	O
then	O
consider	O
the	O
vector	O
w∗	O
with	O
an	O
additional	O
component	O
in	O
the	O
direction	O
orthogonal	O
to	O
the	O
space	O
spanned	O
by	O
x	O
,	O
xn	O
=	O
wt∗	O
xn	O
(	O
17.3.3	O
)	O
figure	O
17.7	O
:	O
the	O
validation	B
error	O
as	O
a	O
function	B
of	O
the	O
basis	O
function	B
width	O
for	O
the	O
validation	B
data	O
in	O
ﬁg	O
(	O
17.6	O
)	O
and	O
rbfs	O
in	O
ﬁg	O
(	O
17.5	O
)	O
.	O
based	O
on	O
the	O
validation	B
error	O
,	O
the	O
optimal	O
setting	O
of	O
the	O
basis	O
function	B
width	O
parameter	B
is	O
α	O
=	O
0.25	O
.	O
316	O
draft	O
march	O
9	O
,	O
2010	O
00.10.20.30.40.50.60.70.80.91−1.5−1−0.500.511.500.20.40.60.81012345678alphavalidation	O
error	O
the	O
dual	B
representation	I
and	O
kernels	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
cid:0	O
)	O
x	O
−	O
m1	O
(	O
cid:1	O
)	O
2	O
(	O
a	O
)	O
:	O
the	O
output	O
of	O
an	O
rbf	O
figure	O
17.8	O
:	O
/α2	O
)	O
.	O
here	O
m1	O
=	O
function	B
exp	O
(	O
−	O
1	O
(	O
b	O
)	O
:	O
the	O
combined	O
(	O
0	O
,	O
0.3	O
)	O
t	O
and	O
α	O
=	O
0.25.	O
output	O
for	O
two	O
rbfs	O
with	O
m1	O
as	O
above	O
and	O
m2	O
=	O
(	O
0.5	O
,	O
−0.5	O
)	O
t	O
.	O
2	O
this	O
means	O
that	O
adding	O
a	O
contribution	O
to	O
w∗	O
outside	O
of	O
the	O
space	O
spanned	O
by	O
x	O
,	O
has	O
no	O
eﬀect	O
on	O
the	O
predictions	O
on	O
the	O
train	O
data	O
.	O
if	O
the	O
training	B
criterion	O
depends	O
only	O
on	O
how	O
well	O
the	O
lpm	O
predicts	O
the	O
train	O
data	O
,	O
there	O
is	O
therefore	O
no	O
need	O
to	O
consider	O
contributions	O
to	O
w	O
from	O
outside	O
of	O
x	O
.	O
that	O
is	O
,	O
without	O
loss	O
of	O
generality	O
we	O
may	O
consider	O
the	O
representation	B
w	O
=	O
anxn	O
(	O
17.3.4	O
)	O
the	O
parameters	O
a	O
=	O
(	O
a1	O
,	O
.	O
.	O
.	O
,	O
an	O
)	O
are	O
called	O
the	O
dual	B
parameters	I
.	O
we	O
can	O
then	O
write	O
the	O
output	O
of	O
the	O
lpm	O
directly	O
in	O
terms	O
of	O
the	O
dual	B
parameters	I
,	O
wtxn	O
=	O
am	O
(	O
xm	O
)	O
t	O
xn	O
(	O
17.3.5	O
)	O
more	O
generally	O
,	O
for	O
a	O
vector	O
function	O
φ	O
(	O
x	O
)	O
,	O
the	O
solution	O
will	O
lie	O
in	O
the	O
space	O
spanned	O
by	O
φ	O
(	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
φ	O
(	O
xn	O
)	O
,	O
n	O
(	O
cid:88	O
)	O
n=1	O
n	O
(	O
cid:88	O
)	O
n	O
(	O
cid:88	O
)	O
m=1	O
w	O
=	O
anφ	O
(	O
xn	O
)	O
n=1	O
and	O
we	O
may	O
write	O
wtφ	O
(	O
xn	O
)	O
=	O
n	O
(	O
cid:88	O
)	O
m=1	O
amφ	O
(	O
xm	O
)	O
t	O
φ	O
(	O
xn	O
)	O
=	O
n	O
(	O
cid:88	O
)	O
m=1	O
amk	O
(	O
xm	O
,	O
xn	O
)	O
where	O
we	O
have	O
deﬁned	O
a	O
kernel	B
function	O
k	O
(	O
xm	O
,	O
xn	O
)	O
≡	O
φ	O
(	O
xm	O
)	O
t	O
φ	O
(	O
xn	O
)	O
≡	O
[	O
k	O
]	O
m	O
,	O
n	O
in	O
matrix	B
form	O
,	O
the	O
output	O
of	O
the	O
lpm	O
on	O
a	O
training	B
input	O
x	O
is	O
then	O
wtφ	O
(	O
xn	O
)	O
=	O
[	O
ka	O
]	O
n	O
=	O
atkn	O
where	O
kn	O
is	O
the	O
nth	O
column	O
of	O
the	O
gram	O
matrix	B
k.	O
17.3.1	O
regression	B
in	O
the	O
dual-space	O
for	O
ordinary	B
least	I
squares	I
regression	O
,	O
using	O
equation	B
(	O
17.3.9	O
)	O
,	O
we	O
have	O
a	O
train	O
error	O
n	O
(	O
cid:88	O
)	O
n=1	O
(	O
cid:16	O
)	O
yn	O
−	O
atkn	O
(	O
cid:17	O
)	O
2	O
e	O
(	O
a	O
)	O
=	O
equation	B
(	O
17.3.10	O
)	O
is	O
analogous	O
to	O
the	O
standard	O
regression	O
equation	B
(	O
17.2.3	O
)	O
on	O
interchanging	O
a	O
for	O
w	O
and	O
kn	O
for	O
φ	O
(	O
xn	O
)	O
.	O
similarly	O
,	O
the	O
regularisation	B
term	O
can	O
be	O
expressed	O
as	O
n	O
(	O
cid:88	O
)	O
n	O
,	O
m=1	O
draft	O
march	O
9	O
,	O
2010	O
wtw	O
=	O
anamφ	O
(	O
xn	O
)	O
φ	O
(	O
xm	O
)	O
=	O
atka	O
(	O
17.3.6	O
)	O
(	O
17.3.7	O
)	O
(	O
17.3.8	O
)	O
(	O
17.3.9	O
)	O
(	O
17.3.10	O
)	O
(	O
17.3.11	O
)	O
317	O
−1−0.500.51−1−0.8−0.6−0.4−0.200.20.40.60.8100.51x	O
(	O
1	O
)	O
x	O
(	O
2	O
)	O
−1−0.500.51−1−0.500.5100.511.5x	O
(	O
1	O
)	O
x	O
(	O
2	O
)	O
the	O
dual	B
representation	I
and	O
kernels	O
by	O
direct	O
analogy	O
the	O
optimal	O
solution	O
for	O
a	O
is	O
therefore	O
a	O
=	O
kn	O
(	O
kn	O
)	O
t	O
+	O
λk	O
ynkn	O
we	O
can	O
express	O
the	O
above	O
solution	O
more	O
conveniently	O
by	O
ﬁrst	O
writing	O
a	O
=	O
k−1kn	O
(	O
kn	O
)	O
t	O
+	O
λi	O
ynk−1kn	O
(	O
cid:32	O
)	O
n	O
(	O
cid:88	O
)	O
(	O
cid:32	O
)	O
n	O
(	O
cid:88	O
)	O
n=1	O
(	O
cid:33	O
)	O
−1	O
n	O
(	O
cid:88	O
)	O
(	O
cid:33	O
)	O
−1	O
n	O
(	O
cid:88	O
)	O
n=1	O
(	O
17.3.12	O
)	O
(	O
17.3.13	O
)	O
n=1	O
n=1	O
since	O
kn	O
is	O
the	O
nth	O
column	O
of	O
k	O
then	O
k−1kn	O
is	O
the	O
nth	O
column	O
of	O
the	O
identity	B
matrix	I
.	O
with	O
a	O
little	O
thought	O
,	O
we	O
can	O
rewrite	O
equation	B
(	O
17.3.13	O
)	O
more	O
simply	O
as	O
a	O
=	O
(	O
k	O
+	O
λi	O
)	O
−1	O
y	O
where	O
y	O
is	O
the	O
vector	O
with	O
components	O
formed	O
from	O
the	O
training	B
inputs	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
.	O
using	O
this	O
,	O
the	O
prediction	B
for	O
a	O
new	O
input	O
x∗	O
is	O
given	O
by	O
y	O
(	O
x∗	O
)	O
=	O
kt∗	O
(	O
k	O
+	O
λi	O
)	O
−1	O
y	O
where	O
the	O
vector	O
k∗	O
has	O
components	O
(	O
17.3.14	O
)	O
(	O
17.3.15	O
)	O
[	O
k∗	O
]	O
m	O
=	O
k	O
(	O
x∗	O
,	O
xm	O
)	O
(	O
17.3.16	O
)	O
this	O
dual	B
space	O
solution	O
shows	O
that	O
predictions	O
can	O
be	O
expressed	O
purely	O
in	O
terms	O
of	O
the	O
kernel	B
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
.	O
this	O
means	O
that	O
we	O
may	O
dispense	O
with	O
deﬁning	O
the	O
vector	O
functions	O
φ	O
(	O
x	O
)	O
and	O
deﬁne	O
a	O
kernel	B
function	O
di-	O
rectly	O
.	O
this	O
approach	B
is	O
also	O
used	O
in	O
gaussian	O
processes	O
,	O
chapter	O
(	O
19	O
)	O
and	O
enables	O
us	O
to	O
use	O
eﬀectively	O
very	O
large	O
(	O
even	O
inﬁnite	O
)	O
dimensional	O
vectors	O
φ	O
without	O
ever	O
explicitly	O
needing	O
to	O
compute	O
them	O
.	O
note	O
that	O
the	O
gram	O
matrix	B
k	O
has	O
dimension	O
n	O
×	O
n	O
,	O
which	O
means	O
that	O
the	O
computational	B
complexity	I
of	O
performing	O
the	O
be	O
prohibitively	O
expensive	O
,	O
and	O
numerical	B
approximations	O
are	O
required	O
.	O
this	O
is	O
in	O
contrast	O
to	O
the	O
compu-	O
tational	O
complexity	O
of	O
solving	B
the	O
normal	B
equations	I
in	O
the	O
original	O
weight	O
space	O
viewpoint	O
is	O
o	O
.	O
matrix	B
inversion	O
in	O
equation	B
(	O
17.3.16	O
)	O
is	O
o	O
(	O
cid:0	O
)	O
n	O
3	O
(	O
cid:1	O
)	O
.	O
for	O
moderate	O
to	O
large	O
n	O
(	O
greater	O
than	O
5000	O
)	O
,	O
this	O
will	O
dim	O
(	O
φ	O
)	O
3	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
to	O
an	O
extent	O
,	O
the	O
dual	B
parameterisation	O
helps	O
us	O
with	O
the	O
curse	B
of	I
dimensionality	I
since	O
the	O
complexity	O
of	O
learning	B
in	O
the	O
dual	B
parameterisation	O
scales	O
cubically	O
with	O
the	O
number	O
of	O
training	B
points	O
–	O
not	O
cubically	O
with	O
the	O
dimension	O
of	O
the	O
φ	O
vector	O
.	O
17.3.2	O
positive	B
deﬁnite	I
kernels	O
(	O
covariance	B
functions	O
)	O
the	O
kernel	B
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
in	O
(	O
17.3.8	O
)	O
was	O
deﬁned	O
as	O
the	O
scalar	B
product	I
between	O
two	O
vectors	O
φ	O
(	O
x	O
)	O
and	O
φ	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
.	O
for	O
any	O
set	O
of	O
points	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
,	O
the	O
resulting	O
matrix	B
[	O
k	O
]	O
m	O
,	O
n	O
=	O
φ	O
(	O
xm	O
)	O
tφ	O
(	O
xn	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
zmkmnzn	O
=	O
(	O
cid:88	O
)	O
ztkz	O
=	O
(	O
cid:88	O
)	O
i	O
φm	O
i	O
φn	O
i	O
is	O
positive	O
semi-deﬁnite	O
since	O
for	O
any	O
z	O
(	O
cid:33	O
)	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
(	O
cid:33	O
)	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
=	O
(	O
cid:88	O
)	O
znφn	O
i	O
zmφm	O
i	O
(	O
17.3.17	O
)	O
(	O
cid:33	O
)	O
2	O
zmφm	O
i	O
≥	O
0	O
(	O
17.3.18	O
)	O
m	O
,	O
n	O
i	O
m	O
n	O
i	O
m	O
instead	O
of	O
specifying	O
high-dimensional	O
φ	O
(	O
x	O
)	O
vectors	O
,	O
we	O
may	O
instead	O
specify	O
a	O
function	B
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
that	O
produces	O
a	O
positive	B
deﬁnite	I
matrix	O
k	O
for	O
any	O
inputs	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
.	O
such	O
a	O
function	B
is	O
called	O
a	O
covariance	B
function	I
,	O
or	O
a	O
positive	O
kernel	O
.	O
for	O
example	O
a	O
popular	O
choice	O
is	O
−λ|x−x	O
(	O
cid:48	O
)	O
|ν	O
e	O
,	O
0	O
<	O
ν	O
≤	O
2	O
,	O
λ	O
≥	O
0	O
(	O
17.3.19	O
)	O
for	O
ν	O
=	O
2	O
this	O
is	O
commonly	O
called	O
the	O
squared	B
exponential	I
kernel	O
.	O
for	O
ν	O
=	O
1	O
this	O
is	O
known	O
as	O
the	O
ornstein-uhlenbeck	O
kernel	B
.	O
covariance	B
functions	O
are	O
discussed	O
in	O
more	O
detail	O
in	O
section	O
(	O
19.3	O
)	O
.	O
318	O
draft	O
march	O
9	O
,	O
2010	O
linear	O
parameter	O
models	O
for	O
classiﬁcation	B
figure	O
17.9	O
:	O
the	O
logistic	B
sigmoid	I
function	O
σβ	O
(	O
x	O
)	O
=	O
1/	O
(	O
1+e−βx	O
)	O
.	O
the	O
parameter	B
β	O
determines	O
the	O
steepness	O
of	O
the	O
sigmoid	B
.	O
the	O
full	O
(	O
blue	O
)	O
line	O
is	O
for	O
β	O
=	O
1	O
and	O
the	O
dashed	O
(	O
red	O
)	O
for	O
β	O
=	O
10.	O
as	O
β	O
→	O
∞	O
,	O
the	O
logistic	B
sigmoid	I
tends	O
to	O
a	O
heaviside	O
step	O
func-	O
tion	O
.	O
the	O
dotted	O
curve	O
(	O
magenta	O
)	O
is	O
the	O
error	B
function	I
(	O
probit	B
)	O
0.5	O
(	O
1	O
+	O
erf	O
(	O
λx	O
)	O
)	O
for	O
λ	O
=	O
√π/4	O
,	O
which	O
closely	O
matches	O
the	O
stan-	O
dard	O
logistic	B
sigmoid	I
with	O
β	O
=	O
1	O
.	O
17.4	O
linear	O
parameter	O
models	O
for	O
classiﬁcation	B
in	O
a	O
binary	O
classiﬁcation	O
problem	B
we	O
are	O
given	O
some	O
training	B
data	O
,	O
d	O
=	O
{	O
(	O
xn	O
,	O
cn	O
)	O
,	O
n	O
=	O
1	O
.	O
.	O
.	O
,	O
n	O
}	O
,	O
where	O
the	O
targets	O
c	O
∈	O
{	O
0	O
,	O
1	O
}	O
.	O
inspired	O
by	O
the	O
lpm	O
regression	B
model	O
,	O
we	O
can	O
assign	O
the	O
probability	B
that	O
a	O
novel	O
input	O
x	O
belongs	O
to	O
class	O
1	O
using	O
p	O
(	O
c	O
=	O
1|x	O
)	O
=	O
f	O
(	O
xtw	O
)	O
(	O
17.4.1	O
)	O
where	O
0	O
≤	O
f	O
(	O
x	O
)	O
≤	O
1.	O
in	O
the	O
statistics	O
literature	O
,	O
f	O
(	O
x	O
)	O
is	O
termed	O
a	O
mean	B
function	O
–	O
the	O
inverse	O
function	O
f−1	O
(	O
x	O
)	O
is	O
the	O
link	O
function	B
.	O
two	O
popular	O
choices	O
for	O
the	O
function	B
f	O
(	O
x	O
)	O
are	O
the	O
and	O
probit	B
functions	O
.	O
the	O
logit	B
is	O
given	O
by	O
f	O
(	O
x	O
)	O
=	O
ex	O
1	O
+	O
ex	O
=	O
1	O
1	O
+	O
e−x	O
(	O
17.4.2	O
)	O
which	O
is	O
also	O
called	O
the	O
logistic	B
sigmoid	I
and	O
written	O
σ	O
(	O
x	O
)	O
,	O
ﬁg	O
(	O
17.9	O
)	O
.	O
the	O
scaled	O
version	O
is	O
deﬁned	O
as	O
σβ	O
(	O
x	O
)	O
=	O
σ	O
(	O
βx	O
)	O
(	O
17.4.3	O
)	O
a	O
closely	O
related	O
model	B
is	O
probit	B
regression	I
which	O
uses	O
in	O
place	O
of	O
the	O
logistic	B
sigmoid	I
the	O
error	B
function	I
the	O
cumulative	O
distribution	B
of	O
the	O
standard	B
normal	I
distribution	I
f	O
(	O
x	O
)	O
=	O
1	O
√2π	O
−	O
1	O
2	O
t2	O
e	O
dt	O
=	O
1	O
2	O
(	O
1	O
+	O
erf	O
(	O
x	O
)	O
)	O
−∞	O
this	O
can	O
also	O
be	O
written	O
in	O
terms	O
of	O
the	O
standard	O
error	O
function	B
,	O
(	O
cid:90	O
)	O
x	O
(	O
cid:90	O
)	O
x	O
0	O
erf	O
(	O
x	O
)	O
≡	O
2	O
√π	O
−t2	O
e	O
dt	O
(	O
17.4.4	O
)	O
(	O
17.4.5	O
)	O
the	O
shape	O
of	O
the	O
probit	B
and	O
logistic	B
functions	O
are	O
similar	O
under	O
rescaling	O
,	O
see	O
ﬁg	O
(	O
17.9	O
)	O
.	O
we	O
focus	O
below	O
on	O
the	O
logit	B
function	O
.	O
similar	O
derivations	O
carry	O
over	O
in	O
a	O
straightforward	O
manner	O
to	O
any	O
monotonic	O
mean	B
function	O
.	O
17.4.1	O
logistic	B
regression	I
logistic	O
regression	B
corresponds	O
to	O
the	O
model	B
p	O
(	O
c	O
=	O
1|x	O
)	O
=	O
σ	O
(	O
b	O
+	O
xtw	O
)	O
(	O
17.4.6	O
)	O
where	O
b	O
is	O
a	O
scalar	O
,	O
and	O
w	O
is	O
a	O
vector	O
.	O
as	O
the	O
argument	O
b	O
+	O
xtw	O
of	O
the	O
sigmoid	B
function	I
increases	O
,	O
the	O
probability	B
x	O
belongs	O
to	O
class	O
1	O
increases	O
.	O
the	O
decision	B
boundary	I
the	O
decision	B
boundary	I
is	O
deﬁned	O
as	O
that	O
set	O
of	O
x	O
for	O
which	O
p	O
(	O
c	O
=	O
1|x	O
)	O
=	O
p	O
(	O
c	O
=	O
0|x	O
)	O
=	O
0.5.	O
this	O
is	O
given	O
by	O
the	O
hyperplane	B
b	O
+	O
xtw	O
=	O
0	O
draft	O
march	O
9	O
,	O
2010	O
(	O
17.4.7	O
)	O
319	O
−5−4−3−2−101234500.10.20.30.40.50.60.70.80.91	O
linear	O
parameter	O
models	O
for	O
classiﬁcation	B
w	O
figure	O
17.10	O
:	O
the	O
decision	B
boundary	I
p	O
(	O
c	O
=	O
1|x	O
)	O
=	O
0.5	O
(	O
solid	O
line	O
)	O
.	O
for	O
two	O
dimensional	O
data	B
,	O
the	O
decision	B
boundary	I
is	O
a	O
line	O
.	O
if	O
all	O
the	O
training	B
data	O
for	O
class	O
1	O
(	O
ﬁlled	O
circles	O
)	O
lie	O
on	O
one	O
side	O
of	O
the	O
line	O
,	O
and	O
for	O
class	O
0	O
(	O
open	O
circles	O
)	O
on	O
the	O
other	O
,	O
the	O
data	B
is	O
said	O
to	O
be	O
linearly	B
separable	I
.	O
on	O
the	O
side	O
of	O
the	O
hyperplane	B
for	O
which	O
b	O
+	O
xtw	O
>	O
0	O
,	O
inputs	O
x	O
are	O
classiﬁed	O
as	O
1	O
’	O
s	O
,	O
and	O
on	O
the	O
other	O
side	O
they	O
are	O
classiﬁed	O
as	O
0	O
’	O
s	O
.	O
the	O
‘	O
bias	B
’	O
parameter	B
b	O
simply	O
shifts	O
the	O
decision	B
boundary	I
by	O
a	O
constant	O
amount	O
.	O
the	O
orientation	O
of	O
the	O
decision	B
boundary	I
is	O
determined	O
by	O
w	O
,	O
the	O
normal	B
to	O
the	O
hyperplane	B
,	O
see	O
ﬁg	O
(	O
17.10	O
)	O
.	O
to	O
clarify	O
the	O
geometric	O
interpretation	O
,	O
let	O
x	O
be	O
a	O
point	O
on	O
the	O
decision	B
boundary	I
and	O
consider	O
a	O
new	O
point	O
x∗	O
=	O
x	O
+	O
w⊥	O
,	O
where	O
w⊥	O
is	O
a	O
vector	O
perpendicular	O
to	O
w	O
,	O
so	O
that	O
wtw⊥	O
=	O
0.	O
then	O
b	O
+	O
wtx∗	O
=	O
b	O
+	O
wt	O
(	O
cid:16	O
)	O
x	O
+	O
w⊥	O
(	O
cid:17	O
)	O
=	O
b	O
+	O
wtx	O
+	O
wtw⊥	O
=	O
b	O
+	O
wtx	O
=	O
0	O
(	O
17.4.8	O
)	O
thus	O
if	O
x	O
is	O
on	O
the	O
decision	B
boundary	I
,	O
so	O
is	O
x	O
plus	O
any	O
vector	O
perpendicular	O
to	O
w.	O
in	O
d	O
dimensions	O
,	O
the	O
space	O
of	O
vectors	O
that	O
are	O
perpendicular	O
to	O
w	O
occupy	O
a	O
d	O
−	O
1	O
dimensional	O
hyperplane	B
.	O
for	O
example	O
,	O
if	O
the	O
data	B
is	O
two	O
dimensional	O
,	O
the	O
decision	B
boundary	I
is	O
a	O
one	O
dimensional	O
hyperplane	B
,	O
a	O
line	O
,	O
as	O
depicted	O
in	O
ﬁg	O
(	O
17.10	O
)	O
.	O
linear	B
separability	I
and	O
linear	B
independence	O
deﬁnition	O
90	O
(	O
linear	B
separability	I
)	O
.	O
if	O
all	O
the	O
training	B
data	O
for	O
class	O
1	O
lies	O
on	O
one	O
side	O
of	O
a	O
hyperplane	B
,	O
and	O
for	O
class	O
0	O
on	O
the	O
other	O
,	O
the	O
data	B
is	O
said	O
to	O
be	O
linearly	B
separable	I
.	O
for	O
d	O
dimensional	O
data	B
,	O
provided	O
there	O
are	O
no	O
more	O
than	O
d	O
training	B
points	O
,	O
then	O
these	O
are	O
linearly	B
separable	I
provided	O
they	O
are	O
linearly	B
independent	I
.	O
to	O
see	O
this	O
,	O
let	O
cn	O
=	O
+1	O
if	O
xn	O
is	O
in	O
class	O
1	O
,	O
and	O
cn	O
=	O
−1	O
if	O
xn	O
is	O
in	O
class	O
0.	O
for	O
the	O
data	B
to	O
be	O
linearly	B
separable	I
we	O
require	O
wtxn	O
+	O
b	O
=	O
cn	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
(	O
17.4.9	O
)	O
where	O
	O
is	O
an	O
arbitrarily	O
small	O
positive	O
constant	O
.	O
the	O
above	O
equations	O
state	O
that	O
each	O
input	O
is	O
just	O
the	O
correct	O
side	O
of	O
the	O
decision	B
boundary	I
.	O
if	O
there	O
are	O
n	O
=	O
d	O
datapoints	O
,	O
the	O
above	O
can	O
be	O
written	O
in	O
matrix	B
form	O
as	O
xw	O
+	O
b	O
=	O
c	O
(	O
17.4.10	O
)	O
where	O
x	O
is	O
a	O
square	O
matrix	B
whose	O
nth	O
column	O
contains	O
xn	O
.	O
provided	O
that	O
x	O
is	O
invertible	O
the	O
solution	O
is	O
w	O
=	O
x−1	O
(	O
c	O
−	O
b	O
)	O
(	O
17.4.11	O
)	O
the	O
bias	B
b	O
can	O
be	O
set	O
arbitrarily	O
.	O
this	O
shows	O
that	O
provided	O
the	O
xn	O
are	O
linearly	B
independent	I
,	O
we	O
can	O
always	O
ﬁnd	O
a	O
hyperplane	B
that	O
linearly	O
separates	O
the	O
data	B
.	O
provided	O
the	O
data	B
are	O
not-collinear	O
(	O
all	O
occupying	O
the	O
same	O
d	O
−	O
1	O
dimensional	O
subspace	O
)	O
the	O
bias	B
can	O
be	O
used	O
to	O
improve	O
this	O
to	O
enabling	O
d	O
+	O
1	O
arbitrarily	O
labelled	B
points	O
to	O
be	O
linearly	O
separated	O
in	O
d	O
dimensions	O
.	O
a	O
dataset	O
that	O
is	O
not	O
linearly	B
separable	I
is	O
given	O
by	O
the	O
following	O
four	O
training	B
points	O
and	O
class	O
labels	O
{	O
(	O
[	O
0	O
,	O
0	O
]	O
,	O
0	O
)	O
,	O
(	O
[	O
0	O
,	O
1	O
]	O
,	O
1	O
)	O
,	O
(	O
[	O
1	O
,	O
0	O
]	O
,	O
1	O
)	O
,	O
(	O
[	O
1	O
,	O
1	O
]	O
,	O
0	O
)	O
}	O
320	O
(	O
17.4.12	O
)	O
draft	O
march	O
9	O
,	O
2010	O
linear	O
parameter	O
models	O
for	O
classiﬁcation	B
figure	O
17.11	O
:	O
the	O
xor	O
problem	B
.	O
this	O
is	O
not	O
linearly	B
separable	I
.	O
this	O
data	B
represents	O
the	O
xor	O
function	B
,	O
and	O
is	O
plotted	O
in	O
ﬁg	O
(	O
17.11	O
)	O
.	O
this	O
function	B
is	O
not	O
linearly	B
separable	I
since	O
no	O
straight	O
line	O
has	O
all	O
inputs	O
from	O
one	O
class	O
on	O
one	O
side	O
and	O
the	O
other	O
class	O
on	O
the	O
other	O
.	O
classifying	O
data	B
which	O
is	O
not	O
linearly	B
separable	I
can	O
only	O
be	O
achieved	O
using	O
a	O
non-linear	B
decision	O
boundary	B
.	O
it	O
might	O
be	O
that	O
data	B
is	O
non-linearly	O
separable	O
in	O
the	O
original	O
data	B
space	O
.	O
however	O
,	O
by	O
mapping	O
to	O
a	O
higher	O
dimension	O
using	O
a	O
non-linear	B
vector	O
function	B
,	O
we	O
generate	O
a	O
set	O
of	O
non-linearly	O
dependent	O
high-	O
dimensional	O
vectors	O
,	O
which	O
can	O
then	O
be	O
separated	O
using	O
a	O
high-dimensional	O
hyperplane	B
.	O
we	O
will	O
discuss	O
this	O
in	O
section	O
(	O
17.5	O
)	O
.	O
the	O
perceptron	B
the	O
perceptron	B
assigns	O
x	O
to	O
class	O
1	O
if	O
b	O
+	O
wtx	O
≥	O
0	O
,	O
and	O
to	O
class	O
0	O
otherwise	O
.	O
that	O
is	O
p	O
(	O
c	O
=	O
1|x	O
)	O
=	O
θ	O
(	O
b	O
+	O
xtw	O
)	O
where	O
the	O
step	O
function	B
is	O
deﬁned	O
as	O
θ	O
(	O
x	O
)	O
=	O
if	O
we	O
consider	O
the	O
logistic	B
regression	I
model	O
(	O
cid:17	O
)	O
p	O
(	O
c	O
=	O
1|x	O
)	O
=	O
σβ	O
b	O
+	O
xtw	O
(	O
cid:26	O
)	O
1	O
x	O
>	O
0	O
0	O
x	O
≤	O
0	O
(	O
cid:16	O
)	O
	O
1	O
and	O
take	O
the	O
limit	O
β	O
→	O
∞	O
,	O
we	O
have	O
the	O
perceptron	B
like	O
classiﬁer	B
p	O
(	O
c	O
=	O
1|x	O
)	O
=	O
b	O
+	O
xtw	O
>	O
0	O
0.5	O
b	O
+	O
xtw	O
=	O
0	O
b	O
+	O
xtw	O
<	O
0	O
0	O
(	O
17.4.13	O
)	O
(	O
17.4.14	O
)	O
(	O
17.4.15	O
)	O
(	O
17.4.16	O
)	O
the	O
only	O
diﬀerence	O
between	O
this	O
‘	O
probabilistic	B
perceptron	O
’	O
and	O
the	O
standard	O
perceptron	O
is	O
in	O
the	O
technical	O
deﬁnition	O
of	O
the	O
value	B
of	O
the	O
step	O
function	B
at	O
0.	O
the	O
perceptron	B
may	O
therefore	O
essentially	O
be	O
viewed	O
as	O
a	O
limiting	O
case	O
of	O
logistic	B
regression	I
.	O
17.4.2	O
maximum	B
likelihood	I
training	O
given	O
a	O
data	B
set	O
d	O
,	O
how	O
can	O
we	O
learn	O
the	O
weights	O
to	O
obtain	O
good	O
classiﬁcation	B
?	O
probabilistically	O
,	O
if	O
we	O
assume	O
that	O
each	O
data	B
point	O
has	O
been	O
drawn	O
independently	O
from	O
the	O
same	O
distribution	B
that	O
generates	O
the	O
data	B
(	O
the	O
standard	O
i.i.d	O
.	O
assumption	O
)	O
,	O
the	O
likelihood	B
of	O
the	O
observed	O
data	O
is	O
(	O
writing	O
explicitly	O
the	O
conditional	B
dependence	O
on	O
the	O
parameters	O
b	O
,	O
w	O
)	O
p	O
(	O
d|b	O
,	O
w	O
)	O
=	O
p	O
(	O
cn|xn	O
,	O
b	O
,	O
w	O
)	O
p	O
(	O
xn	O
)	O
=	O
p	O
(	O
c	O
=	O
1|xn	O
,	O
b	O
,	O
w	O
)	O
cn	O
(	O
1	O
−	O
p	O
(	O
c	O
=	O
1|xn	O
,	O
b	O
,	O
w	O
)	O
)	O
1−cn	O
p	O
(	O
xn	O
)	O
(	O
17.4.17	O
)	O
where	O
we	O
have	O
used	O
the	O
fact	O
that	O
cn	O
∈	O
{	O
0	O
,	O
1	O
}	O
.	O
for	O
this	O
discriminative	B
model	O
,	O
we	O
do	O
not	O
model	B
the	O
input	O
distribution	B
p	O
(	O
x	O
)	O
so	O
that	O
we	O
may	O
equivalently	O
consider	O
the	O
log	O
likelihood	B
of	O
the	O
output	O
class	O
variables	O
conditioned	O
on	O
the	O
training	B
inputs	O
:	O
for	O
logistic	B
regression	I
this	O
gives	O
n	O
(	O
cid:89	O
)	O
n=1	O
n	O
(	O
cid:89	O
)	O
n=1	O
n	O
(	O
cid:88	O
)	O
n=1	O
l	O
(	O
w	O
,	O
b	O
)	O
=	O
cn	O
log	O
σ	O
(	O
b	O
+	O
wtxn	O
)	O
+	O
(	O
1	O
−	O
cn	O
)	O
log	O
draft	O
march	O
9	O
,	O
2010	O
(	O
17.4.18	O
)	O
321	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
1	O
−	O
σ	O
(	O
b	O
+	O
wtxn	O
)	O
gradient	B
ascent	O
linear	O
parameter	O
models	O
for	O
classiﬁcation	B
there	O
is	O
no	O
closed	O
form	O
solution	O
to	O
the	O
maximisation	B
of	O
l	O
(	O
w	O
,	O
b	O
)	O
which	O
needs	O
to	O
be	O
carried	O
out	O
numerically	O
.	O
one	O
of	O
the	O
simplest	O
methods	O
is	O
gradient	B
ascent	O
for	O
which	O
the	O
gradient	B
is	O
given	O
by	O
n	O
(	O
cid:88	O
)	O
n=1	O
n	O
(	O
cid:88	O
)	O
n=1	O
∇wl	O
=	O
(	O
cn	O
−	O
σ	O
(	O
wtxn	O
+	O
b	O
)	O
)	O
xn	O
here	O
we	O
made	O
use	O
of	O
the	O
derivative	O
relation	O
dσ	O
(	O
x	O
)	O
/dx	O
=	O
σ	O
(	O
x	O
)	O
(	O
1	O
−	O
σ	O
(	O
x	O
)	O
)	O
dl	O
db	O
=	O
(	O
cn	O
−	O
σ	O
(	O
wtxn	O
+	O
b	O
)	O
)	O
for	O
the	O
logistic	B
sigmoid	I
.	O
the	O
derivative	O
with	O
respect	O
to	O
the	O
bias	B
is	O
the	O
gradient	B
ascent	O
procedure	O
then	O
corresponds	O
to	O
updating	O
the	O
weights	O
and	O
bias	B
using	O
wnew	O
=	O
w	O
+	O
η∇wl	O
,	O
bnew	O
=	O
b	O
+	O
η	O
dl	O
db	O
where	O
η	O
,	O
the	O
learning	B
rate	I
is	O
a	O
scalar	O
chosen	O
small	O
enough	O
to	O
ensure	O
convergence	O
.	O
the	O
application	O
of	O
the	O
above	O
rule	O
will	O
lead	O
to	O
a	O
gradual	O
increase	O
in	O
the	O
log	O
likelihood	B
.	O
batch	B
training	O
writing	O
the	O
updates	O
(	O
17.4.22	O
)	O
explicitly	O
gives	O
wnew	O
=	O
w	O
+	O
η	O
(	O
cn	O
−	O
σ	O
(	O
wtxn	O
+	O
b	O
)	O
)	O
xn	O
,	O
n	O
(	O
cid:88	O
)	O
n=1	O
n	O
(	O
cid:88	O
)	O
bnew	O
=	O
b	O
+	O
η	O
(	O
cn	O
−	O
σ	O
(	O
wtxn	O
+	O
b	O
)	O
)	O
n=1	O
(	O
17.4.23	O
)	O
this	O
is	O
called	O
a	O
batch	B
update	I
since	O
the	O
parameters	O
w	O
and	O
b	O
are	O
updated	O
only	O
after	O
passing	B
through	O
the	O
whole	O
(	O
batch	B
)	O
of	O
training	B
data	O
.	O
this	O
batch	B
version	O
‘	O
converges	O
’	O
in	O
all	O
cases	O
since	O
the	O
error	O
surface	O
is	O
bowl	O
shaped	O
(	O
see	O
below	O
)	O
.	O
for	O
linearly	B
separable	I
data	O
,	O
however	O
,	O
the	O
optimal	O
setting	O
is	O
for	O
the	O
weights	O
to	O
become	O
inﬁnitely	O
large	O
,	O
since	O
then	O
the	O
logistic	B
sigmoids	O
will	O
saturate	O
to	O
1	O
or	O
0	O
(	O
see	O
below	O
)	O
.	O
a	O
stopping	O
criterion	O
based	O
on	O
either	O
minimal	O
changes	O
to	O
the	O
log	O
likelihood	B
or	O
the	O
parameters	O
is	O
therefore	O
required	O
to	O
halt	O
the	O
optimisation	B
routine	O
.	O
for	O
non-linearly	O
separable	O
data	B
,	O
the	O
likelihood	B
has	O
a	O
maximum	O
at	O
ﬁnite	O
w	O
so	O
the	O
algorithm	B
converges	O
.	O
however	O
,	O
the	O
predictions	O
will	O
be	O
less	O
certain	O
,	O
reﬂected	O
in	O
a	O
broad	O
conﬁdence	O
interval	O
–	O
see	O
ﬁg	O
(	O
17.12	O
)	O
.	O
in	O
batch	B
training	O
,	O
the	O
zero	O
gradient	B
criterion	O
is	O
(	O
cn	O
−	O
σ	O
(	O
wtxn	O
+	O
b	O
)	O
)	O
xn	O
=	O
0	O
n=1	O
ment	O
that	O
for	O
a	O
zero	O
gradient	B
,	O
cn	O
=	O
σ	O
(	O
cid:0	O
)	O
wtxn	O
+	O
b	O
(	O
cid:1	O
)	O
,	O
meaning	O
that	O
the	O
weights	O
must	O
tend	O
to	O
inﬁnity	O
for	O
this	O
in	O
the	O
case	O
that	O
the	O
inputs	O
xn	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
are	O
linearly	B
independent	I
,	O
we	O
immediately	O
have	O
the	O
require-	O
(	O
17.4.24	O
)	O
condition	O
to	O
hold	O
.	O
for	O
linearly	B
separable	I
data	O
,	O
we	O
can	O
also	O
show	O
that	O
the	O
weights	O
must	O
become	O
inﬁnite	O
at	O
convergence	O
.	O
taking	O
the	O
scalar	B
product	I
of	O
equation	B
(	O
17.4.24	O
)	O
with	O
w	O
,	O
we	O
have	O
the	O
zero	O
gradient	B
requirement	O
n	O
(	O
cid:88	O
)	O
n	O
(	O
cid:88	O
)	O
(	O
17.4.19	O
)	O
(	O
17.4.20	O
)	O
(	O
17.4.21	O
)	O
(	O
17.4.22	O
)	O
n=1	O
(	O
cn	O
−	O
σn	O
)	O
wtxn	O
=	O
0	O
where	O
σn	O
≡	O
σ	O
(	O
cid:0	O
)	O
wtxn	O
+	O
b	O
(	O
cid:1	O
)	O
.	O
for	O
simplicity	O
we	O
assume	O
b	O
=	O
0.	O
for	O
linearly	B
separable	I
data	O
we	O
have	O
(	O
cid:26	O
)	O
>	O
0	O
c	O
=	O
1	O
<	O
0	O
c	O
=	O
0	O
wtxn	O
322	O
(	O
17.4.25	O
)	O
(	O
17.4.26	O
)	O
draft	O
march	O
9	O
,	O
2010	O
linear	O
parameter	O
models	O
for	O
classiﬁcation	B
(	O
a	O
)	O
(	O
b	O
)	O
(	O
cid:26	O
)	O
then	O
,	O
using	O
the	O
fact	O
that	O
0	O
≤	O
σn	O
≤	O
1	O
,	O
we	O
have	O
(	O
cn	O
−	O
σn	O
)	O
wtxn	O
≥	O
0	O
c	O
=	O
1	O
≥	O
0	O
c	O
=	O
0	O
figure	O
17.12	O
:	O
the	O
decision	B
boundary	I
p	O
(	O
c	O
=	O
1|x	O
)	O
=	O
0.5	O
(	O
solid	O
line	O
)	O
and	O
conﬁdence	O
boundaries	O
p	O
(	O
c	O
=	O
1|x	O
)	O
=	O
0.9	O
and	O
p	O
(	O
c	O
=	O
1|x	O
)	O
=	O
0.1	O
and	O
10000	O
itera-	O
(	O
a	O
)	O
:	O
tions	O
of	O
batch	B
gradient	O
ascent	O
with	O
η	O
=	O
0.1	O
.	O
(	O
b	O
)	O
:	O
non-linearly	O
separa-	O
linearly	B
separable	I
data	O
.	O
ble	O
data	B
.	O
note	O
how	O
the	O
conﬁdence	O
interval	O
remains	O
broad	O
,	O
see	O
demologreg.m	O
.	O
(	O
17.4.27	O
)	O
each	O
term	O
(	O
cn	O
−	O
σn	O
)	O
wtxn	O
is	O
non-negative	O
and	O
the	O
zero	O
gradient	B
condition	O
requires	O
the	O
sum	O
of	O
these	O
terms	O
to	O
be	O
zero	O
.	O
this	O
can	O
only	O
happen	O
if	O
all	O
the	O
terms	O
are	O
zero	O
,	O
implying	O
that	O
cn	O
=	O
σn	O
,	O
requiring	O
the	O
sigmoid	B
to	O
saturate	O
,	O
for	O
which	O
the	O
weights	O
must	O
be	O
inﬁnite	O
.	O
online	B
training	O
in	O
practice	O
it	O
is	O
common	O
to	O
update	O
the	O
parameters	O
after	O
each	O
training	B
example	O
has	O
been	O
considered	O
:	O
wnew	O
=	O
w	O
+	O
η	O
n	O
(	O
cn	O
−	O
σ	O
(	O
wtxn	O
+	O
b	O
)	O
)	O
xn	O
,	O
bnew	O
=	O
b	O
+	O
η	O
n	O
(	O
cn	O
−	O
σ	O
(	O
wtxn	O
+	O
b	O
)	O
)	O
(	O
17.4.28	O
)	O
an	O
advantage	O
of	O
online	B
training	O
is	O
that	O
the	O
dataset	O
does	O
not	O
need	O
to	O
be	O
stored	O
since	O
only	O
the	O
performance	B
on	O
the	O
current	O
input	O
is	O
required	O
.	O
provided	O
that	O
the	O
data	B
is	O
linearly	B
separable	I
,	O
the	O
above	O
online	B
procedure	O
converges	O
(	O
provided	O
η	O
is	O
not	O
too	O
large	O
)	O
.	O
however	O
,	O
if	O
the	O
data	B
is	O
not	O
linearly	B
separable	I
,	O
the	O
online	B
version	O
will	O
not	O
converge	O
since	O
the	O
opposing	O
class	O
labels	O
will	O
continually	O
pull	O
the	O
weights	O
one	O
way	O
and	O
then	O
the	O
other	O
as	O
each	O
conﬂicting	O
example	O
is	O
used	O
to	O
form	O
an	O
update	O
.	O
for	O
the	O
limiting	O
case	O
of	O
the	O
perceptron	B
(	O
replacing	O
σ	O
(	O
x	O
)	O
with	O
θ	O
(	O
x	O
)	O
)	O
and	O
linearly	B
separable	I
data	O
,	O
online	B
updating	O
converges	O
in	O
a	O
ﬁnite	O
number	O
of	O
steps	O
[	O
212	O
,	O
41	O
]	O
,	O
but	O
does	O
not	O
converge	O
for	O
non-linearly	O
separable	O
data	B
.	O
geometry	O
of	O
the	O
error	O
surface	O
the	O
hessian	O
of	O
the	O
log	O
likelihood	B
l	O
(	O
w	O
)	O
is	O
the	O
matrix	B
with	O
elements2	O
(	O
cid:88	O
)	O
n	O
=	O
−	O
(	O
cid:88	O
)	O
i	O
,	O
j	O
,	O
n	O
(	O
cid:88	O
)	O
ij	O
hij	O
≡	O
∂2l	O
∂wiwj	O
xn	O
i	O
xn	O
j	O
σn	O
(	O
1	O
−	O
σn	O
)	O
this	O
is	O
negative	O
(	O
semi	O
)	O
deﬁnite	O
since	O
for	O
any	O
z	O
zihijzj	O
=	O
−	O
zixn	O
i	O
zjxn	O
j	O
σn	O
(	O
1	O
−	O
σn	O
)	O
≤	O
−	O
2	O
zixn	O
i	O
	O
(	O
cid:88	O
)	O
i	O
,	O
n	O
≤	O
0	O
(	O
17.4.29	O
)	O
(	O
17.4.30	O
)	O
this	O
means	O
that	O
the	O
error	O
surface	O
is	O
concave	O
(	O
an	O
upside	O
down	O
bowl	O
)	O
and	O
gradient	B
ascent	O
is	O
guaranteed	O
to	O
converge	O
to	O
the	O
optimal	O
solution	O
,	O
provided	O
the	O
learning	B
rate	I
η	O
is	O
small	O
enough	O
.	O
example	O
80	O
(	O
classifying	O
handwritten	B
digits	I
)	O
.	O
we	O
apply	O
logistic	B
regression	I
to	O
the	O
600	O
handwritten	B
digits	I
of	O
example	O
(	O
67	O
)	O
,	O
in	O
which	O
there	O
are	O
300	O
ones	O
and	O
300	O
sevens	O
in	O
the	O
train	O
data	O
.	O
using	O
gradient	B
ascent	O
training	B
with	O
a	O
suitably	O
chosen	O
stopping	O
criterion	O
,	O
the	O
number	O
of	O
errors	O
made	O
on	O
the	O
600	O
test	O
points	O
is	O
12	O
,	O
compared	O
with	O
14	O
errors	O
using	O
nearest	B
neighbour	I
methods	O
.	O
see	O
ﬁg	O
(	O
17.13	O
)	O
for	O
a	O
visualisation	O
of	O
the	O
learned	O
w.	O
2for	O
simplicity	O
we	O
ignore	O
the	O
bias	B
b.	O
this	O
can	O
readily	O
be	O
dealt	O
with	O
by	O
extending	O
x	O
to	O
a	O
d	O
+	O
1	O
dimensional	O
vector	O
ˆx	O
with	O
a	O
1	O
in	O
the	O
d	O
+	O
1	O
component	O
.	O
then	O
for	O
a	O
d	O
+	O
1	O
dimensional	O
ˆw	O
=	O
(	O
w	O
,	O
wd+1	O
)	O
,	O
we	O
have	O
ˆwt	O
ˆx	O
=	O
wtx	O
+	O
wd+1	O
.	O
draft	O
march	O
9	O
,	O
2010	O
323	O
00.20.40.60.8100.10.20.30.40.50.60.70.80.9100.20.40.60.8100.10.20.30.40.50.60.70.80.91	O
the	O
kernel	B
trick	O
for	O
classiﬁcation	B
figure	O
17.13	O
:	O
logistic	B
regression	I
for	O
classifying	O
hand	O
written	O
digits	O
1	O
and	O
7.	O
displayed	O
is	O
a	O
hinton	O
diagram	O
of	O
the	O
784	O
learned	O
weight	B
vector	O
w	O
,	O
plotted	O
as	O
a	O
28	O
×	O
28	O
image	O
for	O
visual	O
interpretation	O
.	O
green	O
are	O
positive	O
and	O
an	O
input	O
x	O
with	O
a	O
(	O
positive	O
)	O
value	B
in	O
this	O
component	O
will	O
tend	O
to	O
increase	O
the	O
probability	B
that	O
the	O
input	O
is	O
classed	O
as	O
a	O
7.	O
similarly	O
,	O
inputs	O
with	O
positive	O
contributions	O
in	O
the	O
red	O
regions	O
tend	O
to	O
increase	O
the	O
probability	B
as	O
being	O
classed	O
as	O
a	O
1	O
digit	O
.	O
note	O
that	O
the	O
elements	O
of	O
each	O
input	O
x	O
are	O
either	O
positive	O
or	O
zero	O
.	O
17.4.3	O
beyond	O
ﬁrst	B
order	I
gradient	O
ascent	O
since	O
the	O
surface	O
has	O
a	O
single	O
optimum	O
,	O
a	O
newton	O
update	O
wnew	O
=	O
wold	O
+	O
ηh−1wold	O
(	O
17.4.31	O
)	O
where	O
h	O
is	O
the	O
hessian	O
matrix	B
as	O
above	O
and	O
0	O
<	O
η	O
<	O
1	O
,	O
will	O
typically	O
converge	O
much	O
faster	O
than	O
gradient	B
ascent	O
.	O
for	O
large	O
scale	O
problems	O
,	O
the	O
inversion	B
of	O
the	O
hessian	O
is	O
computationally	O
demanding	O
and	O
limited	O
memory	O
bfgs	O
or	O
conjugate	B
gradient	I
methods	O
may	O
be	O
considered	O
as	O
more	O
practical	O
alternatives	O
,	O
see	O
section	O
(	O
a.5	O
)	O
.	O
17.4.4	O
avoiding	O
overconﬁdent	O
classiﬁcation	B
provided	O
the	O
data	B
is	O
linearly	B
separable	I
the	O
weights	O
will	O
continue	O
to	O
increase	O
and	O
the	O
classiﬁcations	O
will	O
become	O
extreme	O
.	O
this	O
is	O
undesirable	O
since	O
the	O
classiﬁcations	O
will	O
be	O
over-conﬁdent	O
.	O
this	O
can	O
be	O
prevented	O
by	O
adding	O
a	O
penalty	O
term	O
to	O
the	O
objective	O
function	B
l	O
(	O
cid:48	O
)	O
(	O
w	O
,	O
b	O
)	O
=	O
l	O
(	O
w	O
,	O
b	O
)	O
−	O
αwtw	O
.	O
(	O
17.4.32	O
)	O
the	O
scalar	O
constant	O
α	O
>	O
0	O
encourages	O
smaller	O
values	O
of	O
w	O
(	O
remember	O
that	O
we	O
wish	O
to	O
maximise	O
the	O
log	O
likelihood	B
)	O
.	O
an	O
appropriate	O
value	B
for	O
α	O
can	O
be	O
determined	O
using	O
validation	B
data	O
.	O
17.4.5	O
multiple	B
classes	I
for	O
more	O
than	O
two	O
classes	O
,	O
one	O
may	O
use	O
the	O
softmax	B
function	I
p	O
(	O
c	O
=	O
i|x	O
)	O
=	O
(	O
cid:80	O
)	O
c	O
ewt	O
i	O
x+bi	O
j=1	O
ewt	O
j	O
x+bj	O
(	O
17.4.33	O
)	O
where	O
c	O
is	O
the	O
number	O
of	O
classes	O
.	O
when	O
c	O
=	O
2	O
this	O
reduced	O
to	O
the	O
logistic	B
sigmoid	I
.	O
one	O
can	O
show	O
that	O
the	O
likelihood	B
for	O
this	O
case	O
is	O
also	O
concave	O
,	O
see	O
exercise	O
(	O
176	O
)	O
and	O
[	O
297	O
]	O
.	O
17.5	O
the	O
kernel	B
trick	O
for	O
classiﬁcation	B
a	O
drawback	O
of	O
logistic	B
regression	I
as	O
described	O
above	O
is	O
the	O
simplicity	O
of	O
the	O
decision	O
surface	O
–	O
a	O
hyperplane	B
.	O
one	O
way	O
to	O
extend	O
the	O
method	O
to	O
more	O
complex	O
non-linear	B
decision	O
boundaries	O
is	O
to	O
consider	O
mapping	O
the	O
inputs	O
x	O
in	O
a	O
non-linear	B
way	O
to	O
φ	O
(	O
x	O
)	O
:	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
p	O
(	O
c	O
=	O
1|x	O
)	O
=	O
σ	O
wtφ	O
(	O
x	O
)	O
+	O
b	O
for	O
example	O
,	O
the	O
one-dimensional	O
input	O
x	O
could	O
get	O
mapped	O
to	O
a	O
two	O
dimensional	O
vector	O
(	O
x2	O
,	O
sin	O
(	O
x	O
)	O
)	O
.	O
map-	O
ping	O
into	O
a	O
higher	O
dimensional	O
space	O
makes	O
it	O
easier	O
to	O
ﬁnd	O
a	O
separating	O
hyperplane	B
since	O
any	O
set	O
of	O
points	O
that	O
are	O
linearly	B
independent	I
can	O
be	O
linearly	O
separated	O
provided	O
we	O
have	O
as	O
many	O
dimensions	O
as	O
datapoints	O
.	O
324	O
draft	O
march	O
9	O
,	O
2010	O
(	O
17.5.1	O
)	O
support	O
vector	O
machines	O
figure	O
17.14	O
:	O
logistic	B
regression	I
p	O
(	O
c	O
=	O
1|x	O
)	O
=	O
σ	O
(	O
wtφ	O
(	O
x	O
)	O
)	O
us-	O
ing	O
a	O
quadratic	O
function	O
φ	O
(	O
x	O
)	O
=	O
(	O
1	O
,	O
x1	O
,	O
x2	O
,	O
x2	O
2	O
,	O
x1x2	O
)	O
t.	O
1000	O
iterations	O
of	O
gradient	B
ascent	O
training	B
were	O
performed	O
with	O
a	O
learning	B
rate	I
η	O
=	O
0.1.	O
plotted	O
are	O
the	O
datapoints	O
for	O
the	O
two	O
classes	O
(	O
red	O
cross	B
)	O
and	O
(	O
blue	O
circle	O
)	O
and	O
the	O
equal	O
probability	B
contours	O
.	O
the	O
decision	B
boundary	I
is	O
the	O
0.5-probability	O
contour	O
.	O
see	O
demologregnonlinear.m	O
.	O
1	O
,	O
x2	O
for	O
the	O
maximum	B
likelihood	I
criterion	O
,	O
we	O
can	O
use	O
exactly	O
the	O
same	O
algorithm	B
as	O
before	O
on	O
replacing	O
x	O
with	O
φ	O
(	O
x	O
)	O
.	O
see	O
ﬁg	O
(	O
17.14	O
)	O
for	O
a	O
demonstration	O
using	O
a	O
quadratic	O
function	O
.	O
since	O
only	O
the	O
scalar	B
product	I
between	O
the	O
φ	O
vectors	O
plays	O
a	O
role	O
the	O
dual	B
representation	I
may	O
be	O
used	O
in	O
which	O
we	O
assume	O
the	O
weight	B
can	O
be	O
expressed	O
in	O
the	O
form	O
αnφn	O
(	O
17.5.2	O
)	O
w	O
=	O
(	O
cid:88	O
)	O
n	O
we	O
then	O
subsequently	O
ﬁnd	O
a	O
solution	O
in	O
terms	O
of	O
the	O
dual	B
parameters	I
αn	O
.	O
this	O
is	O
potentially	O
advantageous	O
since	O
there	O
may	O
be	O
less	O
training	B
points	O
than	O
dimensions	O
of	O
φ.	O
the	O
classiﬁer	B
depends	O
only	O
on	O
scalar	O
products	O
which	O
can	O
be	O
written	O
in	O
terms	O
of	O
a	O
positive	B
deﬁnite	I
kernel	O
,	O
(	O
cid:33	O
)	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
(	O
cid:16	O
)	O
n	O
(	O
cid:17	O
)	O
p	O
(	O
c	O
=	O
1|x	O
)	O
=	O
σ	O
ank	O
(	O
x	O
,	O
xn	O
)	O
for	O
convenience	O
,	O
we	O
can	O
write	O
the	O
above	O
as	O
p	O
(	O
c	O
=	O
1|x	O
)	O
=	O
σ	O
atk	O
(	O
x	O
)	O
(	O
17.5.3	O
)	O
(	O
17.5.4	O
)	O
where	O
the	O
n	O
dimensional	O
vector	O
k	O
(	O
x	O
)	O
has	O
elements	O
[	O
k	O
(	O
x	O
)	O
]	O
n	O
=	O
k	O
(	O
x	O
,	O
xn	O
)	O
.	O
then	O
the	O
above	O
is	O
of	O
exactly	O
the	O
same	O
form	O
as	O
the	O
original	O
speciﬁcation	O
of	O
logistic	B
regression	I
,	O
namely	O
as	O
a	O
function	B
of	O
a	O
linear	B
combination	O
of	O
vectors	O
.	O
hence	O
the	O
same	O
training	B
algorithm	O
to	O
maximise	O
the	O
likelihood	B
can	O
be	O
employed	O
,	O
simply	O
on	O
replacing	O
xn	O
with	O
k	O
(	O
xn	O
)	O
.	O
the	O
details	O
are	O
left	O
to	O
the	O
interested	O
reader	O
,	O
and	O
follow	O
closely	O
the	O
treatment	O
of	O
gaussian	O
processes	O
for	O
classiﬁcation	B
,	O
section	O
(	O
19.5	O
)	O
.	O
17.6	O
support	O
vector	O
machines	O
like	O
kernel	B
logistic	O
regression	B
,	O
svms	O
are	O
a	O
form	O
of	O
kernel	B
linear	O
classiﬁer	B
.	O
however	O
,	O
the	O
svm	O
uses	O
an	O
objective	O
which	O
more	O
explicitly	O
encourages	O
good	O
generalisation	B
performance	O
.	O
svms	O
do	O
not	O
ﬁt	O
comfortably	O
within	O
a	O
probabilistic	B
framework	O
and	O
as	O
such	O
we	O
describe	O
them	O
here	O
only	O
brieﬂy	O
,	O
referring	O
the	O
reader	O
to	O
the	O
wealth	O
of	O
excellent	O
literature	O
on	O
this	O
topic3	O
.	O
the	O
description	O
here	O
is	O
inspired	O
largely	O
by	O
[	O
71	O
]	O
.	O
17.6.1	O
maximum	O
margin	O
linear	B
classiﬁer	O
in	O
the	O
svm	O
literature	O
it	O
is	O
common	O
to	O
use	O
+1	O
and	O
−1	O
to	O
denote	O
the	O
two	O
classes	O
.	O
for	O
a	O
hyperplane	B
deﬁned	O
by	O
weight	B
w	O
and	O
bias	B
b	O
,	O
a	O
linear	O
discriminant	O
is	O
given	O
by	O
wtx	O
+	O
b	O
≥	O
0	O
wtx	O
+	O
b	O
<	O
0	O
class	O
+1	O
class	O
-1	O
3http	O
:	O
//www.support-vector.net	O
draft	O
march	O
9	O
,	O
2010	O
(	O
17.6.1	O
)	O
(	O
17.6.2	O
)	O
325	O
0.10.10.10.10.10.20.20.20.20.20.20.30.30.30.30.30.30.40.40.40.40.40.40.50.50.50.50.50.60.60.60.60.60.70.70.70.70.70.80.80.80.80.80.90.90.90.90.9−0.500.511.5−0.500.511.5	O
support	O
vector	O
machines	O
figure	O
17.15	O
:	O
svm	O
classiﬁcation	B
of	O
data	B
from	O
two	O
classes	O
(	O
open	O
circles	O
and	O
ﬁlled	O
circles	O
)	O
.	O
the	O
decision	B
boundary	I
wtx	O
+	O
b	O
=	O
0	O
(	O
solid	O
line	O
)	O
.	O
for	O
linearly	B
separable	I
data	O
the	O
maximum	O
margin	O
hyperplane	B
is	O
equidistant	O
from	O
the	O
closest	O
opposite	O
class	O
points	O
.	O
these	O
support	B
vectors	I
are	O
highlighted	O
in	O
blue	O
and	O
the	O
margin	B
in	O
red	O
.	O
the	O
distance	O
of	O
the	O
decision	B
boundary	I
from	O
the	O
origin	O
is	O
−b/√wtw	O
,	O
and	O
the	O
distance	O
of	O
a	O
general	O
point	O
x	O
from	O
the	O
origin	O
along	O
the	O
direction	O
w	O
is	O
xtw/√wtw	O
.	O
w	O
origin	O
for	O
a	O
point	O
x	O
that	O
is	O
close	O
to	O
the	O
decision	B
boundary	I
at	O
wtx	O
+	O
b	O
=	O
0	O
,	O
a	O
small	O
change	O
in	O
x	O
can	O
lead	O
to	O
a	O
change	O
in	O
classiﬁcation	B
.	O
to	O
make	O
the	O
classiﬁer	B
more	O
robust	O
we	O
therefore	O
impose	O
that	O
for	O
the	O
training	B
data	O
at	O
least	O
,	O
the	O
decision	B
boundary	I
should	O
be	O
separated	O
from	O
the	O
data	B
by	O
some	O
ﬁnite	O
margin	O
(	O
assuming	O
in	O
the	O
ﬁrst	O
instance	O
that	O
the	O
data	B
is	O
linearly	B
separable	I
)	O
:	O
wtx	O
+	O
b	O
≥	O
2	O
wtx	O
+	O
b	O
<	O
−2	O
class	O
+1	O
class	O
-1	O
(	O
17.6.3	O
)	O
(	O
17.6.4	O
)	O
since	O
w	O
,	O
b	O
and	O
2	O
can	O
all	O
be	O
rescaled	O
arbitrary	O
,	O
we	O
need	O
to	O
ﬁx	O
the	O
scale	O
of	O
the	O
above	O
to	O
break	O
this	O
invariance	O
.	O
it	O
is	O
convenient	O
to	O
set	O
	O
=	O
1	O
so	O
that	O
a	O
point	O
x+	O
from	O
class	O
+1	O
that	O
is	O
closest	O
to	O
the	O
decision	B
boundary	I
satisﬁes	O
wtx+	O
+	O
b	O
=	O
1	O
and	O
a	O
point	O
x−	O
from	O
class	O
-1	O
that	O
is	O
closest	O
to	O
the	O
decision	B
boundary	I
satisﬁes	O
wtx−	O
+	O
b	O
=	O
−1	O
(	O
17.6.5	O
)	O
(	O
17.6.6	O
)	O
from	O
vector	B
algebra	I
,	O
ﬁg	O
(	O
17.15	O
)	O
,	O
the	O
distance	O
from	O
the	O
origin	O
along	O
the	O
direction	O
w	O
to	O
a	O
point	O
x	O
is	O
given	O
by	O
wtx	O
√wtw	O
(	O
17.6.7	O
)	O
the	O
margin	B
between	O
the	O
hyperplanes	O
for	O
the	O
two	O
classes	O
is	O
then	O
the	O
diﬀerence	O
between	O
the	O
two	O
distances	O
along	O
the	O
direction	O
w	O
which	O
is	O
wt	O
√wtw	O
(	O
x+	O
−	O
x−	O
)	O
=	O
2	O
√wtw	O
(	O
17.6.8	O
)	O
to	O
set	O
the	O
distance	O
between	O
the	O
two	O
hyperplanes	O
to	O
be	O
maximal	O
,	O
we	O
need	O
to	O
minimise	O
the	O
length	O
wtw	O
.	O
given	O
that	O
for	O
each	O
xn	O
we	O
have	O
a	O
corresponding	O
label	O
yn	O
∈	O
{	O
+1	O
,	O
−1	O
}	O
,	O
in	O
order	O
to	O
classify	O
the	O
training	B
labels	O
correctly	O
and	O
maximise	O
the	O
margin	B
,	O
the	O
optimisation	B
problem	O
is	O
therefore	O
equivalent	B
to	O
:	O
subject	O
to	O
yn	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
minimise	O
1	O
2	O
wtw	O
yn	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
this	O
is	O
a	O
quadratic	B
programming	I
problem	O
.	O
note	O
that	O
the	O
factor	B
0.5	O
is	O
just	O
for	O
convenience	O
.	O
wtxn	O
+	O
b	O
≥	O
1	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
(	O
17.6.9	O
)	O
to	O
account	O
for	O
potentially	O
mislabelled	O
training	B
points	O
(	O
or	O
for	O
data	B
that	O
is	O
not	O
linearly	B
separable	I
)	O
,	O
we	O
relax	O
the	O
exact	O
classiﬁcation	O
constraint	O
and	O
use	O
instead	O
wtxn	O
+	O
b	O
≥	O
1	O
−	O
ξn	O
(	O
17.6.10	O
)	O
where	O
ξn	O
≥	O
0.	O
here	O
each	O
ξn	O
measures	O
how	O
far	O
xn	O
is	O
from	O
the	O
correct	O
margin	B
.	O
for	O
0	O
<	O
ξn	O
<	O
1	O
datapoint	O
xn	O
is	O
on	O
the	O
correct	O
side	O
of	O
the	O
decision	B
boundary	I
.	O
however	O
for	O
ξn	O
>	O
1	O
,	O
the	O
datapoint	O
is	O
assigned	O
the	O
opposite	O
class	O
to	O
its	O
training	B
label	O
.	O
ideally	O
we	O
want	O
to	O
limit	O
the	O
size	O
of	O
these	O
‘	O
violations	O
’	O
ξn	O
.	O
here	O
we	O
brieﬂy	O
describe	O
two	O
standard	O
approaches	O
.	O
326	O
draft	O
march	O
9	O
,	O
2010	O
support	O
vector	O
machines	O
2-norm	O
soft-margin	O
ξn	O
w	O
origin	O
figure	O
17.16	O
:	O
slack	O
margin	B
.	O
the	O
term	O
ξn	O
measures	O
how	O
far	O
a	O
variable	B
is	O
from	O
the	O
wrong	O
side	O
of	O
the	O
mar-	O
gin	O
for	O
its	O
class	O
.	O
if	O
ξn	O
>	O
1	O
then	O
the	O
point	O
will	O
be	O
misclassiﬁed	O
–	O
treated	O
as	O
an	O
outlier	B
.	O
the	O
2-norm	O
soft-margin	O
objective	O
is	O
minimise	O
1	O
2	O
wtw	O
+	O
c	O
2	O
(	O
ξn	O
)	O
2	O
(	O
cid:88	O
)	O
n	O
subject	O
to	O
yn	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
wtxn	O
+	O
b	O
≥	O
1	O
−	O
ξn	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
(	O
17.6.11	O
)	O
where	O
c	O
controls	O
the	O
number	O
of	O
mislabellings	O
of	O
the	O
training	B
data	O
.	O
the	O
constant	O
c	O
needs	O
to	O
be	O
determined	O
empirically	O
using	O
a	O
validation	B
set	O
.	O
the	O
optimisation	B
problem	O
expressed	O
by	O
(	O
17.6.11	O
)	O
can	O
be	O
formulated	O
using	O
the	O
lagrangian	O
(	O
cid:88	O
)	O
l	O
(	O
w	O
,	O
b	O
,	O
ξ	O
,	O
α	O
)	O
=	O
−	O
1	O
+	O
ξn	O
(	O
cid:105	O
)	O
for	O
points	O
xn	O
on	O
the	O
‘	O
correct	O
’	O
side	O
of	O
the	O
decision	B
boundary	I
yn	O
(	O
cid:0	O
)	O
wtxn	O
+	O
b	O
(	O
cid:1	O
)	O
which	O
is	O
to	O
be	O
minimised	O
with	O
respect	O
to	O
x	O
,	O
b	O
,	O
ξ	O
and	O
maximised	O
with	O
respect	O
to	O
α.	O
wtw	O
+	O
c	O
2	O
yn	O
(	O
cid:16	O
)	O
αn	O
(	O
cid:104	O
)	O
(	O
ξn	O
)	O
2−	O
(	O
cid:88	O
)	O
wtxn	O
+	O
b	O
(	O
cid:17	O
)	O
n	O
n	O
1	O
2	O
−1+	O
ξn	O
>	O
0	O
so	O
that	O
maximising	O
l	O
with	O
respect	O
to	O
α	O
requires	O
the	O
corresponding	O
αn	O
to	O
be	O
set	O
to	O
zero	O
.	O
only	O
training	B
points	O
that	O
are	O
support	B
vectors	I
lying	O
on	O
the	O
decision	B
boundary	I
will	O
have	O
non-zero	O
αn	O
.	O
,	O
αn	O
≥	O
0	O
,	O
ξn	O
≥	O
0	O
(	O
17.6.12	O
)	O
diﬀerentiating	O
the	O
lagrangian	O
and	O
equating	O
to	O
zero	O
,	O
we	O
have	O
the	O
conditions	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
l	O
(	O
w	O
,	O
b	O
,	O
ξ	O
,	O
α	O
)	O
=	O
wi	O
−	O
l	O
(	O
w	O
,	O
b	O
,	O
ξ	O
,	O
α	O
)	O
=	O
−	O
n	O
n	O
∂	O
∂wi	O
∂	O
∂b	O
αnynxn	O
i	O
=	O
0	O
αnyn	O
=	O
0	O
∂	O
∂ξn	O
l	O
(	O
w	O
,	O
b	O
,	O
ξ	O
,	O
α	O
)	O
=	O
cξn	O
−	O
αn	O
=	O
0	O
from	O
this	O
we	O
see	O
that	O
the	O
solution	O
for	O
w	O
is	O
given	O
by	O
w	O
=	O
(	O
cid:88	O
)	O
n	O
αnynxn	O
(	O
17.6.13	O
)	O
(	O
17.6.14	O
)	O
(	O
17.6.15	O
)	O
(	O
17.6.16	O
)	O
since	O
only	O
the	O
support	B
vectors	I
have	O
non-zero	O
αn	O
,	O
the	O
solution	O
for	O
w	O
will	O
typically	O
depend	O
on	O
only	O
a	O
small	O
number	O
of	O
the	O
training	B
data	O
.	O
using	O
these	O
conditions	O
and	O
substituting	O
back	O
into	O
the	O
original	O
problem	B
,	O
the	O
objective	O
is	O
equivalent	B
to	O
minimising	O
n	O
(	O
17.6.17	O
)	O
(	O
17.6.18	O
)	O
327	O
(	O
cid:88	O
)	O
l	O
(	O
α	O
)	O
=	O
(	O
cid:88	O
)	O
subject	O
to	O
(	O
cid:88	O
)	O
αn	O
−	O
1	O
2	O
n	O
,	O
m	O
n	O
1	O
ynymαnαm	O
(	O
xn	O
)	O
t	O
xm	O
−	O
2c	O
αn	O
≥	O
0	O
ynαn	O
=	O
0	O
,	O
n	O
(	O
cid:88	O
)	O
(	O
αn	O
)	O
2	O
if	O
we	O
deﬁne	O
k	O
(	O
xn	O
,	O
xm	O
)	O
=	O
(	O
xn	O
)	O
t	O
xm	O
draft	O
march	O
9	O
,	O
2010	O
support	O
vector	O
machines	O
figure	O
17.17	O
:	O
svm	O
training	B
.	O
the	O
solid	O
red	O
and	O
solid	O
blue	O
circles	O
represent	O
training	B
data	O
from	O
diﬀerent	O
classes	O
.	O
the	O
support	B
vectors	I
are	O
highlighted	O
in	O
green	O
.	O
for	O
the	O
unﬁlled	O
test	O
points	O
,	O
the	O
class	O
assigned	O
to	O
them	O
by	O
the	O
svm	O
is	O
given	O
by	O
the	O
colour	O
.	O
see	O
demosvm.m	O
the	O
optimisation	B
problem	O
is	O
maximize	O
(	O
cid:88	O
)	O
subject	O
to	O
(	O
cid:88	O
)	O
n	O
(	O
cid:88	O
)	O
1	O
2	O
αn	O
−	O
n	O
,	O
m	O
ynαn	O
=	O
0	O
,	O
n	O
(	O
cid:18	O
)	O
k	O
(	O
xn	O
,	O
xm	O
)	O
+	O
(	O
cid:19	O
)	O
1	O
c	O
δn	O
,	O
m	O
ynymαnαm	O
αn	O
≥	O
0	O
optimising	O
this	O
objective	O
is	O
discussed	O
in	O
section	O
(	O
17.6.3	O
)	O
.	O
1-norm	O
soft-margin	O
(	O
box	O
constraint	O
)	O
in	O
the	O
1-norm	O
soft-margin	O
version	O
,	O
one	O
uses	O
a	O
1-norm	O
penalty	O
(	O
cid:88	O
)	O
n	O
c	O
ξn	O
(	O
17.6.19	O
)	O
(	O
17.6.20	O
)	O
to	O
give	O
the	O
optimisation	B
problem	O
:	O
minimise	O
1	O
2	O
wtw+c	O
subject	O
to	O
yn	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
wtxn	O
+	O
b	O
≥	O
1−ξn	O
,	O
ξn	O
≥	O
0	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
(	O
17.6.21	O
)	O
where	O
c	O
is	O
an	O
empirically	O
determined	O
penalty	O
factor	B
that	O
controls	O
the	O
number	O
of	O
mislabellings	O
of	O
the	O
training	B
data	O
.	O
to	O
reformulate	O
the	O
optimisation	B
problem	O
we	O
use	O
the	O
lagrangian	O
l	O
(	O
w	O
,	O
b	O
,	O
ξ	O
)	O
=	O
1	O
2	O
wtw+c	O
(	O
cid:88	O
)	O
αn	O
(	O
cid:104	O
)	O
yn	O
(	O
cid:16	O
)	O
ξn−	O
n	O
(	O
cid:17	O
)	O
−	O
1	O
+	O
ξn	O
(	O
cid:105	O
)	O
(	O
cid:88	O
)	O
−	O
n	O
wtxn	O
+	O
b	O
(	O
cid:88	O
)	O
n	O
ξn	O
(	O
cid:88	O
)	O
n	O
rnξn	O
,	O
αn	O
≥	O
0	O
,	O
ξn	O
≥	O
0	O
,	O
rn	O
≥	O
0	O
(	O
17.6.22	O
)	O
the	O
variables	O
rn	O
are	O
introduced	O
in	O
order	O
to	O
give	O
a	O
non-trivial	O
solution	O
(	O
otherwise	O
αn	O
=	O
c	O
)	O
.	O
following	O
a	O
similar	O
argument	O
as	O
for	O
the	O
2-norm	O
case	O
,	O
by	O
diﬀerentiating	O
the	O
lagrangian	O
and	O
equating	O
to	O
zero	O
,	O
we	O
arrive	O
at	O
the	O
optimisation	B
problem	O
maximize	O
(	O
cid:88	O
)	O
subject	O
to	O
(	O
cid:88	O
)	O
n	O
(	O
cid:88	O
)	O
1	O
2	O
αn	O
−	O
n	O
,	O
m	O
ynαn	O
=	O
0	O
,	O
n	O
ynymαnαmk	O
(	O
xn	O
,	O
xm	O
)	O
(	O
17.6.23	O
)	O
0	O
≤	O
αn	O
≤	O
c	O
which	O
is	O
closely	O
related	O
to	O
the	O
2-norm	O
problem	B
except	O
that	O
we	O
now	O
have	O
the	O
box-constraint	O
0	O
≤	O
αn	O
≤	O
c.	O
17.6.2	O
using	O
kernels	O
the	O
ﬁnal	O
objectives	O
(	O
17.6.19	O
)	O
and	O
(	O
17.6.17	O
)	O
depend	O
on	O
the	O
inputs	O
xn	O
only	O
via	O
the	O
scalar	B
product	I
(	O
xn	O
)	O
t	O
xn	O
.	O
if	O
we	O
map	B
x	O
to	O
a	O
vector	O
function	O
of	O
x	O
,	O
then	O
we	O
can	O
write	O
k	O
(	O
xn	O
,	O
xm	O
)	O
=	O
φ	O
(	O
xn	O
)	O
tφ	O
(	O
xm	O
)	O
(	O
17.6.24	O
)	O
this	O
means	O
that	O
we	O
can	O
use	O
any	O
positive	O
(	O
semi	O
)	O
deﬁnite	O
kernel	B
k	O
and	O
make	O
a	O
non-linear	B
classiﬁer	O
.	O
see	O
section	O
(	O
19.3	O
)	O
.	O
328	O
draft	O
march	O
9	O
,	O
2010	O
−2−1012−2.5−2−1.5−1−0.500.511.52	O
soft	B
zero-one	O
loss	O
for	O
outlier	B
robustness	O
17.6.3	O
performing	O
the	O
optimisation	B
both	O
of	O
the	O
above	O
soft-margin	O
svm	O
optimisations	O
problems	O
(	O
17.6.19	O
)	O
and	O
(	O
17.6.17	O
)	O
are	O
quadratic	O
programs	O
for	O
which	O
the	O
exact	O
computational	O
cost	O
scales	O
as	O
o	O
(	O
cid:0	O
)	O
n	O
3	O
(	O
cid:1	O
)	O
.	O
whilst	O
these	O
can	O
be	O
solved	O
with	O
general	O
purpose	O
mal	O
optimisation	B
algorithm	O
[	O
224	O
]	O
,	O
whose	O
practical	O
performance	B
is	O
typically	O
o	O
(	O
cid:0	O
)	O
n	O
2	O
(	O
cid:1	O
)	O
or	O
better	O
.	O
a	O
variant	O
of	O
routines	O
,	O
speciﬁcally	O
tailored	O
routines	O
that	O
exploit	O
the	O
structure	B
of	O
the	O
problem	B
are	O
preferred	O
in	O
practice	O
.	O
of	O
particular	O
practical	O
interest	O
are	O
‘	O
chunking	B
’	O
techniques	O
that	O
optimise	O
over	O
a	O
subset	O
of	O
the	O
α.	O
in	O
the	O
limit	O
of	O
updating	O
only	O
two	O
components	O
of	O
α	O
,	O
this	O
can	O
be	O
achieved	O
analytically	O
,	O
resulting	O
in	O
the	O
sequential	B
mini-	O
this	O
algorithm	B
[	O
92	O
]	O
is	O
provided	O
in	O
svmtrain.m	O
.	O
once	O
the	O
optimal	O
solution	O
α∗	O
is	O
found	O
the	O
decision	B
function	I
for	O
a	O
new	O
point	O
x	O
is	O
αn∗	O
ynk	O
(	O
xn	O
,	O
x	O
)	O
+	O
b∗	O
assign	O
to	O
class	O
1	O
assign	O
to	O
class	O
-1	O
the	O
optimal	O
b∗	O
is	O
determined	O
using	O
the	O
maximum	O
margin	O
condition	O
,	O
equations	O
(	O
17.6.5,17.6.6	O
)	O
:	O
(	O
cid:88	O
)	O
n	O
(	O
cid:20	O
)	O
(	O
cid:26	O
)	O
>	O
0	O
<	O
0	O
(	O
cid:21	O
)	O
b∗	O
=	O
1	O
2	O
min	O
yn=1	O
wt∗	O
xn	O
−	O
max	O
yn=−1	O
wt∗	O
xn	O
(	O
17.6.25	O
)	O
(	O
17.6.26	O
)	O
17.6.4	O
probabilistic	B
interpretation	O
kernelised	O
logistic-regression	O
has	O
some	O
of	O
the	O
characteristics	O
of	O
the	O
svm	O
but	O
does	O
not	O
express	O
the	O
large	O
margin	B
requirement	O
.	O
also	O
the	O
sparse	B
data	O
usage	O
of	O
the	O
svm	O
is	O
similar	O
to	O
that	O
of	O
the	O
relevance	B
vector	I
machine	I
we	O
discuss	O
in	O
section	O
(	O
18.2.4	O
)	O
.	O
however	O
,	O
a	O
probabilistic	B
model	O
whose	O
map	B
assignment	O
matches	O
exactly	O
the	O
svm	O
is	O
hampered	O
by	O
the	O
normalisation	B
requirement	O
for	O
a	O
probability	B
distribution	O
.	O
whilst	O
,	O
arguably	O
,	O
no	O
fully	O
satisfactory	O
direct	O
match	O
between	O
the	O
svm	O
and	O
a	O
related	O
probabilistic	B
model	O
has	O
been	O
achieved	O
,	O
approximate	B
matches	O
have	O
been	O
obtained	O
[	O
255	O
]	O
.	O
17.7	O
soft	B
zero-one	O
loss	O
for	O
outlier	B
robustness	O
both	O
the	O
support	B
vector	I
machine	I
and	O
logistic	B
regression	I
are	O
potentially	O
mislead	O
by	O
outliers	O
.	O
for	O
the	O
svm	O
,	O
a	O
mislabelled	O
datapoint	O
that	O
is	O
far	O
from	O
the	O
correct	O
side	O
of	O
the	O
decision	B
boundary	I
would	O
require	O
a	O
large	O
slack	O
ξ.	O
however	O
,	O
since	O
exactly	O
such	O
large	O
ξ	O
are	O
discouraged	O
,	O
it	O
is	O
unlikely	O
that	O
the	O
svm	O
would	O
admit	O
such	O
a	O
solution	O
.	O
for	O
logistic	B
regression	I
,	O
the	O
probability	B
of	O
generating	O
a	O
mislabelled	O
point	O
far	O
from	O
the	O
cor-	O
rect	O
side	O
of	O
the	O
decision	B
boundary	I
is	O
so	O
exponentially	O
small	O
that	O
this	O
will	O
never	O
happen	O
in	O
practice	O
.	O
this	O
means	O
that	O
the	O
model	B
trained	O
with	O
maximum	O
likelihood	B
will	O
never	O
present	O
such	O
a	O
solution	O
.	O
in	O
both	O
cases	O
therefore	O
mislabelled	O
points	O
(	O
or	O
outliers	O
)	O
have	O
a	O
signiﬁcant	O
impact	O
on	O
the	O
location	O
of	O
the	O
decision	B
boundary	I
.	O
a	O
robust	O
technique	O
to	O
deal	O
with	O
outliers	O
is	O
to	O
use	O
the	O
zero-one	B
loss	I
in	O
which	O
a	O
mislabeled	O
point	O
contributes	O
only	O
a	O
relatively	O
small	O
loss	O
.	O
soft	B
variants	O
of	O
this	O
are	O
obtained	O
by	O
using	O
the	O
objective	O
n	O
(	O
cid:88	O
)	O
n=1	O
(	O
cid:104	O
)	O
σβ	O
(	O
b	O
+	O
wtxn	O
)	O
−	O
cn	O
(	O
cid:105	O
)	O
2	O
+	O
λwtw	O
(	O
17.7.1	O
)	O
which	O
is	O
to	O
be	O
minimised	O
with	O
respect	O
to	O
w	O
and	O
b.	O
for	O
β	O
→	O
∞	O
the	O
ﬁrst	O
term	O
above	O
tends	O
to	O
the	O
zero-one	B
loss	I
.	O
the	O
second	O
term	O
represents	O
a	O
penalty	O
on	O
the	O
length	O
of	O
w	O
and	O
prevents	O
overﬁtting	B
.	O
kernel	B
extensions	O
of	O
this	O
soft	B
zero-one	O
loss	O
are	O
straightforward	O
.	O
unfortunately	O
,	O
the	O
objective	O
(	O
17.7.1	O
)	O
is	O
highly	O
non-convex	O
and	O
ﬁnding	O
the	O
optimal	O
w	O
,	O
b	O
is	O
computationally	O
diﬃcult	O
.	O
a	O
simple-minded	O
scheme	O
is	O
to	O
ﬁx	O
all	O
components	O
of	O
w	O
except	O
one	O
,	O
wi	O
and	O
then	O
perform	O
a	O
nu-	O
merical	O
one-dimensional	O
optimisation	B
over	O
this	O
single	O
parameter	B
wi	O
.	O
at	O
the	O
next	O
step	O
,	O
another	O
parameter	B
wj	O
is	O
chosen	O
,	O
and	O
the	O
procedure	O
repeated	O
until	O
convergence	O
.	O
as	O
usual	O
,	O
λ	O
can	O
be	O
set	O
using	O
validation	B
.	O
the	O
practical	O
diﬃculties	O
of	O
minimising	O
non-convex	O
high-dimensional	O
objective	O
functions	O
means	O
that	O
these	O
approaches	O
are	O
rarely	O
used	O
in	O
practice	O
.	O
a	O
discussion	O
of	O
practical	O
attempts	O
in	O
this	O
area	O
is	O
given	O
in	O
[	O
286	O
]	O
.	O
draft	O
march	O
9	O
,	O
2010	O
329	O
exercises	O
figure	O
17.18	O
:	O
soft	B
zero-one	O
loss	O
decision	O
boundary	B
(	O
solid	O
line	O
)	O
versus	O
logis-	O
tic	O
regression	B
(	O
dotted	O
line	O
)	O
.	O
the	O
number	O
of	O
mis-classiﬁed	O
training	B
points	O
using	O
the	O
soft	B
zero-one	O
loss	O
is	O
2	O
,	O
compared	O
to	O
3	O
for	O
logistic	B
regression	I
.	O
the	O
penalty	O
λ	O
=	O
0.01	O
was	O
used	O
for	O
the	O
soft-loss	O
,	O
with	O
β	O
=	O
10.	O
for	O
logistic	B
re-	O
gression	O
,	O
no	O
penalty	O
term	O
was	O
used	O
.	O
the	O
outliers	O
have	O
a	O
signiﬁcant	O
impact	O
on	O
the	O
decision	B
boundary	I
for	O
logistic	B
regression	I
,	O
whilst	O
the	O
soft	B
zero-one	O
loss	O
essentially	O
gives	O
up	O
on	O
the	O
outliers	O
and	O
ﬁts	O
a	O
large	O
margin	B
classiﬁer	O
between	O
the	O
remaining	O
points	O
.	O
see	O
demosoftloss.m	O
.	O
an	O
illustration	O
of	O
the	O
diﬀerence	O
between	O
logistic	B
regression	I
and	O
this	O
soft	B
zero-one	O
loss	O
is	O
given	O
in	O
ﬁg	O
(	O
17.18	O
)	O
,	O
which	O
demonstrates	O
how	O
logistic	B
regression	I
is	O
inﬂuenced	O
by	O
the	O
mass	O
of	O
the	O
data	B
points	O
,	O
whereas	O
the	O
zero-one	B
loss	I
attempts	O
to	O
mimimise	O
the	O
number	O
of	O
mis-classiﬁcations	O
whilst	O
maintaining	O
a	O
large	O
margin	B
.	O
17.8	O
notes	O
the	O
perceptron	B
has	O
a	O
long	O
history	O
in	O
artiﬁcial	O
intelligence	O
and	O
machine	O
learning	B
.	O
rosenblatt	O
discussed	O
the	O
perceptron	B
as	O
a	O
model	B
for	O
human	O
learning	B
,	O
arguing	O
that	O
its	O
distributive	O
nature	O
(	O
the	O
input-output	B
‘	O
patterns	O
’	O
are	O
stored	O
in	O
the	O
weight	B
vector	O
)	O
is	O
closely	O
related	O
to	O
the	O
kind	O
of	O
information	O
storage	O
believed	O
to	O
be	O
present	O
in	O
biological	O
systems	O
[	O
234	O
]	O
.	O
to	O
deal	O
with	O
non-linear	O
decision	O
boundaries	O
,	O
the	O
main	O
thrust	O
of	O
research	O
in	O
the	O
ensuing	O
neural	B
network	I
community	O
was	O
on	O
the	O
use	O
of	O
multilayered	O
structures	O
in	O
which	O
the	O
outputs	O
of	O
perceptrons	O
are	O
used	O
as	O
the	O
inputs	O
to	O
other	O
perceptrons	O
,	O
resulting	O
in	O
potentially	O
highly	O
non-linear	B
discriminant	O
functions	O
.	O
this	O
line	O
of	O
research	O
was	O
largely	O
inspired	O
by	O
analogies	O
to	O
biological	O
information	O
processing	O
in	O
which	O
layered	O
structures	O
are	O
prevalent	O
.	O
such	O
multilayered	O
artiﬁcial	O
neural	O
networks	O
are	O
fascinating	O
and	O
,	O
once	O
trained	O
,	O
are	O
extremely	O
fast	O
in	O
forming	O
their	O
decisions	O
.	O
however	O
,	O
reliably	O
training	B
these	O
systems	O
is	O
a	O
highly	O
complex	O
task	O
and	O
probabilistic	B
generalisations	O
in	O
which	O
priors	O
are	O
placed	O
on	O
the	O
parameters	O
lead	O
to	O
computational	O
diﬃculties	O
.	O
whilst	O
perhaps	O
less	O
inspiring	O
from	O
a	O
biological	O
viewpoint	O
,	O
the	O
alternative	O
route	O
of	O
using	O
the	O
kernel	B
trick	O
to	O
boost	O
the	O
power	O
of	O
a	O
linear	B
classiﬁer	O
has	O
the	O
advantage	O
of	O
ease	O
of	O
training	B
and	O
generalisation	B
to	O
probabilistic	B
variants	O
.	O
more	O
recently	O
,	O
however	O
,	O
there	O
has	O
been	O
a	O
resurgence	O
of	O
interest	O
in	O
the	O
multilayer	O
systems	O
,	O
with	O
new	O
heuristics	O
aimed	O
at	O
improving	O
the	O
diﬃculties	O
in	O
training	B
,	O
see	O
for	O
example	O
[	O
135	O
]	O
.	O
17.9	O
code	O
democubicpoly.m	O
:	O
demo	O
of	O
fitting	O
a	O
cubic	O
polynomial	O
demologreg.m	O
:	O
demo	O
logistic	B
regression	I
logreg.m	O
:	O
logistic	B
regression	I
gradient	O
ascent	O
training	B
demologregnonlinear.m	O
:	O
demo	O
of	O
logistic	B
regression	I
with	O
a	O
non-linear	B
φ	O
(	O
x	O
)	O
svmtrain.m	O
:	O
svm	O
training	B
using	O
the	O
smo	O
algorithm	B
demosvm.m	O
:	O
svm	O
demo	O
demosoftloss.m	O
:	O
softloss	O
demo	O
softloss.m	O
:	O
softloss	O
function	B
17.10	O
exercises	O
exercise	O
174	O
.	O
1.	O
give	O
an	O
example	O
of	O
a	O
two-dimensional	O
dataset	O
for	O
which	O
the	O
data	B
are	O
linearly	B
separable	I
,	O
but	O
not	O
linearly	B
independent	I
.	O
2.	O
can	O
you	O
ﬁnd	O
a	O
dataset	O
which	O
is	O
linearly	B
independent	I
but	O
not	O
linearly	B
separable	I
?	O
330	O
draft	O
march	O
9	O
,	O
2010	O
−4−202468−4−2024681012	O
exercises	O
the	O
ﬁtted	O
lines	O
go	O
through	O
the	O
point	O
(	O
cid:80	O
)	O
n	O
n=1	O
(	O
xn	O
,	O
yn	O
)	O
/n	O
.	O
exercise	O
175.	O
show	O
that	O
for	O
both	O
ordinary	O
and	O
orthogonal	B
least	I
squares	I
regression	O
ﬁts	O
to	O
data	B
{	O
(	O
xn	O
,	O
yn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
)	O
}	O
exercise	O
176.	O
consider	O
the	O
softmax	B
function	I
for	O
classifying	O
an	O
input	O
vector	O
x	O
into	O
one	O
of	O
c	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
c	O
classes	O
using	O
c	O
x	O
(	O
cid:80	O
)	O
c	O
ewt	O
c	O
(	O
cid:48	O
)	O
=1	O
ewt	O
c	O
(	O
cid:48	O
)	O
x	O
p	O
(	O
c|x	O
)	O
=	O
(	O
17.10.1	O
)	O
a	O
set	O
of	O
input-class	O
examples	O
is	O
given	O
by	O
d	O
=	O
{	O
(	O
xn	O
,	O
cn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
.	O
1.	O
write	O
down	O
the	O
log-likelihood	O
l	O
of	O
the	O
classes	O
conditional	B
on	O
the	O
inputs	O
,	O
assuming	O
that	O
the	O
data	B
is	O
i.i.d	O
.	O
2.	O
compute	O
the	O
hessian	O
with	O
elements	O
hij	O
=	O
∂2l	O
(	O
d	O
)	O
∂wiwj	O
where	O
w	O
is	O
is	O
the	O
stacked	O
vector	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
t	O
w	O
=	O
wt	O
1	O
,	O
.	O
.	O
.	O
,	O
wt	O
c	O
(	O
17.10.2	O
)	O
(	O
17.10.3	O
)	O
and	O
show	O
that	O
the	O
hessian	O
is	O
positive	O
semi-deﬁnite	O
,	O
that	O
is	O
zthz	O
≥	O
0	O
for	O
any	O
z.	O
exercise	O
177.	O
derive	O
from	O
equation	B
(	O
17.6.11	O
)	O
the	O
dual	B
optimisation	O
problem	B
equation	O
(	O
17.6.17	O
)	O
.	O
exercise	O
178.	O
a	O
datapoint	O
x	O
is	O
projected	O
to	O
a	O
lower	O
dimensional	O
vector	O
´x	O
using	O
´x	O
=	O
mx	O
(	O
17.10.4	O
)	O
where	O
m	O
is	O
a	O
fat	O
matrix	B
.	O
for	O
a	O
set	O
of	O
data	B
xn	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
and	O
corresponding	O
binary	O
class	O
labels	O
yn	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
using	O
logistic	B
regression	I
on	O
the	O
projected	O
datapoints	O
´xn	O
corresponds	O
to	O
a	O
form	O
of	O
constrained	O
logistic	O
regression	B
in	O
the	O
original	O
higher	O
dimensional	O
space	O
x.	O
explain	O
if	O
it	O
is	O
reasonable	O
to	O
use	O
an	O
algorithm	B
such	O
as	O
pca	O
to	O
ﬁrst	O
reduce	O
the	O
data	B
dimensionality	O
before	O
using	O
logistic	B
regression	I
.	O
exercise	O
179.	O
the	O
logistic	B
sigmoid	I
function	O
is	O
deﬁned	O
as	O
σ	O
(	O
x	O
)	O
=	O
ex/	O
(	O
1+ex	O
)	O
.	O
what	O
is	O
the	O
inverse	O
function	O
,	O
σ−1	O
(	O
x	O
)	O
?	O
exercise	O
180.	O
given	O
a	O
dataset	O
d	O
=	O
{	O
(	O
xn	O
,	O
cn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
,	O
where	O
cn	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
logistic	B
regression	I
uses	O
the	O
model	B
p	O
(	O
c	O
=	O
1|x	O
)	O
=	O
σ	O
(	O
wtx	O
+	O
b	O
)	O
.	O
assuming	O
the	O
data	B
is	O
drawn	O
independently	O
and	O
identically	O
,	O
show	O
that	O
the	O
derivative	O
of	O
the	O
log	O
likelihood	B
l	O
with	O
respect	O
to	O
w	O
is	O
n	O
(	O
cid:88	O
)	O
(	O
cid:16	O
)	O
n=1	O
∇wl	O
=	O
cn	O
−	O
σ	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
wtxn	O
+	O
b	O
xn	O
(	O
17.10.5	O
)	O
exercise	O
181.	O
consider	O
a	O
dataset	O
d	O
=	O
{	O
(	O
xn	O
,	O
cn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
,	O
where	O
cn	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
and	O
x	O
is	O
a	O
d	O
dimen-	O
sional	O
vector	O
.	O
1.	O
show	O
that	O
if	O
the	O
training	B
data	O
is	O
linearly	B
separable	I
with	O
the	O
hyperplane	B
wtx	O
+	O
b	O
,	O
the	O
data	B
is	O
also	O
separable	O
with	O
the	O
hyperplane	B
˜wtx	O
+	O
˜b	O
,	O
where	O
˜w	O
=	O
λw	O
,	O
˜b	O
=	O
λb	O
for	O
any	O
scalar	O
λ	O
>	O
0	O
.	O
2.	O
what	O
consequence	O
does	O
the	O
above	O
result	O
have	O
for	O
maximum	B
likelihood	I
training	O
of	O
linearly	B
separable	I
data	O
?	O
exercise	O
182.	O
consider	O
a	O
dataset	O
d	O
=	O
{	O
(	O
xn	O
,	O
cn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
,	O
where	O
cn	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
and	O
x	O
is	O
a	O
n	O
dimen-	O
sional	O
vector	O
.	O
(	O
hence	O
we	O
have	O
n	O
datapoints	O
in	O
a	O
n	O
dimensional	O
space	O
)	O
.	O
in	O
the	O
text	O
we	O
showed	O
that	O
we	O
can	O
ﬁnd	O
a	O
hyperplane	B
(	O
parameterised	O
by	O
(	O
w	O
,	O
b	O
)	O
)	O
that	O
linearly	O
separates	O
this	O
data	B
we	O
need	O
,	O
for	O
each	O
datapoint	O
xn	O
,	O
wtxn	O
+	O
b	O
=	O
n	O
where	O
n	O
>	O
0	O
for	O
cn	O
=	O
1	O
and	O
n	O
<	O
0	O
for	O
cn	O
=	O
0.	O
comment	O
on	O
the	O
relation	O
between	O
maximum	B
likelihood	I
training	O
and	O
the	O
algorithm	B
suggested	O
above	O
.	O
draft	O
march	O
9	O
,	O
2010	O
331	O
exercise	O
183.	O
given	O
training	B
data	O
d	O
=	O
{	O
(	O
xn	O
,	O
yn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
,	O
you	O
decide	O
to	O
ﬁt	O
a	O
regression	B
model	O
y	O
=	O
mx	O
+	O
c	O
to	O
this	O
data	B
.	O
derive	O
an	O
expression	O
for	O
m	O
and	O
c	O
in	O
terms	O
of	O
d	O
using	O
the	O
minimum	O
sum	O
squared	O
error	O
criterion	O
.	O
exercise	O
184.	O
given	O
training	B
data	O
d	O
=	O
{	O
(	O
xn	O
,	O
cn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
,	O
cn	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
where	O
x	O
are	O
vector	O
inputs	O
,	O
a	O
discriminative	B
model	O
is	O
exercises	O
p	O
(	O
c	O
=	O
1|x	O
)	O
=	O
σ	O
(	O
b0	O
+	O
v1g	O
(	O
wt	O
(	O
17.10.6	O
)	O
where	O
g	O
(	O
x	O
)	O
=	O
exp	O
(	O
−0.5x2	O
)	O
.	O
and	O
σ	O
(	O
x	O
)	O
=	O
ex/	O
(	O
1	O
+	O
ex	O
)	O
(	O
this	O
is	O
a	O
neural	B
network	I
[	O
41	O
]	O
with	O
a	O
single	O
hidden	B
layer	O
and	O
two	O
hidden	B
units	O
)	O
.	O
1	O
x	O
+	O
b1	O
)	O
+	O
v2g	O
(	O
wt	O
2	O
x	O
+	O
b2	O
)	O
)	O
1.	O
write	O
down	O
the	O
log	O
likelihood	B
for	O
the	O
class	O
conditioned	O
on	O
the	O
inputs	O
,	O
based	O
on	O
the	O
usual	O
i.i.d	O
.	O
as-	O
sumption	O
.	O
2.	O
calculate	O
the	O
derivatives	O
of	O
the	O
log	O
likelihood	B
as	O
a	O
function	B
of	O
the	O
network	O
parameters	O
,	O
w1	O
,	O
w2	O
,	O
b1	O
,	O
b2	O
,	O
v	O
,	O
b0	O
3.	O
comment	O
on	O
the	O
relationship	O
between	O
this	O
model	B
and	O
logistic	B
regression	I
.	O
4.	O
comment	O
on	O
the	O
decision	B
boundary	I
of	O
this	O
model	B
.	O
332	O
draft	O
march	O
9	O
,	O
2010	O
chapter	O
18	O
bayesian	O
linear	B
models	O
18.1	O
regression	B
with	O
additive	O
gaussian	O
noise	O
the	O
linear	B
models	O
in	O
chapter	O
(	O
17	O
)	O
were	O
trained	O
under	O
maximum	B
likelihood	I
and	O
do	O
not	O
deal	O
with	O
the	O
issue	O
that	O
,	O
from	O
a	O
probabilistic	B
perspective	O
,	O
parameter	B
estimates	O
are	O
inherently	O
uncertain	B
due	O
to	O
the	O
limited	O
available	O
training	B
data	O
.	O
regression	B
refers	O
to	O
inferring	O
a	O
mapping	O
on	O
the	O
basis	O
of	O
observed	O
data	O
d	O
=	O
{	O
(	O
xn	O
,	O
yn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
,	O
where	O
(	O
xn	O
,	O
yn	O
)	O
represents	O
an	O
input-output	B
pair	O
.	O
we	O
discuss	O
here	O
the	O
scalar	O
output	O
case	O
(	O
and	O
vector	O
inputs	O
x	O
)	O
with	O
the	O
extension	O
to	O
the	O
vector	O
output	O
case	O
y	O
is	O
straightforward	O
.	O
we	O
assume	O
that	O
each	O
(	O
clean	O
)	O
output	O
is	O
generated	O
from	O
a	O
model	B
f	O
(	O
x	O
;	O
w	O
)	O
where	O
the	O
parameters	O
w	O
of	O
the	O
function	B
f	O
are	O
unknown	O
.	O
an	O
output	O
y	O
is	O
generated	O
by	O
the	O
addition	O
of	O
noise	O
η	O
to	O
the	O
clean	O
model	B
output	O
,	O
(	O
cid:0	O
)	O
η	O
0	O
,	O
σ2	O
(	O
cid:1	O
)	O
,	O
the	O
model	B
generates	O
an	O
output	O
y	O
for	O
input	O
x	O
with	O
(	O
18.1.1	O
)	O
y	O
=	O
f	O
(	O
x	O
;	O
w	O
)	O
+	O
η	O
if	O
the	O
noise	O
is	O
gaussian	O
distributed	O
,	O
η	O
∼	O
n	O
probability	B
(	O
cid:0	O
)	O
y	O
f	O
(	O
x	O
;	O
w	O
)	O
,	O
σ2	O
(	O
cid:1	O
)	O
=	O
(	O
18.1.4	O
)	O
(	O
18.1.5	O
)	O
p	O
(	O
y|w	O
,	O
x	O
)	O
=	O
n	O
1	O
√2πσ2	O
e	O
2σ2	O
[	O
y−f	O
(	O
x	O
;	O
w	O
)	O
]	O
2	O
−	O
1	O
(	O
18.1.2	O
)	O
if	O
we	O
assume	O
that	O
each	O
data	B
input-output	O
pair	O
is	O
generated	O
identically	O
and	O
independently	O
,	O
the	O
likelihood	B
the	O
model	B
generates	O
the	O
data	B
is	O
p	O
(	O
d|w	O
)	O
=	O
p	O
(	O
yn|w	O
,	O
xn	O
)	O
p	O
(	O
xn	O
)	O
(	O
18.1.3	O
)	O
we	O
may	O
use	O
a	O
prior	B
weight	O
distribution	B
p	O
(	O
w	O
)	O
to	O
quantify	O
our	O
a	O
priori	O
belief	O
in	O
the	O
suitability	O
each	O
parameter	B
setting	O
.	O
writing	O
d	O
=	O
{	O
dx	O
,	O
dy	O
}	O
,	O
the	O
posterior	B
weight	O
distribution	B
is	O
then	O
given	O
by	O
p	O
(	O
w|d	O
)	O
∝	O
p	O
(	O
d|w	O
)	O
p	O
(	O
w	O
)	O
∝	O
p	O
(	O
dy|w	O
,	O
dx	O
)	O
p	O
(	O
w	O
)	O
using	O
the	O
gaussian	O
noise	O
assumption	O
,	O
and	O
for	O
convenience	O
deﬁning	O
β	O
=	O
1/σ2	O
,	O
this	O
gives	O
log	O
p	O
(	O
w|d	O
)	O
=	O
−	O
β	O
2	O
[	O
yn	O
−	O
f	O
(	O
xn	O
;	O
w	O
)	O
]	O
2	O
+	O
log	O
p	O
(	O
w	O
)	O
+	O
n	O
2	O
log	O
β	O
+	O
const	O
.	O
note	O
the	O
similarity	O
between	O
equation	B
(	O
18.1.5	O
)	O
and	O
the	O
regularised	B
training	O
error	O
equation	O
(	O
17.2.16	O
)	O
.	O
in	O
the	O
probabilistic	B
framework	O
,	O
we	O
identify	O
the	O
choice	O
of	O
a	O
sum	O
square	O
error	O
with	O
the	O
assumption	O
of	O
additive	O
gaussian	O
noise	O
.	O
similarly	O
,	O
the	O
regularising	O
term	O
is	O
identiﬁed	O
with	O
log	O
p	O
(	O
w	O
)	O
.	O
333	O
n	O
(	O
cid:89	O
)	O
n=1	O
n	O
(	O
cid:88	O
)	O
n=1	O
regression	B
with	O
additive	O
gaussian	O
noise	O
α	O
w	O
xn	O
yn	O
n	O
figure	O
18.1	O
:	O
belief	B
network	I
representation	O
of	O
a	O
bayesian	O
model	B
for	O
regression	B
under	O
the	O
i.i.d	O
.	O
data	B
assumption	O
.	O
the	O
hyperparameter	B
α	O
acts	O
as	O
a	O
form	O
of	O
regulariser	O
,	O
controlling	O
the	O
ﬂexibility	O
of	O
the	O
prior	B
on	O
the	O
weights	O
w.	O
the	O
hyperparameter	B
β	O
controls	O
the	O
level	O
of	O
noise	O
on	O
the	O
observations	O
.	O
β	O
18.1.1	O
bayesian	O
linear	O
parameter	O
models	O
linear	O
parameter	O
models	O
,	O
as	O
discussed	O
in	O
chapter	O
(	O
17	O
)	O
,	O
have	O
the	O
form	O
f	O
(	O
x	O
;	O
w	O
)	O
=	O
wiφi	O
(	O
x	O
)	O
≡	O
wtφ	O
(	O
x	O
)	O
(	O
18.1.6	O
)	O
where	O
the	O
parameters	O
wi	O
are	O
also	O
called	O
‘	O
weights	O
’	O
and	O
dim	O
w	O
=	O
b.	O
such	O
models	O
have	O
a	O
linear	O
parameter	O
dependence	O
,	O
but	O
may	O
represent	O
a	O
non-linear	B
input-output	O
mapping	O
if	O
the	O
basis	O
functions	O
φi	O
(	O
x	O
)	O
are	O
non-	O
linear	B
in	O
x.	O
since	O
the	O
output	O
scales	O
linearly	O
with	O
w	O
,	O
we	O
can	O
discourage	O
extreme	O
output	O
values	O
by	O
penalising	O
large	O
weight	B
values	O
.	O
a	O
natural	B
weight	O
prior	B
is	O
thus	O
b	O
(	O
cid:88	O
)	O
i=1	O
(	O
18.1.7	O
)	O
(	O
cid:0	O
)	O
w	O
0	O
,	O
α	O
p	O
(	O
w|α	O
)	O
=	O
n	O
−1i	O
(	O
cid:1	O
)	O
=	O
(	O
cid:104	O
)	O
n	O
(	O
cid:88	O
)	O
n=1	O
(	O
cid:16	O
)	O
α	O
(	O
cid:17	O
)	O
b	O
2π	O
−	O
α	O
2	O
wtw	O
2	O
e	O
(	O
cid:105	O
)	O
2	O
yn	O
−	O
wtφ	O
(	O
xn	O
)	O
where	O
the	O
precision	B
α	O
is	O
the	O
inverse	O
variance	O
.	O
if	O
α	O
is	O
large	O
,	O
the	O
total	O
squared	O
length	O
of	O
the	O
weight	B
vector	O
w	O
is	O
encouraged	O
to	O
be	O
small	O
.	O
under	O
the	O
gaussian	O
noise	O
assumption	O
,	O
the	O
posterior	B
distribution	O
is	O
log	O
p	O
(	O
w|γ	O
,	O
d	O
)	O
=	O
−	O
β	O
2	O
α	O
2	O
−	O
wtw	O
+	O
const	O
.	O
(	O
18.1.8	O
)	O
where	O
γ	O
=	O
{	O
α	O
,	O
β	O
}	O
represents	O
the	O
hyperparameter	B
set	O
.	O
parameters	O
that	O
determine	O
the	O
functions	O
φ	O
may	O
also	O
be	O
included	O
in	O
the	O
hyperparameter	B
set	O
.	O
using	O
the	O
lpm	O
in	O
equation	B
(	O
18.1.5	O
)	O
with	O
a	O
gaussian	O
prior	B
,	O
equation	B
(	O
18.1.7	O
)	O
,	O
and	O
completing	O
the	O
square	O
,	O
section	O
(	O
8.6.2	O
)	O
,	O
the	O
weight	B
posterior	O
is	O
a	O
gaussian	O
distribution	B
,	O
p	O
(	O
w|γ	O
,	O
d	O
)	O
=	O
n	O
(	O
w	O
m	O
,	O
s	O
)	O
where	O
the	O
covariance	B
and	O
mean	B
are	O
given	O
by	O
(	O
cid:33	O
)	O
−1	O
s	O
=	O
αi	O
+	O
β	O
φ	O
(	O
xn	O
)	O
φt	O
(	O
xn	O
)	O
,	O
m	O
=	O
βs	O
(	O
18.1.9	O
)	O
n	O
(	O
cid:88	O
)	O
n=1	O
ynφ	O
(	O
xn	O
)	O
(	O
18.1.10	O
)	O
(	O
cid:32	O
)	O
(	O
cid:90	O
)	O
n	O
(	O
cid:88	O
)	O
n=1	O
(	O
cid:28	O
)	O
(	O
cid:104	O
)	O
the	O
mean	B
prediction	O
for	O
an	O
input	O
x	O
is	O
then	O
given	O
by	O
¯f	O
(	O
x	O
)	O
≡	O
f	O
(	O
x	O
;	O
w	O
)	O
p	O
(	O
w|d	O
,	O
γ	O
)	O
dw	O
=	O
mtφ	O
(	O
x	O
)	O
.	O
similarly	O
,	O
the	O
variance	B
of	O
the	O
underlying	O
estimated	O
clean	O
function	B
is	O
(	O
cid:105	O
)	O
2	O
(	O
cid:29	O
)	O
var	O
(	O
f	O
(	O
x	O
)	O
)	O
=	O
wtφ	O
(	O
x	O
)	O
−	O
¯f	O
(	O
x	O
)	O
2	O
=	O
φt	O
(	O
x	O
)	O
sφ	O
(	O
x	O
)	O
(	O
18.1.11	O
)	O
(	O
18.1.12	O
)	O
the	O
output	O
variance	B
var	O
(	O
f	O
(	O
x	O
)	O
)	O
depends	O
only	O
on	O
the	O
input	O
variables	O
and	O
not	O
on	O
the	O
training	B
outputs	O
y.	O
since	O
the	O
additive	O
noise	O
η	O
is	O
uncorrelated	O
with	O
the	O
model	B
outputs	O
,	O
the	O
predictive	B
variance	I
is	O
var	O
(	O
y	O
(	O
x	O
)	O
)	O
=	O
var	O
(	O
f	O
(	O
x	O
)	O
)	O
+	O
σ2	O
(	O
18.1.13	O
)	O
and	O
represents	O
the	O
variance	B
of	O
the	O
‘	O
noisy	O
’	O
output	O
for	O
an	O
input	O
x	O
.	O
334	O
draft	O
march	O
9	O
,	O
2010	O
regression	B
with	O
additive	O
gaussian	O
noise	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
18.2	O
:	O
along	O
the	O
horizontal	O
axis	O
we	O
plot	O
the	O
input	O
x	O
and	O
along	O
the	O
vertical	O
axis	O
the	O
output	O
t.	O
(	O
a	O
)	O
:	O
(	O
b	O
)	O
:	O
prediction	B
using	O
regularised	B
training	O
and	O
ﬁxed	O
hyperparame-	O
the	O
raw	O
input-output	B
training	O
data	B
.	O
(	O
c	O
)	O
:	O
prediction	B
using	O
ml-ii	O
optimised	O
hyperparameters	O
.	O
also	O
plotted	O
are	O
standard	O
error	O
bars	O
on	O
ters	O
.	O
the	O
clean	O
underlying	O
function	B
,	O
(	O
cid:112	O
)	O
var	O
(	O
f	O
(	O
x	O
)	O
)	O
.	O
example	O
81.	O
in	O
ﬁg	O
(	O
18.2b	O
)	O
,	O
we	O
show	O
the	O
mean	B
prediction	O
on	O
the	O
data	B
in	O
ﬁg	O
(	O
18.2a	O
)	O
using	O
15	O
gaussian	O
basis	O
functions	O
φi	O
(	O
x	O
)	O
=	O
exp	O
(	O
cid:0	O
)	O
−0.5	O
(	O
x	O
−	O
ci	O
)	O
2/λ2	O
(	O
cid:1	O
)	O
(	O
18.1.14	O
)	O
with	O
width	O
λ	O
=	O
0.032	O
and	O
centres	O
ci	O
spread	O
out	O
evenly	O
over	O
the	O
1-dimensional	O
input	O
space	O
from	O
−2	O
to	O
2.	O
we	O
set	O
the	O
other	O
hyperparameters	O
by	O
hand	O
to	O
β	O
=	O
100	O
and	O
α	O
=	O
1.	O
the	O
prediction	B
severely	O
overﬁts	O
the	O
data	B
,	O
a	O
result	O
of	O
a	O
poor	O
choice	O
of	O
hyperparameter	B
settings	O
.	O
this	O
is	O
resolved	O
in	O
ﬁg	O
(	O
18.2c	O
)	O
using	O
the	O
ml-ii	O
parameters	O
,	O
as	O
described	O
below	O
.	O
18.1.2	O
determining	O
hyperparameters	O
:	O
ml-ii	O
the	O
hyperparameter	B
posterior	O
distribution	B
is	O
p	O
(	O
γ|d	O
)	O
∝	O
p	O
(	O
d|γ	O
)	O
p	O
(	O
γ	O
)	O
(	O
18.1.15	O
)	O
a	O
simple	O
summarisation	O
of	O
the	O
posterior	B
is	O
given	O
by	O
the	O
map	B
assignment	O
which	O
takes	O
the	O
single	O
‘	O
optimal	O
’	O
setting	O
:	O
γ∗	O
=	O
argmax	O
γ	O
p	O
(	O
γ|d	O
)	O
(	O
18.1.16	O
)	O
if	O
the	O
prior	B
belief	O
about	O
the	O
hyperparameters	O
is	O
weak	O
(	O
p	O
(	O
γ	O
)	O
≈	O
const	O
.	O
)	O
,	O
this	O
is	O
equivalent	B
to	O
using	O
the	O
γ	O
that	O
maximises	O
the	O
marginal	B
likelihood	I
p	O
(	O
d|γ	O
)	O
=	O
p	O
(	O
d|γ	O
,	O
w	O
)	O
p	O
(	O
w|γ	O
)	O
dw	O
(	O
18.1.17	O
)	O
this	O
approach	B
to	O
setting	O
hyperparameters	O
is	O
called	O
‘	O
ml-ii	O
’	O
[	O
33	O
]	O
or	O
the	O
evidence	B
procedure	O
[	O
181	O
]	O
.	O
in	O
the	O
case	O
of	O
bayesian	O
linear	O
parameter	O
models	O
under	O
gaussian	O
additive	O
noise	O
computing	O
the	O
marginal	B
likelihood	I
equation	O
(	O
18.1.17	O
)	O
involves	O
only	O
gaussian	O
integration	O
.	O
a	O
direct	O
approach	B
to	O
deriving	O
an	O
expres-	O
sion	O
for	O
the	O
marginal	B
likelihood	I
is	O
to	O
consider	O
(	O
cid:90	O
)	O
(	O
cid:18	O
)	O
(	O
cid:104	O
)	O
(	O
cid:105	O
)	O
2	O
yn	O
−	O
wtφ	O
(	O
xn	O
)	O
β	O
2	O
−	O
(	O
cid:19	O
)	O
α	O
2	O
−	O
wtw	O
p	O
(	O
d|γ	O
,	O
w	O
)	O
p	O
(	O
w	O
)	O
=	O
exp	O
draft	O
march	O
9	O
,	O
2010	O
(	O
2πβ	O
)	O
n/2	O
(	O
2πα	O
)	O
b/2	O
(	O
18.1.18	O
)	O
335	O
regression	B
with	O
additive	O
gaussian	O
noise	O
(	O
cid:88	O
)	O
where	O
d	O
=	O
β	O
by	O
collating	O
terms	O
in	O
w	O
(	O
completing	O
the	O
square	O
,	O
section	O
(	O
8.6.2	O
)	O
)	O
,	O
the	O
above	O
represents	O
a	O
gaussian	O
in	O
w	O
with	O
additional	O
factors	O
.	O
after	O
integrating	O
over	O
this	O
gaussian	O
we	O
have	O
2	O
log	O
p	O
(	O
d|γ	O
)	O
=	O
−β	O
(	O
yn	O
)	O
2	O
+	O
dts−1d	O
−	O
log	O
det	O
(	O
s	O
)	O
+	O
b	O
log	O
α	O
+	O
n	O
log	O
β	O
−	O
n	O
log	O
(	O
2π	O
)	O
(	O
18.1.19	O
)	O
n	O
(	O
cid:88	O
)	O
n=1	O
φ	O
(	O
xn	O
)	O
yn	O
(	O
18.1.20	O
)	O
n	O
see	O
exercise	O
(	O
186	O
)	O
for	O
an	O
alternative	O
expression	O
.	O
example	O
82.	O
using	O
the	O
hyperparameters	O
α	O
,	O
β	O
,	O
λ	O
that	O
optimise	O
expression	O
(	O
18.1.19	O
)	O
gives	O
the	O
results	O
in	O
ﬁg	O
(	O
18.2c	O
)	O
where	O
we	O
plot	O
both	O
the	O
mean	B
predictions	O
and	O
standard	O
predictive	O
error	O
bars	O
.	O
this	O
demonstrates	O
that	O
an	O
acceptable	O
setting	O
for	O
the	O
hyperparameters	O
can	O
be	O
obtained	O
by	O
maximising	O
the	O
marginal	B
likelihood	I
.	O
18.1.3	O
learning	B
the	O
hyperparameters	O
using	O
em	O
we	O
can	O
set	O
hyperparameters	O
such	O
as	O
α	O
and	O
β	O
by	O
maximising	O
the	O
marginal	B
likelihood	I
equation	O
(	O
18.1.17	O
)	O
.	O
a	O
convenient	O
computational	O
procedure	O
to	O
achieve	O
this	O
is	O
to	O
interpret	O
the	O
w	O
as	O
latent	O
variables	O
and	O
apply	O
the	O
em	O
algorithm	B
,	O
section	O
(	O
11.2	O
)	O
.	O
in	O
this	O
case	O
the	O
energy	B
term	O
is	O
according	O
to	O
the	O
general	O
em	O
procedure	O
we	O
need	O
to	O
maximise	O
the	O
energy	B
term	O
.	O
for	O
a	O
hyperparameter	B
γ	O
the	O
derivative	O
of	O
the	O
energy	B
is	O
given	O
by	O
(	O
18.1.21	O
)	O
(	O
18.1.22	O
)	O
(	O
18.1.23	O
)	O
(	O
18.1.24	O
)	O
(	O
18.1.25	O
)	O
(	O
18.1.26	O
)	O
for	O
the	O
bayesian	O
lpm	O
with	O
gaussian	O
weight	B
and	O
noise	O
distributions	O
,	O
we	O
obtain	O
p	O
(	O
w|d	O
,	O
γold	O
)	O
∂γ	O
(	O
cid:28	O
)	O
∂	O
∂	O
∂γ	O
e	O
≡	O
e	O
≡	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
d|w	O
,	O
γ	O
)	O
p	O
(	O
w|γ	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
w|d	O
,	O
γold	O
)	O
(	O
cid:29	O
)	O
log	O
p	O
(	O
d|w	O
,	O
γ	O
)	O
p	O
(	O
w|γ	O
)	O
n	O
(	O
cid:88	O
)	O
n	O
(	O
cid:88	O
)	O
(	O
cid:104	O
)	O
yn	O
−	O
mtφ	O
(	O
xn	O
)	O
(	O
cid:105	O
)	O
2	O
yn	O
−	O
mtφ	O
(	O
xn	O
)	O
1	O
βnew	O
=	O
(	O
cid:28	O
)	O
(	O
cid:104	O
)	O
(	O
cid:104	O
)	O
(	O
cid:105	O
)	O
2	O
(	O
cid:29	O
)	O
yn	O
−	O
wtφ	O
(	O
xn	O
)	O
(	O
cid:105	O
)	O
2	O
=	O
n	O
2β	O
−	O
e	O
=	O
n	O
2β	O
−	O
+	O
trace	O
1	O
2	O
1	O
2	O
n=1	O
n=1	O
∂	O
∂β	O
n	O
(	O
cid:88	O
)	O
n=1	O
1	O
n	O
1	O
2	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
sˆs	O
solving	B
for	O
the	O
zero	O
derivatives	O
gives	O
the	O
m-step	O
update	O
p	O
(	O
w|γold	O
,	O
d	O
)	O
trace	O
s	O
−	O
(	O
cid:32	O
)	O
(	O
cid:33	O
)	O
φ	O
(	O
xn	O
)	O
φt	O
(	O
xn	O
)	O
n	O
(	O
cid:88	O
)	O
n=1	O
where	O
ˆs	O
is	O
the	O
empirical	B
covariance	O
of	O
the	O
basis-function	O
vectors	O
φ	O
(	O
xn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
similarly	O
,	O
for	O
α	O
,	O
e	O
=	O
b	O
∂	O
∂α	O
2α	O
−	O
(	O
cid:69	O
)	O
wtw	O
(	O
cid:68	O
)	O
1	O
2	O
(	O
cid:16	O
)	O
=	O
b	O
2α	O
−	O
1	O
2	O
(	O
cid:17	O
)	O
trace	O
(	O
s	O
)	O
+	O
mtm	O
(	O
cid:16	O
)	O
p	O
(	O
w|γold	O
,	O
d	O
)	O
(	O
cid:17	O
)	O
which	O
,	O
on	O
equating	O
to	O
zero	O
,	O
gives	O
the	O
update	O
1	O
αnew	O
=	O
1	O
b	O
trace	O
(	O
s	O
)	O
+	O
mtm	O
where	O
s	O
and	O
m	O
are	O
given	O
in	O
equation	B
(	O
18.1.10	O
)	O
.	O
an	O
alternative	O
ﬁxed	O
point	O
procedure	O
that	O
is	O
often	O
more	O
rapidly	O
convergent	O
than	O
em	O
is	O
given	O
in	O
equation	B
(	O
18.1.36	O
)	O
.	O
closed	O
form	O
updates	O
for	O
other	O
hyperparameters	O
,	O
such	O
as	O
the	O
width	O
of	O
the	O
basis	O
functions	O
,	O
are	O
generally	O
not	O
available	O
,	O
and	O
the	O
corresponding	O
energy	B
term	O
needs	O
to	O
be	O
optimised	O
numerically	O
.	O
336	O
draft	O
march	O
9	O
,	O
2010	O
regression	B
with	O
additive	O
gaussian	O
noise	O
figure	O
18.3	O
:	O
predictions	O
for	O
an	O
rbf	O
for	O
diﬀerent	O
widths	O
λ.	O
for	O
each	O
λ	O
the	O
ml-ii	O
optimal	O
α	O
,	O
β	O
are	O
obtained	O
by	O
running	O
the	O
em	O
procedure	O
to	O
convergence	O
and	O
subsequently	O
used	O
to	O
form	O
the	O
predictions	O
.	O
in	O
each	O
panel	O
the	O
dots	O
represent	O
the	O
training	B
points	O
,	O
with	O
x	O
along	O
the	O
horizontal	O
axis	O
and	O
y	O
along	O
the	O
vertical	O
axis	O
.	O
mean	B
predictions	O
are	O
plotted	O
,	O
along	O
with	O
predictive	O
error	O
bars	O
of	O
one	O
standard	B
deviation	I
.	O
according	O
to	O
ml-ii	O
,	O
the	O
best	O
model	B
corresponds	O
to	O
λ	O
=	O
0.37	O
,	O
see	O
ﬁg	O
(	O
18.4	O
)	O
.	O
the	O
smaller	O
values	O
of	O
λ	O
overﬁt	O
the	O
data	B
,	O
giving	O
rise	O
to	O
too	O
‘	O
rough	O
’	O
functions	O
.	O
the	O
largest	O
values	O
of	O
λ	O
underﬁt	O
,	O
giving	O
too	O
‘	O
smooth	O
’	O
functions	O
.	O
see	O
demobayeslinreg.m	O
.	O
18.1.4	O
hyperparameter	B
optimisation	O
:	O
using	O
the	O
gradient	B
hyperparameters	O
such	O
as	O
α	O
can	O
be	O
set	O
by	O
maximising	O
the	O
marginal	B
likelihood	I
(	O
cid:90	O
)	O
p	O
(	O
d|α	O
)	O
=	O
w	O
p	O
(	O
d|w	O
,	O
β	O
)	O
p	O
(	O
w|α	O
)	O
(	O
18.1.27	O
)	O
to	O
ﬁnd	O
the	O
optimal	O
α	O
,	O
we	O
search	O
for	O
the	O
zero	O
derivative	O
of	O
log	O
p	O
(	O
d|α	O
)	O
.	O
from	O
equation	B
(	O
18.2.18	O
)	O
we	O
can	O
use	O
the	O
general	O
derivative	O
identity	O
to	O
arrive	O
at	O
(	O
cid:28	O
)	O
∂	O
∂α	O
(	O
cid:29	O
)	O
log	O
p	O
(	O
w|α	O
)	O
p	O
(	O
w|α	O
,	O
d	O
)	O
∂	O
∂α	O
log	O
p	O
(	O
d|α	O
)	O
=	O
since	O
log	O
p	O
(	O
w|α	O
)	O
=	O
−	O
α	O
2	O
wtw	O
+	O
b	O
2	O
log	O
α	O
+	O
const	O
.	O
(	O
18.1.28	O
)	O
(	O
18.1.29	O
)	O
figure	O
18.4	O
:	O
the	O
log	O
marginal	B
likelihood	I
log	O
p	O
(	O
d|λ	O
,	O
α∗	O
(	O
λ	O
)	O
,	O
β∗	O
(	O
λ	O
)	O
having	O
found	O
the	O
optimal	O
values	O
of	O
the	O
hyperparameters	O
α	O
and	O
β	O
using	O
ml-ii	O
.	O
these	O
optimal	O
values	O
are	O
dependent	O
on	O
λ.	O
ac-	O
cording	O
to	O
ml-ii	O
,	O
the	O
best	O
model	B
corresponds	O
to	O
λ	O
=	O
0.37.	O
draft	O
march	O
9	O
,	O
2010	O
337	O
−202−2−1012lambda=0.01−202−2−1012lambda=0.05−202−2−1012lambda=0.09−202−2−1012lambda=0.13−202−2−1012lambda=0.17−202−2−1012lambda=0.21−202−2−1012lambda=0.25−202−2−1012lambda=0.29−202−2−1012lambda=0.33−202−2−1012lambda=0.37−202−2−1012lambda=0.41−202−2−1012lambda=0.45−202−2−1012lambda=0.49−202−2−1012lambda=0.53−202−2−1012lambda=0.57−202−2−1012lambda=0.61−202−2−1012lambda=0.65−202−2−1012lambda=0.69−202−2−1012lambda=0.73−202−2−1012lambda=0.77−0.100.10.20.30.40.50.60.702468101214x	O
109log	O
marginal	B
likelihoodlambda	O
(	O
cid:28	O
)	O
1	O
2	O
−wtw	O
+	O
b	O
α	O
(	O
cid:29	O
)	O
p	O
(	O
w|α	O
,	O
d	O
)	O
we	O
obtain	O
∂	O
∂α	O
log	O
p	O
(	O
d|α	O
)	O
=	O
(	O
cid:68	O
)	O
(	O
cid:69	O
)	O
wtw	O
0	O
=	O
−	O
p	O
(	O
w|α	O
,	O
d	O
)	O
+	O
b	O
α	O
setting	O
the	O
derivative	O
to	O
zero	O
,	O
the	O
optimal	O
α	O
satisﬁes	O
one	O
may	O
now	O
form	O
a	O
ﬁxed	O
point	O
equation	B
regression	O
with	O
additive	O
gaussian	O
noise	O
(	O
18.1.30	O
)	O
(	O
18.1.31	O
)	O
(	O
18.1.32	O
)	O
αnew	O
=	O
b	O
(	O
cid:104	O
)	O
wtw	O
(	O
cid:105	O
)	O
p	O
(	O
w|α	O
,	O
d	O
)	O
(	O
cid:16	O
)	O
(	O
cid:68	O
)	O
=	O
trace	O
(	O
cid:68	O
)	O
which	O
is	O
in	O
fact	O
a	O
re-derivation	O
of	O
the	O
em	O
procedure	O
for	O
this	O
model	B
.	O
for	O
a	O
gaussian	O
posterior	B
,	O
p	O
(	O
w|α	O
,	O
d	O
)	O
=	O
(	O
cid:69	O
)	O
n	O
(	O
w	O
m	O
,	O
s	O
)	O
,	O
wtw	O
−	O
(	O
cid:104	O
)	O
w	O
(	O
cid:105	O
)	O
(	O
cid:104	O
)	O
w	O
(	O
cid:105	O
)	O
t	O
+	O
(	O
cid:104	O
)	O
w	O
(	O
cid:105	O
)	O
(	O
cid:104	O
)	O
w	O
(	O
cid:105	O
)	O
t	O
(	O
cid:17	O
)	O
wwt	O
(	O
cid:69	O
)	O
=	O
trace	O
(	O
s	O
)	O
+	O
mtm	O
(	O
18.1.33	O
)	O
αnew	O
=	O
b	O
trace	O
(	O
s	O
)	O
+	O
mtm	O
gull-mackay	O
ﬁxed	O
point	O
iteration	B
from	O
equation	B
(	O
18.1.31	O
)	O
we	O
have	O
(	O
cid:68	O
)	O
(	O
cid:69	O
)	O
0	O
=	O
−α	O
wtw	O
p	O
(	O
w|α	O
,	O
d	O
)	O
+	O
b	O
=	O
−αs	O
−	O
αmtwt	O
+	O
b	O
so	O
that	O
an	O
alternative	O
ﬁxed	O
point	O
equation	B
[	O
123	O
,	O
180	O
]	O
is	O
αnew	O
=	O
b	O
−	O
αs	O
mtm	O
in	O
practice	O
this	O
update	O
converges	O
more	O
rapidly	O
than	O
equation	B
(	O
18.1.34	O
)	O
.	O
(	O
18.1.34	O
)	O
(	O
18.1.35	O
)	O
(	O
18.1.36	O
)	O
example	O
83	O
(	O
learning	B
the	O
basis	O
function	B
widths	O
)	O
.	O
in	O
ﬁg	O
(	O
18.3	O
)	O
we	O
plot	O
the	O
training	B
data	O
for	O
a	O
regression	B
problem	O
using	O
a	O
bayesian	O
lpm	O
.	O
a	O
set	O
of	O
10	O
radial	B
basis	I
functions	I
are	O
used	O
,	O
φi	O
(	O
x	O
)	O
=	O
exp	O
(	O
cid:0	O
)	O
−0.5	O
(	O
x	O
−	O
ci	O
)	O
2/λ2	O
(	O
cid:1	O
)	O
(	O
18.1.37	O
)	O
with	O
ci	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
10	O
spread	O
out	O
evenly	O
between	O
−2	O
and	O
2.	O
the	O
hyperparameters	O
α	O
and	O
β	O
are	O
learned	O
by	O
ml-ii	O
under	O
em	O
updating	O
.	O
for	O
a	O
ﬁxed	O
width	O
λ	O
we	O
then	O
present	O
the	O
predictions	O
,	O
each	O
time	O
ﬁnding	O
the	O
optimal	O
α	O
and	O
β	O
for	O
this	O
width	O
.	O
the	O
optimal	O
joint	B
α	O
,	O
β	O
,	O
λ	O
hyperparameter	B
setting	O
is	O
obtained	O
as	O
described	O
in	O
ﬁg	O
(	O
18.4	O
)	O
which	O
shows	O
the	O
marginal	B
log	O
likelihood	B
for	O
a	O
range	O
of	O
widths	O
.	O
18.1.5	O
validation	B
likelihood	O
the	O
hyperparameters	O
found	O
by	O
ml-ii	O
are	O
those	O
which	O
are	O
best	O
at	O
explaining	O
the	O
training	B
data	O
.	O
in	O
principle	O
,	O
this	O
is	O
diﬀerent	O
from	O
those	O
that	O
are	O
best	O
for	O
prediction	B
and	O
,	O
in	O
practice	O
therefore	O
,	O
it	O
is	O
reasonable	O
to	O
set	O
hyperparameters	O
also	O
by	O
validation	B
techniques	O
.	O
one	O
such	O
method	O
is	O
to	O
set	O
hyperparameters	O
by	O
minimal	O
prediction	B
error	O
on	O
a	O
validation	B
set	O
.	O
another	O
common	O
technique	O
is	O
to	O
set	O
hyperparameters	O
γ	O
by	O
their	O
likelihood	B
on	O
a	O
validation	B
set	O
{	O
xval	O
,	O
yval	O
}	O
≡	O
{	O
(	O
xm	O
val	O
,	O
ym	O
(	O
cid:90	O
)	O
w	O
val	O
)	O
,	O
m	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
}	O
:	O
p	O
(	O
yval|w	O
,	O
γ	O
)	O
p	O
(	O
w|γ	O
,	O
xtrain	O
,	O
ytrain	O
)	O
p	O
(	O
yval|γ	O
,	O
xtrain	O
,	O
ytrain	O
,	O
xval	O
)	O
=	O
338	O
(	O
18.1.38	O
)	O
draft	O
march	O
9	O
,	O
2010	O
regression	B
with	O
additive	O
gaussian	O
noise	O
from	O
which	O
we	O
obtain	O
(	O
see	O
exercise	O
(	O
187	O
)	O
)	O
log	O
p	O
(	O
yval|γ	O
,	O
dtrain	O
,	O
xval	O
)	O
=	O
−	O
(	O
cid:3	O
)	O
t	O
where	O
yval	O
=	O
(	O
cid:2	O
)	O
y1	O
val	O
=	O
(	O
cid:2	O
)	O
φ	O
(	O
x1	O
cval	O
≡	O
φvalsφt	O
and	O
the	O
design	B
matrix	I
φt	O
val	O
,	O
.	O
.	O
.	O
,	O
ym	O
val	O
val	O
+	O
σ2im	O
val	O
)	O
(	O
cid:3	O
)	O
val	O
)	O
,	O
.	O
.	O
.	O
,	O
φ	O
(	O
xm	O
1	O
2	O
log	O
det	O
(	O
2πcval	O
)	O
−	O
1	O
2	O
(	O
yval	O
−	O
φvalm	O
)	O
t	O
c−1	O
val	O
(	O
yval	O
−	O
φvalm	O
)	O
(	O
18.1.39	O
)	O
(	O
18.1.40	O
)	O
(	O
18.1.41	O
)	O
the	O
optimal	O
hyperparameters	O
γ∗	O
can	O
then	O
be	O
found	O
by	O
maximising	O
(	O
18.1.39	O
)	O
with	O
respect	O
to	O
γ	O
.	O
18.1.6	O
prediction	B
the	O
mean	B
function	O
predictor	O
based	O
on	O
hyperparameters	O
γ	O
and	O
weights	O
w	O
is	O
given	O
by	O
(	O
cid:90	O
)	O
(	O
cid:26	O
)	O
(	O
cid:90	O
)	O
(	O
cid:27	O
)	O
¯f	O
(	O
x	O
)	O
=	O
f	O
(	O
x	O
;	O
w	O
)	O
p	O
(	O
w	O
,	O
γ|d	O
)	O
dwdγ	O
=	O
f	O
(	O
x	O
;	O
w	O
)	O
p	O
(	O
w|γ	O
,	O
d	O
)	O
dw	O
p	O
(	O
γ|d	O
)	O
dγ	O
(	O
18.1.42	O
)	O
the	O
term	O
in	O
curly	O
brackets	O
is	O
the	O
mean	B
predictor	O
for	O
ﬁxed	O
hyperparameters	O
.	O
equation	B
(	O
18.1.42	O
)	O
then	O
weights	O
each	O
mean	B
predictor	O
by	O
the	O
posterior	B
probability	O
of	O
the	O
hyperparameter	B
p	O
(	O
γ|d	O
)	O
.	O
this	O
is	O
a	O
general	O
recipe	O
for	O
combining	O
model	B
predictions	O
,	O
where	O
each	O
model	B
is	O
weighted	O
by	O
its	O
posterior	B
probability	O
.	O
however	O
,	O
computing	O
the	O
integral	O
over	O
the	O
hyperparameter	B
posterior	O
is	O
numerically	O
challenging	O
and	O
approximations	O
are	O
usually	O
required	O
.	O
provided	O
the	O
hyperparameters	O
are	O
well	O
determined	O
by	O
the	O
data	B
,	O
we	O
may	O
instead	O
approximate	B
the	O
above	O
hyperparameter	B
integral	O
by	O
ﬁnding	O
the	O
map	B
hyperparameters	O
and	O
use	O
¯f	O
(	O
x	O
)	O
≈	O
f	O
(	O
x	O
;	O
w	O
)	O
p	O
(	O
w|γ∗	O
,	O
d	O
)	O
dw	O
(	O
18.1.43	O
)	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
18.1.7	O
the	O
relevance	B
vector	I
machine	I
the	O
relevance	B
vector	I
machine	I
assumes	O
that	O
only	O
a	O
small	O
number	O
of	O
components	O
of	O
the	O
basis	O
function	B
vector	O
are	O
relevant	O
in	O
determining	O
the	O
solution	O
for	O
w.	O
for	O
a	O
predictor	O
,	O
f	O
(	O
x	O
;	O
w	O
)	O
=	O
wiφi	O
(	O
x	O
)	O
≡	O
wtφ	O
(	O
x	O
)	O
(	O
18.1.44	O
)	O
it	O
is	O
often	O
the	O
case	O
that	O
some	O
basis	O
functions	O
will	O
be	O
redundant	O
in	O
the	O
sense	O
that	O
a	O
linear	B
combination	O
of	O
the	O
other	O
basis	O
functions	O
can	O
reproduce	O
the	O
training	B
outputs	O
with	O
insigniﬁcant	O
loss	O
in	O
accuracy	O
.	O
to	O
exploit	O
this	O
eﬀect	O
and	O
seek	O
a	O
parsimonious	O
solution	O
we	O
may	O
use	O
a	O
more	O
reﬁned	O
prior	B
that	O
encourages	O
each	O
wi	O
itself	O
to	O
be	O
small	O
:	O
(	O
18.1.45	O
)	O
(	O
18.1.46	O
)	O
(	O
18.1.47	O
)	O
the	O
modiﬁcations	O
required	O
to	O
the	O
description	O
of	O
section	O
(	O
18.1.1	O
)	O
are	O
to	O
replace	O
s	O
with	O
s	O
=	O
diag	O
(	O
α	O
)	O
+	O
β	O
φ	O
(	O
xn	O
)	O
φt	O
(	O
xn	O
)	O
the	O
marginal	B
likelihood	I
is	O
then	O
given	O
by	O
2	O
log	O
p	O
(	O
d|γ	O
)	O
=	O
−β	O
(	O
yn	O
)	O
2	O
+	O
dts−1d	O
−	O
log	O
det	O
(	O
s	O
)	O
+	O
b	O
(	O
cid:88	O
)	O
i=1	O
log	O
αi	O
+	O
n	O
log	O
β	O
−	O
n	O
log	O
(	O
2π	O
)	O
(	O
18.1.48	O
)	O
the	O
em	O
update	O
for	O
β	O
is	O
unchanged	O
,	O
and	O
the	O
em	O
update	O
for	O
each	O
αi	O
is	O
1	O
αnew	O
i	O
=	O
[	O
s	O
]	O
ii	O
+	O
m2	O
i	O
draft	O
march	O
9	O
,	O
2010	O
(	O
18.1.49	O
)	O
339	O
b	O
(	O
cid:88	O
)	O
i=1	O
p	O
(	O
w|α	O
)	O
=	O
(	O
cid:89	O
)	O
i	O
p	O
(	O
wi|αi	O
)	O
=	O
n	O
(	O
cid:32	O
)	O
where	O
the	O
prior	B
on	O
each	O
individual	O
weight	B
is	O
given	O
by	O
(	O
cid:1	O
)	O
=	O
−1	O
i	O
(	O
cid:16	O
)	O
αi	O
(	O
cid:17	O
)	O
1	O
2π	O
−	O
αi	O
2	O
w2	O
i	O
2	O
e	O
(	O
cid:33	O
)	O
−1	O
p	O
(	O
wi|αi	O
)	O
(	O
cid:0	O
)	O
wi	O
0	O
,	O
α	O
n	O
(	O
cid:88	O
)	O
n	O
(	O
cid:88	O
)	O
n=1	O
n=1	O
algorithm	B
18	O
evidence	B
procedure	O
for	O
bayesian	O
logistic	B
regression	I
1	O
:	O
initialise	O
w	O
and	O
α	O
.	O
2	O
:	O
while	O
not	O
converged	O
do	O
3	O
:	O
4	O
:	O
5	O
:	O
end	O
while	O
find	O
optimal	O
w∗	O
by	O
iterating	O
equation	B
(	O
18.2.16	O
)	O
,	O
equation	B
(	O
18.2.15	O
)	O
to	O
convergence	O
.	O
update	O
α	O
according	O
to	O
equation	B
(	O
18.2.9	O
)	O
.	O
18.2	O
classiﬁcation	B
for	O
the	O
logistic	B
regression	I
model	O
(	O
cid:33	O
)	O
(	O
cid:32	O
)	O
b	O
(	O
cid:88	O
)	O
i	O
p	O
(	O
c	O
=	O
1|w	O
,	O
x	O
)	O
=	O
σ	O
wiφi	O
(	O
x	O
)	O
classiﬁcation	B
(	O
cid:46	O
)	O
e-step	O
(	O
cid:46	O
)	O
m-step	O
(	O
18.2.1	O
)	O
the	O
maximum	B
likelihood	I
method	O
returns	O
only	O
a	O
single	O
optimal	O
w.	O
to	O
deal	O
with	O
the	O
inevitable	O
uncertainty	B
in	O
estimating	O
w	O
we	O
need	O
to	O
determine	O
the	O
posterior	B
distribution	O
of	O
the	O
weights	O
w.	O
to	O
do	O
so	O
we	O
ﬁrst	O
deﬁne	O
a	O
prior	B
on	O
the	O
weights	O
p	O
(	O
w	O
)	O
for	O
which	O
a	O
convenient	O
choice	O
is	O
a	O
gaussian	O
:	O
−αwtw/2	O
(	O
18.2.2	O
)	O
(	O
cid:0	O
)	O
w	O
0	O
,	O
α	O
−1i	O
(	O
cid:1	O
)	O
=	O
αb/2	O
(	O
2π	O
)	O
b/2	O
e	O
p	O
(	O
w|α	O
)	O
=	O
n	O
where	O
α	O
is	O
the	O
inverse	O
variance	O
(	O
also	O
called	O
the	O
precision	B
)	O
.	O
given	O
a	O
dataset	O
of	O
input-class	O
labels	O
,	O
d	O
=	O
{	O
(	O
xn	O
,	O
cn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
,	O
the	O
parameter	B
posterior	O
is	O
p	O
(	O
d|α	O
)	O
p	O
(	O
w|α	O
)	O
p	O
(	O
w|α	O
,	O
d	O
)	O
=	O
p	O
(	O
d|w	O
,	O
α	O
)	O
p	O
(	O
w|α	O
)	O
p	O
(	O
cn|xn	O
,	O
w	O
)	O
p	O
(	O
d|α	O
)	O
n	O
(	O
cid:89	O
)	O
(	O
18.2.3	O
)	O
n=1	O
=	O
1	O
unfortunately	O
,	O
this	O
distribution	B
is	O
not	O
of	O
any	O
standard	O
form	O
and	O
exactly	O
inferring	O
statistics	O
such	O
as	O
the	O
mean	B
,	O
or	O
the	O
most	O
probable	O
value	O
are	O
formally	O
computationally	O
intractable	O
.	O
18.2.1	O
hyperparameter	B
optimisation	O
hyperparameters	O
such	O
as	O
α	O
can	O
be	O
set	O
by	O
maximising	O
the	O
marginal	B
likelihood	I
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
n	O
(	O
cid:89	O
)	O
(	O
cid:16	O
)	O
α	O
(	O
cid:17	O
)	O
b/2	O
p	O
(	O
d|α	O
)	O
=	O
p	O
(	O
d|w	O
)	O
p	O
(	O
w|α	O
)	O
=	O
w	O
p	O
(	O
cn|xn	O
,	O
w	O
)	O
2π	O
w	O
n=1	O
−	O
α	O
e	O
2	O
wtw	O
(	O
18.2.4	O
)	O
there	O
are	O
several	O
approaches	O
one	O
could	O
take	O
to	O
approximate	B
this	O
and	O
below	O
we	O
discuss	O
the	O
laplace	O
and	O
a	O
variational	O
technique	O
.	O
common	O
to	O
all	O
approaches	O
,	O
however	O
,	O
is	O
the	O
form	O
of	O
the	O
gradient	B
,	O
diﬀering	O
only	O
in	O
the	O
statistics	O
under	O
an	O
approximation	B
to	O
the	O
posterior	B
.	O
for	O
this	O
reason	O
we	O
derive	O
ﬁrst	O
generic	O
hyperparameter	B
update	O
formulae	O
that	O
apply	O
under	O
all	O
approximations	O
.	O
to	O
ﬁnd	O
the	O
optimal	O
α	O
,	O
we	O
search	O
for	O
the	O
zero	O
derivative	O
of	O
log	O
p	O
(	O
d|α	O
)	O
.	O
this	O
is	O
equivalent	B
to	O
the	O
linear	B
regression	O
case	O
,	O
and	O
we	O
immediately	O
obtain	O
setting	O
the	O
derivative	O
to	O
zero	O
,	O
an	O
exact	O
equation	O
is	O
that	O
the	O
optimal	O
α	O
satisﬁes	O
(	O
cid:28	O
)	O
(	O
cid:29	O
)	O
1	O
2	O
−wtw	O
+	O
b	O
α	O
p	O
(	O
w|α	O
,	O
d	O
)	O
∂	O
∂α	O
log	O
p	O
(	O
d|α	O
)	O
=	O
(	O
cid:69	O
)	O
(	O
cid:68	O
)	O
wtw	O
0	O
=	O
−	O
p	O
(	O
w|α	O
,	O
d	O
)	O
+	O
b	O
α	O
one	O
may	O
now	O
form	O
a	O
ﬁxed	O
point	O
equation	B
αnew	O
=	O
b	O
(	O
cid:104	O
)	O
wtw	O
(	O
cid:105	O
)	O
p	O
(	O
w|α	O
,	O
d	O
)	O
340	O
(	O
18.2.5	O
)	O
(	O
18.2.6	O
)	O
(	O
18.2.7	O
)	O
draft	O
march	O
9	O
,	O
2010	O
classiﬁcation	B
the	O
averages	O
in	O
the	O
above	O
expression	O
can	O
not	O
be	O
computed	O
exactly	O
and	O
are	O
replaced	O
by	O
averages	O
with	O
respect	O
an	O
approximation	B
of	O
the	O
posterior	B
q	O
(	O
w|α	O
,	O
d	O
)	O
.	O
note	O
that	O
since	O
we	O
only	O
have	O
an	O
approximation	B
to	O
the	O
posterior	B
,	O
and	O
therefore	O
the	O
mean	B
and	O
covariance	B
statistics	O
,	O
we	O
can	O
not	O
guarantee	O
that	O
the	O
likelihood	B
will	O
always	O
increase	O
.	O
for	O
a	O
gaussian	O
approximation	B
of	O
the	O
posterior	B
,	O
q	O
(	O
w|α	O
,	O
d	O
)	O
=	O
n	O
(	O
w	O
m	O
,	O
s	O
)	O
(	O
cid:68	O
)	O
(	O
cid:69	O
)	O
wtw	O
=	O
trace	O
(	O
cid:16	O
)	O
(	O
cid:68	O
)	O
wwt	O
(	O
cid:69	O
)	O
−	O
(	O
cid:104	O
)	O
w	O
(	O
cid:105	O
)	O
(	O
cid:104	O
)	O
w	O
(	O
cid:105	O
)	O
t	O
+	O
(	O
cid:104	O
)	O
w	O
(	O
cid:105	O
)	O
(	O
cid:104	O
)	O
w	O
(	O
cid:105	O
)	O
t	O
(	O
cid:17	O
)	O
=	O
trace	O
(	O
s	O
)	O
+	O
mtm	O
αnew	O
=	O
b	O
trace	O
(	O
s	O
)	O
+	O
mtm	O
in	O
this	O
case	O
the	O
gull-mackay	O
alternative	O
ﬁxed	O
point	O
equation	B
[	O
123	O
,	O
180	O
]	O
is	O
αnew	O
=	O
b	O
−	O
αs	O
mtm	O
(	O
18.2.8	O
)	O
(	O
18.2.9	O
)	O
(	O
18.2.10	O
)	O
the	O
hyperparameter	B
updates	O
(	O
18.2.9	O
)	O
and	O
(	O
18.2.10	O
)	O
have	O
the	O
same	O
form	O
as	O
for	O
the	O
regression	B
model	O
.	O
the	O
mean	B
m	O
and	O
covariance	B
s	O
of	O
the	O
posterior	B
in	O
the	O
regression	B
and	O
classiﬁcation	B
cases	O
are	O
however	O
diﬀerent	O
.	O
in	O
the	O
classiﬁcation	B
case	O
we	O
need	O
to	O
approximate	B
the	O
mean	B
and	O
covariance	B
,	O
as	O
discussed	O
below	O
.	O
18.2.2	O
laplace	O
approximation	B
the	O
weight	B
posterior	O
is	O
given	O
by	O
−e	O
(	O
w	O
)	O
p	O
(	O
w|α	O
,	O
d	O
)	O
∝	O
e	O
where	O
e	O
(	O
w	O
)	O
=	O
α	O
2	O
wtw	O
−	O
(	O
cid:16	O
)	O
wthn	O
(	O
cid:17	O
)	O
,	O
log	O
σ	O
n	O
(	O
cid:88	O
)	O
n=1	O
hn	O
≡	O
(	O
2cn	O
−	O
1	O
)	O
φn	O
(	O
18.2.11	O
)	O
(	O
18.2.12	O
)	O
by	O
approximating	O
e	O
(	O
w	O
)	O
by	O
a	O
quadratic	O
function	O
in	O
w	O
,	O
we	O
obtain	O
a	O
gaussian	O
approximation	B
q	O
(	O
w|d	O
,	O
α	O
)	O
to	O
p	O
(	O
w|d	O
,	O
α	O
)	O
.	O
to	O
do	O
so	O
we	O
ﬁrst	O
ﬁnd	O
the	O
minimum	O
of	O
e	O
(	O
w	O
)	O
.	O
diﬀerentiating	O
,	O
we	O
obtain	O
(	O
cid:16	O
)	O
wthn	O
(	O
cid:17	O
)	O
it	O
is	O
convenient	O
to	O
use	O
a	O
newton	O
method	O
to	O
ﬁnd	O
the	O
optimum	O
.	O
the	O
hessian	O
matrix	B
with	O
elements	O
n	O
(	O
cid:88	O
)	O
n=1	O
∇e	O
=	O
αw	O
−	O
(	O
1	O
−	O
σn	O
)	O
hn	O
,	O
σn	O
≡	O
σ	O
∂2	O
∂wi∂wj	O
e	O
(	O
w	O
)	O
hij	O
≡	O
is	O
given	O
by	O
h	O
=	O
αi	O
+	O
n	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
n=1	O
(	O
cid:125	O
)	O
σn	O
(	O
1	O
−	O
σn	O
)	O
φn	O
(	O
φn	O
)	O
t	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
j	O
(	O
18.2.13	O
)	O
(	O
18.2.14	O
)	O
(	O
18.2.15	O
)	O
note	O
that	O
the	O
hessian	O
is	O
positive	O
semideﬁnite	O
(	O
see	O
exercise	O
(	O
188	O
)	O
)	O
so	O
that	O
the	O
function	B
e	O
(	O
w	O
)	O
is	O
convex	O
(	O
bowl	O
shaped	O
)	O
,	O
and	O
ﬁnding	O
a	O
minimum	O
of	O
e	O
(	O
w	O
)	O
is	O
numerically	O
unproblematic	O
.	O
a	O
newton	O
update	O
then	O
is	O
wnew	O
=	O
w	O
−	O
h−1	O
(	O
∇e	O
)	O
given	O
a	O
converged	O
w	O
,	O
the	O
posterior	B
approximation	O
is	O
given	O
by	O
(	O
18.2.16	O
)	O
q	O
(	O
w|d	O
,	O
α	O
)	O
=	O
n	O
(	O
w	O
m	O
,	O
s	O
)	O
,	O
(	O
18.2.17	O
)	O
where	O
m	O
=	O
w∗	O
is	O
the	O
converged	O
estimate	O
of	O
the	O
minimum	O
point	O
of	O
e	O
(	O
w	O
)	O
and	O
h	O
is	O
the	O
hessian	O
of	O
e	O
(	O
w	O
)	O
at	O
this	O
point	O
.	O
s	O
≡	O
h−1	O
draft	O
march	O
9	O
,	O
2010	O
341	O
classiﬁcation	B
(	O
cid:16	O
)	O
α	O
(	O
cid:17	O
)	O
b/2	O
2π	O
−	O
α	O
2	O
wtw	O
∝	O
e	O
(	O
cid:90	O
)	O
e	O
w	O
−e	O
(	O
w	O
)	O
(	O
18.2.18	O
)	O
approximating	O
the	O
marginal	B
likelihood	I
the	O
marginal	B
likelihood	I
is	O
given	O
by	O
(	O
cid:90	O
)	O
p	O
(	O
d|α	O
)	O
=	O
p	O
(	O
d|w	O
)	O
p	O
(	O
w|α	O
)	O
=	O
w	O
(	O
cid:90	O
)	O
n	O
(	O
cid:89	O
)	O
w	O
n=1	O
p	O
(	O
cn|xn	O
,	O
w	O
)	O
(	O
cid:16	O
)	O
log	O
σ	O
n	O
(	O
w∗	O
)	O
tw∗	O
+	O
(	O
cid:88	O
)	O
(	O
w∗	O
)	O
t	O
hn	O
(	O
cid:17	O
)	O
for	O
an	O
optimum	O
value	B
m	O
=	O
w∗	O
,	O
we	O
approximate	B
the	O
marginal	B
likelihood	I
using	O
(	O
see	O
section	O
(	O
28.2	O
)	O
)	O
log	O
p	O
(	O
d|α	O
)	O
≈	O
l	O
(	O
α	O
)	O
≡	O
−	O
α	O
2	O
1	O
2	O
−	O
log	O
det	O
(	O
αi	O
+	O
j	O
)	O
+	O
b	O
2	O
log	O
α	O
(	O
18.2.19	O
)	O
given	O
this	O
approximation	B
l	O
(	O
α	O
)	O
to	O
the	O
marginal	B
likelihood	I
,	O
an	O
alternative	O
strategy	O
for	O
hyperparameter	B
optimisation	O
,	O
is	O
to	O
optimises	O
l	O
(	O
α	O
)	O
with	O
respect	O
to	O
α.	O
by	O
diﬀerentiating	O
l	O
(	O
α	O
)	O
directly	O
,	O
the	O
reader	O
may	O
show	O
that	O
the	O
resulting	O
updates	O
are	O
in	O
fact	O
equivalent	B
to	O
using	O
the	O
general	O
condition	O
equation	B
(	O
18.2.6	O
)	O
under	O
a	O
laplace	O
approximation	B
to	O
the	O
posterior	B
statistics	O
.	O
18.2.3	O
making	O
predictions	O
ultimately	O
,	O
our	O
interest	O
is	O
to	O
classify	O
in	O
novel	O
situations	O
,	O
averaging	O
over	O
posterior	B
weight	O
uncertainty	B
,	O
p	O
(	O
c	O
=	O
1|x	O
,	O
α	O
,	O
d	O
)	O
=	O
w	O
p	O
(	O
c	O
=	O
1|x	O
,	O
w	O
)	O
p	O
(	O
w|α	O
,	O
d	O
)	O
(	O
18.2.20	O
)	O
the	O
b	O
dimensional	O
integrals	O
over	O
w	O
can	O
not	O
be	O
computed	O
analytically	O
and	O
numerical	B
approximation	O
is	O
required	O
.	O
in	O
this	O
particular	O
case	O
the	O
relative	O
benign	O
nature	O
of	O
the	O
posterior	B
(	O
the	O
log	O
posterior	B
is	O
concave	O
,	O
see	O
below	O
)	O
suggests	O
that	O
a	O
simple	O
laplace	O
approximation	B
may	O
suﬃce	O
(	O
see	O
[	O
142	O
]	O
for	O
a	O
variational	B
approximation	I
)	O
.	O
to	O
make	O
a	O
class	O
prediction	B
for	O
a	O
novel	O
input	O
x	O
,	O
we	O
use	O
p	O
(	O
c	O
=	O
1|x	O
,	O
d	O
,	O
α	O
∗	O
)	O
=	O
p	O
(	O
c	O
=	O
1|x	O
,	O
w	O
)	O
p	O
(	O
w|d	O
,	O
α	O
∗	O
)	O
dw	O
=	O
σ	O
xtw	O
p	O
(	O
w|d	O
,	O
α	O
∗	O
)	O
dw	O
however	O
,	O
since	O
the	O
term	O
σ	O
(	O
cid:0	O
)	O
xtw	O
(	O
cid:1	O
)	O
depends	O
on	O
w	O
via	O
the	O
scalar	B
product	I
xtw	O
,	O
we	O
only	O
require	O
the	O
integral	O
to	O
compute	O
the	O
predictions	O
it	O
would	O
appear	O
that	O
we	O
need	O
to	O
carry	O
out	O
an	O
integral	O
in	O
b	O
dimensions	O
.	O
over	O
the	O
one-dimensional	O
projection	B
(	O
see	O
exercise	O
(	O
189	O
)	O
)	O
(	O
cid:90	O
)	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
∗	O
)	O
dh	O
(	O
18.2.21	O
)	O
(	O
18.2.22	O
)	O
h	O
≡	O
xtw	O
so	O
that	O
p	O
(	O
c	O
=	O
1|x	O
,	O
d	O
,	O
α	O
∗	O
)	O
=	O
(	O
cid:90	O
)	O
σ	O
(	O
h	O
)	O
p	O
(	O
h|x	O
,	O
d	O
,	O
α	O
(	O
cid:17	O
)	O
h	O
xtm	O
,	O
xtσx	O
(	O
cid:16	O
)	O
p	O
(	O
h|x	O
,	O
d	O
,	O
α	O
∗	O
)	O
≈	O
n	O
under	O
the	O
laplace	O
approximation	B
,	O
w	O
is	O
gaussian	O
,	O
p	O
(	O
w|d	O
,	O
α∗	O
)	O
≈	O
n	O
(	O
w	O
m	O
,	O
s	O
)	O
.	O
since	O
h	O
is	O
a	O
projection	B
of	O
w	O
,	O
h	O
is	O
also	O
gaussian	O
distributed	O
(	O
18.2.23	O
)	O
predictions	O
may	O
then	O
be	O
made	O
by	O
numerically	O
evaluating	O
the	O
one-dimensional	O
integral	O
over	O
the	O
gaussian	O
distribution	B
in	O
h	O
,	O
equation	B
(	O
18.2.22	O
)	O
.	O
approximating	O
the	O
gaussian	O
average	B
of	O
a	O
logistic	B
sigmoid	I
predictions	O
under	O
a	O
gaussian	O
posterior	B
approximation	O
require	O
the	O
computation	O
of	O
i	O
≡	O
(	O
cid:104	O
)	O
σ	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
n	O
(	O
x	O
µ	O
,	O
σ2	O
)	O
342	O
(	O
18.2.24	O
)	O
draft	O
march	O
9	O
,	O
2010	O
classiﬁcation	B
figure	O
18.5	O
:	O
bayesian	O
logistic	B
regression	I
with	O
the	O
rbf	O
e−λ	O
(	O
x−m	O
)	O
2	O
,	O
placing	O
basis	O
functions	O
centred	O
on	O
a	O
subset	O
of	O
the	O
training	B
points	O
.	O
the	O
green	O
points	O
are	O
training	B
data	O
from	O
class	O
1	O
,	O
and	O
the	O
red	O
points	O
are	O
training	B
data	O
from	O
class	O
0.	O
the	O
contours	O
represent	O
the	O
probability	B
of	O
being	O
in	O
class	O
1.	O
the	O
optimal	O
value	B
of	O
α	O
found	O
by	O
the	O
evidence	B
procedure	O
in	O
this	O
case	O
is	O
0.45	O
(	O
λ	O
is	O
set	O
by	O
hand	O
to	O
2	O
)	O
.	O
see	O
demobayeslogregression.m	O
where	O
µ	O
=	O
xtµ	O
,	O
σ2	O
=	O
xtσx	O
.	O
gaussian	O
quadrature	O
is	O
an	O
obvious	O
numerical	B
candidate	O
[	O
227	O
]	O
.	O
an	O
alternative	O
is	O
to	O
replace	O
the	O
logistic	B
sigmoid	I
by	O
a	O
suitably	O
transformed	O
erf	O
function	B
[	O
181	O
]	O
,	O
the	O
reason	O
being	O
that	O
the	O
gaussian	O
average	B
of	O
an	O
erf	O
function	B
is	O
another	O
erf	O
function	B
.	O
using	O
a	O
single	O
erf	O
,	O
an	O
approximation	B
is1	O
σ	O
(	O
x	O
)	O
≈	O
1	O
2	O
(	O
1	O
+	O
erf	O
(	O
νx	O
)	O
)	O
(	O
18.2.25	O
)	O
these	O
two	O
functions	O
agree	O
at	O
−∞	O
,	O
0	O
,	O
∞	O
.	O
a	O
reasonable	O
criterion	O
is	O
that	O
the	O
derivatives	O
of	O
these	O
two	O
should	O
agree	O
at	O
x	O
=	O
0	O
since	O
then	O
they	O
have	O
locally	O
the	O
same	O
slope	O
around	O
the	O
origin	O
and	O
have	O
globally	O
similar	O
shape	O
.	O
using	O
σ	O
(	O
0	O
)	O
=	O
0.5	O
and	O
that	O
the	O
derivative	O
is	O
σ	O
(	O
0	O
)	O
(	O
1	O
−	O
σ	O
(	O
0	O
)	O
)	O
,	O
this	O
requires	O
a	O
more	O
accurate	O
approximation	B
can	O
be	O
found	O
by	O
considering	O
√π	O
4	O
=	O
ν	O
1	O
4	O
√π	O
⇒	O
ν	O
=	O
(	O
cid:88	O
)	O
ui	O
2	O
i	O
(	O
1	O
+	O
erf	O
(	O
νix	O
)	O
)	O
σ	O
(	O
x	O
)	O
≈	O
where	O
(	O
cid:80	O
)	O
(	O
18.2.26	O
)	O
(	O
18.2.27	O
)	O
(	O
18.2.28	O
)	O
(	O
18.2.29	O
)	O
(	O
18.2.30	O
)	O
i	O
ui	O
=	O
1.	O
suitable	O
values	O
for	O
ui	O
and	O
νi	O
are	O
given	O
in	O
logsigapp.m	O
which	O
uses	O
a	O
linear	B
combination	O
of	O
11	O
erf	O
functions	O
to	O
approximate	B
the	O
logistic	B
sigmoid	I
.	O
to	O
compute	O
the	O
approximate	B
average	I
of	O
σ	O
(	O
x	O
)	O
over	O
a	O
gaussian	O
,	O
one	O
may	O
then	O
make	O
use	O
of	O
the	O
result	O
(	O
cid:90	O
)	O
∞	O
∞	O
1	O
√2π	O
(	O
cid:18	O
)	O
1	O
√2	O
−	O
x2	O
2	O
erf	O
e	O
(	O
cx	O
+	O
d	O
)	O
dx	O
=	O
erf	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
d	O
√2	O
+	O
2c2	O
since	O
(	O
cid:90	O
)	O
∞	O
e	O
∞	O
1	O
√2πσ2	O
we	O
have	O
−	O
(	O
x−µ	O
)	O
2	O
2σ2	O
erf	O
(	O
νx	O
)	O
dx	O
=	O
(	O
cid:90	O
)	O
∞	O
e	O
∞	O
1	O
√2π	O
−	O
x2	O
2	O
erf	O
(	O
ν	O
(	O
σx	O
+	O
µ	O
)	O
)	O
dx	O
(	O
cid:104	O
)	O
σ	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
n	O
(	O
x	O
µ	O
,	O
σ	O
)	O
≈	O
1	O
2	O
+	O
1	O
2	O
	O
(	O
cid:88	O
)	O
i	O
uierf	O
νiµ	O
(	O
cid:113	O
)	O
1	O
+	O
2ν2	O
i	O
σ2	O
	O
further	O
approximate	B
statistics	O
can	O
be	O
obtained	O
using	O
the	O
results	O
derived	O
in	O
[	O
22	O
]	O
.	O
(	O
cid:82	O
)	O
x	O
1note	O
that	O
the	O
deﬁnition	O
of	O
the	O
erf	O
function	B
used	O
here	O
is	O
taken	O
to	O
be	O
consistent	B
with	O
matlab	O
,	O
namely	O
that	O
erf	O
(	O
x	O
)	O
≡	O
0	O
e−t2	O
dt	O
.	O
other	O
authors	O
deﬁne	O
it	O
to	O
be	O
the	O
cumulative	O
density	B
function	O
of	O
a	O
standard	O
gaussian	O
,	O
(	O
cid:82	O
)	O
x	O
−∞	O
e−	O
1	O
2	O
t2	O
2√	O
π	O
dt	O
.	O
2√	O
π	O
draft	O
march	O
9	O
,	O
2010	O
343	O
−6−4−20246−6−4−202460.10.10.10.10.20.20.20.20.20.20.30.30.30.30.30.30.30.30.40.40.40.40.40.40.40.40.50.50.50.50.50.50.60.60.60.60.70.70.70.70.80.80.80.90.9	O
classiﬁcation	B
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
18.6	O
:	O
classiﬁcation	B
using	O
the	O
rvm	O
with	O
rbf	O
e−λ	O
(	O
x−m	O
)	O
2	O
,	O
placing	O
a	O
basis	O
function	B
on	O
a	O
subset	O
of	O
the	O
training	B
data	O
points	O
.	O
the	O
green	O
points	O
are	O
training	B
data	O
from	O
class	O
1	O
,	O
and	O
the	O
red	O
points	O
are	O
training	B
data	O
from	O
class	O
0.	O
the	O
contours	O
represent	O
the	O
probability	B
of	O
being	O
in	O
class	O
1	O
.	O
(	O
a	O
)	O
:	O
training	B
points	O
.	O
(	O
b	O
)	O
:	O
the	O
training	B
points	O
weighted	O
by	O
their	O
relevance	O
value	O
1/αn	O
.	O
nearly	O
all	O
the	O
points	O
have	O
a	O
value	B
so	O
small	O
that	O
they	O
eﬀectively	O
vanish	O
.	O
see	O
demobayeslogregrvm.m	O
18.2.4	O
relevance	B
vector	I
machine	I
for	O
classiﬁcation	B
in	O
adopting	O
the	O
rvm	O
prior	B
to	O
classiﬁcation	B
,	O
we	O
encourage	O
individual	O
weights	O
to	O
be	O
small	O
(	O
18.2.31	O
)	O
(	O
18.2.32	O
)	O
(	O
18.2.33	O
)	O
p	O
(	O
w|α	O
)	O
=	O
(	O
cid:89	O
)	O
i	O
where	O
p	O
(	O
wi|αi	O
)	O
=	O
n	O
p	O
(	O
wi|αi	O
)	O
(	O
cid:18	O
)	O
wi	O
0	O
,	O
1	O
αi	O
(	O
cid:19	O
)	O
(	O
cid:88	O
)	O
n	O
the	O
only	O
alterations	O
in	O
the	O
previous	O
evidence	B
procedure	O
are	O
[	O
∇e	O
]	O
i	O
=	O
αiwi	O
−	O
(	O
1	O
−	O
σn	O
)	O
hn	O
i	O
,	O
h	O
=	O
diag	O
(	O
α	O
)	O
+	O
j	O
these	O
are	O
used	O
in	O
the	O
newton	O
update	O
formula	O
as	O
before	O
.	O
the	O
em	O
update	O
equation	B
for	O
the	O
α	O
’	O
s	O
is	O
given	O
by	O
αnew	O
i	O
=	O
1	O
m2	O
i	O
+	O
sii	O
where	O
σ	O
=	O
h−1	O
.	O
similarly	O
,	O
the	O
gull-mackay	O
update	O
is	O
given	O
by	O
αnew	O
i	O
=	O
1	O
−	O
αisii	O
m2	O
i	O
(	O
18.2.34	O
)	O
(	O
18.2.35	O
)	O
running	O
this	O
procedure	O
,	O
one	O
typically	O
ﬁnds	O
that	O
many	O
of	O
the	O
α	O
’	O
s	O
tend	O
to	O
inﬁnity	O
and	O
the	O
corresponding	O
weights	O
are	O
pruned	O
from	O
the	O
system	B
.	O
the	O
remaining	O
weights	O
tend	O
correspond	O
to	O
basis	O
functions	O
(	O
in	O
the	O
rbf	O
case	O
)	O
in	O
the	O
centres	O
of	O
mass	O
of	O
clusters	O
of	O
datapoints	O
of	O
the	O
same	O
class	O
,	O
see	O
ﬁg	O
(	O
18.6	O
)	O
.	O
contrast	O
this	O
with	O
the	O
situation	O
in	O
svms	O
,	O
where	O
the	O
retained	O
datapoints	O
tend	O
to	O
be	O
on	O
the	O
decision	O
boundaries	O
.	O
the	O
number	O
of	O
training	B
points	O
retained	O
by	O
the	O
rvm	O
tends	O
to	O
be	O
very	O
small	O
–	O
smaller	O
indeed	O
that	O
the	O
number	O
retained	O
in	O
the	O
svm	O
framework	O
.	O
whilst	O
the	O
rvm	O
does	O
not	O
support	O
large	O
margins	O
,	O
and	O
hence	O
may	O
be	O
a	O
less	O
robust	O
classiﬁer	O
,	O
it	O
does	O
retain	O
the	O
advantages	O
of	O
a	O
probabilistic	B
framework	O
[	O
276	O
]	O
.	O
a	O
potential	B
critique	O
of	O
the	O
rvm	O
,	O
coupled	B
with	O
an	O
ml-ii	O
procedure	O
for	O
learning	B
the	O
αi	O
is	O
that	O
it	O
is	O
overly	O
aggressive	O
in	O
terms	O
of	O
pruning	O
.	O
indeed	O
,	O
as	O
one	O
may	O
verify	O
running	O
demobayeslogregrvm.m	O
it	O
is	O
common	O
to	O
ﬁnd	O
an	O
instance	O
of	O
a	O
problem	B
for	O
which	O
there	O
exists	O
a	O
set	O
of	O
αi	O
such	O
that	O
the	O
training	B
data	O
can	O
be	O
classiﬁed	O
perfectly	O
;	O
however	O
,	O
after	O
using	O
ml-ii	O
,	O
so	O
many	O
of	O
the	O
αi	O
are	O
set	O
to	O
zero	O
that	O
the	O
training	B
data	O
can	O
no	O
longer	O
be	O
classiﬁed	O
perfectly	O
.	O
344	O
draft	O
march	O
9	O
,	O
2010	O
−8−6−4−202468−8−6−4−2024680.10.10.10.10.10.10.20.20.20.20.20.20.30.30.30.30.30.30.30.30.40.40.40.40.40.40.40.40.40.50.50.50.50.50.60.60.60.70.70.70.80.80.90.9−8−6−4−202468−8−6−4−2024680.10.10.10.10.10.10.20.20.20.20.20.20.30.30.30.30.30.30.30.30.40.40.40.40.40.40.40.40.40.50.50.50.50.50.60.60.60.70.70.70.80.80.90.9	O
exercises	O
18.2.5	O
multi-class	O
case	O
we	O
brieﬂy	O
note	O
that	O
the	O
multi-class	O
case	O
can	O
be	O
treated	O
by	O
using	O
the	O
softmax	B
function	I
under	O
a	O
one-of-m	O
class	O
coding	O
scheme	O
.	O
the	O
class	O
probabilities	O
are	O
p	O
(	O
c	O
=	O
m|y	O
)	O
=	O
eym	O
σm	O
(	O
cid:48	O
)	O
eym	O
(	O
cid:48	O
)	O
which	O
automatically	O
enforces	O
the	O
constraint	O
(	O
cid:80	O
)	O
classes	O
,	O
the	O
cost	O
of	O
the	O
laplace	O
approximation	B
scales	O
as	O
o	O
(	O
cid:0	O
)	O
c3n	O
3	O
(	O
cid:1	O
)	O
.	O
however	O
,	O
one	O
may	O
show	O
by	O
careful	O
implementation	O
that	O
the	O
cost	O
may	O
be	O
reduced	O
to	O
only	O
o	O
(	O
cid:0	O
)	O
cn	O
3	O
(	O
cid:1	O
)	O
,	O
analogous	O
to	O
the	O
cost	O
savings	O
possible	O
in	O
m	O
p	O
(	O
c	O
=	O
m	O
)	O
=	O
1.	O
naively	O
it	O
would	O
appear	O
that	O
for	O
c	O
(	O
18.2.36	O
)	O
the	O
gaussian	O
process	O
classiﬁcation	B
model	O
[	O
297	O
,	O
230	O
]	O
.	O
18.3	O
code	O
demobayeslinreg.m	O
:	O
demo	O
of	O
bayesian	O
linear	B
regression	O
bayeslinreg.m	O
:	O
bayesian	O
linear	B
regression	O
demobayeslogregrvm.m	O
:	O
demo	O
of	O
bayesian	O
logistic	B
regression	I
(	O
rvm	O
)	O
bayeslogregressionrvm.m	O
:	O
bayesian	O
logistic	B
regression	I
(	O
rvm	O
)	O
avsigmagauss.m	O
:	O
approximation	B
of	O
the	O
gaussian	O
average	B
of	O
a	O
logistic	B
sigmoid	I
logsigapp.m	O
:	O
approximation	B
of	O
the	O
logistic	B
sigmoid	I
using	O
mixture	O
of	O
erfs	O
18.4	O
exercises	O
exercise	O
185.	O
the	O
exercise	O
concerns	O
bayesian	O
regression	B
.	O
1.	O
show	O
that	O
for	O
f	O
=	O
wtx	O
(	O
18.4.1	O
)	O
and	O
p	O
(	O
w	O
)	O
∼	O
n	O
(	O
w	O
0	O
,	O
σ	O
)	O
,	O
that	O
p	O
(	O
f|x	O
)	O
is	O
gaussian	O
distributed	O
.	O
furthermore	O
,	O
ﬁnd	O
the	O
mean	B
and	O
covariance	B
of	O
this	O
gaussian	O
.	O
(	O
cid:0	O
)	O
	O
0	O
,	O
σ2	O
(	O
cid:1	O
)	O
.	O
what	O
is	O
p	O
(	O
f|t	O
,	O
x	O
)	O
?	O
2.	O
consider	O
a	O
target	O
point	O
t	O
=	O
f	O
+	O
	O
,	O
where	O
	O
∼	O
n	O
exercise	O
186.	O
a	O
bayesian	O
linear	O
parameter	O
regression	O
model	B
is	O
given	O
by	O
yn	O
=	O
wtφ	O
(	O
xn	O
)	O
+	O
ηn	O
in	O
vector	O
notation	O
y	O
=	O
(	O
cid:0	O
)	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
(	O
cid:1	O
)	O
t	O
this	O
can	O
be	O
written	O
with	O
φt	O
=	O
(	O
cid:2	O
)	O
φ	O
(	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
φ	O
(	O
xn	O
)	O
(	O
cid:3	O
)	O
and	O
η	O
is	O
a	O
zero	O
mean	B
gaussian	O
distributed	O
vector	O
with	O
covariance	O
β−1i	O
.	O
y	O
=	O
φw	O
+	O
η	O
(	O
18.4.2	O
)	O
(	O
18.4.3	O
)	O
an	O
expression	O
for	O
the	O
marginal	B
likelihood	I
of	O
a	O
dataset	O
is	O
given	O
in	O
equation	B
(	O
18.1.19	O
)	O
.	O
a	O
more	O
compact	O
expression	O
can	O
be	O
obtained	O
by	O
considering	O
p	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn|x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
γ	O
)	O
since	O
yn	O
is	O
linearly	O
related	O
to	O
xn	O
through	O
w.	O
then	O
y	O
is	O
gaussian	O
distributed	O
with	O
mean	B
(	O
cid:104	O
)	O
y	O
(	O
cid:105	O
)	O
=	O
φ	O
(	O
cid:104	O
)	O
w	O
(	O
cid:105	O
)	O
=	O
0	O
yyt	O
(	O
cid:69	O
)	O
(	O
cid:68	O
)	O
and	O
covariance	B
matrix	O
(	O
cid:68	O
)	O
(	O
cid:0	O
)	O
w	O
0	O
,	O
α−1i	O
(	O
cid:1	O
)	O
:	O
−	O
(	O
cid:104	O
)	O
y	O
(	O
cid:105	O
)	O
(	O
cid:104	O
)	O
y	O
(	O
cid:105	O
)	O
t	O
=	O
for	O
p	O
(	O
w	O
)	O
=	O
n	O
draft	O
march	O
9	O
,	O
2010	O
(	O
φw	O
+	O
η	O
)	O
(	O
φw	O
+	O
η	O
)	O
t	O
(	O
cid:69	O
)	O
(	O
18.4.4	O
)	O
(	O
18.4.5	O
)	O
(	O
18.4.6	O
)	O
345	O
1.	O
show	O
that	O
the	O
covariance	B
matrix	O
can	O
be	O
expressed	O
as	O
c	O
=	O
1	O
β	O
i	O
+	O
1	O
α	O
φφt	O
2.	O
hence	O
show	O
that	O
the	O
log	O
marginal	B
likelihood	I
can	O
we	O
written	O
as	O
log	O
p	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn|x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
γ	O
)	O
=	O
−	O
1	O
2	O
log	O
det	O
(	O
2πc	O
)	O
−	O
ytc−1y	O
1	O
2	O
exercises	O
(	O
18.4.7	O
)	O
(	O
18.4.8	O
)	O
exercise	O
187.	O
using	O
exercise	O
(	O
186	O
)	O
as	O
a	O
basis	O
,	O
derive	O
expression	O
(	O
18.1.39	O
)	O
for	O
the	O
log	O
likelihood	B
on	O
a	O
validation	B
set	O
.	O
exercise	O
188.	O
consider	O
the	O
function	B
e	O
(	O
w	O
)	O
as	O
deﬁned	O
in	O
equation	B
(	O
18.2.12	O
)	O
.	O
1.	O
compute	O
the	O
hessian	O
matrix	B
which	O
has	O
elements	O
,	O
hij	O
≡	O
∂2	O
∂wi∂wj	O
e	O
(	O
w	O
)	O
=	O
αδij	O
+	O
σn	O
(	O
1	O
−	O
σn	O
)	O
φn	O
(	O
φn	O
)	O
t	O
n	O
(	O
cid:88	O
)	O
n=1	O
2.	O
show	O
that	O
the	O
hessian	O
is	O
positive	O
semideﬁnite	O
exercise	O
189.	O
show	O
that	O
for	O
any	O
function	B
f	O
(	O
·	O
)	O
,	O
f	O
(	O
xtw	O
)	O
p	O
(	O
w	O
)	O
dw	O
=	O
f	O
(	O
h	O
)	O
p	O
(	O
h	O
)	O
dh	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
(	O
18.4.9	O
)	O
(	O
18.4.10	O
)	O
where	O
p	O
(	O
h	O
)	O
is	O
the	O
distribution	B
of	O
the	O
scalar	O
xtw	O
.	O
the	O
signiﬁcance	O
of	O
this	O
result	O
is	O
that	O
any	O
high-dimensional	O
integral	O
of	O
the	O
above	O
form	O
can	O
be	O
reduced	O
to	O
a	O
one-dimensional	O
integral	O
over	O
the	O
distribution	B
of	O
the	O
‘	O
ﬁeld	O
’	O
h	O
[	O
22	O
]	O
.	O
exercise	O
190.	O
this	O
exercise	O
concerns	O
bayesian	O
logistic	B
regression	I
.	O
our	O
interest	O
to	O
derive	O
a	O
formula	O
for	O
the	O
optimal	O
regularisation	B
parameter	O
α	O
based	O
on	O
the	O
laplace	O
approximation	B
to	O
the	O
marginal	B
log-likelihood	O
given	O
by	O
log	O
p	O
(	O
d|α	O
)	O
≈	O
l	O
(	O
α	O
)	O
≡	O
−	O
α	O
2	O
(	O
w	O
)	O
tw	O
+	O
(	O
cid:88	O
)	O
log	O
σ	O
n	O
(	O
cid:16	O
)	O
(	O
w	O
)	O
t	O
hn	O
(	O
cid:17	O
)	O
1	O
2	O
−	O
log	O
det	O
(	O
αi	O
+	O
j	O
)	O
+	O
b	O
2	O
(	O
cid:80	O
)	O
log	O
α	O
(	O
18.4.11	O
)	O
n	O
log	O
σ	O
(	O
cid:0	O
)	O
wthn	O
(	O
cid:1	O
)	O
,	O
as	O
in	O
equation	B
the	O
laplace	O
procedure	O
ﬁnds	O
ﬁrst	O
an	O
optimal	O
w∗	O
that	O
minimises	O
αwtw/2−	O
(	O
18.2.12	O
)	O
which	O
will	O
depend	O
on	O
the	O
setting	O
of	O
α.	O
formally	O
,	O
therefore	O
,	O
in	O
ﬁnding	O
the	O
α	O
that	O
optimises	O
l	O
(	O
α	O
)	O
we	O
should	O
make	O
use	O
of	O
the	O
total	O
derivative	O
formula	O
dl	O
dα	O
=	O
∂l	O
∂α	O
∂l	O
∂wi	O
∂wi	O
∂α	O
(	O
18.4.12	O
)	O
however	O
,	O
when	O
evaluated	O
at	O
w	O
=	O
w∗	O
,	O
∂l	O
∂w	O
=	O
0.	O
this	O
means	O
that	O
in	O
order	O
to	O
compute	O
the	O
derivative	O
with	O
respect	O
to	O
α	O
,	O
we	O
only	O
need	O
consider	O
the	O
terms	O
with	O
an	O
explicit	O
α	O
dependence	O
.	O
equating	O
the	O
derivative	O
to	O
zero	O
and	O
using	O
∂	O
log	O
det	O
(	O
m	O
)	O
=	O
trace	O
(	O
cid:0	O
)	O
m−1∂m	O
(	O
cid:1	O
)	O
+	O
(	O
cid:88	O
)	O
i	O
show	O
that	O
the	O
optimal	O
α	O
satisﬁes	O
the	O
ﬁxed	O
point	O
equation	B
αnew	O
=	O
n	O
(	O
w∗	O
)	O
tw∗	O
+	O
trace	O
(	O
cid:16	O
)	O
(	O
αi	O
+	O
j	O
)	O
−1	O
(	O
cid:17	O
)	O
(	O
18.4.13	O
)	O
(	O
18.4.14	O
)	O
346	O
draft	O
march	O
9	O
,	O
2010	O
chapter	O
19	O
gaussian	O
processes	O
19.1	O
non-parametric	B
prediction	O
gaussian	O
processes	O
are	O
ﬂexible	O
bayesian	O
models	O
that	O
ﬁt	O
well	O
within	O
the	O
probabilistic	B
modelling	O
framework	O
.	O
in	O
developing	O
gps	O
it	O
is	O
useful	O
to	O
ﬁrst	O
step	O
back	O
and	O
see	O
what	O
information	O
we	O
need	O
to	O
form	O
a	O
predictor	O
.	O
given	O
a	O
set	O
of	O
training	B
data	O
d	O
=	O
{	O
(	O
xn	O
,	O
yn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
=	O
x	O
∪	O
y	O
(	O
19.1.1	O
)	O
where	O
xn	O
is	O
the	O
input	O
for	O
datapoint	O
n	O
and	O
yn	O
the	O
corresponding	O
output	O
(	O
a	O
continuous	B
variable	O
in	O
the	O
regression	B
case	O
and	O
a	O
discrete	B
variable	O
in	O
the	O
classiﬁcation	B
case	O
)	O
,	O
our	O
aim	O
is	O
to	O
make	O
a	O
prediction	B
y∗	O
for	O
a	O
new	O
input	O
x∗	O
.	O
in	O
the	O
discriminative	B
framework	O
no	O
model	O
of	O
the	O
inputs	O
x	O
is	O
assumed	O
and	O
only	O
the	O
outputs	O
are	O
modelled	O
,	O
conditioned	O
on	O
the	O
inputs	O
.	O
given	O
a	O
joint	B
model	O
p	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
,	O
y	O
∗	O
|x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
x	O
∗	O
)	O
=	O
p	O
(	O
y	O
,	O
y	O
∗	O
∗	O
)	O
|x	O
,	O
x	O
(	O
19.1.2	O
)	O
we	O
may	O
subsequently	O
use	O
conditioning	B
to	O
form	O
a	O
predictor	O
p	O
(	O
y∗	O
much	O
use	O
of	O
the	O
i.i.d	O
.	O
assumption	O
that	O
each	O
datapoint	O
is	O
independently	O
sampled	O
from	O
the	O
same	O
generating	O
distribution	B
.	O
in	O
this	O
context	O
,	O
this	O
might	O
appear	O
to	O
suggest	O
the	O
assumption	O
|x∗	O
,	O
d	O
)	O
.	O
in	O
previous	O
chapters	O
we	O
’	O
ve	O
made	O
p	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
,	O
y	O
∗	O
|x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
x	O
∗	O
)	O
=	O
p	O
(	O
y	O
∗	O
|x	O
,	O
x	O
∗	O
)	O
p	O
(	O
yn|x	O
,	O
x	O
(	O
19.1.3	O
)	O
∗	O
)	O
(	O
cid:89	O
)	O
n	O
however	O
,	O
this	O
is	O
clearly	O
of	O
little	O
use	O
since	O
the	O
predictive	O
conditional	O
is	O
simply	O
p	O
(	O
y∗	O
meaning	O
the	O
predictions	O
make	O
no	O
use	O
of	O
the	O
training	B
outputs	O
.	O
for	O
a	O
non-trivial	O
predictor	O
we	O
therefore	O
need	O
to	O
specify	O
a	O
joint	B
non-factorised	O
distribution	B
over	O
outputs	O
.	O
|d	O
,	O
x∗	O
)	O
=	O
p	O
(	O
y∗	O
|x	O
,	O
x∗	O
)	O
19.1.1	O
from	O
parametric	O
to	O
non-parametric	B
if	O
we	O
revisit	O
our	O
i.i.d	O
.	O
assumptions	O
for	O
parametric	O
models	O
,	O
we	O
used	O
a	O
parameter	B
θ	O
to	O
make	O
a	O
model	B
of	O
the	O
input-output	B
distribution	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
.	O
for	O
a	O
parametric	O
model	B
predictions	O
are	O
formed	O
using	O
∗	O
∗	O
,	O
x	O
,	O
d	O
)	O
=	O
∗	O
∗	O
,	O
y	O
,	O
x	O
∗	O
p	O
(	O
y	O
∗	O
,	O
y|θ	O
,	O
x	O
,	O
x	O
)	O
p	O
(	O
θ|	O
,	O
x	O
)	O
x	O
∗	O
θ	O
under	O
the	O
assumption	O
that	O
,	O
given	O
θ	O
,	O
the	O
data	B
is	O
i.i.d.	O
,	O
we	O
obtain	O
∗	O
∗	O
∗	O
∗	O
|x	O
p	O
(	O
y	O
p	O
(	O
y	O
,	O
d	O
)	O
∝	O
p	O
(	O
y	O
(	O
cid:90	O
)	O
p	O
(	O
θ|d	O
)	O
∝	O
p	O
(	O
θ	O
)	O
(	O
cid:89	O
)	O
,	O
d	O
)	O
∝	O
|x	O
θ	O
n	O
where	O
p	O
(	O
y	O
(	O
cid:90	O
)	O
,	O
θ	O
)	O
p	O
(	O
θ	O
)	O
(	O
cid:89	O
)	O
θ	O
(	O
cid:90	O
)	O
,	O
x	O
,	O
θ	O
)	O
∝	O
(	O
cid:90	O
)	O
p	O
(	O
y	O
∗	O
∗	O
|x	O
p	O
(	O
yn|θ	O
,	O
xn	O
)	O
∝	O
n	O
∗	O
p	O
(	O
y	O
∗	O
|x	O
,	O
θ	O
)	O
p	O
(	O
θ|d	O
)	O
θ	O
p	O
(	O
yn|θ	O
,	O
xn	O
)	O
347	O
(	O
19.1.4	O
)	O
(	O
19.1.5	O
)	O
(	O
19.1.6	O
)	O
θ	O
yn	O
xn	O
(	O
a	O
)	O
y1	O
x1	O
y∗	O
x∗	O
y∗	O
x∗	O
y1	O
x1	O
yn	O
xn	O
(	O
b	O
)	O
non-parametric	B
prediction	O
figure	O
19.1	O
:	O
(	O
a	O
)	O
:	O
a	O
parametric	O
model	B
for	O
predic-	O
(	O
b	O
)	O
:	O
the	O
form	O
of	O
the	O
tion	O
assuming	O
i.i.d	O
.	O
data	B
.	O
model	B
after	O
integrating	O
out	O
the	O
parameters	O
θ.	O
our	O
non-parametric	B
model	O
will	O
have	O
this	O
structure	B
.	O
after	O
integrating	O
over	O
the	O
parameters	O
θ	O
,	O
the	O
joint	B
data	O
distribution	B
is	O
given	O
by	O
∗	O
p	O
(	O
y	O
∗	O
,	O
y|x	O
,	O
x	O
)	O
=	O
∗	O
p	O
(	O
y	O
∗	O
|x	O
θ	O
p	O
(	O
yn|θ	O
,	O
xn	O
)	O
(	O
19.1.7	O
)	O
(	O
cid:90	O
)	O
,	O
θ	O
)	O
p	O
(	O
θ	O
)	O
(	O
cid:89	O
)	O
n	O
which	O
does	O
not	O
in	O
general	O
factorise	O
into	O
individual	O
datapoint	O
terms	O
,	O
see	O
ﬁg	O
(	O
19.1	O
)	O
.	O
the	O
idea	O
of	O
a	O
non-	O
parametric	O
approach	B
is	O
to	O
specify	O
the	O
form	O
of	O
these	O
dependencies	O
without	O
reference	O
to	O
an	O
explicit	O
parametric	O
model	B
.	O
one	O
route	O
towards	O
a	O
non-parametric	B
model	O
is	O
to	O
start	O
with	O
a	O
parametric	O
model	B
and	O
integrate	O
out	O
the	O
parameters	O
.	O
in	O
order	O
to	O
make	O
this	O
tractable	O
,	O
we	O
use	O
a	O
simple	O
linear	O
parameter	B
predictor	O
with	O
a	O
gaussian	O
parameter	B
prior	O
.	O
for	O
regression	B
this	O
leads	O
to	O
closed	O
form	O
expressions	O
,	O
although	O
the	O
classiﬁcation	B
case	O
will	O
require	O
numerical	B
approximation	O
.	O
19.1.2	O
from	O
bayesian	O
linear	B
models	O
to	O
gaussian	O
processes	O
to	O
develop	O
the	O
gp	O
,	O
we	O
brieﬂy	O
revisit	O
the	O
bayesian	O
linear	B
parameter	I
model	I
of	O
section	O
(	O
18.1.1	O
)	O
.	O
for	O
param-	O
eters	O
w	O
and	O
basis	O
functions	O
φi	O
(	O
x	O
)	O
the	O
output	O
is	O
given	O
by	O
(	O
assuming	O
zero	O
output	O
noise	O
)	O
y	O
=	O
(	O
cid:88	O
)	O
wiφi	O
(	O
x	O
)	O
(	O
19.1.8	O
)	O
(	O
19.1.9	O
)	O
(	O
19.1.10	O
)	O
i	O
if	O
we	O
stack	O
all	O
the	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
into	O
a	O
vector	O
y	O
,	O
then	O
y	O
=	O
φw	O
where	O
φ	O
=	O
(	O
cid:2	O
)	O
φ	O
(	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
φ	O
(	O
xn	O
)	O
(	O
cid:3	O
)	O
t	O
is	O
the	O
design	B
matrix	I
.	O
assuming	O
a	O
gaussian	O
weight	B
prior	O
p	O
(	O
w	O
)	O
=	O
n	O
(	O
w	O
0	O
,	O
σw	O
)	O
and	O
since	O
y	O
is	O
linear	B
in	O
w	O
,	O
the	O
joint	B
output	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
is	O
gaussian	O
distributed	O
.	O
a	O
gaussian	O
prior	B
on	O
w	O
induces	O
a	O
gaussian	O
on	O
the	O
joint	B
y	O
with	O
mean	O
and	O
covariance	B
(	O
cid:104	O
)	O
y	O
(	O
cid:105	O
)	O
=	O
φ	O
(	O
cid:104	O
)	O
w	O
(	O
cid:105	O
)	O
p	O
(	O
w	O
)	O
=	O
0	O
(	O
cid:68	O
)	O
wwt	O
(	O
cid:69	O
)	O
yyt	O
(	O
cid:69	O
)	O
(	O
cid:68	O
)	O
=	O
φ	O
p	O
(	O
w	O
)	O
(	O
cid:18	O
)	O
φς	O
1	O
2	O
w	O
φt	O
=	O
φσwφt	O
=	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
φς	O
1	O
2	O
w	O
(	O
cid:19	O
)	O
t	O
(	O
19.1.11	O
)	O
(	O
19.1.12	O
)	O
from	O
this	O
we	O
see	O
that	O
the	O
σw	O
can	O
be	O
absorbed	O
into	O
φ	O
using	O
its	O
cholesky	O
decomposition	B
.	O
in	O
other	O
words	O
,	O
without	O
loss	O
of	O
generality	O
we	O
may	O
assume	O
σw	O
=	O
i.	O
after	O
integrating	O
out	O
the	O
weights	O
,	O
the	O
bayesian	O
linear	B
regression	O
model	B
induces	O
a	O
gaussian	O
distribution	B
on	O
any	O
set	O
of	O
outputs	O
y	O
as	O
p	O
(	O
y|x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
=	O
n	O
(	O
y	O
0	O
,	O
k	O
)	O
where	O
the	O
covariance	B
matrix	O
k	O
depends	O
on	O
the	O
training	B
inputs	O
alone	O
via	O
[	O
k	O
]	O
n	O
,	O
n	O
(	O
cid:48	O
)	O
=	O
φ	O
(	O
xn	O
)	O
tφ	O
(	O
xn	O
(	O
cid:48	O
)	O
)	O
,	O
(	O
cid:48	O
)	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
n	O
,	O
n	O
(	O
19.1.13	O
)	O
(	O
19.1.14	O
)	O
since	O
the	O
matrix	B
k	O
is	O
formed	O
as	O
the	O
scalar	B
product	I
of	O
vectors	O
,	O
it	O
is	O
by	O
construction	B
positive	O
semideﬁnite	O
,	O
as	O
we	O
saw	O
in	O
section	O
(	O
17.3.2	O
)	O
.	O
after	O
integrating	O
out	O
the	O
weights	O
,	O
the	O
only	O
thing	O
the	O
model	B
directly	O
depends	O
348	O
draft	O
march	O
9	O
,	O
2010	O
non-parametric	B
prediction	O
on	O
is	O
the	O
covariance	B
matrix	O
k.	O
in	O
a	O
gaussian	O
process	O
we	O
directly	O
specify	O
the	O
joint	B
output	O
covariance	B
k	O
as	O
a	O
function	B
of	O
two	O
inputs	O
.	O
speciﬁcally	O
we	O
need	O
to	O
deﬁne	O
the	O
n	O
,	O
n	O
(	O
cid:48	O
)	O
element	O
of	O
the	O
covariance	B
matrix	O
for	O
any	O
two	O
inputs	O
xn	O
and	O
xn	O
(	O
cid:48	O
)	O
[	O
k	O
]	O
n	O
,	O
n	O
(	O
cid:48	O
)	O
=	O
k	O
(	O
xn	O
,	O
xn	O
(	O
cid:48	O
)	O
.	O
this	O
is	O
achieved	O
using	O
a	O
covariance	B
function	I
k	O
(	O
xn	O
,	O
xn	O
(	O
cid:48	O
)	O
)	O
)	O
(	O
19.1.15	O
)	O
the	O
required	O
form	O
of	O
the	O
function	B
k	O
(	O
xn	O
,	O
xn	O
(	O
cid:48	O
)	O
)	O
is	O
very	O
special	O
–	O
when	O
applied	O
to	O
create	O
the	O
elements	O
of	O
the	O
matrix	B
k	O
it	O
must	O
produce	O
a	O
positive	B
deﬁnite	I
matrix	O
.	O
we	O
discuss	O
how	O
to	O
create	O
such	O
covariance	B
functions	O
in	O
section	O
(	O
19.3	O
)	O
.	O
one	O
explicit	O
straightforward	O
construction	B
is	O
to	O
form	O
the	O
covariance	B
function	I
from	O
the	O
scalar	B
product	I
of	O
the	O
basis	O
vector	O
φ	O
(	O
xn	O
)	O
and	O
φ	O
(	O
xn	O
(	O
cid:48	O
)	O
)	O
.	O
for	O
ﬁnite-dimensional	O
φ	O
this	O
is	O
known	O
as	O
a	O
ﬁnite	O
dimensional	O
gaussian	O
process	O
.	O
given	O
any	O
covariance	B
function	I
we	O
can	O
always	O
ﬁnd	O
a	O
corresponding	O
basis	O
vector	O
representation	O
–	O
that	O
is	O
,	O
for	O
any	O
gp	O
,	O
we	O
can	O
always	O
relate	O
this	O
back	O
to	O
a	O
parametric	O
bayesian	O
lpm	O
.	O
however	O
,	O
for	O
many	O
commonly	O
used	O
covariance	B
functions	O
,	O
the	O
basis	O
functions	O
corresponds	O
to	O
inﬁnite	O
dimensional	O
vectors	O
.	O
it	O
is	O
in	O
such	O
cases	O
that	O
the	O
advantages	O
of	O
using	O
the	O
gp	O
framework	O
are	O
particularly	O
evident	O
since	O
we	O
would	O
not	O
be	O
able	O
to	O
compute	O
eﬃciently	O
with	O
the	O
corresponding	O
inﬁnite	O
dimensional	O
parametric	O
model	B
.	O
19.1.3	O
a	O
prior	B
on	O
functions	O
the	O
nature	O
of	O
many	O
machine	O
learning	B
applications	O
is	O
such	O
that	O
the	O
knowledge	O
about	O
the	O
true	O
underlying	O
mechanism	O
behind	O
the	O
data	B
generation	O
process	O
is	O
limited	O
.	O
instead	O
one	O
relies	O
on	O
generic	O
‘	O
smoothness	B
’	O
as-	O
sumptions	O
of	O
the	O
form	O
that	O
for	O
two	O
inputs	O
x	O
and	O
x	O
(	O
cid:48	O
)	O
that	O
are	O
close	O
,	O
the	O
corresponding	O
outputs	O
y	O
and	O
y	O
(	O
cid:48	O
)	O
should	O
be	O
similar	O
.	O
many	O
generic	O
techniques	O
in	O
machine	O
learning	B
can	O
be	O
viewed	O
as	O
diﬀerent	O
characterisations	O
of	O
smoothness	B
.	O
an	O
advantage	O
of	O
the	O
gp	O
framework	O
in	O
this	O
respect	O
is	O
that	O
the	O
mathematical	O
smoothness	B
properties	O
of	O
the	O
functions	O
are	O
well	O
understood	O
,	O
giving	O
conﬁdence	O
in	O
the	O
procedure	O
.	O
for	O
a	O
given	O
covariance	B
matrix	O
k	O
,	O
equation	B
(	O
19.1.13	O
)	O
speciﬁes	O
a	O
distribution	B
on	O
functions1	O
in	O
the	O
following	O
sense	O
:	O
we	O
specify	O
a	O
set	O
of	O
input	O
points	O
x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
and	O
a	O
n	O
×	O
n	O
covariance	B
matrix	O
k.	O
then	O
we	O
draw	O
a	O
vector	O
y	O
from	O
the	O
gaussian	O
deﬁned	O
by	O
equation	B
(	O
19.1.13	O
)	O
.	O
we	O
can	O
then	O
plot	O
the	O
sampled	O
‘	O
function	B
’	O
at	O
the	O
ﬁnite	O
set	O
of	O
points	O
(	O
xn	O
,	O
yn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
what	O
kind	O
of	O
functions	O
does	O
a	O
gp	O
correspond	O
to	O
?	O
consider	O
two	O
scalar	O
inputs	O
,	O
xi	O
and	O
xj	O
separated	O
by	O
a	O
distance	O
|xi	O
−	O
xj|	O
.	O
the	O
corresponding	O
sampled	O
outputs	O
yi	O
and	O
yj	O
ﬂuctuate	O
as	O
diﬀerent	O
functions	O
are	O
drawn	O
.	O
for	O
a	O
covariance	B
function	I
that	O
has	O
a	O
high	O
value	O
for	O
|xi	O
−	O
xj|	O
small	O
,	O
we	O
expect	O
yi	O
and	O
yj	O
to	O
be	O
very	O
similar	O
since	O
they	O
are	O
highly	O
correlated	O
.	O
conversely	O
,	O
for	O
a	O
covariance	B
function	I
that	O
has	O
low	O
value	O
for	O
a	O
given	O
small	O
separation	B
|xi	O
−	O
xj|	O
,	O
we	O
expect	O
yi	O
and	O
yj	O
to	O
be	O
eﬀectively	O
independent2	O
.	O
in	O
general	O
,	O
we	O
would	O
expect	O
the	O
correlation	B
between	O
yi	O
and	O
yj	O
to	O
decrease	O
the	O
further	O
apart	O
xi	O
and	O
xj	O
are	O
.	O
in	O
ﬁg	O
(	O
19.2a	O
)	O
we	O
show	O
three	O
sample	B
functions	O
drawn	O
from	O
a	O
squared	B
exponential	I
covariance	O
function	B
de-	O
ﬁned	O
over	O
500	O
points	O
uniformly	O
spaced	O
from	O
−2	O
to	O
3.	O
each	O
sampled	O
function	B
looks	O
reasonably	O
smooth	O
.	O
conversely	O
,	O
for	O
the	O
ornstein	O
uhlenbeck	O
covariance	B
function	I
,	O
the	O
sampled	O
functions	O
ﬁg	O
(	O
19.2c	O
)	O
look	O
locally	O
rough	O
.	O
these	O
smoothness	B
properties	O
are	O
related	O
to	O
the	O
form	O
of	O
the	O
covariance	B
function	I
,	O
as	O
discussed	O
in	O
section	O
(	O
19.4.1	O
)	O
.	O
the	O
zero	O
mean	B
assumption	O
implies	O
that	O
if	O
we	O
were	O
to	O
draw	O
a	O
large	O
number	O
of	O
such	O
‘	O
functions	O
’	O
,	O
the	O
mean	B
across	O
these	O
functions	O
at	O
a	O
given	O
point	O
x	O
tends	O
to	O
zero	O
.	O
similarly	O
,	O
for	O
any	O
two	O
points	O
x	O
and	O
x	O
(	O
cid:48	O
)	O
if	O
we	O
compute	O
the	O
sample	B
covariance	O
between	O
the	O
corresponding	O
y	O
and	O
y	O
(	O
cid:48	O
)	O
for	O
all	O
such	O
sampled	O
functions	O
,	O
this	O
will	O
tend	O
to	O
the	O
covariance	B
function	I
value	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
.	O
the	O
zero-mean	O
assumption	O
can	O
be	O
easily	O
relaxed	O
by	O
deﬁning	O
a	O
mean	B
function	O
m	O
(	O
x	O
)	O
to	O
give	O
p	O
(	O
y|x	O
)	O
=	O
n	O
(	O
y	O
m	O
,	O
k	O
)	O
.	O
in	O
many	O
practical	O
situations	O
one	O
typically	O
deals	O
with	O
‘	O
detrended	O
’	O
data	B
in	O
which	O
such	O
mean	B
trends	O
have	O
been	O
already	O
removed	O
.	O
for	O
this	O
reason	O
much	O
of	O
the	O
development	O
of	O
gps	O
in	O
the	O
machine	O
learning	B
literature	O
is	O
for	O
the	O
zero	O
mean	B
case	O
.	O
1the	O
term	O
‘	O
function	B
’	O
is	O
potentially	O
confusing	O
since	O
we	O
do	O
not	O
have	O
an	O
explicit	O
functional	O
form	O
for	O
the	O
input	O
output-mapping	O
.	O
for	O
any	O
ﬁnite	O
set	O
of	O
inputs	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
the	O
values	O
for	O
the	O
‘	O
function	B
’	O
are	O
given	O
by	O
the	O
outputs	O
at	O
those	O
points	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
.	O
2for	O
periodic	B
functions	O
,	O
however	O
,	O
we	O
would	O
expect	O
high	O
correlation	O
at	O
separating	O
distances	O
corresponding	O
to	O
the	O
period	O
of	O
the	O
function	B
.	O
draft	O
march	O
9	O
,	O
2010	O
349	O
gaussian	O
process	O
prediction	B
19.2	O
gaussian	O
process	O
prediction	B
for	O
a	O
dataset	O
d	O
and	O
novel	O
input	O
x∗	O
,	O
a	O
zero	O
mean	B
gp	O
makes	O
a	O
gaussian	O
model	B
of	O
the	O
joint	B
outputs	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
,	O
y∗	O
given	O
the	O
joint	B
inputs	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
x∗	O
.	O
for	O
convenience	O
we	O
write	O
this	O
as	O
(	O
cid:0	O
)	O
y	O
,	O
y	O
∗	O
0n	O
+1	O
,	O
k+	O
(	O
cid:1	O
)	O
p	O
(	O
y	O
,	O
y	O
∗	O
|x	O
,	O
x	O
∗	O
)	O
=	O
n	O
(	O
19.2.1	O
)	O
where	O
0n	O
+1	O
is	O
a	O
n	O
+1	O
dimensional	O
zero-vector	O
.	O
the	O
covariance	B
matrix	O
k+	O
is	O
a	O
block	O
matrix	B
with	O
elements	O
	O
k+	O
≡	O
kx	O
,	O
x	O
kx	O
,	O
x∗	O
	O
kx∗	O
,	O
x	O
where	O
kx	O
,	O
x	O
is	O
the	O
covariance	B
matrix	O
of	O
the	O
training	B
inputs	O
x	O
=	O
(	O
cid:8	O
)	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
(	O
cid:9	O
)	O
–	O
that	O
is	O
kx∗	O
,	O
x∗	O
[	O
kx	O
,	O
x	O
]	O
n	O
,	O
n	O
(	O
cid:48	O
)	O
≡	O
k	O
(	O
xn	O
,	O
xn	O
(	O
cid:48	O
)	O
)	O
,	O
(	O
cid:48	O
)	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
n	O
,	O
n	O
the	O
n	O
×	O
1	O
vector	O
kx	O
,	O
x∗	O
has	O
elements	O
∗	O
)	O
[	O
kx	O
,	O
x∗	O
]	O
n	O
,	O
∗	O
≡	O
k	O
(	O
xn	O
,	O
x	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
kx∗	O
,	O
x	O
is	O
the	O
transpose	O
of	O
the	O
above	O
vector	O
.	O
the	O
scalar	O
covariance	O
is	O
given	O
by	O
(	O
19.2.2	O
)	O
(	O
19.2.3	O
)	O
(	O
19.2.4	O
)	O
(	O
19.2.5	O
)	O
kx∗	O
,	O
x∗	O
≡	O
k	O
(	O
x	O
∗	O
∗	O
)	O
,	O
x	O
the	O
predictive	O
distribution	O
p	O
(	O
y∗	O
giving	O
a	O
gaussian	O
distribution	B
(	O
cid:0	O
)	O
y	O
|y	O
,	O
d	O
)	O
is	O
obtained	O
by	O
gaussian	O
conditioning	B
using	O
the	O
results	O
in	O
deﬁnition	O
(	O
78	O
)	O
,	O
x	O
,	O
xkx	O
,	O
x∗	O
(	O
cid:1	O
)	O
∗	O
p	O
(	O
y	O
∗	O
|x	O
,	O
d	O
)	O
=	O
n	O
∗	O
kx∗	O
,	O
xk−1	O
x	O
,	O
xy	O
,	O
kx∗	O
,	O
x∗	O
−	O
kx∗	O
,	O
xk−1	O
for	O
ﬁxed	O
hyperparameters	O
,	O
gp	O
regression	B
is	O
an	O
exact	O
method	O
and	O
there	O
are	O
no	O
issues	O
with	O
local	O
minima	O
.	O
furthermore	O
,	O
gps	O
are	O
attractive	O
since	O
they	O
automatically	O
model	B
uncertainty	O
in	O
the	O
predictions	O
.	O
however	O
,	O
the	O
computational	B
complexity	I
for	O
making	O
a	O
prediction	B
is	O
o	O
(	O
cid:0	O
)	O
n	O
3	O
(	O
cid:1	O
)	O
due	O
to	O
the	O
requirement	O
of	O
performing	O
the	O
matrix	B
inversion	O
(	O
or	O
solving	B
the	O
corresponding	O
linear	B
system	O
by	O
gaussian	O
elimination	O
)	O
.	O
this	O
can	O
be	O
prohibitively	O
expensive	O
for	O
large	O
datasets	O
and	O
a	O
large	O
body	O
of	O
research	O
on	O
eﬃcient	B
approximations	O
exists	O
.	O
a	O
discussion	O
of	O
these	O
techniques	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
,	O
and	O
the	O
reader	O
is	O
referred	O
to	O
[	O
230	O
]	O
.	O
19.2.1	O
regression	B
with	O
noisy	O
training	O
outputs	O
to	O
prevent	O
overﬁtting	B
to	O
noisy	O
data	O
it	O
is	O
useful	O
to	O
assume	O
that	O
a	O
training	B
output	O
yn	O
is	O
the	O
result	O
of	O
some	O
clean	O
process	O
f	O
n	O
corrupted	O
by	O
additive	O
gaussian	O
noise	O
,	O
yn	O
=	O
f	O
n	O
+	O
n	O
,	O
where	O
(	O
19.2.6	O
)	O
in	O
this	O
case	O
our	O
interest	O
is	O
to	O
predict	O
the	O
clean	O
signal	O
f∗	O
for	O
a	O
novel	O
input	O
x∗	O
.	O
then	O
the	O
distribution	B
p	O
(	O
y	O
,	O
f∗	O
(	O
cid:18	O
)	O
kx	O
,	O
x	O
+	O
σ2i	O
kx	O
,	O
x∗	O
|x	O
,	O
x∗	O
)	O
is	O
a	O
zero	O
mean	B
gaussian	O
with	O
block	O
covariance	B
matrix	O
n	O
∼	O
n	O
(	O
cid:19	O
)	O
(	O
19.2.7	O
)	O
kx∗	O
,	O
x	O
kx∗	O
,	O
x∗	O
so	O
that	O
kx	O
,	O
x	O
is	O
replaced	O
by	O
kx	O
,	O
x	O
+	O
σ2i	O
in	O
forming	O
the	O
prediction	B
,	O
equation	B
(	O
19.2.5	O
)	O
.	O
350	O
draft	O
march	O
9	O
,	O
2010	O
(	O
cid:0	O
)	O
n	O
0	O
,	O
σ2	O
(	O
cid:1	O
)	O
covariance	B
functions	O
example	O
84.	O
training	B
data	O
from	O
a	O
one-dimensional	O
input	O
x	O
and	O
one	O
dimensional	O
output	O
y	O
are	O
plotted	O
in	O
ﬁg	O
(	O
19.2b	O
,	O
d	O
)	O
,	O
along	O
with	O
the	O
mean	B
regression	O
function	B
ﬁt	O
,	O
based	O
on	O
two	O
diﬀerent	O
covariance	B
functions	O
.	O
note	O
how	O
the	O
smoothness	B
of	O
the	O
prior	B
translates	O
into	O
smoothness	B
of	O
the	O
prediction	B
.	O
the	O
smoothness	B
of	O
the	O
function	B
space	O
prior	B
is	O
a	O
consequence	O
of	O
the	O
choice	O
of	O
covariance	B
function	I
.	O
naively	O
,	O
we	O
can	O
partially	O
understand	O
this	O
by	O
the	O
behaviour	O
of	O
the	O
covariance	B
function	I
at	O
the	O
origin	O
,	O
section	O
(	O
19.4.1	O
)	O
.	O
see	O
demogpreg.m	O
the	O
marginal	B
likelihood	I
and	O
hyperparameter	B
learning	O
for	O
a	O
set	O
of	O
n	O
one-dimensional	O
training	B
inputs	O
represented	O
by	O
the	O
n	O
×	O
1	O
dimensional	O
vector	O
y	O
and	O
a	O
covariance	B
matrix	O
k	O
deﬁned	O
on	O
the	O
inputs	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
the	O
log	O
marginal	B
likelihood	I
is	O
one	O
can	O
learn	O
any	O
free	O
parameters	O
of	O
the	O
covariance	B
function	I
by	O
maximising	O
the	O
marginal	B
likelihood	I
.	O
for	O
example	O
,	O
a	O
squared	B
exponential	I
covariance	O
function	B
may	O
have	O
parameters	O
λ	O
,	O
v0	O
:	O
log	O
p	O
(	O
y|x	O
)	O
=	O
−	O
1	O
2	O
log	O
det	O
(	O
2πk	O
)	O
1	O
2	O
ytk−1y	O
−	O
(	O
cid:26	O
)	O
2	O
λ	O
(	O
cid:0	O
)	O
x	O
−	O
x	O
−	O
1	O
(	O
cid:27	O
)	O
(	O
cid:48	O
)	O
(	O
cid:1	O
)	O
2	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
v0	O
exp	O
(	O
19.2.8	O
)	O
(	O
19.2.9	O
)	O
the	O
λ	O
parameter	B
in	O
equation	B
(	O
19.2.12	O
)	O
speciﬁes	O
the	O
appropriate	O
length-scale	O
of	O
the	O
inputs	O
,	O
and	O
v0	O
the	O
variance	B
of	O
the	O
function	B
.	O
the	O
dependence	B
of	O
the	O
marginal	B
likelihood	I
(	O
19.2.8	O
)	O
on	O
the	O
parameters	O
is	O
typically	O
complex	O
and	O
no	O
closed	O
form	O
expression	O
for	O
the	O
maximum	B
likelihood	I
optimum	O
exists	O
;	O
in	O
this	O
case	O
on	O
resorts	O
to	O
numerical	B
optimisation	O
techniques	O
such	O
as	O
conjugate	O
gradients	O
.	O
vector	O
inputs	O
for	O
regression	B
with	O
vector	O
inputs	O
and	O
scalar	O
outputs	O
we	O
need	O
to	O
deﬁne	O
a	O
covariance	B
as	O
a	O
function	B
of	O
the	O
two	O
vectors	O
,	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
.	O
using	O
the	O
multiplicative	O
property	O
of	O
covariance	B
functions	O
,	O
deﬁnition	O
(	O
93	O
)	O
,	O
a	O
simple	O
way	O
to	O
do	O
this	O
is	O
to	O
deﬁne	O
(	O
cid:48	O
)	O
i	O
)	O
k	O
(	O
xi	O
,	O
x	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
(	O
cid:89	O
)	O
(	O
19.2.10	O
)	O
i	O
for	O
example	O
,	O
for	O
the	O
squared	B
exponential	I
covariance	O
function	B
this	O
gives	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
e	O
−	O
(	O
x−x	O
(	O
cid:48	O
)	O
)	O
2	O
(	O
19.2.11	O
)	O
though	O
‘	O
correlated	O
’	O
forms	O
are	O
possible	O
as	O
well	O
,	O
see	O
exercise	O
(	O
195	O
)	O
.	O
we	O
can	O
generalise	O
the	O
above	O
using	O
parameters	O
:	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
v0	O
exp	O
(	O
cid:40	O
)	O
d	O
(	O
cid:88	O
)	O
l=1	O
1	O
2	O
−	O
(	O
cid:41	O
)	O
(	O
cid:1	O
)	O
2	O
(	O
cid:0	O
)	O
xl	O
−	O
x	O
(	O
cid:48	O
)	O
l	O
λl	O
(	O
19.2.12	O
)	O
where	O
xl	O
is	O
the	O
lth	O
component	O
of	O
x	O
and	O
θ	O
=	O
(	O
v0	O
,	O
λ1	O
,	O
.	O
.	O
.	O
,	O
λd	O
)	O
are	O
parameters	O
.	O
the	O
λl	O
in	O
equation	B
(	O
19.2.12	O
)	O
allow	O
a	O
diﬀerent	O
length	O
scale	O
on	O
each	O
input	O
dimension	O
and	O
can	O
be	O
learned	O
by	O
numerically	O
maximising	O
the	O
marginal	B
likelihood	I
.	O
for	O
irrelevant	O
inputs	O
,	O
the	O
corresponding	O
λl	O
will	O
become	O
small	O
,	O
and	O
the	O
model	B
will	O
ignore	O
the	O
lth	O
input	O
dimension	O
.	O
this	O
is	O
closely	O
related	O
to	O
automatic	B
relevance	I
determination	I
[	O
181	O
]	O
.	O
19.3	O
covariance	B
functions	O
covariance	B
functions	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
are	O
special	O
in	O
that	O
they	O
deﬁne	O
elements	O
of	O
a	O
positive	B
deﬁnite	I
matrix	O
.	O
these	O
functions	O
are	O
also	O
referred	O
to	O
as	O
‘	O
kernels	O
’	O
,	O
particulary	O
in	O
the	O
machine	O
learning	B
literature	O
.	O
draft	O
march	O
9	O
,	O
2010	O
351	O
covariance	B
functions	O
(	O
a	O
)	O
(	O
c	O
)	O
(	O
b	O
)	O
(	O
d	O
)	O
(	O
a	O
)	O
:	O
figure	O
19.2	O
:	O
the	O
input	O
space	O
from	O
-2	O
to	O
3	O
is	O
split	O
evenly	O
into	O
1000	O
points	O
x1	O
,	O
.	O
.	O
.	O
,	O
x1000	O
.	O
three	O
samples	O
from	O
a	O
gp	O
prior	B
with	O
squared	B
exponential	I
(	O
se	O
)	O
covariance	B
function	I
,	O
λ	O
=	O
2.	O
the	O
1000	O
×	O
1000	O
covariance	B
matrix	O
k	O
is	O
deﬁned	O
using	O
the	O
se	O
kernel	B
,	O
from	O
which	O
the	O
samples	O
are	O
drawn	O
using	O
mvrandn	O
(	O
zeros	O
(	O
1000,1	O
)	O
,	O
k,3	O
)	O
.	O
(	O
b	O
)	O
:	O
prediction	B
based	O
on	O
training	B
points	O
.	O
plotted	O
is	O
the	O
posterior	B
predicted	O
function	B
based	O
on	O
the	O
se	O
covariance	B
.	O
the	O
central	O
line	O
is	O
the	O
mean	B
prediction	O
,	O
with	O
standard	O
(	O
c	O
)	O
:	O
three	O
samples	O
from	O
the	O
ornstein-	O
errors	O
bars	O
on	O
either	O
side	O
.	O
the	O
log	O
marginal	B
likelihood	I
is	O
≈	O
70	O
.	O
(	O
d	O
)	O
:	O
posterior	B
prediction	O
for	O
the	O
ou	O
covariance	B
.	O
the	O
log	O
marginal	B
uhlenbeck	O
gp	O
prior	B
with	O
λ	O
=	O
2.	O
likelihood	B
is	O
≈	O
3	O
,	O
meaning	O
that	O
the	O
se	O
covariance	B
is	O
much	O
more	O
heavily	O
supported	O
by	O
the	O
data	B
than	O
the	O
rougher	O
ou	O
covariance	B
.	O
deﬁnition	O
91	O
(	O
covariance	B
function	I
)	O
.	O
given	O
any	O
collection	O
of	O
points	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
,	O
a	O
covariance	B
function	I
k	O
(	O
xi	O
,	O
xj	O
)	O
deﬁnes	O
the	O
elements	O
of	O
a	O
m	O
×	O
m	O
matrix	B
[	O
c	O
]	O
i	O
,	O
j	O
=	O
k	O
(	O
xi	O
,	O
xj	O
)	O
such	O
that	O
c	O
is	O
positive	O
semideﬁnite	O
.	O
19.3.1	O
making	O
new	O
covariance	B
functions	O
from	O
old	O
the	O
following	O
rules	O
(	O
which	O
can	O
all	O
be	O
proved	O
directly	O
)	O
generate	O
new	O
covariance	B
functions	O
from	O
existing	O
covariance	B
functions	O
k1	O
,	O
k2	O
[	O
182	O
]	O
,	O
[	O
230	O
]	O
.	O
deﬁnition	O
92	O
(	O
sum	O
)	O
.	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
k1	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
+	O
k2	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
(	O
19.3.1	O
)	O
352	O
draft	O
march	O
9	O
,	O
2010	O
−2−1.5−1−0.500.511.522.53−3−2.5−2−1.5−1−0.500.511.52−2−1.5−1−0.500.511.522.53−1.5−1−0.500.511.52−2−1.5−1−0.500.511.522.53−2.5−2−1.5−1−0.500.511.522.5−2−1.5−1−0.500.511.522.53−1.5−1−0.500.511.5	O
covariance	B
functions	O
deﬁnition	O
93	O
(	O
product	O
)	O
.	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
k1	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
k2	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
(	O
cid:18	O
)	O
x	O
y	O
(	O
cid:19	O
)	O
,	O
deﬁnition	O
94	O
(	O
product	O
spaces	O
)	O
.	O
for	O
z	O
=	O
k	O
(	O
z	O
,	O
z	O
(	O
cid:48	O
)	O
)	O
=	O
k1	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
+	O
k2	O
(	O
y	O
,	O
y	O
(	O
cid:48	O
)	O
)	O
and	O
k	O
(	O
z	O
,	O
z	O
(	O
cid:48	O
)	O
)	O
=	O
k1	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
k2	O
(	O
y	O
,	O
y	O
(	O
cid:48	O
)	O
)	O
deﬁnition	O
95	O
(	O
vertical	O
rescaling	O
)	O
.	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
a	O
(	O
x	O
)	O
k1	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
a	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
for	O
any	O
function	B
a	O
(	O
x	O
)	O
.	O
deﬁnition	O
96	O
(	O
warping	O
and	O
embedding	O
)	O
.	O
(	O
cid:0	O
)	O
u	O
(	O
x	O
)	O
,	O
u	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
(	O
cid:1	O
)	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
k1	O
(	O
19.3.2	O
)	O
(	O
19.3.3	O
)	O
(	O
19.3.4	O
)	O
(	O
19.3.5	O
)	O
(	O
19.3.6	O
)	O
for	O
any	O
mapping	O
x	O
→	O
u	O
(	O
x	O
)	O
,	O
where	O
the	O
mapping	O
u	O
(	O
x	O
)	O
has	O
arbitrary	O
dimension	O
.	O
a	O
small	O
collection	O
of	O
covariance	B
functions	O
commonly	O
used	O
in	O
machine	O
learning	B
is	O
given	O
below	O
.	O
we	O
refer	O
the	O
reader	O
to	O
[	O
230	O
]	O
and	O
[	O
107	O
]	O
for	O
further	O
popular	O
covariance	B
functions	O
.	O
19.3.2	O
stationary	B
covariance	O
functions	O
deﬁnition	O
97	O
(	O
stationary	B
kernel	O
)	O
.	O
a	O
kernel	B
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
is	O
stationary	B
if	O
the	O
kernel	B
depends	O
only	O
on	O
the	O
separation	B
x	O
−	O
x	O
(	O
cid:48	O
)	O
.	O
that	O
is	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
k	O
(	O
x	O
−	O
x	O
(	O
cid:48	O
)	O
)	O
(	O
19.3.7	O
)	O
following	O
the	O
notation	O
in	O
[	O
230	O
]	O
,	O
for	O
a	O
stationary	B
covariance	O
function	B
we	O
may	O
write	O
k	O
(	O
d	O
)	O
(	O
19.3.8	O
)	O
where	O
d	O
=	O
x	O
−	O
x	O
(	O
cid:48	O
)	O
.	O
this	O
means	O
that	O
for	O
functions	O
drawn	O
from	O
the	O
gp	O
,	O
on	O
average	B
,	O
the	O
functions	O
depend	O
only	O
on	O
the	O
distance	O
between	O
inputs	O
and	O
not	O
on	O
the	O
absolute	O
position	O
of	O
an	O
input	O
.	O
in	O
other	O
words	O
,	O
the	O
functions	O
are	O
on	O
average	B
translation	O
invariant	O
.	O
for	O
isotropic	B
covariance	I
functions	I
,	O
the	O
covariance	B
is	O
deﬁned	O
as	O
a	O
function	B
of	O
the	O
distance	O
k	O
(	O
|d|	O
)	O
.	O
draft	O
march	O
9	O
,	O
2010	O
353	O
deﬁnition	O
98	O
(	O
squared	B
exponential	I
)	O
.	O
k	O
(	O
d	O
)	O
=	O
e	O
−|d|2	O
covariance	B
functions	O
(	O
19.3.9	O
)	O
the	O
squared	B
exponential	I
is	O
one	O
of	O
the	O
most	O
common	O
covariance	B
functions	O
.	O
there	O
are	O
many	O
ways	O
to	O
show	O
that	O
this	O
is	O
a	O
covariance	B
function	I
.	O
an	O
elementary	O
technique	O
is	O
to	O
consider	O
2|xn|2	O
−	O
1	O
=	O
e	O
2|xn	O
(	O
cid:48	O
)	O
|2	O
−	O
1	O
e	O
e	O
(	O
xn	O
)	O
txn	O
(	O
cid:48	O
)	O
(	O
19.3.10	O
)	O
(	O
cid:16	O
)	O
xn−xn	O
(	O
cid:48	O
)	O
(	O
cid:17	O
)	O
t	O
(	O
cid:16	O
)	O
xn−xn	O
(	O
cid:48	O
)	O
(	O
cid:17	O
)	O
−	O
1	O
2	O
e	O
the	O
ﬁrst	O
two	O
factors	O
form	O
a	O
kernel	B
of	O
the	O
form	O
φ	O
(	O
xn	O
)	O
φ	O
(	O
xn	O
(	O
cid:48	O
)	O
is	O
the	O
linear	B
kernel	O
.	O
taking	O
the	O
exponential	B
and	O
writing	O
the	O
power	O
series	O
expansion	O
of	O
the	O
exponential	B
,	O
we	O
have	O
)	O
.	O
in	O
the	O
ﬁnal	O
term	O
k1	O
(	O
xn	O
,	O
xn	O
(	O
cid:48	O
)	O
)	O
=	O
(	O
xn	O
)	O
t	O
xn	O
(	O
cid:48	O
)	O
ek1	O
(	O
xn	O
,	O
xn	O
(	O
cid:48	O
)	O
)	O
=	O
1	O
i	O
!	O
ki	O
1	O
(	O
xn	O
,	O
xn	O
(	O
cid:48	O
)	O
)	O
(	O
19.3.11	O
)	O
∞	O
(	O
cid:88	O
)	O
i=1	O
this	O
can	O
be	O
expressed	O
as	O
a	O
series	O
of	O
integer	O
powers	O
of	O
k1	O
,	O
with	O
positive	O
coeﬃcients	O
.	O
by	O
the	O
product	O
(	O
with	O
itself	O
)	O
and	O
sum	O
rules	O
above	O
,	O
this	O
is	O
therefore	O
a	O
kernel	B
as	O
well	O
.	O
we	O
then	O
use	O
the	O
fact	O
that	O
equation	B
(	O
19.3.10	O
)	O
is	O
the	O
product	O
of	O
two	O
kernels	O
,	O
and	O
hence	O
also	O
a	O
kernel	B
.	O
deﬁnition	O
99	O
(	O
γ-exponential	B
)	O
.	O
k	O
(	O
d	O
)	O
=	O
e	O
−|d|γ	O
,	O
0	O
<	O
γ	O
≤	O
2	O
(	O
19.3.12	O
)	O
when	O
γ	O
=	O
2	O
we	O
have	O
the	O
squared	B
exponential	I
covariance	O
function	B
.	O
when	O
γ	O
=	O
1	O
this	O
is	O
the	O
ornstein-	O
uhlenbeck	O
covariance	B
function	I
.	O
deﬁnition	O
100	O
(	O
mat´ern	O
)	O
.	O
k	O
(	O
d	O
)	O
=	O
|d|νkν	O
(	O
|d|	O
)	O
where	O
kν	O
is	O
a	O
modiﬁed	O
bessel	O
function	B
,	O
ν	O
>	O
0	O
.	O
(	O
19.3.13	O
)	O
deﬁnition	O
101	O
(	O
rational	B
quadratic	I
)	O
.	O
k	O
(	O
d	O
)	O
=	O
(	O
cid:0	O
)	O
1	O
+	O
|d|2	O
(	O
cid:1	O
)	O
−α	O
,	O
α	O
>	O
0	O
(	O
19.3.14	O
)	O
deﬁnition	O
102	O
(	O
periodic	B
)	O
.	O
for	O
1-dimensional	O
x	O
and	O
x	O
(	O
cid:48	O
)	O
,	O
a	O
stationary	B
(	O
and	O
isotropic	B
)	O
covariance	B
function	I
can	O
be	O
obtained	O
by	O
ﬁrst	O
mapping	O
x	O
to	O
the	O
two	O
dimensional	O
vector	O
u	O
(	O
x	O
)	O
=	O
(	O
cos	O
(	O
x	O
)	O
,	O
sin	O
(	O
x	O
)	O
)	O
and	O
then	O
using	O
the	O
se	O
covariance	B
e−	O
(	O
u	O
(	O
x	O
)	O
−u	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
)	O
2	O
−λ	O
sin2	O
(	O
ω	O
(	O
x−x	O
(	O
cid:48	O
)	O
)	O
)	O
,	O
(	O
19.3.15	O
)	O
(	O
cid:48	O
)	O
)	O
=	O
e	O
λ	O
>	O
0	O
k	O
(	O
x	O
−	O
x	O
see	O
[	O
182	O
]	O
and	O
[	O
230	O
]	O
.	O
354	O
draft	O
march	O
9	O
,	O
2010	O
covariance	B
functions	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
19.3	O
:	O
(	O
a	O
)	O
:	O
plots	O
of	O
the	O
gamma-exponential	O
covariance	B
e−|x|γ	O
versus	O
x.	O
the	O
case	O
γ	O
=	O
2	O
corresponds	O
to	O
the	O
se	O
covariance	B
function	I
.	O
the	O
drop	O
in	O
the	O
covariance	B
is	O
much	O
more	O
rapid	O
as	O
a	O
function	B
of	O
the	O
separation	B
x	O
for	O
small	O
γ	O
,	O
suggesting	O
that	O
the	O
functions	O
corresponding	O
to	O
smaller	O
γ	O
will	O
be	O
locally	O
rough	O
(	O
b	O
)	O
:	O
as	O
for	O
(	O
a	O
)	O
but	O
zoomed	O
in	O
towards	O
the	O
(	O
though	O
possess	O
relatively	O
higher	O
long	O
range	O
correlation	B
)	O
.	O
origin	O
.	O
for	O
the	O
se	O
case	O
,	O
γ	O
=	O
2	O
,	O
the	O
derivative	O
of	O
the	O
covariance	B
function	I
is	O
zero	O
,	O
whereas	O
the	O
ou	O
covariance	B
γ	O
=	O
1	O
has	O
a	O
ﬁrst	B
order	I
contribution	O
to	O
the	O
drop	O
in	O
the	O
covariance	B
,	O
suggesting	O
that	O
locally	O
ou	O
sampled	O
functions	O
will	O
be	O
much	O
rougher	O
than	O
se	O
functions	O
.	O
19.3.3	O
non-stationary	B
covariance	O
functions	O
deﬁnition	O
103	O
(	O
linear	B
)	O
.	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
xtx	O
(	O
cid:48	O
)	O
deﬁnition	O
104	O
(	O
neural	B
network	I
)	O
.	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
arcsin	O
(	O
cid:32	O
)	O
2xtx	O
(	O
cid:48	O
)	O
(	O
cid:112	O
)	O
√1	O
+	O
2xtx	O
1	O
+	O
2x	O
(	O
cid:48	O
)	O
tx	O
(	O
cid:48	O
)	O
(	O
cid:33	O
)	O
(	O
19.3.16	O
)	O
(	O
19.3.17	O
)	O
the	O
functions	O
deﬁned	O
by	O
this	O
covariance	B
always	O
go	O
through	O
the	O
origin	O
.	O
to	O
shift	O
this	O
,	O
one	O
may	O
use	O
the	O
embedding	O
x	O
→	O
(	O
1	O
,	O
x	O
)	O
where	O
the	O
1	O
has	O
the	O
eﬀect	O
of	O
a	O
‘	O
bias	B
’	O
from	O
the	O
origin	O
.	O
to	O
change	O
the	O
scale	O
of	O
the	O
bias	B
and	O
and	O
non-bias	O
contributions	O
one	O
may	O
use	O
additional	O
parameters	O
x	O
→	O
(	O
b	O
,	O
λx	O
)	O
.	O
the	O
nn	O
covariance	B
function	I
can	O
be	O
derived	O
as	O
a	O
limiting	O
case	O
of	O
a	O
neural	B
network	I
with	O
inﬁnite	O
hidden	O
units	O
[	O
296	O
]	O
,	O
and	O
making	O
use	O
of	O
exact	O
integral	O
results	O
in	O
[	O
21	O
]	O
.	O
deﬁnition	O
105	O
(	O
gibbs	O
)	O
.	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
(	O
cid:89	O
)	O
i	O
(	O
cid:18	O
)	O
ri	O
(	O
x	O
)	O
ri	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
i	O
(	O
x	O
)	O
+	O
r2	O
r2	O
i	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
for	O
functions	O
ri	O
(	O
x	O
)	O
>	O
0.	O
draft	O
march	O
9	O
,	O
2010	O
(	O
cid:19	O
)	O
1	O
2	O
−	O
(	O
xi−x	O
(	O
cid:48	O
)	O
i	O
)	O
2	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
e	O
(	O
x	O
)	O
+r2	O
i	O
r2	O
i	O
(	O
19.3.18	O
)	O
355	O
00.511.522.5300.20.40.60.81	O
21.51.00.500.020.040.060.080.10.120.140.160.180.20.650.70.750.80.850.90.951	O
21.51.00.5	O
analysis	B
of	O
covariance	B
functions	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
19.4	O
:	O
samples	O
from	O
a	O
gp	O
prior	B
for	O
500	O
x	O
points	O
uniformly	O
placed	O
from	O
-20	O
to	O
20.	O
from	O
the	O
periodic	B
covariance	O
function	B
exp	O
(	O
cid:0	O
)	O
with	O
bias	O
b	O
=	O
5	O
and	O
λ	O
=	O
1	O
.	O
−2	O
sin2	O
0.5	O
(	O
x	O
−	O
x	O
(	O
cid:48	O
)	O
)	O
(	O
cid:1	O
)	O
.	O
(	O
b	O
)	O
:	O
neural	B
network	I
covariance	O
function	B
(	O
a	O
)	O
:	O
sampled	O
19.4	O
analysis	B
of	O
covariance	B
functions	O
19.4.1	O
smoothness	B
of	O
the	O
functions	O
we	O
examine	O
local	B
smoothness	O
for	O
a	O
translation	O
invariant	O
kernel	B
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
k	O
(	O
x	O
−	O
x	O
(	O
cid:48	O
)	O
)	O
.	O
for	O
two	O
one-	O
dimensional	O
points	O
x	O
and	O
x	O
(	O
cid:48	O
)	O
,	O
separated	O
by	O
a	O
small	O
amount	O
δ	O
(	O
cid:28	O
)	O
1	O
,	O
x	O
(	O
cid:48	O
)	O
=	O
x	O
+	O
δ	O
,	O
the	O
covariance	B
between	O
the	O
outputs	O
y	O
and	O
y	O
(	O
cid:48	O
)	O
is	O
,	O
by	O
taylor	O
expansion	O
,	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
≈	O
k	O
(	O
0	O
)	O
+	O
δ	O
dx|x=0	O
+	O
o	O
(	O
cid:0	O
)	O
δ2	O
(	O
cid:1	O
)	O
dk	O
(	O
19.4.1	O
)	O
so	O
that	O
the	O
change	O
in	O
the	O
covariance	B
at	O
the	O
local	B
level	O
is	O
dominated	O
by	O
the	O
ﬁrst	O
derivative	O
of	O
the	O
covariance	B
function	I
.	O
for	O
the	O
se	O
covariance	B
k	O
(	O
x	O
)	O
=	O
e−x2	O
,	O
dk	O
dx	O
=	O
−2xe	O
−x2	O
(	O
19.4.2	O
)	O
is	O
zero	O
at	O
x	O
=	O
0.	O
this	O
means	O
that	O
for	O
the	O
se	O
covariance	B
function	I
,	O
the	O
ﬁrst	B
order	I
change	O
in	O
the	O
covariance	B
is	O
zero	O
,	O
and	O
only	O
higher	O
order	O
δ2	O
terms	O
contribute	O
.	O
for	O
the	O
ornstein-uhlenbeck	O
covariance	B
,	O
k	O
(	O
x	O
)	O
=	O
e−|x|	O
,	O
the	O
right	O
derivative	O
at	O
the	O
origin	O
is	O
lim	O
δ→0	O
k	O
(	O
δ	O
)	O
−	O
k	O
(	O
0	O
)	O
δ	O
=	O
lim	O
δ→0	O
e−δ	O
−	O
1	O
δ	O
=	O
−1	O
(	O
19.4.3	O
)	O
where	O
this	O
result	O
is	O
obtained	O
using	O
l	O
’	O
hˆopital	O
’	O
s	O
rule	O
.	O
hence	O
for	O
the	O
ou	O
covariance	B
function	I
,	O
there	O
is	O
a	O
ﬁrst	B
order	I
negative	O
change	O
in	O
the	O
covariance	B
;	O
at	O
the	O
local	B
level	O
,	O
this	O
decrease	O
in	O
the	O
covariance	B
is	O
therefore	O
much	O
more	O
rapid	O
than	O
for	O
the	O
se	O
covariance	B
,	O
see	O
ﬁg	O
(	O
19.3	O
)	O
.	O
since	O
low	O
covariance	O
implies	O
low	O
dependence	O
(	O
in	O
gaussian	O
distributions	O
)	O
,	O
locally	O
the	O
functions	O
generated	O
from	O
the	O
ou	O
process	O
are	O
rough	O
,	O
whereas	O
they	O
are	O
smooth	O
in	O
the	O
se	O
case	O
.	O
a	O
more	O
formal	O
treatment	O
for	O
the	O
stationary	B
case	O
can	O
be	O
obtained	O
by	O
examining	O
the	O
eigenvalue-frequency	O
plot	O
of	O
the	O
covariance	B
function	I
(	O
spectral	O
density	B
)	O
,	O
section	O
(	O
19.4.3	O
)	O
.	O
for	O
rough	O
functions	O
the	O
density	B
of	O
eigenvalues	O
for	O
high	O
frequency	O
components	O
is	O
higher	O
than	O
for	O
smooth	O
functions	O
.	O
19.4.2	O
mercer	O
kernels	O
consider	O
the	O
function	B
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
φ	O
(	O
x	O
)	O
tφ	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
b	O
(	O
cid:88	O
)	O
s=1	O
(	O
cid:48	O
)	O
)	O
φs	O
(	O
x	O
)	O
φs	O
(	O
x	O
(	O
19.4.4	O
)	O
where	O
φ	O
(	O
x	O
)	O
is	O
a	O
vector	O
with	O
component	O
functions	O
φ1	O
(	O
x	O
)	O
,	O
φ2	O
(	O
x	O
)	O
,	O
.	O
.	O
.	O
,	O
φb	O
(	O
x	O
)	O
.	O
then	O
for	O
a	O
set	O
of	O
points	O
x1	O
,	O
.	O
.	O
.	O
,	O
xp	O
,	O
we	O
construct	O
the	O
matrix	B
k	O
with	O
elements	O
[	O
k	O
]	O
ij	O
=	O
k	O
(	O
xi	O
,	O
xj	O
)	O
=	O
s=1	O
356	O
φs	O
(	O
xi	O
)	O
φs	O
(	O
xj	O
)	O
(	O
19.4.5	O
)	O
draft	O
march	O
9	O
,	O
2010	O
b	O
(	O
cid:88	O
)	O
−20−15−10−505101520−2−1.5−1−0.500.511.5−20−15−10−505101520−3−2−101234	O
analysis	B
of	O
covariance	B
functions	O
we	O
claim	O
that	O
the	O
matrix	B
k	O
so	O
constructed	O
is	O
positive	O
semideﬁnite	O
and	O
hence	O
a	O
valid	O
covariance	B
matrix	O
.	O
recalling	O
that	O
a	O
matrix	B
is	O
positive	O
semideﬁnite	O
if	O
for	O
any	O
non	O
zero	O
vector	O
z	O
,	O
ztkz	O
≥	O
0.	O
using	O
the	O
deﬁnition	O
of	O
k	O
above	O
we	O
have	O
p	O
(	O
cid:88	O
)	O
ztkz	O
=	O
zikijzj	O
=	O
i	O
,	O
j=1	O
s=1	O
b	O
(	O
cid:88	O
)	O
(	O
cid:34	O
)	O
p	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
i=1	O
(	O
cid:35	O
)	O
(	O
cid:125	O
)	O
	O
p	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
j=1	O
ziφs	O
(	O
xi	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
γs	O
	O
(	O
cid:125	O
)	O
b	O
(	O
cid:88	O
)	O
s=1	O
φs	O
(	O
xj	O
)	O
zj	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
γs	O
=	O
γ2	O
s	O
≥	O
0	O
(	O
19.4.6	O
)	O
hence	O
any	O
function	B
of	O
the	O
form	O
equation	B
(	O
19.4.4	O
)	O
is	O
a	O
covariance	B
function	I
.	O
we	O
can	O
generalise	O
the	O
mercer	O
kernel	B
to	O
complex	O
functions	O
φ	O
(	O
x	O
)	O
using	O
(	O
cid:48	O
)	O
)	O
=	O
φ	O
(	O
x	O
)	O
tφ†	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
k	O
(	O
x	O
,	O
x	O
(	O
19.4.7	O
)	O
where	O
†	O
represents	O
the	O
complex	O
conjugate	B
.	O
then	O
the	O
matrix	B
k	O
formed	O
from	O
inputs	O
xi	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
p	O
is	O
positive	O
semideﬁnite	O
since	O
for	O
any	O
real	O
vector	O
z	O
,	O
ztkz	O
=	O
b	O
(	O
cid:88	O
)	O
s=1	O
(	O
cid:34	O
)	O
p	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
i=1	O
(	O
cid:35	O
)	O
(	O
cid:125	O
)	O
	O
p	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
j=1	O
ziφs	O
(	O
xi	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
γs	O
	O
(	O
cid:125	O
)	O
b	O
(	O
cid:88	O
)	O
s=1	O
†	O
s	O
(	O
xj	O
)	O
zj	O
φ	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
†	O
γ	O
s	O
=	O
|γs|2	O
≥	O
0	O
(	O
19.4.8	O
)	O
where	O
we	O
made	O
use	O
of	O
the	O
general	O
result	O
for	O
a	O
complex	O
variable	B
xx†	O
=	O
|x|2	O
.	O
a	O
further	O
generalisation	B
is	O
to	O
write	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
f	O
(	O
s	O
)	O
φ	O
(	O
x	O
,	O
s	O
)	O
φ	O
(	O
cid:48	O
)	O
†	O
(	O
x	O
,	O
s	O
)	O
ds	O
(	O
19.4.9	O
)	O
for	O
f	O
(	O
s	O
)	O
≥	O
0	O
,	O
and	O
scalar	O
complex	O
functions	O
φ	O
(	O
x	O
,	O
s	O
)	O
.	O
then	O
replacing	O
summations	O
with	O
integration	O
(	O
and	O
assuming	O
we	O
can	O
interchange	O
the	O
sum	O
over	O
the	O
components	O
of	O
z	O
with	O
the	O
integral	O
over	O
s	O
)	O
,	O
we	O
obtain	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
ztkz	O
=	O
f	O
(	O
s	O
)	O
ziφ	O
(	O
xi	O
,	O
s	O
)	O
ds	O
=	O
f	O
(	O
s	O
)	O
|γ	O
(	O
s	O
)	O
|2ds	O
≥	O
0	O
(	O
19.4.10	O
)	O
(	O
cid:34	O
)	O
p	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
i=1	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
γ	O
(	O
s	O
)	O
(	O
cid:35	O
)	O
(	O
cid:125	O
)	O
	O
p	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
j=1	O
	O
(	O
cid:125	O
)	O
†	O
(	O
xj	O
,	O
s	O
)	O
zj	O
φ	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
γ†	O
(	O
s	O
)	O
(	O
cid:90	O
)	O
spectral	O
decomposition	B
equation	O
(	O
19.4.9	O
)	O
is	O
a	O
generalisation	B
of	O
the	O
spectral	O
decomposition	B
of	O
a	O
kernel	B
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
since	O
if	O
we	O
write	O
f	O
(	O
s	O
)	O
as	O
a	O
sum	O
of	O
dirac	O
delta	O
functions	O
,	O
(	O
19.4.11	O
)	O
∞	O
(	O
cid:88	O
)	O
k=1	O
f	O
(	O
s	O
)	O
=	O
λkδ	O
(	O
s	O
−	O
k	O
)	O
∞	O
(	O
cid:88	O
)	O
and	O
using	O
φ	O
(	O
x	O
,	O
k	O
)	O
=	O
ψk	O
(	O
x	O
)	O
,	O
for	O
an	O
eigenfunction	O
ψk	O
(	O
x	O
)	O
indexed	O
by	O
k	O
with	O
eigenvalue	O
λk	O
,	O
we	O
obtain	O
the	O
spectral	O
decomposition	B
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
λkψk	O
(	O
x	O
)	O
ψk	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
(	O
19.4.12	O
)	O
k=1	O
if	O
all	O
the	O
eigenvalues	O
of	O
a	O
kernel	B
are	O
non-negative	O
,	O
the	O
kernel	B
is	O
a	O
covariance	B
function	I
.	O
consider	O
for	O
example	O
the	O
following	O
function	B
(	O
cid:48	O
)	O
)	O
=	O
e	O
−	O
(	O
x−x	O
(	O
cid:48	O
)	O
)	O
2	O
k	O
(	O
x	O
,	O
x	O
(	O
19.4.13	O
)	O
we	O
claim	O
that	O
this	O
is	O
a	O
covariance	B
function	I
.	O
this	O
is	O
indeed	O
a	O
valid	O
covariance	B
function	I
in	O
the	O
sense	O
that	O
for	O
any	O
set	O
of	O
points	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
,	O
the	O
d	O
×	O
d	O
matrix	B
formed	O
with	O
elements	O
k	O
(	O
xd	O
,	O
xd	O
(	O
cid:48	O
)	O
)	O
is	O
positive	B
deﬁnite	I
,	O
as	O
discussed	O
after	O
deﬁnition	O
(	O
98	O
)	O
.	O
the	O
solution	O
given	O
to	O
exercise	O
(	O
193	O
)	O
shows	O
that	O
there	O
do	O
indeed	O
exist	O
real-valued	O
vectors	O
such	O
that	O
one	O
can	O
represent	O
(	O
cid:48	O
)	O
)	O
=	O
φ	O
(	O
x	O
)	O
tφ	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
k	O
(	O
x	O
,	O
x	O
(	O
19.4.14	O
)	O
where	O
the	O
vectors	O
are	O
inﬁnite	O
dimensional	O
.	O
this	O
demonstrates	O
the	O
generalisation	B
of	O
the	O
ﬁnite-dimensional	O
‘	O
weight	O
space	O
’	O
viewpoint	O
of	O
a	O
gp	O
to	O
the	O
potentially	O
implicit	O
inﬁnite	O
dimensional	O
representation	B
.	O
draft	O
march	O
9	O
,	O
2010	O
357	O
gaussian	O
processes	O
for	O
classiﬁcation	B
c1	O
cn	O
y1	O
x1	O
yn	O
xn	O
c∗	O
y∗	O
x∗	O
figure	O
19.5	O
:	O
gp	O
classiﬁcation	B
.	O
the	O
gp	O
induces	O
a	O
gaussian	O
distribution	B
on	O
the	O
latent	B
activations	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
,	O
y∗	O
,	O
given	O
the	O
observed	O
values	O
of	O
c1	O
,	O
.	O
.	O
.	O
,	O
cn	O
.	O
the	O
classiﬁcation	B
of	O
the	O
new	O
input	O
x∗	O
is	O
then	O
given	O
via	O
the	O
correlation	B
induced	O
by	O
the	O
training	B
points	O
on	O
the	O
latent	B
activation	O
y∗	O
.	O
19.4.3	O
fourier	O
analysis	B
for	O
stationary	B
kernels	O
(	O
cid:90	O
)	O
g	O
(	O
x	O
)	O
=	O
1	O
2π	O
for	O
a	O
function	B
g	O
(	O
x	O
)	O
with	O
fourier	O
transform	O
˜g	O
(	O
s	O
)	O
,	O
we	O
may	O
use	O
the	O
inverse	O
fourier	O
transform	O
to	O
write	O
˜g	O
(	O
s	O
)	O
e	O
−ixsds	O
(	O
19.4.15	O
)	O
where	O
i	O
≡	O
√−1	O
.	O
for	O
a	O
stationary	B
kernel	O
k	O
(	O
x	O
)	O
with	O
fourier	O
transform	O
˜k	O
(	O
s	O
)	O
,	O
we	O
can	O
therefore	O
write	O
k	O
(	O
x	O
−	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
1	O
2π	O
˜k	O
(	O
s	O
)	O
e	O
−i	O
(	O
x−x	O
(	O
cid:48	O
)	O
)	O
sds	O
=	O
˜k	O
(	O
s	O
)	O
e	O
−ixseix	O
(	O
cid:48	O
)	O
sds	O
(	O
19.4.16	O
)	O
which	O
is	O
of	O
the	O
same	O
form	O
as	O
equation	O
(	O
19.4.9	O
)	O
where	O
the	O
fourier	O
transform	O
˜k	O
(	O
s	O
)	O
is	O
identiﬁed	O
with	O
f	O
(	O
s	O
)	O
and	O
φ	O
(	O
x	O
,	O
s	O
)	O
=	O
e−isx	O
.	O
hence	O
,	O
provided	O
the	O
fourier	O
transform	O
˜k	O
(	O
s	O
)	O
is	O
positive	O
,	O
the	O
translation	O
invariant	O
kernel	B
k	O
(	O
x−	O
x	O
(	O
cid:48	O
)	O
)	O
is	O
a	O
covariance	B
function	I
.	O
bochner	O
’	O
s	O
theorem	B
[	O
230	O
]	O
asserts	O
the	O
converse	O
that	O
any	O
translation	O
invariant	O
covariance	B
function	I
must	O
have	O
such	O
a	O
fourier	O
representation	B
.	O
(	O
cid:90	O
)	O
1	O
2π	O
(	O
cid:90	O
)	O
∞	O
−∞	O
(	O
cid:90	O
)	O
application	O
to	O
the	O
squared	B
exponential	I
kernel	O
for	O
the	O
translation	O
invariant	O
squared	B
exponential	I
kernel	O
,	O
k	O
(	O
x	O
)	O
=	O
e	O
−	O
s2	O
−	O
s2	O
−	O
1	O
−	O
1	O
2	O
(	O
x+is	O
)	O
2	O
dx	O
=	O
√2πe	O
2	O
e	O
2	O
x2+isxdx	O
=	O
e	O
2	O
˜k	O
(	O
s	O
)	O
=	O
(	O
cid:90	O
)	O
∞	O
e	O
−∞	O
−	O
1	O
2	O
x2	O
,	O
its	O
fourier	O
transform	O
is	O
(	O
19.4.17	O
)	O
hence	O
the	O
fourier	O
transform	O
of	O
the	O
se	O
kernel	B
is	O
a	O
gaussian	O
.	O
since	O
this	O
is	O
positive	O
the	O
se	O
kernel	B
is	O
a	O
covariance	B
function	I
.	O
19.5	O
gaussian	O
processes	O
for	O
classiﬁcation	B
adapting	O
the	O
gp	O
framework	O
to	O
classiﬁcation	B
requires	O
replacing	O
the	O
gaussian	O
regression	B
term	O
p	O
(	O
y|x	O
)	O
with	O
a	O
corresponding	O
classiﬁcation	B
term	O
p	O
(	O
c|x	O
)	O
for	O
a	O
discrete	B
label	O
c.	O
to	O
do	O
so	O
we	O
will	O
use	O
the	O
gp	O
to	O
deﬁne	O
a	O
latent	B
continuous	O
space	O
which	O
will	O
then	O
be	O
mapped	O
to	O
a	O
class	O
probability	B
using	O
p	O
(	O
c|x	O
)	O
=	O
given	O
training	B
data	O
inputs	O
x	O
=	O
(	O
cid:8	O
)	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
(	O
cid:9	O
)	O
,	O
corresponding	O
class	O
labels	O
c	O
=	O
(	O
cid:8	O
)	O
c1	O
,	O
.	O
.	O
.	O
,	O
cn	O
(	O
cid:9	O
)	O
,	O
and	O
a	O
novel	O
p	O
(	O
c|y	O
,	O
x	O
)	O
p	O
(	O
y|x	O
)	O
dy	O
=	O
p	O
(	O
c|y	O
)	O
p	O
(	O
y|x	O
)	O
dy	O
(	O
19.5.1	O
)	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
input	O
x∗	O
,	O
then	O
∗	O
|x	O
∗	O
p	O
(	O
c	O
where	O
∗	O
p	O
(	O
y	O
∗	O
p	O
(	O
c	O
∗	O
)	O
p	O
(	O
y	O
∗	O
∗	O
=	O
|y	O
,	O
c|x	O
)	O
∗	O
p	O
(	O
y	O
,	O
c	O
,	O
x	O
)	O
=	O
(	O
cid:90	O
)	O
|x	O
,	O
c	O
)	O
∝	O
p	O
(	O
y	O
(	O
cid:90	O
)	O
∗	O
)	O
dy	O
(	O
cid:41	O
)	O
(	O
cid:90	O
)	O
(	O
cid:40	O
)	O
n	O
(	O
cid:89	O
)	O
p	O
(	O
c|y	O
)	O
p	O
(	O
y	O
,	O
y|x	O
,	O
x	O
(	O
cid:124	O
)	O
(	O
cid:125	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
p	O
(	O
cn|yn	O
)	O
,	O
y	O
,	O
c|x	O
,	O
x	O
(	O
cid:124	O
)	O
n=1	O
=	O
=	O
∗	O
class	O
mapping	O
∗	O
(	O
19.5.2	O
)	O
|x	O
,	O
c	O
)	O
dy	O
∗	O
)	O
dy	O
p	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
,	O
y	O
∗	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
∗	O
)	O
|x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
x	O
(	O
cid:125	O
)	O
gaussian	O
process	O
dy1	O
,	O
.	O
.	O
.	O
,	O
dyn	O
(	O
19.5.3	O
)	O
358	O
draft	O
march	O
9	O
,	O
2010	O
gaussian	O
processes	O
for	O
classiﬁcation	B
the	O
graphical	O
structure	B
of	O
this	O
model	B
is	O
depicted	O
in	O
ﬁg	O
(	O
19.5	O
.	O
)	O
the	O
posterior	B
latent	O
y∗	O
is	O
then	O
formed	O
from	O
the	O
standard	O
regression	O
term	O
from	O
the	O
gaussian	O
process	O
,	O
multiplied	O
by	O
a	O
set	O
of	O
non-gaussian	O
maps	O
from	O
the	O
latent	B
activations	O
to	O
the	O
class	O
probabilities	O
.	O
we	O
can	O
reformulate	O
the	O
prediction	B
problem	O
more	O
conveniently	O
as	O
follows	O
:	O
∗	O
p	O
(	O
y	O
∗	O
,	O
y|x	O
,	O
x	O
,	O
c	O
)	O
∝	O
p	O
(	O
y	O
,	O
y	O
,	O
c|x	O
,	O
x	O
)	O
∝	O
p	O
(	O
y	O
∗	O
∗	O
∗	O
∗	O
|y	O
,	O
x	O
,	O
x	O
)	O
p	O
(	O
y|c	O
,	O
x	O
)	O
where	O
(	O
cid:40	O
)	O
n	O
(	O
cid:89	O
)	O
p	O
(	O
y|c	O
,	O
x	O
)	O
∝	O
(	O
cid:41	O
)	O
p	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn|x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
p	O
(	O
cn|yn	O
)	O
in	O
equation	B
(	O
19.5.4	O
)	O
the	O
term	O
p	O
(	O
y∗	O
|y	O
,	O
x∗	O
,	O
x	O
)	O
does	O
not	O
contain	O
any	O
class	O
label	O
information	O
and	O
is	O
simply	O
a	O
conditional	B
gaussian	O
.	O
the	O
advantage	O
of	O
the	O
above	O
description	O
is	O
that	O
we	O
can	O
therefore	O
form	O
an	O
approx-	O
imation	O
to	O
p	O
(	O
y|c	O
,	O
x	O
)	O
and	O
then	O
reuse	O
this	O
approximation	B
in	O
the	O
prediction	B
for	O
many	O
diﬀerent	O
x∗	O
without	O
(	O
19.5.5	O
)	O
n=1	O
needing	O
to	O
rerun	O
the	O
approximation	B
[	O
297	O
,	O
230	O
]	O
.	O
(	O
19.5.4	O
)	O
19.5.1	O
binary	O
classiﬁcation	O
for	O
the	O
binary	O
class	O
case	O
we	O
will	O
use	O
the	O
convention	O
that	O
c	O
∈	O
{	O
1	O
,	O
0	O
}	O
.	O
we	O
therefore	O
need	O
to	O
specify	O
p	O
(	O
c	O
=	O
1|y	O
)	O
for	O
a	O
real	O
valued	O
activation	O
y.	O
a	O
convenient	O
choice	O
is	O
the	O
logistic	B
transfer	O
function3	O
σ	O
(	O
x	O
)	O
=	O
1	O
1	O
+	O
e−x	O
then	O
p	O
(	O
c|y	O
)	O
=	O
σ	O
(	O
(	O
2c	O
−	O
1	O
)	O
y	O
)	O
(	O
19.5.6	O
)	O
(	O
19.5.7	O
)	O
is	O
a	O
valid	O
distribution	B
since	O
σ	O
(	O
−x	O
)	O
=	O
1	O
−	O
σ	O
(	O
x	O
)	O
,	O
ensuring	O
that	O
the	O
sum	O
over	O
the	O
class	O
states	O
is	O
1.	O
a	O
diﬃculty	O
is	O
that	O
the	O
non-linear	B
class	O
mapping	O
term	O
makes	O
the	O
computation	O
of	O
the	O
posterior	B
distribution	O
equation	B
(	O
19.5.3	O
)	O
diﬃcult	O
since	O
the	O
integrals	O
over	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
can	O
not	O
be	O
carried	O
out	O
analytically	O
.	O
there	O
are	O
many	O
approximate	B
techniques	O
one	O
could	O
apply	O
in	O
this	O
case	O
,	O
including	O
variational	O
methods	O
analogous	O
to	O
that	O
described	O
in	O
section	O
(	O
?	O
?	O
)	O
.	O
below	O
we	O
describe	O
the	O
straightforward	O
laplace	O
method	O
,	O
leaving	O
the	O
more	O
sophisticated	O
methods	O
for	O
further	O
reading	O
[	O
230	O
]	O
.	O
19.5.2	O
laplace	O
’	O
s	O
approximation	B
in	O
the	O
laplace	O
method	O
we	O
approximate	B
the	O
non-gaussian	O
distribution	B
(	O
19.5.5	O
)	O
by	O
a	O
gaussian4	O
q	O
(	O
y|c	O
,	O
x	O
)	O
,	O
(	O
19.5.8	O
)	O
p	O
(	O
y|c	O
,	O
x	O
)	O
≈	O
q	O
(	O
y|c	O
,	O
x	O
)	O
predictions	O
can	O
be	O
formed	O
from	O
the	O
joint	B
gaussian	O
for	O
compactness	O
we	O
deﬁne	O
the	O
class	O
label	O
vector	O
,	O
and	O
outputs	O
∗	O
∗	O
|y	O
,	O
x	O
,	O
x	O
)	O
q	O
(	O
y|c	O
,	O
x	O
)	O
y	O
=	O
(	O
cid:0	O
)	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
(	O
cid:1	O
)	O
t	O
∗	O
p	O
(	O
y	O
∗	O
,	O
y|x	O
,	O
x	O
,	O
c	O
)	O
≈	O
p	O
(	O
y	O
c	O
=	O
(	O
cid:0	O
)	O
c1	O
,	O
.	O
.	O
.	O
,	O
cn	O
(	O
cid:1	O
)	O
t	O
σ	O
=	O
(	O
cid:0	O
)	O
σ	O
(	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
σ	O
(	O
yn	O
)	O
(	O
cid:1	O
)	O
t	O
,	O
and	O
notationally	O
drop	O
the	O
(	O
ever	O
present	O
)	O
conditioning	B
on	O
the	O
inputs	O
x.	O
also	O
for	O
convenience	O
,	O
we	O
deﬁne	O
(	O
19.5.9	O
)	O
(	O
19.5.10	O
)	O
(	O
19.5.11	O
)	O
3we	O
will	O
also	O
refer	O
to	O
this	O
as	O
‘	O
the	O
sigmoid	B
function	I
’	O
.	O
more	O
strictly	O
a	O
sigmoid	B
function	I
refers	O
to	O
any	O
‘	O
s-shaped	O
’	O
function	B
(	O
from	O
the	O
greek	O
for	O
‘	O
s	O
’	O
)	O
.	O
4some	O
authors	O
use	O
the	O
term	O
laplace	O
approximation	B
solely	O
for	O
approximating	O
an	O
integral	O
.	O
here	O
we	O
use	O
the	O
term	O
to	O
refer	O
to	O
a	O
gaussian	O
approximation	B
of	O
a	O
non-gaussian	O
distribution	B
.	O
draft	O
march	O
9	O
,	O
2010	O
359	O
finding	O
the	O
mode	B
the	O
laplace	O
approximation	B
,	O
section	O
(	O
28.2	O
)	O
,	O
corresponds	O
to	O
a	O
second	O
order	O
expansion	O
around	O
the	O
mode	B
of	O
the	O
distribution	B
.	O
our	O
task	O
is	O
therefore	O
to	O
ﬁnd	O
the	O
maximum	O
of	O
gaussian	O
processes	O
for	O
classiﬁcation	B
p	O
(	O
y|c	O
)	O
∝	O
p	O
(	O
y	O
,	O
c	O
)	O
=	O
eψ	O
(	O
y	O
)	O
where	O
ψ	O
(	O
y	O
)	O
=	O
cty	O
−	O
n	O
(	O
cid:88	O
)	O
n=1	O
log	O
(	O
1	O
+	O
eyn	O
)	O
−	O
1	O
2	O
ytk−1	O
x	O
,	O
xy	O
−	O
1	O
2	O
log	O
det	O
(	O
kx	O
,	O
x	O
)	O
−	O
n	O
2	O
log	O
2π	O
the	O
maximum	O
needs	O
to	O
be	O
found	O
numerically	O
,	O
and	O
it	O
is	O
convenient	O
to	O
use	O
the	O
newton	O
method	O
[	O
120	O
,	O
297	O
,	O
230	O
]	O
.	O
ynew	O
=	O
y	O
−	O
(	O
∇∇ψ	O
)	O
−1	O
∇ψ	O
diﬀerentiating	O
equation	B
19.5.13	O
with	O
respect	O
to	O
y	O
we	O
obtain	O
the	O
gradient	B
and	O
hessian	O
∇ψ	O
=	O
(	O
c	O
−	O
σ	O
)	O
−	O
k−1	O
x	O
,	O
xy	O
∇∇ψ	O
=	O
−k−1	O
x	O
,	O
x	O
−	O
d	O
where	O
the	O
‘	O
noise	O
’	O
matrix	B
is	O
given	O
by	O
(	O
19.5.12	O
)	O
(	O
19.5.13	O
)	O
(	O
19.5.14	O
)	O
(	O
19.5.15	O
)	O
(	O
19.5.16	O
)	O
(	O
19.5.17	O
)	O
(	O
19.5.21	O
)	O
(	O
19.5.24	O
)	O
(	O
19.5.25	O
)	O
(	O
19.5.26	O
)	O
d	O
=	O
diag	O
(	O
σ1	O
(	O
1	O
−	O
σ1	O
)	O
,	O
.	O
.	O
.	O
,	O
σn	O
(	O
1	O
−	O
σn	O
)	O
)	O
using	O
these	O
expressions	O
in	O
the	O
newton	O
update	O
,	O
(	O
19.5.14	O
)	O
gives	O
to	O
avoid	O
unnecessary	O
inversions	O
,	O
one	O
may	O
rewrite	O
this	O
in	O
the	O
form	O
ynew	O
=	O
y	O
+	O
(	O
cid:0	O
)	O
k−1	O
x	O
,	O
x	O
+	O
d	O
(	O
cid:1	O
)	O
−1	O
(	O
cid:0	O
)	O
c	O
−	O
σ	O
−	O
k−1	O
x	O
,	O
xy	O
(	O
cid:1	O
)	O
ynew	O
=	O
kx	O
,	O
x	O
(	O
i	O
+	O
dkx	O
,	O
x	O
)	O
−1	O
(	O
dy	O
+	O
c	O
−	O
σ	O
)	O
x	O
,	O
x	O
+	O
d	O
(	O
cid:1	O
)	O
−1	O
(	O
cid:17	O
)	O
|y	O
)	O
and	O
q	O
(	O
y|x	O
,	O
x∗	O
,	O
c	O
)	O
in	O
equation	B
(	O
19.5.9	O
)	O
.	O
predictions	O
are	O
then	O
made	O
y	O
˜y	O
,	O
(	O
cid:0	O
)	O
k−1	O
∗	O
q	O
(	O
y|x	O
,	O
x	O
(	O
19.5.19	O
)	O
(	O
19.5.18	O
)	O
(	O
19.5.20	O
)	O
(	O
cid:16	O
)	O
given	O
a	O
converged	O
solution	O
˜y	O
we	O
have	O
found	O
a	O
gaussian	O
approximation	B
we	O
now	O
have	O
gaussians	O
for	O
p	O
(	O
y∗	O
using	O
making	O
predictions	O
,	O
c	O
)	O
=	O
n	O
(	O
cid:90	O
)	O
p	O
(	O
y	O
∗	O
∗	O
|x	O
,	O
x	O
,	O
c	O
)	O
≈	O
p	O
(	O
y	O
∗	O
∗	O
|x	O
∗	O
,	O
x	O
,	O
y	O
)	O
q	O
(	O
y|x	O
,	O
x	O
,	O
c	O
)	O
dy	O
(	O
cid:0	O
)	O
y	O
where	O
,	O
by	O
conditioning	B
,	O
section	O
(	O
8.6.1	O
)	O
,	O
∗	O
kx∗	O
,	O
xk−1	O
we	O
can	O
also	O
write	O
this	O
as	O
a	O
linear	B
system	O
,	O
x	O
)	O
=	O
n	O
∗	O
|y	O
,	O
x	O
p	O
(	O
y	O
∗	O
y	O
x	O
,	O
xy	O
+	O
η	O
∗	O
=	O
kx∗	O
,	O
xk−1	O
(	O
cid:0	O
)	O
η	O
0	O
,	O
kx∗	O
,	O
x∗	O
−	O
kx∗	O
,	O
xk−1	O
,	O
x	O
,	O
c	O
(	O
cid:105	O
)	O
≈	O
kx∗	O
,	O
xk−1	O
where	O
η	O
∼	O
n	O
aging	O
over	O
y	O
and	O
the	O
noise	O
η	O
,	O
we	O
obtain	O
∗	O
(	O
cid:104	O
)	O
y	O
|x	O
∗	O
x	O
,	O
xkx	O
,	O
x∗	O
(	O
cid:1	O
)	O
x	O
,	O
xy	O
,	O
kx∗	O
,	O
x∗	O
−	O
kx∗	O
,	O
xk−1	O
x	O
,	O
xkx	O
,	O
x∗	O
(	O
cid:1	O
)	O
.	O
using	O
equation	B
(	O
19.5.23	O
)	O
and	O
equation	B
(	O
19.5.20	O
)	O
and	O
aver-	O
(	O
19.5.22	O
)	O
(	O
19.5.23	O
)	O
similarly	O
,	O
the	O
variance	B
of	O
the	O
latent	B
prediction	O
is	O
x	O
,	O
x˜y	O
=	O
kx∗	O
,	O
x	O
(	O
c	O
−	O
σ	O
(	O
˜y	O
)	O
)	O
(	O
cid:0	O
)	O
k−1	O
x	O
,	O
x	O
+	O
d	O
(	O
cid:1	O
)	O
−1	O
k−1	O
(	O
cid:0	O
)	O
kx	O
,	O
x	O
+	O
d−1	O
(	O
cid:1	O
)	O
−1	O
kx	O
,	O
x∗	O
,	O
x	O
,	O
c	O
)	O
≈	O
kx∗	O
,	O
xk−1	O
x	O
,	O
x	O
=	O
kx∗	O
,	O
x∗	O
−	O
kx∗	O
,	O
x	O
var	O
(	O
y	O
∗	O
∗	O
|x	O
360	O
x	O
,	O
xkx	O
,	O
x∗	O
+	O
kx∗	O
,	O
x∗	O
−	O
kx∗	O
,	O
xk−1	O
x	O
,	O
xkx	O
,	O
x∗	O
draft	O
march	O
9	O
,	O
2010	O
gaussian	O
processes	O
for	O
classiﬁcation	B
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
19.6	O
:	O
gaussian	O
process	O
classiﬁcation	B
.	O
the	O
x-axis	O
are	O
the	O
inputs	O
,	O
and	O
the	O
class	O
is	O
the	O
y-axis	O
.	O
green	O
points	O
are	O
training	B
points	O
from	O
class	O
1	O
and	O
red	O
from	O
class	O
0.	O
the	O
dots	O
are	O
the	O
predictions	O
p	O
(	O
c	O
=	O
1|x∗	O
)	O
for	O
a	O
rand	O
of	O
points	O
x∗	O
.	O
(	O
b	O
)	O
:	O
ou	O
covariance	B
(	O
γ	O
=	O
1	O
)	O
.	O
see	O
demogpclass1d.m	O
.	O
(	O
a	O
)	O
:	O
square	O
exponential	B
covariance	O
(	O
γ	O
=	O
2	O
)	O
.	O
where	O
the	O
last	O
line	O
is	O
obtained	O
using	O
the	O
matrix	B
inversion	O
lemma	O
,	O
deﬁnition	O
(	O
132	O
)	O
.	O
the	O
class	O
prediction	B
for	O
a	O
new	O
input	O
x∗	O
is	O
then	O
given	O
by	O
∗	O
∗	O
=	O
1|x	O
p	O
(	O
c	O
(	O
19.5.27	O
)	O
in	O
order	O
to	O
calculate	O
the	O
gaussian	O
integral	O
over	O
the	O
logistic	B
sigmoid	I
function	O
,	O
we	O
use	O
an	O
approximation	B
of	O
the	O
sigmoid	B
function	I
based	O
on	O
the	O
error	B
function	I
erf	O
(	O
x	O
)	O
,	O
see	O
section	O
(	O
18.2.3	O
)	O
and	O
avsigmagauss.m	O
.	O
,	O
x	O
,	O
c	O
)	O
≈	O
(	O
cid:104	O
)	O
σ	O
(	O
y	O
∗	O
)	O
(	O
cid:105	O
)	O
n	O
(	O
y∗	O
(	O
cid:104	O
)	O
y∗	O
(	O
cid:105	O
)	O
,	O
var	O
(	O
y∗	O
)	O
)	O
example	O
85.	O
an	O
example	O
of	O
binary	O
classiﬁcation	O
is	O
given	O
in	O
ﬁg	O
(	O
19.6	O
)	O
in	O
which	O
one-dimensional	O
input	O
training	B
data	O
with	O
binary	O
class	O
labels	O
is	O
plotted	O
along	O
with	O
the	O
class	O
probability	B
predictions	O
on	O
a	O
range	O
of	O
input	O
points	O
.	O
in	O
both	O
cases	O
the	O
covariance	B
function	I
is	O
of	O
the	O
form	O
2e|xi−xj|γ	O
+	O
0.001δij	O
.	O
the	O
square	O
expo-	O
nential	O
covariance	B
produces	O
a	O
smoother	O
class	O
prediction	B
than	O
the	O
ornstein-uhlenbeck	O
covariance	B
function	I
.	O
see	O
demogpclass1d.m	O
and	O
demogpclass.m	O
.	O
marginal	B
likelihood	I
the	O
marginal	B
likelihood	I
is	O
given	O
by	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
under	O
the	O
laplace	O
approximation	B
,	O
the	O
marginal	B
likelihood	I
is	O
approximated	O
by	O
p	O
(	O
c|x	O
)	O
=	O
p	O
(	O
c|y	O
)	O
p	O
(	O
y|x	O
)	O
y	O
p	O
(	O
c|x	O
)	O
≈	O
y	O
eψ	O
(	O
˜y	O
)	O
e	O
−	O
1	O
2	O
(	O
y−˜y	O
)	O
ta	O
(	O
y−˜y	O
)	O
where	O
a	O
=	O
−∇∇ψ	O
.	O
integrating	O
over	O
y	O
gives	O
log	O
p	O
(	O
c|x	O
)	O
≈	O
log	O
q	O
(	O
c|x	O
)	O
where	O
log	O
det	O
(	O
2πa	O
)	O
log	O
det	O
(	O
cid:0	O
)	O
k−1	O
log	O
q	O
(	O
c|x	O
)	O
=	O
ψ	O
(	O
˜y	O
)	O
−	O
n	O
(	O
cid:88	O
)	O
=	O
ψ	O
(	O
˜y	O
)	O
−	O
=	O
ct˜y	O
−	O
log	O
(	O
1	O
+	O
e˜yn	O
)	O
−	O
1	O
2	O
1	O
2	O
x	O
,	O
x	O
+	O
d	O
(	O
cid:1	O
)	O
+	O
n	O
n=1	O
log	O
2π	O
2	O
˜ytk−1	O
x	O
,	O
x˜y	O
−	O
1	O
2	O
1	O
2	O
log	O
det	O
(	O
i	O
+	O
kx	O
,	O
xd	O
)	O
(	O
19.5.28	O
)	O
(	O
19.5.29	O
)	O
(	O
19.5.30	O
)	O
(	O
19.5.31	O
)	O
(	O
19.5.32	O
)	O
(	O
19.5.33	O
)	O
where	O
˜y	O
is	O
the	O
converged	O
iterate	O
of	O
equation	B
19.5.18.	O
one	O
can	O
also	O
simplify	O
the	O
above	O
using	O
that	O
at	O
convergence	O
k−1	O
x	O
,	O
x˜y	O
=	O
c	O
−	O
σ	O
(	O
y	O
)	O
.	O
draft	O
march	O
9	O
,	O
2010	O
361	O
−10−8−6−4−2024681000.20.40.60.81−10−8−6−4−2024681000.20.40.60.81	O
19.5.3	O
hyperparameter	B
optimisation	O
code	O
the	O
approximate	B
marginal	O
likelihood	B
can	O
be	O
used	O
to	O
assess	O
hyperparameters	O
θ	O
of	O
the	O
kernel	B
.	O
a	O
little	O
care	O
is	O
required	O
in	O
computing	O
derivatives	O
of	O
the	O
approximate	B
marginal	O
likelihood	B
since	O
the	O
optimum	O
˜y	O
depends	O
on	O
θ.	O
we	O
use	O
the	O
total	O
derivative	O
formula	O
[	O
25	O
]	O
log	O
q	O
(	O
c|x	O
)	O
+	O
(	O
cid:88	O
)	O
(	O
cid:104	O
)	O
ytk−1	O
i	O
d	O
dθ	O
log	O
q	O
(	O
c|x	O
)	O
=	O
∂	O
∂θ	O
∂	O
∂θ	O
log	O
q	O
(	O
c|x	O
)	O
=	O
−	O
1	O
2	O
∂	O
∂θ	O
∂	O
∂	O
˜yi	O
log	O
q	O
(	O
c|x	O
)	O
d	O
dθ	O
˜yi	O
(	O
cid:105	O
)	O
x	O
,	O
xy	O
+	O
log	O
det	O
(	O
i	O
+	O
kx	O
,	O
xd	O
)	O
(	O
19.5.34	O
)	O
(	O
19.5.35	O
)	O
which	O
can	O
be	O
evaluated	O
using	O
the	O
standard	O
results	O
for	O
the	O
derivative	O
of	O
a	O
matrix	B
determinant	O
and	O
inverse	O
.	O
since	O
the	O
derivative	O
of	O
ψ	O
is	O
zero	O
at	O
˜y	O
,	O
and	O
noting	O
that	O
d	O
depends	O
explicitly	O
on	O
˜y	O
,	O
∂	O
∂	O
˜yi	O
log	O
q	O
(	O
c|x	O
)	O
=	O
−	O
1	O
2	O
∂	O
∂	O
˜yi	O
log	O
det	O
(	O
i	O
+	O
kx	O
,	O
xd	O
)	O
the	O
implicit	O
derivative	O
is	O
obtained	O
from	O
using	O
the	O
fact	O
that	O
at	O
convergence	O
˜y	O
=	O
kx	O
,	O
x	O
(	O
c	O
−	O
σ	O
(	O
y	O
)	O
)	O
to	O
give	O
d	O
dθ	O
˜y	O
=	O
(	O
i	O
+	O
kx	O
,	O
xd	O
)	O
−1	O
∂	O
∂θ	O
kx	O
,	O
x	O
(	O
c	O
−	O
σ	O
)	O
(	O
19.5.36	O
)	O
(	O
19.5.37	O
)	O
(	O
19.5.38	O
)	O
these	O
results	O
are	O
substituted	O
into	O
equation	B
(	O
19.5.34	O
)	O
to	O
ﬁnd	O
an	O
explicit	O
expression	O
for	O
the	O
derivative	O
.	O
19.5.4	O
multiple	B
classes	I
the	O
extension	O
of	O
the	O
preceding	O
framework	O
to	O
multiple	B
classes	I
is	O
essentially	O
straightforward	O
and	O
may	O
be	O
achieved	O
using	O
the	O
softmax	B
function	I
p	O
(	O
c	O
=	O
m|y	O
)	O
=	O
eym	O
σm	O
(	O
cid:48	O
)	O
eym	O
(	O
cid:48	O
)	O
which	O
automatically	O
enforces	O
the	O
constraint	O
(	O
cid:80	O
)	O
classes	O
,	O
the	O
cost	O
of	O
implementing	O
the	O
laplace	O
approximation	B
for	O
the	O
multiclass	O
case	O
scales	O
as	O
o	O
(	O
cid:0	O
)	O
c3n	O
3	O
(	O
cid:1	O
)	O
.	O
however	O
,	O
one	O
may	O
show	O
by	O
careful	O
implementation	O
that	O
the	O
cost	O
is	O
only	O
o	O
(	O
cid:0	O
)	O
cn	O
3	O
(	O
cid:1	O
)	O
,	O
and	O
we	O
refer	O
the	O
reader	O
m	O
p	O
(	O
c	O
=	O
m	O
)	O
=	O
1.	O
naively	O
it	O
would	O
appear	O
that	O
the	O
for	O
c	O
(	O
19.5.39	O
)	O
to	O
[	O
297	O
,	O
230	O
]	O
for	O
details	O
.	O
19.6	O
further	O
reading	O
gaussian	O
processes	O
have	O
been	O
heavily	O
developed	O
within	O
the	O
machine	O
learning	B
community	O
over	O
recent	O
years	O
for	O
which	O
eﬃcient	B
approximations	O
for	O
both	O
regression	B
and	O
classiﬁcation	B
remains	O
an	O
active	B
research	O
topic	O
.	O
we	O
direct	O
the	O
interested	O
reader	O
to	O
[	O
245	O
]	O
and	O
[	O
230	O
]	O
for	O
further	O
discussion	O
.	O
19.7	O
code	O
gpreg.m	O
:	O
gaussian	O
process	O
regression	B
demogpreg.m	O
:	O
demo	O
gp	O
regression	B
covfnge.m	O
:	O
gamma-exponential	O
covariance	B
function	I
gpclass.m	O
:	O
gaussian	O
process	O
classiﬁcation	B
demogpclass.m	O
:	O
demo	O
gaussian	O
process	O
classiﬁcation	B
362	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
19.8	O
exercises	O
exercise	O
191.	O
show	O
that	O
the	O
sample	B
covariance	O
matrix	B
with	O
elements	O
sij	O
=	O
(	O
cid:80	O
)	O
n	O
¯xi	O
=	O
(	O
cid:80	O
)	O
n	O
i	O
/n	O
,	O
is	O
positive	O
semideﬁnite	O
.	O
n=1	O
xn	O
n=1	O
xn	O
i	O
xn	O
j	O
/n	O
−	O
¯xi¯xj	O
,	O
where	O
exercise	O
192.	O
show	O
that	O
(	O
cid:48	O
)	O
)	O
=	O
e	O
−|	O
sin	O
(	O
x−x	O
(	O
cid:48	O
)	O
)	O
|	O
k	O
(	O
x	O
−	O
x	O
is	O
a	O
covariance	B
function	I
.	O
exercise	O
193.	O
consider	O
the	O
function	B
f	O
(	O
xi	O
,	O
xj	O
)	O
=	O
e	O
−	O
1	O
2	O
(	O
xi−xj	O
)	O
2	O
for	O
one	O
dimensional	O
inputs	O
xi	O
.	O
show	O
that	O
f	O
(	O
xi	O
,	O
xj	O
)	O
=	O
e	O
−	O
1	O
2	O
x2	O
i	O
exixj	O
e	O
−	O
1	O
2	O
x2	O
j	O
(	O
19.8.3	O
)	O
−	O
1	O
2	O
(	O
xi−xj	O
)	O
2	O
is	O
a	O
kernel	B
and	O
ﬁnd	O
an	O
explicit	O
representation	B
by	O
taylor	O
expanding	O
the	O
central	O
term	O
,	O
show	O
that	O
e	O
for	O
the	O
kernel	B
f	O
(	O
xi	O
,	O
xj	O
)	O
as	O
the	O
scalar	B
product	I
of	O
two	O
inﬁnite	O
dimensional	O
vectors	O
.	O
exercise	O
194.	O
show	O
that	O
for	O
a	O
covariance	B
function	I
k1	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
then	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
f	O
(	O
cid:0	O
)	O
k1	O
(	O
cid:0	O
)	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
19.8.4	O
)	O
is	O
also	O
a	O
covariance	B
function	I
for	O
any	O
polynomial	O
f	O
(	O
x	O
)	O
with	O
positive	O
coeﬃcients	O
.	O
show	O
therefore	O
that	O
ek1	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
and	O
tan	O
(	O
k1	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
)	O
are	O
covariance	B
functions	O
.	O
exercise	O
195.	O
for	O
a	O
covariance	B
function	I
k1	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
f	O
(	O
(	O
cid:0	O
)	O
x	O
−	O
x	O
(	O
cid:48	O
)	O
(	O
cid:1	O
)	O
t	O
(	O
cid:0	O
)	O
x	O
−	O
x	O
(	O
cid:48	O
)	O
(	O
cid:1	O
)	O
)	O
k2	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
f	O
(	O
(	O
cid:0	O
)	O
x	O
−	O
x	O
(	O
cid:48	O
)	O
(	O
cid:1	O
)	O
t	O
a	O
(	O
cid:0	O
)	O
x	O
−	O
x	O
(	O
cid:48	O
)	O
(	O
cid:1	O
)	O
)	O
show	O
that	O
(	O
19.8.1	O
)	O
(	O
19.8.2	O
)	O
(	O
19.8.5	O
)	O
(	O
19.8.6	O
)	O
is	O
also	O
a	O
valid	O
covariance	B
function	I
for	O
a	O
positive	B
deﬁnite	I
symmetric	O
matrix	B
a.	O
exercise	O
196	O
(	O
string	O
kernel	B
)	O
.	O
let	O
x	O
and	O
x	O
(	O
cid:48	O
)	O
be	O
two	O
strings	O
of	O
characters	O
and	O
φs	O
(	O
x	O
)	O
be	O
the	O
number	O
of	O
times	O
that	O
substring	O
s	O
appears	O
in	O
string	O
x.	O
then	O
wsφs	O
(	O
x	O
)	O
φs	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
(	O
19.8.7	O
)	O
(	O
cid:48	O
)	O
)	O
=	O
(	O
cid:88	O
)	O
k	O
(	O
x	O
,	O
x	O
s	O
is	O
a	O
(	O
string	O
kernel	B
)	O
covariance	B
function	I
,	O
provided	O
the	O
weight	B
of	O
each	O
substring	O
ws	O
is	O
positive	O
.	O
1.	O
given	O
a	O
collection	O
of	O
strings	O
about	O
politics	O
and	O
another	O
collection	O
about	O
sport	O
,	O
explain	O
how	O
to	O
form	O
a	O
gp	O
classiﬁer	B
using	O
a	O
string	O
kernel	B
.	O
2.	O
explain	O
how	O
the	O
weights	O
ws	O
can	O
be	O
adjusted	O
to	O
improve	O
the	O
ﬁt	O
of	O
the	O
classiﬁer	B
to	O
the	O
data	B
and	O
give	O
an	O
explicit	O
formula	O
for	O
the	O
derivative	O
with	O
respect	O
to	O
ws	O
of	O
the	O
log	O
marginal	B
likelihood	I
under	O
the	O
laplace	O
approximation	B
.	O
exercise	O
197	O
(	O
vector	O
regression	O
)	O
.	O
consider	O
predicting	O
a	O
vector	O
output	O
y	O
given	O
training	B
data	O
x	O
∪	O
y	O
=	O
{	O
xn	O
,	O
yn	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
.	O
to	O
make	O
a	O
gp	O
predictor	O
(	O
19.8.8	O
)	O
p	O
(	O
y∗	O
|x∗	O
,	O
x	O
,	O
y	O
)	O
we	O
need	O
a	O
gaussian	O
model	B
p	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
,	O
y∗	O
|x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
x∗	O
)	O
a	O
gp	O
requires	O
then	O
a	O
speciﬁcation	O
of	O
the	O
covariance	B
c	O
(	O
ym	O
two	O
diﬀerent	O
input	O
vectors	O
.	O
show	O
that	O
under	O
the	O
dimension	O
independence	O
assumption	O
i	O
,	O
yn	O
(	O
19.8.9	O
)	O
j	O
|xn	O
,	O
xm	O
)	O
of	O
the	O
components	O
of	O
the	O
outputs	O
for	O
c	O
(	O
ym	O
j	O
|xn	O
,	O
xm	O
)	O
=	O
ci	O
(	O
ym	O
(	O
19.8.10	O
)	O
i	O
|xn	O
,	O
xm	O
)	O
is	O
a	O
covariance	B
function	I
for	O
the	O
ith	O
dimension	O
,	O
that	O
separate	O
gp	O
predictors	O
can	O
be	O
where	O
ci	O
(	O
ym	O
constructed	O
independently	O
,	O
one	O
for	O
each	O
output	O
dimension	O
i.	O
i	O
|xn	O
,	O
xm	O
)	O
δij	O
i	O
,	O
yn	O
i	O
,	O
yn	O
i	O
,	O
yn	O
draft	O
march	O
9	O
,	O
2010	O
363	O
exercise	O
198.	O
consider	O
the	O
markov	O
update	O
of	O
a	O
linear	B
dynamical	I
system	I
,	O
section	O
(	O
24.1	O
)	O
,	O
exercises	O
xt	O
=	O
axt−1	O
+	O
ηt	O
,	O
where	O
a	O
is	O
a	O
given	O
matrix	B
and	O
ηt	O
is	O
zero	O
mean	B
gaussian	O
noise	O
with	O
covariance	O
(	O
cid:10	O
)	O
ηi	O
,	O
tηj	O
,	O
t	O
(	O
cid:48	O
)	O
(	O
cid:11	O
)	O
=	O
σ2δijδt	O
,	O
t	O
(	O
cid:48	O
)	O
.	O
also	O
,	O
(	O
19.8.11	O
)	O
t	O
≥	O
2	O
1.	O
show	O
that	O
x1	O
,	O
.	O
.	O
.	O
,	O
xt	O
is	O
gaussian	O
distributed	O
.	O
2.	O
show	O
that	O
the	O
covariance	B
matrix	O
of	O
x1	O
,	O
.	O
.	O
.	O
,	O
xt	O
has	O
elements	O
p	O
(	O
x1	O
)	O
=	O
n	O
(	O
x1	O
0	O
,	O
σ	O
)	O
.	O
(	O
cid:69	O
)	O
(	O
cid:68	O
)	O
xt	O
(	O
cid:48	O
)	O
xt	O
t	O
=	O
(	O
cid:40	O
)	O
at	O
(	O
cid:48	O
)	O
−tς	O
atς	O
(	O
cid:0	O
)	O
at	O
(	O
cid:1	O
)	O
t	O
t	O
(	O
cid:54	O
)	O
=	O
t	O
(	O
cid:48	O
)	O
t	O
=	O
t	O
(	O
cid:48	O
)	O
(	O
19.8.12	O
)	O
and	O
explain	O
why	O
a	O
linear	B
dynamical	I
system	I
is	O
a	O
(	O
constrained	O
)	O
gaussian	O
process	O
.	O
3.	O
consider	O
yt	O
=	O
bxt	O
+	O
t	O
where	O
t	O
is	O
zero	O
mean	B
gaussian	O
noise	O
with	O
covariance	O
(	O
cid:10	O
)	O
i	O
,	O
tj	O
,	O
t	O
(	O
cid:48	O
)	O
(	O
cid:11	O
)	O
=	O
ν2δijδt	O
,	O
t	O
(	O
cid:48	O
)	O
.	O
the	O
vectors	O
	O
are	O
(	O
19.8.13	O
)	O
uncorrelated	O
with	O
the	O
vectors	O
η.	O
show	O
that	O
the	O
sequence	O
of	O
vectors	O
y1	O
,	O
.	O
.	O
.	O
,	O
yt	O
is	O
a	O
gaussian	O
process	O
with	O
a	O
suitably	O
deﬁned	O
covariance	B
function	I
.	O
exercise	O
199.	O
a	O
form	O
of	O
independent	B
components	I
analysis	I
,	O
section	O
(	O
21.6	O
)	O
,	O
of	O
a	O
one-dimensional	O
signal	O
y1	O
,	O
.	O
.	O
.	O
,	O
yt	O
is	O
obtained	O
from	O
the	O
joint	B
model	O
(	O
cid:40	O
)	O
(	O
cid:89	O
)	O
(	O
cid:0	O
)	O
yt	O
w1x1	O
t	O
p	O
(	O
y1	O
:	O
t	O
,	O
x1	O
1	O
:	O
t	O
,	O
x2	O
1	O
:	O
t|w	O
)	O
=	O
p	O
(	O
yt|x1	O
t	O
,	O
x2	O
t	O
,	O
w	O
)	O
n	O
with	O
p	O
(	O
yt|x1	O
t	O
,	O
x2	O
t	O
,	O
w	O
)	O
=	O
n	O
t	O
,	O
ν2	O
(	O
cid:1	O
)	O
t	O
+	O
w2x2	O
(	O
cid:41	O
)	O
(	O
cid:0	O
)	O
x1	O
1	O
:	O
t	O
0	O
,	O
σ	O
(	O
cid:1	O
)	O
(	O
cid:0	O
)	O
x2	O
1	O
:	O
t	O
0	O
,	O
σ	O
(	O
cid:1	O
)	O
n	O
(	O
19.8.14	O
)	O
(	O
19.8.15	O
)	O
where	O
ν2	O
is	O
a	O
given	O
noise	O
variance	B
.	O
the	O
signal	O
y1	O
:	O
t	O
can	O
be	O
viewed	O
as	O
the	O
linear	B
combination	O
of	O
two	O
independent	O
gaussian	O
processes	O
.	O
the	O
covariance	B
matrices	O
of	O
the	O
two	O
processes	O
have	O
elements	O
from	O
a	O
stationary	B
kernel	O
,	O
σt	O
,	O
t	O
(	O
cid:48	O
)	O
=	O
e	O
−	O
(	O
t−t	O
(	O
cid:48	O
)	O
)	O
2	O
(	O
19.8.16	O
)	O
1.	O
write	O
down	O
an	O
em	O
algorithm	B
for	O
learning	B
the	O
mixing	O
parameters	O
w1	O
,	O
w2	O
given	O
an	O
observation	O
se-	O
quence	O
y1	O
:	O
t	O
.	O
2.	O
consider	O
an	O
extension	O
of	O
the	O
above	O
model	B
to	O
the	O
case	O
of	O
two	O
outputs	O
:	O
p	O
(	O
y1	O
1	O
:	O
t	O
,	O
y2	O
1	O
:	O
t	O
,	O
x1	O
1	O
:	O
t	O
,	O
x2	O
1	O
:	O
t|w	O
)	O
=	O
p	O
(	O
yi	O
t|x1	O
t	O
,	O
x2	O
t	O
,	O
wi	O
)	O
n	O
(	O
cid:40	O
)	O
(	O
cid:89	O
)	O
2	O
(	O
cid:89	O
)	O
i=1	O
t	O
(	O
cid:41	O
)	O
(	O
cid:0	O
)	O
x1	O
1	O
:	O
t	O
0	O
,	O
σ	O
(	O
cid:1	O
)	O
with	O
p	O
(	O
y1	O
p	O
(	O
y2	O
t	O
,	O
x2	O
t	O
,	O
x2	O
t	O
|x1	O
t	O
|x1	O
t	O
,	O
w	O
)	O
=	O
n	O
t	O
,	O
w	O
)	O
=	O
n	O
(	O
cid:0	O
)	O
y1	O
(	O
cid:0	O
)	O
y1	O
t	O
,	O
ν2	O
(	O
cid:1	O
)	O
t	O
,	O
ν2	O
(	O
cid:1	O
)	O
t	O
w11x1	O
t	O
w21x1	O
t	O
+	O
w12x2	O
t	O
+	O
w22x2	O
(	O
cid:0	O
)	O
x2	O
1	O
:	O
t	O
0	O
,	O
σ	O
(	O
cid:1	O
)	O
n	O
(	O
19.8.17	O
)	O
(	O
19.8.18	O
)	O
(	O
19.8.19	O
)	O
show	O
that	O
for	O
t	O
>	O
1	O
the	O
likelihood	B
p	O
(	O
y1	O
1	O
:	O
t	O
,	O
y2	O
1	O
:	O
t|w	O
)	O
(	O
19.8.20	O
)	O
is	O
not	O
invariant	O
with	O
respect	O
to	O
an	O
orthogonal	B
rotation	O
w	O
(	O
cid:48	O
)	O
=	O
wr	O
,	O
with	O
rrt	O
=	O
i	O
,	O
and	O
explain	O
the	O
signiﬁcance	O
of	O
this	O
with	O
respect	O
to	O
identifying	O
independent	O
components	O
.	O
364	O
draft	O
march	O
9	O
,	O
2010	O
chapter	O
20	O
mixture	B
models	O
20.1	O
density	B
estimation	I
using	O
mixtures	O
a	O
mixture	B
model	I
is	O
one	O
in	O
which	O
a	O
set	O
of	O
component	O
models	O
is	O
combined	O
to	O
produce	O
a	O
richer	O
model	B
:	O
p	O
(	O
v	O
)	O
=	O
p	O
(	O
v|h	O
)	O
p	O
(	O
h	O
)	O
(	O
20.1.1	O
)	O
the	O
variable	B
v	O
is	O
‘	O
visible	B
’	O
or	O
‘	O
observable	O
’	O
and	O
h	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
h	O
indexes	O
each	O
component	O
model	B
p	O
(	O
v|h	O
)	O
,	O
along	O
with	O
its	O
weight	B
p	O
(	O
h	O
)	O
.	O
mixture	B
models	O
have	O
natural	B
application	O
in	O
clustering	B
data	O
,	O
where	O
h	O
indexes	O
the	O
cluster	O
.	O
this	O
interpretation	O
can	O
be	O
gained	O
from	O
considering	O
how	O
to	O
generate	O
a	O
sample	B
datapoint	O
v	O
from	O
the	O
model	B
equation	O
(	O
20.1.1	O
)	O
.	O
first	O
we	O
sample	B
a	O
cluster	O
h	O
from	O
p	O
(	O
h	O
)	O
,	O
and	O
then	O
draw	O
a	O
visible	B
state	O
v	O
from	O
p	O
(	O
v|h	O
)	O
.	O
for	O
a	O
set	O
of	O
i.i.d	O
.	O
data	B
v1	O
,	O
.	O
.	O
.	O
,	O
vn	O
,	O
a	O
mixture	B
model	I
is	O
of	O
the	O
form	O
,	O
ﬁg	O
(	O
20.1	O
)	O
,	O
h	O
(	O
cid:88	O
)	O
h=1	O
n	O
(	O
cid:89	O
)	O
(	O
cid:88	O
)	O
n=1	O
hn	O
p	O
(	O
v1	O
,	O
.	O
.	O
.	O
,	O
vn	O
)	O
=	O
p	O
(	O
vn|hn	O
)	O
p	O
(	O
hn	O
)	O
clustering	B
is	O
achieved	O
by	O
inference	B
of	O
argmax	O
h1	O
,	O
...	O
,	O
hn	O
p	O
(	O
h1	O
,	O
.	O
.	O
.	O
,	O
hn|v1	O
,	O
.	O
.	O
.	O
,	O
vn	O
)	O
(	O
20.1.2	O
)	O
(	O
20.1.3	O
)	O
which	O
,	O
thanks	O
to	O
the	O
factorised	B
form	O
of	O
the	O
distribution	B
is	O
equivalent	B
to	O
computing	O
arg	O
maxhn	O
p	O
(	O
hn|vn	O
)	O
for	O
each	O
datapoint	O
.	O
in	O
this	O
way	O
we	O
can	O
cluster	O
many	O
kinds	O
of	O
data	B
for	O
which	O
a	O
‘	O
distance	O
’	O
measure	O
in	O
the	O
sense	O
of	O
the	O
classical	O
k-means	B
algorithm	O
,	O
section	O
(	O
20.3.5	O
)	O
,	O
is	O
not	O
directly	O
apparent	O
.	O
explicitly	O
writing	O
the	O
dependence	B
on	O
the	O
parameters	O
,	O
the	O
model	B
is	O
p	O
(	O
v	O
,	O
h|θ	O
)	O
=	O
p	O
(	O
v|h	O
,	O
θv|h	O
)	O
p	O
(	O
h|θh	O
)	O
(	O
20.1.4	O
)	O
the	O
optimal	O
parameters	O
θv|h	O
,	O
θh	O
of	O
a	O
mixture	B
model	I
are	O
then	O
most	O
commonly	O
set	O
by	O
maximum	B
likelihood	I
,	O
θopt	O
=	O
argmax	O
θ	O
p	O
(	O
v1	O
,	O
.	O
.	O
.	O
,	O
vn|θ	O
)	O
(	O
20.1.5	O
)	O
numerically	O
this	O
can	O
be	O
achieved	O
using	O
an	O
optimisation	B
procedure	O
such	O
as	O
gradient	O
based	O
approaches	O
.	O
alternatively	O
,	O
by	O
treating	O
the	O
component	O
indices	O
as	O
latent	O
variables	O
,	O
one	O
may	O
also	O
apply	O
the	O
em	O
algorithm	B
,	O
as	O
described	O
in	O
the	O
following	O
section	O
,	O
which	O
in	O
many	O
classical	O
models	O
produces	O
simple	O
update	O
formulae	O
.	O
365	O
expectation	B
maximisation	I
for	O
mixture	B
models	O
θh	O
θv|h	O
hn	O
vn	O
n	O
figure	O
20.1	O
:	O
a	O
mixture	B
model	I
has	O
a	O
trivial	O
graphical	O
represen-	O
tation	O
as	O
a	O
dag	O
with	O
a	O
single	O
hidden	B
node	O
,	O
which	O
can	O
be	O
in	O
and	O
one	O
of	O
h	O
states	O
,	O
i	O
=	O
1	O
.	O
.	O
.	O
,	O
h.	O
the	O
parameters	O
are	O
assumed	O
common	O
across	O
all	O
datapoints	O
.	O
example	O
86.	O
the	O
data	B
in	O
ﬁg	O
(	O
20.2	O
)	O
naturally	O
has	O
two	O
clusters	O
and	O
can	O
be	O
modelled	O
with	O
a	O
mixture	O
of	O
two	O
two-dimensional	O
gaussians	O
,	O
each	O
gaussian	O
describing	O
one	O
of	O
the	O
clusters	O
.	O
here	O
there	O
is	O
a	O
clear	O
visual	O
interpretation	O
of	O
the	O
meaning	O
of	O
‘	O
cluster	O
’	O
,	O
with	O
the	O
mixture	B
model	I
placing	O
two	O
datapoints	O
in	O
the	O
same	O
cluster	O
if	O
they	O
are	O
both	O
likely	O
to	O
be	O
generated	O
by	O
the	O
same	O
model	B
component	O
.	O
20.2	O
expectation	B
maximisation	I
for	O
mixture	B
models	O
by	O
treating	O
the	O
index	O
h	O
as	O
a	O
missing	B
variable	O
,	O
mixture	B
models	O
can	O
be	O
trained	O
using	O
the	O
em	O
algorithm	B
,	O
section	O
(	O
11.2	O
)	O
.	O
there	O
are	O
two	O
sets	O
of	O
parameters	O
–	O
θv|h	O
for	O
each	O
component	O
model	B
p	O
(	O
v|h	O
,	O
θv|h	O
)	O
and	O
θh	O
for	O
the	O
mixture	B
weights	O
p	O
(	O
h|θh	O
)	O
.	O
according	O
to	O
the	O
general	O
approach	B
for	O
i.i.d	O
.	O
data	B
of	O
section	O
(	O
11.2	O
)	O
,	O
we	O
need	O
to	O
consider	O
the	O
energy	B
term	O
:	O
n	O
(	O
cid:88	O
)	O
n	O
(	O
cid:88	O
)	O
n=1	O
n=1	O
where	O
e	O
(	O
θ	O
)	O
=	O
=	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
vn	O
,	O
h|θ	O
)	O
(	O
cid:105	O
)	O
pold	O
(	O
h|vn	O
)	O
(	O
cid:10	O
)	O
log	O
p	O
(	O
vn|h	O
,	O
θv|h	O
)	O
(	O
cid:11	O
)	O
pold	O
(	O
h|vn	O
)	O
+	O
n	O
(	O
cid:88	O
)	O
n=1	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
h|θh	O
)	O
(	O
cid:105	O
)	O
pold	O
(	O
h|vn	O
)	O
(	O
20.2.1	O
)	O
(	O
20.2.2	O
)	O
(	O
20.2.3	O
)	O
pold	O
(	O
h|vn	O
)	O
∝	O
p	O
(	O
vn|h	O
,	O
θold	O
v|h	O
)	O
p	O
(	O
h|θold	O
h	O
)	O
and	O
maximise	O
(	O
20.2.2	O
)	O
with	O
respect	O
to	O
the	O
parameters	O
θv|h	O
,	O
θh	O
,	O
h	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
h.	O
20.2.1	O
unconstrained	O
discrete	B
tables	O
here	O
we	O
consider	O
training	B
a	O
simple	O
belief	O
networkp	O
(	O
v|h	O
,	O
θv|h	O
)	O
p	O
(	O
h|θh	O
)	O
,	O
v	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
v	O
,	O
h	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
h	O
in	O
which	O
the	O
tables	O
are	O
unconstrained	O
.	O
this	O
is	O
a	O
special	O
case	O
of	O
the	O
more	O
general	O
framework	O
discussed	O
in	O
section	O
(	O
11.2	O
)	O
.	O
figure	O
20.2	O
:	O
two	O
dimensional	O
data	B
which	O
displays	O
clusters	O
.	O
in	O
this	O
case	O
a	O
gaus-	O
sian	O
mixture	B
model	I
1/2n	O
(	O
x	O
m1	O
,	O
c1	O
)	O
+	O
1/2n	O
(	O
x	O
m2	O
,	O
c2	O
)	O
would	O
ﬁt	O
the	O
data	B
well	O
for	O
suitable	O
means	O
m1	O
,	O
m2	O
and	O
covariances	O
c1	O
,	O
c2	O
.	O
366	O
draft	O
march	O
9	O
,	O
2010	O
−3−2−10123−3−2−101234	O
expectation	B
maximisation	I
for	O
mixture	B
models	O
n	O
(	O
cid:88	O
)	O
h	O
p	O
(	O
h	O
)	O
=	O
1.	O
isolating	O
the	O
dependence	B
of	O
equation	B
(	O
20.2.2	O
)	O
on	O
p	O
(	O
h	O
)	O
we	O
obtain	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
h	O
)	O
(	O
cid:105	O
)	O
pold	O
(	O
h|vn	O
)	O
=	O
(	O
cid:88	O
)	O
m-step	O
:	O
p	O
(	O
h	O
)	O
if	O
no	O
constraint	O
is	O
placed	O
on	O
p	O
(	O
h|θh	O
)	O
we	O
may	O
write	O
the	O
parameters	O
as	O
simply	O
p	O
(	O
h	O
)	O
,	O
with	O
the	O
understanding	O
that	O
0	O
≤	O
p	O
(	O
h	O
)	O
≤	O
1	O
and	O
(	O
cid:80	O
)	O
we	O
now	O
wish	O
to	O
maximise	O
equation	B
(	O
20.2.4	O
)	O
with	O
respect	O
to	O
p	O
(	O
h	O
)	O
under	O
the	O
constraint	O
that	O
(	O
cid:80	O
)	O
j	O
p	O
(	O
h	O
)	O
=	O
1.	O
it	O
is	O
standard	O
to	O
treat	O
this	O
maximisation	B
problem	O
using	O
lagrange	O
multipliers	O
,	O
see	O
exercise	O
(	O
203	O
)	O
.	O
here	O
we	O
take	O
an	O
alternative	O
approach	B
based	O
on	O
recognising	O
the	O
similarity	O
of	O
the	O
above	O
to	O
a	O
form	O
of	O
kullback-leibler	O
divergence	B
.	O
first	O
we	O
deﬁne	O
the	O
distribution	B
pold	O
(	O
h|vn	O
)	O
n	O
(	O
cid:88	O
)	O
(	O
20.2.4	O
)	O
log	O
p	O
(	O
h	O
)	O
n=1	O
n=1	O
h	O
(	O
cid:80	O
)	O
n	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
n	O
n=1	O
pold	O
(	O
h|vn	O
)	O
n=1	O
pold	O
(	O
h|vn	O
)	O
h	O
˜p	O
(	O
h	O
)	O
≡	O
then	O
maximising	O
equation	B
(	O
20.2.4	O
)	O
is	O
equivalent	B
to	O
maximising	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
h	O
)	O
(	O
cid:105	O
)	O
˜p	O
(	O
h	O
)	O
since	O
the	O
two	O
expressions	O
are	O
related	O
by	O
the	O
constant	O
factor	B
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
n	O
n=1	O
pold	O
(	O
h|vn	O
)	O
.	O
by	O
subtracting	O
the	O
θ	O
independent	O
term	O
(	O
cid:104	O
)	O
log	O
˜p	O
(	O
h	O
)	O
(	O
cid:105	O
)	O
˜p	O
(	O
h	O
)	O
from	O
equation	B
(	O
20.2.6	O
)	O
,	O
we	O
obtain	O
the	O
negative	O
kullback-leibler	O
divergence	B
kl	O
(	O
˜p|p	O
)	O
.	O
this	O
means	O
that	O
the	O
optimal	O
p	O
(	O
h	O
)	O
is	O
that	O
distribution	B
which	O
minimises	O
the	O
kullback-leibler	O
divergence	B
.	O
optimally	O
,	O
therefore	O
˜p	O
(	O
h	O
)	O
=	O
p	O
(	O
h	O
)	O
,	O
so	O
that	O
(	O
20.2.6	O
)	O
h	O
h	O
(	O
cid:88	O
)	O
h=1	O
λ	O
(	O
h	O
)	O
(	O
cid:32	O
)	O
1	O
−	O
v	O
(	O
cid:88	O
)	O
v=1	O
(	O
cid:33	O
)	O
p	O
(	O
v|h	O
)	O
−	O
λ	O
(	O
h	O
=	O
j	O
)	O
p	O
(	O
v	O
=	O
i|h	O
=	O
j	O
)	O
=	O
0	O
(	O
cid:88	O
)	O
n	O
(	O
cid:88	O
)	O
h	O
(	O
cid:88	O
)	O
v	O
n=1	O
h=1	O
l	O
≡	O
diﬀerentiating	O
with	O
respect	O
to	O
p	O
(	O
v	O
=	O
i|h	O
=	O
j	O
)	O
and	O
equating	O
to	O
zero	O
,	O
i	O
[	O
vn	O
=	O
v	O
]	O
pold	O
(	O
h|vn	O
)	O
log	O
p	O
(	O
v|h	O
)	O
+	O
n	O
(	O
cid:88	O
)	O
p	O
(	O
v	O
=	O
i|h	O
=	O
j	O
)	O
i	O
[	O
vn	O
=	O
i	O
]	O
pold	O
(	O
h	O
=	O
j|v	O
=	O
i	O
)	O
n	O
(	O
cid:88	O
)	O
i	O
[	O
vn	O
=	O
i	O
]	O
pold	O
(	O
h	O
=	O
j|v	O
=	O
i	O
)	O
(	O
cid:80	O
)	O
n	O
(	O
cid:80	O
)	O
v	O
(	O
cid:80	O
)	O
n	O
i	O
[	O
vn	O
=	O
i	O
]	O
pold	O
(	O
h	O
=	O
j|v	O
=	O
i	O
)	O
n=1	O
n=1	O
i=1	O
n=1	O
i	O
[	O
vn	O
=	O
i	O
]	O
pold	O
(	O
h	O
=	O
j|v	O
=	O
i	O
)	O
∂l	O
∂p	O
(	O
v	O
=	O
i|h	O
=	O
j	O
)	O
hence	O
=	O
n=1	O
pnew	O
(	O
v	O
=	O
i|h	O
=	O
j	O
)	O
∝	O
pnew	O
(	O
v	O
=	O
i|h	O
=	O
j	O
)	O
=	O
draft	O
march	O
9	O
,	O
2010	O
which	O
,	O
using	O
the	O
normalisation	B
requirement	O
,	O
gives	O
pnew	O
(	O
h	O
)	O
=	O
(	O
cid:80	O
)	O
n	O
(	O
cid:80	O
)	O
n	O
(	O
cid:80	O
)	O
n=1	O
pold	O
(	O
h|vn	O
)	O
n=1	O
pold	O
(	O
h|vn	O
)	O
m-step	O
:	O
p	O
(	O
v|h	O
)	O
the	O
dependence	B
of	O
equation	B
(	O
20.2.2	O
)	O
on	O
p	O
(	O
v|h	O
)	O
is	O
n	O
(	O
cid:88	O
)	O
1	O
n	O
n=1	O
=	O
h	O
pold	O
(	O
h|vn	O
)	O
n	O
(	O
cid:88	O
)	O
(	O
cid:10	O
)	O
log	O
p	O
(	O
cid:0	O
)	O
vn|h	O
,	O
θv|h	O
(	O
cid:1	O
)	O
(	O
cid:11	O
)	O
if	O
the	O
distributions	O
p	O
(	O
cid:0	O
)	O
v|h	O
,	O
θv|h	O
n=1	O
pold	O
(	O
h|vn	O
)	O
(	O
cid:1	O
)	O
are	O
not	O
constrained	O
,	O
we	O
can	O
apply	O
a	O
similar	O
kullback-leibler	O
method	O
,	O
as	O
we	O
(	O
20.2.8	O
)	O
did	O
in	O
section	O
(	O
11.2	O
)	O
.	O
for	O
compatibility	O
with	O
other	O
texts	O
,	O
here	O
we	O
demonstrate	O
the	O
more	O
standard	O
lagrange	O
procedure	O
.	O
we	O
need	O
to	O
ensure	O
that	O
p	O
(	O
v|h	O
)	O
is	O
a	O
distribution	B
for	O
each	O
of	O
the	O
mixture	B
states	O
h	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
h.	O
this	O
can	O
be	O
achieved	O
using	O
a	O
set	O
of	O
lagrange	O
multipliers	O
,	O
giving	O
the	O
lagrangian	O
:	O
(	O
20.2.5	O
)	O
(	O
20.2.7	O
)	O
(	O
20.2.9	O
)	O
(	O
20.2.10	O
)	O
(	O
20.2.11	O
)	O
(	O
20.2.12	O
)	O
367	O
expectation	B
maximisation	I
for	O
mixture	B
models	O
θh	O
θvi|h	O
hn	O
vn	O
i	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
figure	O
20.3	O
:	O
mixture	O
of	O
a	O
product	O
of	O
bernoulli	O
distributions	O
.	O
in	O
a	O
bayesian	O
treatment	O
,	O
a	O
parameter	B
prior	O
is	O
used	O
.	O
in	O
the	O
text	O
we	O
simply	O
set	O
the	O
parameters	O
using	O
maximum	B
likelihood	I
.	O
e-step	O
according	O
to	O
the	O
general	O
em	O
procedure	O
,	O
section	O
(	O
11.2	O
)	O
,	O
optimally	O
we	O
set	O
pnew	O
(	O
h|vn	O
)	O
=	O
p	O
(	O
h|vn	O
)	O
:	O
(	O
cid:80	O
)	O
pnew	O
(	O
h|vn	O
)	O
=	O
p	O
(	O
vn|h	O
)	O
p	O
(	O
h	O
)	O
h	O
p	O
(	O
vn|h	O
)	O
p	O
(	O
h	O
)	O
(	O
20.2.13	O
)	O
equations	O
(	O
20.2.7,20.2.12,20.2.13	O
)	O
are	O
repeated	O
until	O
convergence	O
.	O
the	O
initialisation	O
of	O
the	O
tables	O
and	O
mixture	B
probabilities	O
can	O
severely	O
aﬀect	O
the	O
quality	O
of	O
the	O
solution	O
found	O
since	O
the	O
likelihood	B
often	O
has	O
local	B
optima	O
.	O
if	O
random	O
initialisations	O
are	O
used	O
,	O
it	O
is	O
recommended	O
to	O
record	O
the	O
converged	O
value	B
of	O
the	O
likelihood	B
itself	O
,	O
to	O
see	O
which	O
parameters	O
have	O
the	O
higher	O
likelihood	B
.	O
the	O
solution	O
with	O
the	O
highest	O
likelihood	B
is	O
to	O
be	O
preferred	O
.	O
20.2.2	O
mixture	O
of	O
product	O
of	O
bernoulli	O
distributions	O
we	O
describe	O
a	O
simple	O
mixture	O
model	B
that	O
can	O
be	O
used	O
to	O
clustering	B
binary	O
vectors	O
,	O
v	O
=	O
(	O
v1	O
,	O
.	O
.	O
.	O
,	O
vd	O
)	O
t	O
,	O
vi	O
∈	O
{	O
0	O
,	O
1	O
}	O
.	O
the	O
mixture	O
of	O
bernoulli	O
products1	O
model	B
is	O
given	O
by	O
h	O
(	O
cid:88	O
)	O
d	O
(	O
cid:89	O
)	O
p	O
(	O
v	O
)	O
=	O
p	O
(	O
h	O
)	O
h=1	O
i=1	O
p	O
(	O
vi|h	O
)	O
(	O
20.2.14	O
)	O
where	O
each	O
term	O
p	O
(	O
vi|h	O
)	O
is	O
a	O
bernoulli	O
distribution	B
.	O
the	O
model	B
is	O
depicted	O
in	O
ﬁg	O
(	O
20.3	O
)	O
and	O
has	O
parameters	O
p	O
(	O
h	O
)	O
and	O
p	O
(	O
vi	O
=	O
1|h	O
)	O
,	O
h	O
=	O
1	O
.	O
.	O
.	O
,	O
h.	O
em	O
training	B
to	O
train	O
the	O
model	B
under	O
maximum	B
likelihood	I
it	O
is	O
convenient	O
to	O
use	O
the	O
em	O
algorithm	B
which	O
,	O
as	O
usual	O
,	O
may	O
be	O
derived	O
by	O
writing	O
down	O
the	O
energy	B
:	O
n	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
vn	O
,	O
h	O
)	O
(	O
cid:105	O
)	O
pold	O
(	O
h|vn	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
n	O
(	O
cid:88	O
)	O
i	O
i	O
|h	O
)	O
(	O
cid:105	O
)	O
pold	O
(	O
h|vn	O
)	O
+	O
(	O
cid:88	O
)	O
n	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
h	O
)	O
(	O
cid:105	O
)	O
pold	O
(	O
h|vn	O
)	O
(	O
20.2.15	O
)	O
and	O
then	O
performing	O
the	O
maximisation	B
over	O
the	O
table	O
entries	O
.	O
from	O
our	O
general	O
results	O
,	O
section	O
(	O
11.2	O
)	O
,	O
we	O
may	O
immediately	O
jump	O
to	O
the	O
updates	O
.	O
the	O
m-step	O
is	O
given	O
by	O
i	O
=	O
1	O
]	O
pold	O
(	O
h	O
=	O
j|vn	O
)	O
+	O
(	O
cid:80	O
)	O
i	O
[	O
vn	O
i	O
=	O
1	O
]	O
pold	O
(	O
h	O
=	O
j|vn	O
)	O
i	O
[	O
vn	O
n	O
i	O
=	O
0	O
]	O
pold	O
(	O
h	O
=	O
j|vn	O
)	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
vn	O
(	O
cid:80	O
)	O
n	O
n	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
i	O
[	O
vn	O
(	O
cid:80	O
)	O
h	O
(	O
cid:48	O
)	O
(	O
cid:80	O
)	O
n	O
pold	O
(	O
h	O
=	O
j|vn	O
)	O
|vn	O
)	O
d	O
(	O
cid:89	O
)	O
n	O
pold	O
(	O
h	O
(	O
cid:48	O
)	O
p	O
(	O
vn	O
i	O
|h	O
=	O
j	O
)	O
i=1	O
pnew	O
(	O
vi	O
=	O
1|h	O
=	O
j	O
)	O
=	O
pnew	O
(	O
h	O
=	O
j	O
)	O
=	O
and	O
the	O
e-step	O
by	O
pold	O
(	O
h	O
=	O
j|vn	O
)	O
∝	O
p	O
(	O
h	O
=	O
j	O
)	O
(	O
20.2.16	O
)	O
(	O
20.2.17	O
)	O
1this	O
is	O
similar	O
to	O
the	O
naive	O
bayes	O
classiﬁer	B
in	O
which	O
the	O
class	O
labels	O
are	O
always	O
hidden	B
.	O
368	O
draft	O
march	O
9	O
,	O
2010	O
expectation	B
maximisation	I
for	O
mixture	B
models	O
figure	O
20.4	O
:	O
data	B
from	O
questionnaire	O
responses	O
.	O
150	O
people	O
were	O
each	O
asked	O
5	O
questions	O
,	O
with	O
‘	O
yes	O
’	O
(	O
white	O
)	O
and	O
‘	O
no	O
’	O
(	O
gray	O
)	O
answers	O
.	O
black	O
denotes	O
that	O
the	O
absence	O
of	O
a	O
response	O
(	O
missing	B
data	I
)	O
.	O
this	O
training	B
data	O
was	O
generated	O
by	O
two	O
component	O
binomial	B
mixture	O
.	O
missing	B
data	I
was	O
simulated	O
by	O
randomly	O
removing	O
values	O
from	O
the	O
dataset	O
.	O
equations	O
(	O
20.2.16,20.2.17	O
)	O
are	O
iterated	O
until	O
convergence	O
.	O
if	O
an	O
attribute	O
i	O
is	O
missing	B
for	O
datapoint	O
n	O
,	O
one	O
needs	O
to	O
sum	O
over	O
the	O
states	O
of	O
the	O
corresponding	O
vn	O
eﬀect	O
of	O
performing	O
the	O
summation	O
for	O
this	O
model	B
is	O
simply	O
to	O
remove	O
the	O
corresponding	O
factor	B
p	O
(	O
vn	O
from	O
the	O
algorithm	B
,	O
see	O
exercise	O
(	O
200	O
)	O
.	O
i	O
.	O
the	O
i	O
|h	O
)	O
initialisation	O
the	O
em	O
algorithm	B
can	O
be	O
very	O
sensitive	O
to	O
initial	O
conditions	O
.	O
consider	O
the	O
following	O
initialisation	O
:	O
p	O
(	O
vi	O
=	O
1|h	O
=	O
j	O
)	O
=	O
0.5	O
,	O
with	O
p	O
(	O
h	O
)	O
set	O
arbitrarily	O
.	O
this	O
means	O
that	O
at	O
the	O
ﬁrst	O
iteration	O
,	O
pold	O
(	O
h	O
=	O
j|vn	O
)	O
=	O
p	O
(	O
h	O
=	O
j	O
)	O
.	O
the	O
subsequent	O
m-step	O
updates	O
are	O
(	O
cid:48	O
)	O
)	O
(	O
20.2.18	O
)	O
pnew	O
(	O
h	O
)	O
=	O
p	O
(	O
h	O
)	O
,	O
pnew	O
(	O
vi|h	O
=	O
j	O
)	O
=	O
pnew	O
(	O
vi|h	O
=	O
j	O
for	O
any	O
j	O
,	O
j	O
(	O
cid:48	O
)	O
.	O
this	O
means	O
that	O
the	O
parameters	O
p	O
(	O
v|h	O
)	O
immediately	O
become	O
independent	O
of	O
h	O
and	O
the	O
model	B
is	O
numerically	O
trapped	O
in	O
a	O
symmetric	O
solution	O
.	O
it	O
makes	O
sense	O
,	O
therefore	O
,	O
to	O
initialise	O
the	O
parameters	O
in	O
a	O
non-symmetric	O
fashion	O
.	O
example	O
87	O
(	O
questionnaire	O
)	O
.	O
a	O
company	O
sends	O
out	O
a	O
questionnaire	O
containing	O
a	O
set	O
of	O
d	O
‘	O
yes/no	O
’	O
ques-	O
tions	O
to	O
a	O
set	O
of	O
customers	O
.	O
the	O
binary	O
responses	O
of	O
a	O
customer	O
are	O
stored	O
in	O
a	O
vector	O
v	O
=	O
(	O
v1	O
,	O
.	O
.	O
.	O
,	O
vd	O
)	O
t.	O
in	O
total	O
n	O
customers	O
send	O
back	O
their	O
questionnaires	O
,	O
v1	O
,	O
.	O
.	O
.	O
,	O
vn	O
,	O
and	O
the	O
company	O
wishes	O
to	O
perform	O
an	O
analysis	B
to	O
ﬁnd	O
what	O
kinds	O
of	O
customers	O
it	O
has	O
.	O
the	O
company	O
assumes	O
there	O
are	O
h	O
essential	O
types	O
of	O
customer	O
for	O
which	O
the	O
proﬁle	O
of	O
responses	O
is	O
deﬁned	O
by	O
only	O
the	O
customer	O
type	O
.	O
data	B
from	O
a	O
questionnaire	O
containing	O
5	O
questions	O
,	O
with	O
150	O
respondents	O
is	O
presented	O
in	O
ﬁg	O
(	O
20.4	O
)	O
.	O
the	O
data	B
has	O
a	O
large	O
number	O
of	O
missing	B
values	O
.	O
we	O
assume	O
there	O
are	O
h	O
=	O
2	O
kinds	O
of	O
respondents	O
and	O
attempt	O
to	O
assign	O
each	O
respondent	O
into	O
one	O
of	O
the	O
two	O
clusters	O
.	O
running	O
the	O
em	O
algorithm	B
on	O
this	O
data	B
,	O
with	O
random	O
initial	O
values	O
for	O
the	O
tables	O
,	O
produces	O
the	O
results	O
in	O
ﬁg	O
(	O
20.5	O
)	O
.	O
based	O
on	O
assigning	O
each	O
datapoint	O
vn	O
to	O
the	O
cluster	O
with	O
maximal	O
posterior	B
probability	O
hn	O
=	O
arg	O
maxh	O
p	O
(	O
h|vn	O
)	O
,	O
given	O
a	O
trained	O
model	B
p	O
(	O
v|h	O
)	O
p	O
(	O
h	O
)	O
,	O
the	O
model	B
assigns	O
86	O
%	O
of	O
the	O
data	B
to	O
the	O
correct	O
cluster	O
(	O
which	O
is	O
known	O
in	O
this	O
simulated	O
case	O
)	O
.	O
see	O
ﬁg	O
(	O
20.5	O
)	O
and	O
mixprodbern.m	O
.	O
(	O
a	O
)	O
(	O
b	O
)	O
draft	O
march	O
9	O
,	O
2010	O
figure	O
20.5	O
:	O
em	O
learning	B
of	O
a	O
mixture	B
(	O
a	O
)	O
:	O
true	O
p	O
(	O
h	O
)	O
of	O
bernoulli	O
products	O
.	O
(	O
left	O
)	O
and	O
learned	O
p	O
(	O
h	O
)	O
(	O
right	O
)	O
for	O
h	O
=	O
1	O
,	O
2	O
.	O
(	O
b	O
)	O
:	O
true	O
p	O
(	O
v|h	O
)	O
(	O
left	O
)	O
and	O
learned	O
p	O
(	O
v|h	O
)	O
(	O
right	O
)	O
for	O
v	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
5.	O
each	O
column	O
pair	O
corresponds	O
to	O
p	O
(	O
vi|h	O
=	O
1	O
)	O
(	O
red	O
)	O
and	O
p	O
(	O
vi|h	O
=	O
2	O
)	O
(	O
blue	O
)	O
with	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
5.	O
the	O
learned	O
probabilities	O
are	O
reasonably	O
close	O
to	O
the	O
true	O
values	O
.	O
369	O
20406080100120140123451200.10.20.30.40.50.60.70.80.911200.10.20.30.40.50.60.70.80.911234500.10.20.30.40.50.60.70.80.911234500.10.20.30.40.50.60.70.80.91	O
the	O
gaussian	O
mixture	B
model	I
figure	O
20.6	O
:	O
top	O
:	O
a	O
selection	O
of	O
200	O
of	O
the	O
5000	O
handwritten	B
digits	I
in	O
the	O
training	B
set	O
.	O
bottom	O
:	O
the	O
trained	O
cluster	O
outputs	O
p	O
(	O
vi	O
=	O
1|h	O
)	O
for	O
h	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
20	O
mixtures	O
.	O
see	O
demomixbernoullidigits.m	O
.	O
example	O
88	O
(	O
handwritten	B
digits	I
)	O
.	O
we	O
have	O
a	O
collection	O
of	O
5000	O
handwritten	B
digits	I
which	O
we	O
wish	O
to	O
cluster	O
into	O
20	O
groups	O
,	O
ﬁg	O
(	O
20.6	O
)	O
.	O
each	O
digit	O
is	O
a	O
28×	O
28	O
=	O
784	O
dimensional	O
binary	O
vector	O
.	O
using	O
a	O
mixture	O
of	O
bernoulli	O
products	O
,	O
trained	O
with	O
50	O
iterations	O
of	O
em	O
(	O
with	O
a	O
random	O
perturbation	O
of	O
the	O
mean	B
of	O
the	O
data	B
used	O
as	O
initialisation	O
)	O
,	O
the	O
clusters	O
are	O
presented	O
in	O
ﬁg	O
(	O
20.6	O
)	O
.	O
as	O
we	O
see	O
,	O
the	O
method	O
captures	O
natural	B
clusters	O
in	O
the	O
data	B
–	O
for	O
example	O
,	O
there	O
are	O
two	O
kinds	O
of	O
1	O
,	O
one	O
slightly	O
more	O
slanted	O
than	O
the	O
other	O
,	O
two	O
kinds	O
of	O
4	O
,	O
etc	O
.	O
20.3	O
the	O
gaussian	O
mixture	B
model	I
gaussians	O
are	O
particularly	O
convenient	O
continuous	B
mixture	O
components	O
since	O
they	O
constitute	O
‘	O
bumps	O
’	O
of	O
probability	B
mass	O
,	O
aiding	O
an	O
intuitive	O
interpretation	O
of	O
the	O
model	B
.	O
as	O
a	O
reminder	O
,	O
a	O
d	O
dimensional	O
gaussian	O
distribution	B
for	O
a	O
continuous	B
variable	O
x	O
is	O
(	O
cid:26	O
)	O
1	O
(	O
cid:112	O
)	O
det	O
(	O
2πs	O
)	O
exp	O
1	O
2	O
−	O
(	O
x	O
−	O
m	O
)	O
t	O
s−1	O
(	O
x	O
−	O
m	O
)	O
(	O
cid:27	O
)	O
(	O
20.3.1	O
)	O
where	O
m	O
is	O
the	O
mean	B
and	O
s	O
is	O
the	O
covariance	B
matrix	O
.	O
a	O
mixture	O
of	O
gaussians	O
is	O
then	O
p	O
(	O
x|m	O
,	O
s	O
)	O
=	O
h	O
(	O
cid:88	O
)	O
p	O
(	O
x	O
)	O
=	O
where	O
p	O
(	O
i	O
)	O
is	O
the	O
mixture	B
weight	O
for	O
component	O
i.	O
for	O
a	O
set	O
of	O
data	B
x	O
=	O
(	O
cid:8	O
)	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
(	O
cid:9	O
)	O
and	O
under	O
the	O
usual	O
(	O
20.3.2	O
)	O
i=1	O
(	O
cid:26	O
)	O
exp	O
1	O
2	O
−	O
(	O
xn	O
−	O
mi	O
)	O
t	O
s−1	O
i	O
(	O
cid:27	O
)	O
(	O
xn	O
−	O
mi	O
)	O
(	O
20.3.3	O
)	O
p	O
(	O
x|mi	O
,	O
si	O
)	O
p	O
(	O
i	O
)	O
h	O
(	O
cid:88	O
)	O
n	O
(	O
cid:88	O
)	O
i.i.d	O
.	O
assumption	O
,	O
the	O
log	O
likelihood	B
is	O
log	O
p	O
(	O
x|θ	O
)	O
=	O
1	O
(	O
cid:112	O
)	O
det	O
(	O
2πsi	O
)	O
matrices	O
,	O
in	O
addition	O
to	O
0	O
≤	O
p	O
(	O
i	O
)	O
≤	O
1	O
,	O
(	O
cid:80	O
)	O
p	O
(	O
i	O
)	O
log	O
n=1	O
i=1	O
where	O
the	O
parameters	O
are	O
θ	O
=	O
{	O
mi	O
,	O
si	O
,	O
p	O
(	O
i	O
)	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
h	O
}	O
.	O
the	O
optimal	O
parameters	O
θ	O
can	O
be	O
set	O
using	O
maximum	B
likelihood	I
,	O
bearing	O
in	O
mind	O
the	O
constraint	O
that	O
the	O
si	O
must	O
be	O
symmetric	O
positive	B
deﬁnite	I
i	O
p	O
(	O
i	O
)	O
=	O
1.	O
gradient	B
based	O
optimisation	B
approaches	O
are	O
feasible	O
under	O
a	O
parameterisation	B
of	O
the	O
si	O
(	O
e.g	O
.	O
cholesky	O
decomposition	B
)	O
and	O
p	O
(	O
i	O
)	O
(	O
e.g	O
.	O
softmax	B
)	O
that	O
enforce	O
the	O
constraints	O
.	O
an	O
alternative	O
is	O
the	O
em	O
approach	B
which	O
in	O
this	O
case	O
is	O
particularly	O
convenient	O
since	O
it	O
automatically	O
provides	O
parameter	B
updates	O
that	O
ensure	O
these	O
constraints	O
.	O
20.3.1	O
em	O
algorithm	B
the	O
energy	B
term	O
is	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
xn	O
,	O
i	O
)	O
(	O
cid:105	O
)	O
pold	O
(	O
i|xn	O
)	O
=	O
n	O
(	O
cid:88	O
)	O
n=1	O
370	O
n	O
(	O
cid:88	O
)	O
n=1	O
(	O
cid:104	O
)	O
log	O
[	O
p	O
(	O
xn|h	O
)	O
p	O
(	O
i	O
)	O
]	O
(	O
cid:105	O
)	O
pold	O
(	O
i|xn	O
)	O
(	O
20.3.4	O
)	O
draft	O
march	O
9	O
,	O
2010	O
the	O
gaussian	O
mixture	B
model	I
(	O
cid:26	O
)	O
n	O
(	O
cid:88	O
)	O
n=1	O
plugging	O
in	O
the	O
deﬁnition	O
of	O
the	O
gaussian	O
components	O
,	O
we	O
have	O
pold	O
(	O
i|xn	O
)	O
1	O
2	O
−	O
(	O
xn	O
−	O
mi	O
)	O
t	O
s−1	O
i	O
(	O
xn	O
−	O
mi	O
)	O
−	O
1	O
2	O
log	O
det	O
(	O
2πsi	O
)	O
+	O
log	O
p	O
(	O
i	O
)	O
the	O
m-step	O
requires	O
the	O
maximisation	B
of	O
the	O
above	O
with	O
respect	O
to	O
mi	O
,	O
si	O
,	O
p	O
(	O
i	O
)	O
.	O
m-step	O
:	O
optimal	O
mi	O
maximising	O
equation	B
(	O
20.3.5	O
)	O
with	O
respect	O
to	O
mi	O
is	O
equivalent	B
to	O
minimising	O
(	O
cid:27	O
)	O
pold	O
(	O
i|xn	O
)	O
(	O
xn	O
−	O
mi	O
)	O
t	O
s−1	O
i	O
(	O
xn	O
−	O
mi	O
)	O
diﬀerentiating	O
with	O
respect	O
to	O
mi	O
and	O
equating	O
to	O
zero	O
we	O
have	O
(	O
xn	O
−	O
mi	O
)	O
=	O
0	O
hence	O
,	O
optimally	O
,	O
−2	O
mi	O
=	O
i	O
n=1	O
n	O
(	O
cid:88	O
)	O
pold	O
(	O
i|xn	O
)	O
s−1	O
(	O
cid:80	O
)	O
n	O
(	O
cid:80	O
)	O
n	O
n=1	O
pold	O
(	O
i|xn	O
)	O
xn	O
n=1	O
pold	O
(	O
i|xn	O
)	O
(	O
cid:80	O
)	O
n	O
pold	O
(	O
i|xn	O
)	O
n=1	O
pold	O
(	O
i|xn	O
)	O
pold	O
(	O
n|i	O
)	O
≡	O
n	O
(	O
cid:88	O
)	O
n=1	O
mi	O
=	O
pold	O
(	O
n|i	O
)	O
xn	O
by	O
deﬁning	O
the	O
membership	O
distribution	B
h	O
(	O
cid:88	O
)	O
i=1	O
n	O
(	O
cid:88	O
)	O
h	O
(	O
cid:88	O
)	O
n=1	O
i=1	O
n	O
(	O
cid:88	O
)	O
(	O
cid:68	O
)	O
n=1	O
n	O
(	O
cid:88	O
)	O
n=1	O
(	O
20.3.5	O
)	O
(	O
20.3.6	O
)	O
(	O
20.3.7	O
)	O
(	O
20.3.8	O
)	O
(	O
20.3.9	O
)	O
(	O
20.3.10	O
)	O
(	O
20.3.11	O
)	O
(	O
20.3.12	O
)	O
(	O
20.3.13	O
)	O
(	O
20.3.14	O
)	O
371	O
which	O
quantiﬁes	O
the	O
membership	O
of	O
datapoints	O
to	O
cluster	O
i	O
,	O
we	O
can	O
write	O
equation	B
(	O
20.3.8	O
)	O
more	O
compactly	O
as	O
m-step	O
:	O
optimal	O
si	O
optimising	O
equation	B
(	O
20.3.5	O
)	O
with	O
respect	O
to	O
si	O
is	O
equivalent	B
to	O
minimising	O
i	O
−	O
log	O
det	O
(	O
cid:0	O
)	O
s−1	O
i	O
pold	O
(	O
i|xn	O
)	O
(	O
cid:1	O
)	O
(	O
cid:69	O
)	O
(	O
cid:33	O
)	O
(	O
∆n	O
i	O
∆n	O
i	O
)	O
ts−1	O
n	O
(	O
cid:88	O
)	O
s−1	O
where	O
∆n	O
(	O
cid:32	O
)	O
i	O
≡	O
xn	O
−	O
mi	O
.	O
to	O
aid	O
the	O
matrix	B
calculus	O
,	O
we	O
isolate	O
the	O
dependency	O
on	O
si	O
to	O
give	O
−	O
log	O
det	O
(	O
cid:0	O
)	O
s−1	O
i	O
(	O
cid:1	O
)	O
n	O
(	O
cid:88	O
)	O
n=1	O
pold	O
(	O
i|xn	O
)	O
i	O
trace	O
pold	O
(	O
i|xn	O
)	O
∆n	O
diﬀerentiating	O
with	O
respect	O
to	O
s−1	O
n=1	O
i	O
i	O
(	O
∆n	O
i	O
)	O
t	O
and	O
equating	O
to	O
zero	O
,	O
we	O
obtain	O
n	O
(	O
cid:88	O
)	O
n=1	O
i	O
(	O
∆n	O
i	O
)	O
t	O
−	O
si	O
pold	O
(	O
i|xn	O
)	O
=	O
0	O
pold	O
(	O
i|xn	O
)	O
∆n	O
n	O
(	O
cid:88	O
)	O
using	O
the	O
membership	O
pold	O
(	O
n|i	O
)	O
,	O
the	O
optimal	O
si	O
is	O
given	O
by	O
si	O
=	O
n=1	O
pold	O
(	O
n|i	O
)	O
(	O
xn	O
−	O
mi	O
)	O
(	O
xn	O
−	O
mi	O
)	O
t	O
draft	O
march	O
9	O
,	O
2010	O
the	O
gaussian	O
mixture	B
model	I
figure	O
20.7	O
:	O
training	B
a	O
mixture	O
of	O
10	O
isotropic	B
gaussians	O
(	O
a	O
)	O
:	O
if	O
we	O
start	O
with	O
large	O
variances	O
for	O
the	O
gaussians	O
,	O
even	O
after	O
one	O
iteration	B
,	O
the	O
gaussians	O
are	O
centred	O
close	O
to	O
the	O
mean	B
of	O
the	O
(	O
b	O
)	O
:	O
the	O
gaussians	O
begin	O
to	O
separate	O
data	B
.	O
(	O
c	O
)	O
:	O
one	O
by	O
one	O
,	O
the	O
gaussians	O
move	O
towards	O
appropriate	O
parts	O
of	O
the	O
data	B
(	O
d	O
)	O
:	O
the	O
ﬁnal	O
converged	O
solution	O
.	O
the	O
gaussians	O
are	O
con-	O
strained	O
to	O
have	O
variances	O
greater	O
than	O
a	O
set	O
amount	O
.	O
see	O
demogmmem.m	O
.	O
(	O
a	O
)	O
1	O
iteration	O
(	O
b	O
)	O
50	O
iterations	O
(	O
c	O
)	O
125	O
iterations	O
(	O
d	O
)	O
150	O
iterations	O
this	O
ensures	O
that	O
si	O
is	O
symmetric	O
positive	O
semi-deﬁnite	O
.	O
a	O
special	O
case	O
is	O
to	O
constrain	O
the	O
covariances	O
si	O
to	O
be	O
diagonal	O
for	O
which	O
the	O
update	O
is	O
,	O
see	O
exercise	O
(	O
201	O
)	O
,	O
n	O
(	O
cid:88	O
)	O
n=1	O
si	O
=	O
pold	O
(	O
n|i	O
)	O
diag	O
(	O
cid:16	O
)	O
(	O
xn	O
−	O
mi	O
)	O
(	O
xn	O
−	O
mi	O
)	O
t	O
(	O
cid:17	O
)	O
where	O
above	O
diag	O
(	O
m	O
)	O
means	O
forming	O
a	O
new	O
matrix	B
from	O
the	O
matrix	B
m	O
with	O
zero	O
entries	O
except	O
for	O
the	O
diagonal	O
entries	O
of	O
m.	O
a	O
more	O
extreme	O
case	O
is	O
that	O
of	O
isotropic	B
gaussians	O
si	O
=	O
σ2	O
i	O
i.	O
the	O
reader	O
may	O
show	O
that	O
the	O
optimal	O
update	O
for	O
σ2	O
in	O
this	O
case	O
is	O
given	O
by	O
taking	O
the	O
average	B
of	O
the	O
diagonal	O
entries	O
of	O
the	O
i	O
diagonally	O
constrained	O
covariance	O
update	O
,	O
pold	O
(	O
n|i	O
)	O
(	O
xn	O
−	O
mi	O
)	O
2	O
(	O
20.3.16	O
)	O
(	O
20.3.15	O
)	O
(	O
20.3.17	O
)	O
(	O
20.3.18	O
)	O
(	O
20.3.19	O
)	O
if	O
no	O
constraint	O
is	O
placed	O
on	O
the	O
weights	O
,	O
the	O
update	O
follows	O
the	O
general	O
formula	O
given	O
in	O
equation	B
(	O
20.2.7	O
)	O
,	O
n	O
(	O
cid:88	O
)	O
n=1	O
σ2	O
i	O
=	O
1	O
dim	O
x	O
m-step	O
:	O
optimal	O
mixture	B
coeﬃcients	O
n	O
(	O
cid:88	O
)	O
n=1	O
pold	O
(	O
i|xn	O
)	O
p	O
(	O
i	O
)	O
=	O
1	O
n	O
e-step	O
explicitly	O
,	O
this	O
is	O
given	O
by	O
the	O
responsibility	B
pold	O
(	O
i|xn	O
)	O
∝	O
pold	O
(	O
xn|i	O
)	O
p	O
(	O
i	O
)	O
(	O
cid:110	O
)	O
(	O
cid:110	O
)	O
2	O
(	O
xn	O
−	O
mi	O
)	O
t	O
s−1	O
−	O
1	O
2	O
(	O
xn	O
−	O
mi	O
(	O
cid:48	O
)	O
)	O
t	O
s−1	O
−	O
1	O
p	O
(	O
i	O
)	O
exp	O
i	O
(	O
cid:48	O
)	O
p	O
(	O
i	O
(	O
cid:48	O
)	O
)	O
exp	O
pold	O
(	O
i|xn	O
)	O
=	O
(	O
cid:80	O
)	O
i	O
(	O
cid:111	O
)	O
(	O
cid:111	O
)	O
(	O
xn	O
−	O
mi	O
)	O
i	O
(	O
cid:48	O
)	O
(	O
xn	O
−	O
mi	O
(	O
cid:48	O
)	O
)	O
the	O
above	O
equations	O
(	O
20.3.8,20.3.14,20.3.17,20.3.19	O
)	O
are	O
iterated	O
until	O
convergence	O
.	O
the	O
performance	B
of	O
em	O
for	O
gaussian	O
mixtures	O
can	O
be	O
strongly	O
dependent	O
on	O
the	O
initialisation	O
,	O
which	O
we	O
discuss	O
below	O
.	O
in	O
addition	O
,	O
constraints	O
on	O
the	O
covariance	B
matrix	O
are	O
required	O
in	O
order	O
to	O
ﬁnd	O
sensible	O
solutions	O
.	O
372	O
draft	O
march	O
9	O
,	O
2010	O
the	O
gaussian	O
mixture	B
model	I
20.3.2	O
practical	O
issues	O
inﬁnite	O
troubles	O
a	O
diﬃculty	O
arises	O
with	O
using	O
maximum	B
likelihood	I
to	O
ﬁt	O
a	O
gaussian	O
mixture	B
model	I
.	O
consider	O
placing	O
a	O
component	O
p	O
(	O
x|mi	O
,	O
si	O
)	O
with	O
mean	O
mi	O
set	O
to	O
one	O
of	O
the	O
datapoints	O
mi	O
=	O
xn	O
.	O
the	O
contribution	O
from	O
that	O
gaussian	O
will	O
be	O
p	O
(	O
xn|mi	O
,	O
si	O
)	O
=	O
1	O
(	O
cid:112	O
)	O
det	O
(	O
2πsi	O
)	O
2	O
(	O
xn−xn	O
)	O
ts−1	O
−	O
1	O
e	O
i	O
(	O
xn−xn	O
)	O
=	O
1	O
(	O
cid:112	O
)	O
det	O
(	O
2πsi	O
)	O
(	O
20.3.20	O
)	O
in	O
the	O
limit	O
that	O
the	O
‘	O
width	O
’	O
of	O
the	O
covariance	B
goes	O
to	O
zero	O
(	O
the	O
eigenvalues	O
of	O
si	O
tend	O
to	O
zero	O
)	O
,	O
this	O
probability	B
density	O
becomes	O
inﬁnite	O
.	O
this	O
means	O
that	O
one	O
can	O
obtain	O
a	O
maximum	B
likelihood	I
solution	O
by	O
placing	O
zero-width	O
gaussians	O
on	O
a	O
selection	O
of	O
the	O
datapoints	O
,	O
resulting	O
in	O
an	O
inﬁnite	O
likelihood	O
.	O
this	O
is	O
clearly	O
undesirable	O
and	O
arises	O
because	O
,	O
in	O
this	O
case	O
,	O
the	O
maximum	B
likelihood	I
solution	O
does	O
not	O
constrain	O
the	O
parameters	O
in	O
a	O
sensible	O
way	O
.	O
note	O
that	O
this	O
is	O
not	O
related	O
to	O
the	O
em	O
algorithm	B
,	O
but	O
a	O
property	O
of	O
the	O
maximum	B
likelihood	I
method	O
itself	O
.	O
all	O
computational	O
methods	O
which	O
aim	O
to	O
ﬁt	O
unconstrained	O
mixtures	O
of	O
gaussians	O
using	O
maximum	B
likelihood	I
therefore	O
succeed	O
in	O
ﬁnding	O
‘	O
reasonable	O
’	O
solutions	O
merely	O
by	O
getting	O
trapped	O
in	O
favourable	O
local	B
maxima	O
.	O
a	O
remedy	O
is	O
to	O
include	O
an	O
additional	O
constraint	O
on	O
the	O
width	O
of	O
the	O
gaussians	O
,	O
ensuring	O
that	O
they	O
can	O
not	O
become	O
too	O
small	O
.	O
one	O
approach	B
is	O
to	O
monitor	O
the	O
eigenvalues	O
of	O
each	O
covariance	B
matrix	O
and	O
if	O
an	O
update	O
would	O
result	O
in	O
a	O
new	O
eigenvalue	O
smaller	O
than	O
a	O
desired	O
threshold	O
,	O
the	O
update	O
is	O
rejected	O
.	O
in	O
gmmem.m	O
we	O
use	O
a	O
similar	O
approach	B
in	O
which	O
we	O
constrain	O
the	O
determinant	B
(	O
the	O
product	O
of	O
the	O
eigenvalues	O
)	O
of	O
the	O
covariances	O
to	O
be	O
greater	O
than	O
a	O
desired	O
speciﬁed	O
minimum	O
value	O
.	O
one	O
can	O
view	O
the	O
formal	O
failure	O
of	O
maximum	B
likelihood	I
in	O
the	O
case	O
of	O
gaussian	O
mixtures	O
as	O
a	O
result	O
of	O
an	O
inappropriate	O
prior	B
.	O
maximum	B
likelihood	I
is	O
equivalent	B
to	O
map	B
in	O
which	O
a	O
ﬂat	O
prior	B
is	O
placed	O
on	O
each	O
matrix	B
si	O
.	O
this	O
is	O
unreasonable	O
since	O
the	O
matrices	O
are	O
required	O
to	O
be	O
positive	B
deﬁnite	I
and	O
of	O
non-vanishing	O
width	O
.	O
a	O
bayesian	O
solution	O
to	O
this	O
problem	B
is	O
possible	O
,	O
placing	O
a	O
prior	B
on	O
covariance	B
matrices	O
.	O
the	O
natural	B
prior	O
in	O
this	O
case	O
is	O
the	O
wishart	O
distribution	B
,	O
or	O
a	O
gamma	B
distribution	O
in	O
the	O
case	O
of	O
a	O
diagonal	O
covariance	B
.	O
initialisation	O
a	O
useful	O
intialisation	O
strategy	O
is	O
to	O
set	O
the	O
covariances	O
to	O
be	O
diagonal	O
with	O
large	O
variances	O
.	O
this	O
gives	O
the	O
components	O
a	O
chance	O
to	O
‘	O
sense	O
’	O
where	O
data	B
lies	O
.	O
an	O
illustration	O
of	O
the	O
performance	B
of	O
the	O
algorithm	B
is	O
given	O
in	O
ﬁg	O
(	O
20.7	O
)	O
.	O
symmetry	B
breaking	I
if	O
the	O
covariances	O
are	O
initialised	O
to	O
large	O
values	O
,	O
the	O
em	O
algorithm	B
appears	O
to	O
make	O
little	O
progress	O
in	O
the	O
ﬁrst	O
iterations	O
as	O
each	O
component	O
jostles	O
with	O
the	O
others	O
to	O
try	O
to	O
explain	O
the	O
data	B
.	O
eventually	O
one	O
gaussian	O
component	O
breaks	O
away	O
and	O
takes	O
responsibility	B
for	O
explaining	O
the	O
data	B
in	O
its	O
vicinity	O
,	O
see	O
ﬁg	O
(	O
20.7	O
)	O
.	O
the	O
origin	O
of	O
this	O
jostling	O
is	O
an	O
inherent	O
symmetry	O
in	O
the	O
solution	O
–	O
it	O
makes	O
no	O
diﬀerence	O
to	O
the	O
likelihood	B
if	O
we	O
relabel	O
what	O
the	O
components	O
are	O
called	O
.	O
this	O
permutation	O
symmetry	O
causes	O
initial	O
confusion	O
as	O
to	O
which	O
model	B
should	O
explain	O
which	O
parts	O
of	O
the	O
data	B
.	O
eventually	O
,	O
this	O
symmetry	O
is	O
broken	O
,	O
and	O
a	O
local	B
solution	O
is	O
found	O
.	O
the	O
symmetries	O
can	O
severely	O
handicap	O
em	O
in	O
ﬁtting	O
a	O
large	O
number	O
of	O
models	O
in	O
the	O
mixture	B
since	O
the	O
number	O
of	O
permutations	O
increases	O
dramatically	O
with	O
the	O
number	O
of	O
components	O
.	O
a	O
heuristic	O
is	O
to	O
begin	O
with	O
a	O
small	O
number	O
of	O
components	O
,	O
say	O
two	O
,	O
for	O
which	O
symmetry	B
breaking	I
is	O
less	O
problematic	O
.	O
once	O
a	O
local	B
broken	O
solution	O
has	O
been	O
found	O
,	O
more	O
models	O
are	O
included	O
into	O
the	O
mixture	B
,	O
initialised	O
close	O
to	O
the	O
currently	O
found	O
solutions	O
.	O
in	O
this	O
way	O
,	O
a	O
hierarchical	O
breaking	O
scheme	O
is	O
envisaged	O
.	O
another	O
popular	O
method	O
for	O
initialisation	O
is	O
to	O
center	O
the	O
means	O
to	O
those	O
found	O
by	O
the	O
k-means	B
algorithm	O
–	O
however	O
,	O
this	O
itself	O
requires	O
a	O
heuristic	O
initialisation	O
.	O
20.3.3	O
classiﬁcation	B
using	O
gaussian	O
mixture	B
models	O
consider	O
data	B
drawn	O
from	O
two	O
classes	O
,	O
c	O
∈	O
{	O
1	O
,	O
2	O
}	O
.	O
we	O
can	O
ﬁt	O
a	O
gmm	O
p	O
(	O
x|c	O
=	O
1	O
,	O
x1	O
)	O
to	O
the	O
data	B
x1	O
from	O
class	O
1	O
,	O
and	O
another	O
gmm	O
p	O
(	O
x|c	O
=	O
2	O
,	O
x2	O
)	O
to	O
the	O
data	B
x2	O
from	O
class	O
2.	O
this	O
gives	O
rise	O
to	O
two	O
class-conditional	O
gmms	O
,	O
h	O
(	O
cid:88	O
)	O
i=1	O
p	O
(	O
x|c	O
,	O
xc	O
)	O
=	O
p	O
(	O
i|c	O
)	O
n	O
(	O
x	O
mc	O
i	O
,	O
sc	O
i	O
)	O
draft	O
march	O
9	O
,	O
2010	O
(	O
20.3.21	O
)	O
373	O
the	O
gaussian	O
mixture	B
model	I
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
20.8	O
:	O
(	O
a	O
)	O
:	O
a	O
gaussian	O
mixture	B
model	I
with	O
h	O
=	O
4	O
components	O
.	O
there	O
is	O
a	O
component	O
(	O
purple	O
)	O
with	O
large	O
variance	B
and	O
small	O
weight	B
that	O
has	O
little	O
eﬀect	O
on	O
the	O
distribution	B
close	O
to	O
where	O
the	O
other	O
three	O
components	O
have	O
appreciable	O
mass	O
.	O
as	O
we	O
move	O
further	O
away	O
this	O
additional	O
component	O
gains	O
in	O
inﬂuence	O
.	O
(	O
b	O
)	O
:	O
the	O
gmm	O
probability	B
density	O
function	B
from	O
(	O
a	O
)	O
.	O
(	O
c	O
)	O
:	O
plotted	O
on	O
a	O
log	O
scale	O
,	O
the	O
inﬂuence	O
of	O
each	O
gaussian	O
far	O
from	O
the	O
origin	O
becomes	O
clearer	O
.	O
for	O
a	O
novel	O
point	O
x∗	O
,	O
the	O
posterior	B
class	O
probability	B
is	O
p	O
(	O
c|x∗	O
,	O
x	O
)	O
∝	O
p	O
(	O
x∗	O
|c	O
,	O
xc	O
)	O
p	O
(	O
c	O
)	O
(	O
20.3.22	O
)	O
where	O
p	O
(	O
c	O
)	O
is	O
the	O
prior	B
class	O
probability	B
.	O
the	O
maximum	B
likelihood	I
setting	O
is	O
that	O
p	O
(	O
c	O
)	O
is	O
proportional	O
to	O
the	O
number	O
of	O
training	B
points	O
in	O
class	O
c.	O
consider	O
a	O
testpoint	O
x∗	O
a	O
long	O
way	O
from	O
the	O
training	B
data	O
for	O
both	O
classes	O
.	O
for	O
such	O
a	O
point	O
,	O
the	O
proba-	O
bility	O
that	O
either	O
of	O
the	O
two	O
class	O
models	O
generated	O
the	O
data	B
is	O
very	O
low	O
.	O
however	O
,	O
one	O
will	O
be	O
much	O
lower	O
than	O
the	O
other	O
(	O
since	O
gaussians	O
drop	O
exponentially	O
quickly	O
)	O
,	O
meaning	O
that	O
the	O
posterior	B
probability	O
will	O
be	O
conﬁdently	O
close	O
to	O
1	O
for	O
that	O
class	O
which	O
has	O
a	O
component	O
closest	O
to	O
x∗	O
.	O
this	O
is	O
an	O
unfortunate	O
prop-	O
erty	O
since	O
we	O
would	O
end	O
up	O
conﬁdently	O
predicting	O
the	O
class	O
of	O
novel	O
data	B
that	O
is	O
not	O
similar	O
to	O
anything	O
we	O
’	O
ve	O
seen	O
before	O
.	O
we	O
would	O
prefer	O
the	O
opposite	O
eﬀect	O
that	O
for	O
novel	O
data	B
far	O
from	O
the	O
training	B
data	O
,	O
the	O
classiﬁcation	B
conﬁdence	O
drops	O
and	O
all	O
classes	O
become	O
equally	O
likely	O
.	O
a	O
remedy	O
for	O
this	O
situation	O
is	O
to	O
include	O
an	O
additional	O
component	O
in	O
the	O
gaussian	O
mixture	B
for	O
each	O
class	O
that	O
is	O
very	O
broad	O
.	O
we	O
ﬁrst	O
collect	O
the	O
input	O
data	B
from	O
all	O
classes	O
into	O
a	O
dataset	O
x	O
,	O
and	O
let	O
m	O
be	O
the	O
mean	B
of	O
all	O
this	O
data	B
and	O
s	O
the	O
covariance	B
.	O
then	O
for	O
the	O
model	B
of	O
each	O
class	O
c	O
data	B
we	O
include	O
an	O
additional	O
gaussian	O
(	O
dropping	O
the	O
notational	O
dependency	O
on	O
x	O
)	O
p	O
(	O
x|c	O
)	O
=	O
in	O
(	O
x	O
mc	O
˜pc	O
i	O
,	O
sc	O
i	O
)	O
+	O
˜pc	O
h+1n	O
(	O
x	O
m	O
,	O
λs	O
)	O
h	O
(	O
cid:88	O
)	O
(	O
cid:26	O
)	O
pc	O
i=1	O
i	O
δ	O
where	O
˜pc	O
i	O
∝	O
i	O
≤	O
h	O
i	O
=	O
h	O
+	O
1	O
(	O
20.3.23	O
)	O
(	O
20.3.24	O
)	O
where	O
δ	O
is	O
a	O
small	O
positive	O
value	O
and	O
λ	O
inﬂates	O
the	O
covariance	B
(	O
we	O
take	O
δ	O
=	O
0.0001	O
and	O
λ	O
=	O
10	O
in	O
demogmmclass.m	O
)	O
.	O
the	O
eﬀect	O
of	O
the	O
additional	O
component	O
on	O
the	O
training	B
likelihood	O
is	O
negligible	O
since	O
it	O
has	O
small	O
weight	B
and	O
large	O
variance	B
compared	O
to	O
the	O
other	O
components	O
,	O
see	O
ﬁg	O
(	O
20.8	O
)	O
.	O
however	O
,	O
as	O
we	O
move	O
away	O
from	O
the	O
region	O
where	O
the	O
ﬁrst	O
h	O
components	O
have	O
appreciable	O
mass	O
,	O
the	O
additional	O
component	O
gains	O
in	O
inﬂuence	O
since	O
it	O
has	O
a	O
higher	O
variance	B
.	O
if	O
we	O
include	O
the	O
same	O
additional	O
component	O
in	O
the	O
gmm	O
for	O
each	O
class	O
c	O
then	O
the	O
inﬂuence	O
of	O
this	O
additional	O
component	O
will	O
be	O
the	O
same	O
for	O
each	O
class	O
,	O
dominating	O
as	O
we	O
move	O
far	O
from	O
the	O
inﬂuence	O
of	O
the	O
other	O
components	O
.	O
for	O
a	O
point	O
far	O
from	O
the	O
training	B
data	O
the	O
likelihood	B
will	O
be	O
roughly	O
equal	O
for	O
each	O
class	O
since	O
in	O
this	O
region	O
the	O
additional	O
broad	O
component	O
dominates	O
with	O
equal	O
measure	O
.	O
the	O
posterior	B
distribution	O
will	O
then	O
tend	O
to	O
the	O
prior	B
class	O
probability	B
p	O
(	O
c	O
)	O
,	O
mitigating	O
the	O
deleterious	O
eﬀect	O
of	O
a	O
single	O
gmm	O
dominating	O
when	O
a	O
testpoint	O
is	O
far	O
from	O
the	O
training	B
data	O
.	O
374	O
draft	O
march	O
9	O
,	O
2010	O
−10−8−6−4−2024681000.050.10.150.20.25−10−8−6−4−2024681000.050.10.150.20.25−10−8−6−4−20246810−180−160−140−120−100−80−60−40−200	O
the	O
gaussian	O
mixture	B
model	I
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
20.9	O
:	O
class	O
conditional	B
gmm	O
train-	O
ing	O
and	O
classiﬁcation	B
.	O
(	O
a	O
)	O
:	O
data	B
from	O
two	O
diﬀerent	O
classes	O
.	O
we	O
ﬁt	O
a	O
gmm	O
with	O
two	O
components	O
to	O
the	O
data	B
from	O
each	O
class	O
.	O
the	O
(	O
magenta	O
)	O
diamond	O
is	O
a	O
test	O
point	O
far	O
from	O
the	O
training	B
data	O
we	O
will	O
clas-	O
(	O
b	O
)	O
:	O
upper	O
subpanel	O
are	O
the	O
class	O
sify	O
.	O
probabilities	O
p	O
(	O
c	O
=	O
1|n	O
)	O
for	O
the	O
40	O
train-	O
ing	O
points	O
,	O
and	O
the	O
41st	O
point	O
,	O
being	O
the	O
test	O
point	O
.	O
the	O
lower	O
subpanel	O
are	O
the	O
class	O
probabilities	O
but	O
including	O
the	O
addi-	O
tional	O
large	O
variance	B
gaussian	O
term	O
.	O
see	O
demogmmclass.m	O
.	O
example	O
89.	O
the	O
data	B
in	O
ﬁg	O
(	O
20.9a	O
)	O
has	O
a	O
cluster	O
structure	B
for	O
each	O
class	O
.	O
based	O
on	O
ﬁtting	O
a	O
gmm	O
to	O
each	O
of	O
the	O
two	O
classes	O
,	O
a	O
test	O
point	O
(	O
diamond	O
)	O
far	O
from	O
the	O
training	B
data	O
is	O
conﬁdently	O
classiﬁed	O
as	O
belonging	O
to	O
class	O
1.	O
this	O
is	O
an	O
undesired	O
eﬀect	O
since	O
we	O
would	O
prefer	O
that	O
points	O
far	O
from	O
the	O
training	B
data	O
are	O
not	O
classiﬁed	O
with	O
any	O
certainty	O
.	O
by	O
including	O
an	O
additional	O
large	O
variance	B
gaussian	O
component	O
for	O
each	O
class	O
this	O
has	O
little	O
eﬀect	O
on	O
the	O
class	O
probabilities	O
of	O
the	O
training	B
data	O
,	O
yet	O
has	O
the	O
desired	O
eﬀect	O
of	O
making	O
the	O
class	O
probability	B
for	O
the	O
test	O
point	O
maximally	O
uncertain	B
,	O
ﬁg	O
(	O
20.9b	O
)	O
.	O
20.3.4	O
the	O
parzen	O
estimator	B
the	O
parzen	O
density	B
estimator	O
is	O
formed	O
by	O
placing	O
a	O
‘	O
bump	O
of	O
mass	O
’	O
,	O
ρ	O
(	O
x|xn	O
)	O
,	O
on	O
each	O
datapoint	O
,	O
a	O
popular	O
choice	O
is	O
(	O
for	O
a	O
d	O
dimensional	O
x	O
)	O
n=1	O
1	O
n	O
p	O
(	O
x	O
)	O
=	O
ρ	O
(	O
x|xn	O
)	O
n	O
(	O
cid:88	O
)	O
(	O
cid:0	O
)	O
x	O
xn	O
,	O
σ2id	O
ρ	O
(	O
x|xn	O
)	O
=	O
n	O
n	O
(	O
cid:88	O
)	O
(	O
cid:1	O
)	O
p	O
(	O
x	O
)	O
=	O
1	O
n	O
giving	O
the	O
mixture	O
of	O
gaussians	O
1	O
2σ2	O
(	O
x−xn	O
)	O
2	O
−	O
1	O
(	O
2πσ2	O
)	O
d/2	O
e	O
n=1	O
(	O
20.3.25	O
)	O
(	O
20.3.26	O
)	O
(	O
20.3.27	O
)	O
(	O
20.3.28	O
)	O
there	O
is	O
no	O
training	O
required	O
for	O
a	O
parzen	O
estimator	B
–	O
only	O
the	O
positions	O
of	O
the	O
n	O
datapoints	O
need	O
storing	O
.	O
whilst	O
the	O
parzen	O
technique	O
is	O
a	O
reasonable	O
and	O
cheap	O
way	O
to	O
form	O
a	O
density	B
estimator	O
,	O
it	O
does	O
not	O
enable	O
us	O
to	O
form	O
any	O
simpler	O
description	O
of	O
the	O
data	B
.	O
in	O
particular	O
,	O
we	O
can	O
not	O
perform	O
clustering	B
since	O
there	O
is	O
no	O
lower	O
number	O
of	O
clusters	O
assumed	O
to	O
underly	O
the	O
data	B
generating	O
process	O
.	O
this	O
is	O
in	O
contrast	O
to	O
gmms	O
trained	O
using	O
maximum	B
likelihood	I
on	O
a	O
ﬁxed	O
number	O
h	O
≤	O
n	O
of	O
components	O
.	O
20.3.5	O
k-means	B
consider	O
a	O
mixture	O
of	O
k	O
isotropic	B
gaussians	O
in	O
which	O
each	O
covariance	B
is	O
constrained	O
to	O
be	O
equal	O
to	O
σ2i	O
,	O
k	O
(	O
cid:88	O
)	O
i=1	O
p	O
(	O
x	O
)	O
=	O
pin	O
(	O
cid:0	O
)	O
x	O
mi	O
,	O
σ2i	O
(	O
cid:1	O
)	O
whilst	O
the	O
em	O
algorithm	B
breaks	O
down	O
if	O
a	O
gaussian	O
component	O
is	O
allowed	O
to	O
set	O
mi	O
equal	O
to	O
a	O
datapoint	O
with	O
σ2	O
→	O
0	O
,	O
by	O
constraining	O
all	O
components	O
to	O
have	O
the	O
same	O
variance	B
σ2	O
,	O
the	O
algorithm	B
has	O
a	O
well	O
375	O
draft	O
march	O
9	O
,	O
2010	O
−8−6−4−20246810−6−4−2024681012051015202530354000.20.40.60.81051015202530354000.20.40.60.81	O
algorithm	B
19	O
k-means	B
the	O
gaussian	O
mixture	B
model	I
1	O
:	O
initialise	O
the	O
centres	O
mi	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k.	O
2	O
:	O
while	O
not	O
converged	O
do	O
3	O
:	O
4	O
:	O
5	O
:	O
for	O
each	O
centre	O
i	O
,	O
ﬁnd	O
all	O
the	O
xn	O
for	O
which	O
i	O
is	O
the	O
nearest	O
(	O
in	O
euclidean	O
sense	O
)	O
centre	O
.	O
call	O
this	O
set	O
of	O
points	O
ni	O
.	O
let	O
ni	O
be	O
the	O
number	O
of	O
datapoints	O
in	O
set	O
ni	O
.	O
update	O
the	O
means	O
(	O
cid:88	O
)	O
n∈ni	O
xn	O
mnew	O
i	O
=	O
1	O
ni	O
6	O
:	O
end	O
while	O
figure	O
20.10	O
:	O
(	O
a	O
)	O
:	O
550	O
datapoints	O
clustered	O
using	O
k-means	B
with	O
3	O
com-	O
ponents	O
.	O
the	O
means	O
are	O
given	O
by	O
the	O
(	O
b	O
)	O
:	O
evolution	O
of	O
the	O
red	O
crosses	O
.	O
mean	B
square	O
distance	O
to	O
nearest	O
cen-	O
tre	O
with	O
iterations	O
of	O
the	O
algorithm	B
.	O
the	O
means	O
were	O
initialised	O
to	O
close	O
to	O
the	O
overall	O
mean	B
of	O
the	O
data	B
.	O
see	O
demokmeans.m	O
.	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
cid:26	O
)	O
1	O
0	O
deﬁned	O
limit	O
as	O
σ2	O
→	O
0.	O
the	O
reader	O
may	O
show	O
,	O
exercise	O
(	O
202	O
)	O
,	O
that	O
in	O
this	O
case	O
the	O
membership	O
distribution	B
equation	O
(	O
20.3.9	O
)	O
becomes	O
deterministic	B
pold	O
(	O
n|i	O
)	O
∝	O
if	O
mi	O
is	O
closest	O
to	O
xn	O
otherwise	O
(	O
20.3.29	O
)	O
in	O
this	O
limit	O
the	O
em	O
update	O
(	O
20.3.10	O
)	O
for	O
the	O
mean	B
mi	O
is	O
given	O
by	O
taking	O
the	O
average	B
of	O
the	O
points	O
closest	O
to	O
mi	O
.	O
this	O
limiting	O
and	O
constrained	O
gmm	O
then	O
reduces	O
to	O
the	O
so-called	O
k-means	B
algorithm	O
,	O
algorithm	B
(	O
19	O
)	O
.	O
despite	O
its	O
simplicity	O
the	O
k-means	B
algorithm	O
converges	O
quickly	O
and	O
often	O
gives	O
a	O
reasonable	O
clustering	B
,	O
provided	O
the	O
centres	O
are	O
initialised	O
reasonably	O
.	O
see	O
ﬁg	O
(	O
20.10	O
)	O
.	O
k-means	B
is	O
often	O
used	O
as	O
a	O
simple	O
form	O
of	O
data	B
compression	I
.	O
rather	O
than	O
sending	O
the	O
datapoint	O
xn	O
,	O
one	O
sends	O
instead	O
the	O
index	O
of	O
the	O
centre	O
to	O
which	O
it	O
is	O
associated	O
.	O
this	O
is	O
called	O
vector	B
quantisation	I
and	O
is	O
a	O
form	O
of	O
lossy	O
compression	O
.	O
to	O
improve	O
the	O
quality	O
,	O
more	O
information	O
can	O
be	O
transmitted	O
such	O
as	O
an	O
approximation	B
of	O
the	O
diﬀerence	O
between	O
x	O
and	O
the	O
corresponding	O
mean	B
m	O
,	O
which	O
can	O
be	O
used	O
to	O
improve	O
the	O
reconstruction	O
of	O
the	O
compressed	O
datapoint	O
.	O
20.3.6	O
bayesian	O
mixture	B
models	O
bayesian	O
extensions	O
include	O
placing	O
priors	O
on	O
the	O
parameters	O
of	O
each	O
model	B
in	O
the	O
mixture	B
,	O
and	O
also	O
on	O
the	O
component	O
distribution	B
.	O
in	O
most	O
cases	O
this	O
will	O
give	O
rise	O
to	O
the	O
marginal	B
likelihood	I
being	O
an	O
intractable	O
integral	O
.	O
methods	O
that	O
approximate	B
the	O
integral	O
include	O
sampling	B
techniques	O
[	O
98	O
]	O
.	O
see	O
also	O
[	O
109	O
,	O
67	O
]	O
for	O
an	O
approximate	B
variational	O
treatment	O
focussed	O
on	O
bayesian	O
gaussian	O
mixture	B
models	O
.	O
20.3.7	O
semi-supervised	B
learning	I
in	O
some	O
cases	O
we	O
may	O
know	O
to	O
which	O
mixture	B
component	O
certain	O
datapoints	O
belong	O
.	O
given	O
this	O
information	O
we	O
want	O
to	O
ﬁt	O
a	O
mixture	B
model	I
with	O
a	O
speciﬁed	O
number	O
of	O
components	O
h	O
and	O
parameters	O
θ.	O
we	O
write	O
(	O
vm∗	O
,	O
hm∗	O
)	O
,	O
m	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
for	O
the	O
m	O
known	O
datapoints	O
and	O
corresponding	O
components	O
,	O
and	O
(	O
vn	O
,	O
hn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
for	O
the	O
remaining	O
datapoints	O
whose	O
components	O
hn	O
are	O
unknown	O
.	O
we	O
aim	O
then	O
to	O
maximise	O
the	O
376	O
draft	O
march	O
9	O
,	O
2010	O
−8−6−4−202468−4−20246811.522.533.544.550510152025	O
mixture	B
of	I
experts	I
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
xn	O
yn	O
hn	O
w	O
u	O
figure	O
20.11	O
:	O
mixture	B
of	I
experts	I
model	O
.	O
the	O
prediction	B
of	O
the	O
output	O
yn	O
(	O
real	O
or	O
continuous	B
)	O
given	O
the	O
input	O
xn	O
aver-	O
ages	O
over	O
individual	O
experts	O
p	O
(	O
yn|xn	O
,	O
whn	O
)	O
.	O
the	O
expert	O
hn	O
is	O
selected	O
by	O
the	O
gating	O
mechanism	O
with	O
probability	O
p	O
(	O
hn|xn	O
,	O
u	O
)	O
,	O
so	O
that	O
some	O
experts	O
will	O
be	O
more	O
able	O
to	O
predict	O
the	O
output	O
for	O
xn	O
in	O
‘	O
their	O
’	O
part	O
of	O
the	O
space	O
.	O
the	O
parameters	O
w	O
,	O
u	O
can	O
be	O
learned	O
by	O
maximum	B
likelihood	I
after	O
marginalising	O
over	O
the	O
hidden	B
expert	O
variables	O
.	O
likelihood	B
p	O
(	O
v1	O
:	O
m∗	O
,	O
v1	O
:	O
n|h1	O
:	O
m∗	O
,	O
θ	O
)	O
=	O
(	O
cid:89	O
)	O
m	O
p	O
(	O
vm∗	O
|hm∗	O
,	O
θ	O
)	O
(	O
cid:89	O
)	O
n	O
(	O
cid:88	O
)	O
hn	O
p	O
(	O
vn|hn	O
,	O
θ	O
)	O
p	O
(	O
hn	O
)	O
(	O
20.3.30	O
)	O
if	O
we	O
were	O
to	O
lump	O
all	O
the	O
datapoints	O
together	O
,	O
this	O
is	O
essentially	O
equivalent	B
to	O
the	O
standard	O
unsupervised	O
case	O
,	O
expect	O
that	O
some	O
of	O
the	O
h	O
are	O
ﬁxed	O
into	O
known	O
states	O
.	O
the	O
only	O
eﬀect	O
on	O
the	O
em	O
algorithm	B
is	O
therefore	O
in	O
the	O
terms	O
pold	O
(	O
h|v	O
)	O
which	O
are	O
delta	O
functions	O
in	O
the	O
known	O
state	O
,	O
resulting	O
in	O
a	O
minor	O
modiﬁcation	O
of	O
the	O
standard	O
algorithm	O
,	O
exercise	O
(	O
205	O
)	O
.	O
20.4	O
mixture	B
of	I
experts	I
the	O
mixture	B
of	I
experts	I
model	O
[	O
149	O
]	O
is	O
related	O
to	O
discriminative	B
training	I
of	O
an	O
output	O
y	O
distribution	B
con-	O
ditioned	O
on	O
an	O
input	O
x.	O
this	O
can	O
be	O
used	O
in	O
either	O
the	O
regression	B
of	O
classiﬁcation	B
contexts	O
and	O
has	O
the	O
general	O
form	O
,	O
see	O
ﬁg	O
(	O
20.11	O
)	O
,	O
p	O
(	O
y|x	O
,	O
w	O
,	O
u	O
)	O
=	O
p	O
(	O
y|x	O
,	O
wh	O
)	O
p	O
(	O
h|x	O
,	O
u	O
)	O
,	O
(	O
20.4.1	O
)	O
here	O
h	O
indexes	O
the	O
mixture	B
component	O
.	O
each	O
expert	O
has	O
parameters	O
w	O
=	O
[	O
w1	O
,	O
.	O
.	O
.	O
,	O
wh	O
]	O
and	O
corresponding	O
gating	O
parameters	O
u	O
=	O
[	O
u1	O
,	O
.	O
.	O
.	O
,	O
uh	O
]	O
.	O
unlike	O
a	O
standard	O
mixture	O
model	B
,	O
the	O
component	O
distribution	B
p	O
(	O
h|x	O
,	O
u	O
)	O
is	O
dependent	O
on	O
the	O
input	O
x.	O
this	O
so-called	O
gating	O
distribution	B
is	O
conventionally	O
taken	O
to	O
be	O
of	O
the	O
softmax	B
form	O
h	O
(	O
cid:88	O
)	O
h=1	O
hx	O
(	O
cid:80	O
)	O
p	O
(	O
h|x	O
,	O
u	O
)	O
=	O
eut	O
h	O
eut	O
hx	O
(	O
20.4.2	O
)	O
the	O
idea	O
is	O
that	O
we	O
have	O
a	O
set	O
of	O
h	O
predictive	O
models	O
(	O
experts	O
)	O
,	O
p	O
(	O
y|x	O
,	O
wh	O
)	O
,	O
each	O
with	O
a	O
diﬀerent	O
param-	O
eter	O
wh	O
,	O
h	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
h.	O
how	O
suitable	O
model	B
h	O
is	O
for	O
predicting	O
with	O
the	O
current	O
input	O
x	O
is	O
determined	O
by	O
the	O
alignment	O
of	O
input	O
x	O
with	O
the	O
weight	B
vector	O
uh	O
.	O
in	O
this	O
way	O
the	O
input	O
x	O
is	O
softly	O
assigned	O
to	O
the	O
appropriate	O
experts	O
.	O
maximum	B
likelihood	I
training	O
can	O
be	O
achieved	O
using	O
a	O
form	O
of	O
em	O
.	O
we	O
will	O
not	O
derive	O
the	O
em	O
algorithm	B
for	O
the	O
mixture	B
of	I
experts	I
model	O
in	O
full	O
,	O
merely	O
pointing	O
the	O
direction	O
along	O
which	O
the	O
derivation	O
would	O
continue	O
.	O
for	O
a	O
single	O
datapoint	O
x	O
,	O
the	O
em	O
energy	B
term	O
is	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
y|x	O
,	O
wh	O
)	O
p	O
(	O
h|x	O
,	O
u	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
h|x	O
,	O
wold	O
,	O
uold	O
)	O
for	O
regression	B
a	O
simple	O
choice	O
is	O
(	O
cid:16	O
)	O
y	O
xtwh	O
,	O
σ2	O
(	O
cid:17	O
)	O
p	O
(	O
y|x	O
,	O
wh	O
)	O
=	O
n	O
and	O
for	O
(	O
binary	O
)	O
classiﬁcation	B
p	O
(	O
y	O
=	O
1|x	O
,	O
wh	O
)	O
=	O
σ	O
(	O
xtwh	O
)	O
draft	O
march	O
9	O
,	O
2010	O
(	O
20.4.3	O
)	O
(	O
20.4.4	O
)	O
(	O
20.4.5	O
)	O
377	O
in	O
both	O
cases	O
computing	O
the	O
derivatives	O
of	O
the	O
energy	B
with	O
respect	O
to	O
the	O
parameters	O
w	O
is	O
straightforward	O
,	O
so	O
that	O
an	O
em	O
algorithm	B
is	O
readily	O
available	O
.	O
an	O
alternative	O
to	O
em	O
is	O
to	O
compute	O
the	O
gradient	B
of	O
the	O
likelihood	B
directly	O
using	O
the	O
standard	O
approach	O
discussed	O
in	O
section	O
(	O
11.7	O
)	O
.	O
indicator	O
models	O
a	O
bayesian	O
treatment	O
is	O
to	O
consider	O
(	O
cid:90	O
)	O
(	O
cid:88	O
)	O
p	O
(	O
y|x	O
)	O
=	O
where	O
it	O
is	O
conventional	O
to	O
assume	O
p	O
(	O
w	O
)	O
=	O
(	O
cid:81	O
)	O
p	O
(	O
y|x	O
,	O
wh	O
)	O
p	O
(	O
h|x	O
,	O
u	O
)	O
p	O
(	O
w	O
)	O
p	O
(	O
u	O
)	O
h	O
p	O
(	O
wh	O
)	O
,	O
p	O
(	O
u	O
)	O
=	O
(	O
cid:81	O
)	O
w	O
,	O
u	O
h	O
h	O
p	O
(	O
uh	O
)	O
.	O
the	O
integrals	O
are	O
generally	O
intractable	O
and	O
approximations	O
are	O
required	O
.	O
see	O
[	O
290	O
]	O
for	O
a	O
variational	O
treatment	O
for	O
regression	B
and	O
[	O
43	O
]	O
for	O
a	O
variational	O
treatment	O
of	O
classiﬁcation	B
.	O
an	O
extension	O
to	O
bayesian	O
model	B
selection	I
in	O
which	O
the	O
number	O
of	O
experts	O
is	O
estimated	O
is	O
considered	O
in	O
[	O
143	O
]	O
.	O
(	O
20.4.6	O
)	O
20.5	O
indicator	O
models	O
in	O
the	O
indicator	B
approach	I
we	O
specify	O
a	O
distribution	B
over	O
the	O
cluster	O
assignments	O
.	O
for	O
consistency	O
with	O
the	O
literature	O
we	O
use	O
an	O
indicator	O
z	O
,	O
as	O
opposed	O
to	O
a	O
hidden	B
variable	I
h	O
,	O
although	O
they	O
play	O
the	O
same	O
role	O
.	O
a	O
clustering	B
model	O
with	O
parameters	O
θ	O
on	O
the	O
component	O
models	O
and	O
joint	B
indicator	O
prior	B
p	O
(	O
z1	O
:	O
n	O
)	O
takes	O
the	O
form	O
(	O
20.5.1	O
)	O
(	O
20.5.2	O
)	O
(	O
20.5.3	O
)	O
(	O
20.5.4	O
)	O
p	O
(	O
v1	O
:	O
n|θ	O
)	O
=	O
(	O
cid:88	O
)	O
p	O
(	O
v1	O
:	O
n|θ	O
)	O
=	O
(	O
cid:88	O
)	O
z1	O
:	O
n	O
z1	O
:	O
n	O
p	O
(	O
v1	O
:	O
n|z1	O
:	O
n	O
,	O
θ	O
)	O
p	O
(	O
z1	O
:	O
n	O
)	O
n	O
(	O
cid:89	O
)	O
n=1	O
p	O
(	O
z1	O
:	O
n	O
)	O
p	O
(	O
vn|zn	O
,	O
θ	O
)	O
since	O
the	O
zn	O
indicate	O
cluster	O
membership	O
,	O
below	O
we	O
discuss	O
the	O
role	O
of	O
diﬀerent	O
indicator	O
priors	O
p	O
(	O
z1	O
:	O
n	O
)	O
in	O
clustering	B
.	O
20.5.1	O
joint	B
indicator	O
approach	B
:	O
factorised	B
prior	O
assuming	O
prior	B
independence	O
of	O
indicators	O
,	O
n	O
(	O
cid:89	O
)	O
n=1	O
p	O
(	O
z1	O
:	O
n	O
)	O
=	O
p	O
(	O
zn	O
)	O
,	O
we	O
obtain	O
from	O
equation	B
(	O
20.5.2	O
)	O
p	O
(	O
v1	O
:	O
n|θ	O
)	O
=	O
(	O
cid:88	O
)	O
n	O
(	O
cid:89	O
)	O
z1	O
:	O
n	O
n=1	O
zn	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
n	O
(	O
cid:89	O
)	O
(	O
cid:88	O
)	O
p	O
(	O
vn|zn	O
,	O
θ	O
)	O
p	O
(	O
zn	O
)	O
=	O
p	O
(	O
vn|zn	O
,	O
θ	O
)	O
p	O
(	O
zn	O
)	O
n=1	O
zn	O
which	O
recovers	O
the	O
standard	O
mixture	O
model	B
equation	O
(	O
20.1.2	O
)	O
.	O
as	O
we	O
discuss	O
below	O
,	O
more	O
sophisticated	O
joint	B
indicator	O
priors	O
can	O
be	O
used	O
to	O
explicitly	O
control	O
the	O
complexity	O
of	O
the	O
indicator	O
assignments	O
and	O
open	O
the	O
path	B
to	O
essentially	O
‘	O
inﬁnite	O
dimensional	O
’	O
models	O
.	O
20.5.2	O
joint	B
indicator	O
approach	B
:	O
polya	O
prior	B
for	O
a	O
large	O
number	O
of	O
available	O
clusters	O
(	O
mixture	B
components	O
)	O
k	O
(	O
cid:29	O
)	O
1	O
,	O
using	O
a	O
factorised	B
joint	O
indicator	O
distribution	O
could	O
potentially	O
lead	O
to	O
overﬁtting	B
,	O
resulting	O
in	O
little	O
or	O
no	O
meaningful	O
clustering	B
.	O
one	O
way	O
to	O
control	O
the	O
eﬀective	O
number	O
of	O
components	O
that	O
are	O
used	O
is	O
via	O
a	O
parameter	B
π	O
that	O
regulates	O
the	O
complexity	O
,	O
(	O
cid:90	O
)	O
(	O
cid:40	O
)	O
(	O
cid:89	O
)	O
π	O
n	O
(	O
cid:41	O
)	O
p	O
(	O
zn|π	O
)	O
p	O
(	O
z1	O
:	O
n	O
)	O
=	O
378	O
p	O
(	O
π	O
)	O
(	O
20.5.5	O
)	O
draft	O
march	O
9	O
,	O
2010	O
indicator	O
models	O
z1	O
v1	O
zn	O
vn	O
z1	O
v1	O
θ	O
(	O
a	O
)	O
zn	O
vn	O
π	O
θ	O
(	O
b	O
)	O
π	O
zn	O
vn	O
θ	O
(	O
c	O
)	O
n	O
figure	O
20.12	O
:	O
(	O
a	O
)	O
:	O
a	O
generic	O
mixture	B
model	I
for	O
data	B
v1	O
:	O
n	O
.	O
each	O
zn	O
indicates	O
the	O
cluster	O
of	O
each	O
datapoint	O
.	O
θ	O
is	O
a	O
set	O
of	O
parameters	O
and	O
zn	O
=	O
k	O
selects	O
parameter	B
θk	O
for	O
datapoint	O
vn	O
.	O
(	O
b	O
)	O
:	O
for	O
a	O
potentially	O
large	O
number	O
of	O
clusters	O
one	O
way	O
to	O
control	O
complexity	O
is	O
to	O
constrain	O
the	O
joint	B
(	O
c	O
)	O
:	O
plate	B
notation	O
of	O
indicator	O
distribution	O
.	O
(	O
b	O
)	O
.	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
20.13	O
:	O
the	O
number	O
of	O
unique	O
clusters	O
u	O
when	O
indicators	O
are	O
sampled	O
from	O
a	O
polya	O
distribution	B
(	O
a	O
)	O
:	O
k	O
=	O
50	O
,	O
(	O
b	O
)	O
:	O
k	O
=	O
100	O
,	O
(	O
c	O
)	O
:	O
k	O
=	O
1000.	O
equation	B
(	O
20.5.8	O
)	O
,	O
with	O
α	O
=	O
2	O
,	O
and	O
n	O
=	O
50	O
datapoints	O
.	O
even	O
though	O
the	O
number	O
of	O
available	O
clusters	O
k	O
is	O
larger	O
than	O
the	O
number	O
of	O
datapoints	O
,	O
the	O
eﬀective	O
number	O
of	O
used	O
clusters	O
remains	O
constrained	O
.	O
see	O
demopolya.m	O
.	O
where	O
p	O
(	O
z|π	O
)	O
is	O
a	O
categorical	O
distribution	B
,	O
p	O
(	O
zn	O
=	O
k|π	O
)	O
=	O
πk	O
(	O
20.5.6	O
)	O
a	O
convenient	O
choice	O
for	O
p	O
(	O
π	O
)	O
is	O
the	O
dirichlet	O
distribution	B
(	O
since	O
this	O
is	O
conjugate	B
to	O
the	O
categorical	O
distri-	O
bution	O
)	O
,	O
α/k−1	O
π	O
k	O
(	O
20.5.7	O
)	O
k	O
(	O
cid:89	O
)	O
k=1	O
p	O
(	O
π	O
)	O
=	O
dirichlet	O
(	O
π|α	O
)	O
∝	O
k	O
(	O
cid:89	O
)	O
γ	O
(	O
α	O
)	O
(	O
cid:88	O
)	O
the	O
number	O
of	O
unique	O
clusters	O
used	O
is	O
then	O
given	O
by	O
u	O
=	O
(	O
cid:80	O
)	O
γ	O
(	O
nk	O
+	O
α/k	O
)	O
p	O
(	O
z1	O
:	O
n	O
)	O
=	O
γ	O
(	O
n	O
+	O
α	O
)	O
γ	O
(	O
α/k	O
)	O
k=1	O
,	O
nk	O
≡	O
n	O
the	O
integral	O
over	O
π	O
in	O
equation	B
(	O
20.5.5	O
)	O
can	O
be	O
performed	O
analytically	O
to	O
give	O
a	O
polya	O
distribution	B
:	O
i	O
[	O
zn	O
=	O
k	O
]	O
(	O
20.5.8	O
)	O
i	O
[	O
nk	O
>	O
0	O
]	O
.	O
the	O
distribution	B
over	O
likely	O
cluster	O
numbers	O
is	O
controlled	O
by	O
the	O
parameter	B
α.	O
the	O
scaling	O
α/k	O
in	O
equation	B
(	O
20.5.7	O
)	O
ensures	O
a	O
sensible	O
limit	O
as	O
k	O
→	O
∞	O
,	O
see	O
ﬁg	O
(	O
20.13	O
)	O
,	O
in	O
which	O
limit	O
the	O
models	O
are	O
known	O
as	O
dirichlet	O
process	O
mixture	B
models	O
.	O
this	O
approach	B
means	O
that	O
we	O
do	O
not	O
need	O
to	O
explicitly	O
constrain	O
the	O
number	O
of	O
possible	O
components	O
k	O
since	O
the	O
number	O
of	O
active	B
components	O
u	O
remains	O
limited	O
even	O
for	O
very	O
large	O
k.	O
k	O
clustering	B
is	O
achieved	O
by	O
considering	O
argmax	O
z1	O
:	O
n	O
p	O
(	O
z1	O
:	O
n|v1	O
:	O
n	O
)	O
.	O
in	O
practice	O
it	O
is	O
common	O
to	O
consider	O
argmax	O
zn	O
p	O
(	O
zn|v1	O
:	O
n	O
)	O
(	O
20.5.9	O
)	O
unfortunately	O
,	O
posterior	B
inference	O
of	O
p	O
(	O
zn|v1	O
:	O
n	O
)	O
for	O
this	O
class	O
of	O
models	O
is	O
formally	O
computationally	O
in-	O
tractable	O
and	O
approximate	B
inference	I
techniques	O
are	O
required	O
.	O
a	O
detailed	O
discussion	O
of	O
these	O
techniques	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
and	O
we	O
refer	O
the	O
reader	O
to	O
[	O
165	O
]	O
for	O
a	O
deterministic	B
(	O
variational	O
)	O
approach	B
and	O
[	O
206	O
]	O
for	O
a	O
discussion	O
of	O
sampling	B
approaches	O
.	O
draft	O
march	O
9	O
,	O
2010	O
379	O
051015200501001502002500510152005010015020025005101520050100150200250	O
mixed	B
membership	I
models	O
figure	O
20.14	O
:	O
latent	B
dirichlet	O
allocation	O
.	O
for	O
document	O
n	O
we	O
ﬁrst	O
sample	O
a	O
distribution	B
of	O
topics	O
πn	O
.	O
then	O
for	O
each	O
word	O
position	O
w	O
in	O
the	O
document	O
we	O
sample	B
a	O
topic	O
zn	O
w	O
from	O
the	O
topic	O
distribution	B
.	O
given	O
the	O
topic	O
we	O
then	O
sample	B
a	O
word	O
from	O
the	O
word	O
distribution	B
of	O
that	O
topic	O
.	O
the	O
parameters	O
of	O
the	O
model	B
are	O
the	O
word	O
distributions	O
for	O
each	O
topic	O
θ	O
,	O
and	O
the	O
parameters	O
of	O
the	O
topic	O
distribution	B
α.	O
α	O
πn	O
zn	O
w	O
vn	O
w	O
wn	O
n	O
θ	O
20.6	O
mixed	B
membership	I
models	O
unlike	O
standard	O
mixture	O
models	O
in	O
which	O
each	O
object	O
is	O
assumed	O
to	O
have	O
been	O
generated	O
from	O
a	O
single	O
cluster	O
,	O
in	O
mixed	B
membership	I
models	O
an	O
object	O
may	O
be	O
a	O
member	O
of	O
more	O
than	O
one	O
group	O
.	O
latent	B
dirichlet	O
allocation	O
discussed	O
below	O
is	O
an	O
example	O
of	O
such	O
a	O
mixed	B
membership	I
model	I
,	O
and	O
is	O
one	O
of	O
a	O
number	O
of	O
models	O
developed	O
in	O
recent	O
years	O
[	O
4	O
,	O
91	O
]	O
.	O
20.6.1	O
latent	B
dirichlet	O
allocation	O
so	O
far	O
we	O
’	O
ve	O
considered	O
clustering	B
in	O
the	O
sense	O
that	O
each	O
observation	O
is	O
assumed	O
to	O
have	O
been	O
generated	O
from	O
a	O
single	O
cluster	O
.	O
in	O
contrast	O
,	O
latent	B
dirichlet	O
allocation	O
[	O
44	O
]	O
and	O
related	O
methods	O
are	O
generative	B
mixed	O
membership	O
models	O
in	O
which	O
each	O
datapoint	O
may	O
belong	O
to	O
more	O
than	O
a	O
single	O
cluster	O
.	O
a	O
typical	O
application	O
is	O
to	O
identify	O
topic	O
clusters	O
in	O
a	O
collection	O
of	O
documents	O
.	O
a	O
single	O
document	O
contains	O
a	O
sequence	O
of	O
words	O
,	O
for	O
example	O
v	O
=	O
(	O
the	O
,	O
cat	O
,	O
sat	O
,	O
on	O
,	O
the	O
,	O
cat	O
,	O
mat	O
)	O
(	O
20.6.1	O
)	O
vn	O
=	O
(	O
cid:0	O
)	O
vn	O
(	O
cid:1	O
)	O
,	O
if	O
each	O
word	O
in	O
the	O
available	O
dictionary	O
is	O
assigned	O
to	O
a	O
unique	O
state	O
(	O
say	O
dog	O
=	O
1	O
,	O
tree	B
=	O
2	O
,	O
cat	O
=	O
3	O
,	O
.	O
.	O
.	O
)	O
,	O
we	O
can	O
represent	O
then	O
the	O
nth	O
document	O
as	O
a	O
vector	O
1	O
,	O
.	O
.	O
.	O
,	O
vn	O
wn	O
vn	O
i	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
}	O
(	O
20.6.2	O
)	O
where	O
wn	O
is	O
the	O
number	O
of	O
words	O
in	O
the	O
nth	O
document	O
.	O
the	O
number	O
of	O
words	O
wn	O
in	O
each	O
document	O
can	O
vary	O
although	O
the	O
overall	O
dictionary	O
from	O
which	O
they	O
came	O
is	O
ﬁxed	O
.	O
the	O
aim	O
is	O
to	O
ﬁnd	O
common	O
topics	O
in	O
documents	O
,	O
assuming	O
that	O
any	O
document	O
could	O
potentially	O
contain	O
more	O
than	O
one	O
topic	O
.	O
it	O
is	O
useful	O
to	O
think	O
ﬁrst	O
of	O
an	O
underlying	O
generative	B
model	O
of	O
words	O
,	O
including	O
latent	B
topics	O
(	O
which	O
we	O
will	O
later	O
integrate	O
out	O
)	O
.	O
we	O
ﬁrst	O
sample	O
a	O
probability	B
distribution	O
(	O
histogram	O
)	O
that	O
represents	O
the	O
topics	O
likely	O
to	O
occur	O
for	O
this	O
document	O
.	O
then	O
,	O
for	O
each	O
word-position	O
in	O
the	O
document	O
,	O
sample	B
a	O
topic	O
and	O
subsequently	O
a	O
word	O
from	O
the	O
distribution	B
of	O
words	O
for	O
that	O
topic	O
.	O
mathematically	O
,	O
for	O
document	O
n	O
and	O
the	O
wth	O
word-position	O
in	O
the	O
document	O
,	O
vn	O
w	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
to	O
indicate	O
which	O
of	O
the	O
k	O
possible	O
topics	O
that	O
word	O
belongs	O
.	O
for	O
each	O
topic	O
k	O
,	O
one	O
then	O
has	O
a	O
categorical	O
distribution	B
over	O
all	O
the	O
words	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
,	O
in	O
the	O
dictionary	O
:	O
w	O
,	O
we	O
use	O
zn	O
p	O
(	O
vn	O
w	O
=	O
i|zn	O
w	O
=	O
k	O
,	O
θ	O
)	O
=	O
θi|k	O
distribution	B
of	O
topics	O
πn	O
with	O
(	O
cid:80	O
)	O
k	O
the	O
‘	O
animal	O
’	O
topic	O
has	O
high	O
probability	O
to	O
emit	O
animal-like	O
words	O
,	O
etc	O
.	O
for	O
each	O
document	O
n	O
we	O
have	O
a	O
k	O
=	O
1	O
which	O
gives	O
a	O
latent	B
description	O
of	O
the	O
document	O
in	O
terms	O
of	O
its	O
topic	O
membership	O
.	O
for	O
example	O
,	O
document	O
n	O
(	O
which	O
discusses	O
issues	O
related	O
to	O
wildlife	O
conservation	O
)	O
might	O
have	O
a	O
topic	O
distribution	B
with	O
high	O
mass	O
on	O
the	O
latent	B
‘	O
animals	O
’	O
and	O
‘	O
environment	O
’	O
,	O
topics	O
.	O
note	O
that	O
the	O
topics	O
are	O
indeed	O
latent	B
–	O
the	O
name	O
‘	O
animal	O
’	O
would	O
be	O
given	O
post-hoc	O
based	O
on	O
the	O
kinds	O
of	O
words	O
k=1	O
πn	O
(	O
20.6.3	O
)	O
380	O
draft	O
march	O
9	O
,	O
2010	O
mixed	B
membership	I
models	O
that	O
the	O
latent	B
topic	I
would	O
generate	O
,	O
θi|k	O
.	O
as	O
in	O
section	O
(	O
20.5.2	O
)	O
,	O
to	O
control	O
complexity	O
one	O
may	O
use	O
a	O
dirichlet	O
prior	B
to	O
limit	O
the	O
number	O
of	O
topics	O
active	B
in	O
any	O
particular	O
document	O
:	O
p	O
(	O
πn|α	O
)	O
=	O
dirichlet	O
(	O
πn|α	O
)	O
where	O
α	O
is	O
a	O
vector	O
of	O
length	O
the	O
number	O
of	O
topics	O
.	O
(	O
20.6.4	O
)	O
a	O
generative	B
model	O
for	O
sampling	B
a	O
document	O
vn	O
with	O
wn	O
word	O
positions	O
is	O
:	O
1.	O
choose	O
πn	O
∼	O
dirichlet	O
(	O
πn|α	O
)	O
2.	O
for	O
each	O
of	O
word	O
position	O
vn	O
w	O
,	O
w	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
wn	O
:	O
(	O
a	O
)	O
choose	O
a	O
topic	O
zn	O
(	O
b	O
)	O
choose	O
a	O
word	O
vn	O
w	O
∼	O
p	O
(	O
zn	O
w	O
∼	O
p	O
(	O
cid:0	O
)	O
vn	O
(	O
cid:1	O
)	O
w|πn	O
)	O
w|θ·|zn	O
w	O
training	B
the	O
lda	O
model	B
corresponds	O
to	O
learning	B
the	O
parameters	O
α	O
,	O
which	O
relates	O
to	O
the	O
number	O
of	O
topics	O
,	O
and	O
θ	O
,	O
which	O
describes	O
the	O
distribution	B
of	O
words	O
within	O
each	O
topic	O
.	O
unfortunately	O
,	O
ﬁnding	O
the	O
requisite	O
marginals	O
for	O
learning	B
from	O
the	O
posterior	B
is	O
formally	O
computationally	O
intractable	O
.	O
eﬃcient	B
approximate	O
inference	B
for	O
this	O
class	O
of	O
models	O
is	O
a	O
topic	O
of	O
research	O
interest	O
and	O
both	O
variational	O
and	O
sampling	B
ap-	O
proaches	O
have	O
recently	O
been	O
developed	O
[	O
44	O
,	O
273	O
,	O
225	O
]	O
.	O
there	O
are	O
close	O
similarities	O
between	O
lda	O
and	O
plsa	O
[	O
113	O
]	O
,	O
section	O
(	O
15.6.1	O
)	O
,	O
both	O
of	O
which	O
describe	O
a	O
doc-	O
ument	O
in	O
terms	O
of	O
a	O
distribution	B
over	O
latent	B
topics	O
.	O
lda	O
is	O
a	O
probabilistic	B
model	O
for	O
which	O
issues	O
such	O
as	O
setting	O
hyperparameters	O
can	O
be	O
addressed	O
using	O
maximum	B
likelihood	I
.	O
plsa	O
on	O
the	O
other	O
hand	O
is	O
essentially	O
a	O
matrix	B
decomposition	O
technique	O
(	O
such	O
as	O
pca	O
)	O
.	O
issues	O
such	O
as	O
hyperparameters	O
setting	O
for	O
plsa	O
are	O
therefore	O
addressed	O
using	O
validation	B
data	O
.	O
whilst	O
plsa	O
is	O
a	O
description	O
only	O
of	O
the	O
training	B
data	O
,	O
lda	O
is	O
a	O
generative	B
data	O
model	B
and	O
can	O
in	O
principle	O
be	O
used	O
to	O
synthesise	O
new	O
documents	O
.	O
example	O
90.	O
an	O
illustration	O
of	O
the	O
use	O
of	O
lda	O
is	O
given	O
in	O
ﬁg	O
(	O
20.15	O
)	O
[	O
44	O
]	O
.	O
the	O
documents	O
are	O
taken	O
from	O
the	O
trec	O
associated	O
press	O
corpus	O
containing	O
16,333	O
newswire	O
articles	O
with	O
23,075	O
unique	O
terms	O
.	O
after	O
removing	O
a	O
standard	O
list	O
of	O
stop	B
words	I
(	O
frequent	O
words	O
such	O
as	O
‘	O
the	O
’	O
,	O
‘	O
a	O
’	O
etc	O
.	O
that	O
would	O
otherwise	O
dominate	O
the	O
statistics	O
)	O
,	O
the	O
em	O
algorithm	B
(	O
with	O
variational	O
approximate	B
inference	I
)	O
was	O
used	O
to	O
ﬁnd	O
the	O
dirichlet	O
and	O
conditional	B
categorical	O
parameters	O
for	O
a	O
100-topic	O
lda	O
model	B
.	O
the	O
top	O
words	O
from	O
four	O
resulting	O
categorical	O
distributions	O
θi|k	O
are	O
illustrated	O
ﬁg	O
(	O
20.15a	O
)	O
.	O
these	O
distributions	O
capture	O
some	O
of	O
the	O
underlying	O
topics	O
in	O
the	O
corpus	O
.	O
an	O
example	O
document	O
from	O
the	O
corpus	O
is	O
presented	O
along	O
with	O
the	O
words	O
coloured	O
by	O
the	O
most	O
probable	O
latent	O
topic	O
they	O
correspond	O
to	O
.	O
20.6.2	O
graph	B
based	O
representations	O
of	O
data	B
mixed	O
membership	O
models	O
are	O
used	O
in	O
a	O
variety	O
of	O
contexts	O
and	O
are	O
distinguished	O
also	O
by	O
the	O
form	O
of	O
data	B
available	O
.	O
here	O
we	O
focus	O
on	O
analysing	O
a	O
representation	B
of	O
the	O
interactions	O
amongst	O
a	O
collection	O
of	O
objects	O
;	O
in	O
particular	O
,	O
the	O
data	B
has	O
been	O
processed	O
such	O
that	O
all	O
the	O
information	O
of	O
interest	O
is	O
characterised	O
by	O
an	O
interaction	O
matrix	B
.	O
for	O
graph	B
based	O
representations	O
of	O
data	B
,	O
two	O
objects	O
are	O
similar	O
if	O
they	O
are	O
neighbours	O
on	O
a	O
graph	B
representing	O
the	O
data	B
objects	O
.	O
in	O
the	O
ﬁeld	O
of	O
social-networks	O
,	O
for	O
example	O
,	O
each	O
individual	O
is	O
represented	O
as	O
a	O
node	B
in	O
a	O
graph	B
,	O
with	O
a	O
link	O
between	O
two	O
nodes	O
if	O
the	O
individuals	O
are	O
friends	O
.	O
given	O
a	O
graph	B
one	O
might	O
wish	O
to	O
identify	O
communities	O
of	O
closely	O
linked	O
friends	O
.	O
interpreted	O
as	O
a	O
social	O
network	O
,	O
in	O
ﬁg	O
(	O
20.16a	O
)	O
,	O
individual	O
3	O
is	O
a	O
member	O
of	O
his	O
work	O
group	O
(	O
1	O
,	O
2	O
,	O
3	O
)	O
and	O
also	O
the	O
poker	O
group	O
(	O
3	O
,	O
4	O
,	O
5	O
)	O
.	O
these	O
two	O
groups	O
of	O
individuals	O
are	O
otherwise	O
disjoint	O
.	O
discovering	O
such	O
groupings	O
contrasts	O
with	O
graph	O
parti-	O
tioning	O
in	O
which	O
each	O
node	B
is	O
assigned	O
to	O
only	O
one	O
of	O
a	O
set	O
of	O
subgraphs	O
,	O
ﬁg	O
(	O
20.16b	O
)	O
,	O
for	O
which	O
a	O
typical	O
criterion	O
is	O
that	O
each	O
subgraph	O
should	O
be	O
roughly	O
of	O
the	O
same	O
size	O
and	O
that	O
there	O
are	O
few	O
connections	O
between	O
the	O
subgraphs	O
[	O
156	O
]	O
.	O
draft	O
march	O
9	O
,	O
2010	O
381	O
mixed	B
membership	I
models	O
arts	O
new	O
ﬁlm	O
show	O
music	O
movie	O
play	O
musical	O
best	O
actor	O
ﬁrst	O
york	O
opera	O
theater	O
actress	O
love	O
budgets	O
million	O
tax	O
program	O
budget	O
billion	O
federal	O
year	O
spending	O
new	O
state	O
plan	O
money	B
programs	O
government	O
congress	O
children	B
education	O
children	B
women	O
people	O
child	O
years	O
families	O
work	O
parents	B
says	O
family	B
welfare	O
men	O
percent	O
care	O
life	O
school	O
students	O
schools	O
education	O
teachers	O
high	O
public	O
teacher	O
bennett	O
manigat	O
namphy	O
state	O
president	O
elementary	O
haiti	O
(	O
a	O
)	O
the	O
william	O
randolph	O
hearst	O
foundation	O
will	O
give	O
$	O
1.25	O
million	O
to	O
lin-	O
coln	O
center	O
,	O
metropolitan	O
opera	O
co.	O
,	O
new	O
york	O
philharmonic	O
and	O
juil-	O
liard	O
school	O
.	O
our	O
board	O
felt	O
that	O
we	O
had	O
a	O
real	O
opportunity	O
to	O
make	O
a	O
mark	O
on	O
the	O
future	O
of	O
the	O
performing	O
arts	O
with	O
these	O
grants	O
an	O
act	O
every	O
bit	O
as	O
important	O
as	O
our	O
traditional	O
areas	O
of	O
support	O
in	O
health	O
,	O
medical	O
research	O
,	O
education	O
and	O
the	O
social	O
services	O
,	O
hearst	O
foundation	O
president	O
randolph	O
a.	O
hearst	O
said	O
monday	O
in	O
announcing	O
the	O
grants	O
.	O
lincoln	O
centers	O
share	O
will	O
be	O
$	O
200,000	O
for	O
its	O
new	O
building	O
,	O
which	O
will	O
house	O
young	O
artists	O
and	O
provide	O
new	O
public	O
facilities	O
.	O
the	O
metropolitan	O
opera	O
co.	O
and	O
new	O
york	O
philharmonic	O
will	O
receive	O
$	O
400,000	O
each	O
.	O
the	O
juilliard	O
school	O
,	O
where	O
music	O
and	O
the	O
performing	O
arts	O
are	O
taught	O
,	O
will	O
get	O
$	O
250,000	O
.	O
the	O
hearst	O
foun-	O
dation	O
,	O
a	O
leading	O
supporter	O
of	O
the	O
lincoln	O
center	O
consolidated	O
corporate	O
fund	O
,	O
will	O
make	O
its	O
usual	O
annual	O
$	O
100,000	O
donation	O
,	O
too	O
.	O
(	O
b	O
)	O
figure	O
20.15	O
:	O
(	O
a	O
)	O
:	O
a	O
subset	O
of	O
the	O
latent	B
topics	O
discovered	O
by	O
lda	O
and	O
the	O
high	O
probability	O
words	O
associated	O
with	O
each	O
topic	O
.	O
each	O
column	O
represents	O
a	O
topic	O
,	O
with	O
the	O
topic	O
name	O
such	O
as	O
‘	O
art	O
’	O
assigned	O
by	O
hand	O
after	O
viewing	O
the	O
most	O
likely	O
words	O
corresponding	O
to	O
the	O
topic	O
.	O
(	O
b	O
)	O
:	O
a	O
document	O
from	O
the	O
training	B
data	O
in	O
which	O
the	O
words	O
are	O
coloured	O
according	O
to	O
the	O
most	O
likely	O
latent	O
topic	O
.	O
this	O
demonstrates	O
the	O
mixed-membership	O
nature	O
of	O
the	O
model	B
,	O
assigning	O
the	O
datapoint	O
(	O
document	O
in	O
this	O
case	O
)	O
to	O
several	O
clusters	O
(	O
topics	O
)	O
.	O
reproduced	O
from	O
[	O
44	O
]	O
.	O
1	O
2	O
3	O
(	O
a	O
)	O
4	O
5	O
1	O
2	O
3	O
(	O
b	O
)	O
4	O
5	O
figure	O
20.16	O
:	O
(	O
a	O
)	O
the	O
social	O
network	O
of	O
a	O
set	O
of	O
5	O
individuals	O
,	O
repre-	O
sented	O
as	O
an	O
undirected	B
graph	I
.	O
here	O
individual	O
3	O
belongs	O
to	O
the	O
group	O
(	O
1	O
,	O
2	O
,	O
3	O
)	O
and	O
also	O
(	O
3	O
,	O
4	O
,	O
5	O
)	O
.	O
(	O
b	O
)	O
by	O
contrast	O
,	O
in	O
graph	B
partitioning	I
,	O
one	O
breaks	O
the	O
graph	B
into	O
roughly	O
equally	O
sized	O
disjoint	O
partitions	O
such	O
that	O
each	O
node	B
is	O
a	O
member	O
of	O
only	O
a	O
single	O
partition	O
,	O
with	O
a	O
minimal	O
number	O
of	O
edges	O
between	O
partitions	O
.	O
another	O
example	O
is	O
that	O
nodes	O
in	O
the	O
graph	B
represent	O
products	O
and	O
a	O
link	O
between	O
nodes	O
i	O
and	O
j	O
indicates	O
that	O
customers	O
who	O
by	O
product	O
i	O
frequently	O
also	O
buy	O
product	O
j.	O
the	O
aim	O
is	O
to	O
decompose	O
the	O
graph	B
into	O
groups	O
,	O
each	O
corresponding	O
to	O
products	O
that	O
are	O
commonly	O
co-bought	O
by	O
customers	O
[	O
116	O
]	O
.	O
a	O
growing	O
area	O
of	O
application	O
of	O
graph	B
based	O
representations	O
is	O
in	O
bioinformatics	B
in	O
which	O
nodes	O
represent	O
genes	O
,	O
and	O
a	O
link	O
between	O
them	O
representing	O
that	O
the	O
two	O
genes	O
have	O
similar	O
activity	O
proﬁles	O
.	O
the	O
task	O
is	O
then	O
to	O
identify	O
groups	O
of	O
similarly	O
behaving	O
genes	O
[	O
5	O
]	O
.	O
20.6.3	O
dyadic	B
data	I
consider	O
two	O
kinds	O
of	O
objects	O
,	O
for	O
example	O
,	O
ﬁlms	O
and	O
customers	O
.	O
each	O
ﬁlm	O
is	O
indexed	O
by	O
f	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
f	O
and	O
each	O
user	O
by	O
u	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
u.	O
the	O
interaction	O
of	O
user	O
u	O
with	O
ﬁlm	O
f	O
can	O
be	O
described	O
by	O
the	O
element	O
of	O
a	O
matrix	B
muf	O
representing	O
the	O
rating	O
a	O
user	O
gives	O
to	O
a	O
ﬁlm	O
.	O
a	O
dyadic	B
dataset	O
consists	O
of	O
such	O
a	O
matrix	B
and	O
the	O
aim	O
is	O
to	O
decompose	O
this	O
matrix	B
to	O
explain	O
the	O
ratings	O
by	O
ﬁnding	O
types	O
of	O
ﬁlms	O
and	O
types	O
of	O
user	O
.	O
another	O
example	O
is	O
to	O
consider	O
a	O
collection	O
of	O
documents	O
,	O
summarised	O
by	O
an	O
interaction	O
matrix	B
in	O
which	O
mwd	O
is	O
1	O
if	O
word	O
w	O
appears	O
in	O
document	O
d	O
and	O
zero	O
otherwise	O
.	O
this	O
matrix	B
can	O
be	O
represented	O
as	O
a	O
bipartite	O
graph	B
,	O
as	O
in	O
ﬁg	O
(	O
20.17a	O
)	O
.	O
the	O
upper	O
nodes	O
represent	O
documents	O
,	O
and	O
the	O
lower	O
nodes	O
words	O
,	O
with	O
a	O
link	O
between	O
them	O
if	O
that	O
word	O
occurs	O
in	O
that	O
document	O
.	O
one	O
the	O
seeks	O
assignments	O
of	O
documents	O
to	O
groups	O
or	O
latent	B
‘	O
topics	O
’	O
to	O
succinctly	O
explain	O
the	O
link	O
structure	B
of	O
the	O
bipartite	O
graph	B
via	O
a	O
small	O
number	O
of	O
latent	B
nodes	O
,	O
as	O
schematically	O
depicted	O
in	O
ﬁg	O
(	O
20.17b	O
)	O
.	O
one	O
may	O
view	O
this	O
as	O
a	O
form	O
of	O
matrix	B
factorisation	I
[	O
136	O
,	O
189	O
]	O
(	O
cid:88	O
)	O
t	O
mwd	O
≈	O
uwtv	O
t	O
td	O
(	O
20.6.5	O
)	O
where	O
t	O
indexes	O
the	O
topics	O
and	O
the	O
feature	O
matrices	O
u	O
and	O
v	O
control	O
the	O
word-to-topic	O
mapping	O
and	O
the	O
topic-to-document	O
mapping	O
.	O
this	O
diﬀers	O
from	O
latent	B
dirichlet	O
allocation	O
which	O
has	O
a	O
probabilistic	B
inter-	O
pretation	O
of	O
ﬁrst	O
generating	O
a	O
topic	O
and	O
then	O
a	O
word	O
,	O
conditional	B
on	O
the	O
chosen	O
topic	O
.	O
here	O
the	O
interaction	O
between	O
document-topic	O
matrix	B
v	O
and	O
word-topic	O
matrix	B
u	O
is	O
non-probabilistic	O
.	O
more	O
generally	O
,	O
we	O
can	O
382	O
draft	O
march	O
9	O
,	O
2010	O
(	O
a	O
)	O
:	O
there	O
are	O
figure	O
20.17	O
:	O
graphical	O
representation	B
of	O
dyadic	B
data	I
.	O
6	O
documents	O
and	O
13	O
words	O
.	O
a	O
link	O
represents	O
that	O
a	O
particular	O
word-	O
document	O
pair	O
occurs	O
in	O
the	O
dataset	O
.	O
(	O
b	O
)	O
:	O
a	O
latent	B
decomposition	O
of	O
(	O
a	O
)	O
using	O
3	O
‘	O
topics	O
’	O
.	O
a	O
topic	O
corresponds	O
to	O
a	O
collection	O
of	O
words	O
,	O
and	O
each	O
document	O
a	O
collection	O
of	O
topics	O
.	O
the	O
open	O
nodes	O
indicate	O
latent	B
variables	O
.	O
where	O
u	O
and	O
v	O
are	O
feature	O
matrices	O
.	O
in	O
[	O
189	O
]	O
,	O
real-valued	O
data	B
is	O
modelled	O
using	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
p	O
(	O
m|u	O
,	O
w	O
,	O
v	O
)	O
=	O
n	O
m	O
uwvt	O
,	O
σ2i	O
where	O
u	O
and	O
v	O
are	O
assumed	O
binary	O
and	O
the	O
real-valued	O
w	O
is	O
a	O
topic-interaction	O
matrix	B
.	O
in	O
this	O
viewpoint	O
learning	B
then	O
consists	O
of	O
inferring	O
u	O
,	O
w	O
,	O
v	O
,	O
given	O
the	O
dyadic	B
observation	O
matrix	B
m.	O
assuming	O
factorised	B
priors	O
,	O
the	O
posterior	B
over	O
the	O
matrices	O
is	O
p	O
(	O
u	O
,	O
w	O
,	O
v|m	O
)	O
∝	O
p	O
(	O
m|u	O
,	O
w	O
,	O
v	O
)	O
p	O
(	O
u	O
)	O
p	O
(	O
w	O
)	O
p	O
(	O
v	O
)	O
(	O
20.6.8	O
)	O
a	O
convenient	O
choice	O
is	O
a	O
gaussian	O
prior	B
distribution	O
for	O
w	O
,	O
with	O
the	O
feature	O
matrices	O
u	O
and	O
v	O
sampled	O
from	O
beta-bernoulli	O
priors	O
.	O
the	O
resulting	O
posterior	B
distribution	O
is	O
formally	O
computationally	O
intractable	O
,	O
and	O
in	O
[	O
189	O
]	O
this	O
is	O
addressed	O
using	O
a	O
sampling	B
approximation	O
.	O
20.6.4	O
monadic	B
data	I
in	O
monadic	B
data	I
there	O
is	O
only	O
one	O
type	O
of	O
object	O
and	O
the	O
interaction	O
between	O
the	O
objects	O
is	O
represented	O
by	O
a	O
square	O
interaction	O
matrix	B
.	O
for	O
example	O
one	O
might	O
have	O
a	O
matrix	B
with	O
elements	O
aij	O
=	O
1	O
if	O
proteins	O
i	O
and	O
j	O
can	O
bind	O
to	O
each	O
other	O
and	O
0	O
otherwise	O
.	O
a	O
depiction	O
of	O
the	O
interaction	O
matrix	B
is	O
given	O
by	O
a	O
graph	B
in	O
which	O
an	O
edge	O
represents	O
an	O
interaction	O
,	O
for	O
example	O
ﬁg	O
(	O
20.18	O
)	O
.	O
in	O
the	O
following	O
section	O
we	O
discuss	O
a	O
particular	O
mixed	B
membership	I
model	I
and	O
highlight	O
potential	B
applications	O
.	O
the	O
method	O
is	O
based	O
on	O
clique	B
decompositions	O
of	O
graphs	O
and	O
as	O
such	O
we	O
require	O
a	O
short	O
digression	O
into	O
clique-based	O
graph	B
representations	O
.	O
20.6.5	O
cliques	O
and	O
adjacency	B
matrices	O
for	O
monadic	B
binary	O
data	B
a	O
symmetric	O
adjacency	B
matrix	I
has	O
elements	O
aij	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
with	O
a	O
1	O
indicating	O
a	O
link	O
between	O
nodes	O
i	O
and	O
j.	O
for	O
the	O
graph	B
in	O
ﬁg	O
(	O
20.18	O
)	O
,	O
the	O
adjacency	B
matrix	I
is	O
(	O
20.6.6	O
)	O
(	O
20.6.7	O
)	O
(	O
20.6.9	O
)	O
mixed	B
membership	I
models	O
(	O
a	O
)	O
(	O
b	O
)	O
write	O
a	O
distribution	B
p	O
(	O
m|u	O
,	O
v	O
)	O
	O
1	O
1	O
1	O
1	O
1	O
1	O
0	O
1	O
a	O
=	O
	O
1	O
1	O
1	O
1	O
0	O
1	O
1	O
1	O
where	O
we	O
include	O
self	O
connections	O
on	O
the	O
diagonal	O
.	O
given	O
a	O
,	O
our	O
aim	O
is	O
to	O
ﬁnd	O
a	O
‘	O
simpler	O
’	O
description	O
that	O
reveals	O
the	O
underlying	O
cluster	O
structure	B
,	O
such	O
as	O
(	O
1	O
,	O
2	O
,	O
3	O
)	O
and	O
(	O
2	O
,	O
3	O
,	O
4	O
)	O
in	O
ﬁg	O
(	O
20.18	O
)	O
.	O
given	O
the	O
undirected	B
graph	I
in	O
ﬁg	O
(	O
20.18	O
)	O
,	O
the	O
incidence	B
matrix	I
finc	O
is	O
an	O
alternative	O
description	O
of	O
the	O
adjacency	B
structure	O
[	O
81	O
]	O
.	O
1	O
2	O
3	O
4	O
draft	O
march	O
9	O
,	O
2010	O
figure	O
20.18	O
:	O
the	O
minimal	O
clique	B
cover	O
is	O
(	O
1	O
,	O
2	O
,	O
3	O
)	O
,	O
(	O
2	O
,	O
3	O
,	O
4	O
)	O
.	O
383	O
mixed	B
membership	I
models	O
1	O
2	O
3	O
4	O
1	O
2	O
3	O
4	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
20.19	O
:	O
bipartite	O
representations	O
of	O
the	O
decomposi-	O
tions	O
of	O
ﬁg	O
(	O
20.18	O
)	O
.	O
shaded	O
nodes	O
represent	O
observed	O
vari-	O
ables	O
,	O
and	O
open	O
nodes	O
latent	B
variables	O
.	O
(	O
a	O
)	O
incidence	B
ma-	O
trix	O
representation	B
.	O
(	O
b	O
)	O
minimal	O
clique	B
decomposition	O
.	O
given	O
the	O
v	O
nodes	O
in	O
the	O
graph	B
,	O
we	O
construct	O
finc	O
as	O
follows	O
:	O
for	O
each	O
link	O
i	O
∼	O
j	O
in	O
the	O
graph	B
,	O
form	O
a	O
column	O
of	O
the	O
matrix	B
finc	O
with	O
zero	O
entries	O
except	O
for	O
a	O
1	O
in	O
the	O
ith	O
and	O
jth	O
row	O
.	O
the	O
column	O
ordering	O
is	O
arbitrary	O
.	O
for	O
example	O
,	O
for	O
the	O
graph	B
in	O
ﬁg	O
(	O
20.18	O
)	O
an	O
incidence	B
matrix	I
is	O
the	O
incidence	B
matrix	I
has	O
the	O
property	O
that	O
the	O
adjacency	B
structure	O
of	O
the	O
original	O
graph	B
is	O
given	O
by	O
the	O
outer	O
product	O
of	O
the	O
incidence	B
matrix	I
with	O
itself	O
.	O
the	O
diagonal	O
entries	O
contain	O
the	O
degree	B
(	O
number	O
of	O
links	O
)	O
of	O
each	O
node	B
.	O
for	O
our	O
example	O
,	O
this	O
gives	O
	O
1	O
1	O
1	O
0	O
0	O
1	O
0	O
0	O
	O
0	O
0	O
0	O
1	O
1	O
0	O
1	O
0	O
1	O
0	O
1	O
1	O
finc	O
=	O
	O
1	O
1	O
3	O
1	O
0	O
1	O
1	O
2	O
	O
2	O
1	O
(	O
cid:17	O
)	O
1	O
3	O
1	O
1	O
0	O
1	O
fincft	O
inc	O
fincft	O
inc	O
=	O
so	O
that	O
a	O
=	O
h	O
(	O
cid:16	O
)	O
(	O
20.6.10	O
)	O
(	O
20.6.11	O
)	O
(	O
20.6.12	O
)	O
(	O
20.6.13	O
)	O
(	O
20.6.14	O
)	O
here	O
h	O
(	O
·	O
)	O
is	O
the	O
element-wise	O
heaviside	O
step	O
function	B
,	O
[	O
h	O
(	O
m	O
)	O
]	O
ij	O
=	O
1	O
if	O
mij	O
>	O
0	O
and	O
is	O
0	O
otherwise	O
.	O
a	O
useful	O
viewpoint	O
of	O
the	O
incidence	B
matrix	I
is	O
that	O
it	O
identiﬁes	O
two-cliques	O
in	O
the	O
graph	B
(	O
here	O
we	O
are	O
using	O
the	O
term	O
‘	O
clique	B
’	O
in	O
the	O
non-maximal	O
sense	O
)	O
.	O
there	O
are	O
ﬁve	O
2-cliques	O
in	O
ﬁg	O
(	O
20.18	O
)	O
,	O
and	O
each	O
column	O
of	O
finc	O
speciﬁes	O
which	O
elements	O
are	O
in	O
each	O
2-clique	O
.	O
graphically	O
we	O
can	O
depict	O
this	O
incidence	B
decomposition	O
as	O
a	O
bipartite	O
graph	B
,	O
as	O
in	O
ﬁg	O
(	O
20.19a	O
)	O
where	O
the	O
open	O
nodes	O
represent	O
the	O
ﬁve	O
2-cliques	O
.	O
the	O
incidence	B
matrix	I
can	O
be	O
generalised	B
to	O
describe	O
larger	O
cliques	O
.	O
consider	O
the	O
following	O
matrix	B
as	O
a	O
decomposition	B
for	O
ﬁg	O
(	O
20.18	O
)	O
,	O
and	O
its	O
outer-product	O
:	O
	O
1	O
0	O
1	O
1	O
1	O
1	O
0	O
1	O
	O
,	O
f	O
=	O
fft	O
=	O
	O
1	O
1	O
1	O
2	O
1	O
2	O
0	O
1	O
	O
1	O
2	O
2	O
1	O
0	O
1	O
1	O
1	O
the	O
interpretation	O
is	O
that	O
f	O
represents	O
a	O
decomposition	B
into	O
two	O
3-cliques	O
.	O
as	O
in	O
the	O
incidence	B
matrix	I
,	O
each	O
column	O
represents	O
a	O
clique	B
,	O
and	O
the	O
rows	O
containing	O
a	O
‘	O
1	O
’	O
express	O
which	O
elements	O
are	O
in	O
the	O
clique	B
deﬁned	O
by	O
that	O
column	O
.	O
this	O
decomposition	B
can	O
be	O
represented	O
as	O
the	O
bipartite	O
graph	B
of	O
ﬁg	O
(	O
20.19b	O
)	O
.	O
for	O
the	O
graph	B
of	O
ﬁg	O
(	O
20.18	O
)	O
,	O
both	O
finc	O
and	O
f	O
satisfy	O
(	O
cid:16	O
)	O
fft	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
a	O
=	O
h	O
=	O
h	O
fincft	O
inc	O
(	O
cid:17	O
)	O
one	O
can	O
view	O
equation	B
(	O
20.6.14	O
)	O
as	O
a	O
form	O
of	O
binary	O
matrix	O
factoristation	O
of	O
the	O
binary	O
square	O
(	O
symmetric	O
)	O
matrix	B
a	O
into	O
non-square	O
binary	O
matrices	O
.	O
for	O
our	O
clustering	B
purposes	O
,	O
the	O
decomposition	B
using	O
f	O
is	O
to	O
be	O
preferred	O
to	O
the	O
incidence	B
decomposition	O
since	O
f	O
decomposes	O
the	O
graph	B
into	O
a	O
smaller	O
number	O
of	O
larger	O
cliques	O
.	O
a	O
formal	O
speciﬁcation	O
of	O
the	O
problem	B
of	O
ﬁnding	O
a	O
minimum	O
number	O
of	O
maximal	O
fully-connected	O
subsets	O
is	O
the	O
computational	O
problem	O
min	O
clique	B
cover	O
[	O
103	O
,	O
251	O
]	O
.	O
indeed	O
,	O
f	O
solves	O
min	O
clique	B
cover	O
for	O
ﬁg	O
(	O
20.18	O
)	O
.	O
384	O
draft	O
march	O
9	O
,	O
2010	O
mixed	B
membership	I
models	O
figure	O
20.20	O
:	O
the	O
function	B
σ	O
(	O
x	O
)	O
≡	O
β	O
increases	O
,	O
this	O
sigmoid	B
function	I
tends	O
to	O
a	O
step	O
function	B
.	O
(	O
cid:0	O
)	O
1	O
+	O
eβ	O
(	O
0.5−x	O
)	O
(	O
cid:1	O
)	O
−1	O
for	O
β	O
=	O
1	O
,	O
10	O
,	O
100.	O
as	O
(	O
cid:2	O
)	O
fft	O
(	O
cid:3	O
)	O
ii	O
express	O
the	O
number	O
of	O
cliques/columns	O
that	O
node	B
i	O
occurs	O
in	O
.	O
oﬀ-diagonal	O
elements	O
(	O
cid:2	O
)	O
fft	O
(	O
cid:3	O
)	O
deﬁnition	O
106	O
(	O
clique	B
matrix	I
)	O
.	O
given	O
an	O
adjacency	B
matrix	I
[	O
a	O
]	O
ij	O
,	O
i	O
,	O
j	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
v	O
(	O
aii	O
=	O
1	O
)	O
,	O
a	O
clique	B
matrix	I
f	O
has	O
elements	O
fic	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
v	O
,	O
c	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
c	O
such	O
that	O
a	O
=	O
h	O
(	O
fft	O
)	O
.	O
diagonal	O
elements	O
contain	O
the	O
number	O
of	O
cliques/columns	O
that	O
nodes	O
i	O
and	O
j	O
jointly	O
inhabit	O
[	O
18	O
]	O
.	O
ij	O
whilst	O
ﬁnding	O
a	O
clique	B
decomposition	O
f	O
is	O
easy	O
(	O
use	O
the	O
incidence	B
matrix	I
for	O
example	O
)	O
,	O
ﬁnding	O
a	O
clique	B
decomposition2	O
with	O
the	O
minimal	O
number	O
of	O
columns	O
,	O
i.e	O
.	O
solving	B
min	O
clique	B
cover	O
,	O
is	O
np-hard	O
[	O
103	O
,	O
10	O
]	O
.	O
a	O
generative	B
model	O
of	O
adjacency	B
matrices	O
solving	B
min	O
clique	B
cover	O
is	O
a	O
computationally	O
hard	B
problem	O
and	O
approximations	O
are	O
in	O
general	O
unavoid-	O
able	O
.	O
below	O
we	O
relax	O
the	O
strict	O
clique	B
requirement	O
and	O
assume	O
that	O
provided	O
only	O
a	O
small	O
number	O
of	O
links	O
in	O
an	O
‘	O
almost	O
clique	B
’	O
are	O
missing	B
,	O
this	O
may	O
be	O
considered	O
a	O
suﬃciently	O
well-connected	O
group	O
of	O
nodes	O
to	O
form	O
a	O
cluster	O
.	O
given	O
an	O
adjacency	B
matrix	I
a	O
and	O
a	O
prior	B
on	O
clique	B
matrices	O
f	O
,	O
our	O
interest	O
is	O
the	O
posterior	B
p	O
(	O
f|a	O
)	O
∝	O
p	O
(	O
a|f	O
)	O
p	O
(	O
f	O
)	O
(	O
20.6.15	O
)	O
we	O
ﬁrst	O
concentrate	O
on	O
the	O
generative	B
term	O
p	O
(	O
a|f	O
)	O
.	O
to	O
ﬁnd	O
‘	O
well-connected	O
’	O
clusters	O
,	O
we	O
relax	O
the	O
con-	O
straint	O
that	O
the	O
decomposition	B
is	O
in	O
the	O
form	O
of	O
cliques	O
in	O
the	O
original	O
graph	B
and	O
view	O
the	O
absence	O
of	O
links	O
as	O
statistical	O
ﬂuctuations	O
away	O
from	O
a	O
perfect	O
clique	O
.	O
given	O
a	O
v	O
×	O
c	O
matrix	B
f	O
,	O
we	O
desire	O
that	O
the	O
higher	O
the	O
overlap	O
between	O
rows3	O
fi	O
and	O
fj	O
is	O
,	O
the	O
greater	O
the	O
probability	B
of	O
a	O
link	O
between	O
i	O
and	O
j.	O
this	O
may	O
be	O
achieved	O
using	O
,	O
for	O
example	O
,	O
p	O
(	O
aij	O
=	O
1|f	O
)	O
=	O
σ	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
1	O
+	O
eβ	O
(	O
0.5−x	O
)	O
(	O
cid:17	O
)	O
−1	O
fif	O
t	O
j	O
(	O
cid:16	O
)	O
with	O
σ	O
(	O
x	O
)	O
≡	O
(	O
20.6.16	O
)	O
(	O
20.6.17	O
)	O
where	O
β	O
controls	O
the	O
steepness	O
of	O
the	O
function	B
,	O
see	O
ﬁg	O
(	O
20.20	O
)	O
.	O
the	O
0.5	O
shift	O
in	O
equation	B
(	O
20.6.17	O
)	O
ensures	O
that	O
σ	O
approximates	O
the	O
step-function	O
since	O
the	O
argument	O
of	O
σ	O
is	O
an	O
integer	O
.	O
under	O
equation	B
(	O
20.6.16	O
)	O
,	O
if	O
j	O
−	O
0.5	O
>	O
0	O
and	O
p	O
(	O
aij	O
=	O
1|f	O
)	O
is	O
high	O
.	O
absent	O
links	O
fi	O
and	O
fj	O
have	O
at	O
least	O
one	O
‘	O
1	O
’	O
in	O
the	O
same	O
position	O
,	O
fif	O
t	O
contribute	O
p	O
(	O
aij	O
=	O
0|f	O
)	O
=	O
1	O
−	O
p	O
(	O
aij	O
=	O
1|f	O
)	O
.	O
the	O
parameter	B
β	O
controls	O
how	O
strictly	O
σ	O
(	O
fft	O
)	O
matches	O
a	O
;	O
for	O
large	O
β	O
,	O
very	O
little	O
ﬂexibility	O
is	O
allowed	O
and	O
only	O
cliques	O
will	O
be	O
identiﬁed	O
.	O
for	O
small	O
β	O
,	O
subsets	O
that	O
would	O
be	O
cliques	O
if	O
it	O
were	O
not	O
for	O
a	O
small	O
number	O
of	O
missing	B
links	O
,	O
are	O
clustered	O
together	O
.	O
the	O
setting	O
of	O
β	O
is	O
user	O
and	O
problem	B
dependent	O
.	O
assuming	O
each	O
element	O
of	O
the	O
adjacency	B
matrix	I
is	O
sampled	O
independently	O
from	O
the	O
generating	O
process	O
,	O
the	O
joint	B
probability	O
of	O
observing	O
a	O
is	O
(	O
neglecting	O
its	O
diagonal	O
elements	O
)	O
,	O
p	O
(	O
a|f	O
)	O
=	O
(	O
cid:89	O
)	O
i∼j	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
(	O
cid:89	O
)	O
(	O
cid:16	O
)	O
i	O
(	O
cid:54	O
)	O
∼j	O
σ	O
fif	O
t	O
j	O
1	O
−	O
σ	O
fif	O
t	O
j	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
(	O
20.6.18	O
)	O
the	O
ultimate	O
quantity	O
of	O
interest	O
is	O
the	O
posterior	B
distribution	O
of	O
clique	B
structure	O
,	O
equation	B
(	O
20.6.15	O
)	O
,	O
for	O
which	O
we	O
now	O
specify	O
a	O
prior	B
p	O
(	O
f	O
)	O
over	O
clique	B
matrices	O
.	O
3we	O
use	O
lower	O
indices	O
fi	O
to	O
denote	O
the	O
ith	O
row	O
of	O
f.	O
draft	O
march	O
9	O
,	O
2010	O
385	O
−2−1.5−1−0.500.511.5200.20.40.60.81x	O
mixed	B
membership	I
models	O
figure	O
20.21	O
:	O
(	O
a	O
)	O
:	O
adjacency	B
ma-	O
trix	O
of	O
105	O
political	O
books	O
(	O
black=1	O
)	O
.	O
(	O
b	O
)	O
:	O
clique	B
matrix	I
:	O
521	O
non-zero	O
en-	O
tries	O
.	O
(	O
c	O
)	O
:	O
adjacency	B
reconstruction	O
using	O
an	O
approximate	B
clique	O
matrix	B
with	O
10	O
cliques	O
–	O
see	O
also	O
ﬁg	O
(	O
20.22	O
)	O
and	O
democliquedecomp.m	O
.	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
20.22	O
:	O
political	O
books	O
.	O
105	O
×	O
10	O
dimensional	O
clique	B
matrix	I
broken	O
into	O
3	O
groups	O
by	O
a	O
politically	O
astute	O
reader	O
.	O
a	O
black	O
square	O
indicates	O
q	O
(	O
fic	O
)	O
>	O
0.5.	O
liberal	O
books	O
(	O
red	O
)	O
,	O
conservative	O
books	O
(	O
green	O
)	O
,	O
neutral	O
books	O
(	O
yellow	O
)	O
.	O
by	O
inspection	O
,	O
cliques	O
5,6,7,8,9	O
largely	O
correspond	O
to	O
‘	O
conservative	O
’	O
books	O
.	O
clique	B
matrix	I
prior	O
p	O
(	O
f	O
)	O
since	O
we	O
are	O
interested	O
in	O
clustering	B
,	O
ideally	O
we	O
want	O
to	O
place	O
as	O
many	O
nodes	O
in	O
the	O
graph	B
as	O
possible	O
in	O
a	O
cluster	O
.	O
this	O
means	O
that	O
we	O
wish	O
to	O
bias	B
the	O
contributions	O
to	O
the	O
adjacency	B
matrix	I
a	O
to	O
occur	O
from	O
a	O
small	O
number	O
of	O
columns	O
of	O
f.	O
to	O
achieve	O
this	O
we	O
ﬁrst	O
reparameterise	O
f	O
as	O
f	O
=	O
(	O
cid:0	O
)	O
α1f	O
1	O
,	O
.	O
.	O
.	O
,	O
αcmaxf	O
cmax	O
(	O
cid:1	O
)	O
(	O
20.6.19	O
)	O
(	O
20.6.21	O
)	O
where	O
αc	O
∈	O
{	O
0	O
,	O
1	O
}	O
play	O
the	O
role	O
of	O
indicators	O
and	O
f	O
c	O
is	O
column	O
c	O
of	O
f.	O
cmax	O
is	O
an	O
assumed	O
maximal	O
number	O
of	O
clusters	O
.	O
ideally	O
,	O
we	O
would	O
like	O
to	O
ﬁnd	O
an	O
f	O
with	O
a	O
low	O
number	O
of	O
indicators	O
α1	O
,	O
.	O
.	O
.	O
,	O
αcmax	O
in	O
state	O
1.	O
to	O
achieve	O
this	O
we	O
deﬁne	O
a	O
prior	B
distribution	O
on	O
the	O
binary	O
hypercube	O
α	O
=	O
(	O
α1	O
,	O
.	O
.	O
.	O
,	O
αcmax	O
)	O
,	O
ναc	O
(	O
1	O
−	O
ν	O
)	O
1−αc	O
(	O
20.6.20	O
)	O
to	O
encourage	O
a	O
small	O
number	O
of	O
the	O
α	O
(	O
cid:48	O
)	O
ensure	O
that	O
ν	O
is	O
less	O
than	O
0.5.	O
this	O
gives	O
rise	O
to	O
a	O
beta-bernoulli	O
distribution	B
cs	O
to	O
be	O
1	O
,	O
we	O
use	O
a	O
beta	B
prior	O
p	O
(	O
ν	O
)	O
with	O
suitable	O
parameters	O
to	O
p	O
(	O
α|ν	O
)	O
=	O
(	O
cid:89	O
)	O
(	O
cid:90	O
)	O
c	O
p	O
(	O
α	O
)	O
=	O
p	O
(	O
α|ν	O
)	O
p	O
(	O
ν	O
)	O
=	O
b	O
(	O
a	O
+	O
n	O
,	O
b	O
+	O
cmax	O
−	O
n	O
)	O
where	O
b	O
(	O
a	O
,	O
b	O
)	O
is	O
the	O
beta	B
function	O
and	O
n	O
=	O
(	O
cid:80	O
)	O
cmax	O
b	O
(	O
a	O
,	O
b	O
)	O
ν	O
c=1	O
αc	O
is	O
the	O
number	O
of	O
indicators	O
in	O
state	O
1.	O
to	O
encourage	O
that	O
only	O
a	O
small	O
number	O
of	O
components	O
should	O
be	O
active	B
,	O
we	O
set	O
a	O
=	O
1	O
,	O
b	O
=	O
3	O
(	O
which	O
corresponds	O
to	O
a	O
mean	B
ν	O
of	O
0.25	O
and	O
variance	B
0.0375.	O
the	O
distribution	B
(	O
20.6.21	O
)	O
is	O
on	O
the	O
vertices	O
of	O
the	O
binary	O
hypercube	O
{	O
0	O
,	O
1	O
}	O
cmax	O
with	O
a	O
bias	B
towards	O
vertices	O
close	O
to	O
the	O
origin	O
(	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
)	O
.	O
through	O
equation	B
(	O
20.6.19	O
)	O
,	O
the	O
prior	B
on	O
α	O
induces	O
a	O
prior	B
on	O
f.	O
the	O
resulting	O
distribution	B
p	O
(	O
f	O
,	O
α|a	O
)	O
∝	O
p	O
(	O
f|α	O
)	O
p	O
(	O
α	O
)	O
is	O
formally	O
intractable	O
and	O
in	O
[	O
18	O
]	O
this	O
is	O
addressed	O
using	O
a	O
variational	O
technique	O
.	O
see	O
cliquedecomp.c	O
and	O
cliquedecomp.m	O
.	O
clique	B
matrices	O
also	O
play	O
a	O
natural	B
role	O
in	O
the	O
parameterisation	B
of	O
positive	B
deﬁnite	I
matrices	O
,	O
see	O
exercise	O
(	O
206	O
)	O
[	O
18	O
]	O
.	O
386	O
draft	O
march	O
9	O
,	O
2010	O
20406080100204060801002040608010012014020406080100204060801002040608010005101000	O
years	O
for	O
revenge	O
bush	O
vs.	O
the	O
beltway	O
charlie	O
wilson	O
’	O
s	O
war	O
losing	O
bin	O
laden	O
sleeping	O
with	O
the	O
devil	O
the	O
man	O
who	O
warned	O
america	O
why	O
america	O
slept	O
ghost	O
wars	O
a	O
national	O
party	O
no	O
more	O
bush	O
country	O
dereliction	O
of	O
duty	O
legacy	O
off	O
with	O
their	O
heads	O
persecution	O
rumsfeld	O
’	O
s	O
war	O
breakdown	O
betrayal	O
shut	O
up	O
and	O
sing	O
meant	O
to	O
be	O
the	O
right	O
man	O
ten	O
minutes	O
from	O
normal	B
hillary	O
’	O
s	O
scheme	O
the	O
french	O
betrayal	O
of	O
america	O
tales	O
from	O
the	O
left	O
coast	O
hating	O
america	O
the	O
third	O
terrorist	O
endgame	O
spin	O
sisters	O
all	O
the	O
shah	O
’	O
s	O
men	O
dangerous	O
dimplomacy	O
the	O
price	O
of	O
loyalty	O
house	O
of	O
bush	O
,	O
house	O
of	O
saud	O
the	O
death	O
of	O
right	O
and	O
wrong	O
useful	O
idiots	O
the	O
o	O
’	O
reilly	O
factor	B
let	O
freedom	O
ring	O
those	O
who	O
trespass	O
bias	B
slander	O
the	O
savage	O
nation	O
deliver	O
us	O
from	O
evil	O
give	O
me	O
a	O
break	O
the	O
enemy	O
within	O
the	O
real	O
america	O
who	O
’	O
s	O
looking	O
out	O
for	O
you	O
?	O
the	O
official	O
handbook	O
vast	O
right	O
wing	O
conspiracy	O
power	O
plays	O
arrogance	O
the	O
perfect	O
wife	O
the	O
bushes	O
things	O
worth	O
fighting	O
for	O
surprise	O
,	O
security	O
,	O
the	O
american	O
experience	O
allies	O
why	O
courage	O
matters	O
hollywood	O
interrupted	O
fighting	O
back	O
we	O
will	O
prevail	O
the	O
faith	O
of	O
george	O
w	O
bush	O
rise	O
of	O
the	O
vulcans	O
downsize	O
this	O
!	O
stupid	O
white	O
men	O
rush	O
limbaugh	O
is	O
a	O
big	O
fat	O
idiot	O
the	O
best	O
democracy	O
money	B
can	O
buy	O
the	O
culture	O
of	O
fear	O
america	O
unbound	O
the	O
choice	O
the	O
great	O
unraveling	O
rogue	O
nation	O
soft	B
power	O
colossus	O
the	O
sorrows	O
of	O
empire	O
against	O
all	O
enemies	O
american	O
dynasty	O
big	O
lies	O
the	O
lies	O
of	O
george	O
w.	O
bush	O
worse	O
than	O
watergate	O
plan	O
of	O
attack	O
bush	O
at	O
war	O
the	O
new	O
pearl	O
harbor	O
bushwomen	O
the	O
bubble	O
of	O
american	O
supremacy	O
living	O
history	O
the	O
politics	O
of	O
truth	O
fanatics	O
and	O
fools	O
bushwhacked	O
disarming	O
iraq	O
lies	O
and	O
the	O
lying	O
liars	O
who	O
tell	O
them	O
moveon	O
’	O
s	O
50	O
ways	O
to	O
love	O
your	O
country	O
the	O
buying	O
of	O
the	O
president	O
2004	O
perfectly	O
legal	O
hegemony	O
or	O
survival	O
the	O
exception	O
to	O
the	O
rulers	O
freethinkers	O
had	O
enough	O
?	O
it	O
’	O
s	O
still	O
the	O
economy	O
,	O
stupid	O
!	O
we	O
’	O
re	O
right	O
they	O
’	O
re	O
wrong	O
what	O
liberal	O
media	O
?	O
the	O
clinton	O
wars	O
weapons	O
of	O
mass	O
deception	O
dude	O
,	O
where	O
’	O
s	O
my	O
country	O
?	O
thieves	O
in	O
high	O
places	O
shrub	O
buck	O
up	O
suck	O
up	O
the	O
future	O
of	O
freedom	O
empire	O
exercises	O
example	O
91	O
(	O
political	O
books	O
clustering	B
)	O
.	O
the	O
data	B
consists	O
of	O
105	O
books	O
on	O
us	O
politics	O
sold	O
by	O
the	O
online	B
bookseller	O
amazon	O
.	O
the	O
adjacency	B
matrix	I
with	O
element	O
aij	O
=	O
1	O
ﬁg	O
(	O
20.21a	O
)	O
,	O
represents	O
fre-	O
quent	O
co-purchasing	O
of	O
books	O
i	O
and	O
j	O
(	O
valdis	O
krebs	O
,	O
www.orgnet.com	O
)	O
.	O
additionally	O
,	O
books	O
are	O
la-	O
belled	O
‘	O
liberal	O
’	O
,	O
‘	O
neutral	O
’	O
,	O
or	O
‘	O
conservative	O
’	O
according	O
to	O
the	O
judgement	O
of	O
a	O
politically	O
astute	O
reader	O
(	O
www-personal.umich.edu/∼mejn/netdata/	O
)	O
.	O
the	O
interest	O
is	O
to	O
assign	O
books	O
to	O
clusters	O
,	O
using	O
a	O
alone	O
,	O
and	O
then	O
see	O
if	O
these	O
clusters	O
correspond	O
in	O
some	O
way	O
to	O
the	O
ascribed	O
political	O
leanings	O
of	O
each	O
book	O
.	O
note	O
that	O
the	O
information	O
here	O
is	O
minimal	O
–	O
all	O
that	O
is	O
known	O
to	O
the	O
clustering	B
algorithm	O
is	O
which	O
books	O
were	O
co-bought	O
(	O
matrix	B
a	O
)	O
;	O
no	O
other	O
information	O
on	O
the	O
content	O
or	O
title	O
of	O
the	O
books	O
are	O
exploited	O
by	O
the	O
algorithm	B
.	O
with	O
an	O
initial	O
cmax	O
=	O
200	O
cliques	O
,	O
beta	B
parameters	O
a	O
=	O
1	O
,	O
b	O
=	O
3	O
and	O
steepness	O
β	O
=	O
10	O
,	O
the	O
most	O
probably	O
posterior	B
marginal	O
solution	O
contains	O
142	O
cliques	O
ﬁg	O
(	O
20.21b	O
)	O
,	O
giving	O
a	O
perfect	O
reconstruction	O
of	O
the	O
adjacency	B
a.	O
for	O
comparison	O
,	O
the	O
incidence	B
matrix	I
has	O
441	O
2-cliques	O
.	O
however	O
,	O
this	O
clique	B
matrix	I
is	O
too	O
large	O
to	O
provide	O
a	O
compact	O
interpretation	O
of	O
the	O
data	B
–	O
indeed	O
there	O
are	O
more	O
clusters	O
than	O
books	O
.	O
to	O
cluster	O
the	O
data	B
more	O
aggressively	O
,	O
we	O
ﬁx	O
cmax	O
=	O
10	O
and	O
re-run	O
the	O
algorithm	B
.	O
this	O
results	O
only	O
in	O
an	O
approximate	B
clique	O
decomposition	B
,	O
a	O
≈	O
h	O
(	O
fft	O
)	O
,	O
as	O
plotted	O
in	O
ﬁg	O
(	O
20.21c	O
)	O
.	O
the	O
resulting	O
105	O
×	O
10	O
approximate	B
clique	O
matrix	B
is	O
plotted	O
in	O
ﬁg	O
(	O
20.22	O
)	O
and	O
demonstrates	O
how	O
individual	O
books	O
are	O
present	O
in	O
more	O
than	O
one	O
cluster	O
.	O
interestingly	O
,	O
the	O
clusters	O
found	O
only	O
on	O
the	O
basis	O
of	O
the	O
adjacency	B
matrix	I
have	O
some	O
correspondence	O
with	O
the	O
ascribed	O
political	O
leanings	O
of	O
each	O
book	O
;	O
cliques	O
5	O
,	O
6	O
,	O
7	O
,	O
8	O
,	O
9	O
correspond	O
to	O
largely	O
‘	O
conservative	O
’	O
books	O
.	O
most	O
books	O
belong	O
to	O
more	O
than	O
a	O
single	O
clique/cluster	O
,	O
suggesting	O
that	O
they	O
are	O
not	O
single	O
topic	O
books	O
,	O
consistent	B
with	O
the	O
assumption	O
of	O
a	O
mixed	B
membership	I
model	I
.	O
20.7	O
further	O
reading	O
the	O
literature	O
on	O
mixture	B
modelling	O
is	O
extensive	O
,	O
and	O
a	O
good	O
overview	O
and	O
entrance	O
to	O
the	O
literature	O
is	O
contained	O
in	O
[	O
188	O
]	O
.	O
20.8	O
code	O
mixprodbern.m	O
:	O
em	O
training	B
of	O
a	O
mixture	O
of	O
product	O
bernoulli	O
distributions	O
demomixbernoulli.m	O
:	O
demo	O
of	O
a	O
mixture	O
of	O
product	O
bernoulli	O
distributions	O
gmmem.m	O
:	O
em	O
training	B
of	O
a	O
mixture	O
of	O
gaussians	O
gmmloglik.m	O
:	O
gmm	O
log	O
likelihood	B
demogmmem.m	O
:	O
demo	O
of	O
a	O
em	O
for	O
mixture	O
of	O
gaussians	O
demogmmclass.m	O
:	O
demo	O
gmm	O
for	O
classiﬁcation	B
kmeans.m	O
:	O
k-means	B
demokmeans.m	O
:	O
demo	O
of	O
k-means	B
demopolya.m	O
:	O
demo	O
of	O
the	O
number	O
of	O
active	B
clusters	O
from	O
a	O
polya	O
distribution	B
dirrnd.m	O
:	O
dirichlet	O
random	O
distribution	O
generator	O
cliquedecomp.m	O
:	O
clique	B
matrix	I
decomposition	O
cliquedecomp.c	O
:	O
clique	B
matrix	I
decomposition	O
(	O
c-code	O
)	O
democliquedecomp.m	O
:	O
demo	O
clique	B
matrix	I
decomposition	O
20.9	O
exercises	O
p	O
(	O
v	O
)	O
=	O
(	O
cid:88	O
)	O
p	O
(	O
h	O
)	O
(	O
cid:89	O
)	O
exercise	O
200.	O
consider	O
a	O
mixture	O
of	O
factorised	O
models	O
h	O
i	O
draft	O
march	O
9	O
,	O
2010	O
p	O
(	O
vi|h	O
)	O
(	O
20.9.1	O
)	O
387	O
n	O
(	O
cid:88	O
)	O
n=1	O
n	O
(	O
cid:88	O
)	O
n=1	O
show	O
that	O
,	O
optimally	O
n	O
(	O
cid:88	O
)	O
n=1	O
p	O
(	O
h	O
)	O
=	O
1	O
n	O
(	O
cid:33	O
)	O
(	O
cid:32	O
)	O
1	O
−	O
(	O
cid:88	O
)	O
for	O
assumed	O
i.i.d	O
.	O
data	B
vn	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
some	O
observation	O
components	O
may	O
be	O
missing	B
so	O
that	O
,	O
for	O
example	O
the	O
third	O
component	O
of	O
the	O
ﬁfth	O
datapoint	O
,	O
v5	O
3	O
is	O
unknown	O
.	O
show	O
that	O
maximum	B
likelihood	I
training	O
on	O
the	O
observed	O
data	O
corresponds	O
to	O
ignoring	O
components	O
vn	O
i	O
that	O
are	O
missing	B
.	O
exercise	O
201.	O
derive	O
the	O
optimal	O
em	O
update	O
for	O
ﬁtting	O
a	O
mixture	O
of	O
gaussians	O
under	O
the	O
constraint	O
that	O
the	O
covariances	O
are	O
diagonal	O
.	O
exercises	O
exercise	O
202.	O
consider	O
a	O
mixture	O
of	O
k	O
isotropic	B
gaussians	O
,	O
each	O
with	O
the	O
same	O
covariance	B
,	O
si	O
=	O
σ2i	O
.	O
in	O
the	O
limit	O
σ2	O
→	O
0	O
show	O
that	O
the	O
em	O
algorithm	B
tends	O
to	O
the	O
k-means	B
clustering	O
algorithm	B
.	O
exercise	O
203.	O
consider	O
the	O
term	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
h	O
)	O
(	O
cid:105	O
)	O
pold	O
(	O
h|vn	O
)	O
(	O
20.9.2	O
)	O
we	O
wish	O
to	O
optimise	O
the	O
above	O
with	O
respect	O
to	O
the	O
distribution	B
p	O
(	O
h	O
)	O
.	O
this	O
can	O
be	O
achieved	O
by	O
deﬁning	O
the	O
lagrangian	O
l	O
=	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
h	O
)	O
(	O
cid:105	O
)	O
pold	O
(	O
h|vn	O
)	O
+	O
λ	O
by	O
diﬀerentiating	O
the	O
lagrangian	O
with	O
respect	O
to	O
p	O
(	O
h	O
)	O
and	O
using	O
the	O
normalisation	B
constraint	O
(	O
cid:80	O
)	O
p	O
(	O
h	O
)	O
h	O
(	O
20.9.3	O
)	O
h	O
p	O
(	O
h	O
)	O
=	O
1	O
,	O
pold	O
(	O
h|vn	O
)	O
(	O
20.9.4	O
)	O
exercise	O
204.	O
we	O
showed	O
that	O
ﬁtting	O
an	O
unconstrained	O
mixture	O
of	O
gaussians	O
using	O
maximum	B
likelihood	I
is	O
problematic	O
since	O
,	O
by	O
placing	O
one	O
of	O
the	O
gaussians	O
over	O
a	O
datapoints	O
and	O
letting	O
the	O
covariance	B
determinant	O
go	O
to	O
zero	O
,	O
we	O
obtain	O
an	O
inﬁnite	O
likelihood	O
.	O
in	O
contrast	O
,	O
when	O
ﬁtting	O
a	O
single	O
gaussian	O
n	O
(	O
x	O
µ	O
,	O
σ	O
)	O
to	O
i.i.d	O
.	O
data	B
x1	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
xn	O
show	O
that	O
the	O
maximum	B
likelihood	I
optimum	O
for	O
σ	O
has	O
non-zero	O
determinant	B
,	O
and	O
that	O
the	O
optimal	O
likelihood	B
remains	O
ﬁnite	O
.	O
exercise	O
205.	O
modify	O
gmmem.m	O
suitably	O
so	O
that	O
it	O
can	O
deal	O
with	O
the	O
semi-supervised	B
scenario	O
in	O
which	O
the	O
mixture	B
component	O
h	O
of	O
some	O
of	O
the	O
observations	O
v	O
is	O
known	O
.	O
exercise	O
206.	O
you	O
wish	O
to	O
parameterise	O
covariance	B
matrices	O
s	O
under	O
the	O
constraint	O
that	O
speciﬁed	O
elements	O
are	O
zero	O
.	O
the	O
constraints	O
are	O
speciﬁed	O
using	O
a	O
matrix	B
a	O
with	O
elements	O
aij	O
=	O
0	O
if	O
sij	O
=	O
0	O
and	O
aij	O
=	O
1	O
otherwise	O
.	O
consider	O
a	O
clique	B
matrix	I
z	O
,	O
for	O
which	O
a	O
=	O
h	O
(	O
zzt	O
)	O
and	O
matrix	B
s∗	O
=	O
z∗zt∗	O
with	O
[	O
z∗	O
]	O
ij	O
=	O
(	O
cid:26	O
)	O
0	O
θij	O
if	O
zij	O
=	O
0	O
if	O
zij	O
=	O
1	O
(	O
20.9.5	O
)	O
(	O
20.9.6	O
)	O
(	O
20.9.7	O
)	O
for	O
parameters	O
θ.	O
show	O
that	O
for	O
any	O
θ	O
,	O
s∗	O
is	O
positive	O
semideﬁnite	O
and	O
parameterises	O
covariance	B
matrices	O
under	O
the	O
zero	O
constraints	O
speciﬁed	O
by	O
a	O
.	O
388	O
draft	O
march	O
9	O
,	O
2010	O
chapter	O
21	O
latent	O
linear	O
models	O
21.1	O
factor	B
analysis	I
in	O
chapter	O
(	O
15	O
)	O
we	O
discussed	O
principal	O
components	O
analysis	B
which	O
forms	O
lower	O
dimensional	O
representa-	O
tions	O
of	O
data	B
based	O
on	O
assuming	O
that	O
the	O
data	B
lies	O
close	O
to	O
a	O
hyperplane	B
.	O
here	O
we	O
describe	O
a	O
related	O
probabilistic	B
model	O
for	O
which	O
extensions	O
to	O
bayesian	O
methods	O
can	O
be	O
envisaged	O
.	O
any	O
probabilistic	B
model	O
may	O
also	O
be	O
used	O
as	O
a	O
component	O
of	O
a	O
larger	O
more	O
complex	O
model	B
,	O
such	O
as	O
a	O
mixture	B
model	I
,	O
enabling	O
natural	B
generalisations	O
.	O
we	O
use	O
v	O
to	O
describe	O
a	O
real	O
data	B
vector	O
to	O
emphasise	O
that	O
this	O
is	O
a	O
visible	B
(	O
observable	O
)	O
quantity	O
.	O
the	O
dataset	O
is	O
then	O
given	O
by	O
a	O
set	O
of	O
vectors	O
,	O
v	O
=	O
(	O
cid:8	O
)	O
v1	O
,	O
.	O
.	O
.	O
,	O
vn	O
(	O
cid:9	O
)	O
(	O
21.1.1	O
)	O
where	O
dim	O
(	O
v	O
)	O
=	O
d.	O
our	O
interest	O
is	O
to	O
ﬁnd	O
a	O
lower	O
dimensional	O
probabilistic	O
description	O
of	O
this	O
data	B
.	O
if	O
data	B
lies	O
close	O
to	O
a	O
h-dimensional	O
hyperplane	B
we	O
may	O
accurately	O
approximate	B
each	O
datapoint	O
by	O
a	O
low	O
h-dimensional	O
coordinate	O
system	B
.	O
in	O
general	O
,	O
datapoints	O
will	O
not	O
lie	O
exactly	O
on	O
the	O
hyperplane	B
and	O
we	O
model	B
this	O
discrepancy	O
with	O
gaussian	O
noise	O
.	O
mathematically	O
,	O
the	O
fa	O
model	B
generates	O
an	O
observation	O
v	O
according	O
to	O
v	O
=	O
fh	O
+	O
c	O
+	O
	O
where	O
the	O
noise	O
	O
is	O
gaussian	O
distributed	O
,	O
	O
∼	O
n	O
(	O
	O
0	O
,	O
ψ	O
)	O
(	O
21.1.2	O
)	O
(	O
21.1.3	O
)	O
the	O
constant	O
bias	B
c	O
sets	O
the	O
origin	O
of	O
the	O
coordinate	O
system	B
.	O
the	O
factor	B
loading	I
matrix	O
f	O
plays	O
a	O
similar	O
role	O
as	O
the	O
basis	O
matrix	B
in	O
pca	O
,	O
see	O
section	O
(	O
15.2	O
)	O
.	O
similarly	O
,	O
the	O
hidden	B
coordinates	O
h	O
plays	O
the	O
role	O
of	O
the	O
components	O
we	O
used	O
in	O
section	O
(	O
15.2	O
)	O
.	O
the	O
diﬀerence	O
between	O
pca	O
and	O
factor	B
analysis	I
is	O
in	O
the	O
choice	O
of	O
ψ	O
:	O
probabilistic	B
pca	O
ψ	O
=	O
σ2i	O
factor	B
analysis	I
ψ	O
=	O
diag	O
(	O
ψ1	O
,	O
.	O
.	O
.	O
,	O
ψd	O
)	O
389	O
(	O
21.1.4	O
)	O
(	O
21.1.5	O
)	O
factor	B
analysis	I
h1	O
h2	O
h3	O
v1	O
v2	O
v3	O
v4	O
v5	O
figure	O
21.1	O
:	O
factor	B
analysis	I
.	O
the	O
visible	B
vector	O
variable	B
v	O
is	O
related	O
to	O
the	O
vector	O
hidden	O
variable	B
h	O
by	O
a	O
linear	B
mapping	O
,	O
with	O
independent	O
additive	O
gaussian	O
noise	O
on	O
each	O
visible	B
variable	O
.	O
the	O
prior	B
on	O
the	O
hidden	B
variable	I
may	O
be	O
taken	O
to	O
be	O
an	O
isotropic	B
gaussian	O
,	O
thus	O
being	O
independent	O
across	O
its	O
components	O
.	O
a	O
probabilistic	B
description	O
from	O
equation	B
(	O
21.1.2	O
)	O
and	O
equation	B
(	O
21.1.3	O
)	O
,	O
given	O
h	O
,	O
the	O
data	B
is	O
gaussian	O
distributed	O
with	O
mean	B
fh	O
+	O
c	O
and	O
covariance	B
ψ	O
p	O
(	O
v|h	O
)	O
=	O
n	O
(	O
v	O
fh	O
+	O
c	O
,	O
ψ	O
)	O
∝	O
e	O
−	O
1	O
2	O
(	O
v−fh−c	O
)	O
tψ−1	O
(	O
v−fh−c	O
)	O
(	O
21.1.6	O
)	O
to	O
complete	O
the	O
model	B
,	O
we	O
need	O
to	O
specify	O
the	O
hidden	B
distribution	O
p	O
(	O
h	O
)	O
.	O
a	O
convenient	O
choice	O
is	O
a	O
gaussian	O
p	O
(	O
h	O
)	O
=	O
n	O
(	O
h	O
0	O
,	O
i	O
)	O
∝	O
e	O
−hth/2	O
(	O
21.1.7	O
)	O
under	O
this	O
prior	B
the	O
coordinates	O
h	O
will	O
be	O
preferentially	O
concentrated	O
around	O
values	O
close	O
to	O
0.	O
if	O
we	O
sample	B
a	O
h	O
from	O
p	O
(	O
h	O
)	O
and	O
then	O
draw	O
a	O
value	B
for	O
v	O
using	O
p	O
(	O
v|h	O
)	O
,	O
the	O
sampled	O
v	O
vectors	O
would	O
produce	O
a	O
saucer	O
or	O
‘	O
pancake	O
’	O
of	O
points	O
in	O
the	O
v	O
space	O
.	O
using	O
a	O
correlated	O
gaussian	O
prior	B
p	O
(	O
h	O
)	O
=	O
n	O
(	O
h	O
0	O
,	O
σh	O
)	O
has	O
no	O
eﬀect	O
on	O
the	O
complexity	O
of	O
the	O
model	B
since	O
σh	O
can	O
be	O
absorbed	O
into	O
f	O
,	O
exercise	O
(	O
209	O
)	O
.	O
since	O
v	O
is	O
linearly	O
related	O
to	O
h	O
through	O
equation	B
(	O
21.1.2	O
)	O
and	O
both	O
	O
and	O
h	O
are	O
gaussian	O
,	O
then	O
v	O
is	O
gaussian	O
distributed	O
.	O
the	O
mean	B
and	O
covariance	B
can	O
be	O
computed	O
using	O
the	O
propagation	B
results	O
in	O
section	O
(	O
8.6.3	O
)	O
:	O
(	O
cid:90	O
)	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
p	O
(	O
v	O
)	O
=	O
p	O
(	O
v|h	O
)	O
p	O
(	O
h	O
)	O
dh	O
=	O
n	O
v	O
c	O
,	O
fft	O
+	O
ψ	O
(	O
21.1.8	O
)	O
invariance	O
of	O
the	O
likelihood	B
under	O
factor	B
rotation	I
since	O
the	O
matrix	B
f	O
only	O
appears	O
in	O
the	O
ﬁnal	O
model	B
p	O
(	O
v	O
)	O
through	O
fft	O
+	O
ψ	O
,	O
the	O
likelihood	B
is	O
unchanged	O
if	O
we	O
rotate	O
f	O
using	O
fr	O
,	O
with	O
rrt	O
=	O
i	O
:	O
fr	O
(	O
fr	O
)	O
t	O
+	O
ψ	O
=	O
frrtft	O
+	O
ψ	O
=	O
fft	O
+	O
ψ	O
(	O
21.1.9	O
)	O
the	O
solution	O
space	O
for	O
f	O
is	O
therefore	O
not	O
unique	O
–	O
we	O
can	O
arbitrarily	O
rotate	O
the	O
matrix	B
f	O
and	O
produce	O
an	O
equally	O
likely	O
model	B
of	O
the	O
data	B
.	O
some	O
care	O
is	O
therefore	O
required	O
when	O
interpreting	O
the	O
entries	O
of	O
f.	O
varimax	B
provides	O
a	O
more	O
interpretable	O
f	O
by	O
using	O
a	O
suitable	O
rotation	O
matrix	B
r.	O
the	O
aim	O
is	O
to	O
produce	O
a	O
rotated	O
f	O
for	O
which	O
each	O
column	O
has	O
only	O
a	O
small	O
number	O
of	O
large	O
values	O
.	O
finding	O
a	O
suitable	O
rotation	O
results	O
in	O
a	O
non-linear	B
optimisation	O
problem	B
and	O
needs	O
to	O
be	O
solved	O
numerically	O
.	O
see	O
[	O
185	O
]	O
for	O
details	O
.	O
21.1.1	O
finding	O
the	O
optimal	O
bias	B
for	O
a	O
set	O
of	O
data	B
v	O
and	O
using	O
the	O
usual	O
i.i.d	O
.	O
assumption	O
,	O
the	O
log	O
likelihood	B
is	O
n	O
(	O
cid:88	O
)	O
n=1	O
log	O
p	O
(	O
vn	O
)	O
=	O
−	O
1	O
2	O
n	O
(	O
cid:88	O
)	O
n=1	O
(	O
vn	O
−	O
c	O
)	O
t	O
σ−1	O
d	O
(	O
vn	O
−	O
c	O
)	O
−	O
n	O
2	O
log	O
det	O
(	O
2πσd	O
)	O
(	O
21.1.10	O
)	O
(	O
21.1.11	O
)	O
log	O
p	O
(	O
v|f	O
,	O
ψ	O
)	O
=	O
where	O
σd	O
≡	O
fft	O
+	O
ψ	O
n	O
(	O
cid:88	O
)	O
n=1	O
c	O
=	O
1	O
n	O
390	O
diﬀerentiating	O
equation	B
(	O
21.1.10	O
)	O
with	O
respect	O
to	O
c	O
and	O
equating	O
to	O
zero	O
,	O
we	O
arrive	O
at	O
the	O
maximum	B
likelihood	I
optimal	O
setting	O
that	O
the	O
bias	B
c	O
is	O
the	O
mean	B
of	O
the	O
data	B
,	O
vn	O
≡	O
¯v	O
(	O
21.1.12	O
)	O
draft	O
march	O
9	O
,	O
2010	O
factor	B
analysis	I
:	O
maximum	B
likelihood	I
(	O
a	O
)	O
(	O
b	O
)	O
(	O
a	O
)	O
:	O
1000	O
latent	B
two-dimensional	O
figure	O
21.2	O
:	O
factor	B
analysis	I
:	O
1000	O
points	O
generated	O
from	O
the	O
model	B
.	O
points	O
hn	O
sampled	O
from	O
n	O
(	O
h	O
0	O
,	O
i	O
)	O
.	O
these	O
are	O
transformed	O
to	O
a	O
point	O
on	O
the	O
three-dimensional	O
plane	O
by	O
xn	O
0	O
=	O
c	O
+	O
fhn	O
.	O
the	O
covariance	B
of	O
x0	O
is	O
degenerate	O
,	O
with	O
covariance	O
matrix	B
fft	O
.	O
(	O
b	O
)	O
:	O
for	O
each	O
point	O
xn	O
0	O
on	O
the	O
plane	O
a	O
random	O
noise	O
vector	O
is	O
drawn	O
from	O
n	O
(	O
	O
0	O
,	O
ψ	O
)	O
and	O
added	O
to	O
the	O
in-plane	O
vector	O
to	O
form	O
a	O
sample	B
xn	O
,	O
plotted	O
in	O
red	O
.	O
the	O
distribution	B
of	O
points	O
forms	O
a	O
‘	O
pancake	O
’	O
in	O
space	O
.	O
points	O
‘	O
underneath	O
’	O
the	O
plane	O
are	O
not	O
shown	O
.	O
we	O
will	O
use	O
this	O
setting	O
throughout	O
.	O
with	O
this	O
setting	O
the	O
log	O
likelihood	B
equation	O
(	O
21.1.10	O
)	O
can	O
be	O
written	O
log	O
p	O
(	O
v|f	O
,	O
ψ	O
)	O
=	O
−	O
n	O
2	O
(	O
cid:0	O
)	O
trace	O
(	O
cid:0	O
)	O
σ−1	O
d	O
s	O
(	O
cid:1	O
)	O
+	O
log	O
det	O
(	O
2πσd	O
)	O
(	O
cid:1	O
)	O
where	O
s	O
is	O
the	O
sample	B
covariance	O
matrix	B
n	O
(	O
cid:88	O
)	O
n=1	O
s	O
=	O
1	O
n	O
(	O
v	O
−	O
¯v	O
)	O
(	O
v	O
−	O
¯v	O
)	O
t	O
21.2	O
factor	B
analysis	I
:	O
maximum	B
likelihood	I
we	O
now	O
specialise	O
to	O
the	O
assumption	O
that	O
ψ	O
=	O
diag	O
(	O
ψ1	O
,	O
.	O
.	O
.	O
,	O
ψd	O
)	O
.	O
we	O
consider	O
two	O
methods	O
for	O
learning	B
the	O
factor	B
loadings	O
f	O
:	O
a	O
‘	O
direct	O
’	O
approach1	O
,	O
section	O
(	O
21.2.1	O
)	O
and	O
an	O
em	O
approach	B
,	O
section	O
(	O
21.2.2	O
)	O
.	O
21.2.1	O
direct	O
likelihood	B
optimisation	O
optimal	O
f	O
for	O
ﬁxed	O
ψ	O
to	O
ﬁnd	O
the	O
maximum	B
likelihood	I
setting	O
of	O
f	O
we	O
diﬀerentiate	O
the	O
log	O
likelihood	B
equation	O
(	O
21.1.13	O
)	O
with	O
respect	O
to	O
f	O
and	O
equate	O
to	O
zero	O
.	O
this	O
gives	O
0	O
=	O
trace	O
(	O
cid:0	O
)	O
σ−1	O
d	O
(	O
∂fσd	O
)	O
σ−1	O
d	O
s	O
(	O
cid:1	O
)	O
−	O
trace	O
(	O
cid:0	O
)	O
σ−1	O
d	O
∂fς	O
(	O
cid:1	O
)	O
using	O
∂f	O
(	O
fft	O
)	O
=	O
f	O
(	O
∂fft	O
)	O
+	O
(	O
∂ff	O
)	O
ft	O
,	O
a	O
stationary	B
point	O
is	O
given	O
when	O
σ−1	O
d	O
f	O
=	O
σ−1	O
d	O
sς−1	O
d	O
f	O
since	O
σd	O
is	O
invertible	O
,	O
the	O
optimal	O
f	O
satisﬁes	O
f	O
=	O
sς−1	O
d	O
f	O
using	O
the	O
deﬁnition	O
of	O
σd	O
,	O
equation	B
(	O
21.1.11	O
)	O
,	O
one	O
can	O
rewrite	O
σ−1	O
d	O
f	O
as	O
(	O
see	O
exercise	O
(	O
210	O
)	O
)	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
−1	O
σ−1	O
d	O
f	O
=	O
ψ−1f	O
i	O
+	O
ftψ−1f	O
1the	O
presentation	O
here	O
follows	O
closely	O
that	O
of	O
[	O
302	O
]	O
.	O
draft	O
march	O
9	O
,	O
2010	O
(	O
21.1.13	O
)	O
(	O
21.1.14	O
)	O
(	O
21.2.1	O
)	O
(	O
21.2.2	O
)	O
(	O
21.2.3	O
)	O
(	O
21.2.4	O
)	O
391	O
−4−20246−4−20246−2−101234−4−20246−4−20246−2−101234	O
factor	B
analysis	I
:	O
maximum	B
likelihood	I
plugging	O
this	O
into	O
the	O
zero	O
derivative	O
condition	O
,	O
equation	B
(	O
21.2.3	O
)	O
becomes	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
f	O
i	O
+	O
ftψ−1f	O
=	O
sψ−1f	O
using	O
the	O
reparameterisations	O
equation	B
(	O
21.2.5	O
)	O
can	O
be	O
written	O
in	O
the	O
‘	O
isotropic	B
’	O
form	O
˜s	O
=	O
ψ−	O
1	O
2	O
sψ−	O
1	O
2	O
˜f	O
≡	O
ψ−	O
1	O
2	O
f	O
,	O
(	O
cid:16	O
)	O
i	O
+	O
˜ft	O
˜f	O
˜f	O
(	O
cid:17	O
)	O
=	O
˜s˜f	O
we	O
assume	O
that	O
the	O
transformed	O
factor	B
matrix	O
˜f	O
has	O
a	O
thin	B
svd	O
decomposition	B
˜f	O
=	O
uhlvt	O
where	O
dim	O
uh	O
=	O
d	O
×	O
h	O
,	O
dim	O
l	O
=	O
h	O
×	O
h	O
,	O
dim	O
v	O
=	O
h	O
×	O
h	O
and	O
(	O
21.2.5	O
)	O
(	O
21.2.6	O
)	O
(	O
21.2.7	O
)	O
(	O
21.2.8	O
)	O
ut	O
huh	O
=	O
ih	O
,	O
(	O
21.2.9	O
)	O
and	O
l	O
=	O
diag	O
(	O
l1	O
,	O
.	O
.	O
.	O
,	O
lh	O
)	O
are	O
the	O
singular	B
values	O
of	O
˜f	O
.	O
plugging	O
this	O
assumption	O
into	O
equation	B
(	O
21.2.7	O
)	O
we	O
obtain	O
vtv	O
=	O
ih	O
ih	O
+	O
vl2vt	O
(	O
cid:17	O
)	O
uhlvt	O
(	O
cid:16	O
)	O
(	O
cid:0	O
)	O
ih	O
+	O
l2	O
(	O
cid:1	O
)	O
=	O
˜suh	O
,	O
uh	O
which	O
gives	O
=	O
˜suhlvt	O
l2	O
=	O
diag	O
(	O
cid:0	O
)	O
l2	O
1	O
,	O
.	O
.	O
.	O
,	O
l2	O
h	O
(	O
cid:1	O
)	O
(	O
21.2.10	O
)	O
(	O
21.2.11	O
)	O
equation	B
(	O
21.2.11	O
)	O
is	O
then	O
an	O
eigen-equation	O
for	O
uh	O
.	O
intuitively	O
,	O
it	O
’	O
s	O
clear	O
that	O
we	O
need	O
to	O
ﬁnd	O
then	O
the	O
eigendecomposition	O
of	O
˜s	O
and	O
then	O
set	O
the	O
columns	O
of	O
uh	O
to	O
those	O
eigenvectors	O
corresponding	O
to	O
the	O
largest	O
eigenvalues	O
.	O
this	O
is	O
derived	O
more	O
formally	O
below	O
.	O
determining	O
the	O
appropriate	O
eigenvalues	O
we	O
can	O
relate	O
the	O
form	O
of	O
the	O
solution	O
to	O
the	O
eigen-decomposition	O
of	O
˜s	O
˜s	O
=	O
uλut	O
,	O
u	O
=	O
[	O
uh|ur	O
]	O
(	O
21.2.12	O
)	O
where	O
ur	O
are	O
arbitrary	O
additional	O
columns	O
chosen	O
to	O
complete	O
uh	O
to	O
form	O
an	O
orthogonal	B
u	O
,	O
utu	O
=	O
i	O
=	O
λi	O
,	O
or	O
li	O
=	O
√λi	O
−	O
1.	O
given	O
uut	O
=	O
i.	O
using	O
λ	O
=	O
diag	O
(	O
λ1	O
,	O
.	O
.	O
.	O
,	O
λd	O
)	O
,	O
equation	B
(	O
21.2.11	O
)	O
stipulates	O
1	O
+	O
l2	O
the	O
solution	O
for	O
˜f	O
,	O
the	O
solution	O
for	O
f	O
is	O
found	O
from	O
equation	B
(	O
21.2.6	O
)	O
.	O
to	O
determine	O
the	O
optimal	O
λi	O
we	O
write	O
the	O
log	O
likelihood	B
in	O
terms	O
of	O
the	O
λi	O
as	O
follows	O
.	O
using	O
the	O
new	O
parameterisation	B
,	O
(	O
21.2.13	O
)	O
(	O
21.2.14	O
)	O
+	O
log	O
det	O
(	O
2πψ	O
)	O
(	O
21.2.15	O
)	O
(	O
21.2.16	O
)	O
draft	O
march	O
9	O
,	O
2010	O
σd	O
=	O
ψ	O
1	O
2	O
ψ	O
1	O
2	O
(	O
cid:16	O
)	O
˜f˜ft	O
+	O
i	O
(	O
cid:17	O
)	O
d	O
s	O
(	O
cid:1	O
)	O
=	O
trace	O
2	O
,	O
we	O
have	O
and	O
s	O
=	O
ψ	O
1	O
2	O
˜sψ	O
1	O
trace	O
(	O
cid:0	O
)	O
σ−1	O
2	O
n	O
−	O
log	O
p	O
(	O
v|f	O
,	O
ψ	O
)	O
=	O
trace	O
the	O
log	O
likelihood	B
equation	O
(	O
21.1.10	O
)	O
in	O
this	O
new	O
parameterisation	B
is	O
(	O
cid:18	O
)	O
(	O
cid:16	O
)	O
˜f˜ft	O
+	O
id	O
(	O
cid:18	O
)	O
(	O
cid:16	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
−1	O
˜s	O
id	O
+	O
˜f˜ft	O
(	O
cid:17	O
)	O
−1	O
˜s	O
(	O
cid:19	O
)	O
(	O
cid:16	O
)	O
id	O
+	O
˜f˜ft	O
(	O
cid:17	O
)	O
+	O
log	O
det	O
using	O
λi	O
=	O
1	O
+	O
l2	O
i	O
,	O
and	O
equation	B
(	O
21.2.8	O
)	O
we	O
can	O
write	O
id	O
+	O
˜f˜ft	O
=	O
id	O
+	O
uhl2ut	O
h	O
=	O
udiag	O
(	O
λ1	O
,	O
.	O
.	O
.	O
,	O
λh	O
,	O
1	O
,	O
.	O
.	O
.	O
,	O
1	O
)	O
ut	O
392	O
(	O
cid:26	O
)	O
λi	O
1	O
(	O
cid:48	O
)	O
i	O
=	O
λ	O
i	O
≤	O
h	O
i	O
>	O
h	O
=	O
(	O
cid:88	O
)	O
i	O
λi	O
λ	O
(	O
cid:48	O
)	O
i	O
,	O
log	O
λi	O
(	O
21.2.17	O
)	O
(	O
21.2.18	O
)	O
using	O
this	O
we	O
can	O
write	O
the	O
log	O
likelihood	B
as	O
a	O
function	B
of	O
the	O
eigenvalues	O
(	O
for	O
ﬁxed	O
ψ	O
)	O
as	O
to	O
maximise	O
the	O
likelihood	B
we	O
need	O
to	O
minimise	O
the	O
right	O
hand	O
side	O
of	O
the	O
above	O
.	O
since	O
log	O
λ	O
<	O
λ	O
we	O
i	O
log	O
λi	O
term	O
.	O
a	O
solution	O
for	O
ﬁxed	O
ψ	O
is	O
therefore	O
λi	O
+	O
log	O
det	O
(	O
2πψ	O
)	O
(	O
21.2.19	O
)	O
factor	B
analysis	I
:	O
maximum	B
likelihood	I
so	O
that	O
trace	O
similarly	O
log	O
det	O
(	O
cid:18	O
)	O
(	O
cid:16	O
)	O
(	O
cid:19	O
)	O
id	O
+	O
˜f˜ft	O
(	O
cid:17	O
)	O
−1	O
˜s	O
id	O
+	O
˜f˜ft	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
h	O
(	O
cid:88	O
)	O
h	O
(	O
cid:88	O
)	O
=	O
i=1	O
d	O
(	O
cid:88	O
)	O
should	O
place	O
the	O
largest	O
h	O
eigenvalues	O
in	O
the	O
(	O
cid:80	O
)	O
log	O
p	O
(	O
v|f	O
,	O
ψ	O
)	O
=	O
log	O
λi	O
+	O
h	O
+	O
2	O
n	O
−	O
i=1	O
i=h+1	O
f	O
=	O
ψ	O
1	O
2	O
uh	O
(	O
λh	O
−	O
ih	O
)	O
1	O
2	O
r	O
where	O
λh	O
≡	O
diag	O
(	O
λ1	O
,	O
.	O
.	O
.	O
,	O
λh	O
)	O
are	O
the	O
h	O
largest	O
eigenvalues	O
of	O
ψ−	O
1	O
r	O
is	O
an	O
arbitrary	O
orthogonal	B
matrix	O
.	O
2	O
sψ−	O
1	O
2	O
,	O
with	O
uh	O
being	O
the	O
matrix	B
of	O
the	O
corresponding	O
eigenvectors	O
.	O
svd	O
based	O
approach	B
rather	O
than	O
ﬁnding	O
the	O
eigen-decomposition	O
of	O
ψ−	O
1	O
considering	O
the	O
thin	B
svd	O
decomposition	B
of	O
2	O
sψ−	O
1	O
2	O
we	O
can	O
avoid	O
forming	O
the	O
covariance	B
matrix	O
by	O
(	O
21.2.20	O
)	O
(	O
21.2.21	O
)	O
(	O
21.2.22	O
)	O
(	O
21.2.23	O
)	O
(	O
21.2.25	O
)	O
(	O
21.2.26	O
)	O
where	O
the	O
data	B
matrix	O
is	O
˜x	O
=	O
1	O
√n	O
ψ−	O
1	O
2	O
x	O
(	O
cid:2	O
)	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
(	O
cid:3	O
)	O
x	O
≡	O
given	O
a	O
thin	B
decomposition	O
˜x	O
=	O
uh	O
˜λvt	O
we	O
obtain	O
the	O
eigenvalues	O
λi	O
=	O
˜λ2	O
using	O
this	O
svd	O
method	O
is	O
o	O
svd	O
methods	O
are	O
available	O
[	O
49	O
]	O
.	O
finding	O
the	O
optimal	O
ψ	O
(	O
cid:16	O
)	O
s	O
−	O
fft	O
(	O
cid:17	O
)	O
s	O
−	O
fft	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
ψ	O
=	O
diag	O
ψnew	O
=	O
diag	O
(	O
cid:16	O
)	O
min	O
(	O
h	O
,	O
n	O
)	O
3	O
(	O
cid:17	O
)	O
(	O
21.2.24	O
)	O
ii	O
.	O
for	O
d	O
>	O
n	O
this	O
is	O
convenient	O
since	O
the	O
computational	B
complexity	I
.	O
when	O
the	O
matrix	B
x	O
is	O
too	O
large	O
to	O
store	O
in	O
memory	O
,	O
online	B
the	O
zero	O
derivative	O
of	O
the	O
log	O
likelihood	B
occurs	O
when	O
where	O
f	O
is	O
given	O
by	O
equation	B
(	O
21.2.20	O
)	O
.	O
there	O
is	O
no	O
closed	O
form	O
solution	O
to	O
equations	O
(	O
21.2.25	O
,	O
21.2.20	O
)	O
.	O
a	O
simple	O
iterative	O
scheme	O
is	O
to	O
ﬁrst	O
guess	O
values	O
for	O
the	O
diagonal	O
entries	O
of	O
ψ	O
and	O
then	O
ﬁnd	O
the	O
optimal	O
f	O
using	O
equation	B
(	O
21.2.20	O
)	O
.	O
subsequently	O
ψ	O
is	O
updated	O
using	O
we	O
update	O
f	O
using	O
equation	B
(	O
21.2.20	O
)	O
and	O
ψ	O
using	O
equation	B
(	O
21.2.26	O
)	O
until	O
convergence	O
.	O
alternative	O
schemes	O
for	O
updating	O
the	O
noise	O
matrix	B
ψ	O
can	O
improve	O
convergence	O
considerably	O
.	O
for	O
ex-	O
ample	O
updating	O
only	O
a	O
single	O
component	O
of	O
ψ	O
with	O
the	O
rest	O
ﬁxed	O
can	O
be	O
achieved	O
using	O
a	O
closed	O
form	O
expression	O
[	O
302	O
]	O
.	O
draft	O
march	O
9	O
,	O
2010	O
393	O
21.2.2	O
expectation	B
maximisation	I
a	O
popular	O
way	O
to	O
train	O
factor	O
analysis	B
in	O
machine	O
learning	B
is	O
to	O
use	O
em	O
.	O
we	O
assume	O
that	O
the	O
bias	B
c	O
has	O
been	O
optimally	O
set	O
to	O
the	O
data	B
mean	O
¯v	O
.	O
factor	B
analysis	I
:	O
maximum	B
likelihood	I
m-step	O
as	O
usual	O
,	O
we	O
need	O
to	O
consider	O
the	O
energy	B
which	O
,	O
neglecting	O
constants	O
,	O
is	O
e	O
(	O
f	O
,	O
ψ	O
)	O
=	O
−	O
(	O
dn	O
−	O
fh	O
)	O
t	O
ψ−1	O
(	O
dn	O
−	O
fh	O
)	O
q	O
(	O
h|vn	O
)	O
−	O
n	O
2	O
log	O
det	O
(	O
ψ	O
)	O
(	O
21.2.27	O
)	O
n	O
(	O
cid:88	O
)	O
(	O
cid:28	O
)	O
1	O
2	O
n=1	O
(	O
cid:29	O
)	O
where	O
dn	O
≡	O
vn	O
−	O
¯v	O
.	O
the	O
optimal	O
variational	O
distribution	O
q	O
(	O
h|vn	O
)	O
is	O
determined	O
by	O
the	O
e-step	O
below	O
.	O
maximising	O
e	O
(	O
f	O
,	O
ψ	O
)	O
with	O
respect	O
to	O
f	O
gives	O
fnew	O
=	O
ah−1	O
where	O
a	O
≡	O
1	O
n	O
(	O
cid:88	O
)	O
n	O
finally	O
ψnew	O
=	O
1	O
n	O
1	O
n	O
h	O
≡	O
(	O
cid:68	O
)	O
hht	O
(	O
cid:69	O
)	O
(	O
cid:88	O
)	O
(	O
dn	O
−	O
fh	O
)	O
(	O
dn	O
−	O
fh	O
)	O
t	O
(	O
cid:69	O
)	O
n	O
q	O
(	O
h|vn	O
)	O
q	O
(	O
h|vn	O
)	O
,	O
dn	O
(	O
cid:104	O
)	O
h	O
(	O
cid:105	O
)	O
t	O
(	O
cid:88	O
)	O
diag	O
n	O
(	O
cid:18	O
)	O
(	O
cid:68	O
)	O
q	O
(	O
h|vn	O
)	O
(	O
cid:19	O
)	O
=	O
diag	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
n	O
1	O
n	O
(	O
21.2.28	O
)	O
(	O
21.2.29	O
)	O
(	O
cid:33	O
)	O
dn	O
(	O
dn	O
)	O
t	O
−	O
2fat	O
+	O
fhft	O
(	O
21.2.30	O
)	O
e-step	O
the	O
above	O
recursions	O
depend	O
on	O
the	O
statistics	O
(	O
cid:104	O
)	O
h	O
(	O
cid:105	O
)	O
q	O
(	O
h|vn	O
)	O
and	O
(	O
cid:10	O
)	O
hht	O
(	O
cid:11	O
)	O
for	O
the	O
e-step	O
we	O
have	O
q	O
(	O
h|vn	O
)	O
.	O
using	O
the	O
em	O
optimal	O
choice	O
with	O
q	O
(	O
h|vn	O
)	O
∝	O
p	O
(	O
vn|h	O
)	O
p	O
(	O
h	O
)	O
=	O
n	O
(	O
h	O
mn	O
,	O
σ	O
)	O
(	O
cid:17	O
)	O
−1	O
(	O
cid:16	O
)	O
i	O
+	O
ftψ−1f	O
ftψ−1dn	O
,	O
(	O
cid:16	O
)	O
σ	O
=	O
i	O
+	O
ftψ−1f	O
(	O
cid:17	O
)	O
−1	O
using	O
these	O
results	O
we	O
can	O
express	O
the	O
statistics	O
in	O
equation	B
(	O
21.2.29	O
)	O
as	O
mn	O
=	O
(	O
cid:104	O
)	O
h	O
(	O
cid:105	O
)	O
q	O
(	O
h|vn	O
)	O
=	O
(	O
cid:88	O
)	O
h	O
=	O
σ	O
+	O
1	O
n	O
n	O
mn	O
(	O
mn	O
)	O
t	O
(	O
21.2.31	O
)	O
(	O
21.2.32	O
)	O
(	O
21.2.33	O
)	O
equations	O
(	O
21.2.28,21.2.30,21.2.32	O
)	O
are	O
iterated	O
till	O
convergence	O
.	O
as	O
for	O
any	O
em	O
algorithm	B
,	O
the	O
likelihood	B
equation	O
(	O
21.1.10	O
)	O
(	O
under	O
the	O
diagonal	O
constraint	O
on	O
ψ	O
)	O
increases	O
at	O
each	O
iteration	B
.	O
convergence	O
using	O
this	O
em	O
technique	O
can	O
be	O
slower	O
than	O
that	O
of	O
the	O
direct	O
eigen-approach	O
of	O
section	O
(	O
21.2.1	O
)	O
and	O
commercial	O
implementations	O
usually	O
avoid	O
em	O
for	O
this	O
reason	O
.	O
provided	O
however	O
that	O
a	O
reasonable	O
initialisation	O
is	O
used	O
,	O
the	O
performance	B
of	O
the	O
two	O
training	B
algorithms	O
can	O
be	O
similar	O
.	O
a	O
useful	O
initialisation	O
is	O
to	O
use	O
pca	O
and	O
then	O
set	O
f	O
to	O
the	O
principal	B
directions	I
.	O
mixtures	O
of	O
fa	O
an	O
advantage	O
of	O
probabilistic	B
models	O
is	O
that	O
they	O
may	O
be	O
used	O
as	O
components	O
in	O
more	O
complex	O
models	O
,	O
such	O
as	O
mixtures	O
of	O
fa	O
[	O
275	O
]	O
.	O
training	B
can	O
then	O
be	O
achieved	O
using	O
em	O
or	O
gradient	B
based	O
approaches	O
.	O
bayesian	O
extensions	O
are	O
clearly	O
of	O
interest	O
;	O
whilst	O
formally	O
intractable	O
they	O
can	O
be	O
addressed	O
using	O
approximate	B
methods	O
,	O
for	O
example	O
[	O
98	O
,	O
178	O
,	O
109	O
]	O
.	O
394	O
draft	O
march	O
9	O
,	O
2010	O
interlude	O
:	O
modelling	B
faces	O
g	O
¯x12	O
¯x13	O
f1	O
¯x11	O
g	O
¯x21	O
f2	O
µ	O
¯x22	O
origin	O
θ	O
f	O
figure	O
21.3	O
:	O
latent	B
identity	O
model	B
.	O
the	O
mean	B
µ	O
rep-	O
resents	O
the	O
mean	B
of	O
the	O
faces	B
.	O
the	O
subspace	O
f	O
repre-	O
sents	O
the	O
directions	O
of	O
variation	O
of	O
diﬀerent	O
faces	B
so	O
that	O
f1	O
=	O
µ	O
+	O
fh1	O
is	O
a	O
mean	B
face	O
for	O
individual	O
1	O
,	O
and	O
similarly	O
for	O
f2	O
=	O
µ+fh2	O
.	O
the	O
subspace	O
g	O
denotes	O
the	O
directions	O
of	O
variability	O
for	O
any	O
individual	O
face	O
,	O
caused	O
by	O
pose	O
,	O
lighting	O
etc	O
.	O
this	O
variability	O
is	O
assumed	O
the	O
same	O
for	O
each	O
person	O
.	O
a	O
particular	O
mean	B
face	O
is	O
then	O
given	O
by	O
the	O
mean	B
face	O
of	O
the	O
person	O
plus	O
pose/illumination	O
variation	O
,	O
for	O
example	O
¯x12	O
=	O
f1	O
+	O
gw12	O
.	O
a	O
sample	B
face	O
is	O
then	O
given	O
by	O
a	O
mean	B
face	O
¯xij	O
plus	O
gaussian	O
noise	O
from	O
n	O
(	O
ij	O
0	O
,	O
σ	O
)	O
.	O
wij	O
hi	O
xij	O
j	O
i	O
figure	O
21.4	O
:	O
the	O
jth	O
image	O
of	O
the	O
ith	O
person	O
,	O
xij	O
,	O
is	O
modelled	O
using	O
a	O
linear	B
latent	O
model	B
with	O
parameters	O
θ	O
.	O
21.3	O
interlude	O
:	O
modelling	B
faces	O
factor	B
analysis	I
has	O
widespread	O
application	O
in	O
statistics	O
and	O
machine	O
learning	B
.	O
as	O
an	O
inventive	O
application	O
of	O
fa	O
,	O
highlighting	O
the	O
probabilistic	B
nature	O
of	O
the	O
model	B
,	O
we	O
describe	O
a	O
face	O
modelling	O
technique	O
that	O
has	O
as	O
its	O
heart	O
a	O
latent	B
linear	I
model	I
[	O
228	O
]	O
.	O
consider	O
a	O
gallery	O
of	O
face	O
images	O
x	O
=	O
{	O
xij	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
i	O
;	O
j	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
j	O
}	O
so	O
that	O
the	O
vector	O
xij	O
represents	O
the	O
jth	O
image	O
of	O
the	O
ith	O
person	O
.	O
as	O
a	O
latent	B
linear	I
model	I
of	O
faces	B
we	O
consider	O
xij	O
=	O
µ	O
+	O
fhi	O
+	O
gwij	O
+	O
ij	O
(	O
21.3.1	O
)	O
here	O
f	O
(	O
dim	O
f	O
=	O
d	O
×	O
f	O
)	O
is	O
used	O
to	O
model	B
variability	O
between	O
people	O
,	O
and	O
g	O
(	O
dim	O
g	O
=	O
d	O
×	O
g	O
)	O
models	O
variability	O
related	O
to	O
pose	O
,	O
illumination	O
etc	O
.	O
within	O
the	O
diﬀerent	O
images	O
of	O
each	O
person	O
.	O
the	O
contribution	O
fi	O
≡	O
µ	O
+	O
fhi	O
accounts	O
for	O
variability	O
between	O
diﬀerent	O
people	O
,	O
being	O
constant	O
for	O
individual	O
i.	O
for	O
ﬁxed	O
i	O
,	O
the	O
contri-	O
bution	O
gwij	O
+	O
ij	O
(	O
21.3.3	O
)	O
accounts	O
for	O
the	O
variability	O
over	O
the	O
images	O
of	O
person	O
i	O
,	O
explaining	O
why	O
two	O
images	O
of	O
the	O
same	O
person	O
do	O
not	O
look	O
identical	O
.	O
see	O
ﬁg	O
(	O
21.3	O
)	O
for	O
a	O
graphical	O
representation	B
.	O
as	O
a	O
probabilistic	B
linear	O
latent	B
variable	I
model	O
,	O
we	O
have	O
for	O
an	O
image	O
xij	O
:	O
(	O
21.3.2	O
)	O
(	O
21.3.4	O
)	O
(	O
21.3.5	O
)	O
(	O
21.3.6	O
)	O
(	O
21.3.7	O
)	O
395	O
the	O
parameters	O
are	O
θ	O
=	O
{	O
f	O
,	O
g	O
,	O
µ	O
,	O
σ	O
}	O
.	O
for	O
the	O
collection	O
of	O
images	O
,	O
assuming	O
i.i.d	O
.	O
data	B
,	O
p	O
(	O
xij|hi	O
,	O
wij	O
,	O
θ	O
)	O
=	O
n	O
(	O
xij	O
µ	O
+	O
fhi	O
+	O
gwij	O
,	O
σ	O
)	O
p	O
(	O
hi	O
)	O
=	O
n	O
(	O
hi	O
0	O
,	O
i	O
)	O
,	O
p	O
(	O
wij	O
)	O
=	O
n	O
(	O
wij	O
0	O
,	O
i	O
)	O
i	O
(	O
cid:89	O
)	O
p	O
(	O
xij|hi	O
,	O
wij	O
,	O
θ	O
)	O
p	O
(	O
wij	O
)	O
	O
j	O
(	O
cid:89	O
)	O
j=1	O
i=1	O
	O
p	O
(	O
hi	O
)	O
for	O
which	O
the	O
graphical	O
model	B
is	O
depicted	O
in	O
ﬁg	O
(	O
21.4	O
)	O
.	O
the	O
task	O
of	O
learning	B
is	O
then	O
to	O
maximise	O
the	O
likelihood	B
p	O
(	O
x	O
,	O
w	O
,	O
h|θ	O
)	O
=	O
(	O
cid:90	O
)	O
p	O
(	O
x|θ	O
)	O
=	O
p	O
(	O
x	O
,	O
w	O
,	O
h|θ	O
)	O
w	O
,	O
h	O
draft	O
march	O
9	O
,	O
2010	O
interlude	O
:	O
modelling	B
faces	O
(	O
a	O
)	O
mean	B
(	O
b	O
)	O
variance	B
(	O
c	O
)	O
(	O
d	O
)	O
(	O
e	O
)	O
(	O
f	O
)	O
(	O
g	O
)	O
(	O
h	O
)	O
figure	O
21.5	O
:	O
latent	B
identity	O
model	B
of	O
face	O
images	O
.	O
each	O
image	O
is	O
represented	O
by	O
a	O
70	O
×	O
70	O
×	O
3	O
vector	O
(	O
the	O
3	O
comes	O
from	O
the	O
rgb	O
colour	O
coding	O
)	O
.	O
there	O
are	O
i	O
=	O
195	O
individuals	O
in	O
the	O
database	O
and	O
j	O
=	O
4	O
(	O
b	O
)	O
:	O
per	O
pixel	O
standard	B
deviation	I
–	O
black	O
is	O
low	O
,	O
white	O
is	O
images	O
per	O
person	O
.	O
(	O
f	O
,	O
g	O
,	O
h	O
)	O
:	O
three	O
samples	O
from	O
high	O
.	O
the	O
model	B
with	O
h	O
ﬁxed	O
and	O
drawing	O
randomly	O
from	O
w	O
in	O
the	O
within-individual	O
subspace	O
g.	O
reproduced	O
from	O
[	O
228	O
]	O
.	O
(	O
c	O
,	O
d	O
,	O
e	O
)	O
:	O
three	O
directions	O
from	O
the	O
between-individual	O
subspace	O
f.	O
(	O
a	O
)	O
:	O
mean	B
of	O
the	O
data	B
.	O
this	O
model	B
can	O
be	O
seem	O
as	O
a	O
constrained	O
version	O
of	O
factor	B
analysis	I
by	O
using	O
stacked	O
vectors	O
(	O
here	O
for	O
only	O
a	O
single	O
individual	O
,	O
i	O
=	O
1	O
)	O
	O
x11	O
x12	O
...	O
x1j	O
	O
=	O
	O
µ	O
µ	O
...	O
µ	O
	O
+	O
	O
f	O
g	O
0	O
.	O
.	O
.	O
0	O
f	O
0	O
g	O
.	O
.	O
.	O
0	O
...	O
...	O
...	O
...	O
0	O
.	O
.	O
.	O
g	O
f	O
0	O
...	O
	O
	O
h1	O
w11	O
w12	O
...	O
w1j	O
	O
+	O
	O
11	O
12	O
...	O
1j	O
	O
(	O
21.3.8	O
)	O
the	O
generalisation	B
to	O
multiple	O
individuals	O
i	O
>	O
1	O
is	O
straightforward	O
.	O
the	O
model	B
can	O
be	O
trained	O
using	O
either	O
a	O
constrained	O
form	O
of	O
the	O
direct	O
method	O
,	O
or	O
em	O
as	O
described	O
in	O
[	O
228	O
]	O
.	O
example	O
images	O
from	O
the	O
trained	O
model	B
are	O
presented	O
in	O
ﬁg	O
(	O
21.5	O
)	O
.	O
recognition	O
in	O
closed	O
set	O
face	O
recognition	O
a	O
new	O
‘	O
probe	O
’	O
face	O
x∗	O
is	O
to	O
be	O
matched	O
to	O
a	O
person	O
n	O
in	O
the	O
gallery	O
of	O
training	B
faces	O
.	O
in	O
model	B
mn	O
the	O
nth	O
gallery	O
face	O
is	O
forced	O
to	O
share	O
its	O
latent	B
identity	O
variable	B
hn	O
with	O
the	O
test	O
face	O
,	O
indicating	O
that	O
these	O
faces	B
belong	O
to	O
the	O
same	O
person2	O
.	O
assuming	O
a	O
single	O
exemplar	O
per	O
person	O
(	O
j	O
=	O
1	O
)	O
,	O
i	O
(	O
cid:89	O
)	O
i=1	O
,	O
i	O
(	O
cid:54	O
)	O
=n	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xi	O
,	O
x∗|mn	O
)	O
=	O
p	O
(	O
xn	O
,	O
x∗	O
)	O
p	O
(	O
xi	O
)	O
(	O
21.3.9	O
)	O
bayes	O
’	O
rule	O
then	O
gives	O
the	O
posterior	B
class	O
assignment	O
p	O
(	O
mn|x1	O
,	O
.	O
.	O
.	O
,	O
xi	O
,	O
x∗	O
)	O
∝	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xi	O
,	O
x∗|mn	O
)	O
p	O
(	O
mn	O
)	O
(	O
21.3.10	O
)	O
for	O
a	O
uniform	B
prior	O
,	O
the	O
term	O
p	O
(	O
mn	O
)	O
is	O
constant	O
and	O
can	O
be	O
neglected	O
.	O
the	O
quantities	O
we	O
require	O
above	O
are	O
given	O
by	O
p	O
(	O
xn	O
)	O
=	O
p	O
(	O
xn	O
,	O
hn	O
,	O
wn	O
)	O
,	O
p	O
(	O
x∗	O
)	O
=	O
p	O
(	O
x∗	O
,	O
h∗	O
,	O
w∗	O
)	O
(	O
21.3.11	O
)	O
(	O
cid:90	O
)	O
hnwn	O
(	O
cid:90	O
)	O
h∗w∗	O
2this	O
is	O
analogous	O
to	O
bayesian	O
outcome	B
analysis	I
in	O
section	O
(	O
13.5	O
)	O
in	O
which	O
the	O
hypotheses	O
assume	O
that	O
either	O
the	O
errors	O
were	O
generated	O
from	O
the	O
same	O
or	O
a	O
diﬀerent	O
model	B
.	O
396	O
draft	O
march	O
9	O
,	O
2010	O
probabilistic	B
principal	O
components	O
analysis	B
h1	O
x1	O
h2	O
x2	O
h1	O
x1	O
x∗	O
h2	O
x2	O
w1	O
w2	O
w∗	O
w1	O
w2	O
(	O
a	O
)	O
m1	O
(	O
b	O
)	O
m2	O
(	O
cid:90	O
)	O
hnwn	O
,	O
w∗	O
p	O
(	O
xn	O
,	O
x∗	O
)	O
=	O
x∗	O
w∗	O
(	O
a	O
)	O
:	O
figure	O
21.6	O
:	O
face	O
recognition	O
model	B
(	O
de-	O
picted	O
only	O
for	O
a	O
single	O
examplar	O
per	O
per-	O
son	O
,	O
j	O
=	O
1	O
)	O
.	O
in	O
model	B
m1	O
the	O
test	O
image	O
(	O
or	O
‘	O
probe	O
’	O
)	O
x∗	O
is	O
assumed	O
to	O
be	O
from	O
person	O
1	O
,	O
albeit	O
with	O
a	O
diﬀerent	O
(	O
b	O
)	O
:	O
for	O
model	B
m2	O
pose/illumination	O
.	O
the	O
test	O
image	O
is	O
assumed	O
to	O
be	O
from	O
per-	O
son	O
2.	O
one	O
calculates	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x∗|m1	O
)	O
and	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x∗|m2	O
)	O
and	O
then	O
uses	O
bayes	O
’	O
rule	O
to	O
infer	O
which	O
person	O
the	O
test	O
image	O
x∗	O
most	O
likely	O
belongs	O
.	O
p	O
(	O
xn	O
,	O
x∗	O
,	O
hn	O
,	O
wn	O
,	O
w∗	O
)	O
(	O
21.3.12	O
)	O
where	O
p	O
(	O
x	O
,	O
h	O
,	O
w	O
)	O
is	O
obtained	O
from	O
equation	B
(	O
21.3.6	O
)	O
(	O
where	O
we	O
assume	O
the	O
parameters	O
θ	O
are	O
ﬁxed	O
,	O
having	O
been	O
learned	O
using	O
maximum	B
likelihood	I
)	O
.	O
note	O
that	O
in	O
equation	B
(	O
21.3.12	O
)	O
we	O
do	O
not	O
introduce	O
h∗	O
since	O
both	O
xn	O
and	O
x∗	O
are	O
assumed	O
to	O
be	O
generated	O
from	O
the	O
same	O
person	O
with	O
the	O
latent	B
identity	O
hn	O
.	O
these	O
marginal	B
probabilities	O
are	O
straightforward	O
to	O
derive	O
since	O
they	O
are	O
marginals	O
of	O
gaussians	O
.	O
in	O
practice	O
,	O
the	O
best	O
results	O
are	O
obtained	O
using	O
a	O
between-individual	O
subspace	O
dimension	O
f	O
and	O
within-	O
individual	O
subspace	O
dimension	O
g	O
both	O
equal	O
to	O
128.	O
this	O
model	B
then	O
has	O
performance	B
competitive	O
with	O
the	O
state-of-the-art	O
[	O
228	O
]	O
.	O
a	O
beneﬁt	O
of	O
the	O
probabilistic	B
model	O
is	O
that	O
the	O
extension	O
to	O
mixtures	O
of	O
this	O
model	B
is	O
essentially	O
straightforward	O
,	O
which	O
boosts	O
performance	B
further	O
.	O
related	O
models	O
can	O
also	O
be	O
used	O
for	O
the	O
‘	O
open	O
set	O
’	O
face	O
recognition	O
problem	B
in	O
which	O
the	O
probe	O
face	O
may	O
or	O
may	O
not	O
belong	O
to	O
one	O
of	O
the	O
individuals	O
in	O
the	O
database	O
[	O
228	O
]	O
.	O
21.4	O
probabilistic	B
principal	O
components	O
analysis	B
ppca	O
corresponds	O
to	O
factor	B
analysis	I
under	O
the	O
restriction	O
ψ	O
=	O
σ2id	O
.	O
plugging	O
this	O
assumption	O
into	O
the	O
direct	O
optimisation	B
solution	O
equation	B
(	O
21.2.20	O
)	O
gives	O
f	O
=	O
σuh	O
(	O
λh	O
−	O
ih	O
)	O
1	O
2	O
r	O
(	O
21.4.1	O
)	O
where	O
the	O
eigenvalues	O
(	O
diagonal	O
entries	O
of	O
λh	O
)	O
and	O
corresponding	O
eigenvectors	O
(	O
columns	O
of	O
uh	O
)	O
are	O
the	O
largest	O
eigenvalues	O
of	O
σ−2s	O
.	O
since	O
the	O
eigenvalues	O
of	O
σ−2s	O
are	O
those	O
of	O
s	O
simply	O
scaled	O
by	O
σ−2	O
(	O
and	O
the	O
eigenvectors	O
are	O
unchanged	O
)	O
,	O
we	O
can	O
equivalently	O
write	O
(	O
cid:0	O
)	O
λh	O
−	O
σ2ih	O
(	O
cid:1	O
)	O
1	O
2	O
r	O
f	O
=	O
uh	O
(	O
21.4.2	O
)	O
where	O
r	O
is	O
an	O
arbitrary	O
orthogonal	B
matrix	O
with	O
rtr	O
=	O
i	O
and	O
uh	O
,	O
λh	O
are	O
the	O
eigenvectors	O
and	O
corre-	O
sponding	O
eigenvalues	O
of	O
the	O
sample	B
covariance	O
s.	O
classical	O
pca	O
,	O
section	O
(	O
15.2	O
)	O
,	O
is	O
recovered	O
in	O
the	O
limit	O
σ2	O
→	O
0.	O
note	O
that	O
for	O
a	O
full	O
correspondence	O
with	O
pca	O
,	O
one	O
needs	O
to	O
set	O
r	O
=	O
i	O
,	O
which	O
points	O
f	O
along	O
the	O
principal	B
directions	I
.	O
optimal	O
σ2	O
a	O
particular	O
convenience	O
of	O
ppca	O
is	O
that	O
the	O
optimal	O
noise	O
σ2	O
can	O
be	O
found	O
immediately	O
.	O
we	O
order	O
the	O
eigenvalues	O
of	O
s	O
so	O
that	O
λ1	O
≥	O
λ2	O
,	O
...	O
≥	O
λd	O
.	O
in	O
equation	B
(	O
21.2.19	O
)	O
an	O
expression	O
for	O
the	O
log	O
likelihood	B
is	O
given	O
in	O
which	O
the	O
eigenvalues	O
are	O
those	O
σ−2s	O
.	O
on	O
replacing	O
λi	O
with	O
λi/σ2	O
we	O
can	O
therefore	O
write	O
an	O
explicit	O
expression	O
for	O
the	O
log	O
likelihood	B
in	O
terms	O
of	O
σ2	O
and	O
the	O
eigenvalues	O
of	O
a	O
,	O
l	O
(	O
σ2	O
)	O
=	O
−	O
n	O
2	O
d	O
log	O
(	O
2π	O
)	O
+	O
draft	O
march	O
9	O
,	O
2010	O
λi	O
+	O
(	O
d	O
−	O
h	O
)	O
log	O
σ2	O
+	O
h	O
(	O
21.4.3	O
)	O
397	O
(	O
cid:32	O
)	O
h	O
(	O
cid:88	O
)	O
i=1	O
log	O
λi	O
+	O
1	O
σ2	O
d	O
(	O
cid:88	O
)	O
i=h+1	O
(	O
cid:33	O
)	O
canonical	B
correlation	I
analysis	I
and	O
factor	B
analysis	I
figure	O
21.7	O
:	O
for	O
a	O
5	O
hidden	B
unit	O
model	B
,	O
here	O
are	O
plotted	O
the	O
results	O
of	O
training	B
ppca	O
and	O
fa	O
on	O
100	O
examples	O
of	O
the	O
hand-	O
written	O
digit	O
seven	O
.	O
the	O
top	O
row	O
contains	O
the	O
5	O
factor	B
analy-	O
sis	O
factors	O
and	O
the	O
bottom	O
row	O
the	O
5	O
largest	O
eigenvectors	O
from	O
ppca	O
are	O
plotted	O
.	O
by	O
diﬀerentiating	O
l	O
(	O
σ2	O
)	O
and	O
equating	O
to	O
zero	O
,	O
the	O
maximum	B
likelihood	I
optimal	O
setting	O
for	O
σ2	O
is	O
d	O
(	O
cid:88	O
)	O
d	O
−	O
h	O
j=h+1	O
1	O
σ2	O
=	O
λj	O
(	O
21.4.4	O
)	O
in	O
summary	O
ppca	O
is	O
obtained	O
by	O
taking	O
the	O
principal	O
eigenvalues	O
and	O
corresponding	O
eigenvectors	O
of	O
the	O
sample	B
covariance	O
matrix	B
s	O
,	O
and	O
setting	O
the	O
variance	B
by	O
equation	B
(	O
21.4.4	O
)	O
.	O
the	O
single-shot	O
training	B
nature	O
of	O
ppca	O
makes	O
it	O
an	O
attractive	O
algorithm	O
and	O
also	O
gives	O
a	O
useful	O
initialisation	O
for	O
factor	B
analysis	I
.	O
example	O
92	O
(	O
a	O
comparison	O
of	O
fa	O
and	O
ppca	O
)	O
.	O
we	O
trained	O
both	O
ppca	O
and	O
fa	O
to	O
model	B
handwritten	O
digits	O
of	O
the	O
number	O
7.	O
from	O
a	O
database	O
of	O
100	O
such	O
images	O
,	O
we	O
ﬁtted	O
both	O
ppca	O
and	O
fa	O
(	O
100	O
iterations	O
of	O
em	O
in	O
each	O
case	O
from	O
the	O
same	O
random	O
initialisation	O
)	O
using	O
5	O
hidden	B
units	O
.	O
the	O
learned	O
factors	O
for	O
these	O
models	O
are	O
in	O
ﬁg	O
(	O
21.7	O
)	O
.	O
to	O
get	O
a	O
feeling	O
for	O
how	O
well	O
each	O
of	O
these	O
models	O
the	O
data	B
,	O
we	O
drew	O
25	O
samples	O
from	O
each	O
model	B
,	O
as	O
given	O
in	O
ﬁg	O
(	O
21.8a	O
)	O
.	O
compared	O
with	O
ppca	O
,	O
in	O
fa	O
the	O
individual	O
noise	O
on	O
each	O
observation	O
pixel	O
enables	O
a	O
cleaner	O
representation	B
of	O
the	O
regions	O
of	O
zero	O
sample	B
variance	O
.	O
21.5	O
canonical	B
correlation	I
analysis	I
and	O
factor	B
analysis	I
we	O
outline	O
how	O
cca	O
,	O
as	O
discussed	O
in	O
section	O
(	O
15.8	O
)	O
,	O
is	O
related	O
to	O
a	O
constrained	O
form	O
of	O
fa	O
.	O
as	O
a	O
brief	O
reminder	O
,	O
cca	O
considers	O
two	O
spaces	O
x	O
and	O
y	O
where	O
,	O
for	O
example	O
,	O
x	O
might	O
represent	O
an	O
audio	O
sequence	O
of	O
a	O
person	O
speaking	O
and	O
y	O
the	O
corresponding	O
video	O
sequence	O
of	O
the	O
face	O
of	O
the	O
person	O
speaking	O
.	O
the	O
two	O
streams	O
of	O
data	B
are	O
dependent	O
since	O
we	O
would	O
expect	O
the	O
parts	O
around	O
the	O
mouth	O
region	O
to	O
be	O
correlated	O
with	O
the	O
speech	O
signal	O
.	O
the	O
aim	O
in	O
cca	O
is	O
to	O
ﬁnd	O
a	O
low	B
dimensional	I
representation	O
that	O
explains	O
the	O
correlation	B
between	O
the	O
x	O
and	O
y	O
spaces	O
.	O
a	O
model	B
that	O
achieves	O
a	O
similar	O
eﬀect	O
to	O
cca	O
is	O
to	O
use	O
a	O
latent	B
factor	O
h	O
to	O
underlie	O
the	O
data	B
in	O
both	O
the	O
x	O
and	O
y	O
spaces	O
.	O
that	O
is	O
(	O
cid:90	O
)	O
p	O
(	O
x	O
,	O
y	O
)	O
=	O
where	O
p	O
(	O
x|h	O
)	O
p	O
(	O
y|h	O
)	O
p	O
(	O
h	O
)	O
dh	O
p	O
(	O
x|h	O
)	O
=	O
n	O
(	O
x	O
ha	O
,	O
ψx	O
)	O
,	O
p	O
(	O
y|h	O
)	O
=	O
n	O
(	O
y	O
hb	O
,	O
ψy	O
)	O
,	O
p	O
(	O
h	O
)	O
=	O
n	O
(	O
h	O
0	O
,	O
1	O
)	O
(	O
21.5.1	O
)	O
(	O
21.5.2	O
)	O
figure	O
21.8	O
:	O
(	O
a	O
)	O
:	O
25	O
samples	O
from	O
the	O
learned	O
fa	O
model	B
.	O
note	O
how	O
the	O
noise	O
variance	B
de-	O
pends	O
on	O
the	O
pixel	O
,	O
being	O
zero	O
for	O
pixels	O
on	O
the	O
boundary	B
of	O
(	O
b	O
)	O
:	O
25	O
samples	O
the	O
image	O
.	O
from	O
the	O
learned	O
ppca	O
model	B
.	O
(	O
a	O
)	O
factor	B
analysis	I
(	O
b	O
)	O
ppca	O
398	O
draft	O
march	O
9	O
,	O
2010	O
fafafafafapcapcapcapcapca	O
independent	B
components	I
analysis	I
h	O
x	O
y	O
canonical	B
correlation	I
analysis	I
.	O
cca	O
corresponds	O
to	O
the	O
latent	B
vari-	O
able	O
model	B
in	O
which	O
a	O
common	O
latent	B
variable	I
generates	O
both	O
the	O
observed	O
x	O
and	O
y	O
variables	O
.	O
this	O
is	O
therefore	O
a	O
formed	O
of	O
constrained	B
factor	I
analysis	I
.	O
we	O
can	O
express	O
equation	B
(	O
21.5.2	O
)	O
as	O
a	O
form	O
of	O
factor	B
analysis	I
by	O
writing	O
x	O
∼	O
n	O
(	O
x	O
0	O
,	O
ψx	O
)	O
,	O
y	O
∼	O
n	O
(	O
y	O
0	O
,	O
ψy	O
)	O
(	O
cid:18	O
)	O
x	O
y	O
z	O
=	O
=	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
x	O
y	O
(	O
cid:18	O
)	O
a	O
(	O
cid:19	O
)	O
b	O
,	O
(	O
cid:19	O
)	O
h	O
+	O
(	O
cid:18	O
)	O
x	O
(	O
cid:18	O
)	O
a	O
y	O
b	O
(	O
cid:19	O
)	O
,	O
(	O
cid:19	O
)	O
,	O
f	O
=	O
by	O
using	O
the	O
stacked	O
vectors	O
and	O
integrating	O
out	O
the	O
latent	B
variable	I
h	O
,	O
we	O
obtain	O
σ	O
=	O
ﬀ	O
t	O
+	O
ψ	O
,	O
ψ	O
=	O
(	O
cid:18	O
)	O
ψx	O
0	O
0	O
ψy	O
(	O
cid:19	O
)	O
from	O
the	O
fa	O
results	O
equation	B
(	O
21.2.5	O
)	O
the	O
optimal	O
f	O
is	O
given	O
by	O
p	O
(	O
z	O
)	O
=	O
n	O
(	O
z	O
0	O
,	O
σ	O
)	O
,	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
1	O
+	O
f	O
tψ−1f	O
f	O
=	O
sψ−1f	O
⇒	O
f	O
∝	O
sψ−1f	O
(	O
21.5.3	O
)	O
(	O
21.5.4	O
)	O
(	O
21.5.5	O
)	O
(	O
21.5.6	O
)	O
(	O
21.5.7	O
)	O
(	O
21.5.8	O
)	O
(	O
21.5.9	O
)	O
so	O
that	O
optimally	O
f	O
is	O
given	O
by	O
the	O
principal	O
eigenvector	O
of	O
sψ−1	O
.	O
by	O
imposing	O
ψx	O
=	O
σ2	O
above	O
equation	B
can	O
be	O
expressed	O
as	O
the	O
coupled	B
equations	O
xi	O
,	O
ψy	O
=	O
σ2	O
yi	O
the	O
1	O
σ2	O
x	O
1	O
σ2	O
x	O
sxxa	O
+	O
syxa	O
+	O
1	O
σ2	O
y	O
1	O
σ2	O
y	O
sxyb	O
syyb	O
a	O
∝	O
b	O
∝	O
(	O
cid:18	O
)	O
i	O
−	O
eliminating	O
b	O
we	O
have	O
,	O
for	O
an	O
arbitrary	O
proportionality	O
constant	O
γ	O
,	O
(	O
cid:19	O
)	O
γ	O
σ2	O
x	O
sxx	O
a	O
=	O
γ2	O
σ2	O
xσ2	O
y	O
sxy	O
γ	O
σ2	O
y	O
syy	O
syxa	O
(	O
cid:19	O
)	O
−1	O
(	O
cid:18	O
)	O
i	O
−	O
x	O
,	O
σ2	O
in	O
the	O
limit	O
σ2	O
y	O
→	O
0	O
,	O
this	O
tends	O
to	O
the	O
zero	O
derivative	O
condition	O
equation	B
(	O
15.8.8	O
)	O
so	O
that	O
cca	O
can	O
be	O
seen	O
as	O
in	O
fact	O
a	O
form	O
limiting	O
of	O
fa	O
(	O
see	O
[	O
12	O
]	O
for	O
a	O
more	O
thorough	O
correspondence	O
)	O
.	O
by	O
viewing	O
cca	O
in	O
this	O
manner	O
extensions	O
to	O
using	O
more	O
than	O
a	O
single	O
latent	B
dimension	O
h	O
become	O
clear	O
,	O
see	O
exercise	O
(	O
208	O
)	O
,	O
in	O
addition	O
to	O
the	O
beneﬁts	O
of	O
a	O
probabilistic	B
interpretation	O
.	O
as	O
we	O
’	O
ve	O
indicated	O
,	O
cca	O
corresponds	O
to	O
training	B
a	O
form	O
of	O
fa	O
by	O
maximising	O
the	O
joint	B
likelihood	O
p	O
(	O
x	O
,	O
y|w	O
,	O
u	O
)	O
.	O
alternatively	O
,	O
training	B
based	O
on	O
the	O
maximising	O
the	O
conditional	B
p	O
(	O
y|x	O
,	O
w	O
,	O
u	O
)	O
corresponds	O
to	O
a	O
special	O
case	O
of	O
a	O
technique	O
called	O
partial	O
least	O
squares	O
,	O
see	O
for	O
example	O
[	O
78	O
]	O
.	O
this	O
correspondence	O
is	O
left	O
as	O
an	O
exercise	O
for	O
the	O
interested	O
reader	O
.	O
extending	O
fa	O
to	O
kernel	B
variants	O
is	O
not	O
straightforward	O
since	O
under	O
replacing	O
x	O
with	O
a	O
non-linear	B
mapping	O
φ	O
(	O
x	O
)	O
,	O
normalising	O
the	O
expression	O
e−	O
(	O
φ	O
(	O
x	O
)	O
−wh	O
)	O
2	O
is	O
in	O
general	O
intractable	O
.	O
21.6	O
independent	B
components	I
analysis	I
independent	O
components	O
analysis	B
(	O
ica	O
)	O
is	O
a	O
linear	B
decomposition	O
of	O
the	O
data	B
v	O
in	O
which	O
the	O
components	O
of	O
underlying	O
latent	B
vector	O
variable	B
h	O
are	O
independent	O
[	O
221	O
,	O
138	O
]	O
.	O
in	O
other	O
words	O
,	O
we	O
seek	O
a	O
linear	B
coordinate	O
system	B
in	O
which	O
the	O
coordinates	O
are	O
independent	O
.	O
such	O
independent	O
coordinate	O
systems	O
arguably	O
form	O
draft	O
march	O
9	O
,	O
2010	O
399	O
independent	B
components	I
analysis	I
exp	O
(	O
−5	O
(	O
cid:112	O
)	O
figure	O
21.9	O
:	O
latent	B
data	O
is	O
sampled	O
from	O
the	O
prior	B
p	O
(	O
xi	O
)	O
∝	O
|xi|	O
)	O
with	O
the	O
mixing	B
matrix	I
a	O
shown	O
in	O
green	O
to	O
cre-	O
ate	O
the	O
observed	O
two	O
dimensional	O
vectors	O
y	O
=	O
ax	O
.	O
the	O
red	O
lines	O
are	O
the	O
mixing	B
matrix	I
estimated	O
by	O
ica.m	O
based	O
on	O
the	O
obser-	O
vations	O
.	O
for	O
comparison	O
,	O
pca	O
produces	O
the	O
blue	O
(	O
dashed	O
)	O
com-	O
ponents	O
.	O
note	O
that	O
the	O
components	O
have	O
been	O
scaled	O
to	O
improve	O
visualisation	O
.	O
as	O
expected	O
,	O
pca	O
ﬁnds	O
the	O
orthogonal	B
directions	O
of	O
maximal	O
variation	O
.	O
ica	O
however	O
,	O
correctly	O
estimates	O
the	O
di-	O
rections	O
in	O
which	O
the	O
components	O
were	O
independently	O
generated	O
.	O
see	O
demoica.m	O
.	O
a	O
natural	B
representation	O
of	O
the	O
data	B
and	O
can	O
give	O
rise	O
to	O
very	O
diﬀerent	O
representations	O
than	O
pca	O
,	O
see	O
ﬁg	O
(	O
21.9	O
)	O
.	O
from	O
a	O
probabilistic	B
viewpoint	O
,	O
the	O
model	B
is	O
described	O
by	O
p	O
(	O
hi	O
)	O
(	O
21.6.1	O
)	O
p	O
(	O
v	O
,	O
h|a	O
)	O
=	O
p	O
(	O
v|h	O
,	O
a	O
)	O
(	O
cid:89	O
)	O
i	O
in	O
ica	O
it	O
is	O
common	O
to	O
assume	O
that	O
the	O
observations	O
are	O
linearly	O
related	O
to	O
the	O
latent	B
variables	O
h.	O
for	O
technical	O
reasons	O
,	O
the	O
most	O
convenient	O
practical	O
choice	O
is	O
to	O
use3	O
where	O
a	O
is	O
a	O
square	O
mixing	B
matrix	I
so	O
that	O
the	O
likelihood	B
of	O
an	O
observation	O
v	O
is	O
v	O
=	O
ah	O
(	O
cid:90	O
)	O
p	O
(	O
v|h	O
,	O
a	O
)	O
(	O
cid:89	O
)	O
i	O
p	O
(	O
v	O
)	O
=	O
p	O
(	O
hi	O
)	O
dh	O
=	O
(	O
cid:90	O
)	O
δ	O
(	O
v	O
−	O
ah	O
)	O
(	O
cid:89	O
)	O
i	O
p	O
(	O
hi	O
)	O
dh	O
=	O
1	O
|det	O
(	O
a	O
)	O
|	O
i	O
p	O
(	O
(	O
cid:2	O
)	O
a−1v	O
(	O
cid:3	O
)	O
i	O
)	O
(	O
21.6.2	O
)	O
(	O
21.6.3	O
)	O
(	O
cid:89	O
)	O
the	O
underlying	O
independence	B
assumptions	O
are	O
then	O
the	O
same	O
as	O
for	O
ppca	O
(	O
in	O
the	O
limit	O
of	O
zero	O
output	O
noise	O
)	O
.	O
below	O
,	O
however	O
,	O
we	O
will	O
choose	O
a	O
non-gaussian	O
prior	B
p	O
(	O
hi	O
)	O
.	O
for	O
a	O
given	O
set	O
of	O
data	B
v	O
=	O
(	O
cid:0	O
)	O
v1	O
,	O
.	O
.	O
.	O
,	O
vn	O
(	O
cid:1	O
)	O
and	O
prior	B
p	O
(	O
h	O
)	O
,	O
our	O
aim	O
is	O
to	O
ﬁnd	O
a.	O
for	O
i.i.d	O
.	O
data	B
,	O
the	O
log	O
likelihood	B
is	O
conveniently	O
written	O
in	O
terms	O
of	O
b	O
=	O
a−1	O
,	O
(	O
21.6.4	O
)	O
(	O
21.6.5	O
)	O
(	O
21.6.6	O
)	O
(	O
21.6.7	O
)	O
(	O
21.6.8	O
)	O
l	O
(	O
b	O
)	O
=	O
n	O
log	O
det	O
(	O
b	O
)	O
+	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
log	O
p	O
(	O
[	O
bvn	O
]	O
i	O
)	O
n	O
i	O
note	O
that	O
for	O
a	O
gaussian	O
prior	B
−h2	O
p	O
(	O
h	O
)	O
∝	O
e	O
the	O
log	O
likelihood	B
becomes	O
l	O
(	O
b	O
)	O
=	O
n	O
log	O
det	O
(	O
b	O
)	O
−	O
(	O
cid:88	O
)	O
n	O
(	O
vn	O
)	O
t	O
btbvn	O
+	O
const	O
.	O
l	O
(	O
b	O
)	O
=	O
n	O
aba	O
+	O
(	O
cid:88	O
)	O
∂	O
∂bab	O
where	O
φ	O
(	O
x	O
)	O
≡	O
d	O
dx	O
log	O
p	O
(	O
x	O
)	O
=	O
φ	O
(	O
[	O
bv	O
]	O
a	O
)	O
vn	O
b	O
n	O
1	O
p	O
(	O
x	O
)	O
d	O
dx	O
p	O
(	O
x	O
)	O
which	O
is	O
invariant	O
with	O
respect	O
to	O
an	O
orthogonal	B
rotation	O
b	O
→	O
rb	O
,	O
with	O
rtr	O
=	O
i.	O
this	O
means	O
that	O
for	O
a	O
gaussian	O
prior	B
p	O
(	O
h	O
)	O
,	O
we	O
can	O
not	O
estimate	O
uniquely	O
the	O
mixing	B
matrix	I
.	O
to	O
break	O
this	O
rotational	O
invariance	O
we	O
therefore	O
need	O
to	O
use	O
a	O
non-gaussian	O
prior	B
.	O
assuming	O
we	O
have	O
a	O
non-gaussian	O
prior	B
p	O
(	O
h	O
)	O
,	O
taking	O
the	O
derivative	O
w.r.t	O
.	O
bab	O
we	O
obtain	O
3this	O
treatment	O
follows	O
that	O
presented	O
in	O
[	O
183	O
]	O
.	O
400	O
draft	O
march	O
9	O
,	O
2010	O
−2−1.5−1−0.500.511.5−1.5−1−0.500.511.5	O
exercises	O
a	O
simple	O
gradient	O
ascent	O
learning	B
rule	O
for	O
b	O
is	O
then	O
(	O
cid:32	O
)	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
n	O
b−t	O
+	O
1	O
n	O
(	O
cid:88	O
)	O
n	O
1	O
n	O
(	O
cid:33	O
)	O
(	O
cid:33	O
)	O
bnew	O
=	O
b	O
+	O
η	O
φ	O
(	O
bvn	O
)	O
(	O
vn	O
)	O
t	O
(	O
21.6.9	O
)	O
an	O
alternative	O
‘	O
natural	B
gradient	I
’	O
algorithm	B
[	O
8	O
,	O
183	O
]	O
that	O
approximates	O
a	O
newton	O
update	O
is	O
given	O
by	O
multi-	O
plying	O
the	O
gradient	B
by	O
btb	O
on	O
the	O
right	O
to	O
give	O
the	O
update	O
bnew	O
=	O
b	O
+	O
η	O
i	O
+	O
φ	O
(	O
bvn	O
)	O
(	O
bvn	O
)	O
t	O
b	O
(	O
21.6.10	O
)	O
here	O
η	O
is	O
a	O
learning	B
rate	I
which	O
in	O
the	O
code	O
ica.m	O
we	O
nominally	O
set	O
to	O
0.5.	O
a	O
natural	B
extension	O
is	O
to	O
consider	O
noise	O
on	O
the	O
outputs	O
,	O
exercise	O
(	O
212	O
)	O
,	O
for	O
which	O
an	O
em	O
algorithm	B
is	O
readily	O
available	O
.	O
however	O
,	O
in	O
the	O
limit	O
of	O
low	O
output	O
noise	O
,	O
the	O
em	O
formally	O
fails	O
,	O
an	O
eﬀect	O
which	O
is	O
related	O
to	O
the	O
general	O
discussion	O
in	O
section	O
(	O
11.4	O
)	O
.	O
a	O
popular	O
alternative	O
estimation	O
method	O
is	O
fastica4	O
and	O
can	O
be	O
related	O
to	O
an	O
iterative	O
maximum	O
like-	O
lihood	O
optimisation	B
procedure	O
.	O
ica	O
can	O
also	O
be	O
motivated	O
from	O
several	O
alternative	O
directions	O
,	O
including	O
information	O
theory	O
[	O
29	O
]	O
.	O
we	O
refer	O
the	O
reader	O
to	O
[	O
138	O
]	O
for	O
an	O
in-depth	O
discussion	O
of	O
ica	O
and	O
related	O
exten-	O
sions	O
.	O
21.7	O
code	O
fa.m	O
:	O
factor	B
analysis	I
demofa.m	O
:	O
demo	O
of	O
factor	B
analysis	I
ica.m	O
:	O
independent	B
components	I
analysis	I
demoica.m	O
:	O
demo	O
ica	O
21.8	O
exercises	O
exercise	O
207.	O
factor	B
analysis	I
and	O
scaling	O
.	O
assume	O
that	O
a	O
h-factor	O
model	B
holds	O
for	O
x.	O
now	O
consider	O
the	O
the	O
transformation	O
y	O
=	O
cx	O
,	O
where	O
c	O
is	O
a	O
non-singular	O
square	O
diagonal	O
matrix	B
.	O
show	O
that	O
factor	B
analysis	I
is	O
scale	O
invariant	O
,	O
i.e	O
.	O
that	O
the	O
h-factor	O
model	B
also	O
holds	O
for	O
y	O
,	O
with	O
the	O
factor	B
loadings	O
appropriately	O
scaled	O
.	O
how	O
must	O
the	O
speciﬁc	O
factors	O
be	O
scaled	O
?	O
exercise	O
208.	O
for	O
the	O
constrained	B
factor	I
analysis	I
model	O
h	O
+	O
	O
,	O
	O
∼	O
n	O
(	O
	O
0	O
,	O
diag	O
(	O
ψ1	O
,	O
.	O
.	O
.	O
,	O
ψn	O
)	O
)	O
,	O
h	O
∼	O
n	O
(	O
h	O
0	O
,	O
i	O
)	O
(	O
21.8.1	O
)	O
derive	O
a	O
maximum	B
likelihood	I
em	O
algorithm	B
for	O
the	O
matrices	O
a	O
and	O
b	O
,	O
assuming	O
the	O
datapoints	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
are	O
i.i.d	O
.	O
exercise	O
209.	O
an	O
apparent	O
extension	O
of	O
fa	O
analysis	B
is	O
to	O
consider	O
a	O
correlated	O
prior	B
p	O
(	O
h	O
)	O
=	O
n	O
(	O
h	O
0	O
,	O
σh	O
)	O
(	O
21.8.2	O
)	O
show	O
that	O
,	O
provided	O
no	O
constraints	O
are	O
placed	O
on	O
the	O
factor	B
loading	I
matrix	O
f	O
,	O
using	O
a	O
correlated	O
prior	B
p	O
(	O
h	O
)	O
is	O
an	O
equivalent	B
model	O
to	O
the	O
original	O
uncorrelated	O
fa	O
model	B
.	O
exercise	O
210.	O
using	O
the	O
woodbury	O
identity	O
and	O
the	O
deﬁnition	O
of	O
σd	O
in	O
equation	B
(	O
21.2.2	O
)	O
,	O
show	O
that	O
one	O
can	O
rewrite	O
σd	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
a	O
0	O
0	O
b	O
x	O
=	O
−1f	O
as	O
−1f	O
=	O
ψ−1f	O
(	O
cid:16	O
)	O
σd	O
i	O
+	O
ftψ−1f	O
(	O
cid:17	O
)	O
−1	O
4see	O
www.cis.hut.fi/projects/ica/fastica/	O
draft	O
march	O
9	O
,	O
2010	O
(	O
21.8.3	O
)	O
401	O
exercise	O
212.	O
consider	O
an	O
ica	O
model	B
show	O
l	O
(	O
σ2	O
)	O
is	O
maximal	O
for	O
λj	O
j=h+1	O
1	O
σ2	O
=	O
d	O
−	O
h	O
d	O
(	O
cid:88	O
)	O
p	O
(	O
yj|x	O
,	O
w	O
)	O
(	O
cid:89	O
)	O
p	O
(	O
y	O
,	O
x|w	O
)	O
=	O
(	O
cid:89	O
)	O
j	O
x	O
,	O
σ2	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
p	O
(	O
yj|x	O
,	O
w	O
)	O
=	O
n	O
yj	O
wt	O
j	O
i	O
with	O
p	O
(	O
xi	O
)	O
,	O
w	O
=	O
[	O
w1	O
,	O
.	O
.	O
.	O
,	O
wj	O
]	O
exercises	O
(	O
21.8.4	O
)	O
(	O
21.8.5	O
)	O
(	O
21.8.6	O
)	O
(	O
21.8.7	O
)	O
exercise	O
211.	O
for	O
the	O
log	O
likelihood	B
function	O
l	O
(	O
σ2	O
)	O
=	O
−	O
n	O
2	O
d	O
log	O
(	O
2π	O
)	O
+	O
log	O
λi	O
+	O
1	O
σ2	O
(	O
cid:32	O
)	O
h	O
(	O
cid:88	O
)	O
i=1	O
d	O
(	O
cid:88	O
)	O
i=h+1	O
λi	O
+	O
(	O
d	O
−	O
h	O
)	O
log	O
σ2	O
+	O
h	O
(	O
cid:33	O
)	O
1.	O
for	O
the	O
above	O
model	B
derive	O
an	O
em	O
algorithm	B
for	O
a	O
set	O
of	O
i.i.d	O
.	O
data	B
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
and	O
show	O
that	O
the	O
required	O
statistics	O
for	O
the	O
m-step	O
are	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
p	O
(	O
x|yn	O
,	O
w	O
)	O
and	O
(	O
cid:10	O
)	O
xxt	O
(	O
cid:11	O
)	O
p	O
(	O
x|yn	O
,	O
w	O
)	O
.	O
2.	O
show	O
that	O
for	O
a	O
non-gaussian	O
prior	B
p	O
(	O
xi	O
)	O
,	O
the	O
posterior	B
p	O
(	O
x|y	O
,	O
w	O
)	O
(	O
21.8.8	O
)	O
is	O
non-factorised	O
,	O
non-gaussian	O
and	O
generally	O
intractable	O
(	O
its	O
normalisation	B
constant	I
can	O
not	O
be	O
com-	O
puted	O
eﬃciently	O
)	O
.	O
3.	O
show	O
that	O
in	O
the	O
limit	O
σ2	O
→	O
0	O
,	O
the	O
em	O
algorithm	B
fails	O
.	O
402	O
draft	O
march	O
9	O
,	O
2010	O
chapter	O
22	O
latent	O
ability	O
models	O
22.1	O
the	O
rasch	O
model	B
consider	O
an	O
exam	O
in	O
which	O
student	O
s	O
answers	O
question	O
q	O
either	O
correctly	O
xqs	O
=	O
1	O
or	O
incorrectly	O
xqs	O
=	O
0.	O
for	O
a	O
set	O
of	O
n	O
students	O
and	O
q	O
questions	O
,	O
the	O
performance	B
of	O
all	O
students	O
is	O
given	O
in	O
the	O
q	O
×	O
n	O
binary	O
matrix	O
x.	O
based	O
on	O
this	O
data	B
alone	O
we	O
wish	O
to	O
evaluate	O
the	O
ability	O
of	O
each	O
student	O
.	O
one	O
approach	B
is	O
to	O
deﬁne	O
the	O
ability	O
as	O
as	O
the	O
fraction	O
of	O
questions	O
student	O
s	O
answered	O
correctly	O
.	O
a	O
more	O
subtle	O
analysis	B
is	O
to	O
accept	O
that	O
some	O
questions	O
are	O
more	O
diﬃcult	O
than	O
others	O
so	O
that	O
a	O
student	O
who	O
answered	O
diﬃcult	O
questions	O
should	O
be	O
awarded	O
more	O
highly	O
than	O
a	O
student	O
who	O
answered	O
the	O
same	O
number	O
of	O
easy	O
questions	O
.	O
a	O
priori	O
we	O
do	O
not	O
know	O
which	O
are	O
the	O
diﬃcult	O
questions	O
and	O
this	O
needs	O
to	O
be	O
estimated	O
based	O
on	O
x.	O
to	O
account	O
for	O
inherent	O
diﬀerences	O
in	O
question	O
diﬃculty	O
we	O
may	O
model	B
the	O
probability	B
that	O
a	O
student	O
s	O
gets	O
a	O
question	O
q	O
correct	O
based	O
on	O
the	O
student	O
’	O
s	O
latent	O
ability	O
αs	O
and	O
the	O
latent	B
diﬃculty	O
of	O
the	O
question	O
δq	O
.	O
a	O
simple	O
generative	O
model	B
of	O
the	O
response	O
is	O
p	O
(	O
xqs	O
=	O
1|α	O
,	O
δ	O
)	O
=	O
σ	O
(	O
αs	O
−	O
δq	O
)	O
(	O
22.1.1	O
)	O
where	O
σ	O
(	O
x	O
)	O
=	O
1/	O
(	O
1	O
+	O
e−x	O
)	O
.	O
under	O
this	O
model	B
,	O
the	O
higher	O
the	O
latent	O
ability	O
is	O
above	O
the	O
latent	B
diﬃculty	O
of	O
the	O
question	O
,	O
the	O
more	O
likely	O
it	O
is	O
that	O
the	O
student	O
will	O
answer	O
the	O
question	O
correctly	O
.	O
22.1.1	O
maximum	B
likelihood	I
training	O
making	O
the	O
i.i.d	O
.	O
assumption	O
,	O
the	O
likelihood	B
of	O
the	O
data	B
x	O
under	O
this	O
model	B
is	O
s	O
(	O
cid:89	O
)	O
p	O
(	O
x|α	O
,	O
δ	O
)	O
=	O
q	O
(	O
cid:89	O
)	O
l	O
≡	O
log	O
p	O
(	O
x|α	O
,	O
δ	O
)	O
=	O
(	O
cid:88	O
)	O
s=1	O
the	O
log	O
likelihood	B
is	O
q=1	O
σ	O
(	O
αs	O
−	O
δq	O
)	O
xqs	O
(	O
1	O
−	O
σ	O
(	O
αs	O
−	O
δq	O
)	O
)	O
1−xqs	O
xqs	O
log	O
σ	O
(	O
αs	O
−	O
δq	O
)	O
+	O
(	O
1	O
−	O
xqs	O
)	O
log	O
(	O
1	O
−	O
σ	O
(	O
αs	O
−	O
δq	O
)	O
)	O
q	O
,	O
s	O
(	O
22.1.2	O
)	O
(	O
22.1.3	O
)	O
q	O
δq	O
xqs	O
αs	O
s	O
figure	O
22.1	O
:	O
the	O
rasch	O
model	B
for	O
analysing	O
questions	O
.	O
each	O
element	O
of	O
the	O
binary	O
matrix	O
x	O
,	O
with	O
xqs	O
=	O
1	O
if	O
student	O
s	O
gets	O
question	O
q	O
correct	O
,	O
is	O
generated	O
using	O
the	O
latent	O
ability	O
of	O
the	O
student	O
αs	O
and	O
the	O
latent	B
diﬃculty	O
of	O
the	O
question	O
δq	O
.	O
403	O
with	O
derivatives	O
competition	B
models	I
(	O
xqs	O
−	O
σ	O
(	O
αs	O
−	O
δq	O
)	O
)	O
,	O
∂l	O
∂δq	O
=	O
−	O
s	O
(	O
cid:88	O
)	O
s=1	O
(	O
xqs	O
−	O
σ	O
(	O
αs	O
−	O
δq	O
)	O
)	O
(	O
22.1.4	O
)	O
q	O
(	O
cid:88	O
)	O
q=1	O
∂l	O
∂αs	O
=	O
a	O
simple	O
way	O
to	O
learn	O
the	O
parameters	O
is	O
to	O
use	O
gradient	B
ascent	O
,	O
see	O
demorasch.m	O
,	O
with	O
extensions	O
to	O
newton	O
methods	O
being	O
straightforward	O
.	O
the	O
generalisation	B
to	O
more	O
than	O
two	O
responses	O
xqs	O
∈	O
{	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
}	O
can	O
be	O
achieved	O
using	O
a	O
softmax-style	O
function	B
.	O
more	O
generally	O
,	O
the	O
rasch	O
model	B
is	O
an	O
example	O
of	O
an	O
item	B
response	I
theory	I
,	O
a	O
subject	O
dealing	O
with	O
the	O
analysis	B
of	O
questionnaires	O
[	O
94	O
]	O
.	O
missing	B
data	I
assuming	O
the	O
data	B
is	O
missing	B
at	I
random	I
,	O
missing	B
data	I
can	O
be	O
treated	O
by	O
computing	O
the	O
likelihood	B
of	O
only	O
the	O
observed	O
elements	O
of	O
x.	O
in	O
rasch.m	O
missing	B
data	I
is	O
assumed	O
to	O
be	O
coded	O
as	O
nan	O
so	O
that	O
the	O
likelihood	B
and	O
gradients	O
are	O
straightforward	O
to	O
compute	O
based	O
on	O
summing	O
only	O
over	O
terms	O
containing	O
non	O
nan	O
entries	O
.	O
example	O
93.	O
we	O
display	O
an	O
example	O
of	O
the	O
use	O
of	O
the	O
rasch	O
model	B
in	O
ﬁg	O
(	O
22.2	O
)	O
,	O
estimat-	O
ing	O
the	O
latent	B
abilities	O
of	O
20	O
students	O
based	O
on	O
a	O
set	O
of	O
50	O
questions	O
.	O
based	O
on	O
using	O
the	O
number	O
of	O
questions	O
each	O
student	O
answered	O
correctly	O
,	O
the	O
best	O
students	O
are	O
(	O
ranked	O
from	O
ﬁrst	O
)	O
8	O
,	O
6	O
,	O
1	O
,	O
19	O
,	O
4	O
,	O
17	O
,	O
20	O
,	O
7	O
,	O
15	O
,	O
5	O
,	O
12	O
,	O
16	O
,	O
2	O
,	O
3	O
,	O
18	O
,	O
9	O
,	O
11	O
,	O
14	O
,	O
10	O
,	O
13.	O
alternatively	O
,	O
ranking	O
students	O
according	O
to	O
the	O
latent	O
ability	O
gives	O
8	O
,	O
6	O
,	O
19	O
,	O
1	O
,	O
20	O
,	O
4	O
,	O
17	O
,	O
7	O
,	O
15	O
,	O
12	O
,	O
5	O
,	O
16	O
,	O
2	O
,	O
3	O
,	O
18	O
,	O
9	O
,	O
11	O
,	O
14	O
,	O
10	O
,	O
13.	O
this	O
diﬀers	O
(	O
only	O
slightly	O
in	O
this	O
case	O
)	O
from	O
the	O
number-correct	O
ranking	O
since	O
the	O
rasch	O
model	B
takes	O
into	O
account	O
the	O
fact	O
that	O
some	O
students	O
answered	O
diﬃcult	O
questions	O
correctly	O
.	O
for	O
example	O
student	O
20	O
answered	O
some	O
diﬃcult	O
questions	O
correctly	O
.	O
22.1.2	O
bayesian	O
rasch	O
models	O
the	O
rasch	O
model	B
will	O
potentially	O
overﬁt	O
the	O
data	B
especially	O
when	O
there	O
is	O
only	O
a	O
small	O
amount	O
of	O
data	B
.	O
for	O
this	O
case	O
a	O
natural	B
extension	O
is	O
to	O
use	O
a	O
bayesian	O
technique	O
,	O
placing	O
independent	O
priors	O
on	O
the	O
ability	O
and	O
question	O
diﬃculty	O
,	O
so	O
that	O
the	O
posterior	B
ability	O
and	O
question	O
diﬃculty	O
is	O
given	O
by	O
p	O
(	O
α	O
,	O
δ|x	O
)	O
∝	O
p	O
(	O
x|α	O
,	O
δ	O
)	O
p	O
(	O
α	O
)	O
p	O
(	O
δ	O
)	O
natural	B
priors	O
are	O
p	O
(	O
α	O
)	O
=	O
(	O
cid:89	O
)	O
(	O
cid:0	O
)	O
αs	O
0	O
,	O
σ2	O
(	O
cid:1	O
)	O
,	O
s	O
n	O
p	O
(	O
δ	O
)	O
=	O
(	O
cid:89	O
)	O
q	O
n	O
(	O
cid:0	O
)	O
δq	O
0	O
,	O
τ	O
2	O
(	O
cid:1	O
)	O
(	O
22.1.5	O
)	O
(	O
22.1.6	O
)	O
where	O
σ2	O
and	O
τ	O
2	O
are	O
hyperparameters	O
that	O
can	O
be	O
learned	O
by	O
maximising	O
p	O
(	O
x|σ2	O
,	O
τ	O
2	O
)	O
.	O
even	O
using	O
gaussian	O
priors	O
,	O
the	O
posterior	B
distribution	O
p	O
(	O
α	O
,	O
δ|x	O
)	O
is	O
not	O
of	O
a	O
standard	O
form	O
and	O
approximations	O
are	O
required	O
.	O
in	O
this	O
case	O
however	O
,	O
the	O
posterior	B
is	O
log	O
concave	O
so	O
that	O
approximation	B
methods	O
based	O
on	O
variational	O
or	O
laplace	O
techniques	O
are	O
potentially	O
adequate	O
,	O
or	O
alternatively	O
one	O
may	O
use	O
sampling	B
approximations	O
.	O
22.2	O
competition	B
models	I
22.2.1	O
bradly-terry-luce	O
model	B
the	O
bradly-terry-luce	O
model	B
assesses	O
the	O
ability	O
of	O
players	O
based	O
on	O
one-on-one	O
matches	O
.	O
here	O
we	O
describe	O
games	O
in	O
which	O
only	O
win/lose	O
outcomes	O
arise	O
,	O
leaving	O
aside	O
the	O
complicating	O
possibility	O
of	O
draws	O
.	O
for	O
this	O
404	O
draft	O
march	O
9	O
,	O
2010	O
competition	B
models	I
(	O
a	O
)	O
(	O
c	O
)	O
(	O
b	O
)	O
(	O
d	O
)	O
figure	O
22.2	O
:	O
rasch	O
model	B
.	O
(	O
a	O
)	O
:	O
the	O
data	B
of	O
correct	O
answers	O
(	O
white	O
)	O
and	O
incorrect	O
answers	O
(	O
black	O
)	O
.	O
(	O
b	O
)	O
:	O
(	O
c	O
)	O
:	O
the	O
fraction	O
of	O
questions	O
each	O
student	O
answered	O
the	O
estimated	O
latent	B
diﬃculty	O
of	O
each	O
question	O
.	O
correctly	O
.	O
(	O
d	O
)	O
:	O
the	O
estimated	O
latent	O
ability	O
.	O
win/lose	O
scenario	O
,	O
the	O
btl	O
model	B
is	O
a	O
straightforward	O
modiﬁcation	O
of	O
the	O
rasch	O
model	B
so	O
that	O
for	O
latent	O
ability	O
αi	O
of	O
player	O
i	O
and	O
latent	O
ability	O
αj	O
of	O
player	O
j	O
,	O
the	O
probability	B
that	O
i	O
beats	O
j	O
is	O
given	O
by	O
where	O
i	O
(	O
cid:66	O
)	O
j	O
stands	O
for	O
player	O
i	O
beats	O
player	O
j.	O
based	O
on	O
a	O
matrix	B
of	O
games	O
data	B
x	O
with	O
(	O
22.2.1	O
)	O
(	O
22.2.2	O
)	O
(	O
22.2.3	O
)	O
p	O
(	O
i	O
(	O
cid:66	O
)	O
j|α	O
)	O
=	O
σ	O
(	O
αi	O
−	O
αj	O
)	O
if	O
i	O
(	O
cid:66	O
)	O
j	O
in	O
game	O
n	O
otherwise	O
0	O
xn	O
ij	O
=	O
(	O
cid:26	O
)	O
1	O
p	O
(	O
x|α	O
)	O
=	O
(	O
cid:89	O
)	O
where	O
mij	O
=	O
(	O
cid:80	O
)	O
(	O
cid:89	O
)	O
the	O
likelihood	B
of	O
the	O
model	B
is	O
given	O
by	O
[	O
σ	O
(	O
αi	O
−	O
αj	O
)	O
]	O
xn	O
n	O
ij	O
ij	O
=	O
(	O
cid:89	O
)	O
ij	O
[	O
σ	O
(	O
αi	O
−	O
αj	O
)	O
]	O
mij	O
or	O
a	O
bayesian	O
technique	O
can	O
then	O
proceed	O
as	O
for	O
the	O
rasch	O
model	B
.	O
n	O
xn	O
ij	O
is	O
the	O
number	O
of	O
times	O
player	O
i	O
beat	O
player	O
j.	O
training	B
using	O
maximum	B
likelihood	I
for	O
the	O
case	O
of	O
only	O
two	O
objects	O
interacting	O
,	O
these	O
models	O
are	O
called	O
pairwise	B
comparison	I
models	I
.	O
thurstone	O
in	O
the	O
1920	O
’	O
s	O
applied	O
such	O
models	O
to	O
a	O
wide	O
range	O
of	O
data	B
,	O
and	O
the	O
bradley-terry-luce	O
model	B
is	O
in	O
fact	O
a	O
special	O
case	O
of	O
his	O
work	O
[	O
73	O
]	O
.	O
example	O
94.	O
an	O
example	O
application	O
of	O
the	O
btl	O
model	B
is	O
given	O
in	O
ﬁg	O
(	O
22.3	O
)	O
in	O
which	O
a	O
matrix	B
x	O
containing	O
the	O
number	O
of	O
times	O
that	O
competitor	O
i	O
beat	O
competitor	O
j	O
is	O
given	O
.	O
the	O
matrix	B
entries	O
x	O
were	O
drawn	O
from	O
a	O
btl	O
model	B
based	O
on	O
‘	O
true	O
abilities	O
’	O
.	O
using	O
x	O
alone	O
the	O
maximum	B
likelihood	I
estimate	O
of	O
these	O
latent	B
abilities	O
is	O
in	O
close	O
agreement	O
with	O
the	O
true	O
abilities	O
.	O
draft	O
march	O
9	O
,	O
2010	O
405	O
questionstudent5101520253035404550246810121416182005101520253035404550−8−6−4−20246questionestimated	O
difficulty0246810121416182000.10.20.30.40.50.60.7studentfraction	O
of	O
questions	O
correct02468101214161820−2.5−2−1.5−1−0.500.511.5studentestimated	O
ability	O
competition	B
models	I
(	O
a	O
)	O
(	O
b	O
)	O
(	O
a	O
)	O
:	O
the	O
data	B
x	O
with	O
xij	O
being	O
the	O
number	O
of	O
times	O
that	O
competitor	O
i	O
beat	O
figure	O
22.3	O
:	O
btl	O
model	B
.	O
competitor	O
j	O
.	O
(	O
b	O
)	O
:	O
the	O
true	O
versus	O
estimated	O
ability	O
.	O
even	O
though	O
the	O
data	B
is	O
quite	O
sparse	B
,	O
a	O
reasonable	O
estimate	O
of	O
the	O
latent	O
ability	O
of	O
each	O
competitor	O
is	O
found	O
.	O
22.2.2	O
elo	O
ranking	O
model	B
the	O
elo	O
system	B
[	O
89	O
]	O
used	O
in	O
chess	O
ranking	O
is	O
closely	O
related	O
to	O
the	O
btl	O
model	B
above	O
,	O
though	O
there	O
is	O
the	O
added	O
complication	O
of	O
the	O
possibility	O
of	O
draws	O
.	O
in	O
addition	O
,	O
the	O
elo	O
system	B
takes	O
into	O
account	O
a	O
measure	O
of	O
the	O
variability	O
in	O
performance	B
.	O
for	O
a	O
given	O
ability	O
αi	O
,	O
the	O
actual	O
performance	B
πi	O
of	O
player	O
i	O
in	O
a	O
game	O
is	O
given	O
by	O
πi	O
=	O
αi	O
+	O
i	O
where	O
i	O
∼	O
n	O
variability	O
in	O
the	O
performance	B
.	O
more	O
formally	O
the	O
elo	O
model	B
modiﬁes	O
the	O
btl	O
model	B
to	O
give	O
(	O
cid:0	O
)	O
i	O
0	O
,	O
σ2	O
(	O
cid:1	O
)	O
.	O
the	O
variance	B
σ2	O
is	O
ﬁxed	O
across	O
all	O
players	O
and	O
thus	O
takes	O
into	O
account	O
intrinsic	O
(	O
cid:90	O
)	O
(	O
22.2.4	O
)	O
(	O
cid:0	O
)	O
π	O
α	O
,	O
σ2i	O
(	O
cid:1	O
)	O
p	O
(	O
x|α	O
)	O
=	O
p	O
(	O
x|π	O
)	O
p	O
(	O
π|α	O
)	O
,	O
π	O
p	O
(	O
π|α	O
)	O
=	O
n	O
where	O
p	O
(	O
x|π	O
)	O
is	O
given	O
by	O
equation	B
(	O
22.2.3	O
)	O
on	O
replacing	O
α	O
with	O
π	O
.	O
22.2.3	O
glicko	O
and	O
trueskill	O
glicko	O
[	O
114	O
]	O
and	O
trueskill	O
[	O
130	O
]	O
are	O
essentially	O
bayesian	O
versions	O
of	O
the	O
elo	O
model	B
with	O
the	O
reﬁnement	O
that	O
the	O
latent	O
ability	O
is	O
modelled	O
,	O
not	O
by	O
a	O
single	O
number	O
,	O
but	O
by	O
a	O
gaussian	O
distribution	B
this	O
can	O
capture	O
the	O
fact	O
that	O
a	O
player	O
may	O
be	O
consistently	O
reasonable	O
(	O
quite	O
high	O
µi	O
and	O
low	O
σ2	O
erratic	O
genius	O
(	O
high	O
µi	O
but	O
with	O
large	O
σ2	O
i	O
)	O
.	O
the	O
parameters	O
of	O
the	O
model	B
are	O
then	O
i	O
)	O
or	O
an	O
(	O
22.2.7	O
)	O
for	O
a	O
set	O
of	O
s	O
players	O
.	O
the	O
interaction	O
model	B
p	O
(	O
x|α	O
)	O
is	O
as	O
for	O
the	O
win/lose	O
elo	O
model	B
,	O
equation	B
(	O
22.2.1	O
)	O
.	O
the	O
likelihood	B
for	O
the	O
model	B
given	O
the	O
parameters	O
is	O
(	O
22.2.5	O
)	O
(	O
22.2.6	O
)	O
(	O
22.2.8	O
)	O
i	O
p	O
(	O
αi|θi	O
)	O
=	O
n	O
(	O
cid:0	O
)	O
αi	O
µi	O
,	O
σ2	O
(	O
cid:1	O
)	O
θ	O
=	O
(	O
cid:8	O
)	O
µi	O
,	O
σ2	O
i	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
s	O
(	O
cid:9	O
)	O
(	O
cid:90	O
)	O
p	O
(	O
x|θ	O
)	O
=	O
p	O
(	O
x|α	O
)	O
p	O
(	O
α|θ	O
)	O
α	O
this	O
integral	O
is	O
formally	O
intractable	O
and	O
numerical	B
approximations	O
are	O
required	O
.	O
in	O
this	O
context	O
expecta-	O
tion	O
propagation	B
has	O
proven	O
to	O
be	O
a	O
useful	O
technique	O
[	O
195	O
]	O
.	O
the	O
trueskill	O
system	B
is	O
used	O
for	O
example	O
to	O
assess	O
the	O
abilities	O
of	O
players	O
in	O
online	B
gaming	O
,	O
also	O
taking	O
into	O
account	O
the	O
abilities	O
of	O
teams	O
of	O
individuals	O
in	O
tournaments	O
.	O
a	O
temporal	O
extension	O
has	O
recently	O
been	O
used	O
to	O
reevaluate	O
the	O
change	O
in	O
ability	O
of	O
chess	O
players	O
with	O
time	O
[	O
72	O
]	O
.	O
406	O
draft	O
march	O
9	O
,	O
2010	O
competitorcompetitor	O
2040608010010203040506070809010000.511.522.533.544.55−10−50510−10−50510true	O
abilityestimated	O
ability	O
exercises	O
22.3	O
code	O
rasch.m	O
:	O
rasch	O
model	B
training	O
demorasch.m	O
:	O
demo	O
for	O
the	O
rasch	O
model	B
22.4	O
exercises	O
exercise	O
213	O
(	O
bucking	O
bronco	O
)	O
.	O
bronco.mat	O
contains	O
information	O
about	O
a	O
bucking	O
bronco	O
competition	O
.	O
there	O
are	O
500	O
competitors	O
and	O
20	O
bucking	O
broncos	O
.	O
a	O
competitor	O
j	O
attempts	O
to	O
stay	O
on	O
a	O
bucking	O
bronco	O
i	O
for	O
a	O
minute	O
.	O
if	O
the	O
competitor	O
succeeds	O
,	O
the	O
entry	O
xij	O
is	O
1	O
,	O
otherwise	O
0.	O
each	O
competitor	O
gets	O
to	O
ride	O
three	O
bucking	O
broncos	O
only	O
(	O
the	O
missing	B
data	I
is	O
coded	O
as	O
nan	O
)	O
.	O
having	O
viewed	O
all	O
the	O
500	O
amateurs	O
,	O
desperate	O
dan	O
enters	O
the	O
competition	O
and	O
bribes	O
the	O
organisers	O
into	O
letting	O
him	O
avoid	O
having	O
to	O
ride	O
the	O
diﬃcult	O
broncos	O
.	O
based	O
on	O
using	O
a	O
rasch	O
model	B
,	O
which	O
are	O
the	O
top	O
10	O
most	O
diﬃcult	O
broncos	O
,	O
in	O
order	O
of	O
the	O
most	O
diﬃcult	O
ﬁrst	O
?	O
exercise	O
214	O
(	O
btl	O
training	B
)	O
.	O
1.	O
show	O
that	O
the	O
log	O
likelihood	B
for	O
the	O
bradly-terry-luce	O
model	B
is	O
given	O
by	O
xij	O
log	O
σ	O
(	O
αi	O
−	O
αj	O
)	O
(	O
22.4.1	O
)	O
l	O
(	O
α	O
)	O
=	O
(	O
cid:88	O
)	O
ij	O
where	O
xij	O
is	O
the	O
number	O
of	O
times	O
that	O
player	O
i	O
beats	O
player	O
j	O
in	O
a	O
set	O
of	O
games	O
.	O
2.	O
compute	O
the	O
gradient	B
of	O
l	O
(	O
α	O
)	O
.	O
3.	O
compute	O
the	O
hessian	O
of	O
the	O
btl	O
model	B
and	O
verify	O
that	O
it	O
is	O
negative	O
semideﬁnite	O
.	O
exercise	O
215	O
(	O
la	O
reine	O
)	O
.	O
1.	O
program	O
a	O
simple	O
gradient	O
ascent	O
routine	O
to	O
learn	O
the	O
latent	B
abilities	O
of	O
competitors	O
based	O
on	O
a	O
series	O
of	O
win/lose	O
outcomes	O
.	O
2.	O
in	O
a	O
modiﬁed	O
form	O
of	O
swiss	O
cow	O
‘	O
ﬁghting	O
’	O
,	O
a	O
set	O
of	O
cows	O
compete	O
by	O
pushing	O
each	O
other	O
until	O
submis-	O
sion	O
.	O
at	O
the	O
end	O
of	O
the	O
competition	O
one	O
cow	O
is	O
deemed	O
to	O
be	O
‘	O
la	O
reine	O
’	O
.	O
based	O
on	O
the	O
data	B
in	O
btl.mat	O
(	O
for	O
which	O
xij	O
contains	O
the	O
number	O
of	O
times	O
cow	O
i	O
beat	O
cow	O
j	O
)	O
,	O
ﬁt	O
a	O
btl	O
model	B
and	O
return	O
a	O
ranked	O
list	O
of	O
the	O
top	O
ten	O
best	O
ﬁghting	O
cows	O
,	O
‘	O
la	O
reine	O
’	O
ﬁrst	O
.	O
exercise	O
216.	O
an	O
extension	O
of	O
the	O
btl	O
model	B
is	O
to	O
consider	O
additional	O
‘	O
factors	O
’	O
that	O
describe	O
the	O
state	O
of	O
the	O
competitors	O
when	O
they	O
play	O
.	O
for	O
example	O
,	O
we	O
have	O
a	O
set	O
of	O
s	O
football	O
teams	O
,	O
and	O
a	O
set	O
of	O
matrices	O
ij	O
=	O
1	O
if	O
team	O
i	O
beat	O
team	O
j	O
in	O
match	O
n.	O
in	O
addition	O
we	O
have	O
for	O
each	O
match	O
and	O
team	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
with	O
x	O
n	O
i	O
∈	O
{	O
0	O
,	O
1	O
}	O
that	O
describes	O
the	O
team	O
.	O
for	O
example	O
,	O
for	O
the	O
team	O
i	O
=	O
1	O
(	O
madchester	O
a	O
vector	O
of	O
binary	O
factors	O
f	O
n	O
united	O
)	O
,	O
the	O
factor	B
f1,1	O
=	O
1	O
if	O
bozo	O
is	O
playing	O
,	O
0	O
if	O
not	O
.	O
it	O
is	O
suggested	O
that	O
the	O
ability	O
of	O
team	O
i	O
in	O
game	O
n	O
is	O
measured	O
by	O
αn	O
i	O
=	O
di	O
+	O
wh	O
,	O
if	O
n	O
h	O
,	O
i	O
(	O
22.4.2	O
)	O
h=1	O
h	O
,	O
i	O
=	O
1	O
if	O
factor	B
h	O
is	O
present	O
in	O
team	O
i	O
in	O
game	O
n.	O
di	O
is	O
a	O
default	O
latent	O
ability	O
of	O
the	O
team	O
which	O
where	O
f	O
n	O
is	O
assumed	O
constant	O
across	O
all	O
games	O
.	O
we	O
have	O
such	O
a	O
set	O
of	O
factors	O
for	O
each	O
match	O
,	O
giving	O
f	O
n	O
h	O
,	O
i	O
.	O
1.	O
using	O
the	O
above	O
deﬁnition	O
of	O
the	O
latent	O
ability	O
in	O
the	O
btl	O
model	B
,	O
our	O
interest	O
is	O
to	O
ﬁnd	O
the	O
weights	O
w	O
and	O
abilities	O
d	O
that	O
best	O
predict	O
the	O
ability	O
of	O
the	O
team	O
,	O
given	O
that	O
we	O
have	O
a	O
set	O
of	O
historical	O
plays	O
(	O
xn	O
,	O
fn	O
)	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
write	O
down	O
the	O
likelihood	B
for	O
the	O
btl	O
model	B
as	O
a	O
function	B
of	O
the	O
set	O
of	O
all	O
team	O
weights	O
w	O
,	O
d.	O
draft	O
march	O
9	O
,	O
2010	O
407	O
h	O
(	O
cid:88	O
)	O
exercises	O
2.	O
compute	O
the	O
gradient	B
of	O
the	O
log	O
likelihood	B
of	O
this	O
model	B
.	O
3.	O
explain	O
how	O
this	O
model	B
can	O
be	O
used	O
to	O
assess	O
the	O
importance	B
of	O
bozo	O
’	O
s	O
contribution	O
to	O
madchester	O
united	O
’	O
s	O
ability	O
.	O
4.	O
given	O
learned	O
w	O
,	O
d	O
and	O
the	O
knowledge	O
that	O
madchester	O
united	O
(	O
team	O
1	O
)	O
will	O
play	O
chelski	O
(	O
team	O
2	O
)	O
tomorrow	O
explain	O
how	O
,	O
given	O
the	O
list	O
of	O
factors	O
f	O
for	O
chelski	O
(	O
which	O
includes	O
issues	O
such	O
as	O
who	O
will	O
be	O
playing	O
in	O
the	O
team	O
)	O
,	O
one	O
can	O
select	O
the	O
best	O
madchester	O
united	O
team	O
to	O
maximise	O
the	O
probability	B
of	O
winning	O
the	O
game	O
.	O
408	O
draft	O
march	O
9	O
,	O
2010	O
part	O
iv	O
dynamical	O
models	O
409	O
chapter	O
23	O
discrete-state	O
markov	O
models	O
23.1	O
markov	O
models	O
time-series	O
are	O
datasets	O
for	O
which	O
the	O
constituent	O
datapoints	O
can	O
be	O
naturally	O
ordered	O
.	O
this	O
order	O
often	O
corresponds	O
to	O
an	O
underlying	O
single	O
physical	O
dimension	O
,	O
typically	O
time	O
,	O
though	O
any	O
other	O
single	O
dimension	O
may	O
be	O
used	O
.	O
the	O
time-series	O
models	O
we	O
consider	O
are	O
probability	B
models	O
over	O
a	O
collection	O
of	O
random	O
variables	O
v1	O
,	O
.	O
.	O
.	O
,	O
vt	O
with	O
individual	O
variables	O
vt	O
indexed	O
by	O
a	O
time	O
index	O
t.	O
these	O
indices	O
are	O
elements	O
of	O
the	O
index	O
set	O
t	O
.	O
for	O
nonnegative	O
indices	O
,	O
t	O
=	O
n+	O
,	O
the	O
model	B
is	O
a	O
discrete-time	O
process	O
.	O
continuous-	O
time	O
processes	O
,	O
t	O
=	O
r	O
,	O
are	O
natural	B
in	O
particular	O
application	O
domains	O
yet	O
require	O
additional	O
notation	O
and	O
concepts	O
.	O
we	O
therefore	O
focus	O
exclusively	O
on	O
discrete-time	O
models	O
.	O
a	O
probabilistic	B
time-series	O
model	B
requires	O
a	O
speciﬁcation	O
of	O
the	O
joint	B
distribution	O
p	O
(	O
v1	O
,	O
.	O
.	O
.	O
,	O
vt	O
)	O
.	O
for	O
the	O
case	O
in	O
which	O
the	O
observed	O
data	O
vt	O
is	O
discrete	B
,	O
the	O
joint	B
probability	O
table	O
for	O
p	O
(	O
v1	O
,	O
.	O
.	O
.	O
,	O
vt	O
)	O
has	O
exponentially	O
many	O
entries	O
.	O
we	O
can	O
not	O
expect	O
to	O
independently	O
specify	O
all	O
the	O
exponentially	O
many	O
entries	O
and	O
are	O
therefore	O
forced	O
to	O
make	O
simpliﬁed	O
models	O
under	O
which	O
these	O
entries	O
can	O
be	O
parameterised	O
in	O
a	O
lower	O
dimensional	O
manner	O
.	O
such	O
simpliﬁcations	O
are	O
at	O
the	O
heart	O
of	O
time-series	O
modelling	B
and	O
we	O
will	O
discuss	O
some	O
classical	O
models	O
in	O
the	O
following	O
sections	O
.	O
deﬁnition	O
107	O
(	O
time-series	O
notation	O
)	O
.	O
xa	O
:	O
b	O
≡	O
xa	O
,	O
xa+1	O
,	O
.	O
.	O
.	O
,	O
xb	O
,	O
with	O
xa	O
:	O
b	O
=	O
xa	O
for	O
b	O
≤	O
a	O
(	O
23.1.1	O
)	O
for	O
timeseries	O
data	B
v1	O
,	O
.	O
.	O
.	O
,	O
vt	O
,	O
we	O
need	O
a	O
model	B
p	O
(	O
v1	O
:	O
t	O
)	O
.	O
for	O
consistency	O
with	O
the	O
causal	B
nature	O
of	O
time	O
,	O
it	O
is	O
meaningful	O
to	O
consider	O
the	O
decomposition	B
t	O
(	O
cid:89	O
)	O
t=1	O
p	O
(	O
v1	O
:	O
t	O
)	O
=	O
p	O
(	O
vt|v1	O
:	O
t−1	O
)	O
(	O
23.1.2	O
)	O
with	O
the	O
convention	O
p	O
(	O
vt|v1	O
:	O
t−1	O
)	O
=	O
p	O
(	O
v1	O
)	O
for	O
t	O
=	O
1.	O
it	O
is	O
often	O
natural	B
to	O
assume	O
that	O
the	O
inﬂuence	O
of	O
the	O
immediate	O
past	O
is	O
more	O
relevant	O
than	O
the	O
remote	O
past	O
and	O
in	O
markov	O
models	O
only	O
a	O
limited	O
number	O
of	O
previous	O
observations	O
are	O
required	O
to	O
predict	O
the	O
future	O
.	O
deﬁnition	O
108	O
(	O
markov	O
chain	B
)	O
.	O
a	O
markov	O
chain	B
deﬁned	O
on	O
either	O
discrete	B
or	O
continuous	B
variables	O
v1	O
:	O
t	O
is	O
one	O
in	O
which	O
the	O
following	O
conditional	B
independence	O
assumption	O
holds	O
:	O
p	O
(	O
vt|v1	O
,	O
.	O
.	O
.	O
,	O
vt−1	O
)	O
=	O
p	O
(	O
vt|vt−l	O
,	O
.	O
.	O
.	O
,	O
vt−1	O
)	O
(	O
23.1.3	O
)	O
411	O
markov	O
models	O
v1	O
v2	O
v3	O
v4	O
v1	O
v2	O
v3	O
v4	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
23.1	O
:	O
(	O
a	O
)	O
:	O
first	O
order	O
markov	O
chain	B
.	O
(	O
b	O
)	O
:	O
second	O
order	O
markov	O
chain	B
.	O
where	O
l	O
≥	O
1	O
is	O
the	O
order	O
of	O
the	O
markov	O
chain	B
and	O
vt	O
=	O
∅	O
for	O
t	O
<	O
1.	O
for	O
a	O
ﬁrst	B
order	I
markov	O
chain	B
,	O
p	O
(	O
v1	O
:	O
t	O
)	O
=	O
p	O
(	O
v1	O
)	O
p	O
(	O
v2|v1	O
)	O
p	O
(	O
v3|v2	O
)	O
.	O
.	O
.	O
p	O
(	O
vt|vt−1	O
)	O
for	O
a	O
stationary	B
markov	O
chain	B
the	O
transitions	O
p	O
(	O
vt	O
=	O
s	O
(	O
cid:48	O
)	O
wise	O
the	O
chain	B
is	O
non-stationary	B
,	O
p	O
(	O
vt	O
=	O
s	O
(	O
cid:48	O
)	O
|vt−1	O
=	O
s	O
)	O
=	O
f	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
s	O
,	O
t	O
)	O
.	O
|vt−1	O
=	O
s	O
)	O
=	O
f	O
(	O
s	O
(	O
cid:48	O
)	O
,	O
s	O
)	O
are	O
time-independent	O
.	O
other-	O
(	O
23.1.4	O
)	O
23.1.1	O
equilibrium	O
and	O
stationary	B
distribution	I
of	O
a	O
markov	O
chain	B
the	O
stationary	B
distribution	I
p∞	O
of	O
a	O
markov	O
chain	B
with	O
transition	B
matrix	I
m	O
is	O
deﬁned	O
by	O
the	O
condition	O
p∞	O
(	O
i	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:125	O
)	O
(	O
cid:124	O
)	O
p	O
(	O
xt	O
=	O
i|xt−1	O
=	O
j	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
mij	O
j	O
p∞	O
(	O
j	O
)	O
in	O
matrix	B
notation	O
this	O
can	O
be	O
written	O
as	O
the	O
vector	O
equation	O
p∞	O
=	O
mp∞	O
so	O
that	O
the	O
stationary	B
distribution	I
is	O
proportional	O
to	O
the	O
eigenvector	O
with	O
unit	O
eigenvalue	O
of	O
the	O
transition	B
matrix	I
.	O
note	O
that	O
there	O
may	O
be	O
more	O
than	O
one	O
stationary	B
distribution	I
.	O
see	O
exercise	O
(	O
217	O
)	O
and	O
[	O
122	O
]	O
.	O
given	O
a	O
state	O
x1	O
,	O
we	O
can	O
iteratively	O
draw	O
samples	O
x2	O
,	O
.	O
.	O
.	O
,	O
xt	O
from	O
the	O
markov	O
chain	B
drawing	O
a	O
sample	B
from	O
p	O
(	O
x2|x1	O
=	O
x1	O
)	O
,	O
and	O
then	O
from	O
p	O
(	O
x3|x2	O
)	O
etc	O
.	O
as	O
we	O
repeatedly	O
sample	B
a	O
new	O
state	O
from	O
the	O
chain	B
,	O
the	O
distribution	B
at	O
time	O
t	O
,	O
for	O
an	O
initial	O
distribution	B
p1	O
(	O
i	O
)	O
=	O
δ	O
(	O
i	O
,	O
x1	O
)	O
,	O
is	O
pt	O
=	O
mtp1	O
(	O
23.1.7	O
)	O
if	O
for	O
t	O
→	O
∞	O
,	O
p∞	O
is	O
independent	O
of	O
the	O
initial	O
distribution	B
p1	O
,	O
then	O
p∞	O
is	O
called	O
the	O
equilibrium	O
distribution	B
of	O
the	O
chain	B
.	O
see	O
exercise	O
(	O
218	O
)	O
for	O
an	O
example	O
of	O
a	O
markov	O
chain	B
which	O
does	O
not	O
have	O
an	O
equilibrium	O
distribution	B
.	O
(	O
23.1.5	O
)	O
(	O
23.1.6	O
)	O
(	O
23.1.8	O
)	O
(	O
23.1.9	O
)	O
example	O
95	O
(	O
pagerank	O
)	O
.	O
despite	O
their	O
apparent	O
simplicity	O
,	O
markov	O
chains	O
have	O
been	O
put	O
to	O
interesting	O
use	O
in	O
information	B
retrieval	I
and	O
search-engines	O
.	O
deﬁne	O
the	O
matrix	B
(	O
cid:26	O
)	O
1	O
0	O
aij	O
=	O
if	O
website	B
j	O
has	O
a	O
hyperlink	O
to	O
website	B
i	O
otherwise	O
from	O
this	O
we	O
can	O
deﬁne	O
a	O
markov	O
transition	B
matrix	I
with	O
elements	O
mij	O
=	O
aij	O
(	O
cid:80	O
)	O
i	O
aij	O
the	O
equilibrium	O
distribution	B
of	O
this	O
markov	O
chain	B
has	O
the	O
interpretation	O
:	O
if	O
follow	O
links	O
at	O
random	O
,	O
jumping	O
from	O
website	B
to	O
website	B
,	O
the	O
equilibrium	O
distribution	B
component	O
p∞	O
(	O
i	O
)	O
is	O
the	O
relative	O
number	O
of	O
times	O
we	O
will	O
visit	O
website	B
i.	O
this	O
has	O
a	O
natural	B
interpretation	O
as	O
the	O
‘	O
importance	B
’	O
of	O
website	B
i	O
;	O
if	O
a	O
website	B
412	O
draft	O
march	O
9	O
,	O
2010	O
markov	O
models	O
is	O
isolated	O
in	O
the	O
web	B
,	O
it	O
will	O
be	O
visited	O
infrequently	O
by	O
random	O
hopping	O
;	O
if	O
a	O
website	B
is	O
linked	O
by	O
many	O
others	O
it	O
will	O
be	O
visited	O
more	O
frequently	O
.	O
a	O
crude	O
search	B
engine	I
works	O
then	O
as	O
follows	O
.	O
for	O
each	O
website	B
i	O
a	O
list	O
of	O
words	O
associated	O
with	O
that	O
website	B
is	O
collected	O
.	O
after	O
doing	O
this	O
for	O
all	O
websites	O
,	O
one	O
can	O
make	O
an	O
‘	O
inverse	O
’	O
list	O
of	O
which	O
websites	O
contain	O
word	O
w.	O
when	O
a	O
user	O
searches	O
for	O
word	O
w	O
,	O
the	O
list	O
of	O
websites	O
that	O
that	O
word	O
is	O
then	O
returned	O
,	O
ranked	O
according	O
to	O
the	O
importance	B
of	O
the	O
site	O
(	O
as	O
deﬁned	O
by	O
the	O
equilibrium	O
distribution	B
)	O
.	O
this	O
is	O
a	O
crude	O
summary	O
as	O
how	O
early	O
search	O
engines	O
worked	O
,	O
infolab.stanford.edu/∼backrub/google.html	O
.	O
1	O
3	O
2	O
figure	O
23.2	O
:	O
a	O
state	O
transition	O
diagram	O
for	O
a	O
three	O
state	O
markov	O
chain	B
.	O
note	O
that	O
a	O
state	O
transition	O
diagram	O
is	O
not	O
a	O
graphical	O
model	B
–	O
it	O
simply	O
displays	O
the	O
non-zero	O
entries	O
of	O
the	O
transition	B
matrix	I
p	O
(	O
i|j	O
)	O
.	O
the	O
absence	O
of	O
link	O
from	O
j	O
to	O
i	O
indicates	O
that	O
p	O
(	O
i|j	O
)	O
=	O
0	O
.	O
23.1.2	O
fitting	O
markov	O
models	O
given	O
a	O
sequence	O
v1	O
:	O
t	O
,	O
ﬁtting	O
a	O
stationary	B
markov	O
chain	B
by	O
maximum	B
likelihood	I
corresponds	O
to	O
setting	O
the	O
transitions	O
by	O
counting	B
the	O
number	O
of	O
observed	O
(	O
ﬁrst-order	O
)	O
transitions	O
in	O
the	O
sequence	O
:	O
p	O
(	O
vτ	O
=	O
i|vτ−1	O
=	O
j	O
)	O
∝	O
i	O
[	O
vt	O
=	O
i	O
,	O
vt−1	O
=	O
j	O
]	O
(	O
23.1.10	O
)	O
to	O
show	O
this	O
,	O
for	O
convenience	O
we	O
write	O
p	O
(	O
vτ	O
=	O
i|vτ−1	O
=	O
j	O
)	O
≡	O
θi|j	O
,	O
so	O
that	O
the	O
likelihood	B
is	O
(	O
assuming	O
v1	O
is	O
known	O
)	O
:	O
t	O
(	O
cid:88	O
)	O
t=2	O
i	O
[	O
vt=i	O
,	O
vt−1=j	O
]	O
i|j	O
θ	O
(	O
23.1.11	O
)	O
t	O
(	O
cid:89	O
)	O
(	O
cid:89	O
)	O
t	O
(	O
cid:89	O
)	O
(	O
cid:89	O
)	O
θvt|vt−1	O
=	O
t=2	O
ij	O
t=2	O
ij	O
p	O
(	O
v2	O
:	O
t|θ	O
,	O
v1	O
)	O
=	O
(	O
cid:88	O
)	O
t	O
(	O
cid:88	O
)	O
l	O
(	O
θ	O
)	O
=	O
taking	O
logs	O
and	O
adding	O
the	O
lagrange	O
constraint	O
for	O
the	O
normalisation	B
,	O
i	O
[	O
vt	O
=	O
i	O
,	O
vt−1	O
=	O
j	O
]	O
log	O
θi|j	O
+	O
(	O
cid:88	O
)	O
λj	O
t=2	O
ij	O
j	O
(	O
cid:33	O
)	O
(	O
cid:32	O
)	O
1	O
−	O
(	O
cid:88	O
)	O
i	O
θi|j	O
(	O
23.1.12	O
)	O
diﬀerentiating	O
with	O
respect	O
to	O
θi|j	O
and	O
equating	O
to	O
zero	O
,	O
we	O
immediately	O
arrive	O
at	O
the	O
intuitive	O
setting	O
,	O
equation	B
(	O
23.1.10	O
)	O
.	O
for	O
a	O
set	O
of	O
timeseries	O
,	O
vn	O
1	O
:	O
tn	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
the	O
transition	O
is	O
given	O
by	O
counting	B
all	O
transitions	O
across	O
time	O
and	O
datapoints	O
.	O
the	O
maximum	B
likelihood	I
setting	O
for	O
the	O
initial	O
ﬁrst	O
timestep	O
distribution	B
is	O
p	O
(	O
v1	O
=	O
i	O
)	O
∝	O
bayesian	O
ﬁtting	O
(	O
cid:80	O
)	O
1	O
=	O
i	O
]	O
.	O
i	O
[	O
vn	O
n	O
for	O
simplicity	O
,	O
we	O
assume	O
a	O
factorised	B
prior	O
on	O
the	O
transition	O
p	O
(	O
θ·|j	O
)	O
(	O
23.1.13	O
)	O
a	O
convenient	O
choice	O
for	O
each	O
conditional	B
transition	O
is	O
a	O
dirichlet	O
distribution	B
with	O
hyperparameters	O
uj	O
,	O
p	O
(	O
θ	O
)	O
=	O
(	O
cid:89	O
)	O
j	O
p	O
(	O
θ·|j	O
)	O
=	O
dirichlet	O
(	O
cid:0	O
)	O
θ·|j|uj	O
where	O
ˆuj	O
=	O
(	O
cid:80	O
)	O
t	O
t=2	O
p	O
(	O
θ|v1	O
:	O
t	O
)	O
∝	O
p	O
(	O
v1	O
:	O
t|θ	O
)	O
p	O
(	O
θ	O
)	O
∝	O
draft	O
march	O
9	O
,	O
2010	O
(	O
cid:1	O
)	O
,	O
since	O
this	O
is	O
conjugate	B
to	O
the	O
categorical	O
transition	O
and	O
(	O
cid:1	O
)	O
dirichlet	O
(	O
cid:0	O
)	O
θ·|j|ˆuj	O
i	O
[	O
vt=i	O
,	O
vt−1=j	O
]	O
i|j	O
=	O
(	O
cid:89	O
)	O
(	O
cid:89	O
)	O
(	O
cid:89	O
)	O
uij−1	O
i|j	O
θ	O
θ	O
t	O
ij	O
j	O
i	O
[	O
vt−1	O
=	O
i	O
,	O
vt	O
=	O
j	O
]	O
,	O
being	O
the	O
number	O
of	O
j	O
→	O
i	O
transitions	O
in	O
the	O
dataset	O
.	O
(	O
23.1.14	O
)	O
413	O
markov	O
models	O
h	O
v1	O
v2	O
v3	O
v4	O
(	O
a	O
)	O
figure	O
23.3	O
:	O
mixture	O
of	O
ﬁrst	O
order	O
markov	O
chains	O
.	O
the	O
discrete	B
hidden	O
variable	B
dom	O
(	O
h	O
)	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
h	O
}	O
indexes	O
the	O
markov	O
chain	B
p	O
(	O
vt|vt−1	O
,	O
h	O
)	O
.	O
such	O
models	O
can	O
be	O
useful	O
as	O
simple	O
sequence	O
clustering	O
tools	O
.	O
23.1.3	O
mixture	O
of	O
markov	O
models	O
given	O
a	O
set	O
of	O
sequences	B
v	O
=	O
{	O
vn	O
1	O
:	O
t	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
,	O
how	O
might	O
we	O
cluster	O
them	O
?	O
to	O
keep	O
the	O
notation	O
less	O
cluttered	O
,	O
we	O
assume	O
that	O
all	O
sequences	B
are	O
of	O
the	O
same	O
length	O
t	O
with	O
the	O
extension	O
to	O
diﬀering	O
lengths	O
being	O
straightforward	O
.	O
one	O
simple	O
approach	O
is	O
to	O
ﬁt	O
a	O
mixture	O
of	O
markov	O
models	O
.	O
assuming	O
the	O
data	B
1	O
:	O
t	O
)	O
,	O
we	O
deﬁne	O
a	O
mixture	B
model	I
for	O
a	O
single	O
datapoint	O
v1	O
:	O
t	O
.	O
here	O
we	O
assume	O
each	O
n	O
p	O
(	O
vn	O
component	O
model	B
is	O
ﬁrst	B
order	I
markov	O
is	O
i.i.d.	O
,	O
p	O
(	O
v	O
)	O
=	O
(	O
cid:81	O
)	O
h	O
(	O
cid:88	O
)	O
p	O
(	O
v1	O
:	O
t	O
)	O
=	O
p	O
(	O
h	O
)	O
p	O
(	O
v1	O
:	O
t|h	O
)	O
=	O
h=1	O
p	O
(	O
h	O
)	O
p	O
(	O
vt|vt−1	O
,	O
h	O
)	O
(	O
23.1.15	O
)	O
the	O
graphical	O
model	B
is	O
depicted	O
in	O
ﬁg	O
(	O
23.3	O
)	O
.	O
clustering	B
can	O
then	O
be	O
achieved	O
by	O
ﬁnding	O
the	O
maximum	B
likelihood	I
parameters	O
p	O
(	O
h	O
)	O
,	O
p	O
(	O
vt|vt−1	O
,	O
h	O
)	O
and	O
subsequently	O
assigning	O
the	O
clusters	O
according	O
to	O
p	O
(	O
h|vn	O
1	O
:	O
t	O
)	O
.	O
below	O
we	O
discuss	O
the	O
application	O
of	O
the	O
em	O
algorithm	B
to	O
this	O
model	B
to	O
learn	O
the	O
maximum	B
likelihood	I
parameters	O
.	O
em	O
algorithm	B
h	O
(	O
cid:88	O
)	O
t	O
(	O
cid:89	O
)	O
h=1	O
t=1	O
under	O
the	O
i.i.d	O
.	O
data	B
assumption	O
,	O
the	O
log	O
likelihood	B
is	O
n	O
(	O
cid:88	O
)	O
h	O
(	O
cid:88	O
)	O
t	O
(	O
cid:89	O
)	O
n=1	O
h=1	O
t=1	O
log	O
p	O
(	O
h	O
)	O
p	O
(	O
vn	O
t−1	O
,	O
h	O
)	O
for	O
the	O
m-step	O
,	O
our	O
task	O
is	O
to	O
maximise	O
the	O
energy	B
(	O
cid:104	O
)	O
log	O
p	O
(	O
vn	O
1	O
:	O
t	O
,	O
h	O
)	O
(	O
cid:105	O
)	O
pold	O
(	O
h|vn	O
1	O
:	O
t	O
)	O
=	O
the	O
contribution	O
to	O
the	O
energy	B
from	O
the	O
parameter	B
p	O
(	O
h	O
)	O
is	O
t	O
|vn	O
n	O
(	O
cid:88	O
)	O
n=1	O
(	O
cid:40	O
)	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
h	O
)	O
(	O
cid:105	O
)	O
pold	O
(	O
h|vn	O
1	O
:	O
t	O
)	O
+	O
(	O
23.1.16	O
)	O
(	O
cid:41	O
)	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
vt|vt−1	O
,	O
h	O
)	O
(	O
cid:105	O
)	O
pold	O
(	O
h|vn	O
1	O
:	O
t	O
)	O
t	O
(	O
cid:88	O
)	O
t=1	O
log	O
p	O
(	O
v	O
)	O
=	O
n	O
(	O
cid:88	O
)	O
e	O
=	O
n=1	O
n	O
(	O
cid:88	O
)	O
n=1	O
by	O
deﬁning	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
h	O
)	O
(	O
cid:105	O
)	O
pold	O
(	O
h|vn	O
1	O
:	O
t	O
)	O
n	O
(	O
cid:88	O
)	O
n=1	O
ˆpold	O
(	O
h	O
)	O
∝	O
(	O
cid:16	O
)	O
kl	O
1	O
:	O
t	O
)	O
pold	O
(	O
h|vn	O
(	O
cid:17	O
)	O
ˆpold	O
(	O
h	O
)	O
|p	O
(	O
h	O
)	O
n	O
(	O
cid:88	O
)	O
pnew	O
(	O
h	O
)	O
∝	O
pold	O
(	O
h|vn	O
1	O
:	O
t	O
)	O
n=1	O
(	O
23.1.17	O
)	O
(	O
23.1.18	O
)	O
(	O
23.1.19	O
)	O
(	O
23.1.20	O
)	O
one	O
can	O
view	O
maximising	O
(	O
23.1.17	O
)	O
as	O
equivalent	O
to	O
minimising	O
so	O
that	O
the	O
optimal	O
choice	O
from	O
the	O
m-step	O
is	O
to	O
set	O
pnew	O
=	O
ˆpold	O
,	O
namely	O
for	O
those	O
less	O
comfortable	O
with	O
this	O
argument	O
,	O
a	O
direct	O
maximisation	B
including	O
a	O
lagrange	O
term	O
to	O
ensure	O
normalisation	B
of	O
p	O
(	O
h	O
)	O
can	O
be	O
used	O
to	O
derive	O
the	O
same	O
result	O
.	O
414	O
draft	O
march	O
9	O
,	O
2010	O
markov	O
models	O
similarly	O
,	O
the	O
m-step	O
for	O
p	O
(	O
vt|vt−1	O
,	O
h	O
)	O
is	O
n	O
(	O
cid:88	O
)	O
pnew	O
(	O
vt	O
=	O
i|vt−1	O
=	O
j	O
,	O
h	O
=	O
k	O
)	O
∝	O
n=1	O
t	O
(	O
cid:88	O
)	O
t=2	O
pold	O
(	O
h	O
=	O
k|vn	O
1	O
:	O
t	O
)	O
t	O
=	O
i	O
]	O
i	O
(	O
cid:2	O
)	O
vn	O
t−1	O
=	O
j	O
(	O
cid:3	O
)	O
i	O
[	O
vn	O
the	O
initial	O
term	O
p	O
(	O
v1|h	O
)	O
is	O
updated	O
using	O
pnew	O
(	O
v1	O
=	O
i|h	O
=	O
k	O
)	O
∝	O
the	O
e-step	O
sets	O
1	O
:	O
t	O
)	O
i	O
[	O
vn	O
1	O
=	O
i	O
]	O
n	O
(	O
cid:88	O
)	O
n=1	O
pold	O
(	O
h	O
=	O
k|vn	O
t	O
(	O
cid:89	O
)	O
pold	O
(	O
h|vn	O
1	O
:	O
t	O
)	O
∝	O
p	O
(	O
h	O
)	O
p	O
(	O
vn	O
1	O
:	O
t|h	O
)	O
=	O
p	O
(	O
h	O
)	O
p	O
(	O
vn	O
t	O
|vn	O
t−1	O
,	O
h	O
)	O
t=1	O
(	O
23.1.21	O
)	O
(	O
23.1.22	O
)	O
(	O
23.1.23	O
)	O
given	O
an	O
initialisation	O
,	O
the	O
em	O
algorithm	B
then	O
iterates	O
(	O
23.1.20	O
)	O
,	O
(	O
23.1.21	O
)	O
,	O
(	O
23.1.22	O
)	O
and	O
(	O
23.1.23	O
)	O
until	O
convergence	O
.	O
for	O
long	O
sequences	B
,	O
explicitly	O
computing	O
the	O
product	O
of	O
many	O
terms	O
may	O
lead	O
to	O
numerical	B
underﬂow	O
issues	O
.	O
in	O
practice	O
it	O
is	O
therefore	O
best	O
to	O
work	O
with	O
logs	O
,	O
t	O
(	O
cid:88	O
)	O
t=1	O
log	O
pold	O
(	O
h|vn	O
1	O
:	O
t	O
)	O
=	O
log	O
p	O
(	O
h	O
)	O
+	O
log	O
p	O
(	O
vn	O
t	O
|vn	O
t−1	O
,	O
h	O
)	O
+	O
const	O
.	O
(	O
23.1.24	O
)	O
in	O
this	O
way	O
any	O
large	O
constants	O
common	O
to	O
all	O
h	O
can	O
be	O
removed	O
and	O
the	O
distribution	B
may	O
be	O
computed	O
accurately	O
.	O
see	O
mixmarkov.m	O
.	O
example	O
96	O
(	O
gene	O
clustering	B
)	O
.	O
consider	O
the	O
20	O
ﬁctitious	O
gene	O
sequences	B
below	O
presented	O
in	O
an	O
arbitrarily	O
chosen	O
order	O
.	O
each	O
sequence	O
consists	O
of	O
20	O
symbols	O
from	O
the	O
set	O
{	O
a	O
,	O
c	O
,	O
g	O
,	O
t	O
}	O
.	O
the	O
task	O
is	O
to	O
try	O
to	O
cluster	O
these	O
sequences	B
into	O
two	O
groups	O
,	O
based	O
on	O
the	O
(	O
perhaps	O
biologically	O
unrealistic	O
)	O
assumption	O
that	O
gene	O
sequences	B
in	O
the	O
same	O
cluster	O
follow	O
a	O
stationary	B
markov	O
chain	B
.	O
cataggcattctatgtgctg	O
gtgcctggacctgaaaagcc	O
gttggtcagcacacggactg	O
taagtgtcctctgctcctaa	O
gccaagcagggtctcaactt	O
ccagttacggacgccgaaag	O
cggccgcgcctccgggaacg	O
cctcccctcccctttcctgc	O
caccatcacccttgctaagg	O
catggactgctccacaaagg	O
tggaaccttaaaaaaaaaaa	O
aaagtgctctgaaaactcac	O
cactacggctacctgggcaa	O
aaagaactcccctccctgcc	O
aaaaaaacgaaaaacctaag	O
gtctcctgccctctctgaac	O
acatgaactacatagtataa	O
cggtccgtccgaggcactc	O
caaatgcctcacgcgtctca	O
gcgtaaaaaaagtcctgggt	O
(	O
23.1.25	O
)	O
a	O
simple	O
approach	O
is	O
to	O
assume	O
that	O
the	O
sequences	B
are	O
generated	O
from	O
a	O
two-component	O
h	O
=	O
2	O
mixture	O
of	O
markov	O
models	O
and	O
train	O
the	O
model	B
using	O
maximum	B
likelihood	I
.	O
the	O
likelihood	B
has	O
local	B
optima	O
so	O
that	O
the	O
procedure	O
needs	O
to	O
be	O
run	O
several	O
times	O
and	O
the	O
solution	O
with	O
the	O
highest	O
likelihood	B
chosen	O
.	O
one	O
can	O
1	O
:	O
t	O
)	O
.	O
if	O
this	O
posterior	B
probability	O
is	O
greater	O
than	O
then	O
assign	O
each	O
of	O
the	O
sequences	B
by	O
examining	O
p	O
(	O
h	O
=	O
1|vn	O
0.5	O
,	O
we	O
assign	O
it	O
to	O
class	O
1	O
,	O
otherwise	O
to	O
class	O
2.	O
using	O
this	O
procedure	O
,	O
we	O
ﬁnd	O
the	O
following	O
clusters	O
:	O
cataggcattctatgtgctg	O
ccagttacggacgccgaaag	O
cggccgcgcctccgggaacg	O
acatgaactacatagtataa	O
gttggtcagcacacggactg	O
cactacggctacctgggcaa	O
cggtccgtccgaggcactcg	O
caccatcacccttgctaagg	O
caaatgcctcacgcgtctca	O
gccaagcagggtctcaactt	O
catggactgctccacaaagg	O
tggaaccttaaaaaaaaaaa	O
gtctcctgccctctctgaac	O
gtgcctggacctgaaaagcc	O
aaagtgctctgaaaactcac	O
cctcccctcccctttcctgc	O
taagtgtcctctgctcctaa	O
aaagaactcccctccctgcc	O
aaaaaaacgaaaaacctaag	O
gcgtaaaaaaagtcctgggt	O
(	O
23.1.26	O
)	O
where	O
sequences	B
in	O
the	O
ﬁrst	O
column	O
are	O
assigned	O
to	O
cluster	O
1	O
,	O
and	O
sequences	B
in	O
the	O
second	O
column	O
to	O
cluster	O
2.	O
in	O
this	O
case	O
the	O
data	B
in	O
(	O
23.1.25	O
)	O
was	O
in	O
fact	O
generated	O
by	O
a	O
two-component	O
markov	O
mixture	B
and	O
the	O
posterior	B
assignment	O
(	O
23.1.26	O
)	O
is	O
in	O
agreement	O
with	O
the	O
known	O
clusters	O
.	O
see	O
demomixmarkov.m	O
draft	O
march	O
9	O
,	O
2010	O
415	O
hidden	B
markov	O
models	O
h1	O
v1	O
h2	O
v2	O
h3	O
v3	O
h4	O
v4	O
figure	O
23.4	O
:	O
a	O
ﬁrst	B
order	I
hidden	O
markov	O
model	B
with	O
‘	O
hidden	B
’	O
variables	O
dom	O
(	O
ht	O
)	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
h	O
}	O
,	O
t	O
=	O
1	O
:	O
t	O
.	O
the	O
‘	O
visible	B
’	O
variables	O
vt	O
can	O
be	O
either	O
discrete	B
or	O
con-	O
tinuous	O
.	O
23.2	O
hidden	B
markov	O
models	O
the	O
hidden	B
markov	O
model	B
(	O
hmm	O
)	O
deﬁnes	O
a	O
markov	O
chain	B
on	O
hidden	B
(	O
or	O
‘	O
latent	B
’	O
)	O
variables	O
h1	O
:	O
t	O
.	O
the	O
observed	O
(	O
or	O
‘	O
visible	B
’	O
)	O
variables	O
are	O
dependent	O
on	O
the	O
hidden	B
variables	I
through	O
an	O
emission	O
p	O
(	O
vt|ht	O
)	O
.	O
this	O
deﬁnes	O
a	O
joint	B
distribution	O
p	O
(	O
h1	O
:	O
t	O
,	O
v1	O
:	O
t	O
)	O
=	O
p	O
(	O
v1|h1	O
)	O
p	O
(	O
h1	O
)	O
p	O
(	O
vt|ht	O
)	O
p	O
(	O
ht|ht−1	O
)	O
(	O
23.2.1	O
)	O
for	O
which	O
the	O
graphical	O
model	B
is	O
depicted	O
in	O
ﬁg	O
(	O
23.4	O
)	O
.	O
for	O
a	O
stationary	B
hmm	O
the	O
transition	O
p	O
(	O
ht|ht−1	O
)	O
and	O
emission	O
p	O
(	O
vt|ht	O
)	O
distributions	O
are	O
constant	O
through	O
time	O
.	O
the	O
use	O
of	O
the	O
hmm	O
is	O
widespread	O
and	O
a	O
subset	O
of	O
the	O
many	O
applications	O
of	O
hmms	O
is	O
given	O
in	O
section	O
(	O
23.5	O
)	O
.	O
t	O
(	O
cid:89	O
)	O
t=2	O
deﬁnition	O
109	O
(	O
transition	B
distribution	I
)	O
.	O
for	O
a	O
stationary	B
hmm	O
the	O
transition	B
distribution	I
p	O
(	O
ht+1|ht	O
)	O
is	O
deﬁned	O
by	O
the	O
h	O
×	O
h	O
transition	B
matrix	I
(	O
cid:48	O
)	O
ai	O
(	O
cid:48	O
)	O
,	O
i	O
=	O
p	O
(	O
ht+1	O
=	O
i	O
|ht	O
=	O
i	O
)	O
and	O
an	O
initial	O
distribution	B
ai	O
=	O
p	O
(	O
h1	O
=	O
i	O
)	O
.	O
(	O
23.2.2	O
)	O
(	O
23.2.3	O
)	O
deﬁnition	O
110	O
(	O
emission	B
distribution	I
)	O
.	O
for	O
a	O
stationary	B
hmm	O
and	O
emission	B
distribution	I
p	O
(	O
vt|ht	O
)	O
with	O
discrete	O
states	O
vt	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
v	O
}	O
,	O
we	O
deﬁne	O
a	O
v	O
×	O
h	O
emission	B
matrix	I
bi	O
,	O
j	O
=	O
p	O
(	O
vt	O
=	O
i|ht	O
=	O
j	O
)	O
(	O
23.2.4	O
)	O
for	O
continuous	B
outputs	O
,	O
ht	O
selects	O
one	O
of	O
h	O
possible	O
output	O
distributions	O
p	O
(	O
vt|ht	O
)	O
,	O
ht	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
h	O
}	O
.	O
in	O
the	O
engineering	O
and	O
machine	O
learning	B
communities	O
,	O
the	O
term	O
hmm	O
typically	O
refers	O
to	O
the	O
case	O
of	O
discrete	B
variables	O
ht	O
,	O
a	O
convention	O
that	O
we	O
adopt	O
here	O
.	O
in	O
statistics	O
the	O
term	O
hmm	O
often	O
refers	O
to	O
any	O
model	B
with	O
the	O
independence	B
structure	O
in	O
equation	B
(	O
23.2.1	O
)	O
,	O
regardless	O
of	O
the	O
form	O
of	O
the	O
variables	O
ht	O
(	O
see	O
for	O
example	O
[	O
54	O
]	O
)	O
.	O
23.2.1	O
the	O
classical	O
inference	B
problems	O
filtering	O
prediction	B
smoothing	O
likelihood	B
most	O
likely	O
hidden	B
path	O
(	O
viterbi	B
alignment	O
)	O
(	O
inferring	O
the	O
present	O
)	O
(	O
inferring	O
the	O
future	O
)	O
(	O
inferring	O
the	O
past	O
)	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
p	O
(	O
ht|v1	O
:	O
s	O
)	O
p	O
(	O
ht|v1	O
:	O
u	O
)	O
p	O
(	O
v1	O
:	O
t	O
)	O
argmax	O
h1	O
:	O
t	O
p	O
(	O
h1	O
:	O
t|v1	O
:	O
t	O
)	O
t	O
>	O
s	O
t	O
<	O
u	O
the	O
most	O
likely	O
hidden	O
path	B
problem	O
is	O
termed	O
viterbi	B
alignment	O
in	O
the	O
engineering	O
literature	O
.	O
all	O
these	O
classical	O
inference	B
problems	O
are	O
straightforward	O
since	O
the	O
distribution	B
is	O
singly-connected	B
,	O
so	O
that	O
any	O
standard	O
inference	O
method	O
can	O
be	O
adopted	O
for	O
these	O
problems	O
.	O
the	O
factor	B
graph	I
and	O
junction	O
trees	O
for	O
416	O
draft	O
march	O
9	O
,	O
2010	O
hidden	B
markov	O
models	O
h1	O
v1	O
h3	O
v3	O
h2	O
v2	O
(	O
a	O
)	O
h4	O
v4	O
v1	O
,	O
h1	O
h1	O
h1	O
,	O
h2	O
h2	O
h2	O
,	O
h3	O
h3	O
h3	O
,	O
h4	O
h2	O
v2	O
,	O
h2	O
h3	O
v3	O
,	O
h3	O
h4	O
v4	O
,	O
h4	O
(	O
b	O
)	O
figure	O
23.5	O
:	O
(	O
a	O
)	O
:	O
factor	B
graph	I
for	O
the	O
ﬁrst	B
order	I
hmm	O
of	O
ﬁg	O
(	O
23.4	O
)	O
.	O
(	O
b	O
)	O
:	O
junction	B
tree	I
for	O
ﬁg	O
(	O
23.4	O
)	O
.	O
the	O
ﬁrst	B
order	I
hmm	O
are	O
given	O
in	O
ﬁg	O
(	O
23.5	O
)	O
.	O
in	O
both	O
cases	O
,	O
after	O
suitable	O
setting	O
of	O
the	O
factors	O
and	O
clique	B
potentials	O
,	O
ﬁltering	B
corresponds	O
to	O
passing	B
messages	O
from	O
left	O
to	O
right	O
and	O
upwards	O
;	O
smoothing	B
corresponds	O
to	O
a	O
valid	O
schedule	B
of	O
message	B
passing/absorption	O
both	O
forwards	O
and	O
backwards	O
along	O
all	O
edges	O
.	O
it	O
is	O
also	O
straightforward	O
to	O
derive	O
appropriate	O
recursions	O
directly	O
.	O
this	O
is	O
instructive	O
and	O
also	O
useful	O
in	O
constructing	O
compact	O
and	O
numerically	O
stable	O
algorithms	O
.	O
23.2.2	O
filtering	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
we	O
ﬁrst	O
compute	O
the	O
joint	B
marginal	O
p	O
(	O
ht	O
,	O
v1	O
:	O
t	O
)	O
from	O
which	O
the	O
conditional	B
marginal	I
p	O
(	O
ht|v1	O
:	O
t	O
)	O
can	O
subse-	O
quently	O
be	O
obtained	O
by	O
normalisation	B
.	O
a	O
recursion	O
for	O
p	O
(	O
ht	O
,	O
v1	O
:	O
t	O
)	O
is	O
obtained	O
by	O
considering	O
:	O
p	O
(	O
ht	O
,	O
v1	O
:	O
t	O
)	O
=	O
(	O
cid:88	O
)	O
=	O
(	O
cid:88	O
)	O
=	O
(	O
cid:88	O
)	O
ht−1	O
ht−1	O
ht−1	O
p	O
(	O
ht	O
,	O
ht−1	O
,	O
v1	O
:	O
t−1	O
,	O
vt	O
)	O
ht−1	O
)	O
p	O
(	O
ht|v1	O
:	O
t−1	O
,	O
ht−1	O
)	O
p	O
(	O
v1	O
:	O
t−1	O
,	O
ht−1	O
)	O
p	O
(	O
vt|v1	O
:	O
t−1	O
,	O
ht	O
,	O
	O
p	O
(	O
vt|ht	O
)	O
p	O
(	O
ht|ht−1	O
)	O
p	O
(	O
ht−1	O
,	O
v1	O
:	O
t−1	O
)	O
(	O
23.2.5	O
)	O
(	O
23.2.6	O
)	O
(	O
23.2.7	O
)	O
the	O
cancellations	O
follow	O
from	O
the	O
conditional	B
independence	O
assumptions	O
of	O
the	O
model	B
.	O
hence	O
if	O
we	O
deﬁne	O
α	O
(	O
ht	O
)	O
=	O
p	O
(	O
ht	O
,	O
v1	O
:	O
t	O
)	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
α	O
(	O
ht	O
)	O
=	O
p	O
(	O
vt|ht	O
)	O
corrector	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
ht−1	O
equation	B
(	O
23.2.7	O
)	O
above	O
gives	O
the	O
α-recursion	B
(	O
cid:125	O
)	O
p	O
(	O
ht|ht−1	O
)	O
α	O
(	O
ht−1	O
)	O
,	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
predictor	O
with	O
α	O
(	O
h1	O
)	O
=	O
p	O
(	O
h1	O
,	O
v1	O
)	O
=	O
p	O
(	O
v1|h1	O
)	O
p	O
(	O
h1	O
)	O
t	O
>	O
1	O
(	O
23.2.8	O
)	O
(	O
23.2.9	O
)	O
(	O
23.2.10	O
)	O
this	O
recursion	O
has	O
the	O
interpretation	O
that	O
the	O
ﬁltered	O
distribution	B
α	O
(	O
ht−1	O
)	O
is	O
propagated	O
forwards	O
by	O
the	O
dynamics	O
for	O
one	O
timestep	O
to	O
reveal	O
a	O
new	O
‘	O
prior	B
’	O
distribution	B
at	O
time	O
t.	O
this	O
distribution	B
is	O
then	O
mod-	O
ulated	O
by	O
the	O
observation	O
vt	O
,	O
incorporating	O
the	O
new	O
evidence	B
into	O
the	O
ﬁltered	O
distribution	B
(	O
this	O
is	O
also	O
referred	O
to	O
as	O
a	O
predictor-corrector	B
method	O
)	O
.	O
since	O
each	O
α	O
is	O
smaller	O
than	O
1	O
,	O
and	O
the	O
recursion	O
involves	O
multiplication	O
by	O
terms	O
less	O
than	O
1	O
,	O
the	O
α	O
’	O
s	O
can	O
become	O
very	O
small	O
.	O
to	O
avoid	O
numerical	B
problems	O
it	O
is	O
therefore	O
advisable	O
to	O
work	O
with	O
log	O
α	O
(	O
ht	O
)	O
,	O
see	O
hmmforward.m	O
.	O
normalisation	B
gives	O
the	O
ﬁltered	O
posterior	B
p	O
(	O
ht|v1	O
:	O
t	O
)	O
∝	O
α	O
(	O
ht	O
)	O
(	O
23.2.11	O
)	O
if	O
we	O
only	O
require	O
the	O
ﬁltered	O
posterior	B
we	O
are	O
free	O
to	O
rescale	O
the	O
α	O
’	O
s	O
as	O
we	O
wish	O
.	O
in	O
this	O
case	O
an	O
alternative	O
to	O
working	O
with	O
log	O
α	O
messages	O
is	O
to	O
work	O
with	O
normalised	O
α	O
messages	O
so	O
that	O
the	O
sum	O
of	O
the	O
components	O
is	O
always	O
1.	O
draft	O
march	O
9	O
,	O
2010	O
417	O
hidden	B
markov	O
models	O
we	O
can	O
write	O
equation	B
(	O
23.2.7	O
)	O
above	O
directly	O
as	O
a	O
recursion	O
for	O
the	O
ﬁltered	O
distribution	B
p	O
(	O
ht|v1	O
:	O
t	O
)	O
∝	O
p	O
(	O
vt|ht	O
)	O
p	O
(	O
ht|ht−1	O
)	O
p	O
(	O
ht−1|v1	O
:	O
t−1	O
)	O
t	O
>	O
1	O
(	O
23.2.12	O
)	O
(	O
cid:88	O
)	O
ht−1	O
intuitively	O
,	O
the	O
term	O
p	O
(	O
ht−1|v1	O
:	O
t−1	O
)	O
has	O
the	O
eﬀect	O
of	O
removing	O
all	O
nodes	O
in	O
the	O
graph	B
before	O
time	O
t	O
−	O
1	O
and	O
replacing	O
their	O
inﬂuence	O
by	O
a	O
modiﬁed	O
‘	O
prior	B
’	O
distribution	B
on	O
ht	O
.	O
one	O
may	O
interpret	O
p	O
(	O
vt|ht	O
)	O
p	O
(	O
ht|ht−1	O
)	O
as	O
a	O
likelihood	B
,	O
giving	O
rise	O
to	O
the	O
joint	B
posterior	O
p	O
(	O
ht	O
,	O
ht−1|v1	O
:	O
t	O
)	O
under	O
bayesian	O
updating	O
.	O
at	O
the	O
next	O
timestep	O
the	O
previous	O
posterior	B
becomes	O
the	O
new	O
prior	B
.	O
23.2.3	O
parallel	B
smoothing	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
there	O
are	O
two	O
main	O
approaches	O
to	O
computing	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
.	O
perhaps	O
the	O
most	O
common	O
in	O
the	O
hmm	O
literature	O
is	O
the	O
parallel	B
method	O
which	O
is	O
equivalent	B
to	O
message	B
passing	I
on	O
factor	B
graphs	O
.	O
in	O
this	O
one	O
separates	O
the	O
smoothed	O
posterior	B
into	O
contributions	O
from	O
the	O
past	O
and	O
future	O
:	O
(	O
cid:124	O
)	O
(	O
cid:125	O
)	O
p	O
(	O
vt+1	O
:	O
t|ht	O
,	O
v1	O
:	O
t	O
)	O
p	O
(	O
ht	O
,	O
v1	O
:	O
t	O
)	O
=	O
p	O
(	O
ht	O
,	O
v1	O
:	O
t	O
,	O
vt+1	O
:	O
t	O
)	O
=	O
p	O
(	O
ht	O
,	O
v1	O
:	O
t	O
)	O
=	O
α	O
(	O
ht	O
)	O
β	O
(	O
ht	O
)	O
(	O
23.2.13	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:124	O
)	O
(	O
cid:125	O
)	O
past	O
future	O
the	O
term	O
α	O
(	O
ht	O
)	O
is	O
obtained	O
from	O
the	O
‘	O
forward	O
’	O
α	B
recursion	I
,	O
(	O
23.2.9	O
)	O
.	O
the	O
term	O
β	O
(	O
ht	O
)	O
may	O
be	O
obtained	O
using	O
a	O
‘	O
backward	O
’	O
β	B
recursion	I
as	O
we	O
show	O
below	O
.	O
the	O
forward	O
and	O
backward	O
recursions	O
are	O
independent	O
and	O
may	O
therefore	O
be	O
run	O
in	O
parallel	B
,	O
with	O
their	O
results	O
combined	O
to	O
obtain	O
the	O
smoothed	O
posterior	B
.	O
this	O
approach	B
is	O
also	O
sometimes	O
termed	O
the	O
two-ﬁlter	B
smoother	I
.	O
the	O
β	B
recursion	I
p	O
(	O
vt	O
:	O
t|ht−1	O
)	O
=	O
(	O
cid:88	O
)	O
=	O
(	O
cid:88	O
)	O
=	O
(	O
cid:88	O
)	O
ht	O
ht	O
ht	O
deﬁning	O
p	O
(	O
vt	O
,	O
vt+1	O
:	O
t	O
,	O
ht|ht−1	O
)	O
p	O
(	O
vt|vt+1	O
:	O
t	O
,	O
ht	O
,	O
	O
p	O
(	O
vt|ht	O
)	O
p	O
(	O
vt+1	O
:	O
t|ht	O
,	O
	O
ht−1	O
)	O
p	O
(	O
vt+1	O
:	O
t	O
,	O
ht|ht−1	O
)	O
ht−1	O
)	O
p	O
(	O
ht|ht−1	O
)	O
β	O
(	O
ht	O
)	O
≡	O
p	O
(	O
vt+1	O
:	O
t|ht	O
)	O
β	O
(	O
ht−1	O
)	O
=	O
(	O
cid:88	O
)	O
ht	O
equation	B
(	O
23.2.16	O
)	O
above	O
gives	O
the	O
β-recursion	B
p	O
(	O
vt|ht	O
)	O
p	O
(	O
ht|ht−1	O
)	O
β	O
(	O
ht	O
)	O
,	O
2	O
≤	O
t	O
≤	O
t	O
(	O
23.2.14	O
)	O
(	O
23.2.15	O
)	O
(	O
23.2.16	O
)	O
(	O
23.2.17	O
)	O
(	O
23.2.18	O
)	O
(	O
23.2.19	O
)	O
with	O
β	O
(	O
ht	O
)	O
=	O
1.	O
as	O
for	O
the	O
forward	O
pass	O
,	O
working	O
in	O
log	O
space	O
is	O
recommended	O
to	O
avoid	O
numerical	B
diﬃculties	O
.	O
if	O
one	O
only	O
desires	O
posterior	B
distributions	O
,	O
one	O
can	O
also	O
perform	O
local	B
normalisation	O
at	O
each	O
stage	O
since	O
only	O
the	O
relative	O
magnitude	O
of	O
the	O
components	O
of	O
β	O
are	O
of	O
importance	B
.	O
the	O
smoothed	O
posterior	B
is	O
then	O
given	O
by	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
≡	O
γ	O
(	O
ht	O
)	O
=	O
α	O
(	O
ht	O
)	O
β	O
(	O
ht	O
)	O
ht	O
α	O
(	O
ht	O
)	O
β	O
(	O
ht	O
)	O
(	O
cid:80	O
)	O
together	O
the	O
α	O
−	O
β	O
recursions	O
are	O
called	O
the	O
forward-backward	O
algorithm	B
.	O
23.2.4	O
correction	O
smoothing	O
an	O
alternative	O
to	O
the	O
parallel	B
method	O
is	O
to	O
form	O
a	O
recursion	O
directly	O
for	O
the	O
smoothed	O
posterior	B
.	O
this	O
can	O
be	O
achieved	O
by	O
recognising	O
that	O
conditioning	B
on	O
the	O
present	O
makes	O
the	O
future	O
redundant	O
:	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
=	O
(	O
cid:88	O
)	O
p	O
(	O
ht	O
,	O
ht+1|v1	O
:	O
t	O
)	O
=	O
(	O
cid:88	O
)	O
ht+1	O
ht+1	O
p	O
(	O
ht|ht+1	O
,	O
v1	O
:	O
t	O
,	O
vt+1	O
:	O
t	O
)	O
p	O
(	O
ht+1|v1	O
:	O
t	O
)	O
(	O
23.2.20	O
)	O
418	O
draft	O
march	O
9	O
,	O
2010	O
hidden	B
markov	O
models	O
γ	O
(	O
ht	O
)	O
=	O
(	O
cid:88	O
)	O
ht+1	O
this	O
gives	O
a	O
recursion	O
for	O
γ	O
(	O
ht	O
)	O
≡	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
:	O
p	O
(	O
ht|ht+1	O
,	O
v1	O
:	O
t	O
)	O
γ	O
(	O
ht+1	O
)	O
(	O
23.2.21	O
)	O
with	O
γ	O
(	O
ht	O
)	O
∝	O
α	O
(	O
ht	O
)	O
.	O
the	O
term	O
p	O
(	O
ht|ht+1	O
,	O
v1	O
:	O
t	O
)	O
may	O
be	O
computed	O
using	O
the	O
ﬁltered	O
results	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
:	O
p	O
(	O
ht|ht+1	O
,	O
v1	O
:	O
t	O
)	O
∝	O
p	O
(	O
ht+1	O
,	O
ht|v1	O
:	O
t	O
)	O
∝	O
p	O
(	O
ht+1|ht	O
)	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
(	O
23.2.22	O
)	O
where	O
the	O
proportionality	O
constant	O
is	O
found	O
by	O
normalisation	B
.	O
this	O
is	O
a	O
form	O
of	O
dynamics	B
reversal	I
,	O
as	O
if	O
we	O
are	O
reversing	O
the	O
direction	O
of	O
the	O
hidden	B
to	O
hidden	B
arrow	O
in	O
the	O
hmm	O
.	O
this	O
procedure	O
,	O
also	O
termed	O
the	O
rauch-tung-striebel	O
smoother1	O
,	O
is	O
sequential	B
since	O
we	O
need	O
to	O
ﬁrst	O
complete	O
the	O
α	O
recursions	O
,	O
after	O
which	O
the	O
γ	O
recursion	O
may	O
begin	O
.	O
this	O
is	O
a	O
so-called	O
correction	B
smoother	I
since	O
it	O
‘	O
corrects	O
’	O
the	O
ﬁltered	O
result	O
.	O
interestingly	O
,	O
once	O
ﬁltering	B
has	O
been	O
carried	O
out	O
,	O
the	O
evidential	O
states	O
v1	O
:	O
t	O
are	O
not	O
needed	O
during	O
the	O
subsequent	O
γ	O
recursion	O
.	O
the	O
α	O
−	O
β	O
and	O
α	O
−	O
γ	O
recursions	O
are	O
related	O
through	O
γ	O
(	O
ht	O
)	O
∝	O
α	O
(	O
ht	O
)	O
β	O
(	O
ht	O
)	O
(	O
23.2.23	O
)	O
computing	O
the	O
pairwise	B
marginal	I
p	O
(	O
ht	O
,	O
ht−1|v1	O
:	O
t	O
)	O
to	O
implement	O
the	O
em	O
algorithm	B
for	O
learning	B
,	O
section	O
(	O
23.3.1	O
)	O
,	O
we	O
require	O
terms	O
such	O
as	O
p	O
(	O
ht	O
,	O
ht−1|v1	O
:	O
t	O
)	O
.	O
these	O
can	O
be	O
obtained	O
by	O
message	B
passing	I
on	O
either	O
a	O
factor	B
graph	I
or	O
junction	B
tree	I
(	O
for	O
which	O
the	O
pairwise	B
marginals	O
are	O
contained	O
in	O
the	O
cliques	O
,	O
see	O
ﬁg	O
(	O
23.4b	O
)	O
)	O
.	O
alternatively	O
,	O
an	O
explicit	O
recursion	O
is	O
as	O
follows	O
:	O
p	O
(	O
ht	O
,	O
ht+1|v1	O
:	O
t	O
)	O
∝	O
p	O
(	O
v1	O
:	O
t	O
,	O
vt+1	O
,	O
vt+2	O
:	O
t	O
,	O
ht+1	O
,	O
ht	O
)	O
=	O
p	O
(	O
vt+2	O
:	O
t|	O
(	O
(	O
(	O
(	O
(	O
(	O
=	O
p	O
(	O
vt+2	O
:	O
t|ht+1	O
)	O
p	O
(	O
vt+1|	O
v1	O
:	O
t	O
,	O
ht	O
,	O
ht+1	O
)	O
p	O
(	O
v1	O
:	O
t	O
,	O
ht+1	O
,	O
ht	O
)	O
=	O
p	O
(	O
vt+2	O
:	O
t|ht+1	O
)	O
p	O
(	O
vt+1|ht+1	O
)	O
p	O
(	O
ht+1|v1	O
:	O
t	O
,	O
ht	O
)	O
p	O
(	O
v1	O
:	O
t	O
,	O
ht	O
)	O
v1	O
:	O
t	O
,	O
vt+1	O
,	O
ht	O
,	O
ht+1	O
)	O
p	O
(	O
v1	O
:	O
t	O
,	O
vt+1	O
,	O
ht+1	O
,	O
ht	O
)	O
rearranging	O
,	O
we	O
therefore	O
have	O
p	O
(	O
ht	O
,	O
ht+1|v1	O
:	O
t	O
)	O
∝	O
α	O
(	O
ht	O
)	O
p	O
(	O
vt+1|ht+1	O
)	O
p	O
(	O
ht+1|ht	O
)	O
β	O
(	O
ht+1	O
)	O
see	O
hmmsmooth.m	O
.	O
the	O
likelihood	B
p	O
(	O
v1	O
:	O
t	O
)	O
the	O
likelihood	B
of	O
a	O
sequence	O
of	O
observations	O
can	O
be	O
computed	O
from	O
p	O
(	O
ht	O
,	O
v1	O
:	O
t	O
)	O
=	O
(	O
cid:88	O
)	O
α	O
(	O
ht	O
)	O
ht	O
an	O
alternative	O
computation	O
can	O
be	O
found	O
by	O
making	O
use	O
of	O
the	O
decomposition	B
ht	O
p	O
(	O
v1	O
:	O
t	O
)	O
=	O
p	O
(	O
v1	O
:	O
t	O
)	O
=	O
(	O
cid:88	O
)	O
t	O
(	O
cid:89	O
)	O
p	O
(	O
vt|v1	O
:	O
t−1	O
)	O
=	O
(	O
cid:88	O
)	O
=	O
(	O
cid:88	O
)	O
=	O
(	O
cid:88	O
)	O
t=1	O
p	O
(	O
vt|v1	O
:	O
t−1	O
)	O
each	O
factor	B
can	O
be	O
computed	O
using	O
p	O
(	O
vt	O
,	O
ht|v1	O
:	O
t−1	O
)	O
p	O
(	O
vt|ht	O
,	O
v1	O
:	O
t−1	O
)	O
p	O
(	O
ht|v1	O
:	O
t−1	O
)	O
ht	O
p	O
(	O
vt|ht	O
)	O
(	O
cid:88	O
)	O
ht−1	O
ht	O
ht	O
p	O
(	O
ht|ht−1	O
,	O
v1	O
:	O
t−1	O
)	O
p	O
(	O
ht−1|v1	O
:	O
t−1	O
)	O
(	O
23.2.24	O
)	O
(	O
23.2.25	O
)	O
(	O
23.2.26	O
)	O
(	O
23.2.27	O
)	O
(	O
23.2.28	O
)	O
(	O
23.2.29	O
)	O
(	O
23.2.30	O
)	O
1it	O
is	O
most	O
common	O
to	O
use	O
this	O
terminology	O
for	O
the	O
continuous	B
variable	O
case	O
,	O
though	O
we	O
adopt	O
it	O
here	O
also	O
for	O
the	O
discrete	B
variable	O
case	O
.	O
draft	O
march	O
9	O
,	O
2010	O
419	O
hidden	B
markov	O
models	O
figure	O
23.6	O
:	O
localising	O
the	O
burglar	O
.	O
the	O
latent	B
variable	I
ht	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
25	O
}	O
denotes	O
the	O
positions	O
,	O
deﬁned	O
over	O
the	O
5	O
×	O
5	O
grid	O
of	O
the	O
ground	O
ﬂoor	O
of	O
the	O
house	O
.	O
(	O
a	O
)	O
:	O
a	O
representation	B
of	O
the	O
probability	B
that	O
the	O
‘	O
ﬂoor	O
will	O
creak	O
’	O
at	O
each	O
of	O
the	O
25	O
positions	O
,	O
p	O
(	O
vcreak|h	O
)	O
.	O
light	O
squares	O
represent	O
probability	B
0.9	O
and	O
dark	O
square	O
0.1	O
.	O
(	O
b	O
)	O
:	O
a	O
representation	B
of	O
the	O
probability	B
p	O
(	O
vbump|h	O
)	O
that	O
the	O
burglar	O
will	O
bump	O
into	O
something	O
in	O
each	O
of	O
the	O
25	O
positions	O
.	O
(	O
a	O
)	O
‘	O
creaks	O
’	O
(	O
b	O
)	O
‘	O
bumps	O
’	O
where	O
the	O
ﬁnal	O
term	O
p	O
(	O
ht−1|v1	O
:	O
t−1	O
)	O
is	O
the	O
ﬁltered	O
result	O
.	O
in	O
both	O
approaches	O
the	O
likelihood	B
of	O
an	O
output	O
sequence	O
requires	O
only	O
a	O
forward	O
computation	O
(	O
ﬁltering	B
)	O
.	O
if	O
required	O
,	O
one	O
can	O
also	O
compute	O
the	O
likelihood	B
using	O
,	O
(	O
23.2.13	O
)	O
,	O
p	O
(	O
v1	O
:	O
t	O
)	O
=	O
(	O
cid:88	O
)	O
ht	O
α	O
(	O
ht	O
)	O
β	O
(	O
ht	O
)	O
(	O
23.2.31	O
)	O
(	O
23.2.32	O
)	O
which	O
is	O
valid	O
for	O
any	O
1	O
≤	O
t	O
≤	O
t	O
.	O
23.2.5	O
most	O
likely	O
joint	O
state	O
the	O
most	O
likely	O
path	O
h1	O
:	O
t	O
of	O
p	O
(	O
h1	O
:	O
t|v1	O
:	O
t	O
)	O
is	O
the	O
same	O
as	O
the	O
most	B
likely	I
state	I
(	O
for	O
ﬁxed	O
v1	O
:	O
t	O
)	O
of	O
p	O
(	O
h1	O
:	O
t	O
,	O
v1	O
:	O
t	O
)	O
=	O
(	O
cid:89	O
)	O
p	O
(	O
vt|ht	O
)	O
p	O
(	O
ht|ht−1	O
)	O
t	O
the	O
most	O
likely	O
path	O
can	O
be	O
found	O
using	O
the	O
max-product	B
version	O
of	O
the	O
factor	B
graph	I
or	O
max-absorption	O
on	O
the	O
junction	B
tree	I
.	O
alternatively	O
,	O
an	O
explicit	O
derivation	O
can	O
be	O
obtained	O
by	O
considering	O
:	O
t	O
(	O
cid:89	O
)	O
t=1	O
max	O
ht	O
p	O
(	O
vt|ht	O
)	O
p	O
(	O
ht|ht−1	O
)	O
=	O
(	O
cid:40	O
)	O
t−1	O
(	O
cid:89	O
)	O
t=1	O
(	O
cid:41	O
)	O
p	O
(	O
vt|ht	O
)	O
p	O
(	O
ht|ht−1	O
)	O
max	O
ht	O
(	O
cid:124	O
)	O
(	O
cid:125	O
)	O
p	O
(	O
vt|ht	O
)	O
p	O
(	O
ht|ht−1	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
µ	O
(	O
ht−1	O
)	O
(	O
23.2.33	O
)	O
the	O
message	B
µ	O
(	O
ht−1	O
)	O
conveys	O
information	O
from	O
the	O
end	O
of	O
the	O
chain	B
to	O
the	O
penultimate	O
timestep	O
.	O
we	O
can	O
continue	O
in	O
this	O
manner	O
,	O
deﬁning	O
the	O
recursion	O
µ	O
(	O
ht−1	O
)	O
=	O
max	O
ht	O
p	O
(	O
vt|ht	O
)	O
p	O
(	O
ht|ht−1	O
)	O
µ	O
(	O
ht	O
)	O
,	O
2	O
≤	O
t	O
≤	O
t	O
(	O
23.2.34	O
)	O
with	O
µ	O
(	O
ht	O
)	O
=	O
1.	O
this	O
means	O
that	O
the	O
eﬀect	O
of	O
maximising	O
over	O
h2	O
,	O
.	O
.	O
.	O
,	O
ht	O
is	O
compressed	O
into	O
a	O
message	B
µ	O
(	O
h1	O
)	O
so	O
that	O
the	O
most	B
likely	I
state	I
h∗	O
p	O
(	O
v1|h1	O
)	O
p	O
(	O
h1	O
)	O
µ	O
(	O
h1	O
)	O
∗	O
1	O
=	O
argmax	O
h	O
1	O
is	O
given	O
by	O
(	O
23.2.35	O
)	O
h1	O
once	O
computed	O
,	O
backtracking	B
gives	O
∗	O
t	O
=	O
argmax	O
h	O
ht	O
p	O
(	O
vt|ht	O
)	O
p	O
(	O
ht|h	O
∗	O
t−1	O
)	O
µ	O
(	O
ht	O
)	O
(	O
23.2.36	O
)	O
this	O
special	O
case	O
of	O
the	O
max-product	B
algorithm	O
is	O
called	O
the	O
viterbi	B
algorithm	O
.	O
similarly	O
,	O
one	O
may	O
use	O
the	O
n-max-product	O
algorithm	B
,	O
section	O
(	O
5.2.1	O
)	O
,	O
to	O
obtain	O
the	O
n-most	O
likely	O
hidden	B
paths	O
.	O
example	O
97	O
(	O
a	O
localisation	B
example	O
)	O
.	O
you	O
’	O
re	O
asleep	O
upstairs	O
in	O
your	O
house	O
and	O
awoken	O
by	O
noises	O
from	O
downstairs	O
.	O
you	O
realise	O
that	O
a	O
burglar	O
is	O
on	O
the	O
ground	O
ﬂoor	O
and	O
attempt	O
to	O
understand	O
where	O
he	O
his	O
from	O
listening	O
to	O
his	O
movements	O
.	O
you	O
mentally	O
partition	O
the	O
ground	O
ﬂoor	O
into	O
a	O
5	O
×	O
5	O
grid	O
.	O
for	O
each	O
grid	O
position	O
you	O
know	O
the	O
probability	B
that	O
if	O
someone	O
is	O
in	O
that	O
position	O
the	O
ﬂoorboard	O
will	O
creak	O
,	O
ﬁg	O
(	O
23.6a	O
)	O
.	O
420	O
draft	O
march	O
9	O
,	O
2010	O
hidden	B
markov	O
models	O
(	O
a	O
)	O
creaks	O
and	O
bumps	O
(	O
b	O
)	O
filtering	O
(	O
c	O
)	O
smoothing	B
(	O
d	O
)	O
viterbi	B
(	O
e	O
)	O
true	O
burglar	O
position	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
t	O
t	O
vcreak	O
t	O
,	O
vbump	O
,	O
where	O
vcreak	O
t	O
t	O
=	O
2	O
otherwise	O
)	O
and	O
vbump	O
(	O
a	O
)	O
:	O
each	O
panel	O
represents	O
the	O
=	O
1	O
means	O
that	O
there	O
was	O
a	O
‘	O
creak	O
in	O
the	O
ﬂoorboard	O
’	O
=	O
1	O
meaning	O
‘	O
bumped	O
into	O
something	O
’	O
(	O
and	O
is	O
in	O
state	O
2	O
otherwise	O
)	O
.	O
t	O
and	O
the	O
right	O
t	O
.	O
the	O
lighter	O
colour	O
represents	O
the	O
occurrence	O
of	O
a	O
creak	O
or	O
bump	O
,	O
the	O
darker	O
colour	O
the	O
absence	O
.	O
(	O
c	O
)	O
:	O
the	O
smoothed	O
(	O
d	O
)	O
:	O
the	O
most	O
likely	O
figure	O
23.7	O
:	O
localising	O
the	O
burglar	O
through	O
time	O
for	O
10	O
time	O
steps	O
.	O
visible	B
information	O
vt	O
=	O
(	O
vcreak	O
there	O
are	O
10	O
panels	O
,	O
one	O
for	O
each	O
time	O
t	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
10.	O
the	O
left	O
half	O
of	O
the	O
panel	O
represents	O
v1	O
half	O
v2	O
(	O
b	O
)	O
:	O
the	O
ﬁltered	O
distribution	B
p	O
(	O
ht|v1	O
:	O
t	O
)	O
representing	O
where	O
we	O
think	O
the	O
burglar	O
is	O
.	O
distribution	B
p	O
(	O
ht|v1:10	O
)	O
so	O
that	O
we	O
can	O
ﬁgure	O
out	O
where	O
we	O
think	O
the	O
burglar	O
went	O
.	O
(	O
viterbi	B
)	O
burglar	O
path	B
arg	O
maxh1:10	O
p	O
(	O
h1:10|v1:10	O
)	O
.	O
(	O
e	O
)	O
:	O
the	O
actual	O
path	B
of	O
the	O
burglar	O
.	O
similarly	O
you	O
know	O
for	O
each	O
position	O
the	O
probability	B
that	O
someone	O
will	O
bump	O
into	O
something	O
in	O
the	O
dark	O
,	O
ﬁg	O
(	O
23.6b	O
)	O
.	O
the	O
ﬂoorboard	O
creaking	O
and	O
bumping	O
into	O
objects	O
can	O
occur	O
independently	O
.	O
in	O
addition	O
you	O
assume	O
that	O
the	O
burglar	O
will	O
move	O
only	O
one	O
grid	O
square	O
–	O
forwards	O
,	O
backwards	O
,	O
left	O
or	O
right	O
in	O
a	O
single	O
timestep	O
.	O
based	O
on	O
a	O
series	O
of	O
bump/no	O
bump	O
and	O
creak/no	O
creak	O
information	O
,	O
ﬁg	O
(	O
23.7a	O
)	O
,	O
you	O
try	O
to	O
ﬁgure	O
out	O
based	O
on	O
your	O
knowledge	O
of	O
the	O
ground	O
ﬂoor	O
,	O
where	O
the	O
burglar	O
might	O
be	O
.	O
we	O
can	O
represent	O
the	O
scenario	O
using	O
a	O
hmm	O
where	O
h	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
25	O
}	O
denotes	O
the	O
grid	O
square	O
.	O
the	O
visible	B
variable	O
has	O
a	O
factorised	B
form	O
v	O
=	O
vcreak	O
⊗	O
vbump	O
and	O
,	O
to	O
use	O
our	O
standard	O
code	O
,	O
we	O
form	O
a	O
new	O
visible	B
variable	O
with	O
4	O
states	O
using	O
p	O
(	O
v|h	O
)	O
=	O
p	O
(	O
vcreak|h	O
)	O
p	O
(	O
vbump|h	O
)	O
(	O
23.2.37	O
)	O
based	O
on	O
the	O
past	O
information	O
,	O
our	O
belief	O
as	O
to	O
where	O
the	O
burglar	O
might	O
be	O
is	O
represented	O
by	O
the	O
ﬁltered	O
distribution	B
p	O
(	O
ht|v1	O
:	O
t	O
)	O
,	O
ﬁg	O
(	O
23.7	O
)	O
.	O
after	O
the	O
burglar	O
has	O
left	O
at	O
t	O
=	O
10	O
,	O
the	O
police	O
arrive	O
and	O
try	O
to	O
piece	O
together	O
where	O
the	O
burglar	O
went	O
,	O
based	O
on	O
the	O
sequence	O
of	O
creaks	O
and	O
bumps	O
you	O
provide	O
.	O
at	O
any	O
time	O
t	O
,	O
the	O
information	O
as	O
to	O
where	O
the	O
burglar	O
could	O
have	O
been	O
is	O
represented	O
by	O
the	O
smoothed	O
distribution	B
p	O
(	O
ht|v1:10	O
)	O
.	O
the	O
police	O
’	O
s	O
single	O
best-guess	O
for	O
the	O
sequence	O
of	O
burglar	O
positions	O
is	O
provided	O
by	O
the	O
most	O
likely	O
joint	O
hidden	B
state	O
arg	O
maxh1:10	O
p	O
(	O
h1:10|v1:10	O
)	O
.	O
23.2.6	O
self	O
localisation	O
and	O
kidnapped	O
robots	O
a	O
robot	O
has	O
an	O
internal	O
grid-based	O
map	B
of	O
its	O
environment	O
and	O
for	O
each	O
location	O
h	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
h	O
}	O
,	O
knows	O
the	O
likely	O
sensor	O
readings	O
he	O
would	O
expect	O
in	O
that	O
location	O
.	O
the	O
robot	O
is	O
‘	O
kidnapped	O
’	O
and	O
placed	O
somewhere	O
in	O
the	O
environment	O
.	O
the	O
robot	O
then	O
starts	O
to	O
move	O
,	O
gathering	O
sensor	O
information	O
.	O
based	O
on	O
these	O
readings	O
v1	O
:	O
t	O
and	O
intended	O
movements	O
m1	O
:	O
t	O
,	O
the	O
robot	O
attempts	O
to	O
ﬁgure	O
out	O
his	O
location	O
.	O
due	O
to	O
wheel	O
slippage	O
on	O
the	O
ﬂoor	O
an	O
intended	O
action	O
by	O
the	O
robot	O
,	O
such	O
as	O
‘	O
move	O
forwards	O
’	O
,	O
might	O
not	O
be	O
successful	O
.	O
given	O
all	O
the	O
information	O
the	O
robot	O
has	O
,	O
he	O
would	O
like	O
to	O
infer	O
p	O
(	O
ht|v1	O
:	O
t	O
,	O
m1	O
:	O
t	O
)	O
.	O
this	O
problem	B
diﬀers	O
from	O
the	O
burglar	O
scenario	O
in	O
that	O
the	O
robot	O
now	O
has	O
knowledge	O
of	O
the	O
intended	O
movements	O
he	O
makes	O
.	O
this	O
should	O
give	O
more	O
draft	O
march	O
9	O
,	O
2010	O
421	O
learning	B
hmms	O
h1	O
m3	O
m2	O
m1	O
figure	O
23.8	O
:	O
a	O
model	B
for	O
robot	O
self-localisation	O
.	O
at	O
each	O
time	O
the	O
robot	O
makes	O
an	O
intended	O
movement	O
,	O
mt	O
.	O
as	O
a	O
generative	B
model	O
,	O
knowing	O
the	O
intended	O
movement	O
mt	O
and	O
the	O
current	O
grid	O
position	O
ht	O
,	O
the	O
robot	O
has	O
an	O
idea	O
of	O
where	O
he	O
should	O
be	O
at	O
the	O
next	O
time-step	O
and	O
what	O
sensor	O
reading	O
vt+1	O
he	O
would	O
ex-	O
pect	O
there	O
.	O
based	O
on	O
only	O
the	O
sensor	O
information	O
v1	O
:	O
t	O
and	O
the	O
intended	O
movements	O
m1	O
:	O
t	O
,	O
the	O
task	O
is	O
to	O
infer	O
a	O
distribution	B
over	O
robot	O
locations	O
p	O
(	O
h1	O
:	O
t|m1	O
:	O
t	O
,	O
v1	O
:	O
t	O
)	O
.	O
information	O
as	O
to	O
where	O
he	O
could	O
be	O
.	O
one	O
can	O
view	O
this	O
as	O
extra	O
‘	O
visible	B
’	O
information	O
,	O
though	O
it	O
is	O
more	O
natural	B
to	O
think	O
of	O
this	O
as	O
additional	O
input	O
information	O
.	O
a	O
model	B
of	O
this	O
scenario	O
is	O
,	O
see	O
ﬁg	O
(	O
23.8	O
)	O
,	O
h4	O
v4	O
h2	O
v2	O
h3	O
v3	O
v1	O
t	O
(	O
cid:89	O
)	O
t=1	O
t	O
(	O
cid:89	O
)	O
t=1	O
p	O
(	O
v1	O
:	O
t	O
,	O
m1	O
:	O
t	O
,	O
h1	O
:	O
t	O
)	O
=	O
p	O
(	O
vt|ht	O
)	O
p	O
(	O
ht|ht−1	O
,	O
mt−1	O
)	O
p	O
(	O
mt	O
)	O
(	O
23.2.38	O
)	O
the	O
visible	B
variables	O
v1	O
:	O
t	O
are	O
known	O
,	O
as	O
are	O
the	O
intended	O
movements	O
m1	O
:	O
t	O
.	O
the	O
model	B
expresses	O
that	O
the	O
movements	O
selected	O
by	O
the	O
robot	O
are	O
random	O
(	O
hence	O
no	O
decision	O
making	O
in	O
terms	O
of	O
where	O
to	O
go	O
next	O
)	O
.	O
we	O
assume	O
that	O
the	O
robot	O
has	O
full	O
knowledge	O
of	O
the	O
conditional	B
distributions	O
deﬁning	O
the	O
model	B
(	O
he	O
knows	O
the	O
‘	O
map	B
’	O
of	O
his	O
environment	O
and	O
all	O
state	O
transition	O
and	O
emission	O
probabilities	O
)	O
.	O
if	O
our	O
interest	O
is	O
only	O
in	O
localising	O
the	O
robot	O
,	O
since	O
the	O
inputs	O
m	O
are	O
known	O
,	O
this	O
model	B
is	O
in	O
fact	O
a	O
form	O
of	O
time-dependent	O
hmm	O
:	O
p	O
(	O
v1	O
:	O
t	O
,	O
h1	O
:	O
t	O
)	O
=	O
p	O
(	O
vt|ht	O
)	O
p	O
(	O
ht|ht−1	O
,	O
t	O
)	O
(	O
23.2.39	O
)	O
for	O
a	O
time-dependent	O
transition	O
p	O
(	O
ht|ht−1	O
,	O
t	O
)	O
deﬁned	O
by	O
the	O
intended	O
movement	O
mt−1	O
.	O
any	O
inference	B
task	O
required	O
then	O
follows	O
the	O
standard	O
stationary	O
hmm	O
algorithms	O
,	O
albeit	O
on	O
replacing	O
the	O
time-independent	O
transitions	O
p	O
(	O
ht|ht−1	O
)	O
with	O
the	O
known	O
time-dependent	O
transitions	O
.	O
in	O
self	B
localisation	I
and	I
mapping	I
(	O
slam	O
)	O
the	O
robot	O
does	O
not	O
know	O
the	O
map	B
of	O
his	O
environment	O
.	O
this	O
corresponds	O
to	O
having	O
to	O
learn	O
the	O
transition	O
and	O
emission	O
distributions	O
on-the-ﬂy	O
as	O
he	O
explores	O
the	O
environment	O
.	O
23.2.7	O
natural	B
language	O
models	O
a	O
simple	O
generative	O
model	B
of	O
language	O
can	O
be	O
obtained	O
from	O
the	O
letter-to-letter	O
transitions	O
(	O
a	O
so-called	O
bigram	B
)	O
.	O
in	O
the	O
example	O
below	O
,	O
we	O
use	O
this	O
in	O
a	O
hmm	O
to	O
clean	O
up	O
the	O
mis-typings	O
.	O
example	O
98	O
(	O
stubby	O
ﬁngers	O
)	O
.	O
a	O
‘	O
stubby	O
ﬁngers	O
’	O
typist	O
has	O
the	O
tendency	O
to	O
hit	O
either	O
the	O
correct	O
key	O
or	O
a	O
neighbouring	O
key	O
.	O
for	O
simplicity	O
we	O
assume	O
that	O
there	O
are	O
27	O
keys	O
:	O
lower	O
case	O
a	O
to	O
lower	O
case	O
z	O
and	O
the	O
space	O
bar	O
.	O
to	O
model	B
this	O
we	O
use	O
an	O
emission	B
distribution	I
bij	O
=	O
p	O
(	O
v	O
=	O
i|h	O
=	O
j	O
)	O
where	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
27	O
,	O
j	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
27	O
,	O
as	O
depicted	O
in	O
ﬁg	O
(	O
23.9	O
)	O
.	O
a	O
database	O
of	O
letter-to-next-letter	O
frequencies	O
(	O
www.data-compression.com/english.shtml	O
)	O
,	O
yields	O
the	O
transition	B
matrix	I
aij	O
=	O
p	O
(	O
h	O
(	O
cid:48	O
)	O
=	O
i|h	O
=	O
j	O
)	O
in	O
en-	O
glish	O
.	O
for	O
simplicity	O
we	O
assume	O
that	O
p	O
(	O
h1	O
)	O
is	O
uniform	B
.	O
also	O
we	O
assume	O
that	O
each	O
intended	O
key	O
press	O
results	O
in	O
a	O
single	O
press	O
.	O
given	O
a	O
typed	O
sequence	O
kezrninh	O
what	O
is	O
the	O
most	O
likely	O
word	O
that	O
this	O
corresponds	O
to	O
?	O
by	O
listing	O
the	O
200	O
most	O
likely	O
hidden	O
sequences	B
(	O
using	O
the	O
n-max-product	O
algorithm	B
)	O
and	O
discarding	O
those	O
that	O
are	O
not	O
in	O
a	O
standard	O
english	O
dictionary	O
(	O
www.curlewcommunications.co.uk/wordlist.html	O
)	O
,	O
the	O
most	O
likely	O
word	O
that	O
was	O
intended	O
is	O
learning	B
.	O
see	O
demohmmbigram.m	O
.	O
23.3	O
learning	B
hmms	O
given	O
a	O
set	O
of	O
data	B
v	O
=	O
(	O
cid:8	O
)	O
v1	O
,	O
.	O
.	O
.	O
,	O
vn	O
(	O
cid:9	O
)	O
of	O
n	O
sequences	B
,	O
where	O
sequence	O
vn	O
=	O
vn	O
1	O
:	O
tn	O
is	O
of	O
length	O
tn	O
,	O
we	O
seek	O
the	O
hmm	O
transition	B
matrix	I
a	O
,	O
emission	B
matrix	I
b	O
,	O
and	O
initial	O
vector	O
a	O
most	O
likely	O
to	O
have	O
have	O
generated	O
422	O
draft	O
march	O
9	O
,	O
2010	O
learning	B
hmms	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
23.9	O
:	O
(	O
a	O
)	O
:	O
the	O
letter-to-letter	O
transition	B
matrix	I
for	O
english	O
p	O
(	O
h	O
(	O
cid:48	O
)	O
=	O
i|h	O
=	O
j	O
)	O
.	O
(	O
b	O
)	O
:	O
the	O
letter	O
emission	B
matrix	I
for	O
a	O
typist	O
with	O
‘	O
stubby	O
ﬁngers	O
’	O
in	O
which	O
the	O
key	O
or	O
its	O
neighbours	O
on	O
the	O
keyboard	O
are	O
likely	O
to	O
be	O
hit	O
.	O
v.	O
we	O
make	O
the	O
i.i.d	O
.	O
assumption	O
so	O
that	O
each	O
sequence	O
is	O
independently	O
generated	O
and	O
assume	O
that	O
we	O
know	O
the	O
number	O
of	O
hidden	B
states	O
h.	O
for	O
simplicity	O
we	O
concentrate	O
here	O
on	O
the	O
case	O
of	O
discrete	B
visible	O
variables	O
,	O
assuming	O
also	O
we	O
know	O
the	O
number	O
of	O
states	O
v	O
.	O
23.3.1	O
em	O
algorithm	B
the	O
application	O
of	O
em	O
to	O
the	O
hmm	O
model	B
is	O
called	O
the	O
baum-welch	O
algorithm	B
and	O
follows	O
the	O
general	O
strategy	O
outlined	O
in	O
section	O
(	O
11.2	O
)	O
.	O
m-step	O
assuming	O
i.i.d	O
.	O
data	B
,	O
the	O
m-step	O
is	O
given	O
by	O
maximising	O
the	O
‘	O
energy	B
’	O
:	O
n=1	O
t	O
n	O
,	O
hn	O
1	O
,	O
vn	O
2	O
.	O
.	O
.	O
,	O
vn	O
n	O
(	O
cid:88	O
)	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
vn	O
(	O
cid:40	O
)	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
h1	O
)	O
(	O
cid:105	O
)	O
pold	O
(	O
h1|vn	O
)	O
+	O
n	O
(	O
cid:88	O
)	O
n=1	O
tn−1	O
(	O
cid:88	O
)	O
t=1	O
1	O
,	O
hn	O
2	O
,	O
.	O
.	O
.	O
,	O
hn	O
t	O
n	O
)	O
(	O
cid:105	O
)	O
pold	O
(	O
hn|vn	O
)	O
tn	O
(	O
cid:88	O
)	O
t=1	O
with	O
respect	O
to	O
the	O
parameters	O
a	O
,	O
b	O
,	O
a	O
;	O
hn	O
denotes	O
h1	O
:	O
tn	O
.	O
using	O
the	O
form	O
of	O
the	O
hmm	O
,	O
we	O
obtain	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
ht+1|ht	O
)	O
(	O
cid:105	O
)	O
pold	O
(	O
ht	O
,	O
ht+1|vn	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
vn	O
t	O
|ht	O
)	O
(	O
cid:105	O
)	O
pold	O
(	O
ht|vn	O
)	O
(	O
23.3.2	O
)	O
where	O
for	O
compactness	O
we	O
drop	O
the	O
sequence	O
index	O
from	O
the	O
h	O
variables	O
.	O
to	O
avoid	O
potential	B
confusion	O
,	O
we	O
write	O
pnew	O
(	O
h1	O
=	O
i	O
)	O
to	O
denote	O
the	O
(	O
new	O
)	O
table	O
entry	O
for	O
the	O
probability	B
that	O
the	O
initial	O
hidden	B
variable	I
is	O
in	O
state	O
i.	O
optimising	O
equation	B
(	O
23.3.2	O
)	O
with	O
respect	O
to	O
p	O
(	O
h1	O
)	O
,	O
(	O
and	O
enforcing	O
p	O
(	O
h1	O
)	O
to	O
be	O
a	O
distribution	B
)	O
we	O
obtain	O
anew	O
i	O
≡	O
pnew	O
(	O
h1	O
=	O
i	O
)	O
=	O
1	O
n	O
n	O
(	O
cid:88	O
)	O
n=1	O
pold	O
(	O
h1	O
=	O
i|vn	O
)	O
tn−1	O
(	O
cid:88	O
)	O
n	O
(	O
cid:88	O
)	O
which	O
is	O
the	O
average	B
number	O
of	O
times	O
that	O
the	O
ﬁrst	O
hidden	O
variable	B
is	O
in	O
state	O
i.	O
similarly	O
,	O
which	O
is	O
the	O
number	O
of	O
times	O
that	O
a	O
transition	O
from	O
hidden	B
state	O
i	O
to	O
hidden	B
state	O
i	O
(	O
cid:48	O
)	O
occurs	O
,	O
averaged	O
over	O
all	O
times	O
(	O
since	O
we	O
assumed	O
stationarity	O
)	O
and	O
training	B
sequences	O
.	O
normalising	O
,	O
we	O
obtain	O
pold	O
(	O
ht	O
=	O
i	O
,	O
ht+1	O
=	O
i	O
(	O
cid:48	O
)	O
|vn	O
)	O
n=1	O
|ht	O
=	O
i	O
)	O
∝	O
(	O
cid:48	O
)	O
anew	O
i	O
(	O
cid:48	O
)	O
,	O
i	O
≡	O
pnew	O
(	O
ht+1	O
=	O
i	O
(	O
cid:80	O
)	O
tn−1	O
(	O
cid:80	O
)	O
tn−1	O
t=1	O
pold	O
(	O
ht	O
=	O
i	O
,	O
ht+1	O
=	O
i	O
(	O
cid:48	O
)	O
|vn	O
)	O
t=1	O
pold	O
(	O
ht	O
=	O
i	O
,	O
ht+1	O
=	O
i	O
(	O
cid:48	O
)	O
|vn	O
)	O
(	O
cid:80	O
)	O
n	O
(	O
cid:80	O
)	O
i	O
(	O
cid:48	O
)	O
(	O
cid:80	O
)	O
n	O
i	O
(	O
cid:48	O
)	O
,	O
i	O
=	O
anew	O
n=1	O
n=1	O
t=1	O
draft	O
march	O
9	O
,	O
2010	O
(	O
23.3.1	O
)	O
(	O
cid:41	O
)	O
(	O
23.3.3	O
)	O
(	O
23.3.4	O
)	O
(	O
23.3.5	O
)	O
423	O
abcdefghijklmnopqrstuvwxyz	O
abcdefghijklmnopqrstuvwxyz	O
00.10.20.30.40.50.60.70.80.9	O
abcdefghijklmnopqrstuvwxyz	O
abcdefghijklmnopqrstuvwxyz	O
0.050.10.150.20.250.30.350.40.450.50.55	O
finally	O
,	O
bnew	O
j	O
,	O
i	O
≡	O
pnew	O
(	O
vt	O
=	O
j|ht	O
=	O
i	O
)	O
∝	O
n	O
(	O
cid:88	O
)	O
tn	O
(	O
cid:88	O
)	O
n=1	O
t=1	O
i	O
[	O
vn	O
t	O
=	O
j	O
]	O
pold	O
(	O
ht	O
=	O
i|vn	O
)	O
learning	B
hmms	O
(	O
23.3.6	O
)	O
which	O
is	O
the	O
expected	O
number	O
of	O
times	O
that	O
,	O
for	O
the	O
observation	O
being	O
in	O
state	O
j	O
,	O
the	O
hidden	B
state	O
is	O
i.	O
the	O
proportionality	O
constant	O
is	O
determined	O
by	O
the	O
normalisation	B
requirement	O
.	O
e-step	O
in	O
computing	O
the	O
m-step	O
above	O
the	O
quantities	O
pold	O
(	O
h1	O
=	O
i|vn	O
)	O
,	O
pold	O
(	O
ht	O
=	O
i	O
,	O
ht+1	O
=	O
i	O
(	O
cid:48	O
)	O
are	O
obtained	O
by	O
inference	B
using	O
the	O
techniques	O
described	O
in	O
section	O
(	O
23.2.1	O
)	O
.	O
|vn	O
)	O
and	O
pold	O
(	O
ht	O
=	O
i|vn	O
)	O
equations	O
(	O
23.3.3,23.3.5,23.3.6	O
)	O
are	O
repeated	O
until	O
convergence	O
.	O
see	O
hmmem.m	O
and	O
demohmmlearn.m	O
.	O
parameter	B
initialisation	O
the	O
em	O
algorithm	B
converges	O
to	O
a	O
local	B
maxima	O
of	O
the	O
likelihood	B
and	O
,	O
in	O
general	O
,	O
there	O
is	O
no	O
guarantee	O
that	O
the	O
algorithm	B
will	O
ﬁnd	O
the	O
global	B
maximum	O
.	O
how	O
best	O
to	O
initialise	O
the	O
parameters	O
is	O
a	O
thorny	O
issue	O
,	O
(	O
cid:80	O
)	O
with	O
a	O
suitable	O
initialisation	O
of	O
the	O
emission	B
distribution	I
often	O
being	O
critical	O
for	O
success	O
[	O
229	O
]	O
.	O
a	O
practical	O
strategy	O
is	O
to	O
initialise	O
the	O
emission	O
p	O
(	O
v|h	O
)	O
based	O
on	O
ﬁrst	O
ﬁtting	O
a	O
simpler	O
non-temporal	O
mixture	B
model	I
h	O
p	O
(	O
v|h	O
)	O
p	O
(	O
h	O
)	O
to	O
the	O
data	B
.	O
continuous	B
observations	O
for	O
a	O
continuous	B
vector	O
observation	O
vt	O
,	O
with	O
dim	O
vt	O
=	O
d	O
,	O
we	O
require	O
a	O
model	B
p	O
(	O
vt|ht	O
)	O
mapping	O
the	O
discrete	B
state	O
ht	O
to	O
a	O
distribution	B
over	O
outputs	O
.	O
using	O
a	O
continuous	B
output	O
does	O
not	O
change	O
any	O
of	O
the	O
standard	O
inference	O
message	B
passing	I
equations	O
so	O
that	O
inference	B
can	O
be	O
carried	O
out	O
for	O
essentially	O
arbitrarily	O
complex	O
emission	O
distributions	O
.	O
indeed	O
,	O
ﬁltering	B
,	O
smoothing	B
and	O
viterbi	B
inference	O
,	O
the	O
normalisation	B
z	O
of	O
the	O
emission	O
p	O
(	O
v|h	O
)	O
=	O
φ	O
(	O
v	O
,	O
h	O
)	O
/z	O
is	O
not	O
required	O
.	O
for	O
learning	B
,	O
however	O
,	O
the	O
emission	O
normalisation	O
constant	O
is	O
required	O
since	O
this	O
is	O
a	O
dependent	O
on	O
the	O
parameters	O
of	O
the	O
model	B
.	O
23.3.2	O
mixture	B
emission	O
to	O
make	O
a	O
richer	O
emission	O
model	O
(	O
particularly	O
for	O
continuous	B
observations	O
)	O
,	O
one	O
approach	B
is	O
use	O
a	O
mixture	B
p	O
(	O
vt|ht	O
)	O
=	O
(	O
cid:88	O
)	O
kt	O
(	O
cid:88	O
)	O
t	O
(	O
cid:88	O
)	O
n	O
t=1	O
p	O
(	O
vt|kt	O
,	O
ht	O
)	O
p	O
(	O
kt|ht	O
)	O
(	O
23.3.7	O
)	O
where	O
kt	O
is	O
a	O
discrete	B
summation	O
variable	B
.	O
for	O
learning	B
,	O
it	O
is	O
useful	O
to	O
consider	O
the	O
kt	O
as	O
additional	O
latent	B
variables	O
so	O
that	O
updates	O
for	O
each	O
component	O
of	O
the	O
emission	O
model	O
can	O
be	O
derived	O
.	O
to	O
achieve	O
this	O
,	O
consider	O
the	O
contribution	O
to	O
the	O
energy	B
from	O
the	O
emission	O
(	O
assuming	O
equal	O
length	O
sequences	B
)	O
:	O
ev	O
≡	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
vn	O
t	O
|ht	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
ht|vn	O
1	O
:	O
t	O
)	O
(	O
23.3.8	O
)	O
as	O
it	O
stands	O
,	O
the	O
parameters	O
of	O
each	O
component	O
p	O
(	O
vt|kt	O
,	O
ht	O
)	O
are	O
coupled	B
in	O
the	O
above	O
expression	O
.	O
one	O
approach	B
is	O
to	O
consider	O
kl	O
(	O
q	O
(	O
kt|ht	O
)	O
|p	O
(	O
kt|ht	O
,	O
vt	O
)	O
)	O
≥	O
0	O
from	O
which	O
we	O
immediately	O
obtain	O
the	O
bound	B
log	O
p	O
(	O
vt	O
,	O
ht	O
)	O
≥	O
−	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
kt|ht	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
kt|ht	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
vt	O
,	O
kt	O
,	O
ht	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
kt|ht	O
)	O
(	O
23.3.9	O
)	O
(	O
23.3.10	O
)	O
and	O
424	O
log	O
p	O
(	O
vn	O
t	O
|hn	O
t	O
)	O
≥	O
−	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
kt|hn	O
t	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
kt|hn	O
t	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
vn	O
t	O
|kt	O
,	O
hn	O
t	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
kt|hn	O
t	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
kt|hn	O
t	O
)	O
(	O
23.3.11	O
)	O
t	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
kt|hn	O
draft	O
march	O
9	O
,	O
2010	O
learning	B
hmms	O
(	O
cid:88	O
)	O
t	O
(	O
cid:88	O
)	O
(	O
cid:68	O
)	O
n	O
t=1	O
(	O
cid:88	O
)	O
t	O
(	O
cid:88	O
)	O
n	O
t=1	O
(	O
cid:69	O
)	O
using	O
this	O
in	O
the	O
energy	B
contribution	O
(	O
23.3.8	O
)	O
we	O
have	O
the	O
bound	B
on	O
the	O
energy	B
contribution	O
ev	O
≥	O
−	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
kt|hn	O
t	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
kt|hn	O
t	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
vn	O
t	O
|kt	O
,	O
hn	O
t	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
kt|hn	O
t	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
kt|hn	O
t	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
kt|hn	O
t	O
)	O
q	O
(	O
hn	O
t	O
|vn	O
1	O
:	O
t	O
)	O
(	O
23.3.12	O
)	O
we	O
may	O
now	O
maximise	O
this	O
lower	O
bound	O
on	O
the	O
energy	B
(	O
instead	O
of	O
the	O
energy	B
itself	O
)	O
.	O
the	O
contribution	O
from	O
each	O
emission	O
component	O
p	O
(	O
v	O
=	O
v|h	O
=	O
h	O
,	O
k	O
=	O
k	O
)	O
is	O
q	O
(	O
kt	O
=	O
k|hn	O
t	O
=	O
h	O
)	O
q	O
(	O
hn	O
t	O
=	O
h|vn	O
1	O
:	O
t	O
)	O
log	O
p	O
(	O
vn	O
t	O
|h	O
=	O
h	O
,	O
k	O
=	O
k	O
)	O
(	O
23.3.13	O
)	O
the	O
above	O
can	O
then	O
be	O
optimised	O
(	O
m-step	O
)	O
for	O
ﬁxed	O
q	O
(	O
kt	O
=	O
k|hn	O
using	O
t	O
=	O
h	O
)	O
,	O
with	O
these	O
distributions	O
updated	O
the	O
contribution	O
to	O
the	O
energy	B
bound	O
from	O
the	O
mixture	B
weights	O
is	O
given	O
by	O
t	O
,	O
kt	O
)	O
p	O
(	O
kt|ht	O
)	O
t	O
(	O
cid:88	O
)	O
qnew	O
(	O
kt|hn	O
t	O
)	O
∝	O
p	O
(	O
vn|hn	O
log	O
p	O
(	O
k	O
=	O
k|h	O
=	O
h	O
)	O
(	O
cid:88	O
)	O
t	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
n	O
p	O
(	O
k	O
=	O
k|h	O
=	O
h	O
)	O
∝	O
t=1	O
n	O
t=1	O
q	O
(	O
kt	O
=	O
k|hn	O
t	O
=	O
h	O
)	O
q	O
(	O
hn	O
t	O
=	O
h|vn	O
1	O
:	O
t	O
)	O
q	O
(	O
kt	O
=	O
k|hn	O
t	O
=	O
h	O
)	O
q	O
(	O
hn	O
t	O
=	O
h|vn	O
1	O
:	O
t	O
)	O
so	O
that	O
the	O
m-step	O
update	O
for	O
the	O
mixture	B
weights	O
is	O
,	O
(	O
23.3.14	O
)	O
(	O
23.3.15	O
)	O
(	O
23.3.16	O
)	O
1	O
:	O
t	O
)	O
is	O
ﬁxed	O
,	O
during	O
which	O
the	O
emissions	O
p	O
(	O
v|h	O
,	O
k	O
)	O
are	O
learned	O
,	O
along	O
with	O
updating	O
q	O
(	O
kt	O
=	O
k|hn	O
t	O
=	O
in	O
this	O
case	O
the	O
em	O
algorithm	B
is	O
composed	O
of	O
an	O
‘	O
emission	O
’	O
em	O
loop	O
in	O
which	O
the	O
transitions	O
and	O
q	O
(	O
hn	O
t	O
=	O
h	O
)	O
.	O
h|vn	O
the	O
‘	O
transition	O
’	O
em	O
loop	O
ﬁxes	O
the	O
emission	B
distribution	I
p	O
(	O
v|h	O
)	O
and	O
learns	O
the	O
best	O
transition	O
p	O
(	O
ht|ht−1	O
)	O
.	O
an	O
alternative	O
to	O
the	O
above	O
derivation	O
is	O
to	O
consider	O
the	O
k	O
as	O
hidden	O
variables	O
,	O
and	O
then	O
use	O
standard	O
em	O
algorithm	B
on	O
the	O
joint	B
latent	O
variables	O
(	O
ht	O
,	O
kt	O
)	O
.	O
the	O
reader	O
may	O
show	O
that	O
the	O
two	O
approaches	O
are	O
equivalent	B
.	O
23.3.3	O
the	O
hmm-gmm	O
(	O
cid:0	O
)	O
vt	O
µkt	O
,	O
ht	O
,	O
σkt	O
,	O
ht	O
(	O
cid:1	O
)	O
a	O
common	O
continuous	B
observation	O
mixture	B
emission	O
model	B
component	O
is	O
a	O
gaussian	O
p	O
(	O
vt|kt	O
,	O
ht	O
)	O
=	O
n	O
(	O
23.3.17	O
)	O
so	O
that	O
kt	O
,	O
ht	O
indexes	O
the	O
k	O
×	O
h	O
mean	B
vectors	O
and	O
covariance	B
matrices	O
.	O
em	O
updates	O
for	O
these	O
means	O
and	O
covariances	O
are	O
straightforward	O
to	O
derive	O
from	O
equation	B
(	O
23.3.12	O
)	O
,	O
see	O
exercise	O
(	O
232	O
)	O
.	O
these	O
models	O
are	O
common	O
in	O
tracking	O
applications	O
,	O
in	O
particular	O
in	O
speech	B
recognition	I
(	O
usually	O
under	O
the	O
constraint	O
that	O
the	O
covariances	O
are	O
diagonal	O
)	O
.	O
23.3.4	O
discriminative	B
training	I
hmms	O
can	O
be	O
used	O
for	O
supervised	B
learning	I
of	O
sequences	B
.	O
that	O
is	O
,	O
for	O
each	O
sequence	O
vn	O
1	O
:	O
t	O
,	O
we	O
have	O
a	O
corresponding	O
class	O
label	O
cn	O
.	O
for	O
example	O
,	O
we	O
might	O
associated	O
a	O
particular	O
composer	O
c	O
with	O
a	O
sequence	O
v1	O
:	O
t	O
and	O
wish	O
to	O
make	O
a	O
model	B
that	O
will	O
predict	O
the	O
composer	O
for	O
a	O
novel	O
music	O
sequence	O
.	O
a	O
generative	B
approach	I
to	O
using	O
hmms	O
for	O
classiﬁcation	B
is	O
to	O
train	O
a	O
separate	O
hmm	O
for	O
each	O
class	O
,	O
p	O
(	O
v1	O
:	O
t|c	O
)	O
and	O
subsequently	O
use	O
bayes	O
’	O
rule	O
to	O
form	O
the	O
classiﬁcation	B
for	O
a	O
novel	O
sequence	O
v∗	O
1	O
:	O
t	O
using	O
(	O
23.3.18	O
)	O
∗	O
p	O
(	O
c	O
|v	O
∗	O
1	O
:	O
t	O
)	O
=	O
(	O
cid:80	O
)	O
c	O
p	O
(	O
v∗	O
c	O
(	O
cid:48	O
)	O
=1	O
p	O
(	O
v∗	O
1	O
:	O
t|c∗	O
)	O
p	O
(	O
c∗	O
)	O
1	O
:	O
t|c	O
(	O
cid:48	O
)	O
)	O
p	O
(	O
c	O
(	O
cid:48	O
)	O
)	O
if	O
the	O
data	B
is	O
noisy	O
and	O
diﬃcult	O
to	O
model	B
,	O
however	O
,	O
this	O
generative	B
approach	I
may	O
not	O
work	O
well	O
since	O
much	O
of	O
the	O
expressive	O
power	O
of	O
each	O
model	B
is	O
used	O
to	O
model	B
the	O
complex	O
data	B
,	O
rather	O
than	O
focussing	O
on	O
draft	O
march	O
9	O
,	O
2010	O
425	O
c1	O
h1	O
v1	O
c2	O
h2	O
v2	O
c3	O
h3	O
v3	O
c4	O
h4	O
v4	O
related	O
models	O
figure	O
23.10	O
:	O
an	O
explicit	O
duration	O
hmm	O
.	O
the	O
counter	O
variables	O
ct	O
deterministically	O
count	O
down	O
to	O
zero	O
.	O
when	O
they	O
reach	O
one	O
,	O
a	O
h	O
transition	O
is	O
allowed	O
,	O
and	O
the	O
new	O
value	B
for	O
ct	O
is	O
sampled	O
.	O
the	O
decision	B
boundary	I
.	O
in	O
applications	O
such	O
as	O
speech	O
recognition	O
,	O
improvements	O
in	O
performance	B
are	O
often	O
reported	O
when	O
the	O
models	O
are	O
trained	O
in	O
a	O
discriminative	B
way	O
.	O
in	O
discriminative	B
training	I
,	O
see	O
for	O
example	O
[	O
150	O
]	O
,	O
one	O
deﬁnes	O
a	O
new	O
single	O
discriminative	B
model	O
,	O
formed	O
from	O
the	O
c	O
hmms	O
using	O
(	O
23.3.19	O
)	O
p	O
(	O
c|v1	O
:	O
t	O
)	O
=	O
(	O
cid:80	O
)	O
c	O
p	O
(	O
v1	O
:	O
t|c	O
)	O
p	O
(	O
c	O
)	O
c	O
(	O
cid:48	O
)	O
=1	O
p	O
(	O
v1	O
:	O
t|c	O
(	O
cid:48	O
)	O
)	O
p	O
(	O
c	O
(	O
cid:48	O
)	O
)	O
log	O
p	O
(	O
cn|vn	O
1	O
:	O
t	O
)	O
=	O
log	O
p	O
(	O
vn	O
+	O
log	O
p	O
(	O
c	O
)	O
−	O
log	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
1	O
:	O
t|c	O
)	O
generative	B
likelihood	O
c	O
(	O
cid:88	O
)	O
c	O
(	O
cid:48	O
)	O
=1	O
and	O
then	O
maximises	O
the	O
likelihood	B
of	O
a	O
set	O
of	O
observed	O
classes	O
and	O
corresponding	O
observations	O
v1	O
:	O
t	O
.	O
for	O
a	O
single	O
data	B
pair	O
,	O
(	O
cn	O
,	O
vn	O
1	O
:	O
t	O
)	O
,	O
the	O
log	O
likelihood	B
is	O
(	O
cid:48	O
)	O
)	O
p	O
(	O
c	O
(	O
cid:48	O
)	O
)	O
p	O
(	O
vn	O
1	O
:	O
t|c	O
(	O
23.3.20	O
)	O
p	O
(	O
v1	O
:	O
t	O
,	O
c1	O
:	O
t	O
)	O
=	O
(	O
cid:89	O
)	O
t	O
the	O
ﬁrst	O
term	O
above	O
represents	O
the	O
generative	B
likelihood	O
term	O
,	O
with	O
the	O
last	O
term	O
accounting	O
for	O
the	O
dis-	O
crimination	O
.	O
whilst	O
deriving	O
em	O
style	O
updates	O
is	O
hampered	O
by	O
the	O
discriminative	B
terms	O
,	O
computing	O
the	O
gradient	B
is	O
straightforward	O
using	O
the	O
technique	O
described	O
in	O
section	O
(	O
11.7	O
)	O
.	O
in	O
some	O
applications	O
,	O
a	O
class	O
label	O
ct	O
is	O
available	O
at	O
each	O
timestep	O
,	O
together	O
with	O
an	O
observation	O
vt.	O
given	O
a	O
training	B
sequence	O
v1	O
:	O
t	O
,	O
c1	O
:	O
t	O
(	O
or	O
more	O
generally	O
a	O
set	O
of	O
sequences	B
)	O
the	O
aim	O
is	O
to	O
ﬁnd	O
the	O
optimal	O
class	O
sequence	O
c∗	O
1	O
:	O
t	O
for	O
a	O
novel	O
observation	O
sequence	O
v∗	O
1	O
:	O
t	O
.	O
one	O
approach	B
is	O
to	O
train	O
a	O
generative	B
model	O
p	O
(	O
vt|ct	O
)	O
p	O
(	O
ct|ct−1	O
)	O
(	O
23.3.21	O
)	O
(	O
cid:82	O
)	O
and	O
subsequently	O
use	O
viterbi	B
to	O
form	O
the	O
class	O
c∗	O
1	O
:	O
t	O
=	O
arg	O
maxc1	O
:	O
t	O
p	O
(	O
c1	O
:	O
t|v1	O
:	O
t	O
)	O
.	O
however	O
,	O
this	O
approach	B
may	O
not	O
be	O
optimal	O
in	O
terms	O
of	O
class	O
discrimination	O
.	O
a	O
cheap	O
surrogate	O
is	O
to	O
train	O
a	O
discriminative	B
classiﬁcation	O
model	B
˜p	O
(	O
ct|vt	O
)	O
separately	O
.	O
with	O
this	O
one	O
can	O
form	O
the	O
emission	O
(	O
here	O
written	O
for	O
continuous	B
vt	O
)	O
˜p	O
(	O
ct|vt	O
)	O
˜p	O
(	O
vt	O
)	O
˜p	O
(	O
ct|vt	O
)	O
˜p	O
(	O
vt	O
)	O
vt	O
p	O
(	O
vt|ct	O
)	O
=	O
where	O
˜p	O
(	O
vt	O
)	O
is	O
user	O
deﬁned	O
.	O
whilst	O
computing	O
the	O
local	B
normalisation	O
(	O
cid:82	O
)	O
if	O
the	O
only	O
use	O
of	O
p	O
(	O
vt|ct	O
)	O
is	O
to	O
ﬁnd	O
the	O
optimal	O
class	O
sequence	O
for	O
a	O
novel	O
observation	O
sequence	O
v∗	O
1	O
:	O
t	O
,	O
(	O
23.3.23	O
)	O
˜p	O
(	O
ct|vt	O
)	O
˜p	O
(	O
vt	O
)	O
may	O
be	O
problematic	O
,	O
∗	O
1	O
:	O
t	O
=	O
argmax	O
c	O
(	O
23.3.22	O
)	O
∗	O
1	O
:	O
t	O
)	O
vt	O
p	O
(	O
c1	O
:	O
t|v	O
c1	O
:	O
t	O
then	O
the	O
local	B
normalisations	O
play	O
no	O
role	O
since	O
they	O
are	O
independent	O
of	O
c.	O
hence	O
,	O
during	O
viterbi	B
decoding	O
we	O
may	O
replace	O
the	O
term	O
p	O
(	O
vt|ht	O
)	O
with	O
˜p	O
(	O
ct|vt	O
)	O
without	O
aﬀecting	O
the	O
optimal	O
sequence	O
.	O
using	O
a	O
model	B
in	O
this	O
way	O
is	O
a	O
special	O
case	O
of	O
the	O
general	O
hybrid	O
procedure	O
described	O
in	O
section	O
(	O
13.2.4	O
)	O
.	O
the	O
approach	B
is	O
suboptimal	O
since	O
learning	B
the	O
classiﬁer	B
is	O
divorced	O
from	O
learning	B
the	O
transition	O
model	O
.	O
nevertheless	O
,	O
this	O
heuristic	O
historically	O
has	O
some	O
support	O
in	O
the	O
speech	B
recognition	I
community	O
.	O
23.4	O
related	O
models	O
23.4.1	O
explicit	O
duration	B
model	I
for	O
a	O
hmm	O
with	O
self-transition	O
p	O
(	O
ht	O
=	O
i|ht−1	O
=	O
i	O
)	O
≡	O
γi	O
,	O
the	O
probability	B
that	O
the	O
latent	B
dynamics	O
stays	O
i	O
,	O
which	O
decays	O
exponentially	O
with	O
time	O
.	O
in	O
practice	O
,	O
however	O
,	O
we	O
would	O
in	O
state	O
i	O
for	O
τ	O
timesteps	O
is	O
γτ	O
426	O
draft	O
march	O
9	O
,	O
2010	O
related	O
models	O
x1	O
h1	O
v1	O
x2	O
h2	O
v2	O
x3	O
h3	O
v3	O
x4	O
h4	O
v4	O
figure	O
23.11	O
:	O
a	O
ﬁrst	B
order	I
input-output	O
hidden	B
markov	O
model	B
.	O
the	O
input	O
x	O
and	O
output	O
v	O
nodes	O
are	O
shaded	O
to	O
emphasise	O
that	O
their	O
states	O
are	O
known	O
dur-	O
ing	O
training	B
.	O
during	O
testing	O
,	O
the	O
inputs	O
are	O
known	O
and	O
the	O
outputs	O
are	O
predicted	O
.	O
often	O
like	O
to	O
constrain	O
the	O
dynamics	O
to	O
remain	O
in	O
the	O
same	O
state	O
for	O
a	O
minimum	O
number	O
of	O
timesteps	O
,	O
or	O
to	O
have	O
a	O
speciﬁed	O
duration	O
distribution	O
.	O
a	O
way	O
to	O
enforce	O
this	O
is	O
to	O
use	O
a	O
latent	B
counter	O
variable	B
ct	O
which	O
at	O
the	O
beginning	O
is	O
initialised	O
to	O
a	O
duration	O
sampled	O
from	O
the	O
duration	O
distribution	O
pdur	O
(	O
ct	O
)	O
with	O
maximal	O
duration	O
dmax	O
.	O
then	O
at	O
each	O
timestep	O
the	O
counter	O
decrements	O
by	O
1	O
,	O
until	O
it	O
reaches	O
1	O
,	O
after	O
which	O
a	O
new	O
duration	O
is	O
sampled	O
:	O
p	O
(	O
ct|ct−1	O
)	O
=	O
the	O
state	O
ht	O
can	O
transition	O
only	O
when	O
ct	O
=	O
1	O
:	O
(	O
cid:26	O
)	O
δ	O
(	O
ct	O
,	O
ct−1	O
−	O
1	O
)	O
(	O
cid:26	O
)	O
δ	O
(	O
ht	O
,	O
ht−1	O
)	O
pdur	O
(	O
ct	O
)	O
ct−1	O
>	O
1	O
ct−1	O
=	O
1	O
ct	O
>	O
1	O
ct	O
=	O
1	O
p	O
(	O
ht|ht−1	O
,	O
ct	O
)	O
=	O
ptran	O
(	O
ht|ht−1	O
)	O
(	O
23.4.1	O
)	O
(	O
23.4.2	O
)	O
including	O
the	O
counter	O
variable	B
c	O
deﬁnes	O
a	O
joint	B
latent	O
variable	B
(	O
c1	O
:	O
t	O
,	O
h1	O
:	O
t	O
)	O
distribution	B
that	O
ensures	O
h	O
re-	O
mains	O
in	O
a	O
desired	O
minimal	O
number	O
of	O
timesteps	O
,	O
see	O
ﬁg	O
(	O
23.10	O
)	O
.	O
since	O
dim	O
ct	O
⊗	O
ht	O
=	O
dmaxh	O
,	O
naively	O
the	O
the	O
forward	O
and	O
backward	O
recursions	O
,	O
the	O
deterministic	B
nature	O
of	O
the	O
transitions	O
means	O
that	O
this	O
can	O
be	O
computational	B
complexity	I
of	O
inference	B
in	O
this	O
model	B
scales	O
as	O
o	O
(	O
cid:0	O
)	O
t	O
h	O
2d2	O
reduced	O
to	O
o	O
(	O
cid:0	O
)	O
t	O
h	O
2dmax	O
(	O
cid:1	O
)	O
[	O
199	O
]	O
–	O
see	O
also	O
exercise	O
(	O
233	O
)	O
.	O
(	O
cid:1	O
)	O
.	O
however	O
,	O
when	O
one	O
runs	O
max	O
the	O
hidden	B
semi-markov	O
model	B
generalises	O
the	O
explicit	O
duration	B
model	I
in	O
that	O
once	O
a	O
new	O
duration	O
ct	O
is	O
sampled	O
,	O
the	O
model	B
emits	O
a	O
distribution	B
p	O
(	O
vt	O
:	O
t+ct−1|ht	O
)	O
deﬁned	O
on	O
a	O
segment	O
of	O
the	O
next	O
ct	O
observations	O
[	O
216	O
]	O
.	O
23.4.2	O
input-output	B
hmm	O
the	O
iohmm	O
[	O
31	O
]	O
is	O
a	O
hmm	O
with	O
additional	O
input	O
variables	O
x1	O
:	O
t	O
,	O
see	O
ﬁg	O
(	O
23.11	O
)	O
.	O
each	O
input	O
can	O
be	O
con-	O
tinuous	O
or	O
discrete	B
and	O
modulates	O
the	O
transitions	O
p	O
(	O
vt|ht	O
,	O
xt	O
)	O
p	O
(	O
ht|ht−1	O
,	O
xt	O
)	O
(	O
23.4.3	O
)	O
p	O
(	O
v1	O
:	O
t	O
,	O
h1	O
:	O
t|x1	O
:	O
t	O
)	O
=	O
(	O
cid:89	O
)	O
t	O
the	O
iohmm	O
may	O
be	O
used	O
as	O
a	O
conditional	B
predictor	O
,	O
where	O
the	O
outputs	O
vt	O
represent	O
the	O
prediction	B
at	O
time	O
t.	O
in	O
the	O
case	O
of	O
continuous	B
inputs	O
and	O
discrete	B
outputs	O
,	O
the	O
tables	O
p	O
(	O
vt|ht	O
,	O
xt	O
)	O
and	O
p	O
(	O
ht|ht−1	O
,	O
xt	O
)	O
are	O
usually	O
parameterised	O
using	O
a	O
non-linear	B
function	O
,	O
for	O
example	O
p	O
(	O
vt	O
=	O
y|ht	O
=	O
h	O
,	O
xt	O
=	O
x	O
,	O
w	O
)	O
∝	O
ewt	O
h	O
,	O
yx	O
inference	B
then	O
follows	O
in	O
a	O
similar	O
manner	O
as	O
for	O
the	O
standard	O
hmm	O
.	O
deﬁning	O
α	O
(	O
ht	O
)	O
≡	O
p	O
(	O
ht	O
,	O
v1	O
:	O
t|x1	O
:	O
t	O
)	O
the	O
forward	O
pass	O
is	O
given	O
by	O
α	O
(	O
ht	O
)	O
=	O
(	O
cid:88	O
)	O
=	O
(	O
cid:88	O
)	O
=	O
p	O
(	O
vt|xt	O
,	O
ht	O
)	O
(	O
cid:88	O
)	O
ht−1	O
ht−1	O
ht−1	O
draft	O
march	O
9	O
,	O
2010	O
p	O
(	O
ht	O
,	O
ht−1	O
,	O
v1	O
:	O
t−1	O
,	O
vt|x1	O
:	O
t	O
)	O
p	O
(	O
vt|v1	O
:	O
t−1	O
,	O
x1	O
:	O
t	O
,	O
ht	O
,	O
ht−1	O
)	O
p	O
(	O
ht|v1	O
:	O
t−1	O
,	O
x1	O
:	O
t	O
,	O
ht−1	O
)	O
p	O
(	O
v1	O
:	O
t−1	O
,	O
ht−1|x1	O
:	O
t	O
)	O
p	O
(	O
ht|ht−1	O
,	O
xt	O
)	O
α	O
(	O
ht−1	O
)	O
(	O
23.4.4	O
)	O
(	O
23.4.5	O
)	O
(	O
23.4.6	O
)	O
(	O
23.4.7	O
)	O
(	O
23.4.8	O
)	O
427	O
x	O
y2	O
y1	O
figure	O
23.12	O
:	O
linear	B
chain	O
crf	O
.	O
since	O
the	O
input	O
x	O
is	O
observed	O
,	O
the	O
dis-	O
tribution	O
is	O
just	O
a	O
linear	B
chain	O
factor	B
graph	I
.	O
the	O
inference	B
of	O
pairwise	B
marginals	O
p	O
(	O
yt	O
,	O
yt−1|x	O
)	O
is	O
therefore	O
straightforward	O
using	O
message	B
passing	I
.	O
y3	O
related	O
models	O
the	O
γ	O
backward	O
pass	O
is	O
p	O
(	O
ht|x1	O
:	O
t	O
,	O
v1	O
:	O
t	O
)	O
=	O
(	O
cid:88	O
)	O
ht+1	O
p	O
(	O
ht	O
,	O
ht+1|x1	O
:	O
t+1	O
,	O
xt+2	O
:	O
t	O
,	O
v1	O
:	O
t	O
)	O
=	O
(	O
cid:88	O
)	O
ht+1	O
p	O
(	O
ht|ht+1	O
,	O
x1	O
:	O
t+1	O
,	O
v1	O
:	O
t	O
)	O
p	O
(	O
ht+1|x1	O
:	O
t	O
,	O
v1	O
:	O
t	O
)	O
for	O
which	O
we	O
need	O
p	O
(	O
ht|ht+1	O
,	O
x1	O
:	O
t+1	O
,	O
v1	O
:	O
t	O
)	O
=	O
p	O
(	O
ht+1	O
,	O
ht|x1	O
:	O
t+1	O
,	O
v1	O
:	O
t	O
)	O
p	O
(	O
ht+1|x1	O
:	O
t+1	O
,	O
v1	O
:	O
t	O
)	O
the	O
likelihood	B
can	O
be	O
found	O
from	O
(	O
cid:80	O
)	O
α	O
(	O
ht	O
)	O
.	O
ht	O
(	O
cid:80	O
)	O
=	O
p	O
(	O
ht+1|ht	O
,	O
xt+1	O
)	O
p	O
(	O
ht|x1	O
:	O
t	O
,	O
v1	O
:	O
t	O
)	O
ht	O
p	O
(	O
ht+1|ht	O
,	O
xt+1	O
)	O
p	O
(	O
ht|x1	O
:	O
t	O
,	O
v1	O
:	O
t	O
)	O
(	O
23.4.9	O
)	O
(	O
23.4.10	O
)	O
direction	B
bias	I
consider	O
predicting	O
the	O
output	O
distribution	B
p	O
(	O
vt|x1	O
:	O
t	O
)	O
given	O
both	O
past	O
and	O
future	O
input	O
information	O
x1	O
:	O
t	O
.	O
because	O
the	O
hidden	B
states	O
are	O
unobserved	O
we	O
have	O
p	O
(	O
vt|x1	O
:	O
t	O
)	O
=	O
p	O
(	O
vt|x1	O
:	O
t	O
)	O
.	O
thus	O
the	O
prediction	B
uses	O
only	O
past	O
information	O
and	O
discards	O
any	O
future	O
contextual	O
information	O
.	O
this	O
‘	O
direction	B
bias	I
’	O
is	O
sometimes	O
considered	O
problematic	O
(	O
particularly	O
in	O
natural	B
language	O
modelling	B
)	O
and	O
motivates	O
the	O
use	O
of	O
undirected	B
models	O
,	O
such	O
as	O
conditional	O
random	O
ﬁelds	O
.	O
23.4.3	O
linear	B
chain	O
crfs	O
linear	B
chain	O
conditional	O
random	O
fields	O
(	O
crfs	O
)	O
are	O
an	O
extension	O
of	O
the	O
unstructured	O
crfs	O
we	O
brieﬂy	O
discussed	O
in	O
section	O
(	O
9.4.6	O
)	O
and	O
have	O
application	O
to	O
modelling	B
the	O
distribution	B
of	O
a	O
set	O
of	O
outputs	O
y1	O
:	O
t	O
given	O
an	O
input	O
vector	O
x.	O
for	O
example	O
,	O
x	O
might	O
represent	O
a	O
sentence	O
in	O
english	O
,	O
and	O
y1	O
:	O
t	O
should	O
represent	O
the	O
translation	O
into	O
french	O
.	O
note	O
that	O
the	O
vector	O
x	O
does	O
not	O
have	O
to	O
have	O
dimension	O
t	O
.	O
a	O
ﬁrst	B
order	I
linear	O
chain	B
crf	O
has	O
the	O
form	O
p	O
(	O
y1	O
:	O
t|x	O
,	O
λ	O
)	O
=	O
1	O
z	O
(	O
x	O
,	O
λ	O
)	O
t=2	O
φt	O
(	O
yt	O
,	O
yt−1	O
,	O
x	O
,	O
λ	O
)	O
(	O
23.4.11	O
)	O
where	O
λ	O
are	O
the	O
free	O
parameters	O
of	O
the	O
potentials	O
.	O
in	O
practice	O
it	O
is	O
common	O
to	O
use	O
potentials	O
of	O
the	O
form	O
t	O
(	O
cid:89	O
)	O
(	O
cid:33	O
)	O
exp	O
λkfk	O
,	O
t	O
(	O
yt	O
,	O
yt−1	O
,	O
x	O
)	O
(	O
23.4.12	O
)	O
where	O
fk	O
,	O
t	O
(	O
yt	O
,	O
yt−1	O
,	O
x	O
)	O
are	O
‘	O
features	O
’	O
,	O
see	O
also	O
section	O
(	O
9.4.6	O
)	O
.	O
given	O
a	O
set	O
of	O
input-output	B
sequence	O
pairs	O
,	O
xn	O
,	O
yn	O
1	O
:	O
t	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
(	O
assuming	O
all	O
sequenced	O
have	O
equal	O
length	O
t	O
for	O
simplicity	O
)	O
,	O
we	O
can	O
learn	O
the	O
parameters	O
λ	O
by	O
maximum	B
likelihood	I
.	O
under	O
the	O
standard	O
i.i.d	O
.	O
data	B
assumption	O
,	O
the	O
log	O
likelihood	B
is	O
λkfk	O
(	O
yn	O
t	O
,	O
yn	O
t−1	O
,	O
xn	O
)	O
−	O
log	O
z	O
(	O
xn	O
,	O
λ	O
)	O
(	O
23.4.13	O
)	O
(	O
cid:88	O
)	O
n	O
the	O
reader	O
may	O
readily	O
check	B
that	O
the	O
log	O
likelihood	B
is	O
concave	O
so	O
that	O
the	O
objective	O
function	B
has	O
no	O
local	O
optima	O
.	O
the	O
gradient	B
is	O
given	O
by	O
fi	O
(	O
yn	O
t	O
,	O
yn	O
t−1	O
,	O
xn	O
)	O
−	O
(	O
cid:104	O
)	O
fi	O
(	O
yt	O
,	O
yt−1	O
,	O
xn	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
yt	O
,	O
yt−1|xn	O
,	O
λ	O
)	O
(	O
23.4.14	O
)	O
learning	B
therefore	O
requires	O
inference	B
of	O
the	O
marginal	B
terms	O
p	O
(	O
yt	O
,	O
yt−1|x	O
,	O
λ	O
)	O
.	O
since	O
equation	B
(	O
23.4.11	O
)	O
cor-	O
responds	O
to	O
a	O
linear	B
chain	O
factor	B
graph	I
,	O
see	O
ﬁg	O
(	O
23.12	O
)	O
,	O
inference	B
of	O
pairwise	B
marginals	O
is	O
straightforward	O
428	O
draft	O
march	O
9	O
,	O
2010	O
(	O
cid:32	O
)	O
k	O
(	O
cid:88	O
)	O
k=1	O
l	O
(	O
λ	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
t	O
,	O
n	O
k	O
l	O
(	O
λ	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:16	O
)	O
n	O
,	O
t	O
∂	O
∂λi	O
(	O
cid:17	O
)	O
related	O
models	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
23.13	O
:	O
using	O
a	O
linear	B
chain	O
crf	O
to	O
learn	O
the	O
sequences	B
in	O
table	O
(	O
23.1	O
)	O
.	O
log	O
likelihood	B
under	O
gradient	B
ascent	O
.	O
(	O
b	O
)	O
:	O
the	O
learned	O
parameter	B
vector	O
λ	O
at	O
the	O
end	O
of	O
training	B
.	O
(	O
a	O
)	O
:	O
the	O
evolution	O
of	O
the	O
using	O
message	B
passing	I
.	O
this	O
can	O
be	O
achieved	O
using	O
either	O
the	O
standard	O
factor	O
graph	B
message	O
passing	B
or	O
by	O
deriving	O
an	O
explicit	O
algorithm	B
,	O
see	O
exercise	O
(	O
227	O
)	O
.	O
finding	O
the	O
most	O
likely	O
output	O
sequence	O
for	O
a	O
novel	O
input	O
x∗	O
is	O
straightforward	O
since	O
(	O
cid:89	O
)	O
t	O
∗	O
1	O
:	O
t	O
=	O
argmax	O
y	O
y1	O
:	O
t	O
φt	O
(	O
yt	O
,	O
yt−1	O
,	O
x∗	O
,	O
λ	O
)	O
(	O
23.4.15	O
)	O
corresponds	O
again	O
to	O
a	O
simple	O
linear	O
chain	B
,	O
for	O
which	O
max-product	B
inference	O
yields	O
the	O
required	O
result	O
,	O
see	O
also	O
exercise	O
(	O
226	O
)	O
.	O
in	O
some	O
applications	O
,	O
particularly	O
in	O
natural	B
language	O
processing	O
,	O
the	O
dimension	O
k	O
of	O
the	O
vector	O
of	O
features	O
f1	O
,	O
.	O
.	O
.	O
,	O
fk	O
may	O
be	O
many	O
hundreds	O
of	O
thousands	O
.	O
this	O
means	O
that	O
the	O
storage	O
of	O
the	O
hessian	O
is	O
not	O
feasible	O
for	O
newton	O
based	O
training	B
and	O
either	O
limited	O
memory	O
methods	O
or	O
conjugate	B
gradient	I
techniques	O
are	O
typically	O
preferred	O
[	O
288	O
]	O
.	O
7	O
3	O
10	O
1	O
9	O
2	O
7	O
2	O
7	O
3	O
4	O
1	O
3	O
1	O
8	O
3	O
8	O
3	O
9	O
1	O
7	O
3	O
2	O
1	O
3	O
3	O
8	O
3	O
3	O
2	O
2	O
1	O
9	O
3	O
4	O
1	O
3	O
3	O
3	O
2	O
6	O
3	O
4	O
3	O
4	O
3	O
10	O
1	O
8	O
3	O
5	O
3	O
2	O
1	O
8	O
3	O
7	O
1	O
7	O
3	O
3	O
2	O
7	O
2	O
5	O
1	O
6	O
2	O
table	O
23.1	O
:	O
a	O
subset	O
of	O
the	O
10	O
training	B
input-output	O
sequences	B
.	O
each	O
row	O
contains	O
an	O
input	O
xt	O
(	O
upper	O
en-	O
try	O
)	O
and	O
output	O
yt	O
(	O
lower	O
entry	O
)	O
.	O
there	O
are	O
10	O
input	O
states	O
and	O
3	O
output	O
states	O
.	O
6	O
3	O
10	O
1	O
potentials	O
φ	O
(	O
yt	O
,	O
yt−1	O
,	O
xt	O
)	O
=	O
exp	O
(	O
(	O
cid:80	O
)	O
example	O
99	O
(	O
linear	B
chain	O
crf	O
)	O
.	O
as	O
a	O
model	B
for	O
the	O
data	B
in	O
table	O
(	O
23.1	O
)	O
,	O
a	O
linear	B
crf	O
model	B
has	O
i	O
λifi	O
(	O
yt	O
,	O
yt−1	O
,	O
xt	O
)	O
)	O
where	O
we	O
set	O
the	O
binary	O
feature	O
functions	O
by	O
ﬁrst	O
mapping	O
each	O
of	O
the	O
dim	O
(	O
x	O
)	O
×	O
dim	O
(	O
y	O
)	O
2	O
states	O
to	O
a	O
unique	O
integer	O
i	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
from	O
1	O
to	O
dim	O
(	O
x	O
)	O
×	O
dim	O
(	O
y	O
)	O
2	O
(	O
23.4.16	O
)	O
fi	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
(	O
yt	O
,	O
yt−1	O
,	O
xt	O
)	O
=	O
i	O
[	O
yt	O
=	O
a	O
]	O
i	O
[	O
yt−1	O
=	O
b	O
]	O
i	O
[	O
xt	O
=	O
c	O
]	O
that	O
is	O
,	O
each	O
joint	B
conﬁguration	O
of	O
yt	O
,	O
yt−1	O
,	O
xt	O
is	O
mapped	O
to	O
an	O
index	O
,	O
and	O
in	O
this	O
case	O
the	O
feature	O
vector	O
f	O
will	O
trivially	O
have	O
only	O
a	O
single	O
non-zero	O
entry	O
.	O
the	O
evolution	O
of	O
the	O
gradient	B
ascent	O
training	B
algorithm	O
is	O
plotted	O
in	O
ﬁg	O
(	O
23.13	O
)	O
.	O
in	O
practice	O
one	O
would	O
use	O
richer	O
feature	O
functions	O
deﬁned	O
to	O
seek	O
features	O
of	O
the	O
input	O
sequence	O
x	O
and	O
also	O
to	O
produce	O
a	O
feature	O
vector	O
with	O
more	O
than	O
one	O
non-zero	O
entry	O
.	O
see	O
demolinearcrf.m	O
.	O
draft	O
march	O
9	O
,	O
2010	O
429	O
05101520253035404550−14−12−10−8−6−4−200510152025303540−20−15−10−50510152025	O
applications	O
figure	O
23.14	O
:	O
a	O
dynamic	B
bayesian	O
network	O
.	O
possi-	O
ble	O
transitions	O
between	O
variables	O
at	O
the	O
same	O
time-	O
slice	O
have	O
not	O
been	O
shown	O
.	O
figure	O
23.15	O
:	O
a	O
coupled	B
hmm	O
.	O
for	O
example	O
the	O
up-	O
per	O
hmm	O
might	O
model	B
speech	O
,	O
and	O
the	O
lower	O
the	O
cor-	O
responding	O
video	O
sequence	O
.	O
the	O
upper	O
hidden	B
units	O
then	O
correspond	O
to	O
phonemes	O
,	O
and	O
the	O
lower	O
to	O
mouth	O
positions	O
;	O
this	O
model	B
therefore	O
captures	O
the	O
expected	O
coupling	O
between	O
mouth	O
positions	O
and	O
phonemes	O
.	O
x3	O
(	O
t	O
)	O
x2	O
(	O
t	O
)	O
x1	O
(	O
t	O
)	O
x3	O
(	O
t	O
+	O
1	O
)	O
x2	O
(	O
t	O
+	O
1	O
)	O
x1	O
(	O
t	O
+	O
1	O
)	O
v1	O
(	O
t	O
)	O
v1	O
(	O
t	O
+	O
1	O
)	O
h1	O
(	O
t	O
)	O
h1	O
(	O
t	O
+	O
1	O
)	O
h2	O
(	O
t	O
)	O
h2	O
(	O
t	O
+	O
1	O
)	O
v2	O
(	O
t	O
)	O
v2	O
(	O
t	O
+	O
1	O
)	O
23.4.4	O
dynamic	B
bayesian	O
networks	O
a	O
dbn	O
is	O
deﬁned	O
as	O
a	O
belief	O
networkreplicated	O
through	O
time	O
.	O
for	O
a	O
multivariate	B
xt	O
,	O
with	O
dim	O
xt	O
=	O
d	O
,	O
the	O
dbn	O
deﬁnes	O
a	O
joint	B
model	O
t	O
(	O
cid:89	O
)	O
d	O
(	O
cid:89	O
)	O
t=1	O
i=1	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xt	O
)	O
=	O
p	O
(	O
xi	O
(	O
t	O
)	O
|x\i	O
(	O
t	O
)	O
,	O
x	O
(	O
t	O
−	O
1	O
)	O
)	O
(	O
23.4.17	O
)	O
where	O
x\i	O
(	O
t	O
)	O
denotes	O
the	O
set	O
of	O
variables	O
at	O
time	O
t	O
,	O
except	O
for	O
xi	O
(	O
t	O
)	O
.	O
the	O
form	O
of	O
each	O
p	O
(	O
xi	O
(	O
t	O
)	O
|x\i	O
(	O
t	O
)	O
,	O
x	O
(	O
t−1	O
)	O
)	O
is	O
chosen	O
such	O
that	O
the	O
overall	O
distribution	B
remains	O
acyclic	O
.	O
at	O
each	O
time-step	O
t	O
there	O
is	O
a	O
set	O
of	O
variables	O
xi	O
(	O
t	O
)	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
,	O
some	O
of	O
which	O
may	O
be	O
observed	O
.	O
in	O
a	O
ﬁrst	B
order	I
dbn	O
,	O
each	O
variable	B
xi	O
(	O
t	O
)	O
has	O
parental	O
variables	O
taken	O
from	O
the	O
set	O
of	O
variables	O
in	O
the	O
previous	O
time-slice	O
,	O
xt−1	O
,	O
or	O
from	O
the	O
present	O
time-slice	O
.	O
in	O
most	O
applications	O
,	O
the	O
model	B
is	O
temporally	O
homogeneous	O
so	O
that	O
one	O
may	O
fully	O
describe	O
the	O
distribution	B
in	O
terms	O
of	O
a	O
two-time-slice	O
model	B
,	O
ﬁg	O
(	O
23.14	O
)	O
.	O
the	O
generalisation	B
to	O
higher-order	O
models	O
is	O
straightforward	O
.	O
a	O
coupled	B
hmm	O
is	O
a	O
special	O
dbn	O
that	O
may	O
be	O
used	O
to	O
model	B
coupled	O
‘	O
streams	O
’	O
of	O
information	O
,	O
for	O
example	O
video	O
and	O
audio	O
,	O
see	O
ﬁg	O
(	O
23.15	O
)	O
[	O
209	O
]	O
.	O
23.5	O
applications	O
23.5.1	O
object	O
tracking	O
hmms	O
are	O
used	O
to	O
track	O
moving	O
objects	O
,	O
based	O
on	O
an	O
understanding	O
of	O
the	O
dynamics	O
of	O
the	O
object	O
(	O
encoded	O
in	O
the	O
transition	B
distribution	I
)	O
and	O
an	O
understanding	O
of	O
how	O
an	O
object	O
with	O
a	O
known	O
position	O
would	O
be	O
observed	O
(	O
encoded	O
in	O
the	O
emission	B
distribution	I
)	O
.	O
given	O
an	O
observed	O
sequence	O
,	O
the	O
hidden	B
position	O
can	O
then	O
be	O
inferred	O
.	O
the	O
burglar	O
,	O
example	O
(	O
10	O
)	O
is	O
a	O
case	O
in	O
point	O
.	O
hmms	O
have	O
been	O
applied	O
in	O
a	O
many	O
tracking	O
contexts	O
,	O
including	O
tracking	O
people	O
in	O
videos	O
,	O
musical	O
pitch	O
,	O
and	O
many	O
more	O
[	O
54	O
,	O
229	O
,	O
51	O
]	O
.	O
23.5.2	O
automatic	O
speech	O
recognition	O
many	O
speech	B
recognition	I
systems	O
make	O
use	O
of	O
hmms	O
[	O
300	O
]	O
.	O
roughly	O
speaking	O
,	O
a	O
continuous	B
output	O
vector	O
vt	O
at	O
time	O
t	O
,	O
represents	O
which	O
frequencies	O
are	O
present	O
in	O
the	O
speech	O
signal	O
in	O
a	O
small	O
window	O
around	O
time	O
t.	O
these	O
acoustic	O
vectors	O
are	O
typically	O
formed	O
from	O
taking	O
a	O
discrete	B
fourier	O
transform	O
of	O
the	O
speech	O
signal	O
over	O
a	O
small	O
window	O
around	O
time	O
t	O
,	O
with	O
additional	O
transformations	O
to	O
mimic	O
human	O
auditory	O
processing	O
.	O
alternatively	O
,	O
related	O
forms	O
of	O
linear	B
coding	O
of	O
the	O
observed	O
acoustic	O
waveform	O
may	O
be	O
used	O
[	O
131	O
]	O
.	O
the	O
corresponding	O
discrete	B
latent	O
state	O
ht	O
represents	O
a	O
phoneme	O
–	O
a	O
basic	O
unit	O
of	O
human	O
speech	O
(	O
for	O
which	O
there	O
are	O
44	O
in	O
standard	O
english	O
)	O
.	O
training	B
data	O
is	O
painstakingly	O
constructed	O
by	O
a	O
human	O
linguist	O
who	O
430	O
draft	O
march	O
9	O
,	O
2010	O
applications	O
determines	O
the	O
phoneme	O
ht	O
for	O
each	O
time	O
t	O
and	O
many	O
diﬀerent	O
observed	O
sequences	O
vt.	O
given	O
then	O
each	O
acoustic	O
vector	O
vt	O
and	O
an	O
associated	O
phoneme	O
ht	O
,	O
one	O
may	O
use	O
maximum	B
likelihood	I
to	O
ﬁt	O
a	O
mixture	O
of	O
(	O
usually	O
isotropic	B
)	O
gaussians	O
p	O
(	O
vt|ht	O
)	O
to	O
vt.	O
this	O
forms	O
the	O
emission	B
distribution	I
for	O
a	O
hmm	O
.	O
using	O
the	O
database	O
of	O
labelled	B
phonemes	O
,	O
the	O
phoneme	O
transition	O
p	O
(	O
ht|ht−1	O
)	O
can	O
be	O
learned	O
(	O
by	O
simple	O
counting	O
)	O
and	O
forms	O
the	O
transition	B
distribution	I
for	O
a	O
hmm	O
.	O
note	O
that	O
in	O
this	O
case	O
,	O
since	O
the	O
‘	O
hidden	B
’	O
variable	B
h	O
and	O
observation	O
v	O
are	O
known	O
during	O
training	B
,	O
training	B
the	O
hmm	O
is	O
straightforward	O
and	O
boils	O
down	O
to	O
training	B
the	O
emission	O
and	O
transition	O
distributions	O
independently	O
.	O
for	O
a	O
new	O
sequence	O
of	O
‘	O
acoustic	O
’	O
vectors	O
v1	O
:	O
t	O
we	O
can	O
then	O
use	O
the	O
hmm	O
to	O
infer	O
the	O
most	O
likely	O
phoneme	O
sequence	O
through	O
time	O
,	O
arg	O
maxh1	O
:	O
t	O
p	O
(	O
h1	O
:	O
t|v1	O
:	O
t	O
)	O
,	O
which	O
takes	O
into	O
account	O
both	O
the	O
way	O
that	O
phonemes	O
appear	O
as	O
acoustic	O
vectors	O
,	O
and	O
also	O
the	O
prior	B
language	O
constraints	O
of	O
likely	O
phoneme	O
to	O
phoneme	O
transi-	O
tions	O
.	O
the	O
fact	O
that	O
people	O
speak	O
at	O
diﬀerent	O
speeds	O
can	O
be	O
addressed	O
using	O
time-warping	O
in	O
which	O
the	O
latent	B
phoneme	O
remains	O
in	O
the	O
same	O
state	O
for	O
a	O
number	O
of	O
timesteps	O
.	O
hmm	O
models	O
are	O
typically	O
trained	O
on	O
the	O
assumption	O
of	O
‘	O
clean	O
’	O
underlying	O
speech	O
.	O
in	O
practice	O
noise	O
cor-	O
rupts	O
the	O
speech	O
signal	O
in	O
a	O
complex	O
way	O
,	O
so	O
that	O
the	O
resulting	O
model	B
is	O
inappropriate	O
,	O
and	O
performance	B
degrades	O
signiﬁcantly	O
.	O
to	O
account	O
for	O
this	O
,	O
it	O
is	O
traditional	O
to	O
attempt	O
to	O
denoise	O
the	O
signal	O
before	O
sending	O
this	O
to	O
a	O
standard	O
hmm	O
recogniser	O
.	O
if	O
the	O
hmm	O
is	O
used	O
to	O
model	B
a	O
single	O
word	O
,	O
it	O
is	O
natural	B
to	O
constrain	O
the	O
hidden	B
state	O
sequence	O
to	O
go	O
‘	O
forwards	O
’	O
through	O
time	O
,	O
visiting	O
a	O
set	O
of	O
states	O
in	O
sequence	O
(	O
since	O
the	O
phoneme	O
order	O
for	O
the	O
word	O
is	O
known	O
)	O
.	O
in	O
this	O
case	O
the	O
structure	B
of	O
the	O
transition	O
matrices	O
is	O
upper	O
triangular	O
(	O
or	O
lower	O
,	O
depending	O
on	O
your	O
deﬁnition	O
)	O
,	O
or	O
even	O
a	O
banded	O
triangular	O
matrix	B
.	O
such	O
forward	O
constraints	O
describe	O
a	O
so-called	O
left-to-right	O
transition	B
matrix	I
.	O
23.5.3	O
bioinformatics	B
in	O
the	O
ﬁeld	O
of	O
bioinformatics	B
hmms	O
have	O
been	O
widely	O
applied	O
to	O
modelling	B
genetic	O
sequences	B
.	O
multiple	O
sequence	O
alignment	O
using	O
forms	O
of	O
constrained	O
hmms	O
have	O
been	O
particularly	O
successful	O
.	O
other	O
applications	O
involve	O
gene	O
ﬁnding	O
and	O
protein	O
family	B
modelling	O
[	O
163	O
,	O
84	O
]	O
.	O
23.5.4	O
part-of-speech	B
tagging	I
consider	O
the	O
sentence	O
below	O
in	O
which	O
each	O
word	O
has	O
been	O
linguistically	O
tagged	O
hospitality_nn	O
is_bez	O
an_at	O
excellent_jj	O
virtue_nn	O
,	O
_	O
,	O
but_cc	O
not_xnot	O
when_wrb	O
the_ati	O
guests_nns	O
have_hv	O
to_to	O
sleep_vb	O
in_in	O
rows_nns	O
in_in	O
the_ati	O
cellar_nn	O
!	O
_	O
!	O
the	O
subscripts	O
denote	O
a	O
linguistic	O
tag	O
,	O
for	O
example	O
nn	O
is	O
the	O
singular	B
common	O
noun	O
tag	O
,	O
ati	O
is	O
the	O
article	O
tag	O
etc	O
.	O
given	O
a	O
training	B
set	O
of	O
such	O
tagged	O
sequences	B
,	O
the	O
task	O
is	O
to	O
tag	O
a	O
novel	O
word	O
sequence	O
.	O
one	O
approach	B
is	O
to	O
use	O
ht	O
to	O
be	O
a	O
tag	O
,	O
and	O
vt	O
to	O
be	O
a	O
word	O
and	O
ﬁt	O
a	O
hmm	O
to	O
this	O
data	B
.	O
for	O
the	O
training	B
data	O
,	O
both	O
the	O
tags	O
and	O
words	O
are	O
observed	O
so	O
that	O
maximum	B
likelihood	I
training	O
of	O
the	O
transition	O
and	O
emission	B
distribution	I
can	O
be	O
achieved	O
by	O
simple	O
counting	O
.	O
given	O
a	O
new	O
sequence	O
of	O
words	O
,	O
the	O
most	O
likely	O
tag	O
sequence	O
can	O
be	O
inferred	O
using	O
the	O
viterbi	B
algorithm	O
.	O
more	O
recent	O
part-of-speech	O
taggers	O
tend	O
to	O
use	O
conditional	O
random	O
ﬁelds	O
in	O
which	O
the	O
input	O
sequence	O
x1	O
:	O
t	O
is	O
the	O
sentence	O
and	O
the	O
output	O
sequence	O
y1	O
:	O
t	O
is	O
the	O
tag	O
sequence	O
.	O
one	O
possible	O
parameterisation	B
of	O
for	O
a	O
linear	B
chain	O
crf	O
is	O
to	O
use	O
a	O
potential	B
of	O
the	O
form	O
φ	O
(	O
yt−1	O
,	O
yt	O
)	O
φ	O
(	O
yt	O
,	O
x	O
)	O
in	O
which	O
the	O
ﬁrst	O
factor	O
encodes	O
the	O
grammatical	O
structure	B
of	O
the	O
language	O
and	O
the	O
second	O
the	O
a	O
priori	O
likely	O
tag	O
yt	O
[	O
166	O
]	O
.	O
draft	O
march	O
9	O
,	O
2010	O
431	O
exercises	O
23.6	O
code	O
demomixmarkov.m	O
:	O
demo	O
for	O
mixture	O
of	O
markov	O
models	O
mixmarkov.m	O
:	O
mixture	O
of	O
markov	O
models	O
demohmminference.m	O
:	O
demo	O
of	O
hmm	O
inference	B
hmmforward.m	O
:	O
forward	O
α	O
recursion	O
hmmbackward.m	O
:	O
forward	O
β	O
recursion	O
hmmgamma.m	O
:	O
rts	O
γ	O
‘	O
correction	O
’	O
recursion	O
hmmsmooth.m	O
:	O
single	O
and	O
pairwise	B
α	O
−	O
β	O
smoothing	O
hmmviterbi.m	O
:	O
most	B
likely	I
state	I
(	O
viterbi	B
)	O
algorithm	B
demohmmburglar.m	O
:	O
demo	O
of	O
burglar	O
localisation	B
demohmmbigram.m	O
:	O
demo	O
of	O
stubby	O
ﬁngers	O
typing	O
hmmem.m	O
:	O
em	O
algorithm	B
for	O
hmm	O
(	O
baum-welch	O
)	O
demohmmlearn.m	O
:	O
demo	O
of	O
em	O
algorithm	B
for	O
hmm	O
(	O
baum-welch	O
)	O
demolinearcrf.m	O
:	O
demo	O
of	O
learning	B
a	O
linear	B
chain	O
crf	O
the	O
following	O
linear	B
chain	O
crf	O
potential	B
is	O
particularly	O
simple	O
and	O
in	O
practice	O
one	O
would	O
use	O
a	O
more	O
complex	O
one	O
.	O
linearcrfpotential.m	O
:	O
linear	B
crf	O
potential	B
the	O
following	O
likelihood	B
and	O
gradient	B
routines	O
are	O
valid	O
for	O
any	O
linear	B
crf	O
potential	B
φ	O
(	O
yt−1	O
,	O
yt	O
,	O
x	O
)	O
.	O
linearcrfgrad.m	O
:	O
linear	B
crf	O
gradient	B
linearcrfloglik.m	O
:	O
linear	B
crf	O
log	O
likelihood	B
23.7	O
exercises	O
exercise	O
217.	O
a	O
stochastic	O
matrix	B
mij	O
as	O
non-negative	O
entries	O
with	O
(	O
cid:80	O
)	O
λ	O
and	O
eigenvector	O
e	O
such	O
(	O
cid:80	O
)	O
j	O
mijej	O
=	O
λei	O
.	O
by	O
summing	O
over	O
i	O
show	O
that	O
,	O
provided	O
(	O
cid:80	O
)	O
i	O
mij	O
=	O
1.	O
consider	O
an	O
eigenvalue	O
i	O
ei	O
>	O
0	O
,	O
then	O
λ	O
must	O
exercise	O
218.	O
consider	O
the	O
markov	O
chain	B
with	O
transition	B
matrix	I
be	O
equal	O
to	O
1	O
.	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
0	O
1	O
1	O
0	O
m	O
=	O
show	O
that	O
this	O
markov	O
chain	B
does	O
not	O
have	O
an	O
equilibrium	O
distribution	B
and	O
state	O
a	O
stationary	B
distribution	I
for	O
this	O
chain	B
.	O
exercise	O
219.	O
consider	O
a	O
hmm	O
with	O
3	O
states	O
(	O
m	O
=	O
3	O
)	O
and	O
2	O
output	O
symbols	O
,	O
with	O
a	O
left-to-right	O
state	O
transition	B
matrix	I
(	O
23.7.1	O
)	O
(	O
23.7.2	O
)	O
(	O
23.7.3	O
)	O
	O
0.5	O
0.0	O
0.0	O
(	O
cid:18	O
)	O
0.7	O
0.4	O
0.8	O
0.3	O
0.6	O
0.0	O
0.2	O
0.4	O
1.0	O
0.3	O
0.6	O
0.2	O
	O
(	O
cid:19	O
)	O
a	O
=	O
b	O
=	O
where	O
aij	O
≡	O
p	O
(	O
h	O
(	O
t	O
+	O
1	O
)	O
=	O
i|h	O
(	O
t	O
)	O
=	O
j	O
)	O
,	O
emission	B
matrix	I
bij	O
≡	O
p	O
(	O
v	O
(	O
t	O
)	O
=	O
i|h	O
(	O
t	O
)	O
=	O
j	O
)	O
and	O
initial	O
state	O
probability	B
vector	O
a	O
=	O
(	O
0.9	O
0.1	O
0.0	O
)	O
t.	O
given	O
the	O
observed	O
symbol	O
sequence	O
is	O
v1:3	O
=	O
(	O
0	O
,	O
1	O
,	O
1	O
)	O
:	O
1.	O
compute	O
p	O
(	O
v1:3	O
)	O
.	O
2.	O
compute	O
p	O
(	O
h1|v1:3	O
)	O
.	O
3.	O
find	O
the	O
most	O
probable	O
hidden	O
state	O
sequence	O
arg	O
maxh1:3	O
p	O
(	O
h1:3|v1:3	O
)	O
.	O
432	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
exercise	O
220.	O
this	O
exercise	O
follows	O
from	O
example	O
(	O
98	O
)	O
.	O
given	O
the	O
27	O
long	O
character	O
string	O
rgenmonleunosbpnntje	O
vrancg	O
typed	O
with	O
‘	O
stubby	O
ﬁngers	O
’	O
,	O
what	O
is	O
the	O
most	O
likely	O
correct	O
english	O
sentence	O
intended	O
?	O
in	O
the	O
list	O
of	O
decoded	O
sequences	B
,	O
what	O
value	B
is	O
log	O
p	O
(	O
h1:27|v1:27	O
)	O
for	O
this	O
sequence	O
?	O
you	O
will	O
need	O
to	O
modify	O
demohmmbigram.m	O
suitably	O
.	O
exercise	O
221.	O
show	O
that	O
if	O
a	O
transition	O
probability	O
aij	O
=	O
p	O
(	O
ht	O
=	O
i|ht−1	O
=	O
j	O
)	O
in	O
a	O
hmm	O
is	O
initialised	O
to	O
zero	O
for	O
em	O
training	B
,	O
then	O
it	O
will	O
remain	O
at	O
zero	O
throughout	O
training	B
.	O
exercise	O
222.	O
consider	O
the	O
problem	B
:	O
find	O
the	O
most	O
likely	O
joint	O
output	O
sequence	O
v1	O
:	O
t	O
for	O
a	O
hmm	O
.	O
that	O
is	O
,	O
v	O
∗	O
1	O
:	O
t	O
≡	O
argmax	O
v1	O
:	O
t	O
p	O
(	O
v1	O
:	O
t	O
)	O
where	O
p	O
(	O
h1	O
:	O
t	O
,	O
v1	O
:	O
t	O
)	O
=	O
t	O
(	O
cid:89	O
)	O
t=1	O
p	O
(	O
vt|ht	O
)	O
p	O
(	O
ht|ht−1	O
)	O
(	O
23.7.4	O
)	O
(	O
23.7.5	O
)	O
1.	O
explain	O
why	O
a	O
local	B
message	O
passing	B
algorithm	O
can	O
not	O
,	O
in	O
general	O
,	O
be	O
found	O
for	O
this	O
problem	B
and	O
discuss	O
the	O
computational	B
complexity	I
of	O
ﬁnding	O
an	O
exact	O
solution	O
.	O
2.	O
explain	O
how	O
to	O
adapt	O
the	O
expectation-maximisation	O
algorithm	B
to	O
form	O
a	O
recursive	O
algorithm	B
,	O
for	O
1	O
:	O
t	O
.	O
explain	O
which	O
it	O
guarantees	O
an	O
improved	O
solution	O
at	O
each	O
iteration	B
.	O
ﬁnding	O
an	O
approximate	B
v∗	O
additionally	O
,	O
explain	O
how	O
the	O
algorithm	B
can	O
be	O
implemented	O
using	O
local	B
message	O
passing	B
.	O
exercise	O
223.	O
explain	O
how	O
to	O
train	O
a	O
hmm	O
using	O
em	O
,	O
but	O
with	O
a	O
constrained	O
transition	O
matrix	B
.	O
particular	O
,	O
explain	O
how	O
to	O
learn	O
a	O
transition	B
matrix	I
with	O
an	O
upper	O
triangular	O
structure	B
.	O
in	O
exercise	O
224.	O
write	O
a	O
program	O
to	O
ﬁt	O
a	O
mixture	O
of	O
lth	O
order	O
markov	O
models	O
.	O
exercise	O
225	O
.	O
1.	O
using	O
the	O
correspondence	O
a	O
=	O
1	O
,	O
c	O
=	O
2	O
,	O
g	O
=	O
3	O
,	O
t	O
=	O
4	O
deﬁne	O
a	O
4×	O
4	O
transition	B
matrix	I
p	O
that	O
produces	O
sequences	B
of	O
the	O
form	O
a	O
,	O
c	O
,	O
g	O
,	O
t	O
,	O
a	O
,	O
c	O
,	O
g	O
,	O
t	O
,	O
a	O
,	O
c	O
,	O
g	O
,	O
t	O
,	O
a	O
,	O
c	O
,	O
g	O
,	O
t	O
,	O
.	O
.	O
.	O
now	O
deﬁne	O
a	O
new	O
transition	B
matrix	I
pnew	O
=	O
0.9*p	O
+	O
0.1*ones	O
(	O
4	O
)	O
/4	O
deﬁne	O
a	O
4	O
×	O
4	O
transition	B
matrix	I
q	O
that	O
produces	O
sequences	B
of	O
the	O
form	O
t	O
,	O
g	O
,	O
c	O
,	O
a	O
,	O
t	O
,	O
g	O
,	O
c	O
,	O
a	O
,	O
t	O
,	O
g	O
,	O
c	O
,	O
a	O
,	O
t	O
,	O
g	O
,	O
c	O
,	O
a	O
,	O
.	O
.	O
.	O
now	O
deﬁne	O
a	O
new	O
transition	B
matrix	I
qnew	O
=	O
0.9*q	O
+	O
0.1*ones	O
(	O
4	O
)	O
/4	O
(	O
23.7.6	O
)	O
(	O
23.7.7	O
)	O
(	O
23.7.8	O
)	O
(	O
23.7.9	O
)	O
assume	O
that	O
the	O
probability	B
of	O
being	O
in	O
the	O
initial	O
state	O
of	O
the	O
markov	O
chain	B
p	O
(	O
h1	O
)	O
is	O
constant	O
for	O
all	O
four	O
states	O
a	O
,	O
c	O
,	O
g	O
,	O
t	O
.	O
what	O
is	O
the	O
probability	B
that	O
the	O
markov	O
chain	B
pnew	O
generated	O
the	O
sequence	O
s	O
given	O
by	O
s	O
≡	O
a	O
,	O
a	O
,	O
g	O
,	O
t	O
,	O
a	O
,	O
c	O
,	O
t	O
,	O
t	O
,	O
a	O
,	O
c	O
,	O
c	O
,	O
t	O
,	O
a	O
,	O
c	O
,	O
g	O
,	O
c	O
(	O
23.7.10	O
)	O
2.	O
similarly	O
what	O
is	O
the	O
probability	B
that	O
s	O
was	O
generated	O
by	O
qnew	O
?	O
does	O
it	O
make	O
sense	O
that	O
s	O
has	O
a	O
higher	O
likelihood	B
under	O
pnew	O
compared	O
with	O
qnew	O
?	O
draft	O
march	O
9	O
,	O
2010	O
433	O
3.	O
using	O
the	O
function	B
randgen.m	O
,	O
generate	O
100	O
sequences	B
of	O
length	O
16	O
from	O
the	O
markov	O
chain	B
deﬁned	O
by	O
pnew	O
.	O
similarly	O
,	O
generate	O
100	O
sequences	B
each	O
of	O
length	O
16	O
from	O
the	O
markov	O
chain	B
deﬁned	O
by	O
qnew	O
.	O
concatenate	O
all	O
these	O
sequences	B
into	O
a	O
cell	O
array	O
v	O
so	O
that	O
v	O
{	O
1	O
}	O
contains	O
the	O
ﬁrst	O
sequence	O
and	O
v	O
{	O
200	O
}	O
the	O
last	O
sequence	O
.	O
use	O
mixmarkov.m	O
to	O
learn	O
the	O
optimum	O
maximum	B
likelihood	I
parameters	O
that	O
generated	O
these	O
sequences	B
.	O
assume	O
that	O
there	O
are	O
h	O
=	O
2	O
kinds	O
of	O
markov	O
chain	B
.	O
the	O
result	O
returned	O
in	O
phgvn	O
indicate	O
the	O
posterior	B
probability	O
of	O
sequence	O
assignment	O
.	O
do	O
you	O
agree	O
with	O
the	O
solution	O
found	O
?	O
exercises	O
4.	O
take	O
the	O
sequence	O
s	O
as	O
deﬁned	O
in	O
equation	B
(	O
23.7.10	O
)	O
.	O
deﬁne	O
an	O
emission	B
distribution	I
that	O
has	O
4	O
output	O
states	O
such	O
that	O
p	O
(	O
v	O
=	O
i|h	O
=	O
j	O
)	O
=	O
(	O
cid:26	O
)	O
0.7	O
i	O
=	O
j	O
0.1	O
i	O
(	O
cid:54	O
)	O
=	O
j	O
using	O
this	O
emission	B
distribution	I
and	O
the	O
transition	O
given	O
by	O
pnew	O
deﬁned	O
in	O
equation	B
(	O
23.7.7	O
)	O
,	O
adapt	O
demohmminferencesimple.m	O
suitably	O
to	O
ﬁnd	O
the	O
most	O
likely	O
hidden	O
sequence	O
hp	O
1:16	O
that	O
generated	O
the	O
observed	O
sequence	O
s.	O
repeat	O
this	O
computation	O
but	O
for	O
the	O
transition	O
qnew	O
to	O
give	O
hq	O
1:16.	O
which	O
hidden	B
sequence	O
–	O
hp	O
1:16	O
is	O
to	O
be	O
preferred	O
?	O
justify	O
your	O
answer	O
.	O
1:16	O
or	O
hq	O
exercise	O
226.	O
derive	O
an	O
algorithm	B
that	O
will	O
ﬁnd	O
the	O
most	O
likely	O
joint	O
state	O
t	O
(	O
cid:89	O
)	O
t=2	O
t	O
(	O
cid:89	O
)	O
t=2	O
argmax	O
h1	O
:	O
t	O
φt	O
(	O
ht−1	O
,	O
ht	O
)	O
for	O
arbitrarily	O
deﬁned	O
potentials	O
φt	O
(	O
ht−1	O
,	O
ht	O
)	O
.	O
1.	O
first	O
consider	O
max	O
h1	O
:	O
t	O
φt	O
(	O
ht−1	O
,	O
ht	O
)	O
γt−1←t	O
(	O
ht−1	O
)	O
2.	O
derive	O
the	O
recursion	O
γt−1←t	O
(	O
ht−1	O
)	O
=	O
max	O
ht	O
φt	O
(	O
ht	O
,	O
ht−1	O
)	O
γt←t+1	O
(	O
ht	O
)	O
3.	O
explain	O
how	O
the	O
above	O
recursion	O
enables	O
the	O
computation	O
of	O
t	O
(	O
cid:89	O
)	O
t=2	O
argmax	O
h1	O
φt	O
(	O
ht	O
,	O
ht−1	O
)	O
show	O
that	O
how	O
the	O
maximisation	B
over	O
ht	O
may	O
be	O
pushed	O
inside	O
the	O
product	O
and	O
that	O
the	O
result	O
of	O
the	O
maximisation	B
can	O
be	O
interpreted	O
as	O
a	O
message	B
(	O
23.7.11	O
)	O
(	O
23.7.12	O
)	O
(	O
23.7.13	O
)	O
(	O
23.7.14	O
)	O
(	O
23.7.15	O
)	O
(	O
23.7.16	O
)	O
(	O
23.7.17	O
)	O
(	O
23.7.18	O
)	O
4.	O
explain	O
how	O
once	O
the	O
most	B
likely	I
state	I
for	O
h1	O
is	O
computed	O
,	O
one	O
may	O
eﬃciently	O
compute	O
the	O
remaining	O
optimal	O
states	O
h2	O
,	O
.	O
.	O
.	O
,	O
ht	O
.	O
exercise	O
227.	O
derive	O
an	O
algorithm	B
that	O
will	O
compute	O
pairwise	B
marginals	O
p	O
(	O
ht	O
,	O
ht−1	O
)	O
from	O
the	O
joint	B
distribution	O
t	O
(	O
cid:89	O
)	O
t=2	O
p	O
(	O
h1	O
:	O
t	O
)	O
∝	O
φt	O
(	O
ht−1	O
,	O
ht	O
)	O
for	O
arbitrarily	O
deﬁned	O
potentials	O
φt	O
(	O
ht−1	O
,	O
ht	O
)	O
.	O
434	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
1.	O
first	O
consider	O
(	O
cid:88	O
)	O
t	O
(	O
cid:89	O
)	O
h1	O
,	O
...	O
,	O
ht	O
t=2	O
φt	O
(	O
ht	O
,	O
ht−1	O
)	O
(	O
23.7.19	O
)	O
show	O
that	O
how	O
the	O
summation	O
over	O
h1	O
may	O
be	O
pushed	O
inside	O
the	O
product	O
and	O
that	O
the	O
result	O
of	O
the	O
maximisation	B
can	O
be	O
interpreted	O
as	O
a	O
message	B
2.	O
derive	O
the	O
recursion	O
φt	O
(	O
ht−1	O
,	O
ht	O
)	O
αt−2→t−1	O
(	O
ht−1	O
)	O
3.	O
similarly	O
,	O
show	O
that	O
one	O
can	O
push	O
the	O
summation	O
of	O
ht	O
inside	O
the	O
product	O
to	O
deﬁne	O
φ2	O
(	O
h1	O
,	O
h2	O
)	O
h1	O
ht−1	O
α1→2	O
(	O
h2	O
)	O
=	O
(	O
cid:88	O
)	O
αt−1→t	O
(	O
ht	O
)	O
=	O
(	O
cid:88	O
)	O
βt−1←t	O
(	O
ht−1	O
)	O
=	O
(	O
cid:88	O
)	O
βt←t+1	O
(	O
ht	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
ht+1	O
ht	O
p	O
(	O
ht	O
,	O
ht−1	O
)	O
∝	O
ht+1	O
φt	O
(	O
ht−1	O
,	O
ht	O
)	O
and	O
that	O
by	O
pushing	O
in	O
ht−1	O
etc	O
.	O
one	O
can	O
deﬁne	O
messages	O
φt+1	O
(	O
ht	O
,	O
ht+1	O
)	O
βt+1←t+2	O
(	O
ht+1	O
)	O
4.	O
show	O
that	O
αt−2→t−1	O
(	O
ht−1	O
)	O
φ	O
(	O
ht−1	O
,	O
ht	O
)	O
βt←t+1	O
(	O
ht	O
)	O
(	O
23.7.20	O
)	O
(	O
23.7.21	O
)	O
(	O
23.7.22	O
)	O
(	O
23.7.23	O
)	O
(	O
23.7.24	O
)	O
(	O
23.7.27	O
)	O
(	O
23.7.28	O
)	O
435	O
exercise	O
228.	O
a	O
second	O
order	O
hmm	O
is	O
deﬁned	O
as	O
phm	O
m	O
2	O
(	O
h1	O
:	O
t	O
,	O
v1	O
:	O
t	O
)	O
=	O
p	O
(	O
h1	O
)	O
p	O
(	O
v1|h1	O
)	O
p	O
(	O
h2|h1	O
)	O
p	O
(	O
v2|h2	O
)	O
t	O
(	O
cid:89	O
)	O
t=3	O
p	O
(	O
ht|ht−1	O
,	O
ht−2	O
)	O
p	O
(	O
vt|ht	O
)	O
(	O
23.7.25	O
)	O
following	O
a	O
similar	O
approach	B
to	O
the	O
ﬁrst	B
order	I
hmm	O
,	O
derive	O
explicitly	O
a	O
message	B
passing	I
algorithm	O
to	O
compute	O
the	O
most	O
likely	O
joint	O
state	O
argmax	O
h1	O
:	O
t	O
phm	O
m	O
2	O
(	O
h1	O
:	O
t|v1	O
:	O
t	O
)	O
(	O
23.7.26	O
)	O
exercise	O
229.	O
since	O
the	O
likelihood	B
of	O
the	O
hmm	O
can	O
be	O
computed	O
using	O
ﬁltering	B
only	O
,	O
in	O
principle	O
we	O
do	O
not	O
need	O
smoothing	B
to	O
maximise	O
the	O
likelihood	B
(	O
contrary	O
to	O
the	O
em	O
approach	B
)	O
.	O
explain	O
how	O
to	O
compute	O
the	O
likelihood	B
gradient	O
by	O
the	O
use	O
of	O
ﬁltered	O
information	O
alone	O
(	O
i.e	O
.	O
using	O
only	O
a	O
forward	O
pass	O
)	O
.	O
exercise	O
230.	O
derive	O
the	O
em	O
updates	O
for	O
ﬁtting	O
a	O
hmm	O
with	O
an	O
emission	B
distribution	I
given	O
by	O
a	O
mixture	O
of	O
multi-variate	O
gaussians	O
.	O
exercise	O
231.	O
consider	O
the	O
hmm	O
deﬁned	O
on	O
hidden	B
variables	I
h	O
=	O
{	O
h1	O
,	O
.	O
.	O
.	O
,	O
ht	O
}	O
and	O
observations	O
v	O
=	O
{	O
v1	O
,	O
.	O
.	O
.	O
,	O
vt	O
}	O
t	O
(	O
cid:89	O
)	O
p	O
(	O
v	O
,	O
h	O
)	O
=	O
p	O
(	O
h1	O
)	O
p	O
(	O
v1|h1	O
)	O
p	O
(	O
ht|ht−1	O
)	O
p	O
(	O
vt|ht	O
)	O
show	O
that	O
the	O
posterior	B
p	O
(	O
h|v	O
)	O
is	O
a	O
markov	O
chain	B
t=2	O
t	O
(	O
cid:89	O
)	O
t=2	O
p	O
(	O
h|v	O
)	O
=	O
˜p	O
(	O
h1	O
)	O
˜p	O
(	O
ht|ht−1	O
)	O
where	O
˜p	O
(	O
ht|ht−1	O
)	O
and	O
˜p	O
(	O
h1	O
)	O
are	O
suitably	O
deﬁned	O
distributions	O
.	O
draft	O
march	O
9	O
,	O
2010	O
exercise	O
232.	O
for	O
training	B
a	O
hmm	O
with	O
a	O
gaussian	O
mixture	B
emission	O
(	O
the	O
hmm-gmm	O
model	B
)	O
in	O
section	O
(	O
23.3.3	O
)	O
,	O
derive	O
the	O
following	O
em	O
update	O
formulae	O
for	O
the	O
means	O
and	O
covariances	O
:	O
n	O
(	O
cid:88	O
)	O
t	O
(	O
cid:88	O
)	O
n=1	O
t=1	O
n	O
(	O
cid:88	O
)	O
t	O
(	O
cid:88	O
)	O
n=1	O
t=1	O
µnew	O
k	O
,	O
h	O
=	O
µnew	O
k	O
,	O
h	O
=	O
ρk	O
,	O
h	O
(	O
t	O
,	O
n	O
)	O
=	O
(	O
cid:80	O
)	O
n	O
and	O
where	O
ρk	O
,	O
h	O
(	O
t	O
,	O
n	O
)	O
vn	O
t	O
ρk	O
,	O
h	O
(	O
t	O
,	O
n	O
)	O
(	O
cid:0	O
)	O
vn	O
(	O
cid:80	O
)	O
q	O
(	O
kt	O
=	O
k|hn	O
t	O
q	O
(	O
kt	O
=	O
k|hn	O
(	O
cid:1	O
)	O
(	O
cid:0	O
)	O
vn	O
t	O
−	O
µk	O
,	O
h	O
(	O
cid:1	O
)	O
t	O
t	O
−	O
µk	O
,	O
h	O
t	O
=	O
h	O
)	O
q	O
(	O
hn	O
t	O
=	O
h	O
)	O
q	O
(	O
hn	O
t	O
=	O
h|vn	O
1	O
:	O
t	O
)	O
t	O
=	O
h|vn	O
1	O
:	O
t	O
)	O
exercises	O
(	O
23.7.29	O
)	O
(	O
23.7.30	O
)	O
(	O
23.7.31	O
)	O
exercise	O
233.	O
consider	O
the	O
hmm	O
duration	B
model	I
deﬁned	O
by	O
equation	B
(	O
23.4.2	O
)	O
and	O
equation	B
(	O
23.4.1	O
)	O
with	O
emission	O
distribution	B
p	O
(	O
vt|ht	O
)	O
.	O
our	O
interest	O
is	O
to	O
derive	O
a	O
recursion	O
for	O
the	O
ﬁltered	O
distribution	B
αt	O
(	O
ht	O
,	O
ct	O
)	O
≡	O
p	O
(	O
ht	O
,	O
ct	O
,	O
v1	O
:	O
t	O
)	O
1.	O
show	O
that	O
:	O
αt	O
(	O
ht	O
,	O
ct	O
)	O
=	O
p	O
(	O
vt|ht	O
)	O
(	O
cid:88	O
)	O
ht−1	O
,	O
ct−1	O
p	O
(	O
ht|ht−1	O
,	O
ct	O
)	O
p	O
(	O
ct|ct−1	O
)	O
αt−1	O
(	O
ht−1	O
,	O
ct−1	O
)	O
2.	O
using	O
this	O
derive	O
=	O
(	O
cid:88	O
)	O
ht−1	O
αt	O
(	O
ht	O
,	O
ct	O
)	O
p	O
(	O
vt|ht	O
)	O
p	O
(	O
ht|ht−1	O
,	O
c	O
)	O
p	O
(	O
ct|ct−1	O
=	O
1	O
)	O
αt−1	O
(	O
ht−1	O
,	O
ct−1	O
=	O
1	O
)	O
dmax	O
(	O
cid:88	O
)	O
+	O
(	O
cid:88	O
)	O
p	O
(	O
ht|ht−1	O
,	O
c	O
)	O
ht−1	O
ct−1=2	O
p	O
(	O
c|ct−1	O
)	O
αt−1	O
(	O
ht−1	O
,	O
ct−1	O
)	O
(	O
23.7.32	O
)	O
(	O
23.7.33	O
)	O
(	O
23.7.34	O
)	O
3.	O
show	O
that	O
the	O
right	O
hand	O
side	O
of	O
the	O
above	O
can	O
be	O
written	O
as	O
(	O
cid:88	O
)	O
ht−1	O
p	O
(	O
ht|ht−1	O
,	O
ct	O
=	O
c	O
)	O
p	O
(	O
ct	O
=	O
c|ct−1	O
=	O
1	O
)	O
αt−1	O
(	O
ht−1	O
,	O
1	O
)	O
+	O
i	O
[	O
c	O
(	O
cid:54	O
)	O
=	O
dmax	O
]	O
(	O
cid:88	O
)	O
ht−1	O
p	O
(	O
ht|ht−1	O
,	O
c	O
)	O
αt−1	O
(	O
ht−1	O
,	O
c	O
+	O
1	O
)	O
(	O
23.7.35	O
)	O
4.	O
show	O
that	O
the	O
recursion	O
for	O
α	O
is	O
then	O
given	O
by	O
αt	O
(	O
h	O
,	O
1	O
)	O
=	O
p	O
(	O
vt|ht	O
=	O
h	O
)	O
pdur	O
(	O
1	O
)	O
(	O
cid:88	O
)	O
ht−1	O
ptran	O
(	O
h|ht−1	O
)	O
αt−1	O
(	O
ht−1	O
,	O
1	O
)	O
+	O
i	O
[	O
dmax	O
(	O
cid:54	O
)	O
=	O
1	O
]	O
p	O
(	O
vt|ht	O
=	O
h	O
)	O
(	O
cid:88	O
)	O
ht−1	O
ptran	O
(	O
ht|ht−1	O
)	O
αt−1	O
(	O
ht−1	O
,	O
2	O
)	O
(	O
23.7.36	O
)	O
and	O
for	O
c	O
>	O
1	O
αt	O
(	O
h	O
,	O
c	O
)	O
=	O
p	O
(	O
vt|ht	O
=	O
h	O
)	O
{	O
pdur	O
(	O
c	O
)	O
αt−1	O
(	O
h	O
,	O
1	O
)	O
+	O
i	O
[	O
c	O
(	O
cid:54	O
)	O
=	O
dmax	O
]	O
αt−1	O
(	O
h	O
,	O
c	O
+	O
1	O
)	O
}	O
5.	O
explain	O
why	O
the	O
computational	B
complexity	I
of	O
ﬁltered	O
inference	B
in	O
the	O
duration	B
model	I
is	O
o	O
(	O
cid:0	O
)	O
t	O
h	O
2dmax	O
(	O
23.7.37	O
)	O
(	O
cid:1	O
)	O
.	O
6.	O
derive	O
an	O
eﬃcient	B
smoothing	O
algorithm	B
for	O
this	O
duration	B
model	I
.	O
436	O
draft	O
march	O
9	O
,	O
2010	O
chapter	O
24	O
continuous-state	O
markov	O
models	O
24.1	O
observed	O
linear	O
dynamical	O
systems	O
in	O
many	O
practical	O
timeseries	O
applications	O
the	O
data	B
is	O
naturally	O
continuous	B
,	O
particularly	O
for	O
models	O
of	O
the	O
physical	O
environment	O
.	O
in	O
contrast	O
to	O
discrete-state	O
markov	O
models	O
,	O
chapter	O
(	O
23	O
)	O
,	O
continuous	B
state	O
distributions	O
are	O
not	O
automatically	O
closed	O
under	O
operations	O
such	O
as	O
products	O
and	O
marginalisation	B
.	O
to	O
make	O
practical	O
algorithms	O
for	O
which	O
inference	B
and	O
learning	B
can	O
be	O
carried	O
eﬃciently	O
,	O
we	O
therefore	O
are	O
heavily	O
restricted	B
in	O
the	O
form	O
of	O
the	O
continuous	B
transition	O
p	O
(	O
vt|vt−1	O
)	O
.	O
a	O
simple	O
yet	O
powerful	O
class	O
of	O
such	O
transitions	O
are	O
the	O
linear	O
dynamical	O
systems	O
.	O
a	O
deterministic	B
observed	O
linear	O
dynamical	O
system1	O
(	O
olds	O
)	O
deﬁnes	O
the	O
temporal	O
evolution	O
of	O
a	O
vector	O
vt	O
according	O
to	O
the	O
discrete-time	O
update	O
equation	B
vt	O
=	O
atvt−1	O
(	O
24.1.1	O
)	O
where	O
at	O
is	O
the	O
transition	B
matrix	I
at	O
time	O
t.	O
for	O
the	O
case	O
that	O
at	O
is	O
invariant	O
with	O
t	O
,	O
the	O
process	O
is	O
called	O
time-invariant	O
,	O
which	O
we	O
assume	O
throughout	O
unless	O
explicitly	O
stated	O
otherwise	O
.	O
a	O
motivation	O
for	O
studying	O
oldss	O
is	O
that	O
many	O
equations	O
that	O
describe	O
the	O
physical	O
world	O
can	O
be	O
written	O
as	O
an	O
olds	O
.	O
oldss	O
are	O
interesting	O
since	O
they	O
may	O
be	O
used	O
as	O
simple	O
prediction	B
models	O
:	O
if	O
vt	O
describes	O
the	O
state	O
of	O
the	O
environment	O
at	O
time	O
t	O
,	O
then	O
avt	O
predicts	O
the	O
environment	O
at	O
time	O
t+1	O
.	O
as	O
such	O
,	O
these	O
models	O
,	O
have	O
widespread	O
application	O
in	O
many	O
branches	O
of	O
science	O
,	O
from	O
engineering	O
and	O
physics	O
to	O
economics	O
.	O
the	O
olds	O
equation	B
(	O
24.1.1	O
)	O
is	O
deterministic	B
so	O
that	O
if	O
we	O
specify	O
v1	O
,	O
all	O
future	O
values	O
v2	O
,	O
v3	O
,	O
.	O
.	O
.	O
,	O
are	O
deﬁned	O
.	O
for	O
a	O
dim	O
v	O
=	O
v	O
dimensional	O
vector	O
,	O
its	O
evolution	O
is	O
described	O
by	O
vt	O
=	O
at−1v1	O
=	O
pλt−1p−1v1	O
(	O
24.1.2	O
)	O
where	O
λ	O
=	O
diag	O
(	O
λ1	O
,	O
.	O
.	O
.	O
,	O
λv	O
)	O
,	O
is	O
the	O
diagonal	O
eigenvalue	O
matrix	B
,	O
and	O
p	O
is	O
the	O
corresponding	O
eigenvector	O
matrix	B
of	O
a.	O
if	O
λi	O
>	O
1	O
then	O
for	O
large	O
t	O
,	O
vt	O
will	O
explode	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
λi	O
<	O
1	O
,	O
then	O
λt−1	O
i	O
will	O
tend	O
to	O
zero	O
.	O
for	O
stable	O
systems	O
we	O
require	O
therefore	O
no	O
eigenvalues	O
of	O
magnitude	O
greater	O
than	O
1	O
and	O
only	O
unit	O
eigenvalues	O
will	O
contribute	O
in	O
long	O
term	O
.	O
note	O
that	O
the	O
eigenvalues	O
may	O
be	O
complex	O
which	O
corresponds	O
to	O
rotational	O
behaviour	O
,	O
see	O
exercise	O
(	O
234	O
)	O
.	O
more	O
generally	O
,	O
we	O
may	O
consider	O
additive	O
noise	O
on	O
v	O
and	O
deﬁne	O
a	O
stochastic	O
olds	O
.	O
deﬁnition	O
111	O
(	O
observed	B
linear	I
dynamical	I
system	I
)	O
.	O
vt	O
=	O
atvt−1	O
+	O
ηt	O
(	O
24.1.3	O
)	O
1we	O
use	O
the	O
terminology	O
‘	O
observed	O
’	O
lds	O
to	O
diﬀerentiate	O
from	O
the	O
more	O
general	O
lds	O
state-space	O
model	B
.	O
in	O
some	O
texts	O
,	O
however	O
,	O
the	O
term	O
lds	O
is	O
applied	O
to	O
the	O
models	O
under	O
discussion	O
in	O
this	O
chapter	O
.	O
437	O
where	O
ηt	O
is	O
a	O
noise	O
vector	O
sampled	O
from	O
a	O
gaussian	O
distribution	B
,	O
n	O
(	O
ηt	O
µt	O
,	O
σt	O
)	O
this	O
is	O
equivalent	B
to	O
a	O
ﬁrst	B
order	I
markov	O
model	B
p	O
(	O
vt|vt−1	O
)	O
=	O
n	O
(	O
vt	O
atvt−1	O
+	O
µt	O
,	O
σt	O
)	O
auto-regressive	B
models	O
(	O
24.1.4	O
)	O
(	O
24.1.5	O
)	O
at	O
t	O
=	O
1	O
we	O
have	O
an	O
initial	O
distribution	B
p	O
(	O
v1	O
)	O
=	O
n	O
(	O
v1	O
µ1	O
,	O
σ1	O
)	O
.	O
for	O
t	O
>	O
1	O
if	O
the	O
parameters	O
are	O
time-	O
independent	O
,	O
µt	O
≡	O
µ	O
,	O
at	O
≡	O
a	O
,	O
σt	O
≡	O
σ	O
,	O
the	O
process	O
is	O
called	O
time-invariant	O
.	O
24.1.1	O
stationary	B
distribution	I
with	O
noise	O
consider	O
the	O
one-dimensional	O
linear	B
system	O
(	O
cid:0	O
)	O
ηt	O
0	O
,	O
σ2	O
v	O
(	O
cid:1	O
)	O
vt	O
=	O
avt−1	O
+	O
ηt	O
,	O
ηt	O
∼	O
n	O
if	O
we	O
start	O
at	O
some	O
state	O
v1	O
,	O
and	O
then	O
for	O
t	O
>	O
1	O
recursively	O
sample	B
according	O
to	O
vt	O
=	O
avt−1	O
+	O
ηt	O
,	O
does	O
the	O
distribution	B
of	O
the	O
vt	O
,	O
t	O
(	O
cid:29	O
)	O
1	O
tend	O
to	O
a	O
steady	O
,	O
ﬁxed	O
distribution	B
?	O
assuming	O
that	O
we	O
can	O
represent	O
the	O
distribution	B
of	O
vt−1	O
as	O
a	O
gaussian	O
with	O
mean	O
µt−1	O
and	O
variance	B
σ2	O
using	O
(	O
cid:104	O
)	O
ηt	O
(	O
cid:105	O
)	O
=	O
0	O
we	O
have	O
t−1	O
,	O
vt−1	O
∼	O
n	O
(	O
cid:0	O
)	O
vt−1	O
µt−1	O
,	O
σ2	O
(	O
cid:1	O
)	O
,	O
then	O
t−1	O
(	O
cid:11	O
)	O
+	O
2	O
(	O
cid:104	O
)	O
vt−1	O
(	O
cid:105	O
)	O
(	O
cid:104	O
)	O
ηt	O
(	O
cid:105	O
)	O
+	O
(	O
cid:10	O
)	O
η2	O
t	O
(	O
cid:11	O
)	O
(	O
cid:10	O
)	O
v2	O
t	O
⇒	O
σ2	O
(	O
cid:104	O
)	O
vt	O
(	O
cid:105	O
)	O
=	O
a	O
(	O
cid:104	O
)	O
vt−1	O
(	O
cid:105	O
)	O
+	O
(	O
cid:104	O
)	O
ηt	O
(	O
cid:105	O
)	O
⇒	O
µt	O
=	O
aµt−1	O
(	O
cid:11	O
)	O
=	O
(	O
cid:104	O
)	O
avt−1	O
+	O
ηt	O
(	O
cid:105	O
)	O
2	O
=	O
a2	O
(	O
cid:10	O
)	O
v2	O
(	O
cid:0	O
)	O
vt	O
aµt−1	O
,	O
a2σ2	O
(	O
cid:1	O
)	O
t	O
=	O
a2σ2	O
t−1	O
+	O
σ2	O
v	O
t−1	O
+	O
σ2	O
v	O
t−1	O
vt	O
∼	O
n	O
so	O
that	O
(	O
24.1.6	O
)	O
(	O
24.1.7	O
)	O
(	O
24.1.8	O
)	O
(	O
24.1.9	O
)	O
(	O
24.1.10	O
)	O
(	O
24.1.11	O
)	O
(	O
24.1.12	O
)	O
(	O
24.2.1	O
)	O
assuming	O
there	O
is	O
a	O
ﬁxed	O
variance	B
σ2∞	O
for	O
the	O
inﬁnite	O
time	O
case	O
,	O
the	O
stationary	B
distribution	I
satisﬁes	O
σ2∞	O
=	O
a2σ2∞	O
+	O
σ2	O
v	O
⇒	O
σ2∞	O
=	O
σ2	O
1	O
−	O
a2	O
v	O
similarly	O
,	O
the	O
mean	B
is	O
given	O
by	O
µ∞	O
=	O
a∞µ1	O
.	O
if	O
a	O
≥	O
1	O
the	O
variance	B
(	O
and	O
mean	B
)	O
increases	O
indeﬁnitely	O
with	O
t.	O
for	O
a	O
<	O
1	O
,	O
the	O
mean	B
tends	O
to	O
zero	O
yet	O
the	O
variance	B
remains	O
ﬁnite	O
.	O
even	O
though	O
the	O
magnitude	O
of	O
vt−1	O
is	O
decreased	O
by	O
a	O
factor	B
of	O
a	O
at	O
each	O
iteration	B
,	O
the	O
additive	O
noise	O
on	O
average	B
boosts	O
the	O
magnitude	O
so	O
that	O
it	O
remains	O
steady	O
in	O
the	O
long	O
run	O
.	O
more	O
generally	O
for	O
a	O
system	B
updating	O
a	O
vector	O
vt	O
according	O
to	O
vt	O
=	O
avt−1	O
+	O
ηt	O
for	O
the	O
existence	O
of	O
a	O
steady	O
state	O
we	O
require	O
that	O
all	O
eigenvalues	O
of	O
a	O
must	O
be	O
≤	O
1	O
.	O
24.2	O
auto-regressive	B
models	O
a	O
scalar	O
time-invariant	O
auto-regressive	B
model	I
is	O
deﬁned	O
by	O
vt	O
=	O
alvt−l	O
+	O
ηt	O
,	O
ηt	O
∼	O
n	O
(	O
cid:0	O
)	O
ηt	O
µ	O
,	O
σ2	O
(	O
cid:1	O
)	O
where	O
a	O
=	O
(	O
a1	O
,	O
.	O
.	O
.	O
,	O
al	O
)	O
t	O
are	O
called	O
the	O
ar	O
coeﬃcients	O
and	O
σ2	O
is	O
called	O
the	O
innovation	B
noise	I
.	O
the	O
model	B
predicts	O
the	O
future	O
based	O
on	O
a	O
linear	B
combination	O
of	O
the	O
previous	O
l	O
observations	O
.	O
as	O
a	O
belief	B
network	I
,	O
the	O
ar	O
model	B
can	O
be	O
written	O
as	O
an	O
lth	O
order	O
markov	O
model	B
:	O
p	O
(	O
v1	O
:	O
t	O
)	O
=	O
t=1	O
438	O
p	O
(	O
vt|vt−1	O
,	O
.	O
.	O
.	O
,	O
vt−l	O
)	O
,	O
with	O
vi	O
=	O
∅	O
for	O
i	O
≤	O
0	O
(	O
24.2.2	O
)	O
draft	O
march	O
9	O
,	O
2010	O
l	O
(	O
cid:88	O
)	O
l=1	O
t	O
(	O
cid:89	O
)	O
auto-regressive	B
models	O
figure	O
24.1	O
:	O
fitting	O
an	O
order	O
3	O
ar	O
model	B
to	O
the	O
training	B
points	O
.	O
the	O
x	O
axis	O
represents	O
time	O
,	O
and	O
the	O
y	O
axis	O
the	O
value	B
of	O
the	O
timeseries	O
.	O
the	O
solid	O
line	O
is	O
the	O
mean	B
prediction	O
and	O
the	O
dashed	O
lines	O
±	O
one	O
standard	B
deviation	I
.	O
see	O
demoartrain.m	O
with	O
(	O
cid:32	O
)	O
p	O
(	O
vt|vt−1	O
,	O
.	O
.	O
.	O
,	O
vt−l	O
)	O
=	O
n	O
vt	O
(	O
cid:33	O
)	O
alvt−l	O
,	O
σ2	O
l	O
(	O
cid:88	O
)	O
l=1	O
introducing	O
the	O
vector	O
of	O
the	O
l	O
previous	O
observations	O
ˆvt−1	O
≡	O
[	O
vt−1	O
,	O
vt−2	O
,	O
.	O
.	O
.	O
,	O
vt−l	O
]	O
t	O
(	O
cid:16	O
)	O
we	O
can	O
write	O
more	O
compactly	O
p	O
(	O
vt|vt−1	O
,	O
.	O
.	O
.	O
,	O
vt−l	O
)	O
=	O
n	O
vt	O
atˆvt−1	O
,	O
σ2	O
(	O
cid:17	O
)	O
(	O
24.2.3	O
)	O
(	O
24.2.4	O
)	O
(	O
24.2.5	O
)	O
ar	O
models	O
are	O
heavily	O
used	O
in	O
ﬁnancial	B
time-series	O
prediction	B
(	O
see	O
for	O
example	O
[	O
272	O
]	O
)	O
,	O
being	O
able	O
to	O
capture	O
simple	O
trends	O
in	O
the	O
data	B
.	O
another	O
common	O
application	O
area	O
is	O
in	O
speech	O
processing	O
whereby	O
for	O
a	O
one-	O
dimensional	O
speech	O
signal	O
partitioned	B
into	O
windows	O
of	O
length	O
t	O
,	O
the	O
ar	O
coeﬃcients	O
best	O
able	O
to	O
describe	O
the	O
signal	O
in	O
each	O
window	O
are	O
found	O
[	O
215	O
]	O
.	O
these	O
ar	O
coeﬃcients	O
then	O
form	O
a	O
compressed	O
representation	B
of	O
the	O
signal	O
and	O
subsequently	O
transmitted	O
for	O
each	O
window	O
,	O
rather	O
than	O
the	O
original	O
signal	O
itself	O
.	O
the	O
signal	O
can	O
then	O
be	O
approximately	O
reconstructed	O
based	O
on	O
the	O
ar	O
coeﬃcients	O
.	O
such	O
a	O
representation	B
is	O
used	O
for	O
example	O
in	O
telephones	O
and	O
known	O
as	O
a	O
linear	B
predictive	O
vocoder	O
[	O
257	O
]	O
.	O
24.2.1	O
training	B
an	O
ar	O
model	B
maximum	O
likelihood	B
training	O
of	O
the	O
ar	O
coeﬃcients	O
is	O
straightforward	O
based	O
on	O
log	O
p	O
(	O
v1	O
:	O
t	O
)	O
=	O
vt	O
−	O
ˆvt	O
diﬀerentiating	O
w.r.t	O
.	O
a	O
and	O
equating	O
to	O
zero	O
we	O
arrive	O
at	O
log	O
p	O
(	O
vt|ˆvt−1	O
)	O
=	O
−	O
t=1	O
t−1a	O
t	O
(	O
cid:88	O
)	O
(	O
cid:16	O
)	O
1	O
2σ2	O
(	O
cid:17	O
)	O
2	O
t	O
2	O
−	O
log	O
(	O
2πσ2	O
)	O
t	O
(	O
cid:88	O
)	O
(	O
cid:17	O
)	O
t=1	O
(	O
cid:88	O
)	O
(	O
cid:16	O
)	O
(	O
cid:32	O
)	O
(	O
cid:88	O
)	O
t	O
(	O
cid:88	O
)	O
t	O
1	O
t	O
t=1	O
(	O
cid:16	O
)	O
t−1a	O
ˆvt−1	O
=	O
0	O
vt	O
−	O
ˆvt	O
so	O
that	O
optimally	O
t	O
a	O
=	O
ˆvt−1ˆvt	O
t−1	O
vtˆvt−1	O
(	O
cid:33	O
)	O
−1	O
(	O
cid:88	O
)	O
(	O
cid:17	O
)	O
2	O
t	O
σ2	O
=	O
vt	O
−	O
ˆvt	O
t−1a	O
(	O
24.2.6	O
)	O
(	O
24.2.7	O
)	O
(	O
24.2.8	O
)	O
(	O
24.2.9	O
)	O
these	O
equations	O
can	O
be	O
solved	O
by	O
gaussian	O
elimination	O
.	O
similarly	O
,	O
optimally	O
,	O
above	O
we	O
assume	O
that	O
‘	O
negative	O
’	O
timepoints	O
are	O
available	O
in	O
order	O
to	O
keep	O
the	O
notation	O
simple	O
.	O
if	O
times	O
before	O
the	O
window	O
over	O
which	O
we	O
learn	O
the	O
coeﬃcients	O
are	O
not	O
available	O
,	O
a	O
minor	O
adjustment	O
is	O
required	O
to	O
start	O
the	O
summations	O
from	O
t	O
=	O
l	O
+	O
1.	O
given	O
a	O
trained	O
a	O
,	O
future	O
predictions	O
can	O
be	O
made	O
using	O
vt+1	O
=	O
ˆvt	O
capturing	O
the	O
trend	O
in	O
the	O
data	B
.	O
t	O
a.	O
as	O
we	O
see	O
,	O
the	O
model	B
is	O
capable	O
of	O
draft	O
march	O
9	O
,	O
2010	O
439	O
020406080100120140−50050100150200	O
auto-regressive	B
models	O
a1	O
v1	O
a2	O
v2	O
a3	O
v3	O
a4	O
v4	O
figure	O
24.2	O
:	O
a	O
time-varying	B
ar	O
model	B
as	O
a	O
latent	B
lds	O
.	O
since	O
the	O
observations	O
are	O
known	O
,	O
this	O
model	B
is	O
a	O
time-varying	B
latent	O
lds	O
,	O
for	O
which	O
smoothed	O
infer-	O
ence	O
determines	O
the	O
time-varying	B
ar	O
coeﬃcients	O
.	O
24.2.2	O
ar	O
model	B
as	O
an	O
olds	O
we	O
can	O
write	O
equation	B
(	O
24.2.1	O
)	O
as	O
an	O
olds	O
using	O
	O
vt	O
vt−1	O
...	O
vt−l+1	O
	O
=	O
	O
a1	O
1	O
...	O
0	O
	O
	O
vt−1	O
vt−2	O
...	O
vt−l	O
	O
+	O
	O
ηt	O
0	O
...	O
0	O
	O
a2	O
0	O
1	O
.	O
.	O
.	O
.	O
.	O
.	O
al	O
0	O
.	O
.	O
.	O
0	O
0	O
.	O
.	O
.	O
1	O
we	O
can	O
write	O
equation	B
(	O
24.2.1	O
)	O
as	O
the	O
olds	O
ηt	O
∼	O
n	O
(	O
ηt	O
0	O
,	O
σ	O
)	O
ˆvt	O
=	O
aˆvt−1	O
+	O
ηt	O
,	O
where	O
we	O
deﬁne	O
the	O
block	O
matrices	O
(	O
cid:18	O
)	O
a1	O
:	O
l−1	O
al	O
(	O
cid:19	O
)	O
i	O
0	O
a	O
=	O
(	O
cid:18	O
)	O
,	O
σ	O
=	O
σ2	O
01,1	O
:	O
l−1	O
01	O
:	O
l−1,1	O
01	O
:	O
l−1,1	O
:	O
l−1	O
(	O
cid:19	O
)	O
(	O
24.2.10	O
)	O
(	O
24.2.11	O
)	O
(	O
24.2.12	O
)	O
in	O
this	O
representation	B
,	O
the	O
ﬁrst	O
component	O
of	O
the	O
vector	O
is	O
updated	O
according	O
to	O
the	O
standard	O
ar	O
model	B
,	O
with	O
the	O
remaining	O
components	O
being	O
copies	O
of	O
the	O
previous	O
values	O
.	O
24.2.3	O
time-varying	B
ar	O
model	B
an	O
alternative	O
to	O
maximum	B
likelihood	I
is	O
to	O
view	O
learning	B
the	O
ar	O
coeﬃcients	O
as	O
a	O
problem	B
in	O
inference	B
in	O
a	O
latent	B
lds	O
,	O
a	O
model	B
which	O
is	O
discussed	O
in	O
detail	O
in	O
section	O
(	O
24.3	O
)	O
.	O
if	O
at	O
are	O
the	O
latent	B
ar	O
coeﬃcients	O
,	O
the	O
term	O
vt	O
=	O
ˆvt	O
t−1at	O
+	O
ηt	O
,	O
ηt	O
∼	O
n	O
can	O
be	O
viewed	O
as	O
the	O
emission	B
distribution	I
of	O
a	O
latent	B
lds	O
in	O
which	O
the	O
hidden	B
variable	I
is	O
at	O
and	O
the	O
time-dependent	O
emission	B
matrix	I
is	O
given	O
by	O
ˆvt	O
t−1	O
.	O
by	O
placing	O
a	O
simple	O
latent	O
transition	O
(	O
cid:0	O
)	O
ηt	O
0	O
,	O
σ2	O
(	O
cid:1	O
)	O
ai	O
(	O
cid:1	O
)	O
(	O
cid:0	O
)	O
ηa	O
t	O
0	O
,	O
σ2	O
(	O
24.2.13	O
)	O
(	O
24.2.14	O
)	O
(	O
24.2.15	O
)	O
we	O
encourage	O
the	O
ar	O
coeﬃcients	O
to	O
change	O
slowly	O
with	O
time	O
.	O
this	O
deﬁnes	O
a	O
model	B
ηa	O
t	O
∼	O
n	O
at	O
=	O
at−1	O
+	O
ηa	O
t	O
,	O
p	O
(	O
v1	O
:	O
t	O
,	O
a1	O
:	O
t	O
)	O
=	O
(	O
cid:89	O
)	O
p	O
(	O
vt|at	O
,	O
ˆvt−1	O
)	O
p	O
(	O
at|at−1	O
)	O
t	O
our	O
interest	O
is	O
then	O
in	O
the	O
conditional	B
p	O
(	O
a1	O
:	O
t|v1	O
:	O
t	O
)	O
from	O
which	O
we	O
can	O
compute	O
the	O
a-posteriori	O
most	O
likely	O
sequence	O
of	O
ar	O
coeﬃcients	O
.	O
standard	O
smoothing	O
algorithms	O
can	O
then	O
be	O
applied	O
to	O
yield	O
the	O
time-varying	B
ar	O
coeﬃcients	O
,	O
see	O
demoarlds.m	O
.	O
n−1	O
(	O
cid:88	O
)	O
n=0	O
deﬁnition	O
112	O
(	O
discrete	B
fourier	O
transform	O
)	O
.	O
for	O
a	O
sequence	O
x0	O
:	O
n−1	O
the	O
dft	O
f0	O
:	O
n−1	O
is	O
deﬁned	O
as	O
fk	O
=	O
−	O
2πi	O
n	O
kn	O
,	O
xne	O
k	O
=	O
0	O
,	O
.	O
.	O
.	O
,	O
n	O
−	O
1	O
(	O
24.2.16	O
)	O
fk	O
is	O
a	O
(	O
complex	O
)	O
representation	B
as	O
to	O
how	O
much	O
frequency	O
k	O
is	O
present	O
in	O
the	O
sequence	O
x0	O
:	O
n−1	O
.	O
the	O
power	O
of	O
component	O
k	O
is	O
deﬁned	O
as	O
the	O
absolute	O
length	O
of	O
the	O
complex	O
fk	O
.	O
440	O
draft	O
march	O
9	O
,	O
2010	O
auto-regressive	B
models	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
(	O
d	O
)	O
(	O
e	O
)	O
(	O
b	O
)	O
:	O
spectrogram	B
of	O
(	O
a	O
)	O
up	O
to	O
20,000	O
hz	O
.	O
figure	O
24.3	O
:	O
(	O
a	O
)	O
:	O
the	O
raw	O
recording	O
of	O
5	O
seconds	O
of	O
a	O
nightingale	O
song	O
(	O
with	O
additional	O
background	O
(	O
c	O
)	O
:	O
clustering	B
of	O
the	O
results	O
in	O
panel	O
(	O
b	O
)	O
using	O
birdsong	O
)	O
.	O
an	O
8	O
component	O
gaussian	O
mixture	B
model	I
.	O
the	O
index	O
(	O
from	O
1	O
to	O
8	O
)	O
of	O
the	O
component	O
most	O
probably	O
responsible	O
for	O
the	O
observation	O
is	O
indicated	O
vertically	O
in	O
black	O
.	O
(	O
d	O
)	O
:	O
the	O
20	O
ar	O
coeﬃcients	O
learned	O
using	O
(	O
e	O
)	O
:	O
clustering	B
the	O
results	O
in	O
panel	O
(	O
d	O
)	O
using	O
a	O
gaussian	O
mixture	B
v	O
=	O
0.001	O
,	O
σ2	O
σ2	O
model	B
with	O
8	O
components	O
.	O
the	O
ar	O
components	O
group	O
roughly	O
according	O
to	O
the	O
diﬀerent	O
song	O
regimes	O
.	O
h	O
=	O
0.001	O
,	O
see	O
arlds.m	O
.	O
deﬁnition	O
113	O
(	O
spectrogram	B
)	O
.	O
given	O
a	O
timeseries	O
x1	O
:	O
t	O
the	O
spectrogram	B
at	O
time	O
t	O
is	O
a	O
representation	B
of	O
the	O
frequencies	O
present	O
in	O
a	O
window	O
localised	O
around	O
t.	O
for	O
each	O
window	O
one	O
computes	O
the	O
discrete	B
fourier	O
transform	O
,	O
from	O
which	O
we	O
obtain	O
a	O
vector	O
of	O
log	O
power	O
in	O
each	O
frequency	O
.	O
the	O
window	O
is	O
then	O
moved	O
(	O
usually	O
)	O
one	O
step	O
forward	O
and	O
the	O
dft	O
recomputed	O
.	O
note	O
that	O
by	O
taking	O
the	O
logarithm	O
,	O
small	O
values	O
in	O
the	O
original	O
signal	O
can	O
translate	O
to	O
visibly	O
appreciable	O
values	O
in	O
the	O
spectrogram	B
.	O
example	O
100	O
(	O
nightingale	O
)	O
.	O
in	O
ﬁg	O
(	O
24.3a	O
)	O
we	O
plot	O
the	O
raw	O
acoustic	O
recording	O
for	O
a	O
5	O
second	O
fragment	O
of	O
a	O
nightingale	O
song	O
freesound.org/samplesviewsingle.php	O
?	O
id=17185	O
.	O
the	O
spectrogram	B
is	O
also	O
plotted	O
and	O
gives	O
an	O
indication	O
of	O
which	O
frequencies	O
are	O
present	O
in	O
the	O
signal	O
as	O
a	O
function	B
of	O
time	O
.	O
the	O
nightingale	O
song	O
is	O
very	O
complicated	O
but	O
at	O
least	O
locally	O
can	O
be	O
very	O
repetitive	O
.	O
a	O
crude	O
way	O
to	O
ﬁnd	O
which	O
segments	O
repeat	O
is	O
to	O
form	O
a	O
cluster	O
analysis	B
of	O
the	O
spectrogram	B
.	O
in	O
ﬁg	O
(	O
24.3c	O
)	O
we	O
show	O
the	O
results	O
of	O
ﬁtting	O
a	O
gaussian	O
mixture	B
model	I
,	O
section	O
(	O
20.3	O
)	O
,	O
with	O
8	O
components	O
,	O
from	O
which	O
we	O
see	O
there	O
is	O
some	O
repetition	O
of	O
components	O
locally	O
in	O
time	O
.	O
an	O
alternative	O
representation	B
of	O
the	O
signal	O
is	O
given	O
by	O
the	O
time-varying	B
ar	O
coeﬃcients	O
,	O
section	O
(	O
24.2.3	O
)	O
,	O
as	O
plotted	O
in	O
ﬁg	O
(	O
24.3d	O
)	O
.	O
a	O
gmm	O
clustering	B
with	O
8	O
components	O
ﬁg	O
(	O
24.3e	O
)	O
draft	O
march	O
9	O
,	O
2010	O
441	O
latent	O
linear	O
dynamical	O
systems	O
h1	O
v1	O
h2	O
v2	O
h3	O
v3	O
h4	O
v4	O
figure	O
24.4	O
:	O
a	O
(	O
latent	B
)	O
lds	O
.	O
both	O
hidden	B
and	O
visible	B
variables	O
are	O
gaussian	O
distributed	O
.	O
in	O
this	O
case	O
produces	O
a	O
somewhat	O
clearer	O
depiction	O
of	O
the	O
diﬀerent	O
phases	O
of	O
the	O
nightingale	O
singing	O
than	O
that	O
aﬀorded	O
by	O
the	O
spectrogram	B
.	O
24.3	O
latent	O
linear	O
dynamical	O
systems	O
the	O
latent	B
lds	O
deﬁnes	O
a	O
stochastic	O
linear	B
dynamical	I
system	I
in	O
a	O
latent	B
(	O
or	O
‘	O
hidden	B
’	O
)	O
space	O
on	O
a	O
sequence	O
of	O
vectors	O
h1	O
:	O
t	O
.	O
each	O
observation	O
vt	O
is	O
as	O
linear	O
function	B
of	O
the	O
latent	B
vector	O
ht	O
.	O
this	O
model	B
is	O
also	O
called	O
a	O
linear	B
gaussian	O
state	O
space	O
model	B
2.	O
the	O
model	B
can	O
also	O
be	O
considered	O
a	O
form	O
of	O
lds	O
on	O
the	O
joint	B
variables	O
xt	O
=	O
(	O
vt	O
,	O
ht	O
)	O
,	O
with	O
parts	O
of	O
the	O
vector	O
xt	O
missing	B
.	O
for	O
this	O
reason	O
we	O
will	O
also	O
refer	O
to	O
this	O
model	B
as	O
a	O
linear	B
dynamical	I
system	I
(	O
without	O
the	O
‘	O
latent	B
’	O
preﬁx	O
)	O
.	O
t	O
are	O
noise	O
vectors	O
.	O
at	O
is	O
called	O
the	O
transition	B
matrix	I
and	O
bt	O
the	O
emission	B
matrix	I
.	O
the	O
where	O
ηh	O
terms	O
¯ht	O
and	O
¯vt	O
are	O
the	O
hidden	B
and	O
output	O
bias	B
respectively	O
.	O
the	O
transition	O
and	O
emission	O
models	O
deﬁne	O
a	O
ﬁrst	B
order	I
markov	O
model	B
deﬁnition	O
114	O
(	O
latent	O
linear	O
dynamical	O
system	B
)	O
.	O
(	O
cid:0	O
)	O
ηh	O
(	O
cid:0	O
)	O
ηv	O
¯ht	O
,	O
σh	O
t	O
t	O
t	O
¯vt	O
,	O
σv	O
t	O
(	O
cid:1	O
)	O
transition	O
model	O
(	O
cid:1	O
)	O
emission	O
model	O
ht	O
=	O
atht−1	O
+	O
ηh	O
vt	O
=	O
btht	O
+	O
ηv	O
t	O
t	O
and	O
ηv	O
t	O
ηh	O
t	O
∼	O
n	O
ηv	O
t	O
∼	O
n	O
t	O
(	O
cid:89	O
)	O
(	O
cid:0	O
)	O
ht	O
atht−1	O
+	O
¯ht	O
,	O
σh	O
(	O
cid:0	O
)	O
vt	O
btht	O
+	O
¯vt	O
,	O
σv	O
(	O
cid:1	O
)	O
t=2	O
t	O
t	O
(	O
cid:1	O
)	O
,	O
p	O
(	O
h1	O
:	O
t	O
,	O
v1	O
:	O
t	O
)	O
=	O
p	O
(	O
h1	O
)	O
p	O
(	O
v1|h1	O
)	O
p	O
(	O
ht|ht−1	O
)	O
p	O
(	O
vt|ht	O
)	O
with	O
the	O
transitions	O
and	O
emissions	O
given	O
by	O
gaussian	O
distributions	O
p	O
(	O
ht|ht−1	O
)	O
=	O
n	O
p	O
(	O
vt|ht	O
)	O
=	O
n	O
p	O
(	O
h1	O
)	O
=	O
n	O
(	O
h1	O
µπ	O
,	O
σπ	O
)	O
(	O
24.3.1	O
)	O
(	O
24.3.2	O
)	O
(	O
24.3.3	O
)	O
(	O
24.3.4	O
)	O
(	O
24.3.5	O
)	O
(	O
24.3.6	O
)	O
this	O
(	O
latent	B
)	O
lds	O
can	O
be	O
represented	O
as	O
a	O
belief	O
networkin	O
ﬁg	O
(	O
24.4	O
)	O
with	O
the	O
extension	O
to	O
higher	O
orders	O
being	O
intuitive	O
.	O
one	O
may	O
also	O
include	O
an	O
external	O
input	O
ot	O
at	O
each	O
time	O
,	O
which	O
will	O
add	O
cot	O
to	O
the	O
mean	B
of	O
the	O
hidden	B
variable	I
and	O
dot	O
to	O
the	O
mean	B
of	O
the	O
observation	O
.	O
1	O
(	O
cid:112	O
)	O
|2πσh|	O
(	O
cid:18	O
)	O
(	O
cid:18	O
)	O
explicit	O
expressions	O
for	O
the	O
transition	O
and	O
emission	O
distributions	O
are	O
given	O
below	O
for	O
the	O
time-invariant	O
case	O
with	O
¯vt	O
=	O
0	O
,	O
¯ht	O
=	O
0.	O
each	O
hidden	B
variable	I
is	O
a	O
multidimensional	O
gaussian	O
distributed	O
vector	O
ht	O
,	O
with	O
(	O
cid:19	O
)	O
p	O
(	O
ht|ht−1	O
)	O
=	O
exp	O
1	O
2	O
(	O
ht	O
−	O
aht−1	O
)	O
t	O
σ−1	O
h	O
(	O
ht	O
−	O
aht−1	O
)	O
−	O
which	O
states	O
that	O
ht+1	O
has	O
a	O
mean	B
equal	O
to	O
aht	O
with	O
gaussian	O
ﬂuctuations	O
described	O
by	O
the	O
covariance	B
matrix	O
σh	O
.	O
similarly	O
,	O
p	O
(	O
vt|ht	O
)	O
=	O
1	O
(	O
cid:112	O
)	O
|2πσv	O
|	O
exp	O
1	O
2	O
−	O
(	O
vt	O
−	O
bht	O
)	O
t	O
σ−1	O
(	O
cid:19	O
)	O
v	O
(	O
vt	O
−	O
bht	O
)	O
describes	O
an	O
output	O
vt	O
with	O
mean	O
bht	O
and	O
covariance	B
σv	O
.	O
2these	O
models	O
are	O
also	O
often	O
called	O
kalman	O
filters	O
.	O
we	O
avoid	O
this	O
terminology	O
here	O
since	O
the	O
word	O
‘	O
ﬁlter	O
’	O
refers	O
to	O
a	O
speciﬁc	O
kind	O
of	O
inference	B
and	O
runs	O
the	O
risk	B
of	O
confusing	O
a	O
ﬁltering	B
algorithm	O
with	O
the	O
model	B
itself	O
.	O
442	O
draft	O
march	O
9	O
,	O
2010	O
inference	B
figure	O
24.5	O
:	O
a	O
single	O
phasor	O
plotted	O
as	O
a	O
damped	O
two	O
dimensional	O
rotation	O
ht+1	O
=	O
γrθht	O
with	O
a	O
damping	O
factor	B
0	O
<	O
γ	O
<	O
1.	O
by	O
taking	O
a	O
projection	B
onto	O
the	O
y	O
axis	O
,	O
the	O
phasor	O
generates	O
a	O
damped	O
sinusoid	O
.	O
example	O
101.	O
consider	O
a	O
dynamical	B
system	I
deﬁned	O
on	O
two	O
dimensional	O
vectors	O
ht	O
:	O
(	O
cid:18	O
)	O
cos	O
θ	O
−	O
sin	O
θ	O
cos	O
θ	O
sin	O
θ	O
(	O
cid:19	O
)	O
ht+1	O
=	O
rθht	O
,	O
with	O
rθ	O
=	O
(	O
24.3.7	O
)	O
rθ	O
rotates	O
the	O
vector	O
ht	O
through	O
angle	O
θ	O
in	O
one	O
timestep	O
.	O
under	O
this	O
lds	O
h	O
will	O
trace	O
out	O
points	O
on	O
a	O
circle	O
through	O
time	O
.	O
by	O
taking	O
a	O
scalar	O
projection	O
of	O
ht	O
,	O
for	O
example	O
,	O
vt	O
=	O
[	O
ht	O
]	O
1	O
=	O
[	O
1	O
0	O
]	O
tht	O
,	O
(	O
24.3.8	O
)	O
the	O
elements	O
vt	O
,	O
t	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
t	O
describe	O
a	O
sinusoid	O
through	O
time	O
,	O
see	O
ﬁg	O
(	O
24.5	O
)	O
.	O
by	O
using	O
a	O
block	O
diagonal	O
r	O
=	O
blkdiag	O
(	O
rθ1	O
,	O
.	O
.	O
.	O
,	O
rθm	O
)	O
and	O
taking	O
a	O
scalar	O
projection	O
of	O
the	O
extended	O
m	O
×	O
2	O
dimensional	O
h	O
vector	O
,	O
one	O
can	O
construct	O
a	O
representation	B
of	O
a	O
signal	O
in	O
terms	O
of	O
m	O
sinusoidal	O
components	O
.	O
24.4	O
inference	B
given	O
an	O
observation	O
sequence	O
v1	O
:	O
t	O
we	O
wish	O
to	O
consider	O
ﬁltering	B
and	O
smoothing	B
,	O
as	O
we	O
did	O
for	O
the	O
hmm	O
,	O
section	O
(	O
23.2.1	O
)	O
.	O
for	O
the	O
hmm	O
,	O
in	O
deriving	O
the	O
various	O
message	B
passing	I
recursions	O
,	O
we	O
used	O
only	O
the	O
independence	B
structure	O
encoded	O
by	O
the	O
belief	B
network	I
.	O
since	O
the	O
lds	O
has	O
the	O
same	O
independence	B
structure	O
as	O
the	O
hmm	O
,	O
we	O
can	O
use	O
the	O
same	O
independence	B
assumptions	O
in	O
deriving	O
the	O
updates	O
for	O
the	O
lds	O
.	O
however	O
,	O
in	O
implementing	O
them	O
we	O
need	O
to	O
deal	O
with	O
the	O
issue	O
that	O
we	O
now	O
have	O
continuous	B
hidden	O
variables	O
,	O
rather	O
than	O
discrete	B
states	O
.	O
the	O
fact	O
that	O
the	O
distributions	O
are	O
gaussian	O
means	O
that	O
we	O
can	O
deal	O
with	O
continuous	O
messages	O
exactly	O
.	O
in	O
translating	O
the	O
hmm	O
message	B
passing	I
equations	O
,	O
we	O
ﬁrst	O
replace	O
summation	O
with	O
integration	O
.	O
for	O
example	O
,	O
the	O
ﬁltering	B
recursion	O
(	O
23.2.7	O
)	O
becomes	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
∝	O
ht−1	O
p	O
(	O
vt|ht	O
)	O
p	O
(	O
ht|ht−1	O
)	O
p	O
(	O
ht−1|v1	O
:	O
t−1	O
)	O
,	O
t	O
>	O
1	O
(	O
24.4.1	O
)	O
since	O
the	O
product	O
of	O
two	O
gaussians	O
is	O
another	O
gaussian	O
,	O
and	O
the	O
integral	O
of	O
a	O
gaussian	O
is	O
another	O
gaussian	O
,	O
the	O
resulting	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
is	O
also	O
gaussian	O
.	O
this	O
closure	O
property	O
of	O
gaussians	O
means	O
that	O
we	O
may	O
represent	O
p	O
(	O
ht−1|v1	O
:	O
t−1	O
)	O
=	O
n	O
(	O
ht−1	O
ft−1	O
,	O
ft−1	O
)	O
with	O
mean	O
ft−1	O
and	O
covariance	B
ft−1	O
.	O
the	O
eﬀect	O
of	O
equation	B
(	O
24.4.1	O
)	O
is	O
equivalent	B
to	O
updating	O
the	O
mean	B
ft−1	O
and	O
covariance	B
ft−1	O
into	O
a	O
mean	B
ft	O
and	O
covariance	B
ft	O
for	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
.	O
our	O
task	O
below	O
is	O
to	O
ﬁnd	O
explicit	O
algebraic	O
formulae	O
for	O
these	O
updates	O
.	O
numerical	B
stability	I
translating	O
the	O
message	B
passing	I
inference	O
techniques	O
we	O
developed	O
for	O
the	O
hmm	O
into	O
the	O
lds	O
is	O
largely	O
straightforward	O
.	O
indeed	O
,	O
one	O
could	O
simply	O
run	O
a	O
standard	O
sum-product	O
algorithm	B
(	O
albeit	O
for	O
continuous	B
variables	O
)	O
,	O
see	O
demosumprodgausscanonlds.m	O
.	O
in	O
long	O
timeseries	O
numerical	B
instabilities	O
can	O
build	O
up	O
and	O
may	O
result	O
in	O
grossly	O
inaccurate	O
results	O
,	O
depending	O
on	O
the	O
transition	O
and	O
emission	B
distribution	I
parameters	O
and	O
the	O
method	O
of	O
implementing	O
the	O
message	B
updates	O
.	O
for	O
this	O
reason	O
specialised	O
routines	O
have	O
been	O
developed	O
that	O
are	O
reasonably	O
numerically	O
stable	O
under	O
certain	O
parameter	B
regimes	O
[	O
283	O
]	O
.	O
for	O
the	O
hmm	O
in	O
section	O
(	O
23.2.1	O
)	O
,	O
we	O
discussed	O
two	O
alternative	O
methods	O
for	O
smoothing	B
,	O
the	O
parallel	B
β	O
approach	B
,	O
and	O
the	O
sequential	B
γ	O
approach	B
.	O
the	O
β	B
recursion	I
is	O
suitable	O
when	O
the	O
emission	O
and	O
transition	O
covariance	O
entries	O
are	O
small	O
,	O
and	O
the	O
γ	O
recursion	O
usually	O
preferable	O
in	O
the	O
more	O
standard	O
case	O
of	O
small	O
covariance	B
values	O
.	O
draft	O
march	O
9	O
,	O
2010	O
443	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
inference	B
analytical	O
shortcuts	O
in	O
deriving	O
the	O
inference	B
recursions	O
we	O
need	O
to	O
frequently	O
multiply	O
and	O
integrate	O
gaussians	O
.	O
whilst	O
in	O
principle	O
straightforward	O
,	O
this	O
can	O
be	O
algebraically	O
tedious	O
and	O
,	O
wherever	O
possible	O
,	O
it	O
is	O
useful	O
to	O
appeal	O
to	O
known	O
shortcuts	O
.	O
for	O
example	O
,	O
one	O
can	O
exploit	O
the	O
general	O
result	O
that	O
the	O
linear	B
transform	O
of	O
a	O
gaus-	O
sian	O
random	O
variable	O
is	O
another	O
gaussian	O
random	O
variable	O
.	O
similarly	O
it	O
is	O
convenient	O
to	O
make	O
use	O
of	O
the	O
conditioning	B
formulae	O
,	O
as	O
well	O
as	O
the	O
dynamics	B
reversal	I
intuition	O
.	O
these	O
results	O
are	O
stated	O
in	O
section	O
(	O
8.6	O
)	O
,	O
and	O
below	O
we	O
derive	O
the	O
most	O
useful	O
for	O
our	O
purposes	O
here	O
.	O
consider	O
a	O
linear	B
transformation	I
of	O
a	O
gaussian	O
random	O
variable	O
:	O
x	O
∼	O
n	O
(	O
x	O
µx	O
,	O
σx	O
)	O
η	O
∼	O
n	O
(	O
η	O
µ	O
,	O
σ	O
)	O
,	O
y	O
=	O
mx	O
+	O
η	O
,	O
(	O
24.4.2	O
)	O
where	O
x	O
and	O
η	O
are	O
assumed	O
to	O
be	O
generated	O
from	O
independent	O
processes	O
.	O
to	O
ﬁnd	O
the	O
distribution	B
p	O
(	O
y	O
)	O
,	O
one	O
approach	B
would	O
be	O
to	O
write	O
this	O
formally	O
as	O
p	O
(	O
y	O
)	O
=	O
n	O
(	O
y	O
mx	O
+	O
µ	O
,	O
σ	O
)	O
n	O
(	O
x	O
µx	O
,	O
σx	O
)	O
dx	O
(	O
24.4.3	O
)	O
and	O
carry	O
out	O
the	O
integral	O
(	O
by	O
completing	O
the	O
square	O
)	O
.	O
however	O
,	O
since	O
a	O
gaussian	O
variable	B
under	O
linear	B
transformation	I
is	O
another	O
gaussian	O
,	O
we	O
can	O
take	O
a	O
shortcut	O
and	O
just	O
ﬁnd	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
transformed	O
variable	B
.	O
its	O
mean	B
is	O
given	O
by	O
(	O
cid:104	O
)	O
y	O
(	O
cid:105	O
)	O
=	O
m	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
+	O
(	O
cid:104	O
)	O
η	O
(	O
cid:105	O
)	O
=	O
mµx	O
+	O
µ	O
(	O
24.4.4	O
)	O
to	O
ﬁnd	O
the	O
covariance	B
,	O
consider	O
the	O
displacement	O
of	O
a	O
variable	B
h	O
from	O
its	O
mean	B
,	O
which	O
we	O
write	O
as	O
the	O
covariance	B
is	O
,	O
by	O
deﬁnition	O
,	O
(	O
cid:10	O
)	O
∆h∆ht	O
(	O
cid:11	O
)	O
.	O
for	O
y	O
,	O
the	O
displacement	O
is	O
∆h	O
≡	O
h	O
−	O
(	O
cid:104	O
)	O
h	O
(	O
cid:105	O
)	O
∆y	O
=	O
m∆x	O
+	O
∆η	O
,	O
(	O
cid:68	O
)	O
∆y∆yt	O
(	O
cid:69	O
)	O
so	O
that	O
the	O
covariance	B
is	O
(	O
cid:68	O
)	O
(	O
m∆x	O
+	O
∆η	O
)	O
(	O
m∆x	O
+	O
∆η	O
)	O
t	O
(	O
cid:69	O
)	O
∆x∆ηt	O
(	O
cid:69	O
)	O
∆x∆xt	O
(	O
cid:69	O
)	O
(	O
cid:68	O
)	O
(	O
cid:68	O
)	O
since	O
the	O
noises	O
η	O
and	O
x	O
are	O
assumed	O
independent	O
,	O
(	O
cid:10	O
)	O
∆η∆xt	O
(	O
cid:11	O
)	O
=	O
0	O
we	O
have	O
∆η∆xt	O
(	O
cid:69	O
)	O
(	O
cid:68	O
)	O
mt	O
+	O
m	O
mt	O
+	O
=	O
=	O
m	O
(	O
cid:68	O
)	O
+	O
∆η∆ηt	O
(	O
cid:69	O
)	O
σy	O
=	O
mσxmt	O
+	O
σ	O
24.4.1	O
filtering	O
we	O
represent	O
the	O
ﬁltered	O
distribution	B
as	O
a	O
gaussian	O
with	O
mean	O
ft	O
and	O
covariance	B
ft	O
,	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
∼	O
n	O
(	O
ht	O
ft	O
,	O
ft	O
)	O
(	O
24.4.5	O
)	O
(	O
24.4.6	O
)	O
(	O
24.4.7	O
)	O
(	O
24.4.8	O
)	O
this	O
is	O
called	O
the	O
moment	B
representation	I
.	O
our	O
task	O
is	O
then	O
to	O
ﬁnd	O
a	O
recursion	O
for	O
ft	O
,	O
ft	O
in	O
terms	O
of	O
ft−1	O
,	O
ft−1	O
.	O
a	O
convenient	O
approach	B
is	O
to	O
ﬁrst	O
ﬁnd	O
the	O
joint	B
distribution	O
p	O
(	O
ht	O
,	O
vt|v1	O
:	O
t−1	O
)	O
and	O
then	O
condition	O
on	O
vt	O
to	O
ﬁnd	O
the	O
distribution	B
p	O
(	O
ht|v1	O
:	O
t	O
)	O
.	O
the	O
term	O
p	O
(	O
ht	O
,	O
vt|v1	O
:	O
t−1	O
)	O
is	O
a	O
gaussian	O
whose	O
statistics	O
can	O
be	O
found	O
from	O
the	O
relations	O
vt	O
=	O
bht	O
+	O
ηv	O
t	O
,	O
ht	O
=	O
aht−1	O
+	O
ηh	O
t	O
(	O
24.4.9	O
)	O
using	O
the	O
above	O
,	O
and	O
assuming	O
time-invariance	O
and	O
zero	O
biases	O
,	O
we	O
readily	O
ﬁnd	O
(	O
cid:68	O
)	O
(	O
cid:69	O
)	O
(	O
cid:68	O
)	O
(	O
cid:69	O
)	O
∆ht∆ht	O
t	O
|v1	O
:	O
t−1	O
=	O
a	O
∆ht−1∆ht	O
t−1|v1	O
:	O
t−1	O
at	O
+	O
σh	O
=	O
aft−1at	O
+	O
σh	O
(	O
24.4.10	O
)	O
444	O
draft	O
march	O
9	O
,	O
2010	O
inference	B
parameters	O
θt	O
=	O
(	O
cid:8	O
)	O
a	O
,	O
b	O
,	O
σh	O
,	O
σv	O
,	O
¯h	O
,	O
¯v	O
(	O
cid:9	O
)	O
algorithm	B
20	O
lds	O
forward	O
pass	O
.	O
compute	O
the	O
ﬁltered	O
posteriors	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
≡	O
n	O
(	O
ft	O
,	O
ft	O
)	O
for	O
a	O
lds	O
with	O
t.	O
the	O
log-likelihood	O
l	O
=	O
log	O
p	O
(	O
v1	O
:	O
t	O
)	O
is	O
also	O
returned	O
.	O
{	O
f1	O
,	O
f1	O
,	O
p1	O
}	O
=	O
ldsforward	O
(	O
0	O
,	O
0	O
,	O
v1	O
;	O
θt	O
)	O
f0	O
←	O
0	O
,	O
f0	O
←	O
0	O
,	O
l	O
←	O
log	O
p1	O
for	O
t	O
←	O
2	O
,	O
t	O
do	O
{	O
ft	O
,	O
ft	O
,	O
pt	O
}	O
=	O
ldsforward	O
(	O
ft−1	O
,	O
ft−1	O
,	O
vt	O
;	O
θ	O
)	O
l	O
←	O
l	O
+	O
log	O
pt	O
end	O
for	O
function	B
ldsforward	O
(	O
f	O
,	O
f	O
,	O
v	O
;	O
θ	O
)	O
(	O
cid:46	O
)	O
mean	B
of	O
p	O
(	O
ht	O
,	O
vt|v1	O
:	O
t−1	O
)	O
(	O
cid:46	O
)	O
covariance	B
of	O
p	O
(	O
ht	O
,	O
vt|v1	O
:	O
t−1	O
)	O
(	O
cid:46	O
)	O
find	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
by	O
conditioning	B
:	O
(	O
cid:46	O
)	O
compute	O
p	O
(	O
vt|v1	O
:	O
t−1	O
)	O
(	O
cid:17	O
)	O
bt	O
+	O
σv	O
aft−1at	O
+	O
σh	O
/	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
vv	O
σvh	O
vhς−1	O
(	O
cid:17	O
)	O
f	O
(	O
cid:48	O
)	O
vv	O
(	O
v	O
−	O
µv	O
)	O
µh	O
←	O
af	O
+	O
¯h	O
,	O
µv	O
←	O
bµh	O
+	O
¯v	O
σhh	O
←	O
afat	O
+	O
σh	O
,	O
σvv	O
←	O
bσhhbt	O
+	O
σv	O
,	O
σvh	O
←	O
bσhh	O
vhς−1	O
f	O
(	O
cid:48	O
)	O
vv	O
(	O
v	O
−	O
µv	O
)	O
,	O
←	O
µh	O
+	O
σt	O
2	O
(	O
v	O
−	O
µv	O
)	O
t	O
σ−1	O
p	O
(	O
cid:48	O
)	O
←	O
exp	O
−	O
1	O
return	O
f	O
(	O
cid:48	O
)	O
,	O
f	O
(	O
cid:48	O
)	O
,	O
p	O
(	O
cid:48	O
)	O
(	O
cid:68	O
)	O
(	O
cid:69	O
)	O
end	O
function	B
(	O
cid:68	O
)	O
(	O
cid:69	O
)	O
(	O
cid:112	O
)	O
det	O
(	O
2πσvv	O
)	O
←	O
σhh	O
−	O
σt	O
(	O
cid:69	O
)	O
(	O
cid:16	O
)	O
(	O
cid:69	O
)	O
(	O
cid:104	O
)	O
ht|v1	O
:	O
t−1	O
(	O
cid:105	O
)	O
=	O
a	O
(	O
cid:104	O
)	O
ht−1|v1	O
:	O
t−1	O
(	O
cid:105	O
)	O
in	O
the	O
above	O
,	O
using	O
our	O
moment	B
representation	I
of	O
the	O
forward	O
messages	O
(	O
cid:68	O
)	O
(	O
cid:68	O
)	O
∆vt∆vt	O
∆ht∆ht	O
(	O
cid:104	O
)	O
vt|v1	O
:	O
t−1	O
(	O
cid:105	O
)	O
=	O
ba	O
(	O
cid:104	O
)	O
ht−1|v1	O
:	O
t−1	O
(	O
cid:105	O
)	O
,	O
(	O
cid:68	O
)	O
t	O
|v1	O
:	O
t−1	O
t	O
|v1	O
:	O
t−1	O
t	O
|v1	O
:	O
t−1	O
t	O
|v1	O
:	O
t−1	O
aft−1at	O
+	O
σh	O
bt	O
+	O
σv	O
=	O
b	O
∆ht∆ht	O
∆vt∆ht	O
=	O
b	O
(	O
cid:104	O
)	O
ht−1|v1	O
:	O
t−1	O
(	O
cid:105	O
)	O
≡	O
ft−1	O
,	O
(	O
cid:68	O
)	O
(	O
cid:69	O
)	O
then	O
,	O
using	O
conditioning3	O
p	O
(	O
ht|vt	O
,	O
v1	O
:	O
t−1	O
)	O
will	O
have	O
mean	B
t	O
|v1	O
:	O
t−1	O
(	O
cid:69	O
)	O
(	O
cid:68	O
)	O
t	O
|v1	O
:	O
t−1	O
(	O
cid:68	O
)	O
ft	O
≡	O
(	O
cid:104	O
)	O
ht|v1	O
:	O
t−1	O
(	O
cid:105	O
)	O
+	O
t−1|v1	O
:	O
t−1	O
and	O
covariance	B
∆ht−1∆ht	O
(	O
cid:69	O
)	O
(	O
cid:68	O
)	O
∆ht∆vt	O
∆vt∆vt	O
(	O
cid:68	O
)	O
(	O
cid:69	O
)	O
≡	O
ft−1	O
(	O
cid:69	O
)	O
−1	O
(	O
cid:69	O
)	O
−1	O
(	O
cid:68	O
)	O
=	O
b	O
=	O
b	O
(	O
cid:16	O
)	O
(	O
vt	O
−	O
(	O
cid:104	O
)	O
vt|v1	O
:	O
t−1	O
(	O
cid:105	O
)	O
)	O
(	O
cid:69	O
)	O
∆ht∆vt	O
−	O
∆vt∆vt	O
t	O
|v1	O
:	O
t−1	O
∆vt∆ht	O
t	O
|v1	O
:	O
t−1	O
writing	O
out	O
the	O
above	O
explicitly	O
we	O
have	O
for	O
the	O
mean	B
:	O
bpbt	O
+	O
σv	O
t	O
|v1	O
:	O
t−1	O
(	O
cid:17	O
)	O
−1	O
(	O
vt	O
−	O
baft−1	O
)	O
(	O
cid:17	O
)	O
−1	O
bpbt	O
+	O
σv	O
bp	O
ft	O
≡	O
∆ht∆ht	O
t	O
|v1	O
:	O
t−1	O
ft	O
=	O
aft−1	O
+	O
pbt	O
(	O
cid:16	O
)	O
ft	O
=	O
p	O
+	O
σh	O
−	O
pbt	O
(	O
cid:16	O
)	O
and	O
covariance	B
where	O
p	O
≡	O
aft−1at	O
+	O
σh	O
(	O
24.4.11	O
)	O
(	O
24.4.12	O
)	O
(	O
24.4.13	O
)	O
(	O
24.4.14	O
)	O
(	O
24.4.15	O
)	O
(	O
24.4.16	O
)	O
(	O
24.4.17	O
)	O
(	O
24.4.18	O
)	O
(	O
24.4.19	O
)	O
the	O
ﬁltering	B
procedure	O
is	O
presented	O
in	O
algorithm	B
(	O
20	O
)	O
with	O
a	O
single	O
update	O
in	O
ldsforwardupdate.m	O
.	O
one	O
can	O
write	O
the	O
covariance	B
update	O
as	O
where	O
we	O
deﬁne	O
the	O
kalman	O
gain	O
matrix	B
ft	O
=	O
(	O
i	O
−	O
kb	O
)	O
p	O
k	O
=	O
pbt	O
(	O
cid:16	O
)	O
σv	O
+	O
bpbt	O
(	O
cid:17	O
)	O
−1	O
(	O
24.4.20	O
)	O
(	O
24.4.21	O
)	O
we	O
present	O
in	O
algorithm	B
(	O
?	O
?	O
)	O
the	O
recursion	O
in	O
standard	O
engineering	O
notation	O
.	O
see	O
also	O
ldssmooth.m	O
.	O
the	O
iteration	B
is	O
expected	O
to	O
be	O
numerically	O
stable	O
when	O
the	O
noise	O
covariances	O
are	O
small	O
.	O
(	O
cid:0	O
)	O
y	O
−	O
µy	O
(	O
cid:1	O
)	O
and	O
covariance	B
σxx	O
−	O
σxyς−1	O
yy	O
σyx	O
.	O
3p	O
(	O
x|y	O
)	O
is	O
a	O
gaussian	O
with	O
mean	O
µx	O
+	O
σxyς−1	O
yy	O
draft	O
march	O
9	O
,	O
2010	O
445	O
symmetrising	O
the	O
updates	O
inference	B
a	O
potential	B
numerical	O
issue	O
with	O
the	O
covariance	B
update	O
(	O
24.4.20	O
)	O
is	O
that	O
it	O
is	O
the	O
diﬀerence	O
of	O
two	O
positive	B
deﬁnite	I
matrices	O
.	O
if	O
there	O
are	O
numerical	B
errors	O
,	O
the	O
ft	O
may	O
not	O
be	O
positive	B
deﬁnite	I
,	O
nor	O
symmetric	O
.	O
using	O
the	O
woodbury	O
identity	O
,	O
deﬁnition	O
(	O
132	O
)	O
,	O
equation	B
(	O
24.4.18	O
)	O
can	O
be	O
written	O
more	O
compactly	O
as	O
(	O
cid:16	O
)	O
ft	O
=	O
p−1	O
+	O
btς−1	O
v	O
b	O
(	O
cid:17	O
)	O
−1	O
whilst	O
this	O
is	O
positive	O
semideﬁnite	O
,	O
this	O
is	O
numerically	O
expensive	O
since	O
it	O
involves	O
two	O
matrix	B
inversions	O
.	O
an	O
alternative	O
is	O
to	O
use	O
the	O
deﬁnition	O
of	O
k	O
,	O
from	O
which	O
we	O
can	O
write	O
kσv	O
kt	O
=	O
(	O
i	O
−	O
kb	O
)	O
pbtkt	O
hence	O
we	O
arrive	O
at	O
joseph	O
’	O
s	O
symmetrized	O
update	O
[	O
104	O
]	O
(	O
cid:16	O
)	O
p	O
(	O
i	O
−	O
kb	O
)	O
t	O
+	O
pbtkt	O
(	O
cid:17	O
)	O
(	O
i	O
−	O
kb	O
)	O
p	O
(	O
i	O
−	O
kb	O
)	O
t	O
+	O
kσv	O
kt	O
≡	O
(	O
i	O
−	O
kb	O
)	O
≡	O
(	O
i	O
−	O
kb	O
)	O
p	O
(	O
24.4.24	O
)	O
the	O
left	O
hand	O
side	O
is	O
the	O
addition	O
of	O
two	O
positive	B
deﬁnite	I
matrices	O
so	O
that	O
the	O
resulting	O
update	O
for	O
the	O
covariance	B
is	O
more	O
numerically	O
stable	O
.	O
a	O
similar	O
method	O
can	O
be	O
used	O
in	O
the	O
backward	O
pass	O
below	O
.	O
an	O
alternative	O
is	O
to	O
avoid	O
using	O
covariance	B
matrices	O
directly	O
and	O
use	O
their	O
square	O
root	O
as	O
the	O
parameter	B
,	O
deriving	O
updates	O
for	O
these	O
instead	O
[	O
226	O
,	O
38	O
]	O
.	O
24.4.2	O
smoothing	B
:	O
rauch-tung-striebel	O
correction	O
method	O
the	O
smoothed	O
posterior	B
p	O
(	O
ht|v1	O
:	O
t	O
)	O
is	O
necessarily	O
gaussian	O
since	O
it	O
is	O
the	O
conditional	B
marginal	I
of	O
a	O
larger	O
gaussian	O
.	O
by	O
representing	O
the	O
posterior	B
as	O
a	O
gaussian	O
with	O
mean	O
gt	O
and	O
covariance	B
gt	O
,	O
(	O
24.4.22	O
)	O
(	O
24.4.23	O
)	O
(	O
24.4.25	O
)	O
(	O
24.4.26	O
)	O
(	O
24.4.27	O
)	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
∼	O
n	O
(	O
ht	O
gt	O
,	O
gt	O
)	O
we	O
can	O
form	O
a	O
recursion	O
for	O
gt	O
and	O
gt	O
as	O
follows	O
:	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
=	O
=	O
p	O
(	O
ht	O
,	O
ht+1|v1	O
:	O
t	O
)	O
p	O
(	O
ht|v1	O
:	O
t	O
,	O
ht+1	O
)	O
p	O
(	O
ht+1|v1	O
:	O
t	O
)	O
=	O
(	O
cid:90	O
)	O
ht+1	O
p	O
(	O
ht|v1	O
:	O
t	O
,	O
ht+1	O
)	O
p	O
(	O
ht+1|v1	O
:	O
t	O
)	O
ht+1	O
ht+1	O
the	O
term	O
p	O
(	O
ht|v1	O
:	O
t	O
,	O
ht+1	O
)	O
can	O
be	O
found	O
by	O
conditioning	B
the	O
joint	B
distribution	O
p	O
(	O
ht	O
,	O
ht+1|v1	O
:	O
t	O
)	O
=	O
p	O
(	O
ht+1|ht	O
,	O
v1	O
:	O
t	O
)	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
(	O
24.4.28	O
)	O
which	O
is	O
obtained	O
in	O
the	O
usual	O
manner	O
by	O
ﬁnding	O
its	O
mean	B
and	O
covariance	B
:	O
the	O
term	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
is	O
a	O
known	O
gaussian	O
from	O
ﬁltering	B
with	O
mean	B
ft	O
and	O
covariance	B
ft.	O
hence	O
the	O
joint	B
distribution	O
p	O
(	O
ht	O
,	O
ht+1|v1	O
:	O
t	O
)	O
has	O
means	O
(	O
cid:104	O
)	O
ht+1|v1	O
:	O
t	O
(	O
cid:105	O
)	O
=	O
aft	O
and	O
covariance	B
elements	O
(	O
cid:104	O
)	O
ht|v1	O
:	O
t	O
(	O
cid:105	O
)	O
=	O
ft	O
,	O
(	O
cid:68	O
)	O
∆ht∆ht	O
t	O
|v1	O
:	O
t	O
(	O
cid:69	O
)	O
(	O
cid:68	O
)	O
(	O
cid:69	O
)	O
=	O
ftat	O
,	O
(	O
cid:68	O
)	O
=	O
ft	O
,	O
∆ht∆ht	O
t+1|v1	O
:	O
t	O
∆ht+1∆ht	O
t+1|v1	O
:	O
t	O
(	O
24.4.29	O
)	O
(	O
cid:69	O
)	O
=	O
aftat	O
+	O
σh	O
(	O
24.4.30	O
)	O
to	O
ﬁnd	O
p	O
(	O
ht|v1	O
:	O
t	O
,	O
ht+1	O
)	O
we	O
may	O
use	O
the	O
conditioned	O
gaussian	O
results	O
,	O
deﬁnition	O
(	O
78	O
)	O
.	O
it	O
is	O
useful	O
to	O
use	O
the	O
system	B
reversal	I
result	O
,	O
section	O
(	O
8.6.1	O
)	O
,	O
which	O
interprets	O
p	O
(	O
ht|v1	O
:	O
t	O
,	O
ht+1	O
)	O
as	O
an	O
equivalent	B
linear	O
system	B
going	O
backwards	O
in	O
time	O
:	O
ht	O
=	O
←−atht+1	O
+	O
←−mt	O
+	O
←−η	O
t	O
(	O
cid:69	O
)	O
(	O
cid:68	O
)	O
(	O
cid:68	O
)	O
where	O
←−at	O
≡	O
∆ht∆ht	O
t+1|v1	O
:	O
t	O
∆ht+1∆ht	O
t+1|v1	O
:	O
t	O
(	O
cid:69	O
)	O
−1	O
(	O
24.4.31	O
)	O
(	O
24.4.32	O
)	O
446	O
draft	O
march	O
9	O
,	O
2010	O
inference	B
algorithm	O
21	O
lds	O
backward	O
pass	O
.	O
compute	O
the	O
smoothed	O
posteriors	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
.	O
this	O
requires	O
the	O
ﬁltered	O
results	O
from	O
algorithm	B
(	O
20	O
)	O
.	O
{	O
gt	O
,	O
gt	O
}	O
=	O
ldsbackward	O
(	O
gt+1	O
,	O
gt+1	O
,	O
ft	O
,	O
ft	O
;	O
θ	O
)	O
gt	O
←	O
ft	O
,	O
gt	O
←	O
ft	O
for	O
t	O
←	O
t	O
−	O
1	O
,	O
1	O
do	O
end	O
for	O
function	B
ldsbackward	O
(	O
g	O
,	O
g	O
,	O
f	O
,	O
f	O
;	O
θ	O
)	O
σh	O
(	O
cid:48	O
)	O
h	O
(	O
cid:48	O
)	O
←	O
afat	O
+	O
σh	O
,	O
h	O
(	O
cid:48	O
)	O
hς−1	O
h	O
(	O
cid:48	O
)	O
h	O
(	O
cid:48	O
)	O
σh	O
(	O
cid:48	O
)	O
h	O
,	O
←−a	O
←	O
σt	O
←	O
←−ag←−at	O
+	O
←−σ	O
(	O
cid:69	O
)	O
(	O
cid:68	O
)	O
µh	O
←	O
af	O
+	O
¯h	O
,	O
h	O
(	O
cid:48	O
)	O
hς−1	O
←−σ	O
←	O
f	O
−	O
σt	O
←	O
←−ag	O
+	O
←−m	O
,	O
g	O
(	O
cid:48	O
)	O
g	O
(	O
cid:48	O
)	O
return	O
g	O
(	O
cid:48	O
)	O
,	O
g	O
(	O
cid:48	O
)	O
(	O
cid:68	O
)	O
(	O
cid:17	O
)	O
←−mt	O
≡	O
(	O
cid:104	O
)	O
ht|v1	O
:	O
t	O
(	O
cid:105	O
)	O
−	O
←−η	O
t	O
0	O
,	O
←−σ	O
t	O
end	O
function	B
∆ht∆ht	O
,	O
with	O
t+1|v1	O
:	O
t	O
(	O
cid:68	O
)	O
(	O
cid:69	O
)	O
(	O
cid:69	O
)	O
(	O
cid:68	O
)	O
(	O
cid:16	O
)	O
(	O
cid:68	O
)	O
and	O
←−η	O
t	O
∼	O
n	O
←−σ	O
t	O
≡	O
∆ht+1∆ht	O
t+1|v1	O
:	O
t	O
(	O
cid:69	O
)	O
−1	O
(	O
cid:104	O
)	O
ht+1|v1	O
:	O
t	O
(	O
cid:105	O
)	O
(	O
cid:69	O
)	O
−1	O
(	O
cid:68	O
)	O
σh	O
(	O
cid:48	O
)	O
h	O
←	O
af	O
h	O
(	O
cid:48	O
)	O
h	O
(	O
cid:48	O
)	O
,	O
←−m	O
←	O
f	O
−	O
←−aµh	O
(	O
cid:46	O
)	O
statistics	O
of	O
p	O
(	O
ht	O
,	O
ht+1|v1	O
:	O
t	O
)	O
(	O
cid:46	O
)	O
dynamics	B
reversal	I
p	O
(	O
ht|ht+1	O
,	O
v1	O
:	O
t	O
)	O
(	O
cid:46	O
)	O
backward	O
propagation	B
(	O
cid:69	O
)	O
(	O
24.4.33	O
)	O
(	O
24.4.34	O
)	O
∆ht∆ht	O
t	O
|v1	O
:	O
t	O
−	O
∆ht∆ht	O
t+1|v1	O
:	O
t	O
∆ht+1∆ht	O
t+1|v1	O
:	O
t	O
∆ht+1∆ht	O
t	O
|v1	O
:	O
t	O
using	O
dynamics	B
reversal	I
,	O
equation	B
(	O
24.4.31	O
)	O
and	O
assuming	O
that	O
ht+1	O
is	O
gaussian	O
distributed	O
,	O
it	O
is	O
then	O
straightforward	O
to	O
work	O
out	O
the	O
statistics	O
of	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
.	O
the	O
mean	B
is	O
given	O
by	O
and	O
covariance	B
gt	O
≡	O
(	O
cid:104	O
)	O
ht|v1	O
:	O
t	O
(	O
cid:105	O
)	O
=	O
←−at	O
(	O
cid:104	O
)	O
ht+1|v1	O
:	O
t	O
(	O
cid:105	O
)	O
+	O
←−mt	O
≡	O
←−atgt+1	O
+	O
←−mt	O
(	O
cid:69	O
)	O
←−at	O
t	O
+	O
←−σ	O
t	O
≡	O
←−atgt+1←−at	O
t+1|v1	O
:	O
t	O
∆ht+1∆ht	O
∆ht∆ht	O
t	O
|v1	O
:	O
t	O
gt	O
≡	O
=	O
←−at	O
(	O
cid:68	O
)	O
(	O
cid:69	O
)	O
(	O
cid:68	O
)	O
t	O
+	O
←−σ	O
t	O
(	O
24.4.35	O
)	O
(	O
24.4.36	O
)	O
this	O
procedure	O
is	O
the	O
rauch-tung-striebel	O
kalman	O
smoother	O
[	O
231	O
]	O
.	O
this	O
is	O
called	O
a	O
‘	O
correction	O
’	O
method	O
since	O
it	O
takes	O
the	O
ﬁltered	O
estimate	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
and	O
‘	O
corrects	O
’	O
it	O
to	O
form	O
a	O
smoothed	O
estimate	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
.	O
the	O
procedure	O
is	O
outlined	O
in	O
algorithm	B
(	O
21	O
)	O
and	O
is	O
detailed	O
in	O
ldsbackwardupdate.m	O
.	O
see	O
also	O
ldssmooth.m	O
.	O
the	O
cross	B
moment	I
an	O
advantage	O
of	O
the	O
dynamics	B
reversal	I
interpretation	O
given	O
above	O
is	O
that	O
the	O
cross	B
moment	I
(	O
which	O
is	O
required	O
for	O
learning	B
)	O
is	O
immediately	O
obtained	O
from	O
=	O
←−atgt+1	O
+	O
gtgt	O
t+1	O
(	O
24.4.37	O
)	O
(	O
cid:68	O
)	O
htht	O
t+1|v1	O
:	O
t	O
(	O
cid:69	O
)	O
(	O
cid:104	O
)	O
∆ht∆ht+1|v1	O
:	O
t	O
(	O
cid:105	O
)	O
=	O
←−atgt+1	O
⇒	O
24.4.3	O
the	O
likelihood	B
we	O
can	O
compute	O
the	O
likelihood	B
using	O
the	O
decomposition	B
p	O
(	O
v1	O
:	O
t	O
)	O
=	O
p	O
(	O
vt|v1	O
:	O
t−1	O
)	O
(	O
24.4.38	O
)	O
in	O
which	O
each	O
conditional	B
p	O
(	O
vt|v1	O
:	O
t−1	O
)	O
is	O
a	O
gaussian	O
in	O
vt.	O
it	O
is	O
straightforward	O
to	O
show	O
that	O
the	O
term	O
p	O
(	O
vt|v1	O
:	O
t−1	O
)	O
has	O
mean	B
and	O
covariance	B
t	O
(	O
cid:89	O
)	O
t=1	O
µ1	O
≡	O
bµ	O
µt	O
≡	O
baft−1	O
σt	O
≡	O
b	O
(	O
cid:0	O
)	O
aft−1at	O
+	O
σh	O
(	O
vt	O
−	O
µt	O
)	O
t	O
σ−1	O
σ1	O
≡	O
bσbt	O
+	O
σv	O
t	O
(	O
cid:88	O
)	O
log	O
p	O
(	O
v1	O
:	O
t	O
)	O
=	O
−	O
(	O
cid:104	O
)	O
1	O
2	O
t	O
t=1	O
the	O
log	O
likelihood	B
is	O
then	O
given	O
by	O
(	O
cid:1	O
)	O
bt	O
+	O
σv	O
t	O
=	O
1	O
t	O
>	O
1	O
(	O
cid:105	O
)	O
(	O
vt	O
−	O
µt	O
)	O
+	O
log	O
det	O
(	O
2πσt	O
)	O
draft	O
march	O
9	O
,	O
2010	O
(	O
24.4.39	O
)	O
(	O
24.4.40	O
)	O
447	O
24.4.4	O
most	B
likely	I
state	I
since	O
the	O
mode	B
of	O
a	O
gaussian	O
is	O
equal	O
to	O
its	O
mean	B
,	O
there	O
is	O
no	O
diﬀerence	O
between	O
the	O
most	O
probable	O
joint	O
posterior	B
state	O
inference	B
argmax	O
h1	O
:	O
t	O
p	O
(	O
h1	O
:	O
t|v1	O
:	O
t	O
)	O
and	O
the	O
set	O
of	O
most	O
probable	O
marginal	O
states	O
ht	O
=	O
argmax	O
htt	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
,	O
t	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
t	O
(	O
24.4.41	O
)	O
(	O
24.4.42	O
)	O
hence	O
the	O
most	O
likely	O
hidden	O
state	O
sequence	O
is	O
equivalent	B
to	O
the	O
smoothed	O
mean	B
sequence	O
.	O
24.4.5	O
time	O
independence	B
and	O
riccati	O
equations	O
both	O
the	O
ﬁltered	O
ft	O
and	O
smoothed	O
gt	O
covariance	B
recursions	O
are	O
independent	O
of	O
the	O
observations	O
v1	O
:	O
t	O
,	O
depending	O
only	O
on	O
the	O
parameters	O
of	O
the	O
model	B
.	O
this	O
is	O
a	O
general	O
characteristic	O
of	O
linear	B
gaussian	O
systems	O
.	O
typically	O
the	O
covariance	B
recursions	O
converge	O
quickly	O
to	O
values	O
that	O
are	O
reasonably	O
constant	O
throughout	O
the	O
dynamics	O
,	O
with	O
only	O
appreciable	O
diﬀerences	O
at	O
the	O
boundaries	O
t	O
=	O
1	O
and	O
t	O
=	O
t	O
.	O
in	O
practice	O
one	O
often	O
drops	O
the	O
time-dependence	O
of	O
the	O
covariances	O
and	O
approximates	O
them	O
with	O
a	O
single	O
time-independent	O
covariance	B
.	O
this	O
approximation	B
dramatically	O
reduces	O
storage	O
requirements	O
.	O
the	O
converged	O
ﬁltered	O
f	O
satisﬁes	O
the	O
recursion	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
−1	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
afat	O
+	O
σh	O
b	O
afat	O
+	O
σh	O
bt	O
+	O
σv	O
b	O
afat	O
+	O
σh	O
(	O
24.4.43	O
)	O
f	O
=	O
afat	O
+	O
σh	O
−	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
bt	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
which	O
can	O
be	O
related	O
to	O
a	O
form	O
of	O
algebraic	O
riccati	O
equation	B
.	O
a	O
technique	O
to	O
solve	O
these	O
equations	O
is	O
to	O
being	O
with	O
setting	O
the	O
covariance	B
to	O
σ.	O
with	O
this	O
,	O
a	O
new	O
f	O
is	O
found	O
using	O
the	O
right	O
hand	O
side	O
of	O
(	O
24.4.43	O
)	O
,	O
and	O
subsequently	O
recursively	O
updated	O
.	O
alternatively	O
,	O
using	O
the	O
woodbury	O
identity	O
,	O
the	O
converged	O
covariance	B
satisﬁes	O
(	O
cid:18	O
)	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
−1	O
(	O
cid:19	O
)	O
−1	O
f	O
=	O
afat	O
+	O
σh	O
+	O
btς−1	O
v	O
b	O
(	O
24.4.44	O
)	O
although	O
this	O
form	O
is	O
less	O
numerically	O
convenient	O
in	O
forming	O
an	O
iterative	O
solver	O
for	O
f	O
since	O
it	O
requires	O
two	O
matrix	B
inversions	O
.	O
example	O
102	O
(	O
newtonian	O
trajectory	O
analysis	B
)	O
.	O
a	O
toy	O
rocket	O
with	O
unknown	O
mass	O
and	O
initial	O
velocity	O
is	O
launched	O
in	O
the	O
air	O
.	O
in	O
addition	O
,	O
the	O
constant	O
accelerations	O
from	O
the	O
rocket	O
’	O
s	O
propulsion	O
system	B
are	O
unknown	O
.	O
it	O
is	O
known	O
is	O
that	O
newton	O
’	O
s	O
laws	O
apply	O
and	O
an	O
instrument	O
can	O
measure	O
the	O
vertical	O
height	O
and	O
horizontal	O
distance	O
of	O
the	O
rocket	O
at	O
each	O
time	O
x	O
(	O
t	O
)	O
,	O
y	O
(	O
t	O
)	O
from	O
the	O
origin	O
.	O
based	O
on	O
noisy	O
measurements	O
of	O
x	O
(	O
t	O
)	O
and	O
y	O
(	O
t	O
)	O
,	O
our	O
task	O
is	O
to	O
infer	O
the	O
position	O
of	O
the	O
rocket	O
at	O
each	O
time	O
.	O
although	O
this	O
is	O
perhaps	O
most	O
appropriately	O
considered	O
from	O
the	O
using	O
continuous	B
time	O
dynamics	O
,	O
we	O
will	O
translate	O
this	O
into	O
a	O
discrete	B
time	O
approximation	B
.	O
newton	O
’	O
s	O
law	O
states	O
that	O
d2	O
dt2	O
x	O
=	O
fx	O
(	O
t	O
)	O
m	O
,	O
d2	O
dt2	O
y	O
=	O
fy	O
(	O
t	O
)	O
m	O
(	O
24.4.45	O
)	O
where	O
m	O
is	O
the	O
mass	O
of	O
the	O
object	O
and	O
fx	O
(	O
t	O
)	O
,	O
fy	O
(	O
t	O
)	O
are	O
the	O
horizontal	O
and	O
vertical	O
forces	O
respectively	O
.	O
hence	O
as	O
they	O
stand	O
,	O
these	O
equations	O
are	O
not	O
in	O
a	O
form	O
directly	O
usable	O
in	O
the	O
lds	O
framework	O
.	O
a	O
naive	O
approach	O
is	O
to	O
reparameterise	O
time	O
to	O
use	O
the	O
variable	B
˜t	O
such	O
that	O
t	O
≡	O
˜t∆	O
,	O
where	O
˜t	O
is	O
integer	O
and	O
∆	O
is	O
a	O
unit	O
of	O
time	O
.	O
the	O
dynamics	O
is	O
then	O
x	O
(	O
(	O
˜t	O
+	O
1	O
)	O
∆	O
)	O
=	O
x	O
(	O
˜t∆	O
)	O
+	O
∆x	O
y	O
(	O
(	O
˜t	O
+	O
1	O
)	O
∆	O
)	O
=	O
y	O
(	O
˜t∆	O
)	O
+	O
∆y	O
(	O
cid:48	O
)	O
(	O
˜t∆	O
)	O
(	O
cid:48	O
)	O
(	O
˜t∆	O
)	O
(	O
24.4.46	O
)	O
(	O
24.4.47	O
)	O
448	O
draft	O
march	O
9	O
,	O
2010	O
learning	B
linear	O
dynamical	O
systems	O
figure	O
24.6	O
:	O
estimate	O
of	O
the	O
trajectory	O
of	O
a	O
new-	O
tonian	O
ballistic	O
object	O
based	O
on	O
noisy	O
observations	O
(	O
small	O
circles	O
)	O
.	O
all	O
time	O
labels	O
are	O
known	O
but	O
omit-	O
ted	O
in	O
the	O
plot	O
.	O
the	O
‘	O
x	O
’	O
points	O
are	O
the	O
true	O
po-	O
sitions	O
of	O
the	O
object	O
,	O
and	O
the	O
crosses	O
‘	O
+	O
’	O
are	O
the	O
estimated	O
smoothed	O
mean	B
positions	O
(	O
cid:104	O
)	O
xt	O
,	O
yt|v1	O
:	O
t	O
(	O
cid:105	O
)	O
of	O
the	O
object	O
plotted	O
every	O
several	O
time	O
steps	O
.	O
see	O
demoldstracking.m	O
where	O
y	O
(	O
cid:48	O
)	O
(	O
t	O
)	O
≡	O
dy	O
(	O
cid:48	O
)	O
(	O
(	O
˜t	O
+	O
1	O
)	O
∆	O
)	O
=	O
x	O
x	O
dt	O
.	O
we	O
can	O
write	O
an	O
update	O
equation	B
for	O
the	O
x	O
(	O
cid:48	O
)	O
and	O
y	O
(	O
cid:48	O
)	O
as	O
(	O
cid:48	O
)	O
(	O
˜t∆	O
)	O
+	O
fy∆/m	O
(	O
cid:48	O
)	O
(	O
˜t∆	O
)	O
+	O
fx∆/m	O
,	O
(	O
cid:48	O
)	O
(	O
(	O
˜t	O
+	O
1	O
)	O
∆	O
)	O
=	O
y	O
y	O
(	O
24.4.48	O
)	O
these	O
are	O
discrete	B
time	O
diﬀerence	O
equations	O
indexed	O
by	O
˜t	O
.	O
the	O
instrument	O
which	O
measures	O
x	O
(	O
t	O
)	O
and	O
y	O
(	O
t	O
)	O
is	O
not	O
completely	B
accurate	O
.	O
for	O
simplicity	O
,	O
we	O
relabel	O
ax	O
(	O
t	O
)	O
=	O
fx	O
(	O
t	O
)	O
/m	O
(	O
t	O
)	O
,	O
ay	O
(	O
t	O
)	O
=	O
fy	O
(	O
t	O
)	O
/m	O
(	O
t	O
)	O
–	O
these	O
accelerations	O
will	O
be	O
assumed	O
to	O
be	O
roughly	O
constant	O
,	O
but	O
unknown	O
:	O
ax	O
(	O
(	O
˜t	O
+	O
1	O
)	O
∆	O
)	O
=	O
ax	O
(	O
˜t∆	O
)	O
+	O
ηx	O
,	O
ay	O
(	O
(	O
˜t	O
+	O
1	O
)	O
∆	O
)	O
=	O
ay	O
(	O
˜t∆	O
)	O
+	O
ηy	O
,	O
(	O
24.4.49	O
)	O
where	O
ηx	O
and	O
ηy	O
are	O
small	O
noise	O
terms	O
.	O
the	O
initial	O
distributions	O
for	O
the	O
accelerations	O
are	O
assumed	O
vague	O
,	O
using	O
a	O
zero	O
mean	B
gaussian	O
with	O
large	O
variance	B
.	O
we	O
describe	O
the	O
above	O
model	B
by	O
considering	O
x	O
(	O
t	O
)	O
,	O
x	O
(	O
cid:48	O
)	O
(	O
t	O
)	O
,	O
y	O
(	O
t	O
)	O
,	O
y	O
(	O
cid:48	O
)	O
(	O
t	O
)	O
,	O
ax	O
(	O
t	O
)	O
,	O
ay	O
(	O
t	O
)	O
as	O
hidden	O
variables	O
,	O
giving	O
rise	O
to	O
a	O
h	O
=	O
6	O
dimensional	O
lds	O
with	O
transition	O
and	O
emission	O
matrices	O
as	O
below	O
:	O
(	O
cid:18	O
)	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
(	O
cid:19	O
)	O
b	O
=	O
(	O
24.4.50	O
)	O
	O
a	O
=	O
	O
,	O
0	O
0	O
1	O
0	O
∆	O
0	O
1	O
0	O
0	O
0	O
0	O
∆	O
1	O
0	O
∆	O
0	O
0	O
0	O
0	O
0	O
0	O
∆	O
1	O
0	O
0	O
0	O
0	O
1	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
we	O
place	O
a	O
large	O
variance	B
on	O
their	O
initial	O
values	O
,	O
and	O
attempt	O
to	O
infer	O
the	O
unknown	O
trajectory	O
.	O
a	O
demonstra-	O
tion	O
is	O
given	O
in	O
ﬁg	O
(	O
24.6	O
)	O
.	O
despite	O
the	O
signiﬁcant	O
observation	O
noise	O
,	O
the	O
object	O
trajectory	O
can	O
be	O
accurately	O
inferred	O
.	O
24.5	O
learning	B
linear	O
dynamical	O
systems	O
whilst	O
in	O
many	O
applications	O
,	O
particularly	O
of	O
underlying	O
known	O
physical	O
processes	O
,	O
the	O
paremeters	O
of	O
the	O
lds	O
are	O
known	O
,	O
in	O
many	O
machine	O
learning	B
tasks	O
we	O
need	O
to	O
learn	O
the	O
parameters	O
of	O
the	O
lds	O
based	O
on	O
v1	O
:	O
t	O
.	O
for	O
simplicity	O
we	O
assume	O
that	O
we	O
know	O
the	O
dimensionality	O
h	O
of	O
the	O
lds	O
.	O
24.5.1	O
identiﬁability	B
issues	O
an	O
interesting	O
question	O
is	O
whether	O
we	O
can	O
uniquely	O
identify	O
(	O
learn	O
)	O
the	O
parameters	O
of	O
an	O
lds	O
.	O
there	O
are	O
always	O
trivial	O
redundancies	O
in	O
the	O
solution	O
obtained	O
by	O
permuting	O
the	O
hidden	B
variables	I
arbitrarily	O
and	O
ﬂipping	O
their	O
signs	O
.	O
to	O
show	O
that	O
there	O
are	O
potentially	O
many	O
more	O
equivalent	B
solutions	O
,	O
consider	O
the	O
following	O
lds	O
vt	O
=	O
bht	O
+	O
ηv	O
t	O
,	O
ht	O
=	O
aht−1	O
+	O
ηh	O
t	O
draft	O
march	O
9	O
,	O
2010	O
(	O
24.5.1	O
)	O
449	O
−1000100200300400500600700800−150−100−50050100150200250300xy	O
we	O
now	O
attempt	O
to	O
transform	O
this	O
original	O
system	B
to	O
a	O
new	O
form	O
which	O
will	O
produce	O
exactly	O
the	O
same	O
outputs	O
v1	O
:	O
t	O
.	O
for	O
an	O
invertible	O
matrix	B
r	O
we	O
consider	O
learning	B
linear	O
dynamical	O
systems	O
rht	O
=	O
rar−1rht−1	O
+	O
rηh	O
t	O
which	O
is	O
representable	O
as	O
a	O
new	O
latent	B
dynamics	O
ˆht	O
=	O
ˆaˆht−1	O
+	O
ˆηh	O
t	O
(	O
24.5.2	O
)	O
(	O
24.5.3	O
)	O
where	O
ˆa	O
≡	O
rar−1	O
,	O
ˆht	O
≡	O
rht	O
,	O
ˆηh	O
the	O
transformed	O
h	O
:	O
t	O
≡	O
rηh	O
t	O
.	O
in	O
addition	O
,	O
we	O
can	O
reexpress	O
the	O
outputs	O
to	O
be	O
a	O
function	B
of	O
vt	O
=	O
br−1rht	O
+	O
ηv	O
t	O
=	O
ˆbˆht	O
+	O
ηv	O
t	O
(	O
24.5.4	O
)	O
hence	O
,	O
provided	O
we	O
place	O
no	O
constraints	O
on	O
a	O
,	O
b	O
and	O
σh	O
there	O
exists	O
an	O
inﬁnite	O
space	O
of	O
equivalent	B
solutions	O
,	O
ˆa	O
=	O
ra	O
r−1	O
,	O
ˆb	O
=	O
br−1	O
,	O
ˆσh	O
=	O
rσhrt	O
,	O
all	O
with	O
the	O
same	O
likelihood	B
value	O
.	O
this	O
means	O
that	O
directly	O
interpreting	O
the	O
learned	O
parameters	O
needs	O
to	O
be	O
done	O
with	O
some	O
care	O
.	O
this	O
redundancy	O
can	O
be	O
mitigated	O
by	O
imposing	O
constraints	O
on	O
the	O
parameters	O
.	O
24.5.2	O
em	O
algorithm	B
for	O
simplicity	O
,	O
we	O
assume	O
we	O
have	O
a	O
single	O
sequence	O
v1	O
:	O
t	O
,	O
to	O
which	O
we	O
wish	O
to	O
ﬁt	O
a	O
lds	O
using	O
maximum	B
likelihood	I
.	O
since	O
the	O
lds	O
contains	O
latent	B
variables	O
one	O
approach	B
is	O
to	O
use	O
the	O
em	O
algorithm	B
.	O
as	O
usual	O
,	O
the	O
m-step	O
of	O
the	O
em	O
algorithm	B
requires	O
us	O
to	O
maximise	O
the	O
energy	B
with	O
respect	O
to	O
the	O
parameters	O
a	O
,	O
b	O
,	O
a	O
,	O
σ	O
,	O
σv	O
,	O
σh	O
.	O
thanks	O
to	O
the	O
form	O
of	O
the	O
lds	O
the	O
energy	B
decomposes	O
as	O
t=2	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
h1	O
)	O
(	O
cid:105	O
)	O
pold	O
(	O
h1|v1	O
:	O
t	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
ht|ht−1	O
)	O
(	O
cid:105	O
)	O
pold	O
(	O
ht	O
,	O
ht−1|v1	O
:	O
t	O
)	O
+	O
it	O
is	O
straightforward	O
to	O
derive	O
that	O
the	O
m-step	O
for	O
the	O
parameters	O
is	O
given	O
by	O
(	O
angled	O
brackets	O
(	O
cid:104	O
)	O
·	O
(	O
cid:105	O
)	O
denote	O
(	O
cid:69	O
)	O
expectation	B
with	O
respect	O
to	O
the	O
smoothed	O
posterior	B
p	O
(	O
h1	O
:	O
t|v1	O
:	O
t	O
)	O
)	O
:	O
(	O
cid:69	O
)	O
htht	O
t	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
vt|ht	O
)	O
(	O
cid:105	O
)	O
pold	O
(	O
ht|v1	O
:	O
t	O
)	O
(	O
cid:16	O
)	O
(	O
cid:68	O
)	O
(	O
cid:69	O
)	O
t	O
−	O
vt	O
(	O
cid:104	O
)	O
ht	O
(	O
cid:105	O
)	O
t	O
bt	O
−	O
b	O
(	O
cid:104	O
)	O
ht	O
(	O
cid:105	O
)	O
vt	O
ht+1ht	O
at	O
(	O
cid:17	O
)	O
bt	O
(	O
cid:17	O
)	O
(	O
cid:88	O
)	O
at	O
+	O
a	O
(	O
24.5.6	O
)	O
(	O
24.5.8	O
)	O
(	O
24.5.7	O
)	O
σnew	O
v	O
=	O
t	O
+	O
b	O
htht	O
vtvt	O
(	O
cid:69	O
)	O
(	O
cid:68	O
)	O
(	O
cid:68	O
)	O
(	O
cid:68	O
)	O
(	O
cid:68	O
)	O
(	O
cid:69	O
)	O
t	O
1	O
1	O
t	O
ht+1ht	O
t	O
htht	O
t	O
t+1	O
−	O
a	O
t+1	O
−	O
t	O
(	O
cid:88	O
)	O
t=1	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
v1	O
:	O
t	O
,	O
h1	O
:	O
t	O
)	O
(	O
cid:105	O
)	O
pold	O
(	O
h1	O
:	O
t	O
|v1	O
:	O
t	O
)	O
t	O
(	O
cid:88	O
)	O
(	O
cid:16	O
)	O
(	O
cid:88	O
)	O
(	O
cid:69	O
)	O
t	O
ht+1ht	O
t	O
σnew	O
h	O
=	O
h1ht	O
1	O
anew	O
=	O
t	O
−	O
1	O
(	O
cid:68	O
)	O
µnew	O
=	O
(	O
cid:104	O
)	O
h1	O
(	O
cid:105	O
)	O
(	O
cid:68	O
)	O
t−1	O
(	O
cid:88	O
)	O
σnew	O
=	O
t	O
(	O
cid:88	O
)	O
vt	O
(	O
cid:104	O
)	O
ht	O
(	O
cid:105	O
)	O
t	O
(	O
cid:16	O
)	O
(	O
cid:88	O
)	O
bnew	O
=	O
t=1	O
t=1	O
σnew	O
v	O
=	O
1	O
t	O
t	O
vtvt	O
(	O
cid:88	O
)	O
1	O
t	O
−	O
1	O
t	O
σnew	O
h	O
=	O
450	O
−	O
µµt	O
t=1	O
(	O
cid:68	O
)	O
htht	O
t	O
htht	O
t	O
(	O
cid:69	O
)	O
(	O
cid:33	O
)	O
−1	O
(	O
cid:69	O
)	O
(	O
cid:32	O
)	O
t−1	O
(	O
cid:88	O
)	O
(	O
cid:32	O
)	O
t	O
(	O
cid:88	O
)	O
(	O
cid:69	O
)	O
(	O
cid:33	O
)	O
−1	O
(	O
cid:68	O
)	O
t	O
−	O
vt	O
(	O
cid:104	O
)	O
ht	O
(	O
cid:105	O
)	O
t	O
bt	O
(	O
cid:17	O
)	O
(	O
cid:69	O
)	O
(	O
cid:16	O
)	O
(	O
cid:68	O
)	O
(	O
cid:68	O
)	O
t=1	O
ht+1ht	O
t+1	O
(	O
cid:69	O
)	O
(	O
cid:17	O
)	O
−	O
a	O
htht	O
t+1	O
(	O
24.5.5	O
)	O
(	O
24.5.9	O
)	O
(	O
24.5.10	O
)	O
(	O
24.5.11	O
)	O
(	O
24.5.12	O
)	O
(	O
24.5.13	O
)	O
if	O
b	O
is	O
updated	O
according	O
to	O
the	O
above	O
,	O
the	O
ﬁrst	O
equation	O
can	O
be	O
simpliﬁed	O
to	O
similarly	O
,	O
if	O
a	O
is	O
updated	O
according	O
to	O
em	O
algorithm	B
,	O
then	O
the	O
second	O
equation	B
can	O
be	O
simpliﬁed	O
to	O
(	O
24.5.14	O
)	O
draft	O
march	O
9	O
,	O
2010	O
learning	B
linear	O
dynamical	O
systems	O
the	O
statistics	O
required	O
therefore	O
include	O
smoothed	O
means	O
,	O
covariances	O
,	O
and	O
cross	B
moments	O
.	O
the	O
extension	O
to	O
learning	B
multiple	O
timeseries	O
is	O
straightforward	O
since	O
the	O
energy	B
is	O
simply	O
summed	O
over	O
the	O
individual	O
sequences	B
.	O
the	O
performance	B
of	O
the	O
em	O
algorithm	B
for	O
the	O
lds	O
often	O
depends	O
heavily	O
on	O
a	O
the	O
initialisation	O
.	O
if	O
we	O
remove	O
the	O
hidden	B
to	O
hidden	B
links	O
,	O
the	O
model	B
is	O
closely	O
related	O
to	O
factor	B
analysis	I
(	O
the	O
lds	O
can	O
be	O
considered	O
a	O
temporal	O
extension	O
of	O
factor	B
analysis	I
)	O
.	O
one	O
initialisation	O
technique	O
is	O
therefore	O
to	O
learn	O
the	O
b	O
matrix	B
using	O
factor	B
analysis	I
by	O
treating	O
the	O
observations	O
as	O
temporally	O
independent	O
.	O
24.5.3	O
subspace	O
methods	O
an	O
alternative	O
to	O
em	O
and	O
maximum	B
likelihood	I
training	O
of	O
an	O
lds	O
is	O
to	O
use	O
a	O
subspace	B
method	I
[	O
281	O
,	O
249	O
]	O
.	O
the	O
chief	O
beneﬁt	O
of	O
these	O
techniques	O
is	O
that	O
they	O
avoid	O
the	O
convergence	O
diﬃculties	O
of	O
em	O
.	O
to	O
motivate	O
subspace	O
techniques	O
,	O
consider	O
a	O
deterministic	B
lds	O
vt	O
=	O
bht	O
ht	O
=	O
aht−1	O
,	O
(	O
24.5.15	O
)	O
under	O
this	O
assumption	O
,	O
vt	O
=	O
bht	O
=	O
baht−1	O
and	O
,	O
more	O
generally	O
,	O
vt	O
=	O
bath1	O
.	O
this	O
means	O
that	O
a	O
low	B
dimensional	I
system	O
underlies	O
all	O
visible	B
information	O
since	O
all	O
points	O
ath1	O
lie	O
in	O
a	O
h-dimensional	O
subspace	O
,	O
which	O
is	O
then	O
projected	O
to	O
form	O
the	O
observation	O
.	O
this	O
suggests	O
that	O
some	O
form	O
of	O
subspace	O
identiﬁcation	O
technique	O
will	O
enable	O
us	O
to	O
learn	O
a	O
and	O
b.	O
given	O
a	O
set	O
of	O
observation	O
vectors	O
v1	O
,	O
.	O
.	O
.	O
,	O
vt	O
,	O
consider	O
the	O
block	O
hankel	O
matrix	B
formed	O
from	O
stacking	O
the	O
vectors	O
.	O
for	O
an	O
order	O
l	O
matrix	B
,	O
this	O
is	O
a	O
v	O
l×	O
t	O
−	O
l	O
+	O
1	O
matrix	O
.	O
for	O
example	O
,	O
for	O
t	O
=	O
6	O
and	O
l	O
=	O
3	O
,	O
this	O
is	O
	O
=	O
	O
b	O
ba	O
ba2	O
	O
(	O
h1	O
h2	O
h3	O
h4	O
)	O
(	O
24.5.16	O
)	O
(	O
24.5.17	O
)	O
(	O
24.5.18	O
)	O
m	O
=	O
m	O
=	O
	O
v2	O
v3	O
v4	O
v5	O
v3	O
v4	O
v5	O
v6	O
	O
v1	O
v2	O
v3	O
v4	O
	O
bh1	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
bh2	O
w	O
we	O
now	O
ﬁnd	O
the	O
svd	O
of	O
m	O
,	O
m	O
=	O
ˆu	O
ˆs	O
ˆvt	O
if	O
the	O
v	O
are	O
generated	O
from	O
a	O
(	O
noise	O
free	O
)	O
lds	O
,	O
we	O
can	O
write	O
bh4	O
bah1	O
bah2	O
bah3	O
bah4	O
ba2h1	O
ba2h2	O
ba2h3	O
ba2h4	O
bh3	O
t	O
(	O
cid:88	O
)	O
t=2	O
where	O
w	O
is	O
termed	O
the	O
extended	B
observability	I
matrix	I
.	O
the	O
matrix	B
ˆs	O
will	O
contain	O
the	O
singular	B
values	O
up	O
to	O
the	O
dimension	O
of	O
the	O
hidden	B
variables	I
h	O
,	O
with	O
the	O
remaining	O
singular	B
values	O
0.	O
from	O
equation	B
(	O
24.5.17	O
)	O
,	O
this	O
means	O
that	O
the	O
emission	B
matrix	I
b	O
is	O
contained	O
in	O
ˆu1	O
:	O
v,1	O
:	O
h.	O
the	O
estimated	O
hidden	B
variables	I
are	O
then	O
contained	O
in	O
the	O
submatrix	O
w1	O
:	O
h,1	O
:	O
t−l+1	O
,	O
(	O
h1	O
h2	O
h3	O
h4	O
)	O
=	O
w1	O
:	O
h,1	O
:	O
t−l+1	O
(	O
24.5.19	O
)	O
based	O
on	O
the	O
relation	O
ht	O
=	O
aht−1	O
one	O
can	O
then	O
ﬁnd	O
the	O
best	O
least	O
squares	O
estimate	O
for	O
a	O
by	O
minimising	O
(	O
ht	O
−	O
aht−1	O
)	O
2	O
(	O
24.5.20	O
)	O
for	O
which	O
the	O
optimal	O
solution	O
is	O
.	O
.	O
.	O
ht−1	O
)	O
†	O
a	O
=	O
(	O
h2	O
h3	O
.	O
.	O
.	O
ht	O
)	O
(	O
h1	O
h2	O
(	O
24.5.21	O
)	O
where	O
†	O
denotes	O
the	O
pseudo	B
inverse	I
,	O
see	O
ldssubspace.m	O
.	O
estimates	O
for	O
the	O
covariance	B
matrices	O
can	O
also	O
be	O
obtained	O
from	O
the	O
residual	O
errors	O
in	O
ﬁtting	O
the	O
block	O
hankel	O
matrix	B
(	O
σv	O
)	O
and	O
extended	B
observability	I
matrix	I
(	O
σh	O
)	O
.	O
whilst	O
this	O
derivation	O
formally	O
holds	O
only	O
for	O
the	O
noise	O
free	O
case	O
one	O
can	O
nevertheless	O
apply	O
this	O
in	O
the	O
case	O
of	O
non-zero	O
noise	O
and	O
hope	O
to	O
gain	O
an	O
estimate	O
for	O
a	O
and	O
b	O
that	O
is	O
correct	O
in	O
the	O
mean	B
.	O
in	O
addition	O
to	O
forming	O
a	O
solution	O
in	O
its	O
own	O
right	O
,	O
the	O
subspace	B
method	I
forms	O
a	O
potentially	O
useful	O
way	O
to	O
initialise	O
the	O
em	O
algorithm	B
.	O
draft	O
march	O
9	O
,	O
2010	O
451	O
switching	B
auto-regressive	O
models	O
s1	O
v1	O
s2	O
v2	O
s3	O
v3	O
s4	O
v4	O
figure	O
24.7	O
:	O
a	O
ﬁrst	B
order	I
switching	O
ar	O
model	B
.	O
in	O
terms	O
of	O
inference	B
,	O
conditioned	O
on	O
v1	O
:	O
t	O
,	O
this	O
is	O
a	O
hmm	O
.	O
24.5.4	O
structured	B
ldss	O
many	O
physical	O
equations	O
are	O
local	B
both	O
in	O
time	O
and	O
space	O
.	O
for	O
example	O
in	O
weather	O
models	O
the	O
atmosphere	O
is	O
partitioned	B
into	O
cells	O
hi	O
(	O
t	O
)	O
each	O
containing	O
the	O
pressure	O
at	O
that	O
location	O
.	O
the	O
equations	O
describing	O
how	O
the	O
pressure	O
updates	O
only	O
depend	O
on	O
the	O
pressure	O
at	O
the	O
current	O
cell	O
and	O
small	O
number	O
of	O
neighbouring	O
cells	O
at	O
the	O
previous	O
time	O
t	O
−	O
1.	O
if	O
we	O
use	O
a	O
linear	B
model	I
and	O
measure	O
some	O
aspects	O
of	O
the	O
cells	O
at	O
each	O
time	O
,	O
then	O
the	O
weather	O
is	O
describable	O
by	O
a	O
lds	O
with	O
a	O
highly	O
structured	B
sparse	O
transition	B
matrix	I
a.	O
in	O
practice	O
,	O
the	O
weather	O
models	O
are	O
non-linear	B
but	O
local	B
linear	O
approximations	O
are	O
often	O
employed	O
[	O
254	O
]	O
.	O
a	O
similar	O
situation	O
arises	O
in	O
brain	O
imaging	O
in	O
which	O
voxels	O
(	O
local	B
cubes	O
of	O
activity	O
)	O
depend	O
only	O
on	O
their	O
neighbours	O
from	O
the	O
previous	O
timestep	O
[	O
101	O
]	O
.	O
another	O
application	O
of	O
structured	B
ldss	O
is	O
in	O
temporal	O
independent	O
component	O
analysis	B
.	O
this	O
is	O
deﬁned	O
as	O
the	O
discovery	O
of	O
a	O
set	O
of	O
independent	O
latent	O
dynamical	O
processes	O
,	O
from	O
which	O
the	O
data	B
is	O
a	O
projected	O
observation	O
.	O
if	O
each	O
independent	O
dynamical	O
process	O
can	O
itself	O
be	O
described	O
by	O
a	O
lds	O
,	O
this	O
gives	O
rise	O
to	O
a	O
structured	B
lds	O
with	O
a	O
block	O
diagonal	O
transition	B
matrix	I
a.	O
such	O
models	O
can	O
be	O
used	O
to	O
extract	O
independent	O
components	O
under	O
prior	B
knowledge	O
of	O
the	O
likely	O
underlying	O
frequencies	O
in	O
each	O
of	O
the	O
temporal	O
compoments	O
[	O
59	O
]	O
.	O
see	O
also	O
exercise	O
(	O
199	O
)	O
.	O
24.5.5	O
bayesian	O
ldss	O
p	O
(	O
v1	O
:	O
t	O
)	O
=	O
(	O
cid:82	O
)	O
the	O
extension	O
to	O
placing	O
priors	O
on	O
the	O
transition	O
and	O
emission	O
parameters	O
of	O
the	O
lds	O
leads	O
in	O
general	O
to	O
computational	O
diﬃculties	O
in	O
computing	O
the	O
likelihood	B
.	O
for	O
example	O
,	O
for	O
a	O
prior	B
on	O
a	O
,	O
the	O
likelihood	B
is	O
a	O
p	O
(	O
v1	O
:	O
t|a	O
)	O
p	O
(	O
a	O
)	O
which	O
is	O
diﬃcult	O
to	O
evaluate	O
since	O
the	O
dependence	B
of	O
the	O
likelihood	B
on	O
the	O
matrix	B
a	O
is	O
a	O
complicated	O
function	B
.	O
approximate	B
treatments	O
of	O
this	O
case	O
are	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
,	O
although	O
we	O
brieﬂy	O
note	O
that	O
sampling	B
methods	O
[	O
54	O
,	O
98	O
]	O
are	O
popular	O
in	O
this	O
context	O
,	O
in	O
addition	O
to	O
deterministic	B
variational	O
approximations	O
[	O
27	O
,	O
23	O
,	O
59	O
]	O
.	O
24.6	O
switching	B
auto-regressive	O
models	O
for	O
a	O
time-series	O
of	O
scalar	O
values	O
v1	O
:	O
t	O
an	O
lth	O
order	O
switching	B
ar	O
model	B
can	O
be	O
written	O
as	O
vt	O
=	O
ˆvt	O
t−1a	O
(	O
st	O
)	O
+	O
ηt	O
,	O
(	O
cid:0	O
)	O
ηt	O
0	O
,	O
σ2	O
(	O
st	O
)	O
(	O
cid:1	O
)	O
where	O
we	O
now	O
have	O
a	O
set	O
of	O
ar	O
coeﬃcients	O
θ	O
=	O
(	O
cid:8	O
)	O
a	O
(	O
s	O
)	O
,	O
σ2	O
(	O
s	O
)	O
,	O
s	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
s	O
}	O
themselves	O
have	O
a	O
markov	O
transition	O
p	O
(	O
s1	O
:	O
t	O
)	O
=	O
(	O
cid:81	O
)	O
p	O
(	O
v1	O
:	O
t	O
,	O
s1	O
:	O
t|θ	O
)	O
=	O
(	O
cid:89	O
)	O
p	O
(	O
vt|vt−1	O
,	O
.	O
.	O
.	O
,	O
vt−l	O
,	O
st	O
,	O
|θ	O
)	O
p	O
(	O
st|st−1	O
)	O
ηt	O
∼	O
n	O
t	O
t	O
p	O
(	O
st|st−1	O
)	O
so	O
that	O
the	O
full	O
model	B
is	O
(	O
cid:9	O
)	O
.	O
the	O
discrete	B
switch	O
variables	O
(	O
24.6.1	O
)	O
(	O
24.6.2	O
)	O
given	O
an	O
observed	O
sequence	O
v1	O
:	O
t	O
and	O
parameters	O
θ	O
inference	B
is	O
straightforward	O
since	O
this	O
is	O
a	O
form	O
of	O
hmm	O
.	O
to	O
make	O
this	O
more	O
apparent	O
we	O
may	O
write	O
24.6.1	O
inference	B
p	O
(	O
v1	O
:	O
t	O
,	O
s1	O
:	O
t	O
)	O
=	O
(	O
cid:89	O
)	O
t	O
where	O
ˆp	O
(	O
vt|st	O
)	O
p	O
(	O
st|st−1	O
)	O
ˆp	O
(	O
vt|st	O
)	O
≡	O
p	O
(	O
vt|vt−1	O
,	O
.	O
.	O
.	O
,	O
vt−l	O
,	O
st	O
)	O
=	O
n	O
452	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
vt	O
ˆvt	O
t−1a	O
(	O
st	O
)	O
,	O
σ2	O
(	O
st	O
)	O
(	O
24.6.3	O
)	O
(	O
24.6.4	O
)	O
draft	O
march	O
9	O
,	O
2010	O
switching	B
auto-regressive	O
models	O
figure	O
24.8	O
:	O
learning	B
a	O
switching	B
ar	O
model	B
.	O
the	O
upper	O
plot	O
shows	O
the	O
train-	O
ing	O
data	B
.	O
the	O
colour	O
indicates	O
which	O
of	O
the	O
two	O
ar	O
models	O
is	O
active	B
at	O
that	O
time	O
.	O
whilst	O
this	O
information	O
is	O
plotted	O
here	O
,	O
this	O
is	O
assumed	O
unknown	O
to	O
the	O
learning	B
algorithm	O
,	O
as	O
are	O
the	O
coeﬃcients	O
a	O
(	O
s	O
)	O
.	O
we	O
assume	O
that	O
the	O
order	O
l	O
=	O
2	O
and	O
num-	O
ber	O
of	O
switches	O
s	O
=	O
2	O
however	O
is	O
known	O
.	O
in	O
the	O
bottom	O
plot	O
we	O
show	O
the	O
time	O
se-	O
ries	O
again	O
after	O
training	B
in	O
which	O
we	O
colour	O
the	O
points	O
according	O
to	O
the	O
most	O
likely	O
smoothed	O
ar	O
model	B
at	O
each	O
timestep	O
.	O
see	O
demosarlearn.m	O
.	O
note	O
that	O
the	O
emission	B
distribution	I
ˆp	O
(	O
vt|st	O
)	O
is	O
time-dependent	O
.	O
the	O
ﬁltering	B
recursion	O
is	O
then	O
ˆp	O
(	O
vt|st	O
)	O
p	O
(	O
st|st−1	O
)	O
α	O
(	O
st−1	O
)	O
(	O
24.6.5	O
)	O
α	O
(	O
st	O
)	O
=	O
(	O
cid:88	O
)	O
st−1	O
smoothing	B
can	O
be	O
achieved	O
using	O
the	O
standard	O
recursions	O
,	O
modiﬁed	O
to	O
use	O
the	O
time-dependent	O
emissions	O
,	O
see	O
demosarinference.m	O
.	O
24.6.2	O
maximum	B
likelihood	I
learning	O
using	O
em	O
to	O
ﬁt	O
the	O
set	O
of	O
ar	O
coeﬃcients	O
and	O
innovation	O
variances	O
,	O
a	O
(	O
s	O
)	O
,	O
σ2	O
(	O
s	O
)	O
,	O
s	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
s	O
,	O
using	O
maximum	O
like-	O
lihood	O
training	B
for	O
a	O
set	O
of	O
data	B
v1	O
:	O
t	O
,	O
we	O
may	O
make	O
use	O
of	O
the	O
em	O
algorithm	B
.	O
which	O
we	O
need	O
to	O
maximise	O
with	O
respect	O
to	O
the	O
parameters	O
θ.	O
using	O
the	O
deﬁnition	O
of	O
the	O
emission	O
and	O
isolating	O
the	O
dependency	O
on	O
a	O
,	O
we	O
have	O
t	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
st|st−1	O
)	O
(	O
cid:105	O
)	O
pold	O
(	O
st	O
,	O
st−1	O
)	O
(	O
24.6.6	O
)	O
m-step	O
up	O
to	O
negligible	O
constants	O
,	O
the	O
energy	B
is	O
given	O
by	O
t	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
vt|ˆvt−1	O
,	O
a	O
(	O
st	O
)	O
)	O
(	O
cid:105	O
)	O
pold	O
(	O
st|v1	O
:	O
t	O
)	O
+	O
(	O
cid:88	O
)	O
(	O
cid:28	O
)	O
1	O
(	O
cid:17	O
)	O
2	O
(	O
cid:16	O
)	O
σ2	O
(	O
st	O
)	O
vt	O
−	O
ˆvt	O
t−1a	O
(	O
st	O
)	O
e	O
=	O
(	O
cid:88	O
)	O
−2e	O
=	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
t	O
(	O
cid:34	O
)	O
(	O
cid:88	O
)	O
(	O
cid:29	O
)	O
pold	O
(	O
st|v1	O
:	O
t	O
)	O
(	O
cid:35	O
)	O
+	O
log	O
σ2	O
(	O
st	O
)	O
+	O
const	O
.	O
(	O
24.6.7	O
)	O
on	O
diﬀerentiating	O
with	O
respect	O
to	O
a	O
(	O
s	O
)	O
and	O
equating	O
to	O
zero	O
,	O
the	O
optimal	O
a	O
(	O
s	O
)	O
satisﬁes	O
the	O
linear	B
equation	O
which	O
may	O
be	O
solved	O
using	O
gaussian	O
elimination	O
.	O
similarly	O
one	O
may	O
show	O
that	O
updates	O
that	O
maximise	O
the	O
energy	B
with	O
respect	O
to	O
σ2	O
are	O
pold	O
(	O
st	O
=	O
s|v1	O
:	O
t	O
)	O
vtˆvt−1	O
σ2	O
(	O
s	O
)	O
=	O
t	O
σ2	O
(	O
s	O
)	O
=	O
(	O
cid:80	O
)	O
1	O
t	O
(	O
cid:48	O
)	O
pold	O
(	O
s	O
(	O
cid:48	O
)	O
t	O
=	O
s|v1	O
:	O
t	O
)	O
t−1	O
ˆvt−1ˆvt	O
σ2	O
(	O
s	O
)	O
t	O
pold	O
(	O
st	O
=	O
s|v1	O
:	O
t	O
)	O
(	O
cid:104	O
)	O
vt	O
−	O
ˆvt	O
pold	O
(	O
st	O
=	O
s|v1	O
:	O
t	O
)	O
(	O
cid:88	O
)	O
t	O
(	O
cid:105	O
)	O
2	O
t−1a	O
(	O
st	O
)	O
a	O
(	O
s	O
)	O
(	O
24.6.8	O
)	O
the	O
update	O
for	O
p	O
(	O
st|st−1	O
)	O
follows	O
the	O
standard	O
em	O
for	O
hmm	O
rule	O
,	O
equation	B
(	O
23.3.5	O
)	O
,	O
see	O
sarlearn.m	O
.	O
here	O
we	O
don	O
’	O
t	O
include	O
an	O
update	O
for	O
the	O
prior	B
p	O
(	O
s1	O
)	O
since	O
there	O
is	O
insuﬃcient	O
information	O
at	O
the	O
start	O
of	O
the	O
sequence	O
and	O
assume	O
p	O
(	O
s1	O
)	O
is	O
ﬂat	O
.	O
with	O
high	O
frequency	O
data	B
it	O
is	O
unlikely	O
that	O
a	O
change	O
in	O
the	O
switch	O
variable	B
is	O
reasonable	O
at	O
each	O
time	O
t.	O
a	O
simple	O
constraint	O
to	O
account	O
for	O
this	O
is	O
to	O
use	O
a	O
modiﬁed	O
transition	O
(	O
cid:26	O
)	O
p	O
(	O
st|st−1	O
)	O
ˆp	O
(	O
st|st−1	O
)	O
=	O
δ	O
(	O
st	O
−	O
st−1	O
)	O
otherwise	O
mod	O
(	O
t	O
,	O
tskip	O
)	O
=	O
0	O
draft	O
march	O
9	O
,	O
2010	O
(	O
24.6.9	O
)	O
(	O
24.6.10	O
)	O
453	O
050100150200250300350400−100−50050100sample	O
switches050100150200250300350400−100−50050100learned	O
switches	O
code	O
s1	O
v1	O
˜v1	O
s2	O
v2	O
˜v2	O
s3	O
v3	O
˜v3	O
s4	O
v4	O
˜v4	O
figure	O
24.9	O
:	O
(	O
a	O
)	O
:	O
a	O
latent	B
switching	O
(	O
second	O
order	O
)	O
ar	O
model	B
.	O
here	O
the	O
st	O
indicates	O
which	O
of	O
a	O
set	O
of	O
10	O
available	O
ar	O
models	O
is	O
active	B
at	O
time	O
t.	O
the	O
square	O
nodes	O
emphasise	O
that	O
these	O
are	O
discrete	B
variables	O
.	O
the	O
‘	O
clean	O
’	O
ar	O
signal	O
vt	O
,	O
which	O
is	O
not	O
observed	O
,	O
is	O
corrupted	O
by	O
additive	O
noise	O
to	O
form	O
the	O
noisy	O
observations	O
˜vt	O
.	O
in	O
terms	O
of	O
inference	B
,	O
conditioned	O
on	O
˜v1	O
:	O
t	O
,	O
this	O
can	O
be	O
expressed	O
as	O
a	O
switching	B
lds	O
,	O
chapter	O
(	O
25	O
)	O
.	O
(	O
b	O
)	O
:	O
signal	O
reconstruction	O
using	O
the	O
latent	B
switching	O
ar	O
model	B
in	O
(	O
a	O
)	O
.	O
top	O
:	O
noisy	O
signal	O
˜v1	O
:	O
t	O
;	O
bottom	O
:	O
reconstructed	O
clean	O
signal	O
v1	O
:	O
t	O
.	O
the	O
dashed	O
lines	O
and	O
the	O
numbers	O
show	O
the	O
most-likely	O
state	O
segmentation	O
arg	O
maxs1	O
:	O
t	O
p	O
(	O
s1	O
:	O
t|˜v1	O
:	O
t	O
)	O
.	O
e-step	O
the	O
m-step	O
requires	O
the	O
smoothed	O
statistics	O
pold	O
(	O
st	O
=	O
s|v1	O
:	O
t	O
)	O
and	O
pold	O
(	O
st	O
=	O
s	O
,	O
st−1	O
=	O
s	O
(	O
cid:48	O
)	O
obtained	O
from	O
hmm	O
inference	B
.	O
|v1	O
:	O
t	O
)	O
which	O
can	O
be	O
example	O
103	O
(	O
learning	B
a	O
switching	B
ar	O
model	B
)	O
.	O
in	O
ﬁg	O
(	O
24.8	O
)	O
the	O
training	B
data	O
is	O
generated	O
by	O
an	O
switch-	O
ing	O
ar	O
model	B
so	O
that	O
we	O
know	O
the	O
ground	O
truth	O
as	O
to	O
which	O
model	B
generated	O
which	O
parts	O
of	O
the	O
data	B
.	O
based	O
on	O
the	O
training	B
data	O
(	O
assuming	O
the	O
labels	O
st	O
are	O
unknown	O
)	O
,	O
a	O
switching	B
ar	O
model	B
is	O
ﬁtted	O
using	O
em	O
.	O
in	O
this	O
case	O
the	O
problem	B
is	O
straightforward	O
so	O
that	O
a	O
good	O
estimate	O
is	O
obtained	O
of	O
both	O
the	O
sets	O
of	O
ar	O
parameters	O
and	O
which	O
switches	O
were	O
used	O
at	O
which	O
time	O
.	O
example	O
104	O
(	O
modelling	B
parts	O
of	O
speech	O
)	O
.	O
in	O
ﬁg	O
(	O
24.9	O
)	O
a	O
segment	O
of	O
a	O
speech	O
signal	O
is	O
shown	O
described	O
by	O
a	O
switching	B
ar	O
model	B
.	O
each	O
of	O
the	O
10	O
available	O
ar	O
models	O
is	O
responsible	O
for	O
modelling	B
the	O
dynamics	O
of	O
a	O
basic	O
subunit	O
of	O
speech	O
[	O
90	O
]	O
[	O
192	O
]	O
.	O
the	O
model	B
was	O
trained	O
on	O
many	O
example	O
sequences	B
using	O
s	O
=	O
10	O
states	O
with	O
a	O
left-to-right	O
transition	B
matrix	I
.	O
the	O
interest	O
is	O
to	O
determine	O
when	O
each	O
subunit	O
is	O
most	O
likely	O
to	O
be	O
active	B
.	O
this	O
corresponds	O
to	O
the	O
computation	O
of	O
the	O
most-likely	O
switch	O
path	B
s1	O
:	O
t	O
given	O
the	O
observed	O
signal	O
p	O
(	O
s1	O
:	O
t|˜v1	O
:	O
t	O
)	O
.	O
24.7	O
code	O
in	O
the	O
linear	B
dynamical	I
system	I
code	O
below	O
only	O
the	O
simplest	O
form	O
of	O
the	O
recursions	O
is	O
given	O
.	O
no	O
attempt	O
has	O
been	O
made	O
to	O
ensure	O
numerical	B
stability	I
.	O
ldsforwardupdate.m	O
:	O
lds	O
forward	O
ldsbackwardupdate.m	O
:	O
lds	O
backward	O
ldssmooth.m	O
:	O
linear	B
dynamical	I
system	I
:	O
ﬁltering	B
and	O
smoothing	B
ldsforward.m	O
:	O
alternative	O
lds	O
forward	O
algorithm	O
(	O
see	O
slds	O
chapter	O
)	O
ldsbackward.m	O
:	O
alternative	O
lds	O
backward	O
algorithm	B
(	O
see	O
slds	O
chapter	O
)	O
demosumprodgausscanonlds.m	O
:	O
sum-product	B
algorithm	I
for	O
smoothed	O
inference	B
demoldstracking.m	O
:	O
demo	O
of	O
tracking	O
in	O
a	O
newtonian	O
system	B
454	O
draft	O
march	O
9	O
,	O
2010	O
1234567891012345678910	O
exercises	O
ldssubspace.m	O
:	O
subspace	O
learning	O
(	O
hankel	O
matrix	B
method	O
)	O
demoldssubspace.m	O
:	O
demo	O
of	O
subspace	O
learning	O
method	O
24.7.1	O
autoregressive	O
models	O
note	O
that	O
in	O
the	O
code	O
the	O
autoregressive	O
vector	O
a	O
has	O
as	O
its	O
last	O
entry	O
the	O
ﬁrst	O
ar	O
coeﬃcient	O
(	O
i.e	O
.	O
reverse	O
order	O
to	O
that	O
presented	O
in	O
the	O
text	O
)	O
.	O
artrain.m	O
:	O
learn	O
ar	O
coeﬃcients	O
(	O
gaussian	O
elimination	O
)	O
demoartrain.m	O
:	O
demo	O
of	O
ﬁtting	O
an	O
ar	O
model	B
to	O
data	B
arlds.m	O
:	O
learn	O
ar	O
coeﬃcients	O
using	O
a	O
lds	O
demoarlds.m	O
:	O
demo	O
of	O
learning	B
ar	O
coeﬃcients	O
using	O
an	O
lds	O
demosarinference.m	O
:	O
demo	O
for	O
inference	B
in	O
a	O
switching	B
autoregressive	O
model	B
in	O
in	O
sarlearn.m	O
a	O
slight	O
fudge	O
is	O
used	O
since	O
we	O
do	O
not	O
deal	O
fully	O
with	O
the	O
case	O
at	O
the	O
start	O
where	O
there	O
is	O
insuﬃcient	O
information	O
to	O
deﬁne	O
the	O
ar	O
model	B
.	O
for	O
long	O
timeseries	O
this	O
will	O
have	O
a	O
negligible	O
eﬀect	O
,	O
although	O
it	O
might	O
lead	O
to	O
small	O
decreases	O
in	O
the	O
log	O
likelihood	B
under	O
the	O
em	O
algorithm	B
.	O
sarlearn.m	O
:	O
learning	B
of	O
a	O
sar	O
using	O
em	O
demosarlearn.m	O
:	O
demo	O
of	O
sar	O
learning	B
hmmforwardsar.m	O
:	O
switching	B
autoregressive	O
hmm	O
forward	O
pass	O
hmmbackwardsar.m	O
:	O
switching	B
autoregressive	O
hmm	O
backward	O
pass	O
24.8	O
exercises	O
exercise	O
234.	O
consider	O
the	O
two-dimension	O
linear	B
model	I
ht	O
=	O
rθht−1	O
where	O
rθ	O
=	O
rθ	O
is	O
rotation	O
matrix	B
which	O
rotates	O
the	O
vector	O
ht	O
through	O
angle	O
θ	O
in	O
one	O
timestep	O
.	O
(	O
cid:19	O
)	O
sin	O
θ	O
(	O
cid:18	O
)	O
cos	O
θ	O
−	O
sin	O
θ	O
(	O
cid:19	O
)	O
cos	O
θ	O
=	O
1.	O
by	O
writing	O
(	O
cid:18	O
)	O
xt	O
(	O
cid:18	O
)	O
r11	O
r12	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
xt−1	O
(	O
cid:19	O
)	O
yt	O
r21	O
r22	O
yt−1	O
(	O
24.8.1	O
)	O
(	O
24.8.2	O
)	O
(	O
24.8.3	O
)	O
eliminate	O
yt	O
to	O
write	O
an	O
equation	B
for	O
xt+1	O
in	O
terms	O
of	O
xt	O
and	O
xt−1	O
.	O
2.	O
explain	O
why	O
the	O
eigenvalues	O
of	O
a	O
rotation	O
matrix	B
are	O
(	O
in	O
general	O
)	O
imaginary	O
.	O
3.	O
explain	O
how	O
to	O
model	B
a	O
sinusoid	O
,	O
rotating	O
with	O
angular	O
velocity	O
ω	O
using	O
a	O
two-dimensional	O
lds	O
.	O
4.	O
explain	O
how	O
to	O
model	B
a	O
sinusoid	O
using	O
an	O
ar	O
model	B
.	O
5.	O
explain	O
the	O
relationship	O
between	O
the	O
second	O
order	O
diﬀerential	B
equation	O
¨x	O
=	O
−λx	O
,	O
which	O
describes	O
a	O
harmonic	O
oscillator	O
,	O
and	O
the	O
second	O
order	O
diﬀerence	O
equation	B
which	O
approximates	O
this	O
diﬀeren-	O
tial	O
equation	B
.	O
is	O
it	O
possible	O
to	O
ﬁnd	O
a	O
diﬀerence	O
equation	B
which	O
exactly	O
matches	O
the	O
solution	O
of	O
the	O
diﬀerential	B
equation	O
at	O
chosen	O
points	O
?	O
exercise	O
235.	O
show	O
that	O
for	O
any	O
anti-symmetric	O
matrix	B
m	O
,	O
m	O
=	O
−mt	O
the	O
matrix	B
exponential	O
(	O
in	O
matlab	O
this	O
is	O
expm	O
)	O
a	O
=	O
em	O
draft	O
march	O
9	O
,	O
2010	O
(	O
24.8.4	O
)	O
(	O
24.8.5	O
)	O
455	O
is	O
orthogonal	B
,	O
namely	O
ata	O
=	O
i	O
exercises	O
(	O
24.8.6	O
)	O
explain	O
how	O
one	O
may	O
then	O
construct	O
random	O
orthogonal	O
matrices	O
with	O
some	O
control	O
over	O
the	O
angles	O
of	O
the	O
complex	O
eigenvalues	O
.	O
discuss	O
how	O
this	O
relates	O
to	O
the	O
frequencies	O
encountered	O
in	O
a	O
lds	O
where	O
a	O
is	O
the	O
transition	B
matrix	I
.	O
exercise	O
236.	O
run	O
the	O
demo	O
demoldstracking.m	O
which	O
tracks	O
a	O
ballistic	O
object	O
using	O
a	O
linear	B
dynamical	I
system	I
,	O
see	O
example	O
(	O
102	O
)	O
.	O
modify	O
demoldstracking.m	O
so	O
that	O
in	O
addition	O
to	O
the	O
x	O
and	O
y	O
positions	O
,	O
the	O
x	O
speed	O
is	O
also	O
observed	O
.	O
compare	O
and	O
contrast	O
the	O
accuracy	O
of	O
the	O
tracking	O
with	O
and	O
without	O
this	O
extra	O
information	O
.	O
exercise	O
237.	O
nightsong.mat	O
contains	O
a	O
small	O
stereo	O
segment	O
nightingale	O
song	O
sampled	O
at	O
44100	O
hertz	O
.	O
1.	O
plot	O
the	O
original	O
waveform	O
using	O
plot	O
(	O
x	O
(	O
:	O
,1	O
)	O
)	O
2.	O
download	O
the	O
program	O
myspecgram.m	O
from	O
labrosa.ee.columbia.edu/matlab/sgram/myspecgram.m	O
and	O
plot	O
the	O
spectrogram	B
y=myspecgram	O
(	O
x,1024,44100	O
)	O
;	O
imagesc	O
(	O
log	O
(	O
abs	O
(	O
y	O
)	O
)	O
)	O
3.	O
the	O
routine	O
demogmmem.m	O
demonstrates	O
ﬁtting	O
a	O
mixture	O
of	O
gaussians	O
to	O
data	B
.	O
the	O
mixture	B
assign-	O
ment	O
probabilities	O
are	O
contained	O
in	O
phgn	O
.	O
write	O
a	O
routine	O
to	O
cluster	O
the	O
data	B
v=log	O
(	O
abs	O
(	O
y	O
)	O
)	O
using	O
8	O
gaussian	O
components	O
,	O
and	O
explain	O
how	O
one	O
might	O
segment	O
the	O
series	O
x	O
into	O
diﬀerent	O
regions	O
.	O
4.	O
examine	O
the	O
routine	O
demoarlds.m	O
which	O
ﬁts	O
autoregressive	O
coeﬃcients	O
using	O
an	O
interpretation	O
as	O
a	O
linear	B
dynamical	I
system	I
.	O
adapt	O
the	O
routine	O
demoarlds.m	O
to	O
learn	O
the	O
ar	O
coeﬃcients	O
of	O
the	O
data	B
x.	O
you	O
will	O
almost	O
certainly	O
need	O
to	O
subsample	O
the	O
data	B
x	O
–	O
for	O
example	O
by	O
taking	O
every	O
4th	O
datapoint	O
.	O
with	O
the	O
learned	O
ar	O
coeﬃcients	O
(	O
use	O
the	O
smoothed	O
results	O
)	O
ﬁt	O
a	O
gaussian	O
mixture	B
with	O
8	O
components	O
.	O
compare	O
and	O
contrast	O
your	O
results	O
with	O
those	O
obtained	O
from	O
the	O
gaussian	O
mixture	B
model	I
ﬁt	O
to	O
the	O
spectrogram	B
.	O
exercise	O
238.	O
consider	O
a	O
supervised	B
learning	I
problem	O
in	O
which	O
we	O
make	O
a	O
linear	B
model	I
of	O
the	O
scaler	O
output	O
yt	O
based	O
on	O
vector	O
input	O
xt	O
:	O
t	O
where	O
ηy	O
t	O
xt	O
+	O
ηy	O
yt	O
=	O
wt	O
t	O
is	O
zero	O
mean	B
gaussian	O
noise	O
.	O
training	B
data	O
d	O
=	O
{	O
(	O
xt	O
,	O
yt	O
)	O
,	O
t	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
t	O
}	O
is	O
available	O
.	O
(	O
24.8.7	O
)	O
1.	O
for	O
a	O
time-invariant	O
weight	O
vector	O
wt	O
≡	O
w	O
,	O
explain	O
how	O
to	O
ﬁnd	O
the	O
single	O
weight	B
vector	O
w	O
and	O
the	O
noise	O
variance	B
σ2	O
by	O
maximum	B
likelihood	I
.	O
2.	O
extend	O
the	O
above	O
model	B
to	O
include	O
a	O
transition	O
wt	O
=	O
wt−1	O
+	O
ηw	O
t	O
(	O
24.8.8	O
)	O
where	O
ηw	O
is	O
zero	O
mean	B
gaussian	O
noise	O
with	O
a	O
given	O
covariance	B
σ	O
;	O
w1	O
has	O
zero	O
mean	B
.	O
explain	O
t	O
how	O
to	O
cast	O
ﬁnding	O
(	O
cid:104	O
)	O
wt|d	O
(	O
cid:105	O
)	O
as	O
smoothing	O
in	O
a	O
related	O
linear	B
dynamical	I
system	I
.	O
write	O
a	O
routine	O
w	O
=	O
linpredar	O
(	O
x	O
,	O
y	O
,	O
sigmaw	O
,	O
sigmay	O
)	O
that	O
takes	O
an	O
input	O
data	B
matrix	O
x	O
=	O
[	O
x1	O
,	O
.	O
.	O
.	O
,	O
xt	O
]	O
where	O
each	O
column	O
contains	O
an	O
input	O
,	O
and	O
vector	O
y	O
=	O
[	O
y1	O
,	O
.	O
.	O
.	O
,	O
yt	O
]	O
t	O
;	O
sigmaw	O
is	O
the	O
additive	O
weight	B
noise	O
and	O
sigmay	O
is	O
an	O
assumed	O
known	O
time-invariant	O
output	O
noise	O
.	O
the	O
returned	O
w	O
contains	O
the	O
smoothed	O
mean	B
weights	O
.	O
456	O
draft	O
march	O
9	O
,	O
2010	O
chapter	O
25	O
switching	O
linear	O
dynamical	O
systems	O
25.1	O
introduction	O
complex	O
timeseries	O
which	O
are	O
not	O
well	O
described	O
globally	O
by	O
a	O
single	O
linear	B
dynamical	I
system	I
may	O
be	O
divided	O
into	O
segments	O
,	O
each	O
modelled	O
by	O
a	O
potentially	O
diﬀerent	O
lds	O
.	O
such	O
models	O
can	O
handle	O
situations	O
in	O
which	O
the	O
underlying	O
model	B
‘	O
jumps	O
’	O
from	O
one	O
parameter	B
setting	O
to	O
another	O
.	O
for	O
example	O
a	O
single	O
lds	O
might	O
well	O
represent	O
the	O
normal	B
ﬂows	O
in	O
a	O
chemical	O
plant	O
.	O
when	O
a	O
break	O
in	O
a	O
pipeline	O
occurs	O
,	O
the	O
dynamics	O
of	O
the	O
system	B
changes	O
from	O
one	O
set	O
of	O
linear	B
ﬂow	O
equations	O
to	O
another	O
.	O
this	O
scenario	O
can	O
be	O
modelled	O
suing	O
a	O
sets	O
of	O
two	O
linear	B
systems	O
,	O
each	O
with	O
diﬀerent	O
parameters	O
.	O
the	O
discrete	B
latent	O
variable	B
at	O
each	O
time	O
st	O
∈	O
{	O
normal	B
,	O
pipe	O
broken	O
}	O
indicates	O
which	O
of	O
the	O
ldss	O
is	O
most	O
appropriate	O
at	O
the	O
current	O
time	O
.	O
this	O
is	O
called	O
a	O
switching	B
lds	O
and	O
used	O
in	O
many	O
disciplines	O
,	O
from	O
econometrics	O
to	O
machine	O
learning	B
[	O
13	O
,	O
110	O
,	O
174	O
,	O
161	O
,	O
160	O
,	O
60	O
,	O
57	O
,	O
218	O
,	O
303	O
,	O
175	O
]	O
.	O
25.2	O
the	O
switching	B
lds	O
at	O
each	O
time	O
t	O
,	O
a	O
switch	O
variable	B
st	O
∈	O
1	O
,	O
.	O
.	O
.	O
,	O
s	O
describes	O
which	O
of	O
a	O
set	O
of	O
ldss	O
is	O
to	O
be	O
used	O
.	O
the	O
observation	O
(	O
or	O
‘	O
visible	B
’	O
)	O
variable	B
vt	O
∈	O
rv	O
is	O
linearly	O
related	O
to	O
the	O
hidden	B
state	O
ht	O
∈	O
rh	O
by	O
vt	O
=	O
b	O
(	O
st	O
)	O
ht	O
+	O
ηv	O
(	O
st	O
)	O
,	O
ηv	O
(	O
st	O
)	O
∼	O
n	O
(	O
ηv	O
(	O
st	O
)	O
¯v	O
(	O
st	O
)	O
,	O
σv	O
(	O
st	O
)	O
)	O
(	O
25.2.1	O
)	O
here	O
st	O
describes	O
which	O
of	O
the	O
set	O
of	O
emission	O
matrices	O
b	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
b	O
(	O
s	O
)	O
is	O
active	B
at	O
time	O
t.	O
the	O
observation	O
noise	O
ηv	O
(	O
st	O
)	O
is	O
drawn	O
from	O
a	O
gaussian	O
with	O
mean	O
¯v	O
(	O
st	O
)	O
and	O
covariance	B
σv	O
(	O
st	O
)	O
.	O
the	O
transition	O
dynamics	O
of	O
the	O
continuous	B
hidden	O
state	O
ht	O
is	O
linear	B
,	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
ht	O
=	O
a	O
(	O
st	O
)	O
ht−1	O
+	O
ηh	O
(	O
st	O
)	O
,	O
ηh	O
(	O
st	O
)	O
∼	O
n	O
ηh	O
(	O
st	O
)	O
¯h	O
(	O
st	O
)	O
,	O
σh	O
(	O
st	O
)	O
(	O
25.2.2	O
)	O
and	O
the	O
switch	O
variable	B
st	O
selects	O
a	O
single	O
transition	B
matrix	I
from	O
the	O
available	O
set	O
a	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
a	O
(	O
s	O
)	O
.	O
the	O
gaussian	O
transition	O
noise	O
ηh	O
(	O
st	O
)	O
also	O
depends	O
on	O
the	O
switch	O
variable	B
.	O
the	O
dynamics	O
of	O
st	O
itself	O
is	O
marko-	O
vian	O
,	O
with	O
transition	O
p	O
(	O
st|st−1	O
)	O
.	O
for	O
the	O
more	O
general	O
‘	O
augmented	B
’	O
aslds	O
model	B
the	O
switch	O
st	O
is	O
dependent	O
on	O
both	O
the	O
previous	O
st−1	O
and	O
ht−1	O
.	O
the	O
model	B
deﬁnes	O
a	O
joint	B
distribution	O
(	O
see	O
ﬁg	O
(	O
25.1	O
)	O
)	O
t	O
(	O
cid:89	O
)	O
p	O
(	O
v1	O
:	O
t	O
,	O
h1	O
:	O
t	O
,	O
s1	O
:	O
t	O
)	O
=	O
p	O
(	O
vt|ht	O
,	O
st	O
)	O
p	O
(	O
ht|ht−1	O
,	O
st	O
)	O
p	O
(	O
st|ht−1	O
,	O
st−1	O
)	O
with	O
t=1	O
p	O
(	O
vt|ht	O
,	O
st	O
)	O
=	O
n	O
(	O
vt	O
¯v	O
(	O
st	O
)	O
+	O
b	O
(	O
st	O
)	O
ht	O
,	O
σv	O
(	O
st	O
)	O
)	O
,	O
p	O
(	O
ht|ht−1	O
,	O
st	O
)	O
=	O
n	O
457	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
ht	O
¯h	O
(	O
st	O
)	O
+	O
a	O
(	O
st	O
)	O
ht	O
,	O
σh	O
(	O
st	O
)	O
(	O
25.2.3	O
)	O
gaussian	O
sum	O
filtering	O
s1	O
h1	O
v1	O
s2	O
h2	O
v2	O
s3	O
h3	O
v3	O
s4	O
h4	O
v4	O
figure	O
25.1	O
:	O
the	O
independence	B
structure	O
of	O
the	O
aslds	O
.	O
square	O
nodes	O
st	O
denote	O
discrete	B
switch	O
variables	O
;	O
ht	O
are	O
continuous	B
latent/hidden	O
variables	O
,	O
and	O
vt	O
continuous	B
ob-	O
served/visible	O
variables	O
.	O
the	O
discrete	B
state	O
st	O
determines	O
which	O
linear	B
dynamical	I
system	I
from	O
a	O
ﬁnite	O
set	O
of	O
linear	O
dynamical	O
systems	O
is	O
operational	O
at	O
time	O
t.	O
in	O
the	O
slds	O
links	O
from	O
h	O
to	O
s	O
are	O
not	O
normally	O
considered	O
.	O
at	O
time	O
t	O
=	O
1	O
,	O
p	O
(	O
s1|h0	O
,	O
s0	O
)	O
denotes	O
the	O
prior	B
p	O
(	O
s1	O
)	O
,	O
and	O
p	O
(	O
h1|h0	O
,	O
s1	O
)	O
denotes	O
p	O
(	O
h1|s1	O
)	O
.	O
the	O
slds	O
can	O
be	O
thought	O
of	O
as	O
a	O
marriage	O
between	O
a	O
hidden	B
markov	O
model	B
and	O
a	O
linear	B
dynamical	I
system	I
.	O
the	O
slds	O
is	O
also	O
called	O
a	O
jump	O
markov	O
model/process	O
,	O
switching	B
kalman	O
filter	O
,	O
switching	O
linear	O
gaussian	O
state	O
space	O
model	B
,	O
conditional	B
linear	O
gaussian	O
model	B
.	O
25.2.1	O
exact	O
inference	O
is	O
computationally	O
intractable	O
both	O
exact	O
ﬁltered	O
and	O
smoothed	O
inference	B
in	O
the	O
slds	O
is	O
intractable	O
,	O
scaling	O
exponentially	O
with	O
time	O
.	O
as	O
an	O
informal	O
explanation	O
,	O
consider	O
ﬁltered	O
posterior	B
inference	O
,	O
for	O
which	O
,	O
by	O
analogy	O
with	O
equation	O
(	O
23.2.9	O
)	O
,	O
the	O
forward	O
pass	O
is	O
p	O
(	O
st+1	O
,	O
ht+1|st	O
,	O
ht	O
,	O
vt+1	O
)	O
p	O
(	O
st	O
,	O
ht|v1	O
:	O
t	O
)	O
(	O
25.2.4	O
)	O
p	O
(	O
st+1	O
,	O
ht+1|v1	O
:	O
t+1	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:90	O
)	O
st	O
ht	O
at	O
timestep	O
1	O
,	O
p	O
(	O
s1	O
,	O
h1|v1	O
)	O
=	O
p	O
(	O
h1|s1	O
,	O
v1	O
)	O
p	O
(	O
s1|v1	O
)	O
is	O
an	O
indexed	O
set	O
of	O
gaussians	O
.	O
at	O
timestep	O
2	O
,	O
due	O
to	O
the	O
summation	O
over	O
the	O
states	O
s1	O
,	O
p	O
(	O
s2	O
,	O
h2|v1:2	O
)	O
will	O
be	O
an	O
indexed	O
set	O
of	O
s	O
gaussians	O
;	O
similarly	O
at	O
timestep	O
3	O
,	O
it	O
will	O
be	O
s2	O
and	O
,	O
in	O
general	O
,	O
gives	O
rise	O
to	O
st−1	O
gaussians	O
at	O
time	O
t.	O
even	O
for	O
small	O
t	O
,	O
the	O
number	O
of	O
components	O
required	O
to	O
exactly	O
represent	O
the	O
ﬁltered	O
distribution	B
is	O
therefore	O
computationally	O
intractable	O
.	O
analogously	O
,	O
smoothing	B
is	O
also	O
intractable	O
.	O
the	O
origin	O
of	O
the	O
intractability	O
of	O
the	O
slds	O
diﬀers	O
from	O
‘	O
struc-	O
tural	O
intractability	O
’	O
that	O
we	O
’	O
ve	O
previously	O
encountered	O
.	O
in	O
the	O
slds	O
,	O
in	O
terms	O
of	O
the	O
cluster	O
variables	O
x1	O
:	O
t	O
,	O
with	O
xt	O
≡	O
(	O
st	O
,	O
ht	O
)	O
and	O
visible	B
variables	O
v1	O
:	O
t	O
,	O
the	O
graph	B
of	O
the	O
distribution	B
is	O
singly-connected	B
.	O
from	O
a	O
purely	O
graph	B
theoretic	O
viewpoint	O
,	O
one	O
would	O
therefore	O
envisage	O
little	O
diﬃculty	O
in	O
carrying	O
out	O
inference	B
.	O
indeed	O
,	O
as	O
we	O
saw	O
above	O
,	O
the	O
derivation	O
of	O
the	O
ﬁltering	B
algorithm	O
is	O
straightforward	O
since	O
the	O
graph	B
is	O
singly-connected	B
.	O
however	O
,	O
the	O
numerical	B
implementation	O
of	O
the	O
algorithm	B
is	O
intractable	O
since	O
the	O
de-	O
scription	O
of	O
the	O
messages	O
requires	O
an	O
exponentially	O
increasing	O
number	O
of	O
terms	O
.	O
in	O
order	O
to	O
deal	O
with	O
this	O
intractability	O
,	O
several	O
approximation	B
schemes	O
have	O
been	O
introduced	O
[	O
98	O
,	O
110	O
,	O
174	O
,	O
161	O
,	O
160	O
]	O
.	O
here	O
we	O
focus	O
on	O
techniques	O
which	O
approximate	B
the	O
switch	O
conditional	B
posteriors	O
using	O
a	O
limited	O
mixture	O
of	O
gaussians	O
.	O
since	O
the	O
exact	O
posterior	O
distributions	O
are	O
mixtures	O
of	O
gaussians	O
,	O
albeit	O
with	O
an	O
exponentially	O
large	O
number	O
of	O
components	O
,	O
the	O
aim	O
is	O
to	O
drop	O
low	O
weight	O
components	O
such	O
that	O
the	O
resulting	O
approximation	B
accurately	O
represents	O
the	O
posterior	B
.	O
25.3	O
gaussian	O
sum	O
filtering	O
equation	B
(	O
25.2.4	O
)	O
describes	O
the	O
exact	O
ﬁltering	O
recursion	O
with	O
an	O
exponentially	O
increasing	O
number	O
of	O
com-	O
ponents	O
with	O
time	O
.	O
in	O
general	O
,	O
the	O
inﬂuence	O
of	O
ancient	O
observations	O
will	O
be	O
much	O
less	O
relevant	O
than	O
that	O
of	O
recent	O
observations	O
.	O
this	O
suggests	O
that	O
the	O
‘	O
eﬀective	O
time	O
’	O
is	O
limited	O
and	O
that	O
therefore	O
a	O
corresponding	O
limited	O
number	O
of	O
components	O
in	O
the	O
gaussian	O
mixture	B
should	O
suﬃce	O
to	O
accurately	O
represent	O
the	O
ﬁltered	O
posterior	B
[	O
6	O
]	O
.	O
our	O
aim	O
is	O
to	O
form	O
a	O
recursion	O
for	O
p	O
(	O
st	O
,	O
ht|v1	O
:	O
t	O
)	O
based	O
on	O
a	O
gaussian	O
mixture	B
approximation	O
of	O
p	O
(	O
ht|st	O
,	O
v1	O
:	O
t	O
)	O
.	O
given	O
an	O
approximation	B
of	O
the	O
ﬁltered	O
distribution	B
p	O
(	O
st	O
,	O
ht|v1	O
:	O
t	O
)	O
≈	O
q	O
(	O
st	O
,	O
ht|v1	O
:	O
t	O
)	O
,	O
the	O
exact	O
recursion	O
equa-	O
tion	O
(	O
25.2.4	O
)	O
is	O
approximated	O
by	O
p	O
(	O
st+1	O
,	O
ht+1|st	O
,	O
ht	O
,	O
vt+1	O
)	O
q	O
(	O
st	O
,	O
ht|v1	O
:	O
t	O
)	O
(	O
25.3.1	O
)	O
q	O
(	O
st+1	O
,	O
ht+1|v1	O
:	O
t+1	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:90	O
)	O
st	O
ht	O
458	O
draft	O
march	O
9	O
,	O
2010	O
gaussian	O
sum	O
filtering	O
this	O
approximation	B
to	O
the	O
ﬁltered	O
posterior	B
at	O
the	O
next	O
timestep	O
will	O
contain	O
s	O
times	O
more	O
components	O
than	O
in	O
the	O
previous	O
timestep	O
and	O
,	O
to	O
prevent	O
an	O
exponential	B
explosion	O
in	O
mixture	B
components	O
,	O
we	O
will	O
need	O
to	O
subsequently	O
collapse	O
the	O
mixture	B
q	O
(	O
st+1	O
,	O
ht+1|v1	O
:	O
t+1	O
)	O
in	O
a	O
suitable	O
way	O
.	O
we	O
will	O
deal	O
with	O
this	O
issue	O
once	O
q	O
(	O
st+1	O
,	O
ht+1|v1	O
:	O
t+1	O
)	O
has	O
been	O
computed	O
.	O
to	O
derive	O
the	O
updates	O
it	O
is	O
useful	O
to	O
break	O
the	O
new	O
ﬁltered	O
approximation	B
from	O
equation	B
(	O
25.2.4	O
)	O
into	O
continuous	B
and	O
discrete	B
parts	O
:	O
q	O
(	O
ht	O
,	O
st|v1	O
:	O
t	O
)	O
=	O
q	O
(	O
ht|st	O
,	O
v1	O
:	O
t	O
)	O
q	O
(	O
st|v1	O
:	O
t	O
)	O
and	O
derive	O
separate	O
ﬁltered	O
update	O
formulae	O
,	O
as	O
described	O
below	O
.	O
(	O
25.3.2	O
)	O
25.3.1	O
continuous	B
ﬁltering	O
the	O
exact	O
representation	O
of	O
p	O
(	O
ht|st	O
,	O
v1	O
:	O
t	O
)	O
is	O
a	O
mixture	B
with	O
o	O
(	O
cid:0	O
)	O
st−1	O
(	O
cid:1	O
)	O
components	O
.	O
to	O
retain	O
computational	O
feasibility	O
we	O
therefore	O
approximate	B
this	O
with	O
a	O
limited	O
i-component	O
mixture	B
q	O
(	O
ht|st	O
,	O
v1	O
:	O
t	O
)	O
=	O
q	O
(	O
ht|it	O
,	O
st	O
,	O
v1	O
:	O
t	O
)	O
q	O
(	O
it|st	O
,	O
v1	O
:	O
t	O
)	O
(	O
25.3.3	O
)	O
where	O
q	O
(	O
ht|it	O
,	O
st	O
,	O
v1	O
:	O
t	O
)	O
is	O
a	O
gaussian	O
parameterised	O
with	O
mean	O
f	O
(	O
it	O
,	O
st	O
)	O
and	O
covariance	B
f	O
(	O
it	O
,	O
st	O
)	O
.	O
strictly	O
speaking	O
,	O
we	O
should	O
use	O
the	O
notation	O
ft	O
(	O
it	O
,	O
st	O
)	O
since	O
,	O
for	O
each	O
time	O
t	O
,	O
we	O
have	O
a	O
set	O
of	O
means	O
indexed	O
by	O
it	O
,	O
st	O
,	O
although	O
we	O
drop	O
these	O
dependencies	O
in	O
the	O
notation	O
used	O
here	O
.	O
urally	O
,	O
this	O
gives	O
rise	O
to	O
a	O
mixture	O
of	O
gaussians	O
for	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
=	O
(	O
cid:80	O
)	O
an	O
important	O
remark	O
is	O
that	O
many	O
techniques	O
approximate	B
p	O
(	O
ht|st	O
,	O
v1	O
:	O
t	O
)	O
using	O
a	O
single	O
gaussian	O
.	O
nat-	O
st	O
p	O
(	O
ht|st	O
,	O
v1	O
:	O
t	O
)	O
p	O
(	O
st|v1	O
:	O
t	O
)	O
.	O
however	O
,	O
in	O
making	O
a	O
single	O
gaussian	O
approximation	B
to	O
p	O
(	O
ht|st	O
,	O
v1	O
:	O
t	O
)	O
the	O
representation	B
of	O
the	O
posterior	B
may	O
be	O
poor	O
.	O
our	O
aim	O
here	O
is	O
to	O
maintain	O
an	O
accurate	O
approximation	B
to	O
p	O
(	O
ht|st	O
,	O
v1	O
:	O
t	O
)	O
by	O
using	O
a	O
mixture	O
of	O
gaussians	O
.	O
to	O
ﬁnd	O
a	O
recursion	O
for	O
the	O
approximating	O
distribution	B
we	O
ﬁrst	O
assume	O
that	O
we	O
know	O
the	O
ﬁltered	O
approx-	O
imation	O
q	O
(	O
ht	O
,	O
st|v1	O
:	O
t	O
)	O
and	O
then	O
propagate	O
this	O
forwards	O
using	O
the	O
exact	O
dynamics	O
.	O
to	O
do	O
so	O
consider	O
ﬁrst	O
the	O
relation	O
i	O
(	O
cid:88	O
)	O
it=1	O
q	O
(	O
ht+1|st+1	O
,	O
v1	O
:	O
t+1	O
)	O
=	O
(	O
cid:88	O
)	O
=	O
(	O
cid:88	O
)	O
st	O
,	O
it	O
st	O
,	O
it	O
q	O
(	O
ht+1	O
,	O
st	O
,	O
it|st+1	O
,	O
v1	O
:	O
t+1	O
)	O
q	O
(	O
ht+1|st	O
,	O
it	O
,	O
st+1	O
,	O
v1	O
:	O
t+1	O
)	O
q	O
(	O
st	O
,	O
it|st+1	O
,	O
v1	O
:	O
t+1	O
)	O
(	O
25.3.4	O
)	O
wherever	O
possible	O
we	O
now	O
substitute	O
the	O
exact	O
dynamics	O
and	O
evaluate	O
each	O
of	O
the	O
two	O
factors	O
above	O
.	O
the	O
usefulness	O
of	O
decomposing	O
the	O
update	O
in	O
this	O
way	O
is	O
that	O
the	O
new	O
ﬁltered	O
approximation	B
is	O
of	O
the	O
form	O
of	O
a	O
gaussian	O
mixture	B
,	O
where	O
q	O
(	O
ht+1|st	O
,	O
it	O
,	O
st+1	O
,	O
v1	O
:	O
t+1	O
)	O
is	O
gaussian	O
and	O
q	O
(	O
st	O
,	O
it|st+1	O
,	O
v1	O
:	O
t+1	O
)	O
are	O
the	O
weights	O
or	O
mixing	O
proportions	O
of	O
the	O
components	O
.	O
we	O
describe	O
below	O
how	O
to	O
compute	O
these	O
terms	O
explicitly	O
.	O
equation	B
(	O
25.3.4	O
)	O
produces	O
a	O
new	O
gaussian	O
mixture	B
with	O
i	O
×	O
s	O
components	O
which	O
we	O
will	O
collapse	O
back	O
to	O
i	O
components	O
at	O
the	O
end	O
of	O
the	O
computation	O
.	O
evaluating	O
q	O
(	O
ht+1|st	O
,	O
it	O
,	O
st+1	O
,	O
v1	O
:	O
t+1	O
)	O
we	O
aim	O
to	O
ﬁnd	O
a	O
ﬁltering	B
recursion	O
for	O
q	O
(	O
ht+1|st	O
,	O
it	O
,	O
st+1	O
,	O
v1	O
:	O
t+1	O
)	O
.	O
since	O
this	O
is	O
conditional	B
on	O
switch	O
states	O
and	O
components	O
,	O
this	O
corresponds	O
to	O
a	O
single	O
lds	O
forward	O
step	O
which	O
can	O
be	O
evaluated	O
by	O
considering	O
(	O
cid:90	O
)	O
ﬁrst	O
the	O
joint	B
distribution	O
q	O
(	O
ht+1	O
,	O
vt+1|st	O
,	O
it	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
=	O
ht	O
p	O
(	O
ht+1	O
,	O
vt+1|ht	O
,	O
st	O
,	O
it	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
q	O
(	O
ht|st	O
,	O
it	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
(	O
25.3.5	O
)	O
and	O
subsequently	O
conditioning	B
on	O
vt+1	O
.	O
in	O
the	O
above	O
we	O
used	O
the	O
exact	O
dynamics	O
where	O
possible	O
.	O
equation	B
(	O
25.3.5	O
)	O
states	O
that	O
we	O
know	O
the	O
ﬁltered	O
information	O
up	O
to	O
time	O
t	O
,	O
in	O
addition	O
to	O
knowing	O
the	O
switch	O
states	O
st	O
,	O
st+1	O
draft	O
march	O
9	O
,	O
2010	O
459	O
gaussian	O
sum	O
filtering	O
and	O
the	O
mixture	B
component	O
index	O
it	O
.	O
to	O
ease	O
the	O
burden	O
on	O
notation	O
we	O
derive	O
this	O
for	O
¯ht	O
,	O
¯vt	O
≡	O
0	O
for	O
all	O
t.	O
the	O
exact	O
forward	O
dynamics	O
is	O
then	O
given	O
by	O
(	O
25.3.6	O
)	O
vt+1	O
=	O
b	O
(	O
st+1	O
)	O
ht+1	O
+	O
ηv	O
(	O
st+1	O
)	O
,	O
ht+1	O
=	O
a	O
(	O
st+1	O
)	O
ht	O
+	O
ηh	O
(	O
st+1	O
)	O
,	O
given	O
the	O
mixture	B
component	O
index	O
it	O
,	O
q	O
(	O
ht|v1	O
:	O
t	O
,	O
it	O
,	O
st	O
)	O
=	O
n	O
(	O
ht	O
f	O
(	O
it	O
,	O
st	O
)	O
,	O
f	O
(	O
it	O
,	O
st	O
)	O
)	O
(	O
25.3.7	O
)	O
we	O
propagate	O
this	O
gaussian	O
with	O
the	O
exact	O
dynamics	O
equation	B
(	O
25.3.6	O
)	O
.	O
then	O
q	O
(	O
ht+1	O
,	O
vt+1|st	O
,	O
it	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
is	O
a	O
gaussian	O
with	O
covariance	O
and	O
mean	B
elements	O
σhh	O
=	O
a	O
(	O
st+1	O
)	O
f	O
(	O
it	O
,	O
st	O
)	O
at	O
(	O
st+1	O
)	O
+	O
σh	O
(	O
st+1	O
)	O
,	O
σvv	O
=	O
b	O
(	O
st+1	O
)	O
σhhbt	O
(	O
st+1	O
)	O
+	O
σv	O
(	O
st+1	O
)	O
σvh	O
=	O
b	O
(	O
st+1	O
)	O
σhh	O
=	O
σt	O
(	O
25.3.8	O
)	O
these	O
results	O
are	O
obtained	O
from	O
integrating	O
the	O
forward	O
dynamics	O
,	O
equations	O
(	O
25.2.1,25.2.2	O
)	O
over	O
ht	O
,	O
using	O
the	O
results	O
in	O
section	O
(	O
8.6.3	O
)	O
.	O
hv	O
,	O
µv	O
=	O
b	O
(	O
st+1	O
)	O
a	O
(	O
st+1	O
)	O
f	O
(	O
it	O
,	O
st	O
)	O
,	O
µh	O
=	O
a	O
(	O
st+1	O
)	O
f	O
(	O
it	O
,	O
st	O
)	O
to	O
ﬁnd	O
q	O
(	O
ht+1|st	O
,	O
it	O
,	O
st+1	O
,	O
v1	O
:	O
t+1	O
)	O
we	O
condition	O
q	O
(	O
ht+1	O
,	O
vt+1|st	O
,	O
it	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
on	O
vt+1	O
using	O
the	O
standard	O
gaussian	O
conditioning	B
formulae	O
,	O
deﬁnition	O
(	O
78	O
)	O
,	O
to	O
obtain	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
q	O
(	O
ht+1|st	O
,	O
it	O
,	O
st+1	O
,	O
v1	O
:	O
t+1	O
)	O
=	O
n	O
ht+1	O
µh|v	O
,	O
σh|v	O
(	O
25.3.9	O
)	O
(	O
25.3.10	O
)	O
with	O
µh|v	O
=	O
µh	O
+	O
σhvς−1	O
vv	O
(	O
vt+1	O
−	O
µv	O
)	O
,	O
σh|v	O
=	O
σhh	O
−	O
σhvς−1	O
vv	O
σvh	O
where	O
the	O
quantities	O
required	O
are	O
deﬁned	O
in	O
equation	B
(	O
25.3.8	O
)	O
.	O
evaluating	O
the	O
mixture	B
weights	O
q	O
(	O
st	O
,	O
it|st+1	O
,	O
v1	O
:	O
t+1	O
)	O
up	O
to	O
a	O
normalisation	B
constant	I
the	O
mixture	B
weight	O
in	O
equation	B
(	O
25.3.4	O
)	O
can	O
be	O
found	O
from	O
q	O
(	O
st	O
,	O
it|st+1	O
,	O
v1	O
:	O
t+1	O
)	O
∝	O
q	O
(	O
vt+1|it	O
,	O
st	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
q	O
(	O
st+1|it	O
,	O
st	O
,	O
v1	O
:	O
t	O
)	O
q	O
(	O
it|st	O
,	O
v1	O
:	O
t	O
)	O
q	O
(	O
st|v1	O
:	O
t	O
)	O
(	O
25.3.11	O
)	O
the	O
ﬁrst	O
factor	O
in	O
equation	B
(	O
25.3.11	O
)	O
,	O
q	O
(	O
vt+1|it	O
,	O
st	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
is	O
gaussian	O
with	O
mean	O
µv	O
and	O
covariance	B
σvv	O
,	O
as	O
given	O
in	O
equation	B
(	O
25.3.8	O
)	O
.	O
the	O
last	O
two	O
factors	O
q	O
(	O
it|st	O
,	O
v1	O
:	O
t	O
)	O
and	O
q	O
(	O
st|v1	O
:	O
t	O
)	O
are	O
given	O
from	O
the	O
previous	O
ﬁltered	O
iteration	B
.	O
finally	O
,	O
q	O
(	O
st+1|it	O
,	O
st	O
,	O
v1	O
:	O
t	O
)	O
is	O
found	O
from	O
(	O
cid:26	O
)	O
q	O
(	O
st+1|it	O
,	O
st	O
,	O
v1	O
:	O
t	O
)	O
=	O
(	O
cid:104	O
)	O
p	O
(	O
st+1|ht	O
,	O
st	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
ht|it	O
,	O
st	O
,	O
v1	O
:	O
t	O
)	O
p	O
(	O
st+1|st	O
)	O
augmented	B
slds	O
standard	O
slds	O
where	O
the	O
result	O
above	O
for	O
the	O
standard	O
slds	O
follows	O
from	O
the	O
independence	B
assumptions	O
present	O
in	O
the	O
standard	O
slds	O
.	O
in	O
the	O
aslds	O
,	O
the	O
term	O
in	O
equation	B
(	O
25.3.12	O
)	O
will	O
generally	O
need	O
to	O
be	O
computed	O
numerically	O
.	O
a	O
simple	O
approximation	O
is	O
to	O
evaluate	O
equation	B
(	O
25.3.12	O
)	O
at	O
the	O
mean	B
value	O
of	O
the	O
distribution	B
q	O
(	O
ht|it	O
,	O
st	O
,	O
v1	O
:	O
t	O
)	O
.	O
to	O
take	O
covariance	B
information	O
into	O
account	O
an	O
alternative	O
would	O
be	O
to	O
draw	O
samples	O
from	O
the	O
gaussian	O
q	O
(	O
ht|it	O
,	O
st	O
,	O
v1	O
:	O
t	O
)	O
and	O
thus	O
approximate	B
the	O
average	B
of	O
p	O
(	O
st+1|ht	O
,	O
st	O
)	O
by	O
sampling	B
.	O
note	O
that	O
this	O
does	O
not	O
equate	O
gaussian	O
sum	O
ﬁltering	B
with	O
a	O
sequential	B
sampling	O
procedure	O
,	O
such	O
as	O
particle	O
filtering	O
,	O
section	O
(	O
27.6.2	O
)	O
.	O
the	O
sampling	B
here	O
is	O
exact	O
,	O
for	O
which	O
no	O
convergence	O
issues	O
arise	O
.	O
(	O
25.3.12	O
)	O
closing	O
the	O
recursion	O
we	O
are	O
now	O
in	O
a	O
position	O
to	O
calculate	O
equation	B
(	O
25.3.4	O
)	O
.	O
for	O
each	O
setting	O
of	O
the	O
variable	B
st+1	O
,	O
we	O
have	O
a	O
mixture	O
of	O
i	O
×	O
s	O
gaussians	O
.	O
to	O
prevent	O
the	O
number	O
of	O
components	O
increasing	O
exponentially	O
with	O
time	O
,	O
we	O
numerically	O
collapse	O
q	O
(	O
ht+1|st+1	O
,	O
v1	O
:	O
t+1	O
)	O
back	O
to	O
i	O
gaussians	O
to	O
form	O
q	O
(	O
ht+1|st+1	O
,	O
v1	O
:	O
t+1	O
)	O
→	O
q	O
(	O
ht+1|it+1	O
,	O
st+1	O
,	O
v1	O
:	O
t+1	O
)	O
q	O
(	O
it+1|st+1	O
,	O
v1	O
:	O
t+1	O
)	O
(	O
25.3.13	O
)	O
i	O
(	O
cid:88	O
)	O
it+1=1	O
any	O
method	O
of	O
choice	O
may	O
be	O
supplied	O
to	O
collapse	O
a	O
mixture	B
to	O
a	O
smaller	O
mixture	B
.	O
a	O
straightforward	O
approach	B
is	O
to	O
repeatedly	O
merge	O
low-weight	O
components	O
,	O
as	O
explained	O
in	O
section	O
(	O
25.3.4	O
)	O
.	O
in	O
this	O
way	O
the	O
new	O
mixture	B
coeﬃcients	O
q	O
(	O
it+1|st+1	O
,	O
v1	O
:	O
t+1	O
)	O
,	O
it+1	O
∈	O
1	O
,	O
.	O
.	O
.	O
,	O
i	O
are	O
deﬁned	O
.	O
this	O
completes	O
the	O
description	O
of	O
how	O
to	O
form	O
a	O
recursion	O
for	O
the	O
continuous	B
ﬁltered	O
posterior	B
approximation	O
q	O
(	O
ht+1|st+1	O
,	O
v1	O
:	O
t+1	O
)	O
in	O
equation	B
(	O
25.3.2	O
)	O
.	O
460	O
draft	O
march	O
9	O
,	O
2010	O
gaussian	O
sum	O
filtering	O
figure	O
25.2	O
:	O
gaussian	O
sum	O
filtering	O
.	O
the	O
leftmost	O
column	O
depicts	O
the	O
previous	O
gaussian	O
mixture	B
approximation	O
q	O
(	O
ht	O
,	O
it|v1	O
:	O
t	O
)	O
for	O
two	O
states	O
s	O
=	O
2	O
(	O
red	O
and	O
blue	O
)	O
and	O
three	O
mixture	B
components	O
i	O
=	O
3	O
,	O
with	O
the	O
mixture	B
weight	O
represented	O
by	O
the	O
area	O
of	O
each	O
oval	O
.	O
there	O
are	O
s	O
=	O
2	O
diﬀerent	O
linear	B
systems	O
which	O
take	O
each	O
of	O
the	O
components	O
of	O
the	O
mixture	B
into	O
a	O
new	O
ﬁltered	O
state	O
,	O
the	O
colour	O
of	O
the	O
arrow	O
indicating	O
which	O
dynamic	B
system	O
is	O
used	O
.	O
after	O
one	O
time-step	O
each	O
mixture	B
component	O
branches	O
into	O
a	O
further	O
s	O
components	O
so	O
that	O
the	O
joint	B
approximation	O
q	O
(	O
ht+1	O
,	O
st+1|v1	O
:	O
t+1	O
)	O
contains	O
s2i	O
components	O
(	O
middle	O
column	O
)	O
.	O
to	O
keep	O
the	O
representation	B
computationally	O
tractable	O
the	O
mixture	O
of	O
gaussians	O
for	O
each	O
state	O
st+1	O
is	O
collapsed	O
back	O
to	O
i	O
components	O
.	O
this	O
means	O
that	O
each	O
coloured	O
state	O
needs	O
to	O
be	O
approximated	O
by	O
a	O
smaller	O
i	O
component	O
mixture	O
of	O
gaussians	O
.	O
there	O
are	O
many	O
ways	O
to	O
achieve	O
this	O
.	O
a	O
naive	O
but	O
computationally	O
eﬃcient	B
approach	O
is	O
to	O
simply	O
ignore	O
the	O
lowest	O
weight	B
components	O
,	O
as	O
depicted	O
on	O
the	O
right	O
column	O
,	O
see	O
mix2mix.m	O
.	O
25.3.2	O
discrete	B
ﬁltering	O
a	O
recursion	O
for	O
the	O
switch	O
variable	B
distribution	O
in	O
equation	B
(	O
25.3.2	O
)	O
is	O
q	O
(	O
st+1	O
,	O
it	O
,	O
st	O
,	O
vt+1	O
,	O
v1	O
:	O
t	O
)	O
(	O
cid:88	O
)	O
it	O
,	O
st	O
q	O
(	O
st+1|v1	O
:	O
t+1	O
)	O
∝	O
(	O
cid:88	O
)	O
st	O
,	O
it	O
the	O
r.h.s	O
.	O
of	O
the	O
above	O
equation	B
is	O
proportional	O
to	O
q	O
(	O
vt+1|st+1	O
,	O
it	O
,	O
st	O
,	O
v1	O
:	O
t	O
)	O
q	O
(	O
st+1|it	O
,	O
st	O
,	O
v1	O
:	O
t	O
)	O
q	O
(	O
it|st	O
,	O
v1	O
:	O
t	O
)	O
q	O
(	O
st|v1	O
:	O
t	O
)	O
(	O
25.3.14	O
)	O
(	O
25.3.15	O
)	O
for	O
which	O
all	O
terms	O
have	O
been	O
computed	O
during	O
the	O
recursion	O
for	O
q	O
(	O
ht+1|st+1	O
,	O
v1	O
:	O
t+1	O
)	O
.	O
we	O
now	O
have	O
all	O
the	O
quantities	O
required	O
to	O
compute	O
the	O
gaussian	O
sum	O
approximation	B
of	O
the	O
ﬁltering	B
forward	O
pass	O
.	O
a	O
schematic	O
representation	B
of	O
gaussian	O
sum	O
ﬁltering	B
is	O
given	O
in	O
ﬁg	O
(	O
25.2	O
)	O
and	O
the	O
pseudo	B
code	O
is	O
presented	O
in	O
algorithm	B
(	O
22	O
)	O
.	O
see	O
also	O
sldsforward.m	O
.	O
25.3.3	O
the	O
likelihood	B
p	O
(	O
v1	O
:	O
t	O
)	O
the	O
likelihood	B
p	O
(	O
v1	O
:	O
t	O
)	O
may	O
be	O
found	O
from	O
t−1	O
(	O
cid:89	O
)	O
p	O
(	O
v1	O
:	O
t	O
)	O
=	O
t=0	O
where	O
p	O
(	O
vt+1|v1	O
:	O
t	O
)	O
≈	O
p	O
(	O
vt+1|v1	O
:	O
t	O
)	O
(	O
cid:88	O
)	O
it	O
,	O
st	O
,	O
st+1	O
q	O
(	O
vt+1|it	O
,	O
st	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
q	O
(	O
st+1|it	O
,	O
st	O
,	O
v1	O
:	O
t	O
)	O
q	O
(	O
it|st	O
,	O
v1	O
:	O
t	O
)	O
q	O
(	O
st|v1	O
:	O
t	O
)	O
(	O
25.3.16	O
)	O
in	O
the	O
above	O
expression	O
,	O
all	O
terms	O
have	O
been	O
computed	O
in	O
forming	O
the	O
recursion	O
for	O
the	O
ﬁltered	O
posterior	B
q	O
(	O
ht+1	O
,	O
st+1|v1	O
:	O
t+1	O
)	O
.	O
25.3.4	O
collapsing	O
gaussians	O
given	O
a	O
mixture	O
of	O
n	O
gaussians	O
n	O
(	O
cid:88	O
)	O
i=1	O
p	O
(	O
x	O
)	O
=	O
pin	O
(	O
x	O
µi	O
,	O
σi	O
)	O
(	O
25.3.17	O
)	O
we	O
wish	O
to	O
collapse	O
this	O
to	O
a	O
smaller	O
k	O
<	O
n	O
mixture	O
of	O
gaussians	O
.	O
we	O
describe	O
a	O
simple	O
method	O
which	O
has	O
the	O
advantage	O
of	O
computational	O
eﬃciency	O
,	O
but	O
the	O
disadvantage	O
that	O
no	O
spatial	O
information	O
about	O
draft	O
march	O
9	O
,	O
2010	O
461	O
t+1t	O
gaussian	O
sum	O
smoothing	B
(	O
cid:80	O
)	O
algorithm	B
22	O
aslds	O
forward	O
pass	O
.	O
approximate	B
the	O
ﬁltered	O
posterior	B
p	O
(	O
st|v1	O
:	O
t	O
)	O
≡	O
αt	O
,	O
p	O
(	O
ht|st	O
,	O
v1	O
:	O
t	O
)	O
≡	O
it	O
wt	O
(	O
it	O
,	O
st	O
)	O
n	O
(	O
ht	O
ft	O
(	O
it	O
,	O
st	O
)	O
,	O
ft	O
(	O
it	O
,	O
st	O
)	O
)	O
.	O
also	O
return	O
the	O
approximate	B
log-likelihood	O
l	O
≡	O
log	O
p	O
(	O
v1	O
:	O
t	O
)	O
.	O
it	O
are	O
the	O
number	O
of	O
components	O
in	O
each	O
gaussian	O
mixture	B
approximation	O
.	O
we	O
require	O
i1	O
=	O
1	O
,	O
i2	O
≤	O
s	O
,	O
it	O
≤	O
s	O
×	O
it−1	O
.	O
θ	O
(	O
s	O
)	O
=	O
a	O
(	O
s	O
)	O
,	O
b	O
(	O
s	O
)	O
,	O
σh	O
(	O
s	O
)	O
,	O
σv	O
(	O
s	O
)	O
,	O
¯h	O
(	O
s	O
)	O
,	O
¯v	O
(	O
s	O
)	O
.	O
for	O
s1	O
←	O
1	O
to	O
s	O
do	O
{	O
f1	O
(	O
1	O
,	O
s1	O
)	O
,	O
f1	O
(	O
1	O
,	O
s1	O
)	O
,	O
ˆp	O
}	O
=	O
ldsforward	O
(	O
0	O
,	O
0	O
,	O
v1	O
;	O
θ	O
(	O
s1	O
)	O
)	O
α1	O
←	O
p	O
(	O
s1	O
)	O
ˆp	O
end	O
for	O
for	O
t	O
←	O
2	O
to	O
t	O
do	O
for	O
st	O
←	O
1	O
to	O
s	O
do	O
for	O
i	O
←	O
1	O
to	O
it−1	O
,	O
and	O
s	O
←	O
1	O
to	O
s	O
do	O
{	O
µx|y	O
(	O
i	O
,	O
s	O
)	O
,	O
σx|y	O
(	O
i	O
,	O
s	O
)	O
,	O
ˆp	O
}	O
=	O
ldsforward	O
(	O
ft−1	O
(	O
i	O
,	O
s	O
)	O
,	O
ft−1	O
(	O
i	O
,	O
s	O
)	O
,	O
vt	O
;	O
θ	O
(	O
st	O
)	O
)	O
p∗	O
(	O
st|i	O
,	O
s	O
)	O
≡	O
(	O
cid:104	O
)	O
p	O
(	O
st|ht−1	O
,	O
st−1	O
=	O
s	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
ht−1|it−1=i	O
,	O
st−1=s	O
,	O
v1	O
:	O
t−1	O
)	O
p	O
(	O
cid:48	O
)	O
(	O
st	O
,	O
i	O
,	O
s	O
)	O
←	O
wt−1	O
(	O
i	O
,	O
s	O
)	O
p∗	O
(	O
st|i	O
,	O
s	O
)	O
αt−1	O
(	O
s	O
)	O
ˆp	O
end	O
for	O
(	O
cid:80	O
)	O
it	O
collapse	O
the	O
it−1	O
×	O
s	O
mixture	O
of	O
gaussians	O
deﬁned	O
by	O
µx|y	O
,	O
σx|y	O
,	O
and	O
weights	O
p	O
(	O
i	O
,	O
s|st	O
)	O
∝	O
p	O
(	O
cid:48	O
)	O
(	O
st	O
,	O
i	O
,	O
s	O
)	O
p	O
(	O
ht|st	O
,	O
v1	O
:	O
t	O
)	O
≈	O
it=1	O
p	O
(	O
it|st	O
,	O
v1	O
:	O
t	O
)	O
p	O
(	O
ht|st	O
,	O
it	O
,	O
v1	O
:	O
t	O
)	O
.	O
this	O
deﬁnes	O
the	O
new	O
means	O
ft	O
(	O
it	O
,	O
st	O
)	O
,	O
covariances	O
ft	O
(	O
it	O
,	O
st	O
)	O
and	O
mixture	B
weights	O
wt	O
(	O
it	O
,	O
st	O
)	O
≡	O
p	O
(	O
it|st	O
,	O
v1	O
:	O
t	O
)	O
.	O
compute	O
αt	O
(	O
st	O
)	O
∝	O
l	O
←	O
l	O
+	O
log	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
i	O
,	O
s	O
p	O
(	O
cid:48	O
)	O
(	O
st	O
,	O
i	O
,	O
s	O
)	O
end	O
for	O
normalise	O
αt	O
to	O
a	O
gaussian	O
with	O
it	O
st	O
,	O
i	O
,	O
s	O
p	O
(	O
cid:48	O
)	O
(	O
st	O
,	O
i	O
,	O
s	O
)	O
components	O
,	O
end	O
for	O
the	O
mixture	B
is	O
used	O
[	O
277	O
]	O
.	O
first	O
we	O
describe	O
how	O
to	O
collapse	O
a	O
mixture	B
to	O
a	O
single	O
gaussian	O
.	O
this	O
can	O
be	O
achieved	O
by	O
ﬁnding	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
mixture	B
distribution	O
(	O
25.3.17	O
)	O
.	O
these	O
are	O
µ	O
=	O
(	O
cid:88	O
)	O
piµi	O
,	O
σ	O
=	O
(	O
cid:88	O
)	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
i	O
i	O
pi	O
σi	O
+	O
µiµt	O
i	O
−	O
µµt	O
(	O
25.3.18	O
)	O
to	O
collapse	O
a	O
mixture	B
then	O
to	O
a	O
k-component	O
mixture	B
we	O
may	O
ﬁrst	O
retain	O
the	O
k	O
−	O
1	O
gaussians	O
with	O
the	O
largest	O
mixture	B
weights	O
.	O
the	O
remaining	O
n	O
−	O
k	O
+	O
1	O
gaussians	O
are	O
simply	O
merged	O
to	O
a	O
single	O
gaussian	O
using	O
the	O
above	O
method	O
.	O
alternative	O
heuristics	O
such	O
as	O
recursively	O
merging	O
the	O
two	O
gaussians	O
with	O
the	O
lowest	O
mixture	B
weights	O
are	O
also	O
reasonable	O
.	O
more	O
sophisticated	O
methods	O
which	O
retain	O
some	O
spatial	O
information	O
would	O
clearly	O
be	O
potentially	O
useful	O
.	O
the	O
method	O
presented	O
in	O
[	O
174	O
]	O
is	O
a	O
suitable	O
approach	B
which	O
considers	O
removing	O
gaussians	O
which	O
are	O
spatially	O
similar	O
(	O
and	O
not	O
just	O
low-weight	O
components	O
)	O
,	O
thereby	O
retaining	O
a	O
sense	O
of	O
diversity	O
over	O
the	O
possible	O
solutions	O
.	O
in	O
applications	O
with	O
many	O
thousands	O
of	O
timesteps	O
,	O
speed	O
can	O
be	O
a	O
factor	B
in	O
determining	O
which	O
method	O
of	O
collapsing	O
gaussians	O
is	O
to	O
be	O
preferred	O
.	O
25.3.5	O
relation	O
to	O
other	O
methods	O
gaussian	O
sum	O
filtering	O
can	O
be	O
considered	O
a	O
form	O
of	O
‘	O
analytical	O
particle	O
ﬁltering	O
’	O
,	O
section	O
(	O
27.6.2	O
)	O
,	O
in	O
which	O
instead	O
of	O
point	O
distributions	O
(	O
delta	O
functions	O
)	O
being	O
propagated	O
,	O
gaussians	O
are	O
propagated	O
.	O
the	O
collapse	O
operation	O
to	O
a	O
smaller	O
number	O
of	O
gaussians	O
is	O
analogous	O
to	O
resampling	B
in	O
particle	O
filtering	O
.	O
since	O
a	O
gaussian	O
is	O
more	O
expressive	O
than	O
a	O
delta-function	O
,	O
the	O
gaussian	O
sum	O
ﬁlter	O
is	O
generally	O
an	O
improved	O
approximation	B
technique	O
over	O
using	O
point	O
particles	O
.	O
see	O
[	O
17	O
]	O
for	O
a	O
numerical	B
comparison	O
.	O
25.4	O
gaussian	O
sum	O
smoothing	B
approximating	O
the	O
smoothed	O
posterior	B
p	O
(	O
ht	O
,	O
st|v1	O
:	O
t	O
)	O
is	O
more	O
involved	O
than	O
ﬁltering	B
and	O
requires	O
additional	O
approximations	O
.	O
for	O
this	O
reason	O
smoothing	B
is	O
more	O
prone	O
to	O
failure	O
since	O
there	O
are	O
more	O
assumptions	O
that	O
need	O
to	O
be	O
satisﬁed	O
for	O
the	O
approximations	O
to	O
hold	O
.	O
the	O
route	O
we	O
take	O
here	O
is	O
to	O
assume	O
that	O
a	O
gaussian	O
462	O
draft	O
march	O
9	O
,	O
2010	O
gaussian	O
sum	O
smoothing	B
p	O
(	O
ht	O
,	O
st|v1	O
:	O
t	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:90	O
)	O
st+1	O
ht+1	O
p	O
(	O
st	O
,	O
ht|v1	O
:	O
t	O
)	O
=	O
(	O
cid:88	O
)	O
p	O
(	O
st	O
,	O
ht|v1	O
:	O
t	O
)	O
=	O
(	O
cid:88	O
)	O
st+1	O
st+1	O
sum	O
ﬁltered	O
approximation	B
has	O
been	O
carried	O
out	O
,	O
and	O
then	O
approximate	B
the	O
γ	O
backward	O
pass	O
,	O
analogous	O
to	O
that	O
of	O
section	O
(	O
23.2.4	O
)	O
.	O
by	O
analogy	O
with	O
the	O
rts	O
smoothing	B
recursion	O
equation	B
(	O
23.2.20	O
)	O
,	O
the	O
exact	O
backward	O
pass	O
for	O
the	O
slds	O
reads	O
p	O
(	O
ht	O
,	O
st|ht+1	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
p	O
(	O
ht+1	O
,	O
st+1|v1	O
:	O
t	O
)	O
(	O
25.4.1	O
)	O
where	O
p	O
(	O
ht+1	O
,	O
st+1|v1	O
:	O
t	O
)	O
=	O
p	O
(	O
st+1|v1	O
:	O
t	O
)	O
p	O
(	O
ht+1|st+1	O
,	O
v1	O
:	O
t	O
)	O
is	O
composed	O
of	O
the	O
discrete	B
and	O
continuous	B
com-	O
ponents	O
of	O
the	O
smoothed	O
posterior	B
at	O
the	O
next	O
time	O
step	O
.	O
the	O
recursion	O
runs	O
backwards	O
in	O
time	O
,	O
beginning	O
with	O
the	O
initialisation	O
p	O
(	O
ht	O
,	O
st|v1	O
:	O
t	O
)	O
set	O
by	O
the	O
ﬁltered	O
result	O
(	O
at	O
time	O
t	O
=	O
t	O
,	O
the	O
ﬁltered	O
and	O
smoothed	O
posteriors	O
coincide	O
)	O
.	O
apart	O
from	O
the	O
fact	O
that	O
the	O
number	O
of	O
mixture	B
components	O
will	O
increase	O
at	O
each	O
step	O
,	O
computing	O
the	O
integral	O
over	O
ht+1	O
in	O
equation	B
(	O
25.4.1	O
)	O
is	O
problematic	O
since	O
the	O
conditional	B
distribu-	O
tion	O
term	O
is	O
non-gaussian	O
in	O
ht+1	O
.	O
for	O
this	O
reason	O
it	O
is	O
more	O
useful	O
derive	O
an	O
approximate	B
recursion	O
by	O
beginning	O
with	O
the	O
exact	O
relation	O
p	O
(	O
st+1|v1	O
:	O
t	O
)	O
p	O
(	O
ht|st	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
p	O
(	O
st|st+1	O
,	O
v1	O
:	O
t	O
)	O
(	O
25.4.2	O
)	O
which	O
can	O
be	O
expressed	O
more	O
directly	O
in	O
terms	O
of	O
the	O
slds	O
dynamics	O
as	O
p	O
(	O
st+1|v1	O
:	O
t	O
)	O
(	O
cid:104	O
)	O
p	O
(	O
ht|ht+1	O
,	O
st	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
,	O
vt+1	O
:	O
t	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
ht+1|st	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
(	O
25.4.3	O
)	O
in	O
forming	O
the	O
recursion	O
we	O
assume	O
access	O
to	O
the	O
distribution	B
p	O
(	O
st+1	O
,	O
ht+1|v1	O
:	O
t	O
)	O
from	O
the	O
future	O
timestep	O
.	O
however	O
,	O
we	O
also	O
require	O
the	O
distribution	B
p	O
(	O
ht+1|st	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
which	O
is	O
not	O
directly	O
known	O
and	O
needs	O
to	O
be	O
inferred	O
,	O
in	O
itself	O
a	O
computationally	O
challenging	O
task	O
.	O
in	O
the	O
expectation	B
correction	I
(	O
ec	O
)	O
approach	B
[	O
17	O
]	O
one	O
assumes	O
the	O
approximation	B
(	O
see	O
ﬁg	O
(	O
25.3	O
)	O
)	O
p	O
(	O
ht+1|st	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
≈	O
p	O
(	O
ht+1|st+1	O
,	O
v1	O
:	O
t	O
)	O
×	O
(	O
cid:104	O
)	O
p	O
(	O
st|ht+1	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
ht+1|st+1	O
,	O
v1	O
:	O
t	O
)	O
(	O
25.4.4	O
)	O
resulting	O
in	O
an	O
approximate	B
recursion	O
for	O
the	O
smoothed	O
posterior	B
,	O
p	O
(	O
st	O
,	O
ht|v1	O
:	O
t	O
)	O
≈	O
p	O
(	O
st+1|v1	O
:	O
t	O
)	O
(	O
cid:104	O
)	O
p	O
(	O
ht|ht+1	O
,	O
st	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
(	O
cid:105	O
)	O
ht+1	O
(	O
cid:104	O
)	O
p	O
(	O
st|ht+1	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
(	O
cid:105	O
)	O
ht+1	O
(	O
25.4.5	O
)	O
(	O
cid:88	O
)	O
st+1	O
where	O
(	O
cid:104	O
)	O
·	O
(	O
cid:105	O
)	O
ht+1	O
represents	O
averaging	O
with	O
respect	O
to	O
the	O
distribution	B
p	O
(	O
ht+1|st+1	O
,	O
v1	O
:	O
t	O
)	O
.	O
in	O
carrying	O
out	O
the	O
approximate	B
recursion	O
,	O
(	O
25.4.5	O
)	O
we	O
will	O
end	O
up	O
with	O
a	O
mixture	O
of	O
gaussians	O
that	O
grows	O
at	O
each	O
timestep	O
.	O
to	O
avoid	O
the	O
exponential	B
explosion	O
problem	B
,	O
we	O
use	O
a	O
ﬁnite	O
mixture	O
approximation	B
,	O
q	O
(	O
ht+1	O
,	O
st+1|v1	O
:	O
t	O
)	O
:	O
p	O
(	O
ht+1	O
,	O
st+1|v1	O
:	O
t	O
)	O
≈	O
q	O
(	O
ht+1	O
,	O
st+1|v1	O
:	O
t	O
)	O
=	O
q	O
(	O
ht+1|st+1	O
,	O
v1	O
:	O
t	O
)	O
q	O
(	O
st+1|v1	O
:	O
t	O
)	O
(	O
25.4.6	O
)	O
and	O
plug	O
this	O
into	O
the	O
approximate	B
recursion	O
above	O
.	O
from	O
equation	B
(	O
25.4.5	O
)	O
a	O
recursion	O
for	O
the	O
approxi-	O
mation	O
is	O
given	O
by	O
(	O
cid:125	O
)	O
q	O
(	O
st+1|v1	O
:	O
t	O
)	O
(	O
cid:104	O
)	O
q	O
(	O
ht|ht+1	O
,	O
st	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
ht+1|st+1	O
,	O
v1	O
:	O
t	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:124	O
)	O
(	O
cid:124	O
)	O
(	O
cid:125	O
)	O
(	O
cid:104	O
)	O
q	O
(	O
st|ht+1	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
ht+1|st+1	O
,	O
v1	O
:	O
t	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
q	O
(	O
ht|st	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
q	O
(	O
st|st+1	O
,	O
v1	O
:	O
t	O
)	O
q	O
(	O
ht	O
,	O
st|v1	O
:	O
t	O
)	O
=	O
(	O
cid:88	O
)	O
st+1	O
as	O
for	O
ﬁltering	B
,	O
wherever	O
possible	O
,	O
we	O
replace	O
approximate	B
terms	O
by	O
their	O
exact	O
counterparts	O
and	O
param-	O
eterise	O
the	O
posterior	B
using	O
q	O
(	O
ht+1	O
,	O
st+1|v1	O
:	O
t	O
)	O
=	O
q	O
(	O
ht+1|st+1	O
,	O
v1	O
:	O
t	O
)	O
q	O
(	O
st+1|v1	O
:	O
t	O
)	O
(	O
25.4.8	O
)	O
to	O
reduce	O
the	O
notational	O
burden	O
here	O
we	O
outline	O
the	O
method	O
only	O
for	O
the	O
case	O
of	O
using	O
a	O
single	O
component	O
approximation	B
in	O
both	O
the	O
forward	O
and	O
backward	O
passes	O
.	O
the	O
extension	O
to	O
using	O
a	O
mixture	B
to	O
approxi-	O
mate	O
each	O
p	O
(	O
ht+1|st+1	O
,	O
v1	O
:	O
t	O
)	O
is	O
conceptually	O
straightforward	O
and	O
deferred	O
to	O
section	O
(	O
25.4.4	O
)	O
.	O
in	O
the	O
single	O
gaussian	O
case	O
we	O
assume	O
we	O
have	O
a	O
gaussian	O
approximation	B
available	O
for	O
(	O
25.4.7	O
)	O
q	O
(	O
ht+1|st+1	O
,	O
v1	O
:	O
t	O
)	O
=	O
n	O
(	O
ht+1	O
g	O
(	O
st+1	O
,	O
g	O
(	O
st+1	O
)	O
)	O
draft	O
march	O
9	O
,	O
2010	O
(	O
25.4.9	O
)	O
463	O
gaussian	O
sum	O
smoothing	B
figure	O
25.3	O
:	O
the	O
ec	O
backpass	O
approximates	O
p	O
(	O
ht+1|st+1	O
,	O
st	O
,	O
v1	O
:	O
t	O
)	O
by	O
p	O
(	O
ht+1|st+1	O
,	O
v1	O
:	O
t	O
)	O
.	O
the	O
moti-	O
vation	O
for	O
this	O
is	O
that	O
st	O
inﬂuences	O
ht+1	O
only	O
indirectly	O
through	O
ht	O
.	O
however	O
,	O
ht	O
will	O
most	O
likely	O
be	O
heavily	O
inﬂuenced	O
by	O
v1	O
:	O
t	O
,	O
so	O
that	O
not	O
knowing	O
the	O
state	O
of	O
st	O
is	O
likely	O
to	O
be	O
of	O
secondary	O
importance	B
.	O
the	O
green	O
shaded	O
node	B
is	O
the	O
variable	B
we	O
wish	O
to	O
ﬁnd	O
the	O
pos-	O
terior	O
for	O
.	O
the	O
values	O
of	O
the	O
blue	O
shaded	O
nodes	O
are	O
known	O
,	O
and	O
the	O
red	O
shaded	O
node	B
indicates	O
a	O
known	O
variable	B
which	O
is	O
assumed	O
unknown	O
in	O
forming	O
the	O
approximation	B
.	O
st−1	O
ht−1	O
vt−1	O
st	O
ht	O
vt	O
st+1	O
st+2	O
ht+1	O
ht+2	O
vt+1	O
vt+2	O
25.4.1	O
continuous	B
smoothing	O
for	O
given	O
st	O
,	O
st+1	O
,	O
an	O
rts	O
style	O
recursion	O
for	O
the	O
smoothed	O
continuous	B
is	O
obtained	O
from	O
equation	B
(	O
25.4.7	O
)	O
,	O
giving	O
(	O
cid:90	O
)	O
q	O
(	O
ht|st	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
=	O
ht+1	O
p	O
(	O
ht|ht+1	O
,	O
st	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
q	O
(	O
ht+1|st+1	O
,	O
v1	O
:	O
t	O
)	O
(	O
25.4.10	O
)	O
to	O
compute	O
equation	B
(	O
25.4.10	O
)	O
we	O
then	O
perform	O
a	O
single	O
update	O
of	O
the	O
lds	O
backward	O
recursion	O
,	O
section	O
(	O
24.4.2	O
)	O
.	O
25.4.2	O
discrete	B
smoothing	O
the	O
second	O
average	B
in	O
equation	B
(	O
25.4.7	O
)	O
corresponds	O
to	O
a	O
recursion	O
for	O
the	O
discrete	B
variable	O
and	O
is	O
given	O
by	O
(	O
cid:104	O
)	O
q	O
(	O
st|ht+1	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
ht+1|st+1	O
,	O
v1	O
:	O
t	O
)	O
≡	O
q	O
(	O
st|st+1	O
,	O
v1	O
:	O
t	O
)	O
.	O
(	O
25.4.11	O
)	O
the	O
average	B
of	O
q	O
(	O
st|ht+1	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
with	O
respect	O
to	O
q	O
(	O
ht+1|st+1	O
,	O
v1	O
:	O
t	O
)	O
can	O
not	O
be	O
achieved	O
in	O
closed	O
form	O
.	O
a	O
simple	O
approach	O
is	O
to	O
approximate	B
the	O
average	B
by	O
evaluation	O
at	O
the	O
mean1	O
(	O
cid:104	O
)	O
q	O
(	O
st|ht+1	O
,	O
st+1v1	O
:	O
t	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
ht+1|st+1	O
,	O
v1	O
:	O
t	O
)	O
≈	O
q	O
(	O
st|ht+1	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
ht+1=	O
(	O
cid:104	O
)	O
ht+1|st+1	O
,	O
v1	O
:	O
t	O
(	O
cid:105	O
)	O
(	O
25.4.12	O
)	O
where	O
(	O
cid:104	O
)	O
ht+1|st+1	O
,	O
v1	O
:	O
t	O
(	O
cid:105	O
)	O
is	O
the	O
mean	B
of	O
ht+1	O
with	O
respect	O
to	O
q	O
(	O
ht+1|st+1	O
,	O
v1	O
:	O
t	O
)	O
.	O
replacing	O
ht+1	O
by	O
its	O
mean	B
gives	O
the	O
approximation	B
(	O
cid:104	O
)	O
q	O
(	O
st|ht+1	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
ht+1|st+1	O
,	O
v1	O
:	O
t	O
)	O
≈	O
where	O
e	O
1	O
z	O
−	O
1	O
2	O
zt	O
t+1	O
(	O
st	O
,	O
st+1	O
)	O
σ−1	O
(	O
st	O
,	O
st+1|v1	O
:	O
t	O
)	O
zt+1	O
(	O
st	O
,	O
st+1	O
)	O
(	O
cid:112	O
)	O
det	O
(	O
σ	O
(	O
st	O
,	O
st+1|v1	O
:	O
t	O
)	O
)	O
q	O
(	O
st|st+1	O
,	O
v1	O
:	O
t	O
)	O
(	O
25.4.13	O
)	O
zt+1	O
(	O
st	O
,	O
st+1	O
)	O
≡	O
(	O
cid:104	O
)	O
ht+1|st+1	O
,	O
v1	O
:	O
t	O
(	O
cid:105	O
)	O
−	O
(	O
cid:104	O
)	O
ht+1|st	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
(	O
cid:105	O
)	O
(	O
25.4.14	O
)	O
and	O
z	O
ensures	O
normalisation	B
over	O
st.	O
σ	O
(	O
st	O
,	O
st+1|v1	O
:	O
t	O
)	O
is	O
the	O
ﬁltered	O
covariance	B
of	O
ht+1	O
given	O
st	O
,	O
st+1	O
and	O
the	O
observations	O
v1	O
:	O
t	O
,	O
which	O
may	O
be	O
taken	O
from	O
σhh	O
in	O
equation	B
(	O
25.3.8	O
)	O
.	O
approximations	O
which	O
take	O
covariance	B
information	O
into	O
account	O
can	O
also	O
be	O
considered	O
,	O
although	O
the	O
above	O
simple	O
(	O
and	O
fast	O
)	O
method	O
may	O
suﬃce	O
in	O
practice	O
[	O
17	O
,	O
192	O
]	O
.	O
25.4.3	O
collapsing	B
the	I
mixture	I
from	O
section	O
(	O
25.4.1	O
)	O
and	O
section	O
(	O
25.4.2	O
)	O
we	O
now	O
have	O
all	O
the	O
terms	O
in	O
equation	B
(	O
25.4.8	O
)	O
to	O
compute	O
the	O
approximation	B
to	O
equation	B
(	O
25.4.7	O
)	O
.	O
due	O
to	O
the	O
summatino	O
over	O
st+1	O
in	O
equation	B
(	O
25.4.7	O
)	O
,	O
the	O
number	O
of	O
mixture	B
components	O
is	O
multiplied	O
by	O
s	O
at	O
each	O
iteration	B
.	O
to	O
prevent	O
an	O
exponential	B
explosion	O
of	O
components	O
,	O
the	O
mixture	B
equation	O
(	O
25.4.7	O
)	O
is	O
then	O
collapsed	O
to	O
a	O
single	O
gaussian	O
q	O
(	O
ht	O
,	O
st|v1	O
:	O
t	O
)	O
→	O
q	O
(	O
ht|st	O
,	O
v1	O
:	O
t	O
)	O
q	O
(	O
st|v1	O
:	O
t	O
)	O
)	O
the	O
collapse	O
to	O
a	O
mixture	B
is	O
discussed	O
in	O
section	O
(	O
25.4.4	O
)	O
.	O
1in	O
general	O
this	O
approximation	B
has	O
the	O
form	O
(	O
cid:104	O
)	O
f	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
≈	O
f	O
(	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
)	O
.	O
(	O
25.4.15	O
)	O
464	O
draft	O
march	O
9	O
,	O
2010	O
gaussian	O
sum	O
smoothing	B
routine	O
needs	O
the	O
results	O
from	O
algorithm	B
(	O
22	O
)	O
.	O
algorithm	B
23	O
aslds	O
:	O
ec	O
backward	O
pass	O
.	O
(	O
cid:80	O
)	O
jt	O
approximates	O
p	O
(	O
st|v1	O
:	O
t	O
)	O
and	O
p	O
(	O
ht|st	O
,	O
v1	O
:	O
t	O
)	O
≡	O
jt=1	O
ut	O
(	O
jt	O
,	O
st	O
)	O
n	O
(	O
gt	O
(	O
jt	O
,	O
st	O
)	O
,	O
gt	O
(	O
jt	O
,	O
st	O
)	O
)	O
using	O
a	O
mixture	O
of	O
gaussians	O
.	O
jt	O
=	O
it	O
,	O
jt	O
≤	O
s	O
×	O
it	O
×	O
jt+1	O
.	O
this	O
gt	O
←	O
ft	O
,	O
gt	O
←	O
ft	O
,	O
ut	O
←	O
wt	O
for	O
t	O
←	O
t	O
−	O
1	O
to	O
1	O
do	O
for	O
s	O
←	O
1	O
to	O
s	O
,	O
s	O
(	O
cid:48	O
)	O
(	O
µ	O
,	O
σ	O
)	O
(	O
i	O
,	O
s	O
,	O
j	O
(	O
cid:48	O
)	O
,	O
s	O
(	O
cid:48	O
)	O
)	O
=	O
ldsbackward	O
(	O
gt+1	O
(	O
j	O
(	O
cid:48	O
)	O
,	O
s	O
(	O
cid:48	O
)	O
)	O
,	O
gt+1	O
(	O
j	O
(	O
cid:48	O
)	O
,	O
s	O
(	O
cid:48	O
)	O
)	O
,	O
ft	O
(	O
i	O
,	O
s	O
)	O
,	O
ft	O
(	O
i	O
,	O
s	O
)	O
,	O
θ	O
(	O
s	O
(	O
cid:48	O
)	O
)	O
)	O
p	O
(	O
it	O
,	O
st|jt+1	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
=	O
(	O
cid:104	O
)	O
p	O
(	O
st	O
=	O
s	O
,	O
it	O
=	O
i|ht+1	O
,	O
st+1	O
=	O
s	O
(	O
cid:48	O
)	O
,	O
jt+1	O
=	O
j	O
(	O
cid:48	O
)	O
,	O
v1	O
:	O
t	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
ht+1|st+1=s	O
(	O
cid:48	O
)	O
,	O
jt+1=j	O
(	O
cid:48	O
)	O
,	O
v1	O
:	O
t	O
)	O
p	O
(	O
i	O
,	O
s	O
,	O
j	O
(	O
cid:48	O
)	O
,	O
s	O
(	O
cid:48	O
)	O
←	O
1	O
to	O
s	O
,	O
i	O
←	O
1	O
to	O
it	O
,	O
j	O
(	O
cid:48	O
)	O
←	O
1	O
to	O
jt+1	O
do	O
|v1	O
:	O
t	O
)	O
←	O
p	O
(	O
st+1	O
=	O
s	O
(	O
cid:48	O
)	O
|v1	O
:	O
t	O
)	O
ut+1	O
(	O
j	O
(	O
cid:48	O
)	O
,	O
s	O
(	O
cid:48	O
)	O
)	O
p	O
(	O
it	O
,	O
st|jt+1	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
end	O
for	O
for	O
st	O
←	O
1	O
to	O
s	O
do	O
collapse	O
the	O
mixture	B
deﬁned	O
by	O
weights	O
p	O
(	O
it	O
=	O
i	O
,	O
st+1	O
=	O
s	O
(	O
cid:48	O
)	O
,	O
jt+1	O
=	O
j	O
(	O
cid:48	O
)	O
|st	O
,	O
v1t	O
)	O
∝	O
p	O
(	O
i	O
,	O
st	O
,	O
j	O
(	O
cid:48	O
)	O
,	O
s	O
(	O
cid:48	O
)	O
|v1	O
:	O
t	O
)	O
,	O
means	O
µ	O
(	O
it	O
,	O
st	O
,	O
jt+1	O
,	O
st+1	O
)	O
and	O
covariances	O
σ	O
(	O
it	O
,	O
st	O
,	O
jt+1	O
,	O
st+1	O
)	O
to	O
a	O
mix-	O
(	O
cid:80	O
)	O
ture	O
with	O
jt	O
components	O
.	O
this	O
deﬁnes	O
the	O
new	O
means	O
gt	O
(	O
jt	O
,	O
st	O
)	O
,	O
covariances	O
gt	O
(	O
jt	O
,	O
st	O
)	O
and	O
mixture	B
weights	O
ut	O
(	O
jt	O
,	O
st	O
)	O
.	O
it	O
,	O
j	O
(	O
cid:48	O
)	O
,	O
s	O
(	O
cid:48	O
)	O
p	O
(	O
it	O
,	O
st	O
,	O
j	O
(	O
cid:48	O
)	O
,	O
s	O
(	O
cid:48	O
)	O
p	O
(	O
st|v1	O
:	O
t	O
)	O
←	O
|v1	O
:	O
t	O
)	O
end	O
for	O
end	O
for	O
25.4.4	O
using	O
mixtures	O
in	O
smoothing	B
the	O
extension	O
to	O
the	O
mixture	B
case	O
is	O
straightforward	O
based	O
on	O
the	O
representation	B
p	O
(	O
it	O
,	O
jt+1	O
,	O
st+1|st	O
,	O
v1	O
:	O
t	O
)	O
p	O
(	O
ht|it	O
,	O
st	O
,	O
jt+1	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
this	O
mixture	B
can	O
then	O
be	O
collapsed	O
to	O
a	O
smaller	O
mixture	B
using	O
any	O
method	O
of	O
choice	O
,	O
to	O
give	O
p	O
(	O
ht|st	O
,	O
v1	O
:	O
t	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
p	O
(	O
ht|st	O
,	O
v1	O
:	O
t	O
)	O
≈	O
jt	O
it	O
,	O
jt+1	O
,	O
st+1	O
q	O
(	O
jt|st	O
,	O
v1	O
:	O
t	O
)	O
q	O
(	O
ht|jt	O
,	O
v1	O
:	O
t	O
)	O
(	O
25.4.19	O
)	O
(	O
25.4.20	O
)	O
(	O
25.4.21	O
)	O
the	O
resulting	O
procedure	O
is	O
sketched	O
in	O
algorithm	B
(	O
23	O
)	O
,	O
including	O
using	O
mixtures	O
in	O
both	O
the	O
forward	O
and	O
backward	O
passes	O
.	O
draft	O
march	O
9	O
,	O
2010	O
465	O
p	O
(	O
ht|st	O
,	O
v1	O
:	O
t	O
)	O
≈	O
j	O
(	O
cid:88	O
)	O
q	O
(	O
ht	O
,	O
st|v1	O
:	O
t	O
)	O
=	O
(	O
cid:88	O
)	O
jt=1	O
it	O
,	O
jt+1	O
,	O
st+1	O
q	O
(	O
jt|st	O
,	O
v1	O
:	O
t	O
)	O
q	O
(	O
ht|jt	O
,	O
v1	O
:	O
t	O
)	O
.	O
(	O
25.4.16	O
)	O
analogously	O
to	O
the	O
case	O
with	O
a	O
single	O
component	O
,	O
p	O
(	O
st+1|v1	O
:	O
t	O
)	O
p	O
(	O
jt+1|st+1	O
,	O
v1	O
:	O
t	O
)	O
q	O
(	O
ht|jt+1	O
,	O
st+1	O
,	O
it	O
,	O
st	O
,	O
v1	O
:	O
t	O
)	O
×	O
(	O
cid:104	O
)	O
q	O
(	O
it	O
,	O
st|ht+1	O
,	O
jt+1	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
ht+1|jt+1	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
(	O
25.4.17	O
)	O
the	O
average	B
in	O
the	O
last	O
line	O
of	O
the	O
above	O
equation	B
can	O
be	O
tackled	O
using	O
the	O
same	O
techniques	O
as	O
outlined	O
in	O
the	O
single	O
gaussian	O
case	O
.	O
to	O
approximate	B
q	O
(	O
ht|jt+1	O
,	O
st+1	O
,	O
it	O
,	O
st	O
,	O
v1	O
:	O
t	O
)	O
we	O
consider	O
this	O
as	O
the	O
marginal	B
of	O
the	O
joint	B
distribution	O
q	O
(	O
ht	O
,	O
ht+1|it	O
,	O
st	O
,	O
jt+1	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
=	O
q	O
(	O
ht|ht+1	O
,	O
it	O
,	O
st	O
,	O
jt+1	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
q	O
(	O
ht+1|it	O
,	O
st	O
,	O
jt+1	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
(	O
25.4.18	O
)	O
as	O
in	O
the	O
case	O
of	O
a	O
single	O
mixture	B
,	O
the	O
problematic	O
term	O
is	O
q	O
(	O
ht+1|it	O
,	O
st	O
,	O
jt+1	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
.	O
analogously	O
to	O
equation	B
(	O
25.4.4	O
)	O
,	O
we	O
make	O
the	O
assumption	O
q	O
(	O
ht+1|it	O
,	O
st	O
,	O
jt+1	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
≈	O
q	O
(	O
ht+1|jt+1	O
,	O
st+1	O
,	O
v1	O
:	O
t	O
)	O
meaning	O
that	O
information	O
about	O
the	O
current	O
switch	O
state	O
st	O
,	O
it	O
is	O
ignored	O
.	O
we	O
can	O
then	O
form	O
25.4.5	O
relation	O
to	O
other	O
methods	O
gaussian	O
sum	O
smoothing	B
a	O
classical	O
smoothing	B
approximation	O
for	O
the	O
slds	O
is	O
generalised	O
pseudo	O
bayes	O
in	O
gpb	O
one	O
starts	O
from	O
the	O
exact	O
recursion	O
(	O
gpb	O
)	O
[	O
13	O
,	O
160	O
,	O
159	O
]	O
.	O
p	O
(	O
st|v1	O
:	O
t	O
)	O
=	O
(	O
cid:88	O
)	O
p	O
(	O
st	O
,	O
st+1|v1	O
:	O
t	O
)	O
=	O
(	O
cid:88	O
)	O
st+1	O
p	O
(	O
st|st+1	O
,	O
v1	O
:	O
t	O
)	O
p	O
(	O
st+1|v1	O
:	O
t	O
)	O
st+1	O
the	O
quantity	O
p	O
(	O
st|st+1	O
,	O
v1	O
:	O
t	O
)	O
is	O
diﬃcult	O
to	O
obtain	O
and	O
gpb	O
makes	O
the	O
approximation	B
p	O
(	O
st|st+1	O
,	O
v1	O
:	O
t	O
)	O
≈	O
p	O
(	O
st|st+1	O
,	O
v1	O
:	O
t	O
)	O
plugging	O
this	O
into	O
equation	B
(	O
25.4.22	O
)	O
we	O
have	O
p	O
(	O
st|v1	O
:	O
t	O
)	O
≈	O
(	O
cid:88	O
)	O
=	O
(	O
cid:88	O
)	O
st+1	O
st+1	O
p	O
(	O
st|st+1	O
,	O
v1	O
:	O
t	O
)	O
p	O
(	O
st+1|v1	O
:	O
t	O
)	O
(	O
cid:80	O
)	O
p	O
(	O
st+1|st	O
)	O
p	O
(	O
st|v1	O
:	O
t	O
)	O
st	O
p	O
(	O
st+1|st	O
)	O
p	O
(	O
st|v1	O
:	O
t	O
)	O
p	O
(	O
st+1|v1	O
:	O
t	O
)	O
(	O
25.4.22	O
)	O
(	O
25.4.23	O
)	O
(	O
25.4.24	O
)	O
(	O
25.4.25	O
)	O
the	O
recursion	O
is	O
initialised	O
with	O
the	O
approximate	B
ﬁltered	O
p	O
(	O
st|v1	O
:	O
t	O
)	O
.	O
computing	O
the	O
smoothed	O
recursion	O
for	O
the	O
switch	O
states	O
in	O
gpb	O
is	O
then	O
equivalent	B
to	O
running	O
the	O
rts	O
backward	O
pass	O
on	O
a	O
hidden	B
markov	O
model	B
,	O
independently	O
of	O
the	O
backward	O
recursion	O
for	O
the	O
continuous	B
variables	O
.	O
the	O
only	O
information	O
the	O
gpb	O
method	O
uses	O
to	O
form	O
the	O
smoothed	O
distribution	B
p	O
(	O
st|v1	O
:	O
t	O
)	O
from	O
the	O
ﬁltered	O
distribution	B
p	O
(	O
st|v1	O
:	O
t	O
)	O
is	O
the	O
markov	O
switch	O
transition	O
p	O
(	O
st+1|st	O
)	O
.	O
this	O
approximation	B
drops	O
information	O
from	O
the	O
future	O
since	O
information	O
passed	O
via	O
the	O
continuous	B
variables	O
is	O
not	O
taken	O
into	O
account	O
.	O
in	O
contrast	O
to	O
gpb	O
,	O
the	O
ec	O
gaussian	O
smoothing	B
technique	O
preserves	O
future	O
information	O
passing	O
through	O
the	O
continuous	B
variables	O
.	O
as	O
for	O
ec	O
,	O
gpb	O
forms	O
an	O
approximation	B
for	O
p	O
(	O
ht|st	O
,	O
v1	O
:	O
t	O
)	O
by	O
using	O
the	O
recursion	O
(	O
25.4.8	O
)	O
where	O
q	O
(	O
st|st+1	O
,	O
v1	O
:	O
t	O
)	O
is	O
given	O
by	O
q	O
(	O
st|st+1	O
,	O
v1	O
:	O
t	O
)	O
.	O
in	O
sldsbackward.m	O
one	O
may	O
choose	O
to	O
use	O
either	O
ec	O
or	O
gbp	O
.	O
example	O
105	O
(	O
traﬃc	O
flow	O
)	O
.	O
a	O
illustration	O
of	O
modelling	B
and	O
inference	B
with	O
a	O
slds	O
is	O
to	O
consider	O
a	O
simple	O
network	O
of	O
traﬃc	O
ﬂow	O
,	O
ﬁg	O
(	O
25.4	O
)	O
.	O
here	O
there	O
are	O
4	O
junctions	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
and	O
traﬃc	O
ﬂows	O
along	O
the	O
roads	O
in	O
the	O
direction	O
indicated	O
.	O
traﬃc	O
ﬂows	O
into	O
the	O
junction	O
at	O
a	O
and	O
then	O
goes	O
via	O
diﬀerent	O
routes	O
to	O
d.	O
flow	O
out	O
of	O
a	O
junction	O
must	O
match	O
the	O
ﬂow	O
in	O
to	O
a	O
junction	O
(	O
up	O
to	O
noise	O
)	O
.	O
there	O
are	O
traﬃc	O
light	O
switches	O
at	O
junctions	O
a	O
and	O
b	O
which	O
,	O
depending	O
on	O
their	O
state	O
,	O
route	O
traﬃc	O
diﬀerently	O
along	O
the	O
roads	O
.	O
using	O
φ	O
to	O
denote	O
the	O
clean	O
(	O
noise	O
free	O
)	O
ﬂow	O
,	O
we	O
model	B
the	O
ﬂows	O
using	O
the	O
switching	O
linear	O
system	O
:	O
φa	O
(	O
t	O
−	O
1	O
)	O
φa	O
(	O
t	O
−	O
1	O
)	O
(	O
0.75	O
×	O
i	O
[	O
sa	O
(	O
t	O
)	O
=	O
1	O
]	O
+	O
1	O
×	O
i	O
[	O
sa	O
(	O
t	O
)	O
=	O
2	O
]	O
+	O
0	O
×	O
i	O
[	O
sa	O
(	O
t	O
)	O
=	O
3	O
]	O
)	O
φa	O
(	O
t	O
−	O
1	O
)	O
(	O
0.25	O
×	O
i	O
[	O
sa	O
(	O
t	O
)	O
=	O
1	O
]	O
+	O
0	O
×	O
i	O
[	O
sa	O
(	O
t	O
)	O
=	O
2	O
]	O
+	O
1	O
×	O
i	O
[	O
sa	O
(	O
t	O
)	O
=	O
3	O
]	O
)	O
φa→b	O
(	O
t	O
−	O
1	O
)	O
(	O
0.5	O
×	O
i	O
[	O
sb	O
(	O
t	O
)	O
=	O
1	O
]	O
+	O
0	O
×	O
i	O
[	O
sb	O
(	O
t	O
)	O
=	O
2	O
]	O
)	O
φa→b	O
(	O
t	O
−	O
1	O
)	O
(	O
0.5	O
×	O
i	O
[	O
sb	O
(	O
t	O
)	O
=	O
1	O
]	O
+	O
1	O
×	O
i	O
[	O
sb	O
(	O
t	O
)	O
=	O
2	O
]	O
)	O
φb→c	O
(	O
t	O
−	O
1	O
)	O
(	O
25.4.26	O
)	O
by	O
identifying	O
the	O
ﬂows	O
at	O
time	O
t	O
with	O
a	O
6	O
dimensional	O
vector	O
hidden	O
variable	B
ht	O
,	O
we	O
can	O
write	O
the	O
above	O
ﬂow	O
equations	O
as	O
	O
=	O
	O
φa	O
(	O
t	O
)	O
φa→d	O
(	O
t	O
)	O
φa→b	O
(	O
t	O
)	O
φb→d	O
(	O
t	O
)	O
φb→c	O
(	O
t	O
)	O
φc→d	O
(	O
t	O
)	O
ht	O
=	O
a	O
(	O
s	O
)	O
ht−1	O
+	O
ηh	O
t	O
(	O
25.4.27	O
)	O
(	O
cid:78	O
)	O
sb	O
,	O
which	O
takes	O
for	O
a	O
set	O
of	O
suitably	O
deﬁned	O
matrices	O
a	O
(	O
s	O
)	O
indexed	O
by	O
the	O
switch	O
variable	B
s	O
=	O
sa	O
3	O
×	O
2	O
=	O
6	O
states	O
.	O
we	O
additionally	O
include	O
noise	O
terms	O
to	O
model	B
cars	O
parking	O
or	O
de-parking	O
during	O
a	O
single	O
timestep	O
.	O
the	O
covariance	B
σh	O
is	O
diagonal	O
with	O
a	O
larger	O
variance	B
at	O
the	O
inﬂow	O
point	O
a	O
to	O
model	B
that	O
the	O
total	O
volume	O
of	O
traﬃc	O
entering	O
the	O
system	B
can	O
vary	O
.	O
noisy	O
measurements	O
of	O
the	O
ﬂow	O
into	O
the	O
network	O
are	O
taken	O
at	O
a	O
v1	O
,	O
t	O
=	O
φa	O
(	O
t	O
)	O
+	O
ηv	O
1	O
(	O
t	O
)	O
466	O
(	O
25.4.28	O
)	O
draft	O
march	O
9	O
,	O
2010	O
gaussian	O
sum	O
smoothing	B
a	O
d	O
b	O
c	O
figure	O
25.4	O
:	O
a	O
representation	B
of	O
the	O
traﬃc	O
ﬂow	O
between	O
junctions	O
at	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
,	O
with	O
traﬃc	O
lights	O
at	O
a	O
and	O
b.	O
if	O
sa	O
=	O
1	O
a	O
→	O
d	O
and	O
a	O
→	O
b	O
carry	O
0.75	O
and	O
0.25	O
of	O
the	O
ﬂow	O
out	O
of	O
a	O
respectively	O
.	O
if	O
sa	O
=	O
2	O
all	O
the	O
ﬂow	O
from	O
a	O
goes	O
through	O
a	O
→	O
d	O
;	O
for	O
sa	O
=	O
3	O
,	O
all	O
the	O
ﬂow	O
goes	O
through	O
a	O
→	O
b.	O
for	O
sb	O
=	O
1	O
the	O
ﬂow	O
out	O
of	O
b	O
is	O
split	O
equally	O
between	O
b	O
→	O
d	O
and	O
b	O
→	O
c.	O
for	O
sb	O
=	O
2	O
all	O
ﬂow	O
out	O
of	O
b	O
goes	O
along	O
b	O
→	O
c.	O
figure	O
25.5	O
:	O
time	O
evolution	O
of	O
the	O
traﬃc	O
ﬂow	O
measured	O
at	O
two	O
points	O
in	O
the	O
network	O
.	O
sensors	O
measure	O
the	O
total	O
ﬂow	O
into	O
the	O
network	O
φa	O
(	O
t	O
)	O
and	O
the	O
total	O
ﬂow	O
out	O
of	O
the	O
network	O
,	O
φd	O
(	O
t	O
)	O
=	O
φa→d	O
(	O
t	O
)	O
+	O
φb→d	O
(	O
t	O
)	O
+	O
φc→d	O
(	O
t	O
)	O
.	O
the	O
total	O
inﬂow	O
at	O
a	O
undergoes	O
a	O
random	O
walk	O
.	O
note	O
that	O
the	O
ﬂow	O
measured	O
at	O
d	O
can	O
momentarily	O
drop	O
to	O
zero	O
if	O
all	O
traﬃc	O
is	O
routed	O
through	O
a	O
→	O
b	O
→	O
c	O
in	O
two	O
consecutive	O
time	O
steps	O
.	O
along	O
with	O
a	O
noisy	O
measurement	O
of	O
the	O
total	O
ﬂow	O
out	O
of	O
the	O
system	B
at	O
d	O
,	O
v2	O
,	O
t	O
=	O
φd	O
(	O
t	O
)	O
=	O
φa→d	O
(	O
t	O
)	O
+	O
φb→d	O
(	O
t	O
)	O
+	O
φc→d	O
(	O
t	O
)	O
+	O
ηv	O
2	O
(	O
t	O
)	O
(	O
25.4.29	O
)	O
the	O
observation	O
model	B
can	O
be	O
represented	O
by	O
vt	O
=	O
bht	O
+	O
ηv	O
t	O
using	O
a	O
constant	O
2	O
×	O
6	O
projection	B
matrix	O
b.	O
the	O
switch	O
variables	O
follow	O
a	O
simple	O
markov	O
transition	O
p	O
(	O
st|st−1	O
)	O
which	O
biases	O
the	O
switches	O
to	O
remain	O
in	O
the	O
same	O
state	O
in	O
preference	O
to	O
jumping	O
to	O
another	O
state	O
.	O
see	O
demosldstraffic.m	O
for	O
details	O
.	O
given	O
the	O
above	O
system	B
and	O
a	O
prior	B
which	O
initialises	O
all	O
ﬂow	O
at	O
a	O
,	O
we	O
draw	O
samples	O
from	O
the	O
model	B
using	O
forward	O
(	O
ancestral	B
)	O
sampling	B
which	O
form	O
the	O
observations	O
v1:100	O
,	O
ﬁg	O
(	O
25.5	O
)	O
.	O
using	O
only	O
the	O
observations	O
and	O
the	O
known	O
model	B
structure	O
we	O
then	O
attempt	O
to	O
infer	O
the	O
latent	B
switch	O
variables	O
and	O
traﬃc	O
ﬂows	O
using	O
gaussian	O
sum	O
ﬁltering	B
and	O
smoothing	B
(	O
ec	O
method	O
)	O
with	O
2	O
mixture	B
components	O
per	O
switch	O
state	O
,	O
ﬁg	O
(	O
25.6	O
)	O
.	O
we	O
note	O
that	O
a	O
naive	O
hmm	O
approximation	B
based	O
on	O
discretising	O
each	O
continuous	B
ﬂow	O
into	O
20	O
bins	O
would	O
contain	O
2	O
×	O
3	O
×	O
206	O
or	O
384	O
million	O
states	O
.	O
even	O
for	O
modest	O
size	O
problems	O
,	O
a	O
naive	O
approximation	O
based	O
on	O
discretisation	O
is	O
therefore	O
impractical	O
.	O
example	O
106	O
(	O
following	O
the	O
price	O
trend	O
)	O
.	O
the	O
following	O
is	O
a	O
simple	O
model	O
of	O
the	O
price	O
trend	O
of	O
a	O
stock	O
,	O
which	O
assumes	O
that	O
the	O
price	O
tends	O
to	O
continue	O
going	O
up	O
(	O
or	O
down	O
)	O
for	O
a	O
while	O
before	O
it	O
reverses	O
direction	O
:	O
h1	O
(	O
t	O
)	O
=	O
h1	O
(	O
t	O
−	O
1	O
)	O
+	O
h2	O
(	O
t	O
−	O
1	O
)	O
+	O
ηh	O
h2	O
(	O
t	O
)	O
=	O
i	O
[	O
s	O
(	O
t	O
)	O
=	O
1	O
]	O
h2	O
(	O
t	O
−	O
1	O
)	O
+	O
ηh	O
v	O
(	O
t	O
)	O
=	O
h1	O
(	O
t	O
)	O
+	O
ηv	O
(	O
st	O
)	O
1	O
(	O
st	O
)	O
2	O
(	O
st	O
)	O
(	O
25.4.30	O
)	O
(	O
25.4.31	O
)	O
(	O
25.4.32	O
)	O
here	O
h1	O
represents	O
the	O
price	O
and	O
h2	O
the	O
direction	O
.	O
there	O
is	O
only	O
a	O
single	O
observation	O
variable	B
at	O
each	O
time	O
,	O
which	O
is	O
the	O
price	O
plus	O
a	O
small	O
amount	O
of	O
noise	O
.	O
there	O
are	O
two	O
switch	O
states	O
,	O
dom	O
(	O
st	O
)	O
=	O
{	O
1	O
,	O
2	O
}	O
.	O
when	O
st	O
=	O
1	O
,	O
the	O
model	B
functions	O
normally	O
,	O
with	O
the	O
direction	O
being	O
equal	O
to	O
the	O
previous	O
direction	O
plus	O
a	O
small	O
amount	O
of	O
noise	O
ηh	O
2	O
(	O
st	O
=	O
1	O
)	O
.	O
when	O
st	O
=	O
2	O
however	O
,	O
the	O
direction	O
is	O
sampled	O
from	O
a	O
gaussian	O
with	O
a	O
large	O
variance	B
.	O
the	O
transition	O
p	O
(	O
st|st−1	O
)	O
is	O
set	O
so	O
that	O
normal	B
dynamics	O
is	O
more	O
likely	O
,	O
and	O
when	O
st	O
=	O
2	O
it	O
is	O
likely	O
to	O
go	O
back	O
to	O
normal	B
dynamics	O
the	O
next	O
timestep	O
.	O
full	O
details	O
are	O
in	O
sldspricemodel.mat	O
.	O
in	O
ﬁg	O
(	O
25.7	O
)	O
we	O
plot	O
some	O
samples	O
from	O
the	O
model	B
and	O
also	O
smoothed	O
inference	B
of	O
the	O
switch	O
distribution	B
,	O
showing	O
how	O
we	O
can	O
a	O
posteriori	O
infer	O
the	O
likely	O
changes	O
in	O
the	O
stock	O
price	O
direction	O
.	O
see	O
also	O
exercise	O
(	O
239	O
)	O
.	O
draft	O
march	O
9	O
,	O
2010	O
467	O
0204060801000204002040608010002040	O
reset	O
models	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
25.6	O
:	O
given	O
the	O
observations	O
from	O
ﬁg	O
(	O
25.5	O
)	O
we	O
infer	O
the	O
ﬂows	O
and	O
switch	O
states	O
of	O
all	O
the	O
latent	B
(	O
a	O
)	O
:	O
the	O
correct	O
latent	B
ﬂows	O
through	O
time	O
along	O
with	O
the	O
switch	O
variable	B
state	O
used	O
to	O
variables	O
.	O
generate	O
the	O
data	B
.	O
the	O
colours	O
corresponds	O
to	O
the	O
ﬂows	O
at	O
the	O
corresponding	O
coloured	O
edges/nodes	O
in	O
(	O
b	O
)	O
:	O
filtered	O
ﬂows	O
based	O
on	O
a	O
i	O
=	O
2	O
gaussian	O
sum	O
forward	O
pass	O
approximation	B
.	O
plotted	O
are	O
ﬁg	O
(	O
25.4	O
)	O
.	O
the	O
6	O
components	O
of	O
the	O
vector	O
(	O
cid:104	O
)	O
ht|v1	O
:	O
t	O
(	O
cid:105	O
)	O
with	O
the	O
posterior	B
distribution	O
of	O
the	O
sa	O
and	O
sb	O
traﬃc	O
light	O
states	O
(	O
c	O
)	O
:	O
smoothed	O
ﬂows	O
(	O
cid:104	O
)	O
ht|v1	O
:	O
t	O
(	O
cid:105	O
)	O
and	O
corresponding	O
smoothed	O
switch	O
p	O
(	O
sa	O
states	O
p	O
(	O
st|v1	O
:	O
t	O
)	O
using	O
a	O
gaussian	O
sum	O
smoothing	B
approximation	O
(	O
ec	O
with	O
j	O
=	O
1	O
)	O
.	O
t|v1	O
:	O
t	O
)	O
plotted	O
below	O
.	O
t	O
|v1	O
:	O
t	O
)	O
,	O
p	O
(	O
sb	O
figure	O
25.7	O
:	O
the	O
top	O
panel	O
is	O
a	O
time	O
series	O
of	O
‘	O
prices	O
’	O
.	O
the	O
prices	O
tend	O
to	O
keep	O
going	O
up	O
or	O
down	O
with	O
infrequent	O
changes	O
in	O
the	O
direction	O
.	O
based	O
on	O
ﬁtting	O
a	O
simple	O
slds	O
model	B
to	O
capture	O
this	O
kind	O
of	O
behaviour	O
,	O
the	O
probability	B
of	O
a	O
signiﬁcant	O
change	O
in	O
the	O
price	O
direction	O
is	O
given	O
in	O
the	O
panel	O
below	O
based	O
on	O
the	O
smoothed	O
distribution	B
p	O
(	O
st	O
=	O
2|v1	O
:	O
t	O
)	O
.	O
25.5	O
reset	O
models	O
reset	O
models	O
are	O
special	O
switching	B
models	O
in	O
which	O
the	O
switch	O
state	O
isolates	O
the	O
present	O
from	O
the	O
past	O
,	O
resetting	O
the	O
position	O
of	O
the	O
latent	B
dynamics	O
(	O
these	O
are	O
also	O
known	O
as	O
changepoint	O
models	O
)	O
.	O
whilst	O
these	O
models	O
are	O
rather	O
general	O
,	O
it	O
can	O
be	O
helpful	O
to	O
consider	O
a	O
speciﬁc	O
model	B
,	O
and	O
here	O
we	O
consider	O
the	O
slds	O
changepoint	B
model	I
with	O
two	O
states	O
.	O
we	O
use	O
the	O
state	O
st	O
=	O
0	O
to	O
denote	O
that	O
the	O
lds	O
continues	O
with	O
the	O
standard	O
dynamics	O
.	O
with	O
st	O
=	O
1	O
,	O
however	O
,	O
the	O
continuous	B
dynamics	O
is	O
reset	O
to	O
a	O
prior	B
:	O
(	O
cid:0	O
)	O
ht	O
µ1	O
,	O
σ1	O
(	O
cid:1	O
)	O
p1	O
(	O
ht	O
)	O
=	O
n	O
(	O
25.5.1	O
)	O
(	O
25.5.2	O
)	O
(	O
25.5.3	O
)	O
st	O
=	O
1	O
p1	O
(	O
ht	O
)	O
(	O
cid:26	O
)	O
p0	O
(	O
ht|ht−1	O
)	O
st	O
=	O
0	O
(	O
cid:0	O
)	O
ht	O
aht−1	O
+	O
µ0	O
,	O
σ0	O
(	O
cid:1	O
)	O
,	O
(	O
cid:26	O
)	O
p0	O
(	O
vt|ht	O
)	O
st	O
=	O
0	O
p1	O
(	O
vt|ht	O
)	O
st	O
=	O
1	O
p	O
(	O
ht|ht−1	O
,	O
st	O
)	O
=	O
where	O
p0	O
(	O
ht|ht−1	O
)	O
=	O
n	O
similarly	O
we	O
write	O
p	O
(	O
vt|ht	O
,	O
st	O
)	O
=	O
for	O
simplicity	O
we	O
assume	O
the	O
switch	O
dynamics	O
are	O
ﬁrst	B
order	I
markov	O
with	O
transition	O
p	O
(	O
st|st−1	O
)	O
.	O
under	O
this	O
model	B
the	O
dynamics	O
follows	O
a	O
standard	O
lds	O
,	O
but	O
when	O
st	O
=	O
1	O
,	O
ht	O
is	O
reset	O
to	O
a	O
value	B
drawn	O
from	O
468	O
draft	O
march	O
9	O
,	O
2010	O
020406080100010200204060801000102002040608010001020020406080100010200204060801000102002040608010001020204060801001232040608010012020406080100010200204060801000102002040608010001020020406080100010200204060801000102002040608010001020204060801001232040608010012020406080100010200204060801000102002040608010001020020406080100010200204060801000102002040608010001020204060801001232040608010012020406080100120140160180200−40−30−20−1001002040608010012014016018020000.20.40.60.81	O
reset	O
models	O
c1	O
h1	O
v1	O
c2	O
h2	O
v2	O
c3	O
h3	O
v3	O
c4	O
h4	O
v4	O
figure	O
25.8	O
:	O
the	O
independence	B
structure	O
of	O
a	O
reset	B
model	I
.	O
square	O
nodes	O
ct	O
denote	O
the	O
binary	O
reset	O
variables	O
and	O
st	O
the	O
state	O
dynamics	O
.	O
the	O
ht	O
are	O
continuous	B
variables	O
,	O
and	O
vt	O
continuous	B
observations	O
.	O
if	O
the	O
dynamics	O
resets	O
,	O
the	O
dependence	B
of	O
the	O
continuous	B
ht	O
on	O
the	O
past	O
is	O
cut	B
.	O
a	O
gaussian	O
distribution	B
,	O
independent	O
of	O
the	O
past	O
.	O
such	O
models	O
might	O
be	O
of	O
interest	O
in	O
prediction	B
where	O
the	O
time-series	O
is	O
following	O
a	O
trend	O
but	O
suddenly	O
changes	O
and	O
the	O
past	O
is	O
forgotten	O
.	O
whilst	O
this	O
may	O
not	O
seem	O
like	O
a	O
big	O
change	O
to	O
the	O
model	B
,	O
this	O
model	B
is	O
computationally	O
more	O
tractable	O
,	O
scaling	O
with	O
o	O
(	O
cid:0	O
)	O
t	O
2	O
(	O
cid:1	O
)	O
,	O
compared	O
to	O
o	O
(	O
cid:0	O
)	O
t	O
2t	O
(	O
cid:1	O
)	O
in	O
the	O
general	O
two-state	O
slds	O
.	O
to	O
see	O
this	O
,	O
consider	O
the	O
ﬁltering	B
recursion	O
(	O
cid:90	O
)	O
(	O
cid:88	O
)	O
α	O
(	O
ht	O
,	O
st	O
)	O
∝	O
ht−1	O
st−1	O
(	O
cid:90	O
)	O
p	O
(	O
vt|ht	O
,	O
st	O
)	O
p	O
(	O
ht|ht−1	O
,	O
st	O
)	O
p	O
(	O
st|st−1	O
)	O
α	O
(	O
ht−1	O
,	O
st−1	O
)	O
(	O
cid:88	O
)	O
p0	O
(	O
vt|ht	O
)	O
p0	O
(	O
ht|ht−1	O
)	O
p	O
(	O
st	O
=	O
0|st−1	O
)	O
α	O
(	O
ht−1	O
,	O
st−1	O
)	O
we	O
now	O
consider	O
the	O
two	O
cases	O
α	O
(	O
ht	O
,	O
st	O
=	O
0	O
)	O
∝	O
(	O
cid:88	O
)	O
st−1	O
p	O
(	O
st	O
=	O
1|st−1	O
)	O
α	O
(	O
ht−1	O
,	O
st−1	O
)	O
p	O
(	O
st	O
=	O
1|st−1	O
)	O
α	O
(	O
st−1	O
)	O
α	O
(	O
ht	O
,	O
st	O
=	O
1	O
)	O
∝	O
p1	O
(	O
vt|ht	O
)	O
p1	O
(	O
ht	O
)	O
ht−1	O
ht−1	O
st−1	O
(	O
cid:90	O
)	O
∝	O
p1	O
(	O
vt|ht	O
)	O
p1	O
(	O
ht	O
)	O
(	O
cid:88	O
)	O
(	O
cid:90	O
)	O
st−1	O
(	O
cid:90	O
)	O
(	O
25.5.4	O
)	O
(	O
25.5.5	O
)	O
(	O
25.5.6	O
)	O
(	O
25.5.8	O
)	O
(	O
25.5.9	O
)	O
469	O
equation	B
(	O
25.5.6	O
)	O
shows	O
that	O
p	O
(	O
ht	O
,	O
st	O
=	O
1|v1	O
:	O
t	O
)	O
is	O
not	O
a	O
mixture	B
model	I
in	O
ht	O
,	O
but	O
contains	O
only	O
a	O
single	O
component	O
proportional	O
to	O
p1	O
(	O
vt|ht	O
)	O
p1	O
(	O
ht	O
)	O
.	O
if	O
we	O
use	O
this	O
information	O
in	O
equation	B
(	O
25.5.5	O
)	O
we	O
have	O
α	O
(	O
ht	O
,	O
st	O
=	O
0	O
)	O
∝	O
ht−1	O
p0	O
(	O
vt|ht	O
)	O
p0	O
(	O
ht|ht−1	O
)	O
p	O
(	O
st	O
=	O
0|st−1	O
=	O
0	O
)	O
α	O
(	O
ht−1	O
,	O
st−1	O
=	O
0	O
)	O
+	O
ht−1	O
p0	O
(	O
vt|ht	O
)	O
p0	O
(	O
ht|ht−1	O
)	O
p	O
(	O
st	O
=	O
0|st−1	O
=	O
1	O
)	O
α	O
(	O
ht−1	O
,	O
st−1	O
=	O
1	O
)	O
(	O
25.5.7	O
)	O
assuming	O
α	O
(	O
ht−1	O
,	O
st−1	O
=	O
0	O
)	O
is	O
a	O
mixture	B
distribution	O
with	O
k	O
components	O
,	O
then	O
α	O
(	O
ht	O
,	O
st	O
=	O
0	O
)	O
will	O
be	O
a	O
in	O
general	O
,	O
therefore	O
,	O
α	O
(	O
ht	O
,	O
st	O
=	O
0	O
)	O
will	O
contain	O
t	O
components	O
and	O
mixture	B
with	O
k	O
+	O
1	O
components	O
.	O
α	O
(	O
ht	O
,	O
st	O
=	O
1	O
)	O
a	O
single	O
component	O
.	O
as	O
opposed	O
to	O
the	O
full	O
slds	O
case	O
,	O
the	O
number	O
of	O
components	O
therefore	O
grows	O
only	O
linearly	O
with	O
time	O
,	O
as	O
opposed	O
to	O
exponentially	O
.	O
this	O
means	O
that	O
the	O
computational	O
eﬀort	O
to	O
perform	O
exact	O
ﬁltering	O
scales	O
as	O
o	O
(	O
cid:0	O
)	O
t	O
2	O
(	O
cid:1	O
)	O
.	O
run-length	O
formalism	O
one	O
may	O
also	O
describe	O
reset	O
models	O
using	O
a	O
‘	O
run-length	O
’	O
formalism	O
using	O
at	O
each	O
time	O
t	O
a	O
latent	B
variable	I
rt	O
which	O
describes	O
the	O
length	O
of	O
the	O
current	O
segment	O
[	O
3	O
]	O
.	O
if	O
there	O
is	O
a	O
change	O
,	O
the	O
run-length	O
variable	B
is	O
reset	O
to	O
zero	O
,	O
otherwise	O
it	O
is	O
increased	O
by	O
1	O
:	O
rt	O
=	O
0	O
rt	O
=	O
rt−1	O
+	O
1	O
1	O
−	O
pcp	O
p	O
(	O
rt|rt−1	O
)	O
=	O
(	O
cid:26	O
)	O
pcp	O
p	O
(	O
v1	O
:	O
t	O
,	O
r1	O
:	O
t	O
)	O
=	O
(	O
cid:89	O
)	O
t	O
draft	O
march	O
9	O
,	O
2010	O
where	O
pcp	O
is	O
the	O
probability	B
of	O
a	O
reset	O
(	O
or	O
‘	O
changepoint	B
’	O
)	O
.	O
the	O
joint	B
distribution	O
is	O
given	O
by	O
p	O
(	O
rt|rt−1	O
)	O
p	O
(	O
vt|v1	O
:	O
t−1	O
,	O
rt	O
)	O
,	O
p	O
(	O
vt|v1	O
:	O
t−1	O
,	O
rt	O
)	O
=	O
p	O
(	O
vt|vt−rt	O
:	O
t−1	O
)	O
with	O
the	O
understanding	O
that	O
if	O
rt	O
=	O
0	O
then	O
p	O
(	O
vt|vt−rt	O
:	O
t−1	O
)	O
=	O
p	O
(	O
vt	O
)	O
.	O
the	O
graphical	O
model	B
of	O
this	O
distribution	B
is	O
awkward	O
to	O
draw	O
since	O
the	O
number	O
of	O
links	O
depends	O
on	O
the	O
run-length	O
rt	O
.	O
predictions	O
can	O
be	O
made	O
using	O
p	O
(	O
vt+1|vt−rt	O
:	O
t	O
)	O
p	O
(	O
rt|v1	O
:	O
t	O
)	O
(	O
25.5.10	O
)	O
reset	O
models	O
rt	O
p	O
(	O
vt+1|v1	O
:	O
t	O
)	O
=	O
(	O
cid:88	O
)	O
p	O
(	O
rt	O
,	O
v1	O
:	O
t	O
)	O
=	O
(	O
cid:88	O
)	O
=	O
(	O
cid:88	O
)	O
=	O
(	O
cid:88	O
)	O
rt−1	O
rt−1	O
where	O
the	O
ﬁltered	O
‘	O
run-length	O
’	O
p	O
(	O
rt|v1	O
:	O
t	O
)	O
is	O
given	O
by	O
the	O
forward	O
recursion	O
:	O
p	O
(	O
rt	O
,	O
rt−1	O
,	O
v1	O
:	O
t−1	O
,	O
vt	O
)	O
=	O
(	O
cid:88	O
)	O
p	O
(	O
vt|rt	O
,	O
rt−1	O
,	O
v1	O
:	O
t−1	O
)	O
p	O
(	O
rt|rt−1	O
,	O
v1	O
:	O
t−1	O
)	O
p	O
(	O
rt−1	O
,	O
v1	O
:	O
t−1	O
)	O
p	O
(	O
rt|rt−1	O
)	O
p	O
(	O
vt|vt−rt	O
:	O
t−1	O
)	O
p	O
(	O
rt−1	O
,	O
v1	O
:	O
t−1	O
)	O
rt−1	O
p	O
(	O
rt	O
,	O
vt|rt−1	O
,	O
v1	O
:	O
t−1	O
)	O
p	O
(	O
rt−1	O
,	O
v1	O
:	O
t−1	O
)	O
which	O
shows	O
that	O
ﬁltered	O
inference	B
scales	O
with	O
o	O
(	O
cid:0	O
)	O
t	O
2	O
(	O
cid:1	O
)	O
.	O
rt−1	O
25.5.1	O
a	O
poisson	O
reset	B
model	I
the	O
changepoint	B
structure	O
is	O
not	O
limited	O
to	O
conditionally	O
gaussian	O
cases	O
only	O
.	O
to	O
illustrate	O
this	O
,	O
we	O
consider	O
the	O
following	O
model2	O
:	O
at	O
each	O
time	O
t	O
,	O
we	O
observe	O
a	O
count	O
yt	O
which	O
we	O
assume	O
is	O
poisson	O
distributed	O
with	O
an	O
unknown	O
positive	O
intensity	O
h.	O
the	O
intensity	O
is	O
constant	O
,	O
but	O
at	O
certain	O
unknown	O
times	O
t	O
,	O
it	O
jumps	O
to	O
a	O
new	O
value	B
.	O
the	O
indicator	O
variable	O
ct	O
denotes	O
whether	O
time	O
t	O
is	O
such	O
a	O
changepoint	B
or	O
not	O
.	O
mathematically	O
,	O
the	O
model	B
is	O
:	O
p	O
(	O
h0	O
)	O
=	O
g	O
(	O
h0	O
;	O
a0	O
,	O
b0	O
)	O
p	O
(	O
ct	O
)	O
=	O
be	O
(	O
ct	O
;	O
π	O
)	O
p	O
(	O
vt|ht	O
)	O
=	O
po	O
(	O
vt	O
;	O
ht	O
)	O
p	O
(	O
ht|ht−1	O
,	O
ct	O
)	O
=	O
i	O
[	O
ct	O
=	O
0	O
]	O
δ	O
(	O
ht	O
,	O
ht−1	O
)	O
+	O
i	O
[	O
ct	O
=	O
1	O
]	O
g	O
(	O
ht	O
;	O
ν	O
,	O
b	O
)	O
(	O
25.5.11	O
)	O
(	O
25.5.12	O
)	O
(	O
25.5.13	O
)	O
(	O
25.5.14	O
)	O
the	O
symbols	O
g	O
,	O
be	O
and	O
po	O
denote	O
the	O
gamma	B
,	O
bernoulli	O
and	O
the	O
poisson	O
distributions	O
respectively	O
:	O
g	O
(	O
h	O
;	O
a	O
,	O
b	O
)	O
=	O
exp	O
(	O
(	O
a	O
−	O
1	O
)	O
log	O
h	O
−	O
bh	O
−	O
log	O
γ	O
(	O
a	O
)	O
+	O
a	O
log	O
b	O
)	O
be	O
(	O
c	O
;	O
π	O
)	O
=	O
exp	O
(	O
c	O
log	O
π	O
+	O
(	O
1	O
−	O
c	O
)	O
log	O
(	O
1	O
−	O
π	O
)	O
)	O
po	O
(	O
v	O
;	O
h	O
)	O
=	O
exp	O
(	O
v	O
log	O
h	O
−	O
h	O
−	O
log	O
γ	O
(	O
v	O
+	O
1	O
)	O
)	O
(	O
25.5.15	O
)	O
(	O
25.5.16	O
)	O
(	O
25.5.17	O
)	O
given	O
observed	O
counts	O
v1	O
:	O
t	O
,	O
the	O
task	O
is	O
to	O
ﬁnd	O
the	O
posterior	B
probability	O
of	O
a	O
change	O
and	O
the	O
associated	O
intensity	O
levels	O
for	O
each	O
region	O
between	O
two	O
consecutive	O
changepoints	O
.	O
plugging	O
the	O
above	O
deﬁnitions	O
in	O
the	O
generic	O
updates	O
equation	B
(	O
25.5.5	O
)	O
and	O
equation	B
(	O
25.5.6	O
)	O
,	O
we	O
see	O
that	O
α	O
(	O
ht	O
,	O
ct	O
=	O
0	O
)	O
is	O
a	O
gamma	B
potential	O
,	O
and	O
that	O
α	O
(	O
gt	O
,	O
ct	O
=	O
1	O
)	O
is	O
a	O
mixture	O
of	O
gamma	O
potentials	O
,	O
where	O
a	O
gamma	B
potential	O
is	O
deﬁned	O
as	O
φ	O
(	O
h	O
)	O
=	O
elg	O
(	O
h	O
;	O
a	O
,	O
b	O
)	O
(	O
25.5.18	O
)	O
via	O
the	O
triple	O
(	O
a	O
,	O
b	O
,	O
l	O
)	O
.	O
for	O
the	O
corrector	O
update	O
step	O
we	O
need	O
to	O
calculate	O
the	O
product	O
of	O
a	O
poisson	O
term	O
with	O
the	O
observation	O
model	B
p	O
(	O
vt|ht	O
)	O
=	O
po	O
(	O
vt	O
;	O
ht	O
)	O
.	O
a	O
useful	O
property	O
of	O
the	O
poisson	O
distribution	B
is	O
that	O
,	O
given	O
the	O
observation	O
,	O
the	O
latent	B
variable	I
is	O
gamma	B
distributed	O
:	O
po	O
(	O
v	O
;	O
h	O
)	O
=	O
v	O
log	O
h	O
−	O
h	O
−	O
log	O
γ	O
(	O
v	O
+	O
1	O
)	O
=	O
(	O
v	O
+	O
1	O
−	O
1	O
)	O
log	O
h	O
−	O
h	O
−	O
log	O
γ	O
(	O
v	O
+	O
1	O
)	O
=	O
g	O
(	O
h	O
;	O
v	O
+	O
1	O
,	O
1	O
)	O
(	O
25.5.19	O
)	O
(	O
25.5.20	O
)	O
(	O
25.5.21	O
)	O
hence	O
,	O
the	O
update	O
equation	B
requires	O
multiplication	O
of	O
two	O
gamma	B
potentials	O
.	O
a	O
nice	O
property	O
of	O
the	O
gamma	B
density	O
is	O
that	O
the	O
product	O
of	O
two	O
gamma	B
densities	O
is	O
also	O
a	O
gamma	B
potential	O
:	O
(	O
a1	O
,	O
b1	O
,	O
l1	O
)	O
×	O
(	O
a2	O
,	O
b2	O
,	O
l2	O
)	O
=	O
(	O
a1	O
+	O
a2	O
−	O
1	O
,	O
b1	O
+	O
b2	O
,	O
l1	O
+	O
l2	O
+	O
g	O
(	O
a1	O
,	O
b1	O
,	O
a2	O
,	O
b2	O
)	O
)	O
(	O
25.5.22	O
)	O
2this	O
example	O
is	O
due	O
to	O
taylan	O
cemgil	O
.	O
470	O
draft	O
march	O
9	O
,	O
2010	O
reset	O
models	O
where	O
g	O
(	O
a1	O
,	O
b1	O
,	O
a2	O
,	O
b2	O
)	O
≡	O
log	O
γ	O
(	O
a1	O
+	O
a2	O
−	O
1	O
)	O
γ	O
(	O
a1	O
)	O
γ	O
(	O
a2	O
)	O
+	O
log	O
(	O
b1	O
+	O
b2	O
)	O
+	O
a1	O
log	O
(	O
b1/	O
(	O
b1	O
+	O
b2	O
)	O
)	O
+	O
a2	O
log	O
(	O
b2/	O
(	O
b1	O
+	O
b2	O
)	O
)	O
(	O
25.5.23	O
)	O
the	O
α	O
recursions	O
for	O
this	O
reset	B
model	I
are	O
therefore	O
closed	O
in	O
the	O
space	O
of	O
a	O
mixture	O
of	O
gamma	O
potentials	O
,	O
with	O
an	O
additional	O
gamma	B
potential	O
in	O
the	O
mixture	B
at	O
each	O
timestep	O
.	O
a	O
similar	O
approach	B
can	O
be	O
used	O
to	O
form	O
the	O
smoothing	B
recursions	O
.	O
example	O
107	O
(	O
coal	O
mining	O
disasters	O
)	O
.	O
we	O
illustrate	O
the	O
algorithm	B
on	O
the	O
coal	O
mining	O
disaster	O
dataset	O
[	O
144	O
]	O
.	O
the	O
data	B
set	O
consists	O
of	O
the	O
number	O
of	O
deadly	O
coal-mining	O
disasters	O
in	O
england	O
per	O
year	O
over	O
a	O
time	O
span	O
of	O
112	O
years	O
from	O
1851	O
to	O
1962.	O
it	O
is	O
widely	O
agreed	O
in	O
the	O
statistical	O
literature	O
that	O
a	O
change	O
in	O
the	O
intensity	O
(	O
the	O
expected	O
value	B
of	O
the	O
number	O
of	O
disasters	O
)	O
occurs	O
around	O
the	O
year	O
1890	O
,	O
after	O
new	O
health	O
and	O
safety	O
regulations	O
were	O
introduced	O
.	O
in	O
ﬁg	O
(	O
25.9	O
)	O
we	O
show	O
the	O
marginals	O
p	O
(	O
ht|y1	O
:	O
t	O
)	O
along	O
with	O
the	O
ﬁltering	B
density	O
.	O
note	O
that	O
we	O
are	O
not	O
constraining	O
the	O
number	O
of	O
changepoints	O
and	O
in	O
principle	O
allow	O
any	O
number	O
.	O
the	O
smoothed	O
density	B
suggests	O
a	O
sharp	O
decrease	O
around	O
t	O
=	O
1890.	O
figure	O
25.9	O
:	O
estimation	O
of	O
change	O
points	O
.	O
(	O
top	O
)	O
coal	O
mining	O
disaster	O
dataset	O
.	O
(	O
middle	O
)	O
filtered	O
estimate	O
of	O
the	O
marginal	B
intensity	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
and	O
(	O
bottom	O
)	O
smoothed	O
estimate	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
.	O
we	O
evaluate	O
these	O
mix-	O
ture	O
of	O
gamma	B
distributions	O
on	O
a	O
ﬁxed	O
grid	O
of	O
h	O
and	O
show	O
the	O
density	B
as	O
a	O
function	B
of	O
t.	O
here	O
,	O
darker	O
color	O
means	O
higher	O
probability	B
.	O
25.5.2	O
hmm-reset	O
the	O
reset	B
model	I
deﬁned	O
by	O
equations	O
(	O
25.5.1,25.5.3	O
)	O
above	O
is	O
useful	O
in	O
many	O
applications	O
,	O
but	O
is	O
limited	O
since	O
only	O
a	O
single	O
dynamical	O
model	O
is	O
considered	O
.	O
an	O
important	O
extension	O
is	O
to	O
consider	O
a	O
set	O
of	O
available	O
dynamical	O
models	O
,	O
indexed	O
by	O
st	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
s	O
}	O
,	O
with	O
a	O
reset	O
that	O
cuts	O
dependency	O
of	O
the	O
continuous	B
variable	O
on	O
the	O
past	O
[	O
93	O
,	O
57	O
]	O
:	O
(	O
cid:26	O
)	O
p0	O
(	O
ht|ht−1	O
,	O
st	O
)	O
p1	O
(	O
ht|st	O
)	O
p	O
(	O
ht|ht−1	O
,	O
st	O
,	O
ct	O
)	O
=	O
ct	O
=	O
0	O
ct	O
=	O
1	O
(	O
25.5.24	O
)	O
with	O
the	O
reset	O
α	O
recursions	O
,	O
equations	O
(	O
25.5.5,25.5.6	O
)	O
on	O
replacing	O
ht	O
by	O
(	O
ht	O
,	O
st	O
)	O
.	O
to	O
see	O
this	O
we	O
consider	O
the	O
ﬁltering	B
recursion	O
for	O
the	O
two	O
cases	O
the	O
computational	B
complexity	I
of	O
ﬁltering	B
for	O
this	O
model	B
is	O
o	O
(	O
cid:0	O
)	O
s2t	O
2	O
(	O
cid:1	O
)	O
which	O
can	O
be	O
understood	O
by	O
analogy	O
the	O
states	O
st	O
follow	O
a	O
markovian	O
dynamics	O
p	O
(	O
st|st−1	O
,	O
ct−1	O
)	O
,	O
see	O
ﬁg	O
(	O
25.10	O
)	O
.	O
a	O
reset	O
occurs	O
if	O
the	O
state	O
st	O
changes	O
,	O
otherwise	O
,	O
no	O
reset	O
occurs	O
:	O
p	O
(	O
ct	O
=	O
1|st	O
,	O
st−1	O
)	O
=	O
i	O
[	O
st	O
(	O
cid:54	O
)	O
=	O
st−1	O
]	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
p0	O
(	O
vt|ht	O
,	O
st	O
)	O
p0	O
(	O
ht|ht−1	O
,	O
st	O
)	O
p	O
(	O
st|st−1	O
,	O
ct−1	O
)	O
p	O
(	O
ct	O
=	O
0|st	O
,	O
st−1	O
)	O
α	O
(	O
ht−1	O
,	O
ct−1	O
)	O
(	O
25.5.26	O
)	O
α	O
(	O
ht	O
,	O
st	O
,	O
ct	O
=	O
0	O
)	O
=	O
ht−1	O
st−1	O
,	O
ct−1	O
(	O
25.5.25	O
)	O
(	O
cid:90	O
)	O
α	O
(	O
ht	O
,	O
st	O
,	O
ct	O
=	O
1	O
)	O
=	O
p1	O
(	O
vt|ht	O
,	O
st	O
)	O
p1	O
(	O
ht|st	O
)	O
p	O
(	O
st|st−1	O
,	O
ct	O
)	O
p	O
(	O
ct	O
=	O
1|st	O
,	O
st−1	O
)	O
α	O
(	O
ht−1	O
,	O
st−1	O
,	O
ct−1	O
)	O
(	O
cid:90	O
)	O
=	O
p1	O
(	O
vt|ht	O
,	O
st	O
)	O
p1	O
(	O
ht|st	O
)	O
(	O
cid:88	O
)	O
st−1	O
,	O
ct−1	O
ht−1	O
st−1	O
,	O
ct−1	O
draft	O
march	O
9	O
,	O
2010	O
p	O
(	O
ct	O
=	O
1|st	O
,	O
st−1	O
)	O
p	O
(	O
st|st−1	O
,	O
ct−1	O
)	O
α	O
(	O
st−1	O
,	O
ct−1	O
)	O
(	O
25.5.27	O
)	O
471	O
186018701880189019001910192019301940195019600246	O
#	O
of	O
accidentsfiltered	O
intensity18601870188018901900191019201930194019501960246smoothed	O
intensityyear18601870188018901900191019201930194019501960246	O
c1	O
s1	O
h1	O
v1	O
c2	O
s2	O
h2	O
v2	O
c3	O
s3	O
h3	O
v3	O
c4	O
s4	O
h4	O
v4	O
exercises	O
figure	O
25.10	O
:	O
the	O
independence	B
structure	O
of	O
a	O
hmm-reset	O
model	B
.	O
square	O
nodes	O
ct	O
denote	O
discrete	B
switch	O
variables	O
;	O
ht	O
are	O
continuous	B
latent	O
variables	O
,	O
and	O
vt	O
continuous	B
ob-	O
servations	O
.	O
the	O
discrete	B
state	O
ct	O
determines	O
which	O
linear	B
dynamical	I
system	I
from	O
a	O
ﬁnite	O
set	O
of	O
linear	O
dynamical	O
systems	O
is	O
operational	O
at	O
time	O
t.	O
from	O
equation	B
(	O
25.5.27	O
)	O
we	O
see	O
that	O
α	O
(	O
ht	O
,	O
st	O
,	O
ct	O
=	O
1	O
)	O
contains	O
only	O
a	O
single	O
component	O
proportional	O
to	O
p1	O
(	O
vt|ht	O
,	O
st	O
)	O
p1	O
(	O
ht|st	O
)	O
.	O
this	O
is	O
therefore	O
exactly	O
analogous	O
to	O
the	O
standard	O
reset	O
model	B
,	O
except	O
that	O
we	O
need	O
now	O
to	O
index	O
a	O
set	O
of	O
messages	O
with	O
st	O
,	O
therefore	O
each	O
message	B
taking	O
o	O
(	O
s	O
)	O
steps	O
to	O
compute	O
.	O
the	O
computational	O
eﬀort	O
to	O
perform	O
exact	O
ﬁltering	O
scales	O
as	O
o	O
(	O
cid:0	O
)	O
s2t	O
2	O
(	O
cid:1	O
)	O
.	O
25.6	O
code	O
sldsforward.m	O
:	O
slds	O
forward	O
sldsbackward.m	O
:	O
slds	O
backward	O
(	O
expectation	B
correction	I
)	O
mix2mix.m	O
:	O
collapse	O
a	O
mixture	O
of	O
gaussians	O
to	O
a	O
smaller	O
mixture	O
of	O
gaussians	O
sldsmarggauss.m	O
:	O
marginalise	O
an	O
slds	O
gaussian	O
mixture	B
logeps.m	O
:	O
logarithm	O
with	O
oﬀset	O
to	O
deal	O
with	O
log	O
(	O
0	O
)	O
demosldstraffic.m	O
:	O
demo	O
of	O
traﬃc	O
flow	O
using	O
a	O
switching	B
linear	I
dynamical	I
system	I
25.7	O
exercises	O
exercise	O
239.	O
consider	O
the	O
setup	O
described	O
in	O
example	O
(	O
106	O
)	O
,	O
for	O
which	O
the	O
full	O
slds	O
model	B
is	O
given	O
in	O
sldspricemodel.m	O
,	O
following	O
the	O
notation	O
used	O
in	O
demosldstraffic.m	O
.	O
given	O
the	O
data	B
in	O
the	O
vec-	O
tor	O
v	O
your	O
task	O
is	O
to	O
ﬁt	O
a	O
prediction	B
model	O
to	O
the	O
data	B
.	O
to	O
do	O
so	O
,	O
approximate	B
the	O
ﬁltered	O
distribution	B
p	O
(	O
h	O
(	O
t	O
)	O
,	O
s	O
(	O
t	O
)	O
|v1	O
:	O
t	O
)	O
using	O
a	O
mixture	O
of	O
i	O
=	O
2	O
components	O
.	O
the	O
prediction	B
of	O
the	O
price	O
at	O
the	O
next	O
day	O
is	O
then	O
(	O
25.7.1	O
)	O
vpred	O
(	O
t	O
+	O
1	O
)	O
=	O
(	O
cid:104	O
)	O
h1	O
(	O
t	O
)	O
+	O
h2	O
(	O
t	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
h	O
(	O
t	O
)	O
|v1	O
:	O
t	O
)	O
where	O
p	O
(	O
h	O
(	O
t	O
)	O
|v1	O
:	O
t	O
)	O
=	O
(	O
cid:80	O
)	O
st	O
p	O
(	O
h	O
(	O
t	O
)	O
,	O
st|v1	O
:	O
t	O
)	O
.	O
1.	O
compute	O
the	O
mean	B
prediction	O
error	O
mean_abs_pred_error=mean	O
(	O
abs	O
(	O
vpred	O
(	O
2	O
:	O
end	O
)	O
-v	O
(	O
2	O
:	O
end	O
)	O
)	O
)	O
2.	O
compute	O
the	O
mean	B
naive	O
prediction	B
error	O
mean_abs_pred_error_naive=mean	O
(	O
abs	O
(	O
v	O
(	O
1	O
:	O
end-1	O
)	O
-v	O
(	O
2	O
:	O
end	O
)	O
)	O
)	O
which	O
corresponds	O
to	O
saying	O
that	O
tomorrow	O
’	O
s	O
price	O
will	O
be	O
the	O
same	O
as	O
today	O
’	O
s	O
.	O
you	O
might	O
ﬁnd	O
sldsmarggauss.m	O
of	O
interest	O
.	O
exercise	O
240.	O
the	O
data	B
in	O
ﬁg	O
(	O
25.11	O
)	O
are	O
observed	O
prices	O
from	O
an	O
intermittent	O
mean-reverting	O
process	O
,	O
contained	O
in	O
meanrev.mat	O
.	O
there	O
are	O
two	O
states	O
s	O
=	O
2.	O
there	O
is	O
a	O
true	O
(	O
latent	B
)	O
price	O
pt	O
and	O
an	O
observed	O
price	O
vt	O
(	O
which	O
is	O
plotted	O
)	O
.	O
when	O
s	O
=	O
1	O
,	O
the	O
true	O
underlying	O
price	O
reverts	O
back	O
towards	O
the	O
mean	B
m	O
=	O
10	O
with	O
rate	O
r	O
=	O
0.9.	O
otherwise	O
the	O
true	O
price	O
follows	O
a	O
random	O
walk	O
:	O
(	O
cid:26	O
)	O
r	O
(	O
pt−1	O
−	O
m	O
)	O
+	O
m	O
+	O
ηp	O
t	O
pt−1	O
+	O
ηp	O
t	O
pt	O
=	O
472	O
st	O
=	O
1	O
st	O
=	O
2	O
(	O
25.7.2	O
)	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
figure	O
25.11	O
:	O
data	B
from	O
an	O
intermittent	O
mean-reverting	O
process	O
.	O
see	O
exercise	O
(	O
240	O
)	O
.	O
where	O
(	O
cid:26	O
)	O
ηp	O
t	O
∼	O
n	O
(	O
ηp	O
n	O
(	O
ηp	O
t	O
0	O
,	O
0.0001	O
)	O
st	O
=	O
1	O
st	O
=	O
2	O
t	O
0	O
,	O
0.01	O
)	O
the	O
observed	O
price	O
vt	O
is	O
related	O
to	O
the	O
unknown	O
price	O
pt	O
by	O
vt	O
∼	O
n	O
(	O
vt	O
pt	O
,	O
0.001	O
)	O
(	O
25.7.3	O
)	O
(	O
25.7.4	O
)	O
it	O
is	O
known	O
that	O
95	O
%	O
of	O
the	O
time	O
st+1	O
is	O
in	O
the	O
same	O
state	O
as	O
at	O
time	O
t	O
and	O
that	O
at	O
time	O
t	O
=	O
1	O
either	O
state	O
of	O
s	O
is	O
equally	O
likely	O
.	O
also	O
at	O
t	O
=	O
1	O
,	O
p1	O
∼	O
n	O
(	O
p1	O
m	O
,	O
0.1	O
)	O
.	O
based	O
on	O
this	O
information	O
,	O
and	O
using	O
gaussian	O
sum	O
ﬁltering	B
with	O
i	O
=	O
2	O
components	O
(	O
use	O
sldsforward.m	O
)	O
,	O
what	O
is	O
the	O
probability	B
at	O
time	O
t	O
=	O
280	O
that	O
the	O
dynamics	O
is	O
following	O
a	O
random	O
walk	O
,	O
p	O
(	O
s280	O
=	O
2|v1:280	O
)	O
?	O
repeat	O
this	O
computation	O
for	O
smoothing	B
p	O
(	O
s280	O
=	O
2|v1:400	O
)	O
based	O
on	O
using	O
expectation	B
correction	I
with	O
i	O
=	O
j	O
=	O
2	O
components	O
.	O
draft	O
march	O
9	O
,	O
2010	O
473	O
0501001502002503003504009.51010.511	O
exercises	O
474	O
draft	O
march	O
9	O
,	O
2010	O
chapter	O
26	O
distributed	B
computation	I
26.1	O
introduction	O
how	O
natural	B
organisms	O
process	O
information	O
is	O
a	O
fascinating	O
subject	O
and	O
one	O
of	O
the	O
grand	O
challenges	O
of	O
science	O
.	O
whilst	O
this	O
subject	O
is	O
still	O
in	O
its	O
early	O
stages	O
,	O
loosely	O
speaking	O
,	O
there	O
are	O
some	O
generic	O
properties	B
that	O
most	O
such	O
systems	O
are	O
believed	O
to	O
possess	O
:	O
patterns	O
are	O
stored	O
in	O
a	O
set	O
of	O
neurons	O
;	O
recall	O
of	O
patterns	O
is	O
robust	O
to	O
noise	O
;	O
transmission	O
between	O
neurons	O
is	O
of	O
a	O
binary	O
nature	O
and	O
is	O
stochastic	O
;	O
information	O
processing	O
is	O
distributed	O
and	O
highly	O
modular	O
.	O
in	O
this	O
chapter	O
we	O
discuss	O
some	O
of	O
the	O
classical	O
toy	O
models	O
that	O
have	O
been	O
developed	O
as	O
a	O
test	O
bed	O
for	O
analysing	O
such	O
properties	B
[	O
62	O
,	O
76	O
,	O
65	O
,	O
132	O
]	O
.	O
in	O
particular	O
we	O
discuss	O
some	O
classical	O
models	O
from	O
a	O
probabilistic	B
viewpoint	O
.	O
26.2	O
stochastic	O
hopﬁeld	O
networks	O
hopﬁeld	O
networks	O
are	O
models	O
of	O
biological	O
memory	O
in	O
which	O
a	O
pattern	O
is	O
represented	O
by	O
the	O
activity	O
of	O
a	O
set	O
of	O
v	O
interconnected	O
neurons	O
.	O
the	O
term	O
‘	O
network	O
’	O
here	O
refers	O
to	O
the	O
set	O
of	O
neurons	O
,	O
see	O
ﬁg	O
(	O
26.1	O
)	O
,	O
and	O
not	O
the	O
belief	B
network	I
representation	O
of	O
distribution	B
of	O
neural	O
states	O
unrolled	O
through	O
time	O
,	O
ﬁg	O
(	O
26.2	O
)	O
.	O
at	O
time	O
t	O
neuron	O
i	O
ﬁres	O
vi	O
(	O
t	O
)	O
=	O
+1	O
or	O
is	O
quiescent	O
vi	O
(	O
t	O
)	O
=	O
−1	O
(	O
not	O
ﬁring	O
)	O
depending	O
on	O
the	O
states	O
of	O
the	O
neurons	O
at	O
the	O
preceding	O
time	O
t	O
−	O
1.	O
explicitly	O
,	O
neuron	O
i	O
ﬁres	O
depending	O
on	O
the	O
potential	B
v	O
(	O
cid:88	O
)	O
j=1	O
ai	O
(	O
t	O
)	O
≡	O
θi	O
+	O
wijvj	O
(	O
t	O
)	O
(	O
26.2.1	O
)	O
where	O
wij	O
characterizes	O
the	O
eﬃcacy	O
with	O
which	O
neuron	O
j	O
transmits	O
a	O
binary	O
signal	O
to	O
neuron	O
i.	O
the	O
threshold	O
θi	O
relates	O
to	O
the	O
neuron	O
’	O
s	O
predisposition	O
to	O
ﬁring	O
.	O
writing	O
the	O
state	O
of	O
the	O
network	O
at	O
time	O
t	O
as	O
v	O
(	O
t	O
)	O
≡	O
(	O
v1	O
(	O
t	O
)	O
,	O
.	O
.	O
.	O
,	O
vv	O
(	O
t	O
)	O
)	O
,	O
the	O
probability	B
that	O
neuron	O
i	O
ﬁres	O
at	O
time	O
t	O
+	O
1	O
is	O
modelled	O
as	O
p	O
(	O
vi	O
(	O
t	O
+	O
1	O
)	O
=	O
1|v	O
(	O
t	O
)	O
)	O
=	O
σβ	O
(	O
ai	O
(	O
t	O
)	O
)	O
(	O
26.2.2	O
)	O
where	O
σβ	O
(	O
x	O
)	O
=	O
1/	O
(	O
1+	O
e−βx	O
)	O
and	O
β	O
controls	O
the	O
level	O
of	O
stochastic	O
behaviour	O
of	O
the	O
neuron	O
.	O
the	O
probability	B
of	O
being	O
in	O
the	O
quiescent	O
state	O
is	O
given	O
by	O
normalization	O
p	O
(	O
vi	O
(	O
t	O
+	O
1	O
)	O
=	O
−1|v	O
(	O
t	O
)	O
)	O
=	O
1	O
−	O
p	O
(	O
vi	O
(	O
t	O
+	O
1	O
)	O
=	O
1|v	O
(	O
t	O
)	O
)	O
=	O
1	O
−	O
σβ	O
(	O
ai	O
(	O
t	O
)	O
)	O
these	O
two	O
rules	O
can	O
be	O
compactly	O
written	O
as	O
p	O
(	O
vi	O
(	O
t	O
+	O
1	O
)	O
|v	O
(	O
t	O
)	O
)	O
=	O
σβ	O
(	O
vi	O
(	O
t	O
+	O
1	O
)	O
ai	O
(	O
t	O
)	O
)	O
which	O
follows	O
directly	O
from	O
1	O
−	O
σβ	O
(	O
x	O
)	O
=	O
σβ	O
(	O
−x	O
)	O
.	O
475	O
(	O
26.2.3	O
)	O
(	O
26.2.4	O
)	O
learning	B
sequences	O
v4	O
v4	O
v1	O
v3	O
v2	O
figure	O
26.1	O
:	O
a	O
depiction	O
of	O
a	O
hopﬁeld	O
network	O
(	O
for	O
5	O
neurons	O
)	O
.	O
the	O
connectivity	O
of	O
the	O
neurons	O
is	O
described	O
by	O
a	O
weight	B
matrix	O
with	O
elements	O
wij	O
.	O
the	O
graph	B
represents	O
a	O
snapshot	O
of	O
the	O
state	O
of	O
all	O
neurons	O
at	O
time	O
t	O
which	O
simultaneously	O
update	O
as	O
function	O
of	O
the	O
network	O
at	O
the	O
previous	O
time	O
t	O
−	O
1.	O
in	O
the	O
limit	O
β	O
→	O
∞	O
,	O
the	O
neuron	O
updates	O
deterministically	O
vi	O
(	O
t	O
+	O
1	O
)	O
=	O
sgn	O
(	O
ai	O
(	O
t	O
)	O
)	O
(	O
26.2.5	O
)	O
in	O
a	O
synchronous	O
hopﬁeld	O
network	O
,	O
all	O
neurons	O
update	O
independently	O
and	O
simultaneously	O
,	O
so	O
that	O
we	O
can	O
represent	O
the	O
temporal	O
evolution	O
of	O
the	O
neurons	O
as	O
a	O
dynamic	B
bayes	O
network	O
,	O
ﬁg	O
(	O
26.2	O
)	O
p	O
(	O
v	O
(	O
t	O
+	O
1	O
)	O
|v	O
(	O
t	O
)	O
)	O
=	O
p	O
(	O
vi	O
(	O
t	O
+	O
1	O
)	O
|v	O
(	O
t	O
)	O
)	O
.	O
(	O
26.2.6	O
)	O
v	O
(	O
cid:89	O
)	O
i=1	O
given	O
this	O
toy	O
description	O
of	O
how	O
neurons	O
update	O
,	O
how	O
can	O
we	O
use	O
the	O
network	O
to	O
do	O
interesting	O
things	O
,	O
for	O
example	O
to	O
store	O
a	O
set	O
of	O
patterns	O
and	O
recall	O
them	O
under	O
some	O
cue	O
.	O
the	O
patterns	O
will	O
be	O
stored	O
in	O
the	O
weights	O
and	O
in	O
the	O
following	O
section	O
we	O
address	O
how	O
to	O
learn	O
suitable	O
parameters	O
wij	O
and	O
θi	O
to	O
learn	O
temporal	O
sequences	B
based	O
on	O
a	O
simple	O
local	O
learning	B
rule	O
.	O
26.3	O
learning	B
sequences	O
26.3.1	O
a	O
single	O
sequence	O
given	O
a	O
sequence	O
of	O
network	O
states	O
,	O
v	O
=	O
{	O
v	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
v	O
(	O
t	O
)	O
}	O
,	O
we	O
would	O
like	O
the	O
network	O
to	O
‘	O
store	O
’	O
this	O
se-	O
quence	O
such	O
that	O
it	O
can	O
be	O
recalled	O
under	O
some	O
cue	O
.	O
that	O
is	O
,	O
if	O
the	O
network	O
is	O
initialized	O
in	O
the	O
correct	O
starting	O
state	O
of	O
the	O
training	B
sequence	O
v	O
(	O
t	O
=	O
1	O
)	O
,	O
the	O
remainder	O
of	O
the	O
training	B
sequence	O
for	O
t	O
>	O
1	O
should	O
be	O
reproduced	O
under	O
the	O
deterministic	B
dynamics	O
equation	B
(	O
26.2.5	O
)	O
,	O
without	O
error	O
.	O
two	O
classical	O
approaches	O
to	O
learning	B
a	O
temporal	O
sequence	O
are	O
the	O
hebb1	O
and	O
pseudo	B
inverse	I
rules	O
[	O
132	O
]	O
.	O
in	O
both	O
the	O
standard	O
hebb	O
and	O
pi	O
cases	O
,	O
the	O
thresholds	O
θi	O
are	O
usually	O
set	O
to	O
zero	O
.	O
standard	O
hebb	O
rule	O
t−1	O
(	O
cid:88	O
)	O
t=1	O
wij	O
=	O
1	O
v	O
vi	O
(	O
t	O
+	O
1	O
)	O
vj	O
(	O
t	O
)	O
(	O
26.3.1	O
)	O
1donald	O
hebb	O
,	O
a	O
neurobiologist	O
actually	O
stated	O
[	O
127	O
]	O
let	O
us	O
assume	O
that	O
the	O
persistence	O
or	O
repetition	O
of	O
a	O
reverberatory	O
activity	O
(	O
or	O
‘	O
trace	O
’	O
)	O
tends	O
to	O
induce	O
lasting	O
cellular	O
changes	O
that	O
add	O
to	O
its	O
stability	O
.	O
.	O
.	O
when	O
an	O
axon	O
of	O
cell	O
a	O
is	O
near	O
enough	O
to	O
excite	O
a	O
cell	O
b	O
and	O
repeatedly	O
or	O
persistently	O
takes	O
part	O
in	O
ﬁring	O
it	O
,	O
some	O
growth	O
process	O
or	O
metabolic	O
change	O
takes	O
place	O
in	O
one	O
or	O
both	O
cells	O
such	O
that	O
a	O
’	O
s	O
eﬃciency	O
,	O
as	O
one	O
of	O
the	O
cells	O
ﬁring	O
b	O
,	O
is	O
increased	O
.	O
this	O
statement	O
is	O
sometimes	O
interpreted	O
to	O
mean	B
that	O
weights	O
are	O
exclusively	O
of	O
the	O
correlation	B
form	O
equation	B
(	O
26.3.1	O
)	O
(	O
see	O
[	O
270	O
]	O
for	O
a	O
discussion	O
)	O
.	O
this	O
can	O
severely	O
limit	O
the	O
performance	B
and	O
introduce	O
adverse	O
storage	O
artifacts	O
including	O
local	B
minima	O
[	O
132	O
]	O
.	O
476	O
draft	O
march	O
9	O
,	O
2010	O
learning	B
sequences	O
v4	O
(	O
t	O
)	O
v3	O
(	O
t	O
)	O
v2	O
(	O
t	O
)	O
v1	O
(	O
t	O
)	O
v4	O
(	O
t	O
+	O
1	O
)	O
v3	O
(	O
t	O
+	O
1	O
)	O
v2	O
(	O
t	O
+	O
1	O
)	O
v1	O
(	O
t	O
+	O
1	O
)	O
figure	O
26.2	O
:	O
a	O
dynamic	B
bayesian	O
network	O
representation	O
of	O
a	O
hopﬁeld	O
network	O
.	O
the	O
network	O
operates	O
by	O
simultaneously	O
generating	O
a	O
new	O
set	O
of	O
neuron	O
states	O
according	O
to	O
equation	B
(	O
26.2.6	O
)	O
.	O
equation	B
(	O
26.2.6	O
)	O
deﬁnes	O
a	O
markov	O
transition	B
matrix	I
,	O
modelling	B
the	O
transition	O
probability	O
v	O
(	O
t	O
)	O
→	O
v	O
(	O
t+1	O
)	O
and	O
further-	O
more	O
imposes	O
the	O
constraint	O
that	O
the	O
neurons	O
are	O
conditionally	O
independent	O
given	O
the	O
previous	O
state	O
of	O
the	O
network	O
.	O
the	O
hebb	O
rule	O
can	O
be	O
motivated	O
mathematically	O
by	O
considering	O
(	O
cid:88	O
)	O
j	O
wijvj	O
(	O
t	O
)	O
=	O
=	O
1	O
v	O
1	O
v	O
τ	O
=1	O
vi	O
(	O
τ	O
+	O
1	O
)	O
(	O
cid:88	O
)	O
t−1	O
(	O
cid:88	O
)	O
vi	O
(	O
t	O
+	O
1	O
)	O
(	O
cid:88	O
)	O
t−1	O
(	O
cid:88	O
)	O
j	O
j	O
1	O
v	O
τ	O
(	O
cid:54	O
)	O
=t	O
=	O
vi	O
(	O
t	O
+	O
1	O
)	O
+	O
v2	O
j	O
(	O
t	O
)	O
+	O
vj	O
(	O
τ	O
)	O
vj	O
(	O
t	O
)	O
t−1	O
(	O
cid:88	O
)	O
vi	O
(	O
τ	O
+	O
1	O
)	O
(	O
cid:88	O
)	O
1	O
v	O
τ	O
(	O
cid:54	O
)	O
=t	O
j	O
vi	O
(	O
τ	O
+	O
1	O
)	O
(	O
cid:88	O
)	O
j	O
vj	O
(	O
τ	O
)	O
vj	O
(	O
t	O
)	O
if	O
the	O
patterns	O
are	O
uncorrelated	O
then	O
the	O
‘	O
interference	O
’	O
term	O
t−1	O
(	O
cid:88	O
)	O
vi	O
(	O
τ	O
+	O
1	O
)	O
(	O
cid:88	O
)	O
τ	O
=1	O
j	O
ω	O
≡	O
vj	O
(	O
τ	O
)	O
vj	O
(	O
t	O
)	O
/v	O
vj	O
(	O
τ	O
)	O
vj	O
(	O
t	O
)	O
(	O
26.3.2	O
)	O
(	O
26.3.3	O
)	O
(	O
26.3.4	O
)	O
(	O
26.3.5	O
)	O
will	O
be	O
relatively	O
small	O
.	O
to	O
see	O
this	O
,	O
we	O
ﬁrst	O
note	O
that	O
for	O
randomly	O
drawn	O
patterns	O
,	O
the	O
mean	B
of	O
ω	O
is	O
zero	O
,	O
since	O
τ	O
(	O
cid:54	O
)	O
=	O
t	O
and	O
the	O
patterns	O
are	O
randomly	O
±1	O
.	O
the	O
variance	B
is	O
therefore	O
given	O
by	O
for	O
j	O
(	O
cid:54	O
)	O
=	O
k	O
,	O
all	O
the	O
terms	O
are	O
independent	O
and	O
contribute	O
zero	O
on	O
average	B
.	O
therefore	O
(	O
cid:48	O
)	O
)	O
vk	O
(	O
t	O
)	O
(	O
cid:11	O
)	O
j	O
(	O
t	O
)	O
(	O
cid:11	O
)	O
1	O
v	O
2	O
τ	O
,	O
τ	O
(	O
cid:48	O
)	O
(	O
cid:54	O
)	O
=t	O
j	O
,	O
k	O
1	O
v	O
2	O
(	O
cid:88	O
)	O
t−1	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
t−1	O
(	O
cid:88	O
)	O
(	O
cid:10	O
)	O
ω2	O
(	O
cid:11	O
)	O
=	O
(	O
cid:10	O
)	O
ω2	O
(	O
cid:11	O
)	O
=	O
(	O
cid:10	O
)	O
ω2	O
(	O
cid:11	O
)	O
=	O
(	O
cid:10	O
)	O
vi	O
(	O
τ	O
+	O
1	O
)	O
vi	O
(	O
τ	O
(	O
cid:10	O
)	O
vi	O
(	O
τ	O
+	O
1	O
)	O
vi	O
(	O
τ	O
(	O
cid:10	O
)	O
v2	O
meaning	O
that	O
the	O
sign	O
of	O
(	O
cid:80	O
)	O
i	O
(	O
τ	O
+	O
1	O
)	O
v2	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
τ	O
,	O
τ	O
(	O
cid:48	O
)	O
=1	O
j	O
1	O
v	O
2	O
τ	O
(	O
cid:54	O
)	O
=t	O
j	O
(	O
cid:48	O
)	O
+	O
1	O
)	O
vj	O
(	O
τ	O
)	O
vj	O
(	O
t	O
)	O
vk	O
(	O
τ	O
(	O
cid:48	O
)	O
+	O
1	O
)	O
vj	O
(	O
τ	O
)	O
vj	O
(	O
τ	O
(	O
cid:48	O
)	O
)	O
v2	O
j	O
(	O
t	O
)	O
(	O
cid:11	O
)	O
=	O
t	O
−	O
1	O
v	O
j	O
(	O
τ	O
)	O
v2	O
when	O
τ	O
(	O
cid:54	O
)	O
=	O
τ	O
(	O
cid:48	O
)	O
all	O
the	O
terms	O
are	O
independent	O
zero	O
mean	B
and	O
contribute	O
zero	O
.	O
hence	O
(	O
26.3.6	O
)	O
(	O
26.3.7	O
)	O
(	O
26.3.8	O
)	O
(	O
26.3.9	O
)	O
477	O
provided	O
that	O
the	O
number	O
of	O
neurons	O
v	O
is	O
signiﬁcantly	O
larger	O
than	O
the	O
length	O
of	O
the	O
sequence	O
,	O
t	O
,	O
then	O
the	O
average	B
size	O
of	O
the	O
interference	O
will	O
be	O
small	O
.	O
in	O
this	O
case	O
the	O
term	O
vi	O
(	O
t+1	O
)	O
in	O
equation	B
(	O
26.3.4	O
)	O
dominates	O
,	O
j	O
wijvj	O
(	O
t	O
)	O
will	O
be	O
that	O
of	O
vi	O
(	O
t	O
+	O
1	O
)	O
,	O
and	O
the	O
correct	O
pattern	O
sequence	O
recalled	O
.	O
the	O
hebb	O
rule	O
is	O
capable	O
of	O
storing	O
a	O
random	O
(	O
uncorrelated	O
)	O
temporal	O
sequence	O
of	O
length	O
0.269v	O
time	O
steps	O
[	O
85	O
]	O
.	O
however	O
,	O
the	O
hebb	O
rule	O
performs	O
poorly	O
for	O
the	O
case	O
of	O
correlated	O
patterns	O
since	O
interference	O
from	O
the	O
other	O
patterns	O
becomes	O
signiﬁcant	O
[	O
132	O
,	O
65	O
]	O
.	O
pseudo	B
inverse	I
rule	I
the	O
pi	O
rule	O
ﬁnds	O
a	O
matrix	B
[	O
w	O
]	O
ij	O
=	O
wij	O
that	O
solves	O
the	O
linear	B
equations	O
wijvj	O
(	O
t	O
)	O
=	O
vi	O
(	O
t	O
+	O
1	O
)	O
,	O
t	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
t	O
−	O
1	O
(	O
cid:88	O
)	O
j	O
draft	O
march	O
9	O
,	O
2010	O
(	O
cid:16	O
)	O
(	O
cid:80	O
)	O
(	O
cid:17	O
)	O
under	O
this	O
condition	O
sgn	O
j	O
wijvj	O
(	O
t	O
)	O
recalled	O
.	O
in	O
matrix	B
notation	O
we	O
require	O
wv	O
=	O
ˆv	O
where	O
[	O
v	O
]	O
it	O
=	O
vi	O
(	O
t	O
)	O
,	O
t	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
t	O
−	O
1	O
,	O
(	O
cid:17	O
)	O
−1	O
(	O
cid:16	O
)	O
w	O
=	O
ˆv	O
vtv	O
vt	O
=	O
sgn	O
(	O
vi	O
(	O
t	O
+	O
1	O
)	O
)	O
=	O
vi	O
(	O
t	O
+	O
1	O
)	O
so	O
that	O
patterns	O
will	O
be	O
correctly	O
learning	B
sequences	O
(	O
cid:105	O
)	O
(	O
cid:104	O
)	O
ˆv	O
it	O
=	O
vi	O
(	O
t	O
+	O
1	O
)	O
,	O
t	O
=	O
2	O
,	O
.	O
.	O
.	O
,	O
t	O
(	O
26.3.11	O
)	O
(	O
26.3.10	O
)	O
(	O
26.3.12	O
)	O
for	O
t	O
<	O
v	O
the	O
problem	B
is	O
under-determined	O
.	O
one	O
solution	O
is	O
given	O
by	O
the	O
pseudo	B
inverse	I
:	O
the	O
pseudo	B
inverse	I
(	O
pi	O
)	O
rule	O
can	O
store	O
any	O
sequence	O
of	O
v	O
linearly	B
independent	I
patterns	O
.	O
whilst	O
attractive	O
compared	O
to	O
the	O
standard	O
hebb	O
in	O
terms	O
of	O
its	O
ability	O
to	O
store	O
longer	O
correlated	O
sequences	B
,	O
this	O
rule	O
suﬀers	O
from	O
very	O
small	O
basins	O
of	O
attraction	O
for	O
temporally	O
correlated	O
patterns	O
,	O
see	O
ﬁg	O
(	O
26.3	O
)	O
.	O
the	O
maximum	B
likelihood	I
hebb	O
rule	O
an	O
alternative	O
to	O
the	O
above	O
classical	O
algorithms	O
is	O
to	O
view	O
this	O
as	O
a	O
problem	B
of	O
pattern	O
storage	O
in	O
the	O
dbn	O
,	O
equation	B
(	O
26.2.6	O
)	O
[	O
19	O
]	O
.	O
first	O
,	O
we	O
need	O
to	O
clarify	O
what	O
we	O
mean	B
by	O
‘	O
store	O
’	O
.	O
given	O
that	O
we	O
initialize	O
the	O
network	O
in	O
a	O
state	O
v	O
(	O
t	O
=	O
1	O
)	O
,	O
we	O
wish	O
that	O
the	O
remaining	O
sequence	O
will	O
be	O
generated	O
with	O
high	O
probability	B
.	O
that	O
is	O
,	O
we	O
wish	O
to	O
adjust	O
the	O
network	O
parameters	O
such	O
that	O
the	O
probability	B
p	O
(	O
v	O
(	O
t	O
)	O
,	O
v	O
(	O
t	O
−	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
v	O
(	O
2	O
)	O
|v	O
(	O
1	O
)	O
)	O
(	O
26.3.13	O
)	O
is	O
maximal2	O
.	O
furthermore	O
,	O
we	O
might	O
hope	O
that	O
the	O
sequence	O
will	O
be	O
recalled	O
with	O
high	O
probability	B
not	O
just	O
when	O
initialized	O
in	O
the	O
correct	O
state	O
but	O
also	O
for	O
states	O
close	O
(	O
in	O
hamming	O
distance	O
)	O
to	O
the	O
correct	O
initial	O
state	O
v	O
(	O
1	O
)	O
.	O
due	O
to	O
the	O
markov	O
nature	O
of	O
the	O
dynamics	O
,	O
the	O
conditional	B
likelihood	I
is	O
p	O
(	O
v	O
(	O
t	O
)	O
,	O
v	O
(	O
t	O
−	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
v	O
(	O
2	O
)	O
|v	O
(	O
1	O
)	O
)	O
=	O
p	O
(	O
v	O
(	O
t	O
+	O
1	O
)	O
|v	O
(	O
t	O
)	O
)	O
(	O
26.3.14	O
)	O
this	O
is	O
a	O
product	O
of	O
transitions	O
from	O
given	O
states	O
to	O
given	O
states	O
.	O
since	O
these	O
transition	O
probabilities	O
are	O
known	O
(	O
26.2.6,26.2.2	O
)	O
,	O
the	O
conditional	B
likelihood	I
can	O
be	O
easily	O
evaluated	O
.	O
the	O
sequence	O
log	O
(	O
conditional	B
)	O
likelihood	B
is	O
t−1	O
(	O
cid:89	O
)	O
t=1	O
p	O
(	O
v	O
(	O
t	O
+	O
1	O
)	O
|v	O
(	O
t	O
)	O
)	O
=	O
(	O
cid:88	O
)	O
t	O
l	O
(	O
w	O
,	O
θ	O
)	O
≡	O
log	O
t−1	O
(	O
cid:88	O
)	O
v	O
(	O
cid:88	O
)	O
t=1	O
i=1	O
log	O
p	O
(	O
v	O
(	O
t	O
+	O
1	O
)	O
|v	O
(	O
t	O
)	O
)	O
=	O
log	O
σβ	O
(	O
vi	O
(	O
t	O
+	O
1	O
)	O
ai	O
(	O
t	O
)	O
)	O
(	O
26.3.15	O
)	O
our	O
task	O
is	O
then	O
to	O
ﬁnd	O
weights	O
w	O
and	O
thresholds	O
θ	O
that	O
maximisise	O
l	O
(	O
w	O
,	O
θ	O
)	O
.	O
there	O
is	O
no	O
closed	O
form	O
solution	O
and	O
the	O
weights	O
therefore	O
need	O
to	O
be	O
determined	O
numerically	O
.	O
this	O
corresponds	O
to	O
a	O
straightfor-	O
ward	O
computational	O
problem	O
since	O
the	O
log	O
likelihood	B
is	O
a	O
convex	B
function	I
.	O
to	O
show	O
this	O
,	O
we	O
compute	O
the	O
hessian	O
(	O
neglecting	O
θ	O
for	O
expositional	O
clarity	O
–	O
this	O
does	O
not	O
aﬀect	O
the	O
conclusions	O
)	O
:	O
t−1	O
(	O
cid:89	O
)	O
t=1	O
t−1	O
(	O
cid:88	O
)	O
t=1	O
d2l	O
dwijdwkl	O
=	O
−β2	O
where	O
we	O
deﬁned	O
(	O
vi	O
(	O
t	O
+	O
1	O
)	O
vj	O
(	O
t	O
)	O
)	O
γi	O
(	O
t	O
)	O
(	O
1	O
−	O
γi	O
(	O
t	O
)	O
)	O
vk	O
(	O
t	O
+	O
1	O
)	O
vl	O
(	O
t	O
)	O
δik	O
γi	O
(	O
t	O
)	O
≡	O
1	O
−	O
σβ	O
(	O
vi	O
(	O
t	O
+	O
1	O
)	O
ai	O
(	O
t	O
)	O
)	O
.	O
2static	O
patterns	O
can	O
also	O
be	O
considered	O
in	O
this	O
framework	O
as	O
a	O
set	O
of	O
patterns	O
that	O
map	B
to	O
each	O
other	O
.	O
478	O
draft	O
march	O
9	O
,	O
2010	O
(	O
26.3.16	O
)	O
(	O
26.3.17	O
)	O
learning	B
sequences	O
figure	O
26.3	O
:	O
leftmost	O
panel	O
:	O
the	O
temporally	O
highly-	O
correlated	O
training	B
sequence	O
we	O
desire	O
to	O
store	O
.	O
the	O
other	O
panels	O
show	O
the	O
temporal	O
evolution	O
of	O
the	O
network	O
af-	O
ter	O
initialization	O
in	O
the	O
correct	O
starting	O
state	O
but	O
cor-	O
rupted	O
with	O
30	O
%	O
noise	O
.	O
during	O
recall	O
,	O
deterministic	B
up-	O
dates	O
β	O
=	O
∞	O
were	O
used	O
.	O
the	O
maximum	B
likelihood	I
rule	O
was	O
trained	O
using	O
10	O
batch	B
epochs	O
with	O
η	O
=	O
0.1.	O
see	O
also	O
demohopfield.m	O
it	O
is	O
straightforward	O
to	O
show	O
that	O
the	O
hessian	O
is	O
negative	O
deﬁnite	O
(	O
see	O
exercise	O
(	O
244	O
)	O
)	O
and	O
hence	O
the	O
likeli-	O
hood	O
has	O
a	O
single	O
global	B
maximum	O
.	O
to	O
increase	O
the	O
likelihood	B
of	O
the	O
sequence	O
,	O
we	O
can	O
use	O
a	O
simple	O
method	O
such	O
as	O
gradient	O
ascent3	O
wnew	O
ij	O
=	O
wij	O
+	O
η	O
dl	O
dwij	O
,	O
θnew	O
i	O
=	O
θi	O
+	O
η	O
dl	O
dθi	O
where	O
t−1	O
(	O
cid:88	O
)	O
t=1	O
dl	O
dwij	O
=	O
β	O
γi	O
(	O
t	O
)	O
vi	O
(	O
t	O
+	O
1	O
)	O
vj	O
(	O
t	O
)	O
,	O
t−1	O
(	O
cid:88	O
)	O
t=1	O
dl	O
dθi	O
=	O
β	O
(	O
26.3.18	O
)	O
γi	O
(	O
t	O
)	O
vi	O
(	O
t	O
+	O
1	O
)	O
(	O
26.3.19	O
)	O
the	O
learning	B
rate	I
η	O
is	O
chosen	O
empirically	O
to	O
be	O
suﬃciently	O
small	O
to	O
ensure	O
convergence	O
.	O
the	O
learning	B
rule	O
equation	B
(	O
26.3.19	O
)	O
can	O
be	O
seen	O
as	O
a	O
modiﬁed	O
hebb	O
learning	B
rule	O
,	O
the	O
basic	O
hebb	O
rule	O
being	O
given	O
when	O
γi	O
(	O
t	O
)	O
≡	O
1.	O
as	O
learning	O
progresses	O
,	O
the	O
γi	O
(	O
t	O
)	O
will	O
typically	O
tend	O
to	O
values	O
close	O
to	O
either	O
1	O
or	O
0	O
,	O
and	O
hence	O
the	O
learning	B
rule	O
can	O
be	O
seen	O
as	O
asymptotically	O
equivalent	B
to	O
making	O
an	O
update	O
only	O
in	O
the	O
case	O
of	O
disagreement	O
(	O
ai	O
(	O
t	O
)	O
and	O
vi	O
(	O
t	O
+	O
1	O
)	O
are	O
of	O
diﬀerent	O
signs	O
)	O
.	O
this	O
batch	B
training	O
procedure	O
can	O
be	O
readily	O
converted	O
to	O
an	O
online	B
in	O
which	O
an	O
update	O
occurs	O
immediately	O
after	O
the	O
presentation	O
of	O
two	O
consecutive	O
patterns	O
.	O
storage	O
capacity	B
of	O
the	O
ml	O
hebb	O
rule	O
the	O
ml	O
hebb	O
rule	O
is	O
capable	O
of	O
storing	O
a	O
sequence	O
of	O
v	O
linearly	B
independent	I
patterns	O
.	O
to	O
see	O
this	O
,	O
we	O
can	O
form	O
an	O
input-output	B
training	O
set	O
for	O
each	O
neuron	O
i	O
,	O
{	O
(	O
v	O
(	O
t	O
)	O
,	O
vi	O
(	O
t	O
+	O
1	O
)	O
)	O
,	O
t	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
t	O
−	O
1	O
}	O
.	O
each	O
neuron	O
has	O
an	O
associated	O
weight	B
vector	O
wi	O
≡	O
wij	O
,	O
j	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
v	O
,	O
which	O
forms	O
a	O
logistic	B
regressor	O
or	O
,	O
in	O
the	O
limit	O
β	O
=	O
∞	O
,	O
a	O
perceptron	B
[	O
132	O
]	O
.	O
for	O
perfect	O
recall	O
of	O
the	O
patterns	O
,	O
we	O
therefore	O
need	O
only	O
that	O
the	O
patterns	O
on	O
the	O
sequence	O
be	O
linearly	B
separable	I
.	O
this	O
will	O
be	O
the	O
case	O
if	O
the	O
patterns	O
are	O
linearly	B
independent	I
,	O
regardless	O
of	O
the	O
outputs	O
vi	O
(	O
t	O
+	O
1	O
)	O
,	O
t	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
t	O
−	O
1.	O
relation	O
to	O
the	O
perceptron	B
rule	O
in	O
the	O
limit	O
that	O
the	O
activation	O
is	O
large	O
,	O
|ai|	O
(	O
cid:29	O
)	O
1	O
(	O
cid:26	O
)	O
1	O
vi	O
(	O
t	O
+	O
1	O
)	O
ai	O
<	O
0	O
γi	O
≈	O
0	O
vi	O
(	O
t	O
+	O
1	O
)	O
ai	O
≥	O
0	O
(	O
26.3.20	O
)	O
provided	O
the	O
activation	O
and	O
desired	O
next	O
output	O
are	O
the	O
same	O
sign	O
,	O
no	O
update	O
is	O
made	O
for	O
neuron	O
i.	O
in	O
this	O
limit	O
,	O
equation	B
(	O
26.3.19	O
)	O
is	O
called	O
the	O
perceptron	B
rule	O
[	O
132	O
,	O
80	O
]	O
.	O
for	O
an	O
activation	O
a	O
that	O
is	O
close	O
to	O
the	O
3naturally	O
,	O
one	O
can	O
use	O
more	O
sophisticated	O
methods	O
such	O
as	O
the	O
newton	O
method	O
,	O
or	O
conjugate	O
gradients	O
.	O
in	O
theoretical	O
neurobiology	O
the	O
emphasis	O
is	O
small	O
gradient	B
style	O
updates	O
since	O
these	O
are	O
deemed	O
to	O
be	O
biologically	O
more	O
plausible	O
.	O
draft	O
march	O
9	O
,	O
2010	O
479	O
training	B
sequencetimeneuron	O
number1020102030405060708090100max	O
likelihood1020102030405060708090100hebb1020102030405060708090100pseudo	O
inverse1020102030405060708090100	O
learning	B
sequences	O
figure	O
26.4	O
:	O
the	O
fraction	O
of	O
neurons	O
correct	O
for	O
the	O
ﬁnal	O
state	O
of	O
the	O
network	O
t	O
=	O
50	O
for	O
a	O
100	O
neuron	O
hopﬁeld	O
network	O
trained	O
to	O
store	O
a	O
length	O
50	O
sequence	O
patterns	O
.	O
after	O
initialization	O
in	O
the	O
correct	O
initial	O
state	O
at	O
t	O
=	O
1	O
,	O
the	O
hopﬁeld	O
network	O
is	O
up-	O
dated	O
deterministically	O
,	O
with	O
a	O
randomly	O
chosen	O
percentage	O
of	O
the	O
neurons	O
ﬂipped	O
post	O
updating	O
.	O
the	O
correlated	O
sequence	O
of	O
length	O
t	O
=	O
50	O
was	O
produced	O
by	O
ﬂipping	O
with	O
probability	O
0.5	O
,	O
20	O
%	O
of	O
the	O
previous	O
state	O
of	O
the	O
network	O
.	O
a	O
fraction	O
correct	O
value	B
of	O
1	O
indicates	O
perfect	O
recall	O
of	O
the	O
ﬁnal	O
state	O
,	O
and	O
a	O
value	B
of	O
0.5	O
indicates	O
a	O
performance	B
no	O
better	O
than	O
random	O
guess-	O
ing	O
of	O
the	O
ﬁnal	O
state	O
.	O
for	O
maximum	B
likelihood	I
50	O
epochs	O
of	O
training	B
were	O
used	O
with	O
η	O
=	O
0.02.	O
during	O
recall	O
,	O
deterministic	B
updates	O
β	O
=	O
∞	O
were	O
used	O
.	O
the	O
results	O
presented	O
are	O
averages	O
over	O
5000	O
simulations	O
,	O
resulting	O
in	O
standard	O
errors	O
of	O
the	O
order	O
of	O
the	O
symbol	O
sizes	O
.	O
(	O
a	O
)	O
decision	B
boundary	I
,	O
a	O
small	O
change	O
can	O
lead	O
to	O
a	O
diﬀerent	O
sign	O
of	O
the	O
neural	O
ﬁring	O
.	O
to	O
guard	O
against	O
this	O
it	O
is	O
common	O
to	O
include	O
a	O
stability	O
criterion	O
(	O
cid:26	O
)	O
1	O
vi	O
(	O
t	O
+	O
1	O
)	O
ai	O
<	O
m	O
0	O
vi	O
(	O
t	O
+	O
1	O
)	O
ai	O
≥	O
m	O
γi	O
=	O
(	O
26.3.21	O
)	O
where	O
m	O
is	O
an	O
empirically	O
chosen	O
positive	O
threshold	O
.	O
example	O
108	O
(	O
storing	O
a	O
correlated	O
sequence	O
)	O
.	O
in	O
ﬁg	O
(	O
26.3	O
)	O
we	O
consider	O
storage	O
of	O
a	O
highly-correlated	O
temporal	O
sequence	O
of	O
length	O
t	O
=	O
20	O
of	O
100	O
neurons	O
using	O
the	O
three	O
learning	B
rules	O
:	O
hebb	O
,	O
maximum	B
likelihood	I
and	O
pseudo	B
inverse	I
.	O
the	O
sequence	O
is	O
chosen	O
to	O
be	O
highly	O
correlated	O
,	O
which	O
constitutes	O
a	O
diﬃcult	O
learning	B
task	O
.	O
the	O
thresholds	O
θi	O
are	O
set	O
to	O
zero	O
throughout	O
to	O
facilitate	O
comparison	O
.	O
the	O
initial	O
state	O
of	O
the	O
training	B
sequence	O
,	O
corrupted	O
by	O
30	O
%	O
noise	O
is	O
presented	O
to	O
the	O
trained	O
networks	O
,	O
and	O
we	O
desire	O
that	O
the	O
training	B
sequence	O
will	O
be	O
generated	O
from	O
this	O
initial	O
noisy	O
state	O
.	O
whilst	O
the	O
hebb	O
rule	O
is	O
operating	O
in	O
a	O
feasible	O
limit	O
for	O
uncorrelated	O
patterns	O
,	O
the	O
strong	B
correlations	O
in	O
this	O
training	B
sequence	O
entails	O
poor	O
results	O
.	O
the	O
pi	O
rule	O
is	O
capable	O
of	O
storing	O
a	O
sequence	O
of	O
length	O
100	O
yet	O
is	O
not	O
robust	O
to	O
perturbations	O
from	O
the	O
correct	O
initial	O
state	O
.	O
the	O
maximum	B
likelihood	I
rule	O
performs	O
well	O
after	O
a	O
small	O
amount	O
of	O
training	B
.	O
stochastic	O
interpretation	O
by	O
straightforward	O
manipulations	O
,	O
the	O
weight	B
update	O
rule	O
in	O
equation	B
(	O
26.3.19	O
)	O
can	O
be	O
written	O
as	O
(	O
cid:16	O
)	O
t−1	O
(	O
cid:88	O
)	O
t=1	O
1	O
2	O
dl	O
dwij	O
=	O
vi	O
(	O
t	O
+	O
1	O
)	O
−	O
(	O
cid:104	O
)	O
vi	O
(	O
t	O
+	O
1	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
vi	O
(	O
t+1	O
)	O
|ai	O
(	O
t	O
)	O
)	O
a	O
stochastic	O
,	O
online	B
learning	I
rule	O
is	O
therefore	O
∆wij	O
(	O
t	O
)	O
=	O
η	O
(	O
vi	O
(	O
t	O
+	O
1	O
)	O
−	O
˜vi	O
(	O
t	O
+	O
1	O
)	O
)	O
vj	O
(	O
t	O
)	O
(	O
cid:17	O
)	O
vj	O
(	O
t	O
)	O
(	O
26.3.22	O
)	O
(	O
26.3.23	O
)	O
where	O
˜vi	O
(	O
t	O
+	O
1	O
)	O
is	O
1	O
with	O
probability	B
σβ	O
(	O
ai	O
(	O
t	O
)	O
)	O
,	O
and	O
−1	O
otherwise	O
.	O
provided	O
that	O
the	O
learning	B
rate	I
η	O
is	O
small	O
,	O
this	O
stochastic	O
updating	O
will	O
approximate	B
the	O
learning	B
rule	O
(	O
26.3.18,26.3.19	O
)	O
.	O
example	O
109	O
(	O
recalling	O
sequences	B
under	O
perpetual	O
noise	O
)	O
.	O
we	O
compare	O
the	O
performance	B
of	O
the	O
maximum	B
likelihood	I
learning	O
rule	O
(	O
with	O
zero	O
thresholds	O
θ	O
)	O
with	O
the	O
standard	O
hebb	O
,	O
pseudo	B
inverse	I
,	O
and	O
perceptron	B
rule	O
for	O
learning	B
a	O
single	O
temporal	O
sequence	O
.	O
the	O
network	O
is	O
initialized	O
to	O
a	O
noise	O
corrupted	O
480	O
draft	O
march	O
9	O
,	O
2010	O
00.050.10.150.20.250.30.350.40.450.50.50.550.60.650.70.750.80.850.90.951flip	O
probabilityfraction	O
correctsequence	O
length=50max	O
likelihoodnoise	O
trained	O
max	O
likelihoodperceptron	O
(	O
m=10	O
)	O
perceptron	B
(	O
m=0	O
)	O
hebbpseudo	O
inverse	O
learning	O
sequences	B
(	O
a	O
)	O
(	O
b	O
)	O
(	O
b	O
)	O
:	O
the	O
figure	O
26.5	O
:	O
(	O
a	O
)	O
:	O
original	O
t	O
=	O
25	O
binary	O
video	O
sequence	O
on	O
a	O
set	O
of	O
81	O
×	O
111	O
=	O
8991	O
neurons	O
.	O
reconstructions	O
beginning	O
from	O
a	O
20	O
%	O
noise	O
perturbed	O
initial	O
state	O
.	O
every	O
odd	O
time	O
reconstruction	O
is	O
also	O
randomly	O
perturbed	O
.	O
despite	O
the	O
high	O
level	O
of	O
noise	O
the	O
basis	O
of	O
attraction	O
of	O
the	O
pattern	O
sequence	O
is	O
very	O
broad	O
and	O
the	O
patterns	O
immediately	O
fall	O
back	O
close	O
to	O
the	O
pattern	O
sequence	O
even	O
after	O
a	O
single	O
timestep	O
.	O
version	O
of	O
the	O
correct	O
initial	O
state	O
v	O
(	O
t	O
=	O
1	O
)	O
from	O
the	O
training	B
sequence	O
.	O
the	O
dynamics	O
is	O
then	O
run	O
(	O
at	O
β	O
=	O
∞	O
)	O
for	O
the	O
same	O
number	O
of	O
steps	O
as	O
the	O
length	O
of	O
the	O
training	B
sequence	O
,	O
and	O
the	O
fraction	O
of	O
bits	O
of	O
the	O
recalled	O
ﬁnal	O
state	O
which	O
are	O
the	O
same	O
as	O
the	O
training	B
sequence	O
ﬁnal	O
state	O
v	O
(	O
t	O
)	O
is	O
measured	O
,	O
ﬁg	O
(	O
26.4	O
)	O
.	O
at	O
each	O
stage	O
in	O
the	O
dynamics	O
(	O
except	O
the	O
last	O
)	O
,	O
the	O
state	O
of	O
the	O
network	O
was	O
corrupted	O
with	O
noise	O
by	O
ﬂipping	O
each	O
neuron	O
state	O
with	O
the	O
speciﬁed	O
ﬂip	O
probability	B
.	O
the	O
training	B
sequences	O
are	O
produced	O
by	O
starting	O
from	O
a	O
random	O
initial	O
state	O
,	O
v	O
(	O
1	O
)	O
,	O
and	O
then	O
choosing	O
at	O
random	O
20	O
%	O
percent	O
of	O
the	O
neurons	O
to	O
ﬂip	O
,	O
each	O
of	O
the	O
chosen	O
neurons	O
being	O
ﬂipped	O
with	O
probability	O
0.5	O
,	O
giving	O
a	O
random	O
training	O
sequence	O
with	O
a	O
high	O
degree	O
of	O
temporal	O
correlation	B
.	O
the	O
standard	O
hebb	O
rule	O
performs	O
relatively	O
poorly	O
,	O
particularly	O
for	O
small	O
ﬂip	O
rates	O
,	O
whilst	O
the	O
other	O
methods	O
perform	O
relatively	O
well	O
,	O
being	O
robust	O
at	O
small	O
ﬂip	O
rates	O
.	O
as	O
the	O
ﬂip	O
rate	O
increases	O
,	O
the	O
pseudo	B
inverse	I
rule	I
becomes	O
unstable	O
,	O
especially	O
for	O
the	O
longer	O
temporal	O
sequence	O
which	O
places	O
more	O
demands	O
on	O
the	O
network	O
.	O
the	O
perceptron	B
rule	O
can	O
perform	O
as	O
well	O
as	O
the	O
maximum	B
likelihood	I
rule	O
,	O
although	O
its	O
performance	B
is	O
critically	O
dependent	O
on	O
an	O
appropriate	O
choice	O
of	O
the	O
threshold	O
m.	O
the	O
results	O
for	O
m	O
=	O
0	O
perceptron	B
training	O
are	O
poor	O
for	O
small	O
ﬂip	O
rates	O
.	O
an	O
advantage	O
of	O
the	O
maximum	B
likelihood	I
rule	O
is	O
that	O
it	O
performs	O
well	O
without	O
the	O
need	O
for	O
ﬁne	O
tuning	O
of	O
parameters	O
.	O
in	O
all	O
cases	O
,	O
batch	B
training	O
was	O
used	O
.	O
an	O
example	O
for	O
a	O
larger	O
network	O
is	O
given	O
in	O
ﬁg	O
(	O
26.5	O
)	O
which	O
consists	O
of	O
highly	O
correlated	O
sequences	B
.	O
for	O
such	O
short	O
sequences	B
the	O
basin	O
of	O
attraction	O
is	O
very	O
large	O
and	O
the	O
video	O
sequence	O
can	O
be	O
stored	O
robustly	O
.	O
26.3.2	O
multiple	O
sequences	O
the	O
previous	O
section	O
detailed	O
how	O
to	O
train	O
a	O
hopﬁeld	O
network	O
for	O
a	O
single	O
temporal	O
sequence	O
.	O
we	O
now	O
address	O
the	O
learning	B
a	O
set	O
of	O
sequences	B
{	O
v	O
n	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
.	O
if	O
we	O
assume	O
that	O
the	O
sequences	B
are	O
indepen-	O
dent	O
,	O
the	O
log	O
likelihood	B
of	O
a	O
set	O
of	O
sequences	B
is	O
the	O
sum	O
of	O
the	O
individual	O
sequences	B
.	O
the	O
gradient	B
is	O
given	O
by	O
γn	O
i	O
(	O
t	O
)	O
vn	O
i	O
(	O
t	O
+	O
1	O
)	O
vn	O
j	O
(	O
t	O
)	O
,	O
γn	O
i	O
(	O
t	O
)	O
vn	O
i	O
(	O
t	O
+	O
1	O
)	O
(	O
26.3.24	O
)	O
n	O
(	O
cid:88	O
)	O
t−1	O
(	O
cid:88	O
)	O
n=1	O
t=1	O
dl	O
dwij	O
=	O
β	O
where	O
t−1	O
(	O
cid:88	O
)	O
t=1	O
=	O
β	O
dl	O
dθi	O
n	O
(	O
cid:88	O
)	O
i	O
(	O
t	O
)	O
=	O
θi	O
+	O
(	O
cid:88	O
)	O
n=1	O
an	O
wijvn	O
j	O
(	O
t	O
)	O
j	O
γn	O
i	O
(	O
t	O
)	O
≡	O
1	O
−	O
σβ	O
(	O
vn	O
i	O
(	O
t	O
+	O
1	O
)	O
an	O
i	O
(	O
t	O
)	O
)	O
,	O
(	O
26.3.25	O
)	O
the	O
log	O
likelihood	B
remains	O
convex	O
since	O
it	O
is	O
the	O
sum	O
of	O
convex	O
functions	O
,	O
so	O
that	O
the	O
standard	O
gradient	O
based	O
learning	B
algorithms	O
can	O
be	O
used	O
here	O
as	O
well	O
.	O
draft	O
march	O
9	O
,	O
2010	O
481	O
tractable	O
continuous	B
latent	O
variable	B
models	O
26.3.3	O
boolean	O
networks	O
the	O
hopﬁeld	O
network	O
is	O
one	O
particular	O
parameterisation	B
of	O
the	O
table	O
p	O
(	O
vi	O
(	O
t	O
+	O
1	O
)	O
=	O
1|v	O
(	O
t	O
)	O
)	O
.	O
however	O
,	O
less	O
constrained	O
parameters	O
may	O
be	O
considered	O
–	O
indeed	O
one	O
could	O
consider	O
the	O
fully	O
unconstrained	O
case	O
in	O
which	O
each	O
neuron	O
i	O
would	O
have	O
an	O
associated	O
2v	O
parental	O
states	O
.	O
this	O
exponentially	O
large	O
number	O
of	O
states	O
is	O
impractical	O
and	O
an	O
interesting	O
restriction	O
is	O
to	O
consider	O
that	O
each	O
neuron	O
has	O
only	O
k	O
parents	B
,	O
so	O
that	O
each	O
table	O
contains	O
2k	O
entries	O
.	O
learning	B
the	O
table	O
parameters	O
by	O
maximum	B
likelihood	I
is	O
straightforward	O
since	O
the	O
log	O
likelihood	B
is	O
a	O
convex	B
function	I
of	O
the	O
table	O
entries	O
.	O
hence	O
,	O
for	O
given	O
any	O
sequence	O
(	O
or	O
set	O
of	O
sequences	B
)	O
one	O
may	O
readily	O
ﬁnd	O
parameters	O
that	O
maximise	O
the	O
sequence	O
reconstruction	O
probability	B
.	O
the	O
maximum	B
likelihood	I
method	O
also	O
produces	O
large	O
basins	O
of	O
attraction	O
for	O
the	O
associated	O
stochastic	O
dynamical	B
system	I
.	O
such	O
models	O
are	O
of	O
potential	B
interest	O
in	O
artiﬁcial	O
life	O
and	O
random	O
boolean	O
networks	O
in	O
which	O
emergent	O
macroscopic	O
behaviour	O
appears	O
from	O
local	B
update	O
rules	O
[	O
158	O
]	O
.	O
26.3.4	O
sequence	O
disambiguation	O
a	O
limitation	O
of	O
ﬁrst	B
order	I
networks	O
deﬁned	O
on	O
visible	B
variables	O
alone	O
(	O
such	O
as	O
the	O
hopﬁeld	O
network	O
)	O
is	O
that	O
the	O
observation	O
transition	O
p	O
(	O
vt+1|vt	O
=	O
v	O
)	O
is	O
the	O
same	O
every	O
time	O
the	O
joint	B
state	O
v	O
is	O
encountered	O
.	O
this	O
means	O
that	O
if	O
the	O
sequence	O
contains	O
a	O
subsequence	O
such	O
as	O
a	O
,	O
b	O
,	O
a	O
,	O
c	O
this	O
can	O
not	O
be	O
recalled	O
with	O
high	O
probability	B
since	O
a	O
transitions	O
to	O
diﬀerent	O
states	O
,	O
depending	O
on	O
time	O
.	O
whilst	O
one	O
could	O
attempt	O
to	O
resolve	O
this	O
sequence	O
disambiguation	O
problem	B
using	O
a	O
higher	O
order	O
markov	O
model	B
to	O
account	O
for	O
a	O
longer	O
temporal	O
context	O
,	O
we	O
would	O
lose	O
biological	O
plausibility	O
.	O
using	O
latent	B
variables	O
is	O
an	O
alternative	O
way	O
to	O
sequence	O
disambiguation	O
.	O
in	O
the	O
hopﬁeld	O
model	B
the	O
recall	O
capacity	B
can	O
be	O
increased	O
using	O
latent	B
variables	O
by	O
make	O
a	O
sequencing	O
in	O
the	O
joint	B
latent-visible	O
space	O
that	O
is	O
linearly	B
independent	I
,	O
even	O
if	O
the	O
visible	B
variable	O
sequence	O
alone	O
is	O
not	O
.	O
in	O
section	O
(	O
26.4	O
)	O
we	O
discuss	O
a	O
general	O
method	O
that	O
extends	O
dynamic	B
bayes	O
networks	O
deﬁned	O
on	O
visible	B
variables	O
alone	O
,	O
such	O
as	O
the	O
hopﬁeld	O
network	O
,	O
to	O
include	O
continuous	B
non-linearly	O
updating	O
latent	B
variables	O
,	O
without	O
requiring	O
additional	O
approximations	O
.	O
26.4	O
tractable	O
continuous	B
latent	O
variable	B
models	O
a	O
dynamic	B
bayes	O
network	O
with	O
latent	B
variables	O
takes	O
the	O
form	O
t−1	O
(	O
cid:89	O
)	O
t=1	O
p	O
(	O
v	O
(	O
1	O
:	O
t	O
)	O
,	O
h	O
(	O
1	O
:	O
t	O
)	O
)	O
=	O
p	O
(	O
v	O
(	O
1	O
)	O
)	O
p	O
(	O
h	O
(	O
1	O
)	O
|v	O
(	O
1	O
)	O
)	O
p	O
(	O
v	O
(	O
t+1	O
)	O
|v	O
(	O
t	O
)	O
,	O
h	O
(	O
t	O
)	O
)	O
p	O
(	O
h	O
(	O
t+1	O
)	O
|v	O
(	O
t	O
)	O
,	O
v	O
(	O
t+1	O
)	O
,	O
h	O
(	O
t	O
)	O
)	O
(	O
26.4.1	O
)	O
as	O
we	O
saw	O
in	O
chapter	O
(	O
23	O
)	O
,	O
provided	O
all	O
hidden	B
variables	I
are	O
discrete	B
,	O
inference	B
in	O
these	O
models	O
is	O
straight-	O
forward	O
.	O
however	O
,	O
in	O
many	O
physical	O
systems	O
it	O
is	O
more	O
natural	B
to	O
assume	O
continuous	B
h	O
(	O
t	O
)	O
.	O
in	O
chapter	O
(	O
24	O
)	O
we	O
saw	O
that	O
one	O
such	O
tractable	O
continuous	B
h	O
(	O
t	O
)	O
model	B
is	O
given	O
by	O
linear	B
gaussian	O
transitions	O
and	O
emissions	O
-	O
the	O
lds	O
.	O
whilst	O
this	O
is	O
useful	O
,	O
we	O
can	O
not	O
represent	O
non-linear	B
changes	O
in	O
the	O
latent	B
process	O
using	O
an	O
lds	O
alone	O
.	O
the	O
switching	B
lds	O
of	O
chapter	O
(	O
25	O
)	O
is	O
able	O
to	O
model	B
non-linear	O
continuous	B
dynamics	O
(	O
via	O
switching	B
)	O
although	O
we	O
saw	O
that	O
this	O
leads	O
to	O
computational	O
diﬃculties	O
.	O
for	O
computational	O
reasons	O
we	O
therefore	O
seem	O
limited	O
to	O
either	O
purely	O
discrete	B
h	O
(	O
with	O
no	O
limitation	O
on	O
the	O
discrete	B
transitions	O
)	O
or	O
purely	O
continuous	B
h	O
(	O
but	O
be	O
forced	O
to	O
use	O
simple	O
linear	O
dynamics	O
)	O
.	O
is	O
there	O
a	O
way	O
to	O
have	O
a	O
continuous	B
state	O
with	O
non-linear	O
dynamics	O
for	O
which	O
posterior	B
inference	O
remains	O
tractable	O
?	O
the	O
answer	O
is	O
yes	O
,	O
provided	O
that	O
we	O
assume	O
the	O
hidden	B
transitions	O
are	O
deterministic	B
[	O
14	O
]	O
.	O
when	O
conditioned	O
on	O
the	O
visible	B
variables	O
,	O
this	O
renders	O
the	O
hidden	B
unit	O
distribution	B
trivial	O
.	O
this	O
allows	O
the	O
consideration	O
of	O
rich	O
non-linear	B
dynamics	O
in	O
the	O
hidden	B
space	O
if	O
required	O
.	O
26.4.1	O
deterministic	O
latent	O
variables	O
consider	O
a	O
belief	B
network	I
deﬁned	O
on	O
a	O
sequence	O
of	O
visible	B
variables	O
v1	O
:	O
t	O
.	O
to	O
enrich	O
the	O
model	B
we	O
include	O
additional	O
continuous	B
latent	O
variables	O
h1	O
:	O
t	O
that	O
will	O
follow	O
a	O
non-linear	B
markov	O
transition	O
.	O
to	O
retain	O
tractability	O
of	O
inference	B
,	O
we	O
constrain	O
the	O
latent	B
dynamics	O
to	O
be	O
deterministic	B
,	O
described	O
by	O
p	O
(	O
h	O
(	O
t	O
+	O
1	O
)	O
|v	O
(	O
t	O
+	O
1	O
)	O
,	O
v	O
(	O
t	O
)	O
,	O
h	O
(	O
t	O
)	O
)	O
=	O
δ	O
(	O
h	O
(	O
t	O
+	O
1	O
)	O
−	O
f	O
(	O
v	O
(	O
t	O
+	O
1	O
)	O
,	O
v	O
(	O
t	O
)	O
,	O
h	O
(	O
t	O
)	O
,	O
θh	O
)	O
)	O
(	O
26.4.2	O
)	O
here	O
δ	O
(	O
x	O
)	O
represents	O
the	O
dirac	O
delta	B
function	I
for	O
continuous	B
hidden	O
variables	O
.	O
the	O
(	O
possibly	O
non-linear	B
)	O
function	B
f	O
parameterises	O
the	O
cpt	O
.	O
whilst	O
the	O
restriction	O
to	O
deterministic	B
cpts	O
appears	O
severe	O
,	O
the	O
model	B
482	O
draft	O
march	O
9	O
,	O
2010	O
tractable	O
continuous	B
latent	O
variable	B
models	O
h	O
(	O
1	O
)	O
h	O
(	O
2	O
)	O
v	O
(	O
1	O
)	O
v	O
(	O
2	O
)	O
(	O
a	O
)	O
h	O
(	O
t	O
)	O
v	O
(	O
t	O
)	O
h	O
(	O
1	O
)	O
h	O
(	O
2	O
)	O
h	O
(	O
t	O
)	O
v	O
(	O
1	O
)	O
(	O
b	O
)	O
v	O
(	O
t	O
)	O
v	O
(	O
2	O
)	O
(	O
c	O
)	O
figure	O
26.6	O
:	O
(	O
a	O
)	O
:	O
a	O
ﬁrst	B
order	I
dynamic	O
bayesian	O
network	O
with	O
deterministic	B
hidden	O
cpts	O
(	O
represented	O
(	O
b	O
)	O
:	O
by	O
diamonds	O
)	O
that	O
is	O
,	O
the	O
hidden	B
node	O
is	O
certainly	O
in	O
a	O
single	O
state	O
,	O
determined	O
by	O
its	O
parents	B
.	O
conditioning	B
on	O
the	O
visible	B
variables	O
forms	O
a	O
directed	B
chain	O
in	O
the	O
hidden	B
space	O
which	O
is	O
deterministic	B
.	O
hidden	B
unit	O
inference	B
can	O
be	O
achieved	O
by	O
forward	O
propagation	O
alone	O
.	O
(	O
c	O
)	O
:	O
integrating	O
out	O
hidden	B
variables	I
gives	O
a	O
cascade	B
style	O
directed	B
visible	O
graph	B
which	O
so	O
that	O
each	O
v	O
(	O
t	O
)	O
depends	O
on	O
all	O
v	O
(	O
1	O
:	O
t	O
−	O
1	O
)	O
.	O
retains	O
some	O
attractive	O
features	O
:	O
the	O
marginal	B
p	O
(	O
v	O
(	O
1	O
:	O
t	O
)	O
)	O
is	O
non-markovian	O
,	O
coupling	O
all	O
the	O
variables	O
in	O
the	O
sequence	O
,	O
see	O
ﬁg	O
(	O
26.6c	O
)	O
,	O
whilst	O
hidden	B
unit	O
inference	B
p	O
(	O
h	O
(	O
1	O
:	O
t	O
)	O
|v	O
(	O
1	O
:	O
t	O
)	O
)	O
is	O
deterministic	B
,	O
as	O
illustrated	O
in	O
ﬁg	O
(	O
26.6b	O
)	O
.	O
the	O
adjustable	O
parameters	O
of	O
the	O
hidden	B
and	O
visible	B
cpts	O
are	O
represented	O
by	O
θh	O
and	O
θv	O
respectively	O
.	O
for	O
learning	B
,	O
the	O
log	O
likelihood	B
of	O
a	O
single	O
training	B
sequence	O
v	O
is	O
to	O
maximise	O
the	O
log	O
likelihood	B
using	O
gradient	B
techniques	O
we	O
need	O
to	O
the	O
derivatives	O
with	O
respect	O
to	O
the	O
model	B
parameters	O
.	O
these	O
can	O
be	O
calculated	O
as	O
follows	O
:	O
f	O
(	O
t	O
)	O
≡	O
f	O
(	O
v	O
(	O
t	O
)	O
,	O
v	O
(	O
t	O
−	O
1	O
)	O
,	O
h	O
(	O
t	O
−	O
1	O
)	O
,	O
θh	O
)	O
hence	O
the	O
derivatives	O
can	O
be	O
calculated	O
by	O
deterministic	B
forward	O
propagation	B
of	O
errors	O
alone	O
.	O
the	O
case	O
of	O
training	B
multiple	O
independently	O
generated	O
sequences	B
v	O
n	O
,	O
n	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
is	O
a	O
straightforward	O
extension	O
.	O
26.4.2	O
an	O
augmented	B
hopﬁeld	O
network	O
to	O
make	O
the	O
deterministic	B
latent	I
variable	I
model	I
more	O
explicit	O
,	O
we	O
consider	O
the	O
case	O
of	O
continuous	B
hidden	O
units	O
and	O
discrete	B
,	O
binary	O
visible	O
units	O
,	O
vi	O
(	O
t	O
)	O
∈	O
{	O
−1	O
,	O
1	O
}	O
.	O
in	O
particular	O
,	O
we	O
restrict	O
attention	O
to	O
the	O
hopﬁeld	O
model	B
augmented	O
with	O
latent	O
variables	O
that	O
have	O
a	O
simple	O
linear	O
dynamics	O
(	O
see	O
exercise	O
(	O
245	O
)	O
for	O
a	O
non-	O
linear	B
extension	O
)	O
:	O
h	O
(	O
t	O
+	O
1	O
)	O
=	O
2σ	O
(	O
ah	O
(	O
t	O
)	O
+	O
bv	O
(	O
t	O
)	O
)	O
−	O
1	O
deterministic	O
latent	B
transition	O
σ	O
(	O
vi	O
(	O
t	O
+	O
1	O
)	O
φi	O
(	O
t	O
)	O
)	O
,	O
φ	O
(	O
t	O
)	O
≡	O
ch	O
(	O
t	O
)	O
+	O
dv	O
(	O
t	O
)	O
v	O
(	O
cid:89	O
)	O
i=1	O
p	O
(	O
v	O
(	O
t	O
+	O
1	O
)	O
|v	O
(	O
t	O
)	O
,	O
h	O
(	O
t	O
)	O
)	O
=	O
draft	O
march	O
9	O
,	O
2010	O
t−1	O
(	O
cid:88	O
)	O
t=1	O
t−1	O
(	O
cid:88	O
)	O
t=1	O
l	O
(	O
θv	O
,	O
θh|v	O
)	O
=	O
log	O
p	O
(	O
v	O
(	O
1	O
)	O
|θv	O
)	O
+	O
log	O
p	O
(	O
v	O
(	O
t	O
+	O
1	O
)	O
|v	O
(	O
t	O
)	O
,	O
h	O
(	O
t	O
)	O
,	O
θv	O
)	O
where	O
the	O
hidden	B
unit	O
values	O
are	O
calculated	O
recursively	O
using	O
h	O
(	O
t	O
+	O
1	O
)	O
=	O
f	O
(	O
v	O
(	O
t	O
+	O
1	O
)	O
,	O
v	O
(	O
t	O
)	O
,	O
h	O
(	O
t	O
)	O
,	O
θh	O
)	O
log	O
p	O
(	O
v	O
(	O
1	O
)	O
|θv	O
)	O
+	O
∂	O
∂θv	O
log	O
p	O
(	O
v	O
(	O
t	O
+	O
1	O
)	O
|v	O
(	O
t	O
)	O
,	O
h	O
(	O
t	O
)	O
,	O
θv	O
)	O
=	O
∂	O
∂θv	O
t−1	O
(	O
cid:88	O
)	O
=	O
dl	O
dθv	O
dl	O
dθh	O
dh	O
(	O
t	O
)	O
dθh	O
where	O
∂	O
t=1	O
∂h	O
(	O
t	O
)	O
=	O
∂f	O
(	O
t	O
)	O
∂θh	O
log	O
p	O
(	O
v	O
(	O
t	O
+	O
1	O
)	O
|v	O
(	O
t	O
)	O
,	O
h	O
(	O
t	O
)	O
,	O
θv	O
)	O
dh	O
(	O
t	O
)	O
+	O
∂f	O
(	O
t	O
)	O
∂h	O
(	O
t	O
−	O
1	O
)	O
dh	O
(	O
t	O
−	O
1	O
)	O
dθh	O
dθh	O
(	O
26.4.3	O
)	O
(	O
26.4.4	O
)	O
(	O
26.4.5	O
)	O
(	O
26.4.6	O
)	O
(	O
26.4.7	O
)	O
(	O
26.4.8	O
)	O
(	O
26.4.9	O
)	O
(	O
26.4.10	O
)	O
483	O
this	O
model	B
generalises	O
a	O
recurrent	O
stochastic	O
heteroassociative	B
hopﬁeld	O
network	O
[	O
132	O
]	O
to	O
include	O
determin-	O
istic	O
hidden	B
units	O
dependent	O
on	O
past	O
network	O
states	O
.	O
the	O
parameters	O
of	O
the	O
model	B
are	O
a	O
,	O
b	O
,	O
c	O
,	O
d.	O
for	O
gradient	B
based	O
training	B
we	O
require	O
the	O
derivatives	O
with	O
respect	O
to	O
each	O
of	O
these	O
parameters	O
.	O
the	O
derivative	O
of	O
the	O
log	O
likelihood	B
for	O
a	O
generic	O
parameter	B
θ	O
is	O
this	O
gives	O
(	O
where	O
all	O
indices	O
are	O
summed	O
over	O
the	O
dimensions	O
of	O
the	O
quantities	O
they	O
relate	O
to	O
)	O
:	O
neural	O
models	O
(	O
26.4.11	O
)	O
(	O
26.4.12	O
)	O
(	O
26.4.13	O
)	O
(	O
26.4.14	O
)	O
(	O
26.4.15	O
)	O
(	O
26.4.16	O
)	O
(	O
26.4.17	O
)	O
(	O
26.4.18	O
)	O
	O
vi	O
(	O
t	O
+	O
1	O
)	O
φi	O
(	O
t	O
)	O
d	O
dθ	O
where	O
νi	O
(	O
t	O
)	O
≡	O
i	O
φi	O
(	O
t	O
)	O
νi	O
(	O
t	O
)	O
d	O
dθ	O
l	O
=	O
(	O
cid:88	O
)	O
1	O
−	O
σ	O
(	O
vi	O
(	O
t	O
+	O
1	O
)	O
)	O
(	O
cid:88	O
)	O
φi	O
(	O
t	O
)	O
=	O
(	O
cid:88	O
)	O
φi	O
(	O
t	O
)	O
=	O
(	O
cid:88	O
)	O
daαβ	O
cij	O
cij	O
d	O
d	O
j	O
d	O
d	O
j	O
j	O
hj	O
(	O
t	O
)	O
hj	O
(	O
t	O
)	O
dbαβ	O
daαβ	O
dbαβ	O
d	O
dcαβ	O
d	O
ddαβ	O
d	O
daαβ	O
d	O
φi	O
(	O
t	O
)	O
=	O
δiαhβ	O
(	O
t	O
)	O
φi	O
(	O
t	O
)	O
=	O
δiαvβ	O
(	O
t	O
)	O
hi	O
(	O
t	O
+	O
1	O
)	O
=	O
2σ	O
(	O
cid:48	O
)	O
i	O
(	O
t	O
+	O
1	O
)	O
(	O
cid:88	O
)	O
i	O
(	O
t	O
+	O
1	O
)	O
(	O
cid:88	O
)	O
(	O
cid:48	O
)	O
j	O
hi	O
(	O
t	O
+	O
1	O
)	O
=	O
2σ	O
dbαβ	O
(	O
cid:48	O
)	O
i	O
(	O
t	O
)	O
≡	O
σ	O
(	O
hi	O
(	O
t	O
)	O
)	O
(	O
1	O
−	O
σ	O
(	O
hi	O
(	O
t	O
)	O
)	O
)	O
j	O
aij	O
d	O
daαβ	O
aij	O
d	O
dbαβ	O
hj	O
(	O
t	O
)	O
+	O
δiαhβ	O
(	O
t	O
)	O
hj	O
(	O
t	O
)	O
+	O
δiαvβ	O
(	O
t	O
)	O
σ	O
(	O
26.4.19	O
)	O
if	O
we	O
assume	O
that	O
h	O
(	O
1	O
)	O
is	O
a	O
given	O
ﬁxed	O
value	B
(	O
say	O
0	O
)	O
,	O
we	O
can	O
compute	O
the	O
derivatives	O
recursively	O
by	O
forward	O
propagation	O
.	O
gradient	B
based	O
training	B
for	O
this	O
augmented	B
hopﬁeld	O
network	O
is	O
therefore	O
straightforward	O
to	O
implement	O
.	O
this	O
model	B
extends	O
the	O
power	O
of	O
the	O
original	O
hopﬁeld	O
model	B
,	O
being	O
capable	O
of	O
resolving	O
ambiguous	O
transitions	O
in	O
sequences	B
such	O
as	O
a	O
,	O
b	O
,	O
a	O
,	O
c	O
,	O
see	O
example	O
(	O
110	O
)	O
and	O
demohopfieldlatent.m	O
.	O
in	O
terms	O
of	O
a	O
dynamic	B
system	O
,	O
the	O
learned	O
network	O
is	O
an	O
attractor	O
with	O
the	O
training	B
sequence	O
as	O
a	O
stable	O
point	O
and	O
demonstrates	O
that	O
such	O
models	O
are	O
capable	O
of	O
learning	B
attractor	O
recurrent	O
networks	O
more	O
powerful	O
than	O
those	O
without	O
hidden	B
units	O
.	O
example	O
110	O
(	O
sequence	O
disambiguation	O
)	O
.	O
the	O
sequence	O
in	O
ﬁg	O
(	O
26.7a	O
)	O
contains	O
repeated	O
patterns	O
and	O
therefore	O
can	O
not	O
be	O
reliably	O
recalled	O
with	O
a	O
ﬁrst	B
order	I
model	O
containing	O
visible	B
variables	O
alone	O
.	O
to	O
deal	O
with	O
this	O
we	O
consider	O
a	O
hopﬁeld	O
network	O
with	O
3	O
visible	B
units	O
and	O
7	O
additional	O
hidden	B
units	O
with	O
deterministic	O
(	O
linear	B
)	O
latent	B
dynamics	O
.	O
the	O
model	B
was	O
trained	O
with	O
gradient	O
ascent	O
to	O
maximise	O
the	O
likelihood	B
of	O
the	O
binary	O
sequence	O
in	O
ﬁg	O
(	O
26.7a	O
)	O
.	O
as	O
shown	O
in	O
ﬁg	O
(	O
26.7b	O
)	O
,	O
the	O
learned	O
network	O
is	O
capable	O
of	O
recalling	O
the	O
sequence	O
correctly	O
,	O
even	O
when	O
initialised	O
in	O
an	O
incorrect	O
state	O
,	O
having	O
no	O
diﬃculty	O
with	O
the	O
fact	O
that	O
the	O
sequence	O
transitions	O
are	O
ambiguous	O
.	O
26.5	O
neural	O
models	O
the	O
tractable	O
deterministic	B
latent	I
variable	I
model	I
introduced	O
in	O
section	O
(	O
26.4	O
)	O
presents	O
an	O
opportunity	O
to	O
extend	O
models	O
such	O
as	O
the	O
hopﬁeld	O
network	O
to	O
include	O
more	O
biologically	O
realistic	O
processes	O
without	O
losing	O
computational	O
tractability	O
.	O
first	O
we	O
discuss	O
a	O
general	O
framework	O
for	O
learning	B
in	O
a	O
class	O
of	O
neural	O
models	O
[	O
15	O
,	O
223	O
]	O
,	O
this	O
being	O
a	O
special	O
case	O
of	O
the	O
deterministic	B
latent	I
variable	I
models	O
[	O
14	O
]	O
and	O
a	O
generalisation	B
of	O
the	O
spike-response	O
model	B
of	O
theoretical	O
neurobiology	O
[	O
108	O
]	O
.	O
484	O
draft	O
march	O
9	O
,	O
2010	O
neural	O
models	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
26.7	O
:	O
(	O
a	O
)	O
:	O
the	O
training	B
sequence	O
consists	O
of	O
a	O
random	O
set	O
of	O
vectors	O
(	O
v	O
=	O
3	O
)	O
(	O
b	O
)	O
:	O
the	O
reconstruction	O
using	O
h	O
=	O
7	O
hidden	B
units	O
.	O
the	O
over	O
t	O
=	O
10	O
time	O
steps	O
.	O
initial	O
state	O
v	O
(	O
t	O
=	O
1	O
)	O
for	O
the	O
recalled	O
sequence	O
was	O
set	O
to	O
the	O
correct	O
initial	O
training	B
value	O
albeit	O
with	O
one	O
of	O
the	O
values	O
ﬂipped	O
.	O
note	O
that	O
the	O
method	O
is	O
capable	O
of	O
sequence	O
disambiguation	O
in	O
the	O
sense	O
that	O
the	O
transitions	O
of	O
the	O
form	O
a	O
,	O
b	O
,	O
.	O
.	O
.	O
,	O
a	O
,	O
c	O
can	O
be	O
recalled	O
.	O
26.5.1	O
stochastically	O
spiking	O
neurons	O
we	O
assume	O
that	O
neuron	O
i	O
ﬁres	O
depending	O
on	O
the	O
membrane	O
potential	B
ai	O
(	O
t	O
)	O
through	O
p	O
(	O
vi	O
(	O
t	O
+	O
1	O
)	O
=	O
1|v	O
(	O
t	O
)	O
,	O
h	O
(	O
t	O
)	O
)	O
=	O
p	O
(	O
vi	O
(	O
t	O
+	O
1	O
)	O
=	O
1|ai	O
(	O
t	O
)	O
)	O
to	O
be	O
speciﬁc	O
,	O
we	O
take	O
throughout	O
p	O
(	O
vi	O
(	O
t	O
+	O
1	O
)	O
=	O
1|ai	O
(	O
t	O
)	O
)	O
=	O
σ	O
(	O
ai	O
(	O
t	O
)	O
)	O
here	O
we	O
to	O
deﬁne	O
the	O
quiescent	O
state	O
as	O
vi	O
(	O
t	O
+	O
1	O
)	O
=	O
0	O
,	O
so	O
that	O
p	O
(	O
vi	O
(	O
t	O
+	O
1	O
)	O
|ai	O
(	O
t	O
)	O
)	O
=	O
σ	O
(	O
(	O
2vi	O
(	O
t	O
+	O
1	O
)	O
−	O
1	O
)	O
ai	O
(	O
t	O
)	O
)	O
(	O
26.5.1	O
)	O
(	O
26.5.2	O
)	O
(	O
26.5.3	O
)	O
the	O
choice	O
of	O
the	O
sigmoid	B
function	I
σ	O
(	O
x	O
)	O
is	O
not	O
fundamental	O
and	O
is	O
chosen	O
merely	O
for	O
analytical	O
convenience	O
.	O
the	O
log-likelihood	O
of	O
a	O
sequence	O
of	O
visible	B
states	O
v	O
is	O
l	O
=	O
log	O
σ	O
(	O
(	O
2vi	O
(	O
t	O
+	O
1	O
)	O
−	O
1	O
)	O
ai	O
(	O
t	O
)	O
)	O
and	O
the	O
gradient	B
of	O
the	O
log-likelihood	O
is	O
then	O
dl	O
dwij	O
=	O
(	O
vi	O
(	O
t	O
+	O
1	O
)	O
−	O
σ	O
(	O
ai	O
(	O
t	O
)	O
)	O
)	O
dai	O
(	O
t	O
)	O
dwij	O
(	O
26.5.4	O
)	O
(	O
26.5.5	O
)	O
t−1	O
(	O
cid:88	O
)	O
v	O
(	O
cid:88	O
)	O
t=1	O
i=1	O
t−1	O
(	O
cid:88	O
)	O
t=1	O
v	O
(	O
cid:88	O
)	O
j=1	O
where	O
we	O
used	O
the	O
fact	O
that	O
vi	O
∈	O
{	O
0	O
,	O
1	O
}	O
.	O
here	O
wij	O
are	O
parameters	O
of	O
the	O
membrane	O
potential	B
(	O
see	O
below	O
)	O
.	O
we	O
take	O
equation	B
(	O
26.5.5	O
)	O
as	O
common	O
in	O
the	O
following	O
models	O
in	O
which	O
the	O
membrane	O
potential	B
ai	O
(	O
t	O
)	O
is	O
described	O
with	O
increasing	O
sophistication	O
.	O
26.5.2	O
hopﬁeld	O
membrane	O
potential	B
as	O
a	O
ﬁrst	O
step	O
,	O
we	O
show	O
how	O
the	O
hopﬁeld	O
network	O
training	O
,	O
as	O
described	O
in	O
section	O
(	O
26.3.1	O
)	O
,	O
can	O
be	O
recovered	O
as	O
a	O
special	O
case	O
of	O
the	O
above	O
framework	O
.	O
the	O
hopﬁeld	O
membrane	O
potential	B
is	O
ai	O
(	O
t	O
)	O
≡	O
wijvj	O
(	O
t	O
)	O
−	O
bi	O
(	O
26.5.6	O
)	O
where	O
wij	O
characterizes	O
the	O
eﬃcacy	O
of	O
information	O
transmission	O
from	O
neuron	O
j	O
to	O
neuron	O
i	O
,	O
and	O
bi	O
is	O
a	O
threshold	O
.	O
applying	O
the	O
maximum	B
likelihood	I
framework	O
to	O
this	O
model	B
to	O
learn	O
a	O
temporal	O
sequence	O
v	O
by	O
adjustment	O
of	O
the	O
parameters	O
wij	O
(	O
the	O
bi	O
are	O
ﬁxed	O
for	O
simplicity	O
)	O
,	O
we	O
obtain	O
the	O
(	O
batch	B
)	O
learning	B
rule	O
(	O
using	O
dai/dwij	O
=	O
vj	O
(	O
t	O
)	O
in	O
equation	B
(	O
26.5.4	O
)	O
)	O
t−1	O
(	O
cid:88	O
)	O
t=1	O
wnew	O
ij	O
=	O
wij	O
+	O
η	O
dl	O
dwij	O
,	O
dl	O
dwij	O
=	O
(	O
vi	O
(	O
t	O
+	O
1	O
)	O
−	O
σ	O
(	O
ai	O
(	O
t	O
)	O
)	O
)	O
vj	O
(	O
t	O
)	O
,	O
(	O
26.5.7	O
)	O
where	O
the	O
learning	B
rate	I
η	O
is	O
chosen	O
empirically	O
to	O
be	O
suﬃciently	O
small	O
to	O
ensure	O
convergence	O
.	O
equation	B
(	O
26.5.7	O
)	O
matches	O
equation	B
(	O
26.3.19	O
)	O
(	O
which	O
uses	O
the	O
±1	O
encoding	O
)	O
.	O
draft	O
march	O
9	O
,	O
2010	O
485	O
neural	O
models	O
figure	O
26.8	O
:	O
learning	B
with	O
depression	B
:	O
u	O
=	O
0.5	O
,	O
τ	O
=	O
5	O
,	O
δt	O
=	O
1	O
,	O
η	O
=	O
0.25.	O
despite	O
the	O
apparent	O
com-	O
plexity	O
of	O
the	O
dynamics	O
,	O
learning	B
appropriate	O
neu-	O
ral	O
connection	O
weights	O
is	O
straightforward	O
using	O
max-	O
imum	O
likelihood	B
.	O
the	O
reconstruction	O
using	O
the	O
stan-	O
dard	O
hebb	O
rule	O
by	O
contrast	O
is	O
poor	O
[	O
15	O
]	O
.	O
26.5.3	O
dynamic	B
synapses	I
in	O
more	O
realistic	O
synaptic	O
models	O
,	O
neurotransmitter	O
generation	O
depends	O
on	O
a	O
ﬁnite	O
rate	O
of	O
cell	O
subcom-	O
ponent	O
production	O
,	O
and	O
the	O
quantity	O
of	O
vesicles	O
released	O
is	O
aﬀected	O
by	O
the	O
history	O
of	O
ﬁring	O
[	O
1	O
]	O
.	O
loosely	O
speaking	O
,	O
when	O
a	O
neuron	O
ﬁres	O
it	O
releases	O
a	O
chemical	O
substance	O
from	O
a	O
local	B
reservoir	O
,	O
this	O
reservoir	O
being	O
reﬁlled	O
at	O
a	O
lower	O
rate	O
than	O
the	O
neuron	O
can	O
ﬁre	O
.	O
if	O
the	O
neuron	O
ﬁres	O
continually	O
,	O
its	O
ability	O
to	O
continue	O
ﬁring	O
weakens	O
since	O
the	O
reservoir	O
of	O
release	O
chemical	O
is	O
depleted	O
.	O
this	O
can	O
be	O
accounted	O
for	O
by	O
using	O
a	O
depression	B
mechanism	O
that	O
aﬀects	O
the	O
membrane	O
potential	B
for	O
depression	B
factors	O
xj	O
(	O
t	O
)	O
∈	O
[	O
0	O
,	O
1	O
]	O
.	O
a	O
simple	O
dynamics	O
for	O
these	O
depression	B
factors	O
is	O
[	O
280	O
]	O
ai	O
(	O
t	O
)	O
=	O
wijxj	O
(	O
t	O
)	O
vj	O
(	O
t	O
)	O
xj	O
(	O
t	O
+	O
1	O
)	O
=	O
xj	O
(	O
t	O
)	O
+	O
δt	O
(	O
cid:18	O
)	O
1	O
−	O
xj	O
(	O
t	O
)	O
τ	O
(	O
cid:19	O
)	O
−	O
u	O
xj	O
(	O
t	O
)	O
vj	O
(	O
t	O
)	O
(	O
26.5.8	O
)	O
(	O
26.5.9	O
)	O
where	O
δt	O
,	O
τ	O
,	O
and	O
u	O
represent	O
time	O
scales	O
,	O
recovery	O
times	O
and	O
spiking	O
eﬀect	O
parameters	O
respectively	O
.	O
note	O
that	O
these	O
depression	B
factor	O
dynamics	O
are	O
exactly	O
of	O
the	O
form	O
of	O
deterministic	B
hidden	O
variables	O
.	O
it	O
is	O
straightforward	O
to	O
include	O
these	O
dynamic	B
synapses	I
in	O
a	O
principled	O
way	O
using	O
the	O
maximum	B
likelihood	I
learning	O
framework	O
.	O
for	O
the	O
hopﬁeld	O
potential	B
,	O
the	O
learning	B
dynamics	O
is	O
simply	O
given	O
by	O
equations	O
(	O
26.5.5,26.5.9	O
)	O
,	O
with	O
dai	O
(	O
t	O
)	O
dwij	O
=	O
xj	O
(	O
t	O
)	O
vj	O
(	O
t	O
)	O
(	O
26.5.10	O
)	O
example	O
111	O
(	O
learning	B
with	O
depression	B
)	O
.	O
in	O
ﬁg	O
(	O
26.4	O
)	O
we	O
demonstrate	O
learning	B
a	O
random	O
temporal	O
sequence	O
of	O
20	O
time	O
steps	O
for	O
an	O
assembly	O
of	O
50	O
neurons	O
with	O
dynamic	O
depressive	O
synapses	O
.	O
after	O
learning	B
wij	O
the	O
trained	O
network	O
is	O
initialised	O
in	O
the	O
ﬁrst	O
state	O
of	O
the	O
training	B
sequence	O
.	O
the	O
remaining	O
states	O
of	O
the	O
sequence	O
were	O
then	O
correctly	O
recalled	O
by	O
iteration	B
of	O
the	O
learned	O
model	B
.	O
the	O
corresponding	O
generated	O
factors	O
xi	O
(	O
t	O
)	O
are	O
also	O
plotted	O
.	O
for	O
comparison	O
,	O
we	O
plot	O
the	O
results	O
of	O
using	O
the	O
dynamics	O
having	O
set	O
the	O
wij	O
using	O
the	O
temporal	O
hebb	O
rule	O
,	O
equation	B
(	O
26.3.1	O
)	O
.	O
the	O
poor	O
performance	B
of	O
the	O
correlation	B
based	O
hebb	O
rule	O
demonstrates	O
the	O
necessity	O
,	O
in	O
general	O
,	O
to	O
couple	O
a	O
dynamical	B
system	I
with	O
an	O
appropriate	O
learning	B
mechanism	O
.	O
26.5.4	O
leaky	B
integrate	I
and	I
ﬁre	I
models	O
leaky	B
integrate	I
and	I
ﬁre	I
models	O
move	O
a	O
step	O
further	O
towards	O
biological	O
realism	O
in	O
which	O
the	O
membrane	O
potential	B
increments	O
if	O
it	O
receives	O
an	O
excitatory	O
stimulus	O
(	O
wij	O
>	O
0	O
)	O
,	O
and	O
decrements	O
if	O
it	O
receives	O
an	O
inhibitory	O
stimulus	O
(	O
wij	O
<	O
0	O
)	O
.	O
after	O
ﬁring	O
,	O
the	O
membrane	O
potential	B
is	O
reset	O
to	O
a	O
low	O
value	O
below	O
the	O
ﬁring	O
threshold	O
,	O
and	O
thereafter	O
steadily	O
increases	O
to	O
a	O
resting	O
level	O
(	O
see	O
for	O
example	O
[	O
62	O
,	O
108	O
]	O
)	O
.	O
a	O
model	B
that	O
incorporates	O
such	O
eﬀects	O
is	O
αai	O
(	O
t	O
−	O
1	O
)	O
+	O
(	O
cid:88	O
)	O
j	O
	O
(	O
1	O
−	O
vi	O
(	O
t	O
−	O
1	O
)	O
)	O
+	O
vi	O
(	O
t	O
−	O
1	O
)	O
θf	O
ired	O
wijvj	O
(	O
t	O
)	O
+	O
θrest	O
(	O
1	O
−	O
α	O
)	O
ai	O
(	O
t	O
)	O
=	O
486	O
(	O
26.5.11	O
)	O
draft	O
march	O
9	O
,	O
2010	O
neuron	O
numberoriginalt10205101520253035404550reconstructiont10205101520253035404550x	O
valuest10205101520253035404550hebb	O
reconstructiont10205101520253035404550	O
exercises	O
since	O
vi	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
if	O
neuron	O
i	O
ﬁres	O
at	O
time	O
t	O
−	O
1	O
the	O
potential	B
is	O
reset	O
to	O
θf	O
ired	O
at	O
time	O
t.	O
similarly	O
,	O
with	O
no	O
synaptic	O
input	O
,	O
the	O
potential	B
equilibrates	O
to	O
θrest	O
with	O
time	O
constant	O
−1/	O
log	O
α	O
[	O
15	O
]	O
.	O
despite	O
the	O
increase	O
in	O
complexity	O
of	O
the	O
membrane	O
potential	B
over	O
the	O
hopﬁeld	O
case	O
,	O
deriving	O
appropriate	O
learning	B
dynamics	O
for	O
this	O
new	O
system	B
is	O
straightforward	O
since	O
,	O
as	O
before	O
,	O
the	O
hidden	B
variables	I
(	O
here	O
the	O
membrane	O
potentials	O
)	O
update	O
in	O
a	O
deterministic	B
fashion	O
.	O
the	O
potential	B
derivatives	O
are	O
(	O
cid:18	O
)	O
(	O
cid:19	O
)	O
dai	O
(	O
t	O
)	O
dwij	O
=	O
(	O
1	O
−	O
vi	O
(	O
t	O
−	O
1	O
)	O
)	O
α	O
dai	O
(	O
t	O
−	O
1	O
)	O
dwij	O
+	O
vj	O
(	O
t	O
)	O
(	O
26.5.12	O
)	O
by	O
initialising	O
the	O
derivative	O
dai	O
(	O
t=1	O
)	O
=	O
0	O
,	O
equations	O
(	O
26.5.5,26.5.11,26.5.12	O
)	O
deﬁne	O
a	O
ﬁrst	B
order	I
recursion	O
for	O
the	O
gradient	B
which	O
can	O
be	O
used	O
to	O
adapt	O
wij	O
in	O
the	O
usual	O
manner	O
wij	O
←	O
wij	O
+	O
ηdl/dwij	O
.	O
we	O
could	O
also	O
apply	O
synaptic	O
dynamics	O
to	O
this	O
case	O
by	O
replacing	O
the	O
term	O
vj	O
(	O
t	O
)	O
in	O
equation	B
(	O
26.5.12	O
)	O
by	O
xj	O
(	O
t	O
)	O
vj	O
(	O
t	O
)	O
.	O
dwij	O
although	O
a	O
detailed	O
discussion	O
of	O
the	O
properties	B
of	O
the	O
neuronal	O
responses	O
for	O
networks	O
trained	O
in	O
this	O
way	O
is	O
beyond	O
the	O
scope	O
of	O
these	O
notes	O
,	O
an	O
interesting	O
consequence	O
of	O
the	O
learning	B
rule	O
equation	B
(	O
26.5.12	O
)	O
is	O
a	O
spike	O
time	O
dependent	O
learning	B
window	O
in	O
qualitative	O
agreement	O
with	O
experimental	O
results	O
[	O
223	O
,	O
186	O
]	O
.	O
in	O
summary	O
,	O
provided	O
one	O
deals	O
with	O
deterministic	O
latent	B
dynamics	O
,	O
essentially	O
arbitrarily	O
complex	O
spatio-	O
temporal	O
patterns	O
may	O
potentially	O
be	O
learned	O
,	O
and	O
generated	O
under	O
cued	O
retrieval	O
,	O
for	O
very	O
complex	O
neural	O
dynamics	O
.	O
the	O
spike-response	O
model	B
[	O
108	O
]	O
can	O
be	O
seen	O
as	O
a	O
special	O
case	O
of	O
the	O
deterministic	B
latent	I
variable	I
model	I
in	O
which	O
the	O
latent	B
variables	O
have	O
been	O
explicitly	O
integrated	O
out	O
.	O
26.6	O
code	O
demohopfield.m	O
:	O
demo	O
of	O
hopﬁeld	O
sequence	B
learning	I
hebbml.m	O
:	O
gradient	B
ascent	O
training	B
of	O
a	O
set	O
of	O
sequences	B
using	O
max	O
likelihood	B
hopfieldhiddennl.m	O
:	O
hopﬁeld	O
network	O
with	O
additional	O
non-linear	B
latent	O
variables	O
demohopfieldlatent.m	O
:	O
demo	O
of	O
hopﬁeld	O
net	O
with	O
deterministic	O
latent	B
variables	O
hopfieldhiddenliknl.m	O
:	O
hopﬁeld	O
net	O
with	O
hidden	O
variables	O
sequence	O
likelihood	O
t−1	O
(	O
cid:88	O
)	O
26.7	O
exercises	O
exercise	O
241.	O
consider	O
a	O
very	O
large	O
hopﬁeld	O
network	O
v	O
(	O
cid:29	O
)	O
1	O
used	O
to	O
store	O
a	O
single	O
temporal	O
sequence	O
of	O
length	O
v	O
(	O
1	O
:	O
t	O
)	O
,	O
t	O
(	O
cid:28	O
)	O
v	O
.	O
in	O
this	O
case	O
the	O
weight	B
matrix	O
w	O
may	O
be	O
diﬃcult	O
to	O
store	O
.	O
explain	O
how	O
to	O
justify	O
the	O
assumption	O
wij	O
=	O
ui	O
(	O
t	O
)	O
vi	O
(	O
t	O
+	O
1	O
)	O
vj	O
(	O
t	O
)	O
(	O
26.7.1	O
)	O
t=1	O
where	O
ui	O
(	O
t	O
)	O
are	O
the	O
dual	B
parameters	I
and	O
derive	O
an	O
update	O
rule	O
for	O
the	O
dual	B
parameters	I
u.	O
exercise	O
242.	O
a	O
hopﬁeld	O
network	O
is	O
used	O
to	O
store	O
a	O
raw	O
uncompressed	O
binary	O
video	O
sequence	O
.	O
each	O
image	O
in	O
the	O
sequence	O
contains	O
106	O
binary	O
pixels	O
.	O
at	O
a	O
rate	O
of	O
10	O
frames	O
per	O
second	O
,	O
how	O
many	O
hours	O
of	O
video	O
can	O
106	O
neurons	O
store	O
?	O
exercise	O
243.	O
derive	O
the	O
update	O
equation	B
(	O
26.3.22	O
)	O
.	O
exercise	O
244.	O
show	O
that	O
the	O
hessian	O
equation	B
(	O
26.3.16	O
)	O
is	O
negative	O
deﬁnite	O
.	O
that	O
is	O
(	O
cid:88	O
)	O
i	O
,	O
j	O
,	O
k	O
,	O
l	O
xijxkl	O
d2l	O
dwijdwkl	O
≤	O
0	O
for	O
any	O
x	O
(	O
cid:54	O
)	O
=	O
0.	O
draft	O
march	O
9	O
,	O
2010	O
(	O
26.7.2	O
)	O
487	O
exercise	O
245.	O
for	O
the	O
augmented	B
hopﬁeld	O
network	O
of	O
section	O
(	O
26.4.2	O
)	O
,	O
with	O
latent	O
dynamics	O
hi	O
(	O
t	O
+	O
1	O
)	O
=	O
2σ	O
aijhj	O
(	O
t	O
)	O
+	O
bijvj	O
(	O
t	O
)	O
	O
(	O
cid:88	O
)	O
j	O
	O
−	O
1	O
exercises	O
(	O
26.7.3	O
)	O
derive	O
the	O
derivative	O
recursions	O
described	O
in	O
section	O
(	O
26.4.2	O
)	O
.	O
488	O
draft	O
march	O
9	O
,	O
2010	O
part	O
v	O
approximate	B
inference	I
489	O
chapter	O
27	O
sampling	B
27.1	O
introduction	O
sampling	B
concerns	O
drawing	O
realisations	O
x1	O
,	O
.	O
.	O
.	O
,	O
xl	O
of	O
a	O
variable	B
x	O
from	O
a	O
distribution	B
p	O
(	O
x	O
)	O
.	O
for	O
a	O
discrete	B
variable	O
x	O
,	O
in	O
the	O
limit	O
of	O
a	O
large	O
number	O
of	O
samples	O
,	O
the	O
fraction	O
of	O
samples	O
in	O
state	O
x	O
tends	O
to	O
p	O
(	O
x	O
=	O
x	O
)	O
.	O
that	O
is	O
,	O
xl	O
=	O
x	O
=	O
p	O
(	O
x	O
=	O
x	O
)	O
(	O
27.1.1	O
)	O
(	O
cid:82	O
)	O
in	O
the	O
continuous	B
case	O
,	O
one	O
can	O
consider	O
a	O
small	O
region	O
∆	O
such	O
that	O
the	O
probability	B
that	O
the	O
samples	O
occupy	O
∆	O
tends	O
to	O
the	O
integral	O
of	O
p	O
(	O
x	O
)	O
over	O
∆	O
.	O
in	O
other	O
words	O
,	O
the	O
relative	O
frequency	O
x	O
∈	O
∆	O
tends	O
to	O
x∈∆	O
p	O
(	O
x	O
)	O
.	O
given	O
a	O
ﬁnite	O
set	O
of	O
samples	O
,	O
one	O
can	O
then	O
approximate	B
expectations	O
using	O
(	O
cid:105	O
)	O
(	O
cid:69	O
)	O
l	O
(	O
cid:88	O
)	O
l=1	O
l	O
(	O
cid:88	O
)	O
i	O
(	O
cid:104	O
)	O
l=1	O
lim	O
l→∞	O
1	O
l	O
(	O
cid:104	O
)	O
f	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
)	O
≈	O
1	O
l	O
=	O
(	O
cid:68	O
)	O
ˆf	O
(	O
cid:69	O
)	O
(	O
cid:68	O
)	O
ˆf	O
2	O
(	O
cid:69	O
)	O
−	O
(	O
cid:68	O
)	O
1	O
l	O
l	O
(	O
cid:88	O
)	O
(	O
cid:69	O
)	O
2	O
(	O
cid:68	O
)	O
ˆf	O
l=1	O
f	O
(	O
xl	O
)	O
≡	O
ˆf	O
(	O
27.1.2	O
)	O
this	O
approximation	B
holds	O
for	O
both	O
discrete	B
and	O
continuous	B
variables	O
.	O
provided	O
the	O
samples	O
are	O
indeed	O
from	O
p	O
(	O
x	O
)	O
,	O
then	O
the	O
average	B
of	O
the	O
approximation	B
is	O
the	O
variance	B
of	O
the	O
approximation	B
is	O
f	O
(	O
xl	O
)	O
p	O
(	O
xl=xl	O
)	O
=	O
(	O
cid:104	O
)	O
f	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
)	O
(	O
cid:16	O
)	O
(	O
cid:10	O
)	O
f	O
2	O
(	O
x	O
)	O
(	O
cid:11	O
)	O
=	O
1	O
l	O
p	O
(	O
x	O
)	O
−	O
(	O
cid:104	O
)	O
f	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
2	O
p	O
(	O
x	O
)	O
(	O
cid:17	O
)	O
(	O
27.1.3	O
)	O
(	O
27.1.4	O
)	O
hence	O
the	O
mean	B
of	O
the	O
approximation	B
is	O
the	O
exact	O
mean	O
of	O
f	O
and	O
the	O
variance	B
of	O
the	O
approximation	B
scales	O
inversely	O
with	O
the	O
number	O
of	O
samples	O
.	O
in	O
principle	O
,	O
therefore	O
,	O
provided	O
the	O
samples	O
are	O
independently	O
drawn	O
from	O
p	O
(	O
x	O
)	O
,	O
only	O
a	O
small	O
number	O
of	O
samples	O
is	O
required	O
to	O
accurately	O
estimate	O
this	O
mean	B
.	O
impor-	O
tantly	O
,	O
this	O
result	O
is	O
independent	O
of	O
the	O
dimension	O
of	O
x.	O
however	O
,	O
the	O
critical	O
diﬃculty	O
is	O
in	O
actually	O
generating	O
independent	O
samples	O
from	O
p	O
(	O
x	O
)	O
.	O
drawing	O
samples	O
from	O
high-dimensional	O
distributions	O
is	O
gen-	O
erally	O
diﬃcult	O
and	O
few	O
guarantees	O
exist	O
to	O
ensure	O
that	O
in	O
a	O
practical	O
timeframe	O
the	O
samples	O
produced	O
are	O
representative	O
enough	O
such	O
that	O
expectations	O
can	O
be	O
approximated	O
accurately	O
.	O
there	O
are	O
many	O
diﬀerent	O
sampling	B
algorithms	O
,	O
all	O
of	O
which	O
‘	O
work	O
in	O
principle	O
’	O
,	O
but	O
each	O
‘	O
working	O
in	O
practice	O
’	O
only	O
when	O
the	O
distri-	O
bution	O
satisﬁes	O
particular	O
properties	B
[	O
112	O
]	O
.	O
before	O
we	O
develop	O
schemes	O
for	O
multi-variate	B
distributions	O
,	O
we	O
consider	O
the	O
univariate	B
case	O
.	O
491	O
introduction	O
1	O
×	O
2	O
3	O
figure	O
27.1	O
:	O
a	O
representation	B
of	O
the	O
discrete	B
distri-	O
bution	O
equation	B
(	O
27.1.5	O
)	O
.	O
the	O
unit	O
interval	O
from	O
0	O
to	O
1	O
is	O
partitioned	B
in	O
parts	O
whose	O
lengths	O
are	O
equal	O
to	O
0.6	O
,	O
0.1	O
and	O
0.3	O
.	O
27.1.1	O
univariate	B
sampling	O
in	O
the	O
following	O
,	O
we	O
assume	O
that	O
a	O
random	O
number	O
generator	O
exists	O
which	O
is	O
able	O
to	O
produce	O
a	O
value	B
uniformly	O
at	O
random	O
from	O
the	O
unit	O
interval	O
[	O
0	O
,	O
1	O
]	O
.	O
we	O
will	O
make	O
use	O
of	O
this	O
uniform	B
random	O
number	O
generator	O
to	O
draw	O
samples	O
from	O
non-uniform	O
distributions	O
.	O
discrete	B
case	O
consider	O
the	O
one	O
dimensional	O
discrete	B
distribution	O
p	O
(	O
x	O
)	O
where	O
dom	O
(	O
x	O
)	O
=	O
{	O
1	O
,	O
2	O
,	O
3	O
}	O
,	O
with	O
	O
0.6	O
x	O
=	O
1	O
0.1	O
x	O
=	O
2	O
0.3	O
x	O
=	O
3	O
p	O
(	O
x	O
)	O
=	O
(	O
27.1.5	O
)	O
this	O
represents	O
a	O
partitioning	O
of	O
the	O
unit	O
interval	O
[	O
0	O
,	O
1	O
]	O
in	O
which	O
the	O
interval	O
[	O
0	O
,	O
0.6	O
]	O
has	O
been	O
labelled	B
as	O
state	O
1	O
,	O
[	O
0.6	O
,	O
0.7	O
]	O
as	O
state	O
2	O
,	O
and	O
[	O
0.7	O
,	O
1.0	O
]	O
as	O
state	O
3	O
,	O
ﬁg	O
(	O
27.1	O
)	O
.	O
if	O
we	O
were	O
to	O
drop	O
a	O
point	O
×	O
anywhere	O
at	O
random	O
,	O
uniformly	O
in	O
the	O
interval	O
[	O
0	O
,	O
1	O
]	O
,	O
the	O
chance	O
that	O
×	O
would	O
land	O
in	O
interval	O
1	O
is	O
0.6	O
,	O
and	O
the	O
chance	O
that	O
it	O
would	O
be	O
in	O
interval	O
2	O
is	O
0.1	O
and	O
similarly	O
,	O
for	O
interval	O
3	O
,	O
0.3.	O
this	O
therefore	O
deﬁnes	O
for	O
us	O
a	O
valid	O
sampling	B
procedure	O
for	O
discrete	B
one-dimensional	O
distributions	O
as	O
described	O
in	O
algorithm	B
(	O
24	O
)	O
.	O
in	O
our	O
example	O
,	O
we	O
have	O
(	O
c0	O
,	O
c1	O
,	O
c2	O
,	O
c3	O
)	O
=	O
(	O
0	O
,	O
0.6	O
,	O
0.7	O
,	O
1	O
)	O
.	O
we	O
then	O
draw	O
a	O
sample	B
uniformly	O
from	O
[	O
0	O
,	O
1	O
]	O
,	O
say	O
u	O
=	O
0.66.	O
then	O
the	O
sampled	O
state	O
would	O
be	O
state	O
2	O
,	O
since	O
this	O
is	O
in	O
the	O
interval	O
(	O
c1	O
,	O
c2	O
]	O
.	O
sampling	B
from	O
a	O
discrete	B
univariate	O
distribution	B
is	O
straightforward	O
since	O
computing	O
the	O
cumulant	B
takes	O
only	O
o	O
(	O
k	O
)	O
steps	O
for	O
a	O
k	O
state	O
discrete	B
variable	O
.	O
continuous	B
case	O
in	O
the	O
following	O
we	O
assume	O
that	O
a	O
method	O
exists	O
to	O
generate	O
samples	O
from	O
the	O
uniform	B
distribution	I
intuitively	O
,	O
the	O
generalisation	B
of	O
the	O
discrete	B
case	O
to	O
the	O
continuous	B
case	O
is	O
clear	O
.	O
first	O
u	O
(	O
x|	O
[	O
0	O
,	O
1	O
]	O
)	O
.	O
we	O
calculate	O
the	O
cumulant	B
density	O
function	B
c	O
(	O
y	O
)	O
=	O
p	O
(	O
x	O
)	O
dx	O
(	O
27.1.6	O
)	O
(	O
cid:90	O
)	O
y	O
−∞	O
then	O
we	O
sample	B
u	O
uniformly	O
from	O
[	O
0	O
,	O
1	O
]	O
,	O
and	O
obtain	O
the	O
corresponding	O
sample	B
x	O
by	O
solving	B
c	O
(	O
x	O
)	O
=	O
u	O
⇒	O
x	O
=	O
c−1	O
(	O
u	O
)	O
.	O
formally	O
,	O
therefore	O
,	O
sampling	B
of	O
a	O
continuous	B
univariate	O
variable	B
is	O
straightforward	O
provided	O
we	O
can	O
compute	O
the	O
integral	O
of	O
the	O
corresponding	O
probability	B
density	O
function	B
.	O
algorithm	B
24	O
sampling	B
from	O
a	O
univariate	B
discrete	O
distribution	B
p	O
with	O
k	O
states	O
.	O
1	O
:	O
label	O
the	O
k	O
states	O
as	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
,	O
with	O
associated	O
probabilities	O
pi	O
.	O
2	O
:	O
calculate	O
the	O
cumulant	B
ci	O
=	O
(	O
cid:88	O
)	O
j≤i	O
pj	O
and	O
set	O
c0	O
=	O
0	O
.	O
3	O
:	O
draw	O
a	O
value	B
u	O
uniformly	O
at	O
random	O
from	O
the	O
unit	O
interval	O
[	O
0	O
,	O
1	O
]	O
.	O
4	O
:	O
find	O
that	O
i	O
for	O
which	O
ci−1	O
<	O
u	O
≤	O
ci	O
.	O
5	O
:	O
return	O
state	O
i	O
as	O
a	O
sample	B
from	O
p.	O
492	O
draft	O
march	O
9	O
,	O
2010	O
introduction	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
27.2	O
:	O
histograms	O
of	O
the	O
samples	O
from	O
the	O
three	O
(	O
a	O
)	O
:	O
20	O
state	O
distribution	B
p	O
(	O
x	O
)	O
=	O
{	O
0.6	O
,	O
0.1	O
,	O
0.3	O
}	O
.	O
samples	O
.	O
(	O
b	O
)	O
:	O
1000	O
samples	O
.	O
as	O
the	O
number	O
of	O
sam-	O
ples	O
increases	O
,	O
the	O
relative	O
frequency	O
of	O
the	O
samples	O
tends	O
to	O
the	O
distribution	B
p	O
(	O
x	O
)	O
.	O
for	O
special	O
distributions	O
,	O
such	O
as	O
gaussians	O
,	O
numerically	O
eﬃcient	B
alternative	O
procedures	O
exist	O
,	O
usually	O
based	O
on	O
co-ordinate	O
transformations	O
,	O
see	O
exercise	O
(	O
246	O
)	O
.	O
27.1.2	O
multi-variate	B
sampling	O
one	O
way	O
to	O
generalise	O
the	O
one	O
dimensional	O
discrete	B
case	O
to	O
a	O
higher	O
dimensional	O
distribution	B
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
is	O
to	O
translate	O
this	O
into	O
an	O
equivalent	B
one-dimensional	O
distribution	B
.	O
this	O
can	O
be	O
achieved	O
by	O
enumerating	O
all	O
the	O
possible	O
joint	B
states	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
,	O
giving	O
each	O
a	O
unique	O
integer	O
i	O
from	O
1	O
to	O
the	O
total	O
number	O
of	O
states	O
,	O
and	O
constructing	O
a	O
univariate	B
distribution	O
with	O
probability	O
p	O
(	O
i	O
)	O
=	O
p	O
(	O
x	O
)	O
for	O
i	O
corresponding	O
to	O
the	O
multivari-	O
ate	O
state	O
x.	O
this	O
then	O
transforms	O
the	O
multi-dimensional	O
distribution	B
into	O
an	O
equivalent	B
one-dimensional	O
distribution	B
,	O
and	O
sampling	B
can	O
be	O
achieved	O
as	O
before	O
.	O
in	O
general	O
,	O
of	O
course	O
,	O
this	O
procedure	O
is	O
impractical	O
since	O
the	O
number	O
of	O
states	O
will	O
grow	O
exponentially	O
with	O
the	O
number	O
of	O
variables	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
.	O
an	O
alternative	O
exact	O
approach	O
would	O
be	O
to	O
capitalise	O
on	O
the	O
relation	O
p	O
(	O
x1	O
,	O
x2	O
)	O
=	O
p	O
(	O
x2|x1	O
)	O
p	O
(	O
x1	O
)	O
(	O
27.1.7	O
)	O
we	O
can	O
sample	B
from	O
the	O
joint	B
distribution	O
p	O
(	O
x1	O
,	O
x2	O
)	O
by	O
ﬁrst	O
sampling	O
a	O
state	O
for	O
x1	O
from	O
the	O
one-dimensional	O
p	O
(	O
x1	O
)	O
,	O
and	O
then	O
,	O
with	O
x1	O
clamped	O
to	O
this	O
state	O
,	O
sampling	B
a	O
state	O
for	O
x2	O
from	O
the	O
one-dimensional	O
p	O
(	O
x2|x1	O
)	O
.	O
it	O
is	O
clear	O
how	O
to	O
generalise	O
this	O
to	O
more	O
variables	O
by	O
using	O
a	O
cascade	B
decomposition	O
:	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
=	O
p	O
(	O
xn|xn−1	O
,	O
.	O
.	O
.	O
,	O
x1	O
)	O
p	O
(	O
xn−1|xn−2	O
,	O
.	O
.	O
.	O
,	O
x1	O
)	O
.	O
.	O
.	O
,	O
p	O
(	O
x2|x1	O
)	O
p	O
(	O
x1	O
)	O
(	O
27.1.8	O
)	O
however	O
,	O
in	O
order	O
to	O
apply	O
this	O
technique	O
,	O
we	O
need	O
to	O
know	O
the	O
conditionals	O
p	O
(	O
xi|xi−1	O
,	O
.	O
.	O
.	O
,	O
x1	O
)	O
.	O
unless	O
these	O
are	O
explicitly	O
given	O
we	O
need	O
to	O
compute	O
these	O
from	O
the	O
joint	B
distribution	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
.	O
such	O
con-	O
ditionals	O
will	O
,	O
in	O
general	O
,	O
require	O
the	O
summation	O
over	O
an	O
exponential	B
number	O
of	O
states	O
and	O
,	O
except	O
for	O
small	O
n	O
,	O
generally	O
also	O
be	O
impractical	O
.	O
for	O
belief	B
networks	I
,	O
however	O
,	O
by	O
construction	B
the	O
conditionals	O
are	O
speciﬁed	O
so	O
that	O
this	O
technique	O
becomes	O
practical	O
,	O
as	O
we	O
discuss	O
in	O
section	O
(	O
27.2	O
)	O
.	O
drawing	O
samples	O
from	O
a	O
multi-variate	B
distribution	O
is	O
in	O
general	O
therefore	O
a	O
complex	O
task	O
and	O
one	O
seeks	O
to	O
exploit	O
any	O
structural	O
properties	B
of	O
the	O
distribution	B
to	O
make	O
this	O
computationally	O
more	O
feasible	O
.	O
a	O
common	O
approach	B
is	O
to	O
seek	O
to	O
transform	O
the	O
distribution	B
into	O
a	O
product	O
of	O
lower	O
dimensional	O
distributions	O
.	O
a	O
classic	O
example	O
of	O
this	O
is	O
sampling	B
from	O
a	O
multi-variate	B
gaussian	O
,	O
which	O
can	O
be	O
reduced	O
to	O
sampling	B
from	O
a	O
set	O
of	O
univariate	B
gaussians	O
by	O
a	O
suitable	O
coordinate	O
transformation	O
,	O
as	O
discussed	O
in	O
example	O
(	O
112	O
)	O
.	O
example	O
112	O
(	O
sampling	B
from	O
a	O
multi-variate	B
gaussian	O
)	O
.	O
our	O
interest	O
is	O
to	O
draw	O
a	O
sample	B
from	O
the	O
multi-variate	B
gaussian	O
p	O
(	O
x	O
)	O
=	O
n	O
(	O
x	O
m	O
,	O
s	O
)	O
.	O
for	O
a	O
general	O
covariance	B
matrix	O
s	O
,	O
p	O
(	O
x	O
)	O
does	O
not	O
factorise	O
into	O
a	O
product	O
of	O
univariate	B
distributions	O
.	O
however	O
,	O
consider	O
the	O
transformation	O
where	O
c	O
is	O
chosen	O
so	O
that	O
cct	O
=	O
s.	O
since	O
this	O
is	O
a	O
linear	B
transformation	I
,	O
y	O
is	O
also	O
gaussian	O
distributed	O
with	O
mean	B
(	O
27.1.9	O
)	O
y	O
=	O
c−1	O
(	O
x	O
−	O
m	O
)	O
(	O
cid:104	O
)	O
y	O
(	O
cid:105	O
)	O
=	O
(	O
cid:10	O
)	O
c−1	O
(	O
x	O
−	O
m	O
)	O
(	O
cid:11	O
)	O
=	O
c−1	O
(	O
cid:68	O
)	O
yyt	O
(	O
cid:69	O
)	O
(	O
cid:68	O
)	O
p	O
(	O
x	O
)	O
(	O
cid:17	O
)	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
)	O
−	O
m	O
p	O
(	O
x	O
)	O
=	O
c−1	O
(	O
cid:16	O
)	O
(	O
x	O
−	O
m	O
)	O
(	O
x	O
−	O
m	O
)	O
t	O
(	O
cid:69	O
)	O
since	O
the	O
mean	B
of	O
y	O
is	O
zero	O
,	O
the	O
covariance	B
is	O
given	O
by	O
=	O
c−1	O
(	O
m	O
−	O
m	O
)	O
=	O
0	O
(	O
27.1.10	O
)	O
c−t	O
=	O
c−1sc−t	O
=	O
c−1cctc−t	O
=	O
i	O
(	O
27.1.11	O
)	O
p	O
(	O
x	O
)	O
draft	O
march	O
9	O
,	O
2010	O
493	O
123024681230200400600	O
x1	O
x2	O
x3	O
x4	O
x5	O
x6	O
ancestral	B
sampling	I
figure	O
27.3	O
:	O
an	O
ancestral	B
belief	O
network	O
without	O
any	O
evidential	O
variables	O
.	O
to	O
sample	B
from	O
this	O
distribu-	O
tion	O
,	O
we	O
draw	O
a	O
sample	B
from	O
variable	B
1	O
,	O
and	O
then	O
variables	O
,	O
2	O
,	O
.	O
.	O
.	O
,6	O
in	O
order	O
.	O
hence	O
p	O
(	O
y	O
)	O
=	O
n	O
(	O
y	O
0	O
,	O
i	O
)	O
=	O
(	O
cid:89	O
)	O
i	O
n	O
(	O
yi	O
0	O
,	O
1	O
)	O
(	O
27.1.12	O
)	O
hence	O
a	O
sample	B
from	O
y	O
can	O
be	O
obtained	O
by	O
independently	O
drawing	O
a	O
sample	B
from	O
each	O
of	O
the	O
univariate	B
zero	O
mean	B
unit	O
variance	B
gaussians	O
.	O
given	O
a	O
sample	B
for	O
y	O
,	O
a	O
sample	B
for	O
x	O
is	O
obtained	O
using	O
x	O
=	O
cy	O
+	O
m	O
(	O
27.1.13	O
)	O
drawing	O
samples	O
from	O
a	O
univariate	B
gaussian	O
is	O
a	O
well-studied	O
topic	O
,	O
with	O
a	O
popular	O
method	O
being	O
the	O
box-muller	O
technique	O
,	O
exercise	O
(	O
246	O
)	O
.	O
27.2	O
ancestral	B
sampling	I
belief	O
networks	O
take	O
the	O
general	O
form	O
:	O
p	O
(	O
x	O
)	O
=	O
(	O
cid:89	O
)	O
i	O
p	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
(	O
27.2.1	O
)	O
where	O
each	O
of	O
the	O
conditional	B
distributions	O
p	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
is	O
known	O
.	O
provided	O
that	O
no	O
variables	O
are	O
evidential	O
,	O
we	O
can	O
sample	B
from	O
this	O
distribution	B
in	O
a	O
straightforward	O
manner	O
.	O
for	O
convenience	O
,	O
we	O
ﬁrst	O
rename	O
the	O
variable	B
indices	O
so	O
that	O
parent	O
variables	O
always	O
come	O
before	O
their	O
children	B
(	O
ancestral	B
ordering	I
)	O
,	O
for	O
example	O
(	O
see	O
ﬁg	O
(	O
27.3	O
)	O
)	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
x6	O
)	O
=	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2	O
)	O
p	O
(	O
x3|x1	O
,	O
x2	O
)	O
p	O
(	O
x4|x3	O
)	O
p	O
(	O
x5|x3	O
)	O
p	O
(	O
x6|x4	O
,	O
x5	O
)	O
(	O
27.2.2	O
)	O
one	O
can	O
sample	B
ﬁrst	O
from	O
those	O
nodes	O
that	O
do	O
not	O
have	O
any	O
parents	B
(	O
here	O
,	O
x1	O
and	O
x2	O
)	O
.	O
given	O
these	O
values	O
,	O
one	O
can	O
then	O
sample	B
x3	O
,	O
and	O
then	O
x4	O
and	O
x5	O
and	O
ﬁnally	O
x6	O
.	O
despite	O
the	O
presence	O
of	O
loops	O
in	O
the	O
graph	B
,	O
such	O
a	O
forward	B
sampling	I
procedure	O
is	O
straightforward	O
.	O
this	O
procedure	O
holds	O
for	O
both	O
discrete	B
and	O
continuous	B
variables	O
.	O
if	O
one	O
attempted	O
to	O
carry	O
out	O
an	O
exact	O
inference	O
scheme	O
using	O
moralisation	B
and	O
triangulation	B
,	O
in	O
more	O
complex	O
multiply	O
connected	B
graphs	O
,	O
cliques	O
can	O
become	O
very	O
large	O
.	O
however	O
,	O
regardless	O
of	O
the	O
loop	O
struc-	O
ture	O
,	O
ancestral	B
sampling	I
is	O
straightforward	O
.	O
ancestral	B
or	O
‘	O
forward	O
’	O
sampling	B
is	O
a	O
case	O
of	O
perfect	B
sampling	I
(	O
also	O
termed	O
exact	B
sampling	I
)	O
since	O
each	O
sample	B
is	O
indeed	O
drawn	O
from	O
the	O
required	O
distribution	B
.	O
this	O
is	O
in	O
contrast	O
to	O
markov	O
chain	B
monte	O
carlo	O
methods	O
sections	O
(	O
27.3,27.4	O
)	O
for	O
which	O
the	O
samples	O
are	O
from	O
p	O
(	O
x	O
)	O
only	O
in	O
the	O
limit	O
of	O
a	O
large	O
number	O
of	O
iterations	O
.	O
27.2.1	O
dealing	O
with	O
evidence	O
how	O
can	O
we	O
sample	B
from	O
a	O
distribution	B
in	O
which	O
certain	O
variables	O
xe	O
are	O
clamped	O
to	O
evidential	O
states	O
?	O
formally	O
we	O
need	O
to	O
sample	B
from	O
p	O
(	O
x\e|xe	O
)	O
=	O
p	O
(	O
x\e	O
,	O
xe	O
)	O
p	O
(	O
xe	O
)	O
494	O
(	O
27.2.3	O
)	O
draft	O
march	O
9	O
,	O
2010	O
gibbs	O
sampling	B
x1	O
x2	O
x3	O
x4	O
x5	O
x6	O
x7	O
figure	O
27.4	O
:	O
the	O
markov	O
blanket	O
of	O
x4	O
.	O
to	O
draw	O
a	O
sample	B
from	O
p	O
(	O
x4|x\4	O
)	O
we	O
clamp	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x5	O
,	O
x7	O
into	O
their	O
evidential	O
states	O
and	O
draw	O
a	O
sam-	O
ple	O
from	O
p	O
(	O
x4|x1	O
,	O
x2	O
)	O
p	O
(	O
x6|x3	O
,	O
x4	O
)	O
p	O
(	O
x7|x4	O
,	O
x5	O
)	O
/z	O
where	O
z	O
is	O
a	O
normalisation	B
constant	I
.	O
if	O
an	O
evidential	O
variable	B
xi	O
has	O
no	O
parents	O
,	O
then	O
one	O
can	O
simply	O
set	O
the	O
variable	B
into	O
this	O
state	O
and	O
con-	O
tinue	O
forward	B
sampling	I
as	O
before	O
.	O
for	O
example	O
,	O
to	O
compute	O
a	O
sample	B
from	O
p	O
(	O
x1	O
,	O
x3	O
,	O
x4	O
,	O
x5	O
,	O
x6|x2	O
)	O
deﬁned	O
in	O
equation	B
(	O
27.2.2	O
)	O
,	O
one	O
simply	O
clamps	O
the	O
x2	O
into	O
its	O
evidential	O
state	O
and	O
continues	O
forward	B
sampling	I
.	O
the	O
reason	O
this	O
is	O
straightforward	O
is	O
that	O
conditioning	B
on	O
x2	O
merely	O
deﬁnes	O
a	O
new	O
distribution	B
on	O
a	O
subset	O
of	O
the	O
variables	O
,	O
for	O
which	O
the	O
distribution	B
is	O
immediately	O
known	O
.	O
on	O
the	O
other	O
hand	O
,	O
consider	O
sampling	B
from	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
,	O
x5|x6	O
)	O
.	O
using	O
bayes	O
’	O
rule	O
,	O
we	O
have	O
(	O
cid:80	O
)	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
,	O
x5|x6	O
)	O
=	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2	O
)	O
p	O
(	O
x3|x1	O
,	O
x2	O
)	O
p	O
(	O
x4|x3	O
)	O
p	O
(	O
x5|x3	O
)	O
p	O
(	O
x6|x4	O
,	O
x5	O
)	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
,	O
x5	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2	O
)	O
p	O
(	O
x3|x1	O
,	O
x2	O
)	O
p	O
(	O
x4|x3	O
)	O
p	O
(	O
x5|x3	O
)	O
p	O
(	O
x6|x4	O
,	O
x5	O
)	O
(	O
27.2.4	O
)	O
the	O
conditioning	B
on	O
x6	O
means	O
that	O
the	O
structure	B
of	O
the	O
distribution	B
on	O
the	O
non-evidential	O
variables	O
changes	O
–	O
for	O
example	O
x4	O
and	O
x5	O
become	O
coupled	B
.	O
one	O
could	O
attempt	O
to	O
work	O
out	O
an	O
equivalent	B
new	O
forward	O
sam-	O
pling	O
structure	B
,	O
(	O
see	O
exercise	O
(	O
247	O
)	O
)	O
although	O
generally	O
this	O
will	O
be	O
as	O
complex	O
as	O
running	O
an	O
exact	O
inference	O
approach	B
.	O
probability	B
that	O
a	O
sample	B
from	O
p	O
(	O
x	O
)	O
will	O
be	O
consistent	B
with	O
the	O
evidence	B
is	O
roughly	O
o	O
(	O
1/	O
(	O
cid:81	O
)	O
an	O
alternative	O
is	O
to	O
proceed	O
with	O
forward	O
sampling	B
from	O
the	O
non-evidential	O
distribution	B
,	O
and	O
then	O
discard	O
any	O
samples	O
which	O
do	O
not	O
match	O
the	O
evidential	O
states	O
.	O
this	O
is	O
generally	O
not	O
recommended	O
since	O
the	O
i	O
)	O
where	O
i	O
is	O
the	O
number	O
of	O
states	O
of	O
evidential	O
variable	B
i.	O
in	O
principle	O
one	O
can	O
ease	O
this	O
eﬀect	O
by	O
discarding	O
dim	O
xe	O
the	O
sample	B
as	O
soon	O
as	O
any	O
variable	B
state	O
is	O
inconsistent	O
with	O
the	O
evidence	B
.	O
nevertheless	O
,	O
the	O
number	O
of	O
re-starts	O
required	O
to	O
obtain	O
a	O
valid	O
sample	B
would	O
on	O
average	B
be	O
very	O
large	O
.	O
i	O
dim	O
xe	O
27.2.2	O
perfect	B
sampling	I
for	O
a	O
markov	O
network	O
for	O
a	O
markov	O
network	O
we	O
can	O
draw	O
exact	O
samples	O
by	O
forming	O
an	O
equivalent	B
directed	O
representation	B
of	O
the	O
graph	B
,	O
see	O
section	O
(	O
6.8	O
)	O
,	O
and	O
subsequently	O
using	O
ancestral	B
sampling	I
on	O
this	O
directed	B
graph	O
.	O
this	O
is	O
achieved	O
by	O
ﬁrst	O
choosing	O
a	O
root	O
clique	B
and	O
then	O
consistently	O
orienting	O
edges	O
away	O
from	O
this	O
clique	B
.	O
an	O
exact	O
sample	O
can	O
then	O
be	O
drawn	O
from	O
the	O
markov	O
network	O
by	O
ﬁrst	O
sampling	O
from	O
the	O
root	O
clique	B
and	O
then	O
recursively	O
from	O
the	O
children	B
of	O
this	O
clique	B
.	O
see	O
potsample.m	O
,	O
jtsample.m	O
and	O
demojtreesample.m	O
.	O
27.3	O
gibbs	O
sampling	B
the	O
ineﬃciency	O
of	O
methods	O
such	O
as	O
ancestral	O
sampling	B
under	O
evidence	B
,	O
motivates	O
alternative	O
techniques	O
.	O
an	O
important	O
and	O
widespread	O
technique	O
is	O
gibbs	O
sampling	B
which	O
is	O
generally	O
straightforward	O
to	O
implement	O
.	O
no	O
evidence	O
assume	O
we	O
have	O
a	O
joint	B
sample	O
state	O
x1	O
from	O
the	O
multivariate	B
distribution	O
p	O
(	O
x	O
)	O
.	O
we	O
then	O
consider	O
a	O
particular	O
variable	B
,	O
xi	O
.	O
using	O
bayes	O
’	O
rule	O
we	O
may	O
write	O
p	O
(	O
x	O
)	O
=	O
p	O
(	O
xi|x1	O
,	O
.	O
.	O
.	O
,	O
xi−1	O
,	O
xi+1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xi−1	O
,	O
xi+1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
(	O
27.3.1	O
)	O
given	O
a	O
joint	B
initial	O
state	O
x1	O
,	O
from	O
which	O
we	O
can	O
read	O
oﬀ	O
the	O
‘	O
parental	O
’	O
state	O
x1	O
can	O
then	O
draw	O
a	O
sample	B
x2	O
i	O
from	O
1	O
,	O
.	O
.	O
.	O
,	O
x1	O
i−1	O
,	O
x1	O
i+1	O
,	O
.	O
.	O
.	O
,	O
x1	O
n	O
,	O
we	O
p	O
(	O
xi|x1	O
1	O
,	O
.	O
.	O
.	O
,	O
x1	O
i−1	O
,	O
x1	O
i+1	O
,	O
.	O
.	O
.	O
,	O
x1	O
n	O
)	O
≡	O
p	O
(	O
xi|x\i	O
)	O
draft	O
march	O
9	O
,	O
2010	O
(	O
27.3.2	O
)	O
495	O
gibbs	O
sampling	B
x1	O
x4	O
x2	O
x1	O
x2	O
x3	O
x4	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
27.5	O
:	O
(	O
a	O
)	O
:	O
a	O
toy	O
‘	O
intractable	O
’	O
distribution	B
.	O
gibbs	O
sampling	B
by	O
conditioning	B
on	O
all	O
variables	O
ex-	O
cept	O
one	O
leads	O
to	O
a	O
simple	O
univariate	O
conditional	B
dis-	O
(	O
b	O
)	O
:	O
conditioning	B
on	O
x3	O
yields	O
a	O
new	O
tribution	O
.	O
distribution	B
that	O
is	O
singly-connected	B
,	O
for	O
which	O
exact	B
sampling	I
is	O
straightforward	O
.	O
(	O
cid:1	O
)	O
.	O
one	O
then	O
selects	O
another	O
variable	B
xj	O
which	O
only	O
xi	O
has	O
been	O
updated	O
)	O
x2	O
=	O
(	O
cid:0	O
)	O
x1	O
1	O
,	O
.	O
.	O
.	O
,	O
x1	O
i−1	O
,	O
x2	O
i	O
,	O
x1	O
i+1	O
,	O
.	O
.	O
.	O
,	O
x1	O
n	O
we	O
assume	O
this	O
distribution	B
is	O
easy	O
to	O
sample	B
from	O
since	O
it	O
is	O
univariate	B
.	O
we	O
call	O
this	O
new	O
joint	B
sample	O
(	O
in	O
to	O
sample	B
and	O
,	O
by	O
continuing	O
this	O
procedure	O
,	O
generates	O
a	O
set	O
x1	O
,	O
.	O
.	O
.	O
,	O
xl	O
of	O
samples	O
in	O
which	O
each	O
xl+1	O
diﬀers	O
from	O
xl	O
in	O
only	O
a	O
single	O
component	O
.	O
the	O
reason	O
this	O
is	O
valid	O
sampling	B
scheme	O
is	O
outlined	O
in	O
section	O
(	O
27.3.1	O
)	O
.	O
for	O
a	O
belief	B
network	I
,	O
the	O
conditional	B
p	O
(	O
xi|x\i	O
)	O
is	O
deﬁned	O
by	O
the	O
markov	O
blanket	O
of	O
xi	O
:	O
p	O
(	O
xj|pa	O
(	O
xj	O
)	O
)	O
(	O
27.3.3	O
)	O
p	O
(	O
xi|x\i	O
)	O
=	O
1	O
z	O
p	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
(	O
cid:89	O
)	O
j∈ch	O
(	O
i	O
)	O
z	O
=	O
(	O
cid:88	O
)	O
p	O
(	O
xi|pa	O
(	O
xi	O
)	O
)	O
(	O
cid:89	O
)	O
xi	O
j∈ch	O
(	O
i	O
)	O
see	O
for	O
example	O
,	O
ﬁg	O
(	O
27.4	O
)	O
.	O
this	O
means	O
that	O
only	O
the	O
parent	O
and	O
parents	B
of	O
children	B
states	O
are	O
required	O
in	O
forming	O
the	O
sample	B
update	O
.	O
the	O
normalisation	B
constant	I
for	O
this	O
univariate	B
distribution	O
is	O
straightforward	O
to	O
work	O
out	O
from	O
the	O
requirement	O
:	O
p	O
(	O
xj|pa	O
(	O
xj	O
)	O
)	O
(	O
27.3.4	O
)	O
in	O
the	O
case	O
of	O
a	O
continuous	B
variable	O
xi	O
the	O
summation	O
above	O
is	O
replaced	O
with	O
integration	O
.	O
evidence	B
evidence	O
is	O
readily	O
dealt	O
with	O
by	O
clamping	O
for	O
all	O
samples	O
the	O
evidential	O
variables	O
into	O
their	O
evidential	O
states	O
.	O
there	O
is	O
also	O
no	O
need	O
to	O
sample	B
for	O
these	O
variables	O
,	O
since	O
their	O
states	O
are	O
known	O
.	O
27.3.1	O
gibbs	O
sampling	B
as	O
a	O
markov	O
chain	B
in	O
gibbs	O
sampling	B
we	O
have	O
a	O
sample	B
of	O
the	O
joint	B
variables	O
xl	O
at	O
stage	O
l.	O
based	O
on	O
this	O
we	O
produce	O
a	O
new	O
joint	B
sample	O
xl+1	O
.	O
this	O
means	O
that	O
we	O
can	O
write	O
gibbs	O
sampling	B
as	O
a	O
procedure	O
that	O
draws	O
from	O
xl+1	O
∼	O
q	O
(	O
xl+1|xl	O
)	O
(	O
27.3.5	O
)	O
for	O
some	O
distribution	B
q	O
(	O
xl+1|xl	O
)	O
.	O
if	O
we	O
choose	O
the	O
variable	B
to	O
update	O
,	O
xi	O
,	O
at	O
random	O
from	O
a	O
distribution	B
q	O
(	O
i	O
)	O
,	O
then	O
gibbs	O
sampling	B
corresponds	O
to	O
drawing	O
samples	O
using	O
the	O
markov	O
transition	O
q	O
(	O
xl+1|xl	O
,	O
i	O
)	O
q	O
(	O
i	O
)	O
,	O
q	O
(	O
xl+1|xl	O
,	O
i	O
)	O
=	O
p	O
(	O
xl+1	O
i	O
δ	O
xl+1	O
j	O
,	O
xl	O
j	O
(	O
27.3.6	O
)	O
(	O
cid:16	O
)	O
|xl\i	O
)	O
(	O
cid:89	O
)	O
j	O
(	O
cid:54	O
)	O
=i	O
(	O
cid:17	O
)	O
with	O
q	O
(	O
i	O
)	O
>	O
0.	O
our	O
interest	O
is	O
to	O
show	O
that	O
the	O
stationary	B
distribution	I
of	O
q	O
(	O
x	O
(	O
cid:48	O
)	O
out	O
assuming	O
x	O
is	O
continuous	B
–	O
the	O
discrete	B
case	O
is	O
analogous	O
:	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
q	O
(	O
i	O
)	O
q	O
(	O
i	O
)	O
q	O
(	O
i	O
)	O
xi	O
q	O
(	O
i	O
)	O
p	O
(	O
x	O
(	O
cid:48	O
)	O
δ	O
(	O
cid:0	O
)	O
x	O
q	O
(	O
x	O
(	O
cid:89	O
)	O
j	O
(	O
cid:54	O
)	O
=i	O
x	O
x	O
|x\i	O
)	O
p	O
(	O
x	O
)	O
(	O
cid:48	O
)	O
j	O
,	O
xj	O
(	O
cid:1	O
)	O
p	O
(	O
x	O
\i	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
\i	O
)	O
p	O
(	O
xi	O
,	O
x	O
\i	O
)	O
p	O
(	O
x	O
(	O
cid:48	O
)	O
i|x	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
\i	O
)	O
p	O
(	O
x	O
i|x	O
(	O
cid:48	O
)	O
i|x\i	O
)	O
p	O
(	O
xi	O
,	O
x\i	O
)	O
q	O
(	O
i	O
)	O
p	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
p	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
|x	O
)	O
is	O
p	O
(	O
x	O
)	O
.	O
we	O
carry	O
this	O
(	O
27.3.7	O
)	O
(	O
27.3.8	O
)	O
(	O
27.3.9	O
)	O
(	O
27.3.10	O
)	O
i	O
q	O
(	O
xl+1|xl	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:90	O
)	O
|x	O
)	O
p	O
(	O
x	O
)	O
=	O
(	O
cid:88	O
)	O
=	O
(	O
cid:88	O
)	O
=	O
(	O
cid:88	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:48	O
)	O
q	O
(	O
x	O
x	O
i	O
i	O
i	O
i	O
496	O
i	O
draft	O
march	O
9	O
,	O
2010	O
gibbs	O
sampling	B
figure	O
27.6	O
:	O
a	O
two	O
dimensional	O
distribution	B
for	O
which	O
gibbs	O
sampling	B
fails	O
.	O
the	O
distribution	B
has	O
mass	O
only	O
in	O
the	O
shaded	O
quadrants	O
.	O
gibbs	O
sampling	B
proceeds	O
from	O
the	O
lth	O
sample	B
state	O
(	O
xl	O
1	O
)	O
,	O
2	O
)	O
and	O
then	O
sampling	B
from	O
p	O
(	O
x2|xl	O
which	O
we	O
write	O
(	O
xl+1	O
,	O
xl+1	O
1.	O
one	O
then	O
continues	O
with	O
a	O
)	O
,	O
etc	O
.	O
if	O
we	O
start	O
in	O
the	O
lower	O
left	O
quadrant	O
sample	B
from	O
p	O
(	O
x1|x2	O
=	O
xl+1	O
and	O
proceed	O
this	O
way	O
,	O
the	O
upper	O
right	O
region	O
is	O
never	O
explored	O
.	O
1	O
,	O
xl	O
)	O
where	O
xl+1	O
1	O
=	O
xl	O
2	O
1	O
2	O
hence	O
,	O
as	O
long	O
as	O
we	O
continue	O
to	O
draw	O
samples	O
according	O
to	O
the	O
distribution	B
q	O
(	O
x	O
(	O
cid:48	O
)	O
|x	O
)	O
,	O
in	O
the	O
limit	O
of	O
a	O
large	O
number	O
of	O
samples	O
we	O
will	O
ultimately	O
tend	O
to	O
draw	O
samples	O
from	O
p	O
(	O
x	O
)	O
.	O
any	O
distribution	B
q	O
(	O
i	O
)	O
>	O
0	O
suﬃces	O
so	O
visiting	O
all	O
variables	O
equally	O
often	O
is	O
also	O
a	O
valid	O
choice	O
.	O
technically	O
,	O
we	O
also	O
require	O
that	O
q	O
(	O
x	O
(	O
cid:48	O
)	O
|x	O
)	O
has	O
p	O
(	O
x	O
)	O
as	O
its	O
equilibrium	O
distribution	B
,	O
so	O
that	O
no	O
matter	O
in	O
which	O
state	O
we	O
start	O
,	O
we	O
always	O
converge	O
to	O
p	O
(	O
x	O
)	O
;	O
see	O
ﬁg	O
(	O
27.6	O
)	O
for	O
a	O
discussion	O
of	O
this	O
issue	O
.	O
27.3.2	O
structured	B
gibbs	O
sampling	B
one	O
can	O
extend	O
gibbs	O
sampling	B
by	O
using	O
conditioning	B
to	O
reveal	O
a	O
tractable	O
distribution	B
on	O
the	O
remaining	O
variables	O
.	O
for	O
example	O
,	O
consider	O
the	O
simple	O
distribution	O
,	O
ﬁg	O
(	O
27.5a	O
)	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
=	O
φ	O
(	O
x1	O
,	O
x2	O
)	O
φ	O
(	O
x2	O
,	O
x3	O
)	O
φ	O
(	O
x3	O
,	O
x4	O
)	O
φ	O
(	O
x4	O
,	O
x1	O
)	O
φ	O
(	O
x1	O
,	O
x3	O
)	O
(	O
27.3.11	O
)	O
in	O
single-site	O
gibbs	O
sampling	B
we	O
would	O
condition	O
on	O
three	O
of	O
the	O
four	O
variables	O
,	O
and	O
sample	B
from	O
the	O
remaining	O
variable	B
.	O
for	O
example	O
p	O
(	O
x1|x2	O
,	O
x3	O
,	O
x4	O
)	O
∝	O
φ	O
(	O
x1	O
,	O
x2	O
)	O
φ	O
(	O
x4	O
,	O
x1	O
)	O
φ	O
(	O
x1	O
,	O
x3	O
)	O
(	O
27.3.12	O
)	O
however	O
,	O
we	O
may	O
use	O
more	O
limited	O
conditioning	B
as	O
long	O
as	O
the	O
conditioned	O
distribution	B
is	O
easy	O
to	O
sample	B
from	O
.	O
in	O
the	O
case	O
of	O
equation	B
(	O
27.3.11	O
)	O
we	O
can	O
condition	O
on	O
x3	O
alone	O
to	O
give	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x4|x3	O
)	O
∝	O
φ	O
(	O
x1	O
,	O
x2	O
)	O
φ	O
(	O
x2	O
,	O
x3	O
)	O
φ	O
(	O
x3	O
,	O
x4	O
)	O
φ	O
(	O
x4	O
,	O
x1	O
)	O
φ	O
(	O
x1	O
,	O
x3	O
)	O
this	O
can	O
be	O
written	O
as	O
a	O
modiﬁed	O
distribution	B
,	O
ﬁg	O
(	O
27.5b	O
)	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x4|x3	O
)	O
∝	O
φ	O
(	O
cid:48	O
)	O
(	O
x1	O
,	O
x2	O
)	O
φ	O
(	O
cid:48	O
)	O
(	O
x4	O
,	O
x1	O
)	O
(	O
27.3.13	O
)	O
(	O
27.3.14	O
)	O
as	O
a	O
distribution	B
on	O
x1	O
,	O
x2	O
,	O
x4	O
this	O
is	O
a	O
singly-connected	B
linear	O
chain	B
from	O
which	O
samples	O
can	O
be	O
drawn	O
exactly	O
.	O
a	O
simple	O
approach	O
is	O
compute	O
the	O
normalisation	B
constant	I
by	O
any	O
of	O
the	O
standard	O
techniques	O
,	O
for	O
example	O
,	O
using	O
the	O
factor	B
graph	I
method	O
.	O
one	O
may	O
then	O
convert	O
this	O
undirected	B
linear	O
chain	B
to	O
a	O
directed	B
graph	O
,	O
and	O
use	O
ancestral	B
sampling	I
.	O
these	O
operations	O
are	O
linear	B
in	O
the	O
number	O
of	O
variables	O
in	O
the	O
condi-	O
tioned	O
distribution	B
.	O
alternatively	O
,	O
one	O
may	O
form	O
a	O
junction	B
tree	I
from	O
a	O
set	O
of	O
potentials	O
,	O
choose	O
a	O
root	O
and	O
then	O
form	O
a	O
set	B
chain	I
by	O
reabsorption	B
on	O
the	O
junction	B
tree	I
.	O
ancestral	B
sampling	I
can	O
then	O
be	O
performed	O
on	O
the	O
resulting	O
oriented	O
clique	B
tree	O
.	O
this	O
is	O
the	O
approach	B
taken	O
in	O
gibbssample.m	O
.	O
in	O
the	O
above	O
example	O
one	O
can	O
also	O
reveal	O
a	O
tractable	O
distribution	B
by	O
conditioning	B
on	O
x1	O
,	O
p	O
(	O
x3	O
,	O
x2	O
,	O
x4|x1	O
)	O
∝	O
φ	O
(	O
x1	O
,	O
x2	O
)	O
φ	O
(	O
x2	O
,	O
x3	O
)	O
φ	O
(	O
x3	O
,	O
x4	O
)	O
φ	O
(	O
x4	O
,	O
x1	O
)	O
φ	O
(	O
x1	O
,	O
x3	O
)	O
(	O
27.3.15	O
)	O
and	O
then	O
draw	O
a	O
sample	B
of	O
x2	O
,	O
x3	O
,	O
x4	O
from	O
this	O
distribution	B
.	O
a	O
valid	O
sampling	B
procedure	O
is	O
then	O
to	O
draw	O
a	O
sample	B
x1	O
,	O
x2	O
,	O
x4	O
from	O
equation	B
(	O
27.3.13	O
)	O
and	O
then	O
a	O
sample	B
x3	O
,	O
x2	O
,	O
x4	O
from	O
equation	B
(	O
27.3.15	O
)	O
.	O
these	O
two	O
steps	O
are	O
then	O
iterated	O
.	O
note	O
that	O
x2	O
and	O
x4	O
are	O
not	O
constrained	O
to	O
be	O
equal	O
to	O
their	O
values	O
in	O
the	O
previous	O
sample	B
.	O
this	O
procedure	O
is	O
generally	O
to	O
be	O
preferred	O
to	O
the	O
single-site	O
gibbs	O
updating	O
since	O
the	O
samples	O
are	O
less	O
correlated	O
from	O
one	O
sample	B
to	O
the	O
next	O
.	O
see	O
demogibbssample.m	O
for	O
a	O
comparison	O
of	O
unstructured	O
and	O
structured	B
sampling	O
from	O
a	O
set	O
of	O
potentials	O
.	O
draft	O
march	O
9	O
,	O
2010	O
497	O
1x2x	O
gibbs	O
sampling	B
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
27.7	O
:	O
two	O
hundred	O
gibbs	O
samples	O
for	O
a	O
two	O
dimensional	O
gaussian	O
.	O
at	O
each	O
stage	O
only	O
a	O
single	O
component	O
is	O
updated	O
.	O
(	O
a	O
)	O
:	O
for	O
a	O
gaussian	O
with	O
low	O
correlation	B
,	O
gibbs	O
sampling	B
can	O
move	O
through	O
the	O
(	O
b	O
)	O
:	O
for	O
a	O
strongly	O
correlated	O
gaussian	O
,	O
gibbs	O
sampling	B
is	O
less	O
eﬀective	O
and	O
likely	O
regions	O
eﬀectively	O
.	O
does	O
not	O
rapidly	O
explore	O
the	O
likely	O
regions	O
,	O
see	O
demogibbsgauss.m	O
.	O
27.3.3	O
remarks	O
if	O
the	O
initial	O
sample	B
x1	O
is	O
in	O
a	O
part	O
of	O
the	O
state	O
space	O
that	O
is	O
very	O
unlikely	O
then	O
it	O
may	O
take	O
some	O
time	O
for	O
the	O
samples	O
to	O
become	O
representative	O
,	O
as	O
only	O
a	O
single	O
component	O
of	O
x	O
is	O
updated	O
at	O
each	O
iteration	B
.	O
this	O
motivates	O
a	O
so-called	O
burn	B
in	I
stage	O
in	O
which	O
the	O
initial	O
samples	O
are	O
discarded	O
.	O
in	O
single	O
site	O
gibbs	O
sampling	B
there	O
will	O
be	O
a	O
high	O
degree	O
of	O
correlation	B
in	O
any	O
two	O
successive	O
samples	O
,	O
since	O
only	O
one	O
variable	B
(	O
in	O
the	O
single-site	O
updating	O
version	O
)	O
is	O
updated	O
at	O
each	O
stage	O
.	O
an	O
ideal	O
‘	O
perfect	O
’	O
sampling	B
scheme	O
would	O
draw	O
each	O
x	O
‘	O
at	O
random	O
’	O
from	O
p	O
(	O
x	O
)	O
–	O
clearly	O
,	O
in	O
general	O
,	O
two	O
such	O
perfect	O
samples	O
will	O
not	O
possess	O
the	O
same	O
degree	O
of	O
correlation	O
as	O
those	O
from	O
gibbs	O
sampling	B
.	O
this	O
motivates	O
subsampling	B
in	O
which	O
,	O
say	O
,	O
every	O
10th	O
,	O
sample	B
xk	O
,	O
xk+10	O
,	O
xk+20	O
,	O
.	O
.	O
.	O
,	O
is	O
taken	O
,	O
and	O
the	O
rest	O
discarded	O
.	O
due	O
to	O
its	O
simplicity	O
,	O
gibbs	O
sampling	B
is	O
one	O
of	O
the	O
most	O
popular	O
sampling	B
methods	O
and	O
is	O
particularly	O
convenient	O
when	O
applied	O
to	O
belief	B
networks	I
,	O
due	O
to	O
the	O
markov	O
blanket	O
property1	O
.	O
gibbs	O
sampling	B
is	O
a	O
special	O
case	O
of	O
the	O
mcmc	O
framework	O
and	O
,	O
as	O
with	O
all	O
mcmc	O
methods	O
,	O
one	O
should	O
bear	O
in	O
mind	O
that	O
convergence	O
can	O
be	O
a	O
major	O
issue	O
–	O
that	O
is	O
,	O
answering	O
questions	O
such	O
as	O
‘	O
how	O
many	O
samples	O
are	O
needed	O
to	O
be	O
reasonably	O
sure	O
that	O
my	O
sample	B
estimate	O
p	O
(	O
x5	O
)	O
is	O
accurate	O
?	O
’	O
is	O
diﬃcult	O
.	O
despite	O
mathematical	O
results	O
for	O
special	O
cases	O
,	O
general	O
rules	O
of	O
thumb	O
and	O
awareness	O
on	O
behalf	O
of	O
the	O
user	O
are	O
required	O
to	O
monitor	O
the	O
eﬃciency	O
of	O
the	O
sampling	B
.	O
gibbs	O
sampling	B
assumes	O
that	O
we	O
can	O
move	O
throughout	O
the	O
space	O
eﬀectively	O
by	O
only	O
single	O
co-ordinate	O
updates	O
.	O
we	O
also	O
require	O
that	O
every	O
state	O
can	O
be	O
visited	O
inﬁnitely	O
often	O
.	O
in	O
ﬁg	O
(	O
27.6	O
)	O
,	O
we	O
show	O
a	O
case	O
in	O
which	O
the	O
two	O
dimensional	O
continuous	B
distribution	O
has	O
mass	O
only	O
in	O
the	O
lower	O
left	O
and	O
upper	O
right	O
regions	O
.	O
in	O
that	O
case	O
,	O
if	O
we	O
start	O
in	O
the	O
lower	O
left	O
region	O
,	O
we	O
will	O
always	O
remain	O
there	O
,	O
and	O
never	O
explore	O
the	O
upper	O
right	O
region	O
.	O
this	O
problem	B
occurs	O
when	O
two	O
regions	O
which	O
are	O
not	O
connected	B
by	O
a	O
‘	O
probable	O
’	O
gibbs	O
path	B
.	O
gibbs	O
sampling	B
becomes	O
a	O
perfect	O
sampler	O
when	O
the	O
distribution	B
is	O
factorised	B
–	O
that	O
is	O
the	O
variables	O
are	O
independent	O
.	O
this	O
suggests	O
that	O
in	O
general	O
gibbs	O
sampling	B
will	O
be	O
less	O
eﬀective	O
when	O
variables	O
are	O
strongly	O
correlated	O
.	O
for	O
example	O
,	O
if	O
we	O
consider	O
gibbs	O
sampling	B
from	O
a	O
strongly	O
correlated	O
two	O
variable	B
gaussian	O
distribution	B
,	O
then	O
updates	O
will	O
move	O
very	O
slowly	O
in	O
space	O
,	O
ﬁg	O
(	O
27.7	O
)	O
.	O
1the	O
bugs	O
package	O
www.mrc-bsu.cam.ac.uk/bugs	O
is	O
general	O
purpose	O
software	O
for	O
sampling	B
from	O
belief	B
networks	I
.	O
498	O
draft	O
march	O
9	O
,	O
2010	O
−4−3−2−101234−3−2−10123−3−2−10123−2−1.5−1−0.500.511.52	O
markov	O
chain	B
monte	O
carlo	O
(	O
mcmc	O
)	O
27.4	O
markov	O
chain	B
monte	O
carlo	O
(	O
mcmc	O
)	O
we	O
assume	O
we	O
have	O
a	O
multivariate	B
distribution	O
in	O
the	O
form	O
p	O
(	O
x	O
)	O
=	O
∗	O
(	O
x	O
)	O
p	O
1	O
z	O
assume	O
we	O
are	O
able	O
to	O
evaluate	O
p∗	O
(	O
x	O
=	O
x	O
)	O
,	O
for	O
any	O
state	O
x	O
,	O
but	O
not	O
z	O
,	O
since	O
z	O
=	O
(	O
cid:82	O
)	O
where	O
z	O
is	O
the	O
normalisation	B
constant	I
of	O
the	O
distribution	B
and	O
p∗	O
(	O
x	O
)	O
is	O
the	O
unnormalised	O
distribution	B
.	O
we	O
x	O
p∗	O
(	O
x	O
)	O
is	O
an	O
intractable	O
high	O
dimensional	O
summation/integration	O
.	O
(	O
27.4.1	O
)	O
the	O
idea	O
in	O
mcmc	O
sampling	B
is	O
to	O
sample	B
,	O
not	O
directly	O
from	O
p	O
(	O
x	O
)	O
,	O
but	O
from	O
a	O
diﬀerent	O
distribution	B
such	O
that	O
,	O
in	O
the	O
limit	O
of	O
a	O
large	O
number	O
of	O
samples	O
,	O
eﬀectively	O
the	O
samples	O
will	O
be	O
from	O
p	O
(	O
x	O
)	O
.	O
to	O
achieve	O
this	O
we	O
forward	O
sample	O
from	O
a	O
markov	O
transition	O
whose	O
stationary	B
distribution	I
is	O
equal	O
to	O
p	O
(	O
x	O
)	O
.	O
27.4.1	O
markov	O
chains	O
consider	O
the	O
conditional	B
distribution	O
q	O
(	O
xl+1|xl	O
)	O
.	O
if	O
we	O
are	O
given	O
an	O
initial	O
sample	B
x1	O
,	O
then	O
we	O
can	O
re-	O
cursively	O
generate	O
samples	O
x1	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
xl	O
.	O
after	O
a	O
long	O
time	O
l	O
(	O
cid:29	O
)	O
1	O
,	O
(	O
and	O
provided	O
the	O
markov	O
chain	B
is	O
‘	O
irreducible	O
’	O
,	O
meaning	O
that	O
we	O
can	O
eventually	O
get	O
from	O
any	O
state	O
to	O
any	O
other	O
state	O
)	O
the	O
samples	O
are	O
from	O
the	O
stationary	B
distribution	I
q∞	O
(	O
x	O
)	O
which	O
is	O
deﬁned	O
as	O
(	O
for	O
a	O
continuous	B
variable	O
)	O
(	O
cid:90	O
)	O
x	O
q∞	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
(	O
cid:48	O
)	O
q	O
(	O
x	O
|x	O
)	O
q∞	O
(	O
x	O
)	O
(	O
27.4.2	O
)	O
the	O
condition	O
for	O
a	O
discrete	B
variable	O
is	O
analogous	O
on	O
replacing	O
integration	O
with	O
summation	O
.	O
the	O
idea	O
in	O
mcmc	O
is	O
,	O
for	O
a	O
given	O
distribution	B
p	O
(	O
x	O
)	O
,	O
to	O
ﬁnd	O
a	O
transition	O
q	O
(	O
x	O
(	O
cid:48	O
)	O
|x	O
)	O
which	O
has	O
p	O
(	O
x	O
)	O
as	O
its	O
stationary	B
distribution	I
.	O
if	O
we	O
can	O
do	O
so	O
,	O
then	O
we	O
can	O
draw	O
samples	O
from	O
the	O
markov	O
chain	B
by	O
forward	B
sampling	I
and	O
take	O
these	O
as	O
samples	O
from	O
p	O
(	O
x	O
)	O
.	O
note	O
that	O
for	O
every	O
distribution	B
p	O
(	O
x	O
)	O
there	O
will	O
be	O
more	O
than	O
one	O
transition	O
q	O
(	O
x	O
(	O
cid:48	O
)	O
|x	O
)	O
with	O
p	O
(	O
x	O
)	O
as	O
its	O
stationary	B
distribution	I
.	O
this	O
is	O
why	O
there	O
are	O
very	O
many	O
diﬀerent	O
mcmc	O
sampling	B
methods	O
,	O
each	O
with	O
diﬀerent	O
characteristics	O
and	O
varying	O
suitability	O
for	O
the	O
particular	O
distribution	B
at	O
hand	O
.	O
detailed	B
balance	I
how	O
do	O
we	O
construct	O
a	O
transition	O
q	O
(	O
x	O
(	O
cid:48	O
)	O
|x	O
)	O
with	O
given	O
p	O
(	O
x	O
)	O
as	O
its	O
stationary	B
distribution	I
?	O
this	O
problem	B
can	O
be	O
simpliﬁed	O
if	O
we	O
consider	O
special	O
transitions	O
that	O
satisfy	O
the	O
detailed	B
balance	I
condition	O
.	O
if	O
we	O
are	O
given	O
the	O
marginal	B
distribution	O
p	O
(	O
x	O
)	O
,	O
the	O
detailed	B
balance	I
condition	O
for	O
a	O
transition	O
q	O
is	O
under	O
this	O
we	O
see	O
q	O
(	O
x	O
(	O
cid:48	O
)	O
|x	O
)	O
q	O
(	O
x|x	O
(	O
cid:48	O
)	O
)	O
(	O
cid:90	O
)	O
(	O
cid:48	O
)	O
q	O
(	O
x	O
x	O
=	O
p	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
p	O
(	O
x	O
)	O
,	O
|x	O
)	O
p	O
(	O
x	O
)	O
=	O
(	O
cid:48	O
)	O
∀x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
p	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
=	O
p	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
q	O
(	O
x|x	O
(	O
cid:90	O
)	O
x	O
(	O
27.4.3	O
)	O
(	O
27.4.4	O
)	O
so	O
that	O
p	O
(	O
x	O
)	O
is	O
the	O
stationary	B
distribution	I
of	O
q	O
(	O
x	O
(	O
cid:48	O
)	O
|x	O
)	O
.	O
the	O
detailed	B
balance	I
requirement	O
can	O
make	O
the	O
process	O
of	O
constructing	O
a	O
suitable	O
transition	O
easier	O
since	O
only	O
the	O
relative	O
value	B
of	O
p	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
to	O
p	O
(	O
x	O
)	O
is	O
required	O
in	O
equation	B
(	O
27.4.3	O
)	O
,	O
and	O
not	O
the	O
absolute	O
value	B
of	O
p	O
(	O
x	O
)	O
or	O
p	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
.	O
27.4.2	O
metropolis-hastings	O
sampling	B
consider	O
the	O
following	O
transition	O
(	O
cid:48	O
)	O
q	O
(	O
x	O
|x	O
)	O
=	O
˜q	O
(	O
x	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
|x	O
)	O
f	O
(	O
x	O
,	O
x	O
)	O
+	O
δ	O
(	O
x	O
(	O
cid:48	O
)	O
,	O
x	O
)	O
(	O
cid:18	O
)	O
1	O
−	O
(	O
cid:90	O
)	O
x	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
(	O
cid:19	O
)	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
˜q	O
(	O
x	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
,	O
x	O
)	O
|x	O
)	O
f	O
(	O
x	O
draft	O
march	O
9	O
,	O
2010	O
(	O
27.4.5	O
)	O
499	O
algorithm	B
25	O
metropolis-hastings	O
mcmc	O
sampling	B
.	O
markov	O
chain	B
monte	O
carlo	O
(	O
mcmc	O
)	O
1	O
:	O
choose	O
a	O
starting	O
point	O
x1	O
.	O
2	O
:	O
for	O
i	O
=	O
2	O
to	O
l	O
do	O
3	O
:	O
draw	O
a	O
candidate	O
sample	B
xcand	O
from	O
the	O
proposal	O
˜q	O
(	O
x	O
(	O
cid:48	O
)	O
let	O
a	O
=	O
˜q	O
(	O
xl−1|xcand	O
)	O
p	O
(	O
xcand	O
)	O
˜q	O
(	O
xcand|xl−1	O
)	O
p	O
(	O
xl−1	O
)	O
if	O
a	O
≥	O
1	O
then	O
xl	O
=	O
xcand	O
else	O
|xl−1	O
)	O
.	O
draw	O
a	O
random	O
value	O
u	O
uniformly	O
from	O
the	O
unit	O
interval	O
[	O
0	O
,	O
1	O
]	O
.	O
if	O
u	O
<	O
a	O
then	O
xl	O
=	O
xcand	O
else	O
4	O
:	O
5	O
:	O
6	O
:	O
7	O
:	O
8	O
:	O
9	O
:	O
10	O
:	O
11	O
:	O
12	O
:	O
13	O
:	O
end	O
for	O
xl	O
=	O
xl−1	O
end	O
if	O
end	O
if	O
(	O
cid:46	O
)	O
accept	O
the	O
candidate	O
(	O
cid:46	O
)	O
accept	O
the	O
candidate	O
(	O
cid:46	O
)	O
reject	O
the	O
candidate	O
where	O
˜q	O
(	O
x	O
(	O
cid:48	O
)	O
valid	O
distribution	B
q	O
(	O
x	O
(	O
cid:48	O
)	O
|x	O
)	O
is	O
a	O
so-called	O
proposal	B
distribution	I
and	O
0	O
<	O
f	O
(	O
x	O
(	O
cid:48	O
)	O
,	O
x	O
)	O
≤	O
1	O
a	O
positive	O
function	O
.	O
this	O
deﬁnes	O
a	O
(	O
cid:90	O
)	O
|x	O
)	O
since	O
it	O
is	O
non-negative	O
and	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
˜q	O
(	O
x	O
˜q	O
(	O
x	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
|x	O
)	O
f	O
(	O
x	O
,	O
x	O
)	O
+	O
1	O
−	O
x	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
|x	O
)	O
f	O
(	O
x	O
(	O
cid:90	O
)	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
,	O
x	O
)	O
=	O
1	O
(	O
27.4.6	O
)	O
our	O
interest	O
is	O
to	O
set	O
f	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
such	O
that	O
the	O
stationary	B
distribution	I
of	O
q	O
(	O
x	O
(	O
cid:48	O
)	O
proposal	O
˜q	O
(	O
x	O
(	O
cid:48	O
)	O
|x	O
)	O
is	O
equal	O
to	O
p	O
(	O
x	O
)	O
for	O
any	O
(	O
cid:48	O
)	O
q	O
(	O
x	O
x	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
)	O
=	O
p	O
(	O
x	O
x	O
(	O
cid:48	O
)	O
|x	O
)	O
=	O
(	O
cid:90	O
)	O
|x	O
)	O
.	O
that	O
is	O
(	O
cid:90	O
)	O
q	O
(	O
x	O
x	O
(	O
cid:48	O
)	O
|x	O
)	O
p	O
(	O
x	O
)	O
(	O
cid:48	O
)	O
|x	O
)	O
f	O
(	O
x	O
(	O
cid:48	O
)	O
=	O
˜q	O
(	O
x	O
x	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
(	O
cid:18	O
)	O
(	O
cid:48	O
)	O
)	O
1	O
−	O
(	O
cid:90	O
)	O
x	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
˜q	O
(	O
x	O
|x	O
(	O
cid:48	O
)	O
)	O
f	O
(	O
x	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
(	O
cid:19	O
)	O
(	O
cid:48	O
)	O
)	O
,	O
x	O
,	O
x	O
)	O
p	O
(	O
x	O
)	O
+	O
p	O
(	O
x	O
in	O
order	O
that	O
this	O
holds	O
,	O
we	O
require	O
(	O
changing	O
the	O
integral	O
variable	B
from	O
x	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
to	O
x	O
)	O
now	O
consider	O
the	O
metropolis-hastings	O
acceptance	B
function	I
(	O
cid:90	O
)	O
x	O
(	O
cid:48	O
)	O
˜q	O
(	O
x	O
(	O
cid:48	O
)	O
|x	O
)	O
f	O
(	O
x	O
x	O
(	O
cid:48	O
)	O
f	O
(	O
x	O
,	O
x	O
)	O
=	O
min	O
(	O
cid:48	O
)	O
f	O
(	O
x	O
,	O
x	O
)	O
˜q	O
(	O
x	O
(	O
cid:48	O
)	O
,	O
x	O
)	O
p	O
(	O
x	O
)	O
=	O
˜q	O
(	O
x|x	O
(	O
cid:19	O
)	O
1	O
,	O
˜q	O
(	O
x|x	O
(	O
cid:48	O
)	O
)	O
p	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
˜q	O
(	O
x	O
(	O
cid:48	O
)	O
|x	O
)	O
p	O
(	O
x	O
)	O
(	O
cid:18	O
)	O
|x	O
)	O
p	O
(	O
x	O
)	O
=	O
min	O
(	O
cid:0	O
)	O
˜q	O
(	O
x	O
=	O
min	O
(	O
cid:0	O
)	O
˜q	O
(	O
x|x	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
)	O
f	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
p	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
(	O
cid:18	O
)	O
=	O
min	O
1	O
,	O
|x	O
)	O
p	O
(	O
x	O
)	O
,	O
˜q	O
(	O
x|x	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
)	O
p	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
,	O
˜q	O
(	O
x	O
(	O
cid:19	O
)	O
˜q	O
(	O
x|x	O
(	O
cid:48	O
)	O
)	O
p∗	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
|x	O
)	O
p∗	O
(	O
x	O
)	O
˜q	O
(	O
x	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
)	O
(	O
cid:1	O
)	O
|x	O
)	O
p	O
(	O
x	O
)	O
(	O
cid:1	O
)	O
=	O
f	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
p	O
(	O
x	O
which	O
is	O
deﬁned	O
for	O
all	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
and	O
has	O
the	O
detailed	B
balance	I
property	O
(	O
cid:48	O
)	O
)	O
˜q	O
(	O
x|x	O
(	O
cid:48	O
)	O
)	O
p	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
(	O
27.4.7	O
)	O
(	O
27.4.8	O
)	O
(	O
27.4.9	O
)	O
(	O
27.4.10	O
)	O
(	O
27.4.11	O
)	O
(	O
27.4.12	O
)	O
hence	O
the	O
function	B
f	O
(	O
x	O
(	O
cid:48	O
)	O
,	O
x	O
)	O
as	O
deﬁned	O
above	O
ensures	O
equation	B
(	O
27.4.9	O
)	O
holds	O
and	O
that	O
q	O
(	O
x	O
(	O
cid:48	O
)	O
its	O
stationary	B
distribution	I
.	O
how	O
do	O
we	O
sample	B
from	O
q	O
(	O
x	O
(	O
cid:48	O
)	O
|x	O
)	O
?	O
equation	B
(	O
27.4.5	O
)	O
can	O
be	O
interpreted	O
as	O
a	O
mixture	O
of	O
two	O
distributions	O
,	O
|x	O
)	O
f	O
(	O
x	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
,	O
x	O
)	O
.	O
one	O
proportional	O
to	O
˜q	O
(	O
x	O
(	O
cid:48	O
)	O
to	O
draw	O
a	O
sample	B
from	O
this	O
,	O
we	O
draw	O
a	O
sample	B
from	O
˜q	O
(	O
x	O
(	O
cid:48	O
)	O
|x	O
)	O
and	O
accept	O
this	O
with	O
probability	O
f	O
(	O
x	O
(	O
cid:48	O
)	O
,	O
x	O
)	O
.	O
since	O
drawing	O
from	O
˜q	O
(	O
x	O
(	O
cid:48	O
)	O
|x	O
)	O
and	O
accepting	O
are	O
performed	O
independently	O
,	O
the	O
probability	B
of	O
accepting	O
the	O
drawn	O
candidate	O
is	O
the	O
product	O
of	O
these	O
probabilities	O
,	O
namely	O
˜q	O
(	O
x	O
(	O
cid:48	O
)	O
|x	O
)	O
f	O
(	O
x	O
(	O
cid:48	O
)	O
,	O
x	O
)	O
.	O
otherwise	O
the	O
candidate	O
is	O
rejected	O
and	O
we	O
take	O
the	O
sample	B
x	O
(	O
cid:48	O
)	O
=	O
x.	O
using	O
the	O
properties	B
of	O
the	O
acceptance	B
function	I
,	O
equation	B
(	O
27.4.10	O
)	O
,	O
the	O
following	O
is	O
equivalent	B
to	O
deciding	O
on	O
accepting/rejecting	O
the	O
candidate	O
.	O
if	O
|x	O
)	O
f	O
(	O
x	O
(	O
cid:48	O
)	O
,	O
x	O
)	O
and	O
the	O
other	O
δ	O
(	O
x	O
(	O
cid:48	O
)	O
,	O
x	O
)	O
with	O
mixture	O
coeﬃcient	O
1	O
−	O
(	O
cid:82	O
)	O
x	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
˜q	O
(	O
x	O
(	O
cid:48	O
)	O
(	O
cid:48	O
)	O
|x	O
)	O
has	O
p	O
(	O
x	O
)	O
as	O
˜q	O
(	O
x|x	O
(	O
cid:48	O
)	O
)	O
p	O
∗	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
>	O
˜q	O
(	O
x	O
(	O
cid:48	O
)	O
∗	O
(	O
x	O
)	O
|x	O
)	O
p	O
500	O
(	O
27.4.13	O
)	O
draft	O
march	O
9	O
,	O
2010	O
auxiliary	B
variable	I
methods	O
figure	O
27.8	O
:	O
metropolis-hastings	O
samples	O
from	O
a	O
bi-	O
variate	O
distribution	B
p	O
(	O
x1	O
,	O
x2	O
)	O
using	O
a	O
proposal	O
˜q	O
(	O
x	O
(	O
cid:48	O
)	O
|x	O
)	O
=	O
n	O
(	O
x	O
(	O
cid:48	O
)	O
x	O
,	O
i	O
)	O
.	O
we	O
also	O
plot	O
the	O
iso-probability	O
contours	O
of	O
p.	O
although	O
p	O
(	O
x	O
)	O
is	O
multimodal	O
,	O
the	O
dimensionality	O
is	O
low	O
enough	O
and	O
the	O
modes	O
suﬃciently	O
close	O
such	O
that	O
a	O
simple	O
gaussian	O
proposal	B
distribution	I
is	O
able	O
to	O
bridge	O
the	O
two	O
modes	O
.	O
in	O
higher	O
dimensions	O
,	O
such	O
multi-modality	O
is	O
more	O
problematic	O
.	O
see	O
demometropolis.m	O
|x	O
)	O
p∗	O
(	O
x	O
)	O
.	O
if	O
we	O
reject	O
the	O
candidate	O
we	O
take	O
x	O
(	O
cid:48	O
)	O
=	O
x	O
.	O
|x	O
)	O
.	O
otherwise	O
we	O
accept	O
the	O
sample	B
x	O
(	O
cid:48	O
)	O
from	O
q	O
(	O
x	O
(	O
cid:48	O
)	O
we	O
accept	O
the	O
sample	B
from	O
˜q	O
(	O
x	O
(	O
cid:48	O
)	O
˜q	O
(	O
x|x	O
(	O
cid:48	O
)	O
)	O
p∗	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
/˜q	O
(	O
x	O
(	O
cid:48	O
)	O
note	O
that	O
if	O
the	O
candidate	O
x	O
(	O
cid:48	O
)	O
is	O
rejected	O
,	O
we	O
take	O
the	O
original	O
x	O
as	O
the	O
new	O
sample	B
.	O
hence	O
at	O
each	O
iteration	B
of	O
the	O
algorithm	B
produces	O
a	O
sample	B
–	O
either	O
a	O
copy	O
of	O
the	O
current	O
sample	B
,	O
or	O
the	O
candidate	O
sample	B
.	O
a	O
rough	O
rule	O
of	O
thumb	O
is	O
to	O
choose	O
a	O
proposal	B
distribution	I
for	O
which	O
the	O
acceptance	O
rate	O
is	O
between	O
50	O
%	O
and	O
85	O
%	O
[	O
105	O
]	O
.	O
|x	O
)	O
with	O
probability	O
gaussian	O
proposal	B
distribution	I
a	O
common	O
proposal	B
distribution	I
for	O
multivariate	B
x	O
(	O
writing	O
explicitly	O
as	O
a	O
vector	O
)	O
is	O
(	O
27.4.14	O
)	O
(	O
27.4.15	O
)	O
˜q	O
(	O
x	O
(	O
cid:48	O
)	O
|x	O
)	O
=	O
n	O
for	O
which	O
˜q	O
(	O
x	O
(	O
cid:48	O
)	O
(	O
cid:0	O
)	O
x	O
(	O
cid:48	O
)	O
x	O
,	O
σ2i	O
(	O
cid:1	O
)	O
(	O
cid:18	O
)	O
|x	O
)	O
=	O
˜q	O
(	O
x|x	O
(	O
cid:48	O
)	O
)	O
and	O
the	O
acceptance	O
criterion	O
becomes	O
−	O
1	O
2σ2	O
(	O
x	O
(	O
cid:48	O
)	O
−x	O
)	O
2	O
(	O
cid:19	O
)	O
∝	O
e	O
f	O
(	O
x	O
(	O
cid:48	O
)	O
,	O
x	O
)	O
=	O
min	O
1	O
,	O
p∗	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
p∗	O
(	O
x	O
)	O
if	O
the	O
unnormalised	O
probability	B
of	O
the	O
candidate	O
state	O
is	O
higher	O
than	O
the	O
current	O
state	O
,	O
we	O
therefore	O
accept	O
the	O
candidate	O
.	O
otherwise	O
,	O
if	O
the	O
unnormalised	O
probability	B
of	O
the	O
candidate	O
state	O
is	O
lower	O
than	O
the	O
current	O
state	O
,	O
we	O
accept	O
the	O
candidate	O
only	O
with	O
probability	O
p∗	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
/p∗	O
(	O
x	O
)	O
.	O
if	O
the	O
candidate	O
is	O
rejected	O
,	O
the	O
new	O
sample	B
is	O
taken	O
to	O
be	O
a	O
copy	O
of	O
the	O
previous	O
sample	B
x.	O
see	O
ﬁg	O
(	O
27.8	O
)	O
for	O
a	O
demonstration	O
.	O
in	O
high	O
dimensions	O
it	O
is	O
unlikely	O
that	O
a	O
random	O
candidate	O
sampled	O
from	O
a	O
gaussian	O
will	O
result	O
in	O
a	O
candi-	O
date	O
probability	B
higher	O
than	O
the	O
current	O
value	B
,	O
exercise	O
(	O
249	O
)	O
.	O
because	O
of	O
this	O
,	O
only	O
very	O
small	O
jumps	O
(	O
σ2	O
small	O
)	O
are	O
likely	O
to	O
be	O
accepted	O
.	O
this	O
limits	O
the	O
speed	O
at	O
which	O
we	O
explore	O
the	O
space	O
x.	O
this	O
acceptance	B
function	I
above	O
highlights	O
that	O
sampling	B
is	O
diﬀerent	O
from	O
ﬁnding	O
the	O
optimum	O
.	O
provided	O
x	O
(	O
cid:48	O
)	O
has	O
a	O
higher	O
probability	B
than	O
x	O
,	O
we	O
accept	O
x	O
(	O
cid:48	O
)	O
.	O
however	O
,	O
we	O
also	O
accept	O
(	O
with	O
a	O
speciﬁed	O
acceptance	O
probability	O
)	O
candidates	O
that	O
have	O
also	O
a	O
lower	O
probability	O
than	O
the	O
current	O
sample	B
.	O
27.5	O
auxiliary	B
variable	I
methods	O
a	O
practical	O
concern	O
in	O
mcmc	O
methods	O
is	O
ensuring	O
that	O
one	O
moves	O
eﬀectively	O
through	O
the	O
signiﬁcant	O
probability	B
regions	O
of	O
the	O
distribution	B
.	O
for	O
methods	O
such	O
as	O
metropolis-hastings	O
with	O
local	O
proposal	O
dis-	O
tributions	O
(	O
local	B
in	O
the	O
sense	O
they	O
are	O
unlikely	O
to	O
propose	O
a	O
candidate	O
far	O
from	O
the	O
current	O
sample	B
)	O
,	O
if	O
the	O
target	O
distribution	B
has	O
isolated	O
islands	O
of	O
high	O
density	O
,	O
then	O
the	O
likelihood	B
that	O
we	O
would	O
be	O
able	O
to	O
move	O
from	O
one	O
island	O
to	O
the	O
other	O
is	O
very	O
small	O
.	O
if	O
we	O
attempt	O
to	O
make	O
the	O
proposal	O
less	O
local	B
by	O
using	O
one	O
with	O
a	O
high	O
variance	O
the	O
chance	O
then	O
of	O
landing	O
at	O
random	O
on	O
a	O
high	O
density	O
island	O
is	O
remote	O
.	O
auxiliary	B
variable	I
methods	O
use	O
additional	O
dimensions	O
to	O
exploration	O
and	O
in	O
certain	O
cases	O
to	O
provide	O
a	O
bridge	O
between	O
draft	O
march	O
9	O
,	O
2010	O
501	O
auxiliary	B
variable	I
methods	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
27.9	O
:	O
hybrid	O
monte	O
carlo	O
.	O
(	O
a	O
)	O
:	O
multi-modal	O
distribution	B
p	O
(	O
x	O
)	O
for	O
which	O
we	O
desire	O
samples	O
.	O
(	O
b	O
)	O
:	O
hmc	O
forms	O
the	O
joint	B
distibution	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
where	O
p	O
(	O
y	O
)	O
is	O
gaussian	O
.	O
(	O
c	O
)	O
:	O
starting	O
from	O
the	O
point	O
x	O
,	O
we	O
ﬁrst	O
draw	O
a	O
y	O
from	O
the	O
gaussian	O
p	O
(	O
y	O
)	O
,	O
giving	O
a	O
point	O
(	O
x	O
,	O
y	O
)	O
,	O
green	O
line	O
.	O
then	O
we	O
use	O
hamiltonian	O
dynamics	O
(	O
white	O
line	O
)	O
to	O
traverse	O
the	O
distribution	B
at	O
roughly	O
constant	O
energy	B
for	O
a	O
ﬁxed	O
number	O
of	O
steps	O
,	O
giving	O
x	O
(	O
cid:48	O
)	O
,	O
y	O
(	O
cid:48	O
)	O
.	O
we	O
accept	O
this	O
point	O
if	O
h	O
(	O
x	O
(	O
cid:48	O
)	O
,	O
y	O
(	O
cid:48	O
)	O
)	O
>	O
h	O
(	O
x	O
,	O
y	O
(	O
cid:48	O
)	O
)	O
and	O
make	O
the	O
new	O
sample	B
x	O
(	O
cid:48	O
)	O
(	O
red	O
line	O
)	O
.	O
otherwise	O
this	O
candidate	O
is	O
accepted	O
with	O
probability	O
exp	O
(	O
h	O
(	O
x	O
(	O
cid:48	O
)	O
,	O
y	O
(	O
cid:48	O
)	O
)	O
−	O
h	O
(	O
x	O
,	O
y	O
(	O
cid:48	O
)	O
)	O
)	O
.	O
if	O
rejected	O
the	O
new	O
sample	B
x	O
(	O
cid:48	O
)	O
is	O
taken	O
as	O
a	O
copy	O
of	O
x.	O
isolated	O
high	O
density	O
islands	O
.	O
consider	O
drawing	O
samples	O
from	O
p	O
(	O
x	O
)	O
where	O
x	O
is	O
a	O
high-dimensional	O
vector	O
.	O
for	O
an	O
auxiliary	B
variable	I
y	O
we	O
introduce	O
a	O
distribution	B
p	O
(	O
y|x	O
)	O
,	O
to	O
form	O
the	O
joint	B
distribution	O
p	O
(	O
x	O
,	O
y	O
)	O
=	O
p	O
(	O
y|x	O
)	O
p	O
(	O
x	O
)	O
(	O
27.5.1	O
)	O
if	O
we	O
draw	O
samples	O
(	O
xl	O
,	O
yl	O
)	O
from	O
this	O
joint	B
distribution	O
then	O
a	O
valid	O
set	O
of	O
samples	O
from	O
p	O
(	O
x	O
)	O
is	O
given	O
by	O
taking	O
the	O
xl	O
alone	O
.	O
if	O
we	O
sampled	O
x	O
directly	O
from	O
p	O
(	O
x	O
)	O
and	O
then	O
y	O
from	O
p	O
(	O
y|x	O
)	O
,	O
introducing	O
y	O
is	O
pointless	O
since	O
there	O
is	O
no	O
eﬀect	O
on	O
the	O
x	O
sampling	B
procedure	O
.	O
in	O
order	O
for	O
this	O
to	O
be	O
useful	O
,	O
therefore	O
,	O
the	O
auxiliary	B
variable	I
must	O
inﬂuence	O
how	O
we	O
sample	B
x.	O
below	O
we	O
discuss	O
some	O
of	O
the	O
common	O
auxiliary	B
variable	I
schemes	O
.	O
27.5.1	O
hybrid	O
monte	O
carlo	O
hybrid	O
mc	O
is	O
a	O
method	O
for	O
continuous	B
variables	O
that	O
aims	O
to	O
make	O
non-local	O
jumps	O
in	O
the	O
samples	O
and	O
,	O
in	O
so	O
doing	O
,	O
to	O
jump	O
potentially	O
from	O
one	O
mode	B
to	O
another	O
.	O
we	O
deﬁne	O
the	O
distribution	B
from	O
which	O
we	O
wish	O
to	O
sample	B
as	O
p	O
(	O
x	O
)	O
=	O
1	O
zx	O
ehx	O
(	O
x	O
)	O
(	O
27.5.2	O
)	O
for	O
some	O
given	O
‘	O
hamiltonian	O
’	O
hx	O
(	O
x	O
)	O
(	O
this	O
is	O
just	O
a	O
potential	B
)	O
.	O
we	O
then	O
deﬁne	O
another	O
,	O
‘	O
easy	O
’	O
distribution	B
from	O
which	O
we	O
can	O
readily	O
generate	O
samples	O
,	O
p	O
(	O
y	O
)	O
=	O
1	O
zy	O
ehy	O
(	O
y	O
)	O
so	O
that	O
the	O
joint	B
distribution	O
is	O
given	O
by	O
p	O
(	O
x	O
,	O
y	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
=	O
1	O
z	O
ehx	O
(	O
x	O
)	O
+hy	O
(	O
y	O
)	O
=	O
1	O
z	O
eh	O
(	O
x	O
,	O
y	O
)	O
(	O
27.5.3	O
)	O
(	O
27.5.4	O
)	O
in	O
the	O
standard	O
form	O
of	O
the	O
algorithm	B
,	O
a	O
multi-dimensional	O
gaussian	O
is	O
chosen	O
for	O
the	O
auxiliary	O
distribution	O
with	O
dim	O
y	O
=	O
dim	O
x	O
,	O
so	O
that	O
hy	O
(	O
y	O
)	O
=	O
−	O
1	O
2	O
yty	O
(	O
27.5.5	O
)	O
the	O
hmc	O
algorithm	B
ﬁrst	O
draws	O
from	O
p	O
(	O
y	O
)	O
and	O
subsequently	O
from	O
p	O
(	O
x	O
,	O
y	O
)	O
.	O
for	O
a	O
gaussian	O
p	O
(	O
y	O
)	O
,	O
sampling	B
from	O
is	O
straightforward	O
.	O
in	O
the	O
next	O
,	O
‘	O
dynamic	B
’	O
step	O
,	O
a	O
sample	B
is	O
drawn	O
from	O
p	O
(	O
x	O
,	O
y	O
)	O
using	O
a	O
metropolis	O
502	O
draft	O
march	O
9	O
,	O
2010	O
−5−4−3−2−101234500.511.522.53x	O
10−3	O
auxiliary	B
variable	I
methods	O
mcmc	O
sampler	O
.	O
the	O
idea	O
is	O
to	O
go	O
from	O
one	O
point	O
of	O
the	O
space	O
x	O
,	O
y	O
to	O
a	O
new	O
point	O
x	O
(	O
cid:48	O
)	O
,	O
y	O
(	O
cid:48	O
)	O
that	O
is	O
a	O
non-trivial	O
distance	O
from	O
x	O
,	O
y	O
and	O
which	O
will	O
be	O
accepted	O
with	O
a	O
high	O
probability	O
.	O
the	O
candidate	O
(	O
x	O
(	O
cid:48	O
)	O
,	O
y	O
(	O
cid:48	O
)	O
)	O
will	O
have	O
a	O
good	O
chance	O
to	O
be	O
accepted	O
if	O
h	O
(	O
x	O
(	O
cid:48	O
)	O
,	O
y	O
(	O
cid:48	O
)	O
)	O
is	O
close	O
to	O
h	O
(	O
x	O
,	O
y	O
)	O
–	O
this	O
can	O
be	O
achieved	O
by	O
following	O
a	O
contour	O
of	O
equal	O
‘	O
energy	B
’	O
h	O
,	O
as	O
described	O
in	O
the	O
next	O
section	O
.	O
hamiltonian	O
dynamics	O
we	O
wish	O
to	O
make	O
an	O
update	O
x	O
(	O
cid:48	O
)	O
=	O
x	O
+	O
∆x	O
,	O
y	O
(	O
cid:48	O
)	O
=	O
y	O
+	O
∆y	O
for	O
small	O
∆x	O
and	O
∆y	O
such	O
that	O
the	O
hamiltonian	O
h	O
(	O
x	O
,	O
y	O
)	O
≡	O
hx	O
(	O
x	O
)	O
+	O
hy	O
(	O
y	O
)	O
is	O
conserved	O
,	O
we	O
can	O
satisfy	O
this	O
(	O
up	O
to	O
ﬁrst	B
order	I
)	O
by	O
considering	O
the	O
taylor	O
expansion	O
h	O
(	O
x	O
(	O
cid:48	O
)	O
,	O
y	O
(	O
cid:48	O
)	O
)	O
=	O
h	O
(	O
x	O
,	O
y	O
)	O
h	O
(	O
x	O
(	O
cid:48	O
)	O
,	O
y	O
(	O
cid:48	O
)	O
)	O
=	O
h	O
(	O
x	O
+	O
∆x	O
,	O
y	O
+	O
∆y	O
)	O
≈	O
h	O
(	O
x	O
)	O
+	O
∆xt∇xh	O
(	O
x	O
,	O
y	O
)	O
+	O
h	O
(	O
y	O
)	O
+	O
∆yt∇yh	O
(	O
x	O
,	O
y	O
)	O
+	O
o	O
(	O
cid:0	O
)	O
|∆x|2	O
(	O
cid:1	O
)	O
+	O
o	O
(	O
cid:0	O
)	O
conservation	O
,	O
up	O
to	O
ﬁrst	B
order	I
,	O
therefore	O
requires	O
∆xt∇xh	O
(	O
x	O
,	O
y	O
)	O
+	O
∆yt∇yh	O
(	O
x	O
,	O
y	O
)	O
=	O
0	O
|∆y|2	O
(	O
cid:1	O
)	O
(	O
27.5.6	O
)	O
(	O
27.5.7	O
)	O
(	O
27.5.8	O
)	O
this	O
is	O
a	O
single	O
scalar	O
requirement	O
,	O
and	O
there	O
are	O
therefore	O
many	O
diﬀerent	O
solutions	O
for	O
∆x	O
and	O
∆y	O
that	O
satisfy	O
this	O
single	O
condition	O
.	O
it	O
is	O
customary	O
to	O
use	O
hamiltonian	O
dynamics	O
,	O
which	O
correspond	O
to	O
the	O
setting	O
:	O
∆x	O
=	O
∇yh	O
(	O
x	O
,	O
y	O
)	O
∆y	O
=	O
−∇xh	O
(	O
x	O
,	O
y	O
)	O
where	O
	O
is	O
a	O
small	O
value	B
to	O
ensure	O
that	O
the	O
taylor	O
expansion	O
is	O
accurate	O
.	O
hence	O
x	O
(	O
t	O
+	O
1	O
)	O
=	O
x	O
(	O
t	O
)	O
+	O
∇yhy	O
(	O
y	O
)	O
y	O
(	O
t	O
+	O
1	O
)	O
=	O
y	O
(	O
t	O
)	O
−	O
∇xhx	O
(	O
x	O
)	O
(	O
27.5.9	O
)	O
(	O
27.5.10	O
)	O
for	O
the	O
hmc	O
method	O
,	O
h	O
(	O
x	O
,	O
y	O
)	O
=	O
hx	O
(	O
x	O
)	O
+	O
hy	O
(	O
y	O
)	O
,	O
so	O
that	O
∇xh	O
(	O
x	O
,	O
y	O
)	O
=	O
∇xhx	O
(	O
x	O
)	O
and	O
∇yh	O
(	O
x	O
,	O
y	O
)	O
=	O
∇yhy	O
(	O
y	O
)	O
for	O
the	O
gaussian	O
case	O
,	O
∇yhy	O
(	O
y	O
)	O
=	O
−y	O
so	O
that	O
y	O
(	O
t	O
+	O
1	O
)	O
=	O
y	O
(	O
t	O
)	O
−	O
∇xh	O
(	O
x	O
)	O
x	O
(	O
t	O
+	O
1	O
)	O
=	O
x	O
(	O
t	O
)	O
−	O
y	O
(	O
27.5.11	O
)	O
there	O
are	O
speciﬁc	O
ways	O
to	O
implement	O
the	O
hamiltonian	O
dynamics	O
called	O
leapfrog	O
discretisation	O
that	O
are	O
more	O
accurate	O
than	O
the	O
simple	O
time-discretisation	O
used	O
above	O
,	O
and	O
we	O
refer	O
the	O
reader	O
to	O
[	O
205	O
]	O
for	O
details	O
.	O
in	O
order	O
to	O
make	O
a	O
symmetric	O
proposal	B
distribution	I
,	O
at	O
the	O
start	O
of	O
the	O
dynamic	B
step	O
,	O
we	O
choose	O
	O
=	O
+0	O
or	O
	O
=	O
−0	O
uniformly	O
.	O
this	O
means	O
that	O
there	O
is	O
the	O
same	O
chance	O
that	O
we	O
go	O
back	O
to	O
the	O
point	O
x	O
,	O
y	O
starting	O
from	O
x	O
(	O
cid:48	O
)	O
,	O
y	O
(	O
cid:48	O
)	O
,	O
as	O
vice	O
versa	O
.	O
we	O
then	O
follow	O
the	O
hamiltonian	O
dynamics	O
for	O
many	O
time	O
steps	O
(	O
usually	O
of	O
the	O
order	O
of	O
several	O
hundred	O
)	O
to	O
reach	O
a	O
candidate	O
point	O
x	O
(	O
cid:48	O
)	O
,	O
y	O
(	O
cid:48	O
)	O
.	O
if	O
the	O
hamiltonian	O
dynamics	O
is	O
numerically	O
accurate	O
,	O
h	O
(	O
x	O
(	O
cid:48	O
)	O
,	O
y	O
(	O
cid:48	O
)	O
)	O
will	O
have	O
roughly	O
the	O
same	O
value	B
as	O
h	O
(	O
x	O
,	O
y	O
)	O
.	O
we	O
then	O
do	O
a	O
metropolis	O
step	O
,	O
and	O
accept	O
the	O
point	O
x	O
(	O
cid:48	O
)	O
,	O
y	O
(	O
cid:48	O
)	O
if	O
h	O
(	O
x	O
(	O
cid:48	O
)	O
,	O
y	O
(	O
cid:48	O
)	O
)	O
>	O
h	O
(	O
x	O
,	O
y	O
)	O
and	O
otherwise	O
accept	O
it	O
with	O
probability	O
exp	O
(	O
h	O
(	O
x	O
(	O
cid:48	O
)	O
,	O
y	O
(	O
cid:48	O
)	O
)	O
−	O
h	O
(	O
x	O
,	O
y	O
)	O
)	O
.	O
if	O
rejected	O
,	O
we	O
take	O
the	O
initial	O
point	O
x	O
,	O
y	O
as	O
the	O
sample	B
.	O
combined	O
with	O
the	O
p	O
(	O
y	O
)	O
sample	B
step	O
,	O
we	O
then	O
have	O
the	O
general	O
procedure	O
as	O
described	O
in	O
algorithm	B
(	O
26	O
)	O
.	O
in	O
hmc	O
we	O
use	O
not	O
just	O
the	O
potential	B
hx	O
(	O
x	O
)	O
to	O
deﬁne	O
candidate	O
samples	O
,	O
but	O
the	O
gradient	B
of	O
hx	O
(	O
x	O
)	O
as	O
well	O
.	O
an	O
intuitive	O
explanation	O
for	O
the	O
success	O
of	O
the	O
algorithm	B
is	O
that	O
it	O
is	O
less	O
myopic	O
than	O
straightforward	O
metropolis	O
since	O
the	O
gradient	B
enables	O
the	O
algorithm	B
to	O
feel	O
its	O
way	O
to	O
other	O
regions	O
of	O
high	O
probability	O
by	O
contouring	O
paths	O
in	O
the	O
augmented	B
space	O
.	O
one	O
can	O
also	O
view	O
the	O
auxiliary	O
variables	O
as	O
momentum	O
variables	O
–	O
it	O
is	O
as	O
if	O
the	O
sample	B
has	O
now	O
a	O
momentum	B
which	O
can	O
carry	O
it	O
through	O
the	O
low-density	O
x-	O
regions	O
.	O
provided	O
this	O
momentum	B
is	O
high	O
enough	O
,	O
we	O
can	O
escape	O
local	B
regions	O
of	O
signiﬁcant	O
probability	B
,	O
see	O
ﬁg	O
(	O
27.9	O
)	O
.	O
draft	O
march	O
9	O
,	O
2010	O
503	O
auxiliary	B
variable	I
methods	O
algorithm	B
26	O
hybrid	O
monte	O
carlo	O
sampling	B
1	O
:	O
start	O
from	O
x	O
2	O
:	O
for	O
i	O
=	O
1	O
to	O
l	O
do	O
3	O
:	O
4	O
:	O
5	O
:	O
6	O
:	O
7	O
:	O
8	O
:	O
end	O
for	O
draw	O
a	O
new	O
sample	B
y	O
from	O
p	O
(	O
y	O
)	O
.	O
choose	O
a	O
random	O
(	O
forwards	O
or	O
backwards	O
)	O
trajectory	O
direction	O
.	O
starting	O
from	O
x	O
,	O
y	O
,	O
follow	O
hamiltonian	O
dynamics	O
for	O
a	O
ﬁxed	O
number	O
of	O
steps	O
,	O
giving	O
a	O
candidate	O
x	O
(	O
cid:48	O
)	O
,	O
y	O
(	O
cid:48	O
)	O
.	O
accept	O
x	O
(	O
cid:48	O
)	O
,	O
y	O
(	O
cid:48	O
)	O
if	O
h	O
(	O
x	O
(	O
cid:48	O
)	O
,	O
y	O
(	O
cid:48	O
)	O
)	O
>	O
h	O
(	O
x	O
,	O
y	O
)	O
,	O
otherwise	O
accept	O
it	O
with	O
probability	O
exp	O
(	O
h	O
(	O
x	O
(	O
cid:48	O
)	O
,	O
y	O
(	O
cid:48	O
)	O
)	O
−	O
h	O
(	O
x	O
,	O
y	O
)	O
)	O
.	O
if	O
rejected	O
,	O
we	O
take	O
the	O
sample	B
as	O
x	O
,	O
y	O
.	O
(	O
a	O
)	O
(	O
c	O
)	O
(	O
b	O
)	O
(	O
cid:81	O
)	O
(	O
a	O
)	O
:	O
current	O
sample	B
of	O
states	O
figure	O
27.10	O
:	O
swendson-wang	O
updating	O
for	O
p	O
(	O
x	O
)	O
∝	O
(	O
here	O
on	O
a	O
nearest	B
neighbour	I
lattice	O
)	O
.	O
(	O
b	O
)	O
:	O
like	O
coloured	O
neighbours	O
are	O
bonded	O
together	O
with	O
probability	O
1−e−β	O
,	O
forming	O
clusters	O
of	O
variables	O
.	O
(	O
c	O
)	O
:	O
each	O
cluster	O
is	O
given	O
a	O
random	O
colour	O
,	O
forming	O
the	O
new	O
sample	B
.	O
i∼j	O
exp	O
βi	O
[	O
xi	O
=	O
xj	O
]	O
.	O
27.5.2	O
swendson-wang	O
originally	O
,	O
the	O
sw	O
method	O
was	O
introduced	O
to	O
alleviate	O
the	O
problems	O
encountered	O
in	O
sampling	B
from	O
ising	O
models	O
close	O
to	O
their	O
critical	O
temperature	O
[	O
268	O
]	O
.	O
at	O
this	O
point	O
large	O
islands	O
of	O
same-state	O
variables	O
form	O
so	O
that	O
strong	B
correlations	O
appear	O
in	O
the	O
distribution	B
–	O
the	O
scenario	O
under	O
which	O
,	O
for	O
example	O
,	O
gibbs	O
sampling	B
is	O
not	O
well	O
suited	O
.	O
the	O
method	O
has	O
been	O
generalised	B
to	O
other	O
models	O
[	O
88	O
]	O
,	O
although	O
here	O
we	O
outline	O
the	O
procedure	O
for	O
the	O
ising	O
model	B
only	O
,	O
referring	O
the	O
reader	O
to	O
more	O
specialised	O
text	O
for	O
the	O
extensions	O
[	O
37	O
]	O
.	O
see	O
also	O
[	O
198	O
]	O
for	O
the	O
use	O
of	O
auxiliary	O
variables	O
in	O
perfect	B
sampling	I
.	O
the	O
ising	O
model	B
with	O
no	O
external	O
ﬁelds	O
is	O
deﬁned	O
on	O
variables	O
x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
,	O
xi	O
∈	O
{	O
0	O
,	O
1	O
}	O
and	O
takes	O
the	O
form	O
eβi	O
[	O
xi=xj	O
]	O
(	O
27.5.12	O
)	O
(	O
cid:89	O
)	O
i∼j	O
p	O
(	O
x	O
)	O
=	O
1	O
z	O
which	O
means	O
that	O
this	O
is	O
a	O
pairwise	B
markov	O
network	O
with	O
a	O
potential	B
contribution	O
eβ	O
if	O
neighbouring	O
nodes	O
i	O
and	O
j	O
on	O
a	O
square	O
lattice	O
are	O
in	O
the	O
same	O
state	O
,	O
and	O
a	O
contribution	O
1	O
otherwise	O
.	O
we	O
assume	O
that	O
β	O
>	O
0	O
which	O
encourages	O
neighbours	O
to	O
be	O
in	O
the	O
same	O
state	O
.	O
the	O
lattice	O
based	O
neighbourhood	O
structure	B
makes	O
this	O
diﬃcult	O
to	O
sample	B
from	O
,	O
and	O
especially	O
when	O
β	O
≈	O
0.9	O
which	O
encourages	O
large	O
scale	O
islands	O
of	O
same-state	O
variables	O
to	O
form	O
.	O
the	O
aim	O
is	O
to	O
remove	O
the	O
problematic	O
terms	O
eβi	O
[	O
xi=xj	O
]	O
by	O
the	O
use	O
of	O
the	O
auxiliary	O
‘	O
bond	O
’	O
variables	O
,	O
yij	O
,	O
one	O
for	O
each	O
edge	O
on	O
the	O
lattice	O
,	O
making	O
the	O
conditional	B
p	O
(	O
x|y	O
)	O
easy	O
to	O
sample	B
from	O
.	O
this	O
is	O
given	O
by	O
p	O
(	O
x|y	O
)	O
∝	O
p	O
(	O
y|x	O
)	O
p	O
(	O
x	O
)	O
∝	O
p	O
(	O
y|x	O
)	O
(	O
cid:89	O
)	O
0	O
<	O
yij	O
<	O
eβi	O
[	O
xi=xj	O
]	O
(	O
cid:105	O
)	O
using	O
p	O
(	O
y|x	O
)	O
we	O
can	O
cancel	O
the	O
terms	O
eβi	O
[	O
xi=xj	O
]	O
by	O
setting	O
p	O
(	O
yij|xi	O
,	O
xj	O
)	O
=	O
(	O
cid:89	O
)	O
p	O
(	O
y|x	O
)	O
=	O
(	O
cid:89	O
)	O
where	O
i	O
(	O
cid:2	O
)	O
0	O
<	O
yij	O
<	O
eβi	O
[	O
xi=xj	O
]	O
(	O
cid:3	O
)	O
denotes	O
a	O
uniform	B
distribution	I
between	O
0	O
and	O
eβi	O
[	O
xi=xj	O
]	O
;	O
zij	O
is	O
the	O
normalisa-	O
eβi	O
[	O
xi=xj	O
]	O
(	O
27.5.13	O
)	O
(	O
27.5.14	O
)	O
i	O
(	O
cid:104	O
)	O
i∼j	O
i∼j	O
1	O
zij	O
i∼j	O
504	O
draft	O
march	O
9	O
,	O
2010	O
auxiliary	B
variable	I
methods	O
,	O
with	O
figure	O
27.11	O
:	O
ten	O
successive	O
samples	O
from	O
a	O
25	O
×	O
25	O
ising	O
model	B
p	O
(	O
x	O
)	O
∝	O
exp	O
β	O
=	O
0.88	O
,	O
close	O
to	O
the	O
critical	O
temperature	O
.	O
the	O
swendson-wang	O
procedure	O
is	O
used	O
.	O
starting	O
in	O
a	O
random	O
initial	O
conﬁguration	O
,	O
the	O
samples	O
quickly	O
move	O
away	O
from	O
this	O
initial	O
state	O
,	O
with	O
the	O
characteristic	O
long-	O
range	O
correlations	O
of	O
the	O
variables	O
seen	O
close	O
to	O
the	O
critical	O
temperature	O
.	O
i∼j	O
βi	O
[	O
xi	O
=	O
xj	O
]	O
(	O
cid:16	O
)	O
(	O
cid:80	O
)	O
(	O
cid:17	O
)	O
tion	O
constant	O
zij	O
=	O
eβi	O
[	O
xi=xj	O
]	O
.	O
hence	O
i	O
(	O
cid:104	O
)	O
0	O
<	O
yij	O
<	O
eβi	O
[	O
xi=xj	O
]	O
(	O
cid:105	O
)	O
p	O
(	O
x|y	O
)	O
∝	O
p	O
(	O
y|x	O
)	O
p	O
(	O
x	O
)	O
1	O
eβi	O
[	O
xi=xj	O
]	O
∝	O
i	O
(	O
cid:104	O
)	O
0	O
<	O
yij	O
<	O
eβi	O
[	O
xi=xj	O
]	O
(	O
cid:105	O
)	O
(	O
cid:89	O
)	O
(	O
cid:89	O
)	O
i∼j	O
i∼j	O
∝	O
eβi	O
[	O
xi=xj	O
]	O
(	O
27.5.15	O
)	O
(	O
27.5.16	O
)	O
(	O
27.5.17	O
)	O
let	O
’	O
s	O
assume	O
that	O
we	O
have	O
a	O
sample	B
{	O
yij	O
}	O
.	O
if	O
yij	O
>	O
1	O
,	O
then	O
to	O
draw	O
a	O
sample	B
from	O
p	O
(	O
x|y	O
)	O
,	O
we	O
must	O
have	O
1	O
<	O
eβi	O
[	O
xi=xj	O
]	O
,	O
which	O
means	O
that	O
xi	O
and	O
xj	O
are	O
constrained	O
to	O
be	O
in	O
the	O
same	O
state	O
.	O
otherwise	O
,	O
if	O
yij	O
<	O
1	O
,	O
then	O
this	O
introduces	O
no	O
extra	O
constraint	O
on	O
xi	O
and	O
xj	O
.	O
hence	O
,	O
wherever	O
yij	O
>	O
1	O
,	O
we	O
bond	O
xi	O
and	O
xj	O
to	O
be	O
in	O
the	O
same	O
state	O
.	O
same	O
state	O
.	O
then	O
p	O
(	O
yij|xi	O
=	O
xj	O
)	O
=	O
u	O
(	O
cid:0	O
)	O
yij|	O
to	O
sample	B
from	O
the	O
bond	O
variables	O
p	O
(	O
yij|xi	O
,	O
xj	O
)	O
consider	O
ﬁrst	O
the	O
situation	O
that	O
xi	O
and	O
xj	O
are	O
in	O
the	O
p	O
(	O
yij|xi	O
(	O
cid:54	O
)	O
=	O
xj	O
)	O
=	O
u	O
(	O
yij|	O
[	O
0	O
,	O
1	O
]	O
)	O
.	O
a	O
bond	O
will	O
occur	O
if	O
yij	O
>	O
1	O
,	O
which	O
occurs	O
with	O
probability	O
(	O
cid:2	O
)	O
0	O
,	O
eβ	O
(	O
cid:3	O
)	O
(	O
cid:1	O
)	O
.	O
similarly	O
when	O
xi	O
and	O
xj	O
are	O
in	O
diﬀerent	O
states	O
,	O
(	O
cid:90	O
)	O
∞	O
1	O
zij	O
yij	O
=1	O
i	O
(	O
cid:104	O
)	O
0	O
<	O
yij	O
<	O
eβ	O
(	O
cid:105	O
)	O
p	O
(	O
yij	O
>	O
1|xi	O
=	O
xj	O
)	O
=	O
=	O
eβ	O
−	O
1	O
eβ	O
=	O
1	O
−	O
e	O
−β	O
(	O
27.5.18	O
)	O
hence	O
,	O
if	O
xi	O
=	O
xj	O
,	O
we	O
bind	O
xi	O
and	O
xj	O
to	O
be	O
in	O
the	O
same	O
state	O
with	O
probability	O
1	O
−	O
e−β	O
.	O
on	O
the	O
other	O
hand	O
if	O
xi	O
and	O
xj	O
are	O
in	O
diﬀerent	O
states	O
,	O
yij	O
is	O
uniformly	O
distributed	O
between	O
0	O
and	O
1.	O
after	O
doing	O
this	O
for	O
all	O
the	O
xi	O
and	O
xj	O
pairs	O
,	O
we	O
will	O
end	O
up	O
with	O
a	O
graph	B
in	O
which	O
we	O
have	O
clusters	O
of	O
like-state	O
bonded	O
variables	O
.	O
the	O
algorithm	B
simply	O
chooses	O
a	O
random	O
state	O
for	O
each	O
cluster	O
–	O
that	O
is	O
,	O
with	O
probability	O
0.5	O
all	O
variables	O
in	O
the	O
cluster	O
are	O
in	O
state	O
1.	O
the	O
algorithm	B
is	O
described	O
below	O
,	O
see	O
ﬁg	O
(	O
27.10	O
)	O
:	O
algorithm	B
27	O
swendson-wang	O
sampling	B
1	O
,	O
.	O
.	O
.	O
,	O
x1	O
n.	O
for	O
i	O
,	O
j	O
in	O
the	O
edge	O
set	O
do	O
1	O
:	O
start	O
from	O
a	O
random	O
conﬁguration	O
of	O
all	O
x1	O
2	O
:	O
for	O
l	O
=	O
1	O
to	O
l	O
do	O
3	O
:	O
4	O
:	O
5	O
:	O
6	O
:	O
7	O
:	O
8	O
:	O
end	O
for	O
if	O
xi	O
=	O
xj	O
,	O
we	O
bond	O
variables	O
xi	O
and	O
xj	O
with	O
probability	O
1	O
−	O
e−β	O
.	O
end	O
for	O
for	O
each	O
cluster	O
formed	O
from	O
the	O
above	O
,	O
set	O
the	O
state	O
of	O
the	O
cluster	O
uniformly	O
at	O
random	O
.	O
this	O
gives	O
a	O
new	O
joint	B
conﬁguration	O
xl	O
1	O
,	O
.	O
.	O
.	O
,	O
xl	O
n.	O
this	O
technique	O
has	O
found	O
application	O
in	O
spatial	O
statistics	O
,	O
particularly	O
image	O
restoration	O
[	O
134	O
]	O
.	O
27.5.3	O
slice	B
sampling	I
slice	O
sampling	B
[	O
207	O
]	O
is	O
an	O
auxiliary	B
variable	I
technique	O
that	O
aims	O
to	O
overcome	O
some	O
of	O
the	O
diﬃculties	O
in	O
choosing	O
an	O
appropriate	O
‘	O
length	O
scale	O
’	O
in	O
methods	O
such	O
as	O
metropolis	O
sampling	B
.	O
the	O
brief	O
discussion	O
draft	O
march	O
9	O
,	O
2010	O
505	O
importance	B
sampling	O
figure	O
27.12	O
:	O
the	O
full	O
slice	O
for	O
a	O
given	O
y.	O
ideally	O
slice	B
sampling	I
would	O
draw	O
an	O
x	O
sample	B
from	O
anywhere	O
on	O
the	O
full	O
slice	O
(	O
green	O
)	O
.	O
in	O
general	O
this	O
is	O
intractable	O
for	O
a	O
complex	O
distribution	B
and	O
a	O
local	B
approximate	O
slice	O
is	O
formed	O
instead	O
,	O
see	O
ﬁg	O
(	O
27.13	O
)	O
.	O
z	O
p∗	O
(	O
x	O
)	O
where	O
the	O
here	O
follows	O
that	O
presented	O
in	O
[	O
183	O
]	O
and	O
[	O
42	O
]	O
.	O
we	O
want	O
to	O
draw	O
samples	O
from	O
p	O
(	O
x	O
)	O
=	O
1	O
normalisation	O
constant	O
z	O
is	O
unknown	O
.	O
by	O
introducing	O
the	O
auxiliary	B
variable	I
y	O
and	O
deﬁning	O
the	O
distribution	B
(	O
cid:26	O
)	O
1/z	O
for	O
0	O
≤	O
y	O
≤	O
p∗	O
(	O
x	O
)	O
(	O
cid:90	O
)	O
p∗	O
(	O
x	O
)	O
otherwise	O
0	O
p	O
(	O
x	O
,	O
y	O
)	O
=	O
we	O
have	O
(	O
cid:90	O
)	O
(	O
27.5.19	O
)	O
(	O
27.5.20	O
)	O
p	O
(	O
x	O
,	O
y	O
)	O
dy	O
=	O
0	O
1	O
z	O
dy	O
=	O
1	O
z	O
∗	O
(	O
x	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
which	O
shows	O
that	O
the	O
marginal	B
of	O
p	O
(	O
x	O
,	O
y	O
)	O
over	O
y	O
is	O
equal	O
to	O
the	O
distribution	B
we	O
wish	O
to	O
draw	O
samples	O
from	O
.	O
hence	O
if	O
we	O
draw	O
samples	O
from	O
p	O
(	O
x	O
,	O
y	O
)	O
,	O
we	O
can	O
ignore	O
the	O
y	O
samples	O
and	O
we	O
will	O
have	O
a	O
valid	O
sampling	B
scheme	O
for	O
p	O
(	O
x	O
)	O
.	O
to	O
draw	O
from	O
p	O
(	O
x	O
,	O
y	O
)	O
we	O
use	O
gibbs	O
sampling	B
,	O
ﬁrst	O
drawing	O
from	O
p	O
(	O
y|x	O
)	O
and	O
then	O
from	O
p	O
(	O
x|y	O
)	O
.	O
drawing	O
a	O
sample	B
from	O
p	O
(	O
y|x	O
)	O
means	O
that	O
we	O
draw	O
a	O
value	B
y	O
from	O
the	O
uniform	B
distribution	I
u	O
(	O
y|	O
[	O
0	O
,	O
p∗	O
(	O
x	O
)	O
]	O
)	O
.	O
given	O
a	O
sample	B
y	O
,	O
one	O
then	O
draws	O
a	O
sample	B
x	O
from	O
p	O
(	O
x|y	O
)	O
.	O
using	O
p	O
(	O
x|y	O
)	O
∝	O
p	O
(	O
x	O
,	O
y	O
)	O
we	O
see	O
that	O
p	O
(	O
x|y	O
)	O
is	O
the	O
distribution	B
over	O
x	O
such	O
that	O
p∗	O
(	O
x	O
)	O
>	O
y	O
:	O
p	O
(	O
x|y	O
)	O
∝	O
i	O
[	O
p	O
∗	O
(	O
x	O
)	O
>	O
y	O
]	O
(	O
27.5.21	O
)	O
for	O
a	O
given	O
y	O
,	O
we	O
call	O
the	O
x	O
that	O
satisfy	O
this	O
a	O
‘	O
slice	O
’	O
,	O
ﬁg	O
(	O
27.12	O
)	O
.	O
computing	O
the	O
normalisation	B
of	O
this	O
distribution	B
is	O
in	O
general	O
non-trivial	O
since	O
we	O
would	O
in	O
principle	O
need	O
to	O
search	O
over	O
all	O
x	O
to	O
ﬁnd	O
those	O
for	O
which	O
p∗	O
(	O
x	O
)	O
>	O
y.	O
ideally	O
we	O
would	O
like	O
to	O
get	O
as	O
much	O
of	O
the	O
slice	O
as	O
feasible	O
,	O
since	O
this	O
will	O
improve	O
the	O
mixing	O
of	O
the	O
chain	B
.	O
if	O
we	O
concentrate	O
on	O
the	O
part	O
of	O
the	O
slice	O
only	O
very	O
local	B
to	O
the	O
current	O
x	O
,	O
then	O
the	O
samples	O
move	O
through	O
the	O
space	O
very	O
slowly	O
.	O
if	O
we	O
attempt	O
to	O
guess	O
at	O
random	O
a	O
point	O
a	O
long	O
way	O
from	O
x	O
and	O
check	B
if	O
is	O
in	O
the	O
slice	O
,	O
this	O
will	O
be	O
mostly	O
unsuccessful	O
.	O
the	O
happy	O
compromise	O
presented	O
in	O
algorithm	B
(	O
28	O
)	O
[	O
207	O
]	O
and	O
described	O
in	O
ﬁg	O
(	O
27.13	O
)	O
determines	O
an	O
appropriate	O
local	B
slice	O
by	O
adjusting	O
the	O
left	O
and	O
right	O
regions	O
.	O
the	O
technique	O
is	O
to	O
start	O
from	O
the	O
current	O
x	O
and	O
attempt	O
to	O
ﬁnd	O
the	O
largest	O
local	B
slice	O
by	O
incrementally	O
widening	O
the	O
candidate	O
slice	O
.	O
once	O
we	O
have	O
the	O
largest	O
potential	B
slice	O
we	O
attempt	O
to	O
sample	B
from	O
this	O
.	O
if	O
the	O
sample	B
point	O
within	O
the	O
local	B
slice	O
is	O
in	O
fact	O
not	O
in	O
the	O
slice	O
,	O
this	O
is	O
rejected	O
and	O
the	O
slice	O
is	O
shrunk	O
.	O
this	O
describes	O
a	O
procedure	O
for	O
sampling	B
from	O
a	O
univariate	B
distribution	O
p∗	O
(	O
x	O
)	O
.	O
to	O
sample	B
from	O
a	O
multivariate	B
distribution	O
p	O
(	O
x	O
)	O
,	O
single	O
variable	B
gibbs	O
sampling	B
can	O
be	O
used	O
to	O
sample	B
from	O
p	O
(	O
xj|x\j	O
)	O
,	O
repeatedly	O
choosing	O
a	O
new	O
variable	B
xj	O
to	O
sample	B
.	O
27.6	O
importance	B
sampling	O
z	O
where	O
p∗	O
(	O
x	O
)	O
can	O
be	O
evaluated	O
but	O
z	O
=	O
(	O
cid:82	O
)	O
importance	B
sampling	O
is	O
a	O
technique	O
to	O
approximate	B
averages	O
with	O
respect	O
to	O
an	O
intractable	O
distribution	O
p	O
(	O
x	O
)	O
.	O
the	O
term	O
‘	O
sampling	B
’	O
is	O
arguably	O
a	O
misnomer	O
since	O
the	O
method	O
does	O
not	O
attempt	O
to	O
draw	O
samples	O
from	O
p	O
(	O
x	O
)	O
.	O
rather	O
the	O
method	O
draws	O
samples	O
from	O
a	O
simpler	O
importance	B
distribution	O
q	O
(	O
x	O
)	O
and	O
then	O
reweights	O
them	O
such	O
that	O
averages	O
with	O
respect	O
to	O
p	O
(	O
x	O
)	O
can	O
be	O
approximated	O
using	O
the	O
samples	O
from	O
q	O
(	O
x	O
)	O
.	O
consider	O
p	O
(	O
x	O
)	O
=	O
p∗	O
(	O
x	O
)	O
x	O
p∗	O
(	O
x	O
)	O
is	O
an	O
intractable	O
normalisation	O
constant	O
.	O
the	O
(	O
cid:82	O
)	O
average	B
of	O
f	O
(	O
x	O
)	O
with	O
respect	O
to	O
p	O
(	O
x	O
)	O
is	O
given	O
by	O
(	O
cid:82	O
)	O
x	O
f	O
(	O
x	O
)	O
p∗	O
(	O
x	O
)	O
p∗	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
(	O
cid:82	O
)	O
(	O
cid:82	O
)	O
x	O
f	O
(	O
x	O
)	O
p∗	O
(	O
x	O
)	O
x	O
p∗	O
(	O
x	O
)	O
f	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
=	O
q	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
(	O
27.6.1	O
)	O
(	O
cid:90	O
)	O
x	O
=	O
x	O
506	O
draft	O
march	O
9	O
,	O
2010	O
importance	B
sampling	O
(	O
a	O
)	O
(	O
c	O
)	O
(	O
b	O
)	O
(	O
d	O
)	O
figure	O
27.13	O
:	O
(	O
a	O
)	O
:	O
for	O
the	O
current	O
sample	B
x	O
,	O
a	O
point	O
y	O
is	O
sampled	O
between	O
0	O
and	O
p∗	O
(	O
x	O
)	O
,	O
giving	O
a	O
point	O
(	O
x	O
,	O
y	O
)	O
(	O
black	O
circle	O
)	O
.	O
then	O
an	O
interval	O
of	O
width	O
w	O
is	O
placed	O
around	O
x	O
,	O
the	O
blue	O
bar	O
.	O
the	O
ends	O
of	O
the	O
bar	O
(	O
b	O
)	O
:	O
the	O
interval	O
is	O
increased	O
until	O
it	O
denote	O
if	O
the	O
point	O
is	O
in	O
the	O
slice	O
(	O
green	O
)	O
or	O
out	O
of	O
the	O
slice	O
(	O
red	O
)	O
.	O
(	O
c	O
)	O
:	O
given	O
an	O
interval	O
a	O
sample	B
x	O
(	O
cid:48	O
)	O
is	O
taken	O
uniformly	O
in	O
the	O
interval	O
.	O
if	O
the	O
hits	O
a	O
point	O
out	O
of	O
the	O
slice	O
.	O
candidate	O
x	O
(	O
cid:48	O
)	O
is	O
not	O
in	O
the	O
slice	O
(	O
red	O
)	O
,	O
p	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
<	O
y	O
,	O
the	O
candidate	O
is	O
rejected	O
and	O
the	O
interval	O
is	O
shrunk	O
.	O
(	O
d	O
)	O
:	O
the	O
sampling	B
from	O
the	O
interval	O
is	O
repeated	O
until	O
a	O
candidate	O
is	O
in	O
the	O
slice	O
(	O
green	O
)	O
,	O
and	O
is	O
subsequently	O
accepted	O
.	O
let	O
x1	O
,	O
.	O
.	O
.	O
,	O
xl	O
be	O
samples	O
from	O
q	O
(	O
x	O
)	O
,	O
then	O
we	O
can	O
approximate	B
the	O
average	B
by	O
(	O
cid:90	O
)	O
f	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
≈	O
x	O
(	O
cid:80	O
)	O
l	O
(	O
cid:80	O
)	O
l	O
l=1	O
f	O
(	O
xl	O
)	O
p∗	O
(	O
xl	O
)	O
q	O
(	O
xl	O
)	O
p∗	O
(	O
xl	O
)	O
q	O
(	O
xl	O
)	O
l=1	O
l	O
(	O
cid:88	O
)	O
l=1	O
=	O
f	O
(	O
xl	O
)	O
wl	O
where	O
we	O
deﬁne	O
the	O
normalised	B
importance	I
weights	I
(	O
cid:80	O
)	O
l	O
p∗	O
(	O
xl	O
)	O
/q	O
(	O
xl	O
)	O
l=1	O
p∗	O
(	O
xl	O
)	O
/q	O
(	O
xl	O
)	O
wl	O
=	O
,	O
with	O
wl	O
=	O
1	O
l	O
(	O
cid:88	O
)	O
l=1	O
(	O
27.6.2	O
)	O
(	O
27.6.3	O
)	O
in	O
principle	O
,	O
reweighing	O
the	O
samples	O
from	O
q	O
will	O
give	O
the	O
correct	O
result	O
for	O
the	O
average	B
with	O
respect	O
to	O
p.	O
since	O
the	O
weight	B
is	O
a	O
measure	O
of	O
how	O
well	O
q	O
matches	O
p	O
,	O
there	O
will	O
typically	O
be	O
only	O
a	O
single	O
dominant	O
weight	B
.	O
this	O
is	O
particularly	O
evident	O
in	O
high	O
dimensions	O
there	O
will	O
typically	O
only	O
be	O
one	O
dominant	O
weight	B
with	O
value	B
close	O
to	O
1	O
,	O
and	O
the	O
rest	O
will	O
be	O
zero	O
,	O
particularly	O
if	O
the	O
sampling	B
distribution	O
q	O
is	O
not	O
well	O
matched	O
to	O
p.	O
as	O
an	O
indication	O
of	O
this	O
eﬀect	O
,	O
consider	O
a	O
d-dimensional	O
multivariate	B
x	O
with	O
two	O
samples	O
x1	O
and	O
x2	O
.	O
for	O
simplicity	O
we	O
assume	O
that	O
both	O
p	O
and	O
q	O
are	O
factorised	B
over	O
their	O
variables	O
.	O
the	O
associated	O
unnormalised	O
importance	B
weights	O
are	O
then	O
p∗	O
(	O
x2	O
d	O
)	O
q	O
(	O
x2	O
d	O
)	O
p∗	O
(	O
x1	O
d	O
)	O
d	O
)	O
,	O
q	O
(	O
x1	O
d	O
(	O
cid:89	O
)	O
d	O
(	O
cid:89	O
)	O
(	O
27.6.4	O
)	O
˜w1	O
=	O
˜w2	O
=	O
d=1	O
d=1	O
if	O
we	O
assume	O
the	O
the	O
match	O
between	O
q	O
and	O
p	O
better	O
is	O
worse	O
by	O
a	O
factor	B
α	O
≤	O
1	O
in	O
each	O
of	O
the	O
dimensions	O
at	O
x2	O
than	O
x1	O
then	O
(	O
27.6.5	O
)	O
˜w1	O
=	O
o	O
(	O
cid:0	O
)	O
αd	O
(	O
cid:1	O
)	O
˜w2	O
so	O
that	O
importance	B
weight	O
at	O
w1	O
will	O
exponentially	O
dominate	O
w2	O
.	O
a	O
method	O
that	O
can	O
help	O
address	O
this	O
weight	B
dominance	O
is	O
resampling	B
.	O
given	O
the	O
weight	B
distribution	O
w1	O
,	O
.	O
.	O
.	O
,	O
wl	O
,	O
one	O
draws	O
a	O
set	O
of	O
l	O
sample	B
indices	O
.	O
this	O
new	O
set	O
of	O
indices	O
will	O
almost	O
certainly	O
contain	O
repeats	O
since	O
any	O
of	O
the	O
original	O
low-weight	O
samples	O
will	O
most	O
likely	O
not	O
be	O
included	O
.	O
the	O
weight	B
of	O
each	O
of	O
these	O
new	O
samples	O
is	O
set	O
uniformly	O
to	O
1/l	O
.	O
this	O
procedure	O
helps	O
select	O
only	O
the	O
‘	O
ﬁttest	O
’	O
of	O
the	O
samples	O
and	O
is	O
known	O
as	O
sampling	O
importance	B
resampling	O
[	O
235	O
]	O
.	O
draft	O
march	O
9	O
,	O
2010	O
507	O
draw	O
a	O
vertical	O
coordinate	O
y	O
uniformly	O
from	O
the	O
interval	O
(	O
cid:0	O
)	O
0	O
,	O
p∗	O
(	O
xi	O
)	O
(	O
cid:1	O
)	O
.	O
create	O
a	O
horizontal	O
interval	O
(	O
xlef	O
t	O
,	O
xright	O
)	O
that	O
contains	O
xi	O
as	O
follows	O
:	O
draw	O
r	O
∼	O
u	O
(	O
r|	O
(	O
0	O
,	O
1	O
)	O
)	O
xlef	O
t	O
=	O
xi	O
−	O
rw	O
,	O
xright	O
=	O
xi	O
+	O
(	O
1	O
−	O
r	O
)	O
w	O
while	O
p∗	O
(	O
xlef	O
t	O
)	O
>	O
y	O
do	O
xlef	O
t	O
=	O
xlef	O
t	O
−	O
w	O
algorithm	B
28	O
slice	B
sampling	I
(	O
univariate	B
)	O
.	O
end	O
while	O
while	O
p∗	O
(	O
xright	O
)	O
>	O
y	O
do	O
xright	O
=	O
xright	O
+	O
w	O
1	O
:	O
choose	O
a	O
starting	O
point	O
x1	O
and	O
step	O
size	O
w.	O
2	O
:	O
for	O
i	O
=	O
1	O
to	O
l	O
do	O
3	O
:	O
4	O
:	O
5	O
:	O
6	O
:	O
7	O
:	O
8	O
:	O
9	O
:	O
10	O
:	O
11	O
:	O
12	O
:	O
13	O
:	O
14	O
:	O
15	O
:	O
16	O
:	O
17	O
:	O
18	O
:	O
19	O
:	O
20	O
:	O
21	O
:	O
22	O
:	O
23	O
:	O
24	O
:	O
25	O
:	O
26	O
:	O
27	O
:	O
28	O
:	O
end	O
for	O
end	O
if	O
end	O
while	O
xi+1	O
=	O
x	O
(	O
cid:48	O
)	O
end	O
if	O
else	O
end	O
while	O
accept	O
=	O
false	O
while	O
accept	O
=	O
false	O
do	O
draw	O
a	O
random	O
value	O
x	O
(	O
cid:48	O
)	O
uniformly	O
from	O
the	O
unit	O
interval	O
(	O
xlef	O
t	O
,	O
xright	O
)	O
.	O
if	O
p∗	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
>	O
y	O
then	O
accept	O
=	O
true	O
modify	O
the	O
interval	O
(	O
xlef	O
t	O
,	O
xright	O
)	O
as	O
follows	O
:	O
if	O
x	O
(	O
cid:48	O
)	O
>	O
xi	O
then	O
xright	O
=	O
x	O
(	O
cid:48	O
)	O
xlef	O
t	O
=	O
x	O
(	O
cid:48	O
)	O
else	O
importance	B
sampling	O
(	O
cid:46	O
)	O
create	O
an	O
initial	O
interval	O
(	O
cid:46	O
)	O
step	O
out	O
left	O
(	O
cid:46	O
)	O
step	O
out	O
right	O
(	O
cid:46	O
)	O
found	O
a	O
valid	O
sample	B
(	O
cid:46	O
)	O
shrinking	O
27.6.1	O
sequential	B
importance	I
sampling	I
one	O
can	O
apply	O
importance	B
sampling	O
to	O
temporal	O
distributions	O
p	O
(	O
x1	O
:	O
t	O
)	O
for	O
which	O
the	O
importance	B
distribution	O
samples	O
from	O
q	O
(	O
x1	O
:	O
t	O
)	O
are	O
paths	O
.	O
in	O
many	O
applications	O
such	O
as	O
tracking	O
,	O
one	O
wishes	O
to	O
update	O
ones	O
beliefs	O
as	O
time	O
increases	O
and	O
,	O
as	O
such	O
,	O
is	O
required	O
to	O
resample	O
and	O
then	O
reweight	O
the	O
whole	O
path	B
.	O
for	O
distributions	O
p	O
(	O
x1	O
:	O
t	O
)	O
with	O
a	O
markov	O
structure	B
,	O
one	O
would	O
expect	O
that	O
a	O
local	B
update	O
is	O
possible	O
,	O
without	O
needing	O
to	O
deal	O
with	O
the	O
previous	O
path	B
.	O
to	O
show	O
this	O
,	O
consider	O
the	O
unnormalised	O
importance	B
weights	O
for	O
a	O
sample	B
path	O
xl	O
1	O
:	O
t	O
t	O
=	O
p∗	O
(	O
xl	O
1	O
:	O
t	O
)	O
1	O
:	O
t	O
)	O
q	O
(	O
xl	O
˜wl	O
=	O
p∗	O
(	O
xl	O
q	O
(	O
xl	O
1	O
:	O
t−1	O
)	O
1	O
:	O
t−1	O
)	O
p∗	O
(	O
xl	O
1	O
:	O
t	O
)	O
1	O
:	O
t−1	O
)	O
q	O
(	O
xl	O
t|xl	O
p∗	O
(	O
xl	O
1	O
:	O
t−1	O
)	O
,	O
1	O
=	O
p∗	O
(	O
xl	O
1	O
)	O
q	O
(	O
xl	O
1	O
)	O
˜wl	O
we	O
can	O
recursively	O
deﬁne	O
the	O
un-normalised	O
weights	O
using	O
˜wl	O
t	O
=	O
˜wl	O
t−1αl	O
t	O
,	O
t	O
>	O
1	O
where	O
αl	O
t	O
≡	O
p∗	O
(	O
xl	O
1	O
:	O
t	O
)	O
1	O
:	O
t−1	O
)	O
q	O
(	O
xl	O
t|xl	O
p∗	O
(	O
xl	O
1	O
:	O
t−1	O
)	O
(	O
27.6.6	O
)	O
(	O
27.6.7	O
)	O
(	O
27.6.8	O
)	O
this	O
means	O
that	O
in	O
sis	O
we	O
need	O
only	O
deﬁne	O
the	O
conditional	B
importance	O
distribution	B
q	O
(	O
xt|x1	O
:	O
t−1	O
)	O
.	O
the	O
ideal	O
setting	O
of	O
the	O
sequential	O
importance	O
distribution	O
is	O
q	O
=	O
p	O
and	O
q	O
(	O
xt|x1	O
:	O
t−1	O
)	O
=	O
p	O
(	O
xt|x1	O
:	O
t−1	O
)	O
,	O
although	O
this	O
choice	O
is	O
impractical	O
in	O
most	O
cases	O
.	O
508	O
draft	O
march	O
9	O
,	O
2010	O
importance	B
sampling	O
h1	O
v1	O
h2	O
v2	O
h3	O
v3	O
h4	O
v4	O
figure	O
27.14	O
:	O
a	O
dynamic	B
bayesian	O
network	O
.	O
in	O
many	O
applications	O
of	O
interest	O
,	O
the	O
emission	O
distribu-	O
tion	O
p	O
(	O
vt|ht	O
)	O
is	O
non-gaussian	O
,	O
leading	O
to	O
the	O
formal	O
intractability	O
of	O
ﬁltering/smoothing	O
.	O
for	O
dynamic	B
bayes	O
networks	O
,	O
equation	B
(	O
27.6.8	O
)	O
will	O
simplify	O
considerably	O
.	O
for	O
example	O
consider	O
distribu-	O
tions	O
with	O
a	O
hidden	B
markov	O
independence	B
structure	O
,	O
p	O
(	O
v1	O
:	O
t	O
,	O
h1	O
:	O
t	O
)	O
=	O
p	O
(	O
v1|h1	O
)	O
p	O
(	O
h1	O
)	O
p	O
(	O
vt|ht	O
)	O
p	O
(	O
ht|ht−1	O
)	O
(	O
27.6.9	O
)	O
t	O
(	O
cid:89	O
)	O
t=2	O
where	O
v1	O
:	O
t	O
are	O
observations	O
and	O
h1	O
:	O
t	O
are	O
the	O
random	O
variables	O
.	O
a	O
cancelation	O
of	O
terms	O
in	O
the	O
numerator	O
and	O
denominator	O
occurs	O
,	O
leaving	O
simply	O
αl	O
t	O
≡	O
t−1	O
)	O
p	O
(	O
vt|hl	O
q	O
(	O
hl	O
t	O
)	O
p	O
(	O
hl	O
t|hl	O
t|hl	O
1	O
:	O
t	O
)	O
sequential	B
importance	I
sampling	I
is	O
also	O
known	O
as	O
particle	O
ﬁltering	B
.	O
(	O
27.6.10	O
)	O
particularly	O
in	O
cases	O
where	O
the	O
transition	O
is	O
easy	O
to	O
sample	B
from	O
,	O
a	O
common	O
sequential	O
importance	O
distri-	O
bution	O
is	O
q	O
(	O
ht|h1	O
:	O
t−1	O
)	O
=	O
p	O
(	O
ht|ht−1	O
)	O
in	O
which	O
case	O
,	O
from	O
equation	B
(	O
27.6.8	O
)	O
,	O
αl	O
by	O
(	O
27.6.11	O
)	O
t	O
=	O
p	O
(	O
vt|ht	O
)	O
and	O
the	O
unnormalised	O
weights	O
are	O
recursively	O
deﬁned	O
˜wl	O
t	O
=	O
˜wl	O
t−1p	O
(	O
vt|hl	O
t	O
)	O
(	O
27.6.12	O
)	O
a	O
drawback	O
of	O
this	O
procedure	O
is	O
that	O
after	O
a	O
small	O
number	O
of	O
iterations	O
only	O
very	O
few	O
particle	O
weights	O
will	O
be	O
signiﬁcantly	O
non-zero	O
due	O
to	O
the	O
mismatch	O
between	O
the	O
importance	B
distribution	O
q	O
and	O
the	O
target	O
distribution	B
p.	O
this	O
can	O
be	O
addressed	O
using	O
resampling	B
,	O
as	O
described	O
in	O
section	O
(	O
27.6	O
)	O
[	O
82	O
]	O
.	O
27.6.2	O
particle	O
ﬁltering	O
as	O
an	O
approximate	B
forward	O
pass	O
particle	O
ﬁltering	O
can	O
be	O
viewed	O
as	O
an	O
approximation	B
to	O
the	O
exact	O
ﬁltering	O
recursion	O
.	O
using	O
ρ	O
to	O
represent	O
the	O
ﬁltered	O
distribution	B
,	O
ρ	O
(	O
ht	O
)	O
∝	O
p	O
(	O
ht|v1	O
:	O
t	O
)	O
(	O
cid:90	O
)	O
the	O
exact	O
ﬁltering	O
recursion	O
is	O
(	O
27.6.13	O
)	O
ρ	O
(	O
ht	O
)	O
∝	O
p	O
(	O
vt|ht	O
)	O
p	O
(	O
ht|ht−1	O
)	O
ρ	O
(	O
ht−1	O
)	O
ht−1	O
a	O
pf	O
can	O
be	O
viewed	O
as	O
an	O
approximation	B
of	O
equation	B
(	O
27.6.14	O
)	O
in	O
which	O
the	O
message	B
ρ	O
(	O
ht−1	O
)	O
is	O
approxi-	O
mated	O
by	O
a	O
sum	O
of	O
δ-peaks	O
:	O
(	O
27.6.14	O
)	O
(	O
27.6.15	O
)	O
l	O
(	O
cid:88	O
)	O
(	O
cid:17	O
)	O
ρ	O
(	O
ht−1	O
)	O
≈	O
wl	O
t−1δ	O
ht−1	O
,	O
hl	O
t−1	O
t−1	O
are	O
the	O
normalised	B
importance	I
weights	I
(	O
cid:80	O
)	O
l	O
l=1	O
(	O
cid:16	O
)	O
l	O
(	O
cid:88	O
)	O
l=1	O
where	O
wl	O
t−1	O
are	O
the	O
particles	O
.	O
in	O
other	O
words	O
,	O
the	O
ρ	O
message	B
is	O
represented	O
as	O
a	O
weighted	O
mixture	O
of	O
delta-spikes	O
where	O
the	O
weight	B
and	O
position	O
of	O
the	O
spikes	O
are	O
the	O
parameters	O
of	O
the	O
distribution	B
.	O
using	O
equation	B
(	O
27.6.15	O
)	O
in	O
equation	B
(	O
27.6.14	O
)	O
,	O
we	O
have	O
t−1	O
=	O
1	O
,	O
and	O
hl	O
l=1	O
wl	O
ρ	O
(	O
ht	O
)	O
=	O
1	O
z	O
p	O
(	O
vt|ht	O
)	O
draft	O
march	O
9	O
,	O
2010	O
p	O
(	O
ht|hl	O
t−1	O
)	O
wl	O
t−1	O
(	O
27.6.16	O
)	O
509	O
importance	B
sampling	O
the	O
constant	O
z	O
is	O
used	O
to	O
normalise	O
the	O
distribution	B
ρ	O
(	O
ht	O
)	O
.	O
although	O
ρ	O
(	O
ht−1	O
)	O
was	O
a	O
simple	O
sum	O
of	O
delta	O
peaks	O
,	O
in	O
general	O
ρ	O
(	O
ht	O
)	O
will	O
not	O
be	O
–	O
the	O
delta-peaks	O
get	O
‘	O
broadened	O
’	O
by	O
the	O
hidden-to-hidden	O
and	O
hidden-	O
to-observation	O
factors	O
.	O
our	O
task	O
is	O
then	O
to	O
approximate	B
ρ	O
(	O
ht	O
)	O
as	O
a	O
new	O
sum	O
of	O
delta-peaks	O
.	O
below	O
we	O
discuss	O
a	O
method	O
to	O
achieve	O
this	O
for	O
which	O
explicit	O
knowledge	O
of	O
the	O
normalisation	B
z	O
is	O
not	O
required	O
.	O
this	O
is	O
useful	O
since	O
in	O
many	O
tracking	O
applications	O
the	O
normalisation	B
of	O
the	O
emission	O
p	O
(	O
vt|ht	O
)	O
is	O
unknown	O
.	O
a	O
monte-carlo	O
sampling	B
approximation	O
a	O
simple	O
approach	O
to	O
forming	O
an	O
approximate	B
mixture-of-delta	O
functions	O
representation	B
of	O
equation	B
(	O
27.6.16	O
)	O
is	O
to	O
generate	O
a	O
set	O
of	O
sample	B
points	O
using	O
importance	B
sampling	O
.	O
that	O
is	O
we	O
generate	O
a	O
set	O
from	O
some	O
importance	B
distribution	O
q	O
(	O
ht	O
)	O
which	O
gives	O
the	O
unnormalised	O
importance	B
of	O
samples	O
h1	O
weights	O
t	O
,	O
.	O
.	O
.	O
,	O
hl	O
t	O
(	O
27.6.17	O
)	O
(	O
27.6.18	O
)	O
(	O
27.6.19	O
)	O
t	O
)	O
(	O
cid:80	O
)	O
l	O
p	O
(	O
vt|hl	O
˜wl	O
t	O
=	O
t−1	O
)	O
wl	O
(	O
cid:48	O
)	O
t|hl	O
(	O
cid:48	O
)	O
t−1	O
l	O
(	O
cid:48	O
)	O
=1	O
p	O
(	O
hl	O
q	O
(	O
hl	O
t	O
)	O
deﬁning	O
the	O
normalised	O
weights	O
:	O
we	O
obtain	O
an	O
approximation	B
wl	O
t	O
=	O
t	O
˜wl	O
l	O
(	O
cid:48	O
)	O
˜wl	O
(	O
cid:48	O
)	O
t	O
(	O
cid:80	O
)	O
l	O
(	O
cid:88	O
)	O
l=1	O
ρ	O
(	O
ht	O
)	O
≈	O
wl	O
tδ	O
ht	O
,	O
hl	O
t	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
l	O
(	O
cid:88	O
)	O
l=1	O
ideally	O
one	O
would	O
use	O
the	O
importance	B
distribution	O
that	O
makes	O
the	O
importance	B
weights	O
unity	O
,	O
namely	O
q	O
(	O
ht	O
)	O
∝	O
p	O
(	O
vt|ht	O
)	O
p	O
(	O
ht|hl	O
t−1	O
)	O
wl	O
t−1	O
(	O
27.6.20	O
)	O
l	O
(	O
cid:88	O
)	O
l=1	O
however	O
,	O
this	O
is	O
often	O
diﬃcult	O
to	O
sample	B
from	O
directly	O
due	O
to	O
the	O
unknown	O
normalisation	B
of	O
the	O
emission	O
p	O
(	O
vt|ht	O
)	O
.	O
a	O
simpler	O
alternative	O
is	O
to	O
sample	B
from	O
the	O
transition	O
mixture	O
:	O
q	O
(	O
ht	O
)	O
=	O
p	O
(	O
ht|hl	O
t−1	O
)	O
wl	O
t−1	O
(	O
27.6.21	O
)	O
to	O
do	O
so	O
,	O
one	O
ﬁrst	O
samples	O
a	O
component	O
l∗	O
from	O
the	O
histogram	O
with	O
weights	O
from	O
w1	O
this	O
sample	B
index	O
,	O
say	O
l∗	O
,	O
one	O
then	O
draws	O
a	O
sample	B
from	O
p	O
(	O
ht|hl∗	O
t−1	O
.	O
given	O
t−1	O
)	O
.	O
in	O
this	O
case	O
the	O
un-normalised	O
weights	O
t−1	O
,	O
.	O
.	O
.	O
,	O
wl	O
become	O
simply	O
t	O
=	O
p	O
(	O
vt|hl	O
˜wl	O
t	O
)	O
(	O
27.6.22	O
)	O
this	O
forward-sampling-resampling	O
procedure	O
is	O
used	O
in	O
demoparticlefilter.m	O
and	O
in	O
the	O
following	O
toy	O
example	O
.	O
example	O
113	O
(	O
a	O
toy	O
face-tracking	O
example	O
)	O
.	O
at	O
time	O
t	O
a	O
binary	O
face	O
template	O
is	O
in	O
a	O
location	O
ht	O
,	O
which	O
describes	O
the	O
upper-left	O
corner	O
of	O
the	O
template	O
using	O
a	O
two-dimensional	O
vector	O
.	O
at	O
time	O
t	O
=	O
1	O
the	O
position	O
of	O
the	O
face	O
is	O
known	O
,	O
see	O
ﬁg	O
(	O
27.15a	O
)	O
.	O
the	O
face	O
template	O
is	O
known	O
.	O
in	O
subsequent	O
times	O
the	O
face	O
moves	O
randomly	O
according	O
to	O
ht	O
=	O
ht−1	O
+	O
σηt	O
(	O
27.6.23	O
)	O
where	O
ηt	O
∼	O
n	O
(	O
ηt	O
0	O
,	O
i	O
)	O
is	O
a	O
two	O
dimensional	O
zero	O
mean	B
unit	O
covariance	B
noise	O
vector	O
.	O
in	O
addition	O
,	O
a	O
fraction	O
of	O
the	O
binary	O
pixels	O
in	O
the	O
whole	O
image	O
are	O
selected	O
at	O
random	O
and	O
their	O
states	O
ﬂipped	O
.	O
the	O
aim	O
510	O
draft	O
march	O
9	O
,	O
2010	O
importance	B
sampling	O
figure	O
27.15	O
:	O
tracking	O
an	O
object	O
with	O
a	O
particle	B
ﬁlter	I
containing	O
50	O
particles	O
.	O
the	O
small	O
circles	O
are	O
the	O
particles	O
,	O
scaled	O
by	O
their	O
weights	O
.	O
the	O
correct	O
corner	O
position	O
of	O
the	O
face	O
is	O
given	O
by	O
the	O
‘	O
×	O
’	O
,	O
the	O
ﬁl-	O
tered	O
average	B
by	O
the	O
large	O
circle	O
‘	O
o	O
’	O
,	O
and	O
the	O
most	O
likely	O
particle	O
by	O
‘	O
+	O
’	O
.	O
initial	O
position	O
of	O
the	O
face	O
without	O
noise	O
and	O
corre-	O
sponding	O
weights	O
of	O
the	O
particles	O
.	O
(	O
b	O
)	O
:	O
face	O
with	O
noisy	O
background	O
and	O
the	O
tracked	O
corner	O
position	O
af-	O
ter	O
20	O
timesteps	O
.	O
the	O
forward-	O
sampling-resampling	O
pf	O
method	O
is	O
used	O
to	O
maintain	O
a	O
healthy	O
pro-	O
portion	O
of	O
non-zero	O
weights	O
.	O
see	O
demoparticlefilter.m	O
(	O
a	O
)	O
:	O
(	O
a	O
)	O
(	O
b	O
)	O
is	O
to	O
try	O
to	O
track	O
the	O
upper-left	O
corner	O
of	O
the	O
face	O
through	O
time	O
.	O
we	O
need	O
to	O
deﬁne	O
the	O
emission	B
distribution	I
p	O
(	O
vt|ht	O
)	O
on	O
the	O
binary	O
pixels	O
with	O
vi	O
∈	O
{	O
0	O
,	O
1	O
}	O
.	O
consider	O
the	O
following	O
compatibility	B
function	I
φ	O
(	O
vt	O
,	O
ht	O
)	O
=	O
vt	O
t	O
˜v	O
(	O
ht	O
)	O
(	O
27.6.24	O
)	O
where	O
˜v	O
(	O
ht	O
)	O
is	O
the	O
vector	O
representing	O
the	O
image	O
with	O
a	O
clean	O
face	O
placed	O
at	O
position	O
ht	O
.	O
this	O
measures	O
the	O
overlap	O
between	O
the	O
face	O
template	O
and	O
the	O
noisy	O
image	O
restricted	B
to	O
the	O
template	O
pixels	O
.	O
the	O
compatibility	B
function	I
is	O
maximal	O
when	O
the	O
observed	O
image	O
vt	O
has	O
the	O
face	O
placed	O
at	O
position	O
ht	O
.	O
we	O
can	O
therefore	O
tentatively	O
deﬁne	O
p	O
(	O
vt|ht	O
)	O
∝	O
φ	O
(	O
vt	O
,	O
ht	O
)	O
(	O
27.6.25	O
)	O
a	O
subtlety	O
is	O
that	O
ht	O
is	O
continuous	B
,	O
and	O
in	O
the	O
compatibility	B
function	I
we	O
ﬁrst	O
map	O
ht	O
to	O
the	O
nearest	O
integer	O
pixel	O
representation	B
.	O
we	O
have	O
not	O
speciﬁed	O
the	O
normalisation	B
constant	I
of	O
this	O
distribution	B
,	O
which	O
fortunately	O
this	O
is	O
not	O
required	O
by	O
the	O
particle	O
ﬁltering	O
algorithm	B
.	O
in	O
ﬁg	O
(	O
27.15a	O
)	O
50	O
particles	O
are	O
used	O
to	O
track	O
the	O
face	O
.	O
the	O
particles	O
are	O
plotted	O
along	O
with	O
their	O
corresponding	O
weights	O
.	O
for	O
each	O
t	O
>	O
1	O
,	O
5	O
%	O
of	O
the	O
pixels	O
are	O
selected	O
at	O
random	O
in	O
the	O
image	O
and	O
their	O
states	O
ﬂipped	O
.	O
using	O
the	O
forward-sampling-	O
resampling	B
method	O
we	O
can	O
successfully	O
track	O
the	O
face	O
despite	O
the	O
presence	O
of	O
the	O
background	O
clutter	O
.	O
real	O
tracking	O
applications	O
involve	O
complex	O
issues	O
,	O
including	O
tracking	O
multiple	O
objects	O
,	O
transformations	O
of	O
the	O
object	O
(	O
scaling	O
,	O
rotation	O
,	O
morphology	O
changes	O
)	O
.	O
nevertheless	O
,	O
the	O
principles	O
are	O
largely	O
the	O
same	O
and	O
many	O
tracking	O
applications	O
work	O
by	O
seeking	O
simple	O
compatibility	O
functions	O
,	O
often	O
based	O
on	O
the	O
colour	O
histogram	O
in	O
a	O
template	O
.	O
indeed	O
,	O
tracking	O
objects	O
in	O
complex	O
environments	O
was	O
one	O
of	O
the	O
original	O
appli-	O
cations	O
of	O
particle	O
ﬁlters	O
[	O
140	O
]	O
.	O
draft	O
march	O
9	O
,	O
2010	O
511	O
510152025303540510152025303540102030405000.010.020.030.040.050.060.07particle	O
weights510152025303540510152025303540102030405000.010.020.030.040.050.060.070.080.09particle	O
weights	O
27.7	O
code	O
potsample.m	O
:	O
exact	O
sample	O
from	O
a	O
set	O
of	O
potentials	O
ancestralsample.m	O
:	O
ancestral	B
sample	O
from	O
a	O
belief	O
net	O
jtsample.m	O
:	O
sampling	B
from	O
a	O
consistent	B
junction	O
tree	B
gibbssample.m	O
:	O
gibbs	O
sampling	B
from	O
a	O
set	O
of	O
potentials	O
demometropolis.m	O
:	O
demo	O
of	O
metropolis	O
sampling	B
for	O
a	O
bimodal	O
distribution	B
metropolis.m	O
:	O
metropolis	O
sample	B
logp.m	O
:	O
log	O
of	O
a	O
bimodal	O
distribution	B
demoparticlefilter.m	O
:	O
demo	O
particle	O
filtering	O
(	O
forward-sampling-resampling	O
method	O
)	O
placeobject.m	O
:	O
place	O
an	O
object	O
in	O
a	O
grid	O
compat.m	O
:	O
compatibility	B
function	I
demosamplehmm.m	O
:	O
naive	O
gibbs	O
sampling	B
for	O
a	O
hmm	O
27.8	O
exercises	O
exercise	O
246	O
(	O
box-muller	O
method	O
)	O
.	O
let	O
x1	O
∼	O
u	O
(	O
x1|	O
[	O
0	O
,	O
1	O
]	O
)	O
,	O
x2	O
∼	O
u	O
(	O
x2|	O
[	O
0	O
,	O
1	O
]	O
)	O
and	O
y2	O
=	O
(	O
cid:112	O
)	O
−2	O
log	O
x2	O
sin	O
2πx2	O
y1	O
=	O
(	O
cid:112	O
)	O
show	O
that	O
−2	O
log	O
x1	O
cos	O
2πx2	O
,	O
(	O
cid:90	O
)	O
p	O
(	O
y1	O
,	O
y2	O
)	O
=	O
p	O
(	O
y1|x1	O
,	O
x2	O
)	O
p	O
(	O
y2|x1	O
,	O
x2	O
)	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2	O
)	O
dx1dx2	O
=	O
n	O
(	O
y1	O
0	O
,	O
1	O
)	O
n	O
(	O
y2	O
0	O
,	O
1	O
)	O
exercises	O
(	O
27.8.1	O
)	O
(	O
27.8.2	O
)	O
and	O
suggest	O
an	O
algorithm	B
to	O
sample	B
from	O
a	O
univariate	B
normal	O
distribution	B
.	O
exercise	O
247.	O
consider	O
the	O
distribution	B
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
x6	O
)	O
=	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2	O
)	O
p	O
(	O
x3|x1	O
,	O
x2	O
)	O
p	O
(	O
x4|x3	O
)	O
p	O
(	O
x5|x3	O
)	O
p	O
(	O
x6|x4	O
,	O
x5	O
)	O
(	O
27.8.3	O
)	O
for	O
x5	O
ﬁxed	O
in	O
a	O
given	O
state	O
x5	O
,	O
write	O
down	O
a	O
distribution	B
on	O
the	O
remaining	O
variables	O
p	O
(	O
cid:48	O
)	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
,	O
x6	O
)	O
and	O
explain	O
how	O
forward	O
(	O
ancestral	B
)	O
sampling	B
can	O
be	O
carried	O
out	O
for	O
this	O
new	O
distribution	B
.	O
exercise	O
248.	O
consider	O
an	O
ising	O
model	B
on	O
an	O
m	O
×	O
m	O
square	O
lattice	O
with	O
nearest	B
neighbour	I
interactions	O
:	O
(	O
27.8.4	O
)	O
i	O
[	O
xi	O
=	O
xj	O
]	O
(	O
cid:88	O
)	O
i∼j	O
p	O
(	O
x	O
)	O
∝	O
exp	O
β	O
now	O
consider	O
the	O
m	O
×	O
m	O
grid	O
as	O
a	O
checkerboard	B
,	O
and	O
give	O
each	O
white	O
square	O
a	O
label	O
wi	O
,	O
and	O
each	O
black	O
square	O
a	O
label	O
bj	O
,	O
so	O
that	O
each	O
square	O
is	O
associated	O
with	O
a	O
particular	O
variable	B
.	O
show	O
that	O
p	O
(	O
b1	O
,	O
b2	O
,	O
.	O
.	O
.	O
,	O
|w1	O
,	O
w2	O
,	O
.	O
.	O
.	O
)	O
=	O
p	O
(	O
b1|w1	O
,	O
w2	O
,	O
.	O
.	O
.	O
)	O
p	O
(	O
b2|w1	O
,	O
w2	O
,	O
.	O
.	O
.	O
)	O
.	O
.	O
.	O
that	O
is	O
,	O
conditioned	O
on	O
the	O
white	O
variables	O
,	O
the	O
black	O
variables	O
are	O
independent	O
.	O
the	O
converse	O
is	O
also	O
true	O
,	O
that	O
conditioned	O
on	O
the	O
black	O
variables	O
,	O
the	O
white	O
variables	O
are	O
independent	O
.	O
explain	O
how	O
this	O
can	O
be	O
exploited	O
by	O
a	O
gibbs	O
sampling	B
procedure	O
.	O
this	O
procedure	O
is	O
known	O
as	O
checkerboard	O
or	O
black	B
and	I
white	I
sampling	I
.	O
exercise	O
249.	O
consider	O
the	O
symmetric	O
gaussian	O
proposal	B
distribution	I
and	O
the	O
target	O
distribution	B
q	O
i	O
(	O
cid:1	O
)	O
(	O
cid:0	O
)	O
x	O
(	O
cid:48	O
)	O
x	O
,	O
σ2	O
(	O
cid:0	O
)	O
x	O
0	O
,	O
σ2	O
pi	O
(	O
cid:1	O
)	O
(	O
cid:29	O
)	O
=	O
−	O
n	O
σ2	O
q	O
2σ2	O
p	O
˜q	O
(	O
x	O
(	O
cid:48	O
)	O
|x	O
)	O
˜q	O
(	O
x	O
(	O
cid:48	O
)	O
|x	O
)	O
=	O
n	O
p	O
(	O
x	O
)	O
=	O
n	O
(	O
cid:28	O
)	O
log	O
p	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
p	O
(	O
x	O
)	O
where	O
dim	O
x	O
=	O
n.	O
show	O
that	O
(	O
27.8.5	O
)	O
(	O
27.8.6	O
)	O
(	O
27.8.7	O
)	O
(	O
27.8.8	O
)	O
discuss	O
how	O
this	O
result	O
relates	O
to	O
the	O
probability	B
of	O
accepting	O
a	O
metropolis-hastings	O
update	O
under	O
a	O
gaussian	O
proposal	B
distribution	I
in	O
high-dimensions	O
.	O
512	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
exercise	O
250.	O
the	O
ﬁle	O
demosamplehmm.m	O
performs	O
naive	O
gibbs	O
sampling	B
of	O
the	O
posterior	B
p	O
(	O
h1	O
:	O
t|v1	O
:	O
t	O
)	O
for	O
a	O
hmm	O
.	O
at	O
each	O
gibbs	O
update	O
a	O
single	O
variable	B
ht	O
is	O
chosen	O
,	O
with	O
the	O
remaining	O
h	O
variables	O
clamped	O
.	O
the	O
procedure	O
starts	O
from	O
t	O
=	O
1	O
and	O
sweeps	O
forwards	O
through	O
time	O
.	O
when	O
the	O
end	O
time	O
t	O
=	O
t	O
is	O
reached	O
,	O
the	O
joint	B
state	O
h1	O
:	O
t	O
is	O
taken	O
as	O
a	O
sample	B
from	O
the	O
posterior	B
.	O
the	O
parameter	B
λ	O
controls	O
how	O
deterministic	B
the	O
hidden	B
transition	O
matrix	B
p	O
(	O
ht|ht−1	O
)	O
will	O
be	O
.	O
adjust	O
demosamplehmm.m	O
to	O
run	O
100	O
times	O
,	O
each	O
time	O
for	O
the	O
same	O
λ	O
,	O
computing	O
a	O
mean	B
absolute	O
error	O
over	O
these	O
100	O
runs	O
.	O
then	O
repeat	O
this	O
for	O
λ	O
=	O
0.1	O
,	O
1	O
,	O
10	O
,	O
20.	O
discuss	O
why	O
the	O
performance	B
of	O
this	O
gibbs	O
sampling	B
routine	O
deteriorates	O
with	O
increasing	O
λ.	O
draft	O
march	O
9	O
,	O
2010	O
513	O
exercises	O
514	O
draft	O
march	O
9	O
,	O
2010	O
chapter	O
28	O
deterministic	B
approximate	O
inference	B
28.1	O
introduction	O
deterministic	B
approximate	O
inference	B
methods	O
are	O
an	O
alternative	O
to	O
the	O
stochastic	O
techniques	O
discussed	O
in	O
chapter	O
(	O
27	O
)	O
.	O
whilst	O
stochastic	O
methods	O
are	O
powerful	O
and	O
often	O
generally	O
applicable	O
,	O
they	O
nevertheless	O
produce	O
sample	B
estimates	O
of	O
a	O
quantity	O
.	O
even	O
if	O
we	O
are	O
able	O
to	O
perform	O
perfect	B
sampling	I
,	O
we	O
would	O
still	O
only	O
obtain	O
an	O
approximate	B
result	O
due	O
to	O
the	O
inherent	O
uncertainty	B
introduced	O
by	O
sampling	B
.	O
furthermore	O
,	O
in	O
practice	O
,	O
drawing	O
exact	O
samples	O
is	O
typically	O
computationally	O
intractable	O
and	O
assessing	O
the	O
quality	O
of	O
the	O
sample	B
estimates	O
is	O
diﬃcult	O
.	O
in	O
this	O
chapter	O
we	O
discuss	O
some	O
alternative	O
deterministic	B
approximate	O
inference	B
schemes	O
.	O
the	O
ﬁrst	O
,	O
laplace	O
’	O
s	O
method	O
,	O
is	O
a	O
simple	O
perturbation	O
technique	O
.	O
the	O
second	O
class	O
of	O
methods	O
are	O
those	O
that	O
produce	O
rigorous	O
bounds	O
on	O
quantities	O
of	O
interest	O
.	O
such	O
methods	O
are	O
interesting	O
since	O
they	O
provide	O
certain	O
knowledge	O
–	O
it	O
may	O
be	O
suﬃcient	O
,	O
for	O
example	O
,	O
to	O
show	O
that	O
a	O
marginal	B
prob-	O
ability	O
is	O
greater	O
than	O
0.1	O
in	O
order	O
to	O
make	O
an	O
informed	O
decision	O
.	O
a	O
further	O
class	O
of	O
methods	O
are	O
the	O
consistency	O
methods	O
,	O
such	O
as	O
loopy	O
belief	B
propagation	I
.	O
such	O
methods	O
have	O
revolutionised	O
certain	O
ﬁelds	O
,	O
including	O
error	O
correction	O
[	O
183	O
]	O
,	O
providing	O
performance	B
unobtainable	O
from	O
sampling	B
based	O
procedures	O
.	O
it	O
is	O
important	O
to	O
bear	O
in	O
mind	O
that	O
no	O
single	O
approximation	B
technique	O
,	O
deterministic	B
or	O
stochastic	O
,	O
is	O
going	O
to	O
beat	O
all	O
others	O
on	O
all	O
problems	O
,	O
given	O
the	O
same	O
computational	O
resources	O
.	O
in	O
this	O
sense	O
,	O
insight	O
as	O
to	O
the	O
properties	B
of	O
the	O
approximation	B
method	O
used	O
is	O
useful	O
in	O
matching	O
an	O
approximation	B
method	O
to	O
the	O
problem	B
at	O
hand	O
.	O
28.2	O
the	O
laplace	O
approximation	B
consider	O
a	O
distribution	B
on	O
a	O
continuous	B
variable	O
of	O
the	O
form	O
p	O
(	O
x	O
)	O
=	O
−e	O
(	O
x	O
)	O
1	O
z	O
e	O
(	O
28.2.1	O
)	O
the	O
laplace	O
method	O
makes	O
a	O
gaussian	O
approximation	B
of	O
p	O
(	O
x	O
)	O
based	O
on	O
a	O
local	B
perturbation	O
expansion	O
around	O
a	O
mode	B
x∗	O
.	O
first	O
we	O
ﬁnd	O
the	O
mode	B
numerically	O
,	O
giving	O
x∗	O
=	O
argmin	O
x	O
e	O
(	O
x	O
)	O
then	O
a	O
taylor	O
expansion	O
up	O
to	O
second	O
order	O
around	O
this	O
mode	B
gives	O
(	O
28.2.2	O
)	O
e	O
(	O
x	O
)	O
≈	O
e	O
(	O
x∗	O
)	O
+	O
(	O
x	O
−	O
x∗	O
)	O
t	O
∇e|x∗	O
+	O
(	O
28.2.3	O
)	O
where	O
h	O
≡	O
∇∇e	O
(	O
x	O
)	O
|x∗	O
is	O
the	O
hessian	O
evaluated	O
at	O
the	O
mode	B
.	O
at	O
the	O
mode	B
,	O
∇e|x∗	O
=	O
0	O
,	O
and	O
an	O
approxi-	O
mation	O
of	O
the	O
distribution	B
is	O
given	O
by	O
the	O
gaussian	O
1	O
2	O
(	O
x	O
−	O
x∗	O
)	O
t	O
h	O
(	O
x	O
−	O
x∗	O
)	O
(	O
cid:0	O
)	O
x	O
x∗	O
,	O
h−1	O
(	O
cid:1	O
)	O
515	O
∗	O
(	O
x	O
)	O
=	O
p	O
1	O
z∗	O
e	O
2	O
(	O
x−x∗	O
)	O
th	O
(	O
x−x∗	O
)	O
=	O
n	O
−	O
1	O
(	O
28.2.4	O
)	O
properties	B
of	O
kullback-leibler	O
variational	B
inference	I
figure	O
28.1	O
:	O
fitting	O
a	O
mixture	O
of	O
gaussians	O
p	O
(	O
x	O
)	O
(	O
blue	O
)	O
with	O
a	O
single	O
gaus-	O
sian	O
.	O
the	O
green	O
curve	O
minimises	O
kl	O
(	O
q|p	O
)	O
corresponding	O
to	O
ﬁtting	O
a	O
local	B
model	O
.	O
the	O
red	O
curve	O
minimises	O
kl	O
(	O
p|q	O
)	O
corresponding	O
to	O
moment	O
match-	O
ing	O
.	O
which	O
has	O
mean	B
x∗	O
and	O
covariance	B
h−1	O
,	O
with	O
z∗	O
=	O
(	O
cid:112	O
)	O
det	O
(	O
2πh−1	O
)	O
.	O
similarly	O
,	O
we	O
can	O
use	O
the	O
above	O
−e	O
(	O
x∗	O
)	O
(	O
cid:112	O
)	O
det	O
(	O
2πh−1	O
)	O
(	O
28.2.5	O
)	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
expansion	O
to	O
estimate	O
the	O
integral	O
e	O
−e	O
(	O
x	O
)	O
≈	O
x	O
x	O
−e	O
(	O
x∗	O
)	O
−	O
1	O
e	O
2	O
(	O
x−x∗	O
)	O
th	O
(	O
x−x∗	O
)	O
=	O
e	O
although	O
the	O
laplace	O
approximation	B
ﬁts	O
a	O
gaussian	O
to	O
a	O
distribution	B
,	O
it	O
is	O
not	O
necessarily	O
the	O
‘	O
best	O
’	O
gaussian	O
approximation	B
.	O
as	O
we	O
’	O
ll	O
see	O
below	O
,	O
other	O
criteria	O
,	O
such	O
as	O
based	O
on	O
minimal	O
kl	O
divergence	B
between	O
p	O
(	O
x	O
)	O
and	O
a	O
gaussian	O
approximation	B
may	O
be	O
more	O
appropriate	O
,	O
depending	O
on	O
the	O
context	O
.	O
a	O
beneﬁt	O
of	O
laplace	O
’	O
s	O
method	O
is	O
its	O
relative	O
speed	O
and	O
simplicity	O
compared	O
with	O
other	O
approximate	B
inference	I
techniques	O
.	O
28.3	O
properties	B
of	O
kullback-leibler	O
variational	B
inference	I
variational	O
methods	O
can	O
be	O
used	O
to	O
approximate	B
a	O
complex	O
distribution	B
p	O
(	O
x	O
)	O
by	O
a	O
simpler	O
distribution	B
q	O
(	O
x	O
)	O
.	O
given	O
a	O
deﬁnition	O
of	O
discrepancy	O
between	O
an	O
approximation	B
q	O
(	O
x	O
)	O
to	O
p	O
(	O
x	O
)	O
,	O
any	O
free	O
parameters	O
of	O
q	O
(	O
x	O
)	O
are	O
then	O
set	O
by	O
minimising	O
the	O
discrepancy	O
.	O
a	O
particularly	O
popular	O
measure	O
of	O
the	O
discrepancy	O
between	O
an	O
approximation	B
q	O
(	O
x	O
)	O
and	O
the	O
intractable	O
distribution	O
p	O
(	O
x	O
)	O
is	O
the	O
kullback-leibler	O
divergence	B
kl	O
(	O
q|p	O
)	O
=	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
cid:105	O
)	O
q	O
−	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
cid:105	O
)	O
q	O
(	O
28.3.1	O
)	O
it	O
is	O
straightforward	O
to	O
show	O
that	O
kl	O
(	O
q|p	O
)	O
≥	O
0	O
and	O
is	O
zero	O
if	O
and	O
only	O
if	O
the	O
distributions	O
p	O
and	O
q	O
are	O
identical	O
,	O
see	O
section	O
(	O
8.8	O
)	O
.	O
note	O
that	O
whilst	O
the	O
kl	O
divergence	B
can	O
not	O
be	O
negative	O
,	O
there	O
is	O
no	O
upper	O
bound	B
on	O
the	O
value	B
it	O
can	O
potentially	O
take	O
so	O
that	O
the	O
discrepancy	O
can	O
be	O
‘	O
inﬁnitely	O
’	O
bad	O
.	O
28.3.1	O
bounding	O
the	O
normalisation	B
constant	I
for	O
a	O
distribution	B
of	O
the	O
form	O
p	O
(	O
x	O
)	O
=	O
1	O
z	O
eφ	O
(	O
x	O
)	O
we	O
have	O
kl	O
(	O
q|p	O
)	O
=	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
−	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
=	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
−	O
(	O
cid:104	O
)	O
φ	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
+	O
log	O
z	O
since	O
kl	O
(	O
q|p	O
)	O
≥	O
0	O
this	O
immediately	O
gives	O
the	O
bound	B
(	O
cid:125	O
)	O
log	O
z	O
≥	O
−	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:124	O
)	O
entropy	B
(	O
cid:125	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:124	O
)	O
+	O
(	O
cid:104	O
)	O
φ	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
energy	B
(	O
28.3.2	O
)	O
(	O
28.3.3	O
)	O
(	O
28.3.4	O
)	O
which	O
is	O
called	O
the	O
‘	O
free	O
energy	B
’	O
bound	B
in	O
the	O
physics	O
community	O
[	O
236	O
]	O
.	O
using	O
the	O
notation	O
hq	O
for	O
the	O
entropy	B
of	O
q	O
,	O
we	O
can	O
write	O
the	O
bound	B
more	O
compactly	O
as	O
log	O
z	O
≥	O
hq	O
+	O
(	O
cid:104	O
)	O
φ	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
(	O
28.3.5	O
)	O
the	O
kl	O
(	O
q|p	O
)	O
method	O
provides	O
therefore	O
a	O
lower	O
bound	O
on	O
the	O
normalisation	B
constant	I
.	O
for	O
some	O
models	O
it	O
is	O
possible	O
(	O
using	O
alternative	O
methods	O
,	O
see	O
for	O
example	O
[	O
287	O
]	O
and	O
exercise	O
(	O
258	O
)	O
)	O
to	O
also	O
form	O
an	O
upper	O
516	O
draft	O
march	O
9	O
,	O
2010	O
−30−20−10010203000.050.10.150.20.250.30.350.4	O
properties	B
of	O
kullback-leibler	O
variational	B
inference	I
bound	O
on	O
the	O
normalisation	B
constant	I
.	O
with	O
both	O
an	O
upper	O
and	O
lower	O
bound	O
on	O
the	O
normalisation	B
terms	O
,	O
we	O
are	O
able	O
to	O
bracket	O
marginals	O
l	O
≤	O
p	O
(	O
xi	O
)	O
≤	O
u	O
,	O
see	O
exercise	O
(	O
261	O
)	O
.	O
the	O
tightness	O
of	O
the	O
resulting	O
bracket	O
gives	O
an	O
indication	O
as	O
to	O
how	O
tight	O
the	O
bounding	O
procedures	O
are	O
.	O
even	O
in	O
cases	O
where	O
the	O
resulting	O
bracket	O
is	O
weak	O
–	O
for	O
example	O
it	O
might	O
be	O
that	O
the	O
result	O
is	O
that	O
0.1	O
<	O
p	O
(	O
cancer	O
=	O
true	O
)	O
<	O
0.99	O
,	O
this	O
may	O
be	O
suﬃcient	O
for	O
decision	O
making	O
purposes	O
since	O
the	O
probability	B
of	O
cancer	O
is	O
suﬃciently	O
large	O
to	O
merit	O
action	O
.	O
28.3.2	O
bounding	O
the	O
marginal	B
likelihood	I
in	O
bayesian	O
modelling	B
the	O
likelihood	B
of	O
the	O
model	B
m	O
with	O
parameters	O
θ	O
generating	O
data	B
d	O
is	O
given	O
by	O
(	O
cid:90	O
)	O
p	O
(	O
d|m	O
)	O
=	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
p	O
(	O
d|θ	O
,	O
m	O
)	O
likelihood	B
θ	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
p	O
(	O
θ|m	O
)	O
prior	B
(	O
28.3.6	O
)	O
this	O
quantity	O
is	O
fundamental	O
to	O
model	B
comparison	O
.	O
however	O
,	O
in	O
cases	O
where	O
θ	O
is	O
high-dimensional	O
,	O
the	O
integral	O
over	O
θ	O
is	O
diﬃcult	O
to	O
perform	O
.	O
using	O
bayes	O
’	O
rule	O
,	O
p	O
(	O
θ|d	O
,	O
m	O
)	O
=	O
p	O
(	O
d|θ	O
,	O
m	O
)	O
p	O
(	O
θ|m	O
)	O
p	O
(	O
d|m	O
)	O
and	O
considering	O
kl	O
(	O
q	O
(	O
θ	O
)	O
|p	O
(	O
θ|d	O
,	O
m	O
)	O
)	O
=	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
θ	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
θ	O
)	O
−	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
θ|d	O
,	O
m	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
θ	O
)	O
=	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
θ	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
θ	O
)	O
−	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
d|θ	O
,	O
m	O
)	O
p	O
(	O
θ|m	O
(	O
cid:105	O
)	O
q	O
(	O
θ	O
)	O
+	O
log	O
p	O
(	O
d|m	O
)	O
the	O
non-negativity	O
of	O
the	O
kullback-leibler	O
divergence	B
gives	O
the	O
bound	B
log	O
p	O
(	O
d|m	O
)	O
≥	O
−	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
θ	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
θ	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
d|θ	O
,	O
m	O
)	O
p	O
(	O
θ|m	O
(	O
cid:105	O
)	O
q	O
(	O
θ	O
)	O
(	O
28.3.7	O
)	O
(	O
28.3.8	O
)	O
(	O
28.3.9	O
)	O
(	O
28.3.10	O
)	O
this	O
bound	B
holds	O
for	O
any	O
distribution	B
q	O
(	O
θ	O
)	O
and	O
saturates	O
when	O
q	O
(	O
θ	O
)	O
=	O
p	O
(	O
θ|d	O
,	O
m	O
)	O
.	O
since	O
using	O
the	O
optimal	O
setting	O
is	O
assumed	O
computationally	O
intractable	O
,	O
the	O
idea	O
in	O
variational	O
bounding	O
is	O
to	O
choose	O
a	O
distribution	B
family	O
for	O
q	O
(	O
θ	O
)	O
for	O
which	O
the	O
bound	B
is	O
computationally	O
tractable	O
(	O
for	O
example	O
factorised	B
,	O
or	O
gaussian	O
)	O
,	O
and	O
then	O
maximise	O
the	O
bound	B
with	O
respect	O
to	O
any	O
free	O
parameters	O
of	O
q	O
(	O
θ	O
)	O
.	O
the	O
resulting	O
bound	B
then	O
can	O
be	O
used	O
as	O
a	O
surrogate	O
for	O
the	O
exact	O
marginal	O
likelihood	B
in	O
model	B
comparison	O
.	O
28.3.3	O
gaussian	O
approximations	O
using	O
kl	O
divergence	B
minimising	O
kl	O
(	O
q|p	O
)	O
using	O
a	O
simple	O
approximation	O
q	O
(	O
x	O
)	O
of	O
a	O
more	O
complex	O
distribution	B
p	O
(	O
x	O
)	O
by	O
minimising	O
kl	O
(	O
q|p	O
)	O
tends	O
to	O
give	O
a	O
solution	O
for	O
q	O
(	O
x	O
)	O
that	O
focuses	O
on	O
a	O
local	B
mode	O
of	O
p	O
(	O
x	O
)	O
,	O
thereby	O
underestimating	O
the	O
variance	B
of	O
p	O
(	O
x	O
)	O
.	O
to	O
show	O
this	O
,	O
consider	O
approximating	O
a	O
mixture	O
of	O
two	O
gaussians	O
with	O
equal	O
variance	B
σ2	O
,	O
(	O
cid:0	O
)	O
x	O
µ	O
,	O
σ2	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
n	O
(	O
cid:0	O
)	O
x	O
−	O
µ	O
,	O
σ2	O
(	O
cid:1	O
)	O
+	O
n	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
x	O
m	O
,	O
s2	O
(	O
cid:1	O
)	O
p	O
(	O
x	O
)	O
=	O
1	O
2	O
q	O
(	O
x	O
)	O
=	O
n	O
see	O
ﬁg	O
(	O
28.1	O
)	O
,	O
with	O
a	O
single	O
gaussian	O
we	O
wish	O
to	O
ﬁnd	O
the	O
optimal	O
m	O
,	O
s2	O
that	O
minimise	O
kl	O
(	O
q|p	O
)	O
=	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
−	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
(	O
28.3.11	O
)	O
(	O
28.3.12	O
)	O
(	O
28.3.13	O
)	O
if	O
we	O
consider	O
the	O
case	O
that	O
the	O
two	O
gaussian	O
components	O
of	O
p	O
(	O
x	O
)	O
are	O
well	O
separated	O
,	O
µ	O
(	O
cid:29	O
)	O
σ	O
,	O
then	O
setting	O
q	O
(	O
x	O
)	O
to	O
be	O
centred	O
on	O
the	O
left	O
mode	B
at	O
−µ	O
the	O
gaussian	O
q	O
(	O
x	O
)	O
only	O
has	O
appreciable	O
mass	O
close	O
to	O
−µ	O
,	O
so	O
that	O
the	O
second	O
mode	B
at	O
µ	O
has	O
negligible	O
contribution	O
to	O
the	O
kullback-leibler	O
divergence	B
.	O
in	O
this	O
sense	O
one	O
can	O
approximate	B
p	O
(	O
x	O
)	O
≈	O
1	O
2	O
q	O
(	O
x	O
)	O
,	O
so	O
that	O
kl	O
(	O
q|p	O
)	O
≈	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
−	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
=	O
log	O
2	O
draft	O
march	O
9	O
,	O
2010	O
(	O
28.3.14	O
)	O
517	O
properties	B
of	O
kullback-leibler	O
variational	B
inference	I
representing	O
a	O
distribution	B
of	O
the	O
form	O
(	O
cid:81	O
)	O
figure	O
28.2	O
:	O
a	O
planar	O
pairwise	B
markov	O
random	B
ﬁeld	I
on	O
a	O
set	O
of	O
variables	O
x1	O
,	O
.	O
.	O
.	O
,	O
x25	O
,	O
in	O
statistical	O
physics	O
such	O
lattice	O
models	O
include	O
the	O
ising	O
model	B
on	O
binary	O
‘	O
spin	O
’	O
variables	O
xi	O
∈	O
{	O
+1	O
,	O
−1	O
}	O
with	O
φ	O
(	O
xi	O
,	O
xj	O
)	O
=	O
ewij	O
xixj	O
.	O
i∼j	O
φ	O
(	O
xi	O
,	O
xj	O
)	O
.	O
on	O
the	O
other	O
hand	O
,	O
setting	O
m	O
=	O
0	O
,	O
which	O
is	O
the	O
correct	O
mean	B
of	O
the	O
distribution	B
p	O
(	O
x	O
)	O
,	O
very	O
little	O
of	O
the	O
mass	O
of	O
the	O
mixture	B
is	O
captured	O
unless	O
s2	O
is	O
large	O
,	O
giving	O
a	O
poor	O
ﬁt	O
and	O
large	O
kl	O
divergence	B
.	O
another	O
way	O
to	O
view	O
this	O
is	O
to	O
consider	O
kl	O
(	O
q|p	O
)	O
=	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
x	O
)	O
/p	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
;	O
provided	O
q	O
is	O
close	O
to	O
p	O
around	O
where	O
q	O
has	O
signiﬁcant	O
mass	O
,	O
the	O
ratio	O
q	O
(	O
x	O
)	O
/p	O
(	O
x	O
)	O
will	O
be	O
order	O
1	O
and	O
the	O
kl	O
divergence	B
small	O
.	O
setting	O
m	O
=	O
0	O
means	O
that	O
q	O
(	O
x	O
)	O
/p	O
(	O
x	O
)	O
is	O
large	O
where	O
q	O
has	O
signiﬁcant	O
mass	O
,	O
and	O
is	O
therefore	O
a	O
poor	O
ﬁt	O
.	O
the	O
optimal	O
solution	O
in	O
this	O
case	O
is	O
to	O
place	O
the	O
gaussian	O
close	O
to	O
a	O
single	O
mode	B
.	O
note	O
,	O
however	O
,	O
that	O
for	O
two	O
modes	O
that	O
are	O
less	O
well-separated	O
,	O
the	O
optimal	O
solution	O
will	O
not	O
necessarily	O
be	O
to	O
place	O
the	O
gaussian	O
around	O
a	O
local	B
mode	O
.	O
in	O
general	O
,	O
the	O
optimal	O
gaussian	O
ﬁt	O
needs	O
to	O
be	O
determined	O
numerically	O
–	O
that	O
is	O
,	O
there	O
is	O
no	O
closed	O
form	O
solution	O
to	O
ﬁnding	O
the	O
optimal	O
mean	B
and	O
(	O
co	O
)	O
variance	B
parameters	O
.	O
minimising	O
kl	O
(	O
p|q	O
)	O
for	O
ﬁtting	O
a	O
gaussian	O
q	O
to	O
p	O
based	O
on	O
kl	O
(	O
p|q	O
)	O
,	O
we	O
have	O
log	O
det	O
(	O
cid:0	O
)	O
σ2	O
(	O
cid:1	O
)	O
+	O
const	O
.	O
kl	O
(	O
p|q	O
)	O
=	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
)	O
−	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
)	O
1	O
2σ2	O
=	O
−	O
1	O
2	O
p	O
(	O
x	O
)	O
−	O
(	O
cid:68	O
)	O
(	O
x	O
−	O
m	O
)	O
2	O
(	O
cid:69	O
)	O
σ2	O
=	O
(	O
cid:10	O
)	O
(	O
x	O
−	O
m	O
)	O
2	O
(	O
cid:11	O
)	O
p	O
(	O
x	O
)	O
minimising	O
this	O
with	O
respect	O
to	O
m	O
and	O
σ2	O
we	O
obtain	O
:	O
m	O
=	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
)	O
,	O
so	O
that	O
the	O
optimal	O
gaussian	O
ﬁt	O
matches	O
the	O
ﬁrst	O
and	O
second	O
moments	O
of	O
p	O
(	O
x	O
)	O
.	O
in	O
the	O
case	O
of	O
ﬁg	O
(	O
28.1	O
)	O
,	O
the	O
mean	B
of	O
p	O
(	O
x	O
)	O
is	O
a	O
zero	O
,	O
and	O
the	O
variance	B
of	O
p	O
(	O
x	O
)	O
is	O
large	O
.	O
this	O
solution	O
is	O
therefore	O
dramatically	O
diﬀerent	O
from	O
that	O
produced	O
by	O
ﬁtting	O
the	O
gaussian	O
using	O
kl	O
(	O
q|p	O
)	O
.	O
the	O
ﬁt	O
found	O
using	O
kl	O
(	O
q|p	O
)	O
focusses	O
on	O
making	O
q	O
ﬁt	O
p	O
locally	O
well	O
,	O
whereas	O
kl	O
(	O
p|q	O
)	O
focusses	O
on	O
making	O
q	O
ﬁt	O
p	O
well	O
to	O
the	O
global	B
statistics	O
of	O
the	O
distribution	B
(	O
possibly	O
at	O
the	O
expense	O
of	O
a	O
good	O
local	B
match	O
)	O
.	O
for	O
simplicity	O
,	O
consider	O
a	O
factorised	B
approximation	O
q	O
(	O
x	O
)	O
=	O
(	O
cid:81	O
)	O
28.3.4	O
moment	O
matching	O
properties	B
of	O
minimising	O
kl	O
(	O
p|q	O
)	O
i	O
q	O
(	O
xi	O
)	O
.	O
then	O
(	O
28.3.15	O
)	O
(	O
28.3.16	O
)	O
(	O
28.3.17	O
)	O
(	O
28.3.19	O
)	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
xi	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
xi	O
)	O
(	O
28.3.18	O
)	O
the	O
ﬁrst	O
entropic	O
term	O
is	O
independent	O
of	O
q	O
(	O
x	O
)	O
so	O
that	O
up	O
to	O
a	O
constant	O
independent	O
of	O
q	O
(	O
x	O
)	O
,	O
the	O
above	O
is	O
(	O
cid:88	O
)	O
i	O
kl	O
(	O
p|q	O
)	O
=	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
)	O
−	O
(	O
cid:88	O
)	O
kl	O
(	O
p	O
(	O
xi	O
)	O
|q	O
(	O
xi	O
)	O
)	O
i	O
so	O
that	O
optimally	O
q	O
(	O
xi	O
)	O
=	O
p	O
(	O
xi	O
)	O
.	O
that	O
is	O
,	O
the	O
optimal	O
factorised	B
approximation	O
is	O
to	O
set	O
the	O
factors	O
of	O
q	O
(	O
xi	O
)	O
to	O
the	O
marginals	O
of	O
p	O
(	O
xi	O
)	O
,	O
exercise	O
(	O
265	O
)	O
.	O
for	O
any	O
approximating	O
distribution	B
in	O
the	O
exponential	B
family	I
,	O
minimising	O
kl	O
(	O
p|q	O
)	O
corresponds	O
to	O
moment	O
matching	O
,	O
see	O
exercise	O
(	O
264	O
)	O
.	O
in	O
practice	O
,	O
one	O
generally	O
can	O
not	O
compute	O
the	O
moments	O
of	O
p	O
(	O
x	O
)	O
(	O
since	O
the	O
distribution	B
p	O
(	O
x	O
)	O
is	O
considered	O
‘	O
intractable	O
’	O
)	O
,	O
so	O
that	O
ﬁtting	O
q	O
to	O
p	O
based	O
only	O
on	O
kl	O
(	O
p|q	O
)	O
does	O
not	O
itself	O
lead	O
to	O
a	O
practical	O
algorithm	B
for	O
approximate	B
inference	I
.	O
neverthe-	O
less	O
,	O
as	O
we	O
will	O
see	O
,	O
it	O
is	O
a	O
useful	O
subroutine	O
for	O
local	B
approximations	O
,	O
in	O
particular	O
expectation	B
propagation	I
.	O
518	O
draft	O
march	O
9	O
,	O
2010	O
variational	O
bounding	O
using	O
kl	O
(	O
q|p	O
)	O
28.4	O
variational	O
bounding	O
using	O
kl	O
(	O
q|p	O
)	O
in	O
this	O
section	O
we	O
discuss	O
how	O
to	O
ﬁt	O
a	O
distribution	B
q	O
(	O
x	O
)	O
from	O
some	O
assumed	O
family	B
to	O
an	O
‘	O
intractable	O
’	O
distribution	B
p	O
(	O
x	O
)	O
.	O
as	O
we	O
saw	O
above	O
for	O
the	O
case	O
of	O
ﬁtting	O
gaussians	O
,	O
the	O
optimal	O
q	O
needs	O
to	O
be	O
found	O
numerically	O
.	O
this	O
itself	O
can	O
be	O
a	O
complex	O
task	O
(	O
indeed	O
,	O
formally	O
this	O
can	O
be	O
just	O
as	O
diﬃcult	O
as	O
performing	O
inference	B
directly	O
with	O
the	O
intractable	O
p	O
)	O
and	O
the	O
reader	O
may	O
wonder	O
why	O
we	O
trade	O
a	O
diﬃcult	O
inference	B
task	O
for	O
a	O
potentially	O
diﬃcult	O
optimisation	B
problem	O
.	O
the	O
general	O
idea	O
is	O
that	O
the	O
optimisation	B
problem	O
has	O
some	O
local	B
smoothness	O
properties	B
that	O
enable	O
one	O
to	O
rapidly	O
ﬁnd	O
a	O
reasonable	O
optimum	O
based	O
on	O
generic	O
optimisation	B
methods	O
.	O
to	O
make	O
these	O
ideas	O
more	O
concrete	O
,	O
we	O
discuss	O
a	O
particular	O
case	O
of	O
ﬁtting	O
q	O
to	O
a	O
formally	O
intractable	O
p	O
in	O
section	O
(	O
28.4.1	O
)	O
below	O
.	O
28.4.1	O
pairwise	B
markov	O
random	B
ﬁeld	I
a	O
canonical	B
‘	O
intractable	O
’	O
distribution	B
is	O
the	O
pairwise	B
markov	O
random	O
field	O
deﬁned	O
on	O
binary	O
variables	O
xi	O
∈	O
{	O
+1	O
,	O
−1	O
}	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
,	O
p	O
(	O
x	O
)	O
=	O
1	O
(	O
cid:80	O
)	O
i	O
,	O
j	O
wij	O
xixj	O
+	O
(	O
cid:80	O
)	O
z	O
(	O
w	O
,	O
b	O
)	O
e	O
(	O
cid:80	O
)	O
i	O
,	O
j	O
wij	O
xixj	O
+	O
(	O
cid:80	O
)	O
z	O
(	O
w	O
,	O
b	O
)	O
=	O
(	O
cid:88	O
)	O
e	O
i	O
bixi	O
i	O
bixi	O
here	O
the	O
‘	O
partition	B
function	I
’	O
z	O
(	O
w	O
,	O
b	O
)	O
ensures	O
normalisation	B
,	O
(	O
28.4.1	O
)	O
(	O
28.4.2	O
)	O
x	O
i	O
=	O
1	O
,	O
the	O
terms	O
wiix2	O
i	O
are	O
constant	O
and	O
without	O
loss	O
of	O
generality	O
we	O
may	O
set	O
wii	O
to	O
zero1	O
.	O
this	O
since	O
x2	O
gives	O
an	O
undirected	B
distribution	O
with	O
connection	O
geometry	O
deﬁned	O
by	O
the	O
weights	O
w.	O
in	O
practice	O
,	O
the	O
weights	O
often	O
deﬁne	O
local	B
interactions	O
on	O
a	O
lattice	O
,	O
see	O
ﬁg	O
(	O
28.2	O
)	O
.	O
a	O
case	O
for	O
which	O
inference	B
in	O
this	O
model	B
is	O
required	O
is	O
given	O
in	O
example	O
(	O
114	O
)	O
.	O
p	O
(	O
y|x	O
)	O
=	O
(	O
cid:89	O
)	O
i	O
(	O
cid:80	O
)	O
p	O
(	O
x	O
)	O
∝	O
e	O
example	O
114	O
(	O
bayesian	O
image	B
denoising	I
)	O
.	O
consider	O
a	O
binary	O
image	O
,	O
where	O
x	O
describes	O
the	O
state	O
of	O
the	O
clean	O
pixels	O
(	O
±1	O
encoding	O
)	O
.	O
we	O
assume	O
a	O
noisy	O
pixel	O
generating	O
process	O
that	O
takes	O
each	O
clean	O
pixel	O
xi	O
and	O
ﬂips	O
its	O
binary	O
state	O
:	O
p	O
(	O
yi|xi	O
)	O
,	O
p	O
(	O
yi|xi	O
)	O
∝	O
eγyixi	O
(	O
28.4.3	O
)	O
the	O
probability	B
that	O
yi	O
and	O
xi	O
are	O
in	O
the	O
same	O
state	O
is	O
eγ/	O
(	O
eγ	O
+	O
e−γ	O
)	O
.	O
our	O
interest	O
is	O
to	O
the	O
posterior	B
distribution	O
on	O
clean	O
pixels	O
p	O
(	O
x|y	O
)	O
.	O
in	O
order	O
to	O
do	O
this	O
we	O
need	O
to	O
make	O
an	O
assumption	O
as	O
to	O
what	O
clean	O
images	O
look	O
like	O
.	O
we	O
do	O
this	O
using	O
a	O
mrf	O
i∼j	O
wij	O
xixj	O
(	O
28.4.4	O
)	O
for	O
some	O
settings	O
of	O
wij	O
>	O
0	O
,	O
with	O
ı	O
∼	O
j	O
indicating	O
that	O
i	O
and	O
j	O
are	O
neighbours	O
.	O
this	O
encodes	O
the	O
assumption	O
that	O
clean	O
images	O
tend	O
to	O
have	O
neighbouring	O
pixels	O
in	O
the	O
same	O
state	O
.	O
an	O
isolated	O
pixel	O
in	O
a	O
diﬀerent	O
state	O
to	O
its	O
neighbours	O
is	O
unlikely	O
under	O
this	O
prior	B
.	O
we	O
now	O
have	O
the	O
joint	B
distribution	O
p	O
(	O
x	O
,	O
y	O
)	O
=	O
p	O
(	O
x	O
)	O
(	O
cid:89	O
)	O
p	O
(	O
yi|xi	O
)	O
i	O
see	O
ﬁg	O
(	O
28.3	O
)	O
,	O
from	O
which	O
the	O
posterior	B
is	O
given	O
by	O
p	O
(	O
x|y	O
)	O
=	O
p	O
(	O
y|x	O
)	O
p	O
(	O
x	O
)	O
(	O
cid:80	O
)	O
x	O
p	O
(	O
y|x	O
)	O
p	O
(	O
x	O
)	O
∝	O
e	O
(	O
cid:80	O
)	O
i∼j	O
wij	O
xixj	O
+	O
(	O
cid:80	O
)	O
i	O
γyixi	O
(	O
28.4.5	O
)	O
(	O
28.4.6	O
)	O
quantities	O
such	O
as	O
the	O
map	B
state	O
(	O
most	O
a	O
posteriori	O
probable	O
image	O
)	O
,	O
marginals	O
p	O
(	O
xi|y	O
)	O
and	O
the	O
normali-	O
sation	O
constant	O
are	O
of	O
interest	O
.	O
1whilst	O
inference	B
with	O
a	O
general	O
mrf	O
is	O
formally	O
computationally	O
intractable	O
(	O
no	O
exact	O
polynomial	O
time	O
methods	O
are	O
known	O
)	O
,	O
two	O
celebrated	O
results	O
that	O
we	O
mention	O
in	O
passing	B
are	O
that	O
for	O
the	O
planar	O
mrf	O
model	B
with	O
pure	O
interactions	O
(	O
b	O
=	O
0	O
)	O
,	O
the	O
partition	B
function	I
is	O
computable	O
in	O
polynomial	O
time	O
[	O
157	O
,	O
95	O
,	O
177	O
,	O
115	O
,	O
243	O
]	O
,	O
as	O
is	O
the	O
map	B
state	O
for	O
attractive	O
planar	O
ising	O
models	O
w	O
>	O
0	O
[	O
121	O
]	O
,	O
see	O
section	O
(	O
28.8	O
)	O
.	O
draft	O
march	O
9	O
,	O
2010	O
519	O
variational	O
bounding	O
using	O
kl	O
(	O
q|p	O
)	O
figure	O
28.3	O
:	O
a	O
distribution	B
on	O
pixels	O
.	O
the	O
ﬁlled	O
nodes	O
indicate	O
observed	O
noisy	O
pixels	O
,	O
the	O
unshaded	O
nodes	O
a	O
markov	O
random	O
field	O
on	O
latent	B
clean	O
pixels	O
.	O
the	O
task	O
is	O
to	O
infer	O
the	O
clean	O
pixels	O
given	O
the	O
noisy	O
pixels	O
.	O
the	O
mrf	O
encourages	O
the	O
posterior	B
distribution	O
on	O
the	O
clean	O
pixels	O
to	O
contain	O
neighbouring	O
pixels	O
in	O
the	O
same	O
state	O
.	O
kullback-leibler	O
based	O
methods	O
for	O
the	O
mrf	O
we	O
have	O
kl	O
(	O
q|p	O
)	O
=	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
cid:105	O
)	O
q	O
−	O
bi	O
(	O
cid:104	O
)	O
xi	O
(	O
cid:105	O
)	O
q	O
+	O
log	O
z	O
≥	O
0	O
rewriting	O
,	O
this	O
gives	O
a	O
bound	B
on	O
the	O
log-partition	O
function	B
ij	O
wij	O
(	O
cid:104	O
)	O
xixj	O
(	O
cid:105	O
)	O
q	O
−	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
+	O
(	O
cid:88	O
)	O
wij	O
(	O
cid:104	O
)	O
xixj	O
(	O
cid:105	O
)	O
q	O
+	O
(	O
cid:88	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:124	O
)	O
ij	O
i	O
i	O
energy	B
bi	O
(	O
cid:104	O
)	O
xi	O
(	O
cid:105	O
)	O
q	O
(	O
cid:125	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
log	O
z	O
≥	O
−	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
cid:105	O
)	O
q	O
(	O
cid:124	O
)	O
entropy	B
(	O
28.4.7	O
)	O
(	O
28.4.8	O
)	O
(	O
28.4.9	O
)	O
(	O
28.4.10	O
)	O
(	O
28.4.11	O
)	O
(	O
28.4.12	O
)	O
(	O
28.4.13	O
)	O
the	O
bound	B
saturates	O
when	O
q	O
=	O
p.	O
however	O
,	O
this	O
is	O
of	O
little	O
help	O
since	O
we	O
can	O
not	O
compute	O
the	O
averages	O
of	O
variables	O
with	O
respect	O
to	O
this	O
intractable	O
distribution	O
(	O
cid:104	O
)	O
xixj	O
(	O
cid:105	O
)	O
p	O
,	O
(	O
cid:104	O
)	O
xi	O
(	O
cid:105	O
)	O
p.	O
the	O
idea	O
of	O
a	O
variational	O
method	O
is	O
to	O
assume	O
a	O
simpler	O
‘	O
tractable	O
’	O
distribution	B
q	O
for	O
which	O
these	O
averages	O
can	O
be	O
computed	O
,	O
along	O
with	O
the	O
entropy	B
of	O
q.	O
minimising	O
the	O
kl	O
divergence	B
with	O
respect	O
to	O
any	O
free	O
parameters	O
of	O
q	O
(	O
x	O
)	O
is	O
then	O
equivalent	B
to	O
maximising	O
the	O
lower	O
bound	O
on	O
the	O
log	O
partition	B
function	I
.	O
factorised	B
approximation	O
a	O
simple	O
‘	O
naive	O
’	O
assumption	O
is	O
the	O
fully	O
factorised	B
distribution	O
q	O
(	O
x	O
)	O
=	O
(	O
cid:89	O
)	O
qi	O
(	O
xi	O
)	O
i	O
the	O
graphical	O
model	B
of	O
this	O
approximation	B
is	O
given	O
in	O
ﬁg	O
(	O
28.4a	O
)	O
.	O
in	O
this	O
case	O
(	O
cid:104	O
)	O
log	O
qi	O
(	O
cid:105	O
)	O
qi	O
+	O
(	O
cid:88	O
)	O
wij	O
(	O
cid:104	O
)	O
xixj	O
(	O
cid:105	O
)	O
q	O
(	O
xi	O
,	O
xj	O
)	O
+	O
(	O
cid:88	O
)	O
ij	O
bi	O
(	O
cid:104	O
)	O
xi	O
(	O
cid:105	O
)	O
q	O
(	O
xi	O
)	O
i	O
log	O
z	O
≥	O
−	O
for	O
a	O
factorised	B
distribution	O
and	O
bearing	O
in	O
mind	O
that	O
xi	O
∈	O
{	O
+1	O
,	O
−1	O
}	O
,	O
(	O
cid:88	O
)	O
(	O
cid:26	O
)	O
1	O
i	O
(	O
cid:104	O
)	O
xixj	O
(	O
cid:105	O
)	O
=	O
(	O
cid:104	O
)	O
xi	O
(	O
cid:105	O
)	O
(	O
cid:104	O
)	O
xj	O
(	O
cid:105	O
)	O
i	O
=	O
j	O
i	O
(	O
cid:54	O
)	O
=	O
j	O
for	O
a	O
binary	O
variable	O
,	O
one	O
may	O
use	O
the	O
convenient	O
parametrization	O
qi	O
(	O
xi	O
=	O
1	O
)	O
=	O
eθi	O
eθi	O
+	O
e−θi	O
so	O
that	O
(	O
cid:104	O
)	O
xi	O
(	O
cid:105	O
)	O
qi	O
=	O
+1	O
×	O
q	O
(	O
xi	O
=	O
1	O
)	O
−	O
1	O
×	O
q	O
(	O
xi	O
=	O
−1	O
)	O
=	O
tanh	O
(	O
θi	O
)	O
q	O
(	O
x	O
)	O
=	O
(	O
cid:81	O
)	O
tion	O
.	O
tion	O
.	O
figure	O
28.4	O
:	O
(	O
a	O
)	O
:	O
naive	O
mean	O
field	O
approximation	B
(	O
b	O
)	O
:	O
a	O
spanning	B
tree	I
approxima-	O
(	O
c	O
)	O
:	O
a	O
decomposable	B
(	O
hypertree	O
)	O
approxima-	O
i	O
qi	O
(	O
xi	O
)	O
.	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
520	O
draft	O
march	O
9	O
,	O
2010	O
variational	O
bounding	O
using	O
kl	O
(	O
q|p	O
)	O
this	O
gives	O
the	O
following	O
lower	O
bound	O
on	O
the	O
log	O
partition	B
function	I
:	O
wij	O
tanh	O
(	O
θi	O
)	O
tanh	O
(	O
θj	O
)	O
+	O
(	O
cid:88	O
)	O
i	O
(	O
cid:88	O
)	O
i	O
h	O
(	O
θi	O
)	O
+	O
(	O
cid:88	O
)	O
(	O
cid:17	O
)	O
i	O
(	O
cid:54	O
)	O
=j	O
−θi	O
eθ	O
i	O
+	O
e	O
−	O
θi	O
tanh	O
(	O
θi	O
)	O
log	O
z	O
≥	O
b	O
(	O
θ	O
)	O
≡	O
(	O
cid:16	O
)	O
h	O
(	O
θi	O
)	O
=	O
log	O
bi	O
tanh	O
(	O
θi	O
)	O
(	O
28.4.14	O
)	O
(	O
28.4.15	O
)	O
where	O
h	O
(	O
θi	O
)	O
is	O
the	O
binary	B
entropy	I
of	O
a	O
distribution	B
parameterised	O
according	O
to	O
equation	B
(	O
28.4.12	O
)	O
:	O
finding	O
the	O
best	O
factorised	B
approximation	O
in	O
the	O
minimal	O
kullback-liebler	O
divergence	B
sense	O
then	O
corre-	O
sponds	O
to	O
maximising	O
the	O
bound	B
b	O
(	O
θ	O
)	O
with	O
respect	O
to	O
the	O
variational	O
parameters	O
θ.	O
the	O
bound	B
b	O
,	O
equation	B
(	O
28.4.14	O
)	O
,	O
is	O
generally	O
non-convex	O
in	O
θ	O
and	O
riddled	O
with	O
local	O
optima	O
.	O
finding	O
the	O
globally	O
optimal	O
θ	O
is	O
therefore	O
typically	O
a	O
computationally	O
hard	B
problem	O
.	O
it	O
seems	O
that	O
we	O
have	O
simply	O
replaced	O
a	O
computationally	O
hard	B
problem	O
of	O
computing	O
log	O
z	O
by	O
an	O
equally	O
hard	B
computational	O
problem	B
of	O
maximising	O
b	O
(	O
θ	O
)	O
.	O
indeed	O
,	O
the	O
‘	O
graphical	O
structure	B
’	O
of	O
this	O
optimisation	B
problem	O
matches	O
exactly	O
that	O
of	O
the	O
original	O
mrf	O
.	O
however	O
,	O
the	O
hope	O
is	O
that	O
by	O
transforming	O
a	O
diﬃcult	O
discrete	B
summation	O
into	O
a	O
continuous	B
optimisation	O
problem	B
,	O
we	O
will	O
be	O
able	O
to	O
bring	O
to	O
the	O
table	O
techniques	O
of	O
continuous	B
variable	O
numerical	B
optimisation	O
to	O
ﬁnd	O
a	O
good	O
approximation	B
.	O
a	O
particularly	O
simple	O
optimisation	O
technique	O
is	O
to	O
diﬀerentiate	O
the	O
bound	B
equation	O
(	O
28.4.14	O
)	O
and	O
equate	O
to	O
zero	O
.	O
straightforward	O
algebra	O
leads	O
to	O
requirement	O
that	O
the	O
optimal	O
solution	O
satisﬁes	O
the	O
equations	O
wij	O
tanh	O
(	O
θj	O
)	O
,	O
∀i	O
(	O
28.4.16	O
)	O
θi	O
=	O
bi	O
+	O
(	O
cid:88	O
)	O
j	O
(	O
cid:54	O
)	O
=i	O
one	O
may	O
show	O
that	O
updating	O
any	O
θi	O
according	O
to	O
equation	B
(	O
28.4.16	O
)	O
increases	O
b	O
(	O
θ	O
)	O
.	O
this	O
is	O
called	O
asy-	O
chronous	O
updating	O
and	O
is	O
guaranteed	O
to	O
lead	O
to	O
a	O
(	O
local	B
)	O
minimum	O
of	O
the	O
kl	O
divergence	B
,	O
section	O
(	O
28.4.3	O
)	O
.	O
once	O
a	O
converged	O
solution	O
has	O
been	O
identiﬁed	O
,	O
in	O
addition	O
to	O
a	O
bound	B
on	O
log	O
z	O
,	O
we	O
can	O
approximate	B
(	O
cid:104	O
)	O
xi	O
(	O
cid:105	O
)	O
p	O
≈	O
(	O
cid:104	O
)	O
xi	O
(	O
cid:105	O
)	O
q	O
=	O
tanh	O
(	O
θi	O
)	O
validity	O
of	O
the	O
factorised	B
approximation	O
(	O
28.4.17	O
)	O
when	O
might	O
one	O
expect	O
such	O
a	O
naive	O
factorised	O
approximation	B
to	O
work	O
well	O
?	O
clearly	O
,	O
if	O
wij	O
is	O
very	O
small	O
for	O
i	O
(	O
cid:54	O
)	O
=	O
j	O
,	O
the	O
distribution	B
p	O
will	O
be	O
eﬀectively	O
factorised	B
.	O
however	O
,	O
a	O
more	O
interesting	O
case	O
is	O
when	O
each	O
variable	B
xi	O
has	O
many	O
neighbours	O
.	O
it	O
is	O
useful	O
to	O
write	O
the	O
mrf	O
as	O
p	O
(	O
x	O
)	O
=	O
ij	O
wij	O
xixj	O
=	O
1	O
z	O
ed	O
(	O
cid:80	O
)	O
i	O
xi	O
1	O
d	O
(	O
cid:80	O
)	O
ed	O
(	O
cid:80	O
)	O
j	O
wij	O
xj	O
=	O
1	O
z	O
i	O
xizi	O
(	O
28.4.18	O
)	O
(	O
28.4.19	O
)	O
where	O
the	O
local	B
‘	O
ﬁelds	O
’	O
are	O
deﬁned	O
as	O
1	O
d	O
zi	O
≡	O
wijxj	O
j	O
zi	O
,	O
each	O
of	O
the	O
terms	O
xj	O
in	O
the	O
summation	O
(	O
cid:80	O
)	O
we	O
now	O
invoke	O
a	O
circular	O
(	O
but	O
self-consistent	O
)	O
argument	O
:	O
let	O
’	O
s	O
assume	O
that	O
p	O
(	O
x	O
)	O
is	O
factorised	B
.	O
then	O
for	O
j	O
wijxj	O
is	O
independent	O
.	O
provided	O
the	O
wij	O
are	O
not	O
strongly	O
correlated	O
the	O
conditions	O
of	O
validity	O
of	O
the	O
central	O
limit	O
theorem	B
hold	O
[	O
122	O
]	O
,	O
and	O
each	O
zi	O
will	O
be	O
gaussian	O
distributed	O
.	O
assuming	O
that	O
each	O
wij	O
is	O
o	O
(	O
1	O
)	O
,	O
the	O
mean	B
of	O
zi	O
is	O
the	O
variance	B
is	O
(	O
cid:104	O
)	O
zi	O
(	O
cid:105	O
)	O
=	O
(	O
cid:11	O
)	O
(	O
cid:10	O
)	O
z2	O
i	O
wij	O
(	O
cid:104	O
)	O
xj	O
(	O
cid:105	O
)	O
=	O
o	O
(	O
1	O
)	O
(	O
cid:16	O
)	O
d	O
(	O
cid:88	O
)	O
w2	O
ik	O
k=1	O
1	O
−	O
(	O
cid:104	O
)	O
xk	O
(	O
cid:105	O
)	O
2	O
(	O
cid:17	O
)	O
−	O
(	O
cid:104	O
)	O
zi	O
(	O
cid:105	O
)	O
2	O
=	O
1	O
d2	O
draft	O
march	O
9	O
,	O
2010	O
=	O
o	O
(	O
1/d	O
)	O
(	O
28.4.20	O
)	O
(	O
28.4.21	O
)	O
521	O
e	O
1	O
z	O
(	O
cid:80	O
)	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
j	O
1	O
d	O
variational	O
bounding	O
using	O
kl	O
(	O
q|p	O
)	O
hence	O
the	O
variance	B
of	O
the	O
ﬁeld	O
zi	O
is	O
much	O
smaller	O
than	O
its	O
mean	B
value	O
.	O
as	O
d	O
increases	O
the	O
ﬂuctuations	O
around	O
the	O
mean	B
diminish	O
,	O
and	O
we	O
may	O
write	O
p	O
(	O
xi	O
)	O
(	O
28.4.22	O
)	O
ed	O
(	O
cid:80	O
)	O
p	O
(	O
x	O
)	O
≈	O
1	O
z	O
i	O
xi	O
(	O
cid:104	O
)	O
zi	O
(	O
cid:105	O
)	O
≈	O
(	O
cid:89	O
)	O
i	O
the	O
assumption	O
that	O
p	O
is	O
approximately	O
factorised	B
is	O
therefore	O
self-consistent	O
in	O
the	O
limit	O
of	O
mrfs	O
with	O
a	O
large	O
number	O
of	O
neighbours	O
.	O
hence	O
the	O
factorised	B
approximation	O
would	O
appear	O
to	O
be	O
reasonable	O
in	O
the	O
extreme	O
limits	O
(	O
i	O
)	O
a	O
very	O
weakly	O
connected	B
system	O
wij	O
≈	O
0	O
,	O
or	O
(	O
ii	O
)	O
a	O
large	O
densely	O
connected	B
system	O
.	O
the	O
fully	O
factorised	B
approximation	O
is	O
also	O
called	O
the	O
naive	O
mean	O
field	O
theory	O
since	O
for	O
the	O
mrf	O
case	O
it	O
assumes	O
that	O
we	O
can	O
replace	O
the	O
eﬀect	O
of	O
the	O
neighbours	O
by	O
a	O
mean	B
of	O
the	O
ﬁeld	O
at	O
each	O
site	O
.	O
28.4.2	O
general	O
mean	O
ﬁeld	O
equations	O
for	O
a	O
general	O
intractable	O
distribution	O
p	O
(	O
x	O
)	O
on	O
discrete	B
or	O
continuous	B
x	O
,	O
the	O
kl	O
divergence	B
between	O
a	O
factorised	B
approximation	O
q	O
(	O
x	O
)	O
and	O
p	O
(	O
x	O
)	O
is	O
isolating	O
the	O
dependency	O
of	O
the	O
above	O
on	O
a	O
single	O
factor	B
q	O
(	O
xi	O
)	O
we	O
have	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
xi	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
xi	O
)	O
−	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
(	O
cid:81	O
)	O
i	O
q	O
(	O
xi	O
)	O
(	O
cid:32	O
)	O
(	O
cid:89	O
)	O
i	O
kl	O
=	O
(	O
cid:88	O
)	O
i	O
(	O
cid:33	O
)	O
q	O
(	O
xi	O
)	O
|p	O
(	O
x	O
)	O
(	O
cid:68	O
)	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
(	O
cid:81	O
)	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
xi	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
xi	O
)	O
−	O
(	O
cid:16	O
)	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
(	O
cid:81	O
)	O
(	O
cid:16	O
)	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
(	O
cid:81	O
)	O
q	O
(	O
xi	O
)	O
∝	O
exp	O
j	O
(	O
cid:54	O
)	O
=i	O
q	O
(	O
xj	O
)	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
j	O
(	O
cid:54	O
)	O
=i	O
q	O
(	O
xj	O
)	O
(	O
cid:69	O
)	O
j	O
(	O
cid:54	O
)	O
=i	O
q	O
(	O
xj	O
)	O
q	O
(	O
xi	O
)	O
up	O
to	O
a	O
normalisation	B
constant	I
,	O
this	O
is	O
therefore	O
the	O
kl	O
divergence	B
between	O
q	O
(	O
xi	O
)	O
and	O
a	O
distribution	B
proportional	O
to	O
exp	O
so	O
that	O
the	O
optimal	O
setting	O
for	O
q	O
(	O
xi	O
)	O
satisﬁes	O
these	O
are	O
known	O
as	O
the	O
mean-ﬁeld	O
equations	O
and	O
deﬁne	O
a	O
new	O
approximation	B
factor	O
in	O
terms	O
of	O
the	O
previous	O
approximation	B
factors	O
.	O
note	O
that	O
if	O
the	O
normalisation	B
constant	I
of	O
p	O
(	O
x	O
)	O
is	O
unknown	O
,	O
this	O
presents	O
no	O
problem	O
since	O
this	O
constant	O
is	O
simply	O
absorbed	O
into	O
the	O
normalisation	B
of	O
the	O
factors	O
q	O
(	O
xi	O
)	O
.	O
in	O
other	O
words	O
one	O
may	O
replace	O
p	O
(	O
x	O
)	O
with	O
the	O
unnormalised	O
p∗	O
(	O
x	O
)	O
in	O
equation	B
(	O
28.4.25	O
)	O
.	O
beginning	O
with	O
an	O
initial	O
randomly	O
chosen	O
set	O
of	O
distributions	O
q	O
(	O
xi	O
)	O
,	O
the	O
mean-ﬁeld	O
equations	O
are	O
iterated	O
until	O
convergence	O
.	O
asynchronous	B
updating	I
is	O
guaranteed	O
to	O
decrease	O
the	O
kl	O
divergence	B
at	O
each	O
stage	O
,	O
section	O
(	O
28.4.3	O
)	O
.	O
28.4.3	O
asynchronous	B
updating	I
guarantees	O
approximation	B
improvement	O
for	O
a	O
factorised	B
variational	O
approximation	B
equation	O
(	O
28.4.23	O
)	O
,	O
we	O
claim	O
that	O
each	O
update	O
equation	B
(	O
28.4.25	O
)	O
reduces	O
the	O
kullback-leibler	O
approximation	B
error	O
.	O
to	O
show	O
this	O
we	O
write	O
a	O
single	O
updated	O
distribution	B
as	O
(	O
28.4.23	O
)	O
(	O
28.4.24	O
)	O
(	O
28.4.25	O
)	O
(	O
28.4.26	O
)	O
(	O
28.4.27	O
)	O
(	O
28.4.28	O
)	O
(	O
28.4.29	O
)	O
the	O
joint	B
distribution	O
under	O
this	O
single	O
update	O
is	O
qnew	O
i	O
=	O
1	O
zi	O
exp	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
(	O
cid:81	O
)	O
(	O
cid:89	O
)	O
j	O
(	O
cid:54	O
)	O
=i	O
qold	O
j	O
qnew	O
=	O
qnew	O
i	O
qold	O
j	O
j	O
(	O
cid:54	O
)	O
=i	O
(	O
cid:16	O
)	O
∆	O
≡	O
kl	O
(	O
qnew|p	O
)	O
−	O
kl	O
qold|p	O
using	O
522	O
kl	O
(	O
qnew|p	O
)	O
=	O
(	O
cid:104	O
)	O
log	O
qnew	O
i	O
(	O
cid:105	O
)	O
qnew	O
i	O
(	O
cid:17	O
)	O
+	O
(	O
cid:88	O
)	O
j	O
(	O
cid:54	O
)	O
=i	O
our	O
interest	O
is	O
the	O
change	O
in	O
the	O
approximation	B
error	O
under	O
this	O
single	O
mean-ﬁeld	O
update	O
:	O
(	O
cid:68	O
)	O
log	O
qold	O
j	O
(	O
cid:69	O
)	O
j	O
−	O
qold	O
(	O
cid:68	O
)	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
(	O
cid:81	O
)	O
(	O
cid:69	O
)	O
j	O
(	O
cid:54	O
)	O
=i	O
qold	O
j	O
qnew	O
i	O
draft	O
march	O
9	O
,	O
2010	O
variational	O
bounding	O
using	O
kl	O
(	O
q|p	O
)	O
x1	O
x4	O
x2	O
x1	O
x3	O
x4	O
x2	O
x3	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
28.5	O
:	O
(	O
a	O
)	O
:	O
a	O
toy	O
‘	O
intractable	O
’	O
distribution	B
.	O
(	O
b	O
)	O
:	O
a	O
structured	B
singly-connected	O
approximation	B
.	O
j	O
(	O
cid:54	O
)	O
=i	O
qold	O
j	O
=	O
ziqnew	O
(	O
cid:69	O
)	O
(	O
cid:68	O
)	O
qold	O
i	O
−	O
log	O
qold	O
i	O
i	O
(	O
cid:68	O
)	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
cid:105	O
)	O
(	O
cid:81	O
)	O
(	O
cid:69	O
)	O
i	O
−	O
(	O
cid:104	O
)	O
log	O
q	O
qold	O
∗	O
i	O
(	O
cid:105	O
)	O
qold	O
i	O
+	O
(	O
cid:104	O
)	O
log	O
q	O
qold	O
i	O
(	O
cid:69	O
)	O
(	O
cid:68	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
cid:105	O
)	O
(	O
cid:81	O
)	O
j	O
(	O
cid:54	O
)	O
=i	O
qold	O
j	O
qnew	O
i	O
∗	O
i	O
(	O
cid:105	O
)	O
qnew	O
i	O
+	O
(	O
cid:104	O
)	O
log	O
q	O
∗	O
i	O
(	O
cid:105	O
)	O
qold	O
i	O
and	O
deﬁning	O
the	O
un-normalised	O
distribution	B
q	O
then	O
∗	O
i	O
(	O
xi	O
)	O
=	O
exp	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
(	O
cid:81	O
)	O
(	O
cid:68	O
)	O
∆	O
=	O
(	O
cid:104	O
)	O
log	O
qnew	O
log	O
qold	O
i	O
(	O
cid:105	O
)	O
qnew	O
i	O
−	O
(	O
cid:68	O
)	O
(	O
cid:69	O
)	O
i	O
−	O
log	O
zi	O
−	O
log	O
qold	O
i	O
i	O
∗	O
=	O
(	O
cid:104	O
)	O
log	O
q	O
i	O
(	O
cid:105	O
)	O
qnew	O
(	O
cid:16	O
)	O
=	O
−	O
log	O
zi	O
−	O
qold	O
=	O
−kl	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
i	O
qnew|p	O
i	O
|qnew	O
(	O
cid:16	O
)	O
≤	O
kl	O
(	O
cid:17	O
)	O
≤	O
0	O
(	O
cid:17	O
)	O
qold|p	O
hence	O
kl	O
(	O
cid:69	O
)	O
j	O
(	O
cid:54	O
)	O
=i	O
qold	O
j	O
qold	O
i	O
(	O
28.4.30	O
)	O
(	O
28.4.31	O
)	O
(	O
28.4.32	O
)	O
(	O
28.4.33	O
)	O
(	O
28.4.34	O
)	O
(	O
28.4.35	O
)	O
so	O
that	O
updating	O
a	O
single	O
component	O
of	O
q	O
at	O
a	O
time	O
is	O
guaranteed	O
to	O
improve	O
the	O
approximation	B
.	O
note	O
that	O
this	O
result	O
is	O
quite	O
general	O
,	O
holding	O
for	O
any	O
distribution	B
p	O
(	O
x	O
)	O
.	O
in	O
the	O
case	O
of	O
a	O
markov	O
network	O
the	O
guaranteed	O
approximation	B
improvement	O
is	O
equivalent	B
to	O
a	O
guaranteed	O
increase	O
(	O
strictly	O
speaking	O
a	O
non-decrease	O
)	O
in	O
the	O
lower	O
bound	O
on	O
the	O
partition	B
function	I
.	O
28.4.4	O
intractable	B
energy	I
whilst	O
the	O
fully	O
factorised	B
approximation	O
is	O
rather	O
severe	O
,	O
even	O
this	O
may	O
not	O
be	O
enough	O
to	O
render	O
the	O
mean-	O
j	O
(	O
cid:54	O
)	O
=i	O
q	O
(	O
xj	O
)	O
.	O
for	O
ﬁeld	O
equations	O
tractably	O
implementable	O
.	O
to	O
do	O
so	O
we	O
need	O
to	O
be	O
able	O
to	O
compute	O
(	O
cid:104	O
)	O
log	O
p∗	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
(	O
cid:81	O
)	O
some	O
models	O
of	O
interest	O
this	O
is	O
still	O
not	O
possible	O
and	O
additional	O
approximations	O
are	O
required	O
.	O
(	O
cid:0	O
)	O
w	O
0	O
,	O
s2i	O
(	O
cid:1	O
)	O
(	O
cid:89	O
)	O
(	O
cid:16	O
)	O
cnwtxn	O
(	O
cid:17	O
)	O
example	O
115	O
(	O
‘	O
intractable	O
’	O
mean-ﬁeld	O
approximation	B
)	O
.	O
consider	O
the	O
posterior	B
distribution	O
from	O
a	O
rel-	O
evance	O
vector	O
machine	O
classiﬁcation	B
problem	O
,	O
section	O
(	O
18.2.4	O
)	O
:	O
p	O
(	O
w|d	O
)	O
∝	O
n	O
the	O
terms	O
σ	O
(	O
cid:0	O
)	O
cnwtxn	O
(	O
cid:1	O
)	O
render	O
p	O
(	O
w|d	O
)	O
non-gaussian	O
.	O
we	O
can	O
ﬁnd	O
a	O
gaussian	O
approximation	B
q	O
(	O
w	O
)	O
=	O
(	O
28.4.36	O
)	O
σ	O
n	O
n	O
(	O
w	O
µ	O
,	O
σ	O
)	O
by	O
minimising	O
the	O
kullback-leibler	O
divergence	B
kl	O
(	O
q	O
(	O
w	O
)	O
|p	O
(	O
w|d	O
)	O
)	O
=	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
w	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
w	O
)	O
−	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
w|d	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
w	O
)	O
(	O
cid:68	O
)	O
(	O
cid:16	O
)	O
cnwtxn	O
(	O
cid:17	O
)	O
(	O
cid:69	O
)	O
log	O
σ	O
n	O
(	O
w	O
µ	O
,	O
σ	O
)	O
the	O
entropic	O
term	O
is	O
straightforward	O
since	O
this	O
is	O
the	O
negative	O
entropy	B
of	O
a	O
gaussian	O
.	O
however	O
,	O
we	O
also	O
require	O
the	O
‘	O
energy	B
’	O
which	O
includes	O
a	O
contribution	O
(	O
28.4.37	O
)	O
(	O
28.4.38	O
)	O
there	O
is	O
no	O
closed	O
form	O
expression	O
for	O
this	O
.	O
one	O
approach	B
is	O
to	O
use	O
additional	O
variational	O
approximations	O
,	O
[	O
141	O
,	O
238	O
]	O
.	O
another	O
approach	B
is	O
to	O
recognise	O
that	O
the	O
multi-variate	B
average	O
can	O
be	O
reduced	O
to	O
a	O
uni-variate	O
gaussian	O
average	B
:	O
n	O
(	O
w	O
µ	O
,	O
σ	O
)	O
=	O
(	O
cid:104	O
)	O
log	O
σ	O
(	O
cna	O
)	O
(	O
cid:105	O
)	O
n	O
(	O
a	O
m	O
,	O
τ	O
2	O
)	O
,	O
m	O
=	O
µtxn	O
,	O
τ	O
2	O
=	O
(	O
xn	O
)	O
t	O
σxn	O
(	O
28.4.39	O
)	O
(	O
cid:68	O
)	O
log	O
σ	O
(	O
cid:16	O
)	O
cnwtxn	O
(	O
cid:17	O
)	O
(	O
cid:69	O
)	O
draft	O
march	O
9	O
,	O
2010	O
523	O
mutual	B
information	I
maximisation	O
:	O
a	O
kl	O
variational	B
approach	I
and	O
the	O
uni-variate	O
gaussian	O
average	B
can	O
be	O
carried	O
out	O
using	O
quadrature	O
.	O
this	O
approach	B
was	O
used	O
in	O
[	O
22	O
]	O
to	O
approximate	B
the	O
posterior	B
distribution	O
of	O
bayesian	O
neural	O
networks	O
.	O
28.4.5	O
structured	B
variational	O
approximation	B
one	O
can	O
extend	O
the	O
factorised	B
kl	O
variational	B
approximation	I
by	O
using	O
non-factorised	O
q	O
(	O
x	O
)	O
[	O
239	O
,	O
24	O
]	O
.	O
those	O
for	O
which	O
averages	O
of	O
the	O
variables	O
can	O
be	O
computed	O
in	O
linear	B
time	O
include	O
spanning	O
trees	O
,	O
ﬁg	O
(	O
28.4b	O
)	O
and	O
decomposable	B
graphs	O
ﬁg	O
(	O
28.4c	O
)	O
.	O
for	O
example	O
,	O
for	O
the	O
distribution	B
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
=	O
1	O
z	O
φ	O
(	O
x1	O
,	O
x2	O
)	O
φ	O
(	O
x2	O
,	O
x3	O
)	O
φ	O
(	O
x3	O
,	O
x4	O
)	O
φ	O
(	O
x4	O
,	O
x1	O
)	O
φ	O
(	O
x1	O
,	O
x3	O
)	O
(	O
28.4.40	O
)	O
a	O
tractable	O
q	O
distribution	B
would	O
be	O
,	O
ﬁg	O
(	O
28.5	O
)	O
q	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
=	O
˜φ	O
(	O
x1	O
,	O
x2	O
)	O
˜φ	O
(	O
x1	O
,	O
x3	O
)	O
˜φ	O
(	O
x1	O
,	O
x4	O
)	O
(	O
28.4.41	O
)	O
1	O
˜z	O
in	O
this	O
case	O
we	O
have	O
kl	O
(	O
q|p	O
)	O
=	O
hq	O
(	O
x1	O
,	O
x2	O
)	O
+	O
hq	O
(	O
x1	O
,	O
x3	O
)	O
+	O
hq	O
(	O
x1	O
,	O
x4	O
)	O
−	O
3hq	O
(	O
x1	O
)	O
+	O
(	O
cid:88	O
)	O
(	O
cid:104	O
)	O
log	O
φ	O
(	O
xi	O
,	O
xj	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
xi	O
,	O
xj	O
)	O
(	O
28.4.42	O
)	O
i∼j	O
since	O
q	O
is	O
singly-connected	B
,	O
computing	O
the	O
marginals	O
and	O
entropy	B
is	O
straightforward	O
(	O
since	O
the	O
entropy	B
requires	O
only	O
pairwise	B
marginals	O
on	O
graph	B
neighbours	O
)	O
.	O
more	O
generally	O
one	O
can	O
exploit	O
any	O
structural	O
approximation	B
with	O
an	O
arbitrary	O
hypertree	O
width	O
w	O
by	O
use	O
of	O
the	O
junction	B
tree	I
algorithm	O
in	O
combination	O
with	O
the	O
kl	O
divergence	B
.	O
however	O
,	O
the	O
computational	O
expense	O
increases	O
exponentially	O
with	O
the	O
hypertree	O
width	O
[	O
294	O
]	O
.	O
28.5	O
mutual	B
information	I
maximisation	O
:	O
a	O
kl	O
variational	B
approach	I
here	O
we	O
take	O
a	O
short	O
interlude	O
here	O
to	O
demonstrate	O
an	O
application	O
of	O
the	O
kullback-leibler	O
variational	B
approach	I
in	O
information	O
theory	O
.	O
a	O
fundamental	O
goal	O
is	O
to	O
maximise	O
information	O
transfer	O
,	O
measured	O
by	O
the	O
(	O
see	O
also	O
deﬁnition	O
(	O
87	O
)	O
)	O
i	O
(	O
x	O
,	O
y	O
)	O
≡	O
h	O
(	O
x	O
)	O
−	O
h	O
(	O
x|y	O
)	O
where	O
the	O
and	O
are	O
deﬁned	O
h	O
(	O
x	O
)	O
≡	O
−	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
)	O
,	O
h	O
(	O
x|y	O
)	O
≡	O
−	O
(	O
cid:104	O
)	O
log	O
p	O
(	O
x|y	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
,	O
y	O
)	O
(	O
28.5.1	O
)	O
(	O
28.5.2	O
)	O
here	O
we	O
are	O
interested	O
in	O
the	O
situation	O
in	O
which	O
p	O
(	O
x	O
)	O
is	O
ﬁxed	O
,	O
but	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
has	O
adjustable	O
parameters	O
θ	O
that	O
we	O
wish	O
to	O
set	O
in	O
order	O
to	O
maximise	O
i	O
(	O
x	O
,	O
y	O
)	O
.	O
in	O
this	O
case	O
h	O
(	O
x	O
)	O
is	O
constant	O
and	O
the	O
optimisation	B
problem	O
is	O
equivalent	B
to	O
minimising	O
the	O
conditional	B
entropy	I
h	O
(	O
x|y	O
)	O
.	O
unfortunately	O
,	O
in	O
many	O
cases	O
of	O
practical	O
interest	O
h	O
(	O
x|y	O
)	O
is	O
computationally	O
intractable	O
.	O
see	O
example	O
(	O
116	O
)	O
below	O
for	O
a	O
motivating	O
example	O
.	O
we	O
discuss	O
in	O
section	O
(	O
28.5.1	O
)	O
a	O
general	O
procedure	O
based	O
on	O
the	O
kullback-leibler	O
divergence	B
to	O
approximately	O
maximise	O
the	O
mutual	B
information	I
.	O
example	O
116.	O
consider	O
a	O
neural	O
transmission	O
system	B
in	O
which	O
xi	O
∈	O
{	O
0	O
,	O
1	O
}	O
denotes	O
an	O
emitting	O
neuron	O
in	O
a	O
non-ﬁring	O
state	O
(	O
0	O
)	O
or	O
ﬁring	O
state	O
(	O
1	O
)	O
,	O
and	O
yj	O
∈	O
{	O
0	O
,	O
1	O
}	O
a	O
receiving	O
neuron	O
.	O
if	O
each	O
receiving	O
neuron	O
ﬁres	O
independently	O
,	O
depending	O
only	O
on	O
the	O
emitting	O
neurons	O
,	O
we	O
have	O
p	O
(	O
yi|x	O
)	O
(	O
28.5.3	O
)	O
draft	O
march	O
9	O
,	O
2010	O
p	O
(	O
y|x	O
)	O
=	O
(	O
cid:89	O
)	O
i	O
524	O
mutual	B
information	I
maximisation	O
:	O
a	O
kl	O
variational	B
approach	I
x1	O
x2	O
x3	O
x4	O
y1	O
y2	O
y3	O
y4	O
algorithm	B
29	O
im	O
algorithm	B
bution	O
p	O
(	O
x	O
)	O
=	O
(	O
cid:81	O
)	O
σ	O
(	O
cid:0	O
)	O
wt	O
i	O
x	O
(	O
cid:1	O
)	O
,	O
ﬁnd	O
the	O
optimal	O
parameters	O
wi	O
that	O
maximise	O
the	O
mutual	O
figure	O
28.6	O
:	O
an	O
information	O
transfer	O
problem	B
.	O
for	O
a	O
ﬁxed	O
distri-	O
i	O
p	O
(	O
xi	O
)	O
and	O
parameterised	O
distributions	O
p	O
(	O
yj|x	O
)	O
=	O
information	O
between	O
the	O
variables	O
x	O
and	O
y.	O
such	O
considerations	O
are	O
popular	O
in	O
theoretical	O
neuroscience	O
and	O
aim	O
to	O
understand	O
how	O
the	O
receptive	O
ﬁelds	O
wi	O
of	O
a	O
neuron	O
relate	O
to	O
the	O
statistics	O
of	O
the	O
environ-	O
ment	O
p	O
(	O
x	O
)	O
.	O
1	O
:	O
choose	O
a	O
class	O
of	O
approximating	O
distributions	O
q	O
(	O
for	O
example	O
factorised	B
)	O
2	O
:	O
initialise	O
the	O
parameters	O
θ	O
3	O
:	O
repeat	O
4	O
:	O
for	O
ﬁxed	O
q	O
(	O
x|y	O
)	O
,	O
ﬁnd	O
θnew	O
=	O
argmax	O
˜i	O
(	O
x	O
,	O
y	O
)	O
θ	O
5	O
:	O
for	O
ﬁxed	O
θ	O
,	O
qnew	O
(	O
x|y	O
)	O
=	O
argmax	O
q	O
(	O
x|y	O
)	O
∈q	O
˜i	O
(	O
x	O
,	O
y	O
)	O
where	O
q	O
is	O
a	O
chosen	O
class	O
of	O
distributions	O
.	O
6	O
:	O
until	O
converged	O
where	O
for	O
example	O
we	O
could	O
use	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
p	O
(	O
yi	O
=	O
1|x	O
)	O
=	O
σ	O
wt	O
i	O
x	O
p	O
(	O
x	O
)	O
=	O
(	O
cid:89	O
)	O
p	O
(	O
xi	O
)	O
if	O
we	O
make	O
the	O
simple	O
assumption	O
that	O
emitting	O
neurons	O
ﬁre	O
independently	O
,	O
(	O
28.5.6	O
)	O
(	O
28.5.7	O
)	O
(	O
28.5.4	O
)	O
(	O
28.5.5	O
)	O
i	O
then	O
for	O
p	O
(	O
x|y	O
)	O
all	O
components	O
of	O
the	O
x	O
variable	B
are	O
dependent	O
,	O
see	O
ﬁg	O
(	O
28.6	O
)	O
.	O
this	O
deﬁnes	O
a	O
complex	O
high-dimensional	O
p	O
(	O
x	O
,	O
y	O
)	O
for	O
which	O
the	O
conditional	B
entropy	I
is	O
typically	O
intractable	O
.	O
28.5.1	O
the	O
information	B
maximisation	I
algorithm	O
consider	O
this	O
immediately	O
gives	O
a	O
bound	B
kl	O
(	O
p	O
(	O
x|y	O
)	O
|q	O
(	O
x|y	O
)	O
)	O
≥	O
0	O
(	O
cid:88	O
)	O
p	O
(	O
x|y	O
)	O
log	O
p	O
(	O
x|y	O
)	O
−	O
(	O
cid:88	O
)	O
x	O
(	O
cid:88	O
)	O
x	O
multiplying	O
both	O
sides	O
by	O
p	O
(	O
y	O
)	O
,	O
we	O
obtain	O
p	O
(	O
x|y	O
)	O
log	O
q	O
(	O
x|y	O
)	O
≥	O
0	O
(	O
cid:88	O
)	O
⇒	O
x	O
,	O
y	O
p	O
(	O
y	O
)	O
p	O
(	O
x|y	O
)	O
log	O
p	O
(	O
x|y	O
)	O
≥	O
p	O
(	O
x	O
,	O
y	O
)	O
log	O
q	O
(	O
x|y	O
)	O
x	O
,	O
y	O
from	O
the	O
deﬁnition	O
,	O
the	O
left	O
of	O
the	O
above	O
is	O
−h	O
(	O
x|y	O
)	O
.	O
hence	O
i	O
(	O
x	O
,	O
y	O
)	O
≥	O
h	O
(	O
x	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
x|y	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
,	O
y	O
)	O
≡	O
˜i	O
(	O
x	O
,	O
y	O
)	O
draft	O
march	O
9	O
,	O
2010	O
(	O
28.5.8	O
)	O
(	O
28.5.9	O
)	O
(	O
28.5.10	O
)	O
(	O
28.5.11	O
)	O
525	O
loopy	B
belief	O
propagation	B
c	O
a	O
e	O
d	O
b	O
f	O
figure	O
28.7	O
:	O
classical	O
belief	B
propagation	I
can	O
be	O
derived	O
by	O
con-	O
sidering	O
how	O
to	O
compute	O
the	O
marginal	B
of	O
a	O
variable	B
on	O
a	O
mrf	O
.	O
in	O
this	O
case	O
the	O
marginal	B
p	O
(	O
d	O
)	O
depends	O
on	O
messages	O
transmitted	O
via	O
the	O
neighbours	O
of	O
d.	O
by	O
deﬁning	O
local	B
messages	O
on	O
the	O
links	O
of	O
the	O
graph	B
,	O
a	O
recursive	O
algorithm	B
for	O
computing	O
all	O
marginals	O
can	O
be	O
derived	O
,	O
see	O
text	O
.	O
from	O
this	O
lower	O
bound	O
on	O
the	O
mutual	B
information	I
we	O
arrive	O
at	O
the	O
information	B
maximisation	I
(	O
im	O
)	O
algorithm	B
[	O
20	O
]	O
.	O
given	O
a	O
distribution	B
p	O
(	O
x	O
)	O
and	O
a	O
parameterised	O
distribution	B
p	O
(	O
y|x	O
,	O
θ	O
)	O
,	O
we	O
seek	O
to	O
max-	O
imise	O
˜i	O
(	O
x	O
,	O
y	O
)	O
with	O
respect	O
to	O
θ.	O
a	O
co-ordinate	O
wise	O
optimisation	B
procedure	O
is	O
presented	O
in	O
algorithm	B
(	O
29	O
)	O
.	O
the	O
blahut-arimoto	O
algorithm	B
in	O
information	O
theory	O
(	O
see	O
for	O
example	O
[	O
184	O
]	O
)	O
is	O
a	O
special	O
case	O
in	O
which	O
the	O
optimal	O
decoder	O
q	O
(	O
x|y	O
)	O
∝	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
p	O
(	O
x	O
)	O
(	O
28.5.12	O
)	O
is	O
used	O
.	O
in	O
applications	O
where	O
the	O
blahut-arimoto	O
algorithm	B
is	O
intractable	O
to	O
implement	O
,	O
the	O
im	O
algorithm	B
can	O
provide	O
an	O
alternative	O
by	O
restricting	O
q	O
to	O
a	O
tractable	O
family	B
of	O
distributions	O
(	O
tractable	O
in	O
the	O
sense	O
that	O
the	O
lower	O
bound	O
can	O
be	O
computed	O
)	O
.	O
the	O
blahut-arimoto	O
algorithm	B
is	O
analogous	O
to	O
the	O
em	O
algorithm	B
for	O
maximum	B
likelihood	I
and	O
guarantees	O
a	O
non-decrease	O
of	O
the	O
mutual	B
information	I
at	O
each	O
stage	O
of	O
the	O
update	O
.	O
similarly	O
,	O
the	O
im	O
procedure	O
is	O
analogous	O
to	O
a	O
generalised	B
em	O
procedure	O
and	O
each	O
step	O
of	O
the	O
procedure	O
can	O
not	O
decrease	O
the	O
lower	O
bound	O
on	O
the	O
mutual	B
information	I
.	O
28.5.2	O
linear	B
gaussian	O
decoder	O
a	O
special	O
case	O
of	O
the	O
im	O
framework	O
is	O
to	O
use	O
a	O
linear	B
gaussian	O
decoder	O
q	O
(	O
x|y	O
)	O
=	O
n	O
(	O
x	O
uy	O
,	O
σ	O
)	O
⇒	O
log	O
q	O
(	O
x|y	O
)	O
=	O
(	O
x	O
−	O
uy	O
)	O
t	O
σ−1	O
(	O
x	O
−	O
uy	O
)	O
(	O
28.5.13	O
)	O
plugging	O
this	O
into	O
the	O
mi	O
bound	B
,	O
equation	B
(	O
28.5.11	O
)	O
,	O
and	O
optimising	O
with	O
respect	O
to	O
σ	O
,	O
and	O
u	O
,	O
we	O
obtain	O
(	O
cid:68	O
)	O
(	O
x	O
−	O
uy	O
)	O
(	O
x	O
−	O
uy	O
)	O
t	O
(	O
cid:69	O
)	O
σ	O
=	O
i	O
(	O
x	O
,	O
y	O
)	O
≥	O
h	O
(	O
x	O
)	O
−	O
1	O
2	O
log	O
det	O
,	O
(	O
cid:18	O
)	O
(	O
cid:68	O
)	O
u	O
=	O
xxt	O
(	O
cid:69	O
)	O
−	O
where	O
(	O
cid:104	O
)	O
·	O
(	O
cid:105	O
)	O
≡	O
(	O
cid:104	O
)	O
·	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
,	O
y	O
)	O
.	O
using	O
this	O
in	O
the	O
mi	O
bound	B
we	O
obtain	O
xyt	O
(	O
cid:69	O
)	O
(	O
cid:68	O
)	O
(	O
cid:68	O
)	O
xyt	O
(	O
cid:69	O
)	O
(	O
cid:68	O
)	O
(	O
cid:68	O
)	O
yyt	O
(	O
cid:69	O
)	O
−1	O
yyt	O
(	O
cid:69	O
)	O
−1	O
(	O
cid:68	O
)	O
yxt	O
(	O
cid:69	O
)	O
(	O
cid:19	O
)	O
(	O
28.5.14	O
)	O
+	O
const	O
.	O
(	O
28.5.15	O
)	O
up	O
to	O
irrelevant	O
constants	O
,	O
this	O
is	O
equivalent	B
to	O
linsker	O
’	O
s	O
as-if-gaussian	O
approximation	B
to	O
the	O
mutual	B
information	I
[	O
176	O
]	O
.	O
one	O
can	O
therefore	O
view	O
linsker	O
’	O
s	O
approach	B
as	O
a	O
special	O
case	O
of	O
the	O
im	O
algorithm	B
restricted	O
to	O
linear-gaussian	O
decoders	O
.	O
in	O
principle	O
,	O
one	O
can	O
therefore	O
improve	O
on	O
linsker	O
’	O
s	O
method	O
by	O
considering	O
more	O
powerful	O
non-linear-gaussian	O
decoders	O
.	O
applications	O
of	O
this	O
technique	O
to	O
neural	O
systems	O
are	O
discussed	O
in	O
[	O
20	O
]	O
.	O
28.6	O
loopy	B
belief	O
propagation	B
belief	O
propagation	B
is	O
a	O
technique	O
for	O
exact	O
inference	O
of	O
marginals	O
p	O
(	O
xi	O
)	O
for	O
singly-connected	B
distributions	O
p	O
(	O
x	O
)	O
.	O
there	O
are	O
diﬀerent	O
formulations	O
of	O
bp	O
,	O
the	O
most	O
modern	O
treatment	O
being	O
the	O
sum-product	B
algo-	O
rithm	B
on	O
the	O
corresponding	O
factor	B
graph	I
,	O
as	O
described	O
in	O
section	O
(	O
5.1.2	O
)	O
.	O
an	O
important	O
observation	O
is	O
that	O
the	O
algorithm	B
is	O
purely	O
local	B
–	O
the	O
updates	O
are	O
unaware	O
of	O
the	O
global	B
structure	O
of	O
the	O
graph	B
,	O
depending	O
only	O
on	O
the	O
local	B
neighbourhood	O
structure	B
.	O
this	O
means	O
that	O
even	O
if	O
the	O
graph	B
is	O
multiply-connected	B
(	O
it	O
is	O
loopy	B
)	O
one	O
can	O
still	O
apply	O
the	O
algorithm	B
and	O
‘	O
see	O
what	O
happens	O
’	O
.	O
provided	O
the	O
loops	O
in	O
the	O
graph	B
are	O
relatively	O
long	O
,	O
one	O
may	O
hope	O
that	O
running	O
‘	O
loopy	B
’	O
bp	O
will	O
converge	O
to	O
a	O
good	O
approximation	B
of	O
the	O
true	O
marginals	O
.	O
in	O
general	O
,	O
this	O
can	O
not	O
be	O
guaranteed	O
;	O
however	O
,	O
when	O
the	O
method	O
converges	O
,	O
the	O
results	O
can	O
be	O
surprisingly	O
accurate	O
.	O
in	O
the	O
following	O
we	O
will	O
show	O
how	O
loopy	B
bp	O
can	O
also	O
be	O
motivated	O
by	O
a	O
variational	O
objective	O
.	O
to	O
do	O
so	O
,	O
the	O
most	O
natural	O
connection	O
is	O
with	O
the	O
classical	O
bp	O
algorithm	B
(	O
rather	O
than	O
the	O
factor	B
graph	I
sum-product	O
)	O
algorithm	B
.	O
for	O
this	O
reason	O
we	O
brieﬂy	O
describe	O
below	O
the	O
classical	O
bp	O
approach	B
.	O
526	O
draft	O
march	O
9	O
,	O
2010	O
loopy	B
belief	O
propagation	B
x2	O
λ	O
x	O
2→	O
x	O
i	O
(	O
x	O
i	O
)	O
xi	O
λxi→xj	O
(	O
xj	O
)	O
xj	O
x1	O
λx1→xi	O
(	O
xi	O
)	O
(	O
x	O
i	O
)	O
→	O
x	O
i	O
λ	O
x	O
3	O
x3	O
figure	O
28.8	O
:	O
loopy	B
belief	O
propagation	B
.	O
once	O
a	O
node	B
has	O
received	O
incoming	O
messages	O
from	O
all	O
neighbours	O
(	O
excluding	O
the	O
one	O
it	O
wants	O
to	O
send	O
a	O
message	B
to	O
)	O
,	O
it	O
may	O
send	O
an	O
outgoing	O
message	B
to	O
a	O
neighbour	B
:	O
φ	O
(	O
xi	O
,	O
xj	O
)	O
λx1→xi	O
(	O
xi	O
)	O
λx2→xi	O
(	O
xi	O
)	O
λx3→xi	O
(	O
xi	O
)	O
λxi→xj	O
(	O
xj	O
)	O
=	O
(	O
cid:88	O
)	O
xi	O
28.6.1	O
classical	O
bp	O
on	O
an	O
undirected	B
graph	I
graph	O
.	O
consider	O
calculating	O
the	O
marginal	B
p	O
(	O
d	O
)	O
=	O
(	O
cid:80	O
)	O
work	O
in	O
ﬁg	O
(	O
28.7	O
)	O
.	O
we	O
denote	O
both	O
a	O
node	B
and	O
its	O
state	O
by	O
the	O
same	O
symbol	O
,	O
so	O
that	O
(	O
cid:80	O
)	O
bp	O
can	O
be	O
derived	O
by	O
considering	O
how	O
to	O
calculate	O
a	O
marginal	B
in	O
terms	O
of	O
messages	O
on	O
an	O
undirected	B
a	O
,	O
b	O
,	O
c	O
,	O
e	O
,	O
f	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
,	O
e	O
,	O
f	O
)	O
for	O
the	O
pairwise	B
markov	O
net-	O
b	O
φ	O
(	O
d	O
,	O
b	O
)	O
denotes	O
summation	O
over	O
the	O
states	O
of	O
the	O
variable	B
b.	O
carrying	O
out	O
such	O
a	O
summation	O
results	O
in	O
a	O
‘	O
message	B
’	O
λb→d	O
(	O
d	O
)	O
containing	O
information	O
from	O
node	B
b	O
to	O
node	B
d.	O
in	O
order	O
to	O
compute	O
the	O
summation	O
eﬃciently	O
,	O
we	O
distribute	O
summations	O
as	O
follows	O
:	O
p	O
(	O
d	O
)	O
=	O
1	O
z	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
b	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
a	O
(	O
cid:125	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
f	O
(	O
cid:125	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
φ	O
(	O
b	O
,	O
d	O
)	O
φ	O
(	O
a	O
,	O
d	O
)	O
φ	O
(	O
d	O
,	O
f	O
)	O
λb→d	O
(	O
d	O
)	O
λa→d	O
(	O
d	O
)	O
λf→d	O
(	O
d	O
)	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
e	O
φ	O
(	O
d	O
,	O
e	O
)	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
c	O
λe→d	O
(	O
d	O
)	O
(	O
cid:125	O
)	O
φ	O
(	O
c	O
,	O
e	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
λc→e	O
(	O
e	O
)	O
(	O
cid:125	O
)	O
(	O
cid:125	O
)	O
(	O
28.6.1	O
)	O
where	O
we	O
deﬁne	O
messages	O
λn1→n2	O
(	O
n2	O
)	O
sending	O
information	O
from	O
node	B
n1	O
to	O
node	B
n2	O
as	O
a	O
function	B
of	O
the	O
state	O
of	O
node	B
n2	O
.	O
in	O
general	O
,	O
a	O
node	B
xi	O
passes	O
a	O
message	B
to	O
node	B
xj	O
via	O
λxi→xj	O
(	O
xj	O
)	O
=	O
(	O
cid:88	O
)	O
φ	O
(	O
xi	O
,	O
xj	O
)	O
(	O
cid:89	O
)	O
xi	O
k∈ne	O
(	O
i	O
)	O
,	O
k	O
(	O
cid:54	O
)	O
=j	O
λxk→xi	O
(	O
xi	O
)	O
(	O
28.6.2	O
)	O
this	O
algorithm	B
is	O
equivalent	B
to	O
the	O
sum-product	B
algorithm	I
provided	O
the	O
graph	B
is	O
singly-connected	B
.	O
28.6.2	O
loopy	B
bp	O
as	O
a	O
variational	O
procedure	O
a	O
variational	O
procedure	O
that	O
corresponds	O
to	O
loopy	B
bp	O
can	O
be	O
derived	O
by	O
considering	O
the	O
terms	O
of	O
a	O
standard	O
variational	O
approximation	B
based	O
on	O
the	O
kullback-leibler	O
divergence	B
kl	O
(	O
q|p	O
)	O
[	O
299	O
]	O
.	O
for	O
a	O
pairwise	B
mrf	O
deﬁned	O
on	O
potentials	O
φ	O
(	O
xi	O
,	O
xj	O
)	O
,	O
and	O
approximating	O
distribution	B
q	O
(	O
x	O
)	O
,	O
the	O
kullback-leibler	O
bound	B
is	O
(	O
cid:89	O
)	O
i∼j	O
p	O
(	O
x	O
)	O
=	O
1	O
z	O
φ	O
(	O
xi	O
,	O
xj	O
)	O
log	O
z	O
≥	O
−	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
+	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
i∼j	O
(	O
cid:125	O
)	O
(	O
cid:104	O
)	O
log	O
φ	O
(	O
xi	O
,	O
xj	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
energy	B
since	O
(	O
cid:104	O
)	O
log	O
φ	O
(	O
xi	O
,	O
xj	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
=	O
(	O
cid:104	O
)	O
log	O
φ	O
(	O
xi	O
,	O
xj	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
xi	O
,	O
xj	O
)	O
(	O
28.6.3	O
)	O
(	O
28.6.4	O
)	O
(	O
28.6.5	O
)	O
each	O
contribution	O
to	O
the	O
energy	B
depends	O
on	O
q	O
(	O
x	O
)	O
only	O
via	O
the	O
pairwise	B
marginals	O
q	O
(	O
xi	O
,	O
xj	O
)	O
.	O
this	O
suggests	O
that	O
these	O
marginals	O
should	O
form	O
the	O
natural	B
parameters	O
of	O
any	O
approximation	B
.	O
can	O
we	O
then	O
ﬁnd	O
an	O
expression	O
for	O
the	O
entropy	B
−	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
in	O
terms	O
of	O
these	O
pairwise	B
marginals	O
?	O
consider	O
a	O
case	O
in	O
which	O
the	O
required	O
marginals	O
are	O
q	O
(	O
x1	O
,	O
x2	O
)	O
,	O
q	O
(	O
x2	O
,	O
x3	O
)	O
,	O
q	O
(	O
x3	O
,	O
x4	O
)	O
draft	O
march	O
9	O
,	O
2010	O
(	O
28.6.6	O
)	O
527	O
loopy	B
belief	O
propagation	B
given	O
these	O
marginals	O
,	O
the	O
energy	B
term	O
is	O
straightforward	O
to	O
compute	O
,	O
and	O
we	O
are	O
left	O
with	O
requiring	O
only	O
the	O
entropy	B
of	O
q.	O
either	O
by	O
appealing	O
to	O
the	O
junction	B
tree	I
representation	O
,	O
or	O
by	O
straightforward	O
algebra	O
,	O
one	O
can	O
show	O
that	O
we	O
can	O
uniquely	O
express	O
q	O
in	O
terms	O
of	O
the	O
marginals	O
as	O
q	O
(	O
x	O
)	O
=	O
q	O
(	O
x1	O
,	O
x2	O
)	O
q	O
(	O
x2	O
,	O
x3	O
)	O
q	O
(	O
x3	O
,	O
x4	O
)	O
q	O
(	O
x2	O
)	O
q	O
(	O
x3	O
)	O
(	O
28.6.7	O
)	O
an	O
intuitive	O
way	O
to	O
arrive	O
at	O
this	O
result	O
is	O
by	O
examining	O
the	O
numerator	O
of	O
equation	B
(	O
28.6.7	O
)	O
.	O
the	O
variable	B
x2	O
appears	O
twice	O
,	O
as	O
does	O
the	O
variable	B
x3	O
and	O
,	O
since	O
any	O
joint	B
distribution	O
can	O
not	O
have	O
replicated	O
variables	O
on	O
the	O
left	O
of	O
any	O
conditioning	B
,	O
we	O
must	O
compensate	O
for	O
the	O
additional	O
x2	O
and	O
x3	O
variables	O
by	O
dividing	O
by	O
these	O
marginals	O
.	O
in	O
this	O
case	O
,	O
the	O
entropy	B
of	O
q	O
(	O
x	O
)	O
can	O
be	O
written	O
as	O
hq	O
(	O
x	O
)	O
=	O
−	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
=	O
hq	O
(	O
x1	O
,	O
x2	O
)	O
+	O
hq	O
(	O
x2	O
,	O
x3	O
)	O
+	O
hq	O
(	O
x3	O
,	O
x4	O
)	O
−	O
hq	O
(	O
x2	O
)	O
−	O
hq	O
(	O
x3	O
)	O
more	O
generally	O
,	O
chapter	O
(	O
6	O
)	O
,	O
any	O
decomposable	B
graph	O
can	O
be	O
represented	O
as	O
(	O
cid:81	O
)	O
(	O
cid:81	O
)	O
c	O
q	O
(	O
xc	O
)	O
s	O
q	O
(	O
xs	O
)	O
q	O
(	O
x	O
)	O
=	O
(	O
28.6.8	O
)	O
(	O
28.6.9	O
)	O
where	O
the	O
q	O
(	O
xc	O
)	O
are	O
the	O
marginals	O
deﬁned	O
on	O
cliques	O
of	O
the	O
graph	B
,	O
with	O
xc	O
being	O
the	O
variables	O
of	O
the	O
clique	B
,	O
and	O
the	O
q	O
(	O
xs	O
)	O
are	O
deﬁned	O
on	O
the	O
separators	O
(	O
intersections	O
of	O
neighbouring	O
cliques	O
)	O
.	O
the	O
expression	O
for	O
the	O
entropy	B
of	O
the	O
distribution	B
is	O
then	O
given	O
by	O
a	O
sum	O
of	O
marginal	B
entropies	O
minus	O
the	O
entropy	B
of	O
the	O
marginals	O
deﬁned	O
on	O
the	O
separators	O
.	O
bethe	O
free	O
energy	B
consider	O
now	O
a	O
mrf	O
corresponding	O
to	O
a	O
non-decomposable	O
graph	B
,	O
for	O
example	O
the	O
4-cycle	O
p	O
(	O
x	O
)	O
=	O
1	O
z	O
φ	O
(	O
x1	O
,	O
x2	O
)	O
φ	O
(	O
x2	O
,	O
x3	O
)	O
φ	O
(	O
x3	O
,	O
x4	O
)	O
φ	O
(	O
x4	O
,	O
x1	O
)	O
the	O
energy	B
requires	O
therefore	O
that	O
we	O
know	O
q	O
(	O
x1	O
,	O
x2	O
)	O
,	O
q	O
(	O
x2	O
,	O
x3	O
)	O
,	O
q	O
(	O
x3	O
,	O
x4	O
)	O
,	O
q	O
(	O
x4	O
,	O
x1	O
)	O
(	O
28.6.10	O
)	O
(	O
28.6.11	O
)	O
assuming	O
that	O
these	O
marginals	O
are	O
given	O
,	O
can	O
we	O
ﬁnd	O
an	O
expression	O
for	O
the	O
entropy	B
of	O
the	O
joint	B
distribution	O
q	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
in	O
terms	O
of	O
its	O
pairwise	B
marginals	O
q	O
(	O
xi	O
,	O
xj	O
)	O
?	O
in	O
general	O
this	O
is	O
not	O
possible	O
since	O
the	O
graph	B
corresponding	O
to	O
the	O
marginals	O
contains	O
loops	O
(	O
so	O
that	O
the	O
junction	B
tree	I
representation	O
would	O
result	O
in	O
cliques	O
greater	O
than	O
size	O
2	O
)	O
.	O
however	O
,	O
a	O
simple	O
‘	O
no	O
overcounting	O
’	O
approximation	B
is	O
to	O
write	O
q	O
(	O
x1	O
,	O
x2	O
)	O
q	O
(	O
x2	O
,	O
x3	O
)	O
q	O
(	O
x3	O
,	O
x4	O
)	O
q	O
(	O
x4	O
,	O
x1	O
)	O
q	O
(	O
x1	O
)	O
q	O
(	O
x2	O
)	O
q	O
(	O
x3	O
)	O
q	O
(	O
x4	O
)	O
q	O
(	O
x	O
)	O
≈	O
(	O
cid:88	O
)	O
subject	O
to	O
the	O
constraints	O
q	O
(	O
xi	O
,	O
xj	O
)	O
=	O
q	O
(	O
xj	O
)	O
(	O
28.6.12	O
)	O
(	O
28.6.13	O
)	O
xi	O
an	O
entropy	B
approximation	O
using	O
this	O
representation	B
is	O
therefore	O
hq	O
(	O
x	O
)	O
≈	O
hq	O
(	O
x1	O
,	O
x2	O
)	O
+	O
hq	O
(	O
x2	O
,	O
x3	O
)	O
+	O
hq	O
(	O
x3	O
,	O
x4	O
)	O
+	O
hq	O
(	O
x1	O
,	O
x4	O
)	O
−	O
4	O
(	O
cid:88	O
)	O
i=1	O
hq	O
(	O
xi	O
)	O
(	O
28.6.14	O
)	O
with	O
this	O
approximation	B
the	O
log	O
partition	B
function	I
is	O
known	O
in	O
statistical	O
physics	O
as	O
the	O
(	O
negative	O
)	O
bethe	O
free	O
energy	B
.	O
our	O
interest	O
is	O
then	O
to	O
maximise	O
this	O
expression	O
with	O
respect	O
to	O
the	O
parameters	O
q	O
(	O
xi	O
,	O
xj	O
)	O
xi	O
q	O
(	O
xi	O
,	O
xj	O
)	O
=	O
q	O
(	O
xj	O
)	O
.	O
these	O
may	O
be	O
enforced	O
using	O
lagrange	O
(	O
cid:104	O
)	O
log	O
φ	O
(	O
xi	O
,	O
xj	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
xi	O
,	O
xj	O
)	O
+	O
(	O
cid:88	O
)	O
i∼j	O
(	O
cid:32	O
)	O
q	O
(	O
xi	O
)	O
−	O
(	O
cid:88	O
)	O
xi	O
λi	O
,	O
j	O
(	O
cid:33	O
)	O
q	O
(	O
xi	O
,	O
xj	O
)	O
draft	O
march	O
9	O
,	O
2010	O
multipliers	O
.	O
one	O
can	O
write	O
the	O
bethe	O
free	O
energy	B
as	O
subject	O
to	O
marginal	B
consistency	O
constraints	O
,	O
(	O
cid:80	O
)	O
hq	O
(	O
xi	O
)	O
+	O
(	O
cid:88	O
)	O
hq	O
(	O
xi	O
,	O
xj	O
)	O
+	O
(	O
cid:88	O
)	O
f	O
(	O
q	O
,	O
λ	O
)	O
≡	O
−	O
(	O
cid:88	O
)	O
i∼j	O
i	O
i∼j	O
528	O
loopy	B
belief	O
propagation	B
where	O
i	O
∼	O
j	O
denotes	O
the	O
unique	O
neighbouring	O
edges	O
on	O
the	O
graph	B
(	O
each	O
edge	O
is	O
counted	O
only	O
once	O
)	O
.	O
this	O
is	O
no	O
longer	O
a	O
bound	B
on	O
the	O
log	O
partition	B
function	I
since	O
the	O
entropy	B
approximation	O
is	O
not	O
a	O
lower	O
bound	O
on	O
the	O
true	O
entropy	B
.	O
the	O
task	O
is	O
now	O
to	O
maximise	O
this	O
‘	O
approximate	B
bound	O
’	O
with	O
respect	O
to	O
the	O
parameters	O
,	O
namely	O
all	O
the	O
pairwise	B
marginals	O
q	O
(	O
xi	O
,	O
xj	O
)	O
and	O
the	O
lagrange	O
multipliers	O
λ	O
.	O
(	O
28.6.15	O
)	O
a	O
simple	O
scheme	O
to	O
maximise	O
equation	B
(	O
28.6.15	O
)	O
is	O
to	O
use	O
a	O
ﬁxed	O
point	O
iteration	B
by	O
equating	O
the	O
derivatives	O
of	O
the	O
bethe	O
free	O
energy	B
with	O
respect	O
to	O
the	O
parameters	O
q	O
(	O
xi	O
,	O
xj	O
)	O
to	O
zero	O
,	O
and	O
likewise	O
for	O
the	O
lagrange	O
multipliers	O
.	O
one	O
may	O
show	O
that	O
the	O
resulting	O
set	O
of	O
ﬁxed	O
point	O
equations	O
,	O
on	O
eliminating	O
q	O
,	O
is	O
equivalent	B
to	O
(	O
undirected	B
)	O
belief	B
propagation	I
[	O
299	O
]	O
for	O
which	O
,	O
in	O
general	O
,	O
a	O
node	B
xi	O
passes	O
a	O
message	B
to	O
node	B
xj	O
using	O
(	O
28.6.16	O
)	O
(	O
28.6.17	O
)	O
λxi→xj	O
(	O
xj	O
)	O
=	O
(	O
cid:88	O
)	O
φ	O
(	O
xi	O
,	O
xj	O
)	O
(	O
cid:89	O
)	O
xi	O
k∈ne	O
(	O
i	O
)	O
,	O
k	O
(	O
cid:54	O
)	O
=j	O
λxk→xi	O
(	O
xi	O
)	O
at	O
convergence	O
the	O
marginal	B
p	O
(	O
xi	O
)	O
is	O
then	O
approximated	O
by	O
(	O
cid:89	O
)	O
q	O
(	O
xi	O
)	O
∝	O
i∈ne	O
(	O
j	O
)	O
λxj→xi	O
(	O
xi	O
)	O
the	O
prefactor	O
being	O
determined	O
by	O
normalisation	B
.	O
for	O
a	O
singly-connected	B
distribution	O
p	O
,	O
this	O
message	B
pass-	O
ing	O
scheme	O
converges	O
and	O
the	O
marginal	B
corresponds	O
to	O
the	O
exact	O
result	O
.	O
for	O
multiply	O
connected	B
(	O
loopy	B
)	O
structures	O
,	O
running	O
this	O
loopy	B
belief	O
propagation	B
will	O
generally	O
result	O
in	O
an	O
approximation	B
.	O
naturally	O
,	O
we	O
can	O
dispense	O
with	O
the	O
bethe	O
free	O
energy	B
if	O
desired	O
and	O
run	O
the	O
associated	O
loopy	B
belief	O
propagation	B
algorithm	O
directly	O
on	O
the	O
undirected	B
graph	I
.	O
the	O
convergence	O
of	O
loopy	B
belief	O
propagation	B
which	O
can	O
be	O
heavily	O
dependent	O
on	O
the	O
topology	O
of	O
the	O
graph	B
and	O
also	O
the	O
message	B
updating	O
schedule	B
[	O
291	O
,	O
201	O
]	O
.	O
the	O
potential	B
beneﬁt	O
of	O
the	O
bethe	O
free	O
energy	B
viewpoint	O
is	O
that	O
it	O
gives	O
an	O
objective	O
that	O
is	O
required	O
to	O
be	O
optimised	O
,	O
opening	O
up	O
the	O
possibility	O
of	O
more	O
general	O
optimisation	B
techniques	O
than	O
bp	O
.	O
the	O
so-called	O
double-loop	O
techniques	O
iteratively	O
isolate	O
convex	O
contributions	O
to	O
the	O
bethe	O
free	O
energy	B
,	O
interleaved	O
with	O
concave	O
contributions	O
.	O
at	O
each	O
stage	O
,	O
the	O
resulting	O
optimisiations	O
can	O
be	O
carried	O
out	O
eﬃciently	O
[	O
301	O
,	O
133	O
,	O
299	O
]	O
.	O
validity	O
of	O
loopy	B
belief	O
propagation	B
for	O
a	O
mrf	O
which	O
has	O
a	O
loop	O
,	O
computationally	O
this	O
means	O
that	O
a	O
perturbation	O
in	O
a	O
variable	B
on	O
the	O
loop	O
eventually	O
reverberates	O
to	O
the	O
same	O
variable	B
.	O
however	O
,	O
if	O
there	O
are	O
a	O
large	O
number	O
of	O
variables	O
in	O
the	O
loop	O
,	O
and	O
the	O
individual	O
neighbouring	O
links	O
are	O
not	O
all	O
extremely	O
strong	B
,	O
the	O
numerical	B
eﬀect	O
of	O
the	O
loop	O
is	O
small	O
in	O
the	O
sense	O
that	O
inﬂuence	O
of	O
the	O
variable	B
on	O
itself	O
is	O
negligible	O
.	O
in	O
such	O
cases	O
one	O
would	O
expect	O
the	O
loopy	B
bp	O
approximation	B
to	O
be	O
accurate	O
.	O
an	O
area	O
of	O
particular	O
success	O
for	O
loopy	B
belief	O
propagation	B
inference	O
is	O
in	O
error	O
correction	O
based	O
on	O
low	O
density	O
parity	O
check	B
codes	O
,	O
which	O
are	O
designed	O
to	O
have	O
this	O
long-	O
loop	O
property	O
[	O
183	O
]	O
.	O
in	O
many	O
examples	O
of	O
practical	O
interest	O
(	O
for	O
example	O
an	O
mrf	O
with	O
nearest	O
neighbour	B
interactions	O
on	O
a	O
lattice	O
)	O
,	O
however	O
,	O
loops	O
can	O
be	O
very	O
short	O
.	O
in	O
such	O
cases	O
a	O
naive	O
implementation	O
of	O
loopy	B
bp	O
will	O
fail	O
.	O
a	O
natural	B
extension	O
is	O
to	O
cluster	O
variables	O
to	O
alleviate	O
some	O
of	O
the	O
issues	O
arising	O
from	O
strong	B
local	O
dependencies	O
;	O
this	O
technique	O
is	O
called	O
the	O
kikuchi	O
or	O
cluster	O
variation	O
method	O
[	O
154	O
]	O
.	O
more	O
elaborate	O
ways	O
of	O
clustering	B
variables	O
can	O
be	O
considered	O
using	O
region	B
graphs	I
[	O
299	O
,	O
292	O
]	O
.	O
example	O
117.	O
the	O
ﬁle	O
demomfbpgibbs.m	O
compares	O
the	O
performance	B
of	O
naive	O
mean	O
field	O
theory	O
,	O
belief	B
propagation	I
and	O
unstructured	O
gibbs	O
sampling	B
on	O
marginal	B
inference	O
in	O
a	O
pairwise	B
markov	O
network	O
p	O
(	O
w	O
,	O
x	O
,	O
y	O
,	O
z	O
)	O
=	O
φwx	O
(	O
w	O
,	O
x	O
)	O
φwy	O
(	O
w	O
,	O
y	O
)	O
φwz	O
(	O
w	O
,	O
z	O
)	O
φxy	O
(	O
x	O
,	O
y	O
)	O
φxz	O
(	O
x	O
,	O
z	O
)	O
φyz	O
(	O
y	O
,	O
z	O
)	O
(	O
28.6.18	O
)	O
in	O
which	O
all	O
variables	O
take	O
6	O
states	O
.	O
in	O
the	O
experiment	O
the	O
tables	O
are	O
selected	O
from	O
a	O
uniform	B
distribution	I
raised	O
to	O
a	O
power	O
α.	O
for	O
α	O
close	O
to	O
zero	O
,	O
all	O
the	O
tables	O
are	O
essentially	O
ﬂat	O
and	O
therefore	O
the	O
variables	O
become	O
independent	O
,	O
a	O
situation	O
for	O
which	O
mf	O
,	O
bp	O
and	O
gibbs	O
sampling	B
are	O
ideally	O
suited	O
.	O
as	O
α	O
is	O
increased	O
to	O
draft	O
march	O
9	O
,	O
2010	O
529	O
expectation	B
propagation	I
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
(	O
a	O
)	O
:	O
the	O
markov	O
network	O
(	O
left	O
)	O
figure	O
28.9	O
:	O
that	O
we	O
wish	O
to	O
approximate	B
the	O
marginals	O
p	O
(	O
w	O
)	O
,	O
p	O
(	O
x	O
)	O
,	O
p	O
(	O
y	O
)	O
,	O
p	O
(	O
z	O
)	O
for	O
.	O
all	O
tables	O
are	O
drawn	O
from	O
a	O
uniform	B
distribution	I
raised	O
to	O
a	O
power	O
α.	O
on	O
(	O
b	O
)	O
:	O
there	O
are	O
64	O
=	O
1295	O
the	O
right	O
is	O
shown	O
the	O
naive	O
mean	O
ﬁeld	O
approximation	O
factorised	B
structure	O
.	O
states	O
of	O
the	O
distribution	B
.	O
shown	O
is	O
a	O
randomly	O
sampled	O
distribution	B
for	O
α	O
=	O
5	O
which	O
has	O
many	O
isolated	O
peaks	O
,	O
suggesting	O
the	O
distribution	B
is	O
far	O
from	O
factorised	B
.	O
in	O
this	O
case	O
the	O
mf	O
and	O
gibbs	O
sampling	B
approx-	O
(	O
c	O
)	O
:	O
as	O
α	O
is	O
increased	O
to	O
25	O
,	O
typically	O
only	O
one	O
state	O
of	O
the	O
distribution	B
imations	O
may	O
perform	O
poorly	O
.	O
dominates	O
.	O
see	O
demomfbpgibbs.m	O
.	O
x1	O
x4	O
x2	O
x1	O
x2	O
x1	O
x3	O
x4	O
x3	O
x4	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
x2	O
x3	O
figure	O
28.10	O
:	O
(	O
a	O
)	O
:	O
multiply-connected	B
factor	O
(	O
b	O
)	O
:	O
expectation	B
graph	O
representing	O
p	O
(	O
x	O
)	O
.	O
propagation	B
approximates	O
(	O
a	O
)	O
in	O
terms	O
of	O
a	O
tractable	O
factor	B
graph	I
.	O
the	O
open	O
squares	O
indi-	O
cate	O
that	O
the	O
factors	O
are	O
parameters	O
of	O
the	O
ap-	O
proximation	O
.	O
the	O
basic	O
ep	O
approximation	B
is	O
to	O
replace	O
all	O
factors	O
in	O
p	O
(	O
x	O
)	O
by	O
product	O
factors	O
.	O
(	O
c	O
)	O
:	O
tree	B
structured	O
ep	O
.	O
5	O
,	O
the	O
dependencies	O
amongst	O
the	O
variables	O
increase	O
and	O
the	O
methods	O
perform	O
worse	O
,	O
especially	O
mf	O
and	O
gibbs	O
.	O
as	O
α	O
is	O
increased	O
to	O
25	O
,	O
the	O
distribution	B
becomes	O
sharply	O
peaked	O
around	O
a	O
single	O
state	O
,	O
such	O
that	O
the	O
posterior	B
is	O
eﬀectively	O
factorised	B
,	O
see	O
ﬁg	O
(	O
28.9	O
)	O
.	O
this	O
suggests	O
that	O
a	O
mf	O
approximation	B
(	O
and	O
also	O
gibbs	O
sampling	B
)	O
should	O
work	O
well	O
.	O
however	O
,	O
ﬁnding	O
this	O
state	O
is	O
computationally	O
diﬃcult	O
and	O
both	O
methods	O
often	O
get	O
stuck	O
in	O
local	B
minima	O
.	O
belief	B
propagation	I
seems	O
less	O
susceptible	O
to	O
being	O
trapped	O
in	O
local	B
minima	O
in	O
this	O
regime	O
and	O
tends	O
to	O
outperform	O
both	O
mf	O
and	O
gibbs	O
sampling	B
.	O
28.7	O
expectation	B
propagation	I
the	O
messages	O
in	O
schemes	O
such	O
as	O
belief	O
propagation	B
are	O
not	O
always	O
representable	O
in	O
a	O
compact	O
form	O
.	O
the	O
switching	B
linear	I
dynamical	I
system	I
,	O
chapter	O
(	O
25	O
)	O
is	O
such	O
an	O
instance	O
,	O
in	O
which	O
the	O
messages	O
require	O
an	O
exponential	B
amount	O
of	O
storage	O
.	O
this	O
limits	O
bp	O
to	O
cases	O
such	O
as	O
discrete	O
networks	O
,	O
or	O
more	O
generally	O
expo-	O
nential	O
family	B
messages	O
.	O
expectation	B
propagation	I
extends	O
the	O
applicability	O
of	O
bp	O
to	O
cases	O
in	O
which	O
the	O
messages	O
are	O
not	O
in	O
the	O
exponential	B
family	I
by	O
projecting	O
the	O
messages	O
back	O
to	O
the	O
exponential	B
family	I
at	O
each	O
stage	O
.	O
this	O
projection	B
is	O
obtained	O
by	O
using	O
a	O
kullback-leibler	O
measure	O
[	O
195	O
,	O
246	O
,	O
197	O
]	O
.	O
consider	O
a	O
distribution	B
of	O
the	O
form	O
(	O
cid:89	O
)	O
i	O
p	O
(	O
x	O
)	O
=	O
1	O
z	O
φi	O
(	O
x	O
)	O
(	O
28.7.1	O
)	O
in	O
ep	O
one	O
identiﬁes	O
those	O
factors	O
φi	O
(	O
x	O
)	O
which	O
,	O
if	O
replaced	O
by	O
simpler	O
factors	O
˜φi	O
(	O
x	O
)	O
,	O
would	O
render	O
the	O
distribution	B
˜p	O
(	O
x	O
)	O
tractable	O
.	O
one	O
then	O
sets	O
any	O
free	O
parameters	O
of	O
˜φi	O
(	O
x	O
)	O
by	O
minimising	O
the	O
kullback-	O
530	O
draft	O
march	O
9	O
,	O
2010	O
wxyzoriginal	O
graphwxyzfactorised	O
graph0500100000.050.10.150.2p	O
distribution0500100000.20.40.60.81p	O
distribution	B
expectation	O
propagation	B
leibler	O
divergence	B
kl	O
(	O
p|˜p	O
)	O
.	O
for	O
example	O
,	O
consider	O
a	O
pairwise	B
mrf	O
p	O
(	O
x	O
)	O
=	O
1	O
z	O
φ1,2	O
(	O
x1	O
,	O
x2	O
)	O
φ2,3	O
(	O
x2	O
,	O
x3	O
)	O
φ3,4	O
(	O
x3	O
,	O
x4	O
)	O
φ4,1	O
(	O
x4	O
,	O
x1	O
)	O
(	O
28.7.2	O
)	O
with	O
factor	O
graph	B
as	O
depicted	O
in	O
ﬁg	O
(	O
28.10a	O
)	O
.	O
if	O
we	O
replace	O
all	O
terms	O
φij	O
(	O
xi	O
,	O
xj	O
)	O
by	O
approximate	B
factors	O
˜φij	O
(	O
xi	O
)	O
˜φij	O
(	O
xj	O
)	O
then	O
the	O
resulting	O
joint	B
distribution	O
˜p	O
would	O
be	O
factorised	B
and	O
hence	O
tractable	O
.	O
since	O
the	O
variable	B
xi	O
appears	O
in	O
more	O
than	O
one	O
term	O
from	O
p	O
(	O
x	O
)	O
,	O
we	O
need	O
to	O
index	O
the	O
approximation	B
factors	O
.	O
a	O
convenient	O
way	O
to	O
do	O
this	O
is	O
˜p	O
=	O
1	O
˜z	O
˜φ2→1	O
(	O
x1	O
)	O
˜φ1→2	O
(	O
x2	O
)	O
˜φ3→2	O
(	O
x2	O
)	O
˜φ2→3	O
(	O
x3	O
)	O
˜φ4→3	O
(	O
x3	O
)	O
˜φ3→4	O
(	O
x4	O
)	O
˜φ1→4	O
(	O
x4	O
)	O
˜φ4→1	O
(	O
x1	O
)	O
(	O
28.7.3	O
)	O
which	O
is	O
represented	O
in	O
ﬁg	O
(	O
28.10b	O
)	O
.	O
the	O
idea	O
in	O
ep	O
is	O
now	O
to	O
determine	O
the	O
optimal	O
approximation	B
term	O
by	O
the	O
self-consistent	O
requirement	O
that	O
,	O
on	O
replacing	O
it	O
with	O
its	O
exact	O
form	O
,	O
there	O
is	O
no	O
diﬀerence	O
to	O
the	O
marginal	B
of	O
˜p	O
.	O
consider	O
the	O
approximation	B
parameters	O
˜φ3→2	O
(	O
x2	O
)	O
and	O
˜φ2→3	O
(	O
x3	O
)	O
.	O
to	O
set	O
these	O
we	O
ﬁrst	O
replace	O
the	O
contribution	O
˜φ3→2	O
(	O
x2	O
)	O
˜φ2→3	O
(	O
x3	O
)	O
by	O
φ	O
(	O
x2	O
,	O
x3	O
)	O
.	O
this	O
gives	O
˜p∗	O
=	O
1	O
˜z∗	O
˜φ2→1	O
(	O
x1	O
)	O
˜φ1→2	O
(	O
x2	O
)	O
φ	O
(	O
x2	O
,	O
x3	O
)	O
˜φ4→3	O
(	O
x3	O
)	O
˜φ3→4	O
(	O
x4	O
)	O
˜φ1→4	O
(	O
x4	O
)	O
˜φ4→1	O
(	O
x1	O
)	O
(	O
28.7.4	O
)	O
now	O
consider	O
the	O
kullback-leibler	O
divergence	B
between	O
this	O
distribution	B
and	O
our	O
approximation	B
,	O
kl	O
(	O
˜p∗|˜p	O
)	O
=	O
(	O
cid:104	O
)	O
log	O
˜p∗	O
(	O
cid:105	O
)	O
˜p∗	O
−	O
(	O
cid:104	O
)	O
log	O
˜p	O
(	O
cid:105	O
)	O
˜p∗	O
(	O
28.7.5	O
)	O
since	O
our	O
interest	O
is	O
in	O
updating	O
˜φ3→2	O
(	O
x2	O
)	O
and	O
˜φ2→3	O
(	O
x3	O
)	O
,	O
we	O
isolate	O
the	O
contribution	O
from	O
these	O
parameters	O
in	O
the	O
kullback-leibler	O
divergence	B
which	O
gives	O
(	O
cid:68	O
)	O
(	O
cid:69	O
)	O
kl	O
(	O
˜p∗|˜p	O
)	O
=	O
log	O
˜z	O
−	O
log	O
˜φ3→2	O
(	O
x2	O
)	O
˜φ2→3	O
(	O
x3	O
)	O
˜p∗	O
(	O
x2	O
,	O
x3	O
)	O
+	O
const	O
.	O
(	O
28.7.6	O
)	O
also	O
,	O
since	O
˜p	O
is	O
factorised	B
,	O
up	O
to	O
a	O
constant	O
proportionality	O
factor	B
,	O
the	O
dependence	B
of	O
˜z	O
on	O
˜φ3→2	O
(	O
x2	O
)	O
and	O
˜φ2→3	O
(	O
x3	O
)	O
is	O
˜z	O
∝	O
˜φ1→2	O
(	O
x2	O
)	O
˜φ3→2	O
(	O
x2	O
)	O
(	O
cid:88	O
)	O
˜φ2→3	O
(	O
x3	O
)	O
˜φ4→3	O
(	O
x3	O
)	O
(	O
28.7.7	O
)	O
(	O
cid:88	O
)	O
x2	O
x3	O
diﬀerentiating	O
the	O
kullback-leibler	O
divergence	B
equation	O
(	O
28.7.6	O
)	O
with	O
respect	O
to	O
˜φ3→2	O
(	O
x2	O
)	O
and	O
equating	O
to	O
zero	O
,	O
we	O
obtain	O
(	O
cid:80	O
)	O
(	O
cid:80	O
)	O
˜φ1→2	O
(	O
x2	O
)	O
˜φ3→2	O
(	O
x2	O
)	O
˜φ1→2	O
(	O
x2	O
)	O
˜φ3→2	O
(	O
x2	O
)	O
x2	O
=	O
˜p∗	O
(	O
x2	O
)	O
similarly	O
,	O
optimising	O
w.r.t	O
.	O
˜φ2→3	O
(	O
x3	O
)	O
gives	O
˜φ2→3	O
(	O
x3	O
)	O
˜φ4→3	O
(	O
x3	O
)	O
˜φ2→3	O
(	O
x3	O
)	O
˜φ4→3	O
(	O
x3	O
)	O
x3	O
=	O
˜p∗	O
(	O
x3	O
)	O
(	O
28.7.8	O
)	O
(	O
28.7.9	O
)	O
these	O
equations	O
only	O
determine	O
the	O
approximation	B
factors	O
up	O
to	O
a	O
proportionality	O
constant	O
.	O
we	O
can	O
therefore	O
write	O
the	O
optimal	O
updates	O
as	O
˜φ3→2	O
(	O
x2	O
)	O
=	O
z3→2	O
˜p∗	O
(	O
x2	O
)	O
˜φ1→2	O
(	O
x2	O
)	O
and	O
˜φ2→3	O
(	O
x3	O
)	O
=	O
z2→3	O
˜p∗	O
(	O
x3	O
)	O
˜φ4→3	O
(	O
x3	O
)	O
draft	O
march	O
9	O
,	O
2010	O
(	O
28.7.10	O
)	O
(	O
28.7.11	O
)	O
531	O
where	O
z3→2	O
and	O
z2→3	O
are	O
proportionality	O
terms	O
.	O
we	O
can	O
determine	O
the	O
proportionalities	O
by	O
the	O
requirement	O
that	O
the	O
term	O
approximation	B
˜φ3→2	O
(	O
x2	O
)	O
˜φ2→3	O
(	O
x3	O
)	O
has	O
the	O
same	O
eﬀect	O
on	O
the	O
normalisation	B
of	O
˜p	O
as	O
it	O
has	O
expectation	B
propagation	I
on	O
˜p∗	O
.	O
that	O
is	O
(	O
cid:88	O
)	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
=	O
(	O
cid:88	O
)	O
˜φ2→1	O
(	O
x1	O
)	O
˜φ1→2	O
(	O
x2	O
)	O
˜φ3→2	O
(	O
x2	O
)	O
˜φ2→3	O
(	O
x3	O
)	O
˜φ4→3	O
(	O
x3	O
)	O
˜φ3→4	O
(	O
x4	O
)	O
˜φ1→4	O
(	O
x4	O
)	O
˜φ4→1	O
(	O
x1	O
)	O
˜φ2→1	O
(	O
x1	O
)	O
˜φ1→2	O
(	O
x2	O
)	O
φ	O
(	O
x2	O
,	O
x3	O
)	O
˜φ4→3	O
(	O
x3	O
)	O
˜φ3→4	O
(	O
x4	O
)	O
˜φ1→4	O
(	O
x4	O
)	O
˜φ4→1	O
(	O
x1	O
)	O
(	O
28.7.12	O
)	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
which	O
,	O
on	O
substituting	O
in	O
the	O
updates	O
equation	B
(	O
28.7.10	O
)	O
and	O
equation	B
(	O
28.7.11	O
)	O
,	O
reduces	O
to	O
z2→3z3→2	O
=	O
z∗	O
2,3	O
˜z2,3	O
where	O
and	O
z	O
˜z2,3	O
=	O
(	O
cid:88	O
)	O
2,3	O
=	O
(	O
cid:88	O
)	O
∗	O
x2	O
,	O
x3	O
x2	O
,	O
x3	O
˜φ1→2	O
(	O
x2	O
)	O
˜p∗	O
(	O
x2	O
)	O
˜φ1→2	O
(	O
x2	O
)	O
˜p∗	O
(	O
x3	O
)	O
˜φ4→3	O
(	O
x3	O
)	O
˜φ4→3	O
(	O
x3	O
)	O
˜φ1→2	O
(	O
x2	O
)	O
φ	O
(	O
x2	O
,	O
x3	O
)	O
˜φ4→3	O
(	O
x3	O
)	O
(	O
28.7.13	O
)	O
(	O
28.7.14	O
)	O
(	O
28.7.15	O
)	O
any	O
choice	O
of	O
local	B
normalisations	O
z2→3	O
,	O
z3→2	O
that	O
satisﬁes	O
equation	B
(	O
28.7.13	O
)	O
suﬃces	O
to	O
ensure	O
that	O
the	O
scale	O
of	O
the	O
term	O
approximation	B
matches	O
.	O
for	O
example	O
,	O
one	O
may	O
set	O
z2→3	O
=	O
z3→2	O
=	O
z∗	O
2,3	O
˜z2,3	O
(	O
28.7.16	O
)	O
(	O
cid:115	O
)	O
once	O
set	O
,	O
an	O
approximation	B
for	O
the	O
global	B
normalisation	O
constant	O
of	O
p	O
is	O
z	O
≈	O
˜z	O
(	O
28.7.17	O
)	O
the	O
above	O
gives	O
a	O
procedure	O
for	O
updating	O
the	O
terms	O
˜φ3→2	O
(	O
x2	O
)	O
and	O
˜φ2→3	O
(	O
x3	O
)	O
.	O
one	O
then	O
chooses	O
another	O
term	O
and	O
replaces	O
it	O
with	O
its	O
approximation	B
,	O
until	O
the	O
parameters	O
of	O
the	O
approximation	B
converge	O
.	O
the	O
generic	O
procedure	O
is	O
outlined	O
in	O
algorithm	B
(	O
30	O
)	O
.	O
comments	O
on	O
ep	O
•	O
for	O
the	O
mrf	O
example	O
above	O
,	O
ep	O
corresponds	O
to	O
belief	B
propagation	I
(	O
the	O
sum-product	B
form	O
on	O
the	O
factor	B
graph	I
)	O
.	O
this	O
is	O
intuitively	O
clear	O
since	O
in	O
both	O
ep	O
and	O
bp	O
the	O
product	O
of	O
messages	O
incoming	O
to	O
a	O
variable	B
is	O
proportional	O
to	O
the	O
approximation	B
of	O
the	O
marginal	B
of	O
that	O
variable	B
.	O
a	O
diﬀerence	O
,	O
however	O
,	O
is	O
the	O
schedule	B
:	O
in	O
ep	O
all	O
messages	O
corresponding	O
to	O
a	O
term	O
approximation	B
are	O
updated	O
simultaneously	O
(	O
in	O
the	O
above	O
˜φ3→2	O
(	O
x2	O
)	O
and	O
˜φ2→3	O
(	O
x3	O
)	O
)	O
,	O
whereas	O
in	O
bp	O
they	O
are	O
updated	O
in	O
arbitrary	O
order	O
.	O
•	O
ep	O
is	O
a	O
useful	O
extension	O
of	O
bp	O
to	O
cases	O
in	O
which	O
the	O
bp	O
messages	O
can	O
not	O
be	O
easily	O
represented	O
.	O
in	O
the	O
case	O
that	O
the	O
approximating	O
distribution	B
˜p	O
is	O
in	O
the	O
exponential	B
family	I
,	O
the	O
minimal	O
kullback-leibler	O
criterion	O
equates	O
to	O
matching	O
moments	O
of	O
the	O
approximating	O
distribution	B
to	O
p∗	O
.	O
see	O
[	O
246	O
]	O
for	O
a	O
more	O
detailed	O
discussion	O
.	O
•	O
in	O
general	O
there	O
is	O
no	O
need	O
to	O
replace	O
all	O
terms	O
in	O
the	O
joint	B
distribution	O
with	O
factorised	O
approximations	O
.	O
one	O
only	O
needs	O
that	O
the	O
resulting	O
approximating	O
distribution	B
is	O
tractable	O
;	O
this	O
results	O
in	O
a	O
structured	B
expectation	O
propagation	B
algorithm	O
,	O
see	O
ﬁg	O
(	O
28.10c	O
)	O
.	O
•	O
ep	O
and	O
its	O
extensions	O
are	O
closely	O
related	O
to	O
other	O
variational	O
procedures	O
such	O
as	O
tree-reweighting	O
[	O
287	O
]	O
and	O
fractional	O
ep	O
[	O
295	O
]	O
designed	O
to	O
compensate	O
for	O
message	B
overcounting	O
eﬀects	O
.	O
532	O
draft	O
march	O
9	O
,	O
2010	O
algorithm	B
30	O
expectation	B
propagation	I
:	O
approximation	B
of	O
p	O
(	O
x	O
)	O
=	O
1	O
z	O
1	O
:	O
decide	O
on	O
a	O
set	O
of	O
terms	O
φi	O
(	O
xi	O
)	O
to	O
replace	O
with	O
˜φi	O
(	O
xi	O
)	O
in	O
order	O
to	O
reveal	O
a	O
tractable	O
distribution	B
i	O
φi	O
(	O
xi	O
)	O
.	O
(	O
cid:81	O
)	O
map	B
for	O
mrfs	O
(	O
cid:89	O
)	O
i	O
˜p	O
(	O
x	O
)	O
=	O
1	O
˜z	O
˜φi	O
(	O
xi	O
)	O
2	O
:	O
initialise	O
the	O
all	O
parameters	O
˜φi	O
(	O
xi	O
)	O
.	O
3	O
:	O
repeat	O
4	O
:	O
5	O
:	O
select	O
a	O
term	O
φi	O
(	O
xi	O
)	O
from	O
p	O
to	O
update	O
.	O
replace	O
the	O
term	O
φi	O
(	O
xi	O
)	O
by	O
the	O
tractable	O
term	O
˜φi	O
(	O
xi	O
)	O
to	O
form	O
˜φ∗	O
≡	O
φi	O
(	O
xi	O
)	O
=	O
φi	O
(	O
xi	O
)	O
(	O
cid:89	O
)	O
(	O
cid:81	O
)	O
˜φj	O
(	O
xj	O
)	O
˜φi	O
(	O
xi	O
)	O
˜φj	O
(	O
xj	O
)	O
j	O
(	O
cid:54	O
)	O
=i	O
j	O
find	O
the	O
parameters	O
of	O
˜φi	O
(	O
xi	O
)	O
by	O
˜φi	O
(	O
xi	O
)	O
∝	O
argmin	O
˜φi	O
(	O
xi	O
)	O
˜p	O
(	O
x	O
)	O
∝	O
kl	O
(	O
˜p∗|˜p	O
)	O
(	O
cid:89	O
)	O
˜φj	O
(	O
xj	O
)	O
=	O
(	O
cid:88	O
)	O
x	O
˜p∗	O
∝	O
˜φ∗	O
,	O
(	O
cid:88	O
)	O
set	O
any	O
proportionality	O
terms	O
of	O
˜φi	O
(	O
xi	O
)	O
by	O
requiring	O
˜φi	O
(	O
xi	O
)	O
(	O
cid:89	O
)	O
i	O
6	O
:	O
7	O
:	O
where	O
x	O
j	O
(	O
cid:54	O
)	O
=i	O
φi	O
(	O
xi	O
)	O
(	O
cid:89	O
)	O
(	O
cid:89	O
)	O
1	O
˜z	O
i	O
8	O
:	O
until	O
converged	O
9	O
:	O
return	O
j	O
˜φj	O
(	O
xj	O
)	O
˜z	O
=	O
(	O
cid:88	O
)	O
(	O
cid:89	O
)	O
x	O
i	O
˜p	O
(	O
x	O
)	O
=	O
˜φi	O
(	O
xi	O
)	O
,	O
˜φi	O
(	O
xi	O
)	O
(	O
28.7.18	O
)	O
(	O
28.7.19	O
)	O
(	O
28.7.20	O
)	O
(	O
28.7.21	O
)	O
(	O
28.7.22	O
)	O
(	O
28.7.23	O
)	O
as	O
an	O
approximation	B
to	O
p	O
(	O
x	O
)	O
,	O
where	O
˜z	O
approximates	O
the	O
normalisation	B
constant	I
z	O
.	O
28.8	O
map	B
for	O
mrfs	O
consider	O
a	O
pairwise	B
mrf	O
p	O
(	O
x	O
)	O
∝	O
ee	O
(	O
x	O
)	O
with	O
(	O
cid:88	O
)	O
i∼j	O
f	O
(	O
xi	O
,	O
xj	O
)	O
+	O
(	O
cid:88	O
)	O
i	O
e	O
(	O
x	O
)	O
≡	O
g	O
(	O
xi	O
,	O
x0	O
i	O
)	O
(	O
28.8.1	O
)	O
where	O
i	O
∼	O
j	O
denotes	O
neighbouring	O
variables	O
.	O
here	O
the	O
terms	O
f	O
(	O
xi	O
,	O
xj	O
)	O
represent	O
pairwise	B
interactions	O
.	O
the	O
i	O
)	O
represent	O
unary	O
interactions	O
,	O
written	O
for	O
convenience	O
in	O
terms	O
of	O
a	O
pairwise	B
interaction	O
with	O
terms	O
g	O
(	O
xi	O
,	O
x0	O
a	O
ﬁxed	O
(	O
non-variable	O
)	O
x0	O
.	O
typically	O
the	O
term	O
f	O
(	O
xi	O
,	O
xj	O
)	O
is	O
used	O
to	O
ensure	O
that	O
neighbouring	O
variables	O
xi	O
and	O
xj	O
are	O
in	O
similar	O
states	O
;	O
the	O
term	O
g	O
(	O
xi	O
,	O
x0	O
i	O
.	O
such	O
models	O
have	O
application	O
in	O
areas	O
such	O
as	O
computer	O
vision	O
and	O
image	O
restoration	O
in	O
which	O
an	O
observed	O
noisy	O
image	O
x0	O
is	O
to	O
be	O
cleaned	O
,	O
ﬁg	O
(	O
28.3	O
)	O
.	O
to	O
do	O
so	O
we	O
seek	O
a	O
clean	O
image	O
x	O
for	O
which	O
each	O
clean	O
pixel	O
value	B
xi	O
is	O
close	O
to	O
the	O
observed	O
noisy	O
pixel	O
value	B
x0	O
i	O
)	O
is	O
used	O
to	O
bias	B
xi	O
to	O
be	O
close	O
to	O
a	O
desired	O
state	O
x0	O
i	O
,	O
whilst	O
being	O
in	O
a	O
similar	O
state	O
to	O
its	O
clean	O
neighbours	O
.	O
28.8.1	O
map	B
assignment	O
the	O
map	B
assignment	O
of	O
a	O
set	O
of	O
variables	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
corresponds	O
to	O
that	O
joint	B
x	O
that	O
maximises	O
e	O
(	O
x	O
)	O
.	O
for	O
a	O
general	O
graph	B
connectivity	O
we	O
can	O
not	O
naively	O
exploit	O
dynamic	B
programming	O
intuitions	O
to	O
ﬁnd	O
an	O
draft	O
march	O
9	O
,	O
2010	O
533	O
map	B
for	O
mrfs	O
b	O
c	O
f	O
(	O
a	O
)	O
a	O
e	O
d	O
a	O
e	O
b	O
c	O
f	O
(	O
b	O
)	O
figure	O
28.11	O
:	O
(	O
a	O
)	O
:	O
a	O
graph	B
with	O
bidirectional	O
weights	O
(	O
b	O
)	O
:	O
a	O
graph	B
cut	I
partitions	O
the	O
nodes	O
wij	O
=	O
wji	O
.	O
into	O
two	O
groups	O
s	O
(	O
blue	O
)	O
and	O
t	O
(	O
red	O
)	O
.	O
the	O
weight	B
of	O
the	O
cut	B
is	O
the	O
sum	O
of	O
the	O
edge	O
weights	O
from	O
s	O
(	O
blue	O
)	O
to	O
t	O
(	O
red	O
)	O
.	O
intuitively	O
,	O
it	O
is	O
clear	O
that	O
after	O
assign-	O
ing	O
nodes	O
to	O
state	O
1	O
(	O
for	O
blue	O
)	O
and	O
0	O
(	O
red	O
)	O
that	O
the	O
weight	B
of	O
the	O
cut	B
corresponds	O
to	O
the	O
summed	O
weights	O
of	O
neighbours	O
in	O
diﬀerent	O
states	O
.	O
here	O
we	O
high-	O
light	O
those	O
weight	B
contributions	O
.	O
the	O
non-highlighted	O
edges	O
do	O
not	O
contribute	O
to	O
the	O
cut	B
weight	O
.	O
note	O
that	O
only	O
one	O
of	O
the	O
edge	O
directions	O
contributes	O
to	O
the	O
cut	B
.	O
d	O
exact	O
solution	O
since	O
the	O
graph	B
is	O
loopy	B
.	O
as	O
we	O
see	O
below	O
,	O
in	O
special	O
cases	O
,	O
even	O
though	O
the	O
graph	B
is	O
loopy	B
,	O
this	O
is	O
possible	O
.	O
in	O
general	O
,	O
however	O
,	O
approximate	B
algorithms	O
are	O
required	O
.	O
a	O
simple	O
general	O
approximate	B
solution	O
can	O
be	O
found	O
as	O
follows	O
:	O
ﬁrst	O
initialise	O
all	O
x	O
at	O
random	O
.	O
then	O
select	O
a	O
variable	B
xi	O
and	O
ﬁnd	O
the	O
state	O
of	O
xi	O
that	O
maximally	O
improves	O
e	O
(	O
x	O
)	O
,	O
keeping	O
all	O
other	O
variables	O
ﬁxed	O
.	O
one	O
then	O
repeats	O
this	O
selection	O
and	O
local	B
maximal	O
state	O
computation	O
until	O
convergence	O
.	O
this	O
is	O
called	O
iterated	O
conditional	B
modes	O
[	O
36	O
]	O
.	O
due	O
to	O
the	O
markov	O
properties	B
its	O
clear	O
that	O
we	O
can	O
improve	O
on	O
this	O
icm	O
method	O
by	O
simultaneously	O
optimising	O
all	O
variables	O
conditioned	O
on	O
their	O
respective	O
markov	O
blankets	O
(	O
similar	O
to	O
the	O
approach	B
used	O
in	O
black-white	O
sampling	B
)	O
.	O
another	O
improvement	O
is	O
to	O
update	O
only	O
a	O
subset	O
of	O
the	O
variables	O
,	O
where	O
the	O
subset	O
has	O
the	O
form	O
of	O
singly-connected	B
structure	O
.	O
by	O
recursively	O
clamping	O
variables	O
to	O
reveal	O
a	O
singly-connected	B
structure	O
on	O
un-clamped	O
variables	O
,	O
one	O
may	O
ﬁnd	O
an	O
approximate	B
solution	O
by	O
solving	B
a	O
sequence	O
of	O
tractable	O
problems	O
.	O
remarkably	O
,	O
in	O
the	O
special	O
case	O
of	O
binary	O
variables	O
and	O
positive	O
w	O
discussed	O
below	O
,	O
an	O
eﬃcient	B
exact	O
algorithm	B
exists	O
for	O
ﬁnding	O
the	O
map	B
state	O
,	O
regardless	O
of	O
the	O
topology	O
.	O
28.8.2	O
attractive	B
binary	I
mrfs	O
consider	O
ﬁnding	O
the	O
map	B
of	O
a	O
mrf	O
with	O
binary	O
variables	O
dom	O
(	O
xi	O
)	O
=	O
{	O
0	O
,	O
1	O
}	O
and	O
positive	O
connections	O
wij	O
≥	O
0.	O
in	O
this	O
case	O
our	O
task	O
is	O
to	O
ﬁnd	O
the	O
assignment	O
x	O
that	O
maximises	O
cixi	O
(	O
28.8.2	O
)	O
(	O
cid:88	O
)	O
i∼j	O
wiji	O
[	O
xi	O
=	O
xj	O
]	O
+	O
(	O
cid:88	O
)	O
i	O
e	O
(	O
x	O
)	O
≡	O
where	O
i	O
∼	O
j	O
denotes	O
neighbouring	O
variables	O
and	O
real	O
ci	O
.	O
note	O
that	O
for	O
binary	O
variables	O
xi	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
(	O
28.8.3	O
)	O
for	O
this	O
particular	O
case	O
an	O
eﬃcient	B
map	O
algorithm	B
exists	O
for	O
arbitrary	O
topology	O
of	O
w	O
[	O
121	O
]	O
.	O
the	O
algorithm	B
ﬁrst	O
translates	O
the	O
map	B
assignment	O
problem	B
into	O
an	O
equivalent	B
min	O
s-t-cut	O
problem	B
[	O
39	O
]	O
,	O
for	O
which	O
eﬃcient	B
algorithms	O
exist	O
.	O
in	O
min	O
s-t-cut	O
,	O
we	O
need	O
a	O
graph	B
with	O
positive	O
weights	O
on	O
the	O
edges	O
.	O
this	O
is	O
clearly	O
satisﬁed	O
i	O
cixi	O
need	O
to	O
be	O
addressed	O
.	O
(	O
28.8.4	O
)	O
(	O
28.8.5	O
)	O
draft	O
march	O
9	O
,	O
2010	O
to	O
translate	O
the	O
map	B
assignment	O
problem	B
to	O
a	O
min-cut	O
problem	B
we	O
need	O
to	O
deal	O
with	O
the	O
additional	O
linear	B
i	O
cixi	O
.	O
first	O
consider	O
the	O
eﬀect	O
of	O
including	O
a	O
new	O
node	B
x∗	O
and	O
connecting	O
this	O
to	O
each	O
existing	O
dealing	O
with	O
the	O
bias	B
terms	O
i	O
[	O
xi	O
=	O
xj	O
]	O
=	O
xixj	O
+	O
(	O
1	O
−	O
xi	O
)	O
(	O
1	O
−	O
xj	O
)	O
if	O
wij	O
>	O
0	O
,	O
although	O
the	O
bias	B
terms	O
(	O
cid:80	O
)	O
terms	O
(	O
cid:80	O
)	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
cii	O
[	O
xi	O
=	O
x∗	O
]	O
=	O
(	O
cid:88	O
)	O
i	O
i	O
cixi	O
node	B
i	O
with	O
weight	O
ci	O
.	O
this	O
adds	O
then	O
a	O
term	O
if	O
we	O
set	O
x∗	O
in	O
state	O
1	O
,	O
this	O
will	O
then	O
add	O
terms	O
ci	O
(	O
xix∗	O
+	O
(	O
1	O
−	O
xi	O
)	O
(	O
1	O
−	O
x∗	O
)	O
)	O
i	O
534	O
map	B
for	O
mrfs	O
s	O
s	O
a+	O
e−	O
a	O
e	O
b−	O
c+	O
b	O
c	O
f+	O
(	O
a	O
)	O
f	O
(	O
b	O
)	O
d−	O
d	O
t	O
t	O
(	O
a	O
)	O
:	O
a	O
graph	B
with	O
bidirectional	O
figure	O
28.12	O
:	O
weights	O
wij	O
=	O
wji	O
augmented	B
with	O
a	O
source	O
node	B
s	O
and	O
sink	O
node	B
t.	O
each	O
node	B
has	O
a	O
corresponding	O
bias	B
whose	O
sign	O
is	O
indicated	O
.	O
the	O
source	O
node	B
is	O
linked	O
to	O
the	O
nodes	O
corresponding	O
to	O
positive	O
bias	O
,	O
and	O
the	O
(	O
b	O
)	O
:	O
a	O
graph	B
nodes	O
with	O
negative	O
bias	B
to	O
the	O
sink	O
.	O
cut	B
partitions	O
the	O
nodes	O
into	O
two	O
groups	O
s	O
(	O
blue	O
)	O
and	O
t	O
(	O
red	O
)	O
,	O
where	O
s	O
is	O
the	O
union	O
of	O
the	O
source	O
node	B
and	O
nodes	O
in	O
state	O
1	O
,	O
t	O
is	O
the	O
union	O
of	O
the	O
sink	O
node	B
and	O
nodes	O
in	O
state	O
0.	O
the	O
weight	B
of	O
the	O
cut	B
is	O
the	O
sum	O
of	O
the	O
edge	O
weights	O
from	O
s	O
(	O
blue	O
)	O
to	O
t	O
(	O
red	O
)	O
.	O
the	O
red	O
lines	O
indicate	O
contributions	O
to	O
the	O
cut	B
,	O
and	O
can	O
be	O
considered	O
penalties	O
since	O
we	O
wish	O
to	O
ﬁnd	O
the	O
mini-	O
mal	O
cut	B
.	O
for	O
example	O
a	O
being	O
in	O
state	O
1	O
(	O
blue	O
)	O
does	O
not	O
incur	O
a	O
penalty	O
since	O
ca	O
>	O
0	O
;	O
on	O
the	O
other	O
hand	O
,	O
variable	B
f	O
being	O
in	O
state	O
0	O
(	O
red	O
)	O
incurs	O
a	O
penalty	O
since	O
cf	O
>	O
0.	O
otherwise	O
,	O
if	O
we	O
set	O
x∗	O
in	O
state	O
0	O
we	O
obtain	O
(	O
cid:88	O
)	O
i	O
ci	O
(	O
1	O
−	O
xi	O
)	O
=	O
−	O
(	O
cid:88	O
)	O
i	O
cixi	O
+	O
const	O
.	O
(	O
28.8.6	O
)	O
since	O
our	O
requirement	O
is	O
that	O
we	O
need	O
the	O
weights	O
to	O
be	O
positive	O
we	O
see	O
that	O
we	O
can	O
achieve	O
this	O
by	O
deﬁning	O
two	O
additional	O
nodes	O
.	O
we	O
deﬁne	O
a	O
source	O
node	B
xs	O
,	O
set	O
to	O
state	O
1	O
and	O
connect	O
it	O
to	O
those	O
xi	O
which	O
have	O
positive	O
ci	O
,	O
deﬁning	O
wsi	O
=	O
ci	O
.	O
in	O
addition	O
we	O
deﬁne	O
a	O
sink	O
node	B
xt	O
=	O
0	O
and	O
connect	O
all	O
nodes	O
with	O
negative	O
ci	O
,	O
to	O
xt	O
,	O
using	O
weight	B
wit	O
=	O
−ci	O
,	O
(	O
which	O
is	O
therefore	O
positive	O
)	O
.	O
for	O
the	O
source	O
node	B
clamped	O
to	O
xs	O
=	O
1	O
and	O
the	O
sink	O
node	B
to	O
xt	O
=	O
0	O
,	O
then	O
including	O
the	O
source	O
and	O
sink	O
,	O
we	O
have	O
wiji	O
[	O
xi	O
=	O
xj	O
]	O
+	O
const	O
.	O
(	O
28.8.7	O
)	O
e	O
(	O
x	O
)	O
=	O
(	O
cid:88	O
)	O
i∼j	O
is	O
equal	O
to	O
the	O
energy	B
function	O
,	O
equation	B
(	O
28.8.2	O
)	O
.	O
deﬁnition	O
115	O
(	O
graph	B
cut	I
)	O
.	O
for	O
a	O
graph	B
g	O
with	O
vertices	O
v1	O
,	O
.	O
.	O
.	O
,	O
vd	O
,	O
and	O
weights	O
wij	O
>	O
0	O
a	O
cut	B
is	O
a	O
partition	O
of	O
the	O
vertices	O
into	O
two	O
disjoint	O
groups	O
,	O
called	O
s	O
and	O
t	O
.	O
the	O
weight	B
of	O
a	O
cut	B
is	O
then	O
deﬁned	O
as	O
the	O
sum	O
of	O
the	O
weights	O
that	O
leave	O
s	O
and	O
land	O
in	O
t	O
,	O
see	O
ﬁg	O
(	O
28.11	O
)	O
.	O
the	O
weight	B
of	O
a	O
cut	B
corresponds	O
to	O
the	O
sum	O
of	O
weights	O
between	O
mismatched	O
neighbours	O
,	O
see	O
ﬁg	O
(	O
28.11b	O
)	O
.	O
that	O
is	O
,	O
(	O
28.8.8	O
)	O
(	O
28.8.9	O
)	O
535	O
wiji	O
[	O
xi	O
(	O
cid:54	O
)	O
=	O
xj	O
]	O
cut	B
(	O
x	O
)	O
=	O
(	O
cid:88	O
)	O
cut	B
(	O
x	O
)	O
=	O
(	O
cid:88	O
)	O
i∼j	O
i∼j	O
since	O
i	O
[	O
xi	O
(	O
cid:54	O
)	O
=	O
xj	O
]	O
=	O
1	O
−	O
i	O
[	O
xi	O
=	O
xj	O
]	O
,	O
we	O
can	O
deﬁne	O
the	O
weight	B
of	O
the	O
cut	B
equivalently	O
as	O
wij	O
(	O
1	O
−	O
i	O
[	O
xi	O
=	O
xj	O
]	O
)	O
=	O
−	O
wiji	O
[	O
xi	O
=	O
xj	O
]	O
+	O
const	O
.	O
(	O
cid:88	O
)	O
i∼j	O
draft	O
march	O
9	O
,	O
2010	O
map	B
for	O
mrfs	O
(	O
a	O
)	O
:	O
clean	O
im-	O
figure	O
28.13	O
:	O
(	O
c	O
)	O
:	O
age	O
.	O
restored	O
image	O
using	O
icm	O
.	O
see	O
demomrfclean.m	O
.	O
(	O
b	O
)	O
:	O
noisy	O
image	O
.	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
so	O
that	O
the	O
minimal	O
cut	B
assignment	O
will	O
correspond	O
to	O
maximising	O
(	O
cid:80	O
)	O
which	O
take	O
o	O
(	O
cid:0	O
)	O
d3	O
(	O
cid:1	O
)	O
operations	O
or	O
less	O
.	O
this	O
means	O
that	O
one	O
can	O
ﬁnd	O
the	O
exact	O
map	O
assignment	O
of	O
an	O
attractive	B
binary	I
mrf	O
eﬃciently	O
in	O
o	O
(	O
cid:0	O
)	O
d3	O
(	O
cid:1	O
)	O
operations	O
.	O
in	O
maxflow.m	O
we	O
implement	O
the	O
ford-fulkerson	O
i∼j	O
wiji	O
[	O
xi	O
=	O
xj	O
]	O
.	O
in	O
the	O
mrf	O
case	O
,	O
our	O
translation	O
into	O
a	O
weighted	O
graph	B
with	O
positive	O
interactions	O
then	O
requires	O
that	O
we	O
identify	O
the	O
source	O
and	O
all	O
other	O
variables	O
assigned	O
to	O
state	O
1	O
with	O
s	O
,	O
and	O
the	O
sink	O
and	O
all	O
variables	O
in	O
state	O
0	O
with	O
t	O
,	O
see	O
ﬁg	O
(	O
28.12	O
)	O
.	O
a	O
fundamental	O
result	O
is	O
that	O
the	O
min	O
s-t-cut	O
solution	O
corresponds	O
to	O
the	O
max-ﬂow	O
solution	O
from	O
the	O
source	O
s	O
to	O
the	O
sink	O
t	O
[	O
39	O
]	O
.	O
there	O
are	O
eﬃcient	B
algorithms	O
for	O
max-ﬂow	O
,	O
see	O
for	O
example	O
[	O
47	O
]	O
,	O
(	O
edmonds-karp-dinic	O
breadth	O
ﬁrst	O
search	O
variant	O
)	O
[	O
87	O
]	O
,	O
see	O
also	O
exercise	O
(	O
251	O
)	O
.	O
e	O
(	O
x	O
)	O
=	O
(	O
cid:88	O
)	O
wiji	O
[	O
xi	O
=	O
xj	O
]	O
+	O
(	O
cid:88	O
)	O
example	O
118	O
(	O
analysing	O
dirty	O
pictures	O
)	O
.	O
in	O
ﬁg	O
(	O
28.13	O
)	O
we	O
present	O
a	O
noisy	O
binary	O
y	O
image	O
that	O
we	O
wish	O
to	O
clean	O
.	O
to	O
do	O
so	O
we	O
use	O
an	O
objective	O
i	O
[	O
xi	O
=	O
yi	O
]	O
(	O
28.8.10	O
)	O
ij	O
i	O
the	O
variables	O
xi	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
784	O
are	O
deﬁned	O
on	O
a	O
28×28	O
grid	O
and	O
where	O
wij	O
=	O
10	O
if	O
xi	O
and	O
xj	O
are	O
neighbours	O
on	O
the	O
grid	O
.	O
using	O
i	O
[	O
xi	O
=	O
yi	O
]	O
=	O
xi	O
(	O
2yi	O
−	O
1	O
)	O
+	O
const	O
.	O
(	O
28.8.11	O
)	O
we	O
have	O
a	O
standard	O
binary	O
mrf	O
map	B
problem	O
with	O
‘	O
bias	B
’	O
b	O
=	O
2y	O
−	O
1.	O
once	O
can	O
then	O
ﬁnd	O
the	O
exact	O
optimal	O
x	O
by	O
the	O
min-cut	O
procedure	O
.	O
however	O
,	O
our	O
implementation	O
of	O
this	O
is	O
slow	O
and	O
instead	O
we	O
use	O
the	O
simpler	O
icm	O
algorithm	B
,	O
with	O
results	O
as	O
shown	O
in	O
ﬁg	O
(	O
28.13	O
)	O
.	O
28.8.3	O
potts	O
model	B
an	O
extension	O
of	O
the	O
previous	O
model	B
is	O
to	O
the	O
case	O
when	O
the	O
variables	O
are	O
non-binary	O
,	O
which	O
is	O
termed	O
the	O
potts	O
model	B
:	O
e	O
(	O
x	O
)	O
=	O
(	O
cid:88	O
)	O
wiji	O
[	O
xi	O
=	O
xj	O
]	O
+	O
(	O
cid:88	O
)	O
cii	O
(	O
cid:2	O
)	O
xi	O
=	O
x0	O
i	O
(	O
cid:3	O
)	O
(	O
28.8.12	O
)	O
i∼j	O
i	O
i	O
are	O
known	O
.	O
this	O
model	B
has	O
immediate	O
application	O
in	O
non-binary	O
image	O
restora-	O
where	O
wij	O
>	O
0	O
and	O
the	O
x0	O
tion	O
,	O
and	O
also	O
in	O
clustering	B
based	O
on	O
a	O
similarity	O
score	O
.	O
whilst	O
no	O
eﬃcient	O
exact	O
algorithm	O
is	O
known	O
,	O
a	O
useful	O
approach	B
is	O
to	O
approximate	B
the	O
problem	B
as	O
a	O
sequence	O
of	O
binary	O
problems	O
,	O
as	O
we	O
describe	O
below	O
.	O
potts	O
to	O
binary	O
mrf	O
translation	O
consider	O
the	O
α-expansion	B
representation	O
xi	O
=	O
siα	O
+	O
(	O
1	O
−	O
si	O
)	O
xold	O
i	O
(	O
28.8.13	O
)	O
or	O
α	O
,	O
where	O
si	O
∈	O
{	O
0	O
,	O
1	O
}	O
and	O
for	O
a	O
given	O
α	O
∈	O
{	O
0	O
,	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
.	O
this	O
restricts	O
xi	O
to	O
be	O
either	O
the	O
state	O
xold	O
depending	O
on	O
the	O
binary	O
variable	O
si	O
.	O
using	O
a	O
new	O
binary	O
variable	O
s	O
we	O
can	O
therefore	O
restrict	O
x	O
to	O
a	O
subpart	O
i	O
536	O
draft	O
march	O
9	O
,	O
2010	O
map	B
for	O
mrfs	O
figure	O
28.14	O
:	O
(	O
a	O
)	O
:	O
noisy	O
im-	O
age	O
.	O
(	O
b	O
)	O
:	O
restored	O
image	O
.	O
the	O
α-expansion	B
method	O
was	O
used	O
,	O
with	O
suitable	O
interactions	O
w	O
and	O
bias	B
c	O
to	O
ensure	O
reasonable	O
re-	O
sults	O
.	O
from	O
[	O
47	O
]	O
.	O
(	O
a	O
)	O
(	O
b	O
)	O
of	O
the	O
full	O
space	O
and	O
write	O
a	O
new	O
objective	O
function	B
in	O
terms	O
of	O
s	O
alone	O
:	O
e	O
(	O
s	O
)	O
=	O
(	O
cid:88	O
)	O
i∼j	O
i	O
[	O
si	O
=	O
sj	O
]	O
+	O
(	O
cid:88	O
)	O
(	O
cid:48	O
)	O
ij	O
w	O
i	O
(	O
cid:48	O
)	O
isi	O
+	O
const	O
.	O
c	O
(	O
28.8.14	O
)	O
for	O
w	O
(	O
cid:48	O
)	O
ij	O
>	O
0.	O
this	O
new	O
problem	B
is	O
of	O
the	O
form	O
of	O
an	O
attractive	B
binary	I
mrf	O
which	O
can	O
be	O
solved	O
exactly	O
using	O
the	O
graph	B
cuts	O
procedure	O
.	O
the	O
idea	O
is	O
then	O
to	O
choose	O
another	O
α	O
value	O
(	O
at	O
random	O
)	O
and	O
then	O
ﬁnd	O
the	O
optimal	O
s	O
for	O
the	O
new	O
α.	O
in	O
this	O
way	O
we	O
are	O
guaranteed	O
to	O
iteratively	O
increase	O
e.	O
for	O
a	O
given	O
α	O
and	O
xold	O
,	O
the	O
transformation	O
of	O
the	O
potts	O
model	B
objective	O
is	O
given	O
by	O
using	O
si	O
∈	O
{	O
0	O
,	O
1	O
}	O
and	O
considering	O
i	O
[	O
xi	O
=	O
xj	O
]	O
=	O
i	O
(	O
cid:104	O
)	O
i	O
=	O
sjα	O
+	O
(	O
1	O
−	O
sj	O
)	O
xold	O
siα	O
+	O
(	O
1	O
−	O
si	O
)	O
xold	O
i	O
=	O
xold	O
xold	O
=	O
sisjuij	O
+	O
aisi	O
+	O
bjsj	O
+	O
const	O
.	O
j	O
=	O
(	O
1	O
−	O
si	O
)	O
(	O
1	O
−	O
sj	O
)	O
i	O
(	O
cid:104	O
)	O
−	O
i	O
(	O
cid:104	O
)	O
xold	O
i	O
=	O
α	O
(	O
cid:105	O
)	O
(	O
cid:105	O
)	O
j	O
(	O
cid:105	O
)	O
+	O
(	O
1	O
−	O
si	O
)	O
sji	O
(	O
cid:104	O
)	O
(	O
cid:105	O
)	O
with	O
uij	O
≡	O
1	O
−	O
i	O
(	O
cid:104	O
)	O
(	O
cid:105	O
)	O
+	O
i	O
(	O
cid:104	O
)	O
xold	O
j	O
=	O
α	O
xold	O
i	O
=	O
xold	O
j	O
(	O
cid:105	O
)	O
+	O
si	O
(	O
1	O
−	O
sj	O
)	O
i	O
(	O
cid:104	O
)	O
(	O
cid:105	O
)	O
xold	O
j	O
=	O
α	O
xold	O
i	O
=	O
α	O
+	O
sisj	O
(	O
28.8.15	O
)	O
(	O
28.8.16	O
)	O
and	O
similarly	O
deﬁned	O
ai	O
,	O
bi	O
.	O
by	O
enumeration	O
it	O
is	O
straightforward	O
to	O
show	O
that	O
uij	O
is	O
either	O
0	O
,	O
1	O
or	O
2.	O
using	O
the	O
mathematical	O
identity	O
sisj	O
=	O
1	O
2	O
(	O
i	O
[	O
si	O
=	O
sj	O
]	O
+	O
si	O
+	O
sj	O
−	O
1	O
)	O
we	O
can	O
write	O
,	O
i	O
[	O
xi	O
=	O
xj	O
]	O
=	O
uij	O
2	O
(	O
i	O
[	O
si	O
=	O
sj	O
]	O
+	O
si	O
+	O
sj	O
)	O
+	O
aisi	O
+	O
bjsj	O
+	O
const	O
.	O
(	O
28.8.17	O
)	O
(	O
28.8.18	O
)	O
hence	O
terms	O
wiji	O
[	O
xi	O
=	O
xj	O
]	O
translate	O
to	O
positive	O
interaction	O
terms	O
i	O
[	O
si	O
=	O
sj	O
]	O
wijuij/2	O
.	O
all	O
the	O
unary	O
terms	O
are	O
easily	O
exactly	O
mapped	O
into	O
corresponding	O
unary	O
terms	O
c	O
(	O
cid:48	O
)	O
i	O
deﬁned	O
as	O
the	O
sum	O
of	O
all	O
unary	O
terms	O
in	O
si	O
.	O
this	O
shows	O
that	O
the	O
positive	O
interaction	O
wij	O
in	O
terms	O
of	O
the	O
original	O
variables	O
x	O
maps	O
to	O
a	O
positive	O
interaction	O
in	O
the	O
new	O
variables	O
s.	O
hence	O
we	O
can	O
ﬁnd	O
the	O
maximal	O
state	O
of	O
s	O
using	O
a	O
graph	B
cut	I
algorithm	I
.	O
a	O
related	O
(	O
though	O
diﬀerent	O
)	O
procedure	O
is	O
outlined	O
in	O
[	O
48	O
]	O
.	O
isi	O
for	O
c	O
(	O
cid:48	O
)	O
draft	O
march	O
9	O
,	O
2010	O
537	O
exercises	O
example	O
119	O
(	O
potts	O
model	B
for	O
image	O
reconstruction	O
)	O
.	O
an	O
example	O
image	O
restoration	O
problem	B
for	O
nearest	B
neighbour	I
interactions	O
on	O
a	O
pixel	O
lattice	O
and	O
suitably	O
chosen	O
w	O
,	O
c	O
is	O
given	O
in	O
ﬁg	O
(	O
28.14	O
)	O
.	O
the	O
images	O
are	O
non-binary	O
and	O
therefore	O
the	O
optimal	O
map	B
assignment	O
can	O
not	O
be	O
computed	O
exactly	O
in	O
an	O
eﬃcient	B
way	O
.	O
the	O
alpha-expansion	B
technique	O
was	O
used	O
here	O
combined	O
with	O
an	O
eﬃcient	B
min-cut	O
approach	B
,	O
see	O
[	O
47	O
]	O
for	O
details	O
.	O
28.9	O
further	O
reading	O
approximate	B
inference	I
is	O
a	O
highly	O
active	B
research	O
area	O
and	O
increasingly	O
links	O
to	O
convex	O
optimisation	O
[	O
46	O
]	O
are	O
being	O
developed	O
.	O
see	O
[	O
287	O
]	O
for	O
a	O
general	O
overview	O
and	O
[	O
247	O
]	O
for	O
recent	O
application	O
of	O
convex	O
optimisation	O
to	O
approximate	B
inference	I
in	O
a	O
practical	O
machine	O
learning	B
application	O
.	O
28.10	O
code	O
loopybp.m	O
:	O
loopy	B
belief	O
propagation	B
(	O
factor	B
graph	I
formalism	O
)	O
demoloopybp.m	O
:	O
demo	O
of	O
loopy	B
belief	O
propagation	B
demomfbpgibbs.m	O
:	O
comparison	O
of	O
mean	B
field	O
,	O
belief	B
propagation	I
and	O
gibbs	O
sampling	B
demomrfclean.m	O
:	O
demo	O
of	O
analysing	O
a	O
dirty	O
picture	O
maxflow.m	O
:	O
max-flow	O
min-cut	O
algorithm	B
(	O
ford-fulkerson	O
)	O
binarymrfmap.m	O
:	O
optimising	O
a	O
binary	O
mrf	O
28.11	O
exercises	O
exercise	O
251.	O
for	O
the	O
max-ﬂow-min-cut	O
problem	B
,	O
under	O
the	O
convention	O
that	O
the	O
source	O
node	B
xs	O
is	O
clamped	O
to	O
state	O
1	O
,	O
and	O
the	O
sink	O
node	B
xt	O
to	O
state	O
0	O
,	O
a	O
standard	O
min-cut	O
algorithm	B
returns	O
that	O
joint	B
x	O
which	O
wiji	O
[	O
xi	O
=	O
1	O
]	O
i	O
[	O
xj	O
=	O
0	O
]	O
minimises	O
(	O
cid:88	O
)	O
(	O
cid:88	O
)	O
ij	O
explain	O
how	O
this	O
can	O
be	O
written	O
in	O
the	O
form	O
˜wiji	O
[	O
xi	O
(	O
cid:54	O
)	O
=	O
xj	O
]	O
ij	O
(	O
28.11.1	O
)	O
(	O
28.11.2	O
)	O
exercise	O
252.	O
using	O
brmltoolbox	O
,	O
write	O
a	O
routine	O
kldiv	O
(	O
q	O
,	O
p	O
)	O
that	O
returns	O
the	O
kullback-leibler	O
di-	O
vergence	O
between	O
two	O
discrete	B
distributions	O
q	O
and	O
p	O
deﬁned	O
as	O
potentials	O
q	O
and	O
p.	O
exercise	O
253.	O
the	O
ﬁle	O
p.mat	O
contains	O
a	O
distribution	B
p	O
(	O
x	O
,	O
y	O
,	O
z	O
)	O
on	O
ternary	O
state	O
variables	O
.	O
using	O
brml-	O
toolbox	O
,	O
ﬁnd	O
the	O
best	O
approximation	B
q	O
(	O
x	O
,	O
y	O
)	O
q	O
(	O
z	O
)	O
that	O
minimises	O
the	O
kullback-leibler	O
divergence	B
kl	O
(	O
q|p	O
)	O
and	O
state	O
the	O
value	B
of	O
the	O
minimal	O
kullback-leibler	O
divergence	B
for	O
the	O
optimal	O
q.	O
exercise	O
254.	O
consider	O
the	O
pairwise	B
mrf	O
deﬁned	O
on	O
a	O
2	O
×	O
2	O
lattice	O
,	O
as	O
given	O
in	O
pmrf.mat	O
.	O
using	O
brmltoolbox	O
,	O
1.	O
find	O
the	O
optimal	O
fully	O
factorised	B
approximation	O
(	O
cid:81	O
)	O
4	O
2.	O
find	O
the	O
optimal	O
fully	O
factorised	B
approximation	O
(	O
cid:81	O
)	O
4	O
factor	B
graph	I
formalism	O
.	O
equations	O
.	O
i=1	O
qbp	O
i	O
by	O
loopy	B
belief	O
propagation	B
,	O
based	O
on	O
the	O
i=1	O
qm	O
f	O
i	O
by	O
solving	B
the	O
variational	O
mean	O
field	O
3.	O
by	O
pure	O
enumeration	O
,	O
compute	O
the	O
exact	O
marginals	O
pi	O
.	O
538	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
4.	O
averaged	O
over	O
all	O
4	O
variables	O
,	O
compute	O
the	O
mean	B
expected	O
deviation	O
in	O
the	O
marginals	O
4	O
(	O
cid:88	O
)	O
2	O
(	O
cid:88	O
)	O
1	O
2	O
i=1	O
j=1	O
1	O
4	O
|qi	O
(	O
x	O
=	O
j	O
)	O
−	O
pi	O
(	O
x	O
=	O
j	O
)	O
|	O
for	O
both	O
the	O
bp	O
and	O
mf	O
approximations	O
,	O
and	O
comment	O
on	O
your	O
results	O
.	O
exercise	O
255.	O
in	O
loopybp.m	O
the	O
message	B
schedule	O
is	O
chosen	O
at	O
random	O
.	O
modify	O
the	O
routine	O
to	O
choose	O
a	O
schedule	B
using	O
a	O
forward-reverse	O
elimination	O
sequence	O
on	O
a	O
random	O
spanning	O
tree	B
.	O
exercise	O
256	O
(	O
double	O
integration	O
bounds	O
)	O
.	O
consider	O
a	O
bound	B
then	O
for	O
f	O
(	O
x	O
)	O
≥	O
g	O
(	O
x	O
)	O
(	O
cid:90	O
)	O
x	O
˜f	O
(	O
x	O
)	O
≡	O
show	O
that	O
:	O
a	O
f	O
(	O
x	O
)	O
dx	O
,	O
(	O
cid:90	O
)	O
x	O
a	O
˜g	O
(	O
x	O
)	O
≡	O
g	O
(	O
x	O
)	O
dx	O
1	O
.	O
2.	O
where	O
˜f	O
(	O
x	O
)	O
≥	O
˜g	O
(	O
x	O
)	O
,	O
for	O
x	O
≥	O
a	O
ˆf	O
(	O
x	O
)	O
≥	O
ˆg	O
(	O
x	O
)	O
(	O
cid:90	O
)	O
x	O
ˆf	O
(	O
x	O
)	O
≡	O
a	O
for	O
all	O
x	O
˜f	O
(	O
x	O
)	O
dx	O
,	O
ˆg	O
(	O
x	O
)	O
≡	O
(	O
cid:90	O
)	O
x	O
a	O
˜g	O
(	O
x	O
)	O
dx	O
(	O
28.11.3	O
)	O
(	O
28.11.4	O
)	O
(	O
28.11.5	O
)	O
(	O
28.11.6	O
)	O
(	O
28.11.7	O
)	O
the	O
signiﬁcance	O
is	O
that	O
this	O
double	O
integration	O
(	O
or	O
summation	O
in	O
the	O
case	O
of	O
discrete	B
variables	O
)	O
is	O
a	O
general	O
procedure	O
for	O
generating	O
a	O
new	O
bound	B
from	O
an	O
existing	O
bound	B
[	O
172	O
]	O
.	O
exercise	O
257.	O
starting	O
from	O
ex	O
≥	O
0	O
and	O
using	O
the	O
double	O
integration	O
procedure	O
,	O
show	O
that	O
(	O
28.11.8	O
)	O
ex	O
≥	O
ea	O
(	O
1	O
+	O
x	O
−	O
a	O
)	O
z	O
=	O
(	O
cid:88	O
)	O
1.	O
by	O
replacing	O
x	O
→	O
stws	O
for	O
s	O
∈	O
{	O
0	O
,	O
1	O
}	O
d	O
,	O
and	O
a	O
→	O
hts	O
derive	O
a	O
bound	B
on	O
the	O
partition	B
function	I
of	O
a	O
boltzmann	O
distribution	B
estws	O
(	O
28.11.9	O
)	O
s	O
2.	O
show	O
that	O
this	O
bound	B
is	O
equivalent	B
to	O
the	O
mean	B
field	O
bound	B
on	O
the	O
partition	B
function	I
.	O
3.	O
discuss	O
how	O
one	O
can	O
generate	O
tighter	O
bounds	O
on	O
the	O
partition	B
function	I
of	O
a	O
boltzmann	O
distribution	B
by	O
further	O
application	O
of	O
the	O
double	O
integration	O
procedure	O
.	O
exercise	O
258.	O
consider	O
a	O
pairwise	B
mrf	O
for	O
symmetric	O
w.	O
consider	O
the	O
decomposition	B
extwx+btx	O
1	O
z	O
p	O
(	O
x	O
)	O
=	O
w	O
=	O
(	O
cid:88	O
)	O
qiwi	O
,	O
where	O
0	O
≤	O
qi	O
≤	O
1	O
and	O
(	O
cid:80	O
)	O
i	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
i	O
(	O
28.11.10	O
)	O
(	O
28.11.11	O
)	O
bound	B
on	O
the	O
normalisation	B
z	O
and	O
discuss	O
a	O
naive	O
method	O
to	O
ﬁnd	O
the	O
tightest	O
upper	O
bound	B
.	O
i	O
qi	O
=	O
1	O
,	O
and	O
the	O
graph	B
of	O
each	O
matrix	B
wi	O
is	O
a	O
tree	B
.	O
explain	O
how	O
to	O
form	O
an	O
upper	O
draft	O
march	O
9	O
,	O
2010	O
539	O
exercise	O
259.	O
derive	O
linkser	O
’	O
s	O
bound	B
on	O
the	O
mutual	B
information	I
,	O
equation	B
(	O
28.5.15	O
)	O
.	O
exercises	O
exercise	O
260.	O
consider	O
the	O
average	B
of	O
a	O
positive	O
function	O
f	O
(	O
x	O
)	O
with	O
respect	O
to	O
a	O
distribution	B
p	O
(	O
x	O
)	O
(	O
cid:90	O
)	O
x	O
(	O
cid:90	O
)	O
j	O
=	O
log	O
p	O
(	O
x	O
)	O
f	O
(	O
x	O
)	O
j	O
≥	O
x	O
p	O
(	O
x	O
)	O
log	O
f	O
(	O
x	O
)	O
where	O
f	O
(	O
x	O
)	O
≥	O
0.	O
the	O
simplest	O
version	O
of	O
jensen	O
’	O
s	O
inequality	O
states	O
that	O
(	O
28.11.12	O
)	O
(	O
28.11.13	O
)	O
1.	O
by	O
considering	O
a	O
distribution	B
r	O
(	O
x	O
)	O
∝	O
p	O
(	O
x	O
)	O
f	O
(	O
x	O
)	O
,	O
and	O
kl	O
(	O
q|r	O
)	O
,	O
for	O
some	O
variational	O
distribution	O
q	O
(	O
x	O
)	O
,	O
show	O
that	O
j	O
≥	O
−kl	O
(	O
q	O
(	O
x	O
)	O
|p	O
(	O
x	O
)	O
)	O
+	O
(	O
cid:104	O
)	O
log	O
f	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
(	O
28.11.14	O
)	O
the	O
bound	B
saturates	O
when	O
q	O
(	O
x	O
)	O
∝	O
p	O
(	O
x	O
)	O
f	O
(	O
x	O
)	O
.	O
this	O
shows	O
that	O
if	O
we	O
wish	O
to	O
approximate	B
the	O
average	B
j	O
,	O
the	O
optimal	O
choice	O
for	O
the	O
approximating	O
distribution	B
depends	O
on	O
both	O
the	O
distribution	B
p	O
(	O
x	O
)	O
and	O
integrand	O
f	O
(	O
x	O
)	O
.	O
2.	O
furthermore	O
,	O
show	O
that	O
j	O
≥	O
−kl	O
(	O
q	O
(	O
x	O
)	O
|p	O
(	O
x	O
)	O
)	O
−	O
kl	O
(	O
q	O
(	O
x	O
)	O
|f	O
(	O
x	O
)	O
)	O
−	O
h	O
(	O
q	O
(	O
x	O
)	O
)	O
(	O
28.11.15	O
)	O
where	O
h	O
(	O
q	O
(	O
x	O
)	O
)	O
is	O
the	O
entropy	B
of	O
q	O
(	O
x	O
)	O
.	O
the	O
ﬁrst	O
term	O
encourages	O
q	O
to	O
be	O
close	O
to	O
p.	O
the	O
second	O
encourages	O
q	O
to	O
be	O
close	O
to	O
f	O
,	O
and	O
the	O
third	O
encourages	O
q	O
to	O
be	O
sharply	O
peaked	O
.	O
exercise	O
261.	O
for	O
a	O
markov	O
random	B
ﬁeld	I
over	O
d	O
binary	O
variables	O
xi	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
,	O
we	O
deﬁne	O
(	O
28.11.16	O
)	O
extwx	O
p	O
(	O
x	O
)	O
=	O
1	O
z	O
show	O
that	O
p	O
(	O
xi	O
)	O
=	O
z\i	O
z	O
where	O
z\i	O
≡	O
x1	O
,	O
...	O
,	O
xi−1	O
,	O
xi+1	O
,	O
...	O
,	O
xd	O
(	O
cid:88	O
)	O
extwx	O
(	O
28.11.17	O
)	O
(	O
28.11.18	O
)	O
and	O
explain	O
why	O
a	O
bound	B
on	O
the	O
marginal	B
p	O
(	O
xi	O
)	O
requires	O
both	O
upper	O
and	O
lower	O
bounds	O
on	O
partition	O
functions	O
.	O
exercise	O
262.	O
consider	O
a	O
directed	B
graph	O
such	O
that	O
the	O
capacity	B
of	O
an	O
edge	O
x	O
→	O
y	O
is	O
c	O
(	O
x	O
,	O
y	O
)	O
≥	O
0.	O
the	O
ﬂow	O
on	O
an	O
edge	O
f	O
(	O
x	O
,	O
y	O
)	O
≥	O
0	O
must	O
not	O
exceed	O
the	O
capacity	B
of	O
the	O
edge	O
.	O
the	O
aim	O
is	O
to	O
maximise	O
the	O
ﬂow	O
from	O
a	O
deﬁned	O
source	O
node	B
s	O
to	O
a	O
deﬁned	O
sink	O
node	B
t.	O
in	O
addition	O
ﬂow	O
must	O
be	O
conserved	O
such	O
that	O
for	O
any	O
node	B
other	O
than	O
the	O
source	O
or	O
sink	O
(	O
y	O
(	O
cid:54	O
)	O
=	O
s	O
,	O
t	O
)	O
,	O
f	O
(	O
y	O
,	O
x	O
)	O
(	O
28.11.19	O
)	O
(	O
cid:88	O
)	O
f	O
(	O
x	O
,	O
y	O
)	O
=	O
(	O
cid:88	O
)	O
x	O
x	O
a	O
cut	B
is	O
deﬁned	O
as	O
a	O
partition	O
of	O
the	O
nodes	O
into	O
two	O
non-overlapping	O
sets	O
s	O
and	O
t	O
such	O
that	O
s	O
is	O
in	O
s	O
and	O
t	O
in	O
t	O
.	O
show	O
that	O
:	O
1.	O
the	O
net	O
ﬂow	O
from	O
s	O
to	O
t	O
,	O
val	O
(	O
f	O
)	O
is	O
the	O
same	O
as	O
the	O
net	O
ﬂow	O
from	O
s	O
to	O
t	O
:	O
f	O
(	O
y	O
,	O
x	O
)	O
(	O
28.11.20	O
)	O
val	O
(	O
f	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:80	O
)	O
x∈s	O
,	O
y∈t	O
(	O
cid:88	O
)	O
f	O
(	O
x	O
,	O
y	O
)	O
−	O
y∈t	O
,	O
x∈s	O
2.	O
val	O
(	O
f	O
)	O
≤	O
x∈s	O
,	O
y∈t	O
f	O
(	O
x	O
,	O
y	O
)	O
namely	O
that	O
the	O
ﬂow	O
is	O
upper	O
bounded	O
by	O
the	O
capacity	B
of	O
the	O
cut	B
.	O
540	O
draft	O
march	O
9	O
,	O
2010	O
exercises	O
the	O
max-ﬂow-min-cut	O
theorem	B
further	O
states	O
that	O
the	O
maximal	O
ﬂow	O
is	O
actually	O
equal	O
to	O
the	O
capacity	B
of	O
the	O
cut	B
.	O
exercise	O
263	O
(	O
potts	O
to	O
ising	O
translation	O
)	O
.	O
consider	O
the	O
function	B
e	O
(	O
x	O
)	O
deﬁned	O
on	O
a	O
set	O
of	O
multistate	O
variables	O
dom	O
(	O
xi	O
)	O
=	O
{	O
0	O
,	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
,	O
e	O
(	O
x	O
)	O
=	O
(	O
cid:88	O
)	O
wiji	O
[	O
xi	O
=	O
xj	O
]	O
+	O
(	O
cid:88	O
)	O
cii	O
(	O
cid:2	O
)	O
xi	O
=	O
x0	O
i	O
(	O
cid:3	O
)	O
(	O
28.11.21	O
)	O
i∼j	O
i	O
where	O
wij	O
>	O
0	O
and	O
observed	O
pixel	O
states	O
x0	O
maximisation	B
of	O
e	O
(	O
x	O
)	O
.	O
using	O
the	O
restricted	B
parameteristion	O
i	O
are	O
known	O
,	O
as	O
are	O
ci	O
.	O
our	O
interest	O
is	O
to	O
ﬁnd	O
an	O
approximate	B
xi	O
=	O
siα	O
+	O
(	O
1	O
−	O
si	O
)	O
xold	O
i	O
(	O
28.11.22	O
)	O
where	O
si	O
∈	O
{	O
0	O
,	O
1	O
}	O
and	O
for	O
a	O
given	O
α	O
∈	O
{	O
0	O
,	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
,	O
show	O
how	O
to	O
write	O
e	O
(	O
x	O
)	O
as	O
a	O
function	B
of	O
the	O
binary	O
variables	O
(	O
cid:48	O
)	O
isi	O
+	O
const	O
.	O
c	O
(	O
28.11.23	O
)	O
e	O
(	O
s	O
)	O
=	O
(	O
cid:88	O
)	O
i∼j	O
i	O
[	O
si	O
=	O
sj	O
]	O
+	O
(	O
cid:88	O
)	O
(	O
cid:48	O
)	O
ij	O
w	O
i	O
for	O
w	O
(	O
cid:48	O
)	O
using	O
the	O
graph	B
cuts	O
procedure	O
.	O
ij	O
>	O
0.	O
this	O
new	O
problem	B
is	O
of	O
the	O
form	O
of	O
an	O
attractive	B
binary	I
mrf	O
which	O
can	O
be	O
solved	O
exactly	O
exercise	O
264.	O
consider	O
an	O
approximating	O
distribution	B
in	O
the	O
exponential	B
family	I
,	O
q	O
(	O
x	O
)	O
=	O
1	O
z	O
(	O
φ	O
)	O
eφtg	O
(	O
x	O
)	O
we	O
wish	O
to	O
use	O
q	O
(	O
x	O
)	O
to	O
approximate	B
a	O
distribution	B
p	O
(	O
x	O
)	O
using	O
the	O
kl	O
divergence	B
kl	O
(	O
p|q	O
)	O
1.	O
show	O
that	O
optimally	O
(	O
cid:104	O
)	O
g	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
)	O
=	O
(	O
cid:104	O
)	O
g	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
q	O
(	O
x	O
)	O
2.	O
show	O
that	O
a	O
gaussian	O
can	O
be	O
written	O
in	O
the	O
exponential	B
form	O
(	O
cid:0	O
)	O
x	O
µ	O
,	O
σ2	O
(	O
cid:1	O
)	O
=	O
n	O
1	O
z	O
(	O
φ	O
)	O
eφtg	O
(	O
x	O
)	O
(	O
28.11.24	O
)	O
(	O
28.11.25	O
)	O
(	O
28.11.26	O
)	O
(	O
28.11.27	O
)	O
where	O
g1	O
(	O
x	O
)	O
=	O
x	O
,	O
g2	O
(	O
x	O
)	O
=	O
x2	O
and	O
suitably	O
chosen	O
φ	O
.	O
3.	O
hence	O
show	O
that	O
the	O
optimal	O
gaussian	O
ﬁt	O
n	O
sense	O
matches	O
the	O
moments	O
:	O
σ2	O
=	O
(	O
cid:10	O
)	O
x2	O
(	O
cid:11	O
)	O
µ	O
=	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
)	O
,	O
p	O
(	O
x	O
)	O
−	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
2	O
p	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
x	O
µ	O
,	O
σ2	O
(	O
cid:1	O
)	O
to	O
any	O
distribution	B
,	O
in	O
the	O
minimal	O
kl	O
(	O
p|q	O
)	O
(	O
cid:0	O
)	O
x	O
m	O
,	O
s2	O
(	O
cid:1	O
)	O
to	O
a	O
distribution	B
p	O
(	O
x	O
)	O
.	O
(	O
28.11.28	O
)	O
exercise	O
265.	O
we	O
wish	O
to	O
ﬁnd	O
a	O
gaussian	O
approximation	B
q	O
(	O
x	O
)	O
=	O
n	O
show	O
that	O
kl	O
(	O
p|q	O
)	O
=	O
−	O
(	O
cid:104	O
)	O
log	O
q	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
)	O
+	O
const	O
.	O
(	O
28.11.29	O
)	O
write	O
the	O
kl	O
divergence	B
explicitly	O
as	O
a	O
function	B
of	O
m	O
and	O
s2	O
and	O
conﬁrm	O
the	O
general	O
result	O
that	O
the	O
optimal	O
m	O
and	O
s2	O
that	O
minimise	O
kl	O
(	O
p|q	O
)	O
are	O
given	O
by	O
setting	O
the	O
mean	B
and	O
variance	B
of	O
q	O
to	O
those	O
of	O
p.	O
draft	O
march	O
9	O
,	O
2010	O
541	O
exercise	O
266.	O
for	O
a	O
pairwise	B
binary	O
markov	O
random	O
field	O
,	O
p	O
with	O
partition	O
function	B
z	O
(	O
w	O
,	O
b	O
)	O
=	O
(	O
cid:88	O
)	O
(	O
cid:80	O
)	O
e	O
x	O
∂	O
∂bi	O
log	O
z	O
(	O
w	O
,	O
b	O
)	O
=	O
i	O
bixi	O
i	O
,	O
j	O
wij	O
xixj	O
+	O
(	O
cid:80	O
)	O
(	O
cid:88	O
)	O
1	O
z	O
(	O
w	O
,	O
b	O
)	O
x	O
show	O
that	O
the	O
means	O
can	O
be	O
computed	O
using	O
(	O
cid:80	O
)	O
i∼j	O
wij	O
xixj	O
+	O
(	O
cid:80	O
)	O
xie	O
i	O
bixi	O
=	O
(	O
cid:104	O
)	O
xi	O
(	O
cid:105	O
)	O
p	O
and	O
that	O
similarly	O
the	O
covariance	B
is	O
given	O
by	O
(	O
cid:104	O
)	O
xixj	O
(	O
cid:105	O
)	O
p	O
−	O
(	O
cid:104	O
)	O
xi	O
(	O
cid:105	O
)	O
p	O
(	O
cid:104	O
)	O
xj	O
(	O
cid:105	O
)	O
p	O
=	O
∂2	O
∂bi∂bj	O
log	O
z	O
(	O
w	O
,	O
b	O
)	O
exercises	O
(	O
28.11.30	O
)	O
(	O
28.11.31	O
)	O
(	O
28.11.32	O
)	O
exercise	O
267.	O
the	O
naive	B
mean	I
ﬁeld	I
theory	I
applied	O
to	O
a	O
pairwise	B
mrf	O
(	O
cid:80	O
)	O
p	O
(	O
x	O
)	O
∝	O
e	O
i	O
,	O
j	O
wij	O
xixj	O
+	O
(	O
cid:80	O
)	O
dom	O
(	O
xi	O
)	O
=	O
{	O
0	O
,	O
1	O
}	O
,	O
gives	O
a	O
factorised	B
approximation	O
q	O
(	O
x	O
)	O
=	O
(	O
cid:81	O
)	O
i	O
bixi	O
this	O
we	O
can	O
approximate	B
(	O
28.11.33	O
)	O
i	O
q	O
(	O
xi	O
)	O
,	O
based	O
on	O
minimising	O
kl	O
(	O
q|p	O
)	O
.	O
using	O
(	O
cid:104	O
)	O
xixj	O
(	O
cid:105	O
)	O
p	O
≈	O
(	O
cid:104	O
)	O
xi	O
(	O
cid:105	O
)	O
q	O
(	O
cid:104	O
)	O
xj	O
(	O
cid:105	O
)	O
q	O
,	O
(	O
28.11.34	O
)	O
to	O
produce	O
a	O
better	O
,	O
non-factorised	O
approximation	B
to	O
(	O
cid:104	O
)	O
xixj	O
(	O
cid:105	O
)	O
p	O
we	O
could	O
ﬁt	O
a	O
non-factorised	O
q.	O
the	O
linear-	O
response	O
method	O
[	O
153	O
]	O
may	O
also	O
be	O
used	O
,	O
based	O
on	O
a	O
perturbation	O
expansion	O
of	O
the	O
free	O
energy	B
.	O
alternatively	O
,	O
consider	O
the	O
relation	O
i	O
(	O
cid:54	O
)	O
=	O
j	O
p	O
(	O
xi	O
,	O
xj	O
)	O
=	O
p	O
(	O
xi|xj	O
)	O
p	O
(	O
xj	O
)	O
show	O
that	O
(	O
28.11.35	O
)	O
(	O
cid:104	O
)	O
xixj	O
(	O
cid:105	O
)	O
p	O
=	O
p	O
(	O
xi	O
=	O
1	O
,	O
xj	O
=	O
1	O
)	O
=	O
p	O
(	O
xi	O
=	O
1|xj	O
=	O
1	O
)	O
p	O
(	O
xj	O
=	O
1	O
)	O
(	O
28.11.36	O
)	O
explain	O
how	O
to	O
use	O
a	O
modiﬁed	O
naive	O
mean	O
ﬁeld	O
method	O
to	O
ﬁnd	O
a	O
non-factorised	O
approximation	B
to	O
(	O
cid:104	O
)	O
xixj	O
(	O
cid:105	O
)	O
p.	O
exercise	O
268.	O
derive	O
the	O
ep	O
updates	O
equation	B
(	O
28.7.8	O
)	O
and	O
equation	B
(	O
28.7.9	O
)	O
.	O
exercise	O
269.	O
you	O
are	O
given	O
a	O
set	O
of	O
datapoints	O
labelled	B
1	O
to	O
n	O
and	O
a	O
similarity	O
‘	O
metric	O
’	O
wij	O
≥	O
0	O
,	O
i	O
,	O
j	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
which	O
denotes	O
the	O
similarity	O
of	O
the	O
points	O
i	O
and	O
j.	O
you	O
want	O
to	O
assign	O
each	O
datapoint	O
to	O
a	O
cluster	O
index	O
cn	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
.	O
for	O
a	O
subset	O
of	O
the	O
datapoints	O
you	O
have	O
a	O
preference	O
for	O
the	O
cluster	O
index	O
.	O
explain	O
how	O
to	O
use	O
a	O
potts	O
model	B
to	O
formulate	O
an	O
objective	O
function	B
for	O
this	O
‘	O
semi-supervised	B
’	O
clustering	B
problem	O
.	O
542	O
draft	O
march	O
9	O
,	O
2010	O
appendix	O
a	O
background	O
mathematics	O
(	O
a.1.1	O
)	O
(	O
a.1.2	O
)	O
a.1	O
linear	B
algebra	I
a.1.1	O
vector	B
algebra	I
let	O
x	O
denote	O
the	O
n-dimensional	O
column	O
vector	O
with	O
components	O
	O
x1	O
x2	O
...	O
xn	O
	O
n	O
(	O
cid:88	O
)	O
i=1	O
deﬁnition	O
116	O
(	O
scalar	B
product	I
)	O
.	O
the	O
scalar	B
product	I
w	O
·	O
x	O
is	O
deﬁned	O
as	O
:	O
w	O
·	O
x	O
=	O
wixi	O
=	O
wtx	O
and	O
has	O
a	O
natural	B
geometric	O
interpretation	O
as	O
:	O
w	O
·	O
x	O
=	O
|w||x|	O
cos	O
(	O
θ	O
)	O
where	O
θ	O
is	O
the	O
angle	O
between	O
the	O
two	O
vectors	O
.	O
thus	O
if	O
the	O
lengths	O
of	O
two	O
vectors	O
are	O
ﬁxed	O
their	O
inner	O
product	O
is	O
largest	O
when	O
θ	O
=	O
0	O
,	O
whereupon	O
one	O
vector	O
is	O
a	O
constant	O
multiple	O
of	O
the	O
other	O
.	O
if	O
the	O
scalar	B
product	I
xty	O
=	O
0	O
,	O
then	O
x	O
and	O
y	O
are	O
orthogonal	B
(	O
they	O
are	O
a	O
right	O
angles	O
to	O
each	O
other	O
)	O
.	O
the	O
length	O
of	O
a	O
vector	O
is	O
denoted	O
|x|	O
,	O
the	O
squared	O
length	O
is	O
given	O
by	O
|x|2	O
=	O
xtx	O
=	O
x2	O
=	O
x2	O
a	O
unit	B
vector	I
x	O
has	O
xtx	O
=	O
1	O
.	O
1	O
+	O
x2	O
2	O
+	O
···	O
+	O
x2	O
n	O
(	O
a.1.3	O
)	O
deﬁnition	O
117	O
(	O
linear	B
dependence	O
)	O
.	O
a	O
set	O
of	O
vectors	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
is	O
linearly	O
dependent	O
if	O
there	O
exists	O
a	O
vector	O
xj	O
that	O
can	O
be	O
expressed	O
as	O
a	O
linear	B
combination	O
of	O
the	O
other	O
vectors	O
.	O
vice-versa	O
,	O
if	O
the	O
only	O
solution	O
to	O
αixi	O
=	O
0	O
(	O
a.1.4	O
)	O
n	O
(	O
cid:88	O
)	O
i=1	O
543	O
linear	B
algebra	I
figure	O
a.1	O
:	O
resolving	O
a	O
vector	O
a	O
into	O
components	O
along	O
the	O
orthogonal	B
directions	O
e	O
and	O
e∗	O
.	O
the	O
projection	B
of	O
a	O
onto	O
these	O
two	O
directions	O
are	O
lengths	O
α	O
and	O
β	O
along	O
the	O
directions	O
e	O
and	O
e∗	O
.	O
is	O
for	O
all	O
αi	O
=	O
0	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
the	O
vectors	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
are	O
linearly	B
independent	I
.	O
a.1.2	O
the	O
scalar	B
product	I
as	O
a	O
projection	B
suppose	O
that	O
we	O
wish	O
to	O
resolve	O
the	O
vector	O
a	O
into	O
its	O
components	O
along	O
the	O
orthogonal	B
directions	O
speciﬁed	O
by	O
the	O
unit	O
vectors	O
e	O
and	O
e∗	O
.	O
that	O
is	O
|e|	O
=	O
|e|	O
∗	O
=	O
1	O
and	O
e	O
·	O
e∗	O
=	O
0.	O
this	O
is	O
depicted	O
in	O
ﬁg	O
(	O
a.1	O
)	O
.	O
we	O
are	O
required	O
to	O
ﬁnd	O
the	O
scalar	O
values	O
α	O
and	O
β	O
such	O
that	O
a	O
=	O
αe	O
+	O
βe∗	O
from	O
this	O
we	O
obtain	O
a	O
·	O
e	O
=	O
αe	O
·	O
e	O
+	O
βe∗	O
·	O
e	O
,	O
a	O
·	O
e∗	O
=	O
αe	O
·	O
e∗	O
+	O
βe∗	O
·	O
e∗	O
from	O
the	O
orthogonality	O
and	O
unit	O
lengths	O
of	O
the	O
vectors	O
e	O
and	O
e∗	O
,	O
this	O
becomes	O
simply	O
a	O
·	O
e	O
=	O
α	O
,	O
a	O
·	O
e∗	O
=	O
β	O
(	O
a.1.5	O
)	O
(	O
a.1.6	O
)	O
(	O
a.1.7	O
)	O
a	O
set	O
of	O
vectors	O
is	O
orthonormal	B
if	O
they	O
are	O
mutually	O
orthogonal	B
and	O
have	O
unit	O
length	O
.	O
this	O
means	O
that	O
we	O
can	O
write	O
the	O
vector	O
a	O
in	O
terms	O
of	O
the	O
orthonormal	B
components	O
e	O
and	O
e∗	O
as	O
a	O
=	O
(	O
a	O
·	O
e	O
)	O
e	O
+	O
(	O
a	O
·	O
e∗	O
)	O
e∗	O
one	O
can	O
see	O
therefore	O
that	O
the	O
scalar	B
product	I
between	O
a	O
and	O
e	O
projects	O
the	O
vector	O
a	O
onto	O
the	O
(	O
unit	O
)	O
direction	O
e.	O
the	O
projection	B
of	O
a	O
vector	O
a	O
onto	O
a	O
direction	O
speciﬁed	O
by	O
f	O
is	O
therefore	O
a	O
·	O
f	O
|f|2	O
f	O
a.1.3	O
lines	O
in	O
space	O
(	O
a.1.9	O
)	O
a	O
line	O
in	O
2	O
(	O
or	O
more	O
)	O
dimensions	O
can	O
be	O
speciﬁed	O
as	O
follows	O
.	O
the	O
vector	O
of	O
any	O
point	O
along	O
the	O
line	O
is	O
given	O
,	O
for	O
some	O
s	O
,	O
by	O
the	O
equation	B
(	O
a.1.8	O
)	O
(	O
a.1.10	O
)	O
(	O
a.1.11	O
)	O
where	O
u	O
is	O
parallel	B
to	O
the	O
line	O
,	O
and	O
the	O
line	O
passes	O
through	O
the	O
point	O
a	O
,	O
see	O
ﬁg	O
(	O
a.2	O
)	O
.	O
this	O
is	O
called	O
the	O
parametric	O
representation	B
of	O
the	O
line	O
.	O
an	O
alternative	O
speciﬁcation	O
can	O
be	O
given	O
by	O
realising	O
that	O
all	O
vectors	O
along	O
the	O
line	O
are	O
orthogonal	B
to	O
the	O
normal	B
of	O
the	O
line	O
,	O
n	O
(	O
u	O
and	O
n	O
are	O
orthonormal	B
)	O
.	O
that	O
is	O
p	O
=	O
a	O
+	O
su	O
,	O
s	O
∈	O
r.	O
(	O
p	O
−	O
a	O
)	O
·	O
n	O
=	O
0	O
⇔	O
p	O
·	O
n	O
=	O
a	O
·	O
n	O
if	O
the	O
vector	O
n	O
is	O
of	O
unit	O
length	O
,	O
the	O
right	O
hand	O
side	O
of	O
the	O
above	O
represents	O
the	O
shortest	O
distance	O
from	O
the	O
origin	O
to	O
the	O
line	O
,	O
drawn	O
by	O
the	O
dashed	O
line	O
in	O
ﬁg	O
(	O
a.2	O
)	O
(	O
since	O
this	O
is	O
the	O
projection	B
of	O
a	O
onto	O
the	O
normal	B
direction	O
)	O
.	O
figure	O
a.2	O
:	O
a	O
line	O
can	O
be	O
speciﬁed	O
by	O
some	O
position	O
vector	O
on	O
the	O
line	O
,	O
a	O
,	O
and	O
a	O
unit	B
vector	I
along	O
the	O
direction	O
of	O
the	O
line	O
,	O
u.	O
in	O
2	O
dimensions	O
,	O
there	O
is	O
a	O
unique	O
direction	O
,	O
n	O
,	O
perpendicular	O
to	O
the	O
line	O
.	O
in	O
three	O
dimensions	O
,	O
the	O
vectors	O
perpendicular	O
to	O
the	O
direction	O
of	O
the	O
line	O
lie	O
in	O
a	O
plane	O
,	O
whose	O
normal	B
vector	O
is	O
in	O
the	O
direction	O
of	O
the	O
line	O
,	O
u	O
.	O
544	O
draft	O
march	O
9	O
,	O
2010	O
e*eβαaee*aapnu	O
linear	B
algebra	I
figure	O
a.3	O
:	O
a	O
plane	O
can	O
be	O
speciﬁed	O
by	O
a	O
point	O
in	O
the	O
plane	O
,	O
a	O
and	O
two	O
,	O
non-parallel	O
directions	O
in	O
the	O
plane	O
,	O
u	O
and	O
v.	O
the	O
normal	B
to	O
the	O
plane	O
is	O
unique	O
,	O
and	O
in	O
the	O
same	O
direction	O
as	O
the	O
directed	B
line	O
from	O
the	O
origin	O
to	O
the	O
nearest	O
point	O
on	O
the	O
plane	O
.	O
a.1.4	O
planes	O
and	O
hyperplanes	O
a	O
line	O
is	O
a	O
one	O
dimensional	O
hyperplane	B
.	O
to	O
deﬁne	O
a	O
two-dimensional	O
plane	O
(	O
in	O
arbitrary	O
dimensional	O
space	O
)	O
one	O
may	O
specify	O
two	O
vectors	O
u	O
and	O
v	O
that	O
lie	O
in	O
the	O
plane	O
(	O
they	O
need	O
not	O
be	O
mutually	O
orthogonal	B
)	O
,	O
and	O
a	O
position	O
vector	O
a	O
in	O
the	O
plane	O
,	O
see	O
ﬁg	O
(	O
a.3	O
)	O
.	O
any	O
vector	O
p	O
in	O
the	O
plane	O
can	O
then	O
be	O
written	O
as	O
p	O
=	O
a	O
+	O
su	O
+	O
tv	O
,	O
(	O
s	O
,	O
t	O
)	O
∈	O
r.	O
(	O
a.1.12	O
)	O
an	O
alternative	O
deﬁnition	O
is	O
given	O
by	O
considering	O
that	O
any	O
vector	O
within	O
the	O
plane	O
must	O
be	O
orthogonal	B
to	O
the	O
normal	B
of	O
the	O
plane	O
n.	O
(	O
p	O
−	O
a	O
)	O
·	O
n	O
=	O
0	O
⇔	O
p	O
·	O
n	O
=	O
a	O
·	O
n	O
(	O
a.1.13	O
)	O
the	O
right	O
hand	O
side	O
of	O
the	O
above	O
represents	O
the	O
shortest	O
distance	O
from	O
the	O
origin	O
to	O
the	O
plane	O
,	O
drawn	O
by	O
the	O
dashed	O
line	O
in	O
ﬁg	O
(	O
a.3	O
)	O
.	O
the	O
advantage	O
of	O
this	O
representation	B
is	O
that	O
it	O
has	O
the	O
same	O
form	O
as	O
a	O
line	O
.	O
indeed	O
,	O
this	O
representation	B
of	O
(	O
hyper	B
)	O
planes	O
is	O
independent	O
of	O
the	O
dimension	O
of	O
the	O
space	O
.	O
in	O
addition	O
,	O
only	O
two	O
vectors	O
need	O
to	O
be	O
deﬁned	O
–	O
a	O
point	O
in	O
the	O
plane	O
,	O
a	O
,	O
and	O
the	O
normal	B
to	O
the	O
plane	O
n.	O
a.1.5	O
matrices	O
an	O
m×	O
n	O
matrix	B
a	O
is	O
a	O
collection	O
of	O
scalar	O
m×	O
n	O
values	O
arranged	O
in	O
a	O
rectangle	O
of	O
m	O
rows	O
and	O
n	O
columns	O
.	O
a	O
vector	O
can	O
be	O
considered	O
a	O
n×	O
1	O
matrix	O
.	O
if	O
the	O
element	O
of	O
the	O
i-th	O
row	O
and	O
j-th	O
column	O
is	O
aij	O
,	O
then	O
at	O
denotes	O
the	O
matrix	B
that	O
has	O
aji	O
there	O
instead	O
-	O
the	O
transpose	O
of	O
a.	O
for	O
example	O
a	O
and	O
its	O
transpose	O
are	O
:	O
	O
	O
2	O
3	O
4	O
example	O
(	O
cid:2	O
)	O
a−1	O
(	O
cid:3	O
)	O
4	O
5	O
9	O
6	O
7	O
1	O
a	O
=	O
ij	O
)	O
.	O
	O
2	O
4	O
6	O
3	O
5	O
7	O
4	O
9	O
1	O
	O
at	O
=	O
the	O
i	O
,	O
j	O
element	O
of	O
matrix	B
a	O
can	O
be	O
written	O
aij	O
or	O
in	O
cases	O
where	O
more	O
clarity	O
is	O
required	O
,	O
[	O
a	O
]	O
ij	O
(	O
for	O
(	O
a.1.14	O
)	O
(	O
cid:104	O
)	O
bt	O
(	O
cid:105	O
)	O
deﬁnition	O
118	O
(	O
transpose	O
)	O
.	O
the	O
transpose	O
bt	O
of	O
the	O
n	O
by	O
m	O
matrix	B
b	O
is	O
the	O
m	O
by	O
n	O
matrix	B
d	O
with	O
components	O
=	O
bjk	O
;	O
b	O
,	O
(	O
cid:0	O
)	O
bt	O
(	O
cid:1	O
)	O
t	O
=	O
b	O
and	O
(	O
ab	O
)	O
t	O
=	O
btat	O
.	O
if	O
the	O
shapes	O
of	O
the	O
matrices	O
a	O
,	O
b	O
and	O
c	O
are	O
such	O
that	O
it	O
makes	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
j	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
.	O
(	O
a.1.15	O
)	O
kj	O
sense	O
to	O
calculate	O
the	O
product	O
abc	O
,	O
then	O
(	O
abc	O
)	O
t	O
=	O
ctbtat	O
(	O
a.1.16	O
)	O
a	O
square	O
matrix	B
a	O
is	O
symmetric	O
if	O
at	O
=	O
a.	O
a	O
square	O
matrix	B
is	O
called	O
hermitian	O
if	O
a	O
=	O
at∗	O
(	O
a.1.17	O
)	O
where	O
∗	O
denotes	O
the	O
complex	O
conjugate	B
operator	O
.	O
for	O
hermitian	O
matrices	O
,	O
the	O
eigenvectors	O
form	O
an	O
or-	O
thogonal	O
set	O
,	O
with	O
real	O
eigenvalues	O
.	O
draft	O
march	O
9	O
,	O
2010	O
545	O
anuvp	O
deﬁnition	O
119	O
(	O
matrix	B
addition	O
)	O
.	O
for	O
two	O
matrix	B
a	O
and	O
b	O
of	O
the	O
same	O
size	O
,	O
[	O
a	O
+	O
b	O
]	O
ij	O
=	O
[	O
a	O
]	O
ij	O
+	O
[	O
b	O
]	O
ij	O
linear	B
algebra	I
(	O
a.1.18	O
)	O
deﬁnition	O
120	O
(	O
matrix	B
multiplication	O
)	O
.	O
for	O
an	O
l	O
by	O
n	O
matrix	B
a	O
and	O
an	O
n	O
by	O
m	O
matrix	B
b	O
,	O
the	O
product	O
ab	O
is	O
the	O
l	O
by	O
m	O
matrix	B
with	O
elements	O
[	O
a	O
]	O
ij	O
[	O
b	O
]	O
jk	O
;	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
l	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
.	O
[	O
ab	O
]	O
ij	O
=	O
n	O
(	O
cid:88	O
)	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
x1	O
(	O
cid:18	O
)	O
a11	O
a12	O
j=1	O
a21	O
a22	O
x2	O
for	O
example	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
a11x1	O
+	O
a12x2	O
a21x2	O
+	O
a22x2	O
(	O
cid:19	O
)	O
=	O
(	O
a.1.19	O
)	O
(	O
a.1.20	O
)	O
note	O
that	O
even	O
if	O
ba	O
is	O
deﬁned	O
as	O
well	O
,	O
that	O
is	O
if	O
l	O
=	O
n	O
,	O
generally	O
ba	O
is	O
not	O
equal	O
to	O
ab	O
(	O
when	O
they	O
do	O
we	O
say	O
they	O
commute	B
)	O
.	O
the	O
matrix	B
i	O
is	O
the	O
identity	B
matrix	I
,	O
necessarily	O
square	O
,	O
with	O
1	O
’	O
s	O
on	O
the	O
diagonal	O
and	O
0	O
’	O
s	O
everywhere	O
else	O
.	O
for	O
clarity	O
we	O
may	O
also	O
write	O
im	O
for	O
an	O
square	O
m	O
×	O
m	O
identity	B
matrix	I
.	O
then	O
for	O
an	O
m	O
×	O
n	O
matrix	B
a	O
ima	O
=	O
ain	O
=	O
a	O
(	O
a.1.21	O
)	O
the	O
identity	B
matrix	I
has	O
elements	O
[	O
i	O
]	O
ij	O
=	O
δij	O
given	O
by	O
the	O
kronecker	O
delta	O
:	O
(	O
cid:26	O
)	O
1	O
i	O
=	O
j	O
0	O
i	O
(	O
cid:54	O
)	O
=	O
j	O
δij	O
≡	O
(	O
a.1.22	O
)	O
(	O
a.1.23	O
)	O
λi	O
deﬁnition	O
121	O
(	O
trace	O
)	O
.	O
trace	O
(	O
a	O
)	O
=	O
(	O
cid:88	O
)	O
aii	O
=	O
(	O
cid:88	O
)	O
i	O
i	O
where	O
λi	O
are	O
the	O
eigenvalues	O
of	O
a.	O
a.1.6	O
linear	B
transformations	O
rotations	O
if	O
we	O
assume	O
that	O
rotation	O
of	O
a	O
two-dimensional	O
vector	O
x	O
=	O
(	O
x	O
,	O
y	O
)	O
t	O
can	O
be	O
accomplished	O
by	O
matrix	B
multi-	O
plication	O
rx	O
then	O
,	O
since	O
matrix	B
multiplication	O
is	O
distributive	O
,	O
we	O
only	O
need	O
to	O
work	O
out	O
how	O
the	O
axes	O
unit	O
vectors	O
i	O
=	O
(	O
1	O
,	O
0	O
)	O
t	O
and	O
j	O
=	O
(	O
0	O
,	O
1	O
)	O
t	O
transform	O
since	O
rx	O
=	O
xri	O
+	O
yrj	O
the	O
unit	O
vectors	O
i	O
and	O
j	O
under	O
rotation	O
by	O
θ	O
degrees	O
transform	O
to	O
vectors	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
r11	O
r21	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
cos	O
θ	O
sin	O
θ	O
=	O
rj	O
=	O
(	O
cid:18	O
)	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
r12	O
r22	O
=	O
−	O
sin	O
θ	O
cos	O
θ	O
(	O
cid:19	O
)	O
ri	O
=	O
546	O
(	O
a.1.24	O
)	O
(	O
a.1.25	O
)	O
draft	O
march	O
9	O
,	O
2010	O
linear	B
algebra	I
from	O
this	O
,	O
one	O
can	O
simply	O
read	O
oﬀ	O
the	O
values	O
for	O
the	O
elements	O
(	O
cid:18	O
)	O
cos	O
θ	O
−	O
sin	O
θ	O
cos	O
θ	O
sin	O
θ	O
(	O
cid:19	O
)	O
r	O
=	O
a.1.7	O
determinants	O
(	O
a.1.26	O
)	O
deﬁnition	O
122	O
(	O
determinant	B
)	O
.	O
for	O
a	O
square	O
matrix	B
a	O
,	O
the	O
determinant	B
is	O
the	O
volume	O
of	O
the	O
transfor-	O
mation	O
of	O
the	O
matrix	B
a	O
(	O
up	O
to	O
a	O
sign	O
change	O
)	O
.	O
that	O
is	O
,	O
we	O
take	O
a	O
hypercube	O
of	O
unit	O
volume	O
and	O
map	B
each	O
vertex	B
under	O
the	O
transformation	O
,	O
and	O
the	O
volume	O
of	O
the	O
resulting	O
object	O
is	O
deﬁned	O
as	O
the	O
determinant	B
.	O
writing	O
[	O
a	O
]	O
ij	O
=	O
aij	O
,	O
det	O
det	O
a21	O
a22	O
=	O
a11a22	O
−	O
a21a12	O
(	O
cid:18	O
)	O
a11	O
a12	O
(	O
cid:19	O
)	O
a11	O
a12	O
a13	O
	O
=	O
a11	O
(	O
a22a33	O
−	O
a23a32	O
)	O
−	O
a12	O
(	O
a21a33	O
−	O
a31a23	O
)	O
+	O
a13	O
(	O
a21a32	O
−	O
a31a22	O
)	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
a12	O
a23	O
(	O
cid:18	O
)	O
a21	O
a22	O
(	O
cid:18	O
)	O
a21	O
a23	O
a21	O
a22	O
a23	O
a31	O
a32	O
a33	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
the	O
determinant	B
in	O
the	O
(	O
3	O
×	O
3	O
)	O
case	O
has	O
the	O
form	O
(	O
a.1.27	O
)	O
(	O
a.1.28	O
)	O
(	O
a.1.29	O
)	O
a11det	O
a32	O
a33	O
−	O
a12det	O
a31	O
a33	O
+	O
a13det	O
a31	O
a32	O
the	O
determinant	B
of	O
the	O
(	O
3	O
×	O
3	O
)	O
matrix	B
a	O
is	O
given	O
by	O
the	O
sum	O
of	O
terms	O
(	O
−1	O
)	O
i+1a1idet	O
(	O
ai	O
)	O
where	O
ai	O
is	O
the	O
(	O
2	O
×	O
2	O
)	O
matrix	B
formed	O
from	O
a	O
by	O
removing	O
the	O
ith	O
row	O
and	O
column	O
.	O
this	O
form	O
of	O
the	O
determinant	B
generalises	O
to	O
any	O
dimension	O
.	O
that	O
is	O
,	O
we	O
can	O
deﬁne	O
the	O
determinant	B
recursively	O
as	O
an	O
expansion	O
along	O
the	O
top	O
row	O
of	O
determinants	O
of	O
reduced	O
matrices	O
.	O
the	O
absolute	O
value	B
of	O
the	O
determinant	B
is	O
the	O
volume	O
of	O
the	O
transformation	O
.	O
(	O
cid:16	O
)	O
at	O
(	O
cid:17	O
)	O
det	O
=	O
det	O
(	O
a	O
)	O
(	O
a.1.30	O
)	O
(	O
a.1.31	O
)	O
for	O
square	O
matrices	O
a	O
and	O
b	O
of	O
equal	O
dimensions	O
,	O
det	O
(	O
ab	O
)	O
=	O
det	O
(	O
a	O
)	O
det	O
(	O
b	O
)	O
,	O
det	O
(	O
i	O
)	O
=	O
1	O
⇒	O
det	O
(	O
cid:0	O
)	O
a−1	O
(	O
cid:1	O
)	O
=	O
1/det	O
(	O
a	O
)	O
for	O
any	O
matrix	B
a	O
which	O
collapses	O
dimensions	O
,	O
then	O
the	O
volume	O
of	O
the	O
transformation	O
is	O
zero	O
,	O
and	O
so	O
is	O
the	O
determinant	B
.	O
if	O
the	O
determinant	B
is	O
zero	O
,	O
the	O
matrix	B
can	O
not	O
be	O
invertible	O
since	O
given	O
any	O
vector	O
x	O
,	O
given	O
a	O
‘	O
projection	B
’	O
y	O
=	O
ax	O
,	O
we	O
can	O
not	O
uniquely	O
compute	O
which	O
vector	O
x	O
was	O
projected	O
to	O
y–	O
there	O
will	O
in	O
general	O
be	O
an	O
inﬁnite	O
number	O
of	O
solutions	O
.	O
deﬁnition	O
123	O
(	O
orthogonal	B
matrix	O
)	O
.	O
a	O
square	O
matrix	B
a	O
is	O
orthogonal	B
if	O
aat	O
=	O
i	O
=	O
ata	O
.	O
from	O
the	O
properties	B
of	O
the	O
determinant	B
,	O
we	O
see	O
therefore	O
that	O
an	O
orthogonal	B
matrix	O
has	O
determinant	B
±1	O
and	O
hence	O
corresponds	O
to	O
a	O
volume	O
preserving	O
transformation	O
–	O
i.e	O
.	O
a	O
rotation	O
.	O
x	O
=	O
(	O
cid:2	O
)	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
(	O
cid:3	O
)	O
deﬁnition	O
124	O
(	O
matrix	B
rank	O
)	O
.	O
for	O
an	O
m	O
×	O
n	O
matrix	B
x	O
with	O
n	O
columns	O
,	O
each	O
written	O
as	O
an	O
m-vector	O
:	O
(	O
a.1.32	O
)	O
the	O
rank	B
of	O
x	O
is	O
the	O
maximum	O
number	O
of	O
linearly	B
independent	I
columns	O
(	O
or	O
equivalently	O
rows	O
)	O
.	O
a	O
n	O
×	O
n	O
square	O
matrix	B
is	O
full	O
rank	B
if	O
the	O
rank	B
is	O
n	O
and	O
the	O
matrix	B
is	O
non-singular	O
.	O
otherwise	O
the	O
matrix	B
is	O
reduced	O
rank	B
and	O
is	O
singular	B
.	O
draft	O
march	O
9	O
,	O
2010	O
547	O
a.1.8	O
matrix	B
inversion	O
linear	B
algebra	I
deﬁnition	O
125	O
(	O
matrix	B
inversion	O
)	O
.	O
for	O
a	O
square	O
matrix	B
a	O
,	O
its	O
inverse	O
satisﬁes	O
a−1a	O
=	O
i	O
=	O
aa−1	O
(	O
a.1.33	O
)	O
it	O
is	O
not	O
always	O
possible	O
to	O
ﬁnd	O
a	O
matrix	B
a−1	O
such	O
that	O
a−1a	O
=	O
i.	O
in	O
that	O
case	O
,	O
we	O
call	O
the	O
matrix	B
a	O
singular	B
.	O
geometrically	O
,	O
singular	B
matrices	O
correspond	O
to	O
‘	O
projections	O
’	O
:	O
if	O
we	O
were	O
to	O
take	O
the	O
transform	O
of	O
each	O
of	O
the	O
vertices	O
v	O
of	O
a	O
binary	O
hypercube	O
av	O
,	O
the	O
volume	O
of	O
the	O
transformed	O
hypercube	O
would	O
be	O
zero	O
.	O
if	O
you	O
are	O
given	O
a	O
vector	O
y	O
and	O
a	O
singular	B
transformation	O
,	O
a	O
,	O
one	O
can	O
not	O
uniquely	O
identify	O
a	O
vector	O
x	O
for	O
which	O
y	O
=	O
ax	O
-	O
typically	O
there	O
will	O
be	O
a	O
whole	O
space	O
of	O
possibilities	O
.	O
provided	O
the	O
inverses	O
matrices	O
exist	O
(	O
ab	O
)	O
−1	O
=	O
b−1a−1	O
for	O
a	O
non-square	O
matrix	B
a	O
such	O
that	O
aat	O
is	O
invertible	O
,	O
then	O
the	O
pseudo	B
inverse	I
,	O
deﬁned	O
as	O
a†	O
=	O
at	O
(	O
cid:16	O
)	O
aat	O
(	O
cid:17	O
)	O
−1	O
satisﬁes	O
aa†	O
=	O
i	O
.	O
(	O
a.1.34	O
)	O
(	O
a.1.35	O
)	O
a.1.9	O
computing	O
the	O
matrix	B
inverse	O
for	O
a	O
2	O
×	O
2	O
matrix	B
,	O
it	O
is	O
straightforward	O
to	O
work	O
out	O
for	O
a	O
general	O
matrix	B
,	O
the	O
explicit	O
form	O
of	O
the	O
inverse	O
.	O
if	O
the	O
matrix	B
whose	O
inverse	O
we	O
wish	O
to	O
ﬁnd	O
is	O
a	O
=	O
,	O
then	O
the	O
condition	O
for	O
the	O
inverse	O
is	O
(	O
cid:18	O
)	O
a	O
b	O
(	O
cid:19	O
)	O
c	O
d	O
=	O
c	O
d	O
g	O
h	O
0	O
1	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
1	O
0	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
e	O
f	O
(	O
cid:18	O
)	O
a	O
b	O
(	O
cid:18	O
)	O
1	O
0	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
ae	O
+	O
bg	O
af	O
+	O
bh	O
(	O
cid:18	O
)	O
e	O
f	O
(	O
cid:18	O
)	O
d	O
−c	O
(	O
cid:19	O
)	O
cf	O
+	O
dh	O
ce	O
+	O
dg	O
(	O
cid:19	O
)	O
0	O
1	O
=	O
ad	O
−	O
bc	O
−b	O
a	O
(	O
cid:19	O
)	O
1	O
=	O
g	O
h	O
=	O
a−1	O
(	O
a.1.36	O
)	O
(	O
a.1.37	O
)	O
(	O
a.1.38	O
)	O
multiplying	O
out	O
the	O
left	O
hand	O
side	O
,	O
we	O
obtain	O
the	O
four	O
conditions	O
it	O
is	O
readily	O
veriﬁed	O
that	O
the	O
solution	O
to	O
this	O
set	O
of	O
four	O
linear	B
equations	O
is	O
given	O
by	O
the	O
quantity	O
ad	O
−	O
bc	O
is	O
the	O
determinant	B
of	O
a.	O
there	O
are	O
many	O
ways	O
to	O
compute	O
the	O
inverse	O
of	O
a	O
general	O
matrix	B
,	O
and	O
we	O
refer	O
the	O
reader	O
to	O
more	O
specialised	O
texts	O
.	O
note	O
that	O
,	O
if	O
one	O
wants	O
to	O
solve	O
only	O
a	O
linear	B
system	O
,	O
although	O
the	O
solution	O
can	O
be	O
obtained	O
through	O
matrix	B
inversion	O
,	O
this	O
should	O
not	O
be	O
use	O
.	O
often	O
,	O
one	O
needs	O
to	O
solve	O
huge	O
dimensional	O
linear	B
systems	O
of	O
equations	O
,	O
and	O
speed	O
becomes	O
an	O
issue	O
.	O
these	O
equations	O
can	O
be	O
solved	O
much	O
more	O
accurately	O
and	O
quickly	O
using	O
elimination	O
techniques	O
such	O
as	O
gaussian	O
elimination	O
.	O
a.1.10	O
eigenvalues	O
and	O
eigenvectors	O
the	O
eigenvectors	O
of	O
a	O
matrix	B
correspond	O
to	O
the	O
natural	B
coordinate	O
system	B
,	O
in	O
which	O
the	O
geometric	O
trans-	O
formation	O
represented	O
by	O
a	O
can	O
be	O
most	O
easily	O
understood	O
.	O
548	O
draft	O
march	O
9	O
,	O
2010	O
linear	B
algebra	I
deﬁnition	O
126	O
(	O
eigenvalues	O
and	O
eigenvectors	O
)	O
.	O
for	O
a	O
square	O
matrix	B
a	O
,	O
e	O
is	O
an	O
eigenvector	O
of	O
a	O
with	O
eigenvalue	O
λ	O
if	O
ae	O
=	O
λe	O
det	O
(	O
a	O
)	O
=	O
n	O
(	O
cid:89	O
)	O
trace	O
(	O
a	O
)	O
=	O
(	O
cid:88	O
)	O
λi	O
i=1	O
λi	O
(	O
a.1.39	O
)	O
(	O
a.1.40	O
)	O
(	O
a.1.41	O
)	O
hence	O
a	O
matrix	B
is	O
singular	B
if	O
it	O
has	O
a	O
zero	O
eigenvalue	O
.	O
the	O
trace	O
of	O
a	O
matrix	B
can	O
be	O
expressed	O
as	O
i	O
for	O
an	O
(	O
n	O
×	O
n	O
)	O
dimensional	O
matrix	B
,	O
there	O
are	O
(	O
including	O
repetitions	O
)	O
n	O
eigenvalues	O
,	O
each	O
with	O
a	O
correspond-	O
ing	O
eigenvector	O
.	O
we	O
can	O
reform	O
equation	B
(	O
a.1.39	O
)	O
as	O
(	O
a	O
−	O
λi	O
)	O
e	O
=	O
0	O
(	O
a.1.42	O
)	O
this	O
is	O
a	O
linear	B
equation	O
,	O
for	O
which	O
the	O
eigenvector	O
e	O
and	O
eigenvalue	O
λ	O
is	O
a	O
solution	O
.	O
we	O
can	O
write	O
equation	B
(	O
a.1.42	O
)	O
as	O
be	O
=	O
0	O
,	O
where	O
b	O
≡	O
a	O
−	O
λi	O
.	O
if	O
b	O
has	O
an	O
inverse	O
,	O
then	O
a	O
solution	O
is	O
e	O
=	O
b−10	O
=	O
0	O
,	O
which	O
trivially	O
satisﬁes	O
the	O
eigen-equation	O
.	O
for	O
any	O
non-trivial	O
solution	O
to	O
the	O
problem	B
be	O
=	O
0	O
,	O
we	O
therefore	O
need	O
b	O
to	O
be	O
non-invertible	O
.	O
this	O
is	O
equivalent	B
to	O
the	O
condition	O
that	O
b	O
has	O
zero	O
determinant	B
.	O
hence	O
λ	O
is	O
an	O
eigenvalue	O
of	O
a	O
if	O
det	O
(	O
a	O
−	O
λi	O
)	O
=	O
0	O
(	O
a.1.43	O
)	O
this	O
is	O
known	O
as	O
the	O
characteristic	O
equation	B
.	O
this	O
determinant	B
equation	O
will	O
be	O
a	O
polynomial	O
of	O
degree	B
n	O
and	O
the	O
resulting	O
equation	B
is	O
known	O
as	O
the	O
characteristic	O
polynomial	O
.	O
once	O
we	O
have	O
found	O
an	O
eigenvalue	O
,	O
the	O
corresponding	O
eigenvector	O
can	O
be	O
found	O
by	O
substituting	O
this	O
value	B
for	O
λ	O
in	O
equation	B
(	O
a.1.39	O
)	O
and	O
solving	B
the	O
linear	B
equations	O
for	O
e.	O
it	O
may	O
be	O
that	O
the	O
for	O
an	O
eigenvalue	O
λ	O
the	O
eigenvector	O
is	O
not	O
unique	O
and	O
there	O
is	O
a	O
space	O
of	O
corresponding	O
vectors	O
.	O
geometrically	O
,	O
the	O
eigenvectors	O
are	O
special	O
directions	O
such	O
that	O
the	O
eﬀect	O
of	O
the	O
transformation	O
a	O
along	O
a	O
direction	O
e	O
is	O
simply	O
to	O
scale	O
the	O
vector	O
e.	O
for	O
a	O
rotation	O
matric	O
r	O
in	O
general	O
there	O
will	O
be	O
no	O
direction	O
preserved	O
under	O
the	O
rotation	O
so	O
that	O
the	O
eigenvalues	O
and	O
eigenvectors	O
are	O
complex	O
valued	O
(	O
which	O
is	O
why	O
the	O
fourier	O
representation	B
,	O
which	O
corresponds	O
to	O
representation	B
in	O
a	O
rotated	O
basis	O
,	O
is	O
necessarily	O
complex	O
)	O
.	O
remark	O
11	O
(	O
orthogonality	O
of	O
eigenvectors	O
of	O
symmetric	O
matrices	O
)	O
.	O
for	O
a	O
real	O
symmetric	O
matric	O
a	O
=	O
at	O
,	O
and	O
two	O
of	O
its	O
eigenvectors	O
ei	O
and	O
ej	O
of	O
a	O
are	O
orthogonal	B
(	O
ei	O
)	O
tej	O
=	O
0	O
if	O
the	O
eigenvalues	O
λi	O
and	O
λj	O
are	O
diﬀerent	O
.	O
the	O
above	O
can	O
be	O
shown	O
by	O
considering	O
:	O
aei	O
=	O
λiei	O
⇒	O
(	O
ej	O
)	O
taei	O
=	O
λi	O
(	O
ej	O
)	O
tei	O
since	O
a	O
is	O
symmetric	O
,	O
the	O
left	O
hand	O
side	O
is	O
equivalent	B
to	O
(	O
(	O
ej	O
)	O
ta	O
)	O
ei	O
=	O
(	O
aej	O
)	O
tei	O
=	O
λj	O
(	O
ej	O
)	O
tei	O
⇒	O
λi	O
(	O
ej	O
)	O
tei	O
=	O
λj	O
(	O
ej	O
)	O
tei	O
(	O
a.1.44	O
)	O
(	O
a.1.45	O
)	O
if	O
λi	O
(	O
cid:54	O
)	O
=	O
λj	O
,	O
this	O
condition	O
can	O
be	O
satisﬁed	O
only	O
if	O
(	O
ej	O
)	O
tei	O
=	O
0	O
,	O
namely	O
that	O
the	O
eigenvectors	O
are	O
orthogonal	B
.	O
549	O
draft	O
march	O
9	O
,	O
2010	O
a.1.11	O
matrix	B
decompositions	O
the	O
observation	O
that	O
the	O
eigenvectors	O
of	O
a	O
symmetric	O
matrix	B
are	O
orthogonal	B
leads	O
directly	O
to	O
the	O
spectral	O
decomposition	B
formula	O
below	O
.	O
linear	B
algebra	I
deﬁnition	O
127	O
(	O
spectral	O
decomposition	B
)	O
.	O
a	O
symmetric	O
matrix	B
a	O
has	O
an	O
eigen-decomposition	O
n	O
(	O
cid:88	O
)	O
a	O
=	O
λieiet	O
i	O
i=1	O
where	O
λi	O
is	O
the	O
eigenvalue	O
of	O
eigenvector	O
ei	O
and	O
the	O
eigenvectors	O
form	O
an	O
orthogonal	B
set	O
,	O
(	O
cid:0	O
)	O
ei	O
(	O
cid:1	O
)	O
t	O
ej	O
=	O
δij	O
(	O
cid:0	O
)	O
ei	O
(	O
cid:1	O
)	O
t	O
ei	O
in	O
matrix	B
notation	O
a	O
=	O
eλet	O
(	O
a.1.46	O
)	O
(	O
a.1.47	O
)	O
(	O
a.1.48	O
)	O
where	O
e	O
is	O
the	O
matrix	B
of	O
eigenvectors	O
and	O
λ	O
the	O
corresponding	O
diagonal	O
eigenvalue	O
matrix	B
.	O
more	O
generally	O
,	O
for	O
a	O
square	O
non-symmetric	O
non-singular	O
a	O
we	O
can	O
write	O
a	O
=	O
eλe−1	O
(	O
a.1.49	O
)	O
deﬁnition	O
128	O
(	O
singular	B
value	O
decomposition	B
)	O
.	O
the	O
svd	O
decomposition	B
of	O
a	O
n	O
×	O
p	O
matrix	B
x	O
is	O
x	O
=	O
usvt	O
(	O
a.1.50	O
)	O
where	O
dim	O
u	O
=	O
n×n	O
with	O
utu	O
=	O
in	O
.	O
also	O
dim	O
v	O
=	O
p×p	O
with	O
vtv	O
=	O
ip	O
.	O
the	O
matrix	B
s	O
has	O
dim	O
s	O
=	O
n×p	O
with	O
zeros	O
everywhere	O
except	O
on	O
the	O
diagonal	O
entries	O
.	O
the	O
‘	O
singular	B
values	O
’	O
are	O
the	O
diagonal	O
entries	O
[	O
s	O
]	O
ii	O
and	O
are	O
positive	O
.	O
the	O
singular	B
values	O
are	O
ordered	O
so	O
that	O
the	O
upper	O
left	O
diagonal	O
element	O
of	O
s	O
contains	O
the	O
largest	O
singular	B
value	O
.	O
quadratic	O
forms	O
deﬁnition	O
129	O
(	O
quadratic	B
form	I
)	O
.	O
xtax	O
+	O
xtb	O
(	O
a.1.51	O
)	O
deﬁnition	O
130	O
(	O
positive	B
deﬁnite	I
matrix	O
)	O
.	O
a	O
symmetric	O
matrix	B
a	O
,	O
with	O
the	O
property	O
that	O
xtax	O
≥	O
0	O
for	O
any	O
vector	O
x	O
is	O
called	O
nonnegative	O
deﬁnite	O
.	O
a	O
symmetric	O
matrix	B
a	O
,	O
with	O
the	O
property	O
that	O
xtax	O
>	O
0	O
for	O
any	O
vector	O
x	O
(	O
cid:54	O
)	O
=	O
0	O
is	O
called	O
positive	B
deﬁnite	I
.	O
a	O
positive	B
deﬁnite	I
matrix	O
has	O
full	O
rank	B
and	O
is	O
thus	O
invertible	O
.	O
using	O
the	O
eigen-decomposition	O
of	O
a	O
,	O
xtax	O
=	O
(	O
cid:88	O
)	O
λiytei	O
(	O
ei	O
)	O
tx	O
=	O
(	O
cid:88	O
)	O
λi	O
(	O
cid:16	O
)	O
xtei	O
(	O
cid:17	O
)	O
2	O
i	O
i	O
which	O
is	O
greater	O
than	O
zero	O
if	O
and	O
only	O
if	O
all	O
the	O
eigenvalues	O
are	O
positive	O
.	O
hence	O
a	O
is	O
positive	B
deﬁnite	I
if	O
and	O
only	O
if	O
all	O
its	O
eigenvalues	O
are	O
positive	O
.	O
550	O
draft	O
march	O
9	O
,	O
2010	O
(	O
a.1.52	O
)	O
multivariate	B
calculus	O
a.2	O
matrix	B
identities	O
deﬁnition	O
131	O
(	O
trace-log	B
formula	I
)	O
.	O
for	O
a	O
positive	B
deﬁnite	I
matrix	O
a	O
,	O
trace	O
(	O
log	O
a	O
)	O
≡	O
log	O
det	O
(	O
a	O
)	O
(	O
a.2.1	O
)	O
note	O
that	O
the	O
above	O
logarithm	O
of	O
a	O
matrix	B
is	O
not	O
the	O
element-wise	O
logarithm	O
.	O
in	O
matlab	O
the	O
required	O
function	B
is	O
logm	O
.	O
in	O
general	O
for	O
an	O
analytic	O
function	B
f	O
(	O
x	O
)	O
,	O
f	O
(	O
m	O
)	O
is	O
deﬁned	O
via	O
the	O
power-series	O
expansion	O
of	O
the	O
function	B
.	O
on	O
the	O
right	O
,	O
since	O
det	O
(	O
a	O
)	O
is	O
a	O
scalar	O
,	O
the	O
logarithm	O
is	O
the	O
standard	O
logarithm	O
of	O
a	O
scalar	O
.	O
deﬁnition	O
132	O
(	O
matrix	B
inversion	O
lemma	O
(	O
woodbury	O
formula	O
)	O
)	O
.	O
provided	O
the	O
appropriate	O
inverses	O
exist	O
:	O
(	O
cid:16	O
)	O
a	O
+	O
uvt	O
(	O
cid:17	O
)	O
−1	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
−1	O
=	O
a−1	O
−	O
a−1u	O
i	O
+	O
vta−1u	O
vta−1	O
(	O
a.2.2	O
)	O
(	O
a.2.3	O
)	O
eigenfunctions	O
(	O
cid:48	O
)	O
k	O
(	O
x	O
,	O
x	O
)	O
φa	O
(	O
x	O
)	O
=	O
λaφa	O
(	O
x	O
(	O
cid:48	O
)	O
)	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
x	O
x	O
by	O
an	O
analogous	O
argument	O
that	O
proves	O
the	O
theorem	B
of	O
linear	B
algebra	I
above	O
,	O
the	O
eigenfunctions	O
are	O
orthog-	O
onal	O
of	O
a	O
real	O
symmetric	O
kernel	B
,	O
k	O
(	O
x	O
(	O
cid:48	O
)	O
,	O
x	O
)	O
=	O
k	O
(	O
x	O
,	O
x	O
(	O
cid:48	O
)	O
)	O
are	O
orthogonal	B
:	O
φa	O
(	O
x	O
)	O
φ	O
∗	O
b	O
(	O
x	O
)	O
=	O
δab	O
(	O
a.2.4	O
)	O
where	O
φ∗	O
(	O
x	O
)	O
is	O
the	O
complex	O
conjugate	B
of	O
φ	O
(	O
x	O
)	O
1.	O
from	O
the	O
previous	O
results	O
,	O
we	O
know	O
that	O
a	O
symmetric	O
real	O
matrix	B
k	O
must	O
have	O
a	O
decomposition	B
in	O
terms	O
eigenvectors	O
with	O
positive	O
,	O
real	O
eigenvalues	O
.	O
since	O
this	O
is	O
to	O
be	O
true	O
for	O
any	O
dimension	O
of	O
matrix	B
,	O
it	O
suggests	O
that	O
we	O
need	O
the	O
(	O
real	O
symmetric	O
)	O
kernel	B
function	O
itself	O
to	O
have	O
a	O
decomposition	B
(	O
provided	O
the	O
eigenvalues	O
are	O
countable	O
)	O
λµφµ	O
(	O
xi	O
)	O
φ	O
∗	O
µ	O
(	O
xj	O
)	O
k	O
(	O
xi	O
,	O
xj	O
)	O
=	O
(	O
cid:88	O
)	O
yik	O
(	O
xi	O
,	O
xj	O
)	O
yj	O
=	O
(	O
cid:88	O
)	O
µ	O
i	O
,	O
j	O
i	O
,	O
j	O
,	O
µ	O
since	O
then	O
(	O
cid:88	O
)	O
λµyiφµ	O
(	O
xi	O
)	O
φ	O
µ	O
(	O
xj	O
)	O
yj	O
=	O
(	O
cid:88	O
)	O
∗	O
µ	O
λµ	O
(	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
i	O
(	O
(	O
cid:88	O
)	O
(	O
cid:124	O
)	O
i	O
(	O
cid:125	O
)	O
yiφµ	O
(	O
xi	O
)	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
zi	O
(	O
a.2.5	O
)	O
(	O
a.2.6	O
)	O
∗	O
µ	O
(	O
xi	O
)	O
)	O
(	O
cid:125	O
)	O
yiφ	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
z∗	O
i	O
which	O
is	O
greater	O
than	O
zero	O
if	O
the	O
eigenvalues	O
are	O
all	O
positive	O
(	O
since	O
for	O
complex	O
z	O
,	O
zz∗	O
if	O
the	O
eigenvalues	O
are	O
uncountable	O
(	O
which	O
happens	O
when	O
the	O
domain	B
of	O
the	O
kernel	B
is	O
unbounded	O
)	O
,	O
the	O
appropriate	O
decomposition	B
is	O
≥	O
0	O
)	O
.	O
k	O
(	O
xi	O
,	O
xj	O
)	O
=	O
λ	O
(	O
s	O
)	O
φ	O
(	O
xi	O
,	O
s	O
)	O
φ	O
∗	O
(	O
xj	O
,	O
s	O
)	O
ds	O
(	O
a.2.7	O
)	O
(	O
cid:90	O
)	O
a.3	O
multivariate	B
calculus	O
1this	O
deﬁnition	O
of	O
the	O
inner	O
product	O
is	O
useful	O
,	O
and	O
particularly	O
natural	B
in	O
the	O
context	O
of	O
translation	O
invariant	O
kernels	O
.	O
we	O
are	O
free	O
to	O
deﬁne	O
the	O
inner	O
product	O
,	O
but	O
this	O
conjugate	B
form	O
is	O
often	O
the	O
most	O
useful	O
.	O
draft	O
march	O
9	O
,	O
2010	O
551	O
multivariate	B
calculus	O
figure	O
a.4	O
:	O
interpreting	O
the	O
gradient	B
.	O
the	O
ellipses	O
are	O
contours	O
of	O
constant	O
function	B
value	O
,	O
f	O
=	O
const	O
.	O
at	O
any	O
point	O
x	O
,	O
the	O
gradi-	O
ent	O
vector	O
∇f	O
(	O
x	O
)	O
points	O
along	O
the	O
direction	O
of	O
maximal	O
increase	O
of	O
the	O
function	B
.	O
deﬁnition	O
133	O
(	O
partial	O
derivative	O
)	O
.	O
consider	O
a	O
function	B
of	O
n	O
variables	O
,	O
f	O
(	O
x1	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
or	O
f	O
(	O
x	O
)	O
.	O
the	O
partial	O
derivative	O
of	O
f	O
w.r.t	O
.	O
x1	O
at	O
x∗	O
is	O
deﬁned	O
as	O
the	O
following	O
limit	O
(	O
when	O
it	O
exists	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
x=x∗	O
∂f	O
∂x1	O
f	O
(	O
x∗	O
1	O
+	O
h	O
,	O
x∗	O
2	O
,	O
.	O
.	O
.	O
,	O
x∗	O
n	O
)	O
−	O
f	O
(	O
x∗	O
)	O
h	O
=	O
lim	O
h→0	O
the	O
gradient	B
vector	O
of	O
f	O
will	O
be	O
denoted	O
by	O
∇f	O
or	O
g	O
	O
	O
∂f	O
∂x1	O
...	O
∂f	O
∂xn	O
∇f	O
(	O
x	O
)	O
≡	O
g	O
(	O
x	O
)	O
≡	O
(	O
a.3.1	O
)	O
(	O
a.3.2	O
)	O
(	O
a.3.3	O
)	O
(	O
a.3.4	O
)	O
a.3.1	O
interpreting	O
the	O
gradient	B
vector	O
consider	O
a	O
function	B
f	O
(	O
x	O
)	O
that	O
depends	O
on	O
a	O
vector	O
x.	O
we	O
are	O
interested	O
in	O
how	O
the	O
function	B
changes	O
when	O
the	O
vector	O
x	O
changes	O
by	O
a	O
small	O
amount	O
:	O
x	O
→	O
x	O
+	O
δ	O
,	O
where	O
δ	O
is	O
a	O
vector	O
whose	O
length	O
is	O
very	O
small	O
.	O
according	O
to	O
a	O
taylor	O
expansion	O
,	O
the	O
function	B
φ	O
will	O
change	O
to	O
δi	O
∂f	O
∂xi	O
f	O
(	O
x	O
+	O
δ	O
)	O
=	O
f	O
(	O
x	O
)	O
+	O
(	O
cid:88	O
)	O
+	O
o	O
(	O
cid:0	O
)	O
δ2	O
(	O
cid:1	O
)	O
f	O
(	O
x	O
+	O
δ	O
)	O
=	O
f	O
(	O
x	O
)	O
+	O
(	O
∇f	O
)	O
tδ	O
+	O
o	O
(	O
cid:0	O
)	O
δ2	O
(	O
cid:1	O
)	O
∂xi	O
i	O
we	O
can	O
interpret	O
the	O
summation	O
above	O
as	O
the	O
scalar	B
product	I
between	O
the	O
vector	O
∇f	O
with	O
components	O
[	O
∇f	O
]	O
i	O
=	O
∂f	O
and	O
δ.	O
the	O
gradient	B
points	O
along	O
the	O
direction	O
in	O
which	O
the	O
function	B
increases	O
most	O
rapidly	O
.	O
why	O
?	O
consider	O
a	O
direction	O
ˆp	O
(	O
a	O
unit	O
length	O
vector	O
)	O
.	O
then	O
a	O
displacement	O
,	O
δ	O
units	O
along	O
this	O
direction	O
changes	O
the	O
function	B
value	O
to	O
f	O
(	O
x	O
+	O
δˆp	O
)	O
≈	O
f	O
(	O
x	O
)	O
+	O
δ∇f	O
(	O
x	O
)	O
·	O
ˆp	O
(	O
a.3.5	O
)	O
the	O
direction	O
ˆp	O
for	O
which	O
the	O
function	B
has	O
the	O
largest	O
change	O
is	O
that	O
which	O
maximises	O
the	O
overlap	O
∇f	O
(	O
x	O
)	O
·	O
ˆp	O
=	O
|∇f	O
(	O
x	O
)	O
||ˆp|	O
cos	O
θ	O
=	O
|∇f	O
(	O
x	O
)	O
|	O
cos	O
θ	O
(	O
a.3.6	O
)	O
where	O
θ	O
is	O
the	O
angle	O
between	O
ˆp	O
and	O
∇f	O
(	O
x	O
)	O
.	O
the	O
overlap	O
is	O
maximised	O
when	O
θ	O
=	O
0	O
,	O
giving	O
ˆp	O
=	O
∇f	O
(	O
x	O
)	O
/|∇f	O
(	O
x	O
)	O
|	O
.	O
hence	O
,	O
the	O
direction	O
along	O
which	O
the	O
function	B
changes	O
the	O
most	O
rapidly	O
is	O
along	O
∇f	O
(	O
x	O
)	O
.	O
a.3.2	O
higher	O
derivatives	O
the	O
‘	O
ﬁrst	O
derivative	O
’	O
of	O
a	O
function	B
of	O
n	O
variables	O
is	O
an	O
n-vector	O
;	O
the	O
‘	O
second-derivative	O
’	O
of	O
an	O
n-variable	O
function	B
is	O
deﬁned	O
by	O
the	O
n2	O
partial	O
derivatives	O
of	O
the	O
n	O
ﬁrst	O
partial	O
derivatives	O
w.r.t	O
.	O
the	O
n	O
variables	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
;	O
j	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
(	O
a.3.7	O
)	O
draft	O
march	O
9	O
,	O
2010	O
(	O
cid:18	O
)	O
∂f	O
(	O
cid:19	O
)	O
∂xj	O
∂	O
∂xi	O
552	O
x1x2f	O
(	O
x	O
)	O
multivariate	B
calculus	O
which	O
is	O
usually	O
written	O
∂2f	O
∂xi∂xj	O
,	O
i	O
(	O
cid:54	O
)	O
=	O
j	O
∂2f	O
2	O
,	O
∂xi	O
i	O
=	O
j	O
(	O
a.3.8	O
)	O
if	O
the	O
partial	O
derivatives	O
∂f	O
/∂xi	O
,	O
∂f	O
/∂xj	O
and	O
∂2f	O
/∂xi∂xj	O
are	O
continuous	B
,	O
then	O
∂2f	O
/∂xi∂xj	O
exists	O
and	O
∂2f	O
/∂xi∂xj	O
=	O
∂2f	O
/∂xj∂xi	O
.	O
(	O
a.3.9	O
)	O
these	O
n2	O
second	O
partial	O
derivatives	O
are	O
represented	O
by	O
a	O
square	O
,	O
symmetric	O
matrix	B
called	O
the	O
hessian	O
matrix	B
of	O
f	O
(	O
x	O
)	O
.	O
	O
∂2f	O
∂x1	O
...	O
2	O
∂2f	O
∂x1∂xn	O
hf	O
(	O
x	O
)	O
=	O
	O
.	O
.	O
.	O
.	O
.	O
.	O
∂2f	O
∂x1∂xn	O
...	O
∂2f	O
2	O
∂xn	O
(	O
a.3.10	O
)	O
a.3.3	O
chain	B
rule	I
consider	O
f	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
.	O
now	O
let	O
each	O
xj	O
be	O
parameterized	O
by	O
u1	O
,	O
.	O
.	O
.	O
,	O
um	O
,	O
i.e	O
.	O
xj	O
=	O
xj	O
(	O
u1	O
,	O
.	O
.	O
.	O
,	O
um	O
)	O
.	O
what	O
is	O
∂f	O
/∂uα	O
?	O
δf	O
=	O
f	O
(	O
x1	O
+	O
δx1	O
,	O
.	O
.	O
.	O
,	O
xn	O
+	O
δxn	O
)	O
−	O
f	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
=	O
δxj	O
=	O
δuα	O
+	O
higher	O
order	O
terms	O
∂f	O
∂xj	O
∂xj	O
∂uα	O
δuα	O
+	O
higher	O
order	O
terms	O
∂xj	O
∂uα	O
m	O
(	O
cid:88	O
)	O
n	O
(	O
cid:88	O
)	O
m	O
(	O
cid:88	O
)	O
α=1	O
j=1	O
α=1	O
so	O
δf	O
=	O
therefore	O
deﬁnition	O
134	O
(	O
chain	B
rule	I
)	O
.	O
n	O
(	O
cid:88	O
)	O
j=1	O
∂f	O
∂uα	O
=	O
∂f	O
∂xj	O
∂xj	O
∂uα	O
or	O
in	O
vector	O
notation	O
∂	O
∂uα	O
f	O
(	O
x	O
(	O
u	O
)	O
)	O
=	O
∇f	O
t	O
(	O
x	O
(	O
u	O
)	O
)	O
∂x	O
(	O
u	O
)	O
∂uα	O
n	O
(	O
cid:88	O
)	O
j=1	O
∂f	O
∂xj	O
δxj	O
+	O
higher	O
order	O
terms	O
(	O
a.3.11	O
)	O
(	O
a.3.12	O
)	O
(	O
a.3.13	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
h=0	O
=	O
(	O
cid:88	O
)	O
j	O
vj	O
∂f	O
∂xj	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
x=x∗	O
deﬁnition	O
135	O
(	O
directional	B
derivative	I
)	O
.	O
assume	O
f	O
is	O
diﬀerentiable	O
.	O
we	O
deﬁne	O
the	O
scalar	O
directional	O
derivative	O
(	O
dvf	O
)	O
(	O
x∗	O
)	O
of	O
f	O
in	O
a	O
direction	O
v	O
at	O
a	O
point	O
x∗	O
.	O
let	O
x	O
=	O
x∗	O
+	O
hv	O
,	O
then	O
(	O
dvf	O
)	O
(	O
x∗	O
)	O
=	O
d	O
dh	O
f	O
(	O
x∗	O
+	O
hv	O
)	O
a.3.4	O
matrix	B
calculus	O
draft	O
march	O
9	O
,	O
2010	O
=	O
∇f	O
tv	O
(	O
a.3.14	O
)	O
553	O
deﬁnition	O
136	O
(	O
derivative	O
of	O
a	O
matrix	B
trace	O
)	O
.	O
for	O
matrices	O
a	O
,	O
and	O
b	O
∂	O
∂a	O
trace	O
(	O
ab	O
)	O
≡	O
bt	O
deﬁnition	O
137	O
(	O
derivative	O
of	O
log	O
det	O
(	O
a	O
)	O
)	O
.	O
∂	O
log	O
det	O
(	O
a	O
)	O
=	O
∂trace	O
(	O
log	O
a	O
)	O
=	O
trace	O
(	O
cid:0	O
)	O
a−1∂a	O
(	O
cid:1	O
)	O
so	O
that	O
log	O
det	O
(	O
a	O
)	O
=	O
a−t	O
∂	O
∂a	O
deﬁnition	O
138	O
(	O
derivative	O
of	O
a	O
matrix	B
inverse	O
)	O
.	O
for	O
an	O
invertible	O
matrix	B
a	O
,	O
∂a−1	O
≡	O
−a−t∂aa−1	O
a.4	O
inequalities	O
a.4.1	O
convexity	O
inequalities	O
(	O
a.3.15	O
)	O
(	O
a.3.16	O
)	O
(	O
a.3.17	O
)	O
(	O
a.3.18	O
)	O
deﬁnition	O
139	O
(	O
convex	B
function	I
)	O
.	O
a	O
function	B
f	O
(	O
x	O
)	O
is	O
deﬁned	O
as	O
convex	O
if	O
for	O
any	O
x	O
,	O
y	O
and	O
0	O
≤	O
λ	O
≤	O
1	O
f	O
(	O
λx	O
+	O
(	O
1	O
−	O
λ	O
)	O
y	O
)	O
≤	O
λf	O
(	O
x	O
)	O
+	O
(	O
1	O
−	O
λ	O
)	O
f	O
(	O
y	O
)	O
if	O
−f	O
(	O
x	O
)	O
is	O
convex	O
,	O
f	O
(	O
x	O
)	O
is	O
called	O
concave	O
.	O
(	O
a.4.1	O
)	O
an	O
intuitive	O
picture	O
of	O
a	O
convex	B
function	I
is	O
to	O
consider	O
ﬁrst	O
the	O
quantity	O
λx	O
+	O
(	O
1	O
−	O
λ	O
)	O
y.	O
as	O
we	O
vary	O
λ	O
from	O
0	O
to	O
1	O
,	O
this	O
traces	O
points	O
between	O
x	O
(	O
λ	O
=	O
0	O
)	O
and	O
y	O
(	O
λ	O
=	O
1	O
)	O
.	O
hence	O
for	O
λ	O
=	O
0	O
we	O
start	O
at	O
the	O
point	O
x	O
,	O
f	O
(	O
x	O
)	O
and	O
as	O
λ	O
increase	O
trace	O
a	O
straight	O
line	O
towards	O
the	O
point	O
y	O
,	O
f	O
(	O
y	O
)	O
at	O
λ	O
=	O
1.	O
convexity	O
states	O
that	O
the	O
function	B
f	O
always	O
lies	O
below	O
this	O
straight	O
line	O
.	O
geometrically	O
this	O
means	O
that	O
the	O
function	B
f	O
(	O
x	O
)	O
is	O
always	O
always	O
increasing	O
(	O
never	O
non-decreasing	O
)	O
.	O
hence	O
if	O
d2f	O
(	O
x	O
)	O
/dx2	O
>	O
0	O
the	O
function	B
is	O
convex	O
.	O
as	O
an	O
example	O
,	O
the	O
function	B
log	O
x	O
is	O
concave	O
since	O
its	O
second	O
derivative	O
is	O
negative	O
:	O
d	O
dx	O
log	O
x	O
=	O
1	O
x	O
,	O
d2	O
dx2	O
log	O
x	O
=	O
−	O
1	O
x2	O
a.4.2	O
jensen	O
’	O
s	O
inequality	O
for	O
a	O
convex	B
function	I
f	O
(	O
x	O
)	O
,	O
it	O
follows	O
directly	O
from	O
the	O
deﬁnition	O
of	O
convexity	O
that	O
f	O
(	O
(	O
cid:104	O
)	O
x	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
)	O
)	O
≤	O
(	O
cid:104	O
)	O
f	O
(	O
x	O
)	O
(	O
cid:105	O
)	O
p	O
(	O
x	O
)	O
for	O
any	O
distribution	B
p	O
(	O
x	O
)	O
.	O
554	O
(	O
a.4.2	O
)	O
(	O
a.4.3	O
)	O
draft	O
march	O
9	O
,	O
2010	O
gradient	B
descent	I
a.5	O
optimisation	B
a.5.1	O
critical	O
points	O
when	O
all	O
ﬁrst-order	O
partial	O
derivatives	O
at	O
a	O
point	O
are	O
zero	O
(	O
i.e	O
.	O
∇f	O
=	O
0	O
)	O
then	O
the	O
point	O
is	O
said	O
to	O
be	O
a	O
stationary	B
or	O
critical	B
point	I
.	O
can	O
be	O
a	O
minimum	O
,	O
maximum	O
or	O
saddle	O
point	O
.	O
necessary	O
ﬁrst-order	O
condition	O
for	O
a	O
minimum	O
there	O
is	O
a	O
minimum	O
of	O
f	O
at	O
x∗	O
if	O
f	O
(	O
x∗	O
)	O
≤	O
f	O
(	O
x	O
)	O
for	O
all	O
x	O
suﬃciently	O
close	O
to	O
x∗	O
.	O
let	O
x	O
=	O
x∗	O
+	O
hv	O
for	O
small	O
h	O
and	O
some	O
direction	O
v.	O
then	O
by	O
a	O
taylor	O
expansion	O
,	O
for	O
small	O
h	O
,	O
f	O
(	O
x∗	O
+	O
hv	O
)	O
=	O
f	O
(	O
x∗	O
)	O
+	O
h∇f	O
tv	O
+	O
o	O
(	O
h2	O
)	O
and	O
thus	O
for	O
a	O
minimum	O
h∇f	O
tv	O
+	O
o	O
(	O
h2	O
)	O
≥	O
0	O
choosing	O
v	O
to	O
be	O
−∇f	O
the	O
condition	O
becomes	O
−h∇f	O
t∇f	O
+	O
o	O
(	O
h2	O
)	O
≥	O
0	O
(	O
a.5.1	O
)	O
(	O
a.5.2	O
)	O
(	O
a.5.3	O
)	O
and	O
is	O
violated	O
for	O
small	O
positive	O
h	O
unless	O
|∇f|2	O
=	O
∇f	O
t∇f	O
=	O
0.	O
so	O
x∗	O
can	O
only	O
be	O
a	O
local	B
minimum	O
if	O
|∇f	O
(	O
x∗	O
)	O
|	O
=	O
0	O
,	O
i.e	O
.	O
if	O
∇f	O
(	O
x∗	O
)	O
=	O
0.	O
necessary	O
second-order	O
condition	O
for	O
a	O
minimum	O
at	O
a	O
stationary	B
point	O
∇f	O
=	O
0.	O
hence	O
the	O
taylor	O
expansion	O
is	O
given	O
by	O
f	O
(	O
x∗	O
+	O
hv	O
)	O
=	O
f	O
(	O
x∗	O
)	O
+	O
h2vthf	O
v	O
+	O
o	O
(	O
h3	O
)	O
(	O
a.5.4	O
)	O
thus	O
the	O
minimum	O
condition	O
requires	O
that	O
vthf	O
v	O
≥	O
0	O
,	O
i.e	O
.	O
the	O
hessian	O
is	O
non-negative	O
deﬁnite	O
.	O
deﬁnition	O
140	O
(	O
conditions	O
for	O
a	O
minimum	O
)	O
.	O
suﬃcient	O
conditions	O
for	O
a	O
minimum	O
at	O
x∗	O
are	O
(	O
i	O
)	O
∇f	O
(	O
x∗	O
)	O
=	O
0	O
and	O
(	O
ii	O
)	O
hf	O
(	O
x∗	O
)	O
is	O
positive	B
deﬁnite	I
.	O
for	O
a	O
quadratic	O
function	O
f	O
(	O
x	O
)	O
=	O
1	O
reads	O
:	O
2xtax−btx+	O
c	O
,	O
with	O
symmetric	O
a	O
the	O
necessary	O
condition	O
∇f	O
(	O
x∗	O
)	O
=	O
0	O
ax∗	O
−	O
b	O
=	O
0	O
if	O
a	O
is	O
invertible	O
this	O
equation	B
has	O
the	O
unique	O
solution	O
x∗	O
=	O
a−1b	O
.	O
minimum	O
.	O
(	O
a.5.5	O
)	O
if	O
a	O
is	O
positive	B
deﬁnite	I
,	O
x∗	O
is	O
a	O
a.6	O
gradient	B
descent	I
almost	O
all	O
of	O
the	O
search	O
techniques	O
that	O
we	O
consider	O
are	O
iterative	O
,	O
i.e	O
.	O
we	O
proceed	O
towards	O
the	O
minimum	O
x∗	O
by	O
a	O
sequence	O
of	O
steps	O
.	O
on	O
the	O
kth	O
step	O
we	O
take	O
a	O
step	O
of	O
length	O
αk	O
in	O
the	O
direction	O
pk	O
,	O
xk+1	O
=	O
xk	O
+	O
αkpk	O
(	O
a.6.1	O
)	O
the	O
length	O
of	O
the	O
step	O
can	O
either	O
be	O
chosen	O
using	O
prior	B
knowledge	O
,	O
or	O
by	O
carrying	O
out	O
a	O
line	B
search	I
in	O
the	O
direction	O
pk	O
.	O
it	O
is	O
the	O
way	O
that	O
pk	O
is	O
chosen	O
that	O
tends	O
to	O
distinguish	O
the	O
diﬀerent	O
methods	O
of	O
multivariate	B
optimization	O
that	O
we	O
will	O
discuss	O
.	O
draft	O
march	O
9	O
,	O
2010	O
555	O
gradient	B
descent	I
we	O
shall	O
assume	O
that	O
we	O
can	O
analytically	O
evaluate	O
the	O
gradient	B
of	O
f	O
and	O
will	O
often	O
use	O
the	O
shorthand	O
notation	O
gk	O
=	O
∇f	O
(	O
xk	O
)	O
.	O
(	O
a.6.2	O
)	O
typically	O
we	O
will	O
want	O
to	O
choose	O
pk	O
using	O
only	O
gradient	B
information	O
;	O
for	O
large	O
problems	O
it	O
can	O
be	O
very	O
expensive	O
to	O
compute	O
the	O
hessian	O
,	O
and	O
this	O
can	O
also	O
require	O
a	O
large	O
amount	O
of	O
storage	O
.	O
consider	O
the	O
change	B
of	I
variables	I
x	O
=	O
my	O
.	O
then	O
g	O
(	O
y	O
)	O
=	O
f	O
(	O
x	O
)	O
=	O
f	O
(	O
my	O
)	O
and	O
=	O
(	O
cid:88	O
)	O
j	O
∂g	O
∂yi	O
∂f	O
∂xi	O
∂xi	O
∂yj	O
so	O
that	O
∇yg	O
=	O
m∇xf	O
(	O
a.6.3	O
)	O
(	O
a.6.4	O
)	O
(	O
a.6.5	O
)	O
then	O
the	O
change	O
in	O
g	O
is	O
diﬀerent	O
from	O
the	O
change	O
in	O
f	O
,	O
even	O
though	O
the	O
only	O
diﬀerence	O
between	O
the	O
two	O
functions	O
is	O
the	O
coordinate	O
system	B
.	O
this	O
unfortunate	O
sensitivity	O
to	O
the	O
parameterisation	B
of	O
the	O
function	B
is	O
partially	O
addressed	O
in	O
ﬁrst	B
order	I
methods	O
such	O
as	O
gradient	O
descent	B
by	O
the	O
natural	B
gradient	I
which	O
uses	O
a	O
prefactor	O
designed	O
to	O
compensate	O
for	O
some	O
of	O
the	O
lost	O
invariance	O
.	O
we	O
refer	O
the	O
reader	O
to	O
[	O
7	O
]	O
for	O
a	O
description	O
of	O
this	O
method	O
.	O
a.6.1	O
gradient	B
descent	I
with	O
ﬁxed	O
stepsize	O
locally	O
,	O
if	O
we	O
are	O
at	O
point	O
xk	O
,	O
we	O
can	O
decrease	O
f	O
(	O
x	O
)	O
by	O
taking	O
a	O
step	O
in	O
the	O
direction	O
−g	O
(	O
x	O
)	O
.	O
if	O
we	O
make	O
the	O
update	O
equation	B
xk+1	O
=	O
xk	O
−	O
ηgk	O
(	O
a.6.6	O
)	O
then	O
we	O
are	O
doing	O
gradient	B
descent	I
with	O
ﬁxed	O
stepsize	O
η.	O
if	O
η	O
is	O
non-inﬁnitesimal	O
,	O
it	O
is	O
always	O
possible	O
that	O
we	O
will	O
step	O
over	O
the	O
true	O
minimum	O
.	O
making	O
η	O
very	O
small	O
guards	O
against	O
this	O
,	O
but	O
means	O
that	O
the	O
optimization	O
process	O
will	O
take	O
a	O
very	O
long	O
time	O
to	O
reach	O
a	O
minimum	O
.	O
to	O
see	O
why	O
gradient	B
descent	I
works	O
,	O
consider	O
the	O
general	O
update	O
xk+1	O
=	O
xk	O
+	O
αpk	O
for	O
small	O
α	O
we	O
can	O
expand	O
f	O
around	O
xk	O
using	O
taylor	O
’	O
s	O
theorem	B
:	O
f	O
(	O
xk	O
+	O
αkpk	O
)	O
≈	O
f	O
(	O
xk	O
)	O
+	O
αkgt	O
k	O
pk	O
.	O
with	O
pk	O
=	O
−gk	O
and	O
for	O
small	O
positive	O
αk	O
,	O
we	O
see	O
a	O
guaranteed	O
reduction	O
:	O
f	O
(	O
xk	O
+	O
αkpk	O
)	O
≈	O
f	O
(	O
xk	O
)	O
−	O
αk||gk||2	O
.	O
a.6.2	O
gradient	B
descent	I
with	O
momentum	B
(	O
a.6.7	O
)	O
(	O
a.6.8	O
)	O
(	O
a.6.9	O
)	O
a	O
simple	O
idea	O
that	O
can	O
improve	O
convergence	O
of	O
gradient	B
descent	I
is	O
to	O
include	O
at	O
each	O
iteration	B
a	O
proportion	O
of	O
the	O
change	O
from	O
the	O
previous	O
iteration	B
.	O
one	O
uses	O
∆xk+1	O
=	O
−η	O
∂e	O
∂x	O
+	O
α∆xk	O
where	O
α	O
is	O
the	O
momentum	B
coeﬃcient	O
.	O
556	O
(	O
a.6.10	O
)	O
draft	O
march	O
9	O
,	O
2010	O
multivariate	B
minimization	O
:	O
quadratic	O
functions	O
figure	O
a.5	O
:	O
optimisation	B
using	O
line	B
search	I
along	O
steepest	O
descent	B
directions	O
.	O
rushing	O
oﬀ	O
following	O
the	O
steepest	O
way	O
downhill	O
from	O
a	O
point	O
(	O
and	O
continuing	O
for	O
a	O
ﬁnite	O
time	O
in	O
that	O
direction	O
)	O
doesn	O
’	O
t	O
always	O
re-	O
sult	O
in	O
the	O
fastest	O
way	O
to	O
get	O
to	O
the	O
bottom	O
!	O
a.6.3	O
gradient	B
descent	I
with	O
line	O
searches	O
an	O
extension	O
to	O
the	O
idea	O
of	O
gradient	B
descent	I
is	O
to	O
choose	O
the	O
direction	O
of	O
steepest	O
descent	B
,	O
as	O
indicated	O
by	O
the	O
gradient	B
g	O
,	O
but	O
to	O
calculate	O
the	O
value	B
of	O
the	O
step	O
to	O
take	O
which	O
most	O
reduces	O
the	O
value	B
of	O
e	O
when	O
moving	O
in	O
that	O
direction	O
.	O
this	O
involves	O
solving	B
the	O
one-dimensional	O
problem	B
of	O
minimizing	O
e	O
(	O
xk	O
−	O
λgk	O
)	O
with	O
respect	O
to	O
λ	O
,	O
and	O
is	O
known	O
as	O
a	O
line	B
search	I
.	O
that	O
step	O
is	O
then	O
taken	O
and	O
the	O
process	O
repeated	O
again	O
.	O
finding	O
the	O
size	O
of	O
the	O
step	O
takes	O
a	O
little	O
work	O
;	O
for	O
example	O
,	O
you	O
might	O
ﬁnd	O
three	O
points	O
along	O
the	O
line	O
such	O
that	O
the	O
error	O
at	O
the	O
intermediate	O
point	O
is	O
less	O
than	O
at	O
the	O
other	O
two	O
,	O
so	O
that	O
there	O
is	O
some	O
minimum	O
along	O
the	O
line	O
lies	O
between	O
the	O
ﬁrst	O
and	O
second	O
or	O
between	O
the	O
second	O
and	O
third	O
,	O
and	O
some	O
kind	O
of	O
interval-	O
halving	O
approach	B
can	O
then	O
be	O
used	O
to	O
ﬁnd	O
it	O
.	O
(	O
the	O
minimum	O
found	O
in	O
this	O
way	O
,	O
just	O
as	O
with	O
any	O
sort	O
of	O
gradient-descent	O
algorithm	B
,	O
may	O
not	O
be	O
a	O
global	B
minimum	O
of	O
course	O
.	O
)	O
there	O
are	O
several	O
variants	O
of	O
this	O
theme	O
.	O
notice	O
that	O
if	O
the	O
step	O
size	O
is	O
chosen	O
to	O
reduce	O
e	O
as	O
much	O
as	O
it	O
can	O
in	O
that	O
direction	O
,	O
then	O
no	O
further	O
improvement	O
in	O
e	O
can	O
be	O
made	O
by	O
moving	O
in	O
that	O
direction	O
for	O
the	O
moment	O
.	O
thus	O
the	O
next	O
step	O
will	O
have	O
no	O
component	O
in	O
that	O
direction	O
;	O
that	O
is	O
,	O
the	O
next	O
step	O
will	O
be	O
at	O
right	O
angles	O
to	O
the	O
one	O
just	O
taken	O
.	O
this	O
can	O
lead	O
to	O
zig-zag	O
type	O
behaviour	O
in	O
the	O
optimisation	B
,	O
see	O
ﬁg	O
(	O
a.5	O
)	O
.	O
a.6.4	O
exact	O
line	O
search	O
condition	O
at	O
the	O
k-th	O
step	O
,	O
we	O
chose	O
αk	O
to	O
minimize	O
f	O
(	O
xk	O
+	O
αkpk	O
)	O
.	O
so	O
setting	O
f	O
(	O
λ	O
)	O
=	O
f	O
(	O
xk	O
+	O
λpk	O
)	O
,	O
at	O
this	O
step	O
we	O
solve	O
the	O
one-dimensional	O
minimization	O
problem	B
for	O
f	O
(	O
λ	O
)	O
.	O
thus	O
our	O
choice	O
of	O
αk	O
=	O
λ∗	O
will	O
satisfy	O
f	O
(	O
cid:48	O
)	O
(	O
αk	O
)	O
=	O
0.	O
now	O
f	O
(	O
αk	O
+	O
h	O
)	O
|h=0	O
=	O
d	O
dh	O
f	O
(	O
xk	O
+	O
αkpk	O
+	O
hpk	O
)	O
|h=0	O
f	O
(	O
cid:48	O
)	O
(	O
αk	O
)	O
=	O
d	O
dh	O
=	O
d	O
dh	O
(	O
a.6.11	O
)	O
so	O
f	O
(	O
cid:48	O
)	O
(	O
αk	O
)	O
=	O
0	O
means	O
the	O
directional	B
derivative	I
in	O
the	O
search	O
direction	O
must	O
vanish	O
at	O
the	O
new	O
point	O
and	O
this	O
gives	O
the	O
exact	O
line	O
search	O
condition	O
:	O
f	O
(	O
xk+1	O
+	O
hpk	O
)	O
|h=0	O
=	O
(	O
dpk	O
f	O
)	O
(	O
xk+1	O
)	O
=	O
∇f	O
t	O
(	O
xk+1	O
)	O
pk	O
0	O
=	O
gt	O
k+1pk	O
.	O
(	O
a.6.12	O
)	O
for	O
a	O
quadratic	O
function	O
f	O
(	O
x	O
)	O
=	O
1	O
calculate	O
αk	O
.	O
since	O
∇f	O
(	O
xk+1	O
)	O
=	O
axk	O
+	O
αkapk	O
−	O
b	O
=	O
∇f	O
(	O
xk	O
)	O
+	O
αkapk	O
we	O
ﬁnd	O
2xtax−btx+c	O
,	O
with	O
symmetric	O
,	O
we	O
can	O
use	O
the	O
condition	O
to	O
analytically	O
αk	O
=	O
−	O
pt	O
k	O
gk	O
pt	O
k	O
apk	O
.	O
(	O
a.6.13	O
)	O
a.7	O
multivariate	B
minimization	O
:	O
quadratic	O
functions	O
the	O
goal	O
of	O
this	O
section	O
is	O
to	O
derive	O
eﬃcient	B
algorithms	O
for	O
minimizing	O
multivariate	B
quadratic	O
functions	O
.	O
we	O
shall	O
begin	O
by	O
summarizing	O
some	O
properties	B
of	O
quadratic	O
functions	O
,	O
and	O
as	O
byproduct	O
obtain	O
an	O
eﬃcient	B
method	O
for	O
checking	O
whether	O
a	O
symmetric	O
matrix	B
is	O
positive	B
deﬁnite	I
.	O
a.7.1	O
minimising	O
quadratic	O
functions	O
using	O
line	B
search	I
consider	O
minimising	O
the	O
quadratic	O
function	O
f	O
(	O
x	O
)	O
=	O
1	O
2	O
xtax	O
−	O
btx	O
+	O
c	O
draft	O
march	O
9	O
,	O
2010	O
(	O
a.7.1	O
)	O
557	O
multivariate	B
minimization	O
:	O
quadratic	O
functions	O
(	O
cid:0	O
)	O
b	O
−	O
ax0	O
(	O
cid:1	O
)	O
ptap	O
λ	O
=	O
where	O
a	O
is	O
positive	B
deﬁnite	I
and	O
symmetric	O
.	O
(	O
if	O
a	O
is	O
not	O
symmetric	O
,	O
consider	O
instead	O
the	O
symmetrised	O
matrix	B
(	O
a	O
+	O
at	O
)	O
/2	O
,	O
which	O
gives	O
the	O
same	O
function	B
f	O
)	O
.	O
although	O
we	O
know	O
where	O
the	O
minimum	O
of	O
this	O
function	B
is	O
,	O
just	O
using	O
linear	B
algebra	I
,	O
we	O
wish	O
to	O
use	O
this	O
function	B
as	O
a	O
toy	O
model	B
for	O
more	O
complex	O
functions	O
which	O
however	O
locally	O
look	O
approximately	O
quadratic	O
.	O
one	O
approach	B
is	O
to	O
search	O
along	O
a	O
particular	O
direction	O
p	O
,	O
and	O
ﬁnd	O
a	O
minimum	O
along	O
this	O
direction	O
.	O
we	O
can	O
then	O
search	O
for	O
a	O
deeper	O
minima	O
by	O
looking	O
in	O
diﬀerent	O
directions	O
.	O
that	O
is	O
,	O
we	O
can	O
search	O
along	O
a	O
line	O
x0	O
+	O
λp	O
such	O
that	O
the	O
function	B
attains	O
a	O
minimum	O
.	O
that	O
is	O
,	O
the	O
directional	B
derivative	I
is	O
zero	O
along	O
this	O
line	O
,	O
this	O
has	O
solution	O
,	O
·	O
p	O
≡	O
−∇f	O
(	O
x0	O
)	O
·	O
p	O
ptap	O
(	O
a.7.2	O
)	O
now	O
we	O
’	O
ve	O
found	O
the	O
minimum	O
along	O
the	O
line	O
through	O
x0	O
with	O
direction	O
p.	O
but	O
how	O
should	O
we	O
choose	O
the	O
line	B
search	I
direction	O
p	O
?	O
it	O
would	O
seem	O
sensible	O
to	O
choose	O
successive	O
line	B
search	I
directions	O
p	O
according	O
to	O
pnew	O
=	O
−∇f	O
(	O
x∗	O
)	O
,	O
so	O
that	O
each	O
time	O
we	O
minimise	O
the	O
function	B
along	O
the	O
line	O
of	O
steepest	O
descent	B
.	O
however	O
,	O
this	O
is	O
far	O
from	O
the	O
optimal	O
choice	O
in	O
the	O
case	O
of	O
minimising	O
quadratic	O
functions	O
.	O
a	O
much	O
better	O
set	O
of	O
search	O
directions	O
are	O
those	O
deﬁned	O
by	O
the	O
vectors	O
conjugate	B
to	O
a.	O
if	O
the	O
matrix	B
a	O
were	O
diagonal	O
,	O
then	O
the	O
minimisation	O
is	O
straightforward	O
and	O
can	O
be	O
carried	O
out	O
inde-	O
pendently	O
for	O
each	O
dimension	O
.	O
if	O
we	O
could	O
ﬁnd	O
an	O
invertible	O
matrix	B
p	O
with	O
the	O
property	O
that	O
ptap	O
is	O
diagonal	O
then	O
the	O
solution	O
is	O
easy	O
since	O
for	O
f	O
(	O
ˆx	O
)	O
=	O
1	O
2	O
ˆxtptapˆx	O
−	O
btpˆx	O
+	O
c	O
(	O
a.7.3	O
)	O
with	O
x	O
=	O
pˆx	O
,	O
we	O
can	O
compute	O
the	O
minimum	O
for	O
each	O
dimension	O
of	O
ˆx	O
separately	O
and	O
then	O
retransform	O
to	O
ﬁnd	O
x∗	O
=	O
pˆx∗	O
.	O
deﬁnition	O
141	O
(	O
conjugate	O
vectors	O
)	O
.	O
the	O
vectors	O
pi	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
are	O
called	O
conjugate	B
to	O
the	O
matrix	B
a	O
,	O
if	O
and	O
only	O
if	O
for	O
i	O
,	O
j	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
and	O
i	O
(	O
cid:54	O
)	O
=	O
j	O
:	O
i	O
api	O
>	O
0	O
.	O
pt	O
i	O
apj	O
=	O
0	O
and	O
pt	O
(	O
a.7.4	O
)	O
the	O
two	O
conditions	O
guarantee	O
that	O
conjugate	O
vectors	O
are	O
linearly	B
independent	I
:	O
assume	O
that	O
k	O
(	O
cid:88	O
)	O
0	O
=	O
i−1	O
(	O
cid:88	O
)	O
k	O
(	O
cid:88	O
)	O
αjpj	O
=	O
αjpj	O
+	O
αipi	O
+	O
αjpj	O
(	O
a.7.5	O
)	O
j=1	O
j=1	O
j=i+1	O
now	O
multiplying	O
from	O
the	O
left	O
with	O
pt	O
as	O
we	O
can	O
make	O
this	O
argument	O
for	O
any	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
,	O
all	O
of	O
the	O
αi	O
must	O
be	O
zero	O
.	O
i	O
a	O
yields	O
0	O
=	O
αipt	O
i	O
api	O
.	O
so	O
αi	O
is	O
zero	O
since	O
we	O
know	O
that	O
pt	O
i	O
api	O
>	O
0.	O
a.7.2	O
gram-schmidt	O
construction	B
of	O
conjugate	O
vectors	O
let	O
p	O
=	O
(	O
p1	O
,	O
p2	O
,	O
.	O
.	O
.	O
,	O
pk	O
)	O
,	O
where	O
the	O
columns	O
are	O
formed	O
from	O
a-conjugate	O
vectors	O
and	O
note	O
that	O
we	O
start	O
with	O
an	O
n	O
by	O
k	O
matrix	B
,	O
k	O
≤	O
n.	O
the	O
reason	O
for	O
this	O
is	O
that	O
we	O
are	O
aiming	O
at	O
an	O
incremental	O
procedure	O
,	O
where	O
columns	O
are	O
successively	O
added	O
to	O
p.	O
since	O
(	O
ptap	O
)	O
ij	O
=	O
pt	O
i	O
apj	O
the	O
matrix	B
ptap	O
will	O
be	O
diagonal	O
i	O
apj	O
=	O
0	O
for	O
i	O
(	O
cid:54	O
)	O
=	O
j.	O
assume	O
we	O
already	O
have	O
k	O
conjugate	O
vectors	O
p1	O
,	O
.	O
.	O
.	O
,	O
pk	O
and	O
let	O
v	O
be	O
a	O
vector	O
if	O
pt	O
which	O
is	O
linearly	B
independent	I
of	O
p1	O
,	O
.	O
.	O
.	O
,	O
pk	O
.	O
we	O
then	O
set	O
pk+1	O
=	O
v	O
−	O
pt	O
j	O
av	O
j	O
apj	O
pt	O
pj	O
(	O
a.7.6	O
)	O
k	O
(	O
cid:88	O
)	O
j=1	O
for	O
which	O
it	O
is	O
clear	O
that	O
the	O
vectors	O
p1	O
,	O
.	O
.	O
.	O
,	O
pk+1	O
are	O
conjugate	B
if	O
a	O
is	O
positive	B
deﬁnite	I
.	O
using	O
the	O
gram-	O
schmidt	O
procedure	O
we	O
can	O
construct	O
n	O
conjugate	O
vectors	O
for	O
a	O
positive	B
deﬁnite	I
matrix	O
in	O
the	O
following	O
way	O
.	O
we	O
start	O
with	O
n	O
linearly	B
independent	I
vectors	O
u1	O
,	O
.	O
.	O
.	O
,	O
un	O
,	O
we	O
might	O
chose	O
ui	O
=	O
ei	O
,	O
the	O
unit	B
vector	I
in	O
558	O
draft	O
march	O
9	O
,	O
2010	O
multivariate	B
minimization	O
:	O
quadratic	O
functions	O
the	O
ith	O
direction	O
.	O
we	O
then	O
set	O
p1	O
=	O
u1	O
and	O
use	O
(	O
a.7.6	O
)	O
to	O
compute	O
p2	O
from	O
p1	O
and	O
v	O
=	O
u2	O
.	O
next	O
we	O
set	O
v	O
=	O
u3	O
and	O
compute	O
p3	O
from	O
p1	O
,	O
p2	O
and	O
v.	O
continuing	O
in	O
this	O
manner	O
we	O
obtain	O
n	O
conjugate	O
vectors	O
.	O
note	O
that	O
at	O
each	O
stage	O
of	O
the	O
procedure	O
the	O
vectors	O
u1	O
,	O
.	O
.	O
.	O
,	O
uk	O
span	O
the	O
same	O
subspace	O
as	O
the	O
vectors	O
p1	O
,	O
.	O
.	O
.	O
,	O
pk	O
.	O
what	O
is	O
going	O
to	O
happen	O
if	O
a	O
is	O
not	O
positive	B
deﬁnite	I
?	O
if	O
we	O
could	O
ﬁnd	O
n	O
conjugate	O
vectors	O
,	O
a	O
would	O
be	O
positive	B
deﬁnite	I
,	O
and	O
so	O
at	O
some	O
point	O
k	O
the	O
gram-schmidt	O
procedure	O
must	O
break	O
down	O
.	O
this	O
will	O
happen	O
if	O
pt	O
k	O
apk	O
≤	O
0.	O
so	O
by	O
trying	O
out	O
the	O
gram-schmidt	O
procedure	O
,	O
we	O
can	O
in	O
fact	O
ﬁnd	O
out	O
whether	O
a	O
matrix	B
is	O
positive	B
deﬁnite	I
.	O
a.7.3	O
the	O
conjugate	B
vectors	I
algorithm	I
let	O
us	O
assume	O
that	O
when	O
minimising	O
f	O
(	O
x	O
)	O
=	O
1	O
conjugate	O
to	O
a	O
which	O
we	O
use	O
as	O
our	O
search	O
directions	O
.	O
so	O
2xtax	O
−	O
btx	O
+	O
c	O
we	O
ﬁrst	O
construct	O
n	O
vectors	O
p1	O
,	O
.	O
.	O
.	O
,	O
pn	O
xk+1	O
=	O
xk	O
+	O
αkpk	O
.	O
at	O
each	O
step	O
we	O
chose	O
αk	O
by	O
an	O
exact	O
line	O
search	O
,	O
thus	O
αk	O
=	O
−	O
pt	O
k	O
gk	O
k	O
apk	O
pt	O
.	O
(	O
a.7.7	O
)	O
(	O
a.7.8	O
)	O
this	O
conjugate	B
vectors	I
algorithm	I
,	O
has	O
the	O
geometrical	O
interpretation	O
that	O
not	O
only	O
is	O
the	O
directional	O
deriva-	O
tive	O
zero	O
at	O
the	O
new	O
point	O
along	O
the	O
direction	O
pk	O
,	O
it	O
is	O
zero	O
along	O
all	O
the	O
previous	O
search	O
directions	O
p1	O
,	O
.	O
.	O
.	O
,	O
pk	O
)	O
.	O
theorem	B
1	O
(	O
luenberger	O
expanding	O
subspace	O
theorem	O
)	O
.	O
i=1	O
be	O
a	O
sequence	O
of	O
vectors	O
in	O
rn	O
conjugate	B
to	O
the	O
(	O
positive	B
deﬁnite	I
)	O
matrix	B
a	O
and	O
let	O
{	O
pi	O
}	O
n	O
f	O
(	O
x	O
)	O
=	O
1	O
2xtax	O
−	O
btx	O
+	O
c.	O
then	O
for	O
any	O
x1	O
the	O
sequence	O
{	O
xk	O
}	O
generated	O
according	O
to	O
(	O
a.7.7	O
)	O
and	O
(	O
a.7.8	O
)	O
has	O
the	O
property	O
that	O
the	O
directional	B
derivative	I
of	O
f	O
in	O
the	O
direction	O
pi	O
vanishes	O
at	O
the	O
point	O
xk+1	O
if	O
i	O
≤	O
k	O
;	O
i.e	O
.	O
dpif	O
(	O
xk+1	O
)	O
=	O
0.	O
proof	O
:	O
for	O
i	O
≤	O
k	O
,	O
we	O
can	O
write	O
xk+1	O
as	O
:	O
k	O
(	O
cid:88	O
)	O
xk+1	O
=	O
xi+1	O
+	O
αjpj	O
.	O
(	O
a.7.9	O
)	O
j=i+1	O
since	O
∇f	O
(	O
x	O
)	O
=	O
ax	O
−	O
b	O
we	O
have	O
∇f	O
(	O
xk+1	O
)	O
=	O
axk+1	O
−	O
b	O
=	O
axi+1	O
−	O
b	O
+	O
a	O
k	O
(	O
cid:88	O
)	O
j=i+1	O
αjpj	O
=	O
∇f	O
(	O
xi+1	O
)	O
+	O
k	O
(	O
cid:88	O
)	O
j=i+1	O
αjapj	O
(	O
a.7.10	O
)	O
so	O
dpif	O
(	O
xk+1	O
)	O
=	O
pt	O
i	O
∇f	O
(	O
xk+1	O
)	O
=	O
pt	O
i	O
∇f	O
(	O
xi+1	O
)	O
+	O
k	O
(	O
cid:88	O
)	O
j=i+1	O
αjpt	O
i	O
apj	O
=	O
(	O
dpif	O
)	O
(	O
xi+1	O
)	O
+	O
k	O
(	O
cid:88	O
)	O
j=i+1	O
αjpt	O
i	O
apj	O
(	O
a.7.11	O
)	O
now	O
(	O
dxi+1f	O
)	O
(	O
pi	O
)	O
=	O
0	O
since	O
the	O
point	O
xi+1	O
was	O
obtained	O
by	O
an	O
exact	O
line	O
search	O
in	O
the	O
direction	O
pi	O
.	O
but	O
all	O
i	O
apj	O
=	O
0	O
by	O
conjugacy	O
.	O
so	O
(	O
dpif	O
)	O
(	O
xk+1	O
)	O
=	O
0.	O
of	O
the	O
terms	O
in	O
the	O
sum	O
over	O
j	O
also	O
vanish	O
since	O
j	O
>	O
i	O
and	O
pt	O
the	O
subspace	O
theorem	O
shows	O
,	O
that	O
because	O
we	O
use	O
conjugate	O
vectors	O
,	O
optimizing	O
in	O
the	O
direction	O
pk	O
,	O
does	O
not	O
spoil	O
the	O
optimality	O
w.r.t	O
.	O
to	O
the	O
previous	O
search	O
directions	O
.	O
in	O
particular	O
after	O
having	O
carried	O
out	O
n	O
steps	O
of	O
the	O
algorithm	B
we	O
have	O
(	O
dxn+1f	O
)	O
(	O
pi	O
)	O
=	O
∇f	O
t	O
(	O
xn+1	O
)	O
pi	O
=	O
0	O
,	O
for	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
the	O
n	O
equations	O
can	O
be	O
written	O
in	O
a	O
more	O
compact	O
form	O
as	O
:	O
∇f	O
t	O
(	O
xn+1	O
)	O
(	O
p1	O
,	O
p2	O
,	O
.	O
.	O
.	O
,	O
pn	O
)	O
=	O
0	O
.	O
(	O
a.7.12	O
)	O
draft	O
march	O
9	O
,	O
2010	O
559	O
k	O
(	O
cid:88	O
)	O
i=1	O
multivariate	B
minimization	O
:	O
quadratic	O
functions	O
the	O
square	O
matrix	B
p	O
=	O
(	O
p1	O
,	O
p2	O
,	O
.	O
.	O
.	O
pn	O
)	O
is	O
invertible	O
since	O
the	O
pi	O
are	O
conjugate	B
,	O
so	O
∇f	O
(	O
xn+1	O
)	O
=	O
0	O
:	O
the	O
point	O
xn+1	O
is	O
the	O
minimum	O
x∗	O
of	O
the	O
quadratic	O
function	O
f.	O
so	O
in	O
contrast	O
to	O
gradient	B
descent	I
,	O
for	O
a	O
quadratic	O
function	O
the	O
conjugate	B
vectors	I
algorithm	I
converges	O
in	O
a	O
ﬁnite	O
number	O
of	O
steps	O
.	O
a.7.4	O
the	O
conjugate	B
gradients	I
algorithm	I
the	O
conjugate	B
gradients	I
algorithm	I
is	O
a	O
special	O
case	O
of	O
the	O
conjugate	B
vectors	I
algorithm	I
,	O
in	O
which	O
the	O
gram-	O
schmidt	O
procedure	O
becomes	O
very	O
simple	O
.	O
we	O
do	O
not	O
use	O
a	O
predetermined	O
set	O
of	O
conjugate	O
vectors	O
but	O
construct	O
these	O
‘	O
on-the-ﬂy	O
’	O
.	O
after	O
k-steps	O
of	O
the	O
conjugate	B
vectors	I
algorithm	I
we	O
need	O
to	O
construct	O
a	O
vector	O
pk+1	O
which	O
is	O
conjugate	B
to	O
p1	O
,	O
.	O
.	O
.	O
,	O
pk	O
.	O
this	O
could	O
be	O
done	O
by	O
applying	O
the	O
gram-schmidt	O
procedure	O
to	O
any	O
vector	O
v	O
which	O
is	O
linearly	B
independent	I
of	O
the	O
vectors	O
p1	O
,	O
.	O
.	O
.	O
,	O
pk	O
.	O
in	O
the	O
conjugate	B
gradients	I
algorithm	I
one	O
makes	O
the	O
special	O
choice	O
v	O
=	O
−∇f	O
(	O
xk+1	O
)	O
.	O
by	O
the	O
subspace	O
theorem	O
the	O
gradient	B
at	O
the	O
new	O
point	O
xk+1	O
is	O
orthogonal	B
to	O
pi	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k.	O
so	O
∇f	O
(	O
xk+1	O
)	O
is	O
linearly	B
independent	I
of	O
p1	O
,	O
.	O
.	O
.	O
,	O
pk	O
and	O
a	O
valid	O
choice	O
for	O
v	O
,	O
unless	O
∇f	O
(	O
xk+1	O
)	O
=	O
0.	O
in	O
the	O
latter	O
case	O
xk+1	O
is	O
our	O
minimum	O
and	O
we	O
are	O
done	O
,	O
and	O
from	O
now	O
on	O
we	O
assume	O
that	O
∇f	O
(	O
xk+1	O
)	O
(	O
cid:54	O
)	O
=	O
0.	O
using	O
the	O
notation	O
gk	O
=	O
∇f	O
(	O
xk	O
)	O
,	O
the	O
equation	B
for	O
the	O
new	O
search	O
direction	O
given	O
by	O
the	O
gram-schmidt	O
procedure	O
is	O
:	O
pk+1	O
=	O
−gk+1	O
+	O
pt	O
i	O
agk+1	O
pt	O
i	O
api	O
pi	O
.	O
(	O
a.7.13	O
)	O
since	O
gk+1	O
is	O
orthogonal	B
to	O
pi	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
,	O
by	O
the	O
subspace	O
theorem	O
we	O
have	O
pt	O
αk+1	O
can	O
be	O
written	O
as	O
k+1gk+1	O
=	O
−gt	O
k+1gk+1	O
.	O
so	O
αk+1	O
=	O
k+1gk+1	O
gt	O
k+1apk+1	O
pt	O
,	O
(	O
a.7.14	O
)	O
and	O
in	O
particular	O
αk+1	O
(	O
cid:54	O
)	O
=	O
0.	O
we	O
now	O
want	O
to	O
show	O
that	O
because	O
we	O
have	O
been	O
using	O
the	O
conjugate	B
gradients	I
algorithm	I
at	O
the	O
previous	O
steps	O
as	O
well	O
,	O
in	O
equation	B
(	O
a.7.13	O
)	O
all	O
terms	O
but	O
the	O
last	O
in	O
the	O
sum	O
over	O
i	O
vanish	O
.	O
we	O
shall	O
assume	O
that	O
k	O
>	O
0	O
since	O
in	O
the	O
ﬁrst	O
step	O
(	O
k	O
=	O
0	O
)	O
we	O
just	O
set	O
p1	O
=	O
−g1	O
.	O
first	O
note	O
that	O
gi+1	O
−	O
gi	O
=	O
axi+1	O
−	O
b	O
−	O
(	O
axi	O
−	O
b	O
)	O
=	O
a	O
(	O
xi+1	O
−	O
xi	O
)	O
=	O
αiapi	O
and	O
since	O
αi	O
(	O
cid:54	O
)	O
=	O
0	O
:	O
api	O
=	O
(	O
gi+1	O
−	O
gi	O
)	O
/αi	O
.	O
so	O
in	O
equation	B
(	O
a.7.13	O
)	O
:	O
i	O
agk+1	O
=	O
gt	O
pt	O
k+1api	O
=	O
gt	O
k+1	O
(	O
gi+1	O
−	O
gi	O
)	O
/αi	O
=	O
(	O
gt	O
k+1gi+1	O
−	O
gt	O
k+1gi	O
)	O
/αi	O
(	O
a.7.15	O
)	O
(	O
a.7.16	O
)	O
(	O
a.7.17	O
)	O
since	O
the	O
pi	O
where	O
obtained	O
by	O
applying	O
the	O
gram-schmidt	O
procedure	O
to	O
the	O
gradients	O
gi	O
,	O
the	O
subspace	O
theorem	O
gt	O
k+1gi	O
=	O
0	O
for	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k.	O
this	O
shows	O
that	O
k+1pi	O
=	O
0	O
,	O
implies	O
,	O
also	O
gt	O
(	O
cid:26	O
)	O
0	O
pt	O
i	O
agk+1	O
=	O
(	O
gt	O
k+1gi+1	O
−	O
gt	O
k+1gi	O
)	O
/αi	O
=	O
gt	O
k+1gk+1/αk	O
if	O
1	O
≤	O
i	O
<	O
k	O
if	O
i	O
=	O
k	O
hence	O
equation	B
(	O
a.7.13	O
)	O
simpliﬁes	O
to	O
pk+1	O
=	O
−gk+1	O
+	O
gt	O
k+1gk+1/αk	O
pt	O
k	O
apk	O
pk	O
.	O
this	O
can	O
be	O
brought	O
into	O
an	O
even	O
simpler	O
form	O
by	O
applying	O
equation	B
(	O
a.7.14	O
)	O
to	O
αk	O
:	O
pk+1	O
=	O
−gk+1	O
+	O
k+1gk+1	O
gt	O
pt	O
k	O
apk	O
we	O
shall	O
write	O
this	O
in	O
the	O
form	O
k	O
apk	O
pt	O
gt	O
k	O
gk	O
pk	O
=	O
−gk+1	O
+	O
gt	O
k+1gk+1	O
gt	O
k	O
gk	O
pk	O
pk+1	O
=	O
−gk+1	O
+	O
βkpk	O
where	O
βk	O
=	O
gt	O
k+1gk+1	O
gt	O
k	O
gk	O
.	O
(	O
a.7.18	O
)	O
(	O
a.7.19	O
)	O
(	O
a.7.20	O
)	O
(	O
a.7.21	O
)	O
560	O
draft	O
march	O
9	O
,	O
2010	O
multivariate	B
minimization	O
:	O
quadratic	O
functions	O
algorithm	B
31	O
conjugate	O
gradients	O
for	O
minimising	O
a	O
function	B
f	O
(	O
x	O
)	O
5	O
:	O
1	O
:	O
k	O
=	O
1	O
2	O
:	O
choose	O
x1	O
.	O
3	O
:	O
p1	O
=	O
−g1	O
4	O
:	O
while	O
gk	O
(	O
cid:54	O
)	O
=	O
0	O
do	O
αk	O
=	O
argmin	O
xk+1	O
:	O
=	O
xk	O
+	O
αkpk	O
βk	O
:	O
=	O
gt	O
k+1gk+1/	O
(	O
gt	O
k	O
gk	O
)	O
pk+1	O
:	O
=	O
−gk+1	O
+	O
βkpk	O
k	O
=	O
k	O
+	O
1	O
6	O
:	O
7	O
:	O
8	O
:	O
9	O
:	O
10	O
:	O
end	O
while	O
f	O
(	O
xk	O
+	O
αkpk	O
)	O
αk	O
(	O
cid:46	O
)	O
line	B
search	I
the	O
formula	O
(	O
a.7.21	O
)	O
for	O
βk	O
is	O
due	O
to	O
fletcher	O
and	O
reeves	O
.	O
since	O
the	O
gradients	O
are	O
orthogonal	B
,	O
βk	O
can	O
also	O
be	O
written	O
as	O
βk	O
=	O
gt	O
k+1	O
(	O
gk+1	O
−	O
gk	O
)	O
k	O
gk	O
gt	O
,	O
(	O
a.7.22	O
)	O
this	O
is	O
the	O
polak-ribiere	O
formula	O
.	O
the	O
choice	O
between	O
the	O
two	O
expression	O
for	O
βk	O
can	O
be	O
of	O
some	O
importance	B
if	O
f	O
is	O
not	O
quadratic	O
.	O
a.7.5	O
newton	O
’	O
s	O
method	O
consider	O
a	O
function	B
f	O
(	O
x	O
)	O
that	O
we	O
wish	O
to	O
ﬁnd	O
the	O
minimum	O
of	O
.	O
a	O
taylor	O
expansion	O
up	O
to	O
second	O
order	O
gives	O
f	O
(	O
x	O
+	O
∆	O
)	O
=	O
f	O
(	O
x	O
)	O
+	O
∆t∇f	O
+	O
1	O
2	O
∆th∆	O
+	O
o	O
(	O
|∆|3	O
)	O
(	O
a.7.23	O
)	O
the	O
matrix	B
h	O
is	O
the	O
hessian	O
.	O
diﬀerentiating	O
the	O
right	O
hand	O
side	O
with	O
respect	O
to	O
∆	O
(	O
or	O
,	O
equivalently	O
,	O
completing	O
the	O
square	O
)	O
,	O
we	O
ﬁnd	O
that	O
the	O
right	O
hand	O
side	O
has	O
its	O
lowest	O
value	B
when	O
∇f	O
=	O
h∆	O
⇒	O
∆	O
=	O
h−1∇f	O
hence	O
,	O
an	O
optimisation	B
routine	O
to	O
minimise	O
e	O
is	O
given	O
by	O
the	O
newton	O
update	O
xk+1	O
=	O
xk	O
−	O
h−1∇f	O
(	O
a.7.24	O
)	O
(	O
a.7.25	O
)	O
a	O
beneﬁt	O
of	O
newton	O
method	O
over	O
gradient	B
descent	I
is	O
that	O
the	O
decrease	O
in	O
the	O
objective	O
function	B
is	O
invariant	O
under	O
a	O
linear	B
change	O
of	O
co-ordinates	O
,	O
y	O
=	O
mx	O
.	O
a.7.6	O
quasi-newton	O
methods	O
for	O
large-scale	O
problems	O
the	O
inversion	B
of	O
the	O
hessian	O
is	O
computationally	O
demanding	O
,	O
especially	O
if	O
the	O
matrix	B
is	O
close	O
to	O
singular	B
.	O
an	O
alternative	O
is	O
to	O
set	O
up	O
the	O
iteration	B
xk+1	O
=	O
xk	O
−	O
αkskgk	O
.	O
(	O
a.7.26	O
)	O
this	O
is	O
a	O
very	O
general	O
form	O
;	O
if	O
sk	O
=	O
a−1	O
then	O
we	O
have	O
newton	O
’	O
s	O
method	O
,	O
while	O
if	O
sk	O
=	O
i	O
we	O
have	O
steepest	O
descent	B
.	O
in	O
general	O
it	O
would	O
seem	O
to	O
be	O
a	O
good	O
idea	O
to	O
choose	O
sk	O
to	O
be	O
an	O
approximation	B
to	O
the	O
inverse	O
hessian	O
.	O
also	O
note	O
that	O
it	O
is	O
important	O
that	O
sk	O
be	O
positive	B
deﬁnite	I
so	O
that	O
for	O
small	O
αk	O
we	O
obtain	O
a	O
descent	B
method	O
.	O
the	O
idea	O
behind	O
most	O
quasi-newton	O
methods	O
is	O
to	O
try	O
to	O
construct	O
an	O
approximate	B
inverse	O
hessian	O
˜hk	O
using	O
information	O
gathered	O
as	O
the	O
descent	B
progresses	O
,	O
and	O
to	O
set	O
sk	O
=	O
˜hk	O
.	O
as	O
we	O
have	O
seen	O
,	O
for	O
a	O
quadratic	O
optimization	O
problem	B
we	O
have	O
the	O
relationship	O
gk+1	O
−	O
gk	O
=	O
a	O
(	O
xk+1	O
−	O
xk	O
)	O
draft	O
march	O
9	O
,	O
2010	O
(	O
a.7.27	O
)	O
561	O
algorithm	B
32	O
quasi-newton	O
for	O
minimising	O
a	O
function	B
f	O
(	O
x	O
)	O
constrained	B
optimisation	I
using	O
lagrange	O
multipliers	O
1	O
:	O
k	O
=	O
1	O
2	O
:	O
choose	O
x1	O
3	O
:	O
˜h1	O
=	O
i	O
4	O
:	O
while	O
gk	O
(	O
cid:54	O
)	O
=	O
0	O
do	O
pk	O
=	O
−	O
˜hkgk	O
αk	O
=	O
argmin	O
xk+1	O
:	O
=	O
xk	O
+	O
αkpk	O
sk	O
=	O
xk+1	O
−	O
xk	O
,	O
yk	O
=	O
gk+1	O
−	O
gk	O
,	O
and	O
update	O
˜hk+1	O
k	O
=	O
k	O
+	O
1	O
f	O
(	O
xk	O
+	O
αkpk	O
)	O
5	O
:	O
6	O
:	O
αk	O
7	O
:	O
8	O
:	O
9	O
:	O
10	O
:	O
end	O
while	O
deﬁning	O
sk	O
=	O
xk+1	O
−	O
xk	O
and	O
yk	O
=	O
gk+1	O
−	O
gk	O
we	O
see	O
that	O
equation	B
a.7.27	O
becomes	O
yk	O
=	O
ask	O
(	O
cid:46	O
)	O
line	B
search	I
(	O
a.7.28	O
)	O
(	O
a.7.29	O
)	O
it	O
is	O
reasonable	O
to	O
demand	O
that	O
1	O
≤	O
i	O
≤	O
k	O
˜hk+1yi	O
=	O
si	O
(	O
a.7.30	O
)	O
after	O
n	O
linearly	B
independent	I
steps	O
we	O
would	O
then	O
have	O
˜hn+1	O
=	O
a−1	O
.	O
for	O
k	O
<	O
n	O
there	O
are	O
an	O
inﬁnity	O
of	O
solutions	O
for	O
˜hk+1	O
satisfying	O
equation	B
a.7.30	O
.	O
a	O
popular	O
choice	O
is	O
the	O
broyden-fletcher-goldfarb-shanno	O
(	O
or	O
bfgs	O
)	O
update	O
,	O
given	O
by	O
(	O
cid:32	O
)	O
(	O
cid:33	O
)	O
˜hk+1	O
=	O
˜hk	O
+	O
1	O
+	O
˜hkyk	O
yt	O
k	O
k	O
sk	O
yt	O
skyt	O
k	O
skst	O
k	O
k	O
yk	O
−	O
st	O
˜hk	O
+	O
˜hkykst	O
k	O
k	O
yk	O
st	O
(	O
a.7.31	O
)	O
(	O
a.7.32	O
)	O
(	O
a.7.33	O
)	O
this	O
is	O
a	O
rank-2	O
correction	O
to	O
˜hk	O
constructed	O
from	O
the	O
vectors	O
sk	O
and	O
˜hkyk	O
.	O
the	O
direction	O
vectors	O
p1	O
,	O
p2	O
,	O
.	O
.	O
.	O
,	O
pk	O
,	O
pk	O
=	O
−	O
˜hkgk	O
,	O
produced	O
by	O
the	O
algorithm	B
obey	O
pt	O
i	O
apj	O
=	O
0	O
˜hk+1api	O
=	O
pi	O
1	O
≤	O
i	O
<	O
j	O
≤	O
k	O
1	O
≤	O
i	O
≤	O
k	O
equation	B
a.7.33	O
is	O
called	O
the	O
hereditary	O
property	O
.	O
in	O
our	O
notation	O
sk	O
=	O
αkpk	O
,	O
and	O
as	O
the	O
α	O
’	O
s	O
are	O
non-zero	O
,	O
equation	B
a.7.32	O
can	O
also	O
be	O
written	O
as	O
st	O
i	O
asj	O
=	O
0	O
1	O
≤	O
i	O
<	O
j	O
≤	O
k	O
(	O
a.7.34	O
)	O
since	O
the	O
pk	O
’	O
s	O
are	O
a-conjugate	O
and	O
since	O
we	O
successively	O
minimize	O
f	O
in	O
these	O
directions	O
,	O
we	O
see	O
that	O
the	O
bfgs	O
algorithm	B
is	O
a	O
conjugate	B
direction	O
method	O
;	O
with	O
the	O
choice	O
of	O
h1	O
=	O
i	O
it	O
is	O
in	O
fact	O
the	O
conjugate	B
gradient	I
method	O
.	O
note	O
that	O
the	O
storage	O
requirements	O
for	O
quasi	O
newton	O
methods	O
scale	O
quadratically	O
with	O
the	O
number	O
of	O
variables	O
,	O
and	O
hence	O
tends	O
to	O
be	O
used	O
for	O
smaller	O
problems	O
.	O
limited	O
memory	O
bfgs	O
reduces	O
the	O
storage	O
by	O
only	O
using	O
the	O
l	O
latest	O
updates	O
in	O
computing	O
the	O
approximate	B
hessian	O
inverse	O
,	O
equation	B
(	O
a.7.31	O
)	O
.	O
in	O
contrast	O
,	O
the	O
memory	O
requirements	O
for	O
pure	O
conjugate	B
gradient	I
methods	O
scale	O
only	O
linearly	O
with	O
the	O
dimension	O
of	O
x.	O
a.7	O
constrained	B
optimisation	I
using	O
lagrange	O
multipliers	O
single	O
constraint	O
consider	O
ﬁrst	O
the	O
problem	B
of	O
minimising	O
f	O
(	O
x	O
)	O
subject	O
to	O
a	O
single	O
constraint	O
c	O
(	O
x	O
)	O
=	O
0.	O
imagine	O
that	O
we	O
have	O
already	O
identiﬁed	O
an	O
x	O
that	O
satisﬁes	O
the	O
constraint	O
,	O
that	O
is	O
c	O
(	O
x	O
)	O
=	O
0.	O
how	O
can	O
we	O
tell	O
if	O
this	O
x	O
minimises	O
562	O
draft	O
march	O
9	O
,	O
2010	O
constrained	B
optimisation	I
using	O
lagrange	O
multipliers	O
the	O
function	B
f	O
?	O
we	O
are	O
only	O
allowed	O
to	O
search	O
for	O
lower	O
function	O
values	O
around	O
this	O
x	O
in	O
directions	O
which	O
are	O
consistent	B
with	O
the	O
constraint	O
.	O
for	O
a	O
small	O
change	O
δ	O
,	O
the	O
change	O
in	O
the	O
constraint	O
is	O
,	O
c	O
(	O
x	O
+	O
δ	O
)	O
≈	O
c	O
(	O
x	O
)	O
+	O
δ	O
·	O
∇c	O
(	O
x	O
)	O
(	O
a.7.1	O
)	O
hence	O
,	O
in	O
order	O
that	O
the	O
constraint	O
remains	O
satisﬁed	O
,	O
we	O
can	O
only	O
search	O
in	O
a	O
direction	O
such	O
that	O
δ·∇c	O
(	O
x	O
)	O
=	O
0	O
,	O
that	O
is	O
in	O
directions	O
δ	O
that	O
are	O
orthogonal	B
to	O
∇c	O
(	O
x	O
)	O
.	O
so	O
,	O
let	O
us	O
explore	O
the	O
change	O
in	O
f	O
along	O
a	O
direction	O
δ	O
where	O
δ	O
·	O
∇c	O
(	O
x	O
)	O
=	O
0	O
,	O
f	O
(	O
x	O
+	O
δ	O
)	O
≈	O
f	O
(	O
x	O
)	O
+	O
∇f	O
(	O
x	O
)	O
·	O
δ	O
.	O
(	O
a.7.2	O
)	O
since	O
we	O
are	O
looking	O
for	O
a	O
point	O
x	O
that	O
minimises	O
the	O
function	B
f	O
,	O
we	O
require	O
x	O
to	O
be	O
a	O
stationary	B
point	O
,	O
∇f	O
(	O
x	O
)	O
·	O
δ	O
=	O
0.	O
thus	O
δ	O
must	O
be	O
orthogonal	B
to	O
both	O
∇f	O
(	O
x	O
)	O
and	O
∇c	O
(	O
x	O
)	O
.	O
since	O
we	O
wish	O
to	O
constrain	O
δ	O
as	O
little	O
as	O
possible	O
,	O
the	O
most	O
freedom	O
is	O
given	O
by	O
enforcing	O
∇f	O
(	O
x	O
)	O
to	O
be	O
parallel	B
to	O
∇c	O
(	O
x	O
)	O
,	O
so	O
that	O
∇f	O
(	O
x	O
)	O
=	O
λ∇c	O
(	O
x	O
)	O
(	O
a.7.3	O
)	O
for	O
some	O
λ	O
∈	O
r.	O
to	O
solve	O
the	O
optimisation	B
problem	O
therefore	O
,	O
we	O
look	O
for	O
a	O
point	O
x	O
such	O
that	O
∇f	O
(	O
x	O
)	O
=	O
λ∇c	O
(	O
x	O
)	O
,	O
for	O
some	O
λ	O
,	O
and	O
for	O
which	O
c	O
(	O
x	O
)	O
=	O
0.	O
an	O
alternative	O
formulation	O
of	O
this	O
dual	B
requirement	O
is	O
to	O
look	O
for	O
x	O
and	O
λ	O
that	O
jointly	O
minimise	O
the	O
lagrangian	O
l	O
(	O
x	O
,	O
λ	O
)	O
=	O
f	O
(	O
x	O
)	O
−	O
λc	O
(	O
x	O
)	O
(	O
a.7.4	O
)	O
diﬀerentiating	O
with	O
respect	O
to	O
x	O
,	O
we	O
get	O
the	O
requirement	O
∇f	O
(	O
x	O
)	O
=	O
λ∇c	O
(	O
x	O
)	O
,	O
and	O
diﬀerentiating	O
with	O
respect	O
to	O
λ	O
,	O
we	O
get	O
that	O
c	O
(	O
x	O
)	O
=	O
0.	O
multiple	O
constraints	O
consider	O
the	O
problem	B
of	O
optimising	O
f	O
(	O
x	O
)	O
subject	O
to	O
the	O
constraints	O
ci	O
(	O
x	O
)	O
=	O
0	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
r	O
<	O
n	O
,	O
where	O
n	O
is	O
the	O
dimensionality	O
of	O
the	O
space	O
.	O
denote	O
by	O
s	O
the	O
n	O
−	O
r	O
dimensional	O
subspace	O
of	O
x	O
which	O
obeys	O
the	O
constraints	O
.	O
assume	O
that	O
x∗	O
is	O
such	O
an	O
optimum	O
.	O
as	O
in	O
the	O
unconstrained	O
case	O
,	O
we	O
consider	O
perturbations	O
v	O
to	O
x∗	O
,	O
but	O
now	O
such	O
that	O
v	O
lies	O
in	O
s	O
2	O
,	O
.	O
.	O
.	O
,	O
a∗	O
ci	O
(	O
x∗	O
+	O
hv	O
)	O
=	O
ci	O
(	O
x∗	O
)	O
+	O
vt∇ci	O
(	O
x∗	O
)	O
+	O
o	O
(	O
h2	O
)	O
(	O
a.7.5	O
)	O
i	O
=	O
∇ci	O
(	O
x∗	O
)	O
.	O
thus	O
for	O
the	O
perturbation	O
to	O
stay	O
within	O
s	O
,	O
we	O
require	O
that	O
vta∗	O
let	O
a∗	O
i	O
=	O
0	O
for	O
all	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
r.	O
r.	O
then	O
this	O
condition	O
can	O
be	O
rewritten	O
as	O
a∗v	O
=	O
0	O
.	O
1	O
,	O
a∗	O
let	O
a∗	O
be	O
the	O
matrix	B
whose	O
columns	O
are	O
a∗	O
we	O
also	O
require	O
for	O
a	O
local	B
optimum	O
that	O
vt∇f	O
=	O
0	O
for	O
all	O
v	O
in	O
s.	O
we	O
see	O
that	O
∇f	O
must	O
be	O
orthogonal	B
to	O
v	O
,	O
and	O
that	O
v	O
must	O
be	O
orthogonal	B
to	O
the	O
a∗	O
i	O
’	O
s	O
.	O
this	O
can	O
be	O
achieved	O
by	O
forcing	O
∇f	O
to	O
be	O
a	O
linear	B
combination	O
of	O
the	O
a∗	O
i	O
a∗	O
∗	O
(	O
a.7.6	O
)	O
geometrically	O
this	O
says	O
that	O
the	O
gradient	B
vector	O
is	O
normal	B
to	O
the	O
tangent	O
plane	O
to	O
s	O
at	O
x∗	O
.	O
these	O
conditions	O
give	O
rise	O
to	O
the	O
method	O
of	O
lagrange	O
multipliers	O
for	O
optimisation	B
problems	O
with	O
equality	O
constraints	O
.	O
the	O
method	O
requires	O
ﬁnding	O
x∗	O
and	O
λ∗	O
which	O
solve	O
the	O
equations	O
r	O
(	O
cid:88	O
)	O
∇f	O
=	O
i	O
’	O
s	O
,	O
i.e	O
.	O
i=1	O
λ	O
i	O
∇f	O
=	O
(	O
cid:88	O
)	O
i	O
ai	O
(	O
x	O
)	O
λi	O
(	O
a.7.7	O
)	O
ci	O
(	O
x	O
)	O
=	O
0	O
for	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
r	O
(	O
a.7.8	O
)	O
there	O
are	O
n	O
+	O
r	O
equations	O
and	O
n	O
+	O
r	O
unknowns	O
,	O
so	O
the	O
system	B
is	O
well-determined	O
.	O
however	O
,	O
the	O
system	B
is	O
nonlinear	O
(	O
in	O
x	O
)	O
in	O
general	O
,	O
and	O
so	O
may	O
not	O
be	O
easy	O
to	O
solve	O
.	O
we	O
can	O
restate	O
these	O
conditions	O
by	O
introducing	O
the	O
lagrangian	O
function	B
l	O
(	O
x	O
,	O
λ	O
)	O
=	O
f	O
(	O
x	O
)	O
−	O
(	O
cid:88	O
)	O
λici	O
(	O
x	O
)	O
.	O
(	O
a.7.9	O
)	O
i	O
the	O
partial	O
derivatives	O
of	O
l	O
with	O
respect	O
to	O
x	O
and	O
λ	O
reproduce	O
equations	O
a.7.7	O
and	O
a.7.8	O
.	O
hence	O
a	O
necessary	O
condition	O
for	O
a	O
local	B
minimizer	O
is	O
that	O
x∗	O
,	O
λ∗	O
is	O
a	O
stationary	B
point	O
of	O
the	O
lagrangian	O
function	B
.	O
note	O
that	O
this	O
stationary	B
point	O
is	O
not	O
a	O
minimum	O
but	O
a	O
saddle	O
point	O
,	O
as	O
l	O
depends	O
linearly	O
on	O
λ.	O
we	O
have	O
given	O
ﬁrst-order	O
necessary	O
and	O
suﬃcient	O
conditions	O
for	O
a	O
local	B
optimum	O
.	O
to	O
show	O
that	O
this	O
optimum	O
is	O
a	O
local	B
minimum	O
,	O
we	O
would	O
need	O
to	O
consider	O
second-order	O
conditions	O
,	O
analogous	O
to	O
the	O
positive	O
deﬁniteness	O
of	O
the	O
hessian	O
in	O
the	O
unconstrained	O
case	O
;	O
this	O
can	O
be	O
done	O
,	O
but	O
will	O
not	O
be	O
considered	O
here	O
.	O
draft	O
march	O
9	O
,	O
2010	O
563	O
constrained	B
optimisation	I
using	O
lagrange	O
multipliers	O
564	O
draft	O
march	O
9	O
,	O
2010	O
bibliography	O
[	O
1	O
]	O
l.	O
f.	O
abbott	O
,	O
j.	O
a.	O
varela	O
,	O
k.	O
sen	O
,	O
and	O
s.	O
b.	O
nelson	O
.	O
synaptic	O
depression	B
and	O
cortical	O
gain	O
control	O
.	O
science	O
,	O
275:220–223	O
,	O
1997	O
.	O
[	O
2	O
]	O
d.	O
h.	O
ackley	O
,	O
g.	O
e.	O
hinton	O
,	O
and	O
t.	O
j.	O
sejnowski	O
.	O
a	O
learning	B
algorithm	O
for	O
boltzmann	O
machines	O
.	O
cognitive	O
science	O
,	O
9:147–169	O
,	O
1985	O
.	O
[	O
3	O
]	O
r.	O
p.	O
adams	O
and	O
d.	O
j.	O
c.	O
mackay	O
.	O
bayesian	O
online	B
changepoint	O
detection	O
.	O
cavendish	O
laboratory	O
,	O
department	O
of	O
physics	O
,	O
university	O
of	O
cambridge	O
,	O
cambridge	O
,	O
uk	O
,	O
2006.	O
arxiv:0710.3742v1	O
[	O
stat.ml	O
]	O
.	O
[	O
4	O
]	O
e.	O
airoldi	O
,	O
d.	O
blei	O
,	O
e.	O
xing	O
,	O
and	O
s.	O
fienberg	O
.	O
a	O
latent	B
mixed	O
membership	O
model	B
for	O
relational	O
data	B
.	O
in	O
linkkdd	O
’	O
05	O
:	O
proceedings	O
of	O
the	O
3rd	O
international	O
workshop	O
on	O
link	O
discovery	O
,	O
pages	O
82–89	O
,	O
new	O
york	O
,	O
ny	O
,	O
usa	O
,	O
2005.	O
acm	O
.	O
[	O
5	O
]	O
e.	O
m.	O
airoldi	O
,	O
d.	O
m.	O
blei	O
,	O
s.	O
e.	O
fienberg	O
,	O
and	O
e.	O
p.	O
xing	O
.	O
mixed	B
membership	I
stochastic	O
blockmodels	O
.	O
journal	O
of	O
machine	O
learning	B
research	O
,	O
9:1981–2014	O
,	O
2008	O
.	O
[	O
6	O
]	O
d.	O
l.	O
alspach	O
and	O
h.	O
w.	O
sorenson	O
.	O
nonlinear	O
bayesian	O
estimation	O
using	O
gaussian	O
sum	O
approximations	O
.	O
ieee	O
transactions	O
on	O
automatic	O
control	O
,	O
17	O
(	O
4	O
)	O
:439–448	O
,	O
1972	O
.	O
[	O
7	O
]	O
s-i	O
.	O
amari	O
.	O
natural	B
gradient	I
works	O
eﬃciently	O
in	O
learning	B
.	O
neural	B
computation	I
,	O
10	O
(	O
2	O
)	O
:251–276	O
,	O
1998	O
.	O
[	O
8	O
]	O
s-i	O
.	O
amari	O
.	O
natural	B
gradient	I
learning	O
for	O
over	O
and	O
under-complete	B
bases	O
in	O
ica	O
.	O
neural	B
computation	I
,	O
11:1875–1883	O
,	O
1999	O
.	O
[	O
9	O
]	O
i.	O
androutsopoulos	O
,	O
j.	O
koutsias	O
,	O
k.	O
v.	O
chandrinos	O
,	O
and	O
c.	O
d.	O
spyropoulos	O
.	O
an	O
experimental	O
comparison	O
in	O
proceedings	O
of	O
of	O
naive	O
bayesian	O
and	O
keyword-based	O
anti-spam	O
ﬁltering	B
with	O
personal	O
e-mail	O
messages	O
.	O
the	O
23rd	O
annual	O
international	O
acm	O
sigir	O
conference	O
on	O
research	O
and	O
development	O
in	O
information	B
retrieval	I
,	O
pages	O
160–167	O
,	O
new	O
york	O
,	O
ny	O
,	O
usa	O
,	O
2000.	O
acm	O
.	O
[	O
10	O
]	O
s.	O
arora	O
and	O
c.	O
lund	O
.	O
hardness	O
of	O
approximations	O
.	O
in	O
approximation	B
algorithms	O
for	O
np-hard	O
problems	O
,	O
pages	O
399–446	O
.	O
pws	O
publishing	O
co.	O
,	O
boston	O
,	O
ma	O
,	O
usa	O
,	O
1997	O
.	O
[	O
11	O
]	O
f.	O
r.	O
bach	O
and	O
m.	O
i.	O
jordan	O
.	O
thin	B
junction	O
trees	O
.	O
in	O
t.	O
g.	O
dietterich	O
,	O
s.	O
becker	O
,	O
and	O
z.	O
ghahramani	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
number	O
14	O
,	O
pages	O
569–576	O
,	O
cambridge	O
,	O
ma	O
,	O
2001.	O
mit	O
press	O
.	O
[	O
12	O
]	O
f.	O
r.	O
bach	O
and	O
m.	O
i.	O
jordan	O
.	O
a	O
probabilistic	B
interpretation	O
of	O
canonical	B
correlation	I
analysis	I
.	O
computer	O
science	O
division	O
and	O
department	O
of	O
statistics	O
688	O
,	O
university	O
of	O
california	O
berkeley	O
,	O
berkeley	O
,	O
usa	O
,	O
2005	O
.	O
[	O
13	O
]	O
y.	O
bar-shalom	O
and	O
xiao-rong	O
li	O
.	O
estimation	O
and	O
tracking	O
:	O
principles	O
,	O
techniques	O
and	O
software	O
.	O
artech	O
house	O
,	O
norwood	O
,	O
ma	O
,	O
1998	O
.	O
[	O
14	O
]	O
d.	O
barber	O
.	O
dynamic	B
bayesian	O
networks	O
with	O
deterministic	O
tables	O
.	O
in	O
s.	O
becker	O
,	O
s.	O
thrun	O
,	O
and	O
k.	O
obermayer	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
number	O
15	O
,	O
pages	O
713–720	O
,	O
cambridge	O
,	O
ma	O
,	O
2003.	O
mit	O
press	O
.	O
[	O
15	O
]	O
d.	O
barber	O
.	O
learning	B
in	O
spiking	O
neural	O
assemblies	O
.	O
in	O
s.	O
becker	O
,	O
s.	O
thrun	O
,	O
and	O
k.	O
obermayer	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
number	O
15	O
,	O
pages	O
149–156	O
,	O
cambridge	O
,	O
ma	O
,	O
2003.	O
mit	O
press	O
.	O
565	O
bibliography	O
bibliography	O
[	O
16	O
]	O
d.	O
barber	O
.	O
are	O
two	O
classiﬁers	O
performing	O
equally	O
?	O
a	O
treatment	O
using	O
bayesian	O
hypothesis	B
testing	I
.	O
idiap-	O
rr	O
57	O
,	O
idiap	O
,	O
rue	O
de	O
simplon	O
4	O
,	O
martigny	O
,	O
ch-1920	O
,	O
switerland	O
,	O
may	O
2004.	O
idiap-rr	O
04-57	O
.	O
[	O
17	O
]	O
d.	O
barber	O
.	O
expectation	B
correction	I
for	O
smoothing	B
in	O
switching	O
linear	O
gaussian	O
state	O
space	O
models	O
.	O
journal	O
of	O
machine	O
learning	B
research	O
,	O
7:2515–2540	O
,	O
2006	O
.	O
[	O
18	O
]	O
d.	O
barber	O
.	O
clique	B
matrices	O
for	O
statistical	O
graph	B
decomposition	O
and	O
parameterising	O
restricted	B
positive	O
in	O
d.	O
a.	O
mcallester	O
and	O
p.	O
myllymaki	O
,	O
editors	O
,	O
uncertainty	B
in	O
artiﬁcial	O
intelligence	O
,	O
deﬁnite	O
matrices	O
.	O
number	O
24	O
,	O
pages	O
26–33	O
,	O
corvallis	O
,	O
oregon	O
,	O
usa	O
,	O
2008.	O
auai	O
press	O
.	O
[	O
19	O
]	O
d.	O
barber	O
and	O
f.	O
v.	O
agakov	O
.	O
correlated	O
sequence	B
learning	I
in	O
a	O
network	O
of	O
spiking	O
neurons	O
using	O
maximum	B
likelihood	I
.	O
informatics	O
research	O
reports	O
edi-inf-rr-0149	O
,	O
edinburgh	O
university	O
,	O
2002	O
.	O
[	O
20	O
]	O
d.	O
barber	O
and	O
f.v	O
.	O
agakov	O
.	O
the	O
im	O
algorithm	B
:	O
a	O
variational	B
approach	I
to	O
information	O
maximization	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
number	O
16	O
,	O
2004	O
.	O
[	O
21	O
]	O
d.	O
barber	O
and	O
c.	O
m.	O
bishop	O
.	O
bayesian	O
model	B
comparison	O
by	O
monte	O
carlo	O
chaining	O
.	O
in	O
m.	O
c.	O
mozer	O
,	O
m.	O
i.	O
jordan	O
,	O
and	O
t.	O
petsche	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
number	O
9	O
,	O
pages	O
333–339	O
,	O
cambridge	O
,	O
ma	O
,	O
1997.	O
mit	O
press	O
.	O
[	O
22	O
]	O
d.	O
barber	O
and	O
c.	O
m.	O
bishop	O
.	O
ensemble	O
learning	B
in	O
bayesian	O
neural	O
networks	O
.	O
machine	O
learning	B
,	O
pages	O
215–237	O
.	O
springer	O
,	O
1998.	O
in	O
neural	O
networks	O
and	O
[	O
23	O
]	O
d.	O
barber	O
and	O
s.	O
chiappa	O
.	O
uniﬁed	O
inference	B
for	O
variational	O
bayesian	O
linear	B
gaussian	O
state-space	O
models	O
.	O
in	O
b.	O
sch¨olkopf	O
,	O
j.	O
platt	O
,	O
and	O
t.	O
hoﬀman	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
number	O
19	O
,	O
pages	O
81–88	O
,	O
cambridge	O
,	O
ma	O
,	O
2007.	O
mit	O
press	O
.	O
[	O
24	O
]	O
d.	O
barber	O
and	O
w.	O
wiegerinck	O
.	O
tractable	O
variational	O
structures	O
for	O
approximating	O
graphical	O
models	O
.	O
in	O
m.	O
s.	O
kearns	O
,	O
s.	O
a.	O
solla	O
,	O
and	O
d.	O
a.	O
cohn	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
number	O
11	O
,	O
pages	O
183–189	O
,	O
cambridge	O
,	O
ma	O
,	O
1999.	O
mit	O
press	O
.	O
[	O
25	O
]	O
d.	O
barber	O
and	O
c.	O
k.	O
i.	O
williams	O
.	O
gaussian	O
processes	O
for	O
bayesian	O
classiﬁcation	B
via	O
hybrid	O
monte	O
carlo	O
.	O
in	O
m.	O
c.	O
mozer	O
,	O
m.	O
i.	O
jordan	O
,	O
and	O
t.	O
petsche	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
nips	O
9	O
,	O
pages	O
340–346	O
,	O
cambridge	O
,	O
ma	O
,	O
1997.	O
mit	O
press	O
.	O
[	O
26	O
]	O
r.	O
j.	O
baxter	O
.	O
exactly	O
solved	O
models	O
in	O
statistical	O
mechanics	O
.	O
academic	O
press	O
,	O
1982	O
.	O
[	O
27	O
]	O
m.	O
j.	O
beal	O
,	O
f.	O
falciani	O
,	O
z.	O
ghahramani	O
,	O
c.	O
rangel	O
,	O
and	O
d.	O
l.	O
wild	O
.	O
a	O
bayesian	O
approach	B
to	O
reconstructing	O
genetic	O
regulatory	O
networks	O
with	O
hidden	O
factors	O
.	O
bioinformatics	B
,	O
(	O
21	O
)	O
:349–356	O
,	O
2005	O
.	O
[	O
28	O
]	O
a.	O
becker	O
and	O
d.	O
geiger	O
.	O
a	O
suﬃciently	O
fast	O
algorithm	B
for	O
ﬁnding	O
close	O
to	O
optimal	O
clique	B
trees	O
.	O
artiﬁcial	O
intelligence	O
,	O
125	O
(	O
1-2	O
)	O
:3–17	O
,	O
2001	O
.	O
[	O
29	O
]	O
a.	O
j.	O
bell	O
and	O
t.	O
j.	O
sejnowski	O
.	O
an	O
information-maximization	O
approach	B
to	O
blind	O
separation	B
and	O
blind	O
deconvolution	O
.	O
neural	B
computation	I
,	O
7	O
(	O
6	O
)	O
:1129–1159	O
,	O
1995	O
.	O
[	O
30	O
]	O
r.	O
e.	O
bellman	O
.	O
dynamic	B
programming	O
.	O
princeton	O
university	O
press	O
,	O
princeton	O
,	O
nj	O
,	O
1957.	O
paperback	O
edition	O
by	O
dover	O
publications	O
(	O
2003	O
)	O
.	O
[	O
31	O
]	O
y.	O
bengio	O
and	O
p.	O
frasconi	O
.	O
(	O
7	O
)	O
:1231–1249	O
,	O
1996.	O
input-output	B
hmms	O
for	O
sequence	O
processing	O
.	O
ieee	O
trans	O
.	O
neural	O
networks	O
,	O
[	O
32	O
]	O
a.	O
l.	O
berger	O
,	O
s.	O
d.	O
della	O
pietra	O
,	O
and	O
v.	O
j.	O
d.	O
della	O
pietra	O
.	O
a	O
maximum	O
entropy	O
approach	B
to	O
natural	B
language	O
processing	O
.	O
computational	O
linguistics	O
,	O
22	O
(	O
1	O
)	O
:39–71	O
,	O
1996	O
.	O
[	O
33	O
]	O
j.	O
o.	O
berger	O
.	O
statistical	O
decision	B
theory	I
and	O
bayesian	O
analysis	B
.	O
springer	O
,	O
second	O
edition	O
,	O
1985	O
.	O
[	O
34	O
]	O
d.	O
p.	O
bertsekas	O
.	O
dynamic	B
programming	O
and	O
optimal	O
control	O
.	O
athena	O
scientiﬁc	O
,	O
second	O
edition	O
,	O
2000	O
.	O
[	O
35	O
]	O
j.	O
besag	O
.	O
spatial	O
interactions	O
and	O
the	O
statistical	O
analysis	B
of	O
lattice	O
systems	O
.	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
,	O
series	O
b	O
,	O
36	O
(	O
2	O
)	O
:192–236	O
,	O
1974	O
.	O
[	O
36	O
]	O
j.	O
besag	O
.	O
on	O
the	O
statistical	O
analysis	B
of	O
dirty	O
pictures	O
.	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
,	O
series	O
b	O
,	O
48:259–302	O
,	O
1986	O
.	O
[	O
37	O
]	O
j.	O
besag	O
and	O
p.	O
green	O
.	O
spatial	O
statistics	O
and	O
bayesian	O
computation	O
.	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
,	O
series	O
b	O
,	O
55:25–37	O
,	O
1993	O
.	O
566	O
draft	O
march	O
9	O
,	O
2010	O
bibliography	O
bibliography	O
[	O
38	O
]	O
g.	O
j.	O
bierman	O
.	O
measurement	O
updating	O
using	O
the	O
u-d	O
factorization	O
.	O
automatica	O
,	O
12:375–382	O
,	O
1976	O
.	O
[	O
39	O
]	O
n.	O
l.	O
biggs	O
.	O
discrete	B
mathematics	O
.	O
oxford	O
university	O
press	O
,	O
1990	O
.	O
[	O
40	O
]	O
k.	O
binder	O
and	O
a.	O
p.	O
young	O
.	O
spin	O
glasses	O
:	O
experimental	O
facts	O
,	O
theoretical	O
concepts	O
,	O
and	O
open	O
questions	O
.	O
rev	O
.	O
mod	O
.	O
phys.	O
,	O
58	O
(	O
4	O
)	O
:801–976	O
,	O
oct	O
1986	O
.	O
[	O
41	O
]	O
c.	O
m.	O
bishop	O
.	O
neural	O
networks	O
for	O
pattern	O
recognition	O
.	O
oxford	O
university	O
press	O
,	O
1995	O
.	O
[	O
42	O
]	O
c.	O
m.	O
bishop	O
.	O
pattern	O
recognition	O
and	O
machine	O
learning	B
.	O
springer	O
,	O
2006	O
.	O
[	O
43	O
]	O
c.	O
m.	O
bishop	O
and	O
m.	O
svens´en	O
.	O
bayesian	O
hierarchical	O
mixtures	O
of	O
experts	O
.	O
in	O
u.	O
kjaerulﬀ	O
and	O
c.	O
meek	O
,	O
editors	O
,	O
proceedings	O
nineteenth	O
conference	O
on	O
uncertainty	B
in	O
artiﬁcial	O
intelligence	O
,	O
pages	O
57–64	O
.	O
morgan	O
kaufmann	O
,	O
2003	O
.	O
[	O
44	O
]	O
d.	O
blei	O
,	O
a.	O
ng	O
,	O
and	O
m.	O
jordan	O
.	O
latent	B
dirichlet	O
allocation	O
.	O
journal	O
of	O
machine	O
learning	B
research	O
,	O
(	O
3	O
)	O
:993–	O
1022	O
,	O
2003	O
.	O
[	O
45	O
]	O
r.	O
r.	O
bouckaert	O
.	O
bayesian	O
belief	B
networks	I
:	O
from	O
construction	B
to	O
inference	B
.	O
phd	O
thesis	O
,	O
university	O
of	O
utrecht	O
,	O
1995	O
.	O
[	O
46	O
]	O
s.	O
boyd	O
and	O
l.	O
vandenberghe	O
.	O
convex	O
optimization	O
.	O
cambridge	O
university	O
press	O
,	O
2004	O
.	O
[	O
47	O
]	O
y.	O
boykov	O
and	O
v.	O
kolmogorov	O
.	O
an	O
experimental	O
comparison	O
of	O
min-cut/max-ﬂow	O
algorithms	O
for	O
energy	B
minimization	O
in	O
vision	O
.	O
ieee	O
trans	O
.	O
pattern	O
anal	O
.	O
mach	O
.	O
intell.	O
,	O
26	O
(	O
9	O
)	O
:1124–1137	O
,	O
2004	O
.	O
[	O
48	O
]	O
y.	O
boykov	O
,	O
o.	O
veksler	O
,	O
and	O
r.	O
zabih	O
.	O
fast	O
approximate	B
energy	O
minimization	O
via	O
graph	B
cuts	O
.	O
ieee	O
trans	O
.	O
pattern	O
anal	O
.	O
mach	O
.	O
intell.	O
,	O
23:1222–1239	O
,	O
2001	O
.	O
[	O
49	O
]	O
m.	O
brand	O
.	O
incremental	O
singular	B
value	O
decomposition	B
of	O
uncertain	B
data	O
with	O
missing	O
values	O
.	O
conference	O
on	O
computer	O
vision	O
(	O
eccv	O
)	O
,	O
pages	O
707–720	O
,	O
2002.	O
in	O
european	O
[	O
50	O
]	O
j.	O
breese	O
and	O
d.	O
heckerman	O
.	O
decision-theoretic	O
troubleshooting	O
:	O
a	O
framework	O
for	O
repair	O
and	O
experiment	O
.	O
in	O
e.	O
horvitz	O
and	O
f.	O
jensen	O
,	O
editors	O
,	O
uncertainty	B
in	O
artiﬁcial	O
intelligence	O
,	O
number	O
12	O
,	O
pages	O
124–132	O
,	O
san	O
francisco	O
,	O
ca	O
,	O
1996.	O
morgan	O
kaufmann	O
.	O
[	O
51	O
]	O
h.	O
bunke	O
and	O
t.	O
caelli	O
.	O
hidden	B
markov	O
models	O
:	O
applications	O
in	O
computer	O
vision	O
.	O
machine	O
perception	O
and	O
artiﬁcial	O
intelligence	O
.	O
world	O
scientiﬁc	O
publishing	O
co.	O
,	O
inc.	O
,	O
river	O
edge	O
,	O
nj	O
,	O
usa	O
,	O
2001	O
.	O
[	O
52	O
]	O
w.	O
buntine	O
.	O
theory	O
reﬁnement	O
on	O
bayesian	O
networks	O
.	O
in	O
uncertainty	B
in	O
artiﬁcial	O
intelligence	O
,	O
number	O
7	O
,	O
pages	O
52–60	O
,	O
san	O
francisco	O
,	O
ca	O
,	O
1991.	O
morgan	O
kaufmann	O
.	O
[	O
53	O
]	O
a.	O
cano	O
and	O
s.	O
moral	O
.	O
advances	O
in	O
intelligent	O
computing	O
–	O
ipmu	O
1994	O
,	O
chapter	O
heuristic	O
algorithms	O
for	O
the	O
triangulation	B
of	O
graphs	O
,	O
pages	O
98–107	O
.	O
number	O
945	O
in	O
lectures	O
notes	O
in	O
computer	O
sciences	O
.	O
springer-verlag	O
,	O
1995	O
.	O
[	O
54	O
]	O
o.	O
capp´e	O
,	O
e.	O
moulines	O
,	O
and	O
t.	O
ryden	O
.	O
inference	B
in	O
hidden	B
markov	O
models	O
.	O
springer	O
,	O
new	O
york	O
,	O
2005	O
.	O
[	O
55	O
]	O
e.	O
castillo	O
,	O
j.	O
m.	O
gutierrez	O
,	O
and	O
a.	O
s.	O
hadi	O
.	O
expert	O
systems	O
and	O
probabilistic	B
network	O
models	O
.	O
springer	O
,	O
1997	O
.	O
[	O
56	O
]	O
a.	O
t.	O
cemgil	O
.	O
bayesian	O
inference	B
in	O
non-negative	B
matrix	I
factorisation	I
models	O
.	O
technical	O
report	O
cued/f-	O
infeng/tr.609	O
,	O
university	O
of	O
cambridge	O
,	O
july	O
2008	O
.	O
[	O
57	O
]	O
a.	O
t.	O
cemgil	O
,	O
b.	O
kappen	O
,	O
and	O
d.	O
barber	O
.	O
a	O
generative	B
model	O
for	O
music	O
transcription	O
.	O
ieee	O
transactions	O
on	O
audio	O
,	O
speech	O
and	O
language	O
processing	O
,	O
14	O
(	O
2	O
)	O
:679–694	O
,	O
2006	O
.	O
[	O
58	O
]	O
h.	O
s.	O
chang	O
,	O
m.	O
c.	O
fu	O
,	O
j.	O
hu	O
,	O
and	O
s.	O
i.	O
marcus	O
.	O
simulation-based	O
algorithms	O
for	O
markov	O
decision	O
processes	O
.	O
springer	O
,	O
2007	O
.	O
[	O
59	O
]	O
s.	O
chiappa	O
and	O
d.	O
barber	O
.	O
bayesian	O
linear	B
gaussian	O
state	O
space	O
models	O
for	O
biosignal	O
decomposition	B
.	O
signal	O
processing	O
letters	O
,	O
14	O
(	O
4	O
)	O
:267–270	O
,	O
2007	O
.	O
[	O
60	O
]	O
s.	O
chib	O
and	O
m.	O
dueker	O
.	O
non-markovian	O
regime	O
switching	B
with	O
endogenous	O
states	O
and	O
time-varying	B
state	O
strengths	O
.	O
econometric	O
society	O
2004	O
north	O
american	O
summer	O
meetings	O
600	O
,	O
econometric	O
society	O
,	O
august	O
2004	O
.	O
[	O
61	O
]	O
c.	O
k.	O
chow	O
and	O
c.	O
n.	O
liu	O
.	O
approximating	O
discrete	B
probability	O
distributions	O
with	O
dependence	O
trees	O
.	O
ieee	O
transactions	O
on	O
information	O
theory	O
,	O
14	O
(	O
3	O
)	O
:462–467	O
,	O
1968.	O
draft	O
march	O
9	O
,	O
2010	O
567	O
bibliography	O
bibliography	O
[	O
62	O
]	O
p.	O
s.	O
churchland	O
and	O
t.	O
j.	O
sejnowski	O
.	O
the	O
computational	O
brain	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
,	O
usa	O
,	O
1994	O
.	O
[	O
63	O
]	O
d.	O
cohn	O
and	O
h.	O
chang	O
.	O
learning	B
to	O
probabilistically	O
identify	O
authoritative	O
documents	O
.	O
in	O
p.	O
langley	O
,	O
editor	O
,	O
international	O
conference	O
on	O
machine	O
learning	B
,	O
number	O
17	O
,	O
pages	O
167–174	O
.	O
morgan	O
kaufmann	O
,	O
2000	O
.	O
[	O
64	O
]	O
d.	O
cohn	O
and	O
t.	O
hofmann	O
.	O
the	O
missing	B
link	O
-	O
a	O
probabilistic	B
model	O
of	O
document	O
content	O
and	O
hypertext	O
connectivity	O
.	O
number	O
13	O
,	O
pages	O
430–436	O
,	O
cambridge	O
,	O
ma	O
,	O
2001.	O
mit	O
press	O
.	O
[	O
65	O
]	O
a.	O
c.	O
c.	O
coolen	O
,	O
r.	O
k¨uhn	O
,	O
and	O
p.	O
sollich	O
.	O
theory	O
of	O
neural	O
information	O
processing	O
systems	O
.	O
oxford	O
university	O
press	O
,	O
2005	O
.	O
[	O
66	O
]	O
g.	O
f.	O
cooper	O
and	O
e.	O
herskovits	O
.	O
a	O
bayesian	O
method	O
for	O
the	O
induction	O
of	O
probabilistic	B
networks	O
from	O
data	B
.	O
machine	O
learning	B
,	O
9	O
(	O
4	O
)	O
:309–347	O
,	O
1992	O
.	O
[	O
67	O
]	O
a.	O
corduneanu	O
and	O
c.	O
m.	O
bishop	O
.	O
variational	O
bayesian	O
model	B
selection	I
for	O
mixture	B
distributions	O
.	O
in	O
t.	O
jaakkola	O
and	O
t.	O
richardson	O
,	O
editors	O
,	O
artifcial	O
intelligence	O
and	O
statistics	O
,	O
pages	O
27–34	O
.	O
morgan	O
kaufmann	O
,	O
2001	O
.	O
[	O
68	O
]	O
m.	O
t.	O
cover	O
and	O
j.	O
a.	O
thomas	O
.	O
elements	O
of	O
information	O
theory	O
.	O
wiley	O
,	O
1991	O
.	O
[	O
69	O
]	O
r.	O
g.	O
cowell	O
,	O
a.	O
p.	O
dawid	O
,	O
s.	O
l.	O
lauritzen	O
,	O
and	O
d.	O
j.	O
spiegelhalter	O
.	O
probabilistic	B
networks	O
and	O
expert	O
systems	O
.	O
springer	O
,	O
1999	O
.	O
[	O
70	O
]	O
d.	O
r.	O
cox	O
and	O
n.	O
wermuth	O
.	O
multivariate	B
dependencies	O
.	O
chapman	O
and	O
hall	O
,	O
1996	O
.	O
[	O
71	O
]	O
n.	O
cristianini	O
and	O
j.	O
shawe-taylor	O
.	O
an	O
introduction	O
to	O
support	O
vector	O
machines	O
.	O
cambridge	O
university	O
press	O
,	O
2000	O
.	O
[	O
72	O
]	O
p.	O
dangauthier	O
,	O
r.	O
herbrich	O
,	O
t.	O
minka	O
,	O
and	O
t.	O
graepel	O
.	O
trueskill	O
through	O
time	O
:	O
revisiting	O
the	O
history	O
of	O
chess	O
.	O
in	O
b.	O
sch¨olkopf	O
,	O
j.	O
platt	O
,	O
and	O
t.	O
hoﬀman	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
number	O
19	O
,	O
pages	O
569–576	O
,	O
cambridge	O
,	O
ma	O
,	O
2007.	O
mit	O
press	O
.	O
[	O
73	O
]	O
h.	O
a.	O
david	O
.	O
the	O
method	O
of	O
paired	O
comparisons	O
.	O
oxford	O
university	O
press	O
,	O
new	O
york	O
,	O
1988	O
.	O
[	O
74	O
]	O
a.	O
p.	O
dawid	O
.	O
inﬂuence	B
diagrams	I
for	O
causal	B
modelling	O
and	O
inference	B
.	O
international	O
statistical	O
review	O
,	O
70:161–	O
189	O
,	O
2002	O
.	O
[	O
75	O
]	O
a.	O
p.	O
dawid	O
and	O
s.	O
l.	O
lauritzen	O
.	O
hyper	B
markov	O
laws	O
in	O
the	O
statistical	O
analysis	B
of	O
decomposable	B
graphical	O
models	O
.	O
annals	O
of	O
statistics	O
,	O
21	O
(	O
3	O
)	O
:1272–1317	O
,	O
1993	O
.	O
[	O
76	O
]	O
p.	O
dayan	O
and	O
l.f.	O
abbott	O
.	O
theoretical	O
neuroscience	O
.	O
mit	O
press	O
,	O
2001	O
.	O
[	O
77	O
]	O
p.	O
dayan	O
and	O
g.	O
e.	O
hinton	O
.	O
using	O
expectation-maximization	O
for	O
reinforcement	B
learning	I
.	O
neural	O
computa-	O
tion	O
,	O
9:271–278	O
,	O
1997	O
.	O
[	O
78	O
]	O
t.	O
de	O
bie	O
,	O
n.	O
cristianini	O
,	O
and	O
r.	O
rosipal	O
.	O
handbook	O
of	O
geometric	O
computing	O
:	O
applications	O
in	O
pattern	O
recognition	O
,	O
computer	O
vision	O
,	O
neuralcomputing	O
,	O
and	O
robotics	O
,	O
chapter	O
eigenproblems	O
in	O
pattern	O
recognition	O
.	O
springer-verlag	O
,	O
2005	O
.	O
[	O
79	O
]	O
r.	O
dechter	O
.	O
bucket	B
elimination	I
:	O
a	O
unifying	O
framework	O
for	O
probabilistic	B
inference	O
algorithms	O
.	O
in	O
e.	O
horvitz	O
and	O
f.	O
jensen	O
,	O
editors	O
,	O
uncertainty	B
in	O
artiﬁcial	O
intelligence	O
,	O
pages	O
211–219	O
,	O
san	O
francisco	O
,	O
ca	O
,	O
1996.	O
morgan	O
kaufmann	O
.	O
[	O
80	O
]	O
s.	O
diederich	O
and	O
m.	O
opper	O
.	O
learning	B
of	O
correlated	O
patterns	O
in	O
spin-glass	O
networks	O
by	O
local	B
learning	O
rules	O
.	O
physical	O
review	O
letters	O
,	O
58	O
(	O
9	O
)	O
:949–952	O
,	O
1986	O
.	O
[	O
81	O
]	O
r.	O
diestel	O
.	O
graph	B
theory	O
.	O
springer	O
,	O
2005	O
.	O
[	O
82	O
]	O
a.	O
doucet	O
and	O
a.	O
m.	O
johansen	O
.	O
a	O
tutorial	O
on	O
particle	O
filtering	O
and	O
smoothing	B
:	O
fifteen	O
years	O
later	O
.	O
in	O
d.	O
crisan	O
and	O
b.	O
rozovsky	O
,	O
editors	O
,	O
oxford	O
handbook	O
of	O
nonlinear	O
filtering	O
.	O
oxford	O
university	O
press	O
,	O
2009	O
.	O
[	O
83	O
]	O
r.	O
o.	O
duda	O
,	O
p.	O
e.	O
hart	O
,	O
and	O
d.	O
g.	O
stork	O
.	O
pattern	O
classiﬁcation	B
.	O
wiley-interscience	O
publication	O
,	O
2000	O
.	O
[	O
84	O
]	O
r.	O
durbin	O
,	O
s.	O
r.	O
eddy	O
,	O
a.	O
krogh	O
,	O
and	O
g.	O
mitchison	O
.	O
biological	O
sequence	O
analysis	O
:	O
probabilistic	B
models	O
of	O
proteins	O
and	O
nucleic	O
acids	O
.	O
cambridge	O
university	O
press	O
,	O
1999	O
.	O
[	O
85	O
]	O
a.	O
d¨uring	O
,	O
a.	O
c.	O
c.	O
coolen	O
,	O
and	O
d.	O
sherrington	O
.	O
phase	O
diagram	O
and	O
storage	O
capacity	B
of	O
sequence	O
processing	O
neural	O
networks	O
.	O
journal	O
of	O
physics	O
a	O
,	O
31:8607–8621	O
,	O
1998	O
.	O
568	O
draft	O
march	O
9	O
,	O
2010	O
bibliography	O
bibliography	O
[	O
86	O
]	O
j.	O
m.	O
gutierrez	O
e.	O
castillo	O
and	O
a.	O
s.	O
hadi	O
.	O
expert	O
systems	O
and	O
probabilistic	B
network	O
models	O
.	O
springer	O
verlag	O
,	O
1997	O
.	O
[	O
87	O
]	O
j.	O
edmonds	O
and	O
r.	O
m.	O
karp	O
.	O
theoretical	O
improvements	O
in	O
algorithmic	O
eﬃciency	O
for	O
network	B
ﬂow	I
problems	O
.	O
journal	O
of	O
the	O
acm	O
,	O
19	O
(	O
2	O
)	O
:248–264	O
,	O
1972	O
.	O
[	O
88	O
]	O
r.	O
edwards	O
and	O
a.	O
sokal	O
.	O
generalization	O
of	O
the	O
fortium-kasteleyn-swendson-wang	O
representation	B
and	O
monte	O
carlo	O
algorithm	B
.	O
physical	O
review	O
d	O
,	O
38:2009–2012	O
,	O
1988	O
.	O
[	O
89	O
]	O
a.	O
e.	O
elo	O
.	O
the	O
rating	O
of	O
chess	O
players	O
,	O
past	O
and	O
present	O
.	O
arco	O
,	O
new	O
york	O
,	O
second	O
edition	O
,	O
1986	O
.	O
[	O
90	O
]	O
y.	O
ephraim	O
and	O
w.	O
j.	O
j.	O
roberts	O
.	O
revisiting	O
autoregressive	O
hidden	B
markov	O
modeling	O
of	O
speech	O
signals	O
.	O
ieee	O
signal	O
processing	O
letters	O
,	O
12	O
(	O
2	O
)	O
:166–169	O
,	O
february	O
2005	O
.	O
[	O
91	O
]	O
e.	O
erosheva	O
,	O
s.	O
fienberg	O
,	O
and	O
j.	O
laﬀerty	O
.	O
mixed	B
membership	I
models	O
of	O
scientiﬁc	O
publications	O
.	O
in	O
proceedings	O
of	O
the	O
national	O
academy	O
of	O
sciences	O
,	O
volume	O
101	O
,	O
pages	O
5220–5227	O
,	O
2004	O
.	O
[	O
92	O
]	O
r-e.	O
fan	O
,	O
p-h.	O
chen	O
,	O
and	O
c-j	O
.	O
lin	O
.	O
working	O
set	O
selection	O
using	O
second	O
order	O
information	O
for	O
training	B
support	O
vector	O
machines	O
.	O
journal	O
of	O
machine	O
learning	B
research	O
,	O
6:1889–1918	O
,	O
2005	O
.	O
[	O
93	O
]	O
p.	O
fearnhead	O
.	O
exact	O
and	O
eﬃcient	B
bayesian	O
inference	B
for	O
multiple	O
changepoint	O
problems	O
.	O
technical	O
report	O
,	O
deptartment	O
of	O
mathematics	O
and	O
statistics	O
,	O
lancaster	O
university	O
,	O
2003	O
.	O
[	O
94	O
]	O
g.	O
h.	O
fischer	O
and	O
i.	O
w.	O
molenaar	O
.	O
rasch	O
models	O
:	O
foundations	O
,	O
recent	O
developments	O
,	O
and	O
applications	O
.	O
springer	O
,	O
new	O
york	O
,	O
1995	O
.	O
[	O
95	O
]	O
m.	O
e.	O
fisher	O
.	O
statistical	O
mechanics	O
of	O
dimers	O
on	O
a	O
plane	O
lattice	O
.	O
physical	O
review	O
,	O
124:1664–1672	O
,	O
1961	O
.	O
[	O
96	O
]	O
b.	O
frey	O
.	O
extending	O
factor	B
graphs	O
as	O
to	O
unify	O
directed	B
and	O
undirected	B
graphical	O
models	O
.	O
in	O
c.	O
meek	O
and	O
u.	O
kjærulﬀ	O
,	O
editors	O
,	O
uncertainty	B
in	O
artiﬁcial	O
intelligence	O
,	O
number	O
19	O
,	O
pages	O
257–264	O
.	O
morgan	O
kaufmann	O
,	O
2003	O
.	O
[	O
97	O
]	O
n.	O
friedman	O
,	O
d.	O
geiger	O
,	O
and	O
m.	O
goldszmidt	O
.	O
bayesian	O
network	O
classiﬁers	O
.	O
machine	O
learning	B
,	O
29:131–163	O
,	O
1997	O
.	O
[	O
98	O
]	O
s.	O
fr¨uhwirth-schnatter	O
.	O
finite	O
mixture	B
and	O
markov	O
switching	B
models	O
.	O
springer	O
,	O
2006	O
.	O
[	O
99	O
]	O
m.	O
frydenberg	O
.	O
the	O
chain	B
graph	I
markov	O
property	O
.	O
scandanavian	O
journal	O
of	O
statistics	O
,	O
17:333–353	O
,	O
1990	O
.	O
[	O
100	O
]	O
t.	O
furmston	O
and	O
d.	O
barber	O
.	O
solving	B
deterministic	O
policy	B
(	O
po	O
)	O
mdps	O
using	O
expectation-maximisation	O
and	O
antifreeze	B
.	O
in	O
e.	O
suzuki	O
and	O
m.	O
sebag	O
,	O
editors	O
,	O
european	O
conference	O
on	O
machine	O
learning	B
and	O
principles	O
and	O
practice	O
of	O
knowledge	O
discovery	O
in	O
databases	O
,	O
september	O
2009.	O
workshop	O
on	O
learning	B
and	O
data	B
mining	O
for	O
robotics	O
.	O
[	O
101	O
]	O
a.	O
galka	O
,	O
o.	O
yamashita	O
,	O
t.	O
ozaki	O
,	O
r.	O
biscay	O
,	O
and	O
p.	O
valdes-sosa	O
.	O
a	O
solution	O
to	O
the	O
dynamical	O
inverse	O
problem	B
of	O
eeg	O
generation	O
using	O
spatiotemporal	O
kalman	O
ﬁltering	B
.	O
neuroimage	O
,	O
(	O
23	O
)	O
:435–453	O
,	O
2004	O
.	O
[	O
102	O
]	O
p.	O
gandhi	O
,	O
f.	O
bromberg	O
,	O
and	O
d.	O
margaritis	O
.	O
learning	B
markov	O
network	O
structure	O
using	O
few	O
independence	B
tests	O
.	O
in	O
proceedings	O
of	O
the	O
siam	O
international	O
conference	O
on	O
data	B
mining	O
,	O
pages	O
680–691	O
,	O
2008	O
.	O
[	O
103	O
]	O
m.	O
r.	O
garey	O
and	O
d.	O
s.	O
johnson	O
.	O
computers	O
and	O
intractability	O
,	O
a	O
guide	O
to	O
the	O
theory	O
of	O
np-completeness	O
.	O
w.h	O
.	O
freeman	O
and	O
company	O
,	O
new	O
york	O
,	O
1979	O
.	O
[	O
104	O
]	O
a.	O
gelb	O
.	O
applied	O
optimal	O
estimation	O
.	O
mit	O
press	O
,	O
1974	O
.	O
[	O
105	O
]	O
a.	O
gelman	O
,	O
g.	O
o.	O
roberts	O
,	O
and	O
w.	O
r.	O
gilks	O
.	O
eﬃcient	B
metropolis	O
jumping	O
rules	O
.	O
in	O
j.	O
o.	O
bernardo	O
,	O
j.	O
m.	O
berger	O
,	O
a.	O
p.	O
dawid	O
,	O
and	O
a.	O
f.	O
m.	O
smith	O
,	O
editors	O
,	O
bayesian	O
statistics	O
,	O
volume	O
5	O
,	O
pages	O
599–607	O
.	O
oxford	O
university	O
press	O
,	O
1996	O
.	O
[	O
106	O
]	O
s.	O
geman	O
and	O
d.	O
geman	O
.	O
stochastic	O
relaxation	O
,	O
gibbs	O
distributions	O
,	O
and	O
the	O
bayesian	O
restoration	O
of	O
images	O
.	O
in	O
readings	O
in	O
uncertain	B
reasoning	O
,	O
pages	O
452–472	O
,	O
san	O
francisco	O
,	O
ca	O
,	O
usa	O
,	O
1990.	O
morgan	O
kaufmann	O
publishers	O
inc.	O
[	O
107	O
]	O
m.	O
g.	O
genton	O
.	O
classes	O
of	O
kernels	O
for	O
machine	O
learning	B
:	O
a	O
statistics	O
perspective	O
.	O
journal	O
of	O
machine	O
learning	B
research	O
,	O
2:299–312	O
,	O
2001	O
.	O
[	O
108	O
]	O
w.	O
gerstner	O
and	O
w.	O
m.	O
kistler	O
.	O
spiking	O
neuron	O
models	O
.	O
cambridge	O
university	O
press	O
,	O
2002.	O
draft	O
march	O
9	O
,	O
2010	O
569	O
bibliography	O
bibliography	O
[	O
109	O
]	O
z.	O
ghahramani	O
and	O
m.	O
j.	O
beal	O
.	O
variational	B
inference	I
for	O
bayesian	O
mixtures	O
of	O
factor	B
analysers	O
.	O
in	O
s.	O
a.	O
solla	O
,	O
t.	O
k.	O
leen	O
,	O
and	O
k-r.	O
m¨uller	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
number	O
12	O
,	O
pages	O
449–455	O
,	O
cambridge	O
,	O
ma	O
,	O
2000.	O
mit	O
press	O
.	O
[	O
110	O
]	O
z.	O
ghahramani	O
and	O
g.	O
e.	O
hinton	O
.	O
variational	O
learning	O
for	O
switching	B
state-space	O
models	O
.	O
neural	B
computation	I
,	O
12	O
(	O
4	O
)	O
:963–996	O
,	O
1998	O
.	O
[	O
111	O
]	O
a.	O
gibbons	O
.	O
algorithmic	O
graph	B
theory	O
.	O
cambridge	O
university	O
press	O
,	O
1991	O
.	O
[	O
112	O
]	O
w.	O
r.	O
gilks	O
,	O
s.	O
richardson	O
,	O
and	O
d.	O
j.	O
spiegelhalter	O
.	O
markov	O
chain	B
monte	O
carlo	O
in	O
practice	O
.	O
chapman	O
&	O
hall	O
,	O
1996	O
.	O
[	O
113	O
]	O
m.	O
girolami	O
and	O
a.	O
kaban	O
.	O
on	O
an	O
equivalence	O
between	O
plsi	O
and	O
lda	O
.	O
in	O
proceedings	O
of	O
the	O
26th	O
annual	O
international	O
acm	O
sigir	O
conference	O
on	O
research	O
and	O
development	O
in	O
information	B
retrieval	I
,	O
pages	O
433–434	O
,	O
new	O
york	O
,	O
ny	O
,	O
usa	O
,	O
2003.	O
acm	O
press	O
.	O
[	O
114	O
]	O
m.	O
e.	O
glickman	O
.	O
parameter	B
estimation	O
in	O
large	O
dynamic	B
paired	O
comparison	O
experiments	O
.	O
applied	O
statistics	O
,	O
48:377–394	O
,	O
1999	O
.	O
[	O
115	O
]	O
a.	O
globerson	O
and	O
t.	O
jaakkola	O
.	O
approximate	B
inference	I
using	O
planar	O
graph	B
decomposition	O
.	O
in	O
b.	O
sch¨olkopf	O
,	O
j.	O
platt	O
,	O
and	O
t.	O
hoﬀman	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
number	O
19	O
,	O
pages	O
473–480	O
,	O
cambridge	O
,	O
ma	O
,	O
2007.	O
mit	O
press	O
.	O
[	O
116	O
]	O
d.	O
goldberg	O
,	O
d.	O
nichols	O
,	O
b.	O
m.	O
oki	O
,	O
and	O
d.	O
terry	O
.	O
using	O
collaborative	B
ﬁltering	I
to	O
weave	O
an	O
information	O
tapestry	O
.	O
communications	O
acm	O
,	O
35:61–70	O
,	O
1992	O
.	O
[	O
117	O
]	O
g.	O
h.	O
golub	O
and	O
c.	O
f.	O
van	O
loan	O
.	O
matrix	B
computations	O
.	O
johns	O
hopkins	O
university	O
press	O
,	O
3rd	O
edition	O
,	O
1996	O
.	O
[	O
118	O
]	O
m.	O
c.	O
golumbic	O
and	O
i.	O
ben-arroyo	O
hartman	O
.	O
graph	B
theory	O
,	O
combinatorics	O
,	O
and	O
algorithms	O
.	O
springer-verlag	O
,	O
2005	O
.	O
[	O
119	O
]	O
c.	O
goutis	O
.	O
a	O
graphical	O
method	O
for	O
solving	B
a	O
decision	O
analysis	O
problem	B
.	O
ieee	O
transactions	O
on	O
systems	O
,	O
man	O
and	O
cybernetics	O
,	O
25:1181–1193	O
,	O
1995	O
.	O
[	O
120	O
]	O
p.	O
j.	O
green	O
and	O
b.	O
w.	O
silverman	O
.	O
nonparametric	O
regression	B
and	O
generalized	O
linear	B
models	O
,	O
volume	O
58	O
of	O
monographs	O
on	O
statistics	O
and	O
applied	O
probability	B
.	O
chapman	O
and	O
hall	O
,	O
1994	O
.	O
[	O
121	O
]	O
d.	O
m.	O
greig	O
,	O
b.	O
t.	O
porteous	O
,	O
and	O
a.	O
h.	O
seheult	O
.	O
exact	O
maximum	O
a	O
posteriori	O
estimation	O
for	O
binary	O
images	O
.	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
,	O
series	O
b	O
,	O
2:271–279	O
,	O
1989	O
.	O
[	O
122	O
]	O
g.	O
grimmett	O
and	O
d.	O
stirzaker	O
.	O
probability	B
and	O
random	O
processes	O
.	O
oxford	O
university	O
press	O
,	O
second	O
edition	O
,	O
1992	O
.	O
[	O
123	O
]	O
s.	O
f.	O
gull	O
.	O
bayesian	O
data	B
analysis	O
:	O
straight-line	O
ﬁtting	O
.	O
in	O
j.	O
skilling	O
,	O
editor	O
,	O
maximum	O
entropy	O
and	O
bayesian	O
methods	O
(	O
cambridge	O
1988	O
)	O
,	O
pages	O
511–518	O
.	O
kluwer	O
,	O
1989	O
.	O
[	O
124	O
]	O
a.	O
k.	O
gupta	O
and	O
d.	O
k.	O
nagar	O
.	O
matrix	B
variate	O
distributions	O
.	O
chapman	O
and	O
hall/crc	O
,	O
boca	O
raton	O
,	O
florida	O
usa	O
,	O
1999	O
.	O
[	O
125	O
]	O
d.	O
j.	O
hand	O
and	O
k.	O
yu	O
.	O
idiot	O
’	O
s	O
bayes—not	O
so	O
stupid	O
after	O
all	O
?	O
international	O
statistical	O
review	O
,	O
69	O
(	O
3	O
)	O
:385–398	O
,	O
2001	O
.	O
[	O
126	O
]	O
d.	O
r.	O
hardoon	O
,	O
s.	O
szedmak	O
,	O
and	O
j.	O
shawe-taylor	O
.	O
canonical	B
correlation	I
analysis	I
:	O
an	O
overview	O
with	O
appli-	O
cation	O
to	O
learning	B
methods	O
.	O
neural	B
computation	I
,	O
16	O
(	O
12	O
)	O
:2639–2664	O
,	O
2004	O
.	O
[	O
127	O
]	O
d.	O
o.	O
hebb	O
.	O
the	O
organization	O
of	O
behavior	O
.	O
wiley	O
,	O
new	O
york	O
,	O
1949	O
.	O
[	O
128	O
]	O
d.	O
heckerman	O
.	O
a	O
tutorial	O
on	O
learning	B
with	O
bayesian	O
networks	O
.	O
technical	O
report	O
msr-tr-95-06	O
,	O
microsoft	O
research	O
,	O
redmond	O
,	O
wa	O
,	O
march	O
1996.	O
revised	O
november	O
1996	O
.	O
[	O
129	O
]	O
d.	O
heckerman	O
,	O
d.	O
geiger	O
,	O
and	O
d.	O
chickering	O
.	O
learning	B
bayesian	O
networks	O
:	O
the	O
combination	O
of	O
knowledge	O
and	O
statistical	O
data	B
.	O
machine	O
learning	B
,	O
20	O
(	O
3	O
)	O
:197–243	O
,	O
1995	O
.	O
[	O
130	O
]	O
r.	O
herbrich	O
,	O
t.	O
minka	O
,	O
and	O
t.	O
graepel	O
.	O
trueskilltm	O
:	O
a	O
bayesian	O
skill	O
rating	O
system	B
.	O
in	O
b.	O
sch¨olkopf	O
,	O
j.	O
platt	O
,	O
and	O
t.	O
hoﬀman	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
number	O
19	O
,	O
pages	O
569–576	O
,	O
cambridge	O
,	O
ma	O
,	O
2007.	O
mit	O
press	O
.	O
[	O
131	O
]	O
h.	O
hermansky	O
.	O
should	O
recognizers	O
have	O
ears	O
?	O
speech	O
communication	O
,	O
25:3–27	O
,	O
1998	O
.	O
570	O
draft	O
march	O
9	O
,	O
2010	O
bibliography	O
bibliography	O
[	O
132	O
]	O
j.	O
hertz	O
,	O
a.	O
krogh	O
,	O
and	O
r.	O
palmer	O
.	O
introduction	O
to	O
the	O
theory	O
of	O
neural	B
computation	I
.	O
addison-wesley	O
,	O
1991	O
.	O
[	O
133	O
]	O
t.	O
heskes	O
.	O
convexity	O
arguments	O
for	O
eﬃcient	B
minimization	O
of	O
the	O
bethe	O
and	O
kikuchi	O
free	O
energies	O
.	O
journal	O
of	O
artiﬁcial	O
intelligence	O
research	O
,	O
26:153–190	O
,	O
2006	O
.	O
[	O
134	O
]	O
d.	O
m.	O
higdon	O
.	O
auxiliary	B
variable	I
methods	O
for	O
markov	O
chain	B
monte	O
carlo	O
with	O
applications	O
.	O
journal	O
of	O
the	O
american	O
statistical	O
association	O
,	O
93	O
(	O
442	O
)	O
:585–595	O
,	O
1998	O
.	O
[	O
135	O
]	O
g.	O
e.	O
hinton	O
and	O
r.	O
r.	O
salakhutdinov	O
.	O
reducing	O
the	O
dimensionality	O
of	O
data	B
with	O
neural	O
networks	O
.	O
science	O
,	O
(	O
313	O
)	O
:504–507	O
,	O
2006	O
.	O
[	O
136	O
]	O
t.	O
hofmann	O
,	O
j.	O
puzicha	O
,	O
and	O
m.	O
i.	O
jordan	O
.	O
learning	B
from	O
dyadic	B
data	I
.	O
in	O
m.	O
s.	O
kearns	O
,	O
s.	O
a.	O
solla	O
,	O
and	O
d.	O
a.	O
cohn	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
pages	O
466–472	O
,	O
cambridge	O
,	O
ma	O
,	O
1999.	O
mit	O
press	O
.	O
[	O
137	O
]	O
r.	O
a.	O
howard	O
and	O
j.	O
e.	O
matheson	O
.	O
inﬂuence	B
diagrams	I
.	O
decision	O
analysis	O
,	O
2	O
(	O
3	O
)	O
,	O
2005.	O
republished	O
version	O
of	O
the	O
original	O
1981	O
report	O
.	O
[	O
138	O
]	O
a.	O
hyv¨arinen	O
,	O
j.	O
karhunen	O
,	O
and	O
e.	O
oja	O
.	O
independent	O
component	O
analysis	B
.	O
wiley	O
,	O
2001	O
.	O
[	O
139	O
]	O
aapo	O
hyv¨arinen	O
.	O
consistency	O
of	O
pseudolikelihood	O
estimation	O
of	O
fully	O
visible	B
boltzmann	O
machines	O
.	O
neural	B
computation	I
,	O
18	O
(	O
10	O
)	O
:2283–2292	O
,	O
2006	O
.	O
[	O
140	O
]	O
m.	O
isard	O
and	O
a.	O
blake	O
.	O
condensation	O
conditional	B
density	O
propagation	B
for	O
visual	O
tracking	O
.	O
international	O
journal	O
of	O
computer	O
vision	O
,	O
29:5–28	O
,	O
1998	O
.	O
[	O
141	O
]	O
t.	O
s.	O
jaakkola	O
and	O
m.	O
i.	O
jordan	O
.	O
variational	O
probabilistic	O
inference	B
and	O
the	O
qmr-dt	O
network	O
.	O
journal	O
of	O
artiﬁcial	O
intelligence	O
research	O
,	O
10:291–322	O
,	O
1999	O
.	O
[	O
142	O
]	O
t.	O
s.	O
jaakkola	O
and	O
m.	O
i.	O
jordan	O
.	O
bayesian	O
parameter	B
estimation	O
via	O
variational	O
methods	O
.	O
statistics	O
and	O
computing	O
,	O
10	O
(	O
1	O
)	O
:25–37	O
,	O
2000	O
.	O
[	O
143	O
]	O
r.	O
a.	O
jacobs	O
,	O
f.	O
peng	O
,	O
and	O
m.	O
a.	O
tanner	O
.	O
a	O
bayesian	O
approach	B
to	O
model	B
selection	I
in	O
hierarchical	O
mixtures-	O
of-experts	O
architectures	O
.	O
neural	O
networks	O
,	O
10	O
(	O
2	O
)	O
:231–241	O
,	O
1997	O
.	O
[	O
144	O
]	O
r.	O
g.	O
jarrett	O
.	O
a	O
note	O
on	O
the	O
intervals	O
between	O
coal-mining	O
disasters	O
.	O
biometrika	O
,	O
(	O
66	O
)	O
:191–193	O
,	O
1979	O
.	O
[	O
145	O
]	O
e.	O
t.	O
jaynes	O
.	O
probability	B
theory	O
:	O
the	O
logic	B
of	O
science	O
.	O
cambridge	O
university	O
press	O
,	O
2003	O
.	O
[	O
146	O
]	O
f.	O
jensen	O
,	O
f.	O
v.	O
jensen	O
,	O
and	O
d.	O
dittmer	O
.	O
from	O
inﬂuence	B
diagrams	I
to	O
junction	O
trees	O
.	O
in	O
proceedings	O
of	O
the	O
10th	O
annual	O
conference	O
on	O
uncertainty	B
in	O
artiﬁcial	O
intelligence	O
(	O
uai-94	O
)	O
,	O
pages	O
367–373	O
,	O
san	O
francisco	O
,	O
ca	O
,	O
1994.	O
morgan	O
kaufmann	O
.	O
[	O
147	O
]	O
f.	O
v.	O
jensen	O
and	O
f.	O
jensen	O
.	O
optimal	O
junction	O
trees	O
.	O
in	O
r.	O
lopez	O
de	O
mantaras	O
and	O
d.	O
poole	O
,	O
editors	O
,	O
uncertainty	B
in	O
artiﬁcial	O
intelligence	O
,	O
number	O
10	O
,	O
pages	O
360–366	O
,	O
san	O
francisco	O
,	O
ca	O
,	O
1994.	O
morgan	O
kaufmann	O
.	O
[	O
148	O
]	O
f.	O
v.	O
jensen	O
and	O
t.	O
d.	O
nielson	O
.	O
bayesian	O
networks	O
and	O
decision	O
graphs	O
.	O
springer	O
verlag	O
,	O
second	O
edition	O
,	O
2007	O
.	O
[	O
149	O
]	O
m.	O
i.	O
jordan	O
and	O
r.	O
a.	O
jacobs	O
.	O
hierarchical	O
mixtures	O
of	O
experts	O
and	O
the	O
em	O
algorithm	B
.	O
neural	B
computation	I
,	O
6:181–214	O
,	O
1994	O
.	O
[	O
150	O
]	O
b.	O
h.	O
juang	O
,	O
w.	O
chou	O
,	O
and	O
c.	O
h.	O
lee	O
.	O
minimum	O
classiﬁcation	O
error	O
rate	O
methods	O
for	O
speech	B
recognition	I
.	O
ieee	O
transactions	O
on	O
speech	O
and	O
audio	O
processing	O
,	O
5:257–265	O
,	O
1997	O
.	O
[	O
151	O
]	O
l.	O
p.	O
kaelbling	O
,	O
m.	O
l.	O
littman	O
,	O
and	O
a.	O
r.	O
cassandra	O
.	O
planning	B
and	O
acting	O
in	O
partially	B
observable	I
stochastic	O
domains	O
.	O
artiﬁcial	O
intelligence	O
,	O
101	O
(	O
1-2	O
)	O
:99–134	O
,	O
1998	O
.	O
[	O
152	O
]	O
h.	O
j.	O
kappen	O
.	O
an	O
introduction	O
to	O
stochastic	O
control	O
theory	O
,	O
path	B
integrals	O
and	O
reinforcement	B
learning	I
.	O
in	O
proceedings	O
9th	O
granada	O
seminar	O
on	O
computational	O
physics	O
:	O
computational	O
and	O
mathematical	O
modeling	O
of	O
cooperative	O
behavior	O
in	O
neural	O
systems	O
,	O
volume	O
887	O
,	O
pages	O
149–181	O
.	O
american	O
institute	O
of	O
physics	O
,	O
2007	O
.	O
[	O
153	O
]	O
h.	O
j.	O
kappen	O
and	O
f.	O
b.	O
rodr´ıguez	O
.	O
eﬃcient	B
learning	O
in	O
boltzmann	O
machines	O
using	O
linear	B
response	O
theory	O
.	O
neural	O
compution	O
,	O
10	O
(	O
5	O
)	O
:1137–1156	O
,	O
1998	O
.	O
[	O
154	O
]	O
h.	O
j.	O
kappen	O
and	O
w.	O
wiegerinck	O
.	O
novel	O
iteration	B
schemes	O
for	O
the	O
cluster	O
variation	O
method	O
.	O
in	O
t.	O
g.	O
dietterich	O
,	O
s.	O
becker	O
,	O
and	O
z.	O
ghahramani	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
number	O
14	O
,	O
pages	O
415–422	O
,	O
cambridge	O
,	O
ma	O
,	O
2002.	O
mit	O
press	O
.	O
draft	O
march	O
9	O
,	O
2010	O
571	O
bibliography	O
bibliography	O
[	O
155	O
]	O
y.	O
karklin	O
and	O
m.	O
s.	O
lewicki	O
.	O
emergence	O
of	O
complex	O
cell	O
properties	B
by	O
learning	B
to	O
generalize	O
in	O
natural	B
scenes	O
.	O
nature	O
,	O
(	O
457	O
)	O
:83–86	O
,	O
november	O
2008	O
.	O
[	O
156	O
]	O
g.	O
karypis	O
and	O
v.	O
kumar	O
.	O
a	O
fast	O
and	O
high	O
quality	O
multilevel	O
scheme	O
for	O
partitioning	O
irregular	O
graphs	O
.	O
siam	O
journal	O
on	O
scientiﬁc	O
computing	O
,	O
20	O
(	O
1	O
)	O
:359–392	O
,	O
1998	O
.	O
[	O
157	O
]	O
p.	O
w.	O
kasteleyn	O
.	O
dimer	O
statistics	O
and	O
phase	O
transitions	O
.	O
journal	O
of	O
mathematical	O
physics	O
,	O
4	O
(	O
2	O
)	O
:287–293	O
,	O
1963	O
.	O
[	O
158	O
]	O
s.	O
a.	O
kauﬀman	O
.	O
at	O
home	O
in	O
the	O
universe	O
:	O
the	O
search	O
for	O
laws	O
of	O
self-organization	O
and	O
complexity	O
.	O
oxford	O
university	O
press	O
,	O
oxford	O
,	O
uk	O
,	O
1995	O
.	O
[	O
159	O
]	O
c-j	O
.	O
kim	O
.	O
dynamic	B
linear	O
models	O
with	O
markov-switching	O
.	O
journal	O
of	O
econometrics	O
,	O
60:1–22	O
,	O
1994	O
.	O
[	O
160	O
]	O
c-j	O
.	O
kim	O
and	O
c.	O
r.	O
nelson	O
.	O
state-space	O
models	O
with	O
regime	O
switching	B
.	O
mit	O
press	O
,	O
1999	O
.	O
[	O
161	O
]	O
g.	O
kitagawa	O
.	O
the	O
two-filter	O
formula	O
for	O
smoothing	B
and	O
an	O
implementation	O
of	O
the	O
gaussian-sum	O
smoother	O
.	O
annals	O
of	O
the	O
institute	O
of	O
statistical	O
mathematics	O
,	O
46	O
(	O
4	O
)	O
:605–623	O
,	O
1994	O
.	O
[	O
162	O
]	O
u.	O
b.	O
kjaerulﬀ	O
and	O
a.	O
l.	O
madsen	O
.	O
bayesian	O
networks	O
and	O
inﬂuence	B
diagrams	I
:	O
a	O
guide	O
to	O
construction	B
and	O
analysis	B
.	O
springer	O
,	O
2008	O
.	O
[	O
163	O
]	O
a.	O
krogh	O
,	O
m.	O
brown	O
,	O
i.	O
mian	O
,	O
k.	O
sjolander	O
,	O
and	O
d.	O
haussler	O
.	O
hidden	B
markov	O
models	O
in	O
computational	O
biology	O
:	O
applications	O
to	O
protein	O
modeling	O
.	O
journal	O
of	O
molecular	O
biology	O
,	O
235:1501–1531	O
,	O
1994	O
.	O
[	O
164	O
]	O
s.	O
kullback	O
.	O
information	O
theory	O
and	O
statistics	O
.	O
dover	O
,	O
1968	O
.	O
[	O
165	O
]	O
k.	O
kurihara	O
,	O
m.	O
welling	O
,	O
and	O
y.	O
w.	O
teh	O
.	O
collapsed	O
variational	O
dirichlet	O
process	O
mixture	B
models	O
.	O
in	O
proceedings	O
of	O
the	O
international	O
joint	B
conference	O
on	O
artiﬁcial	O
intelligence	O
,	O
volume	O
20	O
,	O
pages	O
2796–2801	O
,	O
2007	O
.	O
[	O
166	O
]	O
j.	O
laﬀerty	O
,	O
a.	O
mccallum	O
,	O
and	O
f.	O
pereira	O
.	O
conditional	O
random	O
ﬁelds	O
:	O
probabilistic	B
models	O
for	O
segmenting	O
and	O
labeling	O
sequence	O
data	O
.	O
in	O
c.	O
e.	O
brodley	O
and	O
a.	O
p.	O
danyluk	O
,	O
editors	O
,	O
international	O
conference	O
on	O
machine	O
learning	B
,	O
number	O
18	O
,	O
pages	O
282–289	O
,	O
san	O
francisco	O
,	O
ca	O
,	O
2001.	O
morgan	O
kaufmann	O
.	O
[	O
167	O
]	O
h.	O
lass	O
.	O
elements	O
of	O
pure	O
and	O
applied	O
mathematics	O
.	O
mcgraw-hill	O
(	O
reprinted	O
by	O
dover	O
)	O
,	O
1957	O
.	O
[	O
168	O
]	O
s.	O
l.	O
lauritzen	O
.	O
graphical	O
models	O
.	O
oxford	O
university	O
press	O
,	O
1996	O
.	O
[	O
169	O
]	O
s.	O
l.	O
lauritzen	O
,	O
a.	O
p.	O
dawid	O
,	O
b.	O
n.	O
larsen	O
,	O
and	O
h-g.	O
leimer	O
.	O
independence	B
properties	O
of	O
directed	B
markov	O
ﬁelds	O
.	O
networks	O
,	O
20:491–505	O
,	O
1990	O
.	O
[	O
170	O
]	O
s.	O
l.	O
lauritzen	O
and	O
d.	O
j.	O
spiegelhalter	O
.	O
local	B
computations	O
with	O
probabilities	O
on	O
graphical	O
structures	O
and	O
their	O
application	O
to	O
expert	O
systems	O
.	O
journal	O
of	O
royal	O
statistical	O
society	O
b	O
,	O
50	O
(	O
2	O
)	O
:157	O
–	O
224	O
,	O
1988	O
.	O
[	O
171	O
]	O
d.	O
d.	O
lee	O
and	O
h.	O
s.	O
seung	O
.	O
algorithms	O
for	O
non-negative	O
matrix	O
factorization	O
.	O
in	O
t.	O
k.	O
leen	O
,	O
t.	O
g.	O
dietterich	O
,	O
and	O
v.	O
tresp	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
number	O
13	O
,	O
pages	O
556–562	O
,	O
cambridge	O
,	O
ma	O
,	O
2001.	O
mit	O
press	O
.	O
[	O
172	O
]	O
m.	O
a.	O
r.	O
leisink	O
and	O
h.	O
j.	O
kappen	O
.	O
a	O
tighter	O
bound	B
for	O
graphical	O
models	O
.	O
in	O
neural	B
computation	I
,	O
volume	O
13	O
,	O
pages	O
2149–2171	O
.	O
mit	O
press	O
,	O
2001	O
.	O
[	O
173	O
]	O
v.	O
lepar	O
and	O
p.	O
p.	O
shenoy	O
.	O
a	O
comparison	O
of	O
lauritzen-spiegelhalter	O
,	O
hugin	O
,	O
and	O
shenoy-shafer	O
architectures	O
for	O
computing	O
marginals	O
of	O
probability	B
distributions	O
.	O
in	O
g.	O
cooper	O
and	O
s.	O
moral	O
,	O
editors	O
,	O
uncertainty	B
in	O
artiﬁcial	O
intelligence	O
,	O
number	O
14	O
,	O
pages	O
328–333	O
,	O
san	O
francisco	O
,	O
ca	O
,	O
1998.	O
morgan	O
kaufmann	O
.	O
[	O
174	O
]	O
u.	O
lerner	O
,	O
r.	O
parr	O
,	O
d.	O
koller	O
,	O
and	O
g.	O
biswas	O
.	O
bayesian	O
fault	O
detection	O
and	O
diagnosis	O
in	O
dynamic	B
systems	O
.	O
in	O
proceedings	O
of	O
the	O
seventeenth	O
national	O
conference	O
on	O
artiﬁcial	O
intelligence	O
(	O
aiii-00	O
)	O
,	O
pages	O
531–537	O
,	O
2000	O
.	O
[	O
175	O
]	O
u.	O
n.	O
lerner	O
.	O
hybrid	O
bayesian	O
networks	O
for	O
reasoning	O
about	O
complex	O
systems	O
.	O
computer	O
science	O
department	O
,	O
stanford	O
university	O
,	O
2002	O
.	O
[	O
176	O
]	O
r.	O
linsker	O
.	O
improved	O
local	B
learning	O
rule	O
for	O
information	O
maximization	O
and	O
related	O
applications	O
.	O
neural	O
networks	O
,	O
18	O
(	O
3	O
)	O
:261–265	O
,	O
2005	O
.	O
[	O
177	O
]	O
y.	O
l.	O
loh	O
,	O
e.	O
w.	O
carlson	O
,	O
and	O
m.	O
y.	O
j.	O
tan	O
.	O
bond-propagation	O
algorithm	B
for	O
thermodynamic	O
functions	O
in	O
general	O
two-dimensional	O
ising	O
models	O
.	O
physical	O
review	O
b	O
,	O
76	O
(	O
1	O
)	O
:014404	O
,	O
2007	O
.	O
[	O
178	O
]	O
h.	O
lopes	O
and	O
m.	O
west	O
.	O
bayesian	O
model	B
assessment	O
in	O
factor	B
analysis	I
.	O
statistica	O
sinica	O
,	O
(	O
14	O
)	O
:41–67	O
,	O
2003	O
.	O
572	O
draft	O
march	O
9	O
,	O
2010	O
bibliography	O
bibliography	O
[	O
179	O
]	O
t.	O
j.	O
loredo	O
.	O
from	O
laplace	O
to	O
supernova	O
sn	O
1987a	O
:	O
bayesian	O
inference	B
in	O
astrophysics	O
.	O
in	O
p.f	O
.	O
fougere	O
,	O
editor	O
,	O
maximum	O
entropy	O
and	O
bayesian	O
methods	O
,	O
pages	O
81–142	O
.	O
kluwer	O
,	O
1990	O
.	O
[	O
180	O
]	O
d.	O
j.	O
c.	O
mackay	O
.	O
bayesian	O
interpolation	O
.	O
neural	B
computation	I
,	O
4	O
(	O
3	O
)	O
:415–447	O
,	O
1992	O
.	O
[	O
181	O
]	O
d.	O
j.	O
c.	O
mackay	O
.	O
probable	O
networks	O
and	O
plausisble	O
predictions	O
–	O
a	O
review	O
of	O
practical	O
bayesian	O
methods	O
for	O
supervised	B
neural	O
networks	O
.	O
network	O
:	O
computation	O
in	O
neural	O
systems	O
,	O
6	O
(	O
3	O
)	O
:469–505	O
,	O
1995	O
.	O
[	O
182	O
]	O
d.	O
j.	O
c.	O
mackay	O
.	O
introduction	O
to	O
gaussian	O
processes	O
.	O
in	O
neural	O
networks	O
and	O
machine	O
learning	B
,	O
volume	O
168	O
of	O
nato	O
advanced	O
study	O
institute	O
on	O
generalization	O
in	O
neural	O
networks	O
and	O
machine	O
learning	B
,	O
pages	O
133–165	O
.	O
springer	O
,	O
august	O
1998	O
.	O
[	O
183	O
]	O
d.	O
j.	O
c.	O
mackay	O
.	O
information	O
theory	O
,	O
inference	B
and	O
learning	B
algorithms	O
.	O
cambridge	O
university	O
press	O
,	O
2003	O
.	O
[	O
184	O
]	O
u.	O
madhow	O
.	O
fundamentals	O
of	O
digital	O
communication	O
.	O
cambridge	O
university	O
press	O
,	O
2008	O
.	O
[	O
185	O
]	O
k.	O
v.	O
mardia	O
,	O
j.	O
t.	O
kent	O
,	O
and	O
j.	O
m.	O
bibby	O
.	O
multivariate	B
analysis	O
.	O
academic	O
press	O
,	O
1997	O
.	O
[	O
186	O
]	O
h.	O
markram	O
,	O
j.	O
lubke	O
,	O
m.	O
frotscher	O
,	O
and	O
b.	O
sakmann	O
.	O
regulation	O
of	O
synaptic	O
eﬃcacy	O
by	O
coincidence	O
of	O
postsynaptic	O
aps	O
and	O
epsps	O
.	O
science	O
,	O
275:213–215	O
,	O
1997	O
.	O
[	O
187	O
]	O
g.	O
mclachlan	O
and	O
t.	O
krishnan	O
.	O
the	O
em	O
algorithm	B
and	O
extensions	O
.	O
john	O
wiley	O
and	O
sons	O
,	O
1997	O
.	O
[	O
188	O
]	O
g.	O
mclachlan	O
and	O
d.	O
peel	O
.	O
finite	O
mixture	B
models	O
.	O
wiley	O
series	O
in	O
probability	B
and	O
statistics	O
.	O
wiley-	O
interscience	O
,	O
2000	O
.	O
[	O
189	O
]	O
e.	O
meeds	O
,	O
z.	O
ghahramani	O
,	O
r.	O
m.	O
neal	O
,	O
and	O
s.	O
t.	O
roweis	O
.	O
modeling	O
dyadic	B
data	I
with	O
binary	O
latent	O
factors	O
.	O
in	O
b.	O
sch¨olkopf	O
,	O
j.	O
platt	O
,	O
and	O
t.	O
hoﬀman	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
volume	O
19	O
,	O
pages	O
977–984	O
,	O
cambridge	O
,	O
ma	O
,	O
2007.	O
mit	O
press	O
.	O
[	O
190	O
]	O
m.	O
meila	O
.	O
an	O
accelerated	O
chow	O
and	O
liu	O
algorithm	B
:	O
fitting	O
tree	B
distributions	O
to	O
high-dimensional	O
sparse	B
data	O
.	O
in	O
i.	O
bratko	O
,	O
editor	O
,	O
international	O
conference	O
on	O
machine	O
learning	B
,	O
pages	O
249–257	O
,	O
san	O
francisco	O
,	O
ca	O
,	O
1999.	O
morgan	O
kaufmann	O
.	O
[	O
191	O
]	O
m.	O
meila	O
and	O
m.	O
i.	O
jordan	O
.	O
triangulation	B
by	O
continuous	B
embedding	O
.	O
in	O
m.	O
c.	O
mozer	O
,	O
m.	O
i.	O
jordan	O
,	O
and	O
t.	O
petsche	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
number	O
9	O
,	O
pages	O
557–563	O
,	O
cambridge	O
,	O
ma	O
,	O
1997.	O
mit	O
press	O
.	O
[	O
192	O
]	O
b.	O
mesot	O
and	O
d.	O
barber	O
.	O
switching	O
linear	O
dynamical	O
systems	O
for	O
noise	O
robust	O
speech	O
recognition	O
.	O
ieee	O
transactions	O
of	O
audio	O
,	O
speech	O
and	O
language	O
processing	O
,	O
15	O
(	O
6	O
)	O
:1850–1858	O
,	O
2007	O
.	O
[	O
193	O
]	O
n.	O
meuleau	O
,	O
m.	O
hauskrecht	O
,	O
k-e.	O
kim	O
,	O
l.	O
peshkin	O
,	O
kaelbling	O
.	O
l.	O
p.	O
,	O
t.	O
dean	O
,	O
and	O
c.	O
boutilier	O
.	O
solving	B
very	O
large	O
weakly	O
coupled	B
markov	O
decision	O
processes	O
.	O
in	O
proceedings	O
of	O
the	O
fifteenth	O
national	O
conference	O
on	O
artiﬁcial	O
intelligence	O
,	O
pages	O
165–172	O
,	O
1998	O
.	O
[	O
194	O
]	O
t.	O
mills	O
.	O
the	O
econometric	O
modelling	B
of	O
financial	O
time	O
series	O
.	O
cambridge	O
university	O
press	O
,	O
2000	O
.	O
[	O
195	O
]	O
t.	O
minka	O
.	O
expectation	B
propagation	I
for	O
approximate	B
bayesian	O
inference	B
.	O
in	O
j.	O
breese	O
and	O
d.	O
koller	O
,	O
editors	O
,	O
uncertainty	B
in	O
artiﬁcial	O
intelligence	O
,	O
number	O
17	O
,	O
pages	O
362–369	O
,	O
san	O
francisco	O
,	O
ca	O
,	O
2001.	O
morgan	O
kaufmann	O
.	O
[	O
196	O
]	O
t.	O
minka	O
.	O
a	O
comparison	O
of	O
numerical	B
optimizers	O
for	O
logistic	B
regression	I
.	O
technical	O
report	O
,	O
microsoft	O
research	O
,	O
2003.	O
research.microsoft.com/∼minka/papers/logreg	O
.	O
[	O
197	O
]	O
t.	O
minka	O
.	O
divergence	B
measures	O
and	O
message	B
passing	I
.	O
technical	O
report	O
msr-tr-2005-173	O
,	O
microsoft	O
research	O
ltd.	O
,	O
cambridge	O
,	O
uk	O
,	O
december	O
2005	O
.	O
[	O
198	O
]	O
a.	O
mira	O
,	O
j.	O
møller	O
,	O
and	O
g.	O
o.	O
roberts	O
.	O
perfect	O
slice	O
samplers	O
.	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
,	O
63	O
(	O
3	O
)	O
:593–606	O
,	O
2001.	O
series	O
b	O
(	O
statistical	O
methodology	O
)	O
.	O
[	O
199	O
]	O
c.	O
mitchell	O
,	O
m.	O
harper	O
,	O
and	O
l.	O
jamieson	O
.	O
on	O
the	O
complexity	O
of	O
explicit	O
duration	O
hmm	O
’	O
s	O
.	O
speech	O
and	O
audio	O
processing	O
,	O
ieee	O
transactions	O
on	O
,	O
3	O
(	O
3	O
)	O
:213–217	O
,	O
may	O
1995	O
.	O
[	O
200	O
]	O
t.	O
mitchell	O
.	O
machine	O
learning	B
.	O
mcgraw-hill	O
,	O
1997	O
.	O
[	O
201	O
]	O
j.	O
mooij	O
and	O
h.	O
j.	O
kappen	O
.	O
suﬃcient	O
conditions	O
for	O
convergence	O
of	O
loopy	B
belief	O
propagation	B
.	O
ieee	O
infor-	O
mation	O
theory	O
,	O
53:4422–4437	O
,	O
2007	O
.	O
[	O
202	O
]	O
a.	O
moore	O
.	O
a	O
tutorial	O
http	O
:	O
//www.cs.cmu.edu/∼awm/papers.html	O
.	O
on	O
kd-trees	O
.	O
technical	O
report	O
,	O
1991.	O
available	O
from	O
draft	O
march	O
9	O
,	O
2010	O
573	O
bibliography	O
bibliography	O
[	O
203	O
]	O
j.	O
moussouris	O
.	O
gibbs	O
and	O
markov	O
random	O
systems	O
with	O
constraints	O
.	O
journal	O
of	O
statistical	O
physics	O
,	O
10:11–33	O
,	O
1974	O
.	O
[	O
204	O
]	O
r.	O
m.	O
neal	O
.	O
connectionist	O
learning	B
of	O
belief	B
networks	I
.	O
artiﬁcial	O
intelligence	O
,	O
56:71–113	O
,	O
1992	O
.	O
[	O
205	O
]	O
r.	O
m.	O
neal	O
.	O
probabilistic	B
inference	O
using	O
markov	O
chain	B
monte	O
carlo	O
methods	O
.	O
crg-tr-93-1	O
,	O
dept	O
.	O
of	O
computer	O
science	O
,	O
university	O
of	O
toronto	O
,	O
1993	O
.	O
[	O
206	O
]	O
r.	O
m.	O
neal	O
.	O
markov	O
chain	B
sampling	O
methods	O
for	O
dirichlet	O
process	O
mixture	B
models	O
.	O
journal	O
of	O
computational	O
and	O
graphical	O
statistics	O
,	O
9	O
(	O
2	O
)	O
:249–265	O
,	O
2000	O
.	O
[	O
207	O
]	O
r.	O
m.	O
neal	O
.	O
slice	B
sampling	I
.	O
annals	O
of	O
statistics	O
,	O
31:705–767	O
,	O
2003	O
.	O
[	O
208	O
]	O
r.	O
e.	O
neapolitan	O
.	O
learning	B
bayesian	O
networks	O
.	O
prentice	O
hall	O
,	O
2003	O
.	O
[	O
209	O
]	O
a.	O
v.	O
neﬁan	O
,	O
luhong	O
l.	O
,	O
xiaobo	O
p.	O
,	O
liu	O
x.	O
,	O
c.	O
mao	O
,	O
and	O
k.	O
murphy	O
.	O
a	O
coupled	B
hmm	O
for	O
audio-visual	O
speech	B
recognition	I
.	O
in	O
ieee	O
international	O
conference	O
on	O
acoustics	O
,	O
speech	O
,	O
and	O
signal	O
processing	O
,	O
volume	O
2	O
,	O
pages	O
2013–2016	O
,	O
2002	O
.	O
[	O
210	O
]	O
d.	O
nilsson	O
.	O
an	O
eﬃcient	B
algorithm	O
for	O
ﬁnding	O
the	O
m	O
most	O
probable	O
conﬁgurations	O
in	O
a	O
probabilistic	B
expert	O
system	B
.	O
statistics	O
and	O
computing	O
,	O
8:159–173	O
,	O
1998	O
.	O
[	O
211	O
]	O
d.	O
nilsson	O
and	O
j.	O
goldberger	O
.	O
sequentially	O
ﬁnnding	O
the	O
n-best	O
list	O
in	O
hidden	B
markov	O
models	O
.	O
internation	O
joint	B
conference	O
on	O
artiﬁcial	O
intelligence	O
(	O
ijcai	O
)	O
,	O
17	O
,	O
2001	O
.	O
[	O
212	O
]	O
a.	O
b.	O
novikoﬀ	O
.	O
on	O
convergence	O
proofs	O
on	O
perceptrons	O
.	O
in	O
symposium	O
on	O
the	O
mathematical	O
theory	O
of	O
automata	O
(	O
new	O
york	O
,	O
1962	O
)	O
,	O
volume	O
12	O
,	O
pages	O
615–622	O
,	O
brooklyn	O
,	O
n.y.	O
,	O
1963.	O
polytechnic	O
press	O
of	O
polytechnic	O
institute	O
of	O
brooklyn	O
.	O
[	O
213	O
]	O
f.	O
j.	O
och	O
and	O
h.	O
ney	O
.	O
discriminative	B
training	I
and	O
maximum	O
entropy	O
models	O
for	O
statistical	O
machine	O
transla-	O
tion	O
.	O
in	O
proceedings	O
of	O
the	O
annual	O
meeting	O
of	O
the	O
association	O
for	O
computational	O
linguistics	O
,	O
pages	O
295–302	O
,	O
philadelphia	O
,	O
july	O
2002	O
.	O
[	O
214	O
]	O
b.	O
a.	O
olshausen	O
and	O
d.	O
j.	O
field	O
.	O
sparse	B
coding	O
with	O
an	O
overcomplete	O
basis	O
set	O
:	O
a	O
strategy	O
employed	O
by	O
v1	O
?	O
vision	O
research	O
,	O
37:3311–3325	O
,	O
1998	O
.	O
[	O
215	O
]	O
a.	O
v.	O
oppenheim	O
,	O
r.	O
w.	O
shafer	O
,	O
m.	O
t.	O
yoder	O
,	O
and	O
w.	O
t.	O
padgett	O
.	O
discrete-time	O
signal	O
processing	O
.	O
prentice	O
hall	O
,	O
third	O
edition	O
,	O
2009	O
.	O
[	O
216	O
]	O
m.	O
ostendorf	O
,	O
v.	O
digalakis	O
,	O
and	O
o.	O
a.	O
kimball	O
.	O
from	O
hmms	O
to	O
segment	O
models	O
:	O
a	O
uniﬁed	O
view	O
of	O
stochastic	O
modeling	O
for	O
speech	B
recognition	I
.	O
ieee	O
transactions	O
on	O
speech	O
and	O
audio	O
processing	O
,	O
4:360–378	O
,	O
1995	O
.	O
[	O
217	O
]	O
p.	O
paatero	O
and	O
u.	O
tapper	O
.	O
positive	O
matrix	O
factorization	O
:	O
a	O
non-negative	O
factor	O
model	B
with	O
optimal	O
utilization	O
of	O
error	O
estimates	O
of	O
data	B
values	O
.	O
environmetrics	O
,	O
5:111–126	O
,	O
1994	O
.	O
[	O
218	O
]	O
v.	O
pavlovic	O
,	O
j.	O
m.	O
rehg	O
,	O
and	O
j.	O
maccormick	O
.	O
learning	B
switching	O
linear	B
models	O
of	O
human	O
motion	O
.	O
in	O
t.	O
k.	O
leen	O
,	O
t.	O
g.	O
dietterich	O
,	O
and	O
v.	O
tresp	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
number	O
13	O
,	O
pages	O
981–987	O
,	O
cambridge	O
,	O
ma	O
,	O
2001.	O
mit	O
press	O
.	O
[	O
219	O
]	O
j.	O
pearl	O
.	O
probabilistic	B
reasoning	O
in	O
intelligent	O
systems	O
:	O
networks	O
of	O
plausible	O
inference	B
.	O
morgan	O
kaufmann	O
,	O
1988	O
.	O
[	O
220	O
]	O
j.	O
pearl	O
.	O
causality	B
:	O
models	O
,	O
reasoning	O
and	O
inference	B
.	O
cambridge	O
university	O
press	O
,	O
2000	O
.	O
[	O
221	O
]	O
b.	O
a.	O
pearlmutter	O
and	O
l.	O
c.	O
parra	O
.	O
maximum	B
likelihood	I
blind	O
source	O
separation	B
:	O
a	O
context-sensitive	O
generalization	O
of	O
ica	O
.	O
in	O
m.	O
c.	O
mozer	O
,	O
m.	O
i.	O
jordan	O
,	O
and	O
t.	O
petsche	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
number	O
9	O
,	O
pages	O
613–619	O
,	O
cambridge	O
,	O
ma	O
,	O
1997.	O
mit	O
press	O
.	O
[	O
222	O
]	O
k.	O
b.	O
petersen	O
and	O
o.	O
winther	O
.	O
the	O
em	O
algorithm	B
in	O
independent	O
component	O
analysis	B
.	O
in	O
ieee	O
international	O
conference	O
on	O
acoustics	O
,	O
speech	O
,	O
and	O
signal	O
processing	O
,	O
volume	O
5	O
,	O
pages	O
169–172	O
,	O
2005	O
.	O
[	O
223	O
]	O
j-p.	O
pﬁster	O
,	O
t.	O
toyiozumi	O
,	O
d.	O
barber	O
,	O
and	O
w.	O
gerstner	O
.	O
optimal	O
spike-timing	O
dependent	O
plasticity	O
for	O
precise	O
action	O
potential	B
firing	O
in	O
supervised	B
learning	I
.	O
neural	B
computation	I
,	O
18:1309–1339	O
,	O
2006	O
.	O
[	O
224	O
]	O
j.	O
platt	O
.	O
fast	O
training	B
of	O
support	O
vector	O
machines	O
using	O
sequential	O
minimal	O
optimization	O
.	O
in	O
b.	O
sch¨olkopf	O
,	O
c.	O
j.	O
c.	O
burges	O
,	O
and	O
a.	O
j.	O
smola	O
,	O
editors	O
,	O
advances	O
in	O
kernel	B
methods	O
-	O
support	O
vector	O
learning	O
,	O
pages	O
185–208	O
.	O
mit	O
press	O
,	O
1999	O
.	O
574	O
draft	O
march	O
9	O
,	O
2010	O
bibliography	O
bibliography	O
[	O
225	O
]	O
i.	O
porteous	O
,	O
d.	O
newman	O
,	O
a.	O
ihler	O
,	O
a.	O
asuncion	O
,	O
p.	O
smyth	O
,	O
and	O
m.	O
welling	O
.	O
fast	O
collapsed	O
gibbs	O
sampling	B
for	O
latent	B
dirichlet	O
allocation	O
.	O
in	O
kdd	O
’	O
08	O
:	O
proceeding	O
of	O
the	O
14th	O
acm	O
sigkdd	O
international	O
conference	O
on	O
knowledge	O
discovery	O
and	O
data	B
mining	O
,	O
pages	O
569–577	O
,	O
new	O
york	O
,	O
ny	O
,	O
usa	O
,	O
2008.	O
acm	O
.	O
[	O
226	O
]	O
j.	O
e.	O
potter	O
and	O
r.	O
g.	O
stern	O
.	O
statistical	O
ﬁltering	B
of	O
space	O
navigation	O
measurements	O
.	O
in	O
american	O
institute	O
of	O
aeronautics	O
and	O
astronautics	O
guidance	O
and	O
control	O
conference	O
,	O
volume	O
13	O
,	O
pages	O
775–801	O
,	O
cambridge	O
,	O
mass.	O
,	O
august	O
1963	O
.	O
[	O
227	O
]	O
w.	O
press	O
,	O
w.	O
vettering	O
,	O
s.	O
teukolsky	O
,	O
and	O
b.	O
flannery	O
.	O
numerical	B
recipes	O
in	O
fortran	O
.	O
cambridge	O
university	O
press	O
,	O
1992	O
.	O
[	O
228	O
]	O
s.	O
j.	O
d.	O
prince	O
and	O
j.	O
h.	O
elder	O
.	O
probabilistic	B
linear	O
discriminant	O
analysis	B
for	O
inferences	O
about	O
identity	O
.	O
in	O
ieee	O
11th	O
international	O
conference	O
on	O
computer	O
vision	O
iccv	O
,	O
pages	O
1–8	O
,	O
2007	O
.	O
[	O
229	O
]	O
l.	O
r.	O
rabiner	O
.	O
a	O
tutorial	O
on	O
hidden	B
markov	O
models	O
and	O
selected	O
applications	O
in	O
speech	B
recognition	I
.	O
proc	O
.	O
of	O
the	O
ieee	O
,	O
77	O
(	O
2	O
)	O
:257–286	O
,	O
1989	O
.	O
[	O
230	O
]	O
c.	O
e.	O
rasmussen	O
and	O
c.	O
k.	O
i.	O
williams	O
.	O
gaussian	O
processes	O
for	O
machine	O
learning	B
.	O
mit	O
press	O
,	O
2006	O
.	O
[	O
231	O
]	O
h.	O
e.	O
rauch	O
,	O
g.	O
tung	O
,	O
and	O
c.	O
t.	O
striebel	O
.	O
maximum	B
likelihood	I
estimates	O
of	O
linear	B
dynamic	O
systems	O
.	O
american	O
institute	O
of	O
aeronautics	O
and	O
astronautics	O
journal	O
(	O
aiaaj	O
)	O
,	O
3	O
(	O
8	O
)	O
:1445–1450	O
,	O
1965	O
.	O
[	O
232	O
]	O
t.	O
richardson	O
and	O
p.	O
spirtes	O
.	O
ancestral	B
graph	O
markov	O
models	O
.	O
annals	O
of	O
statistics	O
,	O
30	O
(	O
4	O
)	O
:962–1030	O
,	O
2002	O
.	O
[	O
233	O
]	O
d.	O
rose	O
,	O
r.	O
e.	O
tarjan	O
,	O
and	O
e.	O
s.	O
lueker	O
.	O
algorithmic	O
aspects	O
of	O
vertex	B
elimination	O
of	O
graphs	O
.	O
siam	O
journal	O
on	O
computing	O
,	O
(	O
5	O
)	O
:266–283	O
,	O
1976	O
.	O
[	O
234	O
]	O
f.	O
rosenblatt	O
.	O
the	O
perceptron	B
:	O
a	O
probabilistic	B
model	O
for	O
information	O
storage	O
and	O
organization	O
in	O
the	O
brain	O
.	O
psychological	O
review	O
,	O
65	O
(	O
6	O
)	O
:386–408	O
,	O
1958	O
.	O
[	O
235	O
]	O
d.	O
b.	O
rubin	O
.	O
using	O
the	O
sir	O
algorithm	B
to	O
simulate	O
posterior	B
distributions	O
.	O
in	O
m.	O
h.	O
bernardo	O
,	O
k.	O
m.	O
degroot	O
,	O
d.	O
v.	O
lindley	O
,	O
and	O
a.	O
f.	O
m.	O
smith	O
,	O
editors	O
,	O
bayesian	O
statistics	O
3.	O
oxford	O
university	O
press	O
,	O
1988	O
.	O
[	O
236	O
]	O
d.	O
saad	O
and	O
m.	O
opper	O
.	O
advanced	O
mean	B
field	O
methods	O
theory	O
and	O
practice	O
.	O
mit	O
press	O
,	O
2001	O
.	O
[	O
237	O
]	O
r.	O
salakhutdinov	O
,	O
s.	O
roweis	O
,	O
and	O
z.	O
ghahramani	O
.	O
optimization	O
with	O
em	O
and	O
expectation-conjugate-	O
gradient	B
.	O
in	O
t.	O
fawcett	O
and	O
n.	O
mishra	O
,	O
editors	O
,	O
international	O
conference	O
on	O
machine	O
learning	B
,	O
number	O
20	O
,	O
pages	O
672–679	O
,	O
menlo	O
park	O
,	O
ca	O
,	O
2003.	O
aaai	O
press	O
.	O
[	O
238	O
]	O
l.	O
k.	O
saul	O
,	O
t.	O
s.	O
jaakkola	O
,	O
and	O
m.	O
j.	O
jordan	O
.	O
mean	B
ﬁeld	I
theory	I
for	O
sigmoid	O
belief	O
networks	O
.	O
journal	O
of	O
artiﬁcial	O
intelligence	O
research	O
,	O
4:61–76	O
,	O
1996	O
.	O
[	O
239	O
]	O
l.	O
k.	O
saul	O
and	O
m.	O
i.	O
jordan	O
.	O
exploiting	O
tractable	O
substructures	O
in	O
intractable	O
networks	O
.	O
in	O
d.	O
s.	O
touretzky	O
,	O
m.	O
mozer	O
,	O
and	O
m.	O
e.	O
hasselmo	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
number	O
8	O
,	O
pages	O
486–492	O
,	O
cambridge	O
,	O
ma	O
,	O
1996.	O
mit	O
press	O
.	O
[	O
240	O
]	O
l.	O
savage	O
.	O
the	O
foundations	O
of	O
statistics	O
.	O
wiley	O
,	O
1954	O
.	O
[	O
241	O
]	O
r.	O
d.	O
schachter	O
.	O
bayes-ball	O
:	O
the	O
rational	O
pastime	O
(	O
for	O
determining	O
irrelevance	O
and	O
requisite	O
information	O
in	O
g.	O
cooper	O
and	O
s.	O
moral	O
,	O
editors	O
,	O
uncertainty	B
in	O
artiﬁcial	O
in	O
belief	B
networks	I
and	O
inﬂuence	B
diagrams	I
)	O
.	O
intelligence	O
,	O
number	O
14	O
,	O
pages	O
480–487	O
,	O
san	O
francisco	O
,	O
ca	O
,	O
1998.	O
morgan	O
kaufmann	O
.	O
[	O
242	O
]	O
b.	O
sch¨olkopf	O
,	O
a.	O
smola	O
,	O
and	O
k.	O
r.	O
m¨uller	O
.	O
nonlinear	O
component	O
analysis	B
as	O
a	O
kernel	B
eigenvalue	O
problem	B
.	O
neural	B
computation	I
,	O
10:1299–1319	O
,	O
1998	O
.	O
[	O
243	O
]	O
n.	O
n.	O
schraudolph	O
and	O
d.	O
kamenetsky	O
.	O
eﬃcient	B
exact	O
inference	B
in	O
planar	O
ising	O
models	O
.	O
in	O
d.	O
koller	O
,	O
d.	O
schu-	O
urmans	O
,	O
y.	O
bengio	O
,	O
and	O
l.	O
bottou	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
number	O
21	O
,	O
pages	O
1417–1424	O
,	O
cambridge	O
,	O
ma	O
,	O
2009.	O
mit	O
press	O
.	O
[	O
244	O
]	O
e.	O
schwarz	O
.	O
estimating	O
the	O
dimension	O
of	O
a	O
model	B
.	O
annals	O
of	O
statistics	O
,	O
6	O
(	O
2	O
)	O
:461–464	O
,	O
1978	O
.	O
[	O
245	O
]	O
m.	O
seeger	O
.	O
gaussian	O
processes	O
for	O
machine	O
learning	B
.	O
international	O
journal	O
of	O
neural	O
systems	O
,	O
14	O
(	O
2	O
)	O
:69–106	O
,	O
2004	O
.	O
[	O
246	O
]	O
m.	O
seeger	O
.	O
expectation	B
propagation	I
for	O
exponential	B
families	O
.	O
technical	O
report	O
,	O
department	O
of	O
eecs	O
,	O
berkeley	O
,	O
2005.	O
www.kyb.tuebingen.mpg.de/bs/people/seeger	O
.	O
draft	O
march	O
9	O
,	O
2010	O
575	O
bibliography	O
bibliography	O
[	O
247	O
]	O
m.	O
seeger	O
and	O
h.	O
nickisch	O
.	O
large	O
scale	O
variational	B
inference	I
and	O
experimental	O
design	O
for	O
sparse	B
generalized	O
linear	B
models	O
.	O
technical	O
report	O
175	O
,	O
max	O
planck	O
institute	O
for	O
biological	O
cybernetics	O
,	O
september	O
2008	O
.	O
[	O
248	O
]	O
j.	O
shawe-taylor	O
and	O
n.	O
cristianini	O
.	O
kernel	B
methods	O
for	O
pattern	O
analysis	B
.	O
cambridge	O
university	O
press	O
,	O
2004	O
.	O
[	O
249	O
]	O
s.	O
siddiqi	O
,	O
b.	O
boots	O
,	O
and	O
g.	O
gordon	O
.	O
a	O
constraint	O
generation	O
approach	B
to	O
learning	B
stable	O
linear	O
dynamical	O
systems	O
.	O
in	O
j.	O
c.	O
platt	O
,	O
d.	O
koller	O
,	O
y.	O
singer	O
,	O
and	O
s.	O
roweis	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
number	O
20	O
,	O
pages	O
1329–1336	O
,	O
cambridge	O
,	O
ma	O
,	O
2008.	O
mit	O
press	O
.	O
[	O
250	O
]	O
t.	O
silander	O
,	O
p.	O
kontkanen	O
,	O
and	O
p.	O
myllym¨aki	O
.	O
on	O
sensitivity	O
of	O
the	O
map	B
bayesian	O
network	O
structure	O
to	O
the	O
equivalent	B
sample	O
size	O
parameter	B
.	O
in	O
r.	O
parr	O
and	O
l.	O
van	O
der	O
gaag	O
,	O
editors	O
,	O
uncertainty	B
in	O
artiﬁcial	O
intelligence	O
,	O
number	O
23	O
,	O
pages	O
360–367	O
,	O
corvallis	O
,	O
oregon	O
,	O
usa	O
,	O
2007.	O
auai	O
press	O
.	O
[	O
251	O
]	O
s.	O
s.	O
skiena	O
.	O
the	O
algorithm	B
design	O
manual	O
.	O
springer-verlag	O
,	O
new	O
york	O
,	O
usa	O
,	O
1998	O
.	O
[	O
252	O
]	O
e.	O
smith	O
and	O
m.	O
s.	O
lewicki	O
.	O
eﬃcient	B
auditory	O
coding	O
.	O
nature	O
,	O
439	O
(	O
7079	O
)	O
:978–982	O
,	O
2006	O
.	O
[	O
253	O
]	O
p.	O
smolensky	O
.	O
parallel	B
distributed	O
processing	O
:	O
volume	O
1	O
:	O
foundations	O
,	O
chapter	O
information	O
processing	O
in	O
dynamical	O
systems	O
:	O
foundations	O
of	O
harmony	O
theory	O
,	O
pages	O
194–281	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
,	O
1986	O
.	O
[	O
254	O
]	O
g.	O
sneddon	O
.	O
studies	O
in	O
the	O
atmospheric	O
sciences	O
,	O
chapter	O
a	O
statistical	O
perspective	O
on	O
data	B
assimilation	O
in	O
numerical	B
models	O
.	O
number	O
144	O
in	O
lecture	O
notes	O
in	O
statistics	O
.	O
springer-verlag	O
,	O
2000	O
.	O
[	O
255	O
]	O
p.	O
sollich	O
.	O
bayesian	O
methods	O
for	O
support	O
vector	O
machines	O
:	O
evidence	B
and	O
predictive	O
class	O
probabilities	O
.	O
machine	O
learning	B
,	O
46	O
(	O
1-3	O
)	O
:21–52	O
,	O
2002	O
.	O
[	O
256	O
]	O
d.	O
x.	O
song	O
,	O
d.	O
wagner	O
,	O
and	O
x.	O
tian	O
.	O
timing	O
analysis	B
of	O
keystrokes	O
and	O
timing	O
attacks	O
on	O
ssh	O
.	O
proceedings	O
of	O
the	O
10th	O
conference	O
on	O
usenix	O
security	O
symposium	O
.	O
usenix	O
association	O
,	O
2001.	O
in	O
[	O
257	O
]	O
a.	O
s.	O
spanias	O
.	O
speech	O
coding	O
:	O
a	O
tutorial	O
review	O
.	O
proceedings	O
of	O
the	O
ieee	O
,	O
82	O
(	O
10	O
)	O
:1541–1582	O
,	O
oct	O
1994	O
.	O
[	O
258	O
]	O
d.	O
j.	O
spiegelhalter	O
,	O
a.	O
p.	O
dawid	O
,	O
s.	O
l.	O
lauritzen	O
,	O
and	O
r.	O
g.	O
cowell	O
.	O
bayesian	O
analysis	B
in	O
expert	O
systems	O
.	O
statistical	O
science	O
,	O
8	O
(	O
3	O
)	O
:219–247	O
,	O
1993	O
.	O
[	O
259	O
]	O
p.	O
spirtes	O
,	O
c.	O
glymour	O
,	O
and	O
r.	O
scheines	O
.	O
causation	O
,	O
prediction	B
,	O
and	O
search	O
.	O
mit	O
press	O
,	O
2	O
edition	O
,	O
2000	O
.	O
[	O
260	O
]	O
n.	O
srebro	O
.	O
maximum	B
likelihood	I
bounded	O
tree-width	O
markov	O
networks	O
.	O
in	O
j.	O
breese	O
and	O
d.	O
koller	O
,	O
editors	O
,	O
uncertainty	B
in	O
artiﬁcial	O
intelligence	O
,	O
number	O
17	O
,	O
pages	O
504–511	O
,	O
san	O
francisco	O
,	O
ca	O
,	O
2001.	O
morgan	O
kaufmann	O
.	O
[	O
261	O
]	O
h.	O
steck	O
.	O
constraint-based	O
structural	O
learning	B
in	O
bayesian	O
networks	O
using	O
finite	O
data	B
sets	O
.	O
phd	O
thesis	O
,	O
technical	O
university	O
munich	O
,	O
2001	O
.	O
[	O
262	O
]	O
h.	O
steck	O
.	O
learning	B
the	O
bayesian	O
network	O
structure	O
:	O
dirichlet	O
prior	B
vs	O
data	B
.	O
in	O
d.	O
a.	O
mcallester	O
and	O
p.	O
myllymaki	O
,	O
editors	O
,	O
uncertainty	B
in	O
artiﬁcial	O
intelligence	O
,	O
number	O
24	O
,	O
pages	O
511–518	O
,	O
corvallis	O
,	O
oregon	O
,	O
usa	O
,	O
2008.	O
auai	O
press	O
.	O
[	O
263	O
]	O
h.	O
steck	O
and	O
t.	O
jaakkola	O
.	O
on	O
the	O
dirichlet	O
prior	B
and	O
bayesian	O
regularization	O
.	O
in	O
s.	O
becker	O
,	O
s.	O
thrun	O
,	O
and	O
k.	O
obermayer	O
,	O
editors	O
,	O
nips	O
,	O
pages	O
697–704	O
.	O
mit	O
press	O
,	O
2002	O
.	O
[	O
264	O
]	O
m.	O
studen´y	O
.	O
on	O
mathematical	O
description	O
of	O
probabilistic	B
conditional	O
independence	B
structures	O
.	O
phd	O
thesis	O
,	O
academy	O
of	O
sciences	O
of	O
the	O
czech	O
republic	O
,	O
2001	O
.	O
[	O
265	O
]	O
m.	O
studen´y	O
.	O
on	O
non-graphical	O
description	O
of	O
models	O
of	O
conditional	B
independence	O
structure	B
.	O
in	O
hsss	O
workshop	O
on	O
stochastic	O
systems	O
for	O
individual	O
behaviours	O
.	O
louvain	O
la	O
neueve	O
,	O
belgium	O
,	O
22-23	O
january	O
2001	O
.	O
[	O
266	O
]	O
c.	O
sutton	O
and	O
a.	O
mccallum	O
.	O
an	O
introduction	O
to	O
conditional	O
random	O
ﬁelds	O
for	O
relational	O
learning	B
.	O
in	O
l.	O
getoor	O
and	O
b.	O
taskar	O
,	O
editors	O
,	O
introduction	O
to	O
statistical	O
relational	O
learning	B
.	O
mit	O
press	O
,	O
2006	O
.	O
[	O
267	O
]	O
r.	O
s.	O
sutton	O
and	O
a.	O
g.	O
barto	O
.	O
reinforcement	B
learning	I
:	O
an	O
introduction	O
.	O
mit	O
press	O
,	O
1998	O
.	O
[	O
268	O
]	O
r.	O
j.	O
swendsen	O
and	O
j-s.	O
wang	O
.	O
nonuniversal	O
critical	O
dynamics	O
in	O
monte	O
carlo	O
simulations	O
.	O
physical	O
review	O
letters	O
,	O
58:86–88	O
,	O
1987	O
.	O
[	O
269	O
]	O
b.	O
k.	O
sy	O
.	O
a	O
recurrence	O
local	B
computation	O
approach	B
towards	O
ordering	O
composite	O
beliefs	O
in	O
bayesian	O
belief	B
networks	I
.	O
international	O
journal	O
of	O
approximate	B
reasoning	O
,	O
8:17–50	O
,	O
1993	O
.	O
[	O
270	O
]	O
t.	O
sejnowski	O
.	O
the	O
book	O
of	O
hebb	O
.	O
neuron	O
,	O
24:773–776	O
,	O
1999	O
.	O
576	O
draft	O
march	O
9	O
,	O
2010	O
bibliography	O
bibliography	O
[	O
271	O
]	O
r.	O
e.	O
tarjan	O
and	O
m.	O
yannakakis	O
.	O
simple	O
linear-time	O
algorithms	O
to	O
test	O
chordality	O
of	O
graphs	O
,	O
test	O
acyclicity	O
of	O
hypergraphs	O
,	O
and	O
selectively	O
reduce	O
acyclic	O
hypergraphs	O
.	O
siam	O
journal	O
on	O
computing	O
,	O
13	O
(	O
3	O
)	O
:566–579	O
,	O
1984	O
.	O
[	O
272	O
]	O
s.	O
j.	O
taylor	O
.	O
modelling	B
financial	O
time	O
series	O
.	O
world	O
scientiﬁc	O
,	O
second	O
edition	O
,	O
2008	O
.	O
[	O
273	O
]	O
y.	O
w.	O
teh	O
,	O
d.	O
newman	O
,	O
and	O
m.	O
welling	O
.	O
a	O
collapsed	O
variational	O
bayesian	O
inference	B
algorithm	O
for	O
la-	O
tent	O
dirichlet	O
allocation	O
.	O
in	O
j.	O
c.	O
platt	O
,	O
d.	O
koller	O
,	O
y.	O
singer	O
,	O
and	O
s.	O
roweis	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
number	O
20	O
,	O
pages	O
1481–1488	O
,	O
cambridge	O
,	O
ma	O
,	O
2008.	O
mit	O
press	O
.	O
[	O
274	O
]	O
y.	O
w.	O
teh	O
and	O
m.	O
welling	O
.	O
the	O
uniﬁed	O
propagation	B
and	O
scaling	O
algorithm	B
.	O
in	O
t.	O
g.	O
dietterich	O
,	O
s.	O
becker	O
,	O
and	O
z.	O
ghahramani	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
number	O
14	O
,	O
pages	O
953–960	O
,	O
cambridge	O
,	O
ma	O
,	O
2002.	O
mit	O
press	O
.	O
[	O
275	O
]	O
m.	O
tipping	O
and	O
c.	O
m.	O
bishop	O
.	O
mixtures	O
of	O
probabilistic	B
principal	O
component	O
analysers	O
.	O
neural	B
computation	I
,	O
11	O
(	O
2	O
)	O
:443–482	O
,	O
1999	O
.	O
[	O
276	O
]	O
m.	O
e.	O
tipping	O
.	O
sparse	B
bayesian	O
learning	B
and	O
the	O
relevance	B
vector	I
machine	I
.	O
journal	O
of	O
machine	O
learning	B
research	O
,	O
1:211–244	O
,	O
2001	O
.	O
[	O
277	O
]	O
d.	O
m.	O
titterington	O
,	O
a.	O
f.	O
m.	O
smith	O
,	O
and	O
u.	O
e.	O
makov	O
.	O
statistical	O
analysis	B
of	O
ﬁnite	O
mixture	O
distributions	O
.	O
wiley	O
,	O
1985	O
.	O
[	O
278	O
]	O
e.	O
todorov	O
.	O
eﬃcient	B
computation	O
of	O
optimal	O
actions	O
.	O
proceedings	O
of	O
the	O
national	O
academy	O
of	O
sciences	O
of	O
the	O
united	O
states	O
of	O
america	O
(	O
pnas	O
)	O
,	O
106	O
(	O
28	O
)	O
:11478–11483	O
,	O
2009	O
.	O
[	O
279	O
]	O
m.	O
toussaint	O
,	O
s.	O
harmeling	O
,	O
and	O
a.	O
storkey	O
.	O
probabilistic	B
inference	O
for	O
solving	B
(	O
po	O
)	O
mdps	O
.	O
research	O
report	O
edi-inf-rr-0934	O
,	O
university	O
of	O
edinburgh	O
,	O
school	O
of	O
informatics	O
,	O
2006	O
.	O
[	O
280	O
]	O
m.	O
tsodyks	O
,	O
k.	O
pawelzik	O
,	O
and	O
h.	O
markram	O
.	O
neural	O
networks	O
with	O
dynamic	O
synapses	O
.	O
neural	B
computation	I
,	O
10:821–835	O
,	O
1998	O
.	O
[	O
281	O
]	O
p.	O
van	O
overschee	O
and	O
b.	O
de	O
moor	O
.	O
subspace	O
identiﬁcation	O
for	O
linear	B
systems	O
;	O
theory	O
,	O
implementations	O
,	O
applications	O
.	O
kluwer	O
,	O
1996	O
.	O
[	O
282	O
]	O
v.	O
vapnik	O
.	O
the	O
nature	O
of	O
statistical	O
learning	B
theory	O
.	O
springer	O
,	O
new	O
york	O
,	O
1995	O
.	O
[	O
283	O
]	O
m.	O
verhaegen	O
and	O
p.	O
van	O
dooren	O
.	O
numerical	B
aspects	O
of	O
diﬀerent	O
kalman	O
filter	O
implementations	O
.	O
ieee	O
transactions	O
of	O
automatic	O
control	O
,	O
31	O
(	O
10	O
)	O
:907–917	O
,	O
1986	O
.	O
[	O
284	O
]	O
t.	O
verma	O
and	O
j.	O
pearl	O
.	O
causal	B
networks	O
:	O
semantics	O
and	O
expressiveness	O
.	O
in	O
r.	O
d.	O
schacter	O
,	O
t.	O
s.	O
levitt	O
,	O
l.	O
n.	O
kanal	O
,	O
and	O
j.f	O
.	O
lemmer	O
,	O
editors	O
,	O
uncertainty	B
in	O
artiﬁcial	O
intelligence	O
,	O
volume	O
4	O
,	O
pages	O
69–76	O
,	O
amsterdam	O
,	O
1990.	O
north-holland	O
.	O
[	O
285	O
]	O
t.	O
o.	O
virtanen	O
,	O
a.	O
t.	O
cemgil	O
,	O
and	O
s.	O
j.	O
godsill	O
.	O
bayesian	O
extensions	O
to	O
nonnegative	O
matrix	B
factorisation	I
for	O
audio	O
signal	O
modelling	B
.	O
in	O
ieee	O
international	O
conference	O
on	O
acoustics	O
,	O
speech	O
,	O
and	O
signal	O
processing	O
,	O
pages	O
1825–1828	O
,	O
2008	O
.	O
[	O
286	O
]	O
g.	O
wahba	O
.	O
support	O
vector	O
machines	O
,	O
repreducing	O
kernel	B
hilbert	O
spaces	O
,	O
and	O
randomized	O
gacv	O
,	O
pages	O
69–88	O
.	O
mit	O
press	O
,	O
1999	O
.	O
[	O
287	O
]	O
m.	O
j.	O
wainwright	O
and	O
m.	O
i.	O
jordan	O
.	O
graphical	O
models	O
,	O
exponential	B
families	O
,	O
and	O
variational	B
inference	I
.	O
foun-	O
dations	O
and	O
trends	O
in	O
machine	O
learning	B
,	O
1	O
(	O
1-2	O
)	O
:1–305	O
,	O
2008	O
.	O
[	O
288	O
]	O
h.	O
wallach	O
.	O
eﬃcient	B
training	O
of	O
conditional	O
random	O
ﬁelds	O
.	O
master	O
’	O
s	O
thesis	O
,	O
division	O
of	O
informatics	O
,	O
university	O
of	O
edinburgh	O
,	O
2002	O
.	O
[	O
289	O
]	O
y.	O
wang	O
,	O
j.	O
hodges	O
,	O
and	O
b.	O
tang	O
.	O
classiﬁcation	B
of	O
web	B
documents	O
using	O
a	O
naive	O
bayes	O
method	O
.	O
15th	O
ieee	O
international	O
conference	O
on	O
tools	O
with	O
artiﬁcial	O
intelligence	O
,	O
pages	O
560–564	O
,	O
2003	O
.	O
[	O
290	O
]	O
s.	O
waterhouse	O
,	O
d.	O
mackay	O
,	O
and	O
t.	O
robinson	O
.	O
bayesian	O
methods	O
for	O
mixtures	O
of	O
experts	O
.	O
in	O
d.	O
s.	O
touretzky	O
,	O
m.	O
mozer	O
,	O
and	O
m.	O
e.	O
hasselmo	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
number	O
8	O
,	O
pages	O
351–357	O
,	O
cambridge	O
,	O
ma	O
,	O
1996.	O
mit	O
press	O
.	O
[	O
291	O
]	O
y.	O
weiss	O
and	O
w.	O
t.	O
freeman	O
.	O
correctness	O
of	O
belief	B
propagation	I
in	O
gaussian	O
graphical	O
models	O
of	O
arbitrary	O
topology	O
.	O
neural	B
computation	I
,	O
13	O
(	O
10	O
)	O
:2173–2200	O
,	O
2001.	O
draft	O
march	O
9	O
,	O
2010	O
577	O
bibliography	O
bibliography	O
[	O
292	O
]	O
m.	O
welling	O
,	O
t.	O
p.	O
minka	O
,	O
and	O
y.	O
w.	O
teh	O
.	O
structured	B
region	O
graphs	O
:	O
morphing	O
ep	O
into	O
gbp	O
.	O
in	O
f.	O
bacchus	O
and	O
t.	O
jaakkola	O
,	O
editors	O
,	O
uncertainty	B
in	O
artiﬁcial	O
intelligence	O
,	O
number	O
21	O
,	O
pages	O
609–614	O
,	O
corvallis	O
,	O
oregon	O
,	O
usa	O
,	O
2005.	O
auai	O
press	O
.	O
[	O
293	O
]	O
j.	O
whittaker	O
.	O
graphical	O
models	O
in	O
applied	O
multivariate	B
statistics	O
.	O
john	O
wiley	O
&	O
sons	O
,	O
1990	O
.	O
[	O
294	O
]	O
w.	O
wiegerinck	O
.	O
variational	O
approximations	O
between	O
mean	B
ﬁeld	I
theory	I
and	O
the	O
junction	B
tree	I
algorithm	O
.	O
in	O
c.	O
boutilier	O
and	O
m.	O
goldszmidt	O
,	O
editors	O
,	O
uncertainty	B
in	O
artiﬁcial	O
intelligence	O
,	O
number	O
16	O
,	O
pages	O
626–633	O
,	O
san	O
francisco	O
,	O
ca	O
,	O
2000.	O
morgan	O
kaufmann	O
.	O
[	O
295	O
]	O
w.	O
wiegerinck	O
and	O
t.	O
heskes	O
.	O
fractional	O
belief	B
propagation	I
.	O
in	O
s.	O
becker	O
,	O
s.	O
thrun	O
,	O
and	O
k.	O
obermayer	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
number	O
15	O
,	O
pages	O
438–445	O
,	O
cambridge	O
,	O
ma	O
,	O
2003.	O
mit	O
press	O
.	O
[	O
296	O
]	O
c.	O
k.	O
i.	O
williams	O
.	O
computing	O
with	O
inﬁnite	O
networks	O
.	O
in	O
m.	O
c.	O
mozer	O
,	O
m.	O
i.	O
jordan	O
,	O
and	O
t.	O
petsche	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
nips	O
9	O
,	O
pages	O
295–301	O
,	O
cambridge	O
,	O
ma	O
,	O
1997.	O
mit	O
press	O
.	O
[	O
297	O
]	O
c.	O
k.	O
i.	O
williams	O
and	O
d.	O
barber	O
.	O
bayesian	O
classiﬁcation	B
with	O
gaussian	O
processes	O
.	O
ieee	O
trans	O
pattern	O
analysis	B
and	O
machine	O
intelligence	O
,	O
20:1342–1351	O
,	O
1998	O
.	O
[	O
298	O
]	O
c.	O
yanover	O
and	O
y.	O
weiss	O
.	O
finding	O
the	O
m	O
most	O
probable	O
conﬁgurations	O
using	O
loopy	B
belief	O
propagation	B
.	O
in	O
s.	O
thrun	O
,	O
l.	O
saul	O
,	O
and	O
b.	O
sch¨olkopf	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
(	O
nips	O
)	O
,	O
number	O
16	O
,	O
pages	O
1457–1464	O
,	O
cambridge	O
,	O
ma	O
,	O
2004.	O
mit	O
press	O
.	O
[	O
299	O
]	O
j.	O
s.	O
yedidia	O
,	O
w.	O
t.	O
freeman	O
,	O
and	O
y.	O
weiss	O
.	O
constructing	O
free-energy	O
approximations	O
and	O
generalized	O
belief	B
propagation	I
algorithms	O
.	O
information	O
theory	O
,	O
ieee	O
transactions	O
on	O
,	O
51	O
(	O
7	O
)	O
:2282–2312	O
,	O
july	O
2005	O
.	O
[	O
300	O
]	O
s.	O
young	O
,	O
d.	O
kershaw	O
,	O
j.	O
odell	O
,	O
d.	O
ollason	O
,	O
v.	O
valtchev	O
,	O
and	O
p.	O
woodland	O
.	O
the	O
htk	O
book	O
version	O
3.0.	O
cambridge	O
university	O
press	O
,	O
2000	O
.	O
[	O
301	O
]	O
a.	O
l.	O
yuille	O
and	O
a.	O
rangarajan	O
.	O
the	O
concave-convex	O
procedure	O
.	O
neural	B
computation	I
,	O
15	O
(	O
4	O
)	O
:915–936	O
,	O
2003	O
.	O
[	O
302	O
]	O
j.-h.	O
zhao	O
,	O
p.	O
l.	O
h.	O
yu	O
,	O
and	O
q.	O
jiang	O
.	O
ml	O
estimation	O
for	O
factor	B
analysis	I
:	O
em	O
or	O
non-em	O
?	O
statistics	O
and	O
computing	O
,	O
18	O
(	O
2	O
)	O
:109–123	O
,	O
2008	O
.	O
[	O
303	O
]	O
o.	O
zoeter	O
.	O
monitoring	O
non-linear	B
and	O
switching	B
dynamical	O
systems	O
.	O
phd	O
thesis	O
,	O
radboud	O
university	O
nijmegen	O
,	O
2005	O
.	O
578	O
draft	O
march	O
9	O
,	O
2010	O