a	O
course	O
in	O
machine	O
learning	O
about	O
this	O
book	O
machine	O
learning	O
is	O
a	O
broad	O
and	O
fascinating	O
field	O
.	O
it	O
has	O
been	O
called	O
one	O
of	O
the	O
attractive	O
ﬁelds	O
to	O
work	O
in1	O
.	O
it	O
has	O
applications	O
in	O
an	O
incredibly	O
wide	O
variety	O
of	O
application	O
areas	O
,	O
from	O
medicine	O
to	O
advertising	O
,	O
from	O
military	O
to	O
pedestrian	O
.	O
its	O
importance	O
is	O
likely	O
to	O
grow	O
,	O
as	O
more	O
and	O
more	O
areas	O
turn	O
to	O
it	O
as	O
a	O
way	O
of	O
dealing	O
with	O
the	O
massive	O
amounts	O
of	O
data	O
available	O
.	O
1	O
0.1	O
how	O
to	O
use	O
this	O
book	O
0.2	O
why	O
another	O
textbook	O
?	O
2	O
mitchell	O
1997	O
the	O
purpose	O
of	O
this	O
book	O
is	O
to	O
provide	O
a	O
gentle	O
and	O
pedagogically	O
orga-	O
nized	O
introduction	O
to	O
the	O
ﬁeld	O
.	O
this	O
is	O
in	O
contrast	O
to	O
most	O
existing	O
ma-	O
chine	O
learning	O
texts	O
,	O
which	O
tend	O
to	O
organize	O
things	O
topically	O
,	O
rather	O
than	O
pedagogically	O
(	O
an	O
exception	O
is	O
mitchell	O
’	O
s	O
book2	O
,	O
but	O
unfortu-	O
nately	O
that	O
is	O
getting	O
more	O
and	O
more	O
outdated	O
)	O
.	O
this	O
makes	O
sense	O
for	O
researchers	O
in	O
the	O
ﬁeld	O
,	O
but	O
less	O
sense	O
for	O
learners	O
.	O
a	O
second	O
goal	O
of	O
this	O
book	O
is	O
to	O
provide	O
a	O
view	O
of	O
machine	O
learning	O
that	O
focuses	O
on	O
ideas	O
and	O
models	O
,	O
not	O
on	O
math	O
.	O
it	O
is	O
not	O
possible	O
(	O
or	O
even	O
advisable	O
)	O
to	O
avoid	O
math	O
.	O
but	O
math	O
should	O
be	O
there	O
to	O
aid	O
understanding	O
,	O
not	O
hinder	O
it	O
.	O
finally	O
,	O
this	O
book	O
attempts	O
to	O
have	O
minimal	O
dependencies	O
,	O
so	O
that	O
one	O
can	O
fairly	O
easily	O
pick	O
and	O
choose	O
chapters	O
to	O
read	O
.	O
when	O
dependencies	O
exist	O
,	O
they	O
are	O
listed	O
at	O
the	O
start	O
of	O
the	O
chapter	O
,	O
as	O
well	O
as	O
the	O
list	O
of	O
dependencies	O
at	O
the	O
end	O
of	O
this	O
chapter	O
.	O
the	O
audience	O
of	O
this	O
book	O
is	O
anyone	O
who	O
knows	O
differential	O
calcu-	O
lus	O
and	O
discrete	O
math	O
,	O
and	O
can	O
program	O
reasonably	O
well	O
.	O
(	O
a	O
little	O
bit	O
of	O
linear	O
algebra	O
and	O
probability	O
will	O
not	O
hurt	O
.	O
)	O
an	O
undergraduate	O
in	O
their	O
fourth	O
or	O
ﬁfth	O
semester	O
should	O
be	O
fully	O
capable	O
of	O
understand-	O
ing	O
this	O
material	O
.	O
however	O
,	O
it	O
should	O
also	O
be	O
suitable	O
for	O
ﬁrst	O
year	O
graduate	O
students	O
,	O
perhaps	O
at	O
a	O
slightly	O
faster	O
pace	O
.	O
7	O
0.3	O
organization	O
and	O
auxilary	O
material	O
there	O
is	O
an	O
associated	O
web	O
page	O
,	O
http	O
:	O
//hal3.name/courseml/	O
,	O
which	O
contains	O
an	O
online	B
copy	O
of	O
this	O
book	O
,	O
as	O
well	O
as	O
associated	O
code	O
and	O
data	O
.	O
it	O
also	O
contains	O
errate	O
.	O
for	O
instructors	O
,	O
there	O
is	O
the	O
ability	O
to	O
get	O
a	O
solutions	O
manual	O
.	O
this	O
book	O
is	O
suitable	O
for	O
a	O
single-semester	O
undergraduate	O
course	O
,	O
graduate	O
course	O
or	O
two	O
semester	O
course	O
(	O
perhaps	O
the	O
latter	O
supple-	O
mented	O
with	O
readings	O
decided	O
upon	O
by	O
the	O
instructor	O
)	O
.	O
here	O
are	O
suggested	O
course	O
plans	O
for	O
the	O
ﬁrst	O
two	O
courses	O
;	O
a	O
year-long	O
course	O
could	O
be	O
obtained	O
simply	O
by	O
covering	O
the	O
entire	O
book	O
.	O
0.4	O
acknowledgements	O
1	O
|	O
decision	B
trees	I
learning	O
objectives	O
:	O
•	O
explain	O
the	O
difference	O
between	O
memorization	O
and	O
generalization	O
.	O
•	O
deﬁne	O
“	O
inductive	B
bias	I
”	O
and	O
recog-	O
nize	O
the	O
role	O
of	O
inductive	B
bias	I
in	O
learning	O
.	O
•	O
take	O
a	O
concrete	O
task	O
and	O
cast	O
it	O
as	O
a	O
learning	O
problem	O
,	O
with	O
a	O
formal	O
no-	O
tion	O
of	O
input	O
space	O
,	O
features	B
,	O
output	O
space	O
,	O
generating	O
distribution	O
and	O
loss	B
function	I
.	O
•	O
illustrate	O
how	O
regularization	O
trades	O
off	O
between	O
underﬁtting	O
and	O
overﬁt-	O
ting	O
.	O
•	O
evaluate	O
whether	O
a	O
use	O
of	O
test	B
data	I
is	O
“	O
cheating	O
”	O
or	O
not	O
.	O
dependencies	O
:	O
none	O
.	O
the	O
words	O
printed	O
here	O
are	O
concepts	O
.	O
you	O
must	O
go	O
through	O
the	O
experiences	O
.	O
–	O
carl	O
frederick	O
at	O
a	O
basic	O
level	O
,	O
machine	O
learning	O
is	O
about	O
predicting	O
the	O
fu-	O
ture	O
based	O
on	O
the	O
past	O
.	O
for	O
instance	O
,	O
you	O
might	O
wish	O
to	O
predict	B
how	O
much	O
a	O
user	O
alice	O
will	O
like	O
a	O
movie	O
that	O
she	O
hasn	O
’	O
t	O
seen	O
,	O
based	O
on	O
her	O
ratings	O
of	O
movies	O
that	O
she	O
has	O
seen	O
.	O
this	O
means	O
making	O
informed	O
guesses	O
about	O
some	O
unobserved	O
property	O
of	O
some	O
object	O
,	O
based	O
on	O
observed	O
properties	O
of	O
that	O
object	O
.	O
the	O
ﬁrst	O
question	O
we	O
’	O
ll	O
ask	O
is	O
:	O
what	O
does	O
it	O
mean	O
to	O
learn	O
?	O
in	O
order	O
to	O
develop	O
learning	O
machines	O
,	O
we	O
must	O
know	O
what	O
learning	O
actually	O
means	O
,	O
and	O
how	O
to	O
determine	O
success	O
(	O
or	O
failure	O
)	O
.	O
you	O
’	O
ll	O
see	O
this	O
question	O
answered	O
in	O
a	O
very	O
limited	O
learning	O
setting	O
,	O
which	O
will	O
be	O
progressively	O
loosened	O
and	O
adapted	O
throughout	O
the	O
rest	O
of	O
this	O
book	O
.	O
for	O
concreteness	O
,	O
our	O
focus	O
will	O
be	O
on	O
a	O
very	O
simple	O
model	B
of	O
learning	O
called	O
a	O
decision	B
tree	I
.	O
vignette	O
:	O
alice	O
decides	O
which	O
classes	O
to	O
take	O
todo	O
1.1	O
what	O
does	O
it	O
mean	O
to	O
learn	O
?	O
alice	O
has	O
just	O
begun	O
taking	O
a	O
course	O
on	O
machine	O
learning	O
.	O
she	O
knows	O
that	O
at	O
the	O
end	O
of	O
the	O
course	O
,	O
she	O
will	O
be	O
expected	O
to	O
have	O
“	O
learned	O
”	O
all	O
about	O
this	O
topic	O
.	O
a	O
common	O
way	O
of	O
gauging	O
whether	O
or	O
not	O
she	O
has	O
learned	O
is	O
for	O
her	O
teacher	O
,	O
bob	O
,	O
to	O
give	O
her	O
a	O
exam	O
.	O
she	O
has	O
done	O
well	O
at	O
learning	O
if	O
she	O
does	O
well	O
on	O
the	O
exam	O
.	O
but	O
what	O
makes	O
a	O
reasonable	O
exam	O
?	O
if	O
bob	O
spends	O
the	O
entire	O
semester	O
talking	O
about	O
machine	O
learning	O
,	O
and	O
then	O
gives	O
alice	O
an	O
exam	O
on	O
history	O
of	O
pottery	O
,	O
then	O
alice	O
’	O
s	O
performance	O
on	O
this	O
exam	O
will	O
not	O
be	O
representative	O
of	O
her	O
learning	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
the	O
exam	O
only	O
asks	O
questions	O
that	O
bob	O
has	O
answered	O
exactly	O
during	O
lec-	O
tures	O
,	O
then	O
this	O
is	O
also	O
a	O
bad	O
test	O
of	O
alice	O
’	O
s	O
learning	O
,	O
especially	O
if	O
it	O
’	O
s	O
an	O
“	O
open	O
notes	O
”	O
exam	O
.	O
what	O
is	O
desired	O
is	O
that	O
alice	O
observes	O
speciﬁc	O
examples	B
from	O
the	O
course	O
,	O
and	O
then	O
has	O
to	O
answer	O
new	O
,	O
but	O
related	O
questions	O
on	O
the	O
exam	O
.	O
this	O
tests	O
whether	O
alice	O
has	O
the	O
ability	O
to	O
generalize	B
.	O
generalization	O
is	O
perhaps	O
the	O
most	O
central	O
concept	B
in	O
machine	O
learning	O
.	O
as	O
a	O
running	O
concrete	O
example	O
in	O
this	O
book	O
,	O
we	O
will	O
use	O
that	O
of	O
a	O
course	O
recommendation	O
system	O
for	O
undergraduate	O
computer	O
science	O
students	O
.	O
we	O
have	O
a	O
collection	O
of	O
students	O
and	O
a	O
collection	O
of	O
courses	O
.	O
each	O
student	O
has	O
taken	O
,	O
and	O
evaluated	O
,	O
a	O
subset	O
of	O
the	O
courses	O
.	O
the	O
evaluation	O
is	O
simply	O
a	O
score	O
from	O
−2	O
(	O
terrible	O
)	O
to	O
+2	O
(	O
awesome	O
)	O
.	O
the	O
job	O
of	O
the	O
recommender	O
system	O
is	O
to	O
predict	B
how	O
much	O
a	O
particular	O
student	O
(	O
say	O
,	O
alice	O
)	O
will	O
like	O
a	O
particular	O
course	O
(	O
say	O
,	O
algorithms	O
)	O
.	O
given	O
historical	O
data	O
from	O
course	O
ratings	O
(	O
i.e.	O
,	O
the	O
past	O
)	O
we	O
are	O
trying	O
to	O
predict	B
unseen	O
ratings	O
(	O
i.e.	O
,	O
the	O
future	O
)	O
.	O
now	O
,	O
we	O
could	O
be	O
unfair	O
to	O
this	O
system	O
as	O
well	O
.	O
we	O
could	O
ask	O
it	O
whether	O
alice	O
is	O
likely	O
to	O
enjoy	O
the	O
history	O
of	O
pottery	O
course	O
.	O
this	O
is	O
unfair	O
because	O
the	O
system	O
has	O
no	O
idea	O
what	O
history	O
of	O
pottery	O
even	O
is	O
,	O
and	O
has	O
no	O
prior	B
experience	O
with	O
this	O
course	O
.	O
on	O
the	O
other	O
hand	O
,	O
we	O
could	O
ask	O
it	O
how	O
much	O
alice	O
will	O
like	O
artiﬁcial	O
intelligence	O
,	O
which	O
she	O
took	O
last	O
year	O
and	O
rated	O
as	O
+2	O
(	O
awesome	O
)	O
.	O
we	O
would	O
expect	O
the	O
system	O
to	O
predict	B
that	O
she	O
would	O
really	O
like	O
it	O
,	O
but	O
this	O
isn	O
’	O
t	O
demonstrating	O
that	O
the	O
system	O
has	O
learned	O
:	O
it	O
’	O
s	O
simply	O
recalling	O
its	O
past	O
experience	O
.	O
in	O
the	O
former	O
case	O
,	O
we	O
’	O
re	O
expecting	O
the	O
system	O
to	O
generalize	B
beyond	O
its	O
experience	O
,	O
which	O
is	O
unfair	O
.	O
in	O
the	O
latter	O
case	O
,	O
we	O
’	O
re	O
not	O
expecting	O
it	O
to	O
generalize	B
at	O
all	O
.	O
this	O
general	O
set	O
up	O
of	O
predicting	O
the	O
future	O
based	O
on	O
the	O
past	O
is	O
at	O
the	O
core	O
of	O
most	O
machine	O
learning	O
.	O
the	O
objects	O
that	O
our	O
algorithm	B
will	O
make	O
predictions	O
about	O
are	O
examples	B
.	O
in	O
the	O
recommender	O
sys-	O
tem	O
setting	O
,	O
an	O
example	O
would	O
be	O
some	O
particular	O
student/course	O
pair	O
(	O
such	O
as	O
alice/algorithms	O
)	O
.	O
the	O
desired	O
prediction	O
would	O
be	O
the	O
rating	O
that	O
alice	O
would	O
give	O
to	O
algorithms	O
.	O
to	O
make	O
this	O
concrete	O
,	O
figure	O
1.1	O
shows	O
the	O
general	O
framework	O
of	O
induction	B
.	O
we	O
are	O
given	O
training	B
data	I
on	O
which	O
our	O
algorithm	B
is	O
ex-	O
pected	O
to	O
learn	O
.	O
this	O
training	B
data	I
is	O
the	O
examples	O
that	O
alice	O
observes	O
in	O
her	O
machine	O
learning	O
course	O
,	O
or	O
the	O
historical	O
ratings	O
data	O
for	O
the	O
recommender	O
system	O
.	O
based	O
on	O
this	O
training	B
data	I
,	O
our	O
learning	O
algorithm	B
induces	O
a	O
function	O
f	O
that	O
will	O
map	O
a	O
new	O
example	O
to	O
a	O
cor-	O
responding	O
prediction	O
.	O
for	O
example	O
,	O
our	O
function	O
might	O
guess	O
that	O
f	O
(	O
alice/machine	O
learning	O
)	O
might	O
be	O
high	O
because	O
our	O
training	B
data	I
said	O
that	O
alice	O
liked	O
artiﬁcial	O
intelligence	O
.	O
we	O
want	O
our	O
algorithm	B
to	O
be	O
able	O
to	O
make	O
lots	O
of	O
predictions	O
,	O
so	O
we	O
refer	O
to	O
the	O
collection	O
of	O
examples	B
on	O
which	O
we	O
will	O
evaluate	O
our	O
algorithm	B
as	O
the	O
test	O
set	O
.	O
the	O
test	O
set	O
is	O
a	O
closely	O
guarded	O
secret	O
:	O
it	O
is	O
the	O
ﬁnal	O
exam	O
on	O
which	O
our	O
learning	O
algorithm	B
is	O
being	O
tested	O
.	O
if	O
our	O
algorithm	B
gets	O
to	O
peek	O
at	O
it	O
ahead	O
of	O
time	O
,	O
it	O
’	O
s	O
going	O
to	O
cheat	O
and	O
do	O
better	O
than	O
it	O
should	O
.	O
decision	B
trees	I
9	O
figure	O
1.1	O
:	O
the	O
general	O
supervised	O
ap-	O
proach	O
to	O
machine	O
learning	O
:	O
a	O
learning	O
algorithm	B
reads	O
in	O
training	B
data	I
and	O
computes	O
a	O
learned	O
function	O
f	O
.	O
this	O
function	O
can	O
then	O
automatically	O
label	B
future	O
text	O
examples	O
.	O
?	O
why	O
is	O
it	O
bad	O
if	O
the	O
learning	O
algo-	O
rithm	O
gets	O
to	O
peek	O
at	O
the	O
test	O
data	O
?	O
10	O
a	O
course	O
in	O
machine	O
learning	O
the	O
goal	O
of	O
inductive	O
machine	O
learning	O
is	O
to	O
take	O
some	O
training	B
data	I
and	O
use	O
it	O
to	O
induce	B
a	O
function	O
f	O
.	O
this	O
function	O
f	O
will	O
be	O
evalu-	O
ated	O
on	O
the	O
test	O
data	O
.	O
the	O
machine	O
learning	O
algorithm	B
has	O
succeeded	O
if	O
its	O
performance	O
on	O
the	O
test	O
data	O
is	O
high	O
.	O
1.2	O
some	O
canonical	O
learning	O
problems	O
there	O
are	O
a	O
large	O
number	O
of	O
typical	O
inductive	O
learning	O
problems	O
.	O
the	O
primary	O
difference	O
between	O
them	O
is	O
in	O
what	O
type	O
of	O
thing	O
they	O
’	O
re	O
trying	O
to	O
predict	B
.	O
here	O
are	O
some	O
examples	B
:	O
regression	O
:	O
trying	O
to	O
predict	B
a	O
real	O
value	O
.	O
for	O
instance	O
,	O
predict	B
the	O
value	O
of	O
a	O
stock	O
tomorrow	O
given	O
its	O
past	O
performance	O
.	O
or	O
predict	B
alice	O
’	O
s	O
score	O
on	O
the	O
machine	O
learning	O
ﬁnal	O
exam	O
based	O
on	O
her	O
homework	O
scores	O
.	O
binary	O
classiﬁcation	O
:	O
trying	O
to	O
predict	B
a	O
simple	O
yes/no	O
response	O
.	O
for	O
instance	O
,	O
predict	B
whether	O
alice	O
will	O
enjoy	O
a	O
course	O
or	O
not	O
.	O
or	O
predict	B
whether	O
a	O
user	O
review	O
of	O
the	O
newest	O
apple	O
product	O
is	O
positive	O
or	O
negative	O
about	O
the	O
product	O
.	O
multiclass	O
classiﬁcation	O
:	O
trying	O
to	O
put	O
an	O
example	O
into	O
one	O
of	O
a	O
num-	O
ber	O
of	O
classes	O
.	O
for	O
instance	O
,	O
predict	B
whether	O
a	O
news	O
story	O
is	O
about	O
entertainment	O
,	O
sports	O
,	O
politics	O
,	O
religion	O
,	O
etc	O
.	O
or	O
predict	B
whether	O
a	O
cs	O
course	O
is	O
systems	O
,	O
theory	O
,	O
ai	O
or	O
other	O
.	O
ranking	O
:	O
trying	O
to	O
put	O
a	O
set	O
of	O
objects	O
in	O
order	O
of	O
relevance	O
.	O
for	O
in-	O
stance	O
,	O
predicting	O
what	O
order	O
to	O
put	O
web	O
pages	O
in	O
,	O
in	O
response	O
to	O
a	O
user	O
query	O
.	O
or	O
predict	B
alice	O
’	O
s	O
ranked	O
preferences	O
over	O
courses	O
she	O
hasn	O
’	O
t	O
taken	O
.	O
the	O
reason	O
that	O
it	O
is	O
convenient	O
to	O
break	O
machine	O
learning	O
prob-	O
lems	O
down	O
by	O
the	O
type	O
of	O
object	O
that	O
they	O
’	O
re	O
trying	O
to	O
predict	B
has	O
to	O
do	O
with	O
measuring	O
error	O
.	O
recall	B
that	O
our	O
goal	O
is	O
to	O
build	O
a	O
system	O
that	O
can	O
make	O
“	O
good	O
predictions.	O
”	O
this	O
begs	O
the	O
question	O
:	O
what	O
does	O
it	O
mean	O
for	O
a	O
prediction	O
to	O
be	O
“	O
good	O
?	O
”	O
the	O
different	O
types	O
of	O
learning	O
problems	O
differ	O
in	O
how	O
they	O
deﬁne	O
goodness	O
.	O
for	O
instance	O
,	O
in	O
regres-	O
sion	O
,	O
predicting	O
a	O
stock	O
price	O
that	O
is	O
off	O
by	O
$	O
0.05	O
is	O
perhaps	O
much	O
better	O
than	O
being	O
off	O
by	O
$	O
200.00	O
.	O
the	O
same	O
does	O
not	O
hold	O
of	O
multi-	O
class	O
classiﬁcation	O
.	O
there	O
,	O
accidentally	O
predicting	O
“	O
entertainment	O
”	O
instead	O
of	O
“	O
sports	O
”	O
is	O
no	O
better	O
or	O
worse	O
than	O
predicting	O
“	O
politics.	O
”	O
1.3	O
the	O
decision	O
tree	O
model	B
of	O
learning	O
the	O
decision	O
tree	O
is	O
a	O
classic	O
and	O
natural	O
model	B
of	O
learning	O
.	O
it	O
is	O
closely	O
related	O
to	O
the	O
fundamental	O
computer	O
science	O
notion	O
of	O
“	O
di-	O
vide	O
and	O
conquer.	O
”	O
although	O
decision	B
trees	I
can	O
be	O
applied	O
to	O
many	O
?	O
for	O
each	O
of	O
these	O
types	O
of	O
canon-	O
ical	O
machine	O
learning	O
problems	O
,	O
come	O
up	O
with	O
one	O
or	O
two	O
concrete	O
examples	B
.	O
learning	O
problems	O
,	O
we	O
will	O
begin	O
with	O
the	O
simplest	O
case	O
:	O
binary	O
clas-	O
siﬁcation	O
.	O
suppose	O
that	O
your	O
goal	O
is	O
to	O
predict	B
whether	O
some	O
unknown	O
user	O
will	O
enjoy	O
some	O
unknown	O
course	O
.	O
you	O
must	O
simply	O
answer	O
“	O
yes	O
”	O
or	O
“	O
no.	O
”	O
in	O
order	O
to	O
make	O
a	O
guess	O
,	O
you	O
’	O
re	O
allowed	O
to	O
ask	O
binary	O
ques-	O
tions	O
about	O
the	O
user/course	O
under	O
consideration	O
.	O
for	O
example	O
:	O
you	O
:	O
is	O
the	O
course	O
under	O
consideration	O
in	O
systems	O
?	O
me	O
:	O
yes	O
you	O
:	O
has	O
this	O
student	O
taken	O
any	O
other	O
systems	O
courses	O
?	O
me	O
:	O
yes	O
you	O
:	O
has	O
this	O
student	O
liked	O
most	O
previous	O
systems	O
courses	O
?	O
me	O
:	O
no	O
you	O
:	O
i	O
predict	B
this	O
student	O
will	O
not	O
like	O
this	O
course	O
.	O
the	O
goal	O
in	O
learning	O
is	O
to	O
ﬁgure	O
out	O
what	O
questions	O
to	O
ask	O
,	O
in	O
what	O
order	O
to	O
ask	O
them	O
,	O
and	O
what	O
answer	O
to	O
predict	B
once	O
you	O
have	O
asked	O
enough	O
questions	O
.	O
the	O
decision	O
tree	O
is	O
so-called	O
because	O
we	O
can	O
write	O
our	O
set	O
of	O
ques-	O
tions	O
and	O
guesses	O
in	O
a	O
tree	O
format	O
,	O
such	O
as	O
that	O
in	O
figure	O
1.2.	O
in	O
this	O
ﬁgure	O
,	O
the	O
questions	O
are	O
written	O
in	O
the	O
internal	O
tree	O
nodes	O
(	O
rectangles	O
)	O
and	O
the	O
guesses	O
are	O
written	O
in	O
the	O
leaves	O
(	O
ovals	O
)	O
.	O
each	O
non-terminal	O
node	O
has	O
two	O
children	O
:	O
the	O
left	O
child	O
speciﬁes	O
what	O
to	O
do	O
if	O
the	O
an-	O
swer	O
to	O
the	O
question	O
is	O
“	O
no	O
”	O
and	O
the	O
right	O
child	O
speciﬁes	O
what	O
to	O
do	O
if	O
it	O
is	O
“	O
yes.	O
”	O
in	O
order	O
to	O
learn	O
,	O
i	O
will	O
give	O
you	O
training	B
data	I
.	O
this	O
data	O
consists	O
of	O
a	O
set	O
of	O
user/course	O
examples	B
,	O
paired	O
with	O
the	O
correct	O
answer	O
for	O
these	O
examples	B
(	O
did	O
the	O
given	O
user	O
enjoy	O
the	O
given	O
course	O
?	O
)	O
.	O
from	O
this	O
,	O
you	O
must	O
construct	O
your	O
questions	O
.	O
for	O
concreteness	O
,	O
there	O
is	O
a	O
small	O
data	O
set	O
in	O
table	O
?	O
?	O
in	O
the	O
appendix	O
of	O
this	O
book	O
.	O
this	O
training	B
data	I
consists	O
of	O
20	O
course	O
rating	O
examples	B
,	O
with	O
course	O
ratings	O
and	O
answers	O
to	O
questions	O
that	O
you	O
might	O
ask	O
about	O
this	O
pair	O
.	O
we	O
will	O
interpret	O
ratings	O
of	O
0	O
,	O
+1	O
and	O
+2	O
as	O
“	O
liked	O
”	O
and	O
ratings	O
of	O
−2	O
and	O
−1	O
as	O
“	O
hated.	O
”	O
in	O
what	O
follows	O
,	O
we	O
will	O
refer	O
to	O
the	O
questions	O
that	O
you	O
can	O
ask	O
as	O
features	B
and	O
the	O
responses	O
to	O
these	O
questions	O
as	O
feature	B
values	I
.	O
the	O
rating	O
is	O
called	O
the	O
label	O
.	O
an	O
example	O
is	O
just	O
a	O
set	O
of	O
feature	B
values	I
.	O
and	O
our	O
training	B
data	I
is	O
a	O
set	O
of	O
examples	B
,	O
paired	O
with	O
labels	O
.	O
there	O
are	O
a	O
lot	O
of	O
logically	O
possible	O
trees	O
that	O
you	O
could	O
build	O
,	O
even	O
over	O
just	O
this	O
small	O
number	O
of	O
features	B
(	O
the	O
number	O
is	O
in	O
the	O
millions	O
)	O
.	O
it	O
is	O
computationally	O
infeasible	O
to	O
consider	O
all	O
of	O
these	O
to	O
try	O
to	O
choose	O
the	O
“	O
best	O
”	O
one	O
.	O
instead	O
,	O
we	O
will	O
build	O
our	O
decision	B
tree	I
greedily	O
.	O
we	O
will	O
begin	O
by	O
asking	O
:	O
if	O
i	O
could	O
only	O
ask	O
one	O
question	O
,	O
what	O
question	O
would	O
i	O
ask	O
?	O
you	O
want	O
to	O
ﬁnd	O
a	O
feature	O
that	O
is	O
most	O
useful	O
in	O
helping	O
you	O
guess	O
whether	O
this	O
student	O
will	O
enjoy	O
this	O
course.1	O
a	O
useful	O
way	O
to	O
think	O
decision	B
trees	I
11	O
figure	O
1.2	O
:	O
a	O
decision	B
tree	I
for	O
a	O
course	O
recommender	O
system	O
,	O
from	O
which	O
the	O
in-text	O
“	O
dialog	O
”	O
is	O
drawn	O
.	O
figure	O
1.3	O
:	O
a	O
histogram	B
of	O
labels	O
for	O
(	O
a	O
)	O
the	O
entire	O
data	O
set	O
;	O
(	O
b-e	O
)	O
the	O
examples	O
in	O
the	O
data	O
set	O
for	O
each	O
value	O
of	O
the	O
ﬁrst	O
four	O
features	B
.	O
1	O
a	O
colleague	O
related	O
the	O
story	O
of	O
getting	O
his	O
8-year	O
old	O
nephew	O
to	O
guess	O
a	O
number	O
between	O
1	O
and	O
100.	O
his	O
nephew	O
’	O
s	O
ﬁrst	O
four	O
questions	O
were	O
:	O
is	O
it	O
bigger	O
than	O
20	O
?	O
(	O
yes	O
)	O
is	O
it	O
even	O
?	O
(	O
yes	O
)	O
does	O
it	O
have	O
a	O
7	O
in	O
it	O
?	O
12	O
a	O
course	O
in	O
machine	O
learning	O
about	O
this	O
is	O
to	O
look	O
at	O
the	O
histogram	O
of	O
labels	O
for	O
each	O
feature	O
.	O
this	O
is	O
shown	O
for	O
the	O
ﬁrst	O
four	O
features	B
in	O
figure	O
1.3.	O
each	O
histogram	B
shows	O
the	O
frequency	O
of	O
“	O
like	O
”	O
/	O
“	O
hate	O
”	O
labels	O
for	O
each	O
possible	O
value	O
of	O
an	O
associated	O
feature	O
.	O
from	O
this	O
ﬁgure	O
,	O
you	O
can	O
see	O
that	O
asking	O
the	O
ﬁrst	O
feature	O
is	O
not	O
useful	O
:	O
if	O
the	O
value	O
is	O
“	O
no	O
”	O
then	O
it	O
’	O
s	O
hard	O
to	O
guess	O
the	O
label	O
;	O
similarly	O
if	O
the	O
answer	O
is	O
“	O
yes.	O
”	O
on	O
the	O
other	O
hand	O
,	O
asking	O
the	O
second	O
feature	O
is	O
useful	O
:	O
if	O
the	O
value	O
is	O
“	O
no	O
,	O
”	O
you	O
can	O
be	O
pretty	O
conﬁdent	O
that	O
this	O
student	O
will	O
hate	O
this	O
course	O
;	O
if	O
the	O
answer	O
is	O
“	O
yes	O
,	O
”	O
you	O
can	O
be	O
pretty	O
conﬁdent	O
that	O
this	O
student	O
will	O
like	O
this	O
course	O
.	O
more	O
formally	O
,	O
you	O
will	O
consider	O
each	O
feature	O
in	O
turn	O
.	O
you	O
might	O
consider	O
the	O
feature	O
“	O
is	O
this	O
a	O
system	O
’	O
s	O
course	O
?	O
”	O
this	O
feature	O
has	O
two	O
possible	O
value	O
:	O
no	O
and	O
yes	O
.	O
some	O
of	O
the	O
training	O
examples	B
have	O
an	O
answer	O
of	O
“	O
no	O
”	O
–	O
let	O
’	O
s	O
call	O
that	O
the	O
“	O
no	O
”	O
set	O
.	O
some	O
of	O
the	O
training	O
examples	B
have	O
an	O
answer	O
of	O
“	O
yes	O
”	O
–	O
let	O
’	O
s	O
call	O
that	O
the	O
“	O
yes	O
”	O
set	O
.	O
for	O
each	O
set	O
(	O
no	O
and	O
yes	O
)	O
we	O
will	O
build	O
a	O
histogram	B
over	O
the	O
labels	O
.	O
this	O
is	O
the	O
second	O
histogram	B
in	O
figure	O
1.3.	O
now	O
,	O
suppose	O
you	O
were	O
to	O
ask	O
this	O
question	O
on	O
a	O
random	O
example	O
and	O
observe	O
a	O
value	O
of	O
“	O
no.	O
”	O
further	O
suppose	O
that	O
you	O
must	O
immediately	O
guess	O
the	O
label	O
for	O
this	O
example	O
.	O
you	O
will	O
guess	O
“	O
like	O
,	O
”	O
because	O
that	O
’	O
s	O
the	O
more	O
preva-	O
lent	O
label	B
in	O
the	O
no	O
set	O
(	O
actually	O
,	O
it	O
’	O
s	O
the	O
only	O
label	B
in	O
the	O
no	O
set	O
)	O
.	O
alternatively	O
,	O
if	O
you	O
recieve	O
an	O
answer	O
of	O
“	O
yes	O
,	O
”	O
you	O
will	O
guess	O
“	O
hate	O
”	O
because	O
that	O
is	O
more	O
prevalent	O
in	O
the	O
yes	O
set	O
.	O
so	O
,	O
for	O
this	O
single	O
feature	O
,	O
you	O
know	O
what	O
you	O
would	O
guess	O
if	O
you	O
had	O
to	O
.	O
now	O
you	O
can	O
ask	O
yourself	O
:	O
if	O
i	O
made	O
that	O
guess	O
on	O
the	O
train-	O
ing	O
data	O
,	O
how	O
well	O
would	O
i	O
have	O
done	O
?	O
in	O
particular	O
,	O
how	O
many	O
ex-	O
amples	O
would	O
i	O
classify	O
correctly	O
?	O
in	O
the	O
no	O
set	O
(	O
where	O
you	O
guessed	O
“	O
like	O
”	O
)	O
you	O
would	O
classify	O
all	O
10	O
of	O
them	O
correctly	O
.	O
in	O
the	O
yes	O
set	O
(	O
where	O
you	O
guessed	O
“	O
hate	O
”	O
)	O
you	O
would	O
classify	O
8	O
(	O
out	O
of	O
10	O
)	O
of	O
them	O
correctly	O
.	O
so	O
overall	O
you	O
would	O
classify	O
18	O
(	O
out	O
of	O
20	O
)	O
correctly	O
.	O
thus	O
,	O
we	O
’	O
ll	O
say	O
that	O
the	O
score	O
of	O
the	O
“	O
is	O
this	O
a	O
system	O
’	O
s	O
course	O
?	O
”	O
question	O
is	O
18/20	O
.	O
you	O
will	O
then	O
repeat	O
this	O
computation	O
for	O
each	O
of	O
the	O
available	O
features	B
to	O
us	O
,	O
compute	O
the	O
scores	O
for	O
each	O
of	O
them	O
.	O
when	O
you	O
must	O
choose	O
which	O
feature	O
consider	O
ﬁrst	O
,	O
you	O
will	O
want	O
to	O
choose	O
the	O
one	O
with	O
the	O
highest	O
score	O
.	O
but	O
this	O
only	O
lets	O
you	O
choose	O
the	O
ﬁrst	O
feature	O
to	O
ask	O
about	O
.	O
this	O
is	O
the	O
feature	O
that	O
goes	O
at	O
the	O
root	O
of	O
the	O
decision	O
tree	O
.	O
how	O
do	O
we	O
choose	O
subsequent	O
features	B
?	O
this	O
is	O
where	O
the	O
notion	O
of	O
divide	O
and	O
conquer	O
comes	O
in	O
.	O
you	O
’	O
ve	O
already	O
decided	O
on	O
your	O
ﬁrst	O
feature	O
:	O
“	O
is	O
this	O
a	O
systems	O
course	O
?	O
”	O
you	O
can	O
now	O
partition	O
the	O
data	O
into	O
two	O
parts	O
:	O
the	O
no	O
part	O
and	O
the	O
yes	O
part	O
.	O
the	O
no	O
part	O
is	O
the	O
subset	O
of	O
the	O
data	O
on	O
which	O
value	O
for	O
this	O
feature	O
is	O
“	O
no	O
”	O
;	O
the	O
yes	O
half	O
is	O
the	O
rest	O
.	O
this	O
is	O
the	O
divide	O
step	O
.	O
?	O
how	O
many	O
training	O
examples	O
would	O
you	O
classify	O
correctly	O
for	O
each	O
of	O
the	O
other	O
three	O
features	B
from	O
figure	O
1.3	O
?	O
decision	B
trees	I
13	O
return	O
leaf	O
(	O
guess	O
)	O
algorithm	B
1	O
decisiontreetrain	O
(	O
data	O
,	O
remaining	O
features	B
)	O
1	O
:	O
guess	O
←	O
most	O
frequent	O
answer	O
in	O
data	O
2	O
:	O
if	O
the	O
labels	O
in	O
data	O
are	O
unambiguous	O
then	O
3	O
:	O
4	O
:	O
else	O
if	O
remaining	O
features	B
is	O
empty	O
then	O
5	O
:	O
6	O
:	O
else	O
7	O
:	O
return	O
leaf	O
(	O
guess	O
)	O
for	O
all	O
f	O
∈	O
remaining	O
features	B
do	O
no	O
←	O
the	O
subset	O
of	O
data	O
on	O
which	O
f	O
=no	O
yes	O
←	O
the	O
subset	O
of	O
data	O
on	O
which	O
f	O
=yes	O
score	O
[	O
f	O
]	O
←	O
#	O
of	O
majority	O
vote	B
answers	O
in	O
no	O
+	O
#	O
of	O
majority	O
vote	B
answers	O
in	O
yes	O
//	O
default	O
answer	O
for	O
this	O
data	O
//	O
base	O
case	O
:	O
no	O
need	O
to	O
split	O
further	O
//	O
base	O
case	O
:	O
can	O
not	O
split	O
further	O
//	O
we	O
need	O
to	O
query	O
more	O
features	B
//	O
the	O
accuracy	O
we	O
would	O
get	O
if	O
we	O
only	O
queried	O
on	O
f	O
end	O
for	O
f	O
←	O
the	O
feature	O
with	O
maximal	O
score	O
(	O
f	O
)	O
no	O
←	O
the	O
subset	O
of	O
data	O
on	O
which	O
f	O
=no	O
yes	O
←	O
the	O
subset	O
of	O
data	O
on	O
which	O
f	O
=yes	O
left	O
←	O
decisiontreetrain	O
(	O
no	O
,	O
remaining	O
features	B
\	O
{	O
f	O
}	O
)	O
right	O
←	O
decisiontreetrain	O
(	O
yes	O
,	O
remaining	O
features	B
\	O
{	O
f	O
}	O
)	O
return	O
node	O
(	O
f	O
,	O
left	O
,	O
right	O
)	O
18	O
:	O
19	O
:	O
end	O
if	O
8	O
:	O
9	O
:	O
10	O
:	O
11	O
:	O
12	O
:	O
13	O
:	O
14	O
:	O
15	O
:	O
16	O
:	O
17	O
:	O
5	O
:	O
6	O
:	O
7	O
:	O
algorithm	B
2	O
decisiontreetest	O
(	O
tree	O
,	O
test	O
point	O
)	O
1	O
:	O
if	O
tree	O
is	O
of	O
the	O
form	O
leaf	O
(	O
guess	O
)	O
then	O
2	O
:	O
3	O
:	O
else	O
if	O
tree	O
is	O
of	O
the	O
form	O
node	O
(	O
f	O
,	O
left	O
,	O
right	O
)	O
then	O
4	O
:	O
if	O
f	O
=	O
yes	O
in	O
test	O
point	O
then	O
return	O
guess	O
return	O
decisiontreetest	O
(	O
left	O
,	O
test	O
point	O
)	O
else	O
return	O
decisiontreetest	O
(	O
right	O
,	O
test	O
point	O
)	O
end	O
if	O
8	O
:	O
9	O
:	O
end	O
if	O
the	O
conquer	O
step	O
is	O
to	O
recurse	O
,	O
and	O
run	O
the	O
same	O
routine	O
(	O
choosing	O
the	O
feature	O
with	O
the	O
highest	O
score	O
)	O
on	O
the	O
no	O
set	O
(	O
to	O
get	O
the	O
left	O
half	O
of	O
the	O
tree	O
)	O
and	O
then	O
separately	O
on	O
the	O
yes	O
set	O
(	O
to	O
get	O
the	O
right	O
half	O
of	O
the	O
tree	O
)	O
.	O
at	O
some	O
point	O
it	O
will	O
become	O
useless	O
to	O
query	O
on	O
additional	O
fea-	O
tures	O
.	O
for	O
instance	O
,	O
once	O
you	O
know	O
that	O
this	O
is	O
a	O
systems	O
course	O
,	O
you	O
know	O
that	O
everyone	O
will	O
hate	O
it	O
.	O
so	O
you	O
can	O
immediately	O
predict	B
“	O
hate	O
”	O
without	O
asking	O
any	O
additional	O
questions	O
.	O
similarly	O
,	O
at	O
some	O
point	O
you	O
might	O
have	O
already	O
queried	O
every	O
available	O
feature	O
and	O
still	O
not	O
whittled	O
down	O
to	O
a	O
single	O
answer	O
.	O
in	O
both	O
cases	O
,	O
you	O
will	O
need	O
to	O
create	O
a	O
leaf	O
node	O
and	O
guess	O
the	O
most	O
prevalent	O
answer	O
in	O
the	O
current	O
piece	O
of	O
the	O
training	O
data	O
that	O
you	O
are	O
looking	O
at	O
.	O
putting	O
this	O
all	O
together	O
,	O
we	O
arrive	O
at	O
the	O
algorithm	O
shown	O
in	O
al-	O
gorithm	O
1.3.2	O
this	O
function	O
,	O
decisiontreetrain	O
takes	O
two	O
argu-	O
2	O
there	O
are	O
more	O
nuanced	O
algorithms	O
for	O
building	O
decision	B
trees	I
,	O
some	O
of	O
which	O
are	O
discussed	O
in	O
later	O
chapters	O
of	O
this	O
book	O
.	O
they	O
primarily	O
differ	O
in	O
how	O
they	O
compute	O
the	O
score	O
funciton	O
.	O
14	O
a	O
course	O
in	O
machine	O
learning	O
ments	O
:	O
our	O
data	O
,	O
and	O
the	O
set	O
of	O
as-yet	O
unused	O
features	B
.	O
it	O
has	O
two	O
base	O
cases	O
:	O
either	O
the	O
data	O
is	O
unambiguous	O
,	O
or	O
there	O
are	O
no	O
remaining	O
features	B
.	O
in	O
either	O
case	O
,	O
it	O
returns	O
a	O
leaf	O
node	O
containing	O
the	O
most	O
likely	O
guess	O
at	O
this	O
point	O
.	O
otherwise	O
,	O
it	O
loops	O
over	O
all	O
remaining	O
fea-	O
tures	O
to	O
ﬁnd	O
the	O
one	O
with	O
the	O
highest	O
score	O
.	O
it	O
then	O
partitions	O
the	O
data	O
into	O
a	O
no/yes	O
split	O
based	O
on	O
the	O
best	O
feature	O
.	O
it	O
constructs	O
its	O
left	O
and	O
right	O
subtrees	O
by	O
recursing	O
on	O
itself	O
.	O
in	O
each	O
recursive	O
call	O
,	O
it	O
uses	O
one	O
of	O
the	O
partitions	O
of	O
the	O
data	O
,	O
and	O
removes	O
the	O
just-selected	O
feature	O
from	O
consideration	O
.	O
the	O
corresponding	O
prediction	O
algorithm	B
is	O
shown	O
in	O
algorithm	B
1.3.	O
this	O
function	O
recurses	O
down	O
the	O
decision	O
tree	O
,	O
following	O
the	O
edges	O
speciﬁed	O
by	O
the	O
feature	O
values	O
in	O
some	O
test	O
point	O
.	O
when	O
it	O
reaches	O
a	O
leaf	O
,	O
it	O
returns	O
the	O
guess	O
associated	O
with	O
that	O
leaf	O
.	O
todo	O
:	O
deﬁne	O
outlier	O
somewhere	O
!	O
1.4	O
formalizing	O
the	O
learning	O
problem	O
as	O
you	O
’	O
ve	O
seen	O
,	O
there	O
are	O
several	O
issues	O
that	O
we	O
must	O
take	O
into	O
ac-	O
count	O
when	O
formalizing	O
the	O
notion	O
of	O
learning	O
.	O
•	O
the	O
performance	O
of	O
the	O
learning	O
algorithm	B
should	O
be	O
measured	O
on	O
unseen	O
“	O
test	O
”	O
data	O
.	O
•	O
the	O
way	O
in	O
which	O
we	O
measure	O
performance	O
should	O
depend	O
on	O
the	O
problem	O
we	O
are	O
trying	O
to	O
solve	O
.	O
•	O
there	O
should	O
be	O
a	O
strong	O
relationship	O
between	O
the	O
data	O
that	O
our	O
algorithm	B
sees	O
at	O
training	O
time	O
and	O
the	O
data	O
it	O
sees	O
at	O
test	O
time	O
.	O
in	O
order	O
to	O
accomplish	O
this	O
,	O
let	O
’	O
s	O
assume	O
that	O
someone	O
gives	O
us	O
a	O
loss	B
function	I
,	O
(	O
cid:96	O
)	O
(	O
·	O
,	O
·	O
)	O
,	O
of	O
two	O
arguments	O
.	O
the	O
job	O
of	O
(	O
cid:96	O
)	O
is	O
to	O
tell	O
us	O
how	O
“	O
bad	O
”	O
a	O
system	O
’	O
s	O
prediction	O
is	O
in	O
comparison	O
to	O
the	O
truth	O
.	O
in	O
particu-	O
lar	O
,	O
if	O
y	O
is	O
the	O
truth	O
and	O
ˆy	O
is	O
the	O
system	O
’	O
s	O
prediction	O
,	O
then	O
(	O
cid:96	O
)	O
(	O
y	O
,	O
ˆy	O
)	O
is	O
a	O
measure	O
of	O
error	O
.	O
for	O
three	O
of	O
the	O
canonical	O
tasks	O
discussed	O
above	O
,	O
we	O
might	O
use	O
the	O
following	O
loss	O
functions	O
:	O
regression	O
:	O
squared	B
loss	I
(	O
cid:96	O
)	O
(	O
y	O
,	O
ˆy	O
)	O
=	O
(	O
y	O
−	O
ˆy	O
)	O
2	O
or	O
absolute	B
loss	I
(	O
cid:96	O
)	O
(	O
y	O
,	O
ˆy	O
)	O
=	O
|y	O
−	O
ˆy|	O
.	O
binary	O
classiﬁcation	O
:	O
zero/one	B
loss	I
(	O
cid:96	O
)	O
(	O
y	O
,	O
ˆy	O
)	O
=	O
multiclass	O
classiﬁcation	O
:	O
also	O
zero/one	B
loss	I
.	O
(	O
cid:40	O
)	O
if	O
y	O
=	O
ˆy	O
0	O
1	O
otherwise	O
note	O
that	O
the	O
loss	O
function	O
is	O
something	O
that	O
you	O
must	O
decide	O
on	O
based	O
on	O
the	O
goals	O
of	O
learning	O
.	O
?	O
is	O
algorithm	B
1.3	O
guaranteed	O
to	O
terminate	O
?	O
this	O
notation	O
means	O
that	O
the	O
loss	O
is	O
zero	O
if	O
the	O
prediction	O
is	O
correct	O
and	O
is	O
one	O
otherwise	O
.	O
?	O
why	O
might	O
it	O
be	O
a	O
bad	O
idea	O
to	O
use	O
zero/one	B
loss	I
to	O
measure	O
perfor-	O
mance	O
for	O
a	O
regression	O
problem	O
?	O
now	O
that	O
we	O
have	O
deﬁned	O
our	O
loss	B
function	I
,	O
we	O
need	O
to	O
consider	O
where	O
the	O
data	O
(	O
training	O
and	O
test	O
)	O
comes	O
from	O
.	O
the	O
model	O
that	O
we	O
will	O
use	O
is	O
the	O
probabilistic	O
model	B
of	O
learning	O
.	O
namely	O
,	O
there	O
is	O
a	O
prob-	O
ability	O
distribution	O
d	O
over	O
input/output	O
pairs	O
.	O
this	O
is	O
often	O
called	O
the	O
data	O
generating	O
distribution	O
.	O
if	O
we	O
write	O
x	O
for	O
the	O
input	O
(	O
the	O
user/course	O
pair	O
)	O
and	O
y	O
for	O
the	O
output	O
(	O
the	O
rating	O
)	O
,	O
then	O
d	O
is	O
a	O
distri-	O
bution	O
over	O
(	O
x	O
,	O
y	O
)	O
pairs	O
.	O
a	O
useful	O
way	O
to	O
think	O
about	O
d	O
is	O
that	O
it	O
gives	O
high	O
probability	O
to	O
reasonable	O
(	O
x	O
,	O
y	O
)	O
pairs	O
,	O
and	O
low	O
probability	O
to	O
unreasonable	O
(	O
x	O
,	O
y	O
)	O
pairs	O
.	O
a	O
(	O
x	O
,	O
y	O
)	O
pair	O
can	O
be	O
unreasonable	O
in	O
two	O
ways	O
.	O
first	O
,	O
x	O
might	O
be	O
an	O
unusual	O
input	O
.	O
for	O
example	O
,	O
a	O
x	O
related	O
to	O
an	O
“	O
intro	O
to	O
java	O
”	O
course	O
might	O
be	O
highly	O
probable	O
;	O
a	O
x	O
related	O
to	O
a	O
“	O
geometric	O
and	O
solid	O
modeling	B
”	O
course	O
might	O
be	O
less	O
probable	O
.	O
second	O
,	O
y	O
might	O
be	O
an	O
unusual	O
rating	O
for	O
the	O
paired	O
x.	O
for	O
instance	O
,	O
if	O
alice	O
were	O
to	O
take	O
ai	O
100	O
times	O
(	O
without	O
remembering	O
that	O
she	O
took	O
it	O
before	O
!	O
)	O
,	O
she	O
would	O
give	O
the	O
course	O
a	O
+2	O
almost	O
every	O
time	O
.	O
perhaps	O
some	O
semesters	O
she	O
might	O
give	O
a	O
slightly	O
lower	O
score	O
,	O
but	O
it	O
would	O
be	O
un-	O
likely	O
to	O
see	O
x	O
=alice/ai	O
paired	O
with	O
y	O
=	O
−2	O
.	O
it	O
is	O
important	O
to	O
remember	O
that	O
we	O
are	O
not	O
making	O
any	O
assump-	O
tions	O
about	O
what	O
the	O
distribution	O
d	O
looks	O
like	O
.	O
(	O
for	O
instance	O
,	O
we	O
’	O
re	O
not	O
assuming	O
it	O
looks	O
like	O
a	O
gaussian	O
or	O
some	O
other	O
,	O
common	O
distri-	O
bution	O
.	O
)	O
we	O
are	O
also	O
not	O
assuming	O
that	O
we	O
know	O
what	O
d	O
is	O
.	O
in	O
fact	O
,	O
if	O
you	O
know	O
a	O
priori	O
what	O
your	O
data	B
generating	I
distribution	I
is	O
,	O
your	O
learning	O
problem	O
becomes	O
signiﬁcantly	O
easier	O
.	O
perhaps	O
the	O
hardest	O
thing	O
about	O
machine	O
learning	O
is	O
that	O
we	O
don	O
’	O
t	O
know	O
what	O
d	O
is	O
:	O
all	O
we	O
get	O
is	O
a	O
random	O
sample	O
from	O
it	O
.	O
this	O
random	O
sample	O
is	O
our	O
training	B
data	I
.	O
our	O
learning	O
problem	O
,	O
then	O
,	O
is	O
deﬁned	O
by	O
two	O
quantities	O
:	O
1.	O
the	O
loss	O
function	O
(	O
cid:96	O
)	O
,	O
which	O
captures	O
our	O
notion	O
of	O
what	O
is	O
important	O
to	O
learn	O
.	O
2.	O
the	O
data	O
generating	O
distribution	O
d	O
,	O
which	O
deﬁnes	O
what	O
sort	O
of	O
data	O
we	O
expect	O
to	O
see	O
.	O
decision	B
trees	I
15	O
?	O
consider	O
the	O
following	O
prediction	O
task	O
.	O
given	O
a	O
paragraph	O
written	O
about	O
a	O
course	O
,	O
we	O
have	O
to	O
predict	B
whether	O
the	O
paragraph	O
is	O
a	O
positive	O
or	O
negative	O
review	O
of	O
the	O
course	O
.	O
(	O
this	O
is	O
the	O
sentiment	O
analysis	O
prob-	O
lem	O
.	O
)	O
what	O
is	O
a	O
reasonable	O
loss	B
function	I
?	O
how	O
would	O
you	O
deﬁne	O
the	O
data	O
generating	O
distribution	O
?	O
we	O
are	O
given	O
access	O
to	O
training	B
data	I
,	O
which	O
is	O
a	O
random	O
sample	O
of	O
input/output	O
pairs	O
drawn	O
from	O
d.	O
based	O
on	O
this	O
training	B
data	I
,	O
we	O
need	O
to	O
induce	B
a	O
function	O
f	O
that	O
maps	O
new	O
inputs	O
ˆx	O
to	O
corresponding	O
prediction	O
ˆy	O
.	O
the	O
key	O
property	O
that	O
f	O
should	O
obey	O
is	O
that	O
it	O
should	O
do	O
well	O
(	O
as	O
measured	O
by	O
(	O
cid:96	O
)	O
)	O
on	O
future	O
examples	B
that	O
are	O
also	O
drawn	O
from	O
d.	O
formally	O
,	O
it	O
’	O
s	O
expected	B
loss	I
	O
over	O
d	O
with	O
repsect	O
to	O
(	O
cid:96	O
)	O
should	O
be	O
as	O
small	O
as	O
possible	O
:	O
(	O
x	O
,	O
y	O
)	O
∼d	O
(	O
cid:2	O
)	O
(	O
cid:96	O
)	O
(	O
y	O
,	O
f	O
(	O
x	O
)	O
)	O
(	O
cid:3	O
)	O
=	O
∑	O
	O
(	O
cid:44	O
)	O
e	O
d	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:96	O
)	O
(	O
y	O
,	O
f	O
(	O
x	O
)	O
)	O
(	O
1.1	O
)	O
(	O
x	O
,	O
y	O
)	O
16	O
a	O
course	O
in	O
machine	O
learning	O
the	O
difﬁculty	O
in	O
minimizing	O
our	O
expected	B
loss	I
from	O
eq	O
(	O
1.1	O
)	O
is	O
that	O
we	O
don	O
’	O
t	O
know	O
what	O
d	O
is	O
!	O
all	O
we	O
have	O
access	O
to	O
is	O
some	O
training	B
data	I
sampled	O
from	O
it	O
!	O
suppose	O
that	O
we	O
denote	O
our	O
training	B
data	I
set	O
by	O
d.	O
the	O
training	O
data	O
consists	O
of	O
n-many	O
input/output	O
pairs	O
,	O
(	O
x1	O
,	O
y1	O
)	O
,	O
(	O
x2	O
,	O
y2	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xn	O
,	O
yn	O
)	O
.	O
given	O
a	O
learned	O
function	O
f	O
,	O
we	O
can	O
compute	O
our	O
training	B
error	I
,	O
ˆ	O
:	O
ˆ	O
(	O
cid:44	O
)	O
1	O
n	O
n∑	O
n=1	O
(	O
cid:96	O
)	O
(	O
yn	O
,	O
f	O
(	O
xn	O
)	O
)	O
(	O
1.8	O
)	O
that	O
is	O
,	O
our	O
training	B
error	I
is	O
simply	O
our	O
average	O
error	O
over	O
the	O
train-	O
ing	O
data	O
.	O
of	O
course	O
,	O
we	O
can	O
drive	O
ˆ	O
to	O
zero	O
by	O
simply	O
memorizing	O
our	O
train-	O
ing	O
data	O
.	O
but	O
as	O
alice	O
might	O
ﬁnd	O
in	O
memorizing	O
past	O
exams	O
,	O
this	O
might	O
not	O
generalize	B
well	O
to	O
a	O
new	O
exam	O
!	O
this	O
is	O
the	O
fundamental	O
difﬁculty	O
in	O
machine	O
learning	O
:	O
the	O
thing	O
we	O
have	O
access	O
to	O
is	O
our	O
training	B
error	I
,	O
ˆ	O
.	O
but	O
the	O
thing	O
we	O
care	O
about	O
minimizing	O
is	O
our	O
expected	O
error	O
	O
.	O
in	O
order	O
to	O
get	O
the	O
expected	O
error	O
down	O
,	O
our	O
learned	O
function	O
needs	O
to	O
generalize	B
beyond	O
the	O
training	O
data	O
to	O
some	O
future	O
data	O
that	O
it	O
might	O
not	O
have	O
seen	O
yet	O
!	O
so	O
,	O
putting	O
it	O
all	O
together	O
,	O
we	O
get	O
a	O
formal	O
deﬁnition	O
of	O
induction	B
machine	O
learning	O
:	O
given	O
(	O
i	O
)	O
a	O
loss	B
function	I
(	O
cid:96	O
)	O
and	O
(	O
ii	O
)	O
a	O
sample	O
d	O
from	O
some	O
unknown	O
distribution	O
d	O
,	O
you	O
must	O
compute	O
a	O
function	O
f	O
that	O
has	O
low	O
expected	O
error	O
	O
over	O
d	O
with	O
respect	O
to	O
(	O
cid:96	O
)	O
.	O
?	O
1.5	O
inductive	B
bias	I
:	O
what	O
we	O
know	O
before	O
the	O
data	O
arrives	O
(	O
x	O
,	O
y	O
)	O
∼d	O
(	O
cid:2	O
)	O
(	O
cid:96	O
)	O
(	O
y	O
,	O
f	O
(	O
x	O
)	O
)	O
(	O
cid:3	O
)	O
,	O
by	O
thinking	O
verify	O
by	O
calculation	O
that	O
we	O
can	O
write	O
our	O
training	B
error	I
as	O
e	O
of	O
d	O
as	O
a	O
distribution	O
that	O
places	O
probability	O
1/n	O
to	O
each	O
example	O
in	O
d	O
and	O
probabiliy	O
0	O
on	O
everything	O
else	O
.	O
decision	B
trees	I
17	O
math	O
review	O
|	O
expectated	O
values	O
(	O
x	O
,	O
y	O
)	O
∼d	O
[	O
(	O
cid:96	O
)	O
(	O
y	O
,	O
f	O
(	O
x	O
)	O
)	O
]	O
for	O
the	O
expected	O
loss	O
.	O
here	O
,	O
as	O
always	O
,	O
in	O
this	O
book	O
,	O
we	O
will	O
often	O
write	O
things	O
like	O
e	O
expectation	O
means	O
“	O
average.	O
”	O
in	O
words	O
,	O
this	O
is	O
saying	O
“	O
if	O
you	O
drew	O
a	O
bunch	O
of	O
(	O
x	O
,	O
y	O
)	O
pairs	O
indepen-	O
dently	O
at	O
random	O
from	O
d	O
,	O
what	O
would	O
your	O
average	O
loss	O
be	O
?	O
(	O
more	O
formally	O
,	O
what	O
would	O
be	O
the	O
aver-	O
age	O
of	O
(	O
cid:96	O
)	O
(	O
y	O
,	O
f	O
(	O
x	O
)	O
)	O
be	O
over	O
these	O
random	O
draws	O
?	O
)	O
more	O
formally	O
,	O
if	O
d	O
is	O
a	O
discrete	O
probability	O
distribution	O
,	O
then	O
this	O
expectation	O
can	O
be	O
expanded	O
as	O
:	O
e	O
(	O
x	O
,	O
y	O
)	O
∼d	O
[	O
(	O
cid:96	O
)	O
(	O
y	O
,	O
f	O
(	O
x	O
)	O
)	O
]	O
=	O
∑	O
(	O
x	O
,	O
y	O
)	O
∈d	O
[	O
d	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:96	O
)	O
(	O
y	O
,	O
f	O
(	O
x	O
)	O
)	O
]	O
(	O
1.2	O
)	O
this	O
is	O
exactly	O
the	O
weighted	O
average	O
loss	O
over	O
the	O
all	O
(	O
x	O
,	O
y	O
)	O
pairs	O
in	O
d	O
,	O
weighted	O
by	O
their	O
probability	O
(	O
namely	O
,	O
d	O
(	O
x	O
,	O
y	O
)	O
)	O
under	O
this	O
distribution	O
d.	O
in	O
particular	O
,	O
if	O
d	O
is	O
a	O
ﬁnite	O
discrete	B
distribution	I
,	O
for	O
instance	O
one	O
deﬁned	O
by	O
a	O
ﬁnite	O
data	O
set	O
{	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xn	O
,	O
yn	O
)	O
that	O
puts	O
equal	O
weight	O
on	O
each	O
example	O
(	O
in	O
this	O
case	O
,	O
equal	O
weight	O
means	O
proba-	O
bility	O
1/n	O
)	O
,	O
then	O
we	O
get	O
:	O
e	O
(	O
x	O
,	O
y	O
)	O
∼d	O
[	O
(	O
cid:96	O
)	O
(	O
y	O
,	O
f	O
(	O
x	O
)	O
)	O
]	O
=	O
∑	O
(	O
x	O
,	O
y	O
)	O
∈d	O
n∑	O
=	O
[	O
d	O
(	O
xn	O
,	O
yn	O
)	O
(	O
cid:96	O
)	O
(	O
yn	O
,	O
f	O
(	O
xn	O
)	O
)	O
]	O
[	O
d	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:96	O
)	O
(	O
y	O
,	O
f	O
(	O
x	O
)	O
)	O
]	O
deﬁnition	O
of	O
expectation	O
d	O
is	O
discrete	O
and	O
ﬁnite	O
deﬁnition	O
of	O
d	O
rearranging	O
terms	O
(	O
1.3	O
)	O
(	O
1.4	O
)	O
(	O
1.5	O
)	O
(	O
1.6	O
)	O
n=1	O
n∑	O
n=1	O
1	O
n	O
[	O
1	O
n	O
n∑	O
n=1	O
=	O
=	O
(	O
cid:96	O
)	O
(	O
yn	O
,	O
f	O
(	O
xn	O
)	O
)	O
]	O
[	O
(	O
cid:96	O
)	O
(	O
yn	O
,	O
f	O
(	O
xn	O
)	O
)	O
]	O
(	O
cid:90	O
)	O
which	O
is	O
exactly	O
the	O
average	O
loss	O
on	O
that	O
dataset	O
.	O
in	O
the	O
case	O
that	O
the	O
distribution	O
is	O
continuous	O
,	O
we	O
need	O
to	O
replace	O
the	O
discrete	O
sum	O
with	O
a	O
continuous	O
integral	O
over	O
some	O
space	O
ω	O
:	O
e	O
(	O
x	O
,	O
y	O
)	O
∼d	O
[	O
(	O
cid:96	O
)	O
(	O
y	O
,	O
f	O
(	O
x	O
)	O
)	O
]	O
=	O
d	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:96	O
)	O
(	O
y	O
,	O
f	O
(	O
x	O
)	O
)	O
dxdy	O
ω	O
(	O
1.7	O
)	O
this	O
is	O
exactly	O
the	O
same	O
but	O
in	O
continuous	O
space	O
rather	O
than	O
discrete	O
space	O
.	O
the	O
most	O
important	O
thing	O
to	O
remember	O
is	O
that	O
there	O
are	O
two	O
equivalent	O
ways	O
to	O
think	O
about	O
expections	O
:	O
1.	O
the	O
expectation	O
of	O
some	O
function	O
g	O
is	O
the	O
weighted	O
average	O
value	O
of	O
g	O
,	O
where	O
the	O
weights	O
are	O
given	O
by	O
the	O
underlying	O
probability	O
distribution	O
.	O
2.	O
the	O
expectation	O
of	O
some	O
function	O
g	O
is	O
your	O
best	O
guess	O
of	O
the	O
value	O
of	O
g	O
if	O
you	O
were	O
to	O
draw	O
a	O
single	O
item	O
from	O
the	O
underlying	O
probability	O
distribution	O
.	O
figure	O
1.4	O
:	O
18	O
a	O
course	O
in	O
machine	O
learning	O
in	O
figure	O
1.5	O
you	O
’	O
ll	O
ﬁnd	O
training	B
data	I
for	O
a	O
binary	O
classiﬁcation	O
problem	O
.	O
the	O
two	O
labels	O
are	O
“	O
a	O
”	O
and	O
“	O
b	O
”	O
and	O
you	O
can	O
see	O
ﬁve	O
exam-	O
ples	O
for	O
each	O
label	B
.	O
below	O
,	O
in	O
figure	O
1.6	O
,	O
you	O
will	O
see	O
some	O
test	B
data	I
.	O
these	O
images	O
are	O
left	O
unlabeled	O
.	O
go	O
through	O
quickly	O
and	O
,	O
based	O
on	O
the	O
training	O
data	O
,	O
label	B
these	O
images	O
.	O
(	O
really	O
do	O
it	O
before	O
you	O
read	O
further	O
!	O
i	O
’	O
ll	O
wait	O
!	O
)	O
most	O
likely	O
you	O
produced	O
one	O
of	O
two	O
labelings	O
:	O
either	O
abbaab	O
or	O
abbaba	O
.	O
which	O
of	O
these	O
solutions	O
is	O
right	O
?	O
the	O
answer	O
is	O
that	O
you	O
can	O
not	O
tell	O
based	O
on	O
the	O
training	O
data	O
.	O
if	O
you	O
give	O
this	O
same	O
example	O
to	O
100	O
people	O
,	O
60	O
−	O
70	O
of	O
them	O
come	O
up	O
with	O
the	O
abbaab	O
prediction	O
and	O
30	O
−	O
40	O
come	O
up	O
with	O
the	O
abbaba	O
prediction	O
.	O
why	O
are	O
they	O
doing	O
this	O
?	O
presumably	O
because	O
the	O
ﬁrst	O
group	O
believes	O
that	O
the	O
relevant	O
distinction	O
is	O
between	O
“	O
bird	O
”	O
and	O
“	O
non-bird	O
”	O
while	O
the	O
second	O
group	O
believes	O
that	O
the	O
relevant	O
distinc-	O
tion	O
is	O
between	O
“	O
ﬂy	O
”	O
and	O
“	O
no-ﬂy.	O
”	O
this	O
preference	O
for	O
one	O
distinction	O
(	O
bird/non-bird	O
)	O
over	O
another	O
(	O
ﬂy/no-ﬂy	O
)	O
is	O
a	O
bias	B
that	O
different	O
human	O
learners	O
have	O
.	O
in	O
the	O
con-	O
text	O
of	O
machine	O
learning	O
,	O
it	O
is	O
called	O
inductive	B
bias	I
:	O
in	O
the	O
absense	O
of	O
data	O
that	O
narrow	O
down	O
the	O
relevant	O
concept	B
,	O
what	O
type	O
of	O
solutions	O
are	O
we	O
more	O
likely	O
to	O
prefer	O
?	O
two	O
thirds	O
of	O
people	O
seem	O
to	O
have	O
an	O
inductive	B
bias	I
in	O
favor	O
of	O
bird/non-bird	O
,	O
and	O
one	O
third	O
seem	O
to	O
have	O
an	O
inductive	B
bias	I
in	O
favor	O
of	O
ﬂy/no-ﬂy	O
.	O
throughout	O
this	O
book	O
you	O
will	O
learn	O
about	O
several	O
approaches	O
to	O
machine	O
learning	O
.	O
the	O
decision	O
tree	O
model	B
is	O
the	O
ﬁrst	O
such	O
approach	O
.	O
these	O
approaches	O
differ	O
primarily	O
in	O
the	O
sort	O
of	O
inductive	B
bias	I
that	O
they	O
exhibit	O
.	O
consider	O
a	O
variant	O
of	O
the	O
decision	O
tree	O
learning	O
algorithm	B
.	O
in	O
this	O
variant	O
,	O
we	O
will	O
not	O
allow	O
the	O
trees	O
to	O
grow	O
beyond	O
some	O
pre-deﬁned	O
maximum	B
depth	I
,	O
d.	O
that	O
is	O
,	O
once	O
we	O
have	O
queried	O
on	O
d-many	O
fea-	O
tures	O
,	O
we	O
can	O
not	O
query	O
on	O
any	O
more	O
and	O
must	O
just	O
make	O
the	O
best	O
guess	O
we	O
can	O
at	O
that	O
point	O
.	O
this	O
variant	O
is	O
called	O
a	O
shallow	B
decision	I
tree	I
.	O
the	O
key	O
question	O
is	O
:	O
what	O
is	O
the	O
inductive	O
bias	B
of	O
shallow	O
decision	O
trees	O
?	O
roughly	O
,	O
their	O
bias	B
is	O
that	O
decisions	O
can	O
be	O
made	O
by	O
only	O
look-	O
ing	O
at	O
a	O
small	O
number	O
of	O
features	B
.	O
for	O
instance	O
,	O
a	O
shallow	B
decision	I
tree	I
would	O
be	O
very	O
good	O
at	O
learning	O
a	O
function	O
like	O
“	O
students	O
only	O
like	O
ai	O
courses.	O
”	O
it	O
would	O
be	O
very	O
bad	O
at	O
learning	O
a	O
function	O
like	O
“	O
if	O
this	O
student	O
has	O
liked	O
an	O
odd	O
number	O
of	O
his	O
past	O
courses	O
,	O
he	O
will	O
like	O
the	O
next	O
one	O
;	O
otherwise	O
he	O
will	O
not.	O
”	O
this	O
latter	O
is	O
the	O
parity	O
function	O
,	O
which	O
requires	O
you	O
to	O
inspect	O
every	O
feature	O
to	O
make	O
a	O
prediction	O
.	O
the	O
inductive	O
bias	B
of	O
a	O
decision	B
tree	I
is	O
that	O
the	O
sorts	O
of	O
things	O
we	O
want	O
to	O
learn	O
to	O
predict	B
are	O
more	O
like	O
the	O
ﬁrst	O
example	O
and	O
less	O
like	O
the	O
second	O
example	O
.	O
figure	O
1.5	O
:	O
dt	O
:	O
bird	O
:	O
bird	O
training	O
images	O
figure	O
1.6	O
:	O
dt	O
:	O
birdtest	O
:	O
bird	O
test	O
images	O
?	O
it	O
is	O
also	O
possible	O
that	O
the	O
correct	O
classiﬁcation	O
on	O
the	O
test	O
data	O
is	O
babaaa	O
.	O
this	O
corresponds	O
to	O
the	O
bias	O
“	O
is	O
the	O
background	O
in	O
focus.	O
”	O
somehow	O
no	O
one	O
seems	O
to	O
come	O
up	O
with	O
this	O
classiﬁcation	O
rule	O
.	O
decision	B
trees	I
19	O
1.6	O
not	O
everything	O
is	O
learnable	O
although	O
machine	O
learning	O
works	O
well—perhaps	O
astonishingly	O
well—in	O
many	O
cases	O
,	O
it	O
is	O
important	O
to	O
keep	O
in	O
mind	O
that	O
it	O
is	O
not	O
magical	O
.	O
there	O
are	O
many	O
reasons	O
why	O
a	O
machine	O
learning	O
algorithm	B
might	O
fail	O
on	O
some	O
learning	O
task	O
.	O
there	O
could	O
be	O
noise	B
in	O
the	O
training	O
data	O
.	O
noise	B
can	O
occur	O
both	O
at	O
the	O
feature	O
level	O
and	O
at	O
the	O
label	O
level	O
.	O
some	O
features	B
might	O
corre-	O
spond	O
to	O
measurements	O
taken	O
by	O
sensors	O
.	O
for	O
instance	O
,	O
a	O
robot	O
might	O
use	O
a	O
laser	O
range	O
ﬁnder	O
to	O
compute	O
its	O
distance	B
to	O
a	O
wall	O
.	O
however	O
,	O
this	O
sensor	O
might	O
fail	O
and	O
return	O
an	O
incorrect	O
value	O
.	O
in	O
a	O
sentiment	O
classiﬁcation	O
problem	O
,	O
someone	O
might	O
have	O
a	O
typo	O
in	O
their	O
review	O
of	O
a	O
course	O
.	O
these	O
would	O
lead	O
to	O
noise	B
at	O
the	O
feature	O
level	O
.	O
there	O
might	O
also	O
be	O
noise	B
at	O
the	O
label	O
level	O
.	O
a	O
student	O
might	O
write	O
a	O
scathingly	O
negative	O
review	O
of	O
a	O
course	O
,	O
but	O
then	O
accidentally	O
click	O
the	O
wrong	O
button	O
for	O
the	O
course	O
rating	O
.	O
the	O
features	O
available	O
for	O
learning	O
might	O
simply	O
be	O
insufﬁcient	O
.	O
for	O
example	O
,	O
in	O
a	O
medical	O
context	O
,	O
you	O
might	O
wish	O
to	O
diagnose	O
whether	O
a	O
patient	O
has	O
cancer	O
or	O
not	O
.	O
you	O
may	O
be	O
able	O
to	O
collect	O
a	O
large	O
amount	O
of	O
data	O
about	O
this	O
patient	O
,	O
such	O
as	O
gene	O
expressions	O
,	O
x-rays	O
,	O
family	O
histories	O
,	O
etc	O
.	O
but	O
,	O
even	O
knowing	O
all	O
of	O
this	O
information	O
exactly	O
,	O
it	O
might	O
still	O
be	O
impossible	O
to	O
judge	O
for	O
sure	O
whether	O
this	O
pa-	O
tient	O
has	O
cancer	O
or	O
not	O
.	O
as	O
a	O
more	O
contrived	O
example	O
,	O
you	O
might	O
try	O
to	O
classify	O
course	O
reviews	O
as	O
positive	O
or	O
negative	O
.	O
but	O
you	O
may	O
have	O
erred	O
when	O
downloading	O
the	O
data	O
and	O
only	O
gotten	O
the	O
ﬁrst	O
ﬁve	O
char-	O
acters	O
of	O
each	O
review	O
.	O
if	O
you	O
had	O
the	O
rest	O
of	O
the	O
features	O
you	O
might	O
be	O
able	O
to	O
do	O
well	O
.	O
but	O
with	O
this	O
limited	O
feature	O
set	O
,	O
there	O
’	O
s	O
not	O
much	O
you	O
can	O
do	O
.	O
some	O
examples	B
may	O
not	O
have	O
a	O
single	O
correct	O
answer	O
.	O
you	O
might	O
be	O
building	O
a	O
system	O
for	O
“	O
safe	O
web	O
search	O
,	O
”	O
which	O
removes	O
offen-	O
sive	O
web	O
pages	O
from	O
search	O
results	O
.	O
to	O
build	O
this	O
system	O
,	O
you	O
would	O
collect	O
a	O
set	O
of	O
web	O
pages	O
and	O
ask	O
people	O
to	O
classify	O
them	O
as	O
“	O
offen-	O
sive	O
”	O
or	O
not	O
.	O
however	O
,	O
what	O
one	O
person	O
considers	O
offensive	O
might	O
be	O
completely	O
reasonable	O
for	O
another	O
person	O
.	O
it	O
is	O
common	O
to	O
consider	O
this	O
as	O
a	O
form	O
of	O
label	B
noise	O
.	O
nevertheless	O
,	O
since	O
you	O
,	O
as	O
the	O
designer	O
of	O
the	O
learning	O
system	O
,	O
have	O
some	O
control	O
over	O
this	O
problem	O
,	O
it	O
is	O
sometimes	O
helpful	O
to	O
isolate	O
it	O
as	O
a	O
source	O
of	O
difﬁculty	O
.	O
finally	O
,	O
learning	O
might	O
fail	O
because	O
the	O
inductive	O
bias	B
of	O
the	O
learn-	O
ing	O
algorithm	B
is	O
too	O
far	O
away	O
from	O
the	O
concept	O
that	O
is	O
being	O
learned	O
.	O
in	O
the	O
bird/non-bird	O
data	O
,	O
you	O
might	O
think	O
that	O
if	O
you	O
had	O
gotten	O
a	O
few	O
more	O
training	O
examples	O
,	O
you	O
might	O
have	O
been	O
able	O
to	O
tell	O
whether	O
this	O
was	O
intended	O
to	O
be	O
a	O
bird/non-bird	O
classiﬁcation	O
or	O
a	O
ﬂy/no-ﬂy	O
classiﬁcation	O
.	O
however	O
,	O
no	O
one	O
i	O
’	O
ve	O
talked	O
to	O
has	O
ever	O
come	O
up	O
with	O
the	O
“	O
background	O
is	O
in	O
focus	O
”	O
classiﬁcation	O
.	O
even	O
with	O
many	O
20	O
a	O
course	O
in	O
machine	O
learning	O
more	O
training	O
points	O
,	O
this	O
is	O
such	O
an	O
unusual	O
distinction	O
that	O
it	O
may	O
be	O
hard	O
for	O
anyone	O
to	O
ﬁgure	O
out	O
it	O
.	O
in	O
this	O
case	O
,	O
the	O
inductive	O
bias	B
of	O
the	O
learner	O
is	O
simply	O
too	O
misaligned	O
with	O
the	O
target	O
classiﬁcation	O
to	O
learn	O
.	O
note	O
that	O
the	O
inductive	O
bias	B
source	O
of	O
error	O
is	O
fundamentally	O
dif-	O
ferent	O
than	O
the	O
other	O
three	O
sources	O
of	O
error	O
.	O
in	O
the	O
inductive	O
bias	B
case	O
,	O
it	O
is	O
the	O
particular	O
learning	O
algorithm	B
that	O
you	O
are	O
using	O
that	O
can	O
not	O
cope	O
with	O
the	O
data	O
.	O
maybe	O
if	O
you	O
switched	O
to	O
a	O
different	O
learning	O
algorithm	B
,	O
you	O
would	O
be	O
able	O
to	O
learn	O
well	O
.	O
for	O
instance	O
,	O
neptunians	O
might	O
have	O
evolved	O
to	O
care	O
greatly	O
about	O
whether	O
backgrounds	O
are	O
in	O
focus	O
,	O
and	O
for	O
them	O
this	O
would	O
be	O
an	O
easy	O
classiﬁcation	O
to	O
learn	O
.	O
for	O
the	O
other	O
three	O
sources	O
of	O
error	O
,	O
it	O
is	O
not	O
an	O
issue	O
to	O
do	O
with	O
the	O
particular	O
learning	O
algorithm	B
.	O
the	O
error	O
is	O
a	O
fundamental	O
part	O
of	O
the	O
learning	O
problem	O
.	O
1.7	O
underﬁtting	O
and	O
overﬁtting	O
as	O
with	O
many	O
problems	O
,	O
it	O
is	O
useful	O
to	O
think	O
about	O
the	O
extreme	O
cases	O
of	O
learning	O
algorithms	O
.	O
in	O
particular	O
,	O
the	O
extreme	O
cases	O
of	O
decision	B
trees	I
.	O
in	O
one	O
extreme	O
,	O
the	O
tree	O
is	O
“	O
empty	O
”	O
and	O
we	O
do	O
not	O
ask	O
any	O
questions	O
at	O
all	O
.	O
we	O
simply	O
immediately	O
make	O
a	O
prediction	O
.	O
in	O
the	O
other	O
extreme	O
,	O
the	O
tree	O
is	O
“	O
full.	O
”	O
that	O
is	O
,	O
every	O
possible	O
question	O
is	O
asked	O
along	O
every	O
branch	O
.	O
in	O
the	O
full	O
tree	O
,	O
there	O
may	O
be	O
leaves	O
with	O
no	O
associated	O
training	B
data	I
.	O
for	O
these	O
we	O
must	O
simply	O
choose	O
arbitrarily	O
whether	O
to	O
say	O
“	O
yes	O
”	O
or	O
“	O
no.	O
”	O
consider	O
the	O
course	O
recommendation	O
data	O
from	O
table	O
?	O
?	O
.	O
sup-	O
pose	O
we	O
were	O
to	O
build	O
an	O
“	O
empty	O
”	O
decision	B
tree	I
on	O
this	O
data	O
.	O
such	O
a	O
decision	B
tree	I
will	O
make	O
the	O
same	O
prediction	O
regardless	O
of	O
its	O
input	O
,	O
because	O
it	O
is	O
not	O
allowed	O
to	O
ask	O
any	O
questions	O
about	O
its	O
input	O
.	O
since	O
there	O
are	O
more	O
“	O
likes	O
”	O
than	O
“	O
hates	O
”	O
in	O
the	O
training	O
data	O
(	O
12	O
versus	O
8	O
)	O
,	O
our	O
empty	O
decision	B
tree	I
will	O
simply	O
always	O
predict	B
“	O
likes.	O
”	O
the	O
training	O
error	O
,	O
ˆ	O
,	O
is	O
8/20	O
=	O
40	O
%	O
.	O
on	O
the	O
other	O
hand	O
,	O
we	O
could	O
build	O
a	O
“	O
full	O
”	O
decision	B
tree	I
.	O
since	O
each	O
row	O
in	O
this	O
data	O
is	O
unique	O
,	O
we	O
can	O
guarantee	O
that	O
any	O
leaf	O
in	O
a	O
full	O
decision	B
tree	I
will	O
have	O
either	O
0	O
or	O
1	O
examples	B
assigned	O
to	O
it	O
(	O
20	O
of	O
the	O
leaves	O
will	O
have	O
one	O
example	O
;	O
the	O
rest	O
will	O
have	O
none	O
)	O
.	O
for	O
the	O
leaves	O
corresponding	O
to	O
training	O
points	O
,	O
the	O
full	O
decision	B
tree	I
will	O
always	O
make	O
the	O
correct	O
prediction	O
.	O
given	O
this	O
,	O
the	O
training	O
error	O
,	O
ˆ	O
,	O
is	O
0/20	O
=	O
0	O
%	O
.	O
of	O
course	O
our	O
goal	O
is	O
not	O
to	O
build	O
a	O
model	B
that	O
gets	O
0	O
%	O
error	O
on	O
the	O
training	O
data	O
.	O
this	O
would	O
be	O
easy	O
!	O
our	O
goal	O
is	O
a	O
model	B
that	O
will	O
do	O
well	O
on	O
future	O
,	O
unseen	O
data	O
.	O
how	O
well	O
might	O
we	O
expect	O
these	O
two	O
models	O
to	O
do	O
on	O
future	O
data	O
?	O
the	O
“	O
empty	O
”	O
tree	O
is	O
likely	O
to	O
do	O
not	O
much	O
better	O
and	O
not	O
much	O
worse	O
on	O
future	O
data	O
.	O
we	O
might	O
expect	O
decision	B
trees	I
21	O
?	O
convince	O
yourself	O
(	O
either	O
by	O
proof	O
or	O
by	O
simulation	O
)	O
that	O
even	O
in	O
the	O
case	O
of	O
imbalanced	B
data	I
–	O
for	O
in-	O
stance	O
data	O
that	O
is	O
on	O
average	O
80	O
%	O
positive	O
and	O
20	O
%	O
negative	O
–	O
a	O
pre-	O
dictor	O
that	O
guesses	O
randomly	O
(	O
50/50	O
positive/negative	O
)	O
will	O
get	O
about	O
50	O
%	O
error	O
.	O
?	O
which	O
feature	O
is	O
it	O
,	O
and	O
what	O
is	O
it	O
’	O
s	O
training	B
error	I
?	O
that	O
it	O
would	O
continue	O
to	O
get	O
around	O
40	O
%	O
error	O
.	O
life	O
is	O
more	O
complicated	O
for	O
the	O
“	O
full	O
”	O
decision	B
tree	I
.	O
certainly	O
if	O
it	O
is	O
given	O
a	O
test	O
example	O
that	O
is	O
identical	O
to	O
one	O
of	O
the	O
training	O
examples	B
,	O
it	O
will	O
do	O
the	O
right	O
thing	O
(	O
assuming	O
no	O
noise	B
)	O
.	O
but	O
for	O
everything	O
else	O
,	O
it	O
will	O
only	O
get	O
about	O
50	O
%	O
error	O
.	O
this	O
means	O
that	O
even	O
if	O
every	O
other	O
test	O
point	O
happens	O
to	O
be	O
identical	O
to	O
one	O
of	O
the	O
training	O
points	O
,	O
it	O
would	O
only	O
get	O
about	O
25	O
%	O
error	O
.	O
in	O
practice	O
,	O
this	O
is	O
probably	O
optimistic	O
,	O
and	O
maybe	O
only	O
one	O
in	O
every	O
10	O
examples	B
would	O
match	O
a	O
training	O
example	O
,	O
yielding	O
a	O
35	O
%	O
error	O
.	O
so	O
,	O
in	O
one	O
case	O
(	O
empty	O
tree	O
)	O
we	O
’	O
ve	O
achieved	O
about	O
40	O
%	O
error	O
and	O
in	O
the	O
other	O
case	O
(	O
full	O
tree	O
)	O
we	O
’	O
ve	O
achieved	O
35	O
%	O
error	O
.	O
this	O
is	O
not	O
very	O
promising	O
!	O
one	O
would	O
hope	O
to	O
do	O
better	O
!	O
in	O
fact	O
,	O
you	O
might	O
notice	O
that	O
if	O
you	O
simply	O
queried	O
on	O
a	O
single	O
feature	O
for	O
this	O
data	O
,	O
you	O
would	O
be	O
able	O
to	O
get	O
very	O
low	O
training	B
error	I
,	O
but	O
wouldn	O
’	O
t	O
be	O
forced	O
to	O
“	O
guess	O
”	O
randomly	O
.	O
this	O
example	O
illustrates	O
the	O
key	O
concepts	O
of	O
underﬁtting	O
and	O
overﬁtting	O
.	O
underﬁtting	O
is	O
when	O
you	O
had	O
the	O
opportunity	O
to	O
learn	O
something	O
but	O
didn	O
’	O
t	O
.	O
a	O
student	O
who	O
hasn	O
’	O
t	O
studied	O
much	O
for	O
an	O
up-	O
coming	O
exam	O
will	O
be	O
underﬁt	O
to	O
the	O
exam	O
,	O
and	O
consequently	O
will	O
not	O
do	O
well	O
.	O
this	O
is	O
also	O
what	O
the	O
empty	O
tree	O
does	O
.	O
overﬁtting	O
is	O
when	O
you	O
pay	O
too	O
much	O
attention	O
to	O
idiosyncracies	O
of	O
the	O
training	O
data	O
,	O
and	O
aren	O
’	O
t	O
able	O
to	O
generalize	B
well	O
.	O
often	O
this	O
means	O
that	O
your	O
model	B
is	O
ﬁtting	O
noise	B
,	O
rather	O
than	O
whatever	O
it	O
is	O
supposed	O
to	O
ﬁt	O
.	O
a	O
student	O
who	O
memorizes	O
answers	O
to	O
past	O
exam	O
questions	O
without	O
understand-	O
ing	O
them	O
has	O
overﬁt	O
the	O
training	O
data	O
.	O
like	O
the	O
full	O
tree	O
,	O
this	O
student	O
also	O
will	O
not	O
do	O
well	O
on	O
the	O
exam	O
.	O
a	O
model	B
that	O
is	O
neither	O
overﬁt	O
nor	O
underﬁt	O
is	O
the	O
one	O
that	O
is	O
expected	O
to	O
do	O
best	O
in	O
the	O
future	O
.	O
1.8	O
separation	O
of	O
training	O
and	O
test	B
data	I
suppose	O
that	O
,	O
after	O
graduating	O
,	O
you	O
get	O
a	O
job	O
working	O
for	O
a	O
company	O
that	O
provides	O
personalized	O
recommendations	O
for	O
pottery	O
.	O
you	O
go	O
in	O
and	O
implement	O
new	O
algorithms	O
based	O
on	O
what	O
you	O
learned	O
in	O
your	O
machine	O
learning	O
class	O
(	O
you	O
have	O
learned	O
the	O
power	O
of	O
generaliza-	O
tion	O
!	O
)	O
.	O
all	O
you	O
need	O
to	O
do	O
now	O
is	O
convince	O
your	O
boss	O
that	O
you	O
have	O
done	O
a	O
good	O
job	O
and	O
deserve	O
a	O
raise	O
!	O
how	O
can	O
you	O
convince	O
your	O
boss	O
that	O
your	O
fancy	O
learning	O
algo-	O
rithms	O
are	O
really	O
working	O
?	O
based	O
on	O
what	O
we	O
’	O
ve	O
talked	O
about	O
already	O
with	O
underﬁtting	O
and	O
overﬁtting	O
,	O
it	O
is	O
not	O
enough	O
to	O
just	O
tell	O
your	O
boss	O
what	O
your	O
training	B
error	I
is	O
.	O
noise	B
notwithstanding	O
,	O
it	O
is	O
easy	O
to	O
get	O
a	O
training	B
error	I
of	O
zero	O
using	O
a	O
simple	O
database	O
query	O
(	O
or	O
grep	O
,	O
if	O
you	O
prefer	O
)	O
.	O
your	O
boss	O
will	O
not	O
fall	O
for	O
that	O
.	O
the	O
easiest	O
approach	O
is	O
to	O
set	O
aside	O
some	O
of	O
your	O
available	O
data	O
as	O
22	O
a	O
course	O
in	O
machine	O
learning	O
“	O
test	B
data	I
”	O
and	O
use	O
this	O
to	O
evaluate	O
the	O
performance	O
of	O
your	O
learning	O
algorithm	B
.	O
for	O
instance	O
,	O
the	O
pottery	O
recommendation	O
service	O
that	O
you	O
work	O
for	O
might	O
have	O
collected	O
1000	O
examples	B
of	O
pottery	O
ratings	O
.	O
you	O
will	O
select	O
800	O
of	O
these	O
as	O
training	B
data	I
and	O
set	O
aside	O
the	O
ﬁnal	O
200	O
as	O
test	B
data	I
.	O
you	O
will	O
run	O
your	O
learning	O
algorithms	O
only	O
on	O
the	O
800	O
training	O
points	O
.	O
only	O
once	O
you	O
’	O
re	O
done	O
will	O
you	O
apply	O
your	O
learned	O
model	B
to	O
the	O
200	O
test	O
points	O
,	O
and	O
report	O
your	O
test	B
error	I
on	O
those	O
200	O
points	O
to	O
your	O
boss	O
.	O
the	O
hope	O
in	O
this	O
process	O
is	O
that	O
however	O
well	O
you	O
do	O
on	O
the	O
200	O
test	O
points	O
will	O
be	O
indicative	O
of	O
how	O
well	O
you	O
are	O
likely	O
to	O
do	O
in	O
the	O
future	O
.	O
this	O
is	O
analogous	O
to	O
estimating	O
support	O
for	O
a	O
presidential	O
candidate	O
by	O
asking	O
a	O
small	O
(	O
random	O
!	O
)	O
sample	O
of	O
people	O
for	O
their	O
opinions	O
.	O
statistics	O
(	O
speciﬁcally	O
,	O
concentration	O
bounds	O
of	O
which	O
the	O
“	O
central	O
limit	O
theorem	O
”	O
is	O
a	O
famous	O
example	O
)	O
tells	O
us	O
that	O
if	O
the	O
sam-	O
ple	O
is	O
large	O
enough	O
,	O
it	O
will	O
be	O
a	O
good	O
representative	O
.	O
the	O
80/20	O
split	O
is	O
not	O
magic	O
:	O
it	O
’	O
s	O
simply	O
fairly	O
well	O
established	O
.	O
occasionally	O
people	O
use	O
a	O
90/10	O
split	O
instead	O
,	O
especially	O
if	O
they	O
have	O
a	O
lot	O
of	O
data	O
.	O
the	O
cardinal	O
rule	O
of	O
machine	O
learning	O
is	O
:	O
never	O
touch	O
your	O
test	B
data	I
.	O
ever	O
.	O
if	O
that	O
’	O
s	O
not	O
clear	O
enough	O
:	O
never	O
ever	O
touch	O
your	O
test	B
data	I
!	O
if	O
there	O
is	O
only	O
one	O
thing	O
you	O
learn	O
from	O
this	O
book	O
,	O
let	O
it	O
be	O
that	O
.	O
do	O
not	O
look	O
at	O
your	O
test	B
data	I
.	O
even	O
once	O
.	O
even	O
a	O
tiny	O
peek	O
.	O
once	O
you	O
do	O
that	O
,	O
it	O
is	O
not	O
test	B
data	I
any	O
more	O
.	O
yes	O
,	O
perhaps	O
your	O
algorithm	B
hasn	O
’	O
t	O
seen	O
it	O
.	O
but	O
you	O
have	O
.	O
and	O
you	O
are	O
likely	O
a	O
better	O
learner	O
than	O
your	O
learning	O
algorithm	B
.	O
consciously	O
or	O
otherwise	O
,	O
you	O
might	O
make	O
decisions	O
based	O
on	O
whatever	O
you	O
might	O
have	O
seen	O
.	O
once	O
you	O
look	O
at	O
the	O
test	O
data	O
,	O
your	O
model	B
’	O
s	O
performance	O
on	O
it	O
is	O
no	O
longer	O
indicative	O
of	O
it	O
’	O
s	O
performance	O
on	O
future	O
unseen	O
data	O
.	O
this	O
is	O
simply	O
because	O
future	O
data	O
is	O
unseen	O
,	O
but	O
your	O
“	O
test	O
”	O
data	O
no	O
longer	O
is	O
.	O
1.9	O
models	O
,	O
parameters	O
and	O
hyperparameters	O
the	O
general	O
approach	O
to	O
machine	O
learning	O
,	O
which	O
captures	O
many	O
ex-	O
isting	O
learning	O
algorithms	O
,	O
is	O
the	O
modeling	O
approach	O
.	O
the	O
idea	O
is	O
that	O
we	O
come	O
up	O
with	O
some	O
formal	O
model	B
of	O
our	O
data	O
.	O
for	O
instance	O
,	O
we	O
might	O
model	B
the	O
classiﬁcation	O
decision	O
of	O
a	O
student/course	O
pair	O
as	O
a	O
decision	B
tree	I
.	O
the	O
choice	O
of	O
using	O
a	O
tree	O
to	O
represent	O
this	O
model	B
is	O
our	O
choice	O
.	O
we	O
also	O
could	O
have	O
used	O
an	O
arithmetic	O
circuit	O
or	O
a	O
polynomial	O
or	O
some	O
other	O
function	O
.	O
the	O
model	O
tells	O
us	O
what	O
sort	O
of	O
things	O
we	O
can	O
learn	O
,	O
and	O
also	O
tells	O
us	O
what	O
our	O
inductive	B
bias	I
is	O
.	O
for	O
most	O
models	O
,	O
there	O
will	O
be	O
associated	O
parameters	O
.	O
these	O
are	O
the	O
things	O
that	O
we	O
use	O
the	O
data	O
to	O
decide	O
on	O
.	O
parameters	O
in	O
a	O
decision	O
?	O
if	O
you	O
have	O
more	O
data	O
at	O
your	O
dis-	O
posal	O
,	O
why	O
might	O
a	O
90/10	O
split	O
be	O
preferable	O
to	O
an	O
80/20	O
split	O
?	O
decision	B
trees	I
23	O
?	O
go	O
back	O
to	O
the	O
decisiontree-	O
train	O
algorithm	B
and	O
modify	O
it	O
so	O
that	O
it	O
takes	O
a	O
maximum	B
depth	I
pa-	O
rameter	O
.	O
this	O
should	O
require	O
adding	O
two	O
lines	O
of	O
code	O
and	O
modifying	O
three	O
others	O
.	O
tree	O
include	O
:	O
the	O
speciﬁc	O
questions	O
we	O
asked	O
,	O
the	O
order	O
in	O
which	O
we	O
asked	O
them	O
,	O
and	O
the	O
classiﬁcation	O
decisions	O
at	O
the	O
leaves	O
.	O
the	O
job	O
of	O
our	O
decision	B
tree	I
learning	O
algorithm	B
decisiontreetrain	O
is	O
to	O
take	O
data	O
and	O
ﬁgure	O
out	O
a	O
good	O
set	O
of	O
parameters	O
.	O
many	O
learning	O
algorithms	O
will	O
have	O
additional	O
knobs	O
that	O
you	O
can	O
adjust	O
.	O
in	O
most	O
cases	O
,	O
these	O
knobs	O
amount	O
to	O
tuning	O
the	O
inductive	O
bias	B
of	O
the	O
algorithm	O
.	O
in	O
the	O
case	O
of	O
the	O
decision	O
tree	O
,	O
an	O
obvious	O
knob	O
that	O
one	O
can	O
tune	O
is	O
the	O
maximum	O
depth	O
of	O
the	O
decision	O
tree	O
.	O
that	O
is	O
,	O
we	O
could	O
modify	O
the	O
decisiontreetrain	O
function	O
so	O
that	O
it	O
stops	O
recursing	O
once	O
it	O
reaches	O
some	O
pre-deﬁned	O
maximum	B
depth	I
.	O
by	O
playing	O
with	O
this	O
depth	O
knob	O
,	O
we	O
can	O
adjust	O
between	O
underﬁtting	O
(	O
the	O
empty	O
tree	O
,	O
depth=	O
0	O
)	O
and	O
overﬁtting	O
(	O
the	O
full	O
tree	O
,	O
depth=	O
∞	O
)	O
.	O
such	O
a	O
knob	O
is	O
called	O
a	O
hyperparameter	B
.	O
it	O
is	O
so	O
called	O
because	O
it	O
is	O
a	O
parameter	O
that	O
controls	O
other	O
parameters	O
of	O
the	O
model	O
.	O
the	O
exact	O
deﬁnition	O
of	O
hyperparameter	B
is	O
hard	O
to	O
pin	O
down	O
:	O
it	O
’	O
s	O
one	O
of	O
those	O
things	O
that	O
are	O
easier	O
to	O
identify	O
than	O
deﬁne	O
.	O
however	O
,	O
one	O
of	O
the	O
key	O
identiﬁers	O
for	O
hyperparameters	O
(	O
and	O
the	O
main	O
reason	O
that	O
they	O
cause	O
consternation	O
)	O
is	O
that	O
they	O
can	O
not	O
be	O
naively	O
adjusted	O
using	O
the	O
training	O
data	O
.	O
in	O
decisiontreetrain	O
,	O
as	O
in	O
most	O
machine	O
learning	O
,	O
the	O
learn-	O
ing	O
algorithm	B
is	O
essentially	O
trying	O
to	O
adjust	O
the	O
parameters	O
of	O
the	O
model	O
so	O
as	O
to	O
minimize	O
training	B
error	I
.	O
this	O
suggests	O
an	O
idea	O
for	O
choosing	O
hyperparameters	O
:	O
choose	O
them	O
so	O
that	O
they	O
minimize	O
train-	O
ing	O
error	O
.	O
what	O
is	O
wrong	O
with	O
this	O
suggestion	O
?	O
suppose	O
that	O
you	O
were	O
to	O
treat	O
“	O
maximum	B
depth	I
”	O
as	O
a	O
hyperparameter	B
and	O
tried	O
to	O
tune	O
it	O
on	O
your	O
training	B
data	I
.	O
to	O
do	O
this	O
,	O
maybe	O
you	O
simply	O
build	O
a	O
collection	O
of	O
decision	B
trees	I
,	O
tree0	O
,	O
tree1	O
,	O
tree2	O
,	O
.	O
.	O
.	O
,	O
tree100	O
,	O
where	O
treed	O
is	O
a	O
tree	O
of	O
maximum	B
depth	I
d.	O
we	O
then	O
computed	O
the	O
training	O
error	O
of	O
each	O
of	O
these	O
trees	O
and	O
chose	O
the	O
“	O
ideal	O
”	O
maximum	B
depth	I
as	O
that	O
which	O
minimizes	O
training	B
error	I
?	O
which	O
one	O
would	O
it	O
pick	O
?	O
the	O
answer	O
is	O
that	O
it	O
would	O
pick	O
d	O
=	O
100.	O
or	O
,	O
in	O
general	O
,	O
it	O
would	O
pick	O
d	O
as	O
large	O
as	O
possible	O
.	O
why	O
?	O
because	O
choosing	O
a	O
bigger	O
d	O
will	O
never	O
hurt	O
on	O
the	O
training	O
data	O
.	O
by	O
making	O
d	O
larger	O
,	O
you	O
are	O
simply	O
encouraging	O
overﬁtting	O
.	O
but	O
by	O
evaluating	O
on	O
the	O
training	O
data	O
,	O
over-	O
ﬁtting	O
actually	O
looks	O
like	O
a	O
good	O
idea	O
!	O
an	O
alternative	O
idea	O
would	O
be	O
to	O
tune	O
the	O
maximum	O
depth	O
on	O
test	B
data	I
.	O
this	O
is	O
promising	O
because	O
test	B
data	I
peformance	O
is	O
what	O
we	O
really	O
want	O
to	O
optimize	O
,	O
so	O
tuning	O
this	O
knob	O
on	O
the	O
test	O
data	O
seems	O
like	O
a	O
good	O
idea	O
.	O
that	O
is	O
,	O
it	O
won	O
’	O
t	O
accidentally	O
reward	O
overﬁtting	O
.	O
of	O
course	O
,	O
it	O
breaks	O
our	O
cardinal	O
rule	O
about	O
test	B
data	I
:	O
that	O
you	O
should	O
never	O
touch	O
your	O
test	B
data	I
.	O
so	O
that	O
idea	O
is	O
immediately	O
off	O
the	O
table	O
.	O
however	O
,	O
our	O
“	O
test	B
data	I
”	O
wasn	O
’	O
t	O
magic	O
.	O
we	O
simply	O
took	O
our	O
1000	O
examples	B
,	O
called	O
800	O
of	O
them	O
“	O
training	O
”	O
data	O
and	O
called	O
the	O
other	O
200	O
3	O
some	O
people	O
call	O
this	O
“	O
validation	B
data	I
”	O
or	O
“	O
held-out	O
data.	O
”	O
?	O
in	O
step	O
3	O
,	O
you	O
could	O
either	O
choose	O
the	O
model	O
(	O
trained	O
on	O
the	O
70	O
%	O
train-	O
ing	O
data	O
)	O
that	O
did	O
the	O
best	O
on	O
the	O
development	O
data	O
.	O
or	O
you	O
could	O
choose	O
the	O
hyperparameter	O
settings	O
that	O
did	O
best	O
and	O
retrain	O
the	O
model	O
on	O
the	O
80	O
%	O
union	O
of	O
training	O
and	O
development	B
data	I
.	O
is	O
either	O
of	O
these	O
options	O
obviously	O
better	O
or	O
worse	O
?	O
24	O
a	O
course	O
in	O
machine	O
learning	O
“	O
test	O
”	O
data	O
.	O
so	O
instead	O
,	O
let	O
’	O
s	O
do	O
the	O
following	O
.	O
let	O
’	O
s	O
take	O
our	O
original	O
1000	O
data	O
points	O
,	O
and	O
select	O
700	O
of	O
them	O
as	O
training	B
data	I
.	O
from	O
the	O
remainder	O
,	O
take	O
100	O
as	O
development	O
data3	O
and	O
the	O
remaining	O
200	O
as	O
test	B
data	I
.	O
the	O
job	O
of	O
the	O
development	O
data	O
is	O
to	O
allow	O
us	O
to	O
tune	O
hyperparameters	O
.	O
the	O
general	O
approach	O
is	O
as	O
follows	O
:	O
1.	O
split	O
your	O
data	O
into	O
70	O
%	O
training	B
data	I
,	O
10	O
%	O
development	B
data	I
and	O
20	O
%	O
test	B
data	I
.	O
2.	O
for	O
each	O
possible	O
setting	O
of	O
your	O
hyperparameters	O
:	O
(	O
a	O
)	O
train	O
a	O
model	B
using	O
that	O
setting	O
of	O
hyperparameters	O
on	O
the	O
training	O
data	O
.	O
(	O
b	O
)	O
compute	O
this	O
model	B
’	O
s	O
error	B
rate	I
on	O
the	O
development	O
data	O
.	O
3.	O
from	O
the	O
above	O
collection	O
of	O
models	O
,	O
choose	O
the	O
one	O
that	O
achieved	O
the	O
lowest	O
error	B
rate	I
on	O
development	B
data	I
.	O
4.	O
evaluate	O
that	O
model	B
on	O
the	O
test	O
data	O
to	O
estimate	O
future	O
test	O
perfor-	O
mance	O
.	O
1.10	O
chapter	O
summary	O
and	O
outlook	O
at	O
this	O
point	O
,	O
you	O
should	O
be	O
able	O
to	O
use	O
decision	B
trees	I
to	O
do	O
machine	O
learning	O
.	O
someone	O
will	O
give	O
you	O
data	O
.	O
you	O
’	O
ll	O
split	O
it	O
into	O
training	O
,	O
development	O
and	O
test	O
portions	O
.	O
using	O
the	O
training	O
and	O
development	B
data	I
,	O
you	O
’	O
ll	O
ﬁnd	O
a	O
good	O
value	O
for	O
maximum	B
depth	I
that	O
trades	O
off	O
between	O
underﬁtting	O
and	O
overﬁtting	O
.	O
you	O
’	O
ll	O
then	O
run	O
the	O
resulting	O
decision	B
tree	I
model	O
on	O
the	O
test	O
data	O
to	O
get	O
an	O
estimate	O
of	O
how	O
well	O
you	O
are	O
likely	O
to	O
do	O
in	O
the	O
future	O
.	O
you	O
might	O
think	O
:	O
why	O
should	O
i	O
read	O
the	O
rest	O
of	O
this	O
book	O
?	O
aside	O
from	O
the	O
fact	O
that	O
machine	O
learning	O
is	O
just	O
an	O
awesome	O
fun	O
ﬁeld	O
to	O
learn	O
about	O
,	O
there	O
’	O
s	O
a	O
lot	O
left	O
to	O
cover	O
.	O
in	O
the	O
next	O
two	O
chapters	O
,	O
you	O
’	O
ll	O
learn	O
about	O
two	O
models	O
that	O
have	O
very	O
different	O
inductive	O
biases	O
than	O
decision	B
trees	I
.	O
you	O
’	O
ll	O
also	O
get	O
to	O
see	O
a	O
very	O
useful	O
way	O
of	O
thinking	O
about	O
learning	O
:	O
the	O
geometric	O
view	O
of	O
data	O
.	O
this	O
will	O
guide	O
much	O
of	O
what	O
follows	O
.	O
after	O
that	O
,	O
you	O
’	O
ll	O
learn	O
how	O
to	O
solve	O
problems	O
more	O
complicated	O
that	O
simple	O
binary	O
classiﬁcation	O
.	O
(	O
machine	O
learning	O
people	O
like	O
binary	O
classiﬁcation	O
a	O
lot	O
because	O
it	O
’	O
s	O
one	O
of	O
the	O
simplest	O
non-trivial	O
problems	O
that	O
we	O
can	O
work	O
on	O
.	O
)	O
after	O
that	O
,	O
things	O
will	O
diverge	O
:	O
you	O
’	O
ll	O
learn	O
about	O
ways	O
to	O
think	O
about	O
learning	O
as	O
a	O
formal	O
optimization	B
problem	I
,	O
ways	O
to	O
speed	O
up	O
learning	O
,	O
ways	O
to	O
learn	O
without	O
labeled	O
data	O
(	O
or	O
with	O
very	O
little	O
labeled	O
data	O
)	O
and	O
all	O
sorts	O
of	O
other	O
fun	O
topics	O
.	O
decision	B
trees	I
25	O
but	O
throughout	O
,	O
we	O
will	O
focus	O
on	O
the	O
view	O
of	O
machine	O
learning	O
that	O
you	O
’	O
ve	O
seen	O
here	O
.	O
you	O
select	O
a	O
model	B
(	O
and	O
its	O
associated	O
induc-	O
tive	O
biases	O
)	O
.	O
you	O
use	O
data	O
to	O
ﬁnd	O
parameters	O
of	O
that	O
model	B
that	O
work	O
well	O
on	O
the	O
training	O
data	O
.	O
you	O
use	O
development	B
data	I
to	O
avoid	O
under-	O
ﬁtting	O
and	O
overﬁtting	O
.	O
and	O
you	O
use	O
test	B
data	I
(	O
which	O
you	O
’	O
ll	O
never	O
look	O
at	O
or	O
touch	O
,	O
right	O
?	O
)	O
to	O
estimate	O
future	O
model	B
performance	O
.	O
then	O
you	O
conquer	O
the	O
world	O
.	O
1.11	O
exercises	O
exercise	O
1.1.	O
todo	O
.	O
.	O
.	O
2	O
|	O
geometry	O
and	O
nearest	O
neighbors	O
learning	O
objectives	O
:	O
•	O
describe	O
a	O
data	O
set	O
as	O
points	O
in	O
a	O
high	O
dimensional	O
space	O
.	O
•	O
explain	O
the	B
curse	I
of	I
dimensionality	I
.	O
•	O
compute	O
distances	O
between	O
points	O
in	O
high	O
dimensional	O
space	O
.	O
•	O
implement	O
a	O
k-nearest	O
neighbor	O
model	B
of	O
learning	O
.	O
•	O
draw	O
decision	O
boundaries	O
.	O
•	O
implement	O
the	O
k-means	O
algorithm	B
for	O
clustering	B
.	O
dependencies	O
:	O
chapter	O
1	O
our	O
brains	O
have	O
evolved	O
to	O
get	O
us	O
out	O
of	O
the	O
rain	O
,	O
ﬁnd	O
where	O
the	O
berries	O
are	O
,	O
and	O
keep	O
us	O
from	O
getting	O
killed	O
.	O
our	O
brains	O
did	O
not	O
evolve	O
to	O
help	O
us	O
grasp	O
really	O
large	O
numbers	O
or	O
to	O
look	O
at	O
things	O
in	O
a	O
hundred	O
thousand	O
dimensions	O
.	O
–	O
ronald	O
graham	O
you	O
can	O
think	O
of	O
prediction	O
tasks	O
as	O
mapping	O
inputs	O
(	O
course	O
reviews	O
)	O
to	O
outputs	O
(	O
course	O
ratings	O
)	O
.	O
as	O
you	O
learned	O
in	O
the	O
previ-	O
ous	O
chapter	O
,	O
decomposing	O
an	O
input	O
into	O
a	O
collection	O
of	O
features	B
(	O
e.g.	O
,	O
words	O
that	O
occur	O
in	O
the	O
review	O
)	O
forms	O
a	O
useful	O
abstraction	O
for	O
learn-	O
ing	O
.	O
therefore	O
,	O
inputs	O
are	O
nothing	O
more	O
than	O
lists	O
of	O
feature	B
values	I
.	O
this	O
suggests	O
a	O
geometric	B
view	I
of	O
data	O
,	O
where	O
we	O
have	O
one	O
dimen-	O
sion	O
for	O
every	O
feature	O
.	O
in	O
this	O
view	O
,	O
examples	B
are	O
points	O
in	O
a	O
high-	O
dimensional	O
space	O
.	O
once	O
we	O
think	O
of	O
a	O
data	O
set	O
as	O
a	O
collection	O
of	O
points	O
in	O
high	O
dimen-	O
sional	O
space	O
,	O
we	O
can	O
start	O
performing	O
geometric	O
operations	O
on	O
this	O
data	O
.	O
for	O
instance	O
,	O
suppose	O
you	O
need	O
to	O
predict	B
whether	O
alice	O
will	O
like	O
algorithms	O
.	O
perhaps	O
we	O
can	O
try	O
to	O
ﬁnd	O
another	O
student	O
who	O
is	O
most	O
“	O
similar	O
”	O
to	O
alice	O
,	O
in	O
terms	O
of	O
favorite	O
courses	O
.	O
say	O
this	O
student	O
is	O
jeremy	O
.	O
if	O
jeremy	O
liked	O
algorithms	O
,	O
then	O
we	O
might	O
guess	O
that	O
alice	O
will	O
as	O
well	O
.	O
this	O
is	O
an	O
example	O
of	O
a	O
nearest	B
neighbor	I
model	O
of	O
learn-	O
ing	O
.	O
by	O
inspecting	O
this	O
model	B
,	O
we	O
’	O
ll	O
see	O
a	O
completely	O
different	O
set	O
of	O
answers	O
to	O
the	O
key	O
learning	O
questions	O
we	O
discovered	O
in	O
chapter	O
1	O
.	O
2.1	O
from	O
data	O
to	O
feature	O
vectors	O
an	O
example	O
is	O
just	O
a	O
collection	O
of	O
feature	B
values	I
about	O
that	O
example	O
,	O
for	O
instance	O
the	O
data	O
in	O
table	O
?	O
?	O
from	O
the	O
appendix	O
.	O
to	O
a	O
person	O
,	O
these	O
features	B
have	O
meaning	O
.	O
one	O
feature	O
might	O
count	O
how	O
many	O
times	O
the	O
reviewer	O
wrote	O
“	O
excellent	O
”	O
in	O
a	O
course	O
review	O
.	O
another	O
might	O
count	O
the	O
number	O
of	O
exclamation	O
points	O
.	O
a	O
third	O
might	O
tell	O
us	O
if	O
any	O
text	O
is	O
underlined	O
in	O
the	O
review	O
.	O
to	O
a	O
machine	O
,	O
the	O
features	O
themselves	O
have	O
no	O
meaning	O
.	O
only	O
the	O
feature	O
values	O
,	O
and	O
how	O
they	O
vary	O
across	O
examples	B
,	O
mean	O
some-	O
thing	O
to	O
the	O
machine	O
.	O
from	O
this	O
perspective	O
,	O
you	O
can	O
think	O
about	O
an	O
example	O
as	O
being	O
represented	O
by	O
a	O
feature	B
vector	I
consisting	O
of	O
one	O
“	O
dimension	O
”	O
for	O
each	O
feature	O
,	O
where	O
each	O
dimenion	O
is	O
simply	O
some	O
real	O
value	O
.	O
consider	O
a	O
review	O
that	O
said	O
“	O
excellent	O
”	O
three	O
times	O
,	O
had	O
one	O
excla-	O
geometry	O
and	O
nearest	O
neighbors	O
27	O
mation	O
point	O
and	O
no	O
underlined	O
text	O
.	O
this	O
could	O
be	O
represented	O
by	O
the	O
feature	O
vector	B
(	O
cid:104	O
)	O
3	O
,	O
1	O
,	O
0	O
(	O
cid:105	O
)	O
.	O
an	O
almost	O
identical	O
review	O
that	O
happened	O
to	O
have	O
underlined	O
text	O
would	O
have	O
the	O
feature	O
vector	B
(	O
cid:104	O
)	O
3	O
,	O
1	O
,	O
1	O
(	O
cid:105	O
)	O
.	O
note	O
,	O
here	O
,	O
that	O
we	O
have	O
imposed	O
the	O
convention	O
that	O
for	O
binary	B
features	I
(	O
yes/no	O
features	B
)	O
,	O
the	O
corresponding	O
feature	B
values	I
are	O
0	O
and	O
1	O
,	O
respectively	O
.	O
this	O
was	O
an	O
arbitrary	O
choice	O
.	O
we	O
could	O
have	O
made	O
them	O
0.92	O
and	O
−16.1	O
if	O
we	O
wanted	O
.	O
but	O
0/1	O
is	O
convenient	O
and	O
helps	O
us	O
interpret	O
the	O
feature	O
values	O
.	O
when	O
we	O
discuss	O
practical	O
issues	O
in	O
chapter	O
4	O
,	O
you	O
will	O
see	O
other	O
reasons	O
why	O
0/1	O
is	O
a	O
good	O
choice	O
.	O
figure	O
2.1	O
shows	O
the	O
data	O
from	O
table	O
?	O
?	O
in	O
three	O
views	O
.	O
these	O
three	O
views	O
are	O
constructed	O
by	O
considering	O
two	O
features	B
at	O
a	O
time	O
in	O
different	O
pairs	O
.	O
in	O
all	O
cases	O
,	O
the	O
plusses	O
denote	O
positive	O
examples	O
and	O
the	O
minuses	O
denote	O
negative	O
examples	B
.	O
in	O
some	O
cases	O
,	O
the	O
points	O
fall	O
on	O
top	O
of	O
each	O
other	O
,	O
which	O
is	O
why	O
you	O
can	O
not	O
see	O
20	O
unique	O
points	O
in	O
all	O
ﬁgures	O
.	O
the	O
mapping	O
from	O
feature	B
values	I
to	O
vectors	O
is	O
straighforward	O
in	O
the	O
case	O
of	O
real	O
valued	O
features	B
(	O
trivial	O
)	O
and	O
binary	B
features	I
(	O
mapped	O
to	O
zero	O
or	O
one	O
)	O
.	O
it	O
is	O
less	O
clear	O
what	O
to	O
do	O
with	O
categorical	B
features	I
.	O
for	O
example	O
,	O
if	O
our	O
goal	O
is	O
to	O
identify	O
whether	O
an	O
object	O
in	O
an	O
image	O
is	O
a	O
tomato	O
,	O
blueberry	O
,	O
cucumber	O
or	O
cockroach	O
,	O
we	O
might	O
want	O
to	O
know	O
its	O
color	O
:	O
is	O
it	O
red	O
,	O
blue	O
,	O
green	O
or	O
black	O
?	O
one	O
option	O
would	O
be	O
to	O
map	O
red	O
to	O
a	O
value	O
of	O
0	O
,	O
blue	O
to	O
a	O
value	O
of	O
1	O
,	O
green	O
to	O
a	O
value	O
of	O
2	O
and	O
black	O
to	O
a	O
value	O
of	O
3.	O
the	O
problem	O
with	O
this	O
mapping	O
is	O
that	O
it	O
turns	O
an	O
unordered	O
set	O
(	O
the	O
set	O
of	O
colors	O
)	O
into	O
an	O
ordered	O
set	O
(	O
the	O
set	O
{	O
0	O
,	O
1	O
,	O
2	O
,	O
3	O
}	O
)	O
.	O
in	O
itself	O
,	O
this	O
is	O
not	O
necessarily	O
a	O
bad	O
thing	O
.	O
but	O
when	O
we	O
go	O
to	O
use	O
these	O
features	B
,	O
we	O
will	O
measure	O
examples	B
based	O
on	O
their	O
distances	O
to	O
each	O
other	O
.	O
by	O
doing	O
this	O
map-	O
ping	O
,	O
we	O
are	O
essentially	O
saying	O
that	O
red	O
and	O
blue	O
are	O
more	O
similar	O
(	O
distance	B
of	O
1	O
)	O
than	O
red	O
and	O
black	O
(	O
distance	B
of	O
3	O
)	O
.	O
this	O
is	O
probably	O
not	O
what	O
we	O
want	O
to	O
say	O
!	O
a	O
solution	O
is	O
to	O
turn	O
a	O
categorical	O
feature	O
that	O
can	O
take	O
four	O
dif-	O
ferent	O
values	O
(	O
say	O
:	O
red	O
,	O
blue	O
,	O
green	O
and	O
black	O
)	O
into	O
four	O
binary	B
features	I
(	O
say	O
:	O
isitred	O
?	O
,	O
isitblue	O
?	O
,	O
isitgreen	O
?	O
and	O
isitblack	O
?	O
)	O
.	O
in	O
gen-	O
eral	O
,	O
if	O
we	O
start	O
from	O
a	O
categorical	O
feature	O
that	O
takes	O
v	O
values	O
,	O
we	O
can	O
map	O
it	O
to	O
v-many	O
binary	O
indicator	O
features	B
.	O
with	O
that	O
,	O
you	O
should	O
be	O
able	O
to	O
take	O
a	O
data	O
set	O
and	O
map	O
each	O
example	O
to	O
a	O
feature	B
vector	I
through	O
the	O
following	O
mapping	O
:	O
•	O
real-valued	O
features	B
get	O
copied	O
directly	O
.	O
•	O
binary	B
features	I
become	O
0	O
(	O
for	O
false	O
)	O
or	O
1	O
(	O
for	O
true	O
)	O
.	O
•	O
categorical	B
features	I
with	O
v	O
possible	O
values	O
get	O
mapped	O
to	O
v-many	O
binary	O
indicator	O
features	B
.	O
figure	O
2.1	O
:	O
a	O
ﬁgure	O
showing	O
projections	O
of	O
data	O
in	O
two	O
dimension	O
in	O
three	O
ways	O
–	O
see	O
text	O
.	O
top	O
:	O
horizontal	O
axis	O
corresponds	O
to	O
the	O
ﬁrst	O
feature	O
(	O
todo	O
)	O
and	O
the	O
vertical	O
axis	O
corresponds	O
to	O
the	O
second	O
feature	O
(	O
todo	O
)	O
;	O
middle	O
:	O
horizonal	O
is	O
second	O
feature	O
and	O
vertical	O
is	O
third	O
;	O
bottom	O
:	O
horizonal	O
is	O
ﬁrst	O
and	O
vertical	O
is	O
third	O
.	O
?	O
match	O
the	O
example	O
ids	O
from	O
ta-	O
ble	O
?	O
?	O
with	O
the	O
points	O
in	O
figure	O
2.1.	O
?	O
the	O
computer	O
scientist	O
in	O
you	O
might	O
be	O
saying	O
:	O
actually	O
we	O
could	O
map	O
it	O
to	O
log2	O
v-many	O
binary	B
features	I
!	O
is	O
this	O
a	O
good	O
idea	O
or	O
not	O
?	O
28	O
a	O
course	O
in	O
machine	O
learning	O
after	O
this	O
mapping	O
,	O
you	O
can	O
think	O
of	O
a	O
single	O
example	O
as	O
a	O
vec-	O
tor	O
in	O
a	O
high-dimensional	O
feature	B
space	I
.	O
if	O
you	O
have	O
d-many	O
fea-	O
tures	O
(	O
after	O
expanding	O
categorical	B
features	I
)	O
,	O
then	O
this	O
feature	B
vector	I
will	O
have	O
d-many	O
components	O
.	O
we	O
will	O
denote	O
feature	O
vectors	O
as	O
x	O
=	O
(	O
cid:104	O
)	O
x1	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
xd	O
(	O
cid:105	O
)	O
,	O
so	O
that	O
xd	O
denotes	O
the	O
value	O
of	O
the	O
dth	O
fea-	O
ture	O
of	O
x.	O
since	O
these	O
are	O
vectors	O
with	O
real-valued	O
components	O
in	O
d-dimensions	O
,	O
we	O
say	O
that	O
they	O
belong	O
to	O
the	O
space	O
rd	O
.	O
for	O
d	O
=	O
2	O
,	O
our	O
feature	O
vectors	O
are	O
just	O
points	O
in	O
the	O
plane	O
,	O
like	O
in	O
figure	O
2.1.	O
for	O
d	O
=	O
3	O
this	O
is	O
three	O
dimensional	O
space	O
.	O
for	O
d	O
>	O
3	O
it	O
becomes	O
quite	O
hard	O
to	O
visualize	B
.	O
(	O
you	O
should	O
resist	O
the	O
temptation	O
to	O
think	O
of	O
d	O
=	O
4	O
as	O
“	O
time	O
”	O
–	O
this	O
will	O
just	O
make	O
things	O
confusing	O
.	O
)	O
unfortunately	O
,	O
for	O
the	O
sorts	O
of	O
problems	O
you	O
will	O
encounter	O
in	O
ma-	O
chine	O
learning	O
,	O
d	O
≈	O
20	O
is	O
considered	O
“	O
low	O
dimensional	O
,	O
”	O
d	O
≈	O
1000	O
is	O
“	O
medium	O
dimensional	O
”	O
and	O
d	O
≈	O
100000	O
is	O
“	O
high	O
dimensional.	O
”	O
2.2	O
k-nearest	O
neighbors	O
?	O
can	O
you	O
think	O
of	O
problems	O
(	O
per-	O
haps	O
ones	O
already	O
mentioned	O
in	O
this	O
book	O
!	O
)	O
that	O
are	O
low	O
dimensional	O
?	O
that	O
are	O
medium	O
dimensional	O
?	O
that	O
are	O
high	O
dimensional	O
?	O
the	O
biggest	O
advantage	O
to	O
thinking	O
of	O
examples	B
as	O
vectors	O
in	O
a	O
high	O
dimensional	O
space	O
is	O
that	O
it	O
allows	O
us	O
to	O
apply	O
geometric	O
concepts	O
to	O
machine	O
learning	O
.	O
for	O
instance	O
,	O
one	O
of	O
the	O
most	O
basic	O
things	O
that	O
one	O
can	O
do	O
in	O
a	O
vector	B
space	O
is	O
compute	O
distances	O
.	O
in	O
two-	O
dimensional	O
space	O
,	O
the	O
distance	O
between	O
(	O
cid:104	O
)	O
2	O
,	O
3	O
(	O
cid:105	O
)	O
and	O
(	O
cid:104	O
)	O
6	O
,	O
1	O
(	O
cid:105	O
)	O
is	O
given	O
by	O
(	O
cid:112	O
)	O
(	O
2	O
−	O
6	O
)	O
2	O
+	O
(	O
3	O
−	O
1	O
)	O
2	O
=	O
18	O
≈	O
4.24.	O
in	O
general	O
,	O
in	O
d-dimensional	O
√	O
space	O
,	O
the	O
euclidean	O
distance	B
between	O
vectors	O
a	O
and	O
b	O
is	O
given	O
by	O
eq	O
(	O
2.1	O
)	O
(	O
see	O
figure	O
2.2	O
for	O
geometric	O
intuition	O
in	O
three	O
dimensions	O
)	O
:	O
(	O
cid:35	O
)	O
1	O
2	O
(	O
cid:34	O
)	O
d∑	O
d=1	O
d	O
(	O
a	O
,	O
b	O
)	O
=	O
(	O
ad	O
−	O
bd	O
)	O
2	O
(	O
2.1	O
)	O
now	O
that	O
you	O
have	O
access	O
to	O
distances	O
between	O
examples	B
,	O
you	O
can	O
start	O
thinking	O
about	O
what	O
it	O
means	O
to	O
learn	O
again	O
.	O
consider	O
fig-	O
ure	O
2.3.	O
we	O
have	O
a	O
collection	O
of	O
training	B
data	I
consisting	O
of	O
positive	O
examples	O
and	O
negative	O
examples	B
.	O
there	O
is	O
a	O
test	O
point	O
marked	O
by	O
a	O
question	O
mark	O
.	O
your	O
job	O
is	O
to	O
guess	O
the	O
correct	O
label	B
for	O
that	O
point	O
.	O
most	O
likely	O
,	O
you	O
decided	O
that	O
the	O
label	O
of	O
this	O
test	O
point	O
is	O
positive	O
.	O
one	O
reason	O
why	O
you	O
might	O
have	O
thought	O
that	O
is	O
that	O
you	O
believe	O
that	O
the	O
label	O
for	O
an	O
example	O
should	O
be	O
similar	O
to	O
the	O
label	O
of	O
nearby	O
points	O
.	O
this	O
is	O
an	O
example	O
of	O
a	O
new	O
form	O
of	O
inductive	B
bias	I
.	O
the	O
nearest	O
neighbor	O
classiﬁer	O
is	O
build	O
upon	O
this	O
insight	O
.	O
in	O
com-	O
parison	O
to	O
decision	B
trees	I
,	O
the	O
algorithm	O
is	O
ridiculously	O
simple	O
.	O
at	O
training	O
time	O
,	O
we	O
simply	O
store	O
the	O
entire	O
training	O
set	O
.	O
at	O
test	O
time	O
,	O
we	O
get	O
a	O
test	O
example	O
ˆx	O
.	O
to	O
predict	B
its	O
label	B
,	O
we	O
ﬁnd	O
the	O
training	O
ex-	O
ample	O
x	O
that	O
is	O
most	O
similar	O
to	O
ˆx	O
.	O
in	O
particular	O
,	O
we	O
ﬁnd	O
the	O
training	O
figure	O
2.2	O
:	O
a	O
ﬁgure	O
showing	O
euclidean	O
distance	B
in	O
three	O
dimensions	O
?	O
verify	O
that	O
d	O
from	O
eq	O
(	O
2.1	O
)	O
gives	O
the	O
same	O
result	O
(	O
4.24	O
)	O
for	O
the	O
previous	O
computation	O
.	O
geometry	O
and	O
nearest	O
neighbors	O
29	O
//	O
store	O
distance	B
to	O
training	O
example	O
n	O
//	O
put	O
lowest-distance	O
objects	O
ﬁrst	O
s	O
←	O
s	O
⊕	O
(	O
cid:104	O
)	O
d	O
(	O
xn	O
,	O
ˆx	O
)	O
,	O
n	O
(	O
cid:105	O
)	O
algorithm	B
3	O
knn-predict	O
(	O
d	O
,	O
k	O
,	O
ˆx	O
)	O
1	O
:	O
s	O
←	O
[	O
]	O
2	O
:	O
for	O
n	O
=	O
1	O
to	O
n	O
do	O
3	O
:	O
4	O
:	O
end	O
for	O
5	O
:	O
s	O
←	O
sort	O
(	O
s	O
)	O
ˆy	O
←	O
0	O
6	O
:	O
7	O
:	O
for	O
k	O
=	O
1	O
to	O
k	O
do	O
(	O
cid:104	O
)	O
dist	O
,	O
n	O
(	O
cid:105	O
)	O
←	O
sk	O
8	O
:	O
ˆy	O
←	O
ˆy	O
+	O
yn	O
9	O
:	O
10	O
:	O
end	O
for	O
11	O
:	O
return	O
sign	B
(	O
ˆy	O
)	O
//	O
n	O
this	O
is	O
the	O
kth	O
closest	O
data	O
point	O
//	O
vote	B
according	O
to	O
the	O
label	O
for	O
the	O
nth	O
training	O
point	O
//	O
return	O
+1	O
if	O
ˆy	O
>	O
0	O
and	O
−1	O
if	O
ˆy	O
<	O
0	O
example	O
x	O
that	O
minimizes	O
d	O
(	O
x	O
,	O
ˆx	O
)	O
.	O
since	O
x	O
is	O
a	O
training	O
example	O
,	O
it	O
has	O
a	O
corresponding	O
label	B
,	O
y.	O
we	O
predict	B
that	O
the	O
label	O
of	O
ˆx	O
is	O
also	O
y.	O
despite	O
its	O
simplicity	O
,	O
this	O
nearest	B
neighbor	I
classiﬁer	O
is	O
incred-	O
ibly	O
effective	O
.	O
(	O
some	O
might	O
say	O
frustratingly	O
effective	O
.	O
)	O
however	O
,	O
it	O
is	O
particularly	O
prone	O
to	O
overﬁtting	O
label	B
noise	O
.	O
consider	O
the	O
data	O
in	O
figure	O
2.4.	O
you	O
would	O
probably	O
want	O
to	O
label	B
the	O
test	O
point	O
positive	O
.	O
unfortunately	O
,	O
it	O
’	O
s	O
nearest	B
neighbor	I
happens	O
to	O
be	O
negative	O
.	O
since	O
the	O
nearest	O
neighbor	O
algorithm	B
only	O
looks	O
at	O
the	O
single	O
nearest	B
neighbor	I
,	O
it	O
can	O
not	O
consider	O
the	O
“	O
preponderance	O
of	O
evidence	B
”	O
that	O
this	O
point	O
should	O
probably	O
actually	O
be	O
a	O
positive	O
example	O
.	O
it	O
will	O
make	O
an	O
un-	O
necessary	O
error	O
.	O
a	O
solution	O
to	O
this	O
problem	O
is	O
to	O
consider	O
more	O
than	O
just	O
the	O
single	O
nearest	B
neighbor	I
when	O
making	O
a	O
classiﬁcation	O
decision	O
.	O
we	O
can	O
con-	O
sider	O
the	O
k-nearest	O
neighbors	O
and	O
let	O
them	O
vote	B
on	O
the	O
correct	O
class	O
for	O
this	O
test	O
point	O
.	O
if	O
you	O
consider	O
the	O
3-nearest	O
neighbors	O
of	O
the	O
test	O
point	O
in	O
figure	O
2.4	O
,	O
you	O
will	O
see	O
that	O
two	O
of	O
them	O
are	O
positive	O
and	O
one	O
is	O
negative	O
.	O
through	O
voting	B
,	O
positive	O
would	O
win	O
.	O
the	O
full	O
algorithm	B
for	O
k-nearest	O
neighbor	O
classiﬁcation	O
is	O
given	O
in	O
algorithm	B
2.2.	O
note	O
that	O
there	O
actually	O
is	O
no	O
“	O
training	O
”	O
phase	O
for	O
k-nearest	O
neighbors	O
.	O
in	O
this	O
algorithm	B
we	O
have	O
introduced	O
ﬁve	O
new	O
conventions	O
:	O
1.	O
the	O
training	O
data	O
is	O
denoted	O
by	O
d.	O
2.	O
we	O
assume	O
that	O
there	O
are	O
n-many	O
training	O
examples	O
.	O
3.	O
these	O
examples	B
are	O
pairs	O
(	O
x1	O
,	O
y1	O
)	O
,	O
(	O
x2	O
,	O
y2	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xn	O
,	O
yn	O
)	O
.	O
(	O
warning	O
:	O
do	O
not	O
confuse	O
xn	O
,	O
the	O
nth	O
training	O
example	O
,	O
with	O
xd	O
,	O
the	O
dth	O
feature	O
for	O
example	O
x	O
.	O
)	O
4.	O
we	O
use	O
[	O
]	O
to	O
denote	O
an	O
empty	O
list	O
and	O
⊕	O
·	O
to	O
append	O
·	O
to	O
that	O
list	O
.	O
5.	O
our	O
prediction	O
on	O
ˆx	O
is	O
called	O
ˆy	O
.	O
figure	O
2.4	O
:	O
a	O
ﬁgure	O
showing	O
an	O
easy	O
nn	O
classiﬁcation	O
problem	O
where	O
the	O
test	O
point	O
is	O
a	O
?	O
and	O
should	O
be	O
positive	O
,	O
but	O
its	O
nn	O
is	O
actually	O
a	O
negative	O
point	O
that	O
’	O
s	O
noisy	O
.	O
?	O
why	O
is	O
it	O
a	O
good	O
idea	O
to	O
use	O
an	O
odd	O
number	O
for	O
k	O
?	O
30	O
a	O
course	O
in	O
machine	O
learning	O
the	O
ﬁrst	O
step	O
in	O
this	O
algorithm	B
is	O
to	O
compute	O
distances	O
from	O
the	O
test	O
point	O
to	O
all	O
training	O
points	O
(	O
lines	O
2-4	O
)	O
.	O
the	O
data	O
points	O
are	O
then	O
sorted	O
according	O
to	O
distance	B
.	O
we	O
then	O
apply	O
a	O
clever	O
trick	O
of	O
summing	O
the	O
class	O
labels	O
for	O
each	O
of	O
the	O
k	O
nearest	O
neighbors	O
(	O
lines	O
6-10	O
)	O
and	O
using	O
the	O
sign	O
of	O
this	O
sum	O
as	O
our	O
prediction	O
.	O
the	O
big	O
question	O
,	O
of	O
course	O
,	O
is	O
how	O
to	O
choose	O
k.	O
as	O
we	O
’	O
ve	O
seen	O
,	O
with	O
k	O
=	O
1	O
,	O
we	O
run	O
the	O
risk	O
of	O
overﬁtting	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
k	O
is	O
large	O
(	O
for	O
instance	O
,	O
k	O
=	O
n	O
)	O
,	O
then	O
knn-predict	O
will	O
always	O
predict	B
the	O
majority	O
class	O
.	O
clearly	O
that	O
is	O
underﬁtting	O
.	O
so	O
,	O
k	O
is	O
a	O
hyperparameter	B
of	O
the	O
knn	O
algorithm	B
that	O
allows	O
us	O
to	O
trade-off	O
between	O
overﬁtting	O
(	O
small	O
value	O
of	O
k	O
)	O
and	O
underﬁtting	O
(	O
large	O
value	O
of	O
k	O
)	O
.	O
one	O
aspect	O
of	O
inductive	B
bias	I
that	O
we	O
’	O
ve	O
seen	O
for	O
knn	O
is	O
that	O
it	O
assumes	O
that	O
nearby	O
points	O
should	O
have	O
the	O
same	O
label	B
.	O
another	O
aspect	O
,	O
which	O
is	O
quite	O
different	O
from	O
decision	B
trees	I
,	O
is	O
that	O
all	O
features	O
are	O
equally	O
important	O
!	O
recall	B
that	O
for	O
decision	B
trees	I
,	O
the	O
key	O
question	O
was	O
which	O
features	B
are	O
most	O
useful	O
for	O
classiﬁcation	O
?	O
the	O
whole	O
learning	O
algorithm	B
for	O
a	O
decision	B
tree	I
hinged	O
on	O
ﬁnding	O
a	O
small	O
set	O
of	O
good	O
features	B
.	O
this	O
is	O
all	O
thrown	O
away	O
in	O
knn	O
classiﬁers	O
:	O
every	O
feature	O
is	O
used	O
,	O
and	O
they	O
are	O
all	O
used	O
the	O
same	O
amount	O
.	O
this	O
means	O
that	O
if	O
you	O
have	O
data	O
with	O
only	O
a	O
few	O
relevant	O
features	B
and	O
lots	O
of	O
irrelevant	O
features	B
,	O
knn	O
is	O
likely	O
to	O
do	O
poorly	O
.	O
a	O
related	O
issue	O
with	O
knn	O
is	O
feature	B
scale	I
.	O
suppose	O
that	O
we	O
are	O
trying	O
to	O
classify	O
whether	O
some	O
object	O
is	O
a	O
ski	O
or	O
a	O
snowboard	O
(	O
see	O
figure	O
2.5	O
)	O
.	O
we	O
are	O
given	O
two	O
features	B
about	O
this	O
data	O
:	O
the	O
width	O
and	O
height	O
.	O
as	O
is	O
standard	O
in	O
skiing	O
,	O
width	O
is	O
measured	O
in	O
millime-	O
ters	O
and	O
height	O
is	O
measured	O
in	O
centimeters	O
.	O
since	O
there	O
are	O
only	O
two	O
features	B
,	O
we	O
can	O
actually	O
plot	O
the	O
entire	O
training	O
set	O
;	O
see	O
figure	O
2.6	O
where	O
ski	O
is	O
the	O
positive	O
class	O
.	O
based	O
on	O
this	O
data	O
,	O
you	O
might	O
guess	O
that	O
a	O
knn	O
classiﬁer	O
would	O
do	O
well	O
.	O
suppose	O
,	O
however	O
,	O
that	O
our	O
measurement	O
of	O
the	O
width	O
was	O
com-	O
puted	O
in	O
millimeters	O
(	O
instead	O
of	O
centimeters	O
)	O
.	O
this	O
yields	O
the	O
data	O
shown	O
in	O
figure	O
2.7.	O
since	O
the	O
width	O
values	O
are	O
now	O
tiny	O
,	O
in	O
compar-	O
ison	O
to	O
the	O
height	O
values	O
,	O
a	O
knn	O
classiﬁer	O
will	O
effectively	O
ignore	O
the	O
width	O
values	O
and	O
classify	O
almost	O
purely	O
based	O
on	O
height	O
.	O
the	O
pre-	O
dicted	O
class	O
for	O
the	O
displayed	O
test	O
point	O
had	O
changed	O
because	O
of	O
this	O
feature	O
scaling	O
.	O
we	O
will	O
discuss	O
feature	O
scaling	O
more	O
in	O
chapter	O
4.	O
for	O
now	O
,	O
it	O
is	O
just	O
important	O
to	O
keep	O
in	O
mind	O
that	O
knn	O
does	O
not	O
have	O
the	O
power	O
to	O
decide	O
which	O
features	B
are	O
important	O
.	O
?	O
why	O
is	O
the	O
sign	O
of	O
the	O
sum	O
com-	O
puted	O
in	O
lines	O
2-4	O
the	O
same	O
as	O
the	O
majority	O
vote	B
of	O
the	O
associated	O
training	O
examples	O
?	O
?	O
why	O
can	O
’	O
t	O
you	O
simply	O
pick	O
the	O
value	O
of	O
k	O
that	O
does	O
best	O
on	O
the	O
training	O
data	O
?	O
in	O
other	O
words	O
,	O
why	O
do	O
we	O
have	O
to	O
treat	O
it	O
like	O
a	O
hy-	O
perparameter	O
rather	O
than	O
just	O
a	O
parameter	O
.	O
figure	O
2.5	O
:	O
a	O
ﬁgure	O
of	O
a	O
ski	O
and	O
snow-	O
board	O
with	O
width	O
(	O
mm	O
)	O
and	O
height	O
(	O
cm	O
)	O
.	O
figure	O
2.6	O
:	O
classiﬁcation	O
data	O
for	O
ski	O
vs	O
snowboard	O
in	O
2d	O
geometry	O
and	O
nearest	O
neighbors	O
31	O
2.3	O
decision	O
boundaries	O
the	O
standard	O
way	O
that	O
we	O
’	O
ve	O
been	O
thinking	O
about	O
learning	O
algo-	O
rithms	O
up	O
to	O
now	O
is	O
in	O
the	O
query	O
model	B
.	O
based	O
on	O
training	B
data	I
,	O
you	O
learn	O
something	O
.	O
i	O
then	O
give	O
you	O
a	O
query	O
example	O
and	O
you	O
have	O
to	O
guess	O
it	O
’	O
s	O
label	B
.	O
an	O
alternative	O
,	O
less	O
passive	O
,	O
way	O
to	O
think	O
about	O
a	O
learned	O
model	B
is	O
to	O
ask	O
:	O
what	O
sort	O
of	O
test	O
examples	O
will	O
it	O
classify	O
as	O
positive	O
,	O
and	O
what	O
sort	O
will	O
it	O
classify	O
as	O
negative	O
.	O
in	O
figure	O
2.9	O
,	O
we	O
have	O
a	O
set	O
of	O
training	B
data	I
.	O
the	O
background	O
of	O
the	O
image	O
is	O
colored	O
blue	O
in	O
regions	O
that	O
would	O
be	O
classiﬁed	O
as	O
positive	O
(	O
if	O
a	O
query	O
were	O
issued	O
there	O
)	O
and	O
colored	O
red	O
in	O
regions	O
that	O
would	O
be	O
classiﬁed	O
as	O
negative	O
.	O
this	O
coloring	O
is	O
based	O
on	O
a	O
1-nearest	O
neighbor	O
classiﬁer	O
.	O
in	O
figure	O
2.9	O
,	O
there	O
is	O
a	O
solid	O
line	O
separating	O
the	O
positive	O
regions	O
from	O
the	O
negative	O
regions	O
.	O
this	O
line	O
is	O
called	O
the	O
decision	O
boundary	O
for	O
this	O
classiﬁer	O
.	O
it	O
is	O
the	O
line	O
with	O
positive	O
land	O
on	O
one	O
side	O
and	O
negative	O
land	O
on	O
the	O
other	O
side	O
.	O
decision	O
boundaries	O
are	O
useful	O
ways	O
to	O
visualize	B
the	O
complex-	O
ity	O
of	O
a	O
learned	O
model	B
.	O
intuitively	O
,	O
a	O
learned	O
model	B
with	O
a	O
decision	B
boundary	I
that	O
is	O
really	O
jagged	O
(	O
like	O
the	O
coastline	O
of	O
norway	O
)	O
is	O
really	O
complex	O
and	O
prone	O
to	O
overﬁtting	O
.	O
a	O
learned	O
model	B
with	O
a	O
decision	B
boundary	I
that	O
is	O
really	O
simple	O
(	O
like	O
the	O
bounary	O
between	O
arizona	O
and	O
utah	O
)	O
is	O
potentially	O
underﬁt	O
.	O
in	O
figure	O
?	O
?	O
,	O
you	O
can	O
see	O
the	O
deci-	O
sion	O
boundaries	O
for	O
knn	O
models	O
with	O
k	O
∈	O
{	O
1	O
,	O
3	O
,	O
5	O
,	O
7	O
}	O
.	O
as	O
you	O
can	O
see	O
,	O
the	O
boundaries	O
become	O
simpler	O
and	O
simpler	O
as	O
k	O
gets	O
bigger	O
.	O
now	O
that	O
you	O
know	O
about	O
decision	O
boundaries	O
,	O
it	O
is	O
natural	O
to	O
ask	O
:	O
what	O
do	O
decision	O
boundaries	O
for	O
decision	B
trees	I
look	O
like	O
?	O
in	O
order	O
to	O
answer	O
this	O
question	O
,	O
we	O
have	O
to	O
be	O
a	O
bit	O
more	O
formal	O
about	O
how	O
to	O
build	O
a	O
decision	B
tree	I
on	O
real-valued	O
features	B
.	O
(	O
remember	O
that	O
the	O
algorithm	O
you	O
learned	O
in	O
the	O
previous	O
chapter	O
implicitly	O
assumed	O
binary	O
feature	O
values	O
.	O
)	O
the	O
idea	O
is	O
to	O
allow	O
the	O
decision	O
tree	O
to	O
ask	O
questions	O
of	O
the	O
form	O
:	O
“	O
is	O
the	O
value	O
of	O
feature	O
5	O
greater	O
than	O
0.2	O
?	O
”	O
that	O
is	O
,	O
for	O
real-valued	O
features	B
,	O
the	O
decision	O
tree	O
nodes	O
are	O
param-	O
eterized	O
by	O
a	O
feature	O
and	O
a	O
threshold	B
for	O
that	O
feature	O
.	O
an	O
example	O
decision	O
tree	O
for	O
classifying	O
skis	O
versus	O
snowboards	O
is	O
shown	O
in	O
fig-	O
ure	O
2.10.	O
now	O
that	O
a	O
decision	B
tree	I
can	O
handle	O
feature	O
vectors	O
,	O
we	O
can	O
talk	O
about	O
decision	O
boundaries	O
.	O
by	O
example	O
,	O
the	O
decision	O
boundary	O
for	O
the	O
decision	O
tree	O
in	O
figure	O
2.10	O
is	O
shown	O
in	O
figure	O
2.11.	O
in	O
the	O
ﬁgure	O
,	O
space	O
is	O
ﬁrst	O
split	O
in	O
half	O
according	O
to	O
the	O
ﬁrst	O
query	O
along	O
one	O
axis	O
.	O
then	O
,	O
depending	O
on	O
which	O
half	O
of	O
the	O
space	O
you	O
look	O
at	O
,	O
it	O
is	O
either	O
split	O
again	O
along	O
the	O
other	O
axis	O
,	O
or	O
simply	O
classiﬁed	O
.	O
figure	O
2.11	O
is	O
a	O
good	O
visualization	O
of	O
decision	O
boundaries	O
for	O
decision	B
trees	I
in	O
general	O
.	O
their	O
decision	O
boundaries	O
are	O
axis-aligned	O
figure	O
2.8	O
:	O
decision	B
boundary	I
for	O
1nn	O
.	O
figure	O
2.9	O
:	O
decision	B
boundary	I
for	O
knn	O
with	O
k=3	O
.	O
figure	O
2.10	O
:	O
decision	B
tree	I
for	O
ski	O
vs.	O
snowboard	O
figure	O
2.11	O
:	O
decision	B
boundary	I
for	O
dt	O
in	O
previous	O
ﬁgure	O
32	O
a	O
course	O
in	O
machine	O
learning	O
cuts	O
.	O
the	O
cuts	O
must	O
be	O
axis-aligned	O
because	O
nodes	O
can	O
only	O
query	O
on	O
a	O
single	O
feature	O
at	O
a	O
time	O
.	O
in	O
this	O
case	O
,	O
since	O
the	O
decision	O
tree	O
was	O
so	O
shallow	O
,	O
the	O
decision	O
boundary	O
was	O
relatively	O
simple	O
.	O
2.4	O
k-means	O
clustering	B
up	O
through	O
this	O
point	O
,	O
you	O
have	O
learned	O
all	O
about	O
supervised	O
learn-	O
ing	O
(	O
in	O
particular	O
,	O
binary	O
classiﬁcation	O
)	O
.	O
as	O
another	O
example	O
of	O
the	O
use	O
of	O
geometric	O
intuitions	O
and	O
data	O
,	O
we	O
are	O
going	O
to	O
temporarily	O
consider	O
an	O
unsupervised	B
learning	I
problem	O
.	O
in	O
unsupervised	O
learn-	O
ing	O
,	O
our	O
data	O
consists	O
only	O
of	O
examples	B
xn	O
and	O
does	O
not	O
contain	O
corre-	O
sponding	O
labels	O
.	O
your	O
job	O
is	O
to	O
make	O
sense	O
of	O
this	O
data	O
,	O
even	O
though	O
no	O
one	O
has	O
provided	O
you	O
with	O
correct	O
labels	O
.	O
the	O
particular	O
notion	O
of	O
“	O
making	O
sense	O
of	O
”	O
that	O
we	O
will	O
talk	O
about	O
now	O
is	O
the	O
clustering	O
task	O
.	O
consider	O
the	O
data	O
shown	O
in	O
figure	O
2.12.	O
since	O
this	O
is	O
unsupervised	B
learning	I
and	O
we	O
do	O
not	O
have	O
access	O
to	O
labels	O
,	O
the	O
data	O
points	O
are	O
simply	O
drawn	O
as	O
black	O
dots	O
.	O
your	O
job	O
is	O
to	O
split	O
this	O
data	O
set	O
into	O
three	O
clusters	O
.	O
that	O
is	O
,	O
you	O
should	O
label	B
each	O
data	O
point	O
as	O
a	O
,	O
b	O
or	O
c	O
in	O
whatever	O
way	O
you	O
want	O
.	O
for	O
this	O
data	O
set	O
,	O
it	O
’	O
s	O
pretty	O
clear	O
what	O
you	O
should	O
do	O
.	O
you	O
prob-	O
ably	O
labeled	O
the	O
upper-left	O
set	O
of	O
points	O
a	O
,	O
the	O
upper-right	O
set	O
of	O
points	O
b	O
and	O
the	O
bottom	O
set	O
of	O
points	O
c.	O
or	O
perhaps	O
you	O
permuted	O
these	O
labels	O
.	O
but	O
chances	O
are	O
your	O
clusters	O
were	O
the	O
same	O
as	O
mine	O
.	O
the	O
k-means	O
clustering	B
algorithm	O
is	O
a	O
particularly	O
simple	O
and	O
effective	O
approach	O
to	O
producing	O
clusters	O
on	O
data	O
like	O
you	O
see	O
in	O
fig-	O
ure	O
2.12.	O
the	O
idea	O
is	O
to	O
represent	O
each	O
cluster	O
by	O
it	O
’	O
s	O
cluster	O
center	O
.	O
given	O
cluster	O
centers	O
,	O
we	O
can	O
simply	O
assign	O
each	O
point	O
to	O
its	O
nearest	O
center	O
.	O
similarly	O
,	O
if	O
we	O
know	O
the	O
assignment	O
of	O
points	O
to	O
clusters	O
,	O
we	O
can	O
compute	O
the	O
centers	O
.	O
this	O
introduces	O
a	O
chicken-and-egg	O
problem	O
.	O
if	O
we	O
knew	O
the	O
clusters	O
,	O
we	O
could	O
compute	O
the	O
centers	O
.	O
if	O
we	O
knew	O
the	O
centers	O
,	O
we	O
could	O
compute	O
the	O
clusters	O
.	O
but	O
we	O
don	O
’	O
t	O
know	O
either	O
.	O
the	O
general	O
computer	O
science	O
answer	O
to	O
chicken-and-egg	O
problems	O
is	O
iteration	B
.	O
we	O
will	O
start	O
with	O
a	O
guess	O
of	O
the	O
cluster	O
centers	O
.	O
based	O
on	O
that	O
guess	O
,	O
we	O
will	O
assign	O
each	O
data	O
point	O
to	O
its	O
closest	O
center	O
.	O
given	O
these	O
new	O
assignments	O
,	O
we	O
can	O
recompute	O
the	O
cluster	O
centers	O
.	O
we	O
repeat	O
this	O
process	O
until	O
clusters	O
stop	O
moving	O
.	O
the	O
ﬁrst	O
few	O
it-	O
erations	O
of	O
the	O
k-means	O
algorithm	B
are	O
shown	O
in	O
figure	O
2.13.	O
in	O
this	O
example	O
,	O
the	O
clusters	O
converge	O
very	O
quickly	O
.	O
algorithm	B
2.4	O
spells	O
out	O
the	O
k-means	O
clustering	B
algorithm	O
in	O
de-	O
tail	O
.	O
the	O
cluster	O
centers	O
are	O
initialized	O
randomly	O
.	O
in	O
line	O
6	O
,	O
data	O
point	O
xn	O
is	O
compared	O
against	O
each	O
cluster	O
center	O
µk	O
.	O
it	O
is	O
assigned	O
to	O
cluster	O
k	O
if	O
k	O
is	O
the	O
center	O
with	O
the	O
smallest	O
distance	B
.	O
(	O
that	O
is	O
the	O
“	O
argmin	O
”	O
step	O
.	O
)	O
the	O
variable	O
zn	O
stores	O
the	O
assignment	O
(	O
a	O
value	O
from	O
1	O
to	O
k	O
)	O
of	O
example	O
n.	O
in	O
lines	O
8-12	O
,	O
the	O
cluster	O
centers	O
are	O
re-computed	O
.	O
first	O
,	O
xk	O
?	O
what	O
sort	O
of	O
data	O
might	O
yield	O
a	O
very	O
simple	O
decision	B
boundary	I
with	O
a	O
decision	B
tree	I
and	O
very	O
complex	O
decision	B
boundary	I
with	O
1-nearest	O
neighbor	O
?	O
what	O
about	O
the	O
other	O
way	O
around	O
?	O
figure	O
2.12	O
:	O
simple	O
clustering	B
data	O
...	O
clusters	O
in	O
ul	O
,	O
ur	O
and	O
bc	O
.	O
figure	O
2.13	O
:	O
ﬁrst	O
few	O
iterations	O
of	O
k-means	O
running	O
on	O
previous	O
data	O
set	O
µk	O
←	O
some	O
random	O
location	O
algorithm	B
4	O
k-means	O
(	O
d	O
,	O
k	O
)	O
1	O
:	O
for	O
k	O
=	O
1	O
to	O
k	O
do	O
2	O
:	O
3	O
:	O
end	O
for	O
4	O
:	O
repeat	O
5	O
:	O
for	O
n	O
=	O
1	O
to	O
n	O
do	O
zn	O
←	O
argmink	O
||µk	O
−	O
xn||	O
end	O
for	O
for	O
k	O
=	O
1	O
to	O
k	O
do	O
xk	O
←	O
{	O
xn	O
:	O
zn	O
=	O
k	O
}	O
µk	O
←	O
mean	O
(	O
xk	O
)	O
6	O
:	O
7	O
:	O
8	O
:	O
9	O
:	O
10	O
:	O
end	O
for	O
11	O
:	O
12	O
:	O
until	O
µs	O
stop	O
changing	O
13	O
:	O
return	O
z	O
geometry	O
and	O
nearest	O
neighbors	O
33	O
//	O
randomly	O
initialize	O
mean	O
for	O
kth	O
cluster	O
//	O
assign	O
example	O
n	O
to	O
closest	O
center	O
//	O
points	O
assigned	O
to	O
cluster	O
k	O
//	O
re-estimate	O
mean	O
of	O
cluster	O
k	O
//	O
return	O
cluster	O
assignments	O
math	O
review	O
|	O
vector	B
arithmetic	O
,	O
norms	O
and	O
means	O
deﬁne	O
vector	B
addition	O
,	O
scalar	O
addition	O
,	O
subtraction	O
,	O
scalar	O
multiplication	O
and	O
norms	O
.	O
deﬁne	O
mean	O
.	O
figure	O
2.14	O
:	O
stores	O
all	O
examples	O
that	O
have	O
been	O
assigned	O
to	O
cluster	O
k.	O
the	O
center	O
of	O
cluster	O
k	O
,	O
µk	O
is	O
then	O
computed	O
as	O
the	O
mean	O
of	O
the	O
points	O
assigned	O
to	O
it	O
.	O
this	O
process	O
repeats	O
until	O
the	O
means	O
converge	O
.	O
an	O
obvious	O
question	O
about	O
this	O
algorithm	B
is	O
:	O
does	O
it	O
converge	O
?	O
a	O
second	O
question	O
is	O
:	O
how	O
long	O
does	O
it	O
take	O
to	O
converge	O
.	O
the	O
ﬁrst	O
question	O
is	O
actually	O
easy	O
to	O
answer	O
.	O
yes	O
,	O
it	O
does	O
.	O
and	O
in	O
practice	O
,	O
it	O
usually	O
converges	O
quite	O
quickly	O
(	O
usually	O
fewer	O
than	O
20	O
iterations	O
)	O
.	O
in	O
chapter	O
13	O
,	O
we	O
will	O
actually	O
prove	O
that	O
it	O
converges	O
.	O
the	O
question	O
of	O
how	O
long	O
it	O
takes	O
to	O
converge	O
is	O
actually	O
a	O
really	O
interesting	O
question	O
.	O
even	O
though	O
the	O
k-means	O
algorithm	B
dates	O
back	O
to	O
the	O
mid	O
1950s	O
,	O
the	O
best	O
known	O
convergence	O
rates	O
were	O
terrible	O
for	O
a	O
long	O
time	O
.	O
here	O
,	O
ter-	O
rible	O
means	O
exponential	O
in	O
the	O
number	O
of	O
data	O
points	O
.	O
this	O
was	O
a	O
sad	O
situation	O
because	O
empirically	O
we	O
knew	O
that	O
it	O
converged	O
very	O
quickly	O
.	O
new	O
algorithm	B
analysis	O
techniques	O
called	O
“	O
smoothed	B
analysis	I
”	O
were	O
invented	O
in	O
2001	O
and	O
have	O
been	O
used	O
to	O
show	O
very	O
fast	O
convergence	O
for	O
k-means	O
(	O
among	O
other	O
algorithms	O
)	O
.	O
these	O
techniques	O
are	O
well	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
(	O
and	O
this	O
author	O
!	O
)	O
but	O
sufﬁce	O
it	O
to	O
say	O
that	O
k-means	O
is	O
fast	O
in	O
practice	O
and	O
is	O
provably	O
fast	O
in	O
theory	O
.	O
it	O
is	O
important	O
to	O
note	O
that	O
although	O
k-means	O
is	O
guaranteed	O
to	O
converge	O
and	O
guaranteed	O
to	O
converge	O
quickly	O
,	O
it	O
is	O
not	O
guaranteed	O
to	O
converge	O
to	O
the	O
“	O
right	O
answer.	O
”	O
the	O
key	O
problem	O
with	O
unsupervised	B
learning	I
is	O
that	O
we	O
have	O
no	O
way	O
of	O
knowing	O
what	O
the	O
“	O
right	O
answer	O
”	O
is	O
.	O
convergence	O
to	O
a	O
bad	O
solution	O
is	O
usually	O
due	O
to	O
poor	O
initialization	O
.	O
for	O
example	O
,	O
poor	O
initialization	O
in	O
the	O
data	O
set	O
from	O
before	O
yields	O
convergence	O
like	O
that	O
seen	O
in	O
figure	O
?	O
?	O
.	O
as	O
you	O
can	O
see	O
,	O
the	O
algorithm	O
34	O
a	O
course	O
in	O
machine	O
learning	O
has	O
converged	O
.	O
it	O
has	O
just	O
converged	O
to	O
something	O
less	O
than	O
satisfac-	O
tory	O
.	O
2.5	O
warning	O
:	O
high	O
dimensions	O
are	O
scary	O
visualizing	O
one	O
hundred	O
dimensional	O
space	O
is	O
incredibly	O
difﬁcult	O
for	O
humans	O
.	O
after	O
huge	O
amounts	O
of	O
training	O
,	O
some	O
people	O
have	O
reported	O
that	O
they	O
can	O
visualize	B
four	O
dimensional	O
space	O
in	O
their	O
heads	O
.	O
but	O
beyond	O
that	O
seems	O
impossible.1	O
in	O
addition	O
to	O
being	O
hard	O
to	O
visualize	B
,	O
there	O
are	O
at	O
least	O
two	O
addi-	O
tional	O
problems	O
in	O
high	O
dimensions	O
,	O
both	O
refered	O
to	O
as	O
the	B
curse	I
of	I
dimensionality	I
.	O
one	O
is	O
computational	O
,	O
the	O
other	O
is	O
mathematical	O
.	O
from	O
a	O
computational	O
perspective	O
,	O
consider	O
the	O
following	O
prob-	O
lem	O
.	O
for	O
k-nearest	O
neighbors	O
,	O
the	O
speed	O
of	O
prediction	O
is	O
slow	O
for	O
a	O
very	O
large	O
data	O
set	O
.	O
at	O
the	O
very	O
least	O
you	O
have	O
to	O
look	O
at	O
every	O
train-	O
ing	O
example	O
every	O
time	O
you	O
want	O
to	O
make	O
a	O
prediction	O
.	O
to	O
speed	O
things	O
up	O
you	O
might	O
want	O
to	O
create	O
an	O
indexing	O
data	O
structure	O
.	O
you	O
can	O
break	O
the	O
plane	O
up	O
into	O
a	O
grid	O
like	O
that	O
shown	O
in	O
figure	O
2.15.	O
now	O
,	O
when	O
the	O
test	O
point	O
comes	O
in	O
,	O
you	O
can	O
quickly	O
identify	O
the	O
grid	O
cell	O
in	O
which	O
it	O
lies	O
.	O
now	O
,	O
instead	O
of	O
considering	O
all	O
training	O
points	O
,	O
you	O
can	O
limit	O
yourself	O
to	O
training	O
points	O
in	O
that	O
grid	O
cell	O
(	O
and	O
perhaps	O
the	O
neighboring	O
cells	O
)	O
.	O
this	O
can	O
potentially	O
lead	O
to	O
huge	O
computa-	O
tional	O
savings	O
.	O
space	O
up	O
into	O
a	O
grid	O
whose	O
cells	O
are	O
0.2×0.2	O
,	O
we	O
can	O
clearly	O
do	O
this	O
with	O
25	O
grid	O
cells	O
in	O
two	O
dimensions	O
(	O
assuming	O
the	O
range	O
of	O
the	O
features	O
is	O
0	O
to	O
1	O
for	O
simplicity	O
)	O
.	O
in	O
three	O
dimensions	O
,	O
we	O
’	O
ll	O
need	O
125	O
=	O
5×5×5	O
grid	O
cells	O
.	O
in	O
four	O
dimensions	O
,	O
we	O
’	O
ll	O
need	O
625.	O
by	O
the	O
time	O
we	O
get	O
to	O
“	O
low	O
dimensional	O
”	O
data	O
in	O
20	O
dimensions	O
,	O
we	O
’	O
ll	O
need	O
95	O
,	O
367	O
,	O
431	O
,	O
640	O
,	O
625	O
grid	O
cells	O
(	O
that	O
’	O
s	O
95	O
trillion	O
,	O
which	O
is	O
about	O
6	O
to	O
7	O
times	O
the	O
us	O
national	O
debt	O
as	O
of	O
january	O
2011	O
)	O
.	O
so	O
if	O
you	O
’	O
re	O
in	O
20	O
dimensions	O
,	O
this	O
gridding	O
technique	O
will	O
only	O
be	O
useful	O
if	O
you	O
have	O
at	O
least	O
95	O
trillion	O
training	O
examples	O
.	O
in	O
two	O
dimensions	O
,	O
this	O
procedure	O
is	O
effective	O
.	O
if	O
we	O
want	O
to	O
break	O
for	O
“	O
medium	O
dimensional	O
”	O
data	O
(	O
approximately	O
1000	O
)	O
dimesions	O
,	O
the	O
number	O
of	O
grid	O
cells	O
is	O
a	O
9	O
followed	O
by	O
698	O
numbers	O
before	O
the	O
decimal	O
point	O
.	O
for	O
comparison	O
,	O
the	O
number	O
of	O
atoms	O
in	O
the	O
universe	O
is	O
approximately	O
1	O
followed	O
by	O
80	O
zeros	O
.	O
so	O
even	O
if	O
each	O
atom	O
yielded	O
a	O
googul	O
training	O
examples	O
,	O
we	O
’	O
d	O
still	O
have	O
far	O
fewer	O
examples	B
than	O
grid	O
cells	O
.	O
for	O
“	O
high	O
dimensional	O
”	O
data	O
(	O
approximately	O
100000	O
)	O
di-	O
mensions	O
,	O
we	O
have	O
a	O
1	O
followed	O
by	O
just	O
under	O
70	O
,	O
000	O
zeros	O
.	O
far	O
too	O
big	O
a	O
number	O
to	O
even	O
really	O
comprehend	O
.	O
sufﬁce	O
it	O
to	O
say	O
that	O
for	O
even	O
moderately	O
high	O
dimensions	O
,	O
the	O
amount	O
of	O
computation	O
involved	O
in	O
these	O
problems	O
is	O
enormous	O
.	O
in	O
addition	O
to	O
the	O
computational	O
difﬁculties	O
of	O
working	O
in	O
high	O
?	O
what	O
is	O
the	O
difference	O
between	O
un-	O
supervised	O
and	O
supervised	O
learning	O
that	O
means	O
that	O
we	O
know	O
what	O
the	O
“	O
right	O
answer	O
”	O
is	O
for	O
supervised	O
learning	O
but	O
not	O
for	O
unsupervised	B
learning	I
?	O
1	O
if	O
you	O
want	O
to	O
try	O
to	O
get	O
an	O
intu-	O
itive	O
sense	O
of	O
what	O
four	O
dimensions	O
looks	O
like	O
,	O
i	O
highly	O
recommend	O
the	O
short	O
1884	O
book	O
flatland	O
:	O
a	O
romance	O
of	O
many	O
dimensions	O
by	O
edwin	O
abbott	O
abbott	O
.	O
you	O
can	O
even	O
read	O
it	O
online	B
at	O
gutenberg.org/ebooks/201	O
.	O
figure	O
2.15	O
:	O
2d	O
knn	O
with	O
an	O
overlaid	O
grid	O
,	O
cell	O
with	O
test	O
point	O
highlighted	O
?	O
how	O
does	O
the	O
above	O
analysis	O
relate	O
to	O
the	O
number	O
of	O
data	O
points	O
you	O
would	O
need	O
to	O
ﬁll	O
out	O
a	O
full	O
decision	B
tree	I
with	O
d-many	O
features	B
?	O
what	O
does	O
this	O
say	O
about	O
the	O
importance	O
of	O
shallow	O
trees	O
?	O
geometry	O
and	O
nearest	O
neighbors	O
35	O
dimensions	O
,	O
there	O
are	O
a	O
large	O
number	O
of	O
strange	O
mathematical	O
oc-	O
curances	O
there	O
.	O
in	O
particular	O
,	O
many	O
of	O
your	O
intuitions	O
that	O
you	O
’	O
ve	O
built	O
up	O
from	O
working	O
in	O
two	O
and	O
three	O
dimensions	O
just	O
do	O
not	O
carry	O
over	O
to	O
high	O
dimensions	O
.	O
we	O
will	O
consider	O
two	O
effects	O
,	O
but	O
there	O
are	O
countless	O
others	O
.	O
the	O
ﬁrst	O
is	O
that	O
high	O
dimensional	O
spheres	O
look	O
more	O
like	O
porcupines	O
than	O
like	O
balls.2	O
the	O
second	O
is	O
that	O
distances	O
between	O
points	O
in	O
high	O
dimensions	O
are	O
all	O
approximately	O
the	O
same	O
.	O
let	O
’	O
s	O
start	O
in	O
two	O
dimensions	O
as	O
in	O
figure	O
2.16.	O
we	O
’	O
ll	O
start	O
with	O
four	O
green	O
spheres	O
,	O
each	O
of	O
radius	O
one	O
and	O
each	O
touching	O
exactly	O
two	O
other	O
green	O
spheres	O
.	O
(	O
remember	O
that	O
in	O
two	O
dimensions	O
a	O
“	O
sphere	O
”	O
is	O
just	O
a	O
“	O
circle.	O
”	O
)	O
we	O
’	O
ll	O
place	O
a	O
red	O
sphere	O
in	O
the	O
middle	O
so	O
that	O
it	O
touches	O
all	O
four	O
green	O
spheres	O
.	O
we	O
can	O
easily	O
compute	O
the	O
radius	O
of	O
this	O
small	O
sphere	O
.	O
the	O
pythagorean	O
theorem	O
says	O
that	O
12	O
+	O
12	O
=	O
(	O
1	O
+	O
2	O
−	O
1	O
≈	O
0.41.	O
thus	O
,	O
by	O
calculation	O
,	O
r	O
)	O
2	O
,	O
so	O
solving	O
for	O
r	O
we	O
get	O
r	O
=	O
the	O
blue	O
sphere	O
lies	O
entirely	O
within	O
the	O
cube	O
(	O
cube	O
=	O
square	O
)	O
that	O
contains	O
the	O
grey	O
spheres	O
.	O
(	O
yes	O
,	O
this	O
is	O
also	O
obvious	O
from	O
the	O
picture	O
,	O
but	O
perhaps	O
you	O
can	O
see	O
where	O
this	O
is	O
going	O
.	O
)	O
√	O
now	O
we	O
can	O
do	O
the	O
same	O
experiment	O
in	O
three	O
dimensions	O
,	O
as	O
shown	O
in	O
figure	O
2.17.	O
again	O
,	O
we	O
can	O
use	O
the	O
pythagorean	O
theorem	O
to	O
compute	O
the	O
radius	O
of	O
the	O
blue	O
sphere	O
.	O
now	O
,	O
we	O
get	O
12	O
+	O
12	O
+	O
12	O
=	O
3	O
−	O
1	O
≈	O
0.73.	O
this	O
is	O
still	O
entirely	O
enclosed	O
in	O
the	O
(	O
1	O
+	O
r	O
)	O
2	O
,	O
so	O
r	O
=	O
cube	O
of	O
width	O
four	O
that	O
holds	O
all	O
eight	O
grey	O
spheres	O
.	O
√	O
at	O
this	O
point	O
it	O
becomes	O
difﬁcult	O
to	O
produce	O
ﬁgures	O
,	O
so	O
you	O
’	O
ll	O
have	O
to	O
apply	O
your	O
imagination	O
.	O
in	O
four	O
dimensions	O
,	O
we	O
would	O
have	O
16	O
green	O
spheres	O
(	O
called	O
hyperspheres	B
)	O
,	O
each	O
of	O
radius	O
one	O
.	O
they	O
would	O
still	O
be	O
inside	O
a	O
cube	O
(	O
called	O
a	O
hypercube	B
)	O
of	O
width	O
four	O
.	O
the	O
4	O
−	O
1	O
=	O
1.	O
continuing	O
blue	O
hypersphere	O
would	O
have	O
radius	O
r	O
=	O
√	O
to	O
ﬁve	O
dimensions	O
,	O
the	O
blue	O
hypersphere	O
embedded	O
in	O
256	O
green	O
hyperspheres	B
would	O
have	O
radius	O
r	O
=	O
5	O
−	O
1	O
≈	O
1.23	O
and	O
so	O
on	O
.	O
√	O
in	O
general	O
,	O
in	O
d-dimensional	O
space	O
,	O
there	O
will	O
be	O
2d	O
green	O
hyper-	O
spheres	O
of	O
radius	O
one	O
.	O
each	O
green	O
hypersphere	O
will	O
touch	O
exactly	O
n-many	O
other	O
hyperspheres	B
.	O
the	O
blue	O
hyperspheres	B
in	O
the	O
middle	O
will	O
touch	O
them	O
all	O
and	O
will	O
have	O
radius	O
r	O
=	O
d	O
−	O
1	O
.	O
√	O
2	O
this	O
result	O
was	O
related	O
to	O
me	O
by	O
mark	O
reid	O
,	O
who	O
heard	O
about	O
it	O
from	O
marcus	O
hutter	O
.	O
figure	O
2.16	O
:	O
2d	O
spheres	O
in	O
spheres	O
figure	O
2.17	O
:	O
3d	O
spheres	O
in	O
spheres	O
think	O
about	O
this	O
for	O
a	O
moment	O
.	O
as	O
the	O
number	O
of	O
dimensions	O
grows	O
,	O
the	O
radius	O
of	O
the	O
blue	O
hypersphere	O
grows	O
without	O
bound	O
!	O
.	O
for	O
√	O
example	O
,	O
in	O
9-dimensions	O
the	O
radius	O
of	O
the	O
blue	O
hypersphere	O
is	O
now	O
9	O
−	O
1	O
=	O
2.	O
but	O
with	O
a	O
radius	O
of	O
two	O
,	O
the	O
blue	O
hypersphere	O
is	O
now	O
“	O
squeezing	O
”	O
between	O
the	O
green	O
hypersphere	O
and	O
touching	O
the	O
edges	O
of	O
the	O
hypercube	O
.	O
in	O
10	O
dimensional	O
space	O
,	O
the	O
radius	O
is	O
approxi-	O
mately	O
2.16	O
and	O
it	O
pokes	O
outside	O
the	O
cube	O
.	O
this	O
is	O
why	O
we	O
say	O
that	O
high	O
dimensional	O
spheres	O
look	O
like	O
por-	O
cupines	O
and	O
not	O
balls	O
(	O
see	O
figure	O
2.18	O
)	O
.	O
the	O
moral	O
of	O
this	O
story	O
from	O
a	O
machine	O
learning	O
perspective	O
is	O
that	O
intuitions	O
you	O
have	O
about	O
space	O
might	O
not	O
carry	O
over	O
to	O
high	O
dimensions	O
.	O
for	O
example	O
,	O
what	O
you	O
figure	O
2.18	O
:	O
porcupine	O
versus	O
ball	O
36	O
a	O
course	O
in	O
machine	O
learning	O
think	O
looks	O
like	O
a	O
“	O
round	O
”	O
cluster	O
in	O
two	O
or	O
three	O
dimensions	O
,	O
might	O
not	O
look	O
so	O
“	O
round	O
”	O
in	O
high	O
dimensions	O
.	O
the	O
second	O
strange	O
fact	O
we	O
will	O
consider	O
has	O
to	O
do	O
with	O
the	O
dis-	O
tances	O
between	O
points	O
in	O
high	O
dimensions	O
.	O
we	O
start	O
by	O
considering	O
random	O
points	O
in	O
one	O
dimension	O
.	O
that	O
is	O
,	O
we	O
generate	O
a	O
fake	O
data	O
set	O
consisting	O
of	O
100	O
random	O
points	O
between	O
zero	O
and	O
one	O
.	O
we	O
can	O
do	O
the	O
same	O
in	O
two	O
dimensions	O
and	O
in	O
three	O
dimensions	O
.	O
see	O
figure	O
2.19	O
for	O
data	O
distributed	O
uniformly	O
on	O
the	O
unit	O
hypercube	B
in	O
different	O
dimensions	O
.	O
now	O
,	O
pick	O
two	O
of	O
these	O
points	O
at	O
random	O
and	O
compute	O
the	O
dis-	O
tance	O
between	O
them	O
.	O
repeat	O
this	O
process	O
for	O
all	B
pairs	I
of	O
points	O
and	O
average	O
the	O
results	O
.	O
for	O
the	O
data	O
shown	O
in	O
figure	O
2.19	O
,	O
the	O
average	O
distance	B
between	O
points	O
in	O
one	O
dimension	O
is	O
about	O
0.346	O
;	O
in	O
two	O
di-	O
mensions	O
is	O
about	O
0.518	O
;	O
and	O
in	O
three	O
dimensions	O
is	O
0.615.	O
the	O
fact	O
that	O
these	O
increase	O
as	O
the	O
dimension	O
increases	O
is	O
not	O
surprising	O
.	O
the	O
furthest	O
two	O
points	O
can	O
be	O
in	O
a	O
1-dimensional	O
hypercube	B
(	O
line	O
)	O
is	O
1	O
;	O
the	O
furthest	O
in	O
a	O
2-dimensional	O
hypercube	B
(	O
square	O
)	O
is	O
2	O
(	O
opposite	O
corners	O
)	O
;	O
the	O
furthest	O
in	O
a	O
3-d	O
hypercube	B
is	O
the	O
furthest	O
two	O
points	O
in	O
a	O
d-dimensional	O
hypercube	B
will	O
be	O
√	O
d.	O
you	O
can	O
actually	O
compute	O
these	O
values	O
analytically	O
.	O
write	O
unid	O
for	O
the	O
uniform	O
distribution	O
in	O
d	O
dimensions	O
.	O
the	O
quantity	O
we	O
are	O
interested	O
in	O
computing	O
is	O
:	O
√	O
√	O
3	O
and	O
so	O
on	O
.	O
in	O
general	O
,	O
(	O
cid:104	O
)	O
(	O
cid:104	O
)	O
||a	O
−	O
b||	O
(	O
cid:105	O
)	O
(	O
cid:105	O
)	O
avgdist	O
(	O
d	O
)	O
=	O
ea∼unid	O
eb∼unid	O
(	O
2.2	O
)	O
we	O
can	O
actually	O
compute	O
this	O
in	O
closed	O
form	O
(	O
see	O
exercise	O
?	O
?	O
for	O
a	O
bit	O
of	O
calculus	O
refresher	O
)	O
and	O
arrive	O
at	O
avgdist	O
(	O
d	O
)	O
=	O
√	O
we	O
know	O
that	O
the	O
maximum	O
distance	B
between	O
two	O
points	O
grows	O
like	O
d	O
,	O
this	O
says	O
that	O
the	O
ratio	O
between	O
average	O
distance	B
and	O
maximum	O
d/3	O
.	O
because	O
√	O
distance	B
converges	O
to	O
1/3	O
.	O
√	O
what	O
is	O
more	O
interesting	O
,	O
however	O
,	O
is	O
the	O
variance	O
of	O
the	O
distribu-	O
tion	O
of	O
distances	O
.	O
you	O
can	O
show	O
that	O
in	O
d	O
dimensions	O
,	O
the	O
variance	O
is	O
constant	O
1/	O
18	O
,	O
independent	O
of	O
d.	O
this	O
means	O
that	O
when	O
you	O
look	O
at	O
(	O
variance	B
)	O
divided-by	O
(	O
max	O
distance	B
)	O
,	O
the	O
variance	O
behaves	O
like	O
1/	O
as	O
d	O
grows	O
3	O
.	O
18d	O
,	O
which	O
means	O
that	O
the	O
effective	O
variance	B
continues	O
to	O
shrink	O
√	O
when	O
i	O
ﬁrst	O
saw	O
and	O
re-proved	O
this	O
result	O
,	O
i	O
was	O
skeptical	O
,	O
as	O
i	O
imagine	O
you	O
are	O
.	O
so	O
i	O
implemented	O
it	O
.	O
in	O
figure	O
2.20	O
you	O
can	O
see	O
the	O
results	O
.	O
this	O
presents	O
a	O
histogram	B
of	O
distances	O
between	O
random	O
points	O
in	O
d	O
dimensions	O
for	O
d	O
∈	O
{	O
1	O
,	O
2	O
,	O
3	O
,	O
10	O
,	O
20	O
,	O
100	O
}	O
.	O
as	O
you	O
can	O
see	O
,	O
d	O
,	O
even	O
for	O
all	O
of	O
these	O
distances	O
begin	O
to	O
concentrate	O
around	O
0.4	O
“	O
medium	O
dimension	O
”	O
problems	O
.	O
√	O
you	O
should	O
now	O
be	O
terriﬁed	O
:	O
the	O
only	O
bit	O
of	O
information	O
that	O
knn	O
gets	O
is	O
distances	O
.	O
and	O
you	O
’	O
ve	O
just	O
seen	O
that	O
in	O
moderately	O
high	O
di-	O
mensions	O
,	O
all	O
distances	O
becomes	O
equal	O
.	O
so	O
then	O
isn	O
’	O
t	O
it	O
the	O
case	O
that	O
figure	O
2.19	O
:	O
100	O
uniform	O
random	O
points	O
in	O
1	O
,	O
2	O
and	O
3	O
dimensions	O
3	O
sergey	O
brin	O
.	O
near	O
neighbor	O
search	O
in	O
large	O
metric	O
spaces	O
.	O
in	O
conference	O
on	O
very	O
large	O
databases	O
(	O
vldb	O
)	O
,	O
1995	O
figure	O
2.20	O
:	O
histogram	B
of	O
distances	O
in	O
d=2,8,32,128,512	O
0.00.20.40.60.81.00.060.040.020.000.020.040.060.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.0distance	O
/	O
sqrt	O
(	O
dimensionality	O
)	O
02000400060008000100001200014000	O
#	O
of	O
pairs	O
of	O
points	O
at	O
that	O
distancedimensionality	O
versus	O
uniform	O
point	O
distances2	O
dims8	O
dims32	O
dims128	O
dims512	O
dims	O
geometry	O
and	O
nearest	O
neighbors	O
37	O
knn	O
simply	O
can	O
not	O
work	O
?	O
the	O
answer	O
has	O
to	O
be	O
no	O
.	O
the	O
reason	O
is	O
that	O
the	O
data	O
that	O
we	O
get	O
is	O
not	O
uniformly	O
distributed	O
over	O
the	O
unit	O
hypercube	B
.	O
we	O
can	O
see	O
this	O
by	O
looking	O
at	O
two	O
real-world	O
data	O
sets	O
.	O
the	O
ﬁrst	O
is	O
an	O
image	O
data	O
set	O
of	O
hand-written	O
digits	O
(	O
zero	O
through	O
nine	O
)	O
;	O
see	O
section	O
?	O
?	O
.	O
although	O
this	O
data	O
is	O
originally	O
in	O
256	O
dimensions	O
(	O
16	O
pixels	O
by	O
16	O
pixels	O
)	O
,	O
we	O
can	O
artiﬁcally	O
reduce	O
the	O
dimensionality	O
of	O
this	O
data	O
.	O
in	O
figure	O
2.21	O
you	O
can	O
see	O
the	O
histogram	O
of	O
average	O
distances	O
between	O
points	O
in	O
this	O
data	O
at	O
a	O
number	O
of	O
dimensions	O
.	O
as	O
you	O
can	O
see	O
from	O
these	O
histograms	O
,	O
distances	O
have	O
not	O
con-	O
centrated	O
around	O
a	O
single	O
value	O
.	O
this	O
is	O
very	O
good	O
news	O
:	O
it	O
means	O
that	O
there	O
is	O
hope	O
for	O
learning	O
algorithms	O
to	O
work	O
!	O
nevertheless	O
,	O
the	O
moral	O
is	O
that	O
high	O
dimensions	O
are	O
weird	O
.	O
2.6	O
extensions	O
to	O
knn	O
there	O
are	O
several	O
fundamental	O
problems	O
with	O
knn	O
classiﬁers	O
.	O
first	O
,	O
some	O
neighbors	O
might	O
be	O
“	O
better	O
”	O
than	O
others	O
.	O
second	O
,	O
test-time	O
per-	O
formance	O
scales	O
badly	O
as	O
your	O
number	O
of	O
training	O
examples	O
increases	O
.	O
third	O
,	O
it	O
treats	O
each	O
dimension	O
independently	B
.	O
we	O
will	O
not	O
address	O
the	O
third	O
issue	O
,	O
as	O
it	O
has	O
not	O
really	O
been	O
solved	O
(	O
though	O
it	O
makes	O
a	O
great	O
thought	O
question	O
!	O
)	O
.	O
regarding	O
neighborliness	O
,	O
consider	O
figure	O
2.22.	O
using	O
k	O
=	O
5	O
near-	O
est	O
neighbors	O
,	O
the	O
test	O
point	O
would	O
be	O
classiﬁed	O
as	O
positive	O
.	O
however	O
,	O
we	O
might	O
actually	O
believe	O
that	O
it	O
should	O
be	O
classiﬁed	O
negative	O
because	O
the	O
two	O
negative	O
neighbors	O
are	O
much	O
closer	O
than	O
the	O
three	O
positive	O
neighbors	O
.	O
there	O
are	O
at	O
least	O
two	O
ways	O
of	O
addressing	O
this	O
issue	O
.	O
the	O
ﬁrst	O
is	O
the	O
-ball	O
solution	O
.	O
instead	O
of	O
connecting	O
each	O
data	O
point	O
to	O
some	O
ﬁxed	O
number	O
(	O
k	O
)	O
of	O
nearest	O
neighbors	O
,	O
we	O
simply	O
connect	O
it	O
to	O
all	O
neigh-	O
bors	O
that	O
fall	O
within	O
some	O
ball	O
of	O
radius	O
	O
.	O
then	O
,	O
the	O
majority	O
class	O
of	O
all	O
the	O
points	O
in	O
the	O
	O
ball	O
wins	O
.	O
in	O
the	O
case	O
of	O
a	O
tie	O
,	O
you	O
would	O
have	O
to	O
either	O
guess	O
,	O
or	O
report	O
the	O
majority	O
class	O
.	O
figure	O
2.23	O
shows	O
an	O
	O
ball	O
around	O
the	O
test	O
point	O
that	O
happens	O
to	O
yield	O
the	O
proper	O
classiﬁca-	O
tion	O
.	O
when	O
using	O
-ball	B
nearest	O
neighbors	O
rather	O
than	O
knn	O
,	O
the	O
hyper-	O
parameter	O
changes	O
from	O
k	O
to	O
	O
.	O
you	O
would	O
need	O
to	O
set	O
it	O
in	O
the	O
same	O
way	O
as	O
you	O
would	O
for	O
knn	O
.	O
an	O
alternative	O
to	O
the	O
-ball	O
solution	O
is	O
to	O
do	O
weighted	B
nearest	I
neighbors	I
.	O
the	O
idea	O
here	O
is	O
to	O
still	O
consider	O
the	O
k-nearest	O
neighbors	O
of	O
a	O
test	O
point	O
,	O
but	O
give	O
them	O
uneven	O
votes	O
.	O
closer	O
points	O
get	O
more	O
vote	B
than	O
further	O
points	O
.	O
when	O
classifying	O
a	O
point	O
ˆx	O
,	O
the	O
usual	O
strat-	O
egy	O
is	O
to	O
give	O
a	O
training	O
point	O
xn	O
a	O
vote	B
that	O
decays	O
exponentially	O
in	O
the	O
distance	O
between	O
ˆx	O
and	O
xn	O
.	O
mathematically	O
,	O
the	O
vote	O
that	O
neigh-	O
figure	O
2.21	O
:	O
knn	O
:	O
mnist	O
:	O
histogram	B
of	O
distances	O
in	O
multiple	O
d	O
for	O
mnist	O
figure	O
2.22	O
:	O
data	O
set	O
with	O
5nn	O
,	O
test	O
point	O
closest	O
to	O
two	O
negatives	O
,	O
then	O
to	O
three	O
far	O
positives	O
figure	O
2.23	O
:	O
same	O
as	O
previous	O
with	O
	O
ball	O
?	O
one	O
issue	O
with	O
-balls	O
is	O
that	O
the	O
-ball	O
for	O
some	O
test	O
point	O
might	O
be	O
empty	O
.	O
how	O
would	O
you	O
handle	O
this	O
?	O
38	O
a	O
course	O
in	O
machine	O
learning	O
bor	O
n	O
gets	O
is	O
:	O
(	O
cid:20	O
)	O
exp	O
||ˆx	O
−	O
xn||2	O
−	O
1	O
2	O
(	O
cid:21	O
)	O
(	O
2.3	O
)	O
thus	O
,	O
nearby	O
points	O
get	O
a	O
vote	B
very	O
close	O
to	O
1	O
and	O
far	O
away	O
points	O
get	O
a	O
vote	B
very	O
close	O
to	O
0.	O
the	O
overall	O
prediction	O
is	O
positive	O
if	O
the	O
sum	O
of	O
votes	O
from	O
positive	O
neighbors	O
outweighs	O
the	O
sum	O
of	O
votes	O
from	O
negative	O
neighbors	O
.	O
the	O
second	O
issue	O
with	O
knn	O
is	O
scaling	O
.	O
to	O
predict	B
the	O
label	B
of	O
a	O
single	O
test	O
point	O
,	O
we	O
need	O
to	O
ﬁnd	O
the	O
k	O
nearest	O
neighbors	O
of	O
that	O
test	O
point	O
in	O
the	O
training	O
data	O
.	O
with	O
a	O
standard	O
implementation	O
,	O
this	O
will	O
take	O
o	O
(	O
nd	O
+	O
k	O
log	O
k	O
)	O
time4	O
.	O
for	O
very	O
large	O
data	O
sets	O
,	O
this	O
is	O
impractical	O
.	O
a	O
ﬁrst	O
attempt	O
to	O
speed	O
up	O
the	O
computation	O
is	O
to	O
represent	O
each	O
class	O
by	O
a	O
representative	O
.	O
a	O
natural	O
choice	O
for	O
a	O
representative	O
would	O
be	O
the	O
mean	O
.	O
we	O
would	O
collapse	O
all	O
positive	O
examples	B
down	O
to	O
their	O
mean	O
,	O
and	O
all	O
negative	O
examples	B
down	O
to	O
their	O
mean	O
.	O
we	O
could	O
then	O
just	O
run	O
1-nearest	O
neighbor	O
and	O
check	O
whether	O
a	O
test	O
point	O
is	O
closer	O
to	O
the	O
mean	O
of	O
the	O
positive	O
points	O
or	O
the	O
mean	O
of	O
the	O
negative	O
points	O
.	O
figure	O
2.24	O
shows	O
an	O
example	O
in	O
which	O
this	O
would	O
probably	O
work	O
well	O
,	O
and	O
an	O
example	O
in	O
which	O
this	O
would	O
probably	O
work	O
poorly	O
.	O
the	O
problem	O
is	O
that	O
collapsing	O
each	O
class	O
to	O
its	O
mean	O
is	O
too	O
aggressive	O
.	O
a	O
less	O
aggressive	O
approach	O
is	O
to	O
make	O
use	O
of	O
the	O
k-means	O
algo-	O
rithm	O
for	O
clustering	B
.	O
you	O
can	O
cluster	O
the	O
positive	O
examples	B
into	O
l	O
clusters	O
(	O
we	O
are	O
using	O
l	O
to	O
avoid	O
variable	O
overloading	O
!	O
)	O
and	O
then	O
cluster	O
the	O
negative	O
examples	B
into	O
l	O
separate	O
clusters	O
.	O
this	O
is	O
shown	O
in	O
figure	O
2.25	O
with	O
l	O
=	O
2.	O
instead	O
of	O
storing	O
the	O
entire	O
data	O
set	O
,	O
you	O
would	O
only	O
store	O
the	O
means	O
of	O
the	O
l	O
positive	O
clusters	O
and	O
the	O
means	O
of	O
the	O
l	O
negative	O
clusters	O
.	O
at	O
test	O
time	O
,	O
you	O
would	O
run	O
the	O
k-nearest	O
neighbors	O
algorithm	B
against	O
these	O
means	O
rather	O
than	O
against	O
the	O
full	O
training	O
set	O
.	O
this	O
leads	O
to	O
a	O
much	O
faster	O
runtime	O
of	O
just	O
o	O
(	O
ld	O
+	O
k	O
log	O
k	O
)	O
,	O
which	O
is	O
probably	O
dominated	O
by	O
ld	O
.	O
?	O
could	O
you	O
combine	O
the	O
-ball	O
idea	O
with	O
the	O
weighted	O
voting	B
idea	O
?	O
does	O
it	O
make	O
sense	O
,	O
or	O
does	O
one	O
idea	O
seem	O
to	O
trump	O
the	O
other	O
?	O
4	O
the	O
nd	O
term	O
comes	O
from	O
computing	O
distances	O
between	O
the	O
test	O
point	O
and	O
all	O
training	O
points	O
.	O
the	O
k	O
log	O
k	O
term	O
comes	O
from	O
ﬁnding	O
the	O
k	O
smallest	O
values	O
in	O
the	O
list	O
of	O
distances	O
,	O
using	O
a	O
median-ﬁnding	O
algorithm	B
.	O
of	O
course	O
,	O
nd	O
almost	O
always	O
dominates	B
k	O
log	O
k	O
in	O
practice	O
.	O
2.7	O
exercises	O
exercise	O
2.1.	O
todo	O
.	O
.	O
.	O
figure	O
2.24	O
:	O
knn	O
:	O
collapse	O
:	O
two	O
ﬁgures	O
of	O
points	O
collapsed	O
to	O
mean	O
,	O
one	O
with	O
good	O
results	O
and	O
one	O
with	O
dire	O
results	O
figure	O
2.25	O
:	O
knn	O
:	O
collapse2	O
:	O
data	O
from	O
previous	O
bad	O
case	O
collapsed	O
into	O
l=2	O
cluster	O
and	O
test	O
point	O
classiﬁed	O
based	O
on	O
means	O
and	O
1-nn	O
clustering	B
of	O
classes	O
was	O
intro-	O
duced	O
as	O
a	O
way	O
of	O
making	O
things	O
faster	O
.	O
will	O
it	O
make	O
things	O
worse	O
,	O
or	O
?	O
3	O
|	O
the	O
perceptron	O
–	O
so	O
far	O
,	O
you	O
’	O
ve	O
seen	O
two	O
types	O
of	O
learning	O
models	O
:	O
in	O
decision	B
trees	I
,	O
only	O
a	O
small	O
number	O
of	O
features	B
are	O
used	O
to	O
make	O
decisions	O
;	O
in	O
nearest	B
neighbor	I
algorithms	O
,	O
all	O
features	O
are	O
used	O
equally	O
.	O
neither	O
of	O
these	O
extremes	O
is	O
always	O
desirable	O
.	O
in	O
some	O
problems	O
,	O
we	O
might	O
want	O
to	O
use	O
most	O
of	O
the	O
features	O
,	O
but	O
use	O
some	O
more	O
than	O
others	O
.	O
in	O
this	O
chapter	O
,	O
we	O
’	O
ll	O
discuss	O
the	O
perceptron	O
algorithm	B
for	O
learn-	O
ing	O
weights	B
for	O
features	B
.	O
as	O
we	O
’	O
ll	O
see	O
,	O
learning	O
weights	B
for	O
features	B
amounts	O
to	O
learning	O
a	O
hyperplane	B
classiﬁer	O
:	O
that	O
is	O
,	O
basically	O
a	O
di-	O
vision	O
of	O
space	O
into	O
two	O
halves	O
by	O
a	O
straight	O
line	O
,	O
where	O
one	O
half	O
is	O
“	O
positive	O
”	O
and	O
one	O
half	O
is	O
“	O
negative.	O
”	O
in	O
this	O
sense	O
,	O
the	O
perceptron	O
can	O
be	O
seen	O
as	O
explicitly	O
ﬁnding	O
a	O
good	O
linear	B
decision	I
boundary	I
.	O
learning	O
objectives	O
:	O
•	O
describe	O
the	O
biological	O
motivation	O
behind	O
the	O
perceptron	O
.	O
•	O
classify	O
learning	O
algorithms	O
based	O
on	O
whether	O
they	O
are	O
error-driven	O
or	O
not	O
.	O
•	O
implement	O
the	O
perceptron	O
algorithm	B
for	O
binary	O
classiﬁcation	O
.	O
•	O
draw	O
perceptron	B
weight	O
vectors	O
and	O
the	O
corresponding	O
decision	O
boundaries	O
in	O
two	O
dimensions	O
.	O
•	O
contrast	O
the	O
decision	O
boundaries	O
of	O
decision	B
trees	I
,	O
nearest	B
neighbor	I
algorithms	O
and	O
perceptrons	O
.	O
•	O
compute	O
the	O
margin	O
of	O
a	O
given	O
weight	O
vector	B
on	O
a	O
given	O
data	O
set	O
.	O
3.1	O
bio-inspired	O
learning	O
dependencies	O
:	O
chapter	O
1	O
,	O
chapter	O
2	O
folk	O
biology	O
tells	O
us	O
that	O
our	O
brains	O
are	O
made	O
up	O
of	O
a	O
bunch	O
of	O
little	O
units	O
,	O
called	O
neurons	B
,	O
that	O
send	O
electrical	O
signals	O
to	O
one	O
another	O
.	O
the	O
rate	O
of	O
ﬁring	O
tells	O
us	O
how	O
“	O
activated	O
”	O
a	O
neuron	O
is	O
.	O
a	O
single	O
neuron	O
,	O
like	O
that	O
shown	O
in	O
figure	O
3.1	O
might	O
have	O
three	O
incoming	O
neurons	B
.	O
these	O
incoming	O
neurons	B
are	O
ﬁring	O
at	O
different	O
rates	O
(	O
i.e.	O
,	O
have	O
dif-	O
ferent	O
activations	B
)	O
.	O
based	O
on	O
how	O
much	O
these	O
incoming	O
neurons	B
are	O
ﬁring	O
,	O
and	O
how	O
“	O
strong	O
”	O
the	O
neural	O
connections	O
are	O
,	O
our	O
main	O
neu-	O
ron	O
will	O
“	O
decide	O
”	O
how	O
strongly	O
it	O
wants	O
to	O
ﬁre	O
.	O
and	O
so	O
on	O
through	O
the	O
whole	O
brain	O
.	O
learning	O
in	O
the	O
brain	O
happens	O
by	O
neurons	B
becom-	O
ming	O
connected	O
to	O
other	O
neurons	B
,	O
and	O
the	O
strengths	O
of	O
connections	O
adapting	O
over	O
time	O
.	O
the	O
real	O
biological	O
world	O
is	O
much	O
more	O
complicated	O
than	O
this	O
.	O
however	O
,	O
our	O
goal	O
isn	O
’	O
t	O
to	O
build	O
a	O
brain	O
,	O
but	O
to	O
simply	O
be	O
inspired	O
by	O
how	O
they	O
work	O
.	O
we	O
are	O
going	O
to	O
think	O
of	O
our	O
learning	O
algorithm	B
as	O
a	O
single	O
neuron	O
.	O
it	O
receives	O
input	O
from	O
d-many	O
other	O
neurons	B
,	O
one	O
for	O
each	O
input	O
feature	O
.	O
the	O
strength	O
of	O
these	O
inputs	O
are	O
the	O
fea-	O
ture	O
values	O
.	O
this	O
is	O
shown	O
schematically	O
in	O
figure	O
?	O
?	O
.	O
each	O
incom-	O
ing	O
connection	O
has	O
a	O
weight	O
and	O
the	O
neuron	O
simply	O
sums	O
up	O
all	O
the	O
weighted	O
inputs	O
.	O
based	O
on	O
this	O
sum	O
,	O
it	O
decides	O
whether	O
to	O
“	O
ﬁre	O
”	O
or	O
figure	O
3.1	O
:	O
a	O
picture	O
of	O
a	O
neuron	O
figure	O
3.2	O
:	O
ﬁgure	O
showing	O
feature	B
vector	I
and	O
weight	O
vector	B
and	O
products	O
and	O
sum	O
40	O
a	O
course	O
in	O
machine	O
learning	O
not	O
.	O
firing	O
is	O
interpreted	O
as	O
being	O
a	O
positive	O
example	O
and	O
not	O
ﬁring	O
is	O
interpreted	O
as	O
being	O
a	O
negative	O
example	O
.	O
in	O
particular	O
,	O
if	O
the	O
weighted	O
sum	O
is	O
positive	O
,	O
it	O
“	O
ﬁres	O
”	O
and	O
otherwise	O
it	O
doesn	O
’	O
t	O
ﬁre	O
.	O
this	O
is	O
shown	O
diagramatically	O
in	O
figure	O
3.2.	O
mathematically	O
,	O
an	O
input	O
vector	B
x	O
=	O
(	O
cid:104	O
)	O
x1	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
xd	O
(	O
cid:105	O
)	O
arrives	O
.	O
the	O
neuron	O
stores	O
d-many	O
weights	B
,	O
w1	O
,	O
w2	O
,	O
.	O
.	O
.	O
,	O
wd	O
.	O
the	O
neuron	O
computes	O
the	O
sum	O
:	O
d∑	O
a	O
=	O
(	O
3.1	O
)	O
wdxd	O
d=1	O
to	O
determine	O
it	O
’	O
s	O
amount	O
of	O
“	O
activation.	O
”	O
if	O
this	O
activiation	O
is	O
posi-	O
tive	O
(	O
i.e.	O
,	O
a	O
>	O
0	O
)	O
it	O
predicts	O
that	O
this	O
example	O
is	O
a	O
positive	O
example	O
.	O
otherwise	O
it	O
predicts	O
a	O
negative	O
example	O
.	O
the	O
weights	O
of	O
this	O
neuron	O
are	O
fairly	O
easy	O
to	O
interpret	O
.	O
suppose	O
that	O
a	O
feature	O
,	O
for	O
instance	O
“	O
is	O
this	O
a	O
system	O
’	O
s	O
class	O
?	O
”	O
gets	O
a	O
zero	O
weight	O
.	O
then	O
the	O
activation	O
is	O
the	O
same	O
regardless	O
of	O
the	O
value	O
of	O
this	O
feature	O
.	O
so	O
features	B
with	O
zero	O
weight	O
are	O
ignored	O
.	O
features	B
with	O
positive	O
weights	O
are	O
indicative	O
of	O
positive	O
examples	O
because	O
they	O
cause	O
the	O
activation	O
to	O
increase	O
.	O
features	B
with	O
negative	O
weights	B
are	O
indicative	O
of	O
negative	O
examples	B
because	O
they	O
cause	O
the	O
activiation	O
to	O
decrease	O
.	O
it	O
is	O
often	O
convenient	O
to	O
have	O
a	O
non-zero	O
threshold	B
.	O
in	O
other	O
words	O
,	O
we	O
might	O
want	O
to	O
predict	B
positive	O
if	O
a	O
>	O
θ	O
for	O
some	O
value	O
θ.	O
the	O
way	O
that	O
is	O
most	O
convenient	O
to	O
achieve	O
this	O
is	O
to	O
introduce	O
a	O
bias	B
term	O
into	O
the	O
neuron	O
,	O
so	O
that	O
the	O
activation	O
is	O
always	O
increased	O
by	O
some	O
ﬁxed	O
value	O
b.	O
thus	O
,	O
we	O
compute	O
:	O
(	O
3.2	O
)	O
(	O
cid:35	O
)	O
(	O
cid:34	O
)	O
d∑	O
d=1	O
a	O
=	O
wdxd	O
+	O
b	O
?	O
what	O
would	O
happen	O
if	O
we	O
encoded	O
binary	B
features	I
like	O
“	O
is	O
this	O
a	O
sys-	O
tem	O
’	O
s	O
class	O
”	O
as	O
no=0	O
and	O
yes=−1	O
(	O
rather	O
than	O
the	O
standard	O
no=0	O
and	O
yes=+1	O
)	O
?	O
this	O
is	O
the	O
complete	O
neural	O
model	O
of	O
learning	O
.	O
the	O
model	O
is	O
pa-	O
rameterized	O
by	O
d-many	O
weights	B
,	O
w1	O
,	O
w2	O
,	O
.	O
.	O
.	O
,	O
wd	O
,	O
and	O
a	O
single	O
scalar	O
bias	B
value	O
b.	O
?	O
if	O
you	O
wanted	O
the	O
activation	O
thresh-	O
old	O
to	O
be	O
a	O
>	O
θ	O
instead	O
of	O
a	O
>	O
0	O
,	O
what	O
value	O
would	O
b	O
have	O
to	O
be	O
?	O
3.2	O
error-driven	O
updating	O
:	O
the	O
perceptron	O
algorithm	B
vignette	O
:	O
the	O
history	O
of	O
the	O
perceptron	O
todo	O
the	O
perceptron	O
is	O
a	O
classic	O
learning	O
algorithm	B
for	O
the	O
neural	O
model	B
of	O
learning	O
.	O
like	O
k-nearest	O
neighbors	O
,	O
it	O
is	O
one	O
of	O
those	O
frustrating	O
algorithms	O
that	O
is	O
incredibly	O
simple	O
and	O
yet	O
works	O
amazingly	O
well	O
,	O
for	O
some	O
types	O
of	O
problems	O
.	O
the	O
perceptron	O
41	O
//	O
initialize	O
weights	B
//	O
initialize	O
bias	B
algorithm	O
5	O
perceptrontrain	O
(	O
d	O
,	O
maxiter	O
)	O
1	O
:	O
wd	O
←	O
0	O
,	O
for	O
all	O
d	O
=	O
1	O
.	O
.	O
.	O
d	O
2	O
:	O
b	O
←	O
0	O
3	O
:	O
for	O
iter	O
=	O
1	O
.	O
.	O
.	O
maxiter	O
do	O
4	O
:	O
for	O
all	O
(	O
x	O
,	O
y	O
)	O
∈	O
d	O
do	O
d=1	O
wd	O
xd	O
+	O
b	O
a	O
←	O
∑d	O
if	O
ya	O
≤	O
0	O
then	O
wd	O
←	O
wd	O
+	O
yxd	O
,	O
for	O
all	O
d	O
=	O
1	O
.	O
.	O
.	O
d	O
b	O
←	O
b	O
+	O
y	O
5	O
:	O
6	O
:	O
7	O
:	O
8	O
:	O
9	O
:	O
//	O
compute	O
activation	O
for	O
this	O
example	O
//	O
update	O
weights	B
//	O
update	O
bias	B
end	O
if	O
end	O
for	O
10	O
:	O
11	O
:	O
end	O
for	O
12	O
:	O
return	O
w0	O
,	O
w1	O
,	O
.	O
.	O
.	O
,	O
wd	O
,	O
b	O
algorithm	B
6	O
perceptrontest	O
(	O
w0	O
,	O
w1	O
,	O
.	O
.	O
.	O
,	O
wd	O
,	O
b	O
,	O
ˆx	O
)	O
1	O
:	O
a	O
←	O
∑d	O
2	O
:	O
return	O
sign	B
(	O
a	O
)	O
d=1	O
wd	O
ˆxd	O
+	O
b	O
//	O
compute	O
activation	O
for	O
the	O
test	O
example	O
the	O
algorithm	B
is	O
actually	O
quite	O
different	O
than	O
either	O
the	O
decision	O
tree	O
algorithm	B
or	O
the	O
knn	O
algorithm	B
.	O
first	O
,	O
it	O
is	O
online	B
.	O
this	O
means	O
that	O
instead	O
of	O
considering	O
the	O
entire	O
data	O
set	O
at	O
the	O
same	O
time	O
,	O
it	O
only	O
ever	O
looks	O
at	O
one	O
example	O
.	O
it	O
processes	O
that	O
example	O
and	O
then	O
goes	O
on	O
to	O
the	O
next	O
one	O
.	O
second	O
,	O
it	O
is	O
error	B
driven	I
.	O
this	O
means	O
that	O
,	O
so	O
long	O
as	O
it	O
is	O
doing	O
well	O
,	O
it	O
doesn	O
’	O
t	O
bother	O
updating	O
its	O
parameters	O
.	O
the	O
algorithm	O
maintains	O
a	O
“	O
guess	O
”	O
at	O
good	O
parameters	O
(	O
weights	B
and	O
bias	B
)	O
as	O
it	O
runs	O
.	O
it	O
processes	O
one	O
example	O
at	O
a	O
time	O
.	O
for	O
a	O
given	O
example	O
,	O
it	O
makes	O
a	O
prediction	O
.	O
it	O
checks	O
to	O
see	O
if	O
this	O
prediction	O
is	O
correct	O
(	O
recall	B
that	O
this	O
is	O
training	B
data	I
,	O
so	O
we	O
have	O
access	O
to	O
true	O
labels	O
)	O
.	O
if	O
the	O
prediction	O
is	O
correct	O
,	O
it	O
does	O
nothing	O
.	O
only	O
when	O
the	O
prediction	O
is	O
incorrect	O
does	O
it	O
change	O
its	O
parameters	O
,	O
and	O
it	O
changes	O
them	O
in	O
such	O
a	O
way	O
that	O
it	O
would	O
do	O
better	O
on	O
this	O
example	O
next	O
time	O
around	O
.	O
it	O
then	O
goes	O
on	O
to	O
the	O
next	O
example	O
.	O
once	O
it	O
hits	O
the	O
last	O
example	O
in	O
the	O
training	O
set	O
,	O
it	O
loops	O
back	O
around	O
for	O
a	O
speciﬁed	O
number	O
of	O
iterations	O
.	O
the	O
training	O
algorithm	B
for	O
the	O
perceptron	O
is	O
shown	O
in	O
algo-	O
rithm	O
3.2	O
and	O
the	O
corresponding	O
prediction	O
algorithm	B
is	O
shown	O
in	O
algorithm	B
3.2.	O
there	O
is	O
one	O
“	O
trick	O
”	O
in	O
the	O
training	O
algorithm	B
,	O
which	O
probably	O
seems	O
silly	O
,	O
but	O
will	O
be	O
useful	O
later	O
.	O
it	O
is	O
in	O
line	O
6	O
,	O
when	O
we	O
check	O
to	O
see	O
if	O
we	O
want	O
to	O
make	O
an	O
update	O
or	O
not	O
.	O
we	O
want	O
to	O
make	O
an	O
update	O
if	O
the	O
current	O
prediction	O
(	O
just	O
sign	B
(	O
a	O
)	O
)	O
is	O
incorrect	O
.	O
the	O
trick	O
is	O
to	O
multiply	O
the	O
true	O
label	B
y	O
by	O
the	O
activation	O
a	O
and	O
compare	O
this	O
against	O
zero	O
.	O
since	O
the	O
label	O
y	O
is	O
either	O
+1	O
or	O
−1	O
,	O
you	O
just	O
need	O
to	O
realize	O
that	O
ya	O
is	O
positive	O
whenever	O
a	O
and	O
y	O
have	O
the	O
same	O
sign	B
.	O
in	O
other	O
words	O
,	O
the	O
product	O
ya	O
is	O
positive	O
if	O
the	O
current	O
prediction	O
is	O
correct	O
.	O
?	O
it	O
is	O
very	O
very	O
important	O
to	O
check	O
ya	O
≤	O
0	O
rather	O
than	O
ya	O
<	O
0.	O
why	O
?	O
42	O
a	O
course	O
in	O
machine	O
learning	O
the	O
particular	O
form	O
of	O
update	O
for	O
the	O
perceptron	O
is	O
quite	O
simple	O
.	O
the	O
weight	O
wd	O
is	O
increased	O
by	O
yxd	O
and	O
the	O
bias	O
is	O
increased	O
by	O
y.	O
the	O
goal	O
of	O
the	O
update	O
is	O
to	O
adjust	O
the	O
parameters	O
so	O
that	O
they	O
are	O
“	O
bet-	O
ter	O
”	O
for	O
the	O
current	O
example	O
.	O
in	O
other	O
words	O
,	O
if	O
we	O
saw	O
this	O
example	O
twice	O
in	O
a	O
row	O
,	O
we	O
should	O
do	O
a	O
better	O
job	O
the	O
second	O
time	O
around	O
.	O
to	O
see	O
why	O
this	O
particular	O
update	O
achieves	O
this	O
,	O
consider	O
the	O
fol-	O
lowing	O
scenario	O
.	O
we	O
have	O
some	O
current	O
set	O
of	O
parameters	O
w1	O
,	O
.	O
.	O
.	O
,	O
wd	O
,	O
b.	O
we	O
observe	O
an	O
example	O
(	O
x	O
,	O
y	O
)	O
.	O
for	O
simplicity	O
,	O
suppose	O
this	O
is	O
a	O
posi-	O
tive	O
example	O
,	O
so	O
y	O
=	O
+1	O
.	O
we	O
compute	O
an	O
activation	O
a	O
,	O
and	O
make	O
an	O
error	O
.	O
namely	O
,	O
a	O
<	O
0.	O
we	O
now	O
update	O
our	O
weights	B
and	O
bias	B
.	O
let	O
’	O
s	O
call	O
the	O
new	O
weights	B
w	O
(	O
cid:48	O
)	O
d	O
,	O
b	O
(	O
cid:48	O
)	O
.	O
suppose	O
we	O
observe	O
the	O
same	O
exam-	O
ple	O
again	O
and	O
need	O
to	O
compute	O
a	O
new	O
activation	O
a	O
(	O
cid:48	O
)	O
.	O
we	O
proceed	O
by	O
a	O
little	O
algebra	O
:	O
1	O
,	O
.	O
.	O
.	O
,	O
w	O
(	O
cid:48	O
)	O
a	O
(	O
cid:48	O
)	O
=	O
=	O
=	O
d∑	O
d=1	O
d∑	O
d=1	O
d∑	O
d=1	O
w	O
(	O
cid:48	O
)	O
dxd	O
+	O
b	O
(	O
cid:48	O
)	O
(	O
wd	O
+	O
xd	O
)	O
xd	O
+	O
(	O
b	O
+	O
1	O
)	O
wdxd	O
+	O
b	O
+	O
d∑	O
d=1	O
xdxd	O
+	O
1	O
=	O
a	O
+	O
d∑	O
d=1	O
x2	O
d	O
+	O
1	O
>	O
a	O
(	O
3.3	O
)	O
(	O
3.4	O
)	O
(	O
3.5	O
)	O
(	O
3.6	O
)	O
d	O
+	O
1.	O
but	O
x2	O
so	O
the	O
difference	O
between	O
the	O
old	O
activation	O
a	O
and	O
the	O
new	O
activa-	O
d	O
≥	O
0	O
,	O
since	O
it	O
’	O
s	O
squared	O
.	O
so	O
this	O
value	O
is	O
tion	O
a	O
(	O
cid:48	O
)	O
is	O
∑d	O
x2	O
always	O
at	O
least	O
one	O
.	O
thus	O
,	O
the	O
new	O
activation	O
is	O
always	O
at	O
least	O
the	O
old	O
activation	O
plus	O
one	O
.	O
since	O
this	O
was	O
a	O
positive	O
example	O
,	O
we	O
have	O
suc-	O
cessfully	O
moved	O
the	O
activation	O
in	O
the	O
proper	O
direction	O
.	O
(	O
though	O
note	O
that	O
there	O
’	O
s	O
no	O
guarantee	O
that	O
we	O
will	O
correctly	O
classify	O
this	O
point	O
the	O
second	O
,	O
third	O
or	O
even	O
fourth	O
time	O
around	O
!	O
)	O
the	O
only	O
hyperparameter	B
of	O
the	O
perceptron	O
algorithm	B
is	O
maxiter	O
,	O
the	O
number	O
of	O
passes	O
to	O
make	O
over	O
the	O
training	O
data	O
.	O
if	O
we	O
make	O
many	O
many	O
passes	O
over	O
the	O
training	O
data	O
,	O
then	O
the	O
algorithm	O
is	O
likely	O
to	O
overﬁt	O
.	O
(	O
this	O
would	O
be	O
like	O
studying	O
too	O
long	O
for	O
an	O
exam	O
and	O
just	O
confusing	O
yourself	O
.	O
)	O
on	O
the	O
other	O
hand	O
,	O
going	O
over	O
the	O
data	O
only	O
one	O
time	O
might	O
lead	O
to	O
underﬁtting	O
.	O
this	O
is	O
shown	O
experimentally	O
in	O
figure	O
3.3.	O
the	O
x-axis	O
shows	O
the	O
number	O
of	O
passes	O
over	O
the	O
data	O
and	O
the	O
y-axis	O
shows	O
the	O
training	O
error	O
and	O
the	O
test	O
error	O
.	O
as	O
you	O
can	O
see	O
,	O
there	O
is	O
a	O
“	O
sweet	O
spot	O
”	O
at	O
which	O
test	O
performance	O
begins	O
to	O
degrade	O
due	O
to	O
overﬁtting	O
.	O
one	O
aspect	O
of	O
the	O
perceptron	O
algorithm	B
that	O
is	O
left	O
underspeciﬁed	O
is	O
line	O
4	O
,	O
which	O
says	O
:	O
loop	O
over	O
all	O
the	O
training	O
examples	O
.	O
the	O
natural	O
implementation	O
of	O
this	O
would	O
be	O
to	O
loop	O
over	O
them	O
in	O
a	O
constant	O
order	O
.	O
the	O
is	O
actually	O
a	O
bad	O
idea	O
.	O
?	O
this	O
analysis	O
hold	O
for	O
the	O
case	O
pos-	O
itive	O
examples	B
(	O
y	O
=	O
+1	O
)	O
.	O
it	O
should	O
also	O
hold	O
for	O
negative	O
examples	B
.	O
work	O
it	O
out	O
.	O
figure	O
3.3	O
:	O
training	O
and	O
test	B
error	I
via	O
early	B
stopping	I
consider	O
what	O
the	O
perceptron	O
algorithm	B
would	O
do	O
on	O
a	O
data	O
set	O
that	O
consisted	O
of	O
500	O
positive	O
examples	O
followed	O
by	O
500	O
negative	O
examples	B
.	O
after	O
seeing	O
the	O
ﬁrst	O
few	O
positive	O
examples	O
(	O
maybe	O
ﬁve	O
)	O
,	O
it	O
would	O
likely	O
decide	O
that	O
every	O
example	O
is	O
positive	O
,	O
and	O
would	O
stop	O
learning	O
anything	O
.	O
it	O
would	O
do	O
well	O
for	O
a	O
while	O
(	O
next	O
495	O
examples	B
)	O
,	O
until	O
it	O
hit	O
the	O
batch	O
of	O
negative	O
examples	B
.	O
then	O
it	O
would	O
take	O
a	O
while	O
(	O
maybe	O
ten	O
examples	B
)	O
before	O
it	O
would	O
start	O
predicting	O
everything	O
as	O
negative	O
.	O
by	O
the	O
end	O
of	O
one	O
pass	O
through	O
the	O
data	O
,	O
it	O
would	O
really	O
only	O
have	O
learned	O
from	O
a	O
handful	O
of	O
examples	B
(	O
ﬁfteen	O
in	O
this	O
case	O
)	O
.	O
so	O
one	O
thing	O
you	O
need	O
to	O
avoid	O
is	O
presenting	O
the	O
examples	O
in	O
some	O
ﬁxed	O
order	O
.	O
this	O
can	O
easily	O
be	O
accomplished	O
by	O
permuting	O
the	O
order	O
of	O
examples	B
once	O
in	O
the	O
beginning	O
and	O
then	O
cycling	O
over	O
the	O
data	O
set	O
in	O
the	O
same	O
(	O
permuted	O
)	O
order	O
each	O
iteration	B
.	O
however	O
,	O
it	O
turns	O
out	O
that	O
you	O
can	O
actually	O
do	O
better	O
if	O
you	O
re-permute	O
the	O
examples	O
in	O
each	O
iteration	B
.	O
figure	O
3.4	O
shows	O
the	O
effect	O
of	O
re-permuting	O
on	O
convergence	O
speed	O
.	O
in	O
practice	O
,	O
permuting	O
each	O
iteration	B
tends	O
to	O
yield	O
about	O
20	O
%	O
savings	O
in	O
number	O
of	O
iterations	O
.	O
in	O
theory	O
,	O
you	O
can	O
actually	O
prove	O
that	O
it	O
’	O
s	O
expected	O
to	O
be	O
about	O
twice	O
as	O
fast	O
.	O
3.3	O
geometric	O
intrepretation	O
the	O
perceptron	O
43	O
figure	O
3.4	O
:	O
training	O
and	O
test	B
error	I
for	O
permuting	O
versus	O
not-permuting	O
?	O
if	O
permuting	O
the	O
data	O
each	O
iteration	B
saves	O
somewhere	O
between	O
20	O
%	O
and	O
50	O
%	O
of	O
your	O
time	O
,	O
are	O
there	O
any	O
cases	O
in	O
which	O
you	O
might	O
not	O
want	O
to	O
permute	O
the	O
data	O
every	O
iteration	B
?	O
a	O
question	O
you	O
should	O
be	O
asking	O
yourself	O
by	O
now	O
is	O
:	O
what	O
does	O
the	O
decision	O
boundary	O
of	O
a	O
perceptron	B
look	O
like	O
?	O
you	O
can	O
actually	O
answer	O
that	O
question	O
mathematically	O
.	O
for	O
a	O
perceptron	B
,	O
the	O
decision	O
bound-	O
ary	O
is	O
precisely	O
where	O
the	O
sign	O
of	O
the	O
activation	O
,	O
a	O
,	O
changes	O
from	O
−1	O
to	O
+1	O
.	O
in	O
other	O
words	O
,	O
it	O
is	O
the	O
set	O
of	O
points	O
x	O
that	O
achieve	O
zero	O
ac-	O
tivation	O
.	O
the	O
points	O
that	O
are	O
not	O
clearly	O
positive	O
nor	O
negative	O
.	O
for	O
simplicity	O
,	O
we	O
’	O
ll	O
ﬁrst	O
consider	O
the	O
case	O
where	O
there	O
is	O
no	O
“	O
bias	B
”	O
term	O
(	O
or	O
,	O
equivalently	O
,	O
the	O
bias	O
is	O
zero	O
)	O
.	O
formally	O
,	O
the	O
decision	O
boundary	O
b	O
is	O
:	O
(	O
cid:40	O
)	O
(	O
cid:41	O
)	O
b	O
=	O
x	O
:	O
∑	O
d	O
wdxd	O
=	O
0	O
(	O
3.7	O
)	O
we	O
can	O
now	O
apply	O
some	O
linear	O
algebra	O
.	O
recall	B
that	O
∑d	O
wdxd	O
is	O
just	O
the	O
dot	O
product	O
between	O
the	O
vector	O
w	O
=	O
(	O
cid:104	O
)	O
w1	O
,	O
w2	O
,	O
.	O
.	O
.	O
,	O
wd	O
(	O
cid:105	O
)	O
and	O
the	O
vector	O
x.	O
we	O
will	O
write	O
this	O
as	O
w	O
·	O
x.	O
two	O
vectors	O
have	O
a	O
zero	O
dot	B
product	I
if	O
and	O
only	O
if	O
they	O
are	O
perpendicular	B
.	O
thus	O
,	O
if	O
we	O
think	O
of	O
the	O
weights	O
as	O
a	O
vector	B
w	O
,	O
then	O
the	O
decision	O
boundary	O
is	O
simply	O
the	O
plane	O
perpendicular	B
to	O
w.	O
44	O
a	O
course	O
in	O
machine	O
learning	O
math	O
review	O
|	O
dot	O
products	O
dot	O
products	O
,	O
deﬁnition	O
,	O
perpendicular	B
,	O
normalization	O
and	O
projections	O
...	O
think	O
about	O
basis	O
vectors	O
for	O
projections	O
.	O
quadratic	O
rule	O
on	O
vectors	O
.	O
also	O
that	O
dot	O
products	O
onto	O
unit	O
vectors	O
are	O
maximized	O
when	O
they	O
point	O
in	O
the	O
same	O
direction	O
so	O
a*a	O
>	O
=	O
a*b	O
blah	O
blah	O
blah	O
.	O
figure	O
3.5	O
:	O
this	O
is	O
shown	O
pictorially	O
in	O
figure	O
3.6.	O
here	O
,	O
the	O
weight	O
vector	B
is	O
shown	O
,	O
together	O
with	O
it	O
’	O
s	O
perpendicular	B
plane	O
.	O
this	O
plane	O
forms	O
the	O
decision	O
boundary	O
between	O
positive	O
points	O
and	O
negative	O
points	O
.	O
the	O
vector	O
points	O
in	O
the	O
direction	O
of	O
the	O
positive	O
examples	B
and	O
away	O
from	O
the	O
negative	O
examples	B
.	O
one	O
thing	O
to	O
notice	O
is	O
that	O
the	O
scale	O
of	O
the	O
weight	O
vector	B
is	O
irrele-	O
vant	O
from	O
the	O
perspective	O
of	O
classiﬁcation	O
.	O
suppose	O
you	O
take	O
a	O
weight	O
vector	B
w	O
and	O
replace	O
it	O
with	O
2w	O
.	O
all	O
activations	O
are	O
now	O
doubled	O
.	O
but	O
their	O
sign	B
does	O
not	O
change	O
.	O
this	O
makes	O
complete	O
sense	O
geometri-	O
cally	O
,	O
since	O
all	O
that	O
matters	O
is	O
which	O
side	O
of	O
the	O
plane	O
a	O
test	O
point	O
falls	O
on	O
,	O
now	O
how	O
far	O
it	O
is	O
from	O
that	O
plane	O
.	O
for	O
this	O
reason	O
,	O
it	O
is	O
common	O
to	O
work	O
with	O
normalized	O
weight	O
vectors	O
,	O
w	O
,	O
that	O
have	O
length	O
one	O
;	O
i.e.	O
,	O
||w||	O
=	O
1.	O
the	O
geometric	O
intuition	O
can	O
help	O
us	O
even	O
more	O
when	O
we	O
realize	O
that	O
dot	O
products	O
compute	O
projections	O
.	O
that	O
is	O
,	O
the	O
value	O
w	O
·	O
x	O
is	O
just	O
the	O
distance	O
of	O
x	O
from	O
the	O
origin	O
when	O
projected	O
onto	O
the	O
vector	O
w.	O
this	O
is	O
shown	O
in	O
figure	O
3.7.	O
in	O
that	O
ﬁgure	O
,	O
all	O
the	O
data	O
points	O
are	O
projected	O
onto	O
w.	O
below	O
,	O
we	O
can	O
think	O
of	O
this	O
as	O
a	O
one-dimensional	O
version	O
of	O
the	O
data	O
,	O
where	O
each	O
data	O
point	O
is	O
placed	O
according	O
to	O
its	O
projection	O
along	O
w.	O
this	O
distance	B
along	O
w	O
is	O
exactly	O
the	O
activiation	O
of	O
that	O
example	O
,	O
with	O
no	O
bias	B
.	O
from	O
here	O
,	O
you	O
can	O
start	O
thinking	O
about	O
the	O
role	O
of	O
the	O
bias	O
term	O
.	O
previously	O
,	O
the	O
threshold	O
would	O
be	O
at	O
zero	O
.	O
any	O
example	O
with	O
a	O
negative	O
projection	O
onto	O
w	O
would	O
be	O
classiﬁed	O
negative	O
;	O
any	O
exam-	O
ple	O
with	O
a	O
positive	O
projection	O
,	O
positive	O
.	O
the	O
bias	O
simply	O
moves	O
this	O
threshold	B
.	O
now	O
,	O
after	O
the	O
projection	O
is	O
computed	O
,	O
b	O
is	O
added	O
to	O
get	O
the	O
overall	O
activation	O
.	O
the	O
projection	O
plus	O
b	O
is	O
then	O
compared	O
against	O
zero	O
.	O
thus	O
,	O
from	O
a	O
geometric	O
perspective	O
,	O
the	O
role	O
of	O
the	O
bias	O
is	O
to	O
shift	O
the	O
decision	O
boundary	O
away	O
from	O
the	O
origin	O
,	O
in	O
the	O
direction	O
of	O
w.	O
it	O
is	O
shifted	O
exactly	O
−b	O
units	O
.	O
so	O
if	O
b	O
is	O
positive	O
,	O
the	O
boundary	O
is	O
shifted	O
away	O
from	O
w	O
and	O
if	O
b	O
is	O
negative	O
,	O
the	O
boundary	O
is	O
shifted	O
toward	O
w.	O
this	O
is	O
shown	O
in	O
figure	O
?	O
?	O
.	O
this	O
makes	O
intuitive	O
sense	O
:	O
a	O
positive	O
bias	O
means	O
that	O
more	O
examples	B
should	O
be	O
classiﬁed	O
positive	O
.	O
by	O
moving	O
the	O
decision	O
boundary	O
in	O
the	O
negative	O
direction	O
,	O
more	O
space	O
yields	O
a	O
figure	O
3.6	O
:	O
picture	O
of	O
data	O
points	O
with	O
hyperplane	B
and	O
weight	O
vector	B
?	O
if	O
i	O
give	O
you	O
an	O
arbitrary	O
non-zero	O
weight	O
vector	B
w	O
,	O
how	O
do	O
i	O
compute	O
a	O
weight	O
vector	B
w	O
(	O
cid:48	O
)	O
that	O
points	O
in	O
the	O
same	O
direction	O
but	O
has	O
a	O
norm	O
of	O
one	O
?	O
figure	O
3.7	O
:	O
same	O
picture	O
as	O
before	O
,	O
but	O
with	O
projections	O
onto	O
weight	O
vector	B
;	O
todo	O
:	O
then	O
,	O
below	O
,	O
those	O
points	O
along	O
a	O
one-dimensional	O
axis	O
with	O
zero	O
marked	O
.	O
the	O
decision	O
boundary	O
for	O
a	O
perceptron	B
is	O
a	O
very	O
magical	O
thing	O
.	O
in	O
positive	O
classiﬁcation	O
.	O
d	O
dimensional	O
space	O
,	O
it	O
is	O
always	O
a	O
d	O
−	O
1-dimensional	O
hyperplane	B
.	O
(	O
in	O
two	O
dimensions	O
,	O
a	O
1-d	O
hyperplane	B
is	O
simply	O
a	O
line	O
.	O
in	O
three	O
di-	O
mensions	O
,	O
a	O
2-d	O
hyperplane	B
is	O
like	O
a	O
sheet	O
of	O
paper	O
.	O
)	O
this	O
hyperplane	B
divides	O
space	O
in	O
half	O
.	O
in	O
the	O
rest	O
of	O
this	O
book	O
,	O
we	O
’	O
ll	O
refer	O
to	O
the	O
weight	O
vector	B
,	O
and	O
to	O
hyperplane	B
it	O
deﬁnes	O
,	O
interchangeably	O
.	O
the	O
perceptron	O
update	O
can	O
also	O
be	O
considered	O
geometrically	O
.	O
(	O
for	O
simplicity	O
,	O
we	O
will	O
consider	O
the	O
unbiased	O
case	O
.	O
)	O
consider	O
the	O
situ-	O
ation	O
in	O
figure	O
?	O
?	O
.	O
here	O
,	O
we	O
have	O
a	O
current	O
guess	O
as	O
to	O
the	O
hyper-	O
plane	O
,	O
and	O
positive	O
training	O
example	O
comes	O
in	O
that	O
is	O
currently	O
mis-	O
classiﬁed	O
.	O
the	O
weights	O
are	O
updated	O
:	O
w	O
←	O
w	O
+	O
yx	O
.	O
this	O
yields	O
the	O
new	O
weight	O
vector	B
,	O
also	O
shown	O
in	O
the	O
figure	O
.	O
in	O
this	O
case	O
,	O
the	O
weight	O
vector	B
changed	O
enough	O
that	O
this	O
training	O
example	O
is	O
now	O
correctly	O
classiﬁed	O
.	O
3.4	O
interpreting	O
perceptron	B
weights	O
todo	O
3.5	O
perceptron	B
convergence	O
and	O
linear	O
separability	O
you	O
already	O
have	O
an	O
intuitive	O
feeling	O
for	O
why	O
the	O
perceptron	O
works	O
:	O
it	O
moves	O
the	O
decision	O
boundary	O
in	O
the	O
direction	O
of	O
the	O
training	O
exam-	O
ples	O
.	O
a	O
question	O
you	O
should	O
be	O
asking	O
yourself	O
is	O
:	O
does	O
the	O
percep-	O
tron	O
converge	O
?	O
if	O
so	O
,	O
what	O
does	O
it	O
converge	O
to	O
?	O
and	O
how	O
long	O
does	O
it	O
take	O
?	O
it	O
is	O
easy	O
to	O
construct	O
data	O
sets	O
on	O
which	O
the	O
perceptron	O
algorithm	B
will	O
never	O
converge	O
.	O
in	O
fact	O
,	O
consider	O
the	O
(	O
very	O
uninteresting	O
)	O
learn-	O
ing	O
problem	O
with	O
no	O
features	B
.	O
you	O
have	O
a	O
data	O
set	O
consisting	O
of	O
one	O
positive	O
example	O
and	O
one	O
negative	O
example	O
.	O
since	O
there	O
are	O
no	O
fea-	O
tures	O
,	O
the	O
only	O
thing	O
the	O
perceptron	O
algorithm	B
will	O
ever	O
do	O
is	O
adjust	O
the	O
bias	O
.	O
given	O
this	O
data	O
,	O
you	O
can	O
run	O
the	O
perceptron	O
for	O
a	O
bajillion	O
iterations	O
and	O
it	O
will	O
never	O
settle	O
down	O
.	O
as	O
long	O
as	O
the	O
bias	O
is	O
non-	O
negative	O
,	O
the	O
negative	O
example	O
will	O
cause	O
it	O
to	O
decrease	O
.	O
as	O
long	O
as	O
it	O
is	O
non-positive	O
,	O
the	O
positive	O
example	O
will	O
cause	O
it	O
to	O
increase	O
.	O
ad	O
inﬁnitum	O
.	O
(	O
yes	O
,	O
this	O
is	O
a	O
very	O
contrived	O
example	O
.	O
)	O
what	O
does	O
it	O
mean	O
for	O
the	O
perceptron	O
to	O
converge	O
?	O
it	O
means	O
that	O
it	O
can	O
make	O
an	O
entire	O
pass	O
through	O
the	O
training	O
data	O
without	O
making	O
any	O
more	O
updates	O
.	O
in	O
other	O
words	O
,	O
it	O
has	O
correctly	O
classiﬁed	O
every	O
training	O
example	O
.	O
geometrically	O
,	O
this	O
means	O
that	O
it	O
was	O
found	O
some	O
hyperplane	B
that	O
correctly	O
segregates	O
the	O
data	O
into	O
positive	O
and	O
nega-	O
tive	O
examples	B
,	O
like	O
that	O
shown	O
in	O
figure	O
3.9.	O
in	O
this	O
case	O
,	O
this	O
data	O
is	O
linearly	B
separable	I
.	O
this	O
means	O
that	O
there	O
the	O
perceptron	O
45	O
figure	O
3.8	O
:	O
perceptron	B
picture	O
with	O
update	O
,	O
no	O
bias	B
figure	O
3.9	O
:	O
separable	O
data	O
46	O
a	O
course	O
in	O
machine	O
learning	O
exists	O
some	O
hyperplane	B
that	O
puts	O
all	O
the	O
positive	O
examples	O
on	O
one	O
side	O
and	O
all	O
the	O
negative	O
examples	B
on	O
the	O
other	O
side	O
.	O
if	O
the	O
training	O
is	O
not	O
linearly	B
separable	I
,	O
like	O
that	O
shown	O
in	O
figure	O
3.10	O
,	O
then	O
the	O
perceptron	O
has	O
no	O
hope	O
of	O
converging	O
.	O
it	O
could	O
never	O
possibly	O
classify	O
each	O
point	O
correctly	O
.	O
the	O
somewhat	O
surprising	O
thing	O
about	O
the	O
perceptron	O
algorithm	B
is	O
that	O
if	O
the	O
data	O
is	O
linearly	B
separable	I
,	O
then	O
it	O
will	O
converge	O
to	O
a	O
weight	O
vector	B
that	O
separates	O
the	O
data	O
.	O
(	O
and	O
if	O
the	O
data	O
is	O
inseparable	O
,	O
then	O
it	O
will	O
never	O
converge	O
.	O
)	O
this	O
is	O
great	O
news	O
.	O
it	O
means	O
that	O
the	O
perceptron	O
converges	O
whenever	O
it	O
is	O
even	O
remotely	O
possible	O
to	O
converge	O
.	O
the	O
second	O
question	O
is	O
:	O
how	O
long	O
does	O
it	O
take	O
to	O
converge	O
?	O
by	O
“	O
how	O
long	O
,	O
”	O
what	O
we	O
really	O
mean	O
is	O
“	O
how	O
many	O
updates	O
?	O
”	O
as	O
is	O
the	O
case	O
for	O
much	O
learning	O
theory	O
,	O
you	O
will	O
not	O
be	O
able	O
to	O
get	O
an	O
answer	O
of	O
the	O
form	O
“	O
it	O
will	O
converge	O
after	O
5293	O
updates.	O
”	O
this	O
is	O
asking	O
too	O
much	O
.	O
the	O
sort	O
of	O
answer	O
we	O
can	O
hope	O
to	O
get	O
is	O
of	O
the	O
form	O
“	O
it	O
will	O
converge	O
after	O
at	O
most	O
5293	O
updates.	O
”	O
what	O
you	O
might	O
expect	O
to	O
see	O
is	O
that	O
the	O
perceptron	O
will	O
con-	O
verge	O
more	O
quickly	O
for	O
easy	O
learning	O
problems	O
than	O
for	O
hard	O
learning	O
problems	O
.	O
this	O
certainly	O
ﬁts	O
intuition	O
.	O
the	O
question	O
is	O
how	O
to	O
deﬁne	O
“	O
easy	O
”	O
and	O
“	O
hard	O
”	O
in	O
a	O
meaningful	O
way	O
.	O
one	O
way	O
to	O
make	O
this	O
def-	O
inition	O
is	O
through	O
the	O
notion	O
of	O
margin	B
.	O
if	O
i	O
give	O
you	O
a	O
data	O
set	O
and	O
hyperplane	B
that	O
separates	O
it	O
(	O
like	O
that	O
shown	O
in	O
figure	O
?	O
?	O
)	O
then	O
the	O
margin	O
is	O
the	O
distance	O
between	O
the	O
hyperplane	O
and	O
the	O
nearest	O
point	O
.	O
intuitively	O
,	O
problems	O
with	O
large	O
margins	O
should	O
be	O
easy	O
(	O
there	O
’	O
s	O
lots	O
of	O
“	O
wiggle	O
room	O
”	O
to	O
ﬁnd	O
a	O
separating	B
hyperplane	I
)	O
;	O
and	O
problems	O
with	O
small	O
margins	O
should	O
be	O
hard	O
(	O
you	O
really	O
have	O
to	O
get	O
a	O
very	O
speciﬁc	O
well	O
tuned	O
weight	O
vector	B
)	O
.	O
formally	O
,	O
given	O
a	O
data	O
set	O
d	O
,	O
a	O
weight	O
vector	B
w	O
and	O
bias	B
b	O
,	O
the	O
margin	O
of	O
w	O
,	O
b	O
on	O
d	O
is	O
deﬁned	O
as	O
:	O
(	O
cid:40	O
)	O
min	O
(	O
x	O
,	O
y	O
)	O
∈d	O
y	O
(	O
cid:0	O
)	O
w	O
·	O
x	O
+	O
b	O
(	O
cid:1	O
)	O
margin	B
(	O
d	O
,	O
w	O
,	O
b	O
)	O
=	O
−∞	O
if	O
w	O
separates	O
d	O
otherwise	O
(	O
3.8	O
)	O
in	O
words	O
,	O
the	O
margin	O
is	O
only	O
deﬁned	O
if	O
w	O
,	O
b	O
actually	O
separate	O
the	O
data	O
(	O
otherwise	O
it	O
is	O
just	O
−∞	O
)	O
.	O
in	O
the	O
case	O
that	O
it	O
separates	O
the	O
data	O
,	O
we	O
ﬁnd	O
the	O
point	O
with	O
the	O
minimum	O
activation	O
,	O
after	O
the	O
activation	O
is	O
multiplied	O
by	O
the	O
label	O
.	O
for	O
some	O
historical	O
reason	O
(	O
that	O
is	O
unknown	O
to	O
the	O
author	O
)	O
,	O
mar-	O
gins	O
are	O
always	O
denoted	O
by	O
the	O
greek	O
letter	O
γ	O
(	O
gamma	O
)	O
.	O
one	O
often	O
talks	O
about	O
the	O
margin	O
of	O
a	O
data	O
set	O
.	O
the	O
margin	O
of	O
a	O
data	O
set	O
is	O
the	O
largest	O
attainable	O
margin	B
on	O
this	O
data	O
.	O
formally	O
:	O
margin	B
(	O
d	O
)	O
=	O
sup	O
w	O
,	O
b	O
margin	B
(	O
d	O
,	O
w	O
,	O
b	O
)	O
(	O
3.9	O
)	O
in	O
words	O
,	O
to	O
compute	O
the	O
margin	O
of	O
a	O
data	O
set	O
,	O
you	O
“	O
try	O
”	O
every	O
possi-	O
ble	O
w	O
,	O
b	O
pair	O
.	O
for	O
each	O
pair	O
,	O
you	O
compute	O
its	O
margin	B
.	O
we	O
then	O
take	O
the	O
so	O
long	O
as	O
the	O
margin	O
is	O
not	O
−∞	O
,	O
it	O
is	O
always	O
positive	O
.	O
geometrically	O
this	O
makes	O
sense	O
,	O
but	O
what	O
does	O
eq	O
(	O
3.8	O
)	O
yeild	O
this	O
?	O
?	O
the	O
perceptron	O
47	O
1	O
you	O
can	O
read	O
“	O
sup	O
”	O
as	O
“	O
max	O
”	O
if	O
you	O
like	O
:	O
the	O
only	O
difference	O
is	O
a	O
technical	O
difference	O
in	O
how	O
the	O
−∞	O
case	O
is	O
handled	O
.	O
2	O
rosenblatt	O
1958	O
largest	O
of	O
these	O
as	O
the	O
overall	O
margin	O
of	O
the	O
data.1	O
if	O
the	O
data	O
is	O
not	O
linearly	B
separable	I
,	O
then	O
the	O
value	O
of	O
the	O
sup	O
,	O
and	O
therefore	O
the	O
value	O
of	O
the	O
margin	O
,	O
is	O
−∞	O
.	O
there	O
is	O
a	O
famous	O
theorem	O
due	O
to	O
rosenblatt2	O
that	O
shows	O
that	O
the	O
number	O
of	O
errors	O
that	O
the	O
perceptron	O
algorithm	B
makes	O
is	O
bounded	O
by	O
γ−2	O
.	O
more	O
formally	O
:	O
theorem	O
1	O
(	O
perceptron	B
convergence	O
theorem	O
)	O
.	O
suppose	O
the	O
perceptron	O
algorithm	B
is	O
run	O
on	O
a	O
linearly	B
separable	I
data	O
set	O
d	O
with	O
margin	B
γ	O
>	O
0.	O
assume	O
that	O
||x||	O
≤	O
1	O
for	O
all	O
x	O
∈	O
d.	O
then	O
the	O
algorithm	O
will	O
converge	O
after	O
at	O
most	O
1	O
γ2	O
updates	O
.	O
todo	O
:	O
comment	O
on	O
norm	O
of	O
w	O
and	O
norm	O
of	O
x	O
also	O
some	O
picture	O
about	O
maximum	O
margins	O
.	O
the	O
proof	O
of	O
this	O
theorem	O
is	O
elementary	O
,	O
in	O
the	O
sense	O
that	O
it	O
does	O
not	O
use	O
any	O
fancy	O
tricks	O
:	O
it	O
’	O
s	O
all	O
just	O
algebra	O
.	O
the	O
idea	O
behind	O
the	O
proof	O
is	O
as	O
follows	O
.	O
if	O
the	O
data	O
is	O
linearly	B
separable	I
with	O
margin	B
γ	O
,	O
then	O
there	O
exists	O
some	O
weight	O
vector	B
w∗	O
that	O
achieves	O
this	O
margin	B
.	O
obviously	O
we	O
don	O
’	O
t	O
know	O
what	O
w∗	O
is	O
,	O
but	O
we	O
know	O
it	O
exists	O
.	O
the	O
perceptron	O
algorithm	B
is	O
trying	O
to	O
ﬁnd	O
a	O
weight	O
vector	B
w	O
that	O
points	O
roughly	O
in	O
the	O
same	O
direction	O
as	O
w∗	O
.	O
(	O
for	O
large	O
γ	O
,	O
“	O
roughly	O
”	O
can	O
be	O
very	O
rough	O
.	O
for	O
small	O
γ	O
,	O
“	O
roughly	O
”	O
is	O
quite	O
precise	O
.	O
)	O
every	O
time	O
the	O
perceptron	O
makes	O
an	O
update	O
,	O
the	O
angle	O
between	O
w	O
and	O
w∗	O
changes	O
.	O
what	O
we	O
prove	O
is	O
that	O
the	O
angle	O
actually	O
decreases	O
.	O
we	O
show	O
this	O
in	O
two	O
steps	O
.	O
first	O
,	O
the	O
dot	O
product	O
w	O
·	O
w∗	O
increases	O
a	O
lot	O
.	O
second	O
,	O
the	O
norm	O
||w||	O
does	O
not	O
increase	O
very	O
much	O
.	O
since	O
the	O
dot	O
product	O
is	O
increasing	O
,	O
but	O
w	O
isn	O
’	O
t	O
getting	O
too	O
long	O
,	O
the	O
angle	O
between	O
them	O
has	O
to	O
be	O
shrinking	O
.	O
the	O
rest	O
is	O
algebra	O
.	O
proof	O
of	O
theorem	O
1.	O
the	O
margin	O
γ	O
>	O
0	O
must	O
be	O
realized	O
by	O
some	O
set	O
of	O
parameters	O
,	O
say	O
x∗	O
.	O
suppose	O
we	O
train	O
a	O
perceptron	B
on	O
this	O
data	O
.	O
denote	O
by	O
w	O
(	O
0	O
)	O
the	O
initial	O
weight	O
vector	B
,	O
w	O
(	O
1	O
)	O
the	O
weight	O
vector	B
after	O
the	O
ﬁrst	O
update	O
,	O
and	O
w	O
(	O
k	O
)	O
the	O
weight	O
vector	B
after	O
the	O
kth	O
update	O
.	O
(	O
we	O
are	O
essentially	O
ignoring	O
data	O
points	O
on	O
which	O
the	O
perceptron	O
doesn	O
’	O
t	O
update	O
itself	O
.	O
)	O
first	O
,	O
we	O
will	O
show	O
that	O
w∗	O
·	O
w	O
(	O
k	O
)	O
grows	O
quicky	O
as	O
a	O
function	O
of	O
k.	O
second	O
,	O
we	O
will	O
show	O
that	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
w	O
(	O
k	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
does	O
not	O
grow	O
quickly	O
.	O
first	O
,	O
suppose	O
that	O
the	O
kth	O
update	O
happens	O
on	O
example	O
(	O
x	O
,	O
y	O
)	O
.	O
we	O
are	O
trying	O
to	O
show	O
that	O
w	O
(	O
k	O
)	O
is	O
becoming	O
aligned	O
with	O
w∗	O
.	O
because	O
we	O
updated	O
,	O
know	O
that	O
this	O
example	O
was	O
misclassiﬁed	O
:	O
yw	O
(	O
k-1	O
)	O
·	O
x	O
<	O
0.	O
after	O
the	O
update	O
,	O
we	O
get	O
w	O
(	O
k	O
)	O
=	O
w	O
(	O
k-1	O
)	O
+	O
yx	O
.	O
we	O
do	O
a	O
little	O
computa-	O
tion	O
:	O
w∗	O
·	O
w	O
(	O
k	O
)	O
=	O
w∗	O
·	O
w	O
(	O
k-1	O
)	O
+	O
yx	O
=	O
w∗	O
·	O
w	O
(	O
k-1	O
)	O
+	O
yw∗	O
·	O
x	O
≥	O
w∗	O
·	O
w	O
(	O
k-1	O
)	O
+	O
γ	O
deﬁnition	O
of	O
w	O
(	O
k	O
)	O
vector	B
algebra	O
w∗	O
has	O
margin	B
γ	O
(	O
3.10	O
)	O
(	O
3.11	O
)	O
(	O
3.12	O
)	O
48	O
a	O
course	O
in	O
machine	O
learning	O
thus	O
,	O
every	O
time	O
w	O
(	O
k	O
)	O
is	O
updated	O
,	O
its	O
projection	O
onto	O
w∗	O
incrases	O
by	O
at	O
least	O
γ.	O
therefore	O
:	O
w∗	O
·	O
w	O
(	O
k	O
)	O
≥	O
kγ	O
.	O
because	O
w	O
(	O
k	O
)	O
is	O
getting	O
closer	O
to	O
w∗	O
,	O
not	O
just	O
because	O
it	O
’	O
s	O
getting	O
ex-	O
ceptionally	O
long	O
.	O
to	O
do	O
this	O
,	O
we	O
compute	O
the	O
norm	O
of	O
w	O
(	O
k	O
)	O
:	O
next	O
,	O
we	O
need	O
to	O
show	O
that	O
the	O
increase	O
of	O
γ	O
along	O
w∗	O
occurs	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
w	O
(	O
k	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
2	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
2	O
=	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
w	O
(	O
k-1	O
)	O
+	O
yx	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
w	O
(	O
k-1	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
2	O
≤	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
w	O
(	O
k-1	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
2	O
=	O
+	O
1	O
+	O
0	O
+	O
y2	O
||x||2	O
+	O
2yw	O
(	O
k-1	O
)	O
·	O
x	O
deﬁnition	O
of	O
w	O
(	O
k	O
)	O
(	O
3.13	O
)	O
quadratic	O
rule	O
on	O
vectors	O
(	O
3.14	O
)	O
assumption	O
on	O
||x||	O
and	O
a	O
<	O
0	O
(	O
3.15	O
)	O
thus	O
,	O
the	O
squared	O
norm	O
of	O
w	O
(	O
k	O
)	O
increases	O
by	O
at	O
most	O
one	O
every	O
up-	O
date	O
.	O
therefore	O
:	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
w	O
(	O
k	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
2	O
≤	O
k.	O
that	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
w	O
(	O
k	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
≥	O
w∗	O
·	O
w	O
(	O
k	O
)	O
.	O
putting	O
this	O
together	O
,	O
we	O
have	O
:	O
k	O
≥	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
w	O
(	O
k	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
≥	O
w∗	O
·	O
w	O
(	O
k	O
)	O
≥	O
kγ	O
now	O
we	O
put	O
together	O
the	O
two	O
things	O
we	O
have	O
learned	O
before	O
.	O
by	O
our	O
ﬁrst	O
conclusion	O
,	O
we	O
know	O
w∗	O
·	O
w	O
(	O
k	O
)	O
≥	O
kγ	O
.	O
but	O
our	O
second	O
con-	O
clusion	O
,	O
k	O
≥	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
w	O
(	O
k	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
2.	O
finally	O
,	O
because	O
w∗	O
is	O
a	O
unit	O
vector	O
,	O
we	O
know	O
√	O
√	O
(	O
3.16	O
)	O
taking	O
the	O
left-most	O
and	O
right-most	O
terms	O
,	O
we	O
get	O
that	O
dividing	O
both	O
sides	O
by	O
k	O
,	O
we	O
get	O
1√	O
k	O
this	O
means	O
that	O
once	O
we	O
’	O
ve	O
made	O
1	O
γ2	O
updates	O
,	O
we	O
can	O
not	O
make	O
any	O
more	O
!	O
k	O
≥	O
kγ	O
.	O
≥	O
γ	O
and	O
therefore	O
k	O
≤	O
1√	O
γ	O
.	O
√	O
it	O
is	O
important	O
to	O
keep	O
in	O
mind	O
what	O
this	O
proof	O
shows	O
and	O
what	O
it	O
does	O
not	O
show	O
.	O
it	O
shows	O
that	O
if	O
i	O
give	O
the	O
perceptron	O
data	O
that	O
is	O
linearly	B
separable	I
with	O
margin	B
γ	O
>	O
0	O
,	O
then	O
the	O
perceptron	O
will	O
converge	O
to	O
a	O
solution	O
that	O
separates	O
the	O
data	O
.	O
and	O
it	O
will	O
converge	O
quickly	O
when	O
γ	O
is	O
large	O
.	O
it	O
does	O
not	O
say	O
anything	O
about	O
the	O
solution	O
,	O
other	O
than	O
the	O
fact	O
that	O
it	O
separates	O
the	O
data	O
.	O
in	O
particular	O
,	O
the	O
proof	O
makes	O
use	O
of	O
the	O
maximum	O
margin	B
separator	O
.	O
but	O
the	O
perceptron	O
is	O
not	O
guaranteed	O
to	O
ﬁnd	O
this	O
maximum	O
margin	O
separator	O
.	O
the	O
data	O
may	O
be	O
separable	O
with	O
margin	B
0.9	O
and	O
the	O
perceptron	O
might	O
still	O
ﬁnd	O
a	O
separating	B
hyperplane	I
with	O
a	O
margin	O
of	O
only	O
0.000001.	O
later	O
(	O
in	O
chapter	O
?	O
?	O
)	O
,	O
we	O
will	O
see	O
algorithms	O
that	O
explicitly	O
try	O
to	O
ﬁnd	O
the	O
maximum	O
margin	B
solution	O
.	O
3.6	O
improved	O
generalization	O
:	O
voting	B
and	O
averaging	O
in	O
the	O
beginning	O
of	O
this	O
chapter	O
,	O
there	O
was	O
a	O
comment	O
that	O
the	O
per-	O
ceptron	O
works	O
amazingly	O
well	O
.	O
this	O
was	O
a	O
half-truth	O
.	O
the	O
“	O
vanilla	O
”	O
?	O
perhaps	O
we	O
don	O
’	O
t	O
want	O
to	O
assume	O
that	O
all	O
x	O
have	O
norm	O
at	O
most	O
1.	O
if	O
they	O
have	O
all	O
have	O
norm	O
at	O
most	O
r	O
,	O
you	O
can	O
achieve	O
a	O
very	O
simi-	O
lar	O
bound	O
.	O
modify	O
the	O
perceptron	O
convergence	O
proof	O
to	O
handle	O
this	O
case	O
.	O
?	O
why	O
does	O
the	O
perceptron	O
conver-	O
gence	O
bound	O
not	O
contradict	O
the	O
earlier	O
claim	O
that	O
poorly	O
ordered	O
data	O
points	O
(	O
e.g.	O
,	O
all	O
positives	O
fol-	O
lowed	O
by	O
all	O
negatives	O
)	O
will	O
cause	O
the	O
perceptron	O
to	O
take	O
an	O
astronom-	O
ically	O
long	O
time	O
to	O
learn	O
?	O
the	O
perceptron	O
49	O
perceptron	B
algorithm	O
does	O
well	O
,	O
but	O
not	O
amazingly	O
well	O
.	O
in	O
order	O
to	O
make	O
it	O
more	O
competitive	O
with	O
other	O
learning	O
algorithms	O
,	O
you	O
need	O
to	O
modify	O
it	O
a	O
bit	O
to	O
get	O
better	O
generalization	O
.	O
the	O
key	O
issue	O
with	O
the	O
vanilla	O
perceptron	B
is	O
that	O
it	O
counts	O
later	O
points	O
more	O
than	O
it	O
counts	O
earlier	O
points	O
.	O
to	O
see	O
why	O
,	O
consider	O
a	O
data	O
set	O
with	O
10	O
,	O
000	O
examples	B
.	O
suppose	O
that	O
after	O
the	O
ﬁrst	O
100	O
examples	B
,	O
the	O
perceptron	O
has	O
learned	O
a	O
really	O
good	O
classiﬁer	O
.	O
it	O
’	O
s	O
so	O
good	O
that	O
it	O
goes	O
over	O
the	O
next	O
9899	O
exam-	O
ples	O
without	O
making	O
any	O
updates	O
.	O
it	O
reaches	O
the	O
10	O
,	O
000th	O
example	O
and	O
makes	O
an	O
error	O
.	O
it	O
updates	O
.	O
for	O
all	O
we	O
know	O
,	O
the	O
update	O
on	O
this	O
10	O
,	O
000th	O
example	O
completely	O
ruines	O
the	O
weight	O
vector	B
that	O
has	O
done	O
so	O
well	O
on	O
99.99	O
%	O
of	O
the	O
data	O
!	O
what	O
we	O
would	O
like	O
is	O
for	O
weight	O
vectors	O
that	O
“	O
survive	O
”	O
a	O
long	O
time	O
to	O
get	O
more	O
say	O
than	O
weight	O
vectors	O
that	O
are	O
overthrown	O
quickly	O
.	O
one	O
way	O
to	O
achieve	O
this	O
is	O
by	O
voting	B
.	O
as	O
the	O
perceptron	O
learns	O
,	O
it	O
remembers	O
how	O
long	O
each	O
hyperplane	B
survives	O
.	O
at	O
test	O
time	O
,	O
each	O
hyperplane	B
encountered	O
during	O
training	O
“	O
votes	O
”	O
on	O
the	O
class	O
of	O
a	O
test	O
example	O
.	O
if	O
a	O
particular	O
hyperplane	B
survived	O
for	O
20	O
examples	B
,	O
then	O
it	O
gets	O
a	O
vote	B
of	O
20.	O
if	O
it	O
only	O
survived	O
for	O
one	O
example	O
,	O
it	O
only	O
gets	O
a	O
vote	B
of	O
1.	O
in	O
particular	O
,	O
let	O
(	O
w	O
,	O
b	O
)	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
w	O
,	O
b	O
)	O
(	O
k	O
)	O
be	O
the	O
k	O
+	O
1	O
weight	O
vectors	O
encountered	O
during	O
training	O
,	O
and	O
c	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
c	O
(	O
k	O
)	O
be	O
the	O
survival	O
times	O
for	O
each	O
of	O
these	O
weight	O
vectors	O
.	O
(	O
a	O
weight	O
vector	B
that	O
gets	O
immediately	O
updated	O
gets	O
c	O
=	O
1	O
;	O
one	O
that	O
survives	O
another	O
round	O
gets	O
c	O
=	O
2	O
and	O
so	O
on	O
.	O
)	O
then	O
the	O
prediction	O
on	O
a	O
test	O
point	O
is	O
:	O
(	O
cid:32	O
)	O
k∑	O
(	O
cid:16	O
)	O
w	O
(	O
k	O
)	O
·	O
ˆx	O
+	O
b	O
(	O
k	O
)	O
(	O
cid:17	O
)	O
(	O
cid:33	O
)	O
ˆy	O
=	O
sign	B
c	O
(	O
k	O
)	O
sign	B
(	O
3.17	O
)	O
k=1	O
this	O
algorithm	B
,	O
known	O
as	O
the	O
voted	O
perceptron	B
works	O
quite	O
well	O
in	O
practice	O
,	O
and	O
there	O
is	O
some	O
nice	O
theory	O
showing	O
that	O
it	O
is	O
guaranteed	O
to	O
generalize	B
better	O
than	O
the	O
vanilla	O
perceptron	B
.	O
unfortunately	O
,	O
it	O
is	O
also	O
completely	O
impractical	O
.	O
if	O
there	O
are	O
1000	O
updates	O
made	O
during	O
perceptron	B
learning	O
,	O
the	O
voted	O
perceptron	B
requires	O
that	O
you	O
store	O
1000	O
weight	O
vectors	O
,	O
together	O
with	O
their	O
counts	O
.	O
this	O
requires	O
an	O
absurd	O
amount	O
of	O
storage	O
,	O
and	O
makes	O
prediction	O
1000	O
times	O
slower	O
than	O
the	O
vanilla	O
perceptron	B
.	O
a	O
much	O
more	O
practical	O
alternative	O
is	O
the	O
averaged	O
perceptron	B
.	O
the	O
idea	O
is	O
similar	O
:	O
you	O
maintain	O
a	O
collection	O
of	O
weight	O
vectors	O
and	O
survival	O
times	O
.	O
however	O
,	O
at	O
test	O
time	O
,	O
you	O
predict	B
according	O
to	O
the	O
average	O
weight	O
vector	B
,	O
rather	O
than	O
the	O
voting	O
.	O
in	O
particular	O
,	O
the	O
predic-	O
tion	O
is	O
:	O
(	O
cid:32	O
)	O
k∑	O
c	O
(	O
k	O
)	O
(	O
cid:16	O
)	O
w	O
(	O
k	O
)	O
·	O
ˆx	O
+	O
b	O
(	O
k	O
)	O
(	O
cid:17	O
)	O
(	O
cid:33	O
)	O
ˆy	O
=	O
sign	B
(	O
3.18	O
)	O
k=1	O
the	O
only	O
difference	O
between	O
the	O
voted	O
prediction	O
,	O
eq	O
(	O
?	O
?	O
)	O
,	O
and	O
the	O
?	O
the	O
training	O
algorithm	B
for	O
the	O
voted	O
perceptron	B
is	O
the	O
same	O
as	O
the	O
vanilla	O
perceptron	B
.	O
in	O
particular	O
,	O
in	O
line	O
5	O
of	O
algorithm	B
3.2	O
,	O
the	O
ac-	O
tivation	O
on	O
a	O
training	O
example	O
is	O
computed	O
based	O
on	O
the	O
current	O
weight	O
vector	B
,	O
not	O
based	O
on	O
the	O
voted	O
prediction	O
.	O
why	O
?	O
50	O
a	O
course	O
in	O
machine	O
learning	O
algorithm	B
7	O
averagedperceptrontrain	O
(	O
d	O
,	O
maxiter	O
)	O
1	O
:	O
w	O
←	O
(	O
cid:104	O
)	O
0	O
,	O
0	O
,	O
.	O
.	O
.	O
0	O
(	O
cid:105	O
)	O
2	O
:	O
u	O
←	O
(	O
cid:104	O
)	O
0	O
,	O
0	O
,	O
.	O
.	O
.	O
0	O
(	O
cid:105	O
)	O
3	O
:	O
c	O
←	O
1	O
4	O
:	O
for	O
iter	O
=	O
1	O
.	O
.	O
.	O
maxiter	O
do	O
5	O
:	O
b	O
←	O
0	O
,	O
,	O
β	O
←	O
0	O
//	O
initialize	O
weights	B
and	O
bias	B
//	O
initialize	O
cached	O
weights	B
and	O
bias	B
//	O
initialize	O
example	O
counter	O
to	O
one	O
6	O
:	O
7	O
:	O
8	O
:	O
9	O
:	O
10	O
:	O
11	O
:	O
12	O
:	O
if	O
y	O
(	O
w	O
·	O
x	O
+	O
b	O
)	O
≤	O
0	O
then	O
for	O
all	O
(	O
x	O
,	O
y	O
)	O
∈	O
d	O
do	O
w	O
←	O
w	O
+	O
y	O
x	O
b	O
←	O
b	O
+	O
y	O
u	O
←	O
u	O
+	O
y	O
c	O
x	O
β	O
←	O
β	O
+	O
y	O
c	O
end	O
if	O
c	O
←	O
c	O
+	O
1	O
end	O
for	O
13	O
:	O
14	O
:	O
end	O
for	O
15	O
:	O
return	O
w	O
-	O
1	O
c	O
u	O
,	O
b	O
-	O
1	O
c	O
β	O
//	O
update	O
weights	B
//	O
update	O
bias	B
//	O
update	O
cached	O
weights	B
//	O
update	O
cached	O
bias	B
//	O
increment	O
counter	O
regardless	O
of	O
update	O
//	O
return	O
averaged	O
weights	O
and	O
bias	B
averaged	O
prediction	O
,	O
eq	O
(	O
3.18	O
)	O
,	O
is	O
the	O
presense	O
of	O
the	O
interior	O
sign	B
operator	O
.	O
with	O
a	O
little	O
bit	O
of	O
algebra	O
,	O
we	O
can	O
rewrite	O
the	O
test-time	O
prediction	O
as	O
:	O
(	O
cid:33	O
)	O
(	O
cid:32	O
)	O
(	O
cid:32	O
)	O
k∑	O
k=1	O
ˆy	O
=	O
sign	B
c	O
(	O
k	O
)	O
w	O
(	O
k	O
)	O
·	O
ˆx	O
+	O
k∑	O
k=1	O
c	O
(	O
k	O
)	O
b	O
(	O
k	O
)	O
(	O
cid:33	O
)	O
(	O
3.19	O
)	O
the	O
advantage	O
of	O
the	O
averaged	O
perceptron	B
is	O
that	O
we	O
can	O
simply	O
maintain	O
a	O
running	O
sum	O
of	O
the	O
averaged	O
weight	O
vector	B
(	O
the	O
blue	O
term	O
)	O
and	O
averaged	O
bias	O
(	O
the	O
red	O
term	O
)	O
.	O
test-time	O
prediction	O
is	O
then	O
just	O
as	O
efﬁcient	O
as	O
it	O
is	O
with	O
the	O
vanilla	O
perceptron	B
.	O
the	O
full	O
training	O
algorithm	O
for	O
the	O
averaged	O
perceptron	B
is	O
shown	O
in	O
algorithm	B
3.6.	O
some	O
of	O
the	O
notation	O
is	O
changed	O
from	O
the	O
original	O
perceptron	B
:	O
namely	O
,	O
vector	B
operations	O
are	O
written	O
as	O
vector	B
opera-	O
tions	O
,	O
and	O
the	O
activation	O
computation	O
is	O
folded	O
into	O
the	O
error	O
check-	O
ing	O
.	O
it	O
is	O
probably	O
not	O
immediately	O
apparent	O
from	O
algorithm	B
3.6	O
that	O
the	O
computation	O
unfolding	O
is	O
precisely	O
the	O
calculation	O
of	O
the	O
averaged	O
weights	B
and	O
bias	B
.	O
the	O
most	O
natural	O
implementation	O
would	O
be	O
to	O
keep	O
track	O
of	O
an	O
averaged	O
weight	O
vector	B
u.	O
at	O
the	O
end	O
of	O
every	O
example	O
,	O
you	O
would	O
increase	O
u	O
←	O
u	O
+	O
w	O
(	O
and	O
similarly	O
for	O
the	O
bias	O
)	O
.	O
however	O
,	O
such	O
an	O
implementation	O
would	O
require	O
that	O
you	O
updated	O
the	O
aver-	O
aged	O
vector	B
on	O
every	O
example	O
,	O
rather	O
than	O
just	O
on	O
the	O
examples	O
that	O
were	O
incorrectly	O
classiﬁed	O
!	O
since	O
we	O
hope	O
that	O
eventually	O
the	O
per-	O
ceptron	O
learns	O
to	O
do	O
a	O
good	O
job	O
,	O
we	O
would	O
hope	O
that	O
it	O
will	O
not	O
make	O
updates	O
on	O
every	O
example	O
.	O
so	O
,	O
ideally	O
,	O
you	O
would	O
like	O
to	O
only	O
update	O
the	O
averaged	O
weight	O
vector	B
when	O
the	O
actual	O
weight	O
vector	B
changes	O
.	O
the	O
slightly	O
clever	O
computation	O
in	O
algorithm	B
3.6	O
achieves	O
this	O
.	O
the	O
averaged	O
perceptron	B
is	O
almost	O
always	O
better	O
than	O
the	O
per-	O
?	O
by	O
writing	O
out	O
the	O
computation	O
of	O
the	O
averaged	O
weights	B
from	O
eq	O
(	O
?	O
?	O
)	O
as	O
a	O
telescoping	O
sum	O
,	O
derive	O
the	O
computation	O
from	O
algorithm	B
3.6.	O
ceptron	O
,	O
in	O
the	O
sense	O
that	O
it	O
generalizes	O
better	O
to	O
test	B
data	I
.	O
however	O
,	O
that	O
does	O
not	O
free	O
you	O
from	O
having	O
to	O
do	O
early	B
stopping	I
.	O
it	O
will	O
,	O
eventually	O
,	O
overﬁt	O
.	O
figure	O
3.11	O
shows	O
the	O
performance	O
of	O
the	O
vanilla	O
perceptron	B
and	O
the	O
averaged	O
perceptron	B
on	O
the	O
same	O
data	O
set	O
,	O
with	O
both	O
training	O
and	O
test	O
performance	O
.	O
as	O
you	O
can	O
see	O
,	O
the	O
averaged	O
perceptron	B
does	O
generalize	B
better	O
.	O
but	O
it	O
also	O
does	O
begin	O
to	O
overﬁt	O
eventually	O
.	O
3.7	O
limitations	O
of	O
the	O
perceptron	O
although	O
the	O
perceptron	O
is	O
very	O
useful	O
,	O
it	O
is	O
fundamentally	O
limited	O
in	O
a	O
way	O
that	O
neither	O
decision	B
trees	I
nor	O
knn	O
are	O
.	O
its	O
limitation	O
is	O
that	O
its	O
decision	O
boundaries	O
can	O
only	O
be	O
linear	O
.	O
the	O
classic	O
way	O
of	O
showing	O
this	O
limitation	O
is	O
through	O
the	O
xor	O
problem	O
(	O
xor	O
=	O
exclusive	O
or	O
)	O
.	O
the	O
xor	O
problem	O
is	O
shown	O
graphically	O
in	O
figure	O
3.12.	O
it	O
consists	O
of	O
four	O
data	O
points	O
,	O
each	O
at	O
a	O
corner	O
of	O
the	O
unit	O
square	O
.	O
the	O
labels	O
for	O
these	O
points	O
are	O
the	O
same	O
,	O
along	O
the	O
diagonals	O
.	O
you	O
can	O
try	O
,	O
but	O
you	O
will	O
not	O
be	O
able	O
to	O
ﬁnd	O
a	O
linear	B
decision	I
boundary	I
that	O
perfectly	O
separates	O
these	O
data	O
points	O
.	O
one	O
question	O
you	O
might	O
ask	O
is	O
:	O
do	O
xor-like	O
problems	O
exist	O
in	O
the	O
real	O
world	O
?	O
unfortunately	O
for	O
the	O
perceptron	O
,	O
the	O
answer	O
is	O
yes	O
.	O
consider	O
a	O
sentiment	O
classiﬁcation	O
problem	O
that	O
has	O
three	O
features	B
that	O
simply	O
say	O
whether	O
a	O
given	O
word	O
is	O
contained	O
in	O
a	O
review	O
of	O
a	O
course	O
.	O
these	O
features	B
are	O
:	O
excellent	O
,	O
terrible	O
and	O
not	O
.	O
the	O
excellent	O
feature	O
is	O
indicative	O
of	O
positive	O
reviews	O
and	O
the	O
terrible	O
feature	O
is	O
indicative	O
of	O
negative	O
reviews	O
.	O
but	O
in	O
the	O
presence	O
of	O
the	O
not	O
feature	O
,	O
this	O
categorization	O
ﬂips	O
.	O
one	O
way	O
to	O
address	O
this	O
problem	O
is	O
by	O
adding	O
feature	O
combina-	O
tions	O
.	O
we	O
could	O
add	O
two	O
additional	O
features	B
:	O
excellent-and-not	O
and	O
terrible-and-not	O
that	O
indicate	O
a	O
conjunction	O
of	O
these	O
base	O
features	O
.	O
by	O
assigning	O
weights	B
as	O
follows	O
,	O
you	O
can	O
achieve	O
the	O
desired	O
effect	O
:	O
wexecellent	O
=	O
+1	O
wexecllent-and-not	O
=	O
−2	O
wterrible	O
=	O
−1	O
wterrible-and-not	O
=	O
+2	O
wnot	O
=	O
0	O
in	O
this	O
particular	O
case	O
,	O
we	O
have	O
addressed	O
the	O
problem	O
.	O
however	O
,	O
if	O
we	O
start	O
with	O
d-many	O
features	B
,	O
if	O
we	O
want	O
to	O
add	O
all	B
pairs	I
,	O
we	O
’	O
ll	O
blow	O
2	O
)	O
=	O
o	O
(	O
d2	O
)	O
features	B
through	O
this	O
feature	B
mapping	I
.	O
and	O
up	O
to	O
(	O
d	O
there	O
’	O
s	O
no	O
guarantee	O
that	O
pairs	O
of	O
features	B
is	O
enough	O
.	O
we	O
might	O
need	O
3	O
)	O
=	O
o	O
(	O
d2	O
)	O
features	B
.	O
these	O
triples	O
of	O
features	B
,	O
and	O
now	O
we	O
’	O
re	O
up	O
to	O
(	O
d	O
additional	O
features	B
will	O
drastically	O
increase	O
computation	O
and	O
will	O
often	O
result	O
in	O
a	O
stronger	O
propensity	O
to	O
overﬁtting	O
.	O
in	O
fact	O
,	O
the	O
“	O
xor	O
problem	O
”	O
is	O
so	O
signiﬁcant	O
that	O
it	O
basically	O
killed	O
research	O
in	O
classiﬁers	O
with	O
linear	O
decision	O
boundaries	O
for	O
a	O
decade	O
the	O
perceptron	O
51	O
figure	O
3.12	O
:	O
picture	O
of	O
xor	O
problem	O
?	O
suppose	O
that	O
you	O
took	O
the	O
xor	O
problem	O
and	O
added	O
one	O
new	O
fea-	O
ture	O
:	O
x3	O
=	O
x1	O
∧	O
x2	O
(	O
the	O
logical	O
and	O
of	O
the	O
two	O
existing	O
features	B
)	O
.	O
write	O
out	O
feature	O
weights	O
and	O
a	O
bias	B
that	O
would	O
achieve	O
perfect	O
classiﬁcation	O
on	O
this	O
data	O
.	O
52	O
a	O
course	O
in	O
machine	O
learning	O
or	O
two	O
.	O
later	O
in	O
this	O
book	O
,	O
we	O
will	O
see	O
two	O
alternative	O
approaches	O
to	O
taking	O
key	O
ideas	O
from	O
the	O
perceptron	O
and	O
generating	O
classiﬁers	O
with	O
non-linear	B
decision	O
boundaries	O
.	O
one	O
approach	O
is	O
to	O
combine	O
multi-	O
ple	O
perceptrons	O
in	O
a	O
single	O
framework	O
:	O
this	O
is	O
the	O
neural	O
networks	O
approach	O
(	O
see	O
chapter	O
8	O
)	O
.	O
the	O
second	O
approach	O
is	O
to	O
ﬁnd	O
computa-	O
tionally	O
efﬁcient	O
ways	O
of	O
doing	O
feature	B
mapping	I
in	O
a	O
computationally	O
and	O
statistically	O
efﬁcient	O
way	O
:	O
this	O
is	O
the	O
kernels	O
approach	O
(	O
see	O
chap-	O
ter	O
9	O
)	O
.	O
3.8	O
exercises	O
exercise	O
3.1.	O
todo	O
.	O
.	O
.	O
4	O
|	O
practical	O
issues	O
learning	O
objectives	O
:	O
•	O
translate	O
between	O
a	O
problem	O
de-	O
scription	O
and	O
a	O
concrete	O
learning	O
problem	O
.	O
•	O
perform	O
basic	O
feature	O
engineering	O
on	O
image	O
and	O
text	O
data	O
.	O
•	O
explain	O
how	O
to	O
use	O
cross-validation	O
to	O
tune	O
hyperparameters	O
and	O
esti-	O
mate	O
future	O
performance	O
.	O
•	O
compare	O
and	O
contrast	O
the	O
differ-	O
ences	O
between	O
several	O
evaluation	O
metrics	O
.	O
•	O
explain	O
why	O
feature	B
combinations	I
are	O
important	O
for	O
learning	O
with	O
some	O
models	O
but	O
not	O
others	O
.	O
•	O
explain	O
the	O
relationship	O
between	O
the	O
three	O
learning	O
techniques	O
you	O
have	O
seen	O
so	O
far	O
.	O
•	O
apply	O
several	O
debugging	O
techniques	O
to	O
learning	O
algorithms	O
.	O
dependencies	O
:	O
chap-	O
ter	O
?	O
?	O
,	O
chapter	O
?	O
?	O
,	O
chapter	O
?	O
?	O
in	O
theory	O
,	O
there	O
is	O
no	O
difference	O
between	O
theory	O
and	O
practice	O
.	O
but	O
,	O
in	O
practice	O
,	O
there	O
is	O
.	O
–	O
jan	O
l.a.	O
van	O
de	O
snepscheut	O
todo	O
:	O
one	O
two	O
two	O
examples	B
per	O
feature	O
at	O
this	O
point	O
,	O
you	O
have	O
seen	O
three	O
qualitatively	O
different	O
models	O
for	O
learning	O
:	O
decision	B
trees	I
,	O
nearest	O
neighbors	O
,	O
and	O
perceptrons	O
.	O
you	O
have	O
also	O
learned	O
about	O
clustering	B
with	O
the	O
k-means	O
algorithm	B
.	O
you	O
will	O
shortly	O
learn	O
about	O
more	O
complex	O
models	O
,	O
most	O
of	O
which	O
are	O
variants	O
on	O
things	O
you	O
already	O
know	O
.	O
however	O
,	O
before	O
attempting	O
to	O
understand	O
more	O
complex	O
models	O
of	O
learning	O
,	O
it	O
is	O
important	O
to	O
have	O
a	O
ﬁrm	O
grasp	O
on	O
how	O
to	O
use	O
machine	O
learning	O
in	O
practice	O
.	O
this	O
chapter	O
is	O
all	O
about	O
how	O
to	O
go	O
from	O
an	O
abstract	O
learning	O
problem	O
to	O
a	O
concrete	O
implementation	O
.	O
you	O
will	O
see	O
some	O
examples	B
of	O
“	O
best	O
practices	O
”	O
along	O
with	O
justiﬁcations	O
of	O
these	O
practices	O
.	O
in	O
many	O
ways	O
,	O
going	O
from	O
an	O
abstract	O
problem	O
to	O
a	O
concrete	O
learn-	O
ing	O
task	O
is	O
more	O
of	O
an	O
art	O
than	O
a	O
science	O
.	O
however	O
,	O
this	O
art	O
can	O
have	O
a	O
huge	O
impact	O
on	O
the	O
practical	O
performance	O
of	O
learning	O
systems	O
.	O
in	O
many	O
cases	O
,	O
moving	O
to	O
a	O
more	O
complicated	O
learning	O
algorithm	B
will	O
gain	O
you	O
a	O
few	O
percent	O
improvement	O
.	O
going	O
to	O
a	O
better	O
representa-	O
tion	O
will	O
gain	O
you	O
an	O
order	O
of	O
magnitude	O
improvement	O
.	O
to	O
this	O
end	O
,	O
we	O
will	O
discuss	O
several	O
high	O
level	O
ideas	O
to	O
help	O
you	O
develop	O
a	O
better	O
artistic	O
sensibility	O
.	O
4.1	O
the	O
importance	O
of	O
good	O
features	B
machine	O
learning	O
is	O
magical	O
.	O
you	O
give	O
it	O
data	O
and	O
it	O
manages	O
to	O
classify	O
that	O
data	O
.	O
for	O
many	O
,	O
it	O
can	O
actually	O
outperform	O
a	O
human	O
!	O
but	O
,	O
like	O
so	O
many	O
problems	O
in	O
the	O
world	O
,	O
there	O
is	O
a	O
signiﬁcant	O
“	O
garbage	O
in	O
,	O
garbage	O
out	O
”	O
aspect	O
to	O
machine	O
learning	O
.	O
if	O
the	O
data	O
you	O
give	O
it	O
is	O
trash	O
,	O
the	O
learning	O
algorithm	B
is	O
unlikely	O
to	O
be	O
able	O
to	O
overcome	O
it	O
.	O
with	O
a	O
100×100	O
pixel	O
image	O
,	O
a	O
very	O
easy	O
feature	O
representation	O
of	O
this	O
image	O
is	O
as	O
a	O
30	O
,	O
000	O
dimensional	O
vector	B
,	O
where	O
each	O
dimension	O
corresponds	O
to	O
the	O
red	O
,	O
green	O
or	O
blue	O
component	O
of	O
some	O
pixel	O
in	O
the	O
image	O
.	O
so	O
perhaps	O
feature	O
1	O
is	O
the	O
amount	O
of	O
red	O
in	O
pixel	O
(	O
1	O
,	O
1	O
)	O
;	O
feature	O
2	O
is	O
the	O
amount	O
of	O
green	O
in	O
that	O
pixel	O
;	O
and	O
so	O
on	O
.	O
this	O
is	O
the	O
consider	O
a	O
problem	O
of	O
object	O
recognition	O
from	O
images	O
.	O
if	O
you	O
start	O
54	O
a	O
course	O
in	O
machine	O
learning	O
pixel	B
representation	I
of	O
images	O
.	O
one	O
thing	O
to	O
keep	O
in	O
mind	O
is	O
that	O
the	O
pixel	O
representation	O
throws	O
away	O
all	O
locality	O
information	O
in	O
the	O
image	O
.	O
learning	O
algorithms	O
don	O
’	O
t	O
care	O
about	O
features	B
:	O
they	O
only	O
care	O
about	O
feature	B
values	I
.	O
so	O
i	O
can	O
permute	O
all	O
of	O
the	O
features	O
,	O
with	O
no	O
effect	O
on	O
the	O
learning	O
algorithm	B
(	O
so	O
long	O
as	O
i	O
apply	O
the	O
same	O
permutation	O
to	O
all	O
training	O
and	O
test	O
examples	O
)	O
.	O
figure	O
4.1	O
shows	O
some	O
images	O
whos	O
pixels	O
have	O
been	O
randomly	O
permuted	O
(	O
in	O
this	O
case	O
only	O
the	O
pixels	O
are	O
permuted	O
,	O
not	O
the	O
colors	O
)	O
.	O
all	O
of	O
these	O
objects	O
are	O
things	O
that	O
you	O
’	O
ve	O
seen	O
plenty	O
of	O
examples	B
of	O
;	O
can	O
you	O
identify	O
them	O
?	O
should	O
you	O
expect	O
a	O
machine	O
to	O
be	O
able	O
to	O
?	O
an	O
alternative	O
representation	O
of	O
images	O
is	O
the	O
patch	O
represen-	O
tation	O
,	O
where	O
the	O
unit	O
of	O
interest	O
is	O
a	O
small	O
rectangular	O
block	O
of	O
an	O
image	O
,	O
rather	O
than	O
a	O
single	O
pixel	O
.	O
again	O
,	O
permuting	O
the	O
patches	O
has	O
no	O
effect	O
on	O
the	O
classiﬁer	O
.	O
figure	O
4.2	O
shows	O
the	O
same	O
images	O
in	O
patch	B
representation	I
.	O
can	O
you	O
identify	O
them	O
?	O
a	O
ﬁnal	O
representation	O
is	O
a	O
shape	B
representation	I
.	O
here	O
,	O
we	O
throw	O
out	O
all	O
color	O
and	O
pixel	O
infor-	O
mation	O
and	O
simply	O
provide	O
a	O
bounding	O
polygon	O
.	O
figure	O
4.3	O
shows	O
the	O
same	O
images	O
in	O
this	O
representation	O
.	O
is	O
this	O
now	O
enough	O
to	O
iden-	O
tify	O
them	O
?	O
(	O
if	O
not	O
,	O
you	O
can	O
ﬁnd	O
the	O
answers	O
at	O
the	O
end	O
of	O
this	O
chap-	O
ter	O
.	O
)	O
in	O
the	O
context	O
of	O
text	B
categorization	I
(	O
for	O
instance	O
,	O
the	O
sentiment	O
recognition	O
task	O
)	O
,	O
one	O
standard	O
representation	O
is	O
the	O
bag	O
of	O
words	O
representation	O
.	O
here	O
,	O
we	O
have	O
one	O
feature	O
for	O
each	O
unique	O
word	O
that	O
appears	O
in	O
a	O
document	O
.	O
for	O
the	O
feature	O
happy	O
,	O
the	O
feature	O
value	O
is	O
the	O
number	O
of	O
times	O
that	O
the	O
word	O
“	O
happy	O
”	O
appears	O
in	O
the	O
document	O
.	O
the	O
bag	O
of	O
words	O
(	O
bow	O
)	O
representation	O
throws	O
away	O
all	O
position	O
information	O
.	O
figure	O
4.4	O
shows	O
a	O
bow	O
representation	O
for	O
two	O
docu-	O
ments	O
:	O
one	O
positive	O
and	O
one	O
negative	O
.	O
can	O
you	O
tell	O
which	O
is	O
which	O
?	O
4.2	O
irrelevant	O
and	O
redundant	B
features	I
one	O
big	O
difference	O
between	O
learning	O
models	O
is	O
how	O
robust	O
they	O
are	O
to	O
the	O
addition	O
of	O
noisy	O
or	O
irrelevant	O
features	B
.	O
intuitively	O
,	O
an	O
irrelevant	O
feature	O
is	O
one	O
that	O
is	O
completely	O
uncorrelated	O
with	O
the	O
prediction	O
task	O
.	O
a	O
feature	O
f	O
whose	O
expectation	O
does	O
not	O
depend	O
on	O
the	O
label	O
e	O
[	O
f	O
|	O
y	O
]	O
=	O
e	O
[	O
f	O
]	O
might	O
be	O
irrelevant	O
.	O
for	O
instance	O
,	O
the	O
presence	O
of	O
the	O
word	O
“	O
the	O
”	O
might	O
be	O
largely	O
irrelevant	O
for	O
predicting	O
whether	O
a	O
course	O
review	O
is	O
positive	O
or	O
negative	O
.	O
a	O
secondary	O
issue	O
is	O
how	O
well	O
these	O
algorithms	O
deal	O
with	O
redun-	O
dant	O
features	B
.	O
two	O
features	B
are	O
redundant	O
if	O
they	O
are	O
highly	O
cor-	O
related	O
,	O
regardless	O
of	O
whether	O
they	O
are	O
correlated	O
with	O
the	O
task	O
or	O
not	O
.	O
for	O
example	O
,	O
having	O
a	O
bright	O
red	O
pixel	O
in	O
an	O
image	O
at	O
position	O
(	O
20	O
,	O
93	O
)	O
is	O
probably	O
highly	O
redundant	O
with	O
having	O
a	O
bright	O
red	O
pixel	O
figure	O
4.1	O
:	O
prac	O
:	O
imagepix	O
:	O
object	O
recognition	O
in	O
pixels	O
figure	O
4.2	O
:	O
prac	O
:	O
imagepatch	O
:	O
object	O
recognition	O
in	O
patches	O
figure	O
4.3	O
:	O
prac	O
:	O
imageshape	O
:	O
object	O
recognition	O
in	O
shapes	O
figure	O
4.4	O
:	O
prac	O
:	O
bow	O
:	O
bow	O
repr	O
of	O
one	O
positive	O
and	O
one	O
negative	O
review	O
is	O
it	O
possible	O
to	O
have	O
a	O
feature	O
f	O
practical	O
issues	O
55	O
1	O
you	O
might	O
think	O
it	O
’	O
s	O
crazy	O
to	O
have	O
so	O
many	O
irrelevant	O
features	B
,	O
but	O
the	O
cases	O
you	O
’	O
ve	O
seen	O
so	O
far	O
(	O
bag	B
of	I
words	I
,	O
bag	O
of	O
pixels	O
)	O
are	O
both	O
reasonable	O
examples	B
of	O
this	O
!	O
how	O
many	O
words	O
,	O
out	O
of	O
the	O
entire	O
english	O
vocabulary	O
(	O
roughly	O
10	O
,	O
000	O
−	O
100	O
,	O
000	O
words	O
)	O
,	O
are	O
actually	O
useful	O
for	O
predicting	O
positive	O
and	O
negative	O
course	O
reviews	O
?	O
at	O
position	O
(	O
21	O
,	O
93	O
)	O
.	O
both	O
might	O
be	O
useful	O
(	O
e.g.	O
,	O
for	O
identifying	O
ﬁre	O
hy-	O
drants	O
)	O
,	O
but	O
because	O
of	O
how	O
images	O
are	O
structured	O
,	O
these	O
two	O
features	B
are	O
likely	O
to	O
co-occur	O
frequently	O
.	O
when	O
thinking	O
about	O
robustness	O
to	O
irrelevant	O
or	O
redundant	O
fea-	O
tures	O
,	O
it	O
is	O
usually	O
not	O
worthwhile	O
thinking	O
of	O
the	O
case	O
where	O
one	O
has	O
999	O
great	O
features	B
and	O
1	O
bad	O
feature	O
.	O
the	O
interesting	O
case	O
is	O
when	O
the	O
bad	O
features	B
outnumber	O
the	O
good	O
features	B
,	O
and	O
often	O
outnumber	O
by	O
a	O
large	O
degree	O
.	O
for	O
instance	O
,	O
perhaps	O
the	O
number	O
of	O
good	O
features	B
is	O
something	O
like	O
log	O
d	O
out	O
of	O
a	O
set	O
of	O
d	O
total	O
features	B
.	O
the	O
question	O
is	O
how	O
robust	O
are	O
algorithms	O
in	O
this	O
case.1	O
for	O
shallow	O
decision	O
trees	O
,	O
the	O
model	O
explicitly	O
selects	O
features	B
that	O
are	O
highly	O
correlated	O
with	O
the	O
label	O
.	O
in	O
particular	O
,	O
by	O
limiting	O
the	O
depth	O
of	O
the	O
decision	O
tree	O
,	O
one	O
can	O
at	O
least	O
hope	O
that	O
the	O
model	O
will	O
be	O
able	O
to	O
throw	O
away	O
irrelevant	O
features	B
.	O
redundant	B
features	I
are	O
almost	O
certainly	O
thrown	O
out	O
:	O
once	O
you	O
select	O
one	O
feature	O
,	O
the	O
second	O
feature	O
now	O
looks	O
mostly	O
useless	O
.	O
the	O
only	O
possible	O
issue	O
with	O
irrelevant	O
features	B
is	O
that	O
even	O
though	O
they	O
’	O
re	O
irrelevant	O
,	O
they	O
happen	O
to	O
correlate	O
with	O
the	O
class	O
label	B
on	O
the	O
training	O
data	O
,	O
but	O
chance	O
.	O
as	O
a	O
thought	O
experiment	O
,	O
suppose	O
that	O
we	O
have	O
n	O
training	O
ex-	O
amples	O
,	O
and	O
exactly	O
half	O
are	O
positive	O
examples	O
and	O
half	O
are	O
negative	O
examples	B
.	O
suppose	O
there	O
’	O
s	O
some	O
binary	O
feature	O
,	O
f	O
,	O
that	O
is	O
completely	O
uncorrelated	O
with	O
the	O
label	O
.	O
this	O
feature	O
has	O
a	O
50/50	O
chance	O
of	O
ap-	O
pearing	O
in	O
any	O
example	O
,	O
regardless	O
of	O
the	O
label	O
.	O
in	O
principle	O
,	O
the	O
deci-	O
sion	O
tree	O
should	O
not	O
select	O
this	O
feature	O
.	O
but	O
,	O
by	O
chance	O
,	O
especially	O
if	O
n	O
is	O
small	O
,	O
the	O
feature	O
might	O
look	O
correlated	O
with	O
the	O
label	O
.	O
this	O
is	O
anal-	O
ogous	O
to	O
ﬂipping	O
two	O
coins	O
simultaneously	O
n	O
times	O
.	O
even	O
though	O
the	O
coins	O
are	O
independent	O
,	O
it	O
’	O
s	O
entirely	O
possible	O
that	O
you	O
will	O
observe	O
a	O
sequence	O
like	O
(	O
h	O
,	O
h	O
)	O
,	O
(	O
t	O
,	O
t	O
)	O
,	O
(	O
h	O
,	O
h	O
)	O
,	O
(	O
h	O
,	O
h	O
)	O
,	O
which	O
makes	O
them	O
look	O
entirely	O
correlated	O
!	O
the	O
hope	O
is	O
that	O
as	O
n	O
grows	O
,	O
this	O
becomes	O
less	O
and	O
less	O
likely	O
.	O
in	O
fact	O
,	O
we	O
can	O
explicitly	O
compute	O
how	O
likely	O
this	O
is	O
to	O
happen	O
.	O
to	O
do	O
this	O
,	O
let	O
’	O
s	O
ﬁx	O
the	O
sequence	O
of	O
n	O
labels	O
.	O
we	O
now	O
ﬂip	O
a	O
coin	O
n	O
times	O
and	O
consider	O
how	O
likely	O
it	O
is	O
that	O
it	O
exactly	O
matches	O
the	O
label	O
.	O
this	O
is	O
easy	O
:	O
the	O
probability	O
is	O
0.5n	O
.	O
now	O
,	O
we	O
would	O
also	O
be	O
confused	O
if	O
it	O
exactly	O
matched	O
not	O
the	O
label	O
,	O
which	O
has	O
the	O
same	O
probability	O
.	O
so	O
the	O
chance	O
that	O
it	O
looks	O
perfectly	O
correlated	O
is	O
0.5n	O
+	O
0.5n	O
=	O
0.5n−1	O
.	O
thankfully	O
,	O
this	O
shrinks	O
down	O
very	O
small	O
(	O
e.g.	O
,	O
10−6	O
)	O
after	O
only	O
21	O
data	O
points	O
.	O
this	O
makes	O
us	O
happy	O
.	O
the	O
problem	O
is	O
that	O
we	O
don	O
’	O
t	O
have	O
one	O
ir-	O
relevant	O
feature	O
:	O
we	O
have	O
d	O
−	O
log	O
d	O
irrelevant	O
features	B
!	O
if	O
we	O
ran-	O
domly	O
pick	O
two	O
irrelevant	O
feature	B
values	I
,	O
each	O
has	O
the	O
same	O
prob-	O
ability	O
of	O
perfectly	O
correlating	O
:	O
0.5n−1	O
.	O
but	O
since	O
there	O
are	O
two	O
and	O
they	O
’	O
re	O
independent	O
coins	O
,	O
the	O
chance	O
that	O
either	O
correlates	O
perfectly	O
is	O
2×0.5n−1	O
=	O
0.5n−2	O
.	O
in	O
general	O
,	O
if	O
we	O
have	O
k	O
irrelevant	O
features	B
,	O
all	O
56	O
a	O
course	O
in	O
machine	O
learning	O
of	O
which	O
are	O
random	O
independent	O
coins	O
,	O
the	O
chance	O
that	O
at	O
least	O
one	O
of	O
them	O
perfectly	O
correlates	O
is	O
0.5n−k	O
.	O
this	O
suggests	O
that	O
if	O
we	O
have	O
a	O
sizeable	O
number	O
k	O
of	O
irrelevant	O
features	B
,	O
we	O
’	O
d	O
better	O
have	O
at	O
least	O
k	O
+	O
21	O
training	O
examples	O
.	O
unfortunately	O
,	O
the	O
situation	O
is	O
actually	O
worse	O
than	O
this	O
.	O
in	O
the	O
above	O
analysis	O
we	O
only	O
considered	O
the	O
case	O
of	O
perfect	O
correlation	O
.	O
we	O
could	O
also	O
consider	O
the	O
case	O
of	O
partial	O
correlation	O
,	O
which	O
would	O
yield	O
even	O
higher	O
probabilities	O
.	O
(	O
this	O
is	O
left	O
as	O
exercise	O
?	O
?	O
for	O
those	O
who	O
want	O
some	O
practice	O
with	O
probabilistic	O
analysis	O
.	O
)	O
sufﬁce	O
it	O
to	O
say	O
that	O
even	O
decision	B
trees	I
can	O
become	O
confused	O
.	O
in	O
the	O
case	O
of	O
k-nearest	O
neighbors	O
,	O
the	O
situation	O
is	O
perhaps	O
more	O
dire	O
.	O
since	O
knn	O
weighs	O
each	O
feature	O
just	O
as	O
much	O
as	O
another	O
feature	O
,	O
the	O
introduction	O
of	O
irrelevant	O
features	B
can	O
completely	O
mess	O
up	O
knn	O
prediction	O
.	O
in	O
fact	O
,	O
as	O
you	O
saw	O
,	O
in	O
high	O
dimensional	O
space	O
,	O
randomly	O
distributed	O
points	O
all	O
look	O
approximately	O
the	O
same	O
distance	B
apart	O
.	O
if	O
we	O
add	O
lots	O
and	O
lots	O
of	O
randomly	O
distributed	O
features	B
to	O
a	O
data	O
set	O
,	O
then	O
all	O
distances	O
still	O
converge	O
.	O
this	O
is	O
shown	O
experimentally	O
in	O
figure	O
?	O
?	O
,	O
where	O
we	O
start	O
with	O
the	O
digit	O
categorization	O
data	O
and	O
con-	O
tinually	O
add	O
irrelevant	O
,	O
uniformly	O
distributed	O
features	B
,	O
and	O
generate	O
a	O
histogram	B
of	O
distances	O
.	O
eventually	O
,	O
all	O
distances	O
converge	O
.	O
in	O
the	O
case	O
of	O
the	O
perceptron	O
,	O
one	O
can	O
hope	O
that	O
it	O
might	O
learn	O
to	O
assign	O
zero	O
weight	O
to	O
irrelevant	O
features	B
.	O
for	O
instance	O
,	O
consider	O
a	O
binary	O
feature	O
is	O
randomly	O
one	O
or	O
zero	O
independent	O
of	O
the	O
label	O
.	O
if	O
the	O
perceptron	O
makes	O
just	O
as	O
many	O
updates	O
for	O
positive	O
examples	O
as	O
for	O
negative	O
examples	B
,	O
there	O
is	O
a	O
reasonable	O
chance	O
this	O
feature	O
weight	O
will	O
be	O
zero	O
.	O
at	O
the	O
very	O
least	O
,	O
it	O
should	O
be	O
small	O
.	O
to	O
get	O
a	O
better	O
practical	O
sense	O
of	O
how	O
sensitive	O
these	O
algorithms	O
are	O
to	O
irrelevant	O
features	B
,	O
figure	O
4.6	O
shows	O
the	O
test	O
performance	O
of	O
the	O
three	O
algorithms	O
with	O
an	O
increasing	O
number	O
of	O
compltely	O
noisy	O
features	B
.	O
in	O
all	O
cases	O
,	O
the	O
hyperparameters	O
were	O
tuned	O
on	O
validation	B
data	I
.	O
todo	O
...	O
4.3	O
feature	O
pruning	O
and	O
normalization	O
in	O
text	B
categorization	I
problems	O
,	O
some	O
words	O
simply	O
do	O
not	O
appear	O
very	O
often	O
.	O
perhaps	O
the	O
word	O
“	O
groovy	O
”	O
2	O
appears	O
in	O
exactly	O
one	O
train-	O
ing	O
document	O
,	O
which	O
is	O
positive	O
.	O
is	O
it	O
really	O
worth	O
keeping	O
this	O
word	O
around	O
as	O
a	O
feature	O
?	O
it	O
’	O
s	O
a	O
dangerous	O
endeavor	O
because	O
it	O
’	O
s	O
hard	O
to	O
tell	O
with	O
just	O
one	O
training	O
example	O
if	O
it	O
is	O
really	O
correlated	O
with	O
the	O
positive	O
class	O
,	O
or	O
is	O
it	O
just	O
noise	B
.	O
you	O
could	O
hope	O
that	O
your	O
learning	O
algorithm	B
is	O
smart	O
enough	O
to	O
ﬁgure	O
it	O
out	O
.	O
or	O
you	O
could	O
just	O
remove	O
it	O
.	O
that	O
means	O
that	O
(	O
a	O
)	O
the	O
learning	O
algorithm	B
won	O
’	O
t	O
have	O
to	O
ﬁgure	O
it	O
out	O
,	O
and	O
(	O
b	O
)	O
you	O
’	O
ve	O
reduced	O
the	O
number	O
of	O
dimensions	O
you	O
have	O
,	O
so	O
things	O
should	O
be	O
more	O
efﬁcient	O
,	O
and	O
less	O
“	O
scary.	O
”	O
figure	O
4.5	O
:	O
prac	O
:	O
addirel	O
:	O
data	O
from	O
high	O
dimensional	O
warning	O
,	O
interpolated	O
?	O
what	O
happens	O
with	O
the	O
perceptron	O
with	O
truly	O
redundant	B
features	I
(	O
i.e.	O
,	O
one	O
is	O
literally	O
a	O
copy	O
of	O
the	O
other	O
)	O
?	O
figure	O
4.6	O
:	O
prac	O
:	O
noisy	O
:	O
dt	O
,	O
knn	O
,	O
perc	O
on	O
increasing	O
amounts	O
of	O
noise	B
2	O
this	O
is	O
typically	O
positive	O
indicator	O
,	O
or	O
at	O
least	O
it	O
was	O
back	O
in	O
the	O
us	O
in	O
the	O
1970s	O
.	O
practical	O
issues	O
57	O
figure	O
4.8	O
:	O
3	O
according	O
to	O
google	O
,	O
the	O
following	O
words	O
(	O
among	O
many	O
others	O
)	O
appear	O
200	O
times	O
on	O
the	O
web	O
:	O
moudlings	O
,	O
agag-	O
gagctg	O
,	O
setgravity	O
,	O
rogov	O
,	O
prosomeric	O
,	O
spunlaid	O
,	O
piyushtwok	O
,	O
telelesson	O
,	O
nes-	O
mysl	O
,	O
brighnasa	O
.	O
for	O
comparison	O
,	O
the	O
word	O
“	O
the	O
”	O
appears	O
19	O
,	O
401	O
,	O
194	O
,	O
714	O
(	O
19	O
billion	O
)	O
times	O
.	O
figure	O
4.9	O
:	O
prac	O
:	O
variance	B
:	O
effect	O
of	O
pruning	O
on	O
vision	O
?	O
earlier	O
we	O
discussed	O
the	O
problem	O
of	O
scale	O
of	O
features	B
(	O
e.g.	O
,	O
millimeters	O
versus	O
centimeters	O
)	O
.	O
does	O
this	O
have	O
an	O
impact	O
on	O
variance-based	O
feature	O
pruning	O
?	O
math	O
review	O
|	O
data	O
statistics	O
:	O
means	O
and	O
variances	O
data	O
mean	O
,	O
variance	B
,	O
moments	O
,	O
expectations	O
,	O
etc	O
...	O
this	O
idea	O
of	O
feature	O
pruning	O
is	O
very	O
useful	O
and	O
applied	O
in	O
many	O
applications	O
.	O
it	O
is	O
easiest	O
in	O
the	O
case	O
of	O
binary	B
features	I
.	O
if	O
a	O
binary	O
feature	O
only	O
appears	O
some	O
small	O
number	O
k	O
times	O
(	O
in	O
the	O
training	O
data	O
:	O
no	O
fair	O
looking	O
at	O
the	O
test	O
data	O
!	O
)	O
,	O
you	O
simply	O
remove	O
it	O
from	O
consideration	O
.	O
(	O
you	O
might	O
also	O
want	O
to	O
remove	O
features	B
that	O
appear	O
in	O
all-but-k	O
many	O
documents	O
,	O
for	O
instance	O
the	O
word	O
“	O
the	O
”	O
appears	O
in	O
pretty	O
much	O
every	O
english	O
document	O
ever	O
written	O
.	O
)	O
typical	O
choices	O
for	O
k	O
are	O
1	O
,	O
2	O
,	O
5	O
,	O
10	O
,	O
20	O
,	O
50	O
,	O
mostly	O
depending	O
on	O
the	O
size	O
of	O
the	O
data	O
.	O
on	O
a	O
text	O
data	O
set	O
with	O
1000	O
documents	O
,	O
a	O
cutoff	O
of	O
5	O
is	O
probably	O
reasonable	O
.	O
on	O
a	O
text	O
data	O
set	O
the	O
size	O
of	O
the	O
web	O
,	O
a	O
cut	O
of	O
of	O
50	O
or	O
even	O
100	O
or	O
200	O
is	O
probably	O
reasonable3	O
.	O
figure	O
4.7	O
shows	O
the	O
effect	O
of	O
pruning	O
on	O
a	O
sentiment	O
analysis	O
task	O
.	O
in	O
the	O
beginning	O
,	O
pruning	O
does	O
not	O
hurt	O
(	O
and	O
sometimes	O
helps	O
!	O
)	O
but	O
eventually	O
we	O
prune	O
away	O
all	O
the	O
interesting	O
words	O
and	O
performance	O
suffers	O
.	O
in	O
the	O
case	O
of	O
real-valued	O
features	B
,	O
the	O
question	O
is	O
how	O
to	O
extend	O
the	O
idea	O
of	O
“	O
does	O
not	O
occur	O
much	O
”	O
to	O
real	O
values	O
.	O
a	O
reasonable	O
def-	O
inition	O
is	O
to	O
look	O
for	O
features	B
with	O
low	O
variance	B
.	O
in	O
fact	O
,	O
for	O
binary	B
features	I
,	O
ones	O
that	O
almost	O
never	O
appear	O
or	O
almost	O
always	O
appear	O
will	O
also	O
have	O
low	O
variance	B
.	O
figure	O
4.9	O
shows	O
the	O
result	O
of	O
pruning	O
low-	O
variance	B
features	O
on	O
the	O
digit	O
recognition	O
task	O
.	O
again	O
,	O
at	O
ﬁrst	O
pruning	O
does	O
not	O
hurt	O
(	O
and	O
sometimes	O
helps	O
!	O
)	O
but	O
eventually	O
we	O
have	O
thrown	O
out	O
all	O
the	O
useful	O
features	B
.	O
once	O
you	O
have	O
pruned	O
away	O
irrelevant	O
features	B
,	O
it	O
is	O
often	O
useful	O
to	O
normalize	B
the	O
data	O
so	O
that	O
it	O
is	O
consistent	O
in	O
some	O
way	O
.	O
there	O
are	O
two	O
basic	O
types	O
of	O
normalization	O
:	O
feature	B
normalization	I
and	O
exam-	O
ple	O
normalization	O
.	O
in	O
feature	B
normalization	I
,	O
you	O
go	O
through	O
each	O
feature	O
and	O
adjust	O
it	O
the	O
same	O
way	O
across	O
all	O
examples	O
.	O
in	O
example	B
normalization	I
,	O
each	O
example	O
is	O
adjusted	O
individually	O
.	O
the	O
goal	O
of	O
both	O
types	O
of	O
normalization	O
is	O
to	O
make	O
it	O
easier	O
for	O
your	O
learning	O
algorithm	B
to	O
learn	O
.	O
in	O
feature	B
normalization	I
,	O
there	O
are	O
two	O
standard	O
things	O
to	O
do	O
:	O
1.	O
centering	O
:	O
moving	O
the	O
entire	O
data	O
set	O
so	O
that	O
it	O
is	O
centered	O
around	O
the	O
origin	O
.	O
2.	O
scaling	O
:	O
rescaling	O
each	O
feature	O
so	O
that	O
one	O
of	O
the	O
following	O
holds	O
:	O
(	O
a	O
)	O
each	O
feature	O
has	O
variance	B
1	O
across	O
the	O
training	O
data	O
.	O
(	O
b	O
)	O
each	O
feature	O
has	O
maximum	O
absolute	O
value	O
1	O
across	O
the	O
train-	O
ing	O
data	O
.	O
58	O
a	O
course	O
in	O
machine	O
learning	O
these	O
transformations	O
are	O
shown	O
geometrically	O
in	O
figure	O
4.10.	O
the	O
goal	O
of	O
centering	O
is	O
to	O
make	O
sure	O
that	O
no	O
features	B
are	O
arbitrarily	O
large	O
.	O
the	O
goal	O
of	O
scaling	O
is	O
to	O
make	O
sure	O
that	O
all	O
features	O
have	O
roughly	O
the	O
same	O
scale	O
(	O
to	O
avoid	O
the	O
issue	O
of	O
centimeters	O
versus	O
millimeters	O
)	O
.	O
these	O
computations	O
are	O
fairly	O
straightforward	O
.	O
here	O
,	O
xn	O
,	O
d	O
refers	O
to	O
the	O
dth	O
feature	O
of	O
example	O
n.	O
since	O
it	O
is	O
very	O
rare	O
to	O
apply	O
scaling	O
without	O
previously	O
applying	O
centering	O
,	O
the	O
expressions	O
below	O
for	O
scaling	O
assume	O
that	O
the	O
data	O
is	O
already	O
centered	O
.	O
?	O
for	O
the	O
three	O
models	O
you	O
know	O
about	O
(	O
knn	O
,	O
dt	O
,	O
perceptron	B
)	O
,	O
which	O
are	O
most	O
sensitive	O
to	O
center-	O
ing	O
?	O
which	O
are	O
most	O
sensitive	O
to	O
scaling	O
?	O
centering	O
:	O
variance	B
scaling	O
:	O
absolute	O
scaling	O
:	O
where	O
:	O
xn	O
,	O
d	O
←	O
xn	O
,	O
d	O
−	O
µd	O
xn	O
,	O
d	O
←	O
xn	O
,	O
d/σd	O
xn	O
,	O
d	O
←	O
xn	O
,	O
d/rd	O
∑	O
xn	O
,	O
d	O
µd	O
=	O
n	O
1	O
n	O
(	O
cid:115	O
)	O
σd	O
=	O
1	O
n	O
rd	O
=	O
max	O
n	O
(	O
xn	O
,	O
d	O
−	O
µd	O
)	O
2	O
∑	O
n	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
xn	O
,	O
d	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
4.1	O
)	O
(	O
4.2	O
)	O
(	O
4.3	O
)	O
(	O
4.4	O
)	O
(	O
4.5	O
)	O
(	O
4.6	O
)	O
in	O
practice	O
,	O
if	O
the	O
dynamic	O
range	O
of	O
your	O
features	B
is	O
already	O
some	O
subset	O
of	O
[	O
−2	O
,	O
2	O
]	O
or	O
[	O
−3	O
,	O
3	O
]	O
,	O
then	O
it	O
is	O
probably	O
not	O
worth	O
the	O
effort	O
of	O
centering	O
and	O
scaling	O
.	O
(	O
it	O
’	O
s	O
an	O
effort	O
because	O
you	O
have	O
to	O
keep	O
around	O
your	O
centering	O
and	O
scaling	O
calculations	O
so	O
that	O
you	O
can	O
apply	O
them	O
to	O
the	O
test	O
data	O
as	O
well	O
!	O
)	O
however	O
,	O
if	O
some	O
of	O
your	O
features	B
are	O
orders	O
of	O
magnitude	O
larger	O
than	O
others	O
,	O
it	O
might	O
be	O
helpful	O
.	O
remember	O
that	O
you	O
might	O
know	O
best	O
:	O
if	O
the	O
difference	O
in	O
scale	O
is	O
actually	O
signiﬁcant	O
for	O
your	O
problem	O
,	O
then	O
rescaling	O
might	O
throw	O
away	O
useful	O
informa-	O
tion	O
.	O
one	O
thing	O
to	O
be	O
wary	O
of	O
is	O
centering	O
binary	O
data	O
.	O
in	O
many	O
cases	O
,	O
binary	O
data	O
is	O
very	O
sparse	B
:	O
for	O
a	O
given	O
example	O
,	O
only	O
a	O
few	O
of	O
the	O
features	O
are	O
“	O
on.	O
”	O
for	O
instance	O
,	O
out	O
of	O
a	O
vocabulary	O
of	O
10	O
,	O
000	O
or	O
100	O
,	O
000	O
words	O
,	O
a	O
given	O
document	O
probably	O
only	O
contains	O
about	O
100.	O
from	O
a	O
storage	O
and	O
computation	O
perspective	O
,	O
this	O
is	O
very	O
useful	O
.	O
however	O
,	O
after	O
centering	O
,	O
the	O
data	O
will	O
no	O
longer	O
sparse	B
and	O
you	O
will	O
pay	O
dearly	O
with	O
outrageously	O
slow	O
implementations	O
.	O
in	O
example	B
normalization	I
,	O
you	O
view	O
examples	B
one	O
at	O
a	O
time	O
.	O
the	O
most	O
standard	O
normalization	O
is	O
to	O
ensure	O
that	O
the	O
length	O
of	O
each	O
example	O
vector	O
is	O
one	O
:	O
namely	O
,	O
each	O
example	O
lies	O
somewhere	O
on	O
the	O
unit	O
hypersphere	O
.	O
this	O
is	O
a	O
simple	O
transformation	O
:	O
example	B
normalization	I
:	O
xn	O
←	O
xn/	O
||xn||	O
(	O
4.7	O
)	O
figure	O
4.11	O
:	O
prac	O
:	O
exnorm	O
:	O
example	O
of	O
example	B
normalization	I
this	O
transformation	O
is	O
depicted	O
in	O
figure	O
4.11.	O
the	O
main	O
advantage	O
to	O
example	B
normalization	I
is	O
that	O
it	O
makes	O
comparisons	O
more	O
straightforward	O
across	O
data	O
sets	O
.	O
if	O
i	O
hand	O
you	O
practical	O
issues	O
59	O
figure	O
4.12	O
:	O
prac	O
:	O
dttoperc	O
:	O
turning	O
a	O
dt	O
into	O
a	O
set	O
of	O
meta	O
features	B
figure	O
4.13	O
:	O
prac	O
:	O
log	O
:	O
performance	O
on	O
text	O
categ	O
with	O
word	O
counts	O
versus	O
log	O
word	O
counts	O
two	O
data	O
sets	O
that	O
differ	O
only	O
in	O
the	O
norm	O
of	O
the	O
feature	O
vectors	O
(	O
i.e.	O
,	O
one	O
is	O
just	O
a	O
scaled	O
version	O
of	O
the	O
other	O
)	O
,	O
it	O
is	O
difﬁcult	O
to	O
compare	O
the	O
learned	O
models	O
.	O
example	B
normalization	I
makes	O
this	O
more	O
straightfor-	O
ward	O
.	O
moreover	O
,	O
as	O
you	O
saw	O
in	O
the	O
perceptron	O
convergence	O
proof	O
,	O
it	O
is	O
often	O
just	O
mathematically	O
easier	O
to	O
assume	O
normalized	O
data	O
.	O
4.4	O
combinatorial	O
feature	O
explosion	O
you	O
learned	O
in	O
chapter	O
3	O
that	O
linear	O
models	O
(	O
like	O
the	O
perceptron	O
)	O
can	O
not	O
solve	O
the	O
xor	O
problem	O
.	O
you	O
also	O
learned	O
that	O
by	O
performing	O
a	O
combinatorial	O
feature	O
explosion	O
,	O
they	O
could	O
.	O
but	O
that	O
came	O
at	O
the	O
computational	O
expense	O
of	O
gigantic	O
feature	O
vectors	O
.	O
of	O
the	O
algorithms	O
that	O
you	O
’	O
ve	O
seen	O
so	O
far	O
,	O
the	O
perceptron	O
is	O
the	O
one	O
that	O
has	O
the	O
most	O
to	O
gain	O
by	O
feature	O
combination	O
.	O
and	O
the	O
decision	O
tree	O
is	O
the	O
one	O
that	O
has	O
the	O
least	O
to	O
gain	O
.	O
in	O
fact	O
,	O
the	O
decision	O
tree	O
construction	O
is	O
essentially	O
building	O
meta	O
features	B
for	O
you	O
.	O
(	O
or	O
,	O
at	O
least	O
,	O
it	O
is	O
building	O
meta	O
features	B
constructed	O
purely	O
through	O
“	O
logical	O
ands.	O
”	O
)	O
this	O
observation	O
leads	O
to	O
a	O
heuristic	O
for	O
constructing	O
meta	O
features	B
for	O
perceptrons	O
from	O
decision	B
trees	I
.	O
the	O
idea	O
is	O
to	O
train	O
a	O
decision	B
tree	I
on	O
the	O
training	O
data	O
.	O
from	O
that	O
decision	B
tree	I
,	O
you	O
can	O
extract	O
meta	O
features	B
by	O
looking	O
at	O
feature	B
combinations	I
along	O
branches	O
.	O
you	O
can	O
then	O
add	O
only	O
those	O
feature	B
combinations	I
as	O
meta	O
features	B
to	O
the	O
feature	O
set	O
for	O
the	O
perceptron	O
.	O
figure	O
4.12	O
shows	O
a	O
small	O
decision	B
tree	I
and	O
a	O
set	O
of	O
meta	O
features	B
that	O
you	O
might	O
extract	O
from	O
it	O
.	O
there	O
is	O
a	O
hyperparameter	B
here	O
of	O
what	O
length	O
paths	O
to	O
extract	O
from	O
the	O
tree	O
:	O
in	O
this	O
case	O
,	O
only	O
paths	O
of	O
length	O
two	O
are	O
extracted	O
.	O
for	O
bigger	O
trees	O
,	O
or	O
if	O
you	O
have	O
more	O
data	O
,	O
you	O
might	O
beneﬁt	O
from	O
longer	O
paths	O
.	O
in	O
addition	O
to	O
combinatorial	O
transformations	O
,	O
the	O
logarithmic	O
transformation	O
can	O
be	O
quite	O
useful	O
in	O
practice	O
.	O
it	O
seems	O
like	O
a	O
strange	O
thing	O
to	O
be	O
useful	O
,	O
since	O
it	O
doesn	O
’	O
t	O
seem	O
to	O
fundamentally	O
change	O
the	O
data	O
.	O
however	O
,	O
since	O
many	O
learning	O
algorithms	O
operate	O
by	O
linear	O
operations	O
on	O
the	O
features	O
(	O
both	O
perceptron	B
and	O
knn	O
do	O
this	O
)	O
,	O
the	O
log-transform	O
is	O
a	O
way	O
to	O
get	O
product-like	O
operations	O
.	O
the	O
question	O
is	O
which	O
of	O
the	O
following	O
feels	O
more	O
applicable	O
to	O
your	O
data	O
:	O
(	O
1	O
)	O
every	O
time	O
this	O
feature	O
increases	O
by	O
one	O
,	O
i	O
’	O
m	O
equally	O
more	O
likely	O
to	O
predict	B
a	O
positive	O
label	O
;	O
(	O
2	O
)	O
every	O
time	O
this	O
feature	O
doubles	O
,	O
i	O
’	O
m	O
equally	O
more	O
like	O
to	O
predict	B
a	O
positive	O
label	O
.	O
in	O
the	O
ﬁrst	O
case	O
,	O
you	O
should	O
stick	O
with	O
linear	O
features	O
and	O
in	O
the	O
second	O
case	O
you	O
should	O
switch	O
to	O
a	O
log-transform	O
.	O
this	O
is	O
an	O
important	O
transformation	O
in	O
text	O
data	O
,	O
where	O
the	O
presence	O
of	O
the	O
word	O
“	O
excellent	O
”	O
once	O
is	O
a	O
good	O
indicator	O
of	O
a	O
positive	O
review	O
;	O
seeing	O
“	O
excellent	O
”	O
twice	O
is	O
a	O
better	O
indicator	O
;	O
but	O
the	O
difference	O
between	O
seeing	O
“	O
excellent	O
”	O
10	O
times	O
and	O
seeing	O
it	O
11	O
times	O
really	O
isn	O
’	O
t	O
a	O
big	O
deal	O
any	O
more	O
.	O
a	O
log-transform	O
achieves	O
60	O
a	O
course	O
in	O
machine	O
learning	O
this	O
.	O
experimentally	O
,	O
you	O
can	O
see	O
the	O
difference	O
in	O
test	O
performance	O
between	O
word	O
count	O
data	O
and	O
log-word	O
count	O
data	O
in	O
figure	O
4.13.	O
here	O
,	O
the	O
transformation	O
is	O
actually	O
xd	O
(	O
cid:55	O
)	O
→	O
log2	O
(	O
xd	O
+	O
1	O
)	O
to	O
ensure	O
that	O
zeros	O
remain	O
zero	O
and	O
sparsity	O
is	O
retained	O
.	O
4.5	O
evaluating	O
model	B
performance	O
so	O
far	O
,	O
our	O
focus	O
has	O
been	O
on	O
classiﬁers	O
that	O
achieve	O
high	O
accuracy	O
.	O
in	O
some	O
cases	O
,	O
this	O
is	O
not	O
what	O
you	O
might	O
want	O
.	O
for	O
instance	O
,	O
if	O
you	O
are	O
trying	O
to	O
predict	B
whether	O
a	O
patient	O
has	O
cancer	O
or	O
not	O
,	O
it	O
might	O
be	O
better	O
to	O
err	O
on	O
one	O
side	O
(	O
saying	O
they	O
have	O
cancer	O
when	O
they	O
don	O
’	O
t	O
)	O
than	O
the	O
other	O
(	O
because	O
then	O
they	O
die	O
)	O
.	O
similarly	O
,	O
letting	O
a	O
little	O
spam	O
slip	O
through	O
might	O
be	O
better	O
than	O
accidentally	O
blocking	O
one	O
email	O
from	O
your	O
boss	O
.	O
there	O
are	O
two	O
major	O
types	O
of	O
binary	O
classiﬁcation	O
problems	O
.	O
one	O
is	O
“	O
x	O
versus	O
y.	O
”	O
for	O
instance	O
,	O
positive	O
versus	O
negative	O
sentiment	O
.	O
another	O
is	O
“	O
x	O
versus	O
not-x.	O
”	O
for	O
instance	O
,	O
spam	O
versus	O
non-spam	O
.	O
(	O
the	O
argument	O
being	O
that	O
there	O
are	O
lots	O
of	O
types	O
of	O
non-spam	O
.	O
)	O
or	O
in	O
the	O
context	O
of	O
web	O
search	O
,	O
relevant	O
document	O
versus	O
irrelevant	O
document	O
.	O
this	O
is	O
a	O
subtle	O
and	O
subjective	O
decision	O
.	O
but	O
“	O
x	O
versus	O
not-	O
x	O
”	O
problems	O
often	O
have	O
more	O
of	O
the	O
feel	O
of	O
“	O
x	O
spotting	O
”	O
rather	O
than	O
a	O
true	O
distinction	O
between	O
x	O
and	O
y	O
.	O
(	O
can	O
you	O
spot	O
the	O
spam	O
?	O
can	O
you	O
spot	O
the	O
relevant	O
documents	O
?	O
)	O
for	O
spotting	O
problems	O
(	O
x	O
versus	O
not-x	O
)	O
,	O
there	O
are	O
often	O
more	O
ap-	O
propriate	O
success	O
metrics	O
than	O
accuracy	O
.	O
a	O
very	O
popular	O
one	O
from	O
information	O
retrieval	O
is	O
the	O
precision/recall	O
metric	O
.	O
precision	B
asks	O
the	O
question	O
:	O
of	O
all	O
the	O
x	O
’	O
s	O
that	O
you	O
found	O
,	O
how	O
many	O
of	O
them	O
were	O
actually	O
x	O
’	O
s	O
?	O
recall	B
asks	O
:	O
of	O
all	O
the	O
x	O
’	O
s	O
that	O
were	O
out	O
there	O
,	O
how	O
many	O
of	O
them	O
did	O
you	O
ﬁnd	O
?	O
4	O
formally	O
,	O
precision	B
and	O
recall	B
are	O
deﬁned	O
as	O
:	O
p	O
=	O
r	O
=	O
i	O
s	O
i	O
t	O
s	O
=	O
number	O
of	O
xs	O
that	O
your	O
system	O
found	O
t	O
=	O
number	O
of	O
xs	O
in	O
the	O
data	O
i	O
=	O
number	O
of	O
correct	O
xs	O
that	O
your	O
system	O
found	O
(	O
4.8	O
)	O
(	O
4.9	O
)	O
(	O
4.10	O
)	O
(	O
4.11	O
)	O
(	O
4.12	O
)	O
here	O
,	O
s	O
is	O
mnemonic	O
for	O
“	O
system	O
,	O
”	O
t	O
is	O
mnemonic	O
for	O
“	O
truth	O
”	O
and	O
i	O
is	O
mnemonic	O
for	O
“	O
intersection.	O
”	O
it	O
is	O
generally	O
accepted	O
that	O
0/0	O
=	O
1	O
in	O
these	O
deﬁnitions	O
.	O
thus	O
,	O
if	O
you	O
system	O
found	O
nothing	O
,	O
your	O
preci-	O
sion	O
is	O
always	O
perfect	O
;	O
and	O
if	O
there	O
is	O
nothing	O
to	O
ﬁnd	O
,	O
your	O
recall	B
is	O
always	O
perfect	O
.	O
once	O
you	O
can	O
compute	O
precision	B
and	O
recall	B
,	O
you	O
are	O
often	O
able	O
to	O
produce	O
precision/recall	B
curves	I
.	O
suppose	O
that	O
you	O
are	O
attempting	O
4	O
a	O
colleague	O
make	O
the	O
analogy	O
to	O
the	O
us	O
court	O
system	O
’	O
s	O
saying	O
“	O
do	O
you	O
promise	O
to	O
tell	O
the	O
whole	O
truth	O
and	O
nothing	O
but	O
the	O
truth	O
?	O
”	O
in	O
this	O
case	O
,	O
the	O
“	O
whole	O
truth	O
”	O
means	O
high	O
recall	B
and	O
“	O
nothing	O
but	O
the	O
truth	O
”	O
means	O
high	O
precision.	O
”	O
figure	O
4.14	O
:	O
prac	O
:	O
spam	O
:	O
show	O
a	O
bunch	O
of	O
emails	O
spam/nospam	O
sorted	O
by	O
model	B
predicion	O
,	O
not	O
perfect	O
practical	O
issues	O
61	O
?	O
how	O
would	O
you	O
get	O
a	O
conﬁdence	O
out	O
of	O
a	O
decision	B
tree	I
or	O
knn	O
?	O
figure	O
4.15	O
:	O
prac	O
:	O
prcurve	O
:	O
precision	B
recall	O
curve	O
0.0	O
0.00	O
0.00	O
0.00	O
0.00	O
0.00	O
0.00	O
0.8	O
0.00	O
0.0	O
0.32	O
0.2	O
0.53	O
0.4	O
0.68	O
0.6	O
0.80	O
0.8	O
1.0	O
0.88	O
table	O
4.1	O
:	O
table	O
of	O
f-measures	O
when	O
varying	O
precision	B
and	O
recall	B
values	O
.	O
0.6	O
0.00	O
0.30	O
0.48	O
0.60	O
0.68	O
0.74	O
0.2	O
0.00	O
0.20	O
0.26	O
0.30	O
0.32	O
0.33	O
0.4	O
0.00	O
0.26	O
0.40	O
0.48	O
0.53	O
0.57	O
1.0	O
0.00	O
0.33	O
0.57	O
0.74	O
0.88	O
1.00	O
to	O
identify	O
spam	O
.	O
you	O
run	O
a	O
learning	O
algorithm	B
to	O
make	O
predictions	O
on	O
a	O
test	B
set	I
.	O
but	O
instead	O
of	O
just	O
taking	O
a	O
“	O
yes/no	O
”	O
answer	O
,	O
you	O
allow	O
your	O
algorithm	B
to	O
produce	O
its	O
conﬁdence	O
.	O
for	O
instance	O
,	O
in	O
perceptron	B
,	O
you	O
might	O
use	O
the	O
distance	O
from	O
the	O
hyperplane	O
as	O
a	O
conﬁdence	O
measure	O
.	O
you	O
can	O
then	O
sort	O
all	O
of	O
your	O
test	O
emails	O
according	O
to	O
this	O
ranking	O
.	O
you	O
may	O
put	O
the	O
most	O
spam-like	O
emails	O
at	O
the	O
top	O
and	O
the	O
least	O
spam-like	O
emails	O
at	O
the	O
bottom	O
,	O
like	O
in	O
figure	O
4.14.	O
once	O
you	O
have	O
this	O
sorted	O
list	O
,	O
you	O
can	O
choose	O
how	O
aggressively	O
you	O
want	O
your	O
spam	O
ﬁlter	O
to	O
be	O
by	O
setting	O
a	O
threshold	B
anywhere	O
on	O
this	O
list	O
.	O
one	O
would	O
hope	O
that	O
if	O
you	O
set	O
the	O
threshold	O
very	O
high	O
,	O
you	O
are	O
likely	O
to	O
have	O
high	O
precision	B
(	O
but	O
low	O
recall	B
)	O
.	O
if	O
you	O
set	O
the	O
thresh-	O
old	O
very	O
low	O
,	O
you	O
’	O
ll	O
have	O
high	O
recall	B
(	O
but	O
low	O
precision	B
)	O
.	O
by	O
consider-	O
ing	O
every	O
possible	O
place	O
you	O
could	O
put	O
this	O
threshold	B
,	O
you	O
can	O
trace	O
out	O
a	O
curve	O
of	O
precision/recall	O
values	O
,	O
like	O
the	O
one	O
in	O
figure	O
4.15.	O
this	O
allows	O
us	O
to	O
ask	O
the	O
question	O
:	O
for	O
some	O
ﬁxed	O
precision	B
,	O
what	O
sort	O
of	O
recall	B
can	O
i	O
get	O
.	O
obviously	O
,	O
the	O
closer	O
your	O
curve	O
is	O
to	O
the	O
upper-right	O
corner	O
,	O
the	O
better	O
.	O
and	O
when	O
comparing	O
learning	O
algorithms	O
a	O
and	O
b	O
you	O
can	O
say	O
that	O
a	O
dominates	B
b	O
if	O
a	O
’	O
s	O
precision/recall	O
curve	O
is	O
always	O
higher	O
than	O
b	O
’	O
s	O
.	O
precision/recall	B
curves	I
are	O
nice	O
because	O
they	O
allow	O
us	O
to	O
visualize	B
many	O
ways	O
in	O
which	O
we	O
could	O
use	O
the	O
system	O
.	O
however	O
,	O
sometimes	O
we	O
like	O
to	O
have	O
a	O
single	O
number	O
that	O
informs	O
us	O
of	O
the	O
quality	O
of	O
the	O
solution	O
.	O
a	O
popular	O
way	O
of	O
combining	O
precision	B
and	O
recall	B
into	O
a	O
single	O
number	O
is	O
by	O
taking	O
their	O
harmonic	O
mean	O
.	O
this	O
is	O
known	O
as	O
the	O
balanced	O
f-measure	O
(	O
or	O
f-score	O
)	O
:	O
f	O
=	O
2×p×r	O
p	O
+	O
r	O
(	O
4.13	O
)	O
the	O
reason	O
that	O
you	O
want	O
to	O
use	O
a	O
harmonic	O
mean	O
rather	O
than	O
an	O
arithmetic	O
mean	O
(	O
the	O
one	O
you	O
’	O
re	O
more	O
used	O
to	O
)	O
is	O
that	O
it	O
favors	O
sys-	O
tems	O
that	O
achieve	O
roughly	O
equal	O
precision	B
and	O
recall	B
.	O
in	O
the	O
extreme	O
case	O
where	O
p	O
=	O
r	O
,	O
then	O
f	O
=	O
p	O
=	O
r.	O
but	O
in	O
the	O
imbalanced	O
case	O
,	O
for	O
instance	O
p	O
=	O
0.1	O
and	O
r	O
=	O
0.9	O
,	O
the	O
overall	O
f-measure	O
is	O
a	O
modest	O
0.18.	O
table	O
4.1	O
shows	O
f-measures	O
as	O
a	O
function	O
of	O
precision	B
and	O
recall	B
,	O
so	O
that	O
you	O
can	O
see	O
how	O
important	O
it	O
is	O
to	O
get	O
balanced	O
values	O
.	O
in	O
some	O
cases	O
,	O
you	O
might	O
believe	O
that	O
precision	B
is	O
more	O
impor-	O
tant	O
than	O
recall	B
.	O
this	O
idea	O
leads	O
to	O
the	O
weighted	O
f-measure	O
,	O
which	O
is	O
parameterized	O
by	O
a	O
weight	O
β	O
∈	O
[	O
0	O
,	O
∞	O
)	O
(	O
beta	O
)	O
:	O
fβ	O
=	O
(	O
1	O
+	O
β2	O
)	O
×p×r	O
β2×p	O
+	O
r	O
(	O
4.14	O
)	O
for	O
β	O
=	O
1	O
,	O
this	O
reduces	O
to	O
the	O
standard	O
f-measure	O
.	O
for	O
β	O
=	O
0	O
,	O
it	O
focuses	O
entirely	O
on	O
recall	B
and	O
for	O
β	O
→	O
∞	O
it	O
focuses	O
entirely	O
on	O
preci-	O
sion	O
.	O
the	O
interpretation	O
of	O
the	O
weight	O
is	O
that	O
fβ	O
measures	O
the	O
perfor-	O
62	O
a	O
course	O
in	O
machine	O
learning	O
mance	O
for	O
a	O
user	O
who	O
cares	O
β	O
times	O
as	O
much	O
about	O
precision	B
as	O
about	O
recall	B
.	O
one	O
thing	O
to	O
keep	O
in	O
mind	O
is	O
that	O
precision	B
and	O
recall	B
(	O
and	O
hence	O
f-measure	O
)	O
depend	O
crucially	O
on	O
which	O
class	O
is	O
considered	O
the	O
thing	O
you	O
wish	O
to	O
ﬁnd	O
.	O
in	O
particular	O
,	O
if	O
you	O
take	O
a	O
binary	O
data	O
set	O
if	O
ﬂip	O
what	O
it	O
means	O
to	O
be	O
a	O
positive	O
or	O
negative	O
example	O
,	O
you	O
will	O
end	O
up	O
with	O
completely	O
difference	O
precision	B
and	O
recall	B
values	O
.	O
it	O
is	O
not	O
the	O
case	O
that	O
precision	B
on	O
the	O
ﬂipped	O
task	O
is	O
equal	O
to	O
recall	B
on	O
the	O
original	O
task	O
(	O
nor	O
vice	O
versa	O
)	O
.	O
consequently	O
,	O
f-measure	O
is	O
also	O
not	O
the	O
same	O
.	O
for	O
some	O
tasks	O
where	O
people	O
are	O
less	O
sure	O
about	O
what	O
they	O
want	O
,	O
they	O
will	O
occasionally	O
report	O
two	O
sets	O
of	O
precision/recall/f-	O
measure	O
numbers	O
,	O
which	O
vary	O
based	O
on	O
which	O
class	O
is	O
considered	O
the	O
thing	O
to	O
spot	O
.	O
there	O
are	O
other	O
standard	O
metrics	O
that	O
are	O
used	O
in	O
different	O
com-	O
munities	O
.	O
for	O
instance	O
,	O
the	O
medical	O
community	O
is	O
fond	O
of	O
the	O
sensi-	O
tivity/speciﬁcity	O
metric	O
.	O
a	O
sensitive	O
classiﬁer	O
is	O
one	O
which	O
almost	O
always	O
ﬁnds	O
everything	O
it	O
is	O
looking	O
for	O
:	O
it	O
has	O
high	O
recall	B
.	O
in	O
fact	O
,	O
sensitivity	B
is	O
exactly	O
the	O
same	O
as	O
recall	B
.	O
a	O
speciﬁc	O
classiﬁer	O
is	O
one	O
which	O
does	O
a	O
good	O
job	O
not	O
ﬁnding	O
the	O
things	O
that	O
it	O
doesn	O
’	O
t	O
want	O
to	O
ﬁnd	O
.	O
speciﬁcity	O
is	O
precision	B
on	O
the	O
negation	O
of	O
the	O
task	O
at	O
hand	O
.	O
you	O
can	O
compute	O
curves	O
for	O
sensitivity	B
and	O
speciﬁcity	O
much	O
like	O
those	O
for	O
precision	B
and	O
recall	B
.	O
the	O
typical	O
plot	O
,	O
referred	O
to	O
as	O
the	O
re-	O
ceiver	O
operating	O
characteristic	O
(	O
or	O
roc	O
curve	O
)	O
plots	O
the	O
sensitivity	O
against	O
1	O
−	O
speciﬁcity	O
.	O
given	O
an	O
roc	O
curve	O
,	O
you	O
can	O
compute	O
the	O
area	O
under	O
the	O
curve	O
(	O
or	O
auc	O
)	O
metric	O
,	O
which	O
also	O
provides	O
a	O
mean-	O
ingful	O
single	O
number	O
for	O
a	O
system	O
’	O
s	O
performance	O
.	O
unlike	O
f-measures	O
,	O
which	O
tend	O
to	O
be	O
low	O
because	O
the	O
require	O
agreement	O
,	O
auc	O
scores	O
tend	O
to	O
be	O
very	O
high	O
,	O
even	O
for	O
not	O
great	O
systems	O
.	O
this	O
is	O
because	O
ran-	O
dom	O
chance	O
will	O
give	O
you	O
an	O
auc	O
of	O
0.5	O
and	O
the	O
best	O
possible	O
auc	O
is	O
1.0.	O
the	O
main	O
message	O
for	O
evaluation	O
metrics	O
is	O
that	O
you	O
should	O
choose	O
whichever	O
one	O
makes	O
the	O
most	O
sense	O
.	O
in	O
many	O
cases	O
,	O
several	O
might	O
make	O
sense	O
.	O
in	O
that	O
case	O
,	O
you	O
should	O
do	O
whatever	O
is	O
more	O
commonly	O
done	O
in	O
your	O
ﬁeld	O
.	O
there	O
is	O
no	O
reason	O
to	O
be	O
an	O
outlier	O
without	O
cause	O
.	O
4.6	O
cross	B
validation	I
in	O
chapter	O
1	O
,	O
you	O
learned	O
about	O
using	O
development	B
data	I
(	O
or	O
held-out	B
data	I
)	O
to	O
set	O
hyperparameters	O
.	O
the	O
main	O
disadvantage	O
to	O
the	O
develop-	O
ment	O
data	O
approach	O
is	O
that	O
you	O
throw	O
out	O
some	O
of	O
your	O
training	B
data	I
,	O
just	O
for	O
estimating	O
one	O
or	O
two	O
hyperparameters	O
.	O
an	O
alternative	O
is	O
the	O
idea	O
of	O
cross	B
validation	I
.	O
in	O
cross	B
validation	I
,	O
you	O
break	O
your	O
training	B
data	I
up	O
into	O
10	O
equally-sized	O
partitions	O
.	O
you	O
train	O
a	O
learning	O
algorithm	B
on	O
9	O
of	O
them	O
and	O
test	O
it	O
on	O
the	O
remaining	O
practical	O
issues	O
63	O
algorithm	B
8	O
crossvalidate	O
(	O
learningalgorithm	O
,	O
data	O
,	O
k	O
)	O
1	O
:	O
ˆ	O
←	O
∞	O
2	O
:	O
ˆα	O
←	O
unknown	O
3	O
:	O
for	O
all	O
hyperparameter	O
settings	O
α	O
do	O
4	O
:	O
//	O
store	O
lowest	O
error	O
encountered	O
so	O
far	O
//	O
store	O
the	O
hyperparameter	O
setting	O
that	O
yielded	O
it	O
//	O
keep	O
track	O
of	O
the	O
k-many	O
error	O
estimates	O
err	O
←	O
[	O
]	O
for	O
k	O
=	O
1	O
to	O
k	O
do	O
5	O
:	O
6	O
:	O
7	O
:	O
8	O
:	O
9	O
:	O
10	O
:	O
11	O
:	O
12	O
:	O
13	O
:	O
14	O
:	O
train	O
←	O
{	O
(	O
xn	O
,	O
yn	O
)	O
∈	O
data	O
:	O
n	O
mod	O
k	O
(	O
cid:54	O
)	O
=	O
k	O
−	O
1	O
}	O
test	O
←	O
{	O
(	O
xn	O
,	O
yn	O
)	O
∈	O
data	O
:	O
n	O
mod	O
k	O
=	O
k	O
−	O
1	O
}	O
//	O
test	O
every	O
kth	O
example	O
model	O
←	O
run	O
learningalgorithm	O
on	O
train	O
err	O
←	O
err	O
⊕	O
error	O
of	O
model	B
on	O
test	O
//	O
add	O
current	O
error	O
to	O
list	O
of	O
errors	O
end	O
for	O
avgerr	O
←	O
mean	O
of	O
set	O
err	O
if	O
avgerr	O
<	O
ˆ	O
then	O
ˆ	O
←	O
avgerr	O
ˆα	O
←	O
α	O
//	O
remember	O
these	O
settings	O
//	O
because	O
they	O
’	O
re	O
the	O
best	O
so	O
far	O
end	O
if	O
15	O
:	O
16	O
:	O
end	O
for	O
1.	O
you	O
do	O
this	O
10	O
times	O
,	O
each	O
time	O
holding	O
out	O
a	O
different	O
partition	O
as	O
the	O
“	O
development	O
”	O
part	O
.	O
you	O
can	O
then	O
average	O
your	O
performance	O
over	O
all	O
ten	O
parts	O
to	O
get	O
an	O
estimate	O
of	O
how	O
well	O
your	O
model	B
will	O
perform	O
in	O
the	O
future	O
.	O
you	O
can	O
repeat	O
this	O
process	O
for	O
every	O
possible	O
choice	O
of	O
hyperparameters	O
to	O
get	O
an	O
estimate	O
of	O
which	O
one	O
performs	O
best	O
.	O
the	O
general	O
k-fold	O
cross	B
validation	I
technique	O
is	O
shown	O
in	O
algorithm	B
4.6	O
,	O
where	O
k	O
=	O
10	O
in	O
the	O
preceeding	O
discussion	O
.	O
in	O
fact	O
,	O
the	O
development	O
data	O
approach	O
can	O
be	O
seen	O
as	O
an	O
approxi-	O
mation	O
to	O
cross	B
validation	I
,	O
wherein	O
only	O
one	O
of	O
the	O
k	O
loops	O
(	O
line	O
5	O
in	O
algorithm	B
4.6	O
)	O
is	O
executed	O
.	O
typical	O
choices	O
for	O
k	O
are	O
2	O
,	O
5	O
,	O
10	O
and	O
n	O
−	O
1.	O
by	O
far	O
the	O
most	O
com-	O
mon	O
is	O
k	O
=	O
10	O
:	O
10-fold	O
cross	B
validation	I
.	O
sometimes	O
5	O
is	O
used	O
for	O
efﬁciency	O
reasons	O
.	O
and	O
sometimes	O
2	O
is	O
used	O
for	O
subtle	O
statistical	O
rea-	O
sons	O
,	O
but	O
that	O
is	O
quite	O
rare	O
.	O
in	O
the	O
case	O
that	O
k	O
=	O
n	O
−	O
1	O
,	O
this	O
is	O
known	O
as	O
leave-one-out	B
cross	I
validation	I
(	O
or	O
abbreviated	O
as	O
loo	O
cross	O
val-	O
idation	O
)	O
.	O
after	O
running	O
cross	B
validation	I
,	O
you	O
have	O
two	O
choices	O
.	O
you	O
can	O
either	O
select	O
one	O
of	O
the	O
k	O
trained	O
models	O
as	O
your	O
ﬁnal	O
model	B
to	O
make	O
predictions	O
with	O
,	O
or	O
you	O
can	O
train	O
a	O
new	O
model	B
on	O
all	O
of	O
the	O
data	O
,	O
using	O
the	O
hyperparameters	O
selected	O
by	O
cross-validation	O
.	O
if	O
you	O
have	O
the	O
time	O
,	O
the	O
latter	O
is	O
probably	O
a	O
better	O
options	O
.	O
it	O
may	O
seem	O
that	O
loo	O
cross	B
validation	I
is	O
prohibitively	O
expensive	O
to	O
run	O
.	O
this	O
is	O
true	O
for	O
most	O
learning	O
algorithms	O
except	O
for	O
k-nearest	O
neighbors	O
.	O
for	O
knn	O
,	O
leave-one-out	O
is	O
actually	O
very	O
natural	O
.	O
we	O
loop	O
through	O
each	O
training	O
point	O
and	O
ask	O
ourselves	O
whether	O
this	O
example	O
would	O
be	O
correctly	O
classiﬁed	O
for	O
all	O
different	O
possible	O
values	O
of	O
k.	O
this	O
requires	O
only	O
as	O
much	O
computation	O
as	O
computing	O
the	O
k	O
nearest	O
neighbors	O
for	O
the	O
highest	O
value	O
of	O
k.	O
this	O
is	O
such	O
a	O
popular	O
and	O
effective	O
approach	O
for	O
knn	O
classiﬁcation	O
that	O
it	O
is	O
spelled	O
out	O
in	O
64	O
a	O
course	O
in	O
machine	O
learning	O
algorithm	B
9	O
knn-train-loo	O
(	O
d	O
)	O
1	O
:	O
errk	O
←	O
0	O
,	O
∀1	O
≤	O
k	O
≤	O
n	O
−	O
1	O
2	O
:	O
for	O
n	O
=	O
1	O
to	O
n	O
do	O
3	O
:	O
sm	O
←	O
(	O
cid:104	O
)	O
||xn	O
−	O
xm||	O
,	O
m	O
(	O
cid:105	O
)	O
,	O
∀m	O
(	O
cid:54	O
)	O
=	O
n	O
s	O
←	O
sort	O
(	O
s	O
)	O
ˆy	O
←	O
0	O
for	O
k	O
=	O
1	O
to	O
n	O
−	O
1	O
do	O
(	O
cid:104	O
)	O
dist	O
,	O
m	O
(	O
cid:105	O
)	O
←	O
sk	O
ˆy	O
←	O
ˆy	O
+	O
ym	O
if	O
ˆy	O
(	O
cid:54	O
)	O
=	O
ym	O
then	O
errk	O
←	O
errk	O
+	O
1	O
4	O
:	O
5	O
:	O
6	O
:	O
7	O
:	O
8	O
:	O
9	O
:	O
10	O
:	O
11	O
:	O
end	O
if	O
end	O
for	O
12	O
:	O
13	O
:	O
end	O
for	O
14	O
:	O
return	O
argmink	O
errk	O
//	O
errk	O
stores	O
how	O
well	O
you	O
do	O
with	O
knn	O
//	O
compute	O
distances	O
to	O
other	O
points	O
//	O
put	O
lowest-distance	O
objects	O
ﬁrst	O
//	O
current	O
label	B
prediction	O
//	O
let	O
kth	O
closest	O
point	O
vote	B
//	O
one	O
more	O
error	O
for	O
knn	O
//	O
return	O
the	O
k	O
that	O
achieved	O
lowest	O
error	O
algorithm	O
?	O
?	O
.	O
overall	O
,	O
the	O
main	O
advantage	O
to	O
cross	B
validation	I
over	O
develop-	O
ment	O
data	O
is	O
robustness	O
.	O
the	O
main	O
advantage	O
of	O
development	B
data	I
is	O
speed	O
.	O
one	O
warning	O
to	O
keep	O
in	O
mind	O
is	O
that	O
the	O
goal	O
of	O
both	O
cross	O
valida-	O
tion	O
and	O
development	B
data	I
is	O
to	O
estimate	O
how	O
well	O
you	O
will	O
do	O
in	O
the	O
future	O
.	O
this	O
is	O
a	O
question	O
of	O
statistics	O
,	O
and	O
holds	O
only	O
if	O
your	O
test	B
data	I
really	O
looks	O
like	O
your	O
training	B
data	I
.	O
that	O
is	O
,	O
it	O
is	O
drawn	O
from	O
the	O
same	O
distribution	O
.	O
in	O
many	O
practical	O
cases	O
,	O
this	O
is	O
not	O
entirely	O
true	O
.	O
for	O
example	O
,	O
in	O
person	O
identiﬁcation	O
,	O
we	O
might	O
try	O
to	O
classify	O
every	O
pixel	O
in	O
an	O
image	O
based	O
on	O
whether	O
it	O
contains	O
a	O
person	O
or	O
not	O
.	O
if	O
we	O
have	O
100	O
training	O
images	O
,	O
each	O
with	O
10	O
,	O
000	O
pixels	O
,	O
then	O
we	O
have	O
a	O
total	O
of	O
1m	O
training	O
examples	O
.	O
the	O
classiﬁcation	O
for	O
a	O
pixel	O
in	O
image	O
5	O
is	O
highly	O
dependent	O
on	O
the	O
classiﬁcation	O
for	O
a	O
neighboring	O
pixel	O
in	O
the	O
same	O
image	O
.	O
so	O
if	O
one	O
of	O
those	O
pixels	O
happens	O
to	O
fall	O
in	O
training	B
data	I
,	O
and	O
the	O
other	O
in	O
development	O
(	O
or	O
cross	B
validation	I
)	O
data	O
,	O
your	O
model	B
will	O
do	O
unreasonably	O
well	O
.	O
in	O
this	O
case	O
,	O
it	O
is	O
important	O
that	O
when	O
you	O
cross	O
validate	O
(	O
or	O
use	O
development	B
data	I
)	O
,	O
you	O
do	O
so	O
over	O
images	O
,	O
not	O
over	O
pixels	O
.	O
the	O
same	O
goes	O
for	O
text	O
problems	O
where	O
you	O
sometimes	O
want	O
to	O
classify	O
things	O
at	O
a	O
word	O
level	O
,	O
but	O
are	O
handed	O
a	O
collection	O
of	O
documents	O
.	O
the	O
important	O
thing	O
to	O
keep	O
in	O
mind	O
is	O
that	O
it	O
is	O
the	O
images	O
(	O
or	O
documents	O
)	O
that	O
are	O
drawn	O
independently	B
from	O
your	O
data	O
distribution	O
and	O
not	O
the	O
pixels	O
(	O
or	O
words	O
)	O
,	O
which	O
are	O
drawn	O
dependently	O
.	O
practical	O
issues	O
65	O
4.7	O
hypothesis	B
testing	I
and	O
statistical	O
signiﬁcance	O
vignette	O
:	O
the	O
lady	O
drinking	O
tea	O
story	O
suppose	O
that	O
you	O
’	O
ve	O
presented	O
a	O
machine	O
learning	O
solution	O
to	O
your	O
boss	O
that	O
achieves	O
7	O
%	O
error	O
on	O
cross	B
validation	I
.	O
your	O
nemesis	O
,	O
gabe	O
,	O
gives	O
a	O
solution	O
to	O
your	O
boss	O
that	O
achieves	O
6.9	O
%	O
error	O
on	O
cross	O
vali-	O
dation	O
.	O
how	O
impressed	O
should	O
your	O
boss	O
be	O
?	O
it	O
depends	O
.	O
if	O
this	O
0.1	O
%	O
improvement	O
was	O
measured	O
over	O
1000	O
examples	B
,	O
perhaps	O
not	O
too	O
impressed	O
.	O
it	O
would	O
mean	O
that	O
gabe	O
got	O
exactly	O
one	O
more	O
example	O
right	O
than	O
you	O
did	O
.	O
(	O
in	O
fact	O
,	O
he	O
probably	O
got	O
15	O
more	O
right	O
and	O
14	O
more	O
wrong	O
.	O
)	O
if	O
this	O
0.1	O
%	O
impressed	O
was	O
measured	O
over	O
1	O
,	O
000	O
,	O
000	O
examples	B
,	O
perhaps	O
this	O
is	O
more	O
impressive	O
.	O
this	O
is	O
one	O
of	O
the	O
most	O
fundamental	O
questions	O
in	O
statistics	O
.	O
you	O
have	O
a	O
scientiﬁc	O
hypothesis	B
of	O
the	O
form	O
“	O
gabe	O
’	O
s	O
algorithm	B
is	O
better	O
than	O
mine.	O
”	O
you	O
wish	O
to	O
test	O
whether	O
this	O
hypothesis	B
is	O
true	O
.	O
you	O
are	O
testing	O
it	O
against	O
the	O
null	O
hypothesis	B
,	O
which	O
is	O
that	O
gabe	O
’	O
s	O
algo-	O
rithm	O
is	O
no	O
better	O
than	O
yours	O
.	O
you	O
’	O
ve	O
collected	O
data	O
(	O
either	O
1000	O
or	O
1m	O
data	O
points	O
)	O
to	O
measure	O
the	O
strength	O
of	O
this	O
hypothesis	B
.	O
you	O
want	O
to	O
ensure	O
that	O
the	O
difference	O
in	O
performance	O
of	O
these	O
two	O
algorithms	O
is	O
statistically	O
significant	O
:	O
i.e.	O
,	O
is	O
probably	O
not	O
just	O
due	O
to	O
random	O
luck	O
.	O
(	O
a	O
more	O
common	O
question	O
statisticians	O
ask	O
is	O
whether	O
one	O
drug	O
treatment	O
is	O
better	O
than	O
another	O
,	O
where	O
“	O
another	O
”	O
is	O
either	O
a	O
placebo	O
or	O
the	O
competitor	O
’	O
s	O
drug	O
.	O
)	O
there	O
are	O
about	O
∞-many	O
ways	O
of	O
doing	O
hypothesis	B
testing	I
.	O
like	O
evaluation	O
metrics	O
and	O
the	O
number	O
of	O
folds	O
of	O
cross	B
validation	I
,	O
this	O
is	O
something	O
that	O
is	O
very	O
discipline	O
speciﬁc	O
.	O
here	O
,	O
we	O
will	O
discuss	O
two	O
popular	O
tests	O
:	O
the	O
paired	O
t-test	B
and	O
bootstrapping	B
.	O
these	O
tests	O
,	O
and	O
other	O
statistical	O
tests	O
,	O
have	O
underlying	O
assumptions	O
(	O
for	O
instance	O
,	O
as-	O
sumptions	O
about	O
the	O
distribution	O
of	O
observations	O
)	O
and	O
strengths	O
(	O
for	O
instance	O
,	O
small	O
or	O
large	O
samples	O
)	O
.	O
in	O
most	O
cases	O
,	O
the	O
goal	O
of	O
hypoth-	O
esis	O
testing	O
is	O
to	O
compute	O
a	O
p-value	B
:	O
namely	O
,	O
the	O
probability	O
that	O
the	O
observed	O
difference	O
in	O
performance	O
was	O
by	O
chance	O
.	O
the	O
standard	O
way	O
of	O
reporting	O
results	O
is	O
to	O
say	O
something	O
like	O
“	O
there	O
is	O
a	O
95	O
%	O
chance	O
that	O
this	O
difference	O
was	O
not	O
by	O
chance.	O
”	O
the	O
value	O
95	O
%	O
is	O
arbitrary	O
,	O
and	O
occasionally	O
people	O
use	O
weaker	O
(	O
90	O
%	O
)	O
test	O
or	O
stronger	O
(	O
99.5	O
%	O
)	O
tests	O
.	O
the	O
t-test	O
is	O
an	O
example	O
of	O
a	O
parametric	B
test	I
.	O
it	O
is	O
applicable	O
when	O
the	O
null	O
hypothesis	B
states	O
that	O
the	O
difference	O
between	O
two	O
responses	O
has	O
mean	O
zero	O
and	O
unknown	O
variance	B
.	O
the	O
t-test	O
actually	O
assumes	O
that	O
data	O
is	O
distributed	O
according	O
to	O
a	O
gaussian	O
distribution	O
,	O
which	O
is	O
66	O
a	O
course	O
in	O
machine	O
learning	O
probably	O
not	O
true	O
of	O
binary	O
responses	O
.	O
fortunately	O
,	O
for	O
large	O
samples	O
(	O
at	O
least	O
a	O
few	O
hundred	O
)	O
,	O
binary	O
seamples	O
are	O
well	O
approximated	O
by	O
a	O
gaussian	O
distribution	O
.	O
so	O
long	O
as	O
your	O
sample	O
is	O
sufﬁciently	O
large	O
,	O
the	O
t-test	O
is	O
reasonable	O
either	O
for	O
regression	O
or	O
classiﬁcation	O
problems	O
.	O
suppose	O
that	O
you	O
evaluate	O
two	O
algorithm	B
on	O
n-many	O
examples	B
.	O
on	O
each	O
example	O
,	O
you	O
can	O
compute	O
whether	O
the	O
algorithm	O
made	O
the	O
correct	O
prediction	O
.	O
let	O
a1	O
,	O
.	O
.	O
.	O
,	O
an	O
denote	O
the	O
error	O
of	O
the	O
ﬁrst	O
algorithm	B
on	O
each	O
example	O
.	O
let	O
b1	O
,	O
.	O
.	O
.	O
,	O
bn	O
denote	O
the	O
error	O
of	O
the	O
second	O
algorithm	B
.	O
you	O
can	O
compute	O
µa	O
and	O
µb	O
as	O
the	O
means	O
of	O
a	O
and	O
b	O
,	O
respecitively	O
.	O
finally	O
,	O
center	O
the	O
data	O
as	O
ˆa	O
=	O
a	O
−	O
µa	O
and	O
ˆb	O
=	O
b	O
−	O
µb	O
.	O
the	O
t-statistic	O
is	O
deﬁned	O
as	O
:	O
(	O
cid:115	O
)	O
t	O
=	O
(	O
µa	O
−	O
µb	O
)	O
n	O
(	O
n	O
−	O
1	O
)	O
∑n	O
(	O
ˆan	O
−	O
ˆbn	O
)	O
2	O
(	O
4.15	O
)	O
t	O
≥	O
1.28	O
≥	O
1.64	O
≥	O
1.96	O
≥	O
2.58	O
signiﬁcance	O
90.0	O
%	O
95.0	O
%	O
97.5	O
%	O
99.5	O
%	O
table	O
4.2	O
:	O
table	O
of	O
signiﬁcance	O
values	O
for	O
the	O
t-test	O
.	O
after	O
computing	O
the	O
t-value	O
,	O
you	O
can	O
compare	O
it	O
to	O
a	O
list	O
of	O
values	O
for	O
computing	O
conﬁdence	O
intervals	O
.	O
assuming	O
you	O
have	O
a	O
lot	O
of	O
data	O
(	O
n	O
is	O
a	O
few	O
hundred	O
or	O
more	O
)	O
,	O
then	O
you	O
can	O
compare	O
your	O
t-value	O
to	O
table	O
4.2	O
to	O
determine	O
the	O
signiﬁcance	O
level	O
of	O
the	O
difference	O
.	O
one	O
disadvantage	O
to	O
the	O
t-test	O
is	O
that	O
it	O
can	O
not	O
easily	O
be	O
applied	O
to	O
evaluation	O
metrics	O
like	O
f-score	O
.	O
this	O
is	O
because	O
f-score	O
is	O
a	O
com-	O
puted	O
over	O
an	O
entire	O
test	B
set	I
and	O
does	O
not	O
decompose	O
into	O
a	O
set	O
of	O
individual	O
errors	O
.	O
this	O
means	O
that	O
the	O
t-test	O
can	O
not	O
be	O
applied	O
.	O
fortunately	O
,	O
cross	B
validation	I
gives	O
you	O
a	O
way	O
around	O
this	O
problem	O
.	O
?	O
what	O
does	O
it	O
mean	O
for	O
the	O
means	O
µa	O
and	O
µb	O
to	O
become	O
further	O
apart	O
?	O
how	O
does	O
this	O
affect	O
the	O
t-value	O
?	O
what	O
happens	O
if	O
the	O
variance	O
of	O
a	O
increases	O
?	O
when	O
you	O
do	O
k-fold	O
cross	B
validation	I
,	O
you	O
are	O
able	O
to	O
compute	O
k	O
error	O
metrics	O
over	O
the	O
same	O
data	O
.	O
for	O
example	O
,	O
you	O
might	O
run	O
5-fold	O
cross	B
validation	I
and	O
compute	O
f-score	O
for	O
every	O
fold	O
.	O
perhaps	O
the	O
f-	O
scores	O
are	O
92.4	O
,	O
93.9	O
,	O
96.1	O
,	O
92.2	O
and	O
94.4.	O
this	O
gives	O
you	O
an	O
average	O
f-score	O
of	O
93.8	O
over	O
the	O
5	O
folds	O
.	O
the	O
standard	O
deviation	O
of	O
this	O
set	O
of	O
f-scores	O
is	O
:	O
(	O
cid:115	O
)	O
(	O
cid:114	O
)	O
1	O
=	O
4	O
=	O
1.595	O
σ	O
=	O
1	O
n	O
−	O
1	O
(	O
ai	O
−	O
µ	O
)	O
2	O
∑	O
n	O
(	O
1.96	O
+	O
0.01	O
+	O
5.29	O
+	O
2.56	O
+	O
0.36	O
)	O
(	O
4.16	O
)	O
(	O
4.17	O
)	O
(	O
4.18	O
)	O
you	O
can	O
now	O
assume	O
that	O
the	O
distribution	O
of	O
scores	O
is	O
approximately	O
gaussian	O
.	O
if	O
this	O
is	O
true	O
,	O
then	O
approximately	O
70	O
%	O
of	O
the	O
proba-	O
bility	O
mass	O
lies	O
in	O
the	O
range	O
[	O
µ	O
−	O
σ	O
,	O
µ	O
+	O
σ	O
]	O
;	O
95	O
%	O
lies	O
in	O
the	O
range	O
[	O
µ	O
−	O
2σ	O
,	O
µ	O
+	O
2σ	O
]	O
;	O
and	O
99.5	O
%	O
lies	O
in	O
the	O
range	O
[	O
µ	O
−	O
3σ	O
,	O
µ	O
+	O
3σ	O
]	O
.	O
so	O
,	O
if	O
we	O
were	O
comparing	O
our	O
algorithm	B
against	O
one	O
whose	O
average	O
f-score	O
was	O
90.6	O
%	O
,	O
we	O
could	O
be	O
95	O
%	O
certain	O
that	O
our	O
superior	O
performance	O
was	O
not	O
due	O
to	O
chance.5	O
warning	O
:	O
a	O
conﬁdence	O
of	O
95	O
%	O
does	O
not	O
mean	O
“	O
there	O
is	O
a	O
95	O
%	O
chance	O
that	O
i	O
am	O
better.	O
”	O
all	O
it	O
means	O
is	O
that	O
if	O
i	O
reran	O
the	O
same	O
ex-	O
5	O
had	O
we	O
run	O
10-fold	O
cross	B
validation	I
we	O
might	O
be	O
been	O
able	O
to	O
get	O
tighter	O
conﬁdence	O
intervals	O
.	O
practical	O
issues	O
67	O
algorithm	B
10	O
bootstrapevaluate	O
(	O
y	O
,	O
ˆy	O
,	O
numfolds	O
)	O
1	O
:	O
scores	O
←	O
[	O
]	O
2	O
:	O
for	O
k	O
=	O
1	O
to	O
numfolds	O
do	O
truth	O
←	O
[	O
]	O
3	O
:	O
pred	O
←	O
[	O
]	O
for	O
n	O
=	O
1	O
to	O
n	O
do	O
4	O
:	O
m	O
←	O
uniform	O
random	O
value	O
from	O
1	O
to	O
n	O
truth	O
←	O
truth	O
⊕	O
ym	O
pred	O
←	O
pred	O
⊕	O
ˆym	O
5	O
:	O
6	O
:	O
7	O
:	O
8	O
:	O
9	O
:	O
//	O
list	O
of	O
values	O
we	O
want	O
to	O
predict	B
//	O
list	O
of	O
values	O
we	O
actually	O
predicted	O
//	O
sample	O
a	O
test	O
point	O
//	O
add	O
on	O
the	O
truth	O
//	O
add	O
on	O
our	O
prediction	O
//	O
evaluate	O
end	O
for	O
scores	O
←	O
scores	O
⊕	O
f-score	O
(	O
truth	O
,	O
pred	O
)	O
10	O
:	O
11	O
:	O
end	O
for	O
12	O
:	O
return	O
(	O
mean	O
(	O
scores	O
)	O
,	O
stddev	O
(	O
scores	O
)	O
)	O
periment	O
100	O
times	O
,	O
then	O
in	O
95	O
of	O
those	O
experiments	O
i	O
would	O
still	O
win	O
.	O
these	O
are	O
very	O
different	O
statements	O
.	O
if	O
you	O
say	O
the	O
ﬁrst	O
one	O
,	O
people	O
who	O
know	O
about	O
statistics	O
will	O
get	O
very	O
mad	O
at	O
you	O
!	O
one	O
disadvantage	O
to	O
cross	B
validation	I
is	O
that	O
it	O
is	O
computationally	O
expensive	O
.	O
more	O
folds	O
typically	O
leads	O
to	O
better	O
estimates	O
,	O
but	O
every	O
new	O
fold	O
requires	O
training	O
a	O
new	O
classiﬁer	O
.	O
this	O
can	O
get	O
very	O
time	O
consuming	O
.	O
the	O
technique	O
of	O
bootstrapping	B
(	O
and	O
closely	O
related	O
idea	O
of	O
jack-knifing	O
can	O
address	O
this	O
problem	O
.	O
suppose	O
that	O
you	O
didn	O
’	O
t	O
want	O
to	O
run	O
cross	B
validation	I
.	O
all	O
you	O
have	O
is	O
a	O
single	O
held-out	O
test	O
set	O
with	O
1000	O
data	O
points	O
in	O
it	O
.	O
you	O
can	O
run	O
your	O
classiﬁer	O
and	O
get	O
predictions	O
on	O
these	O
1000	O
data	O
points	O
.	O
you	O
would	O
like	O
to	O
be	O
able	O
to	O
compute	O
a	O
metric	O
like	O
f-score	O
on	O
this	O
test	B
set	I
,	O
but	O
also	O
get	O
conﬁdence	O
intervals	O
.	O
the	O
idea	O
behind	O
bootstrapping	B
is	O
that	O
this	O
set	O
of	O
1000	O
is	O
a	O
random	O
draw	O
from	O
some	O
distribution	O
.	O
we	O
would	O
like	O
to	O
get	O
multiple	O
random	O
draws	O
from	O
this	O
distribution	O
on	O
which	O
to	O
evaluate	O
.	O
we	O
can	O
simulate	O
multiple	O
draws	O
by	O
repeatedly	O
subsampling	O
from	O
these	O
1000	O
examples	B
,	O
with	O
replacement	O
.	O
to	O
perform	O
a	O
single	O
bootstrap	O
,	O
you	O
will	O
sample	O
1000	O
random	O
points	O
from	O
your	O
test	B
set	I
of	O
1000	O
random	O
points	O
.	O
this	O
sampling	O
must	O
be	O
done	O
with	O
replacement	O
(	O
so	O
that	O
the	O
same	O
example	O
can	O
be	O
sampled	O
more	O
than	O
once	O
)	O
,	O
otherwise	O
you	O
’	O
ll	O
just	O
end	O
up	O
with	O
your	O
original	O
test	B
set	I
.	O
this	O
gives	O
you	O
a	O
bootstrapped	O
sample	O
.	O
on	O
this	O
sample	O
,	O
you	O
can	O
compute	O
f-score	O
(	O
or	O
whatever	O
metric	O
you	O
want	O
)	O
.	O
you	O
then	O
do	O
this	O
99	O
more	O
times	O
,	O
to	O
get	O
a	O
100-fold	O
bootstrap	O
.	O
for	O
each	O
bootstrapped	O
sam-	O
ple	O
,	O
you	O
will	O
be	O
a	O
different	O
f-score	O
.	O
the	O
mean	O
and	O
standard	O
deviation	O
of	O
this	O
set	O
of	O
f-scores	O
can	O
be	O
used	O
to	O
estimate	O
a	O
conﬁdence	O
interval	O
for	O
your	O
algorithm	B
.	O
the	O
bootstrap	O
resampling	O
procedure	O
is	O
sketched	O
in	O
algorithm	B
4.7.	O
this	O
takes	O
three	O
arguments	O
:	O
the	O
true	O
labels	O
y	O
,	O
the	O
predicted	O
labels	O
ˆy	O
and	O
the	O
number	O
of	O
folds	O
to	O
run	O
.	O
it	O
returns	O
the	O
mean	O
and	O
standard	O
deviation	O
from	O
which	O
you	O
can	O
compute	O
a	O
conﬁdence	O
interval	O
.	O
68	O
a	O
course	O
in	O
machine	O
learning	O
4.8	O
debugging	O
learning	O
algorithms	O
learning	O
algorithms	O
are	O
notoriously	O
hard	O
to	O
debug	O
,	O
as	O
you	O
may	O
have	O
already	O
experienced	O
if	O
you	O
have	O
implemented	O
any	O
of	O
the	O
models	O
presented	O
so	O
far	O
.	O
the	O
main	O
issue	O
is	O
that	O
when	O
a	O
learning	O
algorithm	B
doesn	O
’	O
t	O
learn	O
,	O
it	O
’	O
s	O
unclear	O
if	O
this	O
is	O
because	O
there	O
’	O
s	O
a	O
bug	O
or	O
because	O
the	O
learning	O
problem	O
is	O
too	O
hard	O
(	O
or	O
there	O
’	O
s	O
too	O
much	O
noise	B
,	O
or	O
.	O
.	O
.	O
)	O
.	O
moreover	O
,	O
sometimes	O
bugs	O
lead	O
to	O
learning	O
algorithms	O
performing	O
better	O
than	O
they	O
should	O
:	O
these	O
are	O
especially	O
hard	O
to	O
catch	O
(	O
and	O
always	O
a	O
bit	O
disappointing	O
when	O
you	O
do	O
catch	O
them	O
)	O
.	O
obviously	O
if	O
you	O
have	O
a	O
reference	O
implementation	O
,	O
you	O
can	O
at-	O
tempt	O
to	O
match	O
its	O
output	O
.	O
otherwise	O
,	O
there	O
are	O
two	O
things	O
you	O
can	O
do	O
to	O
try	O
to	O
help	O
debug	O
.	O
the	O
ﬁrst	O
is	O
to	O
do	O
everything	O
in	O
your	O
power	O
to	O
get	O
the	O
learning	O
algorithm	B
to	O
overﬁt	O
.	O
if	O
it	O
can	O
not	O
at	O
least	O
overﬁt	O
the	O
training	O
data	O
,	O
there	O
’	O
s	O
deﬁnitely	O
something	O
wrong	O
.	O
the	O
second	O
is	O
to	O
feed	O
it	O
some	O
tiny	O
hand-speciﬁed	O
two	O
dimensional	O
data	O
set	O
on	O
which	O
you	O
know	O
what	O
it	O
should	O
do	O
and	O
you	O
can	O
plot	O
the	O
output	O
.	O
the	O
easiest	O
way	O
to	O
try	O
to	O
get	O
a	O
learning	O
algorithm	B
to	O
overﬁt	O
is	O
to	O
add	O
a	O
new	O
feature	O
to	O
it	O
.	O
you	O
can	O
call	O
this	O
feature	O
the	O
cheatingisfun	O
feature6	O
.	O
the	O
feature	O
value	O
associated	O
with	O
this	O
feature	O
is	O
+1	O
if	O
this	O
is	O
a	O
positive	O
example	O
and	O
−1	O
(	O
or	O
zero	O
)	O
if	O
this	O
is	O
a	O
negative	O
example	O
.	O
in	O
other	O
words	O
,	O
this	O
feature	O
is	O
a	O
perfect	O
indicator	O
of	O
the	O
class	O
of	O
this	O
example	O
.	O
if	O
you	O
add	O
the	O
cheatingisfun	O
feature	O
and	O
your	O
algorithm	B
does	O
not	O
get	O
near	O
0	O
%	O
training	B
error	I
,	O
this	O
could	O
be	O
because	O
there	O
are	O
too	O
many	O
noisy	O
features	B
confusing	O
it	O
.	O
you	O
could	O
either	O
remove	O
a	O
lot	O
of	O
the	O
other	O
features	B
,	O
or	O
make	O
the	O
feature	O
value	O
for	O
cheatingisfun	O
either	O
+100	O
or	O
−100	O
so	O
that	O
the	O
algorithm	O
really	O
looks	O
at	O
it	O
.	O
if	O
you	O
do	O
this	O
and	O
your	O
algorithm	B
still	O
can	O
not	O
overﬁt	O
then	O
you	O
likely	O
have	O
a	O
bug	O
.	O
(	O
remember	O
to	O
remove	O
the	O
cheatingisfun	O
feature	O
from	O
your	O
ﬁnal	O
implementation	O
!	O
)	O
a	O
second	O
thing	O
to	O
try	O
is	O
to	O
hand-craft	O
a	O
data	O
set	O
on	O
which	O
you	O
know	O
your	O
algorithm	B
should	O
work	O
.	O
this	O
is	O
also	O
useful	O
if	O
you	O
’	O
ve	O
man-	O
aged	O
to	O
get	O
your	O
model	B
to	O
overﬁt	O
and	O
have	O
simply	O
noticed	O
that	O
it	O
does	O
not	O
generalize	B
.	O
for	O
instance	O
,	O
you	O
could	O
run	O
knn	O
on	O
the	O
xor	O
data	O
.	O
or	O
you	O
could	O
run	O
perceptron	B
on	O
some	O
easily	O
linearly	B
separable	I
data	O
(	O
for	O
instance	O
positive	O
points	O
along	O
the	O
line	O
x2	O
=	O
x1	O
+	O
1	O
and	O
neg-	O
ative	O
points	O
along	O
the	O
line	O
x2	O
=	O
x1	O
−	O
1	O
)	O
.	O
or	O
a	O
decision	B
tree	I
on	O
nice	O
axis-aligned	O
data	O
.	O
when	O
debugging	O
on	O
hand-crafted	O
data	O
,	O
remember	O
whatever	O
you	O
know	O
about	O
the	O
models	O
you	O
are	O
considering	O
.	O
for	O
instance	O
,	O
you	O
know	O
that	O
the	O
perceptron	O
should	O
converge	O
on	O
linearly	B
separable	I
data	O
,	O
so	O
try	O
it	O
on	O
a	O
linearly	B
separable	I
data	O
set	O
.	O
you	O
know	O
that	O
decision	B
trees	I
should	O
do	O
well	O
on	O
data	O
with	O
only	O
a	O
few	O
relevant	O
features	B
,	O
so	O
make	O
6	O
note	O
:	O
cheating	O
is	O
actually	O
not	O
fun	O
and	O
you	O
shouldn	O
’	O
t	O
do	O
it	O
!	O
practical	O
issues	O
69	O
your	O
label	B
some	O
easy	O
combination	O
of	O
features	B
,	O
such	O
as	O
y	O
=	O
x1	O
∨	O
(	O
x2	O
∧	O
¬x3	O
)	O
.	O
you	O
know	O
that	O
knn	O
should	O
work	O
well	O
on	O
data	O
sets	O
where	O
the	O
classes	O
are	O
well	O
separated	O
,	O
so	O
try	O
such	O
data	O
sets	O
.	O
the	O
most	O
important	O
thing	O
to	O
keep	O
in	O
mind	O
is	O
that	O
a	O
lot	O
goes	O
in	O
to	O
getting	O
good	O
test	B
set	I
performance	O
.	O
first	O
,	O
the	O
model	O
has	O
to	O
be	O
right	O
for	O
the	O
data	O
.	O
so	O
crafting	O
your	O
own	O
data	O
is	O
helpful	O
.	O
second	O
,	O
the	O
model	O
has	O
to	O
ﬁt	O
the	O
training	O
data	O
well	O
,	O
so	O
try	O
to	O
get	O
it	O
to	O
overﬁt	O
.	O
third	O
,	O
the	O
model	O
has	O
to	O
generalize	B
,	O
so	O
make	O
sure	O
you	O
tune	O
hyperparameters	O
well	O
.	O
todo	O
:	O
answers	O
to	O
image	O
questions	O
4.9	O
exercises	O
exercise	O
4.1.	O
todo	O
.	O
.	O
.	O
figure	O
4.16	O
:	O
prac	O
:	O
imageanswers	O
:	O
object	O
recognition	O
answers	O
5	O
|	O
beyond	O
binary	O
classification	O
learning	O
objectives	O
:	O
•	O
represent	O
complex	O
prediction	O
prob-	O
lems	O
in	O
a	O
formal	O
learning	O
setting	O
.	O
•	O
be	O
able	O
to	O
artiﬁcally	O
“	O
balance	O
”	O
imbalanced	B
data	I
.	O
•	O
understand	O
the	O
positive	O
and	O
neg-	O
ative	O
aspects	O
of	O
several	O
reductions	B
from	O
multiclass	O
classiﬁcation	O
to	O
binary	O
classiﬁcation	O
.	O
•	O
recognize	O
the	O
difference	O
between	O
regression	O
and	O
ordinal	O
regression	O
.	O
•	O
implement	O
stacking	B
as	O
a	O
method	O
of	O
collective	O
classiﬁcation	O
.	O
dependencies	O
:	O
–	O
in	O
the	O
preceeding	O
chapters	O
,	O
you	O
have	O
learned	O
all	O
about	O
a	O
very	O
simple	O
form	O
of	O
prediction	O
:	O
predicting	O
bits	O
.	O
in	O
the	O
real	O
world	O
,	O
however	O
,	O
we	O
often	O
need	O
to	O
predict	B
much	O
more	O
complex	O
objects	O
.	O
you	O
may	O
need	O
to	O
categorize	O
a	O
document	O
into	O
one	O
of	O
several	O
categories	O
:	O
sports	O
,	O
en-	O
tertainment	O
,	O
news	O
,	O
politics	O
,	O
etc	O
.	O
you	O
may	O
need	O
to	O
rank	O
web	O
pages	O
or	O
ads	O
based	O
on	O
relevance	O
to	O
a	O
query	O
.	O
you	O
may	O
need	O
to	O
simultaneously	O
classify	O
a	O
collection	O
of	O
objects	O
,	O
such	O
as	O
web	O
pages	O
,	O
that	O
have	O
important	O
information	O
in	O
the	O
links	O
between	O
them	O
.	O
these	O
problems	O
are	O
all	O
com-	O
monly	O
encountered	O
,	O
yet	O
fundamentally	O
more	O
complex	O
than	O
binary	O
classiﬁcation	O
.	O
in	O
this	O
chapter	O
,	O
you	O
will	O
learn	O
how	O
to	O
use	O
everything	O
you	O
already	O
know	O
about	O
binary	O
classiﬁcation	O
to	O
solve	O
these	O
more	O
complicated	O
problems	O
.	O
you	O
will	O
see	O
that	O
it	O
’	O
s	O
relatively	O
easy	O
to	O
think	O
of	O
a	O
binary	O
classiﬁer	O
as	O
a	O
black	O
box	O
,	O
which	O
you	O
can	O
reuse	O
for	O
solving	O
these	O
more	O
complex	O
problems	O
.	O
this	O
is	O
a	O
very	O
useful	O
abstraction	O
,	O
since	O
it	O
allows	O
us	O
to	O
reuse	O
knowledge	O
,	O
rather	O
than	O
having	O
to	O
build	O
new	O
learning	O
models	O
and	O
algorithms	O
from	O
scratch	O
.	O
5.1	O
learning	O
with	O
imbalanced	B
data	I
your	O
boss	O
tells	O
you	O
to	O
build	O
a	O
classiﬁer	O
that	O
can	O
identify	O
fraudulent	O
transactions	O
in	O
credit	O
card	O
histories	O
.	O
fortunately	O
,	O
most	O
transactions	O
are	O
legitimate	O
,	O
so	O
perhaps	O
only	O
0.1	O
%	O
of	O
the	O
data	O
is	O
a	O
positive	O
in-	O
stance	O
.	O
the	O
imbalanced	O
data	O
problem	O
refers	O
to	O
the	O
fact	O
that	O
for	O
a	O
large	O
number	O
of	O
real	O
world	O
problems	O
,	O
the	O
number	O
of	O
positive	O
exam-	O
ples	O
is	O
dwarfed	O
by	O
the	O
number	O
of	O
negative	O
examples	B
(	O
or	O
vice	O
versa	O
)	O
.	O
this	O
is	O
actually	O
something	O
of	O
a	O
misnomer	O
:	O
it	O
is	O
not	O
the	O
data	O
that	O
is	O
imbalanced	O
,	O
but	O
the	O
distribution	O
from	O
which	O
the	O
data	O
is	O
drawn	O
.	O
(	O
and	O
since	O
the	O
distribution	O
is	O
imbalanced	O
,	O
so	O
must	O
the	O
data	O
be	O
.	O
)	O
imbalanced	B
data	I
is	O
a	O
problem	O
because	O
machine	O
learning	O
algo-	O
rithms	O
are	O
too	O
smart	O
for	O
your	O
own	O
good	O
.	O
for	O
most	O
learning	O
algo-	O
rithms	O
,	O
if	O
you	O
give	O
them	O
data	O
that	O
is	O
99.9	O
%	O
negative	O
and	O
0.1	O
%	O
posi-	O
tive	O
,	O
they	O
will	O
simply	O
learn	O
to	O
always	O
predict	B
negative	O
.	O
why	O
?	O
because	O
beyond	O
binary	O
classification	O
71	O
they	O
are	O
trying	O
to	O
minimize	O
error	O
,	O
and	O
they	O
can	O
achieve	O
0.1	O
%	O
error	O
by	O
doing	O
nothing	O
!	O
if	O
a	O
teacher	O
told	O
you	O
to	O
study	O
for	O
an	O
exam	O
with	O
1000	O
true/false	O
questions	O
and	O
only	O
one	O
of	O
them	O
is	O
true	O
,	O
it	O
is	O
unlikely	O
you	O
will	O
study	O
very	O
long	O
.	O
really	O
,	O
the	O
problem	O
is	O
not	O
with	O
the	O
data	O
,	O
but	O
rather	O
with	O
the	O
way	O
that	O
you	O
have	O
deﬁned	O
the	O
learning	O
problem	O
.	O
that	O
is	O
to	O
say	O
,	O
what	O
you	O
care	O
about	O
is	O
not	O
accuracy	O
:	O
you	O
care	O
about	O
something	O
else	O
.	O
if	O
you	O
want	O
a	O
learning	O
algorithm	B
to	O
do	O
a	O
reasonable	O
job	O
,	O
you	O
have	O
to	O
tell	O
it	O
what	O
you	O
want	O
!	O
most	O
likely	O
,	O
what	O
you	O
want	O
is	O
not	O
to	O
optimize	O
accuracy	O
,	O
but	O
rather	O
to	O
optimize	O
some	O
other	O
measure	O
,	O
like	O
f-score	O
or	O
auc	O
.	O
you	O
want	O
your	O
algorithm	B
to	O
make	O
some	O
positive	O
predictions	O
,	O
and	O
simply	O
prefer	O
those	O
to	O
be	O
“	O
good.	O
”	O
we	O
will	O
shortly	O
discuss	O
two	O
heuristics	O
for	O
dealing	O
with	O
this	O
problem	O
:	O
subsampling	O
and	O
weighting	O
.	O
in	O
subsampling	O
,	O
you	O
throw	O
out	O
some	O
of	O
you	O
negative	O
examples	B
so	O
that	O
you	O
are	O
left	O
with	O
a	O
bal-	O
anced	O
data	O
set	O
(	O
50	O
%	O
positive	O
,	O
50	O
%	O
negative	O
)	O
.	O
this	O
might	O
scare	O
you	O
a	O
bit	O
since	O
throwing	O
out	O
data	O
seems	O
like	O
a	O
bad	O
idea	O
,	O
but	O
at	O
least	O
it	O
makes	O
learning	O
much	O
more	O
efﬁcient	O
.	O
in	O
weighting	O
,	O
instead	O
of	O
throw-	O
ing	O
out	O
positive	O
examples	O
,	O
we	O
just	O
given	O
them	O
lower	O
weight	O
.	O
if	O
you	O
assign	O
an	O
importance	B
weight	I
of	O
0.00101	O
to	O
each	O
of	O
the	O
positive	O
ex-	O
amples	O
,	O
then	O
there	O
will	O
be	O
as	O
much	O
weight	O
associated	O
with	O
positive	O
examples	O
as	O
negative	O
examples	B
.	O
before	O
formally	O
deﬁning	O
these	O
heuristics	O
,	O
we	O
need	O
to	O
have	O
a	O
mech-	O
anism	O
for	O
formally	O
deﬁning	O
supervised	O
learning	O
problems	O
.	O
we	O
will	O
proceed	O
by	O
example	O
,	O
using	O
binary	O
classiﬁcation	O
as	O
the	O
canonical	O
learning	O
problem	O
.	O
task	O
:	O
binary	O
classification	O
given	O
:	O
1.	O
an	O
input	O
space	O
x	O
2.	O
an	O
unknown	O
distribution	O
d	O
over	O
x×	O
{	O
−1	O
,	O
+1	O
}	O
compute	O
:	O
a	O
function	O
f	O
minimizing	O
:	O
e	O
(	O
x	O
,	O
y	O
)	O
∼d	O
(	O
cid:2	O
)	O
f	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=	O
y	O
(	O
cid:3	O
)	O
as	O
in	O
all	O
the	O
binary	O
classiﬁcation	O
examples	B
you	O
’	O
ve	O
seen	O
,	O
you	O
have	O
some	O
input	O
space	O
(	O
which	O
has	O
always	O
been	O
rd	O
)	O
.	O
there	O
is	O
some	O
distri-	O
bution	O
that	O
produces	O
labeled	O
examples	B
over	O
the	O
input	O
space	O
.	O
you	O
do	O
not	O
have	O
access	O
to	O
that	O
distribution	O
,	O
but	O
can	O
obtain	O
samples	O
from	O
it	O
.	O
your	O
goal	O
is	O
to	O
ﬁnd	O
a	O
classiﬁer	O
that	O
minimizes	O
error	O
on	O
that	O
distribu-	O
tion	O
.	O
a	O
small	O
modiﬁcation	O
on	O
this	O
deﬁnition	O
gives	O
a	O
α-weighted	O
classiﬁ-	O
cation	O
problem	O
,	O
where	O
you	O
believe	O
that	O
the	O
positive	O
class	O
is	O
α-times	O
as	O
72	O
a	O
course	O
in	O
machine	O
learning	O
algorithm	B
11	O
subsamplemap	O
(	O
dweighted	O
,	O
α	O
)	O
1	O
:	O
while	O
true	O
do	O
2	O
:	O
(	O
x	O
,	O
y	O
)	O
∼	O
dweighted	O
u	O
∼	O
uniform	O
random	O
variable	O
in	O
[	O
0	O
,	O
1	O
]	O
if	O
y	O
=	O
+1	O
or	O
u	O
<	O
1	O
return	O
(	O
x	O
,	O
y	O
)	O
α	O
then	O
3	O
:	O
4	O
:	O
5	O
:	O
//	O
draw	O
an	O
example	O
from	O
the	O
weighted	O
distribution	O
end	O
if	O
6	O
:	O
7	O
:	O
end	O
while	O
important	O
as	O
the	O
negative	O
class	O
.	O
task	O
:	O
α-weighted	O
binary	O
classification	O
given	O
:	O
1.	O
an	O
input	O
space	O
x	O
2.	O
an	O
unknown	O
distribution	O
d	O
over	O
x×	O
{	O
−1	O
,	O
+1	O
}	O
compute	O
:	O
a	O
function	O
f	O
minimizing	O
:	O
e	O
(	O
x	O
,	O
y	O
)	O
∼d	O
(	O
cid:104	O
)	O
αy=1	O
(	O
cid:2	O
)	O
f	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=	O
y	O
(	O
cid:3	O
)	O
(	O
cid:105	O
)	O
the	O
objects	O
given	O
to	O
you	O
in	O
weighted	O
binary	O
classiﬁcation	O
are	O
iden-	O
tical	O
to	O
standard	O
binary	O
classiﬁcation	O
.	O
the	O
only	O
difference	O
is	O
that	O
the	O
cost	O
of	O
misprediction	O
for	O
y	O
=	O
+1	O
is	O
α	O
,	O
while	O
the	O
cost	O
of	O
misprediction	O
for	O
y	O
=	O
−1	O
is	O
1.	O
in	O
what	O
follows	O
,	O
we	O
assume	O
that	O
α	O
>	O
1.	O
if	O
it	O
is	O
not	O
,	O
you	O
can	O
simply	O
swap	O
the	O
labels	O
and	O
use	O
1/α	O
.	O
the	O
question	O
we	O
will	O
ask	O
is	O
:	O
suppose	O
that	O
i	O
have	O
a	O
good	O
algorithm	B
for	O
solving	O
the	O
binary	O
classification	O
problem	O
.	O
can	O
i	O
turn	O
that	O
into	O
a	O
good	O
algorithm	B
for	O
solving	O
the	O
α-weighted	O
binary	O
classification	O
problem	O
?	O
in	O
order	O
to	O
do	O
this	O
,	O
you	O
need	O
to	O
deﬁne	O
a	O
transformation	O
that	O
maps	O
a	O
concrete	O
weighted	O
problem	O
into	O
a	O
concrete	O
unweighted	O
problem	O
.	O
this	O
transformation	O
needs	O
to	O
happen	O
both	O
at	O
training	O
time	O
and	O
at	O
test	O
time	O
(	O
though	O
it	O
need	O
not	O
be	O
the	O
same	O
transformation	O
!	O
)	O
.	O
algorithm	B
?	O
?	O
sketches	O
a	O
training-time	O
sub-sampling	B
transformation	O
and	O
algo-	O
rithm	O
?	O
?	O
sketches	O
a	O
test-time	O
transformation	O
(	O
which	O
,	O
in	O
this	O
case	O
,	O
is	O
trivial	O
)	O
.	O
all	O
the	O
training	O
algorithm	O
is	O
doing	O
is	O
retaining	O
all	O
positive	O
ex-	O
amples	O
and	O
a	O
1/α	O
fraction	O
of	O
all	O
negative	O
examples	B
.	O
the	O
algorithm	O
is	O
explicitly	O
turning	O
the	O
distribution	O
over	O
weighted	O
examples	O
into	O
a	O
(	O
dif-	O
ferent	O
)	O
distribution	O
over	O
binary	O
examples	O
.	O
a	O
vanilla	O
binary	O
classiﬁer	O
is	O
trained	O
on	O
this	O
induced	B
distribution	I
.	O
aside	O
from	O
the	O
fact	O
that	O
this	O
algorithm	B
throws	O
out	O
a	O
lot	O
of	O
data	O
(	O
especially	O
for	O
large	O
α	O
)	O
,	O
it	O
does	O
seem	O
to	O
be	O
doing	O
a	O
reasonable	O
thing	O
.	O
in	O
fact	O
,	O
from	O
a	O
reductions	B
perspective	O
,	O
it	O
is	O
an	O
optimal	O
algorithm	B
.	O
you	O
can	O
prove	O
the	O
following	O
result	O
:	O
beyond	O
binary	O
classification	O
73	O
?	O
why	O
is	O
it	O
unreasonable	O
to	O
expect	O
√	O
to	O
be	O
able	O
to	O
achieve	O
,	O
for	O
instance	O
,	O
an	O
error	O
of	O
sublinear	O
in	O
α	O
?	O
α	O
,	O
or	O
anything	O
that	O
is	O
theorem	O
2	O
(	O
subsampling	O
optimality	O
)	O
.	O
suppose	O
the	O
binary	O
classiﬁer	O
trained	O
in	O
algorithm	B
?	O
?	O
achieves	O
a	O
binary	O
error	O
rate	O
of	O
	O
.	O
then	O
the	O
error	O
rate	O
of	O
the	O
weighted	O
predictor	O
is	O
equal	O
to	O
α	O
.	O
this	O
theorem	O
states	O
that	O
if	O
your	O
binary	O
classiﬁer	O
does	O
well	O
(	O
on	O
the	O
induced	O
distribution	O
)	O
,	O
then	O
the	O
learned	O
predictor	O
will	O
also	O
do	O
well	O
(	O
on	O
the	O
original	O
distribution	O
)	O
.	O
thus	O
,	O
we	O
have	O
successfully	O
converted	O
a	O
weighted	O
learning	O
problem	O
into	O
a	O
plain	O
classiﬁcation	O
problem	O
!	O
the	O
fact	O
that	O
the	O
error	O
rate	O
of	O
the	O
weighted	O
predictor	O
is	O
exactly	O
α	O
times	O
more	O
than	O
that	O
of	O
the	O
unweighted	O
predictor	O
is	O
unavoidable	O
:	O
the	O
error	O
metric	O
on	O
which	O
it	O
is	O
evaluated	O
is	O
α	O
times	O
bigger	O
!	O
the	O
proof	O
of	O
this	O
theorem	O
is	O
so	O
straightforward	O
that	O
we	O
will	O
prove	O
it	O
here	O
.	O
it	O
simply	O
involves	O
some	O
algebra	O
on	O
expected	O
values	O
.	O
proof	O
of	O
theorem	O
?	O
?	O
.	O
let	O
dw	O
be	O
the	O
original	O
distribution	O
and	O
let	O
db	O
be	O
the	O
induced	O
distribution	O
.	O
let	O
f	O
be	O
the	O
binary	O
classiﬁer	O
trained	O
on	O
data	O
from	O
db	O
that	O
achieves	O
a	O
binary	O
error	O
rate	O
of	O
b	O
on	O
that	O
distribution	O
.	O
we	O
will	O
compute	O
the	O
expected	O
error	O
w	O
of	O
f	O
on	O
the	O
weighted	O
problem	O
:	O
(	O
cid:104	O
)	O
αy=1	O
(	O
cid:2	O
)	O
f	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=	O
y	O
(	O
cid:3	O
)	O
(	O
cid:105	O
)	O
dw	O
(	O
x	O
,	O
y	O
)	O
αy=1	O
(	O
cid:2	O
)	O
f	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=	O
y	O
(	O
cid:3	O
)	O
(	O
cid:16	O
)	O
dw	O
(	O
x	O
,	O
+1	O
)	O
(	O
cid:2	O
)	O
f	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=	O
+1	O
(	O
cid:3	O
)	O
+	O
dw	O
(	O
x	O
,	O
−1	O
)	O
(	O
cid:2	O
)	O
f	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=	O
−1	O
(	O
cid:3	O
)	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
db	O
(	O
x	O
,	O
+1	O
)	O
(	O
cid:2	O
)	O
f	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=	O
+1	O
(	O
cid:3	O
)	O
+	O
db	O
(	O
x	O
,	O
−1	O
)	O
(	O
cid:2	O
)	O
f	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=	O
−1	O
(	O
cid:3	O
)	O
(	O
cid:17	O
)	O
(	O
cid:2	O
)	O
f	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=	O
y	O
(	O
cid:3	O
)	O
1	O
α	O
(	O
5.1	O
)	O
(	O
5.2	O
)	O
(	O
5.3	O
)	O
(	O
5.4	O
)	O
(	O
5.5	O
)	O
(	O
5.6	O
)	O
(	O
x	O
,	O
y	O
)	O
∼dw	O
∑	O
y∈±1	O
w	O
=	O
e	O
=	O
∑	O
x∈x	O
=	O
α	O
∑	O
x∈x	O
=	O
α	O
∑	O
x∈x	O
(	O
x	O
,	O
y	O
)	O
∼db	O
=	O
αe	O
=	O
αb	O
and	O
we	O
’	O
re	O
done	O
!	O
(	O
we	O
implicitly	O
assumed	O
x	O
is	O
discrete	O
.	O
in	O
the	O
case	O
of	O
continuous	O
data	O
,	O
you	O
need	O
to	O
replace	O
all	O
the	O
sums	O
over	O
x	O
with	O
integrals	O
over	O
x	O
,	O
but	O
the	O
result	O
still	O
holds	O
.	O
)	O
instead	O
of	O
subsampling	O
the	O
low-cost	O
class	O
,	O
you	O
could	O
alternatively	O
oversample	B
the	O
high-cost	O
class	O
.	O
the	O
easiest	O
case	O
is	O
when	O
α	O
is	O
an	O
in-	O
teger	O
,	O
say	O
5.	O
now	O
,	O
whenever	O
you	O
get	O
a	O
positive	O
point	O
,	O
you	O
include	O
5	O
copies	O
of	O
it	O
in	O
the	O
induced	O
distribution	O
.	O
whenever	O
you	O
get	O
a	O
negative	O
point	O
,	O
you	O
include	O
a	O
single	O
copy	O
.	O
this	O
oversampling	O
algorithm	B
achieves	O
exactly	O
the	O
same	O
theoretical	O
result	O
as	O
the	O
subsampling	O
algorithm	B
.	O
the	O
main	O
advantage	O
to	O
the	O
over-	O
sampling	O
algorithm	B
is	O
that	O
it	O
does	O
not	O
throw	O
out	O
any	O
data	O
.	O
the	O
main	O
advantage	O
to	O
the	O
subsampling	O
algorithm	B
is	O
that	O
it	O
is	O
more	O
computa-	O
tionally	O
efﬁcient	O
.	O
?	O
how	O
can	O
you	O
handle	O
non-integral	O
α	O
,	O
for	O
instance	O
5.5	O
?	O
?	O
modify	O
the	O
proof	O
of	O
optimality	O
for	O
the	O
subsampling	O
algorithm	B
so	O
that	O
it	O
applies	O
to	O
the	O
oversampling	O
algorithm	B
.	O
74	O
a	O
course	O
in	O
machine	O
learning	O
you	O
might	O
be	O
asking	O
yourself	O
:	O
intuitively	O
,	O
the	O
oversampling	O
algo-	O
rithm	O
seems	O
like	O
a	O
much	O
better	O
idea	O
than	O
the	O
subsampling	O
algorithm	B
,	O
at	O
least	O
if	O
you	O
don	O
’	O
t	O
care	O
about	O
computational	O
efﬁciency	O
.	O
but	O
the	O
the-	O
ory	O
tells	O
us	O
that	O
they	O
are	O
the	O
same	O
!	O
what	O
is	O
going	O
on	O
?	O
of	O
course	O
the	O
theory	O
isn	O
’	O
t	O
wrong	O
.	O
it	O
’	O
s	O
just	O
that	O
the	O
assumptions	O
are	O
effectively	O
dif-	O
ferent	O
in	O
the	O
two	O
cases	O
.	O
both	O
theorems	O
state	O
that	O
if	O
you	O
can	O
get	O
error	O
of	O
	O
on	O
the	O
binary	O
problem	O
,	O
you	O
automatically	O
get	O
error	O
of	O
α	O
on	O
the	O
weighted	O
problem	O
.	O
but	O
they	O
do	O
not	O
say	O
anything	O
about	O
how	O
possible	O
it	O
is	O
to	O
get	O
error	O
	O
on	O
the	O
binary	O
problem	O
.	O
since	O
the	O
oversampling	O
al-	O
gorithm	O
produces	O
more	O
data	O
points	O
than	O
the	O
subsampling	O
algorithm	B
it	O
is	O
very	O
concievable	O
that	O
you	O
could	O
get	O
lower	O
binary	O
error	O
with	O
over-	O
sampling	O
than	O
subsampling	O
.	O
the	O
primary	O
drawback	O
to	O
oversampling	O
is	O
computational	O
inefﬁ-	O
ciency	O
.	O
however	O
,	O
for	O
many	O
learning	O
algorithms	O
,	O
it	O
is	O
straightforward	O
to	O
include	O
weighted	O
copies	O
of	O
data	O
points	O
at	O
no	O
cost	O
.	O
the	O
idea	O
is	O
to	O
store	O
only	O
the	O
unique	O
data	O
points	O
and	O
maintain	O
a	O
counter	O
saying	O
how	O
many	O
times	O
they	O
are	O
replicated	O
.	O
this	O
is	O
not	O
easy	O
to	O
do	O
for	O
the	O
percep-	O
tron	O
(	O
it	O
can	O
be	O
done	O
,	O
but	O
takes	O
work	O
)	O
,	O
but	O
it	O
is	O
easy	O
for	O
both	O
decision	B
trees	I
and	O
knn	O
.	O
for	O
example	O
,	O
for	O
decision	B
trees	I
(	O
recall	B
algorithm	O
1.3	O
)	O
,	O
the	O
only	O
changes	O
are	O
to	O
:	O
(	O
1	O
)	O
ensure	O
that	O
line	O
1	O
computes	O
the	O
most	O
fre-	O
quent	O
weighted	O
answer	O
,	O
and	O
(	O
2	O
)	O
change	O
lines	O
10	O
and	O
11	O
to	O
compute	O
weighted	O
errors	O
.	O
5.2	O
multiclass	O
classiﬁcation	O
multiclass	O
classiﬁcation	O
is	O
a	O
natural	O
extension	O
of	O
binary	O
classiﬁcation	O
.	O
the	O
goal	O
is	O
still	O
to	O
assign	O
a	O
discrete	O
label	O
to	O
examples	B
(	O
for	O
instance	O
,	O
is	O
a	O
document	O
about	O
entertainment	O
,	O
sports	O
,	O
ﬁnance	O
or	O
world	O
news	O
?	O
)	O
.	O
the	O
difference	O
is	O
that	O
you	O
have	O
k	O
>	O
2	O
classes	O
to	O
choose	O
from	O
.	O
task	O
:	O
multiclass	O
classification	O
given	O
:	O
1.	O
an	O
input	O
space	O
x	O
and	O
number	O
of	O
classes	O
k	O
2.	O
an	O
unknown	O
distribution	O
d	O
over	O
x×	O
[	O
k	O
]	O
compute	O
:	O
a	O
function	O
f	O
minimizing	O
:	O
e	O
(	O
x	O
,	O
y	O
)	O
∼d	O
(	O
cid:2	O
)	O
f	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=	O
y	O
(	O
cid:3	O
)	O
note	O
that	O
this	O
is	O
identical	O
to	O
binary	O
classiﬁcation	O
,	O
except	O
for	O
the	O
presence	O
of	O
k	O
classes	O
.	O
(	O
in	O
the	O
above	O
,	O
[	O
k	O
]	O
=	O
{	O
1	O
,	O
2	O
,	O
3	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
.	O
)	O
in	O
fact	O
,	O
if	O
you	O
set	O
k	O
=	O
2	O
you	O
exactly	O
recover	O
binary	O
classiﬁcation	O
.	O
the	O
game	O
we	O
play	O
is	O
the	O
same	O
:	O
someone	O
gives	O
you	O
a	O
binary	O
classi-	O
ﬁer	O
and	O
you	O
have	O
to	O
use	O
it	O
to	O
solve	O
the	O
multiclass	O
classiﬁcation	O
prob-	O
?	O
why	O
is	O
it	O
hard	O
to	O
change	O
the	O
per-	O
ceptron	O
?	O
(	O
hint	O
:	O
it	O
has	O
to	O
do	O
with	O
the	O
fact	O
that	O
perceptron	B
is	O
online	B
.	O
)	O
?	O
how	O
would	O
you	O
modify	O
knn	O
to	O
take	O
into	O
account	O
weights	B
?	O
beyond	O
binary	O
classification	O
75	O
algorithm	B
12	O
oneversusalltrain	O
(	O
dmulticlass	O
,	O
binarytrain	O
)	O
1	O
:	O
for	O
i	O
=	O
1	O
to	O
k	O
do	O
2	O
:	O
dbin	O
←	O
relabel	O
dmulticlass	O
so	O
class	O
i	O
is	O
positive	O
and	O
¬i	O
is	O
negative	O
fi	O
←	O
binarytrain	O
(	O
dbin	O
)	O
3	O
:	O
4	O
:	O
end	O
for	O
5	O
:	O
return	O
f1	O
,	O
.	O
.	O
.	O
,	O
fk	O
algorithm	B
13	O
oneversusalltest	O
(	O
f1	O
,	O
.	O
.	O
.	O
,	O
fk	O
,	O
ˆx	O
)	O
1	O
:	O
score	O
←	O
(	O
cid:104	O
)	O
0	O
,	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
(	O
cid:105	O
)	O
2	O
:	O
for	O
i	O
=	O
1	O
to	O
k	O
do	O
3	O
:	O
y	O
←	O
fi	O
(	O
ˆx	O
)	O
scorei	O
←	O
scorei	O
+	O
y	O
4	O
:	O
5	O
:	O
end	O
for	O
6	O
:	O
return	O
argmaxk	O
scorek	O
//	O
initialize	O
k-many	O
scores	O
to	O
zero	O
lem	O
.	O
a	O
very	O
common	O
approach	O
is	O
the	O
one	O
versus	O
all	O
technique	O
(	O
also	O
called	O
ova	O
or	O
one	B
versus	I
rest	I
)	O
.	O
to	O
perform	O
ova	O
,	O
you	O
train	O
k-many	O
binary	O
classiﬁers	O
,	O
f1	O
,	O
.	O
.	O
.	O
,	O
fk	O
.	O
each	O
classiﬁer	O
sees	O
all	O
of	O
the	O
training	O
data	O
.	O
classiﬁer	O
fi	O
receives	O
all	O
examples	O
labeled	O
class	O
i	O
as	O
positives	O
and	O
all	O
other	O
examples	B
as	O
negatives	O
.	O
at	O
test	O
time	O
,	O
whichever	O
classiﬁer	O
predicts	O
“	O
positive	O
”	O
wins	O
,	O
with	O
ties	O
broken	O
randomly	O
.	O
the	O
training	O
and	O
test	O
algorithms	O
for	O
ova	O
are	O
sketched	O
in	O
algo-	O
rithms	O
5.2	O
and	O
5.2.	O
in	O
the	O
testing	O
procedure	O
,	O
the	O
prediction	O
of	O
the	O
ith	O
classiﬁer	O
is	O
added	O
to	O
the	O
overall	O
score	O
for	O
class	O
i.	O
thus	O
,	O
if	O
the	O
predic-	O
tion	O
is	O
positive	O
,	O
class	O
i	O
gets	O
a	O
vote	B
;	O
if	O
the	O
prdiction	O
is	O
negative	O
,	O
every-	O
one	O
else	O
(	O
implicitly	O
)	O
gets	O
a	O
vote	B
.	O
(	O
in	O
fact	O
,	O
if	O
your	O
learning	O
algorithm	B
can	O
output	O
a	O
conﬁdence	O
,	O
as	O
discussed	O
in	O
section	O
?	O
?	O
,	O
you	O
can	O
often	O
do	O
better	O
by	O
using	O
the	O
conﬁdence	O
as	O
y	O
,	O
rather	O
than	O
a	O
simple	O
±1	O
.	O
)	O
ova	O
is	O
very	O
natural	O
,	O
easy	O
to	O
implement	O
,	O
and	O
quite	O
natural	O
.	O
it	O
also	O
works	O
very	O
well	O
in	O
practice	O
,	O
so	O
long	O
as	O
you	O
do	O
a	O
good	O
job	O
choosing	O
a	O
good	O
binary	O
classiﬁcation	O
algorithm	B
tuning	O
its	O
hyperparameters	O
well	O
.	O
its	O
weakness	O
is	O
that	O
it	O
can	O
be	O
somewhat	O
brittle	O
.	O
intuitively	O
,	O
it	O
is	O
not	O
particularly	O
robust	O
to	O
errors	O
in	O
the	O
underlying	O
classiﬁers	O
.	O
if	O
one	O
classiﬁer	O
makes	O
a	O
mistake	O
,	O
it	O
eis	O
possible	O
that	O
the	O
entire	O
prediction	O
is	O
erroneous	O
.	O
in	O
fact	O
,	O
it	O
is	O
entirely	O
possible	O
that	O
none	O
of	O
the	O
k	O
classiﬁers	O
predicts	O
positive	O
(	O
which	O
is	O
actually	O
the	O
worst-case	O
scenario	O
from	O
a	O
theoretical	O
perspective	O
)	O
!	O
this	O
is	O
made	O
explicit	O
in	O
the	O
ova	O
error	O
bound	O
below	O
.	O
theorem	O
3	O
(	O
ova	O
error	O
bound	O
)	O
.	O
suppose	O
the	O
average	O
binary	O
error	O
of	O
the	O
k	O
binary	O
classiﬁers	O
is	O
	O
.	O
then	O
the	O
error	O
rate	O
of	O
the	O
ova	O
multiclass	O
predictor	O
is	O
at	O
most	O
(	O
k	O
−	O
1	O
)	O
	O
.	O
proof	O
of	O
theorem	O
3.	O
the	O
key	O
question	O
is	O
erroneous	O
predictions	O
from	O
the	O
binary	O
classiﬁers	O
lead	O
to	O
multiclass	O
errors	O
.	O
we	O
break	O
it	O
down	O
into	O
false	O
negatives	O
(	O
predicting	O
-1	O
when	O
the	O
truth	O
is	O
+1	O
)	O
and	O
false	O
positives	O
?	O
suppose	O
that	O
you	O
have	O
n	O
data	O
points	O
in	O
k	O
classes	O
,	O
evenly	O
divided	O
.	O
how	O
long	O
does	O
it	O
take	O
to	O
train	O
an	O
ova	O
classiﬁer	O
,	O
if	O
the	O
base	O
binary	O
classiﬁer	O
takes	O
o	O
(	O
n	O
)	O
time	O
to	O
train	O
?	O
what	O
if	O
the	O
base	O
classiﬁer	O
takes	O
o	O
(	O
n2	O
)	O
time	O
?	O
?	O
why	O
would	O
using	O
a	O
conﬁdence	O
help	O
.	O
76	O
a	O
course	O
in	O
machine	O
learning	O
(	O
predicting	O
+1	O
when	O
the	O
truth	O
is	O
-1	O
)	O
.	O
when	O
a	O
false	O
negative	O
occurs	O
,	O
then	O
the	O
testing	O
procedure	O
chooses	O
randomly	O
between	O
available	O
options	O
,	O
which	O
is	O
all	O
labels	O
.	O
this	O
gives	O
a	O
(	O
k	O
−	O
1	O
)	O
/k	O
probability	O
of	O
multiclass	O
error	O
.	O
since	O
only	O
one	O
binary	O
error	O
is	O
necessary	O
to	O
make	O
this	O
happen	O
,	O
the	O
efﬁciency	O
of	O
this	O
error	O
mode	O
is	O
[	O
(	O
k	O
−	O
1	O
)	O
/k	O
]	O
/1	O
=	O
(	O
k	O
−	O
1	O
)	O
/k	O
.	O
multiple	O
false	O
positives	O
can	O
occur	O
simultaneously	O
.	O
suppose	O
there	O
are	O
m	O
false	O
positives	O
.	O
if	O
there	O
is	O
simultaneously	O
a	O
false	O
negative	O
,	O
the	O
error	O
is	O
1.	O
in	O
order	O
for	O
this	O
to	O
happen	O
,	O
there	O
have	O
to	O
be	O
m	O
+	O
1	O
errors	O
,	O
so	O
the	O
efﬁciency	O
is	O
1/	O
(	O
m	O
+	O
1	O
)	O
.	O
in	O
the	O
case	O
that	O
there	O
is	O
not	O
a	O
simulta-	O
neous	O
false	O
negative	O
,	O
the	O
error	O
probability	O
is	O
m/	O
(	O
m	O
+	O
1	O
)	O
.	O
this	O
requires	O
m	O
errors	O
,	O
leading	O
to	O
an	O
efﬁciency	O
of	O
1/	O
(	O
m	O
+	O
1	O
)	O
.	O
the	O
worse	O
case	O
,	O
therefore	O
,	O
is	O
the	O
false	O
negative	O
case	O
,	O
which	O
gives	O
an	O
efﬁciency	O
of	O
(	O
k	O
−	O
1	O
)	O
/k	O
.	O
since	O
we	O
have	O
k-many	O
opportunities	O
to	O
err	O
,	O
we	O
multiply	O
this	O
by	O
k	O
and	O
get	O
a	O
bound	O
of	O
(	O
k	O
−	O
1	O
)	O
	O
.	O
the	O
constants	O
in	O
this	O
are	O
relatively	O
unimportant	O
:	O
the	O
aspect	O
that	O
matters	O
is	O
that	O
this	O
scales	O
linearly	O
in	O
k.	O
that	O
is	O
,	O
as	O
the	O
number	O
of	O
classes	O
grows	O
,	O
so	O
does	O
your	O
expected	O
error	O
.	O
to	O
develop	O
alternative	O
approaches	O
,	O
a	O
useful	O
way	O
to	O
think	O
about	O
turning	O
multiclass	O
classiﬁcation	O
problems	O
into	O
binary	O
classiﬁcation	O
problems	O
is	O
to	O
think	O
of	O
them	O
like	O
tournaments	O
(	O
football	O
,	O
soccer–aka	O
football	O
,	O
cricket	O
,	O
tennis	O
,	O
or	O
whatever	O
appeals	O
to	O
you	O
)	O
.	O
you	O
have	O
k	O
teams	O
entering	O
a	O
tournament	O
,	O
but	O
unfortunately	O
the	O
sport	O
they	O
are	O
playing	O
only	O
allows	O
two	O
to	O
compete	O
at	O
a	O
time	O
.	O
you	O
want	O
to	O
set	O
up	O
a	O
way	O
of	O
pairing	O
the	O
teams	O
and	O
having	O
them	O
compete	O
so	O
that	O
you	O
can	O
ﬁgure	O
out	O
which	O
team	O
is	O
best	O
.	O
in	O
learning	O
,	O
the	O
teams	O
are	O
now	O
the	O
classes	O
and	O
you	O
’	O
re	O
trying	O
to	O
ﬁgure	O
out	O
which	O
class	O
is	O
best.1	O
one	O
natural	O
approach	O
is	O
to	O
have	O
every	O
team	O
compete	O
against	O
ev-	O
ery	O
other	O
team	O
.	O
the	O
team	O
that	O
wins	O
the	O
majority	O
of	O
its	O
matches	O
is	O
declared	O
the	O
winner	O
.	O
this	O
is	O
the	O
all	O
versus	O
all	O
(	O
or	O
ava	O
)	O
approach	O
(	O
sometimes	O
called	O
all	B
pairs	I
)	O
.	O
the	O
most	O
natural	O
way	O
to	O
think	O
about	O
it	O
2	O
)	O
classiﬁers	O
.	O
say	O
fij	O
for	O
1	O
≤	O
i	O
<	O
j	O
≤	O
k	O
is	O
the	O
classiﬁer	O
is	O
as	O
training	O
(	O
k	O
that	O
pits	O
class	O
i	O
against	O
class	O
j.	O
this	O
classiﬁer	O
receives	O
all	O
of	O
the	O
class	O
i	O
examples	B
as	O
“	O
positive	O
”	O
and	O
all	O
of	O
the	O
class	O
j	O
examples	B
as	O
“	O
negative.	O
”	O
when	O
a	O
test	O
point	O
arrives	O
,	O
it	O
is	O
run	O
through	O
all	O
fij	O
classiﬁers	O
.	O
every	O
time	O
fij	O
predicts	O
positive	O
,	O
class	O
i	O
gets	O
a	O
point	O
;	O
otherwise	O
,	O
class	O
j	O
gets	O
a	O
point	O
.	O
after	O
running	O
all	O
(	O
k	O
2	O
)	O
classiﬁers	O
,	O
the	O
class	O
with	O
the	O
most	O
votes	O
wins	O
.	O
the	O
training	O
and	O
test	O
algorithms	O
for	O
ava	O
are	O
sketched	O
in	O
algo-	O
rithms	O
5.2	O
and	O
5.2.	O
in	O
theory	O
,	O
the	O
ava	O
mapping	O
is	O
more	O
complicated	O
than	O
the	O
weighted	O
binary	O
case	O
.	O
the	O
result	O
is	O
stated	O
below	O
,	O
but	O
the	O
proof	O
is	O
omitted	O
.	O
theorem	O
4	O
(	O
ava	O
error	O
bound	O
)	O
.	O
suppose	O
the	O
average	O
binary	O
error	O
of	O
1	O
the	O
sporting	O
analogy	O
breaks	O
down	O
a	O
bit	O
for	O
ova	O
:	O
k	O
games	O
are	O
played	O
,	O
wherein	O
each	O
team	O
will	O
play	O
simultane-	O
ously	O
against	O
all	O
other	O
teams	O
.	O
?	O
suppose	O
that	O
you	O
have	O
n	O
data	O
points	O
in	O
k	O
classes	O
,	O
evenly	O
divided	O
.	O
how	O
long	O
does	O
it	O
take	O
to	O
train	O
an	O
ava	O
classiﬁer	O
,	O
if	O
the	O
base	O
binary	O
classiﬁer	O
takes	O
o	O
(	O
n	O
)	O
time	O
to	O
train	O
?	O
what	O
if	O
the	O
base	O
classiﬁer	O
takes	O
o	O
(	O
n2	O
)	O
time	O
?	O
how	O
does	O
this	O
com-	O
pare	O
to	O
ova	O
?	O
beyond	O
binary	O
classification	O
77	O
algorithm	B
14	O
allversusalltrain	O
(	O
dmulticlass	O
,	O
binarytrain	O
)	O
fij	O
←	O
∅	O
,	O
∀1	O
≤	O
i	O
<	O
j	O
≤	O
k	O
1	O
:	O
2	O
:	O
for	O
i	O
=	O
1	O
to	O
k-1	O
do	O
3	O
:	O
dpos	O
←	O
all	O
x	O
∈	O
dmulticlass	O
labeled	O
i	O
for	O
j	O
=	O
i+1	O
to	O
k	O
do	O
dneg	O
←	O
all	O
x	O
∈	O
dmulticlass	O
labeled	O
j	O
dbin	O
←	O
{	O
(	O
x	O
,	O
+1	O
)	O
:	O
x	O
∈	O
dpos	O
}	O
∪	O
{	O
(	O
x	O
,	O
−1	O
)	O
:	O
x	O
∈	O
dneg	O
}	O
fij	O
←	O
binarytrain	O
(	O
dbin	O
)	O
end	O
for	O
8	O
:	O
9	O
:	O
end	O
for	O
10	O
:	O
return	O
all	O
fijs	O
algorithm	B
15	O
allversusalltest	O
(	O
all	O
fij	O
,	O
ˆx	O
)	O
1	O
:	O
score	O
←	O
(	O
cid:104	O
)	O
0	O
,	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
(	O
cid:105	O
)	O
2	O
:	O
for	O
i	O
=	O
1	O
to	O
k-1	O
do	O
3	O
:	O
//	O
initialize	O
k-many	O
scores	O
to	O
zero	O
4	O
:	O
5	O
:	O
6	O
:	O
7	O
:	O
4	O
:	O
5	O
:	O
6	O
:	O
for	O
j	O
=	O
i+1	O
to	O
k	O
do	O
y	O
←	O
fij	O
(	O
ˆx	O
)	O
scorei	O
←	O
scorei	O
+	O
y	O
scorej	O
←	O
scorej	O
-	O
y	O
end	O
for	O
7	O
:	O
8	O
:	O
end	O
for	O
9	O
:	O
return	O
argmaxk	O
scorek	O
2	O
)	O
binary	O
classiﬁers	O
is	O
	O
.	O
then	O
the	O
error	O
rate	O
of	O
the	O
ava	O
multiclass	O
the	O
(	O
k	O
predictor	O
is	O
at	O
most	O
2	O
(	O
k	O
−	O
1	O
)	O
	O
.	O
at	O
this	O
point	O
,	O
you	O
might	O
be	O
wondering	O
if	O
it	O
’	O
s	O
possible	O
to	O
do	O
bet-	O
ter	O
than	O
something	O
linear	O
in	O
k.	O
fortunately	O
,	O
the	O
answer	O
is	O
yes	O
!	O
the	O
solution	O
,	O
like	O
so	O
much	O
in	O
computer	O
science	O
,	O
is	O
divide	O
and	O
conquer	O
.	O
the	O
idea	O
is	O
to	O
construct	O
a	O
binary	O
tree	O
of	O
classiﬁers	O
.	O
the	O
leaves	O
of	O
this	O
tree	O
correspond	O
to	O
the	O
k	O
labels	O
.	O
since	O
there	O
are	O
only	O
log2	O
k	O
decisions	O
made	O
to	O
get	O
from	O
the	O
root	O
to	O
a	O
leaf	O
,	O
then	O
there	O
are	O
only	O
log2	O
k	O
chances	O
to	O
make	O
an	O
error	O
.	O
an	O
example	O
of	O
a	O
classiﬁcation	O
tree	O
for	O
k	O
=	O
8	O
classes	O
is	O
shown	O
in	O
figure	O
5.2.	O
at	O
the	O
root	O
,	O
you	O
distinguish	O
between	O
classes	O
{	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
}	O
and	O
classes	O
{	O
5	O
,	O
6	O
,	O
7	O
,	O
8	O
}	O
.	O
this	O
means	O
that	O
you	O
will	O
train	O
a	O
binary	O
clas-	O
siﬁer	O
whose	O
positive	O
examples	O
are	O
all	O
data	O
points	O
with	O
multiclass	O
label	B
{	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
}	O
and	O
whose	O
negative	O
examples	B
are	O
all	O
data	O
points	O
with	O
multiclass	O
label	B
{	O
5	O
,	O
6	O
,	O
7	O
,	O
8	O
}	O
.	O
based	O
on	O
what	O
decision	O
is	O
made	O
by	O
this	O
classiﬁer	O
,	O
you	O
can	O
walk	O
down	O
the	O
appropriate	O
path	O
in	O
the	O
tree	O
.	O
when	O
k	O
is	O
not	O
a	O
powwr	O
of	O
2	O
,	O
the	O
tree	O
will	O
not	O
be	O
full	O
.	O
this	O
classiﬁcation	O
tree	O
algorithm	B
achieves	O
the	O
following	O
bound	O
.	O
theorem	O
5	O
(	O
tree	O
error	O
bound	O
)	O
.	O
suppose	O
the	O
average	O
binary	O
classiﬁers	O
error	O
is	O
	O
.	O
then	O
the	O
error	O
rate	O
of	O
the	O
tree	O
classiﬁer	O
is	O
at	O
most	O
(	O
cid:100	O
)	O
log2	O
k	O
(	O
cid:101	O
)	O
	O
.	O
proof	O
of	O
theorem	O
5.	O
a	O
multiclass	O
error	O
is	O
made	O
if	O
any	O
classiﬁer	O
on	O
the	O
bound	O
for	O
ava	O
is	O
2	O
(	O
k	O
−	O
1	O
)	O
	O
;	O
the	O
bound	O
for	O
ova	O
is	O
(	O
k	O
−	O
1	O
)	O
	O
.	O
does	O
this	O
mean	O
that	O
ova	O
is	O
necessarily	O
better	O
than	O
ava	O
?	O
why	O
or	O
why	O
not	O
?	O
?	O
figure	O
5.1	O
:	O
data	O
set	O
on	O
which	O
ova	O
will	O
do	O
terribly	O
with	O
linear	O
classiﬁers	O
?	O
consider	O
the	O
data	O
in	O
figure	O
5.1	O
and	O
assume	O
that	O
you	O
are	O
using	O
a	O
percep-	O
tron	O
as	O
the	O
base	O
classiﬁer	O
.	O
how	O
well	O
will	O
ova	O
do	O
on	O
this	O
data	O
?	O
what	O
about	O
ava	O
?	O
78	O
a	O
course	O
in	O
machine	O
learning	O
the	O
path	O
from	O
the	O
root	O
to	O
the	O
correct	O
leaf	O
makes	O
an	O
error	O
.	O
each	O
has	O
probability	O
	O
of	O
making	O
an	O
error	O
and	O
the	O
path	O
consists	O
of	O
at	O
most	O
(	O
cid:100	O
)	O
log2	O
k	O
(	O
cid:101	O
)	O
binary	O
decisions	O
.	O
one	O
think	O
to	O
keep	O
in	O
mind	O
with	O
tree	O
classiﬁers	O
is	O
that	O
you	O
have	O
control	O
over	O
how	O
the	O
tree	O
is	O
deﬁned	O
.	O
in	O
ova	O
and	O
ava	O
you	O
have	O
no	O
say	O
in	O
what	O
classiﬁcation	O
problems	O
are	O
created	O
.	O
in	O
tree	O
classiﬁers	O
,	O
the	O
only	O
thing	O
that	O
matters	O
is	O
that	O
,	O
at	O
the	O
root	O
,	O
half	O
of	O
the	O
classes	O
are	O
considered	O
positive	O
and	O
half	O
are	O
considered	O
negative	O
.	O
you	O
want	O
to	O
split	O
the	O
classes	O
in	O
such	O
a	O
way	O
that	O
this	O
classiﬁcation	O
decision	O
is	O
as	O
easy	O
as	O
possible	O
.	O
you	O
can	O
use	O
whatever	O
you	O
happen	O
to	O
know	O
about	O
your	O
classiﬁcation	O
problem	O
to	O
try	O
to	O
separate	O
the	O
classes	O
out	O
in	O
a	O
reasonable	O
way	O
.	O
can	O
you	O
do	O
better	O
than	O
(	O
cid:100	O
)	O
log2	O
k	O
(	O
cid:101	O
)	O
	O
?	O
it	O
turns	O
out	O
the	O
answer	O
is	O
yes	O
,	O
but	O
the	O
algorithms	O
to	O
do	O
so	O
are	O
relatively	O
complicated	O
.	O
you	O
can	O
actu-	O
ally	O
do	O
as	O
well	O
as	O
2	O
using	O
the	O
idea	O
of	O
error-correcting	O
tournaments	O
.	O
moreover	O
,	O
you	O
can	O
prove	O
a	O
lower	O
bound	O
that	O
states	O
that	O
the	O
best	O
you	O
could	O
possible	O
do	O
is	O
/2	O
.	O
this	O
means	O
that	O
error-correcting	O
tourna-	O
ments	O
are	O
at	O
most	O
a	O
factor	O
of	O
four	O
worse	O
than	O
optimal	O
.	O
5.3	O
ranking	O
you	O
start	O
a	O
new	O
web	O
search	O
company	O
called	O
goohooing	O
.	O
like	O
other	O
search	O
engines	O
,	O
a	O
user	O
inputs	O
a	O
query	O
and	O
a	O
set	O
of	O
documents	O
is	O
re-	O
trieved	O
.	O
your	O
goal	O
is	O
to	O
rank	O
the	O
resulting	O
documents	O
based	O
on	O
rel-	O
evance	O
to	O
the	O
query	O
.	O
the	O
ranking	O
problem	O
is	O
to	O
take	O
a	O
collection	O
of	O
items	O
and	O
sort	O
them	O
according	O
to	O
some	O
notion	O
of	O
preference	O
.	O
one	O
of	O
the	O
trickiest	O
parts	O
of	O
doing	O
ranking	O
through	O
learning	O
is	O
to	O
properly	O
deﬁne	O
the	O
loss	O
function	O
.	O
toward	O
the	O
end	O
of	O
this	O
section	O
you	O
will	O
see	O
a	O
very	O
general	O
loss	B
function	I
,	O
but	O
before	O
that	O
let	O
’	O
s	O
consider	O
a	O
few	O
special	O
cases	O
.	O
continuing	O
the	O
web	O
search	O
example	O
,	O
you	O
are	O
given	O
a	O
collection	O
of	O
queries	O
.	O
for	O
each	O
query	O
,	O
you	O
are	O
also	O
given	O
a	O
collection	O
of	O
documents	O
,	O
together	O
with	O
a	O
desired	O
ranking	O
over	O
those	O
documents	O
.	O
in	O
the	O
follow-	O
ing	O
,	O
we	O
’	O
ll	O
assume	O
that	O
you	O
have	O
n-many	O
queries	O
and	O
for	O
each	O
query	O
you	O
have	O
m-many	O
documents	O
.	O
(	O
in	O
practice	O
,	O
m	O
will	O
probably	O
vary	O
by	O
query	O
,	O
but	O
for	O
ease	O
we	O
’	O
ll	O
consider	O
the	O
simpliﬁed	O
case	O
.	O
)	O
the	O
goal	O
is	O
to	O
train	O
a	O
binary	O
classiﬁer	O
to	O
predict	B
a	O
preference	B
function	I
.	O
given	O
a	O
query	O
q	O
and	O
two	O
documents	O
di	O
and	O
dj	O
,	O
the	O
classiﬁer	O
should	O
predict	B
whether	O
di	O
should	O
be	O
preferred	O
to	O
dj	O
with	O
respect	O
to	O
the	O
query	O
q.	O
as	O
in	O
all	O
the	O
previous	O
examples	B
,	O
there	O
are	O
two	O
things	O
we	O
have	O
to	O
take	O
care	O
of	O
:	O
(	O
1	O
)	O
how	O
to	O
train	O
the	O
classiﬁer	O
that	O
predicts	O
preferences	O
;	O
(	O
2	O
)	O
how	O
to	O
turn	O
the	O
predicted	O
preferences	O
into	O
a	O
ranking	O
.	O
unlike	O
the	O
previous	O
examples	B
,	O
the	O
second	O
step	O
is	O
somewhat	O
complicated	O
in	O
the	O
beyond	O
binary	O
classification	O
79	O
algorithm	B
16	O
naiveranktrain	O
(	O
rankingdata	O
,	O
binarytrain	O
)	O
1	O
:	O
d	O
←	O
[	O
]	O
2	O
:	O
for	O
n	O
=	O
1	O
to	O
n	O
do	O
3	O
:	O
for	O
all	O
i	O
,	O
j	O
=	O
1	O
to	O
m	O
and	O
i	O
(	O
cid:54	O
)	O
=	O
j	O
do	O
if	O
i	O
is	O
prefered	O
to	O
j	O
on	O
query	O
n	O
then	O
d	O
←	O
d	O
⊕	O
(	O
xnij	O
,	O
+1	O
)	O
d	O
←	O
d	O
⊕	O
(	O
xnij	O
,	O
−1	O
)	O
else	O
if	O
j	O
is	O
prefered	O
to	O
i	O
on	O
query	O
n	O
then	O
end	O
if	O
end	O
for	O
9	O
:	O
10	O
:	O
end	O
for	O
11	O
:	O
return	O
binarytrain	O
(	O
d	O
)	O
4	O
:	O
5	O
:	O
6	O
:	O
7	O
:	O
8	O
:	O
3	O
:	O
4	O
:	O
algorithm	B
17	O
naiveranktest	O
(	O
f	O
,	O
ˆx	O
)	O
1	O
:	O
score	O
←	O
(	O
cid:104	O
)	O
0	O
,	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
(	O
cid:105	O
)	O
2	O
:	O
for	O
all	O
i	O
,	O
j	O
=	O
1	O
to	O
m	O
and	O
i	O
(	O
cid:54	O
)	O
=	O
j	O
do	O
y	O
←	O
f	O
(	O
ˆxij	O
)	O
scorei	O
←	O
scorei	O
+	O
y	O
scorej	O
←	O
scorej	O
-	O
y	O
5	O
:	O
6	O
:	O
end	O
for	O
7	O
:	O
return	O
argsort	O
(	O
score	O
)	O
//	O
initialize	O
m-many	O
scores	O
to	O
zero	O
//	O
get	O
predicted	O
ranking	O
of	O
i	O
and	O
j	O
//	O
return	O
queries	O
sorted	O
by	O
score	O
ranking	O
case	O
.	O
this	O
is	O
because	O
we	O
need	O
to	O
predict	B
an	O
entire	O
ranking	O
of	O
a	O
large	O
number	O
of	O
documents	O
,	O
somehow	O
assimilating	O
the	O
preference	O
function	O
into	O
an	O
overall	O
permutation	O
.	O
for	O
notationally	O
simplicity	O
,	O
let	O
xnij	O
denote	O
the	O
features	O
associated	O
with	O
comparing	O
document	O
i	O
to	O
document	O
j	O
on	O
query	O
n.	O
training	O
is	O
fairly	O
straightforward	O
.	O
for	O
every	O
n	O
and	O
every	O
pair	O
i	O
(	O
cid:54	O
)	O
=	O
j	O
,	O
we	O
will	O
create	O
a	O
binary	O
classiﬁcation	O
example	O
based	O
on	O
features	B
xnij	O
.	O
this	O
example	O
is	O
positive	O
if	O
i	O
is	O
preferred	O
to	O
j	O
in	O
the	O
true	O
ranking	O
.	O
it	O
is	O
neg-	O
ative	O
if	O
j	O
is	O
preferred	O
to	O
i	O
.	O
(	O
in	O
some	O
cases	O
the	O
true	O
ranking	O
will	O
not	O
express	O
a	O
preference	O
between	O
two	O
objects	O
,	O
in	O
which	O
case	O
we	O
exclude	O
the	O
i	O
,	O
j	O
and	O
j	O
,	O
i	O
pair	O
from	O
training	O
.	O
)	O
now	O
,	O
you	O
might	O
be	O
tempted	O
to	O
evaluate	O
the	O
classiﬁcation	O
perfor-	O
mance	O
of	O
this	O
binary	O
classiﬁer	O
on	O
its	O
own	O
.	O
the	O
problem	O
with	O
this	O
approach	O
is	O
that	O
it	O
’	O
s	O
impossible	O
to	O
tell—just	O
by	O
looking	O
at	O
its	O
output	O
on	O
one	O
i	O
,	O
j	O
pair—how	O
good	O
the	O
overall	O
ranking	O
is	O
.	O
this	O
is	O
because	O
there	O
is	O
the	O
intermediate	O
step	O
of	O
turning	O
these	O
pairwise	O
predictions	O
into	O
a	O
coherent	O
ranking	O
.	O
what	O
you	O
need	O
to	O
do	O
is	O
measure	O
how	O
well	O
the	O
ranking	O
based	O
on	O
your	O
predicted	O
preferences	O
compares	O
to	O
the	O
true	O
ordering	O
.	O
algorithms	O
5.3	O
and	O
5.3	O
show	O
naive	O
algorithms	O
for	O
training	O
and	O
testing	O
a	O
ranking	O
function	O
.	O
these	O
algorithms	O
actually	O
work	O
quite	O
well	O
in	O
the	O
case	O
of	O
bipartite	B
ranking	I
problems	I
.	O
a	O
bipartite	O
ranking	O
problem	O
is	O
one	O
in	O
which	O
you	O
are	O
only	O
ever	O
trying	O
to	O
predict	B
a	O
binary	O
response	O
,	O
for	O
instance	O
“	O
is	O
this	O
80	O
a	O
course	O
in	O
machine	O
learning	O
document	O
relevant	O
or	O
not	O
?	O
”	O
but	O
are	O
being	O
evaluated	O
according	O
to	O
a	O
metric	O
like	O
auc	O
.	O
this	O
is	O
essentially	O
because	O
the	O
only	O
goal	O
in	O
bipartite	O
problems	O
to	O
to	O
ensure	O
that	O
all	O
the	O
relevant	O
documents	O
are	O
ahead	O
of	O
all	O
the	O
irrelevant	O
documents	O
.	O
there	O
is	O
no	O
notion	O
that	O
one	O
relevant	O
document	O
is	O
more	O
relevant	O
than	O
another	O
.	O
for	O
non-bipartite	O
ranking	O
problems	O
,	O
you	O
can	O
do	O
better	O
.	O
first	O
,	O
when	O
the	O
preferences	O
that	O
you	O
get	O
at	O
training	O
time	O
are	O
more	O
nuanced	O
than	O
“	O
relevant	O
or	O
not	O
,	O
”	O
you	O
can	O
incorporate	O
these	O
preferences	O
at	O
training	O
time	O
.	O
effectively	O
,	O
you	O
want	O
to	O
give	O
a	O
higher	O
weight	O
to	O
binary	O
prob-	O
lems	O
that	O
are	O
very	O
different	O
in	O
terms	O
of	O
perference	O
than	O
others	O
.	O
sec-	O
ond	O
,	O
rather	O
than	O
producing	O
a	O
list	O
of	O
scores	O
and	O
then	O
calling	O
an	O
arbi-	O
trary	O
sorting	O
algorithm	B
,	O
you	O
can	O
actually	O
use	O
the	O
preference	O
function	O
as	O
the	O
sorting	O
function	O
inside	O
your	O
own	O
implementation	O
of	O
quicksort	O
.	O
we	O
can	O
now	O
formalize	O
the	O
problem	O
.	O
deﬁne	O
a	O
ranking	O
as	O
a	O
function	O
σ	O
that	O
maps	O
the	O
objects	O
we	O
are	O
ranking	O
(	O
documents	O
)	O
to	O
the	O
desired	O
position	O
in	O
the	O
list	O
,	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
m.	O
if	O
σu	O
<	O
σv	O
then	O
u	O
is	O
preferred	O
to	O
v	O
(	O
i.e.	O
,	O
appears	O
earlier	O
on	O
the	O
ranked	O
document	O
list	O
)	O
.	O
given	O
data	O
with	O
ob-	O
served	O
rankings	O
σ	O
,	O
our	O
goal	O
is	O
to	O
learn	O
to	O
predict	B
rankings	O
for	O
new	O
objects	O
,	O
ˆσ	O
.	O
we	O
deﬁne	O
σm	O
as	O
the	O
set	O
of	O
all	O
ranking	O
functions	O
over	O
m	O
objects	O
.	O
we	O
also	O
wish	O
to	O
express	O
the	O
fact	O
that	O
making	O
a	O
mistake	O
on	O
some	O
pairs	O
is	O
worse	O
than	O
making	O
a	O
mistake	O
on	O
others	O
.	O
this	O
will	O
be	O
encoded	O
in	O
a	O
cost	O
function	O
ω	O
(	O
omega	O
)	O
,	O
where	O
ω	O
(	O
i	O
,	O
j	O
)	O
is	O
the	O
cost	O
for	O
ac-	O
cidentally	O
putting	O
something	O
in	O
position	O
j	O
when	O
it	O
should	O
have	O
gone	O
in	O
position	O
i.	O
to	O
be	O
a	O
valid	O
cost	O
function	O
valid	O
,	O
ω	O
must	O
be	O
(	O
1	O
)	O
symmet-	O
ric	O
,	O
(	O
2	O
)	O
monotonic	O
and	O
(	O
3	O
)	O
satisfy	O
the	O
triangle	O
inequality	O
.	O
namely	O
:	O
(	O
1	O
)	O
ω	O
(	O
i	O
,	O
j	O
)	O
=	O
ω	O
(	O
j	O
,	O
i	O
)	O
;	O
(	O
2	O
)	O
if	O
i	O
<	O
j	O
<	O
k	O
or	O
i	O
>	O
j	O
>	O
k	O
then	O
ω	O
(	O
i	O
,	O
j	O
)	O
≤	O
ω	O
(	O
i	O
,	O
k	O
)	O
;	O
(	O
3	O
)	O
ω	O
(	O
i	O
,	O
j	O
)	O
+	O
ω	O
(	O
j	O
,	O
k	O
)	O
≥	O
ω	O
(	O
i	O
,	O
k	O
)	O
.	O
with	O
these	O
deﬁnitions	O
,	O
we	O
can	O
properly	O
deﬁne	O
the	O
ranking	O
problem	O
.	O
task	O
:	O
ω-ranking	O
given	O
:	O
1.	O
an	O
input	O
space	O
x	O
2.	O
an	O
unknown	O
distribution	O
d	O
over	O
x×σm	O
(	O
cid:35	O
)	O
compute	O
:	O
a	O
function	O
f	O
:	O
x	O
→	O
σm	O
minimizing	O
:	O
(	O
cid:34	O
)	O
∑	O
u	O
(	O
cid:54	O
)	O
=v	O
e	O
(	O
x	O
,	O
σ	O
)	O
∼d	O
[	O
σu	O
<	O
σv	O
]	O
[	O
ˆσv	O
<	O
ˆσu	O
]	O
ω	O
(	O
σu	O
,	O
σv	O
)	O
(	O
5.7	O
)	O
where	O
ˆσ	O
=	O
f	O
(	O
x	O
)	O
in	O
this	O
deﬁnition	O
,	O
the	O
only	O
complex	O
aspect	O
is	O
the	O
loss	O
function	O
5.7.	O
this	O
loss	O
sums	O
over	O
all	B
pairs	I
of	O
objects	O
u	O
and	O
v.	O
if	O
the	O
true	O
ranking	O
(	O
σ	O
)	O
beyond	O
binary	O
classification	O
81	O
algorithm	B
18	O
ranktrain	O
(	O
drank	O
,	O
ω	O
,	O
binarytrain	O
)	O
1	O
:	O
dbin	O
←	O
[	O
]	O
2	O
:	O
for	O
all	O
(	O
x	O
,	O
σ	O
)	O
∈	O
drank	O
do	O
for	O
all	O
u	O
(	O
cid:54	O
)	O
=	O
v	O
do	O
y	O
←	O
sign	B
(	O
σv	O
-	O
σu	O
)	O
w	O
←	O
ω	O
(	O
σu	O
,	O
σv	O
)	O
dbin	O
←	O
dbin	O
⊕	O
(	O
y	O
,	O
w	O
,	O
xuv	O
)	O
3	O
:	O
6	O
:	O
4	O
:	O
5	O
:	O
end	O
for	O
7	O
:	O
8	O
:	O
end	O
for	O
9	O
:	O
return	O
binarytrain	O
(	O
dbin	O
)	O
//	O
y	O
is	O
+1	O
if	O
u	O
is	O
prefered	O
to	O
v	O
//	O
w	O
is	O
the	O
cost	O
of	O
misclassiﬁcation	O
prefers	O
u	O
to	O
v	O
,	O
but	O
the	O
predicted	O
ranking	O
(	O
ˆσ	O
)	O
prefers	O
v	O
to	O
u	O
,	O
then	O
you	O
incur	O
a	O
cost	O
of	O
ω	O
(	O
σu	O
,	O
σv	O
)	O
.	O
depending	O
on	O
the	O
problem	O
you	O
care	O
about	O
,	O
you	O
can	O
set	O
ω	O
to	O
many	O
“	O
standard	O
”	O
options	O
.	O
if	O
ω	O
(	O
i	O
,	O
j	O
)	O
=	O
1	O
whenever	O
i	O
(	O
cid:54	O
)	O
=	O
j	O
,	O
then	O
you	O
achieve	O
the	O
kemeny	O
distance	B
measure	O
,	O
which	O
simply	O
counts	O
the	O
number	O
of	O
pairwise	O
misordered	O
items	O
.	O
in	O
many	O
applications	O
,	O
you	O
may	O
only	O
care	O
about	O
getting	O
the	O
top	O
k	O
predictions	O
correct	O
.	O
for	O
instance	O
,	O
your	O
web	O
search	O
algorithm	B
may	O
only	O
display	O
k	O
=	O
10	O
results	O
to	O
a	O
user	O
.	O
in	O
this	O
case	O
,	O
you	O
can	O
deﬁne	O
:	O
ω	O
(	O
i	O
,	O
j	O
)	O
=	O
if	O
min	O
{	O
i	O
,	O
j	O
}	O
≤	O
k	O
and	O
i	O
(	O
cid:54	O
)	O
=	O
j	O
1	O
0	O
otherwise	O
(	O
5.8	O
)	O
(	O
cid:40	O
)	O
in	O
this	O
case	O
,	O
only	O
errors	O
in	O
the	O
top	O
k	O
elements	O
are	O
penalized	O
.	O
swap-	O
ping	O
items	O
55	O
and	O
56	O
is	O
irrelevant	O
(	O
for	O
k	O
<	O
55	O
)	O
.	O
finally	O
,	O
in	O
the	O
bipartite	O
ranking	O
case	O
,	O
you	O
can	O
express	O
the	O
area	O
under	O
the	O
curve	O
(	O
auc	O
)	O
metric	O
as	O
:	O
ω	O
(	O
i	O
,	O
j	O
)	O
=	O
(	O
m	O
2	O
)	O
m+	O
(	O
m	O
−	O
m+	O
)	O
×	O
if	O
i	O
≤	O
m+	O
and	O
j	O
>	O
m+	O
1	O
if	O
j	O
≤	O
m+	O
and	O
i	O
>	O
m+	O
0	O
otherwise	O
(	O
5.9	O
)	O
	O
1	O
here	O
,	O
m	O
is	O
the	O
total	O
number	O
of	O
objects	O
to	O
be	O
ranked	O
and	O
m+	O
is	O
the	O
number	O
that	O
are	O
actually	O
“	O
good.	O
”	O
(	O
hence	O
,	O
m	O
−	O
m+	O
is	O
the	O
number	O
that	O
are	O
actually	O
“	O
bad	O
,	O
”	O
since	O
this	O
is	O
a	O
bipartite	O
problem	O
.	O
)	O
you	O
are	O
only	O
penalized	O
if	O
you	O
rank	O
a	O
good	O
item	O
in	O
position	O
greater	O
than	O
m+	O
or	O
if	O
you	O
rank	O
a	O
bad	O
item	O
in	O
a	O
position	O
less	O
than	O
or	O
equal	O
to	O
m+	O
.	O
in	O
order	O
to	O
solve	O
this	O
problem	O
,	O
you	O
can	O
follow	O
a	O
recipe	O
similar	O
to	O
the	O
naive	O
approach	O
sketched	O
earlier	O
.	O
at	O
training	O
time	O
,	O
the	O
biggest	O
change	O
is	O
that	O
you	O
can	O
weight	O
each	O
training	O
example	O
by	O
how	O
bad	O
it	O
would	O
be	O
to	O
mess	O
it	O
up	O
.	O
this	O
change	O
is	O
depicted	O
in	O
algorithm	B
5.3	O
,	O
where	O
the	O
binary	O
classiciation	O
data	O
has	O
weights	B
w	O
provided	O
for	O
saying	O
how	O
important	O
a	O
given	O
example	O
is	O
.	O
these	O
weights	B
are	O
derived	O
from	O
the	O
cost	O
function	O
ω.	O
at	O
test	O
time	O
,	O
instead	O
of	O
predicting	O
scores	O
and	O
then	O
sorting	O
the	O
list	O
,	O
you	O
essentially	O
run	O
the	O
quicksort	O
algorith	O
,	O
using	O
f	O
as	O
a	O
comparison	O
82	O
a	O
course	O
in	O
machine	O
learning	O
algorithm	B
19	O
ranktest	O
(	O
f	O
,	O
ˆx	O
,	O
obj	O
)	O
1	O
:	O
if	O
obj	O
contains	O
0	O
or	O
1	O
elements	O
then	O
2	O
:	O
3	O
:	O
else	O
4	O
:	O
return	O
obj	O
p	O
←	O
randomly	O
chosen	O
object	O
in	O
obj	O
left	O
←	O
[	O
]	O
right	O
←	O
[	O
]	O
for	O
all	O
u	O
∈	O
obj	O
\	O
{	O
p	O
}	O
do	O
5	O
:	O
6	O
:	O
7	O
:	O
8	O
:	O
9	O
:	O
10	O
:	O
11	O
:	O
12	O
:	O
13	O
:	O
14	O
:	O
15	O
:	O
16	O
:	O
17	O
:	O
18	O
:	O
end	O
if	O
left	O
←	O
left	O
⊕	O
u	O
right	O
←	O
right	O
⊕	O
u	O
else	O
end	O
if	O
end	O
for	O
left	O
←	O
ranktest	O
(	O
f	O
,	O
ˆx	O
,	O
left	O
)	O
right	O
←	O
ranktest	O
(	O
f	O
,	O
ˆx	O
,	O
right	O
)	O
return	O
left	O
⊕	O
(	O
cid:104	O
)	O
p	O
(	O
cid:105	O
)	O
⊕	O
right	O
//	O
pick	O
pivot	O
//	O
elements	O
that	O
seem	O
smaller	O
than	O
p	O
//	O
elements	O
that	O
seem	O
larger	O
than	O
p	O
ˆy	O
←	O
f	O
(	O
xup	O
)	O
if	O
uniform	O
random	O
variable	O
<	O
ˆy	O
then	O
//	O
what	O
is	O
the	O
probability	O
that	O
u	O
precedes	O
p	O
//	O
sort	O
earlier	O
elements	O
//	O
sort	O
later	O
elements	O
function	O
.	O
at	O
each	O
step	O
in	O
algorithm	B
5.3	O
,	O
a	O
pivot	O
p	O
is	O
chosen	O
.	O
every	O
other	O
object	O
u	O
is	O
compared	O
to	O
p	O
using	O
f	O
.	O
if	O
f	O
thinks	O
u	O
is	O
better	O
,	O
then	O
it	O
is	O
sorted	O
on	O
the	O
left	O
;	O
otherwise	O
it	O
is	O
sorted	O
on	O
the	O
right	O
.	O
there	O
is	O
one	O
major	O
difference	O
between	O
this	O
algorithmand	O
quicksort	O
:	O
the	O
compari-	O
son	O
function	O
is	O
allowed	O
to	O
be	O
probabilistic	O
.	O
if	O
f	O
outputs	O
probabilities	O
,	O
for	O
instance	O
it	O
predicts	O
that	O
u	O
has	O
an	O
80	O
%	O
probability	O
of	O
being	O
better	O
than	O
p	O
,	O
then	O
it	O
puts	O
it	O
on	O
the	O
left	O
with	O
80	O
%	O
probability	O
and	O
on	O
the	O
right	O
with	O
20	O
%	O
probability	O
.	O
(	O
the	O
pseudocode	O
is	O
written	O
in	O
such	O
a	O
way	O
that	O
even	O
if	O
f	O
just	O
predicts	O
−1	O
,	O
+1	O
,	O
the	O
algorithm	O
still	O
works	O
.	O
)	O
this	O
algorithm	B
is	O
better	O
than	O
the	O
naive	O
algorithm	B
in	O
at	O
least	O
two	O
ways	O
.	O
first	O
,	O
it	O
only	O
makes	O
o	O
(	O
m	O
log2	O
m	O
)	O
calls	O
to	O
f	O
(	O
in	O
expectation	O
)	O
,	O
rather	O
than	O
o	O
(	O
m2	O
)	O
calls	O
in	O
the	O
naive	O
case	O
.	O
second	O
,	O
it	O
achieves	O
a	O
better	O
error	O
bound	O
,	O
shown	O
below	O
:	O
theorem	O
6	O
(	O
rank	O
error	O
bound	O
)	O
.	O
suppose	O
the	O
average	O
binary	O
error	O
of	O
f	O
is	O
	O
.	O
then	O
the	O
ranking	O
algorithm	B
achieves	O
a	O
test	B
error	I
of	O
at	O
most	O
2	O
in	O
the	O
general	O
case	O
,	O
and	O
	O
in	O
the	O
bipartite	O
case	O
.	O
5.4	O
collective	O
classiﬁcation	O
you	O
are	O
writing	O
new	O
software	O
for	O
a	O
digital	O
camera	O
that	O
does	O
face	O
identiﬁcation	O
.	O
however	O
,	O
instead	O
of	O
simply	O
ﬁnding	O
a	O
bounding	O
box	O
around	O
faces	O
in	O
an	O
image	O
,	O
you	O
must	O
predict	B
where	O
a	O
face	O
is	O
at	O
the	O
pixel	O
level	O
.	O
so	O
your	O
input	O
is	O
an	O
image	O
(	O
say	O
,	O
100×100	O
pixels	O
:	O
this	O
is	O
a	O
really	O
low	O
resolution	O
camera	O
!	O
)	O
and	O
your	O
output	O
is	O
a	O
set	O
of	O
100×100	O
binary	O
predictions	O
about	O
each	O
pixel	O
.	O
you	O
are	O
given	O
a	O
large	O
collection	O
figure	O
5.3	O
:	O
example	O
face	O
ﬁnding	O
image	O
and	O
pixel	O
mask	O
beyond	O
binary	O
classification	O
83	O
of	O
training	O
examples	O
.	O
an	O
example	O
input/output	O
pair	O
is	O
shown	O
in	O
figure	O
5.3.	O
your	O
ﬁrst	O
attempt	O
might	O
be	O
to	O
train	O
a	O
binary	O
classiﬁer	O
to	O
predict	B
whether	O
pixel	O
(	O
i	O
,	O
j	O
)	O
is	O
part	O
of	O
a	O
face	O
or	O
not	O
.	O
you	O
might	O
feed	O
in	O
features	B
to	O
this	O
classiﬁer	O
about	O
the	O
rgb	O
values	O
of	O
pixel	O
(	O
i	O
,	O
j	O
)	O
as	O
well	O
as	O
pixels	O
in	O
a	O
window	O
arround	O
that	O
.	O
for	O
instance	O
,	O
pixels	O
in	O
the	O
region	O
{	O
(	O
i	O
+	O
k	O
,	O
j	O
+	O
l	O
)	O
:	O
k	O
∈	O
[	O
−5	O
,	O
5	O
]	O
,	O
l	O
∈	O
[	O
−5	O
,	O
5	O
]	O
}	O
.	O
you	O
run	O
your	O
classiﬁer	O
and	O
notice	O
that	O
it	O
predicts	O
weird	O
things	O
,	O
like	O
what	O
you	O
see	O
in	O
figure	O
5.4.	O
you	O
then	O
realize	O
that	O
predicting	O
each	O
pixel	O
independently	O
is	O
a	O
bad	O
idea	O
!	O
if	O
pixel	O
(	O
i	O
,	O
j	O
)	O
is	O
part	O
of	O
a	O
face	O
,	O
then	O
this	O
signiﬁcantly	O
increases	O
the	O
chances	O
that	O
pixel	O
(	O
i	O
+	O
1	O
,	O
j	O
)	O
is	O
also	O
part	O
of	O
a	O
face	O
.	O
(	O
and	O
similarly	O
for	O
other	O
pixels	O
.	O
)	O
this	O
is	O
a	O
collective	O
classiﬁ-	O
cation	O
problem	O
because	O
you	O
are	O
trying	O
to	O
predict	B
multiple	O
,	O
correlated	O
objects	O
at	O
the	O
same	O
time	O
.	O
the	O
most	O
general	O
way	O
to	O
formulate	O
these	O
problems	O
is	O
as	O
(	O
undi-	O
rected	O
)	O
graph	B
prediction	O
problems	O
.	O
our	O
input	O
now	O
takes	O
the	O
form	O
of	O
a	O
graph	B
,	O
where	O
the	O
vertices	O
are	O
input/output	O
pairs	O
and	O
the	O
edges	O
represent	O
the	O
correlations	O
among	O
the	O
putputs	O
.	O
(	O
note	O
that	O
edges	O
do	O
not	O
need	O
to	O
express	O
correlations	O
among	O
the	O
inputs	O
:	O
these	O
can	O
simply	O
be	O
encoded	O
on	O
the	O
nodes	O
themselves	O
.	O
)	O
for	O
example	O
,	O
in	O
the	O
face	O
identi-	O
ﬁcation	O
case	O
,	O
each	O
pixel	O
would	O
correspond	O
to	O
an	O
vertex	O
in	O
the	O
graph	O
.	O
for	O
the	O
vertex	O
that	O
corresponds	O
to	O
pixel	O
(	O
5	O
,	O
10	O
)	O
,	O
the	O
input	O
would	O
be	O
whatever	O
set	O
of	O
features	B
we	O
want	O
about	O
that	O
pixel	O
(	O
including	O
features	B
about	O
neighboring	O
pixels	O
)	O
.	O
there	O
would	O
be	O
edges	O
between	O
that	O
vertex	O
and	O
(	O
for	O
instance	O
)	O
vertices	O
(	O
4	O
,	O
10	O
)	O
,	O
(	O
6	O
,	O
10	O
)	O
,	O
(	O
5	O
,	O
9	O
)	O
and	O
(	O
5	O
,	O
11	O
)	O
.	O
if	O
we	O
are	O
predicting	O
one	O
of	O
k	O
classes	O
at	O
each	O
vertex	O
,	O
then	O
we	O
are	O
given	O
a	O
graph	B
whose	O
vertices	O
are	O
labeled	O
by	O
pairs	O
(	O
x	O
,	O
k	O
)	O
∈	O
x×	O
[	O
k	O
]	O
.	O
we	O
will	O
write	O
g	O
(	O
x×	O
[	O
k	O
]	O
)	O
to	O
denote	O
the	O
set	O
of	O
all	O
such	O
graphs	O
.	O
a	O
graph	B
in	O
this	O
set	O
is	O
denoted	O
as	O
g	O
=	O
(	O
v	O
,	O
e	O
)	O
with	O
vertices	O
v	O
and	O
edges	O
e.	O
our	O
goal	O
is	O
a	O
function	O
f	O
that	O
takes	O
as	O
input	O
a	O
graph	B
from	O
g	O
(	O
x	O
)	O
and	O
predicts	O
a	O
label	B
from	O
[	O
k	O
]	O
for	O
each	O
of	O
its	O
vertices	O
.	O
task	O
:	O
collective	O
classification	O
given	O
:	O
1.	O
an	O
input	O
space	O
x	O
and	O
number	O
of	O
classes	O
k	O
2.	O
an	O
unknown	O
distribution	O
d	O
over	O
g	O
(	O
x×	O
[	O
k	O
]	O
)	O
compute	O
:	O
a	O
function	O
f	O
e	O
with	O
vertex	O
v	O
in	O
g	O
and	O
ˆyv	O
is	O
the	O
label	O
predicted	O
by	O
f	O
(	O
g	O
)	O
.	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
,	O
where	O
yv	O
is	O
the	O
label	O
associated	O
:	O
(	O
v	O
,	O
e	O
)	O
∼d	O
(	O
cid:2	O
)	O
∑v∈v	O
(	O
cid:2	O
)	O
ˆyv	O
(	O
cid:54	O
)	O
=	O
yv	O
g	O
(	O
x	O
)	O
→	O
g	O
(	O
[	O
k	O
]	O
)	O
minimizing	O
:	O
in	O
collective	O
classiﬁcation	O
,	O
you	O
would	O
like	O
to	O
be	O
able	O
to	O
use	O
the	O
figure	O
5.4	O
:	O
bad	O
pixel	O
mask	O
for	O
previous	O
image	O
?	O
similar	O
problems	O
come	O
up	O
all	O
the	O
time	O
.	O
cast	O
the	O
following	O
as	O
collec-	O
tive	O
classiﬁcation	O
problems	O
:	O
web	O
page	O
categorization	O
;	O
labeling	O
words	O
in	O
a	O
sentence	O
as	O
noun	O
,	O
verb	O
,	O
adjec-	O
tive	O
,	O
etc	O
.	O
;	O
ﬁnding	O
genes	O
in	O
dna	O
sequences	O
;	O
predicting	O
the	O
stock	O
market	O
.	O
?	O
formulate	O
the	O
example	O
problems	O
above	O
as	O
graph	B
prediction	O
prob-	O
lems	O
.	O
84	O
a	O
course	O
in	O
machine	O
learning	O
labels	O
of	O
neighboring	O
vertices	O
to	O
help	O
predict	B
the	O
label	B
of	O
a	O
given	O
vertex	O
.	O
for	O
instance	O
,	O
you	O
might	O
want	O
to	O
add	O
features	B
to	O
the	O
predict	O
of	O
a	O
given	O
vertex	O
based	O
on	O
the	O
labels	O
of	O
each	O
neighbor	O
.	O
at	O
training	O
time	O
,	O
this	O
is	O
easy	O
:	O
you	O
get	O
to	O
see	O
the	O
true	O
labels	O
of	O
each	O
neighbor	O
.	O
however	O
,	O
at	O
test	O
time	O
,	O
it	O
is	O
much	O
more	O
difﬁcult	O
:	O
you	O
are	O
,	O
yourself	O
,	O
predicting	O
the	O
labels	O
of	O
each	O
neighbor	O
.	O
this	O
presents	O
a	O
chicken	O
and	O
egg	O
problem	O
.	O
you	O
are	O
trying	O
to	O
predict	B
a	O
collection	O
of	O
labels	O
.	O
but	O
the	O
prediction	O
of	O
each	O
label	B
depends	O
on	O
the	O
prediction	O
of	O
other	O
labels	O
.	O
if	O
you	O
remember	O
from	O
before	O
,	O
a	O
general	O
so-	O
lution	O
to	O
this	O
problem	O
is	O
iteration	B
:	O
you	O
can	O
begin	O
with	O
some	O
guesses	O
,	O
and	O
then	O
try	O
to	O
improve	O
these	O
guesses	O
over	O
time	O
.	O
2	O
this	O
is	O
the	O
idea	O
of	O
stacking	B
for	O
solving	O
collective	O
classiﬁcation	O
(	O
see	O
figure	O
5.5.	O
you	O
can	O
train	O
5	O
classiﬁers	O
.	O
the	O
ﬁrst	O
classiﬁer	O
just	O
predicts	O
the	O
value	O
of	O
each	O
pixel	O
independently	O
,	O
like	O
in	O
figure	O
5.4.	O
this	O
doesn	O
’	O
t	O
use	O
any	O
of	O
the	O
graph	O
structure	O
at	O
all	O
.	O
in	O
the	O
second	O
level	O
,	O
you	O
can	O
repeat	O
the	O
classiﬁcation	O
.	O
however	O
,	O
you	O
can	O
use	O
the	O
outputs	O
from	O
the	O
ﬁrst	O
level	O
as	O
initial	O
guesses	O
of	O
labels	O
.	O
in	O
general	O
,	O
for	O
the	O
kth	O
level	O
in	O
the	O
stack	O
,	O
you	O
can	O
use	O
the	O
inputs	O
(	O
pixel	O
values	O
)	O
as	O
well	O
as	O
the	O
predictions	O
for	O
all	O
of	O
the	O
k	O
−	O
1	O
previous	O
levels	O
of	O
the	O
stack	O
.	O
this	O
means	O
training	O
k-many	O
binary	O
classiﬁers	O
based	O
on	O
different	O
feature	O
sets	O
.	O
the	O
prediction	O
technique	O
for	O
stacking	B
is	O
sketched	O
in	O
algorithm	B
5.4.	O
this	O
takes	O
a	O
list	O
of	O
k	O
classiﬁers	O
,	O
corresponding	O
to	O
each	O
level	O
in	O
the	O
stack	O
,	O
and	O
an	O
input	O
graph	B
g.	O
the	O
variable	O
ˆyk	O
,	O
v	O
stores	O
the	O
prediction	O
of	O
classiﬁer	O
k	O
on	O
vertex	O
v	O
in	O
the	O
graph	O
.	O
you	O
ﬁrst	O
predict	B
every	O
node	O
in	O
the	O
vertex	O
using	O
the	O
ﬁrst	O
layer	O
in	O
the	O
stack	O
,	O
and	O
no	O
neighboring	O
information	O
.	O
for	O
the	O
rest	O
of	O
the	O
layers	O
,	O
you	O
add	O
on	O
features	B
to	O
each	O
node	O
based	O
on	O
the	O
predictions	O
made	O
by	O
lower	O
levels	O
in	O
the	O
stack	O
for	O
neighboring	O
nodes	O
(	O
n	O
(	O
u	O
)	O
denotes	O
the	O
neighbors	O
of	O
u	O
)	O
.	O
the	O
training	O
procedure	O
follows	O
a	O
similar	O
scheme	O
,	O
sketched	O
in	O
al-	O
gorithm	O
5.4.	O
it	O
largely	O
follows	O
the	O
same	O
schematic	O
as	O
the	O
prediction	O
algorithm	B
,	O
but	O
with	O
training	O
fed	O
in	O
.	O
after	O
the	O
classiﬁer	O
for	O
the	O
k	O
level	O
has	O
been	O
trained	O
,	O
it	O
is	O
used	O
to	O
predict	B
labels	O
on	O
every	O
node	O
in	O
the	O
graph	O
.	O
these	O
labels	O
are	O
used	O
by	O
later	O
levels	O
in	O
the	O
stack	O
,	O
as	O
features	B
.	O
one	O
thing	O
to	O
be	O
aware	O
of	O
is	O
that	O
multiclasstrain	O
could	O
con-	O
ceivably	O
overﬁt	O
its	O
training	B
data	I
.	O
for	O
example	O
,	O
it	O
is	O
possible	O
that	O
the	O
ﬁrst	O
layer	O
might	O
actually	O
achieve	O
0	O
%	O
error	O
,	O
in	O
which	O
case	O
there	O
is	O
no	O
reason	O
to	O
iterate	O
.	O
but	O
at	O
test	O
time	O
,	O
it	O
will	O
probably	O
not	O
get	O
0	O
%	O
error	O
,	O
so	O
this	O
is	O
misleading	O
.	O
there	O
are	O
(	O
at	O
least	O
)	O
two	O
ways	O
to	O
address	O
this	O
issue	O
.	O
the	O
ﬁrst	O
is	O
to	O
use	O
cross-validation	O
during	O
training	O
,	O
and	O
to	O
use	O
the	O
predictions	O
obtained	O
during	O
cross-validation	O
as	O
the	O
predictions	O
from	O
stacktest	O
.	O
this	O
is	O
typically	O
very	O
safe	O
,	O
but	O
somewhat	O
expensive	O
.	O
the	O
alternative	O
is	O
to	O
simply	O
over-regularize	O
your	O
training	O
algorithm	O
.	O
in	O
particular	O
,	O
instead	O
of	O
trying	O
to	O
ﬁnd	O
hyperparameters	O
that	O
get	O
the	O
2	O
alternatively	O
,	O
the	O
fact	O
that	O
we	O
’	O
re	O
using	O
a	O
graph	B
might	O
scream	O
to	O
you	O
“	O
dynamic	O
programming.	O
”	O
rest	O
assured	O
that	O
you	O
can	O
do	O
this	O
too	O
:	O
skip	O
forward	O
to	O
chapter	O
18	O
for	O
lots	O
more	O
detail	O
here	O
!	O
figure	O
5.5	O
:	O
a	O
charicature	O
of	O
how	O
stack-	O
ing	O
works	O
beyond	O
binary	O
classification	O
85	O
algorithm	B
20	O
stacktrain	O
(	O
dcc	O
,	O
k	O
,	O
multiclasstrain	O
)	O
1	O
:	O
dmc	O
←	O
[	O
]	O
ˆyk	O
,	O
n	O
,	O
v	O
←	O
0	O
,	O
∀k	O
∈	O
[	O
k	O
]	O
,	O
n	O
∈	O
[	O
n	O
]	O
,	O
v	O
∈	O
gn	O
2	O
:	O
3	O
:	O
for	O
k	O
=	O
1	O
to	O
k	O
do	O
4	O
:	O
for	O
n	O
=	O
1	O
to	O
n	O
do	O
for	O
all	O
v	O
∈	O
gn	O
do	O
//	O
our	O
generated	O
multiclass	O
data	O
//	O
initialize	O
predictions	O
for	O
all	O
levels	O
(	O
x	O
,	O
y	O
)	O
←	O
features	B
and	O
label	B
for	O
node	O
v	O
x	O
←	O
x	O
⊕	O
ˆyl	O
,	O
n	O
,	O
u	O
,	O
∀u	O
∈	O
n	O
(	O
u	O
)	O
,	O
∀l	O
∈	O
[	O
k	O
−	O
1	O
]	O
dmc	O
←	O
dmc	O
⊕	O
(	O
y	O
,	O
x	O
)	O
//	O
add	O
on	O
features	B
for	O
//	O
neighboring	O
nodes	O
from	O
lower	O
levels	O
in	O
the	O
stack	O
//	O
add	O
to	O
multiclass	O
data	O
end	O
for	O
end	O
for	O
fk	O
←	O
multiclasstrain	O
(	O
dbin	O
)	O
for	O
n	O
=	O
1	O
to	O
n	O
do	O
ˆyk	O
,	O
n	O
,	O
v	O
←	O
stacktest	O
(	O
f1	O
,	O
.	O
.	O
.	O
,	O
fk	O
,	O
gn	O
)	O
end	O
for	O
15	O
:	O
16	O
:	O
end	O
for	O
17	O
:	O
return	O
f1	O
,	O
.	O
.	O
.	O
,	O
fk	O
algorithm	B
21	O
stacktest	O
(	O
f1	O
,	O
.	O
.	O
.	O
,	O
fk	O
,	O
g	O
)	O
ˆyk	O
,	O
v	O
←	O
0	O
,	O
∀k	O
∈	O
[	O
k	O
]	O
,	O
v	O
∈	O
g	O
1	O
:	O
2	O
:	O
for	O
k	O
=	O
1	O
to	O
k	O
do	O
for	O
all	O
v	O
∈	O
g	O
do	O
3	O
:	O
//	O
train	O
kth	O
level	O
classiﬁer	O
//	O
predict	B
using	O
kth	O
level	O
classiﬁer	O
//	O
return	O
all	O
classiﬁers	O
//	O
initialize	O
predictions	O
for	O
all	O
levels	O
5	O
:	O
6	O
:	O
7	O
:	O
8	O
:	O
9	O
:	O
10	O
:	O
11	O
:	O
12	O
:	O
13	O
:	O
14	O
:	O
4	O
:	O
5	O
:	O
6	O
:	O
7	O
:	O
x	O
←	O
features	B
for	O
node	O
v	O
x	O
←	O
x	O
⊕	O
ˆyl	O
,	O
u	O
,	O
∀u	O
∈	O
n	O
(	O
u	O
)	O
,	O
∀l	O
∈	O
[	O
k	O
−	O
1	O
]	O
ˆyk	O
,	O
v	O
←	O
fk	O
(	O
x	O
)	O
//	O
add	O
on	O
features	B
for	O
//	O
neighboring	O
nodes	O
from	O
lower	O
levels	O
in	O
the	O
stack	O
//	O
predict	B
according	O
to	O
kth	O
level	O
end	O
for	O
8	O
:	O
9	O
:	O
end	O
for	O
10	O
:	O
return	O
{	O
ˆyk	O
,	O
v	O
:	O
v	O
∈	O
g	O
}	O
//	O
return	O
predictions	O
for	O
every	O
node	O
from	O
the	O
last	O
layer	O
best	O
development	B
data	I
performance	O
,	O
try	O
to	O
ﬁnd	O
hyperparameters	O
that	O
make	O
your	O
training	O
performance	O
approximately	O
equal	O
to	O
your	O
devel-	O
opment	O
performance	O
.	O
this	O
will	O
ensure	O
that	O
your	O
predictions	O
at	O
the	O
kth	O
layer	O
are	O
indicative	O
of	O
how	O
well	O
the	O
algorithm	O
will	O
actually	O
do	O
at	O
test	O
time	O
.	O
todo	O
:	O
ﬁnish	O
this	O
discussion	O
5.5	O
exercises	O
exercise	O
5.1.	O
todo	O
.	O
.	O
.	O
6	O
|	O
linear	O
models	O
learning	O
objectives	O
:	O
•	O
deﬁne	O
and	O
plot	O
four	O
surrogate	B
loss	I
functions	O
:	O
squared	B
loss	I
,	O
logistic	B
loss	I
,	O
exponential	B
loss	I
and	O
hinge	B
loss	I
.	O
•	O
compare	O
and	O
contrast	O
the	O
optimiza-	O
tion	O
of	O
0/1	B
loss	I
and	O
surrogate	B
loss	I
functions	O
.	O
•	O
solve	O
the	O
optimization	O
problem	O
for	O
squared	B
loss	I
with	O
a	O
quadratic	O
regularizer	B
in	O
closed	O
form	O
.	O
•	O
implement	O
and	O
debug	O
gradient	B
descent	I
and	O
subgradient	B
descent	I
.	O
dependencies	O
:	O
the	O
essence	O
of	O
mathematics	O
is	O
not	O
to	O
make	O
simple	O
things	O
compli-	O
cated	O
,	O
but	O
to	O
make	O
complicated	O
things	O
simple	O
.	O
–	O
stanley	O
gudder	O
in	O
chapter	O
?	O
?	O
,	O
you	O
learned	O
about	O
the	O
perceptron	O
algorithm	B
for	O
linear	O
classiﬁcation	O
.	O
this	O
was	O
both	O
a	O
model	B
(	O
linear	O
classiﬁer	O
)	O
and	O
algorithm	B
(	O
the	O
perceptron	O
update	O
rule	O
)	O
in	O
one	O
.	O
in	O
this	O
section	O
,	O
we	O
will	O
separate	O
these	O
two	O
,	O
and	O
consider	O
general	O
ways	O
for	O
optimizing	O
linear	O
models	O
.	O
this	O
will	O
lead	O
us	O
into	O
some	O
aspects	O
of	O
optimization	O
(	O
aka	O
mathematical	O
programming	O
)	O
,	O
but	O
not	O
very	O
far	O
.	O
at	O
the	O
end	O
of	O
this	O
chapter	O
,	O
there	O
are	O
pointers	O
to	O
more	O
literature	O
on	O
optimization	O
for	O
those	O
who	O
are	O
interested	O
.	O
the	O
basic	O
idea	O
of	O
the	O
perceptron	O
is	O
to	O
run	O
a	O
particular	O
algorithm	B
until	O
a	O
linear	O
separator	O
is	O
found	O
.	O
you	O
might	O
ask	O
:	O
are	O
there	O
better	O
al-	O
gorithms	O
for	O
ﬁnding	O
such	O
a	O
linear	O
separator	O
?	O
we	O
will	O
follow	O
this	O
idea	O
and	O
formulate	O
a	O
learning	O
problem	O
as	O
an	O
explicit	O
optimization	O
prob-	O
lem	O
:	O
ﬁnd	O
me	O
a	O
linear	O
separator	O
that	O
is	O
not	O
too	O
complicated	O
.	O
we	O
will	O
see	O
that	O
ﬁnding	O
an	O
“	O
optimal	O
”	O
separator	O
is	O
actually	O
computationally	O
prohibitive	O
,	O
and	O
so	O
will	O
need	O
to	O
“	O
relax	O
”	O
the	O
optimality	O
requirement	O
.	O
this	O
will	O
lead	O
us	O
to	O
a	O
convex	B
objective	O
that	O
combines	O
a	O
loss	O
func-	O
tion	O
(	O
how	O
well	O
are	O
we	O
doing	O
on	O
the	O
training	O
data	O
?	O
)	O
and	O
a	O
regularizer	B
(	O
how	O
complicated	O
is	O
our	O
learned	O
model	B
?	O
)	O
.	O
this	O
learning	O
framework	O
is	O
known	O
as	O
both	O
tikhonov	O
regularization	O
and	O
structural	O
risk	O
mini-	O
mization	O
.	O
6.1	O
the	O
optimization	O
framework	O
for	O
linear	O
models	O
you	O
have	O
already	O
seen	O
the	O
perceptron	O
as	O
a	O
way	O
of	O
ﬁnding	O
a	O
weight	O
vector	B
w	O
and	O
bias	B
b	O
that	O
do	O
a	O
good	O
job	O
of	O
separating	O
positive	O
train-	O
ing	O
examples	B
from	O
negative	O
training	O
examples	O
.	O
the	O
perceptron	O
is	O
a	O
model	B
and	O
algorithm	B
in	O
one	O
.	O
here	O
,	O
we	O
are	O
interested	O
in	O
separating	O
these	O
issues	O
.	O
we	O
will	O
focus	O
on	O
linear	O
models	O
,	O
like	O
the	O
perceptron	O
.	O
but	O
we	O
will	O
think	O
about	O
other	O
,	O
more	O
generic	O
ways	O
of	O
ﬁnding	O
good	O
parameters	O
of	O
these	O
models	O
.	O
the	O
goal	O
of	O
the	O
perceptron	O
was	O
to	O
ﬁnd	O
a	O
separating	B
hyperplane	I
for	O
some	O
training	B
data	I
set	O
.	O
for	O
simplicity	O
,	O
you	O
can	O
ignore	O
the	O
issue	O
of	O
overﬁtting	O
(	O
but	O
just	O
for	O
now	O
!	O
)	O
.	O
not	O
all	O
data	O
sets	O
are	O
linearly	O
sepa-	O
linear	O
models	O
87	O
you	O
should	O
remember	O
the	O
yw	O
·	O
trick	O
from	O
the	O
perceptron	O
discus-	O
sion	O
.	O
if	O
not	O
,	O
re-convince	O
yourself	O
that	O
this	O
is	O
doing	O
the	O
right	O
thing	O
.	O
?	O
x	O
rable	O
.	O
in	O
the	O
case	O
that	O
your	O
training	B
data	I
isn	O
’	O
t	O
linearly	B
separable	I
,	O
you	O
might	O
want	O
to	O
ﬁnd	O
the	O
hyperplane	O
that	O
makes	O
the	O
fewest	O
errors	O
on	O
the	O
training	O
data	O
.	O
we	O
can	O
write	O
this	O
down	O
as	O
a	O
formal	O
mathematics	O
optimization	B
problem	I
as	O
follows	O
:	O
min	O
w	O
,	O
b	O
∑	O
n	O
1	O
[	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
>	O
0	O
]	O
(	O
6.1	O
)	O
in	O
this	O
expression	O
,	O
you	O
are	O
optimizing	O
over	O
two	O
variables	O
,	O
w	O
and	O
b.	O
the	O
objective	O
function	O
is	O
the	O
thing	O
you	O
are	O
trying	O
to	O
minimize	O
.	O
in	O
this	O
case	O
,	O
the	O
objective	O
function	O
is	O
simply	O
the	O
error	O
rate	O
(	O
or	O
0/1	B
loss	I
)	O
of	O
the	O
linear	O
classiﬁer	O
parameterized	O
by	O
w	O
,	O
b.	O
in	O
this	O
expression	O
,	O
1	O
[	O
·	O
]	O
is	O
the	O
indicator	O
function	O
:	O
it	O
is	O
one	O
when	O
(	O
·	O
)	O
is	O
true	O
and	O
zero	O
otherwise	O
.	O
we	O
know	O
that	O
the	O
perceptron	O
algorithm	B
is	O
guaranteed	O
to	O
ﬁnd	O
parameters	O
for	O
this	O
model	B
if	O
the	O
data	O
is	O
linearly	B
separable	I
.	O
in	O
other	O
words	O
,	O
if	O
the	O
optimum	O
of	O
eq	O
(	O
6.1	O
)	O
is	O
zero	O
,	O
then	O
the	O
perceptron	O
will	O
efﬁciently	O
ﬁnd	O
parameters	O
for	O
this	O
model	B
.	O
the	O
notion	O
of	O
“	O
efﬁciency	O
”	O
depends	O
on	O
the	O
margin	O
of	O
the	O
data	O
for	O
the	O
perceptron	O
.	O
you	O
might	O
ask	O
:	O
what	O
happens	O
if	O
the	O
data	O
is	O
not	O
linearly	B
separable	I
?	O
is	O
there	O
an	O
efﬁcient	O
algorithm	B
for	O
ﬁnding	O
an	O
optimal	O
setting	O
of	O
the	O
parameters	O
?	O
unfortunately	O
,	O
the	O
answer	O
is	O
no	O
.	O
there	O
is	O
no	O
polynomial	O
time	O
algorithm	B
for	O
solving	O
eq	O
(	O
6.1	O
)	O
,	O
unless	O
p=np	O
.	O
in	O
other	O
words	O
,	O
this	O
problem	O
is	O
np-hard	O
.	O
sadly	O
,	O
the	O
proof	O
of	O
this	O
is	O
quite	O
complicated	O
and	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
,	O
but	O
it	O
relies	O
on	O
a	O
reduction	O
from	O
a	O
variant	O
of	O
satisﬁability	O
.	O
the	O
key	O
idea	O
is	O
to	O
turn	O
a	O
satisﬁability	O
problem	O
into	O
an	O
optimization	B
problem	I
where	O
a	O
clause	O
is	O
satisﬁed	O
exactly	O
when	O
the	O
hyperplane	O
correctly	O
separates	O
the	O
data	O
.	O
you	O
might	O
then	O
come	O
back	O
and	O
say	O
:	O
okay	O
,	O
well	O
i	O
don	O
’	O
t	O
really	O
need	O
an	O
exact	O
solution	O
.	O
i	O
’	O
m	O
willing	O
to	O
have	O
a	O
solution	O
that	O
makes	O
one	O
or	O
two	O
more	O
errors	O
than	O
it	O
has	O
to	O
.	O
unfortunately	O
,	O
the	O
situation	O
is	O
really	O
bad	O
.	O
zero/one	B
loss	I
is	O
np-hard	O
to	O
even	O
appproximately	O
minimize	O
.	O
in	O
other	O
words	O
,	O
there	O
is	O
no	O
efﬁcient	O
algorithm	B
for	O
even	O
ﬁnding	O
a	O
solution	O
that	O
’	O
s	O
a	O
small	O
constant	O
worse	O
than	O
optimal	O
.	O
(	O
the	O
best	O
known	O
constant	O
at	O
this	O
time	O
is	O
418/415	O
≈	O
1.007	O
.	O
)	O
however	O
,	O
before	O
getting	O
too	O
disillusioned	O
about	O
this	O
whole	O
enter-	O
prise	O
(	O
remember	O
:	O
there	O
’	O
s	O
an	O
entire	O
chapter	O
about	O
this	O
framework	O
,	O
so	O
it	O
must	O
be	O
going	O
somewhere	O
!	O
)	O
,	O
you	O
should	O
remember	O
that	O
optimizing	O
eq	O
(	O
6.1	O
)	O
perhaps	O
isn	O
’	O
t	O
even	O
what	O
you	O
want	O
to	O
do	O
!	O
in	O
particular	O
,	O
all	O
it	O
says	O
is	O
that	O
you	O
will	O
get	O
minimal	O
training	B
error	I
.	O
it	O
says	O
nothing	O
about	O
what	O
your	O
test	B
error	I
will	O
be	O
like	O
.	O
in	O
order	O
to	O
try	O
to	O
ﬁnd	O
a	O
solution	O
that	O
will	O
generalize	B
well	O
to	O
test	B
data	I
,	O
you	O
need	O
to	O
ensure	O
that	O
you	O
do	O
not	O
overﬁt	O
the	O
data	O
.	O
to	O
do	O
this	O
,	O
you	O
can	O
introduce	O
a	O
regularizer	B
over	O
the	O
parameters	O
of	O
the	O
model	O
.	O
for	O
now	O
,	O
we	O
will	O
be	O
vague	O
about	O
what	O
this	O
regularizer	B
looks	O
like	O
,	O
and	O
simply	O
call	O
it	O
an	O
arbitrary	O
function	O
r	O
(	O
w	O
,	O
b	O
)	O
.	O
88	O
a	O
course	O
in	O
machine	O
learning	O
this	O
leads	O
to	O
the	O
following	O
,	O
regularized	B
objective	I
:	O
min	O
w	O
,	O
b	O
∑	O
n	O
1	O
[	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
>	O
0	O
]	O
+	O
λr	O
(	O
w	O
,	O
b	O
)	O
(	O
6.2	O
)	O
in	O
eq	O
(	O
6.2	O
)	O
,	O
we	O
are	O
now	O
trying	O
to	O
optimize	O
a	O
trade-off	O
between	O
a	O
so-	O
lution	O
that	O
gives	O
low	O
training	B
error	I
(	O
the	O
ﬁrst	O
term	O
)	O
and	O
a	O
solution	O
that	O
is	O
“	O
simple	O
”	O
(	O
the	O
second	O
term	O
)	O
.	O
you	O
can	O
think	O
of	O
the	O
maximum	O
depth	O
hyperparameter	B
of	O
a	O
decision	B
tree	I
as	O
a	O
form	O
of	O
regularization	O
for	O
trees	O
.	O
here	O
,	O
r	O
is	O
a	O
form	O
of	O
regularization	O
for	O
hyperplanes	O
.	O
in	O
this	O
formulation	O
,	O
λ	O
becomes	O
a	O
hyperparameter	B
for	O
the	O
optimization	O
.	O
the	O
key	O
remaining	O
questions	O
,	O
given	O
this	O
formalism	O
,	O
are	O
:	O
•	O
how	O
can	O
we	O
adjust	O
the	O
optimization	O
problem	O
so	O
that	O
there	O
are	O
efﬁcient	O
algorithms	O
for	O
solving	O
it	O
?	O
•	O
what	O
are	O
good	O
regularizers	O
r	O
(	O
w	O
,	O
b	O
)	O
for	O
hyperplanes	O
?	O
•	O
assuming	O
we	O
can	O
adjust	O
the	O
optimization	O
problem	O
appropriately	O
,	O
what	O
algorithms	O
exist	O
for	O
efﬁciently	O
solving	O
this	O
regularized	O
opti-	O
mization	O
problem	O
?	O
we	O
will	O
address	O
these	O
three	O
questions	O
in	O
the	O
next	O
sections	O
.	O
6.2	O
convex	B
surrogate	O
loss	O
functions	O
you	O
might	O
ask	O
:	O
why	O
is	O
optimizing	O
zero/one	B
loss	I
so	O
hard	O
?	O
intuitively	O
,	O
one	O
reason	O
is	O
that	O
small	O
changes	O
to	O
w	O
,	O
b	O
can	O
have	O
a	O
large	O
impact	O
on	O
the	O
value	O
of	O
the	O
objective	O
function	O
.	O
for	O
instance	O
,	O
if	O
there	O
is	O
a	O
positive	O
training	O
example	O
with	O
w	O
,	O
x	O
·	O
+b	O
=	O
−0.0000001	O
,	O
then	O
adjusting	O
b	O
up-	O
wards	O
by	O
0.00000011	O
will	O
decrease	O
your	O
error	B
rate	I
by	O
1.	O
but	O
adjusting	O
it	O
upwards	O
by	O
0.00000009	O
will	O
have	O
no	O
effect	O
.	O
this	O
makes	O
it	O
really	O
difﬁcult	O
to	O
ﬁgure	O
out	O
good	O
ways	O
to	O
adjust	O
the	O
parameters	O
.	O
to	O
see	O
this	O
more	O
clearly	O
,	O
it	O
is	O
useful	O
to	O
look	O
at	O
plots	O
that	O
relate	O
margin	B
to	O
loss	O
.	O
such	O
a	O
plot	O
for	O
zero/one	B
loss	I
is	O
shown	O
in	O
figure	O
6.1.	O
in	O
this	O
plot	O
,	O
the	O
horizontal	O
axis	O
measure	O
the	O
margin	O
of	O
a	O
data	O
point	O
and	O
the	O
vertical	O
axis	O
measures	O
the	O
loss	O
associated	O
with	O
that	O
margin	B
.	O
for	O
zero/one	B
loss	I
,	O
the	O
story	O
is	O
simple	O
.	O
if	O
you	O
get	O
a	O
positive	O
margin	O
(	O
i.e.	O
,	O
y	O
(	O
w	O
·	O
x	O
+	O
b	O
)	O
>	O
0	O
)	O
then	O
you	O
get	O
a	O
loss	O
of	O
zero	O
.	O
otherwise	O
you	O
get	O
a	O
loss	O
of	O
one	O
.	O
by	O
thinking	O
about	O
this	O
plot	O
,	O
you	O
can	O
see	O
how	O
changes	O
to	O
the	O
parameters	O
that	O
change	O
the	O
margin	O
just	O
a	O
little	O
bit	O
can	O
have	O
an	O
enormous	O
effect	O
on	O
the	O
overall	O
loss	O
.	O
you	O
might	O
decide	O
that	O
a	O
reasonable	O
way	O
to	O
address	O
this	O
problem	O
is	O
to	O
replace	O
the	O
non-smooth	O
zero/one	B
loss	I
with	O
a	O
smooth	O
approxima-	O
tion	O
.	O
with	O
a	O
bit	O
of	O
effort	O
,	O
you	O
could	O
probably	O
concoct	O
an	O
“	O
s	O
”	O
-shaped	O
function	O
like	O
that	O
shown	O
in	O
figure	O
6.2.	O
the	O
beneﬁt	O
of	O
using	O
such	O
an	O
s-function	O
is	O
that	O
it	O
is	O
smooth	O
,	O
and	O
potentially	O
easier	O
to	O
optimize	O
.	O
the	O
difﬁculty	O
is	O
that	O
it	O
is	O
not	O
convex	B
.	O
?	O
assuming	O
r	O
does	O
the	O
“	O
right	O
thing	O
,	O
”	O
what	O
value	O
(	O
s	O
)	O
of	O
λ	O
will	O
lead	O
to	O
over-	O
ﬁtting	O
?	O
what	O
value	O
(	O
s	O
)	O
will	O
lead	O
to	O
underﬁtting	O
?	O
figure	O
6.1	O
:	O
plot	O
of	O
zero/one	O
versus	O
margin	B
figure	O
6.2	O
:	O
plot	O
of	O
zero/one	O
versus	O
margin	B
and	O
an	O
s	O
version	O
of	O
it	O
linear	O
models	O
89	O
figure	O
6.4	O
:	O
surrogate	B
loss	I
fns	O
if	O
you	O
remember	O
from	O
calculus	O
,	O
a	O
convex	B
function	O
is	O
one	O
that	O
looks	O
like	O
a	O
happy	O
face	O
(	O
(	O
cid:94	O
)	O
)	O
.	O
(	O
on	O
the	O
other	O
hand	O
,	O
a	O
concave	B
function	O
is	O
one	O
that	O
looks	O
like	O
a	O
sad	O
face	O
(	O
(	O
cid:95	O
)	O
)	O
;	O
an	O
easy	O
mnemonic	O
is	O
that	O
you	O
can	O
hide	O
under	O
a	O
concave	B
function	O
.	O
)	O
there	O
are	O
two	O
equivalent	O
deﬁnitions	O
of	O
a	O
concave	B
function	O
.	O
the	O
ﬁrst	O
is	O
that	O
it	O
’	O
s	O
second	O
derivative	O
is	O
always	O
non-negative	O
.	O
the	O
second	O
,	O
more	O
geometric	O
,	O
deﬁtion	O
is	O
that	O
any	O
chord	B
of	O
the	O
function	O
lies	O
above	O
it	O
.	O
this	O
is	O
shown	O
in	O
figure	O
?	O
?	O
.	O
there	O
you	O
can	O
see	O
a	O
convex	B
function	O
and	O
a	O
non-convex	B
function	O
,	O
both	O
with	O
two	O
chords	O
drawn	O
in	O
.	O
in	O
the	O
case	O
of	O
the	O
convex	O
function	O
,	O
the	O
chords	O
lie	O
above	O
the	O
function	O
.	O
in	O
the	O
case	O
of	O
the	O
non-convex	O
function	O
,	O
there	O
are	O
parts	O
of	O
the	O
chord	O
that	O
lie	O
below	O
the	O
function	O
.	O
convex	B
functions	O
are	O
nice	O
because	O
they	O
are	O
easy	O
to	O
minimize	O
.	O
intu-	O
itively	O
,	O
if	O
you	O
drop	O
a	O
ball	O
anywhere	O
in	O
a	O
convex	B
function	O
,	O
it	O
will	O
even-	O
tually	O
get	O
to	O
the	O
minimum	O
.	O
this	O
is	O
not	O
true	O
for	O
non-convex	B
functions	O
.	O
for	O
example	O
,	O
if	O
you	O
drop	O
a	O
ball	O
on	O
the	O
very	O
left	O
end	O
of	O
the	O
s-function	O
from	O
figure	O
6.2	O
,	O
it	O
will	O
not	O
go	O
anywhere	O
.	O
this	O
leads	O
to	O
the	O
idea	O
of	O
convex	B
surrogate	O
loss	O
functions	O
.	O
since	O
zero/one	B
loss	I
is	O
hard	O
to	O
optimize	O
,	O
you	O
want	O
to	O
optimize	O
something	O
else	O
,	O
instead	O
.	O
since	O
convex	B
functions	O
are	O
easy	O
to	O
optimize	O
,	O
we	O
want	O
to	O
approximate	O
zero/one	B
loss	I
with	O
a	O
convex	B
function	O
.	O
this	O
approxi-	O
mating	O
function	O
will	O
be	O
called	O
a	O
surrogate	B
loss	I
.	O
the	O
surrogate	O
losses	O
we	O
construct	O
will	O
always	O
be	O
upper	O
bounds	O
on	O
the	O
true	O
loss	B
function	I
:	O
this	O
guarantees	O
that	O
if	O
you	O
minimize	O
the	O
surrogate	O
loss	O
,	O
you	O
are	O
also	O
pushing	O
down	O
the	O
real	O
loss	O
.	O
there	O
are	O
four	O
common	O
surrogate	B
loss	I
function	O
,	O
each	O
with	O
their	O
own	O
properties	O
:	O
hinge	B
loss	I
,	O
logistic	B
loss	I
,	O
exponential	B
loss	I
and	O
squared	B
loss	I
.	O
these	O
are	O
shown	O
in	O
figure	O
6.4	O
and	O
deﬁned	O
below	O
.	O
these	O
are	O
deﬁned	O
in	O
terms	O
of	O
the	O
true	O
label	B
y	O
(	O
which	O
is	O
just	O
{	O
−1	O
,	O
+1	O
}	O
)	O
and	O
the	O
predicted	O
value	O
ˆy	O
=	O
w	O
·	O
x	O
+	O
b.	O
zero/one	O
:	O
hinge	O
:	O
logistic	O
:	O
exponential	O
:	O
squared	O
:	O
(	O
cid:96	O
)	O
(	O
0/1	O
)	O
(	O
y	O
,	O
ˆy	O
)	O
=	O
1	O
[	O
y	O
ˆy	O
≤	O
0	O
]	O
(	O
cid:96	O
)	O
(	O
hin	O
)	O
(	O
y	O
,	O
ˆy	O
)	O
=	O
max	O
{	O
0	O
,	O
1	O
−	O
y	O
ˆy	O
}	O
(	O
cid:96	O
)	O
(	O
log	O
)	O
(	O
y	O
,	O
ˆy	O
)	O
=	O
(	O
cid:96	O
)	O
(	O
exp	O
)	O
(	O
y	O
,	O
ˆy	O
)	O
=	O
exp	O
[	O
−y	O
ˆy	O
]	O
(	O
cid:96	O
)	O
(	O
sqr	O
)	O
(	O
y	O
,	O
ˆy	O
)	O
=	O
(	O
y	O
−	O
ˆy	O
)	O
2	O
log	O
2	O
1	O
log	O
(	O
1	O
+	O
exp	O
[	O
−y	O
ˆy	O
]	O
)	O
(	O
6.3	O
)	O
(	O
6.4	O
)	O
(	O
6.5	O
)	O
(	O
6.6	O
)	O
(	O
6.7	O
)	O
1	O
log	O
2	O
term	O
out	O
front	O
is	O
there	O
sim-	O
in	O
the	O
deﬁnition	O
of	O
logistic	B
loss	I
,	O
the	O
ply	O
to	O
ensure	O
that	O
(	O
cid:96	O
)	O
(	O
log	O
)	O
(	O
y	O
,	O
0	O
)	O
=	O
1.	O
this	O
ensures	O
,	O
like	O
all	O
the	O
other	O
surrogate	B
loss	I
functions	O
,	O
that	O
logistic	B
loss	I
upper	O
bounds	O
the	O
zero/one	O
loss	O
.	O
(	O
in	O
practice	O
,	O
people	O
typically	O
omit	O
this	O
constant	O
since	O
it	O
does	O
not	O
affect	O
the	O
optimization	O
.	O
)	O
there	O
are	O
two	O
big	O
differences	O
in	O
these	O
loss	O
functions	O
.	O
the	O
ﬁrst	O
difference	O
is	O
how	O
“	O
upset	O
”	O
they	O
get	O
by	O
erroneous	O
predictions	O
.	O
in	O
the	O
90	O
a	O
course	O
in	O
machine	O
learning	O
case	O
of	O
hinge	B
loss	I
and	O
logistic	B
loss	I
,	O
the	O
growth	O
of	O
the	O
function	O
as	O
ˆy	O
goes	O
negative	O
is	O
linear	O
.	O
for	O
squared	B
loss	I
and	O
exponential	B
loss	I
,	O
it	O
is	O
super-linear	O
.	O
this	O
means	O
that	O
exponential	B
loss	I
would	O
rather	O
get	O
a	O
few	O
examples	B
a	O
little	O
wrong	O
than	O
one	O
example	O
really	O
wrong	O
.	O
the	O
other	O
difference	O
is	O
how	O
they	O
deal	O
with	O
very	O
conﬁdent	O
correct	O
predictions	O
.	O
once	O
y	O
ˆy	O
>	O
1	O
,	O
hinge	B
loss	I
does	O
not	O
care	O
any	O
more	O
,	O
but	O
logistic	O
and	O
exponential	O
still	O
think	O
you	O
can	O
do	O
better	O
.	O
on	O
the	O
other	O
hand	O
,	O
squared	B
loss	I
thinks	O
it	O
’	O
s	O
just	O
as	O
bad	O
to	O
predict	B
+3	O
on	O
a	O
positive	O
example	O
as	O
it	O
is	O
to	O
predict	B
−1	O
on	O
a	O
positive	O
example	O
.	O
6.3	O
weight	O
regularization	O
in	O
our	O
learning	O
objective	O
,	O
eq	O
(	O
?	O
?	O
)	O
,	O
we	O
had	O
a	O
term	O
correspond	O
to	O
the	O
zero/one	O
loss	O
on	O
the	O
training	O
data	O
,	O
plus	O
a	O
regularizer	B
whose	O
goal	O
was	O
to	O
ensure	O
that	O
the	O
learned	O
function	O
didn	O
’	O
t	O
get	O
too	O
“	O
crazy.	O
”	O
(	O
or	O
,	O
more	O
formally	O
,	O
to	O
ensure	O
that	O
the	O
function	O
did	O
not	O
overﬁt	O
.	O
)	O
if	O
you	O
re-	O
place	O
to	O
zero/one	B
loss	I
with	O
a	O
surrogate	B
loss	I
,	O
you	O
obtain	O
the	O
following	O
objective	O
:	O
min	O
w	O
,	O
b	O
∑	O
n	O
(	O
cid:96	O
)	O
(	O
yn	O
,	O
w	O
·	O
xn	O
+	O
b	O
)	O
+	O
λr	O
(	O
w	O
,	O
b	O
)	O
(	O
6.8	O
)	O
the	O
question	O
is	O
:	O
what	O
should	O
r	O
(	O
w	O
,	O
b	O
)	O
look	O
like	O
?	O
from	O
the	O
discussion	O
of	O
surrogate	B
loss	I
function	O
,	O
we	O
would	O
like	O
to	O
ensure	O
that	O
r	O
is	O
convex	B
.	O
otherwise	O
,	O
we	O
will	O
be	O
back	O
to	O
the	O
point	O
where	O
optimization	O
becomes	O
difﬁcult	O
.	O
beyond	O
that	O
,	O
a	O
common	O
desire	O
is	O
that	O
the	O
components	O
of	O
the	O
weight	O
vector	B
(	O
i.e.	O
,	O
the	O
wds	O
)	O
should	O
be	O
small	O
(	O
close	O
to	O
zero	O
)	O
.	O
this	O
is	O
a	O
form	O
of	O
inductive	B
bias	I
.	O
why	O
are	O
small	O
values	O
of	O
wd	O
good	O
?	O
or	O
,	O
more	O
precisely	O
,	O
why	O
do	O
small	O
values	O
of	O
wd	O
correspond	O
to	O
simple	O
functions	O
?	O
suppose	O
that	O
we	O
have	O
an	O
example	O
x	O
with	O
label	B
+1	O
.	O
we	O
might	O
believe	O
that	O
other	O
ex-	O
amples	O
,	O
x	O
(	O
cid:48	O
)	O
that	O
are	O
nearby	O
x	O
should	O
also	O
have	O
label	B
+1	O
.	O
for	O
example	O
,	O
if	O
i	O
obtain	O
x	O
(	O
cid:48	O
)	O
by	O
taking	O
x	O
and	O
changing	O
the	O
ﬁrst	O
component	O
by	O
some	O
small	O
value	O
	O
and	O
leaving	O
the	O
rest	O
the	O
same	O
,	O
you	O
might	O
think	O
that	O
the	O
classiﬁcation	O
would	O
be	O
the	O
same	O
.	O
if	O
you	O
do	O
this	O
,	O
the	O
difference	O
be-	O
tween	O
ˆy	O
and	O
ˆy	O
(	O
cid:48	O
)	O
will	O
be	O
exactly	O
w1	O
.	O
so	O
if	O
w1	O
is	O
reasonably	O
small	O
,	O
this	O
is	O
unlikely	O
to	O
have	O
much	O
of	O
an	O
effect	O
on	O
the	O
classiﬁcation	O
decision	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
w1	O
is	O
large	O
,	O
this	O
could	O
have	O
a	O
large	O
effect	O
.	O
another	O
way	O
of	O
saying	O
the	O
same	O
thing	O
is	O
to	O
look	O
at	O
the	O
derivative	O
of	O
the	O
predictions	O
as	O
a	O
function	O
of	O
w1	O
.	O
the	O
derivative	O
of	O
w	O
,	O
x	O
·	O
+b	O
with	O
respect	O
to	O
w1	O
is	O
:	O
=	O
∂	O
[	O
∑d	O
wdxd	O
+	O
b	O
]	O
∂w1	O
=	O
x1	O
(	O
6.9	O
)	O
∂w	O
,	O
x	O
·	O
+b	O
∂w1	O
interpreting	O
the	O
derivative	O
as	O
the	O
rate	O
of	O
change	O
,	O
we	O
can	O
see	O
that	O
the	O
rate	O
of	O
change	O
of	O
the	O
prediction	O
function	O
is	O
proportional	O
to	O
the	O
(	O
cid:113	O
)	O
individual	O
weights	B
.	O
so	O
if	O
you	O
want	O
the	O
function	O
to	O
change	O
slowly	O
,	O
you	O
want	O
to	O
ensure	O
that	O
the	O
weights	O
stay	O
small	O
.	O
one	O
way	O
to	O
accomplish	O
this	O
is	O
to	O
simply	O
use	O
the	O
norm	O
of	O
the	O
∑d	O
w2	O
weight	O
vector	B
.	O
namely	O
r	O
(	O
norm	O
)	O
(	O
w	O
,	O
b	O
)	O
=	O
||w||	O
=	O
is	O
convex	B
and	O
smooth	O
,	O
which	O
makes	O
it	O
easy	O
to	O
minimize	O
.	O
in	O
prac-	O
tice	O
,	O
it	O
’	O
s	O
often	O
easier	O
to	O
use	O
the	O
squared	O
norm	O
,	O
namely	O
r	O
(	O
sqr	O
)	O
(	O
w	O
,	O
b	O
)	O
=	O
||w||2	O
=	O
∑d	O
w2	O
remains	O
convex	B
.	O
an	O
alternative	O
to	O
using	O
the	O
sum	O
of	O
squared	O
weights	O
is	O
to	O
use	O
the	O
sum	O
of	O
absolute	O
weights	O
:	O
r	O
(	O
abs	O
)	O
(	O
w	O
,	O
b	O
)	O
=	O
∑d	O
|wd|	O
.	O
both	O
of	O
these	O
norms	O
are	O
convex	B
.	O
d	O
because	O
it	O
removes	O
the	O
ugly	O
square	O
root	O
term	O
and	O
d.	O
this	O
function	O
in	O
addition	O
to	O
small	O
weights	B
being	O
good	O
,	O
you	O
could	O
argue	O
that	O
zero	O
weights	B
are	O
better	O
.	O
if	O
a	O
weight	O
wd	O
goes	O
to	O
zero	O
,	O
then	O
this	O
means	O
that	O
feature	O
d	O
is	O
not	O
used	O
at	O
all	O
in	O
the	O
classiﬁcation	O
decision	O
.	O
if	O
there	O
are	O
a	O
large	O
number	O
of	O
irrelevant	O
features	B
,	O
you	O
might	O
want	O
as	O
many	O
weights	B
to	O
go	O
to	O
zero	O
as	O
possible	O
.	O
this	O
suggests	O
an	O
alternative	O
regularizer	B
:	O
r	O
(	O
cnt	O
)	O
(	O
w	O
,	O
b	O
)	O
=	O
∑d	O
1	O
[	O
xd	O
(	O
cid:54	O
)	O
=	O
0	O
]	O
.	O
this	O
line	O
of	O
thinking	O
leads	O
to	O
the	O
general	O
concept	B
of	O
p-norms	B
.	O
(	O
technically	O
these	O
are	O
called	O
(	O
cid:96	O
)	O
p	O
(	O
or	O
“	O
ell	O
p	O
”	O
)	O
norms	O
,	O
but	O
this	O
notation	O
clashes	O
with	O
the	O
use	O
of	O
(	O
cid:96	O
)	O
for	O
“	O
loss.	O
”	O
)	O
this	O
is	O
a	O
family	O
of	O
norms	O
that	O
all	O
have	O
the	O
same	O
general	O
ﬂavor	O
.	O
we	O
write	O
||w||p	O
to	O
denote	O
the	O
p-norm	O
of	O
w.	O
(	O
cid:32	O
)	O
(	O
cid:33	O
)	O
1	O
p	O
||w||p	O
=	O
|wd|p	O
∑	O
d	O
(	O
6.10	O
)	O
you	O
can	O
check	O
that	O
the	O
2-norm	O
exactly	O
corresponds	O
to	O
the	O
usual	O
eu-	O
clidean	O
norm	O
,	O
and	O
that	O
the	O
1-norm	O
corresponds	O
to	O
the	O
“	O
absolute	O
”	O
regularizer	B
described	O
above	O
.	O
when	O
p-norms	B
are	O
used	O
to	O
regularize	O
weight	O
vectors	O
,	O
the	O
interest-	O
ing	O
aspect	O
is	O
how	O
they	O
trade-off	O
multiple	O
features	B
.	O
to	O
see	O
the	O
behavior	O
of	O
p-norms	B
in	O
two	O
dimensions	O
,	O
we	O
can	O
plot	O
their	O
contour	B
(	O
or	O
level-	O
set	O
)	O
.	O
figure	O
6.5	O
shows	O
the	O
contours	O
for	O
the	O
same	O
p	O
norms	O
in	O
two	O
dimensions	O
.	O
each	O
line	O
denotes	O
the	O
two-dimensional	O
vectors	O
to	O
which	O
this	O
norm	O
assignes	O
a	O
total	O
value	O
of	O
1.	O
by	O
changing	O
the	O
value	O
of	O
p	O
,	O
you	O
can	O
interpolate	O
between	O
a	O
square	O
(	O
the	O
so-called	O
“	O
max	O
norm	O
”	O
)	O
,	O
down	O
to	O
a	O
circle	O
(	O
2-norm	O
)	O
,	O
diamond	O
(	O
1-norm	O
)	O
and	O
pointy-star-shaped-thing	O
(	O
p	O
<	O
1	O
norm	O
)	O
.	O
in	O
general	O
,	O
smaller	O
values	O
of	O
p	O
“	O
prefer	O
”	O
sparser	O
vectors	O
.	O
you	O
can	O
see	O
this	O
by	O
noticing	O
that	O
the	O
contours	O
of	O
small	O
p-norms	B
“	O
stretch	O
”	O
out	O
along	O
the	O
axes	O
.	O
it	O
is	O
for	O
this	O
reason	O
that	O
small	O
p-norms	B
tend	O
to	O
yield	O
weight	O
vectors	O
with	O
many	O
zero	O
entries	O
(	O
aka	O
sparse	B
weight	O
vec-	O
tors	O
)	O
.	O
unfortunately	O
,	O
for	O
p	O
<	O
1	O
the	O
norm	O
becomes	O
non-convex	B
.	O
as	O
you	O
might	O
guess	O
,	O
this	O
means	O
that	O
the	O
1-norm	O
is	O
a	O
popular	O
choice	O
for	O
sparsity-seeking	O
applications	O
.	O
linear	O
models	O
91	O
?	O
why	O
do	O
we	O
not	O
regularize	O
the	O
bias	O
term	O
b	O
?	O
?	O
why	O
might	O
you	O
not	O
want	O
to	O
use	O
r	O
(	O
cnt	O
)	O
as	O
a	O
regularizer	B
?	O
?	O
you	O
can	O
actually	O
identify	O
the	O
r	O
(	O
cnt	O
)	O
regularizer	B
with	O
a	O
p-norm	O
as	O
well	O
.	O
which	O
value	O
of	O
p	O
gives	O
it	O
to	O
you	O
?	O
(	O
hint	O
:	O
you	O
may	O
have	O
to	O
take	O
a	O
limit	O
.	O
)	O
figure	O
6.5	O
:	O
loss	O
:	O
norms2d	O
:	O
level	O
sets	O
of	O
the	O
same	O
p-norms	B
?	O
the	O
max	O
norm	O
corresponds	O
to	O
limp→∞	O
.	O
why	O
is	O
this	O
called	O
the	O
max	O
norm	O
?	O
92	O
a	O
course	O
in	O
machine	O
learning	O
math	O
review	O
|	O
gradients	O
...	O
be	O
sure	O
to	O
do	O
enough	O
to	O
do	O
the	O
closed	O
for	O
squared	O
error	O
algorithm	B
22	O
gradientdescent	O
(	O
f	O
,	O
k	O
,	O
η1	O
,	O
.	O
.	O
.	O
)	O
1	O
:	O
z	O
(	O
0	O
)	O
←	O
(	O
cid:104	O
)	O
0	O
,	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
(	O
cid:105	O
)	O
2	O
:	O
for	O
k	O
=	O
1	O
.	O
.	O
.	O
k	O
do	O
g	O
(	O
k	O
)	O
←	O
∇zf|z	O
(	O
k-1	O
)	O
3	O
:	O
z	O
(	O
k	O
)	O
←	O
z	O
(	O
k-1	O
)	O
−	O
η	O
(	O
k	O
)	O
g	O
(	O
k	O
)	O
4	O
:	O
5	O
:	O
end	O
for	O
6	O
:	O
return	O
z	O
(	O
k	O
)	O
//	O
initialize	O
variable	O
we	O
are	O
optimizing	O
//	O
compute	O
gradient	B
at	O
current	O
location	O
//	O
take	O
a	O
step	O
down	O
the	O
gradient	O
figure	O
6.6	O
:	O
6.4	O
optimization	O
with	O
gradient	B
descent	I
envision	O
the	O
following	O
problem	O
.	O
you	O
’	O
re	O
taking	O
up	O
a	O
new	O
hobby	O
:	O
blindfolded	O
mountain	O
climbing	O
.	O
someone	O
blindfolds	O
you	O
and	O
drops	O
you	O
on	O
the	O
side	O
of	O
a	O
mountain	O
.	O
your	O
goal	O
is	O
to	O
get	O
to	O
the	O
peak	O
of	O
the	O
mountain	O
as	O
quickly	O
as	O
possible	O
.	O
all	O
you	O
can	O
do	O
is	O
feel	O
the	O
mountain	O
where	O
you	O
are	O
standing	O
,	O
and	O
take	O
steps	O
.	O
how	O
would	O
you	O
get	O
to	O
the	O
top	O
of	O
the	O
mountain	O
?	O
perhaps	O
you	O
would	O
feel	O
to	O
ﬁnd	O
out	O
what	O
direc-	O
tion	O
feels	O
the	O
most	O
“	O
upward	O
”	O
and	O
take	O
a	O
step	O
in	O
that	O
direction	O
.	O
if	O
you	O
do	O
this	O
repeatedly	O
,	O
you	O
might	O
hope	O
to	O
get	O
the	O
the	O
top	O
of	O
the	O
moun-	O
tain	O
.	O
(	O
actually	O
,	O
if	O
your	O
friend	O
promises	O
always	O
to	O
drop	O
you	O
on	O
purely	O
concave	B
mountains	O
,	O
you	O
will	O
eventually	O
get	O
to	O
the	O
peak	O
!	O
)	O
the	O
idea	O
of	O
gradient-based	O
methods	O
of	O
optimization	O
is	O
exactly	O
the	O
same	O
.	O
suppose	O
you	O
are	O
trying	O
to	O
ﬁnd	O
the	O
maximum	O
of	O
a	O
function	O
f	O
(	O
x	O
)	O
.	O
the	O
optimizer	O
maintains	O
a	O
current	O
estimate	O
of	O
the	O
parameter	O
of	O
interest	O
,	O
x.	O
at	O
each	O
step	O
,	O
it	O
measures	O
the	O
gradient	O
of	O
the	O
function	O
it	O
is	O
trying	O
optimize	O
.	O
this	O
measurement	O
occurs	O
at	O
the	O
current	O
location	O
,	O
x.	O
call	O
the	O
gradient	O
g.	O
it	O
then	O
takes	O
a	O
step	O
in	O
the	O
direction	O
of	O
the	O
gradient	O
,	O
where	O
the	O
size	O
of	O
the	O
step	O
is	O
controlled	O
by	O
a	O
parameter	O
η	O
(	O
eta	O
)	O
.	O
the	O
complete	O
step	O
is	O
x	O
←	O
x	O
+	O
ηg	O
.	O
this	O
is	O
the	O
basic	O
idea	O
of	O
gradient	B
ascent	I
.	O
the	O
opposite	O
of	O
gradient	B
ascent	I
is	O
gradient	B
descent	I
.	O
all	O
of	O
our	O
learning	O
problems	O
will	O
be	O
framed	O
as	O
minimization	O
problems	O
(	O
trying	O
to	O
reach	O
the	O
bottom	O
of	O
a	O
ditch	O
,	O
rather	O
than	O
the	O
top	O
of	O
a	O
hill	O
)	O
.	O
there-	O
fore	O
,	O
descent	O
is	O
the	O
primary	O
approach	O
you	O
will	O
use	O
.	O
one	O
of	O
the	O
major	O
conditions	O
for	O
gradient	B
ascent	I
being	O
able	O
to	O
ﬁnd	O
the	O
true	O
,	O
global	O
min-	O
imum	O
,	O
of	O
its	O
objective	B
function	I
is	O
convexity	O
.	O
without	O
convexity	O
,	O
all	O
is	O
lost	O
.	O
the	O
gradient	O
descent	O
algorithm	B
is	O
sketched	O
in	O
algorithm	B
6.4.	O
the	O
function	O
takes	O
as	O
arguments	O
the	O
function	O
f	O
to	O
be	O
minimized	O
,	O
the	O
number	O
of	O
iterations	O
k	O
to	O
run	O
and	O
a	O
sequence	O
of	O
learning	O
rates	O
linear	O
models	O
93	O
η1	O
,	O
.	O
.	O
.	O
,	O
ηk	O
.	O
(	O
this	O
is	O
to	O
address	O
the	O
case	O
that	O
you	O
might	O
want	O
to	O
start	O
your	O
mountain	O
climbing	O
taking	O
large	O
steps	O
,	O
but	O
only	O
take	O
small	O
steps	O
when	O
you	O
are	O
close	O
to	O
the	O
peak	O
.	O
)	O
the	O
only	O
real	O
work	O
you	O
need	O
to	O
do	O
to	O
apply	O
a	O
gradient	B
descent	I
method	O
is	O
be	O
able	O
to	O
compute	O
derivatives	O
.	O
for	O
concreteness	O
,	O
suppose	O
that	O
you	O
choose	O
exponential	B
loss	I
as	O
a	O
loss	B
function	I
and	O
the	O
2-norm	O
as	O
a	O
regularizer	B
.	O
then	O
,	O
the	O
regularized	O
objective	B
function	I
is	O
:	O
exp	O
(	O
cid:2	O
)	O
−	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
(	O
cid:3	O
)	O
+	O
l	O
(	O
w	O
,	O
b	O
)	O
=	O
∑	O
n	O
||w||2	O
λ	O
2	O
(	O
6.11	O
)	O
the	O
only	O
“	O
strange	O
”	O
thing	O
in	O
this	O
objective	O
is	O
that	O
we	O
have	O
replaced	O
λ	O
with	O
λ	O
2	O
.	O
the	O
reason	O
for	O
this	O
change	O
is	O
just	O
to	O
make	O
the	O
gradients	O
cleaner	O
.	O
we	O
can	O
ﬁrst	O
compute	O
derivatives	O
with	O
respect	O
to	O
b	O
:	O
∂l	O
∂b	O
∑	O
n	O
∂	O
∂b	O
exp	O
(	O
cid:2	O
)	O
−	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
(	O
cid:3	O
)	O
+	O
exp	O
(	O
cid:2	O
)	O
−	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
(	O
cid:3	O
)	O
+	O
0	O
(	O
cid:18	O
)	O
∂	O
(	O
cid:19	O
)	O
yn	O
exp	O
(	O
cid:2	O
)	O
−	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
(	O
cid:3	O
)	O
−	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
∂b	O
=	O
∂	O
∂b	O
=	O
∑	O
n	O
=	O
∑	O
n	O
=	O
−	O
∑	O
n	O
∂	O
∂b	O
||w||2	O
λ	O
2	O
exp	O
(	O
cid:2	O
)	O
−	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
(	O
cid:3	O
)	O
(	O
6.12	O
)	O
(	O
6.13	O
)	O
(	O
6.14	O
)	O
(	O
6.15	O
)	O
before	O
proceeding	O
,	O
it	O
is	O
worth	O
thinking	O
about	O
what	O
this	O
says	O
.	O
from	O
a	O
practical	O
perspective	O
,	O
the	O
optimization	O
will	O
operate	O
by	O
updating	O
b	O
←	O
b	O
−	O
η	O
∂l	O
∂b	O
.	O
consider	O
positive	O
examples	O
:	O
examples	B
with	O
yn	O
=	O
+1	O
.	O
we	O
would	O
hope	O
for	O
these	O
examples	B
that	O
the	O
current	O
prediction	O
,	O
w	O
·	O
xn	O
+	O
b	O
,	O
is	O
as	O
large	O
as	O
possible	O
.	O
as	O
this	O
value	O
tends	O
toward	O
∞	O
,	O
the	O
term	O
in	O
the	O
exp	O
[	O
]	O
goes	O
to	O
zero	O
.	O
thus	O
,	O
such	O
points	O
will	O
not	O
contribute	O
to	O
the	O
step	O
.	O
however	O
,	O
if	O
the	O
current	O
prediction	O
is	O
small	O
,	O
then	O
the	O
exp	O
[	O
]	O
term	O
will	O
be	O
positive	O
and	O
non-zero	O
.	O
this	O
means	O
that	O
the	O
bias	O
term	O
b	O
will	O
be	O
increased	O
,	O
which	O
is	O
exactly	O
what	O
you	O
would	O
want	O
.	O
moreover	O
,	O
once	O
all	O
points	O
are	O
very	O
well	O
classiﬁed	O
,	O
the	O
derivative	O
goes	O
to	O
zero	O
.	O
now	O
that	O
we	O
have	O
done	O
the	O
easy	O
case	O
,	O
let	O
’	O
s	O
do	O
the	O
gradient	O
with	O
respect	O
to	O
w.	O
∇wl	O
=	O
∇w	O
∑	O
n	O
exp	O
(	O
cid:2	O
)	O
−	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
(	O
cid:3	O
)	O
+	O
∇w	O
(	O
∇w	O
−	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
)	O
exp	O
(	O
cid:2	O
)	O
−	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
(	O
cid:3	O
)	O
+	O
λw	O
ynxn	O
exp	O
(	O
cid:2	O
)	O
−	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
(	O
cid:3	O
)	O
+	O
λw	O
||w||2	O
λ	O
2	O
(	O
6.16	O
)	O
(	O
6.17	O
)	O
(	O
6.18	O
)	O
=	O
∑	O
n	O
=	O
−	O
∑	O
n	O
now	O
you	O
can	O
repeat	O
the	O
previous	O
exercise	O
.	O
the	O
update	O
is	O
of	O
the	O
form	O
w	O
←	O
w	O
−	O
η∇wl	O
.	O
for	O
well	O
classiﬁed	O
points	O
(	O
ones	O
that	O
are	O
tend	O
toward	O
yn∞	O
)	O
,	O
the	O
gradient	O
is	O
near	O
zero	O
.	O
for	O
poorly	O
classiﬁed	O
points	O
,	O
?	O
this	O
considered	O
the	O
case	O
of	O
posi-	O
tive	O
examples	B
.	O
what	O
happens	O
with	O
negative	O
examples	B
?	O
94	O
a	O
course	O
in	O
machine	O
learning	O
the	O
gradient	O
points	O
in	O
the	O
direction	O
−ynxn	O
,	O
so	O
the	O
update	O
is	O
of	O
the	O
form	O
w	O
←	O
w	O
+	O
cynxn	O
,	O
where	O
c	O
is	O
some	O
constant	O
.	O
this	O
is	O
just	O
like	O
the	O
perceptron	O
update	O
!	O
note	O
that	O
c	O
is	O
large	O
for	O
very	O
poorly	O
classiﬁed	O
points	O
and	O
small	O
for	O
relatively	O
well	O
classiﬁed	O
points	O
.	O
by	O
looking	O
at	O
the	O
part	O
of	O
the	O
gradient	O
related	O
to	O
the	O
regularizer	O
,	O
the	O
update	O
says	O
:	O
w	O
←	O
w	O
−	O
λw	O
=	O
(	O
1	O
−	O
λ	O
)	O
w.	O
this	O
has	O
the	O
effect	O
of	O
shrinking	O
the	O
weights	O
toward	O
zero	O
.	O
this	O
is	O
exactly	O
what	O
we	O
expect	O
the	O
regulaizer	O
to	O
be	O
doing	O
!	O
the	O
success	O
of	O
gradient	B
descent	I
hinges	O
on	O
appropriate	O
choices	O
for	O
the	O
step	O
size	O
.	O
figure	O
6.7	O
shows	O
what	O
can	O
happen	O
with	O
gradient	B
descent	I
with	O
poorly	O
chosen	O
step	O
sizes	O
.	O
if	O
the	O
step	O
size	O
is	O
too	O
big	O
,	O
you	O
can	O
accidentally	O
step	O
over	O
the	O
optimum	O
and	O
end	O
up	O
oscillating	O
.	O
if	O
the	O
step	O
size	O
is	O
too	O
small	O
,	O
it	O
will	O
take	O
way	O
too	O
long	O
to	O
get	O
to	O
the	O
optimum	O
.	O
for	O
a	O
well-chosen	O
step	O
size	O
,	O
you	O
can	O
show	O
that	O
gradient	B
descent	I
will	O
approach	O
the	O
optimal	O
value	O
at	O
a	O
fast	O
rate	O
.	O
the	O
notion	O
of	O
convergence	O
here	O
is	O
that	O
the	O
objective	O
value	O
converges	O
to	O
the	O
true	O
minimum	O
.	O
figure	O
6.7	O
:	O
good	O
and	O
bad	O
step	O
sizes	O
theorem	O
7	O
(	O
gradient	B
descent	I
convergence	O
)	O
.	O
under	O
suitable	O
condi-	O
tions1	O
,	O
for	O
an	O
appropriately	O
chosen	O
constant	O
step	O
size	O
(	O
i.e.	O
,	O
η1	O
=	O
η2	O
,	O
·	O
·	O
·	O
=	O
η	O
)	O
,	O
the	O
convergence	O
rate	O
of	O
gradient	B
descent	I
is	O
o	O
(	O
1/k	O
)	O
.	O
more	O
speciﬁ-	O
cally	O
,	O
letting	O
z∗	O
be	O
the	O
global	O
minimum	O
of	O
f	O
,	O
we	O
have	O
:	O
f	O
(	O
z	O
(	O
k	O
)	O
)	O
−	O
f	O
(	O
z∗	O
)	O
≤	O
2||z	O
(	O
0	O
)	O
−z∗||2	O
.	O
ηk	O
)	O
1	O
speciﬁcally	O
the	O
function	O
to	O
be	O
opti-	O
mized	O
needs	O
to	O
be	O
strongly	B
convex	I
.	O
this	O
is	O
true	O
for	O
all	O
our	O
problems	O
,	O
pro-	O
vided	O
λ	O
>	O
0.	O
for	O
λ	O
=	O
0	O
the	O
rate	O
could	O
be	O
as	O
bad	O
as	O
o	O
(	O
1/	O
√	O
k	O
)	O
.	O
?	O
a	O
naive	O
reading	O
of	O
this	O
theorem	O
seems	O
to	O
say	O
that	O
you	O
should	O
choose	O
huge	O
values	O
of	O
η.	O
it	O
should	O
be	O
obvi-	O
ous	O
that	O
this	O
can	O
not	O
be	O
right	O
.	O
what	O
is	O
missing	O
?	O
the	O
proof	O
of	O
this	O
theorem	O
is	O
a	O
bit	O
complicated	O
because	O
it	O
makes	O
heavy	O
use	O
of	O
some	O
linear	O
algebra	O
.	O
the	O
key	O
is	O
to	O
set	O
the	O
learning	O
rate	O
to	O
1/l	O
,	O
where	O
l	O
is	O
the	O
maximum	O
curvature	B
of	O
the	O
function	O
that	O
is	O
being	O
optimized	O
.	O
the	O
curvature	O
is	O
simply	O
the	O
“	O
size	O
”	O
of	O
the	O
second	O
derivative	O
.	O
functions	O
with	O
high	O
curvature	B
have	O
gradients	O
that	O
change	O
quickly	O
,	O
which	O
means	O
that	O
you	O
need	O
to	O
take	O
small	O
steps	O
to	O
avoid	O
overstepping	O
the	O
optimum	O
.	O
this	O
convergence	O
result	O
suggests	O
a	O
simple	O
approach	O
to	O
decid-	O
ing	O
when	O
to	O
stop	O
optimizing	O
:	O
wait	O
until	O
the	O
objective	O
function	O
stops	O
changing	O
by	O
much	O
.	O
an	O
alternative	O
is	O
to	O
wait	O
until	O
the	O
parameters	O
stop	O
changing	O
by	O
much	O
.	O
a	O
ﬁnal	O
example	O
is	O
to	O
do	O
what	O
you	O
did	O
for	O
percep-	O
tron	O
:	O
early	B
stopping	I
.	O
every	O
iteration	B
,	O
you	O
can	O
check	O
the	O
performance	O
of	O
the	O
current	O
model	B
on	O
some	O
held-out	B
data	I
,	O
and	O
stop	O
optimizing	O
when	O
performance	O
plateaus	O
.	O
6.5	O
from	O
gradients	O
to	O
subgradients	O
as	O
a	O
good	O
exercise	O
,	O
you	O
should	O
try	O
deriving	O
gradient	B
descent	I
update	O
rules	O
for	O
the	O
different	O
loss	O
functions	O
and	O
different	O
regularizers	O
you	O
’	O
ve	O
learned	O
about	O
.	O
however	O
,	O
if	O
you	O
do	O
this	O
,	O
you	O
might	O
notice	O
that	O
hinge	B
loss	I
and	O
the	O
1-norm	O
regularizer	B
are	O
not	O
differentiable	O
everywhere	O
!	O
in	O
particular	O
,	O
the	O
1-norm	O
is	O
not	O
differentiable	O
around	O
wd	O
=	O
0	O
,	O
and	O
the	O
hinge	O
loss	O
is	O
not	O
differentiable	O
around	O
y	O
ˆy	O
=	O
1.	O
linear	O
models	O
95	O
the	O
solution	O
to	O
this	O
is	O
to	O
use	O
subgradient	B
optimization	O
.	O
one	O
way	O
to	O
think	O
about	O
subgradients	O
is	O
just	O
to	O
not	O
think	O
about	O
it	O
:	O
you	O
essen-	O
tially	O
need	O
to	O
just	O
ignore	O
the	O
fact	O
that	O
you	O
forgot	O
that	O
your	O
function	O
wasn	O
’	O
t	O
differentiable	O
,	O
and	O
just	O
try	O
to	O
apply	O
gradient	B
descent	I
anyway	O
.	O
z	O
}	O
.	O
this	O
function	O
is	O
differentiable	O
for	O
z	O
>	O
1	O
and	O
differentiable	O
for	O
z	O
<	O
1	O
,	O
but	O
not	O
differentiable	O
at	O
z	O
=	O
1.	O
you	O
can	O
derive	O
this	O
using	O
differentiation	O
by	O
parts	O
:	O
to	O
be	O
more	O
concrete	O
,	O
consider	O
the	O
hinge	O
function	O
f	O
(	O
z	O
)	O
=	O
max	O
{	O
0	O
,	O
1−	O
∂	O
∂z	O
f	O
(	O
z	O
)	O
=	O
=	O
∂	O
∂z	O
0	O
1	O
−	O
z	O
(	O
cid:40	O
)	O
(	O
cid:40	O
)	O
∂	O
∂z	O
0	O
(	O
cid:40	O
)	O
∂z	O
(	O
1	O
−	O
z	O
)	O
∂	O
if	O
z	O
≥	O
1	O
0	O
−1	O
if	O
z	O
<	O
1	O
if	O
z	O
>	O
1	O
if	O
z	O
<	O
1	O
if	O
z	O
>	O
1	O
if	O
z	O
<	O
1	O
=	O
(	O
6.21	O
)	O
thus	O
,	O
the	O
derivative	O
is	O
zero	O
for	O
z	O
<	O
1	O
and	O
−1	O
for	O
z	O
>	O
1	O
,	O
matching	O
intuition	O
from	O
the	O
figure	O
.	O
at	O
the	O
non-differentiable	O
point	O
,	O
z	O
=	O
1	O
,	O
we	O
can	O
use	O
a	O
subderivative	B
:	O
a	O
generalization	O
of	O
derivatives	O
to	O
non-	O
differentiable	O
functions	O
.	O
intuitively	O
,	O
you	O
can	O
think	O
of	O
the	O
derivative	O
of	O
f	O
at	O
z	O
as	O
the	O
tangent	O
line	O
.	O
namely	O
,	O
it	O
is	O
the	O
line	O
that	O
touches	O
f	O
at	O
z	O
that	O
is	O
always	O
below	O
f	O
(	O
for	O
convex	B
functions	O
)	O
.	O
the	O
subderivative	O
,	O
denoted	O
∂∂∂	O
f	O
,	O
is	O
the	O
set	O
of	O
all	O
such	O
lines	O
.	O
at	O
differentiable	O
positions	O
,	O
this	O
set	O
consists	O
just	O
of	O
the	O
actual	O
derivative	O
.	O
at	O
non-differentiable	O
positions	O
,	O
this	O
contains	O
all	O
slopes	O
that	O
deﬁne	O
lines	O
that	O
always	O
lie	O
under	O
the	O
function	O
and	O
make	O
contact	O
at	O
the	O
operating	O
point	O
.	O
this	O
is	O
shown	O
pictorally	O
in	O
figure	O
6.8	O
,	O
where	O
example	O
subderivatives	O
are	O
shown	O
for	O
the	O
hinge	O
loss	B
function	I
.	O
in	O
the	O
particular	O
case	O
of	O
hinge	B
loss	I
,	O
any	O
value	O
between	O
0	O
and	O
−1	O
is	O
a	O
valid	O
subderivative	B
at	O
z	O
=	O
0.	O
in	O
fact	O
,	O
the	O
subderivative	O
is	O
always	O
a	O
closed	O
set	O
of	O
the	O
form	O
[	O
a	O
,	O
b	O
]	O
,	O
where	O
a	O
and	O
b	O
can	O
be	O
derived	O
by	O
looking	O
at	O
limits	O
from	O
the	O
left	O
and	O
right	O
.	O
figure	O
6.8	O
:	O
hinge	B
loss	I
with	O
sub	O
this	O
gives	O
you	O
a	O
way	O
of	O
computing	O
derivative-like	O
things	O
for	O
non-	O
differentiable	O
functions	O
.	O
take	O
hinge	B
loss	I
as	O
an	O
example	O
.	O
for	O
a	O
given	O
example	O
n	O
,	O
the	O
subgradient	O
of	O
hinge	B
loss	I
can	O
be	O
computed	O
as	O
:	O
(	O
cid:40	O
)	O
=	O
∂∂∂w	O
0	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
otherwise	O
∂∂∂w	O
max	O
{	O
0	O
,	O
1	O
−	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
}	O
(	O
cid:40	O
)	O
(	O
cid:40	O
)	O
∂∂∂w0	O
∂∂∂wyn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
otherwise	O
if	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
>	O
1	O
0	O
ynxn	O
otherwise	O
=	O
=	O
if	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
>	O
1	O
if	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
>	O
1	O
(	O
6.19	O
)	O
(	O
6.20	O
)	O
(	O
6.22	O
)	O
(	O
6.23	O
)	O
(	O
6.24	O
)	O
(	O
6.25	O
)	O
96	O
a	O
course	O
in	O
machine	O
learning	O
,	O
algorithm	B
23	O
hingeregularizedgd	O
(	O
d	O
,	O
λ	O
,	O
maxiter	O
)	O
1	O
:	O
w	O
←	O
(	O
cid:104	O
)	O
0	O
,	O
0	O
,	O
.	O
.	O
.	O
0	O
(	O
cid:105	O
)	O
2	O
:	O
for	O
iter	O
=	O
1	O
.	O
.	O
.	O
maxiter	O
do	O
g	O
←	O
(	O
cid:104	O
)	O
0	O
,	O
0	O
,	O
.	O
.	O
.	O
0	O
(	O
cid:105	O
)	O
,	O
3	O
:	O
for	O
all	O
(	O
x	O
,	O
y	O
)	O
∈	O
d	O
do	O
b	O
←	O
0	O
g	O
←	O
0	O
if	O
y	O
(	O
w	O
·	O
x	O
+	O
b	O
)	O
≤	O
1	O
then	O
4	O
:	O
//	O
initialize	O
weights	B
and	O
bias	B
//	O
initialize	O
gradient	B
of	O
weights	B
and	O
bias	B
5	O
:	O
6	O
:	O
7	O
:	O
g	O
←	O
g	O
+	O
y	O
x	O
g	O
←	O
g	O
+	O
y	O
9	O
:	O
8	O
:	O
10	O
:	O
end	O
if	O
end	O
for	O
g	O
←	O
g	O
−	O
λw	O
11	O
:	O
w	O
←	O
w	O
+	O
ηg	O
b	O
←	O
b	O
+	O
ηg	O
12	O
:	O
13	O
:	O
end	O
for	O
14	O
:	O
return	O
w	O
,	O
b	O
//	O
update	O
weight	O
gradient	B
//	O
update	O
bias	B
derivative	O
//	O
add	O
in	O
regularization	O
term	O
//	O
update	O
weights	B
//	O
update	O
bias	B
math	O
review	O
|	O
matrix	O
multiplication	O
and	O
inversion	O
.	O
.	O
.	O
figure	O
6.9	O
:	O
if	O
you	O
plug	O
this	O
subgradient	B
form	O
into	O
algorithm	B
6.4	O
,	O
you	O
obtain	O
algorithm	B
6.5.	O
this	O
is	O
the	O
subgradient	O
descent	O
for	O
regularized	O
hinge	O
loss	O
(	O
with	O
a	O
2-norm	O
regularizer	B
)	O
.	O
6.6	O
closed-form	O
optimization	O
for	O
squared	B
loss	I
although	O
gradient	B
descent	I
is	O
a	O
good	O
,	O
generic	O
optimization	O
algorithm	O
,	O
there	O
are	O
cases	O
when	O
you	O
can	O
do	O
better	O
.	O
an	O
example	O
is	O
the	O
case	O
of	O
a	O
2-norm	O
regularizer	B
and	O
squared	O
error	O
loss	B
function	I
.	O
for	O
this	O
,	O
you	O
can	O
actually	O
obtain	O
a	O
closed	O
form	O
solution	O
for	O
the	O
optimal	O
weights	B
.	O
how-	O
ever	O
,	O
to	O
obtain	O
this	O
,	O
you	O
need	O
to	O
rewrite	O
the	O
optimization	O
problem	O
in	O
terms	O
of	O
matrix	O
operations	O
.	O
for	O
simplicity	O
,	O
we	O
will	O
only	O
consider	O
the	O
unbiased	O
version	O
,	O
but	O
the	O
extension	O
is	O
exercise	O
?	O
?	O
.	O
this	O
is	O
precisely	O
the	O
linear	O
regression	O
setting	O
.	O
you	O
can	O
think	O
of	O
the	O
training	O
data	O
as	O
a	O
large	O
matrix	O
x	O
of	O
size	O
n×d	O
,	O
where	O
xn	O
,	O
d	O
is	O
the	O
value	O
of	O
the	O
dth	O
feature	O
on	O
the	O
nth	O
example	O
.	O
you	O
can	O
think	O
of	O
the	O
labels	O
as	O
a	O
column	O
(	O
“	O
tall	O
”	O
)	O
vector	B
y	O
of	O
dimension	O
n.	O
finally	O
,	O
you	O
can	O
think	O
of	O
the	O
weights	O
as	O
a	O
column	O
vector	B
w	O
of	O
size	O
d.	O
thus	O
,	O
the	O
matrix-vector	O
product	O
a	O
=	O
xw	O
has	O
dimension	O
n.	O
in	O
particular	O
:	O
an	O
=	O
[	O
xw	O
]	O
n	O
=	O
∑	O
d	O
xn	O
,	O
dwd	O
(	O
6.26	O
)	O
this	O
means	O
,	O
in	O
particular	O
,	O
that	O
a	O
is	O
actually	O
the	O
predictions	O
of	O
the	O
model	O
.	O
instead	O
of	O
calling	O
this	O
a	O
,	O
we	O
will	O
call	O
it	O
ˆy	O
.	O
the	O
squared	O
error	O
linear	O
models	O
97	O
?	O
verify	O
that	O
the	O
squared	O
error	O
can	O
actually	O
be	O
written	O
as	O
this	O
vector	B
norm	O
.	O
says	O
that	O
we	O
should	O
minimize	O
1	O
2	O
in	O
vector	B
form	O
as	O
a	O
minimization	O
of	O
1	O
2	O
this	O
can	O
be	O
expanded	O
visually	O
as	O
:	O
∑n	O
(	O
ˆyn	O
−	O
yn	O
)	O
2	O
,	O
which	O
can	O
be	O
written	O
	O
(	O
cid:124	O
)	O
x1,1	O
x2,1	O
...	O
xn,1	O
.	O
.	O
.	O
.	O
.	O
.	O
...	O
.	O
.	O
.	O
x1	O
,	O
d	O
x2	O
,	O
d	O
...	O
xn	O
,	O
d	O
x1,2	O
x2,2	O
...	O
xn,2	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
x	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
ˆy	O
−	O
y	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
2	O
.	O
	O
	O
=	O
(	O
cid:125	O
)	O
(	O
cid:124	O
)	O
∑d	O
x1	O
,	O
dwd	O
∑d	O
x2	O
,	O
dwd	O
∑d	O
xn	O
,	O
dwd	O
...	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
ˆy	O
	O
(	O
cid:125	O
)	O
	O
(	O
cid:124	O
)	O
w1	O
w2	O
...	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
wd	O
w	O
	O
	O
	O
(	O
cid:125	O
)	O
≈	O
y1	O
y2	O
...	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
yn	O
ˆy	O
(	O
6.27	O
)	O
so	O
,	O
compactly	O
,	O
our	O
optimization	B
problem	I
can	O
be	O
written	O
as	O
:	O
min	O
w	O
l	O
(	O
w	O
)	O
=	O
1	O
2	O
||xw	O
−	O
y||2	O
+	O
||w||2	O
λ	O
2	O
(	O
6.28	O
)	O
if	O
you	O
recall	B
from	O
calculus	O
,	O
you	O
can	O
minimize	O
a	O
function	O
by	O
setting	O
its	O
derivative	O
to	O
zero	O
.	O
we	O
start	O
with	O
the	O
weights	O
w	O
and	O
take	O
gradients	O
:	O
we	O
can	O
equate	O
this	O
to	O
zero	O
and	O
solve	O
,	O
yielding	O
:	O
∇wl	O
(	O
w	O
)	O
=	O
x	O
(	O
cid:62	O
)	O
(	O
xw	O
−	O
y	O
)	O
+	O
λw	O
(	O
cid:16	O
)	O
=	O
x	O
(	O
cid:62	O
)	O
xw	O
−	O
x	O
(	O
cid:62	O
)	O
y	O
+	O
λw	O
w	O
−	O
x	O
(	O
cid:62	O
)	O
y	O
=	O
(	O
cid:17	O
)	O
x	O
(	O
cid:62	O
)	O
x	O
+	O
λi	O
(	O
cid:17	O
)	O
x	O
(	O
cid:62	O
)	O
x	O
+	O
λi	O
(	O
cid:16	O
)	O
x	O
(	O
cid:62	O
)	O
x	O
+	O
λid	O
(	O
cid:17	O
)	O
w	O
−	O
x	O
(	O
cid:62	O
)	O
y	O
=	O
0	O
w	O
=	O
x	O
(	O
cid:62	O
)	O
y	O
x	O
(	O
cid:62	O
)	O
x	O
+	O
λid	O
(	O
cid:17	O
)	O
−1x	O
(	O
cid:62	O
)	O
y	O
(	O
cid:16	O
)	O
⇐⇒	O
(	O
cid:16	O
)	O
⇐⇒	O
w	O
=	O
(	O
6.29	O
)	O
(	O
6.30	O
)	O
(	O
6.31	O
)	O
(	O
6.32	O
)	O
(	O
6.33	O
)	O
(	O
6.34	O
)	O
thus	O
,	O
the	O
optimal	O
solution	O
of	O
the	O
weights	O
can	O
be	O
computed	O
by	O
a	O
few	O
matrix	O
multiplications	O
and	O
a	O
matrix	O
inversion	O
.	O
as	O
a	O
sanity	O
check	O
,	O
you	O
can	O
make	O
sure	O
that	O
the	O
dimensions	O
match	O
.	O
the	O
matrix	O
x	O
(	O
cid:62	O
)	O
x	O
has	O
dimension	O
d×d	O
,	O
and	O
therefore	O
so	O
does	O
the	O
inverse	O
term	O
.	O
the	O
inverse	O
is	O
d×d	O
and	O
x	O
(	O
cid:62	O
)	O
is	O
d×n	O
,	O
so	O
that	O
product	O
is	O
d×n	O
.	O
multiplying	O
through	O
by	O
the	O
n×1	O
vector	B
y	O
yields	O
a	O
d×1	O
vector	B
,	O
which	O
is	O
precisely	O
what	O
we	O
want	O
for	O
the	O
weights	O
.	O
note	O
that	O
this	O
gives	O
an	O
exact	O
solution	O
,	O
modulo	O
numerical	O
innacu-	O
racies	O
with	O
computing	O
matrix	O
inverses	O
.	O
in	O
contrast	O
,	O
gradient	B
descent	I
will	O
give	O
you	O
progressively	O
better	O
solutions	O
and	O
will	O
“	O
eventually	O
”	O
converge	O
to	O
the	O
optimum	O
at	O
a	O
rate	O
of	O
1/k	O
.	O
this	O
means	O
that	O
if	O
you	O
want	O
an	O
answer	O
that	O
’	O
s	O
within	O
an	O
accuracy	O
of	O
	O
=	O
10−4	O
,	O
you	O
will	O
need	O
something	O
on	O
the	O
order	O
of	O
one	O
thousand	O
steps	O
.	O
the	O
question	O
is	O
whether	O
getting	O
this	O
exact	O
solution	O
is	O
always	O
more	O
efﬁcient	O
.	O
to	O
run	O
gradient	B
descent	I
for	O
one	O
step	O
will	O
take	O
o	O
(	O
nd	O
)	O
time	O
,	O
with	O
a	O
relatively	O
small	O
constant	O
.	O
you	O
will	O
have	O
to	O
run	O
k	O
iterations	O
,	O
?	O
for	O
those	O
who	O
are	O
keen	O
on	O
linear	O
algebra	O
,	O
you	O
might	O
be	O
worried	O
that	O
the	O
matrix	O
you	O
must	O
invert	O
might	O
not	O
be	O
invertible	O
.	O
is	O
this	O
actually	O
a	O
problem	O
?	O
98	O
a	O
course	O
in	O
machine	O
learning	O
yielding	O
an	O
overall	O
runtime	O
of	O
o	O
(	O
knd	O
)	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
closed	O
form	O
solution	O
requires	O
constructing	O
x	O
(	O
cid:62	O
)	O
x	O
,	O
which	O
takes	O
o	O
(	O
d2n	O
)	O
time	O
.	O
the	O
inversion	O
take	O
o	O
(	O
d3	O
)	O
time	O
using	O
standard	O
matrix	O
inver-	O
sion	O
routines	O
.	O
the	O
ﬁnal	O
multiplications	O
take	O
o	O
(	O
nd	O
)	O
time	O
.	O
thus	O
,	O
the	O
overall	O
runtime	O
is	O
on	O
the	O
order	O
o	O
(	O
d3	O
+	O
d2n	O
)	O
.	O
in	O
most	O
standard	O
cases	O
(	O
though	O
this	O
is	O
becoming	O
less	O
true	O
over	O
time	O
)	O
,	O
n	O
>	O
d	O
,	O
so	O
this	O
is	O
domi-	O
nated	O
by	O
o	O
(	O
d2n	O
)	O
.	O
thus	O
,	O
the	O
overall	O
question	O
is	O
whether	O
you	O
will	O
need	O
to	O
run	O
more	O
than	O
d-many	O
iterations	O
of	O
gradient	B
descent	I
.	O
if	O
so	O
,	O
then	O
the	O
matrix	O
inversion	O
will	O
be	O
(	O
roughly	O
)	O
faster	O
.	O
otherwise	O
,	O
gradient	B
descent	I
will	O
be	O
(	O
roughly	O
)	O
faster	O
.	O
for	O
low-	O
and	O
medium-dimensional	O
problems	O
(	O
say	O
,	O
d	O
≤	O
100	O
)	O
,	O
it	O
is	O
probably	O
faster	O
to	O
do	O
the	O
closed	O
form	O
solution	O
via	O
matrix	O
inversion	O
.	O
for	O
high	O
dimensional	O
problems	O
(	O
d	O
≥	O
10	O
,	O
000	O
)	O
,	O
it	O
is	O
probably	O
faster	O
to	O
do	O
gradient	B
descent	I
.	O
for	O
things	O
in	O
the	O
middle	O
,	O
it	O
’	O
s	O
hard	O
to	O
say	O
for	O
sure	O
.	O
6.7	O
support	O
vector	O
machines	O
at	O
the	O
beginning	O
of	O
this	O
chapter	O
,	O
you	O
may	O
have	O
looked	O
at	O
the	O
convex	O
surrogate	B
loss	I
functions	O
and	O
asked	O
yourself	O
:	O
where	O
did	O
these	O
come	O
from	O
?	O
!	O
they	O
are	O
all	O
derived	O
from	O
different	O
underlying	O
principles	O
,	O
which	O
essentially	O
correspond	O
to	O
different	O
inductive	O
biases	O
.	O
let	O
’	O
s	O
start	O
by	O
thinking	O
back	O
to	O
the	O
original	O
goal	O
of	O
linear	O
classiﬁers	O
:	O
to	O
ﬁnd	O
a	O
hyperplane	B
that	O
separates	O
the	O
positive	O
training	O
examples	O
from	O
the	O
negative	O
ones	O
.	O
figure	O
6.10	O
shows	O
some	O
data	O
and	O
three	O
po-	O
tential	O
hyperplanes	O
:	O
red	O
,	O
green	O
and	O
blue	O
.	O
which	O
one	O
do	O
you	O
like	O
best	O
?	O
most	O
likely	O
you	O
chose	O
the	O
green	O
hyperplane	B
.	O
and	O
most	O
likely	O
you	O
chose	O
it	O
because	O
it	O
was	O
furthest	O
away	O
from	O
the	O
closest	O
training	O
points	O
.	O
in	O
other	O
words	O
,	O
it	O
had	O
a	O
large	O
margin	B
.	O
the	O
desire	O
for	O
hyperplanes	O
with	O
large	O
margins	O
is	O
a	O
perfect	O
example	O
of	O
an	O
inductive	B
bias	I
.	O
the	O
data	O
does	O
not	O
tell	O
us	O
which	O
of	O
the	O
three	O
hyperplanes	O
is	O
best	O
:	O
we	O
have	O
to	O
choose	O
one	O
using	O
some	O
other	O
source	O
of	O
information	O
.	O
following	O
this	O
line	O
of	O
thinking	O
leads	O
us	O
to	O
the	O
support	O
vector	B
ma-	O
chine	O
(	O
svm	O
)	O
.	O
this	O
is	O
simply	O
a	O
way	O
of	O
setting	O
up	O
an	O
optimization	B
problem	I
that	O
attempts	O
to	O
ﬁnd	O
a	O
separating	B
hyperplane	I
with	O
as	O
large	O
a	O
margin	B
as	O
possible	O
.	O
it	O
is	O
written	O
as	O
a	O
constrained	B
optimization	I
problem	I
:	O
figure	O
6.10	O
:	O
picture	O
of	O
data	O
points	O
with	O
three	O
hyperplanes	O
,	O
rgb	O
with	O
g	O
the	O
best	O
1	O
γ	O
(	O
w	O
,	O
b	O
)	O
min	O
w	O
,	O
b	O
subj	O
.	O
to	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
≥	O
1	O
(	O
6.35	O
)	O
(	O
∀n	O
)	O
in	O
this	O
optimization	O
,	O
you	O
are	O
trying	O
to	O
ﬁnd	O
parameters	O
that	O
maximize	O
the	O
margin	O
,	O
denoted	O
γ	O
,	O
(	O
i.e.	O
,	O
minimize	O
the	O
reciprocal	O
of	O
the	O
margin	O
)	O
linear	O
models	O
99	O
figure	O
6.11	O
:	O
hyperplane	B
with	O
margins	O
on	O
sides	O
figure	O
6.12	O
:	O
one	O
bad	O
point	O
with	O
slack	B
subject	O
to	O
the	O
constraint	O
that	O
all	O
training	O
examples	B
are	O
correctly	O
classi-	O
ﬁed	O
.	O
the	O
“	O
odd	O
”	O
thing	O
about	O
this	O
optimization	B
problem	I
is	O
that	O
we	O
re-	O
quire	O
the	O
classiﬁcation	O
of	O
each	O
point	O
to	O
be	O
greater	O
than	O
one	O
rather	O
than	O
simply	O
greater	O
than	O
zero	O
.	O
however	O
,	O
the	O
problem	O
doesn	O
’	O
t	O
fundamen-	O
tally	O
change	O
if	O
you	O
replace	O
the	O
“	O
1	O
”	O
with	O
any	O
other	O
positive	O
constant	O
(	O
see	O
exercise	O
?	O
?	O
)	O
.	O
as	O
shown	O
in	O
figure	O
6.11	O
,	O
the	O
constant	O
one	O
can	O
be	O
interpreted	O
visually	O
as	O
ensuring	O
that	O
there	O
is	O
a	O
non-trivial	O
margin	B
between	O
the	O
positive	O
points	O
and	O
negative	O
points	O
.	O
the	O
difﬁculty	O
with	O
the	O
optimization	O
problem	O
in	O
eq	O
(	O
?	O
?	O
)	O
is	O
what	O
happens	O
with	O
data	O
that	O
is	O
not	O
linearly	B
separable	I
.	O
in	O
that	O
case	O
,	O
there	O
is	O
no	O
set	O
of	O
parameters	O
w	O
,	O
b	O
that	O
can	O
simultaneously	O
satisfy	O
all	O
the	O
constraints	O
.	O
in	O
optimization	O
terms	O
,	O
you	O
would	O
say	O
that	O
the	O
feasible	O
region	O
is	O
empty	O
.	O
(	O
the	O
feasible	O
region	O
is	O
simply	O
the	O
set	O
of	O
all	O
parame-	O
ters	O
that	O
satify	O
the	O
constraints	O
.	O
)	O
for	O
this	O
reason	O
,	O
this	O
is	O
refered	O
to	O
as	O
the	O
hard-margin	O
svm	O
,	O
because	O
enforcing	O
the	O
margin	O
is	O
a	O
hard	O
con-	O
straint	O
.	O
the	O
question	O
is	O
:	O
how	O
to	O
modify	O
this	O
optimization	B
problem	I
so	O
that	O
it	O
can	O
handle	O
inseparable	O
data	O
.	O
the	O
key	O
idea	O
is	O
the	O
use	O
of	O
slack	B
parameters	I
.	O
the	O
intuition	O
behind	O
slack	B
parameters	I
is	O
the	O
following	O
.	O
suppose	O
we	O
ﬁnd	O
a	O
set	O
of	O
param-	O
eters	O
w	O
,	O
b	O
that	O
do	O
a	O
really	O
good	O
job	O
on	O
9999	O
data	O
points	O
.	O
the	O
points	O
are	O
perfectly	O
classifed	O
and	O
you	O
achieve	O
a	O
large	O
margin	B
.	O
but	O
there	O
’	O
s	O
one	O
pesky	O
data	O
point	O
left	O
that	O
can	O
not	O
be	O
put	O
on	O
the	O
proper	O
side	O
of	O
the	O
margin	O
:	O
perhaps	O
it	O
is	O
noisy	O
.	O
(	O
see	O
figure	O
6.12	O
.	O
)	O
you	O
want	O
to	O
be	O
able	O
to	O
pretend	O
that	O
you	O
can	O
“	O
move	O
”	O
that	O
point	O
across	O
the	O
hyperplane	O
on	O
to	O
the	O
proper	O
side	O
.	O
you	O
will	O
have	O
to	O
pay	O
a	O
little	O
bit	O
to	O
do	O
so	O
,	O
but	O
as	O
long	O
as	O
you	O
aren	O
’	O
t	O
moving	O
a	O
lot	O
of	O
points	O
around	O
,	O
it	O
should	O
be	O
a	O
good	O
idea	O
to	O
do	O
this	O
.	O
in	O
this	O
picture	O
,	O
the	O
amount	O
that	O
you	O
move	O
the	O
point	O
is	O
denoted	O
ξ	O
(	O
xi	O
)	O
.	O
by	O
introducing	O
one	O
slack	O
parameter	O
for	O
each	O
training	O
example	O
,	O
and	O
penalizing	O
yourself	O
for	O
having	O
to	O
use	O
slack	B
,	O
you	O
can	O
create	O
an	O
objective	B
function	I
like	O
the	O
following	O
,	O
soft-margin	O
svm	O
:	O
+	O
c	O
∑	O
n	O
ξn	O
γ	O
(	O
w	O
,	O
b	O
)	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
subj	O
.	O
to	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
≥	O
1	O
−	O
ξn	O
large	O
margin	B
small	O
slack	B
1	O
min	O
w	O
,	O
b	O
,	O
ξ	O
ξn	O
≥	O
0	O
(	O
6.36	O
)	O
(	O
∀n	O
)	O
(	O
∀n	O
)	O
the	O
goal	O
of	O
this	O
objective	B
function	I
is	O
to	O
ensure	O
that	O
all	O
points	O
are	O
correctly	O
classiﬁed	O
(	O
the	O
ﬁrst	O
constraint	O
)	O
.	O
but	O
if	O
a	O
point	O
n	O
can	O
not	O
be	O
correctly	O
classiﬁed	O
,	O
then	O
you	O
can	O
set	O
the	O
slack	O
ξn	O
to	O
something	O
greater	O
than	O
zero	O
to	O
“	O
move	O
”	O
it	O
in	O
the	O
correct	O
direction	O
.	O
however	O
,	O
for	O
all	O
non-	O
zero	O
slacks	O
,	O
you	O
have	O
to	O
pay	O
in	O
the	O
objective	O
function	O
proportional	O
to	O
the	O
amount	O
of	O
slack	B
.	O
the	O
hyperparameter	O
c	O
>	O
0	O
controls	O
overﬁtting	O
?	O
what	O
values	O
of	O
c	O
will	O
lead	O
to	O
over-	O
ﬁtting	O
?	O
what	O
values	O
will	O
lead	O
to	O
underﬁtting	O
?	O
?	O
suppose	O
i	O
give	O
you	O
a	O
data	O
set	O
.	O
without	O
even	O
looking	O
at	O
the	O
data	O
,	O
construct	O
for	O
me	O
a	O
feasible	O
solution	O
to	O
the	O
soft-margin	O
svm	O
.	O
what	O
is	O
the	O
value	O
of	O
the	O
objective	O
for	O
this	O
solution	O
?	O
100	O
a	O
course	O
in	O
machine	O
learning	O
versus	O
underﬁtting	O
.	O
the	O
second	O
constraint	O
simply	O
says	O
that	O
you	O
must	O
not	O
have	O
negative	O
slack	B
.	O
one	O
major	O
advantage	O
of	O
the	O
soft-margin	O
svm	O
over	O
the	O
original	O
hard-margin	O
svm	O
is	O
that	O
the	O
feasible	O
region	O
is	O
never	O
empty	O
.	O
that	O
is	O
,	O
there	O
is	O
always	O
going	O
to	O
be	O
some	O
solution	O
,	O
regardless	O
of	O
whether	O
your	O
training	B
data	I
is	O
linearly	B
separable	I
or	O
not	O
.	O
it	O
’	O
s	O
one	O
thing	O
to	O
write	O
down	O
an	O
optimization	B
problem	I
.	O
it	O
’	O
s	O
another	O
thing	O
to	O
try	O
to	O
solve	O
it	O
.	O
there	O
are	O
a	O
very	O
large	O
number	O
of	O
ways	O
to	O
optimize	O
svms	O
,	O
essentially	O
because	O
they	O
are	O
such	O
a	O
popular	O
learning	O
model	B
.	O
here	O
,	O
we	O
will	O
talk	O
just	O
about	O
one	O
,	O
very	O
simple	O
way	O
.	O
more	O
complex	O
methods	O
will	O
be	O
discussed	O
later	O
in	O
this	O
book	O
once	O
you	O
have	O
a	O
bit	O
more	O
background	O
.	O
by	O
this	O
observation	O
,	O
there	O
is	O
some	O
positive	O
example	O
that	O
that	O
lies	O
to	O
make	O
progress	O
,	O
you	O
need	O
to	O
be	O
able	O
to	O
measure	O
the	O
size	O
of	O
the	O
margin	O
.	O
suppose	O
someone	O
gives	O
you	O
parameters	O
w	O
,	O
b	O
that	O
optimize	O
the	O
hard-margin	O
svm	O
.	O
we	O
wish	O
to	O
measure	O
the	O
size	O
of	O
the	O
margin	O
.	O
the	O
ﬁrst	O
observation	O
is	O
that	O
the	O
hyperplane	O
will	O
lie	O
exactly	O
halfway	O
between	O
the	O
nearest	O
positive	O
point	O
and	O
nearest	O
negative	O
point	O
.	O
if	O
not	O
,	O
the	O
margin	O
could	O
be	O
made	O
bigger	O
by	O
simply	O
sliding	O
it	O
one	O
way	O
or	O
the	O
other	O
by	O
adjusting	O
the	O
bias	O
b.	O
exactly	O
1	O
unit	O
from	O
the	O
hyperplane	O
.	O
call	O
it	O
x+	O
,	O
so	O
that	O
w	O
·	O
x+	O
+	O
b	O
=	O
1.	O
similarly	O
,	O
there	O
is	O
some	O
negative	O
example	O
,	O
x−	O
,	O
that	O
lies	O
exactly	O
on	O
the	O
other	O
side	O
of	O
the	O
margin	O
:	O
for	O
which	O
w	O
·	O
x−	O
+	O
b	O
=	O
−1	O
.	O
these	O
two	O
points	O
,	O
x+	O
and	O
x−	O
give	O
us	O
a	O
way	O
to	O
measure	O
the	O
size	O
of	O
the	O
margin	O
.	O
as	O
shown	O
in	O
figure	O
6.11	O
,	O
we	O
can	O
measure	O
the	O
size	O
of	O
the	O
margin	O
by	O
looking	O
at	O
the	O
difference	O
between	O
the	O
lengths	O
of	O
projections	O
of	O
x+	O
and	O
x−	O
onto	O
the	O
hyperplane	O
.	O
since	O
projection	O
requires	O
a	O
normalized	O
vector	B
,	O
we	O
can	O
measure	O
the	O
distances	O
as	O
:	O
1	O
d+	O
=	O
d−	O
=	O
−	O
1	O
||w||	O
w	O
·	O
x+	O
+	O
b	O
−	O
1	O
||w||	O
w	O
·	O
x−	O
−	O
b	O
+	O
1	O
(	O
cid:2	O
)	O
d+	O
−	O
d−	O
(	O
cid:3	O
)	O
(	O
cid:20	O
)	O
1	O
||w||	O
w	O
·	O
x−	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
1	O
||w||	O
w	O
·	O
x−	O
−	O
b	O
+	O
1	O
||w||	O
w	O
·	O
x+	O
+	O
b	O
−	O
1	O
−	O
1	O
(	O
cid:20	O
)	O
1	O
(	O
cid:21	O
)	O
||w||	O
w	O
·	O
x+	O
−	O
1	O
||w||	O
(	O
+1	O
)	O
−	O
1	O
||w||	O
(	O
−1	O
)	O
we	O
can	O
then	O
compute	O
the	O
margin	O
by	O
algebra	O
:	O
γ	O
=	O
=	O
=	O
=	O
=	O
1	O
2	O
1	O
2	O
1	O
2	O
1	O
2	O
1	O
||w||	O
figure	O
6.13	O
:	O
copy	O
of	O
ﬁgure	O
from	O
p5	O
of	O
cs544	O
svm	O
tutorial	O
(	O
cid:21	O
)	O
(	O
6.37	O
)	O
(	O
6.38	O
)	O
(	O
6.39	O
)	O
(	O
6.40	O
)	O
(	O
6.41	O
)	O
(	O
6.42	O
)	O
(	O
6.43	O
)	O
linear	O
models	O
101	O
this	O
is	O
a	O
remarkable	O
conclusion	O
:	O
the	O
size	O
of	O
the	O
margin	O
is	O
inversely	O
proportional	O
to	O
the	O
norm	O
of	O
the	O
weight	O
vector	B
.	O
thus	O
,	O
maximizing	O
the	O
margin	O
is	O
equivalent	O
to	O
minimizing	O
||w||	O
!	O
this	O
serves	O
as	O
an	O
addi-	O
tional	O
justiﬁcation	O
of	O
the	O
2-norm	O
regularizer	B
:	O
having	O
small	O
weights	B
means	O
having	O
large	O
margins	O
!	O
however	O
,	O
our	O
goal	O
wasn	O
’	O
t	O
to	O
justify	O
the	O
regularizer	O
:	O
it	O
was	O
to	O
un-	O
derstand	O
hinge	B
loss	I
.	O
so	O
let	O
us	O
go	O
back	O
to	O
the	O
soft-margin	O
svm	O
and	O
plug	O
in	O
our	O
new	O
knowledge	O
about	O
margins	O
:	O
min	O
w	O
,	O
b	O
,	O
ξ	O
||w||2	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
+	O
c	O
∑	O
n	O
1	O
2	O
large	O
margin	B
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
subj	O
.	O
to	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
≥	O
1	O
−	O
ξn	O
small	O
slack	B
ξn	O
ξn	O
≥	O
0	O
(	O
6.44	O
)	O
(	O
∀n	O
)	O
(	O
∀n	O
)	O
now	O
,	O
let	O
’	O
s	O
play	O
a	O
thought	O
experiment	O
.	O
suppose	O
someone	O
handed	O
you	O
a	O
solution	O
to	O
this	O
optimization	B
problem	I
that	O
consisted	O
of	O
weights	B
(	O
w	O
)	O
and	O
a	O
bias	B
(	O
b	O
)	O
,	O
but	O
they	O
forgot	O
to	O
give	O
you	O
the	O
slacks	O
.	O
could	O
you	O
recover	O
the	O
slacks	O
from	O
the	O
information	O
you	O
have	O
?	O
in	O
fact	O
,	O
the	O
answer	O
is	O
yes	O
!	O
for	O
simplicity	O
,	O
let	O
’	O
s	O
consider	O
positive	O
examples	O
.	O
suppose	O
that	O
you	O
look	O
at	O
some	O
positive	O
example	O
xn	O
.	O
you	O
need	O
to	O
ﬁgure	O
out	O
what	O
the	O
slack	O
,	O
ξn	O
,	O
would	O
have	O
been	O
.	O
there	O
are	O
two	O
cases	O
.	O
either	O
w	O
·	O
xn	O
+	O
b	O
is	O
at	O
least	O
1	O
or	O
it	O
is	O
not	O
.	O
if	O
it	O
’	O
s	O
large	O
enough	O
,	O
then	O
you	O
want	O
to	O
set	O
ξn	O
=	O
0.	O
why	O
?	O
it	O
can	O
not	O
be	O
less	O
than	O
zero	O
by	O
the	O
second	O
constraint	O
.	O
moreover	O
,	O
if	O
you	O
set	O
it	O
greater	O
than	O
zero	O
,	O
you	O
will	O
“	O
pay	O
”	O
unnecessarily	O
in	O
the	O
objective	O
.	O
so	O
in	O
this	O
case	O
,	O
ξn	O
=	O
0.	O
next	O
,	O
suppose	O
that	O
w	O
·	O
xn	O
+	O
b	O
=	O
0.2	O
,	O
so	O
it	O
is	O
not	O
big	O
enough	O
.	O
in	O
order	O
to	O
satisfy	O
the	O
ﬁrst	O
constraint	O
,	O
you	O
’	O
ll	O
need	O
to	O
set	O
ξn	O
≥	O
0.8.	O
but	O
because	O
of	O
the	O
objective	O
,	O
you	O
’	O
ll	O
not	O
want	O
to	O
set	O
it	O
any	O
larger	O
than	O
necessary	O
,	O
so	O
you	O
’	O
ll	O
set	O
ξn	O
=	O
0.8	O
exactly	O
.	O
following	O
this	O
argument	O
through	O
for	O
both	O
positive	O
and	O
negative	O
points	O
,	O
if	O
someone	O
gives	O
you	O
solutions	O
for	O
w	O
,	O
b	O
,	O
you	O
can	O
automatically	O
compute	O
the	O
optimal	O
ξ	O
variables	O
as	O
:	O
ξn	O
=	O
0	O
1	O
−	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
otherwise	O
if	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
≥	O
1	O
(	O
6.45	O
)	O
(	O
cid:40	O
)	O
in	O
other	O
words	O
,	O
the	O
optimal	O
value	O
for	O
a	O
slack	B
variable	O
is	O
exactly	O
the	O
hinge	O
loss	O
on	O
the	O
corresponding	O
example	O
!	O
thus	O
,	O
we	O
can	O
write	O
the	O
svm	O
objective	O
as	O
an	O
unconstrained	O
optimization	B
problem	I
:	O
min	O
w	O
,	O
b	O
||w||2	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
1	O
2	O
large	O
margin	B
+	O
c	O
∑	O
n	O
(	O
cid:124	O
)	O
(	O
cid:125	O
)	O
(	O
cid:96	O
)	O
(	O
hin	O
)	O
(	O
yn	O
,	O
w	O
·	O
xn	O
+	O
b	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
small	O
slack	B
(	O
6.46	O
)	O
multiplying	O
this	O
objective	O
through	O
by	O
λ/c	O
,	O
we	O
obtain	O
exactly	O
the	O
reg-	O
ularized	O
objective	O
from	O
eq	O
(	O
6.8	O
)	O
with	O
hinge	B
loss	I
as	O
the	O
loss	O
function	O
and	O
the	O
2-norm	O
as	O
the	O
regularizer	O
!	O
102	O
a	O
course	O
in	O
machine	O
learning	O
todo	O
:	O
justify	O
in	O
term	O
of	O
one	O
dimensional	O
projections	O
!	O
6.8	O
exercises	O
exercise	O
6.1.	O
todo	O
.	O
.	O
.	O
7	O
|	O
probabilistic	B
modeling	I
learning	O
objectives	O
:	O
•	O
deﬁne	O
the	O
generative	O
story	O
for	O
a	O
naive	O
bayes	O
classiﬁer	O
.	O
•	O
derive	O
relative	O
frequency	O
as	O
the	O
so-	O
lution	O
to	O
a	O
constrained	B
optimization	I
problem	I
.	O
•	O
compare	O
and	O
contrast	O
generative	O
,	O
conditional	O
and	O
discriminative	O
learning	O
.	O
•	O
explain	O
when	O
generative	O
models	O
are	O
likely	O
to	O
fail	O
.	O
•	O
derive	O
logistic	B
loss	I
with	O
an	O
(	O
cid:96	O
)	O
2	O
regularizer	B
from	O
a	O
probabilistic	O
perspective	O
.	O
dependencies	O
:	O
–	O
many	O
of	O
the	O
models	O
and	O
algorithms	O
you	O
have	O
learned	O
about	O
thus	O
far	O
are	O
relatively	O
disconnected	O
.	O
there	O
is	O
an	O
alternative	O
view	O
of	O
machine	O
learning	O
that	O
unites	O
and	O
generalizes	O
much	O
of	O
what	O
you	O
have	O
already	O
learned	O
.	O
this	O
is	O
the	O
probabilistic	O
modeling	B
framework	O
,	O
in	O
which	O
you	O
will	O
explicitly	O
think	O
of	O
learning	O
as	O
a	O
problem	O
of	O
statistical	B
inference	I
.	O
in	O
this	O
chapter	O
,	O
you	O
will	O
learn	O
about	O
two	O
ﬂavors	O
of	O
probabilistic	O
models	O
:	O
generative	O
and	O
conditional	O
.	O
you	O
will	O
see	O
that	O
many	O
of	O
the	O
ap-	O
proaches	O
(	O
both	O
supervised	O
and	O
unsupervised	O
)	O
we	O
have	O
seen	O
already	O
can	O
be	O
cast	O
as	O
probabilistic	O
models	O
.	O
through	O
this	O
new	O
view	O
,	O
you	O
will	O
be	O
able	O
to	O
develop	O
learning	O
algorithms	O
that	O
have	O
inductive	O
biases	O
closer	O
to	O
what	O
you	O
,	O
as	O
a	O
designer	O
,	O
believe	O
.	O
moreover	O
,	O
the	O
two	O
chap-	O
ters	O
that	O
follow	O
will	O
make	O
heavy	O
use	O
of	O
the	O
probabilistic	O
modeling	B
approach	O
to	O
open	O
doors	O
to	O
other	O
learning	O
problems	O
.	O
7.1	O
classiﬁcation	O
by	O
density	O
estimation	O
our	O
underlying	O
assumption	O
for	O
the	O
majority	O
of	O
this	O
book	O
is	O
that	O
learning	O
problems	O
are	O
characterized	O
by	O
some	O
unknown	O
probability	O
distribution	O
d	O
over	O
input/output	O
pairs	O
(	O
x	O
,	O
y	O
)	O
∈	O
x×y	O
.	O
suppose	O
that	O
someone	O
told	O
you	O
what	O
d	O
was	O
.	O
in	O
particular	O
,	O
they	O
gave	O
you	O
a	O
python	O
function	O
computed	O
that	O
took	O
two	O
inputs	O
,	O
x	O
and	O
y	O
,	O
and	O
returned	O
the	O
probability	O
of	O
that	O
x	O
,	O
y	O
pair	O
under	O
d.	O
if	O
you	O
had	O
access	O
to	O
such	O
a	O
func-	O
tion	O
,	O
classiﬁcation	O
becomes	O
simple	O
.	O
we	O
can	O
deﬁne	O
the	O
bayes	O
optimal	O
classiﬁer	O
as	O
the	O
classiﬁer	O
that	O
,	O
for	O
any	O
test	O
input	O
ˆx	O
,	O
simply	O
returns	O
the	O
ˆy	O
that	O
maximizes	O
computed	O
(	O
ˆx	O
,	O
ˆy	O
)	O
,	O
or	O
,	O
more	O
formally	O
:	O
f	O
(	O
bo	O
)	O
(	O
ˆx	O
)	O
=	O
arg	O
max	O
ˆy∈y	O
d	O
(	O
ˆx	O
,	O
ˆy	O
)	O
(	O
7.1	O
)	O
this	O
classiﬁer	O
is	O
optimal	O
in	O
one	O
speciﬁc	O
sense	O
:	O
of	O
all	O
possible	O
classiﬁers	O
,	O
it	O
achieves	O
the	O
smallest	O
zero/one	O
error	O
.	O
theorem	O
8	O
(	O
bayes	O
optimal	O
classiﬁer	O
)	O
.	O
the	O
bayes	O
optimal	O
classiﬁer	O
f	O
(	O
bo	O
)	O
achieves	O
minimal	O
zero/one	O
error	O
of	O
any	O
deterministic	O
classiﬁer	O
.	O
figure	O
7.1	O
:	O
104	O
a	O
course	O
in	O
machine	O
learning	O
math	O
review	O
|	O
rules	O
of	O
probability	O
chain	B
rule	I
,	O
marginalization	O
and	O
bayes	O
’	O
rule	O
this	O
theorem	O
assumes	O
that	O
you	O
are	O
comparing	O
against	O
deterministic	O
classiﬁers	O
.	O
you	O
can	O
actually	O
prove	O
a	O
stronger	O
result	O
that	O
f	O
(	O
bo	O
)	O
is	O
opti-	O
mal	O
for	O
randomized	O
classiﬁers	O
as	O
well	O
,	O
but	O
the	O
proof	O
is	O
a	O
bit	O
messier	O
.	O
however	O
,	O
the	O
intuition	O
is	O
the	O
same	O
:	O
for	O
a	O
given	O
x	O
,	O
f	O
(	O
bo	O
)	O
chooses	O
the	O
label	O
with	O
highest	O
probability	O
,	O
thus	O
minimizing	O
the	O
probability	O
that	O
it	O
makes	O
an	O
error	O
.	O
proof	O
of	O
theorem	O
8.	O
consider	O
some	O
other	O
classiﬁer	O
g	O
that	O
claims	O
to	O
be	O
better	O
than	O
f	O
.	O
then	O
,	O
there	O
must	O
be	O
some	O
x	O
on	O
which	O
g	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=	O
f	O
(	O
x	O
)	O
.	O
fix	O
such	O
an	O
x.	O
now	O
,	O
the	O
probability	O
that	O
f	O
makes	O
an	O
error	O
on	O
this	O
particular	O
x	O
is	O
1	O
−	O
d	O
(	O
x	O
,	O
f	O
(	O
bo	O
)	O
(	O
x	O
)	O
)	O
and	O
the	O
probability	O
that	O
g	O
makes	O
an	O
error	O
on	O
this	O
x	O
is	O
1	O
−	O
d	O
(	O
x	O
,	O
g	O
(	O
x	O
)	O
)	O
.	O
but	O
f	O
(	O
bo	O
)	O
was	O
chosen	O
in	O
such	O
a	O
way	O
to	O
maximize	O
d	O
(	O
x	O
,	O
f	O
(	O
bo	O
)	O
(	O
x	O
)	O
)	O
,	O
so	O
this	O
must	O
be	O
greater	O
than	O
d	O
(	O
x	O
,	O
g	O
(	O
x	O
)	O
)	O
.	O
thus	O
,	O
the	O
probability	O
that	O
f	O
errs	O
on	O
this	O
particular	O
x	O
is	O
smaller	O
than	O
the	O
probability	O
that	O
g	O
errs	O
on	O
it	O
.	O
this	O
applies	O
to	O
any	O
x	O
for	O
which	O
f	O
(	O
x	O
)	O
(	O
cid:54	O
)	O
=	O
g	O
(	O
x	O
)	O
and	O
therefore	O
f	O
achieves	O
smaller	O
zero/one	O
error	O
than	O
any	O
g.	O
the	O
bayes	O
error	B
rate	I
(	O
or	O
bayes	O
optimal	O
error	B
rate	I
)	O
is	O
the	O
error	O
rate	O
of	O
the	O
bayes	O
optimal	O
classiﬁer	O
.	O
it	O
is	O
the	O
best	O
error	B
rate	I
you	O
can	O
ever	O
hope	O
to	O
achieve	O
on	O
this	O
classiﬁcation	O
problem	O
(	O
under	O
zero/one	B
loss	I
)	O
.	O
the	O
take-home	O
message	O
is	O
that	O
if	O
someone	O
gave	O
you	O
access	O
to	O
the	O
data	O
distribution	O
,	O
forming	O
an	O
optimal	O
classiﬁer	O
would	O
be	O
trivial	O
.	O
unfortunately	O
,	O
no	O
one	O
gave	O
you	O
this	O
distribution	O
,	O
but	O
this	O
analysis	O
suggests	O
that	O
good	O
way	O
to	O
build	O
a	O
classiﬁer	O
is	O
to	O
try	O
to	O
estimate	O
d.	O
in	O
other	O
words	O
,	O
you	O
try	O
to	O
learn	O
a	O
distribution	O
ˆd	O
,	O
which	O
you	O
hope	O
to	O
very	O
similar	O
to	O
d	O
,	O
and	O
then	O
use	O
this	O
distribution	O
for	O
classiﬁcation	O
.	O
just	O
as	O
in	O
the	O
preceding	O
chapters	O
,	O
you	O
can	O
try	O
to	O
form	O
your	O
estimate	O
of	O
d	O
based	O
on	O
a	O
ﬁnite	O
training	O
set	O
.	O
the	O
most	O
direct	O
way	O
that	O
you	O
can	O
attempt	O
to	O
construct	O
such	O
a	O
probability	O
distribution	O
is	O
to	O
select	O
a	O
family	O
of	O
parametric	O
distribu-	O
tions	O
.	O
for	O
instance	O
,	O
a	O
gaussian	O
(	O
or	O
normal	O
)	O
distribution	O
is	O
parametric	O
:	O
it	O
’	O
s	O
parameters	O
are	O
its	O
mean	O
and	O
covariance	O
.	O
the	O
job	O
of	O
learning	O
is	O
then	O
to	O
infer	O
which	O
parameters	O
are	O
“	O
best	O
”	O
as	O
far	O
as	O
the	O
observed	O
train-	O
ing	O
data	O
is	O
concerned	O
,	O
as	O
well	O
as	O
whatever	O
inductive	B
bias	I
you	O
bring	O
.	O
a	O
key	O
assumption	O
that	O
you	O
will	O
need	O
to	O
make	O
is	O
that	O
the	O
training	O
data	O
you	O
have	O
access	O
to	O
is	O
drawn	O
independently	B
from	O
d.	O
in	O
particular	O
,	O
as	O
you	O
draw	O
examples	B
(	O
x1	O
,	O
y1	O
)	O
∼	O
d	O
then	O
(	O
x2	O
,	O
y2	O
)	O
∼	O
d	O
and	O
so	O
on	O
,	O
the	O
nth	O
draw	O
(	O
xn	O
,	O
yn	O
)	O
is	O
drawn	O
from	O
d	O
and	O
does	O
not	O
otherwise	O
depend	O
probabilistic	B
modeling	I
105	O
?	O
describe	O
a	O
case	O
in	O
which	O
at	O
least	O
one	O
of	O
the	O
assumptions	O
we	O
are	O
making	O
about	O
the	O
coin	O
ﬂip	O
is	O
false	O
.	O
on	O
the	O
previous	O
n	O
−	O
1	O
samples	O
.	O
this	O
assumption	O
is	O
usually	O
false	O
,	O
but	O
is	O
also	O
usually	O
sufﬁciently	O
close	O
to	O
being	O
true	O
to	O
be	O
useful	O
.	O
together	O
with	O
the	O
assumption	O
that	O
all	O
the	O
training	B
data	I
is	O
drawn	O
from	O
the	O
same	O
distribution	O
d	O
leads	O
to	O
the	O
i.i.d	O
.	O
assumption	O
or	O
independently	B
and	I
identically	I
distributed	I
assumption	O
.	O
this	O
is	O
a	O
key	O
assumption	O
in	O
al-	O
most	O
all	O
of	O
machine	O
learning	O
.	O
7.2	O
statistical	O
estimation	O
suppose	O
you	O
need	O
to	O
model	B
a	O
coin	O
that	O
is	O
possibly	O
biased	O
(	O
you	O
can	O
think	O
of	O
this	O
as	O
modeling	B
the	O
label	B
in	O
a	O
binary	O
classiﬁcation	O
problem	O
)	O
,	O
and	O
that	O
you	O
observe	O
data	O
hhth	O
(	O
where	O
h	O
means	O
a	O
ﬂip	O
came	O
up	O
heads	O
and	O
t	O
means	O
it	O
came	O
up	O
tails	O
)	O
.	O
you	O
can	O
assume	O
that	O
all	O
the	O
ﬂips	O
came	O
from	O
the	O
same	O
coin	O
,	O
and	O
that	O
each	O
ﬂip	O
was	O
independent	O
(	O
hence	O
,	O
the	O
data	O
was	O
i.i.d.	O
)	O
.	O
further	O
,	O
you	O
may	O
choose	O
to	O
believe	O
that	O
the	O
coin	O
has	O
a	O
ﬁxed	O
probability	O
β	O
of	O
coming	O
up	O
heads	O
(	O
and	O
hence	O
1	O
−	O
β	O
of	O
coming	O
up	O
tails	O
)	O
.	O
thus	O
,	O
the	O
parameter	O
of	O
your	O
model	B
is	O
simply	O
the	O
scalar	O
β.	O
the	O
most	O
basic	O
computation	O
you	O
might	O
perform	O
is	O
maximum	O
like-	O
lihood	O
estimation	O
:	O
namely	O
,	O
select	O
the	O
paramter	O
β	O
the	O
maximizes	O
the	O
probability	O
of	O
the	O
data	O
under	O
that	O
parameter	O
.	O
in	O
order	O
to	O
do	O
so	O
,	O
you	O
need	O
to	O
compute	O
the	O
probability	O
of	O
the	O
data	O
:	O
pβ	O
(	O
d	O
)	O
=	O
pβ	O
(	O
hhth	O
)	O
=	O
pβ	O
(	O
h	O
)	O
pβ	O
(	O
h	O
)	O
pβ	O
(	O
t	O
)	O
pβ	O
(	O
h	O
)	O
=	O
ββ	O
(	O
1	O
−	O
β	O
)	O
β	O
=	O
β3	O
(	O
1	O
−	O
β	O
)	O
=	O
β3	O
−	O
β4	O
deﬁnition	O
of	O
d	O
data	O
is	O
independent	O
(	O
7.2	O
)	O
(	O
7.3	O
)	O
(	O
7.4	O
)	O
(	O
7.5	O
)	O
(	O
7.6	O
)	O
thus	O
,	O
if	O
you	O
want	O
the	O
parameter	O
β	O
that	O
maximizes	O
the	O
probability	O
of	O
the	O
data	O
,	O
you	O
can	O
take	O
the	O
derivative	O
of	O
β3	O
−	O
β4	O
with	O
respect	O
to	O
β	O
,	O
set	O
it	O
equal	O
to	O
zero	O
and	O
solve	O
for	O
β	O
:	O
(	O
cid:104	O
)	O
β3	O
−	O
β4	O
(	O
cid:105	O
)	O
∂	O
∂β	O
=	O
3β2	O
−	O
4β3	O
4β3	O
=	O
3β2	O
⇐⇒4β	O
=	O
3	O
3	O
⇐⇒β	O
=	O
4	O
(	O
7.7	O
)	O
(	O
7.8	O
)	O
(	O
7.9	O
)	O
(	O
7.10	O
)	O
thus	O
,	O
the	O
maximum	O
likelihood	B
β	O
is	O
0.75	O
,	O
which	O
is	O
probably	O
what	O
you	O
would	O
have	O
selected	O
by	O
intuition	O
.	O
you	O
can	O
solve	O
this	O
problem	O
more	O
generally	O
as	O
follows	O
.	O
if	O
you	O
have	O
h-many	O
heads	O
and	O
t-many	O
tails	O
,	O
the	O
probability	O
of	O
your	O
data	O
sequence	O
is	O
βh	O
(	O
1	O
−	O
β	O
)	O
t.	O
you	O
can	O
try	O
to	O
take	O
the	O
derivative	O
of	O
this	O
with	O
respect	O
to	O
β	O
and	O
follow	O
the	O
same	O
recipe	O
,	O
but	O
all	O
of	O
the	O
products	O
make	O
things	O
difﬁcult	O
.	O
a	O
more	O
?	O
how	O
do	O
you	O
know	O
that	O
the	O
solution	O
of	O
β	O
=	O
h/	O
(	O
h	O
+	O
t	O
)	O
is	O
actually	O
a	O
maximum	O
?	O
106	O
a	O
course	O
in	O
machine	O
learning	O
friendly	O
solution	O
is	O
to	O
work	O
with	O
the	O
log	O
likelihood	B
or	O
log	O
proba-	O
bility	O
instead	O
.	O
the	O
log	O
likelihood	B
of	O
this	O
data	O
sequence	O
is	O
h	O
log	O
β	O
+	O
t	O
log	O
(	O
1	O
−	O
β	O
)	O
.	O
differentiating	O
with	O
respect	O
to	O
β	O
,	O
you	O
get	O
h/β	O
−	O
t/	O
(	O
1	O
−	O
β	O
)	O
.	O
to	O
solve	O
,	O
you	O
obtain	O
h/β	O
=	O
t/	O
(	O
1	O
−	O
β	O
)	O
so	O
h	O
(	O
1	O
−	O
β	O
)	O
=	O
tβ	O
.	O
thus	O
h	O
−	O
hβ	O
=	O
tβ	O
and	O
so	O
h	O
=	O
(	O
h	O
+	O
t	O
)	O
β	O
,	O
ﬁnally	O
yeilding	O
that	O
β	O
=	O
h/	O
(	O
h	O
+	O
t	O
)	O
or	O
,	O
simply	O
,	O
the	O
fraction	O
of	O
observed	O
data	O
that	O
came	O
up	O
heads	O
.	O
in	O
this	O
case	O
,	O
the	O
maximum	O
likelihood	B
estimate	O
is	O
nothing	O
but	O
the	O
relative	O
frequency	O
of	O
observing	O
heads	O
!	O
now	O
,	O
suppose	O
that	O
instead	O
of	O
ﬂipping	O
a	O
coin	O
,	O
you	O
’	O
re	O
rolling	O
a	O
k-	O
sided	O
die	O
(	O
for	O
instance	O
,	O
to	O
pick	O
the	O
label	O
for	O
a	O
multiclass	O
classiﬁcation	O
problem	O
)	O
.	O
you	O
might	O
model	B
this	O
by	O
saying	O
that	O
there	O
are	O
parameters	O
θ1	O
,	O
θ2	O
,	O
.	O
.	O
.	O
,	O
θk	O
specifying	O
,	O
respectively	O
,	O
the	O
probabilities	O
that	O
any	O
given	O
side	O
comes	O
up	O
on	O
a	O
role	O
.	O
since	O
these	O
are	O
themselves	O
probabilities	O
,	O
each	O
θk	O
should	O
be	O
at	O
least	O
zero	O
,	O
and	O
the	O
sum	O
of	O
the	O
θks	O
should	O
be	O
one	O
.	O
given	O
a	O
data	O
set	O
that	O
consists	O
of	O
x1	O
rolls	O
of	O
1	O
,	O
x2	O
rolls	O
of	O
2	O
and	O
so	O
on	O
,	O
the	O
probability	O
of	O
this	O
data	O
is	O
∏k	O
θ	O
∑k	O
xk	O
log	O
θk	O
.	O
if	O
you	O
pick	O
some	O
particular	O
parameter	O
,	O
say	O
θ3	O
,	O
the	O
deriva-	O
tive	O
of	O
this	O
with	O
respect	O
to	O
θ3	O
is	O
x3/θ3	O
,	O
which	O
you	O
want	O
to	O
equate	O
to	O
zero	O
.	O
this	O
leads	O
to	O
.	O
.	O
.	O
θ3	O
→	O
∞	O
.	O
xk	O
k	O
,	O
yielding	O
a	O
log	B
probability	I
of	O
this	O
is	O
obviously	O
“	O
wrong.	O
”	O
from	O
the	O
mathematical	O
formulation	O
,	O
xk	O
k	O
it	O
’	O
s	O
correct	O
:	O
in	O
fact	O
,	O
setting	O
all	O
of	O
the	O
θks	O
to	O
∞	O
does	O
maximize	O
∏k	O
θ	O
for	O
any	O
(	O
non-negative	O
)	O
xks	O
.	O
the	O
problem	O
is	O
that	O
you	O
need	O
to	O
constrain	O
the	O
θs	O
to	O
sum	O
to	O
one	O
.	O
in	O
particular	O
,	O
you	O
have	O
a	O
constraint	O
that	O
∑k	O
θk	O
=	O
1	O
that	O
you	O
forgot	O
to	O
enforce	O
.	O
a	O
convenient	O
way	O
to	O
enforce	O
such	O
con-	O
straints	O
is	O
through	O
the	O
technique	O
of	O
lagrange	O
multipliers	O
.	O
to	O
make	O
this	O
problem	O
consistent	O
with	O
standard	O
minimization	O
problems	O
,	O
it	O
is	O
convenient	O
to	O
minimize	O
negative	O
log	O
probabilities	O
,	O
instead	O
of	O
maxi-	O
mizing	O
log	O
probabilities	O
.	O
thus	O
,	O
the	O
constrainted	O
optimization	B
problem	I
is	O
:	O
−	O
∑	O
k	O
xk	O
log	O
θk	O
θk	O
−	O
1	O
=	O
0	O
min	O
θ	O
subj	O
.	O
to	O
∑	O
k	O
(	O
7.11	O
)	O
the	O
lagrange	O
multiplier	O
approach	O
involves	O
adding	O
a	O
new	O
variable	O
λ	O
to	O
the	O
problem	O
(	O
called	O
the	O
lagrange	O
variable	O
)	O
corresponding	O
to	O
the	O
constraint	O
,	O
and	O
to	O
use	O
that	O
to	O
move	O
the	O
constraint	O
into	O
the	O
objective	O
.	O
the	O
result	O
,	O
in	O
this	O
case	O
,	O
is	O
:	O
(	O
cid:32	O
)	O
(	O
cid:33	O
)	O
max	O
λ	O
min	O
θ	O
xk	O
log	O
θk	O
−	O
λ	O
−	O
∑	O
k	O
θk	O
−	O
1	O
∑	O
k	O
(	O
7.12	O
)	O
turning	O
a	O
constrained	B
optimization	I
problem	I
into	O
it	O
’	O
s	O
corresponding	O
lagrangian	O
is	O
straightforward	O
.	O
the	O
mystical	O
aspect	O
is	O
why	O
it	O
works	O
.	O
in	O
this	O
case	O
,	O
the	O
idea	O
is	O
as	O
follows	O
.	O
think	O
of	O
λ	O
as	O
an	O
adversary	O
:	O
λ	O
probabilistic	B
modeling	I
107	O
is	O
trying	O
to	O
maximize	O
this	O
function	O
(	O
you	O
’	O
re	O
trying	O
to	O
minimize	O
it	O
)	O
.	O
if	O
you	O
pick	O
some	O
parameters	O
θ	O
that	O
actually	O
satisfy	O
the	O
constraint	O
,	O
then	O
the	O
green	O
term	O
in	O
eq	O
(	O
?	O
?	O
)	O
goes	O
to	O
zero	O
,	O
and	O
therefore	O
λ	O
does	O
not	O
matter	O
:	O
the	O
adversary	O
can	O
not	O
do	O
anything	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
the	O
constraint	O
is	O
even	O
slightly	O
unsatisﬁed	O
,	O
then	O
λ	O
can	O
tend	O
toward	O
+∞	O
or	O
−∞	O
to	O
blow	O
up	O
the	O
objective	O
.	O
so	O
,	O
in	O
order	O
to	O
have	O
a	O
non-inﬁnite	O
objective	O
value	O
,	O
the	O
optimizer	O
must	O
ﬁnd	O
values	O
of	O
θ	O
that	O
satisfy	O
the	O
constraint	O
.	O
if	O
we	O
solve	O
the	O
inner	O
optimization	O
of	O
eq	O
(	O
?	O
?	O
)	O
by	O
differentiating	O
with	O
respect	O
to	O
θ1	O
,	O
we	O
get	O
x1/θ1	O
=	O
λ	O
,	O
yielding	O
θ1	O
=	O
x1/λ	O
.	O
in	O
general	O
,	O
the	O
solution	O
is	O
θk	O
=	O
xk/λ	O
.	O
remembering	O
that	O
the	O
goal	O
of	O
λ	O
is	O
to	O
enforce	O
the	O
sums-to-one	O
constraint	O
,	O
we	O
can	O
set	O
λ	O
=	O
∑k	O
xk	O
and	O
verify	O
that	O
this	O
is	O
a	O
solution	O
.	O
thus	O
,	O
our	O
optimal	O
θk	O
=	O
xk/	O
∑k	O
xk	O
,	O
which	O
again	O
completely	O
corresponds	O
to	O
intuition	O
.	O
7.3	O
naive	O
bayes	O
models	O
now	O
,	O
consider	O
the	O
binary	O
classiﬁcation	O
problem	O
.	O
you	O
are	O
looking	O
for	O
a	O
parameterized	O
probability	O
distribution	O
that	O
can	O
describe	O
the	O
training	O
data	O
you	O
have	O
.	O
to	O
be	O
concrete	O
,	O
your	O
task	O
might	O
be	O
to	O
predict	B
whether	O
a	O
movie	O
review	O
is	O
positive	O
or	O
negative	O
(	O
label	B
)	O
based	O
on	O
what	O
words	O
(	O
features	B
)	O
appear	O
in	O
that	O
review	O
.	O
thus	O
,	O
the	O
probability	O
for	O
a	O
single	O
data	O
point	O
can	O
be	O
written	O
as	O
:	O
pθ	O
(	O
(	O
y	O
,	O
x	O
)	O
)	O
=	O
pθ	O
(	O
y	O
,	O
x1	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
xd	O
)	O
(	O
7.13	O
)	O
the	O
challenge	O
in	O
working	O
with	O
a	O
probability	O
distribution	O
like	O
eq	O
(	O
7.13	O
)	O
is	O
that	O
it	O
’	O
s	O
a	O
distribution	O
over	O
a	O
lot	O
of	O
variables	O
.	O
you	O
can	O
try	O
to	O
sim-	O
plify	O
it	O
by	O
applying	O
the	O
chain	O
rule	O
of	O
probabilities	O
:	O
pθ	O
(	O
x1	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
xd	O
,	O
y	O
)	O
=	O
pθ	O
(	O
y	O
)	O
pθ	O
(	O
x1	O
|	O
y	O
)	O
pθ	O
(	O
x2	O
|	O
y	O
,	O
x1	O
)	O
pθ	O
(	O
x3	O
|	O
y	O
,	O
x1	O
,	O
x2	O
)	O
(	O
7.14	O
)	O
(	O
7.15	O
)	O
·	O
·	O
·	O
pθ	O
(	O
xd	O
|	O
y	O
,	O
x1	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
xd−1	O
)	O
pθ	O
(	O
xd	O
|	O
y	O
,	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd−1	O
)	O
=	O
pθ	O
(	O
y	O
)	O
∏	O
d	O
at	O
this	O
point	O
,	O
this	O
equality	O
is	O
exact	O
for	O
any	O
probability	O
distribution	O
.	O
however	O
,	O
it	O
might	O
be	O
difﬁcult	O
to	O
craft	O
a	O
probability	O
distribution	O
for	O
the	O
10000th	O
feature	O
,	O
given	O
the	O
previous	O
9999.	O
even	O
if	O
you	O
could	O
,	O
it	O
might	O
be	O
difﬁcult	O
to	O
accurately	O
estimate	O
it	O
.	O
at	O
this	O
point	O
,	O
you	O
can	O
make	O
assumptions	O
.	O
a	O
classic	O
assumption	O
,	O
called	O
the	O
naive	O
bayes	O
as-	O
sumption	O
,	O
is	O
that	O
the	O
features	O
are	O
independent	O
,	O
conditioned	O
on	O
the	O
label	O
.	O
in	O
the	O
movie	O
review	O
example	O
,	O
this	O
is	O
saying	O
that	O
once	O
you	O
know	O
that	O
it	O
’	O
s	O
a	O
positive	O
review	O
,	O
the	O
probability	O
that	O
the	O
word	O
“	O
excellent	O
”	O
appears	O
is	O
independent	O
of	O
whether	O
“	O
amazing	O
”	O
also	O
appeared	O
.	O
(	O
note	O
that	O
this	O
does	O
not	O
imply	O
that	O
these	O
words	O
are	O
independent	O
when	O
you	O
108	O
a	O
course	O
in	O
machine	O
learning	O
don	O
’	O
t	O
know	O
the	O
label—they	O
most	O
certainly	O
are	O
not	O
.	O
)	O
formally	O
this	O
assumption	O
states	O
that	O
:	O
assumption	O
:	O
∀d	O
(	O
cid:54	O
)	O
=	O
d	O
(	O
cid:48	O
)	O
under	O
this	O
assumption	O
,	O
you	O
can	O
simplify	O
eq	O
(	O
7.15	O
)	O
to	O
:	O
p	O
(	O
xd	O
|	O
y	O
,	O
xd	O
(	O
cid:48	O
)	O
)	O
=	O
p	O
(	O
xd	O
|	O
y	O
)	O
,	O
(	O
7.16	O
)	O
pθ	O
(	O
(	O
y	O
,	O
x	O
)	O
)	O
=	O
pθ	O
(	O
y	O
)	O
∏	O
d	O
pθ	O
(	O
xd	O
|	O
y	O
)	O
naive	O
bayes	O
assumption	O
(	O
7.17	O
)	O
at	O
this	O
point	O
,	O
you	O
can	O
start	O
parameterizing	O
p.	O
suppose	O
,	O
for	O
now	O
,	O
that	O
your	O
labels	O
are	O
binary	O
and	O
your	O
features	B
are	O
also	O
binary	O
.	O
in	O
this	O
case	O
,	O
you	O
could	O
model	B
the	O
label	B
as	O
a	O
biased	O
coin	O
,	O
with	O
probability	O
of	O
heads	O
(	O
e.g.	O
,	O
positive	O
review	O
)	O
given	O
by	O
θ0	O
.	O
then	O
,	O
for	O
each	O
label	B
,	O
you	O
can	O
imagine	O
having	O
one	O
(	O
biased	O
)	O
coin	O
for	O
each	O
feature	O
.	O
so	O
if	O
there	O
are	O
d-many	O
features	B
,	O
you	O
’	O
ll	O
have	O
1	O
+	O
2d	O
total	O
coins	O
:	O
one	O
for	O
the	O
label	O
(	O
call	O
it	O
θ0	O
)	O
and	O
one	O
for	O
each	O
label/feature	O
combination	O
(	O
call	O
these	O
θ+1	O
and	O
as	O
θ−1	O
)	O
.	O
in	O
the	O
movie	O
review	O
example	O
,	O
we	O
might	O
expect	O
θ0	O
≈	O
0.4	O
(	O
forty	O
percent	O
of	O
movie	O
reviews	O
are	O
positive	O
)	O
and	O
also	O
that	O
θ+1	O
might	O
give	O
high	O
probability	O
to	O
words	O
like	O
“	O
excellent	O
”	O
and	O
“	O
amazing	O
”	O
and	O
“	O
good	O
”	O
and	O
θ−1	O
might	O
give	O
high	O
probability	O
to	O
words	O
like	O
“	O
terrible	O
”	O
and	O
“	O
boring	O
”	O
and	O
“	O
hate	O
”	O
.	O
you	O
can	O
rewrite	O
the	O
probability	O
of	O
a	O
single	O
example	O
as	O
follows	O
,	O
eventually	O
leading	O
to	O
the	O
log	O
probability	O
of	O
the	O
entire	O
data	O
set	O
:	O
pθ	O
(	O
(	O
y	O
,	O
x	O
)	O
)	O
=	O
pθ	O
(	O
y	O
)	O
∏	O
pθ	O
(	O
xd	O
|	O
y	O
)	O
(	O
cid:16	O
)	O
=	O
[	O
y=+1	O
]	O
0	O
θ	O
d	O
(	O
1	O
−	O
θ0	O
)	O
[	O
y=−1	O
]	O
(	O
cid:17	O
)	O
∏	O
naive	O
bayes	O
assumption	O
[	O
xd=1	O
]	O
(	O
y	O
)	O
,	O
d	O
(	O
1	O
−	O
θ	O
(	O
y	O
)	O
,	O
d	O
)	O
[	O
xd=0	O
]	O
θ	O
(	O
7.18	O
)	O
(	O
7.19	O
)	O
model	B
assumptions	O
d	O
solving	O
for	O
θ0	O
is	O
identical	O
to	O
solving	O
for	O
the	O
biased	O
coin	O
case	O
from	O
before	O
:	O
it	O
is	O
just	O
the	O
relative	O
frequency	O
of	O
positive	O
labels	O
in	O
your	O
data	O
(	O
because	O
θ0	O
doesn	O
’	O
t	O
depend	O
on	O
x	O
at	O
all	O
)	O
.	O
for	O
the	O
other	O
parameters	O
,	O
you	O
can	O
repeat	O
the	O
same	O
exercise	O
as	O
before	O
for	O
each	O
of	O
the	O
2d	O
coins	O
independently	B
.	O
this	O
yields	O
:	O
ˆθ0	O
=	O
ˆθ	O
(	O
+1	O
)	O
,	O
d	O
=	O
ˆθ	O
(	O
−1	O
)	O
,	O
d	O
=	O
∑	O
n	O
[	O
yn	O
=	O
+1	O
]	O
1	O
n	O
∑n	O
[	O
yn	O
=	O
+1	O
∧	O
xn	O
,	O
d	O
=	O
1	O
]	O
∑n	O
[	O
yn	O
=	O
−1	O
∧	O
xn	O
,	O
d	O
=	O
1	O
]	O
∑n	O
[	O
yn	O
=	O
+1	O
]	O
∑n	O
[	O
yn	O
=	O
−1	O
]	O
(	O
7.20	O
)	O
(	O
7.21	O
)	O
(	O
7.22	O
)	O
in	O
the	O
case	O
that	O
the	O
features	O
are	O
not	O
binary	O
,	O
you	O
need	O
to	O
choose	O
a	O
dif-	O
ferent	O
model	B
for	O
p	O
(	O
xd	O
|	O
y	O
)	O
.	O
the	O
model	O
we	O
chose	O
here	O
is	O
the	O
bernouilli	O
distribution	O
,	O
which	O
is	O
effectively	O
a	O
distribution	O
over	O
independent	O
probabilistic	B
modeling	I
109	O
math	O
review	O
|	O
common	O
probability	O
distributions	O
remove	O
people	O
about	O
discrete	O
,	O
bernoulli	O
,	O
binomial	O
,	O
multinomial	O
and	O
gaussian	O
distributions	O
figure	O
7.2	O
:	O
coin	O
ﬂips	O
.	O
for	O
other	O
types	O
of	O
data	O
,	O
other	O
distributions	O
become	O
more	O
appropriate	O
.	O
the	O
die	O
example	O
from	O
before	O
corresponds	O
to	O
a	O
discrete	B
distribution	I
.	O
if	O
the	O
data	O
is	O
continuous	O
,	O
you	O
might	O
choose	O
to	O
use	O
a	O
gaussian	O
distribution	O
(	O
aka	O
normal	O
distribution	O
)	O
.	O
the	O
choice	O
of	O
dis-	O
tribution	O
is	O
a	O
form	O
of	O
inductive	B
bias	I
by	O
which	O
you	O
can	O
inject	O
your	O
knowledge	O
of	O
the	O
problem	O
into	O
the	O
learning	O
algorithm	B
.	O
7.4	O
prediction	O
consider	O
the	O
predictions	O
made	O
by	O
the	O
naive	O
bayes	O
model	B
with	O
bernoulli	O
features	B
in	O
eq	O
(	O
7.18	O
)	O
.	O
you	O
can	O
better	O
understand	O
this	O
model	B
by	O
con-	O
sidering	O
its	O
decision	B
boundary	I
.	O
in	O
the	O
case	O
of	O
probabilistic	O
models	O
,	O
the	O
decision	O
boundary	O
is	O
the	O
set	O
of	O
inputs	O
for	O
which	O
the	O
likelihood	O
of	O
y	O
=	O
+1	O
is	O
precisely	O
50	O
%	O
.	O
or	O
,	O
in	O
other	O
words	O
,	O
the	O
set	O
of	O
inputs	O
x	O
for	O
which	O
p	O
(	O
y	O
=	O
+1	O
|	O
x	O
)	O
/p	O
(	O
y	O
=	O
−1	O
|	O
x	O
)	O
=	O
1.	O
in	O
order	O
to	O
do	O
this	O
,	O
the	O
ﬁrst	O
thing	O
to	O
notice	O
is	O
that	O
p	O
(	O
y	O
|	O
x	O
)	O
=	O
p	O
(	O
y	O
,	O
x	O
)	O
/p	O
(	O
x	O
)	O
.	O
in	O
the	O
ratio	O
,	O
the	O
p	O
(	O
x	O
)	O
terms	O
cancel	O
,	O
leaving	O
p	O
(	O
y	O
=	O
+1	O
,	O
x	O
)	O
/p	O
(	O
y	O
=	O
−1	O
,	O
x	O
)	O
.	O
instead	O
of	O
computing	O
this	O
ratio	O
,	O
it	O
is	O
easier	O
to	O
compute	O
the	O
log-likelihood	O
ratio	O
(	O
or	O
llr	O
)	O
,	O
log	O
p	O
(	O
y	O
=	O
+1	O
,	O
x	O
)	O
−	O
log	O
p	O
(	O
y	O
=	O
−1	O
,	O
x	O
)	O
,	O
computed	O
below	O
:	O
(	O
cid:34	O
)	O
llr	O
=	O
log	O
θ0	O
∏	O
d	O
θ	O
[	O
xd=1	O
]	O
(	O
cid:35	O
)	O
(	O
+1	O
)	O
,	O
d	O
(	O
1	O
−	O
θ	O
(	O
+1	O
)	O
,	O
d	O
)	O
[	O
xd=0	O
]	O
(	O
cid:16	O
)	O
(	O
cid:34	O
)	O
−	O
log	O
(	O
1	O
−	O
θ0	O
)	O
∏	O
d	O
θ	O
[	O
xd=1	O
]	O
(	O
−1	O
)	O
,	O
d	O
(	O
1	O
−	O
θ	O
(	O
−1	O
)	O
,	O
d	O
)	O
[	O
xd=0	O
]	O
(	O
cid:17	O
)	O
(	O
7.23	O
)	O
=	O
log	O
θ0	O
−	O
log	O
(	O
1	O
−	O
θ0	O
)	O
+	O
∑	O
[	O
xd	O
=	O
1	O
]	O
log	O
θ	O
(	O
+1	O
)	O
,	O
d	O
−	O
log	O
θ	O
(	O
−1	O
)	O
,	O
d	O
d	O
log	O
(	O
1	O
−	O
θ	O
(	O
+1	O
)	O
,	O
d	O
)	O
−	O
log	O
(	O
1	O
−	O
θ	O
(	O
−1	O
)	O
,	O
d	O
)	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
+	O
∑	O
d	O
[	O
xd	O
=	O
0	O
]	O
θ	O
(	O
+1	O
)	O
,	O
d	O
θ	O
(	O
−1	O
)	O
,	O
d	O
+	O
∑	O
d	O
(	O
1	O
−	O
xd	O
)	O
log	O
θ	O
(	O
+1	O
)	O
,	O
d	O
θ	O
(	O
−1	O
)	O
,	O
d	O
−	O
log	O
1	O
−	O
θ	O
(	O
+1	O
)	O
,	O
d	O
1	O
−	O
θ	O
(	O
−1	O
)	O
,	O
d	O
1	O
−	O
θ	O
(	O
+1	O
)	O
,	O
d	O
1	O
−	O
θ	O
(	O
−1	O
)	O
,	O
d	O
(	O
cid:35	O
)	O
+	O
∑	O
d	O
log	O
(	O
7.24	O
)	O
+	O
log	O
θ0	O
1	O
−	O
θ0	O
(	O
7.25	O
)	O
1	O
−	O
θ	O
(	O
+1	O
)	O
,	O
d	O
1	O
−	O
θ	O
(	O
−1	O
)	O
,	O
d	O
+	O
log	O
θ0	O
1	O
−	O
θ0	O
=	O
∑	O
d	O
=	O
∑	O
d	O
xd	O
log	O
(	O
cid:34	O
)	O
xd	O
log	O
=	O
x	O
·	O
w	O
+	O
b	O
(	O
7.26	O
)	O
(	O
7.27	O
)	O
(	O
cid:35	O
)	O
model	B
assumptions	O
take	O
logs	O
and	O
rearrange	O
simplify	O
log	O
terms	O
group	O
x-terms	O
110	O
a	O
course	O
in	O
machine	O
learning	O
wd	O
=	O
log	O
θ	O
(	O
+1	O
)	O
,	O
d	O
(	O
1	O
−	O
θ	O
(	O
−1	O
)	O
,	O
d	O
)	O
θ	O
(	O
−1	O
)	O
,	O
d	O
(	O
1	O
−	O
θ	O
(	O
+1	O
)	O
,	O
d	O
)	O
,	O
b	O
=	O
∑	O
d	O
log	O
1	O
−	O
θ	O
(	O
+1	O
)	O
,	O
d	O
1	O
−	O
θ	O
(	O
−1	O
)	O
,	O
d	O
+	O
log	O
θ0	O
1	O
−	O
θ0	O
(	O
7.28	O
)	O
the	O
result	O
of	O
the	O
algebra	O
is	O
that	O
the	O
naive	O
bayes	O
model	B
has	O
precisely	O
the	O
form	O
of	O
a	O
linear	O
model	O
!	O
thus	O
,	O
like	O
perceptron	B
and	O
many	O
of	O
the	O
other	O
models	O
you	O
’	O
ve	O
previous	O
studied	O
,	O
the	O
decision	O
boundary	O
is	O
linear	O
.	O
todo	O
:	O
mbr	O
7.5	O
generative	O
stories	O
a	O
useful	O
way	O
to	O
develop	O
probabilistic	O
models	O
is	O
to	O
tell	O
a	O
generative	B
story	I
.	O
this	O
is	O
a	O
ﬁctional	O
story	O
that	O
explains	O
how	O
you	O
believe	O
your	O
training	B
data	I
came	O
into	O
existence	O
.	O
to	O
make	O
things	O
interesting	O
,	O
con-	O
sider	O
a	O
multiclass	O
classiﬁcation	O
problem	O
,	O
with	O
continuous	O
features	B
modeled	O
by	O
independent	O
gaussians	O
.	O
since	O
the	O
label	O
can	O
take	O
values	O
1	O
.	O
.	O
.	O
k	O
,	O
you	O
can	O
use	O
a	O
discrete	B
distribution	I
(	O
die	O
roll	O
)	O
to	O
model	B
it	O
(	O
as	O
opposed	O
to	O
the	O
bernoilli	O
distribution	O
from	O
before	O
)	O
:	O
1.	O
for	O
each	O
example	O
n	O
=	O
1	O
.	O
.	O
.	O
n	O
:	O
(	O
a	O
)	O
choose	O
a	O
label	B
yn	O
∼	O
disc	O
(	O
θ	O
)	O
(	O
b	O
)	O
for	O
each	O
feature	O
d	O
=	O
1	O
.	O
.	O
.	O
d	O
:	O
i.	O
choose	O
feature	O
value	O
xn	O
,	O
d	O
∼	O
nor	O
(	O
µyn	O
,	O
d	O
,	O
σ2	O
yn	O
,	O
d	O
)	O
this	O
generative	B
story	I
can	O
be	O
directly	O
translated	O
into	O
a	O
likelihood	B
function	O
by	O
replacing	O
the	O
“	O
for	O
each	O
”	O
s	O
with	O
products	O
:	O
(	O
cid:122	O
)	O
∏	O
n	O
p	O
(	O
d	O
)	O
=	O
for	O
each	O
example	O
(	O
cid:34	O
)	O
(	O
cid:125	O
)	O
(	O
cid:124	O
)	O
exp	O
1	O
(	O
cid:113	O
)	O
(	O
cid:124	O
)	O
2πσ2	O
yn	O
,	O
d	O
θyn	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
choose	O
label	B
∏	O
d	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:35	O
)	O
(	O
cid:125	O
)	O
(	O
cid:125	O
)	O
(	O
xn	O
,	O
d	O
−	O
µyn	O
,	O
d	O
)	O
2	O
yn	O
,	O
d	O
−	O
1	O
2σ2	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
choose	O
feature	O
value	O
for	O
each	O
feature	O
(	O
cid:34	O
)	O
you	O
can	O
take	O
logs	O
to	O
arrive	O
at	O
the	O
log-likelihood	O
:	O
log	O
p	O
(	O
d	O
)	O
=	O
∑	O
n	O
log	O
θyn	O
+	O
∑	O
d	O
−	O
1	O
2	O
log	O
(	O
σ2	O
yn	O
,	O
d	O
)	O
−	O
1	O
2σ2	O
yn	O
,	O
d	O
(	O
7.29	O
)	O
(	O
cid:35	O
)	O
+	O
const	O
(	O
xn	O
,	O
d	O
−	O
µyn	O
,	O
d	O
)	O
2	O
(	O
7.30	O
)	O
to	O
optimize	O
for	O
θ	O
,	O
you	O
need	O
to	O
add	O
a	O
“	O
sums	O
to	O
one	O
”	O
constraint	O
as	O
before	O
.	O
this	O
leads	O
to	O
the	O
previous	O
solution	O
where	O
the	O
θks	O
are	O
propor-	O
tional	O
to	O
the	O
number	O
of	O
examples	B
with	O
label	B
k.	O
in	O
the	O
case	O
of	O
the	O
µs	O
probabilistic	B
modeling	I
111	O
you	O
can	O
take	O
a	O
derivative	O
with	O
respect	O
to	O
,	O
say	O
µk	O
,	O
i	O
and	O
obtain	O
:	O
∂	O
log	O
p	O
(	O
d	O
)	O
∂µk	O
,	O
i	O
=	O
∂	O
∂µk	O
,	O
i	O
−	O
∑	O
n	O
∑	O
d	O
1	O
2σ2	O
yn	O
,	O
d	O
(	O
xn	O
,	O
d	O
−	O
µyn	O
,	O
d	O
)	O
2	O
=	O
∂	O
∂µk	O
,	O
i	O
−	O
∑	O
n	O
:	O
yn=k	O
1	O
2σ2	O
k	O
,	O
d	O
(	O
xn	O
,	O
i	O
−	O
µk	O
,	O
i	O
)	O
2	O
=	O
∑	O
n	O
:	O
yn=k	O
1	O
σ2	O
k	O
,	O
d	O
(	O
xn	O
,	O
i	O
−	O
µk	O
,	O
i	O
)	O
setting	O
this	O
equal	O
to	O
zero	O
and	O
solving	O
yields	O
:	O
µk	O
,	O
i	O
=	O
∑n	O
:	O
yn=k	O
xn	O
,	O
i	O
∑n	O
:	O
yn=k	O
1	O
ignore	O
irrelevant	O
terms	O
(	O
7.31	O
)	O
ignore	O
irrelevant	O
terms	O
(	O
7.32	O
)	O
take	O
derivative	O
(	O
7.33	O
)	O
(	O
7.34	O
)	O
namely	O
,	O
the	O
sample	O
mean	O
of	O
the	O
ith	O
feature	O
of	O
the	O
data	O
points	O
that	O
fall	O
in	O
class	O
k.	O
a	O
similar	O
analysis	O
for	O
σ2	O
k	O
,	O
i	O
yields	O
:	O
(	O
cid:35	O
)	O
∂	O
log	O
p	O
(	O
d	O
)	O
∂σ2	O
k	O
,	O
i	O
=	O
∂	O
∂σ2	O
k	O
,	O
i	O
−	O
∑	O
y	O
:	O
yn=k	O
1	O
2	O
log	O
(	O
σ2	O
k	O
,	O
i	O
)	O
+	O
1	O
2σ2	O
k	O
,	O
i	O
(	O
xn	O
,	O
i	O
−	O
µk	O
,	O
i	O
)	O
2	O
(	O
cid:34	O
)	O
(	O
cid:34	O
)	O
(	O
cid:104	O
)	O
(	O
cid:35	O
)	O
(	O
cid:105	O
)	O
=	O
−	O
∑	O
y	O
:	O
yn=k	O
1	O
2σ2	O
k	O
,	O
i	O
−	O
1	O
2	O
(	O
σ2	O
k	O
,	O
i	O
)	O
2	O
(	O
xn	O
,	O
i	O
−	O
µk	O
,	O
i	O
)	O
2	O
=	O
1	O
2σ4	O
k	O
,	O
i	O
∑	O
y	O
:	O
yn=k	O
(	O
xn	O
,	O
i	O
−	O
µk	O
,	O
i	O
)	O
2	O
−	O
σ2	O
k	O
,	O
i	O
ignore	O
irrelevant	O
terms	O
take	O
derivative	O
simplify	O
(	O
7.35	O
)	O
(	O
7.36	O
)	O
(	O
7.37	O
)	O
(	O
7.38	O
)	O
you	O
can	O
now	O
set	O
this	O
equal	O
to	O
zero	O
and	O
solve	O
,	O
yielding	O
:	O
σ2	O
k	O
,	O
i	O
=	O
∑n	O
:	O
yn=k	O
(	O
xn	O
,	O
i	O
−	O
µk	O
,	O
i	O
)	O
2	O
∑n	O
:	O
yn=k	O
1	O
which	O
is	O
just	O
the	O
sample	O
variance	B
of	O
feature	O
i	O
for	O
class	O
k.	O
7.6	O
conditional	O
models	O
in	O
the	O
foregoing	O
examples	B
,	O
the	O
task	O
was	O
formulated	O
as	O
attempting	O
to	O
model	B
the	O
joint	B
distribution	O
of	O
(	O
x	O
,	O
y	O
)	O
pairs	O
.	O
this	O
may	O
seem	O
wasteful	O
:	O
at	O
prediction	O
time	O
,	O
all	O
you	O
care	O
about	O
is	O
p	O
(	O
y	O
|	O
x	O
)	O
,	O
so	O
why	O
not	O
model	B
it	O
directly	O
?	O
starting	O
with	O
the	O
case	O
of	O
regression	O
is	O
actually	O
somewhat	O
simpler	O
than	O
starting	O
with	O
classiﬁcation	O
in	O
this	O
case	O
.	O
suppose	O
you	O
“	O
believe	O
”	O
?	O
what	O
would	O
the	O
estimate	O
be	O
if	O
you	O
decided	O
that	O
,	O
for	O
a	O
given	O
class	O
k	O
,	O
all	O
features	O
had	O
equal	O
variance	B
?	O
what	O
if	O
you	O
assumed	O
feature	O
i	O
had	O
equal	O
variance	B
for	O
each	O
class	O
?	O
under	O
what	O
circumstances	O
might	O
it	O
be	O
a	O
good	O
idea	O
to	O
make	O
such	O
assumptions	O
?	O
112	O
a	O
course	O
in	O
machine	O
learning	O
that	O
the	O
relationship	O
between	O
the	O
real	O
value	O
y	O
and	O
the	O
vector	O
x	O
should	O
be	O
linear	O
.	O
that	O
is	O
,	O
you	O
expect	O
that	O
y	O
=	O
w	O
·	O
x	O
+	O
b	O
should	O
hold	O
for	O
some	O
parameters	O
(	O
w	O
,	O
b	O
)	O
.	O
of	O
course	O
,	O
the	O
data	O
that	O
you	O
get	O
does	O
not	O
exactly	O
obey	O
this	O
:	O
that	O
’	O
s	O
ﬁne	O
,	O
you	O
can	O
think	O
of	O
deviations	O
from	O
y	O
=	O
w	O
·	O
x	O
+	O
b	O
as	O
noise	B
.	O
to	O
form	O
a	O
probabilistic	O
model	O
,	O
you	O
must	O
assume	O
some	O
distribution	O
over	O
noise	B
;	O
a	O
convenient	O
choice	O
is	O
zero-mean	O
gaussian	O
noise	B
.	O
this	O
leads	O
to	O
the	O
following	O
generative	B
story	I
:	O
1.	O
for	O
each	O
example	O
n	O
=	O
1	O
.	O
.	O
.	O
n	O
:	O
(	O
a	O
)	O
compute	O
tn	O
=	O
w	O
·	O
xn	O
+	O
b	O
(	O
b	O
)	O
choose	O
noise	B
en	O
∼	O
nor	O
(	O
0	O
,	O
σ2	O
)	O
(	O
c	O
)	O
return	O
yn	O
=	O
tn	O
+	O
en	O
in	O
this	O
story	O
,	O
the	O
variable	O
tn	O
stands	O
for	O
“	O
target.	O
”	O
it	O
is	O
the	O
noiseless	O
variable	O
that	O
you	O
do	O
not	O
get	O
to	O
observe	O
.	O
similarly	O
en	O
is	O
the	O
error	O
(	O
noise	B
)	O
on	O
example	O
n.	O
the	O
value	O
that	O
you	O
actually	O
get	O
to	O
observe	O
is	O
yn	O
=	O
tn	O
+	O
en	O
.	O
see	O
figure	O
7.3.	O
a	O
basic	O
property	O
of	O
the	O
gaussian	O
distribution	O
is	O
additivity	O
.	O
namely	O
,	O
that	O
if	O
a	O
∼	O
nor	O
(	O
µ	O
,	O
σ2	O
)	O
and	O
b	O
=	O
a	O
+	O
c	O
,	O
then	O
b	O
∼	O
nor	O
(	O
µ	O
+	O
c	O
,	O
σ2	O
)	O
.	O
given	O
this	O
,	O
from	O
the	O
generative	O
story	O
above	O
,	O
you	O
can	O
derive	O
a	O
shorter	O
gener-	O
ative	O
story	O
:	O
1.	O
for	O
each	O
example	O
n	O
=	O
1	O
.	O
.	O
.	O
n	O
:	O
(	O
a	O
)	O
choose	O
yn	O
∼	O
nor	O
(	O
w	O
·	O
xn	O
+	O
b	O
,	O
σ2	O
)	O
figure	O
7.3	O
:	O
pictorial	O
view	O
of	O
targets	O
versus	O
labels	O
reading	O
off	O
the	O
log	O
likelihood	B
of	O
a	O
dataset	O
from	O
this	O
generative	B
story	I
,	O
you	O
obtain	O
:	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
log	O
p	O
(	O
d	O
)	O
=	O
∑	O
n	O
−	O
1	O
2	O
log	O
(	O
σ2	O
)	O
−	O
1	O
2σ2	O
(	O
w	O
·	O
xn	O
+	O
b	O
−	O
yn	O
)	O
2	O
=	O
−	O
1	O
2σ2	O
∑	O
n	O
(	O
w	O
·	O
xn	O
+	O
b	O
−	O
yn	O
)	O
2	O
+	O
const	O
model	B
assumptions	O
(	O
7.39	O
)	O
remove	O
constants	O
(	O
7.40	O
)	O
this	O
is	O
precisely	O
the	O
linear	O
regression	O
model	B
you	O
encountered	O
in	O
section	O
6.6	O
!	O
to	O
minimizing	O
the	O
negative	O
log	B
probability	I
,	O
you	O
need	O
only	O
solve	O
for	O
the	O
regression	O
coefﬁcients	O
w	O
,	O
b	O
as	O
before	O
.	O
in	O
the	O
case	O
of	O
binary	O
classiﬁcation	O
,	O
using	O
a	O
gaussian	O
noise	B
model	O
does	O
not	O
make	O
sense	O
.	O
switching	O
to	O
a	O
bernoulli	O
model	B
,	O
which	O
de-	O
scribes	O
binary	O
outcomes	O
,	O
makes	O
more	O
sense	O
.	O
the	O
only	O
remaining	O
difﬁculty	O
is	O
that	O
the	O
parameter	O
of	O
a	O
bernoulli	O
is	O
a	O
value	O
between	O
zero	O
and	O
one	O
(	O
the	O
probability	O
of	O
“	O
heads	O
”	O
)	O
so	O
your	O
model	B
must	O
produce	O
such	O
values	O
.	O
a	O
classic	O
approach	O
is	O
to	O
produce	O
a	O
real-valued	O
target	O
,	O
as	O
before	O
,	O
and	O
then	O
transform	O
this	O
target	O
into	O
a	O
value	O
between	O
zero	O
and	O
one	O
,	O
so	O
that	O
−∞	O
maps	O
to	O
0	O
and	O
+∞	O
maps	O
to	O
1.	O
a	O
function	O
that	O
does	O
this	O
is	O
the	O
logistic	O
function1	O
,	O
deﬁned	O
below	O
and	O
plotted	O
in	O
figure	O
?	O
?	O
:	O
logistic	O
function	O
:	O
σ	O
(	O
z	O
)	O
=	O
1	O
1	O
+	O
exp	O
[	O
−z	O
]	O
=	O
exp	O
z	O
1	O
+	O
exp	O
z	O
(	O
7.41	O
)	O
the	O
logistic	O
function	O
has	O
several	O
nice	O
properties	O
that	O
you	O
can	O
verify	O
for	O
yourself	O
:	O
σ	O
(	O
−z	O
)	O
=	O
1	O
−	O
σ	O
(	O
z	O
)	O
and	O
∂σ/∂z	O
=	O
zσ2	O
(	O
z	O
)	O
.	O
using	O
the	O
logistic	O
function	O
,	O
you	O
can	O
write	O
down	O
a	O
generative	B
story	I
probabilistic	O
modeling	B
113	O
1	O
also	O
called	O
the	O
sigmoid	O
function	O
because	O
of	O
it	O
’	O
s	O
“	O
s	O
”	O
-shape	O
.	O
figure	O
7.4	O
:	O
sketch	O
of	O
logistic	O
function	O
for	O
binary	O
classiﬁcation	O
:	O
1.	O
for	O
each	O
example	O
n	O
=	O
1	O
.	O
.	O
.	O
n	O
:	O
(	O
a	O
)	O
compute	O
tn	O
=	O
σ	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
(	O
b	O
)	O
compute	O
zn	O
∼	O
ber	O
(	O
tn	O
)	O
(	O
c	O
)	O
return	O
yn	O
=	O
2zn	O
−	O
1	O
(	O
to	O
make	O
it	O
±1	O
)	O
the	O
log-likelihood	O
for	O
this	O
model	B
is	O
:	O
log	O
p	O
(	O
d	O
)	O
=	O
∑	O
n	O
(	O
cid:104	O
)	O
[	O
yn	O
=	O
+1	O
]	O
log	O
σ	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
+	O
[	O
yn	O
=	O
−1	O
]	O
log	O
σ	O
(	O
−w	O
·	O
xn	O
+	O
b	O
)	O
(	O
cid:105	O
)	O
log	O
σ	O
(	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
)	O
=	O
∑	O
n	O
=	O
−	O
∑	O
n	O
=	O
−	O
∑	O
n	O
log	O
[	O
1	O
+	O
exp	O
(	O
−yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
)	O
]	O
(	O
cid:96	O
)	O
(	O
log	O
)	O
(	O
yn	O
,	O
w	O
·	O
xn	O
+	O
b	O
)	O
model	B
and	O
properties	O
of	O
σ	O
(	O
7.42	O
)	O
join	O
terms	O
(	O
7.43	O
)	O
deﬁnition	O
of	O
σ	O
(	O
7.44	O
)	O
deﬁnition	O
of	O
(	O
cid:96	O
)	O
(	O
log	O
)	O
(	O
7.45	O
)	O
as	O
you	O
can	O
see	O
,	O
the	O
log-likelihood	O
is	O
precisely	O
the	O
negative	O
of	O
(	O
a	O
scaled	O
version	O
of	O
)	O
the	O
logistic	O
loss	O
from	O
chapter	O
6.	O
this	O
model	B
is	O
the	O
logistic	O
regression	O
model	B
,	O
and	O
this	O
is	O
where	O
logisitic	O
loss	O
originally	O
derived	O
from	O
.	O
todo	O
:	O
conditional	O
versus	O
joint	B
7.7	O
regularization	O
via	O
priors	O
in	O
the	O
foregoing	O
discussion	O
,	O
parameters	O
of	O
the	O
model	O
were	O
selected	O
according	O
to	O
the	O
maximum	O
likelihood	B
criteria	O
:	O
ﬁnd	O
the	O
parameters	O
θ	O
that	O
maximize	O
pθ	O
(	O
d	O
)	O
.	O
the	O
trouble	O
with	O
this	O
approach	O
is	O
easy	O
to	O
see	O
even	O
in	O
a	O
simple	O
coin	O
ﬂipping	O
example	O
.	O
if	O
you	O
ﬂip	O
a	O
coin	O
twice	O
and	O
it	O
comes	O
up	O
heads	O
both	O
times	O
,	O
the	O
maximum	O
likelihood	B
estimate	O
114	O
a	O
course	O
in	O
machine	O
learning	O
for	O
the	O
bias	O
of	O
the	O
coin	O
is	O
100	O
%	O
:	O
it	O
will	O
always	O
come	O
up	O
heads	O
.	O
this	O
is	O
true	O
even	O
if	O
you	O
had	O
only	O
ﬂipped	O
it	O
once	O
!	O
if	O
course	O
if	O
you	O
had	O
ﬂipped	O
it	O
one	O
million	O
times	O
and	O
it	O
had	O
come	O
up	O
heads	O
every	O
time	O
,	O
then	O
you	O
might	O
ﬁnd	O
this	O
to	O
be	O
a	O
reasonable	O
solution	O
.	O
this	O
is	O
clearly	O
undesirable	O
behavior	O
,	O
especially	O
since	O
data	O
is	O
expen-	O
sive	O
in	O
a	O
machine	O
learning	O
setting	O
.	O
one	O
solution	O
(	O
there	O
are	O
others	O
!	O
)	O
is	O
to	O
seek	O
parameters	O
that	O
balance	O
a	O
tradeoff	O
between	O
the	O
likelihood	O
of	O
the	O
data	O
and	O
some	O
prior	B
belief	O
you	O
have	O
about	O
what	O
values	O
of	O
those	O
parameters	O
are	O
likely	O
.	O
taking	O
the	O
case	O
of	O
the	O
logistic	O
regression	O
,	O
you	O
might	O
a	O
priori	O
believe	O
that	O
small	O
values	O
of	O
w	O
are	O
more	O
likely	O
than	O
large	O
values	O
,	O
and	O
choose	O
to	O
represent	O
this	O
as	O
a	O
gaussian	O
prior	B
on	O
each	O
component	O
of	O
w.	O
the	O
maximum	O
a	O
posteriori	O
principle	O
is	O
a	O
method	O
for	O
incoporat-	O
ing	O
both	O
data	O
and	O
prior	B
beliefs	O
to	O
obtain	O
a	O
more	O
balanced	O
parameter	O
estimate	O
.	O
in	O
abstract	O
terms	O
,	O
consider	O
a	O
probabilistic	O
model	O
over	O
data	O
d	O
that	O
is	O
parameterized	O
by	O
parameters	O
θ.	O
if	O
you	O
think	O
of	O
the	O
pa-	O
rameters	O
as	O
just	O
another	O
random	O
variable	O
,	O
then	O
you	O
can	O
write	O
this	O
model	B
as	O
p	O
(	O
d	O
|	O
θ	O
)	O
,	O
and	O
maximum	O
likelihood	O
amounts	O
to	O
choosing	O
θ	O
to	O
maximize	O
p	O
(	O
d	O
|	O
θ	O
)	O
.	O
however	O
,	O
you	O
might	O
instead	O
with	O
to	O
maximize	O
the	O
probability	O
of	O
the	O
parameters	O
,	O
given	O
the	O
data	O
.	O
namely	O
,	O
maximize	O
p	O
(	O
θ	O
|	O
d	O
)	O
.	O
this	O
term	O
is	O
known	O
as	O
the	O
posterior	O
distribution	O
on	O
θ	O
,	O
and	O
can	O
be	O
computed	O
by	O
bayes	O
’	O
rule	O
:	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
p	O
(	O
θ	O
|	O
d	O
)	O
posterior	B
=	O
evidence	B
likelihood	O
prior	B
p	O
(	O
θ	O
)	O
(	O
cid:122	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:123	O
)	O
(	O
cid:125	O
)	O
(	O
cid:124	O
)	O
p	O
(	O
d	O
|	O
θ	O
)	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
p	O
(	O
d	O
)	O
(	O
cid:90	O
)	O
dθp	O
(	O
θ	O
)	O
p	O
(	O
d	O
|	O
θ	O
)	O
,	O
where	O
p	O
(	O
d	O
)	O
=	O
(	O
7.46	O
)	O
this	O
reads	O
:	O
the	O
posterior	O
is	O
equal	O
to	O
the	O
prior	O
times	O
the	O
likelihood	O
di-	O
vided	O
by	O
the	O
evidence.2	O
the	O
evidence	O
is	O
a	O
scary-looking	O
term	O
(	O
it	O
has	O
an	O
integral	O
!	O
)	O
but	O
note	O
that	O
from	O
the	O
perspective	O
of	O
seeking	O
parameters	O
θ	O
than	O
maximize	O
the	O
posterior	O
,	O
the	O
evidence	O
is	O
just	O
a	O
constant	O
(	O
it	O
does	O
not	O
depend	O
on	O
θ	O
)	O
and	O
therefore	O
can	O
be	O
ignored	O
.	O
returning	O
to	O
the	O
logistic	O
regression	O
example	O
with	O
gaussian	O
priors	O
on	O
the	O
weights	O
,	O
the	O
log	O
posterior	B
looks	O
like	O
:	O
2	O
the	O
evidence	O
is	O
sometimes	O
called	O
the	O
marginal	O
likelihood	B
.	O
log	O
p	O
(	O
θ	O
|	O
d	O
)	O
=	O
−	O
∑	O
n	O
(	O
cid:96	O
)	O
(	O
log	O
)	O
(	O
yn	O
,	O
w	O
·	O
xn	O
+	O
b	O
)	O
−	O
∑	O
d	O
1	O
2σ2	O
w2	O
d	O
+	O
const	O
=	O
−	O
∑	O
n	O
(	O
cid:96	O
)	O
(	O
log	O
)	O
(	O
yn	O
,	O
w	O
·	O
xn	O
+	O
b	O
)	O
−	O
1	O
2σ2	O
||w||2	O
model	B
deﬁnition	O
(	O
7.47	O
)	O
(	O
7.48	O
)	O
and	O
therefore	O
reduces	O
to	O
a	O
regularized	O
logistic	O
function	O
,	O
with	O
a	O
squared	O
2-norm	O
regularizer	B
on	O
the	O
weights	O
.	O
(	O
a	O
1-norm	O
regularizer	B
probabilistic	O
modeling	B
115	O
can	O
be	O
obtained	O
by	O
using	O
a	O
laplace	O
prior	B
on	O
w	O
rather	O
than	O
a	O
gaussian	O
prior	B
on	O
w.	O
)	O
7.8	O
exercises	O
exercise	O
7.1.	O
todo	O
.	O
.	O
.	O
8	O
|	O
neural	B
networks	I
–	O
the	O
first	O
learning	O
models	O
you	O
learned	O
about	O
(	O
decision	B
trees	I
and	O
nearest	B
neighbor	I
models	O
)	O
created	O
complex	O
,	O
non-linear	B
decision	O
boundaries	O
.	O
we	O
moved	O
from	O
there	O
to	O
the	O
perceptron	O
,	O
perhaps	O
the	O
most	O
classic	O
linear	O
model	O
.	O
at	O
this	O
point	O
,	O
we	O
will	O
move	O
back	O
to	O
non-	O
linear	O
learning	O
models	O
,	O
but	O
using	O
all	O
that	O
we	O
have	O
learned	O
about	O
linear	O
learning	O
thus	O
far	O
.	O
this	O
chapter	O
presents	O
an	O
extension	O
of	O
perceptron	B
learning	O
to	O
non-	O
linear	O
decision	O
boundaries	O
,	O
taking	O
the	O
biological	O
inspiration	O
of	O
neu-	O
rons	O
even	O
further	O
.	O
in	O
the	O
perceptron	O
,	O
we	O
thought	O
of	O
the	O
input	O
data	O
point	O
(	O
e.g.	O
,	O
an	O
image	O
)	O
as	O
being	O
directly	O
connected	O
to	O
an	O
output	O
(	O
e.g.	O
,	O
label	B
)	O
.	O
this	O
is	O
often	O
called	O
a	O
single-layer	B
network	I
because	O
there	O
is	O
one	O
layer	O
of	O
weights	B
.	O
now	O
,	O
instead	O
of	O
directly	O
connecting	O
the	O
inputs	O
to	O
the	O
outputs	O
,	O
we	O
will	O
insert	O
a	O
layer	O
of	O
“	O
hidden	O
”	O
nodes	O
,	O
moving	O
from	O
a	O
single-layer	B
network	I
to	O
a	O
multi-layer	B
network	I
.	O
but	O
introducing	O
a	O
non-linearity	O
at	O
inner	O
layers	O
,	O
this	O
will	O
give	O
us	O
non-linear	B
decision	O
boundaires	O
.	O
in	O
fact	O
,	O
such	O
networks	O
are	O
able	O
to	O
express	O
almost	O
any	O
function	O
we	O
want	O
,	O
not	O
just	O
linear	O
functions	O
.	O
the	O
trade-off	O
for	O
this	O
ﬂex-	O
ibility	O
is	O
increased	O
complexity	B
in	O
parameter	O
tuning	O
and	O
model	B
design	O
.	O
8.1	O
bio-inspired	O
multi-layer	O
networks	O
one	O
of	O
the	O
major	O
weaknesses	O
of	O
linear	O
models	O
,	O
like	O
perceptron	B
and	O
the	O
regularized	O
linear	O
models	O
from	O
the	O
previous	O
chapter	O
,	O
is	O
that	O
they	O
are	O
linear	O
!	O
namely	O
,	O
they	O
are	O
unable	O
to	O
learn	O
arbitrary	O
decision	O
bound-	O
aries	O
.	O
in	O
contrast	O
,	O
decision	B
trees	I
and	O
knn	O
could	O
learn	O
arbitrarily	O
complicated	O
decision	O
boundaries	O
.	O
one	O
approach	O
to	O
doing	O
this	O
is	O
to	O
chain	O
together	O
a	O
collection	O
of	O
perceptrons	O
to	O
build	O
more	O
complex	O
neural	B
networks	I
.	O
an	O
example	O
of	O
a	O
two-layer	B
network	I
is	O
shown	O
in	O
figure	O
8.1.	O
here	O
,	O
you	O
can	O
see	O
ﬁve	O
inputs	O
(	O
features	B
)	O
that	O
are	O
fed	O
into	O
two	O
hidden	B
units	I
.	O
these	O
hidden	B
units	I
are	O
then	O
fed	O
in	O
to	O
a	O
single	O
output	B
unit	I
.	O
each	O
edge	O
in	O
this	O
ﬁgure	O
corresponds	O
to	O
a	O
different	O
weight	O
.	O
(	O
even	O
though	O
it	O
looks	O
like	O
there	O
are	O
three	O
layers	O
,	O
this	O
is	O
called	O
a	O
two-layer	B
network	I
because	O
we	O
don	O
’	O
t	O
count	O
learning	O
objectives	O
:	O
•	O
explain	O
the	O
biological	O
inspiration	O
for	O
multi-layer	O
neural	O
networks	O
.	O
•	O
construct	O
a	O
two-layer	B
network	I
that	O
can	O
solve	O
the	O
xor	O
problem	O
.	O
•	O
implement	O
the	O
back-propogation	O
algorithm	B
for	O
training	O
multi-layer	O
networks	O
.	O
•	O
explain	O
the	O
trade-off	O
between	O
depth	O
and	O
breadth	O
in	O
network	O
structure	O
.	O
•	O
contrast	O
neural	B
networks	I
with	O
ra-	O
dial	O
basis	O
functions	O
with	O
k-nearest	O
neighbor	O
learning	O
.	O
dependencies	O
:	O
figure	O
8.1	O
:	O
picture	O
of	O
a	O
two-layer	B
network	I
with	O
5	O
inputs	O
and	O
two	O
hidden	B
units	I
the	O
inputs	O
as	O
a	O
real	O
layer	O
.	O
that	O
is	O
,	O
it	O
’	O
s	O
two	O
layers	O
of	O
trained	O
weights	B
.	O
)	O
prediction	O
with	O
a	O
neural	B
network	I
is	O
a	O
straightforward	O
generaliza-	O
tion	O
of	O
prediction	O
with	O
a	O
perceptron	B
.	O
first	O
you	O
compute	O
activations	B
of	O
the	O
nodes	O
in	O
the	O
hidden	O
unit	O
based	O
on	O
the	O
inputs	O
and	O
the	O
input	O
weights	B
.	O
then	O
you	O
compute	O
activations	B
of	O
the	O
output	O
unit	O
given	O
the	O
hidden	O
unit	O
activations	O
and	O
the	O
second	O
layer	O
of	O
weights	B
.	O
the	O
only	O
major	O
difference	O
between	O
this	O
computation	O
and	O
the	O
per-	O
ceptron	O
computation	O
is	O
that	O
the	O
hidden	O
units	O
compute	O
a	O
non-linear	B
function	O
of	O
their	O
inputs	O
.	O
this	O
is	O
usually	O
called	O
the	O
activation	O
function	O
or	O
link	B
function	I
.	O
more	O
formally	O
,	O
if	O
wi	O
,	O
d	O
is	O
the	O
weights	O
on	O
the	O
edge	O
connecting	O
input	O
d	O
to	O
hidden	O
unit	O
i	O
,	O
then	O
the	O
activation	O
of	O
hidden	O
unit	O
i	O
is	O
computed	O
as	O
:	O
hi	O
=	O
f	O
(	O
wi	O
·	O
x	O
)	O
(	O
8.1	O
)	O
where	O
f	O
is	O
the	O
link	O
function	O
and	O
wi	O
refers	O
to	O
the	O
vector	O
of	O
weights	B
feeding	O
in	O
to	O
node	O
i.	O
one	O
example	O
link	B
function	I
is	O
the	O
sign	O
function	O
.	O
that	O
is	O
,	O
if	O
the	O
incoming	O
signal	O
is	O
negative	O
,	O
the	O
activation	O
is	O
−1	O
.	O
otherwise	O
the	O
activation	O
is	O
+1	O
.	O
this	O
is	O
a	O
potentially	O
useful	O
activiation	O
function	O
,	O
but	O
you	O
might	O
already	O
have	O
guessed	O
the	O
problem	O
with	O
it	O
:	O
it	O
is	O
non-	O
differentiable	O
.	O
explain	O
bias	B
!	O
!	O
!	O
a	O
more	O
popular	O
link	B
function	I
is	O
the	O
hyperbolic	O
tangent	O
function	O
,	O
tanh	O
.	O
a	O
comparison	O
between	O
the	O
sign	O
function	O
and	O
the	O
tanh	O
function	O
is	O
in	O
figure	O
8.2.	O
as	O
you	O
can	O
see	O
,	O
it	O
is	O
a	O
reasonable	O
approximation	O
to	O
the	O
sign	O
function	O
,	O
but	O
is	O
convenient	O
in	O
that	O
it	O
is	O
differentiable.1	O
because	O
it	O
looks	O
like	O
an	O
“	O
s	O
”	O
and	O
because	O
the	O
greek	O
character	O
for	O
“	O
s	O
”	O
is	O
“	O
sigma	O
,	O
”	O
such	O
functions	O
are	O
usually	O
called	O
sigmoid	B
functions	O
.	O
assuming	O
for	O
now	O
that	O
we	O
are	O
using	O
tanh	O
as	O
the	O
link	O
function	O
,	O
the	O
overall	O
prediction	O
made	O
by	O
a	O
two-layer	B
network	I
can	O
be	O
computed	O
using	O
algorithm	B
8.1.	O
this	O
function	O
takes	O
a	O
matrix	O
of	O
weights	B
w	O
cor-	O
responding	O
to	O
the	O
ﬁrst	O
layer	O
weights	B
and	O
a	O
vector	B
of	O
weights	B
v	O
corre-	O
sponding	O
to	O
the	O
second	O
layer	O
.	O
you	O
can	O
write	O
this	O
entire	O
computation	O
out	O
in	O
one	O
line	O
as	O
:	O
vi	O
tanh	O
(	O
wi	O
·	O
ˆx	O
)	O
ˆy	O
=	O
∑	O
i	O
=	O
v	O
·	O
tanh	O
(	O
wˆx	O
)	O
(	O
8.2	O
)	O
(	O
8.3	O
)	O
where	O
the	O
second	O
line	O
is	O
short	O
hand	O
assuming	O
that	O
tanh	O
can	O
take	O
a	O
vector	B
as	O
input	O
and	O
product	O
a	O
vector	B
as	O
output	O
.	O
neural	B
networks	I
117	O
figure	O
8.2	O
:	O
picture	O
of	O
sign	B
versus	O
tanh	O
1	O
it	O
’	O
s	O
derivative	O
is	O
just	O
1	O
−	O
tanh2	O
(	O
x	O
)	O
.	O
?	O
is	O
it	O
necessary	O
to	O
use	O
a	O
link	B
function	I
at	O
all	O
?	O
what	O
would	O
happen	O
if	O
you	O
just	O
used	O
the	O
identify	O
function	O
as	O
a	O
link	O
?	O
118	O
a	O
course	O
in	O
machine	O
learning	O
algorithm	B
24	O
twolayernetworkpredict	O
(	O
w	O
,	O
v	O
,	O
ˆx	O
)	O
1	O
:	O
for	O
i	O
=	O
1	O
to	O
number	O
of	O
hidden	B
units	I
do	O
2	O
:	O
3	O
:	O
end	O
for	O
4	O
:	O
return	O
v	O
·	O
h	O
hi	O
←	O
tanh	O
(	O
wi	O
·	O
ˆx	O
)	O
//	O
compute	O
activation	O
of	O
hidden	O
unit	O
i	O
//	O
compute	O
output	B
unit	I
the	O
claim	O
is	O
that	O
two-layer	O
neural	O
networks	O
are	O
more	O
expressive	O
than	O
single	O
layer	O
networks	O
(	O
i.e.	O
,	O
perceptrons	O
)	O
.	O
to	O
see	O
this	O
,	O
you	O
can	O
construct	O
a	O
very	O
small	O
two-layer	B
network	I
for	O
solving	O
the	O
xor	O
prob-	O
lem	O
.	O
for	O
simplicity	O
,	O
suppose	O
that	O
the	O
data	O
set	O
consists	O
of	O
four	O
data	O
points	O
,	O
given	O
in	O
table	O
8.1.	O
the	O
classiﬁcation	O
rule	O
is	O
that	O
y	O
=	O
+1	O
if	O
an	O
only	O
if	O
x1	O
=	O
x2	O
,	O
where	O
the	O
features	O
are	O
just	O
±1	O
.	O
to	O
achieve	O
the	O
“	O
or	O
”	O
behavior	O
,	O
you	O
can	O
start	O
by	O
setting	O
the	O
bias	O
to	O
you	O
can	O
solve	O
this	O
problem	O
using	O
a	O
two	O
layer	O
network	O
with	O
two	O
hidden	B
units	I
.	O
the	O
key	O
idea	O
is	O
to	O
make	O
the	O
ﬁrst	O
hidden	O
unit	O
compute	O
an	O
“	O
or	O
”	O
function	O
:	O
x1	O
∨	O
x2	O
.	O
the	O
second	O
hidden	O
unit	O
can	O
compute	O
an	O
“	O
and	O
”	O
function	O
:	O
x1	O
∧	O
x2	O
.	O
the	O
the	O
output	O
can	O
combine	O
these	O
into	O
a	O
single	O
prediction	O
that	O
mimics	O
xor	O
.	O
once	O
you	O
have	O
the	O
ﬁrst	O
hidden	O
unit	O
activate	O
for	O
“	O
or	O
”	O
and	O
the	O
second	O
for	O
“	O
and	O
,	O
”	O
you	O
need	O
only	O
set	O
the	O
output	O
weights	B
as	O
−2	O
and	O
+1	O
,	O
respectively	O
.	O
−0.5	O
and	O
the	O
weights	O
for	O
the	O
two	O
“	O
real	O
”	O
features	B
as	O
both	O
being	O
1.	O
you	O
can	O
check	O
for	O
yourself	O
that	O
this	O
will	O
do	O
the	O
“	O
right	O
thing	O
”	O
if	O
the	O
link	O
function	O
were	O
the	O
sign	O
function	O
.	O
of	O
course	O
it	O
’	O
s	O
not	O
,	O
it	O
’	O
s	O
tanh	O
.	O
to	O
get	O
tanh	O
to	O
mimic	O
sign	B
,	O
you	O
need	O
to	O
make	O
the	O
dot	O
product	O
either	O
really	O
really	O
large	O
or	O
really	O
really	O
small	O
.	O
you	O
can	O
accomplish	O
this	O
by	O
set-	O
ting	O
the	O
bias	O
to	O
−500	O
,	O
000	O
and	O
both	O
of	O
the	O
two	O
weights	B
to	O
1	O
,	O
000	O
,	O
000.	O
now	O
,	O
the	O
activation	O
of	O
this	O
unit	O
will	O
be	O
just	O
slightly	O
above	O
−1	O
for	O
x	O
=	O
(	O
cid:104	O
)	O
−1	O
,	O
−1	O
(	O
cid:105	O
)	O
and	O
just	O
slightly	O
below	O
+1	O
for	O
the	O
other	O
three	O
examples	B
.	O
at	O
this	O
point	O
you	O
’	O
ve	O
seen	O
that	O
one-layer	O
networks	O
(	O
aka	O
percep-	O
trons	O
)	O
can	O
represent	O
any	O
linear	O
function	O
and	O
only	O
linear	O
functions	O
.	O
you	O
’	O
ve	O
also	O
seen	O
that	O
two-layer	O
networks	O
can	O
represent	O
non-linear	B
functions	O
like	O
xor	O
.	O
a	O
natural	O
question	O
is	O
:	O
do	O
you	O
get	O
additional	O
representational	O
power	O
by	O
moving	O
beyond	O
two	O
layers	O
?	O
the	O
answer	O
is	O
partially	O
provided	O
in	O
the	O
following	O
theorem	O
,	O
due	O
originally	O
to	O
george	O
cybenko	O
for	O
one	O
particular	O
type	O
of	O
link	B
function	I
,	O
and	O
ex-	O
tended	O
later	O
by	O
kurt	O
hornik	O
to	O
arbitrary	O
link	O
functions	O
.	O
theorem	O
9	O
(	O
two-layer	O
networks	O
are	O
universal	O
function	O
approxima-	O
tors	O
)	O
.	O
let	O
f	O
be	O
a	O
continuous	O
function	O
on	O
a	O
bounded	O
subset	O
of	O
d-dimensional	O
space	O
.	O
then	O
there	O
exists	O
a	O
two-layer	O
neural	O
network	O
ˆf	O
with	O
a	O
ﬁnite	O
number	O
of	O
hidden	B
units	I
that	O
approximate	O
f	O
arbitrarily	O
well	O
.	O
namely	O
,	O
for	O
all	O
x	O
in	O
the	O
domain	O
of	O
f	O
,	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
f	O
(	O
x	O
)	O
−	O
ˆf	O
(	O
x	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
<	O
	O
.	O
or	O
,	O
in	O
colloquial	O
terms	O
“	O
two-layer	O
networks	O
can	O
approximate	O
any	O
x0	O
x1	O
x2	O
y	O
+1	O
+1	O
+1	O
+1	O
+1	O
+1	O
-1	O
-1	O
-1	O
+1	O
+1	O
-1	O
-1	O
+1	O
-1	O
+1	O
table	O
8.1	O
:	O
small	O
xor	O
data	O
set	O
.	O
?	O
verify	O
that	O
these	O
output	O
weights	O
will	O
actually	O
give	O
you	O
xor	O
.	O
?	O
this	O
shows	O
how	O
to	O
create	O
an	O
“	O
or	O
”	O
function	O
.	O
how	O
can	O
you	O
create	O
an	O
“	O
and	O
”	O
function	O
?	O
neural	B
networks	I
119	O
function.	O
”	O
this	O
is	O
a	O
remarkable	O
theorem	O
.	O
practically	O
,	O
it	O
says	O
that	O
if	O
you	O
give	O
me	O
a	O
function	O
f	O
and	O
some	O
error	O
tolerance	O
parameter	O
	O
,	O
i	O
can	O
construct	O
a	O
two	O
layer	O
network	O
that	O
computes	O
f.	O
in	O
a	O
sense	O
,	O
it	O
says	O
that	O
going	O
from	O
one	O
layer	O
to	O
two	O
layers	O
completely	O
changes	O
the	O
representational	O
capacity	O
of	O
your	O
model	B
.	O
when	O
working	O
with	O
two-layer	O
networks	O
,	O
the	O
key	O
question	O
is	O
:	O
how	O
many	O
hidden	B
units	I
should	O
i	O
have	O
?	O
if	O
your	O
data	O
is	O
d	O
dimensional	O
and	O
you	O
have	O
k	O
hidden	B
units	I
,	O
then	O
the	O
total	O
number	O
of	O
parameters	O
is	O
(	O
d	O
+	O
2	O
)	O
k.	O
(	O
the	O
ﬁrst	O
+1	O
is	O
from	O
the	O
bias	O
,	O
the	O
second	O
is	O
from	O
the	O
second	O
layer	O
of	O
weights	B
.	O
)	O
following	O
on	O
from	O
the	O
heuristic	O
that	O
you	O
should	O
have	O
one	O
to	O
two	O
examples	B
for	O
each	O
parameter	O
you	O
are	O
trying	O
to	O
estimate	O
,	O
this	O
suggests	O
a	O
method	O
for	O
choosing	O
the	O
number	O
of	O
hid-	O
den	O
units	O
as	O
roughly	O
(	O
cid:98	O
)	O
n	O
d	O
(	O
cid:99	O
)	O
.	O
in	O
other	O
words	O
,	O
if	O
you	O
have	O
tons	O
and	O
tons	O
of	O
examples	B
,	O
you	O
can	O
safely	O
have	O
lots	O
of	O
hidden	B
units	I
.	O
if	O
you	O
only	O
have	O
a	O
few	O
examples	B
,	O
you	O
should	O
probably	O
restrict	O
the	O
number	O
of	O
hidden	B
units	I
in	O
your	O
network	O
.	O
the	O
number	O
of	O
units	O
is	O
both	O
a	O
form	O
of	O
inductive	B
bias	I
and	O
a	O
form	O
of	O
regularization	O
.	O
in	O
both	O
view	O
,	O
the	O
number	O
of	O
hidden	B
units	I
controls	O
how	O
complex	O
your	O
function	O
will	O
be	O
.	O
lots	O
of	O
hidden	B
units	I
⇒	O
very	O
complicated	O
function	O
.	O
figure	O
?	O
?	O
shows	O
training	O
and	O
test	B
error	I
for	O
neural	B
networks	I
trained	O
with	O
different	O
numbers	O
of	O
hidden	B
units	I
.	O
as	O
the	O
number	O
increases	O
,	O
training	O
performance	O
continues	O
to	O
get	O
better	O
.	O
but	O
at	O
some	O
point	O
,	O
test	O
performance	O
gets	O
worse	O
because	O
the	O
network	O
has	O
overﬁt	O
the	O
data	O
.	O
8.2	O
the	O
back-propagation	O
algorithm	B
the	O
back-propagation	B
algorithm	O
is	O
a	O
classic	O
approach	O
to	O
training	O
neural	O
networks	O
.	O
although	O
it	O
was	O
not	O
originally	O
seen	O
this	O
way	O
,	O
based	O
on	O
what	O
you	O
know	O
from	O
the	O
last	O
chapter	O
,	O
you	O
can	O
summarize	O
back-	O
propagation	O
as	O
:	O
back-propagation	B
=	O
gradient	B
descent	I
+	O
chain	B
rule	I
(	O
8.4	O
)	O
more	O
speciﬁcally	O
,	O
the	O
set	O
up	O
is	O
exactly	O
the	O
same	O
as	O
before	O
.	O
you	O
are	O
going	O
to	O
optimize	O
the	O
weights	O
in	O
the	O
network	O
to	O
minimize	O
some	O
ob-	O
jective	O
function	O
.	O
the	O
only	O
difference	O
is	O
that	O
the	O
predictor	O
is	O
no	O
longer	O
linear	O
(	O
i.e.	O
,	O
ˆy	O
=	O
w	O
·	O
x	O
+	O
b	O
)	O
but	O
now	O
non-linear	B
(	O
i.e.	O
,	O
v	O
·	O
tanh	O
(	O
wˆx	O
)	O
)	O
.	O
the	O
only	O
question	O
is	O
how	O
to	O
do	O
gradient	B
descent	I
on	O
this	O
more	O
compli-	O
cated	O
objective	O
.	O
for	O
now	O
,	O
we	O
will	O
ignore	O
the	O
idea	O
of	O
regularization	O
.	O
this	O
is	O
for	O
two	O
reasons	O
.	O
the	O
ﬁrst	O
is	O
that	O
you	O
already	O
know	O
how	O
to	O
deal	O
with	O
regular-	O
ization	O
,	O
so	O
everything	O
you	O
’	O
ve	O
learned	O
before	O
applies	O
.	O
the	O
second	O
is	O
that	O
historically	O
,	O
neural	B
networks	I
have	O
not	O
been	O
regularized	O
.	O
instead	O
,	O
120	O
a	O
course	O
in	O
machine	O
learning	O
people	O
have	O
used	O
early	B
stopping	I
as	O
a	O
method	O
for	O
controlling	O
overﬁt-	O
ting	O
.	O
presently	O
,	O
it	O
’	O
s	O
not	O
obvious	O
which	O
is	O
a	O
better	O
solution	O
:	O
both	O
are	O
valid	O
options	O
.	O
to	O
be	O
completely	O
explicit	O
,	O
we	O
will	O
focus	O
on	O
optimizing	O
squared	O
error	O
.	O
again	O
,	O
this	O
is	O
mostly	O
for	O
historic	O
reasons	O
.	O
you	O
could	O
easily	O
replace	O
squared	O
error	O
with	O
your	O
loss	B
function	I
of	O
choice	O
.	O
our	O
overall	O
objective	O
is	O
:	O
(	O
cid:32	O
)	O
(	O
cid:33	O
)	O
2	O
min	O
w	O
,	O
v	O
∑	O
n	O
1	O
2	O
yn	O
−	O
∑	O
i	O
vi	O
f	O
(	O
wi	O
·	O
xn	O
)	O
(	O
8.5	O
)	O
here	O
,	O
f	O
is	O
some	O
link	B
function	I
like	O
tanh	O
.	O
the	O
easy	O
case	O
is	O
to	O
differentiate	O
this	O
with	O
respect	O
to	O
v	O
:	O
the	O
weights	O
for	O
the	O
output	O
unit	O
.	O
without	O
even	O
doing	O
any	O
math	O
,	O
you	O
should	O
be	O
able	O
to	O
guess	O
what	O
this	O
looks	O
like	O
.	O
the	O
way	O
to	O
think	O
about	O
it	O
is	O
that	O
from	O
vs	O
perspective	O
,	O
it	O
is	O
just	O
a	O
linear	O
model	O
,	O
attempting	O
to	O
minimize	O
squared	O
error	O
.	O
the	O
only	O
“	O
funny	O
”	O
thing	O
is	O
that	O
its	O
inputs	O
are	O
the	O
activa-	O
tions	O
h	O
rather	O
than	O
the	O
examples	O
x.	O
so	O
the	O
gradient	O
with	O
respect	O
to	O
v	O
is	O
just	O
as	O
for	O
the	O
linear	O
case	O
.	O
to	O
make	O
things	O
notationally	O
more	O
convenient	O
,	O
let	O
en	O
denote	O
the	O
error	O
on	O
the	O
nth	O
example	O
(	O
i.e.	O
,	O
the	O
blue	O
term	O
above	O
)	O
,	O
and	O
let	O
hn	O
denote	O
the	O
vector	O
of	O
hidden	O
unit	O
activations	B
on	O
that	O
example	O
.	O
then	O
:	O
∇v	O
=	O
−	O
∑	O
n	O
enhn	O
(	O
8.6	O
)	O
this	O
is	O
exactly	O
like	O
the	O
linear	O
case	O
.	O
one	O
way	O
of	O
interpreting	O
this	O
is	O
:	O
how	O
would	O
the	O
output	O
weights	B
have	O
to	O
change	O
to	O
make	O
the	O
prediction	O
better	O
?	O
this	O
is	O
an	O
easy	O
question	O
to	O
answer	O
because	O
they	O
can	O
easily	O
measure	O
how	O
their	O
changes	O
affect	O
the	O
output	O
.	O
the	O
more	O
complicated	O
aspect	O
to	O
deal	O
with	O
is	O
the	O
weights	O
corre-	O
sponding	O
to	O
the	O
ﬁrst	O
layer	O
.	O
the	O
reason	O
this	O
is	O
difﬁcult	O
is	O
because	O
the	O
weights	O
in	O
the	O
ﬁrst	O
layer	O
aren	O
’	O
t	O
necessarily	O
trying	O
to	O
produce	O
speciﬁc	O
values	O
,	O
say	O
0	O
or	O
5	O
or	O
−2.1	O
.	O
they	O
are	O
simply	O
trying	O
to	O
produce	O
acti-	O
vations	O
that	O
get	O
fed	O
to	O
the	O
output	O
layer	O
.	O
so	O
the	O
change	O
they	O
want	O
to	O
make	O
depends	O
crucially	O
on	O
how	O
the	O
output	O
layer	O
interprets	O
them	O
.	O
thankfully	O
,	O
the	O
chain	O
rule	O
of	O
calculus	O
saves	O
us	O
.	O
ignoring	O
the	O
sum	O
(	O
cid:32	O
)	O
over	O
data	O
points	O
,	O
we	O
can	O
compute	O
:	O
vi	O
f	O
(	O
wi	O
·	O
x	O
)	O
l	O
(	O
w	O
)	O
=	O
(	O
cid:33	O
)	O
2	O
=	O
1	O
y	O
−	O
∑	O
2	O
i	O
∂l	O
(	O
cid:32	O
)	O
∂	O
fi	O
=	O
−	O
∂	O
fi	O
∂wi	O
y	O
−	O
∑	O
i	O
=	O
f	O
(	O
cid:48	O
)	O
(	O
wi	O
·	O
x	O
)	O
x	O
∂l	O
∂wi	O
∂l	O
∂	O
fi	O
∂	O
fi	O
∂wi	O
(	O
cid:33	O
)	O
vi	O
=	O
−evi	O
vi	O
f	O
(	O
wi	O
·	O
x	O
)	O
(	O
8.7	O
)	O
(	O
8.8	O
)	O
(	O
8.9	O
)	O
(	O
8.10	O
)	O
neural	B
networks	I
121	O
algorithm	B
25	O
twolayernetworktrain	O
(	O
d	O
,	O
η	O
,	O
k	O
,	O
maxiter	O
)	O
1	O
:	O
w	O
←	O
d×k	O
matrix	O
of	O
small	O
random	O
values	O
2	O
:	O
v	O
←	O
k-vector	O
of	O
small	O
random	O
values	O
3	O
:	O
for	O
iter	O
=	O
1	O
.	O
.	O
.	O
maxiter	O
do	O
4	O
:	O
g	O
←	O
d×k	O
matrix	O
of	O
zeros	O
g	O
←	O
k-vector	O
of	O
zeros	O
for	O
all	O
(	O
x	O
,	O
y	O
)	O
∈	O
d	O
do	O
for	O
i	O
=	O
1	O
to	O
k	O
do	O
ai	O
←	O
wi	O
·	O
ˆx	O
hi	O
←	O
tanh	O
(	O
ai	O
)	O
5	O
:	O
6	O
:	O
9	O
:	O
//	O
initialize	O
input	O
layer	O
weights	B
//	O
initialize	O
output	O
layer	O
weights	B
//	O
initialize	O
input	O
layer	O
gradient	B
//	O
initialize	O
output	O
layer	O
gradient	B
7	O
:	O
8	O
:	O
10	O
:	O
11	O
:	O
12	O
:	O
13	O
:	O
14	O
:	O
15	O
:	O
16	O
:	O
end	O
for	O
17	O
:	O
18	O
:	O
w	O
←	O
w	O
−	O
ηg	O
end	O
for	O
v	O
←	O
v	O
−	O
ηg	O
19	O
:	O
20	O
:	O
end	O
for	O
21	O
:	O
return	O
w	O
,	O
v	O
end	O
for	O
ˆy	O
←	O
v	O
·	O
h	O
e	O
←	O
y	O
−	O
ˆy	O
g	O
←	O
g	O
−	O
eh	O
for	O
i	O
=	O
1	O
to	O
k	O
do	O
gi	O
←	O
gi	O
−	O
evi	O
(	O
1	O
−	O
tanh2	O
(	O
ai	O
)	O
)	O
x	O
//	O
compute	O
activation	O
of	O
hidden	O
unit	O
i	O
//	O
compute	O
output	B
unit	I
//	O
compute	O
error	O
//	O
update	O
gradient	B
for	O
output	O
layer	O
//	O
update	O
gradient	B
for	O
input	O
layer	O
//	O
update	O
input	O
layer	O
weights	B
//	O
update	O
output	O
layer	O
weights	B
putting	O
this	O
together	O
,	O
we	O
get	O
that	O
the	O
gradient	O
with	O
respect	O
to	O
wi	O
is	O
:	O
∇wi	O
=	O
−evi	O
f	O
(	O
cid:48	O
)	O
(	O
wi	O
·	O
x	O
)	O
x	O
(	O
8.11	O
)	O
intuitively	O
you	O
can	O
make	O
sense	O
of	O
this	O
.	O
if	O
the	O
overall	O
error	O
of	O
the	O
predictor	O
(	O
e	O
)	O
is	O
small	O
,	O
you	O
want	O
to	O
make	O
small	O
steps	O
.	O
if	O
vi	O
is	O
small	O
for	O
hidden	O
unit	O
i	O
,	O
then	O
this	O
means	O
that	O
the	O
output	O
is	O
not	O
particularly	O
sensitive	O
to	O
the	O
activation	O
of	O
the	O
ith	O
hidden	O
unit	O
.	O
thus	O
,	O
its	O
gradient	B
should	O
be	O
small	O
.	O
if	O
vi	O
ﬂips	O
sign	B
,	O
the	O
gradient	O
at	O
wi	O
should	O
also	O
ﬂip	O
signs	O
.	O
the	O
name	O
back-propagation	B
comes	O
from	O
the	O
fact	O
that	O
you	O
propagate	O
gradients	O
backward	O
through	O
the	O
network	O
,	O
starting	O
at	O
the	O
end	O
.	O
the	O
complete	O
instantiation	O
of	O
gradient	B
descent	I
for	O
a	O
two	O
layer	O
network	O
with	O
k	O
hidden	B
units	I
is	O
sketched	O
in	O
algorithm	B
8.2.	O
note	O
that	O
this	O
really	O
is	O
exactly	O
a	O
gradient	B
descent	I
algorithm	O
;	O
the	O
only	O
different	O
is	O
that	O
the	O
computation	O
of	O
the	O
gradients	O
of	O
the	O
input	O
layer	O
is	O
moderately	O
complicated	O
.	O
as	O
a	O
bit	O
of	O
practical	O
advice	O
,	O
implementing	O
the	O
back-propagation	O
algorithm	B
can	O
be	O
a	O
bit	O
tricky	O
.	O
sign	B
errors	O
often	O
abound	O
.	O
a	O
useful	O
trick	O
is	O
ﬁrst	O
to	O
keep	O
w	O
ﬁxed	O
and	O
work	O
on	O
just	O
training	O
v.	O
then	O
keep	O
v	O
ﬁxed	O
and	O
work	O
on	O
training	O
w.	O
then	O
put	O
them	O
together	O
.	O
?	O
what	O
would	O
happen	O
to	O
this	O
algo-	O
rithm	O
if	O
you	O
wanted	O
to	O
optimize	O
exponential	B
loss	I
instead	O
of	O
squared	O
error	O
?	O
what	O
if	O
you	O
wanted	O
to	O
add	O
in	O
weight	O
regularization	O
?	O
?	O
if	O
you	O
like	O
matrix	O
calculus	O
,	O
derive	O
the	O
same	O
algorithm	B
starting	O
from	O
eq	O
(	O
8.3	O
)	O
.	O
122	O
a	O
course	O
in	O
machine	O
learning	O
8.3	O
initialization	O
and	O
convergence	O
of	O
neural	B
networks	I
based	O
on	O
what	O
you	O
know	O
about	O
linear	O
models	O
,	O
you	O
might	O
be	O
tempted	O
to	O
initialize	O
all	O
the	O
weights	B
in	O
a	O
neural	B
network	I
to	O
zero	O
.	O
you	O
might	O
also	O
have	O
noticed	O
that	O
in	O
algorithm	B
?	O
?	O
,	O
this	O
is	O
not	O
what	O
’	O
s	O
done	O
:	O
they	O
’	O
re	O
initialized	O
to	O
small	O
random	O
values	O
.	O
the	O
question	O
is	O
why	O
?	O
the	O
answer	O
is	O
because	O
an	O
initialization	O
of	O
w	O
=	O
0	O
and	O
v	O
=	O
0	O
will	O
lead	O
to	O
“	O
uninteresting	O
”	O
solutions	O
.	O
in	O
other	O
words	O
,	O
if	O
you	O
initialize	O
the	O
model	O
in	O
this	O
way	O
,	O
it	O
will	O
eventually	O
get	O
stuck	O
in	O
a	O
bad	O
local	O
optimum	O
.	O
to	O
see	O
this	O
,	O
ﬁrst	O
realize	O
that	O
on	O
any	O
example	O
x	O
,	O
the	O
activation	O
hi	O
of	O
the	O
hidden	O
units	O
will	O
all	O
be	O
zero	O
since	O
w	O
=	O
0.	O
this	O
means	O
that	O
on	O
the	O
ﬁrst	O
iteration	B
,	O
the	O
gradient	O
on	O
the	O
output	O
weights	B
(	O
v	O
)	O
will	O
be	O
zero	O
,	O
so	O
they	O
will	O
stay	O
put	O
.	O
furthermore	O
,	O
the	O
gradient	O
w1	O
,	O
d	O
for	O
the	O
dth	O
feature	O
on	O
the	O
ith	O
unit	O
will	O
be	O
exactly	O
the	O
same	O
as	O
the	O
gradient	O
w2	O
,	O
d	O
for	O
the	O
same	O
feature	O
on	O
the	O
second	O
unit	O
.	O
this	O
means	O
that	O
the	O
weight	O
matrix	O
,	O
after	O
a	O
gradient	B
step	O
,	O
will	O
change	O
in	O
exactly	O
the	O
same	O
way	O
for	O
every	O
hidden	O
unit	O
.	O
thinking	O
through	O
this	O
example	O
for	O
iterations	O
2	O
.	O
.	O
.	O
,	O
the	O
values	O
of	O
the	O
hidden	O
units	O
will	O
always	O
be	O
exactly	O
the	O
same	O
,	O
which	O
means	O
that	O
the	O
weights	O
feeding	O
in	O
to	O
any	O
of	O
the	O
hidden	O
units	O
will	O
be	O
exactly	O
the	O
same	O
.	O
eventually	O
the	O
model	O
will	O
converge	O
,	O
but	O
it	O
will	O
converge	O
to	O
a	O
solution	O
that	O
does	O
not	O
take	O
advantage	O
of	O
having	O
access	O
to	O
the	O
hidden	O
units	O
.	O
this	O
shows	O
that	O
neural	B
networks	I
are	O
sensitive	O
to	O
their	O
initialization	O
.	O
in	O
particular	O
,	O
the	O
function	O
that	O
they	O
optimize	O
is	O
non-convex	B
,	O
meaning	O
that	O
it	O
might	O
have	O
plentiful	O
local	O
optima	O
.	O
(	O
one	O
of	O
which	O
is	O
the	O
trivial	O
local	O
optimum	O
described	O
in	O
the	O
preceding	O
paragraph	O
.	O
)	O
in	O
a	O
sense	O
,	O
neural	B
networks	I
must	O
have	O
local	O
optima	O
.	O
suppose	O
you	O
have	O
a	O
two	O
layer	O
network	O
with	O
two	O
hidden	B
units	I
that	O
’	O
s	O
been	O
optimized	O
.	O
you	O
have	O
weights	B
w1	O
from	O
inputs	O
to	O
the	O
ﬁrst	O
hidden	O
unit	O
,	O
weights	B
w2	O
from	O
in-	O
puts	O
to	O
the	O
second	O
hidden	O
unit	O
and	O
weights	B
(	O
v1	O
,	O
v2	O
)	O
from	O
the	O
hidden	O
units	O
to	O
the	O
output	O
.	O
if	O
i	O
give	O
you	O
back	O
another	O
network	O
with	O
w1	O
and	O
w2	O
swapped	O
,	O
and	O
v1	O
and	O
v2	O
swapped	O
,	O
the	O
network	O
computes	O
exactly	O
the	O
same	O
thing	O
,	O
but	O
with	O
a	O
markedly	O
different	O
weight	O
structure	O
.	O
this	O
phenomena	O
is	O
known	O
as	O
symmetric	B
modes	I
(	O
“	O
mode	O
”	O
referring	O
to	O
an	O
optima	O
)	O
meaning	O
that	O
there	O
are	O
symmetries	O
in	O
the	O
weight	O
space	O
.	O
it	O
would	O
be	O
one	O
thing	O
if	O
there	O
were	O
lots	O
of	O
modes	O
and	O
they	O
were	O
all	O
symmetric	O
:	O
then	O
ﬁnding	O
one	O
of	O
them	O
would	O
be	O
as	O
good	O
as	O
ﬁnding	O
any	O
other	O
.	O
unfortunately	O
there	O
are	O
additional	O
local	O
optima	O
that	O
are	O
not	O
global	O
optima	O
.	O
random	O
initialization	O
of	O
the	O
weights	O
of	O
a	O
network	O
is	O
a	O
way	O
to	O
address	O
both	O
of	O
these	O
problems	O
.	O
by	O
initializing	O
a	O
network	O
with	O
small	O
random	O
weights	O
(	O
say	O
,	O
uniform	O
between	O
−0.1	O
and	O
0.1	O
)	O
,	O
the	O
network	O
is	O
unlikely	O
to	O
fall	O
into	O
the	O
trivial	O
,	O
symmetric	O
local	O
optimum	O
.	O
moreover	O
,	O
by	O
training	O
a	O
collection	O
of	O
networks	O
,	O
each	O
with	O
a	O
different	O
random	O
figure	O
8.3	O
:	O
convergence	O
of	O
randomly	O
initialized	O
networks	O
initialization	O
,	O
you	O
can	O
often	O
obtain	O
better	O
solutions	O
that	O
with	O
just	O
one	O
initialization	O
.	O
in	O
other	O
words	O
,	O
you	O
can	O
train	O
ten	O
networks	O
with	O
different	O
random	O
seeds	O
,	O
and	O
then	O
pick	O
the	O
one	O
that	O
does	O
best	O
on	O
held-	O
out	O
data	O
.	O
figure	O
8.3	O
shows	O
prototypical	O
test-set	O
performance	O
for	O
ten	O
networks	O
with	O
different	O
random	O
initialization	O
,	O
plus	O
an	O
eleventh	O
plot	O
for	O
the	O
trivial	O
symmetric	O
network	O
initialized	O
with	O
zeros	O
.	O
one	O
of	O
the	O
typical	O
complaints	O
about	O
neural	B
networks	I
is	O
that	O
they	O
are	O
ﬁnicky	O
.	O
in	O
particular	O
,	O
they	O
have	O
a	O
rather	O
large	O
number	O
of	O
knobs	O
to	O
tune	O
:	O
1.	O
the	O
number	O
of	O
layers	O
2.	O
the	O
number	O
of	O
hidden	B
units	I
per	O
layer	O
3.	O
the	O
gradient	O
descent	O
learning	O
rate	O
η	O
4.	O
the	O
initialization	O
5.	O
the	O
stopping	O
iteration	B
or	O
weight	O
regularization	O
the	O
last	O
of	O
these	O
is	O
minor	O
(	O
early	B
stopping	I
is	O
an	O
easy	O
regularization	O
method	O
that	O
does	O
not	O
require	O
much	O
effort	O
to	O
tune	O
)	O
,	O
but	O
the	O
others	O
are	O
somewhat	O
signiﬁcant	O
.	O
even	O
for	O
two	O
layer	O
networks	O
,	O
having	O
to	O
choose	O
the	O
number	O
of	O
hidden	B
units	I
,	O
and	O
then	O
get	O
the	O
learning	O
rate	O
and	O
initialization	O
“	O
right	O
”	O
can	O
take	O
a	O
bit	O
of	O
work	O
.	O
clearly	O
it	O
can	O
be	O
automated	O
,	O
but	O
nonetheless	O
it	O
takes	O
time	O
.	O
another	O
difﬁculty	O
of	O
neural	B
networks	I
is	O
that	O
their	O
weights	B
can	O
be	O
difﬁcult	O
to	O
interpret	O
.	O
you	O
’	O
ve	O
seen	O
that	O
,	O
for	O
linear	O
networks	O
,	O
you	O
can	O
often	O
interpret	O
high	O
weights	B
as	O
indicative	O
of	O
positive	O
examples	O
and	O
low	O
weights	B
as	O
indicative	O
of	O
negative	O
examples	B
.	O
in	O
multilayer	O
networks	O
,	O
it	O
becomes	O
very	O
difﬁcult	O
to	O
try	O
to	O
understand	O
what	O
the	O
different	O
hidden	B
units	I
are	O
doing	O
.	O
8.4	O
beyond	O
two	O
layers	O
the	O
deﬁnition	O
of	O
neural	B
networks	I
and	O
the	O
back-propagation	O
algo-	O
rithm	O
can	O
be	O
generalized	O
beyond	O
two	O
layers	O
to	O
any	O
arbitrary	O
directed	O
acyclic	O
graph	B
.	O
in	O
practice	O
,	O
it	O
is	O
most	O
common	O
to	O
use	O
a	O
layered	O
net-	O
work	O
like	O
that	O
shown	O
in	O
figure	O
8.4	O
unless	O
one	O
has	O
a	O
very	O
strong	O
rea-	O
son	O
(	O
aka	O
inductive	B
bias	I
)	O
to	O
do	O
something	O
different	O
.	O
however	O
,	O
the	O
view	O
as	O
a	O
directed	O
graph	B
sheds	O
a	O
different	O
sort	O
of	O
insight	O
on	O
the	O
back-	O
propagation	O
algorithm	B
.	O
suppose	O
that	O
your	O
network	O
structure	O
is	O
stored	O
in	O
some	O
directed	O
acyclic	O
graph	B
,	O
like	O
that	O
in	O
figure	O
8.5.	O
we	O
index	O
nodes	O
in	O
this	O
graph	B
as	O
u	O
,	O
v.	O
the	O
activation	O
before	O
applying	O
non-linearity	O
at	O
a	O
node	O
is	O
au	O
and	O
after	O
non-linearity	O
is	O
hu	O
.	O
the	O
graph	O
has	O
a	O
single	O
sink	O
,	O
which	O
is	O
the	O
output	O
node	O
y	O
with	O
activation	O
ay	O
(	O
no	O
non-linearity	O
is	O
performed	O
neural	B
networks	I
123	O
figure	O
8.4	O
:	O
multi-layer	B
network	I
figure	O
8.5	O
:	O
dag	O
network	O
124	O
a	O
course	O
in	O
machine	O
learning	O
hu	O
←	O
corresponding	O
feature	O
of	O
x	O
algorithm	B
26	O
forwardpropagation	O
(	O
x	O
)	O
1	O
:	O
for	O
all	O
input	O
nodes	O
u	O
do	O
2	O
:	O
3	O
:	O
end	O
for	O
4	O
:	O
for	O
all	O
nodes	O
v	O
in	O
the	O
network	O
whose	O
parent	O
’	O
s	O
are	O
computed	O
do	O
5	O
:	O
av	O
←	O
∑u∈par	O
(	O
v	O
)	O
w	O
(	O
u	O
,	O
v	O
)	O
hu	O
hv	O
←	O
tanh	O
(	O
av	O
)	O
6	O
:	O
7	O
:	O
end	O
for	O
8	O
:	O
return	O
ay	O
algorithm	B
27	O
backpropagation	O
(	O
x	O
,	O
y	O
)	O
1	O
:	O
run	O
forwardpropagation	O
(	O
x	O
)	O
to	O
compute	O
activations	B
2	O
:	O
ey	O
←	O
y	O
−	O
ay	O
3	O
:	O
for	O
all	O
nodes	O
v	O
in	O
the	O
network	O
whose	O
error	O
ev	O
is	O
computed	O
do	O
4	O
:	O
for	O
all	O
u	O
∈	O
par	O
(	O
v	O
)	O
do	O
//	O
compute	O
overall	O
network	O
error	O
5	O
:	O
6	O
:	O
gu	O
,	O
v	O
←	O
−evhu	O
//	O
compute	O
gradient	B
of	O
this	O
edge	O
eu	O
←	O
eu	O
+	O
evwu	O
,	O
v	O
(	O
1	O
−	O
tanh2	O
(	O
au	O
)	O
)	O
//	O
compute	O
the	O
“	O
error	O
”	O
of	O
the	O
parent	O
node	O
end	O
for	O
7	O
:	O
8	O
:	O
end	O
for	O
9	O
:	O
return	O
all	O
gradients	O
ge	O
on	O
the	O
output	O
unit	O
)	O
.	O
the	O
graph	O
has	O
d-many	O
inputs	O
(	O
i.e.	O
,	O
nodes	O
with	O
no	O
parent	O
)	O
,	O
whose	O
activations	B
hu	O
are	O
given	O
by	O
an	O
input	O
example	O
.	O
an	O
edge	O
(	O
u	O
,	O
v	O
)	O
is	O
from	O
a	O
parent	O
to	O
a	O
child	O
(	O
i.e.	O
,	O
from	O
an	O
input	O
to	O
a	O
hidden	O
unit	O
,	O
or	O
from	O
a	O
hidden	O
unit	O
to	O
the	O
sink	O
)	O
.	O
each	O
edge	O
has	O
a	O
weight	O
wu	O
,	O
v	O
.	O
we	O
say	O
that	O
par	O
(	O
u	O
)	O
is	O
the	O
set	O
of	O
parents	O
of	O
u.	O
there	O
are	O
two	O
relevant	O
algorithms	O
:	O
forward-propagation	B
and	O
back-	O
propagation	O
.	O
forward-propagation	B
tells	O
you	O
how	O
to	O
compute	O
the	O
activation	O
of	O
the	O
sink	O
y	O
given	O
the	O
inputs	O
.	O
back-propagation	B
computes	O
derivatives	O
of	O
the	O
edge	O
weights	B
for	O
a	O
given	O
input	O
.	O
the	O
key	O
aspect	O
of	O
the	O
forward-propagation	O
algorithm	B
is	O
to	O
iter-	O
atively	O
compute	O
activations	B
,	O
going	O
deeper	O
and	O
deeper	O
in	O
the	O
dag	O
.	O
once	O
the	O
activations	O
of	O
all	O
the	O
parents	O
of	O
a	O
node	O
u	O
have	O
been	O
com-	O
puted	O
,	O
you	O
can	O
compute	O
the	O
activation	O
of	O
node	O
u.	O
this	O
is	O
spelled	O
out	O
in	O
algorithm	B
8.4.	O
this	O
is	O
also	O
explained	O
pictorially	O
in	O
figure	O
8.6.	O
back-propagation	B
(	O
see	O
algorithm	B
8.4	O
)	O
does	O
the	O
opposite	O
:	O
it	O
com-	O
putes	O
gradients	O
top-down	O
in	O
the	O
network	O
.	O
the	O
key	O
idea	O
is	O
to	O
compute	O
an	O
error	O
for	O
each	O
node	O
in	O
the	O
network	O
.	O
the	O
error	O
at	O
the	O
output	O
unit	O
is	O
the	O
“	O
true	O
error.	O
”	O
for	O
any	O
input	O
unit	O
,	O
the	O
error	O
is	O
the	O
amount	O
of	O
gradi-	O
ent	O
that	O
we	O
see	O
coming	O
from	O
our	O
children	O
(	O
i.e.	O
,	O
higher	O
in	O
the	O
network	O
)	O
.	O
these	O
errors	O
are	O
computed	O
backwards	O
in	O
the	O
network	O
(	O
hence	O
the	O
name	O
back-propagation	B
)	O
along	O
with	O
the	O
gradients	O
themselves	O
.	O
this	O
is	O
also	O
explained	O
pictorially	O
in	O
figure	O
8.7.	O
given	O
the	O
back-propagation	O
algorithm	B
,	O
you	O
can	O
directly	O
run	O
gradi-	O
ent	O
descent	O
,	O
using	O
it	O
as	O
a	O
subroutine	O
for	O
computing	O
the	O
gradients	O
.	O
figure	O
8.6	O
:	O
picture	O
of	O
forward	O
prop	O
figure	O
8.7	O
:	O
picture	O
of	O
back	O
prop	O
8.5	O
breadth	O
versus	O
depth	O
at	O
this	O
point	O
,	O
you	O
’	O
ve	O
seen	O
how	O
to	O
train	O
two-layer	O
networks	O
and	O
how	O
to	O
train	O
arbitrary	O
networks	O
.	O
you	O
’	O
ve	O
also	O
seen	O
a	O
theorem	O
that	O
says	O
that	O
two-layer	O
networks	O
are	O
universal	O
function	O
approximators	O
.	O
this	O
begs	O
the	O
question	O
:	O
if	O
two-layer	O
networks	O
are	O
so	O
great	O
,	O
why	O
do	O
we	O
care	O
about	O
deeper	O
networks	O
?	O
to	O
understand	O
the	O
answer	O
,	O
we	O
can	O
borrow	O
some	O
ideas	O
from	O
cs	O
theory	O
,	O
namely	O
the	O
idea	O
of	O
circuit	B
complexity	I
.	O
the	O
goal	O
is	O
to	O
show	O
that	O
there	O
are	O
functions	O
for	O
which	O
it	O
might	O
be	O
a	O
“	O
good	O
idea	O
”	O
to	O
use	O
a	O
deep	O
network	O
.	O
in	O
other	O
words	O
,	O
there	O
are	O
functions	O
that	O
will	O
require	O
a	O
huge	O
number	O
of	O
hidden	B
units	I
if	O
you	O
force	O
the	O
network	O
to	O
be	O
shallow	O
,	O
but	O
can	O
be	O
done	O
in	O
a	O
small	O
number	O
of	O
units	O
if	O
you	O
allow	O
it	O
to	O
be	O
deep	O
.	O
the	O
example	O
that	O
we	O
’	O
ll	O
use	O
is	O
the	O
parity	O
function	O
which	O
,	O
ironically	O
enough	O
,	O
is	O
just	O
a	O
generalization	O
of	O
the	O
xor	O
problem	O
.	O
the	O
function	O
is	O
deﬁned	O
over	O
binary	O
inputs	O
as	O
:	O
xd	O
mod	O
2	O
(	O
8.12	O
)	O
parity	O
(	O
x	O
)	O
=	O
∑	O
d	O
(	O
cid:40	O
)	O
=	O
1	O
if	O
the	O
number	O
of	O
1s	O
in	O
x	O
is	O
odd	O
0	O
if	O
the	O
number	O
of	O
1s	O
in	O
x	O
is	O
even	O
(	O
8.13	O
)	O
it	O
is	O
easy	O
to	O
deﬁne	O
a	O
circuit	O
of	O
depth	O
o	O
(	O
log2	O
d	O
)	O
with	O
o	O
(	O
d	O
)	O
-many	O
gates	O
for	O
computing	O
the	O
parity	O
function	O
.	O
each	O
gate	O
is	O
an	O
xor	O
,	O
ar-	O
ranged	O
in	O
a	O
complete	O
binary	O
tree	O
,	O
as	O
shown	O
in	O
figure	O
8.8	O
.	O
(	O
if	O
you	O
want	O
to	O
disallow	O
xor	O
as	O
a	O
gate	O
,	O
you	O
can	O
ﬁx	O
this	O
by	O
allowing	O
the	O
depth	O
to	O
be	O
doubled	O
and	O
replacing	O
each	O
xor	O
with	O
an	O
and	O
,	O
or	O
and	O
not	O
combination	O
,	O
like	O
you	O
did	O
at	O
the	O
beginning	O
of	O
this	O
chapter	O
.	O
)	O
this	O
shows	O
that	O
if	O
you	O
are	O
allowed	O
to	O
be	O
deep	O
,	O
you	O
can	O
construct	O
a	O
circuit	O
with	O
that	O
computes	O
parity	O
using	O
a	O
number	O
of	O
hidden	B
units	I
that	O
is	O
linear	O
in	O
the	O
dimensionality	O
.	O
so	O
can	O
you	O
do	O
the	O
same	O
with	O
shallow	O
circuits	O
?	O
the	O
answer	O
is	O
no	O
.	O
it	O
’	O
s	O
a	O
famous	O
result	O
of	O
circuit	B
complexity	I
that	O
parity	O
requires	O
exponentially	O
many	O
gates	O
to	O
compute	O
in	O
constant	O
depth	O
.	O
the	O
formal	O
theorem	O
is	O
below	O
:	O
theorem	O
10	O
(	O
parity	B
function	I
complexity	O
)	O
.	O
any	O
circuit	O
of	O
depth	O
k	O
<	O
log2	O
d	O
that	O
computes	O
the	O
parity	O
function	O
of	O
d	O
input	O
bits	O
must	O
contain	O
oed	O
gates	O
.	O
this	O
is	O
a	O
very	O
famous	O
result	O
because	O
it	O
shows	O
that	O
constant-depth	O
circuits	O
are	O
less	O
powerful	O
that	O
deep	O
circuits	O
.	O
although	O
a	O
neural	O
net-	O
work	O
isn	O
’	O
t	O
exactly	O
the	O
same	O
as	O
a	O
circuit	O
,	O
the	O
is	O
generally	O
believed	O
that	O
the	O
same	O
result	O
holds	O
for	O
neural	B
networks	I
.	O
at	O
the	O
very	O
least	O
,	O
this	O
gives	O
a	O
strong	O
indication	O
that	O
depth	O
might	O
be	O
an	O
important	O
considera-	O
tion	O
in	O
neural	B
networks	I
.	O
one	O
way	O
of	O
thinking	O
about	O
the	O
issue	O
of	O
breadth	O
versus	O
depth	O
has	O
to	O
do	O
with	O
the	O
number	O
of	O
parameters	O
that	O
need	O
to	O
be	O
estimated	O
.	O
by	O
neural	B
networks	I
125	O
figure	O
8.8	O
:	O
nnet	O
:	O
paritydeep	O
:	O
deep	O
function	O
for	O
computing	O
parity	O
?	O
what	O
is	O
it	O
about	O
neural	B
networks	I
that	O
makes	O
it	O
so	O
that	O
the	O
theorem	O
about	O
circuits	O
does	O
not	O
apply	O
di-	O
rectly	O
?	O
126	O
a	O
course	O
in	O
machine	O
learning	O
the	O
heuristic	O
that	O
you	O
need	O
roughly	O
one	O
or	O
two	O
examples	B
for	O
every	O
parameter	O
,	O
a	O
deep	O
model	B
could	O
potentially	O
require	O
exponentially	O
fewer	O
examples	B
to	O
train	O
than	O
a	O
shallow	O
model	O
!	O
this	O
now	O
ﬂips	O
the	O
question	O
:	O
if	O
deep	O
is	O
potentially	O
so	O
much	O
better	O
,	O
why	O
doesn	O
’	O
t	O
everyone	O
use	O
deep	O
networks	O
?	O
there	O
are	O
at	O
least	O
two	O
answers	O
.	O
first	O
,	O
it	O
makes	O
the	O
architecture	O
selection	O
problem	O
more	O
signiﬁcant	O
.	O
namely	O
,	O
when	O
you	O
use	O
a	O
two-layer	B
network	I
,	O
the	O
only	O
hyperparameter	B
to	O
choose	O
is	O
how	O
many	O
hidden	B
units	I
should	O
go	O
in	O
the	O
middle	O
layer	O
.	O
when	O
you	O
choose	O
a	O
deep	O
network	O
,	O
you	O
need	O
to	O
choose	O
how	O
many	O
layers	O
,	O
and	O
what	O
is	O
the	O
width	O
of	O
all	O
those	O
layers	O
.	O
this	O
can	O
be	O
somewhat	O
daunting	O
.	O
a	O
second	O
issue	O
has	O
to	O
do	O
with	O
training	O
deep	O
models	O
with	O
back-	O
propagation	O
.	O
in	O
general	O
,	O
as	O
back-propagation	B
works	O
its	O
way	O
down	O
through	O
the	O
model	O
,	O
the	O
sizes	O
of	O
the	O
gradients	O
shrink	O
.	O
you	O
can	O
work	O
this	O
out	O
mathematically	O
,	O
but	O
the	O
intuition	O
is	O
simpler	O
.	O
if	O
you	O
are	O
the	O
beginning	O
of	O
a	O
very	O
deep	O
network	O
,	O
changing	O
one	O
single	O
weight	O
is	O
unlikely	O
to	O
have	O
a	O
signiﬁcant	O
effect	O
on	O
the	O
output	O
,	O
since	O
it	O
has	O
to	O
go	O
through	O
so	O
many	O
other	O
units	O
before	O
getting	O
there	O
.	O
this	O
directly	O
implies	O
that	O
the	O
derivatives	O
are	O
small	O
.	O
this	O
,	O
in	O
turn	O
,	O
means	O
that	O
back-	O
propagation	O
essentially	O
never	O
moves	O
far	O
from	O
its	O
initialization	O
when	O
run	O
on	O
very	O
deep	O
networks	O
.	O
finding	O
good	O
ways	O
to	O
train	O
deep	O
networks	O
is	O
an	O
active	O
research	O
area	O
.	O
there	O
are	O
two	O
general	O
strategies	O
.	O
the	O
ﬁrst	O
is	O
to	O
attempt	O
to	O
ini-	O
tialize	O
the	O
weights	O
better	O
,	O
often	O
by	O
a	O
layer-wise	B
initialization	O
strategy	O
.	O
this	O
can	O
be	O
often	O
done	O
using	O
unlabeled	O
data	O
.	O
after	O
this	O
initializa-	O
tion	O
,	O
back-propagation	B
can	O
be	O
run	O
to	O
tweak	O
the	O
weights	O
for	O
whatever	O
classiﬁcation	O
problem	O
you	O
care	O
about	O
.	O
a	O
second	O
approach	O
is	O
to	O
use	O
a	O
more	O
complex	O
optimization	O
procedure	O
,	O
rather	O
than	O
gradient	B
descent	I
.	O
you	O
will	O
learn	O
about	O
some	O
such	O
procedures	O
later	O
in	O
this	O
book	O
.	O
8.6	O
basis	O
functions	O
at	O
this	O
point	O
,	O
we	O
’	O
ve	O
seen	O
that	O
:	O
(	O
a	O
)	O
neural	B
networks	I
can	O
mimic	O
linear	O
functions	O
and	O
(	O
b	O
)	O
they	O
can	O
learn	O
more	O
complex	O
functions	O
.	O
a	O
rea-	O
sonable	O
question	O
is	O
whether	O
they	O
can	O
mimic	O
a	O
knn	O
classiﬁer	O
,	O
and	O
whether	O
they	O
can	O
do	O
it	O
efﬁciently	O
(	O
i.e.	O
,	O
with	O
not-too-many	O
hidden	B
units	I
)	O
.	O
a	O
natural	O
way	O
to	O
train	O
a	O
neural	B
network	I
to	O
mimic	O
a	O
knn	O
classiﬁer	O
?	O
while	O
these	O
small	O
derivatives	O
might	O
make	O
training	O
difﬁcult	O
,	O
they	O
might	O
be	O
good	O
for	O
other	O
reasons	O
:	O
what	O
reasons	O
?	O
is	O
to	O
replace	O
the	O
sigmoid	O
link	B
function	I
with	O
a	O
radial	B
basis	I
function	I
(	O
rbf	O
)	O
.	O
in	O
a	O
sigmoid	B
network	I
(	O
i.e.	O
,	O
a	O
network	O
with	O
sigmoid	B
links	O
)	O
,	O
the	O
hidden	O
units	O
were	O
computed	O
as	O
hi	O
=	O
tanh	O
(	O
wi	O
,	O
x·	O
)	O
.	O
in	O
an	O
rbf	O
network	O
,	O
the	O
hidden	O
units	O
are	O
computed	O
as	O
:	O
(	O
8.14	O
)	O
(	O
cid:104	O
)	O
−γi	O
||wi	O
−	O
x||2	O
(	O
cid:105	O
)	O
hi	O
=	O
exp	O
in	O
other	O
words	O
,	O
the	O
hidden	O
units	O
behave	O
like	O
little	O
gaussian	O
“	O
bumps	O
”	O
centered	O
around	O
locations	O
speciﬁed	O
by	O
the	O
vectors	O
wi	O
.	O
a	O
one-dimensional	O
example	O
is	O
shown	O
in	O
figure	O
8.9.	O
the	O
parameter	O
γi	O
speciﬁes	O
the	O
width	O
of	O
the	O
gaussian	O
bump	O
.	O
if	O
γi	O
is	O
large	O
,	O
then	O
only	O
data	O
points	O
that	O
are	O
really	O
close	O
to	O
wi	O
have	O
non-zero	O
activations	B
.	O
to	O
distinguish	O
sigmoid	B
networks	O
from	O
rbf	O
networks	O
,	O
the	O
hidden	O
units	O
are	O
typically	O
drawn	O
with	O
sigmoids	O
or	O
with	O
gaussian	O
bumps	O
,	O
as	O
in	O
figure	O
8.10.	O
training	O
rbf	O
networks	O
involves	O
ﬁnding	O
good	O
values	O
for	O
the	O
gas-	O
sian	O
widths	O
,	O
γi	O
,	O
the	O
centers	O
of	O
the	O
gaussian	O
bumps	O
,	O
wi	O
and	O
the	O
con-	O
nections	O
between	O
the	O
gaussian	O
bumps	O
and	O
the	O
output	O
unit	O
,	O
v.	O
this	O
can	O
all	O
be	O
done	O
using	O
back-propagation	B
.	O
the	O
gradient	O
terms	O
for	O
v	O
re-	O
main	O
unchanged	O
from	O
before	O
,	O
the	O
the	O
derivates	O
for	O
the	O
other	O
variables	O
differ	O
(	O
see	O
exercise	O
?	O
?	O
)	O
.	O
one	O
of	O
the	O
big	O
questions	O
with	O
rbf	O
networks	O
is	O
:	O
where	O
should	O
the	O
gaussian	O
bumps	O
be	O
centered	O
?	O
one	O
can	O
,	O
of	O
course	O
,	O
apply	O
back-	O
propagation	O
to	O
attempt	O
to	O
ﬁnd	O
the	O
centers	O
.	O
another	O
option	O
is	O
to	O
spec-	O
ify	O
them	O
ahead	O
of	O
time	O
.	O
for	O
instance	O
,	O
one	O
potential	O
approach	O
is	O
to	O
have	O
one	O
rbf	O
unit	O
per	O
data	O
point	O
,	O
centered	O
on	O
that	O
data	O
point	O
.	O
if	O
you	O
carefully	O
choose	O
the	O
γs	O
and	O
vs	O
,	O
you	O
can	O
obtain	O
something	O
that	O
looks	O
nearly	O
identical	O
to	O
distance-weighted	O
knn	O
by	O
doing	O
so	O
.	O
this	O
has	O
the	O
added	O
advantage	O
that	O
you	O
can	O
go	O
futher	O
,	O
and	O
use	O
back-propagation	B
to	O
learn	O
good	O
gaussian	O
widths	O
(	O
γ	O
)	O
and	O
“	O
voting	B
”	O
factors	O
(	O
v	O
)	O
for	O
the	O
nearest	O
neighbor	O
algorithm	B
.	O
8.7	O
exercises	O
exercise	O
8.1.	O
todo	O
.	O
.	O
.	O
neural	B
networks	I
127	O
figure	O
8.9	O
:	O
nnet	O
:	O
rbfpicture	O
:	O
a	O
one-d	O
picture	O
of	O
rbf	O
bumps	O
figure	O
8.10	O
:	O
nnet	O
:	O
unitsymbols	O
:	O
picture	O
of	O
nnet	O
with	O
sigmoid/rbf	O
units	O
?	O
consider	O
an	O
rbf	O
network	O
with	O
one	O
hidden	O
unit	O
per	O
training	O
point	O
,	O
centered	O
at	O
that	O
point	O
.	O
what	O
bad	O
thing	O
might	O
happen	O
if	O
you	O
use	O
back-	O
propagation	O
to	O
estimate	O
the	O
γs	O
and	O
v	O
on	O
this	O
data	O
if	O
you	O
’	O
re	O
not	O
careful	O
?	O
how	O
could	O
you	O
be	O
careful	O
?	O
9	O
|	O
kernel	B
methods	O
learning	O
objectives	O
:	O
•	O
explain	O
how	O
kernels	B
generalize	O
both	O
feature	B
combinations	I
and	O
basis	O
functions	O
.	O
•	O
contrast	O
dot	O
products	O
with	O
kernel	B
products	O
.	O
•	O
implement	O
kernelized	O
perceptron	B
.	O
•	O
derive	O
a	O
kernelized	O
version	O
of	O
regularized	O
least	O
squares	O
regression	O
.	O
•	O
implement	O
a	O
kernelized	O
version	O
of	O
the	O
perceptron	O
.	O
•	O
derive	O
the	O
dual	O
formulation	O
of	O
the	O
support	O
vector	B
machine	O
.	O
dependencies	O
:	O
–	O
linear	O
models	O
are	O
great	O
because	O
they	O
are	O
easy	O
to	O
understand	O
and	O
easy	O
to	O
optimize	O
.	O
they	O
suffer	O
because	O
they	O
can	O
only	O
learn	O
very	O
simple	O
decision	O
boundaries	O
.	O
neural	B
networks	I
can	O
learn	O
more	O
com-	O
plex	O
decision	O
boundaries	O
,	O
but	O
lose	O
the	O
nice	O
convexity	O
properties	O
of	O
many	O
linear	O
models	O
.	O
one	O
way	O
of	O
getting	O
a	O
linear	O
model	O
to	O
behave	O
non-linearly	O
is	O
to	O
transform	O
the	O
input	O
.	O
for	O
instance	O
,	O
by	O
adding	O
feature	O
pairs	O
as	O
addi-	O
tional	O
inputs	O
.	O
learning	O
a	O
linear	O
model	O
on	O
such	O
a	O
representation	O
is	O
convex	B
,	O
but	O
is	O
computationally	O
prohibitive	O
in	O
all	O
but	O
very	O
low	O
dimen-	O
sional	O
spaces	O
.	O
you	O
might	O
ask	O
:	O
instead	O
of	O
explicitly	O
expanding	O
the	O
fea-	O
ture	O
space	O
,	O
is	O
it	O
possible	O
to	O
stay	O
with	O
our	O
original	O
data	O
representation	O
and	O
do	O
all	O
the	O
feature	O
blow	O
up	O
implicitly	O
?	O
surprisingly	O
,	O
the	O
answer	O
is	O
often	O
“	O
yes	O
”	O
and	O
the	O
family	O
of	O
techniques	O
that	O
makes	O
this	O
possible	O
are	O
known	O
as	O
kernel	B
approaches	O
.	O
9.1	O
from	O
feature	B
combinations	I
to	O
kernels	B
in	O
section	O
4.4	O
,	O
you	O
learned	O
one	O
method	O
for	O
increasing	O
the	O
expressive	O
power	O
of	O
linear	O
models	O
:	O
explode	O
the	O
feature	O
space	O
.	O
for	O
instance	O
,	O
a	O
“	O
quadratic	O
”	O
feature	O
explosion	O
might	O
map	O
a	O
feature	B
vector	I
x	O
=	O
(	O
cid:104	O
)	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
.	O
.	O
.	O
,	O
xd	O
(	O
cid:105	O
)	O
to	O
an	O
expanded	O
version	O
denoted	O
φ	O
(	O
x	O
)	O
:	O
φ	O
(	O
x	O
)	O
=	O
(	O
cid:104	O
)	O
1	O
,	O
2x1	O
,	O
2x2	O
,	O
2x3	O
,	O
.	O
.	O
.	O
,	O
2xd	O
,	O
x2	O
1	O
,	O
x1x2	O
,	O
x1x3	O
,	O
.	O
.	O
.	O
,	O
x1xd	O
,	O
x2x1	O
,	O
x2	O
2	O
,	O
x2x3	O
,	O
.	O
.	O
.	O
,	O
x2xd	O
,	O
x3x1	O
,	O
x3x2	O
,	O
x2	O
3	O
,	O
.	O
.	O
.	O
,	O
x2xd	O
,	O
.	O
.	O
.	O
,	O
d	O
(	O
cid:105	O
)	O
xdx1	O
,	O
xdx2	O
,	O
xdx3	O
,	O
.	O
.	O
.	O
,	O
x2	O
(	O
9.1	O
)	O
(	O
note	O
that	O
there	O
are	O
repetitions	O
here	O
,	O
but	O
hopefully	O
most	O
learning	O
algorithms	O
can	O
deal	O
well	O
with	O
redundant	B
features	I
;	O
in	O
particular	O
,	O
the	O
2x1	O
terms	O
are	O
due	O
to	O
collapsing	O
some	O
repetitions	O
.	O
)	O
kernel	B
methods	O
129	O
you	O
could	O
then	O
train	O
a	O
classiﬁer	O
on	O
this	O
expanded	O
feature	B
space	I
.	O
there	O
are	O
two	O
primary	O
concerns	O
in	O
doing	O
so	O
.	O
the	O
ﬁrst	O
is	O
computa-	O
tional	O
:	O
if	O
your	O
learning	O
algorithm	B
scales	O
linearly	O
in	O
the	O
number	O
of	O
fea-	O
tures	O
,	O
then	O
you	O
’	O
ve	O
just	O
squared	O
the	O
amount	O
of	O
computation	O
you	O
need	O
to	O
perform	O
;	O
you	O
’	O
ve	O
also	O
squared	O
the	O
amount	O
of	O
memory	O
you	O
’	O
ll	O
need	O
.	O
the	O
second	O
is	O
statistical	O
:	O
if	O
you	O
go	O
by	O
the	O
heuristic	O
that	O
you	O
should	O
have	O
about	O
two	O
examples	B
for	O
every	O
feature	O
,	O
then	O
you	O
will	O
now	O
need	O
quadratically	O
many	O
training	O
examples	O
in	O
order	O
to	O
avoid	O
overﬁtting	O
.	O
this	O
chapter	O
is	O
all	O
about	O
dealing	O
with	O
the	O
computational	O
issue	O
.	O
it	O
will	O
turn	O
out	O
in	O
chapter	O
?	O
?	O
that	O
you	O
can	O
also	O
deal	O
with	O
the	O
statistical	O
issue	O
:	O
for	O
now	O
,	O
you	O
can	O
just	O
hope	O
that	O
regularization	O
will	O
be	O
sufﬁcient	O
to	O
attenuate	O
overﬁtting	O
.	O
the	O
key	O
insight	O
in	O
kernel-based	O
learning	O
is	O
that	O
you	O
can	O
rewrite	O
many	O
linear	O
models	O
in	O
a	O
way	O
that	O
doesn	O
’	O
t	O
require	O
you	O
to	O
ever	O
ex-	O
plicitly	O
compute	O
φ	O
(	O
x	O
)	O
.	O
to	O
start	O
with	O
,	O
you	O
can	O
think	O
of	O
this	O
purely	O
as	O
a	O
computational	O
“	O
trick	O
”	O
that	O
enables	O
you	O
to	O
use	O
the	O
power	O
of	O
a	O
quadratic	O
feature	B
mapping	I
without	O
actually	O
having	O
to	O
compute	O
and	O
store	O
the	O
mapped	O
vectors	O
.	O
later	O
,	O
you	O
will	O
see	O
that	O
it	O
’	O
s	O
actually	O
quite	O
a	O
bit	O
deeper	O
.	O
most	O
algorithms	O
we	O
discuss	O
involve	O
a	O
product	O
of	O
the	O
form	O
w	O
·	O
φ	O
(	O
x	O
)	O
,	O
after	O
performing	O
the	O
feature	O
mapping	O
.	O
the	O
goal	O
is	O
to	O
rewrite	O
these	O
algorithms	O
so	O
that	O
they	O
only	O
ever	O
depend	O
on	O
dot	O
products	O
be-	O
tween	O
two	O
examples	B
,	O
say	O
x	O
and	O
z	O
;	O
namely	O
,	O
they	O
depend	O
on	O
φ	O
(	O
x	O
)	O
·	O
φ	O
(	O
z	O
)	O
.	O
to	O
understand	O
why	O
this	O
is	O
helpful	O
,	O
consider	O
the	O
quadratic	O
expansion	O
from	O
above	O
,	O
and	O
the	O
dot-product	O
between	O
two	O
vectors	O
.	O
you	O
get	O
:	O
φ	O
(	O
x	O
)	O
·	O
φ	O
(	O
z	O
)	O
=	O
1	O
+	O
x1z1	O
+	O
x2z2	O
+	O
·	O
·	O
·	O
+	O
xdzd	O
+	O
x2	O
1	O
+	O
·	O
·	O
·	O
+	O
x1xdz1zd+	O
1z2	O
·	O
·	O
·	O
+	O
xdx1zdz1	O
+	O
xdx2zdz2	O
+	O
·	O
·	O
·	O
+	O
x2	O
(	O
9.2	O
)	O
dz2	O
d	O
=	O
1	O
+	O
2	O
∑	O
xdzd	O
+	O
∑	O
∑	O
(	O
9.3	O
)	O
e	O
d	O
d	O
=	O
1	O
+	O
2x	O
·	O
z	O
+	O
(	O
x	O
·	O
z	O
)	O
2	O
=	O
(	O
1	O
+	O
x	O
·	O
z	O
)	O
2	O
(	O
9.4	O
)	O
(	O
9.5	O
)	O
xdxezdze	O
thus	O
,	O
you	O
can	O
compute	O
φ	O
(	O
x	O
)	O
·	O
φ	O
(	O
z	O
)	O
in	O
exactly	O
the	O
same	O
amount	O
of	O
time	O
as	O
you	O
can	O
compute	O
x	O
·	O
z	O
(	O
plus	O
the	O
time	O
it	O
takes	O
to	O
perform	O
an	O
addition	O
and	O
a	O
multiply	O
,	O
about	O
0.02	O
nanoseconds	O
on	O
a	O
circa	O
2011	O
processor	O
)	O
.	O
the	O
rest	O
of	O
the	O
practical	O
challenge	O
is	O
to	O
rewrite	O
your	O
algorithms	O
so	O
that	O
they	O
only	O
depend	O
on	O
dot	O
products	O
between	O
examples	B
and	O
not	O
on	O
any	O
explicit	O
weight	O
vectors	O
.	O
9.2	O
kernelized	O
perceptron	B
consider	O
the	O
original	O
perceptron	B
algorithm	O
from	O
chapter	O
3	O
,	O
re-	O
peated	O
in	O
algorithm	B
9.2	O
using	O
linear	O
algebra	O
notation	O
and	O
using	O
fea-	O
ture	O
expansion	O
notation	O
φ	O
(	O
x	O
)	O
.	O
in	O
this	O
algorithm	B
,	O
there	O
are	O
two	O
places	O
130	O
a	O
course	O
in	O
machine	O
learning	O
algorithm	B
28	O
perceptrontrain	O
(	O
d	O
,	O
maxiter	O
)	O
1	O
:	O
w	O
←	O
0	O
,	O
b	O
←	O
0	O
2	O
:	O
for	O
iter	O
=	O
1	O
.	O
.	O
.	O
maxiter	O
do	O
3	O
:	O
//	O
initialize	O
weights	B
and	O
bias	B
//	O
compute	O
activation	O
for	O
this	O
example	O
//	O
update	O
weights	B
//	O
update	O
bias	B
for	O
all	O
(	O
x	O
,	O
y	O
)	O
∈	O
d	O
do	O
a	O
←	O
w	O
·	O
φ	O
(	O
x	O
)	O
+	O
b	O
if	O
ya	O
≤	O
0	O
then	O
w	O
←	O
w	O
+	O
y	O
φ	O
(	O
x	O
)	O
b	O
←	O
b	O
+	O
y	O
4	O
:	O
5	O
:	O
6	O
:	O
7	O
:	O
8	O
:	O
end	O
if	O
end	O
for	O
9	O
:	O
10	O
:	O
end	O
for	O
11	O
:	O
return	O
w	O
,	O
b	O
math	O
review	O
|	O
spans	O
and	O
null	O
spaces	O
reminder	O
:	O
if	O
u	O
=	O
{	O
ui	O
}	O
i	O
is	O
a	O
set	O
of	O
vectors	O
in	O
rd	O
,	O
then	O
the	O
span	O
of	O
u	O
is	O
the	O
set	O
of	O
vectors	O
that	O
can	O
be	O
writ-	O
ten	O
as	O
linear	O
combinations	O
of	O
uis	O
;	O
namely	O
:	O
span	B
(	O
u	O
)	O
=	O
{	O
∑i	O
aiui	O
:	O
a1	O
∈	O
r	O
,	O
.	O
.	O
.	O
,	O
ai	O
∈	O
r	O
}	O
.	O
the	O
null	O
space	O
of	O
u	O
is	O
everything	O
that	O
’	O
s	O
left	O
:	O
rd\span	O
(	O
u	O
)	O
.	O
todo	O
pictures	O
figure	O
9.1	O
:	O
where	O
φ	O
(	O
x	O
)	O
is	O
used	O
explicitly	O
.	O
the	O
ﬁrst	O
is	O
in	O
computing	O
the	O
activation	O
(	O
line	O
4	O
)	O
and	O
the	O
second	O
is	O
in	O
updating	O
the	O
weights	O
(	O
line	O
6	O
)	O
.	O
the	O
goal	O
is	O
to	O
remove	O
the	O
explicit	O
dependence	O
of	O
this	O
algorithm	B
on	O
φ	O
and	O
on	O
the	O
weight	O
vector	B
.	O
to	O
do	O
so	O
,	O
you	O
can	O
observe	O
that	O
at	O
any	O
point	O
in	O
the	O
algorithm	O
,	O
the	O
weight	O
vector	B
w	O
can	O
be	O
written	O
as	O
a	O
linear	O
combination	O
of	O
expanded	O
training	B
data	I
.	O
in	O
particular	O
,	O
at	O
any	O
point	O
,	O
w	O
=	O
∑n	O
αnφ	O
(	O
xn	O
)	O
for	O
some	O
parameters	O
α.	O
initially	O
,	O
w	O
=	O
0	O
so	O
choosing	O
α	O
=	O
0	O
yields	O
this	O
.	O
if	O
the	O
ﬁrst	O
update	O
occurs	O
on	O
the	O
nth	O
training	O
example	O
,	O
then	O
the	O
resolution	O
weight	O
vector	B
is	O
simply	O
ynφ	O
(	O
xn	O
)	O
,	O
which	O
is	O
equivalent	O
to	O
setting	O
αn	O
=	O
yn	O
.	O
if	O
the	O
second	O
update	O
occurs	O
on	O
the	O
mth	O
training	O
example	O
,	O
then	O
all	O
you	O
need	O
to	O
do	O
is	O
update	O
αm	O
←	O
αm	O
+	O
ym	O
.	O
this	O
is	O
true	O
,	O
even	O
if	O
you	O
make	O
multiple	O
passes	O
over	O
the	O
data	O
.	O
this	O
observation	O
leads	O
to	O
the	O
following	O
representer	B
theorem	I
,	O
which	O
states	O
that	O
the	O
weight	O
vector	B
of	O
the	O
perceptron	O
lies	O
in	O
the	O
span	O
of	O
the	O
training	O
data	O
.	O
theorem	O
11	O
(	O
perceptron	B
representer	O
theorem	O
)	O
.	O
during	O
a	O
run	O
of	O
the	O
perceptron	O
algorithm	B
,	O
the	O
weight	O
vector	B
w	O
is	O
always	O
in	O
the	O
span	O
of	O
the	O
(	O
assumed	O
non-empty	O
)	O
training	B
data	I
,	O
φ	O
(	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
φ	O
(	O
xn	O
)	O
.	O
proof	O
of	O
theorem	O
11.	O
by	O
induction	B
.	O
base	O
case	O
:	O
the	O
span	O
of	O
any	O
non-	O
empty	O
set	O
contains	O
the	O
zero	O
vector	B
,	O
which	O
is	O
the	O
initial	O
weight	O
vec-	O
tor	O
.	O
inductive	O
case	O
:	O
suppose	O
that	O
the	O
theorem	O
is	O
true	O
before	O
the	O
kth	O
update	O
,	O
and	O
suppose	O
that	O
the	O
kth	O
update	O
happens	O
on	O
example	O
n.	O
by	O
the	O
inductive	O
hypothesis	B
,	O
you	O
can	O
write	O
w	O
=	O
∑i	O
αiφ	O
(	O
xi	O
)	O
before	O
kernel	B
methods	O
131	O
algorithm	B
29	O
kernelizedperceptrontrain	O
(	O
d	O
,	O
maxiter	O
)	O
1	O
:	O
α	O
←	O
0	O
,	O
b	O
←	O
0	O
2	O
:	O
for	O
iter	O
=	O
1	O
.	O
.	O
.	O
maxiter	O
do	O
for	O
all	O
(	O
xn	O
,	O
yn	O
)	O
∈	O
d	O
do	O
3	O
:	O
//	O
initialize	O
coefﬁcients	O
and	O
bias	B
a	O
←	O
∑m	O
αmφ	O
(	O
xm	O
)	O
·	O
φ	O
(	O
xn	O
)	O
+	O
b	O
if	O
yna	O
≤	O
0	O
then	O
αn	O
←	O
αn	O
+	O
yn	O
b	O
←	O
b	O
+	O
y	O
//	O
compute	O
activation	O
for	O
this	O
example	O
//	O
update	O
coefﬁcients	O
//	O
update	O
bias	B
4	O
:	O
5	O
:	O
6	O
:	O
7	O
:	O
8	O
:	O
end	O
if	O
end	O
for	O
9	O
:	O
10	O
:	O
end	O
for	O
11	O
:	O
return	O
α	O
,	O
b	O
the	O
update	O
.	O
the	O
new	O
weight	O
vector	B
is	O
[	O
∑i	O
αiφ	O
(	O
xi	O
)	O
]	O
+	O
ynφ	O
(	O
xn	O
)	O
=	O
∑i	O
(	O
αi	O
+	O
yn	O
[	O
i	O
=	O
n	O
]	O
)	O
φ	O
(	O
xi	O
)	O
,	O
which	O
is	O
still	O
in	O
the	O
span	O
of	O
the	O
training	O
data	O
.	O
now	O
that	O
you	O
know	O
that	O
you	O
can	O
always	O
write	O
w	O
=	O
∑n	O
αnφ	O
(	O
xn	O
)	O
for	O
some	O
αis	O
,	O
you	O
can	O
additionall	O
compute	O
the	O
activations	O
(	O
line	O
4	O
)	O
as	O
:	O
(	O
cid:32	O
)	O
(	O
cid:33	O
)	O
w	O
·	O
φ	O
(	O
x	O
)	O
+	O
b	O
=	O
∑	O
n	O
αnφ	O
(	O
xn	O
)	O
·	O
φ	O
(	O
x	O
)	O
+	O
b	O
(	O
cid:105	O
)	O
+	O
b	O
(	O
cid:104	O
)	O
=	O
∑	O
n	O
αn	O
φ	O
(	O
xn	O
)	O
·	O
φ	O
(	O
x	O
)	O
deﬁnition	O
of	O
w	O
(	O
9.6	O
)	O
dot	O
products	O
are	O
linear	O
(	O
9.7	O
)	O
this	O
now	O
depends	O
only	O
on	O
dot-products	O
between	O
data	O
points	O
,	O
and	O
never	O
explicitly	O
requires	O
a	O
weight	O
vector	B
.	O
you	O
can	O
now	O
rewrite	O
the	O
entire	O
perceptron	B
algorithm	O
so	O
that	O
it	O
never	O
refers	O
explicitly	O
to	O
the	O
weights	O
and	O
only	O
ever	O
depends	O
on	O
pairwise	O
dot	O
products	O
between	O
examples	B
.	O
this	O
is	O
shown	O
in	O
algorithm	B
9.2.	O
the	O
advantage	O
to	O
this	O
“	O
kernelized	O
”	O
algorithm	B
is	O
that	O
you	O
can	O
per-	O
form	O
feature	O
expansions	O
like	O
the	O
quadratic	O
feature	O
expansion	O
from	O
the	O
introduction	O
for	O
“	O
free.	O
”	O
for	O
example	O
,	O
for	O
exactly	O
the	O
same	O
cost	O
as	O
the	O
quadratic	O
features	B
,	O
you	O
can	O
use	O
a	O
cubic	B
feature	I
map	I
,	O
computed	O
¨φ	O
(	O
x	O
)	O
φ	O
(	O
z	O
)	O
=	O
(	O
1	O
+	O
x	O
·	O
z	O
)	O
3	O
,	O
which	O
corresponds	O
to	O
three-way	O
inter-	O
as	O
actions	O
between	O
variables	O
.	O
(	O
and	O
,	O
in	O
general	O
,	O
you	O
can	O
do	O
so	O
for	O
any	O
polynomial	O
degree	O
p	O
at	O
the	O
same	O
computational	O
complexity	B
.	O
)	O
9.3	O
kernelized	O
k-means	O
for	O
a	O
complete	O
change	O
of	O
pace	O
,	O
consider	O
the	O
k-means	O
algorithm	B
from	O
section	O
?	O
?	O
.	O
this	O
algorithm	B
is	O
for	O
clustering	B
where	O
there	O
is	O
no	O
notion	O
of	O
“	O
training	O
labels.	O
”	O
instead	O
,	O
you	O
want	O
to	O
partition	O
the	O
data	O
into	O
coher-	O
ent	O
clusters	O
.	O
for	O
data	O
in	O
rd	O
,	O
it	O
involves	O
randomly	O
initializing	O
k-many	O
132	O
a	O
course	O
in	O
machine	O
learning	O
cluster	O
means	O
µ	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
µ	O
(	O
k	O
)	O
.	O
the	O
algorithm	O
then	O
alternates	O
between	O
the	O
following	O
two	O
steps	O
until	O
convergence	O
,	O
with	O
x	O
replaced	O
by	O
φ	O
(	O
x	O
)	O
since	O
that	O
is	O
the	O
eventual	O
goal	O
:	O
1.	O
for	O
each	O
example	O
n	O
,	O
set	O
cluster	O
label	B
zn	O
=	O
arg	O
mink	O
2.	O
for	O
each	O
cluster	O
k	O
,	O
update	O
µ	O
(	O
k	O
)	O
=	O
1	O
nk	O
∑n	O
:	O
zn=k	O
φ	O
(	O
xn	O
)	O
,	O
where	O
nk	O
is	O
the	O
number	O
of	O
n	O
with	O
zn	O
=	O
k.	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
φ	O
(	O
xn	O
)	O
−	O
µ	O
(	O
k	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
2.	O
the	O
question	O
is	O
whether	O
you	O
can	O
perform	O
these	O
steps	O
without	O
ex-	O
plicitly	O
computing	O
φ	O
(	O
xn	O
)	O
.	O
the	O
representer	O
theorem	O
is	O
more	O
straight-	O
forward	O
here	O
than	O
in	O
the	O
perceptron	O
.	O
the	O
mean	O
of	O
a	O
set	O
of	O
data	O
is	O
,	O
almost	O
by	O
deﬁnition	O
,	O
in	O
the	O
span	O
of	O
that	O
data	O
(	O
choose	O
the	O
ais	O
all	O
to	O
be	O
equal	O
to	O
1/n	O
)	O
.	O
thus	O
,	O
so	O
long	O
as	O
you	O
initialize	O
the	O
means	O
in	O
the	O
span	O
of	O
the	O
data	O
,	O
you	O
are	O
guaranteed	O
always	O
to	O
have	O
the	O
means	O
in	O
the	O
span	O
of	O
the	O
data	O
.	O
given	O
this	O
,	O
you	O
know	O
that	O
you	O
can	O
write	O
each	O
mean	O
as	O
an	O
(	O
k	O
)	O
expansion	O
of	O
the	O
data	O
;	O
say	O
that	O
µ	O
(	O
k	O
)	O
=	O
∑n	O
α	O
n	O
φ	O
(	O
xn	O
)	O
for	O
some	O
parame-	O
ters	O
α	O
n	O
(	O
there	O
are	O
n×k-many	O
such	O
parameters	O
)	O
.	O
given	O
this	O
expansion	O
,	O
in	O
order	O
to	O
execute	O
step	O
(	O
1	O
)	O
,	O
you	O
need	O
to	O
(	O
k	O
)	O
compute	O
norms	O
.	O
this	O
can	O
be	O
done	O
as	O
follows	O
:	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
φ	O
(	O
xn	O
)	O
−	O
µ	O
(	O
k	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
2	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
φ	O
(	O
xn	O
)	O
−	O
∑	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
||φ	O
(	O
xn	O
)	O
||2	O
+	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
∑	O
m	O
m	O
α	O
zn	O
=	O
arg	O
min	O
k	O
=	O
arg	O
min	O
k	O
=	O
arg	O
min	O
k	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
2	O
(	O
k	O
)	O
m	O
φ	O
(	O
xm	O
)	O
(	O
cid:34	O
)	O
+	O
φ	O
(	O
xn	O
)	O
·	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
2	O
(	O
9.9	O
)	O
(	O
cid:35	O
)	O
(	O
k	O
)	O
m	O
φ	O
(	O
xm	O
)	O
α	O
∑	O
m	O
(	O
k	O
)	O
m	O
φ	O
(	O
xm	O
)	O
α	O
(	O
9.8	O
)	O
deﬁnition	O
of	O
zn	O
deﬁnition	O
of	O
µ	O
(	O
k	O
)	O
expand	O
quadratic	O
term	O
linearity	O
and	O
constant	O
k	O
α	O
(	O
k	O
)	O
(	O
k	O
)	O
m	O
α	O
∑	O
m	O
∑	O
m	O
(	O
cid:48	O
)	O
=	O
arg	O
min	O
m	O
(	O
cid:48	O
)	O
φ	O
(	O
xm	O
)	O
·	O
φ	O
(	O
xm	O
(	O
cid:48	O
)	O
)	O
+	O
∑	O
(	O
9.10	O
)	O
m	O
φ	O
(	O
xm	O
)	O
·	O
φ	O
(	O
xn	O
)	O
+	O
const	O
(	O
9.11	O
)	O
this	O
computation	O
can	O
replace	O
the	O
assignments	O
in	O
step	O
(	O
1	O
)	O
of	O
k-means	O
.	O
the	O
mean	O
updates	O
are	O
more	O
direct	O
in	O
step	O
(	O
2	O
)	O
:	O
(	O
k	O
)	O
m	O
α	O
(	O
cid:40	O
)	O
1	O
nk	O
0	O
if	O
zn	O
=	O
k	O
otherwise	O
(	O
9.12	O
)	O
µ	O
(	O
k	O
)	O
=	O
1	O
nk	O
∑	O
n	O
:	O
zn=k	O
φ	O
(	O
xn	O
)	O
⇐⇒	O
α	O
(	O
k	O
)	O
n	O
=	O
9.4	O
what	O
makes	O
a	O
kernel	B
a	O
kernel	B
is	O
just	O
a	O
form	O
of	O
generalized	O
dot	B
product	I
.	O
you	O
can	O
also	O
think	O
of	O
it	O
as	O
simply	O
shorthand	O
for	O
φ	O
(	O
x	O
)	O
·	O
φ	O
(	O
z	O
)	O
,	O
which	O
is	O
commonly	O
written	O
kφ	O
(	O
x	O
,	O
z	O
)	O
.	O
or	O
,	O
when	O
φ	O
is	O
clear	O
from	O
context	O
,	O
simply	O
k	O
(	O
x	O
,	O
z	O
)	O
.	O
kernel	B
methods	O
133	O
this	O
is	O
often	O
refered	O
to	O
as	O
the	O
kernel	O
product	O
between	O
x	O
and	O
z	O
(	O
under	O
the	O
mapping	O
φ	O
)	O
.	O
in	O
this	O
view	O
,	O
what	O
you	O
’	O
ve	O
seen	O
in	O
the	O
preceding	O
two	O
sections	O
is	O
that	O
you	O
can	O
rewrite	O
both	O
the	O
perceptron	O
algorithm	B
and	O
the	O
k-means	O
algorithm	B
so	O
that	O
they	O
only	O
ever	O
depend	O
on	O
kernel	B
products	O
between	O
data	O
points	O
,	O
and	O
never	O
on	O
the	O
actual	O
datapoints	O
themselves	O
.	O
this	O
is	O
a	O
very	O
pow-	O
erful	O
notion	O
,	O
as	O
it	O
has	O
enabled	O
the	O
development	O
of	O
a	O
large	O
number	O
of	O
non-linear	B
algorithms	O
essentially	O
“	O
for	O
free	O
”	O
(	O
by	O
applying	O
the	O
so-called	O
kernel	B
trick	I
,	O
that	O
you	O
’	O
ve	O
just	O
seen	O
twice	O
)	O
.	O
this	O
raises	O
an	O
interesting	O
question	O
.	O
if	O
you	O
have	O
rewritten	O
these	O
algorithms	O
so	O
that	O
they	O
only	O
depend	O
on	O
the	O
data	O
through	O
a	O
function	O
k	O
:	O
x×x	O
→	O
r	O
,	O
can	O
you	O
stick	O
any	O
function	O
k	O
in	O
these	O
algorithms	O
,	O
or	O
are	O
there	O
some	O
k	O
that	O
are	O
“	O
forbidden	O
?	O
”	O
in	O
one	O
sense	O
,	O
you	O
“	O
could	O
”	O
use	O
any	O
k	O
,	O
but	O
the	O
real	O
question	O
is	O
:	O
for	O
what	O
types	O
of	O
functions	O
k	O
do	O
these	O
algorithms	O
retain	O
the	O
properties	O
that	O
we	O
expect	O
them	O
to	O
have	O
(	O
like	O
convergence	O
,	O
optimality	O
,	O
etc.	O
)	O
?	O
one	O
way	O
to	O
answer	O
this	O
question	O
is	O
to	O
say	O
that	O
k	O
(	O
·	O
,	O
·	O
)	O
is	O
a	O
valid	O
kernel	B
if	O
it	O
corresponds	O
to	O
the	O
inner	O
product	O
between	O
two	O
vectors	O
.	O
that	O
is	O
,	O
k	O
is	O
valid	O
if	O
there	O
exists	O
a	O
function	O
φ	O
such	O
that	O
k	O
(	O
x	O
,	O
z	O
)	O
=	O
φ	O
(	O
x	O
)	O
·	O
φ	O
(	O
z	O
)	O
.	O
this	O
is	O
a	O
direct	O
deﬁnition	O
and	O
it	O
should	O
be	O
clear	O
that	O
if	O
k	O
satisﬁes	O
this	O
,	O
then	O
the	O
algorithms	O
go	O
through	O
as	O
expected	O
(	O
because	O
this	O
is	O
how	O
we	O
derived	O
them	O
)	O
.	O
you	O
’	O
ve	O
already	O
seen	O
the	O
general	O
class	O
of	O
polynomial	B
kernels	I
,	O
which	O
have	O
the	O
form	O
:	O
k	O
(	O
poly	O
)	O
d	O
(	O
x	O
,	O
z	O
)	O
=	O
1	O
+	O
x	O
·	O
z	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
d	O
(	O
9.13	O
)	O
where	O
d	O
is	O
a	O
hyperparameter	B
of	O
the	O
kernel	O
.	O
these	O
kernels	B
correspond	O
to	O
polynomial	O
feature	O
expansions	O
.	O
there	O
is	O
an	O
alternative	O
characterization	O
of	O
a	O
valid	O
kernel	B
function	O
that	O
is	O
more	O
mathematical	O
.	O
it	O
states	O
that	O
k	O
:	O
x×x	O
→	O
r	O
is	O
a	O
kernel	B
if	O
k	O
is	O
positive	O
semi-definite	O
(	O
or	O
,	O
in	O
shorthand	O
,	O
psd	B
)	O
.	O
this	O
property	O
is	O
also	O
sometimes	O
called	O
mercer	O
’	O
s	O
condition	O
.	O
in	O
this	O
context	O
,	O
this	O
means	O
the	O
for	O
all	O
functions	O
f	O
that	O
are	O
square	O
integrable	O
(	O
i.e.	O
,	O
(	O
cid:82	O
)	O
f	O
(	O
x	O
)	O
2dx	O
<	O
∞	O
)	O
,	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
other	O
than	O
the	O
zero	O
function	O
,	O
the	O
following	O
property	O
holds	O
:	O
f	O
(	O
x	O
)	O
k	O
(	O
x	O
,	O
z	O
)	O
f	O
(	O
z	O
)	O
dxdz	O
>	O
0	O
(	O
9.14	O
)	O
this	O
likely	O
seems	O
like	O
it	O
came	O
out	O
of	O
nowhere	O
.	O
unfortunately	O
,	O
the	O
connection	O
is	O
well	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
,	O
but	O
is	O
covered	O
well	O
is	O
external	O
sources	O
.	O
for	O
now	O
,	O
simply	O
take	O
it	O
as	O
a	O
given	O
that	O
this	O
is	O
an	O
equivalent	O
requirement	O
.	O
(	O
for	O
those	O
so	O
inclined	O
,	O
the	O
appendix	O
of	O
this	O
book	O
gives	O
a	O
proof	O
,	O
but	O
it	O
requires	O
a	O
bit	O
of	O
knowledge	O
of	O
function	O
spaces	O
to	O
understand	O
.	O
)	O
the	O
question	O
is	O
:	O
why	O
is	O
this	O
alternative	O
characterization	O
useful	O
?	O
it	O
is	O
useful	O
because	O
it	O
gives	O
you	O
an	O
alternative	O
way	O
to	O
construct	O
kernel	B
134	O
a	O
course	O
in	O
machine	O
learning	O
functions	O
.	O
for	O
instance	O
,	O
using	O
it	O
you	O
can	O
easily	O
prove	O
the	O
following	O
,	O
which	O
would	O
be	O
difﬁcult	O
from	O
the	O
deﬁnition	O
of	O
kernels	B
as	O
inner	O
prod-	O
ucts	O
after	O
feature	O
mappings	O
.	O
theorem	O
12	O
(	O
kernel	B
addition	O
)	O
.	O
if	O
k1	O
and	O
k2	O
are	O
kernels	B
,	O
the	O
k	O
deﬁned	O
by	O
k	O
(	O
x	O
,	O
z	O
)	O
=	O
k1	O
(	O
x	O
,	O
z	O
)	O
+	O
k2	O
(	O
x	O
,	O
z	O
)	O
is	O
also	O
a	O
kernel	B
.	O
proof	O
of	O
theorem	O
12.	O
you	O
need	O
to	O
verify	O
the	O
positive	O
semi-deﬁnite	O
property	O
on	O
k.	O
you	O
can	O
do	O
this	O
as	O
follows	O
:	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
f	O
(	O
x	O
)	O
k	O
(	O
x	O
,	O
z	O
)	O
f	O
(	O
z	O
)	O
dxdz	O
=	O
=	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
(	O
cid:90	O
)	O
+	O
f	O
(	O
x	O
)	O
[	O
k1	O
(	O
x	O
,	O
z	O
)	O
+	O
k2	O
(	O
x	O
,	O
z	O
)	O
]	O
f	O
(	O
z	O
)	O
dxdz	O
(	O
9.15	O
)	O
deﬁnition	O
of	O
k	O
f	O
(	O
x	O
)	O
k1	O
(	O
x	O
,	O
z	O
)	O
f	O
(	O
z	O
)	O
dxdz	O
f	O
(	O
x	O
)	O
k2	O
(	O
x	O
,	O
z	O
)	O
f	O
(	O
z	O
)	O
dxdz	O
distributive	O
rule	O
(	O
9.16	O
)	O
(	O
9.17	O
)	O
k1	O
and	O
k2	O
are	O
psd	B
>	O
0	O
+	O
0	O
more	O
generally	O
,	O
any	O
positive	O
linear	O
combination	O
of	O
kernels	B
is	O
still	O
a	O
kernel	B
.	O
speciﬁcally	O
,	O
if	O
k1	O
,	O
.	O
.	O
.	O
,	O
km	O
are	O
all	O
kernels	O
,	O
and	O
α1	O
,	O
.	O
.	O
.	O
,	O
αm	O
≥	O
0	O
,	O
then	O
k	O
(	O
x	O
,	O
z	O
)	O
=	O
∑m	O
αmkm	O
(	O
x	O
,	O
z	O
)	O
is	O
also	O
a	O
kernel	B
.	O
you	O
can	O
also	O
use	O
this	O
property	O
to	O
show	O
that	O
the	O
following	O
gaus-	O
sian	O
kernel	B
(	O
also	O
called	O
the	O
rbf	O
kernel	B
)	O
is	O
also	O
psd	B
:	O
(	O
cid:104	O
)	O
−γ	O
||x	O
−	O
z||2	O
(	O
cid:105	O
)	O
k	O
(	O
rbf	O
)	O
γ	O
(	O
x	O
,	O
z	O
)	O
=	O
exp	O
(	O
9.18	O
)	O
here	O
γ	O
is	O
a	O
hyperparameter	B
that	O
controls	O
the	O
width	O
of	O
this	O
gaussian-	O
like	O
bumps	O
.	O
to	O
gain	O
an	O
intuition	O
for	O
what	O
the	O
rbf	O
kernel	B
is	O
doing	O
,	O
consider	O
what	O
prediction	O
looks	O
like	O
in	O
the	O
perceptron	O
:	O
f	O
(	O
x	O
)	O
=	O
∑	O
n	O
=	O
∑	O
n	O
αnk	O
(	O
xn	O
,	O
x	O
)	O
+	O
b	O
(	O
cid:104	O
)	O
−γ	O
||xn	O
−	O
z||2	O
(	O
cid:105	O
)	O
αn	O
exp	O
(	O
9.19	O
)	O
(	O
9.20	O
)	O
in	O
this	O
computation	O
,	O
each	O
training	O
example	O
is	O
getting	O
to	O
“	O
vote	B
”	O
on	O
the	O
label	O
of	O
the	O
test	O
point	O
x.	O
the	O
amount	O
of	O
“	O
vote	B
”	O
that	O
the	O
nth	O
training	O
example	O
gets	O
is	O
proportional	O
to	O
the	O
negative	O
exponential	O
of	O
the	O
dis-	O
tance	O
between	O
the	O
test	O
point	O
and	O
itself	O
.	O
this	O
is	O
very	O
much	O
like	O
an	O
rbf	O
neural	B
network	I
,	O
in	O
which	O
there	O
is	O
a	O
gaussian	O
“	O
bump	O
”	O
at	O
each	O
training	O
example	O
,	O
with	O
variance	B
1/	O
(	O
2γ	O
)	O
,	O
and	O
where	O
the	O
αns	O
act	O
as	O
the	O
weights	O
connecting	O
these	O
rbf	O
bumps	O
to	O
the	O
output	O
.	O
showing	O
that	O
this	O
kernel	B
is	O
positive	O
deﬁnite	O
is	O
a	O
bit	O
of	O
an	O
exercise	O
in	O
analysis	O
(	O
particularly	O
,	O
integration	O
by	O
parts	O
)	O
,	O
but	O
otherwise	O
not	O
difﬁcult	O
.	O
again	O
,	O
the	O
proof	O
is	O
provided	O
in	O
the	O
appendix	O
.	O
kernel	B
methods	O
135	O
so	O
far	O
,	O
you	O
have	O
seen	O
two	O
bsaic	O
classes	O
of	O
kernels	B
:	O
polynomial	B
kernels	I
(	O
k	O
(	O
x	O
,	O
vz	O
)	O
=	O
(	O
1	O
+	O
x	O
·	O
z	O
)	O
d	O
)	O
,	O
which	O
includes	O
the	O
linear	O
kernel	B
(	O
k	O
(	O
x	O
,	O
z	O
)	O
=	O
x	O
·	O
z	O
)	O
and	O
rbf	O
kernels	B
(	O
k	O
(	O
x	O
,	O
z	O
)	O
=	O
exp	O
[	O
−γ	O
||x	O
−	O
z||2	O
]	O
)	O
.	O
the	O
former	O
have	O
a	O
direct	O
connection	O
to	O
feature	O
expansion	O
;	O
the	O
latter	O
to	O
rbf	O
networks	O
.	O
you	O
also	O
know	O
how	O
to	O
combine	O
kernels	B
to	O
get	O
new	O
kernels	B
by	O
addition	O
.	O
in	O
fact	O
,	O
you	O
can	O
do	O
more	O
than	O
that	O
:	O
the	O
product	O
of	O
two	O
kernels	B
is	O
also	O
a	O
kernel	B
.	O
as	O
far	O
as	O
a	O
“	O
library	O
of	O
kernels	B
”	O
goes	O
,	O
there	O
are	O
many	O
.	O
polynomial	O
and	O
rbf	O
are	O
by	O
far	O
the	O
most	O
popular	O
.	O
a	O
commonly	O
used	O
,	O
but	O
techni-	O
cally	O
invalid	O
kernel	B
,	O
is	O
the	O
hyperbolic-tangent	O
kernel	B
,	O
which	O
mimics	O
the	O
behavior	O
of	O
a	O
two-layer	O
neural	O
network	O
.	O
it	O
is	O
deﬁned	O
as	O
:	O
k	O
(	O
tanh	O
)	O
=	O
tanh	O
(	O
1	O
+	O
x	O
·	O
z	O
)	O
warning	O
:	O
not	O
psd	B
(	O
9.21	O
)	O
a	O
ﬁnal	O
example	O
,	O
which	O
is	O
not	O
very	O
common	O
,	O
but	O
is	O
nonetheless	O
interesting	O
,	O
is	O
the	O
all-subsets	O
kernel	B
.	O
suppose	O
that	O
your	O
d	O
features	B
are	O
all	O
binary	O
:	O
all	O
take	O
values	O
0	O
or	O
1.	O
let	O
a	O
⊆	O
{	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
d	O
}	O
be	O
a	O
subset	O
of	O
features	B
,	O
and	O
let	O
fa	O
(	O
x	O
)	O
=	O
(	O
cid:86	O
)	O
d∈a	O
xd	O
be	O
the	O
conjunction	O
of	O
all	O
the	O
features	B
in	O
a.	O
let	O
φ	O
(	O
x	O
)	O
be	O
a	O
feature	B
vector	I
over	O
all	O
such	O
as	O
,	O
so	O
that	O
there	O
are	O
2d	O
features	B
in	O
the	O
vector	O
φ.	O
you	O
can	O
compute	O
the	O
kernel	O
associated	O
with	O
this	O
feature	B
mapping	I
as	O
:	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
k	O
(	O
subs	O
)	O
(	O
x	O
,	O
z	O
)	O
=	O
∏	O
d	O
1	O
+	O
xdzd	O
(	O
9.22	O
)	O
verifying	O
the	O
relationship	O
between	O
this	O
kernel	B
and	O
the	O
all-subsets	O
feature	B
mapping	I
is	O
left	O
as	O
an	O
exercise	O
(	O
but	O
closely	O
resembles	O
the	O
ex-	O
pansion	O
for	O
the	O
quadratic	O
kernel	B
)	O
.	O
9.5	O
support	O
vector	O
machines	O
kernelization	O
predated	O
support	O
vector	O
machines	O
,	O
but	O
svms	O
are	O
def-	O
initely	O
the	O
model	O
that	O
popularized	O
the	O
idea	O
.	O
recall	B
the	O
deﬁnition	O
of	O
the	O
soft-margin	O
svm	O
from	O
chapter	O
6.7	O
and	O
in	O
particular	O
the	O
opti-	O
mization	O
problem	O
(	O
6.36	O
)	O
,	O
which	O
attempts	O
to	O
balance	O
a	O
large	O
margin	B
(	O
small	O
||w||2	O
)	O
with	O
a	O
small	O
loss	O
(	O
small	O
ξns	O
,	O
where	O
ξn	O
is	O
the	O
slack	O
on	O
the	O
nth	O
training	O
example	O
)	O
.	O
this	O
problem	O
is	O
repeated	O
below	O
:	O
ξn	O
(	O
9.23	O
)	O
min	O
w	O
,	O
b	O
,	O
ξ	O
1	O
2	O
||w||2	O
+	O
c	O
∑	O
n	O
subj	O
.	O
to	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
≥	O
1	O
−	O
ξn	O
ξn	O
≥	O
0	O
(	O
∀n	O
)	O
(	O
∀n	O
)	O
previously	O
,	O
you	O
optimized	O
this	O
by	O
explicitly	O
computing	O
the	O
slack	O
variables	O
ξn	O
,	O
given	O
a	O
solution	O
to	O
the	O
decision	O
boundary	O
,	O
w	O
and	O
b.	O
however	O
,	O
you	O
are	O
now	O
an	O
expert	O
with	O
using	O
lagrange	O
multipliers	O
136	O
a	O
course	O
in	O
machine	O
learning	O
to	O
optimize	O
constrained	O
problems	O
!	O
the	O
overall	O
goal	O
is	O
going	O
to	O
be	O
to	O
rewrite	O
the	O
svm	O
optimization	B
problem	I
in	O
a	O
way	O
that	O
it	O
no	O
longer	O
ex-	O
plicitly	O
depends	O
on	O
the	O
weights	O
w	O
and	O
only	O
depends	O
on	O
the	O
examples	O
xn	O
through	O
kernel	B
products	O
.	O
there	O
are	O
2n	O
constraints	O
in	O
this	O
optimization	O
,	O
one	O
for	O
each	O
slack	B
constraint	O
and	O
one	O
for	O
the	O
requirement	O
that	O
the	O
slacks	O
are	O
non-	O
negative	O
.	O
unlike	O
the	O
last	O
time	O
,	O
these	O
constraints	O
are	O
now	O
inequalities	O
,	O
which	O
require	O
a	O
slightly	O
different	O
solution	O
.	O
first	O
,	O
you	O
rewrite	O
all	O
the	O
inequalities	O
so	O
that	O
they	O
read	O
as	O
something	O
≥	O
0	O
and	O
then	O
add	O
cor-	O
responding	O
lagrange	O
multipliers	O
.	O
the	O
main	O
difference	O
is	O
that	O
the	O
lagrange	O
multipliers	O
are	O
now	O
constrained	O
to	O
be	O
non-negative	O
,	O
and	O
their	O
sign	B
in	O
the	O
augmented	O
objective	B
function	I
matters	O
.	O
the	O
second	O
set	O
of	O
constraints	O
is	O
already	O
in	O
the	O
proper	O
form	O
;	O
the	O
ﬁrst	O
set	O
can	O
be	O
rewritten	O
as	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
−	O
1	O
+	O
ξn	O
≥	O
0.	O
you	O
’	O
re	O
now	O
ready	O
to	O
construct	O
the	O
lagrangian	O
,	O
using	O
multipliers	O
αn	O
for	O
the	O
ﬁrst	O
set	O
of	O
constraints	O
and	O
βn	O
for	O
the	O
second	O
set	O
.	O
l	O
(	O
w	O
,	O
b	O
,	O
ξ	O
,	O
α	O
,	O
β	O
)	O
=	O
1	O
2	O
ξn	O
−	O
∑	O
βnξn	O
||w||2	O
+	O
c	O
∑	O
n	O
−	O
∑	O
n	O
n	O
αn	O
[	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
−	O
1	O
+	O
ξn	O
]	O
the	O
new	O
optimization	B
problem	I
is	O
:	O
min	O
w	O
,	O
b	O
,	O
ξ	O
max	O
α≥0	O
max	O
β≥0	O
l	O
(	O
w	O
,	O
b	O
,	O
ξ	O
,	O
α	O
,	O
β	O
)	O
(	O
9.24	O
)	O
(	O
9.25	O
)	O
(	O
9.26	O
)	O
the	O
intuition	O
is	O
exactly	O
the	O
same	O
as	O
before	O
.	O
if	O
you	O
are	O
able	O
to	O
ﬁnd	O
a	O
solution	O
that	O
satisﬁes	O
the	O
constraints	O
(	O
e.g.	O
,	O
the	O
purple	O
term	O
is	O
prop-	O
erly	O
non-negative	O
)	O
,	O
then	O
the	O
βns	O
can	O
not	O
do	O
anything	O
to	O
“	O
hurt	O
”	O
the	O
solution	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
the	O
purple	O
term	O
is	O
negative	O
,	O
then	O
the	O
corresponding	O
βn	O
can	O
go	O
to	O
+∞	O
,	O
breaking	O
the	O
solution	O
.	O
you	O
can	O
solve	O
this	O
problem	O
by	O
taking	O
gradients	O
.	O
this	O
is	O
a	O
bit	O
te-	O
dious	O
,	O
but	O
and	O
important	O
step	O
to	O
realize	O
how	O
everything	O
ﬁts	O
together	O
.	O
since	O
your	O
goal	O
is	O
to	O
remove	O
the	O
dependence	O
on	O
w	O
,	O
the	O
ﬁrst	O
step	O
is	O
to	O
take	O
a	O
gradient	B
with	O
respect	O
to	O
w	O
,	O
set	O
it	O
equal	O
to	O
zero	O
,	O
and	O
solve	O
for	O
w	O
in	O
terms	O
of	O
the	O
other	O
variables	O
.	O
∇wl	O
=	O
w	O
−	O
∑	O
n	O
αnynxn	O
=	O
0	O
⇐⇒	O
w	O
=	O
∑	O
n	O
αnynxn	O
(	O
9.27	O
)	O
at	O
this	O
point	O
,	O
you	O
should	O
immediately	O
recognize	O
a	O
similarity	O
to	O
the	O
kernelized	O
perceptron	B
:	O
the	O
optimal	O
weight	O
vector	B
takes	O
exactly	O
the	O
same	O
form	O
in	O
both	O
algorithms	O
.	O
you	O
can	O
now	O
take	O
this	O
new	O
expression	O
for	O
w	O
and	O
plug	O
it	O
back	O
in	O
to	O
the	O
expression	O
for	O
l	O
,	O
thus	O
removing	O
w	O
from	O
consideration	O
.	O
to	O
avoid	O
subscript	O
overloading	O
,	O
you	O
should	O
replace	O
the	O
n	O
in	O
the	O
expression	O
for	O
kernel	B
methods	O
137	O
(	O
9.28	O
)	O
(	O
cid:35	O
)	O
(	O
9.29	O
)	O
(	O
9.31	O
)	O
(	O
9.32	O
)	O
(	O
9.33	O
)	O
w	O
with	O
,	O
say	O
,	O
m.	O
this	O
yields	O
:	O
l	O
(	O
b	O
,	O
ξ	O
,	O
α	O
,	O
β	O
)	O
=	O
1	O
2	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
∑	O
m	O
−	O
∑	O
n	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
2	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:32	O
)	O
(	O
cid:34	O
)	O
∑	O
m	O
αmymxm	O
(	O
cid:34	O
)	O
αn	O
yn	O
+	O
c	O
∑	O
n	O
ξn	O
−	O
∑	O
(	O
cid:35	O
)	O
n	O
βnξn	O
(	O
cid:33	O
)	O
αmymxm	O
·	O
xn	O
+	O
b	O
−	O
1	O
+	O
ξn	O
at	O
this	O
point	O
,	O
it	O
’	O
s	O
convenient	O
to	O
rewrite	O
these	O
terms	O
;	O
be	O
sure	O
you	O
un-	O
derstand	O
where	O
the	O
following	O
comes	O
from	O
:	O
l	O
(	O
b	O
,	O
ξ	O
,	O
α	O
,	O
β	O
)	O
=	O
1	O
2	O
∑	O
∑	O
n	O
m	O
−	O
∑	O
n	O
αnαmynymxn	O
·	O
xm	O
+	O
∑	O
αnαmynymxn	O
·	O
xm	O
−	O
∑	O
∑	O
m	O
n	O
n	O
(	O
c	O
−	O
βn	O
)	O
ξn	O
(	O
9.30	O
)	O
αn	O
(	O
ynb	O
−	O
1	O
+	O
ξn	O
)	O
=	O
−	O
1	O
αnαmynymxn	O
·	O
xm	O
+	O
∑	O
∑	O
∑	O
2	O
n	O
m	O
−b	O
∑	O
αnyn	O
−	O
∑	O
n	O
αn	O
(	O
ξn	O
−	O
1	O
)	O
n	O
n	O
(	O
c	O
−	O
βn	O
)	O
ξn	O
things	O
are	O
starting	O
to	O
look	O
good	O
:	O
you	O
’	O
ve	O
successfully	O
removed	O
the	O
de-	O
pendence	O
on	O
w	O
,	O
and	O
everything	O
is	O
now	O
written	O
in	O
terms	O
of	O
dot	O
prod-	O
ucts	O
between	O
input	O
vectors	O
!	O
this	O
might	O
still	O
be	O
a	O
difﬁcult	O
problem	O
to	O
solve	O
,	O
so	O
you	O
need	O
to	O
continue	O
and	O
attempt	O
to	O
remove	O
the	O
remaining	O
variables	O
b	O
and	O
ξ.	O
the	O
derivative	O
with	O
respect	O
to	O
b	O
is	O
:	O
∂l	O
∂b	O
=	O
−	O
∑	O
n	O
αnyn	O
=	O
0	O
(	O
9.34	O
)	O
this	O
doesn	O
’	O
t	O
allow	O
you	O
to	O
substitute	O
b	O
with	O
something	O
(	O
as	O
you	O
did	O
with	O
w	O
)	O
,	O
but	O
it	O
does	O
mean	O
that	O
the	O
fourth	O
term	O
(	O
b	O
∑n	O
αnyn	O
)	O
goes	O
to	O
zero	O
at	O
the	O
optimum	O
.	O
the	O
last	O
of	O
the	O
original	O
variables	O
is	O
ξn	O
;	O
the	O
derivatives	O
in	O
this	O
case	O
look	O
like	O
:	O
∂l	O
∂ξn	O
=	O
c	O
−	O
βn	O
−	O
αn	O
⇐⇒	O
c	O
−	O
βn	O
=	O
αn	O
(	O
9.35	O
)	O
again	O
,	O
this	O
doesn	O
’	O
t	O
allow	O
you	O
to	O
substitute	O
,	O
but	O
it	O
does	O
mean	O
that	O
you	O
can	O
rewrite	O
the	O
second	O
term	O
,	O
which	O
as	O
∑n	O
(	O
c	O
−	O
βn	O
)	O
ξn	O
as	O
∑n	O
αnξn	O
.	O
this	O
then	O
cancels	O
with	O
(	O
most	O
of	O
)	O
the	O
ﬁnal	O
term	O
.	O
however	O
,	O
you	O
need	O
to	O
be	O
careful	O
to	O
remember	O
something	O
.	O
when	O
we	O
optimize	O
,	O
both	O
αn	O
and	O
βn	O
are	O
constrained	O
to	O
be	O
non-negative	O
.	O
what	O
this	O
means	O
is	O
that	O
since	O
we	O
are	O
dropping	O
β	O
from	O
the	O
optimization	O
,	O
we	O
need	O
to	O
ensure	O
that	O
αn	O
≤	O
c	O
,	O
otherwise	O
the	O
corresponding	O
β	O
will	O
need	O
to	O
be	O
negative	O
,	O
which	O
is	O
not	O
138	O
a	O
course	O
in	O
machine	O
learning	O
allowed	O
.	O
you	O
ﬁnally	O
wind	O
up	O
with	O
the	O
following	O
,	O
where	O
xn	O
·	O
xm	O
has	O
been	O
replaced	O
by	O
k	O
(	O
xn	O
,	O
xm	O
)	O
:	O
l	O
(	O
α	O
)	O
=	O
∑	O
n	O
αn	O
−	O
1	O
2	O
∑	O
n	O
∑	O
m	O
αnαmynymk	O
(	O
xn	O
,	O
xm	O
)	O
(	O
9.36	O
)	O
if	O
you	O
are	O
comfortable	O
with	O
matrix	O
notation	O
,	O
this	O
has	O
a	O
very	O
compact	O
form	O
.	O
let	O
1	O
denote	O
the	O
n-dimensional	O
vector	B
of	O
all	O
1s	O
,	O
let	O
y	O
denote	O
the	O
vector	O
of	O
labels	O
and	O
let	O
g	O
be	O
the	O
n×n	O
matrix	O
,	O
where	O
gn	O
,	O
m	O
=	O
ynymk	O
(	O
xn	O
,	O
xm	O
)	O
,	O
then	O
this	O
has	O
the	O
following	O
form	O
:	O
(	O
cid:62	O
)	O
1	O
−	O
1	O
2	O
α	O
(	O
cid:62	O
)	O
gα	O
l	O
(	O
α	O
)	O
=	O
α	O
(	O
9.37	O
)	O
the	O
resulting	O
optimization	B
problem	I
is	O
to	O
maximize	O
l	O
(	O
α	O
)	O
as	O
a	O
function	O
of	O
α	O
,	O
subject	O
to	O
the	O
constraint	O
that	O
the	O
αns	O
are	O
all	O
non-negative	O
and	O
less	O
than	O
c	O
(	O
because	O
of	O
the	O
constraint	O
added	O
when	O
removing	O
the	O
β	O
variables	O
)	O
.	O
thus	O
,	O
your	O
problem	O
is	O
:	O
min	O
1	O
−	O
l	O
(	O
α	O
)	O
=	O
2	O
subj	O
.	O
to	O
0	O
≤	O
αn	O
≤	O
c	O
α	O
∑	O
n	O
∑	O
m	O
αnαmynymk	O
(	O
xn	O
,	O
xm	O
)	O
−	O
∑	O
n	O
αn	O
(	O
9.38	O
)	O
(	O
∀n	O
)	O
one	O
way	O
to	O
solve	O
this	O
problem	O
is	O
gradient	B
descent	I
on	O
α.	O
the	O
only	O
complication	O
is	O
making	O
sure	O
that	O
the	O
αs	O
satisfy	O
the	O
constraints	O
.	O
in	O
this	O
case	O
,	O
you	O
can	O
use	O
a	O
projected	B
gradient	I
algorithm	O
:	O
after	O
each	O
gradient	B
update	O
,	O
you	O
adjust	O
your	O
parameters	O
to	O
satisfy	O
the	O
constraints	O
by	O
projecting	O
them	O
into	O
the	O
feasible	O
region	O
.	O
in	O
this	O
case	O
,	O
the	O
projection	O
is	O
trivial	O
:	O
if	O
,	O
after	O
a	O
gradient	B
step	O
,	O
any	O
αn	O
<	O
0	O
,	O
simply	O
set	O
it	O
to	O
0	O
;	O
if	O
any	O
αn	O
>	O
c	O
,	O
set	O
it	O
to	O
c.	O
9.6	O
understanding	O
support	O
vector	O
machines	O
the	O
prior	O
discussion	O
involved	O
quite	O
a	O
bit	O
of	O
math	O
to	O
derive	O
a	O
repre-	O
sentation	O
of	O
the	O
support	O
vector	B
machine	O
in	O
terms	O
of	O
the	O
lagrange	O
variables	O
.	O
this	O
mapping	O
is	O
actually	O
sufﬁciently	O
standard	O
that	O
every-	O
thing	O
in	O
it	O
has	O
a	O
name	O
.	O
the	O
original	O
problem	O
variables	O
(	O
w	O
,	O
b	O
,	O
ξ	O
)	O
are	O
called	O
the	O
primal	O
variables	O
;	O
the	O
lagrange	O
variables	O
are	O
called	O
the	O
dual	O
variables	O
.	O
the	O
optimization	O
problem	O
that	O
results	O
after	O
removing	O
all	O
of	O
the	O
primal	O
variables	O
is	O
called	O
the	O
dual	O
problem	O
.	O
a	O
succinct	O
way	O
of	O
saying	O
what	O
you	O
’	O
ve	O
done	O
is	O
:	O
you	O
found	O
that	O
after	O
converting	O
the	O
svm	O
into	O
its	O
dual	O
,	O
it	O
is	O
possible	O
to	O
kernelize	O
.	O
to	O
understand	O
svms	O
,	O
a	O
ﬁrst	O
step	O
is	O
to	O
peek	O
into	O
the	O
dual	O
formula-	O
tion	O
,	O
eq	O
(	O
9.38	O
)	O
.	O
the	O
objective	O
has	O
two	O
terms	O
:	O
the	O
ﬁrst	O
depends	O
on	O
the	O
data	O
,	O
and	O
the	O
second	O
depends	O
only	O
on	O
the	O
dual	O
variables	O
.	O
the	O
ﬁrst	O
thing	O
to	O
notice	O
is	O
that	O
,	O
because	O
of	O
the	O
second	O
term	O
,	O
the	O
αs	O
“	O
want	O
”	O
to	O
kernel	B
methods	O
139	O
get	O
as	O
large	O
as	O
possible	O
.	O
the	O
constraint	O
ensures	O
that	O
they	O
can	O
not	O
ex-	O
ceed	O
c	O
,	O
which	O
means	O
that	O
the	O
general	O
tendency	O
is	O
for	O
the	O
αs	O
to	O
grow	O
as	O
close	O
to	O
c	O
as	O
possible	O
.	O
to	O
further	O
understand	O
the	O
dual	O
optimization	B
problem	I
,	O
it	O
is	O
useful	O
to	O
think	O
of	O
the	O
kernel	O
as	O
being	O
a	O
measure	O
of	O
similarity	O
between	O
two	O
data	O
points	O
.	O
this	O
analogy	O
is	O
most	O
clear	O
in	O
the	O
case	O
of	O
rbf	O
kernels	B
,	O
but	O
even	O
in	O
the	O
case	O
of	O
linear	O
kernels	O
,	O
if	O
your	O
examples	B
all	O
have	O
unit	O
norm	O
,	O
then	O
their	O
dot	B
product	I
is	O
still	O
a	O
measure	O
of	O
similarity	O
.	O
since	O
you	O
can	O
write	O
the	O
prediction	O
function	O
as	O
f	O
(	O
ˆx	O
)	O
=	O
sign	B
(	O
∑n	O
αnynk	O
(	O
xn	O
,	O
ˆx	O
)	O
)	O
,	O
it	O
is	O
natural	O
to	O
think	O
of	O
αn	O
as	O
the	O
“	O
importance	O
”	O
of	O
training	O
example	O
n	O
,	O
where	O
αn	O
=	O
0	O
means	O
that	O
it	O
is	O
not	O
used	O
at	O
all	O
at	O
test	O
time	O
.	O
consider	O
two	O
data	O
points	O
that	O
have	O
the	O
same	O
label	B
;	O
namely	O
,	O
yn	O
=	O
ym	O
.	O
this	O
means	O
that	O
ynym	O
=	O
+1	O
and	O
the	O
objective	O
function	O
has	O
a	O
term	O
that	O
looks	O
like	O
αnαmk	O
(	O
xn	O
,	O
xm	O
)	O
.	O
since	O
the	O
goal	O
is	O
to	O
make	O
this	O
term	O
small	O
,	O
then	O
one	O
of	O
two	O
things	O
has	O
to	O
happen	O
:	O
either	O
k	O
has	O
to	O
be	O
small	O
,	O
or	O
αnαm	O
has	O
to	O
be	O
small	O
.	O
if	O
k	O
is	O
already	O
small	O
,	O
then	O
this	O
doesn	O
’	O
t	O
affect	O
the	O
setting	O
of	O
the	O
corresponding	O
αs	O
.	O
but	O
if	O
k	O
is	O
large	O
,	O
then	O
this	O
strongly	O
encourages	O
at	O
least	O
one	O
of	O
αn	O
or	O
αm	O
to	O
go	O
to	O
zero	O
.	O
so	O
if	O
you	O
have	O
two	O
data	O
points	O
that	O
are	O
very	O
similar	O
and	O
have	O
the	O
same	O
label	B
,	O
at	O
least	O
one	O
of	O
the	O
corresponding	O
αs	O
will	O
be	O
small	O
.	O
this	O
makes	O
intuitive	O
sense	O
:	O
if	O
you	O
have	O
two	O
data	O
points	O
that	O
are	O
basically	O
the	O
same	O
(	O
both	O
in	O
the	O
x	O
and	O
y	O
sense	O
)	O
then	O
you	O
only	O
need	O
to	O
“	O
keep	O
”	O
one	O
of	O
them	O
around	O
.	O
suppose	O
that	O
you	O
have	O
two	O
data	O
points	O
with	O
different	O
labels	O
:	O
ynym	O
=	O
−1	O
.	O
again	O
,	O
if	O
k	O
(	O
xn	O
,	O
xm	O
)	O
is	O
small	O
,	O
nothing	O
happens	O
.	O
but	O
if	O
it	O
is	O
large	O
,	O
then	O
the	O
corresponding	O
αs	O
are	O
encouraged	O
to	O
be	O
as	O
large	O
as	O
possible	O
.	O
in	O
other	O
words	O
,	O
if	O
you	O
have	O
two	O
similar	O
examples	B
with	O
dif-	O
ferent	O
labels	O
,	O
you	O
are	O
strongly	O
encouraged	O
to	O
keep	O
the	O
corresponding	O
αs	O
as	O
large	O
as	O
c.	O
an	O
alternative	O
way	O
of	O
understanding	O
the	O
svm	O
dual	B
problem	I
is	O
geometrically	O
.	O
remember	O
that	O
the	O
whole	O
point	O
of	O
introducing	O
the	O
variable	O
αn	O
was	O
to	O
ensure	O
that	O
the	O
nth	O
training	O
example	O
was	O
correctly	O
classiﬁed	O
,	O
modulo	O
slack	B
.	O
more	O
formally	O
,	O
the	O
goal	O
of	O
αn	O
is	O
to	O
ensure	O
that	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
−	O
1	O
+	O
ξn	O
≥	O
0.	O
suppose	O
that	O
this	O
constraint	O
it	O
not	O
satisﬁed	O
.	O
there	O
is	O
an	O
important	O
result	O
in	O
optimization	O
theory	O
,	O
called	O
the	O
karush-kuhn-tucker	O
conditions	O
(	O
or	O
kkt	O
conditions	O
,	O
for	O
short	O
)	O
that	O
states	O
that	O
at	O
the	O
optimum	O
,	O
the	O
product	O
of	O
the	O
lagrange	O
multiplier	O
for	O
a	O
constraint	O
,	O
and	O
the	O
value	O
of	O
that	O
constraint	O
,	O
will	O
equal	O
zero	O
.	O
in	O
this	O
case	O
,	O
this	O
says	O
that	O
at	O
the	O
optimum	O
,	O
you	O
have	O
:	O
(	O
cid:104	O
)	O
(	O
cid:105	O
)	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
−	O
1	O
+	O
ξn	O
αn	O
=	O
0	O
(	O
9.39	O
)	O
in	O
order	O
for	O
this	O
to	O
be	O
true	O
,	O
it	O
means	O
that	O
(	O
at	O
least	O
)	O
one	O
of	O
the	O
follow-	O
ing	O
must	O
be	O
true	O
:	O
αn	O
=	O
0	O
or	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
−	O
1	O
+	O
ξn	O
=	O
0	O
(	O
9.40	O
)	O
140	O
a	O
course	O
in	O
machine	O
learning	O
a	O
reasonable	O
question	O
to	O
ask	O
is	O
:	O
under	O
what	O
circumstances	O
will	O
αn	O
be	O
non-zero	O
?	O
from	O
the	O
kkt	O
conditions	O
,	O
you	O
can	O
discern	O
that	O
αn	O
can	O
be	O
non-zero	O
only	O
when	O
the	O
constraint	O
holds	O
exactly	O
;	O
namely	O
,	O
that	O
yn	O
(	O
w	O
·	O
xn	O
+	O
b	O
)	O
−	O
1	O
+	O
ξn	O
=	O
0.	O
when	O
does	O
that	O
constraint	O
hold	O
ex-	O
actly	O
?	O
it	O
holds	O
exactly	O
only	O
for	O
those	O
points	O
precisely	O
on	O
the	O
margin	O
of	O
the	O
hyperplane	O
.	O
in	O
other	O
words	O
,	O
the	O
only	O
training	O
examples	O
for	O
which	O
αn	O
(	O
cid:54	O
)	O
=	O
0	O
are	O
those	O
that	O
lie	O
precisely	O
1	O
unit	O
away	O
from	O
the	O
maximum	O
margin	B
decision	O
boundary	O
!	O
(	O
or	O
those	O
that	O
are	O
“	O
moved	O
”	O
there	O
by	O
the	O
corre-	O
sponding	O
slack	B
.	O
)	O
these	O
points	O
are	O
called	O
the	O
support	O
vectors	O
because	O
they	O
“	O
support	O
”	O
the	O
decision	O
boundary	O
.	O
in	O
general	O
,	O
the	O
number	O
of	O
sup-	O
port	O
vectors	O
is	O
far	O
smaller	O
than	O
the	O
number	O
of	O
training	O
examples	O
,	O
and	O
therefore	O
you	O
naturally	O
end	O
up	O
with	O
a	O
solution	O
that	O
only	O
uses	O
a	O
subset	O
of	O
the	O
training	O
data	O
.	O
from	O
the	O
ﬁrst	O
discussion	O
,	O
you	O
know	O
that	O
the	O
points	O
that	O
wind	O
up	O
being	O
support	B
vectors	I
are	O
exactly	O
those	O
that	O
are	O
“	O
confusable	O
”	O
in	O
the	O
sense	O
that	O
you	O
have	O
to	O
examples	B
that	O
are	O
nearby	O
,	O
but	O
have	O
different	O
la-	O
bels	O
.	O
this	O
is	O
a	O
completely	O
in	O
line	O
with	O
the	O
previous	O
discussion	O
.	O
if	O
you	O
have	O
a	O
decision	B
boundary	I
,	O
it	O
will	O
pass	O
between	O
these	O
“	O
confusable	O
”	O
points	O
,	O
and	O
therefore	O
they	O
will	O
end	O
up	O
being	O
part	O
of	O
the	O
set	O
of	O
support	B
vectors	I
.	O
9.7	O
exercises	O
exercise	O
9.1.	O
todo	O
.	O
.	O
.	O
10	O
|	O
learning	O
theory	O
learning	O
objectives	O
:	O
•	O
explain	O
why	O
inductive	B
bias	I
is	O
necessary	O
.	O
•	O
deﬁne	O
the	O
pac	O
model	B
and	O
explain	O
why	O
both	O
the	O
“	O
p	O
”	O
and	O
“	O
a	O
”	O
are	O
necessary	O
.	O
•	O
explain	O
the	O
relationship	O
between	O
complexity	B
measures	O
and	O
regulariz-	O
ers	O
.	O
•	O
identify	O
the	O
role	O
of	O
complexity	B
in	O
generalization	O
.	O
•	O
formalize	O
the	O
relationship	O
between	O
margins	O
and	O
complexity	B
.	O
dependencies	O
:	O
for	O
nothing	O
ought	O
to	O
be	O
posited	O
without	O
a	O
reason	O
given	O
,	O
unless	O
it	O
is	O
self-evident	O
or	O
known	O
by	O
experience	O
or	O
proved	O
by	O
the	O
authority	O
of	O
sacred	O
scripture	O
.	O
–	O
william	O
of	O
occam	O
,	O
c.	O
1320	O
by	O
now	O
,	O
you	O
are	O
an	O
expert	O
at	O
building	O
learning	O
algorithms	O
.	O
you	O
probably	O
understand	O
how	O
they	O
work	O
,	O
intuitively	O
.	O
and	O
you	O
under-	O
stand	O
why	O
they	O
should	O
generalize	B
.	O
however	O
,	O
there	O
are	O
several	O
basic	O
questions	O
you	O
might	O
want	O
to	O
know	O
the	O
answer	O
to	O
.	O
is	O
learning	O
always	O
possible	O
?	O
how	O
many	O
training	O
examples	O
will	O
i	O
need	O
to	O
do	O
a	O
good	O
job	O
learning	O
?	O
is	O
my	O
test	O
performance	O
going	O
to	O
be	O
much	O
worse	O
than	O
my	O
training	O
performance	O
?	O
the	O
key	O
idea	O
that	O
underlies	O
all	O
these	O
answer	O
is	O
that	O
simple	O
functions	O
generalize	B
well	O
.	O
the	O
amazing	O
thing	O
is	O
that	O
you	O
can	O
actually	O
prove	O
strong	O
results	O
that	O
address	O
the	O
above	O
questions	O
.	O
in	O
this	O
chapter	O
,	O
you	O
will	O
learn	O
some	O
of	O
the	O
most	O
important	O
results	O
in	O
learning	O
theory	O
that	O
attempt	O
to	O
answer	O
these	O
questions	O
.	O
the	O
goal	O
of	O
this	O
chapter	O
is	O
not	O
theory	O
for	O
theory	O
’	O
s	O
sake	O
,	O
but	O
rather	O
as	O
a	O
way	O
to	O
better	O
understand	O
why	O
learning	O
models	O
work	O
,	O
and	O
how	O
to	O
use	O
this	O
theory	O
to	O
build	O
better	O
algorithms	O
.	O
as	O
a	O
concrete	O
example	O
,	O
we	O
will	O
see	O
how	O
2-norm	O
regularization	O
prov-	O
ably	O
leads	O
to	O
better	O
generalization	O
performance	O
,	O
thus	O
justifying	O
our	O
common	O
practice	O
!	O
10.1	O
the	O
role	O
of	O
theory	O
in	O
contrast	O
to	O
the	O
quote	O
at	O
the	O
start	O
of	O
this	O
chapter	O
,	O
a	O
practitioner	O
friend	O
once	O
said	O
“	O
i	O
would	O
happily	O
give	O
up	O
a	O
few	O
percent	O
perfor-	O
mance	O
for	O
an	O
algorithm	B
that	O
i	O
can	O
understand.	O
”	O
both	O
perspectives	O
are	O
completely	O
valid	O
,	O
and	O
are	O
actually	O
not	O
contradictory	O
.	O
the	O
second	O
statement	O
is	O
presupposing	O
that	O
theory	O
helps	O
you	O
understand	O
,	O
which	O
hopefully	O
you	O
’	O
ll	O
ﬁnd	O
to	O
be	O
the	O
case	O
in	O
this	O
chapter	O
.	O
theory	O
can	O
serve	O
two	O
roles	O
.	O
it	O
can	O
justify	O
and	O
help	O
understand	O
why	O
common	O
practice	O
works	O
.	O
this	O
is	O
the	O
“	O
theory	O
after	O
”	O
view	O
.	O
it	O
can	O
also	O
serve	O
to	O
suggest	O
new	O
algorithms	O
and	O
approaches	O
that	O
turn	O
out	O
to	O
work	O
well	O
in	O
practice	O
.	O
this	O
is	O
the	O
“	O
theory	O
before	O
”	O
view	O
.	O
often	O
,	O
it	O
turns	O
out	O
to	O
be	O
a	O
mix	O
.	O
practitioners	O
discover	O
something	O
that	O
works	O
surpris-	O
ingly	O
well	O
.	O
theorists	O
ﬁgure	O
out	O
why	O
it	O
works	O
and	O
prove	O
something	O
about	O
it	O
.	O
and	O
in	O
the	O
process	O
,	O
they	O
make	O
it	O
better	O
or	O
ﬁnd	O
new	O
algo-	O
142	O
a	O
course	O
in	O
machine	O
learning	O
rithms	O
that	O
more	O
directly	O
exploit	O
whatever	O
property	O
it	O
is	O
that	O
made	O
the	O
theory	O
go	O
through	O
.	O
theory	O
can	O
also	O
help	O
you	O
understand	O
what	O
’	O
s	O
possible	O
and	O
what	O
’	O
s	O
not	O
possible	O
.	O
one	O
of	O
the	O
ﬁrst	O
things	O
we	O
’	O
ll	O
see	O
is	O
that	O
,	O
in	O
general	O
,	O
ma-	O
chine	O
learning	O
can	O
not	O
work	O
.	O
of	O
course	O
it	O
does	O
work	O
,	O
so	O
this	O
means	O
that	O
we	O
need	O
to	O
think	O
harder	O
about	O
what	O
it	O
means	O
for	O
learning	O
algo-	O
rithms	O
to	O
work	O
.	O
by	O
understanding	O
what	O
’	O
s	O
not	O
possible	O
,	O
you	O
can	O
focus	O
our	O
energy	O
on	O
things	O
that	O
are	O
.	O
probably	O
the	O
biggest	O
practical	O
success	O
story	O
for	O
theoretical	O
machine	O
learning	O
is	O
the	O
theory	O
of	O
boosting	B
,	O
which	O
you	O
won	O
’	O
t	O
actually	O
see	O
in	O
this	O
chapter	O
.	O
(	O
you	O
’	O
ll	O
have	O
to	O
wait	O
for	O
chapter	O
11	O
.	O
)	O
boosting	B
is	O
a	O
very	O
simple	O
style	O
of	O
algorithm	B
that	O
came	O
out	O
of	O
theoretical	O
machine	O
learn-	O
ing	O
,	O
and	O
has	O
proven	O
to	O
be	O
incredibly	O
successful	O
in	O
practice	O
.	O
so	O
much	O
so	O
that	O
it	O
is	O
one	O
of	O
the	O
de	O
facto	O
algorithms	O
to	O
run	O
when	O
someone	O
gives	O
you	O
a	O
new	O
data	O
set	O
.	O
in	O
fact	O
,	O
in	O
2004	O
,	O
yoav	O
freund	O
and	O
rob	O
schapire	O
won	O
the	O
acm	O
’	O
s	O
paris	O
kanellakis	O
award	O
for	O
their	O
boosting	B
algorithm	O
adaboost	O
.	O
this	O
award	O
is	O
given	O
for	O
theoretical	O
accomplishments	O
that	O
have	O
had	O
a	O
signiﬁcant	O
and	O
demonstrable	O
effect	O
on	O
the	O
practice	O
of	O
computing.1	O
10.2	O
induction	B
is	O
impossible	O
one	O
nice	O
thing	O
about	O
theory	O
is	O
that	O
it	O
forces	O
you	O
to	O
be	O
precise	O
about	O
what	O
you	O
are	O
trying	O
to	O
do	O
.	O
you	O
’	O
ve	O
already	O
seen	O
a	O
formal	O
deﬁnition	O
of	O
binary	O
classiﬁcation	O
in	O
chapter	O
5.	O
but	O
let	O
’	O
s	O
take	O
a	O
step	O
back	O
and	O
re-analyze	O
what	O
it	O
means	O
to	O
learn	O
to	O
do	O
binary	O
classiﬁcation	O
.	O
from	O
an	O
algorithmic	O
perspective	O
,	O
a	O
natural	O
question	O
is	O
whether	O
there	O
is	O
an	O
“	O
ultimate	O
”	O
learning	O
algorithm	B
,	O
aawesome	O
,	O
that	O
solves	O
the	O
binary	O
classiﬁcation	O
problem	O
above	O
.	O
in	O
other	O
words	O
,	O
have	O
you	O
been	O
wasting	O
your	O
time	O
learning	O
about	O
knn	O
and	O
perceptron	B
and	O
decision	B
trees	I
,	O
when	O
aawesome	O
is	O
out	O
there	O
.	O
what	O
would	O
such	O
an	O
ultimate	O
learning	O
algorithm	B
do	O
?	O
you	O
would	O
like	O
it	O
to	O
take	O
in	O
a	O
data	O
set	O
d	O
and	O
produce	O
a	O
function	O
f	O
.	O
no	O
matter	O
what	O
d	O
looks	O
like	O
,	O
this	O
function	O
f	O
should	O
get	O
perfect	O
classiﬁcation	O
on	O
all	O
future	O
examples	B
drawn	O
from	O
the	O
same	O
distribution	O
that	O
produced	O
d.	O
a	O
little	O
bit	O
of	O
introspection	O
should	O
demonstrate	O
that	O
this	O
is	O
impos-	O
sible	O
.	O
for	O
instance	O
,	O
there	O
might	O
be	O
label	B
noise	O
in	O
our	O
distribution	O
.	O
as	O
a	O
very	O
simple	O
example	O
,	O
let	O
x	O
=	O
{	O
−1	O
,	O
+1	O
}	O
(	O
i.e.	O
,	O
a	O
one-dimensional	O
,	O
binary	O
distribution	O
.	O
deﬁne	O
the	O
data	O
distribution	O
as	O
:	O
d	O
(	O
(	O
cid:104	O
)	O
+1	O
(	O
cid:105	O
)	O
,	O
+1	O
)	O
=	O
0.4	O
d	O
(	O
(	O
cid:104	O
)	O
+1	O
(	O
cid:105	O
)	O
,	O
−1	O
)	O
=	O
0.1	O
d	O
(	O
(	O
cid:104	O
)	O
−1	O
(	O
cid:105	O
)	O
,	O
−1	O
)	O
=	O
0.4	O
d	O
(	O
(	O
cid:104	O
)	O
−1	O
(	O
cid:105	O
)	O
,	O
+1	O
)	O
=	O
0.1	O
(	O
10.1	O
)	O
(	O
10.2	O
)	O
in	O
other	O
words	O
,	O
80	O
%	O
of	O
data	O
points	O
in	O
this	O
distrubtion	O
have	O
x	O
=	O
y	O
1	O
in	O
2008	O
,	O
corinna	O
cortes	O
and	O
vladimir	O
vapnik	O
won	O
it	O
for	O
support	O
vector	O
machines	O
.	O
learning	O
theory	O
143	O
?	O
it	O
’	O
s	O
clear	O
that	O
if	O
your	O
algorithm	B
pro-	O
duces	O
a	O
deterministic	O
function	O
that	O
it	O
can	O
not	O
do	O
better	O
than	O
20	O
%	O
error	O
.	O
what	O
if	O
it	O
produces	O
a	O
stochastic	O
(	O
aka	O
randomized	O
)	O
function	O
?	O
2	O
leslie	O
valiant	O
invented	O
the	O
notion	O
of	O
pac	O
learning	O
in	O
1984.	O
in	O
2011	O
,	O
he	O
received	O
the	O
turing	O
award	O
,	O
the	O
highest	O
honor	O
in	O
computing	O
for	O
his	O
work	O
in	O
learning	O
theory	O
,	O
computational	O
complexity	B
and	O
parallel	O
systems	O
.	O
and	O
20	O
%	O
don	O
’	O
t	O
.	O
no	O
matter	O
what	O
function	O
your	O
learning	O
algorithm	B
produces	O
,	O
there	O
’	O
s	O
no	O
way	O
that	O
it	O
can	O
do	O
better	O
than	O
20	O
%	O
error	O
on	O
this	O
data	O
.	O
given	O
this	O
,	O
it	O
seems	O
hopeless	O
to	O
have	O
an	O
algorithm	B
aawesome	O
that	O
always	O
achieves	O
an	O
error	B
rate	I
of	O
zero	O
.	O
the	O
best	O
that	O
we	O
can	O
hope	O
is	O
that	O
the	O
error	O
rate	O
is	O
not	O
“	O
too	O
large.	O
”	O
unfortunately	O
,	O
simply	O
weakening	O
our	O
requirement	O
on	O
the	O
error	O
rate	O
is	O
not	O
enough	O
to	O
make	O
learning	O
possible	O
.	O
the	O
second	O
source	O
of	O
difﬁculty	O
comes	O
from	O
the	O
fact	O
that	O
the	O
only	O
access	O
we	O
have	O
to	O
the	O
data	O
distribution	O
is	O
through	O
sampling	O
.	O
in	O
particular	O
,	O
when	O
trying	O
to	O
learn	O
about	O
a	O
distribution	O
like	O
that	O
in	O
10.1	O
,	O
you	O
only	O
get	O
to	O
see	O
data	O
points	O
drawn	O
from	O
that	O
distribution	O
.	O
you	O
know	O
that	O
“	O
eventually	O
”	O
you	O
will	O
see	O
enough	O
data	O
points	O
that	O
your	O
sample	O
is	O
representative	O
of	O
the	O
distribution	O
,	O
but	O
it	O
might	O
not	O
happen	O
immediately	O
.	O
for	O
instance	O
,	O
even	O
though	O
a	O
fair	O
coin	O
will	O
come	O
up	O
heads	O
only	O
with	O
probability	O
1/2	O
,	O
it	O
’	O
s	O
completely	O
plausible	O
that	O
in	O
a	O
sequence	O
of	O
four	O
coin	O
ﬂips	O
you	O
never	O
see	O
a	O
tails	O
,	O
or	O
perhaps	O
only	O
see	O
one	O
tails	O
.	O
aawesome	O
will	O
always	O
work	O
.	O
in	O
particular	O
,	O
if	O
we	O
happen	O
to	O
get	O
a	O
lousy	O
sample	O
of	O
data	O
from	O
d	O
,	O
we	O
need	O
to	O
allow	O
aawesome	O
to	O
do	O
something	O
completely	O
unreasonable	O
.	O
thus	O
,	O
we	O
can	O
not	O
hope	O
that	O
aawesome	O
will	O
do	O
perfectly	O
,	O
every	O
time	O
.	O
we	O
can	O
not	O
even	O
hope	O
that	O
it	O
will	O
do	O
pretty	O
well	O
,	O
all	O
of	O
the	O
time	O
.	O
nor	O
can	O
we	O
hope	O
that	O
it	O
will	O
do	O
perfectly	O
,	O
most	O
of	O
the	O
time	O
.	O
the	O
best	O
best	O
we	O
can	O
reasonably	O
hope	O
of	O
aawesome	O
is	O
that	O
it	O
it	O
will	O
do	O
pretty	O
well	O
,	O
most	O
of	O
the	O
time	O
.	O
so	O
the	O
second	O
thing	O
that	O
we	O
have	O
to	O
give	O
up	O
is	O
the	O
hope	O
that	O
10.3	O
probably	O
approximately	O
correct	O
learning	O
probably	O
approximately	O
correct	O
(	O
pac	O
)	O
learning	O
is	O
a	O
formalism	O
of	O
inductive	O
learning	O
based	O
on	O
the	O
realization	O
that	O
the	O
best	O
we	O
can	O
hope	O
of	O
an	O
algorithm	B
is	O
that	O
it	O
does	O
a	O
good	O
job	O
(	O
i.e.	O
,	O
is	O
approximately	O
correct	O
)	O
,	O
most	O
of	O
the	O
time	O
(	O
i.e.	O
,	O
it	O
is	O
probably	O
appoximately	O
correct	O
)	O
.2	O
consider	O
a	O
hypothetical	O
learning	O
algorithm	B
.	O
you	O
run	O
it	O
on	O
ten	O
dif-	O
ferent	O
binary	O
classiﬁcation	O
data	O
sets	O
.	O
for	O
each	O
one	O
,	O
it	O
comes	O
back	O
with	O
functions	O
f1	O
,	O
f2	O
,	O
.	O
.	O
.	O
,	O
f10	O
.	O
for	O
some	O
reason	O
,	O
whenever	O
you	O
run	O
f4	O
on	O
a	O
test	O
point	O
,	O
it	O
crashes	O
your	O
computer	O
.	O
for	O
the	O
other	O
learned	O
functions	O
,	O
their	O
performance	O
on	O
test	B
data	I
is	O
always	O
at	O
most	O
5	O
%	O
error	O
.	O
if	O
this	O
situtation	O
is	O
guaranteed	O
to	O
happen	O
,	O
then	O
this	O
hypothetical	O
learning	O
algorithm	B
is	O
a	O
pac	O
learning	O
algorithm	B
.	O
it	O
satisﬁes	O
“	O
probably	O
”	O
because	O
it	O
only	O
failed	O
in	O
one	O
out	O
of	O
ten	O
cases	O
,	O
and	O
it	O
’	O
s	O
“	O
approximate	O
”	O
because	O
it	O
achieved	O
low	O
,	O
but	O
non-zero	O
,	O
error	O
on	O
the	O
remainder	O
of	O
the	O
cases	O
.	O
this	O
leads	O
to	O
the	O
formal	O
deﬁnition	O
of	O
an	O
(	O
	O
,	O
δ	O
)	O
pac-learning	O
algo-	O
rithm	O
.	O
in	O
this	O
deﬁnition	O
,	O
	O
plays	O
the	O
role	O
of	O
measuring	O
accuracy	O
(	O
in	O
144	O
a	O
course	O
in	O
machine	O
learning	O
the	O
previous	O
example	O
,	O
	O
=	O
0.05	O
)	O
and	O
δ	O
plays	O
the	O
role	O
of	O
measuring	O
failure	O
(	O
in	O
the	O
previous	O
,	O
δ	O
=	O
0.1	O
)	O
.	O
deﬁnitions	O
1.	O
an	O
algorithm	B
a	O
is	O
an	O
(	O
	O
,	O
δ	O
)	O
-pac	O
learning	O
algorithm	B
if	O
,	O
for	O
all	O
distributions	O
d	O
:	O
given	O
samples	O
from	O
d	O
,	O
the	O
probability	O
that	O
it	O
returns	O
a	O
“	O
bad	O
function	O
”	O
is	O
at	O
most	O
δ	O
;	O
where	O
a	O
“	O
bad	O
”	O
function	O
is	O
one	O
with	O
test	B
error	I
rate	O
more	O
than	O
	O
on	O
d.	O
there	O
are	O
two	O
notions	O
of	O
efﬁciency	O
that	O
matter	O
in	O
pac	O
learning	O
.	O
the	O
ﬁrst	O
is	O
the	O
usual	O
notion	O
of	O
computational	O
complexity	B
.	O
you	O
would	O
prefer	O
an	O
algorithm	B
that	O
runs	O
quickly	O
to	O
one	O
that	O
takes	O
forever	O
.	O
the	O
second	O
is	O
the	O
notion	O
of	O
sample	B
complexity	I
:	O
the	O
number	O
of	O
examples	B
required	O
for	O
your	O
algorithm	B
to	O
achieve	O
its	O
goals	O
.	O
note	O
that	O
the	O
goal	O
of	O
both	O
of	O
these	O
measure	O
of	O
complexity	B
is	O
to	O
bound	O
how	O
much	O
of	O
a	O
scarse	O
resource	O
your	O
algorithm	B
uses	O
.	O
in	O
the	O
computational	O
case	O
,	O
the	O
resource	O
is	O
cpu	O
cycles	O
.	O
in	O
the	O
sample	O
case	O
,	O
the	O
resource	O
is	O
labeled	O
examples	B
.	O
deﬁnition	O
:	O
an	O
algorithm	B
a	O
is	O
an	O
efﬁcient	O
(	O
	O
,	O
δ	O
)	O
-pac	O
learning	O
al-	O
gorithm	O
if	O
it	O
is	O
an	O
(	O
	O
,	O
δ	O
)	O
-pac	O
learning	O
algorithm	B
whose	O
runtime	O
is	O
polynomial	O
in	O
1	O
	O
and	O
1	O
δ	O
.	O
in	O
other	O
words	O
,	O
suppose	O
that	O
you	O
want	O
your	O
algorithm	B
to	O
achieve	O
4	O
%	O
error	B
rate	I
rather	O
than	O
5	O
%	O
.	O
the	O
runtime	O
required	O
to	O
do	O
so	O
should	O
no	O
go	O
up	O
by	O
an	O
exponential	O
factor	O
.	O
10.4	O
pac	O
learning	O
of	O
conjunctions	O
to	O
get	O
a	O
better	O
sense	O
of	O
pac	O
learning	O
,	O
we	O
will	O
start	O
with	O
a	O
completely	O
irrelevant	O
and	O
uninteresting	O
example	O
.	O
the	O
purpose	O
of	O
this	O
example	O
is	O
only	O
to	O
help	O
understand	O
how	O
pac	O
learning	O
works	O
.	O
the	O
setting	O
is	O
learning	O
conjunctions	O
.	O
your	O
data	O
points	O
are	O
binary	O
vectors	O
,	O
for	O
instance	O
x	O
=	O
(	O
cid:104	O
)	O
0	O
,	O
1	O
,	O
1	O
,	O
0	O
,	O
1	O
(	O
cid:105	O
)	O
.	O
someone	O
guarantees	O
for	O
you	O
that	O
there	O
is	O
some	O
boolean	O
conjunction	O
that	O
deﬁnes	O
the	O
true	O
labeling	O
of	O
this	O
data	O
.	O
for	O
instance	O
,	O
x1	O
∧	O
¬x2	O
∧	O
x5	O
(	O
“	O
or	O
”	O
is	O
not	O
allowed	O
)	O
.	O
in	O
formal	O
terms	O
,	O
we	O
often	O
call	O
the	O
true	O
underlying	O
classiﬁcation	O
function	O
the	O
concept	O
.	O
so	O
this	O
is	O
saying	O
that	O
the	O
concept	O
you	O
are	O
trying	O
to	O
learn	O
is	O
a	O
conjunction	O
.	O
in	O
this	O
case	O
,	O
the	O
boolean	O
function	O
would	O
assign	O
a	O
negative	O
label	B
to	O
the	O
example	O
above	O
.	O
since	O
you	O
know	O
that	O
the	O
concept	O
you	O
are	O
trying	O
to	O
learn	O
is	O
a	O
con-	O
junction	O
,	O
it	O
makes	O
sense	O
that	O
you	O
would	O
represent	O
your	O
function	O
as	O
a	O
conjunction	O
as	O
well	O
.	O
for	O
historical	O
reasons	O
,	O
the	O
function	O
that	O
you	O
learn	O
is	O
often	O
called	O
a	O
hypothesis	B
and	O
is	O
often	O
denoted	O
h.	O
however	O
,	O
in	O
keeping	O
with	O
the	O
other	O
notation	O
in	O
this	O
book	O
,	O
we	O
will	O
continue	O
to	O
denote	O
it	O
f	O
.	O
formally	O
,	O
the	O
set	O
up	O
is	O
as	O
follows	O
.	O
there	O
is	O
some	O
distribution	O
dx	O
over	O
binary	O
data	O
points	O
(	O
vectors	O
)	O
x	O
=	O
(	O
cid:104	O
)	O
x1	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
xd	O
(	O
cid:105	O
)	O
.	O
there	O
is	O
a	O
ﬁxed	O
concept	B
conjunction	O
c	O
that	O
we	O
are	O
trying	O
to	O
learn	O
.	O
there	O
is	O
no	O
noise	B
,	O
so	O
for	O
any	O
example	O
x	O
,	O
its	O
true	O
label	B
is	O
simply	O
y	O
=	O
c	O
(	O
x	O
)	O
.	O
what	O
is	O
a	O
reasonable	O
algorithm	B
in	O
this	O
case	O
?	O
suppose	O
that	O
you	O
observe	O
the	O
example	O
in	O
table	O
10.1.	O
from	O
the	O
ﬁrst	O
example	O
,	O
we	O
know	O
that	O
the	O
true	O
formula	O
can	O
not	O
include	O
the	O
term	O
x1	O
.	O
if	O
it	O
did	O
,	O
this	O
exam-	O
ple	O
would	O
have	O
to	O
be	O
negative	O
,	O
which	O
it	O
is	O
not	O
.	O
by	O
the	O
same	O
reason-	O
ing	O
,	O
it	O
can	O
not	O
include	O
x2	O
.	O
by	O
analogous	O
reasoning	O
,	O
it	O
also	O
can	O
neither	O
include	O
the	O
term	O
¬x3	O
nor	O
the	O
term	O
¬x4	O
.	O
this	O
suggests	O
the	O
algorithm	O
in	O
algorithm	B
10.4	O
,	O
colloquially	O
the	O
learning	O
theory	O
145	O
y	O
+1	O
+1	O
-1	O
x1	O
0	O
0	O
1	O
x2	O
0	O
1	O
1	O
x3	O
1	O
1	O
0	O
x4	O
1	O
1	O
1	O
table	O
10.1	O
:	O
data	O
set	O
for	O
learning	O
con-	O
junctions	O
.	O
?	O
verify	O
that	O
algorithm	B
10.4	O
main-	O
tains	O
an	O
invariant	O
that	O
it	O
always	O
errs	O
on	O
the	O
side	O
of	O
classifying	O
examples	B
negative	O
and	O
never	O
errs	O
the	O
other	O
way	O
.	O
“	O
throw	O
out	O
bad	O
terms	O
”	O
algorithm	B
.	O
in	O
this	O
algorith	O
,	O
you	O
begin	O
with	O
a	O
function	O
that	O
includes	O
all	O
possible	O
2d	O
terms	O
.	O
note	O
that	O
this	O
function	O
will	O
initially	O
classify	O
everything	O
as	O
negative	O
.	O
you	O
then	O
process	O
each	O
example	O
in	O
sequence	O
.	O
on	O
a	O
negative	O
example	O
,	O
you	O
do	O
nothing	O
.	O
on	O
a	O
positive	O
example	O
,	O
you	O
throw	O
out	O
terms	O
from	O
f	O
that	O
contradict	O
the	O
given	O
positive	O
example	O
.	O
if	O
you	O
run	O
this	O
algorithm	B
on	O
the	O
data	O
in	O
table	O
10.1	O
,	O
the	O
sequence	O
of	O
f	O
s	O
that	O
you	O
cycle	O
through	O
are	O
:	O
f	O
0	O
(	O
x	O
)	O
=	O
x1	O
∧	O
¬x1	O
∧	O
x2	O
∧	O
¬x2	O
∧	O
x3	O
∧	O
¬x3	O
∧	O
x4	O
∧	O
¬x4	O
f	O
1	O
(	O
x	O
)	O
=	O
¬x1	O
∧	O
¬x2	O
∧	O
x3	O
∧	O
x4	O
f	O
2	O
(	O
x	O
)	O
=	O
¬x1	O
∧	O
x3	O
∧	O
x4	O
f	O
3	O
(	O
x	O
)	O
=	O
¬x1	O
∧	O
x3	O
∧	O
x4	O
(	O
10.3	O
)	O
(	O
10.4	O
)	O
(	O
10.5	O
)	O
(	O
10.6	O
)	O
the	O
ﬁrst	O
thing	O
to	O
notice	O
about	O
this	O
algorithm	B
is	O
that	O
after	O
processing	O
an	O
example	O
,	O
it	O
is	O
guaranteed	O
to	O
classify	O
that	O
example	O
correctly	O
.	O
this	O
observation	O
requires	O
that	O
there	O
is	O
no	O
noise	B
in	O
the	O
data	O
.	O
the	O
second	O
thing	O
to	O
notice	O
is	O
that	O
it	O
’	O
s	O
very	O
computationally	O
ef-	O
ﬁcient	O
.	O
given	O
a	O
data	O
set	O
of	O
n	O
examples	B
in	O
d	O
dimensions	O
,	O
it	O
takes	O
o	O
(	O
nd	O
)	O
time	O
to	O
process	O
the	O
data	O
.	O
this	O
is	O
linear	O
in	O
the	O
size	O
of	O
the	O
data	O
set	O
.	O
however	O
,	O
in	O
order	O
to	O
be	O
an	O
efﬁcient	O
(	O
	O
,	O
δ	O
)	O
-pac	O
learning	O
algorithm	B
,	O
you	O
need	O
to	O
be	O
able	O
to	O
get	O
a	O
bound	O
on	O
the	O
sample	O
complexity	B
of	O
this	O
algorithm	B
.	O
sure	O
,	O
you	O
know	O
that	O
its	O
run	O
time	O
is	O
linear	O
in	O
the	O
number	O
of	O
example	O
n.	O
but	O
how	O
many	O
examples	B
n	O
do	O
you	O
need	O
to	O
see	O
in	O
order	O
to	O
guarantee	O
that	O
it	O
achieves	O
an	O
error	B
rate	I
of	O
at	O
most	O
	O
(	O
in	O
all	O
but	O
δ-	O
many	O
cases	O
)	O
?	O
perhaps	O
n	O
has	O
to	O
be	O
gigantic	O
(	O
like	O
22d/	O
)	O
to	O
(	O
probably	O
)	O
guarantee	O
a	O
small	O
error	O
.	O
the	O
goal	O
is	O
to	O
prove	O
that	O
the	O
number	O
of	O
samples	O
n	O
required	O
to	O
(	O
probably	O
)	O
achieve	O
a	O
small	O
error	O
is	O
not-too-big	O
.	O
the	O
general	O
proof	O
technique	O
for	O
this	O
has	O
essentially	O
the	O
same	O
ﬂavor	O
as	O
almost	O
every	O
pac	O
learning	O
proof	O
around	O
.	O
first	O
,	O
you	O
deﬁne	O
a	O
“	O
bad	O
thing.	O
”	O
in	O
this	O
case	O
,	O
a	O
“	O
bad	O
thing	O
”	O
is	O
that	O
there	O
is	O
some	O
term	O
(	O
say	O
¬x8	O
)	O
that	O
should	O
have	O
been	O
thrown	O
out	O
,	O
but	O
wasn	O
’	O
t	O
.	O
then	O
you	O
say	O
:	O
well	O
,	O
bad	O
things	O
happen	O
.	O
then	O
you	O
notice	O
that	O
if	O
this	O
bad	O
thing	O
happened	O
,	O
you	O
must	O
not	O
have	O
146	O
a	O
course	O
in	O
machine	O
learning	O
algorithm	B
30	O
binaryconjunctiontrain	O
(	O
d	O
)	O
f	O
←	O
x1	O
∧	O
¬x1	O
∧	O
x2	O
∧	O
¬x2	O
∧	O
·	O
·	O
·	O
∧	O
xd	O
∧	O
¬xd	O
1	O
:	O
2	O
:	O
for	O
all	O
positive	O
examples	B
(	O
x	O
,	O
+1	O
)	O
in	O
d	O
do	O
3	O
:	O
for	O
d	O
=	O
1	O
.	O
.	O
.	O
d	O
do	O
if	O
xd	O
=	O
0	O
then	O
else	O
f	O
←	O
f	O
without	O
term	O
“	O
xd	O
”	O
f	O
←	O
f	O
without	O
term	O
“	O
¬xd	O
”	O
4	O
:	O
5	O
:	O
6	O
:	O
7	O
:	O
8	O
:	O
end	O
if	O
end	O
for	O
9	O
:	O
10	O
:	O
end	O
for	O
11	O
:	O
return	O
f	O
//	O
initialize	O
function	O
seen	O
any	O
positive	O
training	O
examples	B
with	O
x8	O
=	O
0.	O
so	O
example	O
with	O
x8	O
=	O
0	O
must	O
have	O
low	O
probability	O
(	O
otherwise	O
you	O
would	O
have	O
seen	O
them	O
)	O
.	O
so	O
bad	O
things	O
must	O
not	O
be	O
that	O
common	O
.	O
theorem	O
13.	O
with	O
probability	O
at	O
least	O
(	O
1	O
−	O
δ	O
)	O
:	O
algorithm	B
10.4	O
requires	O
at	O
most	O
n	O
=	O
.	O
.	O
.	O
examples	B
to	O
achieve	O
an	O
error	B
rate	I
≤	O
	O
.	O
proof	O
of	O
theorem	O
13.	O
let	O
c	O
be	O
the	O
concept	O
you	O
are	O
trying	O
to	O
learn	O
and	O
let	O
d	O
be	O
the	O
distribution	O
that	O
generates	O
the	O
data	O
.	O
a	O
learned	O
function	O
f	O
can	O
make	O
a	O
mistake	O
if	O
it	O
contains	O
any	O
term	O
t	O
that	O
is	O
not	O
in	O
c.	O
there	O
are	O
initially	O
2d	O
many	O
terms	O
in	O
f	O
,	O
and	O
any	O
(	O
or	O
all	O
!	O
)	O
of	O
them	O
might	O
not	O
be	O
in	O
c.	O
we	O
want	O
to	O
ensure	O
that	O
the	O
probability	O
that	O
f	O
makes	O
an	O
error	O
is	O
at	O
most	O
	O
.	O
it	O
is	O
sufﬁcient	O
to	O
ensure	O
that	O
for	O
a	O
term	O
t	O
(	O
e.g.	O
,	O
¬x5	O
)	O
,	O
we	O
say	O
that	O
t	O
“	O
negates	O
”	O
an	O
example	O
x	O
if	O
t	O
(	O
x	O
)	O
=	O
0.	O
call	O
a	O
term	O
t	O
“	O
bad	O
”	O
if	O
(	O
a	O
)	O
it	O
does	O
not	O
appear	O
in	O
c	O
and	O
(	O
b	O
)	O
has	O
probability	O
at	O
least	O
/2d	O
of	O
appearing	O
(	O
with	O
respect	O
to	O
the	O
unknown	O
distribution	O
d	O
over	O
data	O
points	O
)	O
.	O
first	O
,	O
we	O
show	O
that	O
if	O
we	O
have	O
no	O
bad	O
terms	O
left	O
in	O
f	O
,	O
then	O
f	O
has	O
an	O
error	B
rate	I
at	O
most	O
	O
.	O
we	O
know	O
that	O
f	O
contains	O
at	O
most	O
2d	O
terms	O
,	O
since	O
is	O
begins	O
with	O
2d	O
terms	O
and	O
throws	O
them	O
out	O
.	O
the	O
algorithm	O
begins	O
with	O
2d	O
terms	O
(	O
one	O
for	O
each	O
variable	O
and	O
one	O
for	O
each	O
negated	O
variable	O
)	O
.	O
note	O
that	O
f	O
will	O
only	O
make	O
one	O
type	O
of	O
error	O
:	O
it	O
can	O
call	O
positive	O
examples	O
negative	O
,	O
but	O
can	O
never	O
call	O
a	O
negative	O
example	O
positive	O
.	O
let	O
c	O
be	O
the	O
true	O
concept	B
(	O
true	O
boolean	O
formula	O
)	O
and	O
call	O
a	O
term	O
“	O
bad	O
”	O
if	O
it	O
does	O
not	O
appear	O
in	O
c.	O
a	O
speciﬁc	O
bad	O
term	O
(	O
e.g.	O
,	O
¬x5	O
)	O
will	O
cause	O
f	O
to	O
err	O
only	O
on	O
positive	O
examples	O
that	O
contain	O
a	O
corresponding	O
bad	O
value	O
(	O
e.g.	O
,	O
x5	O
=	O
1	O
)	O
.	O
todo	O
...	O
ﬁnish	O
this	O
what	O
we	O
’	O
ve	O
shown	O
in	O
this	O
theorem	O
is	O
that	O
:	O
if	O
the	O
true	O
underly-	O
ing	O
concept	B
is	O
a	O
boolean	O
conjunction	O
,	O
and	O
there	O
is	O
no	O
noise	B
,	O
then	O
the	O
“	O
throw	O
out	O
bad	O
terms	O
”	O
algorithm	B
needs	O
n	O
≤	O
.	O
.	O
.	O
examples	B
in	O
order	O
learning	O
theory	O
147	O
to	O
learn	O
a	O
boolean	O
conjunction	O
that	O
is	O
(	O
1	O
−	O
δ	O
)	O
-likely	O
to	O
achieve	O
an	O
er-	O
ror	O
of	O
at	O
most	O
	O
.	O
that	O
is	O
to	O
say	O
,	O
that	O
the	O
sample	O
complexity	B
of	O
“	O
throw	O
out	O
bad	O
terms	O
”	O
is	O
.	O
.	O
.	O
.	O
moreover	O
,	O
since	O
the	O
algorithm	O
’	O
s	O
runtime	O
is	O
linear	O
in	O
n	O
,	O
it	O
is	O
an	O
efﬁcient	O
pac	O
learning	O
algorithm	B
.	O
10.5	O
occam	O
’	O
s	O
razor	O
:	O
simple	O
solutions	O
generalize	B
the	O
previous	O
example	O
of	O
boolean	O
conjunctions	O
is	O
mostly	O
just	O
a	O
warm-	O
up	O
exercise	O
to	O
understand	O
pac-style	O
proofs	O
in	O
a	O
concrete	O
setting	O
.	O
in	O
this	O
section	O
,	O
you	O
get	O
to	O
generalize	B
the	O
above	O
argument	O
to	O
a	O
much	O
larger	O
range	O
of	O
learning	O
problems	O
.	O
we	O
will	O
still	O
assume	O
that	O
there	O
is	O
no	O
noise	B
,	O
because	O
it	O
makes	O
the	O
analysis	O
much	O
simpler	O
.	O
(	O
don	O
’	O
t	O
worry	O
:	O
noise	B
will	O
be	O
added	O
eventually	O
.	O
)	O
william	O
of	O
occam	O
(	O
c.	O
1288	O
–	O
c.	O
1348	O
)	O
was	O
an	O
english	O
friar	O
and	O
philosopher	O
is	O
is	O
most	O
famous	O
for	O
what	O
later	O
became	O
known	O
as	O
oc-	O
cam	O
’	O
s	O
razor	O
and	O
popularized	O
by	O
bertrand	O
russell	O
.	O
the	O
principle	O
ba-	O
sically	O
states	O
that	O
you	O
should	O
only	O
assume	O
as	O
much	O
as	O
you	O
need	O
.	O
or	O
,	O
more	O
verbosely	O
,	O
“	O
if	O
one	O
can	O
explain	O
a	O
phenomenon	O
without	O
assuming	O
this	O
or	O
that	O
hypothetical	O
entity	O
,	O
then	O
there	O
is	O
no	O
ground	O
for	O
assuming	O
it	O
i.e	O
.	O
that	O
one	O
should	O
always	O
opt	O
for	O
an	O
explanation	O
in	O
terms	O
of	O
the	O
fewest	O
possible	O
number	O
of	O
causes	O
,	O
factors	O
,	O
or	O
variables.	O
”	O
what	O
occam	O
actually	O
wrote	O
is	O
the	O
quote	O
that	O
began	O
this	O
chapter	O
.	O
in	O
a	O
machine	O
learning	O
context	O
,	O
a	O
reasonable	O
paraphrase	O
is	O
“	O
simple	O
solutions	O
generalize	B
well.	O
”	O
in	O
other	O
words	O
,	O
you	O
have	O
10	O
,	O
000	O
features	B
you	O
could	O
be	O
looking	O
at	O
.	O
if	O
you	O
’	O
re	O
able	O
to	O
explain	O
your	O
predictions	O
using	O
just	O
5	O
of	O
them	O
,	O
or	O
using	O
all	O
10	O
,	O
000	O
of	O
them	O
,	O
then	O
you	O
should	O
just	O
use	O
the	O
5.	O
the	O
occam	O
’	O
s	O
razor	O
theorem	O
states	O
that	O
this	O
is	O
a	O
good	O
idea	O
,	O
theo-	O
retically	O
.	O
it	O
essentially	O
states	O
that	O
if	O
you	O
are	O
learning	O
some	O
unknown	O
concept	B
,	O
and	O
if	O
you	O
are	O
able	O
to	O
ﬁt	O
your	O
training	B
data	I
perfectly	O
,	O
but	O
you	O
don	O
’	O
t	O
need	O
to	O
resort	O
to	O
a	O
huge	O
class	O
of	O
possible	O
functions	O
to	O
do	O
so	O
,	O
then	O
your	O
learned	O
function	O
will	O
generalize	B
well	O
.	O
it	O
’	O
s	O
an	O
amazing	O
theo-	O
rem	O
,	O
due	O
partly	O
to	O
the	O
simplicity	O
of	O
its	O
proof	O
.	O
in	O
some	O
ways	O
,	O
the	O
proof	O
is	O
actually	O
easier	O
than	O
the	O
proof	O
of	O
the	O
boolean	O
conjunctions	O
,	O
though	O
it	O
follows	O
the	O
same	O
basic	O
argument	O
.	O
in	O
order	O
to	O
state	O
the	O
theorem	O
explicitly	O
,	O
you	O
need	O
to	O
be	O
able	O
to	O
think	O
about	O
a	O
hypothesis	B
class	I
.	O
this	O
is	O
the	O
set	O
of	O
possible	O
hypotheses	O
that	O
your	O
algorithm	B
searches	O
through	O
to	O
ﬁnd	O
the	O
“	O
best	O
”	O
one	O
.	O
in	O
the	O
case	O
of	O
the	O
boolean	O
conjunctions	O
example	O
,	O
the	O
hypothesis	O
class	O
,	O
h	O
,	O
is	O
the	O
set	O
of	O
all	O
boolean	O
formulae	O
over	O
d-many	O
variables	O
.	O
in	O
the	O
case	O
of	O
a	O
perceptron	B
,	O
your	O
hypothesis	B
class	I
is	O
the	O
set	O
of	O
all	O
possible	O
linear	O
classiﬁers	O
.	O
the	O
hypothesis	O
class	O
for	O
boolean	O
conjunctions	O
is	O
ﬁnite	O
;	O
the	O
hypothesis	O
class	O
for	O
linear	O
classiﬁers	O
is	O
inﬁnite	O
.	O
for	O
occam	O
’	O
s	O
razor	O
,	O
we	O
can	O
only	O
work	O
with	O
ﬁnite	O
hypothesis	B
classes	O
.	O
148	O
a	O
course	O
in	O
machine	O
learning	O
theorem	O
14	O
(	O
occam	O
’	O
s	O
bound	O
)	O
.	O
suppose	O
a	O
is	O
an	O
algorithm	B
that	O
learns	O
a	O
function	O
f	O
from	O
some	O
ﬁnite	O
hypothesis	B
class	I
h.	O
suppose	O
the	O
learned	O
function	O
always	O
gets	O
zero	O
error	O
on	O
the	O
training	O
data	O
.	O
then	O
,	O
the	O
sample	O
com-	O
plexity	O
of	O
f	O
is	O
at	O
most	O
log|h|	O
.	O
todo	O
comments	O
proof	O
of	O
theorem	O
14.	O
todo	O
this	O
theorem	O
applies	O
directly	O
to	O
the	O
“	O
throw	O
out	O
bad	O
terms	O
”	O
algo-	O
rithm	O
,	O
since	O
(	O
a	O
)	O
the	O
hypothesis	O
class	O
is	O
ﬁnite	O
and	O
(	O
b	O
)	O
the	O
learned	O
func-	O
tion	O
always	O
achieves	O
zero	O
error	O
on	O
the	O
training	O
data	O
.	O
to	O
apply	O
oc-	O
cam	O
’	O
s	O
bound	O
,	O
you	O
need	O
only	O
compute	O
the	O
size	O
of	O
the	O
hypothesis	O
class	O
h	O
of	O
boolean	O
conjunctions	O
.	O
you	O
can	O
compute	O
this	O
by	O
noticing	O
that	O
there	O
are	O
a	O
total	O
of	O
2d	O
possible	O
terms	O
in	O
any	O
formula	O
in	O
h.	O
moreover	O
,	O
each	O
term	O
may	O
or	O
may	O
not	O
be	O
in	O
a	O
formula	O
.	O
so	O
there	O
are	O
22d	O
=	O
4d	O
possible	O
formulae	O
;	O
thus	O
,	O
|h|	O
=	O
4d	O
.	O
applying	O
occam	O
’	O
s	O
bound	O
,	O
we	O
see	O
that	O
the	O
sample	O
complexity	B
of	O
this	O
algorithm	B
is	O
n	O
≤	O
.	O
.	O
.	O
.	O
of	O
course	O
,	O
occam	O
’	O
s	O
bound	O
is	O
general	O
enough	O
to	O
capture	O
other	O
learning	O
algorithms	O
as	O
well	O
.	O
in	O
particular	O
,	O
it	O
can	O
capture	O
decision	B
trees	I
!	O
in	O
the	O
no-noise	O
setting	O
,	O
a	O
decision	B
tree	I
will	O
always	O
ﬁt	O
the	O
train-	O
ing	O
data	O
perfectly	O
.	O
the	O
only	O
remaining	O
difﬁculty	O
is	O
to	O
compute	O
the	O
size	O
of	O
the	O
hypothesis	O
class	O
of	O
a	O
decision	B
tree	I
learner	O
.	O
for	O
simplicity	O
’	O
s	O
sake	O
,	O
suppose	O
that	O
our	O
decision	B
tree	I
algorithm	O
always	O
learns	O
complete	O
trees	O
:	O
i.e.	O
,	O
every	O
branch	O
from	O
root	O
to	O
leaf	O
is	O
length	O
d.	O
so	O
the	O
number	O
of	O
split	O
points	O
in	O
the	O
tree	O
(	O
i.e.	O
,	O
places	O
where	O
a	O
feature	O
is	O
queried	O
)	O
is	O
2d−1	O
.	O
(	O
see	O
figure	O
10.1	O
.	O
)	O
each	O
split	O
point	O
needs	O
to	O
be	O
assigned	O
a	O
feature	O
:	O
there	O
d-many	O
choices	O
here	O
.	O
this	O
gives	O
d2d−1	O
trees	O
.	O
the	O
last	O
thing	O
is	O
that	O
there	O
are	O
2d	O
leaves	O
of	O
the	O
tree	O
,	O
each	O
of	O
which	O
can	O
take	O
two	O
possible	O
values	O
,	O
depending	O
on	O
whether	O
this	O
leaf	O
is	O
classiﬁed	O
as	O
+1	O
or	O
−1	O
:	O
this	O
is	O
2×2d	O
=	O
2d+1	O
possibilities	O
.	O
putting	O
this	O
all	O
togeter	O
gives	O
a	O
total	O
number	O
of	O
trees	O
|h|	O
=	O
d2d−12d+1	O
=	O
d22d	O
=	O
d4d	O
.	O
applying	O
occam	O
’	O
s	O
bound	O
,	O
we	O
see	O
that	O
todo	O
examples	B
is	O
enough	O
to	O
learn	O
a	O
decision	B
tree	I
!	O
10.6	O
complexity	B
of	O
inﬁnite	O
hypothesis	B
spaces	O
occam	O
’	O
s	O
bound	O
is	O
a	O
fantastic	O
result	O
for	O
learning	O
over	O
ﬁnite	O
hypothesis	B
spaces	O
.	O
unfortunately	O
,	O
it	O
is	O
completely	O
useless	O
when	O
|h|	O
=	O
∞	O
.	O
this	O
is	O
because	O
the	O
proof	O
works	O
by	O
using	O
each	O
of	O
the	O
n	O
training	O
examples	O
to	O
“	O
throw	O
out	O
”	O
bad	O
hypotheses	O
until	O
only	O
a	O
small	O
number	O
are	O
left	O
.	O
but	O
if	O
|h|	O
=	O
∞	O
,	O
and	O
you	O
’	O
re	O
throwing	O
out	O
a	O
ﬁnite	O
number	O
at	O
each	O
step	O
,	O
there	O
will	O
always	O
be	O
an	O
inﬁnite	O
number	O
remaining	O
.	O
this	O
means	O
that	O
,	O
if	O
you	O
want	O
to	O
establish	O
sample	B
complexity	I
results	O
for	O
inﬁnite	O
hypothesis	B
spaces	O
,	O
you	O
need	O
some	O
new	O
way	O
of	O
measuring	O
figure	O
10.1	O
:	O
thy	O
:	O
dt	O
:	O
picture	O
of	O
full	O
decision	B
tree	I
learning	O
theory	O
149	O
figure	O
10.2	O
:	O
thy	O
:	O
vcex	O
:	O
ﬁgure	O
with	O
three	O
and	O
four	O
examples	B
3	O
yes	O
,	O
this	O
is	O
the	O
same	O
vapnik	O
who	O
is	O
credited	O
with	O
the	O
creation	O
of	O
the	O
support	O
vector	B
machine	O
.	O
?	O
what	O
is	O
that	O
labeling	O
?	O
what	O
is	O
it	O
’	O
s	O
name	O
?	O
their	O
“	O
size	O
”	O
or	O
“	O
complexity.	O
”	O
a	O
prototypical	O
way	O
of	O
doing	O
this	O
is	O
to	O
measure	O
the	O
complexity	O
of	O
a	O
hypothesis	B
class	I
as	O
the	O
number	O
of	O
different	O
things	O
it	O
can	O
do	O
.	O
as	O
a	O
silly	O
example	O
,	O
consider	O
boolean	O
conjunctions	O
again	O
.	O
your	O
input	O
is	O
a	O
vector	B
of	O
binary	B
features	I
.	O
however	O
,	O
instead	O
of	O
representing	O
your	O
hypothesis	B
as	O
a	O
boolean	O
conjunction	O
,	O
you	O
choose	O
to	O
represent	O
it	O
as	O
a	O
conjunction	O
of	O
inequalities	O
.	O
that	O
is	O
,	O
instead	O
of	O
writing	O
x1	O
∧	O
¬x2	O
∧	O
x5	O
,	O
you	O
write	O
[	O
x1	O
>	O
0.2	O
]	O
∧	O
[	O
x2	O
<	O
0.77	O
]	O
∧	O
[	O
x5	O
<	O
π/4	O
]	O
.	O
in	O
this	O
representation	O
,	O
for	O
each	O
feature	O
,	O
you	O
need	O
to	O
choose	O
an	O
inequality	O
(	O
<	O
or	O
>	O
)	O
and	O
a	O
threshold	B
.	O
since	O
the	O
thresholds	O
can	O
be	O
arbitrary	O
real	O
values	O
,	O
there	O
are	O
now	O
inﬁnitely	O
many	O
possibilities	O
:	O
|h|	O
=	O
2d×∞	O
=	O
∞	O
.	O
however	O
,	O
you	O
can	O
immediately	O
recognize	O
that	O
on	O
binary	B
features	I
,	O
there	O
really	O
is	O
no	O
difference	O
between	O
[	O
x2	O
<	O
0.77	O
]	O
and	O
[	O
x2	O
<	O
0.12	O
]	O
and	O
any	O
other	O
number	O
of	O
inﬁnitely	O
many	O
possibilities	O
.	O
in	O
other	O
words	O
,	O
even	O
though	O
there	O
are	O
inﬁnitely	O
many	O
hypotheses	O
,	O
there	O
are	O
only	O
ﬁnitely	O
many	O
behaviors	O
.	O
the	O
vapnik-chernovenkis	O
dimension	O
(	O
or	O
vc	O
dimension	O
)	O
is	O
a	O
classic	O
measure	O
of	O
complexity	B
of	O
inﬁnite	O
hypothesis	B
classes	O
based	O
on	O
this	O
intuition3	O
.	O
the	O
vc	O
dimension	O
is	O
a	O
very	O
classiﬁcation-oriented	O
no-	O
tion	O
of	O
complexity	B
.	O
the	O
idea	O
is	O
to	O
look	O
at	O
a	O
ﬁnite	O
set	O
of	O
unlabeled	O
ex-	O
amples	O
,	O
such	O
as	O
those	O
in	O
figure	O
10.2.	O
the	O
question	O
is	O
:	O
no	O
matter	O
how	O
these	O
points	O
were	O
labeled	O
,	O
would	O
we	O
be	O
able	O
to	O
ﬁnd	O
a	O
hypothesis	B
that	O
correctly	O
classiﬁes	O
them	O
.	O
the	O
idea	O
is	O
that	O
as	O
you	O
add	O
more	O
points	O
,	O
being	O
able	O
to	O
represent	O
an	O
arbitrary	O
labeling	O
becomes	O
harder	O
and	O
harder	O
.	O
for	O
instance	O
,	O
regardless	O
of	O
how	O
the	O
three	O
points	O
are	O
labeled	O
,	O
you	O
can	O
ﬁnd	O
a	O
linear	O
classiﬁer	O
that	O
agrees	O
with	O
that	O
classiﬁcation	O
.	O
however	O
,	O
for	O
the	O
four	O
points	O
,	O
there	O
exists	O
a	O
labeling	O
for	O
which	O
you	O
can	O
not	O
ﬁnd	O
a	O
perfect	O
classiﬁer	O
.	O
the	O
vc	O
dimension	O
is	O
the	O
maximum	O
number	O
of	O
points	O
for	O
which	O
you	O
can	O
always	O
ﬁnd	O
such	O
a	O
classiﬁer	O
.	O
you	O
can	O
think	O
of	O
vc	O
dimension	O
as	O
a	O
game	O
between	O
you	O
and	O
an	O
adversary	O
.	O
to	O
play	O
this	O
game	O
,	O
you	O
choose	O
k	O
unlabeled	O
points	O
however	O
you	O
want	O
.	O
then	O
your	O
adversary	O
looks	O
at	O
those	O
k	O
points	O
and	O
assigns	O
binary	O
labels	O
to	O
them	O
them	O
however	O
he	O
wants	O
.	O
you	O
must	O
then	O
ﬁnd	O
a	O
hypothesis	B
(	O
classiﬁer	O
)	O
that	O
agrees	O
with	O
his	O
labeling	O
.	O
you	O
win	O
if	O
you	O
can	O
ﬁnd	O
such	O
a	O
hypothesis	B
;	O
he	O
wins	O
if	O
you	O
can	O
not	O
.	O
the	O
vc	O
dimension	O
of	O
your	O
hypothesis	B
class	I
is	O
the	O
maximum	O
number	O
of	O
points	O
k	O
so	O
that	O
you	O
can	O
always	O
win	O
this	O
game	O
.	O
this	O
leads	O
to	O
the	O
following	O
formal	O
deﬁnition	O
,	O
where	O
you	O
can	O
interpret	O
there	O
exists	O
as	O
your	O
move	O
and	O
for	O
all	O
as	O
adversary	O
’	O
s	O
move	O
.	O
deﬁnitions	O
2.	O
for	O
data	O
drawn	O
from	O
some	O
space	O
x	O
,	O
the	O
vc	O
dimension	O
of	O
a	O
hypothesis	B
space	O
h	O
over	O
x	O
is	O
the	O
maximal	O
k	O
such	O
that	O
:	O
there	O
exists	O
a	O
set	O
x	O
⊆	O
x	O
of	O
size	O
|x|	O
=	O
k	O
,	O
such	O
that	O
for	O
all	O
binary	O
labelings	O
of	O
x	O
,	O
there	O
exists	O
a	O
function	O
f	O
∈	O
h	O
that	O
matches	O
this	O
labeling	O
.	O
150	O
a	O
course	O
in	O
machine	O
learning	O
in	O
general	O
,	O
it	O
is	O
much	O
easier	O
to	O
show	O
that	O
the	O
vc	O
dimension	O
is	O
at	O
least	O
some	O
value	O
;	O
it	O
is	O
much	O
harder	O
to	O
show	O
that	O
it	O
is	O
at	O
most	O
some	O
value	O
.	O
for	O
example	O
,	O
following	O
on	O
the	O
example	O
from	O
figure	O
10.2	O
,	O
the	O
image	O
of	O
three	O
points	O
(	O
plus	O
a	O
little	O
argumentation	O
)	O
is	O
enough	O
to	O
show	O
that	O
the	O
vc	O
dimension	O
of	O
linear	O
classiﬁers	O
in	O
two	O
dimension	O
is	O
at	O
least	O
three	O
.	O
to	O
show	O
that	O
the	O
vc	O
dimension	O
is	O
exactly	O
three	O
it	O
sufﬁces	O
to	O
show	O
that	O
you	O
can	O
not	O
ﬁnd	O
a	O
set	O
of	O
four	O
points	O
such	O
that	O
you	O
win	O
this	O
game	O
against	O
the	O
adversary	O
.	O
this	O
is	O
much	O
more	O
difﬁcult	O
.	O
in	O
the	O
proof	O
that	O
the	O
vc	O
dimension	O
is	O
at	O
least	O
three	O
,	O
you	O
simply	O
need	O
to	O
provide	O
an	O
example	O
of	O
three	O
points	O
,	O
and	O
then	O
work	O
through	O
the	O
small	O
number	O
of	O
possible	O
labelings	O
of	O
that	O
data	O
.	O
to	O
show	O
that	O
it	O
is	O
at	O
most	O
three	O
,	O
you	O
need	O
to	O
argue	O
that	O
no	O
matter	O
what	O
set	O
of	O
four	O
point	O
you	O
pick	O
,	O
you	O
can	O
not	O
win	O
the	O
game	O
.	O
vc	O
margins	O
small	O
norms	O
10.7	O
learning	O
with	O
noise	B
10.8	O
agnostic	O
learning	O
10.9	O
error	O
versus	O
regret	O
despite	O
the	O
fact	O
that	O
there	O
’	O
s	O
no	O
way	O
to	O
get	O
better	O
than	O
20	O
%	O
error	O
on	O
this	O
distribution	O
,	O
it	O
would	O
be	O
nice	O
to	O
say	O
that	O
you	O
can	O
still	O
learn	O
some-	O
thing	O
from	O
it	O
.	O
for	O
instance	O
,	O
the	O
predictor	O
that	O
always	O
guesses	O
y	O
=	O
x	O
seems	O
like	O
the	O
“	O
right	O
”	O
thing	O
to	O
do	O
.	O
based	O
on	O
this	O
observation	O
,	O
maybe	O
we	O
can	O
rephrase	O
the	O
goal	O
of	O
learning	O
as	O
to	O
ﬁnd	O
a	O
function	O
that	O
does	O
as	O
well	O
as	O
the	O
distribution	O
allows	O
.	O
in	O
other	O
words	O
,	O
on	O
this	O
data	O
,	O
you	O
would	O
hope	O
to	O
get	O
20	O
%	O
error	O
.	O
on	O
some	O
other	O
distribution	O
,	O
you	O
would	O
hope	O
to	O
get	O
x	O
%	O
error	O
,	O
where	O
x	O
%	O
is	O
the	O
best	O
you	O
could	O
do	O
.	O
this	O
notion	O
of	O
“	O
best	O
you	O
could	O
do	O
”	O
is	O
sufﬁciently	O
important	O
that	O
it	O
has	O
a	O
name	O
:	O
the	O
bayes	O
error	B
rate	I
.	O
this	O
is	O
the	O
error	O
rate	O
of	O
the	O
best	O
possible	O
classiﬁer	O
,	O
the	O
so-called	O
bayes	O
optimal	O
classiﬁer	O
.	O
if	O
you	O
knew	O
the	O
underlying	O
distribution	O
d	O
,	O
you	O
could	O
actually	O
write	O
down	O
the	O
exact	O
bayes	O
optimal	O
classiﬁer	O
explicitly	O
.	O
(	O
this	O
is	O
why	O
learning	O
is	O
unin-	O
teresting	O
in	O
the	O
case	O
that	O
you	O
know	O
d.	O
)	O
it	O
simply	O
has	O
the	O
form	O
:	O
f	O
bayes	O
(	O
x	O
)	O
=	O
if	O
d	O
(	O
x	O
,	O
+1	O
)	O
>	O
d	O
(	O
x	O
,	O
−1	O
)	O
+1	O
−1	O
otherwise	O
(	O
10.7	O
)	O
(	O
cid:40	O
)	O
the	O
bayes	O
optimal	O
error	B
rate	I
is	O
the	O
error	O
rate	O
that	O
this	O
(	O
hypothetical	O
)	O
classiﬁer	O
achieves	O
:	O
(	O
x	O
,	O
y	O
)	O
∼d	O
(	O
cid:2	O
)	O
y	O
(	O
cid:54	O
)	O
=	O
f	O
bayes	O
(	O
x	O
)	O
(	O
cid:3	O
)	O
bayes	O
=	O
e	O
(	O
10.8	O
)	O
10.10	O
exercises	O
exercise	O
10.1.	O
todo	O
.	O
.	O
.	O
learning	O
theory	O
151	O
11	O
|	O
ensemble	B
methods	O
learning	O
objectives	O
:	O
•	O
implement	O
bagging	B
and	O
explain	O
how	O
it	O
reduces	O
variance	B
in	O
a	O
predictor	O
.	O
•	O
explain	O
the	O
difference	O
between	O
a	O
weak	B
learner	I
and	O
a	O
strong	B
learner	I
.	O
•	O
derive	O
the	O
adaboost	O
algorithm	B
.	O
•	O
understand	O
the	O
relationship	O
between	O
boosting	B
decision	O
stumps	O
and	O
linear	O
classiﬁcation	O
.	O
dependencies	O
:	O
–	O
groups	O
of	O
people	O
can	O
often	O
make	O
better	O
decisions	O
than	O
individuals	O
,	O
especially	O
when	O
group	O
members	O
each	O
come	O
in	O
with	O
their	O
own	O
biases	O
.	O
the	O
same	O
is	O
true	O
in	O
machine	O
learning	O
.	O
ensemble	B
methods	O
are	O
learning	O
models	O
that	O
achieve	O
performance	O
by	O
combining	O
the	O
opinions	O
of	O
multiple	O
learners	O
.	O
in	O
doing	O
so	O
,	O
you	O
can	O
often	O
get	O
away	O
with	O
using	O
much	O
simpler	O
learners	O
and	O
still	O
achieve	O
great	O
performance	O
.	O
moreover	O
,	O
ensembles	O
are	O
inherantly	O
parallel	O
,	O
which	O
can	O
make	O
them	O
much	O
more	O
efﬁcient	O
at	O
training	O
and	O
test	O
time	O
,	O
if	O
you	O
have	O
access	O
to	O
multiple	O
processors	O
.	O
in	O
this	O
chapter	O
,	O
you	O
will	O
learn	O
about	O
various	O
ways	O
of	O
combining	O
base	O
learners	O
into	O
ensembles	O
.	O
one	O
of	O
the	O
shocking	O
results	O
we	O
will	O
see	O
is	O
that	O
you	O
can	O
take	O
a	O
learning	O
model	B
that	O
only	O
ever	O
does	O
slightly	O
better	O
than	O
chance	O
,	O
and	O
turn	O
it	O
into	O
an	O
arbitrarily	O
good	O
learning	O
model	B
,	O
though	O
a	O
technique	O
known	O
as	O
boosting	B
.	O
you	O
will	O
also	O
learn	O
how	O
ensembles	O
can	O
decrease	O
the	O
variance	O
of	O
predictors	O
as	O
well	O
as	O
perform	O
regularization	O
.	O
11.1	O
voting	B
multiple	O
classiﬁers	O
all	O
of	O
the	O
learning	O
algorithms	O
you	O
have	O
seen	O
so	O
far	O
are	O
deterministic	O
.	O
if	O
you	O
train	O
a	O
decision	B
tree	I
multiple	O
times	O
on	O
the	O
same	O
data	O
set	O
,	O
you	O
will	O
always	O
get	O
the	O
same	O
tree	O
back	O
.	O
in	O
order	O
to	O
get	O
an	O
effect	O
out	O
of	O
voting	B
multiple	O
classiﬁers	O
,	O
they	O
need	O
to	O
differ	O
.	O
there	O
are	O
two	O
primary	O
ways	O
to	O
get	O
variability	O
.	O
you	O
can	O
either	O
change	O
the	O
learning	O
algorithm	B
or	O
change	O
the	O
data	O
set	O
.	O
building	O
an	O
emsemble	O
by	O
training	O
different	O
classiﬁers	O
is	O
the	O
most	O
straightforward	O
approach	O
.	O
as	O
in	O
single-model	O
learning	O
,	O
you	O
are	O
given	O
a	O
data	O
set	O
(	O
say	O
,	O
for	O
classiﬁcation	O
)	O
.	O
instead	O
of	O
learning	O
a	O
single	O
classiﬁer	O
(	O
e.g.	O
,	O
a	O
decision	B
tree	I
)	O
on	O
this	O
data	O
set	O
,	O
you	O
learn	O
multiple	O
different	O
classiﬁers	O
.	O
for	O
instance	O
,	O
you	O
might	O
train	O
a	O
decision	B
tree	I
,	O
a	O
perceptron	B
,	O
a	O
knn	O
,	O
and	O
multiple	O
neural	B
networks	I
with	O
different	O
architectures	O
.	O
call	O
these	O
classiﬁers	O
f1	O
,	O
.	O
.	O
.	O
,	O
fm	O
.	O
at	O
test	O
time	O
,	O
you	O
can	O
make	O
a	O
predic-	O
tion	O
by	O
voting	B
.	O
on	O
a	O
test	O
example	O
ˆx	O
,	O
you	O
compute	O
ˆy1	O
=	O
f1	O
(	O
ˆx	O
)	O
,	O
.	O
.	O
.	O
,	O
ensemble	B
methods	O
153	O
?	O
which	O
of	O
the	O
classiﬁers	O
you	O
’	O
ve	O
learned	O
about	O
so	O
far	O
have	O
high	O
variance	B
?	O
figure	O
11.1	O
:	O
picture	O
of	O
sampling	O
with	O
replacement	O
1	O
to	O
sample	O
with	O
replacement	O
,	O
imagine	O
putting	O
all	O
items	O
from	O
d	O
in	O
a	O
hat	O
.	O
to	O
draw	O
a	O
single	O
sample	O
,	O
pick	O
an	O
element	O
at	O
random	O
from	O
that	O
hat	O
,	O
write	O
it	O
down	O
,	O
and	O
then	O
put	O
it	O
back	O
.	O
ˆym	O
=	O
fm	O
(	O
ˆx	O
)	O
.	O
if	O
there	O
are	O
more	O
+1s	O
in	O
the	O
list	O
(	O
cid:104	O
)	O
y1	O
,	O
.	O
.	O
.	O
,	O
ym	O
then	O
you	O
predict	B
+1	O
;	O
otherwise	O
you	O
predict	B
−1	O
.	O
the	O
main	O
advantage	O
of	O
ensembles	O
of	O
different	O
classiﬁers	O
is	O
that	O
it	O
is	O
unlikely	O
that	O
all	O
classiﬁers	O
will	O
make	O
the	O
same	O
mistake	O
.	O
in	O
fact	O
,	O
as	O
long	O
as	O
every	O
error	O
is	O
made	O
by	O
a	O
minority	O
of	O
the	O
classiﬁers	O
,	O
you	O
will	O
achieve	O
optimal	O
classiﬁcation	O
!	O
unfortunately	O
,	O
the	O
inductive	O
biases	O
of	O
different	O
learning	O
algorithms	O
are	O
highly	O
correlated	O
.	O
this	O
means	O
that	O
different	O
algorithms	O
are	O
prone	O
to	O
similar	O
types	O
of	O
errors	O
.	O
in	O
particular	O
,	O
ensembles	O
tend	O
to	O
reduce	O
the	O
variance	O
of	O
classiﬁers	O
.	O
so	O
if	O
you	O
have	O
a	O
classiﬁcation	O
algorithm	B
that	O
tends	O
to	O
be	O
very	O
sensitive	O
to	O
small	O
changes	O
in	O
the	O
training	O
data	O
,	O
ensembles	O
are	O
likely	O
to	O
be	O
useful	O
.	O
note	O
that	O
the	O
voting	O
scheme	O
naturally	O
extends	O
to	O
multiclass	O
clas-	O
siﬁcation	O
.	O
however	O
,	O
it	O
does	O
not	O
make	O
sense	O
in	O
the	O
contexts	O
of	O
regres-	O
sion	O
,	O
ranking	O
or	O
collective	O
classiﬁcation	O
.	O
this	O
is	O
because	O
you	O
will	O
rarely	O
see	O
the	O
same	O
exact	O
output	O
predicted	O
twice	O
by	O
two	O
different	O
regression	O
models	O
(	O
or	O
ranking	O
models	O
or	O
collective	O
classiﬁcation	O
mod-	O
els	O
)	O
.	O
for	O
regression	O
,	O
a	O
simple	O
solution	O
is	O
to	O
take	O
the	O
mean	O
or	O
median	O
prediction	O
from	O
the	O
different	O
models	O
.	O
for	O
ranking	O
and	O
collective	O
clas-	O
siﬁcation	O
,	O
different	O
approaches	O
are	O
required	O
.	O
instead	O
of	O
training	O
different	O
types	O
of	O
classiﬁers	O
on	O
the	O
same	O
data	O
set	O
,	O
you	O
can	O
train	O
a	O
single	O
type	O
of	O
classiﬁer	O
(	O
e.g.	O
,	O
decision	B
tree	I
)	O
on	O
multiple	O
data	O
sets	O
.	O
the	O
question	O
is	O
:	O
where	O
do	O
these	O
multiple	O
data	O
sets	O
come	O
from	O
,	O
since	O
you	O
’	O
re	O
only	O
given	O
one	O
at	O
training	O
time	O
?	O
one	O
option	O
is	O
to	O
fragment	O
your	O
original	O
data	O
set	O
.	O
for	O
instance	O
,	O
you	O
could	O
break	O
it	O
into	O
10	O
pieces	O
and	O
build	O
decision	B
trees	I
on	O
each	O
of	O
these	O
pieces	O
individually	O
.	O
unfortunately	O
,	O
this	O
means	O
that	O
each	O
decision	B
tree	I
is	O
trained	O
on	O
only	O
a	O
very	O
small	O
part	O
of	O
the	O
entire	O
data	O
set	O
and	O
is	O
likely	O
to	O
perform	O
poorly	O
.	O
a	O
better	O
solution	O
is	O
to	O
use	O
bootstrap	B
resampling	I
.	O
this	O
is	O
a	O
tech-	O
nique	O
from	O
the	O
statistics	O
literature	O
based	O
on	O
the	O
following	O
observa-	O
tion	O
.	O
the	O
data	O
set	O
we	O
are	O
given	O
,	O
d	O
,	O
is	O
a	O
sample	O
drawn	O
i.i.d	O
.	O
from	O
an	O
unknown	O
distribution	O
d.	O
if	O
we	O
draw	O
a	O
new	O
data	O
set	O
˜d	O
by	O
random	O
sampling	O
from	O
d	O
with	O
replacement1	O
,	O
then	O
˜d	O
is	O
also	O
a	O
sample	O
from	O
d.	O
figure	O
11.1	O
shows	O
the	O
process	O
of	O
bootstrap	B
resampling	I
of	O
ten	O
objects	O
.	O
applying	O
this	O
idea	O
to	O
ensemble	B
methods	O
yields	O
a	O
technique	O
known	O
as	O
bagging	B
.	O
you	O
start	O
with	O
a	O
single	O
data	O
set	O
d	O
that	O
contains	O
n	O
train-	O
ing	O
examples	B
.	O
from	O
this	O
single	O
data	O
set	O
,	O
you	O
create	O
m-many	O
“	O
boot-	O
strapped	O
training	O
sets	O
”	O
˜d1	O
,	O
.	O
.	O
.	O
˜dm	O
.	O
each	O
of	O
these	O
bootstrapped	O
sets	O
also	O
contains	O
n	O
training	O
examples	O
,	O
drawn	O
randomly	O
from	O
d	O
with	O
replacement	O
.	O
you	O
can	O
then	O
train	O
a	O
decision	B
tree	I
(	O
or	O
other	O
model	B
)	O
seperately	O
on	O
each	O
of	O
these	O
data	O
sets	O
to	O
obtain	O
classiﬁers	O
f1	O
,	O
.	O
.	O
.	O
,	O
fm	O
.	O
as	O
before	O
,	O
you	O
can	O
use	O
these	O
classiﬁers	O
to	O
vote	B
on	O
new	O
test	O
points	O
.	O
note	O
that	O
the	O
bootstrapped	O
data	O
sets	O
will	O
be	O
similar	O
.	O
however	O
,	O
they	O
will	O
not	O
be	O
too	O
similar	O
.	O
for	O
example	O
,	O
if	O
n	O
is	O
large	O
then	O
the	O
number	O
of	O
figure	O
11.2	O
:	O
graph	B
depicting	O
overﬁtting	O
using	O
regularization	O
versus	O
bagging	B
154	O
a	O
course	O
in	O
machine	O
learning	O
examples	B
that	O
are	O
not	O
present	O
in	O
any	O
particular	O
bootstrapped	O
sample	O
is	O
relatively	O
large	O
.	O
the	O
probability	O
that	O
the	O
ﬁrst	O
training	O
example	O
is	O
not	O
selected	O
once	O
is	O
(	O
1	O
−	O
1/n	O
)	O
.	O
the	O
probability	O
that	O
it	O
is	O
not	O
selected	O
at	O
all	O
is	O
(	O
1	O
−	O
1/n	O
)	O
n.	O
as	O
n	O
→	O
∞	O
,	O
this	O
tends	O
to	O
1/e	O
≈	O
0.3679	O
.	O
(	O
already	O
for	O
n	O
=	O
1000	O
this	O
is	O
correct	O
to	O
four	O
decimal	O
points	O
.	O
)	O
so	O
only	O
about	O
63	O
%	O
of	O
the	O
original	O
training	O
examples	O
will	O
be	O
represented	O
in	O
any	O
given	O
bootstrapped	O
set	O
.	O
since	O
bagging	B
tends	O
to	O
reduce	O
variance	B
,	O
it	O
provides	O
an	O
alternative	O
approach	O
to	O
regularization	O
.	O
that	O
is	O
,	O
even	O
if	O
each	O
of	O
the	O
learned	O
clas-	O
siﬁers	O
f1	O
,	O
.	O
.	O
.	O
,	O
fm	O
are	O
individually	O
overﬁt	O
,	O
they	O
are	O
likely	O
to	O
be	O
overﬁt	O
to	O
different	O
things	O
.	O
through	O
voting	B
,	O
you	O
are	O
able	O
to	O
overcome	O
a	O
sig-	O
niﬁcant	O
portion	O
of	O
this	O
overﬁtting	O
.	O
figure	O
?	O
?	O
shows	O
this	O
effect	O
by	O
comparing	O
regularization	O
via	O
hyperparameters	O
to	O
regularization	O
via	O
bagging	B
.	O
11.2	O
boosting	B
weak	O
learners	O
boosting	B
is	O
the	O
process	O
of	O
taking	O
a	O
crummy	O
learning	O
algorithm	B
(	O
tech-	O
nically	O
called	O
a	O
weak	B
learner	I
)	O
and	O
turning	O
it	O
into	O
a	O
great	O
learning	O
algorithm	B
(	O
technically	O
,	O
a	O
strong	B
learner	I
)	O
.	O
of	O
all	O
the	O
ideas	O
that	O
origi-	O
nated	O
in	O
the	O
theoretical	O
machine	O
learning	O
community	O
,	O
boosting	B
has	O
had—perhaps—the	O
greatest	O
practical	O
impact	O
.	O
the	O
idea	O
of	O
boosting	B
is	O
reminiscent	O
of	O
what	O
you	O
(	O
like	O
me	O
!	O
)	O
might	O
have	O
thought	O
when	O
you	O
ﬁrst	O
learned	O
about	O
ﬁle	O
compression	O
.	O
if	O
i	O
compress	O
a	O
ﬁle	O
,	O
and	O
then	O
re-compress	O
it	O
,	O
and	O
then	O
re-compress	O
it	O
,	O
eventually	O
i	O
’	O
ll	O
end	O
up	O
with	O
a	O
ﬁnal	O
that	O
’	O
s	O
only	O
one	O
byte	O
in	O
size	O
!	O
to	O
be	O
more	O
formal	O
,	O
let	O
’	O
s	O
deﬁne	O
a	O
strong	B
learning	I
algorithm	I
l	O
as	O
follows	O
.	O
when	O
given	O
a	O
desired	O
error	B
rate	I
	O
,	O
a	O
failure	O
probability	O
δ	O
and	O
access	O
to	O
“	O
enough	O
”	O
labeled	O
examples	B
from	O
some	O
distribution	O
d	O
,	O
then	O
,	O
with	O
high	O
probability	O
(	O
at	O
least	O
1	O
−	O
δ	O
)	O
,	O
l	O
learns	O
a	O
classiﬁer	O
f	O
that	O
has	O
error	O
at	O
most	O
	O
.	O
this	O
is	O
precisely	O
the	O
deﬁnition	O
of	O
pac	O
learning	O
that	O
you	O
learned	O
about	O
in	O
chapter	O
10.	O
building	O
a	O
strong	B
learning	I
algorithm	I
might	O
be	O
difﬁcult	O
.	O
we	O
can	O
as	O
if	O
,	O
instead	O
,	O
it	O
is	O
possible	O
to	O
build	O
a	O
weak	B
learning	I
algorithm	I
w	O
that	O
only	O
has	O
to	O
achieve	O
an	O
error	B
rate	I
of	O
49	O
%	O
,	O
rather	O
than	O
some	O
arbitrary	O
user-deﬁned	O
parameter	O
	O
.	O
(	O
49	O
%	O
is	O
arbitrary	O
:	O
anything	O
strictly	O
less	O
than	O
50	O
%	O
would	O
be	O
ﬁne	O
.	O
)	O
work	O
for	O
taking	O
a	O
weak	B
learning	I
algorithm	I
w	O
and	O
turning	O
it	O
into	O
a	O
strong	B
learning	I
algorithm	I
.	O
the	O
particular	O
boosting	B
algorithm	O
dis-	O
cussed	O
here	O
is	O
adaboost	O
,	O
short	O
for	O
“	O
adaptive	O
boosting	B
algorithm.	O
”	O
adaboost	O
is	O
famous	O
because	O
it	O
was	O
one	O
of	O
the	O
ﬁrst	O
practical	O
boosting	B
algorithms	O
:	O
it	O
runs	O
in	O
polynomial	O
time	O
and	O
does	O
not	O
require	O
you	O
to	O
deﬁne	O
a	O
large	O
number	O
of	O
hyperparameters	O
.	O
it	O
gets	O
its	O
name	O
from	O
the	O
latter	O
beneﬁt	O
:	O
it	O
automatically	O
adapts	O
to	O
the	O
data	O
that	O
you	O
give	O
it	O
.	O
boosting	B
is	O
more	O
of	O
a	O
“	O
framework	O
”	O
than	O
an	O
algorithm	B
.	O
it	O
’	O
s	O
a	O
frame-	O
ensemble	B
methods	O
155	O
?	O
what	O
happens	O
if	O
the	O
weak	O
learn-	O
ing	O
assumption	O
is	O
violated	O
and	O
ˆ	O
is	O
equal	O
to	O
50	O
%	O
?	O
what	O
if	O
it	O
is	O
worse	O
than	O
50	O
%	O
?	O
what	O
does	O
this	O
mean	O
,	O
in	O
practice	O
?	O
n	O
,	O
1	O
algorithm	B
31	O
adaboost	O
(	O
w	O
,	O
d	O
,	O
k	O
)	O
1	O
:	O
d	O
(	O
0	O
)	O
←	O
(	O
cid:104	O
)	O
1	O
2	O
:	O
for	O
k	O
=	O
1	O
.	O
.	O
.	O
k	O
do	O
3	O
:	O
n	O
(	O
cid:105	O
)	O
n	O
,	O
.	O
.	O
.	O
,	O
1	O
f	O
(	O
k	O
)	O
←	O
w	O
(	O
d	O
,	O
d	O
(	O
k-1	O
)	O
)	O
ˆyn	O
←	O
f	O
(	O
k	O
)	O
(	O
xn	O
)	O
,	O
∀n	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
1−	O
ˆ	O
(	O
k	O
)	O
ˆ	O
(	O
k	O
)	O
←	O
∑n	O
d	O
(	O
k-1	O
)	O
[	O
yn	O
(	O
cid:54	O
)	O
=	O
ˆyn	O
]	O
α	O
(	O
k	O
)	O
←	O
1	O
exp	O
[	O
−α	O
(	O
k	O
)	O
yn	O
ˆyn	O
]	O
,	O
∀n	O
n	O
←	O
1	O
d	O
(	O
k	O
)	O
n	O
2	O
log	O
z	O
d	O
(	O
k-1	O
)	O
n	O
4	O
:	O
5	O
:	O
6	O
:	O
ˆ	O
(	O
k	O
)	O
7	O
:	O
8	O
:	O
end	O
for	O
9	O
:	O
return	O
f	O
(	O
ˆx	O
)	O
=	O
sgn	O
(	O
cid:2	O
)	O
∑k	O
α	O
(	O
k	O
)	O
f	O
(	O
k	O
)	O
(	O
ˆx	O
)	O
(	O
cid:3	O
)	O
//	O
initialize	O
uniform	O
importance	O
to	O
each	O
example	O
//	O
train	O
kth	O
classiﬁer	O
on	O
weighted	O
data	O
//	O
make	O
predictions	O
on	O
training	B
data	I
//	O
compute	O
weighted	O
training	O
error	O
//	O
compute	O
“	O
adaptive	O
”	O
parameter	O
//	O
re-weight	O
examples	B
and	O
normalize	B
//	O
return	O
(	O
weighted	O
)	O
voted	O
classiﬁer	O
the	O
intuition	O
behind	O
adaboost	O
is	O
like	O
studying	O
for	O
an	O
exam	O
by	O
using	O
a	O
past	O
exam	O
.	O
you	O
take	O
the	O
past	O
exam	O
and	O
grade	O
yourself	O
.	O
the	O
questions	O
that	O
you	O
got	O
right	O
,	O
you	O
pay	O
less	O
attention	O
to	O
.	O
those	O
that	O
you	O
got	O
wrong	O
,	O
you	O
study	O
more	O
.	O
then	O
you	O
take	O
the	O
exam	O
again	O
and	O
repeat	O
this	O
process	O
.	O
you	O
continually	O
down-weight	O
the	O
importance	O
of	O
questions	O
you	O
routinely	O
answer	O
correctly	O
and	O
up-weight	O
the	O
importance	O
of	O
ques-	O
tions	O
you	O
routinely	O
answer	O
incorrectly	O
.	O
after	O
going	O
over	O
the	O
exam	O
multiple	O
times	O
,	O
you	O
hope	O
to	O
have	O
mastered	O
everything	O
.	O
the	O
precise	O
adaboost	O
training	O
algorithm	O
is	O
shown	O
in	O
algorithm	B
11.2.	O
the	O
basic	O
functioning	O
of	O
the	O
algorithm	O
is	O
to	O
maintain	O
a	O
weight	O
dis-	O
tribution	O
d	O
,	O
over	O
data	O
points	O
.	O
a	O
weak	B
learner	I
,	O
f	O
(	O
k	O
)	O
is	O
trained	O
on	O
this	O
weighted	O
data	O
.	O
(	O
note	O
that	O
we	O
implicitly	O
assume	O
that	O
our	O
weak	B
learner	I
can	O
accept	O
weighted	O
training	O
data	O
,	O
a	O
relatively	O
mild	O
assumption	O
that	O
is	O
nearly	O
always	O
true	O
.	O
)	O
the	O
(	O
weighted	O
)	O
error	B
rate	I
of	O
f	O
(	O
k	O
)	O
is	O
used	O
to	O
de-	O
termine	O
the	O
adaptive	O
parameter	O
α	O
,	O
which	O
controls	O
how	O
“	O
important	O
”	O
f	O
(	O
k	O
)	O
is	O
.	O
as	O
long	O
as	O
the	O
weak	O
learner	O
does	O
,	O
indeed	O
,	O
achieve	O
<	O
50	O
%	O
error	O
,	O
then	O
α	O
will	O
be	O
greater	O
than	O
zero	O
.	O
as	O
the	O
error	O
drops	O
to	O
zero	O
,	O
α	O
grows	O
without	O
bound	O
.	O
after	O
the	O
adaptive	O
parameter	O
is	O
computed	O
,	O
the	O
weight	O
distibution	O
is	O
updated	O
for	O
the	O
next	O
iteration	B
.	O
as	O
desired	O
,	O
examples	B
that	O
are	O
cor-	O
rectly	O
classiﬁed	O
(	O
for	O
which	O
yn	O
ˆyn	O
=	O
+1	O
)	O
have	O
their	O
weight	O
decreased	O
multiplicatively	O
.	O
examples	B
that	O
are	O
incorrectly	O
classiﬁed	O
(	O
yn	O
ˆyn	O
=	O
−1	O
)	O
have	O
their	O
weight	O
increased	O
multiplicatively	O
.	O
the	O
z	O
term	O
is	O
a	O
nom-	O
ralization	O
constant	O
to	O
ensure	O
that	O
the	O
sum	O
of	O
d	O
is	O
one	O
(	O
i.e.	O
,	O
d	O
can	O
be	O
interpreted	O
as	O
a	O
distribution	O
)	O
.	O
the	O
ﬁnal	O
classiﬁer	O
returned	O
by	O
ad-	O
aboost	O
is	O
a	O
weighted	O
vote	O
of	O
the	O
individual	O
classiﬁers	O
,	O
with	O
weights	B
given	O
by	O
the	O
adaptive	O
parameters	O
.	O
to	O
better	O
understand	O
why	O
α	O
is	O
deﬁned	O
as	O
it	O
is	O
,	O
suppose	O
that	O
our	O
weak	B
learner	I
simply	O
returns	O
a	O
constant	O
function	O
that	O
returns	O
the	O
(	O
weighted	O
)	O
majority	O
class	O
.	O
so	O
if	O
the	O
total	O
weight	O
of	O
positive	O
exam-	O
ples	O
exceeds	O
that	O
of	O
negative	O
examples	B
,	O
f	O
(	O
x	O
)	O
=	O
+1	O
for	O
all	O
x	O
;	O
otherwise	O
f	O
(	O
x	O
)	O
=	O
−1	O
for	O
all	O
x.	O
to	O
make	O
the	O
problem	O
moderately	O
interesting	O
,	O
suppose	O
that	O
in	O
the	O
original	O
training	O
set	O
,	O
there	O
are	O
80	O
positive	O
ex-	O
156	O
a	O
course	O
in	O
machine	O
learning	O
2	O
log	O
4	O
]	O
=	O
1	O
2	O
log	O
4	O
]	O
=	O
2.	O
we	O
can	O
compute	O
z	O
=	O
80×1	O
amples	O
and	O
20	O
negative	O
examples	B
.	O
in	O
this	O
case	O
,	O
f	O
(	O
1	O
)	O
(	O
x	O
)	O
=	O
+1	O
.	O
it	O
’	O
s	O
weighted	O
error	O
rate	O
will	O
be	O
ˆ	O
(	O
1	O
)	O
=	O
0.2	O
because	O
it	O
gets	O
every	O
negative	O
example	O
wrong	O
.	O
computing	O
,	O
we	O
get	O
α	O
(	O
1	O
)	O
=	O
1	O
2	O
log	O
4.	O
before	O
normaliza-	O
tion	O
,	O
we	O
get	O
the	O
new	O
weight	O
for	O
each	O
positive	O
(	O
correct	O
)	O
example	O
to	O
be	O
1	O
exp	O
[	O
−	O
1	O
2.	O
the	O
weight	O
for	O
each	O
negative	O
(	O
incorrect	O
)	O
example	O
2	O
+	O
20×2	O
=	O
80.	O
becomes	O
1	O
exp	O
[	O
1	O
therefore	O
,	O
after	O
normalization	O
,	O
the	O
weight	O
distribution	O
on	O
any	O
single	O
positive	O
example	O
is	O
1	O
160	O
and	O
the	O
weight	O
on	O
any	O
negative	O
example	O
is	O
1	O
40.	O
however	O
,	O
since	O
there	O
are	O
80	O
positive	O
examples	O
and	O
20	O
negative	O
exam-	O
ples	O
,	O
the	O
cumulative	O
weight	O
on	O
all	O
positive	O
examples	B
is	O
80×	O
1	O
160	O
=	O
1	O
2	O
;	O
the	O
cumulative	O
weight	O
on	O
all	O
negative	O
examples	B
is	O
20×	O
1	O
40	O
=	O
1	O
2.	O
thus	O
,	O
after	O
a	O
single	O
boosting	B
iteration	O
,	O
the	O
data	O
has	O
become	O
precisely	O
evenly	O
weighted	O
.	O
this	O
guarantees	O
that	O
in	O
the	O
next	O
iteration	B
,	O
our	O
weak	B
learner	I
must	O
do	O
something	O
more	O
interesting	O
than	O
majority	O
voting	B
if	O
it	O
is	O
to	O
achieve	O
an	O
error	B
rate	I
less	O
than	O
50	O
%	O
,	O
as	O
required	O
.	O
one	O
of	O
the	O
major	O
attractions	O
of	O
boosting	B
is	O
that	O
it	O
is	O
perhaps	O
easy	O
to	O
design	O
computationally	O
efﬁcient	O
weak	O
learners	O
.	O
a	O
very	O
popular	O
type	O
of	O
weak	B
learner	I
is	O
a	O
shallow	B
decision	I
tree	I
:	O
a	O
decision	B
tree	I
with	O
a	O
small	O
depth	O
limit	O
.	O
figure	O
11.3	O
shows	O
test	B
error	I
rates	O
for	O
decision	B
trees	I
of	O
different	O
maximum	O
depths	O
(	O
the	O
different	O
curves	O
)	O
run	O
for	O
differing	O
numbers	O
of	O
boosting	B
iterations	O
(	O
the	O
x-axis	O
)	O
.	O
as	O
you	O
can	O
see	O
,	O
if	O
you	O
are	O
willing	O
to	O
boost	O
for	O
many	O
iterations	O
,	O
very	O
shallow	O
trees	O
are	O
quite	O
effective	O
.	O
in	O
fact	O
,	O
a	O
very	O
popular	O
weak	B
learner	I
is	O
a	O
decision	O
decision	O
stump	O
:	O
a	O
decision	B
tree	I
that	O
can	O
only	O
ask	O
one	O
question	O
.	O
this	O
may	O
seem	O
like	O
a	O
silly	O
model	B
(	O
and	O
,	O
in	O
fact	O
,	O
it	O
is	O
on	O
it	O
’	O
s	O
own	O
)	O
,	O
but	O
when	O
combined	O
with	O
boosting	B
,	O
it	O
becomes	O
very	O
effective	O
.	O
to	O
understand	O
why	O
,	O
suppose	O
for	O
a	O
moment	O
that	O
our	O
data	O
consists	O
only	O
of	O
binary	B
features	I
,	O
so	O
that	O
any	O
question	O
that	O
a	O
decision	B
tree	I
might	O
ask	O
is	O
of	O
the	O
form	O
“	O
is	O
feature	O
5	O
on	O
?	O
”	O
by	O
concentrating	O
on	O
decision	O
stumps	O
,	O
all	O
weak	O
functions	O
must	O
have	O
the	O
form	O
f	O
(	O
x	O
)	O
=	O
s	O
(	O
2xd	O
−	O
1	O
)	O
,	O
where	O
s	O
∈	O
{	O
±1	O
}	O
and	O
d	O
indexes	O
some	O
feature	O
.	O
now	O
,	O
consider	O
the	O
ﬁnal	O
form	O
of	O
a	O
function	O
learned	O
by	O
adaboost	O
.	O
?	O
this	O
example	O
uses	O
concrete	O
num-	O
bers	O
,	O
but	O
the	O
same	O
result	O
holds	O
no	O
matter	O
what	O
the	O
data	O
distribution	O
looks	O
like	O
nor	O
how	O
many	O
examples	B
there	O
are	O
.	O
write	O
out	O
the	O
general	O
case	O
to	O
see	O
that	O
you	O
will	O
still	O
arrive	O
at	O
an	O
even	O
weighting	O
after	O
one	O
iteration	O
.	O
we	O
can	O
expand	O
it	O
as	O
follow	O
,	O
where	O
we	O
let	O
fk	O
denote	O
the	O
single	O
feature	O
selected	O
by	O
the	O
kth	O
decision	B
stump	I
and	O
let	O
sk	O
denote	O
its	O
sign	B
:	O
(	O
cid:35	O
)	O
(	O
cid:35	O
)	O
(	O
cid:35	O
)	O
f	O
(	O
x	O
)	O
=	O
sgn	O
=	O
sgn	O
αk	O
f	O
(	O
k	O
)	O
(	O
x	O
)	O
αksk	O
(	O
2x	O
fk	O
−	O
1	O
)	O
2αkskx	O
fk	O
−	O
∑	O
αksk	O
k	O
=	O
sgn	O
=	O
sgn	O
[	O
w	O
·	O
x	O
+	O
b	O
]	O
(	O
cid:34	O
)	O
(	O
cid:34	O
)	O
(	O
cid:34	O
)	O
∑	O
k	O
∑	O
k	O
∑	O
k	O
figure	O
11.3	O
:	O
perf	O
comparison	O
of	O
depth	O
vs	O
#	O
boost	O
?	O
why	O
do	O
the	O
functions	O
have	O
this	O
form	O
?	O
(	O
11.1	O
)	O
(	O
11.2	O
)	O
(	O
11.3	O
)	O
(	O
11.4	O
)	O
ensemble	B
methods	O
157	O
algorithm	B
32	O
randomforesttrain	O
(	O
d	O
,	O
depth	O
,	O
k	O
)	O
1	O
:	O
for	O
k	O
=	O
1	O
.	O
.	O
.	O
k	O
do	O
2	O
:	O
t	O
(	O
k	O
)	O
←	O
complete	O
binary	O
tree	O
of	O
depth	O
depth	O
with	O
random	O
feature	O
splits	O
f	O
(	O
k	O
)	O
←	O
the	O
function	O
computed	O
by	O
t	O
(	O
k	O
)	O
,	O
with	O
leaves	O
ﬁlled	O
in	O
by	O
d	O
3	O
:	O
4	O
:	O
end	O
for	O
5	O
:	O
return	O
f	O
(	O
ˆx	O
)	O
=	O
sgn	O
(	O
cid:2	O
)	O
∑k	O
f	O
(	O
k	O
)	O
(	O
ˆx	O
)	O
(	O
cid:3	O
)	O
//	O
return	O
voted	O
classiﬁer	O
where	O
wd	O
=	O
∑	O
k	O
:	O
fk=d	O
2αksk	O
and	O
b	O
=	O
−	O
∑	O
k	O
αksk	O
(	O
11.5	O
)	O
thus	O
,	O
when	O
working	O
with	O
decision	O
stumps	O
,	O
adaboost	O
actually	O
pro-	O
vides	O
an	O
algorithm	B
for	O
learning	O
linear	O
classiﬁers	O
!	O
in	O
fact	O
,	O
this	O
con-	O
nection	O
has	O
recently	O
been	O
strengthened	O
:	O
you	O
can	O
show	O
that	O
adaboost	O
provides	O
an	O
algorithm	B
for	O
optimizing	O
exponential	B
loss	I
.	O
(	O
however	O
,	O
this	O
connection	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
.	O
)	O
as	O
a	O
further	O
example	O
,	O
consider	O
the	O
case	O
of	O
boosting	B
a	O
linear	O
classi-	O
ﬁer	O
.	O
in	O
this	O
case	O
,	O
if	O
we	O
let	O
the	O
kth	O
weak	O
classiﬁer	O
be	O
parameterized	O
by	O
w	O
(	O
k	O
)	O
and	O
b	O
(	O
k	O
)	O
,	O
the	O
overall	O
predictor	O
will	O
have	O
the	O
form	O
:	O
(	O
11.6	O
)	O
(	O
cid:34	O
)	O
(	O
cid:16	O
)	O
w	O
(	O
k	O
)	O
·	O
x	O
+	O
b	O
(	O
k	O
)	O
(	O
cid:17	O
)	O
(	O
cid:35	O
)	O
f	O
(	O
x	O
)	O
=	O
sgn	O
∑	O
k	O
αksgn	O
you	O
can	O
notice	O
that	O
this	O
is	O
nothing	O
but	O
a	O
two-layer	O
neural	O
network	O
,	O
with	O
k-many	O
hidden	B
units	I
!	O
of	O
course	O
it	O
’	O
s	O
not	O
a	O
classiﬁcally	O
trained	O
neural	B
network	I
(	O
once	O
you	O
learn	O
w	O
(	O
k	O
)	O
you	O
never	O
go	O
back	O
and	O
update	O
it	O
)	O
,	O
but	O
the	O
structure	O
is	O
identical	O
.	O
11.3	O
random	O
ensembles	O
one	O
of	O
the	O
most	O
computationally	O
expensive	O
aspects	O
of	O
ensembles	O
of	O
decision	B
trees	I
is	O
training	O
the	O
decision	B
trees	I
.	O
this	O
is	O
very	O
fast	O
for	O
de-	O
cision	O
stumps	O
,	O
but	O
for	O
deeper	O
trees	O
it	O
can	O
be	O
prohibitively	O
expensive	O
.	O
the	O
expensive	O
part	O
is	O
choosing	O
the	O
tree	O
structure	O
.	O
once	O
the	O
tree	O
struc-	O
ture	O
is	O
chosen	O
,	O
it	O
is	O
very	O
cheap	O
to	O
ﬁll	O
in	O
the	O
leaves	O
(	O
i.e.	O
,	O
the	O
predictions	O
of	O
the	O
trees	O
)	O
using	O
the	O
training	O
data	O
.	O
an	O
efﬁcient	O
and	O
surprisingly	O
effective	O
alternative	O
is	O
to	O
use	O
trees	O
with	O
ﬁxed	O
structures	O
and	O
random	O
features	O
.	O
collections	O
of	O
trees	O
are	O
called	O
forests	O
,	O
and	O
so	O
classiﬁers	O
built	O
like	O
this	O
are	O
called	O
random	B
forests	I
.	O
the	O
random	O
forest	O
training	O
algorithm	O
,	O
shown	O
in	O
algo-	O
rithm	O
11.3	O
is	O
quite	O
short	O
.	O
it	O
takes	O
three	O
arguments	O
:	O
the	O
data	O
,	O
a	O
desired	O
depth	O
of	O
the	O
decision	O
trees	O
,	O
and	O
a	O
number	O
k	O
of	O
total	O
decision	B
trees	I
to	O
build	O
.	O
the	O
algorithm	O
generates	O
each	O
of	O
the	O
k	O
trees	O
independently	B
,	O
which	O
makes	O
it	O
very	O
easy	O
to	O
parallelize	O
.	O
for	O
each	O
trees	O
,	O
it	O
constructs	O
a	O
full	O
binary	O
tree	O
of	O
depth	O
depth	O
.	O
the	O
features	O
used	O
at	O
the	O
branches	O
of	O
this	O
158	O
a	O
course	O
in	O
machine	O
learning	O
tree	O
are	O
selected	O
randomly	O
,	O
typically	O
with	O
replacement	O
,	O
meaning	O
that	O
the	O
same	O
feature	O
can	O
appear	O
multiple	O
times	O
,	O
even	O
in	O
one	O
branch	O
.	O
the	O
leaves	O
of	O
this	O
tree	O
,	O
where	O
predictions	O
are	O
made	O
,	O
are	O
ﬁlled	O
in	O
based	O
on	O
the	O
training	O
data	O
.	O
this	O
last	O
step	O
is	O
the	O
only	O
point	O
at	O
which	O
the	O
training	O
data	O
is	O
used	O
.	O
the	O
resulting	O
classiﬁer	O
is	O
then	O
just	O
a	O
voting	B
of	O
the	O
k-	O
many	O
random	O
trees	O
.	O
the	O
most	O
amazing	O
thing	O
about	O
this	O
approach	O
is	O
that	O
it	O
actually	O
works	O
remarkably	O
well	O
.	O
it	O
tends	O
to	O
work	O
best	O
when	O
all	O
of	O
the	O
features	O
are	O
at	O
least	O
marginally	O
relevant	O
,	O
since	O
the	O
number	O
of	O
features	B
selected	O
for	O
any	O
given	O
tree	O
is	O
small	O
.	O
an	O
intuitive	O
reason	O
that	O
it	O
works	O
well	O
is	O
the	O
following	O
.	O
some	O
of	O
the	O
trees	O
will	O
query	O
on	O
useless	O
features	B
.	O
these	O
trees	O
will	O
essentially	O
make	O
random	O
predictions	O
.	O
but	O
some	O
of	O
the	O
trees	O
will	O
happen	O
to	O
query	O
on	O
good	O
features	B
and	O
will	O
make	O
good	O
predictions	O
(	O
because	O
the	O
leaves	O
are	O
estimated	O
based	O
on	O
the	O
training	O
data	O
)	O
.	O
if	O
you	O
have	O
enough	O
trees	O
,	O
the	O
random	O
ones	O
will	O
wash	O
out	O
as	O
noise	B
,	O
and	O
only	O
the	O
good	O
trees	O
will	O
have	O
an	O
effect	O
on	O
the	O
ﬁnal	O
classiﬁcation	O
.	O
11.4	O
exercises	O
exercise	O
11.1.	O
todo	O
.	O
.	O
.	O
12	O
|	O
efficient	O
learning	O
learning	O
objectives	O
:	O
•	O
understand	O
and	O
be	O
able	O
to	O
imple-	O
ment	O
stochastic	B
gradient	I
descent	I
algorithms	O
.	O
•	O
compare	O
and	O
contrast	O
small	O
ver-	O
sus	O
large	O
batch	B
sizes	O
in	O
stochastic	B
optimization	I
.	O
•	O
derive	O
subgradients	O
for	O
sparse	B
regularizers	O
.	O
•	O
implement	O
feature	O
hashing	O
.	O
dependencies	O
:	O
–	O
so	O
far	O
,	O
our	O
focus	O
has	O
been	O
on	O
models	O
of	O
learning	O
and	O
basic	O
al-	O
gorithms	O
for	O
those	O
models	O
.	O
we	O
have	O
not	O
placed	O
much	O
emphasis	O
on	O
how	O
to	O
learn	O
quickly	O
.	O
the	O
basic	O
techniques	O
you	O
learned	O
about	O
so	O
far	O
are	O
enough	O
to	O
get	O
learning	O
algorithms	O
running	O
on	O
tens	O
or	O
hundreds	O
of	O
thousands	O
of	O
examples	B
.	O
but	O
if	O
you	O
want	O
to	O
build	O
an	O
algorithm	B
for	O
web	O
page	O
ranking	O
,	O
you	O
will	O
need	O
to	O
deal	O
with	O
millions	O
or	O
billions	O
of	O
examples	B
,	O
in	O
hundreds	O
of	O
thousands	O
of	O
dimensions	O
.	O
the	O
basic	O
approaches	O
you	O
have	O
seen	O
so	O
far	O
are	O
insufﬁcient	O
to	O
achieve	O
such	O
a	O
massive	O
scale	O
.	O
in	O
this	O
chapter	O
,	O
you	O
will	O
learn	O
some	O
techniques	O
for	O
scaling	O
learning	O
algorithms	O
.	O
this	O
are	O
useful	O
even	O
when	O
you	O
do	O
not	O
have	O
billions	O
of	O
training	O
examples	O
,	O
because	O
it	O
’	O
s	O
always	O
nice	O
to	O
have	O
a	O
program	O
that	O
runs	O
quickly	O
.	O
you	O
will	O
see	O
techniques	O
for	O
speeding	O
up	O
both	O
model	B
training	O
and	O
model	B
prediction	O
.	O
the	O
focus	O
in	O
this	O
chapter	O
is	O
on	O
linear	O
models	O
(	O
for	O
simplicity	O
)	O
,	O
but	O
most	O
of	O
what	O
you	O
will	O
learn	O
applies	O
more	O
generally	O
.	O
12.1	O
what	O
does	O
it	O
mean	O
to	O
be	O
fast	O
?	O
everyone	O
always	O
wants	O
fast	O
algorithms	O
.	O
in	O
the	O
context	O
of	O
machine	O
learning	O
,	O
this	O
can	O
mean	O
many	O
things	O
.	O
you	O
might	O
want	O
fast	O
training	O
algorithms	O
,	O
or	O
perhaps	O
training	O
algorithms	O
that	O
scale	O
to	O
very	O
large	O
data	O
sets	O
(	O
for	O
instance	O
,	O
ones	O
that	O
will	O
not	O
ﬁt	O
in	O
main	O
memory	O
)	O
.	O
you	O
might	O
want	O
training	O
algorithms	O
that	O
can	O
be	O
easily	O
parallelized	O
.	O
or	O
,	O
you	O
might	O
not	O
care	O
about	O
training	O
efﬁciency	O
,	O
since	O
it	O
is	O
an	O
ofﬂine	O
process	O
,	O
and	O
only	O
care	O
about	O
how	O
quickly	O
your	O
learned	O
functions	O
can	O
make	O
classiﬁcation	O
decisions	O
.	O
it	O
is	O
important	O
to	O
separate	O
out	O
these	O
desires	O
.	O
if	O
you	O
care	O
about	O
efﬁciency	O
at	O
training	O
time	O
,	O
then	O
what	O
you	O
are	O
really	O
asking	O
for	O
are	O
more	O
efﬁcient	O
learning	O
algorithms	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
you	O
care	O
about	O
efﬁciency	O
at	O
test	O
time	O
,	O
then	O
you	O
are	O
asking	O
for	O
models	O
that	O
can	O
be	O
quickly	O
evaluated	O
.	O
one	O
issue	O
that	O
is	O
not	O
covered	O
in	O
this	O
chapter	O
is	O
parallel	O
learning	O
.	O
160	O
a	O
course	O
in	O
machine	O
learning	O
this	O
is	O
largely	O
because	O
it	O
is	O
currently	O
not	O
a	O
well-understood	O
area	O
in	O
machine	O
learning	O
.	O
there	O
are	O
many	O
aspects	O
of	O
parallelism	O
that	O
come	O
into	O
play	O
,	O
such	O
as	O
the	O
speed	O
of	O
communication	O
across	O
the	O
network	O
,	O
whether	O
you	O
have	O
shared	O
memory	O
,	O
etc	O
.	O
right	O
now	O
,	O
this	O
the	O
general	O
,	O
poor-man	O
’	O
s	O
approach	O
to	O
parallelization	O
,	O
is	O
to	O
employ	O
ensembles	O
.	O
12.2	O
stochastic	B
optimization	I
during	O
training	O
of	O
most	O
learning	O
algorithms	O
,	O
you	O
consider	O
the	O
entire	O
data	O
set	O
simultaneously	O
.	O
this	O
is	O
certainly	O
true	O
of	O
gradient	B
descent	I
algorithms	O
for	O
regularized	O
linear	O
classiﬁers	O
(	O
recall	B
algorithm	O
6.4	O
)	O
,	O
in	O
which	O
you	O
ﬁrst	O
compute	O
a	O
gradient	B
over	O
the	O
entire	O
training	B
data	I
(	O
for	O
simplicity	O
,	O
consider	O
the	O
unbiased	O
case	O
)	O
:	O
g	O
=	O
∑	O
n	O
∇w	O
(	O
cid:96	O
)	O
(	O
yn	O
,	O
w	O
·	O
xn	O
)	O
+	O
λw	O
(	O
12.1	O
)	O
where	O
(	O
cid:96	O
)	O
(	O
y	O
,	O
ˆy	O
)	O
is	O
some	O
loss	B
function	I
.	O
then	O
you	O
update	O
the	O
weights	O
by	O
w	O
←	O
w	O
−	O
ηg	O
.	O
in	O
this	O
algorithm	B
,	O
in	O
order	O
to	O
make	O
a	O
single	O
update	O
,	O
you	O
have	O
to	O
look	O
at	O
every	O
training	O
example	O
.	O
when	O
there	O
are	O
billions	O
of	O
training	O
examples	O
,	O
it	O
is	O
a	O
bit	O
silly	O
to	O
look	O
at	O
every	O
one	O
before	O
doing	O
anything	O
.	O
perhaps	O
just	O
on	O
the	O
basis	O
of	O
the	O
ﬁrst	O
few	O
examples	B
,	O
you	O
can	O
already	O
start	O
learning	O
something	O
!	O
stochastic	B
optimization	I
involves	O
thinking	O
of	O
your	O
training	B
data	I
as	O
a	O
big	O
distribution	O
over	O
examples	B
.	O
a	O
draw	O
from	O
this	O
distribution	O
corresponds	O
to	O
picking	O
some	O
example	O
(	O
uniformly	O
at	O
random	O
)	O
from	O
your	O
data	O
set	O
.	O
viewed	O
this	O
way	O
,	O
the	O
optimization	O
problem	O
becomes	O
a	O
stochastic	B
optimization	I
problem	O
,	O
because	O
you	O
are	O
trying	O
to	O
optimize	O
some	O
function	O
(	O
say	O
,	O
a	O
regularized	O
linear	O
classiﬁer	O
)	O
over	O
a	O
probability	O
distribution	O
.	O
you	O
can	O
derive	O
this	O
intepretation	O
directly	O
as	O
follows	O
:	O
w∗	O
=	O
arg	O
max	O
w	O
=	O
arg	O
max	O
w	O
=	O
arg	O
max	O
w	O
∑	O
n	O
∑	O
n	O
∑	O
n	O
=	O
arg	O
max	O
w	O
e	O
(	O
y	O
,	O
x	O
)	O
∼d	O
(	O
cid:21	O
)	O
r	O
(	O
w	O
)	O
(	O
cid:96	O
)	O
(	O
yn	O
,	O
w	O
·	O
xn	O
)	O
+	O
r	O
(	O
w	O
)	O
(	O
cid:20	O
)	O
(	O
cid:20	O
)	O
1	O
(	O
cid:96	O
)	O
(	O
yn	O
,	O
w	O
·	O
xn	O
)	O
+	O
1	O
n	O
(	O
cid:96	O
)	O
(	O
yn	O
,	O
w	O
·	O
xn	O
)	O
+	O
(	O
cid:20	O
)	O
(	O
cid:96	O
)	O
(	O
y	O
,	O
w	O
·	O
x	O
)	O
+	O
n	O
(	O
cid:21	O
)	O
(	O
cid:21	O
)	O
1	O
n2	O
r	O
(	O
w	O
)	O
1	O
n	O
r	O
(	O
w	O
)	O
deﬁnition	O
(	O
12.2	O
)	O
move	O
r	O
inside	O
sum	O
(	O
12.3	O
)	O
divide	O
through	O
by	O
n	O
(	O
12.4	O
)	O
write	O
as	O
expectation	O
(	O
12.5	O
)	O
(	O
12.6	O
)	O
where	O
d	O
is	O
the	O
training	O
data	O
distribution	O
given	O
this	O
framework	O
,	O
you	O
have	O
the	O
following	O
general	O
form	O
of	O
an	O
efficient	O
learning	O
161	O
algorithm	B
33	O
stochasticgradientdescent	O
(	O
f	O
,	O
d	O
,	O
s	O
,	O
k	O
,	O
η1	O
,	O
.	O
.	O
.	O
)	O
1	O
:	O
z	O
(	O
0	O
)	O
←	O
(	O
cid:104	O
)	O
0	O
,	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
(	O
cid:105	O
)	O
2	O
:	O
for	O
k	O
=	O
1	O
.	O
.	O
.	O
k	O
do	O
3	O
:	O
//	O
initialize	O
variable	O
we	O
are	O
optimizing	O
g	O
(	O
k	O
)	O
←	O
∇zf	O
(	O
d	O
(	O
k	O
)	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
z	O
(	O
k-1	O
)	O
d	O
(	O
k	O
)	O
←	O
s-many	O
random	O
data	O
points	O
from	O
d	O
z	O
(	O
k	O
)	O
←	O
z	O
(	O
k-1	O
)	O
−	O
η	O
(	O
k	O
)	O
g	O
(	O
k	O
)	O
//	O
compute	O
gradient	B
on	O
sample	O
//	O
take	O
a	O
step	O
down	O
the	O
gradient	O
4	O
:	O
5	O
:	O
6	O
:	O
end	O
for	O
7	O
:	O
return	O
z	O
(	O
k	O
)	O
optimization	B
problem	I
:	O
ζ	O
[	O
f	O
(	O
z	O
,	O
ζ	O
)	O
]	O
min	O
e	O
z	O
(	O
12.7	O
)	O
in	O
the	O
example	O
,	O
ζ	O
denotes	O
the	O
random	O
choice	O
of	O
examples	B
over	O
the	O
dataset	O
,	O
z	O
denotes	O
the	O
weight	O
vector	B
and	O
f	O
(	O
w	O
,	O
ζ	O
)	O
denotes	O
the	O
loss	O
on	O
that	O
example	O
plus	O
a	O
fraction	O
of	O
the	O
regularizer	O
.	O
stochastic	B
optimization	I
problems	O
are	O
formally	O
harder	O
than	O
regu-	O
lar	O
(	O
deterministic	O
)	O
optimization	O
problems	O
because	O
you	O
do	O
not	O
even	O
get	O
access	O
to	O
exact	O
function	O
values	O
and	O
gradients	O
.	O
the	O
only	O
access	O
you	O
have	O
to	O
the	O
function	O
f	O
that	O
you	O
wish	O
to	O
optimize	O
are	O
noisy	O
mea-	O
surements	O
,	O
governed	O
by	O
the	O
distribution	O
over	O
ζ.	O
despite	O
this	O
lack	O
of	O
information	O
,	O
you	O
can	O
still	O
run	O
a	O
gradient-based	O
algorithm	B
,	O
where	O
you	O
simply	O
compute	O
local	O
gradients	O
on	O
a	O
current	O
sample	O
of	O
data	O
.	O
more	O
precisely	O
,	O
you	O
can	O
draw	O
a	O
data	O
point	O
at	O
random	O
from	O
your	O
data	O
set	O
.	O
this	O
is	O
analogous	O
to	O
drawing	O
a	O
single	O
value	O
ζ	O
from	O
its	O
distribution	O
.	O
you	O
can	O
compute	O
the	O
gradient	O
of	O
f	O
just	O
at	O
that	O
point	O
.	O
in	O
this	O
case	O
of	O
a	O
2-norm	O
regularized	O
linear	O
model	B
,	O
this	O
is	O
simply	O
g	O
=	O
∇w	O
(	O
cid:96	O
)	O
(	O
y	O
,	O
w	O
·	O
x	O
)	O
+	O
1	O
n	O
w	O
,	O
where	O
(	O
y	O
,	O
x	O
)	O
is	O
the	O
random	O
point	O
you	O
selected	O
.	O
given	O
this	O
estimate	O
of	O
the	O
gradient	O
(	O
it	O
’	O
s	O
an	O
estimate	O
because	O
it	O
’	O
s	O
based	O
on	O
a	O
single	O
random	O
draw	O
)	O
,	O
you	O
can	O
take	O
a	O
small	O
gradient	B
step	O
w	O
←	O
w	O
−	O
ηg	O
.	O
this	O
is	O
the	O
stochastic	O
gradient	B
descent	I
algorithm	O
(	O
sgd	O
)	O
.	O
in	O
prac-	O
tice	O
,	O
taking	O
gradients	O
with	O
respect	O
to	O
a	O
single	O
data	O
point	O
might	O
be	O
too	O
myopic	O
.	O
in	O
such	O
cases	O
,	O
it	O
is	O
useful	O
to	O
use	O
a	O
small	O
batch	B
of	O
data	O
.	O
here	O
,	O
you	O
can	O
draw	O
10	O
random	O
examples	O
from	O
the	O
training	O
data	O
and	O
compute	O
a	O
small	O
gradient	B
(	O
estimate	O
)	O
based	O
on	O
those	O
examples	B
:	O
g	O
=	O
∑10	O
n	O
w	O
,	O
where	O
you	O
need	O
to	O
include	O
10	O
counts	O
of	O
the	O
regularizer	O
.	O
popular	O
batch	B
sizes	O
are	O
1	O
(	O
single	O
points	O
)	O
and	O
10.	O
the	O
generic	O
sgd	O
algorithm	B
is	O
depicted	O
in	O
algorithm	B
12.2	O
,	O
which	O
takes	O
k-many	O
steps	O
over	O
batches	O
of	O
s-many	O
examples	B
.	O
m=1	O
∇w	O
(	O
cid:96	O
)	O
(	O
ym	O
,	O
w	O
·	O
xm	O
)	O
+	O
10	O
in	O
stochastic	B
gradient	I
descent	I
,	O
it	O
is	O
imperative	O
to	O
choose	O
good	O
step	O
sizes	O
.	O
it	O
is	O
also	O
very	O
important	O
that	O
the	O
steps	O
get	O
smaller	O
over	O
time	O
at	O
a	O
reasonable	O
slow	O
rate	O
.	O
in	O
particular	O
,	O
convergence	O
can	O
be	O
guaranteed	O
for	O
learning	O
rates	O
of	O
the	O
form	O
:	O
η	O
(	O
k	O
)	O
=	O
η0√	O
k	O
,	O
where	O
η0	O
is	O
a	O
ﬁxed	O
,	O
initial	O
step	O
size	O
,	O
typically	O
0.01	O
,	O
0.1	O
or	O
1	O
depending	O
on	O
how	O
quickly	O
you	O
ex-	O
162	O
a	O
course	O
in	O
machine	O
learning	O
pect	O
the	O
algorithm	O
to	O
converge	O
.	O
unfortunately	O
,	O
in	O
comparisong	O
to	O
gradient	B
descent	I
,	O
stochastic	O
gradient	O
is	O
quite	O
sensitive	O
to	O
the	O
selection	O
of	O
a	O
good	O
learning	O
rate	O
.	O
there	O
is	O
one	O
more	O
practical	O
issues	O
related	O
to	O
the	O
use	O
of	O
sgd	O
as	O
a	O
learning	O
algorithm	B
:	O
do	O
you	O
really	O
select	O
a	O
random	O
point	O
(	O
or	O
subset	O
of	O
random	O
points	O
)	O
at	O
each	O
step	O
,	O
or	O
do	O
you	O
stream	O
through	O
the	O
data	O
in	O
order	O
.	O
the	O
answer	O
is	O
akin	O
to	O
the	O
answer	O
of	O
the	O
same	O
question	O
for	O
the	O
perceptron	O
algorithm	B
(	O
chapter	O
3	O
)	O
.	O
if	O
you	O
do	O
not	O
permute	O
your	O
data	O
at	O
all	O
,	O
very	O
bad	O
things	O
can	O
happen	O
.	O
if	O
you	O
do	O
permute	O
your	O
data	O
once	O
and	O
then	O
do	O
multiple	O
passes	O
over	O
that	O
same	O
permutation	O
,	O
it	O
will	O
converge	O
,	O
but	O
more	O
slowly	O
.	O
in	O
theory	O
,	O
you	O
really	O
should	O
permute	O
every	O
iteration	B
.	O
if	O
your	O
data	O
is	O
small	O
enough	O
to	O
ﬁt	O
in	O
memory	O
,	O
this	O
is	O
not	O
a	O
big	O
deal	O
:	O
you	O
will	O
only	O
pay	O
for	O
cache	O
misses	O
.	O
however	O
,	O
if	O
your	O
data	O
is	O
too	O
large	O
for	O
memory	O
and	O
resides	O
on	O
a	O
magnetic	O
disk	O
that	O
has	O
a	O
slow	O
seek	O
time	O
,	O
randomly	O
seeking	O
to	O
new	O
data	O
points	O
for	O
each	O
example	O
is	O
prohibitivly	O
slow	O
,	O
and	O
you	O
will	O
likely	O
need	O
to	O
forgo	O
permuting	O
the	O
data	O
.	O
the	O
speed	O
hit	O
in	O
convergence	O
speed	O
will	O
almost	O
certainly	O
be	O
recovered	O
by	O
the	O
speed	O
gain	O
in	O
not	O
having	O
to	O
seek	O
on	O
disk	O
routinely	O
.	O
(	O
note	O
that	O
the	O
story	O
is	O
very	O
different	O
for	O
solid	O
state	O
disks	O
,	O
on	O
which	O
random	O
accesses	O
really	O
are	O
quite	O
efﬁcient	O
.	O
)	O
12.3	O
sparse	B
regularization	O
for	O
many	O
learning	O
algorithms	O
,	O
the	O
test-time	O
efﬁciency	O
is	O
governed	O
by	O
how	O
many	O
features	B
are	O
used	O
for	O
prediction	O
.	O
this	O
is	O
one	O
reason	O
de-	O
cision	O
trees	O
tend	O
to	O
be	O
among	O
the	O
fastest	O
predictors	O
:	O
they	O
only	O
use	O
a	O
small	O
number	O
of	O
features	B
.	O
especially	O
in	O
cases	O
where	O
the	O
actual	O
com-	O
putation	O
of	O
these	O
features	B
is	O
expensive	O
,	O
cutting	O
down	O
on	O
the	O
number	O
that	O
are	O
used	O
at	O
test	O
time	O
can	O
yield	O
huge	O
gains	O
in	O
efﬁciency	O
.	O
moreover	O
,	O
the	O
amount	O
of	O
memory	O
used	O
to	O
make	O
predictions	O
is	O
also	O
typically	O
governed	O
by	O
the	O
number	O
of	O
features	B
.	O
(	O
note	O
:	O
this	O
is	O
not	O
true	O
of	O
kernel	B
methods	O
like	O
support	O
vector	O
machines	O
,	O
in	O
which	O
the	O
dominant	O
cost	O
is	O
the	O
number	O
of	O
support	B
vectors	I
.	O
)	O
furthermore	O
,	O
you	O
may	O
simply	O
believe	O
that	O
your	O
learning	O
problem	O
can	O
be	O
solved	O
with	O
a	O
very	O
small	O
number	O
of	O
features	B
:	O
this	O
is	O
a	O
very	O
reasonable	O
form	O
of	O
inductive	B
bias	I
.	O
this	O
is	O
the	O
idea	O
behind	O
sparse	B
models	O
,	O
and	O
in	O
particular	O
,	O
sparse	B
regularizers	O
.	O
one	O
of	O
the	O
disadvantages	O
of	O
a	O
2-norm	O
regularizer	B
for	O
linear	O
models	O
is	O
that	O
they	O
tend	O
to	O
never	O
produce	O
weights	B
that	O
are	O
exactly	O
zero	O
.	O
they	O
get	O
close	O
to	O
zero	O
,	O
but	O
never	O
hit	O
it	O
.	O
to	O
understand	O
why	O
,	O
as	O
a	O
weight	O
wd	O
approaches	O
zero	O
,	O
its	O
gradient	B
also	O
approaches	O
zero	O
.	O
thus	O
,	O
even	O
if	O
the	O
weight	O
should	O
be	O
zero	O
,	O
it	O
will	O
essentially	O
never	O
get	O
there	O
because	O
of	O
the	O
constantly	O
shrinking	O
gradient	B
.	O
this	O
suggests	O
that	O
an	O
alternative	O
regularizer	B
is	O
required	O
to	O
yield	O
a	O
sparse	B
inductive	O
bias	B
.	O
an	O
ideal	O
case	O
would	O
be	O
the	O
zero-norm	O
regular-	O
efficient	O
learning	O
163	O
izer	O
,	O
which	O
simply	O
counts	O
the	O
number	O
of	O
non-zero	O
values	O
in	O
a	O
vector	B
:	O
||w||0	O
=	O
∑d	O
[	O
wd	O
(	O
cid:54	O
)	O
=	O
0	O
]	O
.	O
if	O
you	O
could	O
minimize	O
this	O
regularizer	B
,	O
you	O
would	O
be	O
explicitly	O
minimizing	O
the	O
number	O
of	O
non-zero	O
features	B
.	O
un-	O
fortunately	O
,	O
not	O
only	O
is	O
the	O
zero-norm	O
non-convex	B
,	O
it	O
’	O
s	O
also	O
discrete	O
.	O
optimizing	O
it	O
is	O
np-hard	O
.	O
a	O
reasonable	O
middle-ground	O
is	O
the	O
one-norm	O
:	O
||w||1	O
=	O
∑d	O
|wd|	O
.	O
it	O
is	O
indeed	O
convex	B
:	O
in	O
fact	O
,	O
it	O
is	O
the	O
tighest	O
(	O
cid:96	O
)	O
p	O
norm	O
that	O
is	O
convex	B
.	O
moreover	O
,	O
its	O
gradients	O
do	O
not	O
go	O
to	O
zero	O
as	O
in	O
the	O
two-norm	O
.	O
just	O
as	O
hinge-loss	O
is	O
the	O
tightest	O
convex	B
upper	O
bound	O
on	O
zero-one	O
error	O
,	O
the	O
one-norm	O
is	O
the	O
tighest	O
convex	B
upper	O
bound	O
on	O
the	O
zero-norm	O
.	O
at	O
this	O
point	O
,	O
you	O
should	O
be	O
content	O
.	O
you	O
can	O
take	O
your	O
subgradi-	O
ent	O
optimizer	O
for	O
arbitrary	O
functions	O
and	O
plug	O
in	O
the	O
one-norm	O
as	O
a	O
regularizer	B
.	O
the	O
one-norm	O
is	O
surely	O
non-differentiable	O
at	O
wd	O
=	O
0	O
,	O
but	O
you	O
can	O
simply	O
choose	O
any	O
value	O
in	O
the	O
range	O
[	O
−1	O
,	O
+1	O
]	O
as	O
a	O
subgradi-	O
ent	O
at	O
that	O
point	O
.	O
(	O
you	O
should	O
choose	O
zero	O
.	O
)	O
unfortunately	O
,	O
this	O
does	O
not	O
quite	O
work	O
the	O
way	O
you	O
might	O
expect	O
.	O
the	O
issue	O
is	O
that	O
the	O
gradient	O
might	O
“	O
overstep	O
”	O
zero	O
and	O
you	O
will	O
never	O
end	O
up	O
with	O
a	O
solution	O
that	O
is	O
particularly	O
sparse	B
.	O
for	O
example	O
,	O
at	O
the	O
end	O
of	O
one	O
gradient	O
step	O
,	O
you	O
might	O
have	O
w3	O
=	O
0.6.	O
your	O
gradient	B
might	O
have	O
g6	O
=	O
0.8	O
and	O
your	O
gradient	B
step	O
(	O
assuming	O
η	O
=	O
1	O
)	O
will	O
update	O
so	O
that	O
the	O
new	O
w3	O
=	O
−0.2	O
.	O
in	O
the	O
subsequent	O
iteration	B
,	O
you	O
might	O
have	O
g6	O
=	O
−0.3	O
and	O
step	O
to	O
w3	O
=	O
0.1.	O
this	O
observation	O
leads	O
to	O
the	O
idea	O
of	O
trucated	B
gradients	I
.	O
the	O
idea	O
is	O
simple	O
:	O
if	O
you	O
have	O
a	O
gradient	B
that	O
would	O
step	O
you	O
over	O
wd	O
=	O
0	O
,	O
then	O
just	O
set	O
wd	O
=	O
0.	O
in	O
the	O
easy	O
case	O
when	O
the	O
learning	O
rate	O
is	O
1	O
,	O
this	O
means	O
that	O
if	O
the	O
sign	O
of	O
wd	O
−	O
gd	O
is	O
different	O
than	O
the	O
sign	O
of	O
wd	O
then	O
you	O
truncate	O
the	O
gradient	O
step	O
and	O
simply	O
set	O
wd	O
=	O
0.	O
in	O
other	O
words	O
,	O
gd	O
should	O
never	O
be	O
larger	O
than	O
wd	O
once	O
you	O
incorporate	O
learning	O
rates	O
,	O
you	O
can	O
express	O
this	O
as	O
:	O
	O
gd	O
gd	O
0	O
gd	O
←	O
if	O
wd	O
>	O
0	O
and	O
gd	O
≤	O
1	O
if	O
wd	O
<	O
0	O
and	O
gd	O
≥	O
1	O
otherwise	O
η	O
(	O
k	O
)	O
wd	O
η	O
(	O
k	O
)	O
wd	O
(	O
12.8	O
)	O
this	O
works	O
quite	O
well	O
in	O
the	O
case	O
of	O
subgradient	B
descent	I
.	O
it	O
works	O
somewhat	O
less	O
well	O
in	O
the	O
case	O
of	O
stochastic	O
subgradient	O
descent	O
.	O
the	O
problem	O
that	O
arises	O
in	O
the	O
stochastic	O
case	O
is	O
that	O
wherever	O
you	O
choose	O
to	O
stop	O
optimizing	O
,	O
you	O
will	O
have	O
just	O
touched	O
a	O
single	O
example	O
(	O
or	O
small	O
batch	B
of	O
examples	B
)	O
,	O
which	O
will	O
increase	O
the	O
weights	O
for	O
a	O
lot	O
of	O
features	B
,	O
before	O
the	O
regularizer	O
“	O
has	O
time	O
”	O
to	O
shrink	O
them	O
back	O
down	O
to	O
zero	O
.	O
you	O
will	O
still	O
end	O
up	O
with	O
somewhat	O
sparse	B
solutions	O
,	O
but	O
not	O
as	O
sparse	B
as	O
they	O
could	O
be	O
.	O
there	O
are	O
algorithms	O
for	O
dealing	O
with	O
this	O
situation	O
,	O
but	O
they	O
all	O
have	O
a	O
heuristic	O
ﬂavor	O
to	O
them	O
and	O
are	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
.	O
164	O
a	O
course	O
in	O
machine	O
learning	O
12.4	O
feature	O
hashing	O
as	O
much	O
as	O
speed	O
is	O
a	O
bottleneck	O
in	O
prediction	O
,	O
so	O
often	O
is	O
memory	O
usage	O
.	O
if	O
you	O
have	O
a	O
very	O
large	O
number	O
of	O
features	B
,	O
the	O
amount	O
of	O
memory	O
that	O
it	O
takes	O
to	O
store	O
weights	B
for	O
all	O
of	O
them	O
can	O
become	O
prohibitive	O
,	O
especially	O
if	O
you	O
wish	O
to	O
run	O
your	O
algorithm	B
on	O
small	O
de-	O
vices	O
.	O
feature	O
hashing	O
is	O
an	O
incredibly	O
simple	O
technique	O
for	O
reducing	O
the	O
memory	O
footprint	O
of	O
linear	O
models	O
,	O
with	O
very	O
small	O
sacriﬁces	O
in	O
accuracy	O
.	O
the	O
basic	O
idea	O
is	O
to	O
replace	O
all	O
of	O
your	O
features	B
with	O
hashed	O
ver-	O
sions	O
of	O
those	O
features	B
,	O
thus	O
reducing	O
your	O
space	O
from	O
d-many	O
fea-	O
ture	O
weights	B
to	O
p-many	O
feature	O
weights	O
,	O
where	O
p	O
is	O
the	O
range	O
of	O
the	O
hash	O
function	O
.	O
you	O
can	O
actually	O
think	O
of	O
hashing	O
as	O
a	O
(	O
random-	O
ized	O
)	O
feature	B
mapping	I
φ	O
:	O
rd	O
→	O
rp	O
,	O
for	O
some	O
p	O
(	O
cid:28	O
)	O
d.	O
the	O
idea	O
is	O
as	O
follows	O
.	O
first	O
,	O
you	O
choose	O
a	O
hash	O
function	O
h	O
whose	O
domain	O
is	O
[	O
d	O
]	O
=	O
{	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
d	O
}	O
and	O
whose	O
range	O
is	O
[	O
p	O
]	O
.	O
then	O
,	O
when	O
you	O
receive	O
a	O
feature	B
vector	I
x	O
∈	O
rd	O
,	O
you	O
map	O
it	O
to	O
a	O
shorter	O
feature	B
vector	I
ˆx	O
∈	O
rp	O
.	O
algorithmically	O
,	O
you	O
can	O
think	O
of	O
this	O
mapping	O
as	O
follows	O
:	O
1.	O
initialize	O
ˆx	O
=	O
(	O
cid:104	O
)	O
0	O
,	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
(	O
cid:105	O
)	O
2.	O
for	O
each	O
d	O
=	O
1	O
.	O
.	O
.	O
d	O
:	O
(	O
a	O
)	O
hash	O
d	O
to	O
position	O
p	O
=	O
h	O
(	O
d	O
)	O
(	O
b	O
)	O
update	O
the	O
pth	O
position	O
by	O
adding	O
xd	O
:	O
ˆxp	O
←	O
ˆxp	O
+	O
xd	O
3.	O
return	O
ˆx	O
mathematically	O
,	O
the	O
mapping	O
looks	O
like	O
:	O
φ	O
(	O
x	O
)	O
p	O
=	O
∑	O
d	O
[	O
h	O
(	O
d	O
)	O
=	O
p	O
]	O
xd	O
=	O
∑	O
d∈h−1	O
(	O
p	O
)	O
xd	O
where	O
h−1	O
(	O
p	O
)	O
=	O
{	O
d	O
:	O
h	O
(	O
d	O
)	O
=	O
p	O
}	O
.	O
(	O
12.9	O
)	O
in	O
the	O
(	O
unrealistic	O
)	O
case	O
where	O
p	O
=	O
d	O
and	O
h	O
simply	O
encodes	O
a	O
per-	O
mutation	O
,	O
then	O
this	O
mapping	O
does	O
not	O
change	O
the	O
learning	O
problem	O
at	O
all	O
.	O
all	O
it	O
does	O
is	O
rename	O
all	O
of	O
the	O
features	O
.	O
in	O
practice	O
,	O
p	O
(	O
cid:28	O
)	O
d	O
and	O
there	O
will	O
be	O
collisions	O
.	O
in	O
this	O
context	O
,	O
a	O
collision	O
means	O
that	O
two	O
features	B
,	O
which	O
are	O
really	O
different	O
,	O
end	O
up	O
looking	O
the	O
same	O
to	O
the	O
learning	O
algorithm	B
.	O
for	O
instance	O
,	O
“	O
is	O
it	O
sunny	O
today	O
?	O
”	O
and	O
“	O
did	O
my	O
favorite	O
sports	O
team	O
win	O
last	O
night	O
?	O
”	O
might	O
get	O
mapped	O
to	O
the	O
same	O
location	O
after	O
hashing	O
.	O
the	O
hope	O
is	O
that	O
the	O
learning	O
algorithm	B
is	O
sufﬁciently	O
robust	O
to	O
noise	B
that	O
it	O
can	O
handle	O
this	O
case	O
well	O
.	O
consider	O
the	O
kernel	O
deﬁned	O
by	O
this	O
hash	O
mapping	O
.	O
namely	O
:	O
k	O
(	O
hash	O
)	O
(	O
x	O
,	O
z	O
)	O
=	O
φ	O
(	O
x	O
)	O
·	O
φ	O
(	O
z	O
)	O
(	O
cid:33	O
)	O
(	O
cid:32	O
)	O
∑	O
d	O
,	O
e	O
=	O
∑	O
p	O
=	O
∑	O
p	O
=	O
∑	O
d	O
e∈h−1	O
(	O
h	O
(	O
d	O
)	O
)	O
=	O
x	O
·	O
z	O
+	O
∑	O
d	O
xdze	O
∑	O
e	O
(	O
cid:54	O
)	O
=d	O
,	O
e∈h−1	O
(	O
h	O
(	O
d	O
)	O
)	O
[	O
h	O
(	O
d	O
)	O
=	O
p	O
]	O
zd	O
∑	O
d	O
[	O
h	O
(	O
d	O
)	O
=	O
p	O
]	O
xd	O
∑	O
d	O
[	O
h	O
(	O
d	O
)	O
=	O
p	O
]	O
[	O
h	O
(	O
e	O
)	O
=	O
p	O
]	O
xdze	O
∑	O
(	O
cid:33	O
)	O
(	O
cid:32	O
)	O
xdze	O
efficient	O
learning	O
165	O
(	O
12.10	O
)	O
(	O
12.11	O
)	O
(	O
12.12	O
)	O
(	O
12.13	O
)	O
(	O
12.14	O
)	O
this	O
hash	B
kernel	I
has	O
the	O
form	O
of	O
a	O
linear	O
kernel	O
plus	O
a	O
small	O
number	O
of	O
quadratic	O
terms	O
.	O
the	O
particular	O
quadratic	O
terms	O
are	O
exactly	O
those	O
given	O
by	O
collisions	O
of	O
the	O
hash	O
function	O
.	O
there	O
are	O
two	O
things	O
to	O
notice	O
about	O
this	O
.	O
the	O
ﬁrst	O
is	O
that	O
collisions	O
might	O
not	O
actually	O
be	O
bad	O
things	O
!	O
in	O
a	O
sense	O
,	O
they	O
’	O
re	O
giving	O
you	O
a	O
little	O
extra	O
representational	O
power	O
.	O
in	O
particular	O
,	O
if	O
the	O
hash	O
function	O
happens	O
to	O
select	O
out	O
feature	O
pairs	O
that	O
beneﬁt	O
from	O
being	O
paired	O
,	O
then	O
you	O
now	O
have	O
a	O
better	O
representation	O
.	O
the	O
second	O
is	O
that	O
even	O
if	O
this	O
doesn	O
’	O
t	O
happen	O
,	O
the	O
quadratic	O
term	O
in	O
the	O
kernel	O
has	O
only	O
a	O
small	O
effect	O
on	O
the	O
overall	O
prediction	O
.	O
in	O
particular	O
,	O
if	O
you	O
assume	O
that	O
your	O
hash	O
function	O
is	O
pairwise	O
independent	O
(	O
a	O
common	O
assumption	O
of	O
hash	O
functions	O
)	O
,	O
then	O
the	O
expected	O
value	O
of	O
this	O
quadratic	O
term	O
is	O
zero	O
,	O
and	O
its	O
variance	B
decreases	O
at	O
a	O
rate	O
of	O
o	O
(	O
p−2	O
)	O
.	O
in	O
other	O
words	O
,	O
if	O
you	O
choose	O
p	O
≈	O
100	O
,	O
then	O
the	O
variance	O
is	O
on	O
the	O
order	O
of	O
0.0001	O
.	O
12.5	O
exercises	O
exercise	O
12.1.	O
todo	O
.	O
.	O
.	O
13	O
|	O
unsupervised	B
learning	I
learning	O
objectives	O
:	O
•	O
explain	O
the	O
difference	O
between	O
linear	O
and	O
non-linear	B
dimensionality	O
reduction	O
.	O
•	O
relate	O
the	O
view	O
of	O
pca	O
as	O
maximiz-	O
ing	O
variance	B
with	O
the	O
view	O
of	O
it	O
as	O
minimizing	O
reconstruction	B
error	I
.	O
•	O
implement	O
latent	O
semantic	O
analysis	O
for	O
text	O
data	O
.	O
•	O
motivate	O
manifold	O
learning	O
from	O
the	O
perspective	O
of	O
reconstruction	B
error	I
.	O
•	O
understand	O
k-means	O
clustering	B
as	O
distance	B
minimization	O
.	O
•	O
explain	O
the	O
importance	O
of	O
initial-	O
ization	O
in	O
k-means	O
and	O
furthest-ﬁrst	O
heuristic	O
.	O
•	O
implement	O
agglomerative	O
clustering	B
.	O
•	O
argue	O
whether	O
spectral	O
cluster-	O
ing	O
is	O
a	O
clustering	B
algorithm	O
or	O
a	O
dimensionality	B
reduction	I
algorithm	O
.	O
dependencies	O
:	O
–	O
if	O
you	O
have	O
access	O
to	O
labeled	O
training	B
data	I
,	O
you	O
know	O
what	O
to	O
do	O
.	O
this	O
is	O
the	O
“	O
supervised	O
”	O
setting	O
,	O
in	O
which	O
you	O
have	O
a	O
teacher	O
telling	O
you	O
the	O
right	O
answers	O
.	O
unfortunately	O
,	O
ﬁnding	O
such	O
a	O
teacher	O
is	O
often	O
difﬁcult	O
,	O
expensive	O
,	O
or	O
down	O
right	O
impossible	O
.	O
in	O
those	O
cases	O
,	O
you	O
might	O
still	O
want	O
to	O
be	O
able	O
to	O
analyze	O
your	O
data	O
,	O
even	O
though	O
you	O
do	O
not	O
have	O
labels	O
.	O
unsupervised	B
learning	I
is	O
learning	O
without	O
a	O
teacher	O
.	O
one	O
basic	O
thing	O
that	O
you	O
might	O
want	O
to	O
do	O
with	O
data	O
is	O
to	O
visualize	B
it	O
.	O
sadly	O
,	O
it	O
is	O
difﬁcult	O
to	O
visualize	B
things	O
in	O
more	O
than	O
two	O
(	O
or	O
three	O
)	O
dimensions	O
,	O
and	O
most	O
data	O
is	O
in	O
hundreds	O
of	O
dimensions	O
(	O
or	O
more	O
)	O
.	O
dimension-	O
ality	O
reduction	O
is	O
the	O
problem	O
of	O
taking	O
high	O
dimensional	O
data	O
and	O
embedding	B
it	O
in	O
a	O
lower	O
dimension	O
space	O
.	O
another	O
thing	O
you	O
might	O
want	O
to	O
do	O
is	O
automatically	O
derive	O
a	O
partitioning	O
of	O
the	O
data	O
into	O
clusters	O
.	O
you	O
’	O
ve	O
already	O
learned	O
a	O
basic	O
approach	O
for	O
doing	O
this	O
:	O
the	O
k-means	O
algorithm	B
(	O
chapter	O
2	O
)	O
.	O
here	O
you	O
will	O
analyze	O
this	O
algorithm	B
to	O
see	O
why	O
it	O
works	O
.	O
you	O
will	O
also	O
learn	O
more	O
advanced	O
clustering	B
approaches	O
.	O
13.1	O
k-means	O
clustering	B
,	O
revisited	O
the	O
k-means	O
clustering	B
algorithm	O
is	O
re-presented	O
in	O
algorithm	B
13.1.	O
there	O
are	O
two	O
very	O
basic	O
questions	O
about	O
this	O
algorithm	B
:	O
(	O
1	O
)	O
does	O
it	O
converge	O
(	O
and	O
if	O
so	O
,	O
how	O
quickly	O
)	O
;	O
(	O
2	O
)	O
how	O
sensitive	O
it	O
is	O
to	O
initializa-	O
tion	O
?	O
the	O
answers	O
to	O
these	O
questions	O
,	O
detailed	O
below	O
,	O
are	O
:	O
(	O
1	O
)	O
yes	O
it	O
converges	O
,	O
and	O
it	O
converges	O
very	O
quickly	O
in	O
practice	O
(	O
though	O
slowly	O
in	O
theory	O
)	O
;	O
(	O
2	O
)	O
yes	O
it	O
is	O
sensitive	O
to	O
initialization	O
,	O
but	O
there	O
are	O
good	O
ways	O
to	O
initialize	O
it	O
.	O
consider	O
the	O
question	O
of	O
convergence	O
.	O
the	O
following	O
theorem	O
states	O
that	O
the	O
k-means	O
algorithm	B
converges	O
,	O
though	O
it	O
does	O
not	O
say	O
how	O
quickly	O
it	O
happens	O
.	O
the	O
method	O
of	O
proving	O
the	O
convergence	O
is	O
to	O
specify	O
a	O
clustering	B
quality	I
objective	O
function	O
,	O
and	O
then	O
to	O
show	O
that	O
the	O
k-means	O
algorithm	B
converges	O
to	O
a	O
(	O
local	O
)	O
optimum	O
of	O
that	O
objective	B
function	I
.	O
the	O
particular	O
objective	B
function	I
that	O
k-means	O
algorithm	B
34	O
k-means	O
(	O
d	O
,	O
k	O
)	O
1	O
:	O
for	O
k	O
=	O
1	O
to	O
k	O
do	O
µk	O
←	O
some	O
random	O
location	O
2	O
:	O
3	O
:	O
end	O
for	O
4	O
:	O
repeat	O
5	O
:	O
for	O
n	O
=	O
1	O
to	O
n	O
do	O
zn	O
←	O
argmink	O
||µk	O
−	O
xn||	O
end	O
for	O
for	O
k	O
=	O
1	O
to	O
k	O
do	O
µk	O
←	O
mean	O
(	O
{	O
xn	O
:	O
zn	O
=	O
k	O
}	O
)	O
6	O
:	O
7	O
:	O
8	O
:	O
9	O
:	O
end	O
for	O
10	O
:	O
11	O
:	O
until	O
converged	O
12	O
:	O
return	O
z	O
unsupervised	B
learning	I
167	O
//	O
randomly	O
initialize	O
mean	O
for	O
kth	O
cluster	O
//	O
assign	O
example	O
n	O
to	O
closest	O
center	O
//	O
re-estimate	O
mean	O
of	O
cluster	O
k	O
//	O
return	O
cluster	O
assignments	O
is	O
optimizing	O
is	O
the	O
sum	O
of	O
squared	O
distances	O
from	O
any	O
data	O
point	O
to	O
its	O
assigned	O
center	O
.	O
this	O
is	O
a	O
natural	O
generalization	O
of	O
the	O
deﬁnition	O
of	O
a	O
mean	O
:	O
the	O
mean	O
of	O
a	O
set	O
of	O
points	O
is	O
the	O
single	O
point	O
that	O
minimizes	O
the	O
sum	O
of	O
squared	O
distances	O
from	O
the	O
mean	O
to	O
every	O
point	O
in	O
the	O
data	O
.	O
formally	O
,	O
the	O
k-means	O
objective	O
is	O
:	O
=	O
∑	O
k	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
xn	O
−	O
µzn	O
l	O
(	O
z	O
,	O
µ	O
;	O
d	O
)	O
=	O
∑	O
n	O
||xn	O
−	O
µk||2	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
2	O
∑	O
n	O
:	O
zn=k	O
(	O
13.1	O
)	O
theorem	O
15	O
(	O
k-means	O
convergence	O
theorem	O
)	O
.	O
for	O
any	O
dataset	O
d	O
and	O
any	O
number	O
of	O
clusters	O
k	O
,	O
the	O
k-means	O
algorithm	B
converges	O
in	O
a	O
ﬁnite	O
num-	O
ber	O
of	O
iterations	O
,	O
where	O
convergence	O
is	O
measured	O
by	O
l	O
ceasing	O
the	O
change	O
.	O
proof	O
of	O
theorem	O
15.	O
the	O
proof	O
works	O
as	O
follows	O
.	O
there	O
are	O
only	O
two	O
points	O
in	O
which	O
the	O
k-means	O
algorithm	B
changes	O
the	O
values	O
of	O
µ	O
or	O
z	O
:	O
lines	O
6	O
and	O
9.	O
we	O
will	O
show	O
that	O
both	O
of	O
these	O
operations	O
can	O
never	O
increase	O
the	O
value	O
of	O
l.	O
assuming	O
this	O
is	O
true	O
,	O
the	O
rest	O
of	O
the	O
argu-	O
ment	O
is	O
as	O
follows	O
.	O
after	O
the	O
ﬁrst	O
pass	O
through	O
the	O
data	O
,	O
there	O
are	O
are	O
only	O
ﬁnitely	O
many	O
possible	O
assignments	O
to	O
z	O
and	O
µ	O
,	O
because	O
z	O
is	O
discrete	O
and	O
because	O
µ	O
can	O
only	O
take	O
on	O
a	O
ﬁnite	O
number	O
of	O
values	O
:	O
means	O
of	O
some	O
subset	O
of	O
the	O
data	O
.	O
furthermore	O
,	O
l	O
is	O
lower-bounded	O
by	O
zero	O
.	O
together	O
,	O
this	O
means	O
that	O
l	O
can	O
not	O
decrease	O
more	O
than	O
a	O
ﬁnite	O
number	O
of	O
times	O
.	O
thus	O
,	O
it	O
must	O
stop	O
decreasing	O
at	O
some	O
point	O
,	O
and	O
at	O
that	O
point	O
the	O
algorithm	O
has	O
converged	O
.	O
it	O
remains	O
to	O
show	O
that	O
lines	O
6	O
and	O
9	O
decrease	O
l.	O
for	O
line	O
6	O
,	O
when	O
looking	O
at	O
example	O
n	O
,	O
suppose	O
that	O
the	O
previous	O
value	O
of	O
zn	O
is	O
a	O
and	O
the	O
new	O
value	O
is	O
b.	O
it	O
must	O
be	O
the	O
case	O
that	O
||xn	O
−	O
µb||	O
≤	O
||xn	O
−	O
µb||	O
.	O
thus	O
,	O
changing	O
from	O
a	O
to	O
b	O
can	O
only	O
decrease	O
l.	O
for	O
line	O
9	O
,	O
consider	O
the	O
second	O
form	O
of	O
l.	O
line	O
9	O
computes	O
µk	O
as	O
the	O
mean	O
of	O
the	O
data	O
points	O
for	O
which	O
zn	O
=	O
k	O
,	O
which	O
is	O
precisely	O
the	O
point	O
that	O
minimizes	O
squared	O
sitances	O
.	O
thus	O
,	O
this	O
update	O
to	O
µk	O
can	O
only	O
decrease	O
l.	O
there	O
are	O
several	O
aspects	O
of	O
k-means	O
that	O
are	O
unfortunate	O
.	O
first	O
,	O
the	O
convergence	O
is	O
only	O
to	O
a	O
local	O
optimum	O
of	O
l.	O
in	O
practice	O
,	O
this	O
168	O
a	O
course	O
in	O
machine	O
learning	O
means	O
that	O
you	O
should	O
usually	O
run	O
it	O
10	O
times	O
with	O
different	O
initial-	O
izations	O
and	O
pick	O
the	O
one	O
with	O
minimal	O
resulting	O
l.	O
second	O
,	O
one	O
can	O
show	O
that	O
there	O
are	O
input	O
datasets	O
and	O
initializations	O
on	O
which	O
it	O
might	O
take	O
an	O
exponential	O
amount	O
of	O
time	O
to	O
converge	O
.	O
fortu-	O
nately	O
,	O
these	O
cases	O
almost	O
never	O
happen	O
in	O
practice	O
,	O
and	O
in	O
fact	O
it	O
has	O
recently	O
been	O
shown	O
that	O
(	O
roughly	O
)	O
if	O
you	O
limit	O
the	O
ﬂoating	O
point	O
pre-	O
cision	O
of	O
your	O
machine	O
,	O
k-means	O
will	O
converge	O
in	O
polynomial	O
time	O
(	O
though	O
still	O
only	O
to	O
a	O
local	O
optimum	O
)	O
,	O
using	O
techniques	O
of	O
smoothed	B
analysis	I
.	O
the	O
biggest	O
practical	O
issue	O
in	O
k-means	O
is	O
initialization	O
.	O
if	O
the	O
clus-	O
ter	O
means	O
are	O
initialized	O
poorly	O
,	O
you	O
often	O
get	O
convergence	O
to	O
uninter-	O
esting	O
solutions	O
.	O
a	O
useful	O
heuristic	O
is	O
the	O
furthest-ﬁrst	O
heuristic	O
.	O
this	O
gives	O
a	O
way	O
to	O
perform	O
a	O
semi-random	O
initialization	O
that	O
attempts	O
to	O
pick	O
initial	O
means	O
as	O
far	O
from	O
each	O
other	O
as	O
possible	O
.	O
the	O
heuristic	O
is	O
sketched	O
below	O
:	O
1.	O
pick	O
a	O
random	O
example	O
m	O
and	O
set	O
µ1	O
=	O
xm	O
.	O
2.	O
for	O
k	O
=	O
2	O
.	O
.	O
.	O
k	O
:	O
(	O
a	O
)	O
find	O
the	O
example	O
m	O
that	O
is	O
as	O
far	O
as	O
possible	O
from	O
all	O
previ-	O
ously	O
selected	O
means	O
;	O
namely	O
:	O
m	O
=	O
arg	O
maxm	O
mink	O
(	O
cid:48	O
)	O
<	O
k	O
||xm	O
−	O
µk	O
(	O
cid:48	O
)	O
||2	O
and	O
set	O
µk	O
=	O
xm	O
in	O
this	O
heuristic	O
,	O
the	O
only	O
bit	O
of	O
randomness	O
is	O
the	O
selection	O
of	O
the	O
ﬁrst	O
data	O
point	O
.	O
after	O
that	O
,	O
it	O
is	O
completely	O
deterministic	O
(	O
except	O
in	O
the	O
rare	O
case	O
that	O
there	O
are	O
multiple	O
equidistant	O
points	O
in	O
step	O
2a	O
)	O
.	O
it	O
is	O
extremely	O
important	O
that	O
when	O
selecting	O
the	O
3rd	O
mean	O
,	O
you	O
select	O
that	O
point	O
that	O
maximizes	O
the	O
minimum	O
distance	B
to	O
the	O
closest	O
other	O
mean	O
.	O
you	O
want	O
the	O
point	O
that	O
’	O
s	O
as	O
far	O
away	O
from	O
all	O
previous	O
means	O
as	O
possible	O
.	O
the	O
furthest-ﬁrst	O
heuristic	O
is	O
just	O
that	O
:	O
a	O
heuristic	O
.	O
it	O
works	O
very	O
well	O
in	O
practice	O
,	O
though	O
can	O
be	O
somewhat	O
sensitive	O
to	O
outliers	O
(	O
which	O
will	O
often	O
get	O
selected	O
as	O
some	O
of	O
the	O
initial	O
means	O
)	O
.	O
however	O
,	O
this	O
outlier	O
sensitivity	B
is	O
usually	O
reduced	O
after	O
one	O
iteration	O
through	O
the	O
k-means	O
algorithm	B
.	O
despite	O
being	O
just	O
a	O
heuristic	O
,	O
it	O
is	O
quite	O
useful	O
in	O
practice	O
.	O
you	O
can	O
turn	O
the	O
heuristic	O
into	O
an	O
algorithm	B
by	O
adding	O
a	O
bit	O
more	O
randomness	O
.	O
this	O
is	O
the	O
idea	O
of	O
the	O
k-means++	O
algorithm	B
,	O
which	O
is	O
a	O
simple	O
randomized	O
tweak	O
on	O
the	O
furthest-ﬁrst	O
heuristic	O
.	O
the	O
idea	O
is	O
that	O
when	O
you	O
select	O
the	O
kth	O
mean	O
,	O
instead	O
of	O
choosing	O
the	O
absolute	O
furthest	O
data	O
point	O
,	O
you	O
choose	O
a	O
data	O
point	O
at	O
random	O
,	O
with	O
probability	O
proportional	O
to	O
its	O
distance	B
squared	O
.	O
this	O
is	O
made	O
formal	O
in	O
algorithm	B
13.1.	O
if	O
you	O
use	O
k-means++	O
as	O
an	O
initialization	O
for	O
k-means	O
,	O
then	O
you	O
are	O
able	O
to	O
achieve	O
an	O
approximation	O
guarantee	O
on	O
the	O
ﬁnal	O
value	O
unsupervised	B
learning	I
169	O
algorithm	B
35	O
k-means++	O
(	O
d	O
,	O
k	O
)	O
1	O
:	O
µ1	O
←	O
xm	O
for	O
m	O
chosen	O
uniformly	O
at	O
random	O
//	O
randomly	O
initialize	O
ﬁrst	O
point	O
2	O
:	O
for	O
k	O
=	O
2	O
to	O
k	O
do	O
3	O
:	O
//	O
compute	O
distances	O
//	O
normalize	B
to	O
probability	O
distribution	O
//	O
pick	O
an	O
example	O
at	O
random	O
4	O
:	O
5	O
:	O
dn	O
←	O
mink	O
(	O
cid:48	O
)	O
<	O
k	O
||xn	O
−	O
µk	O
(	O
cid:48	O
)	O
||2	O
,	O
∀n	O
p	O
←	O
1	O
∑n	O
nd	O
m	O
←	O
random	O
sample	O
from	O
p	O
µk	O
←	O
xm	O
d	O
6	O
:	O
7	O
:	O
end	O
for	O
8	O
:	O
run	O
k-means	O
using	O
µ	O
as	O
initial	O
centers	O
of	O
the	O
objective	O
.	O
this	O
doesn	O
’	O
t	O
tell	O
you	O
that	O
you	O
will	O
reach	O
the	O
global	O
optimum	O
,	O
but	O
it	O
does	O
tell	O
you	O
that	O
you	O
will	O
get	O
reasonably	O
close	O
.	O
in	O
particular	O
,	O
if	O
ˆl	O
is	O
the	O
value	O
obtained	O
by	O
running	O
k-means++	O
,	O
then	O
this	O
will	O
not	O
be	O
“	O
too	O
far	O
”	O
from	O
l	O
(	O
opt	O
)	O
,	O
the	O
true	O
global	B
minimum	I
.	O
theorem	O
16	O
(	O
k-means++	O
approximation	O
guarantee	O
)	O
.	O
the	O
expected	O
value	O
of	O
the	O
objective	O
returned	O
by	O
k-means++	O
is	O
never	O
more	O
than	O
o	O
(	O
log	O
k	O
)	O
from	O
optimal	O
and	O
can	O
be	O
as	O
close	O
as	O
o	O
(	O
1	O
)	O
from	O
optimal	O
.	O
even	O
in	O
the	O
former	O
case	O
,	O
with	O
2k	O
random	O
restarts	O
,	O
one	O
restart	O
will	O
be	O
o	O
(	O
1	O
)	O
from	O
optimal	O
(	O
with	O
high	O
probability	O
)	O
.	O
formally	O
:	O
e	O
(	O
cid:2	O
)	O
ˆl	O
(	O
cid:3	O
)	O
≤	O
8	O
(	O
log	O
k	O
+	O
2	O
)	O
l	O
(	O
opt	O
)	O
.	O
moreover	O
,	O
if	O
the	O
data	O
is	O
“	O
well	O
suited	O
”	O
for	O
clustering	B
,	O
then	O
e	O
(	O
cid:2	O
)	O
ˆl	O
(	O
cid:3	O
)	O
≤	O
o	O
(	O
1	O
)	O
l	O
(	O
opt	O
)	O
.	O
the	O
notion	O
of	O
“	O
well	O
suited	O
”	O
for	O
clustering	B
informally	O
states	O
that	O
the	O
advantage	O
of	O
going	O
from	O
k	O
−	O
1	O
clusters	O
to	O
k	O
clusters	O
is	O
“	O
large.	O
”	O
formally	O
,	O
it	O
means	O
that	O
lk	O
(	O
opt	O
)	O
is	O
the	O
optimal	O
value	O
for	O
clustering	B
with	O
k	O
clusters	O
,	O
and	O
	O
is	O
the	O
desired	O
degree	O
of	O
approximation	O
.	O
the	O
idea	O
is	O
that	O
if	O
this	O
condition	O
does	O
not	O
hold	O
,	O
then	O
you	O
shouldn	O
’	O
t	O
bother	O
clustering	B
the	O
data	O
.	O
(	O
opt	O
)	O
≤	O
2lk−1	O
(	O
opt	O
)	O
,	O
where	O
lk	O
one	O
of	O
the	O
biggest	O
practical	O
issues	O
with	O
k-means	O
clustering	B
is	O
“	O
choosing	O
k.	O
”	O
namely	O
,	O
if	O
someone	O
just	O
hands	O
you	O
a	O
dataset	O
and	O
asks	O
you	O
to	O
cluster	O
it	O
,	O
how	O
many	O
clusters	O
should	O
you	O
produce	O
?	O
this	O
is	O
difﬁcult	O
,	O
because	O
increasing	O
k	O
will	O
always	O
decrease	O
lk	O
(	O
opt	O
)	O
(	O
until	O
k	O
>	O
n	O
)	O
,	O
and	O
so	O
simply	O
using	O
l	O
as	O
a	O
notion	O
of	O
goodness	O
is	O
insufﬁ-	O
cient	O
(	O
analogous	O
to	O
overﬁtting	O
in	O
a	O
supervised	O
setting	O
)	O
.	O
a	O
number	O
of	O
“	O
information	O
criteria	O
”	O
have	O
been	O
proposed	O
to	O
try	O
to	O
address	O
this	O
problem	O
.	O
they	O
all	O
effectively	O
boil	O
down	O
to	O
“	O
regularizing	O
”	O
k	O
so	O
that	O
the	O
model	O
can	O
not	O
grow	O
to	O
be	O
too	O
complicated	O
.	O
the	O
two	O
most	O
popular	O
are	O
the	O
bayes	O
information	O
criteria	O
(	O
bic	O
)	O
and	O
the	O
akaike	O
information	O
criteria	O
(	O
aic	O
)	O
,	O
deﬁned	O
below	O
in	O
the	O
context	O
of	O
k-means	O
:	O
bic	O
:	O
aic	O
:	O
arg	O
min	O
k	O
arg	O
min	O
k	O
ˆlk	O
+	O
k	O
log	O
d	O
ˆlk	O
+	O
2kd	O
(	O
13.2	O
)	O
(	O
13.3	O
)	O
the	O
informal	O
intuition	O
behind	O
these	O
criteria	O
is	O
that	O
increasing	O
k	O
is	O
going	O
to	O
make	O
lk	O
go	O
down	O
.	O
however	O
,	O
if	O
it	O
doesn	O
’	O
t	O
go	O
down	O
“	O
by	O
enough	O
”	O
then	O
it	O
’	O
s	O
not	O
worth	O
doing	O
.	O
in	O
the	O
case	O
of	O
bic	O
,	O
“	O
by	O
enough	O
”	O
170	O
a	O
course	O
in	O
machine	O
learning	O
means	O
by	O
an	O
amount	O
proportional	O
to	O
log	O
d	O
;	O
in	O
the	O
case	O
of	O
aic	O
,	O
it	O
’	O
s	O
proportional	O
to	O
2d	O
.	O
thus	O
,	O
aic	O
provides	O
a	O
much	O
stronger	O
penalty	O
for	O
many	O
clusters	O
than	O
does	O
bic	O
,	O
especially	O
in	O
high	O
dimensions	O
.	O
a	O
more	O
formal	O
intuition	O
for	O
bic	O
is	O
the	O
following	O
.	O
you	O
ask	O
yourself	O
the	O
question	O
“	O
if	O
i	O
wanted	O
to	O
send	O
this	O
data	O
across	O
a	O
network	O
,	O
how	O
many	O
bits	O
would	O
i	O
need	O
to	O
send	O
?	O
”	O
clearly	O
you	O
could	O
simply	O
send	O
all	O
of	O
the	O
n	O
examples	B
,	O
each	O
of	O
which	O
would	O
take	O
roughly	O
log	O
d	O
bits	O
to	O
send	O
.	O
this	O
gives	O
n	O
log	O
d	O
to	O
send	O
all	O
the	O
data	O
.	O
alternatively	O
,	O
you	O
could	O
ﬁrst	O
cluster	O
the	O
data	O
and	O
send	O
the	O
cluster	O
centers	O
.	O
this	O
will	O
take	O
k	O
log	O
d	O
bits	O
.	O
then	O
,	O
for	O
each	O
data	O
point	O
,	O
you	O
send	O
its	O
center	O
as	O
well	O
as	O
its	O
deviation	O
from	O
that	O
center	O
.	O
it	O
turns	O
out	O
this	O
will	O
cost	O
exactly	O
ˆlk	O
bits	O
.	O
therefore	O
,	O
the	O
bic	O
is	O
precisely	O
measuring	O
how	O
many	O
bits	O
it	O
will	O
take	O
to	O
send	O
your	O
data	O
using	O
k	O
clusters	O
.	O
the	O
k	O
that	O
minimizes	O
this	O
number	O
of	O
bits	O
is	O
the	O
optimal	O
value	O
.	O
13.2	O
linear	O
dimensionality	O
reduction	O
dimensionality	B
reduction	I
is	O
the	O
task	O
of	O
taking	O
a	O
dataset	O
in	O
high	O
di-	O
mensions	O
(	O
say	O
10000	O
)	O
and	O
reducing	O
it	O
to	O
low	O
dimensions	O
(	O
say	O
2	O
)	O
while	O
retaining	O
the	O
“	O
important	O
”	O
characteristics	O
of	O
the	O
data	O
.	O
since	O
this	O
is	O
an	O
unsupervised	O
setting	O
,	O
the	O
notion	O
of	O
important	O
characteristics	O
is	O
difﬁcult	O
to	O
deﬁne	O
.	O
consider	O
the	O
dataset	O
in	O
figure	O
?	O
?	O
,	O
which	O
lives	O
in	O
high	O
dimensions	O
(	O
two	O
)	O
and	O
you	O
want	O
to	O
reduce	O
to	O
low	O
dimensions	O
(	O
one	O
)	O
.	O
in	O
the	O
case	O
of	O
linear	O
dimensionality	O
reduction	O
,	O
the	O
only	O
thing	O
you	O
can	O
do	O
is	O
to	O
project	O
the	O
data	O
onto	O
a	O
vector	B
and	O
use	O
the	O
projected	O
distances	O
as	O
the	O
embeddings	O
.	O
figure	O
?	O
?	O
shows	O
a	O
projection	O
of	O
this	O
data	O
onto	O
the	O
vector	O
that	O
points	O
in	O
the	O
direction	O
of	O
maximal	O
variance	B
of	O
the	O
original	O
dataset	O
.	O
intuitively	O
,	O
this	O
is	O
a	O
reasonable	O
notion	O
of	O
importance	O
,	O
since	O
this	O
is	O
the	O
direction	O
in	O
which	O
most	O
information	O
is	O
encoded	O
in	O
the	O
data	O
.	O
for	O
the	O
rest	O
of	O
this	O
section	O
,	O
assume	O
that	O
the	O
data	O
is	O
centered	O
:	O
namely	O
,	O
the	O
mean	O
of	O
all	O
the	O
data	O
is	O
at	O
the	O
origin	O
.	O
(	O
this	O
will	O
sim-	O
ply	O
make	O
the	O
math	O
easier	O
.	O
)	O
suppose	O
the	O
two	O
dimensional	O
data	O
is	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
and	O
you	O
’	O
re	O
looking	O
for	O
a	O
vector	B
u	O
that	O
points	O
in	O
the	O
direc-	O
tion	O
of	O
maximal	O
variance	B
.	O
you	O
can	O
compute	O
this	O
by	O
projecting	O
each	O
point	O
onto	O
u	O
and	O
looking	O
at	O
the	O
variance	O
of	O
the	O
result	O
.	O
in	O
order	O
for	O
the	O
projection	O
to	O
make	O
sense	O
,	O
you	O
need	O
to	O
constrain	O
||u||2	O
=	O
1.	O
in	O
this	O
case	O
,	O
the	O
projections	O
are	O
x1	O
,	O
u·	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
u·	O
.	O
call	O
these	O
values	O
p1	O
,	O
.	O
.	O
.	O
,	O
pn	O
.	O
the	O
goal	O
is	O
to	O
compute	O
the	O
variance	O
of	O
the	O
{	O
pn	O
}	O
s	O
and	O
then	O
choose	O
u	O
to	O
maximize	O
this	O
variance	B
.	O
to	O
compute	O
the	O
variance	O
,	O
you	O
ﬁrst	O
need	O
to	O
compute	O
the	O
mean	O
.	O
because	O
the	O
mean	O
of	O
the	O
xns	O
was	O
zero	O
,	O
the	O
math	O
review	O
|	O
eigenvalues	O
and	O
eigenvectors	O
the	O
usual	O
...	O
unsupervised	B
learning	I
171	O
figure	O
13.1	O
:	O
mean	O
of	O
the	O
ps	O
is	O
also	O
zero	O
.	O
this	O
can	O
be	O
seen	O
as	O
follows	O
:	O
(	O
cid:32	O
)	O
(	O
cid:33	O
)	O
∑	O
n	O
pn	O
=	O
∑	O
n	O
xn	O
·	O
u	O
=	O
∑	O
n	O
xn	O
·	O
u	O
=	O
0	O
·	O
u	O
=	O
0	O
(	O
13.4	O
)	O
the	O
variance	O
of	O
the	O
{	O
pn	O
}	O
is	O
then	O
just	O
∑n	O
p2	O
n.	O
finding	O
the	O
optimal	O
u	O
(	O
from	O
the	O
perspective	O
of	O
variance	B
maximization	O
)	O
reduces	O
to	O
the	O
following	O
optimization	B
problem	I
:	O
max	O
u	O
(	O
xn	O
·	O
u	O
)	O
2	O
∑	O
n	O
subj	O
.	O
to	O
||u||2	O
=	O
1	O
(	O
13.5	O
)	O
in	O
this	O
problem	O
it	O
becomes	O
apparent	O
why	O
keeping	O
u	O
unit	O
length	O
is	O
important	O
:	O
if	O
not	O
,	O
u	O
would	O
simply	O
stretch	O
to	O
have	O
inﬁnite	O
length	O
to	O
maximize	O
the	O
objective	O
.	O
it	O
is	O
now	O
helpful	O
to	O
write	O
the	O
collection	O
of	O
datapoints	O
xn	O
as	O
a	O
n×	O
d	O
matrix	O
x.	O
if	O
you	O
take	O
this	O
matrix	O
x	O
and	O
multiply	O
it	O
by	O
u	O
,	O
which	O
has	O
dimensions	O
d×1	O
,	O
you	O
end	O
up	O
with	O
a	O
n×1	O
vector	B
whose	O
values	O
are	O
exactly	O
the	O
values	O
p.	O
the	O
objective	O
in	O
eq	O
(	O
13.5	O
)	O
is	O
then	O
just	O
the	O
squared	O
norm	O
of	O
p.	O
this	O
simpliﬁes	O
eq	O
(	O
?	O
?	O
)	O
to	O
:	O
||xu||2	O
subj	O
.	O
to	O
||u||2	O
−	O
1	O
=	O
0	O
max	O
u	O
(	O
13.6	O
)	O
where	O
the	O
constraint	O
has	O
been	O
rewritten	O
to	O
make	O
it	O
amenable	O
to	O
con-	O
structing	O
the	O
lagrangian	O
.	O
doing	O
so	O
and	O
taking	O
gradients	O
yields	O
:	O
(	O
cid:17	O
)	O
l	O
(	O
u	O
,	O
λ	O
)	O
=	O
||xu||2	O
−	O
λ	O
(	O
cid:16	O
)	O
||u||2	O
−	O
1	O
(	O
cid:17	O
)	O
∇ul	O
=	O
2x	O
(	O
cid:62	O
)	O
xu	O
−	O
2λu	O
=⇒	O
λu	O
=	O
u	O
x	O
(	O
cid:62	O
)	O
x	O
(	O
cid:16	O
)	O
(	O
13.7	O
)	O
(	O
13.8	O
)	O
(	O
13.9	O
)	O
you	O
can	O
solve	O
this	O
expression	O
(	O
λu	O
=	O
x	O
(	O
cid:62	O
)	O
xu	O
)	O
by	O
computing	O
the	O
ﬁrst	O
eigenvector	O
and	O
eigenvalue	O
of	O
the	O
matrix	O
x	O
(	O
cid:62	O
)	O
x.	O
this	O
gives	O
you	O
the	O
solution	O
to	O
a	O
projection	O
into	O
a	O
one-dimensional	O
space	O
.	O
to	O
get	O
a	O
second	O
dimension	O
,	O
you	O
want	O
to	O
ﬁnd	O
a	O
new	O
vector	B
v	O
on	O
which	O
the	O
data	O
has	O
maximal	O
variance	B
.	O
however	O
,	O
to	O
avoid	O
redundancy	O
,	O
you	O
want	O
v	O
to	O
be	O
orthogonal	O
to	O
u	O
;	O
namely	O
u	O
·	O
v	O
=	O
0.	O
this	O
gives	O
:	O
||xv||2	O
subj	O
.	O
to	O
||v||2	O
=	O
1	O
,	O
and	O
u	O
·	O
v	O
=	O
0	O
(	O
13.10	O
)	O
max	O
v	O
following	O
the	O
same	O
procedure	O
as	O
before	O
,	O
you	O
can	O
construct	O
a	O
la-	O
172	O
a	O
course	O
in	O
machine	O
learning	O
algorithm	B
36	O
pca	O
(	O
d	O
,	O
k	O
)	O
1	O
:	O
µ	O
←	O
mean	O
(	O
x	O
)	O
x	O
−	O
µ1	O
(	O
cid:62	O
)	O
(	O
cid:17	O
)	O
(	O
cid:62	O
)	O
(	O
cid:16	O
)	O
2	O
:	O
d	O
←	O
(	O
cid:16	O
)	O
x	O
−	O
µ1	O
(	O
cid:62	O
)	O
(	O
cid:17	O
)	O
3	O
:	O
{	O
λk	O
,	O
uk	O
}	O
←	O
top	O
k	O
eigenvalues/eigenvectors	O
of	O
d	O
4	O
:	O
return	O
(	O
x	O
−	O
µ1	O
)	O
u	O
//	O
compute	O
data	O
mean	O
for	O
centering	O
//	O
compute	O
covariance	O
,	O
1	O
is	O
a	O
vector	B
of	O
ones	O
grangian	O
and	O
differentiate	O
:	O
l	O
(	O
v	O
,	O
λ1	O
,	O
λ2	O
)	O
=	O
||xv||2	O
−	O
λ1	O
∇ul	O
=	O
2x	O
(	O
cid:62	O
)	O
xv	O
−	O
2λ1v	O
−	O
2λ2u	O
v	O
−	O
λ2u	O
=⇒	O
λ1v	O
=	O
x	O
(	O
cid:62	O
)	O
x	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
−	O
λ2u	O
·	O
v	O
(	O
cid:16	O
)	O
||u||2	O
−	O
1	O
(	O
cid:17	O
)	O
//	O
project	O
data	O
using	O
u	O
(	O
13.11	O
)	O
(	O
13.12	O
)	O
(	O
13.13	O
)	O
however	O
,	O
you	O
know	O
that	O
u	O
is	O
the	O
ﬁrst	O
eigenvector	O
of	O
x	O
(	O
cid:62	O
)	O
x	O
,	O
so	O
the	O
solution	O
to	O
this	O
problem	O
for	O
λ1	O
and	O
v	O
is	O
given	O
by	O
the	O
second	O
eigen-	O
value/eigenvector	O
pair	O
of	O
x	O
(	O
cid:62	O
)	O
x.	O
repeating	O
this	O
analysis	O
inductively	O
tells	O
you	O
that	O
if	O
you	O
want	O
to	O
project	O
onto	O
k	O
mutually	O
orthogonal	O
dimensions	O
,	O
you	O
simply	O
need	O
to	O
take	O
the	O
ﬁrst	O
k	O
eigenvectors	O
of	O
the	O
matrix	O
x	O
(	O
cid:62	O
)	O
x.	O
this	O
matrix	O
is	O
often	O
called	O
the	O
data	O
covariance	O
matrix	O
because	O
[	O
x	O
(	O
cid:62	O
)	O
x	O
]	O
i	O
,	O
j	O
=	O
∑n	O
∑m	O
xn	O
,	O
ixm	O
,	O
j	O
,	O
which	O
is	O
the	O
sample	O
covariance	O
between	O
features	B
i	O
and	O
j.	O
this	O
leads	O
to	O
the	O
technique	O
of	O
principle	B
components	I
analysis	I
,	O
or	O
pca	O
.	O
for	O
completeness	O
,	O
the	O
is	O
depicted	O
in	O
algorithm	B
?	O
?	O
.	O
the	O
important	O
thing	O
to	O
note	O
is	O
that	O
the	O
eigenanalysis	O
only	O
gives	O
you	O
the	O
projection	O
directions	O
.	O
it	O
does	O
not	O
give	O
you	O
the	O
embedded	O
data	O
.	O
to	O
embed	O
a	O
data	O
point	O
x	O
you	O
need	O
to	O
compute	O
its	O
embedding	B
as	O
(	O
cid:104	O
)	O
x	O
·	O
u1	O
,	O
x	O
·	O
u2	O
,	O
.	O
.	O
.	O
,	O
x	O
·	O
uk	O
(	O
cid:105	O
)	O
.	O
if	O
you	O
write	O
u	O
for	O
the	O
d×k	O
matrix	O
of	O
us	O
,	O
then	O
this	O
is	O
just	O
xu	O
.	O
there	O
is	O
an	O
alternative	O
derivation	O
of	O
pca	O
that	O
can	O
be	O
informative	O
,	O
based	O
on	O
reconstruction	B
error	I
.	O
consider	O
the	O
one-dimensional	O
case	O
again	O
,	O
where	O
you	O
are	O
looking	O
for	O
a	O
single	O
projection	O
direction	O
u.	O
if	O
you	O
were	O
to	O
use	O
this	O
direction	O
,	O
your	O
projected	O
data	O
would	O
be	O
z	O
=	O
xu	O
.	O
each	O
zn	O
gives	O
the	O
position	O
of	O
the	O
nth	O
datapoint	O
along	O
u.	O
you	O
can	O
project	O
this	O
one-dimensional	O
data	O
back	O
into	O
the	O
original	O
space	O
by	O
multiplying	O
it	O
by	O
u	O
(	O
cid:62	O
)	O
.	O
this	O
gives	O
you	O
reconstructed	O
values	O
zu	O
(	O
cid:62	O
)	O
.	O
instead	O
of	O
maximizing	O
variance	B
,	O
you	O
might	O
instead	O
want	O
to	O
minimize	O
the	O
reconstruction	O
error	O
,	O
deﬁned	O
by	O
:	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
x	O
−	O
zu	O
(	O
cid:62	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
2	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
x	O
−	O
xuu	O
(	O
cid:62	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
2	O
=	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
xuu	O
(	O
cid:62	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
2	O
−	O
2x	O
(	O
cid:62	O
)	O
xuu	O
(	O
cid:62	O
)	O
=	O
||x||2	O
+	O
deﬁnition	O
of	O
z	O
(	O
13.14	O
)	O
quadratic	O
rule	O
(	O
13.15	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
xuu	O
(	O
cid:62	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
2	O
−	O
2u	O
(	O
cid:62	O
)	O
x	O
(	O
cid:62	O
)	O
xu	O
=	O
||x||2	O
+	O
=	O
||x||2	O
+	O
||x||2	O
−	O
2u	O
(	O
cid:62	O
)	O
x	O
(	O
cid:62	O
)	O
xu	O
unsupervised	B
learning	I
173	O
quadratic	O
rule	O
(	O
13.16	O
)	O
u	O
is	O
a	O
unit	O
vector	O
(	O
13.17	O
)	O
=	O
c	O
−	O
2||xu||2	O
join	O
constants	O
,	O
rewrite	O
last	O
term	O
(	O
13.18	O
)	O
minimizing	O
this	O
ﬁnal	O
term	O
is	O
equivalent	O
to	O
maximizing	O
||xu||2	O
,	O
which	O
is	O
exactly	O
the	O
form	O
of	O
the	O
maximum	O
variance	B
derivation	O
of	O
pca	O
.	O
thus	O
,	O
you	O
can	O
see	O
that	O
maximizing	O
variance	B
is	O
identical	O
to	O
minimiz-	O
ing	O
reconstruction	B
error	I
.	O
the	O
same	O
question	O
of	O
“	O
what	O
should	O
k	O
be	O
”	O
arises	O
in	O
dimension-	O
ality	O
reduction	O
as	O
in	O
clustering	B
.	O
if	O
the	O
purpose	O
of	O
dimensionality	B
reduction	I
is	O
to	O
visualize	B
,	O
then	O
k	O
should	O
be	O
2	O
or	O
3.	O
however	O
,	O
an	O
alter-	O
native	O
purpose	O
of	O
dimensionality	B
reduction	I
is	O
to	O
avoid	O
the	B
curse	I
of	I
dimensionality	I
.	O
for	O
instance	O
,	O
even	O
if	O
you	O
have	O
labeled	O
data	O
,	O
it	O
might	O
be	O
worthwhile	O
to	O
reduce	O
the	O
dimensionality	O
before	O
applying	O
super-	O
vised	O
learning	O
,	O
essentially	O
as	O
a	O
form	O
of	O
regularization	O
.	O
in	O
this	O
case	O
,	O
the	O
question	O
of	O
an	O
optimal	O
k	O
comes	O
up	O
again	O
.	O
in	O
this	O
case	O
,	O
the	O
same	O
criteria	O
(	O
aic	O
and	O
bic	O
)	O
that	O
can	O
be	O
used	O
for	O
clustering	B
can	O
be	O
used	O
for	O
pca	O
.	O
the	O
only	O
difference	O
is	O
the	O
quality	O
measure	O
changes	O
from	O
a	O
sum	O
of	O
squared	O
distances	O
to	O
means	O
(	O
for	O
clustering	B
)	O
to	O
a	O
sum	O
of	O
squared	O
distances	O
to	O
original	O
data	O
points	O
(	O
for	O
pca	O
)	O
.	O
in	O
particular	O
,	O
for	O
bic	O
you	O
get	O
the	O
reconstruction	O
error	O
plus	O
k	O
log	O
d	O
;	O
for	O
aic	O
,	O
you	O
get	O
the	O
recon-	O
struction	O
error	O
plus	O
2kd	O
.	O
13.3	O
manifolds	O
and	O
graphs	O
what	O
is	O
a	O
manifold	O
?	O
graph	B
construction	O
13.4	O
non-linear	B
dimensionality	O
reduction	O
isomap	O
lle	O
mvu	O
mds	O
?	O
13.5	O
non-linear	B
clustering	O
:	O
spectral	O
methods	O
what	O
is	O
a	O
spectrum	O
spectral	O
clustering	B
174	O
a	O
course	O
in	O
machine	O
learning	O
13.6	O
exercises	O
exercise	O
13.1.	O
todo	O
.	O
.	O
.	O
14	O
|	O
expectation	B
maximization	I
learning	O
objectives	O
:	O
•	O
explain	O
the	O
relationship	O
between	O
parameters	O
and	O
hidden	B
variables	I
.	O
•	O
construct	O
generative	O
stories	O
for	O
clustering	B
and	O
dimensionality	B
reduction	I
.	O
•	O
draw	O
a	O
graph	B
explaining	O
how	O
em	O
works	O
by	O
constructing	O
convex	B
lower	O
bounds	O
.	O
•	O
implement	O
em	O
for	O
clustering	B
with	O
mixtures	O
of	O
gaussians	O
,	O
and	O
contrast-	O
ing	O
it	O
with	O
k-means	O
.	O
•	O
evaluate	O
the	O
differences	O
betweem	O
em	O
and	O
gradient	B
descent	I
for	O
hidden	O
variable	O
models	O
.	O
dependencies	O
:	O
–	O
suppose	O
you	O
were	O
building	O
a	O
naive	O
bayes	O
model	B
for	O
a	O
text	O
cate-	O
gorization	O
problem	O
.	O
after	O
you	O
were	O
done	O
,	O
your	O
boss	O
told	O
you	O
that	O
it	O
became	O
prohibitively	O
expensive	O
to	O
obtain	O
labeled	O
data	O
.	O
you	O
now	O
have	O
a	O
probabilistic	O
model	O
that	O
assumes	O
access	O
to	O
labels	O
,	O
but	O
you	O
don	O
’	O
t	O
have	O
any	O
labels	O
!	O
can	O
you	O
still	O
do	O
something	O
?	O
amazingly	O
,	O
you	O
can	O
.	O
you	O
can	O
treat	O
the	O
labels	O
as	O
hidden	B
variables	I
,	O
and	O
attempt	O
to	O
learn	O
them	O
at	O
the	O
same	O
time	O
as	O
you	O
learn	O
the	O
param-	O
eters	O
of	O
your	O
model	B
.	O
a	O
very	O
broad	O
family	O
of	O
algorithms	O
for	O
solving	O
problems	O
just	O
like	O
this	O
is	O
the	O
expectation	O
maximization	O
family	O
.	O
in	O
this	O
chapter	O
,	O
you	O
will	O
derive	O
expectation	B
maximization	I
(	O
em	O
)	O
algorithms	O
for	O
clustering	B
and	O
dimensionality	B
reduction	I
,	O
and	O
then	O
see	O
why	O
em	O
works	O
.	O
14.1	O
clustering	B
with	O
a	O
mixture	O
of	O
gaussians	O
in	O
chapter	O
7	O
,	O
you	O
learned	O
about	O
probabilitic	O
models	O
for	O
classiﬁcation	O
based	O
on	O
density	O
estimation	O
.	O
let	O
’	O
s	O
start	O
with	O
a	O
fairly	O
simple	O
classiﬁca-	O
tion	O
model	B
that	O
assumes	O
we	O
have	O
labeled	O
data	O
.	O
we	O
will	O
shortly	O
remove	O
this	O
assumption	O
.	O
our	O
model	B
will	O
state	O
that	O
we	O
have	O
k	O
classes	O
,	O
and	O
data	O
from	O
class	O
k	O
is	O
drawn	O
from	O
a	O
gaussian	O
with	O
mean	O
µk	O
and	O
vari-	O
ance	O
σ2	O
k	O
.	O
the	O
choice	O
of	O
classes	O
is	O
parameterized	O
by	O
θ.	O
the	O
generative	O
story	O
for	O
this	O
model	B
is	O
:	O
1.	O
for	O
each	O
example	O
n	O
=	O
1	O
.	O
.	O
.	O
n	O
:	O
(	O
a	O
)	O
choose	O
a	O
label	B
yn	O
∼	O
disc	O
(	O
θ	O
)	O
(	O
b	O
)	O
choose	O
example	O
xn	O
∼	O
nor	O
(	O
µyn	O
,	O
σ2	O
yn	O
)	O
this	O
generative	B
story	I
can	O
be	O
directly	O
translated	O
into	O
a	O
likelihood	B
as	O
before	O
:	O
p	O
(	O
d	O
)	O
=	O
∏	O
n	O
mult	O
(	O
yn	O
|	O
θ	O
)	O
nor	O
(	O
xn	O
|	O
µyn	O
,	O
σ2	O
yn	O
)	O
(	O
14.1	O
)	O
176	O
a	O
course	O
in	O
machine	O
learning	O
(	O
cid:122	O
)	O
∏	O
n	O
=	O
(	O
cid:104	O
)	O
(	O
cid:124	O
)	O
θyn	O
(	O
cid:124	O
)	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
(	O
cid:125	O
)	O
choose	O
label	B
2πσ2	O
yn	O
for	O
each	O
example	O
(	O
cid:105	O
)	O
−	O
d	O
(	O
cid:125	O
)	O
(	O
cid:124	O
)	O
(	O
cid:34	O
)	O
2	O
exp	O
−	O
1	O
2σ2	O
yn	O
(	O
cid:123	O
)	O
(	O
cid:122	O
)	O
choose	O
feature	B
values	I
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
xn	O
−	O
µyn	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
2	O
(	O
cid:123	O
)	O
(	O
cid:35	O
)	O
(	O
cid:125	O
)	O
(	O
14.2	O
)	O
if	O
you	O
had	O
access	O
to	O
labels	O
,	O
this	O
would	O
be	O
all	O
well	O
and	O
good	O
,	O
and	O
you	O
could	O
obtain	O
closed	O
form	O
solutions	O
for	O
the	O
maximum	O
likelihood	B
estimates	O
of	O
all	O
parameters	O
by	O
taking	O
a	O
log	O
and	O
then	O
taking	O
gradients	O
of	O
the	O
log	O
likelihood	B
:	O
θk	O
=	O
fraction	O
of	O
training	O
examples	O
in	O
class	O
k	O
=	O
1	O
n	O
∑	O
n	O
[	O
yn	O
=	O
k	O
]	O
µk	O
=	O
mean	O
of	O
training	O
examples	O
in	O
class	O
k	O
=	O
∑n	O
[	O
yn	O
=	O
k	O
]	O
xn	O
∑n	O
[	O
yn	O
=	O
k	O
]	O
σ2	O
k	O
=	O
variance	B
of	O
training	O
examples	O
in	O
class	O
k	O
∑n	O
[	O
yn	O
=	O
k	O
]	O
||xn	O
−	O
µk||	O
∑n	O
[	O
yn	O
=	O
k	O
]	O
=	O
(	O
14.3	O
)	O
(	O
14.4	O
)	O
(	O
14.5	O
)	O
?	O
you	O
should	O
be	O
able	O
to	O
derive	O
the	O
maximum	O
likelihood	B
solution	O
re-	O
sults	O
formally	O
by	O
now	O
.	O
suppose	O
that	O
you	O
don	O
’	O
t	O
have	O
labels	O
.	O
analogously	O
to	O
the	O
k-means	O
algorithm	B
,	O
one	O
potential	O
solution	O
is	O
to	O
iterate	O
.	O
you	O
can	O
start	O
off	O
with	O
guesses	O
for	O
the	O
values	O
of	O
the	O
unknown	O
variables	O
,	O
and	O
then	O
iteratively	O
improve	O
them	O
over	O
time	O
.	O
in	O
k-means	O
,	O
the	O
approach	O
was	O
the	O
assign	O
examples	B
to	O
labels	O
(	O
or	O
clusters	O
)	O
.	O
this	O
time	O
,	O
instead	O
of	O
making	O
hard	O
assignments	O
(	O
“	O
example	O
10	O
belongs	O
to	O
cluster	O
4	O
”	O
)	O
,	O
we	O
’	O
ll	O
make	O
soft	O
as-	O
signments	O
(	O
“	O
example	O
10	O
belongs	O
half	O
to	O
cluster	O
4	O
,	O
a	O
quarter	O
to	O
cluster	O
2	O
and	O
a	O
quarter	O
to	O
cluster	O
5	O
”	O
)	O
.	O
so	O
as	O
not	O
to	O
confuse	O
ourselves	O
too	O
much	O
,	O
we	O
’	O
ll	O
introduce	O
a	O
new	O
variable	O
,	O
zn	O
=	O
(	O
cid:104	O
)	O
zn,1	O
,	O
.	O
.	O
.	O
,	O
zn	O
,	O
k	O
(	O
that	O
sums	O
to	O
one	O
)	O
,	O
to	O
denote	O
a	O
fractional	O
assignment	O
of	O
examples	B
to	O
clusters	O
.	O
this	O
notion	O
of	O
soft-assignments	O
is	O
visualized	O
in	O
figure	O
14.1.	O
here	O
,	O
we	O
’	O
ve	O
depicted	O
each	O
example	O
as	O
a	O
pie	O
chart	O
,	O
and	O
it	O
’	O
s	O
coloring	O
denotes	O
the	O
degree	O
to	O
which	O
it	O
’	O
s	O
been	O
assigned	O
to	O
each	O
(	O
of	O
three	O
)	O
clusters	O
.	O
the	O
size	O
of	O
the	O
pie	O
pieces	O
correspond	O
to	O
the	O
zn	O
values	O
.	O
formally	O
,	O
zn	O
,	O
k	O
denotes	O
the	O
probability	O
that	O
example	O
n	O
is	O
assigned	O
to	O
cluster	O
k	O
:	O
zn	O
,	O
k	O
=	O
p	O
(	O
yn	O
=	O
k	O
|	O
xn	O
)	O
p	O
(	O
yn	O
=	O
k	O
,	O
xn	O
)	O
=	O
=	O
1	O
zn	O
p	O
(	O
xn	O
)	O
mult	O
(	O
k	O
|	O
θ	O
)	O
nor	O
(	O
xn	O
|	O
µk	O
,	O
σ2	O
k	O
)	O
(	O
14.6	O
)	O
(	O
14.7	O
)	O
(	O
14.8	O
)	O
figure	O
14.1	O
:	O
em	O
:	O
piecharts	O
:	O
a	O
ﬁgure	O
showing	O
pie	O
charts	O
here	O
,	O
the	O
normalizer	O
zn	O
is	O
to	O
ensure	O
that	O
zn	O
sums	O
to	O
one	O
.	O
given	O
a	O
set	O
of	O
parameters	O
(	O
the	O
θs	O
,	O
µs	O
and	O
σ2s	O
)	O
,	O
the	O
fractional	O
as-	O
signments	O
zn	O
,	O
k	O
are	O
easy	O
to	O
compute	O
.	O
now	O
,	O
akin	O
to	O
k-means	O
,	O
given	O
algorithm	B
37	O
gmm	O
(	O
x	O
,	O
k	O
)	O
expectation	B
maximization	I
177	O
//	O
randomly	O
initialize	O
mean	O
for	O
kth	O
cluster	O
//	O
initialize	O
variances	O
//	O
each	O
cluster	O
equally	O
likely	O
a	O
priori	O
3	O
:	O
1	O
:	O
for	O
k	O
=	O
1	O
to	O
k	O
do	O
2	O
:	O
µk	O
←	O
some	O
random	O
location	O
k	O
←	O
1	O
σ2	O
θk	O
←	O
1/k	O
4	O
:	O
5	O
:	O
end	O
for	O
6	O
:	O
repeat	O
7	O
:	O
for	O
n	O
=	O
1	O
to	O
n	O
do	O
(	O
cid:2	O
)	O
2πσ2	O
(	O
cid:3	O
)	O
−	O
d	O
for	O
k	O
=	O
1	O
to	O
k	O
do	O
zn	O
,	O
k	O
←	O
θk	O
(	O
unnormalized	O
)	O
fractional	B
assignments	I
2	O
exp	O
2σ2	O
k	O
k	O
(	O
cid:104	O
)	O
−	O
1	O
||xn	O
−	O
µk||2	O
(	O
cid:105	O
)	O
8	O
:	O
9	O
:	O
10	O
:	O
11	O
:	O
12	O
:	O
13	O
:	O
14	O
:	O
15	O
:	O
16	O
:	O
end	O
for	O
zn	O
←	O
1	O
∑k	O
zn	O
,	O
k	O
zn	O
end	O
for	O
for	O
k	O
=	O
1	O
to	O
k	O
do	O
θk	O
←	O
1	O
n	O
∑n	O
zn	O
,	O
k	O
µk	O
←	O
∑n	O
zn	O
,	O
kxn	O
∑n	O
zn	O
,	O
k	O
k	O
←	O
∑n	O
zn	O
,	O
k||xn−µk||	O
σ2	O
∑n	O
zn	O
,	O
k	O
end	O
for	O
17	O
:	O
18	O
:	O
until	O
converged	O
19	O
:	O
return	O
z	O
//	O
compute	O
//	O
normalize	B
fractional	O
assignments	O
//	O
re-estimate	O
prior	B
probability	O
of	O
cluster	O
k	O
//	O
re-estimate	O
mean	O
of	O
cluster	O
k	O
//	O
re-estimate	O
variance	B
of	O
cluster	O
k	O
//	O
return	O
cluster	O
assignments	O
fractional	B
assignments	I
,	O
you	O
need	O
to	O
recompute	O
estimates	O
of	O
the	O
model	O
parameters	O
.	O
in	O
analogy	O
to	O
the	O
maximum	O
likelihood	B
solution	O
(	O
eqs	O
(	O
?	O
?	O
)	O
-	O
(	O
?	O
?	O
)	O
)	O
,	O
you	O
can	O
do	O
this	O
by	O
counting	O
fractional	O
points	O
rather	O
than	O
full	O
points	O
.	O
this	O
gives	O
the	O
following	O
re-estimation	O
updates	O
:	O
θk	O
=	O
fraction	O
of	O
training	O
examples	O
in	O
class	O
k	O
=	O
1	O
n	O
∑	O
n	O
zn	O
,	O
k	O
µk	O
=	O
mean	O
of	O
fractional	O
examples	O
in	O
class	O
k	O
=	O
∑n	O
zn	O
,	O
kxn	O
∑n	O
zn	O
,	O
k	O
σ2	O
k	O
=	O
variance	B
of	O
fractional	O
examples	O
in	O
class	O
k	O
∑n	O
zn	O
,	O
k	O
||xn	O
−	O
µk||	O
=	O
∑n	O
zn	O
,	O
k	O
(	O
14.9	O
)	O
(	O
14.10	O
)	O
(	O
14.11	O
)	O
all	O
that	O
has	O
happened	O
here	O
is	O
that	O
the	O
hard	O
assignments	O
“	O
[	O
yn	O
=	O
k	O
]	O
”	O
have	O
been	O
replaced	O
with	O
soft	B
assignments	I
“	O
zn	O
,	O
k	O
”	O
.	O
as	O
a	O
bit	O
of	O
fore-	O
shadowing	O
of	O
what	O
is	O
to	O
come	O
,	O
what	O
we	O
’	O
ve	O
done	O
is	O
essentially	O
replace	O
known	O
labels	O
with	O
expected	O
labels	O
,	O
hence	O
the	O
name	O
“	O
expectation	O
maxi-	O
mization.	O
”	O
putting	O
this	O
together	O
yields	O
algorithm	B
14.1.	O
this	O
is	O
the	O
gmm	O
(	O
“	O
gaussian	O
mixture	O
models	O
”	O
)	O
algorithm	B
,	O
because	O
the	O
probabilitic	O
model	B
being	O
learned	O
describes	O
a	O
dataset	O
as	O
being	O
drawn	O
from	O
a	O
mix-	O
ture	O
distribution	O
,	O
where	O
each	O
component	O
of	O
this	O
distribution	O
is	O
a	O
178	O
a	O
course	O
in	O
machine	O
learning	O
gaussian	O
.	O
just	O
as	O
in	O
the	O
k-means	O
algorithm	B
,	O
this	O
approach	O
is	O
succeptible	O
to	O
local	O
optima	O
and	O
quality	O
of	O
initialization	O
.	O
the	O
heuristics	O
for	O
comput-	O
ing	O
better	O
initializers	O
for	O
k-means	O
are	O
also	O
useful	O
here	O
.	O
?	O
aside	O
from	O
the	O
fact	O
that	O
gmms	O
use	O
soft	B
assignments	I
and	O
k-means	O
uses	O
hard	O
assignments	O
,	O
there	O
are	O
other	O
differences	O
between	O
the	O
two	O
approaches	O
.	O
what	O
are	O
they	O
?	O
14.2	O
the	O
expectation	O
maximization	O
framework	O
figure	O
14.2	O
:	O
em	O
:	O
lowerbound	O
:	O
a	O
ﬁgure	O
showing	O
successive	O
lower	O
bounds	O
at	O
this	O
point	O
,	O
you	O
’	O
ve	O
seen	O
a	O
method	O
for	O
learning	O
in	O
a	O
particular	O
prob-	O
abilistic	O
model	B
with	O
hidden	B
variables	I
.	O
two	O
questions	O
remain	O
:	O
(	O
1	O
)	O
can	O
you	O
apply	O
this	O
idea	O
more	O
generally	O
and	O
(	O
2	O
)	O
why	O
is	O
it	O
even	O
a	O
reason-	O
able	O
thing	O
to	O
do	O
?	O
expectation	B
maximization	I
is	O
a	O
family	O
of	O
algorithms	O
for	O
performing	O
maximum	B
likelihood	I
estimation	I
in	O
probabilistic	O
mod-	O
els	O
with	O
hidden	B
variables	I
.	O
the	O
general	O
ﬂavor	O
of	O
how	O
we	O
will	O
proceed	O
is	O
as	O
follows	O
.	O
we	O
want	O
to	O
maximize	O
the	O
log	O
likelihood	B
l	O
,	O
but	O
this	O
will	O
turn	O
out	O
to	O
be	O
difﬁ-	O
cult	O
to	O
do	O
directly	O
.	O
instead	O
,	O
we	O
’	O
ll	O
pick	O
a	O
surrogate	O
function	O
˜l	O
that	O
’	O
s	O
a	O
lower	O
bound	O
on	O
l	O
(	O
i.e.	O
,	O
˜l	O
≤	O
l	O
everywhere	O
)	O
that	O
’	O
s	O
(	O
hopefully	O
)	O
easier	O
to	O
maximize	O
.	O
we	O
’	O
ll	O
construct	O
the	O
surrogate	O
in	O
such	O
a	O
way	O
that	O
increas-	O
ing	O
it	O
will	O
force	O
the	O
true	O
likelihood	B
to	O
also	O
go	O
up	O
.	O
after	O
maximizing	O
˜l	O
,	O
we	O
’	O
ll	O
construct	O
a	O
new	O
lower	O
bound	O
and	O
optimize	O
that	O
.	O
this	O
process	O
is	O
shown	O
pictorially	O
in	O
figure	O
14.2.	O
to	O
proceed	O
,	O
consider	O
an	O
arbitrary	O
probabilistic	O
model	O
p	O
(	O
x	O
,	O
y	O
|	O
θ	O
)	O
,	O
where	O
x	O
denotes	O
the	O
observed	O
data	O
,	O
y	O
denotes	O
the	O
hidden	O
data	O
and	O
θ	O
denotes	O
the	O
parameters	O
.	O
in	O
the	O
case	O
of	O
gaussian	O
mixture	O
models	O
,	O
x	O
was	O
the	O
data	O
points	O
,	O
y	O
was	O
the	O
(	O
unknown	O
)	O
labels	O
and	O
θ	O
included	O
the	O
cluster	O
prior	B
probabilities	O
,	O
the	O
cluster	O
means	O
and	O
the	O
cluster	O
vari-	O
ances	O
.	O
now	O
,	O
given	O
access	O
only	O
to	O
a	O
number	O
of	O
examples	B
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
you	O
would	O
like	O
to	O
estimate	O
the	O
parameters	O
(	O
θ	O
)	O
of	O
the	O
model	O
.	O
probabilistically	O
,	O
this	O
means	O
that	O
some	O
of	O
the	O
variables	O
are	O
un-	O
known	O
and	O
therefore	O
you	O
need	O
to	O
marginalize	O
(	O
or	O
sum	O
)	O
over	O
their	O
possible	O
values	O
.	O
now	O
,	O
your	O
data	O
consists	O
only	O
of	O
x	O
=	O
(	O
cid:104	O
)	O
x1	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
xn	O
(	O
cid:105	O
)	O
,	O
not	O
the	O
(	O
x	O
,	O
y	O
)	O
pairs	O
in	O
d.	O
you	O
can	O
then	O
write	O
the	O
likelihood	O
as	O
:	O
p	O
(	O
x	O
|	O
θ	O
)	O
=	O
∑	O
y1	O
∑	O
y2	O
·	O
·	O
·	O
∑	O
yn	O
p	O
(	O
x	O
,	O
y1	O
,	O
y2	O
,	O
.	O
.	O
.	O
yn	O
|	O
θ	O
)	O
=	O
∑	O
y1	O
∑	O
y2	O
·	O
·	O
·	O
∑	O
yn	O
∏	O
n	O
p	O
(	O
xn	O
,	O
yn	O
|	O
θ	O
)	O
=	O
∏	O
n	O
∑	O
yn	O
p	O
(	O
xn	O
,	O
yn	O
|	O
θ	O
)	O
marginalization	O
(	O
14.12	O
)	O
examples	B
are	O
independent	O
(	O
14.13	O
)	O
algebra	O
(	O
14.14	O
)	O
at	O
this	O
point	O
,	O
the	O
natural	O
thing	O
to	O
do	O
is	O
to	O
take	O
logs	O
and	O
then	O
start	O
taking	O
gradients	O
.	O
however	O
,	O
once	O
you	O
start	O
taking	O
logs	O
,	O
you	O
run	O
into	O
a	O
problem	O
:	O
the	O
log	O
can	O
not	O
eat	O
the	O
sum	O
!	O
p	O
(	O
xn	O
,	O
yn	O
|	O
θ	O
)	O
l	O
(	O
x	O
|	O
θ	O
)	O
=	O
∑	O
n	O
log	O
∑	O
yn	O
expectation	B
maximization	I
179	O
(	O
14.15	O
)	O
the	O
next	O
step	O
is	O
to	O
apply	O
the	O
somewhat	O
strange	O
,	O
but	O
strangely	O
namely	O
,	O
the	O
log	O
gets	O
“	O
stuck	O
”	O
outside	O
the	O
sum	O
and	O
can	O
not	O
move	O
in	O
to	O
decompose	O
the	O
rest	O
of	O
the	O
likelihood	O
term	O
!	O
useful	O
,	O
trick	O
of	O
multiplying	O
by	O
1.	O
in	O
particular	O
,	O
let	O
q	O
(	O
·	O
)	O
be	O
an	O
arbitrary	O
probability	O
distribution	O
.	O
we	O
will	O
multiply	O
the	O
p	O
(	O
.	O
.	O
.	O
)	O
term	O
above	O
by	O
q	O
(	O
yn	O
)	O
/q	O
(	O
yn	O
)	O
,	O
a	O
valid	O
step	O
so	O
long	O
as	O
q	O
is	O
never	O
zero	O
.	O
this	O
leads	O
to	O
:	O
l	O
(	O
x	O
|	O
θ	O
)	O
=	O
∑	O
n	O
log	O
∑	O
yn	O
q	O
(	O
yn	O
)	O
p	O
(	O
xn	O
,	O
yn	O
|	O
θ	O
)	O
q	O
(	O
yn	O
)	O
(	O
14.16	O
)	O
we	O
will	O
now	O
construct	O
a	O
lower	O
bound	O
using	O
jensen	O
’	O
s	O
inequality	O
.	O
this	O
is	O
a	O
very	O
useful	O
(	O
and	O
easy	O
to	O
prove	O
!	O
)	O
result	O
that	O
states	O
that	O
f	O
(	O
∑i	O
λixi	O
)	O
≥	O
∑i	O
λi	O
f	O
(	O
xi	O
)	O
,	O
so	O
long	O
as	O
(	O
a	O
)	O
λi	O
≥	O
0	O
for	O
all	O
i	O
,	O
(	O
b	O
)	O
∑i	O
λi	O
=	O
1	O
,	O
and	O
(	O
c	O
)	O
f	O
is	O
concave	B
.	O
if	O
this	O
looks	O
familiar	O
,	O
that	O
’	O
s	O
just	O
because	O
it	O
’	O
s	O
a	O
direct	O
result	O
of	O
the	O
deﬁnition	O
of	O
concavity	B
.	O
recall	B
that	O
f	O
is	O
concave	B
if	O
f	O
(	O
ax	O
+	O
by	O
)	O
≥	O
a	O
f	O
(	O
x	O
)	O
+	O
b	O
f	O
(	O
x	O
)	O
whenever	O
a	O
+	O
b	O
=	O
1.	O
you	O
can	O
now	O
apply	O
jensen	O
’	O
s	O
inequality	O
to	O
the	O
log	O
likelihood	B
by	O
identifying	O
the	O
list	O
of	O
q	O
(	O
yn	O
)	O
s	O
as	O
the	O
λs	O
,	O
log	O
as	O
f	O
(	O
which	O
is	O
,	O
indeed	O
,	O
concave	B
)	O
and	O
each	O
“	O
x	O
”	O
as	O
the	O
p/q	O
term	O
.	O
this	O
yields	O
:	O
?	O
prove	O
jensen	O
’	O
s	O
inequality	O
using	O
the	O
deﬁnition	O
of	O
concavity	B
and	O
induc-	O
tion	O
.	O
p	O
(	O
xn	O
,	O
yn	O
|	O
θ	O
)	O
l	O
(	O
x	O
|	O
θ	O
)	O
≥	O
∑	O
(	O
cid:104	O
)	O
n	O
=	O
∑	O
n	O
(	O
cid:44	O
)	O
˜l	O
(	O
x	O
|	O
θ	O
)	O
∑	O
yn	O
∑	O
yn	O
q	O
(	O
yn	O
)	O
log	O
q	O
(	O
yn	O
)	O
log	O
p	O
(	O
xn	O
,	O
yn	O
|	O
θ	O
)	O
−	O
q	O
(	O
yn	O
)	O
log	O
q	O
(	O
yn	O
)	O
q	O
(	O
yn	O
)	O
(	O
cid:105	O
)	O
(	O
14.17	O
)	O
(	O
14.18	O
)	O
(	O
14.19	O
)	O
note	O
that	O
this	O
inequality	O
holds	O
for	O
any	O
choice	O
of	O
function	O
q	O
,	O
so	O
long	O
as	O
its	O
non-negative	O
and	O
sums	O
to	O
one	O
.	O
in	O
particular	O
,	O
it	O
needn	O
’	O
t	O
even	O
by	O
the	O
same	O
function	O
q	O
for	O
each	O
n.	O
we	O
will	O
need	O
to	O
take	O
advantage	O
of	O
both	O
of	O
these	O
properties	O
.	O
we	O
have	O
succeeded	O
in	O
our	O
ﬁrst	O
goal	O
:	O
constructing	O
a	O
lower	O
bound	O
on	O
l.	O
when	O
you	O
go	O
to	O
optimize	O
this	O
lower	O
bound	O
for	O
θ	O
,	O
the	O
only	O
part	O
that	O
matters	O
is	O
the	O
ﬁrst	O
term	O
.	O
the	O
second	O
term	O
,	O
q	O
log	O
q	O
,	O
drops	O
out	O
as	O
a	O
function	O
of	O
θ.	O
this	O
means	O
that	O
the	O
the	O
maximization	O
you	O
need	O
to	O
be	O
able	O
to	O
compute	O
,	O
for	O
ﬁxed	O
qns	O
,	O
is	O
:	O
θ	O
(	O
new	O
)	O
←	O
arg	O
max	O
θ	O
∑	O
n	O
∑	O
yn	O
qn	O
(	O
yn	O
)	O
log	O
p	O
(	O
xn	O
,	O
yn	O
|	O
θ	O
)	O
(	O
14.20	O
)	O
this	O
is	O
exactly	O
the	O
sort	O
of	O
maximization	O
done	O
for	O
gaussian	O
mixture	O
models	O
when	O
we	O
recomputed	O
new	O
means	O
,	O
variances	O
and	O
cluster	O
prior	B
probabilities	O
.	O
180	O
a	O
course	O
in	O
machine	O
learning	O
the	O
second	O
question	O
is	O
:	O
what	O
should	O
qn	O
(	O
·	O
)	O
actually	O
be	O
?	O
any	O
rea-	O
sonable	O
q	O
will	O
lead	O
to	O
a	O
lower	O
bound	O
,	O
so	O
in	O
order	O
to	O
choose	O
one	O
q	O
over	O
another	O
,	O
we	O
need	O
another	O
criterion	O
.	O
recall	B
that	O
we	O
are	O
hoping	O
to	O
max-	O
imize	O
l	O
by	O
instead	O
maximizing	O
a	O
lower	O
bound	O
.	O
in	O
order	O
to	O
ensure	O
that	O
an	O
increase	O
in	O
the	O
lower	O
bound	O
implies	O
an	O
increase	O
in	O
l	O
,	O
we	O
need	O
to	O
ensure	O
that	O
l	O
(	O
x	O
|	O
θ	O
)	O
=	O
˜l	O
(	O
x	O
|	O
θ	O
)	O
.	O
in	O
words	O
:	O
˜l	O
should	O
be	O
a	O
lower	O
bound	O
on	O
l	O
that	O
makes	O
contact	O
at	O
the	O
current	O
point	O
,	O
θ.	O
this	O
is	O
shown	O
in	O
figure	O
?	O
?	O
,	O
including	O
a	O
case	O
where	O
the	O
lower	O
bound	O
does	O
not	O
make	O
contact	O
,	O
and	O
thereby	O
does	O
not	O
guarantee	O
an	O
increase	O
in	O
l	O
with	O
an	O
increase	O
in	O
˜l	O
.	O
14.3	O
em	O
versus	O
gradient	B
descent	I
computing	O
gradients	O
through	O
marginals	O
step	O
size	O
14.4	O
dimensionality	B
reduction	I
with	O
probabilistic	O
pca	O
derivation	O
advantages	O
over	O
pca	O
14.5	O
exercises	O
exercise	O
14.1.	O
todo	O
.	O
.	O
.	O
15	O
|	O
semi-supervised	B
learning	I
–	O
you	O
may	O
find	O
yourself	O
in	O
a	O
setting	O
where	O
you	O
have	O
access	O
to	O
some	O
labeled	O
data	O
and	O
some	O
unlabeled	O
data	O
.	O
you	O
would	O
like	O
to	O
use	O
the	O
labeled	O
data	O
to	O
learn	O
a	O
classiﬁer	O
,	O
but	O
it	O
seems	O
wasteful	O
to	O
throw	O
out	O
all	O
that	O
unlabeled	O
data	O
.	O
the	O
key	O
question	O
is	O
:	O
what	O
can	O
you	O
do	O
with	O
that	O
unlabeled	O
data	O
to	O
aid	O
learning	O
?	O
and	O
what	O
assumptions	O
do	O
we	O
have	O
to	O
make	O
in	O
order	O
for	O
this	O
to	O
be	O
helpful	O
?	O
one	O
idea	O
is	O
to	O
try	O
to	O
use	O
the	O
unlabeled	O
data	O
to	O
learn	O
a	O
better	O
deci-	O
sion	O
boundary	O
.	O
in	O
a	O
discriminative	O
setting	O
,	O
you	O
can	O
accomplish	O
this	O
by	O
trying	O
to	O
ﬁnd	O
decision	O
boundaries	O
that	O
don	O
’	O
t	O
pass	O
too	O
closely	O
to	O
unlabeled	O
data	O
.	O
in	O
a	O
generative	O
setting	O
,	O
you	O
can	O
simply	O
treat	O
some	O
of	O
the	O
labels	O
as	O
observed	O
and	O
some	O
as	O
hidden	O
.	O
this	O
is	O
semi-supervised	B
learning	I
.	O
an	O
alternative	O
idea	O
is	O
to	O
spend	O
a	O
small	O
amount	O
of	O
money	O
to	O
get	O
labels	O
for	O
some	O
subset	O
of	O
the	O
unlabeled	O
data	O
.	O
however	O
,	O
you	O
would	O
like	O
to	O
get	O
the	O
most	O
out	O
of	O
your	O
money	O
,	O
so	O
you	O
would	O
only	O
like	O
to	O
pay	O
for	O
labels	O
that	O
are	O
useful	O
.	O
this	O
is	O
active	B
learning	I
.	O
learning	O
objectives	O
:	O
•	O
explain	O
the	O
cluster	O
assumption	O
for	O
semi-supervised	O
discriminative	O
learning	O
,	O
and	O
why	O
it	O
is	O
necessary	O
.	O
•	O
dervive	O
an	O
em	O
algorithm	B
for	O
generative	O
semi-supervised	O
text	B
categorization	I
.	O
•	O
compare	O
and	O
contrast	O
the	O
query	O
by	O
uncertainty	O
and	O
query	O
by	O
committee	O
heuristics	O
for	O
active	B
learning	I
.	O
dependencies	O
:	O
15.1	O
em	O
for	O
semi-supervised	B
learning	I
naive	O
bayes	O
model	B
15.2	O
graph-based	O
semi-supervised	B
learning	I
key	O
assumption	O
graphs	O
and	O
manifolds	O
label	B
prop	O
15.3	O
loss-based	O
semi-supervised	B
learning	I
density	O
assumption	O
loss	B
function	I
non-convex	O
182	O
a	O
course	O
in	O
machine	O
learning	O
15.4	O
active	B
learning	I
motivation	O
qbc	O
qbu	O
15.5	O
dangers	O
of	O
semi-supervised	O
learing	O
unlab	O
overwhelms	O
lab	O
biased	O
data	O
from	O
active	O
15.6	O
exercises	O
exercise	O
15.1.	O
todo	O
.	O
.	O
.	O
16.1	O
exercises	O
exercise	O
16.1.	O
todo	O
.	O
.	O
.	O
16	O
|	O
graphical	O
models	O
learning	O
objectives	O
:	O
•	O
foo	O
dependencies	O
:	O
none	O
.	O
17	O
|	O
online	B
learning	I
learning	O
objectives	O
:	O
•	O
explain	O
the	O
experts	O
model	B
,	O
and	O
why	O
it	O
is	O
hard	O
even	O
to	O
compete	O
with	O
the	O
single	O
best	O
expert	O
.	O
•	O
deﬁne	O
what	O
it	O
means	O
for	O
an	O
online	B
learning	I
algorithm	O
to	O
have	O
no	O
regret	O
.	O
•	O
implement	O
the	O
follow-the-leader	O
algorithm	B
.	O
•	O
categorize	O
online	B
learning	I
algo-	O
rithms	O
in	O
terms	O
of	O
how	O
they	O
measure	O
changes	O
in	O
parameters	O
,	O
and	O
how	O
they	O
measure	O
error	O
.	O
dependencies	O
:	O
all	O
of	O
the	O
learning	O
algorithms	O
that	O
you	O
know	O
about	O
at	O
this	O
point	O
are	O
based	O
on	O
the	O
idea	O
of	O
training	O
a	O
model	B
on	O
some	O
data	O
,	O
and	O
evaluating	O
it	O
on	O
other	O
data	O
.	O
this	O
is	O
the	O
batch	O
learning	O
model	B
.	O
how-	O
ever	O
,	O
you	O
may	O
ﬁnd	O
yourself	O
in	O
a	O
situation	O
where	O
students	O
are	O
con-	O
stantly	O
rating	O
courses	O
,	O
and	O
also	O
constantly	O
asking	O
for	O
recommenda-	O
tions	O
.	O
online	B
learning	I
focuses	O
on	O
learning	O
over	O
a	O
stream	O
of	O
data	O
,	O
on	O
which	O
you	O
have	O
to	O
make	O
predictions	O
continually	O
.	O
you	O
have	O
actually	O
already	O
seen	O
an	O
example	O
of	O
an	O
online	B
learning	I
algorithm	O
:	O
the	O
perceptron	O
.	O
however	O
,	O
our	O
use	O
of	O
the	O
perceptron	O
and	O
our	O
analysis	O
of	O
its	O
performance	O
have	O
both	O
been	O
in	O
a	O
batch	B
setting	O
.	O
in	O
this	O
chapter	O
,	O
you	O
will	O
see	O
a	O
formalization	O
of	O
online	B
learning	I
(	O
which	O
differs	O
from	O
the	O
batch	O
learning	O
formalization	O
)	O
and	O
several	O
algorithms	O
for	O
online	B
learning	I
with	O
different	O
properties	O
.	O
17.1	O
online	B
learning	I
framework	O
regret	O
follow	O
the	O
leader	O
agnostic	O
learning	O
algorithm	B
versus	O
problem	O
17.2	O
learning	O
with	O
features	B
change	O
but	O
not	O
too	O
much	O
littlestone	O
analysis	O
for	O
gd	O
and	O
egd	O
17.3	O
passive	O
agressive	O
learning	O
pa	O
algorithm	B
online	O
analysis	O
online	B
learning	I
185	O
17.4	O
learning	O
with	O
lots	O
of	O
irrelevant	O
features	B
winnow	O
relationship	O
to	O
egd	O
17.5	O
exercises	O
exercise	O
17.1.	O
todo	O
.	O
.	O
.	O
18	O
|	O
structured	O
learning	O
tasks	O
–	O
learning	O
objectives	O
:	O
•	O
todo	O
.	O
.	O
.	O
-	O
hidden	O
markov	O
models	O
:	O
viterbi	O
-	O
hidden	O
markov	O
models	O
:	O
forward-backward	O
-	O
maximum	O
entropy	O
markov	O
models	O
-	O
structured	O
perceptronn	O
-	O
conditional	O
random	O
fields	O
-	O
m3ns	O
18.1	O
exercises	O
exercise	O
18.1.	O
todo	O
.	O
.	O
.	O
dependencies	O
:	O
19.1	O
exercises	O
exercise	O
19.1.	O
todo	O
.	O
.	O
.	O
19	O
|	O
bayesian	O
learning	O
learning	O
objectives	O
:	O
•	O
todo	O
.	O
.	O
.	O
bibliography	O
sergey	O
brin	O
.	O
near	O
neighbor	O
search	O
in	O
large	O
metric	O
spaces	O
.	O
in	O
confer-	O
ence	O
on	O
very	O
large	O
databases	O
(	O
vldb	O
)	O
,	O
1995.	O
tom	O
m.	O
mitchell	O
.	O
machine	O
learning	O
.	O
mcgraw	O
hill	O
,	O
1997.	O
frank	O
rosenblatt	O
.	O
the	O
perceptron	O
:	O
a	O
probabilistic	O
model	O
for	O
infor-	O
mation	O
storage	O
and	O
organization	O
in	O
the	O
brain	O
.	O
psychological	O
review	O
,	O
65:386–408	O
,	O
1958.	O
reprinted	O
in	O
neurocomputing	O
(	O
mit	O
press	O
,	O
1998	O
)	O
.	O