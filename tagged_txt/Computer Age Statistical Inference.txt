the	O
work	O
,	O
computer	O
age	O
statistical	O
inference	B
,	O
was	O
ﬁrst	O
published	O
by	O
cambridge	O
university	O
press	O
.	O
c	O
(	O
cid:13	O
)	O
in	O
the	O
work	O
,	O
bradley	O
efron	O
and	O
trevor	O
hastie	O
,	O
2016.	O
cambridge	O
university	O
press	O
’	O
s	O
catalogue	O
entry	O
for	O
the	O
work	O
can	O
be	O
found	O
at	O
http	O
:	O
//	O
www	O
.	O
cambridge	O
.	O
org/	O
9781107149892	O
nb	O
:	O
the	O
copy	O
of	O
the	O
work	O
,	O
as	O
displayed	O
on	O
this	O
website	O
,	O
can	O
be	O
purchased	O
through	O
cambridge	O
university	O
press	O
and	O
other	O
standard	O
distribution	O
channels	O
.	O
this	O
copy	O
is	O
made	O
available	O
for	O
personal	O
use	O
only	O
and	O
must	O
not	O
be	O
adapted	O
,	O
sold	O
or	O
re-distributed	O
.	O
corrected	O
november	O
10	O
,	O
2017.	O
the	O
twenty-first	O
century	O
has	O
seen	O
a	O
breathtaking	O
expansion	O
of	O
statistical	O
methodology	O
,	O
both	O
in	O
scope	O
and	O
in	O
influence	O
.	O
“	O
big	O
data	B
,	O
”	O
“	O
data	B
science	O
,	O
”	O
and	O
“	O
machine	O
learning	O
”	O
have	O
become	O
familiar	O
terms	O
in	O
the	O
news	O
,	O
as	O
statistical	O
methods	O
are	O
brought	O
to	O
bear	O
upon	O
the	O
enormous	O
data	B
sets	O
of	O
modern	O
science	O
and	O
commerce	O
.	O
how	O
did	O
we	O
get	O
here	O
?	O
and	O
where	O
are	O
we	O
going	O
?	O
this	O
book	O
takes	O
us	O
on	O
an	O
exhilarating	O
journey	O
through	O
the	O
revolution	O
in	O
data	B
analysis	O
following	O
the	O
introduction	O
of	O
electronic	O
computation	O
in	O
the	O
1950s	O
.	O
beginning	O
with	O
classical	O
inferential	O
theories	O
–	O
bayesian	O
,	O
frequentist	O
,	O
fisherian	O
–	O
individual	O
chapters	O
take	O
up	O
a	O
series	O
of	O
influential	O
topics	O
:	O
survival	O
analysis	B
,	O
logistic	B
regression	I
,	O
empirical	B
bayes	O
,	O
the	O
jackknife	O
and	O
bootstrap	O
,	O
random	O
forests	O
,	O
neural	O
networks	O
,	O
markov	O
chain	O
monte	O
carlo	O
,	O
inference	B
after	O
model	B
selection	I
,	O
and	O
dozens	O
more	O
.	O
the	O
distinctly	O
modern	O
approach	O
integrates	O
methodology	O
and	O
algorithms	O
with	O
statistical	O
inference	B
.	O
the	O
book	O
ends	O
with	O
speculation	O
on	O
the	O
future	O
direction	O
of	O
statistics	B
and	O
data	B
science.efron	O
&	O
hastiecomputer	O
age	O
statistical	O
inference	B
“	O
how	O
and	O
why	O
is	O
computational	O
statistics	B
taking	O
over	O
the	O
world	O
?	O
in	O
this	O
serious	O
work	O
of	O
synthesis	O
that	O
is	O
also	O
fun	O
to	O
read	O
,	O
efron	O
and	O
hastie	O
give	O
their	O
take	O
on	O
the	O
unreasonable	O
effectiveness	O
of	O
statistics	B
and	O
machine	O
learning	O
in	O
the	O
context	O
of	O
a	O
series	O
of	O
clear	O
,	O
historically	O
informed	O
examples.	O
”	O
—	O
andrew	O
gelman	O
,	O
columbia	O
university	O
“	O
computer	O
age	O
statistical	O
inference	B
is	O
written	O
especially	O
for	O
those	O
who	O
want	O
to	O
hear	O
the	O
big	O
ideas	O
,	O
and	O
see	O
them	O
instantiated	O
through	O
the	O
essential	O
mathematics	O
that	O
defines	O
statistical	O
analysis	B
.	O
it	O
makes	O
a	O
great	O
supplement	O
to	O
the	O
traditional	O
curricula	O
for	O
beginning	O
graduate	O
students.	O
”	O
—	O
rob	O
kass	O
,	O
carnegie	O
mellon	O
university	O
“	O
this	O
is	O
a	O
terrific	O
book	O
.	O
it	O
gives	O
a	O
clear	O
,	O
accessible	O
,	O
and	O
entertaining	O
account	O
of	O
the	O
interplay	O
between	O
theory	B
and	O
methodological	O
development	O
that	O
has	O
driven	O
statistics	B
in	O
the	O
computer	O
age	O
.	O
the	O
authors	O
succeed	O
brilliantly	O
in	O
locating	O
contemporary	O
algorithmic	O
methodologies	O
for	O
analysis	B
of	O
‘	O
big	O
data	B
’	O
within	O
the	O
framework	O
of	O
established	O
statistical	O
theory.	O
”	O
—	O
alastair	O
young	O
,	O
imperial	O
college	O
london	O
“	O
this	O
is	O
a	O
guided	O
tour	O
of	O
modern	O
statistics	B
that	O
emphasizes	O
the	O
conceptual	O
and	O
computational	O
advances	O
of	O
the	O
last	O
century	O
.	O
authored	O
by	O
two	O
masters	O
of	O
the	O
field	O
,	O
it	O
offers	O
just	O
the	O
right	O
mix	O
of	O
mathematical	O
analysis	B
and	O
insightful	O
commentary.	O
”	O
—	O
hal	O
varian	O
,	O
google	O
“	O
efron	O
and	O
hastie	O
guide	O
us	O
through	O
the	O
maze	O
of	O
breakthrough	O
statistical	O
methodologies	O
following	O
the	O
computing	O
evolution	O
:	O
why	O
they	O
were	O
developed	O
,	O
their	O
properties	O
,	O
and	O
how	O
they	O
are	O
used	O
.	O
highlighting	O
their	O
origins	O
,	O
the	O
book	O
helps	O
us	O
understand	O
each	O
method	B
’	O
s	O
roles	O
in	O
inference	B
and/or	O
prediction.	O
”	O
—	O
galit	O
shmueli	O
,	O
national	O
tsing	O
hua	O
university	O
“	O
a	O
masterful	O
guide	O
to	O
how	O
the	O
inferential	O
bases	O
of	O
classical	O
statistics	B
can	O
provide	O
a	O
principled	O
disciplinary	O
frame	O
for	O
the	O
data	B
science	O
of	O
the	O
twenty-first	O
century.	O
”	O
—	O
stephen	O
stigler	O
,	O
university	O
of	O
chicago	O
,	O
author	O
of	O
seven	O
pillars	O
of	O
statistical	O
wisdom	O
“	O
a	O
refreshing	O
view	O
of	O
modern	O
statistics	B
.	O
algorithmics	O
are	O
put	O
on	O
equal	O
footing	O
with	O
intuition	O
,	O
properties	O
,	O
and	O
the	O
abstract	O
arguments	O
behind	O
them	O
.	O
the	O
methods	O
covered	O
are	O
indispensable	O
to	O
practicing	O
statistical	O
analysts	O
in	O
today	O
’	O
s	O
big	O
data	B
and	O
big	O
computing	O
landscape.	O
”	O
—	O
robert	O
gramacy	O
,	O
the	O
university	O
of	O
chicago	O
booth	O
school	O
of	O
businessbradley	O
efron	O
is	O
max	O
h.	O
stein	O
professor	O
,	O
professor	O
of	O
statistics	B
,	O
and	O
professor	O
of	O
biomedical	O
data	B
science	O
at	O
stanford	O
university	O
.	O
he	O
has	O
held	O
visiting	O
faculty	O
appointments	O
at	O
harvard	O
,	O
uc	O
berkeley	O
,	O
and	O
imperial	O
college	O
london	O
.	O
efron	O
has	O
worked	O
extensively	O
on	O
theories	O
of	O
statistical	O
inference	B
,	O
and	O
is	O
the	O
inventor	O
of	O
the	O
bootstrap	O
sampling	O
technique	O
.	O
he	O
received	O
the	O
national	O
medal	O
of	O
science	O
in	O
2005	O
and	O
the	O
guy	O
medal	O
in	O
gold	O
of	O
the	O
royal	O
statistical	O
society	O
in	O
2014.	O
trevor	O
hastie	O
is	O
john	O
a.	O
overdeck	O
professor	O
,	O
professor	O
of	O
statistics	B
,	O
and	O
professor	O
of	O
biomedical	O
data	B
science	O
at	O
stanford	O
university	O
.	O
he	O
is	O
coauthor	O
of	O
elements	O
of	O
statistical	O
learning	O
,	O
a	O
key	O
text	O
in	O
the	O
field	O
of	O
modern	O
data	B
analysis	O
.	O
he	O
is	O
also	O
known	O
for	O
his	O
work	O
on	O
generalized	O
additive	O
models	B
and	O
principal	O
curves	O
,	O
and	O
for	O
his	O
contributions	O
to	O
the	O
r	O
computing	O
environment	O
.	O
hastie	O
was	O
awarded	O
the	O
emmanuel	O
and	O
carol	O
parzen	O
prize	O
for	O
statistical	O
innovation	O
in	O
2014.	O
institute	O
of	O
mathematical	O
statistics	B
monographseditorial	O
board	O
:	O
d.	O
r.	O
cox	O
(	O
university	O
of	O
oxford	O
)	O
b.	O
hambly	O
(	O
university	O
of	O
oxford	O
)	O
s.	O
holmes	O
(	O
stanford	O
university	O
)	O
j.	O
wellner	O
(	O
university	O
of	O
washington	O
)	O
cover	O
illustration	O
:	O
pacific	O
ocean	O
wave	O
,	O
north	O
shore	O
,	O
oahu	O
,	O
hawaii	O
.	O
©	O
brian	O
sytnyk	O
/	O
getty	O
images.cover	O
designed	O
by	O
zoe	O
naylor.printed	O
in	O
the	O
united	O
kingdomcomputer	O
age	O
statistical	O
inferencealgorithms	O
,	O
evidence	O
,	O
and	O
data	O
sciencebradley	O
efron	O
trevor	O
hastie9781107149892	O
efron	O
&	O
hastie	O
jkt	O
c	O
m	O
y	O
k	O
computer	O
age	O
statistical	O
inference	B
algorithms	O
,	O
evidence	O
,	O
and	O
data	O
science	O
bradley	O
efron	O
trevor	O
hastie	O
stanford	O
university	O
to	O
donna	O
and	O
lynda	O
viii	O
contents	O
preface	O
acknowledgments	O
notation	O
1	O
1.1	O
1.2	O
1.3	O
2	O
2.1	O
2.2	O
2.3	O
3	O
3.1	O
3.2	O
3.3	O
3.4	O
3.5	O
4	O
4.1	O
4.2	O
4.3	O
4.4	O
4.5	O
5	O
part	O
i	O
classic	O
statistical	O
inference	B
algorithms	O
and	O
inference	O
a	O
regression	B
example	O
hypothesis	B
testing	I
notes	O
frequentist	O
inference	B
frequentism	O
in	O
practice	O
frequentist	O
optimality	O
notes	O
and	O
details	O
bayesian	O
inference	B
two	O
examples	O
uninformative	O
prior	B
distributions	O
flaws	O
in	O
frequentist	O
inference	B
a	O
bayesian/frequentist	O
comparison	O
list	O
notes	O
and	O
details	O
fisherian	O
inference	B
and	O
maximum	B
likelihood	I
estimation	O
likelihood	B
and	O
maximum	B
likelihood	I
fisher	O
information	B
and	O
the	O
mle	O
conditional	O
inference	B
permutation	O
and	O
randomization	O
notes	O
and	O
details	O
parametric	B
models	O
and	O
exponential	O
families	O
ix	O
xv	O
xviii	O
xix	O
1	O
3	O
4	O
8	O
11	O
12	O
14	O
18	O
20	O
22	O
24	O
28	O
30	O
33	O
36	O
38	O
38	O
41	O
45	O
49	O
51	O
53	O
x	O
5.1	O
5.2	O
5.3	O
5.4	O
5.5	O
5.6	O
6	O
6.1	O
6.2	O
6.3	O
6.4	O
6.5	O
7	O
7.1	O
7.2	O
7.3	O
7.4	O
7.5	O
8	O
8.1	O
8.2	O
8.3	O
8.4	O
8.5	O
contents	O
univariate	O
families	O
the	O
multivariate	B
normal	O
distribution	B
fisher	O
’	O
s	O
information	B
bound	O
for	O
multiparameter	O
families	O
the	O
multinomial	O
distribution	B
exponential	O
families	O
notes	O
and	O
details	O
part	O
ii	O
early	O
computer-age	O
methods	O
empirical	B
bayes	O
robbins	O
’	O
formula	B
the	O
missing-species	O
problem	O
a	O
medical	O
example	O
indirect	O
evidence	O
1	O
notes	O
and	O
details	O
james–stein	O
estimation	B
and	O
ridge	B
regression	I
the	O
james–stein	O
estimator	B
the	O
baseball	B
players	O
ridge	B
regression	I
indirect	O
evidence	O
2	O
notes	O
and	O
details	O
generalized	O
linear	B
models	O
and	O
regression	O
trees	B
logistic	O
regression	B
generalized	O
linear	B
models	O
poisson	O
regression	B
regression	O
trees	B
notes	O
and	O
details	O
survival	O
analysis	B
and	O
the	O
em	O
algorithm	B
life	O
tables	O
and	O
hazard	O
rates	O
censored	O
data	B
and	O
the	O
kaplan–meier	O
estimate	B
the	O
log-rank	O
test	O
the	O
proportional	O
hazards	O
model	B
9	O
9.1	O
9.2	O
9.3	O
9.4	O
9.5	O
missing	B
data	I
and	O
the	O
em	O
algorithm	B
9.6	O
notes	O
and	O
details	O
the	O
jackknife	O
and	O
the	O
bootstrap	O
10	O
10.1	O
the	O
jackknife	O
estimate	O
of	O
standard	B
error	I
10.2	O
the	O
nonparametric	B
bootstrap	O
10.3	O
resampling	O
plans	B
54	O
55	O
59	O
61	O
64	O
69	O
73	O
75	O
75	O
78	O
84	O
88	O
88	O
91	O
91	O
94	O
97	O
102	O
104	O
108	O
109	O
116	O
120	O
124	O
128	O
131	O
131	O
134	O
139	O
143	O
146	O
150	O
155	O
156	O
159	O
162	O
contents	O
10.4	O
the	O
parametric	B
bootstrap	O
10.5	O
10.6	O
notes	O
and	O
details	O
inﬂuence	O
functions	O
and	O
robust	O
estimation	B
bootstrap	O
conﬁdence	B
intervals	I
11	O
11.1	O
neyman	O
’	O
s	O
construction	O
for	O
one-parameter	B
problems	O
11.2	O
the	O
percentile	B
method	I
11.3	O
bias-corrected	O
conﬁdence	B
intervals	I
11.4	O
second-order	O
accuracy	O
11.5	O
bootstrap-t	O
intervals	B
11.6	O
objective	O
bayes	O
intervals	B
and	O
the	O
conﬁdence	O
distribution	O
11.7	O
notes	O
and	O
details	O
cross-validation	O
and	O
cp	O
estimates	O
of	O
prediction	O
error	O
12	O
12.1	O
prediction	O
rules	O
12.2	O
cross-validation	O
12.3	O
covariance	O
penalties	O
12.4	O
training	O
,	O
validation	O
,	O
and	O
ephemeral	O
predictors	O
12.5	O
notes	O
and	O
details	O
objective	O
bayes	O
inference	B
and	O
mcmc	O
13	O
13.1	O
objective	O
prior	O
distributions	O
13.2	O
conjugate	B
prior	O
distributions	O
13.3	O
model	B
selection	I
and	O
the	O
bayesian	O
information	B
criterion	I
13.4	O
gibbs	O
sampling	O
and	O
mcmc	O
13.5	O
example	O
:	O
modeling	O
population	O
admixture	O
13.6	O
notes	O
and	O
details	O
14	O
postwar	O
statistical	O
inference	B
and	O
methodology	O
part	O
iii	O
twenty-first-century	O
topics	O
large-scale	O
hypothesis	O
testing	B
and	O
fdrs	O
15	O
15.1	O
large-scale	B
testing	I
15.2	O
false-discovery	O
rates	O
15.3	O
empirical	B
bayes	O
large-scale	B
testing	I
15.4	O
local	O
false-discovery	O
rates	O
15.5	O
choice	O
of	O
the	O
null	O
distribution	B
15.6	O
relevance	O
15.7	O
notes	O
and	O
details	O
16	O
sparse	O
modeling	O
and	O
the	O
lasso	B
xi	O
169	O
174	O
177	O
181	O
181	O
185	O
190	O
192	O
195	O
198	O
204	O
208	O
208	O
213	O
218	O
227	O
230	O
233	O
234	O
237	O
243	O
251	O
256	O
261	O
264	O
269	O
271	O
272	O
275	O
278	O
282	O
286	O
290	O
294	O
298	O
xii	O
contents	O
16.1	O
forward	O
stepwise	O
regression	B
16.2	O
the	O
lasso	B
16.3	O
fitting	O
lasso	B
models	O
16.4	O
least-angle	O
regression	B
16.5	O
fitting	O
generalized	O
lasso	B
models	O
16.6	O
post-selection	O
inference	B
for	O
the	O
lasso	B
16.7	O
connections	O
and	O
extensions	O
16.8	O
notes	O
and	O
details	O
random	O
forests	O
and	O
boosting	O
17	O
17.1	O
random	O
forests	O
17.2	O
boosting	O
with	O
squared-error	O
loss	O
17.3	O
gradient	O
boosting	O
17.4	O
adaboost	O
:	O
the	O
original	O
boosting	O
algorithm	B
17.5	O
connections	O
and	O
extensions	O
17.6	O
notes	O
and	O
details	O
neural	O
networks	O
and	O
deep	O
learning	O
18	O
18.1	O
neural	O
networks	O
and	O
the	O
handwritten	O
digit	O
problem	O
18.2	O
fitting	O
a	O
neural	O
network	O
18.3	O
autoencoders	O
18.4	O
deep	O
learning	O
18.5	O
learning	O
a	O
deep	O
network	O
18.6	O
notes	O
and	O
details	O
support-vector	O
machines	O
and	O
kernel	O
methods	O
19	O
19.1	O
optimal	O
separating	O
hyperplane	O
19.2	O
soft-margin	O
classiﬁer	O
19.3	O
svm	O
criterion	O
as	O
loss	O
plus	O
penalty	B
19.4	O
computations	B
and	O
the	O
kernel	O
trick	B
19.5	O
function	B
fitting	O
using	O
kernels	O
19.6	O
example	O
:	O
string	O
kernels	O
for	O
protein	O
classiﬁcation	O
19.7	O
svms	O
:	O
concluding	O
remarks	O
19.8	O
kernel	O
smoothing	B
and	O
local	O
regression	B
19.9	O
notes	O
and	O
details	O
inference	B
after	O
model	B
selection	I
20	O
20.1	O
simultaneous	O
conﬁdence	B
intervals	I
20.2	O
accuracy	O
after	O
model	B
selection	I
20.3	O
selection	O
bias	O
20.4	O
combined	O
bayes–frequentist	O
estimation	B
20.5	O
notes	O
and	O
details	O
299	O
303	O
308	O
309	O
313	O
317	O
319	O
321	O
324	O
325	O
333	O
338	O
341	O
345	O
347	O
351	O
353	O
356	O
362	O
364	O
368	O
371	O
375	O
376	O
378	O
379	O
381	O
384	O
385	O
387	O
387	O
390	O
394	O
395	O
402	O
408	O
412	O
417	O
contents	O
empirical	B
bayes	O
estimation	B
strategies	I
21	O
21.1	O
bayes	O
deconvolution	B
21.2	O
g-modeling	B
and	O
estimation	B
21.3	O
likelihood	B
,	O
regularization	B
,	O
and	O
accuracy	O
21.4	O
two	O
examples	O
21.5	O
generalized	O
linear	O
mixed	O
models	O
21.6	O
deconvolution	B
and	O
f	B
-modeling	I
21.7	O
notes	O
and	O
details	O
epilogue	O
references	O
author	O
index	O
subject	O
index	O
xiii	O
421	O
421	O
424	O
427	O
432	O
437	O
440	O
444	O
446	O
453	O
463	O
467	O
xiv	O
preface	O
statistical	O
inference	B
is	O
an	O
unusually	O
wide-ranging	O
discipline	O
,	O
located	O
as	O
it	O
is	O
at	O
the	O
triple-point	O
of	O
mathematics	O
,	O
empirical	B
science	O
,	O
and	O
philosophy	O
.	O
the	O
discipline	O
can	O
be	O
said	O
to	O
date	O
from	O
1763	O
,	O
with	O
the	O
publication	O
of	O
bayes	O
’	O
rule	B
(	O
representing	O
the	O
philosophical	O
side	O
of	O
the	O
subject	O
;	O
the	O
rule	B
’	O
s	O
early	O
ad-	O
vocates	O
considered	O
it	O
an	O
argument	B
for	O
the	O
existence	O
of	O
god	O
)	O
.	O
the	O
most	O
re-	O
cent	O
quarter	O
of	O
this	O
250-year	O
history—from	O
the	O
1950s	O
to	O
the	O
present—is	O
the	O
“	O
computer	O
age	O
”	O
of	O
our	O
book	O
’	O
s	O
title	O
,	O
the	O
time	O
when	O
computation	O
,	O
the	O
tra-	O
ditional	O
bottleneck	O
of	O
statistical	O
applications	O
,	O
became	O
faster	O
and	O
easier	O
by	O
a	O
factor	B
of	O
a	O
million	O
.	O
the	O
book	O
is	O
an	O
examination	O
of	O
how	O
statistics	B
has	O
evolved	O
over	O
the	O
past	O
sixty	O
years—an	O
aerial	O
view	O
of	O
a	O
vast	O
subject	O
,	O
but	O
seen	O
from	O
the	O
height	O
of	O
a	O
small	O
plane	O
,	O
not	O
a	O
jetliner	O
or	O
satellite	O
.	O
the	O
individual	O
chapters	O
take	O
up	O
a	O
se-	O
ries	O
of	O
inﬂuential	O
topics—generalized	O
linear	B
models	O
,	O
survival	O
analysis	B
,	O
the	O
jackknife	O
and	O
bootstrap	O
,	O
false-discovery	O
rates	O
,	O
empirical	B
bayes	O
,	O
mcmc	O
,	O
neural	O
nets	O
,	O
and	O
a	O
dozen	O
more—describing	O
for	O
each	O
the	O
key	O
methodologi-	O
cal	O
developments	O
and	O
their	O
inferential	O
justiﬁcation	O
.	O
needless	O
to	O
say	O
,	O
the	O
role	O
of	O
electronic	O
computation	O
is	O
central	O
to	O
our	O
story	O
.	O
this	O
doesn	O
’	O
t	B
mean	O
that	O
every	O
advance	O
was	O
computer-related	O
.	O
a	O
land	O
bridge	O
had	O
opened	O
to	O
a	O
new	O
continent	O
but	O
not	O
all	O
were	O
eager	O
to	O
cross	O
.	O
topics	O
such	O
as	O
empirical	B
bayes	O
and	O
james–stein	O
estimation	B
could	O
have	O
emerged	O
just	O
as	O
well	O
under	O
the	O
constraints	O
of	O
mechanical	O
computation	O
.	O
oth-	O
ers	O
,	O
like	O
the	O
bootstrap	O
and	O
proportional	O
hazards	O
,	O
were	O
pureborn	O
children	O
of	O
the	O
computer	O
age	O
.	O
almost	O
all	O
topics	O
in	O
twenty-ﬁrst-century	O
statistics	B
are	O
now	O
computer-dependent	O
,	O
but	O
it	O
will	O
take	O
our	O
small	O
plane	O
a	O
while	O
to	O
reach	O
the	O
new	O
millennium	O
.	O
dictionary	O
deﬁnitions	O
of	O
statistical	O
inference	B
tend	O
to	O
equate	O
it	O
with	O
the	O
entire	O
discipline	O
.	O
this	O
has	O
become	O
less	O
satisfactory	O
in	O
the	O
“	O
big	O
data	B
”	O
era	O
of	O
immense	O
computer-based	O
processing	O
algorithms	O
.	O
here	O
we	O
will	O
attempt	O
,	O
not	O
always	O
consistently	O
,	O
to	O
separate	O
the	O
two	O
aspects	O
of	O
the	O
statistical	O
enterprise	O
:	O
algorithmic	O
developments	O
aimed	O
at	O
speciﬁc	O
problem	O
areas	O
,	O
for	O
instance	O
xv	O
xvi	O
preface	O
random	O
forests	O
for	O
prediction	O
,	O
as	O
distinct	O
from	O
the	O
inferential	O
arguments	O
offered	O
in	O
their	O
support	O
.	O
very	O
broadly	O
speaking	O
,	O
algorithms	O
are	O
what	O
statisticians	O
do	O
while	O
infer-	O
ence	O
says	O
why	O
they	O
do	O
them	O
.	O
a	O
particularly	O
energetic	O
brand	O
of	O
the	O
statisti-	O
cal	O
enterprise	O
has	O
ﬂourished	O
in	O
the	O
new	O
century	O
,	O
data	B
science	O
,	O
emphasizing	O
algorithmic	O
thinking	O
rather	O
than	O
its	O
inferential	O
justiﬁcation	O
.	O
the	O
later	O
chap-	O
ters	O
of	O
our	O
book	O
,	O
where	O
large-scale	O
prediction	O
algorithms	O
such	O
as	O
boosting	O
and	O
deep	O
learning	O
are	O
examined	O
,	O
illustrate	O
the	O
data-science	O
point	O
of	O
view	O
.	O
(	O
see	O
the	O
epilogue	O
for	O
a	O
little	O
more	O
on	O
the	O
sometimes	O
fraught	O
statistics/data	O
science	O
marriage	O
.	O
)	O
there	O
are	O
no	O
such	O
subjects	O
as	O
biological	O
inference	B
or	O
astronomical	O
in-	O
ference	O
or	O
geological	O
inference	B
.	O
why	O
do	O
we	O
need	O
“	O
statistical	O
inference	B
”	O
?	O
the	O
answer	O
is	O
simple	O
:	O
the	O
natural	O
sciences	O
have	O
nature	O
to	O
judge	O
the	O
ac-	O
curacy	O
of	O
their	O
ideas	O
.	O
statistics	B
operates	O
one	O
step	O
back	O
from	O
nature	O
,	O
most	O
often	O
interpreting	O
the	O
observations	O
of	O
natural	O
scientists	O
.	O
without	O
nature	O
to	O
serve	O
as	O
a	O
disinterested	O
referee	O
,	O
we	O
need	O
a	O
system	O
of	O
mathematical	O
logic	O
for	O
guidance	O
and	O
correction	O
.	O
statistical	O
inference	B
is	O
that	O
system	O
,	O
distilled	O
from	O
two	O
and	O
a	O
half	O
centuries	O
of	O
data-analytic	O
experience	O
.	O
the	O
book	O
proceeds	O
historically	O
,	O
in	O
three	O
parts	O
.	O
the	O
great	O
themes	O
of	O
clas-	O
sical	O
inference	B
,	O
bayesian	O
,	O
frequentist	O
,	O
and	O
fisherian	O
,	O
reviewed	O
in	O
part	O
i	O
,	O
were	O
set	B
in	O
place	O
before	O
the	O
age	O
of	O
electronic	O
computation	O
.	O
modern	O
practice	O
has	O
vastly	O
extended	O
their	O
reach	O
without	O
changing	O
the	O
basic	O
outlines	O
.	O
(	O
an	O
analogy	O
with	O
classical	O
and	O
modern	O
literature	O
might	O
be	O
made	O
.	O
)	O
part	O
ii	O
con-	O
cerns	O
early	O
computer-age	O
developments	O
,	O
from	O
the	O
1950s	O
through	O
the	O
1990s	O
.	O
as	O
a	O
transitional	O
period	O
,	O
this	O
is	O
the	O
time	O
when	O
it	O
is	O
easiest	O
to	O
see	O
the	O
ef-	O
fects	O
,	O
or	O
noneffects	O
,	O
of	O
fast	O
computation	O
on	O
the	O
progress	O
of	O
statistical	O
meth-	O
odology	O
,	O
both	O
in	O
its	O
theory	B
and	O
practice	O
.	O
part	O
iii	O
,	O
“	O
twenty-first-century	O
topics	O
,	O
”	O
brings	O
the	O
story	O
up	O
to	O
the	O
present	O
.	O
ours	O
is	O
a	O
time	O
of	O
enormously	O
ambitious	O
algorithms	O
(	O
“	O
machine	O
learning	O
”	O
being	O
the	O
somewhat	O
disquieting	O
catchphrase	O
)	O
.	O
their	O
justiﬁcation	O
is	O
the	O
ongoing	O
task	O
of	O
modern	O
statistical	O
inference	B
.	O
neither	O
a	O
catalog	O
nor	O
an	O
encyclopedia	O
,	O
the	O
book	O
’	O
s	O
topics	O
were	O
chosen	O
as	O
apt	O
illustrations	O
of	O
the	O
interplay	O
between	O
computational	O
methodology	O
and	O
inferential	O
theory	B
.	O
some	O
missing	O
topics	O
that	O
might	O
have	O
served	O
just	O
as	O
well	O
include	O
time	O
series	O
,	O
general	O
estimating	O
equations	O
,	O
causal	O
inference	B
,	O
graph-	O
ical	O
models	B
,	O
and	O
experimental	O
design	O
.	O
in	O
any	O
case	O
,	O
there	O
is	O
no	O
implication	O
that	O
the	O
topics	O
presented	O
here	O
are	O
the	O
only	O
ones	O
worthy	O
of	O
discussion	O
.	O
also	O
underrepresented	O
are	O
asymptotics	O
and	O
decision	O
theory	B
,	O
the	O
“	O
math	O
stat	O
”	O
side	O
of	O
the	O
ﬁeld	O
.	O
our	O
intention	O
was	O
to	O
maintain	O
a	O
technical	O
level	O
of	O
discussion	O
appropriate	O
to	O
masters	O
’	O
-level	O
statisticians	O
or	O
ﬁrst-year	O
phd	O
stu-	O
preface	O
xvii	O
dents	O
.	O
inevitably	O
,	O
some	O
of	O
the	O
presentation	O
drifts	O
into	O
more	O
difﬁcult	O
waters	O
,	O
more	O
from	O
the	O
nature	O
of	O
the	O
statistical	O
ideas	O
than	O
the	O
mathematics	O
.	O
readers	O
who	O
ﬁnd	O
our	O
aerial	O
view	O
circling	O
too	O
long	O
over	O
some	O
topic	O
shouldn	O
’	O
t	B
hesi-	O
tate	O
to	O
move	O
ahead	O
in	O
the	O
book	O
.	O
for	O
the	O
most	O
part	O
,	O
the	O
chapters	O
can	O
be	O
read	O
independently	O
of	O
each	O
other	O
(	O
though	O
there	O
is	O
a	O
connecting	O
overall	O
theme	O
)	O
.	O
this	O
comment	O
applies	O
especially	O
to	O
nonstatisticians	O
who	O
have	O
picked	O
up	O
the	O
book	O
because	O
of	O
interest	O
in	O
some	O
particular	O
topic	O
,	O
say	O
survival	O
analysis	B
or	O
boosting	O
.	O
useful	O
disciplines	O
that	O
serve	O
a	O
wide	O
variety	O
of	O
demanding	O
clients	O
run	O
the	O
risk	O
of	O
losing	O
their	O
center	O
.	O
statistics	B
has	O
managed	O
,	O
for	O
the	O
most	O
part	O
,	O
to	O
maintain	O
its	O
philosophical	O
cohesion	O
despite	O
a	O
rising	O
curve	O
of	O
outside	O
de-	O
mand	O
.	O
the	O
center	O
of	O
the	O
ﬁeld	O
has	O
in	O
fact	O
moved	O
in	O
the	O
past	O
sixty	O
years	O
,	O
from	O
its	O
traditional	O
home	O
in	O
mathematics	O
and	O
logic	O
toward	O
a	O
more	O
computational	O
focus	O
.	O
our	O
book	O
traces	O
that	O
movement	O
on	O
a	O
topic-by-topic	O
basis	O
.	O
an	O
answer	O
to	O
the	O
intriguing	O
question	O
“	O
what	O
happens	O
next	O
?	O
”	O
won	O
’	O
t	B
be	O
attempted	O
here	O
,	O
except	O
for	O
a	O
few	O
words	O
in	O
the	O
epilogue	O
,	O
where	O
the	O
rise	O
of	O
data	B
science	O
is	O
discussed	O
.	O
acknowledgments	O
we	O
are	O
indebted	O
to	O
cindy	O
kirby	O
for	O
her	O
skillful	O
work	O
in	O
the	O
preparation	O
of	O
this	O
book	O
,	O
and	O
galit	O
shmueli	O
for	O
her	O
helpful	O
comments	O
on	O
an	O
earlier	O
draft	O
.	O
at	O
cambridge	O
university	O
press	O
,	O
a	O
huge	O
thank	O
you	O
to	O
steven	O
holt	O
for	O
his	O
ex-	O
cellent	O
copy	O
editing	O
,	O
clare	O
dennison	O
for	O
guiding	O
us	O
through	O
the	O
production	O
phase	O
,	O
and	O
to	O
diana	O
gillooly	O
,	O
our	O
editor	O
,	O
for	O
her	O
unfailing	O
support	O
.	O
bradley	O
efron	O
trevor	O
hastie	O
department	O
of	O
statistics	B
stanford	O
university	O
may	O
2016	O
xviii	O
notation	O
xix	O
throughout	O
the	O
book	O
the	O
numbered	O
	O
sign	O
indicates	O
a	O
technical	O
note	O
or	O
reference	O
element	O
which	O
is	O
elaborated	O
on	O
at	O
the	O
end	O
of	O
the	O
chapter	O
.	O
there	O
,	O
next	O
to	O
the	O
number	O
,	O
the	O
page	O
number	O
of	O
the	O
referenced	O
location	O
is	O
given	O
in	O
parenthesis	O
.	O
for	O
example	O
,	O
lowess	B
in	O
the	O
notes	O
on	O
page	O
11	O
was	O
referenced	O
via	O
a	O
1	O
on	O
page	O
6.	O
matrices	O
such	O
as	O
†	O
are	O
represented	O
in	O
bold	O
font	O
,	O
as	O
are	O
certain	O
vectors	O
such	O
as	O
y	O
,	O
a	O
data	B
vector	O
with	O
n	O
elements	O
.	O
most	O
other	O
vectors	O
,	O
such	O
as	O
coefﬁcient	O
vectors	O
,	O
are	O
typically	O
not	O
bold	O
.	O
we	O
use	O
a	O
dark	O
green	O
typewriter	O
font	O
to	O
indicate	O
data	B
set	O
names	O
such	O
as	O
prostate	B
,	O
variable	O
names	O
such	O
as	O
prog	O
from	O
data	O
sets	O
,	O
and	O
r	O
commands	O
such	O
as	O
glmnet	B
or	O
locfdr	B
.	O
no	O
bibliographic	O
references	O
are	O
given	O
in	O
the	O
body	O
of	O
the	O
text	O
;	O
important	O
references	O
are	O
given	O
in	O
the	O
endnotes	O
of	O
each	O
chapter	O
.	O
part	O
i	O
classic	O
statistical	O
inference	B
1	O
algorithms	O
and	O
inference	O
statistics	B
is	O
the	O
science	O
of	O
learning	O
from	O
experience	O
,	O
particularly	O
experi-	O
ence	O
that	O
arrives	O
a	O
little	O
bit	O
at	O
a	O
time	O
:	O
the	O
successes	O
and	O
failures	O
of	O
a	O
new	O
experimental	O
drug	O
,	O
the	O
uncertain	O
measurements	O
of	O
an	O
asteroid	O
’	O
s	O
path	B
to-	O
ward	O
earth	O
.	O
it	O
may	O
seem	O
surprising	O
that	O
any	O
one	O
theory	B
can	O
cover	O
such	O
an	O
amorphous	O
target	O
as	O
“	O
learning	O
from	O
experience.	O
”	O
in	O
fact	O
,	O
there	O
are	O
two	O
main	O
statistical	O
theories	O
,	O
bayesianism	O
and	O
frequentism	O
,	O
whose	O
connections	O
and	O
disagreements	O
animate	O
many	O
of	O
the	O
succeeding	O
chapters	O
.	O
first	O
,	O
however	O
,	O
we	O
want	O
to	O
discuss	O
a	O
less	O
philosophical	O
,	O
more	O
operational	O
division	O
of	O
labor	O
that	O
applies	O
to	O
both	O
theories	O
:	O
between	O
the	O
algorithmic	O
and	O
inferential	O
aspects	O
of	O
statistical	O
analysis	B
.	O
the	O
distinction	O
begins	O
with	O
the	O
most	O
basic	O
,	O
and	O
most	O
popular	O
,	O
statistical	O
method	B
,	O
averaging	B
.	O
suppose	O
we	O
have	O
observed	O
numbers	O
x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn	O
applying	O
to	O
some	O
phenomenon	O
of	O
interest	O
,	O
perhaps	O
the	O
automobile	O
accident	O
rates	O
in	O
the	O
n	O
d	O
50	O
states	O
.	O
the	O
mean	O
xi	O
=n	O
(	O
1.1	O
)	O
nx	O
d	O
nx	O
id1	O
summarizes	O
the	O
results	O
in	O
a	O
single	O
number	O
.	O
how	O
accurate	O
is	O
that	O
number	O
?	O
the	O
textbook	O
answer	O
is	O
given	O
in	O
terms	O
of	O
the	O
standard	B
error	I
,	O
bse	O
d	O
.xi	O
(	O
cid:0	O
)	O
nx/2ı	O
.n.n	O
(	O
cid:0	O
)	O
1//	O
#	O
1=2	O
''	O
nx	O
id1	O
:	O
(	O
1.2	O
)	O
here	O
averaging	B
(	O
1.1	O
)	O
is	O
the	O
algorithm	B
,	O
while	O
the	O
standard	B
error	I
provides	O
an	O
inference	B
of	O
the	O
algorithm	B
’	O
s	O
accuracy	O
.	O
it	O
is	O
a	O
surprising	O
,	O
and	O
crucial	O
,	O
aspect	O
of	O
statistical	O
theory	B
that	O
the	O
same	O
data	B
that	O
supplies	O
an	O
estimate	B
can	O
also	O
assess	O
its	O
accuracy.1	O
1	O
“	O
inference	B
”	O
concerns	O
more	O
than	O
accuracy	O
:	O
speaking	O
broadly	O
,	O
algorithms	O
say	O
what	O
the	O
statistician	O
does	O
while	O
inference	B
says	O
why	O
he	O
or	O
she	O
does	O
it	O
.	O
3	O
4	O
of	O
course	O
,	O
bse	O
(	O
1.2	O
)	O
is	O
itself	O
an	O
algorithm	B
,	O
which	O
could	O
be	O
(	O
and	O
is	O
)	O
subject	O
algorithms	O
and	O
inference	O
to	O
further	O
inferential	O
analysis	B
concerning	O
its	O
accuracy	O
.	O
the	O
point	O
is	O
that	O
the	O
algorithm	B
comes	O
ﬁrst	O
and	O
the	O
inference	B
follows	O
at	O
a	O
second	O
level	O
of	O
statistical	O
consideration	O
.	O
in	O
practice	O
this	O
means	O
that	O
algorithmic	O
invention	O
is	O
a	O
more	O
free-wheeling	O
and	O
adventurous	O
enterprise	O
,	O
with	O
inference	B
playing	O
catch-up	O
as	O
it	O
strives	O
to	O
assess	O
the	O
accuracy	O
,	O
good	O
or	O
bad	O
,	O
of	O
some	O
hot	O
new	O
algorithmic	O
methodology	O
.	O
if	O
the	O
inference/algorithm	O
race	O
is	O
a	O
tortoise-and-hare	O
affair	O
,	O
then	O
modern	O
electronic	O
computation	O
has	O
bred	O
a	O
bionic	O
hare	O
.	O
there	O
are	O
two	O
effects	O
at	O
work	O
here	O
:	O
computer-based	O
technology	O
allows	O
scientists	O
to	O
collect	O
enormous	O
data	B
sets	O
,	O
orders	O
of	O
magnitude	O
larger	O
than	O
those	O
that	O
classic	O
statistical	O
theory	B
was	O
designed	O
to	O
deal	O
with	O
;	O
huge	O
data	B
demands	O
new	O
methodology	O
,	O
and	O
the	O
demand	O
is	O
being	O
met	O
by	O
a	O
burst	O
of	O
innovative	O
computer-based	O
statistical	O
algorithms	O
.	O
when	O
one	O
reads	O
of	O
“	O
big	O
data	B
”	O
in	O
the	O
news	O
,	O
it	O
is	O
usually	O
these	O
algorithms	O
playing	O
the	O
starring	O
roles	O
.	O
our	O
book	O
’	O
s	O
title	O
,	O
computer	O
age	O
statistical	O
inference	B
,	O
emphasizes	O
the	O
tor-	O
toise	O
’	O
s	O
side	O
of	O
the	O
story	O
.	O
the	O
past	O
few	O
decades	O
have	O
been	O
a	O
golden	O
age	O
of	O
statistical	O
methodology	O
.	O
it	O
hasn	O
’	O
t	B
been	O
,	O
quite	O
,	O
a	O
golden	O
age	O
for	O
statistical	O
inference	B
,	O
but	O
it	O
has	O
not	O
been	O
a	O
dark	O
age	O
either	O
.	O
the	O
efﬂorescence	O
of	O
am-	O
bitious	O
new	O
algorithms	O
has	O
forced	O
an	O
evolution	O
(	O
though	O
not	O
a	O
revolution	O
)	O
in	O
inference	B
,	O
the	O
theories	O
by	O
which	O
statisticians	O
choose	O
among	O
competing	O
methods	O
.	O
the	O
book	O
traces	O
the	O
interplay	O
between	O
methodology	O
and	O
infer-	O
ence	O
as	O
it	O
has	O
developed	O
since	O
the	O
1950s	O
,	O
the	O
beginning	O
of	O
our	O
discipline	O
’	O
s	O
computer	O
age	O
.	O
as	O
a	O
preview	O
,	O
we	O
end	O
this	O
chapter	O
with	O
two	O
examples	O
illus-	O
trating	O
the	O
transition	O
from	O
classic	O
to	O
computer-age	O
practice	O
.	O
1.1	O
a	O
regression	B
example	O
figure	O
1.1	O
concerns	O
a	O
study	O
of	O
kidney	B
function	I
.	O
data	B
points	O
.xi	O
;	O
yi	O
/	O
have	O
been	O
observed	O
for	O
n	O
d	O
157	O
healthy	O
volunteers	O
,	O
with	O
xi	O
the	O
ith	O
volunteer	O
’	O
s	O
age	O
in	O
years	O
,	O
and	O
yi	O
a	O
composite	O
measure	O
“	O
tot	O
”	O
of	O
overall	O
function	B
.	O
kid-	O
ney	O
function	B
generally	O
declines	O
with	O
age	O
,	O
as	O
evident	O
in	O
the	O
downward	O
scat-	O
ter	O
of	O
the	O
points	O
.	O
the	O
rate	B
of	O
decline	O
is	O
an	O
important	O
question	O
in	O
kidney	O
transplantation	O
:	O
in	O
the	O
past	O
,	O
potential	O
donors	O
past	O
age	O
60	O
were	O
prohibited	O
,	O
though	O
,	O
given	O
a	O
shortage	O
of	O
donors	O
,	O
this	O
is	O
no	O
longer	O
enforced	O
.	O
the	O
solid	O
line	O
in	O
figure	O
1.1	O
is	O
a	O
linear	B
regression	O
y	O
d	O
o	O
ˇ0	O
c	O
o	O
ˇ1x	O
(	O
1.3	O
)	O
ﬁt	O
to	O
the	O
data	B
by	O
least	B
squares	I
,	O
that	O
is	O
by	O
minimizing	O
the	O
sum	O
of	O
squared	O
1.1	O
a	O
regression	B
example	O
5	O
figure	O
1.1	O
kidney	O
ﬁtness	O
tot	O
vs	O
age	O
for	O
157	O
volunteers	O
.	O
the	O
line	O
is	O
a	O
linear	B
regression	O
ﬁt	O
,	O
showing	O
˙2	O
standard	O
errors	O
at	O
selected	O
values	O
of	O
age	O
.	O
deviations	O
nx	O
.yi	O
(	O
cid:0	O
)	O
ˇ0	O
(	O
cid:0	O
)	O
ˇ1xi	O
/2	O
id1	O
(	O
1.4	O
)	O
over	O
all	O
choices	O
of	O
.ˇ0	O
;	O
ˇ1/	O
.	O
the	O
least	B
squares	I
algorithm	O
,	O
which	O
dates	O
back	O
to	O
gauss	O
and	O
legendre	O
in	O
the	O
early	O
1800s	O
,	O
gives	O
o	O
ˇ1	O
d	O
(	O
cid:0	O
)	O
0:079	O
as	O
the	O
least	B
squares	I
estimates	O
.	O
we	O
can	O
read	O
off	O
of	O
the	O
ﬁtted	O
line	O
an	O
estimated	O
value	O
of	O
kidney	O
ﬁtness	O
for	O
any	O
chosen	O
age	O
.	O
the	O
top	O
line	O
of	O
table	O
1.1	O
shows	O
estimate	B
1.29	O
at	O
age	O
20	O
,	O
down	O
to	O
(	O
cid:0	O
)	O
3:43	O
at	O
age	O
80	O
.	O
ˇ0	O
d	O
2:86	O
and	O
o	O
how	O
accurate	O
are	O
these	O
estimates	O
?	O
this	O
is	O
where	O
inference	B
comes	O
in	O
:	O
an	O
extended	O
version	O
of	O
formula	B
(	O
1.2	O
)	O
,	O
also	O
going	O
back	O
to	O
the	O
1800s	O
,	O
pro-	O
vides	O
the	O
standard	O
errors	O
,	O
shown	O
in	O
line	O
2	O
of	O
the	O
table	O
.	O
the	O
vertical	O
bars	O
in	O
figure	O
1.1	O
are	O
˙	O
two	O
standard	O
errors	O
,	O
giving	O
them	O
about	O
95	O
%	O
chance	O
of	O
containing	O
the	O
true	O
expected	O
value	O
of	O
tot	O
at	O
each	O
age	O
.	O
that	O
95	O
%	O
coverage	O
depends	O
on	O
the	O
validity	O
of	O
the	O
linear	B
regression	O
model	B
(	O
1.3	O
)	O
.	O
we	O
might	O
instead	O
try	O
a	O
quadratic	O
regression	B
y	O
d	O
o	O
ˇ2x2	O
,	O
or	O
a	O
cubic	O
,	O
etc.	O
,	O
all	O
of	O
this	O
being	O
well	O
within	O
the	O
reach	O
of	O
pre-computer	O
statistical	O
theory	B
.	O
ˇ1x	O
c	O
o	O
ˇ0	O
c	O
o	O
*************************************************************************************************************************************************************2030405060708090−6−4−2024agetot	O
6	O
algorithms	O
and	O
inference	O
table	O
1.1	O
regression	B
analysis	O
of	O
the	O
kidney	O
data	O
;	O
(	O
1	O
)	O
linear	B
regression	O
estimates	O
;	O
(	O
2	O
)	O
their	O
standard	O
errors	O
;	O
(	O
3	O
)	O
lowess	B
estimates	O
;	O
(	O
4	O
)	O
their	O
bootstrap	O
standard	O
errors	O
.	O
age	O
1.	O
linear	B
regression	O
2.	O
std	O
error	O
3.	O
lowess	B
4.	O
bootstrap	O
std	O
error	O
20	O
1.29	O
.21	O
1.66	O
.71	O
30	O
80	O
.50	O
(	O
cid:0	O
)	O
.28	O
(	O
cid:0	O
)	O
1.07	O
(	O
cid:0	O
)	O
1.86	O
(	O
cid:0	O
)	O
2.64	O
(	O
cid:0	O
)	O
3.43	O
.15	O
.42	O
.65	O
(	O
cid:0	O
)	O
.59	O
(	O
cid:0	O
)	O
1.27	O
(	O
cid:0	O
)	O
1.91	O
(	O
cid:0	O
)	O
2.68	O
(	O
cid:0	O
)	O
3.50	O
.70	O
.23	O
.15	O
.31	O
60	O
.26	O
70	O
.34	O
.37	O
.47	O
40	O
50	O
.19	O
.32	O
figure	O
1.2	O
local	O
polynomial	O
lowess	B
(	O
x	O
,	O
y,1/3	O
)	O
ﬁt	O
to	O
the	O
kidney-ﬁtness	O
data	B
,	O
with	O
˙2	O
bootstrap	O
standard	B
deviations	I
.	O
1	O
a	O
modern	O
computer-based	O
algorithm	B
lowess	O
produced	O
the	O
somewhat	O
bumpy	O
regression	B
curve	O
in	O
figure	O
1.2.	O
the	O
lowess	B
	O
2	O
algorithm	B
moves	O
its	O
attention	O
along	O
the	O
x-axis	O
,	O
ﬁtting	B
local	O
polynomial	O
curves	O
of	O
differing	O
degrees	O
to	O
nearby	O
.x	O
;	O
y/	O
points	O
.	O
(	O
the	O
1/3	O
in	O
the	O
call3	O
lowess	B
(	O
x	O
,	O
y,1/3	O
)	O
2	O
here	O
and	O
throughout	O
the	O
book	O
,	O
the	O
numbered	O
	O
sign	O
indicates	O
a	O
technical	O
note	O
or	O
reference	O
element	O
which	O
is	O
elaborated	O
on	O
at	O
the	O
end	O
of	O
the	O
chapter	O
.	O
3	O
here	O
and	O
in	O
all	O
our	O
examples	O
we	O
are	O
employing	O
the	O
language	O
r	O
,	O
itself	O
one	O
of	O
the	O
key	O
developments	O
in	O
computer-based	O
statistical	O
methodology	O
.	O
*************************************************************************************************************************************************************2030405060708090−6−4−2024agetot	O
1.1	O
a	O
regression	B
example	O
7	O
determines	O
the	O
deﬁnition	O
of	O
local	O
.	O
)	O
repeated	O
passes	O
over	O
the	O
x-axis	O
reﬁne	O
the	O
ﬁt	O
,	O
reducing	O
the	O
effects	O
of	O
occasional	O
anomalous	O
points	O
.	O
the	O
ﬁtted	O
curve	O
in	O
figure	O
1.2	O
is	O
nearly	O
linear	B
at	O
the	O
right	O
,	O
but	O
more	O
complicated	O
at	O
the	O
left	O
where	O
points	O
are	O
more	O
densely	O
packed	O
.	O
it	O
is	O
ﬂat	O
between	O
ages	O
25	O
and	O
35	O
,	O
a	O
potentially	O
important	O
difference	O
from	O
the	O
uniform	O
decline	O
portrayed	O
in	O
figure	O
1.1.	O
there	O
is	O
no	O
formula	B
such	O
as	O
(	O
1.2	O
)	O
to	O
infer	O
the	O
accuracy	O
of	O
the	O
lowess	B
curve	O
.	O
instead	O
,	O
a	O
computer-intensive	O
inferential	O
engine	O
,	O
the	O
bootstrap	O
,	O
was	O
used	O
to	O
calculate	O
the	O
error	O
bars	O
in	O
figure	O
1.2.	O
a	O
bootstrap	O
data	B
set	O
is	O
pro-	O
duced	O
by	O
resampling	O
157	O
pairs	O
.xi	O
;	O
yi	O
/	O
from	O
the	O
original	O
157	O
with	O
replace-	O
ment	O
,	O
so	O
perhaps	O
.x1	O
;	O
y1/	O
might	O
show	O
up	O
twice	O
in	O
the	O
bootstrap	O
sample	B
,	O
.x2	O
;	O
y2/	O
might	O
be	O
missing	O
,	O
.x3	O
;	O
y3/	O
present	O
once	O
,	O
etc	O
.	O
applying	O
lowess	B
to	O
the	O
bootstrap	O
sample	B
generates	O
a	O
bootstrap	O
replication	B
of	O
the	O
original	O
calculation	O
.	O
figure	O
1.3	O
25	O
bootstrap	O
replications	O
of	O
lowess	B
(	O
x	O
,	O
y,1/3	O
)	O
.	O
figure	O
1.3	O
shows	O
the	O
ﬁrst	O
25	O
(	O
of	O
250	O
)	O
bootstrap	O
lowess	B
replications	O
bouncing	O
around	O
the	O
original	O
curve	O
from	O
figure	O
1.2.	O
the	O
variability	O
of	O
the	O
replications	O
at	O
any	O
one	O
age	O
,	O
the	O
bootstrap	O
standard	B
deviation	I
,	O
determined	O
the	O
original	O
curve	O
’	O
s	O
accuracy	O
.	O
how	O
and	O
why	O
the	O
bootstrap	O
works	O
is	O
dis-	O
cussed	O
in	O
chapter	O
10.	O
it	O
has	O
the	O
great	O
virtue	O
of	O
assessing	O
estimation	B
accu-	O
2030405060708090−4−2024agetot	O
8	O
algorithms	O
and	O
inference	O
racy	O
for	O
any	O
algorithm	B
,	O
no	O
matter	O
how	O
complicated	O
.	O
the	O
price	O
is	O
a	O
hundred-	O
or	O
thousand-fold	O
increase	O
in	O
computation	O
,	O
unthinkable	O
in	O
1930	O
,	O
but	O
routine	O
now	O
.	O
the	O
bottom	O
two	O
lines	O
of	O
table	O
1.1	O
show	O
the	O
lowess	B
estimates	O
and	O
their	O
standard	O
errors	O
.	O
we	O
have	O
paid	O
a	O
price	O
for	O
the	O
increased	O
ﬂexibility	O
of	O
lowess	B
,	O
its	O
standard	O
errors	O
roughly	O
doubling	O
those	O
for	O
linear	B
regression	O
.	O
1.2	O
hypothesis	B
testing	I
our	O
second	O
example	O
concerns	O
the	O
march	O
of	O
methodology	O
and	O
inference	O
for	O
hypothesis	B
testing	I
rather	O
than	O
estimation	B
:	O
72	O
leukemia	B
patients	O
,	O
47	O
with	O
all	O
(	O
acute	O
lymphoblastic	O
leukemia	B
)	O
and	O
25	O
with	O
aml	O
(	O
acute	O
myeloid	O
leuk-	O
emia	O
,	O
a	O
worse	O
prognosis	O
)	O
have	O
each	O
had	O
genetic	O
activity	O
measured	O
for	O
a	O
panel	O
of	O
7,128	O
genes	O
.	O
the	O
histograms	O
in	O
figure	O
1.4	O
compare	O
the	O
genetic	O
activities	O
in	O
the	O
two	O
groups	O
for	O
gene	O
136.	O
figure	O
1.4	O
scores	O
for	O
gene	O
136	O
,	O
leukemia	B
data	O
.	O
top	O
all	O
(	O
n	O
d	O
47	O
)	O
,	O
bottom	O
aml	O
(	O
n	O
d	O
25	O
)	O
.	O
a	O
two-sample	B
t-statistic	O
d	O
3:01	O
with	O
p-value	B
d	O
:0036.	O
the	O
aml	O
group	O
appears	O
to	O
show	O
greater	O
activity	O
,	O
the	O
mean	O
values	O
being	O
all	O
d	O
0:752	O
and	O
aml	O
d	O
0:950	O
:	O
(	O
1.5	O
)	O
all	O
scores	O
−	O
mean	O
.752	O
0.20.40.60.81.01.21.41.60246810aml	O
scores	O
−	O
mean	O
.950	O
0.20.40.60.81.01.21.41.60246810	O
1.2	O
hypothesis	B
testing	I
9	O
is	O
the	O
perceived	O
difference	O
genuine	O
,	O
or	O
perhaps	O
,	O
as	O
people	O
like	O
to	O
say	O
,	O
“	O
a	O
statistical	O
ﬂuke	O
”	O
?	O
the	O
classic	O
answer	O
to	O
this	O
question	O
is	O
via	O
a	O
two-sample	B
t-statistic	O
,	O
t	B
d	O
aml	O
(	O
cid:0	O
)	O
allbsd	O
(	O
1.6	O
)	O
;	O
wherebsd	O
is	O
an	O
estimate	O
of	O
the	O
numerator	O
’	O
s	O
standard	O
deviation.4	O
dividing	O
by	O
bsd	O
allows	O
us	O
(	O
under	O
gaussian	O
assumptions	O
discussed	O
in	O
chapter	O
5	O
)	O
to	O
compare	O
the	O
observed	O
value	O
of	O
t	B
with	O
a	O
standard	O
“	O
null	O
”	O
dis-	O
tribution	O
,	O
in	O
this	O
case	O
a	O
student	O
’	O
s	O
t	B
distribution	O
with	O
70	O
degrees	O
of	O
freedom	O
.	O
we	O
obtain	O
t	B
d	O
3:01	O
from	O
(	O
1.6	O
)	O
,	O
which	O
would	O
classically	O
be	O
considered	O
very	O
strong	O
evidence	O
that	O
the	O
apparent	O
difference	O
(	O
1.5	O
)	O
is	O
genuine	O
;	O
in	O
standard	O
terminology	O
,	O
“	O
with	O
two-sided	O
signiﬁcance	O
level	O
0.0036.	O
”	O
a	O
small	O
signiﬁcance	O
level	O
(	O
or	O
“	O
p-value	B
”	O
)	O
is	O
a	O
statement	O
of	O
statistical	O
sur-	O
prise	O
:	O
something	O
very	O
unusual	O
has	O
happened	O
if	O
in	O
fact	O
there	O
is	O
no	O
difference	O
in	O
gene	O
136	O
expression	O
levels	O
between	O
all	O
and	O
aml	O
patients	O
.	O
we	O
are	O
less	O
surprised	O
by	O
t	B
d	O
3:01	O
if	O
gene	O
136	O
is	O
just	O
one	O
candidate	O
out	O
of	O
thousands	O
that	O
might	O
have	O
produced	O
“	O
interesting	O
”	O
results	O
.	O
that	O
is	O
the	O
case	O
here	O
.	O
figure	O
1.5	O
shows	O
the	O
histogram	O
of	O
the	O
two-sample	B
t-statistics	O
for	O
the	O
panel	O
of	O
7128	O
genes	O
.	O
now	O
t	B
d	O
3:01	O
looks	O
less	O
unusual	O
;	O
400	O
other	O
genes	O
have	O
t	B
exceeding	O
3.01	O
,	O
about	O
5.6	O
%	O
of	O
them	O
.	O
this	O
doesn	O
’	O
t	B
mean	O
that	O
gene	O
136	O
is	O
“	O
signiﬁcant	O
at	O
the	O
0.056	O
level.	O
”	O
there	O
are	O
two	O
powerful	O
complicating	O
factors	O
:	O
1	O
large	O
numbers	O
of	O
candidates	O
,	O
7128	O
here	O
,	O
will	O
produce	O
some	O
large	O
t-	O
values	O
even	O
if	O
there	O
is	O
really	O
no	O
difference	O
in	O
genetic	O
expression	O
between	O
all	O
and	O
aml	O
patients	O
.	O
2	O
the	O
histogram	O
implies	O
that	O
in	O
this	O
study	O
there	O
is	O
something	O
wrong	O
with	O
the	O
theoretical	O
null	O
distribution	B
(	O
“	O
student	O
’	O
s	O
t	B
with	O
70	O
degrees	O
of	O
free-	O
dom	O
”	O
)	O
,	O
the	O
smooth	O
curve	O
in	O
figure	O
1.5.	O
it	O
is	O
much	O
too	O
narrow	O
at	O
the	O
cen-	O
ter	O
,	O
where	O
presumably	O
most	O
of	O
the	O
genes	O
are	O
reporting	O
non-signiﬁcant	O
results	O
.	O
we	O
will	O
see	O
in	O
chapter	O
15	O
that	O
a	O
low	O
false-discovery	B
rate	I
,	O
i.e.	O
,	O
a	O
low	O
chance	O
of	O
crying	O
wolf	O
over	O
an	O
innocuous	O
gene	O
,	O
requires	O
t	B
exceeding	O
6.16	O
in	O
the	O
all/aml	O
study	O
.	O
only	O
47	O
of	O
the	O
7128	O
genes	O
make	O
the	O
cut	O
.	O
false-	O
discovery-rate	O
theory	B
is	O
an	O
impressive	O
advance	O
in	O
statistical	O
inference	B
,	O
in-	O
corporating	O
bayesian	O
,	O
frequentist	O
,	O
and	O
empirical	O
bayesian	O
(	O
chapter	O
6	O
)	O
el-	O
4	O
formally	O
,	O
a	O
standard	B
error	I
is	O
the	O
standard	B
deviation	I
of	O
a	O
summary	O
statistic	B
,	O
andbsd	O
might	O
better	O
be	O
calledbse	O
,	O
but	O
we	O
will	O
follow	O
the	O
distinction	O
less	O
than	O
punctiliously	O
here	O
.	O
10	O
algorithms	O
and	O
inference	O
figure	O
1.5	O
two-sample	B
t-statistics	O
for	O
7128	O
genes	O
,	O
leukemia	B
data	O
.	O
the	O
smooth	O
curve	O
is	O
the	O
theoretical	O
null	O
density	B
for	O
the	O
t-statistic	B
.	O
ements	O
.	O
it	O
was	O
a	O
necessary	O
advance	O
in	O
a	O
scientiﬁc	O
world	O
where	O
computer-	O
based	O
technology	O
routinely	O
presents	O
thousands	O
of	O
comparisons	O
to	O
be	O
eval-	O
uated	O
at	O
once	O
.	O
there	O
is	O
one	O
more	O
thing	O
to	O
say	O
about	O
the	O
algorithm/inference	O
statistical	O
cycle	O
.	O
important	O
new	O
algorithms	O
often	O
arise	O
outside	O
the	O
world	O
of	O
profes-	O
sional	O
statisticians	O
:	O
neural	O
nets	O
,	O
support	O
vector	B
machines	O
,	O
and	O
boosting	O
are	O
three	O
famous	O
examples	O
.	O
none	O
of	O
this	O
is	O
surprising	O
.	O
new	O
sources	O
of	O
data	B
,	O
satellite	O
imagery	O
for	O
example	O
,	O
or	O
medical	O
microarrays	O
,	O
inspire	O
novel	O
meth-	O
odology	O
from	O
the	O
observing	O
scientists	O
.	O
the	O
early	O
literature	O
tends	O
toward	O
the	O
enthusiastic	O
,	O
with	O
claims	O
of	O
enormous	O
applicability	O
and	O
power	O
.	O
in	O
the	O
second	O
phase	O
,	O
statisticians	O
try	O
to	O
locate	O
the	O
new	O
metholodogy	O
within	O
the	O
framework	O
of	O
statistical	O
theory	B
.	O
in	O
other	O
words	O
,	O
they	O
carry	O
out	O
the	O
statistical	O
inference	B
part	O
of	O
the	O
cycle	O
,	O
placing	O
the	O
new	O
methodology	O
within	O
the	O
known	O
bayesian	O
and	O
frequentist	O
limits	O
of	O
performance	O
.	O
(	O
boost-	O
ing	O
offers	O
a	O
nice	O
example	O
,	O
chapter	O
17	O
.	O
)	O
this	O
is	O
a	O
healthy	O
chain	O
of	O
events	O
,	O
good	O
both	O
for	O
the	O
hybrid	O
vigor	O
of	O
the	O
statistics	B
profession	O
and	O
for	O
the	O
further	O
progress	O
of	O
algorithmic	O
technology	O
.	O
t	B
statisticsfrequency−10−5051001002003004005006007003.01	O
1.3	O
notes	O
1.3	O
notes	O
11	O
legendre	O
published	O
the	O
least	B
squares	I
algorithm	O
in	O
1805	O
,	O
causing	O
gauss	O
to	O
state	O
that	O
he	O
had	O
been	O
using	O
the	O
method	B
in	O
astronomical	O
orbit-ﬁtting	O
since	O
1795.	O
given	O
gauss	O
’	O
astonishing	O
production	O
of	O
major	O
mathematical	O
advances	O
,	O
this	O
says	O
something	O
about	O
the	O
importance	O
attached	O
to	O
the	O
least	B
squares	I
idea	O
.	O
chapter	O
8	O
includes	O
its	O
usual	O
algebraic	O
formulation	O
,	O
as	O
well	O
as	O
gauss	O
’	O
formula	B
for	O
the	O
standard	O
errors	O
,	O
line	O
2	O
of	O
table	O
1.1.	O
our	O
division	O
between	O
algorithms	O
and	O
inference	O
brings	O
to	O
mind	O
tukey	O
’	O
s	O
exploratory/conﬁrmatory	O
system	O
.	O
however	O
the	O
current	O
algorithmic	O
world	O
is	O
often	O
bolder	O
in	O
its	O
claims	O
than	O
the	O
word	O
“	O
exploratory	O
”	O
implies	O
,	O
while	O
to	O
our	O
minds	O
“	O
inference	B
”	O
conveys	O
something	O
richer	O
than	O
mere	O
conﬁrmation	O
.	O
1	O
[	O
p.	O
6	O
]	O
lowess	B
was	O
devised	O
by	O
william	O
cleveland	O
(	O
cleveland	O
,	O
1981	O
)	O
and	O
is	O
available	O
in	O
the	O
r	O
statistical	O
computing	O
language	O
.	O
it	O
is	O
applied	O
to	O
the	O
kidney	O
data	O
in	O
efron	O
(	O
2004	O
)	O
.	O
the	O
kidney	O
data	O
originated	O
in	O
the	O
nephrology	O
laboratory	O
of	O
dr.	O
brian	O
myers	O
,	O
stanford	O
university	O
,	O
and	O
is	O
available	O
from	O
this	O
book	O
’	O
s	O
web	O
site	O
.	O
2	O
frequentist	O
inference	B
before	O
the	O
computer	O
age	O
there	O
was	O
the	O
calculator	O
age	O
,	O
and	O
before	O
“	O
big	O
data	B
”	O
there	O
were	O
small	O
data	B
sets	O
,	O
often	O
a	O
few	O
hundred	O
numbers	O
or	O
fewer	O
,	O
labori-	O
ously	O
collected	O
by	O
individual	O
scientists	O
working	O
under	O
restrictive	O
experi-	O
mental	O
constraints	O
.	O
precious	O
data	B
calls	O
for	O
maximally	O
efﬁcient	O
statistical	O
analysis	B
.	O
a	O
remarkably	O
effective	O
theory	B
,	O
feasible	O
for	O
execution	O
on	O
mechan-	O
ical	O
desk	O
calculators	O
,	O
was	O
developed	O
beginning	O
in	O
1900	O
by	O
pearson	O
,	O
fisher	O
,	O
neyman	O
,	O
hotelling	O
,	O
and	O
others	O
,	O
and	O
grew	O
to	O
dominate	O
twentieth-century	O
statistical	O
practice	O
.	O
the	O
theory	B
,	O
now	O
referred	O
to	O
as	O
classical	O
,	O
relied	O
almost	O
entirely	O
on	O
frequentist	O
inferential	O
ideas	O
.	O
this	O
chapter	O
sketches	O
a	O
quick	O
and	O
simpliﬁed	O
picture	O
of	O
frequentist	O
inference	B
,	O
particularly	O
as	O
employed	O
in	O
clas-	O
sical	O
applications	O
.	O
we	O
begin	O
with	O
another	O
example	O
from	O
dr.	O
myers	O
’	O
nephrology	O
laboratory	O
:	O
211	O
kidney	O
patients	O
have	O
had	O
their	O
glomerular	O
ﬁltration	O
rates	O
measured	O
,	O
with	O
the	O
results	O
shown	O
in	O
figure	O
2.1	O
;	O
gfr	O
is	O
an	O
important	O
indicator	O
of	O
kid-	O
ney	O
function	B
,	O
with	O
low	O
values	O
suggesting	O
trouble	O
.	O
(	O
it	O
is	O
a	O
key	O
component	O
of	O
andbse	O
d	O
0:95	O
,	O
typically	O
reported	O
as	O
tot	O
in	O
figure	O
1.1	O
.	O
)	O
the	O
mean	O
and	O
standard	O
error	O
(	O
1.1	O
)	O
–	O
(	O
1.2	O
)	O
are	O
nx	O
d	O
54:25	O
(	O
2.1	O
)	O
˙0:95	O
denotes	O
a	O
frequentist	O
inference	B
for	O
the	O
accuracy	O
of	O
the	O
estimate	B
nx	O
d	O
54:25	O
,	O
and	O
suggests	O
that	O
we	O
shouldn	O
’	O
t	B
take	O
the	O
“	O
.25	O
”	O
very	O
seriously	O
,	O
even	O
the	O
“	O
4	O
”	O
being	O
open	O
to	O
doubt	O
.	O
where	O
the	O
inference	B
comes	O
from	O
and	O
what	O
exactly	O
it	O
means	O
remains	O
to	O
be	O
said	O
.	O
54:25	O
˙	O
0:95i	O
statistical	O
inference	B
usually	O
begins	O
with	O
the	O
assumption	O
that	O
some	O
prob-	O
ability	O
model	B
has	O
produced	O
the	O
observed	O
data	B
x	O
,	O
in	O
our	O
case	O
the	O
vector	B
of	O
n	O
d	O
211	O
gfr	O
measurements	O
x	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn/	O
.	O
let	O
x	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn/	O
indicate	O
n	O
independent	O
draws	O
from	O
a	O
probability	O
distribution	B
f	O
,	O
writ-	O
ten	O
f	O
!	O
x	O
;	O
12	O
(	O
2.2	O
)	O
frequentist	O
inference	B
13	O
figure	O
2.1	O
glomerular	O
ﬁltration	O
rates	O
for	O
211	O
kidney	O
patients	O
;	O
mean	O
54.25	O
,	O
standard	B
error	I
.95.	O
f	O
being	O
the	O
underlying	O
distribution	B
of	O
possible	O
gfr	O
scores	O
here	O
.	O
a	O
realiza-	O
tion	O
x	O
d	O
x	O
of	O
(	O
2.2	O
)	O
has	O
been	O
observed	O
,	O
and	O
the	O
statistician	O
wishes	O
to	O
infer	O
some	O
property	O
of	O
the	O
unknown	O
distribution	B
f	O
.	O
suppose	O
the	O
desired	O
property	O
is	O
the	O
expectation	O
of	O
a	O
single	O
random	O
draw	O
x	O
from	O
f	O
,	O
denoted	O
(	O
which	O
also	O
equals	O
the	O
expectation	O
of	O
the	O
average	O
nx	O
dp	O
xi	O
=n	O
of	O
random	O
	O
d	O
ef	O
fxg	O
vector	B
(	O
2.2	O
)	O
1	O
)	O
.	O
the	O
obvious	O
estimate	O
of	O
	O
is	O
o	O
	O
d	O
nx	O
,	O
the	O
sample	B
average	O
.	O
if	O
n	O
were	O
enormous	O
,	O
say	O
1010	O
,	O
we	O
would	O
expect	O
o	O
	O
to	O
nearly	O
equal	O
	O
,	O
but	O
oth-	O
erwise	O
there	O
is	O
room	O
for	O
error	O
.	O
how	O
much	O
error	O
is	O
the	O
inferential	O
question	O
.	O
the	O
estimate	B
o	O
	O
is	O
calculated	O
from	O
x	O
according	O
to	O
some	O
known	O
algorithm	B
,	O
say	O
t	B
.x/	O
in	O
our	O
example	O
being	O
the	O
averaging	B
function	O
nx	O
d	O
p	O
xi	O
=n	O
;	O
o	O
	O
is	O
a	O
1	O
the	O
fact	O
that	O
eff	O
nxg	O
equals	O
effxg	O
is	O
a	O
crucial	O
,	O
though	O
easily	O
proved	O
,	O
probabilistic	O
o	O
	O
d	O
t	B
.x/	O
;	O
(	O
2.3	O
)	O
(	O
2.4	O
)	O
result	O
.	O
gfrfrequency20406080100051015202530	O
14	O
realization	O
of	O
frequentist	O
inference	B
(	O
cid:22	O
)	O
d	O
ef	O
f	O
o‚g	O
:	O
(	O
2.6	O
)	O
o‚	O
d	O
t	B
.x	O
/	O
;	O
(	O
2.5	O
)	O
the	O
output	O
of	O
t	B
.	O
(	O
cid:1	O
)	O
/	O
applied	O
to	O
a	O
theoretical	O
sample	B
x	O
from	O
f	O
(	O
2.2	O
)	O
.	O
we	O
have	O
chosen	O
t	B
.x	O
/	O
,	O
we	O
hope	O
,	O
to	O
make	O
o‚	O
a	O
good	O
estimator	B
of	O
	O
,	O
the	O
desired	O
prop-	O
erty	O
of	O
f	O
.	O
we	O
can	O
now	O
give	O
a	O
ﬁrst	O
deﬁnition	O
of	O
frequentist	O
inference	B
:	O
the	O
accu-	O
racy	O
of	O
an	O
observed	O
estimate	B
o	O
	O
d	O
t	B
.x/	O
is	O
the	O
probabilistic	O
accuracy	O
of	O
o‚	O
d	O
t	B
.x	O
/	O
as	O
an	O
estimator	B
of	O
	O
.	O
this	O
may	O
seem	O
more	O
a	O
tautology	O
than	O
a	O
deﬁnition	O
,	O
but	O
it	O
contains	O
a	O
powerful	O
idea	O
:	O
o	O
	O
is	O
just	O
a	O
single	O
number	O
but	O
o‚	O
takes	O
on	O
a	O
range	O
of	O
values	O
whose	O
spread	O
can	O
deﬁne	O
measures	O
of	O
accuracy	O
.	O
bias	O
and	O
variance	O
are	O
familiar	O
examples	O
of	O
frequentist	O
inference	B
.	O
deﬁne	O
(	O
cid:22	O
)	O
to	O
be	O
the	O
expectation	O
of	O
o‚	O
d	O
t	B
.x	O
/	O
under	O
model	B
(	O
2.2	O
)	O
,	O
n	O
.	O
o‚	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
/2o	O
then	O
the	O
bias	O
and	O
variance	O
attributed	O
to	O
estimate	B
o	O
bias	O
d	O
(	O
cid:22	O
)	O
(	O
cid:0	O
)	O
	O
var	O
d	O
ef	O
	O
of	O
parameter	O
	O
are	O
and	O
(	O
2.7	O
)	O
again	O
,	O
what	O
keeps	O
this	O
from	O
tautology	O
is	O
the	O
attribution	O
to	O
the	O
single	O
num-	O
ber	O
o	O
	O
of	O
the	O
probabilistic	O
properties	O
of	O
o‚	O
following	O
from	O
model	O
(	O
2.2	O
)	O
.	O
if	O
all	O
of	O
this	O
seems	O
too	O
obvious	O
to	O
worry	O
about	O
,	O
the	O
bayesian	O
criticisms	O
of	O
chapter	O
3	O
may	O
come	O
as	O
a	O
shock	O
.	O
:	O
frequentism	O
is	O
often	O
deﬁned	O
with	O
respect	O
to	O
“	O
an	O
inﬁnite	O
sequence	O
of	O
future	O
trials.	O
”	O
we	O
imagine	O
hypothetical	O
data	B
sets	O
x	O
.1/	O
;	O
x	O
.2/	O
;	O
x	O
.3/	O
;	O
:	O
:	O
:	O
gen-	O
erated	O
by	O
the	O
same	O
mechanism	O
as	O
x	O
providing	O
corresponding	O
values	O
o‚.1/	O
;	O
o‚.2/	O
,	O
o‚.3/	O
;	O
:	O
:	O
:	O
as	O
in	O
(	O
2.5	O
)	O
.	O
the	O
frequentist	O
principle	O
is	O
then	O
to	O
attribute	O
for	O
o	O
	O
the	O
accuracy	O
properties	O
of	O
the	O
ensemble	O
of	O
o‚	O
values.2	O
if	O
the	O
o‚s	O
have	O
empirical	B
variance	O
of	O
,	O
say	O
,	O
0.04	O
,	O
then	O
o	O
0:2	O
d	O
p	O
	O
is	O
claimed	O
to	O
have	O
standard	B
error	I
0:04	O
,	O
etc	O
.	O
this	O
amounts	O
to	O
a	O
more	O
picturesque	O
restatement	O
of	O
the	O
previous	O
deﬁnition	O
.	O
2.1	O
frequentism	O
in	O
practice	O
our	O
working	O
deﬁnition	O
of	O
frequentism	O
is	O
that	O
the	O
probabilistic	O
properties	O
of	O
a	O
procedure	O
of	O
interest	O
are	O
derived	O
and	O
then	O
applied	O
verbatim	O
to	O
the	O
procedure	O
’	O
s	O
output	O
for	O
the	O
observed	O
data	B
.	O
this	O
has	O
an	O
obvious	O
defect	O
:	O
it	O
requires	O
calculating	O
the	O
properties	O
of	O
estimators	O
o‚	O
d	O
t	B
.x	O
/	O
obtained	O
from	O
2	O
in	O
essence	O
,	O
frequentists	O
ask	O
themselves	O
“	O
what	O
would	O
i	O
see	O
if	O
i	O
reran	O
the	O
same	O
situation	O
again	O
(	O
and	O
again	O
and	O
again	O
.	O
.	O
.	O
)	O
?	O
”	O
2.1	O
frequentism	O
in	O
practice	O
15	O
statistics	B
o	O
(	O
2.9	O
)	O
(	O
2.10	O
)	O
the	O
true	O
distribution	B
f	O
,	O
even	O
though	O
f	O
is	O
unknown	O
.	O
practical	O
frequentism	O
uses	O
a	O
collection	O
of	O
more	O
or	O
less	O
ingenious	O
devices	O
to	O
circumvent	O
the	O
defect	O
.	O
1.	O
the	O
plug-in	O
principle	O
.	O
a	O
simple	O
formula	B
relates	O
the	O
standard	B
error	I
of	O
nx	O
dp	O
xi	O
=n	O
to	O
varf	O
.x	O
/	O
,	O
the	O
variance	O
of	O
a	O
single	O
x	O
drawn	O
from	O
f	O
,	O
se	O
(	O
cid:0	O
)	O
nx	O
(	O
cid:1	O
)	O
d	O
œvarf	O
.x	O
/=n1=2	O
:	O
cvarf	O
dx	O
.xi	O
(	O
cid:0	O
)	O
nx/2	O
=.n	O
(	O
cid:0	O
)	O
1/	O
:	O
(	O
2.8	O
)	O
but	O
having	O
observed	O
x	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn/	O
we	O
can	O
estimate	B
varf	O
.x	O
/	O
with-	O
out	O
bias	O
by	O
plugging	O
formula	B
(	O
2.9	O
)	O
into	O
(	O
2.8	O
)	O
givesbse	O
(	O
1.2	O
)	O
,	O
the	O
usual	O
estimate	B
for	O
the	O
standard	B
error	I
of	O
an	O
average	O
nx	O
.	O
in	O
other	O
words	O
,	O
the	O
frequentist	O
accuracy	O
estimate	O
for	O
nx	O
is	O
itself	O
estimated	O
from	O
the	O
observed	O
data.3	O
	O
d	O
t	B
.x/	O
more	O
complicated	O
2.	O
taylor-series	O
approximations	O
.	O
than	O
nx	O
can	O
often	O
be	O
related	O
back	O
to	O
the	O
plug-in	O
formula	B
by	O
local	O
linear	B
approximations	O
,	O
sometimes	O
known	O
as	O
the	O
“	O
delta	O
method.	O
”	O
	O
for	O
example	O
,	O
1	O
o	O
	O
d	O
nx2	O
has	O
d	O
with	O
bse	O
as	O
in	O
(	O
1.2	O
)	O
.	O
large	O
sample	B
calculations	O
,	O
as	O
sample	B
size	I
n	O
goes	O
to	O
o	O
	O
=d	O
nx	O
d	O
2nx	O
.	O
thinking	O
of	O
2nx	O
as	O
a	O
constant	O
gives	O
se	O
(	O
cid:0	O
)	O
nx2	O
(	O
cid:1	O
)	O
:	O
d	O
2jnxjbse	O
;	O
inﬁnity	O
,	O
validate	O
the	O
delta	O
method	B
which	O
,	O
fortunately	O
,	O
often	O
performs	O
well	O
in	O
small	O
samples	O
.	O
3.	O
parametric	B
families	O
and	O
maximum	O
likelihood	B
theory	O
.	O
theoretical	O
ex-	O
pressions	O
for	O
the	O
standard	B
error	I
of	O
a	O
maximum	B
likelihood	I
estimate	O
(	O
mle	O
)	O
are	O
discussed	O
in	O
chapters	O
4	O
and	O
5	O
,	O
in	O
the	O
context	O
of	O
parametric	B
families	O
of	O
distributions	O
.	O
these	O
combine	O
fisherian	O
theory	B
,	O
taylor-series	O
approxima-	O
tions	O
,	O
and	O
the	O
plug-in	O
principle	O
in	O
an	O
easy-to-apply	O
package	O
.	O
4.	O
simulation	O
and	O
the	O
bootstrap	O
.	O
modern	O
computation	O
has	O
opened	O
up	O
the	O
possibility	O
of	O
numerically	O
implementing	O
the	O
“	O
inﬁnite	O
sequence	O
of	O
future	O
trials	O
”	O
deﬁnition	O
,	O
except	O
for	O
the	O
inﬁnite	O
part	O
.	O
an	O
estimate	O
of	O
of	O
f	O
,	O
perhaps	O
the	O
mle	O
,	O
is	O
found	O
,	O
and	O
values	O
o‚.k/	O
d	O
t	B
.x	O
.k//	O
simulated	O
from	O
of	O
for	O
k	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
b	O
,	O
say	O
b	O
d	O
1000.	O
the	O
empirical	B
standard	O
deviation	O
of	O
the	O
o‚s	O
is	O
then	O
the	O
frequentist	O
estimate	B
of	I
standard	I
error	I
for	O
o	O
	O
d	O
t	B
.x/	O
,	O
and	O
similarly	O
with	O
other	O
measures	O
of	O
accuracy	O
.	O
this	O
is	O
a	O
good	O
description	O
of	O
the	O
bootstrap	O
,	O
chapter	O
10	O
.	O
(	O
notice	O
that	O
3	O
the	O
most	O
familiar	O
example	O
is	O
the	O
observed	O
proportion	B
p	O
of	O
heads	O
in	O
n	O
ﬂips	O
of	O
a	O
coin	O
having	O
true	O
probability	O
(	O
cid:25	O
)	O
:	O
the	O
actual	O
standard	B
error	I
is	O
œ	O
(	O
cid:25	O
)	O
.1	O
(	O
cid:0	O
)	O
(	O
cid:25	O
)	O
/=n1=2	O
but	O
we	O
can	O
only	O
report	O
the	O
plug-in	O
estimate	B
œp.1	O
(	O
cid:0	O
)	O
p/=n1=2	O
.	O
16	O
frequentist	O
inference	B
table	O
2.1	O
three	O
estimates	O
of	O
location	O
for	O
the	O
gfr	O
data	B
,	O
and	O
their	O
estimated	O
standard	O
errors	O
;	O
last	O
two	O
standard	O
errors	O
using	O
the	O
bootstrap	O
,	O
b	O
d	O
1000.	O
estimate	B
standard	O
error	O
mean	O
25	O
%	O
winsorized	O
mean	O
median	O
54.25	O
52.61	O
52.24	O
.95	O
.78	O
.87	O
of	O
for	O
f	O
,	O
comes	O
ﬁrst	O
rather	O
than	O
at	O
the	O
end	O
of	O
here	O
the	O
plugging-in	O
,	O
of	O
the	O
process	O
.	O
)	O
the	O
classical	O
methods	O
1–3	O
above	O
are	O
restricted	O
to	O
estimates	O
o	O
	O
d	O
t	B
.x/	O
that	O
are	O
smoothly	O
deﬁned	O
functions	O
of	O
various	O
sample	B
means	O
.	O
simulation	O
calculations	O
remove	O
this	O
restriction	O
.	O
table	O
2.1	O
shows	O
three	O
“	O
lo-	O
cation	O
”	O
estimates	O
for	O
the	O
gfr	O
data	B
,	O
the	O
mean	O
,	O
the	O
25	O
%	O
winsorized	O
mean,4	O
and	O
the	O
median	O
,	O
along	O
with	O
their	O
standard	O
errors	O
,	O
the	O
last	O
two	O
computed	O
by	O
the	O
bootstrap	O
.	O
a	O
happy	O
feature	O
of	O
computer-age	O
statistical	O
inference	B
is	O
the	O
tremendous	O
expansion	O
of	O
useful	O
and	O
usable	O
statistics	B
t	O
.x/	O
in	O
the	O
statis-	O
tician	O
’	O
s	O
working	O
toolbox	O
,	O
the	O
lowess	B
algorithm	O
in	O
figures	O
1.2	O
and	O
1.3	O
providing	O
a	O
nice	O
example	O
.	O
5.	O
pivotal	O
statistics	B
.	O
a	O
pivotal	O
statistic	B
o	O
	O
d	O
t	B
.x/	O
is	O
one	O
whose	O
distri-	O
bution	O
does	O
not	O
depend	O
upon	O
the	O
underlying	O
probability	O
distribution	B
f	O
.	O
in	O
such	O
a	O
case	O
the	O
theoretical	O
distribution	B
of	O
o‚	O
d	O
t	B
.x	O
/	O
applies	O
exactly	O
to	O
o	O
	O
,	O
removing	O
the	O
need	O
for	O
devices	O
1–4	O
above	O
.	O
the	O
classic	O
example	O
concerns	O
student	O
’	O
s	O
two-sample	B
t-test	O
.	O
in	O
a	O
two-sample	B
problem	O
the	O
statistician	O
observes	O
two	O
sets	O
of	O
numbers	O
,	O
x1	O
d	O
.x11	O
;	O
x12	O
;	O
:	O
:	O
:	O
;	O
x1n1	O
/	O
x2	O
d	O
.x21	O
;	O
x22	O
;	O
:	O
:	O
:	O
;	O
x2n2	O
/	O
;	O
(	O
2.11	O
)	O
and	O
wishes	O
to	O
test	O
the	O
null	O
hypothesis	O
that	O
they	O
come	O
from	O
the	O
same	O
dis-	O
tribution	O
(	O
as	O
opposed	O
to	O
,	O
say	O
,	O
the	O
second	O
set	B
tending	O
toward	O
larger	O
values	O
than	O
the	O
ﬁrst	O
)	O
.	O
it	O
is	O
assumed	O
that	O
the	O
distribution	B
f1	O
for	O
x1	O
is	O
normal	B
,	O
or	O
gaussian	O
,	O
ind	O
(	O
cid:24	O
)	O
x1i	O
n	O
.	O
(	O
cid:22	O
)	O
1	O
;	O
(	O
cid:27	O
)	O
2/	O
;	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n1	O
;	O
(	O
2.12	O
)	O
the	O
notation	O
indicating	O
n1	O
independent	O
draws	O
from	O
a	O
normal	B
distribution5	O
4	O
all	O
observations	O
below	O
the	O
25th	O
percentile	O
of	O
the	O
211	O
observations	O
are	O
moved	O
up	O
to	O
that	O
point	O
,	O
similarly	O
those	O
above	O
the	O
75th	O
percentile	O
are	O
moved	O
down	O
,	O
and	O
ﬁnally	O
the	O
mean	O
is	O
taken	O
.	O
(	O
cid:0	O
)	O
1=2	O
expf	O
(	O
cid:0	O
)	O
0:5	O
(	O
cid:1	O
)	O
.x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
1/2=	O
(	O
cid:27	O
)	O
2g	O
.	O
5	O
each	O
draw	O
having	O
probability	O
density	B
.2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2/	O
2.1	O
frequentism	O
in	O
practice	O
with	O
expectation	O
(	O
cid:22	O
)	O
1	O
and	O
variance	O
(	O
cid:27	O
)	O
2.	O
likewise	O
ind	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
2	O
;	O
(	O
cid:27	O
)	O
2/	O
we	O
wish	O
to	O
test	O
the	O
null	O
hypothesis	O
x2i	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n2	O
:	O
17	O
(	O
2.13	O
)	O
(	O
2.14	O
)	O
the	O
obvious	O
test	O
statistic	B
o	O
	O
d	O
nx2	O
(	O
cid:0	O
)	O
nx1	O
,	O
the	O
difference	O
of	O
the	O
means	O
,	O
has	O
distribution	B
o	O
	O
(	O
cid:24	O
)	O
c	O
1	O
(	O
2.15	O
)	O
h0	O
w	O
(	O
cid:22	O
)	O
1	O
d	O
(	O
cid:22	O
)	O
2	O
:	O
(	O
cid:16	O
)	O
0	O
;	O
(	O
cid:27	O
)	O
2	O
(	O
cid:16	O
)	O
1	O
''	O
n1x	O
.x1i	O
(	O
cid:0	O
)	O
nx1/2	O
c	O
n2x	O
.x2i	O
(	O
cid:0	O
)	O
nx2/2	O
n	O
n1	O
	O
#	O
,	O
n2	O
under	O
h0	O
.	O
we	O
could	O
plug	O
in	O
the	O
unbiased	O
estimate	O
of	O
(	O
cid:27	O
)	O
2	O
,	O
o	O
(	O
cid:27	O
)	O
2	O
d	O
.n1	O
c	O
n2	O
(	O
cid:0	O
)	O
2/	O
;	O
(	O
2.16	O
)	O
1	O
1	O
but	O
student	O
provided	O
a	O
more	O
elegant	O
solution	O
:	O
instead	O
of	O
o	O
1=2	O
using	O
the	O
two-sample	B
t-statistic	O
wherebsd	O
d	O
o	O
(	O
cid:27	O
)	O
(	O
cid:16	O
)	O
1	O
c	O
1	O
;	O
n1	O
n2	O
t	B
d	O
nx2	O
(	O
cid:0	O
)	O
nx1bsd	O
	O
,	O
we	O
test	O
h0	O
:	O
(	O
2.17	O
)	O
under	O
h0	O
,	O
t	B
is	O
pivotal	O
,	O
having	O
the	O
same	O
distribution	B
(	O
student	O
’	O
s	O
t	B
distribu-	O
tion	O
with	O
n1	O
c	O
n2	O
(	O
cid:0	O
)	O
2	O
degrees	O
of	O
freedom	O
)	O
,	O
no	O
matter	O
what	O
the	O
value	O
of	O
the	O
“	O
nuisance	O
parameter	O
”	O
(	O
cid:27	O
)	O
.	O
for	O
n1	O
c	O
n2	O
(	O
cid:0	O
)	O
2	O
d	O
70	O
,	O
as	O
in	O
the	O
leukemia	B
example	O
(	O
1.5	O
)	O
–	O
(	O
1.6	O
)	O
,	O
student	O
’	O
s	O
distribution	B
gives	O
prh0	O
(	O
2.18	O
)	O
the	O
hypothesis	O
test	O
that	O
rejects	O
h0	O
if	O
jtj	O
exceeds	O
1.99	O
has	O
probability	O
ex-	O
actly	O
0.05	O
of	O
mistaken	O
rejection	O
.	O
similarly	O
,	O
f	O
(	O
cid:0	O
)	O
1:99	O
	O
t	B
	O
1:99g	O
d	O
0:95	O
:	O
nx2	O
(	O
cid:0	O
)	O
nx1	O
˙	O
1:99	O
(	O
cid:1	O
)	O
bsd	O
(	O
2.19	O
)	O
is	O
an	O
exact	O
0.95	O
conﬁdence	B
interval	I
for	O
the	O
difference	O
(	O
cid:22	O
)	O
2	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
1	O
,	O
covering	O
the	O
true	O
value	O
in	O
95	O
%	O
of	O
repetitions	O
of	O
probability	O
model	B
(	O
2.12	O
)	O
–	O
(	O
2.13	O
)	O
.6	O
6	O
occasionally	O
,	O
one	O
sees	O
frequentism	O
deﬁned	O
in	O
careerist	O
terms	O
,	O
e.g.	O
,	O
“	O
a	O
statistician	O
who	O
always	O
rejects	O
null	O
hypotheses	O
at	O
the	O
95	O
%	O
level	O
will	O
over	O
time	O
make	O
only	O
5	O
%	O
errors	B
of	O
the	O
ﬁrst	O
kind.	O
”	O
this	O
is	O
not	O
a	O
comforting	O
criterion	O
for	O
the	O
statistician	O
’	O
s	O
clients	O
,	O
who	O
are	O
interested	O
in	O
their	O
own	O
situations	O
,	O
not	O
everyone	O
else	O
’	O
s	O
.	O
here	O
we	O
are	O
only	O
assuming	O
hypothetical	O
repetitions	O
of	O
the	O
speciﬁc	O
problem	O
at	O
hand	O
.	O
18	O
frequentist	O
inference	B
what	O
might	O
be	O
called	O
the	O
strong	O
deﬁnition	O
of	O
frequentism	O
insists	O
on	O
exact	O
frequentist	O
correctness	O
under	O
experimental	O
repetitions	O
.	O
pivotality	O
,	O
unfortu-	O
nately	O
,	O
is	O
unavailable	O
in	O
most	O
statistical	O
situations	O
.	O
our	O
looser	O
deﬁnition	O
of	O
frequentism	O
,	O
supplemented	O
by	O
devices	O
such	O
as	O
those	O
above,7	O
presents	O
a	O
more	O
realistic	O
picture	O
of	O
actual	O
frequentist	O
practice	O
.	O
2.2	O
frequentist	O
optimality	O
the	O
popularity	O
of	O
frequentist	O
methods	O
reﬂects	O
their	O
relatively	O
modest	O
math-	O
ematical	O
modeling	O
assumptions	O
:	O
only	O
a	O
probability	O
model	B
f	O
(	O
more	O
exactly	O
a	O
family	O
of	O
probabilities	B
,	O
chapter	O
3	O
)	O
and	O
an	O
algorithm	B
of	O
choice	O
t	B
.x/	O
.	O
this	O
ﬂexibility	O
is	O
also	O
a	O
defect	O
in	O
that	O
the	O
principle	O
of	O
frequentist	O
correctness	O
doesn	O
’	O
t	B
help	O
with	O
the	O
choice	O
of	O
algorithm	B
.	O
should	O
we	O
use	O
the	O
sample	B
mean	O
to	O
estimate	B
the	O
location	O
of	O
the	O
gfr	O
distribution	B
?	O
maybe	O
the	O
25	O
%	O
win-	O
sorized	O
mean	O
would	O
be	O
better	O
,	O
as	O
table	O
2.1	O
suggests	O
.	O
the	O
years	O
1920–1935	O
saw	O
the	O
development	O
of	O
two	O
key	O
results	O
on	O
fre-	O
quentist	O
optimality	O
,	O
that	O
is	O
,	O
ﬁnding	O
the	O
best	O
choice	O
of	O
t	B
.x/	O
given	O
model	B
f	O
.	O
the	O
ﬁrst	O
of	O
these	O
was	O
fisher	O
’	O
s	O
theory	B
of	O
maximum	B
likelihood	I
estimation	O
and	O
the	O
fisher	O
information	B
bound	O
:	O
in	O
parametric	B
probability	O
models	B
of	O
the	O
type	O
discussed	O
in	O
chapter	O
4	O
,	O
the	O
mle	O
is	O
the	O
optimum	O
estimate	B
in	O
terms	O
of	O
minimum	O
(	O
asymptotic	O
)	O
standard	B
error	I
.	O
in	O
the	O
same	O
spirit	O
,	O
the	O
neyman–pearson	O
lemma	O
provides	O
an	O
optimum	O
hypothesis-testing	O
algorithm	B
.	O
this	O
is	O
perhaps	O
the	O
most	O
elegant	O
of	O
frequen-	O
tist	O
constructions	B
.	O
in	O
its	O
simplest	O
formulation	O
,	O
the	O
np	O
lemma	O
assumes	O
we	O
are	O
trying	O
to	O
decide	O
between	O
two	O
possible	O
probability	O
density	B
functions	O
for	O
the	O
observed	O
data	B
x	O
,	O
a	O
null	O
hypothesis	O
density	O
f0.x/	O
and	O
an	O
alternative	O
density	B
f1.x/	O
.	O
a	O
testing	B
rule	O
t	B
.x/	O
says	O
which	O
choice	O
,	O
0	O
or	O
1	O
,	O
we	O
will	O
make	O
having	O
observed	O
data	B
x.	O
any	O
such	O
rule	B
has	O
two	O
associated	O
frequentist	O
error	O
probabilities	B
:	O
choosing	O
f1	O
when	O
actually	O
f0	O
generated	O
x	O
,	O
and	O
vice	O
versa	O
,	O
˛	O
d	O
prf0	O
ˇ	O
d	O
prf1	O
let	O
l.x/	O
be	O
the	O
likelihood	B
ratio	O
,	O
ft	O
.x/	O
d	O
1g	O
;	O
ft	O
.x/	O
d	O
0g	O
:	O
l.x/	O
d	O
f1.x/=f0.x/	O
(	O
2.20	O
)	O
(	O
2.21	O
)	O
7	O
the	O
list	O
of	O
devices	O
is	O
not	O
complete	O
.	O
asymptotic	O
calculations	O
play	O
a	O
major	O
role	O
,	O
as	O
do	O
more	O
elaborate	O
combinations	O
of	O
pivotality	O
and	O
the	O
plug-in	O
principle	O
;	O
see	O
the	O
discussion	O
of	O
approximate	O
bootstrap	O
conﬁdence	B
intervals	I
in	O
chapter	O
11.	O
and	O
deﬁne	O
the	O
testing	B
rule	O
tc.x/	O
by	O
tc.x/	O
d	O
2.2	O
frequentist	O
optimality	O
(	O
1	O
if	O
log	O
l.x/	O
(	O
cid:21	O
)	O
c	O
0	O
if	O
log	O
l.x/	O
<	O
c	O
:	O
19	O
(	O
2.22	O
)	O
there	O
is	O
one	O
such	O
rule	B
for	O
each	O
choice	O
of	O
the	O
cutoff	O
c.	O
the	O
neyman–pearson	O
lemma	O
says	O
that	O
only	O
rules	O
of	O
form	B
(	O
2.22	O
)	O
can	O
be	O
optimum	O
;	O
for	O
any	O
other	O
rule	B
t	O
.x/	O
there	O
will	O
be	O
a	O
rule	B
tc.x/	O
having	O
smaller	O
errors	B
of	O
both	O
kinds,8	O
˛c	O
<	O
˛	O
and	O
ˇc	O
<	O
ˇ	O
:	O
(	O
2.23	O
)	O
figure	O
2.2	O
neyman–pearson	O
alpha–beta	O
curve	O
for	O
f0	O
(	O
cid:24	O
)	O
f1	O
(	O
cid:24	O
)	O
cutoffs	O
c	O
d	O
:8	O
;	O
:6	O
;	O
:4	O
;	O
:	O
:	O
:	O
;	O
(	O
cid:0	O
)	O
:4.	O
n	O
.0	O
;	O
1/	O
,	O
n	O
.	O
:5	O
;	O
1/	O
,	O
and	O
sample	O
size	O
n	O
d	O
10.	O
red	O
dots	O
correspond	O
to	O
figure	O
2.2	O
graphs	O
.˛c	O
;	O
ˇc/	O
as	O
a	O
function	B
of	O
the	O
cutoff	O
c	O
,	O
for	O
the	O
case	O
where	O
x	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
x10/	O
is	O
obtained	O
by	O
independent	O
sampling	O
from	O
a	O
normal	B
distribution	O
,	O
n	O
.0	O
;	O
1/	O
for	O
f0	O
versus	O
n	O
.0:5	O
;	O
1/	O
for	O
f1	O
.	O
the	O
np	O
lemma	O
says	O
that	O
any	O
rule	B
not	O
of	O
form	B
(	O
2.22	O
)	O
must	O
have	O
its	O
.˛	O
;	O
ˇ/	O
point	O
lying	O
above	O
the	O
curve	O
.	O
8	O
here	O
we	O
are	O
ignoring	O
some	O
minor	O
deﬁnitional	O
difﬁculties	O
that	O
can	O
occur	O
if	O
f0	O
and	O
f1	O
are	O
discrete	O
.	O
0.00.20.40.60.81.00.00.20.40.60.81.0ablllllllc	O
=	O
.4a	O
=	O
.10	O
,	O
b	O
=	O
.38	O
20	O
frequentist	O
inference	B
frequentist	O
optimality	O
theory	B
,	O
both	O
for	O
estimation	B
and	O
for	O
testing	B
,	O
an-	O
chored	O
statistical	O
practice	O
in	O
the	O
twentieth	O
century	O
.	O
the	O
larger	O
data	B
sets	O
and	O
more	O
complicated	O
inferential	O
questions	O
of	O
the	O
current	O
era	O
have	O
strained	O
the	O
capabilities	O
of	O
that	O
theory	B
.	O
computer-age	O
statistical	O
inference	B
,	O
as	O
we	O
will	O
see	O
,	O
often	O
displays	O
an	O
unsettling	O
ad	O
hoc	O
character	O
.	O
perhaps	O
some	O
contem-	O
porary	O
fishers	O
and	O
neymans	O
will	O
provide	O
us	O
with	O
a	O
more	O
capacious	O
opti-	O
mality	O
theory	B
equal	O
to	O
the	O
challenges	O
of	O
current	O
practice	O
,	O
but	O
for	O
now	O
that	O
is	O
only	O
a	O
hope	O
.	O
frequentism	O
can	O
not	O
claim	O
to	O
be	O
a	O
seamless	O
philosophy	O
of	O
statistical	O
in-	O
ference	O
.	O
paradoxes	O
and	O
contradictions	O
abound	O
within	O
its	O
borders	O
,	O
as	O
will	O
be	O
shown	O
in	O
the	O
next	O
chapter	O
.	O
that	O
being	O
said	O
,	O
frequentist	O
methods	O
have	O
a	O
natural	O
appeal	O
to	O
working	O
scientists	O
,	O
an	O
impressive	O
history	O
of	O
success-	O
ful	O
application	O
,	O
and	O
,	O
as	O
our	O
list	O
of	O
ﬁve	O
“	O
devices	O
”	O
suggests	O
,	O
the	O
capacity	O
to	O
encourage	O
clever	O
methodology	O
.	O
the	O
story	O
that	O
follows	O
is	O
not	O
one	O
of	O
aban-	O
donment	O
of	O
frequentist	O
thinking	O
,	O
but	O
rather	O
a	O
broadening	O
of	O
connections	O
with	O
other	O
methods	O
.	O
2.3	O
notes	O
and	O
details	O
the	O
name	O
“	O
frequentism	O
”	O
seems	O
to	O
have	O
been	O
suggested	O
by	O
neyman	O
as	O
a	O
statistical	O
analogue	O
of	O
richard	O
von	O
mises	O
’	O
frequentist	O
theory	B
of	O
probability	O
,	O
the	O
connection	O
being	O
made	O
explicit	O
in	O
his	O
1977	O
paper	O
,	O
“	O
frequentist	O
prob-	O
ability	O
and	O
frequentist	O
statistics.	O
”	O
“	O
behaviorism	O
”	O
might	O
have	O
been	O
a	O
more	O
descriptive	O
name9	O
since	O
the	O
theory	B
revolves	O
around	O
the	O
long-run	O
behavior	O
of	O
statistics	B
t	O
.x/	O
,	O
but	O
in	O
any	O
case	O
“	O
frequentism	O
”	O
has	O
stuck	O
,	O
replacing	O
the	O
older	O
(	O
sometimes	O
disparaging	O
)	O
term	O
“	O
objectivism.	O
”	O
neyman	O
’	O
s	O
attempt	O
at	O
a	O
complete	O
frequentist	O
theory	B
of	O
statistical	O
inference	B
,	O
“	O
inductive	O
behavior	O
,	O
”	O
is	O
not	O
much	O
quoted	O
today	O
,	O
but	O
can	O
claim	O
to	O
be	O
an	O
important	O
inﬂuence	O
on	O
wald	O
’	O
s	O
development	O
of	O
decision	O
theory	B
.	O
r.	O
a.	O
fisher	O
’	O
s	O
work	O
on	O
maximum	B
likelihood	I
estimation	O
is	O
featured	O
in	O
chapter	O
4.	O
fisher	O
,	O
arguably	O
the	O
founder	O
of	O
frequentist	O
optimality	O
theory	B
,	O
was	O
not	O
a	O
pure	O
frequentist	O
himself	O
,	O
as	O
discussed	O
in	O
chapter	O
4	O
and	O
efron	O
(	O
1998	O
)	O
,	O
“	O
r	O
.	O
a.	O
fisher	O
in	O
the	O
21st	O
century.	O
”	O
(	O
now	O
that	O
we	O
are	O
well	O
into	O
the	O
twenty-ﬁrst	O
century	O
,	O
the	O
author	O
’	O
s	O
talents	O
as	O
a	O
prognosticator	O
can	O
be	O
frequen-	O
tistically	O
evaluated	O
.	O
)	O
1	O
[	O
p.	O
15	O
]	O
delta	O
method	B
.	O
the	O
delta	O
method	B
uses	O
a	O
ﬁrst-order	O
taylor	O
series	O
to	O
	O
.	O
suppose	O
o	O
o	O
	O
	O
/	O
(	O
cid:25	O
)	O
s.	O
/	O
c	O
o	O
	O
/	O
of	O
a	O
statistic	B
o	O
approximate	O
the	O
variance	O
of	O
a	O
function	B
s.	O
has	O
mean/variance	O
.	O
;	O
(	O
cid:27	O
)	O
2/	O
,	O
and	O
consider	O
the	O
approximation	O
s.	O
9	O
that	O
name	O
is	O
already	O
spoken	O
for	O
in	O
the	O
psychology	O
literature	O
.	O
o	O
	O
(	O
cid:0	O
)	O
	O
/	O
.	O
hence	O
varfs	O
.	O
0	O
s	O
and	O
use	O
an	O
estimate	B
for	O
(	O
cid:27	O
)	O
2	O
.	O
.	O
/	O
.	O
2.3	O
notes	O
and	O
details	O
o	O
	O
/g	O
(	O
cid:25	O
)	O
js	O
.	O
/j2	O
(	O
cid:27	O
)	O
2.	O
we	O
typically	O
plug-in	O
o	O
0	O
21	O
	O
for	O
	O
,	O
3	O
bayesian	O
inference	B
the	O
human	O
mind	O
is	O
an	O
inference	B
machine	O
:	O
“	O
it	O
’	O
s	O
getting	O
windy	O
,	O
the	O
sky	O
is	O
darkening	O
,	O
i	O
’	O
d	O
better	O
bring	O
my	O
umbrella	O
with	O
me.	O
”	O
unfortunately	O
,	O
it	O
’	O
s	O
not	O
a	O
very	O
dependable	O
machine	O
,	O
especially	O
when	O
weighing	O
complicated	O
choices	O
against	O
past	O
experience	O
.	O
bayes	O
’	O
theorem	B
is	O
a	O
surprisingly	O
simple	O
mathemat-	O
ical	O
guide	O
to	O
accurate	O
inference	B
.	O
the	O
theorem	B
(	O
or	O
“	O
rule	B
”	O
)	O
,	O
now	O
250	O
years	O
old	O
,	O
marked	O
the	O
beginning	O
of	O
statistical	O
inference	B
as	O
a	O
serious	O
scientiﬁc	O
sub-	O
ject	O
.	O
it	O
has	O
waxed	O
and	O
waned	O
in	O
inﬂuence	O
over	O
the	O
centuries	O
,	O
now	O
waxing	O
again	O
in	O
the	O
service	O
of	O
computer-age	O
applications	O
.	O
bayesian	O
inference	B
,	O
if	O
not	O
directly	O
opposed	O
to	O
frequentism	O
,	O
is	O
at	O
least	O
or-	O
thogonal	O
.	O
it	O
reveals	O
some	O
worrisome	O
ﬂaws	O
in	O
the	O
frequentist	O
point	O
of	O
view	O
,	O
while	O
at	O
the	O
same	O
time	O
exposing	O
itself	O
to	O
the	O
criticism	O
of	O
dangerous	O
overuse	O
.	O
the	O
struggle	O
to	O
combine	O
the	O
virtues	O
of	O
the	O
two	O
philosophies	O
has	O
become	O
more	O
acute	O
in	O
an	O
era	O
of	O
massively	O
complicated	O
data	B
sets	O
.	O
much	O
of	O
what	O
follows	O
in	O
succeeding	O
chapters	O
concerns	O
this	O
struggle	O
.	O
here	O
we	O
will	O
review	O
some	O
basic	O
bayesian	O
ideas	O
and	O
the	O
ways	O
they	O
impinge	O
on	O
frequentism	O
.	O
the	O
fundamental	O
unit	O
of	O
statistical	O
inference	B
both	O
for	O
frequentists	O
and	O
for	O
bayesians	O
is	O
a	O
family	O
of	O
probability	O
densities	O
d˚f	O
(	O
cid:22	O
)	O
.x/i	O
x	O
2	O
x	O
;	O
(	O
cid:22	O
)	O
2	O
(	O
cid:127	O
)	O
(	O
cid:9	O
)	O
i	O
f	O
(	O
3.1	O
)	O
x	O
,	O
the	O
observed	O
data	B
,	O
is	O
a	O
point1	O
in	O
the	O
sample	B
space	O
x	O
,	O
while	O
the	O
unob-	O
served	O
parameter	O
(	O
cid:22	O
)	O
is	O
a	O
point	O
in	O
the	O
parameter	O
space	B
(	O
cid:127	O
)	O
.	O
the	O
statistician	O
observes	O
x	O
from	O
f	O
(	O
cid:22	O
)	O
.x/	O
,	O
and	O
infers	O
the	O
value	O
of	O
(	O
cid:22	O
)	O
.	O
perhaps	O
the	O
most	O
familiar	O
case	O
is	O
the	O
normal	B
family	O
f	O
(	O
cid:22	O
)	O
.x/	O
d	O
1p	O
2	O
(	O
cid:25	O
)	O
(	O
cid:0	O
)	O
1	O
2	O
.x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
/2	O
e	O
(	O
3.2	O
)	O
1	O
both	O
x	O
and	O
(	O
cid:22	O
)	O
may	O
be	O
scalars	O
,	O
vectors	O
,	O
or	O
more	O
complicated	O
objects	O
.	O
other	O
names	O
for	O
the	O
generic	O
“	O
x	O
”	O
and	O
“	O
(	O
cid:22	O
)	O
”	O
occur	O
in	O
speciﬁc	O
situations	O
,	O
for	O
instance	O
x	O
for	O
x	O
in	O
chapter	O
2.	O
we	O
will	O
also	O
call	O
f	O
a	O
“	O
family	O
of	O
probability	O
distributions.	O
”	O
22	O
bayesian	O
inference	B
23	O
(	O
more	O
exactly	O
,	O
the	O
one-dimensional	O
normal	B
translation	O
family2	O
with	O
vari-	O
ance	O
1	O
)	O
,	O
with	O
both	O
x	O
and	O
(	O
cid:127	O
)	O
equaling	O
r1	O
,	O
the	O
entire	O
real	O
line	O
.	O
(	O
cid:0	O
)	O
1	O
;	O
1/	O
.	O
another	O
central	O
example	O
is	O
the	O
poisson	O
family	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
(	O
cid:22	O
)	O
x=xš	O
;	O
f	O
(	O
cid:22	O
)	O
.x/	O
d	O
e	O
where	O
x	O
is	O
the	O
nonnegative	O
integers	O
f0	O
;	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
g	O
and	O
(	O
cid:127	O
)	O
is	O
the	O
nonnegative	O
real	O
line	O
.0	O
;	O
1/	O
.	O
(	O
here	O
the	O
“	O
density	B
”	O
(	O
3.3	O
)	O
speciﬁes	O
the	O
atoms	O
of	O
probability	O
on	O
the	O
discrete	O
points	O
of	O
x	O
.	O
)	O
probability	O
family	O
f	O
,	O
the	O
knowledge	O
of	O
a	O
prior	B
density	O
bayesian	O
inference	B
requires	O
one	O
crucial	O
assumption	O
in	O
addition	O
to	O
the	O
g.	O
(	O
cid:22	O
)	O
/	O
;	O
(	O
cid:22	O
)	O
2	O
(	O
cid:127	O
)	O
i	O
g.	O
(	O
cid:22	O
)	O
/	O
represents	O
prior	B
information	O
concerning	O
the	O
parameter	O
(	O
cid:22	O
)	O
,	O
available	O
to	O
the	O
statistician	O
before	O
the	O
observation	O
of	O
x.	O
for	O
instance	O
,	O
in	O
an	O
application	O
of	O
the	O
normal	B
model	O
(	O
3.2	O
)	O
,	O
it	O
could	O
be	O
known	O
that	O
(	O
cid:22	O
)	O
is	O
positive	O
,	O
while	O
past	O
experience	O
shows	O
it	O
never	O
exceeding	O
10	O
,	O
in	O
which	O
case	O
we	O
might	O
take	O
g.	O
(	O
cid:22	O
)	O
/	O
to	O
be	O
the	O
uniform	O
density	B
g.	O
(	O
cid:22	O
)	O
/	O
d	O
1=10	O
on	O
the	O
interval	B
œ0	O
;	O
10	O
.	O
exactly	O
what	O
constitutes	O
“	O
prior	B
knowledge	O
”	O
is	O
a	O
crucial	O
question	O
we	O
will	O
consider	O
in	O
ongoing	O
discussions	O
of	O
bayes	O
’	O
theorem	B
.	O
bayes	O
’	O
theorem	B
is	O
a	O
rule	B
for	O
combining	O
the	O
prior	B
knowledge	O
in	O
g.	O
(	O
cid:22	O
)	O
/	O
with	O
the	O
current	O
evidence	O
in	O
x.	O
let	O
g.	O
(	O
cid:22	O
)	O
jx/	O
denote	O
the	O
posterior	O
density	O
of	O
(	O
cid:22	O
)	O
,	O
that	O
is	O
,	O
our	O
update	O
of	O
the	O
prior	B
density	O
g.	O
(	O
cid:22	O
)	O
/	O
after	O
taking	O
account	O
of	O
observation	O
x.	O
bayes	O
’	O
rule	B
provides	O
a	O
simple	O
expression	O
for	O
g.	O
(	O
cid:22	O
)	O
jx/	O
in	O
terms	O
of	O
g.	O
(	O
cid:22	O
)	O
/	O
and	O
f.	O
bayes	O
’	O
rule	B
:	O
g.	O
(	O
cid:22	O
)	O
jx/	O
d	O
g.	O
(	O
cid:22	O
)	O
/f	O
(	O
cid:22	O
)	O
.x/=f	O
.x/	O
;	O
(	O
cid:22	O
)	O
2	O
(	O
cid:127	O
)	O
;	O
(	O
3.3	O
)	O
(	O
3.4	O
)	O
(	O
3.5	O
)	O
(	O
3.6	O
)	O
where	O
f	O
.x/	O
is	O
the	O
marginal	O
density	B
of	O
x	O
,	O
f	O
.x/	O
dz	O
f	O
(	O
cid:22	O
)	O
.x/g	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
(	O
cid:22	O
)	O
:	O
(	O
cid:127	O
)	O
(	O
the	O
integral	O
in	O
(	O
3.6	O
)	O
would	O
be	O
a	O
sum	O
if	O
(	O
cid:127	O
)	O
were	O
discrete	O
.	O
)	O
the	O
rule	B
is	O
a	O
straightforward	O
exercise	O
in	O
conditional	O
probability,3	O
and	O
yet	O
has	O
far-reaching	O
and	O
sometimes	O
surprising	O
consequences	O
.	O
in	O
bayes	O
’	O
formula	B
(	O
3.5	O
)	O
,	O
x	O
is	O
ﬁxed	O
at	O
its	O
observed	O
value	O
while	O
(	O
cid:22	O
)	O
varies	O
over	O
(	O
cid:127	O
)	O
,	O
just	O
the	O
opposite	O
of	O
frequentist	O
calculations	O
.	O
we	O
can	O
emphasize	O
this	O
2	O
standard	O
notation	O
is	O
x	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
2/	O
for	O
a	O
normal	B
distribution	O
with	O
expectation	O
(	O
cid:22	O
)	O
and	O
variance	O
(	O
cid:27	O
)	O
2	O
,	O
so	O
(	O
3.2	O
)	O
has	O
x	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
;	O
1/	O
.	O
3	O
g.	O
(	O
cid:22	O
)	O
jx/	O
is	O
the	O
ratio	O
of	O
g.	O
(	O
cid:22	O
)	O
/f	O
(	O
cid:22	O
)	O
.x/	O
,	O
the	O
joint	O
probability	O
of	O
the	O
pair	O
.	O
(	O
cid:22	O
)	O
;	O
x/	O
,	O
and	O
f	O
.x/	O
,	O
the	O
marginal	O
probability	O
of	O
x	O
.	O
24	O
bayesian	O
inference	B
by	O
rewriting	O
(	O
3.5	O
)	O
as	O
g.	O
(	O
cid:22	O
)	O
jx/	O
d	O
cxlx.	O
(	O
cid:22	O
)	O
/g	O
.	O
(	O
cid:22	O
)	O
/	O
;	O
(	O
3.7	O
)	O
where	O
lx	O
.	O
(	O
cid:22	O
)	O
/	O
is	O
the	O
likelihood	B
function	O
,	O
that	O
is	O
,	O
f	O
(	O
cid:22	O
)	O
.x/	O
with	O
x	O
ﬁxed	O
and	O
(	O
cid:22	O
)	O
varying	O
.	O
having	O
computed	O
lx.	O
(	O
cid:22	O
)	O
/g	O
.	O
(	O
cid:22	O
)	O
/	O
,	O
the	O
constant	O
cx	O
can	O
be	O
determined	O
numerically	O
from	O
the	O
requirement	O
that	O
g.	O
(	O
cid:22	O
)	O
jx/	O
integrate	O
to	O
1	O
,	O
obviating	O
the	O
calculation	O
of	O
f	O
.x/	O
(	O
3.6	O
)	O
.	O
note	O
multiplying	O
the	O
likelihood	B
function	O
by	O
any	O
ﬁxed	O
constant	O
c0	O
has	O
no	O
effect	O
on	O
(	O
3.7	O
)	O
since	O
c0	O
can	O
be	O
absorbed	O
into	O
cx	O
.	O
so	O
for	O
the	O
poisson	O
family	O
(	O
3.3	O
)	O
we	O
can	O
take	O
lx	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
e	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
(	O
cid:22	O
)	O
x	O
,	O
ignoring	O
the	O
xš	O
factor	B
,	O
which	O
acts	O
as	O
a	O
constant	O
in	O
bayes	O
’	O
rule	B
.	O
the	O
luxury	O
of	O
ignoring	O
factors	O
depending	O
only	O
on	O
x	O
often	O
simpliﬁes	O
bayesian	O
calculations	O
.	O
for	O
any	O
two	O
points	O
(	O
cid:22	O
)	O
1	O
and	O
(	O
cid:22	O
)	O
2	O
in	O
(	O
cid:127	O
)	O
,	O
the	O
ratio	O
of	O
posterior	O
densities	O
is	O
,	O
by	O
division	O
in	O
(	O
3.5	O
)	O
,	O
g.	O
(	O
cid:22	O
)	O
1jx/	O
g.	O
(	O
cid:22	O
)	O
2jx/	O
d	O
g.	O
(	O
cid:22	O
)	O
1/	O
g.	O
(	O
cid:22	O
)	O
2/	O
f	O
(	O
cid:22	O
)	O
1	O
.x/	O
f	O
(	O
cid:22	O
)	O
2	O
.x/	O
(	O
3.8	O
)	O
(	O
no	O
longer	O
involving	O
the	O
marginal	O
density	B
f	O
.x/	O
)	O
,	O
that	O
is	O
,	O
“	O
the	O
posterior	O
odds	O
ratio	O
is	O
the	O
prior	B
odds	O
ratio	O
times	O
the	O
likelihood	B
ratio	O
,	O
”	O
a	O
memorable	O
restate-	O
ment	O
of	O
bayes	O
’	O
rule	B
.	O
3.1	O
two	O
examples	O
a	O
simple	O
but	O
genuine	O
example	O
of	O
bayes	O
’	O
rule	B
in	O
action	O
is	O
provided	O
by	O
the	O
story	O
of	O
the	O
physicist	O
’	O
s	O
twins	O
:	O
thanks	O
to	O
sonograms	O
,	O
a	O
physicist	O
found	O
out	O
she	O
was	O
going	O
to	O
have	O
twin	O
boys	O
.	O
“	O
what	O
is	O
the	O
probability	O
my	O
twins	O
will	O
be	O
identical	O
,	O
rather	O
than	O
fraternal	O
?	O
”	O
she	O
asked	O
.	O
the	O
doctor	O
answered	O
that	O
one-third	O
of	O
twin	O
births	O
were	O
identicals	O
,	O
and	O
two-thirds	O
fraternals	O
.	O
in	O
this	O
situation	O
(	O
cid:22	O
)	O
,	O
the	O
unknown	O
parameter	O
(	O
or	O
“	O
state	O
of	O
nature	O
”	O
)	O
is	O
either	O
identical	O
or	O
fraternal	O
with	O
prior	B
probability	O
1/3	O
or	O
2/3	O
;	O
x	O
,	O
the	O
possible	O
sonogram	O
results	O
for	O
twin	O
births	O
,	O
is	O
either	O
same	O
sex	O
or	O
different	O
sexes	O
,	O
and	O
x	O
d	O
same	O
sex	O
was	O
observed	O
.	O
(	O
we	O
can	O
ignore	O
sex	O
since	O
that	O
does	O
not	O
affect	O
the	O
calculation	O
.	O
)	O
a	O
crucial	O
fact	O
is	O
that	O
identical	O
twins	O
are	O
always	O
same-sex	O
while	O
fraternals	O
have	O
probability	O
0.5	O
of	O
same	O
or	O
different	O
,	O
so	O
same	O
sex	O
in	O
the	O
sonogram	O
is	O
twice	O
as	O
likely	O
if	O
the	O
twins	O
are	O
identical	O
.	O
applying	O
bayes	O
’	O
3.1	O
two	O
examples	O
rule	B
in	O
ratio	O
form	B
(	O
3.8	O
)	O
answers	O
the	O
physicist	O
’	O
s	O
question	O
:	O
g.identicalj	O
same/	O
g.fraternalj	O
same/	O
d	O
g.identical/	O
g.fraternal/	O
(	O
cid:1	O
)	O
fidentical.same/	O
ffraternal.same/	O
d	O
1	O
:	O
d	O
1=3	O
2=3	O
(	O
cid:1	O
)	O
1	O
1=2	O
25	O
(	O
3.9	O
)	O
that	O
is	O
,	O
the	O
posterior	O
odds	O
are	O
even	O
,	O
and	O
the	O
physicist	O
’	O
s	O
twins	O
have	O
equal	O
probabilities	B
0.5	O
of	O
being	O
identical	O
or	O
fraternal.4	O
here	O
the	O
doctor	O
’	O
s	O
prior	B
odds	O
ratio	O
,	O
2	O
to	O
1	O
in	O
favor	O
of	O
fraternal	O
,	O
is	O
balanced	O
out	O
by	O
the	O
sonogram	O
’	O
s	O
likelihood	B
ratio	O
of	O
2	O
to	O
1	O
in	O
favor	O
of	O
identical	O
.	O
figure	O
3.1	O
analyzing	O
the	O
twins	O
problem	O
.	O
there	O
are	O
only	O
four	O
possible	O
combinations	O
of	O
parameter	O
(	O
cid:22	O
)	O
and	O
outcome	O
x	O
in	O
the	O
twins	O
problem	O
,	O
labeled	O
a	O
,	O
b	O
,	O
c	O
,	O
and	O
d	O
in	O
figure	O
3.1.	O
cell	O
b	O
has	O
probability	O
0	O
since	O
identicals	O
can	O
not	O
be	O
of	O
different	O
sexes	O
.	O
cells	O
c	O
and	O
d	O
have	O
equal	O
probabilities	B
because	O
of	O
the	O
random	O
sexes	O
of	O
fraternals	O
.	O
finally	O
,	O
a	O
c	O
b	O
must	O
have	O
total	O
probability	O
1/3	O
,	O
and	O
c	O
c	O
d	O
total	O
probability	O
2/3	O
,	O
according	O
to	O
the	O
doctor	O
’	O
s	O
prior	B
distribution	I
.	O
putting	O
all	O
this	O
together	O
,	O
we	O
can	O
ﬁll	O
in	O
the	O
probabilities	B
for	O
all	O
four	O
cells	O
,	O
as	O
shown	O
.	O
the	O
physicist	O
knows	O
she	O
is	O
in	O
the	O
ﬁrst	O
column	O
of	O
the	O
table	O
,	O
where	O
the	O
conditional	O
probabilities	B
of	O
identical	O
or	O
fraternal	O
are	O
equal	O
,	O
just	O
as	O
provided	O
by	O
bayes	O
’	O
rule	B
in	O
(	O
3.9	O
)	O
.	O
presumably	O
the	O
doctor	O
’	O
s	O
prior	B
distribution	I
came	O
from	O
some	O
enormous	O
state	O
or	O
national	O
database	O
,	O
say	O
three	O
million	O
previous	O
twin	O
births	O
,	O
one	O
mil-	O
lion	O
identical	O
pairs	O
and	O
two	O
million	O
fraternals	O
.	O
we	O
deduce	O
that	O
cells	O
a	O
,	O
c	O
,	O
and	O
d	O
must	O
have	O
had	O
one	O
million	O
entries	O
each	O
in	O
the	O
database	O
,	O
while	O
cell	O
b	O
was	O
empty	O
.	O
bayes	O
’	O
rule	B
can	O
be	O
thought	O
of	O
as	O
a	O
big	O
book	O
with	O
one	O
page	O
4	O
they	O
turned	O
out	O
to	O
be	O
fraternal	O
.	O
1	O
identical	O
twins	O
are	O
:	O
fraternal	O
same	O
sex	O
different	O
physicist	O
sonogram	O
shows	O
:	O
doctor	O
2/3	O
1/3	O
1/3	O
1/3	O
0	O
1/3	O
b	O
a	O
c	O
d	O
26	O
bayesian	O
inference	B
for	O
each	O
possible	O
outcome	O
x	O
.	O
(	O
the	O
book	O
has	O
only	O
two	O
pages	O
in	O
figure	O
3.1	O
.	O
)	O
the	O
physicist	O
turns	O
to	O
the	O
page	O
“	O
same	O
sex	O
”	O
and	O
sees	O
two	O
million	O
previous	O
twin	O
births	O
,	O
half	O
identical	O
and	O
half	O
fraternal	O
,	O
correctly	O
concluding	O
that	O
the	O
odds	O
are	O
equal	O
in	O
her	O
situation	O
.	O
given	O
any	O
prior	B
distribution	I
g.	O
(	O
cid:22	O
)	O
/	O
and	O
any	O
family	O
of	O
densities	O
f	O
(	O
cid:22	O
)	O
.x/	O
,	O
bayes	O
’	O
rule	B
will	O
always	O
provide	O
a	O
version	O
of	O
the	O
big	O
book	O
.	O
that	O
doesn	O
’	O
t	B
mean	O
that	O
the	O
book	O
’	O
s	O
contents	O
will	O
always	O
be	O
equally	O
convincing	O
.	O
the	O
prior	B
for	O
the	O
twins	O
problems	O
was	O
based	O
on	O
a	O
large	O
amount	O
of	O
relevant	O
previous	O
experience	O
.	O
such	O
experience	O
is	O
most	O
often	O
unavailable	O
.	O
modern	O
bayesian	O
practice	O
uses	O
various	O
strategies	O
to	O
construct	O
an	O
appropriate	O
“	O
prior	B
”	O
g.	O
(	O
cid:22	O
)	O
/	O
in	O
the	O
absence	O
of	O
prior	B
experience	O
,	O
leaving	O
many	O
statisticians	O
unconvinced	O
by	O
the	O
resulting	O
bayesian	O
inferences	O
.	O
our	O
second	O
example	O
illustrates	O
the	O
difﬁculty	O
.	O
table	O
3.1	O
scores	O
from	O
two	O
tests	O
taken	O
by	O
22	O
students	O
,	O
mechanics	O
and	O
vectors	O
.	O
mechanics	O
vectors	O
mechanics	O
vectors	O
1	O
7	O
51	O
12	O
36	O
59	O
2	O
44	O
69	O
13	O
42	O
60	O
3	O
49	O
41	O
14	O
5	O
30	O
4	O
59	O
70	O
15	O
22	O
58	O
5	O
34	O
42	O
16	O
18	O
51	O
6	O
46	O
40	O
17	O
41	O
63	O
7	O
0	O
40	O
18	O
48	O
38	O
8	O
32	O
45	O
19	O
31	O
42	O
9	O
49	O
57	O
20	O
42	O
69	O
10	O
52	O
64	O
21	O
46	O
49	O
11	O
44	O
61	O
22	O
63	O
63	O
table	O
3.1	O
shows	O
the	O
scores	O
on	O
two	O
tests	O
,	O
mechanics	O
and	O
vectors	O
,	O
achieved	O
by	O
n	O
d	O
22	O
students	O
.	O
the	O
sample	B
correlation	O
coefﬁcient	O
between	O
the	O
two	O
scores	O
is	O
o	O
o	O
,	O
''	O
22x	O
	O
d	O
0:498	O
,	O
#	O
1=2	O
.mi	O
(	O
cid:0	O
)	O
nm/.vi	O
(	O
cid:0	O
)	O
nv/	O
;	O
(	O
3.10	O
)	O
	O
d	O
22x	O
id1	O
22x	O
.vi	O
(	O
cid:0	O
)	O
nv/2	O
id1	O
.mi	O
(	O
cid:0	O
)	O
nm/2	O
id1	O
with	O
m	O
and	O
v	O
short	O
for	O
mechanics	O
and	O
vectors	O
,	O
nm	O
and	O
nv	O
their	O
aver-	O
ages	O
.	O
we	O
wish	O
to	O
assign	O
a	O
bayesian	O
measure	O
of	O
posterior	O
accuracy	O
to	O
the	O
true	O
correlation	B
coefﬁcient	I
	O
,	O
“	O
true	O
”	O
meaning	O
the	O
correlation	O
for	O
the	O
hypo-	O
thetical	O
population	O
of	O
all	O
students	O
,	O
of	O
which	O
we	O
observed	O
only	O
22.	O
if	O
we	O
assume	O
that	O
the	O
joint	O
.m	O
;	O
v/	O
distribution	B
is	O
bivariate	O
normal	B
(	O
as	O
discussed	O
in	O
chapter	O
5	O
)	O
,	O
then	O
the	O
density	B
of	O
o	O
	O
as	O
a	O
function	B
of	O
	O
has	O
a	O
known	O
form	B
,	O
	O
1	O
3.1	O
two	O
examples	O
	O
2.n	O
(	O
cid:0	O
)	O
4/=2	O
1	O
(	O
cid:0	O
)	O
o	O
z	O
1	O
27	O
n	O
(	O
cid:0	O
)	O
1	O
:	O
(	O
cid:16	O
)	O
cosh	O
w	O
(	O
cid:0	O
)	O
	O
dw	O
	O
d	O
.n	O
(	O
cid:0	O
)	O
2/.1	O
(	O
cid:0	O
)	O
	O
2/.n	O
(	O
cid:0	O
)	O
1/=2	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
o	O
	O
(	O
cid:25	O
)	O
f	O
o	O
	O
(	O
3.11	O
)	O
in	O
terms	O
of	O
our	O
general	O
bayes	O
notation	O
,	O
parameter	O
(	O
cid:22	O
)	O
is	O
	O
,	O
observation	O
x	O
is	O
o	O
	O
,	O
and	O
family	O
f	O
is	O
given	O
by	O
(	O
3.11	O
)	O
,	O
with	O
both	O
(	O
cid:127	O
)	O
and	O
x	O
equaling	O
the	O
interval	B
œ	O
(	O
cid:0	O
)	O
1	O
;	O
1	O
.	O
formula	B
(	O
3.11	O
)	O
looks	O
formidable	O
to	O
the	O
human	O
eye	O
but	O
not	O
to	O
the	O
computer	O
eye	O
,	O
which	O
makes	O
quick	O
work	O
of	O
it	O
.	O
0	O
figure	O
3.2	O
student	O
scores	O
data	B
;	O
posterior	O
density	O
of	O
correlation	O
	O
for	O
three	O
possible	O
priors	B
.	O
in	O
this	O
case	O
,	O
as	O
in	O
the	O
majority	O
of	O
scientiﬁc	O
situations	O
,	O
we	O
don	O
’	O
t	B
have	O
a	O
trove	O
of	O
relevant	O
past	O
experience	O
ready	O
to	O
provide	O
a	O
prior	B
g.	O
/	O
.	O
one	O
expe-	O
dient	O
,	O
going	O
back	O
to	O
laplace	O
,	O
is	O
the	O
“	O
principle	O
of	O
insufﬁcient	O
reason	O
,	O
”	O
that	O
is	O
,	O
we	O
take	O
	O
to	O
be	O
uniformly	O
distributed	O
over	O
(	O
cid:127	O
)	O
,	O
g.	O
/	O
d	O
1	O
2	O
for	O
(	O
cid:0	O
)	O
1	O
	O
	O
	O
1	O
;	O
(	O
3.12	O
)	O
a	O
“	O
ﬂat	O
prior.	O
”	O
the	O
solid	O
black	O
curve	O
in	O
figure	O
3.2	O
shows	O
the	O
resulting	O
poste-	O
rior	O
density	B
(	O
3.5	O
)	O
,	O
which	O
is	O
just	O
the	O
likelihood	B
f	O
.0:498/	O
plotted	O
as	O
a	O
func-	O
tion	O
of	O
	O
(	O
and	O
scaled	O
to	O
have	O
integral	O
1	O
)	O
.	O
−0.20.00.20.40.60.81.00.00.51.01.52.02.5qg	O
(	O
q|q^	O
)	O
mle	O
.498flat	O
priorjeffreystriangularl.093.750	O
28	O
bayesian	O
inference	B
jeffreys	O
’	O
prior	B
,	O
gjeff.	O
/	O
d	O
1=.1	O
(	O
cid:0	O
)	O
	O
2/	O
;	O
(	O
3.13	O
)	O
yields	O
posterior	O
density	O
g.j	O
o	O
	O
/	O
shown	O
by	O
the	O
dashed	O
red	O
curve	O
.	O
it	O
suggests	O
somewhat	O
bigger	O
values	O
for	O
the	O
unknown	O
parameter	O
	O
.	O
formula	B
(	O
3.13	O
)	O
arises	O
from	O
a	O
theory	B
of	O
“	O
uninformative	O
priors	B
”	O
discussed	O
in	O
the	O
next	O
sec-	O
tion	O
,	O
an	O
improvement	O
on	O
the	O
principle	O
of	O
insufﬁcient	O
reason	O
;	O
(	O
3.13	O
)	O
is	O
an	O
improper	O
density	B
in	O
thatr	O
1	O
(	O
cid:0	O
)	O
1	O
g.	O
/	O
d	O
d	O
1	O
,	O
but	O
it	O
still	O
provides	O
proper	B
pos-	O
terior	O
densities	O
when	O
deployed	O
in	O
bayes	O
’	O
rule	B
(	O
3.5	O
)	O
.	O
the	O
dotted	O
blue	O
curve	O
in	O
figure	O
3.2	O
is	O
posterior	O
density	O
g.j	O
o	O
from	O
the	O
triangular-shaped	O
prior	B
	O
/	O
obtained	O
g.	O
/	O
d	O
1	O
(	O
cid:0	O
)	O
jj	O
:	O
(	O
3.14	O
)	O
this	O
is	O
a	O
primitive	O
example	O
of	O
a	O
shrinkage	B
prior	O
,	O
one	O
designed	O
to	O
favor	O
smaller	O
values	O
of	O
	O
.	O
its	O
effect	O
is	O
seen	O
in	O
the	O
leftward	O
shift	O
of	O
the	O
posterior	O
density	O
.	O
shrinkage	B
priors	O
will	O
play	O
a	O
major	O
role	O
in	O
our	O
discussion	O
of	O
large-	O
scale	B
estimation	O
and	O
testing	O
problems	O
,	O
where	O
we	O
are	O
hoping	O
to	O
ﬁnd	O
a	O
few	O
large	O
effects	O
hidden	O
among	O
thousands	O
of	O
negligible	O
ones	O
.	O
3.2	O
uninformative	O
prior	B
distributions	O
given	O
a	O
convincing	O
prior	B
distribution	I
,	O
bayes	O
’	O
rule	B
is	O
easier	O
to	O
use	O
and	O
pro-	O
duces	O
more	O
satisfactory	O
inferences	O
than	O
frequentist	O
methods	O
.	O
the	O
domi-	O
nance	O
of	O
frequentist	O
practice	O
reﬂects	O
the	O
scarcity	O
of	O
useful	O
prior	B
information	O
in	O
day-to-day	O
scientiﬁc	O
applications	O
.	O
but	O
the	O
bayesian	O
impulse	O
is	O
strong	O
,	O
and	O
almost	O
from	O
its	O
inception	O
250	O
years	O
ago	O
there	O
have	O
been	O
proposals	O
for	O
the	O
construction	O
of	O
“	O
priors	B
”	O
that	O
permit	O
the	O
use	O
of	O
bayes	O
’	O
rule	B
in	O
the	O
ab-	O
sence	O
of	O
relevant	O
experience	O
.	O
one	O
approach	O
,	O
perhaps	O
the	O
most	O
inﬂuential	O
in	O
current	O
practice	O
,	O
is	O
the	O
employment	O
of	O
uninformative	O
priors	B
.	O
“	O
uninformative	O
”	O
has	O
a	O
positive	O
con-	O
notation	O
here	O
,	O
implying	O
that	O
the	O
use	O
of	O
such	O
a	O
prior	B
in	O
bayes	O
’	O
rule	B
does	O
not	O
tacitly	O
bias	O
the	O
resulting	O
inference	B
.	O
laplace	O
’	O
s	O
principle	O
of	O
insufﬁcient	O
rea-	O
son	O
,	O
i.e.	O
,	O
assigning	O
uniform	O
prior	B
distributions	O
to	O
unknown	O
parameters	O
,	O
is	O
an	O
obvious	O
attempt	O
at	O
this	O
goal	O
.	O
its	O
use	O
went	O
unchallenged	O
for	O
more	O
than	O
a	O
century	O
,	O
perhaps	O
because	O
of	O
laplace	O
’	O
s	O
inﬂuence	O
more	O
than	O
its	O
own	O
virtues	O
.	O
venn	O
(	O
of	O
the	O
venn	O
diagram	O
)	O
in	O
the	O
1860s	O
,	O
and	O
fisher	O
in	O
the	O
1920s	O
,	O
attack-	O
ing	O
the	O
routine	O
use	O
of	O
bayes	O
’	O
theorem	B
,	O
pointed	O
out	O
that	O
laplace	O
’	O
s	O
principle	O
could	O
not	O
be	O
applied	O
consistently	O
.	O
in	O
the	O
student	O
correlation	O
example	O
,	O
for	O
instance	O
,	O
a	O
uniform	O
prior	B
distribution	I
for	O
	O
would	O
not	O
be	O
uniform	O
if	O
we	O
changed	O
parameters	O
to	O
(	O
cid:13	O
)	O
d	O
e	O
;	O
posterior	O
probabilities	O
such	O
as	O
3.2	O
uninformative	O
prior	B
distributions	O
n	O
	O
>	O
0j	O
o	O
	O
o	O
d	O
pr	O
n	O
(	O
cid:13	O
)	O
>	O
1j	O
o	O
	O
o	O
pr	O
29	O
(	O
3.15	O
)	O
would	O
depend	O
on	O
whether	O
	O
or	O
(	O
cid:13	O
)	O
was	O
taken	O
to	O
be	O
uniform	O
a	O
priori	O
.	O
neither	O
choice	O
then	O
could	O
be	O
considered	O
uninformative	O
.	O
a	O
more	O
sophisticated	O
version	O
of	O
laplace	O
’	O
s	O
principle	O
was	O
put	O
forward	O
by	O
jeffreys	O
beginning	O
in	O
the	O
1930s	O
.	O
it	O
depends	O
,	O
interestingly	O
enough	O
,	O
on	O
the	O
frequentist	O
notion	O
of	O
fisher	O
information	B
(	O
chapter	O
4	O
)	O
.	O
for	O
a	O
one-parameter	B
family	O
f	O
(	O
cid:22	O
)	O
.x/	O
,	O
where	O
the	O
parameter	O
space	B
(	O
cid:127	O
)	O
is	O
an	O
interval	B
of	O
the	O
real	O
line	O
r1	O
,	O
the	O
fisher	O
information	B
is	O
deﬁned	O
to	O
be	O
2	O
)	O
(	O
	O
@	O
@	O
(	O
cid:22	O
)	O
i	O
(	O
cid:22	O
)	O
d	O
e	O
(	O
cid:22	O
)	O
log	O
f	O
(	O
cid:22	O
)	O
.x/	O
:	O
(	O
3.16	O
)	O
(	O
for	O
the	O
poisson	O
family	O
(	O
3.3	O
)	O
,	O
@	O
=	O
@	O
(	O
cid:22	O
)	O
.log	O
f	O
(	O
cid:22	O
)	O
.x//	O
d	O
x=	O
(	O
cid:22	O
)	O
(	O
cid:0	O
)	O
1	O
and	O
i	O
(	O
cid:22	O
)	O
d	O
1=	O
(	O
cid:22	O
)	O
.	O
)	O
the	O
jeffreys	O
’	O
prior	B
gjeff	O
.	O
(	O
cid:22	O
)	O
/	O
is	O
by	O
deﬁnition	O
i	O
1=2	O
(	O
cid:22	O
)	O
:	O
gjeff	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
because	O
1=i	O
(	O
cid:22	O
)	O
equals	O
,	O
approximately	O
,	O
the	O
variance	O
(	O
cid:27	O
)	O
2	O
equivalent	O
deﬁnition	O
is	O
(	O
3.17	O
)	O
(	O
cid:22	O
)	O
of	O
the	O
mle	O
o	O
(	O
cid:22	O
)	O
,	O
an	O
formula	B
(	O
3.17	O
)	O
does	O
in	O
fact	O
transform	B
correctly	O
under	O
parameter	O
changes	O
,	O
avoiding	O
the	O
venn–fisher	O
criticism.it	O
is	O
known	O
that	O
o	O
approximate	O
standard	B
deviation	I
	O
in	O
family	O
(	O
3.11	O
)	O
has	O
2	O
gjeff	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
1=	O
(	O
cid:27	O
)	O
(	O
cid:22	O
)	O
:	O
(	O
cid:27	O
)	O
	O
d	O
c.1	O
(	O
cid:0	O
)	O
	O
2/	O
;	O
(	O
3.18	O
)	O
(	O
3.19	O
)	O
yielding	O
jeffreys	O
’	O
prior	B
(	O
3.13	O
)	O
from	O
(	O
3.18	O
)	O
,	O
the	O
constant	O
factor	B
c	O
having	O
no	O
effect	O
on	O
bayes	O
’	O
rule	B
(	O
3.5	O
)	O
–	O
(	O
3.6	O
)	O
.	O
the	O
red	O
triangles	O
in	O
figure	O
3.2	O
indicate	O
the	O
“	O
95	O
%	O
credible	O
interval	B
”	O
[	O
0.093	O
,	O
z	O
0:750	O
0.750	O
]	O
for	O
	O
,	O
based	O
on	O
jeffreys	O
’	O
prior	B
.	O
that	O
is	O
,	O
the	O
posterior	B
probability	I
0:093	O
	O
	O
	O
0:750	O
equals	O
0.95	O
,	O
gjeff	O
(	O
cid:16	O
)	O
j	O
o	O
	O
	O
d	O
d	O
0:95	O
;	O
(	O
3.20	O
)	O
0:093	O
with	O
probability	O
0.025	O
for	O
	O
<	O
0:093	O
or	O
	O
>	O
0:750.	O
it	O
is	O
not	O
an	O
accident	O
that	O
this	O
nearly	O
equals	O
the	O
standard	O
neyman	O
95	O
%	O
conﬁdence	B
interval	I
based	O
on	O
o	O
	O
/	O
(	O
3.11	O
)	O
.	O
jeffreys	O
’	O
prior	B
tends	O
to	O
induce	O
this	O
nice	O
connection	O
between	O
f	O
.	O
the	O
bayesian	O
and	O
frequentist	O
worlds	O
,	O
at	O
least	O
in	O
one-parameter	B
families	O
.	O
multiparameter	O
probability	O
families	O
,	O
chapter	O
4	O
,	O
make	O
everything	O
more	O
30	O
bayesian	O
inference	B
difﬁcult	O
.	O
suppose	O
,	O
for	O
instance	O
,	O
the	O
statistician	O
observes	O
10	O
independent	O
versions	O
of	O
the	O
normal	B
model	O
(	O
3.2	O
)	O
,	O
with	O
possibly	O
different	O
values	O
of	O
(	O
cid:22	O
)	O
,	O
ind	O
(	O
cid:24	O
)	O
xi	O
n	O
.	O
(	O
cid:22	O
)	O
i	O
;	O
1/	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
10	O
;	O
(	O
3.21	O
)	O
in	O
standard	O
notation	O
.	O
jeffreys	O
’	O
prior	B
is	O
ﬂat	O
for	O
any	O
one	O
of	O
the	O
10	O
problems	O
,	O
which	O
is	O
reasonable	O
for	O
dealing	O
with	O
them	O
separately	O
,	O
but	O
the	O
joint	O
jeffreys	O
’	O
prior	B
g.	O
(	O
cid:22	O
)	O
1	O
;	O
(	O
cid:22	O
)	O
2	O
;	O
:	O
:	O
:	O
;	O
(	O
cid:22	O
)	O
10/	O
d	O
constant	O
;	O
(	O
3.22	O
)	O
also	O
ﬂat	O
,	O
can	O
produce	O
disastrous	O
overall	O
results	O
,	O
as	O
discussed	O
in	O
chapter	O
13.	O
computer-age	O
applications	O
are	O
often	O
more	O
like	O
(	O
3.21	O
)	O
than	O
(	O
3.11	O
)	O
,	O
except	O
with	O
hundreds	O
or	O
thousands	O
of	O
cases	O
rather	O
than	O
10	O
to	O
consider	O
simultane-	O
ously	O
.	O
uninformative	O
priors	B
of	O
many	O
sorts	O
,	O
including	O
jeffreys	O
’	O
,	O
are	O
highly	O
popular	O
in	O
current	O
applications	O
,	O
as	O
we	O
will	O
discuss	O
.	O
this	O
leads	O
to	O
an	O
inter-	O
play	O
between	O
bayesian	O
and	O
frequentist	O
methodology	O
,	O
the	O
latter	O
intended	O
to	O
control	B
possible	O
biases	O
in	O
the	O
former	O
,	O
exemplifying	O
our	O
general	O
theme	O
of	O
computer-age	O
statistical	O
inference	B
.	O
3.3	O
flaws	O
in	O
frequentist	O
inference	B
bayesian	O
statistics	B
provides	O
an	O
internally	O
consistent	O
(	O
“	O
coherent	O
”	O
)	O
program	O
of	O
inference	B
.	O
the	O
same	O
can	O
not	O
be	O
said	O
of	O
frequentism	O
.	O
the	O
apocryphal	O
story	O
of	O
the	O
meter	O
reader	O
makes	O
the	O
point	O
:	O
an	O
engineer	O
measures	O
the	O
voltages	O
on	O
a	O
batch	O
of	O
12	O
tubes	O
,	O
using	O
a	O
voltmeter	O
that	O
is	O
normally	O
calibrated	O
,	O
x	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
;	O
1/	O
;	O
(	O
3.23	O
)	O
x	O
being	O
any	O
one	O
measurement	O
and	O
(	O
cid:22	O
)	O
the	O
true	O
batch	O
voltage	O
.	O
the	O
measure-	O
ments	O
range	O
from	O
82	O
to	O
99	O
,	O
with	O
an	O
average	O
of	O
nx	O
d	O
92	O
,	O
which	O
he	O
reports	O
back	O
as	O
an	O
unbiased	O
estimate	O
of	O
(	O
cid:22	O
)	O
.	O
the	O
next	O
day	O
he	O
discovers	O
a	O
glitch	O
in	O
his	O
voltmeter	O
such	O
that	O
any	O
volt-	O
age	O
exceeding	O
100	O
would	O
have	O
been	O
reported	O
as	O
x	O
d	O
100.	O
his	O
frequentist	O
statistician	O
tells	O
him	O
that	O
nx	O
d	O
92	O
is	O
no	O
longer	O
unbiased	O
for	O
the	O
true	O
expecta-	O
tion	O
(	O
cid:22	O
)	O
since	O
(	O
3.23	O
)	O
no	O
longer	O
completely	O
describes	O
the	O
probability	O
family	O
.	O
(	O
the	O
statistician	O
says	O
that	O
92	O
is	O
a	O
little	O
too	O
small	O
.	O
)	O
the	O
fact	O
that	O
the	O
glitch	O
didn	O
’	O
t	B
affect	O
any	O
of	O
the	O
actual	O
measurements	O
doesn	O
’	O
t	B
let	O
him	O
off	O
the	O
hook	O
;	O
nx	O
from	O
the	O
actual	O
nx	O
would	O
not	O
be	O
unbiased	O
for	O
(	O
cid:22	O
)	O
in	O
future	O
realizations	O
of	O
probability	O
model	B
.	O
a	O
bayesian	O
statistician	O
comes	O
to	O
the	O
meter	O
reader	O
’	O
s	O
rescue	O
.	O
for	O
any	O
prior	B
density	O
g.	O
(	O
cid:22	O
)	O
/	O
,	O
the	O
posterior	O
density	O
g.	O
(	O
cid:22	O
)	O
jx/	O
d	O
g.	O
(	O
cid:22	O
)	O
/f	O
(	O
cid:22	O
)	O
.x/=f	O
.x/	O
,	O
where	O
x	O
is	O
the	O
vector	B
of	O
12	O
measurements	O
,	O
depends	O
only	O
on	O
the	O
data	B
x	O
actually	O
3	O
3.3	O
flaws	O
in	O
frequentist	O
inference	B
31	O
observed	O
,	O
and	O
not	O
on	O
other	O
potential	O
data	B
sets	O
x	O
that	O
might	O
have	O
been	O
seen	O
.	O
the	O
ﬂat	O
jeffreys	O
’	O
prior	B
g.	O
(	O
cid:22	O
)	O
/	O
d	O
constant	O
yields	O
posterior	O
expectation	O
nx	O
d	O
92	O
for	O
(	O
cid:22	O
)	O
,	O
irrespective	O
of	O
whether	O
or	O
not	O
the	O
glitch	O
would	O
have	O
affected	O
readings	O
above	O
100.	O
figure	O
3.3	O
z-values	O
against	O
null	O
hypothesis	O
(	O
cid:22	O
)	O
d	O
0	O
for	O
months	O
1	O
through	O
30.	O
a	O
less	O
contrived	O
version	O
of	O
the	O
same	O
phenomenon	O
is	O
illustrated	O
in	O
fig-	O
ure	O
3.3.	O
an	O
ongoing	O
experiment	O
is	O
being	O
run	O
.	O
each	O
month	O
i	O
an	O
independent	O
normal	B
variate	O
is	O
observed	O
,	O
(	O
3.24	O
)	O
with	O
the	O
intention	O
of	O
testing	B
the	O
null	O
hypothesis	O
h0	O
w	O
(	O
cid:22	O
)	O
d	O
0	O
versus	O
the	O
alternative	O
(	O
cid:22	O
)	O
>	O
0.	O
the	O
plotted	O
points	O
are	O
test	O
statistics	B
n	O
.	O
(	O
cid:22	O
)	O
;	O
1/	O
;	O
(	O
3.25	O
)	O
xi	O
(	O
cid:24	O
)	O
zi	O
d	O
ix	O
(	O
cid:16	O
)	O
p	O
jd1	O
xj	O
zi	O
(	O
cid:24	O
)	O
.p	O
i	O
;	O
	O
a	O
“	O
z-value	O
”	O
based	O
on	O
all	O
the	O
data	B
up	O
to	O
month	O
i	O
,	O
(	O
3.26	O
)	O
at	O
month	O
30	O
,	O
the	O
scheduled	O
end	O
of	O
the	O
experiment	O
,	O
z30	O
d	O
1:66	O
,	O
just	O
ex-	O
ceeding	O
1.645	O
,	O
the	O
upper	O
95	O
%	O
point	O
for	O
a	O
n	O
.0	O
;	O
1/	O
distribution	B
.	O
victory	O
!	O
the	O
investigators	O
get	O
to	O
claim	O
“	O
signiﬁcant	O
”	O
rejection	O
of	O
h0	O
at	O
level	O
0.05.	O
i	O
(	O
cid:22	O
)	O
;	O
1	O
n	O
:	O
llllllllllllllllllllllllllllll051015202530−1.5−1.0−0.50.00.51.01.52.0month	O
iz	O
value1.645	O
32	O
bayesian	O
inference	B
unfortunately	O
,	O
it	O
turns	O
out	O
that	O
the	O
investigators	O
broke	O
protocol	O
and	O
peek-	O
ed	O
at	O
the	O
data	B
at	O
month	O
20	O
,	O
in	O
the	O
hope	O
of	O
being	O
able	O
to	O
stop	O
an	O
expensive	O
experiment	O
early	O
.	O
this	O
proved	O
a	O
vain	O
hope	O
,	O
z20	O
d	O
0:79	O
not	O
being	O
anywhere	O
near	O
signiﬁcance	O
,	O
so	O
they	O
continued	O
on	O
to	O
month	O
30	O
as	O
originally	O
planned	O
.	O
this	O
means	O
they	O
effectively	O
used	O
the	O
stopping	O
rule	B
“	O
stop	O
and	O
declare	O
signif-	O
icance	O
if	O
either	O
z20	O
or	O
z30	O
exceeds	O
1.645.	O
”	O
some	O
computation	O
shows	O
that	O
this	O
rule	B
had	O
probability	O
0.074	O
,	O
not	O
0.05	O
,	O
of	O
rejecting	O
h0	O
if	O
it	O
were	O
true	O
.	O
victory	O
has	O
turned	O
into	O
defeat	O
according	O
to	O
the	O
honored	O
frequentist	O
0.05	O
criterion	O
.	O
once	O
again	O
,	O
the	O
bayesian	O
statistician	O
is	O
more	O
lenient	O
.	O
the	O
likelihood	B
function	O
for	O
the	O
full	B
data	O
set	B
x	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
x30/	O
,	O
2	O
.xi	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
/2	O
(	O
cid:0	O
)	O
1	O
lx	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
30y	O
(	O
3.27	O
)	O
e	O
id1	O
;	O
is	O
the	O
same	O
irrespective	O
of	O
whether	O
or	O
not	O
the	O
experiment	O
might	O
have	O
stopped	O
early	O
.	O
the	O
stopping	O
rule	B
doesn	O
’	O
t	B
affect	O
the	O
posterior	B
distribution	I
g.	O
(	O
cid:22	O
)	O
jx/	O
,	O
which	O
depends	O
on	O
x	O
only	O
through	O
the	O
likelihood	B
(	O
3.7	O
)	O
.	O
figure	O
3.4	O
unbiased	O
effect-size	O
estimates	O
for	O
6033	O
genes	O
,	O
prostate	B
cancer	O
study	O
.	O
the	O
estimate	B
for	O
gene	O
610	O
is	O
x610	O
d	O
5:29.	O
what	O
is	O
its	O
effect	O
size	O
?	O
the	O
lenient	O
nature	O
of	O
bayesian	O
inference	B
can	O
look	O
less	O
benign	O
in	O
multi-	O
effect−size	O
estimatesfrequency−4−20240100200300400gene	O
6105.29	O
3.4	O
a	O
bayesian/frequentist	O
comparison	O
list	O
33	O
parameter	O
settings	O
.	O
figure	O
3.4	O
concerns	O
a	O
prostate	B
cancer	O
study	O
comparing	O
52	O
patients	O
with	O
50	O
healthy	O
controls	O
.	O
each	O
man	O
had	O
his	O
genetic	O
activity	O
measured	O
for	O
a	O
panel	O
of	O
n	O
d	O
6033	O
genes	O
.	O
a	O
statistic	B
x	O
was	O
computed	O
for	O
each	O
gene,5	O
comparing	O
the	O
patients	O
with	O
controls	O
,	O
say	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
;	O
(	O
3.28	O
)	O
4	O
xi	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
i	O
;	O
1/	O
where	O
(	O
cid:22	O
)	O
i	O
represents	O
the	O
true	O
effect	O
size	O
for	O
gene	O
i.	O
most	O
of	O
the	O
genes	O
,	O
prob-	O
ably	O
not	O
being	O
involved	O
in	O
prostate	B
cancer	O
,	O
would	O
be	O
expected	O
to	O
have	O
effect	O
sizes	O
near	O
0	O
,	O
but	O
the	O
investigators	O
hoped	O
to	O
spot	O
a	O
few	O
large	O
(	O
cid:22	O
)	O
i	O
values	O
,	O
either	O
positive	O
or	O
negative	O
.	O
the	O
histogram	O
of	O
the	O
6033	O
xi	O
values	O
does	O
in	O
fact	O
reveal	O
some	O
large	O
val-	O
ues	O
,	O
x610	O
d	O
5:29	O
being	O
the	O
winner	O
.	O
question	O
:	O
what	O
estimate	B
should	O
we	O
give	O
for	O
(	O
cid:22	O
)	O
610	O
?	O
even	O
though	O
x610	O
was	O
individually	O
unbiased	O
for	O
(	O
cid:22	O
)	O
610	O
,	O
a	O
fre-	O
quentist	O
would	O
(	O
correctly	O
)	O
worry	O
that	O
focusing	O
attention	O
on	O
the	O
largest	O
of	O
6033	O
values	O
would	O
produce	O
an	O
upward	O
bias	O
,	O
and	O
that	O
our	O
estimate	B
should	O
downwardly	O
correct	O
5.29	O
.	O
“	O
selection	O
bias	O
,	O
”	O
“	O
regression	B
to	O
the	O
mean	O
,	O
”	O
and	O
“	O
the	O
winner	O
’	O
s	O
curse	O
”	O
are	O
three	O
names	O
for	O
this	O
phenomenon	O
.	O
bayesian	O
inference	B
,	O
surprisingly	O
,	O
is	O
immune	O
to	O
selection	O
bias	O
.	O
	O
irrespec-	O
5	O
tive	O
of	O
whether	O
gene	O
610	O
was	O
prespeciﬁed	O
for	O
particular	O
attention	O
or	O
only	O
came	O
to	O
attention	O
as	O
the	O
“	O
winner	O
,	O
”	O
the	O
bayes	O
’	O
estimate	B
for	O
(	O
cid:22	O
)	O
610	O
given	O
all	O
the	O
data	B
stays	O
the	O
same	O
.	O
this	O
isn	O
’	O
t	B
obvious	O
,	O
but	O
follows	O
from	O
the	O
fact	O
that	O
any	O
data-based	O
selection	O
process	O
does	O
not	O
affect	O
the	O
likelihood	B
function	O
in	O
(	O
3.7	O
)	O
.	O
what	O
does	O
affect	O
bayesian	O
inference	B
is	O
the	O
prior	B
g.	O
(	O
cid:22	O
)	O
/	O
for	O
the	O
full	B
vector	O
(	O
cid:22	O
)	O
of	O
6033	O
effect	O
sizes	O
.	O
the	O
ﬂat	O
prior	B
,	O
g.	O
(	O
cid:22	O
)	O
/	O
constant	O
,	O
results	O
in	O
the	O
danger-	O
ous	O
overestimate	O
o	O
(	O
cid:22	O
)	O
610	O
d	O
x610	O
d	O
5:29.	O
a	O
more	O
appropriate	O
uninformative	O
prior	B
appears	O
as	O
part	O
of	O
the	O
empirical	B
bayes	O
calculations	O
of	O
chapter	O
15	O
(	O
and	O
gives	O
o	O
(	O
cid:22	O
)	O
610	O
d	O
4:11	O
)	O
.	O
the	O
operative	O
point	O
here	O
is	O
that	O
there	O
is	O
a	O
price	O
to	O
be	O
paid	O
for	O
the	O
desirable	O
properties	O
of	O
bayesian	O
inference	B
.	O
attention	O
shifts	O
from	O
choosing	O
a	O
good	O
frequentist	O
procedure	O
to	O
choosing	O
an	O
appropriate	O
prior	B
distribution	I
.	O
this	O
can	O
be	O
a	O
formidable	O
task	O
in	O
high-dimensional	O
prob-	O
lems	O
,	O
the	O
very	O
kinds	O
featured	O
in	O
computer-age	O
inference	B
.	O
3.4	O
a	O
bayesian/frequentist	O
comparison	O
list	O
bayesians	O
and	O
frequentists	O
start	O
out	O
on	O
the	O
same	O
playing	O
ﬁeld	O
,	O
a	O
family	O
of	O
probability	O
distributions	O
f	O
(	O
cid:22	O
)	O
.x/	O
(	O
3.1	O
)	O
,	O
but	O
play	O
the	O
game	O
in	O
orthogonal	O
5	O
the	O
statistic	B
was	O
the	O
two-sample	B
t-statistic	O
(	O
2.17	O
)	O
transformed	O
to	O
normality	O
(	O
3.28	O
)	O
;	O
see	O
the	O
endnotes	O
.	O
34	O
bayesian	O
inference	B
directions	O
,	O
as	O
indicated	O
schematically	O
in	O
figure	O
3.5	O
:	O
bayesian	O
inference	B
proceeds	O
vertically	O
,	O
with	O
x	O
ﬁxed	O
,	O
according	O
to	O
the	O
posterior	B
distribution	I
g.	O
(	O
cid:22	O
)	O
jx/	O
,	O
while	O
frequentists	O
reason	O
horizontally	O
,	O
with	O
(	O
cid:22	O
)	O
ﬁxed	O
and	O
x	O
varying	O
.	O
advantages	O
and	O
disadvantages	O
accrue	O
to	O
both	O
strategies	O
,	O
some	O
of	O
which	O
are	O
compared	O
next	O
.	O
figure	O
3.5	O
bayesian	O
inference	B
proceeds	O
vertically	O
,	O
given	O
x	O
;	O
frequentist	O
inference	B
proceeds	O
horizontally	O
,	O
given	O
(	O
cid:22	O
)	O
.	O
(	O
cid:15	O
)	O
bayesian	O
inference	B
requires	O
a	O
prior	B
distribution	I
g.	O
(	O
cid:22	O
)	O
/	O
.	O
when	O
past	O
experi-	O
ence	O
provides	O
g.	O
(	O
cid:22	O
)	O
/	O
,	O
as	O
in	O
the	O
twins	O
example	O
,	O
there	O
is	O
every	O
good	O
reason	O
to	O
employ	O
bayes	O
’	O
theorem	B
.	O
if	O
not	O
,	O
techniques	O
such	O
as	O
those	O
of	O
jeffreys	O
still	O
permit	O
the	O
use	O
of	O
bayes	O
’	O
rule	B
,	O
but	O
the	O
results	O
lack	O
the	O
full	B
logical	O
force	O
of	O
the	O
theorem	B
;	O
the	O
bayesian	O
’	O
s	O
right	O
to	O
ignore	O
selection	O
bias	O
,	O
for	O
instance	O
,	O
must	O
then	O
be	O
treated	O
with	O
caution	O
.	O
(	O
cid:15	O
)	O
frequentism	O
replaces	O
the	O
choice	O
of	O
a	O
prior	B
with	O
the	O
choice	O
of	O
a	O
method	B
,	O
or	O
algorithm	B
,	O
t	B
.x/	O
,	O
designed	O
to	O
answer	O
the	O
speciﬁc	O
question	O
at	O
hand	O
.	O
this	O
adds	O
an	O
arbitrary	O
element	O
to	O
the	O
inferential	O
process	O
,	O
and	O
can	O
lead	O
to	O
meter-	O
reader	O
kinds	O
of	O
contradictions	O
.	O
optimal	O
choice	O
of	O
t	B
.x/	O
reduces	O
arbitrary	O
behavior	O
,	O
but	O
computer-age	O
applications	O
typically	O
move	O
outside	O
the	O
safe	O
waters	O
of	O
classical	O
optimality	O
theory	B
,	O
lending	O
an	O
ad-hoc	O
character	O
to	O
fre-	O
quentist	O
analyses	O
.	O
(	O
cid:15	O
)	O
modern	O
data-analysis	O
problems	O
are	O
often	O
approached	O
via	O
a	O
favored	O
meth-	O
3.4	O
a	O
bayesian/frequentist	O
comparison	O
list	O
35	O
odology	O
,	O
such	O
as	O
logistic	B
regression	I
or	O
regression	B
trees	O
in	O
the	O
examples	O
of	O
chapter	O
8.	O
this	O
plays	O
into	O
the	O
methodological	O
orientation	O
of	O
frequentism	O
,	O
which	O
is	O
more	O
ﬂexible	O
than	O
bayes	O
’	O
rule	B
in	O
dealing	O
with	O
speciﬁc	O
algorithms	O
(	O
though	O
one	O
always	O
hopes	O
for	O
a	O
reasonable	O
bayesian	O
justiﬁcation	O
for	O
the	O
method	B
at	O
hand	O
)	O
.	O
(	O
cid:15	O
)	O
having	O
chosen	O
g.	O
(	O
cid:22	O
)	O
/	O
,	O
only	O
a	O
single	O
probability	O
distribution	B
g.	O
(	O
cid:22	O
)	O
jx/	O
is	O
in	O
play	O
for	O
bayesians	O
.	O
frequentists	O
,	O
by	O
contrast	O
,	O
must	O
struggle	O
to	O
balance	O
the	O
behavior	O
of	O
t	B
.x/	O
over	O
a	O
family	O
of	O
possible	O
distributions	O
,	O
since	O
(	O
cid:22	O
)	O
in	O
figure	O
3.5	O
is	O
unknown	O
.	O
the	O
growing	O
popularity	O
of	O
bayesian	O
applications	O
(	O
usually	O
begun	O
with	O
uninformative	O
priors	B
)	O
reﬂects	O
their	O
simplicity	O
of	O
ap-	O
plication	O
and	O
interpretation	O
.	O
(	O
cid:15	O
)	O
the	O
simplicity	O
argument	B
cuts	O
both	O
ways	O
.	O
the	O
bayesian	O
essentially	O
bets	O
it	O
all	O
on	O
the	O
choice	O
of	O
his	O
or	O
her	O
prior	B
being	O
correct	O
,	O
or	O
at	O
least	O
not	O
harmful	O
.	O
frequentism	O
takes	O
a	O
more	O
defensive	O
posture	O
,	O
hoping	O
to	O
do	O
well	O
,	O
or	O
at	O
least	O
not	O
poorly	O
,	O
whatever	O
(	O
cid:22	O
)	O
might	O
be	O
.	O
(	O
cid:15	O
)	O
a	O
bayesian	O
analysis	B
answers	O
all	O
possible	O
questions	O
at	O
once	O
,	O
for	O
example	O
,	O
estimating	O
efgfrg	O
or	O
prfgfr	O
<	O
40g	O
or	O
anything	O
else	O
relating	O
to	O
figure	O
2.1.	O
frequentism	O
focuses	O
on	O
the	O
problem	O
at	O
hand	O
,	O
requiring	O
different	O
estima-	O
tors	O
for	O
different	O
questions	O
.	O
this	O
is	O
more	O
work	O
,	O
but	O
allows	O
for	O
more	O
intense	O
inspection	O
of	O
particular	O
problems	O
.	O
in	O
situation	O
(	O
2.9	O
)	O
for	O
example	O
,	O
estima-	O
tors	O
of	O
the	O
form	B
x	O
.xi	O
(	O
cid:0	O
)	O
nx/2=.n	O
(	O
cid:0	O
)	O
c/	O
(	O
3.29	O
)	O
might	O
be	O
investigated	O
for	O
different	O
choices	O
of	O
the	O
constant	O
c	O
,	O
hoping	O
to	O
reduce	O
expected	O
mean-squared	O
error	O
.	O
(	O
cid:15	O
)	O
the	O
simplicity	O
of	O
the	O
bayesian	O
approach	O
is	O
especially	O
appealing	O
in	O
dy-	O
namic	O
contexts	O
,	O
where	O
data	B
arrives	O
sequentially	O
and	O
updating	O
one	O
’	O
s	O
beliefs	O
is	O
a	O
natural	O
practice	O
.	O
bayes	O
’	O
rule	B
was	O
used	O
to	O
devastating	O
effect	O
before	O
the	O
2012	O
us	O
presidential	O
election	O
,	O
updating	O
sequential	O
polling	O
results	O
to	O
cor-	O
rectly	O
predict	O
the	O
outcome	O
in	O
all	O
50	O
states	O
.	O
bayes	O
’	O
theorem	B
is	O
an	O
excellent	O
tool	O
in	O
general	O
for	O
combining	O
statistical	O
evidence	O
from	O
disparate	O
sources	O
,	O
the	O
closest	O
frequentist	O
analog	O
being	O
maximum	B
likelihood	I
estimation	O
.	O
(	O
cid:15	O
)	O
in	O
the	O
absence	O
of	O
genuine	O
prior	B
information	O
,	O
a	O
whiff	O
of	O
subjectivity6	O
hangs	O
over	O
bayesian	O
results	O
,	O
even	O
those	O
based	O
on	O
uninformative	O
priors	B
.	O
classical	O
frequentism	O
claimed	O
for	O
itself	O
the	O
high	O
ground	O
of	O
scientiﬁc	O
objectivity	O
,	O
especially	O
in	O
contentious	O
areas	O
such	O
as	O
drug	O
testing	B
and	O
approval	O
,	O
where	O
skeptics	O
as	O
well	O
as	O
friends	O
hang	O
on	O
the	O
statistical	O
details	O
.	O
figure	O
3.5	O
is	O
soothingly	O
misleading	O
in	O
its	O
schematics	O
:	O
(	O
cid:22	O
)	O
and	O
x	O
will	O
6	O
here	O
we	O
are	O
not	O
discussing	O
the	O
important	O
subjectivist	O
school	O
of	O
bayesian	O
inference	B
,	O
of	O
savage	O
,	O
de	O
finetti	O
,	O
and	O
others	O
,	O
covered	O
in	O
chapter	O
13	O
.	O
36	O
bayesian	O
inference	B
typically	O
be	O
high-dimensional	O
in	O
the	O
chapters	O
that	O
follow	O
,	O
sometimes	O
very	O
high-dimensional	O
,	O
straining	O
to	O
the	O
breaking	O
point	O
both	O
the	O
frequentist	O
and	O
the	O
bayesian	O
paradigms	O
.	O
computer-age	O
statistical	O
inference	B
at	O
its	O
most	O
successful	O
combines	O
elements	O
of	O
the	O
two	O
philosophies	O
,	O
as	O
for	O
instance	O
in	O
the	O
empirical	B
bayes	O
methods	O
of	O
chapter	O
6	O
,	O
and	O
the	O
lasso	B
in	O
chapter	O
16.	O
there	O
are	O
two	O
potent	O
arrows	O
in	O
the	O
statistician	O
’	O
s	O
philosophical	O
quiver	O
,	O
and	O
faced	O
,	O
say	O
,	O
with	O
1000	O
parameters	O
and	O
1,000,000	O
data	B
points	O
,	O
there	O
’	O
s	O
no	O
need	O
to	O
go	O
hunting	O
armed	O
with	O
just	O
one	O
of	O
them	O
.	O
3.5	O
notes	O
and	O
details	O
thomas	O
bayes	O
,	O
if	O
transferred	O
to	O
modern	O
times	O
,	O
might	O
well	O
be	O
employed	O
as	O
a	O
successful	O
professor	O
of	O
mathematics	O
.	O
actually	O
,	O
he	O
was	O
a	O
mid-eighteenth-	O
century	O
nonconformist	O
english	O
minister	O
with	O
substantial	O
mathematical	O
in-	O
terests	O
.	O
richard	O
price	O
,	O
a	O
leading	O
ﬁgure	O
of	O
letters	O
,	O
science	O
,	O
and	O
politics	O
,	O
had	O
bayes	O
’	O
theorem	B
published	O
in	O
the	O
1763	O
transactions	O
of	O
the	O
royal	O
society	O
(	O
two	O
years	O
after	O
bayes	O
’	O
death	O
)	O
,	O
his	O
interest	O
being	O
partly	O
theological	O
,	O
with	O
the	O
rule	B
somehow	O
proving	O
the	O
existence	O
of	O
god	O
.	O
bellhouse	O
’	O
s	O
(	O
2004	O
)	O
biog-	O
raphy	O
includes	O
some	O
of	O
bayes	O
’	O
other	O
mathematical	O
accomplishments	O
.	O
harold	O
jeffreys	O
was	O
another	O
part-time	O
statistician	O
,	O
working	O
from	O
his	O
day	O
job	O
as	O
the	O
world	O
’	O
s	O
premier	O
geophysicist	O
of	O
the	O
inter-war	O
period	O
(	O
and	O
ﬁerce	O
opponent	O
of	O
the	O
theory	B
of	O
continental	O
drift	O
)	O
.	O
what	O
we	O
called	O
uninformative	O
priors	B
are	O
also	O
called	O
noninformative	O
or	O
objective	O
.	O
jeffreys	O
’	O
brand	O
of	O
bayes-	O
ianism	O
had	O
a	O
dubious	O
reputation	O
among	O
bayesians	O
in	O
the	O
period	O
1950–	O
1990	O
,	O
with	O
preference	O
going	O
to	O
subjective	O
analysis	B
of	O
the	O
type	O
advocated	O
by	O
savage	O
and	O
de	O
finetti	O
.	O
the	O
introduction	O
of	O
markov	O
chain	O
monte	O
carlo	O
methodology	O
was	O
the	O
kind	O
of	O
technological	O
innovation	O
that	O
changes	O
philoso-	O
phies	O
.	O
mcmc	O
(	O
chapter	O
13	O
)	O
,	O
being	O
very	O
well	O
suited	O
to	O
jeffreys-style	O
anal-	O
ysis	O
of	O
big	O
data	B
problems	O
,	O
moved	O
bayesian	O
statistics	B
out	O
of	O
the	O
textbooks	O
and	O
into	O
the	O
world	O
of	O
computer-age	O
applications	O
.	O
berger	O
(	O
2006	O
)	O
makes	O
a	O
spirited	O
case	O
for	O
the	O
objective	O
bayes	O
approach	O
.	O
1	O
[	O
p.	O
26	O
]	O
correlation	B
coefﬁcient	I
density	O
.	O
formula	B
(	O
3.11	O
)	O
for	O
the	O
correlation	B
coefﬁcient	I
density	O
was	O
r.	O
a.	O
fisher	O
’	O
s	O
debut	O
contribution	O
to	O
the	O
statistics	B
literature	O
.	O
chapter	O
32	O
of	O
johnson	O
and	O
kotz	O
(	O
1970b	O
)	O
gives	O
several	O
equivalent	O
forms	O
.	O
the	O
constant	O
c	O
in	O
(	O
3.19	O
)	O
is	O
often	O
taken	O
to	O
be	O
.n	O
(	O
cid:0	O
)	O
3/	O
(	O
cid:0	O
)	O
1=2	O
,	O
with	O
n	O
the	O
sample	B
size	I
.	O
ters	O
from	O
(	O
cid:22	O
)	O
to	O
q	O
(	O
cid:22	O
)	O
in	O
a	O
smoothly	O
differentiable	O
way	O
.	O
the	O
new	O
family	O
q	O
2	O
[	O
p.	O
29	O
]	O
jeffreys	O
’	O
prior	B
and	O
transformations	O
.	O
suppose	O
we	O
change	O
parame-	O
fq	O
(	O
cid:22	O
)	O
.x/	O
3.5	O
notes	O
and	O
details	O
37	O
satisﬁes	O
@	O
q	O
(	O
cid:22	O
)	O
log	O
q	O
@	O
@	O
q	O
(	O
cid:22	O
)	O
fq	O
(	O
cid:22	O
)	O
.x/	O
d	O
@	O
(	O
cid:22	O
)	O
@	O
q	O
(	O
cid:22	O
)	O
i	O
(	O
cid:22	O
)	O
(	O
3.16	O
)	O
and	O
qgjeff	O
.	O
q	O
(	O
cid:22	O
)	O
/	O
d	O
ˇˇˇ	O
@	O
(	O
cid:22	O
)	O
then	O
q	O
@	O
q	O
(	O
cid:22	O
)	O
says	O
that	O
gjeff	O
.	O
(	O
cid:22	O
)	O
/	O
transforms	O
correctly	O
to	O
qgjeff	O
.	O
q	O
(	O
cid:22	O
)	O
/	O
.	O
iq	O
(	O
cid:22	O
)	O
d	O
(	O
cid:16	O
)	O
@	O
(	O
cid:22	O
)	O
2	O
@	O
@	O
(	O
cid:22	O
)	O
log	O
f	O
(	O
cid:22	O
)	O
.x/	O
:	O
(	O
3.30	O
)	O
ˇˇˇ	O
gjeff.	O
(	O
cid:22	O
)	O
/	O
.	O
but	O
this	O
just	O
3	O
[	O
p.	O
30	O
]	O
the	O
meter-reader	O
fable	O
is	O
taken	O
from	O
edwards	O
’	O
(	O
1992	O
)	O
book	O
likeli-	O
hood	O
,	O
where	O
he	O
credits	O
john	O
pratt	O
.	O
it	O
nicely	O
makes	O
the	O
point	O
that	O
frequentist	O
inferences	O
,	O
which	O
are	O
calibrated	O
in	O
terms	O
of	O
possible	O
observed	O
data	B
sets	O
x	O
,	O
may	O
be	O
inappropriate	O
for	O
the	O
actual	O
observation	O
x.	O
this	O
is	O
the	O
difference	O
between	O
working	O
in	O
the	O
horizontal	O
and	O
vertical	O
directions	O
of	O
figure	O
3.5	O
.	O
4	O
[	O
p.	O
33	O
]	O
two-sample	B
t-statistic	O
.	O
applied	O
to	O
gene	O
i	O
’	O
s	O
data	B
in	O
the	O
prostate	B
study	O
,	O
the	O
two-sample	B
t-statistic	O
ti	O
(	O
2.17	O
)	O
has	O
theoretical	O
null	O
hypothesis	O
distribution	O
t100	O
,	O
a	O
student	O
’	O
s	O
t	B
distribution	O
with	O
100	O
degrees	O
of	O
freedom	O
;	O
xi	O
(	O
cid:0	O
)	O
1.f100.ti	O
//	O
,	O
where	O
ˆ	O
and	O
f100	O
are	O
the	O
cumulative	O
distribu-	O
in	O
(	O
3.28	O
)	O
is	O
ˆ	O
tion	O
functions	O
of	O
standard	O
normal	O
and	O
t100	O
variables	O
.	O
section	O
7.4	O
of	O
efron	O
(	O
2010	O
)	O
motivates	O
approximation	O
(	O
3.28	O
)	O
.	O
5	O
[	O
p.	O
33	O
]	O
selection	O
bias	O
.	O
senn	O
(	O
2008	O
)	O
discusses	O
the	O
immunity	O
of	O
bayesian	O
inferences	O
to	O
selection	O
bias	O
and	O
other	O
“	O
paradoxes	O
,	O
”	O
crediting	O
phil	O
dawid	O
for	O
the	O
original	O
idea	O
.	O
the	O
article	O
catches	O
the	O
possible	O
uneasiness	O
of	O
following	O
bayes	O
’	O
theorem	B
too	O
literally	O
in	O
applications	O
.	O
the	O
22	O
students	O
in	O
table	O
3.1	O
were	O
randomly	O
selected	O
from	O
a	O
larger	O
data	B
set	O
of	O
88	O
in	O
mardia	O
et	O
al	O
.	O
(	O
1979	O
)	O
(	O
which	O
gave	O
o	O
	O
d	O
0:553	O
)	O
.	O
welch	O
and	O
peers	O
(	O
1963	O
)	O
initiated	O
the	O
study	O
of	O
priors	B
whose	O
credible	O
intervals	B
,	O
such	O
as	O
œ0:093	O
;	O
0:750	O
in	O
figure	O
3.2	O
,	O
match	O
frequentist	O
conﬁdence	B
intervals	I
.	O
in	O
one-parameter	B
problems	O
,	O
jeffreys	O
’	O
priors	B
provide	O
good	O
matches	O
,	O
but	O
not	O
ususally	O
in	O
multiparameter	O
situations	O
.	O
in	O
fact	O
,	O
no	O
single	O
multiparameter	O
prior	B
can	O
give	O
good	O
matches	O
for	O
all	O
one-parameter	B
subproblems	O
,	O
a	O
source	O
of	O
tension	O
between	O
bayesian	O
and	O
frequentist	O
methods	O
revisited	O
in	O
chapter	O
11	O
.	O
4	O
fisherian	O
inference	B
and	O
maximum	B
likelihood	I
estimation	O
sir	O
ronald	O
fisher	O
was	O
arguably	O
the	O
most	O
inﬂuential	O
anti-bayesian	O
of	O
all	O
time	O
,	O
but	O
that	O
did	O
not	O
make	O
him	O
a	O
conventional	O
frequentist	O
.	O
his	O
key	O
data-	O
analytic	O
methods—analysis	O
of	O
variance	O
,	O
signiﬁcance	O
testing	B
,	O
and	O
maxi-	O
mum	O
likelihood	B
estimation—were	O
almost	O
always	O
applied	O
frequentistically	O
.	O
their	O
fisherian	O
rationale	O
,	O
however	O
,	O
often	O
drew	O
on	O
ideas	O
neither	O
bayesian	O
nor	O
frequentist	O
in	O
nature	O
,	O
or	O
sometimes	O
the	O
two	O
in	O
combination	O
.	O
fisher	O
’	O
s	O
work	O
held	O
a	O
central	O
place	O
in	O
twentieth-century	O
applied	O
statistics	B
,	O
and	O
some	O
of	O
it	O
,	O
particularly	O
maximum	B
likelihood	I
estimation	O
,	O
has	O
moved	O
forcefully	O
into	O
computer-age	O
practice	O
.	O
this	O
chapter	O
’	O
s	O
brief	O
review	O
of	O
fisherian	O
meth-	O
odology	O
sketches	O
parts	O
of	O
its	O
unique	O
philosophical	O
structure	O
,	O
while	O
concen-	O
trating	O
on	O
those	O
topics	O
of	O
greatest	O
current	O
importance	O
.	O
4.1	O
likelihood	B
and	O
maximum	B
likelihood	I
fisher	O
’	O
s	O
seminal	O
work	O
on	O
estimation	B
focused	O
on	O
the	O
likelihood	B
function	O
,	O
or	O
more	O
exactly	O
its	O
logarithm	O
.	O
for	O
a	O
family	O
of	O
probability	O
densities	O
f	O
(	O
cid:22	O
)	O
.x/	O
(	O
3.1	O
)	O
,	O
the	O
log	O
likelihood	B
function	O
is	O
lx	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
logff	O
(	O
cid:22	O
)	O
.x/g	O
;	O
(	O
4.1	O
)	O
the	O
notation	O
lx	O
.	O
(	O
cid:22	O
)	O
/	O
emphasizing	O
that	O
the	O
parameter	O
vector	B
(	O
cid:22	O
)	O
is	O
varying	O
while	O
the	O
observed	O
data	B
vector	O
x	O
is	O
ﬁxed	O
.	O
the	O
maximum	B
likelihood	I
esti-	O
mate	O
(	O
mle	O
)	O
is	O
the	O
value	O
of	O
(	O
cid:22	O
)	O
in	O
parameter	O
space	B
(	O
cid:127	O
)	O
that	O
maximizes	O
lx	O
.	O
(	O
cid:22	O
)	O
/	O
,	O
mle	O
w	O
o	O
(	O
cid:22	O
)	O
d	O
arg	O
max	O
(	O
cid:22	O
)	O
2	O
(	O
cid:127	O
)	O
flx	O
.	O
(	O
cid:22	O
)	O
/g	O
:	O
(	O
4.2	O
)	O
it	O
can	O
happen	O
that	O
o	O
(	O
cid:22	O
)	O
doesn	O
’	O
t	B
exist	O
or	O
that	O
there	O
are	O
multiple	O
maximizers	O
,	O
but	O
here	O
we	O
will	O
assume	O
the	O
usual	O
case	O
where	O
o	O
(	O
cid:22	O
)	O
exists	O
uniquely	O
.	O
more	O
careful	O
references	O
are	O
provided	O
in	O
the	O
endnotes	O
.	O
deﬁnition	O
(	O
4.2	O
)	O
is	O
extended	O
to	O
provide	O
maximum	B
likelihood	I
estimates	O
38	O
4.1	O
likelihood	B
and	O
maximum	B
likelihood	I
for	O
a	O
function	B
	O
d	O
t	B
.	O
(	O
cid:22	O
)	O
/	O
of	O
(	O
cid:22	O
)	O
according	O
to	O
the	O
simple	O
plug-in	O
rule	B
o	O
	O
d	O
t	B
.	O
o	O
(	O
cid:22	O
)	O
/	O
;	O
39	O
(	O
4.3	O
)	O
most	O
often	O
with	O
	O
being	O
a	O
scalar	O
parameter	O
of	O
particular	O
interest	O
,	O
such	O
as	O
the	O
regression	B
coefﬁcient	O
of	O
an	O
important	O
covariate	O
in	O
a	O
linear	B
model	I
.	O
maximum	B
likelihood	I
estimation	O
came	O
to	O
dominate	O
classical	O
applied	O
es-	O
timation	O
practice	O
.	O
less	O
dominant	O
now	O
,	O
for	O
reasons	O
we	O
will	O
be	O
investigating	O
in	O
subsequent	O
chapters	O
,	O
the	O
mle	O
algorithm	B
still	O
has	O
iconic	O
status	O
,	O
being	O
of-	O
ten	O
the	O
method	B
of	O
ﬁrst	O
choice	O
in	O
any	O
novel	O
situation	O
.	O
there	O
are	O
several	O
good	O
reasons	O
for	O
its	O
ubiquity	O
.	O
1	O
the	O
mle	O
algorithm	B
is	O
automatic	O
:	O
in	O
theory	B
,	O
and	O
almost	O
in	O
practice	O
,	O
a	O
single	O
numerical	O
algorithm	B
produces	O
o	O
(	O
cid:22	O
)	O
without	O
further	O
statistical	O
input	O
.	O
this	O
contrasts	O
with	O
unbiased	O
estimation	O
,	O
for	O
instance	O
,	O
where	O
each	O
new	O
situation	O
requires	O
clever	O
theoretical	O
calculations	O
.	O
2	O
the	O
mle	O
enjoys	O
excellent	O
frequentist	O
properties	O
.	O
in	O
large-sample	O
situa-	O
tions	O
,	O
maximum	B
likelihood	I
estimates	O
tend	O
to	O
be	O
nearly	O
unbiased	O
,	O
with	O
the	O
least	O
possible	O
variance	O
.	O
even	O
in	O
small	O
samples	O
,	O
mles	O
are	O
usually	O
quite	O
efﬁcient	O
,	O
within	O
say	O
a	O
few	O
percent	O
of	O
the	O
best	O
possible	O
performance	O
.	O
3	O
the	O
mle	O
also	O
has	O
reasonable	O
bayesian	O
justiﬁcation	O
.	O
looking	O
at	O
bayes	O
’	O
rule	B
(	O
3.7	O
)	O
,	O
g.	O
(	O
cid:22	O
)	O
jx/	O
d	O
cxg	O
.	O
(	O
cid:22	O
)	O
/elx	O
.	O
(	O
cid:22	O
)	O
/	O
;	O
(	O
4.4	O
)	O
we	O
see	O
that	O
o	O
(	O
cid:22	O
)	O
is	O
the	O
maximizer	O
of	O
the	O
posterior	O
density	O
g.	O
(	O
cid:22	O
)	O
jx/	O
if	O
the	O
prior	B
g.	O
(	O
cid:22	O
)	O
/	O
is	O
ﬂat	O
,	O
that	O
is	O
,	O
constant	O
.	O
because	O
the	O
mle	O
depends	O
on	O
the	O
family	O
f	O
only	O
through	O
the	O
likelihood	B
function	O
,	O
anomalies	O
of	O
the	O
meter-reader	O
type	O
are	O
averted	O
.	O
figure	O
4.1	O
displays	O
two	O
maximum	B
likelihood	I
estimates	O
for	O
the	O
gfr	O
data	B
of	O
figure	O
2.1.	O
here	O
the	O
data1	O
is	O
the	O
vector	B
x	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn/	O
,	O
n	O
d	O
211.	O
we	O
assume	O
that	O
x	O
was	O
obtained	O
as	O
a	O
random	O
sample	B
of	O
size	O
n	O
from	O
a	O
density	B
f	O
(	O
cid:22	O
)	O
.x/	O
,	O
iid	O
(	O
cid:24	O
)	O
f	O
(	O
cid:22	O
)	O
.x/	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
;	O
xi	O
(	O
4.5	O
)	O
“	O
iid	O
”	O
abbreviating	O
“	O
independent	O
and	O
identically	O
distributed.	O
”	O
two	O
families	O
are	O
considered	O
for	O
the	O
component	O
density	B
f	O
(	O
cid:22	O
)	O
.x/	O
,	O
the	O
normal	B
,	O
with	O
(	O
cid:22	O
)	O
d	O
.	O
;	O
(	O
cid:27	O
)	O
/	O
,	O
f	O
(	O
cid:22	O
)	O
.x/	O
d	O
1p	O
2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
(	O
cid:0	O
)	O
1	O
2	O
.	O
x	O
(	O
cid:0	O
)	O
	O
(	O
cid:27	O
)	O
/2	O
;	O
e	O
(	O
4.6	O
)	O
1	O
now	O
x	O
is	O
what	O
we	O
have	O
been	O
calling	O
“	O
x	O
”	O
before	O
,	O
while	O
we	O
will	O
henceforth	O
use	O
x	O
as	O
a	O
symbol	O
for	O
the	O
individual	O
components	O
of	O
x	O
.	O
40	O
fisherian	O
inference	B
and	O
mle	O
figure	O
4.1	O
glomerular	O
ﬁltration	O
data	B
of	O
figure	O
2.1	O
and	O
two	O
maximum-likelihood	O
density	O
estimates	O
,	O
normal	B
(	O
solid	O
black	O
)	O
,	O
and	O
gamma	O
(	O
dashed	O
blue	O
)	O
.	O
and	O
the	O
gamma,2	O
with	O
(	O
cid:22	O
)	O
d	O
.	O
(	O
cid:21	O
)	O
;	O
(	O
cid:27	O
)	O
;	O
(	O
cid:23	O
)	O
/	O
,	O
f	O
(	O
cid:22	O
)	O
.x/	O
d	O
.x	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
/	O
(	O
cid:23	O
)	O
(	O
cid:0	O
)	O
1	O
(	O
cid:0	O
)	O
x	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
(	O
cid:27	O
)	O
e	O
(	O
for	O
x	O
(	O
cid:21	O
)	O
(	O
cid:21	O
)	O
,	O
0	O
otherwise	O
)	O
:	O
(	O
4.7	O
)	O
(	O
4.8	O
)	O
(	O
4.9	O
)	O
under	O
iid	O
sampling	O
,	O
we	O
have	O
since	O
id1	O
(	O
cid:27	O
)	O
(	O
cid:23	O
)	O
	O
.	O
(	O
cid:23	O
)	O
/	O
f	O
(	O
cid:22	O
)	O
.xi	O
/	O
f	O
(	O
cid:22	O
)	O
.x/	O
d	O
ny	O
log	O
f	O
(	O
cid:22	O
)	O
.xi	O
/	O
d	O
nx	O
hx	O
lx	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
nx	O
	O
d	O
.54:3	O
;	O
13:7/	O
dnx	O
;	O
id1	O
(	O
cid:16	O
)	O
o	O
	O
;	O
o	O
(	O
cid:27	O
)	O
lxi	O
.	O
(	O
cid:22	O
)	O
/	O
:	O
id1	O
i1=2	O
maximum	B
likelihood	I
estimates	O
were	O
found	O
by	O
maximizing	O
lx.	O
(	O
cid:22	O
)	O
/	O
.	O
for	O
the	O
normal	B
model	O
(	O
4.6	O
)	O
,	O
(	O
4.10	O
)	O
2	O
the	O
gamma	B
distribution	O
is	O
usually	O
deﬁned	O
with	O
(	O
cid:21	O
)	O
d	O
0	O
as	O
the	O
lower	O
limit	O
of	O
x.	O
here	O
we	O
are	O
allowing	O
the	O
lower	O
limit	O
(	O
cid:21	O
)	O
to	O
vary	O
as	O
a	O
free	O
parameter	O
.	O
:	O
.xi	O
(	O
cid:0	O
)	O
nx/2	O
=n	O
gfrfrequency20406080100051015202530normalgamma	O
4.2	O
fisher	O
information	B
and	O
the	O
mle	O
41	O
there	O
is	O
no	O
closed-form	O
solution	O
for	O
gamma	B
model	O
(	O
4.7	O
)	O
,	O
where	O
numerical	O
maximization	O
gave	O
	O
d	O
.21:4	O
;	O
5:47	O
;	O
6:0/	O
:	O
(	O
cid:16	O
)	O
o	O
(	O
cid:21	O
)	O
;	O
o	O
(	O
cid:27	O
)	O
;	O
o	O
(	O
cid:23	O
)	O
(	O
4.11	O
)	O
the	O
plotted	O
curves	O
in	O
figure	O
4.1	O
are	O
the	O
two	O
mle	O
densities	O
f	O
o	O
(	O
cid:22	O
)	O
.x/	O
.	O
the	O
gamma	B
model	O
gives	O
a	O
better	O
ﬁt	O
than	O
the	O
normal	B
,	O
but	O
neither	O
is	O
really	O
satis-	O
factory	O
.	O
(	O
a	O
more	O
ambitious	O
maximum	B
likelihood	I
ﬁt	O
appears	O
in	O
figure	O
5.7	O
.	O
)	O
most	O
mles	O
require	O
numerical	O
minimization	O
,	O
as	O
for	O
the	O
gamma	B
model	O
.	O
when	O
introduced	O
in	O
the	O
1920s	O
,	O
maximum	B
likelihood	I
was	O
criticized	O
as	O
com-	O
putationally	O
difﬁcult	O
,	O
invidious	O
comparisons	O
being	O
made	O
with	O
the	O
older	O
method	B
of	O
moments	O
,	O
which	O
relied	O
only	O
on	O
sample	B
moments	O
of	O
various	O
kinds	O
.	O
there	O
is	O
a	O
downside	O
to	O
maximum	B
likelihood	I
estimation	O
that	O
remained	O
nearly	O
invisible	O
in	O
classical	O
applications	O
:	O
it	O
is	O
dangerous	O
to	O
rely	O
upon	O
in	O
problems	O
involving	O
large	O
numbers	O
of	O
parameters	O
.	O
if	O
the	O
parameter	O
vector	B
(	O
cid:22	O
)	O
has	O
1000	O
components	O
,	O
each	O
component	O
individually	O
may	O
be	O
well	O
esti-	O
mated	O
by	O
maximum	B
likelihood	I
,	O
while	O
the	O
mle	O
o	O
	O
d	O
t	B
.	O
o	O
(	O
cid:22	O
)	O
/	O
for	O
a	O
quantity	B
of	O
particular	O
interest	O
can	O
be	O
grossly	O
misleading	O
.	O
for	O
the	O
prostate	B
data	O
of	O
figure	O
3.4	O
,	O
model	B
(	O
4.6	O
)	O
gives	O
mle	O
o	O
(	O
cid:22	O
)	O
i	O
d	O
xi	O
for	O
each	O
of	O
the	O
6033	O
genes	O
.	O
this	O
seems	O
reasonable	O
,	O
but	O
if	O
we	O
are	O
interested	O
in	O
the	O
maximum	O
coordinate	O
value	O
	O
d	O
t	B
.	O
(	O
cid:22	O
)	O
/	O
d	O
max	O
f	O
(	O
cid:22	O
)	O
ig	O
;	O
(	O
4.12	O
)	O
the	O
mle	O
is	O
o	O
	O
d	O
5:29	O
,	O
almost	O
certainly	O
a	O
ﬂagrant	O
overestimate	O
.	O
“	O
regular-	O
ized	O
”	O
versions	O
of	O
maximum	B
likelihood	I
estimation	O
more	O
suitable	O
for	O
high-	O
dimensional	O
applications	O
play	O
an	O
important	O
role	O
in	O
succeeding	O
chapters	O
.	O
i	O
4.2	O
fisher	O
information	B
and	O
the	O
mle	O
fisher	O
was	O
not	O
the	O
ﬁrst	O
to	O
suggest	O
the	O
maximum	B
likelihood	I
algorithm	O
for	O
parameter	O
estimation	B
.	O
his	O
paradigm-shifting	O
work	O
concerned	O
the	O
favorable	O
inferential	O
properties	O
of	O
the	O
mle	O
,	O
and	O
in	O
particular	O
its	O
achievement	O
of	O
the	O
fisher	O
information	B
bound	O
.	O
only	O
a	O
brief	O
heuristic	O
review	O
will	O
be	O
provided	O
here	O
,	O
with	O
more	O
careful	O
derivations	O
referenced	O
in	O
the	O
endnotes	O
.	O
we	O
begin3	O
with	O
a	O
one-parameter	B
family	O
of	O
densities	O
g	O
;	O
d	O
ff	O
.x/	O
;	O
	O
2	O
(	O
cid:127	O
)	O
;	O
x	O
2	O
x	O
3	O
the	O
multiparameter	O
case	O
is	O
considered	O
in	O
the	O
next	O
chapter	O
.	O
f	O
(	O
4.13	O
)	O
42	O
fisherian	O
inference	B
and	O
mle	O
where	O
(	O
cid:127	O
)	O
is	O
an	O
interval	B
of	O
the	O
real	O
line	O
,	O
possibly	O
inﬁnite	O
,	O
while	O
the	O
sam-	O
the	O
continuous	O
case	O
,	O
with	O
the	O
probability	O
of	O
set	B
a	O
equalingr	O
ple	O
space	B
x	O
may	O
be	O
multidimensional	O
.	O
(	O
as	O
in	O
the	O
poisson	O
example	O
(	O
3.3	O
)	O
,	O
f	O
.x/	O
can	O
represent	O
a	O
discrete	O
density	B
,	O
but	O
for	O
convenience	O
we	O
assume	O
here	O
a	O
f	O
.x/	O
dx	O
,	O
etc	O
.	O
)	O
the	O
log	O
likelihood	B
function	O
is	O
lx.	O
/	O
d	O
log	O
f	O
.x/	O
and	O
the	O
mle	O
o	O
	O
d	O
arg	O
maxflx.	O
/g	O
,	O
with	O
	O
replacing	O
(	O
cid:22	O
)	O
in	O
(	O
4.1	O
)	O
–	O
(	O
4.2	O
)	O
in	O
the	O
one-dimensional	O
case	O
.	O
dots	O
will	O
indicate	O
differentiation	O
with	O
respect	O
to	O
	O
,	O
e.g.	O
,	O
for	O
the	O
score	O
function	B
p	O
lx.	O
/	O
d	O
@	O
log	O
f	O
.x/	O
d	O
p	O
f	O
.x/=f	O
.x/	O
:	O
the	O
score	O
function	B
has	O
expectation	O
0	O
,	O
z	O
@	O
	O
lx.	O
/f	O
.x/	O
dx	O
dz	O
p	O
x	O
x	O
z	O
x	O
1	O
d	O
0	O
;	O
f	O
.x/	O
dx	O
p	O
f	O
.x/	O
dx	O
d	O
@	O
@	O
	O
d	O
@	O
@	O
	O
(	O
4.14	O
)	O
(	O
4.15	O
)	O
where	O
we	O
are	O
assuming	O
the	O
regularity	O
conditions	B
necessary	O
for	O
differenti-	O
ating	O
under	O
the	O
integral	O
sign	O
at	O
the	O
third	O
step	O
.	O
the	O
fisher	O
information	B
i	O
is	O
deﬁned	O
to	O
be	O
the	O
variance	O
of	O
the	O
score	O
function	B
,	O
p	O
lx.	O
/2f	O
.x/	O
dx	O
;	O
(	O
4.16	O
)	O
i	O
dz	O
x	O
the	O
notation	O
p	O
lx.	O
/	O
(	O
cid:24	O
)	O
.0	O
;	O
i	O
/	O
(	O
4.17	O
)	O
indicating	O
that	O
p	O
lx.	O
/	O
has	O
mean	O
0	O
and	O
variance	O
i	O
.	O
the	O
term	O
“	O
information	B
”	O
is	O
well	O
chosen	O
.	O
the	O
main	O
result	O
for	O
maximum	B
likelihood	I
estimation	O
,	O
sketched	O
next	O
,	O
is	O
that	O
the	O
mle	O
o	O
	O
has	O
an	O
approximately	O
normal	B
distribution	O
with	O
mean	O
	O
and	O
variance	O
1=i	O
,	O
o	O
	O
p	O
(	O
cid:24	O
)	O
n	O
.	O
;	O
1=i	O
/	O
;	O
(	O
4.18	O
)	O
and	O
that	O
no	O
“	O
nearly	O
unbiased	O
”	O
estimator	B
of	O
	O
can	O
do	O
better	O
.	O
in	O
other	O
words	O
,	O
bigger	O
fisher	O
information	B
implies	O
smaller	O
variance	O
for	O
the	O
mle	O
.	O
the	O
second	O
derivative	O
of	O
the	O
log	O
likelihood	B
function	O
r	O
lx.	O
/	O
d	O
@	O
2	O
@	O
	O
2	O
log	O
f	O
.x/	O
d	O
r	O
f	O
.x/	O
f	O
.x/	O
(	O
cid:0	O
)	O
f	O
.x/	O
f	O
.x/	O
(	O
4.19	O
)	O
p	O
!	O
2	O
4.2	O
fisher	O
information	B
and	O
the	O
mle	O
has	O
expectation	O
e	O
(	O
the	O
r	O
nr	O
lx.	O
/	O
o	O
d	O
(	O
cid:0	O
)	O
i	O
43	O
(	O
4.20	O
)	O
(	O
4.21	O
)	O
f	O
.x/=f	O
.x/	O
term	O
having	O
expectation	O
0	O
as	O
in	O
(	O
4.15	O
)	O
)	O
.	O
we	O
can	O
write	O
(	O
cid:0	O
)	O
r	O
lx.	O
/	O
(	O
cid:24	O
)	O
.i	O
;	O
j	O
/	O
;	O
where	O
j	O
is	O
the	O
variance	O
of	O
r	O
lx.	O
/	O
.	O
now	O
suppose	O
that	O
x	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn/	O
is	O
an	O
iid	O
sample	B
from	O
f	O
.x/	O
,	O
as	O
in	O
(	O
4.5	O
)	O
,	O
so	O
that	O
the	O
total	O
score	O
function	B
p	O
lx.	O
/	O
d	O
nx	O
lx.	O
/	O
,	O
as	O
in	O
(	O
4.9	O
)	O
,	O
is	O
p	O
lxi	O
.	O
/	O
;	O
lx.	O
/	O
d	O
nx	O
	O
:	O
d	O
p	O
(	O
cid:16	O
)	O
o	O
lx.	O
/	O
c	O
r	O
o	O
	O
/	O
d	O
0.	O
a	O
ﬁrst-order	O
taylor	O
series	O
gives	O
the	O
approximation	O
	O
based	O
on	O
the	O
full	B
sample	O
x	O
satisﬁes	O
the	O
maximizing	O
condition	B
the	O
mle	O
o	O
p	O
lx	O
.	O
(	O
cid:0	O
)	O
r	O
lxi	O
.	O
/	O
:	O
	O
(	O
cid:0	O
)	O
	O
;	O
0	O
d	O
p	O
lx	O
	O
id1	O
id1	O
p	O
(	O
cid:0	O
)	O
r	O
and	O
similarly	O
(	O
cid:16	O
)	O
o	O
(	O
4.22	O
)	O
(	O
4.23	O
)	O
	O
lx.	O
/	O
(	O
4.24	O
)	O
or	O
o	O
	O
:	O
d	O
	O
c	O
p	O
lx.	O
/=n	O
(	O
cid:0	O
)	O
r	O
lx.	O
/=n	O
:	O
(	O
4.25	O
)	O
under	O
reasonable	O
regularity	O
conditions	B
,	O
(	O
4.17	O
)	O
and	O
the	O
central	O
limit	O
theo-	O
rem	O
imply	O
that	O
p	O
lx.	O
/=n	O
p	O
(	O
cid:24	O
)	O
while	O
the	O
law	O
of	O
large	O
numbers	O
has	O
(	O
cid:0	O
)	O
r	O
(	O
4.21	O
)	O
.	O
(	O
4.26	O
)	O
n	O
.0	O
;	O
i	O
=n/	O
;	O
lx.	O
/=n	O
approaching	O
the	O
constant	O
i	O
putting	O
all	O
of	O
this	O
together	O
,	O
(	O
4.25	O
)	O
produces	O
fisher	O
’	O
s	O
fundamental	O
theo-	O
rem	O
for	O
the	O
mle	O
,	O
that	O
in	O
large	O
samples	O
o	O
	O
p	O
(	O
cid:24	O
)	O
n	O
.	O
;	O
1=.ni	O
//	O
:	O
(	O
4.27	O
)	O
this	O
is	O
the	O
same	O
as	O
result	O
(	O
4.18	O
)	O
since	O
the	O
total	O
fisher	O
information	B
in	O
an	O
iid	O
sample	B
(	O
4.5	O
)	O
is	O
ni	O
,	O
as	O
can	O
be	O
seen	O
by	O
taking	O
expectations	O
in	O
(	O
4.23	O
)	O
.	O
in	O
the	O
case	O
of	O
normal	B
sampling	O
,	O
iid	O
(	O
cid:24	O
)	O
xi	O
n	O
.	O
;	O
(	O
cid:27	O
)	O
2/	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
;	O
(	O
4.28	O
)	O
44	O
fisherian	O
inference	B
and	O
mle	O
(	O
cid:27	O
)	O
2	O
2	O
id1	O
this	O
gives	O
log.2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2/	O
:	O
.xi	O
(	O
cid:0	O
)	O
	O
/2	O
with	O
(	O
cid:27	O
)	O
2	O
known	O
,	O
we	O
compute	O
the	O
log	O
likelihood	B
(	O
cid:0	O
)	O
n	O
2	O
nx	O
lx.	O
/	O
d	O
(	O
cid:0	O
)	O
1	O
nx	O
.xi	O
(	O
cid:0	O
)	O
	O
/	O
lx.	O
/	O
d	O
n	O
id1	O
yielding	O
the	O
familiar	O
result	O
o	O
	O
d	O
nx	O
and	O
,	O
since	O
i	O
d	O
1=	O
(	O
cid:27	O
)	O
2	O
,	O
o	O
	O
(	O
cid:24	O
)	O
p	O
lx.	O
/	O
d	O
1	O
and	O
(	O
cid:0	O
)	O
r	O
(	O
cid:27	O
)	O
2	O
(	O
cid:27	O
)	O
2	O
n	O
.	O
;	O
(	O
cid:27	O
)	O
2=n/	O
(	O
4.29	O
)	O
;	O
(	O
4.30	O
)	O
(	O
4.31	O
)	O
from	O
(	O
4.27	O
)	O
.	O
this	O
brings	O
us	O
to	O
an	O
aspect	O
of	O
fisherian	O
inference	B
neither	O
bayesian	O
nor	O
frequentist	O
.	O
fisher	O
believed	O
there	O
was	O
a	O
“	O
logic	O
of	O
inductive	O
inference	B
”	O
that	O
would	O
produce	O
the	O
correct	O
answer	O
to	O
any	O
statistical	O
question	O
,	O
in	O
the	O
same	O
way	O
ordinary	O
logic	O
solves	O
deductive	O
problems	O
.	O
his	O
principal	O
tactic	O
was	O
to	O
logically	O
reduce	O
a	O
complicated	O
inferential	O
question	O
to	O
a	O
simple	O
form	B
where	O
the	O
solution	O
should	O
be	O
obvious	O
to	O
all	O
.	O
fisher	O
’	O
s	O
favorite	O
target	O
for	O
the	O
obvious	O
was	O
(	O
4.31	O
)	O
,	O
where	O
a	O
single	O
scalar	O
observation	O
o	O
	O
is	O
normally	O
distributed	O
around	O
the	O
unknown	O
parameter	O
of	O
interest	O
	O
,	O
with	O
known	O
variance	O
(	O
cid:27	O
)	O
2=n	O
.	O
then	O
everyone	O
should	O
agree	O
in	O
the	O
absence	O
of	O
prior	B
information	O
that	O
o	O
	O
is	O
the	O
best	O
estimate	O
of	O
	O
,	O
that	O
	O
has	O
about	O
95	O
%	O
chance	O
of	O
lying	O
in	O
the	O
interval	B
o	O
fisher	O
was	O
astoundingly	O
resourceful	O
at	O
reducing	O
statistical	O
problems	O
to	O
the	O
form	B
(	O
4.31	O
)	O
.	O
sufﬁciency	O
,	O
efﬁciency	O
,	O
conditionality	O
,	O
and	O
ancillarity	O
were	O
all	O
brought	O
to	O
bear	O
,	O
with	O
the	O
maximum	B
likelihood	I
approximation	O
(	O
4.27	O
)	O
being	O
the	O
most	O
inﬂuential	O
example	O
.	O
fisher	O
’	O
s	O
logical	O
system	O
is	O
not	O
in	O
favor	O
these	O
days	O
,	O
but	O
its	O
conclusions	O
remain	O
as	O
staples	O
of	O
conventional	O
statistical	O
practice	O
.	O
suppose	O
that	O
q	O
	O
d	O
t	B
.x/	O
is	O
any	O
unbiased	O
estimate	O
of	O
	O
based	O
on	O
an	O
iid	O
sample	B
x	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn/	O
from	O
f	O
.x/	O
.	O
that	O
is	O
,	O
	O
˙	O
1:96o	O
(	O
cid:27	O
)	O
=	O
p	O
n	O
,	O
etc	O
.	O
then	O
the	O
cram´er–rao	O
lower	O
bound	B
,	O
described	O
in	O
the	O
endnotes	O
,	O
says	O
that	O
the	O
variance	O
of	O
q	O
	O
exceeds	O
the	O
fisher	O
information	B
bound	O
(	O
4.27	O
)	O
,	O
	O
1	O
	O
d	O
eft	O
.x/g	O
:	O
n	O
q	O
	O
o	O
(	O
cid:21	O
)	O
1=.ni	O
/	O
:	O
var	O
(	O
4.32	O
)	O
(	O
4.33	O
)	O
a	O
loose	O
interpretation	O
is	O
that	O
the	O
mle	O
has	O
variance	O
at	O
least	O
as	O
small	O
as	O
the	O
best	O
unbiased	O
estimate	O
of	O
	O
.	O
the	O
mle	O
is	O
generally	O
not	O
unbiased	O
,	O
but	O
4.3	O
conditional	O
inference	B
45	O
p	O
its	O
bias	O
is	O
small	O
(	O
of	O
order	O
1=n	O
,	O
compared	O
with	O
standard	B
deviation	I
of	O
order	O
n	O
)	O
,	O
making	O
the	O
comparison	O
with	O
unbiased	O
estimates	O
and	O
the	O
cram´er–	O
1=	O
rao	O
bound	B
appropriate	O
.	O
4.3	O
conditional	O
inference	B
a	O
simple	O
example	O
gets	O
across	O
the	O
idea	O
of	O
conditional	O
inference	B
:	O
an	O
i.i.d	O
.	O
sample	B
xi	O
iid	O
(	O
cid:24	O
)	O
(	O
4.34	O
)	O
n	O
.	O
;	O
1/	O
;	O
has	O
produced	O
estimate	B
o	O
	O
d	O
nx	O
.	O
the	O
investigators	O
originally	O
disagreed	O
on	O
an	O
(	O
affordable	O
sample	B
size	I
n	O
and	O
ﬂipped	O
a	O
fair	O
coin	O
to	O
decide	O
,	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
;	O
25	O
100	O
probability	O
1/2	O
probability	O
1/2i	O
n	O
d	O
(	O
4.35	O
)	O
p	O
if	O
you	O
answered	O
1=	O
n	O
d	O
25	O
won	O
.	O
question	O
:	O
what	O
is	O
the	O
standard	B
deviation	I
of	O
nx	O
?	O
25	O
d	O
0:2	O
then	O
you	O
,	O
like	O
fisher	O
,	O
are	O
an	O
advocate	O
of	O
conditional	O
inference	B
.	O
the	O
unconditional	O
frequentist	O
answer	O
says	O
that	O
nx	O
could	O
have	O
been	O
n	O
.	O
;	O
1=100/	O
or	O
n	O
.	O
;	O
1=25/	O
with	O
equal	O
probability	O
,	O
yield-	O
ing	O
standard	B
deviation	I
œ.0:01	O
c	O
0:04/=21=2	O
d	O
0:158.	O
some	O
less	O
obvious	O
(	O
and	O
less	O
trivial	O
)	O
examples	O
follow	O
in	O
this	O
section	O
,	O
and	O
in	O
chapter	O
9	O
,	O
where	O
conditional	O
inference	B
plays	O
a	O
central	O
role	O
.	O
the	O
data	B
for	O
a	O
typical	O
regression	B
problem	O
consists	O
of	O
pairs	O
.xi	O
;	O
yi	O
/	O
,	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
,	O
where	O
xi	O
is	O
a	O
p-dimensional	O
vector	B
of	O
covariates	O
for	O
the	O
ith	O
subject	O
and	O
yi	O
is	O
a	O
scalar	O
response	O
.	O
in	O
figure	O
1.1	O
,	O
xi	O
is	O
age	O
and	O
yi	O
the	O
kidney	O
ﬁtness	O
measure	O
tot	O
.	O
let	O
x	O
be	O
the	O
n	O
(	O
cid:2	O
)	O
p	O
matrix	B
having	O
xi	O
as	O
its	O
ith	O
row	O
,	O
and	O
y	O
the	O
vector	B
of	O
responses	O
.	O
a	O
regression	B
algorithm	O
uses	O
x	O
and	O
y	O
to	O
construct	O
a	O
function	B
rx	O
;	O
y	O
.x/	O
predicting	O
y	O
for	O
any	O
value	O
of	O
x	O
,	O
as	O
in	O
(	O
1.3	O
)	O
,	O
where	O
o	O
how	O
accurate	O
is	O
rx	O
;	O
y	O
.x/	O
?	O
this	O
question	O
is	O
usually	O
answered	O
under	O
the	O
assumption	O
that	O
x	O
is	O
ﬁxed	O
,	O
not	O
random	O
:	O
in	O
other	O
words	O
,	O
by	O
conditioning	O
on	O
the	O
observed	O
value	O
of	O
x.	O
the	O
standard	O
errors	O
in	O
the	O
second	O
line	O
of	O
ta-	O
ble	O
1.1	O
are	O
conditional	O
in	O
this	O
sense	O
;	O
they	O
are	O
frequentist	O
standard	B
deviations	I
ˇ0	O
c	O
o	O
of	O
o	O
ˇ1x	O
,	O
assuming	O
that	O
the	O
157	O
values	O
for	O
age	O
are	O
ﬁxed	O
as	O
observed	O
.	O
(	O
a	O
correlation	O
analysis	O
between	O
age	O
and	O
tot	O
would	O
not	O
make	O
this	O
as-	O
sumption	O
.	O
)	O
ˇ1	O
were	O
obtained	O
using	O
least	B
squares	I
.	O
ˇ0	O
and	O
o	O
fisher	O
argued	O
for	O
conditional	O
inference	B
on	O
two	O
grounds	O
.	O
46	O
fisherian	O
inference	B
and	O
mle	O
1	O
more	O
relevant	O
inferences	O
.	O
the	O
conditional	O
standard	B
deviation	I
in	O
situ-	O
ation	O
(	O
4.35	O
)	O
seems	O
obviously	O
more	O
relevant	O
to	O
the	O
accuracy	O
of	O
the	O
ob-	O
served	O
o	O
	O
for	O
estimating	O
	O
.	O
it	O
is	O
less	O
obvious	O
in	O
the	O
regression	B
example	O
,	O
though	O
arguably	O
still	O
the	O
case	O
.	O
2	O
simpler	O
inferences	O
.	O
conditional	O
inferences	O
are	O
often	O
simpler	O
to	O
exe-	O
cute	O
and	O
interpret	O
.	O
this	O
is	O
the	O
case	O
with	O
regression	B
,	O
where	O
the	O
statistician	O
doesn	O
’	O
t	B
have	O
to	O
worry	O
about	O
correlation	O
relationships	O
among	O
the	O
covari-	O
ates	O
,	O
and	O
also	O
with	O
our	O
next	O
example	O
,	O
a	O
fisherian	O
classic	O
.	O
table	O
4.1	O
shows	O
the	O
results	O
of	O
a	O
randomized	O
trial	O
on	O
45	O
ulcer	O
patients	O
,	O
comparing	O
new	O
and	O
old	O
surgical	O
treatments	O
.	O
was	O
the	O
new	O
surgery	O
signiﬁ-	O
cantly	O
better	O
?	O
fisher	O
argued	O
for	O
carrying	O
out	O
the	O
hypothesis	O
test	O
conditional	O
on	O
the	O
marginals	O
of	O
the	O
table	O
.16	O
;	O
29	O
;	O
21	O
;	O
24/	O
.	O
with	O
the	O
marginals	O
ﬁxed	O
,	O
the	O
number	O
y	O
in	O
the	O
upper	O
left	O
cell	O
determines	O
the	O
other	O
three	O
cells	O
by	O
subtrac-	O
tion	O
.	O
we	O
need	O
only	O
test	O
whether	O
the	O
number	O
y	O
d	O
9	O
is	O
too	O
big	O
under	O
the	O
null	O
hypothesis	O
of	O
no	O
treatment	O
difference	O
,	O
instead	O
of	O
trying	O
to	O
test	O
the	O
numbers	O
in	O
all	O
four	O
cells.4	O
table	O
4.1	O
forty-ﬁve	O
ulcer	O
patients	O
randomly	O
assigned	O
to	O
either	O
new	O
or	O
old	O
surgery	O
,	O
with	O
results	O
evaluated	O
as	O
either	O
success	O
or	O
failure	O
.	O
was	O
the	O
new	O
surgery	O
signiﬁcantly	O
better	O
?	O
success	O
failure	O
new	O
old	O
9	O
7	O
16	O
12	O
17	O
29	O
21	O
24	O
45	O
an	O
ancillary	O
statistic	B
(	O
again	O
,	O
fisher	O
’	O
s	O
terminology	O
)	O
is	O
one	O
that	O
contains	O
no	O
direct	O
information	B
by	O
itself	O
,	O
but	O
does	O
determine	O
the	O
conditioning	O
frame-	O
work	O
for	O
frequentist	O
calculations	O
.	O
our	O
three	O
examples	O
of	O
ancillaries	O
were	O
the	O
sample	B
size	I
n	O
,	O
the	O
covariate	O
matrix	B
x	O
,	O
and	O
the	O
table	O
’	O
s	O
marginals	O
.	O
“	O
con-	O
tains	O
no	O
information	B
”	O
is	O
a	O
contentious	O
claim	O
.	O
more	O
realistically	O
,	O
the	O
two	O
ad-	O
vantages	O
of	O
conditioning	O
,	O
relevance	O
and	O
simplicity	O
,	O
are	O
thought	O
to	O
outweigh	O
the	O
loss	O
of	O
information	B
that	O
comes	O
from	O
treating	O
the	O
ancillary	O
statistic	B
as	O
nonrandom	O
.	O
chapter	O
9	O
makes	O
this	O
case	O
speciﬁcally	O
for	O
standard	O
survival	O
analysis	B
methods	O
.	O
4	O
section	O
9.3	O
gives	O
the	O
details	O
of	O
such	O
tests	O
;	O
in	O
the	O
surgery	O
example	O
,	O
the	O
difference	O
was	O
not	O
signiﬁcant	O
.	O
4.3	O
conditional	O
inference	B
47	O
our	O
ﬁnal	O
example	O
concerns	O
the	O
accuracy	O
of	O
a	O
maximum	B
likelihood	I
esti-	O
mate	O
o	O
	O
.	O
rather	O
than	O
(	O
4.36	O
)	O
(	O
4.37	O
)	O
:	O
(	O
4.38	O
)	O
ˇˇˇˇo	O
(	O
cid:0	O
)	O
	O
;	O
1ı	O
(	O
cid:0	O
)	O
nio	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
;	O
o	O
	O
p	O
(	O
cid:24	O
)	O
n	O
o	O
	O
p	O
(	O
cid:24	O
)	O
the	O
plug-in	O
version	O
of	O
(	O
4.27	O
)	O
,	O
fisher	O
suggested	O
using	O
n	O
.	O
;	O
1=i.x//	O
;	O
where	O
i.x/	O
is	O
the	O
observed	O
fisher	O
information	B
(	O
cid:16	O
)	O
o	O
	O
	O
d	O
(	O
cid:0	O
)	O
@	O
2	O
lx.	O
/	O
@	O
	O
2	O
i.x/	O
d	O
(	O
cid:0	O
)	O
r	O
lx	O
the	O
expectation	O
of	O
i.x/	O
is	O
ni	O
,	O
so	O
in	O
large	O
samples	O
the	O
distribution	B
(	O
4.37	O
)	O
converges	O
to	O
(	O
4.36	O
)	O
.	O
before	O
convergence	O
,	O
however	O
,	O
fisher	O
suggested	O
that	O
(	O
4.37	O
)	O
gives	O
a	O
better	O
idea	O
of	O
o	O
as	O
a	O
check	O
,	O
a	O
simulation	O
was	O
run	O
involving	O
i.i.d	O
.	O
samples	O
x	O
of	O
size	O
n	O
d	O
	O
’	O
s	O
accuracy	O
.	O
20	O
drawn	O
from	O
a	O
cauchy	O
density	B
f	O
.x/	O
d	O
1	O
1	O
:	O
(	O
cid:25	O
)	O
1	O
c	O
.x	O
(	O
cid:0	O
)	O
	O
/2	O
	O
within	O
each	O
group	O
was	O
then	O
calculated	O
.	O
(	O
4.39	O
)	O
10,000	O
samples	O
x	O
of	O
size	O
n	O
d	O
20	O
were	O
drawn	O
(	O
with	O
	O
d	O
0	O
)	O
and	O
the	O
ob-	O
served	O
information	B
bound	O
1=i.x/	O
computed	O
for	O
each	O
.	O
the	O
10,000	O
o	O
	O
values	O
were	O
grouped	O
according	O
to	O
deciles	O
of	O
1=i.x/	O
,	O
and	O
the	O
observed	O
empirical	B
variance	O
of	O
o	O
this	O
amounts	O
to	O
calculating	O
a	O
somewhat	O
crude	O
estimate	O
of	O
the	O
condi-	O
tional	O
variance	O
of	O
the	O
mle	O
o	O
	O
,	O
given	O
the	O
observed	O
information	B
bound	O
1=i.x/	O
.	O
figure	O
4.2	O
shows	O
the	O
results	O
.	O
we	O
see	O
that	O
the	O
conditional	O
variance	O
is	O
close	O
to	O
1=i.x/	O
,	O
as	O
fisher	O
predicted	O
.	O
the	O
conditioning	O
effect	O
is	O
quite	O
substan-	O
tial	O
;	O
the	O
unconditional	O
variance	O
1=ni	O
is	O
0.10	O
here	O
,	O
while	O
the	O
conditional	O
variance	O
ranges	O
from	O
0.05	O
to	O
0.20.	O
the	O
observed	O
fisher	O
information	B
i.x/	O
acts	O
as	O
an	O
approximate	O
ancillary	O
,	O
enjoying	O
both	O
of	O
the	O
virtues	O
claimed	O
by	O
fisher	O
:	O
it	O
is	O
more	O
relevant	O
than	O
the	O
unconditional	O
information	B
nio	O
,	O
and	O
it	O
is	O
usually	O
easier	O
to	O
calculate	O
.	O
once	O
o	O
	O
has	O
been	O
found	O
,	O
i.x/	O
is	O
obtained	O
by	O
numerical	O
second	O
differentiation	O
.	O
unlike	O
i	O
,	O
no	O
probability	O
calculations	O
are	O
required	O
.	O
there	O
is	O
a	O
strong	O
bayesian	O
current	O
ﬂowing	O
here	O
.	O
a	O
narrow	O
peak	O
for	O
the	O
log	O
likelihood	B
function	O
,	O
i.e.	O
,	O
a	O
large	O
value	O
of	O
i.x/	O
,	O
also	O
implies	O
a	O
narrow	O
posterior	B
distribution	I
for	O
	O
given	O
x.	O
conditional	O
inference	B
,	O
of	O
which	O
fig-	O
ure	O
4.2	O
is	O
an	O
evocative	O
example	O
,	O
helps	O
counter	O
the	O
central	O
bayesian	O
criti-	O
cism	O
of	O
frequentist	O
inference	B
:	O
that	O
the	O
frequentist	O
properties	O
relate	O
to	O
data	B
sets	O
possibly	O
much	O
different	O
than	O
the	O
one	O
actually	O
observed	O
.	O
the	O
maximum	O
48	O
fisherian	O
inference	B
and	O
mle	O
figure	O
4.2	O
conditional	O
variance	O
of	O
mle	O
for	O
cauchy	O
samples	O
of	O
size	O
20	O
,	O
plotted	O
versus	O
the	O
observed	O
information	B
bound	O
1=i.x/	O
.	O
observed	O
information	B
bounds	O
are	O
grouped	O
by	O
quantile	O
intervals	B
for	O
variance	O
calculations	O
(	O
in	O
percentages	O
)	O
:	O
(	O
0–5	O
)	O
,	O
(	O
5–15	O
)	O
,	O
:	O
:	O
:	O
,	O
(	O
85–95	O
)	O
,	O
(	O
95–100	O
)	O
.	O
the	O
broken	O
red	O
horizontal	O
line	O
is	O
the	O
unconditional	O
variance	O
1=ni	O
.	O
likelihood	B
algorithm	O
can	O
be	O
interpreted	O
both	O
vertically	O
and	O
horizontally	O
in	O
figure	O
3.5	O
,	O
acting	O
as	O
a	O
connection	O
between	O
the	O
bayesian	O
and	O
frequentist	O
worlds	O
.	O
the	O
equivalent	O
of	O
result	O
(	O
4.37	O
)	O
for	O
multiparameter	O
families	O
,	O
section	O
5.3	O
,	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
;	O
i.x/	O
(	O
cid:0	O
)	O
1	O
(	O
cid:1	O
)	O
;	O
o	O
(	O
cid:22	O
)	O
p	O
(	O
cid:24	O
)	O
(	O
4.40	O
)	O
plays	O
an	O
important	O
role	O
in	O
succeeding	O
chapters	O
,	O
with	O
(	O
cid:0	O
)	O
i.x/	O
the	O
p	O
(	O
cid:2	O
)	O
p	O
matrix	B
of	O
second	O
derivatives	O
np	O
lx	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
(	O
cid:0	O
)	O
	O
i.x/	O
d	O
(	O
cid:0	O
)	O
r	O
(	O
cid:21	O
)	O
@	O
2	O
@	O
(	O
cid:22	O
)	O
i	O
@	O
(	O
cid:22	O
)	O
j	O
log	O
f	O
(	O
cid:22	O
)	O
.x/	O
:	O
o	O
(	O
cid:22	O
)	O
(	O
4.41	O
)	O
lllllllllll0.050.100.150.200.250.000.050.100.150.200.25observed	O
information	B
boundmle	O
variance	O
4.4	O
permutation	O
and	O
randomization	O
49	O
4.4	O
permutation	O
and	O
randomization	O
fisherian	O
methodology	O
faced	O
criticism	O
for	O
its	O
overdependence	O
on	O
normal	B
sampling	O
assumptions	O
.	O
consider	O
the	O
comparison	O
between	O
the	O
47	O
all	O
and	O
25	O
aml	O
patients	O
in	O
the	O
gene	O
136	O
leukemia	B
example	O
of	O
figure	O
1.4.	O
the	O
two-	O
sample	B
t-statistic	O
(	O
1.6	O
)	O
had	O
value	O
3.13	O
,	O
with	O
two-sided	O
signiﬁcance	O
level	O
0.0025	O
according	O
to	O
a	O
student-t	O
null	O
distribution	B
with	O
70	O
degrees	O
of	O
free-	O
dom	O
.	O
all	O
of	O
this	O
depended	O
on	O
the	O
gaussian	O
,	O
or	O
normal	B
,	O
assumptions	O
(	O
2.12	O
)	O
–	O
(	O
2.13	O
)	O
.	O
as	O
an	O
alternative	O
signiﬁcance-level	O
calculation	O
,	O
fisher	O
suggested	O
using	O
permutations	O
of	O
the	O
72	O
data	B
points	O
.	O
the	O
72	O
values	O
are	O
randomly	O
divided	O
into	O
disjoint	O
sets	O
of	O
size	O
47	O
and	O
25	O
,	O
and	O
the	O
two-sample	B
t-statistic	O
(	O
2.17	O
)	O
is	O
recomputed	O
.	O
this	O
is	O
done	O
some	O
large	O
number	O
b	O
times	O
,	O
yielding	O
permuta-	O
(	O
cid:3	O
)	O
b.	O
the	O
two-sided	O
permutation	O
signiﬁcance	O
level	O
tion	O
t-values	O
t	B
(	O
cid:3	O
)	O
for	O
the	O
original	O
value	O
t	B
is	O
then	O
the	O
proportion	B
of	O
the	O
t	B
i	O
values	O
exceeding	O
t	B
in	O
absolute	O
value	O
,	O
(	O
cid:3	O
)	O
2	O
;	O
:	O
:	O
:	O
;	O
t	B
(	O
cid:3	O
)	O
1	O
;	O
t	B
(	O
4.42	O
)	O
#	O
fjt	O
(	O
cid:3	O
)	O
j	O
(	O
cid:21	O
)	O
jtjg	O
=b	O
:	O
i	O
figure	O
4.3	O
10,000	O
permutation	O
t	B
for	O
gene	O
136	O
in	O
the	O
leukemia	B
data	O
of	O
figure	O
1.3.	O
of	O
these	O
,	O
26	O
(	O
cid:3	O
)	O
-values	O
(	O
red	O
ticks	O
)	O
exceeded	O
in	O
absolute	O
value	O
the	O
observed	O
t	B
t-statistic	O
3.01	O
,	O
giving	O
permutation	O
signiﬁcance	O
level	O
0.0026	O
.	O
(	O
cid:3	O
)	O
-values	O
for	O
testing	B
all	O
vs	O
aml	O
,	O
−4−20240200400600800t*	O
valuesfrequency||||||||||||||||||||||||||−3.013.01originalt−statistic	O
fisherian	O
inference	B
and	O
mle	O
50	O
figure	O
4.3	O
shows	O
the	O
histogram	O
of	O
b	O
d	O
10,000	O
t	B
(	O
cid:3	O
)	O
i	O
values	O
for	O
the	O
gene	O
136	O
data	B
in	O
figure	O
1.3	O
:	O
26	O
of	O
these	O
exceeded	O
t	B
d	O
3:01	O
in	O
absolute	O
value	O
,	O
yielding	O
signiﬁcance	O
level	O
0.0026	O
against	O
the	O
null	O
hypothesis	O
of	O
no	O
all/aml	O
difference	O
,	O
remarkably	O
close	O
to	O
the	O
normal-theory	O
signiﬁcance	O
level	O
0.0025	O
.	O
(	O
we	O
were	O
a	O
little	O
lucky	O
here	O
.	O
)	O
why	O
should	O
we	O
believe	O
the	O
permutation	O
signiﬁcance	O
level	O
(	O
4.42	O
)	O
?	O
fisher	O
provided	O
two	O
arguments	O
.	O
(	O
cid:15	O
)	O
suppose	O
we	O
assume	O
as	O
a	O
null	O
hypothesis	O
that	O
the	O
n	O
d	O
72	O
observed	O
mea-	O
surements	O
x	O
are	O
an	O
iid	O
sample	B
obtained	O
from	O
the	O
same	O
distribution	B
f	O
(	O
cid:22	O
)	O
.x/	O
,	O
iid	O
(	O
cid:24	O
)	O
f	O
(	O
cid:22	O
)	O
.x/	O
xi	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
:	O
(	O
4.43	O
)	O
(	O
there	O
is	O
no	O
normal	B
assumption	O
here	O
,	O
say	O
that	O
f	O
(	O
cid:22	O
)	O
.x/	O
is	O
n	O
.	O
;	O
(	O
cid:27	O
)	O
2/	O
.	O
)	O
let	O
o	O
indicate	O
the	O
order	O
statistic	B
of	O
x	O
,	O
i.e.	O
,	O
the	O
72	O
numbers	O
ordered	O
from	O
smallest	O
to	O
largest	O
,	O
with	O
their	O
aml	O
or	O
all	O
labels	O
removed	O
.	O
then	O
it	O
can	O
be	O
shown	O
that	O
all	O
72š=.47š25š/	O
ways	O
of	O
obtaining	O
x	O
by	O
dividing	O
o	O
into	O
disjoint	O
subsets	O
of	O
sizes	O
47	O
and	O
25	O
are	O
equally	O
likely	O
under	O
null	O
hy-	O
pothesis	O
(	O
4.43	O
)	O
.	O
a	O
small	O
value	O
of	O
the	O
permutation	O
signiﬁcance	O
level	O
(	O
4.42	O
)	O
indicates	O
that	O
the	O
actual	O
division	O
of	O
aml/all	O
measurements	O
was	O
not	O
ran-	O
dom	O
,	O
but	O
rather	O
resulted	O
from	O
negation	O
of	O
the	O
null	O
hypothesis	O
(	O
4.43	O
)	O
.	O
this	O
might	O
be	O
considered	O
an	O
example	O
of	O
fisher	O
’	O
s	O
logic	O
of	O
inductive	O
inference	B
,	O
where	O
the	O
conclusion	O
“	O
should	O
be	O
obvious	O
to	O
all.	O
”	O
it	O
is	O
certainly	O
an	O
exam-	O
ple	O
of	O
conditional	O
inference	B
,	O
now	O
with	O
conditioning	O
used	O
to	O
avoid	O
speciﬁc	O
assumptions	O
about	O
the	O
sampling	O
density	O
f	O
(	O
cid:22	O
)	O
.x/	O
.	O
(	O
cid:15	O
)	O
in	O
experimental	O
situations	O
,	O
fisher	O
forcefully	O
argued	O
for	O
randomization	O
,	O
that	O
is	O
for	O
randomly	O
assigning	O
the	O
experimental	O
units	O
to	O
the	O
possible	O
treat-	O
ment	O
groups	O
.	O
most	O
famously	O
,	O
in	O
a	O
clinical	O
trial	O
comparing	O
drug	O
a	O
with	O
drug	O
b	O
,	O
each	O
patient	O
should	O
be	O
randomly	O
assigned	O
to	O
a	O
or	O
b.	O
randomization	O
greatly	O
strengthens	O
the	O
conclusions	O
of	O
a	O
permutation	O
test	O
.	O
in	O
the	O
aml/all	O
gene-136	O
situation	O
,	O
where	O
randomization	O
wasn	O
’	O
t	B
fea-	O
sible	O
,	O
we	O
wind	O
up	O
almost	O
certain	O
that	O
the	O
aml	O
group	O
has	O
systematically	O
larger	O
numbers	O
,	O
but	O
can	O
not	O
be	O
certain	O
that	O
it	O
is	O
the	O
different	O
disease	O
states	O
causing	O
the	O
difference	O
.	O
perhaps	O
the	O
aml	O
patients	O
are	O
older	O
,	O
or	O
heavier	O
,	O
or	O
have	O
more	O
of	O
some	O
other	O
characteristic	O
affecting	O
gene	O
136.	O
experimen-	O
tal	O
randomization	O
almost	O
guarantees	O
that	O
age	O
,	O
weight	O
,	O
etc.	O
,	O
will	O
be	O
well-	O
balanced	O
between	O
the	O
treatment	O
groups	O
.	O
fisher	O
’	O
s	O
rct	O
(	O
randomized	O
clini-	O
cal	O
trial	O
)	O
was	O
and	O
is	O
the	O
gold	O
standard	O
for	O
statistical	O
inference	B
in	O
medical	O
trials	O
.	O
permutation	O
testing	B
is	O
frequentistic	O
:	O
a	O
statistician	O
following	O
the	O
proce-	O
dure	O
has	O
5	O
%	O
chance	O
of	O
rejecting	O
a	O
valid	O
null	O
hypothesis	O
at	O
level	O
0.05	O
,	O
etc	O
.	O
4.5	O
notes	O
and	O
details	O
51	O
randomization	O
inference	B
is	O
somewhat	O
different	O
,	O
amounting	O
to	O
a	O
kind	O
of	O
forced	O
frequentism	O
,	O
with	O
the	O
statistician	O
imposing	O
his	O
or	O
her	O
preferred	O
prob-	O
ability	O
mechanism	O
upon	O
the	O
data	B
.	O
permutation	O
methods	O
are	O
enjoying	O
a	O
healthy	O
computer-age	O
revival	O
,	O
in	O
contexts	O
far	O
beyond	O
fisher	O
’	O
s	O
original	O
justiﬁcation	O
for	O
the	O
t-test	O
,	O
as	O
we	O
will	O
see	O
in	O
chapter	O
15	O
.	O
4.5	O
notes	O
and	O
details	O
on	O
a	O
linear	B
scale	O
that	O
puts	O
bayesian	O
on	O
the	O
left	O
and	O
frequentist	O
on	O
the	O
right	O
,	O
fisherian	O
inference	B
winds	O
up	O
somewhere	O
in	O
the	O
middle	O
.	O
fisher	O
rejected	O
bayesianism	O
early	O
on	O
,	O
but	O
later	O
criticized	O
as	O
“	O
wooden	O
”	O
the	O
hard-line	O
fre-	O
quentism	O
of	O
the	O
neyman–wald	O
decision-theoretic	O
school	O
.	O
efron	O
(	O
1998	O
)	O
lo-	O
cates	O
fisher	O
along	O
the	O
bayes–frequentist	O
scale	B
for	O
several	O
different	O
criteria	B
;	O
see	O
in	O
particular	O
figure	O
1	O
of	O
that	O
paper	O
.	O
bayesians	O
,	O
of	O
course	O
,	O
believe	O
there	O
is	O
only	O
one	O
true	O
logic	O
of	O
inductive	O
in-	O
ference	O
.	O
fisher	O
disagreed	O
.	O
his	O
most	O
ambitious	O
attempt	O
to	O
“	O
enjoy	O
the	O
bayes-	O
ian	O
omelette	O
without	O
breaking	O
the	O
bayesian	O
eggs	O
”	O
5	O
was	O
ﬁducial	O
inference	B
.	O
the	O
simplest	O
example	O
concerns	O
the	O
normal	B
translation	O
model	B
x	O
(	O
cid:24	O
)	O
n	O
.	O
;	O
1/	O
,	O
where	O
	O
(	O
cid:0	O
)	O
x	O
has	O
a	O
standard	O
n	O
.0	O
;	O
1/	O
distribution	B
,	O
the	O
ﬁducial	O
distribution	B
of	O
	O
given	O
x	O
then	O
being	O
n	O
.x	O
;	O
1/	O
.	O
among	O
fisher	O
’	O
s	O
many	O
contributions	O
,	O
ﬁdu-	O
cial	O
inference	B
was	O
the	O
only	O
outright	O
popular	O
bust	O
.	O
nevertheless	O
the	O
idea	O
has	O
popped	O
up	O
again	O
in	O
the	O
current	O
literature	O
under	O
the	O
name	O
“	O
conﬁdence	O
dis-	O
tribution	O
;	O
”	O
see	O
efron	O
(	O
1993	O
)	O
and	O
xie	O
and	O
singh	O
(	O
2013	O
)	O
.	O
a	O
brief	O
discussion	O
appears	O
in	O
chapter	O
11	O
.	O
1	O
[	O
p.	O
44	O
]	O
for	O
an	O
unbiased	O
estimator	O
q	O
lx.	O
/f	O
.x/	O
d	O
x	O
dz	O
p	O
z	O
t	B
.x/	O
x	O
	O
d	O
t	B
.x/	O
(	O
4.32	O
)	O
,	O
we	O
have	O
z	O
x	O
	O
d	O
1	O
:	O
p	O
f	O
.x/	O
d	O
x	O
d	O
@	O
@	O
	O
d	O
@	O
@	O
	O
t	B
.x/	O
x	O
t	B
.x/f	O
.x/	O
d	O
x	O
(	O
4.44	O
)	O
here	O
x	O
is	O
x	O
n	O
,	O
the	O
sample	B
space	O
of	O
x	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn/	O
,	O
and	O
we	O
are	O
as-	O
(	O
4.44	O
)	O
givesr	O
.t	O
.x/	O
(	O
cid:0	O
)	O
	O
/	O
suming	O
the	O
conditions	B
necessary	O
for	O
differentiating	O
under	O
the	O
integral	O
sign	O
;	O
lx.	O
/	O
has	O
expectation	O
p	O
lx.	O
/f	O
.x/	O
d	O
x	O
d	O
1	O
(	O
since	O
p	O
5	O
attributed	O
to	O
the	O
important	O
bayesian	O
theorist	O
l.	O
j.	O
savage	O
.	O
fisherian	O
inference	B
and	O
mle	O
0	O
)	O
,	O
and	O
then	O
,	O
applying	O
the	O
cauchy–schwarz	O
inequality	O
,	O
52	O
z	O
x	O
or	O
(	O
cid:21	O
)	O
2	O
.t	O
.x/	O
(	O
cid:0	O
)	O
	O
/2	O
f	O
.x/d	O
x	O
p	O
lx.	O
/f	O
.x/	O
d	O
x	O
.t	O
.x/	O
(	O
cid:0	O
)	O
	O
/	O
z	O
x	O
(	O
cid:21	O
)	O
z	O
o	O
n	O
q	O
	O
i	O
:	O
x	O
p	O
lx.	O
/2f	O
.x/	O
d	O
x	O
(	O
cid:21	O
)	O
;	O
(	O
4.45	O
)	O
(	O
4.46	O
)	O
1	O
	O
var	O
this	O
veriﬁes	O
the	O
cram´er–rao	O
lower	O
bound	B
(	O
4.33	O
)	O
:	O
the	O
optimal	O
variance	O
for	O
an	O
unbiased	O
estimator	O
is	O
one	O
over	O
the	O
fisher	O
information	B
.	O
optimality	O
results	O
are	O
a	O
sign	O
of	O
scientiﬁc	O
maturity	O
.	O
fisher	O
information	B
and	O
its	O
estimation	B
bound	O
mark	O
the	O
transition	O
of	O
statistics	B
from	O
a	O
collection	O
of	O
ad-hoc	O
techniques	O
to	O
a	O
coherent	O
discipline	O
.	O
(	O
we	O
have	O
lost	O
some	O
ground	O
recently	O
,	O
where	O
,	O
as	O
discussed	O
in	O
chapter	O
1	O
,	O
ad-hoc	O
algorithmic	O
coinages	O
have	O
outrun	O
their	O
inferential	O
justiﬁcation	O
.	O
)	O
fisher	O
’	O
s	O
information	B
bound	O
was	O
a	O
major	O
mathematical	O
innovation	O
,	O
closely	O
related	O
to	O
and	O
predating	O
,	O
heisen-	O
berg	O
’	O
s	O
uncertainty	O
principle	O
and	O
shannon	O
’	O
s	O
information	B
bound	O
;	O
see	O
dembo	O
et	O
al	O
.	O
(	O
1991	O
)	O
.	O
unbiased	O
estimation	O
has	O
strong	O
appeal	O
in	O
statistical	O
applications	O
,	O
where	O
“	O
biased	O
,	O
”	O
its	O
opposite	O
,	O
carries	O
a	O
hint	O
of	O
self-interested	O
data	B
manipulation	O
.	O
in	O
large-scale	O
settings	O
,	O
such	O
as	O
the	O
prostate	B
study	O
of	O
figure	O
3.4	O
,	O
one	O
can	O
,	O
however	O
,	O
strongly	B
argue	O
for	O
biased	O
estimates	O
.	O
we	O
saw	O
this	O
for	O
gene	O
610	O
,	O
where	O
the	O
usual	O
unbiased	O
estimate	O
o	O
(	O
cid:22	O
)	O
610	O
d	O
5:29	O
is	O
almost	O
certainly	O
too	O
large	O
.	O
biased	O
estimation	B
will	O
play	O
a	O
major	O
role	O
in	O
our	O
subsequent	O
chapters	O
.	O
maximum	B
likelihood	I
estimation	O
is	O
effectively	O
unbiased	O
in	O
most	O
situa-	O
tions	O
.	O
under	O
repeated	O
sampling	O
,	O
the	O
expected	O
mean	O
squared	O
error	O
mse	O
d	O
e	O
	O
(	O
cid:0	O
)	O
	O
(	O
4.47	O
)	O
has	O
order-of-magnitude	O
variance	O
d	O
o.1=n/	O
and	O
bias2	O
d	O
o.1=n2/	O
,	O
the	O
latter	O
usually	O
becoming	O
negligible	O
as	O
sample	B
size	I
n	O
increases	O
.	O
(	O
important	O
exceptions	O
,	O
where	O
bias	O
is	O
substantial	O
,	O
can	O
occur	O
if	O
o	O
	O
d	O
t	B
.	O
o	O
(	O
cid:22	O
)	O
/	O
when	O
o	O
(	O
cid:22	O
)	O
is	O
high-dimensional	O
,	O
as	O
in	O
the	O
james–stein	O
situation	O
of	O
chapter	O
7	O
.	O
)	O
section	O
10	O
of	O
efron	O
(	O
1975	O
)	O
provides	O
a	O
detailed	O
analysis	B
.	O
section	O
9.2	O
of	O
cox	O
and	O
hinkley	O
(	O
1974	O
)	O
gives	O
a	O
careful	O
and	O
wide-ranging	O
account	O
of	O
the	O
mle	O
and	O
fisher	O
information	B
.	O
lehmann	O
(	O
1983	O
)	O
covers	O
the	O
same	O
ground	O
,	O
somewhat	O
more	O
technically	O
,	O
in	O
his	O
chapter	O
6	O
.	O
(	O
cid:26	O
)	O
(	O
cid:16	O
)	O
o	O
2	O
(	O
cid:27	O
)	O
d	O
variance	O
c	O
bias2	O
5	O
parametric	B
models	O
and	O
exponential	O
families	O
we	O
have	O
been	O
reviewing	O
classic	O
approaches	O
to	O
statistical	O
inference—fre-	O
quentist	O
,	O
bayesian	O
,	O
and	O
fisherian—with	O
an	O
eye	O
toward	O
examining	O
their	O
strengths	O
and	O
limitations	O
in	O
modern	O
applications	O
.	O
putting	O
philosophical	O
dif-	O
ferences	O
aside	O
,	O
there	O
is	O
a	O
common	O
methodological	O
theme	O
in	O
classical	O
statis-	O
tics	O
:	O
a	O
strong	O
preference	O
for	O
low-dimensional	O
parametric	B
models	O
;	O
that	O
is	O
,	O
for	O
modeling	O
data-analysis	O
problems	O
using	O
parametric	B
families	O
of	O
probability	O
densities	O
(	O
3.1	O
)	O
,	O
d˚f	O
(	O
cid:22	O
)	O
.x/i	O
x	O
2	O
x	O
;	O
(	O
cid:22	O
)	O
2	O
(	O
cid:127	O
)	O
(	O
cid:9	O
)	O
;	O
f	O
(	O
5.1	O
)	O
where	O
the	O
dimension	O
of	O
parameter	O
(	O
cid:22	O
)	O
is	O
small	O
,	O
perhaps	O
no	O
greater	O
than	O
5	O
or	O
10	O
or	O
20.	O
the	O
inverted	O
nomenclature	O
“	O
nonparametric	B
”	O
suggests	O
the	O
pre-	O
dominance	O
of	O
classical	O
parametric	B
methods	O
.	O
two	O
words	O
explain	O
the	O
classic	O
preference	O
for	O
parametric	B
models	O
:	O
math-	O
ematical	O
tractability	O
.	O
in	O
a	O
world	O
of	O
sliderules	O
and	O
slow	O
mechanical	O
arith-	O
metic	O
,	O
mathematical	O
formulation	O
,	O
by	O
necessity	O
,	O
becomes	O
the	O
computational	O
tool	O
of	O
choice	O
.	O
our	O
new	O
computation-rich	O
environment	O
has	O
unplugged	O
the	O
mathematical	O
bottleneck	O
,	O
giving	O
us	O
a	O
more	O
realistic	O
,	O
ﬂexible	O
,	O
and	O
far-reach-	O
ing	O
body	O
of	O
statistical	O
techniques	O
.	O
but	O
the	O
classic	O
parametric	B
families	O
still	O
play	O
an	O
important	O
role	O
in	O
computer-age	O
statistics	B
,	O
often	O
assembled	O
as	O
small	O
parts	O
of	O
larger	O
methodologies	O
(	O
as	O
with	O
the	O
generalized	O
linear	B
models	O
of	O
chapter	O
8	O
)	O
.	O
this	O
chapter1	O
presents	O
a	O
brief	O
review	O
of	O
the	O
most	O
widely	O
used	O
parametric	B
models	O
,	O
ending	O
with	O
an	O
overview	O
of	O
exponential	O
families	O
,	O
the	O
great	O
connecting	O
thread	O
of	O
classical	O
theory	B
and	O
a	O
player	O
of	O
continuing	O
im-	O
portance	O
in	O
computer-age	O
applications	O
.	O
1	O
this	O
chapter	O
covers	O
a	O
large	O
amount	O
of	O
technical	O
material	O
for	O
use	O
later	O
,	O
and	O
may	O
be	O
reviewed	O
lightly	O
at	O
ﬁrst	O
reading	O
.	O
53	O
54	O
parametric	B
models	O
5.1	O
univariate	O
families	O
univariate	O
parametric	B
families	O
,	O
in	O
which	O
the	O
sample	B
space	O
x	O
of	O
observation	O
x	O
is	O
a	O
subset	O
of	O
the	O
real	O
line	O
r1	O
,	O
are	O
the	O
building	O
blocks	O
of	O
most	O
statistical	O
analyses	O
.	O
table	O
5.1	O
names	O
and	O
describes	O
the	O
ﬁve	O
most	O
familiar	O
univariate	O
families	O
:	O
normal	B
,	O
poisson	O
,	O
binomial	B
,	O
gamma	B
,	O
and	O
beta	O
.	O
(	O
the	O
chi-squared	O
distribution	B
with	O
n	O
degrees	O
of	O
freedom	O
(	O
cid:31	O
)	O
2	O
n	O
is	O
also	O
included	O
since	O
it	O
is	O
dis-	O
tributed	O
as	O
2	O
(	O
cid:1	O
)	O
gam.n=2	O
;	O
1/	O
.	O
)	O
the	O
normal	B
distribution	O
n	O
.	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
2/	O
is	O
a	O
shifted	O
and	O
scaled	O
version	O
of	O
the	O
n	O
.0	O
;	O
1/	O
distribution2	O
used	O
in	O
(	O
3.27	O
)	O
,	O
n	O
.	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
2/	O
(	O
cid:24	O
)	O
(	O
cid:22	O
)	O
c	O
(	O
cid:27	O
)	O
n	O
.0	O
;	O
1/	O
:	O
(	O
5.2	O
)	O
table	O
5.1	O
five	O
familiar	O
univariate	O
densities	O
,	O
and	O
their	O
sample	B
spaces	O
x	O
,	O
parameter	O
spaces	O
(	O
cid:127	O
)	O
,	O
and	O
expectations	O
and	O
variances	O
;	O
chi-squared	O
distribution	B
with	O
n	O
degrees	O
of	O
freedom	O
is	O
2	O
gam.n=2	O
;	O
1/	O
.	O
name	O
,	O
notation	O
expectation	O
,	O
variance	O
density	B
x	O
normal	B
n	O
.	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
2/	O
poisson	O
poi	O
.	O
(	O
cid:22	O
)	O
/	O
binomial	B
bi.n	O
;	O
(	O
cid:25	O
)	O
/	O
gamma	B
gam	O
.	O
(	O
cid:23	O
)	O
;	O
(	O
cid:27	O
)	O
/	O
beta	B
be	O
.	O
(	O
cid:23	O
)	O
1	O
;	O
(	O
cid:23	O
)	O
2/	O
p	O
1	O
(	O
cid:27	O
)	O
2	O
(	O
cid:25	O
)	O
2	O
.	O
x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
(	O
cid:0	O
)	O
1	O
(	O
cid:27	O
)	O
/2	O
e	O
r1	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
(	O
cid:22	O
)	O
x	O
e	O
xš	O
f0	O
;	O
1	O
;	O
:	O
:	O
:	O
g	O
(	O
cid:22	O
)	O
>	O
0	O
xš.n	O
(	O
cid:0	O
)	O
x/š	O
(	O
cid:25	O
)	O
x.1	O
(	O
cid:0	O
)	O
(	O
cid:25	O
)	O
/n	O
(	O
cid:0	O
)	O
x	O
nš	O
f0	O
;	O
1	O
;	O
:	O
:	O
:	O
;	O
ng	O
0	O
<	O
(	O
cid:25	O
)	O
<	O
1	O
x	O
(	O
cid:23	O
)	O
(	O
cid:0	O
)	O
1e	O
(	O
cid:0	O
)	O
x=	O
(	O
cid:27	O
)	O
(	O
cid:27	O
)	O
(	O
cid:23	O
)	O
	O
.	O
(	O
cid:23	O
)	O
/	O
x	O
(	O
cid:21	O
)	O
0	O
.	O
(	O
cid:23	O
)	O
1/	O
.	O
(	O
cid:23	O
)	O
2/	O
x	O
(	O
cid:23	O
)	O
1	O
(	O
cid:0	O
)	O
1.1	O
(	O
cid:0	O
)	O
x/	O
(	O
cid:23	O
)	O
2	O
(	O
cid:0	O
)	O
1	O
0	O
	O
x	O
	O
1	O
	O
.	O
(	O
cid:23	O
)	O
1c	O
(	O
cid:23	O
)	O
2/	O
(	O
cid:23	O
)	O
>	O
0	O
(	O
cid:27	O
)	O
>	O
0	O
(	O
cid:23	O
)	O
1	O
>	O
0	O
(	O
cid:23	O
)	O
2	O
>	O
0	O
(	O
cid:127	O
)	O
(	O
cid:22	O
)	O
2	O
r1	O
(	O
cid:27	O
)	O
2	O
>	O
0	O
(	O
cid:22	O
)	O
(	O
cid:27	O
)	O
2	O
(	O
cid:22	O
)	O
(	O
cid:22	O
)	O
n	O
(	O
cid:25	O
)	O
n	O
(	O
cid:25	O
)	O
.1	O
(	O
cid:0	O
)	O
(	O
cid:25	O
)	O
/	O
(	O
cid:27	O
)	O
(	O
cid:23	O
)	O
(	O
cid:27	O
)	O
2	O
(	O
cid:23	O
)	O
(	O
cid:23	O
)	O
1=	O
.	O
(	O
cid:23	O
)	O
1	O
c	O
(	O
cid:23	O
)	O
2/	O
.	O
(	O
cid:23	O
)	O
1c	O
(	O
cid:23	O
)	O
2/2	O
.	O
(	O
cid:23	O
)	O
1c	O
(	O
cid:23	O
)	O
2c1/	O
(	O
cid:23	O
)	O
1	O
(	O
cid:23	O
)	O
2	O
relationships	O
abound	O
among	O
the	O
table	O
’	O
s	O
families	O
.	O
for	O
instance	O
,	O
indepen-	O
dent	O
gamma	B
variables	O
gam	O
.	O
(	O
cid:23	O
)	O
1	O
;	O
(	O
cid:27	O
)	O
/	O
and	O
gam	O
.	O
(	O
cid:23	O
)	O
2	O
;	O
(	O
cid:27	O
)	O
/	O
yield	O
a	O
beta	B
variate	O
ac-	O
cording	O
to	O
be	O
.	O
(	O
cid:23	O
)	O
1	O
;	O
(	O
cid:23	O
)	O
2/	O
(	O
cid:24	O
)	O
gam	O
.	O
(	O
cid:23	O
)	O
1	O
;	O
(	O
cid:27	O
)	O
/	O
gam	O
.	O
(	O
cid:23	O
)	O
1	O
;	O
(	O
cid:27	O
)	O
/	O
c	O
gam	O
.	O
(	O
cid:23	O
)	O
2	O
;	O
(	O
cid:27	O
)	O
/	O
:	O
(	O
5.3	O
)	O
the	O
binomial	B
and	O
poisson	O
are	O
particularly	O
close	O
cousins	O
.	O
a	O
bi.n	O
;	O
(	O
cid:25	O
)	O
/	O
distri-	O
bution	O
(	O
the	O
number	O
of	O
heads	O
in	O
n	O
independent	O
ﬂips	O
of	O
a	O
coin	O
with	O
probabil-	O
2	O
the	O
notation	O
in	O
(	O
5.2	O
)	O
indicates	O
that	O
if	O
x	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
2/	O
and	O
y	O
(	O
cid:24	O
)	O
n	O
.0	O
;	O
1/	O
then	O
x	O
and	O
(	O
cid:22	O
)	O
c	O
(	O
cid:27	O
)	O
y	O
have	O
the	O
same	O
distribution	B
.	O
5.2	O
the	O
multivariate	B
normal	O
distribution	B
55	O
figure	O
5.1	O
comparison	O
of	O
the	O
binomial	B
distribution	O
bi.30	O
;	O
0:2/	O
(	O
black	O
lines	O
)	O
with	O
the	O
poisson	O
poi.6/	O
(	O
red	O
dots	O
)	O
.	O
in	O
the	O
legend	O
we	O
show	O
the	O
mean	O
and	O
standard	O
deviation	O
for	O
each	O
distribution	B
.	O
ity	O
of	O
heads	O
(	O
cid:25	O
)	O
)	O
approaches	O
a	O
poi.n	O
(	O
cid:25	O
)	O
/	O
distribution	B
,	O
bi.n	O
;	O
(	O
cid:25	O
)	O
/	O
p	O
(	O
cid:24	O
)	O
poi.n	O
(	O
cid:25	O
)	O
/	O
(	O
5.4	O
)	O
as	O
n	O
grows	O
large	O
and	O
(	O
cid:25	O
)	O
small	O
,	O
the	O
notation	O
p	O
(	O
cid:24	O
)	O
indicating	O
approximate	O
equal-	O
ity	O
of	O
the	O
two	O
distributions	O
.	O
figure	O
5.1	O
shows	O
the	O
approximation	O
already	O
working	O
quite	O
effectively	O
for	O
n	O
d	O
30	O
and	O
(	O
cid:25	O
)	O
d	O
0:2.	O
the	O
ﬁve	O
families	O
in	O
table	O
5.1	O
have	O
ﬁve	O
different	O
sample	B
spaces	O
,	O
making	O
them	O
appropriate	O
in	O
different	O
situations	O
.	O
beta	B
distributions	O
,	O
for	O
example	O
,	O
are	O
natural	O
candidates	O
for	O
modeling	O
continuous	O
data	B
on	O
the	O
unit	O
interval	B
œ0	O
;	O
1	O
.	O
choices	O
of	O
the	O
two	O
parameters	O
.	O
(	O
cid:23	O
)	O
1	O
;	O
(	O
cid:23	O
)	O
2/	O
provide	O
a	O
variety	O
of	O
possible	O
shapes	O
,	O
as	O
illustrated	O
in	O
figure	O
5.2.	O
later	O
we	O
will	O
discuss	O
general	O
exponen-	O
tial	O
families	O
,	O
unavailable	O
in	O
classical	O
theory	B
,	O
that	O
greatly	O
expand	O
the	O
catalog	O
of	O
possible	O
shapes	O
.	O
5.2	O
the	O
multivariate	B
normal	O
distribution	B
classical	O
statistics	B
produced	O
a	O
less	O
rich	O
catalog	O
of	O
multivariate	B
distribu-	O
tions	O
,	O
ones	O
where	O
the	O
sample	B
space	O
x	O
exists	O
in	O
rp	O
,	O
p-dimensional	O
eu-	O
0.000.050.100.150.20xf	O
(	O
x	O
)	O
llllllllllllllllll01234567891011121314151617lbinomial	O
:	O
(	O
6	O
,	O
2.19	O
)	O
poisson	O
:	O
(	O
6	O
,	O
2.45	O
)	O
56	O
parametric	B
models	O
figure	O
5.2	O
three	O
beta	B
densities	O
,	O
with	O
.	O
(	O
cid:23	O
)	O
1	O
;	O
(	O
cid:23	O
)	O
2/	O
indicated	O
.	O
clidean	O
space	B
,	O
p	O
>	O
1.	O
by	O
far	O
the	O
greatest	O
amount	O
of	O
attention	O
focused	O
on	O
the	O
multivariate	B
normal	O
distribution	B
.	O
a	O
random	O
vector	B
x	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xp/	O
mean	O
vector	B
;	O
normally	O
distributed	O
or	O
not	O
,	O
has	O
0	O
(	O
cid:22	O
)	O
d	O
efxg	O
d	O
(	O
cid:0	O
)	O
efx1g	O
;	O
efx2g	O
;	O
:	O
:	O
:	O
;	O
efxpg	O
(	O
cid:1	O
)	O
0	O
0	O
(	O
cid:9	O
)	O
d	O
(	O
cid:0	O
)	O
e˚.xi	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
i	O
/.xj	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
j	O
/	O
(	O
cid:9	O
)	O
(	O
cid:1	O
)	O
:	O
and	O
p	O
(	O
cid:2	O
)	O
p	O
covariance	O
matrix3	O
†	O
d	O
e˚.x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
/.x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
/	O
(	O
5.6	O
)	O
0	O
of	O
vectors	O
u	O
and	O
v	O
is	O
the	O
matrix	B
having	O
elements	O
(	O
5.5	O
)	O
(	O
the	O
outer	O
product	O
uv	O
ui	O
vj	O
.	O
)	O
we	O
will	O
use	O
the	O
convenient	O
notation	O
x	O
(	O
cid:24	O
)	O
.	O
(	O
cid:22	O
)	O
;	O
†/	O
(	O
5.7	O
)	O
for	O
(	O
5.5	O
)	O
and	O
(	O
5.6	O
)	O
,	O
reducing	O
to	O
the	O
familiar	O
form	B
x	O
(	O
cid:24	O
)	O
.	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
2/	O
in	O
the	O
uni-	O
variate	O
case	O
.	O
denoting	O
the	O
entries	O
of	O
†	O
by	O
(	O
cid:27	O
)	O
ij	O
,	O
for	O
i	O
and	O
j	O
equaling	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
p	O
,	O
the	O
diagonal	O
elements	O
are	O
variances	O
,	O
(	O
cid:27	O
)	O
i	O
i	O
d	O
var.xi	O
/	O
:	O
(	O
5.8	O
)	O
3	O
the	O
notation	O
†	O
d	O
.	O
(	O
cid:27	O
)	O
ij	O
/	O
deﬁnes	O
the	O
ij	O
th	O
element	O
of	O
a	O
matrix	B
.	O
0.00.20.40.60.81.00.00.51.01.52.02.53.0xf	O
(	O
x	O
)	O
(	O
n1	O
,	O
n2	O
)	O
(	O
8	O
,	O
4	O
)	O
(	O
2	O
,	O
4	O
)	O
(	O
.5	O
,	O
.5	O
)	O
5.2	O
the	O
multivariate	B
normal	O
distribution	B
57	O
the	O
off-diagonal	O
elements	O
relate	O
to	O
the	O
correlations	O
between	O
the	O
coordi-	O
nates	O
of	O
x	O
,	O
cor.xi	O
;	O
xj	O
/	O
d	O
(	O
cid:27	O
)	O
ijp	O
(	O
cid:27	O
)	O
i	O
i	O
(	O
cid:27	O
)	O
jj	O
:	O
(	O
5.9	O
)	O
the	O
multivariate	B
normal	O
distribution	B
extends	O
the	O
univariate	O
deﬁnition	O
0	O
be	O
a	O
vector	B
n	O
.	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
2/	O
in	O
table	O
5.1.	O
to	O
begin	O
with	O
,	O
let	O
z	O
d	O
.z1	O
;	O
z2	O
;	O
:	O
:	O
:	O
;	O
zp/	O
of	O
p	O
independent	O
n	O
.0	O
;	O
1/	O
variates	O
,	O
with	O
probability	O
density	B
function	O
f	O
.z/	O
d	O
.2	O
(	O
cid:25	O
)	O
/	O
(	O
cid:0	O
)	O
p	O
2	O
e	O
(	O
cid:0	O
)	O
1	O
2	O
i	O
d	O
.2	O
(	O
cid:25	O
)	O
/	O
(	O
cid:0	O
)	O
p	O
2	O
e	O
(	O
cid:0	O
)	O
1	O
2	O
z	O
0	O
z	O
(	O
5.10	O
)	O
pp	O
1	O
z2	O
according	O
to	O
line	O
1	O
of	O
table	O
5.1.	O
the	O
multivariate	B
normal	O
family	O
is	O
obtained	O
by	O
linear	B
transformations	O
of	O
z	O
:	O
let	O
(	O
cid:22	O
)	O
be	O
a	O
p-dimensional	O
vector	B
and	O
t	B
a	O
p	O
(	O
cid:2	O
)	O
p	O
nonsingular	O
matrix	B
,	O
and	O
deﬁne	O
the	O
random	O
vector	B
following	O
the	O
usual	O
rules	O
of	O
probability	O
transformations	O
yields	O
the	O
density	B
of	O
x	O
,	O
f	O
(	O
cid:22	O
)	O
;	O
†.x/	O
d	O
.2	O
(	O
cid:25	O
)	O
/	O
(	O
cid:0	O
)	O
1.x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
/	O
;	O
where	O
†	O
is	O
the	O
p	O
(	O
cid:2	O
)	O
p	O
symmetric	O
positive	O
deﬁnite	O
matrix	B
(	O
cid:0	O
)	O
1	O
2	O
.x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
/	O
0	O
e	O
†	O
(	O
cid:0	O
)	O
p=2	O
j†j1=2	O
and	O
j†j	O
its	O
determinant	O
;	O
	O
f	O
(	O
cid:22	O
)	O
;	O
†.x/	O
,	O
the	O
p-dimensional	O
multivariate	B
normal	O
1	O
distribution	B
with	O
mean	O
(	O
cid:22	O
)	O
and	O
covariance	O
†	O
,	O
is	O
denoted	O
x	O
(	O
cid:24	O
)	O
np	O
.	O
(	O
cid:22	O
)	O
;	O
†/	O
:	O
(	O
5.14	O
)	O
figure	O
5.3	O
illustrates	O
the	O
bivariate	O
normal	B
distribution	O
with	O
(	O
cid:22	O
)	O
d	O
.0	O
;	O
0/	O
0	O
and	O
†	O
having	O
(	O
cid:27	O
)	O
11	O
d	O
(	O
cid:27	O
)	O
22	O
d	O
1	O
and	O
(	O
cid:27	O
)	O
12	O
d	O
0:5	O
(	O
so	O
cor.x1	O
;	O
x2/	O
d	O
0:5	O
)	O
.	O
the	O
bell-shaped	O
mountain	O
on	O
the	O
left	O
is	O
a	O
plot	O
of	O
density	B
(	O
5.12	O
)	O
.	O
the	O
right	O
panel	O
shows	O
a	O
scatterplot	O
of	O
2000	O
points	O
drawn	O
from	O
this	O
distribution	B
.	O
concentric	O
ellipses	O
illustrate	O
curves	O
of	O
constant	O
density	B
,	O
.x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
/	O
0	O
(	O
cid:0	O
)	O
1.x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
/	O
d	O
constant	O
:	O
†	O
(	O
5.15	O
)	O
classical	O
multivariate	B
analysis	O
was	O
the	O
study	O
of	O
the	O
multivariate	B
normal	O
distribution	B
,	O
both	O
of	O
its	O
probabilistic	O
and	O
statistical	O
properties	O
.	O
the	O
notes	O
reference	O
some	O
important	O
(	O
and	O
lengthy	O
)	O
multivariate	B
texts	O
.	O
here	O
we	O
will	O
just	O
recall	O
a	O
couple	O
of	O
results	O
useful	O
in	O
the	O
chapters	O
to	O
follow	O
.	O
x	O
d	O
(	O
cid:22	O
)	O
c	O
t	B
z	O
:	O
†	O
d	O
t	B
t	O
0	O
(	O
5.11	O
)	O
(	O
5.12	O
)	O
(	O
5.13	O
)	O
58	O
parametric	B
models	O
figure	O
5.3	O
left	O
:	O
bivariate	O
normal	B
density	O
,	O
with	O
var.x1/	O
d	O
var.x2/	O
d	O
1	O
and	O
cor.x1	O
;	O
x2/	O
d	O
0:5.	O
right	O
:	O
sample	B
of	O
2000	O
.x1	O
;	O
x2/	O
pairs	O
from	O
this	O
bivariate	O
normal	B
density	O
.	O
0	O
;	O
(	O
5.16	O
)	O
0	O
is	O
partitioned	O
into	O
suppose	O
that	O
x	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xp/	O
x.1/	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xp1	O
/	O
and	O
x.2/	O
d	O
.xp1c1	O
;	O
xp1c2	O
;	O
:	O
:	O
:	O
;	O
xp1cp2	O
/	O
!	O
p1	O
c	O
p2	O
d	O
p	O
,	O
with	O
(	O
cid:22	O
)	O
and	O
†	O
similarly	O
partitioned	O
,	O
!	O
†11	O
†12	O
	O
!	O
0	O
np	O
†21	O
†22	O
(	O
5.17	O
)	O
(	O
so	O
†11	O
is	O
p1	O
(	O
cid:2	O
)	O
p1	O
,	O
†12	O
is	O
p1	O
(	O
cid:2	O
)	O
p2	O
,	O
etc.	O
)	O
.	O
then	O
the	O
conditional	O
distribution	B
of	O
x.2/	O
given	O
x.1/	O
is	O
itself	O
normal	B
,	O
	O
x.2/jx.1/	O
(	O
cid:24	O
)	O
11	O
.x.1/	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
.1//	O
;	O
†22	O
(	O
cid:0	O
)	O
†21†	O
(	O
cid:0	O
)	O
1	O
if	O
p1	O
d	O
p2	O
d	O
1	O
,	O
then	O
(	O
5.18	O
)	O
reduces	O
to	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
.2/	O
c	O
†21†	O
(	O
cid:0	O
)	O
1	O
11	O
†12	O
(	O
cid:1	O
)	O
:	O
np2	O
(	O
5.18	O
)	O
x.1/	O
x.2/	O
(	O
cid:22	O
)	O
.1/	O
(	O
cid:22	O
)	O
.2/	O
;	O
(	O
cid:24	O
)	O
2	O
x2jx1	O
(	O
cid:24	O
)	O
n	O
.x1	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
1/	O
;	O
(	O
cid:27	O
)	O
22	O
(	O
cid:0	O
)	O
(	O
cid:27	O
)	O
2	O
12	O
(	O
cid:27	O
)	O
11	O
(	O
5.19	O
)	O
here	O
(	O
cid:27	O
)	O
12=	O
(	O
cid:27	O
)	O
11	O
is	O
familiar	O
as	O
the	O
linear	B
regression	O
coefﬁcient	O
of	O
x2	O
as	O
a	O
func-	O
12=	O
(	O
cid:27	O
)	O
11	O
(	O
cid:27	O
)	O
22	O
equals	O
cor.x1	O
;	O
x2/2	O
,	O
the	O
squared	O
proportion	B
tion	O
of	O
x1	O
,	O
while	O
(	O
cid:27	O
)	O
2	O
r2	O
of	O
the	O
variance	O
of	O
x2	O
explained	O
by	O
x1	O
.	O
hence	O
we	O
can	O
write	O
the	O
(	O
unex-	O
plained	O
)	O
variance	O
term	O
in	O
(	O
5.19	O
)	O
as	O
(	O
cid:27	O
)	O
22.1	O
(	O
cid:0	O
)	O
r2/	O
.	O
bayesian	O
statistics	B
also	O
makes	O
good	O
use	O
of	O
the	O
normal	B
family	O
.	O
it	O
helps	O
to	O
begin	O
with	O
the	O
univariate	O
case	O
x	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
2/	O
,	O
where	O
now	O
we	O
assume	O
that	O
	O
(	O
cid:22	O
)	O
2	O
c	O
(	O
cid:27	O
)	O
12	O
(	O
cid:27	O
)	O
11	O
i	O
****************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************−2−1012−2−1012x1x1x2x2	O
5.3	O
fisher	O
’	O
s	O
information	B
bound	O
59	O
the	O
expectation	O
vector	B
itself	O
has	O
a	O
normal	B
prior	O
distribution	B
n	O
.m	O
;	O
a/	O
:	O
(	O
cid:22	O
)	O
(	O
cid:24	O
)	O
n	O
.m	O
;	O
a/	O
and	O
xj	O
(	O
cid:22	O
)	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
2/	O
:	O
(	O
5.20	O
)	O
bayes	O
’	O
theorem	B
and	O
some	O
algebra	O
show	O
that	O
the	O
posterior	B
distribution	I
of	O
(	O
cid:22	O
)	O
having	O
observed	O
x	O
is	O
normal	B
,	O
	O
3	O
	O
m	O
c	O
a	O
	O
(	O
cid:22	O
)	O
jx	O
(	O
cid:24	O
)	O
.x	O
(	O
cid:0	O
)	O
m	O
/	O
;	O
a	O
(	O
cid:27	O
)	O
2	O
a	O
c	O
(	O
cid:27	O
)	O
2	O
n	O
a	O
c	O
(	O
cid:27	O
)	O
2	O
(	O
5.21	O
)	O
the	O
posterior	O
expectation	O
o	O
(	O
cid:22	O
)	O
bayes	O
d	O
m	O
c	O
.a=.ac	O
(	O
cid:27	O
)	O
2//.x	O
(	O
cid:0	O
)	O
m	O
/	O
is	O
a	O
shrink-	O
age	O
estimator	B
of	O
(	O
cid:22	O
)	O
:	O
if	O
,	O
say	O
,	O
a	O
equals	O
(	O
cid:27	O
)	O
2	O
,	O
then	O
o	O
(	O
cid:22	O
)	O
bayes	O
d	O
m	O
c	O
.x	O
(	O
cid:0	O
)	O
m	O
/=2	O
is	O
shrunk	O
half	O
the	O
way	O
back	O
from	O
the	O
unbiased	O
estimate	O
o	O
(	O
cid:22	O
)	O
d	O
x	O
toward	O
the	O
prior	B
mean	O
m	O
,	O
while	O
the	O
posterior	O
variance	O
(	O
cid:27	O
)	O
2=2	O
of	O
o	O
(	O
cid:22	O
)	O
bayes	O
is	O
only	O
one-half	O
that	O
of	O
o	O
(	O
cid:22	O
)	O
.	O
:	O
the	O
multivariate	B
version	O
of	O
the	O
bayesian	O
setup	O
(	O
5.20	O
)	O
is	O
(	O
cid:22	O
)	O
(	O
cid:24	O
)	O
and	O
xj	O
(	O
cid:22	O
)	O
(	O
cid:24	O
)	O
(	O
5.22	O
)	O
now	O
with	O
m	O
and	O
(	O
cid:22	O
)	O
p-vectors	O
,	O
and	O
a	O
and	O
†	O
positive	O
deﬁnite	O
p	O
(	O
cid:2	O
)	O
p	O
matrices	O
.	O
as	O
indicated	O
in	O
the	O
notes	O
,	O
the	O
posterior	B
distribution	I
of	O
(	O
cid:22	O
)	O
given	O
x	O
is	O
then	O
np	O
.	O
(	O
cid:22	O
)	O
;	O
†/	O
;	O
np.m	O
;	O
a/	O
(	O
cid:0	O
)	O
m	O
c	O
a.a	O
c	O
†/	O
(	O
cid:22	O
)	O
jx	O
(	O
cid:24	O
)	O
np	O
which	O
reduces	O
to	O
(	O
5.21	O
)	O
when	O
p	O
d	O
1	O
.	O
(	O
cid:0	O
)	O
1.x	O
(	O
cid:0	O
)	O
m	O
/	O
;	O
a.a	O
c	O
†/	O
(	O
5.23	O
)	O
(	O
cid:0	O
)	O
1†	O
(	O
cid:1	O
)	O
;	O
5.3	O
fisher	O
’	O
s	O
information	B
bound	O
for	O
multiparameter	O
families	O
the	O
multivariate	B
normal	O
distribution	B
plays	O
its	O
biggest	O
role	O
in	O
applications	O
as	O
a	O
large-sample	O
approximation	O
for	O
maximum	B
likelihood	I
estimates	O
.	O
we	O
suppose	O
that	O
the	O
parametric	B
family	O
of	O
densities	O
ff	O
(	O
cid:22	O
)	O
.x/g	O
,	O
normal	B
or	O
not	O
,	O
is	O
smoothly	O
deﬁned	O
in	O
terms	O
of	O
its	O
p-dimensional	O
parameter	O
vector	B
(	O
cid:22	O
)	O
.	O
(	O
in	O
terms	O
of	O
(	O
5.1	O
)	O
,	O
(	O
cid:127	O
)	O
is	O
a	O
subset	O
of	O
rp	O
.	O
)	O
the	O
mle	O
deﬁnitions	O
and	O
results	O
are	O
direct	O
analogues	O
of	O
the	O
single-param-	O
eter	O
calculations	O
beginning	O
at	O
(	O
4.14	O
)	O
in	O
chapter	O
4.	O
the	O
score	O
function	B
p	O
lx	O
.	O
(	O
cid:22	O
)	O
/	O
is	O
now	O
deﬁned	O
as	O
the	O
gradient	O
of	O
logff	O
(	O
cid:22	O
)	O
.x/g	O
,	O
0	O
˚log	O
f	O
(	O
cid:22	O
)	O
.x/	O
(	O
cid:9	O
)	O
d	O
np	O
e	O
(	O
cid:22	O
)	O
lx	O
.	O
(	O
cid:22	O
)	O
/	O
:	O
:	O
:	O
;	O
@	O
log	O
f	O
(	O
cid:22	O
)	O
.x/	O
@	O
(	O
cid:22	O
)	O
i	O
o	O
d	O
0	O
d	O
.0	O
;	O
0	O
;	O
0	O
;	O
:	O
:	O
:	O
;	O
0/	O
0	O
;	O
:	O
:	O
:	O
;	O
(	O
5.24	O
)	O
:	O
(	O
5.25	O
)	O
the	O
p-vector	O
of	O
partial	O
derivatives	O
of	O
log	O
f	O
(	O
cid:22	O
)	O
.x/	O
with	O
respect	O
to	O
the	O
coordi-	O
nates	O
of	O
(	O
cid:22	O
)	O
.	O
it	O
has	O
mean	O
zero	O
,	O
p	O
lx	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
r	O
(	O
cid:22	O
)	O
parametric	B
models	O
lx	O
.	O
(	O
cid:22	O
)	O
/	O
;	O
using	O
outer	O
product	O
notation	O
,	O
60	O
by	O
deﬁnition	O
,	O
the	O
fisher	O
information	B
matrix	O
i	O
(	O
cid:22	O
)	O
for	O
(	O
cid:22	O
)	O
is	O
the	O
p	O
(	O
cid:2	O
)	O
p	O
covari-	O
ance	O
matrix	B
of	O
p	O
np	O
i	O
(	O
cid:22	O
)	O
d	O
e	O
(	O
cid:22	O
)	O
(	O
5.26	O
)	O
the	O
key	O
result	O
is	O
that	O
the	O
mle	O
o	O
(	O
cid:22	O
)	O
d	O
arg	O
max	O
(	O
cid:22	O
)	O
ff	O
(	O
cid:22	O
)	O
.x/g	O
has	O
an	O
approxi-	O
mately	O
normal	B
distribution	O
with	O
covariance	O
matrix	B
i	O
(	O
cid:0	O
)	O
1	O
(	O
cid:22	O
)	O
,	O
(	O
cid:26	O
)	O
@	O
log	O
f	O
(	O
cid:22	O
)	O
.x/	O
0o	O
d	O
@	O
log	O
f	O
(	O
cid:22	O
)	O
.x/	O
p	O
lx	O
.	O
(	O
cid:22	O
)	O
/	O
(	O
cid:27	O
)	O
	O
lx	O
.	O
(	O
cid:22	O
)	O
/	O
@	O
(	O
cid:22	O
)	O
j	O
@	O
(	O
cid:22	O
)	O
i	O
e	O
(	O
cid:22	O
)	O
:	O
o	O
(	O
cid:22	O
)	O
p	O
(	O
cid:24	O
)	O
np	O
.	O
(	O
cid:22	O
)	O
;	O
i	O
(	O
cid:0	O
)	O
1	O
(	O
cid:22	O
)	O
/	O
:	O
(	O
5.27	O
)	O
approximation	O
(	O
5.27	O
)	O
is	O
justiﬁed	O
by	O
large-sample	O
arguments	O
,	O
say	O
with	O
x	O
an	O
iid	O
sample	B
in	O
rp	O
,	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn/	O
,	O
n	O
going	O
to	O
inﬁnity	O
.	O
suppose	O
the	O
statistician	O
is	O
particularly	O
interested	O
in	O
(	O
cid:22	O
)	O
1	O
,	O
the	O
ﬁrst	O
coordi-	O
nate	O
of	O
(	O
cid:22	O
)	O
.	O
let	O
(	O
cid:22	O
)	O
.2/	O
d	O
.	O
(	O
cid:22	O
)	O
2	O
;	O
(	O
cid:22	O
)	O
3	O
;	O
:	O
:	O
:	O
;	O
(	O
cid:22	O
)	O
p/	O
denote	O
the	O
other	O
p	O
(	O
cid:0	O
)	O
1	O
coordinates	O
of	O
(	O
cid:22	O
)	O
,	O
which	O
are	O
now	O
“	O
nuisance	O
parameters	O
”	O
as	O
far	O
as	O
the	O
estimation	B
of	O
(	O
cid:22	O
)	O
1	O
goes	O
.	O
according	O
to	O
(	O
5.27	O
)	O
,	O
the	O
mle	O
o	O
(	O
cid:22	O
)	O
1	O
,	O
which	O
is	O
the	O
ﬁrst	O
coordinate	O
of	O
o	O
(	O
cid:22	O
)	O
,	O
has	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
1	O
;	O
.i	O
(	O
cid:0	O
)	O
1	O
(	O
cid:22	O
)	O
/11	O
o	O
(	O
cid:22	O
)	O
1	O
p	O
(	O
cid:24	O
)	O
n	O
(	O
5.28	O
)	O
(	O
cid:1	O
)	O
;	O
	O
where	O
the	O
notation	O
indicates	O
the	O
upper	O
leftmost	O
entry	O
of	O
i	O
(	O
cid:0	O
)	O
1	O
(	O
cid:22	O
)	O
.	O
we	O
can	O
partition	O
the	O
information	B
matrix	O
i	O
(	O
cid:22	O
)	O
into	O
the	O
two	O
parts	O
corre-	O
sponding	O
to	O
(	O
cid:22	O
)	O
1	O
and	O
(	O
cid:22	O
)	O
.2/	O
,	O
i	O
(	O
cid:22	O
)	O
d	O
(	O
cid:22	O
)	O
/11	O
d	O
(	O
cid:0	O
)	O
i	O
(	O
cid:22	O
)	O
11	O
i	O
(	O
cid:22	O
)	O
1.2/	O
i	O
(	O
cid:22	O
)	O
.2/1	O
i	O
(	O
cid:22	O
)	O
.22/	O
(	O
5.29	O
)	O
(	O
cid:22	O
)	O
.2/1	O
of	O
dimension	O
1	O
(	O
cid:2	O
)	O
.p	O
(	O
cid:0	O
)	O
1/	O
and	O
i	O
(	O
cid:22	O
)	O
.22/	O
.p	O
(	O
cid:0	O
)	O
1/	O
(	O
cid:2	O
)	O
.p	O
(	O
cid:0	O
)	O
1/	O
)	O
.	O
0	O
i	O
.i	O
(	O
cid:0	O
)	O
1	O
(	O
cid:1	O
)	O
(	O
cid:0	O
)	O
1	O
(	O
5.30	O
)	O
:	O
i	O
(	O
cid:22	O
)	O
11	O
(	O
cid:0	O
)	O
i	O
(	O
cid:22	O
)	O
1.2/i	O
(	O
cid:0	O
)	O
1	O
(	O
cid:22	O
)	O
.22/i	O
(	O
cid:22	O
)	O
.2/1	O
(	O
with	O
i	O
(	O
cid:22	O
)	O
1.2/	O
d	O
4	O
the	O
endnotes	O
show	O
that	O
the	O
subtracted	O
term	O
on	O
the	O
right	O
side	O
of	O
(	O
5.30	O
)	O
is	O
nonnegative	O
,	O
implying	O
that	O
.i	O
(	O
cid:0	O
)	O
1	O
(	O
cid:22	O
)	O
/11	O
(	O
cid:21	O
)	O
(	O
cid:0	O
)	O
1	O
i	O
(	O
cid:22	O
)	O
11	O
:	O
(	O
5.31	O
)	O
if	O
(	O
cid:22	O
)	O
.2/	O
were	O
known	O
to	O
the	O
statistician	O
,	O
rather	O
than	O
requiring	O
estimation	B
,	O
then	O
f	O
(	O
cid:22	O
)	O
1	O
(	O
cid:22	O
)	O
.2/	O
.x/	O
would	O
be	O
a	O
one-parameter	B
family	O
,	O
with	O
fisher	O
information	B
i	O
(	O
cid:22	O
)	O
11	O
for	O
estimating	O
(	O
cid:22	O
)	O
1	O
,	O
giving	O
o	O
(	O
cid:22	O
)	O
1	O
p	O
(	O
cid:24	O
)	O
(	O
5.32	O
)	O
(	O
cid:0	O
)	O
1	O
(	O
cid:22	O
)	O
11/	O
:	O
n	O
.	O
(	O
cid:22	O
)	O
1	O
;	O
i	O
5.4	O
the	O
multinomial	O
distribution	B
61	O
comparing	O
(	O
5.28	O
)	O
with	O
(	O
5.32	O
)	O
,	O
(	O
5.31	O
)	O
shows	O
that	O
the	O
variance	O
of	O
the	O
mle	O
o	O
(	O
cid:22	O
)	O
1	O
must	O
always	O
increase4	O
in	O
the	O
presence	O
of	O
nuisance	O
parameters.	O
maximum	B
likelihood	I
,	O
and	O
in	O
fact	O
any	O
form	B
of	O
unbiased	O
or	O
nearly	O
unbi-	O
ased	O
estimation	B
,	O
pays	O
a	O
nuisance	O
tax	O
for	O
the	O
presence	O
of	O
“	O
other	O
”	O
parameters	O
.	O
modern	O
applications	O
often	O
involve	O
thousands	O
of	O
others	O
;	O
think	O
of	O
regression	B
ﬁts	O
with	O
too	O
many	O
predictors	O
.	O
in	O
some	O
circumstances	O
,	O
biased	O
estimation	B
methods	O
can	O
reverse	O
the	O
situation	O
,	O
using	O
the	O
others	O
to	O
actually	O
improve	O
esti-	O
mation	O
of	O
a	O
target	O
parameter	O
;	O
see	O
chapter	O
6	O
on	O
empirical	B
bayes	O
techniques	O
,	O
and	O
chapter	O
16	O
on	O
`1	O
regularized	O
regression	B
models	O
.	O
5	O
5.4	O
the	O
multinomial	O
distribution	B
second	O
in	O
the	O
small	O
catalog	O
of	O
well-known	O
classic	O
multivariate	B
distribu-	O
tions	O
is	O
the	O
multinomial	O
.	O
the	O
multinomial	O
applies	O
to	O
situations	O
in	O
which	O
the	O
observations	O
take	O
on	O
only	O
a	O
ﬁnite	O
number	O
of	O
discrete	O
values	O
,	O
say	O
l	O
of	O
them	O
.	O
the	O
2	O
(	O
cid:2	O
)	O
2	O
ulcer	O
surgery	O
of	O
table	O
4.1	O
is	O
repeated	O
in	O
table	O
5.2	O
,	O
now	O
with	O
the	O
cells	O
labeled	O
1	O
;	O
2	O
;	O
3	O
;	O
and	O
4.	O
here	O
there	O
are	O
l	O
d	O
4	O
possible	O
outcomes	O
for	O
each	O
patient	O
:	O
(	O
new	O
,	O
success	O
)	O
,	O
(	O
new	O
,	O
failure	O
)	O
,	O
(	O
old	O
,	O
success	O
)	O
,	O
(	O
old	O
,	O
failure	O
)	O
.	O
table	O
5.2	O
the	O
ulcer	O
study	O
of	O
table	O
4.1	O
,	O
now	O
with	O
the	O
cells	O
numbered	O
1	O
through	O
4	O
as	O
shown	O
.	O
success	O
failure	O
new	O
old	O
1	O
3	O
9	O
7	O
2	O
4	O
12	O
17	O
a	O
number	O
n	O
of	O
cases	O
has	O
been	O
observed	O
,	O
n	O
d	O
45	O
in	O
table	O
5.2.	O
let	O
x	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xl/	O
be	O
the	O
vector	B
of	O
counts	O
for	O
the	O
l	O
possible	O
outcomes	O
,	O
xl	O
d	O
#	O
fcases	O
having	O
outcome	O
lg	O
;	O
(	O
5.33	O
)	O
0	O
for	O
the	O
ulcer	O
data	B
.	O
it	O
is	O
convenient	O
to	O
code	O
the	O
outcomes	O
x	O
d	O
.9	O
;	O
12	O
;	O
7	O
;	O
17/	O
in	O
terms	O
of	O
the	O
coordinate	O
vectors	O
el	O
of	O
length	O
l	O
,	O
el	O
d	O
.0	O
;	O
0	O
;	O
:	O
:	O
:	O
;	O
0	O
;	O
1	O
;	O
0	O
;	O
:	O
:	O
:	O
;	O
0/	O
0	O
with	O
a	O
1	O
in	O
the	O
lth	O
place	O
.	O
4	O
unless	O
i	O
(	O
cid:22	O
)	O
1.2/	O
is	O
a	O
vector	B
of	O
zeros	O
,	O
a	O
condition	B
that	O
amounts	O
to	O
approximate	O
independence	O
of	O
o	O
(	O
cid:22	O
)	O
1	O
and	O
o	O
(	O
cid:22	O
)	O
.2/	O
.	O
;	O
(	O
5.34	O
)	O
62	O
parametric	B
models	O
figure	O
5.4	O
the	O
simplex	B
s3	O
is	O
an	O
equilateral	O
triangle	O
set	B
at	O
an	O
angle	O
to	O
the	O
coordinate	O
axes	O
in	O
r3	O
.	O
the	O
multinomial	O
probability	O
model	B
assumes	O
that	O
the	O
n	O
cases	O
are	O
inde-	O
pendent	O
of	O
each	O
other	O
,	O
with	O
each	O
case	O
having	O
probability	O
(	O
cid:25	O
)	O
l	O
for	O
outcome	O
el	O
,	O
(	O
cid:25	O
)	O
l	O
d	O
prfelg	O
;	O
l	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
l	O
:	O
indicate	O
the	O
vector	B
of	O
probabilities	B
.	O
the	O
count	O
vector	B
x	O
then	O
follows	O
the	O
multinomial	O
distribution	B
,	O
let	O
denoted	O
0	O
(	O
cid:25	O
)	O
d	O
.	O
(	O
cid:25	O
)	O
1	O
;	O
(	O
cid:25	O
)	O
2	O
;	O
:	O
:	O
:	O
;	O
(	O
cid:25	O
)	O
l/	O
ly	O
nš	O
x1šx2š	O
:	O
:	O
:	O
xlš	O
ld1	O
f	O
(	O
cid:25	O
)	O
.x/	O
d	O
(	O
5.35	O
)	O
(	O
5.36	O
)	O
(	O
cid:25	O
)	O
xl	O
l	O
;	O
(	O
5.37	O
)	O
(	O
5.38	O
)	O
:	O
(	O
5.39	O
)	O
(	O
for	O
n	O
observations	O
,	O
l	O
outcomes	O
,	O
probability	O
vector	B
(	O
cid:25	O
)	O
)	O
.	O
the	O
parameter	O
space	B
(	O
cid:127	O
)	O
for	O
(	O
cid:25	O
)	O
is	O
the	O
simplex	B
sl	O
,	O
x	O
(	O
cid:24	O
)	O
multl.n	O
;	O
(	O
cid:25	O
)	O
/	O
lx	O
(	O
(	O
cid:25	O
)	O
w	O
(	O
cid:25	O
)	O
l	O
(	O
cid:21	O
)	O
0	O
and	O
ld1	O
)	O
(	O
cid:25	O
)	O
l	O
d	O
1	O
sl	O
d	O
figure	O
5.4	O
shows	O
s3	O
,	O
an	O
equilateral	O
triangle	O
sitting	O
at	O
an	O
angle	O
to	O
the	O
coordi-	O
nate	O
axes	O
e1	O
;	O
e2	O
;	O
and	O
e3	O
.	O
the	O
midpoint	O
of	O
the	O
triangle	O
(	O
cid:25	O
)	O
d	O
.1=3	O
;	O
1=3	O
;	O
1=3/	O
5.4	O
the	O
multinomial	O
distribution	B
63	O
corresponds	O
to	O
a	O
multinomial	O
distribution	B
putting	O
equal	O
probability	O
on	O
the	O
three	O
possible	O
outcomes	O
.	O
figure	O
5.5	O
sample	B
space	O
x	O
for	O
x	O
(	O
cid:24	O
)	O
mult3.4	O
;	O
(	O
cid:25	O
)	O
/	O
;	O
numbers	O
indicate	O
.x1	O
;	O
x2	O
;	O
x3/	O
.	O
the	O
sample	B
space	O
x	O
for	O
x	O
is	O
the	O
subset	O
of	O
nsl	O
(	O
the	O
set	B
of	O
nonnegative	O
vectors	O
summing	O
to	O
n	O
)	O
having	O
integer	O
components	O
.	O
figure	O
5.5	O
illustrates	O
the	O
case	O
n	O
d	O
4	O
and	O
l	O
d	O
3	O
,	O
now	O
with	O
the	O
triangle	O
of	O
figure	O
5.4	O
multiplied	O
by	O
4	O
and	O
set	O
ﬂat	O
on	O
the	O
page	O
.	O
the	O
point	O
121	O
indicates	O
x	O
d	O
.1	O
;	O
2	O
;	O
1/	O
,	O
with	O
probability	O
12	O
(	O
cid:1	O
)	O
(	O
cid:25	O
)	O
1	O
(	O
cid:25	O
)	O
2	O
in	O
the	O
dichotomous	O
case	O
,	O
l	O
d	O
2	O
,	O
the	O
multinomial	O
distribution	B
reduces	O
to	O
the	O
binomial	B
,	O
with	O
.	O
(	O
cid:25	O
)	O
1	O
;	O
(	O
cid:25	O
)	O
2/	O
equaling	O
.	O
(	O
cid:25	O
)	O
;	O
1	O
(	O
cid:0	O
)	O
(	O
cid:25	O
)	O
/	O
in	O
line	O
3	O
of	O
table	O
5.1	O
,	O
and	O
.x1	O
;	O
x2/	O
equaling	O
.x	O
;	O
n	O
(	O
cid:0	O
)	O
x/	O
.	O
the	O
mean	O
vector	B
and	O
covariance	O
matrix	B
of	O
multl.n	O
;	O
(	O
cid:25	O
)	O
/	O
,	O
for	O
any	O
value	O
of	O
l	O
,	O
are	O
2	O
(	O
cid:25	O
)	O
3	O
according	O
to	O
(	O
5.37	O
)	O
,	O
etc	O
.	O
x	O
(	O
cid:24	O
)	O
(	O
cid:0	O
)	O
n	O
(	O
cid:25	O
)	O
;	O
n	O
(	O
cid:2	O
)	O
diag	O
.	O
(	O
cid:25	O
)	O
/	O
(	O
cid:0	O
)	O
(	O
cid:25	O
)	O
(	O
cid:25	O
)	O
0	O
(	O
cid:3	O
)	O
(	O
cid:1	O
)	O
(	O
5.40	O
)	O
(	O
diag	O
.	O
(	O
cid:25	O
)	O
/	O
is	O
the	O
diagonal	O
matrix	B
with	O
diagonal	O
elements	O
(	O
cid:25	O
)	O
l	O
)	O
,	O
so	O
var.xl	O
/	O
d	O
n	O
(	O
cid:25	O
)	O
l	O
.1	O
(	O
cid:0	O
)	O
(	O
cid:25	O
)	O
l	O
/	O
and	O
covariance	O
.xl	O
;	O
xj	O
/	O
d	O
(	O
cid:0	O
)	O
n	O
(	O
cid:25	O
)	O
l	O
(	O
cid:25	O
)	O
j	O
;	O
(	O
5.40	O
)	O
generalizes	O
the	O
binomial	B
mean	O
and	O
variance	O
.n	O
(	O
cid:25	O
)	O
;	O
n	O
(	O
cid:25	O
)	O
.1	O
(	O
cid:0	O
)	O
(	O
cid:25	O
)	O
//	O
.	O
there	O
is	O
a	O
useful	O
relationship	O
between	O
the	O
multinomial	O
distribution	B
and	O
the	O
poisson	O
.	O
suppose	O
s1	O
;	O
s2	O
;	O
:	O
:	O
:	O
;	O
sl	O
are	O
independent	O
poissons	O
having	O
pos-	O
sibly	O
different	O
parameters	O
,	O
ind	O
(	O
cid:24	O
)	O
poi	O
.	O
(	O
cid:22	O
)	O
l	O
/	O
;	O
sl	O
l	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
l	O
;	O
(	O
5.41	O
)	O
6	O
or	O
,	O
more	O
concisely	O
,	O
s	O
(	O
cid:24	O
)	O
poi	O
.	O
(	O
cid:22	O
)	O
/	O
with	O
s	O
d	O
.s1	O
;	O
s2	O
;	O
:	O
:	O
:	O
;	O
sl/	O
0	O
and	O
(	O
cid:22	O
)	O
d	O
.	O
(	O
cid:22	O
)	O
1	O
;	O
(	O
cid:22	O
)	O
2	O
;	O
:	O
:	O
:	O
;	O
(	O
cid:22	O
)	O
l/	O
(	O
5.42	O
)	O
0	O
,	O
the	O
independence	O
103	O
202	O
301	O
004	O
400	O
211	O
121	O
112	O
310	O
220	O
130	O
040	O
031	O
022	O
013	O
being	O
assumed	O
in	O
notation	O
(	O
5.42	O
)	O
.	O
then	O
the	O
conditional	O
distribution	B
of	O
s	O
7	O
parametric	B
models	O
64	O
given	O
the	O
sum	O
sc	O
dp	O
sl	O
is	O
multinomial	O
,	O
	O
(	O
cid:22	O
)	O
c	O
dp	O
(	O
cid:22	O
)	O
l.	O
sjsc	O
(	O
cid:24	O
)	O
multl.sc	O
;	O
(	O
cid:22	O
)	O
=	O
(	O
cid:22	O
)	O
c/	O
;	O
(	O
5.43	O
)	O
going	O
in	O
the	O
other	O
direction	O
,	O
suppose	O
n	O
(	O
cid:24	O
)	O
poi.n/	O
.	O
then	O
the	O
uncondi-	O
tional	O
or	O
marginal	O
distribution	B
of	O
multl.n	O
;	O
(	O
cid:25	O
)	O
/	O
is	O
poisson	O
,	O
if	O
n	O
(	O
cid:24	O
)	O
poi.n/	O
:	O
multl.n	O
;	O
(	O
cid:25	O
)	O
/	O
(	O
cid:24	O
)	O
poi.n	O
(	O
cid:25	O
)	O
/	O
(	O
5.44	O
)	O
calculations	O
involving	O
x	O
(	O
cid:24	O
)	O
multl.n	O
;	O
(	O
cid:25	O
)	O
/	O
are	O
sometimes	O
complicated	O
by	O
the	O
multinomial	O
’	O
s	O
correlations	O
.	O
the	O
approximation	O
x	O
p	O
(	O
cid:24	O
)	O
poi.n	O
(	O
cid:25	O
)	O
/	O
removes	O
the	O
correlations	O
and	O
is	O
usually	O
quite	O
accurate	O
if	O
n	O
is	O
large	O
.	O
there	O
is	O
one	O
more	O
important	O
thing	O
to	O
say	O
about	O
the	O
multinomial	O
family	O
:	O
it	O
contains	O
all	O
distributions	O
on	O
a	O
sample	B
space	O
x	O
composed	O
of	O
l	O
discrete	O
cat-	O
egories	O
.	O
in	O
this	O
sense	O
it	O
is	O
a	O
model	B
for	O
nonparametric	B
inference	O
on	O
x	O
.	O
the	O
nonparametric	B
bootstrap	O
calculations	O
of	O
chapter	O
10	O
use	O
the	O
multinomial	O
in	O
this	O
way	O
.	O
nonparametrics	O
,	O
and	O
the	O
multinomial	O
,	O
have	O
played	O
a	O
larger	O
role	O
in	O
the	O
modern	O
environment	O
of	O
large	O
,	O
difﬁcult	O
to	O
model	B
,	O
data	B
sets	O
.	O
5.5	O
exponential	O
families	O
classic	O
parametric	B
families	O
dominated	O
statistical	O
theory	B
and	O
practice	O
for	O
a	O
century	O
and	O
more	O
,	O
with	O
an	O
enormous	O
catalog	O
of	O
their	O
individual	O
properties—	O
means	O
,	O
variances	O
,	O
tail	O
areas	O
,	O
etc.—being	O
compiled	O
.	O
a	O
surprise	O
,	O
though	O
a	O
slowly	O
emerging	O
one	O
beginning	O
in	O
the	O
1930s	O
,	O
was	O
that	O
all	O
of	O
them	O
were	O
examples	O
of	O
a	O
powerful	O
general	O
construction	O
:	O
exponential	O
families	O
.	O
what	O
follows	O
here	O
is	O
a	O
brief	O
introduction	O
to	O
the	O
basic	O
theory	B
,	O
with	O
further	O
devel-	O
opment	O
to	O
come	O
in	O
subsequent	O
chapters	O
.	O
to	O
begin	O
with	O
,	O
consider	O
the	O
poisson	O
family	O
,	O
line	O
2	O
of	O
table	O
5.1.	O
the	O
ratio	O
of	O
poisson	O
densities	O
at	O
two	O
parameter	O
values	O
(	O
cid:22	O
)	O
and	O
(	O
cid:22	O
)	O
0	O
is	O
f	O
(	O
cid:22	O
)	O
.x/	O
f	O
(	O
cid:22	O
)	O
0	O
.x/	O
which	O
can	O
be	O
re-expressed	O
as	O
	O
(	O
cid:22	O
)	O
x	O
(	O
cid:22	O
)	O
0	O
d	O
e	O
(	O
cid:0	O
)	O
.	O
(	O
cid:22	O
)	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
0/	O
;	O
f	O
(	O
cid:22	O
)	O
.x/	O
d	O
e˛x	O
(	O
cid:0	O
)	O
.˛/f	O
(	O
cid:22	O
)	O
0	O
.x/	O
;	O
where	O
we	O
have	O
deﬁned	O
˛	O
d	O
logf	O
(	O
cid:22	O
)	O
=	O
(	O
cid:22	O
)	O
0g	O
and	O
.˛/	O
d	O
(	O
cid:22	O
)	O
0.e˛	O
(	O
cid:0	O
)	O
1/	O
:	O
looking	O
at	O
(	O
5.46	O
)	O
,	O
we	O
can	O
describe	O
the	O
poisson	O
family	O
in	O
three	O
steps	O
.	O
(	O
5.45	O
)	O
(	O
5.46	O
)	O
(	O
5.47	O
)	O
5.5	O
exponential	O
families	O
65	O
1	O
start	O
with	O
any	O
one	O
poisson	O
distribution	B
f	O
(	O
cid:22	O
)	O
0	O
.x/	O
.	O
2	O
for	O
any	O
value	O
of	O
(	O
cid:22	O
)	O
>	O
0	O
let	O
˛	O
d	O
logf	O
(	O
cid:22	O
)	O
=	O
(	O
cid:22	O
)	O
0g	O
and	O
calculate	O
for	O
x	O
d	O
0	O
;	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
:	O
q	O
f	O
(	O
cid:22	O
)	O
.x/	O
d	O
e˛xf	O
(	O
cid:22	O
)	O
0	O
.x/	O
3	O
finally	O
,	O
divide	O
q	O
f	O
(	O
cid:22	O
)	O
.x/	O
by	O
exp	O
.	O
.˛//	O
to	O
get	O
the	O
poisson	O
density	B
f	O
(	O
cid:22	O
)	O
.x/	O
.	O
in	O
other	O
words	O
,	O
we	O
“	O
tilt	O
”	O
f	O
(	O
cid:22	O
)	O
0	O
.x/	O
with	O
the	O
exponential	O
factor	B
e˛x	O
to	O
get	O
q	O
f	O
(	O
cid:22	O
)	O
.x/	O
,	O
and	O
then	O
renormalize	O
q	O
f	O
(	O
cid:22	O
)	O
.x/	O
to	O
sum	O
to	O
1.	O
notice	O
that	O
(	O
5.46	O
)	O
gives	O
exp	O
.	O
(	O
cid:0	O
)	O
.˛//	O
as	O
the	O
renormalizing	O
constant	O
since	O
(	O
5.48	O
)	O
1x	O
e	O
.˛/	O
d	O
e˛xf	O
(	O
cid:22	O
)	O
0	O
.x/	O
:	O
(	O
5.49	O
)	O
0	O
figure	O
5.6	O
poisson	O
densities	O
for	O
(	O
cid:22	O
)	O
d	O
3	O
;	O
6	O
;	O
9	O
;	O
12	O
;	O
15	O
;	O
18	O
;	O
heavy	O
green	O
curve	O
with	O
dots	O
for	O
(	O
cid:22	O
)	O
d	O
12.	O
figure	O
5.6	O
graphs	O
the	O
poisson	O
density	B
f	O
(	O
cid:22	O
)	O
.x/	O
for	O
(	O
cid:22	O
)	O
d	O
3	O
;	O
6	O
;	O
9	O
;	O
12	O
;	O
15	O
;	O
18.	O
each	O
poisson	O
density	B
is	O
a	O
renormalized	O
exponential	O
tilt	O
of	O
any	O
other	O
poisson	O
density	B
.	O
so	O
for	O
instance	O
f6.x/	O
is	O
obtained	O
from	O
f12.x/	O
via	O
the	O
tilt	O
e˛x	O
with	O
˛	O
d	O
logf6=12g	O
d	O
(	O
cid:0	O
)	O
0:693.5	O
5	O
alternate	O
expressions	O
for	O
f	O
(	O
cid:22	O
)	O
.x/	O
as	O
an	O
exponential	O
family	O
are	O
available	O
,	O
for	O
example	O
exp.˛x	O
(	O
cid:0	O
)	O
.˛//f0.x/	O
,	O
where	O
˛	O
d	O
log	O
(	O
cid:22	O
)	O
,	O
.˛/	O
d	O
exp.˛/	O
,	O
and	O
f0.x/	O
d	O
1=xš	O
.	O
(	O
it	O
isn	O
’	O
t	B
necessary	O
for	O
f0.x/	O
to	O
be	O
a	O
member	O
of	O
the	O
family	O
.	O
)	O
0510152025300.000.050.100.150.20xf	O
(	O
x	O
)	O
lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll	O
66	O
parametric	B
models	O
the	O
poisson	O
is	O
a	O
one-parameter	B
exponential	O
family	O
,	O
in	O
that	O
˛	O
and	O
x	O
in	O
expression	O
(	O
5.46	O
)	O
are	O
one-dimensional	O
.	O
a	O
p-parameter	B
exponential	O
family	O
has	O
the	O
form	B
f˛.x/	O
d	O
e˛	O
0	O
y	O
(	O
cid:0	O
)	O
.˛/f0.x/	O
(	O
5.50	O
)	O
where	O
˛	O
and	O
y	O
are	O
p-vectors	O
and	O
a	O
is	O
contained	O
in	O
rp	O
.	O
here	O
˛	O
is	O
the	O
“	O
canonical	O
”	O
or	O
“	O
natural	O
”	O
parameter	O
vector	B
and	O
y	O
d	O
t	B
.x/	O
is	O
the	O
“	O
sufﬁcient	O
statistic	B
”	O
vector	B
.	O
the	O
normalizing	O
function	B
.˛/	O
,	O
which	O
makes	O
f˛.x/	O
inte-	O
grate	O
(	O
or	O
sum	O
)	O
to	O
one	O
,	O
satisﬁes	O
for	O
˛	O
2	O
a	O
;	O
0	O
e˛	O
yf0.x/	O
dx	O
;	O
(	O
5.51	O
)	O
8	O
and	O
it	O
can	O
be	O
shown	O
that	O
the	O
parameter	O
space	B
a	O
for	O
which	O
the	O
integral	O
is	O
ﬁnite	O
is	O
a	O
convex	O
set	B
	O
in	O
rp	O
.	O
as	O
an	O
example	O
,	O
the	O
gamma	B
family	O
on	O
line	O
4	O
of	O
table	O
5.1	O
is	O
a	O
two-parameter	O
exponential	O
family	O
,	O
with	O
˛	O
and	O
y	O
d	O
t	B
.x/	O
given	O
by	O
e	O
.˛/	O
dz	O
x	O
	O
.˛1	O
;	O
˛2/	O
d	O
(	O
cid:0	O
)	O
1	O
and	O
.y1	O
;	O
y2/	O
d	O
.x	O
;	O
log	O
x/	O
;	O
;	O
(	O
cid:23	O
)	O
;	O
(	O
cid:27	O
)	O
.˛/	O
d	O
(	O
cid:23	O
)	O
log	O
(	O
cid:27	O
)	O
c	O
log	O
	O
.	O
(	O
cid:23	O
)	O
/	O
d	O
(	O
cid:0	O
)	O
˛2	O
logf	O
(	O
cid:0	O
)	O
˛1g	O
c	O
logf.˛2/g	O
:	O
the	O
parameter	O
space	B
a	O
is	O
f˛1	O
<	O
0	O
and	O
˛2	O
>	O
0g	O
.	O
why	O
are	O
we	O
interested	O
in	O
exponential	O
tilting	O
rather	O
than	O
some	O
other	O
trans-	O
formational	O
form	B
?	O
the	O
answer	O
has	O
to	O
do	O
with	O
repeated	O
sampling	O
.	O
suppose	O
x	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn/	O
is	O
an	O
iid	O
sample	B
from	O
a	O
p-parameter	B
exponential	O
family	O
(	O
5.50	O
)	O
.	O
then	O
,	O
letting	O
yi	O
d	O
t	B
.xi	O
/	O
denote	O
the	O
sufﬁcient	O
vector	B
corre-	O
sponding	O
to	O
xi	O
,	O
(	O
5.52	O
)	O
(	O
5.53	O
)	O
(	O
5.54	O
)	O
f˛.x/	O
d	O
ny	O
0	O
yi	O
(	O
cid:0	O
)	O
.˛/f0.xi	O
/	O
e˛	O
id1	O
d	O
en.˛	O
0	O
ny	O
(	O
cid:0	O
)	O
.˛//f0.x/	O
;	O
where	O
ny	O
dpn	O
1	O
yi	O
=n	O
.	O
this	O
is	O
still	O
a	O
p-parameter	B
exponential	O
family	O
,	O
now	O
with	O
natural	O
parameter	O
n˛	O
,	O
sufﬁcient	O
statistic	B
ny	O
,	O
and	O
normalizer	O
n	O
.˛/	O
.	O
no	O
matter	O
how	O
large	O
n	O
may	O
be	O
,	O
the	O
statistician	O
can	O
still	O
compress	O
all	O
the	O
inferential	O
information	B
into	O
a	O
p-dimensional	O
statistic	B
ny	O
.	O
only	O
exponential	O
families	O
enjoy	O
this	O
property	O
.	O
even	O
though	O
they	O
were	O
discovered	O
and	O
developed	O
in	O
quite	O
different	O
con-	O
texts	O
,	O
and	O
at	O
quite	O
different	O
times	O
,	O
all	O
of	O
the	O
distributions	O
discussed	O
in	O
this	O
5.5	O
exponential	O
families	O
67	O
chapter	O
exist	O
in	O
exponential	O
families	O
.	O
this	O
isn	O
’	O
t	B
quite	O
the	O
coincidence	O
it	O
seems	O
.	O
mathematical	O
tractability	O
was	O
the	O
prized	O
property	O
of	O
classic	O
para-	O
metric	O
distributions	O
,	O
and	O
tractability	O
was	O
greatly	O
facilitated	O
by	O
exponential	O
structure	O
,	O
even	O
if	O
that	O
structure	O
went	O
unrecognized	O
.	O
in	O
one-parameter	B
exponential	O
families	O
,	O
the	O
normalizer	O
.˛/	O
is	O
also	O
known	O
as	O
the	O
cumulant	O
generating	O
function	B
.	O
derivatives	O
of	O
.˛/	O
yield	O
the	O
cumu-	O
lants	O
of	O
y,6	O
the	O
ﬁrst	O
two	O
giving	O
the	O
mean	O
and	O
variance	O
9	O
p	O
.˛/	O
d	O
e˛fyg	O
r	O
.˛/	O
d	O
var˛fyg	O
:	O
and	O
(	O
5.55	O
)	O
(	O
5.56	O
)	O
(	O
5.57	O
)	O
(	O
5.58	O
)	O
similarly	O
,	O
in	O
p-parametric	O
families	O
and	O
p	O
.˛/	O
d	O
.	O
:	O
:	O
:	O
@	O
=	O
@	O
˛j	O
:	O
:	O
:	O
/	O
0	O
d	O
e˛fyg	O
r	O
.˛/	O
d	O
@	O
2	O
.˛/	O
	O
d	O
cov˛fyg	O
:	O
@	O
˛j	O
@	O
˛k	O
the	O
p-dimensional	O
expectation	O
parameter	O
,	O
denoted	O
ˇ	O
d	O
e˛fyg	O
;	O
is	O
a	O
one-to-one	O
function	B
of	O
the	O
natural	O
parameter	O
˛	O
.	O
let	O
v˛	O
indicate	O
the	O
p	O
(	O
cid:2	O
)	O
p	O
covariance	O
matrix	B
,	O
v˛	O
d	O
cov˛.y/	O
:	O
then	O
the	O
p	O
(	O
cid:2	O
)	O
p	O
derivative	O
matrix	B
of	O
ˇ	O
with	O
respect	O
to	O
˛	O
is	O
(	O
5.59	O
)	O
dˇ	O
d	O
.	O
@	O
ˇj	O
=	O
@	O
˛k/	O
d	O
v˛	O
;	O
v	O
d˛	O
(	O
5.60	O
)	O
this	O
following	O
from	O
(	O
5.56	O
)	O
–	O
(	O
5.57	O
)	O
,	O
the	O
inverse	O
mapping	O
being	O
d˛=dˇ	O
d	O
˛	O
.	O
as	O
a	O
one-parameter	B
example	O
,	O
the	O
poisson	O
in	O
table	O
5.1	O
has	O
˛	O
d	O
log	O
(	O
cid:22	O
)	O
,	O
(	O
cid:0	O
)	O
1	O
ˇ	O
d	O
(	O
cid:22	O
)	O
,	O
y	O
d	O
x	O
,	O
and	O
dˇ=d˛	O
d	O
1=.d˛=dˇ/	O
d	O
(	O
cid:22	O
)	O
d	O
v˛	O
.	O
the	O
maximum	B
likelihood	I
estimate	O
for	O
the	O
expectation	O
parameter	O
ˇ	O
is	O
simply	O
y	O
(	O
or	O
ny	O
under	O
repeated	O
sampling	O
(	O
5.54	O
)	O
)	O
,	O
which	O
makes	O
it	O
immediate	O
to	O
calculate	O
in	O
most	O
situations.	O
less	O
immediate	O
is	O
the	O
mle	O
for	O
the	O
natural	O
10	O
parameter	O
˛	O
:	O
the	O
one-to-one	O
mapping	O
ˇ	O
d	O
p	O
.˛/	O
(	O
5.56	O
)	O
has	O
inverse	O
˛	O
d	O
p	O
(	O
cid:0	O
)	O
1.ˇ/	O
,	O
so	O
o˛	O
d	O
p	O
(	O
cid:0	O
)	O
1.y/	O
;	O
(	O
5.61	O
)	O
6	O
the	O
simpliﬁed	O
dot	O
notation	O
leads	O
to	O
more	O
compact	O
expressions	O
:	O
p	O
.˛/	O
d	O
d	O
.˛/=d˛	O
and	O
r	O
.˛/	O
d	O
d	O
2	O
.˛/=d˛2	O
.	O
parametric	B
models	O
68	O
e.g.	O
,	O
o˛	O
d	O
log	O
y	O
for	O
the	O
poisson	O
.	O
the	O
trouble	O
is	O
that	O
p	O
(	O
cid:0	O
)	O
1	O
.	O
(	O
cid:1	O
)	O
/	O
is	O
usually	O
un-	O
available	O
in	O
closed	O
form	B
.	O
numerical	O
approximation	O
algorithms	O
are	O
neces-	O
sary	O
to	O
calculate	O
o˛	O
in	O
most	O
cases	O
.	O
all	O
of	O
the	O
classic	O
exponential	O
families	O
have	O
closed-form	O
expressions	O
for	O
.˛/	O
(	O
and	O
f˛.x/	O
)	O
,	O
yielding	O
pleasant	O
formulas	O
for	O
the	O
mean	O
ˇ	O
and	O
covar-	O
iance	O
v˛	O
,	O
(	O
5.56	O
)	O
–	O
(	O
5.57	O
)	O
.	O
modern	O
computational	O
technology	O
allows	O
us	O
to	O
work	O
with	O
general	O
exponential	O
families	O
,	O
designed	O
for	O
speciﬁc	O
tasks	O
,	O
with-	O
out	O
concern	O
for	O
mathematical	O
tractability	O
.	O
figure	O
5.7	O
a	O
seven-parameter	O
exponential	O
family	O
ﬁt	O
to	O
the	O
gfr	O
data	B
of	O
figure	O
2.1	O
(	O
solid	O
)	O
compared	O
with	O
gamma	B
ﬁt	O
of	O
figure	O
4.1	O
(	O
dashed	O
)	O
.	O
as	O
an	O
example	O
we	O
again	O
consider	O
ﬁtting	B
the	O
gfr	O
data	B
of	O
figure	O
2.1.	O
for	O
our	O
exponential	O
family	O
of	O
possible	O
densities	O
we	O
take	O
f0.x/	O
	O
1	O
,	O
and	O
sufﬁcient	O
statistic	B
vector	O
y.x/	O
d	O
.x	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
x7/	O
;	O
0	O
(	O
5.62	O
)	O
y	O
in	O
(	O
5.50	O
)	O
can	O
represent	O
all	O
7th-order	O
polynomials	O
in	O
x	O
,	O
the	O
gfr	O
so	O
˛	O
measurement.7	O
(	O
stopping	O
at	O
power	O
2	O
gives	O
the	O
n	O
.	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
2/	O
family	O
,	O
which	O
we	O
already	O
know	O
ﬁts	O
poorly	O
from	O
figure	O
4.1	O
.	O
)	O
the	O
heavy	O
curve	O
in	O
figure	O
5.7	O
shows	O
the	O
mle	O
ﬁt	O
fo˛.x/	O
now	O
following	O
the	O
gfr	O
histogram	O
quite	O
closely	O
.	O
chapter	O
10	O
discusses	O
“	O
lindsey	O
’	O
s	O
method	B
,	O
”	O
a	O
simpliﬁed	O
algorithm	B
for	O
cal-	O
culating	O
the	O
mle	O
o˛	O
.	O
7	O
any	O
intercept	O
in	O
the	O
polynomial	O
is	O
absorbed	O
into	O
the	O
.˛/	O
term	O
in	O
(	O
5.57	O
)	O
.	O
gfrfrequency20406080100051015202530gammaexponential	O
family	O
5.6	O
notes	O
and	O
details	O
69	O
a	O
more	O
exotic	O
example	O
concerns	O
the	O
generation	O
of	O
random	O
graphs	O
on	O
a	O
ﬁxed	O
set	B
of	O
n	O
nodes	B
.	O
each	O
possible	O
graph	O
has	O
a	O
certain	O
total	O
number	O
e	O
of	O
edges	O
,	O
and	O
t	O
of	O
triangles	O
.	O
a	O
popular	O
choice	O
for	O
generating	O
such	O
graphs	O
is	O
the	O
two-parameter	O
exponential	O
family	O
having	O
y	O
d	O
.e	O
;	O
t	B
/	O
,	O
so	O
that	O
larger	O
values	O
of	O
˛1	O
and	O
˛2	O
yield	O
more	O
connections	O
.	O
5.6	O
notes	O
and	O
details	O
the	O
notion	O
of	O
sufﬁcient	O
statistics	B
,	O
ones	O
that	O
contain	O
all	O
available	O
inferen-	O
tial	O
information	B
,	O
was	O
perhaps	O
fisher	O
’	O
s	O
happiest	O
contribution	O
to	O
the	O
classic	O
corpus	O
.	O
he	O
noticed	O
that	O
in	O
the	O
exponential	O
family	O
form	B
(	O
5.50	O
)	O
,	O
the	O
fact	O
that	O
the	O
parameter	O
˛	O
interacts	O
with	O
the	O
data	B
x	O
only	O
through	O
the	O
factor	B
exp.˛	O
y/	O
makes	O
y.x/	O
sufﬁcient	O
for	O
estimating	O
˛	O
.	O
in	O
1935–36	O
,	O
a	O
trio	O
of	O
authors	O
,	O
work-	O
ing	O
independently	O
in	O
different	O
countries	O
,	O
pitman	O
,	O
darmois	O
,	O
and	O
koopmans	O
,	O
showed	O
that	O
exponential	O
families	O
are	O
the	O
only	O
ones	O
that	O
enjoy	O
ﬁxed-dimen-	O
sional	O
sufﬁcient	O
statistics	B
under	O
repeated	O
independent	O
sampling	O
.	O
until	O
the	O
late	O
1950s	O
such	O
distributions	O
were	O
called	O
pitman–darmois–koopmans	O
fam-	O
ilies	O
,	O
the	O
long	O
name	O
suggesting	O
infrequent	O
usage	O
.	O
0	O
generalized	O
linear	B
models	O
,	O
chapter	O
8	O
,	O
show	O
the	O
continuing	O
impact	O
of	O
sufﬁciency	O
on	O
statistical	O
practice	O
.	O
peter	O
bickel	O
has	O
pointed	O
out	O
that	O
data	B
compression	O
,	O
a	O
lively	O
topic	O
in	O
areas	O
such	O
as	O
image	O
transmission	O
,	O
is	O
a	O
mod-	O
ern	O
,	O
less	O
stringent	O
,	O
version	O
of	O
sufﬁciency	O
.	O
our	O
only	O
nonexponential	O
family	O
so	O
far	O
was	O
(	O
4.39	O
)	O
,	O
the	O
cauchy	O
transla-	O
tional	O
model	B
.	O
efron	O
and	O
hinkley	O
(	O
1978	O
)	O
analyze	O
the	O
cauchy	O
family	O
in	O
terms	O
of	O
curved	B
exponential	O
families	O
,	O
a	O
generalization	O
of	O
model	B
(	O
5.50	O
)	O
.	O
properties	O
of	O
classical	O
distributions	O
(	O
lots	O
of	O
properties	O
and	O
lots	O
of	O
distri-	O
butions	O
)	O
are	O
covered	O
in	O
johnson	O
and	O
kotz	O
’	O
s	O
invaluable	O
series	O
of	O
reference	O
books	O
,	O
1969–1972	O
.	O
two	O
classic	O
multivariate	B
analysis	O
texts	O
are	O
anderson	O
(	O
2003	O
)	O
and	O
mardia	O
et	O
al	O
.	O
(	O
1979	O
)	O
.	O
(	O
cid:0	O
)	O
1	O
(	O
cid:0	O
)	O
1.x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
/	O
we	O
have	O
dz=dx	O
d	O
t	B
1	O
[	O
p.	O
57	O
]	O
formula	B
(	O
5.12	O
)	O
.	O
from	O
z	O
d	O
t	B
2	O
jt	O
(	O
cid:0	O
)	O
p	O
and	O
f	O
(	O
cid:22	O
)	O
;	O
†.x/	O
d	O
f	O
.z/jt	O
so	O
(	O
5.12	O
)	O
follows	O
from	O
t	O
t	B
(	O
cid:0	O
)	O
1j	O
d	O
.2	O
(	O
cid:25	O
)	O
/	O
2	O
.x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
/	O
(	O
cid:0	O
)	O
1	O
0	O
0	O
d	O
†	O
and	O
jt	O
j	O
d	O
j†j1=2	O
.	O
0	O
(	O
cid:0	O
)	O
1	O
t	B
(	O
cid:0	O
)	O
1.x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
/	O
;	O
(	O
5.63	O
)	O
t	B
(	O
cid:0	O
)	O
1je	O
(	O
cid:1	O
)	O
(	O
cid:0	O
)	O
1	O
	O
d	O
(	O
cid:0	O
)	O
†11	O
(	O
cid:0	O
)	O
†12†	O
ƒ11	O
ƒ12	O
2	O
[	O
p.	O
58	O
]	O
formula	B
(	O
5.18	O
)	O
.	O
let	O
ƒ	O
d	O
†	O
(	O
cid:0	O
)	O
1	O
be	O
partitioned	O
as	O
in	O
(	O
5.17	O
)	O
.	O
then	O
(	O
cid:0	O
)	O
1	O
22	O
†21	O
(	O
cid:1	O
)	O
(	O
cid:0	O
)	O
1	O
(	O
5.64	O
)	O
direct	O
multiplication	O
showing	O
that	O
ƒ†	O
d	O
i	O
,	O
the	O
identity	O
matrix	B
.	O
if	O
†	O
is	O
(	O
cid:0	O
)	O
†22	O
(	O
cid:0	O
)	O
†21†	O
(	O
cid:0	O
)	O
1	O
22	O
†21ƒ11	O
(	O
cid:0	O
)	O
†	O
(	O
cid:0	O
)	O
1	O
11	O
†12ƒ22	O
(	O
cid:0	O
)	O
1	O
11	O
†12	O
ƒ21	O
ƒ22	O
!	O
;	O
(	O
cid:0	O
)	O
†	O
parametric	B
models	O
70	O
symmetric	O
then	O
ƒ21	O
d	O
ƒ	O
12.	O
by	O
redeﬁning	O
x	O
to	O
be	O
x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
we	O
can	O
set	B
(	O
cid:22	O
)	O
.1/	O
0	O
and	O
(	O
cid:22	O
)	O
.2/	O
equal	O
to	O
zero	O
in	O
(	O
5.18	O
)	O
.	O
the	O
quadratic	O
form	B
in	O
the	O
exponent	O
of	O
(	O
5.12	O
)	O
is	O
0	O
.2//ƒ	O
(	O
cid:0	O
)	O
x.1/	O
;	O
x.2/	O
(	O
cid:1	O
)	O
d	O
x	O
(	O
cid:0	O
)	O
x.2/	O
(	O
cid:0	O
)	O
†21†	O
0	O
.1/	O
;	O
x	O
.1/ƒ12x.2/	O
c	O
x	O
0	O
(	O
cid:1	O
)	O
but	O
,	O
using	O
(	O
5.64	O
)	O
,	O
this	O
matches	O
the	O
quadratic	O
form	B
from	O
(	O
5.18	O
)	O
,	O
.2/ƒ22x.2/	O
c	O
2x	O
0	O
(	O
cid:1	O
)	O
0	O
(	O
cid:0	O
)	O
x.2/	O
(	O
cid:0	O
)	O
†21†	O
0	O
.1/ƒ11x.1/	O
:	O
(	O
5.65	O
)	O
(	O
5.66	O
)	O
(	O
cid:0	O
)	O
1	O
11	O
x.1/	O
(	O
cid:0	O
)	O
1	O
11	O
x.1/	O
ƒ22	O
.x	O
except	O
for	O
an	O
added	O
term	O
that	O
does	O
not	O
involve	O
x.2/	O
.	O
for	O
a	O
multivariate	B
nor-	O
mal	O
distribution	B
,	O
this	O
is	O
sufﬁcient	O
to	O
show	O
that	O
the	O
conditional	O
distribution	B
of	O
x.2/	O
given	O
x.1/	O
is	O
indeed	O
(	O
5.18	O
)	O
(	O
see	O
3	O
)	O
.	O
3	O
[	O
p.	O
59	O
]	O
formulas	O
(	O
5.21	O
)	O
and	O
(	O
5.23	O
)	O
.	O
suppose	O
that	O
the	O
continuous	O
univariate	O
random	O
variable	O
z	O
has	O
density	B
of	O
the	O
form	B
f	O
.z/	O
d	O
c0e	O
(	O
cid:0	O
)	O
1	O
2	O
q.z/	O
;	O
where	O
q.z/	O
d	O
az2	O
c	O
2bz	O
c	O
c1	O
;	O
(	O
5.67	O
)	O
a	O
;	O
b	O
;	O
c0	O
and	O
c1	O
constants	O
,	O
a	O
>	O
0.	O
then	O
,	O
by	O
“	O
completing	O
the	O
square	O
,	O
”	O
f	O
.z/	O
d	O
c2e	O
2	O
a.z	O
(	O
cid:0	O
)	O
b	O
(	O
cid:0	O
)	O
1	O
a	O
/2	O
;	O
(	O
5.68	O
)	O
and	O
we	O
see	O
that	O
z	O
(	O
cid:24	O
)	O
n	O
.b=a	O
;	O
1=a/	O
.	O
the	O
key	O
point	O
is	O
that	O
form	B
(	O
5.67	O
)	O
spec-	O
iﬁes	O
z	O
as	O
normal	B
,	O
with	O
mean	O
and	O
variance	O
uniquely	O
determined	O
by	O
a	O
and	O
b.	O
the	O
multivariate	B
version	O
of	O
this	O
fact	O
was	O
used	O
in	O
the	O
derivation	O
of	O
for-	O
mula	O
(	O
5.18	O
)	O
.	O
by	O
redeﬁning	O
(	O
cid:22	O
)	O
and	O
x	O
as	O
(	O
cid:22	O
)	O
(	O
cid:0	O
)	O
m	O
and	O
x	O
(	O
cid:0	O
)	O
m	O
,	O
we	O
can	O
take	O
m	O
d	O
0	O
in	O
(	O
5.21	O
)	O
.	O
setting	O
b	O
d	O
a=.a	O
c	O
(	O
cid:27	O
)	O
2/	O
,	O
density	B
(	O
5.21	O
)	O
for	O
(	O
cid:22	O
)	O
jx	O
is	O
of	O
form	B
(	O
5.67	O
)	O
,	O
with	O
q	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
(	O
cid:22	O
)	O
2	O
(	O
cid:0	O
)	O
2x	O
(	O
cid:22	O
)	O
(	O
cid:27	O
)	O
2	O
c	O
bx2	O
(	O
cid:27	O
)	O
2	O
(	O
5.69	O
)	O
but	O
bayes	O
’	O
rule	B
says	O
that	O
the	O
density	B
of	O
(	O
cid:22	O
)	O
jx	O
is	O
proportional	O
to	O
g.	O
(	O
cid:22	O
)	O
/f	O
(	O
cid:22	O
)	O
.x/	O
,	O
also	O
of	O
form	B
(	O
5.67	O
)	O
,	O
now	O
with	O
b	O
(	O
cid:27	O
)	O
2	O
:	O
q	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
1	O
	O
c	O
1	O
(	O
cid:27	O
)	O
2	O
a	O
(	O
cid:22	O
)	O
2	O
(	O
cid:0	O
)	O
2x	O
(	O
cid:22	O
)	O
(	O
cid:27	O
)	O
2	O
c	O
x2	O
(	O
cid:27	O
)	O
2	O
:	O
(	O
5.70	O
)	O
a	O
little	O
algebra	O
shows	O
that	O
the	O
quadratic	O
and	O
linear	O
coefﬁcients	O
of	O
(	O
cid:22	O
)	O
match	O
in	O
(	O
5.69	O
)	O
–	O
(	O
5.70	O
)	O
,	O
verifying	O
(	O
5.21	O
)	O
.	O
we	O
verify	O
the	O
multivariate	B
result	O
(	O
5.23	O
)	O
using	O
a	O
different	O
argument	B
.	O
the	O
2p	O
vector	B
.	O
(	O
cid:22	O
)	O
;	O
x/	O
0	O
has	O
joint	O
distribution	B
m	O
	O
a	O
n	O
;	O
a	O
a	O
a	O
c	O
†	O
m	O
	O
:	O
(	O
5.71	O
)	O
5.6	O
notes	O
and	O
details	O
71	O
now	O
we	O
employ	O
(	O
5.18	O
)	O
and	O
a	O
little	O
manipulation	O
to	O
get	O
(	O
5.23	O
)	O
.	O
4	O
[	O
p.	O
60	O
]	O
formula	B
(	O
5.30	O
)	O
.	O
this	O
is	O
the	O
matrix	B
identity	O
(	O
5.64	O
)	O
,	O
now	O
with	O
†	O
equaling	O
i	O
(	O
cid:22	O
)	O
.	O
5	O
[	O
p.	O
61	O
]	O
multivariate	B
gaussian	O
and	O
nuisance	O
parameters	O
.	O
the	O
cautionary	O
message	O
here—that	O
increasing	O
the	O
number	O
of	O
unknown	O
nuisance	O
parame-	O
ters	O
decreases	O
the	O
accuracy	O
of	O
the	O
estimate	O
of	O
interest—can	O
be	O
stated	O
more	O
positively	O
:	O
if	O
some	O
nuisance	O
parameters	O
are	O
actually	O
known	O
,	O
then	O
the	O
mle	O
of	O
the	O
parameter	O
of	O
interest	O
becomes	O
more	O
accurate	O
.	O
suppose	O
,	O
for	O
example	O
,	O
we	O
wish	O
to	O
estimate	B
(	O
cid:22	O
)	O
1	O
from	O
a	O
sample	B
of	O
size	O
n	O
in	O
a	O
bivariate	O
normal	B
model	O
x	O
(	O
cid:24	O
)	O
n2	O
.	O
(	O
cid:22	O
)	O
;	O
†/	O
(	O
5.14	O
)	O
.	O
the	O
mle	O
nx1	O
has	O
variance	O
(	O
cid:27	O
)	O
11=n	O
in	O
notation	O
(	O
5.19	O
)	O
.	O
but	O
if	O
(	O
cid:22	O
)	O
2	O
is	O
known	O
then	O
the	O
mle	O
of	O
(	O
cid:22	O
)	O
1	O
becomes	O
nx1	O
(	O
cid:0	O
)	O
.	O
(	O
cid:27	O
)	O
12=	O
(	O
cid:27	O
)	O
22/.nx2	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
2/	O
p	O
with	O
variance	O
.	O
(	O
cid:27	O
)	O
11=n/	O
(	O
cid:1	O
)	O
.1	O
(	O
cid:0	O
)	O
(	O
cid:26	O
)	O
2/	O
,	O
(	O
cid:26	O
)	O
being	O
the	O
correlation	O
(	O
cid:27	O
)	O
12=	O
id1	O
xi	O
,	O
where	O
the	O
xi	O
are	O
iid	O
observations	O
having	O
prfxi	O
d	O
eig	O
d	O
(	O
cid:25	O
)	O
l	O
,	O
as	O
in	O
(	O
5.35	O
)	O
.	O
the	O
mean	O
and	O
covariance	O
of	O
each	O
xi	O
are	O
6	O
[	O
p.	O
63	O
]	O
formula	B
(	O
5.40	O
)	O
.	O
x	O
d	O
pn	O
efxig	O
d	O
lx	O
(	O
cid:25	O
)	O
l	O
el	O
d	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
11	O
(	O
cid:27	O
)	O
22	O
.	O
(	O
5.72	O
)	O
1	O
g	O
dx	O
and	O
0	O
(	O
cid:0	O
)	O
(	O
cid:25	O
)	O
(	O
cid:25	O
)	O
0	O
covfxig	O
d	O
efxi	O
x	O
0	O
g	O
(	O
cid:0	O
)	O
efxigefx	O
0	O
i	O
i	O
0	O
(	O
cid:25	O
)	O
l	O
el	O
e	O
l	O
d	O
diag	O
.	O
(	O
cid:25	O
)	O
/	O
(	O
cid:0	O
)	O
(	O
cid:25	O
)	O
(	O
cid:25	O
)	O
formula	B
(	O
5.40	O
)	O
follows	O
from	O
efxg	O
dp	O
efxig	O
and	O
cov.x/	O
dp	O
cov.xi	O
/	O
.	O
7	O
[	O
p.	O
64	O
]	O
formula	B
(	O
5.43	O
)	O
.	O
the	O
densities	O
of	O
s	O
(	O
5.42	O
)	O
and	O
sc	O
dp	O
sl	O
are	O
f	O
(	O
cid:22	O
)	O
.s	O
/	O
d	O
ly	O
scc	O
=scš	O
:	O
(	O
cid:22	O
)	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
l	O
(	O
cid:22	O
)	O
sl	O
(	O
5.74	O
)	O
(	O
5.73	O
)	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
c	O
e	O
:	O
l	O
=sl	O
š	O
ld1	O
the	O
conditional	O
density	B
of	O
s	O
given	O
sc	O
is	O
the	O
ratio	O
and	O
f	O
(	O
cid:22	O
)	O
c	O
.sc/	O
d	O
e	O
scšql	O
!	O
ly	O
	O
(	O
cid:22	O
)	O
l	O
(	O
cid:22	O
)	O
c	O
1	O
sl	O
š	O
ld1	O
sl	O
;	O
(	O
5.75	O
)	O
f	O
(	O
cid:22	O
)	O
.sjsc/	O
d	O
which	O
is	O
(	O
5.43	O
)	O
.	O
8	O
[	O
p.	O
66	O
]	O
formula	B
(	O
5.51	O
)	O
and	O
the	O
convexity	O
of	O
a.	O
suppose	O
˛1	O
and	O
˛2	O
are	O
any	O
two	O
points	O
in	O
a	O
,	O
i.e.	O
,	O
values	O
of	O
˛	O
having	O
the	O
integral	O
in	O
(	O
5.51	O
)	O
ﬁnite	O
.	O
for	O
any	O
value	O
of	O
c	O
in	O
the	O
interval	B
œ0	O
;	O
1	O
,	O
and	O
any	O
value	O
of	O
y	O
,	O
we	O
have	O
0	O
1y	O
c	O
.1	O
(	O
cid:0	O
)	O
c/e˛	O
0	O
2y	O
(	O
cid:21	O
)	O
eœc˛1c.1	O
(	O
cid:0	O
)	O
c/˛2	O
ce˛	O
(	O
5.76	O
)	O
because	O
of	O
the	O
convexity	O
in	O
c	O
of	O
the	O
function	B
on	O
the	O
right	O
(	O
veriﬁed	O
by	O
show-	O
ing	O
that	O
its	O
second	O
derivative	O
is	O
positive	O
)	O
.	O
integrating	O
both	O
sides	O
of	O
(	O
5.76	O
)	O
y	O
0	O
parametric	B
models	O
72	O
over	O
x	O
with	O
respect	O
to	O
f0.x/	O
shows	O
that	O
the	O
integral	O
on	O
the	O
right	O
must	O
be	O
ﬁnite	O
:	O
that	O
is	O
,	O
c˛1	O
c	O
.1	O
(	O
cid:0	O
)	O
c/˛2	O
is	O
in	O
a	O
,	O
verifying	O
a	O
’	O
s	O
convexity	O
.	O
9	O
[	O
p.	O
67	O
]	O
formula	B
(	O
5.55	O
)	O
.	O
in	O
the	O
univariate	O
case	O
,	O
differentiating	O
both	O
sides	O
of	O
(	O
5.51	O
)	O
with	O
respect	O
to	O
˛	O
gives	O
(	O
5.77	O
)	O
dividing	O
by	O
e	O
.˛/	O
shows	O
that	O
p	O
.˛/	O
d	O
e˛fyg	O
.	O
differentiating	O
(	O
5.77	O
)	O
again	O
gives	O
ye˛yf0.x/	O
dxi	O
p	O
.˛/e	O
.˛/	O
dz	O
(	O
cid:0	O
)	O
r	O
.˛/	O
c	O
p	O
.˛/2	O
(	O
cid:1	O
)	O
e	O
.˛/	O
dz	O
x	O
y2e˛yf0.x/	O
dx	O
;	O
(	O
5.78	O
)	O
x	O
or	O
successive	O
derivatives	O
of	O
.˛/	O
yield	O
the	O
higher	O
cumulants	O
of	O
y	O
,	O
its	O
skew-	O
ness	O
,	O
kurtosis	O
,	O
etc	O
.	O
10	O
[	O
p.	O
67	O
]	O
mle	O
for	O
ˇ.	O
the	O
gradient	O
with	O
respect	O
to	O
˛	O
of	O
log	O
f˛.y/	O
(	O
5.50	O
)	O
is	O
r	O
.˛/	O
d	O
e˛fy2g	O
(	O
cid:0	O
)	O
e˛fyg2	O
d	O
var˛fyg	O
:	O
y	O
(	O
cid:0	O
)	O
.˛/	O
(	O
cid:1	O
)	O
d	O
y	O
(	O
cid:0	O
)	O
p	O
.˛/	O
d	O
y	O
(	O
cid:0	O
)	O
e˛fy	O
(	O
cid:0	O
)	O
˛	O
0	O
(	O
5.79	O
)	O
(	O
cid:3	O
)	O
g	O
;	O
r˛	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
represents	O
a	O
hypothetical	O
realization	O
y.x	O
(	O
5.56	O
)	O
,	O
where	O
y	O
f˛.	O
(	O
cid:1	O
)	O
/	O
.	O
we	O
achieve	O
the	O
mle	O
o˛	O
at	O
ro˛	O
d	O
0	O
,	O
or	O
(	O
cid:3	O
)	O
g	O
d	O
y	O
:	O
eo˛fy	O
(	O
5.81	O
)	O
in	O
other	O
words	O
the	O
mle	O
o˛	O
is	O
the	O
value	O
of	O
˛	O
that	O
makes	O
the	O
expectation	O
(	O
cid:3	O
)	O
g	O
match	O
the	O
observed	O
y.	O
thus	O
(	O
5.58	O
)	O
implies	O
that	O
the	O
mle	O
of	O
pa-	O
e˛fy	O
rameter	O
ˇ	O
is	O
y	O
.	O
/	O
drawn	O
from	O
(	O
5.80	O
)	O
part	O
ii	O
early	O
computer-age	O
methods	O
6	O
empirical	B
bayes	O
the	O
constraints	O
of	O
slow	O
mechanical	O
computation	O
molded	O
classical	O
statistics	B
into	O
a	O
mathematically	O
ingenious	O
theory	B
of	O
sharply	O
delimited	O
scope	O
.	O
emerg-	O
ing	O
after	O
the	O
second	O
world	O
war	O
,	O
electronic	O
computation	O
loosened	O
the	O
com-	O
putational	O
stranglehold	O
,	O
allowing	O
a	O
more	O
expansive	O
and	O
useful	O
statistical	O
methodology	O
.	O
some	O
revolutions	O
start	O
slowly	O
.	O
the	O
journals	O
of	O
the	O
1950s	O
continued	O
to	O
emphasize	O
classical	O
themes	O
:	O
pure	O
mathematical	O
development	O
typically	O
cen-	O
tered	O
around	O
the	O
normal	B
distribution	O
.	O
change	O
came	O
gradually	O
,	O
but	O
by	O
the	O
1990s	O
a	O
new	O
statistical	O
technology	O
,	O
computer	O
enabled	O
,	O
was	O
ﬁrmly	O
in	O
place	O
.	O
key	O
developments	O
from	O
this	O
period	O
are	O
described	O
in	O
the	O
next	O
several	O
chap-	O
ters	O
.	O
the	O
ideas	O
,	O
for	O
the	O
most	O
part	O
,	O
would	O
not	O
startle	O
a	O
pre-war	O
statistician	O
,	O
but	O
their	O
computational	O
demands	O
,	O
factors	O
of	O
100	O
or	O
1000	O
times	O
those	O
of	O
classical	O
methods	O
,	O
would	O
.	O
more	O
factors	O
of	O
a	O
thousand	O
lay	O
ahead	O
,	O
as	O
will	O
be	O
told	O
in	O
part	O
iii	O
,	O
the	O
story	O
of	O
statistics	B
in	O
the	O
twenty-ﬁrst	O
century	O
.	O
empirical	B
bayes	O
methodology	O
,	O
this	O
chapter	O
’	O
s	O
topic	O
,	O
has	O
been	O
a	O
particu-	O
larly	O
slow	O
developer	O
despite	O
an	O
early	O
start	O
in	O
the	O
1940s	O
.	O
the	O
roadblock	O
here	O
was	O
not	O
so	O
much	O
the	O
computational	O
demands	O
of	O
the	O
theory	B
as	O
a	O
lack	O
of	O
ap-	O
propriate	O
data	B
sets	O
.	O
modern	O
scientiﬁc	O
equipment	O
now	O
provides	O
ample	O
grist	O
for	O
the	O
empirical	B
bayes	O
mill	O
,	O
as	O
will	O
be	O
illustrated	O
later	O
in	O
the	O
chapter	O
,	O
and	O
more	O
dramatically	O
in	O
chapters	O
15–21	O
.	O
6.1	O
robbins	O
’	O
formula	B
table	O
6.1	O
shows	O
one	O
year	O
of	O
claims	O
data	B
for	O
a	O
european	O
automobile	O
insur-	O
ance	O
company	O
;	O
7840	O
of	O
the	O
9461	O
policy	O
holders	O
made	O
no	O
claims	O
during	O
the	O
year	O
,	O
1317	O
made	O
a	O
single	O
claim	O
,	O
239	O
made	O
two	O
claims	O
each	O
,	O
etc.	O
,	O
with	O
ta-	O
ble	O
6.1	O
continuing	O
to	O
the	O
one	O
person	O
who	O
made	O
seven	O
claims	O
.	O
of	O
course	O
the	O
insurance	B
company	O
is	O
concerned	O
about	O
the	O
claims	O
each	O
policy	O
holder	O
will	O
make	O
in	O
the	O
next	O
year	O
.	O
bayes	O
’	O
formula	B
seems	O
promising	O
here	O
.	O
we	O
suppose	O
that	O
xk	O
,	O
the	O
number	O
75	O
76	O
empirical	B
bayes	O
table	O
6.1	O
counts	O
yx	O
of	O
number	O
of	O
claims	O
x	O
made	O
in	O
a	O
single	O
year	O
by	O
9461	O
automobile	O
insurance	B
policy	O
holders	O
.	O
robbins	O
’	O
formula	B
(	O
6.7	O
)	O
estimates	O
the	O
number	O
of	O
claims	O
expected	O
in	O
a	O
succeeding	O
year	O
,	O
for	O
instance	O
0:168	O
for	O
a	O
customer	O
in	O
the	O
x	O
d	O
0	O
category	O
.	O
parametric	B
maximum	O
likelihood	B
analysis	O
based	O
on	O
a	O
gamma	B
prior	O
gives	O
less	O
noisy	O
estimates	O
.	O
claims	O
x	O
counts	O
yx	O
formula	B
(	O
6.7	O
)	O
gamma	B
mle	O
0	O
7840	O
.168	O
.164	O
1	O
1317	O
.363	O
.398	O
2	O
239	O
.527	O
.633	O
3	O
42	O
1.33	O
.87	O
4	O
14	O
1.43	O
1.10	O
5	O
4	O
6.00	O
1.34	O
6	O
4	O
1.75	O
1.57	O
7	O
1	O
of	O
claims	O
to	O
be	O
made	O
in	O
a	O
single	O
year	O
by	O
policy	O
holder	O
k	O
,	O
follows	O
a	O
poisson	O
distribution	B
with	O
parameter	O
k	O
,	O
prfxk	O
d	O
xg	O
d	O
pk	O
.x/	O
d	O
e	O
(	O
cid:0	O
)	O
k	O
	O
x	O
(	O
6.1	O
)	O
for	O
x	O
d	O
0	O
;	O
1	O
;	O
2	O
;	O
3	O
;	O
:	O
:	O
:	O
;	O
k	O
is	O
the	O
expected	O
value	O
of	O
xk	O
.	O
a	O
good	O
customer	O
,	O
from	O
the	O
company	O
’	O
s	O
point	O
of	O
view	O
,	O
has	O
a	O
small	O
value	O
of	O
k	O
,	O
though	O
in	O
any	O
one	O
year	O
his	O
or	O
her	O
actual	O
number	O
of	O
accidents	O
xk	O
will	O
vary	O
randomly	O
ac-	O
cording	O
to	O
probability	O
density	B
(	O
6.1	O
)	O
.	O
k	O
=xš	O
;	O
suppose	O
we	O
knew	O
the	O
prior	B
density	O
g.	O
/	O
for	O
the	O
customers	O
’	O
	O
values	O
.	O
then	O
bayes	O
’	O
rule	B
(	O
3.5	O
)	O
would	O
yield	O
efjxg	O
d	O
0	O
p	O
.x/g.	O
/	O
d	O
0	O
p	O
.x/g.	O
/	O
d	O
(	O
6.2	O
)	O
for	O
the	O
expected	O
value	O
of	O
	O
of	O
a	O
customer	O
observed	O
to	O
make	O
x	O
claims	O
in	O
a	O
single	O
year	O
.	O
this	O
would	O
answer	O
the	O
insurance	B
company	O
’	O
s	O
question	O
of	O
what	O
number	O
of	O
claims	O
x	O
to	O
expect	O
the	O
next	O
year	O
from	O
the	O
same	O
customer	O
,	O
since	O
efjxg	O
is	O
also	O
efxjxg	O
(	O
	O
being	O
the	O
expectation	O
of	O
x	O
)	O
.	O
formula	B
(	O
6.2	O
)	O
is	O
just	O
the	O
ticket	O
if	O
the	O
prior	B
g.	O
/	O
is	O
known	O
to	O
the	O
company	O
,	O
but	O
what	O
if	O
it	O
is	O
not	O
?	O
a	O
clever	O
rewriting	O
of	O
(	O
6.2	O
)	O
provides	O
a	O
way	O
forward	O
.	O
using	O
(	O
6.1	O
)	O
,	O
(	O
6.2	O
)	O
becomes	O
r	O
1	O
r	O
1	O
efjxg	O
d	O
0	O
(	O
cid:2	O
)	O
e	O
(	O
cid:0	O
)	O
	O
	O
xc1=xš	O
(	O
cid:3	O
)	O
g.	O
/	O
d	O
r	O
1	O
(	O
cid:0	O
)	O
	O
	O
x=xš	O
(	O
cid:3	O
)	O
g.	O
/	O
d	O
(	O
cid:2	O
)	O
e	O
r	O
1	O
(	O
cid:2	O
)	O
e	O
(	O
cid:0	O
)	O
	O
	O
xc1=.x	O
c	O
1/š	O
(	O
cid:3	O
)	O
g.	O
/	O
d	O
d	O
.x	O
c	O
1/r	O
1	O
(	O
cid:0	O
)	O
	O
	O
x=xš	O
(	O
cid:3	O
)	O
g.	O
/	O
d	O
(	O
cid:2	O
)	O
e	O
r	O
1	O
0	O
0	O
0	O
(	O
6.3	O
)	O
:	O
o	O
f	O
.x/	O
d	O
yx=n	O
;	O
with	O
n	O
dp	O
f	O
.x	O
c	O
1/ı	O
o	O
o	O
f	O
.0/	O
d	O
7840=9461	O
,	O
version	O
of	O
robbins	O
’	O
formula	B
,	O
o	O
oefjxg	O
d	O
.x	O
c	O
1/	O
(	O
6.6	O
)	O
o	O
f	O
.1/	O
d	O
1317=9461	O
,	O
etc	O
.	O
this	O
yields	O
an	O
empirical	B
x	O
yx	O
;	O
the	O
total	O
count	O
;	O
the	O
marginal	O
density	B
of	O
x	O
,	O
integrating	O
p	O
.x/	O
over	O
the	O
prior	B
g.	O
/	O
,	O
is	O
6.1	O
robbins	O
’	O
formula	B
f	O
.x/	O
dz	O
1	O
p	O
.x/g.	O
/	O
d	O
dz	O
1	O
h	B
i	O
(	O
cid:0	O
)	O
	O
	O
x=xš	O
e	O
g.	O
/	O
d	O
:	O
(	O
6.4	O
)	O
77	O
0	O
0	O
comparing	O
(	O
6.3	O
)	O
with	O
(	O
6.4	O
)	O
gives	O
robbins	O
’	O
formula	B
,	O
efjxg	O
d	O
.x	O
c	O
1/f	O
.x	O
c	O
1/=f	O
.x/	O
:	O
(	O
6.5	O
)	O
the	O
surprising	O
and	O
gratifying	O
fact	O
is	O
that	O
,	O
even	O
with	O
no	O
knowledge	O
of	O
the	O
prior	B
density	O
g.	O
/	O
,	O
the	O
insurance	B
company	O
can	O
estimate	B
efjxg	O
(	O
6.2	O
)	O
from	O
formula	O
(	O
6.5	O
)	O
.	O
the	O
obvious	O
estimate	O
of	O
the	O
marginal	O
density	B
f	O
.x/	O
is	O
the	O
proportion	B
of	O
total	O
counts	O
in	O
category	O
x	O
,	O
f	O
.x/	O
d	O
.x	O
c	O
1/yxc1=yx	O
;	O
(	O
6.7	O
)	O
the	O
ﬁnal	O
expression	O
not	O
requiring	O
n	O
.	O
table	O
6.1	O
gives	O
oefj0g	O
d	O
0:168	O
:	O
customers	O
who	O
made	O
zero	O
claims	O
in	O
one	O
year	O
had	O
expectation	O
0.168	O
of	O
a	O
claim	O
the	O
next	O
year	O
;	O
those	O
with	O
one	O
claim	O
had	O
expectation	O
0.363	O
,	O
and	O
so	O
on	O
.	O
robbins	O
’	O
formula	B
came	O
as	O
a	O
surprise1	O
to	O
the	O
statistical	O
world	O
of	O
the	O
1950s	O
:	O
the	O
expectation	O
efkjxkg	O
for	O
a	O
single	O
customer	O
,	O
unavailable	O
without	O
the	O
prior	B
g.	O
/	O
,	O
somehow	O
becomes	O
available	O
in	O
the	O
context	O
of	O
a	O
large	O
study	O
.	O
the	O
terminology	O
empirical	B
bayes	O
is	O
apt	O
here	O
:	O
bayesian	O
formula	B
(	O
6.5	O
)	O
for	O
a	O
single	O
subject	O
is	O
estimated	O
empirically	O
(	O
i.e.	O
,	O
frequentistically	O
)	O
from	O
a	O
col-	O
lection	O
of	O
similar	O
cases	O
.	O
the	O
crucial	O
point	O
,	O
and	O
the	O
surprise	O
,	O
is	O
that	O
large	O
data	B
sets	O
of	O
parallel	O
situations	O
carry	O
within	O
them	O
their	O
own	O
bayesian	O
in-	O
formation	O
.	O
large	O
parallel	O
data	B
sets	O
are	O
a	O
hallmark	O
of	O
twenty-ﬁrst-century	O
scientiﬁc	O
investigation	O
,	O
promoting	O
the	O
popularity	O
of	O
empirical	B
bayes	O
meth-	O
ods	O
.	O
formula	B
(	O
6.7	O
)	O
goes	O
awry	O
at	O
the	O
right	O
end	O
of	O
table	O
6.1	O
,	O
where	O
it	O
is	O
destabi-	O
lized	O
by	O
small	O
count	O
numbers	O
.	O
a	O
parametric	B
approach	O
gives	O
more	O
depend-	O
able	O
results	O
:	O
now	O
we	O
assume	O
that	O
the	O
prior	B
density	O
g.	O
/	O
for	O
the	O
customers	O
’	O
k	O
values	O
has	O
a	O
gamma	B
form	O
(	O
table	O
5.1	O
)	O
g.	O
/	O
d	O
	O
(	O
cid:23	O
)	O
(	O
cid:0	O
)	O
1e	O
(	O
cid:0	O
)	O
=	O
(	O
cid:27	O
)	O
for	O
	O
(	O
cid:21	O
)	O
0	O
;	O
(	O
6.8	O
)	O
but	O
with	O
parameters	O
(	O
cid:23	O
)	O
and	O
(	O
cid:27	O
)	O
unknown	O
.	O
estimates	O
.o	O
(	O
cid:23	O
)	O
;	O
o	O
(	O
cid:27	O
)	O
/	O
are	O
obtained	O
by	O
(	O
cid:27	O
)	O
(	O
cid:23	O
)	O
	O
.	O
(	O
cid:23	O
)	O
/	O
;	O
1	O
perhaps	O
it	O
shouldn	O
’	O
t	B
have	O
;	O
estimation	B
methods	O
similar	O
to	O
(	O
6.7	O
)	O
were	O
familiar	O
in	O
the	O
actuarial	O
literature	O
.	O
78	O
empirical	B
bayes	O
maximum	B
likelihood	I
ﬁtting	O
to	O
the	O
counts	O
yx	O
,	O
yielding	O
a	O
parametrically	O
es-	O
timated	O
marginal	O
density	O
1	O
o	O
f	O
.x/	O
d	O
fo	O
(	O
cid:23	O
)	O
;	O
o	O
(	O
cid:27	O
)	O
.x/	O
;	O
or	O
equivalently	O
oyx	O
d	O
nfo	O
(	O
cid:23	O
)	O
;	O
o	O
(	O
cid:27	O
)	O
.x/	O
.	O
(	O
6.9	O
)	O
figure	O
6.1	O
auto	O
accident	O
data	B
;	O
log	O
(	O
counts	O
)	O
vs	O
claims	O
for	O
9461	O
auto	O
insurance	B
policies	O
.	O
the	O
dashed	O
line	O
is	O
a	O
gamma	B
mle	O
ﬁt	O
.	O
the	O
bottom	O
row	O
of	O
table	O
6.1	O
gives	O
parametric	B
estimates	O
eo	O
(	O
cid:23	O
)	O
;	O
o	O
(	O
cid:27	O
)	O
fjxg	O
d	O
.x	O
c	O
1/	O
oyxc1=	O
oyx	O
,	O
which	O
are	O
seen	O
to	O
be	O
less	O
eccentric	O
for	O
large	O
x.	O
figure	O
6.1	O
compares	O
(	O
on	O
the	O
log	O
scale	B
)	O
the	O
raw	O
counts	O
yx	O
with	O
their	O
parametric	B
cousins	O
oyx	O
.	O
6.2	O
the	O
missing-species	O
problem	O
the	O
very	O
ﬁrst	O
empirical	B
bayes	O
success	O
story	O
related	O
to	O
the	O
butterﬂy	O
data	B
of	O
table	O
6.2.	O
even	O
in	O
the	O
midst	O
of	O
world	O
war	O
ii	O
alexander	O
corbet	O
,	O
a	O
leading	O
naturalist	O
,	O
had	O
been	O
trapping	O
butterﬂies	O
for	O
two	O
years	O
in	O
malaysia	O
(	O
then	O
malaya	O
)	O
:	O
118	O
species	O
were	O
so	O
rare	O
that	O
he	O
had	O
trapped	O
only	O
one	O
specimen	O
each	O
,	O
74	O
species	O
had	O
been	O
trapped	O
twice	O
each	O
,	O
table	O
6.2	O
going	O
on	O
to	O
show	O
that	O
44	O
species	O
were	O
trapped	O
three	O
times	O
each	O
,	O
and	O
so	O
on	O
.	O
some	O
of	O
the	O
more	O
012345670246810claimslog	O
(	O
counts	O
)	O
llllllll	O
6.2	O
the	O
missing-species	O
problem	O
79	O
common	O
species	O
had	O
appeared	O
hundreds	O
of	O
times	O
each	O
,	O
but	O
of	O
course	O
corbet	O
was	O
interested	O
in	O
the	O
rarer	O
specimens	O
.	O
table	O
6.2	O
butterﬂy	O
data	B
;	O
number	O
y	O
of	O
species	O
seen	O
x	O
times	O
each	O
in	O
two	O
years	O
of	O
trapping	O
;	O
118	O
species	O
trapped	O
just	O
once	O
,	O
74	O
trapped	O
twice	O
each	O
,	O
etc	O
.	O
x	O
y	O
x	O
y	O
1	O
118	O
13	O
6	O
2	O
74	O
14	O
12	O
3	O
44	O
15	O
6	O
4	O
24	O
16	O
9	O
5	O
29	O
17	O
9	O
6	O
22	O
18	O
6	O
7	O
20	O
19	O
10	O
8	O
19	O
20	O
10	O
9	O
20	O
21	O
11	O
10	O
15	O
22	O
5	O
11	O
12	O
23	O
3	O
12	O
14	O
24	O
3	O
corbet	O
then	O
asked	O
a	O
seemingly	O
impossible	O
question	O
:	O
if	O
he	O
trapped	O
for	O
one	O
additional	O
year	O
,	O
how	O
many	O
new	O
species	O
would	O
he	O
expect	O
to	O
capture	O
?	O
the	O
question	O
relates	O
to	O
the	O
absent	O
entry	O
in	O
table	O
6.2	O
,	O
x	O
d	O
0	O
,	O
the	O
species	O
that	O
haven	O
’	O
t	B
been	O
seen	O
yet	O
.	O
do	O
we	O
really	O
have	O
any	O
evidence	O
at	O
all	O
for	O
answering	O
corbet	O
?	O
fortunately	O
he	O
asked	O
the	O
right	O
man	O
:	O
r.	O
a.	O
fisher	O
,	O
who	O
produced	O
a	O
surprisingly	O
satisfying	O
solution	O
for	O
the	O
“	O
missing-species	O
problem.	O
”	O
suppose	O
there	O
are	O
s	O
species	O
in	O
all	O
,	O
seen	O
or	O
unseen	O
,	O
and	O
that	O
xk	O
,	O
the	O
num-	O
ber	O
of	O
times	O
species	O
k	O
is	O
trapped	O
in	O
one	O
time	O
unit,2	O
follows	O
a	O
poisson	O
dis-	O
tribution	O
with	O
parameter	O
k	O
as	O
in	O
(	O
6.1	O
)	O
,	O
xk	O
(	O
cid:24	O
)	O
poi.k/	O
;	O
for	O
k	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
s	O
:	O
the	O
entries	O
in	O
table	O
6.2	O
are	O
yx	O
d	O
#	O
fxk	O
d	O
xg	O
;	O
for	O
x	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
24	O
;	O
(	O
6.10	O
)	O
(	O
6.11	O
)	O
the	O
number	O
of	O
species	O
trapped	O
exactly	O
x	O
times	O
each	O
.	O
now	O
consider	O
a	O
further	O
trapping	O
period	O
of	O
t	B
time	O
units	O
,	O
t	B
d	O
1=2	O
in	O
cor-	O
bet	O
’	O
s	O
question	O
,	O
and	O
let	O
xk.t	O
/	O
be	O
the	O
number	O
of	O
times	O
species	O
k	O
is	O
trapped	O
in	O
the	O
new	O
period	O
.	O
fisher	O
’	O
s	O
key	O
assumption	O
is	O
that	O
xk.t	O
/	O
(	O
cid:24	O
)	O
poi.kt	O
/	O
(	O
6.12	O
)	O
independently	O
of	O
xk	O
.	O
that	O
is	O
,	O
any	O
one	O
species	O
is	O
trapped	O
independently	O
over	O
time3	O
at	O
a	O
rate	B
proportional	O
to	O
its	O
parameter	O
k	O
.	O
the	O
probability	O
that	O
species	O
k	O
is	O
not	O
seen	O
in	O
the	O
initial	O
trapping	O
period	O
2	O
one	O
time	O
unit	O
equals	O
two	O
years	O
in	O
corbet	O
’	O
s	O
situation	O
.	O
3	O
this	O
is	O
the	O
deﬁnition	O
of	O
a	O
poisson	O
process	O
.	O
80	O
but	O
is	O
seen	O
in	O
the	O
new	O
period	O
,	O
that	O
is	O
xk	O
d	O
0	O
and	O
xk.t	O
/	O
>	O
0	O
,	O
is	O
empirical	B
bayes	O
;	O
(	O
cid:0	O
)	O
k	O
t	O
1	O
(	O
cid:0	O
)	O
e	O
(	O
cid:16	O
)	O
1	O
(	O
cid:0	O
)	O
e	O
(	O
cid:0	O
)	O
k	O
e	O
(	O
cid:0	O
)	O
k	O
(	O
cid:16	O
)	O
e.t	O
/	O
d	O
sx	O
z	O
1	O
(	O
cid:0	O
)	O
	O
(	O
cid:16	O
)	O
kd1	O
e	O
(	O
cid:0	O
)	O
k	O
t	O
(	O
cid:0	O
)	O
	O
t	O
so	O
that	O
e.t	O
/	O
,	O
the	O
expected	O
number	O
of	O
new	O
species	O
seen	O
in	O
the	O
new	O
trapping	O
period	O
,	O
is	O
(	O
6.13	O
)	O
:	O
(	O
6.14	O
)	O
it	O
is	O
convenient	O
to	O
write	O
(	O
6.14	O
)	O
as	O
an	O
integral	O
,	O
1	O
(	O
cid:0	O
)	O
e	O
e.t	O
/	O
d	O
s	O
e	O
0	O
g.	O
/	O
d	O
;	O
(	O
6.15	O
)	O
e	O
(	O
cid:0	O
)	O
	O
t	B
gives	O
z	O
1	O
expanding	O
1	O
(	O
cid:0	O
)	O
e	O
e.t	O
/	O
d	O
s	O
where	O
g.	O
/	O
is	O
the	O
“	O
empirical	B
density	O
”	O
putting	O
probability	O
1=s	O
on	O
each	O
of	O
the	O
k	O
values	O
.	O
(	O
later	O
we	O
will	O
think	O
of	O
g.	O
/	O
as	O
a	O
continuous	O
prior	B
density	O
on	O
the	O
possible	O
k	O
values	O
.	O
)	O
(	O
cid:0	O
)	O
	O
(	O
cid:2	O
)	O
	O
t	B
(	O
cid:0	O
)	O
.	O
t	B
/2=2š	O
c	O
.	O
t	B
/3=3š	O
(	O
cid:0	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:3	O
)	O
g.	O
/	O
d	O
:	O
ex	O
d	O
efyxg	O
d	O
sx	O
h	B
notice	O
that	O
the	O
expected	O
value	O
ex	O
of	O
yx	O
is	O
the	O
sum	O
of	O
the	O
probabilities	B
of	O
being	O
seen	O
exactly	O
x	O
times	O
in	O
the	O
initial	O
period	O
,	O
(	O
cid:0	O
)	O
k	O
	O
x	O
z	O
1	O
(	O
6.16	O
)	O
k	O
=xš	O
i	O
e	O
0	O
kd1	O
(	O
cid:0	O
)	O
	O
	O
x=xš	O
e	O
d	O
s	O
g.	O
/	O
d	O
:	O
0	O
comparing	O
(	O
6.16	O
)	O
with	O
(	O
6.17	O
)	O
provides	O
a	O
surprising	O
result	O
,	O
e.t	O
/	O
d	O
e1t	O
(	O
cid:0	O
)	O
e2t	O
2	O
c	O
e3t	O
3	O
(	O
cid:0	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
:	O
we	O
don	O
’	O
t	B
know	O
the	O
ex	O
values	O
but	O
,	O
as	O
in	O
robbins	O
’	O
formula	B
,	O
we	O
can	O
esti-	O
mate	O
them	O
by	O
the	O
yx	O
values	O
,	O
yielding	O
an	O
answer	O
to	O
corbet	O
’	O
s	O
question	O
,	O
oe.t	O
/	O
d	O
y1t	O
(	O
cid:0	O
)	O
y2t	O
2	O
c	O
y3t	O
3	O
(	O
cid:0	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
:	O
corbet	O
speciﬁed	O
t	B
d	O
1=2	O
,	O
so4	O
oe.1=2/	O
d	O
118.1=2/	O
(	O
cid:0	O
)	O
74.1=2/2	O
c	O
44.1=2/3	O
(	O
cid:0	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
d	O
45:2	O
:	O
(	O
6.17	O
)	O
(	O
6.18	O
)	O
(	O
6.19	O
)	O
(	O
6.20	O
)	O
4	O
this	O
may	O
have	O
been	O
discouraging	O
;	O
there	O
were	O
no	O
new	O
trapping	O
results	O
reported	O
.	O
6.2	O
the	O
missing-species	O
problem	O
81	O
table	O
6.3	O
expectation	O
(	O
6.19	O
)	O
and	O
its	O
standard	B
error	I
(	O
6.21	O
)	O
for	O
the	O
number	O
of	O
new	O
species	O
captured	O
in	O
t	B
additional	O
fractional	O
units	O
of	O
trapping	O
time	O
.	O
t	B
e.t	O
/	O
bsd.t	O
/	O
0.0	O
0.1	O
0.2	O
0.3	O
0.4	O
0.5	O
0.6	O
0.7	O
0.8	O
0.9	O
1.0	O
0	O
11.10	O
20.96	O
29.79	O
37.79	O
45.2	O
52.1	O
58.9	O
65.6	O
71.6	O
75.0	O
8.95	O
11.2	O
13.4	O
15.7	O
17.9	O
20.1	O
22.4	O
0	O
2.24	O
4.48	O
6.71	O
2	O
3	O
formulas	O
(	O
6.18	O
)	O
and	O
(	O
6.19	O
)	O
do	O
not	O
require	O
the	O
butterﬂies	O
to	O
arrive	O
inde-	O
pendently	O
.	O
if	O
we	O
are	O
willing	O
to	O
add	O
the	O
assumption	O
that	O
the	O
xk	O
’	O
s	O
are	O
mutually	O
independent	O
,	O
we	O
can	O
calculate	O
24x	O
bsd.t	O
/	O
d	O
!	O
1=2	O
as	O
an	O
approximate	O
standard	B
error	I
for	O
oe.t	O
/	O
.	O
table	O
6.3	O
shows	O
oe.t	O
/	O
andbsd.t	O
/	O
for	O
t	B
d	O
0	O
;	O
0:1	O
;	O
0:2	O
;	O
:	O
:	O
:	O
;	O
1	O
;	O
in	O
particular	O
,	O
xd1	O
yxt	O
2x	O
(	O
6.21	O
)	O
oe.0:5/	O
d	O
45:2	O
˙	O
11:2	O
:	O
(	O
6.22	O
)	O
formula	B
(	O
6.19	O
)	O
becomes	O
unstable	O
for	O
t	B
>	O
1.	O
this	O
is	O
our	O
price	O
for	O
sub-	O
stituting	O
the	O
nonparametric	B
estimates	O
yx	O
for	O
ex	O
in	O
(	O
6.18	O
)	O
.	O
fisher	O
actually	O
answered	O
corbet	O
using	O
a	O
parametric	B
empirical	O
bayes	O
model	B
in	O
which	O
the	O
prior	B
g.	O
/	O
for	O
the	O
poisson	O
parameters	O
k	O
(	O
6.12	O
)	O
was	O
assumed	O
to	O
be	O
of	O
the	O
gamma	B
form	O
(	O
6.8	O
)	O
.	O
it	O
can	O
be	O
shown	O
that	O
then	O
e.t	O
/	O
(	O
6.15	O
)	O
is	O
given	O
by	O
e.t	O
/	O
d	O
e1	O
f1	O
(	O
cid:0	O
)	O
.1	O
c	O
(	O
cid:13	O
)	O
t	B
/	O
(	O
6.23	O
)	O
where	O
(	O
cid:13	O
)	O
d	O
(	O
cid:27	O
)	O
=.1	O
c	O
(	O
cid:27	O
)	O
/	O
.	O
taking	O
oe1	O
d	O
y1	O
,	O
maximum	B
likelihood	I
estimation	O
gave	O
(	O
cid:0	O
)	O
(	O
cid:23	O
)	O
gı	O
.	O
(	O
cid:13	O
)	O
(	O
cid:23	O
)	O
/	O
;	O
o	O
(	O
cid:23	O
)	O
d	O
0:104	O
and	O
o	O
(	O
cid:27	O
)	O
d	O
89:79	O
:	O
(	O
6.24	O
)	O
figure	O
6.2	O
shows	O
that	O
the	O
parametric	B
estimate	I
of	O
e.t	O
/	O
(	O
6.23	O
)	O
using	O
oe1	O
,	O
o	O
(	O
cid:23	O
)	O
,	O
and	O
o	O
(	O
cid:27	O
)	O
is	O
just	O
slightly	O
greater	O
than	O
the	O
nonparametric	B
estimate	O
(	O
6.19	O
)	O
over	O
the	O
range	O
0	O
	O
t	B
	O
1.	O
fisher	O
’	O
s	O
parametric	B
estimate	I
,	O
however	O
,	O
gives	O
reason-	O
able	O
results	O
for	O
t	B
>	O
1	O
,	O
oe.2/	O
d	O
123	O
for	O
instance	O
,	O
for	O
a	O
future	O
trapping	O
period	O
of	O
2	O
units	O
(	O
4	O
years	O
)	O
.	O
“	O
reasonable	O
”	O
does	O
not	O
necessarily	O
mean	O
dependable	O
.	O
the	O
gamma	B
prior	O
is	O
a	O
mathematical	O
convenience	O
,	O
not	O
a	O
fact	O
of	O
nature	O
;	O
pro-	O
jections	O
into	O
the	O
far	O
future	O
fall	O
into	O
the	O
category	O
of	O
educated	O
guessing	O
.	O
the	O
missing-species	O
problem	O
encompasses	O
more	O
than	O
butterﬂies	O
.	O
there	O
are	O
884,647	O
words	O
in	O
total	O
in	O
the	O
recognized	O
shakespearean	O
canon	O
,	O
of	O
which	O
14,376	O
are	O
so	O
rare	O
they	O
appear	O
just	O
once	O
each	O
,	O
4343	O
appear	O
twice	O
each	O
,	O
etc.	O
,	O
82	O
empirical	B
bayes	O
figure	O
6.2	O
butterﬂy	O
data	B
;	O
expected	O
number	O
of	O
new	O
species	O
in	O
t	B
units	O
of	O
additional	O
trapping	O
time	O
.	O
nonparametric	B
ﬁt	O
(	O
solid	O
)	O
˙	O
1	O
standard	B
deviation	I
;	O
gamma	B
model	O
(	O
dashed	O
)	O
.	O
table	O
6.4	O
shakespeare	O
’	O
s	O
word	O
counts	O
;	O
14,376	O
distinct	O
words	O
appeared	O
once	O
each	O
in	O
the	O
canon	O
,	O
4343	O
distinct	O
words	O
twice	O
each	O
,	O
etc	O
.	O
the	O
canon	O
has	O
884,647	O
words	O
in	O
total	O
,	O
counting	O
repeats	O
.	O
1	O
0c	O
14376	O
10c	O
305	O
20c	O
104	O
30c	O
73	O
40c	O
49	O
50c	O
25	O
60c	O
30	O
70c	O
13	O
80c	O
13	O
90c	O
4	O
2	O
4343	O
259	O
105	O
47	O
41	O
19	O
19	O
12	O
12	O
7	O
3	O
2292	O
242	O
99	O
56	O
30	O
28	O
21	O
10	O
11	O
6	O
4	O
1463	O
223	O
112	O
59	O
35	O
27	O
18	O
16	O
8	O
7	O
5	O
1043	O
187	O
93	O
53	O
37	O
31	O
15	O
18	O
10	O
10	O
6	O
837	O
181	O
74	O
45	O
21	O
19	O
10	O
11	O
11	O
10	O
7	O
638	O
179	O
83	O
34	O
41	O
19	O
15	O
8	O
7	O
15	O
8	O
519	O
130	O
76	O
49	O
30	O
22	O
14	O
15	O
12	O
7	O
9	O
430	O
127	O
72	O
45	O
28	O
23	O
11	O
12	O
9	O
7	O
10	O
364	O
128	O
63	O
52	O
19	O
14	O
16	O
7	O
8	O
5	O
as	O
in	O
table	O
6.4	O
,	O
which	O
goes	O
on	O
to	O
the	O
ﬁve	O
words	O
appearing	O
100	O
times	O
each	O
.	O
all	O
told	O
,	O
31,534	O
distinct	O
words	O
appear	O
(	O
including	O
those	O
that	O
appear	O
more	O
than	O
100	O
times	O
each	O
)	O
,	O
this	O
being	O
the	O
observed	O
size	O
of	O
shakespeare	O
’	O
s	O
vocab-	O
ulary	O
.	O
but	O
what	O
of	O
the	O
words	O
shakespeare	O
knew	O
but	O
didn	O
’	O
t	B
use	O
?	O
these	O
are	O
the	O
“	O
missing	O
species	O
”	O
in	O
table	O
6.4	O
.	O
0.00.20.40.60.81.0020406080time	O
te^	O
(	O
t	B
)	O
gamma	B
model	O
e^	O
(	O
2	O
)	O
=	O
123e^	O
(	O
4	O
)	O
=	O
176e^	O
(	O
8	O
)	O
=	O
233	O
6.2	O
the	O
missing-species	O
problem	O
83	O
suppose	O
another	O
quantity	B
of	O
previously	O
unknown	O
shakespeare	O
manu-	O
scripts	O
was	O
discovered	O
,	O
comprising	O
884647	O
(	O
cid:1	O
)	O
t	B
words	O
(	O
so	O
t	B
d	O
1	O
would	O
rep-	O
resent	O
a	O
new	O
canon	O
just	O
as	O
large	O
as	O
the	O
old	O
one	O
)	O
.	O
how	O
many	O
previously	O
unseen	O
distinct	O
words	O
would	O
we	O
expect	O
to	O
discover	O
?	O
employing	O
formulas	O
(	O
6.19	O
)	O
and	O
(	O
6.21	O
)	O
gives	O
11430	O
˙	O
178	O
(	O
6.25	O
)	O
for	O
the	O
expected	O
number	O
of	O
distinct	O
new	O
words	O
if	O
t	B
d	O
1.	O
this	O
is	O
a	O
very	O
con-	O
servative	O
lower	O
bound	B
on	O
how	O
many	O
words	O
shakespeare	O
knew	O
but	O
didn	O
’	O
t	B
use	O
.	O
we	O
can	O
imagine	O
t	B
rising	O
toward	O
inﬁnity	O
,	O
revealing	O
ever	O
more	O
unseen	O
vocabulary	O
.	O
formula	B
(	O
6.19	O
)	O
fails	O
for	O
t	B
>	O
1	O
,	O
and	O
fisher	O
’	O
s	O
gamma	B
assump-	O
tion	O
is	O
just	O
that	O
,	O
but	O
more	O
elaborate	O
empirical	B
bayes	O
calculations	O
give	O
a	O
ﬁrm	O
lower	O
bound	B
of	O
35	O
;	O
000c	O
on	O
shakespeare	O
’	O
s	O
unseen	O
vocabulary	O
,	O
exceeding	O
the	O
visible	O
portion	O
!	O
missing	O
mass	O
is	O
an	O
easier	O
version	O
of	O
the	O
missing-species	O
problem	O
,	O
in	O
which	O
we	O
only	O
ask	O
for	O
the	O
proportion	B
of	O
the	O
total	O
sum	O
of	O
k	O
values	O
corre-	O
sponding	O
to	O
the	O
species	O
that	O
went	O
unseen	O
in	O
the	O
original	O
trapping	O
period	O
,	O
the	O
numerator	O
has	O
expectation	O
(	O
cid:30	O
)	O
x	O
m	O
d	O
x	O
z	O
1	O
unseen	O
k	O
all	O
(	O
cid:0	O
)	O
k	O
d	O
s	O
0	O
k	O
:	O
(	O
cid:0	O
)	O
	O
g.	O
/	O
d	O
e1	O
e	O
(	O
x	O
)	O
ke	O
x	O
k	O
dx	O
all	O
x	O
(	O
6.26	O
)	O
(	O
6.27	O
)	O
(	O
6.29	O
)	O
(	O
6.30	O
)	O
as	O
in	O
(	O
6.17	O
)	O
,	O
while	O
the	O
expectation	O
of	O
the	O
denominator	O
is	O
efxsg	O
d	O
e	O
d	O
efng	O
;	O
(	O
6.28	O
)	O
xs	O
all	O
all	O
all	O
where	O
n	O
is	O
the	O
total	O
number	O
of	O
butterﬂies	O
trapped	O
.	O
the	O
obvious	O
missing-	O
mass	O
estimate	B
is	O
then	O
om	O
d	O
y1=n	O
:	O
for	O
the	O
shakespeare	O
data	B
,	O
om	O
d	O
14376=884647	O
d	O
0:016	O
:	O
we	O
have	O
seen	O
most	O
of	O
shakespeare	O
’	O
s	O
vocabulary	O
,	O
as	O
weighted	O
by	O
his	O
usage	O
,	O
though	O
not	O
by	O
his	O
vocabulary	O
count	O
.	O
all	O
of	O
this	O
seems	O
to	O
live	O
in	O
the	O
rareﬁed	O
world	O
of	O
mathematical	O
abstrac-	O
tion	O
,	O
but	O
in	O
fact	O
some	O
previously	O
unknown	O
shakespearean	O
work	O
might	O
have	O
84	O
empirical	B
bayes	O
been	O
discovered	O
in	O
1985.	O
a	O
short	O
poem	O
,	O
“	O
shall	O
i	O
die	O
?	O
,	O
”	O
was	O
found	O
in	O
the	O
archives	O
of	O
the	O
bodleian	O
library	O
and	O
,	O
controversially	O
,	O
attributed	O
to	O
shake-	O
speare	O
by	O
some	O
but	O
not	O
all	O
experts	O
.	O
the	O
poem	O
of	O
429	O
words	O
provided	O
a	O
new	O
“	O
trapping	O
period	O
”	O
of	O
length	O
only	O
t	B
d	O
429=884647	O
d	O
4:85	O
(	O
cid:1	O
)	O
10	O
(	O
cid:0	O
)	O
4	O
;	O
(	O
6.31	O
)	O
(	O
6.32	O
)	O
and	O
a	O
prediction	O
from	O
(	O
6.19	O
)	O
of	O
eftg	O
d	O
6:97	O
new	O
“	O
species	O
,	O
”	O
i.e.	O
,	O
distinct	O
words	O
not	O
appearing	O
in	O
the	O
canon	O
.	O
in	O
fact	O
there	O
were	O
nine	O
such	O
words	O
in	O
the	O
poem	O
.	O
similar	O
empirical	B
bayes	O
predictions	O
for	O
the	O
number	O
of	O
words	O
appearing	O
once	O
each	O
in	O
the	O
canon	O
,	O
twice	O
each	O
,	O
etc.	O
,	O
showed	O
reasonable	O
agreement	O
with	O
the	O
poem	O
’	O
s	O
counts	O
,	O
but	O
not	O
enough	O
to	O
stiﬂe	O
doubters	O
.	O
“	O
shall	O
i	O
die	O
?	O
”	O
is	O
currently	O
grouped	O
with	O
other	O
canonical	O
apocrypha	O
by	O
a	O
majority	O
of	O
experts	O
.	O
6.3	O
a	O
medical	O
example	O
the	O
reader	O
may	O
have	O
noticed	O
that	O
our	O
examples	O
so	O
far	O
have	O
not	O
been	O
par-	O
ticularly	O
computer	O
intensive	O
;	O
all	O
of	O
the	O
calculations	O
could	O
have	O
been	O
(	O
and	O
originally	O
were	O
)	O
done	O
by	O
hand.5	O
this	O
section	O
discusses	O
a	O
medical	O
study	O
where	O
the	O
empirical	B
bayes	O
analysis	B
is	O
more	O
elaborate	O
.	O
cancer	O
surgery	O
sometimes	O
involves	O
the	O
removal	O
of	O
surrounding	O
lymph	O
nodes	B
as	O
well	O
as	O
the	O
primary	O
target	O
at	O
the	O
site	O
.	O
figure	O
6.3	O
concerns	O
n	O
d	O
844	O
surgeries	O
,	O
each	O
reporting	O
n	O
d	O
#	O
nodes	B
removed	O
and	O
x	O
d	O
#	O
nodes	B
found	O
positive	O
;	O
(	O
6.33	O
)	O
“	O
positive	O
”	O
meaning	O
malignant	O
.	O
the	O
ratios	O
pk	O
d	O
xk=nk	O
;	O
k	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
;	O
(	O
6.34	O
)	O
are	O
described	O
in	O
the	O
histogram	O
.	O
a	O
large	O
proportion	B
of	O
them	O
,	O
340=844	O
or	O
40	O
%	O
,	O
were	O
zero	O
,	O
the	O
remainder	O
spreading	O
unevenly	O
between	O
zero	O
and	O
one	O
.	O
the	O
denominators	O
nk	O
ranged	O
from	O
1	O
to	O
69	O
,	O
with	O
a	O
mean	O
of	O
19	O
and	O
standard	O
deviation	O
of	O
11.	O
we	O
suppose	O
that	O
each	O
patient	O
has	O
some	O
true	O
probability	O
of	O
a	O
node	O
being	O
5	O
not	O
so	O
collecting	O
the	O
data	B
.	O
corbet	O
’	O
s	O
work	O
was	O
pre-computer	O
but	O
shakespeare	O
’	O
s	O
word	O
counts	O
were	O
done	O
electronically	O
.	O
twenty-ﬁrst-century	O
scientiﬁc	O
technology	O
excels	O
at	O
the	O
production	O
of	O
the	O
large	O
parallel-structured	O
data	B
sets	O
conducive	O
to	O
empirical	B
bayes	O
analysis	B
.	O
6.3	O
a	O
medical	O
example	O
85	O
figure	O
6.3	O
nodes	B
study	O
;	O
ratio	O
p	O
d	O
x=n	O
for	O
844	O
patients	O
;	O
n	O
d	O
number	O
of	O
nodes	O
removed	O
,	O
x	O
d	O
number	O
positive	O
.	O
positive	O
,	O
say	O
probability	O
k	O
for	O
patient	O
k	O
,	O
and	O
that	O
his	O
or	O
her	O
nodal	O
results	O
occur	O
independently	O
of	O
each	O
other	O
,	O
making	O
xk	O
binomial	B
,	O
xk	O
(	O
cid:24	O
)	O
bi.nk	O
;	O
k/	O
:	O
this	O
gives	O
pk	O
d	O
xk=nk	O
with	O
mean	O
and	O
variance	O
pk	O
(	O
cid:24	O
)	O
.k	O
;	O
k.1	O
(	O
cid:0	O
)	O
k/=nk/	O
;	O
(	O
6.35	O
)	O
(	O
6.36	O
)	O
so	O
that	O
k	O
is	O
estimated	O
more	O
accurately	O
when	O
nk	O
is	O
large	O
.	O
a	O
bayesian	O
analysis	B
would	O
begin	O
with	O
the	O
assumption	O
of	O
a	O
prior	B
density	O
g.	O
/	O
for	O
the	O
k	O
values	O
,	O
k	O
(	O
cid:24	O
)	O
g.	O
/	O
;	O
for	O
k	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
d	O
844	O
:	O
(	O
6.37	O
)	O
we	O
don	O
’	O
t	B
know	O
g.	O
/	O
,	O
but	O
the	O
parallel	O
nature	O
of	O
the	O
nodes	B
data	O
set—844	O
similar	O
cases—suggests	O
an	O
empirical	B
bayes	O
approach	O
.	O
as	O
a	O
ﬁrst	O
try	O
for	O
the	O
nodes	B
study	O
,	O
we	O
assume	O
that	O
logfg.	O
/g	O
is	O
a	O
fourth-degree	O
polynomial	O
in	O
	O
,	O
logfg˛.	O
/g	O
d	O
a0	O
c	O
4x	O
jd1	O
˛j	O
	O
ji	O
(	O
6.38	O
)	O
p	O
=	O
x/nfrequency0.00.20.40.60.81.0020406080100*340	O
86	O
g˛.	O
/	O
is	O
determined	O
by	O
the	O
parameter	O
vector	B
˛	O
d	O
.˛1	O
;	O
˛2	O
;	O
˛3	O
;	O
˛4/	O
since	O
,	O
given	O
˛	O
,	O
a0	O
can	O
be	O
calculated	O
from	O
the	O
requirement	O
that	O
empirical	B
bayes	O
z	O
1	O
0	O
)	O
(	O
a0	O
c	O
4x	O
g˛.	O
/	O
d	O
d	O
1	O
dz	O
1	O
!	O
f˛.xk/	O
dz	O
1	O
	O
xk	O
.1	O
(	O
cid:0	O
)	O
	O
/nk	O
(	O
cid:0	O
)	O
xk	O
g˛.	O
/	O
d	O
:	O
˛j	O
	O
j	O
exp	O
d	O
:	O
nk	O
xk	O
0	O
0	O
1	O
for	O
a	O
given	O
choice	O
of	O
˛	O
,	O
let	O
f˛.xk/	O
be	O
the	O
marginal	O
probability	O
of	O
the	O
observed	O
value	O
xk	O
for	O
patient	O
k	O
,	O
(	O
6.39	O
)	O
(	O
6.40	O
)	O
the	O
maximum	B
likelihood	I
estimate	O
of	O
˛	O
is	O
the	O
maximizer	O
(	O
nx	O
kd1	O
o˛	O
d	O
arg	O
max	O
˛	O
)	O
log	O
f˛.xk/	O
:	O
(	O
6.41	O
)	O
figure	O
6.4	O
estimated	O
prior	B
density	O
g.	O
/	O
for	O
the	O
nodes	B
study	O
;	O
59	O
%	O
of	O
patients	O
have	O
	O
	O
0:2	O
,	O
7	O
%	O
have	O
	O
(	O
cid:21	O
)	O
0:8.	O
figure	O
6.4	O
graphs	O
go˛.	O
/	O
,	O
the	O
empirical	B
bayes	O
estimate	B
for	O
the	O
prior	B
dis-	O
tribution	O
of	O
the	O
k	O
values	O
.	O
the	O
huge	O
spike	O
at	O
zero	O
in	O
figure	O
6.3	O
is	O
now	O
reduced	O
:	O
prfk	O
	O
0:01g	O
d	O
0:12	O
compared	O
with	O
the	O
38	O
%	O
of	O
the	O
pk	O
values	O
0.00.20.40.60.81.00.000.020.040.060.080.100.12qg^	O
(	O
q	O
)	O
–	O
sd	O
6.3	O
a	O
medical	O
example	O
87	O
z	O
1:00	O
go˛.	O
/	O
d	O
d	O
0:59	O
compared	O
with	O
less	O
than	O
0.01.	O
small	O
	O
values	O
are	O
still	O
the	O
rule	B
though	O
,	O
for	O
instance	O
z	O
0:20	O
go˛.	O
/	O
d	O
d	O
0:07	O
:	O
(	O
6.42	O
)	O
the	O
vertical	O
bars	O
in	O
figure	O
6.4	O
indicate	O
˙	O
one	O
standard	B
error	I
for	O
the	O
es-	O
timation	O
of	O
g.	O
/	O
.	O
the	O
curve	O
seems	O
to	O
have	O
been	O
estimated	O
very	O
accurately	O
,	O
at	O
least	O
if	O
we	O
assume	O
the	O
adequacy	O
of	O
model	B
(	O
6.37	O
)	O
.	O
chapter	O
21	O
describes	O
the	O
computations	B
involved	O
in	O
figure	O
6.4	O
.	O
0:80	O
0	O
the	O
posterior	B
distribution	I
of	O
k	O
given	O
xk	O
and	O
nk	O
is	O
estimated	O
according	O
to	O
bayes	O
’	O
rule	B
(	O
3.5	O
)	O
to	O
be	O
og.jxk	O
;	O
nk/	O
d	O
go˛.	O
/	O
!	O
	O
xk	O
.1	O
(	O
cid:0	O
)	O
	O
/nk	O
(	O
cid:0	O
)	O
xk	O
(	O
cid:30	O
)	O
fo˛.xk/	O
;	O
(	O
6.43	O
)	O
nk	O
xk	O
with	O
fo˛.xk/	O
from	O
(	O
6.40	O
)	O
.	O
figure	O
6.5	O
empirical	B
bayes	O
posterior	O
densities	O
of	O
	O
for	O
three	O
patients	O
,	O
given	O
x	O
d	O
number	O
of	O
positive	O
nodes	B
,	O
n	O
d	O
number	O
of	O
nodes	O
.	O
figure	O
6.5	O
graphs	O
og.jxk	O
;	O
nk/	O
for	O
three	O
choices	O
of	O
.xk	O
;	O
nk/	O
:	O
.7	O
;	O
32/	O
,	O
.3	O
;	O
6/	O
,	O
and	O
.17	O
;	O
18/	O
.	O
if	O
we	O
take	O
	O
(	O
cid:21	O
)	O
0:50	O
as	O
indicating	O
poor	O
prognosis	O
(	O
and	O
sug-	O
gesting	O
more	O
aggressive	O
follow-up	O
therapy	O
)	O
,	O
then	O
the	O
ﬁrst	O
patient	O
is	O
almost	O
surely	O
on	O
safe	O
ground	O
,	O
the	O
third	O
patient	O
almost	O
surely	O
needs	O
more	O
follow-up	O
therapy	O
and	O
the	O
situation	O
of	O
the	O
second	O
is	O
uncertain	O
.	O
0.00.20.40.60.81.00246qg	O
(	O
q	O
|	O
x	O
,	O
n	O
)	O
x=7	O
n=32x=17	O
n=18x=3	O
n=60.5	O
88	O
empirical	B
bayes	O
6.4	O
indirect	O
evidence	O
1	O
a	O
good	O
deﬁnition	O
of	O
a	O
statistical	O
argument	B
is	O
one	O
in	O
which	O
many	O
small	O
pieces	O
of	O
evidence	O
,	O
often	O
contradictory	O
,	O
are	O
combined	O
to	O
produce	O
an	O
overall	O
conclusion	O
.	O
in	O
the	O
clinical	O
trial	O
of	O
a	O
new	O
drug	O
,	O
for	O
instance	O
,	O
we	O
don	O
’	O
t	B
expect	O
the	O
drug	O
to	O
cure	O
every	O
patient	O
,	O
or	O
the	O
placebo	O
to	O
always	O
fail	O
,	O
but	O
eventually	O
perhaps	O
we	O
will	O
obtain	O
convincing	O
evidence	O
of	O
the	O
new	O
drug	O
’	O
s	O
efﬁcacy	O
.	O
the	O
clinical	O
trial	O
is	O
collecting	O
direct	O
statistical	O
evidence	O
,	O
in	O
which	O
each	O
subject	O
’	O
s	O
success	O
or	O
failure	O
bears	O
directly	O
upon	O
the	O
question	O
of	O
interest	O
.	O
di-	O
rect	O
evidence	O
,	O
interpreted	O
by	O
frequentist	O
methods	O
,	O
was	O
the	O
dominant	O
mode	O
of	O
statistical	O
application	O
in	O
the	O
twentieth	O
century	O
,	O
being	O
strongly	B
connected	O
to	O
the	O
idea	O
of	O
scientiﬁc	O
objectivity	O
.	O
bayesian	O
inference	B
provides	O
a	O
theoretical	O
basis	O
for	O
incorporating	O
indi-	O
rect	O
evidence	O
,	O
for	O
example	O
the	O
doctor	O
’	O
s	O
prior	B
experience	O
with	O
twin	O
sexes	O
in	O
section	O
3.1.	O
the	O
assertion	O
of	O
a	O
prior	B
density	O
g.	O
/	O
amounts	O
to	O
a	O
claim	O
for	O
the	O
relevance	O
of	O
past	O
data	B
to	O
the	O
case	O
at	O
hand	O
.	O
empirical	B
bayes	O
removes	O
the	O
bayes	O
scaffolding	O
.	O
in	O
place	O
of	O
a	O
reassuring	O
prior	B
g.	O
/	O
,	O
the	O
statistician	O
must	O
put	O
his	O
or	O
her	O
faith	O
in	O
the	O
relevance	O
of	O
the	O
“	O
other	O
”	O
cases	O
in	O
a	O
large	O
data	B
set	O
to	O
the	O
case	O
of	O
direct	O
interest	O
.	O
for	O
the	O
second	O
patient	O
in	O
figure	O
6.5	O
,	O
the	O
direct	O
estimate	O
of	O
his	O
	O
value	O
is	O
o	O
	O
d	O
3=6	O
d	O
0:50.	O
the	O
empirical	B
bayes	O
estimate	B
is	O
a	O
little	O
less	O
,	O
	O
eb	O
dz	O
1	O
o	O
	O
og.jxk	O
d	O
3	O
;	O
nk	O
d	O
6/	O
d	O
0:446	O
:	O
(	O
6.44	O
)	O
0	O
a	O
small	O
difference	O
,	O
but	O
we	O
will	O
see	O
bigger	O
ones	O
in	O
succeeding	O
chapters	O
.	O
the	O
changes	O
in	O
twenty-ﬁrst-century	O
statistics	B
have	O
largely	O
been	O
demand	O
driven	O
,	O
responding	O
to	O
the	O
massive	O
data	B
sets	O
enabled	O
by	O
modern	O
scientiﬁc	O
equipment	O
.	O
philosophically	O
,	O
as	O
opposed	O
to	O
methodologically	O
,	O
the	O
biggest	O
change	O
has	O
been	O
the	O
increased	O
acceptance	O
of	O
indirect	O
evidence	O
,	O
especially	O
as	O
seen	O
in	O
empirical	B
bayes	O
and	O
objective	O
(	O
“	O
uninformative	O
”	O
)	O
bayes	O
appli-	O
cations	O
.	O
false-discovery	O
rates	O
,	O
chapter	O
15	O
,	O
provide	O
a	O
particularly	O
striking	O
shift	O
from	O
direct	O
to	O
indirect	O
evidence	O
in	O
hypothesis	B
testing	I
.	O
indirect	O
evi-	O
dence	O
in	O
estimation	B
is	O
the	O
subject	O
of	O
our	O
next	O
chapter	O
.	O
6.5	O
notes	O
and	O
details	O
robbins	O
(	O
1956	O
)	O
introduced	O
the	O
term	O
“	O
empirical	B
bayes	O
”	O
as	O
well	O
as	O
rule	B
(	O
6.7	O
)	O
as	O
part	O
of	O
a	O
general	O
theory	B
of	O
empirical	B
bayes	O
estimation	B
.	O
1956	O
was	O
also	O
the	O
publication	O
year	O
for	O
good	O
and	O
toulmin	O
’	O
s	O
solution	O
(	O
6.19	O
)	O
to	O
the	O
missing-	O
species	O
problem	O
.	O
good	O
went	O
out	O
of	O
his	O
way	O
to	O
credit	O
his	O
famous	O
bletchley	O
6.5	O
notes	O
and	O
details	O
89	O
colleague	O
alan	O
turing	O
for	O
some	O
of	O
the	O
ideas	O
.	O
the	O
auto	O
accident	O
data	B
is	O
taken	O
from	O
table	O
3.1	O
of	O
carlin	O
and	O
louis	O
(	O
1996	O
)	O
,	O
who	O
provide	O
a	O
more	O
complete	O
discussion	O
.	O
empirical	B
bayes	O
estimates	O
such	O
as	O
11430	O
in	O
(	O
6.25	O
)	O
do	O
not	O
de-	O
pend	O
on	O
independence	O
among	O
the	O
“	O
species	O
,	O
”	O
but	O
accuracies	O
such	O
as	O
˙178	O
do	O
;	O
and	O
similarly	O
for	O
the	O
error	O
bars	O
in	O
figures	O
6.2	O
and	O
6.4.	O
corbet	O
’	O
s	O
enormous	O
efforts	O
illustrate	O
the	O
difﬁculties	O
of	O
amassing	O
large	O
data	B
sets	O
in	O
pre-computer	O
times	O
.	O
dependable	O
data	B
is	O
still	O
hard	O
to	O
come	O
by	O
,	O
but	O
these	O
days	O
it	O
is	O
often	O
the	O
statistician	O
’	O
s	O
job	O
to	O
pry	O
it	O
out	O
of	O
enormous	O
databases	O
.	O
efron	O
and	O
thisted	O
(	O
1976	O
)	O
apply	O
formula	B
(	O
6.19	O
)	O
to	O
the	O
shake-	O
speare	O
word	O
counts	O
,	O
and	O
then	O
use	O
linear	B
programming	O
methods	O
to	O
bound	B
shakespeare	O
’	O
s	O
unseen	O
vocabulary	O
from	O
below	O
at	O
35,000	O
words	O
.	O
(	O
shake-	O
speare	O
was	O
actually	O
less	O
“	O
wordy	O
”	O
than	O
his	O
contemporaries	O
,	O
marlow	O
and	O
donne	O
.	O
)	O
“	O
shall	O
i	O
die	O
,	O
”	O
the	O
possibly	O
shakespearean	O
poem	O
recovered	O
in	O
1985	O
,	O
is	O
analyzed	O
by	O
a	O
variety	O
of	O
empirical	B
bayes	O
techniques	O
in	O
thisted	O
and	O
efron	O
(	O
1987	O
)	O
.	O
comparisons	O
are	O
made	O
with	O
other	O
elizabethan	O
authors	O
,	O
none	O
of	O
whom	O
seem	O
likely	O
candidates	O
for	O
authorship	O
.	O
the	O
shakespeare	O
word	O
counts	O
are	O
from	O
spevack	O
’	O
s	O
(	O
1968	O
)	O
concordance	O
.	O
(	O
the	O
ﬁrst	O
concordance	O
was	O
compiled	O
by	O
hand	O
in	O
the	O
mid	O
1800s	O
,	O
listing	O
every	O
word	O
shakespeare	O
wrote	O
and	O
where	O
it	O
appeared	O
,	O
a	O
full	B
life	O
’	O
s	O
labor	O
.	O
)	O
the	O
nodes	B
example	O
,	O
figure	O
6.3	O
,	O
is	O
taken	O
from	O
gholami	O
et	O
al	O
.	O
(	O
2015	O
)	O
.	O
1	O
[	O
p.	O
78	O
]	O
formula	B
(	O
6.9	O
)	O
.	O
for	O
any	O
positive	O
numbers	O
c	O
and	O
d	O
we	O
have	O
so	O
combining	O
gamma	B
prior	O
(	O
6.8	O
)	O
with	O
poisson	O
density	B
(	O
6.1	O
)	O
gives	O
marginal	O
density	B
z	O
1	O
0	O
	O
c	O
(	O
cid:0	O
)	O
1e	O
f	O
(	O
cid:23	O
)	O
;	O
(	O
cid:27	O
)	O
.x/	O
d	O
(	O
cid:0	O
)	O
=d	O
d	O
d	O
d	O
c.c/	O
;	O
r	O
1	O
0	O
	O
(	O
cid:23	O
)	O
cx	O
(	O
cid:0	O
)	O
1e	O
d	O
(	O
cid:13	O
)	O
(	O
cid:23	O
)	O
cx	O
.	O
(	O
cid:23	O
)	O
c	O
x/	O
(	O
cid:0	O
)	O
=	O
(	O
cid:13	O
)	O
d	O
(	O
cid:27	O
)	O
(	O
cid:23	O
)	O
	O
.	O
(	O
cid:23	O
)	O
/xš	O
;	O
(	O
cid:27	O
)	O
(	O
cid:23	O
)	O
	O
.	O
(	O
cid:23	O
)	O
/xš	O
(	O
6.45	O
)	O
(	O
6.46	O
)	O
where	O
(	O
cid:13	O
)	O
d	O
(	O
cid:27	O
)	O
=.1	O
c	O
(	O
cid:27	O
)	O
/	O
.	O
assuming	O
independence	O
among	O
the	O
counts	O
yx	O
(	O
which	O
is	O
exactly	O
true	O
if	O
the	O
customers	O
act	O
independently	O
of	O
each	O
other	O
and	O
n	O
,	O
the	O
total	O
number	O
of	O
them	O
,	O
is	O
itself	O
poisson	O
)	O
,	O
the	O
log	O
likelihood	B
function	O
for	O
the	O
accident	O
data	B
is	O
yx	O
logff	O
(	O
cid:23	O
)	O
;	O
(	O
cid:27	O
)	O
.x/g	O
:	O
(	O
6.47	O
)	O
xmaxx	O
xd0	O
here	O
xmax	O
is	O
some	O
notional	O
upper	O
bound	B
on	O
the	O
maximum	O
possible	O
number	O
empirical	O
bayes	O
90	O
of	O
accidents	O
for	O
a	O
single	O
customer	O
;	O
since	O
yx	O
d	O
0	O
for	O
x	O
>	O
7	O
the	O
choice	O
of	O
xmax	O
is	O
irrelevant	O
.	O
the	O
values	O
.o	O
(	O
cid:23	O
)	O
;	O
o	O
(	O
cid:27	O
)	O
/	O
in	O
(	O
6.8	O
)	O
maximize	O
(	O
6.47	O
)	O
.	O
2	O
[	O
p.	O
81	O
]	O
formula	B
(	O
6.21	O
)	O
.	O
if	O
n	O
dp	O
yx	O
,	O
the	O
total	O
number	O
trapped	O
,	O
is	O
assumed	O
to	O
be	O
poisson	O
,	O
and	O
if	O
the	O
n	O
observed	O
values	O
xk	O
are	O
mutually	O
independent	O
,	O
then	O
a	O
useful	O
property	O
of	O
the	O
poisson	O
distribution	B
implies	O
that	O
the	O
counts	O
yx	O
are	O
themselves	O
approximately	O
independent	O
poisson	O
variates	O
for	O
x	O
d	O
0	O
;	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
ind	O
(	O
cid:24	O
)	O
poi.ex/	O
;	O
(	O
6.48	O
)	O
yx	O
in	O
notation	O
(	O
6.17	O
)	O
.	O
formula	B
(	O
6.19	O
)	O
and	O
varfyxg	O
d	O
ex	O
then	O
give	O
ext	O
2x	O
:	O
(	O
6.49	O
)	O
n	O
oe.t	O
/	O
o	O
dx	O
x	O
(	O
cid:21	O
)	O
1	O
var	O
substituting	O
yx	O
for	O
ex	O
produces	O
(	O
6.21	O
)	O
.	O
section	O
11.5	O
of	O
efron	O
(	O
2010	O
)	O
shows	O
that	O
(	O
6.49	O
)	O
is	O
an	O
upper	O
bound	B
on	O
varf	O
oe.t	O
/g	O
if	O
n	O
is	O
considered	O
ﬁxed	O
rather	O
than	O
poisson	O
.	O
3	O
[	O
p.	O
81	O
]	O
formula	B
(	O
6.23	O
)	O
.	O
combining	O
the	O
case	O
x	O
d	O
1	O
in	O
(	O
6.17	O
)	O
with	O
(	O
6.15	O
)	O
(	O
cid:2	O
)	O
r	O
1	O
0	O
e	O
(	O
cid:0	O
)	O
	O
g.	O
/	O
d	O
(	O
cid:0	O
)	O
r	O
1	O
r	O
1	O
0	O
e	O
0	O
e	O
(	O
cid:0	O
)	O
	O
g.	O
/	O
d	O
(	O
cid:0	O
)	O
.1ct	O
/g.	O
/	O
d	O
(	O
cid:3	O
)	O
:	O
(	O
6.50	O
)	O
yields	O
e.t	O
/	O
d	O
e1	O
substituting	O
the	O
gamma	B
prior	O
(	O
6.8	O
)	O
for	O
g.	O
/	O
,	O
and	O
using	O
(	O
6.45	O
)	O
three	O
times	O
,	O
gives	O
formula	B
(	O
6.23	O
)	O
.	O
7	O
james–stein	O
estimation	B
and	O
ridge	B
regression	I
if	O
fisher	O
had	O
lived	O
in	O
the	O
era	O
of	O
“	O
apps	O
,	O
”	O
maximum	B
likelihood	I
estimation	O
might	O
have	O
made	O
him	O
a	O
billionaire	O
.	O
arguably	O
the	O
twentieth	O
century	O
’	O
s	O
most	O
inﬂuential	O
piece	O
of	O
applied	O
mathematics	O
,	O
maximum	B
likelihood	I
continues	O
to	O
be	O
a	O
prime	O
method	B
of	O
choice	O
in	O
the	O
statistician	O
’	O
s	O
toolkit	O
.	O
roughly	O
speaking	O
,	O
maximum	B
likelihood	I
provides	O
nearly	O
unbiased	O
estimates	O
of	O
nearly	O
mini-	O
mum	O
variance	O
,	O
and	O
does	O
so	O
in	O
an	O
automatic	O
way	O
.	O
that	O
being	O
said	O
,	O
maximum	B
likelihood	I
estimation	O
has	O
shown	O
itself	O
to	O
be	O
an	O
inadequate	O
and	O
dangerous	O
tool	O
in	O
many	O
twenty-ﬁrst-century	O
applica-	O
tions	O
.	O
again	O
speaking	O
roughly	O
,	O
unbiasedness	O
can	O
be	O
an	O
unaffordable	O
luxury	O
when	O
there	O
are	O
hundreds	O
or	O
thousands	O
of	O
parameters	O
to	O
estimate	B
at	O
the	O
same	O
time	O
.	O
the	O
james–stein	O
estimator	B
made	O
this	O
point	O
dramatically	O
in	O
1961	O
,	O
and	O
made	O
it	O
in	O
the	O
context	O
of	O
just	O
a	O
few	O
unknown	O
parameters	O
,	O
not	O
hundreds	O
or	O
thousands	O
.	O
it	O
begins	O
the	O
story	O
of	O
shrinkage	B
estimation	O
,	O
in	O
which	O
deliberate	O
biases	O
are	O
introduced	O
to	O
improve	O
overall	O
performance	O
,	O
at	O
a	O
possible	O
danger	O
to	O
individual	O
estimates	O
.	O
chapters	O
7	O
and	O
21	O
will	O
carry	O
on	O
the	O
story	O
in	O
its	O
modern	O
implementations	O
.	O
7.1	O
the	O
james–stein	O
estimator	B
suppose	O
we	O
wish	O
to	O
estimate	B
a	O
single	O
parameter	O
(	O
cid:22	O
)	O
from	O
observation	O
x	O
in	O
the	O
bayesian	O
situation	O
(	O
cid:22	O
)	O
(	O
cid:24	O
)	O
and	O
xj	O
(	O
cid:22	O
)	O
(	O
cid:24	O
)	O
(	O
7.1	O
)	O
n	O
.m	O
;	O
a/	O
n	O
.	O
(	O
cid:22	O
)	O
;	O
1/	O
;	O
in	O
which	O
case	O
(	O
cid:22	O
)	O
has	O
posterior	B
distribution	I
(	O
cid:22	O
)	O
jx	O
(	O
cid:24	O
)	O
n	O
.m	O
c	O
b.x	O
(	O
cid:0	O
)	O
m	O
/	O
;	O
b/	O
(	O
7.2	O
)	O
as	O
given	O
in	O
(	O
5.21	O
)	O
(	O
where	O
we	O
take	O
(	O
cid:27	O
)	O
2	O
d	O
1	O
for	O
convenience	O
)	O
.	O
the	O
bayes	O
estimator	B
of	O
(	O
cid:22	O
)	O
,	O
œb	O
d	O
a=.a	O
c	O
1/	O
o	O
(	O
cid:22	O
)	O
bayes	O
d	O
m	O
c	O
b.x	O
(	O
cid:0	O
)	O
m	O
/	O
;	O
(	O
7.3	O
)	O
91	O
(	O
7.4	O
)	O
92	O
james–stein	O
estimation	B
and	O
ridge	B
regression	I
has	O
expected	O
squared	O
error	O
e	O
compared	O
with	O
1	O
for	O
the	O
mle	O
o	O
(	O
cid:22	O
)	O
mle	O
d	O
x	O
,	O
n	O
(	O
cid:0	O
)	O
o	O
(	O
cid:22	O
)	O
bayes	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
(	O
cid:1	O
)	O
2o	O
d	O
b	O
;	O
n	O
(	O
cid:0	O
)	O
o	O
(	O
cid:22	O
)	O
mle	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
(	O
cid:1	O
)	O
2o	O
d	O
1	O
:	O
(	O
7.5	O
)	O
if	O
,	O
say	O
,	O
a	O
d	O
1	O
in	O
(	O
7.1	O
)	O
then	O
b	O
d	O
1=2	O
and	O
o	O
(	O
cid:22	O
)	O
bayes	O
has	O
only	O
half	O
the	O
risk	O
of	O
the	O
mle	O
.	O
e	O
the	O
same	O
calculation	O
applies	O
to	O
a	O
situation	O
where	O
we	O
have	O
n	O
indepen-	O
dent	O
versions	O
of	O
(	O
7.1	O
)	O
,	O
say	O
(	O
cid:22	O
)	O
d	O
.	O
(	O
cid:22	O
)	O
1	O
;	O
(	O
cid:22	O
)	O
2	O
;	O
:	O
:	O
:	O
;	O
(	O
cid:22	O
)	O
n	O
/	O
0	O
and	O
x	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn	O
/	O
0	O
;	O
(	O
7.6	O
)	O
with	O
(	O
cid:22	O
)	O
i	O
(	O
cid:24	O
)	O
and	O
xij	O
(	O
cid:22	O
)	O
i	O
(	O
cid:24	O
)	O
n	O
.m	O
;	O
a/	O
(	O
7.7	O
)	O
independently	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
.	O
(	O
notice	O
that	O
the	O
(	O
cid:22	O
)	O
i	O
differ	O
from	O
each	O
other	O
,	O
and	O
that	O
this	O
situation	O
is	O
not	O
the	O
same	O
as	O
(	O
5.22	O
)	O
–	O
(	O
5.23	O
)	O
.	O
)	O
let	O
o	O
(	O
cid:22	O
)	O
bayes	O
indicate	O
the	O
vector	B
of	O
individual	O
bayes	O
estimates	O
o	O
(	O
cid:22	O
)	O
d	O
m	O
cb.xi	O
(	O
cid:0	O
)	O
m	O
/	O
,	O
(	O
7.8	O
)	O
(	O
cid:2	O
)	O
m	O
d	O
.m	O
;	O
m	O
;	O
:	O
:	O
:	O
;	O
m	O
/	O
0	O
(	O
cid:3	O
)	O
;	O
o	O
(	O
cid:22	O
)	O
bayes	O
d	O
m	O
c	O
b.x	O
(	O
cid:0	O
)	O
m	O
/	O
;	O
n	O
.	O
(	O
cid:22	O
)	O
i	O
;	O
1/	O
;	O
bayes	O
i	O
and	O
similarly	O
o	O
(	O
cid:22	O
)	O
mle	O
d	O
x	O
:	O
2	O
using	O
(	O
7.4	O
)	O
the	O
total	O
squared	O
error	O
risk	O
of	O
o	O
(	O
cid:22	O
)	O
bayes	O
is	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
i	O
n	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
o	O
(	O
cid:22	O
)	O
bayes	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
(	O
nx	O
(	O
cid:16	O
)	O
o	O
(	O
cid:22	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
2o	O
d	O
e	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
2o	O
d	O
n	O
:	O
n	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
o	O
(	O
cid:22	O
)	O
mle	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
compared	O
with	O
bayes	O
i	O
id1	O
e	O
e	O
)	O
d	O
n	O
(	O
cid:1	O
)	O
b	O
(	O
7.9	O
)	O
(	O
7.10	O
)	O
again	O
,	O
o	O
(	O
cid:22	O
)	O
bayes	O
has	O
only	O
b	O
times	O
the	O
risk	O
of	O
o	O
(	O
cid:22	O
)	O
mle	O
.	O
this	O
is	O
ﬁne	O
if	O
we	O
know	O
m	O
and	O
a	O
(	O
or	O
equivalently	O
m	O
and	O
b	O
)	O
in	O
(	O
7.1	O
)	O
.	O
if	O
not	O
,	O
we	O
might	O
try	O
to	O
estimate	B
them	O
from	O
x	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn	O
/	O
.	O
marginally	O
,	O
(	O
7.7	O
)	O
gives	O
then	O
om	O
d	O
nx	O
is	O
an	O
unbiased	O
estimate	O
of	O
m	O
.	O
moreover	O
,	O
ind	O
(	O
cid:24	O
)	O
xi	O
n	O
.m	O
;	O
a	O
c	O
1/	O
:	O
''	O
s	O
d	O
nx	O
id1	O
#	O
(	O
7.11	O
)	O
(	O
7.12	O
)	O
.xi	O
(	O
cid:0	O
)	O
nx/2	O
ob	O
d	O
1	O
(	O
cid:0	O
)	O
.n	O
(	O
cid:0	O
)	O
3/=s	O
7.1	O
the	O
james–stein	O
estimator	B
93	O
unbiasedly	O
estimates	O
b	O
,	O
as	O
long	O
as	O
n	O
>	O
3	O
.	O
	O
the	O
james–stein	O
estimator	B
is	O
1	O
the	O
plug-in	O
version	O
of	O
(	O
7.3	O
)	O
,	O
(	O
cid:16	O
)	O
xi	O
(	O
cid:0	O
)	O
om	O
	O
2	O
o	O
(	O
cid:22	O
)	O
js	O
d	O
om	O
c	O
ob	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
;	O
i	O
(	O
7.13	O
)	O
or	O
equivalently	O
o	O
(	O
cid:22	O
)	O
js	O
d	O
om	O
c	O
ob.x	O
(	O
cid:0	O
)	O
om	O
/	O
,	O
with	O
om	O
d	O
.	O
om	O
;	O
om	O
;	O
:	O
:	O
:	O
;	O
om	O
/	O
0.	O
at	O
this	O
point	O
the	O
terminology	O
“	O
empirical	B
bayes	O
”	O
seems	O
especially	O
apt	O
:	O
bayesian	O
model	B
(	O
7.7	O
)	O
leads	O
to	O
the	O
bayes	O
estimator	B
(	O
7.8	O
)	O
,	O
which	O
itself	O
is	O
estimated	O
empirically	O
(	O
i.e.	O
,	O
frequentistically	O
)	O
from	O
all	O
the	O
data	B
x	O
,	O
and	O
then	O
applied	O
to	O
the	O
individual	O
cases	O
.	O
of	O
course	O
o	O
(	O
cid:22	O
)	O
js	O
can	O
not	O
perform	O
as	O
well	O
as	O
the	O
actual	O
bayes	O
’	O
rule	B
o	O
(	O
cid:22	O
)	O
bayes	O
,	O
but	O
the	O
increased	O
risk	O
is	O
surprisingly	O
modest	O
.	O
the	O
expected	O
squared	O
risk	O
of	O
o	O
(	O
cid:22	O
)	O
js	O
under	O
model	B
(	O
7.7	O
)	O
is	O
n	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
o	O
(	O
cid:22	O
)	O
js	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
2o	O
d	O
nb	O
c	O
3.1	O
(	O
cid:0	O
)	O
b/	O
:	O
e	O
(	O
7.14	O
)	O
if	O
,	O
say	O
,	O
n	O
d	O
20	O
and	O
a	O
d	O
1	O
,	O
then	O
(	O
7.14	O
)	O
equals	O
11.5	O
,	O
compared	O
with	O
true	O
bayes	O
risk	O
10	O
from	O
(	O
7.9	O
)	O
,	O
much	O
less	O
than	O
risk	O
20	O
for	O
o	O
(	O
cid:22	O
)	O
mle	O
.	O
a	O
defender	O
of	O
maximum	B
likelihood	I
might	O
respond	O
that	O
none	O
of	O
this	O
is	O
surprising	O
:	O
bayesian	O
model	B
(	O
7.7	O
)	O
speciﬁes	O
the	O
parameters	O
(	O
cid:22	O
)	O
i	O
to	O
be	O
clustered	O
more	O
or	O
less	O
closely	O
around	O
a	O
central	O
point	O
m	O
,	O
while	O
o	O
(	O
cid:22	O
)	O
mle	O
makes	O
no	O
such	O
assumption	O
,	O
and	O
can	O
not	O
be	O
expected	O
to	O
perform	O
as	O
well	O
.	O
wrong	O
!	O
removing	O
the	O
bayesian	O
assumptions	O
does	O
not	O
rescue	O
o	O
(	O
cid:22	O
)	O
mle	O
,	O
as	O
james	O
and	O
stein	O
proved	O
in	O
1961	O
:	O
james–stein	O
theorem	B
suppose	O
that	O
xij	O
(	O
cid:22	O
)	O
i	O
(	O
cid:24	O
)	O
(	O
7.15	O
)	O
independently	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
,	O
with	O
n	O
(	O
cid:21	O
)	O
4.	O
then	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
2o	O
n	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
o	O
(	O
cid:22	O
)	O
js	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
i	O
;	O
1/	O
n	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
o	O
(	O
cid:22	O
)	O
mle	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
2o	O
<	O
n	O
d	O
e	O
e	O
(	O
7.16	O
)	O
for	O
all	O
choices	O
of	O
(	O
cid:22	O
)	O
2	O
rn	O
.	O
(	O
the	O
expectations	O
in	O
(	O
7.16	O
)	O
are	O
with	O
(	O
cid:22	O
)	O
ﬁxed	O
and	O
x	O
varying	O
according	O
to	O
(	O
7.15	O
)	O
.	O
)	O
in	O
the	O
language	O
of	O
decision	O
theory	B
,	O
equation	B
(	O
7.16	O
)	O
says	O
that	O
o	O
(	O
cid:22	O
)	O
mle	O
is	O
inadmissible	O
:	O
	O
its	O
total	O
squared	O
error	O
risk	O
exceeds	O
that	O
of	O
o	O
(	O
cid:22	O
)	O
js	O
no	O
matter	O
3	O
what	O
(	O
cid:22	O
)	O
may	O
be	O
.	O
this	O
is	O
a	O
strong	O
frequentist	O
form	B
of	O
defeat	O
for	O
o	O
(	O
cid:22	O
)	O
mle	O
,	O
not	O
depending	O
on	O
bayesian	O
assumptions	O
.	O
the	O
james–stein	O
theorem	B
came	O
as	O
a	O
rude	O
shock	O
to	O
the	O
statistical	O
world	O
of	O
1961.	O
first	O
of	O
all	O
,	O
the	O
defeat	O
came	O
on	O
mle	O
’	O
s	O
home	O
ﬁeld	O
:	O
normal	B
observa-	O
tions	O
with	O
squared	O
error	O
loss	O
.	O
fisher	O
’	O
s	O
“	O
logic	O
of	O
inductive	O
inference	B
,	O
”	O
chap-	O
ter	O
4	O
,	O
claimed	O
that	O
o	O
(	O
cid:22	O
)	O
mle	O
d	O
x	O
was	O
the	O
obviously	O
correct	O
estimator	B
in	O
the	O
uni-	O
variate	O
case	O
,	O
an	O
assumption	O
tacitly	O
carried	O
forward	O
to	O
multiparameter	O
linear	B
james–stein	O
estimation	B
and	O
ridge	B
regression	I
94	O
regression	B
problems	O
,	O
where	O
versions	O
of	O
o	O
(	O
cid:22	O
)	O
mle	O
were	O
predominant	O
.	O
there	O
are	O
still	O
some	O
good	O
reasons	O
for	O
sticking	O
with	O
o	O
(	O
cid:22	O
)	O
mle	O
in	O
low-dimensional	O
prob-	O
lems	O
,	O
as	O
discussed	O
in	O
section	O
7.4.	O
but	O
shrinkage	B
estimation	O
,	O
as	O
exempliﬁed	O
by	O
the	O
james–stein	O
rule	B
,	O
has	O
become	O
a	O
necessity	O
in	O
the	O
high-dimensional	O
situations	O
of	O
modern	O
practice	O
.	O
7.2	O
the	O
baseball	B
players	O
the	O
james–stein	O
theorem	B
doesn	O
’	O
t	B
say	O
by	O
how	O
much	O
o	O
(	O
cid:22	O
)	O
js	O
beats	O
o	O
(	O
cid:22	O
)	O
mle	O
.	O
if	O
the	O
improvement	O
were	O
inﬁnitesimal	O
nobody	O
except	O
theorists	O
would	O
be	O
inter-	O
ested	O
.	O
in	O
favorable	O
situations	O
the	O
gains	O
can	O
in	O
fact	O
be	O
substantial	O
,	O
as	O
sug-	O
gested	O
by	O
(	O
7.14	O
)	O
.	O
one	O
such	O
situation	O
appears	O
in	O
table	O
7.1.	O
the	O
batting	O
av-	O
erages1	O
of	O
18	O
major	O
league	O
players	O
have	O
been	O
observed	O
over	O
the	O
1970	O
sea-	O
son	O
.	O
the	O
column	O
labeled	O
mle	O
reports	O
the	O
player	O
’	O
s	O
observed	O
average	O
over	O
his	O
ﬁrst	O
90	O
at	O
bats	O
;	O
truth	O
is	O
the	O
average	O
over	O
the	O
remainder	O
of	O
the	O
1970	O
season	O
(	O
370	O
further	O
at	O
bats	O
on	O
average	O
)	O
.	O
we	O
would	O
like	O
to	O
predict	O
truth	O
from	O
the	O
early-season	O
observations	O
.	O
the	O
column	O
labeled	O
js	O
in	O
table	O
7.1	O
is	O
from	O
a	O
version	O
of	O
the	O
james–	O
stein	O
estimator	B
applied	O
to	O
the	O
18	O
mle	O
numbers	O
.	O
we	O
suppose	O
that	O
each	O
player	O
’	O
s	O
mle	O
value	O
pi	O
(	O
his	O
batting	O
average	O
in	O
the	O
ﬁrst	O
90	O
tries	O
)	O
is	O
a	O
binomial	B
proportion	O
,	O
pi	O
(	O
cid:24	O
)	O
bi.90	O
;	O
pi	O
/=90	O
:	O
(	O
7.17	O
)	O
here	O
pi	O
is	O
his	O
true	O
average	O
,	O
how	O
he	O
would	O
perform	O
over	O
an	O
inﬁnite	O
number	O
of	O
tries	O
;	O
truthi	O
is	O
itself	O
a	O
binomial	B
proportion	O
,	O
taken	O
over	O
an	O
average	O
of	O
370	O
more	O
tries	O
per	O
player	O
.	O
at	O
this	O
point	O
there	O
are	O
two	O
ways	O
to	O
proceed	O
.	O
the	O
simplest	O
uses	O
a	O
normal	B
approximation	O
to	O
(	O
7.17	O
)	O
,	O
pi	O
p	O
(	O
cid:24	O
)	O
0	O
is	O
the	O
binomial	B
variance	O
where	O
(	O
cid:27	O
)	O
2	O
n	O
.pi	O
;	O
(	O
cid:27	O
)	O
2	O
0	O
/	O
;	O
(	O
7.18	O
)	O
d	O
np	O
c	O
opjs	O
d	O
np.1	O
(	O
cid:0	O
)	O
np/=90	O
;	O
(	O
cid:21	O
)	O
d	O
(	O
cid:27	O
)	O
0	O
o	O
(	O
cid:22	O
)	O
js	O
p.pi	O
(	O
cid:0	O
)	O
np/2	O
1	O
(	O
cid:0	O
)	O
.n	O
(	O
cid:0	O
)	O
3/	O
(	O
cid:27	O
)	O
2	O
0	O
i	O
(	O
7.19	O
)	O
with	O
np	O
d	O
0:254	O
the	O
average	O
of	O
the	O
pi	O
values	O
.	O
letting	O
xi	O
d	O
pi	O
=	O
(	O
cid:27	O
)	O
0	O
,	O
applying	O
(	O
7.13	O
)	O
,	O
and	O
transforming	O
back	O
to	O
opjs	O
i	O
,	O
gives	O
james–stein	O
estimates	O
(	O
cid:27	O
)	O
2	O
0	O
i	O
(	O
7.20	O
)	O
1	O
batting	O
average	O
d	O
#	O
hits	O
=	O
#	O
at	O
bats	O
,	O
that	O
is	O
,	O
the	O
success	O
rate	B
.	O
for	O
example	O
,	O
player	O
1	O
hits	O
successfully	O
31	O
times	O
in	O
his	O
ﬁrst	O
90	O
tries	O
,	O
for	O
batting	O
average	O
31=90	O
d	O
0:345.	O
this	O
data	B
is	O
based	O
on	O
1970	O
major	O
league	O
performances	O
,	O
but	O
is	O
partly	O
artiﬁcial	O
;	O
see	O
the	O
endnotes	O
.	O
.pi	O
(	O
cid:0	O
)	O
np/	O
:	O
7.2	O
the	O
baseball	B
players	O
95	O
table	O
7.1	O
eighteen	O
baseball	B
players	O
;	O
mle	O
is	O
batting	O
average	O
in	O
ﬁrst	O
90	O
at	O
bats	O
;	O
truth	O
is	O
average	O
in	O
remainder	O
of	O
1970	O
season	O
;	O
james–stein	O
estimator	B
js	O
is	O
based	O
on	O
arcsin	B
transformation	I
of	O
mles	O
.	O
sum	O
of	O
squared	O
errors	B
for	O
predicting	O
truth	O
:	O
mle	O
.0425	O
,	O
js	O
.0218.	O
player	O
mle	O
js	O
truth	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
11	O
12	O
13	O
14	O
15	O
16	O
17	O
18	O
.345	O
.333	O
.322	O
.311	O
.289	O
.289	O
.278	O
.255	O
.244	O
.233	O
.233	O
.222	O
.222	O
.222	O
.211	O
.211	O
.200	O
.145	O
.283	O
.279	O
.276	O
.272	O
.265	O
.264	O
.261	O
.253	O
.249	O
.245	O
.245	O
.242	O
.241	O
.241	O
.238	O
.238	O
.234	O
.212	O
.298	O
.346	O
.222	O
.276	O
.263	O
.273	O
.303	O
.270	O
.230	O
.264	O
.264	O
.210	O
.256	O
.269	O
.316	O
.226	O
.285	O
.200	O
x	O
11.96	O
11.74	O
11.51	O
11.29	O
10.83	O
10.83	O
10.60	O
10.13	O
9.88	O
9.64	O
9.64	O
9.40	O
9.39	O
9.39	O
9.14	O
9.14	O
8.88	O
7.50	O
''	O
	O
npi	O
c	O
0:375	O
1=2	O
#	O
a	O
second	O
approach	O
begins	O
with	O
the	O
arcsin	B
transformation	I
xi	O
d	O
2.n	O
c	O
0:5/1=2	O
sin	O
(	O
cid:0	O
)	O
1	O
(	O
7.21	O
)	O
n	O
d	O
90	O
(	O
column	O
labeled	O
x	O
in	O
table	O
7.1	O
)	O
,	O
a	O
classical	O
device	O
that	O
produces	O
approximate	O
normal	B
deviates	O
of	O
variance	O
1	O
,	O
n	O
c	O
0:75	O
;	O
xi	O
p	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
i	O
;	O
1/	O
;	O
(	O
7.22	O
)	O
where	O
(	O
cid:22	O
)	O
i	O
is	O
transformation	O
(	O
7.21	O
)	O
applied	O
to	O
truthi	O
.	O
using	O
(	O
7.13	O
)	O
gives	O
o	O
(	O
cid:22	O
)	O
js	O
i	O
,	O
which	O
is	O
ﬁnally	O
inverted	O
back	O
to	O
the	O
binomial	B
scale	O
,	O
''	O
opjs	O
i	O
d	O
1	O
n	O
n	O
c	O
0:75	O
n	O
c	O
0:5	O
sin	O
o	O
(	O
cid:22	O
)	O
js	O
i	O
#	O
2	O
(	O
cid:0	O
)	O
0:375	O
2	O
:	O
(	O
7.23	O
)	O
formulas	O
(	O
7.20	O
)	O
and	O
(	O
7.23	O
)	O
yielded	O
nearly	O
the	O
same	O
estimates	O
for	O
the	O
baseball	B
players	O
;	O
the	O
js	O
column	O
in	O
table	O
7.1	O
is	O
from	O
(	O
7.23	O
)	O
.	O
james	O
and	O
stein	O
’	O
s	O
theorem	B
requires	O
normality	O
,	O
but	O
the	O
james–stein	O
estimator	B
often	O
96	O
james–stein	O
estimation	B
and	O
ridge	B
regression	I
18x	O
.mlei	O
(	O
cid:0	O
)	O
truthi	O
/2	O
d	O
0:0425	O
while	O
18x	O
works	O
perfectly	O
well	O
in	O
less	O
ideal	O
situations	O
.	O
that	O
is	O
the	O
case	O
in	O
table	O
7.1	O
:	O
.jsi	O
(	O
cid:0	O
)	O
truthi	O
/2	O
d	O
0:0218	O
:	O
(	O
7.24	O
)	O
in	O
other	O
words	O
,	O
the	O
james–stein	O
estimator	B
reduced	O
total	O
predictive	O
squared	O
error	O
by	O
about	O
50	O
%	O
.	O
id1	O
id1	O
figure	O
7.1	O
eighteen	O
baseball	B
players	O
;	O
top	O
line	O
mle	O
,	O
middle	O
james–stein	O
,	O
bottom	O
true	O
values	O
.	O
only	O
13	O
points	O
are	O
visible	O
,	O
since	O
there	O
are	O
ties	O
.	O
the	O
james–stein	O
rule	B
describes	O
a	O
shrinkage	B
estimator	O
,	O
each	O
mle	O
value	O
xi	O
being	O
shrunk	O
by	O
factor	B
ob	O
toward	O
the	O
grand	O
mean	O
om	O
d	O
nx	O
(	O
7.13	O
)	O
.	O
(	O
ob	O
d	O
0:34	O
in	O
(	O
7.20	O
)	O
.	O
)	O
figure	O
7.1	O
illustrates	O
the	O
shrinking	O
process	O
for	O
the	O
baseball	B
players	O
.	O
to	O
see	O
why	O
shrinking	O
might	O
make	O
sense	O
,	O
let	O
us	O
return	O
to	O
the	O
original	O
bayes	O
model	B
(	O
7.8	O
)	O
and	O
take	O
m	O
d	O
0	O
for	O
simplicity	O
,	O
so	O
that	O
the	O
xi	O
are	O
marginally	O
n	O
.0	O
;	O
a	O
c	O
1/	O
(	O
7.11	O
)	O
.	O
even	O
though	O
each	O
xi	O
is	O
unbiased	O
for	O
its	O
(	O
nx	O
d	O
na	O
:	O
(	O
7.25	O
)	O
parameter	O
(	O
cid:22	O
)	O
i	O
,	O
as	O
a	O
group	O
they	O
are	O
“	O
overdispersed	O
,	O
”	O
d	O
n.a	O
c	O
1/	O
compared	O
with	O
e	O
)	O
)	O
e	O
x2	O
i	O
id1	O
(	O
nx	O
id1	O
(	O
cid:22	O
)	O
2	O
i	O
the	O
sum	O
of	O
squares	B
of	O
the	O
mles	O
exceeds	O
that	O
of	O
the	O
true	O
values	O
by	O
expected	O
amount	O
n	O
;	O
shrinkage	B
improves	O
group	O
estimation	B
by	O
removing	O
the	O
excess	O
.	O
0.150.200.250.300.35	O
llllllllllllllllllmlelllllllllllllllllljames−steinlllllllllllllllllltruebatting	O
averages	O
7.3	O
ridge	B
regression	I
97	O
bayes	O
i	O
bayes	O
i	O
(	O
nx	O
id1	O
e	O
a	O
a	O
c	O
1	O
;	O
d	O
bxi	O
have	O
d	O
nb	O
2.a	O
c	O
1/	O
d	O
na	O
in	O
fact	O
the	O
james–stein	O
rule	B
overshrinks	O
the	O
data	B
,	O
as	O
seen	O
in	O
the	O
bottom	O
two	O
lines	O
of	O
figure	O
7.1	O
,	O
a	O
property	O
it	O
inherits	O
from	O
the	O
underlying	O
bayes	O
)	O
model	B
:	O
the	O
bayes	O
estimates	O
o	O
(	O
cid:22	O
)	O
2	O
(	O
7.26	O
)	O
i	O
/	O
d	O
na	O
by	O
factor	B
a=.a	O
c	O
1/	O
.	O
we	O
could	O
use	O
the	O
bxi	O
,	O
which	O
gives	O
the	O
correct	O
expected	O
sum	O
of	O
squares	B
na	O
,	O
but	O
a	O
larger	O
expected	O
sum	O
of	O
squared	O
estimation	B
errors	O
(	O
cid:16	O
)	O
o	O
(	O
cid:22	O
)	O
overshrinking	O
e.p	O
(	O
cid:22	O
)	O
2	O
less	O
extreme	O
shrinking	O
rule	B
q	O
(	O
cid:22	O
)	O
i	O
d	O
p	O
efp	O
.	O
q	O
(	O
cid:22	O
)	O
i	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
i	O
/2jxg	O
.	O
pothesis	O
of	O
no	O
differences	O
among	O
the	O
(	O
cid:22	O
)	O
i	O
values	O
.	O
(	O
this	O
gavep.pi	O
(	O
cid:0	O
)	O
np/2	O
d	O
null	O
indicating	O
that	O
in	O
a	O
classical	O
sense	O
we	O
have	O
accepted	O
the	O
null	O
hy-	O
the	O
most	O
extreme	O
shrinkage	B
rule	O
would	O
be	O
“	O
all	O
the	O
way	O
,	O
”	O
that	O
is	O
,	O
to	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
;	O
o	O
(	O
cid:22	O
)	O
null	O
i	O
d	O
nx	O
(	O
7.27	O
)	O
0:0266	O
for	O
the	O
baseball	B
data	O
(	O
7.24	O
)	O
.	O
)	O
the	O
james–stein	O
estimator	B
is	O
a	O
data-	O
based	O
rule	B
for	O
compromising	O
between	O
the	O
null	O
hypothesis	O
of	O
no	O
differences	O
and	O
the	O
mle	O
’	O
s	O
tacit	O
assumption	O
of	O
no	O
relationship	O
at	O
all	O
among	O
the	O
(	O
cid:22	O
)	O
i	O
values	O
.	O
in	O
this	O
sense	O
it	O
blurs	O
the	O
classical	O
distinction	O
between	O
hypothesis	B
testing	I
and	O
estimation	B
.	O
7.3	O
ridge	B
regression	I
linear	O
regression	B
,	O
perhaps	O
the	O
most	O
widely	O
used	O
estimation	B
technique	O
,	O
is	O
based	O
on	O
a	O
version	O
of	O
o	O
(	O
cid:22	O
)	O
mle	O
.	O
in	O
the	O
usual	O
notation	O
,	O
we	O
observe	O
an	O
n-dimen-	O
sional	O
vector	B
y	O
d	O
.y1	O
;	O
y2	O
;	O
:	O
:	O
:	O
;	O
yn/	O
0	O
from	O
the	O
linear	B
model	I
y	O
d	O
x	O
ˇ	O
c	O
(	O
cid:15	O
)	O
:	O
(	O
7.28	O
)	O
here	O
x	O
is	O
a	O
known	O
n	O
(	O
cid:2	O
)	O
p	O
structure	O
matrix	B
,	O
ˇ	O
is	O
an	O
unknown	O
p-dimensional	O
parameter	O
vector	B
,	O
while	O
the	O
noise	O
vector	B
(	O
cid:15	O
)	O
d	O
.	O
(	O
cid:15	O
)	O
1	O
;	O
(	O
cid:15	O
)	O
2	O
;	O
:	O
:	O
:	O
;	O
(	O
cid:15	O
)	O
n/	O
0	O
has	O
its	O
com-	O
ponents	O
uncorrelated	O
and	O
with	O
constant	O
variance	O
(	O
cid:27	O
)	O
2	O
,	O
(	O
cid:15	O
)	O
(	O
cid:24	O
)	O
.0	O
;	O
(	O
cid:27	O
)	O
2i/	O
;	O
(	O
7.29	O
)	O
where	O
i	O
is	O
the	O
n	O
(	O
cid:2	O
)	O
n	O
identity	O
matrix	B
.	O
often	O
(	O
cid:15	O
)	O
is	O
assumed	O
to	O
be	O
multivariate	B
normal	O
,	O
(	O
cid:15	O
)	O
(	O
cid:24	O
)	O
nn.0	O
;	O
(	O
cid:27	O
)	O
2i/	O
;	O
(	O
7.30	O
)	O
but	O
that	O
is	O
not	O
required	O
for	O
most	O
of	O
what	O
follows	O
.	O
james–stein	O
estimation	B
and	O
ridge	B
regression	I
98	O
the	O
least	B
squares	I
estimate	O
o	O
early	O
1800s	O
,	O
is	O
the	O
minimizer	O
of	O
the	O
total	O
sum	O
of	O
squared	O
errors	B
,	O
ˇ	O
,	O
going	O
back	O
to	O
gauss	O
and	O
legendre	O
in	O
the	O
˚ky	O
(	O
cid:0	O
)	O
x	O
ˇk2	O
(	O
cid:9	O
)	O
:	O
o	O
ˇ	O
d	O
arg	O
min	O
ˇ	O
it	O
is	O
given	O
by	O
o	O
ˇ	O
d	O
s	O
(	O
cid:0	O
)	O
1x	O
0	O
where	O
s	O
is	O
the	O
p	O
(	O
cid:2	O
)	O
p	O
inner	O
product	O
matrix	B
xi	O
s	O
d	O
x	O
0	O
y	O
;	O
o	O
ˇ	O
is	O
unbiased	O
for	O
ˇ	O
and	O
has	O
covariance	O
matrix	B
(	O
cid:27	O
)	O
2s	O
(	O
cid:0	O
)	O
1	O
,	O
ˇ	O
(	O
cid:24	O
)	O
(	O
cid:0	O
)	O
ˇ	O
;	O
(	O
cid:27	O
)	O
2s	O
o	O
(	O
cid:0	O
)	O
1	O
(	O
cid:1	O
)	O
:	O
(	O
7.31	O
)	O
(	O
7.32	O
)	O
(	O
7.33	O
)	O
(	O
7.34	O
)	O
4	O
ˇ	O
is	O
the	O
mle	O
of	O
ˇ.	O
before	O
1950	O
a	O
great	O
deal	O
(	O
cid:0	O
)	O
1	O
could	O
be	O
feasibly	O
in	O
the	O
normal	B
case	O
(	O
7.30	O
)	O
o	O
of	O
effort	O
went	O
into	O
designing	O
matrices	O
x	O
such	O
that	O
s	O
calculated	O
,	O
which	O
is	O
now	O
no	O
longer	O
a	O
concern	O
.	O
a	O
great	O
advantage	O
of	O
the	O
linear	B
model	I
is	O
that	O
it	O
reduces	O
the	O
number	O
of	O
unknown	O
parameters	O
to	O
p	O
(	O
or	O
p	O
c	O
1	O
including	O
(	O
cid:27	O
)	O
2	O
)	O
,	O
no	O
matter	O
how	O
large	O
n	O
may	O
be	O
.	O
in	O
the	O
kidney	O
data	O
example	O
of	O
section	O
1.1	O
,	O
n	O
d	O
157	O
while	O
p	O
d	O
2.	O
in	O
modern	O
applications	O
,	O
however	O
,	O
p	O
has	O
grown	O
larger	O
and	O
larger	O
,	O
sometimes	O
into	O
the	O
thousands	O
or	O
more	O
,	O
as	O
we	O
will	O
see	O
in	O
part	O
iii	O
,	O
causing	O
statisticians	O
again	O
to	O
confront	O
the	O
limitations	O
of	O
high-dimensional	O
unbiased	O
estimation	O
.	O
ridge	B
regression	I
is	O
a	O
shrinkage	B
method	O
designed	O
to	O
improve	O
the	O
estima-	O
tion	O
of	O
ˇ	O
in	O
linear	B
models	O
.	O
by	O
transformations	O
	O
we	O
can	O
standardize	O
(	O
7.28	O
)	O
so	O
that	O
the	O
columns	O
of	O
x	O
each	O
have	O
mean	O
0	O
and	O
sum	O
of	O
squares	B
1	O
,	O
that	O
is	O
,	O
si	O
i	O
d	O
1	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
p	O
:	O
(	O
7.35	O
)	O
(	O
this	O
puts	O
the	O
regression	B
coefﬁcients	O
ˇ1	O
;	O
ˇ2	O
;	O
:	O
:	O
:	O
;	O
ˇp	O
on	O
comparable	O
scales	O
.	O
)	O
for	O
convenience	O
,	O
we	O
also	O
assume	O
ny	O
d	O
0.	O
a	O
ridge	B
regression	I
estimate	O
o	O
ˇ	O
.	O
(	O
cid:21	O
)	O
/	O
is	O
deﬁned	O
,	O
for	O
(	O
cid:21	O
)	O
(	O
cid:21	O
)	O
0	O
,	O
to	O
be	O
y	O
d	O
.s	O
c	O
(	O
cid:21	O
)	O
i/	O
(	O
cid:0	O
)	O
1x	O
ˇ	O
while	O
o	O
o	O
ˇ	O
.	O
(	O
cid:21	O
)	O
/	O
d	O
.s	O
c	O
(	O
cid:21	O
)	O
i/	O
(	O
using	O
(	O
7.32	O
)	O
)	O
;	O
o	O
ˇ	O
.	O
(	O
cid:21	O
)	O
/	O
is	O
a	O
shrunken	O
version	O
of	O
o	O
ˇ	O
,	O
the	O
bigger	O
(	O
cid:21	O
)	O
the	O
more	O
ˇ.0/	O
d	O
o	O
extreme	O
the	O
shrinkage	B
:	O
o	O
ˇ.1/	O
equals	O
the	O
vector	B
of	O
zeros	O
.	O
ridge	B
regression	I
effects	O
can	O
be	O
quite	O
dramatic	O
.	O
as	O
an	O
example	O
,	O
con-	O
sider	O
the	O
diabetes	B
data	O
,	O
partially	O
shown	O
in	O
table	O
7.2	O
,	O
in	O
which	O
10	O
prediction	O
variables	O
measured	O
at	O
baseline—age	O
,	O
sex	O
,	O
bmi	O
(	O
body	O
mass	O
index	O
)	O
,	O
map	O
(	O
mean	O
arterial	O
blood	O
pressure	O
)	O
,	O
and	O
six	O
blood	O
serum	O
measurements—have	O
(	O
cid:0	O
)	O
1s	O
(	O
7.36	O
)	O
o	O
ˇ	O
0	O
7.3	O
ridge	B
regression	I
99	O
table	O
7.2	O
first	O
7	O
of	O
n	O
d	O
442	O
patients	O
in	O
the	O
diabetes	B
study	O
;	O
we	O
wish	O
to	O
predict	O
disease	O
progression	O
at	O
one	O
year	O
“	O
prog	O
”	O
from	O
the	O
10	O
baseline	O
measurements	O
age	O
,	O
sex	O
,	O
.	O
.	O
.	O
,	O
glu	O
.	O
age	O
sex	O
bmi	O
map	O
59	O
48	O
72	O
24	O
50	O
23	O
36	O
:	O
:	O
:	O
1	O
0	O
1	O
0	O
0	O
0	O
1	O
:	O
:	O
:	O
32.1	O
21.6	O
30.5	O
25.3	O
23.0	O
22.6	O
22.0	O
:	O
:	O
:	O
101	O
87	O
93	O
84	O
101	O
89	O
90	O
:	O
:	O
:	O
tc	O
157	O
183	O
156	O
198	O
192	O
139	O
160	O
:	O
:	O
:	O
ldl	O
hdl	O
tch	O
ltg	O
glu	O
prog	O
93.2	O
103.2	O
93.6	O
131.4	O
125.4	O
64.8	O
99.6	O
:	O
:	O
:	O
38	O
70	O
41	O
40	O
52	O
61	O
50	O
:	O
:	O
:	O
4	O
3	O
4	O
5	O
4	O
2	O
3	O
:	O
:	O
:	O
2.11	O
1.69	O
2.03	O
2.12	O
1.86	O
1.82	O
1.72	O
:	O
:	O
:	O
87	O
69	O
85	O
89	O
80	O
68	O
82	O
:	O
:	O
:	O
151	O
75	O
141	O
206	O
135	O
97	O
138	O
:	O
:	O
:	O
been	O
obtained	O
for	O
n	O
d	O
442	O
patients	O
.	O
we	O
wish	O
to	O
use	O
the	O
10	O
variables	O
to	O
pre-	O
dict	O
prog	O
,	O
a	O
quantitative	O
assessment	O
of	O
disease	O
progression	O
one	O
year	O
after	O
baseline	O
.	O
in	O
this	O
case	O
x	O
is	O
the	O
442	O
(	O
cid:2	O
)	O
10	O
matrix	B
of	O
standardized	O
predictor	B
variables	O
,	O
and	O
y	O
is	O
prog	O
with	O
its	O
mean	O
subtracted	O
off	O
.	O
figure	O
7.2	O
ridge	O
coefﬁcient	O
trace	O
for	O
the	O
standardized	O
diabetes	B
data	O
.	O
−5000500lb^	O
(	O
l	O
)	O
0.000.050.150.200.250.1llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllagesexbmimaptcldlhdltchltgglu	O
james–stein	O
estimation	B
and	O
ridge	B
regression	I
100	O
table	O
7.3	O
ordinary	O
least	B
squares	I
estimate	O
o	O
ˇ.0/	O
compared	O
with	O
ridge	B
regression	I
estimate	O
o	O
ˇ.0:1/	O
with	O
(	O
cid:21	O
)	O
d	O
0:1.	O
the	O
columns	O
sd	O
(	O
0	O
)	O
and	O
sd	O
(	O
0.1	O
)	O
are	O
their	O
estimated	O
standard	O
errors	O
.	O
(	O
here	O
(	O
cid:27	O
)	O
was	O
taken	O
to	O
be	O
54.1	O
,	O
the	O
usual	O
ols	O
estimate	B
based	O
on	O
model	B
(	O
7.28	O
)	O
.	O
)	O
o	O
ˇ.0:1/	O
sd	O
(	O
0	O
)	O
sd	O
(	O
0.1	O
)	O
o	O
ˇ.0/	O
(	O
cid:0	O
)	O
10.0	O
age	O
1.3	O
sex	O
(	O
cid:0	O
)	O
239.8	O
(	O
cid:0	O
)	O
207.2	O
bmi	O
489.7	O
519.8	O
map	O
301.8	O
324.4	O
(	O
cid:0	O
)	O
792.2	O
(	O
cid:0	O
)	O
83.5	O
tc	O
(	O
cid:0	O
)	O
70.8	O
ldl	O
476.7	O
101.0	O
(	O
cid:0	O
)	O
188.7	O
hdl	O
tch	O
115.7	O
177.1	O
ltg	O
443.8	O
751.3	O
glu	O
67.6	O
86.7	O
59.7	O
61.2	O
66.5	O
65.3	O
416.2	O
338.6	O
212.3	O
161.3	O
171.7	O
65.9	O
52.7	O
53.2	O
56.3	O
55.7	O
43.6	O
52.4	O
58.4	O
70.8	O
58.4	O
56.6	O
figure	O
7.2	O
vertically	O
plots	O
the	O
10	O
coordinates	O
of	O
o	O
ˇ	O
.	O
(	O
cid:21	O
)	O
/	O
as	O
the	O
ridge	O
pa-	O
rameter	O
(	O
cid:21	O
)	O
increases	O
from	O
0	O
to	O
0.25.	O
four	O
of	O
the	O
coefﬁcients	O
change	O
rapidly	O
at	O
ﬁrst	O
.	O
table	O
7.3	O
compares	O
o	O
ˇ.0:1/	O
.	O
positive	O
coefﬁcients	O
predict	O
increased	O
disease	O
progression	O
.	O
notice	O
that	O
ldl	O
,	O
the	O
“	O
bad	O
cholesterol	B
”	O
measurement	O
,	O
goes	O
from	O
being	O
a	O
strongly	B
positive	O
predictor	B
in	O
o	O
there	O
is	O
a	O
bayesian	O
rationale	O
for	O
ridge	B
regression	I
.	O
assume	O
that	O
the	O
noise	O
ˇ.0/	O
,	O
that	O
is	O
the	O
usual	O
estimate	B
o	O
ˇ	O
,	O
with	O
o	O
ˇ.0:1/	O
.	O
vector	B
(	O
cid:15	O
)	O
is	O
normal	B
as	O
in	O
(	O
7.30	O
)	O
,	O
so	O
that	O
(	O
7.37	O
)	O
(	O
7.38	O
)	O
rather	O
than	O
just	O
(	O
7.34	O
)	O
.	O
then	O
the	O
bayesian	O
prior	B
ˇ	O
to	O
a	O
mildly	O
negative	O
one	O
in	O
o	O
(	O
cid:0	O
)	O
ˇ	O
;	O
(	O
cid:27	O
)	O
2s	O
	O
o	O
ˇ	O
(	O
cid:24	O
)	O
np	O
(	O
cid:0	O
)	O
1	O
(	O
cid:1	O
)	O
	O
ˇ	O
(	O
cid:24	O
)	O
(	O
cid:27	O
)	O
2	O
(	O
cid:21	O
)	O
i	O
np	O
0	O
;	O
n	O
ˇj	O
o	O
o	O
d	O
.s	O
c	O
(	O
cid:21	O
)	O
i/	O
(	O
cid:0	O
)	O
1s	O
o	O
ˇ	O
;	O
makes	O
e	O
ˇ	O
(	O
7.39	O
)	O
ˇ	O
.	O
(	O
cid:21	O
)	O
/	O
(	O
using	O
(	O
5.23	O
)	O
with	O
m	O
d	O
0	O
,	O
(	O
cid:0	O
)	O
1	O
)	O
.	O
ridge	B
regression	I
amounts	O
to	O
an	O
the	O
same	O
as	O
the	O
ridge	B
regression	I
estimate	O
o	O
a	O
d	O
.	O
(	O
cid:27	O
)	O
2=	O
(	O
cid:21	O
)	O
/i	O
,	O
and	O
†	O
d	O
.s	O
=	O
(	O
cid:27	O
)	O
2/	O
increased	O
prior	B
belief	O
that	O
ˇ	O
lies	O
near	O
0.	O
the	O
last	O
two	O
columns	O
of	O
table	O
7.3	O
compare	O
the	O
standard	B
deviations	I
	O
of	O
ˇ	O
and	O
o	O
o	O
ˇ.0:1/	O
.	O
ridging	O
has	O
greatly	O
reduced	O
the	O
variability	O
of	O
the	O
estimated	O
5	O
7.3	O
ridge	B
regression	I
101	O
o	O
(	O
cid:22	O
)	O
.	O
(	O
cid:21	O
)	O
/	O
d	O
x	O
o	O
ˇ	O
.	O
(	O
cid:21	O
)	O
/	O
;	O
regression	B
coefﬁcients	O
.	O
this	O
does	O
not	O
guarantee	O
that	O
the	O
corresponding	O
es-	O
timate	O
of	O
(	O
cid:22	O
)	O
d	O
x	O
ˇ	O
,	O
(	O
7.40	O
)	O
o	O
will	O
be	O
more	O
accurate	O
than	O
the	O
ordinary	O
least	B
squares	I
estimate	O
o	O
(	O
cid:22	O
)	O
d	O
x	O
ˇ.	O
we	O
have	O
(	O
deliberately	O
)	O
introduced	O
bias	O
,	O
and	O
the	O
squared	O
bias	O
term	O
coun-	O
teracts	O
some	O
of	O
the	O
advantage	O
of	O
reduced	O
variability	O
.	O
the	O
cp	O
calculations	O
of	O
chapter	O
12	O
suggest	O
that	O
the	O
two	O
effects	O
nearly	O
offset	O
each	O
other	O
for	O
the	O
diabetes	B
data	O
.	O
however	O
,	O
if	O
interest	O
centers	O
on	O
the	O
coefﬁcients	O
of	O
ˇ	O
,	O
then	O
ridging	O
can	O
be	O
crucial	O
,	O
as	O
table	O
7.3	O
emphasizes	O
.	O
by	O
current	O
standards	O
,	O
p	O
d	O
10	O
is	O
a	O
small	O
number	O
of	O
predictors	O
.	O
data	B
sets	O
with	O
p	O
in	O
the	O
thousands	O
,	O
and	O
more	O
,	O
will	O
show	O
up	O
in	O
part	O
iii	O
.	O
in	O
such	O
situa-	O
tions	O
the	O
scientist	O
is	O
often	O
looking	O
for	O
a	O
few	O
interesting	O
predictor	B
variables	O
hidden	O
in	O
a	O
sea	O
of	O
uninteresting	O
ones	O
:	O
the	O
prior	B
belief	O
is	O
that	O
most	O
of	O
the	O
ˇi	O
values	O
lie	O
near	O
zero	O
.	O
biasing	O
the	O
maximum	B
likelihood	I
estimates	O
o	O
ˇi	O
toward	O
zero	O
then	O
becomes	O
a	O
necessity	O
.	O
o	O
ˇ	O
.	O
(	O
cid:21	O
)	O
/	O
:	O
there	O
is	O
still	O
another	O
way	O
to	O
motivate	O
the	O
ridge	B
regression	I
estimator	O
o	O
ˇ	O
.	O
(	O
cid:21	O
)	O
/	O
d	O
arg	O
min	O
fky	O
(	O
cid:0	O
)	O
x	O
ˇk2	O
c	O
(	O
cid:21	O
)	O
kˇk2g	O
:	O
(	O
cid:0	O
)	O
1x	O
0	O
ˇ	O
(	O
7.41	O
)	O
differentiating	O
the	O
term	O
in	O
brackets	O
with	O
respect	O
to	O
ˇ	O
shows	O
that	O
o	O
ˇ	O
.	O
(	O
cid:21	O
)	O
/	O
d	O
.s	O
c	O
(	O
cid:21	O
)	O
i/	O
y	O
as	O
in	O
(	O
7.36	O
)	O
.	O
if	O
(	O
cid:21	O
)	O
d	O
0	O
then	O
(	O
7.41	O
)	O
describes	O
the	O
ordinary	O
least	B
squares	I
algorithm	O
;	O
(	O
cid:21	O
)	O
>	O
0	O
penalizes	O
choices	O
of	O
ˇ	O
having	O
kˇk	O
large	O
,	O
biasing	O
o	O
various	O
terminologies	O
are	O
used	O
to	O
describe	O
algorithms	O
such	O
as	O
(	O
7.41	O
)	O
:	O
pe-	O
nalized	O
least	B
squares	I
;	O
penalized	O
likelihood	B
;	O
maximized	O
a-posteriori	O
proba-	O
bility	O
(	O
map	O
)	O
;	O
and	O
,	O
generically	O
,	O
regularization	B
describes	O
almost	O
any	O
method	B
6	O
that	O
tamps	O
down	O
statistical	O
variability	O
in	O
high-dimensional	O
estimation	B
or	O
prediction	O
problems	O
.	O
ˇ	O
.	O
(	O
cid:21	O
)	O
/	O
toward	O
the	O
origin	O
.	O
a	O
wide	O
variety	O
of	O
penalty	B
terms	O
are	O
in	O
current	O
use	O
,	O
the	O
most	O
inﬂuential	O
one	O
involving	O
the	O
“	O
`1	O
norm	O
”	O
kˇk1	O
dpp	O
jˇjj	O
,	O
1	O
q	O
ˇ	O
.	O
(	O
cid:21	O
)	O
/	O
d	O
arg	O
min	O
fky	O
(	O
cid:0	O
)	O
x	O
ˇk2	O
c	O
(	O
cid:21	O
)	O
kˇk1g	O
;	O
(	O
7.42	O
)	O
ˇ	O
the	O
so-called	O
lasso	B
estimator	O
,	O
chapter	O
16.	O
despite	O
the	O
bayesian	O
provenance	O
,	O
most	O
regularization	B
research	O
is	O
carried	O
out	O
frequentistically	O
,	O
with	O
various	O
penalty	B
terms	O
investigated	O
for	O
their	O
probabilistic	O
behavior	O
regarding	O
esti-	O
mation	O
,	O
prediction	O
,	O
and	O
variable	O
selection	O
.	O
if	O
we	O
apply	O
the	O
james–stein	O
rule	B
to	O
the	O
normal	B
model	O
(	O
7.37	O
)	O
,	O
we	O
get	O
a	O
different	O
shrinkage	B
rule	O
for	O
o	O
ˇ	O
,	O
say	O
q	O
ˇjs	O
,	O
7	O
102	O
james–stein	O
estimation	B
and	O
ridge	B
regression	I
#	O
o	O
''	O
q	O
ˇjs	O
d	O
s	O
o	O
ˇ	O
1	O
(	O
cid:0	O
)	O
.p	O
(	O
cid:0	O
)	O
2/	O
(	O
cid:27	O
)	O
2	O
o	O
ˇ	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
2o	O
n	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
q	O
(	O
cid:22	O
)	O
js	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
(	O
7.43	O
)	O
q	O
ˇjs	O
be	O
the	O
corresponding	O
estimator	B
of	O
(	O
cid:22	O
)	O
d	O
efyg	O
in	O
ˇ	O
:	O
0	O
letting	O
q	O
(	O
cid:22	O
)	O
js	O
d	O
x	O
(	O
7.28	O
)	O
,	O
the	O
james–stein	O
theorem	B
guarantees	O
that	O
e	O
<	O
p	O
(	O
cid:27	O
)	O
2	O
(	O
7.44	O
)	O
no	O
matter	O
what	O
ˇ	O
is	O
,	O
as	O
long	O
as	O
p	O
(	O
cid:21	O
)	O
3.2	O
there	O
is	O
no	O
such	O
guarantee	O
for	O
ridge	B
regression	I
,	O
and	O
no	O
foolproof	O
way	O
to	O
choose	O
the	O
ridge	O
parameter	O
(	O
cid:21	O
)	O
.	O
on	O
the	O
other	O
hand	O
,	O
q	O
ˇjs	O
does	O
not	O
stabilize	O
the	O
coordinate	O
standard	O
devia-	O
tions	O
,	O
as	O
in	O
the	O
sd	O
(	O
0.1	O
)	O
column	O
of	O
table	O
7.3.	O
the	O
main	O
point	O
here	O
is	O
that	O
at	O
present	O
there	O
is	O
no	O
optimality	O
theory	B
for	O
shrinkage	B
estimation	O
.	O
fisher	O
pro-	O
vided	O
an	O
elegant	O
theory	B
for	O
optimal	O
unbiased	O
estimation	O
.	O
it	O
remains	O
to	O
be	O
seen	O
whether	O
biased	O
estimation	B
can	O
be	O
neatly	O
codiﬁed	O
.	O
241000x	O
jd1	O
351=2	O
241000x	O
jd1	O
(	O
7.45	O
)	O
d	O
351=2	O
:	O
7.4	O
indirect	O
evidence	O
2	O
there	O
is	O
a	O
downside	O
to	O
shrinkage	B
estimation	O
,	O
which	O
we	O
can	O
examine	O
by	O
returning	O
to	O
the	O
baseball	B
data	O
of	O
table	O
7.1.	O
one	O
thousand	O
simulations	O
were	O
run	O
,	O
each	O
one	O
generating	O
simulated	O
batting	O
averages	O
(	O
cid:3	O
)	O
p	O
i	O
(	O
cid:24	O
)	O
bi.90	O
;	O
truthi	O
/=90	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
18	O
:	O
(	O
cid:3	O
)	O
these	O
gave	O
corresponding	O
james–stein	O
(	O
js	O
)	O
estimates	O
(	O
7.20	O
)	O
,	O
with	O
(	O
cid:27	O
)	O
2	O
np	O
0	O
.1	O
(	O
cid:0	O
)	O
np	O
(	O
cid:3	O
)	O
table	O
7.4	O
shows	O
the	O
root	O
mean	O
square	O
error	O
for	O
the	O
mle	O
and	O
js	O
estimates	O
/=90	O
.	O
over	O
1000	O
simulations	O
for	O
each	O
of	O
the	O
18	O
players	O
,	O
(	O
cid:3	O
)	O
ij	O
(	O
cid:0	O
)	O
truthi	O
/2	O
.p	O
and	O
.	O
op	O
(	O
cid:3	O
)	O
js	O
ij	O
(	O
cid:0	O
)	O
truthi	O
/2	O
loses	O
to	O
op	O
(	O
7.46	O
)	O
as	O
foretold	O
by	O
the	O
james–stein	O
theorem	B
,	O
the	O
js	O
estimates	O
are	O
easy	O
victors	O
in	O
terms	O
of	O
total	O
squared	O
error	O
(	O
summing	O
over	O
all	O
18	O
players	O
)	O
.	O
however	O
,	O
op	O
(	O
cid:3	O
)	O
js	O
(	O
cid:3	O
)	O
i	O
for	O
4	O
of	O
the	O
18	O
players	O
,	O
losing	O
badly	O
in	O
the	O
case	O
of	O
player	O
2	O
.	O
(	O
cid:3	O
)	O
js	O
for	O
player	O
2	O
appear	O
in	O
figure	O
7.3.	O
strikingly	O
,	O
all	O
1000	O
of	O
the	O
op	O
(	O
cid:3	O
)	O
js	O
2j	O
values	O
lie	O
histograms	O
comparing	O
the	O
1000	O
simulations	O
of	O
p	O
i	O
with	O
those	O
of	O
op	O
(	O
cid:3	O
)	O
d	O
p	O
(	O
cid:3	O
)	O
mle	O
i	O
i	O
i	O
2	O
of	O
course	O
we	O
are	O
assumimg	O
(	O
cid:27	O
)	O
2	O
is	O
known	O
in	O
(	O
7.43	O
)	O
;	O
if	O
it	O
is	O
estimated	O
,	O
some	O
of	O
the	O
improvement	O
erodes	O
away	O
.	O
7.4	O
indirect	O
evidence	O
2	O
103	O
table	O
7.4	O
simulation	O
study	O
comparing	O
root	O
mean	O
square	O
errors	B
for	O
mle	O
and	O
js	O
estimators	O
(	O
7.20	O
)	O
as	O
estimates	O
of	O
truth	O
.	O
total	O
mean	O
square	O
errors	B
.0384	O
(	O
mle	O
)	O
and	O
.0235	O
(	O
js	O
)	O
.	O
asterisks	O
indicate	O
four	O
players	O
for	O
whom	O
rmsjs	O
exceeded	O
rmsmle	O
;	O
these	O
have	O
two	O
largest	O
and	O
two	O
smallest	O
truth	O
values	O
(	O
player	O
2	O
is	O
clemente	O
)	O
.	O
column	O
rmsjs1	O
is	O
for	O
the	O
limited	O
translation	O
version	O
of	O
js	O
that	O
bounds	O
shrinkage	B
to	O
within	O
one	O
standard	B
deviation	I
of	O
the	O
mle	O
.	O
player	O
truth	O
rmsmle	O
rmsjs	O
rmsjs1	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
11	O
12	O
13	O
14	O
15	O
16	O
17	O
18	O
.298	O
.346*	O
.222	O
.276	O
.263	O
.273	O
.303	O
.270	O
.230	O
.264	O
.264	O
.210*	O
.256	O
.269	O
.316*	O
.226	O
.285	O
.200*	O
.046	O
.049	O
.044	O
.048	O
.047	O
.046	O
.047	O
.049	O
.044	O
.047	O
.047	O
.043	O
.045	O
.048	O
.048	O
.045	O
.046	O
.043	O
.033	O
.077	O
.042	O
.015	O
.011	O
.014	O
.037	O
.012	O
.034	O
.011	O
.012	O
.053	O
.014	O
.012	O
.049	O
.038	O
.022	O
.062	O
.032	O
.056	O
.038	O
.023	O
.020	O
.021	O
.035	O
.022	O
.033	O
.021	O
.020	O
.044	O
.020	O
.021	O
.043	O
.036	O
.026	O
.048	O
(	O
cid:3	O
)	O
js	O
i	O
below	O
truth2	O
d	O
0:346.	O
player	O
2	O
could	O
have	O
had	O
a	O
legitimate	O
complaint	O
if	O
the	O
james–stein	O
estimate	B
were	O
used	O
to	O
set	B
his	O
next	O
year	O
’	O
s	O
salary	O
.	O
the	O
four	O
losing	O
cases	O
for	O
op	O
are	O
the	O
players	O
with	O
the	O
two	O
largest	O
and	O
two	O
smallest	O
values	O
of	O
the	O
truth	O
.	O
shrinkage	B
estimators	O
work	O
against	O
cases	O
that	O
are	O
genuinely	O
outstanding	O
(	O
in	O
a	O
positive	O
or	O
negative	O
sense	O
)	O
.	O
player	O
2	O
was	O
roberto	O
clemente	O
.	O
a	O
better	O
informed	O
bayesian	O
,	O
that	O
is	O
,	O
a	O
baseball	B
fan	O
,	O
would	O
know	O
that	O
clemente	O
had	O
led	O
the	O
league	O
in	O
batting	O
over	O
the	O
previ-	O
ous	O
several	O
years	O
,	O
and	O
shouldn	O
’	O
t	B
be	O
thrown	O
into	O
a	O
shrinkage	B
pool	O
with	O
17	O
ordinary	O
hitters	O
.	O
of	O
course	O
the	O
james–stein	O
estimates	O
were	O
more	O
accurate	O
for	O
14	O
of	O
the	O
18	O
players	O
.	O
shrinkage	B
estimation	O
tends	O
to	O
produce	O
better	O
results	O
in	O
general	O
,	O
at	O
the	O
possible	O
expense	O
of	O
extreme	O
cases	O
.	O
nobody	O
cares	O
much	O
about	O
cold	O
war	O
batting	O
averages	O
,	O
but	O
if	O
the	O
context	O
were	O
the	O
efﬁcacies	O
of	O
18	O
new	O
anti-	O
cancer	O
drugs	O
the	O
stakes	O
would	O
be	O
higher	O
.	O
104	O
james–stein	O
estimation	B
and	O
ridge	B
regression	I
figure	O
7.3	O
comparing	O
mle	O
estimates	O
(	O
solid	O
)	O
with	O
js	O
estimates	O
(	O
line	O
)	O
for	O
clemente	O
;	O
1000	O
simulations	O
,	O
90	O
at	O
bats	O
each	O
.	O
compromise	O
methods	O
are	O
available	O
.	O
the	O
rmsjs1	O
column	O
of	O
table	O
7.4	O
refers	O
to	O
a	O
limited	O
translation	O
version	O
of	O
opjs	O
in	O
which	O
shrinkage	B
is	O
not	O
al-	O
lowed	O
to	O
diverge	O
more	O
than	O
one	O
(	O
cid:27	O
)	O
0	O
unit	O
from	O
opi	O
;	O
in	O
formulaic	O
terms	O
,	O
i	O
d	O
min˚max	O
(	O
cid:0	O
)	O
opjs	O
opjs	O
1	O
i	O
(	O
cid:1	O
)	O
;	O
opi	O
c	O
(	O
cid:27	O
)	O
0	O
(	O
cid:9	O
)	O
:	O
i	O
;	O
opi	O
(	O
cid:0	O
)	O
(	O
cid:27	O
)	O
0	O
(	O
7.47	O
)	O
this	O
mitigates	O
the	O
clemente	O
problem	O
while	O
still	O
gaining	O
most	O
of	O
the	O
shrink-	O
age	O
advantages	O
.	O
the	O
use	O
of	O
indirect	O
evidence	O
amounts	O
to	O
learning	O
from	O
the	O
experience	O
of	O
others	O
,	O
each	O
batter	O
learning	O
from	O
the	O
17	O
others	O
in	O
the	O
baseball	B
exam-	O
ples	O
.	O
“	O
which	O
others	O
?	O
”	O
is	O
a	O
key	O
question	O
in	O
applying	O
computer-age	O
methods	O
.	O
chapter	O
15	O
returns	O
to	O
the	O
question	O
in	O
the	O
context	O
of	O
false-discovery	O
rates	O
.	O
7.5	O
notes	O
and	O
details	O
the	O
bayesian	O
motivation	O
emphasized	O
in	O
chapters	O
6	O
and	O
7	O
is	O
anachronistic	O
:	O
originally	O
the	O
work	O
emerged	O
mainly	O
from	O
frequentist	O
considerations	O
and	O
was	O
justiﬁed	O
frequentistically	O
,	O
as	O
in	O
robbins	O
(	O
1956	O
)	O
.	O
stein	O
(	O
1956	O
)	O
proved	O
the	O
inadmissibility	O
of	O
o	O
(	O
cid:22	O
)	O
mle	O
,	O
the	O
neat	O
version	O
of	O
o	O
(	O
cid:22	O
)	O
js	O
appearing	O
in	O
james	O
and	O
stein	O
(	O
1961	O
)	O
(	O
willard	O
james	O
was	O
stein	O
’	O
s	O
graduate	O
student	O
)	O
;	O
o	O
(	O
cid:22	O
)	O
js	O
is	O
it-	O
self	O
inadmissable	O
,	O
being	O
everywhere	O
improvable	O
by	O
changing	O
ob	O
in	O
(	O
7.13	O
)	O
p^frequency0.200.250.300.350.400.450.500.55050100150200250300350truth	O
0.346p^	O
mlep^	O
james−stein	O
105	O
to	O
max	O
.	O
ob	O
;	O
0/	O
.	O
this	O
in	O
turn	O
is	O
inadmissable	O
,	O
but	O
further	O
gains	O
tend	O
to	O
the	O
minuscule	O
.	O
7.5	O
notes	O
and	O
details	O
in	O
a	O
series	O
of	O
papers	O
in	O
the	O
early	O
1970s	O
,	O
efron	O
and	O
morris	O
emphasized	O
the	O
empirical	B
bayes	O
motivation	O
of	O
the	O
james–stein	O
rule	B
,	O
efron	O
and	O
morris	O
(	O
1972	O
)	O
giving	O
the	O
limited	O
translation	O
version	O
(	O
7.47	O
)	O
.	O
the	O
baseball	B
data	O
in	O
its	O
original	O
form	B
appears	O
in	O
table	O
1.1	O
of	O
efron	O
(	O
2010	O
)	O
.	O
here	O
the	O
original	O
45	O
at	O
bats	O
recorded	O
for	O
each	O
player	O
have	O
been	O
artiﬁcially	O
augmented	O
by	O
adding	O
45	O
binomial	B
draws	O
,	O
bi.45	O
;	O
truthi	O
/	O
for	O
player	O
i.	O
this	O
gives	O
a	O
somewhat	O
less	O
optimistic	O
view	O
of	O
the	O
james–stein	O
rule	B
’	O
s	O
performance	O
.	O
“	O
stein	O
’	O
s	O
paradox	B
in	O
statistics	B
,	O
”	O
efron	O
and	O
morris	O
’	O
title	O
for	O
their	O
1977	O
sci-	O
entiﬁc	O
american	O
article	O
,	O
catches	O
the	O
statistics	B
world	O
’	O
s	O
sense	O
of	O
discomfort	O
with	O
the	O
james–stein	O
theorem	B
.	O
why	O
should	O
our	O
estimate	B
for	O
player	O
a	O
go	O
up	O
or	O
down	O
depending	O
on	O
the	O
other	O
players	O
’	O
performances	O
?	O
this	O
is	O
the	O
question	O
of	O
direct	O
versus	O
indirect	O
evidence	O
,	O
raised	O
again	O
in	O
the	O
context	O
of	O
hypothesis	B
testing	I
in	O
chapter	O
15.	O
unbiased	O
estimation	O
has	O
great	O
scientiﬁc	O
appeal	O
,	O
so	O
the	O
argument	B
is	O
by	O
no	O
means	O
settled	O
.	O
ridge	B
regression	I
was	O
introduced	O
into	O
the	O
statistics	B
literature	O
by	O
hoerl	O
and	O
kennard	O
(	O
1970	O
)	O
.	O
it	O
appeared	O
previously	O
in	O
the	O
numerical	O
analysis	B
liter-	O
ature	O
as	O
tikhonov	O
regularization	B
.	O
of	O
freedom	O
,	O
z	O
(	O
cid:24	O
)	O
(	O
cid:31	O
)	O
2	O
1	O
[	O
p.	O
93	O
]	O
formula	B
(	O
7.12	O
)	O
.	O
if	O
z	O
has	O
a	O
chi-squared	O
distribution	B
with	O
(	O
cid:23	O
)	O
degrees	O
(	O
cid:23	O
)	O
(	O
that	O
is	O
,	O
z	O
(	O
cid:24	O
)	O
gam	O
.	O
(	O
cid:23	O
)	O
=2	O
;	O
2/	O
in	O
table	O
5.1	O
)	O
,	O
it	O
has	O
density	B
f	O
.z/	O
d	O
z	O
(	O
cid:23	O
)	O
=2	O
(	O
cid:0	O
)	O
1e	O
for	O
z	O
(	O
cid:21	O
)	O
0	O
;	O
(	O
7.48	O
)	O
(	O
cid:0	O
)	O
z=2	O
2	O
(	O
cid:23	O
)	O
=2	O
.	O
(	O
cid:23	O
)	O
=2/	O
2	O
(	O
cid:23	O
)	O
=2	O
dz	O
d	O
2	O
(	O
cid:23	O
)	O
=2	O
(	O
cid:0	O
)	O
1	O
(	O
cid:26	O
)	O
n	O
(	O
cid:0	O
)	O
3	O
(	O
cid:27	O
)	O
d	O
1	O
s	O
a	O
c	O
1	O
	O
.	O
(	O
cid:23	O
)	O
=2	O
(	O
cid:0	O
)	O
1/	O
	O
.	O
(	O
cid:23	O
)	O
=2/	O
d	O
1	O
(	O
cid:23	O
)	O
(	O
cid:0	O
)	O
2	O
:	O
(	O
7.49	O
)	O
;	O
(	O
7.50	O
)	O
yielding	O
(	O
cid:26	O
)	O
1	O
(	O
cid:27	O
)	O
dz	O
1	O
e	O
z	O
(	O
cid:0	O
)	O
z=2	O
z	O
(	O
cid:23	O
)	O
=2	O
(	O
cid:0	O
)	O
2e	O
2	O
(	O
cid:23	O
)	O
=2	O
.	O
(	O
cid:23	O
)	O
=2/	O
0	O
e	O
verifying	O
(	O
7.12	O
)	O
.	O
but	O
standard	O
results	O
,	O
starting	O
from	O
(	O
7.11	O
)	O
,	O
show	O
that	O
s	O
(	O
cid:24	O
)	O
.a	O
c	O
1/	O
(	O
cid:31	O
)	O
2	O
with	O
(	O
cid:23	O
)	O
d	O
n	O
(	O
cid:0	O
)	O
1	O
in	O
(	O
7.49	O
)	O
,	O
n	O
(	O
cid:0	O
)	O
1	O
.	O
2	O
[	O
p.	O
93	O
]	O
formula	B
(	O
7.14	O
)	O
.	O
first	O
consider	O
the	O
simpler	O
situation	O
where	O
m	O
in	O
(	O
7.11	O
)	O
is	O
known	O
to	O
equal	O
zero	O
,	O
in	O
which	O
case	O
the	O
james–stein	O
estimator	B
is	O
(	O
7.51	O
)	O
with	O
ob	O
d	O
1	O
(	O
cid:0	O
)	O
.n	O
(	O
cid:0	O
)	O
2/=s	O
;	O
where	O
s	O
dpn	O
oc	O
d	O
1	O
(	O
cid:0	O
)	O
ob	O
d	O
.n	O
(	O
cid:0	O
)	O
2/=s	O
and	O
c	O
d	O
1	O
(	O
cid:0	O
)	O
b	O
d	O
1=.a	O
c	O
1/	O
:	O
d	O
obxi	O
o	O
(	O
cid:22	O
)	O
js	O
i	O
.	O
for	O
convenient	O
notation	O
let	O
1	O
x2	O
(	O
7.52	O
)	O
i	O
james–stein	O
estimation	B
and	O
ridge	B
regression	I
106	O
the	O
conditional	O
distribution	B
(	O
cid:22	O
)	O
ijx	O
(	O
cid:24	O
)	O
e	O
e	O
i	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
i	O
n	O
.bxi	O
;	O
b/	O
gives	O
i	O
;	O
(	O
cid:1	O
)	O
2ˇˇˇx	O
n	O
(	O
cid:0	O
)	O
o	O
(	O
cid:22	O
)	O
js	O
o	O
d	O
b	O
c	O
.	O
oc	O
(	O
cid:0	O
)	O
c	O
/2x2	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
2ˇˇˇx	O
o	O
d	O
nb	O
c	O
.	O
oc	O
(	O
cid:0	O
)	O
c	O
/2s	O
:	O
n	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
o	O
(	O
cid:22	O
)	O
js	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
o	O
d	O
2.1	O
(	O
cid:0	O
)	O
b/	O
;	O
n	O
.	O
oc	O
(	O
cid:0	O
)	O
c	O
/2s	O
n	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
o	O
(	O
cid:22	O
)	O
js	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
2o	O
d	O
nb	O
c	O
2.1	O
(	O
cid:0	O
)	O
b/	O
:	O
e	O
e	O
and	O
so	O
and	O
,	O
adding	O
over	O
the	O
n	O
coordinates	O
,	O
the	O
marginal	O
distribution	B
s	O
(	O
cid:24	O
)	O
.a	O
c	O
1/	O
(	O
cid:31	O
)	O
2	O
calculation	O
,	O
n	O
and	O
(	O
7.49	O
)	O
yields	O
,	O
after	O
a	O
little	O
(	O
7.53	O
)	O
(	O
7.54	O
)	O
(	O
7.55	O
)	O
(	O
7.56	O
)	O
by	O
orthogonal	O
transformations	O
,	O
in	O
situation	O
(	O
7.7	O
)	O
,	O
where	O
m	O
is	O
not	O
as-	O
sumed	O
to	O
be	O
zero	O
,	O
o	O
(	O
cid:22	O
)	O
js	O
can	O
be	O
represented	O
as	O
the	O
sum	O
of	O
two	O
parts	O
:	O
a	O
js	O
estimate	B
in	O
n	O
(	O
cid:0	O
)	O
1	O
dimensions	O
but	O
with	O
m	O
d	O
0	O
as	O
in	O
(	O
7.51	O
)	O
,	O
and	O
a	O
mle	O
estimate	O
of	O
the	O
remaining	O
one	O
coordinate	O
.	O
using	O
(	O
7.56	O
)	O
this	O
gives	O
n	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
o	O
(	O
cid:22	O
)	O
js	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
2o	O
d	O
.n	O
(	O
cid:0	O
)	O
1/b	O
c	O
2.1	O
(	O
cid:0	O
)	O
b/	O
c	O
1	O
(	O
7.57	O
)	O
d	O
nb	O
c	O
3.1	O
(	O
cid:0	O
)	O
b/	O
;	O
e	O
which	O
is	O
(	O
7.14	O
)	O
.	O
3	O
[	O
p.	O
93	O
]	O
the	O
james–stein	O
theorem	B
.	O
stein	O
(	O
1981	O
)	O
derived	O
a	O
simpler	O
proof	O
of	O
the	O
js	O
theorem	B
that	O
appears	O
in	O
section	O
1.2	O
of	O
efron	O
(	O
2010	O
)	O
.	O
4	O
[	O
p.	O
98	O
]	O
transformations	O
to	O
form	B
(	O
7.35	O
)	O
.	O
the	O
linear	B
regression	O
model	B
(	O
7.28	O
)	O
is	O
equivariant	O
under	O
scale	B
changes	O
of	O
the	O
variables	O
xj	O
.	O
what	O
this	O
means	O
is	O
that	O
the	O
space	B
of	O
ﬁts	O
using	O
linear	B
combinations	O
of	O
the	O
xj	O
is	O
the	O
same	O
as	O
the	O
space	B
of	O
linear	B
combinations	O
using	O
scaled	O
versions	O
qxj	O
d	O
xj	O
=sj	O
,	O
with	O
sj	O
>	O
0.	O
furthermore	O
,	O
the	O
least	B
squares	I
ﬁts	O
are	O
the	O
same	O
,	O
and	O
the	O
coefﬁcient	O
estimates	O
map	O
in	O
the	O
obvious	O
way	O
:	O
oq	O
ˇj	O
d	O
sj	O
ridge	B
regression	I
,	O
we	O
see	O
that	O
the	O
penalty	B
term	O
kˇk2	O
dp	O
not	O
so	O
for	O
ridge	B
regression	I
.	O
changing	O
the	O
scales	O
of	O
the	O
columns	O
of	O
x	O
will	O
generally	O
lead	O
to	O
different	O
ﬁts	O
.	O
using	O
the	O
penalty	B
version	O
(	O
7.41	O
)	O
of	O
j	O
treats	O
all	O
the	O
coefﬁcients	O
as	O
equals	O
.	O
this	O
penalty	B
is	O
most	O
natural	O
if	O
all	O
the	O
variables	O
are	O
measured	O
on	O
the	O
same	O
scale	B
.	O
hence	O
we	O
typically	O
use	O
for	O
sj	O
the	O
standard	B
deviation	I
of	O
variable	O
xj	O
,	O
which	O
leads	O
to	O
(	O
7.35	O
)	O
.	O
furthermore	O
,	O
with	O
ridge	B
regression	I
we	O
typically	O
do	O
not	O
penalize	O
the	O
intercept	O
.	O
this	O
can	O
be	O
achieved	O
o	O
ˇj	O
.	O
j	O
ˇ2	O
107	O
by	O
centering	O
and	O
scaling	O
each	O
of	O
the	O
variables	O
,	O
qxj	O
d	O
.xj	O
(	O
cid:0	O
)	O
1nxj	O
/=sj	O
,	O
where	O
7.5	O
notes	O
and	O
details	O
nxj	O
d	O
nx	O
xij	O
=n	O
and	O
sj	O
dhx	O
.xij	O
(	O
cid:0	O
)	O
nxj	O
/2i1=2	O
id1	O
(	O
7.58	O
)	O
with	O
1	O
the	O
n-vector	O
of	O
1s	O
.	O
we	O
now	O
work	O
with	O
qx	O
d	O
.	O
qx1	O
;	O
qx2	O
;	O
:	O
:	O
:	O
;	O
qxp/	O
rather	O
than	O
x	O
,	O
and	O
the	O
intercept	O
is	O
estimated	O
separately	O
as	O
ny	O
.	O
we	O
calculate	O
the	O
covariance	O
matrix	B
of	O
o	O
cov	O
(	O
cid:21	O
)	O
d	O
(	O
cid:27	O
)	O
2.s	O
c	O
(	O
cid:21	O
)	O
i/	O
5	O
[	O
p.	O
100	O
]	O
standard	B
deviations	I
in	O
table	O
7.3.	O
from	O
the	O
ﬁrst	O
equality	O
in	O
(	O
7.36	O
)	O
ˇ	O
.	O
(	O
cid:21	O
)	O
/	O
to	O
be	O
(	O
cid:0	O
)	O
1s	O
.s	O
c	O
(	O
cid:21	O
)	O
i/	O
(	O
7.59	O
)	O
(	O
cid:0	O
)	O
1	O
:	O
;	O
the	O
entries	O
sd	O
(	O
0.1	O
)	O
in	O
table	O
7.3	O
are	O
square	O
roots	O
of	O
the	O
diagonal	O
elements	O
of	O
cov	O
(	O
cid:21	O
)	O
,	O
substituting	O
the	O
ordinary	O
least	B
squares	I
estimate	O
o	O
(	O
cid:27	O
)	O
d	O
54:1	O
for	O
(	O
cid:27	O
)	O
2	O
.	O
6	O
[	O
p.	O
101	O
]	O
penalized	O
likelihood	B
and	O
map	O
.	O
with	O
(	O
cid:27	O
)	O
2	O
ﬁxed	O
and	O
known	O
in	O
the	O
nn.x	O
ˇ	O
;	O
(	O
cid:27	O
)	O
2i/	O
,	O
minimizing	O
ky	O
(	O
cid:0	O
)	O
x	O
ˇk2	O
is	O
the	O
normal	B
linear	O
model	B
y	O
(	O
cid:24	O
)	O
same	O
as	O
maximizing	O
the	O
log	O
density	B
function	O
log	O
fˇ	O
.y/	O
d	O
(	O
cid:0	O
)	O
1	O
ky	O
(	O
cid:0	O
)	O
x	O
ˇk2	O
c	O
constant	O
:	O
2	O
(	O
7.60	O
)	O
in	O
this	O
sense	O
,	O
the	O
term	O
(	O
cid:21	O
)	O
kˇk2	O
in	O
(	O
7.41	O
)	O
penalizes	O
the	O
likelihood	B
log	O
fˇ	O
.y/	O
connected	O
with	O
ˇ	O
in	O
proportion	B
to	O
the	O
magnitude	O
kˇk2	O
.	O
under	O
the	O
prior	B
distribution	I
(	O
7.38	O
)	O
,	O
the	O
log	O
posterior	O
density	O
of	O
ˇ	O
given	O
y	O
(	O
the	O
log	O
of	O
(	O
3.5	O
)	O
)	O
is	O
˚ky	O
(	O
cid:0	O
)	O
x	O
ˇk2	O
c	O
(	O
cid:21	O
)	O
kˇk2	O
(	O
cid:9	O
)	O
;	O
(	O
7.61	O
)	O
(	O
cid:0	O
)	O
1	O
2	O
(	O
cid:27	O
)	O
2	O
plus	O
a	O
term	O
that	O
doesn	O
’	O
t	B
depend	O
on	O
ˇ.	O
that	O
makes	O
the	O
maximizer	O
of	O
(	O
7.41	O
)	O
also	O
the	O
maximizer	O
of	O
the	O
posterior	O
density	O
of	O
ˇ	O
given	O
y	O
,	O
or	O
the	O
map	O
.	O
7	O
[	O
p.	O
101	O
]	O
formula	B
(	O
7.43	O
)	O
.	O
let	O
(	O
cid:13	O
)	O
d	O
.s	O
1=2=	O
(	O
cid:27	O
)	O
/ˇ	O
and	O
o	O
(	O
cid:13	O
)	O
d	O
.s	O
1=2=	O
(	O
cid:27	O
)	O
/	O
(	O
7.37	O
)	O
,	O
where	O
s	O
1=2	O
is	O
a	O
matrix	B
square	O
root	O
of	O
s	O
,	O
.s	O
1=2/2	O
d	O
s	O
.	O
then	O
o	O
ˇ	O
in	O
o	O
(	O
cid:13	O
)	O
(	O
cid:24	O
)	O
and	O
the	O
m	O
d	O
0	O
form	B
of	O
the	O
james–stein	O
rule	B
(	O
7.51	O
)	O
is	O
np	O
.	O
(	O
cid:13	O
)	O
;	O
i/	O
;	O
1	O
(	O
cid:0	O
)	O
p	O
(	O
cid:0	O
)	O
2	O
ko	O
(	O
cid:13	O
)	O
k2	O
transforming	O
back	O
to	O
the	O
ˇ	O
scale	B
gives	O
(	O
7.43	O
)	O
.	O
o	O
(	O
cid:13	O
)	O
js	O
d	O
(	O
cid:21	O
)	O
o	O
(	O
cid:13	O
)	O
:	O
(	O
7.62	O
)	O
(	O
7.63	O
)	O
8	O
generalized	O
linear	B
models	O
and	O
regression	O
trees	B
indirect	O
evidence	O
is	O
not	O
the	O
sole	O
property	O
of	O
bayesians	O
.	O
regression	B
models	O
are	O
the	O
frequentist	O
method	B
of	O
choice	O
for	O
incorporating	O
the	O
experience	O
of	O
“	O
others.	O
”	O
as	O
an	O
example	O
,	O
figure	O
8.1	O
returns	O
to	O
the	O
kidney	O
ﬁtness	O
data	B
of	O
section	O
1.1.	O
a	O
potential	O
new	O
donor	O
,	O
aged	O
55	O
,	O
has	O
appeared	O
,	O
and	O
we	O
wish	O
to	O
assess	O
his	O
kidney	O
ﬁtness	O
without	O
subjecting	O
him	O
to	O
an	O
arduous	O
series	O
of	O
medical	O
tests	O
.	O
only	O
one	O
of	O
the	O
157	O
previously	O
tested	O
volunteers	O
was	O
age	O
55	O
,	O
his	O
tot	O
score	O
being	O
(	O
cid:0	O
)	O
0:01	O
(	O
the	O
upper	O
large	O
dot	O
in	O
figure	O
8.1	O
)	O
.	O
most	O
dtot	O
d	O
(	O
cid:0	O
)	O
1:46.	O
the	O
former	O
is	O
the	O
only	O
direct	O
evidence	O
we	O
have	O
,	O
while	O
the	O
applied	O
statisticians	O
,	O
though	O
,	O
would	O
prefer	O
to	O
read	O
off	O
the	O
height	O
of	O
the	O
least	B
squares	I
regression	O
line	O
at	O
age	O
d	O
55	O
(	O
the	O
green	O
dot	O
on	O
the	O
regression	B
line	O
)	O
,	O
figure	O
8.1	O
kidney	O
data	O
;	O
a	O
new	O
volunteer	O
donor	O
is	O
aged	O
55.	O
which	O
prediction	O
is	O
preferred	O
for	O
his	O
kidney	B
function	I
?	O
108	O
*************************************************************************************************************************************************************2030405060708090−6−4−2024agetotll55	O
8.1	O
logistic	B
regression	I
109	O
regression	B
line	O
lets	O
us	O
incorporate	O
indirect	O
evidence	O
for	O
age	O
55	O
from	O
all	O
157	O
previous	O
cases	O
.	O
increasingly	O
aggressive	O
use	O
of	O
regression	B
techniques	O
is	O
a	O
hallmark	O
of	O
modern	O
statistical	O
practice	O
,	O
“	O
aggressive	O
”	O
applying	O
to	O
the	O
number	O
and	O
type	O
of	O
predictor	B
variables	O
,	O
the	O
coinage	O
of	O
new	O
methodology	O
,	O
and	O
the	O
sheer	O
size	O
of	O
the	O
target	O
data	B
sets	O
.	O
generalized	O
linear	B
models	O
,	O
this	O
chapter	O
’	O
s	O
main	O
topic	O
,	O
have	O
been	O
the	O
most	O
pervasively	O
inﬂuential	O
of	O
the	O
new	O
methods	O
.	O
the	O
chapter	O
ends	O
with	O
a	O
brief	O
review	O
of	O
regression	B
trees	O
,	O
a	O
completely	O
different	O
regres-	O
sion	O
methodology	O
that	O
will	O
play	O
an	O
important	O
role	O
in	O
the	O
prediction	O
algo-	O
rithms	O
of	O
chapter	O
17	O
.	O
8.1	O
logistic	B
regression	I
an	O
experimental	O
new	O
anti-cancer	O
drug	O
called	O
xilathon	O
is	O
under	O
devel-	O
opment	O
.	O
before	O
human	O
testing	O
can	O
begin	O
,	O
animal	O
studies	O
are	O
needed	O
to	O
de-	O
termine	O
safe	O
dosages	O
.	O
to	O
this	O
end	O
,	O
a	O
bioassay	O
or	O
dose–response	O
experiment	O
was	O
carried	O
out	O
:	O
11	O
groups	O
of	O
n	O
d	O
10	O
mice	O
each	O
were	O
injected	O
with	O
in-	O
creasing	O
amounts	O
of	O
xilathon	O
,	O
dosages	O
coded1	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
11.	O
let	O
yi	O
d	O
#	O
mice	O
dying	O
in	O
ith	O
group	O
:	O
the	O
points	O
in	O
figure	O
8.2	O
show	O
the	O
proportion	B
of	O
deaths	O
pi	O
d	O
yi	O
=10	O
;	O
(	O
8.1	O
)	O
(	O
8.2	O
)	O
lethality	O
generally	O
increasing	O
with	O
dose	O
.	O
the	O
counts	O
yi	O
are	O
modeled	O
as	O
in-	O
dependent	O
binomials	O
,	O
ind	O
(	O
cid:24	O
)	O
bi.ni	O
;	O
(	O
cid:25	O
)	O
i	O
/	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
;	O
yi	O
(	O
8.3	O
)	O
n	O
d	O
11	O
and	O
all	O
ni	O
equaling	O
10	O
here	O
;	O
(	O
cid:25	O
)	O
i	O
is	O
the	O
true	O
death	O
rate	B
in	O
group	O
i	O
,	O
estimated	O
unbiasedly	O
by	O
pi	O
,	O
the	O
direct	O
evidence	O
for	O
(	O
cid:25	O
)	O
i.	O
the	O
regression	B
curve	O
in	O
figure	O
8.2	O
uses	O
all	O
the	O
doses	O
to	O
give	O
a	O
better	O
picture	O
of	O
the	O
true	O
dose–response	O
relation	O
.	O
logistic	B
regression	I
is	O
a	O
specialized	O
technique	O
for	O
regression	B
analysis	O
of	O
count	O
or	O
proportion	B
data	O
.	O
the	O
logit	O
parameter	O
(	O
cid:21	O
)	O
is	O
deﬁned	O
as	O
o	O
n	O
(	O
cid:25	O
)	O
1	O
(	O
cid:0	O
)	O
(	O
cid:25	O
)	O
(	O
cid:21	O
)	O
d	O
log	O
;	O
(	O
8.4	O
)	O
1	O
dose	O
would	O
usually	O
be	O
labeled	O
on	O
a	O
log	O
scale	B
,	O
each	O
one	O
,	O
say	O
,	O
50	O
%	O
larger	O
than	O
its	O
predecessor	O
.	O
110	O
glms	O
and	O
regression	O
trees	B
figure	O
8.2	O
dose–response	O
study	O
;	O
groups	O
of	O
10	O
mice	O
exposed	O
to	O
increasing	O
doses	O
of	O
experimental	O
drug	O
.	O
the	O
points	O
are	O
the	O
observed	O
proportions	O
that	O
died	O
in	O
each	O
group	O
.	O
the	O
ﬁtted	O
curve	O
is	O
the	O
maximum-likelihoood	O
estimate	O
of	O
the	O
linear	B
logistic	O
regression	B
model	I
.	O
the	O
open	O
circle	O
on	O
the	O
curve	O
is	O
the	O
ld50	O
,	O
the	O
estimated	O
dose	O
for	O
50	O
%	O
mortality	O
.	O
with	O
(	O
cid:21	O
)	O
increasing	O
from	O
(	O
cid:0	O
)	O
1	O
to	O
1	O
as	O
(	O
cid:25	O
)	O
increases	O
from	O
0	O
to	O
1.	O
a	O
linear	B
lo-	O
gistic	O
regression	B
dose–response	O
analysis	B
begins	O
with	O
binomial	B
model	O
(	O
8.3	O
)	O
,	O
and	O
assumes	O
that	O
the	O
logit	O
is	O
a	O
linear	B
function	O
of	O
dose	O
,	O
(	O
cid:26	O
)	O
(	O
cid:25	O
)	O
i	O
1	O
(	O
cid:0	O
)	O
(	O
cid:25	O
)	O
i	O
(	O
cid:27	O
)	O
d	O
˛0	O
c	O
˛1xi	O
:	O
(	O
cid:21	O
)	O
i	O
d	O
log	O
maximum	B
likelihood	I
gives	O
estimates	O
.o˛0	O
;	O
o˛1/	O
,	O
and	O
ﬁtted	O
curve	O
(	O
8.5	O
)	O
(	O
8.6	O
)	O
(	O
8.7	O
)	O
(	O
8.8	O
)	O
since	O
the	O
inverse	O
transformation	O
of	O
(	O
8.4	O
)	O
is	O
we	O
obtain	O
from	O
(	O
8.6	O
)	O
the	O
linear	B
logistic	O
regression	B
curve	O
o	O
(	O
cid:21	O
)	O
.x/	O
d	O
o˛0	O
c	O
o˛1x	O
:	O
(	O
cid:25	O
)	O
d	O
(	O
cid:16	O
)	O
o	O
(	O
cid:25	O
)	O
.x/	O
d	O
(	O
cid:16	O
)	O
1	O
c	O
e	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
	O
(	O
cid:0	O
)	O
1	O
(	O
cid:0	O
)	O
.o˛0co˛1x/	O
(	O
cid:0	O
)	O
1	O
1	O
c	O
e	O
pictured	O
in	O
figure	O
8.2.	O
table	O
8.1	O
compares	O
the	O
standard	B
deviation	I
of	O
the	O
estimated	O
regression	B
llllllllllldoseproportion	O
of	O
deaths12345678910110.000.250.500.751.00llld50	O
=	O
5.69	O
8.1	O
logistic	B
regression	I
111	O
table	O
8.1	O
standard	B
deviation	I
estimates	O
for	O
o	O
(	O
cid:25	O
)	O
.x/	O
in	O
figure	O
8.1.	O
the	O
ﬁrst	O
row	O
is	O
for	O
the	O
linear	B
logistic	O
regression	B
ﬁt	O
(	O
8.8	O
)	O
;	O
the	O
second	O
row	O
is	O
based	O
on	O
the	O
individual	O
binomial	B
estimates	O
pi	O
.	O
x	O
sd	O
o	O
(	O
cid:25	O
)	O
.x/	O
sd	O
pi	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
11	O
.015	O
.045	O
.027	O
.066	O
.043	O
.094	O
.061	O
.126	O
.071	O
.152	O
.072	O
.157	O
.065	O
.138	O
.050	O
.106	O
.032	O
.076	O
.019	O
.052	O
.010	O
.035	O
curve	O
(	O
8.8	O
)	O
at	O
x	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
11	O
(	O
as	O
discussed	O
in	O
the	O
next	O
section	O
)	O
with	O
the	O
usual	O
binomial	B
standard	O
deviation	O
estimate	B
œpi	O
.1	O
(	O
cid:0	O
)	O
pi	O
/=101=2	O
obtained	O
by	O
considering	O
the	O
11	O
doses	O
separately.2	O
regression	B
has	O
reduced	O
error	O
by	O
better	O
than	O
50	O
%	O
,	O
the	O
price	O
being	O
possible	O
bias	O
if	O
model	B
(	O
8.5	O
)	O
goes	O
seriously	O
wrong	O
.	O
one	O
advantage	O
of	O
the	O
logit	O
transformation	O
is	O
that	O
(	O
cid:21	O
)	O
isn	O
’	O
t	B
restricted	O
to	O
the	O
range	O
œ0	O
;	O
1	O
,	O
so	O
model	B
(	O
8.5	O
)	O
never	O
verges	O
on	O
forbidden	O
territory	O
.	O
a	O
better	O
reason	O
has	O
to	O
do	O
with	O
the	O
exploitation	O
of	O
exponential	O
family	O
properties	O
.	O
we	O
can	O
rewrite	O
the	O
density	B
function	O
for	O
bi.n	O
;	O
y/	O
as	O
!	O
n	O
y	O
(	O
cid:25	O
)	O
y.1	O
(	O
cid:0	O
)	O
(	O
cid:25	O
)	O
/n	O
(	O
cid:0	O
)	O
y	O
d	O
e	O
(	O
cid:21	O
)	O
y	O
(	O
cid:0	O
)	O
n	O
.	O
(	O
cid:21	O
)	O
/	O
!	O
n	O
y	O
(	O
8.9	O
)	O
(	O
8.10	O
)	O
with	O
(	O
cid:21	O
)	O
the	O
logit	O
parameter	O
(	O
8.4	O
)	O
and	O
.	O
(	O
cid:21	O
)	O
/	O
d	O
logf1	O
c	O
e	O
(	O
cid:21	O
)	O
gi	O
(	O
8.9	O
)	O
is	O
a	O
one-parameter	B
exponential	O
family3	O
as	O
described	O
in	O
section	O
5.5	O
,	O
with	O
(	O
cid:21	O
)	O
the	O
natural	O
parameter	O
,	O
called	O
˛	O
there	O
.	O
let	O
y	O
d	O
.y1	O
;	O
y2	O
;	O
:	O
:	O
:	O
;	O
yn	O
/	O
denote	O
the	O
full	B
data	O
set	B
,	O
n	O
d	O
11	O
in	O
figure	O
8.2.	O
!	O
using	O
(	O
8.5	O
)	O
,	O
(	O
8.9	O
)	O
,	O
and	O
the	O
independence	O
of	O
the	O
yi	O
gives	O
the	O
probability	O
density	B
of	O
y	O
as	O
a	O
function	B
of	O
.˛0	O
;	O
˛1/	O
,	O
e	O
(	O
cid:21	O
)	O
i	O
yi	O
(	O
cid:0	O
)	O
ni	O
.	O
(	O
cid:21	O
)	O
i	O
/	O
1	O
ni	O
.˛0c˛1xi	O
/	O
(	O
cid:1	O
)	O
ny	O
(	O
cid:0	O
)	O
pn	O
f˛0	O
;	O
˛1	O
.y/	O
d	O
ny	O
d	O
e˛0s0c˛1s1	O
(	O
cid:1	O
)	O
e	O
(	O
8.11	O
)	O
ni	O
yi	O
!	O
;	O
id1	O
ni	O
yi	O
id1	O
2	O
for	O
the	O
separate-dose	O
standard	B
error	I
,	O
pi	O
was	O
taken	O
equal	O
to	O
the	O
ﬁtted	O
value	O
from	O
the	O
curve	O
in	O
figure	O
8.2	O
.	O
3	O
it	O
is	O
not	O
necessary	O
for	O
f	O
(	O
cid:22	O
)	O
0	O
.x/	O
in	O
(	O
5.46	O
)	O
on	O
page	O
64	O
to	O
be	O
a	O
probability	O
density	B
function	O
,	O
only	O
that	O
it	O
not	O
depend	O
on	O
the	O
parameter	O
(	O
cid:22	O
)	O
.	O
112	O
where	O
glms	O
and	O
regression	O
trees	B
s0	O
d	O
nx	O
yi	O
id1	O
and	O
s1	O
d	O
nx	O
id1	O
xi	O
yi	O
:	O
(	O
8.12	O
)	O
formula	B
(	O
8.11	O
)	O
expresses	O
f˛0	O
;	O
˛1	O
.y/	O
as	O
the	O
product	O
of	O
three	O
factors	O
,	O
f˛0	O
;	O
˛1	O
.y/	O
d	O
g˛0	O
;	O
˛1	O
.s0	O
;	O
s1/h.˛0	O
;	O
˛1/j.y/	O
;	O
(	O
8.13	O
)	O
1	O
only	O
the	O
ﬁrst	O
of	O
which	O
involves	O
both	O
the	O
parameters	O
and	O
the	O
data	B
.	O
this	O
im-	O
plies	O
that	O
.s0	O
;	O
s1/	O
is	O
a	O
sufﬁcient	O
statistic	B
:	O
	O
no	O
matter	O
how	O
large	O
n	O
might	O
be	O
(	O
later	O
we	O
will	O
have	O
n	O
in	O
the	O
thousands	O
)	O
,	O
just	O
the	O
two	O
numbers	O
.s0	O
;	O
s1/	O
con-	O
tain	O
all	O
of	O
the	O
experiment	O
’	O
s	O
information	B
.	O
only	O
the	O
logistic	O
parameterization	O
(	O
8.4	O
)	O
makes	O
this	O
happen.4	O
a	O
more	O
intuitive	O
picture	O
of	O
logistic	B
regression	I
depends	O
on	O
d.pi	O
;	O
o	O
(	O
cid:25	O
)	O
i	O
/	O
,	O
the	O
deviance	O
between	O
an	O
observed	O
proportion	B
pi	O
(	O
8.2	O
)	O
and	O
an	O
estimate	B
o	O
(	O
cid:25	O
)	O
i	O
,	O
	O
	O
pio	O
(	O
cid:25	O
)	O
i	O
	O
c	O
.1	O
(	O
cid:0	O
)	O
pi	O
/	O
log	O
	O
(	O
cid:21	O
)	O
	O
1	O
(	O
cid:0	O
)	O
pi	O
1	O
(	O
cid:0	O
)	O
o	O
(	O
cid:25	O
)	O
i	O
d	O
.pi	O
;	O
o	O
(	O
cid:25	O
)	O
i	O
/	O
d	O
2ni	O
pi	O
log	O
(	O
8.14	O
)	O
the	O
deviance5	O
is	O
zero	O
if	O
o	O
(	O
cid:25	O
)	O
i	O
d	O
pi	O
,	O
otherwise	O
it	O
increases	O
as	O
o	O
(	O
cid:25	O
)	O
i	O
departs	O
further	O
from	O
pi	O
.	O
the	O
logistic	B
regression	I
mle	O
value	O
.o˛0	O
;	O
o˛1/	O
also	O
turns	O
out	O
to	O
be	O
the	O
choice	O
of	O
.˛0	O
;	O
˛1/	O
minimizing	O
the	O
total	O
deviance	O
between	O
the	O
n	O
points	O
pi	O
and	O
their	O
corresponding	O
estimates	O
o	O
(	O
cid:25	O
)	O
i	O
d	O
(	O
cid:25	O
)	O
o˛0	O
;	O
o˛1	O
.xi	O
/	O
(	O
8.8	O
)	O
:	O
:	O
.o˛0	O
;	O
o˛1/	O
d	O
arg	O
min	O
.˛0	O
;	O
˛1/	O
nx	O
id1	O
d	O
.pi	O
;	O
(	O
cid:25	O
)	O
˛0	O
;	O
˛1	O
.xi	O
//	O
:	O
(	O
8.15	O
)	O
the	O
solid	O
line	O
in	O
figure	O
8.2	O
is	O
the	O
linear	B
logistic	O
curve	O
coming	O
closest	O
to	O
the	O
11	O
points	O
,	O
when	O
distance	O
is	O
measured	O
by	O
total	O
deviance	O
.	O
in	O
this	O
way	O
the	O
200-year-old	O
notion	O
of	O
least	B
squares	I
is	O
generalized	O
to	O
binomial	B
regression	O
,	O
as	O
discussed	O
in	O
the	O
next	O
section	O
.	O
a	O
more	O
sophisticated	O
notion	O
of	O
distance	O
between	O
data	B
and	O
models	B
is	O
one	O
of	O
the	O
accomplishments	O
of	O
modern	O
statis-	O
tics	O
.	O
table	O
8.2	O
reports	O
on	O
the	O
data	B
for	O
a	O
more	O
structured	O
logistic	B
regression	I
analysis	O
.	O
human	O
muscle	O
cell	O
colonies	O
were	O
infused	O
with	O
mouse	O
nuclei	O
in	O
ﬁve	O
different	O
ratios	O
,	O
cultured	O
over	O
time	O
periods	O
ranging	O
from	O
one	O
to	O
ﬁve	O
4	O
where	O
the	O
name	O
“	O
logistic	B
regression	I
”	O
comes	O
from	O
is	O
explained	O
in	O
the	O
endnotes	O
,	O
along	O
with	O
a	O
description	O
of	O
its	O
nonexponential	O
family	O
predecessor	O
probit	O
analysis	B
.	O
5	O
deviance	O
is	O
analogous	O
to	O
squared	O
error	O
in	O
ordinary	O
regression	B
theory	O
,	O
as	O
discussed	O
in	O
what	O
follows	O
.	O
it	O
is	O
twice	O
the	O
“	O
kullback–leibler	O
distance	O
,	O
”	O
the	O
preferred	O
name	O
in	O
the	O
information-theory	O
literature	O
.	O
8.1	O
logistic	B
regression	I
113	O
table	O
8.2	O
cell	B
infusion	I
data	O
;	O
human	O
cell	O
colonies	O
infused	O
with	O
mouse	O
nuclei	O
in	O
ﬁve	O
ratios	O
over	O
1	O
to	O
5	O
days	O
and	O
observed	O
to	O
see	O
whether	O
they	O
did	O
or	O
did	O
not	O
thrive	O
.	O
green	O
numbers	O
are	O
estimates	O
o	O
(	O
cid:25	O
)	O
ij	O
from	O
the	O
logistic	B
regression	I
model	O
.	O
for	O
example	O
,	O
5	O
of	O
31	O
colonies	O
in	O
the	O
lowest	O
ratio/days	O
category	O
thrived	O
,	O
with	O
observed	O
proportion	B
5=31	O
d	O
0:16	O
,	O
and	O
logistic	O
regression	B
estimate	O
o	O
(	O
cid:25	O
)	O
11	O
d	O
0:11	O
:	O
1	O
5/31	O
.11	O
15/77	O
.24	O
48/126	O
.38	O
29/92	O
.32	O
11/53	O
.18	O
2	O
3/28	O
.25	O
36/78	O
.45	O
68/116	O
.62	O
35/52	O
.56	O
20/52	O
.37	O
1	O
2	O
ratio	O
3	O
4	O
5	O
time	O
3	O
20/45	O
.42	O
43/71	O
.64	O
145/171	O
.77	O
57/85	O
.73	O
20/48	O
.55	O
4	O
24/47	O
.54	O
56/71	O
.74	O
98/119	O
.85	O
38/50	O
.81	O
40/55	O
.67	O
5	O
29/35	O
.75	O
66/74	O
.88	O
114/129	O
.93	O
72/77	O
.92	O
52/61	O
.84	O
days	O
,	O
and	O
observed	O
to	O
see	O
whether	O
they	O
thrived	O
.	O
for	O
example	O
,	O
of	O
the	O
126	O
colonies	O
having	O
the	O
third	O
ratio	O
and	O
shortest	O
time	O
period	O
,	O
48	O
thrived	O
.	O
let	O
(	O
cid:25	O
)	O
ij	O
denote	O
the	O
true	O
probability	O
of	O
thriving	O
for	O
ratio	O
i	O
during	O
time	O
period	O
j	O
,	O
and	O
(	O
cid:21	O
)	O
ij	O
its	O
logit	O
logf	O
(	O
cid:25	O
)	O
ij	O
=.1	O
(	O
cid:0	O
)	O
(	O
cid:25	O
)	O
ij	O
/g	O
.	O
a	O
two-way	O
additive	O
logistic	B
regression	I
was	O
ﬁt	O
to	O
the	O
data,6	O
(	O
cid:21	O
)	O
ij	O
d	O
(	O
cid:22	O
)	O
c	O
˛i	O
c	O
ˇj	O
;	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
5	O
;	O
j	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
5	O
:	O
(	O
8.16	O
)	O
the	O
green	O
numbers	O
in	O
table	O
8.2	O
show	O
the	O
maximum	B
likelihood	I
estimates	O
.	O
(	O
cid:0	O
)	O
(	O
cid:16	O
)	O
o	O
(	O
cid:22	O
)	O
co˛ic	O
oˇj	O
	O
(	O
cid:21	O
)	O
:	O
o	O
(	O
cid:25	O
)	O
ij	O
d	O
1	O
1	O
c	O
e	O
(	O
8.17	O
)	O
straintsp	O
˛i	O
d	O
p	O
ˇj	O
d	O
0	O
necessary	O
to	O
avoid	O
deﬁnitional	O
difﬁculties	O
)	O
model	B
(	O
8.16	O
)	O
has	O
nine	O
free	O
parameters	O
(	O
taking	O
into	O
account	O
the	O
con-	O
compared	O
with	O
just	O
two	O
in	O
the	O
dose–response	O
experiment	O
.	O
the	O
count	O
can	O
easily	O
go	O
much	O
higher	O
these	O
days	O
.	O
table	O
8.3	O
reports	O
on	O
a	O
57-variable	O
logistic	B
regression	I
applied	O
to	O
the	O
spam	B
data	O
.	O
a	O
researcher	O
(	O
named	O
george	O
)	O
labeled	O
n	O
d	O
4601	O
of	O
his	O
email	O
mes-	O
6	O
using	O
the	O
statistical	O
computing	O
language	O
r	O
;	O
see	O
the	O
endnotes	O
.	O
114	O
glms	O
and	O
regression	O
trees	B
table	O
8.3	O
logistic	B
regression	I
analysis	O
of	O
the	O
spam	B
data	O
,	O
model	B
(	O
8.17	O
)	O
;	O
estimated	O
regression	B
coefﬁcients	O
,	O
standard	O
errors	O
,	O
and	O
z	O
d	O
estimate=se	O
,	O
for	O
57	O
keyword	O
predictors	O
.	O
the	O
notation	O
char	O
$	O
means	O
the	O
relative	O
number	O
of	O
times	O
$	O
appears	O
,	O
etc	O
.	O
the	O
last	O
three	O
entries	O
measure	O
characteristics	O
such	O
as	O
length	O
of	O
capital-letter	O
strings	O
.	O
the	O
word	O
george	O
is	O
special	O
,	O
since	O
the	O
recipient	O
of	O
the	O
email	O
is	O
named	O
george	O
,	O
and	O
the	O
goal	O
here	O
is	O
to	O
build	O
a	O
customized	O
spam	B
ﬁlter	O
.	O
intercept	O
make	O
address	O
all	O
3d	O
our	O
over	O
remove	O
internet	O
order	O
mail	O
receive	O
will	O
people	O
report	O
addresses	O
free	O
business	O
email	O
you	O
credit	O
your	O
font	O
000	O
money	O
hp	O
hpl	O
george	O
650	O
estimate	B
(	O
cid:0	O
)	O
12.27	O
(	O
cid:0	O
)	O
.12	O
(	O
cid:0	O
)	O
.19	O
.06	O
3.14	O
.38	O
.24	O
.89	O
.23	O
.20	O
.08	O
(	O
cid:0	O
)	O
.05	O
(	O
cid:0	O
)	O
.12	O
(	O
cid:0	O
)	O
.02	O
.05	O
.32	O
.86	O
.43	O
.06	O
.14	O
.53	O
.29	O
.21	O
.79	O
.19	O
(	O
cid:0	O
)	O
3.21	O
(	O
cid:0	O
)	O
.92	O
(	O
cid:0	O
)	O
39.62	O
.24	O
se	O
1.99	O
.07	O
.09	O
.06	O
2.10	O
.07	O
.07	O
.13	O
.07	O
.08	O
.05	O
.06	O
.06	O
.07	O
.05	O
.19	O
.12	O
.10	O
.06	O
.06	O
.27	O
.06	O
.17	O
.16	O
.07	O
.52	O
.39	O
7.12	O
.11	O
z-value	O
(	O
cid:0	O
)	O
6.16	O
lab	O
(	O
cid:0	O
)	O
1.68	O
labs	O
(	O
cid:0	O
)	O
2.10	O
telnet	O
1.03	O
857	O
1.49	O
data	B
5.52	O
415	O
3.53	O
85	O
6.85	O
technology	O
3.39	O
1999	O
2.58	O
parts	O
1.75	O
pm	O
(	O
cid:0	O
)	O
.86	O
direct	O
(	O
cid:0	O
)	O
1.87	O
cs	O
(	O
cid:0	O
)	O
.35	O
meeting	O
1.06	O
original	O
1.70	O
project	O
7.13	O
re	O
4.26	O
edu	O
1.03	O
table	O
2.32	O
conference	O
1.95	O
char	O
;	O
4.62	O
char	O
(	O
1.24	O
char	O
4.76	O
char	O
!	O
2.63	O
char	O
$	O
(	O
cid:0	O
)	O
6.14	O
char	O
#	O
(	O
cid:0	O
)	O
2.37	O
cap.ave	O
(	O
cid:0	O
)	O
5.57	O
cap.long	O
2.24	O
cap.tot	O
estimate	B
(	O
cid:0	O
)	O
1.48	O
(	O
cid:0	O
)	O
.15	O
(	O
cid:0	O
)	O
.07	O
.84	O
(	O
cid:0	O
)	O
.41	O
.22	O
(	O
cid:0	O
)	O
1.09	O
.37	O
.02	O
(	O
cid:0	O
)	O
.13	O
(	O
cid:0	O
)	O
.38	O
(	O
cid:0	O
)	O
.11	O
(	O
cid:0	O
)	O
16.27	O
(	O
cid:0	O
)	O
2.06	O
(	O
cid:0	O
)	O
.28	O
(	O
cid:0	O
)	O
.98	O
(	O
cid:0	O
)	O
.80	O
(	O
cid:0	O
)	O
1.33	O
(	O
cid:0	O
)	O
.18	O
(	O
cid:0	O
)	O
1.15	O
(	O
cid:0	O
)	O
.31	O
(	O
cid:0	O
)	O
.05	O
(	O
cid:0	O
)	O
.07	O
.28	O
1.31	O
1.03	O
.38	O
1.78	O
.51	O
se	O
.89	O
.14	O
.19	O
1.08	O
.17	O
.53	O
.42	O
.12	O
.07	O
.09	O
.17	O
.13	O
9.61	O
.64	O
.18	O
.33	O
.16	O
.24	O
.13	O
.46	O
.11	O
.07	O
.09	O
.07	O
.17	O
.48	O
.60	O
.49	O
.14	O
sages	O
as	O
either	O
spam	B
or	O
ham	O
(	O
nonspam7	O
)	O
,	O
say	O
(	O
yi	O
d	O
1	O
if	O
email	O
i	O
is	O
spam	B
0	O
if	O
email	O
i	O
is	O
ham	O
z-value	O
(	O
cid:0	O
)	O
1.66	O
(	O
cid:0	O
)	O
1.05	O
(	O
cid:0	O
)	O
.35	O
.78	O
(	O
cid:0	O
)	O
2.37	O
.42	O
(	O
cid:0	O
)	O
2.61	O
2.99	O
.26	O
(	O
cid:0	O
)	O
1.41	O
(	O
cid:0	O
)	O
2.26	O
(	O
cid:0	O
)	O
.84	O
(	O
cid:0	O
)	O
1.69	O
(	O
cid:0	O
)	O
3.21	O
(	O
cid:0	O
)	O
1.55	O
(	O
cid:0	O
)	O
2.97	O
(	O
cid:0	O
)	O
5.09	O
(	O
cid:0	O
)	O
5.43	O
(	O
cid:0	O
)	O
1.40	O
(	O
cid:0	O
)	O
2.49	O
(	O
cid:0	O
)	O
2.92	O
(	O
cid:0	O
)	O
.75	O
(	O
cid:0	O
)	O
.78	O
3.89	O
7.55	O
2.16	O
.64	O
3.62	O
3.75	O
(	O
8.18	O
)	O
7	O
“	O
ham	O
”	O
refers	O
to	O
“	O
nonspam	O
”	O
or	O
good	O
email	O
;	O
this	O
is	O
a	O
playful	O
connection	O
to	O
the	O
processed	O
8.1	O
logistic	B
regression	I
115	O
(	O
40	O
%	O
of	O
the	O
messages	O
were	O
spam	B
)	O
.	O
the	O
p	O
d	O
57	O
predictor	B
variables	O
repre-	O
sent	O
the	O
most	O
frequently	O
used	O
words	O
and	O
tokens	O
in	O
george	O
’	O
s	O
corpus	O
of	O
email	O
(	O
excluding	O
trivial	O
words	O
such	O
as	O
articles	O
)	O
,	O
and	O
are	O
in	O
fact	O
the	O
relative	O
fre-	O
quencies	O
of	O
these	O
chosen	O
words	O
in	O
each	O
email	O
(	O
standardized	O
by	O
the	O
length	O
of	O
the	O
email	O
)	O
.	O
the	O
goal	O
of	O
the	O
study	O
was	O
to	O
predict	O
whether	O
future	O
emails	O
are	O
spam	B
or	O
ham	O
using	O
these	O
keywords	O
;	O
that	O
is	O
,	O
to	O
build	O
a	O
customized	O
spam	B
ﬁlter	O
.	O
let	O
xij	O
denote	O
the	O
relative	O
frequency	O
of	O
keyword	O
j	O
in	O
email	O
i	O
,	O
and	O
(	O
cid:25	O
)	O
i	O
represent	O
the	O
probability	O
that	O
email	O
i	O
is	O
spam	B
.	O
letting	O
(	O
cid:21	O
)	O
i	O
be	O
the	O
logit	O
trans-	O
form	B
logf	O
(	O
cid:25	O
)	O
i	O
=.1	O
(	O
cid:0	O
)	O
(	O
cid:25	O
)	O
i	O
/g	O
,	O
we	O
ﬁt	O
the	O
additive	O
logistic	O
model	O
(	O
8.19	O
)	O
table	O
8.3	O
shows	O
o˛i	O
for	O
each	O
word—for	O
example	O
,	O
(	O
cid:0	O
)	O
0:12	O
for	O
make—as	O
well	O
as	O
the	O
estimated	O
standard	B
error	I
and	O
the	O
z-value	O
:	O
estimate=se	O
.	O
˛j	O
xij	O
:	O
jd1	O
(	O
cid:21	O
)	O
i	O
d	O
˛0	O
c	O
57x	O
it	O
looks	O
like	O
certain	O
words	O
,	O
such	O
as	O
free	O
and	O
your	O
,	O
are	O
good	O
spam	B
predictors	O
.	O
however	O
,	O
the	O
table	O
as	O
a	O
whole	O
has	O
an	O
unstable	O
appearance	O
,	O
with	O
occasional	O
very	O
large	O
estimates	O
o˛i	O
accompanied	O
by	O
very	O
large	O
standard	O
de-	O
viations.8	O
the	O
dangers	O
of	O
high-dimensional	O
maximum	B
likelihood	I
estima-	O
tion	O
are	O
apparent	O
here	O
.	O
some	O
sort	O
of	O
shrinkage	B
estimation	O
is	O
called	O
for	O
,	O
as	O
discussed	O
in	O
chapter	O
16	O
.	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
——	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
——	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
——	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
regression	B
analysis	O
,	O
either	O
in	O
its	O
classical	O
form	B
or	O
in	O
modern	O
formula-	O
tions	O
,	O
requires	O
covariate	O
information	B
x	O
to	O
put	O
the	O
various	O
cases	O
into	O
some	O
sort	O
of	O
geometrical	O
relationship	O
.	O
given	O
such	O
information	B
,	O
regression	B
is	O
the	O
statistician	O
’	O
s	O
most	O
powerful	O
tool	O
for	O
bringing	O
“	O
other	O
”	O
results	O
to	O
bear	O
on	O
a	O
case	O
of	O
primary	O
interest	O
:	O
for	O
instance	O
,	O
the	O
age-55	O
volunteer	O
in	O
figure	O
8.1.	O
empirical	B
bayes	O
methods	O
do	O
not	O
require	O
covariate	O
information	B
but	O
may	O
be	O
improvable	O
if	O
it	O
exists	O
.	O
if	O
,	O
for	O
example	O
,	O
the	O
player	O
’	O
s	O
age	O
were	O
an	O
impor-	O
tant	O
covariate	O
in	O
the	O
baseball	B
example	O
of	O
table	O
7.1	O
,	O
we	O
might	O
ﬁrst	O
regress	O
the	O
mle	O
values	O
on	O
age	O
,	O
and	O
then	O
shrink	O
them	O
toward	O
the	O
regression	B
line	O
rather	O
than	O
toward	O
the	O
grand	O
mean	O
np	O
as	O
in	O
(	O
7.20	O
)	O
.	O
in	O
this	O
way	O
,	O
two	O
different	O
sorts	O
of	O
indirect	O
evidence	O
would	O
be	O
brought	O
to	O
bear	O
on	O
the	O
estimation	B
of	O
each	O
player	O
’	O
s	O
ability	O
.	O
spam	B
that	O
was	O
fake	O
ham	O
during	O
wwii	O
,	O
and	O
has	O
been	O
adopted	O
by	O
the	O
machine-learning	O
community	O
.	O
8	O
the	O
4601	O
(	O
cid:2	O
)	O
57	O
x	O
matrix	B
.xij	O
/	O
was	O
standardized	O
,	O
so	O
disparate	O
scalings	O
are	O
not	O
the	O
cause	O
of	O
these	O
discrepancies	O
.	O
some	O
of	O
the	O
features	O
have	O
mostly	O
“	O
zero	O
”	O
observations	O
,	O
which	O
may	O
account	O
for	O
their	O
unstable	O
estimation	B
.	O
116	O
glms	O
and	O
regression	O
trees	B
8.2	O
generalized	O
linear	B
models9	O
logistic	B
regression	I
is	O
a	O
special	O
case	O
of	O
generalized	O
linear	B
models	O
(	O
glms	O
)	O
,	O
a	O
key	O
1970s	O
methodology	O
having	O
both	O
algorithmic	O
and	O
inferential	O
inﬂu-	O
ence	O
.	O
glms	O
extend	O
ordinary	O
linear	B
regression	O
,	O
that	O
is	O
least	B
squares	I
curve-	O
ﬁtting	B
,	O
to	O
situations	O
where	O
the	O
response	O
variables	O
are	O
binomial	B
,	O
poisson	O
,	O
gamma	B
,	O
beta	B
,	O
or	O
in	O
fact	O
any	O
exponential	O
family	O
form	B
.	O
n	O
we	O
begin	O
with	O
a	O
one-parameter	B
exponential	O
family	O
,	O
f	O
(	O
cid:21	O
)	O
.y/	O
d	O
e	O
(	O
cid:21	O
)	O
y	O
(	O
cid:0	O
)	O
(	O
cid:13	O
)	O
.	O
(	O
cid:21	O
)	O
/f0.y/	O
;	O
(	O
cid:21	O
)	O
2	O
ƒ	O
(	O
8.20	O
)	O
o	O
;	O
as	O
in	O
(	O
5.46	O
)	O
(	O
now	O
with	O
˛	O
and	O
x	O
replaced	O
by	O
(	O
cid:21	O
)	O
and	O
y	O
,	O
and	O
.˛/	O
replaced	O
by	O
(	O
cid:13	O
)	O
.	O
(	O
cid:21	O
)	O
/	O
,	O
for	O
clearer	O
notation	O
in	O
what	O
follows	O
)	O
.	O
here	O
(	O
cid:21	O
)	O
is	O
the	O
natural	O
parameter	O
and	O
y	O
the	O
sufﬁcient	O
statistic	B
,	O
both	O
being	O
one-dimensional	O
in	O
usual	O
applica-	O
tions	O
;	O
(	O
cid:21	O
)	O
takes	O
its	O
values	O
in	O
an	O
interval	B
of	O
the	O
real	O
line	O
.	O
each	O
coordinate	O
yi	O
of	O
an	O
observed	O
data	B
set	O
y	O
d	O
.y1	O
;	O
y2	O
;	O
:	O
:	O
:	O
;	O
yi	O
;	O
:	O
:	O
:	O
;	O
yn	O
/	O
0	O
is	O
assumed	O
to	O
come	O
from	O
a	O
member	O
of	O
family	O
(	O
8.20	O
)	O
,	O
yi	O
(	O
cid:24	O
)	O
f	O
(	O
cid:21	O
)	O
i	O
.	O
(	O
cid:1	O
)	O
/	O
independently	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
:	O
(	O
8.21	O
)	O
table	O
8.4	O
lists	O
(	O
cid:21	O
)	O
and	O
y	O
for	O
the	O
ﬁrst	O
four	O
families	O
in	O
table	O
5.1	O
,	O
as	O
well	O
as	O
their	O
deviance	O
and	O
normalizing	O
functions	O
.	O
by	O
itself	O
,	O
model	B
(	O
8.21	O
)	O
requires	O
n	O
parameters	O
(	O
cid:21	O
)	O
1	O
;	O
(	O
cid:21	O
)	O
2	O
;	O
:	O
:	O
:	O
;	O
(	O
cid:21	O
)	O
n	O
,	O
usually	O
too	O
many	O
for	O
effective	O
individual	O
estimation	B
.	O
a	O
key	O
glm	O
tactic	O
is	O
to	O
specify	O
the	O
(	O
cid:21	O
)	O
s	O
in	O
terms	O
of	O
a	O
linear	B
regression	O
equation	B
.	O
let	O
x	O
be	O
an	O
n	O
(	O
cid:2	O
)	O
p	O
“	O
structure	O
0	O
matrix	B
,	O
”	O
with	O
ith	O
row	O
say	O
x	O
i	O
,	O
and	O
˛	O
an	O
unknown	O
vector	B
of	O
p	O
parameters	O
;	O
the	O
n	O
-vector	O
(	O
cid:21	O
)	O
d	O
.	O
(	O
cid:21	O
)	O
1	O
;	O
(	O
cid:21	O
)	O
2	O
;	O
:	O
:	O
:	O
;	O
(	O
cid:21	O
)	O
n	O
/	O
0	O
is	O
then	O
speciﬁed	O
by	O
(	O
cid:21	O
)	O
d	O
x	O
˛	O
:	O
(	O
8.22	O
)	O
in	O
the	O
dose–response	O
experiment	O
of	O
figure	O
8.2	O
and	O
model	O
(	O
8.5	O
)	O
,	O
x	O
is	O
n	O
(	O
cid:2	O
)	O
2	O
with	O
ith	O
row	O
.1	O
;	O
xi	O
/	O
and	O
parameter	O
vector	B
˛	O
d	O
.˛0	O
;	O
˛1/	O
.	O
ny	O
the	O
probability	O
density	B
function	O
f˛.y/	O
of	O
the	O
data	B
vector	O
y	O
is	O
pn	O
1	O
.	O
(	O
cid:21	O
)	O
i	O
yi	O
(	O
cid:0	O
)	O
(	O
cid:13	O
)	O
.	O
(	O
cid:21	O
)	O
i	O
//	O
f˛.y/	O
d	O
ny	O
f	O
(	O
cid:21	O
)	O
i	O
.yi	O
/	O
d	O
e	O
(	O
8.23	O
)	O
id1	O
f0.yi	O
/	O
;	O
id1	O
which	O
can	O
be	O
written	O
as	O
f˛.y/	O
d	O
e˛	O
0	O
z	O
(	O
cid:0	O
)	O
.˛/f0.y/	O
;	O
(	O
8.24	O
)	O
9	O
some	O
of	O
the	O
more	O
technical	O
points	O
raised	O
in	O
this	O
section	O
are	O
referred	O
to	O
in	O
later	O
chapters	O
,	O
and	O
can	O
be	O
scanned	O
or	O
omitted	O
at	O
ﬁrst	O
reading	O
.	O
8.2	O
generalized	O
linear	B
models	O
117	O
table	O
8.4	O
exponential	O
family	O
form	B
for	O
ﬁrst	O
four	O
cases	O
in	O
table	O
5.1	O
;	O
natural	O
parameter	O
(	O
cid:21	O
)	O
,	O
sufﬁcient	O
statistic	B
y	O
,	O
deviance	O
(	O
8.31	O
)	O
between	O
family	O
members	O
f1	O
and	O
f2	O
,	O
d.f1	O
;	O
f2/	O
,	O
and	O
normalizing	O
function	B
(	O
cid:13	O
)	O
.	O
(	O
cid:21	O
)	O
/	O
.	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
=	O
(	O
cid:27	O
)	O
2	O
y	O
x	O
log	O
(	O
cid:22	O
)	O
x	O
log	O
(	O
cid:25	O
)	O
1	O
(	O
cid:0	O
)	O
(	O
cid:25	O
)	O
x	O
2n	O
h	B
(	O
cid:27	O
)	O
i	O
d.f1	O
;	O
f2/	O
2	O
	O
(	O
cid:0	O
)	O
log	O
(	O
cid:22	O
)	O
2	O
(	O
cid:16	O
)	O
(	O
cid:22	O
)	O
1	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
2	O
h	B
(	O
cid:16	O
)	O
(	O
cid:22	O
)	O
2	O
c	O
.1	O
(	O
cid:0	O
)	O
(	O
cid:25	O
)	O
1/	O
log	O
1	O
(	O
cid:0	O
)	O
(	O
cid:25	O
)	O
1	O
1	O
(	O
cid:0	O
)	O
(	O
cid:25	O
)	O
2	O
i	O
	O
(	O
cid:0	O
)	O
log	O
(	O
cid:27	O
)	O
1	O
h	B
(	O
cid:16	O
)	O
(	O
cid:27	O
)	O
1	O
(	O
cid:0	O
)	O
1	O
(	O
cid:22	O
)	O
1	O
(	O
cid:13	O
)	O
.	O
(	O
cid:21	O
)	O
/	O
(	O
cid:27	O
)	O
2	O
(	O
cid:21	O
)	O
2=2	O
i	O
e	O
(	O
cid:21	O
)	O
n	O
log.1	O
c	O
e	O
(	O
cid:21	O
)	O
/	O
(	O
cid:0	O
)	O
(	O
cid:23	O
)	O
log	O
.	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
/	O
2	O
(	O
cid:22	O
)	O
1	O
(	O
cid:22	O
)	O
1	O
(	O
cid:25	O
)	O
1	O
log	O
(	O
cid:25	O
)	O
1	O
(	O
cid:25	O
)	O
2	O
(	O
cid:0	O
)	O
1=	O
(	O
cid:27	O
)	O
x	O
2	O
(	O
cid:23	O
)	O
(	O
cid:0	O
)	O
1	O
(	O
cid:27	O
)	O
2	O
(	O
cid:27	O
)	O
2	O
1.	O
normal	B
n	O
.	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
2/	O
,	O
(	O
cid:27	O
)	O
2	O
known	O
2.	O
poisson	O
poi	O
.	O
(	O
cid:22	O
)	O
/	O
3.	O
binomial	B
bi.n	O
;	O
(	O
cid:25	O
)	O
/	O
4.	O
gamma	B
gam	O
.	O
(	O
cid:23	O
)	O
;	O
(	O
cid:27	O
)	O
/	O
,	O
(	O
cid:23	O
)	O
known	O
where	O
y	O
and	O
.˛/	O
d	O
nx	O
id1	O
z	O
d	O
x	O
0	O
0	O
i	O
˛/	O
;	O
(	O
cid:13	O
)	O
.x	O
(	O
8.25	O
)	O
a	O
p-parameter	B
exponential	O
family	O
(	O
5.50	O
)	O
,	O
with	O
natural	O
parameter	O
vector	B
˛	O
and	O
sufﬁcient	O
statistic	B
vector	O
z.	O
the	O
main	O
point	O
is	O
that	O
all	O
the	O
information	B
from	O
a	O
p-parameter	B
glm	O
is	O
summarized	O
in	O
the	O
p-dimensional	O
vector	B
z	O
,	O
no	O
matter	O
how	O
large	O
n	O
may	O
be	O
,	O
making	O
it	O
easier	O
both	O
to	O
understand	O
and	O
to	O
analyze	O
.	O
we	O
have	O
now	O
reduced	O
the	O
n	O
-parameter	O
model	B
(	O
8.20	O
)	O
–	O
(	O
8.21	O
)	O
to	O
the	O
p-	O
parameter	O
exponential	O
family	O
(	O
8.24	O
)	O
,	O
with	O
p	O
usually	O
much	O
smaller	O
than	O
n	O
,	O
in	O
this	O
way	O
avoiding	O
the	O
difﬁculties	O
of	O
high-dimensional	O
estimation	B
.	O
the	O
moments	O
of	O
the	O
one-parameter	B
constituents	O
(	O
8.20	O
)	O
determine	O
the	O
estimation	B
properties	O
in	O
model	B
(	O
8.22	O
)	O
–	O
(	O
8.24	O
)	O
.	O
let	O
.	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
;	O
(	O
cid:27	O
)	O
2	O
(	O
cid:21	O
)	O
/	O
denote	O
the	O
expectation	O
and	O
variance	O
of	O
univariate	O
density	B
f	O
(	O
cid:21	O
)	O
.y/	O
(	O
8.20	O
)	O
,	O
y	O
(	O
cid:24	O
)	O
.	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
;	O
(	O
cid:27	O
)	O
2	O
(	O
cid:21	O
)	O
/	O
;	O
(	O
8.26	O
)	O
(	O
cid:21	O
)	O
/	O
d	O
.e	O
(	O
cid:21	O
)	O
;	O
e	O
(	O
cid:21	O
)	O
/	O
for	O
the	O
poisson	O
.	O
the	O
n	O
-vector	O
y	O
obtained	O
for	O
instance	O
.	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
;	O
(	O
cid:27	O
)	O
2	O
from	O
glm	O
(	O
8.22	O
)	O
then	O
has	O
mean	O
vector	B
and	O
covariance	O
matrix	B
y	O
(	O
cid:24	O
)	O
.	O
(	O
cid:22	O
)	O
.˛/	O
;	O
†.˛//	O
;	O
(	O
8.27	O
)	O
glms	O
and	O
regression	O
trees	B
118	O
where	O
(	O
cid:22	O
)	O
.˛/	O
is	O
the	O
vector	B
with	O
ith	O
component	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
i	O
with	O
(	O
cid:21	O
)	O
i	O
d	O
x	O
0	O
i	O
˛	O
,	O
and	O
†.˛/	O
is	O
the	O
n	O
(	O
cid:2	O
)	O
n	O
diagonal	O
matrix	B
having	O
diagonal	O
elements	O
(	O
cid:27	O
)	O
2	O
.	O
the	O
maximum	B
likelihood	I
estimate	O
o˛	O
of	O
the	O
parameter	O
vector	B
˛	O
can	O
be	O
(	O
cid:21	O
)	O
i	O
shown	O
to	O
satisfy	O
the	O
simple	O
equation	O
0	O
œy	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
.o˛/	O
d	O
0	O
:	O
x	O
for	O
the	O
normal	B
case	O
where	O
yi	O
(	O
cid:24	O
)	O
linear	B
regression	O
,	O
(	O
cid:22	O
)	O
.o˛/	O
d	O
x	O
o˛	O
and	O
(	O
8.28	O
)	O
becomes	O
x	O
the	O
familiar	O
solution	O
(	O
8.28	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
i	O
;	O
(	O
cid:27	O
)	O
2/	O
in	O
(	O
8.21	O
)	O
,	O
that	O
is	O
,	O
for	O
ordinary	O
.y	O
(	O
cid:0	O
)	O
x	O
o˛/	O
d	O
0	O
,	O
with	O
0	O
o˛	O
d	O
.x	O
0	O
(	O
cid:0	O
)	O
1x	O
0	O
yi	O
x	O
/	O
otherwise	O
,	O
(	O
cid:22	O
)	O
.˛/	O
is	O
a	O
nonlinear	B
function	O
of	O
˛	O
,	O
and	O
(	O
8.28	O
)	O
must	O
be	O
solved	O
by	O
numerical	O
iteration	O
.	O
this	O
is	O
made	O
easier	O
by	O
the	O
fact	O
that	O
,	O
for	O
glms	O
,	O
log	O
f˛.y/	O
,	O
the	O
likelihood	B
function	O
we	O
wish	O
to	O
maximize	O
,	O
is	O
a	O
concave	O
func-	O
tion	O
of	O
˛	O
.	O
the	O
mle	O
o˛	O
has	O
approximate	O
expectation	O
and	O
covariance	O
(	O
8.29	O
)	O
(	O
8.30	O
)	O
similar	O
to	O
the	O
exact	O
ols	O
result	O
o˛	O
(	O
cid:24	O
)	O
.˛	O
;	O
(	O
cid:27	O
)	O
generalizing	O
the	O
binomial	B
deﬁnition	O
(	O
8.14	O
)	O
,	O
the	O
deviance	O
between	O
den-	O
sities	O
f1.y/	O
and	O
f2.y/	O
is	O
deﬁned	O
to	O
be	O
d.f1	O
;	O
f2/	O
d	O
2	O
dy	O
;	O
(	O
8.31	O
)	O
o˛	O
p	O
(	O
cid:24	O
)	O
.˛	O
;	O
(	O
cid:0	O
)	O
x	O
z	O
0	O
/	O
;	O
†.˛/x	O
(	O
cid:1	O
)	O
(	O
cid:0	O
)	O
1	O
(	O
cid:26	O
)	O
f1.y/	O
(	O
cid:0	O
)	O
2.x	O
x	O
/	O
0	O
f1.y/	O
log	O
(	O
cid:0	O
)	O
1/.	O
(	O
cid:27	O
)	O
y	O
f2.y/	O
the	O
integral	O
(	O
or	O
sum	O
for	O
discrete	O
distributions	O
)	O
being	O
over	O
their	O
common	O
sample	B
space	O
y.	O
d.f1	O
;	O
f2/	O
is	O
always	O
nonnegative	O
,	O
equaling	O
zero	O
only	O
if	O
f1	O
and	O
f2	O
are	O
the	O
same	O
;	O
in	O
general	O
d.f1	O
;	O
f2/	O
does	O
not	O
equal	O
d.f2	O
;	O
f1/	O
.	O
deviance	O
does	O
not	O
depend	O
on	O
how	O
the	O
two	O
densities	O
are	O
named	O
,	O
for	O
example	O
(	O
8.14	O
)	O
having	O
the	O
same	O
expression	O
as	O
the	O
binomial	B
entry	O
in	O
table	O
8.4.	O
in	O
what	O
follows	O
it	O
will	O
sometimes	O
be	O
useful	O
to	O
label	O
the	O
family	O
(	O
8.20	O
)	O
by	O
its	O
expectation	O
parameter	O
(	O
cid:22	O
)	O
d	O
e	O
(	O
cid:21	O
)	O
fyg	O
rather	O
than	O
by	O
the	O
natural	O
parameter	O
(	O
cid:21	O
)	O
:	O
f	O
(	O
cid:22	O
)	O
.y/	O
d	O
e	O
(	O
cid:21	O
)	O
y	O
(	O
cid:0	O
)	O
(	O
cid:13	O
)	O
.	O
(	O
cid:21	O
)	O
/f0.y/	O
;	O
(	O
8.32	O
)	O
meaning	O
the	O
same	O
thing	O
as	O
(	O
8.20	O
)	O
,	O
only	O
the	O
names	O
attached	O
to	O
the	O
individ-	O
ual	O
family	O
members	O
being	O
changed	O
.	O
in	O
this	O
notation	O
it	O
is	O
easy	O
to	O
show	O
a	O
fundamental	O
result	O
sometimes	O
known	O
as	O
hoeffding	O
’	O
s	O
lemma	O
	O
the	O
maximum	B
likelihood	I
estimate	O
of	O
(	O
cid:22	O
)	O
given	O
y	O
is	O
y	O
itself	O
,	O
and	O
the	O
log	O
likelihood	B
log	O
f	O
(	O
cid:22	O
)	O
.y/	O
decreases	O
from	O
its	O
maximum	O
log	O
fy.y/	O
by	O
an	O
amount	O
that	O
depends	O
on	O
the	O
deviance	O
d.y	O
;	O
(	O
cid:22	O
)	O
/	O
,	O
f	O
(	O
cid:22	O
)	O
.y/	O
d	O
fy.y/e	O
(	O
cid:0	O
)	O
d.y	O
;	O
(	O
cid:22	O
)	O
/=2	O
:	O
(	O
8.33	O
)	O
2	O
3	O
4	O
5	O
8.2	O
generalized	O
linear	B
models	O
119	O
returning	O
to	O
the	O
glm	O
framework	O
(	O
8.21	O
)	O
–	O
(	O
8.22	O
)	O
,	O
parameter	O
vector	B
˛	O
gives	O
(	O
cid:21	O
)	O
.˛/	O
d	O
x	O
˛	O
,	O
which	O
in	O
turn	O
gives	O
the	O
vector	B
of	O
expectation	O
param-	O
eters	O
(	O
cid:22	O
)	O
.˛/	O
d	O
.	O
:	O
:	O
:	O
(	O
cid:22	O
)	O
i	O
.˛/	O
:	O
:	O
:	O
/	O
0	O
#	O
(	O
8.34	O
)	O
for	O
instance	O
(	O
cid:22	O
)	O
i	O
.˛/	O
d	O
expf	O
(	O
cid:21	O
)	O
i	O
.˛/g	O
for	O
the	O
poisson	O
family	O
.	O
multiplying	O
hoeff-	O
ding	O
’	O
s	O
lemma	O
(	O
8.33	O
)	O
over	O
the	O
n	O
cases	O
y	O
d	O
.y1	O
;	O
y2	O
;	O
:	O
:	O
:	O
;	O
yn	O
/	O
0	O
yields	O
;	O
f	O
(	O
cid:22	O
)	O
i	O
.˛/.yi	O
/	O
d	O
''	O
ny	O
f˛.y/	O
d	O
ny	O
minimizes	O
the	O
total	O
deviancepn	O
(	O
8.35	O
)	O
this	O
has	O
an	O
important	O
consequence	O
:	O
the	O
mle	O
o˛	O
is	O
the	O
choice	O
of	O
˛	O
that	O
1	O
d.yi	O
;	O
(	O
cid:22	O
)	O
i	O
.˛//	O
.	O
as	O
in	O
figure	O
8.2	O
,	O
glm	O
maximum	B
likelihood	I
ﬁtting	O
is	O
“	O
least	O
total	O
deviance	O
”	O
in	O
the	O
same	O
way	O
that	O
ordinary	O
linear	B
regression	O
is	O
least	O
sum	O
of	O
squares	B
.	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
——	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
——	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
——	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
d.yi	O
;	O
(	O
cid:22	O
)	O
i	O
.˛//	O
:	O
id1	O
fyi	O
.yi	O
/	O
e	O
id1	O
(	O
cid:0	O
)	O
pn	O
the	O
inner	O
circle	O
of	O
figure	O
8.3	O
represents	O
normal	B
theory	O
,	O
the	O
preferred	O
venue	O
of	O
classical	O
applied	O
statistics	B
.	O
exact	O
inferences—t-tests	O
,	O
f	O
distribu-	O
tions	O
,	O
most	O
of	O
multivariate	B
analysis—were	O
feasible	O
within	O
the	O
circle	O
.	O
out-	O
side	O
the	O
circle	O
was	O
a	O
general	O
theory	B
based	O
mainly	O
on	O
asymptotic	O
(	O
large-	O
sample	B
)	O
approximations	O
involving	O
taylor	O
expansions	O
and	O
the	O
central	O
limit	O
theorem	B
.	O
figure	O
8.3	O
three	O
levels	O
of	O
statistical	O
modeling	O
.	O
a	O
few	O
useful	O
exact	O
results	O
lay	O
outside	O
the	O
normal	B
theory	O
circle	O
,	O
relating	O
normal	B
theory	O
(	O
exact	O
calculations	O
)	O
exponential	O
families	O
(	O
partly	O
exact	O
)	O
general	O
theory	B
(	O
asymptotics	O
)	O
figure	O
8.3.	O
three	O
levels	O
of	O
statistical	O
modeling	O
120	O
glms	O
and	O
regression	O
trees	B
to	O
a	O
few	O
special	O
families	O
:	O
the	O
binomial	B
,	O
poisson	O
,	O
gamma	B
,	O
beta	B
,	O
and	O
others	O
less	O
well	O
known	O
.	O
exponential	O
family	O
theory	B
,	O
the	O
second	O
circle	O
in	O
figure	O
8.3	O
,	O
uniﬁed	O
the	O
special	O
cases	O
into	O
a	O
coherent	O
whole	O
.	O
it	O
has	O
a	O
“	O
partly	O
exact	O
”	O
ﬂa-	O
vor	O
,	O
with	O
some	O
ideal	O
counterparts	O
to	O
normal	B
theory—convex	O
likelihood	B
sur-	O
faces	O
,	O
least	O
deviance	O
regression—but	O
with	O
some	O
approximations	O
necessary	O
,	O
as	O
in	O
(	O
8.30	O
)	O
.	O
even	O
the	O
approximations	O
,	O
though	O
,	O
are	O
often	O
more	O
convincing	O
than	O
those	O
of	O
general	O
theory	B
,	O
exponential	O
families	O
’	O
ﬁxed-dimension	O
sufﬁ-	O
cient	O
statistics	B
making	O
the	O
asymptotics	O
more	O
transparent	O
.	O
logistic	B
regression	I
has	O
banished	O
its	O
predecessors	O
(	O
such	O
as	O
probit	O
anal-	O
ysis	O
)	O
almost	O
entirely	O
from	O
the	O
ﬁeld	O
,	O
and	O
not	O
only	O
because	O
of	O
estimating	O
efﬁciencies	O
and	O
computational	O
advantages	O
(	O
which	O
are	O
actually	O
rather	O
mod-	O
est	O
)	O
,	O
but	O
also	O
because	O
it	O
is	O
seen	O
as	O
a	O
clearer	O
analogue	O
to	O
ordinary	O
least	B
squares	I
,	O
our	O
200-year-old	O
dependable	O
standby	O
.	O
glm	O
research	O
development	O
has	O
been	O
mostly	O
frequentist	O
,	O
but	O
with	O
a	O
substantial	O
admixture	O
of	O
likelihood-	O
based	O
reasoning	O
,	O
and	O
a	O
hint	O
of	O
fisher	O
’	O
s	O
“	O
logic	O
of	O
inductive	O
inference.	O
”	O
helping	O
the	O
statistician	O
choose	O
between	O
competing	O
methodologies	O
is	O
the	O
job	O
of	O
statistical	O
inference	B
.	O
in	O
the	O
case	O
of	O
generalized	O
linear	B
models	O
the	O
choice	O
has	O
been	O
made	O
,	O
at	O
least	O
partly	O
,	O
in	O
terms	O
of	O
aesthetics	O
as	O
well	O
as	O
phi-	O
losophy	O
.	O
8.3	O
poisson	O
regression	B
the	O
third	O
most-used	O
member	O
of	O
the	O
glm	O
family	O
,	O
after	O
normal	O
theory	B
least	O
squares	B
and	O
logistic	B
regression	I
,	O
is	O
poisson	O
regression	B
.	O
n	O
independent	O
pois-	O
son	O
variates	O
are	O
observed	O
,	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
;	O
where	O
(	O
cid:21	O
)	O
i	O
d	O
log	O
(	O
cid:22	O
)	O
i	O
is	O
assumed	O
to	O
follow	O
a	O
linear	B
model	I
,	O
ind	O
(	O
cid:24	O
)	O
poi	O
.	O
(	O
cid:22	O
)	O
i	O
/	O
;	O
yi	O
(	O
8.36	O
)	O
(	O
cid:21	O
)	O
.˛/	O
d	O
x	O
˛	O
;	O
(	O
8.37	O
)	O
where	O
x	O
is	O
a	O
known	O
n	O
(	O
cid:2	O
)	O
p	O
structure	O
matrix	B
and	O
˛	O
an	O
unknown	O
p-vector	O
i	O
˛	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
,	O
where	O
x	O
of	O
regression	B
coefﬁcients	O
.	O
that	O
is	O
,	O
(	O
cid:21	O
)	O
i	O
d	O
x	O
0	O
0	O
is	O
the	O
ith	O
row	O
of	O
x.	O
in	O
the	O
chapters	O
that	O
follow	O
we	O
will	O
see	O
poisson	O
regression	B
come	O
to	O
the	O
rescue	O
in	O
what	O
at	O
ﬁrst	O
appear	O
to	O
be	O
awkward	O
data-analytic	O
situations	O
.	O
here	O
we	O
will	O
settle	O
for	O
an	O
example	O
involving	O
density	B
estimation	O
from	O
a	O
spatially	O
truncated	O
sample	B
.	O
i	O
6	O
table	O
8.5	O
shows	O
galaxy	B
counts	O
	O
from	O
a	O
small	O
portion	O
of	O
the	O
sky	O
:	O
487	O
galaxies	O
have	O
had	O
their	O
redshifts	O
r	O
and	O
apparent	O
magnitudes	O
m	O
measured	O
.	O
8.3	O
poisson	O
regression	B
121	O
table	O
8.5	O
counts	O
for	O
a	O
truncated	O
sample	B
of	O
487	O
galaxies	O
,	O
binned	O
by	O
redshift	O
and	O
magnitude	O
.	O
redshift	O
(	O
farther	O
)	O
(	O
cid:0	O
)	O
!	O
1	O
1	O
3	O
3	O
1	O
1	O
3	O
2	O
4	O
1	O
1	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
2	O
6	O
2	O
2	O
1	O
3	O
2	O
0	O
1	O
0	O
1	O
0	O
1	O
0	O
3	O
0	O
1	O
1	O
1	O
3	O
6	O
3	O
3	O
4	O
2	O
4	O
2	O
1	O
0	O
0	O
0	O
0	O
3	O
1	O
1	O
0	O
0	O
0	O
4	O
3	O
4	O
3	O
3	O
3	O
5	O
4	O
4	O
2	O
2	O
0	O
1	O
1	O
1	O
1	O
0	O
0	O
0	O
5	O
1	O
0	O
3	O
4	O
3	O
3	O
5	O
7	O
2	O
2	O
1	O
1	O
1	O
0	O
1	O
0	O
0	O
0	O
6	O
4	O
5	O
2	O
3	O
4	O
6	O
4	O
3	O
2	O
2	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
7	O
6	O
7	O
9	O
2	O
5	O
4	O
2	O
3	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
8	O
8	O
6	O
9	O
3	O
7	O
3	O
3	O
1	O
2	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
9	O
8	O
6	O
6	O
8	O
6	O
2	O
3	O
2	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
10	O
20	O
7	O
3	O
9	O
7	O
2	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
11	O
10	O
5	O
5	O
4	O
3	O
5	O
1	O
1	O
0	O
1	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
12	O
7	O
7	O
4	O
3	O
4	O
1	O
2	O
1	O
1	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
13	O
16	O
6	O
5	O
4	O
0	O
0	O
0	O
0	O
2	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
14	O
15	O
9	O
8	O
2	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
4	O
5	O
1	O
1	O
1	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
''	O
18	O
17	O
16	O
15	O
14	O
13	O
12	O
11	O
magnitude	O
10	O
9	O
8	O
7	O
6	O
5	O
4	O
3	O
2	O
1	O
(	O
dimmer	O
)	O
distance	O
from	O
earth	O
is	O
an	O
increasing	O
function	B
of	O
r	O
,	O
while	O
apparent	O
bright-	O
ness	O
is	O
a	O
decreasing	O
function10	O
of	O
m.	O
in	O
this	O
survey	O
,	O
counts	O
were	O
limited	O
to	O
galaxies	O
having	O
(	O
8.38	O
)	O
1:22	O
	O
r	O
	O
3:32	O
and	O
17:2	O
	O
m	O
	O
21:5	O
;	O
the	O
upper	O
limit	O
reﬂecting	O
the	O
difﬁculty	O
of	O
measuring	O
very	O
dim	O
galaxies	O
.	O
the	O
range	O
of	O
log	O
r	O
has	O
been	O
divided	O
into	O
15	O
equal	O
intervals	B
and	O
likewise	O
18	O
equal	O
intervals	B
for	O
m.	O
table	O
8.5	O
gives	O
the	O
counts	O
of	O
the	O
487	O
galaxies	O
in	O
the	O
18	O
(	O
cid:2	O
)	O
15	O
d	O
270	O
bins	O
.	O
(	O
the	O
lower	O
right	O
corner	O
of	O
the	O
table	O
is	O
empty	O
be-	O
cause	O
distant	O
galaxies	O
always	O
appear	O
dim	O
.	O
)	O
the	O
multinomial/poisson	O
con-	O
nection	O
(	O
5.44	O
)	O
helps	O
motivate	O
model	B
(	O
8.36	O
)	O
,	O
picturing	O
the	O
table	O
as	O
a	O
multi-	O
nomial	O
observation	O
on	O
270	O
categories	O
,	O
in	O
which	O
the	O
sample	B
size	I
n	O
was	O
itself	O
poisson	O
.	O
we	O
can	O
imagine	O
table	O
8.5	O
as	O
a	O
small	O
portion	O
of	O
a	O
much	O
more	O
extensive	O
table	O
,	O
hypothetically	O
available	O
if	O
the	O
data	B
were	O
not	B
truncated	I
.	O
experience	O
suggests	O
that	O
we	O
might	O
then	O
ﬁt	O
an	O
appropriate	O
bivariate	O
normal	B
density	O
to	O
the	O
data	B
,	O
as	O
in	O
figure	O
5.3.	O
it	O
seems	O
like	O
it	O
might	O
be	O
awkward	O
to	O
ﬁt	O
part	O
of	O
a	O
bivariate	O
normal	B
density	O
to	O
truncated	O
data	B
,	O
but	O
poisson	O
regression	B
offers	O
an	O
easy	O
solution	O
.	O
10	O
an	O
object	O
of	O
the	O
second	O
magnitude	O
is	O
less	O
bright	O
than	O
one	O
of	O
the	O
ﬁrst	O
,	O
and	O
so	O
on	O
,	O
a	O
classiﬁcation	O
system	O
owing	O
to	O
the	O
greeks	O
.	O
122	O
glms	O
and	O
regression	O
trees	B
let	O
r	O
be	O
the	O
270-vector	O
listing	O
the	O
values	O
of	O
r	O
in	O
each	O
bin	O
of	O
the	O
table	O
(	O
in	O
column	O
order	O
)	O
,	O
and	O
likewise	O
m	O
for	O
the	O
270	O
m	O
values—for	O
instance	O
m	O
d	O
.18	O
;	O
17	O
;	O
:	O
:	O
:	O
;	O
1/	O
repeated	O
15	O
times—and	O
deﬁne	O
the	O
270	O
(	O
cid:2	O
)	O
5	O
matrix	B
x	O
as	O
x	O
d	O
œr	O
;	O
m	O
;	O
r	O
2	O
;	O
rm	O
;	O
m2	O
;	O
(	O
8.39	O
)	O
where	O
r	O
2	O
is	O
the	O
vector	B
whose	O
components	O
are	O
the	O
square	O
of	O
r	O
’	O
s	O
,	O
etc	O
.	O
the	O
log	O
density	B
of	O
a	O
bivariate	O
normal	B
distribution	O
in	O
.r	O
;	O
m/	O
is	O
of	O
the	O
form	B
˛1r	O
c	O
˛2m	O
c	O
˛3r	O
2	O
c	O
˛4rm	O
c	O
˛5m2	O
,	O
agreeing	O
with	O
log	O
(	O
cid:22	O
)	O
i	O
d	O
x	O
0	O
i	O
˛	O
as	O
speciﬁed	O
by	O
(	O
8.39	O
)	O
.	O
we	O
can	O
use	O
a	O
poisson	O
glm	O
,	O
with	O
yi	O
the	O
ith	O
bin	O
’	O
s	O
count	O
,	O
to	O
es-	O
timate	O
the	O
portion	O
of	O
our	O
hypothesized	O
bivariate	O
normal	B
distribution	O
in	O
the	O
truncation	O
region	B
(	O
8.38	O
)	O
.	O
figure	O
8.4	O
left	O
galaxy	B
data	O
;	O
binned	O
counts	O
.	O
right	O
poisson	O
glm	O
density	B
estimate	O
.	O
the	O
left	O
panel	O
of	O
figure	O
8.4	O
is	O
a	O
perspective	O
picture	O
of	O
the	O
raw	O
counts	O
in	O
table	O
8.5.	O
on	O
the	O
right	O
is	O
the	O
ﬁtted	O
density	B
from	O
the	O
poisson	O
regression	B
.	O
irrespective	O
of	O
density	B
estimation	O
,	O
poisson	O
regression	B
has	O
done	O
a	O
useful	O
job	O
of	O
smoothing	B
the	O
raw	O
bin	O
counts	O
.	O
contours	O
of	O
equal	O
value	O
of	O
the	O
ﬁtted	O
log	O
density	B
o˛0	O
c	O
o˛1r	O
c	O
o˛2m	O
c	O
o˛3r	O
2	O
c	O
o˛4rm	O
c	O
o˛5m2	O
(	O
8.40	O
)	O
are	O
shown	O
in	O
figure	O
8.5.	O
one	O
can	O
imagine	O
the	O
contours	O
as	O
truncated	O
por-	O
tions	O
of	O
ellipsoids	O
,	O
of	O
the	O
type	O
shown	O
in	O
figure	O
5.3.	O
the	O
right	O
panel	O
of	O
figure	O
8.4	O
makes	O
it	O
clear	O
that	O
we	O
are	O
nowhere	O
near	O
the	O
center	O
of	O
the	O
hypo-	O
thetical	O
bivariate	O
normal	B
density	O
,	O
which	O
must	O
lie	O
well	O
beyond	O
our	O
dimness	O
limit	O
.	O
dimmerfarthercountdimmerfartherdensity	O
8.3	O
poisson	O
regression	B
123	O
figure	O
8.5	O
contour	O
curves	O
for	O
poisson	O
glm	O
density	B
estimate	O
for	O
the	O
galaxy	B
data	O
.	O
the	O
red	O
dot	O
shows	O
the	O
point	O
of	O
maximum	O
density	O
.	O
the	O
poisson	O
deviance	O
residual	O
z	O
between	O
an	O
observed	O
count	O
y	O
and	O
a	O
ﬁtted	O
value	O
o	O
(	O
cid:22	O
)	O
is	O
z	O
d	O
sign.y	O
(	O
cid:0	O
)	O
o	O
(	O
cid:22	O
)	O
/d.y	O
;	O
o	O
(	O
cid:22	O
)	O
/1=2	O
;	O
(	O
8.41	O
)	O
tist	O
glm	O
theory	B
says	O
that	O
s	O
dp	O
with	O
d	O
the	O
poisson	O
deviance	O
from	O
table	O
8.4.	O
zj	O
k	O
,	O
the	O
deviance	O
residual	O
between	O
the	O
count	O
yij	O
in	O
the	O
ij	O
th	O
bin	O
of	O
table	O
8.5	O
and	O
the	O
ﬁtted	O
value	O
o	O
(	O
cid:22	O
)	O
j	O
k	O
from	O
the	O
poisson	O
glm	O
,	O
was	O
calculated	O
for	O
all	O
270	O
bins	O
.	O
standard	O
frequen-	O
j	O
k	O
should	O
be	O
about	O
270	O
if	O
the	O
bivari-	O
ate	O
normal	B
model	O
(	O
8.39	O
)	O
is	O
correct.11	O
actually	O
the	O
ﬁt	O
was	O
poor	O
:	O
s	O
d	O
610.	O
in	O
practice	O
we	O
might	O
try	O
adding	O
columns	O
to	O
x	O
in	O
(	O
8.39	O
)	O
,	O
e.g.	O
,	O
rm2	O
or	O
r	O
2m2	O
,	O
improving	O
the	O
ﬁt	O
where	O
it	O
was	O
worst	O
,	O
near	O
the	O
boundaries	O
of	O
the	O
ta-	O
ble	O
.	O
chapter	O
12	O
demonstrates	O
some	O
other	O
examples	O
of	O
poisson	O
density	B
esti-	O
mation	O
.	O
in	O
general	O
,	O
poisson	O
glms	O
reduce	O
density	B
estimation	O
to	O
regression	B
model	I
ﬁtting	O
,	O
a	O
familiar	O
and	O
ﬂexible	O
inferential	O
technology	O
.	O
j	O
k	O
z2	O
11	O
this	O
is	O
a	O
modern	O
version	O
of	O
the	O
classic	O
chi-squared	O
goodness-of-ﬁt	O
test	O
.	O
−1.5−1.0−0.50.0−21−20−19−18−17fartherdimmer	O
0.5	O
1	O
1.5	O
2	O
2.5	O
3	O
3.5	O
4	O
4.5	O
5	O
5.5	O
6	O
6.5	O
7	O
7.5	O
8	O
8.5	O
9	O
l	O
124	O
glms	O
and	O
regression	O
trees	B
8.4	O
regression	B
trees	O
the	O
data	B
set	O
d	O
for	O
a	O
regression	B
problem	O
typically	O
consists	O
of	O
n	O
pairs	O
.xi	O
;	O
yi	O
/	O
,	O
d	O
d	O
f.xi	O
;	O
yi	O
/	O
;	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
ng	O
;	O
(	O
8.42	O
)	O
where	O
xi	O
is	O
a	O
vector	B
of	O
predictors	O
,	O
or	O
“	O
covariates	O
,	O
”	O
taking	O
its	O
value	O
in	O
some	O
space	B
x	O
,	O
and	O
yi	O
is	O
the	O
response	O
,	O
assumed	O
to	O
be	O
univariate	O
in	O
what	O
follows	O
.	O
the	O
regression	B
algorithm	O
,	O
perhaps	O
a	O
poisson	O
glm	O
,	O
inputs	O
d	O
and	O
outputs	O
a	O
rule	B
rd	O
.x/	O
:	O
for	O
any	O
value	O
of	O
x	O
in	O
x	O
,	O
rd	O
.x/	O
produces	O
an	O
estimate	B
oy	O
for	O
a	O
possible	O
future	O
value	O
of	O
y	O
,	O
oy	O
d	O
rd	O
.x/	O
:	O
(	O
8.43	O
)	O
in	O
the	O
logistic	B
regression	I
example	O
(	O
8.8	O
)	O
,	O
rd	O
.x/	O
is	O
o	O
(	O
cid:25	O
)	O
.x/	O
.	O
there	O
are	O
three	O
principal	O
uses	O
for	O
the	O
rule	B
rd	O
.x/	O
.	O
1	O
for	O
prediction	O
:	O
given	O
a	O
new	O
observation	O
of	O
x	O
,	O
but	O
not	O
of	O
its	O
correspond-	O
ing	O
y	O
,	O
we	O
use	O
oy	O
d	O
rd	O
.x/	O
to	O
predict	O
y.	O
in	O
the	O
spam	B
example	O
,	O
the	O
57	O
keywords	O
of	O
an	O
incoming	O
message	O
could	O
be	O
used	O
to	O
predict	O
whether	O
or	O
not	O
it	O
is	O
spam.12	O
(	O
see	O
chapter	O
12	O
.	O
)	O
2	O
for	O
estimation	B
:	O
the	O
rule	B
rd	O
.x/	O
describes	O
a	O
“	O
regression	B
surface	O
”	O
os	O
over	O
x	O
,	O
(	O
8.44	O
)	O
the	O
right	O
panel	O
of	O
figure	O
8.4	O
shows	O
os	O
for	O
the	O
galaxy	B
example	O
.	O
os	O
can	O
be	O
thought	O
of	O
as	O
estimating	O
s	O
,	O
the	O
true	O
regression	B
surface	O
,	O
often	O
deﬁned	O
in	O
the	O
form	B
of	O
conditional	O
expectation	O
,	O
os	O
d	O
frd	O
.x/	O
;	O
x	O
2	O
g	O
:	O
x	O
s	O
d	O
fefyjxg	O
;	O
x	O
2	O
g	O
:	O
x	O
x	O
g.	O
)	O
(	O
8.45	O
)	O
(	O
in	O
a	O
dichotomous	O
situation	O
where	O
y	O
is	O
coded	O
as	O
0	O
or	O
1	O
,	O
s	O
d	O
fprfy	O
d	O
1jxg	O
;	O
x	O
2	O
for	O
estimation	B
,	O
but	O
not	O
necessarily	O
for	O
prediction	O
,	O
we	O
want	O
os	O
to	O
accu-	O
rately	O
portray	O
s.	O
the	O
right	O
panel	O
of	O
figure	O
8.4	O
shows	O
the	O
estimated	O
galaxy	B
density	O
still	O
increasing	O
monotonically	O
in	O
dimmer	O
at	O
the	O
top	O
end	O
of	O
the	O
truncation	O
region	B
,	O
but	O
not	O
so	O
in	O
farther	O
,	O
perhaps	O
an	O
important	O
clue	O
for	O
directing	O
future	O
search	O
counts.13	O
the	O
ﬂat	O
region	B
in	O
the	O
kidney	B
function	I
re-	O
gression	O
curve	O
of	O
figure	O
1.2	O
makes	O
almost	O
no	O
difference	O
to	O
prediction	O
,	O
but	O
is	O
of	O
scientiﬁc	O
interest	O
if	O
accurate	O
.	O
12	O
prediction	O
of	O
dichotomous	O
outcomes	O
is	O
often	O
called	O
“	O
classiﬁcation.	O
”	O
13	O
physicists	O
call	O
a	O
regression-based	O
search	O
for	O
new	O
objects	O
“	O
bump	O
hunting.	O
”	O
8.4	O
regression	B
trees	O
125	O
3	O
for	O
explanation	O
:	O
the	O
10	O
predictors	O
for	O
the	O
diabetes	B
data	O
of	O
section	O
7.3	O
,	O
age	O
,	O
sex	O
,	O
bmi	O
,	O
.	O
.	O
.	O
,	O
were	O
selected	O
by	O
the	O
researcher	O
in	O
the	O
hope	O
of	O
ex-	O
plaining	O
the	O
etiology	O
of	O
diabetes	B
progression	O
.	O
the	O
relative	O
contribution	O
of	O
the	O
different	O
predictors	O
to	O
rd	O
.x/	O
is	O
then	O
of	O
interest	O
.	O
how	O
the	O
regression	B
surface	O
is	O
composed	O
is	O
of	O
prime	O
concern	O
in	O
this	O
use	O
,	O
but	O
not	O
in	O
use	O
1	O
or	O
2	O
above	O
.	O
the	O
three	O
different	O
uses	O
of	O
rd	O
.x/	O
raise	O
different	O
inferential	O
questions	O
.	O
use	O
1	O
calls	O
for	O
estimates	O
of	O
prediction	O
error	O
.	O
in	O
a	O
dichotomous	O
situation	O
such	O
as	O
the	O
spam	B
study	O
,	O
we	O
would	O
want	O
to	O
know	O
both	O
error	O
probabilities	B
prf	O
oy	O
d	O
spamjy	O
d	O
hamg	O
and	O
prf	O
oy	O
d	O
hamjy	O
d	O
spamg	O
:	O
(	O
8.46	O
)	O
for	O
estimation	B
,	O
the	O
accuracy	O
of	O
rd	O
.x/	O
as	O
a	O
function	B
of	O
x	O
,	O
perhaps	O
in	O
stan-	O
dard	O
deviation	O
terms	O
,	O
sd.x/	O
d	O
sd	O
.	O
oyjx/	O
;	O
(	O
8.47	O
)	O
would	O
tell	O
how	O
closely	O
os	O
approximates	O
s.	O
use	O
3	O
,	O
explanation	O
,	O
requires	O
more	O
elaborate	O
inferential	O
tools	O
,	O
saying	O
for	O
example	O
which	O
of	O
the	O
regression	B
coefﬁcients	O
˛i	O
in	O
(	O
8.19	O
)	O
can	O
safely	O
be	O
set	B
to	O
zero	O
.	O
figure	O
8.6	O
left	O
a	O
hypothetical	O
regression	B
tree	O
based	O
on	O
two	O
predictors	O
x1	O
and	O
x2	O
.	O
right	O
corresponding	O
regression	B
surface	O
.	O
regression	B
trees	O
use	O
a	O
simple	O
but	O
intuitively	O
appealing	O
technique	O
to	O
form	B
a	O
regression	B
surface	O
:	O
recursive	O
partitioning	O
.	O
the	O
left	O
panel	O
of	O
figure	O
8.6	O
illustrates	O
the	O
method	B
for	O
a	O
hypothetical	O
situation	O
involving	O
two	O
predictor	B
variables	O
,	O
x1	O
and	O
x2	O
(	O
e.g.	O
,	O
r	O
and	O
m	O
in	O
the	O
galaxy	B
example	O
)	O
.	O
at	O
the	O
top	O
of	O
|t1t2t3t4r1r1r2r2r3r3r4r4r5r5x1x1x1x2x2x2x1≤t1x2≤t2x1≤t3x2≤t4	O
126	O
glms	O
and	O
regression	O
trees	B
the	O
tree	O
,	O
the	O
sample	B
population	O
of	O
n	O
cases	O
has	O
been	O
split	O
into	O
two	O
groups	O
:	O
those	O
with	O
x1	O
equal	O
to	O
or	O
less	O
than	O
value	O
t1	O
go	O
to	O
the	O
left	O
,	O
those	O
with	O
x1	O
>	O
t1	O
to	O
the	O
right	O
.	O
the	O
leftward	O
group	O
is	O
itself	O
then	O
divided	O
into	O
two	O
groups	O
depending	O
on	O
whether	O
or	O
not	O
x2	O
	O
t2	O
.	O
the	O
division	O
stops	O
there	O
,	O
leaving	O
two	O
terminal	O
nodes	O
r1	O
and	O
r2	O
.	O
on	O
the	O
tree	O
’	O
s	O
right	O
side	O
,	O
two	O
other	O
splits	O
give	O
terminal	O
nodes	O
r3	O
,	O
r4	O
,	O
and	O
r5	O
.	O
a	O
prediction	O
value	O
oyrj	O
is	O
attached	O
to	O
each	O
terminal	B
node	I
rj	O
.	O
the	O
predic-	O
tion	O
oy	O
applying	O
to	O
a	O
new	O
observation	O
x	O
d	O
.x1	O
;	O
x2/	O
is	O
calculated	O
by	O
starting	O
x	O
at	O
the	O
top	O
of	O
the	O
tree	O
and	O
following	O
the	O
splits	O
downward	O
until	O
a	O
terminal	B
node	I
,	O
and	O
its	O
attached	O
prediction	O
oyrj	O
,	O
is	O
reached	O
.	O
the	O
corresponding	O
re-	O
gression	O
surface	O
os	O
is	O
shown	O
in	O
the	O
right	O
panel	O
of	O
figure	O
8.6	O
(	O
here	O
the	O
oyrj	O
happen	O
to	O
be	O
in	O
ascending	O
order	O
)	O
.	O
various	O
algorithmic	O
rules	O
are	O
used	O
to	O
decide	O
which	O
variable	O
to	O
split	O
and	O
which	O
splitting	O
value	O
t	B
to	O
take	O
at	O
each	O
step	O
of	O
the	O
tree	O
’	O
s	O
construction	O
.	O
here	O
is	O
the	O
most	O
common	O
method	B
:	O
suppose	O
at	O
step	O
k	O
of	O
the	O
algorithm	B
,	O
groupk	O
of	O
nk	O
cases	O
remains	O
to	O
be	O
split	O
,	O
those	O
cases	O
having	O
mean	O
and	O
sum	O
of	O
squares	B
yi	O
=nk	O
and	O
s2	O
k	O
.yi	O
(	O
cid:0	O
)	O
mk/2	O
:	O
(	O
8.48	O
)	O
mk	O
d	O
x	O
i2groupk	O
d	O
x	O
i2groupk	O
dividing	O
groupk	O
into	O
groupk	O
;	O
left	O
and	O
groupk	O
;	O
right	O
produces	O
means	O
mk	O
;	O
left	O
and	O
mk	O
;	O
right	O
,	O
and	O
corresponding	O
sums	O
of	O
squares	B
s2	O
k	O
;	O
right	O
.	O
the	O
algorithm	B
proceeds	O
by	O
choosing	O
the	O
splitting	O
variable	O
xk	O
and	O
the	O
threshold	O
tk	O
to	O
min-	O
imize	O
k	O
;	O
left	O
and	O
s2	O
c	O
s2	O
k	O
;	O
right	O
:	O
s2	O
k	O
;	O
left	O
(	O
8.49	O
)	O
7	O
in	O
other	O
words	O
,	O
it	O
splits	O
groupk	O
into	O
two	O
groups	O
that	O
are	O
as	O
different	O
from	O
each	O
other	O
as	O
possible.	O
cross-validation	O
estimates	O
of	O
prediction	O
error	O
,	O
chapter	O
12	O
,	O
are	O
used	O
to	O
decide	O
when	O
the	O
splitting	O
process	O
should	O
stop	O
.	O
if	O
groupk	O
is	O
not	O
to	O
be	O
further	O
divided	O
,	O
it	O
becomes	O
terminal	B
node	I
rk	O
,	O
with	O
prediction	O
value	O
oyrk	O
d	O
mk	O
.	O
none	O
of	O
this	O
would	O
be	O
feasible	O
without	O
electronic	O
computation	O
,	O
but	O
even	O
quite	O
large	O
prediction	O
problems	O
can	O
be	O
short	O
work	O
for	O
modern	O
computers	O
.	O
figure	O
8.7	O
shows	O
a	O
regression	B
tree	O
analysis14	O
of	O
the	O
spam	B
data	O
,	O
ta-	O
ble	O
8.3.	O
there	O
are	O
seven	O
terminal	O
nodes	O
,	O
labeled	O
0	O
or	O
1	O
for	O
decision	O
ham	O
or	O
spam	B
.	O
the	O
leftmost	O
node	O
,	O
say	O
r1	O
,	O
is	O
a	O
0	O
,	O
and	O
contains	O
2462	O
ham	O
cases	O
and	O
275	O
spam	B
(	O
compared	O
with	O
2788	O
and	O
1813	O
in	O
the	O
full	B
data	O
set	B
)	O
.	O
starting	O
at	O
the	O
top	O
of	O
the	O
tree	O
,	O
r1	O
is	O
reached	O
if	O
it	O
has	O
a	O
low	O
proportion	B
of	O
$	O
symbols	O
14	O
using	O
the	O
r	O
program	O
rpart	O
,	O
in	O
classiﬁcation	O
mode	O
,	O
employing	O
a	O
different	O
splitting	O
rule	B
than	O
the	O
version	O
based	O
on	O
(	O
8.49	O
)	O
.	O
8.4	O
regression	B
trees	O
127	O
figure	O
8.7	O
regression	B
tree	O
on	O
the	O
spam	B
data	O
;	O
0	O
d	O
ham	O
,	O
1	O
d	O
spam	B
.	O
error	O
rates	O
:	O
ham	O
5.2	O
%	O
,	O
spam	B
17.4	O
%	O
.	O
captions	O
indicate	O
leftward	O
(	O
ham	O
)	O
moves	O
.	O
char	O
$	O
,	O
a	O
low	O
proportion	B
of	O
the	O
word	O
remove	O
,	O
and	O
a	O
low	O
proportion	B
of	O
exclamation	O
marks	O
char	O
!	O
.	O
regression	B
trees	O
are	O
easy	O
to	O
interpret	O
(	O
“	O
too	O
many	O
dollar	O
signs	O
means	O
spam	B
!	O
”	O
)	O
seemingly	O
suiting	O
them	O
for	O
use	O
3	O
,	O
explanation	O
.	O
unfortunately	O
,	O
they	O
are	O
also	O
easy	O
to	O
overinterpret	O
,	O
with	O
a	O
reputation	O
for	O
being	O
unstable	O
in	O
prac-	O
tice	O
.	O
discontinuous	O
regression	B
surfaces	O
os	O
,	O
as	O
in	O
figure	O
8.6	O
,	O
disqualify	O
them	O
for	O
use	O
2	O
,	O
estimation	B
.	O
their	O
principal	O
use	O
in	O
what	O
follows	O
will	O
be	O
as	O
key	O
parts	O
of	O
prediction	O
algorithms	O
,	O
use	O
1.	O
the	O
tree	O
in	O
figure	O
8.6	O
has	O
apparent	O
error	O
rates	O
(	O
8.46	O
)	O
of	O
5.2	O
%	O
and	O
17.4	O
%	O
.	O
this	O
can	O
be	O
much	O
improved	O
upon	O
by	O
“	O
bagging	O
”	O
(	O
bootstrap	O
aggregation	O
)	O
,	O
chapters	O
17	O
and	O
20	O
,	O
and	O
by	O
other	O
computer-intensive	O
techniques	O
.	O
compared	O
with	O
generalized	O
linear	B
models	O
,	O
regression	B
trees	O
represent	O
a	O
break	O
from	O
classical	O
methodology	O
that	O
is	O
more	O
stark	O
.	O
first	O
of	O
all	O
,	O
they	O
are	O
totally	O
nonparametric	B
;	O
bigger	O
but	O
less	O
structured	O
data	B
sets	O
have	O
promoted	O
nonparametrics	O
in	O
twenty-ﬁrst-century	O
statistics	B
.	O
regression	B
trees	O
are	O
more	O
computer-intensive	O
and	O
less	O
efﬁcient	O
than	O
glms	O
but	O
,	O
as	O
will	O
be	O
seen	O
in	O
part	O
iii	O
,	O
the	O
availability	O
of	O
massive	O
data	B
sets	O
and	O
modern	O
computational	O
equip-	O
|char	O
$	O
<	O
−0.0826remove	O
<	O
−0.1513char	O
!	O
<	O
0.1335capruntot	O
<	O
−0.3757free	O
<	O
0.7219hp	O
>	O
=−0.0894502462/2750129/3211/20133/189130/300063/7170/990figure	O
8.7	O
.	O
regression	B
tree	O
,	O
spam	B
data	O
:	O
0=nonspam	O
,	O
1=spam	O
,	O
error	O
rates	O
:	O
nonspam	O
5.2	O
%	O
,	O
spam	B
17.4	O
%	O
captions	O
indicate	O
leftward	O
(	O
nonspam	O
)	O
moves	O
128	O
glms	O
and	O
regression	O
trees	B
ment	O
has	O
diminished	O
the	O
appeal	O
of	O
efﬁciency	O
in	O
favor	O
of	O
easy	O
assumption-	O
free	O
application	O
.	O
8.5	O
notes	O
and	O
details	O
computer-age	O
algorithms	O
depend	O
for	O
their	O
utility	O
on	O
statistical	O
computing	O
languages	O
.	O
after	O
a	O
period	O
of	O
evolution	O
,	O
the	O
language	O
s	O
(	O
becker	O
et	O
al.	O
,	O
1988	O
)	O
and	O
its	O
open-source	O
successor	O
r	O
(	O
r	O
core	O
team	O
,	O
2015	O
)	O
,	O
have	O
come	O
to	O
dom-	O
inate	O
applied	O
practice.15	O
generalized	O
linear	B
models	O
are	O
available	O
from	O
a	O
single	O
r	O
command	O
,	O
e.g.	O
,	O
glm	O
(	O
y	O
(	O
cid:24	O
)	O
x	O
,	O
family=binomial	O
)	O
for	O
logistic	B
regression	I
(	O
chambers	O
and	O
hastie	O
,	O
1993	O
)	O
,	O
and	O
similarly	O
for	O
re-	O
gression	O
trees	B
and	O
hundreds	O
of	O
other	O
applications	O
.	O
the	O
classic	O
version	O
of	O
bioassay	O
,	O
probit	O
analysis	B
,	O
assumes	O
that	O
each	O
test	O
animal	O
has	O
its	O
own	O
lethal	O
dose	O
level	O
x	O
,	O
and	O
that	O
the	O
population	O
distribution	B
of	O
x	O
is	O
normal	B
,	O
prfx	O
	O
xg	O
d	O
ˆ.˛0	O
c	O
˛1x/	O
(	O
8.50	O
)	O
for	O
unknown	O
parameters	O
.˛0	O
;	O
˛1/	O
and	O
standard	O
normal	B
cdf	O
ˆ.	O
then	O
the	O
number	O
of	O
animals	O
dying	O
at	O
dose	O
x	O
is	O
binomial	B
bi.nx	O
;	O
(	O
cid:25	O
)	O
x/	O
as	O
in	O
(	O
8.3	O
)	O
,	O
with	O
(	O
cid:25	O
)	O
x	O
d	O
ˆ.˛0	O
c	O
˛1x/	O
,	O
or	O
(	O
cid:0	O
)	O
1	O
.	O
(	O
cid:25	O
)	O
x/	O
d	O
˛0	O
c	O
˛1x	O
:	O
ˆ	O
(	O
8.51	O
)	O
replacing	O
the	O
standard	O
normal	O
cdf	B
ˆ.z/	O
with	O
the	O
logistic	O
cdf	O
1=.1	O
c	O
e	O
(	O
cid:0	O
)	O
z/	O
(	O
which	O
resembles	O
ˆ	O
)	O
,	O
changes	O
(	O
8.51	O
)	O
into	O
logistic	B
regression	I
(	O
8.5	O
)	O
.	O
the	O
usual	O
goal	O
of	O
bioassay	O
was	O
to	O
estimate	B
“	O
ld50	O
,	O
”	O
the	O
dose	O
lethal	O
to	O
50	O
%	O
of	O
the	O
test	O
population	O
;	O
it	O
is	O
indicated	O
by	O
the	O
open	O
circle	O
in	O
figure	O
8.2.	O
cox	O
(	O
1970	O
)	O
,	O
the	O
classic	O
text	O
on	O
logistic	B
regression	I
,	O
lists	O
berkson	O
(	O
1944	O
)	O
as	O
an	O
early	O
practitioner	O
.	O
wedderburn	O
(	O
1974	O
)	O
is	O
credited	O
with	O
generalized	O
linear	B
models	O
in	O
mccullagh	O
and	O
nelder	O
’	O
s	O
inﬂuential	O
text	O
of	O
that	O
name	O
,	O
ﬁrst	O
edition	O
1983	O
;	O
birch	O
(	O
1964	O
)	O
developed	O
an	O
important	O
and	O
suggestive	O
special	O
case	O
of	O
glm	O
theory	B
.	O
the	O
twenty-ﬁrst	O
century	O
has	O
seen	O
an	O
efﬂorescence	O
of	O
computer-based	O
re-	O
gression	O
techniques	O
,	O
as	O
described	O
extensively	O
in	O
hastie	O
et	O
al	O
.	O
(	O
2009	O
)	O
.	O
the	O
discussion	O
of	O
regression	B
trees	O
here	O
is	O
taken	O
from	O
their	O
section	O
9.2	O
,	O
including	O
our	O
figure	O
8.6.	O
they	O
use	O
the	O
spam	B
data	O
as	O
a	O
central	O
example	O
;	O
it	O
is	O
publicly	O
15	O
previous	O
computer	O
packages	B
such	O
as	O
sas	O
and	O
spss	O
continue	O
to	O
play	O
a	O
major	O
role	O
in	O
application	O
areas	O
such	O
as	O
the	O
social	O
sciences	O
,	O
biomedical	O
statistics	B
,	O
and	O
the	O
pharmaceutical	O
industry	O
.	O
8.5	O
notes	O
and	O
details	O
129	O
available	O
at	O
ftp.ics.uci.edu	O
.	O
breiman	O
et	O
al	O
.	O
(	O
1984	O
)	O
propelled	O
regres-	O
sion	O
trees	B
into	O
wide	O
use	O
with	O
their	O
cart	O
algorithm	B
.	O
1	O
[	O
p.	O
112	O
]	O
sufﬁciency	O
as	O
in	O
(	O
8.13	O
)	O
.	O
the	O
fisher–neyman	O
criterion	O
says	O
that	O
if	O
f˛.x/	O
d	O
h˛.s.x//g.x/	O
,	O
when	O
g.	O
(	O
cid:1	O
)	O
/	O
does	O
not	O
depend	O
on	O
˛	O
,	O
then	O
s.x/	O
is	O
sufﬁcient	O
for	O
˛	O
.	O
2	O
[	O
p.	O
118	O
]	O
equation	B
(	O
8.28	O
)	O
.	O
from	O
(	O
8.24	O
)	O
–	O
(	O
8.25	O
)	O
we	O
have	O
the	O
log	O
likelihood	B
function	O
with	O
sufﬁcient	O
statistic	B
z	O
d	O
x	O
ing	O
with	O
respect	O
to	O
˛	O
,	O
0	O
l˛.y/	O
d	O
˛	O
z	O
(	O
cid:0	O
)	O
.˛/	O
y	O
and	O
.˛/	O
dpn	O
0	O
(	O
8.52	O
)	O
id1	O
(	O
cid:13	O
)	O
.x	O
0	O
i	O
˛/	O
.	O
differentiat-	O
0	O
y	O
(	O
cid:0	O
)	O
x	O
p	O
l˛.y/	O
d	O
z	O
(	O
cid:0	O
)	O
p	O
.˛/	O
d	O
x	O
0	O
where	O
we	O
have	O
used	O
d	O
(	O
cid:13	O
)	O
=d	O
(	O
cid:21	O
)	O
d	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
5.55	O
)	O
,	O
so	O
p	O
(	O
cid:13	O
)	O
.x	O
(	O
8.53	O
)	O
says	O
p	O
matrix	B
r	O
l˛.y/	O
with	O
respect	O
to	O
˛	O
is	O
l˛.y/	O
d	O
x	O
0	O
.y	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
.˛//	O
,	O
verifying	O
the	O
mle	O
equation	B
(	O
8.28	O
)	O
.	O
i	O
˛/	O
d	O
x	O
0	O
0	O
i	O
(	O
cid:22	O
)	O
i	O
.˛/	O
.	O
but	O
3	O
[	O
p.	O
118	O
]	O
concavity	B
of	O
the	O
log	O
likelihood	B
.	O
from	O
(	O
8.53	O
)	O
,	O
the	O
second	O
derivative	O
(	O
cid:22	O
)	O
.˛/	O
;	O
(	O
8.53	O
)	O
(	O
8.54	O
)	O
(	O
5.57	O
)	O
–	O
(	O
5.59	O
)	O
.	O
but	O
z	O
d	O
x	O
(	O
cid:0	O
)	O
r	O
.˛/	O
d	O
(	O
cid:0	O
)	O
cov˛.z/	O
;	O
0	O
y	O
has	O
cov˛.z/	O
d	O
x	O
0	O
(	O
8.55	O
)	O
a	O
positive	O
deﬁnite	O
p	O
(	O
cid:2	O
)	O
p	O
matrix	B
,	O
verifying	O
the	O
concavity	B
of	O
l˛.y/	O
(	O
which	O
in	O
fact	O
applies	O
to	O
any	O
exponential	O
family	O
,	O
not	O
only	O
glms	O
)	O
.	O
†.˛/x	O
;	O
4	O
[	O
p.	O
118	O
]	O
formula	B
(	O
8.30	O
)	O
.	O
the	O
sufﬁcient	O
statistic	B
z	O
has	O
mean	O
vector	B
and	O
co-	O
variance	O
matrix	B
z	O
(	O
cid:24	O
)	O
.ˇ	O
;	O
v˛/	O
;	O
(	O
8.56	O
)	O
†.˛/x	O
(	O
8.55	O
)	O
.	O
using	O
(	O
5.60	O
)	O
,	O
the	O
with	O
ˇ	O
d	O
e˛fzg	O
(	O
5.58	O
)	O
and	O
v˛	O
d	O
x	O
ﬁrst-order	O
taylor	O
series	O
for	O
o˛	O
as	O
a	O
function	B
of	O
z	O
is	O
˛	O
.z	O
(	O
cid:0	O
)	O
ˇ/	O
:	O
(	O
cid:0	O
)	O
1	O
:	O
d	O
˛	O
c	O
v	O
o˛	O
0	O
(	O
8.57	O
)	O
(	O
cid:0	O
)	O
2	O
rather	O
taken	O
literally	O
,	O
(	O
8.57	O
)	O
gives	O
(	O
8.30	O
)	O
.	O
in	O
the	O
ols	O
formula	B
,	O
we	O
have	O
(	O
cid:27	O
)	O
than	O
(	O
cid:27	O
)	O
2	O
since	O
the	O
natural	O
parameter	O
˛	O
for	O
the	O
normal	B
entry	O
in	O
table	O
8.4	O
is	O
(	O
cid:22	O
)	O
=	O
(	O
cid:27	O
)	O
2	O
.	O
5	O
[	O
p.	O
118	O
]	O
formula	B
(	O
8.33	O
)	O
.	O
this	O
formula	B
,	O
attributed	O
to	O
hoeffding	O
(	O
1965	O
)	O
,	O
is	O
a	O
key	O
result	O
in	O
the	O
interpretation	O
of	O
glm	O
ﬁtting	B
.	O
applying	O
deﬁnition	O
(	O
8.31	O
)	O
130	O
glms	O
and	O
regression	O
trees	B
to	O
family	O
(	O
8.32	O
)	O
gives	O
1	O
2	O
d.	O
(	O
cid:21	O
)	O
1	O
;	O
(	O
cid:21	O
)	O
2/	O
d	O
e	O
(	O
cid:21	O
)	O
1	O
f.	O
(	O
cid:21	O
)	O
1	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
2/y	O
(	O
cid:0	O
)	O
œ	O
(	O
cid:13	O
)	O
.	O
(	O
cid:21	O
)	O
1/	O
(	O
cid:0	O
)	O
(	O
cid:13	O
)	O
.	O
(	O
cid:21	O
)	O
2/g	O
d	O
.	O
(	O
cid:21	O
)	O
1	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
2/	O
(	O
cid:22	O
)	O
1	O
(	O
cid:0	O
)	O
œ	O
(	O
cid:13	O
)	O
.	O
(	O
cid:21	O
)	O
1/	O
(	O
cid:0	O
)	O
(	O
cid:13	O
)	O
.	O
(	O
cid:21	O
)	O
2/	O
:	O
(	O
8.58	O
)	O
if	O
(	O
cid:21	O
)	O
1	O
is	O
the	O
mle	O
o	O
0	O
d	O
d	O
œlog	O
f	O
(	O
cid:21	O
)	O
.y/=d	O
(	O
cid:21	O
)	O
d	O
y	O
(	O
cid:0	O
)	O
p	O
(	O
cid:13	O
)	O
.	O
(	O
cid:21	O
)	O
/	O
d	O
y	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
)	O
,	O
giving16	O
(	O
cid:21	O
)	O
then	O
(	O
cid:22	O
)	O
1	O
d	O
y	O
(	O
from	O
the	O
maximum	B
likelihood	I
equation	O
(	O
cid:16	O
)	O
o	O
	O
d	O
(	O
cid:16	O
)	O
o	O
(	O
cid:21	O
)	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
	O
y	O
(	O
cid:0	O
)	O
h	B
(	O
cid:16	O
)	O
o	O
(	O
cid:21	O
)	O
	O
(	O
cid:0	O
)	O
(	O
cid:13	O
)	O
.	O
(	O
cid:21	O
)	O
/	O
i	O
d	O
(	O
cid:21	O
)	O
;	O
(	O
cid:21	O
)	O
(	O
8.59	O
)	O
for	O
any	O
choice	O
of	O
(	O
cid:21	O
)	O
.	O
but	O
the	O
right-hand	O
side	O
of	O
(	O
8.59	O
)	O
is	O
(	O
cid:0	O
)	O
logœf	O
(	O
cid:21	O
)	O
.y/=fy.y/	O
,	O
verifying	O
(	O
8.33	O
)	O
.	O
(	O
cid:13	O
)	O
6	O
[	O
p.	O
120	O
]	O
table	O
8.5.	O
the	O
galaxy	B
counts	O
are	O
from	O
loh	O
and	O
spillar	O
’	O
s	O
1988	O
redshift	O
survey	O
,	O
as	O
discussed	O
in	O
efron	O
and	O
petrosian	O
(	O
1992	O
)	O
.	O
7	O
[	O
p.	O
126	O
]	O
criteria	B
(	O
8.49	O
)	O
.	O
abbreviating	O
“	O
left	O
”	O
and	O
“	O
right	O
”	O
by	O
l	O
and	O
r	O
,	O
we	O
1	O
2	O
have	O
d	O
s2	O
kl	O
c	O
s2	O
kr	O
s2	O
k	O
c	O
nkl	O
nkr	O
nk	O
.mkl	O
(	O
cid:0	O
)	O
mkr	O
/2	O
;	O
(	O
8.60	O
)	O
with	O
nkl	O
and	O
nkr	O
the	O
subgroup	O
sizes	O
,	O
showing	O
that	O
minimizing	O
(	O
8.49	O
)	O
is	O
the	O
same	O
as	O
maximizing	O
the	O
last	O
term	O
in	O
(	O
8.60	O
)	O
.	O
intuitively	O
,	O
a	O
good	O
split	O
is	O
one	O
that	O
makes	O
the	O
left	O
and	O
right	O
groups	O
as	O
different	O
as	O
possible	O
,	O
the	O
ideal	O
being	O
all	O
0s	O
on	O
the	O
left	O
and	O
all	O
1s	O
on	O
the	O
right	O
,	O
making	O
the	O
terminal	O
nodes	O
“	O
pure.	O
”	O
16	O
in	O
some	O
cases	O
o	O
(	O
cid:21	O
)	O
is	O
undeﬁned	O
;	O
for	O
example	O
,	O
when	O
y	O
d	O
0	O
for	O
a	O
poisson	O
response	O
,	O
o	O
(	O
cid:21	O
)	O
d	O
log.y/	O
which	O
is	O
undeﬁned	O
.	O
but	O
,	O
in	O
(	O
8.59	O
)	O
,	O
we	O
assume	O
that	O
o	O
(	O
cid:21	O
)	O
y	O
d	O
0.	O
similarly	O
for	O
binary	O
y	O
and	O
the	O
binomial	B
family	O
.	O
9	O
survival	O
analysis	B
and	O
the	O
em	O
algorithm	B
survival	O
analysis	B
had	O
its	O
roots	O
in	O
governmental	O
and	O
actuarial	O
statistics	B
,	O
spanning	O
centuries	O
of	O
use	O
in	O
assessing	O
life	O
expectancies	O
,	O
insurance	B
rates	O
,	O
and	O
annuities	O
.	O
in	O
the	O
20	O
years	O
between	O
1955	O
and	O
1975	O
,	O
survival	O
analysis	B
was	O
adapted	O
by	O
statisticians	O
for	O
application	O
to	O
biomedical	O
studies	O
.	O
three	O
of	O
the	O
most	O
popular	O
post-war	O
statistical	O
methodologies	O
emerged	O
during	O
this	O
period	O
:	O
the	O
kaplan–meier	O
estimate	B
,	O
the	O
log-rank	O
test,1	O
and	O
cox	O
’	O
s	O
pro-	O
portional	O
hazards	O
model	B
,	O
the	O
succession	O
showing	O
increased	O
computational	O
demands	O
along	O
with	O
increasingly	O
sophisticated	O
inferential	O
justiﬁcation	O
.	O
a	O
connection	O
with	O
one	O
of	O
fisher	O
’	O
s	O
ideas	O
on	O
maximum	B
likelihood	I
estimation	O
leads	O
in	O
the	O
last	O
section	O
of	O
this	O
chapter	O
to	O
another	O
statistical	O
method	B
that	O
has	O
“	O
gone	O
platinum	O
,	O
”	O
the	O
em	O
algorithm	B
.	O
9.1	O
life	O
tables	O
and	O
hazard	O
rates	O
an	O
insurance	B
company	O
’	O
s	O
life	O
table	O
appears	O
in	O
table	O
9.1	O
,	O
showing	O
its	O
number	O
of	O
clients	O
(	O
that	O
is	O
,	O
life	O
insurance	B
policy	O
holders	O
)	O
by	O
age	O
,	O
and	O
the	O
number	O
of	O
deaths	O
during	O
the	O
past	O
year	O
in	O
each	O
age	O
group,2	O
for	O
example	O
ﬁve	O
deaths	O
among	O
the	O
312	O
clients	O
aged	O
59.	O
the	O
column	O
labeled	O
os	O
is	O
of	O
great	O
interest	O
to	O
the	O
company	O
’	O
s	O
actuaries	O
,	O
who	O
have	O
to	O
set	B
rates	O
for	O
new	O
policy	O
holders	O
.	O
it	O
is	O
an	O
estimate	O
of	O
survival	O
probability	O
:	O
probability	O
0.893	O
of	O
a	O
person	O
aged	O
30	O
(	O
the	O
beginning	O
of	O
the	O
table	O
)	O
surviving	O
past	O
age	O
59	O
,	O
etc	O
.	O
os	O
is	O
calculated	O
according	O
to	O
an	O
ancient	O
but	O
ingenious	O
algorithm	B
.	O
let	O
x	O
represent	O
a	O
typical	O
lifetime	O
,	O
so	O
fi	O
d	O
prfx	O
d	O
ig	O
(	O
9.1	O
)	O
1	O
also	O
known	O
as	O
the	O
mantel–haenszel	O
or	O
cochran–mantel–haenszel	O
test	O
.	O
2	O
the	O
insurance	B
company	O
is	O
ﬁctitious	O
but	O
the	O
deaths	O
y	O
are	O
based	O
on	O
the	O
true	O
2010	O
rates	O
for	O
us	O
men	O
,	O
per	O
social	O
security	O
administration	O
data	B
.	O
131	O
survival	O
analysis	B
and	O
the	O
em	O
algorithm	B
h	O
d	O
hazard	O
rate	B
y=n	O
,	O
os	O
d	O
age	O
n	O
age	O
oh	O
oh	O
132	O
table	O
9.1	O
insurance	B
company	O
life	O
table	O
;	O
at	O
each	O
age	O
,	O
n	O
d	O
number	O
of	O
policy	O
holders	O
,	O
y	O
d	O
number	O
of	O
deaths	O
,	O
o	O
survival	O
probability	O
estimate	B
(	O
9.6	O
)	O
.	O
os	O
1.000	O
1.000	O
1.000	O
1.000	O
1.000	O
.986	O
.986	O
.986	O
.986	O
.986	O
.986	O
.986	O
.986	O
.986	O
.986	O
.976	O
.969	O
.964	O
.964	O
.956	O
.948	O
.945	O
.940	O
.933	O
.927	O
.925	O
.916	O
.910	O
.908	O
.893	O
116	O
44	O
95	O
97	O
120	O
71	O
125	O
122	O
82	O
113	O
79	O
90	O
154	O
103	O
144	O
192	O
153	O
179	O
210	O
259	O
225	O
346	O
370	O
568	O
1081	O
1042	O
1094	O
597	O
359	O
312	O
os	O
.889	O
.871	O
.849	O
.830	O
.820	O
.820	O
.798	O
.785	O
.755	O
.745	O
.723	O
.704	O
.696	O
.689	O
.676	O
.648	O
.611	O
.611	O
.578	O
.528	O
.506	O
.481	O
.431	O
.385	O
.358	O
.321	O
.277	O
.224	O
.192	O
.168	O
.000	O
.000	O
.000	O
.000	O
.000	O
.014	O
.000	O
.000	O
.000	O
.000	O
.000	O
.000	O
.000	O
.000	O
.000	O
.010	O
.007	O
.006	O
.000	O
.008	O
.009	O
.003	O
.005	O
.007	O
.007	O
.002	O
.009	O
.007	O
.003	O
.016	O
.004	O
.020	O
.026	O
.022	O
.012	O
.000	O
.027	O
.016	O
.039	O
.013	O
.030	O
.026	O
.011	O
.011	O
.018	O
.041	O
.058	O
.000	O
.053	O
.087	O
.042	O
.048	O
.104	O
.107	O
.071	O
.103	O
.137	O
.191	O
.143	O
.127	O
n	O
231	O
245	O
196	O
180	O
170	O
114	O
185	O
127	O
127	O
158	O
100	O
155	O
92	O
90	O
110	O
122	O
138	O
46	O
75	O
69	O
95	O
124	O
67	O
112	O
113	O
116	O
124	O
110	O
63	O
79	O
y	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
2	O
1	O
1	O
0	O
2	O
2	O
1	O
2	O
4	O
8	O
2	O
10	O
4	O
1	O
5	O
y	O
1	O
5	O
5	O
4	O
2	O
0	O
5	O
2	O
5	O
2	O
3	O
4	O
1	O
1	O
2	O
5	O
8	O
0	O
4	O
6	O
4	O
6	O
7	O
12	O
8	O
12	O
17	O
21	O
9	O
10	O
30	O
31	O
32	O
33	O
34	O
35	O
36	O
37	O
38	O
39	O
40	O
41	O
42	O
43	O
44	O
45	O
46	O
47	O
48	O
49	O
50	O
51	O
52	O
53	O
54	O
55	O
56	O
57	O
58	O
59	O
60	O
61	O
62	O
63	O
64	O
65	O
66	O
67	O
68	O
69	O
70	O
71	O
72	O
73	O
74	O
75	O
76	O
77	O
78	O
79	O
80	O
81	O
82	O
83	O
84	O
85	O
86	O
87	O
88	O
89	O
is	O
the	O
probability	O
of	O
dying	O
at	O
age	O
i	O
,	O
and	O
si	O
dx	O
j	O
(	O
cid:21	O
)	O
i	O
fj	O
d	O
prfx	O
(	O
cid:21	O
)	O
ig	O
(	O
9.2	O
)	O
is	O
the	O
probability	O
of	O
surviving	O
past	O
age	O
i	O
(	O
cid:0	O
)	O
1.	O
the	O
hazard	O
rate	B
at	O
age	O
i	O
is	O
by	O
9.1	O
life	O
tables	O
and	O
hazard	O
rates	O
133	O
deﬁnition	O
sij	O
d	O
jy	O
hi	O
d	O
fi	O
=si	O
d	O
prfx	O
d	O
ijx	O
(	O
cid:21	O
)	O
ig	O
;	O
(	O
9.3	O
)	O
the	O
probability	O
of	O
dying	O
at	O
age	O
i	O
given	O
survival	O
past	O
age	O
i	O
(	O
cid:0	O
)	O
1.	O
a	O
crucial	O
observation	O
is	O
that	O
the	O
probability	O
sij	O
of	O
surviving	O
past	O
age	O
j	O
given	O
survival	O
past	O
age	O
i	O
(	O
cid:0	O
)	O
1	O
is	O
the	O
product	O
of	O
surviving	O
each	O
intermediate	O
year	O
,	O
.1	O
(	O
cid:0	O
)	O
hk/	O
d	O
prfx	O
>	O
jjx	O
(	O
cid:21	O
)	O
igi	O
kdi	O
(	O
9.4	O
)	O
ﬁrst	O
you	O
have	O
to	O
survive	O
year	O
i	O
,	O
probability	O
1	O
(	O
cid:0	O
)	O
hi	O
;	O
then	O
year	O
i	O
c	O
1	O
,	O
proba-	O
bility	O
1	O
(	O
cid:0	O
)	O
hic1	O
,	O
etc.	O
,	O
up	O
to	O
year	O
j	O
,	O
probability	O
1	O
(	O
cid:0	O
)	O
hj	O
.	O
notice	O
that	O
si	O
(	O
9.2	O
)	O
equals	O
s1	O
;	O
i	O
(	O
cid:0	O
)	O
1.	O
os	O
in	O
table	O
9.1	O
is	O
an	O
estimate	O
of	O
sij	O
for	O
i	O
d	O
30.	O
first	O
,	O
each	O
hi	O
was	O
estimated	O
as	O
the	O
binomial	B
proportion	O
of	O
the	O
number	O
of	O
deaths	O
yi	O
among	O
the	O
ni	O
clients	O
,	O
and	O
then	O
we	O
set	B
o	O
hi	O
d	O
yi	O
=ni	O
;	O
(	O
cid:16	O
)	O
1	O
(	O
cid:0	O
)	O
o	O
os30	O
;	O
j	O
d	O
jy	O
hk	O
kd30	O
	O
:	O
(	O
9.5	O
)	O
(	O
9.6	O
)	O
the	O
insurance	B
company	O
doesn	O
’	O
t	B
have	O
to	O
wait	O
50	O
years	O
to	O
learn	O
the	O
proba-	O
bility	O
of	O
a	O
30-year-old	O
living	O
past	O
80	O
(	O
estimated	O
to	O
be	O
0.506	O
in	O
the	O
table	O
)	O
.	O
one	O
year	O
’	O
s	O
data	B
sufﬁces.3	O
hazard	O
rates	O
are	O
more	O
often	O
described	O
in	O
terms	O
of	O
a	O
continuous	O
positive	O
random	O
variable	O
t	B
(	O
often	O
called	O
“	O
time	O
”	O
)	O
,	O
having	O
density	B
function	O
f	O
.t	O
/	O
and	O
“	O
reverse	O
cdf	B
,	O
”	O
or	O
survival	O
function	B
,	O
s.t	O
/	O
dz	O
1	O
f	O
.x/	O
dx	O
d	O
prft	O
(	O
cid:21	O
)	O
tg	O
:	O
the	O
hazard	O
rate	B
t	O
h.t	O
/	O
d	O
f	O
.t	O
/=s.t	O
/	O
satisﬁes	O
:	O
d	O
prft	O
2	O
.t	O
;	O
t	B
c	O
dt	O
/jt	O
(	O
cid:21	O
)	O
tg	O
for	O
dt	O
!	O
0	O
,	O
in	O
analogy	O
with	O
(	O
9.3	O
)	O
.	O
the	O
analog	O
of	O
(	O
9.4	O
)	O
is	O
h.t	O
/dt	O
3	O
of	O
course	O
the	O
estimates	O
can	O
go	O
badly	O
wrong	O
if	O
the	O
hazard	O
rates	O
change	O
over	O
time	O
.	O
(	O
9.7	O
)	O
(	O
9.8	O
)	O
(	O
9.9	O
)	O
1	O
(	O
cid:26	O
)	O
(	O
cid:0	O
)	O
z	O
t	B
0	O
(	O
cid:0	O
)	O
t	B
=c	O
(	O
cid:27	O
)	O
(	O
9.10	O
)	O
(	O
9.11	O
)	O
(	O
9.12	O
)	O
(	O
9.13	O
)	O
134	O
survival	O
analysis	B
and	O
the	O
em	O
algorithm	B
prft	O
(	O
cid:21	O
)	O
t1jt	O
(	O
cid:21	O
)	O
t0g	O
d	O
exp	O
h.x/	O
dx	O
(	O
cid:26	O
)	O
(	O
cid:0	O
)	O
z	O
t1	O
t0	O
so	O
in	O
particular	O
the	O
reverse	O
cdf	B
(	O
9.7	O
)	O
is	O
given	O
by	O
(	O
cid:27	O
)	O
s.t	O
/	O
d	O
exp	O
h.x/	O
dx	O
:	O
a	O
one-sided	O
exponential	O
density	B
f	O
.t	O
/	O
d	O
.1=c/e	O
for	O
t	B
(	O
cid:21	O
)	O
0	O
has	O
s.t	O
/	O
d	O
expf	O
(	O
cid:0	O
)	O
t	B
=cg	O
and	O
constant	O
hazard	O
rate	B
h.t	O
/	O
d	O
1=c	O
:	O
the	O
name	O
“	O
memoryless	O
”	O
is	O
quite	O
appropriate	O
for	O
density	B
(	O
9.12	O
)	O
:	O
having	O
survived	O
to	O
any	O
time	O
t	B
,	O
the	O
probability	O
of	O
surviving	O
dt	O
units	O
more	O
is	O
always	O
the	O
same	O
,	O
about	O
1	O
(	O
cid:0	O
)	O
dt	O
=c	O
,	O
no	O
matter	O
what	O
t	B
is	O
.	O
if	O
human	O
lifetimes	O
were	O
exponential	O
there	O
wouldn	O
’	O
t	B
be	O
old	O
or	O
young	O
people	O
,	O
only	O
lucky	O
or	O
unlucky	O
ones	O
.	O
9.2	O
censored	O
data	B
and	O
the	O
kaplan–meier	O
estimate	B
table	O
9.2	O
reports	O
the	O
survival	O
data	B
from	O
a	O
randomized	O
clinical	O
trial	O
run	O
by	O
ncog	O
(	O
the	O
northern	O
california	O
oncology	O
group	O
)	O
comparing	O
two	O
treat-	O
ments	O
for	O
head	O
and	O
neck	O
cancer	O
:	O
arm	O
a	O
,	O
chemotherapy	O
,	O
versus	O
arm	O
b	O
,	O
chemotherapy	O
plus	O
radiation	O
.	O
the	O
response	O
for	O
each	O
patient	O
is	O
survival	O
time	O
in	O
days	O
.	O
the	O
c	O
sign	O
following	O
some	O
entries	O
indicates	O
censored	O
data	B
,	O
that	O
is	O
,	O
survival	O
times	O
known	O
only	O
to	O
exceed	O
the	O
reported	O
value	O
.	O
these	O
are	O
patients	O
“	O
lost	O
to	O
followup	O
,	O
”	O
mostly	O
because	O
the	O
ncog	O
experiment	O
ended	O
with	O
some	O
of	O
the	O
patients	O
still	O
alive	O
.	O
this	O
is	O
what	O
the	O
experimenters	O
hoped	O
to	O
see	O
of	O
course	O
,	O
but	O
it	O
compli-	O
cates	O
the	O
comparison	O
.	O
notice	O
that	O
there	O
is	O
more	O
censoring	O
in	O
arm	O
b.	O
in	O
the	O
absence	O
of	O
censoring	O
we	O
could	O
run	O
a	O
simple	O
two-sample	B
test	O
,	O
maybe	O
wilcoxon	O
’	O
s	O
test	O
,	O
to	O
see	O
whether	O
the	O
more	O
aggressive	O
treatment	O
of	O
arm	O
b	O
was	O
increasing	O
the	O
survival	O
times	O
.	O
kaplan–meier	O
curves	O
provide	O
a	O
graph-	O
ical	O
comparison	O
that	O
takes	O
proper	B
account	O
of	O
censoring	O
.	O
(	O
the	O
next	O
section	O
describes	O
an	O
appropriate	O
censored	O
data	B
two-sample	O
test	O
.	O
)	O
kaplan–meier	O
curves	O
have	O
become	O
familiar	O
friends	O
to	O
medical	O
researchers	O
,	O
a	O
lingua	O
franca	O
for	O
reporting	O
clinical	O
trial	O
results	O
.	O
life	O
table	O
methods	O
are	O
appropriate	O
for	O
censored	O
data	B
.	O
table	O
9.3	O
puts	O
the	O
arm	O
a	O
results	O
into	O
the	O
same	O
form	B
as	O
the	O
insurance	B
study	O
of	O
table	O
9.1	O
,	O
now	O
9.2	O
censored	O
data	B
and	O
kaplan–meier	O
135	O
table	O
9.2	O
censored	O
survival	O
times	O
in	O
days	O
,	O
from	O
two	O
arms	O
of	O
the	O
ncog	O
study	O
of	O
head/neck	B
cancer	I
.	O
7	O
108	O
149	O
218	O
405	O
1116+	O
37	O
133	O
195	O
528+	O
1331+	O
34	O
112	O
154	O
225	O
417	O
1146	O
84	O
140	O
209	O
547+	O
1557	O
arm	O
a	O
:	O
chemotherapy	O
42	O
129	O
157	O
241	O
420	O
1226+	O
63	O
133	O
160	O
248	O
440	O
1349+	O
64	O
133	O
160	O
273	O
523	O
1412+	O
74+	O
139	O
165	O
277	O
523+	O
1417	O
83	O
140	O
173	O
279+	O
583	O
84	O
140	O
176	O
297	O
594	O
91	O
146	O
185+	O
319+	O
1101	O
arm	O
b	O
:	O
chemotherapycradiation	O
92	O
146	O
249	O
613+	O
1642+	O
94	O
155	O
281	O
633	O
1771+	O
110	O
159	O
319	O
725	O
1776	O
112	O
169+	O
339	O
759+	O
1897+	O
119	O
173	O
432	O
817	O
2023+	O
127	O
179	O
469	O
1092+	O
2146+	O
130	O
194	O
519	O
1245+	O
2297+	O
with	O
the	O
time	O
unit	O
being	O
months	O
.	O
of	O
the	O
51	O
patients	O
enrolled4	O
in	O
arm	O
a	O
,	O
y1	O
d	O
1	O
was	O
observed	O
to	O
die	O
in	O
the	O
ﬁrst	O
month	O
after	O
treatment	O
;	O
this	O
left	O
50	O
at	O
risk	O
,	O
y2	O
d	O
2	O
of	O
whom	O
died	O
in	O
the	O
second	O
month	O
;	O
y3	O
d	O
5	O
of	O
the	O
remaining	O
48	O
died	O
in	O
their	O
third	O
month	O
after	O
treatment	O
,	O
and	O
one	O
was	O
lost	O
to	O
followup	O
,	O
this	O
being	O
noted	O
in	O
the	O
l	O
column	O
of	O
the	O
table	O
,	O
leaving	O
n4	O
d	O
40	O
patients	O
“	O
at	O
risk	O
”	O
at	O
the	O
beginning	O
of	O
month	O
5	O
,	O
etc	O
.	O
os	O
here	O
is	O
calculated	O
as	O
in	O
(	O
9.6	O
)	O
except	O
starting	O
at	O
time	O
1	O
instead	O
of	O
30.	O
there	O
is	O
nothing	O
wrong	O
with	O
this	O
estimate	B
,	O
but	O
binning	O
the	O
ncog	O
survival	O
data	B
by	O
months	O
is	O
arbitrary	O
.	O
why	O
not	O
go	O
down	O
to	O
days	O
,	O
as	O
the	O
data	B
was	O
originally	O
presented	O
in	O
table	O
9.2	O
?	O
a	O
kaplan–meier	O
survival	O
curve	O
is	O
the	O
limit	O
of	O
life	O
table	O
survival	O
estimates	O
as	O
the	O
time	O
unit	O
goes	O
to	O
zero	O
.	O
observations	O
zi	O
for	O
censored	O
data	B
problems	O
are	O
of	O
the	O
form	B
zi	O
d	O
.ti	O
;	O
di	O
/	O
;	O
(	O
9.14	O
)	O
where	O
ti	O
equals	O
the	O
observed	O
survival	O
time	O
while	O
di	O
indicates	O
whether	O
or	O
not	O
there	O
was	O
censoring	O
,	O
di	O
d	O
1	O
if	O
death	O
observed	O
0	O
if	O
death	O
not	O
observed	O
(	O
(	O
9.15	O
)	O
4	O
the	O
patients	O
were	O
enrolled	O
at	O
different	O
calendar	O
times	O
,	O
as	O
they	O
entered	O
the	O
study	O
,	O
but	O
for	O
each	O
patient	O
“	O
time	O
zero	O
”	O
in	O
the	O
table	O
is	O
set	B
at	O
the	O
beginning	O
of	O
his	O
or	O
her	O
treatment	O
.	O
136	O
survival	O
analysis	B
and	O
the	O
em	O
algorithm	B
table	O
9.3	O
arm	O
a	O
of	O
the	O
ncog	O
head/neck	B
cancer	I
study	O
,	O
binned	O
by	O
month	O
;	O
n	O
d	O
number	O
at	O
risk	O
,	O
y	O
d	O
number	O
of	O
deaths	O
,	O
l	O
d	O
lost	O
to	O
followup	O
,	O
h	B
d	O
hazard	O
rate	B
y=n	O
;	O
os	O
d	O
life	O
table	O
survival	O
estimate	B
.	O
month	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
11	O
12	O
13	O
14	O
15	O
16	O
17	O
18	O
19	O
20	O
21	O
22	O
23	O
24	O
n	O
51	O
50	O
48	O
42	O
40	O
32	O
25	O
24	O
21	O
19	O
16	O
15	O
15	O
15	O
12	O
11	O
11	O
11	O
9	O
9	O
7	O
7	O
7	O
7	O
y	O
1	O
2	O
5	O
2	O
8	O
7	O
0	O
3	O
2	O
2	O
0	O
0	O
0	O
3	O
1	O
0	O
0	O
1	O
0	O
2	O
0	O
0	O
0	O
0	O
l	O
0	O
0	O
1	O
0	O
0	O
0	O
1	O
0	O
0	O
1	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
h	B
.020	O
.040	O
.104	O
.048	O
.200	O
.219	O
.000	O
.125	O
.095	O
.105	O
.000	O
.000	O
.000	O
.200	O
.083	O
.000	O
.000	O
.091	O
.000	O
.222	O
.000	O
.000	O
.000	O
.000	O
os	O
.980	O
.941	O
.843	O
.803	O
.642	O
.502	O
.502	O
.439	O
.397	O
.355	O
.355	O
.355	O
.355	O
.284	O
.261	O
.261	O
.261	O
.237	O
.237	O
.184	O
.184	O
.184	O
.184	O
.184	O
month	O
n	O
y	O
25	O
26	O
27	O
28	O
29	O
30	O
31	O
32	O
33	O
34	O
35	O
36	O
37	O
38	O
39	O
40	O
41	O
42	O
43	O
44	O
45	O
46	O
47	O
7	O
7	O
7	O
7	O
7	O
7	O
7	O
7	O
7	O
7	O
7	O
7	O
7	O
5	O
4	O
4	O
4	O
3	O
3	O
3	O
3	O
2	O
2	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
l	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
1	O
0	O
1	O
h	B
.000	O
.000	O
.000	O
.000	O
.000	O
.000	O
.000	O
.000	O
.000	O
.000	O
.000	O
.000	O
.143	O
.200	O
.000	O
.000	O
.000	O
.000	O
.000	O
.000	O
.000	O
.000	O
.500	O
os	O
.184	O
.184	O
.184	O
.184	O
.184	O
.184	O
.184	O
.184	O
.184	O
.184	O
.184	O
.184	O
.158	O
.126	O
.126	O
.126	O
.126	O
.126	O
.126	O
.126	O
.126	O
.126	O
.063	O
(	O
so	O
di	O
d	O
0	O
corresponds	O
to	O
a	O
c	O
in	O
table	O
9.2	O
)	O
.	O
let	O
t.1/	O
<	O
t.2/	O
<	O
t.3/	O
<	O
:	O
:	O
:	O
<	O
t.n/	O
(	O
9.16	O
)	O
2	O
denote	O
the	O
ordered	O
survival	O
times,5	O
censored	O
or	O
not	O
,	O
with	O
corresponding	O
indicator	O
d.k/	O
for	O
t.k/	O
.	O
the	O
kaplan–meier	O
estimate	B
for	O
survival	O
probability	O
d.k/	O
s.j	O
/	O
d	O
prfx	O
>	O
t.j	O
/g	O
is	O
then	O
the	O
life	O
table	O
estimate	B
	O
n	O
(	O
cid:0	O
)	O
k	O
os.j	O
/	O
dy	O
kj	O
n	O
(	O
cid:0	O
)	O
k	O
c	O
1	O
:	O
(	O
9.17	O
)	O
5	O
assuming	O
no	O
ties	O
among	O
the	O
survival	O
times	O
,	O
which	O
is	O
convenient	O
but	O
not	O
crucial	O
for	O
what	O
follows	O
.	O
9.2	O
censored	O
data	B
and	O
kaplan–meier	O
137	O
os	O
jumps	O
downward	O
at	O
death	O
times	O
tj	O
,	O
and	O
is	O
constant	O
between	O
observed	O
deaths	O
.	O
figure	O
9.1	O
ncog	O
kaplan–meier	O
survival	O
curves	O
;	O
lower	O
arm	O
a	O
(	O
chemotherapy	O
only	O
)	O
;	O
upper	O
arm	O
b	O
(	O
chemotherapycradiation	O
)	O
.	O
vertical	O
lines	O
indicate	O
approximate	O
95	O
%	O
conﬁdence	B
intervals	I
.	O
the	O
kaplan–meier	O
curves	O
for	O
both	O
arms	O
of	O
the	O
ncog	O
study	O
are	O
shown	O
in	O
figure	O
9.1.	O
arm	O
b	O
,	O
the	O
more	O
aggressive	O
treatment	O
,	O
looks	O
better	O
:	O
its	O
50	O
%	O
survival	O
estimate	B
occurs	O
at	O
324	O
days	O
,	O
compared	O
with	O
182	O
days	O
for	O
arm	O
a.	O
the	O
answer	O
to	O
the	O
inferential	O
question—is	O
b	O
really	O
better	O
than	O
a	O
or	O
is	O
this	O
just	O
random	O
variability	O
?	O
—is	O
less	O
clear-cut	O
.	O
the	O
accuracy	O
of	O
os.j	O
/	O
can	O
be	O
estimated	O
from	O
greenwood	O
’	O
s	O
formula	B
	O
for	O
3	O
its	O
standard	B
deviation	I
(	O
now	O
back	O
in	O
life	O
table	O
notation	O
)	O
,	O
(	O
cid:16	O
)	O
os.j	O
/	O
	O
d	O
os.j	O
/	O
sd	O
24x	O
kj	O
351=2	O
yk	O
nk.nk	O
(	O
cid:0	O
)	O
yk/	O
:	O
(	O
9.18	O
)	O
the	O
vertical	O
bars	O
in	O
figure	O
9.1	O
are	O
approximate	O
95	O
%	O
conﬁdence	O
limits	O
for	O
the	O
two	O
curves	O
based	O
on	O
greenwood	O
’	O
s	O
formula	B
.	O
they	O
overlap	O
enough	O
to	O
cast	O
doubt	O
on	O
the	O
superiority	O
of	O
arm	O
b	O
at	O
any	O
one	O
choice	O
of	O
“	O
days	O
,	O
”	O
but	O
the	O
two-	O
sample	B
test	O
of	O
the	O
next	O
section	O
,	O
which	O
compares	O
survival	O
at	O
all	O
timepoints	O
,	O
will	O
provide	O
more	O
deﬁnitive	O
evidence	O
.	O
life	O
tables	O
and	O
the	O
kaplan–meier	O
estimate	B
seem	O
like	O
a	O
textbook	O
example	O
of	O
frequentist	O
inference	B
as	O
described	O
in	O
chapter	O
2	O
:	O
a	O
useful	O
probabilistic	O
02004006008001000120014000.00.20.40.60.81.0dayssurvivalarm	O
a	O
:	O
chemotherapy	O
onlyarm	O
b	O
:	O
chemotherapy	O
+	O
radiation	O
4	O
138	O
survival	O
analysis	B
and	O
the	O
em	O
algorithm	B
result	O
is	O
derived	O
(	O
9.4	O
)	O
,	O
and	O
then	O
implemented	O
by	O
the	O
plug-in	O
principle	O
(	O
9.6	O
)	O
.	O
there	O
is	O
more	O
to	O
the	O
story	O
though	O
,	O
as	O
discussed	O
below	O
.	O
life	O
table	O
curves	O
are	O
nonparametric	B
,	O
in	O
the	O
sense	O
that	O
no	O
particular	O
re-	O
lationship	O
is	O
assumed	O
between	O
the	O
hazard	O
rates	O
hi	O
.	O
a	O
parametric	B
approach	O
can	O
greatly	O
improve	O
the	O
curves	O
’	O
accuracy	O
.	O
	O
reverting	O
to	O
the	O
life	O
table	O
form	B
of	O
table	O
9.3	O
,	O
we	O
assume	O
that	O
the	O
death	O
counts	O
yk	O
are	O
independent	O
binomi-	O
als	O
,	O
(	O
9.19	O
)	O
and	O
that	O
the	O
logits	O
(	O
cid:21	O
)	O
k	O
d	O
logfhk=.1	O
(	O
cid:0	O
)	O
hk/g	O
satisfy	O
some	O
sort	O
of	O
regression	B
equation	O
yk	O
(	O
9.20	O
)	O
as	O
in	O
(	O
8.22	O
)	O
.	O
a	O
cubic	O
regression	B
for	O
instance	O
would	O
set	B
xk	O
d	O
.1	O
;	O
k	O
;	O
k2	O
;	O
k3/	O
0	O
for	O
the	O
kth	O
row	O
of	O
x	O
,	O
with	O
x	O
47	O
(	O
cid:2	O
)	O
4	O
for	O
table	O
9.3.	O
ind	O
(	O
cid:24	O
)	O
bi.nk	O
;	O
hk/	O
;	O
(	O
cid:21	O
)	O
d	O
x	O
˛	O
;	O
figure	O
9.2	O
parametric	B
hazard	O
rate	B
estimates	O
for	O
the	O
ncog	O
study	O
.	O
arm	O
a	O
,	O
black	O
curve	O
,	O
has	O
about	O
2.5	O
times	O
higher	O
hazard	O
than	O
arm	O
b	O
for	O
all	O
times	O
more	O
than	O
a	O
year	O
after	O
treatment	O
.	O
standard	O
errors	O
shown	O
at	O
15	O
and	O
30	O
months	O
.	O
the	O
parametric	B
hazard-rate	O
estimates	O
in	O
figure	O
9.2	O
were	O
instead	O
based	O
on	O
a	O
“	O
cubic-linear	O
spline	O
,	O
”	O
xk	O
d	O
(	O
cid:0	O
)	O
1	O
;	O
k	O
;	O
.k	O
(	O
cid:0	O
)	O
11/2	O
(	O
cid:0	O
)	O
;	O
.k	O
(	O
cid:0	O
)	O
11/3	O
(	O
cid:0	O
)	O
(	O
cid:1	O
)	O
0	O
(	O
9.21	O
)	O
where	O
.k	O
(	O
cid:0	O
)	O
11/	O
(	O
cid:0	O
)	O
equals	O
k	O
(	O
cid:0	O
)	O
11	O
for	O
k	O
	O
11	O
,	O
and	O
0	O
for	O
k	O
(	O
cid:21	O
)	O
11.	O
the	O
vector	B
;	O
0102030400.000.050.100.15monthsdeaths	O
per	O
montharm	O
a	O
:	O
chemotherapy	O
onlyarm	O
b	O
:	O
chemotherapy	O
+	O
radiation	O
9.3	O
the	O
log-rank	O
test	O
139	O
(	O
cid:21	O
)	O
d	O
x	O
˛	O
describes	O
a	O
curve	O
that	O
is	O
cubic	O
for	O
k	O
	O
11	O
,	O
linear	B
for	O
k	O
(	O
cid:21	O
)	O
11	O
,	O
.	O
(	O
cid:16	O
)	O
and	O
joined	O
smoothly	O
at	O
11.	O
the	O
logistic	B
regression	I
maximum	O
likelihood	B
estimate	O
o˛	O
produced	O
hazard	O
rate	B
curves	O
o	O
1	O
c	O
e	O
hk	O
d	O
1	O
o˛	O
as	O
in	O
(	O
8.8	O
)	O
.	O
the	O
black	O
curve	O
in	O
figure	O
9.2	O
traces	O
o	O
red	O
curve	O
is	O
that	O
for	O
arm	O
b	O
,	O
ﬁt	O
separately	O
.	O
hk	O
for	O
arm	O
a	O
,	O
while	O
the	O
(	O
cid:0	O
)	O
x	O
0	O
k	O
(	O
9.22	O
)	O
comparison	O
in	O
terms	O
of	O
hazard	O
rates	O
is	O
more	O
informative	O
than	O
the	O
sur-	O
vival	O
curves	O
of	O
figure	O
9.1.	O
both	O
arms	O
show	O
high	O
initial	O
hazards	O
,	O
peaking	O
at	O
ﬁve	O
months	O
,	O
and	O
then	O
a	O
long	O
slow	O
decline.6	O
arm	O
b	O
hazard	O
is	O
always	O
below	O
arm	O
a	O
,	O
in	O
a	O
ratio	O
of	O
about	O
2.5	O
to	O
1	O
after	O
the	O
ﬁrst	O
year	O
.	O
approximate	O
95	O
%	O
conﬁdence	O
limits	O
,	O
obtained	O
as	O
in	O
(	O
8.30	O
)	O
,	O
don	O
’	O
t	B
overlap	O
,	O
indicating	O
superior-	O
ity	O
of	O
arm	O
b	O
at	O
15	O
and	O
30	O
months	O
after	O
treatment	O
.	O
in	O
addition	O
to	O
its	O
frequentist	O
justiﬁcation	O
,	O
survival	O
analysis	B
takes	O
us	O
into	O
the	O
fisherian	O
realm	O
of	O
conditional	O
inference	B
,	O
section	O
4.3.	O
the	O
yk	O
’	O
s	O
in	O
model	B
(	O
9.19	O
)	O
are	O
considered	O
conditionally	O
on	O
the	O
nk	O
’	O
s	O
,	O
effectively	O
treating	O
the	O
nk	O
values	O
in	O
table	O
9.3	O
as	O
ancillaries	O
,	O
that	O
is	O
as	O
ﬁxed	O
constants	O
,	O
by	O
themselves	O
containing	O
no	O
statistical	O
information	B
about	O
the	O
unknown	O
hazard	O
rates	O
.	O
we	O
will	O
examine	O
this	O
tactic	O
more	O
carefully	O
in	O
the	O
next	O
two	O
sections	O
.	O
9.3	O
the	O
log-rank	O
test	O
a	O
randomized	O
clinical	O
trial	O
,	O
interpreted	O
by	O
a	O
two-sample	B
test	O
,	O
remains	O
the	O
gold	O
standard	O
of	O
medical	O
experimentation	O
.	O
interpretation	O
usually	O
involves	O
student	O
’	O
s	O
two-sample	B
t-test	O
or	O
its	O
nonparametric	B
cousin	O
wilcoxon	O
’	O
s	O
test	O
,	O
but	O
neither	O
of	O
these	O
is	O
suitable	O
for	O
censored	O
data	B
.	O
the	O
log-rank	O
test	O
	O
employs	O
an	O
ingenious	O
extension	O
of	O
life	O
tables	O
for	O
the	O
nonparametric	B
two-	O
sample	B
comparison	O
of	O
censored	O
survival	O
data	B
.	O
table	O
9.4	O
compares	O
the	O
results	O
of	O
the	O
ncog	O
study	O
for	O
the	O
ﬁrst	O
six	O
months7	O
after	O
treatment	O
.	O
at	O
the	O
beginning8	O
of	O
month	O
1	O
there	O
were	O
45	O
patients	O
“	O
at	O
risk	O
”	O
in	O
arm	O
b	O
,	O
none	O
of	O
whom	O
died	O
,	O
compared	O
with	O
51	O
at	O
risk	O
and	O
1	O
death	O
in	O
arm	O
a.	O
this	O
left	O
45	O
at	O
risk	O
in	O
arm	O
b	O
at	O
the	O
beginning	O
of	O
month	O
2	O
,	O
and	O
50	O
in	O
arm	O
a	O
,	O
with	O
1	O
and	O
2	O
deaths	O
during	O
the	O
month	O
respectively	O
.	O
(	O
losses	O
5	O
6	O
the	O
cubic–linear	O
spline	O
(	O
9.21	O
)	O
is	O
designed	O
to	O
show	O
more	O
detail	O
in	O
the	O
early	O
months	O
,	O
where	O
there	O
is	O
more	O
available	O
patient	O
data	B
and	O
where	O
hazard	O
rates	O
usually	O
change	O
more	O
quickly	O
.	O
7	O
a	O
month	O
is	O
deﬁned	O
here	O
as	O
365/12=30.4	O
days	O
.	O
8	O
the	O
“	O
beginning	O
of	O
month	O
1	O
”	O
is	O
each	O
patient	O
’	O
s	O
initial	O
treatment	O
time	O
,	O
at	O
which	O
all	O
45	O
patients	O
ever	O
enrolled	O
in	O
arm	O
b	O
were	O
at	O
risk	O
,	O
that	O
is	O
,	O
available	O
for	O
observation	O
.	O
140	O
survival	O
analysis	B
and	O
the	O
em	O
algorithm	B
table	O
9.4	O
life	O
table	O
comparison	O
for	O
the	O
ﬁrst	O
six	O
months	O
of	O
the	O
ncog	O
study	O
.	O
for	O
example	O
,	O
at	O
the	O
beginning	O
of	O
the	O
sixth	O
month	O
after	O
treatment	O
,	O
there	O
were	O
33	O
remaining	O
arm	O
b	O
patients	O
,	O
of	O
whom	O
4	O
died	O
during	O
the	O
month	O
,	O
compared	O
with	O
32	O
at	O
risk	O
and	O
7	O
dying	O
in	O
arm	O
a.	O
the	O
conditional	O
expected	O
number	O
of	O
deaths	O
in	O
arm	O
a	O
,	O
assuming	O
the	O
null	O
hypothesis	O
of	O
equal	O
hazard	O
rates	O
in	O
both	O
arms	O
,	O
was	O
5.42	O
,	O
using	O
expression	O
(	O
9.24	O
)	O
.	O
month	O
arm	O
b	O
arm	O
a	O
at	O
risk	O
died	O
at	O
risk	O
died	O
expected	O
number	O
arm	O
a	O
deaths	O
1	O
2	O
3	O
4	O
5	O
6	O
45	O
45	O
44	O
43	O
38	O
33	O
0	O
1	O
1	O
5	O
5	O
4	O
51	O
50	O
48	O
42	O
40	O
32	O
1	O
2	O
5	O
2	O
8	O
7	O
.53	O
1.56	O
3.13	O
3.46	O
6.67	O
5.42	O
to	O
followup	O
were	O
assumed	O
to	O
occur	O
at	O
the	O
end	O
of	O
each	O
month	O
;	O
there	O
was	O
1	O
such	O
at	O
the	O
end	O
of	O
month	O
3	O
,	O
reducing	O
the	O
number	O
at	O
risk	O
in	O
arm	O
a	O
to	O
42	O
for	O
month	O
4	O
.	O
)	O
the	O
month	O
6	O
data	B
is	O
displayed	O
in	O
two-by-two	O
tabular	O
form	B
in	O
table	O
9.5	O
,	O
showing	O
the	O
notation	O
used	O
in	O
what	O
follows	O
:	O
na	O
for	O
the	O
number	O
at	O
risk	O
in	O
arm	O
a	O
,	O
nd	O
for	O
the	O
number	O
of	O
deaths	O
,	O
etc	O
.	O
;	O
y	O
indicates	O
the	O
number	O
of	O
arm	O
a	O
deaths	O
.	O
if	O
the	O
marginal	O
totals	O
na	O
;	O
nb	O
;	O
nd	O
;	O
and	O
ns	O
are	O
given	O
,	O
then	O
y	O
deter-	O
mines	O
the	O
other	O
three	O
table	O
entries	O
by	O
subtraction	O
,	O
so	O
we	O
are	O
not	O
losing	O
any	O
information	B
by	O
focusing	O
on	O
y.	O
table	O
9.5	O
two-by-two	O
display	O
of	O
month-6	O
data	B
for	O
the	O
ncog	O
study	O
.	O
e	O
is	O
the	O
expected	O
number	O
of	O
arm	O
a	O
deaths	O
assuming	O
the	O
null	O
hypothesis	O
of	O
equal	O
hazard	O
rates	O
(	O
last	O
column	O
of	O
table	O
9.4	O
)	O
.	O
arm	O
a	O
arm	O
b	O
died	O
y	O
d	O
7	O
e	O
d	O
5:42	O
4	O
d	O
11	O
nd	O
survived	O
25	O
29	O
ns	O
d	O
54	O
na	O
d	O
32	O
nb	O
d	O
33	O
n	O
d	O
65	O
consider	O
the	O
null	O
hypothesis	O
that	O
the	O
hazard	O
rates	O
(	O
9.3	O
)	O
for	O
month	O
6	O
are	O
9.3	O
the	O
log-rank	O
test	O
the	O
same	O
in	O
arm	O
a	O
and	O
arm	O
b	O
,	O
h0.6/	O
w	O
ha6	O
d	O
hb6	O
:	O
under	O
h0.6/	O
,	O
y	O
has	O
mean	O
e	O
and	O
variance	O
v	O
,	O
e	O
d	O
nand	O
=n	O
v	O
d	O
nanb	O
nd	O
ns	O
ı	O
(	O
cid:2	O
)	O
n2.n	O
(	O
cid:0	O
)	O
1/	O
(	O
cid:3	O
)	O
;	O
141	O
(	O
9.23	O
)	O
(	O
9.24	O
)	O
as	O
calculated	O
according	O
to	O
the	O
hypergeometric	O
distribution.	O
e	O
d	O
5:42	O
and	O
6	O
v	O
d	O
2:28	O
in	O
table	O
9.5.	O
we	O
can	O
form	B
a	O
two-by-two	O
table	O
for	O
each	O
of	O
the	O
n	O
d	O
47	O
months	O
of	O
the	O
ncog	O
study	O
,	O
calculating	O
yi	O
;	O
ei	O
,	O
and	O
vi	O
for	O
month	O
i.	O
the	O
log-rank	O
statistic	B
z	O
is	O
then	O
deﬁned	O
to	O
be	O
7	O
z	O
d	O
nx	O
id1	O
.yi	O
(	O
cid:0	O
)	O
ei	O
/	O
!	O
1=2	O
,	O
nx	O
id1	O
vi	O
:	O
(	O
9.25	O
)	O
the	O
idea	O
here	O
is	O
simple	O
but	O
clever	O
.	O
each	O
month	O
we	O
test	O
the	O
null	O
hypothesis	O
of	O
equal	O
hazard	O
rates	O
h0.i	O
/	O
w	O
hai	O
d	O
hbi	O
:	O
(	O
9.26	O
)	O
the	O
numerator	O
yi	O
(	O
cid:0	O
)	O
ei	O
has	O
expectation	O
0	O
under	O
h0.i	O
/	O
,	O
but	O
,	O
if	O
hai	O
is	O
greater	O
than	O
hbi	O
,	O
that	O
is	O
,	O
if	O
treatment	O
b	O
is	O
superior	O
,	O
then	O
the	O
numerator	O
has	O
a	O
pos-	O
itive	O
expectation	O
.	O
adding	O
up	O
the	O
numerators	O
gives	O
us	O
power	O
to	O
detect	O
a	O
general	O
superiority	O
of	O
treatment	O
b	O
over	O
a	O
,	O
against	O
the	O
null	O
hypothesis	O
of	O
equal	O
hazard	O
rates	O
,	O
hai	O
d	O
hbi	O
for	O
all	O
i.	O
for	O
the	O
ncog	O
study	O
,	O
binned	O
by	O
months	O
,	O
ei	O
d	O
32:9	O
;	O
vi	O
d	O
16:0	O
;	O
yi	O
d	O
42	O
;	O
nx	O
(	O
9.27	O
)	O
nx	O
id1	O
nx	O
id1	O
id1	O
giving	O
log-rank	O
test	O
statistic	B
z	O
d	O
2:27	O
:	O
asymptotic	O
calculations	O
based	O
on	O
the	O
central	O
limit	O
theorem	B
suggest	O
z	O
p	O
(	O
cid:24	O
)	O
n	O
.0	O
;	O
1/	O
(	O
9.28	O
)	O
(	O
9.29	O
)	O
under	O
the	O
null	O
hypothesis	O
that	O
the	O
two	O
treatments	O
are	O
equally	O
effective	O
,	O
i.e.	O
,	O
that	O
hai	O
d	O
hbi	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
.	O
in	O
the	O
usual	O
interpretation	O
,	O
z	O
d	O
2:27	O
is	O
signiﬁcant	O
at	O
the	O
one-sided	O
0.012	O
level	O
,	O
providing	O
moderately	O
strong	O
evidence	O
in	O
favor	O
of	O
treatment	O
b.	O
an	O
impressive	O
amount	O
of	O
inferential	O
guile	O
goes	O
into	O
the	O
log-rank	O
test	O
.	O
142	O
survival	O
analysis	B
and	O
the	O
em	O
algorithm	B
1	O
working	O
with	O
hazard	O
rates	O
instead	O
of	O
densities	O
or	O
cdfs	O
is	O
essential	O
for	O
survival	O
data	B
.	O
2	O
conditioning	O
at	O
each	O
period	O
on	O
the	O
numbers	O
at	O
risk	O
,	O
na	O
and	O
nb	O
in	O
ta-	O
ble	O
9.5	O
,	O
ﬁnesses	O
the	O
difﬁculties	O
of	O
censored	O
data	B
;	O
censoring	O
only	O
changes	O
the	O
at-risk	O
numbers	O
in	O
future	O
periods	O
.	O
3	O
also	O
conditioning	O
on	O
the	O
number	O
of	O
deaths	O
and	O
survivals	O
,	O
nd	O
and	O
ns	O
in	O
table	O
9.5	O
,	O
leaves	O
only	O
the	O
univariate	O
statistic	B
y	O
to	O
interpret	O
at	O
each	O
period	O
,	O
which	O
is	O
easily	O
done	O
through	O
the	O
null	O
hypothesis	O
of	O
equal	O
hazard	O
rates	O
(	O
9.26	O
)	O
.	O
4	O
adding	O
the	O
discrepancies	O
yi	O
(	O
cid:0	O
)	O
ei	O
in	O
the	O
numerator	O
of	O
(	O
9.25	O
)	O
(	O
rather	O
than	O
say	O
,	O
adding	O
the	O
individual	O
z	O
values	O
zi	O
d	O
.yi	O
(	O
cid:0	O
)	O
ei	O
/=v	O
1=2	O
,	O
or	O
adding	O
the	O
i	O
values	O
)	O
accrues	O
power	O
for	O
the	O
natural	O
alternative	O
hypothesis	O
“	O
hai	O
>	O
z2	O
hbi	O
for	O
all	O
i	O
,	O
”	O
while	O
avoiding	O
destabilization	O
from	O
small	O
values	O
of	O
vi	O
.	O
i	O
each	O
of	O
the	O
four	O
tactics	O
had	O
been	O
used	O
separately	O
in	O
classical	O
applica-	O
tions	O
.	O
putting	O
them	O
together	O
into	O
the	O
log-rank	O
test	O
was	O
a	O
major	O
inferential	O
accomplishment	O
,	O
foreshadowing	O
a	O
still	O
bigger	O
step	O
forward	O
,	O
the	O
propor-	O
tional	O
hazards	O
model	B
,	O
our	O
subject	O
in	O
the	O
next	O
section	O
.	O
conditional	O
inference	B
takes	O
on	O
an	O
aggressive	O
form	B
in	O
the	O
log-rank	O
test	O
.	O
let	O
di	O
indicate	O
all	O
the	O
data	B
except	O
yi	O
available	O
at	O
the	O
end	O
of	O
the	O
ith	O
period	O
.	O
for	O
month	O
6	O
in	O
the	O
ncog	O
study	O
,	O
d6	O
includes	O
all	O
data	B
for	O
months	O
1–5	O
in	O
table	O
9.4	O
,	O
and	O
the	O
marginals	O
na	O
;	O
nb	O
;	O
nd	O
;	O
and	O
ns	O
in	O
table	O
9.5	O
,	O
but	O
not	O
the	O
y	O
value	O
for	O
month	O
6.	O
the	O
key	O
assumption	O
is	O
that	O
,	O
under	O
the	O
null	O
hypothesis	O
of	O
equal	O
hazard	O
rates	O
(	O
9.26	O
)	O
,	O
yijdi	O
ind	O
(	O
cid:24	O
)	O
.ei	O
;	O
vi	O
/	O
;	O
(	O
9.30	O
)	O
“	O
ind	O
”	O
here	O
meaning	O
that	O
the	O
yi	O
’	O
s	O
can	O
be	O
treated	O
as	O
independent	O
quantities	O
with	O
means	O
and	O
variances	O
(	O
9.24	O
)	O
.	O
in	O
particular	O
,	O
we	O
can	O
add	O
the	O
variances	O
vi	O
to	O
get	O
the	O
denominator	O
of	O
(	O
9.25	O
)	O
.	O
(	O
a	O
“	O
partial	O
likelihood	B
”	O
argument	B
,	O
de-	O
scribed	O
in	O
the	O
endnotes	O
,	O
justiﬁes	O
adding	O
the	O
variances	O
.	O
)	O
the	O
purpose	O
of	O
all	O
this	O
fisherian	O
conditioning	O
is	O
to	O
simplify	O
the	O
infer-	O
ence	O
:	O
the	O
conditional	O
distribution	B
yijdi	O
depends	O
only	O
on	O
the	O
hazard	O
rates	O
hai	O
and	O
hbi	O
;	O
“	O
nuisance	O
parameters	O
,	O
”	O
relating	O
to	O
the	O
survival	O
times	O
and	O
cen-	O
soring	O
mechanism	O
of	O
the	O
data	B
in	O
table	O
9.2	O
,	O
are	O
hidden	O
away	O
.	O
there	O
is	O
a	O
price	O
to	O
pay	O
in	O
testing	B
power	O
,	O
though	O
usually	O
a	O
small	O
one	O
.	O
the	O
lost-to-followup	O
values	O
l	O
in	O
table	O
9.3	O
have	O
been	O
ignored	O
,	O
even	O
though	O
they	O
might	O
contain	O
useful	O
information	B
,	O
say	O
if	O
all	O
the	O
early	O
losses	O
occurred	O
in	O
one	O
arm	O
.	O
9.4	O
the	O
proportional	O
hazards	O
model	B
143	O
9.4	O
the	O
proportional	O
hazards	O
model	B
the	O
kaplan–meier	O
estimator	B
is	O
a	O
one-sample	O
device	O
,	O
dealing	O
with	O
data	B
coming	O
from	O
a	O
single	O
distribution	B
.	O
the	O
log-rank	O
test	O
makes	O
two-sample	B
comparisons	O
.	O
proportional	O
hazards	O
ups	O
the	O
ante	O
to	O
allow	O
for	O
a	O
full	B
regres-	O
sion	O
analysis	B
of	O
censored	O
data	B
.	O
now	O
the	O
individual	O
data	B
points	O
zi	O
are	O
of	O
the	O
form	B
zi	O
d	O
.ci	O
;	O
ti	O
;	O
di	O
/	O
;	O
(	O
9.31	O
)	O
where	O
ti	O
and	O
di	O
are	O
observed	O
survival	O
time	O
and	O
censoring	O
indicator	O
,	O
as	O
in	O
(	O
9.14	O
)	O
–	O
(	O
9.15	O
)	O
,	O
and	O
ci	O
is	O
a	O
known	O
1	O
(	O
cid:2	O
)	O
p	O
vector	B
of	O
covariates	O
whose	O
effect	O
on	O
survival	O
we	O
wish	O
to	O
assess	O
.	O
both	O
of	O
the	O
previous	O
methods	O
are	O
included	O
here	O
:	O
for	O
the	O
log-rank	O
test	O
,	O
ci	O
indicates	O
treatment	O
,	O
say	O
ci	O
equals	O
0	O
or	O
1	O
for	O
arm	O
a	O
or	O
arm	O
b	O
,	O
while	O
ci	O
is	O
absent	O
for	O
kaplan–meier	O
.	O
table	O
9.6	O
pediatric	B
cancer	I
data	O
,	O
ﬁrst	O
20	O
of	O
1620	O
children	O
.	O
sex	O
1	O
d	O
male	O
,	O
2	O
d	O
female	O
;	O
race	O
1	O
d	O
white	O
,	O
2	O
d	O
nonwhite	O
;	O
age	O
in	O
years	O
;	O
entry	O
d	O
calendar	O
date	O
of	O
entry	O
in	O
days	O
since	O
july	O
1	O
,	O
2001	O
;	O
far	O
d	O
home	O
distance	O
from	O
treatment	O
center	O
in	O
miles	O
;	O
t	B
d	O
survival	O
time	O
in	O
days	O
;	O
d	O
d	O
1	O
if	O
death	O
observed	O
,	O
0	O
if	O
not	O
.	O
sex	O
race	O
age	O
entry	O
far	O
t	B
d	O
1	O
2	O
2	O
2	O
1	O
2	O
2	O
2	O
1	O
2	O
1	O
1	O
1	O
1	O
2	O
1	O
2	O
1	O
1	O
2	O
1	O
1	O
2	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
2	O
2	O
2.50	O
10.00	O
18.17	O
3.92	O
11.83	O
11.17	O
5.17	O
10.58	O
1.17	O
6.83	O
13.92	O
5.17	O
2.50	O
.83	O
15.50	O
17.83	O
3.25	O
10.75	O
18.08	O
5.83	O
710	O
1866	O
2531	O
2210	O
875	O
1419	O
1264	O
670	O
1518	O
2101	O
1239	O
518	O
1849	O
2758	O
2004	O
986	O
1443	O
2807	O
1229	O
2727	O
108	O
38	O
100	O
100	O
78	O
0	O
28	O
120	O
73	O
104	O
0	O
117	O
99	O
38	O
12	O
65	O
58	O
42	O
23	O
23	O
325	O
1451	O
221	O
2158	O
760	O
168	O
2976	O
1833	O
131	O
2405	O
969	O
1894	O
193	O
1756	O
682	O
1835	O
2993	O
1616	O
1302	O
174	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
144	O
survival	O
analysis	B
and	O
the	O
em	O
algorithm	B
medical	O
studies	O
regularly	O
produce	O
data	B
of	O
form	B
(	O
9.31	O
)	O
.	O
an	O
example	O
,	O
the	O
pediatric	B
cancer	I
data	O
,	O
is	O
partially	O
listed	O
in	O
table	O
9.6.	O
the	O
ﬁrst	O
20	O
of	O
n	O
d	O
1620	O
cases	O
are	O
shown	O
.	O
there	O
are	O
ﬁve	O
explanatory	O
covariates	O
(	O
deﬁned	O
in	O
the	O
table	O
’	O
s	O
caption	O
)	O
:	O
sex	O
,	O
race	O
,	O
age	O
at	O
entry	O
,	O
calendar	O
date	O
of	O
entry	O
into	O
the	O
study	O
,	O
and	O
far	O
,	O
the	O
distance	O
of	O
the	O
child	O
’	O
s	O
home	O
from	O
the	O
treatment	O
center	O
.	O
the	O
response	O
variable	O
t	B
is	O
survival	O
in	O
days	O
from	O
time	O
of	O
treatment	O
until	O
death	O
.	O
happily	O
,	O
only	O
160	O
of	O
the	O
children	O
were	O
observed	O
to	O
die	O
(	O
d	O
d	O
1	O
)	O
.	O
some	O
left	O
the	O
study	O
for	O
various	O
reasons	O
,	O
but	O
most	O
of	O
the	O
d	O
d	O
0	O
cases	O
were	O
those	O
children	O
still	O
alive	O
at	O
the	O
end	O
of	O
the	O
study	O
period	O
.	O
of	O
particular	O
interest	O
was	O
the	O
effect	O
of	O
far	O
on	O
survival	O
.	O
we	O
wish	O
to	O
carry	O
out	O
a	O
regression	B
analysis	O
of	O
this	O
heavily	O
censored	O
data	B
set	O
.	O
the	O
proportional	O
hazards	O
model	B
assumes	O
that	O
the	O
hazard	O
rate	B
hi	O
.t	O
/	O
for	O
the	O
ith	O
individual	O
(	O
9.8	O
)	O
is	O
hi	O
.t	O
/	O
d	O
h0.t	O
/ec	O
0	O
i	O
ˇ	O
:	O
(	O
9.32	O
)	O
here	O
h0.t	O
/	O
is	O
a	O
baseline	O
hazard	O
(	O
which	O
we	O
need	O
not	O
specify	O
)	O
and	O
ˇ	O
is	O
an	O
unknown	O
p-parameter	B
vector	O
we	O
want	O
to	O
estimate	B
.	O
for	O
concise	O
notation	O
,	O
let	O
model	B
(	O
9.32	O
)	O
says	O
that	O
individual	O
i	O
’	O
s	O
hazard	O
is	O
a	O
constant	O
nonnegative	O
factor	B
i	O
times	O
the	O
baseline	O
hazard	O
.	O
equivalently	O
,	O
from	O
(	O
9.11	O
)	O
,	O
the	O
ith	O
survival	O
function	B
si	O
.t	O
/	O
is	O
a	O
power	O
of	O
the	O
baseline	O
survival	O
function	B
s0.t	O
/	O
,	O
i	O
d	O
ec	O
i	O
ˇi	O
0	O
si	O
.t	O
/	O
d	O
s0.t	O
/i	O
:	O
(	O
9.33	O
)	O
(	O
9.34	O
)	O
larger	O
values	O
of	O
i	O
lead	O
to	O
more	O
quickly	O
declining	O
survival	O
curves	O
,	O
i.e.	O
,	O
to	O
worse	O
survival	O
(	O
as	O
in	O
(	O
9.11	O
)	O
)	O
.	O
let	O
j	O
be	O
the	O
number	O
of	O
observed	O
deaths	O
,	O
j	O
d	O
160	O
here	O
,	O
occurring	O
at	O
times	O
t.1/	O
<	O
t.2/	O
<	O
:	O
:	O
:	O
<	O
t.j	O
/	O
;	O
(	O
9.35	O
)	O
again	O
for	O
convenience	O
assuming	O
no	O
ties.9	O
just	O
before	O
time	O
t.j	O
/	O
there	O
is	O
a	O
risk	O
set	B
of	O
individuals	O
still	O
under	O
observation	O
,	O
whose	O
indices	O
we	O
denote	O
by	O
rj	O
,	O
(	O
9.36	O
)	O
rj	O
d	O
fi	O
w	O
ti	O
(	O
cid:21	O
)	O
t.j	O
/g	O
:	O
let	O
ij	O
be	O
the	O
index	O
of	O
the	O
individual	O
observed	O
to	O
die	O
at	O
time	O
t.j	O
/	O
.	O
the	O
key	O
to	O
proportional	O
hazards	O
regression	B
is	O
the	O
following	O
result	O
.	O
9	O
more	O
precisely	O
,	O
assuming	O
only	O
one	O
event	O
,	O
a	O
death	O
,	O
occurred	O
at	O
t.j	O
/	O
,	O
with	O
none	O
of	O
the	O
other	O
individuals	O
being	O
lost	O
to	O
followup	O
at	O
exact	O
time	O
t.j	O
/	O
.	O
9.4	O
the	O
proportional	O
hazards	O
model	B
145	O
lemma	O
	O
under	O
the	O
proportional	O
hazards	O
model	B
(	O
9.32	O
)	O
,	O
the	O
conditional	O
8	O
probability	O
,	O
given	O
the	O
risk	O
set	B
rj	O
,	O
that	O
individual	O
i	O
in	O
rj	O
is	O
the	O
one	O
ob-	O
served	O
to	O
die	O
at	O
time	O
t.j	O
/	O
is	O
prfij	O
d	O
ij	O
(	O
cid:30	O
)	O
x	O
(	O
9.37	O
)	O
0	O
k	O
ˇ	O
:	O
0	O
i	O
ˇ	O
ec	O
rjg	O
d	O
ec	O
k2	O
rj	O
to	O
put	O
it	O
in	O
words	O
,	O
given	O
that	O
one	O
person	O
dies	O
at	O
time	O
t.j	O
/	O
,	O
the	O
probability	O
0	O
i	O
ˇ/	O
,	O
among	O
the	O
set	B
of	O
individuals	O
it	O
is	O
individual	O
i	O
is	O
proportional	O
to	O
exp.c	O
at	O
risk	O
.	O
for	O
the	O
purpose	O
of	O
estimating	O
the	O
parameter	O
vector	B
ˇ	O
in	O
model	B
(	O
9.32	O
)	O
,	O
we	O
multiply	O
factors	O
(	O
9.37	O
)	O
to	O
form	B
the	O
partial	O
likelihood	B
l.ˇ/	O
d	O
jy	O
jd1	O
0	O
@	O
e	O
0	O
ij	O
c	O
ˇ	O
(	O
cid:30	O
)	O
x	O
k2	O
rj	O
1a	O
:	O
0	O
k	O
ˇ	O
ec	O
l.ˇ/	O
is	O
then	O
treated	O
as	O
an	O
ordinary	O
likelihood	B
function	O
,	O
yielding	O
an	O
approx-	O
imately	O
unbiased	O
mle-like	O
estimate	B
(	O
9.38	O
)	O
(	O
9.39	O
)	O
o	O
ˇ	O
d	O
arg	O
max	O
fl.ˇ/g	O
;	O
ˇ	O
ˇ	O
p	O
(	O
cid:24	O
)	O
	O
o	O
ˇ	O
;	O
i	O
(	O
cid:0	O
)	O
1	O
h	B
(	O
cid:0	O
)	O
r	O
l	O
(	O
cid:16	O
)	O
o	O
ˇ	O
with	O
an	O
approximate	O
covariance	O
obtained	O
from	O
the	O
second-derivative	O
ma-	O
trix	O
of	O
l.ˇ/	O
d	O
log	O
l.ˇ/	O
,	O
	O
as	O
in	O
section	O
4.3	O
,	O
9	O
:	O
(	O
9.40	O
)	O
table	O
9.7	O
shows	O
the	O
proportional	O
hazards	O
analysis	B
of	O
the	O
pediatric	O
can-	O
cer	O
data	B
,	O
with	O
the	O
covariates	O
age	O
,	O
entry	O
,	O
and	O
far	O
standardized	O
to	O
have	O
mean	O
0	O
and	O
standard	O
deviation	O
1	O
for	O
the	O
1620	O
cases.10	O
neither	O
sex	O
nor	O
race	O
seems	O
to	O
make	O
much	O
difference	O
.	O
we	O
see	O
that	O
age	O
is	O
a	O
mildly	O
signif-	O
icant	O
factor	B
,	O
with	O
older	O
children	O
doing	O
better	O
(	O
i.e.	O
,	O
the	O
estimated	O
regression	B
coefﬁcient	O
is	O
negative	O
)	O
.	O
however	O
,	O
the	O
dramatic	O
effects	O
are	O
date	O
of	O
entry	O
and	O
far	O
.	O
individuals	O
who	O
entered	O
the	O
study	O
later	O
survived	O
longer—perhaps	O
the	O
treatment	O
protocol	O
was	O
being	O
improved—while	O
children	O
living	O
farther	O
away	O
from	O
the	O
treatment	O
center	O
did	O
worse	O
.	O
justiﬁcation	O
of	O
the	O
partial	O
likelihood	B
calculations	O
is	O
similar	O
to	O
that	O
for	O
the	O
log-rank	O
test	O
,	O
but	O
there	O
are	O
some	O
important	O
differences	O
,	O
too	O
:	O
the	O
pro-	O
portional	O
hazards	O
model	B
is	O
semiparametric	O
(	O
“	O
semi	O
”	O
because	O
we	O
don	O
’	O
t	B
have	O
to	O
specify	O
h0.t	O
/	O
in	O
(	O
9.32	O
)	O
)	O
,	O
rather	O
than	O
nonparametric	B
as	O
before	O
;	O
and	O
the	O
10	O
table	O
9.7	O
was	O
obtained	O
using	O
the	O
r	O
program	O
coxph	O
.	O
146	O
survival	O
analysis	B
and	O
the	O
em	O
algorithm	B
table	O
9.7	O
proportional	O
hazards	O
analysis	B
of	O
pediatric	B
cancer	I
data	O
(	O
age	O
,	O
entry	O
and	O
far	O
standardized	O
)	O
.	O
age	O
signiﬁcantly	O
negative	O
,	O
older	O
children	O
doing	O
better	O
;	O
entry	O
very	O
signiﬁcantly	O
negative	O
,	O
showing	O
hazard	O
rate	B
declining	O
with	O
calendar	O
date	O
of	O
entry	O
;	O
far	O
very	O
signiﬁcantly	O
positive	O
,	O
indicating	O
worse	O
results	O
for	O
children	O
living	O
farther	O
away	O
from	O
the	O
treatment	O
center	O
.	O
last	O
two	O
columns	O
show	O
limits	O
of	O
approximate	O
95	O
%	O
conﬁdence	B
intervals	I
for	O
exp.ˇ/	O
.	O
ˇ	O
(	O
cid:0	O
)	O
.023	O
sex	O
race	O
.282	O
(	O
cid:0	O
)	O
.235	O
age	O
entry	O
(	O
cid:0	O
)	O
.460	O
far	O
.296	O
sd	O
z-value	O
(	O
cid:0	O
)	O
.142	O
.160	O
.169	O
1.669	O
.088	O
(	O
cid:0	O
)	O
2.664	O
.079	O
(	O
cid:0	O
)	O
5.855	O
4.117	O
.072	O
p-value	B
exp.ˇ/	O
lower	O
upper	O
.887	O
.095	O
.008	O
.000	O
.000	O
.98	O
1.33	O
.79	O
.63	O
1.34	O
.71	O
.95	O
.67	O
.54	O
1.17	O
1.34	O
1.85	O
.94	O
.74	O
1.55	O
emphasis	O
on	O
likelihood	B
has	O
increased	O
the	O
fisherian	O
nature	O
of	O
the	O
inference	B
,	O
moving	O
it	O
further	O
away	O
from	O
pure	O
frequentism	O
.	O
still	O
more	O
fisherian	O
is	O
the	O
emphasis	O
on	O
likelihood	B
inference	O
in	O
(	O
9.38	O
)	O
–	O
(	O
9.40	O
)	O
,	O
rather	O
than	O
the	O
direct	O
frequentist	O
calculations	O
of	O
(	O
9.24	O
)	O
–	O
(	O
9.25	O
)	O
.	O
the	O
conditioning	O
argument	B
here	O
is	O
less	O
obvious	O
than	O
that	O
for	O
the	O
kaplan–	O
meier	O
estimate	B
or	O
the	O
log-rank	O
test	O
.	O
has	O
its	O
convenience	O
possibly	O
come	O
at	O
too	O
high	O
a	O
price	O
?	O
in	O
fact	O
it	O
can	O
be	O
shown	O
that	O
inference	B
based	O
on	O
the	O
partial	O
likelihood	B
is	O
highly	O
efﬁcient	O
,	O
assuming	O
of	O
course	O
the	O
correctness	O
of	O
the	O
proportional	O
hazards	O
model	B
(	O
9.32	O
)	O
.	O
9.5	O
missing	B
data	I
and	O
the	O
em	O
algorithm	B
censored	O
data	B
,	O
the	O
motivating	O
factor	B
for	O
survival	O
analysis	B
,	O
can	O
be	O
thought	O
of	O
as	O
a	O
special	O
case	O
of	O
a	O
more	O
general	O
statistical	O
topic	O
,	O
missing	B
data	I
.	O
what	O
’	O
s	O
missing	O
,	O
in	O
table	O
9.2	O
for	O
example	O
,	O
are	O
the	O
actual	O
survival	O
times	O
for	O
the	O
c	O
cases	O
,	O
which	O
are	O
known	O
only	O
to	O
exceed	O
the	O
tabled	O
values	O
.	O
if	O
the	O
data	B
were	O
not	O
missing	O
,	O
we	O
could	O
use	O
standard	O
statistical	O
methods	O
,	O
for	O
instance	O
wilcoxon	O
’	O
s	O
test	O
,	O
to	O
compare	O
the	O
two	O
arms	O
of	O
the	O
ncog	O
study	O
.	O
the	O
em	O
algo-	O
rithm	O
is	O
an	O
iterative	O
technique	O
for	O
solving	O
missing-data	O
inferential	O
problems	O
using	O
only	O
standard	O
methods	O
.	O
a	O
missing-data	O
situation	O
is	O
shown	O
in	O
figure	O
9.3	O
:	O
n	O
d	O
40	O
points	O
have	O
been	O
independently	O
sampled	O
from	O
a	O
bivariate	O
normal	B
distribution	O
(	O
5.12	O
)	O
,	O
9.5	O
missing	B
data	I
and	O
the	O
em	O
algorithm	B
147	O
figure	O
9.3	O
forty	O
points	O
from	O
a	O
bivariate	O
normal	B
distribution	O
,	O
the	O
last	O
20	O
with	O
x2	O
missing	O
(	O
circled	O
)	O
.	O
means	O
.	O
(	O
cid:22	O
)	O
1	O
;	O
(	O
cid:22	O
)	O
2/	O
,	O
variances	O
.	O
(	O
cid:27	O
)	O
2	O
1	O
;	O
(	O
cid:27	O
)	O
2	O
2	O
/	O
,	O
and	O
correlation	O
(	O
cid:26	O
)	O
,	O
!	O
x1i	O
x2i	O
ind	O
(	O
cid:24	O
)	O
n2	O
!	O
;	O
	O
(	O
cid:27	O
)	O
2	O
1	O
(	O
cid:27	O
)	O
1	O
(	O
cid:27	O
)	O
2	O
(	O
cid:26	O
)	O
(	O
cid:22	O
)	O
1	O
(	O
cid:22	O
)	O
2	O
(	O
cid:27	O
)	O
1	O
(	O
cid:27	O
)	O
2	O
(	O
cid:26	O
)	O
(	O
cid:27	O
)	O
2	O
2	O
	O
!	O
:	O
(	O
9.41	O
)	O
however	O
,	O
the	O
second	O
coordinates	O
of	O
the	O
last	O
20	O
points	O
have	O
been	O
lost	O
.	O
these	O
are	O
represented	O
by	O
the	O
circled	O
points	O
in	O
figure	O
9.3	O
,	O
with	O
their	O
x2	O
values	O
arbitrarily	O
set	B
to	O
0.	O
we	O
wish	O
to	O
ﬁnd	O
the	O
maximum	B
likelihood	I
estimate	O
of	O
the	O
parameter	O
vec-	O
tor	O
	O
d	O
.	O
(	O
cid:22	O
)	O
1	O
;	O
(	O
cid:22	O
)	O
2	O
,	O
(	O
cid:27	O
)	O
1	O
;	O
(	O
cid:27	O
)	O
2	O
;	O
(	O
cid:26	O
)	O
/	O
.	O
the	O
standard	O
maximum	O
likelihood	B
estimates	O
o	O
(	O
cid:22	O
)	O
2	O
d	O
40x	O
''	O
40x	O
#	O
1=2	O
.x2i	O
(	O
cid:0	O
)	O
o	O
(	O
cid:22	O
)	O
2/2	O
=40	O
#	O
,	O
id1	O
.x1i	O
(	O
cid:0	O
)	O
o	O
(	O
cid:22	O
)	O
1/2	O
=40	O
o	O
(	O
cid:22	O
)	O
1	O
d	O
40x	O
''	O
40x	O
id1	O
.x1i	O
(	O
cid:0	O
)	O
o	O
(	O
cid:22	O
)	O
1/	O
.x2i	O
(	O
cid:0	O
)	O
o	O
(	O
cid:22	O
)	O
2/	O
=40	O
''	O
40x	O
id1	O
o	O
(	O
cid:26	O
)	O
d	O
.o	O
(	O
cid:27	O
)	O
1o	O
(	O
cid:27	O
)	O
2/	O
;	O
x1i	O
=40	O
;	O
#	O
1=2	O
o	O
(	O
cid:27	O
)	O
1	O
d	O
o	O
(	O
cid:27	O
)	O
2	O
d	O
;	O
x2i	O
=40	O
;	O
id1	O
;	O
id1	O
(	O
9.42	O
)	O
llllllllllllllllllllllllllllllllllllllll012345−0.50.00.51.01.52.0x1x2llllllllllllllllllll	O
148	O
survival	O
analysis	B
and	O
the	O
em	O
algorithm	B
are	O
unavailable	O
for	O
(	O
cid:22	O
)	O
2	O
,	O
(	O
cid:27	O
)	O
2	O
,	O
and	O
(	O
cid:26	O
)	O
because	O
of	O
the	O
missing	B
data	I
.	O
the	O
em	O
algorithm	B
begins	O
by	O
ﬁlling	O
in	O
the	O
missing	B
data	I
in	O
some	O
way	O
,	O
say	O
by	O
setting	O
x2i	O
d	O
0	O
for	O
the	O
20	O
missing	O
values	O
,	O
giving	O
an	O
artiﬁcially	O
complete	O
data	B
set	O
data.0/	O
.	O
then	O
it	O
proceeds	O
as	O
follows	O
.	O
(	O
cid:15	O
)	O
the	O
standard	O
method	O
(	O
9.42	O
)	O
is	O
applied	O
to	O
the	O
ﬁlled-in	O
data.0/	O
to	O
produce	O
o	O
1	O
;	O
o	O
(	O
cid:22	O
)	O
.0/	O
	O
.0/	O
d	O
.	O
o	O
(	O
cid:22	O
)	O
.0/	O
2	O
;	O
o	O
(	O
cid:26	O
)	O
.0//	O
;	O
this	O
is	O
the	O
m	O
(	O
“	O
maximizing	O
”	O
)	O
step.11	O
(	O
cid:15	O
)	O
each	O
of	O
the	O
missing	O
values	O
is	O
replaced	O
by	O
its	O
conditional	O
expectation	O
(	O
assuming	O
	O
d	O
o	O
	O
.0/	O
)	O
given	O
the	O
nonmissing	O
data	B
;	O
this	O
is	O
the	O
e	O
(	O
“	O
expecta-	O
tion	O
”	O
)	O
step	O
.	O
in	O
our	O
case	O
the	O
missing	O
values	O
x2i	O
are	O
replaced	O
by	O
1	O
;	O
o	O
(	O
cid:27	O
)	O
.0/	O
2	O
;	O
o	O
(	O
cid:27	O
)	O
.0/	O
(	O
cid:16	O
)	O
x1i	O
(	O
cid:0	O
)	O
o	O
(	O
cid:22	O
)	O
.0/	O
	O
o	O
(	O
cid:22	O
)	O
.0/	O
c	O
o	O
(	O
cid:26	O
)	O
.0/	O
o	O
(	O
cid:27	O
)	O
.0/	O
2o	O
(	O
cid:27	O
)	O
.0/	O
1	O
1	O
2	O
:	O
	O
.j	O
/c1	O
(	O
cid:0	O
)	O
o	O
	O
.j	O
/k	O
is	O
suitably	O
small	O
.	O
(	O
9.43	O
)	O
(	O
cid:15	O
)	O
the	O
e	O
and	O
m	O
steps	O
are	O
repeated	O
,	O
at	O
the	O
j	O
th	O
stage	O
giving	O
a	O
new	O
artiﬁcially	O
complete	O
data	B
set	O
data.j	O
/	O
and	O
an	O
updated	O
estimate	B
o	O
	O
.j	O
/	O
.	O
the	O
iteration	O
stops	O
when	O
k	O
o	O
table	O
9.8	O
shows	O
the	O
em	O
algorithm	B
at	O
work	O
on	O
the	O
bivariate	O
normal	B
ex-	O
ample	O
of	O
figure	O
9.3.	O
in	O
exponential	O
families	O
the	O
algorithm	B
is	O
guaranteed	O
to	O
converge	O
to	O
the	O
mle	O
o	O
	O
based	O
on	O
just	O
the	O
observed	O
data	B
o	O
;	O
moreover	O
,	O
the	O
likelihood	B
fo	O
.j	O
/	O
.o/	O
increases	O
with	O
every	O
step	O
j	O
.	O
(	O
the	O
convergence	O
can	O
be	O
sluggish	O
,	O
as	O
it	O
is	O
here	O
for	O
o	O
(	O
cid:27	O
)	O
2	O
and	O
o	O
(	O
cid:26	O
)	O
.	O
)	O
the	O
em	O
algorithm	B
ultimately	O
derives	O
from	O
the	O
fake-data	O
principle	O
,	O
a	O
property	O
of	O
maximum	B
likelihood	I
estimation	O
going	O
back	O
to	O
fisher	O
that	O
can	O
only	O
brieﬂy	O
be	O
summarized	O
here	O
.	O
	O
let	O
x	O
d	O
.o	O
;	O
u/	O
represent	O
the	O
“	O
complete	O
data	B
,	O
”	O
of	O
which	O
o	O
is	O
observed	O
while	O
u	O
is	O
unobserved	O
or	O
missing	O
.	O
write	O
the	O
density	B
for	O
x	O
as	O
f	O
.x/	O
d	O
f	O
.o/f	O
.ujo/	O
;	O
(	O
9.44	O
)	O
	O
.o/	O
be	O
the	O
mle	O
of	O
	O
based	O
just	O
on	O
o.	O
and	O
let	O
o	O
suppose	O
we	O
now	O
generate	O
simulations	O
of	O
u	O
by	O
sampling	O
from	O
the	O
condi-	O
tional	O
distribution	B
fo	O
.o/.ujo/	O
,	O
(	O
cid:3	O
)	O
k	O
(	O
cid:24	O
)	O
fo	O
.o/.ujo/	O
u	O
for	O
k	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
k	O
(	O
9.45	O
)	O
(	O
the	O
stars	O
indicating	O
creation	O
by	O
the	O
statistician	O
and	O
not	O
by	O
observation	O
)	O
,	O
giving	O
fake	O
complete-data	O
values	O
x	O
(	O
cid:3	O
)	O
d	O
fx	O
(	O
cid:3	O
)	O
k	O
d	O
.o	O
;	O
u	O
(	O
cid:3	O
)	O
2	O
;	O
:	O
:	O
:	O
;	O
x	O
(	O
cid:3	O
)	O
1	O
;	O
x	O
(	O
cid:3	O
)	O
k/	O
.	O
let	O
(	O
cid:3	O
)	O
kg	O
;	O
(	O
9.46	O
)	O
11	O
in	O
this	O
example	O
,	O
o	O
(	O
cid:22	O
)	O
.0/	O
1	O
data	B
and	O
o	O
(	O
cid:27	O
)	O
.0/	O
1	O
and	O
,	O
as	O
in	O
table	O
9.8	O
,	O
stay	O
the	O
same	O
in	O
subsequent	O
steps	O
of	O
the	O
algorithm	B
.	O
are	O
available	O
as	O
the	O
complete-data	O
estimates	O
in	O
(	O
9.42	O
)	O
,	O
10	O
9.5	O
missing	B
data	I
and	O
the	O
em	O
algorithm	B
149	O
table	O
9.8	O
em	O
algorithm	B
for	O
estimating	O
means	O
,	O
standard	B
deviations	I
,	O
and	O
the	O
correlation	O
of	O
the	O
bivariate	O
normal	B
distribution	O
that	O
gave	O
the	O
data	B
in	O
figure	O
9.3.	O
step	O
(	O
cid:22	O
)	O
1	O
(	O
cid:22	O
)	O
2	O
(	O
cid:27	O
)	O
1	O
(	O
cid:27	O
)	O
2	O
(	O
cid:26	O
)	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
11	O
12	O
13	O
14	O
15	O
16	O
17	O
18	O
19	O
20	O
1.86	O
1.86	O
1.86	O
1.86	O
1.86	O
1.86	O
1.86	O
1.86	O
1.86	O
1.86	O
1.86	O
1.86	O
1.86	O
1.86	O
1.86	O
1.86	O
1.86	O
1.86	O
1.86	O
1.86	O
.463	O
.707	O
.843	O
.923	O
.971	O
1.002	O
1.023	O
1.036	O
1.045	O
1.051	O
1.055	O
1.058	O
1.060	O
1.061	O
1.062	O
1.063	O
1.064	O
1.064	O
1.064	O
1.064	O
1.08	O
1.08	O
1.08	O
1.08	O
1.08	O
1.08	O
1.08	O
1.08	O
1.08	O
1.08	O
1.08	O
1.08	O
1.08	O
1.08	O
1.08	O
1.08	O
1.08	O
1.08	O
1.08	O
1.08	O
.738	O
.622	O
.611	O
.636	O
.667	O
.694	O
.716	O
.731	O
.743	O
.751	O
.756	O
.760	O
.763	O
.765	O
.766	O
.767	O
.768	O
.768	O
.769	O
.769	O
.162	O
.394	O
.574	O
.679	O
.736	O
.769	O
.789	O
.801	O
.808	O
.813	O
.816	O
.819	O
.820	O
.821	O
.822	O
.822	O
.823	O
.823	O
.823	O
.823	O
	O
	O
1	O
f	O
.x	O
(	O
cid:3	O
)	O
goes	O
to	O
o	O
(	O
cid:3	O
)	O
k/	O
yields	O
mle	O
o	O
whose	O
notional	O
likelihoodqk	O
(	O
cid:3	O
)	O
.	O
it	O
then	O
turns	O
out	O
that	O
o	O
	O
.o/	O
as	O
k	O
goes	O
to	O
inﬁnity	O
.	O
in	O
other	O
words	O
,	O
maximum	O
likeli-	O
hood	O
estimation	B
is	O
self-consistent	O
:	O
generating	O
artiﬁcial	O
data	B
from	O
the	O
mle	O
density	B
fo	O
.o/.ujo/	O
doesn	O
’	O
t	B
change	O
the	O
mle	O
.	O
moreover	O
,	O
any	O
value	O
o	O
	O
.0/	O
not	O
equal	O
to	O
the	O
mle	O
o	O
	O
.o/	O
can	O
not	O
be	O
self-consistent	O
:	O
carrying	O
through	O
(	O
9.45	O
)	O
–	O
(	O
9.46	O
)	O
using	O
fo	O
.0/	O
.ujo/	O
leads	O
to	O
hypothetical	O
mle	O
o	O
	O
.1/	O
having	O
fo	O
.1/	O
.o/	O
>	O
fo	O
.0/	O
.o/	O
,	O
etc.	O
,	O
a	O
more	O
general	O
version	O
of	O
the	O
em	O
algorithm.12	O
modern	O
technology	O
allows	O
social	O
scientists	O
to	O
collect	O
huge	O
data	B
sets	O
,	O
perhaps	O
hundreds	O
of	O
responses	O
for	O
each	O
of	O
thousands	O
or	O
even	O
millions	O
of	O
individuals	O
.	O
inevitably	O
,	O
some	O
entries	O
of	O
the	O
individual	O
responses	O
will	O
be	O
missing	O
.	O
imputation	O
amounts	O
to	O
employing	O
some	O
version	O
of	O
the	O
fake-data	O
principle	O
to	O
ﬁll	O
in	O
the	O
missing	O
values	O
.	O
imputation	O
’	O
s	O
goal	O
goes	O
beyond	O
ﬁnd-	O
be	O
replaced	O
by	O
.o	O
;	O
e	O
.j	O
/.ujo//	O
,	O
with	O
e	O
.j	O
/	O
indicating	O
expectation	O
with	O
respect	O
to	O
o	O
.j	O
/	O
,	O
(	O
cid:3	O
)	O
12	O
simulation	O
(	O
9.45	O
)	O
is	O
unnecessary	O
in	O
exponential	O
families	O
,	O
where	O
at	O
each	O
stage	O
data	B
can	O
as	O
in	O
(	O
9.43	O
)	O
.	O
150	O
survival	O
analysis	B
and	O
the	O
em	O
algorithm	B
11	O
ing	O
the	O
mle	O
,	O
to	O
the	O
creation	O
of	O
graphs	O
,	O
conﬁdence	B
intervals	I
,	O
histograms	O
,	O
and	O
more	O
,	O
using	O
only	O
convenient	O
,	O
standard	O
complete-data	O
methods	O
.	O
finally	O
,	O
returning	O
to	O
survival	O
analysis	B
,	O
the	O
kaplan–meier	O
estimate	B
(	O
9.17	O
)	O
is	O
itself	O
self-consistent	O
.	O
	O
consider	O
the	O
arm	O
a	O
censored	O
observation	O
74c	O
in	O
table	O
9.2.	O
we	O
know	O
that	O
that	O
patient	O
’	O
s	O
survival	O
time	O
exceeded	O
74.	O
sup-	O
pose	O
we	O
distribute	O
his	O
probability	O
mass	O
(	O
1=51	O
of	O
the	O
arm	O
a	O
sample	B
)	O
to	O
the	O
right	O
,	O
in	O
accordance	O
with	O
the	O
conditional	O
distribution	B
for	O
x	O
>	O
74	O
deﬁned	O
by	O
the	O
arm	O
a	O
kaplan–meier	O
survival	O
curve	O
.	O
it	O
turns	O
out	O
that	O
redistributing	O
all	O
the	O
censored	O
cases	O
does	O
not	O
change	O
the	O
original	O
kaplan–meier	O
survival	O
curve	O
;	O
kaplan–meier	O
is	O
self-consistent	O
,	O
leading	O
to	O
its	O
identiﬁcation	O
as	O
the	O
“	O
nonparametric	B
mle	O
”	O
of	O
a	O
survival	O
function	B
.	O
9.6	O
notes	O
and	O
details	O
the	O
progression	O
from	O
life	O
tables	O
,	O
kaplan–meier	O
curves	O
,	O
and	O
the	O
log-rank	O
test	O
to	O
proportional	O
hazards	O
regression	B
was	O
modest	O
in	O
its	O
computational	O
demands	O
,	O
until	O
the	O
ﬁnal	O
step	O
.	O
kaplan–meier	O
curves	O
lie	O
within	O
the	O
capabil-	O
ities	O
of	O
mechanical	O
calculators	O
.	O
not	O
so	O
for	O
proportional	O
hazards	O
,	O
which	O
is	O
emphatically	O
a	O
child	O
of	O
the	O
computer	O
age	O
.	O
as	O
the	O
algorithms	O
grew	O
more	O
in-	O
tricate	O
,	O
their	O
inferential	O
justiﬁcation	O
deepened	O
in	O
scope	O
and	O
sophistication	O
.	O
this	O
is	O
a	O
pattern	O
we	O
also	O
saw	O
in	O
chapter	O
8	O
,	O
in	O
the	O
progression	O
from	O
bioassay	O
to	O
logistic	B
regression	I
to	O
generalized	O
linear	B
models	O
,	O
and	O
will	O
reappear	O
as	O
we	O
move	O
from	O
the	O
jackknife	O
to	O
the	O
bootstrap	O
in	O
chapter	O
10.	O
censoring	O
is	O
not	O
the	O
same	O
as	O
truncation	O
.	O
for	O
the	O
truncated	O
galaxy	B
data	O
of	O
section	O
8.3	O
,	O
we	O
learn	O
of	O
the	O
existence	O
of	O
a	O
galaxy	B
only	O
if	O
it	O
falls	O
into	O
the	O
observation	O
region	B
(	O
8.38	O
)	O
.	O
the	O
censored	O
individuals	O
in	O
table	O
9.2	O
are	O
known	O
to	O
exist	O
,	O
but	O
with	O
imperfect	O
knowledge	O
of	O
their	O
lifetimes	O
.	O
there	O
is	O
a	O
version	O
of	O
the	O
kaplan–meier	O
curve	O
applying	O
to	O
truncated	O
data	B
,	O
which	O
was	O
developed	O
in	O
the	O
astronomy	O
literature	O
by	O
lynden-bell	O
(	O
1971	O
)	O
.	O
the	O
methods	O
of	O
this	O
chapter	O
apply	O
to	O
data	B
that	O
is	O
left-truncated	O
as	O
well	O
as	O
right-censored	O
.	O
in	O
a	O
survival	O
time	O
study	O
of	O
a	O
new	O
hiv	O
drug	O
,	O
for	O
instance	O
,	O
subject	O
i	O
might	O
not	O
enter	O
the	O
study	O
until	O
some	O
time	O
(	O
cid:28	O
)	O
i	O
after	O
his	O
or	O
her	O
initial	O
diagnosis	O
,	O
in	O
which	O
case	O
ti	O
would	O
be	O
left-truncated	O
at	O
(	O
cid:28	O
)	O
i	O
,	O
as	O
well	O
as	O
possibly	O
later	O
right-censored	O
.	O
this	O
only	O
modiﬁes	O
the	O
composition	O
of	O
the	O
various	O
risk	O
sets	O
.	O
however	O
,	O
other	O
missing-data	O
situations	O
,	O
e.g.	O
,	O
left-	O
and	O
right-censoring	O
,	O
require	O
more	O
elaborate	O
,	O
less	O
elegant	O
,	O
treatments	O
.	O
1	O
[	O
p.	O
133	O
]	O
formula	B
(	O
9.10	O
)	O
.	O
let	O
the	O
interval	B
œt0	O
;	O
t1	O
be	O
partitioned	O
into	O
a	O
large	O
number	O
of	O
subintervals	O
of	O
length	O
dt	O
,	O
with	O
tk	O
the	O
midpoint	O
of	O
subinterval	O
k.	O
9.6	O
notes	O
and	O
details	O
as	O
in	O
(	O
9.4	O
)	O
,	O
using	O
(	O
9.9	O
)	O
,	O
prft	O
(	O
cid:21	O
)	O
t1jt	O
(	O
cid:21	O
)	O
t0g	O
:	O
dy	O
.1	O
(	O
cid:0	O
)	O
h.ti	O
/	O
dt	O
/	O
o	O
nx	O
log.1	O
(	O
cid:0	O
)	O
h.ti	O
/	O
dt	O
/	O
o	O
n	O
(	O
cid:0	O
)	O
x	O
h.ti	O
/	O
dt	O
;	O
d	O
exp	O
:	O
d	O
exp	O
151	O
(	O
9.47	O
)	O
which	O
,	O
as	O
dt	O
!	O
0	O
,	O
goes	O
to	O
(	O
9.10	O
)	O
.	O
2	O
[	O
p.	O
136	O
]	O
kaplan–meier	O
estimate	B
.	O
in	O
the	O
life	O
table	O
formula	B
(	O
9.6	O
)	O
(	O
with	O
k	O
d	O
1	O
)	O
,	O
let	O
the	O
time	O
unit	O
be	O
small	O
enough	O
to	O
make	O
each	O
bin	O
contain	O
at	O
most	O
one	O
value	O
t.k/	O
(	O
9.16	O
)	O
.	O
then	O
at	O
t.k/	O
,	O
o	O
h.k/	O
d	O
d.k/	O
(	O
9.48	O
)	O
;	O
n	O
(	O
cid:0	O
)	O
k	O
c	O
1	O
giving	O
expression	O
(	O
9.17	O
)	O
.	O
3	O
[	O
p.	O
137	O
]	O
greenwood	O
’	O
s	O
formula	B
(	O
9.18	O
)	O
.	O
in	O
the	O
life	O
table	O
formulation	O
of	O
sec-	O
tion	O
9.1	O
,	O
(	O
9.6	O
)	O
gives	O
from	O
nk	O
o	O
hk	O
ind	O
(	O
cid:24	O
)	O
bi.nk	O
;	O
hk/	O
we	O
get	O
n	O
n	O
log	O
osj	O
var	O
var	O
log	O
log	O
osj	O
d	O
jx	O
(	O
cid:16	O
)	O
1	O
(	O
cid:0	O
)	O
o	O
log	O
1	O
:	O
hk	O
(	O
cid:16	O
)	O
	O
1	O
(	O
cid:0	O
)	O
o	O
o	O
:	O
d	O
jx	O
hk	O
1	O
hk	O
1	O
(	O
cid:0	O
)	O
hk	O
1	O
nk	O
;	O
o	O
d	O
jx	O
d	O
jx	O
1	O
1	O
(	O
9.49	O
)	O
(	O
9.50	O
)	O
var	O
o	O
.1	O
(	O
cid:0	O
)	O
hk/2	O
hk	O
n	O
log	O
osj	O
o	O
:	O
d	O
jx	O
where	O
we	O
have	O
used	O
the	O
delta-method	O
approximation	O
varflog	O
xg	O
:	O
d	O
varfxg=	O
efxg2	O
.	O
plugging	O
in	O
hk	O
d	O
yk=nk	O
yields	O
yk	O
1	O
:	O
var	O
nk.nk	O
(	O
cid:0	O
)	O
yk/	O
(	O
9.51	O
)	O
then	O
the	O
inverse	O
approximation	O
varfxg	O
d	O
efxg2	O
varflog	O
xg	O
gives	O
green-	O
wood	O
’	O
s	O
formula	B
(	O
9.18	O
)	O
.	O
the	O
censored	O
data	B
situation	O
of	O
section	O
9.2	O
does	O
not	O
enjoy	O
independence	O
between	O
the	O
o	O
hk	O
values	O
.	O
however	O
,	O
successive	O
conditional	O
independence	O
,	O
given	O
the	O
nk	O
values	O
,	O
is	O
enough	O
to	O
verify	O
the	O
result	O
,	O
as	O
in	O
the	O
partial	O
likelihood	B
cal-	O
culations	O
below	O
.	O
note	O
:	O
the	O
conﬁdence	B
intervals	I
in	O
figure	O
9.1	O
were	O
obtained	O
152	O
survival	O
analysis	B
and	O
the	O
em	O
algorithm	B
by	O
exponentiating	O
the	O
intervals	B
,	O
log	O
osj	O
˙	O
1:96	O
h	B
n	O
log	O
osj	O
var	O
oi1=2	O
:	O
(	O
9.52	O
)	O
4	O
[	O
p.	O
138	O
]	O
parametric	B
life	O
tables	O
analysis	B
.	O
figure	O
9.2	O
and	O
the	O
analysis	B
behind	O
it	O
is	O
developed	O
in	O
efron	O
(	O
1988	O
)	O
,	O
where	O
it	O
is	O
called	O
“	O
partial	O
logistic	O
regres-	O
sion	O
”	O
in	O
analogy	O
with	O
partial	O
likelihood	B
.	O
5	O
[	O
p.	O
139	O
]	O
the	O
log-rank	O
test	O
.	O
this	O
chapter	O
featured	O
an	O
all-star	O
cast	O
,	O
includ-	O
ing	O
four	O
of	O
the	O
most	O
referenced	O
papers	O
of	O
the	O
post-war	O
era	O
:	O
kaplan	O
and	O
meier	O
(	O
1958	O
)	O
,	O
cox	O
(	O
1972	O
)	O
on	O
proportional	O
hazards	O
,	O
dempster	O
et	O
al	O
.	O
(	O
1977	O
)	O
codifying	O
and	O
naming	O
the	O
em	O
algorithm	B
,	O
and	O
mantel	O
and	O
haenszel	O
(	O
1959	O
)	O
on	O
the	O
log-rank	O
test	O
.	O
(	O
cox	O
(	O
1958	O
)	O
gives	O
a	O
careful	O
,	O
and	O
early	O
,	O
analysis	B
of	O
the	O
mantel–haenszel	O
idea	O
.	O
)	O
the	O
not	O
very	O
helpful	O
name	O
“	O
log-rank	O
”	O
does	O
at	O
least	O
remind	O
us	O
that	O
the	O
test	O
depends	O
only	O
on	O
the	O
ranks	O
of	O
the	O
survival	O
times	O
,	O
and	O
will	O
give	O
the	O
same	O
result	O
if	O
all	O
the	O
observed	O
survival	O
times	O
ti	O
are	O
monotonically	O
transformed	O
,	O
say	O
to	O
exp.ti	O
/	O
or	O
t	B
1=2	O
.	O
it	O
is	O
often	O
referred	O
to	O
as	O
the	O
mantel–haenszel	O
or	O
cochran–mantel–haenszel	O
test	O
in	O
older	O
literature	O
.	O
kaplan–meier	O
and	O
proportional	O
hazards	O
are	O
also	O
rank-based	O
procedures	O
.	O
i	O
6	O
[	O
p.	O
141	O
]	O
hypergeometric	O
distribution	B
.	O
hypergeometric	O
calculations	O
,	O
as	O
for	O
table	O
9.5	O
,	O
are	O
often	O
stated	O
as	O
follows	O
:	O
n	O
marbles	O
are	O
placed	O
in	O
an	O
urn	O
,	O
na	O
labeled	O
a	O
and	O
nb	O
labeled	O
b	O
;	O
nd	O
marbles	O
are	O
drawn	O
out	O
at	O
random	O
;	O
y	O
is	O
the	O
number	O
of	O
these	O
labeled	O
a.	O
elementary	O
(	O
but	O
not	O
simple	O
)	O
calculations	O
then	O
produce	O
the	O
conditional	O
distribution	B
of	O
y	O
given	O
the	O
table	O
’	O
s	O
marginals	O
na	O
;	O
nb	O
;	O
n	O
;	O
nd	O
;	O
and	O
ns	O
,	O
!	O
!	O
(	O
cid:30	O
)	O
!	O
n	O
nd	O
prfyjmarginalsg	O
d	O
na	O
y	O
nb	O
nd	O
(	O
cid:0	O
)	O
y	O
(	O
9.53	O
)	O
for	O
max.na	O
(	O
cid:0	O
)	O
ns	O
;	O
0/	O
	O
y	O
	O
min.nd	O
;	O
na/	O
;	O
and	O
expressions	O
(	O
9.24	O
)	O
for	O
the	O
mean	O
and	O
variance	O
.	O
if	O
na	O
and	O
nb	O
go	O
to	O
inﬁn-	O
ity	O
such	O
that	O
na=n	O
!	O
pa	O
and	O
nb	O
=n	O
!	O
1	O
(	O
cid:0	O
)	O
pa	O
,	O
then	O
v	O
!	O
nd	O
pa.1	O
(	O
cid:0	O
)	O
pa/	O
,	O
the	O
variance	O
of	O
y	O
(	O
cid:24	O
)	O
bi.nd	O
;	O
pa/	O
.	O
7	O
[	O
p.	O
141	O
]	O
log-rank	O
statistic	B
z	O
(	O
9.25	O
)	O
.	O
why	O
is	O
.pn	O
nominator	O
for	O
z	O
?	O
let	O
ui	O
d	O
yi	O
(	O
cid:0	O
)	O
ei	O
in	O
(	O
9.30	O
)	O
,	O
so	O
z	O
’	O
s	O
numerator	O
ispn	O
1	O
vi	O
/1=2	O
the	O
correct	O
de-	O
1	O
ui	O
,	O
with	O
uijdi	O
(	O
cid:24	O
)	O
.0	O
;	O
vi	O
/	O
(	O
9.54	O
)	O
under	O
the	O
null	O
hypothesis	O
of	O
equal	O
hazard	O
rates	O
.	O
this	O
implies	O
that	O
,	O
uncon-	O
ditionally	O
,	O
efuig	O
d	O
0.	O
for	O
j	O
<	O
i	O
,	O
uj	O
is	O
a	O
function	B
of	O
di	O
(	O
since	O
yj	O
and	O
!	O
d	O
nx	O
u2	O
i	O
varfuig	O
nx	O
1	O
e	O
ui	O
!	O
2	O
d	O
e	O
nx	O
:	O
d	O
nx	O
1	O
vi	O
:	O
1	O
1	O
(	O
9.55	O
)	O
153	O
ej	O
are	O
)	O
,	O
so	O
efuj	O
uijdig	O
d	O
0	O
,	O
and	O
,	O
again	O
unconditionally	O
,	O
efuj	O
uig	O
d	O
0.	O
therefore	O
,	O
assuming	O
equal	O
hazard	O
rates	O
,	O
9.6	O
notes	O
and	O
details	O
the	O
last	O
approximation	O
,	O
replacing	O
unconditional	O
variances	O
varfuig	O
with	O
conditional	O
variances	O
vi	O
,	O
is	O
justiﬁed	O
in	O
crowley	O
(	O
1974	O
)	O
,	O
as	O
is	O
the	O
asymp-	O
totic	O
normality	O
(	O
9.29	O
)	O
.	O
the	O
inﬁnitesimal	O
interval	B
.t.j	O
/	O
;	O
t.j	O
/	O
c	O
d	O
t	B
/	O
is	O
hi	O
.t.j	O
//	O
d	O
t	B
,	O
so	O
rj	O
,	O
the	O
probability	O
pi	O
that	O
death	O
occurs	O
in	O
8	O
[	O
p.	O
145	O
]	O
lemma	O
(	O
9.37	O
)	O
.	O
for	O
i	O
2	O
0	O
i	O
ˇ	O
d	O
t	B
;	O
(	O
9.56	O
)	O
pi	O
d	O
h0.t.j	O
//ec	O
y	O
pi	O
d	O
pi	O
.1	O
(	O
cid:0	O
)	O
pk/	O
:	O
and	O
the	O
probability	O
of	O
event	O
ai	O
that	O
individual	O
i	O
dies	O
while	O
the	O
others	O
don	O
’	O
t	B
is	O
(	O
9.57	O
)	O
but	O
the	O
ai	O
are	O
disjoint	O
events	O
,	O
so	O
,	O
given	O
that	O
[	O
ai	O
has	O
occurred	O
,	O
the	O
proba-	O
bility	O
that	O
it	O
is	O
individual	O
i	O
who	O
died	O
is	O
:	O
d	O
eci	O
ˇ	O
(	O
cid:30	O
)	O
x	O
(	O
cid:30	O
)	O
x	O
k2	O
rj	O
(	O
cid:0	O
)	O
i	O
(	O
9.58	O
)	O
eck	O
ˇ	O
;	O
pj	O
pi	O
this	O
becoming	O
exactly	O
(	O
9.37	O
)	O
as	O
d	O
t	B
!	O
0.	O
rj	O
k2	O
rj	O
9	O
[	O
p.	O
145	O
]	O
partial	O
likelihood	B
(	O
9.40	O
)	O
.	O
cox	O
(	O
1975	O
)	O
introduced	O
partial	O
likelihood	B
as	O
inferential	O
justiﬁcation	O
for	O
the	O
proportional	O
hazards	O
model	B
,	O
which	O
had	O
been	O
questioned	O
in	O
the	O
literature	O
.	O
let	O
dj	O
indicate	O
all	O
the	O
observable	O
infor-	O
mation	O
available	O
just	O
before	O
time	O
t.j	O
/	O
(	O
9.35	O
)	O
,	O
including	O
all	O
the	O
death	O
or	O
loss	O
times	O
for	O
individuals	O
having	O
ti	O
<	O
t.j	O
/	O
.	O
(	O
notice	O
that	O
dj	O
determines	O
the	O
risk	O
set	B
rj	O
.	O
)	O
by	O
successive	O
conditioning	O
we	O
write	O
the	O
full	B
likelihood	O
f	O
.data/	O
as	O
f	O
.data/	O
d	O
f	O
.d1/f	O
.i1j	O
d	O
jy	O
jd1	O
r1/f	O
.d2jd1/f	O
.i2j	O
jy	O
rj	O
/	O
:	O
f	O
.ijj	O
jd1	O
f	O
.djjdj	O
(	O
cid:0	O
)	O
1/	O
r2/	O
:	O
:	O
:	O
(	O
9.59	O
)	O
letting	O
	O
d	O
.˛	O
;	O
ˇ/	O
,	O
where	O
˛	O
is	O
a	O
nuisance	O
parameter	O
vector	B
having	O
to	O
do	O
154	O
survival	O
analysis	B
and	O
the	O
em	O
algorithm	B
with	O
the	O
occurrence	O
and	O
timing	O
of	O
events	O
between	O
observed	O
deaths	O
,	O
24	O
jy	O
jd1	O
35	O
l.ˇ/	O
;	O
f˛	O
;	O
ˇ	O
.data/	O
d	O
f˛	O
;	O
ˇ	O
.djjdj	O
(	O
cid:0	O
)	O
1/	O
(	O
9.60	O
)	O
where	O
l.ˇ/	O
is	O
the	O
partial	O
likelihood	B
(	O
9.38	O
)	O
.	O
the	O
proportional	O
hazards	O
model	B
simply	O
ignores	O
the	O
bracketed	O
factor	B
in	O
(	O
9.60	O
)	O
;	O
l.ˇ/	O
d	O
log	O
l.ˇ/	O
is	O
treated	O
as	O
a	O
genuine	O
likelihood	B
,	O
maximized	O
to	O
give	O
o	O
ˇ	O
,	O
and	O
assigned	O
covariance	O
matrix	B
.	O
(	O
cid:0	O
)	O
r	O
(	O
cid:0	O
)	O
1	O
as	O
in	O
section	O
4.3.	O
efron	O
(	O
1977	O
)	O
shows	O
this	O
tactic	O
is	O
highly	O
efﬁcient	O
for	O
the	O
estimation	B
of	O
ˇ.	O
o	O
ˇ//	O
l.	O
10	O
[	O
p.	O
148	O
]	O
fake-data	O
principle	O
.	O
for	O
any	O
two	O
values	O
of	O
the	O
parameters	O
1	O
and	O
2	O
deﬁne	O
l1	O
.2/	O
dz	O
œlog	O
f2	O
.o	O
;	O
u/	O
f1	O
.ujo/	O
d	O
u	O
;	O
this	O
being	O
the	O
limit	O
as	O
k	O
!	O
1	O
of	O
l1	O
.2/	O
d	O
lim	O
k	O
!	O
1	O
log	O
f2	O
.o	O
;	O
u	O
(	O
cid:3	O
)	O
k/	O
;	O
(	O
9.61	O
)	O
(	O
9.62	O
)	O
1	O
k	O
kd1	O
kx	O
cz	O
	O
(	O
cid:0	O
)	O
1	O
2	O
f1	O
.o/	O
the	O
fake-data	O
log	O
likelihood	B
(	O
9.46	O
)	O
under	O
2	O
,	O
if	O
1	O
were	O
the	O
true	O
value	O
of	O
	O
.	O
using	O
f	O
.o	O
;	O
u/	O
d	O
f	O
.o/f	O
.ujo/	O
,	O
deﬁnition	O
(	O
9.61	O
)	O
gives	O
l1	O
.2/	O
(	O
cid:0	O
)	O
l1	O
.1/	O
d	O
log	O
f1	O
.ujo/	O
d	O
log	O
f1	O
.ujo/	O
log	O
d	O
.f1	O
.ujo/	O
;	O
f2	O
.ujo//	O
;	O
	O
f2	O
.o/	O
	O
f2	O
.o/	O
	O
f2	O
.ujo/	O
(	O
9.63	O
)	O
	O
f1	O
.o/	O
with	O
d	O
the	O
deviance	O
(	O
8.31	O
)	O
,	O
which	O
is	O
always	O
positive	O
unless	O
ujo	O
has	O
the	O
same	O
distribution	B
under	O
1	O
and	O
2	O
,	O
which	O
we	O
will	O
assume	O
doesn	O
’	O
t	B
happen	O
.	O
suppose	O
we	O
begin	O
the	O
em	O
algorithm	B
at	O
	O
d	O
1	O
and	O
ﬁnd	O
the	O
value	O
2	O
maximizing	O
l1	O
.	O
/	O
.	O
then	O
l1	O
.2/	O
>	O
l1	O
.1/	O
and	O
d	O
>	O
0	O
implies	O
f2	O
.o/	O
>	O
f1	O
.o/	O
in	O
(	O
9.63	O
)	O
;	O
that	O
is	O
,	O
we	O
have	O
increased	O
the	O
likelihood	B
of	O
the	O
observed	O
data	B
.	O
now	O
take	O
1	O
d	O
o	O
	O
d	O
arg	O
max	O
f	O
.o/	O
.	O
then	O
the	O
right	O
side	O
of	O
(	O
9.63	O
)	O
is	O
	O
/	O
>	O
lo	O
.2/	O
for	O
any	O
2	O
not	O
equaling	O
1	O
d	O
o	O
o	O
negative	O
,	O
implying	O
lo	O
.	O
	O
.	O
putting	O
this	O
together,13	O
successively	O
computing	O
1	O
;	O
2	O
;	O
3	O
;	O
:	O
:	O
:	O
by	O
fake-data	O
mle	O
calculations	O
increases	O
f	O
.o/	O
at	O
every	O
step	O
,	O
and	O
the	O
only	O
stable	O
point	O
of	O
the	O
algorithm	B
is	O
at	O
	O
d	O
o	O
	O
.o/	O
.	O
11	O
[	O
p.	O
150	O
]	O
kaplan–meier	O
self-consistency	O
.	O
this	O
property	O
was	O
veriﬁed	O
in	O
efron	O
(	O
1967	O
)	O
,	O
where	O
the	O
name	O
was	O
coined	O
.	O
13	O
generating	O
the	O
fake	O
data	B
is	O
equivalent	O
to	O
the	O
e	O
step	O
of	O
the	O
algorithm	B
,	O
the	O
m	O
step	O
being	O
the	O
maximization	O
of	O
lj	O
.	O
/	O
.	O
10	O
the	O
jackknife	O
and	O
the	O
bootstrap	O
a	O
central	O
element	O
of	O
frequentist	O
inference	B
is	O
the	O
standard	B
error	I
.	O
an	O
algo-	O
rithm	O
has	O
produced	O
an	O
estimate	O
of	O
a	O
parameter	O
of	O
interest	O
,	O
for	O
instance	O
the	O
mean	O
nx	O
d	O
0:752	O
for	O
the	O
47	O
all	O
scores	O
in	O
the	O
top	O
panel	O
of	O
figure	O
1.4.	O
how	O
accurate	O
is	O
the	O
estimate	B
?	O
in	O
this	O
case	O
,	O
formula	B
(	O
1.2	O
)	O
for	O
the	O
standard	O
deviation1	O
of	O
a	O
sample	B
mean	O
gives	O
estimated	O
standard	B
error	I
(	O
10.1	O
)	O
so	O
one	O
can	O
’	O
t	B
take	O
the	O
third	O
digit	O
of	O
nx	O
d	O
0:752	O
very	O
seriously	O
,	O
and	O
even	O
the	O
5	O
is	O
dubious	O
.	O
bse	O
d	O
0:040	O
;	O
direct	O
standard	B
error	I
formulas	O
like	O
(	O
1.2	O
)	O
exist	O
for	O
various	O
forms	O
of	O
aver-	O
aging	O
,	O
such	O
as	O
linear	B
regression	O
(	O
7.34	O
)	O
,	O
and	O
for	O
hardly	O
anything	O
else	O
.	O
tay-	O
lor	O
series	O
approximations	O
(	O
“	O
device	O
2	O
”	O
of	O
section	O
2.1	O
)	O
extend	O
the	O
formulas	O
to	O
smooth	O
functions	O
of	O
averages	O
,	O
as	O
in	O
(	O
8.30	O
)	O
.	O
before	O
computers	O
,	O
applied	O
statisticians	O
needed	O
to	O
be	O
taylor	O
series	O
experts	O
in	O
laboriously	O
pursuing	O
the	O
accuracy	O
of	O
even	O
moderately	O
complicated	O
statistics	B
.	O
the	O
jackknife	O
(	O
1957	O
)	O
was	O
a	O
ﬁrst	O
step	O
toward	O
a	O
computation-based	O
,	O
non-	O
formulaic	O
approach	O
to	O
standard	O
errors	O
.	O
the	O
bootstrap	O
(	O
1979	O
)	O
went	O
further	O
toward	O
automating	O
a	O
wide	O
variety	O
of	O
inferential	O
calculations	O
,	O
including	O
stan-	O
dard	O
errors	B
.	O
besides	O
sparing	O
statisticians	O
the	O
exhaustion	O
of	O
tedious	O
routine	O
calculations	O
the	O
jackknife	O
and	O
bootstrap	O
opened	O
the	O
door	O
for	O
more	O
com-	O
plicated	O
estimation	B
algorithms	O
,	O
which	O
could	O
be	O
pursued	O
with	O
the	O
assurance	O
that	O
their	O
accuracy	O
would	O
be	O
easily	O
assessed	O
.	O
this	O
chapter	O
focuses	O
on	O
stan-	O
dard	O
errors	B
,	O
with	O
more	O
adventurous	O
bootstrap	O
ideas	O
deferred	O
to	O
chapter	O
11.	O
we	O
end	O
with	O
a	O
brief	O
discussion	O
of	O
accuracy	O
estimation	O
for	O
robust	O
statistics	B
.	O
1	O
we	O
will	O
use	O
the	O
terms	O
“	O
standard	B
error	I
”	O
and	O
“	O
standard	B
deviation	I
”	O
interchangeably	O
.	O
155	O
156	O
the	O
jackknife	O
and	O
the	O
bootstrap	O
10.1	O
the	O
jackknife	O
estimate	O
of	O
standard	B
error	I
the	O
basic	O
applications	O
of	O
the	O
jackknife	O
apply	O
to	O
one-sample	O
problems	O
,	O
where	O
the	O
statistician	O
has	O
observed	O
an	O
independent	O
and	O
identically	O
distributed	O
(	O
iid	O
)	O
sample	B
x	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn/	O
0	O
from	O
an	O
unknown	O
probability	O
distribution	B
f	O
on	O
some	O
space	B
x	O
,	O
(	O
10.2	O
)	O
x	O
can	O
be	O
anything	O
:	O
the	O
real	O
line	O
,	O
the	O
plane	O
,	O
a	O
function	B
space.2	O
a	O
real-valued	O
statistic	B
o	O
	O
has	O
been	O
computed	O
by	O
applying	O
some	O
algorithm	B
s.	O
(	O
cid:1	O
)	O
/	O
to	O
x	O
,	O
xi	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
:	O
iid	O
(	O
cid:24	O
)	O
f	O
o	O
	O
d	O
s.x/	O
;	O
and	O
we	O
wish	O
to	O
assign	O
a	O
standard	B
error	I
to	O
o	O
the	O
standard	B
deviation	I
of	O
o	O
let	O
x.i	O
/	O
be	O
the	O
sample	B
with	O
xi	O
removed	O
,	O
	O
d	O
s.x/	O
under	O
sampling	O
model	O
(	O
10.2	O
)	O
.	O
	O
.	O
that	O
is	O
,	O
we	O
wish	O
to	O
estimate	B
x.i	O
/	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xi	O
(	O
cid:0	O
)	O
1	O
;	O
xic1	O
;	O
:	O
:	O
:	O
;	O
xn/	O
0	O
;	O
(	O
10.4	O
)	O
(	O
10.3	O
)	O
(	O
10.5	O
)	O
and	O
denote	O
the	O
corresponding	O
value	O
of	O
the	O
statistic	B
of	O
interest	O
as	O
o	O
.i	O
/	O
d	O
s.x.i	O
//	O
:	O
#	O
1=2	O
2	O
''	O
n	O
1	O
n	O
(	O
cid:0	O
)	O
1	O
	O
.	O
(	O
cid:1	O
)	O
/	O
d	O
nx	O
then	O
the	O
jackknife	O
estimate	O
of	O
standard	B
error	I
for	O
o	O
(	O
cid:16	O
)	O
o	O
nx	O
	O
is	O
bsejack	O
d	O
;	O
with	O
o	O
.i	O
/	O
(	O
cid:0	O
)	O
o	O
	O
.	O
(	O
cid:1	O
)	O
/	O
in	O
the	O
case	O
where	O
o	O
	O
is	O
the	O
mean	O
nx	O
of	O
real	O
values	O
x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn	O
(	O
i.e.	O
,	O
x	O
is	O
an	O
interval	B
of	O
the	O
real	O
line	O
)	O
,	O
o	O
be	O
expressed	O
as	O
o	O
.i	O
/	O
d	O
.nnx	O
(	O
cid:0	O
)	O
xi	O
/=.n	O
(	O
cid:0	O
)	O
1/	O
:	O
	O
.	O
(	O
cid:1	O
)	O
/	O
d	O
nx	O
,	O
o	O
	O
.	O
(	O
cid:1	O
)	O
/	O
d	O
.nx	O
(	O
cid:0	O
)	O
xi	O
/=.n	O
(	O
cid:0	O
)	O
1/	O
,	O
and	O
.i	O
/	O
is	O
their	O
average	O
excluding	O
xi	O
,	O
which	O
can	O
o	O
.i	O
/=n	O
:	O
(	O
10.7	O
)	O
(	O
10.6	O
)	O
1	O
equation	B
(	O
10.7	O
)	O
gives	O
o	O
bsejack	O
d	O
''	O
nx	O
id1	O
.i	O
/	O
(	O
cid:0	O
)	O
o	O
.xi	O
(	O
cid:0	O
)	O
nx/2=	O
.n.n	O
(	O
cid:0	O
)	O
1//	O
#	O
1=2	O
fudge	O
factor	B
.n	O
(	O
cid:0	O
)	O
1/=n	O
in	O
deﬁnition	O
(	O
10.6	O
)	O
was	O
inserted	O
to	O
makebsejack	O
agree	O
exactly	O
the	O
same	O
as	O
the	O
classic	O
formula	B
(	O
1.2	O
)	O
.	O
this	O
is	O
no	O
coincidence	O
.	O
the	O
with	O
(	O
1.2	O
)	O
when	O
o	O
2	O
if	O
x	O
is	O
an	O
interval	B
of	O
the	O
real	O
line	O
we	O
might	O
take	O
f	O
to	O
be	O
the	O
usual	O
cumulative	O
	O
is	O
nx	O
.	O
distribution	B
function	O
,	O
but	O
here	O
we	O
will	O
just	O
think	O
of	O
f	O
as	O
any	O
full	B
description	O
of	O
the	O
probability	O
distribution	B
for	O
an	O
xi	O
on	O
x	O
.	O
;	O
(	O
10.8	O
)	O
157	O
10.1	O
the	O
jackknife	O
estimate	O
of	O
standard	B
error	I
the	O
advantage	O
ofbsejack	O
is	O
that	O
deﬁnition	O
(	O
10.6	O
)	O
can	O
be	O
applied	O
in	O
an	O
au-	O
tomatic	O
way	O
to	O
any	O
statistic	B
o	O
	O
d	O
s.x/	O
.	O
all	O
that	O
is	O
needed	O
is	O
an	O
algorithm	B
that	O
computes	O
s.	O
(	O
cid:1	O
)	O
/	O
for	O
the	O
deleted	O
data	B
sets	O
x.i	O
/	O
.	O
computer	O
power	O
is	O
being	O
substituted	O
for	O
theoretical	O
taylor	O
series	O
calculations	O
.	O
later	O
we	O
will	O
see	O
that	O
the	O
underlying	O
inferential	O
ideas—plug-in	O
estimation	B
of	O
frequentist	O
standard	O
errors—haven	O
’	O
t	B
changed	O
,	O
only	O
their	O
implementation	O
.	O
as	O
an	O
example	O
,	O
consider	O
the	O
kidney	B
function	I
data	O
set	B
of	O
section	O
1.1.	O
here	O
the	O
data	B
consists	O
of	O
n	O
d	O
157	O
points	O
.xi	O
;	O
yi	O
/	O
,	O
with	O
x	O
d	O
age	O
and	O
y	O
d	O
tot	O
in	O
figure	O
1.1	O
.	O
(	O
so	O
the	O
generic	O
xi	O
in	O
(	O
10.2	O
)	O
now	O
represents	O
the	O
pair	O
.xi	O
;	O
yi	O
/	O
,	O
and	O
f	O
describes	O
a	O
distribution	B
in	O
the	O
plane	O
.	O
)	O
suppose	O
we	O
are	O
interested	O
in	O
the	O
correlation	O
between	O
age	O
and	O
tot	O
,	O
estimated	O
by	O
the	O
usual	O
sample	B
correlation	O
o	O
	O
d	O
s.x/	O
,	O
,	O
''	O
nx	O
#	O
1=2	O
s.x/	O
d	O
nx	O
nx	O
.yi	O
(	O
cid:0	O
)	O
ny/2	O
.xi	O
(	O
cid:0	O
)	O
nx/2	O
applying	O
(	O
10.6	O
)	O
gavebsejack	O
d	O
0:058	O
for	O
the	O
accuracy	O
of	O
o	O
computed	O
to	O
be	O
o	O
	O
d	O
(	O
cid:0	O
)	O
0:572	O
for	O
the	O
kidney	O
data	O
.	O
case	O
,	O
bsetaylor	O
d	O
	O
.	O
nonpara-	O
metric	O
bootstrap	O
computations	B
,	O
section	O
10.2	O
,	O
also	O
gave	O
estimated	O
standard	B
error	I
0.058.	O
the	O
classic	O
taylor	O
series	O
formula	B
looks	O
quite	O
formidable	O
in	O
this	O
.xi	O
(	O
cid:0	O
)	O
nx/.yi	O
(	O
cid:0	O
)	O
ny/	O
(	O
o	O
;	O
(	O
10.9	O
)	O
c	O
2	O
o	O
(	O
cid:22	O
)	O
22	O
o	O
(	O
cid:22	O
)	O
20	O
o	O
(	O
cid:22	O
)	O
02	O
	O
o	O
(	O
cid:22	O
)	O
40o	O
(	O
cid:22	O
)	O
2	O
20	O
	O
2	O
4n	O
c	O
4	O
o	O
(	O
cid:22	O
)	O
22	O
o	O
(	O
cid:22	O
)	O
2	O
11	O
(	O
cid:0	O
)	O
4	O
o	O
(	O
cid:22	O
)	O
31	O
o	O
(	O
cid:22	O
)	O
11	O
o	O
(	O
cid:22	O
)	O
20	O
(	O
cid:0	O
)	O
4	O
o	O
(	O
cid:22	O
)	O
13	O
o	O
(	O
cid:22	O
)	O
11	O
o	O
(	O
cid:22	O
)	O
02	O
1	O
1	O
id1	O
02	O
c	O
o	O
(	O
cid:22	O
)	O
04o	O
(	O
cid:22	O
)	O
2	O
o	O
(	O
cid:22	O
)	O
hk	O
d	O
nx	O
(	O
cid:21	O
)	O
)	O
1=2	O
(	O
10.10	O
)	O
(	O
10.11	O
)	O
where	O
.xi	O
(	O
cid:0	O
)	O
nx/h.yi	O
(	O
cid:0	O
)	O
ny/k=n	O
:	O
id1	O
need	O
be	O
assumed	O
.	O
it	O
gavebse	O
d	O
0:057.	O
it	O
is	O
worth	O
emphasizing	O
some	O
features	O
of	O
the	O
jackknife	O
formula	O
(	O
10.6	O
)	O
.	O
(	O
cid:15	O
)	O
it	O
is	O
nonparametric	B
;	O
no	O
special	O
form	B
of	O
the	O
underlying	O
distribution	B
f	O
inputs	O
the	O
data	B
set	O
x	O
and	O
the	O
function	B
s.x/	O
,	O
and	O
outputsbsejack	O
.	O
(	O
cid:15	O
)	O
it	O
is	O
completely	O
automatic	O
:	O
a	O
single	O
master	O
algorithm	B
can	O
be	O
written	O
that	O
(	O
cid:15	O
)	O
the	O
algorithm	B
works	O
with	O
data	B
sets	O
of	O
size	O
n	O
(	O
cid:0	O
)	O
1	O
,	O
not	O
n.	O
there	O
is	O
a	O
hidden	O
assumption	O
of	O
smooth	O
behavior	O
across	O
sample	B
sizes	O
.	O
this	O
can	O
be	O
worri-	O
some	O
for	O
statistics	B
like	O
the	O
sample	B
median	O
that	O
have	O
a	O
different	O
deﬁnition	O
for	O
odd	O
and	O
even	O
sample	B
size	I
.	O
the	O
jackknife	O
and	O
the	O
bootstrap	O
158	O
(	O
cid:15	O
)	O
the	O
jackknife	O
standard	O
error	O
is	O
upwardly	O
biased	O
as	O
an	O
estimate	O
of	O
the	O
(	O
cid:15	O
)	O
the	O
connection	O
of	O
the	O
jackknife	O
formula	O
(	O
10.6	O
)	O
with	O
taylor	O
series	O
meth-	O
true	O
standard	O
error.	O
1	O
ods	O
is	O
closer	O
than	O
it	O
appears	O
.	O
we	O
can	O
write	O
bsejack	O
dpn	O
i	O
1	O
d2	O
n2	O
(	O
cid:21	O
)	O
1=2	O
where	O
di	O
d	O
;	O
p	O
.i	O
/	O
(	O
cid:0	O
)	O
o	O
o	O
	O
.	O
(	O
cid:1	O
)	O
/	O
n.n	O
(	O
cid:0	O
)	O
1/	O
1=	O
:	O
(	O
10.12	O
)	O
as	O
discussed	O
in	O
section	O
10.3	O
,	O
the	O
di	O
are	O
approximate	O
directional	O
deriva-	O
tives	O
,	O
measures	O
of	O
how	O
fast	O
the	O
statistic	B
s.x/	O
is	O
changing	O
as	O
we	O
decrease	O
the	O
weight	O
on	O
data	B
point	O
xi	O
.	O
so	O
se2	O
jack	O
is	O
proportional	O
to	O
the	O
sum	O
of	O
squared	O
derivatives	O
of	O
s.x/	O
in	O
the	O
n	O
component	O
directions	O
.	O
taylor	O
series	O
expressions	O
such	O
as	O
(	O
10.10	O
)	O
amount	O
to	O
doing	O
the	O
derivatives	O
by	O
formula	B
rather	O
than	O
numerically	O
.	O
figure	O
10.1	O
the	O
lowess	B
curve	O
for	O
the	O
kidney	O
data	O
of	O
figure	O
1.2.	O
vertical	O
bars	O
indicate	O
˙2	O
standard	O
errors	O
:	O
jackknife	O
(	O
10.6	O
)	O
blue	O
dashed	O
;	O
bootstrap	O
(	O
10.16	O
)	O
red	O
solid	O
.	O
the	O
jackknife	O
greatly	O
overestimates	O
variability	O
at	O
age	O
25.	O
the	O
principal	O
weakness	O
of	O
the	O
jackknife	O
is	O
its	O
dependence	O
on	O
local	O
deriva-	O
figure	O
1.2	O
,	O
can	O
result	O
in	O
erratic	O
behavior	O
forbsejack	O
.	O
figure	O
10.1	O
illustrates	O
tives	O
.	O
unsmooth	O
statistics	B
s.x/	O
,	O
such	O
as	O
the	O
kidney	O
data	O
lowess	B
curve	O
in	O
the	O
point	O
.	O
the	O
dashed	O
blue	O
vertical	O
bars	O
indicate	O
˙2	O
jackknife	O
standard	O
er-	O
−4−202agetot202530354045505560657075808525	O
10.2	O
the	O
nonparametric	B
bootstrap	O
159	O
rors	O
for	O
the	O
lowess	B
curve	O
evaluated	O
at	O
ages	O
20	O
;	O
25	O
;	O
:	O
:	O
:	O
;	O
85.	O
for	O
the	O
most	O
part	O
these	O
agree	O
with	O
the	O
dependable	O
bootstrap	O
standard	O
errors	O
,	O
solid	O
red	O
bars	O
,	O
described	O
in	O
section	O
10.2.	O
but	O
things	O
go	O
awry	O
at	O
age	O
25	O
,	O
where	O
the	O
local	O
derivatives	O
greatly	O
overstate	O
the	O
sensitivity	O
of	O
the	O
lowess	B
curve	O
to	O
global	O
changes	O
in	O
the	O
sample	B
x	O
.	O
10.2	O
the	O
nonparametric	B
bootstrap	O
from	O
the	O
point	O
of	O
view	O
of	O
the	O
bootstrap	O
,	O
the	O
jackknife	O
was	O
a	O
halfway	O
house	O
between	O
classical	O
methodology	O
and	O
a	O
full-throated	O
use	O
of	O
electronic	O
com-	O
putation	O
.	O
(	O
the	O
term	O
“	O
computer-intensive	O
statistics	B
”	O
was	O
coined	O
to	O
describe	O
the	O
bootstrap	O
.	O
)	O
the	O
frequentist	O
standard	B
error	I
of	O
an	O
estimate	B
o	O
	O
d	O
s.x/	O
is	O
,	O
ideally	O
,	O
the	O
standard	B
deviation	I
we	O
would	O
observe	O
by	O
repeatedly	O
sampling	O
new	O
versions	O
of	O
x	O
from	O
f	O
.	O
this	O
is	O
impossible	O
since	O
f	O
is	O
unknown	O
.	O
instead	O
,	O
the	O
bootstrap	O
(	O
“	O
ingenious	O
device	O
”	O
number	O
4	O
in	O
section	O
2.1	O
)	O
substitutes	O
an	O
estimate	O
of	O
for	O
f	O
and	O
then	O
estimates	O
the	O
frequentist	O
standard	O
by	O
direct	O
sim-	O
ulation	O
,	O
a	O
feasible	O
tactic	O
only	O
since	O
the	O
advent	O
of	O
electronic	O
computation	O
.	O
the	O
bootstrap	O
estimate	B
of	I
standard	I
error	I
for	O
a	O
statistic	B
o	O
	O
d	O
s.x/	O
com-	O
puted	O
from	O
a	O
random	O
sample	B
x	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn/	O
(	O
10.2	O
)	O
begins	O
with	O
the	O
notion	O
of	O
a	O
bootstrap	O
sample	B
(	O
cid:3	O
)	O
d	O
.x	O
(	O
cid:3	O
)	O
1	O
;	O
x	O
(	O
cid:3	O
)	O
2	O
;	O
:	O
:	O
:	O
;	O
x	O
(	O
cid:3	O
)	O
n	O
/	O
;	O
x	O
(	O
10.13	O
)	O
(	O
cid:3	O
)	O
where	O
each	O
x	O
i	O
is	O
drawn	O
randomly	O
with	O
equal	O
probability	O
and	O
with	O
replace-	O
ment	O
from	O
fx1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xng	O
.	O
each	O
bootstrap	O
sample	B
provides	O
a	O
bootstrap	O
replication	B
of	O
the	O
statistic	B
of	O
interest,3	O
(	O
10.14	O
)	O
some	O
large	O
number	O
b	O
of	O
bootstrap	O
samples	O
are	O
independently	O
drawn	O
(	O
b	O
d	O
500	O
in	O
figure	O
10.1	O
)	O
.	O
the	O
corresponding	O
bootstrap	O
replications	O
are	O
calculated	O
,	O
say	O
/	O
:	O
o	O
	O
(	O
cid:3	O
)	O
b/	O
for	O
b	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
b	O
:	O
(	O
cid:3	O
)	O
b	O
d	O
s.x	O
the	O
resulting	O
bootstrap	O
estimate	B
of	I
standard	I
error	I
for	O
o	O
#	O
1=2	O
standard	B
deviation	I
of	O
the	O
o	O
bseboot	O
d	O
(	O
cid:3	O
)	O
b	O
(	O
cid:0	O
)	O
o	O
(	O
cid:3	O
)	O
(	O
cid:1	O
)	O
2.	O
''	O
bx	O
;	O
with	O
o	O
(	O
cid:3	O
)	O
b	O
values	O
,	O
.b	O
(	O
cid:0	O
)	O
1/	O
(	O
cid:16	O
)	O
o	O
	O
	O
	O
	O
(	O
10.15	O
)	O
	O
is	O
the	O
empirical	B
(	O
cid:3	O
)	O
(	O
cid:1	O
)	O
d	O
bx	O
(	O
cid:3	O
)	O
bıb	O
:	O
o	O
	O
(	O
10.16	O
)	O
(	O
cid:3	O
)	O
is	O
intended	O
to	O
avoid	O
confusion	O
with	O
the	O
original	O
data	B
x	O
,	O
which	O
stays	O
3	O
the	O
star	O
notation	O
x	O
ﬁxed	O
in	O
bootstrap	O
computations	B
,	O
and	O
likewise	O
o	O
(	O
cid:3	O
)	O
vis-a-vis	O
o	O
.	O
o	O
	O
(	O
cid:3	O
)	O
d	O
s.x	O
(	O
cid:3	O
)	O
bd1	O
bd1	O
f	O
	O
:	O
160	O
iid	O
(	O
cid:0	O
)	O
!	O
x	O
the	O
jackknife	O
and	O
the	O
bootstrap	O
motivation	O
forbseboot	O
begins	O
by	O
noting	O
that	O
o	O
	O
is	O
obtained	O
in	O
two	O
steps	O
:	O
ﬁrst	O
x	O
is	O
generated	O
by	O
iid	O
sampling	O
from	O
probability	O
distribution	B
f	O
,	O
and	O
then	O
o	O
	O
is	O
calculated	O
from	O
x	O
according	O
to	O
algorithm	B
s.	O
(	O
cid:1	O
)	O
/	O
,	O
s	O
(	O
cid:0	O
)	O
!	O
o	O
(	O
10.17	O
)	O
we	O
don	O
’	O
t	B
know	O
f	O
,	O
but	O
we	O
can	O
estimate	B
it	O
by	O
the	O
empirical	B
probability	O
dis-	O
tribution	O
of	O
that	O
puts	O
probability	O
1=n	O
on	O
each	O
point	O
xi	O
(	O
e.g.	O
,	O
weight	O
1=157	O
(	O
cid:3	O
)	O
on	O
each	O
point	O
.xi	O
;	O
yi	O
/	O
in	O
figure	O
1.2	O
)	O
.	O
notice	O
that	O
a	O
bootstrap	O
sample	B
x	O
(	O
10.13	O
)	O
is	O
an	O
iid	O
sample	B
drawn	O
from	O
of	O
,	O
since	O
then	O
each	O
x	O
(	O
cid:3	O
)	O
independently	O
has	O
equal	O
probability	O
of	O
being	O
any	O
member	O
of	O
fx1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xng	O
.	O
it	O
can	O
be	O
shown	O
that	O
of	O
maximizes	O
the	O
probability	O
of	O
obtaining	O
the	O
observed	O
sample	B
x	O
under	O
all	O
possible	O
choices	O
of	O
f	O
in	O
(	O
10.2	O
)	O
,	O
i.e.	O
,	O
it	O
is	O
the	O
nonparametric	B
mle	O
of	O
f	O
.	O
bootstrap	O
replications	O
o	O
(	O
cid:3	O
)	O
are	O
obtained	O
by	O
a	O
process	O
analogous	O
to	O
(	O
10.17	O
)	O
,	O
of	O
in	O
the	O
real	O
world	O
(	O
10.17	O
)	O
we	O
only	O
get	O
to	O
see	O
the	O
single	O
value	O
o	O
	O
,	O
but	O
the	O
boot-	O
strap	O
world	O
(	O
10.18	O
)	O
is	O
more	O
generous	O
:	O
we	O
can	O
generate	O
as	O
many	O
bootstrap	O
replications	O
o	O
(	O
cid:3	O
)	O
b	O
as	O
we	O
want	O
,	O
or	O
have	O
time	O
for	O
,	O
and	O
directly	O
estimate	B
their	O
suggests	O
,	O
correctly	O
in	O
most	O
cases	O
,	O
thatbseboot	O
approaches	O
the	O
true	O
standard	O
of	O
approaches	O
f	O
as	O
n	O
grows	O
large	O
variability	O
as	O
in	O
(	O
10.16	O
)	O
.	O
the	O
fact	O
that	O
error	O
of	O
o	O
	O
.	O
the	O
true	O
standard	B
deviation	I
of	O
o	O
	O
,	O
i.e.	O
,	O
its	O
standard	B
error	I
,	O
can	O
be	O
thought	O
of	O
as	O
a	O
function	B
of	O
the	O
probability	O
distribution	B
f	O
that	O
generates	O
the	O
data	B
,	O
say	O
sd.f	O
/	O
.	O
hypothetically	O
,	O
sd.f	O
/	O
inputs	O
f	O
and	O
outputs	O
the	O
standard	O
devi-	O
ation	O
of	O
o	O
	O
,	O
which	O
we	O
can	O
imagine	O
being	O
evaluated	O
by	O
independently	O
run-	O
ning	O
(	O
10.17	O
)	O
some	O
enormous	O
number	O
of	O
times	O
n	O
,	O
and	O
then	O
computing	O
the	O
empirical	B
standard	O
deviation	O
of	O
the	O
resulting	O
o	O
iid	O
(	O
cid:0	O
)	O
!	O
x	O
(	O
10.18	O
)	O
(	O
cid:3	O
)	O
s	O
(	O
cid:0	O
)	O
!	O
o	O
	O
(	O
cid:3	O
)	O
:	O
	O
	O
	O
values	O
,	O
;	O
with	O
o	O
	O
.	O
(	O
cid:1	O
)	O
/	O
d	O
nx	O
	O
.j	O
/ın	O
:	O
o	O
1	O
(	O
10.19	O
)	O
24	O
nx	O
jd1	O
(	O
cid:16	O
)	O
o	O
	O
.j	O
/	O
(	O
cid:0	O
)	O
o	O
sd.f	O
/	O
d	O
351=2	O
	O
.	O
(	O
cid:1	O
)	O
/2	O
.	O
.n	O
(	O
cid:0	O
)	O
1/	O
bseboot	O
d	O
sd	O
.	O
of	O
/	O
:	O
the	O
bootstrap	O
standard	B
error	I
of	O
o	O
	O
is	O
the	O
plug-in	O
estimate	B
(	O
10.20	O
)	O
more	O
exactly	O
,	O
sd	O
.	O
of	O
/	O
is	O
the	O
ideal	O
bootstrap	O
estimate	B
of	I
standard	I
error	I
,	O
what	O
we	O
would	O
get	O
by	O
letting	O
the	O
number	O
of	O
bootstrap	O
replications	O
b	O
go	O
to	O
in-	O
ﬁnity	O
.	O
in	O
practice	O
we	O
have	O
to	O
stop	O
at	O
some	O
ﬁnite	O
value	O
of	O
b	O
,	O
as	O
discussed	O
in	O
what	O
follows	O
.	O
10.2	O
the	O
nonparametric	B
bootstrap	O
161	O
and	O
multisample	O
versions	O
will	O
be	O
taken	O
up	O
later	O
.	O
as	O
with	O
the	O
jackknife	O
,	O
there	O
are	O
several	O
important	O
points	O
worth	O
empha-	O
sizing	O
aboutbseboot	O
.	O
ten	O
that	O
inputs	O
the	O
data	B
x	O
and	O
the	O
function	B
s.	O
(	O
cid:1	O
)	O
/	O
,	O
and	O
outputsbseboot	O
.	O
(	O
cid:15	O
)	O
it	O
is	O
completely	O
automatic	O
.	O
once	O
again	O
,	O
a	O
master	O
algorithm	B
can	O
be	O
writ-	O
(	O
cid:15	O
)	O
we	O
have	O
described	O
the	O
one-sample	O
nonparametric	B
bootstrap	O
.	O
parametric	B
(	O
cid:15	O
)	O
bootstrapping	O
“	O
shakes	O
”	O
the	O
original	O
data	B
more	O
violently	O
than	O
jackknif-	O
(	O
cid:3	O
)	O
from	O
x.	O
the	O
bootstrap	O
is	O
more	O
ing	O
,	O
producing	O
nonlocal	O
deviations	O
of	O
x	O
dependable	O
than	O
the	O
jackknife	O
for	O
unsmooth	O
statistics	B
since	O
it	O
doesn	O
’	O
t	B
(	O
cid:15	O
)	O
b	O
d	O
200	O
is	O
usually	O
sufﬁcient	O
	O
for	O
evaluatingbseboot	O
.	O
larger	O
values	O
,	O
1000	O
2	O
depend	O
on	O
local	O
derivatives	O
.	O
or	O
2000	O
,	O
will	O
be	O
required	O
for	O
the	O
bootstrap	O
conﬁdence	B
intervals	I
of	O
chap-	O
ter	O
11	O
.	O
(	O
cid:15	O
)	O
there	O
is	O
nothing	O
special	O
about	O
standard	O
errors	O
.	O
we	O
could	O
just	O
as	O
well	O
use	O
the	O
bootstrap	O
replications	O
to	O
estimate	B
the	O
expected	O
absolute	O
error	O
efj	O
o	O
(	O
cid:15	O
)	O
fisher	O
’	O
s	O
mle	O
formula	B
(	O
4.27	O
)	O
is	O
applied	O
in	O
practice	O
via	O
	O
(	O
cid:0	O
)	O
jg	O
,	O
or	O
any	O
other	O
accuracy	O
measure	O
.	O
bseﬁsher	O
d	O
.nio	O
/	O
(	O
cid:0	O
)	O
1=2	O
;	O
that	O
is	O
,	O
by	O
plugging	O
in	O
o	O
	O
for	O
	O
after	O
a	O
theoretical	O
calculation	O
of	O
se	O
.	O
the	O
bootstrap	O
operates	O
in	O
the	O
same	O
way	O
at	O
(	O
10.20	O
)	O
,	O
though	O
the	O
plugging	O
in	O
is	O
done	O
before	O
rather	O
than	O
after	O
the	O
calculation	O
.	O
the	O
connection	O
with	O
fishe-	O
rian	O
theory	B
is	O
more	O
obvious	O
for	O
the	O
parametric	B
bootstrap	O
of	O
section	O
10.4	O
.	O
(	O
10.21	O
)	O
the	O
jackknife	O
is	O
a	O
completely	O
frequentist	O
device	O
,	O
both	O
in	O
its	O
assumptions	O
and	O
in	O
its	O
applications	O
(	O
standard	O
errors	O
and	O
biases	O
)	O
.	O
the	O
bootstrap	O
is	O
also	O
basically	O
frequentist	O
,	O
but	O
with	O
a	O
touch	O
of	O
the	O
fisherian	O
as	O
in	O
the	O
relation	O
with	O
(	O
10.21	O
)	O
.	O
its	O
versatility	O
has	O
led	O
to	O
applications	O
in	O
a	O
variety	O
of	O
estima-	O
tion	O
and	O
prediction	O
problems	O
,	O
with	O
even	O
some	O
bayesian	O
connections	O
.	O
	O
unusual	O
applications	O
can	O
also	O
pop	O
up	O
for	O
the	O
jackknife	O
;	O
see	O
the	O
jackknife-	O
after-bootstrap	O
comment	O
in	O
the	O
chapter	O
endnotes.	O
from	O
a	O
classical	O
point	O
of	O
view	O
,	O
the	O
bootstrap	O
is	O
an	O
incredible	O
computa-	O
tional	O
spendthrift	O
.	O
classical	O
statistics	B
was	O
fashioned	O
to	O
minimize	O
the	O
hard	O
labor	O
of	O
mechanical	O
computation	O
.	O
the	O
bootstrap	O
seems	O
to	O
go	O
out	O
of	O
its	O
way	O
to	O
multiply	O
it	O
,	O
by	O
factors	O
of	O
b	O
d	O
200	O
or	O
2000	O
or	O
more	O
.	O
it	O
is	O
nice	O
to	O
re-	O
port	O
that	O
all	O
this	O
computational	O
largesse	O
can	O
have	O
surprising	O
data	B
analytic	O
payoffs	O
.	O
the	O
22	O
students	O
of	O
table	O
3.1	O
actually	O
each	O
took	O
ﬁve	O
tests	O
,	O
mechanics	O
,	O
vectors	O
,	O
algebra	O
,	O
analytics	O
,	O
and	O
statistics	O
.	O
table	O
10.1	O
shows	O
3	O
4	O
162	O
the	O
jackknife	O
and	O
the	O
bootstrap	O
table	O
10.1	O
correlation	O
matrix	O
for	O
the	O
student	B
score	I
data	O
.	O
the	O
eigenvalues	O
are	O
3.463	O
,	O
0.660	O
,	O
0.447	O
,	O
0.234	O
,	O
and	O
0.197.	O
the	O
eigenratio	O
statistic	B
o	O
	O
d	O
0:693	O
,	O
and	O
its	O
bootstrap	O
standard	B
error	I
estimate	O
is	O
0.075	O
(	O
b	O
d	O
2000	O
)	O
.	O
mechanics	O
vectors	O
algebra	O
analytics	O
statistics	B
mechanics	O
vectors	O
algebra	O
analysis	B
statistics	O
1.00	O
.50	O
.76	O
.65	O
.54	O
.50	O
1.00	O
.59	O
.51	O
.38	O
.76	O
.59	O
1.00	O
.76	O
.67	O
.65	O
.51	O
.76	O
1.00	O
.74	O
.54	O
.38	O
.67	O
.74	O
1.00	O
the	O
sample	B
correlation	O
matrix	B
and	O
also	O
its	O
eigenvalues	O
.	O
the	O
“	O
eigenratio	O
”	O
statistic	B
,	O
(	O
10.22	O
)	O
ﬁdence	O
interval	B
calculations	O
.	O
)	O
the	O
jackknife	O
(	O
10.6	O
)	O
gave	O
a	O
bigger	O
estimate	B
,	O
o	O
	O
d	O
largest	O
eigenvalue=sum	O
eigenvalues	O
;	O
measures	O
how	O
closely	O
the	O
ﬁve	O
scores	O
can	O
be	O
predicted	O
by	O
a	O
single	O
linear	B
combination	O
,	O
essentially	O
an	O
iq	O
score	O
for	O
each	O
student	O
:	O
o	O
	O
d	O
0:693	O
here	O
,	O
indicating	O
strong	O
predictive	O
power	O
for	O
the	O
iq	O
score	O
.	O
how	O
accurate	O
is	O
0.693	O
?	O
ror	O
estimate	B
(	O
10.16	O
)	O
bseboot	O
d	O
0:075	O
.	O
(	O
this	O
was	O
10	O
times	O
more	O
bootstraps	O
b	O
d	O
2000	O
bootstrap	O
replications	O
(	O
10.15	O
)	O
yielded	O
bootstrap	O
standard	O
er-	O
than	O
necessary	O
forbseboot	O
,	O
but	O
will	O
be	O
needed	O
for	O
chapter	O
11	O
’	O
s	O
bootstrap	O
con-	O
bsejack	O
d	O
0:083	O
.	O
	O
˙	O
1:96bse	O
for	O
95	O
%	O
coverage	O
.	O
these	O
are	O
based	O
on	O
an	O
assump-	O
standard	O
errors	O
are	O
usually	O
used	O
to	O
suggest	O
approximate	O
conﬁdence	O
in-	O
tervals	O
,	O
often	O
o	O
tion	O
of	O
normality	O
for	O
o	O
	O
.	O
the	O
histogram	O
of	O
the	O
2000	O
bootstrap	O
replications	O
of	O
o	O
	O
,	O
as	O
seen	O
in	O
figure	O
10.2	O
,	O
disabuses	O
belief	O
in	O
even	O
approximate	O
normality	O
.	O
compared	O
with	O
classical	O
methods	O
,	O
a	O
massive	O
amount	O
of	O
computation	O
has	O
gone	O
into	O
the	O
histogram	O
,	O
but	O
this	O
will	O
pay	O
off	O
in	O
chapter	O
11	O
with	O
more	O
ac-	O
curate	O
conﬁdence	O
limits	O
.	O
we	O
can	O
claim	O
a	O
double	O
reward	O
here	O
for	O
bootstrap	O
methods	O
:	O
much	O
wider	O
applicability	O
and	O
improved	O
inferences	O
.	O
the	O
bootstrap	O
histogram—invisible	O
to	O
classical	O
statisticians—nicely	O
illustrates	O
the	O
advan-	O
tages	O
of	O
computer-age	O
statistical	O
inference	B
.	O
10.3	O
resampling	O
plans	B
there	O
is	O
a	O
second	O
way	O
to	O
think	O
about	O
the	O
jackknife	O
and	O
the	O
bootstrap	O
:	O
as	O
algorithms	O
that	O
reweight	O
,	O
or	O
resample	O
,	O
the	O
original	O
data	B
vector	O
x	O
d	O
10.3	O
resampling	O
plans	B
163	O
figure	O
10.2	O
histogram	O
of	O
b	O
d	O
2000	O
bootstrap	O
replications	O
o	O
(	O
cid:3	O
)	O
for	O
the	O
eigenratio	O
statistic	B
(	O
10.22	O
)	O
for	O
the	O
student	B
score	I
data	O
.	O
the	O
vertical	O
black	O
line	O
is	O
at	O
o	O
	O
d	O
:693.	O
the	O
long	O
left	O
tail	O
shows	O
that	O
normality	O
is	O
a	O
dangerous	O
assumption	O
in	O
this	O
case	O
.	O
	O
0.	O
at	O
the	O
price	O
of	O
a	O
little	O
more	O
abstraction	O
,	O
resampling	O
con-	O
0	O
is	O
by	O
deﬁnition	O
a	O
vector	B
of	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn/	O
nects	O
the	O
two	O
algorithms	O
and	O
suggests	O
a	O
class	O
of	O
other	O
possibilities	O
.	O
a	O
resampling	O
vector	B
p	O
d	O
.p1	O
;	O
p2	O
;	O
:	O
:	O
:	O
;	O
pn/	O
nonnegative	O
weights	O
summing	O
to	O
1	O
,	O
nx	O
id1	O
p	O
d	O
.p1	O
;	O
p2	O
;	O
:	O
:	O
:	O
;	O
pn/	O
0	O
with	O
pi	O
(	O
cid:21	O
)	O
0	O
and	O
pi	O
d	O
1	O
:	O
(	O
10.23	O
)	O
that	O
is	O
,	O
p	O
is	O
a	O
member	O
of	O
the	O
simplex	B
sn	O
(	O
5.39	O
)	O
.	O
resampling	O
plans	B
operate	O
by	O
holding	O
the	O
original	O
data	B
set	O
x	O
ﬁxed	O
,	O
and	O
seeing	O
how	O
the	O
statistic	B
of	O
interest	O
o	O
we	O
denote	O
the	O
value	O
of	O
o	O
	O
changes	O
as	O
the	O
weight	O
vector	B
p	O
varies	O
across	O
sn	O
.	O
	O
for	O
a	O
vector	B
putting	O
weight	O
pi	O
on	O
xi	O
as	O
o	O
	O
(	O
cid:3	O
)	O
d	O
s.p/	O
;	O
(	O
10.24	O
)	O
the	O
star	O
notation	O
now	O
indicating	O
any	O
reweighting	O
,	O
not	O
necessarily	O
from	O
boot-	O
strapping	O
;	O
o	O
	O
d	O
s.x/	O
describes	O
the	O
behavior	O
of	O
o	O
mean	O
s.x/	O
d	O
nx	O
,	O
we	O
have	O
s.p/	O
d	O
pn	O
	O
in	O
the	O
real	O
world	O
(	O
10.17	O
)	O
,	O
while	O
o	O
(	O
cid:3	O
)	O
d	O
s.p/	O
describes	O
it	O
in	O
the	O
resampling	O
world	O
.	O
for	O
the	O
sample	B
1	O
pi	O
xi	O
.	O
the	O
unbiased	O
estimate	O
of	O
	O
q^*frequency0.40.50.60.70.80.9050100150standard	O
errorbootstrap	O
.075jackknife	O
.083	O
164	O
variance	O
s.x/	O
dpn	O
the	O
jackknife	O
and	O
the	O
bootstrap	O
i	O
.xi	O
(	O
cid:0	O
)	O
nx/2=.n	O
(	O
cid:0	O
)	O
1/	O
can	O
be	O
seen	O
to	O
have	O
s.p/	O
d	O
n	O
n	O
(	O
cid:0	O
)	O
1	O
(	O
cid:0	O
)	O
pi	O
x2	O
i	O
pi	O
xi	O
(	O
10.25	O
)	O
24	O
nx	O
id1	O
nx	O
id1	O
!	O
235	O
:	O
figure	O
10.3	O
resampling	O
simplex	B
for	O
sample	B
size	I
n	O
d	O
3.	O
the	O
center	O
point	O
is	O
p0	O
(	O
10.26	O
)	O
;	O
the	O
green	O
circles	O
are	O
the	O
jackknife	O
points	O
p.i	O
/	O
(	O
10.28	O
)	O
;	O
triples	O
indicate	O
bootstrap	O
resampling	O
numbers	O
.n1	O
;	O
n2	O
;	O
n3/	O
(	O
10.29	O
)	O
.	O
the	O
bootstrap	O
probabilities	B
are	O
6=27	O
for	O
p0	O
,	O
1=27	O
for	O
each	O
corner	O
point	O
,	O
and	O
3=27	O
for	O
each	O
of	O
the	O
six	O
starred	O
points	O
.	O
letting	O
p0	O
d	O
.1	O
;	O
1	O
;	O
:	O
:	O
:	O
;	O
1/	O
0	O
=n	O
;	O
(	O
10.26	O
)	O
the	O
resampling	O
vector	B
putting	O
equal	O
weight	O
on	O
each	O
value	O
xi	O
,	O
we	O
require	O
in	O
the	O
deﬁnition	O
of	O
s.	O
(	O
cid:1	O
)	O
/	O
that	O
s.p0/	O
d	O
s.x/	O
d	O
o	O
the	O
original	O
estimate	B
.	O
the	O
ith	O
jackknife	O
value	O
o	O
	O
;	O
(	O
10.27	O
)	O
.i	O
/	O
(	O
10.5	O
)	O
corresponds	O
to	O
p0	O
p	O
(	O
1	O
)	O
p	O
(	O
2	O
)	O
p	O
(	O
3	O
)	O
(	O
3,0,0	O
)	O
(	O
2,1,0	O
)	O
(	O
1,2,0	O
)	O
(	O
0,3,0	O
)	O
(	O
1,1,1	O
)	O
(	O
2,0,1	O
)	O
(	O
0,2,1	O
)	O
(	O
1,0,2	O
)	O
(	O
0,1,2	O
)	O
(	O
0,0,3	O
)	O
0.5	O
1.0	O
1.5	O
0.0	O
–	O
0.5	O
–	O
1.0	O
–	O
1.5	O
0.5	O
1.0	O
1.5	O
0.0	O
–	O
0.5	O
2.0	O
10.3	O
resampling	O
plans	B
165	O
resampling	O
vector	B
=.n	O
(	O
cid:0	O
)	O
1/	O
;	O
p.i	O
/	O
d	O
.1	O
;	O
1	O
;	O
:	O
:	O
:	O
;	O
1	O
;	O
0	O
;	O
1	O
;	O
:	O
:	O
:	O
;	O
1/	O
0	O
(	O
10.28	O
)	O
with	O
0	O
in	O
the	O
ith	O
place	O
.	O
figure	O
10.3	O
illustrates	O
the	O
resampling	O
simplex	B
s3	O
applying	O
to	O
sample	B
size	I
n	O
d	O
3	O
,	O
with	O
the	O
center	O
point	O
being	O
p0	O
and	O
the	O
open	O
circles	O
the	O
three	O
possible	O
jackknife	O
vectors	O
p.i	O
/	O
.	O
with	O
n	O
d	O
3	O
sample	B
points	O
fx1	O
;	O
x2	O
;	O
x3g	O
there	O
are	O
only	O
10	O
distinct	O
boot-	O
strap	O
vectors	O
(	O
10.13	O
)	O
,	O
also	O
shown	O
in	O
figure	O
10.3.	O
let	O
ni	O
d	O
#	O
fx	O
the	O
number	O
of	O
bootstrap	O
draws	O
in	O
x	O
ure	O
are	O
.n1	O
;	O
n2	O
;	O
n3/	O
,	O
for	O
example	O
.1	O
;	O
0	O
;	O
2/	O
for	O
x	O
twice.4	O
the	O
bootstrap	O
resampling	O
vectors	O
are	O
of	O
the	O
form	B
(	O
cid:3	O
)	O
(	O
10.29	O
)	O
(	O
cid:3	O
)	O
equaling	O
xi	O
.	O
the	O
triples	O
in	O
the	O
ﬁg-	O
(	O
cid:3	O
)	O
having	O
x1	O
once	O
and	O
x3	O
d	O
xig	O
;	O
j	O
(	O
cid:3	O
)	O
d	O
.n1	O
;	O
n2	O
;	O
:	O
:	O
:	O
;	O
nn/	O
0	O
p	O
=n	O
;	O
(	O
10.30	O
)	O
where	O
the	O
ni	O
are	O
nonnegative	O
integers	O
summing	O
to	O
n.	O
according	O
to	O
deﬁ-	O
nition	O
(	O
10.13	O
)	O
of	O
bootstrap	O
sampling	O
,	O
the	O
vector	B
n	O
d	O
.n1	O
;	O
n2	O
;	O
:	O
:	O
:	O
;	O
nn/	O
0	O
follows	O
a	O
multinomial	O
distribution	B
(	O
5.38	O
)	O
with	O
n	O
draws	O
on	O
n	O
equally	O
likely	O
categories	O
,	O
n	O
(	O
cid:24	O
)	O
multn.n	O
;	O
p0/	O
:	O
this	O
gives	O
bootstrap	O
probability	O
(	O
5.37	O
)	O
nš	O
n1šn2š	O
:	O
:	O
:	O
nnš	O
1	O
nn	O
on	O
p	O
(	O
cid:3	O
)	O
(	O
10.30	O
)	O
.	O
figure	O
10.3	O
is	O
misleading	O
in	O
that	O
the	O
jackknife	O
vectors	O
p.i	O
/	O
appear	O
only	O
(	O
cid:3	O
)	O
.	O
as	O
n	O
grows	O
large	O
slightly	O
closer	O
to	O
p0	O
than	O
are	O
the	O
bootstrap	O
vectors	O
p	O
they	O
are	O
,	O
in	O
fact	O
,	O
an	O
order	O
of	O
magnitude	O
closer	O
.	O
subtracting	O
(	O
10.26	O
)	O
from	O
(	O
10.28	O
)	O
gives	O
euclidean	O
distance	O
(	O
10.31	O
)	O
(	O
10.32	O
)	O
(	O
10.33	O
)	O
kp.i	O
/	O
(	O
cid:0	O
)	O
p0k	O
d	O
1	O
	O
ni	O
(	O
cid:24	O
)	O
bi	O
n.n	O
(	O
cid:0	O
)	O
1/	O
:	O
	O
.p	O
1	O
n	O
for	O
the	O
bootstrap	O
,	O
notice	O
that	O
ni	O
in	O
(	O
10.29	O
)	O
has	O
a	O
binomial	B
distribution	O
,	O
(	O
10.34	O
)	O
4	O
a	O
hidden	O
assumption	O
of	O
deﬁnition	O
(	O
10.24	O
)	O
is	O
that	O
o	O
d	O
s.x/	O
has	O
the	O
same	O
value	O
for	O
any	O
permutation	O
of	O
x	O
,	O
so	O
for	O
instance	O
s.x1	O
;	O
x3	O
;	O
x3/	O
d	O
s.x3	O
;	O
x1	O
;	O
x3/	O
d	O
s.1=3	O
;	O
0	O
;	O
2=3/	O
.	O
n	O
;	O
;	O
the	O
jackknife	O
and	O
the	O
bootstrap	O
166	O
with	O
mean	O
1	O
and	O
variance	O
.n	O
(	O
cid:0	O
)	O
1/=n	O
.	O
then	O
p	O
d	O
ni	O
=n	O
has	O
mean	O
and	O
vari-	O
ance	O
.1=n	O
;	O
.n	O
(	O
cid:0	O
)	O
1/=n3/	O
.	O
adding	O
over	O
the	O
n	O
coordinates	O
gives	O
the	O
expected	O
(	O
cid:3	O
)	O
,	O
root	O
mean	O
square	O
distance	O
for	O
bootstrap	O
vector	B
p	O
(	O
cid:3	O
)	O
i	O
(	O
cid:0	O
)	O
ekp	O
(	O
cid:3	O
)	O
(	O
cid:0	O
)	O
p0k2	O
(	O
cid:1	O
)	O
1=2	O
dp	O
.n	O
(	O
cid:0	O
)	O
1/=n2	O
;	O
p	O
n	O
times	O
further	O
than	O
(	O
10.33	O
)	O
.	O
(	O
10.35	O
)	O
(	O
10.36	O
)	O
an	O
order	O
of	O
magnitude	O
the	O
function	B
s.p/	O
has	O
approximate	O
directional	O
derivative	O
di	O
d	O
s.p.i	O
//	O
(	O
cid:0	O
)	O
s.p0/	O
kp.i	O
/	O
(	O
cid:0	O
)	O
p0k	O
''	O
10x	O
(	O
cid:16	O
)	O
o	O
	O
ideal	O
bootstrap	O
standard	B
error	I
estimate	O
in	O
the	O
direction	O
from	O
p0	O
toward	O
p.i	O
/	O
(	O
measured	O
along	O
the	O
dashed	O
lines	O
in	O
figure	O
10.3	O
)	O
.	O
di	O
measures	O
the	O
slope	O
of	O
function	B
s.p/	O
at	O
p0	O
,	O
in	O
the	O
direction	O
of	O
p.i	O
/	O
.	O
formula	B
(	O
10.12	O
)	O
showsbsejack	O
as	O
proportional	O
to	O
the	O
root	O
out	O
thatbsejack	O
equalsbseboot	O
(	O
except	O
for	O
the	O
fudge	O
factor	B
.n	O
(	O
cid:0	O
)	O
1/=n	O
in	O
(	O
10.6	O
)	O
)	O
.	O
if	O
s.p/	O
is	O
a	O
linear	B
function	O
of	O
p	O
,	O
as	O
it	O
is	O
for	O
the	O
sample	B
mean	O
,	O
it	O
turns	O
mean	O
square	O
of	O
the	O
slopes	O
.	O
most	O
statistics	B
are	O
not	O
linear	O
,	O
and	O
then	O
the	O
local	O
jackknife	O
resamples	O
may	O
provide	O
a	O
poor	O
approximation	O
to	O
the	O
full	B
resampling	O
behavior	O
of	O
s.p/	O
.	O
this	O
was	O
the	O
case	O
at	O
one	O
point	O
in	O
figure	O
10.1	O
.	O
(	O
cid:3	O
)	O
,	O
we	O
can	O
easily	O
evaluate	O
the	O
with	O
only	O
10	O
possible	O
resampling	O
points	O
p	O
#	O
1=2	O
(	O
cid:3	O
)	O
(	O
cid:1	O
)	O
2	O
	O
;	O
(	O
cid:3	O
)	O
(	O
cid:1	O
)	O
d	O
10x	O
o	O
	O
	O
pk	O
pk	O
o	O
	O
kd1	O
kd1	O
(	O
cid:3	O
)	O
k	O
;	O
(	O
10.37	O
)	O
(	O
cid:3	O
)	O
k	O
(	O
cid:0	O
)	O
o	O
bseboot	O
d	O
with	O
o	O
(	O
cid:3	O
)	O
k	O
d	O
s.p	O
k/	O
and	O
pk	O
the	O
probability	O
from	O
(	O
10.32	O
)	O
(	O
listed	O
in	O
fig-	O
!	O
ure	O
10.3	O
)	O
.	O
this	O
rapidly	O
becomes	O
impractical	O
.	O
the	O
number	O
of	O
distinct	O
boot-	O
strap	O
samples	O
for	O
n	O
points	O
turns	O
out	O
to	O
be	O
2n	O
(	O
cid:0	O
)	O
1	O
(	O
10.38	O
)	O
for	O
n	O
d	O
10	O
this	O
is	O
already	O
92,378	O
,	O
while	O
n	O
d	O
20	O
gives	O
6:9	O
(	O
cid:2	O
)	O
1010	O
distinct	O
(	O
cid:3	O
)	O
at	O
random	O
,	O
which	O
is	O
what	O
al-	O
possible	O
resamples	O
.	O
choosing	O
b	O
vectors	O
p	O
gorithm	O
(	O
10.13	O
)	O
–	O
(	O
10.15	O
)	O
effectively	O
is	O
doing	O
,	O
makes	O
the	O
un-ideal	O
bootstrap	O
standard	B
error	I
estimate	O
(	O
10.16	O
)	O
almost	O
as	O
accurate	O
as	O
(	O
10.37	O
)	O
for	O
b	O
as	O
small	O
as	O
200	O
or	O
even	O
less	O
.	O
n	O
:	O
the	O
luxury	O
of	O
examining	O
the	O
resampling	O
surface	O
provides	O
a	O
major	O
advan-	O
tage	O
to	O
modern	O
statisticians	O
,	O
both	O
in	O
inference	B
and	O
methodology	O
.	O
a	O
variety	O
of	O
other	O
resampling	O
schemes	O
have	O
been	O
proposed	O
,	O
a	O
few	O
of	O
which	O
follow	O
.	O
10.3	O
resampling	O
plans	B
167	O
the	O
inﬁnitesimal	O
jackknife	O
looking	O
at	O
figure	O
10.3	O
again	O
,	O
the	O
vector	B
pi	O
.	O
(	O
cid:15	O
)	O
/	O
d	O
.1	O
(	O
cid:0	O
)	O
(	O
cid:15	O
)	O
/p0	O
c	O
(	O
cid:15	O
)	O
p.i	O
/	O
d	O
p0	O
c	O
(	O
cid:15	O
)	O
.p.i	O
/	O
(	O
cid:0	O
)	O
p0/	O
(	O
10.39	O
)	O
lies	O
proportion	B
(	O
cid:15	O
)	O
of	O
the	O
way	O
from	O
p0	O
to	O
p.i	O
/	O
.	O
then	O
s	O
.pi	O
.	O
(	O
cid:15	O
)	O
//	O
(	O
cid:0	O
)	O
s.p0/	O
qdi	O
d	O
lim	O
(	O
cid:15	O
)	O
!	O
0	O
(	O
10.40	O
)	O
(	O
cid:15	O
)	O
kp.i	O
/	O
(	O
cid:0	O
)	O
p0k	O
!	O
1=2	O
ın2	O
qd2	O
i	O
nx	O
id1	O
bseij	O
d	O
exactly	O
deﬁnes	O
the	O
direction	O
derivative	O
at	O
p0	O
in	O
the	O
direction	O
of	O
p.i	O
/	O
.	O
the	O
inﬁnitesimal	O
jackknife	O
estimate	O
of	O
standard	B
error	I
is	O
;	O
(	O
10.41	O
)	O
usually	O
evaluated	O
numerically	O
by	O
setting	O
(	O
cid:15	O
)	O
to	O
some	O
small	O
value	O
in	O
(	O
10.40	O
)	O
–	O
(	O
10.41	O
)	O
(	O
rather	O
than	O
(	O
cid:15	O
)	O
d	O
1	O
in	O
(	O
10.12	O
)	O
)	O
.	O
we	O
will	O
meet	O
the	O
inﬁnitesimal	O
jack-	O
knife	O
again	O
in	O
chapters	O
17	O
and	O
20.	O
multisample	B
bootstrap	O
the	O
median	O
difference	O
between	O
the	O
aml	O
and	O
the	O
all	O
scores	O
in	O
figure	O
1.4	O
is	O
mediff	O
d	O
0:968	O
(	O
cid:0	O
)	O
0:733	O
d	O
0:235	O
:	O
(	O
10.42	O
)	O
how	O
accurate	O
is	O
0.235	O
?	O
an	O
appropriate	O
form	B
of	O
bootstrapping	O
draws	O
25	O
times	O
with	O
replacement	O
from	O
the	O
25	O
aml	O
patients	O
,	O
47	O
times	O
with	O
replace-	O
ment	O
from	O
the	O
47	O
all	O
patients	O
,	O
and	O
computes	O
mediff	O
(	O
cid:3	O
)	O
as	O
the	O
difference	O
between	O
the	O
medians	O
of	O
the	O
two	O
bootstrap	O
samples	O
.	O
(	O
drawing	O
one	O
boot-	O
strap	O
sample	B
of	O
size	O
72	O
from	O
all	O
the	O
patients	O
would	O
result	O
in	O
random	O
sample	B
sizes	O
for	O
the	O
aml	O
(	O
cid:3	O
)	O
=all	O
(	O
cid:3	O
)	O
groups	O
,	O
adding	O
inappropriate	O
variability	O
to	O
the	O
frequentist	O
standard	B
error	I
estimate	O
.	O
)	O
givebseboot	O
d	O
0:074.	O
the	O
estimate	B
(	O
10.42	O
)	O
is	O
3.18bse	O
units	O
above	O
zero	O
,	O
agree-	O
a	O
histogram	O
of	O
b	O
d	O
500	O
mediff	O
(	O
cid:3	O
)	O
values	O
appears	O
in	O
figure	O
10.4.	O
they	O
ing	O
surprisingly	O
well	O
with	O
the	O
usual	O
two-sample	B
t-statistic	O
3.01	O
(	O
based	O
on	O
mean	O
differences	O
)	O
,	O
and	O
its	O
permutation	O
histogram	O
figure	O
4.3.	O
permutation	O
testing	B
can	O
be	O
considered	O
another	O
form	B
of	O
resampling	O
.	O
168	O
the	O
jackknife	O
and	O
the	O
bootstrap	O
figure	O
10.4	O
b	O
d	O
500	O
bootstrap	O
replications	O
for	O
the	O
median	O
bseboot	O
d	O
0:074.	O
the	O
observed	O
value	O
mediff	O
d	O
0:235	O
(	O
vertical	O
difference	O
between	O
the	O
aml	O
and	O
all	O
scores	O
in	O
figure	O
1.4	O
,	O
giving	O
black	O
line	O
)	O
is	O
more	O
than	O
3	O
standard	O
errors	O
above	O
zero	O
.	O
moving	B
blocks	I
bootstrap	O
suppose	O
x	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn/	O
,	O
instead	O
of	O
being	O
an	O
iid	O
sample	B
(	O
10.2	O
)	O
,	O
is	O
a	O
time	O
series	O
.	O
that	O
is	O
,	O
the	O
x	O
values	O
occur	O
in	O
a	O
meaningful	O
order	O
,	O
perhaps	O
with	O
nearby	O
observations	O
highly	O
correlated	O
with	O
each	O
other	O
.	O
let	O
bm	O
be	O
the	O
set	B
of	O
contiguous	O
blocks	O
of	O
length	O
m	O
,	O
for	O
example	O
b3	O
d	O
f.x1	O
;	O
x2	O
;	O
x3/	O
;	O
.x2	O
;	O
x3	O
;	O
x4/	O
;	O
:	O
:	O
:	O
;	O
.xn	O
(	O
cid:0	O
)	O
2	O
;	O
xn	O
(	O
cid:0	O
)	O
1	O
;	O
xn/g	O
:	O
(	O
10.43	O
)	O
presumably	O
,	O
m	O
is	O
chosen	O
large	O
enough	O
that	O
correlations	O
between	O
xi	O
and	O
xj	O
,	O
jj	O
(	O
cid:0	O
)	O
ij	O
>	O
m	O
,	O
are	O
neglible	O
.	O
the	O
moving	O
block	O
bootstrap	O
ﬁrst	O
selects	O
n=m	O
(	O
cid:3	O
)	O
.	O
having	O
constructed	O
b	O
such	O
samples	O
,	O
bseboot	O
is	O
calculated	O
blocks	O
from	O
bm	O
,	O
and	O
assembles	O
them	O
in	O
random	O
order	O
to	O
construct	O
a	O
boot-	O
strap	O
sample	B
x	O
as	O
in	O
(	O
10.15	O
)	O
–	O
(	O
10.16	O
)	O
.	O
the	O
bayesian	O
bootstrap	O
let	O
g1	O
;	O
g2	O
;	O
:	O
:	O
:	O
;	O
gn	O
be	O
independent	O
one-sided	O
exponential	O
variates	O
(	O
de-	O
noted	O
gam	O
(	O
1,1	O
)	O
in	O
table	O
5.1	O
)	O
,	O
each	O
having	O
density	B
exp	O
.	O
(	O
cid:0	O
)	O
x/	O
for	O
x	O
>	O
0.	O
mediff*frequency0.00.10.20.30.40.5010203040506070	O
10.4	O
the	O
parametric	B
bootstrap	O
169	O
,	O
nx	O
the	O
bayesian	O
bootstrap	O
uses	O
resampling	O
vectors	O
p	O
it	O
can	O
be	O
shown	O
that	O
p	O
(	O
cid:3	O
)	O
d	O
.g1	O
;	O
g2	O
;	O
:	O
:	O
:	O
;	O
gn/	O
(	O
10.44	O
)	O
(	O
cid:3	O
)	O
is	O
then	O
uniformly	O
distributed	O
over	O
the	O
resampling	O
simplex	B
sn	O
;	O
for	O
n	O
d	O
3	O
,	O
uniformly	O
distributed	O
over	O
the	O
triangle	O
in	O
fig-	O
ure	O
10.3.	O
prescription	O
(	O
10.44	O
)	O
is	O
motivated	O
by	O
assuming	O
a	O
jeffreys-style	O
uninformative	O
prior	B
distribution	I
(	O
section	O
3.2	O
)	O
on	O
the	O
unknown	O
distribution	B
f	O
(	O
10.2	O
)	O
.	O
gi	O
:	O
1	O
(	O
cid:3	O
)	O
has	O
mean	O
vector	B
and	O
covariance	O
matrix	B
distribution	O
(	O
10.44	O
)	O
for	O
p	O
p	O
p0	O
;	O
1	O
n	O
c	O
1	O
:	O
(	O
10.45	O
)	O
this	O
is	O
almost	O
identical	O
to	O
the	O
mean	O
and	O
covariance	O
of	O
bootstrap	O
resamples	O
(	O
cid:3	O
)	O
(	O
cid:24	O
)	O
multn.n	O
,	O
p0/=n	O
,	O
p	O
(	O
cid:3	O
)	O
(	O
cid:24	O
)	O
	O
(	O
cid:3	O
)	O
(	O
cid:24	O
)	O
	O
p	O
p0	O
;	O
1	O
n	O
0	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
(	O
cid:0	O
)	O
diag.p0/	O
(	O
cid:0	O
)	O
p0p	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
(	O
cid:0	O
)	O
diag.p0/	O
(	O
cid:0	O
)	O
p0p	O
0	O
;	O
0	O
0	O
(	O
10.46	O
)	O
(	O
5.40	O
)	O
.	O
the	O
bayesian	O
bootstrap	O
and	O
the	O
ordinary	O
bootstrap	O
tend	O
to	O
agree	O
,	O
at	O
least	O
for	O
smoothly	O
deﬁned	O
statistics	B
o	O
there	O
was	O
some	O
bayesian	O
disparagement	O
of	O
the	O
bootstrap	O
when	O
it	O
ﬁrst	O
appeared	O
because	O
of	O
its	O
blatantly	O
frequentist	O
take	O
on	O
estimation	B
accuracy	O
.	O
and	O
yet	O
connections	O
like	O
(	O
10.45	O
)	O
–	O
(	O
10.46	O
)	O
have	O
continued	O
to	O
pop	O
up	O
,	O
as	O
we	O
will	O
see	O
in	O
chapter	O
13	O
.	O
(	O
cid:3	O
)	O
d	O
s.p	O
/	O
.	O
(	O
cid:3	O
)	O
	O
10.4	O
the	O
parametric	B
bootstrap	O
in	O
our	O
description	O
(	O
10.18	O
)	O
of	O
bootstrap	O
resampling	O
,	O
of	O
iid	O
(	O
cid:0	O
)	O
!	O
x	O
(	O
cid:3	O
)	O
(	O
cid:0	O
)	O
!	O
o	O
(	O
cid:3	O
)	O
;	O
	O
(	O
10.47	O
)	O
there	O
is	O
no	O
need	O
to	O
insist	O
that	O
of	O
be	O
the	O
nonparametric	B
mle	O
of	O
f	O
.	O
suppose	O
d˚f	O
(	O
cid:22	O
)	O
.x/	O
;	O
(	O
cid:22	O
)	O
2	O
(	O
cid:127	O
)	O
(	O
cid:9	O
)	O
:	O
we	O
are	O
willing	O
to	O
assume	O
that	O
the	O
observed	O
data	B
vector	O
x	O
comes	O
from	O
a	O
parametric	B
family	O
f	O
as	O
in	O
(	O
5.1	O
)	O
,	O
(	O
10.48	O
)	O
let	O
o	O
(	O
cid:22	O
)	O
be	O
the	O
mle	O
of	O
(	O
cid:22	O
)	O
.	O
the	O
bootstrap	O
parametric	B
resamples	O
from	O
f	O
o	O
(	O
cid:22	O
)	O
.	O
(	O
cid:1	O
)	O
/	O
,	O
and	O
proceeds	O
as	O
in	O
(	O
10.14	O
)	O
–	O
(	O
10.16	O
)	O
to	O
calculatebseboot	O
.	O
(	O
10.49	O
)	O
f	O
o	O
(	O
cid:22	O
)	O
(	O
cid:0	O
)	O
!	O
x	O
(	O
cid:3	O
)	O
(	O
cid:0	O
)	O
!	O
o	O
f	O
(	O
cid:3	O
)	O
	O
;	O
the	O
jackknife	O
and	O
the	O
bootstrap	O
170	O
as	O
an	O
example	O
,	O
suppose	O
that	O
x	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn/	O
is	O
an	O
iid	O
sample	B
of	O
size	O
n	O
from	O
a	O
normal	B
distribution	O
,	O
iid	O
(	O
cid:24	O
)	O
xi	O
n	O
.	O
(	O
cid:22	O
)	O
;	O
1/	O
;	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
:	O
then	O
o	O
(	O
cid:22	O
)	O
d	O
nx	O
,	O
and	O
a	O
parametric	B
bootstrap	O
sample	B
is	O
x	O
where	O
(	O
cid:3	O
)	O
d	O
.x	O
(	O
10.50	O
)	O
(	O
cid:3	O
)	O
1	O
;	O
x	O
(	O
cid:3	O
)	O
2	O
;	O
:	O
:	O
:	O
;	O
x	O
(	O
cid:3	O
)	O
n	O
/	O
,	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
:	O
(	O
cid:3	O
)	O
iid	O
(	O
cid:24	O
)	O
n	O
.nx	O
;	O
1/	O
;	O
x	O
i	O
(	O
10.51	O
)	O
more	O
adventurously	O
,	O
if	O
f	O
were	O
a	O
family	O
of	O
time	O
series	O
models	B
for	O
x	O
,	O
(	O
cid:3	O
)	O
algorithm	B
(	O
10.49	O
)	O
would	O
still	O
apply	O
(	O
now	O
without	O
any	O
iid	O
structure	O
)	O
:	O
x	O
would	O
be	O
a	O
time	O
series	O
sampled	O
from	O
model	O
f	O
o	O
(	O
cid:22	O
)	O
.	O
(	O
cid:1	O
)	O
/	O
,	O
and	O
o	O
(	O
cid:3	O
)	O
d	O
s.x	O
/	O
the	O
(	O
cid:3	O
)	O
b	O
,	O
b	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
b	O
,	O
andbseboot	O
from	O
(	O
10.16	O
)	O
.	O
	O
(	O
cid:3	O
)	O
b	O
would	O
give	O
resampled	O
statistic	B
of	O
interest	O
.	O
b	O
independent	O
realizations	O
x	O
o	O
	O
(	O
cid:3	O
)	O
figure	O
10.5	O
the	O
gfr	O
data	B
of	O
figure	O
5.7	O
(	O
histogram	O
)	O
.	O
curves	O
show	O
the	O
mle	O
ﬁts	O
from	O
polynomial	O
poisson	O
models	B
,	O
for	O
degrees	O
of	O
freedom	O
df	O
d	O
2	O
;	O
3	O
;	O
:	O
:	O
:	O
;	O
7.	O
the	O
points	O
on	O
the	O
curves	O
show	O
the	O
ﬁts	O
computed	O
at	O
the	O
centers	O
x.j	O
/	O
of	O
the	O
bins	O
,	O
with	O
the	O
responses	O
being	O
the	O
counts	O
in	O
the	O
bins	O
.	O
the	O
dashes	O
at	O
the	O
base	O
of	O
the	O
plot	O
show	O
the	O
nine	O
gfr	O
values	O
appearing	O
in	O
table	O
10.2.	O
as	O
an	O
example	O
of	O
parametric	B
bootstrapping	O
,	O
figure	O
10.5	O
expands	O
the	O
gfr	O
investigation	O
of	O
figure	O
5.7.	O
in	O
addition	O
to	O
the	O
seventh-degree	O
polyno-	O
mial	O
ﬁt	O
(	O
5.62	O
)	O
,	O
we	O
now	O
show	O
lower-degree	O
polynomial	O
ﬁts	O
for	O
2	O
,	O
3	O
,	O
4	O
,	O
5	O
,	O
gfrcounts20406080100051015202530lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllldf	O
=	O
2df	O
=	O
3,4,5,6df	O
=	O
7	O
10.4	O
the	O
parametric	B
bootstrap	O
171	O
and	O
6	O
degrees	O
of	O
freedom	O
;	O
df	O
d	O
2	O
obviously	O
gives	O
a	O
poor	O
ﬁt	O
;	O
df	O
d	O
3	O
;	O
4	O
;	O
5	O
;	O
6	O
give	O
nearly	O
identical	O
curves	O
;	O
df	O
d	O
7	O
gives	O
only	O
a	O
slightly	O
better	O
ﬁt	O
to	O
the	O
raw	O
data	B
.	O
the	O
plotted	O
curves	O
were	O
obtained	O
from	O
the	O
poisson	O
regression	B
method	O
used	O
in	O
section	O
8.3	O
,	O
which	O
we	O
refer	O
to	O
as	O
“	O
lindsey	O
’	O
s	O
method	B
”	O
.	O
(	O
cid:15	O
)	O
the	O
x-axis	O
was	O
partitioned	O
into	O
k	O
d	O
32	O
bins	O
,	O
with	O
endpoints	O
13	O
;	O
16	O
;	O
19	O
,	O
:	O
:	O
:	O
;	O
109	O
,	O
and	O
centerpoints	O
,	O
say	O
,	O
x	O
.	O
/	O
d	O
.x.1/	O
;	O
x.2/	O
;	O
:	O
:	O
:	O
;	O
x.k//	O
;	O
x.1/	O
d	O
14:5	O
,	O
x.2/	O
d	O
17:5	O
,	O
etc	O
.	O
(	O
cid:15	O
)	O
count	O
vector	B
y	O
d	O
.y1	O
;	O
y2	O
;	O
:	O
:	O
:	O
;	O
yk/	O
was	O
computed	O
yk	O
d	O
#	O
fxi	O
in	O
binkg	O
(	O
so	O
y	O
gives	O
the	O
heights	O
of	O
the	O
bars	O
in	O
figure	O
10.5	O
)	O
.	O
(	O
cid:15	O
)	O
an	O
independent	O
poisson	O
model	B
was	O
assumed	O
for	O
the	O
counts	O
,	O
(	O
10.52	O
)	O
(	O
10.53	O
)	O
ind	O
(	O
cid:24	O
)	O
poi	O
.	O
(	O
cid:22	O
)	O
k/	O
for	O
k	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
k	O
:	O
yk	O
(	O
10.54	O
)	O
(	O
cid:15	O
)	O
the	O
parametric	B
model	O
of	O
degree	O
“	O
df	O
”	O
assumed	O
that	O
the	O
(	O
cid:22	O
)	O
k	O
values	O
were	O
described	O
by	O
an	O
exponential	O
polynomial	O
of	O
degree	O
df	O
in	O
the	O
x.k/	O
values	O
,	O
ˇj	O
xj	O
.k/	O
:	O
(	O
10.55	O
)	O
(	O
cid:15	O
)	O
the	O
mle	O
oˇ	O
d	O
.	O
(	O
cid:15	O
)	O
the	O
plotted	O
curves	O
in	O
figure	O
10.5	O
trace	O
the	O
mle	O
values	O
o	O
(	O
cid:22	O
)	O
k	O
,	O
o	O
ˇdf/	O
in	O
model	B
(	O
10.54	O
)	O
–	O
(	O
10.55	O
)	O
was	O
found.5	O
o	O
ˇ1	O
;	O
:	O
:	O
:	O
;	O
o	O
ˇ0	O
;	O
log	O
.	O
(	O
cid:22	O
)	O
k/	O
d	O
dfx	O
jd0	O
log	O
.	O
o	O
(	O
cid:22	O
)	O
k/	O
d	O
dfx	O
jd0	O
o	O
ˇj	O
xj	O
.k/	O
:	O
(	O
10.56	O
)	O
y	O
k	O
(	O
cid:3	O
)	O
how	O
accurate	O
are	O
the	O
curves	O
?	O
parametric	B
bootstraps	O
were	O
used	O
to	O
assess	O
their	O
standard	O
errors	O
.	O
that	O
is	O
,	O
poisson	O
resamples	O
were	O
generated	O
according	O
to	O
for	O
k	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
k	O
;	O
ind	O
(	O
cid:24	O
)	O
poi	O
.	O
o	O
(	O
cid:22	O
)	O
k/	O
(	O
10.57	O
)	O
and	O
bootstrap	O
mle	O
values	O
o	O
(	O
cid:22	O
)	O
(	O
cid:3	O
)	O
k	O
calculated	O
as	O
above	O
,	O
but	O
now	O
based	O
on	O
count	O
(	O
cid:3	O
)	O
rather	O
than	O
y.	O
all	O
of	O
this	O
was	O
done	O
b	O
d	O
200	O
times	O
,	O
yielding	O
vector	B
y	O
the	O
results	O
appear	O
in	O
table	O
10.2	O
,	O
showing	O
bseboot	O
for	O
df	O
d	O
2	O
;	O
3	O
;	O
:	O
:	O
:	O
;	O
7	O
bootstrap	O
standard	O
errors	O
(	O
10.16	O
)	O
.	O
5	O
a	O
single	O
r	O
command	O
,	O
glm	O
(	O
y	O
(	O
cid:24	O
)	O
poly	O
(	O
x	O
,	O
df	O
)	O
,	O
family=poisson	O
)	O
accomplishes	O
this	O
.	O
172	O
the	O
jackknife	O
and	O
the	O
bootstrap	O
table	O
10.2	O
bootstrap	O
estimates	O
of	O
standard	B
error	I
for	O
the	O
gfr	O
density	B
.	O
poisson	O
regression	B
models	O
(	O
10.54	O
)	O
–	O
(	O
10.55	O
)	O
,	O
df	O
d	O
2	O
;	O
3	O
;	O
:	O
:	O
:	O
;	O
7	O
,	O
as	O
in	O
figure	O
10.5	O
;	O
each	O
b	O
d	O
200	O
bootstrap	O
replications	O
;	O
nonparametric	B
standard	O
errors	B
based	O
on	O
binomial	B
bin	O
counts	O
.	O
degrees	O
of	O
freedom	O
gfr	O
20.5	O
29.5	O
38.5	O
47.5	O
56.5	O
65.5	O
74.5	O
83.5	O
92.5	O
2	O
.28	O
.65	O
1.05	O
1.47	O
1.57	O
1.15	O
.76	O
.40	O
.13	O
3	O
.07	O
.57	O
1.39	O
1.91	O
1.60	O
1.10	O
.61	O
.30	O
.20	O
4	O
.13	O
.57	O
1.33	O
2.12	O
1.79	O
1.07	O
.62	O
.40	O
.29	O
5	O
.13	O
.66	O
1.52	O
1.93	O
1.93	O
1.31	O
.68	O
.38	O
.29	O
6	O
.12	O
.74	O
1.72	O
2.15	O
1.87	O
1.34	O
.81	O
.49	O
.34	O
7	O
.05	O
1.11	O
1.73	O
2.39	O
2.28	O
1.27	O
.71	O
.68	O
.46	O
nonparametric	B
standard	O
error	O
.00	O
1.72	O
2.77	O
4.25	O
4.35	O
1.72	O
1.72	O
1.72	O
.00	O
degrees	O
of	O
freedom	O
evaluated	O
at	O
nine	O
values	O
of	O
gfr	O
.	O
variability	O
generally	O
increases	O
with	O
increasing	O
df	O
,	O
as	O
expected	O
.	O
choosing	O
a	O
“	O
best	O
”	O
model	B
is	O
a	O
compromise	O
between	O
standard	B
error	I
and	O
possible	O
deﬁnitional	O
bias	O
as	O
sug-	O
gested	O
by	O
figure	O
10.5	O
,	O
with	O
perhaps	O
df	O
d	O
3	O
or	O
4	O
,	O
the	O
winner	O
.	O
if	O
we	O
kept	O
increasing	O
the	O
degrees	O
of	O
freedom	O
,	O
eventually	O
(	O
at	O
df	O
d	O
32	O
)	O
we	O
would	O
exactly	O
match	O
the	O
bar	O
heights	O
yk	O
in	O
the	O
histogram	O
.	O
at	O
this	O
point	O
the	O
parametric	B
bootstrap	O
would	O
merge	O
into	O
the	O
nonparametric	B
bootstrap	O
.	O
“	O
nonparametric	B
”	O
is	O
another	O
name	O
for	O
“	O
very	O
highly	O
parameterized.	O
”	O
the	O
huge	O
sample	B
sizes	O
associated	O
with	O
modern	O
applications	O
have	O
encouraged	O
nonparametric	B
methods	O
,	O
on	O
the	O
sometimes	O
mistaken	O
ground	O
that	O
estimation	B
efﬁciency	O
is	O
no	O
longer	O
of	O
concern	O
.	O
it	O
is	O
costly	O
here	O
,	O
as	O
the	O
“	O
nonparametric	B
”	O
column	O
of	O
table	O
10.2	O
shows.6	O
figure	O
10.6	O
returns	O
to	O
the	O
student	B
score	I
eigenratio	O
calculations	O
of	O
fig-	O
ure	O
10.2.	O
the	O
solid	O
histogram	O
shows	O
2000	O
parametric	B
bootstrap	O
replica-	O
tions	O
(	O
10.49	O
)	O
,	O
with	O
f	O
o	O
(	O
cid:22	O
)	O
the	O
ﬁve-dimensional	O
bivariate	O
normal	B
distribution	O
n5.nx	O
;	O
o†/	O
.	O
here	O
nx	O
and	O
o†	O
are	O
the	O
usual	O
mle	O
estimates	O
for	O
the	O
expectation	O
togram	O
,	O
with	O
bseboot	O
d	O
0:070	O
compared	O
with	O
the	O
nonparametric	B
estimate	O
vector	B
and	O
covariance	O
matrix	B
based	O
on	O
the	O
22	O
ﬁve-component	O
student	B
score	I
vectors	O
.	O
it	O
is	O
narrower	O
than	O
the	O
corresponding	O
nonparametric	B
bootstrap	O
his-	O
6	O
these	O
are	O
the	O
binomial	B
standard	O
errors	B
œyk.n	O
(	O
cid:0	O
)	O
yk/=n1=2	O
,	O
n	O
d	O
211.	O
the	O
nonparametric	B
results	O
look	O
much	O
more	O
competitive	O
when	O
estimating	O
cdf	B
’	O
s	O
rather	O
than	O
densities	O
.	O
10.4	O
the	O
parametric	B
bootstrap	O
173	O
figure	O
10.6	O
eigenratio	O
example	O
,	O
student	B
score	I
data	O
.	O
solid	O
histogram	O
b	O
d	O
2000	O
parametric	B
bootstrap	O
replications	O
o	O
the	O
ﬁve-dimensional	O
normal	B
mle	O
;	O
line	O
histogram	O
the	O
2000	O
nonparametric	B
replications	O
of	O
figure	O
10.2.	O
mle	O
o	O
	O
d	O
:693	O
is	O
vertical	O
red	O
line	O
.	O
	O
(	O
cid:3	O
)	O
from	O
0.075	O
.	O
(	O
note	O
the	O
different	O
histogram	O
bin	O
limits	O
from	O
figure	O
10.2	O
,	O
changing	O
the	O
details	O
of	O
the	O
nonparametric	B
histogram	O
.	O
)	O
parametric	B
families	O
act	O
as	O
regularizers	O
,	O
smoothing	B
out	O
the	O
raw	O
data	B
and	O
de-emphasizing	O
outliers	O
.	O
in	O
fact	O
the	O
student	B
score	I
data	O
is	O
not	O
a	O
good	O
can-	O
didate	O
for	O
normal	B
modeling	O
,	O
having	O
at	O
least	O
one	O
notable	O
outlier,7	O
casting	O
doubt	O
on	O
the	O
smaller	O
estimate	B
of	I
standard	I
error	I
.	O
the	O
classical	O
statistician	O
could	O
only	O
imagine	O
a	O
mathematical	O
device	O
that	O
given	O
any	O
statistic	B
o	O
	O
d	O
s.x/	O
would	O
produce	O
a	O
formula	B
for	O
its	O
standard	O
er-	O
ror	O
,	O
as	O
formula	B
(	O
1.2	O
)	O
does	O
for	O
nx	O
.	O
the	O
electronic	O
computer	O
is	O
such	O
a	O
device	O
.	O
as	O
harnessed	O
by	O
the	O
bootstrap	O
,	O
it	O
automatically	O
produces	O
a	O
numerical	O
esti-	O
mate	O
of	O
standard	B
error	I
(	O
though	O
not	O
a	O
formula	B
)	O
,	O
with	O
no	O
further	O
cleverness	O
required	O
.	O
chapter	O
11	O
discusses	O
a	O
more	O
ambitious	O
substitution	O
of	O
computer	O
power	O
for	O
mathematical	O
analysis	B
:	O
the	O
bootstrap	O
computation	O
of	O
conﬁdence	B
intervals	I
.	O
7	O
as	O
revealed	O
by	O
examining	O
scatterplots	O
of	O
the	O
ﬁve	O
variates	O
taken	O
two	O
at	O
a	O
time	O
.	O
fast	O
and	O
painless	O
plotting	O
is	O
another	O
advantage	O
for	O
twenty-ﬁrst-century	O
data	B
analysts	O
.	O
eigenratio*frequency0.40.50.60.70.80.9020406080100120bootstrap	O
standard	O
errorsnonparametric	O
.075parametric	O
.070	O
174	O
the	O
jackknife	O
and	O
the	O
bootstrap	O
10.5	O
inﬂuence	O
functions	O
and	O
robust	O
estimation	B
the	O
sample	B
mean	O
played	O
a	O
dominant	O
role	O
in	O
classical	O
statistics	B
for	O
reasons	O
heavily	O
weighted	O
toward	O
mathematical	O
tractibility	O
.	O
beginning	O
in	O
the	O
1960s	O
,	O
an	O
important	O
counter-movement	O
,	O
robust	O
estimation	B
,	O
aimed	O
to	O
improve	O
upon	O
the	O
statistical	O
properties	O
of	O
the	O
mean	O
.	O
a	O
central	O
element	O
of	O
that	O
theory	B
,	O
the	O
inﬂuence	O
function	B
,	O
is	O
closely	O
related	O
to	O
the	O
jackknife	O
and	O
inﬁnitesimal	O
jack-	O
knife	O
estimates	O
of	O
standard	B
error	I
.	O
we	O
will	O
only	O
consider	O
the	O
case	O
where	O
x	O
,	O
the	O
sample	B
space	O
,	O
is	O
an	O
interval	B
of	O
the	O
real	O
line	O
.	O
the	O
unknown	O
probability	O
distribution	B
f	O
yielding	O
the	O
iid	O
sample	B
x	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn/	O
in	O
(	O
10.2	O
)	O
is	O
now	O
the	O
cdf	B
of	O
a	O
density	B
function	O
f	O
.x/	O
on	O
x	O
.	O
a	O
parameter	O
of	O
interest	O
,	O
i.e.	O
,	O
a	O
function	B
of	O
f	O
,	O
is	O
to	O
be	O
estimated	O
by	O
the	O
plug-in	O
principle	O
,	O
o	O
of	O
is	O
the	O
empirical	B
probability	O
distribution	B
putting	O
probability	O
1=n	O
on	O
each	O
sample	B
point	O
xi	O
.	O
for	O
the	O
mean	O
,	O
	O
d	O
t	B
.	O
of	O
/	O
,	O
where	O
,	O
as	O
in	O
section	O
10.2	O
,	O
	O
d	O
t	B
.f	O
/	O
dz	O
(	O
cid:16	O
)	O
of	O
	O
d	O
1	O
nx	O
	O
dr	O
xd	O
of	O
.x/	O
.	O
)	O
(	O
in	O
riemann–stieltjes	O
notation	O
,	O
	O
dr	O
xdf	O
.x/	O
and	O
o	O
xf	O
.x/	O
dx	O
and	O
o	O
	O
d	O
t	B
(	O
10.58	O
)	O
id1	O
xi	O
:	O
x	O
n	O
the	O
inﬂuence	O
function	B
of	O
t	B
.f	O
/	O
,	O
evaluated	O
at	O
point	O
x	O
in	O
x	O
,	O
is	O
deﬁned	O
to	O
be	O
if.x/	O
d	O
lim	O
(	O
cid:15	O
)	O
!	O
0	O
t	B
..1	O
(	O
cid:0	O
)	O
(	O
cid:15	O
)	O
/f	O
c	O
(	O
cid:15	O
)	O
ıx/	O
(	O
cid:0	O
)	O
t	B
.f	O
/	O
(	O
cid:15	O
)	O
;	O
(	O
10.59	O
)	O
where	O
ıx	O
is	O
the	O
“	O
one-point	O
probability	O
distribution	B
”	O
putting	O
probability	O
1	O
on	O
x.	O
in	O
words	O
,	O
if.x/	O
measures	O
the	O
differential	O
effect	O
of	O
modifying	O
f	O
by	O
putting	O
additional	O
probability	O
on	O
x.	O
for	O
the	O
mean	O
	O
dr	O
xf	O
.x/dx	O
we	O
cal-	O
culate	O
that	O
5	O
if.x/	O
d	O
x	O
(	O
cid:0	O
)	O
	O
:	O
a	O
fundamental	O
theorem	O
says	O
that	O
o	O
	O
d	O
t	B
.	O
of	O
/	O
is	O
approximately	O
nx	O
:	O
d	O
	O
c	O
1	O
if.xi	O
/	O
;	O
o	O
	O
(	O
10.60	O
)	O
(	O
10.61	O
)	O
n	O
id1	O
with	O
the	O
approximation	O
becoming	O
exact	O
as	O
n	O
goes	O
to	O
inﬁnity	O
.	O
this	O
implies	O
that	O
o	O
	O
(	O
cid:0	O
)	O
	O
is	O
,	O
approximately	O
,	O
the	O
mean	O
of	O
the	O
n	O
iid	O
variates	O
if.xi	O
/	O
,	O
and	O
that	O
the	O
variance	O
of	O
o	O
	O
is	O
approximately	O
varfif.x/g	O
;	O
(	O
10.62	O
)	O
n	O
o	O
	O
o	O
:	O
d	O
1	O
n	O
var	O
10.5	O
inﬂuence	O
functions	O
and	O
robust	O
estimation	B
175	O
varfif.x/g	O
being	O
the	O
variance	O
of	O
if.x/	O
for	O
any	O
one	O
draw	O
of	O
x	O
from	O
f	O
.	O
for	O
the	O
sample	B
mean	O
,	O
using	O
(	O
10.60	O
)	O
in	O
(	O
10.62	O
)	O
gives	O
the	O
familiar	O
equality	O
varfnxg	O
d	O
1	O
n	O
varfxg	O
:	O
(	O
10.63	O
)	O
the	O
sample	B
mean	O
suffers	O
from	O
an	O
unbounded	O
inﬂuence	O
function	B
(	O
10.60	O
)	O
,	O
which	O
grows	O
ever	O
larger	O
as	O
x	O
moves	O
farther	O
from	O
	O
.	O
this	O
makes	O
nx	O
unstable	O
against	O
heavy-tailed	O
densities	O
such	O
as	O
the	O
cauchy	O
(	O
4.39	O
)	O
.	O
robust	O
estimation	B
theory	O
seeks	O
estimators	O
o	O
	O
of	O
bounded	O
inﬂuence	O
,	O
that	O
do	O
well	O
against	O
heavy-	O
tailed	O
densities	O
without	O
giving	O
up	O
too	O
much	O
efﬁciency	O
against	O
light-tailed	O
densities	O
such	O
as	O
the	O
normal	B
.	O
of	O
particular	O
interest	O
have	O
been	O
the	O
trimmed	O
mean	O
and	O
its	O
close	O
cousin	O
the	O
winsorized	O
mean	O
.	O
d	O
˛	O
or	O
equivalently	O
let	O
x.˛/	O
denote	O
the	O
100˛th	O
percentile	O
of	O
distribution	B
f	O
,	O
satisfying	O
f	O
.x.˛//	O
˛	O
dz	O
x.˛/	O
(	O
cid:0	O
)	O
1	O
f	O
.x/	O
dx	O
:	O
z	O
x.1	O
(	O
cid:0	O
)	O
˛/	O
(	O
10.64	O
)	O
the	O
˛th	O
trimmed	O
mean	O
of	O
f	O
,	O
trim.˛/	O
,	O
is	O
deﬁned	O
as	O
trim.˛/	O
d	O
1	O
(	O
10.65	O
)	O
the	O
mean	O
of	O
the	O
central	O
1	O
(	O
cid:0	O
)	O
2˛	O
portion	O
of	O
f	O
,	O
trimming	O
off	O
the	O
lower	O
and	O
upper	O
˛	O
portions	O
.	O
this	O
is	O
not	O
the	O
same	O
as	O
the	O
˛th	O
winsorized	O
mean	O
wins.˛/	O
,	O
xf	O
.x/	O
dx	O
;	O
1	O
(	O
cid:0	O
)	O
2˛	O
x.˛/	O
where	O
wins.˛/	O
dz	O
8ˆ	O
<	O
ˆ	O
:	O
x.˛/	O
x.1	O
(	O
cid:0	O
)	O
˛/	O
x	O
w	O
.x/	O
d	O
x	O
w	O
.x/f	O
.x/	O
dx	O
;	O
(	O
10.66	O
)	O
if	O
x	O
	O
x.˛/	O
if	O
x.˛/	O
	O
x	O
	O
x.1	O
(	O
cid:0	O
)	O
˛/	O
if	O
x	O
(	O
cid:21	O
)	O
x.1	O
(	O
cid:0	O
)	O
˛/i	O
(	O
10.67	O
)	O
trim.˛/	O
removes	O
the	O
outer	O
portions	O
of	O
f	O
,	O
while	O
wins.˛/	O
moves	O
them	O
into	O
x.˛/	O
or	O
x.1	O
(	O
cid:0	O
)	O
˛/	O
.	O
in	O
practice	O
,	O
empirical	B
versions	O
o	O
wins.˛/	O
are	O
used	O
,	O
substituting	O
the	O
empirical	B
density	O
o	O
f	O
,	O
with	O
probability	O
1=n	O
at	O
each	O
xi	O
,	O
for	O
f	O
.	O
trim.˛/	O
and	O
o	O
there	O
turns	O
out	O
to	O
be	O
an	O
interesting	O
relationship	O
between	O
the	O
two	O
:	O
the	O
inﬂuence	O
function	B
of	O
trim.˛/	O
is	O
a	O
function	B
of	O
wins.˛/	O
,	O
if˛.x/	O
d	O
w	O
.x/	O
(	O
cid:0	O
)	O
wins.˛/	O
1	O
(	O
cid:0	O
)	O
2˛	O
:	O
(	O
10.68	O
)	O
this	O
is	O
pictured	O
in	O
figure	O
10.7	O
,	O
where	O
we	O
have	O
plotted	O
empirical	B
inﬂuence	O
176	O
the	O
jackknife	O
and	O
the	O
bootstrap	O
figure	O
10.7	O
empirical	B
inﬂuence	O
functions	O
for	O
the	O
47	O
leukemia	B
all	O
scores	O
of	O
figure	O
1.4.	O
the	O
two	O
dashed	O
curves	O
are	O
if˛.x/	O
for	O
the	O
trimmed	O
means	O
(	O
10.68	O
)	O
,	O
for	O
˛	O
d	O
0:2	O
and	O
˛	O
d	O
0:4.	O
the	O
solid	O
curve	O
is	O
if.x/	O
for	O
the	O
sample	B
mean	O
nx	O
(	O
10.60	O
)	O
.	O
functions	O
(	O
plugging	O
in	O
of	O
for	O
f	O
in	O
deﬁnition	O
(	O
10.59	O
)	O
)	O
relating	O
to	O
the	O
47	O
leukemia	B
all	O
scores	O
of	O
figure	O
1.4	O
:	O
if0:2.x/	O
and	O
if0:4.x/	O
are	O
plotted	O
,	O
along	O
with	O
if0.x/	O
(	O
10.60	O
)	O
,	O
that	O
is	O
,	O
for	O
the	O
mean	O
.	O
table	O
10.3	O
trimmed	O
means	O
and	O
their	O
bootstrap	O
standard	B
deviations	I
for	O
the	O
47	O
leukemia	B
all	O
scores	O
of	O
figure	O
1.4	O
;	O
b	O
d	O
1000	O
bootstrap	O
replications	O
for	O
each	O
trim	O
value	O
.	O
the	O
last	O
column	O
gives	O
empirical	B
inﬂuence	O
function	B
estimates	O
of	O
the	O
standard	B
error	I
,	O
which	O
are	O
also	O
the	O
inﬁnitesimal	O
jackknife	O
estimates	O
(	O
10.41	O
)	O
.	O
these	O
fail	O
for	O
the	O
median	O
.	O
trim	O
.0	O
.1	O
.2	O
.3	O
.4	O
.5	O
mean	O
median	O
trimmed	O
bootstrap	O
mean	O
sd	O
.752	O
.729	O
.720	O
.725	O
.734	O
.733	O
.040	O
.038	O
.035	O
.044	O
.047	O
.053	O
(	O
ifse	O
)	O
(	O
.040	O
)	O
(	O
.034	O
)	O
(	O
.034	O
)	O
(	O
.044	O
)	O
(	O
.054	O
)	O
0.20.40.60.81.01.21.41.6−0.50.00.51.0allinfluence	O
functionmeantrimmed	O
mean	O
a	O
=	O
0	O
.	O
2trimmed	O
mean	O
a	O
=	O
0	O
.	O
4	O
10.6	O
notes	O
and	O
details	O
177	O
the	O
upper	O
panel	O
of	O
figure	O
1.4	O
shows	O
a	O
moderately	O
heavy	O
right	O
tail	O
for	O
the	O
all	O
distribution	B
.	O
would	O
it	O
be	O
more	O
efﬁcient	O
to	O
estimate	B
the	O
center	O
of	O
vides	O
an	O
answer	O
:	O
bseboot	O
(	O
10.16	O
)	O
was	O
calculated	O
for	O
nx	O
and	O
o	O
the	O
distribution	B
with	O
a	O
trimmed	O
mean	O
rather	O
than	O
nx	O
?	O
the	O
bootstrap	O
pro-	O
trim.˛/	O
,	O
˛	O
d	O
0:1	O
;	O
0:2	O
;	O
0:3	O
;	O
0:4	O
,	O
and	O
0.5	O
,	O
the	O
last	O
being	O
the	O
sample	B
median	O
.	O
it	O
appears	O
that	O
o	O
trim.0:2/	O
is	O
moderately	O
better	O
than	O
nx	O
.	O
this	O
brings	O
up	O
an	O
important	O
question	O
discussed	O
in	O
chapter	O
20	O
:	O
if	O
we	O
use	O
something	O
like	O
table	O
10.3	O
to	O
select	O
an	O
estimator	B
,	O
how	O
does	O
the	O
selection	O
process	O
affect	O
the	O
accuracy	O
of	O
the	O
result-	O
ing	O
estimate	B
?	O
we	O
might	O
also	O
use	O
the	O
square	O
root	O
of	O
formula	B
(	O
10.62	O
)	O
to	O
estimate	B
the	O
standard	O
errors	O
of	O
the	O
various	O
estimators	O
,	O
plugging	O
in	O
the	O
empirical	B
inﬂu-	O
ence	O
function	B
for	O
if.x/	O
.	O
this	O
turns	O
out	O
to	O
be	O
the	O
same	O
as	O
using	O
the	O
inﬁnites-	O
imal	O
jackknife	O
(	O
10.41	O
)	O
.	O
these	O
appear	O
in	O
the	O
last	O
column	O
of	O
table	O
10.3.	O
pre-	O
dictably	O
,	O
this	O
approach	O
fails	O
for	O
the	O
sample	B
median	O
,	O
whose	O
inﬂuence	O
func-	O
tion	O
is	O
a	O
square	O
wave	O
,	O
sharply	O
discontinuous	O
at	O
the	O
median	O
	O
,	O
if.x/	O
d	O
˙1ı	O
.2f	O
.	O
//	O
:	O
(	O
10.69	O
)	O
robust	O
estimation	B
offers	O
a	O
nice	O
illustration	O
of	O
statistical	O
progress	O
in	O
the	O
computer	O
age	O
.	O
trimmed	O
means	O
go	O
far	O
back	O
into	O
the	O
classical	O
era	O
.	O
inﬂuence	O
functions	O
are	O
an	O
insightful	O
inferential	O
tool	O
for	O
understanding	O
the	O
tradeoffs	O
in	O
trimmed	O
mean	O
estimation	B
.	O
and	O
ﬁnally	O
the	O
bootstrap	O
allows	O
easy	O
assessment	O
of	O
the	O
accuracy	O
of	O
robust	O
estimation	B
,	O
including	O
some	O
more	O
elaborate	O
ones	O
not	O
discussed	O
here	O
.	O
10.6	O
notes	O
and	O
details	O
quenouille	O
(	O
1956	O
)	O
introduced	O
what	O
is	O
now	O
called	O
the	O
jackknife	O
estimate	O
of	O
bias	O
.	O
tukey	O
(	O
1958	O
)	O
realized	O
that	O
quenouille-type	O
calculations	O
could	O
be	O
repurposed	O
for	O
nonparametric	B
standard-error	O
estimation	B
,	O
inventing	O
formula	B
(	O
10.6	O
)	O
and	O
naming	O
it	O
“	O
the	O
jackknife	O
,	O
”	O
as	O
a	O
rough	O
and	O
ready	O
tool	O
.	O
miller	O
’	O
s	O
im-	O
portant	O
1964	O
paper	O
,	O
“	O
a	O
trustworthy	O
jackknife	O
,	O
”	O
asked	O
when	O
formula	B
(	O
10.6	O
)	O
could	O
be	O
trusted	O
.	O
(	O
not	O
for	O
the	O
median	O
.	O
)	O
the	O
bootstrap	O
(	O
efron	O
,	O
1979	O
)	O
began	O
as	O
an	O
attempt	O
to	O
better	O
understand	O
the	O
jackknife	O
’	O
s	O
successes	O
and	O
failures	O
.	O
its	O
name	O
celebrates	O
baron	O
mun-	O
chausen	O
’	O
s	O
success	O
in	O
pulling	O
himself	O
up	O
by	O
his	O
own	O
bootstraps	O
from	O
the	O
bottom	O
of	O
a	O
lake	O
.	O
burgeoning	O
computer	O
power	O
soon	O
overcame	O
the	O
boot-	O
strap	O
’	O
s	O
main	O
drawback	O
,	O
prodigous	O
amounts	O
of	O
calculation	O
,	O
propelling	O
it	O
into	O
general	O
use	O
.	O
meanwhile	O
,	O
1000c	O
theoretical	O
papers	O
were	O
published	O
asking	O
when	O
the	O
bootstrap	O
itself	O
could	O
be	O
trusted	O
.	O
(	O
most	O
but	O
not	O
all	O
of	O
the	O
time	O
in	O
common	O
practice	O
)	O
.	O
178	O
the	O
jackknife	O
and	O
the	O
bootstrap	O
a	O
main	O
reference	O
for	O
the	O
chapter	O
is	O
efron	O
’	O
s	O
1982	O
monograph	O
the	O
jack-	O
knife	O
,	O
the	O
bootstrap	O
and	O
other	O
resampling	O
plans	B
.	O
its	O
chapter	O
6	O
shows	O
the	O
equality	O
of	O
three	O
nonparametric	B
standard	O
error	O
estimates	O
:	O
jaeckel	O
’	O
s	O
(	O
1972	O
)	O
inﬁnitesimal	O
jackknife	O
(	O
10.41	O
)	O
;	O
the	O
empirical	B
inﬂuence	O
function	B
estimate	O
,	O
based	O
on	O
(	O
10.62	O
)	O
;	O
and	O
what	O
is	O
known	O
as	O
the	O
nonparametric	B
delta	O
method	B
.	O
bootstrap	O
packages	B
various	O
bootstrap	O
packages	B
in	O
r	O
are	O
available	O
on	O
the	O
cran	O
contributed-	O
packages	B
web	O
site	O
,	O
bootstrap	O
being	O
an	O
ambitious	O
one	O
.	O
algorithm	B
10.1	O
shows	O
a	O
simple	O
r	O
program	O
for	O
nonparametric	B
bootstrapping	O
.	O
aside	O
from	O
bookkeeping	O
,	O
it	O
’	O
s	O
only	O
a	O
few	O
lines	O
long	O
.	O
algorithm	B
10.1	O
an	O
r	O
program	O
for	O
the	O
nonparametric	B
bootstrap	O
.	O
boot	O
<	O
-	O
function	B
(	O
x	O
,	O
b	O
,	O
func	O
,	O
...	O
)	O
{	O
#	O
x	O
is	O
data	B
vector	O
or	O
matrix	B
(	O
with	O
each	O
row	O
a	O
case	O
)	O
#	O
b	O
is	O
number	O
of	O
bootstrap	O
replications	O
#	O
func	O
is	O
r	O
function	B
that	O
inputs	O
a	O
data	B
vector	O
or	O
#	O
matrix	B
and	O
returns	O
a	O
numeric	O
number	O
or	O
vector	B
#	O
...	O
other	O
arguments	O
for	O
func	O
x	O
<	O
-	O
as.matrix	O
(	O
x	O
)	O
n	O
<	O
-	O
nrow	O
(	O
x	O
)	O
f0=func	O
(	O
x	O
,	O
...	O
)	O
#	O
get	O
size	O
of	O
output	O
fmat	O
<	O
-	O
matrix	B
(	O
0	O
,	O
length	O
(	O
f0	O
)	O
,	O
b	O
)	O
for	O
(	O
b	O
in	O
1	O
:	O
b	O
)	O
{	O
i=sample	O
(	O
1	O
:	O
n	O
,	O
n	O
,	O
replace	O
=	O
true	O
)	O
fmat	O
[	O
,	O
b	O
]	O
<	O
-	O
func	O
(	O
x	O
[	O
i	O
,	O
]	O
,	O
...	O
)	O
}	O
drop	O
(	O
fmat	O
)	O
}	O
1	O
[	O
p.	O
158	O
]	O
the	O
jackknife	O
standard	O
error	O
.	O
the	O
1982	O
monograph	O
also	O
contains	O
efron	O
and	O
stein	O
’	O
s	O
(	O
1981	O
)	O
result	O
on	O
the	O
bias	O
of	O
the	O
jackknife	O
variance	O
esti-	O
mate	O
,	O
the	O
square	O
of	O
formula	B
(	O
10.6	O
)	O
:	O
modulo	O
certain	O
sample	B
size	I
considera-	O
tions	O
,	O
the	O
expectation	O
of	O
the	O
jackknife	O
variance	O
estimate	B
is	O
biased	O
upward	O
for	O
the	O
true	O
variance	O
.	O
for	O
the	O
sample	B
mean	O
nx	O
,	O
the	O
jackknife	O
yields	O
exactly	O
the	O
usual	O
variance	O
i	O
.xi	O
(	O
cid:0	O
)	O
nx/2=.n.n	O
(	O
cid:0	O
)	O
1//	O
,	O
while	O
the	O
ideal	O
bootstrap	O
estimate	B
estimate	O
(	O
1.2	O
)	O
,	O
p	O
(	O
b	O
!	O
1	O
)	O
gives	O
nx	O
.xi	O
(	O
cid:0	O
)	O
nx/2=n2	O
:	O
id1	O
(	O
10.70	O
)	O
10.6	O
notes	O
and	O
details	O
179	O
as	O
with	O
the	O
jackknife	O
,	O
we	O
could	O
append	O
a	O
fudge	O
factor	B
to	O
get	O
perfect	O
agree-	O
ment	O
with	O
(	O
1.2	O
)	O
,	O
but	O
there	O
is	O
no	O
real	O
gain	O
in	O
doing	O
so	O
.	O
2	O
[	O
p.	O
161	O
]	O
bootstrap	O
sample	B
sizes	O
.	O
letbseb	O
indicate	O
the	O
bootstrap	O
standard	O
er-	O
ror	O
estimate	B
(	O
10.16	O
)	O
based	O
on	O
b	O
replications	O
,	O
andbse1	O
the	O
“	O
ideal	O
bootstrap	O
,	O
”	O
creasing	O
b	O
past	O
a	O
certain	O
point	O
,	O
becausebse1	O
is	O
itself	O
a	O
statistic	B
whose	O
value	O
b	O
!	O
1.	O
in	O
any	O
actual	O
application	O
,	O
there	O
are	O
diminishing	O
returns	O
from	O
in-	O
varies	O
with	O
the	O
observed	O
sample	B
x	O
(	O
as	O
in	O
(	O
10.70	O
)	O
)	O
,	O
leaving	O
an	O
irreducible	O
re-	O
mainder	O
of	O
randomness	O
in	O
any	O
standard	B
error	I
estimate	O
.	O
section	O
6.4	O
of	O
efron	O
and	O
tibshirani	O
(	O
1993	O
)	O
shows	O
that	O
b	O
d	O
200	O
will	O
almost	O
always	O
be	O
plenty	O
(	O
for	O
standard	O
errors	O
,	O
but	O
not	O
for	O
bootstrap	O
conﬁdence	B
intervals	I
,	O
chapter	O
11	O
)	O
.	O
smaller	O
numbers	O
,	O
25	O
or	O
even	O
less	O
,	O
can	O
still	O
be	O
quite	O
useful	O
in	O
complicated	O
situations	O
where	O
resampling	O
is	O
expensive	O
.	O
an	O
early	O
complaint	O
,	O
“	O
bootstrap	O
estimates	O
are	O
random	O
,	O
”	O
is	O
less	O
often	O
heard	O
in	O
an	O
era	O
of	O
frequent	O
and	O
massive	O
simulations	O
.	O
3	O
[	O
p.	O
161	O
]	O
the	O
bayesian	O
bootstrap	O
.	O
rubin	O
(	O
1981	O
)	O
suggested	O
the	O
bayesian	O
bootstrap	O
(	O
10.44	O
)	O
.	O
section	O
10.6	O
of	O
efron	O
(	O
1982	O
)	O
used	O
(	O
10.45	O
)	O
–	O
(	O
10.46	O
)	O
as	O
an	O
objective	O
bayes	O
justiﬁcation	O
for	O
what	O
we	O
will	O
call	O
the	O
percentile-method	O
bootstrap	O
conﬁdence	B
intervals	I
in	O
chapter	O
12	O
.	O
4	O
[	O
p.	O
161	O
]	O
jackknife-after-bootstrap	O
.	O
for	O
the	O
eigenratio	O
example	O
displayed	O
in	O
just	O
the	O
original	O
2000	O
replications	O
.	O
0:075.	O
how	O
accurate	O
is	O
this	O
value	O
?	O
bootstrapping	O
the	O
bootstrap	O
seems	O
like	O
too	O
much	O
work	O
,	O
perhaps	O
200	O
times	O
2000	O
resamples	O
.	O
it	O
turns	O
out	O
,	O
though	O
,	O
figure	O
10.2	O
,	O
b	O
d	O
2000	O
nonparametric	B
bootstrap	O
replications	O
gavebseboot	O
d	O
that	O
we	O
can	O
use	O
the	O
jackknife	O
to	O
estimate	B
the	O
variability	O
ofbseboot	O
based	O
on	O
now	O
the	O
deleted	O
sample	B
estimate	O
in	O
(	O
10.6	O
)	O
isbseboot.i	O
/	O
.	O
the	O
key	O
idea	O
is	O
applying	O
deﬁnition	O
(	O
10.16	O
)	O
to	O
this	O
subset	O
givesbseboot.i	O
/	O
.	O
for	O
the	O
estimate	O
of	O
figure	O
10.2	O
,	O
the	O
jackknife-after-bootstrap	O
calculations	O
gavebsejack	O
d	O
0:022	O
for	O
bseboot	O
d	O
0:075.	O
in	O
other	O
words	O
,	O
0.075	O
isn	O
’	O
t	B
very	O
accurate	O
,	O
which	O
is	O
to	O
be	O
expected	O
for	O
the	O
standard	B
error	I
of	O
a	O
complicated	O
statistic	B
estimated	O
from	O
only	O
n	O
d	O
22	O
observations	O
.	O
an	O
inﬁnitesimal	O
jackknife	O
version	O
of	O
this	O
technique	O
will	O
play	O
a	O
major	O
role	O
in	O
chapter	O
20	O
.	O
(	O
cid:3	O
)	O
(	O
10.13	O
)	O
,	O
among	O
the	O
original	O
2000	O
,	O
to	O
consider	O
those	O
bootstrap	O
samples	O
x	O
that	O
do	O
not	O
include	O
the	O
point	O
xi	O
.	O
about	O
37	O
%	O
of	O
the	O
original	O
b	O
samples	O
will	O
be	O
in	O
this	O
subset	O
.	O
section	O
19.4	O
of	O
efron	O
and	O
tibshirani	O
(	O
1993	O
)	O
shows	O
that	O
5	O
[	O
p.	O
174	O
]	O
a	O
fundamental	O
theorem	B
.	O
tukey	O
can	O
justly	O
be	O
considered	O
the	O
found-	O
ing	O
father	O
of	O
robust	O
statistics	B
,	O
his	O
1960	O
paper	O
being	O
especially	O
inﬂuential	O
.	O
huber	O
’	O
s	O
celebrated	O
1964	O
paper	O
brought	O
the	O
subject	O
into	O
the	O
realm	O
of	O
high-	O
concept	O
mathematical	O
statistics	B
.	O
robust	O
statistics	B
:	O
the	O
approach	O
based	O
on	O
inﬂuence	O
functions	O
,	O
the	O
1986	O
book	O
by	O
hampel	O
et	O
al.	O
,	O
conveys	O
the	O
breadth	O
of	O
a	O
subject	O
only	O
lightly	O
scratched	O
in	O
our	O
section	O
10.5.	O
hampel	O
(	O
1974	O
)	O
180	O
the	O
jackknife	O
and	O
the	O
bootstrap	O
introduced	O
the	O
inﬂuence	O
function	B
as	O
a	O
statistical	O
tool	O
.	O
boos	O
and	O
serﬂing	O
(	O
1980	O
)	O
veriﬁed	O
expression	O
(	O
10.62	O
)	O
.	O
qualitative	O
notions	O
of	O
robustness	O
,	O
more	O
than	O
speciﬁc	O
theoretical	O
results	O
,	O
have	O
had	O
a	O
continuing	O
inﬂuence	O
on	O
modern	O
data	B
analysis	O
.	O
11	O
bootstrap	O
conﬁdence	B
intervals	I
the	O
jackknife	O
and	O
the	O
bootstrap	O
represent	O
a	O
different	O
use	O
of	O
modern	O
com-	O
puter	O
power	O
:	O
rather	O
than	O
extending	O
classical	O
methodology—from	O
ordinary	O
least	B
squares	I
to	O
generalized	O
linear	B
models	O
,	O
for	O
example—they	O
extend	O
the	O
reach	O
of	O
classical	O
inference	B
.	O
chapter	O
10	O
focused	O
on	O
standard	O
errors	O
.	O
here	O
we	O
will	O
take	O
up	O
a	O
more	O
am-	O
bitious	O
inferential	O
goal	O
,	O
the	O
bootstrap	O
automation	O
of	O
conﬁdence	B
intervals	I
.	O
the	O
familiar	O
standard	O
intervals	O
o	O
	O
˙	O
1:96bse	O
;	O
	O
d	O
10	O
from	O
a	O
poisson	O
model	B
o	O
(	O
11.1	O
)	O
for	O
approximate	O
95	O
%	O
coverage	O
,	O
are	O
immensely	O
useful	O
in	O
practice	O
but	O
often	O
the	O
standard	O
95	O
%	O
interval	B
.3:8	O
;	O
16:2/	O
(	O
usingbse	O
d	O
o	O
not	O
very	O
accurate	O
.	O
if	O
we	O
observe	O
o	O
	O
(	O
cid:24	O
)	O
poi.	O
/	O
,	O
	O
1=2	O
)	O
is	O
a	O
mediocre	O
ap-	O
proximation	O
to	O
the	O
exact	O
interval1	O
.5:1	O
;	O
17:8/	O
:	O
(	O
11.2	O
)	O
standard	O
intervals	O
(	O
11.1	O
)	O
are	O
symmetric	O
around	O
o	O
	O
,	O
this	O
being	O
their	O
main	O
weakness	O
.	O
poisson	O
distributions	O
grow	O
more	O
variable	O
as	O
	O
increases	O
,	O
which	O
is	O
why	O
interval	B
(	O
11.2	O
)	O
extends	O
farther	O
to	O
the	O
right	O
of	O
o	O
	O
d	O
10	O
than	O
to	O
the	O
left	O
.	O
correctly	O
capturing	O
such	O
effects	O
in	O
an	O
automatic	O
way	O
is	O
the	O
goal	O
of	O
bootstrap	O
conﬁdence	B
interval	I
theory	O
.	O
11.1	O
neyman	O
’	O
s	O
construction	O
for	O
one-parameter	B
the	O
student	B
score	I
data	O
of	O
table	O
3.1	O
comprised	O
n	O
d	O
22	O
pairs	O
,	O
problems	O
xi	O
d	O
.mi	O
;	O
vi	O
/	O
;	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
22	O
;	O
(	O
11.3	O
)	O
1	O
using	O
the	O
neyman	O
construction	O
of	O
section	O
11.1	O
,	O
as	O
explained	O
there	O
;	O
see	O
also	O
table	O
11.2	O
in	O
section	O
11.4	O
.	O
181	O
bootstrap	O
conﬁdence	B
intervals	I
182	O
where	O
mi	O
and	O
vi	O
were	O
student	O
i	O
’	O
s	O
scores	O
on	O
the	O
“	O
mechanics	O
”	O
and	O
“	O
vectors	O
”	O
tests	O
.	O
the	O
sample	B
correlation	O
coefﬁcient	O
o	O
	O
between	O
mi	O
and	O
vi	O
was	O
com-	O
puted	O
to	O
be	O
o	O
	O
d	O
0:498	O
:	O
(	O
11.4	O
)	O
question	O
:	O
what	O
can	O
we	O
infer	O
about	O
the	O
true	O
correlation	O
	O
between	O
m	O
and	O
v	O
?	O
figure	O
3.2	O
displayed	O
three	O
possible	O
bayesian	O
answers	O
.	O
conﬁdence	B
intervals	I
provide	O
the	O
frequentist	O
solution	O
,	O
by	O
far	O
the	O
most	O
popular	O
in	O
applied	O
practice	O
.	O
figure	O
11.1	O
the	O
solid	O
curve	O
is	O
the	O
normal	B
correlation	O
coefﬁcient	O
density	B
fo	O
.r/	O
(	O
3.11	O
)	O
for	O
o	O
student	B
score	I
data	O
;	O
o	O
endpoints	O
of	O
the	O
95	O
%	O
conﬁdence	B
interval	I
for	O
	O
,	O
with	O
corresponding	O
densities	O
shown	O
by	O
dashed	O
curves	O
.	O
these	O
yield	O
tail	O
areas	O
0.025	O
at	O
o	O
	O
d	O
0:498	O
,	O
the	O
mle	O
estimate	B
for	O
the	O
	O
.up/	O
d	O
0:751	O
are	O
the	O
	O
.lo/	O
d	O
0:093	O
and	O
o	O
	O
(	O
11.6	O
)	O
.	O
suppose	O
,	O
ﬁrst	O
,	O
that	O
we	O
assume	O
a	O
bivariate	O
normal	B
model	O
(	O
5.12	O
)	O
for	O
the	O
o	O
	O
/	O
for	O
sample	B
corre-	O
pairs	O
.mi	O
;	O
vi	O
/	O
.	O
in	O
that	O
case	O
the	O
probability	O
density	B
f	O
.	O
lation	O
o	O
	O
given	O
true	O
correlation	O
	O
has	O
known	O
form	B
(	O
3.11	O
)	O
.	O
the	O
solid	O
curve	O
in	O
figure	O
11.1	O
graphs	O
f	O
for	O
	O
d	O
0:498	O
,	O
that	O
is	O
,	O
for	O
	O
set	B
equal	O
to	O
the	O
observed	O
value	O
o	O
	O
.	O
in	O
more	O
careful	O
notation	O
,	O
the	O
curve	O
graphs	O
fo	O
.r/	O
as	O
a	O
function	B
of	O
the	O
dummy	O
variable2	O
r	O
taking	O
values	O
in	O
œ	O
(	O
cid:0	O
)	O
1	O
;	O
1	O
.	O
2	O
this	O
is	O
an	O
example	O
of	O
a	O
parametric	B
bootstrap	O
distribution	B
(	O
10.49	O
)	O
,	O
here	O
with	O
o	O
(	O
cid:22	O
)	O
being	O
o	O
.	O
−0.4−0.20.00.20.40.60.81.001234correlationdensity0.0930.7510.498l0.0250.025	O
11.1	O
neyman	O
’	O
s	O
construction	O
two	O
other	O
curves	O
f	O
.r/	O
appear	O
in	O
figure	O
11.1	O
:	O
for	O
	O
equaling	O
these	O
were	O
numerically	O
calculated	O
as	O
the	O
solutions	O
to	O
o	O
o	O
	O
.lo/	O
d	O
0:093	O
and	O
	O
.up/	O
d	O
0:751	O
:	O
z	O
o	O
183	O
(	O
11.5	O
)	O
(	O
11.6	O
)	O
(	O
11.7	O
)	O
z	O
1	O
o	O
in	O
words	O
,	O
o	O
above	O
o	O
0.025	O
below	O
o	O
	O
;	O
fo	O
.lo/.r/	O
dr	O
d	O
0:025	O
and	O
fo	O
.up/.r/	O
dr	O
d	O
0:025	O
:	O
(	O
cid:0	O
)	O
1	O
	O
d	O
0:498	O
,	O
while	O
o	O
	O
.lo/	O
is	O
the	O
smallest	O
value	O
of	O
	O
putting	O
probability	O
at	O
least	O
0.025	O
	O
.up/	O
is	O
the	O
largest	O
value	O
with	O
probability	O
at	O
least	O
	O
2h	O
o	O
o	O
	O
.up/	O
	O
.lo/	O
;	O
i	O
is	O
a	O
95	O
%	O
conﬁdence	B
interval	I
for	O
the	O
true	O
correlation	O
,	O
statement	O
(	O
11.7	O
)	O
hold-	O
ing	O
true	O
with	O
probability	O
0.95	O
,	O
for	O
every	O
possible	O
value	O
of	O
	O
.	O
we	O
have	O
just	O
described	O
neyman	O
’	O
s	O
construction	O
of	O
conﬁdence	B
intervals	I
o	O
for	O
one-parameter	B
problems	O
f	O
.	O
	O
/	O
.	O
(	O
later	O
we	O
will	O
consider	O
the	O
more	O
difﬁ-	O
cult	O
situation	O
where	O
there	O
are	O
“	O
nuisance	O
parameters	O
”	O
in	O
addition	O
to	O
the	O
pa-	O
rameter	O
of	O
interest	O
	O
.	O
)	O
one	O
of	O
the	O
jewels	O
of	O
classical	O
frequentist	O
inference	B
,	O
it	O
depends	O
on	O
a	O
pivotal	O
argument—	O
“	O
ingenious	O
device	O
”	O
number	O
5	O
of	O
sec-	O
tion	O
2.1—to	O
show	O
that	O
it	O
produces	O
genuine	O
conﬁdence	B
intervals	I
,	O
i.e.	O
,	O
ones	O
that	O
contain	O
the	O
true	O
parameter	O
value	O
	O
at	O
the	O
claimed	O
probability	O
level	O
,	O
0.95	O
in	O
figure	O
11.1.	O
the	O
argument	B
appears	O
in	O
the	O
chapter	O
endnotes.	O
for	O
the	O
poisson	O
calculation	O
(	O
11.2	O
)	O
it	O
was	O
necessary	O
to	O
deﬁne	O
exactly	O
what	O
“	O
the	O
smallest	O
value	O
of	O
	O
putting	O
probability	O
at	O
least	O
0.025	O
above	O
o	O
	O
”	O
meant	O
.	O
o	O
	O
/	O
at	O
this	O
was	O
done	O
assuming	O
that	O
,	O
for	O
any	O
	O
,	O
half	O
of	O
the	O
probability	O
f	O
.	O
o	O
	O
d	O
10	O
counted	O
as	O
“	O
above	O
,	O
”	O
and	O
similarly	O
for	O
calculating	O
the	O
upper	O
limit	O
.	O
1	O
transformation	O
invariance	O
conﬁdence	B
intervals	I
enjoy	O
the	O
important	O
and	O
useful	O
property	O
of	O
transfor-	O
mation	O
invariance	O
.	O
in	O
the	O
poisson	O
example	O
(	O
11.2	O
)	O
,	O
suppose	O
our	O
interest	O
shifts	O
from	O
parameter	O
	O
to	O
parameter	O
(	O
cid:30	O
)	O
d	O
log	O
	O
.	O
the	O
95	O
%	O
exact	O
inter-	O
val	O
(	O
11.2	O
)	O
for	O
	O
then	O
transforms	O
to	O
the	O
exact	O
95	O
%	O
interval	B
for	O
(	O
cid:30	O
)	O
simply	O
by	O
taking	O
logs	O
of	O
the	O
endpoints	O
,	O
.log.5:1/	O
;	O
log.17:8//	O
d	O
.1:63	O
;	O
2:88/	O
:	O
(	O
11.8	O
)	O
to	O
state	O
things	O
generally	O
,	O
suppose	O
we	O
observe	O
o	O
	O
from	O
a	O
family	O
of	O
densi-	O
o	O
o	O
	O
/	O
for	O
	O
of	O
coverage	O
level	O
	O
/	O
and	O
construct	O
a	O
conﬁdence	B
interval	I
c.	O
ties	O
f	O
.	O
bootstrap	O
conﬁdence	B
intervals	I
184	O
˛	O
(	O
˛	O
d	O
0:95	O
in	O
our	O
examples	O
)	O
.	O
now	O
let	O
parameter	O
(	O
cid:30	O
)	O
be	O
a	O
monotonic	O
in-	O
creasing	O
function	B
of	O
	O
,	O
say	O
(	O
m.	O
/	O
d	O
log	O
	O
in	O
(	O
11.8	O
)	O
)	O
,	O
and	O
likewise	O
o	O
(	O
cid:30	O
)	O
d	O
m.	O
then	O
c.	O
for	O
(	O
cid:30	O
)	O
,	O
o	O
	O
/	O
maps	O
point	O
by	O
point	O
into	O
c	O
(	O
cid:30	O
)	O
.	O
o	O
(	O
cid:30	O
)	O
/	O
,	O
a	O
level-˛	O
conﬁdence	B
interval	I
o	O
	O
/	O
for	O
the	O
point	O
estimate	B
.	O
(	O
11.9	O
)	O
(	O
cid:30	O
)	O
d	O
m.	O
/	O
(	O
cid:30	O
)	O
d	O
m.	O
/	O
for	O
	O
2	O
(	O
11.10	O
)	O
o	O
this	O
just	O
says	O
that	O
the	O
event	O
f	O
2	O
	O
/g	O
is	O
the	O
same	O
as	O
the	O
event	O
f	O
(	O
cid:30	O
)	O
2	O
c	O
(	O
cid:30	O
)	O
.	O
o	O
(	O
cid:30	O
)	O
/g	O
,	O
so	O
if	O
the	O
former	O
always	O
occurs	O
with	O
probability	O
˛	O
then	O
so	O
must	O
the	O
c.	O
c	O
:	O
latter	O
.	O
c	O
(	O
cid:30	O
)	O
.	O
o	O
(	O
cid:30	O
)	O
/	O
dn	O
o	O
(	O
cid:16	O
)	O
o	O
	O
figure	O
11.2	O
the	O
situation	O
in	O
figure	O
11.1	O
after	O
transformation	O
to	O
(	O
cid:30	O
)	O
d	O
m.	O
/	O
according	O
to	O
(	O
11.11	O
)	O
.	O
the	O
curves	O
are	O
nearly	O
n.	O
(	O
cid:30	O
)	O
;	O
(	O
cid:27	O
)	O
2/	O
with	O
standard	B
deviation	I
(	O
cid:27	O
)	O
d	O
1=	O
p	O
19	O
d	O
0:229.	O
transformation	O
invariance	O
has	O
an	O
historical	O
resonance	O
with	O
the	O
normal	B
o	O
correlation	B
coefﬁcient	I
.	O
fisher	O
’	O
s	O
derivation	O
of	O
f	O
.	O
	O
/	O
(	O
3.11	O
)	O
in	O
1915	O
was	O
a	O
mathematical	O
triumph	O
,	O
but	O
a	O
difﬁcult	O
one	O
to	O
exploit	O
in	O
an	O
era	O
of	O
mechanical	O
computation	O
.	O
most	O
ingeniously	O
,	O
fisher	O
suggested	O
instead	O
working	O
with	O
the	O
transformed	O
parameter	O
(	O
cid:30	O
)	O
d	O
m.	O
/	O
where	O
	O
	O
1	O
c	O
	O
1	O
(	O
cid:0	O
)	O
	O
(	O
cid:30	O
)	O
d	O
m.	O
/	O
d	O
1	O
2	O
log	O
;	O
(	O
11.11	O
)	O
−0.50.00.51.01.50.00.51.01.5fisher¢s	O
f	O
transformdensity0.0930.9750.546l0.0250.025	O
and	O
likewise	O
with	O
statistic	B
o	O
(	O
cid:30	O
)	O
d	O
m.	O
imation	O
,	O
11.2	O
the	O
percentile	B
method	I
185	O
o	O
	O
/	O
.	O
then	O
,	O
to	O
a	O
surprisingly	O
good	O
approx-	O
:	O
(	O
11.12	O
)	O
see	O
figure	O
11.2	O
,	O
which	O
shows	O
neyman	O
’	O
s	O
construction	O
on	O
the	O
(	O
cid:30	O
)	O
scale	B
.	O
in	O
other	O
words	O
,	O
we	O
are	O
back	O
in	O
fisher	O
’	O
s	O
favored	O
situation	O
(	O
4.31	O
)	O
,	O
the	O
sim-	O
ple	O
normal	B
translation	O
problem	O
,	O
where	O
	O
	O
o	O
(	O
cid:30	O
)	O
p	O
(	O
cid:24	O
)	O
n	O
(	O
cid:30	O
)	O
;	O
1	O
n	O
(	O
cid:0	O
)	O
3	O
c	O
(	O
cid:30	O
)	O
(	O
cid:16	O
)	O
o	O
(	O
cid:30	O
)	O
	O
d	O
o	O
(	O
cid:30	O
)	O
˙	O
1:96	O
1p	O
n	O
(	O
cid:0	O
)	O
3	O
(	O
11.13	O
)	O
is	O
the	O
“	O
obviously	O
correct	O
”	O
95	O
%	O
conﬁdence	O
interval3	O
for	O
(	O
cid:30	O
)	O
,	O
closely	O
approx-	O
imating	O
neyman	O
’	O
s	O
construction	O
.	O
the	O
endpoints	O
of	O
(	O
11.13	O
)	O
are	O
then	O
trans-	O
formed	O
back	O
to	O
the	O
	O
scale	B
according	O
to	O
the	O
inverse	O
transformation	O
	O
d	O
e2	O
(	O
cid:30	O
)	O
(	O
cid:0	O
)	O
1	O
e2	O
(	O
cid:30	O
)	O
c	O
1	O
o	O
	O
/	O
seen	O
in	O
figure	O
11.1	O
,	O
but	O
without	O
the	O
in-	O
(	O
11.14	O
)	O
;	O
giving	O
(	O
almost	O
)	O
the	O
interval	B
c.	O
volved	O
computations	B
.	O
bayesian	O
conﬁdence	O
statements	O
are	O
inherently	O
transformation	O
invariant	O
.	O
the	O
fact	O
that	O
the	O
neyman	O
intervals	B
are	O
also	O
invariant	O
,	O
unlike	O
the	O
standard	O
intervals	O
(	O
11.1	O
)	O
,	O
has	O
made	O
them	O
more	O
palatable	O
to	O
bayesian	O
statisticians	O
.	O
transformation	O
invariance	O
will	O
play	O
a	O
major	O
role	O
in	O
justifying	O
the	O
bootstrap	O
conﬁdence	B
intervals	I
introduced	O
next	O
.	O
11.2	O
the	O
percentile	B
method	I
our	O
goal	O
is	O
to	O
automate	O
the	O
calculation	O
of	O
conﬁdence	B
intervals	I
:	O
given	O
the	O
bootstrap	O
distribution	B
of	O
a	O
statistical	O
estimator	B
o	O
	O
,	O
we	O
want	O
to	O
automatically	O
produce	O
an	O
appropriate	O
conﬁdence	B
interval	I
for	O
the	O
unseen	O
parameter	O
	O
.	O
to	O
this	O
end	O
,	O
a	O
series	O
of	O
four	O
increasingly	O
accurate	O
bootstrap	O
conﬁdence	B
interval	I
algorithms	O
will	O
be	O
described	O
.	O
	O
˙	O
1:96bse	O
for	O
95	O
%	O
coverage	O
,	O
withbse	O
taken	O
to	O
be	O
the	O
bootstrap	O
standard	B
error	I
bseboot	O
(	O
10.16	O
)	O
.	O
the	O
limitations	O
of	O
this	O
approach	O
become	O
obvious	O
in	O
o	O
figure	O
11.3	O
,	O
where	O
the	O
histogram	O
shows	O
b	O
d	O
2000	O
nonparametric	B
boot-	O
strap	O
replications	O
o	O
(	O
cid:3	O
)	O
of	O
the	O
sample	B
correlation	O
coefﬁcient	O
for	O
the	O
student	O
the	O
ﬁrst	O
and	O
simplest	O
method	B
is	O
to	O
use	O
the	O
standard	O
interval	O
(	O
11.1	O
)	O
,	O
	O
3	O
this	O
is	O
an	O
anachronism	O
.	O
fisher	O
hated	O
the	O
term	O
“	O
conﬁdence	B
interval	I
”	O
after	O
it	O
was	O
later	O
coined	O
by	O
neyman	O
for	O
his	O
comprehensive	O
theory	B
.	O
he	O
thought	O
of	O
(	O
11.13	O
)	O
as	O
an	O
example	O
of	O
the	O
logic	O
of	O
inductive	O
inference	B
.	O
186	O
bootstrap	O
conﬁdence	B
intervals	I
figure	O
11.3	O
histogram	O
of	O
b	O
d	O
2000	O
nonparametric	B
bootstrap	O
replications	O
o	O
(	O
cid:3	O
)	O
for	O
the	O
student	B
score	I
sample	O
correlation	O
;	O
the	O
solid	O
curve	O
is	O
the	O
ideal	O
parametric	O
bootstrap	O
distribution	B
fo	O
.r/	O
as	O
in	O
figure	O
11.1.	O
observed	O
correlation	O
o	O
	O
d	O
0:498.	O
small	O
triangles	O
show	O
histogram	O
’	O
s	O
0.025	O
and	O
0.975	O
quantiles	O
.	O
	O
score	O
data	B
,	O
obtained	O
as	O
in	O
section	O
10.2.	O
the	O
standard	O
intervals	O
are	O
justiﬁed	O
by	O
taking	O
literally	O
the	O
asymptotic	O
normality	O
of	O
o	O
	O
,	O
o	O
	O
p	O
(	O
cid:24	O
)	O
n	O
.	O
;	O
(	O
cid:27	O
)	O
2/	O
;	O
(	O
11.15	O
)	O
(	O
cid:27	O
)	O
the	O
true	O
standard	B
error	I
.	O
relation	O
(	O
11.15	O
)	O
will	O
generally	O
hold	O
for	O
large	O
enough	O
sample	B
size	I
n	O
,	O
but	O
we	O
can	O
see	O
that	O
for	O
the	O
student	B
score	I
data	O
asymptotic	O
normality	O
has	O
not	O
yet	O
set	B
in	O
,	O
with	O
the	O
histogram	O
being	O
notably	O
long-tailed	O
to	O
the	O
left	O
.	O
we	O
can	O
’	O
t	B
expect	O
good	O
performance	O
from	O
the	O
standard	O
method	O
in	O
this	O
case	O
.	O
(	O
the	O
para-	O
metric	O
bootstrap	O
distribution	B
is	O
just	O
as	O
nonnormal	O
,	O
as	O
shown	O
by	O
the	O
smooth	O
curve	O
.	O
)	O
(	O
cid:3	O
)	O
1	O
;	O
	O
o	O
	O
the	O
percentile	B
method	I
uses	O
the	O
shape	O
of	O
the	O
bootstrap	O
distribution	B
to	O
improve	O
upon	O
the	O
standard	O
intervals	O
(	O
11.1	O
)	O
.	O
having	O
generated	O
b	O
bootstrap	O
replications	O
o	O
(	O
cid:3	O
)	O
b	O
,	O
either	O
nonparametrically	O
as	O
in	O
section	O
10.2	O
or	O
parametrically	O
as	O
in	O
section	O
10.4	O
,	O
we	O
use	O
the	O
obvious	O
percentiles	O
of	O
their	O
distribution	B
to	O
deﬁne	O
the	O
percentile	O
conﬁdence	O
limits	O
.	O
the	O
histogram	O
in	O
figure	O
11.3	O
has	O
its	O
0.025	O
and	O
0.975	O
percentiles	O
equal	O
to	O
0.118	O
and	O
0.758	O
,	O
(	O
cid:3	O
)	O
2	O
;	O
:	O
:	O
:	O
;	O
o	O
	O
bootstrap	O
correlationsfrequency−0.4−0.20.00.20.40.60.81.00204060801000.4980.1180.758l	O
11.2	O
the	O
percentile	B
method	I
187	O
and	O
these	O
are	O
the	O
endpoints	O
of	O
the	O
central	O
95	O
%	O
nonparametric	B
percentile	O
interval	B
.	O
figure	O
11.4	O
a	O
95	O
%	O
central	O
conﬁdence	O
interval	B
via	O
the	O
percentile	B
method	I
,	O
based	O
on	O
the	O
2000	O
nonparametric	B
replications	O
o	O
figure	O
11.3	O
.	O
(	O
cid:3	O
)	O
of	O
	O
we	O
can	O
state	O
things	O
more	O
precisely	O
in	O
terms	O
of	O
the	O
bootstrap	O
cdf	B
the	O
proportion	B
of	O
bootstrap	O
samples	O
less	O
than	O
t	B
,	O
n	O
o	O
o	O
.	O
(	O
cid:3	O
)	O
b	O
	O
t	B
og.t	O
/	O
d	O
#	O
(	O
11.16	O
)	O
(	O
cid:3	O
)	O
.˛/	O
of	O
the	O
bootstrap	O
distribution	B
is	O
given	O
by	O
the	O
b	O
:	O
	O
the	O
˛th	O
percentile	O
point	O
o	O
inverse	O
function	B
of	O
og	O
,	O
	O
og.t	O
/	O
,	O
o	O
	O
(	O
cid:3	O
)	O
.˛/	O
d	O
og	O
(	O
cid:0	O
)	O
1.˛/i	O
(	O
11.17	O
)	O
o	O
(	O
cid:3	O
)	O
.˛/	O
is	O
the	O
value	O
putting	O
proportion	B
˛	O
of	O
the	O
bootstrap	O
sample	B
to	O
its	O
left	O
.	O
the	O
level-˛	O
upper	O
endpoint	O
of	O
the	O
percentile	B
interval	I
,	O
say	O
o	O
	O
	O
%	O
ileœ˛	O
,	O
is	O
by	O
deﬁnition	O
(	O
cid:0	O
)	O
1.˛/	O
:	O
in	O
this	O
notation	O
,	O
the	O
95	O
%	O
central	O
percentile	O
interval	B
is	O
(	O
cid:3	O
)	O
.˛/	O
d	O
og	O
	O
	O
%	O
ileœ˛	O
d	O
o	O
o	O
(	O
cid:16	O
)	O
o	O
	O
%	O
ileœ:025	O
;	O
	O
o	O
	O
%	O
ileœ:975	O
:	O
(	O
11.19	O
)	O
(	O
11.18	O
)	O
0.00.20.40.60.81.00.00.20.40.60.81.0q^*allllll.118.758.025.975g^	O
188	O
bootstrap	O
conﬁdence	B
intervals	I
the	O
construction	O
is	O
illustrated	O
in	O
figure	O
11.4.	O
the	O
percentile	O
intervals	O
are	O
transformation	O
invariant	O
.	O
let	O
(	O
cid:30	O
)	O
d	O
m.	O
/	O
as	O
o	O
in	O
(	O
11.9	O
)	O
,	O
and	O
likewise	O
o	O
(	O
cid:30	O
)	O
d	O
m.	O
	O
/	O
(	O
m.	O
(	O
cid:1	O
)	O
/	O
monotonically	O
increasing	O
)	O
,	O
with	O
o	O
bootstrap	O
replications	O
o	O
(	O
cid:30	O
)	O
(	O
cid:3	O
)	O
b/	O
for	O
b	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
b.	O
the	O
bootstrap	O
(	O
cid:3	O
)	O
b	O
d	O
m.	O
	O
percentiles	O
transform	B
in	O
the	O
same	O
way	O
,	O
(	O
cid:3	O
)	O
.˛/	O
d	O
m	O
(	O
cid:3	O
)	O
.˛/	O
(	O
11.20	O
)	O
o	O
(	O
cid:30	O
)	O
;	O
so	O
that	O
,	O
as	O
in	O
(	O
11.18	O
)	O
,	O
o	O
(	O
cid:30	O
)	O
%	O
ileœ˛	O
d	O
m	O
verifying	O
transformation	O
invariance	O
.	O
	O
%	O
ileœ˛	O
;	O
(	O
11.21	O
)	O
(	O
cid:16	O
)	O
o	O
(	O
cid:16	O
)	O
o	O
	O
	O
in	O
what	O
sense	O
does	O
the	O
percentile	B
method	I
improve	O
upon	O
the	O
standard	O
intervals	O
?	O
one	O
answer	O
involves	O
transformation	O
invariance	O
.	O
suppose	O
there	O
exists	O
a	O
monotone	O
transformation	O
(	O
cid:30	O
)	O
d	O
m.	O
/	O
and	O
o	O
(	O
cid:30	O
)	O
d	O
m.	O
o	O
	O
/	O
such	O
that	O
(	O
11.22	O
)	O
o	O
(	O
cid:30	O
)	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:30	O
)	O
;	O
(	O
cid:27	O
)	O
2/	O
(	O
cid:16	O
)	O
o	O
(	O
cid:30	O
)	O
;	O
(	O
cid:27	O
)	O
2	O
o	O
(	O
cid:30	O
)	O
(	O
cid:3	O
)	O
(	O
cid:24	O
)	O
for	O
every	O
	O
,	O
with	O
(	O
cid:27	O
)	O
2	O
constant	O
.	O
fisher	O
’	O
s	O
transformation	O
(	O
11.11	O
)	O
–	O
(	O
11.12	O
)	O
al-	O
most	O
accomplishes	O
this	O
for	O
the	O
normal	B
correlation	O
coefﬁcient	O
.	O
it	O
would	O
then	O
be	O
true	O
that	O
parametric	B
bootstrap	O
replications	O
would	O
also	O
follow	O
(	O
11.22	O
)	O
,	O
(	O
11.23	O
)	O
that	O
is	O
,	O
the	O
bootstrap	O
cdf	B
og	O
(	O
cid:30	O
)	O
would	O
be	O
normal	B
with	O
mean	O
o	O
(	O
cid:30	O
)	O
and	O
variance	O
(	O
cid:27	O
)	O
2.	O
the	O
˛th	O
percentile	O
of	O
og	O
(	O
cid:30	O
)	O
would	O
equal	O
n	O
:	O
o	O
(	O
cid:30	O
)	O
%	O
ileœ˛	O
d	O
o	O
(	O
cid:30	O
)	O
(	O
cid:3	O
)	O
.˛/	O
d	O
o	O
(	O
cid:30	O
)	O
c	O
z.˛/	B
(	O
cid:27	O
)	O
;	O
(	O
11.24	O
)	O
where	O
z.˛/	B
denotes	O
the	O
˛th	O
percentile	O
of	O
a	O
standard	O
normal	O
distribution	B
,	O
z.˛/	B
d	O
ˆ	O
(	O
z.:975/	O
d	O
1:96	O
,	O
z.:025/	O
d	O
(	O
cid:0	O
)	O
1:96	O
,	O
etc.	O
)	O
.	O
(	O
cid:0	O
)	O
1.˛/	O
(	O
11.25	O
)	O
in	O
other	O
words	O
,	O
the	O
percentile	B
method	I
would	O
provide	O
fisher	O
’	O
s	O
“	O
obviously	O
correct	O
”	O
intervals	B
for	O
(	O
cid:30	O
)	O
,	O
o	O
(	O
cid:30	O
)	O
˙	O
1:96	O
(	O
cid:27	O
)	O
(	O
11.26	O
)	O
for	O
95	O
%	O
coverage	O
for	O
example	O
.	O
but	O
,	O
because	O
of	O
transformation	O
invariance	O
,	O
the	O
percentile	O
intervals	O
for	O
our	O
original	O
parameter	O
	O
would	O
also	O
be	O
exactly	O
correct	O
.	O
some	O
comments	O
concerning	O
the	O
percentile	B
method	I
are	O
pertinent	O
.	O
2	O
o	O
	O
/	O
p	O
(	O
cid:24	O
)	O
11.2	O
the	O
percentile	B
method	I
o	O
	O
/	O
,	O
it	O
only	O
assumes	O
its	O
existence	O
.	O
189	O
(	O
cid:15	O
)	O
the	O
method	B
does	O
not	O
require	O
actually	O
knowing	O
the	O
transformation	O
to	O
nor-	O
mality	O
o	O
(	O
cid:30	O
)	O
d	O
m.	O
(	O
cid:15	O
)	O
if	O
a	O
transformation	O
to	O
form	B
(	O
11.22	O
)	O
exists	O
,	O
then	O
the	O
percentile	O
intervals	O
are	O
not	O
only	O
accurate	O
,	O
but	O
also	O
correct	O
in	O
the	O
fisherian	O
sense	O
of	O
giving	O
the	O
logically	O
appropriate	O
inference.	O
(	O
cid:15	O
)	O
the	O
justifying	O
assumption	O
for	O
the	O
standard	O
intervals	O
(	O
11.15	O
)	O
,	O
o	O
	O
p	O
(	O
cid:24	O
)	O
n	O
.	O
,	O
(	O
cid:27	O
)	O
2/	O
,	O
becomes	O
more	O
accurate	O
as	O
the	O
sample	B
size	I
n	O
increases	O
(	O
usually	O
with	O
(	O
cid:27	O
)	O
decreasing	O
as	O
1=	O
n	O
)	O
,	O
but	O
the	O
convergence	O
can	O
be	O
slow	O
in	O
cases	O
like	O
that	O
of	O
the	O
normal	B
correlation	O
coefﬁcient	O
.	O
the	O
broader	O
assumption	O
(	O
11.22	O
)	O
,	O
that	O
m.	O
up	O
convergence	O
,	O
irrespective	O
of	O
whether	O
or	O
not	O
it	O
holds	O
exactly	O
.	O
sec-	O
tion	O
11.4	O
makes	O
this	O
point	O
explicit	O
,	O
in	O
terms	O
of	O
asymptotic	O
rates	O
of	O
con-	O
vergence	O
.	O
(	O
cid:15	O
)	O
the	O
standard	O
method	O
works	O
ﬁne	O
once	O
it	O
is	O
applied	O
on	O
an	O
appropriate	O
scale	B
,	O
as	O
in	O
figure	O
11.2.	O
the	O
trouble	O
is	O
that	O
the	O
method	B
is	O
not	O
transforma-	O
tion	O
invariant	O
,	O
leaving	O
the	O
statistician	O
the	O
job	O
of	O
ﬁnding	O
the	O
correct	O
scale	B
.	O
the	O
percentile	B
method	I
can	O
be	O
thought	O
of	O
as	O
a	O
transformation-invariant	O
version	O
of	O
the	O
standard	O
intervals	O
,	O
an	O
“	O
automatic	O
fisher	O
”	O
that	O
substitutes	O
massive	O
computations	B
for	O
mathematical	O
ingenuity	O
.	O
p	O
n	O
.m.	O
/	O
;	O
(	O
cid:27	O
)	O
2/	O
for	O
some	O
transformation	O
m.	O
(	O
cid:1	O
)	O
/	O
,	O
speeds	O
(	O
cid:15	O
)	O
the	O
method	B
requires	O
bootstrap	O
sample	B
sizes	O
on	O
the	O
order	O
of	O
b	O
d	O
2000	O
.	O
3	O
(	O
cid:15	O
)	O
the	O
percentile	B
method	I
is	O
not	O
the	O
last	O
word	O
in	O
bootstrap	O
conﬁdence	O
in-	O
tervals	O
.	O
two	O
improvements	O
,	O
the	O
“	O
bc	O
”	O
and	O
“	O
bca	O
”	O
methods	O
,	O
will	O
be	O
dis-	O
cussed	O
in	O
the	O
next	O
section	O
.	O
table	O
11.1	O
compares	O
the	O
various	O
intervals	B
as	O
applied	O
to	O
the	O
student	B
score	I
correlation	O
,	O
o	O
	O
d	O
0:498.	O
table	O
11.1	O
bootstrap	O
conﬁdence	O
limits	O
for	O
student	B
score	I
correlation	O
,	O
o	O
	O
d	O
0:498	O
,	O
n	O
d	O
22.	O
parametric	B
exact	O
limits	O
from	O
neyman	O
’	O
s	O
construction	O
as	O
in	O
figure	O
11.1.	O
the	O
bc	O
and	O
bca	O
methods	O
are	O
discussed	O
in	O
the	O
next	O
two	O
sections	O
;	O
.z0	O
;	O
a/	O
,	O
two	O
constants	O
required	O
for	O
bca	O
,	O
are	O
.	O
(	O
cid:0	O
)	O
0:055	O
;	O
0:005/	O
parametric	B
,	O
and	O
.0:000	O
;	O
0:006/	O
nonparametric	B
.	O
parametric	B
.975	O
.025	O
nonparametric	B
.975	O
.025	O
1.	O
standard	O
2.	O
percentile	O
3.	O
bc	O
4.	O
bca	O
exact	O
.17	O
.11	O
.08	O
.08	O
.09	O
.83	O
.77	O
.75	O
.75	O
.75	O
.18	O
.13	O
.13	O
.12	O
.82	O
.76	O
.76	O
.76	O
the	O
label	O
“	O
computer-intensive	O
inference	B
”	O
seems	O
especially	O
apt	O
as	O
ap-	O
190	O
bootstrap	O
conﬁdence	B
intervals	I
plied	O
to	O
bootstrap	O
conﬁdence	B
intervals	I
.	O
neyman	O
and	O
fisher	O
’	O
s	O
constructions	B
are	O
expanded	O
from	O
a	O
few	O
special	O
theoretically	O
tractable	O
cases	O
to	O
almost	O
any	O
situation	O
where	O
the	O
statistician	O
has	O
a	O
repeatable	O
algorithm	B
.	O
automation	O
,	O
the	O
replacement	O
of	O
mathematical	O
formulas	O
with	O
wide-ranging	O
computer	O
algo-	O
rithms	O
,	O
will	O
be	O
a	O
major	O
theme	O
of	O
succeeding	O
chapters	O
.	O
	O
(	O
11.28	O
)	O
	O
for	O
	O
.	O
	O
implies	O
(	O
cid:3	O
)	O
	O
o	O
(	O
cid:30	O
)	O
o	O
d	O
0:50	O
o	O
d	O
0:50	O
:	O
	O
pr	O
(	O
cid:3	O
)	O
n	O
o	O
(	O
cid:30	O
)	O
pr	O
(	O
cid:3	O
)	O
n	O
o	O
z	O
o	O
(	O
cid:0	O
)	O
1	O
fo	O
z	O
:498	O
11.3	O
bias-corrected	O
conﬁdence	B
intervals	I
the	O
ideal	O
form	O
(	O
11.23	O
)	O
for	O
the	O
percentile	B
method	I
,	O
o	O
(	O
cid:30	O
)	O
o	O
that	O
the	O
transformation	O
o	O
(	O
cid:30	O
)	O
d	O
m.	O
	O
/	O
yields	O
an	O
unbiased	O
estimator	O
of	O
con-	O
stant	O
variance	O
.	O
the	O
improved	O
methods	O
of	O
this	O
section	O
and	O
the	O
next	O
take	O
into	O
account	O
the	O
possibility	O
of	O
bias	O
and	O
changing	O
variance	O
.	O
we	O
begin	O
with	O
bias	O
.	O
if	O
o	O
(	O
cid:30	O
)	O
(	O
cid:24	O
)	O
o	O
(	O
cid:30	O
)	O
n	O
.	O
o	O
(	O
cid:30	O
)	O
;	O
(	O
cid:27	O
)	O
2/	O
and	O
(	O
cid:3	O
)	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:30	O
)	O
;	O
(	O
cid:27	O
)	O
2/	O
for	O
all	O
(	O
cid:30	O
)	O
d	O
m.	O
/	O
,	O
as	O
hypothesized	O
in	O
(	O
11.22	O
)	O
,	O
then	O
n	O
.	O
o	O
(	O
cid:30	O
)	O
;	O
(	O
cid:27	O
)	O
2/	O
,	O
says	O
(	O
cid:3	O
)	O
(	O
cid:24	O
)	O
o	O
	O
/	O
,	O
(	O
11.28	O
)	O
(	O
11.27	O
)	O
(	O
pr	O
(	O
cid:3	O
)	O
indicating	O
bootstrap	O
probability	O
)	O
,	O
in	O
which	O
case	O
the	O
monotonicity	O
of	O
m.	O
(	O
cid:1	O
)	O
/	O
gives	O
that	O
is	O
,	O
o	O
we	O
can	O
check	O
that	O
.	O
for	O
a	O
parametric	B
family	O
of	O
densities	O
f	O
.	O
(	O
11.29	O
)	O
for	O
the	O
normal	B
correlation	O
coefﬁcient	O
density	B
(	O
3.11	O
)	O
,	O
n	O
d	O
22	O
,	O
numerical	O
integration	O
gives	O
(	O
cid:3	O
)	O
	O
o	O
(	O
cid:3	O
)	O
is	O
median	O
unbiased4	O
for	O
o	O
	O
,	O
and	O
likewise	O
o	O
(	O
cid:3	O
)	O
	O
(	O
cid:16	O
)	O
o	O
(	O
cid:3	O
)	O
	O
(	O
cid:16	O
)	O
o	O
which	O
is	O
not	O
far	O
removed	O
from	O
0.50	O
,	O
but	O
far	O
enough	O
to	O
have	O
a	O
small	O
impact	O
on	O
proper	B
inference	O
.	O
it	O
suggests	O
that	O
o	O
(	O
cid:3	O
)	O
is	O
biased	O
upward	O
relative	O
to	O
o	O
—	O
that	O
’	O
s	O
why	O
less	O
than	O
half	O
of	O
the	O
bootstrap	O
probability	O
lies	O
below	O
o	O
—and	O
by	O
implication	O
that	O
o	O
	O
is	O
upwardly	O
biased	O
for	O
estimating	O
	O
.	O
accordingly	O
,	O
conﬁdence	B
intervals	I
should	O
be	O
adjusted	O
a	O
little	O
bit	O
downward	O
.	O
the	O
bias-	O
corrected	O
percentile	B
method	I
(	O
bc	O
for	O
short	O
)	O
is	O
a	O
data-based	O
algorithm	B
for	O
making	O
such	O
adjustments	O
.	O
(	O
cid:3	O
)	O
d	O
0:478	O
;	O
(	O
cid:3	O
)	O
d	O
0:50	O
:	O
o	O
	O
d	O
	O
f:498	O
	O
(	O
cid:0	O
)	O
1	O
o	O
	O
d	O
	O
(	O
11.30	O
)	O
4	O
median	O
unbiasedness	O
,	O
unlike	O
the	O
usual	O
mean	O
unbiasedness	O
deﬁnition	O
,	O
has	O
the	O
advantage	O
of	O
being	O
transformation	O
invariant	O
.	O
11.3	O
bias-corrected	O
conﬁdence	B
intervals	I
(	O
cid:3	O
)	O
2	O
;	O
:	O
:	O
:	O
;	O
191	O
having	O
simulated	O
b	O
bootstrap	O
replications	O
o	O
(	O
cid:3	O
)	O
b	O
,	O
paramet-	O
o.	O
ric	O
or	O
nonparametric	B
,	O
let	O
p0	O
be	O
the	O
proportion	B
of	O
replications	O
less	O
than	O
o	O
	O
,	O
(	O
11.31	O
)	O
(	O
cid:3	O
)	O
b	O
	O
o	O
p0	O
d	O
#	O
n	O
o	O
(	O
cid:3	O
)	O
1	O
;	O
o	O
	O
o	O
	O
	O
	O
b	O
	O
(	O
an	O
estimate	O
of	O
(	O
11.29	O
)	O
)	O
,	O
and	O
deﬁne	O
the	O
bias-correction	O
value	O
z0	O
d	O
ˆ	O
(	O
cid:0	O
)	O
1.p0/	O
;	O
(	O
11.32	O
)	O
(	O
cid:0	O
)	O
1	O
is	O
the	O
inverse	O
function	B
of	O
the	O
standard	O
normal	O
cdf	B
.	O
the	O
bc	O
where	O
ˆ	O
level-˛	O
conﬁdence	B
interval	I
endpoint	O
is	O
deﬁned	O
to	O
be	O
2z0	O
c	O
z.˛/i	O
ˆ	O
(	O
cid:0	O
)	O
1h	O
(	O
cid:16	O
)	O
o	O
bcœ˛	O
d	O
og	O
z.˛/i	O
d	O
og	O
(	O
cid:16	O
)	O
(	O
cid:0	O
)	O
1h	O
ˆ	O
;	O
(	O
11.33	O
)	O
where	O
og	O
is	O
the	O
bootstrap	O
cdf	B
(	O
11.16	O
)	O
and	O
z.˛/	O
d	O
ˆ	O
(	O
cid:0	O
)	O
1.˛/	O
(	O
11.25	O
)	O
.	O
if	O
p0	O
d	O
0:50	O
,	O
the	O
median	O
unbiased	O
situation	O
,	O
then	O
z0	O
d	O
0	O
and	O
o	O
bcœ˛	O
d	O
og	O
(	O
cid:0	O
)	O
1.˛/	O
d	O
o	O
	O
%	O
ileœ˛	O
;	O
(	O
11.34	O
)	O
the	O
percentile	O
limit	O
(	O
11.18	O
)	O
.	O
otherwise	O
,	O
a	O
bias	O
correction	O
is	O
made	O
.	O
taking	O
p0	O
d	O
0:478	O
for	O
the	O
normal	B
correlation	O
example	O
(	O
the	O
value	O
we	O
would	O
get	O
from	O
an	O
inﬁnite	O
number	O
of	O
parametric	O
bootstrap	O
replications	O
)	O
gives	O
bias	O
correction	O
value	O
(	O
cid:0	O
)	O
0:055.	O
notice	O
that	O
the	O
bc	O
limits	O
are	O
indeed	O
shifted	O
down-	O
ward	O
from	O
the	O
parametric	B
percentile	O
limits	O
in	O
table	O
11.1.	O
nonparametric	B
bootstrapping	O
gave	O
p0	O
about	O
0.50	O
in	O
this	O
case	O
,	O
making	O
the	O
bc	O
limits	O
nearly	O
the	O
same	O
as	O
the	O
percentile	O
limits	O
.	O
a	O
more	O
general	O
transformation	O
argument	B
motivates	O
the	O
bc	O
deﬁnition	O
(	O
11.33	O
)	O
.	O
suppose	O
there	O
exists	O
a	O
monotone	O
transformation	O
(	O
cid:30	O
)	O
d	O
m.	O
/	O
and	O
o	O
(	O
cid:30	O
)	O
d	O
m.	O
o	O
	O
/	O
such	O
that	O
for	O
any	O
	O
o	O
(	O
cid:30	O
)	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:30	O
)	O
(	O
cid:0	O
)	O
z0	O
(	O
cid:27	O
)	O
;	O
(	O
cid:27	O
)	O
2/	O
;	O
(	O
11.35	O
)	O
with	O
z0	O
and	O
(	O
cid:27	O
)	O
ﬁxed	O
constants	O
.	O
then	O
the	O
bc	O
endpoints	O
are	O
accurate	O
,	O
i.e.	O
,	O
have	O
the	O
claimed	O
coverage	O
probabilities	O
,	O
and	O
are	O
also	O
“	O
obviously	O
correct	O
”	O
in	O
the	O
fisherian	O
sense	O
.	O
see	O
the	O
chapter	O
endnotes	O
for	O
proof	O
and	O
discussion	O
.	O
4	O
as	O
before	O
,	O
the	O
statistican	O
does	O
not	O
need	O
to	O
know	O
the	O
transformation	O
m.	O
(	O
cid:1	O
)	O
/	O
that	O
leads	O
to	O
o	O
(	O
cid:30	O
)	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:30	O
)	O
(	O
cid:0	O
)	O
z0	O
(	O
cid:27	O
)	O
;	O
(	O
cid:27	O
)	O
2/	O
,	O
only	O
that	O
it	O
exists	O
.	O
it	O
is	O
a	O
broader	O
target	O
than	O
o	O
(	O
cid:30	O
)	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:30	O
)	O
;	O
(	O
cid:27	O
)	O
2/	O
(	O
11.22	O
)	O
,	O
making	O
the	O
bc	O
method	B
better	O
justiﬁed	O
than	O
the	O
percentile	B
method	I
,	O
irrespective	O
of	O
whether	O
or	O
not	O
such	O
a	O
transformation	O
exists	O
.	O
there	O
is	O
no	O
extra	O
computational	O
burden	O
:	O
the	O
bootstrap	O
replications	O
f	O
o	O
(	O
cid:3	O
)	O
b	O
;	O
b	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
bg	O
,	O
parametric	B
or	O
nonparametric	B
,	O
provide	O
og	O
(	O
11.16	O
)	O
and	O
z0	O
(	O
11.31	O
)	O
–	O
(	O
11.32	O
)	O
,	O
giving	O
o	O
	O
bcœ˛	O
from	O
(	O
11.33	O
)	O
.	O
192	O
bootstrap	O
conﬁdence	B
intervals	I
11.4	O
second-order	O
accuracy	O
p	O
coverage	O
errors	O
of	O
the	O
standard	O
conﬁdence	O
intervals	B
typically	O
decrease	O
at	O
n/	O
in	O
the	O
sample	B
size	I
n	O
:	O
having	O
calculated	O
o	O
cz.˛/o	O
(	O
cid:27	O
)	O
order	O
o.1=	O
for	O
an	O
iid	O
sample	B
x	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn/	O
,	O
we	O
can	O
expect	O
the	O
actual	O
coverage	O
ıp	O
probability	O
to	O
be	O
stanœ˛	O
d	O
o	O
o	O
:	O
d	O
˛	O
c	O
c1	O
	O
	O
o	O
(	O
11.36	O
)	O
stanœ˛	O
n	O
pr	O
n	O
;	O
where	O
c1	O
depends	O
on	O
the	O
problem	O
at	O
hand	O
;	O
(	O
11.36	O
)	O
deﬁnes	O
“	O
ﬁrst-order	O
accu-	O
racy.	O
”	O
it	O
can	O
connote	O
painfully	O
slow	O
convergence	O
to	O
the	O
nominal	O
coverage	O
level	O
˛	O
,	O
requiring	O
sample	B
size	I
4n	O
to	O
cut	O
the	O
error	O
in	O
half	O
.	O
a	O
second-order	O
accurate	O
method	B
,	O
say	O
o	O
o.1=n/	O
,	O
2ndœ˛	O
,	O
makes	O
errors	B
of	O
order	O
only	O
n	O
	O
	O
o	O
pr	O
2ndœ˛	O
o	O
:	O
d	O
˛	O
c	O
c2=n	O
:	O
(	O
11.37	O
)	O
the	O
improvement	O
is	O
more	O
than	O
theoretical	O
.	O
in	O
practical	O
problems	O
like	O
that	O
of	O
table	O
11.1	O
,	O
second-order	O
accurate	O
methods—bca	O
,	O
deﬁned	O
in	O
the	O
follow-	O
ing	O
,	O
is	O
one	O
such—often	O
provide	O
nearly	O
the	O
claimed	O
coverage	O
probabilities	O
,	O
even	O
in	O
small-size	O
samples	O
.	O
neither	O
the	O
percentile	B
method	I
nor	O
the	O
bc	O
method	B
is	O
second-order	O
ac-	O
curate	O
(	O
although	O
,	O
as	O
in	O
table	O
11.1	O
,	O
they	O
tend	O
to	O
be	O
more	O
accurate	O
than	O
the	O
standard	O
intervals	O
)	O
.	O
the	O
difﬁculty	O
for	O
o	O
bcœ˛	O
lies	O
in	O
the	O
ideal	O
form	O
(	O
11.35	O
)	O
,	O
o	O
o	O
(	O
cid:30	O
)	O
(	O
cid:24	O
)	O
	O
/	O
has	O
constant	O
standard	B
error	I
(	O
cid:27	O
)	O
.	O
instead	O
,	O
we	O
now	O
postulate	O
the	O
existence	O
of	O
a	O
monotone	O
transforma-	O
tion	O
(	O
cid:30	O
)	O
d	O
m.	O
/	O
and	O
o	O
(	O
cid:30	O
)	O
d	O
m.	O
n	O
.	O
(	O
cid:30	O
)	O
(	O
cid:0	O
)	O
z0	O
(	O
cid:27	O
)	O
;	O
(	O
cid:27	O
)	O
2/	O
,	O
where	O
it	O
is	O
assumed	O
o	O
(	O
cid:30	O
)	O
d	O
m.	O
o	O
	O
/	O
less	O
restrictive	O
than	O
(	O
11.35	O
)	O
,	O
(	O
cid:27	O
)	O
(	O
cid:30	O
)	O
d	O
1	O
c	O
a	O
(	O
cid:30	O
)	O
:	O
o	O
(	O
cid:30	O
)	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:30	O
)	O
(	O
cid:0	O
)	O
z0	O
(	O
cid:27	O
)	O
(	O
cid:30	O
)	O
;	O
(	O
cid:27	O
)	O
2	O
(	O
11.38	O
)	O
here	O
the	O
“	O
acceleration	O
”	O
	O
a	O
is	O
a	O
small	O
constant	O
describing	O
how	O
the	O
standard	B
deviation	I
of	O
o	O
(	O
cid:30	O
)	O
varies	O
with	O
(	O
cid:30	O
)	O
.	O
if	O
a	O
d	O
0	O
we	O
are	O
back	O
in	O
situation	O
(	O
11.34	O
)	O
5	O
,	O
but	O
if	O
not	O
,	O
an	O
amendment	O
to	O
the	O
bc	O
formula	B
(	O
11.33	O
)	O
is	O
required	O
.	O
(	O
cid:30	O
)	O
/	O
;	O
the	O
bca	O
method	B
(	O
“	O
bias-corrected	O
and	B
accelerated	I
”	O
)	O
takes	O
its	O
level-˛	O
conﬁdence	O
limit	O
to	O
be	O
o	O
bcaœ˛	O
d	O
og	O
(	O
cid:0	O
)	O
1	O
	O
ˆ	O
	O
z0	O
c	O
	O
(	O
cid:21	O
)	O
z0	O
c	O
z.˛/	B
1	O
(	O
cid:0	O
)	O
a.z0	O
c	O
z.˛//	O
:	O
(	O
11.39	O
)	O
a	O
still	O
more	O
elaborate	O
transformation	O
argument	B
shows	O
that	O
,	O
if	O
there	O
exists	O
a	O
monotone	O
transformation	O
(	O
cid:30	O
)	O
d	O
m.	O
/	O
and	O
constants	O
z0	O
and	O
a	O
yielding	O
5	O
this	O
assumes	O
(	O
cid:27	O
)	O
0	O
d	O
1	O
on	O
the	O
right	O
side	O
of	O
(	O
11.38	O
)	O
,	O
which	O
can	O
always	O
be	O
achieved	O
by	O
further	O
transforming	O
(	O
cid:30	O
)	O
to	O
(	O
cid:30	O
)	O
=	O
(	O
cid:27	O
)	O
.	O
5	O
11.4	O
second-order	O
accuracy	O
193	O
(	O
11.38	O
)	O
,	O
then	O
the	O
bca	O
limits	O
have	O
their	O
claimed	O
coverage	O
probabilities	O
and	O
,	O
moreover	O
,	O
are	O
correct	O
in	O
the	O
fisherian	O
sense	O
.	O
bca	O
makes	O
three	O
corrections	O
to	O
the	O
standard	O
intervals	O
(	O
11.1	O
)	O
:	O
for	O
non-	O
normality	O
of	O
o	O
	O
(	O
through	O
using	O
the	O
bootstrap	O
percentiles	O
rather	O
than	O
just	O
the	O
bootstrap	O
standard	B
error	I
)	O
;	O
for	O
bias	O
(	O
through	O
the	O
bias	O
correction	O
value	O
z0	O
)	O
;	O
and	O
for	O
nonconstant	O
standard	B
error	I
(	O
through	O
a	O
)	O
.	O
notice	O
that	O
if	O
a	O
d	O
0	O
then	O
bca	O
(	O
11.39	O
)	O
reduces	O
to	O
bc	O
(	O
11.33	O
)	O
.	O
if	O
z0	O
d	O
0	O
then	O
bc	O
reduces	O
to	O
the	O
percentile	B
method	I
(	O
11.18	O
)	O
;	O
and	O
if	O
og	O
,	O
the	O
bootstrap	O
histogram	O
,	O
is	O
normal	B
,	O
then	O
(	O
11.18	O
)	O
reduces	O
to	O
the	O
standard	O
interval	O
(	O
11.1	O
)	O
.	O
all	O
three	O
of	O
the	O
correc-	O
tions	O
,	O
for	O
nonnormality	O
,	O
bias	O
,	O
and	O
acceleration	O
,	O
can	O
have	O
substantial	O
effects	O
in	O
practice	O
and	O
are	O
necessary	O
to	O
achieve	O
second-order	O
accuracy	O
.	O
a	O
great	O
deal	O
of	O
theoretical	O
effort	O
was	O
devoted	O
to	O
verifying	O
the	O
second-order	O
accu-	O
racy	O
and	O
bca	O
intervals	B
under	O
reasonably	O
general	O
assumptions.6	O
	O
d	O
10	O
;	O
actual	O
tail	O
areas	O
above	O
and	O
below	O
table	O
11.2	O
nominal	O
95	O
%	O
central	O
conﬁdence	O
intervals	B
for	O
poisson	O
parameter	O
	O
having	O
observed	O
o	O
o	O
	O
d	O
10	O
deﬁned	O
as	O
in	O
figure	O
11.1	O
(	O
atom	O
of	O
probability	O
split	O
at	O
10	O
)	O
.	O
for	O
instance	O
,	O
lower	O
standard	O
limit	O
3.80	O
actually	O
puts	O
probability	O
0.004	O
above	O
10	O
,	O
rather	O
than	O
nominal	O
value	O
0.025.	O
bias	O
correction	O
value	O
z0	O
(	O
11.32	O
)	O
and	O
acceleration	O
a	O
(	O
11.38	O
)	O
both	O
equal	O
0.050.	O
nominal	O
limits	O
.025	O
.975	O
tail	O
areas	O
above	O
below	O
1.	O
standard	O
2	O
.	O
%	O
ile	O
3.	O
bc	O
4.	O
bca	O
exact	O
3.80	O
4.18	O
4.41	O
5.02	O
5.08	O
16.20	O
16.73	O
17.10	O
17.96	O
17.82	O
.004	O
.007	O
.010	O
.023	O
.025	O
.055	O
.042	O
.036	O
.023	O
.025	O
the	O
advantages	O
of	O
increased	O
accuracy	O
are	O
not	O
limited	O
to	O
large	O
sample	B
sizes	O
.	O
table	O
11.2	O
returns	O
to	O
our	O
original	O
example	O
of	O
observing	O
o	O
	O
d	O
10	O
from	O
poisson	O
model	B
o	O
	O
(	O
cid:24	O
)	O
poi.	O
/	O
.	O
according	O
to	O
neyman	O
’	O
s	O
construction	O
,	O
the	O
0.95	O
exact	O
limits	O
give	O
tail	O
areas	O
0.025	O
in	O
both	O
the	O
above	O
and	O
below	O
directions	O
,	O
as	O
in	O
figure	O
11.1	O
,	O
and	O
this	O
is	O
nearly	O
matched	O
by	O
the	O
bca	O
limits	O
.	O
however	O
the	O
standard	O
limits	O
are	O
much	O
too	O
conservative	O
at	O
the	O
left	O
end	O
and	O
anti-conservative	O
at	O
the	O
right	O
.	O
6	O
the	O
mathematical	O
side	O
of	O
statistics	B
has	O
also	O
been	O
affected	O
by	O
electronic	O
computation	O
,	O
where	O
it	O
is	O
called	O
upon	O
to	O
establish	O
the	O
properties	O
of	O
general-purpose	O
computer	O
algorithms	O
such	O
as	O
the	O
bootstrap	O
.	O
asymptotic	O
analysis	B
in	O
particular	O
has	O
been	O
aggressively	O
developed	O
,	O
the	O
veriﬁcation	O
of	O
second-order	O
accuracy	O
being	O
a	O
nice	O
success	O
story	O
.	O
194	O
bootstrap	O
conﬁdence	B
intervals	I
table	O
11.3	O
95	O
%	O
nominal	O
conﬁdence	B
intervals	I
for	O
the	O
parametric	B
and	O
nonparametric	B
eigenratio	O
examples	O
of	O
figures	O
10.2	O
and	O
10.6	O
.	O
1.	O
standard	O
2	O
.	O
%	O
ile	O
3.	O
bc	O
4.	O
bca	O
parametric	B
.975	O
.025	O
.829	O
.815	O
.828	O
.820	O
.556	O
.542	O
.523	O
.555	O
.z0	O
d	O
(	O
cid:0	O
)	O
:029	O
;	O
a	O
d	O
:058/	O
nonparametric	B
.975	O
.025	O
.840	O
.818	O
.813	O
.828	O
.545	O
.517	O
.507	O
.523	O
.z0	O
d	O
(	O
cid:0	O
)	O
:049	O
;	O
a	O
d	O
:051/	O
bootstrap	O
conﬁdence	O
limits	O
continue	O
to	O
provide	O
better	O
inferences	O
in	O
the	O
vast	O
majority	O
of	O
situations	O
too	O
complicated	O
for	O
exact	O
analysis	B
.	O
one	O
such	O
situation	O
is	O
examined	O
in	O
table	O
11.3.	O
it	O
relates	O
to	O
the	O
eigenratio	O
example	O
illustrated	O
in	O
figures	O
10.2–10.6	O
.	O
in	O
this	O
case	O
the	O
nonnormality	O
and	O
bias	O
cor-	O
rections	O
stretch	O
the	O
bootstrap	O
intervals	B
to	O
the	O
left	O
,	O
but	O
the	O
acceleration	O
effect	O
pulls	O
right	O
,	O
partially	O
canceling	O
out	O
the	O
net	O
change	O
from	O
the	O
standard	O
inter-	O
vals	O
.	O
the	O
percentile	O
and	O
bc	O
methods	O
are	O
completely	O
automatic	O
,	O
and	O
can	O
be	O
applied	O
whenever	O
a	O
sufﬁciently	O
large	O
number	O
of	O
bootstrap	O
replications	O
are	O
available	O
.	O
the	O
same	O
can	O
not	O
be	O
said	O
of	O
bca	O
.	O
a	O
drawback	O
of	O
the	O
bca	O
method	B
is	O
that	O
the	O
acceleration	O
a	O
is	O
not	O
a	O
function	B
of	O
the	O
bootstrap	O
distribution	B
and	O
must	O
be	O
computed	O
separately	O
.	O
often	O
this	O
is	O
straightforward	O
:	O
(	O
cid:15	O
)	O
for	O
one-parameter	B
exponential	O
families	O
such	O
as	O
the	O
poisson	O
,	O
a	O
equals	O
z0	O
.	O
(	O
cid:15	O
)	O
in	O
one-sample	O
nonparametric	B
problems	O
,	O
a	O
can	O
be	O
estimated	O
from	O
the	O
jackknife	O
resamples	O
o	O
.i	O
/	O
(	O
10.5	O
)	O
,	O
pn	O
pn	O
(	O
cid:16	O
)	O
o	O
.i	O
/	O
(	O
cid:0	O
)	O
o	O
(	O
cid:16	O
)	O
o	O
	O
.	O
(	O
cid:1	O
)	O
/	O
i	O
(	O
cid:0	O
)	O
o	O
	O
.	O
(	O
cid:1	O
)	O
/	O
3	O
2	O
(	O
cid:21	O
)	O
1:5	O
:	O
(	O
11.40	O
)	O
oa	O
d	O
1	O
6	O
id1	O
id1	O
(	O
cid:15	O
)	O
the	O
abc	B
method	I
computes	O
a	O
in	O
multiparameter	O
exponential	O
families	O
(	O
5.54	O
)	O
,	O
as	O
does	O
the	O
resampling-based	O
r	O
algorithm	B
accel	O
.	O
conﬁdence	B
intervals	I
require	O
the	O
number	O
of	O
bootstrap	O
replications	O
b	O
to	O
be	O
on	O
the	O
order	O
of	O
2000	O
,	O
rather	O
than	O
the	O
200	O
or	O
fewer	O
needed	O
for	O
standard	O
errors	O
;	O
the	O
corrections	O
made	O
to	O
the	O
standard	O
intervals	O
are	O
more	O
delicate	O
than	O
standard	O
errors	O
and	O
require	O
greater	O
accuracy	O
.	O
there	O
is	O
one	O
more	O
cautionary	O
note	O
to	O
sound	O
concerning	O
nuisance	O
param-	O
eters	O
:	O
biases	O
can	O
easily	O
get	O
out	O
of	O
hand	O
when	O
the	O
parameter	O
vector	B
(	O
cid:22	O
)	O
is	O
11.5	O
bootstrap-t	O
intervals	B
195	O
high-dimensional	O
.	O
suppose	O
we	O
observe	O
ind	O
(	O
cid:24	O
)	O
xi	O
and	O
wish	O
to	O
set	B
a	O
conﬁdence	B
interval	I
for	O
	O
dpn	O
1	O
x2	O
will	O
be	O
sharply	O
biased	O
upward	O
if	O
n	O
is	O
at	O
all	O
large	O
.	O
to	O
be	O
speciﬁc	O
,	O
if	O
n	O
d	O
10	O
and	O
o	O
	O
d	O
20	O
,	O
we	O
compute	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
;	O
	O
dpn	O
i	O
.	O
the	O
mle	O
o	O
n	O
.	O
(	O
cid:22	O
)	O
i	O
;	O
1/	O
(	O
11.41	O
)	O
1	O
(	O
cid:22	O
)	O
2	O
i	O
6	O
this	O
makes7	O
o	O
centile	O
,	O
z0	O
d	O
ˆ	O
(	O
cid:0	O
)	O
1.0:156/	O
d	O
(	O
cid:0	O
)	O
1:01	O
:	O
(	O
11.42	O
)	O
bcœ:025	O
(	O
11.33	O
)	O
equal	O
a	O
ludicrously	O
small	O
bootstrap	O
per-	O
og	O
(	O
cid:0	O
)	O
1.0:000034/	O
;	O
(	O
11.43	O
)	O
a	O
warning	O
sign	O
against	O
the	O
bc	O
or	O
bca	O
intervals	B
,	O
which	O
work	O
most	O
depend-	O
ably	O
for	O
jz0j	O
and	O
jaj	O
small	O
,	O
say	O
	O
0:2.	O
a	O
more	O
general	O
warning	O
would	O
be	O
against	O
blind	O
trust	O
in	O
maximum	O
likeli-	O
hood	O
estimates	O
in	O
high	O
dimensions	O
.	O
computing	O
z0	O
is	O
a	O
wise	O
precaution	O
even	O
if	O
it	O
is	O
not	O
used	O
for	O
bc	O
or	O
bca	O
purposes	O
,	O
in	O
case	O
it	O
alerts	O
one	O
to	O
dangerous	O
biases	O
.	O
the	O
standard	O
method	O
(	O
11.1	O
)	O
(	O
withbse	O
estimated	O
by	O
the	O
delta	O
method	B
)	O
except	O
conﬁdence	B
intervals	I
for	O
classical	O
applications	O
were	O
most	O
often	O
based	O
on	O
in	O
a	O
few	O
especially	O
simple	O
situations	O
such	O
as	O
the	O
poisson	O
.	O
second-order	O
ac-	O
curate	O
intervals	B
are	O
very	O
much	O
a	O
computer-age	O
development	O
,	O
with	O
both	O
the	O
algorithms	O
and	O
the	O
inferential	O
theory	B
presupposing	O
high-speed	O
electronic	O
computation	O
.	O
11.5	O
bootstrap-t	O
intervals	B
the	O
initial	O
breakthrough	O
on	O
exact	O
conﬁdence	B
intervals	I
came	O
in	O
the	O
form	B
of	O
student	O
’	O
s	O
t	B
distribution	O
in	O
1908.	O
suppose	O
we	O
independently	O
observe	O
data	B
from	O
two	O
possibly	O
different	O
normal	B
distributions	O
,	O
x	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xnx	O
/	O
and	O
y	O
d	O
.y1	O
;	O
y2	O
;	O
:	O
:	O
:	O
;	O
yny	O
/	O
,	O
iid	O
(	O
cid:24	O
)	O
xi	O
n	O
.	O
(	O
cid:22	O
)	O
x	O
;	O
(	O
cid:27	O
)	O
2/	O
and	O
yi	O
iid	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
y	O
;	O
(	O
cid:27	O
)	O
2/	O
;	O
and	O
wish	O
to	O
form	B
a	O
0.95	O
central	O
conﬁdence	O
interval	B
for	O
(	O
11.44	O
)	O
(	O
11.45	O
)	O
(	O
11.46	O
)	O
	O
d	O
(	O
cid:22	O
)	O
y	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
x	O
:	O
o	O
	O
d	O
ny	O
(	O
cid:0	O
)	O
nx	O
;	O
the	O
obvious	O
estimate	B
is	O
7	O
also	O
obcaœ:025	O
,	O
a	O
is	O
zero	O
in	O
this	O
model	B
.	O
196	O
bootstrap	O
conﬁdence	B
intervals	I
but	O
its	O
distribution	B
depends	O
on	O
the	O
nuisance	O
parameter	O
(	O
cid:27	O
)	O
2.	O
student	O
’	O
s	O
masterstroke	O
was	O
to	O
base	O
inference	B
about	O
	O
on	O
the	O
pivotal	O
quantity	B
o	O
	O
(	O
cid:0	O
)	O
bse	O
t	B
d	O
wherebse2	O
is	O
an	O
unbiased	O
estimate	O
of	O
(	O
cid:27	O
)	O
2	O
,	O
bse2	O
d	O
1	O
pnx	O
1	O
.xi	O
(	O
cid:0	O
)	O
nx/2	O
cpny	O
c	O
1	O
ny	O
1	O
.yi	O
(	O
cid:0	O
)	O
ny/2	O
i	O
(	O
11.47	O
)	O
nx	O
represent	O
the	O
100˛th	O
percentile	O
of	O
a	O
tdf	O
distribution	B
yields	O
nx	O
c	O
ny	O
(	O
cid:0	O
)	O
2	O
(	O
11.48	O
)	O
t	B
then	O
has	O
the	O
“	O
student	O
’	O
s	O
t	B
distribution	O
”	O
with	O
df	O
d	O
nx	O
c	O
ny	O
(	O
cid:0	O
)	O
2	O
degrees	O
of	O
freedom	O
if	O
(	O
cid:22	O
)	O
x	O
d	O
(	O
cid:22	O
)	O
y	O
,	O
no	O
matter	O
what	O
(	O
cid:27	O
)	O
2	O
may	O
be	O
.	O
	O
(	O
cid:0	O
)	O
bse	O
(	O
cid:1	O
)	O
t	B
.1	O
(	O
cid:0	O
)	O
˛/	O
letting	O
t	B
.˛/	O
df	O
	O
d	O
.	O
:062	O
;	O
:314/	O
:	O
as	O
the	O
upper	O
level-˛	O
interval	B
of	O
a	O
student	O
’	O
s	O
t	B
conﬁdence	O
limit	O
.	O
applied	O
to	O
the	O
difference	O
between	O
the	O
aml	O
and	O
all	O
scores	O
in	O
figure	O
1.4	O
,	O
the	O
central	O
0.95	O
student	O
’	O
s	O
t	B
interval	O
for	O
	O
d	O
efamlg	O
(	O
cid:0	O
)	O
efallg	O
was	O
calculated	O
to	O
be	O
(	O
11.50	O
)	O
o	O
t	O
œ˛	O
d	O
o	O
o	O
t	O
œ:975	O
(	O
cid:16	O
)	O
o	O
(	O
11.49	O
)	O
t	O
œ:025	O
;	O
df	O
here	O
nx	O
d	O
47	O
,	O
ny	O
d	O
25	O
,	O
and	O
df	O
d	O
70.	O
student	O
’	O
s	O
theory	B
depends	O
on	O
the	O
normality	O
assumptions	O
of	O
(	O
11.44	O
)	O
.	O
the	O
bootstrap-t	O
approach	O
is	O
to	O
accept	O
(	O
or	O
pretend	O
)	O
that	O
t	B
in	O
(	O
11.47	O
)	O
is	O
pivotal	O
,	O
but	O
to	O
estimate	B
its	O
distribution	B
via	O
bootstrap	O
resampling	O
.	O
nonparametric	B
bootstrap	O
samples	O
are	O
drawn	O
separately	O
from	O
x	O
and	O
y	O
,	O
x	O
(	O
cid:3	O
)	O
1	O
;	O
x	O
(	O
cid:3	O
)	O
d	O
.x	O
(	O
cid:3	O
)	O
nx	O
from	O
which	O
we	O
calculate	O
o	O
(	O
cid:3	O
)	O
2	O
;	O
:	O
:	O
:	O
;	O
x	O
	O
and	O
y	O
/	O
(	O
cid:3	O
)	O
andbse	O
(	O
cid:3	O
)	O
/	O
;	O
(	O
cid:3	O
)	O
ny	O
(	O
cid:3	O
)	O
1	O
;	O
y	O
(	O
cid:3	O
)	O
2	O
;	O
:	O
:	O
:	O
;	O
y	O
(	O
cid:3	O
)	O
d	O
.y	O
,	O
(	O
11.46	O
)	O
and	O
(	O
11.48	O
)	O
,	O
giving	O
(	O
cid:3	O
)	O
(	O
cid:0	O
)	O
o	O
o	O
bse	O
	O
(	O
cid:3	O
)	O
;	O
(	O
11.51	O
)	O
(	O
11.52	O
)	O
(	O
cid:3	O
)	O
d	O
t	B
	O
playing	O
the	O
role	O
of	O
	O
,	O
as	O
appropriate	O
in	O
the	O
bootstrap	O
world	O
.	O
repli-	O
(	O
cid:3	O
)	O
.˛/	O
and	O
cor-	O
(	O
cid:3	O
)	O
b	O
;	O
b	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
bg	O
provide	O
estimated	O
percentiles	O
t	B
with	O
o	O
cations	O
ft	O
responding	O
conﬁdence	O
limits	O
o	O
t	B
œ˛	O
d	O
o	O
(	O
cid:3	O
)	O
	O
	O
(	O
cid:0	O
)	O
bse	O
(	O
cid:1	O
)	O
t	B
(	O
cid:3	O
)	O
.1	O
(	O
cid:0	O
)	O
˛/	O
:	O
(	O
11.53	O
)	O
for	O
the	O
aml–all	O
example	O
,	O
the	O
t	B
(	O
cid:3	O
)	O
distribution	B
differed	O
only	O
slightly	O
from	O
a	O
t70	O
distribution	B
;	O
the	O
resulting	O
0.95	O
interval	B
was	O
.0:072	O
;	O
0:323/	O
,	O
nearly	O
11.5	O
bootstrap-t	O
intervals	B
197	O
the	O
same	O
as	O
(	O
11.50	O
)	O
,	O
lending	O
credence	O
to	O
the	O
original	O
normality	O
assump-	O
tions	O
.	O
figure	O
11.5	O
b	O
d	O
2000	O
nonparametric	B
replications	O
of	O
bootstrap-t	O
statistic	B
for	O
the	O
student	B
score	I
correlation	O
;	O
small	O
triangles	O
show	O
0.025	O
and	O
0.975	O
percentile	O
points	O
.	O
the	O
histogram	O
is	O
sharply	O
skewed	O
to	O
the	O
right	O
;	O
the	O
solid	O
curve	O
is	O
student	O
’	O
s	O
t	B
density	O
for	O
21	O
degrees	O
of	O
freedom	O
.	O
returning	O
to	O
the	O
student	B
score	I
correlation	O
example	O
of	O
table	O
11.1	O
,	O
we	O
can	O
apply	O
bootstrap-t	O
methods	O
by	O
still	O
taking	O
t	B
d	O
.	O
bse	O
the	O
approximate	O
standard	B
error	I
.1	O
(	O
cid:0	O
)	O
o	O
pivotal	O
,	O
but	O
now	O
with	O
	O
the	O
true	O
correlation	O
,	O
o	O
p	O
	O
the	O
sample	B
correlation	O
,	O
and	O
19.	O
figure	O
11.5	O
shows	O
the	O
	O
/=bse	O
histogram	O
of	O
b	O
d	O
2000	O
nonparametric	B
bootstrap	O
replications	O
t	B
(	O
cid:3	O
)	O
(	O
cid:0	O
)	O
o	O
	O
(	O
cid:0	O
)	O
	O
/=bse	O
to	O
be	O
notionally	O
(	O
cid:3	O
)	O
d	O
.	O
.	O
these	O
gave	O
bootstrap	O
percentiles	O
	O
2/=	O
o	O
	O
o	O
(	O
cid:3	O
)	O
(	O
cid:16	O
)	O
(	O
cid:3	O
)	O
.	O
:975/	O
d	O
.	O
(	O
cid:0	O
)	O
1:64	O
;	O
2:59/	O
(	O
cid:3	O
)	O
.	O
:025/	O
;	O
t	B
t	O
(	O
11.54	O
)	O
(	O
which	O
might	O
be	O
compared	O
with	O
.	O
(	O
cid:0	O
)	O
2:08	O
;	O
2:08/	O
for	O
a	O
standard	O
t21	O
distribu-	O
tion	O
)	O
,	O
and	O
0.95	O
interval	B
.0:051	O
;	O
0:781/	O
from	O
(	O
11.53	O
)	O
,	O
somewhat	O
out	O
of	O
place	O
compared	O
with	O
the	O
other	O
entries	O
in	O
the	O
right	O
panel	O
of	O
table	O
11.1.	O
bootstrap-t	O
intervals	B
are	O
not	O
transformation	O
invariant	O
.	O
this	O
means	O
they	O
can	O
perform	O
poorly	O
or	O
well	O
depending	O
on	O
the	O
scale	B
of	O
application	O
.	O
if	O
per-	O
formed	O
on	O
fisher	O
’	O
s	O
scale	B
(	O
11.11	O
)	O
they	O
agree	O
well	O
with	O
exact	O
intervals	B
for	O
t*	O
valuesfrequency050100150200t	O
distributiondf	O
=	O
21−3−2−101234−1.642.59	O
bootstrap	O
conﬁdence	B
intervals	I
198	O
mula	O
forbse	O
.	O
the	O
correlation	B
coefﬁcient	I
.	O
a	O
practical	O
difﬁculty	O
is	O
the	O
requirement	O
of	O
a	O
for-	O
nevertheless	O
,	O
the	O
idea	O
of	O
estimating	O
the	O
actual	O
distribution	B
of	O
a	O
proposed	O
pivotal	O
quantity	B
has	O
great	O
appeal	O
to	O
the	O
modern	O
statistical	O
spirit	O
.	O
calculat-	O
ing	O
the	O
percentiles	O
of	O
the	O
original	O
student	O
t	O
distribution	B
was	O
a	O
multi-year	O
project	O
in	O
the	O
early	O
twentieth	O
century	O
.	O
now	O
we	O
can	O
afford	O
to	O
calculate	O
our	O
own	O
special	O
“	O
t	B
table	O
”	O
for	O
each	O
new	O
application	O
.	O
spending	O
such	O
computa-	O
tional	O
wealth	O
wisely	O
,	O
while	O
not	O
losing	O
one	O
’	O
s	O
inferential	O
footing	O
,	O
is	O
the	O
cen-	O
tral	O
task	O
and	O
goal	O
of	O
twenty-ﬁrst-century	O
statisticians	O
.	O
11.6	O
objective	O
bayes	O
intervals	B
and	O
the	O
conﬁdence	O
distribution	O
interval	B
estimates	O
are	O
ubiquitous	O
.	O
they	O
play	O
a	O
major	O
role	O
in	O
the	O
scientiﬁc	O
discourse	O
of	O
a	O
hundred	O
disciplines	O
,	O
from	O
physics	O
,	O
astronomy	O
,	O
and	O
biology	O
to	O
medicine	O
and	O
the	O
social	O
sciences	O
.	O
neyman-style	O
frequentist	O
conﬁdence	B
intervals	I
dominate	O
the	O
literature	O
,	O
but	O
there	O
have	O
been	O
inﬂuential	O
bayesian	O
and	O
fisherian	O
developments	O
as	O
well	O
,	O
as	O
discussed	O
next	O
.	O
g.	O
/	O
,	O
bayes	O
’	O
rule	B
(	O
3.5	O
)	O
produces	O
the	O
posterior	O
density	O
of	O
	O
,	O
given	O
a	O
one-parameter	B
family	O
of	O
densities	O
f	O
.	O
o	O
	O
/	O
;	O
	O
/	O
is	O
the	O
marginal	O
densityr	O
f	O
.	O
o	O
	O
/	O
d	O
g.	O
/f	O
.	O
	O
/=f	O
.	O
o	O
	O
/g.	O
/d	O
.	O
the	O
bayes	O
0.95	O
cred-	O
g.j	O
o	O
(	O
11.55	O
)	O
o	O
o	O
	O
/	O
and	O
a	O
prior	B
density	O
	O
/	O
spans	O
the	O
central	O
0.95	O
region	B
of	O
g.j	O
o	O
	O
/	O
,	O
say	O
where	O
f	O
.	O
ible	O
interval	B
c.j	O
o	O
	O
/	O
d	O
.a	O
.	O
o	O
	O
/	O
;	O
b.	O
o	O
	O
//	O
;	O
with	O
c.j	O
o	O
z	O
b.o	O
/	O
a.o	O
/	O
g.j	O
o	O
	O
/	O
d	O
d	O
0:95	O
;	O
(	O
11.56	O
)	O
(	O
11.57	O
)	O
and	O
with	O
posterior	B
probability	I
0.025	O
in	O
each	O
tail	O
region	B
.	O
conﬁdence	B
intervals	I
,	O
of	O
course	O
,	O
require	O
no	O
prior	B
information	O
,	O
making	O
them	O
eminently	O
useful	O
in	O
day-to-day	O
applied	O
practice	O
.	O
the	O
bayesian	O
equiv-	O
alents	O
are	O
credible	O
intervals	B
based	O
on	O
uninformative	O
priors	B
,	O
section	O
3.2	O
.	O
“	O
matching	O
priors	B
,	O
”	O
those	O
whose	O
credible	O
intervals	B
nearly	O
match	O
neyman	O
conﬁdence	B
intervals	I
,	O
have	O
been	O
of	O
particular	O
interest	O
.	O
jeffreys	O
’	O
prior	B
(	O
3.17	O
)	O
,	O
g.	O
/	O
d	O
i	O
1=2	O
i	O
dz	O
	O
@	O
;	O
	O
@	O
	O
(	O
cid:21	O
)	O
2	O
o	O
	O
/	O
log	O
f	O
.	O
o	O
	O
/	O
d	O
o	O
	O
;	O
f	O
.	O
(	O
11.58	O
)	O
11.6	O
objective	O
bayes	O
intervals	B
199	O
provides	O
a	O
generally	O
accurate	O
matching	O
prior	B
for	O
one-parameter	B
problems	O
.	O
figure	O
3.2	O
illustrates	O
this	O
for	O
the	O
student	B
score	I
correlation	O
,	O
where	O
the	O
credi-	O
ble	O
interval	B
.0:093	O
;	O
0:750/	O
is	O
a	O
near-exact	O
match	O
to	O
the	O
neyman	O
0.95	O
inter-	O
val	O
of	O
figure	O
11.1.	O
difﬁculties	O
begin	O
with	O
multiparameter	O
families	O
f	O
(	O
cid:22	O
)	O
.x/	O
(	O
5.1	O
)	O
:	O
we	O
wish	O
to	O
construct	O
an	O
interval	B
estimate	O
for	O
a	O
one-dimensional	O
function	B
	O
d	O
t	B
.	O
(	O
cid:22	O
)	O
/	O
of	O
the	O
p-dimensional	O
parameter	O
vector	B
(	O
cid:22	O
)	O
,	O
and	O
must	O
somehow	O
remove	O
the	O
ef-	O
fects	O
of	O
the	O
p	O
(	O
cid:0	O
)	O
1	O
“	O
nuisance	O
parameters.	O
”	O
in	O
a	O
few	O
rare	O
situations	O
,	O
including	O
the	O
normal	B
theory	O
correlation	B
coefﬁcient	I
,	O
this	O
can	O
be	O
done	O
exactly	O
.	O
pivotal	O
methods	O
do	O
the	O
job	O
for	O
student	O
’	O
s	O
t	B
construction	O
.	O
bootstrap	O
conﬁdence	O
inter-	O
vals	O
greatly	O
extend	O
the	O
reach	O
of	O
such	O
methods	O
,	O
at	O
a	O
cost	O
of	O
greatly	O
increased	O
computation	O
.	O
bayesians	O
get	O
rid	O
of	O
nuisance	O
parameters	O
by	O
integrating	O
them	O
out	O
of	O
the	O
posterior	O
density	O
g.	O
(	O
cid:22	O
)	O
jx/	O
d	O
g.	O
(	O
cid:22	O
)	O
/f	O
(	O
cid:22	O
)	O
.x/=f	O
.x/	O
(	O
3.6	O
)	O
(	O
x	O
now	O
representing	O
all	O
the	O
data	B
,	O
“	O
x	O
”	O
equaling	O
.x	O
;	O
y/	O
for	O
the	O
student	O
t	O
setup	O
(	O
11.44	O
)	O
)	O
.	O
that	O
is	O
,	O
we	O
calculate8	O
the	O
marginal	O
density	B
of	O
	O
d	O
t	B
.	O
(	O
cid:22	O
)	O
/	O
given	O
x	O
,	O
and	O
call	O
it	O
h.jx/	O
.	O
a	O
credible	O
interval	B
for	O
	O
,	O
c.jx/	O
,	O
is	O
then	O
constructed	O
as	O
in	O
(	O
11.56	O
)	O
–	O
(	O
11.57	O
)	O
,	O
with	O
h.jx/	O
playing	O
the	O
role	O
of	O
g.j	O
o	O
	O
/	O
.	O
this	O
leaves	O
us	O
the	O
knotty	O
problem	O
of	O
choosing	O
an	O
uninformative	O
multidimensional	O
prior	B
g.	O
(	O
cid:22	O
)	O
/	O
.	O
we	O
will	O
return	O
to	O
the	O
question	O
after	O
ﬁrst	O
discussing	O
ﬁducial	O
methods	O
,	O
a	O
uniquely	O
fisherian	O
device	O
.	O
interpretation	O
of	O
pivotality	O
.	O
we	O
rewrite	O
the	O
student	O
t	O
pivotal	O
t	B
d	O
.	O
(	O
11.47	O
)	O
as	O
	O
(	O
cid:0	O
)	O
bse	O
(	O
cid:1	O
)	O
t	B
;	O
	O
d	O
o	O
(	O
11.59	O
)	O
where	O
t	B
has	O
a	O
student	O
’	O
s	O
t	B
distribution	O
with	O
df	O
degrees	O
of	O
freedom	O
,	O
t	B
(	O
cid:24	O
)	O
	O
andbse	O
were	O
ﬁxed	O
at	O
their	O
calculated	O
tdf	O
.	O
having	O
observed	O
the	O
data	B
.x	O
;	O
y/	O
(	O
11.44	O
)	O
,	O
ﬁducial	O
theory	B
assigns	O
	O
the	O
distribution	B
implied	O
by	O
(	O
11.59	O
)	O
,	O
as	O
if	O
o	O
values	O
while	O
t	B
was	O
distributed	O
as	O
tdf	O
.	O
then	O
o	O
˛	O
conﬁdence	O
limit	O
,	O
is	O
the	O
100˛th	O
percentile	O
of	O
	O
’	O
s	O
ﬁducial	O
distribution	B
.	O
fiducial	O
constructions	B
begin	O
with	O
what	O
seems	O
like	O
an	O
obviously	O
incorrect	O
	O
(	O
cid:0	O
)	O
	O
/=bse	O
o	O
t	O
œ˛	O
(	O
11.49	O
)	O
,	O
the	O
student	O
t	O
level-	O
we	O
seem	O
to	O
have	O
achieved	O
a	O
bayesian	O
posterior	O
conclusion	O
without	O
any	O
prior	B
assumptions.9	O
the	O
historical	O
development	O
here	O
is	O
confused	O
by	O
fisher	O
’	O
s	O
refusal	O
to	O
accept	O
neyman	O
’	O
s	O
conﬁdence	B
interval	I
theory	O
,	O
as	O
well	O
as	O
his	O
dispar-	O
agement	O
of	O
bayesian	O
ideas	O
.	O
as	O
events	O
worked	O
out	O
,	O
all	O
of	O
fisher	O
’	O
s	O
immense	O
prestige	O
was	O
not	O
enough	O
to	O
save	O
ﬁducial	O
theory	B
from	O
the	O
scrapheap	O
of	O
failed	O
statistical	O
methods	O
.	O
8	O
often	O
a	O
difﬁcult	O
calculation	O
,	O
as	O
discussed	O
in	O
chapter	O
13	O
.	O
9	O
“	O
enjoying	O
the	O
bayesian	O
omelette	O
without	O
breaking	O
the	O
bayesian	O
eggs	O
,	O
”	O
in	O
l.	O
j.	O
savage	O
’	O
s	O
words	O
.	O
200	O
bootstrap	O
conﬁdence	B
intervals	I
	O
andbse	O
exhaust	O
the	O
information	B
about	O
	O
available	O
from	O
the	O
and	O
yet	O
,	O
in	O
arthur	O
koestler	O
’	O
s	O
words	O
,	O
“	O
the	O
history	O
of	O
ideas	O
is	O
ﬁlled	O
with	O
barren	O
truths	O
and	O
fertile	O
errors.	O
”	O
fisher	O
’	O
s	O
underlying	O
rationale	O
went	O
some-	O
thing	O
like	O
this	O
:	O
o	O
data	B
,	O
after	O
which	O
there	O
remains	O
an	O
irreducible	O
component	O
of	O
randomness	O
described	O
by	O
t.	O
this	O
is	O
an	O
idea	O
of	O
substantial	O
inferential	O
appeal	O
,	O
and	O
one	O
that	O
can	O
be	O
rephrased	O
in	O
more	O
general	O
terms	O
discussed	O
next	O
that	O
bear	O
on	O
the	O
question	O
of	O
uninformative	O
priors	B
.	O
by	O
deﬁnition	O
,	O
an	O
upper	O
conﬁdence	O
limit	O
o	O
xœ˛	O
satisﬁes	O
n	O
	O
	O
o	O
pr	O
xœ˛	O
o	O
d	O
˛	O
n	O
o	O
xœ˛	O
	O
	O
	O
o	O
pr	O
xœ˛	O
c	O
(	O
cid:15	O
)	O
	O
o	O
d	O
(	O
cid:15	O
)	O
:	O
(	O
where	O
now	O
we	O
have	O
indicated	O
the	O
observed	O
data	B
x	O
in	O
the	O
notation	O
)	O
,	O
and	O
so	O
(	O
11.60	O
)	O
(	O
11.61	O
)	O
(	O
11.62	O
)	O
we	O
can	O
consider	O
o	O
xœ˛	O
as	O
a	O
one-to-one	O
function	B
between	O
˛	O
in	O
.0	O
;	O
1/	O
and	O
	O
a	O
point	O
in	O
its	O
parameter	O
space	B
‚	O
(	O
assuming	O
that	O
o	O
xœ˛	O
is	O
smoothly	O
increasing	O
in	O
˛	O
)	O
.	O
letting	O
(	O
cid:15	O
)	O
go	O
to	O
zero	O
in	O
(	O
11.61	O
)	O
determines	O
the	O
conﬁdence	B
density	I
of	O
	O
,	O
say	O
qgx.	O
/	O
,	O
qgx.	O
/	O
d	O
d˛=d	O
;	O
the	O
local	O
derivative	O
of	O
probability	O
at	O
location	O
	O
for	O
the	O
unknown	O
parameter	O
,	O
the	O
derivative	O
being	O
taken	O
at	O
	O
d	O
o	O
integrating	O
qgx.	O
/	O
recovers	O
˛	O
as	O
a	O
function	B
of	O
	O
.	O
let	O
1	O
d	O
o	O
2	O
d	O
o	O
xœ˛1	O
and	O
xœ˛2	O
for	O
any	O
two	O
values	O
˛1	O
<	O
˛2	O
in	O
.0	O
;	O
1/	O
.	O
then	O
d	O
d	O
˛2	O
(	O
cid:0	O
)	O
˛1	O
qgx.	O
/	O
d	O
dz	O
2	O
z	O
2	O
xœ˛	O
.	O
d˛	O
d	O
(	O
11.63	O
)	O
1	O
1	O
d	O
prf1	O
	O
	O
	O
2g	O
;	O
as	O
in	O
(	O
11.60	O
)	O
.	O
there	O
is	O
nothing	O
controversial	O
about	O
(	O
11.63	O
)	O
as	O
long	O
as	O
we	O
remember	O
that	O
the	O
random	O
quantity	B
in	O
prf1	O
	O
	O
	O
2g	O
is	O
not	O
	O
but	O
rather	O
the	O
interval	B
.1	O
;	O
2/	O
,	O
which	O
varies	O
as	O
a	O
function	B
of	O
x.	O
forgetting	O
this	O
leads	O
to	O
the	O
textbook	O
error	O
of	O
attributing	O
bayesian	O
properties	O
to	O
frequentist	O
results	O
:	O
“	O
there	O
is	O
0.95	O
probability	O
that	O
	O
is	O
in	O
its	O
0.95	O
conﬁdence	B
interval	I
,	O
”	O
etc	O
.	O
this	O
is	O
exactly	O
what	O
the	O
ﬁducial	O
argument	B
does.10	O
whether	O
or	O
not	O
one	O
accepts	O
(	O
11.63	O
)	O
,	O
there	O
is	O
an	O
immediate	O
connection	O
with	O
matching	O
priors	B
.	O
10	O
fiducial	O
and	O
conﬁdence	O
densities	O
agree	O
,	O
as	O
can	O
be	O
seen	O
in	O
the	O
student	O
t	O
situation	O
(	O
11.59	O
)	O
,	O
at	O
least	O
in	O
the	O
somewhat	O
limited	O
catalog	O
of	O
cases	O
fisher	O
thought	O
appropriate	O
for	O
ﬁducial	O
calculations	O
.	O
11.6	O
objective	O
bayes	O
intervals	B
201	O
suppose	O
prior	B
g.	O
(	O
cid:22	O
)	O
/	O
gives	O
a	O
perfect	O
match	O
to	O
the	O
conﬁdence	B
interval	I
system	O
o	O
xœ˛	O
.	O
then	O
,	O
by	O
deﬁnition	O
,	O
its	O
posterior	O
density	O
h.jx/	O
must	O
satisfy	O
(	O
cid:0	O
)	O
1	O
h.jx/	O
d	O
d	O
˛	O
dz	O
ox	O
œ˛	O
z	O
ox	O
œ˛	O
(	O
cid:0	O
)	O
1	O
(	O
11.64	O
)	O
for	O
0	O
<	O
˛	O
<	O
1.	O
but	O
this	O
implies	O
h.jx/	O
equals	O
qgx.	O
/	O
for	O
all	O
	O
.	O
that	O
is	O
,	O
the	O
conﬁdence	B
density	I
qgx.	O
/	O
is	O
the	O
posterior	O
density	O
of	O
	O
given	O
x	O
for	O
any	O
matching	O
prior	B
.	O
qgx.	O
/	O
d	O
figure	O
11.6	O
conﬁdence	B
density	I
(	O
11.62	O
)	O
for	O
poisson	O
parameter	O
	O
having	O
observed	O
o	O
between	O
5.08	O
and	O
17.82	O
,	O
as	O
in	O
table	O
11.2	O
,	O
and	O
areas	O
0.025	O
in	O
each	O
tail	O
.	O
	O
d	O
10.	O
there	O
is	O
area	O
0.95	O
under	O
the	O
curve	O
figure	O
11.6	O
graphs	O
the	O
conﬁdence	B
density	I
for	O
o	O
served	O
o	O
function	B
of	O
	O
(	O
11.62	O
)	O
,	O
	O
(	O
cid:24	O
)	O
poi.	O
/	O
having	O
ob-	O
	O
d	O
10.	O
this	O
was	O
obtained	O
by	O
numerically	O
differentiating	O
˛	O
as	O
a	O
˛	O
d	O
prf10	O
	O
poi.	O
/g	O
;	O
(	O
11.65	O
)	O
“	O
	O
”	O
including	O
splitting	O
the	O
atom	O
of	O
probability	O
at	O
10.	O
according	O
to	O
ta-	O
ble	O
11.2	O
,	O
qg10.	O
/	O
has	O
area	O
0.95	O
between	O
5.08	O
and	O
17.82	O
,	O
and	O
area	O
0.025	O
in	O
each	O
tail	O
.	O
whatever	O
its	O
provenance	O
,	O
the	O
graph	O
delivers	O
a	O
striking	O
picture	O
of	O
the	O
uncertainty	O
in	O
the	O
unknown	O
value	O
of	O
	O
.	O
05101520250.000.020.040.060.080.100.120.14qconfidence	O
density5.0817.82l10	O
202	O
bootstrap	O
conﬁdence	B
intervals	I
bootstrap	O
conﬁdence	B
intervals	I
provide	O
easily	O
computable	O
conﬁdence	O
den-	O
og.	O
/	O
be	O
the	O
bootstrap	O
cdf	B
and	O
og.	O
/	O
its	O
density	B
function	O
(	O
ob-	O
sities	O
.	O
let	O
tained	O
by	O
differentiating	O
a	O
smoothed	O
version	O
of	O
og.	O
/	O
when	O
og	O
is	O
based	O
on	O
b	O
bootstrap	O
replications	O
)	O
.	O
the	O
percentile	O
conﬁdence	O
limits	O
o	O
(	O
cid:0	O
)	O
1.˛/	O
(	O
11.17	O
)	O
have	O
˛	O
d	O
og.	O
/	O
,	O
giving	O
	O
œ˛	O
d	O
og	O
qgx.	O
/	O
d	O
og.	O
/	O
:	O
(	O
11.66	O
)	O
(	O
it	O
is	O
helpful	O
to	O
picture	O
this	O
in	O
figure	O
11.4	O
.	O
)	O
for	O
the	O
percentile	B
method	I
,	O
the	O
bootstrap	O
density	B
is	O
the	O
conﬁdence	B
density	I
.	O
reweighting	O
og.	O
/	O
,	O
for	O
the	O
bca	O
intervals	B
(	O
11.39	O
)	O
,	O
the	O
conﬁdence	B
density	I
is	O
obtained	O
by	O
qgx.	O
/	O
d	O
cw.	O
/og.	O
/	O
;	O
(	O
11.67	O
)	O
7	O
where	O
w.	O
/	O
d	O
'	O
œz	O
=.1	O
c	O
az	O
/	O
(	O
cid:0	O
)	O
z0	O
.1	O
c	O
az	O
/2'.z	O
c	O
z0/	O
;	O
with	O
z	O
d	O
ˆ	O
(	O
cid:0	O
)	O
1	O
og.	O
/	O
(	O
cid:0	O
)	O
z0	O
:	O
(	O
11.68	O
)	O
here	O
'	O
is	O
the	O
standard	O
normal	O
density	B
,	O
ˆ	O
its	O
cdf	B
,	O
and	O
c	O
the	O
constant	O
that	O
makes	O
qgx.	O
/	O
integrate	O
to	O
1.	O
in	O
the	O
usual	O
case	O
where	O
the	O
bootstrap	O
cdf	B
is	O
es-	O
timated	O
from	O
replications	O
o	O
(	O
cid:3	O
)	O
b	O
,	O
b	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
b	O
(	O
either	O
parametric	B
or	O
non-	O
parametric	B
)	O
,	O
the	O
bca	O
conﬁdence	B
density	I
is	O
a	O
reweighted	O
version	O
of	O
og.	O
/	O
.	O
deﬁne	O
	O
(	O
cid:3	O
)	O
b	O
,	O
bx	O
(	O
cid:16	O
)	O
o	O
	O
(	O
cid:3	O
)	O
i	O
(	O
cid:16	O
)	O
o	O
	O
w	O
id1	O
wb	O
d	O
w	O
:	O
(	O
11.69	O
)	O
then	O
the	O
bca	O
conﬁdence	B
density	I
is	O
the	O
discrete	O
density	B
putting	O
weight	O
wb	O
on	O
o	O
(	O
cid:3	O
)	O
b.	O
figure	O
11.7	O
returns	O
to	O
the	O
student	B
score	I
data	O
,	O
n	O
d	O
22	O
students	O
,	O
ﬁve	O
scores	O
	O
each	O
,	O
modeled	O
normally	O
as	O
in	O
figure	O
10.6	O
,	O
iid	O
(	O
cid:24	O
)	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
22	O
:	O
xi	O
(	O
11.70	O
)	O
this	O
is	O
a	O
p	O
d	O
20-dimensional	O
parametric	B
family	O
:	O
5	O
expectations	O
,	O
5	O
vari-	O
ances	O
,	O
10	O
covariances	O
.	O
the	O
parameter	O
of	O
interest	O
was	O
taken	O
to	O
be	O
n5	O
.	O
(	O
cid:21	O
)	O
;	O
†/	O
	O
d	O
maximum	O
eigenvalue	O
of	O
†	O
:	O
(	O
11.71	O
)	O
it	O
had	O
mle	O
o	O
	O
d	O
683	O
,	O
this	O
being	O
the	O
maximum	O
eigenvalue	O
of	O
the	O
mle	O
sample	B
covariance	O
matrix	B
o†	O
(	O
dividing	O
each	O
sum	O
of	O
squares	B
by	O
22	O
rather	O
than	O
21	O
)	O
.	O
b	O
d	O
8000	O
parametric	B
bootstrap	O
replications11	O
o	O
(	O
cid:3	O
)	O
b	O
gave	O
percentile	O
and	O
11	O
b	O
d	O
2000	O
would	O
have	O
been	O
enough	O
for	O
most	O
purposes	O
,	O
but	O
b	O
d	O
8000	O
gave	O
a	O
sharper	O
	O
picture	O
of	O
the	O
different	O
curves	O
.	O
11.6	O
objective	O
bayes	O
intervals	B
203	O
figure	O
11.7	O
conﬁdence	O
densities	O
for	O
the	O
maximum	O
eigenvalue	O
parameter	O
(	O
11.71	O
)	O
,	O
using	O
a	O
multivariate	B
normal	O
model	B
(	O
11.70	O
)	O
for	O
the	O
student	B
score	I
data	O
.	O
the	O
dashed	O
red	O
curve	O
is	O
the	O
percentile	B
method	I
,	O
solid	O
black	O
the	O
bca	O
(	O
with	O
.z0	O
;	O
a/	O
d	O
.0:178	O
;	O
0:093/	O
)	O
.	O
the	O
dotted	O
blue	O
curve	O
is	O
the	O
bayes	O
posterior	O
density	O
for	O
	O
,	O
using	O
jeffreys	O
’	O
prior	B
(	O
11.72	O
)	O
.	O
bca	O
conﬁdence	O
densities	O
as	O
shown	O
.	O
in	O
this	O
case	O
the	O
weights	O
wb	O
(	O
11.69	O
)	O
increased	O
with	O
o	O
(	O
cid:3	O
)	O
b	O
,	O
pushing	O
the	O
bca	O
density	B
to	O
the	O
right	O
.	O
also	O
shown	O
is	O
the	O
bayes	O
posterior	O
density	O
	O
for	O
	O
starting	O
from	O
jeffreys	O
’	O
multiparameter	O
prior	B
8	O
density	B
	O
gjeff	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
ji	O
(	O
cid:22	O
)	O
j1=2	O
;	O
(	O
11.72	O
)	O
where	O
i	O
(	O
cid:22	O
)	O
is	O
the	O
fisher	O
information	B
matrix	O
(	O
5.26	O
)	O
.	O
it	O
isn	O
’	O
t	B
truly	O
uninforma-	O
tive	O
here	O
,	O
moving	O
its	O
credible	O
limits	O
upward	O
from	O
the	O
second-order	O
accurate	O
bca	O
conﬁdence	O
limits	O
.	O
formula	B
(	O
11.72	O
)	O
is	O
discussed	O
further	O
in	O
chapter	O
13.	O
bayesian	O
data	B
analysis	O
has	O
the	O
attractive	O
property	O
that	O
,	O
after	O
examin-	O
ing	O
the	O
data	B
,	O
we	O
can	O
express	O
our	O
remaining	O
uncertainty	O
in	O
the	O
language	O
of	O
probability	O
.	O
fiducial	O
and	O
conﬁdence	O
densities	O
provide	O
something	O
similar	O
for	O
conﬁdence	B
intervals	I
,	O
at	O
least	O
partially	O
freeing	O
the	O
frequentist	O
from	O
the	O
interpretive	O
limitations	O
of	O
neyman	O
’	O
s	O
intervals	B
.	O
500100015000200400600800q	O
:	O
maximum	O
eigenvalueposterior	O
densitybcajeffreyspercentile683l	O
204	O
bootstrap	O
conﬁdence	B
intervals	I
11.7	O
notes	O
and	O
details	O
fisher	O
’	O
s	O
theory	B
of	O
ﬁducial	O
inference	B
(	O
1930	O
)	O
preceded	O
neyman	O
’	O
s	O
approach	O
,	O
formalized	O
in	O
(	O
1937	O
)	O
,	O
which	O
was	O
presented	O
as	O
an	O
attempt	O
to	O
put	O
interval	B
es-	O
timation	O
on	O
a	O
ﬁrm	O
probabilistic	O
basis	O
,	O
as	O
opposed	O
to	O
the	O
mysteries	O
of	O
ﬁdu-	O
cialism	O
.	O
the	O
result	O
was	O
an	O
elegant	O
theory	B
of	O
exact	O
and	O
optimal	O
intervals	B
,	O
phrased	O
in	O
hard-edged	O
frequentistic	O
terms	O
.	O
readers	O
familiar	O
with	O
the	O
the-	O
ory	O
will	O
know	O
that	O
neyman	O
’	O
s	O
construction—a	O
favorite	O
name	O
in	O
the	O
physics	O
literature—as	O
pictured	O
in	O
figure	O
11.1	O
,	O
requires	O
some	O
conditions	B
on	O
the	O
fam-	O
o	O
ily	O
of	O
densities	O
f	O
.	O
	O
/	O
to	O
yield	O
optimal	O
intervals	B
,	O
a	O
sufﬁcient	O
condition	B
being	O
monotone	O
likelihood	B
ratios	O
.	O
bootstrap	O
conﬁdence	B
intervals	I
,	O
efron	O
(	O
1979	O
,	O
1987	O
)	O
,	O
are	O
neither	O
exact	O
nor	O
optimal	O
,	O
but	O
aim	O
instead	O
for	O
wide	O
applicability	O
combined	O
with	O
near-exact	O
accuracy	O
.	O
second-order	O
acuracy	O
of	O
bca	O
intervals	B
was	O
established	O
by	O
hall	O
(	O
1988	O
)	O
.	O
bca	O
is	O
emphatically	O
a	O
child	O
of	O
the	O
computer	O
age	O
,	O
routinely	O
requir-	O
ing	O
b	O
d	O
2000	O
or	O
more	O
bootstrap	O
replications	O
per	O
use	O
.	O
shortcut	O
methods	O
are	O
available	O
.	O
the	O
“	O
abc	B
method	I
”	O
(	O
diciccio	O
and	O
efron	O
,	O
1992	O
)	O
needs	O
only	O
1	O
%	O
as	O
much	O
computation	O
,	O
at	O
the	O
expense	O
of	O
requiring	O
smoothness	O
properties	O
for	O
	O
d	O
t	B
.	O
(	O
cid:22	O
)	O
/	O
,	O
and	O
a	O
less	O
automatic	O
coding	O
of	O
the	O
exponential	O
family	O
setting	O
for	O
individual	O
situations	O
.	O
in	O
other	O
words	O
,	O
it	O
is	O
less	O
convenient	O
.	O
z	O
	O
.	O
:025/	O
denote	O
the	O
central	O
95	O
%	O
interval	B
of	O
density	B
f	O
.	O
1	O
[	O
p.	O
183	O
]	O
neyman	O
’	O
s	O
construction	O
.	O
for	O
any	O
given	O
value	O
of	O
	O
,	O
let	O
.	O
.	O
:025/	O
;	O
	O
.	O
:975//	O
	O
d	O
	O
o	O
	O
d	O
0:025	O
and	O
(	O
cid:16	O
)	O
o	O
(	O
cid:0	O
)	O
1	O
f	O
o	O
	O
/	O
be	O
the	O
indicator	O
function	B
for	O
o	O
1	O
if	O
	O
.	O
:025/	O
<	O
0	O
otherwise	O
.	O
(	O
cid:16	O
)	O
o	O
(	O
	O
(	O
cid:16	O
)	O
o	O
o	O
	O
/	O
,	O
satisfying	O
z	O
	O
.	O
:975/	O
(	O
cid:0	O
)	O
1	O
f	O
	O
2	O
.	O
.	O
:025/	O
;	O
	O
.	O
:975//	O
,	O
d	O
	O
o	O
	O
<	O
	O
.	O
:975/	O
o	O
	O
d	O
0:975i	O
and	O
let	O
i	O
.	O
	O
i	O
o	O
	O
/	O
has	O
a	O
two-point	O
probability	O
distribution	B
,	O
by	O
deﬁnition	O
,	O
i	O
.	O
	O
d	O
(	O
cid:16	O
)	O
o	O
	O
d	O
	O
(	O
i	O
(	O
11.73	O
)	O
(	O
11.74	O
)	O
(	O
11.75	O
)	O
1	O
probability	O
0:95	O
0	O
probability	O
0:05	O
:	O
o	O
	O
/	O
a	O
pivotal	O
statistic	B
,	O
one	O
whose	O
distribution	B
does	O
not	O
de-	O
this	O
makes	O
i	O
.	O
pend	O
upon	O
	O
.	O
neyman	O
’	O
s	O
construction	O
takes	O
the	O
conﬁdence	B
interval	I
c.	O
to	O
observed	O
value	O
o	O
(	O
cid:16	O
)	O
o	O
	O
	O
dn	O
	O
w	O
i	O
(	O
cid:16	O
)	O
o	O
	O
	O
d	O
1	O
o	O
	O
to	O
be	O
c	O
o	O
	O
/	O
corresponding	O
:	O
(	O
11.76	O
)	O
205	O
(	O
11.77	O
)	O
11.7	O
notes	O
and	O
details	O
o	O
	O
/	O
has	O
the	O
desired	O
coverage	O
property	O
n	O
i	O
(	O
cid:16	O
)	O
o	O
	O
	O
d	O
1	O
o	O
d	O
0:95	O
then	O
c.	O
pr	O
o	O
d	O
pr	O
(	O
cid:16	O
)	O
o	O
n	O
	O
2	O
c	O
	O
.	O
:025/	O
and	O
o	O
	O
/	O
,	O
o	O
o	O
	O
for	O
any	O
choice	O
of	O
the	O
true	O
parameter	O
	O
.	O
(	O
for	O
the	O
normal	B
theory	O
correlation	O
density	O
of	O
f	O
.	O
	O
.	O
:975/	O
are	O
increasing	O
functions	O
of	O
	O
.	O
this	O
makes	O
our	O
previous	O
construction	O
(	O
11.6	O
)	O
agree	O
with	O
(	O
11.76	O
)	O
.	O
)	O
the	O
construc-	O
tion	O
applies	O
quite	O
generally	O
,	O
as	O
long	O
as	O
we	O
are	O
able	O
to	O
deﬁne	O
acceptance	O
regions	O
of	O
the	O
sample	B
space	O
having	O
the	O
desired	O
target	O
probability	O
content	O
for	O
every	O
choice	O
of	O
	O
.	O
this	O
can	O
be	O
challenging	O
in	O
multiparameter	O
families	O
.	O
2	O
[	O
p.	O
189	O
]	O
fisherian	O
correctness	O
.	O
fisher	O
,	O
arguing	O
against	O
the	O
neyman	O
para-	O
digm	O
,	O
pointed	O
out	O
that	O
conﬁdence	B
intervals	I
could	O
be	O
accurate	O
without	O
being	O
correct	O
:	O
having	O
observed	O
xi	O
0.95	O
interval	B
based	O
on	O
just	O
the	O
ﬁrst	O
10	O
observations	O
would	O
provide	O
exact	O
0.95	O
coverage	O
while	O
giving	O
obviously	O
incorrect	O
inferences	O
for	O
	O
.	O
if	O
we	O
can	O
reduce	O
the	O
situation	O
to	O
form	B
(	O
11.22	O
)	O
,	O
the	O
percentile	B
method	I
intervals	O
satisfy	O
fisher	O
’	O
s	O
“	O
logic	O
of	O
inductive	O
inference	B
”	O
for	O
correctness	O
,	O
as	O
at	O
(	O
4.31	O
)	O
.	O
3	O
[	O
p.	O
189	O
]	O
bootstrap	O
sample	B
sizes	O
.	O
why	O
we	O
need	O
bootstrap	O
sample	B
sizes	O
on	O
the	O
order	O
of	O
b	O
d	O
2000	O
for	O
conﬁdence	B
interval	I
construction	O
can	O
be	O
seen	O
in	O
the	O
estimation	B
of	O
the	O
bias	O
correction	O
value	O
z0	O
(	O
11.32	O
)	O
.	O
the	O
delta-method	O
standard	B
error	I
of	O
z0	O
d	O
ˆ	O
n	O
.	O
;	O
1/	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
20	O
,	O
the	O
standard	O
(	O
cid:0	O
)	O
1.p0/	O
is	O
calculated	O
to	O
be	O
iid	O
(	O
cid:24	O
)	O
	O
p0.1	O
(	O
cid:0	O
)	O
p0/	O
(	O
cid:21	O
)	O
1=2	O
1	O
'.z0/	O
(	O
11.78	O
)	O
:	O
d	O
0	O
this	O
is	O
with	O
'.z/	O
the	O
standard	O
normal	O
density	B
.	O
with	O
p0	O
about	O
1:25=b	O
1=2	O
,	O
equaling	O
0.028	O
at	O
b	O
d	O
2000	O
,	O
a	O
none-too-small	O
error	O
for	O
use	O
in	O
the	O
bc	O
formula	B
(	O
11.33	O
)	O
or	O
the	O
bca	O
formula	B
(	O
11.39	O
)	O
.	O
4	O
[	O
p.	O
191	O
]	O
bca	O
accuracy	B
and	I
correctness	I
.	O
the	O
bca	O
conﬁdence	O
limit	O
o	O
:	O
d	O
0:5	O
and	O
z0	O
bcaœ˛	O
b	O
;	O
(	O
11.39	O
)	O
is	O
transformation	O
invariant	O
.	O
deﬁne	O
z0	O
c	O
z.˛/	B
zœ˛	O
d	O
z0	O
c	O
;	O
so	O
o	O
o	O
	O
/	O
,	O
and	O
o	O
(	O
cid:30	O
)	O
(	O
cid:30	O
)	O
d	O
m.	O
/	O
,	O
o	O
(	O
cid:30	O
)	O
d	O
m.	O
(	O
cid:0	O
)	O
1.˛/	O
d	O
mœ	O
og	O
(	O
cid:0	O
)	O
1.˛/	O
since	O
o	O
(	O
cid:30	O
)	O
isﬁes	O
percentiles	O
.	O
therefore	O
bcaœ˛	O
d	O
og	O
oh	O
o	O
(	O
cid:30	O
)	O
bcaœ˛	O
d	O
oh	O
(	O
cid:16	O
)	O
og	O
1	O
(	O
cid:0	O
)	O
a.z0	O
c	O
z.˛//	O
(	O
cid:3	O
)	O
d	O
m.r/	O
,	O
the	O
bootstrap	O
cdf	B
(	O
11.79	O
)	O
(	O
cid:0	O
)	O
1fˆœzœ˛g	O
.	O
for	O
a	O
monotone	O
increasing	O
transformation	O
oh	O
of	O
o	O
(	O
cid:30	O
)	O
(	O
cid:3	O
)	O
sat-	O
(	O
cid:3	O
)	O
.˛//	O
for	O
the	O
bootstrap	O
	O
(	O
cid:16	O
)	O
o	O
(	O
cid:0	O
)	O
1œ	O
og	O
.	O
(	O
cid:0	O
)	O
1	O
fˆ	O
.zœ˛/g	O
d	O
m	O
(	O
cid:0	O
)	O
1	O
fˆ	O
.zœ˛/g	O
d	O
m	O
(	O
11.80	O
)	O
o	O
	O
/	O
equals	O
(	O
cid:3	O
)	O
.˛/	O
d	O
m.	O
verifying	O
transformation	O
invariance	O
.	O
(	O
notice	O
that	O
z0	O
d	O
ˆ	O
bcaœ˛	O
o	O
	O
;	O
bootstrap	O
conﬁdence	B
intervals	I
206	O
(	O
cid:0	O
)	O
1œ	O
oh	O
.	O
o	O
(	O
cid:30	O
)	O
/	O
and	O
is	O
also	O
transformation	O
invariant	O
,	O
as	O
is	O
a	O
,	O
as	O
discussed	O
pre-	O
ˆ	O
viously	O
.	O
)	O
exact	O
conﬁdence	B
intervals	I
are	O
transformation	O
invariant	O
,	O
adding	O
consider-	O
ably	O
to	O
their	O
inferential	O
appeal	O
.	O
for	O
approximate	O
intervals	B
,	O
transformation	O
invariance	O
means	O
that	O
if	O
we	O
can	O
demonstrate	O
good	O
behavior	O
on	O
any	O
one	O
n	O
scale	B
then	O
it	O
remains	O
good	O
on	O
all	O
scales	O
.	O
the	O
model	B
(	O
11.38	O
)	O
to	O
the	O
(	O
cid:30	O
)	O
scale	B
can	O
be	O
re-expressed	O
as	O
1	O
c	O
a	O
o	O
(	O
cid:30	O
)	O
o	O
d	O
f1	O
c	O
a	O
(	O
cid:30	O
)	O
gf1	O
c	O
a.z	O
(	O
cid:0	O
)	O
z0/g	O
;	O
(	O
11.81	O
)	O
where	O
z	O
is	O
a	O
standard	O
normal	O
variate	O
,	O
z	O
(	O
cid:24	O
)	O
o	O
(	O
cid:13	O
)	O
d	O
(	O
cid:13	O
)	O
c	O
u	O
;	O
taking	O
logarithms	O
,	O
n	O
.0	O
;	O
1/	O
.	O
(	O
11.82	O
)	O
where	O
o	O
(	O
cid:13	O
)	O
d	O
logf1	O
c	O
a	O
o	O
(	O
cid:30	O
)	O
g	O
,	O
(	O
cid:13	O
)	O
d	O
logf1	O
c	O
a	O
(	O
cid:30	O
)	O
g	O
,	O
and	O
u	O
is	O
the	O
random	O
variable	O
logf1	O
c	O
a.z	O
(	O
cid:0	O
)	O
z0/g	O
;	O
(	O
11.82	O
)	O
represents	O
the	O
simplest	O
kind	O
of	O
translation	O
model	B
,	O
where	O
the	O
unknown	O
value	O
of	O
(	O
cid:13	O
)	O
rigidly	O
shifts	O
the	O
distribution	B
of	O
u	O
.	O
the	O
obvious	O
conﬁdence	O
limit	O
for	O
(	O
cid:13	O
)	O
,	O
o	O
(	O
cid:13	O
)	O
œ˛	O
d	O
o	O
(	O
cid:13	O
)	O
(	O
cid:0	O
)	O
u	O
.1	O
(	O
cid:0	O
)	O
˛/	O
;	O
bcœ˛	O
(	O
11.33	O
)	O
.	O
(	O
11.83	O
)	O
where	O
u	O
.1	O
(	O
cid:0	O
)	O
˛/	O
is	O
the	O
100.1	O
(	O
cid:0	O
)	O
˛/th	O
percentile	O
of	O
u	O
,	O
is	O
then	O
accurate	O
,	O
and	O
also	O
“	O
correct	O
,	O
”	O
according	O
to	O
fisher	O
’	O
s	O
(	O
admittedly	O
vague	O
)	O
logic	O
of	O
inductive	O
inference	B
.	O
it	O
is	O
an	O
algebraic	O
exercise	O
,	O
given	O
in	O
section	O
3	O
of	O
efron	O
(	O
1987	O
)	O
,	O
to	O
reverse	O
the	O
transformations	O
	O
!	O
(	O
cid:30	O
)	O
!	O
(	O
cid:13	O
)	O
and	O
recover	O
o	O
bcaœ˛	O
(	O
11.39	O
)	O
.	O
setting	O
a	O
d	O
0	O
shows	O
the	O
accuracy	B
and	I
correctness	I
of	O
o	O
5	O
[	O
p.	O
192	O
]	O
the	O
acceleration	O
a.	O
this	O
a	O
appears	O
in	O
(	O
11.38	O
)	O
as	O
d	O
(	O
cid:27	O
)	O
(	O
cid:30	O
)	O
=d	O
(	O
cid:30	O
)	O
,	O
the	O
rate	B
of	O
change	O
of	O
o	O
(	O
cid:30	O
)	O
’	O
s	O
standard	B
deviation	I
as	O
a	O
function	B
of	O
its	O
expectation	O
.	O
in	O
one-parameter	B
exponential	O
families	O
it	O
turns	O
out	O
that	O
this	O
is	O
one-third	O
of	O
d	O
(	O
cid:27	O
)	O
	O
=d	O
;	O
that	O
is	O
,	O
the	O
transformation	O
to	O
normality	O
(	O
cid:30	O
)	O
d	O
m.	O
/	O
also	O
decreases	O
the	O
instability	O
of	O
the	O
standard	B
deviation	I
,	O
though	O
not	O
to	O
zero	O
.	O
the	O
variance	O
of	O
the	O
score	O
function	B
p	O
lx.	O
/	O
determines	O
the	O
standard	O
de-	O
viation	O
of	O
the	O
mle	O
o	O
	O
(	O
4.17	O
)	O
–	O
(	O
4.18	O
)	O
.	O
in	O
one-parameter	B
exponential	O
fami-	O
lies	O
,	O
one-sixth	O
the	O
skewness	O
of	O
p	O
lx.	O
/	O
gives	O
a.	O
the	O
skewness	O
connection	O
can	O
be	O
seen	O
at	O
work	O
in	O
estimate	B
(	O
11.40	O
)	O
.	O
in	O
multivariate	B
exponential	O
families	O
(	O
5.50	O
)	O
,	O
the	O
skewness	O
must	O
be	O
evaluated	O
in	O
the	O
“	O
least	O
favorable	O
”	O
direction	O
,	O
discussed	O
further	O
in	O
chapter	O
13.	O
the	O
r	O
algorithm	B
accel	O
(	O
book	O
web	O
site	O
)	O
(	O
cid:3	O
)	O
b/	O
to	O
estimate	B
a.	O
the	O
per-	O
uses	O
b	O
parametric	B
bootstrap	O
replications	O
.	O
centile	O
and	O
bc	O
intervals	B
require	O
only	O
the	O
replications	O
o	O
(	O
cid:3	O
)	O
b	O
,	O
while	O
bca	O
also	O
requires	O
knowledge	O
of	O
the	O
underlying	O
exponential	O
family	O
.	O
see	O
sections	O
4	O
,	O
6	O
,	O
and	O
7	O
of	O
efron	O
(	O
1987	O
)	O
.	O
o	O
	O
o	O
ˇ	O
(	O
cid:3	O
)	O
b	O
;	O
	O
207	O
11.7	O
notes	O
and	O
details	O
	O
d	O
p	O
x2	O
tral	O
chi-square	O
variable	O
with	O
noncentrality	O
parameter	O
	O
d	O
p	O
(	O
cid:22	O
)	O
2	O
6	O
[	O
p.	O
195	O
]	O
equation	B
(	O
11.42	O
)	O
.	O
model	B
(	O
11.41	O
)	O
makes	O
o	O
i	O
a	O
noncen-	O
i	O
and	O
n	O
degrees	O
of	O
freedom	O
,	O
written	O
as	O
o	O
	O
;	O
n.	O
with	O
o	O
	O
d	O
20	O
and	O
n	O
d	O
10	O
,	O
the	O
parametric	B
bootstrap	O
distribution	B
is	O
r	O
(	O
cid:24	O
)	O
(	O
cid:31	O
)	O
2	O
20	O
;	O
10.	O
numerical	O
evaluation	O
gives	O
prf	O
(	O
cid:31	O
)	O
2	O
efron	O
(	O
1985	O
)	O
concerns	O
conﬁdence	B
intervals	I
for	O
parameters	O
	O
d	O
t	B
.	O
(	O
cid:22	O
)	O
/	O
in	O
model	B
(	O
11.41	O
)	O
,	O
where	O
third-order	O
accurate	O
conﬁdence	B
intervals	I
can	O
be	O
cal-	O
culated	O
.	O
the	O
acceleration	O
a	O
equals	O
zero	O
for	O
such	O
problems	O
,	O
making	O
the	O
bc	O
intervals	B
second-order	O
accurate	O
.	O
in	O
practice	O
,	O
the	O
bc	O
intervals	B
usually	O
per-	O
form	B
well	O
,	O
and	O
are	O
a	O
reasonable	O
choice	O
if	O
the	O
accleration	O
a	O
is	O
unavailable	O
.	O
	O
20g	O
d	O
0:156	O
,	O
leading	O
to	O
(	O
11.42	O
)	O
.	O
	O
(	O
cid:24	O
)	O
(	O
cid:31	O
)	O
2	O
20	O
;	O
10	O
7	O
[	O
p.	O
202	O
]	O
bca	O
conﬁdence	B
density	I
(	O
11.68	O
)	O
.	O
deﬁne	O
(	O
cid:0	O
)	O
1h	O
og.	O
/	O
i	O
(	O
cid:0	O
)	O
z0	O
d	O
z	O
d	O
ˆ	O
so	O
that	O
z.˛/	B
d	O
z	O
1	O
c	O
az	O
(	O
cid:0	O
)	O
z0	O
z0	O
c	O
z.˛/	B
1	O
(	O
cid:0	O
)	O
a	O
(	O
cid:0	O
)	O
z0	O
c	O
z.˛/	B
(	O
cid:1	O
)	O
;	O
	O
z	O
1	O
c	O
az	O
(	O
cid:0	O
)	O
z0	O
	O
:	O
(	O
11.84	O
)	O
(	O
11.85	O
)	O
here	O
we	O
are	O
thinking	O
of	O
˛	O
and	O
	O
as	O
functionally	O
related	O
by	O
	O
d	O
o	O
differentiation	O
yields	O
bcaœ˛	O
.	O
and	O
˛	O
d	O
ˆ	O
	O
(	O
cid:16	O
)	O
z	O
(	O
cid:16	O
)	O
z	O
(	O
cid:0	O
)	O
z0	O
1caz	O
.1	O
c	O
az	O
/2	O
1caz	O
	O
.1	O
c	O
az	O
/2'.z	O
c	O
z0/	O
(	O
cid:0	O
)	O
z0	O
;	O
d	O
'	O
d	O
'	O
d˛	O
dz	O
dz	O
d	O
(	O
11.86	O
)	O
og.	O
/	O
;	O
which	O
together	O
give	O
d˛=d	O
,	O
verifying	O
(	O
11.68	O
)	O
.	O
the	O
name	O
“	O
conﬁdence	B
density	I
”	O
seems	O
to	O
appear	O
ﬁrst	O
in	O
efron	O
(	O
1993	O
)	O
,	O
though	O
the	O
idea	O
is	O
familiar	O
in	O
the	O
ﬁducial	O
literature	O
.	O
an	O
ambitious	O
frequen-	O
tist	O
theory	B
of	O
conﬁdence	O
distributions	O
is	O
developed	O
in	O
xie	O
and	O
singh	O
(	O
2013	O
)	O
.	O
8	O
[	O
p.	O
203	O
]	O
jeffreys	O
’	O
prior	B
.	O
formula	B
(	O
11.72	O
)	O
is	O
discussed	O
further	O
in	O
chapter	O
13	O
,	O
in	O
the	O
more	O
general	O
context	O
of	O
uninformative	O
prior	B
distributions	O
.	O
the	O
the-	O
ory	O
of	O
matching	O
priors	B
was	O
initiated	O
by	O
welch	O
and	O
peers	O
(	O
1963	O
)	O
,	O
another	O
important	O
reference	O
being	O
tibshirani	O
(	O
1989	O
)	O
.	O
12	O
cross-validation	O
and	O
cp	O
estimates	O
of	O
prediction	O
error	O
prediction	O
has	O
become	O
a	O
major	O
branch	O
of	O
twenty-ﬁrst-century	O
commerce	O
.	O
questions	O
of	O
prediction	O
arise	O
naturally	O
:	O
how	O
credit-worthy	O
is	O
a	O
loan	O
appli-	O
cant	O
?	O
is	O
a	O
new	O
email	O
message	O
spam	B
?	O
how	O
healthy	O
is	O
the	O
kidney	O
of	O
a	O
poten-	O
tial	O
donor	O
?	O
two	O
problems	O
present	O
themselves	O
:	O
how	O
to	O
construct	O
an	O
effective	O
prediction	O
rule	B
,	O
and	O
how	O
to	O
estimate	B
the	O
accuracy	O
of	O
its	O
predictions	O
.	O
in	O
the	O
language	O
of	O
chapter	O
1	O
,	O
the	O
ﬁrst	O
problem	O
is	O
more	O
algorithmic	O
,	O
the	O
second	O
more	O
inferential	O
.	O
chapters	O
16–19	O
,	O
on	O
machine	O
learning	O
,	O
concern	O
predic-	O
tion	O
rule	B
construction	O
.	O
here	O
we	O
will	O
focus	O
on	O
the	O
second	O
question	O
:	O
having	O
chosen	O
a	O
particular	O
rule	B
,	O
how	O
do	O
we	O
estimate	B
its	O
predictive	O
accuracy	O
?	O
two	O
quite	O
distinct	O
approaches	O
to	O
prediction	O
error	O
assessment	O
developed	O
in	O
the	O
1970s	O
.	O
the	O
ﬁrst	O
,	O
depending	O
on	O
the	O
classical	O
technique	O
of	O
cross-	O
validation	O
,	O
was	O
fully	O
general	O
and	O
nonparametric	O
.	O
a	O
narrower	O
(	O
but	O
more	O
efﬁcient	O
)	O
model-based	O
approach	O
was	O
the	O
second	O
,	O
emerging	O
in	O
the	O
form	B
of	O
mallows	O
’	O
cp	O
estimate	B
and	O
the	O
akaike	O
information	B
criterion	I
(	O
aic	O
)	O
.	O
both	O
theories	O
will	O
be	O
discussed	O
here	O
,	O
beginning	O
with	O
cross-validation	O
,	O
after	O
a	O
brief	O
overview	O
of	O
prediction	O
rules	O
.	O
12.1	O
prediction	O
rules	O
prediction	O
problems	O
typically	O
begin	O
with	O
a	O
training	O
set	B
d	O
consisting	O
of	O
n	O
pairs	O
.xi	O
;	O
yi	O
/	O
,	O
d	O
d	O
f.xi	O
;	O
yi	O
/	O
;	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
ng	O
;	O
(	O
12.1	O
)	O
where	O
xi	O
is	O
a	O
vector	B
of	O
p	O
predictors	O
and	O
yi	O
a	O
real-valued	O
response	O
.	O
on	O
the	O
basis	O
of	O
the	O
training	O
set	B
,	O
a	O
prediction	O
rule	B
rd	O
.x/	O
is	O
constructed	O
such	O
that	O
a	O
prediction	O
oy	O
is	O
produced	O
for	O
any	O
point	O
x	O
in	O
the	O
predictor	B
’	O
s	O
sample	B
space	O
x	O
,	O
oy	O
d	O
rd	O
.x/	O
for	O
x	O
2	O
x	O
:	O
(	O
12.2	O
)	O
208	O
(	O
oy0	O
d	O
if	O
o	O
(	O
cid:25	O
)	O
0	O
(	O
cid:21	O
)	O
0:5	O
if	O
o	O
(	O
cid:25	O
)	O
0	O
<	O
0:5	O
:	O
1	O
0	O
12.1	O
prediction	O
rules	O
209	O
the	O
inferential	O
task	O
is	O
to	O
assess	O
the	O
accuracy	O
of	O
the	O
rule	B
’	O
s	O
predictions	O
.	O
(	O
in	O
practice	O
there	O
are	O
usually	O
several	O
competing	O
rules	O
under	O
consideration	O
and	O
the	O
main	O
question	O
is	O
determining	O
which	O
is	O
best	O
.	O
)	O
in	O
the	O
spam	B
data	O
of	O
section	O
8.1	O
,	O
xi	O
comprised	O
p	O
d	O
57	O
keyword	O
counts	O
,	O
while	O
yi	O
(	O
8.18	O
)	O
indicated	O
whether	O
or	O
not	O
message	O
i	O
was	O
spam	B
.	O
the	O
rule	B
rd	O
.x/	O
in	O
table	O
8.3	O
was	O
an	O
mle	O
logistic	B
regression	I
ﬁt	O
.	O
given	O
a	O
new	O
mes-	O
sage	O
’	O
s	O
count	O
vector	B
,	O
say	O
x0	O
,	O
rd	O
.x0/	O
provided	O
an	O
estimated	O
probability	O
o	O
(	O
cid:25	O
)	O
0	O
of	O
it	O
being	O
spam	B
,	O
which	O
could	O
be	O
converted	O
into	O
a	O
prediction	O
oy0	O
according	O
to	O
(	O
12.3	O
)	O
the	O
diabetes	B
data	O
of	O
table	O
7.2	O
,	O
section	O
7.3	O
,	O
involved	O
the	O
p	O
d	O
10	O
pre-	O
dictors	O
x	O
d	O
(	O
age	O
,	O
sex	O
,	O
.	O
.	O
.	O
,	O
glu	O
)	O
,	O
obtained	O
at	O
baseline	O
,	O
and	O
a	O
response	O
y	O
measuring	O
disease	O
progression	O
one	O
year	O
later	O
.	O
given	O
a	O
new	O
patient	O
’	O
s	O
base-	O
line	O
measurements	O
x0	O
,	O
we	O
would	O
like	O
to	O
predict	O
his	O
or	O
her	O
progression	O
y0	O
.	O
table	O
7.3	O
suggests	O
two	O
possible	O
prediction	O
rules	O
,	O
ordinary	O
least	B
squares	I
and	O
ridge	B
regression	I
using	O
ridge	O
parameter	O
(	O
cid:21	O
)	O
d	O
0:1	O
,	O
either	O
of	O
which	O
will	O
pro-	O
duce	O
a	O
prediction	O
oy0	O
.	O
in	O
this	O
case	O
we	O
might	O
assess	O
prediction	O
error	O
in	O
terms	O
of	O
squared	O
error	O
,	O
.y0	O
(	O
cid:0	O
)	O
oy0/2	O
.	O
in	O
both	O
of	O
these	O
examples	O
,	O
rd	O
.x/	O
was	O
a	O
regression	B
estimator	O
suggested	O
by	O
a	O
probability	O
model	B
.	O
one	O
of	O
the	O
charms	O
of	O
prediction	O
is	O
that	O
the	O
rule	B
rd	O
.x/	O
need	O
not	O
be	O
based	O
on	O
an	O
explicit	O
model	B
.	O
regression	B
trees	O
,	O
as	O
pictured	O
in	O
figure	O
8.7	O
,	O
are	O
widely	O
used1	O
prediction	O
algorithms	O
that	O
do	O
not	O
require	O
model	B
speciﬁcations	O
.	O
prediction	O
,	O
perhaps	O
because	O
of	O
its	O
model-free	O
nature	O
,	O
is	O
an	O
area	O
where	O
algorithmic	O
developments	O
have	O
run	O
far	O
ahead	O
of	O
their	O
in-	O
ferential	O
justiﬁcation	O
.	O
quantifying	O
the	O
prediction	O
error	O
of	O
a	O
rule	B
rd	O
.x/	O
requires	O
speciﬁcation	O
of	O
the	O
discrepancy	O
d.y	O
;	O
oy/	O
between	O
a	O
prediction	O
oy	O
and	O
the	O
actual	O
response	O
y.	O
the	O
two	O
most	O
common	O
choices	O
are	O
squared	O
error	O
d.y	O
;	O
oy/	O
d	O
.y	O
(	O
cid:0	O
)	O
oy/2	O
;	O
(	O
12.4	O
)	O
and	O
classiﬁcation	O
error	O
d.y	O
;	O
oy/	O
d	O
(	O
1	O
if	O
y	O
¤	O
oy	O
0	O
if	O
y	O
d	O
oy	O
;	O
(	O
12.5	O
)	O
when	O
,	O
as	O
with	O
the	O
spam	B
data	O
,	O
the	O
response	O
y	O
is	O
dichotomous	O
.	O
(	O
prediction	O
of	O
a	O
dichotomous	O
response	O
is	O
often	O
called	O
“	O
classiﬁcation.	O
”	O
)	O
1	O
random	O
forests	O
,	O
one	O
of	O
the	O
most	O
popular	O
machine	O
learning	O
prediction	O
algorithms	O
,	O
is	O
an	O
elaboration	O
of	O
regression	B
trees	O
.	O
see	O
chapter	O
17	O
.	O
210	O
cross-validation	O
and	O
cp	O
estimates	O
for	O
the	O
purpose	O
of	O
error	O
estimation	B
,	O
we	O
suppose	O
that	O
the	O
pairs	O
.xi	O
;	O
yi	O
/	O
in	O
the	O
training	O
set	B
d	O
of	O
(	O
12.1	O
)	O
have	O
been	O
obtained	O
by	O
random	O
sampling	O
from	O
some	O
probability	O
distribution	B
f	O
on	O
.p	O
c	O
1/-dimensional	O
space	B
rpc1	O
,	O
iid	O
(	O
cid:24	O
)	O
f	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
:	O
.xi	O
;	O
yi	O
/	O
(	O
12.6	O
)	O
the	O
true	O
error	O
rate	B
errd	O
of	O
rule	B
rd	O
.x/	O
is	O
the	O
expected	O
discrepancy	O
of	O
oy0	O
d	O
rd	O
.x0/	O
from	O
y0	O
given	O
a	O
new	O
pair	O
.x0	O
;	O
y0/	O
drawn	O
from	O
f	O
independently	O
of	O
d	O
,	O
errd	O
d	O
ef	O
fd.y0	O
;	O
oy0/gi	O
(	O
12.7	O
)	O
d	O
(	O
and	O
rd	O
.	O
(	O
cid:1	O
)	O
/	O
)	O
is	O
held	O
ﬁxed	O
in	O
expectation	O
(	O
12.7	O
)	O
,	O
only	O
.x0	O
;	O
y0/	O
varying	O
.	O
figure	O
12.1	O
concerns	O
the	O
supernova	B
data	O
,	O
an	O
example	O
we	O
will	O
return	O
to	O
in	O
the	O
next	O
section.absolute	O
magnitudes	O
yi	O
have	O
been	O
measured	O
for	O
n	O
d	O
39	O
relatively	O
nearby	O
type	O
ia	O
supernovas	O
,	O
with	O
the	O
data	B
scaled	O
such	O
that	O
1	O
ind	O
(	O
cid:24	O
)	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
39	O
;	O
yi	O
(	O
12.8	O
)	O
is	O
a	O
reasonable	O
model	B
.	O
for	O
each	O
supernova	B
,	O
a	O
vector	B
xi	O
of	O
p	O
d	O
10	O
spectral	O
energies	O
has	O
been	O
observed	O
,	O
n	O
.	O
(	O
cid:22	O
)	O
i	O
;	O
1/	O
;	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
39	O
:	O
xi	O
d	O
.xi1	O
;	O
xi	O
2	O
;	O
:	O
:	O
:	O
;	O
xi10/	O
;	O
(	O
12.9	O
)	O
table	O
12.1	O
shows	O
.xi	O
;	O
yi	O
/	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
5	O
.	O
(	O
the	O
frequency	O
measure-	O
ments	O
have	O
been	O
standardized	O
to	O
have	O
mean	O
0	O
and	O
variance	O
1	O
,	O
while	O
y	O
has	O
been	O
adjusted	O
to	O
have	O
mean	O
0	O
.	O
)	O
on	O
the	O
basis	O
of	O
the	O
training	O
set	B
d	O
d	O
f.xi	O
;	O
yi	O
/	O
;	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
39g	O
,	O
we	O
wish	O
to	O
construct	O
a	O
rule	B
rd	O
.x/	O
that	O
,	O
given	O
the	O
frequency	O
vector	B
x0	O
for	O
a	O
newly	O
observed	O
type	O
ia	O
supernova	B
,	O
accurately	O
predicts2	O
its	O
absolute	O
mag-	O
nitude	O
y0	O
.	O
to	O
this	O
end	O
,	O
a	O
lasso	B
estimate	O
q	O
ˇ	O
.	O
(	O
cid:21	O
)	O
/	O
was	O
ﬁt	O
,	O
with	O
y	O
in	O
(	O
7.42	O
)	O
the	O
vector	B
.y1	O
;	O
y2	O
;	O
:	O
:	O
:	O
;	O
y39/	O
and	O
x	O
the	O
39	O
(	O
cid:2	O
)	O
10	O
matrix	B
having	O
ith	O
row	O
xi	O
;	O
(	O
cid:21	O
)	O
was	O
selected	O
to	O
minimize	O
a	O
cp	O
estimate	O
of	O
prediction	O
error	O
,	O
section	O
12.3	O
,	O
yielding	O
prediction	O
rule	B
oy0	O
d	O
x	O
0	O
q	O
ˇ	O
.	O
(	O
cid:21	O
)	O
/	O
:	O
0	O
(	O
12.10	O
)	O
(	O
so	O
in	O
this	O
case	O
constructing	O
rd	O
.x/	O
itself	O
involves	O
error	O
rate	B
estimation	O
.	O
)	O
2	O
type	O
ia	O
supernovas	O
were	O
used	O
as	O
“	O
standard	O
candles	O
”	O
in	O
the	O
discovery	O
of	O
dark	O
energy	O
and	O
the	O
cosmological	O
expansion	O
of	O
the	O
universe	O
,	O
on	O
the	O
grounds	O
that	O
they	O
have	O
constant	O
absolute	O
magnitude	O
.	O
this	O
isn	O
’	O
t	B
exactly	O
true	O
.	O
our	O
training	O
set	B
is	O
unusual	O
in	O
that	O
the	O
39	O
supernovas	O
are	O
close	O
enough	O
to	O
earth	O
to	O
have	O
y	O
ascertained	O
directly	O
.	O
this	O
allows	O
the	O
construction	O
of	O
a	O
prediction	O
rule	B
based	O
on	O
the	O
frequency	O
vector	B
x	O
,	O
which	O
is	O
observable	O
for	O
distant	O
supernovas	O
,	O
leading	O
to	O
improved	O
calibration	O
of	O
the	O
cosmological	O
expansion	O
.	O
12.1	O
prediction	O
rules	O
211	O
figure	O
12.1	O
the	O
supernova	B
data	O
;	O
observed	O
absolute	O
magnitudes	O
yi	O
(	O
on	O
log	O
scale	B
)	O
plotted	O
versus	O
predictions	O
oyi	O
obtained	O
from	O
lasso	O
rule	B
(	O
12.10	O
)	O
,	O
for	O
n	O
d	O
39	O
nearby	O
type	O
ia	O
supernovas	O
.	O
predictions	O
based	O
on	O
10	O
spectral	O
power	O
measurements	O
,	O
7	O
of	O
which	O
had	O
nonzero	O
coefﬁcients	O
in	O
q	O
ˇ.	O
(	O
cid:21	O
)	O
/	O
.	O
the	O
plotted	O
points	O
in	O
figure	O
12.1	O
are	O
.	O
oyi	O
;	O
yi	O
/	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
d	O
39.	O
these	O
gave	O
apparent	O
error	O
nx	O
.yi	O
(	O
cid:0	O
)	O
oyi	O
/2	O
d	O
0:720	O
:	O
err	O
d	O
1	O
comparing	O
this	O
withp.yi	O
(	O
cid:0	O
)	O
ny/2=n	O
d	O
3:91	O
yields	O
an	O
impressive-looking	O
(	O
12.11	O
)	O
n	O
id1	O
“	O
r	O
squared	O
”	O
value	O
r2	O
d	O
1	O
(	O
cid:0	O
)	O
0:720=3:91	O
d	O
0:816	O
:	O
(	O
12.12	O
)	O
things	O
aren	O
’	O
t	B
really	O
that	O
good	O
(	O
see	O
(	O
12.23	O
)	O
)	O
.	O
cross-validation	O
and	O
cp	O
meth-	O
ods	O
allow	O
us	O
to	O
correct	O
apparent	O
errors	B
for	O
the	O
fact	O
that	O
rd	O
.x/	O
was	O
chosen	O
to	O
make	O
the	O
predictions	O
oyi	O
ﬁt	O
the	O
data	B
yi	O
.	O
prediction	O
and	O
estimation	O
are	O
close	O
cousins	O
but	O
they	O
are	O
not	O
twins	O
.	O
as	O
discussed	O
earlier	O
,	O
prediction	O
is	O
less	O
model-dependent	O
,	O
which	O
partly	O
accounts	O
for	O
the	O
distinctions	O
made	O
in	O
section	O
8.4.	O
the	O
prediction	O
criterion	O
err	O
(	O
12.7	O
)	O
lllllllllllllllllllllllllllllllllllllll−4−20246−4−20246predicted	O
magnitude	O
y^iabsolute	O
magnitude	O
yiapparent	O
mean	O
squarederror	O
=	O
0.72	O
212	O
cross-validation	O
and	O
cp	O
estimates	O
table	O
12.1	O
supernova	B
data	O
;	O
10	O
frequency	O
measurements	O
and	O
response	O
variable	O
“	O
absolute	O
magnitude	O
”	O
for	O
the	O
ﬁrst	O
5	O
of	O
n	O
d	O
39	O
type	O
ia	O
supernovas	O
.	O
in	O
terms	O
of	O
notation	O
(	O
12.1	O
)	O
,	O
frequency	O
measurements	O
are	O
x	O
and	O
magnitude	O
y.	O
sn5	O
sn3	O
sn4	O
sn2	O
sn1	O
(	O
cid:0	O
)	O
.08	O
(	O
cid:0	O
)	O
.84	O
(	O
cid:0	O
)	O
1.89	O
.41	O
.26	O
(	O
cid:0	O
)	O
.81	O
(	O
cid:0	O
)	O
.80	O
(	O
cid:0	O
)	O
.93	O
(	O
cid:0	O
)	O
.46	O
1.02	O
(	O
cid:0	O
)	O
.13	O
(	O
cid:0	O
)	O
.21	O
1.14	O
2.41	O
.32	O
(	O
cid:0	O
)	O
.86	O
(	O
cid:0	O
)	O
1.12	O
1.31	O
.77	O
.18	O
(	O
cid:0	O
)	O
.65	O
(	O
cid:0	O
)	O
.94	O
(	O
cid:0	O
)	O
.86	O
(	O
cid:0	O
)	O
.68	O
.68	O
(	O
cid:0	O
)	O
1.27	O
(	O
cid:0	O
)	O
1.53	O
(	O
cid:0	O
)	O
.35	O
.30	O
.72	O
(	O
cid:0	O
)	O
.82	O
.09	O
(	O
cid:0	O
)	O
1.04	O
.62	O
.34	O
.56	O
(	O
cid:0	O
)	O
1.53	O
(	O
cid:0	O
)	O
.43	O
.26	O
(	O
cid:0	O
)	O
1.10	O
.62	O
(	O
cid:0	O
)	O
1.49	O
.18	O
(	O
cid:0	O
)	O
1.32	O
(	O
cid:0	O
)	O
.02	O
(	O
cid:0	O
)	O
.49	O
(	O
cid:0	O
)	O
1.09	O
(	O
cid:0	O
)	O
.3	O
(	O
cid:0	O
)	O
.54	O
(	O
cid:0	O
)	O
1.70	O
(	O
cid:0	O
)	O
:54	O
(	O
cid:0	O
)	O
:22	O
:95	O
(	O
cid:0	O
)	O
3:75	O
2:12	O
x1	O
x2	O
x3	O
x4	O
x5	O
x6	O
x7	O
x8	O
x9	O
x10	O
mag	O
is	O
an	O
expectation	O
over	O
the	O
.x	O
;	O
y/	O
space	B
.	O
this	O
emphasizes	O
good	O
overall	O
per-	O
formance	O
,	O
without	O
much	O
concern	O
for	O
behavior	O
at	O
individual	O
points	O
x	O
in	O
x	O
.	O
shrinkage	B
usually	O
improves	O
prediction	O
.	O
consider	O
a	O
bayesian	O
model	B
like	O
that	O
of	O
section	O
7.1	O
,	O
(	O
cid:22	O
)	O
i	O
(	O
cid:24	O
)	O
n	O
.0	O
;	O
a/	O
and	O
xij	O
(	O
cid:22	O
)	O
i	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
i	O
;	O
1/	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
:	O
the	O
bayes	O
shrinkage	B
estimator	O
,	O
which	O
is	O
ideal	O
for	O
estimation	B
,	O
o	O
(	O
cid:22	O
)	O
i	O
d	O
bxi	O
;	O
b	O
d	O
a=.a	O
c	O
1/	O
;	O
(	O
12.13	O
)	O
(	O
12.14	O
)	O
(	O
12.15	O
)	O
(	O
12.16	O
)	O
(	O
12.17	O
)	O
is	O
also	O
ideal	O
for	O
prediction	O
.	O
suppose	O
that	O
in	O
addition	O
to	O
the	O
observations	O
xi	O
there	O
are	O
independent	O
unobserved	O
replicates	O
,	O
one	O
for	O
each	O
of	O
the	O
n	O
xi	O
values	O
,	O
yi	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
i	O
;	O
1/	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
;	O
that	O
we	O
wish	O
to	O
predict	O
.	O
the	O
bayes	O
predictor	B
oyi	O
d	O
bxi	O
)	O
has	O
overall	O
bayes	O
prediction	O
error	O
(	O
nx	O
.yi	O
(	O
cid:0	O
)	O
oyi	O
/2	O
e	O
1	O
n	O
id1	O
d	O
b	O
c	O
1	O
;	O
213	O
which	O
can	O
not	O
be	O
improved	O
upon	O
.	O
the	O
mle	O
rule	B
oyi	O
d	O
xi	O
has	O
bayes	O
predic-	O
tion	O
error	O
2	O
,	O
which	O
is	O
always	O
worse	O
than	O
(	O
12.17	O
)	O
.	O
12.2	O
cross-validation	O
as	O
far	O
as	O
prediction	O
is	O
concerned	O
it	O
pays	O
to	O
overshrink	O
,	O
as	O
illustrated	O
in	O
figure	O
7.1	O
for	O
the	O
james–stein	O
version	O
of	O
situation	O
(	O
12.13	O
)	O
.	O
this	O
is	O
ﬁne	O
for	O
prediction	O
,	O
but	O
less	O
ﬁne	O
for	O
estimation	B
if	O
we	O
are	O
concerned	O
about	O
extreme	O
cases	O
;	O
see	O
table	O
7.4.	O
prediction	O
rules	O
sacriﬁce	O
the	O
extremes	O
for	O
the	O
sake	O
of	O
the	O
middle	O
,	O
a	O
particularly	O
effective	O
tactic	O
in	O
dichotomous	O
situations	O
(	O
12.5	O
)	O
,	O
where	O
the	O
cost	O
of	O
individual	O
errors	B
is	O
bounded	O
.	O
the	O
most	O
successful	O
ma-	O
chine	O
learning	O
prediction	O
algorithms	O
,	O
discussed	O
in	O
chapters	O
16–19	O
,	O
carry	O
out	O
a	O
version	O
of	O
local	O
bayesian	O
shrinkage	B
in	O
selected	O
regions	O
of	O
x	O
.	O
12.2	O
cross-validation	O
having	O
constructed	O
a	O
prediction	O
rule	B
rd	O
.x/	O
on	O
the	O
basis	O
of	O
training	O
set	B
d	O
,	O
we	O
wish	O
to	O
know	O
its	O
prediction	O
error	O
err	O
d	O
ef	O
fd.y0	O
;	O
oy0/g	O
(	O
12.7	O
)	O
for	O
a	O
new	O
case	O
obtained	O
independently	O
of	O
d.	O
a	O
ﬁrst	O
guess	O
is	O
the	O
apparent	O
error	O
nx	O
id1	O
err	O
d	O
1	O
n	O
d.yi	O
;	O
oyi	O
/	O
;	O
(	O
12.18	O
)	O
the	O
average	O
discrepancy	O
in	O
the	O
training	O
set	B
between	O
yi	O
and	O
its	O
prediction	O
oyi	O
d	O
rd	O
.xi	O
/	O
;	O
err	O
usually	O
underestimates	O
err	O
since	O
rd	O
.x/	O
has	O
been	O
adjusted3	O
to	O
ﬁt	O
the	O
observed	O
responses	O
yi	O
.	O
the	O
ideal	O
remedy	O
,	O
discussed	O
in	O
section	O
12.4	O
,	O
would	O
be	O
to	O
have	O
an	O
inde-	O
pendent	O
validation	O
set	B
(	O
or	O
test	O
set	B
)	O
dval	O
of	O
nval	O
additional	O
cases	O
,	O
nval	O
jd1	O
cross-validation	O
attempts	O
to	O
mimiccerrval	O
without	O
the	O
need	O
for	O
a	O
valida-	O
tion	O
set	B
.	O
deﬁne	O
d	O
.i	O
/	O
to	O
be	O
the	O
reduced	O
training	O
set	B
in	O
which	O
pair	O
.xi	O
;	O
yi	O
/	O
has	O
been	O
omitted	O
,	O
and	O
let	O
rd.i	O
/	O
.	O
(	O
cid:1	O
)	O
/	O
indicate	O
the	O
rule	B
constructed	O
on	O
the	O
basis	O
err	O
dp	O
3	O
linear	B
regression	O
using	O
ordinary	O
least	B
squares	I
ﬁtting	O
provides	O
a	O
classical	O
illustration	O
:	O
i	O
.yi	O
(	O
cid:0	O
)	O
oyi	O
/2=.n	O
(	O
cid:0	O
)	O
p/	O
,	O
where	O
p	O
is	O
i	O
.yi	O
(	O
cid:0	O
)	O
oyi	O
/2=n	O
must	O
be	O
increased	O
top	O
the	O
degrees	O
of	O
freedom	O
,	O
to	O
obtain	O
an	O
unbiased	O
estimate	O
of	O
the	O
noise	O
variance	O
(	O
cid:27	O
)	O
2	O
.	O
(	O
cid:9	O
)	O
:	O
dval	O
d˚.x0j	O
;	O
y0j	O
/	O
;	O
j	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
nval	O
nvalx	O
d.y0j	O
;	O
oy0j	O
/	O
;	O
oy0j	O
d	O
rd	O
.x0j	O
/	O
:	O
cerrval	O
d	O
1	O
this	O
would	O
provide	O
an	O
unbiased	O
estimate	O
of	O
err	O
,	O
(	O
12.19	O
)	O
(	O
12.20	O
)	O
214	O
cross-validation	O
and	O
cp	O
estimates	O
of	O
d	O
.i	O
/	O
.	O
the	O
cross-validation	O
estimate	O
of	O
prediction	O
error	O
is	O
oy.i	O
/	O
d	O
rd.i	O
/.xi	O
/	O
:	O
cerrcv	O
d	O
1	O
d.yi	O
;	O
oy.i	O
//	O
;	O
nx	O
id1	O
n	O
(	O
12.21	O
)	O
now	O
.xi	O
;	O
yi	O
/	O
is	O
not	O
involved	O
in	O
the	O
construction	O
of	O
the	O
prediction	O
rule	B
for	O
yi.cerrcv	O
(	O
12.21	O
)	O
is	O
the	O
“	O
leave	B
one	I
out	I
”	O
version	O
of	O
cross-validation	O
.	O
a	O
more	O
tions	O
for	O
the	O
yi	O
in	O
group	O
j	O
.	O
thencerrcv	O
is	O
evaluated	O
as	O
in	O
(	O
12.21	O
)	O
.	O
besides	O
common	O
tactic	O
is	O
to	O
leave	O
out	O
several	O
pairs	O
at	O
a	O
time	O
:	O
d	O
is	O
randomly	O
parti-	O
tioned	O
into	O
j	O
groups	O
of	O
size	O
about	O
n=j	O
each	O
;	O
d	O
.j	O
/	O
,	O
the	O
training	O
set	B
with	O
group	O
j	O
omitted	O
,	O
provides	O
rule	B
rd.j	O
/.x/	O
,	O
which	O
is	O
used	O
to	O
provide	O
predic-	O
reducing	O
the	O
number	O
of	O
rule	O
constructions	B
necessary	O
,	O
from	O
n	O
to	O
j	O
,	O
group-	O
ing	O
induces	O
larger	O
changes	O
among	O
the	O
j	O
training	O
sets	O
,	O
improving	O
the	O
predic-	O
tive	O
performance	O
on	O
rules	O
rd	O
.x/	O
that	O
include	O
discontinuities	O
.	O
(	O
the	O
argument	B
here	O
is	O
similar	O
to	O
that	O
for	O
the	O
jackknife	O
,	O
section	O
10.1	O
.	O
)	O
cross-validation	O
was	O
applied	O
to	O
the	O
supernova	B
data	O
pictured	O
in	O
figure	O
12.1.	O
the	O
39	O
cases	O
were	O
split	O
,	O
randomly	O
,	O
into	O
j	O
d	O
13	O
groups	O
of	O
three	O
cases	O
each	O
.	O
this	O
gave	O
cerrcv	O
d	O
1:17	O
;	O
(	O
12.22	O
)	O
(	O
12.21	O
)	O
,	O
62	O
%	O
larger	O
than	O
err	O
d	O
0:72	O
(	O
12.11	O
)	O
.	O
the	O
r2	O
calculation	O
(	O
12.12	O
)	O
now	O
yields	O
the	O
smaller	O
value	O
r2	O
d	O
1	O
(	O
cid:0	O
)	O
1:17=3:91	O
d	O
0:701	O
:	O
(	O
12.23	O
)	O
we	O
can	O
apply	O
cross-validation	O
to	O
the	O
spam	B
data	O
of	O
section	O
8.1	O
,	O
having	O
n	O
d	O
4061	O
cases	O
,	O
p	O
d	O
57	O
predictors	O
,	O
and	O
dichotomous	O
response	O
y.	O
for	O
this	O
example	O
,	O
each	O
of	O
the	O
57	O
predictors	O
was	O
itself	O
dichotomized	O
to	O
be	O
either	O
0	O
or	O
1	O
depending	O
on	O
whether	O
the	O
original	O
value	O
xij	O
equaled	O
zero	O
or	O
not	O
.	O
a	O
logistic	B
regression	I
,	O
section	O
8.1	O
,	O
regressing	O
yi	O
on	O
the	O
57	O
dichotomized	O
predictors	O
,	O
gave	O
apparent	O
classiﬁcation	O
error	O
(	O
12.5	O
)	O
i.e.	O
,	O
295	O
wrong	O
predictions	O
among	O
the	O
4061	O
cases	O
.	O
cross-validation	O
,	O
with	O
j	O
d	O
10	O
groups	O
of	O
size	O
460	O
or	O
461	O
each	O
,	O
increased	O
this	O
to	O
err	O
d	O
0:064	O
;	O
cerrcv	O
d	O
0:069	O
;	O
(	O
12.24	O
)	O
(	O
12.25	O
)	O
an	O
increase	O
of	O
8	O
%	O
.	O
glmnet	B
is	O
an	O
automatic	O
model	B
building	O
program	O
that	O
,	O
among	O
other	O
things	O
,	O
constructs	O
a	O
lasso	B
sequence	O
of	O
logistic	B
regression	I
models	O
,	O
adding	O
12.2	O
cross-validation	O
215	O
figure	O
12.2	O
spam	B
data	O
.	O
apparent	O
error	O
rate	B
err	O
(	O
blue	O
)	O
and	O
cross-validated	O
estimate	B
(	O
red	O
)	O
for	O
a	O
sequence	O
of	O
prediction	O
rules	O
generated	O
by	O
glmnet	B
.	O
the	O
degrees	O
of	O
freedom	O
are	O
the	O
number	O
of	O
nonzero	O
regression	B
coefﬁcients	O
:	O
df	O
d	O
57	O
corresponds	O
to	O
ordinary	O
logistic	B
regression	I
,	O
which	O
gave	O
apparent	O
err	O
0.064	O
,	O
cross-validated	O
rate	B
0.069.	O
the	O
minimum	O
cross-validated	O
error	O
rate	B
is	O
0.067.	O
variables	O
one	O
at	O
a	O
time	O
in	O
their	O
order	O
of	O
apparent	O
predictive	O
power	O
;	O
see	O
chapter	O
16.	O
the	O
blue	O
curve	O
in	O
figure	O
12.2	O
tracks	O
the	O
apparent	O
error	O
err	O
(	O
12.18	O
)	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
predictors	O
employed	O
.	O
aside	O
from	O
nu-	O
merical	O
artifacts	O
,	O
err	O
is	O
monotonically	O
decreasing	O
,	O
declining	O
to	O
err	O
d	O
0:064	O
glmnet	B
produced	O
prediction	O
error	O
estimatescerrcv	O
for	O
each	O
of	O
the	O
suc-	O
for	O
the	O
full	B
model	O
that	O
employs	O
all	O
57	O
predictors	O
,	O
i.e.	O
,	O
for	O
the	O
usual	O
logistic	B
regression	I
model	O
,	O
as	O
in	O
(	O
12.24	O
)	O
.	O
cessive	O
models	B
,	O
shown	O
by	O
the	O
red	O
curve	O
.	O
these	O
are	O
a	O
little	O
noisy	O
themselves	O
,	O
but	O
settle	O
down	O
between	O
4	O
%	O
and	O
8	O
%	O
above	O
the	O
corresponding	O
err	O
estimates	O
.	O
the	O
minimum	O
value	O
cerrcv	O
d	O
0:067	O
(	O
12.26	O
)	O
occurred	O
for	O
the	O
model	B
using	O
47	O
predictors	O
.	O
the	O
difference	O
between	O
(	O
12.26	O
)	O
and	O
(	O
12.25	O
)	O
is	O
too	O
small	O
to	O
take	O
seriously	O
given	O
the	O
noise	O
in	O
the	O
cerrcv	O
estimates	O
.	O
there	O
is	O
a	O
more	O
subtle	O
objection	O
:	O
the	O
choice	O
of	O
“	O
best	O
”	O
prediction	O
rule	B
based	O
on	O
comparative	O
cerrcv	O
estimates	O
is	O
not	O
itself	O
cross-validated	O
.	O
each	O
case	O
.xi	O
;	O
yi	O
/	O
is	O
involved	O
in	O
choosing	O
its	O
llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll010203040500.000.050.100.150.200.25degrees	O
of	O
freedommisclassification	O
error	O
ratelllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllogisticregression	O
216	O
own	O
best	O
prediction	O
,	O
socerrcv	O
at	O
the	O
apparently	O
optimum	O
choice	O
can	O
not	O
be	O
cross-validation	O
and	O
cp	O
estimates	O
taken	O
entirely	O
at	O
face	O
value	O
.	O
nevertheless	O
,	O
perhaps	O
the	O
principal	O
use	O
of	O
cross-validation	O
lies	O
in	O
choos-	O
ing	O
among	O
competing	O
prediction	O
rules	O
.	O
whether	O
or	O
not	O
this	O
is	O
fully	O
justiﬁed	O
,	O
it	O
is	O
often	O
the	O
only	O
game	O
in	O
town	O
.	O
that	O
being	O
said	O
,	O
minimum	O
predictive	O
er-	O
ror	O
,	O
no	O
matter	O
how	O
effectuated	O
,	O
is	O
a	O
notably	O
weaker	O
selection	O
principle	O
than	O
minimum	O
variance	O
of	O
estimation	B
.	O
as	O
an	O
example	O
,	O
consider	O
an	O
iid	O
normal	B
sample	O
iid	O
(	O
cid:24	O
)	O
xi	O
(	O
12.27	O
)	O
having	O
mean	O
nx	O
and	O
median	O
mx	O
.	O
both	O
are	O
unbiased	O
for	O
estimating	O
(	O
cid:22	O
)	O
,	O
but	O
nx	O
is	O
much	O
more	O
efﬁcient	O
,	O
n	O
.	O
(	O
cid:22	O
)	O
;	O
1/	O
;	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
25	O
;	O
var.mx/=	O
var.nx/	O
:	O
d	O
1:57	O
:	O
(	O
12.28	O
)	O
e˚.x0	O
(	O
cid:0	O
)	O
mx/2	O
(	O
cid:9	O
)	O
ıe˚.x0	O
(	O
cid:0	O
)	O
nx/2	O
(	O
cid:9	O
)	O
d	O
1:02	O
:	O
suppose	O
we	O
wish	O
to	O
predict	O
a	O
future	O
observation	O
x0	O
independently	O
selected	O
from	O
the	O
same	O
n	O
.	O
(	O
cid:22	O
)	O
;	O
1/	O
distribution	B
.	O
in	O
this	O
case	O
there	O
is	O
very	O
little	O
advan-	O
tage	O
to	O
nx	O
,	O
(	O
12.29	O
)	O
the	O
noise	O
in	O
x0	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
;	O
1/	O
dominates	O
its	O
prediction	O
error	O
.	O
perhaps	O
the	O
proliferation	O
of	O
prediction	O
algorithms	O
to	O
be	O
seen	O
in	O
part	O
iii	O
reﬂects	O
how	O
weakly	O
changes	O
in	O
strategy	O
affect	O
prediction	O
error	O
.	O
table	O
12.2	O
ratio	O
of	O
predictive	O
errors	B
ef.nx0	O
(	O
cid:0	O
)	O
mx/2g=ef.nx0	O
(	O
cid:0	O
)	O
nx/2g	O
for	O
nx0	O
the	O
mean	O
of	O
an	O
independent	O
sample	B
of	O
size	O
n0	O
from	O
n	O
.	O
(	O
cid:22	O
)	O
;	O
1/	O
;	O
nx	O
and	O
mx	O
are	O
the	O
mean	O
and	O
median	O
from	O
xi	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
;	O
1/	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
25.	O
n0	O
ratio	O
1	O
1.02	O
10	O
1.16	O
100	O
1.46	O
1000	O
1	O
1.57	O
1.56	O
in	O
this	O
last	O
example	O
,	O
suppose	O
that	O
our	O
task	O
was	O
to	O
predict	O
the	O
average	O
nx0	O
of	O
n0	O
further	O
draws	O
from	O
the	O
n	O
.	O
(	O
cid:22	O
)	O
;	O
1/	O
distribution	B
.	O
table	O
12.2	O
shows	O
the	O
ratio	O
of	O
predictive	O
errors	B
as	O
a	O
function	B
of	O
n0	O
.	O
the	O
superiority	O
of	O
the	O
mean	O
compared	O
to	O
the	O
median	O
reveals	O
itself	O
as	O
n0	O
gets	O
larger	O
.	O
in	O
this	O
super-	O
simpliﬁed	O
example	O
,	O
the	O
difference	O
between	O
prediction	O
and	O
estimation	O
lies	O
in	O
predicting	O
the	O
average	O
of	O
one	O
versus	O
an	O
inﬁnite	O
number	O
of	O
future	O
obser-	O
vations	O
.	O
doescerrcv	O
actually	O
estimate	B
errd	O
as	O
deﬁned	O
in	O
(	O
12.7	O
)	O
?	O
it	O
seems	O
like	O
the	O
answer	O
must	O
be	O
yes	O
,	O
but	O
there	O
is	O
some	O
doubt	O
expressed	O
in	O
the	O
literature	O
,	O
for	O
12.2	O
cross-validation	O
217	O
reasons	O
demonstrated	O
in	O
the	O
following	O
simulation	O
:	O
we	O
take	O
the	O
true	O
distri-	O
bution	O
f	O
in	O
(	O
12.6	O
)	O
to	O
be	O
the	O
discrete	O
distribution	B
of	O
that	O
puts	O
weight	O
1=39	O
on	O
each	O
of	O
the	O
39	O
.xi	O
;	O
yi	O
/	O
pairs	O
of	O
the	O
supernova	B
data.4	O
a	O
random	O
sample	B
with	O
replacement	O
of	O
size	O
39	O
from	O
of	O
gives	O
simulated	O
data	B
set	O
d	O
(	O
cid:3	O
)	O
and	O
pre-	O
(	O
cid:3	O
)	O
,	O
gives	O
cerr	O
(	O
cid:3	O
)	O
.	O
(	O
cid:1	O
)	O
/	O
based	O
on	O
the	O
lasso/cp	O
recipe	O
used	O
originally	O
.	O
the	O
same	O
diction	O
rule	B
rd	O
(	O
cid:3	O
)	O
cross-validation	O
procedure	O
as	O
before	O
,	O
applied	O
to	O
d	O
cv	O
.	O
because	O
this	O
is	O
a	O
simulation	O
,	O
we	O
can	O
also	O
compute	O
the	O
actual	O
mean-squared	O
error	O
(	O
cid:3	O
)	O
.	O
(	O
cid:1	O
)	O
/	O
applied	O
to	O
the	O
true	O
distribution	B
of	O
,	O
rate	B
of	O
rule	B
rd	O
d	O
.yi	O
;	O
rd	O
(	O
cid:3	O
)	O
.xi	O
//	O
:	O
(	O
12.30	O
)	O
39x	O
id1	O
err	O
(	O
cid:3	O
)	O
d	O
1	O
39	O
with	O
cross-validation	O
estimatecerr	O
supernova	B
data.cerr	O
figure	O
12.3	O
simulation	O
experiment	O
comparing	O
true	O
error	O
err	O
(	O
cid:3	O
)	O
cv	O
;	O
500	O
simulations	O
based	O
on	O
the	O
;	O
cerr	O
(	O
cid:3	O
)	O
cv	O
and	O
err	O
are	O
negatively	O
correlated	O
.	O
(	O
cid:3	O
)	O
figure	O
12.3	O
plots	O
.err	O
(	O
cid:3	O
)	O
ble	O
12.3.	O
cerr	O
cv/	O
for	O
500	O
simulations	O
,	O
using	O
squared	O
er-	O
ror	O
discrepancy	O
d.y	O
;	O
oy/	O
d	O
.y	O
(	O
cid:0	O
)	O
oy/2	O
.	O
summary	O
statistics	B
are	O
given	O
in	O
ta-	O
(	O
cid:3	O
)	O
cv	O
has	O
performed	O
well	O
overall	O
,	O
averaging	B
1.07	O
,	O
quite	O
near	O
the	O
true	O
err	O
1.02	O
,	O
both	O
estimates	O
being	O
80	O
%	O
greater	O
than	O
the	O
average	O
appar-	O
ent	O
error	O
0.57.	O
however	O
,	O
the	O
ﬁgure	O
shows	O
something	O
unsettling	O
:	O
there	O
is	O
a	O
4	O
simulation	O
based	O
on	O
of	O
is	O
the	O
same	O
as	O
nonparametric	B
bootstrap	O
analysis	B
,	O
chapter	O
10	O
.	O
********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************1.01.52.02.50.51.01.52.02.53.0err*errcv*l	O
218	O
cross-validation	O
and	O
cp	O
estimates	O
(	O
cid:3	O
)	O
cv	O
,	O
and	O
apparent	O
(	O
cid:3	O
)	O
table	O
12.3	O
true	O
error	O
err	O
error	O
err	O
(	O
cid:3	O
)	O
;	O
500	O
simulations	O
based	O
on	O
supernova	B
data	O
.	O
correlation	O
(	O
cid:0	O
)	O
0:175	O
(	O
cid:3	O
)	O
between	O
err	O
,	O
cross-validated	O
errorcerr	O
err	O
(	O
cid:3	O
)	O
cerr	O
andcerr	O
(	O
cid:3	O
)	O
cv	O
.	O
(	O
cid:3	O
)	O
cv	O
1.07	O
.34	O
err	O
(	O
cid:3	O
)	O
.57	O
.16	O
mean	O
st	O
dev	O
1.02	O
.27	O
negative	O
correlation	O
betweencerr	O
(	O
cid:3	O
)	O
cv	O
and	O
err	O
(	O
cid:3	O
)	O
.	O
large	O
values	O
ofcerr	O
(	O
cid:3	O
)	O
cv	O
go	O
with	O
smaller	O
values	O
of	O
the	O
true	O
prediction	O
error	O
,	O
and	O
vice	O
versa	O
.	O
our	O
original	O
deﬁnition	O
of	O
err	O
,	O
errd	O
d	O
ef	O
fd.y0	O
;	O
rd	O
.x0//g	O
;	O
for	O
rd	O
(	O
cid:3	O
)	O
.	O
(	O
cid:1	O
)	O
/	O
.	O
ifcerr	O
(	O
12.31	O
)	O
took	O
rd	O
.	O
(	O
cid:1	O
)	O
/	O
ﬁxed	O
as	O
constructed	O
from	O
d	O
,	O
only	O
.x0	O
;	O
y0/	O
(	O
cid:24	O
)	O
f	O
random	O
.	O
in	O
other	O
words	O
,	O
errd	O
was	O
the	O
expected	O
prediction	O
error	O
for	O
the	O
speciﬁc	O
rule	B
rd	O
.	O
(	O
cid:1	O
)	O
/	O
,	O
as	O
(	O
cid:3	O
)	O
as	O
it	O
is	O
,	O
all	O
we	O
can	O
say	O
is	O
thatcerr	O
we	O
would	O
expect	O
to	O
see	O
a	O
positive	O
is	O
err	O
correlation	O
in	O
figure	O
12.3	O
.	O
(	O
cid:3	O
)	O
makes	O
cross-validation	O
a	O
strongly	B
frequentist	O
device	O
:	O
cerrcv	O
is	O
estimating	O
the	O
cv	O
is	O
estimating	O
the	O
expected	O
predictive	O
error	O
,	O
where	O
d	O
as	O
well	O
as	O
.x0	O
;	O
y0/	O
is	O
random	O
in	O
deﬁnition	O
(	O
12.31	O
)	O
.	O
this	O
average	O
prediction	O
error	O
of	O
the	O
algorithm	B
producing	O
rd	O
.	O
(	O
cid:1	O
)	O
/	O
,	O
not	O
of	O
rd	O
.	O
(	O
cid:1	O
)	O
/	O
itself	O
.	O
(	O
cid:3	O
)	O
cv	O
is	O
tracking	O
err	O
(	O
cid:3	O
)	O
12.3	O
covariance	O
penalties	O
cross-validation	O
does	O
its	O
work	O
nonparametrically	O
and	O
without	O
the	O
need	O
for	O
probabilistic	O
modeling	O
.	O
covariance	O
penalty	B
procedures	O
require	O
probability	O
models	B
,	O
but	O
within	O
their	O
ambit	O
they	O
provide	O
less	O
noisy	O
estimates	O
of	O
predic-	O
tion	O
error	O
.	O
some	O
of	O
the	O
most	O
prominent	O
covariance	O
penalty	B
techniques	O
will	O
be	O
examined	O
here	O
,	O
including	O
mallows	O
’	O
cp	O
,	O
akaike	O
’	O
s	O
information	B
criterion	I
(	O
aic	O
)	O
,	O
and	O
stein	O
’	O
s	O
unbiased	B
risk	I
estimate	I
(	O
sure	O
)	O
.	O
the	O
covariance	O
penalty	B
approach	O
treats	O
prediction	O
error	O
estimation	B
in	O
a	O
regression	B
framework	O
:	O
the	O
predictor	B
vectors	O
xi	O
in	O
the	O
training	O
set	B
d	O
d	O
f.xi	O
;	O
yi	O
/	O
;	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
ng	O
(	O
12.1	O
)	O
are	O
considered	O
ﬁxed	O
at	O
their	O
observed	O
values	O
,	O
not	O
random	O
as	O
in	O
(	O
12.6	O
)	O
.	O
an	O
unknown	O
vector	B
(	O
cid:22	O
)	O
of	O
expectations	O
(	O
cid:22	O
)	O
i	O
d	O
efyig	O
has	O
yielded	O
the	O
observed	O
vector	B
of	O
responses	O
y	O
according	O
to	O
some	O
given	O
probability	O
model	B
,	O
which	O
to	O
begin	O
with	O
we	O
assume	O
to	O
have	O
the	O
simple	O
form	B
12.3	O
covariance	O
penalties	O
y	O
(	O
cid:24	O
)	O
.	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
2i/i	O
219	O
(	O
12.32	O
)	O
that	O
is	O
,	O
the	O
yi	O
are	O
uncorrelated	O
,	O
with	O
yi	O
having	O
unknown	O
mean	O
(	O
cid:22	O
)	O
i	O
and	O
vari-	O
ance	O
(	O
cid:27	O
)	O
2.	O
we	O
take	O
(	O
cid:27	O
)	O
2	O
as	O
known	O
,	O
though	O
in	O
practice	O
it	O
must	O
usually	O
be	O
esti-	O
mated	O
.	O
a	O
regression	B
rule	O
r.	O
(	O
cid:1	O
)	O
/	O
has	O
been	O
used	O
to	O
produce	O
an	O
estimate	O
of	O
vector	O
(	O
cid:22	O
)	O
,	O
(	O
12.33	O
)	O
o	O
(	O
cid:22	O
)	O
d	O
r.y/	O
:	O
(	O
only	O
y	O
is	O
included	O
in	O
the	O
notation	O
since	O
the	O
predictors	O
xi	O
are	O
considered	O
ﬁxed	O
and	O
known	O
.	O
)	O
for	O
instance	O
we	O
might	O
take	O
o	O
(	O
cid:22	O
)	O
d	O
r.y/	O
d	O
x	O
.x	O
0	O
(	O
cid:0	O
)	O
1x	O
0	O
y	O
;	O
x	O
/	O
(	O
12.34	O
)	O
where	O
x	O
is	O
the	O
n	O
(	O
cid:2	O
)	O
p	O
matrix	B
having	O
xi	O
as	O
the	O
ith	O
row	O
,	O
as	O
suggested	O
by	O
the	O
linear	B
regression	O
model	B
(	O
cid:22	O
)	O
d	O
x	O
ˇ.	O
in	O
covariance	O
penalty	B
calculations	O
,	O
the	O
estimator	B
o	O
(	O
cid:22	O
)	O
also	O
functions	O
as	O
a	O
predictor	B
.	O
we	O
wonder	O
how	O
accurate	O
o	O
(	O
cid:22	O
)	O
d	O
r.y/	O
will	O
be	O
in	O
predicting	O
a	O
new	O
vector	B
of	O
observations	O
y0	O
from	O
model	O
(	O
12.32	O
)	O
,	O
y0	O
(	O
cid:24	O
)	O
.	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
2i/	O
;	O
independent	O
of	O
y	O
:	O
(	O
12.35	O
)	O
to	O
begin	O
with	O
,	O
prediction	O
error	O
will	O
be	O
assessed	O
in	O
terms	O
of	O
squared	O
dis-	O
crepancy	O
,	O
(	O
12.36	O
)	O
for	O
component	O
i	O
,	O
where	O
e0	O
indicates	O
expectation	O
with	O
y0i	O
random	O
but	O
o	O
(	O
cid:22	O
)	O
i	O
held	O
ﬁxed	O
.	O
overall	O
prediction	O
error	O
is	O
the	O
average5	O
erri	O
d	O
e0	O
˚.y0i	O
(	O
cid:0	O
)	O
o	O
(	O
cid:22	O
)	O
i	O
/2	O
(	O
cid:9	O
)	O
nx	O
erri	O
:	O
err	O
(	O
cid:1	O
)	O
d	O
1	O
id1	O
the	O
apparent	O
error	O
for	O
component	O
i	O
is	O
n	O
erri	O
d	O
.yi	O
(	O
cid:0	O
)	O
o	O
(	O
cid:22	O
)	O
i	O
/2	O
:	O
(	O
12.37	O
)	O
(	O
12.38	O
)	O
a	O
simple	O
but	O
powerful	O
lemma	O
underlies	O
the	O
theory	B
of	O
covariance	O
penalties	O
.	O
lemma	O
let	O
e	O
indicate	O
expectation	O
over	O
both	O
y	O
in	O
(	O
12.32	O
)	O
and	O
y0	O
in	O
(	O
12.35	O
)	O
.	O
then	O
eferrig	O
d	O
eferrig	O
c	O
2	O
cov	O
.	O
o	O
(	O
cid:22	O
)	O
i	O
;	O
yi	O
/	O
;	O
(	O
12.39	O
)	O
5	O
err	O
(	O
cid:1	O
)	O
is	O
sometimes	O
called	O
“	O
insample	O
error	O
,	O
”	O
as	O
opposed	O
to	O
“	O
outsample	O
error	O
”	O
err	O
(	O
12.7	O
)	O
,	O
though	O
in	O
practice	O
the	O
two	O
tend	O
to	O
behave	O
similarly	O
.	O
cross-validation	O
and	O
cp	O
estimates	O
220	O
where	O
the	O
last	O
term	O
is	O
the	O
covariance	O
between	O
the	O
ith	O
components	O
of	O
o	O
(	O
cid:22	O
)	O
and	O
y	O
,	O
cov	O
.	O
o	O
(	O
cid:22	O
)	O
i	O
;	O
yi	O
/	O
d	O
e	O
f.	O
o	O
(	O
cid:22	O
)	O
i	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
i	O
/.yi	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
i	O
/g	O
:	O
(	O
12.40	O
)	O
i	O
(	O
cid:0	O
)	O
2	O
(	O
cid:15	O
)	O
i	O
ıi	O
c	O
ı2	O
(	O
note	O
:	O
(	O
12.40	O
)	O
does	O
not	O
require	O
ef	O
o	O
(	O
cid:22	O
)	O
ig	O
d	O
(	O
cid:22	O
)	O
i	O
.	O
)	O
proof	O
letting	O
(	O
cid:15	O
)	O
i	O
d	O
yi	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
i	O
and	O
ıi	O
d	O
.	O
o	O
(	O
cid:22	O
)	O
i	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
i	O
/	O
,	O
the	O
elementary	O
equality	O
.	O
(	O
cid:15	O
)	O
i	O
(	O
cid:0	O
)	O
ıi	O
/2	O
d	O
(	O
cid:15	O
)	O
2	O
.yi	O
(	O
cid:0	O
)	O
o	O
(	O
cid:22	O
)	O
i	O
/2	O
d	O
.yi	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
i	O
/2	O
(	O
cid:0	O
)	O
2.	O
o	O
(	O
cid:22	O
)	O
i	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
i	O
/.yi	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
i	O
/	O
c	O
.	O
o	O
(	O
cid:22	O
)	O
i	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
i	O
/2	O
;	O
and	O
likewise	O
.y0i	O
(	O
cid:0	O
)	O
o	O
(	O
cid:22	O
)	O
i	O
/2	O
d	O
.y0i	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
i	O
/2	O
(	O
cid:0	O
)	O
2.	O
o	O
(	O
cid:22	O
)	O
i	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
i	O
/.y0i	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
i	O
/c	O
.	O
o	O
(	O
cid:22	O
)	O
i	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
i	O
/2	O
:	O
(	O
12.42	O
)	O
taking	O
expectations	O
,	O
(	O
12.41	O
)	O
gives	O
i	O
becomes	O
(	O
12.41	O
)	O
eferrig	O
d	O
(	O
cid:27	O
)	O
2	O
(	O
cid:0	O
)	O
2	O
cov	O
.	O
o	O
(	O
cid:22	O
)	O
i	O
;	O
yi	O
/	O
c	O
e.	O
o	O
(	O
cid:22	O
)	O
i	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
i	O
/2	O
;	O
(	O
12.43	O
)	O
(	O
12.44	O
)	O
while	O
(	O
12.42	O
)	O
gives	O
eferrig	O
d	O
(	O
cid:27	O
)	O
2	O
c	O
e.	O
o	O
(	O
cid:22	O
)	O
i	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
i	O
/2	O
;	O
the	O
middle	O
term	O
on	O
the	O
right	O
side	O
of	O
(	O
12.42	O
)	O
equaling	O
zero	O
because	O
of	O
the	O
independence	O
of	O
y0i	O
and	O
o	O
(	O
cid:22	O
)	O
i.	O
taking	O
the	O
difference	O
between	O
(	O
12.44	O
)	O
and	O
(	O
cid:4	O
)	O
(	O
12.43	O
)	O
veriﬁes	O
the	O
lemma	O
.	O
note	O
:	O
the	O
lemma	O
remains	O
valid	O
if	O
(	O
cid:27	O
)	O
2	O
varies	O
with	O
i.	O
the	O
lemma	O
says	O
that	O
,	O
on	O
average	O
,	O
the	O
apparent	O
error	O
erri	O
understimates	O
the	O
true	O
prediction	O
error	O
erri	O
by	O
the	O
covariance	O
penalty	B
2	O
cov	O
.	O
o	O
(	O
cid:22	O
)	O
i	O
;	O
yi	O
/	O
.	O
(	O
this	O
makes	O
intuitive	O
sense	O
since	O
cov	O
.	O
(	O
cid:22	O
)	O
i	O
;	O
yi	O
/	O
measures	O
the	O
amount	O
by	O
which	O
yi	O
inﬂuences	O
its	O
own	O
prediction	O
o	O
(	O
cid:22	O
)	O
i	O
.	O
)	O
covariance	O
penalty	B
estimates	O
of	O
predic-	O
tion	O
error	O
take	O
the	O
formcerri	O
d	O
erri	O
c2dcov	O
.	O
o	O
(	O
cid:22	O
)	O
i	O
;	O
yi	O
/	O
;	O
wheredcov	O
.	O
o	O
(	O
cid:22	O
)	O
i	O
;	O
yi	O
/	O
approximates	O
cov	O
.	O
(	O
cid:22	O
)	O
i	O
;	O
yi	O
/	O
;	O
overall	O
prediction	O
error	O
(	O
12.37	O
)	O
cerr	O
(	O
cid:1	O
)	O
d	O
errc	O
2	O
dcov	O
.	O
o	O
(	O
cid:22	O
)	O
i	O
;	O
yi	O
/	O
;	O
where	O
err	O
dp	O
erri	O
=n	O
as	O
before	O
.	O
the	O
form	B
ofdcov	O
.	O
o	O
(	O
cid:22	O
)	O
i	O
;	O
yi	O
/	O
in	O
(	O
12.45	O
)	O
depends	O
on	O
the	O
context	O
assumed	O
for	O
is	O
estimated	O
by	O
nx	O
(	O
12.45	O
)	O
(	O
12.46	O
)	O
n	O
id1	O
the	O
prediction	O
problem	O
.	O
12.3	O
covariance	O
penalties	O
221	O
(	O
1	O
)	O
suppose	O
that	O
o	O
(	O
cid:22	O
)	O
d	O
r.y/	O
in	O
(	O
12.32	O
)	O
–	O
(	O
12.33	O
)	O
is	O
linear	B
,	O
(	O
12.47	O
)	O
where	O
c	O
is	O
a	O
known	O
n	O
-vector	O
and	O
m	O
a	O
known	O
n	O
(	O
cid:2	O
)	O
n	O
matrix	B
.	O
then	O
the	O
covariance	O
matrix	B
between	O
o	O
(	O
cid:22	O
)	O
and	O
y	O
is	O
o	O
(	O
cid:22	O
)	O
d	O
c	O
c	O
my	O
;	O
giving	O
cov	O
.	O
o	O
(	O
cid:22	O
)	O
i	O
;	O
yi	O
/	O
d	O
(	O
cid:27	O
)	O
2mi	O
i	O
,	O
mi	O
i	O
the	O
ith	O
diagonal	O
element	O
of	O
m	O
,	O
and	O
,	O
since	O
err	O
dp	O
cov	O
.	O
o	O
(	O
cid:22	O
)	O
;	O
y/	O
d	O
(	O
cid:27	O
)	O
2m	O
;	O
cerri	O
d	O
erri	O
c2	O
(	O
cid:27	O
)	O
2mi	O
i	O
;	O
i	O
.yi	O
(	O
cid:0	O
)	O
o	O
(	O
cid:22	O
)	O
i	O
/2=n	O
,	O
nx	O
cerr	O
(	O
cid:1	O
)	O
d	O
1	O
.yi	O
(	O
cid:0	O
)	O
o	O
(	O
cid:22	O
)	O
i	O
/2	O
c	O
2	O
(	O
cid:27	O
)	O
2	O
(	O
12.48	O
)	O
(	O
12.49	O
)	O
tr.m	O
/	O
:	O
(	O
12.50	O
)	O
formula	B
(	O
12.50	O
)	O
is	O
mallows	O
’	O
cp	O
estimate	O
of	O
prediction	O
error	O
.	O
for	O
ols	O
0	O
has	O
tr.m	O
/	O
d	O
p	O
,	O
the	O
number	O
of	O
x	O
/	O
0	O
n	O
n	O
id1	O
estimation	B
(	O
12.34	O
)	O
,	O
m	O
d	O
x	O
.x	O
nx	O
cerr	O
(	O
cid:1	O
)	O
d	O
1	O
predictors	O
,	O
so	O
.yi	O
(	O
cid:0	O
)	O
o	O
(	O
cid:22	O
)	O
i	O
/2	O
c	O
2	O
(	O
cid:0	O
)	O
1x	O
n	O
id1	O
y	O
yielded	O
err	O
dp.yi	O
(	O
cid:0	O
)	O
o	O
(	O
cid:22	O
)	O
i	O
/2=39	O
d	O
0:719.	O
the	O
covariance	O
penalty	B
,	O
with	O
for	O
the	O
supernova	B
data	O
(	O
12.8	O
)	O
–	O
(	O
12.9	O
)	O
,	O
the	O
ols	O
predictor	B
o	O
(	O
cid:22	O
)	O
d	O
x	O
.x	O
(	O
cid:0	O
)	O
1	O
0	O
n	O
d	O
39	O
,	O
(	O
cid:27	O
)	O
2	O
d	O
1	O
,	O
and6	O
p	O
d	O
10	O
,	O
was	O
0.513	O
,	O
giving	O
cp	O
estimate	O
of	O
predic-	O
tion	O
error	O
x	O
/	O
x	O
n	O
0	O
cerr	O
(	O
cid:1	O
)	O
d	O
0:719	O
c	O
0:513	O
d	O
1:23	O
:	O
(	O
12.52	O
)	O
(	O
cid:27	O
)	O
2p	O
:	O
(	O
12.51	O
)	O
for	O
ols	O
regression	B
,	O
the	O
degrees	O
of	O
freedom	O
p	O
,	O
the	O
rank	O
of	O
matrix	B
x	O
in	O
(	O
12.34	O
)	O
,	O
determines	O
the	O
covariance	O
penalty	B
.2=n	O
/	O
(	O
cid:27	O
)	O
2p	O
in	O
(	O
12.51	O
)	O
.	O
compar-	O
ing	O
this	O
with	O
(	O
12.46	O
)	O
leads	O
to	O
a	O
general	O
deﬁnition	O
of	O
degrees	O
of	O
freedom	O
df	O
for	O
a	O
regression	B
rule	O
o	O
(	O
cid:22	O
)	O
d	O
r.y/	O
,	O
df	O
d	O
.1=	O
(	O
cid:27	O
)	O
2/	O
nx	O
id1	O
dcov	O
.	O
o	O
(	O
cid:22	O
)	O
i	O
;	O
yi	O
/	O
:	O
(	O
12.53	O
)	O
this	O
deﬁnition	O
provides	O
common	O
ground	O
for	O
comparing	O
different	O
types	O
of	O
regression	B
rules	O
.	O
rules	O
with	O
larger	O
df	O
are	O
more	O
ﬂexible	O
and	O
tend	O
toward	O
better	O
apparent	O
ﬁts	O
to	O
the	O
data	B
,	O
but	O
require	O
bigger	O
covariance	O
penalties	O
for	O
fair	O
comparison	O
.	O
6	O
we	O
are	O
not	O
counting	O
the	O
intercept	O
as	O
an	O
11th	O
predictor	B
since	O
y	O
and	O
all	O
the	O
xi	O
were	O
standardized	O
to	O
have	O
mean	O
0	O
,	O
all	O
our	O
models	B
assuming	O
zero	O
intercept	O
.	O
2	O
3	O
222	O
cross-validation	O
and	O
cp	O
estimates	O
(	O
2	O
)	O
for	O
lasso	B
estimation	O
(	O
7.42	O
)	O
and	O
(	O
12.10	O
)	O
,	O
it	O
can	O
be	O
shown	O
that	O
for-	O
mula	O
(	O
12.51	O
)	O
,	O
with	O
p	O
equaling	O
the	O
number	O
of	O
nonzero	O
regression	B
coefﬁ-	O
cients	O
,	O
holds	O
to	O
a	O
good	O
approximation	O
.	O
	O
the	O
lasso	B
rule	O
used	O
in	O
figure	O
12.1	O
for	O
the	O
supernova	B
data	O
had	O
p	O
d	O
7	O
;	O
err	O
was	O
0.720	O
for	O
this	O
rule	B
,	O
almost	O
the	O
same	O
as	O
for	O
the	O
ols	O
rule	B
above	O
,	O
but	O
the	O
cp	O
penalty	B
is	O
less	O
,	O
2	O
(	O
cid:1	O
)	O
7=39	O
d	O
0:359	O
,	O
giving	O
cerr	O
(	O
cid:1	O
)	O
d	O
0:720	O
c	O
0:359	O
d	O
1:08	O
;	O
compared	O
with	O
1.23	O
for	O
ols	O
.	O
this	O
estimate	B
does	O
not	O
account	O
for	O
the	O
data-	O
based	O
selection	O
of	O
the	O
choice	O
p	O
d	O
7	O
,	O
see	O
item	O
(	O
4	O
)	O
below	O
.	O
(	O
3	O
)	O
if	O
we	O
are	O
willing	O
to	O
add	O
multivariate	B
normality	O
to	O
model	B
(	O
12.32	O
)	O
,	O
(	O
12.54	O
)	O
y	O
(	O
cid:24	O
)	O
np	O
.	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
2i/	O
;	O
(	O
12.55	O
)	O
we	O
can	O
drop	O
the	O
assumption	O
of	O
linearity	O
(	O
12.47	O
)	O
.	O
in	O
this	O
case	O
it	O
can	O
be	O
shown	O
that	O
,	O
for	O
any	O
differentiable	O
estimator	B
o	O
(	O
cid:22	O
)	O
d	O
r.y/	O
,	O
the	O
covariance	O
in	O
formula	B
(	O
12.51	O
)	O
is	O
given	O
by	O
cov	O
.	O
o	O
(	O
cid:22	O
)	O
i	O
;	O
yi	O
/	O
d	O
(	O
cid:27	O
)	O
2ef	O
@	O
o	O
(	O
cid:22	O
)	O
i	O
=	O
@	O
yig	O
;	O
(	O
12.56	O
)	O
(	O
cid:27	O
)	O
2	O
times	O
the	O
partial	O
derivative	O
of	O
o	O
(	O
cid:22	O
)	O
i	O
with	O
respect	O
to	O
yi	O
.	O
(	O
another	O
measure	O
of	O
yi	O
’	O
s	O
inﬂuence	O
on	O
its	O
own	O
prediction	O
.	O
)	O
the	O
sure	O
formula	B
(	O
stein	O
’	O
s	O
unbiased	O
risk	O
estimator	O
)	O
is	O
;	O
(	O
12.57	O
)	O
with	O
corresponding	O
estimate	B
for	O
overall	O
prediction	O
error	O
cerri	O
d	O
erri	O
c2	O
(	O
cid:27	O
)	O
2	O
@	O
o	O
(	O
cid:22	O
)	O
i	O
nx	O
cerr	O
(	O
cid:1	O
)	O
d	O
errc	O
2	O
(	O
cid:27	O
)	O
2	O
@	O
yi	O
@	O
o	O
(	O
cid:22	O
)	O
i	O
(	O
12.58	O
)	O
sure	O
was	O
applied	O
to	O
the	O
rule	B
o	O
(	O
cid:22	O
)	O
d	O
lowess	B
(	O
x	O
,	O
y,1/3	O
)	O
for	O
the	O
kid-	O
ney	O
ﬁtness	O
data	B
of	O
figure	O
1.2.	O
the	O
open	O
circles	O
in	O
figure	O
12.4	O
plot	O
the	O
component-wise	O
degrees	O
of	O
freedom	O
estimates7	O
id1	O
@	O
yi	O
n	O
:	O
(	O
obtained	O
by	O
numerical	O
differentiation	O
)	O
versus	O
agei	O
.	O
their	O
sum	O
@	O
o	O
(	O
cid:22	O
)	O
i	O
@	O
yi	O
;	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
d	O
157	O
;	O
nx	O
@	O
o	O
(	O
cid:22	O
)	O
i	O
d	O
6:67	O
id1	O
@	O
yi	O
(	O
12.59	O
)	O
(	O
12.60	O
)	O
7	O
notice	O
that	O
the	O
factor	B
(	O
cid:27	O
)	O
2	O
in	O
(	O
12.56	O
)	O
cancels	O
out	O
in	O
(	O
12.53	O
)	O
.	O
12.3	O
covariance	O
penalties	O
223	O
estimates	O
the	O
total	O
degrees	O
of	O
freedom	O
,	O
as	O
in	O
(	O
12.53	O
)	O
,	O
implying	O
that	O
lowess	B
(	O
x	O
,	O
y,1/3	O
)	O
is	O
about	O
as	O
ﬂexible	O
as	O
a	O
sixth-degree	O
polynomial	O
ﬁt	O
,	O
with	O
df	O
=	O
7.	O
figure	O
12.4	O
analysis	B
of	O
the	O
lowess	B
(	O
x	O
,	O
y,1/3	O
)	O
ﬁt	O
to	O
kidney	O
data	O
of	O
figure	O
1.2.	O
open	O
circles	O
are	O
sure	O
coordinate-wise	O
df	O
estimates	O
@	O
o	O
(	O
cid:22	O
)	O
i	O
=	O
@	O
yi	O
,	O
plotted	O
versus	O
agei	O
,	O
giving	O
total	O
degrees	O
of	O
freedom	O
6.67.	O
the	O
solid	O
curve	O
tracks	O
bootstrap	O
coordinate-wise	O
estimates	O
(	O
12.65	O
)	O
,	O
with	O
their	O
sum	O
giving	O
total	O
df	O
d	O
6:81	O
.	O
(	O
4	O
)	O
the	O
parametric	B
bootstrap8	O
of	O
section	O
10.4	O
can	O
be	O
used	O
to	O
estimate	B
the	O
covariances	O
cov	O
.	O
o	O
(	O
cid:22	O
)	O
i	O
;	O
yi	O
/	O
in	O
the	O
lemma	O
(	O
12.39	O
)	O
.	O
the	O
data	B
vector	O
y	O
is	O
as-	O
sumed	O
to	O
be	O
generated	O
from	O
a	O
member	O
f	O
(	O
cid:22	O
)	O
.y/	O
of	O
a	O
given	O
parametric	B
family	O
d˚f	O
(	O
cid:22	O
)	O
.y/	O
;	O
(	O
cid:22	O
)	O
2	O
(	O
cid:127	O
)	O
(	O
cid:9	O
)	O
;	O
f	O
yielding	O
o	O
(	O
cid:22	O
)	O
d	O
r.y/	O
,	O
(	O
12.62	O
)	O
parametric	B
bootstrap	O
replications	O
of	O
y	O
and	O
o	O
(	O
cid:22	O
)	O
are	O
obtained	O
by	O
analogy	O
with	O
f	O
(	O
cid:22	O
)	O
!	O
y	O
!	O
o	O
(	O
cid:22	O
)	O
d	O
r.y/	O
:	O
(	O
12.61	O
)	O
8	O
there	O
is	O
also	O
a	O
nonparametric	B
bootstrap	O
competitor	O
to	O
cross-validation	O
,	O
the	O
“	O
.632	O
estimate	O
;	O
”	O
see	O
the	O
chapter	O
endnote	O
4	O
.	O
lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll20304050607080900.00.10.20.30.4agedf	O
estimatedegrees	O
of	O
freedomsure	O
=	O
6.67bootstrap	O
=	O
6.81	O
224	O
(	O
12.62	O
)	O
,9	O
cross-validation	O
and	O
cp	O
estimates	O
a	O
large	O
number	O
b	O
of	O
replications	O
then	O
yield	O
bootstrap	O
estimates	O
f	O
o	O
(	O
cid:22	O
)	O
!	O
y	O
bx	O
dcov	O
.	O
o	O
(	O
cid:22	O
)	O
i	O
;	O
yi	O
/	O
d	O
1	O
.	O
o	O
(	O
cid:22	O
)	O
(	O
cid:3	O
)	O
b	O
i	O
b	O
bd1	O
(	O
cid:3	O
)	O
!	O
o	O
(	O
cid:22	O
)	O
(	O
cid:3	O
)	O
d	O
r.y	O
(	O
cid:3	O
)	O
/	O
:	O
(	O
12.63	O
)	O
(	O
cid:0	O
)	O
o	O
(	O
cid:22	O
)	O
(	O
cid:3	O
)	O
(	O
cid:1	O
)	O
i	O
/.y	O
(	O
cid:3	O
)	O
b	O
i	O
(	O
cid:0	O
)	O
y	O
(	O
cid:3	O
)	O
(	O
cid:1	O
)	O
i	O
/	O
;	O
(	O
12.64	O
)	O
b	O
d	O
1000	O
parametric	B
bootstrap	O
replications	O
.	O
o	O
(	O
cid:22	O
)	O
(	O
cid:3	O
)	O
the	O
dot	O
notation	O
indicating	O
averages	O
over	O
the	O
b	O
replications	O
.	O
/	O
were	O
obtained	O
from	O
the	O
normal	B
model	O
(	O
12.55	O
)	O
,	O
taking	O
o	O
(	O
cid:22	O
)	O
in	O
(	O
12.63	O
)	O
to	O
be	O
the	O
estimate	B
from	O
lowess	B
(	O
x	O
,	O
y,1/3	O
)	O
as	O
in	O
figure	O
1.2.	O
a	O
standard	O
linear	O
regression	B
,	O
of	O
y	O
as	O
a	O
12th-degree	O
polynomial	O
function	B
of	O
age	O
,	O
gave	O
o	O
(	O
cid:27	O
)	O
2	O
d	O
3:28.	O
covariances	O
were	O
computed	O
as	O
in	O
(	O
12.64	O
)	O
,	O
yielding	O
coordinate-wise	O
degrees	O
of	O
freedom	O
estimates	O
(	O
12.53	O
)	O
,	O
;	O
y	O
(	O
cid:3	O
)	O
dfi	O
ddcov	O
.	O
o	O
(	O
cid:22	O
)	O
i	O
;	O
yi	O
/=o	O
(	O
cid:27	O
)	O
2	O
:	O
(	O
12.65	O
)	O
the	O
solid	O
curve	O
in	O
figure	O
12.4	O
plots	O
dfi	O
as	O
a	O
function	B
of	O
agei	O
.	O
these	O
are	O
seen	O
to	O
be	O
similar	O
to	O
but	O
less	O
noisy	O
than	O
the	O
sure	O
estimates	O
.	O
they	O
totaled	O
6.81	O
,	O
nearly	O
the	O
same	O
as	O
(	O
12.60	O
)	O
.	O
the	O
overall	O
covariance	O
penalty	B
term	O
in	O
(	O
12.46	O
)	O
equaled	O
0.284	O
,	O
increasingcerr	O
(	O
cid:1	O
)	O
by	O
about	O
9	O
%	O
over	O
err	O
d	O
3:15.	O
the	O
advantage	O
of	O
parametric	B
bootstrap	O
estimates	O
(	O
12.64	O
)	O
of	O
covariance	O
penalties	O
is	O
their	O
applicability	O
to	O
any	O
prediction	O
rule	B
o	O
(	O
cid:22	O
)	O
d	O
r.y/	O
no	O
matter	O
how	O
exotic	O
.	O
applied	O
to	O
the	O
lasso	B
estimates	O
for	O
the	O
supernova	B
data	O
,	O
b	O
d	O
1000	O
replications	O
yielded	O
total	O
df	O
d	O
6:85	O
for	O
the	O
rule	B
that	O
always	O
used	O
p	O
d	O
7	O
predictors	O
,	O
compared	O
with	O
the	O
theoretical	O
approximation	O
df	O
d	O
7.	O
another	O
1000	O
replications	O
,	O
now	O
letting	O
o	O
(	O
cid:22	O
)	O
/	O
choose	O
the	O
apparently	O
(	O
cid:3	O
)	O
each	O
time	O
,	O
increased	O
the	O
df	O
estimate	B
to	O
7.48	O
,	O
so	O
the	O
adaptive	B
choice	O
best	O
p	O
of	O
p	O
cost	O
about	O
0.6	O
extra	O
degrees	O
of	O
freedom	O
.	O
these	O
calculations	O
exem-	O
plify	O
modern	O
computer-intensive	O
inference	B
,	O
carrying	O
through	O
error	O
estima-	O
tion	O
for	O
complicated	O
adaptive	B
prediction	O
rules	O
on	O
a	O
totally	O
automatic	O
basis	O
.	O
(	O
5	O
)	O
covariance	O
penalties	O
can	O
apply	O
to	O
measures	O
of	O
prediction	O
error	O
other	O
than	O
squared	O
error	O
d.yi	O
;	O
o	O
(	O
cid:22	O
)	O
i	O
/	O
d	O
.yi	O
(	O
cid:0	O
)	O
o	O
(	O
cid:22	O
)	O
i	O
/2	O
.	O
we	O
will	O
discuss	O
two	O
examples	O
of	O
a	O
general	O
theory	B
.	O
first	O
consider	O
classiﬁcation	O
,	O
where	O
yi	O
equals	O
0	O
or	O
1	O
and	O
9	O
it	O
isn	O
’	O
t	B
necessary	O
for	O
the	O
o	O
(	O
cid:22	O
)	O
in	O
(	O
12.63	O
)	O
to	O
equal	O
o	O
(	O
cid:22	O
)	O
d	O
r.y/	O
.	O
the	O
calculation	O
(	O
12.64	O
)	O
was	O
rerun	O
taking	O
o	O
(	O
cid:22	O
)	O
in	O
(	O
12.63	O
)	O
from	O
lowess	O
(	O
x	O
,	O
y,1/6	O
)	O
(	O
but	O
with	O
r.y/	O
still	O
from	O
lowess	O
(	O
x	O
,	O
y,1/3	O
)	O
)	O
with	O
almost	O
identical	O
results	O
.	O
in	O
general	O
,	O
one	O
might	O
take	O
o	O
(	O
cid:22	O
)	O
in	O
(	O
12.63	O
)	O
to	O
be	O
from	O
a	O
more	O
ﬂexible	O
,	O
less	O
biased	O
,	O
estimator	B
than	O
r.y/	O
.	O
(	O
cid:3	O
)	O
d	O
r.y	O
(	O
cid:3	O
)	O
12.3	O
covariance	O
penalties	O
similarly	O
the	O
predictor	B
o	O
(	O
cid:22	O
)	O
i	O
,	O
with	O
dichotomous	O
error	O
if	O
yi	O
¤	O
o	O
(	O
cid:22	O
)	O
i	O
if	O
yi	O
d	O
o	O
(	O
cid:22	O
)	O
i	O
;	O
d.yi	O
;	O
o	O
(	O
cid:22	O
)	O
i	O
/	O
d	O
(	O
1	O
0	O
225	O
(	O
12.66	O
)	O
as	O
in	O
(	O
12.5	O
)	O
.10	O
in	O
this	O
situation	O
,	O
the	O
apparent	O
error	O
is	O
the	O
observed	O
proportion	B
of	O
prediction	O
mistakes	O
in	O
the	O
training	O
set	B
(	O
12.1	O
)	O
,	O
err	O
d	O
#	O
fyi	O
¤	O
o	O
(	O
cid:22	O
)	O
ig=n	O
:	O
(	O
12.67	O
)	O
now	O
the	O
true	O
prediction	O
error	O
for	O
case	O
i	O
is	O
erri	O
d	O
pr0fy0i	O
¤	O
o	O
(	O
cid:22	O
)	O
ig	O
;	O
(	O
12.68	O
)	O
the	O
conditional	O
probability	O
given	O
o	O
(	O
cid:22	O
)	O
i	O
that	O
an	O
independent	O
replicate	O
y0i	O
of	O
yi	O
will	O
be	O
incorrectly	O
predicted	O
.	O
the	O
lemma	O
holds	O
as	O
stated	O
in	O
(	O
12.39	O
)	O
,	O
leading	O
to	O
the	O
prediction	O
error	O
estimate	B
cerr	O
(	O
cid:1	O
)	O
d	O
#	O
fyi	O
¤	O
o	O
(	O
cid:22	O
)	O
ig	O
n	O
c	O
2	O
n	O
nx	O
id1	O
cov	O
.	O
o	O
(	O
cid:22	O
)	O
i	O
;	O
yi	O
/	O
:	O
(	O
12.69	O
)	O
some	O
algebra	O
yields	O
cov	O
.	O
o	O
(	O
cid:22	O
)	O
i	O
;	O
yi	O
/	O
d	O
(	O
cid:22	O
)	O
i	O
.1	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
i	O
/	O
.prf	O
o	O
(	O
cid:22	O
)	O
i	O
d	O
1jyi	O
d	O
1g	O
(	O
cid:0	O
)	O
prf	O
o	O
(	O
cid:22	O
)	O
1	O
d	O
1jyi	O
d	O
0g/	O
;	O
(	O
12.70	O
)	O
with	O
(	O
cid:22	O
)	O
i	O
d	O
prfyi	O
d	O
1g	O
,	O
showing	O
again	O
the	O
covariance	O
penalty	B
measuring	O
the	O
self-inﬂuence	O
of	O
yi	O
on	O
its	O
own	O
prediction	O
.	O
as	O
a	O
second	O
example	O
,	O
suppose	O
that	O
the	O
observations	O
yi	O
are	O
obtained	O
from	O
different	O
members	O
of	O
a	O
one-parameter	B
exponential	O
family	O
f	O
(	O
cid:22	O
)	O
.y/	O
d	O
expf	O
(	O
cid:21	O
)	O
y	O
(	O
cid:0	O
)	O
(	O
cid:13	O
)	O
.	O
(	O
cid:21	O
)	O
/gf0.y/	O
(	O
8.32	O
)	O
,	O
yi	O
(	O
cid:24	O
)	O
f	O
(	O
cid:22	O
)	O
i	O
.yi	O
/	O
z	O
	O
fyi	O
.yi	O
/	O
	O
fy.y	O
/	O
˚log	O
(	O
cid:0	O
)	O
fy	O
.y/	O
(	O
cid:1	O
)	O
(	O
cid:0	O
)	O
log	O
(	O
cid:0	O
)	O
f	O
o	O
(	O
cid:22	O
)	O
.y/	O
(	O
cid:1	O
)	O
(	O
cid:9	O
)	O
:	O
according	O
to	O
(	O
8.33	O
)	O
,	O
the	O
apparent	O
errorp	O
d.yi	O
;	O
o	O
(	O
cid:22	O
)	O
i	O
/	O
is	O
then	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
;	O
	O
d.y	O
;	O
o	O
(	O
cid:22	O
)	O
/	O
d	O
2	O
nx	O
and	O
that	O
error	O
is	O
measured	O
by	O
the	O
deviance	O
(	O
8.31	O
)	O
,	O
y	O
	O
d	O
2	O
err	O
d	O
2	O
fy.y	O
/	O
log	O
(	O
12.71	O
)	O
f	O
o	O
(	O
cid:22	O
)	O
.y	O
/	O
d	O
y	O
:	O
(	O
12.72	O
)	O
log	O
n	O
id1	O
f	O
o	O
(	O
cid:22	O
)	O
i	O
.yi	O
/	O
n	O
(	O
12.73	O
)	O
10	O
more	O
generally	O
,	O
o	O
(	O
cid:25	O
)	O
i	O
is	O
some	O
predictor	B
of	O
prfyi	O
d	O
1g	O
,	O
and	O
o	O
(	O
cid:22	O
)	O
i	O
is	O
the	O
indicator	O
function	B
i.	O
o	O
(	O
cid:25	O
)	O
i	O
(	O
cid:21	O
)	O
0:5/	O
.	O
226	O
cross-validation	O
and	O
cp	O
estimates	O
in	O
this	O
case	O
the	O
general	O
theory	B
gives	O
overall	O
covariance	O
penalty	B
(	O
cid:16	O
)	O
o	O
	O
nx	O
id1	O
penalty	B
d	O
2	O
n	O
(	O
cid:21	O
)	O
i	O
;	O
o	O
(	O
cid:22	O
)	O
i	O
cov	O
;	O
(	O
12.74	O
)	O
where	O
o	O
(	O
cid:21	O
)	O
i	O
is	O
the	O
natural	O
parameter	O
in	O
family	O
(	O
8.32	O
)	O
corresponding	O
to	O
o	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
e.g.	O
,	O
o	O
(	O
cid:21	O
)	O
i	O
d	O
log	O
o	O
(	O
cid:22	O
)	O
i	O
for	O
poisson	O
observations	O
)	O
.	O
moreover	O
,	O
if	O
o	O
(	O
cid:22	O
)	O
is	O
obtained	O
as	O
the	O
mle	O
of	O
(	O
cid:22	O
)	O
in	O
a	O
generalized	O
linear	B
model	I
with	O
p	O
degrees	O
of	O
freedom	O
(	O
8.22	O
)	O
,	O
penalty	B
:	O
d	O
2p	O
n	O
(	O
12.75	O
)	O
˚log	O
(	O
cid:0	O
)	O
f	O
o	O
(	O
cid:22	O
)	O
.y/	O
(	O
cid:1	O
)	O
(	O
cid:0	O
)	O
p	O
(	O
cid:9	O
)	O
c	O
constant	O
;	O
to	O
a	O
good	O
approximation	O
.	O
the	O
corresponding	O
version	O
of	O
cerr	O
(	O
cid:1	O
)	O
(	O
12.46	O
)	O
can	O
then	O
be	O
written	O
ascerr	O
(	O
cid:1	O
)	O
the	O
constant	O
.2=n	O
/	O
log.fy	O
.y//	O
not	O
depending	O
on	O
o	O
(	O
cid:22	O
)	O
.	O
the	O
term	O
in	O
brackets	O
is	O
the	O
akaike	O
information	B
criterion	I
(	O
aic	O
)	O
:	O
if	O
the	O
statistician	O
is	O
comparing	O
possible	O
prediction	O
rules	O
r	O
.j	O
/.y/	O
for	O
a	O
given	O
data	B
set	O
y	O
,	O
the	O
aic	O
says	O
to	O
select	O
the	O
rule	B
maximizing	O
the	O
penalized	O
maximum	B
likelihood	I
:	O
d	O
(	O
cid:0	O
)	O
2	O
n	O
(	O
12.76	O
)	O
log	O
(	O
cid:0	O
)	O
f	O
o	O
(	O
cid:22	O
)	O
.j	O
/	O
.y/	O
(	O
cid:1	O
)	O
(	O
cid:0	O
)	O
p.j	O
/	O
;	O
.	O
(	O
12.77	O
)	O
where	O
o	O
(	O
cid:22	O
)	O
.j	O
/	O
is	O
rule	B
j	O
’	O
s	O
mle	O
and	O
p.j	O
/	O
its	O
degrees	O
of	O
freedom	O
.	O
comparison	O
with	O
the	O
smallest	O
value	O
ofcerr.j	O
/	O
(	O
cid:1	O
)	O
with	O
(	O
12.76	O
)	O
shows	O
that	O
for	O
glms	O
,	O
the	O
aic	O
amounts	O
to	O
selecting	O
the	O
rule	B
is	O
available	O
then	O
the	O
error	O
estimate	B
cerrcv	O
can	O
be	O
improved	O
by	O
bootstrap	O
smoothing.11	O
with	O
the	O
predictor	B
vectors	O
xi	O
considered	O
ﬁxed	O
as	O
observed	O
,	O
a	O
parametric	B
model	O
generates	O
the	O
data	B
set	O
d	O
d	O
f.xi	O
;	O
yi	O
/	O
;	O
i	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
ng	O
as	O
estimatecerrcv	O
(	O
12.21	O
)	O
,	O
in	O
(	O
12.62	O
)	O
,	O
from	O
which	O
we	O
calculate	O
the	O
prediction	O
rule	B
rd	O
.	O
(	O
cid:1	O
)	O
/	O
and	O
the	O
error	O
cross-validation	O
does	O
not	O
require	O
a	O
probability	O
model	B
,	O
but	O
if	O
such	O
a	O
model	B
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
——	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
——	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
——	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
f	O
(	O
cid:22	O
)	O
!	O
d	O
!	O
rd	O
.	O
(	O
cid:1	O
)	O
/	O
!	O
cerrcv	O
:	O
(	O
12.78	O
)	O
substituting	O
the	O
estimated	O
density	B
f	O
o	O
(	O
cid:22	O
)	O
for	O
f	O
(	O
cid:22	O
)	O
,	O
as	O
in	O
(	O
12.63	O
)	O
,	O
provides	O
11	O
perhaps	O
better	O
known	O
as	O
“	O
bagging	O
;	O
”	O
see	O
chapter	O
17	O
.	O
12.4	O
training	O
,	O
validation	O
,	O
and	O
ephemeral	O
predictors	O
227	O
(	O
12.79	O
)	O
parametric	B
bootstrap	O
replicates	O
ofcerrcv	O
,	O
(	O
cid:3	O
)	O
.	O
(	O
cid:1	O
)	O
/	O
!	O
cerr	O
(	O
cid:3	O
)	O
!	O
rd	O
bx	O
cerr	O
f	O
o	O
(	O
cid:22	O
)	O
!	O
d	O
err	O
d	O
1	O
(	O
cid:3	O
)	O
b	O
cv	O
:	O
(	O
cid:3	O
)	O
cv	O
:	O
some	O
large	O
number	O
b	O
of	O
replications	O
can	O
then	O
be	O
averaged	O
to	O
give	O
the	O
smoothed	O
estimate	B
b	O
bd1	O
err	O
averages	O
out	O
the	O
considerable	O
noise	O
incerrcv	O
,	O
often	O
signiﬁcantly	O
reduc-	O
left	O
after	O
excess	O
randomness	O
is	O
squeezed	O
out	O
ofcerrcv	O
(	O
an	O
example	O
of	O
“	O
rao–	O
a	O
surprising	O
result	O
,	O
referenced	O
in	O
the	O
endnotes	O
,	O
shows	O
that	O
err	O
approxi-	O
mates	O
the	O
covariance	O
penalty	B
estimate	O
err	O
(	O
cid:1	O
)	O
.	O
speaking	O
broadly	O
,	O
err	O
(	O
cid:1	O
)	O
is	O
what	O
’	O
s	O
ing	O
its	O
variability.12	O
(	O
12.80	O
)	O
blackwellization	O
,	O
”	O
to	O
use	O
classical	O
terminology	O
)	O
.	O
improvements	O
can	O
be	O
quite	O
substantial	O
.	O
	O
covariance	O
penalty	B
estimates	O
,	O
when	O
believable	O
parametric	B
4	O
models	B
are	O
available	O
,	O
should	O
be	O
preferred	O
to	O
cross-validation	O
.	O
12.4	O
training	O
,	O
validation	O
,	O
and	O
ephemeral	O
predictors	O
good	O
practice	O
suggests	O
splitting	O
the	O
full	B
set	O
of	O
observed	O
predictor–response	O
pairs	O
.x	O
;	O
y/	O
into	O
a	O
training	O
set	B
d	O
of	O
size	O
n	O
(	O
12.1	O
)	O
,	O
and	O
a	O
validation	O
set	B
dval	O
,	O
of	O
size	O
nval	O
(	O
12.19	O
)	O
.	O
the	O
validation	O
set	B
is	O
put	O
into	O
a	O
vault	O
while	O
the	O
training	O
set	B
is	O
used	O
to	O
develop	O
an	O
effective	O
prediction	O
rule	B
rd	O
.x/	O
.	O
finally	O
,	O
dval	O
is	O
removed	O
from	O
the	O
vault	O
and	O
used	O
to	O
calculatecerrval	O
(	O
12.20	O
)	O
,	O
an	O
honest	O
estimate	O
of	O
the	O
predictive	O
error	O
rate	B
of	O
rd	O
.	O
this	O
is	O
a	O
good	O
idea	O
,	O
and	O
seems	O
foolproof	O
,	O
at	O
least	O
if	O
one	O
has	O
enough	O
data	B
to	O
afford	O
setting	O
aside	O
a	O
substantial	O
portion	O
for	O
a	O
validation	O
set	B
during	O
the	O
training	O
process	O
.	O
nevertheless	O
,	O
there	O
remains	O
some	O
peril	O
of	O
underestimating	O
the	O
true	O
error	O
rate	B
,	O
arising	O
from	O
ephemeral	O
predictors	O
,	O
those	O
whose	O
predic-	O
tive	O
powers	O
fade	O
away	O
over	O
time	O
.	O
a	O
contrived	O
,	O
but	O
not	O
completely	O
fanciful	O
,	O
example	O
illustrates	O
the	O
danger	O
.	O
the	O
example	O
takes	O
the	O
form	B
of	O
an	O
imaginary	O
microarray	O
study	O
involving	O
360	O
subjects	O
,	O
180	O
patients	O
and	O
180	O
healthy	O
controls	O
,	O
coded	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
360	O
:	O
yi	O
d	O
1	O
patient	O
0	O
control	B
;	O
(	O
12.81	O
)	O
12	O
a	O
related	O
tactic	O
pertaining	O
to	O
grouped	O
cross-validation	O
is	O
to	O
repeat	O
calculation	O
(	O
12.21	O
)	O
for	O
several	O
different	O
randomly	O
selected	O
splits	O
into	O
j	O
groups	O
,	O
and	O
then	O
average	O
the	O
resulting	O
cerrcv	O
estimates	O
.	O
(	O
228	O
cross-validation	O
and	O
cp	O
estimates	O
each	O
subject	O
is	O
assessed	O
on	O
a	O
microarray	O
measuring	O
the	O
genetic	O
activity	O
of	O
p	O
d	O
100	O
genes	O
,	O
these	O
being	O
the	O
predictors	O
xi	O
d	O
.xi1	O
;	O
xi	O
2	O
;	O
xi	O
3	O
;	O
:	O
:	O
:	O
;	O
xi100/	O
0	O
:	O
(	O
12.82	O
)	O
one	O
subject	O
per	O
day	O
is	O
assessed	O
,	O
alternating	O
patients	O
and	O
controls	O
.	O
figure	O
12.5	O
orange	O
bars	O
indicate	O
transient	O
episodes	O
,	O
(	O
12.84	O
)	O
and	O
the	O
reverse	O
,	O
for	O
imaginary	O
medical	O
study	O
(	O
12.81	O
)	O
–	O
(	O
12.82	O
)	O
.	O
xij	O
ind	O
(	O
cid:24	O
)	O
the	O
measurements	O
xij	O
are	O
independent	O
of	O
each	O
other	O
and	O
of	O
the	O
yi	O
’	O
s	O
,	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
360	O
and	O
j	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
100	O
:	O
(	O
12.83	O
)	O
most	O
of	O
the	O
(	O
cid:22	O
)	O
ij	O
equal	O
zero	O
,	O
but	O
each	O
gene	O
’	O
s	O
measurements	O
can	O
experience	O
“	O
transient	O
episodes	O
”	O
of	O
two	O
possible	O
types	O
:	O
in	O
type	O
1	O
,	O
n	O
.	O
(	O
cid:22	O
)	O
ij	O
;	O
1/	O
(	O
if	O
yi	O
d	O
1	O
(	O
cid:0	O
)	O
2	O
if	O
yi	O
d	O
0	O
;	O
2	O
(	O
cid:22	O
)	O
ij	O
d	O
(	O
12.84	O
)	O
while	O
type	O
2	O
reverses	O
signs	O
.	O
the	O
episodes	O
are	O
about	O
30	O
days	O
long	O
,	O
randomly	O
and	O
independently	O
located	O
between	O
days	O
1	O
and	O
360	O
,	O
with	O
an	O
average	O
of	O
two	O
episodes	O
per	O
gene	O
.	O
the	O
orange	O
bars	O
in	O
figure	O
12.5	O
indicate	O
the	O
episodes	O
.	O
for	O
the	O
purpose	O
of	O
future	O
diagnoses	O
we	O
wish	O
to	O
construct	O
a	O
prediction	O
rule	B
oy	O
d	O
rd	O
.x/	O
.	O
to	O
this	O
end	O
we	O
randomly	O
divide	O
the	O
360	O
subjects	O
into	O
a	O
050100150200250300350020406080100	O
***********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************subjectsgenes	O
12.4	O
ephemeral	O
predictors	O
229	O
training	O
set	B
d	O
of	O
size	O
n	O
d	O
300	O
and	O
a	O
validation	O
set	B
dval	O
of	O
size	O
nval	O
d	O
60.	O
the	O
popular	O
“	O
machine	O
learning	O
”	O
prediction	O
program	O
random	O
forests	O
,	O
chapter	O
17	O
,	O
is	O
applied	O
.	O
random	O
forests	O
forms	O
rd	O
.x/	O
by	O
averaging	B
the	O
pre-	O
dictions	O
of	O
a	O
large	O
number	O
of	O
randomly	O
subsampled	O
regression	B
trees	O
(	O
sec-	O
tion	O
8.4	O
)	O
.	O
figure	O
12.6	O
test	O
error	O
(	O
blue	O
)	O
and	O
cross-validated	O
training	O
error	O
(	O
black	O
)	O
,	O
for	O
random	O
forest	O
prediction	O
rules	O
using	O
the	O
imaginary	O
medical	O
study	O
(	O
12.81	O
)	O
–	O
(	O
12.82	O
)	O
.	O
top	O
panel	O
:	O
training	O
set	B
randomly	O
selected	O
300	O
days	O
,	O
test	O
set	B
the	O
remaining	O
60	O
days	O
.	O
bottom	O
panel	O
:	O
training	O
set	B
the	O
ﬁrst	O
300	O
days	O
,	O
test	O
set	B
the	O
last	O
60	O
days	O
.	O
the	O
top	O
panel	O
of	O
figure	O
12.6	O
shows	O
the	O
results	O
,	O
with	O
blue	O
points	O
indi-	O
cating	O
test-set	O
error	O
and	O
black	O
the	O
(	O
cross-validated	O
)	O
training-set	O
error	O
.	O
both	O
converge	O
to	O
15	O
%	O
as	O
the	O
number	O
of	O
random	O
forest	O
trees	B
grows	O
large	O
.	O
this	O
seems	O
to	O
conﬁrm	O
an	O
85	O
%	O
success	O
rate	B
for	O
prediction	O
rule	B
rd	O
.x/	O
.	O
one	O
change	O
has	O
been	O
made	O
for	O
the	O
bottom	O
panel	O
:	O
now	O
the	O
training	O
set	B
is	O
the	O
data	B
for	O
days	O
1	O
through	O
300	O
,	O
and	O
the	O
test	O
set	B
days	O
301	O
through	O
360.	O
llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll01002003004000.00.10.20.30.40.5training	O
set	B
random	O
300	O
days	O
,	O
test	O
set	B
the	O
remainder	O
#	O
treesprediction	O
errorllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll15	O
%	O
llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll01002003004000.00.10.20.30.40.5training	O
days	O
1−300	O
,	O
test	O
days	O
301−360	O
#	O
treesprediction	O
errorllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll27	O
%	O
15	O
%	O
cross-validation	O
and	O
cp	O
estimates	O
230	O
cerrval	O
is	O
now	O
27	O
%	O
,	O
nearly	O
double	O
.	O
the	O
cross-validated	O
training-set	O
prediction	O
error	O
still	O
converges	O
to	O
15	O
%	O
,	O
but	O
the	O
reason	O
isn	O
’	O
t	B
hard	O
to	O
see	O
.	O
any	O
predictive	O
power	O
must	O
come	O
from	O
the	O
transient	O
episodes	O
,	O
which	O
lose	O
efﬁcacy	O
outside	O
of	O
their	O
limited	O
span	O
.	O
in	O
the	O
ﬁrst	O
example	O
the	O
test	O
days	O
are	O
located	O
among	O
the	O
training	O
days	O
,	O
and	O
inherit	O
their	O
predictive	O
accuracy	O
from	O
them	O
.	O
this	O
mostly	O
fails	O
in	O
the	O
second	O
setup	O
,	O
where	O
the	O
test	O
days	O
are	O
farther	O
removed	O
from	O
the	O
training	O
days	O
.	O
(	O
only	O
the	O
orange	O
bars	O
crossing	O
the	O
300-day	O
line	O
can	O
help	O
lower	O
cerrval	O
in	O
this	O
situa-	O
an	O
obvious	O
,	O
but	O
often	O
ignored	O
,	O
dictum	O
is	O
thatcerrval	O
is	O
more	O
believable	O
if	O
tion	O
.	O
)	O
the	O
test	O
set	B
is	O
further	O
separated	O
from	O
the	O
training	O
set	B
.	O
“	O
further	O
”	O
has	O
a	O
clear	O
meaning	O
in	O
studies	O
with	O
a	O
time	O
or	O
location	O
factor	B
,	O
but	O
not	O
necessarily	O
in	O
general	O
.	O
for	O
j	O
-fold	O
cross-validation	O
,	O
separation	O
is	O
improved	O
by	O
removing	O
contiguous	O
blocks	O
of	O
n=j	O
cases	O
for	O
each	O
group	O
,	O
rather	O
than	O
by	O
random	O
selection	O
,	O
but	O
the	O
amount	O
of	O
separation	O
is	O
still	O
limited	O
,	O
making	O
cerrcv	O
less	O
believable	O
than	O
a	O
suitably	O
constructedcerrval	O
.	O
the	O
distinction	O
between	O
transient	O
,	O
ephemeral	O
predictors	O
and	O
dependable	O
ones	O
is	O
sometimes	O
phrased	O
as	O
the	O
difference	O
between	O
correlation	O
and	O
cau-	O
sation	O
.	O
for	O
prediction	O
purposes	O
,	O
if	O
not	O
for	O
scientiﬁc	O
exegesis	O
,	O
we	O
may	O
be	O
happy	O
to	O
settle	O
for	O
correlations	O
as	O
long	O
as	O
they	O
are	O
persistent	O
enough	O
for	O
our	O
purposes	O
.	O
we	O
return	O
to	O
this	O
question	O
in	O
chapter	O
15	O
in	O
the	O
discussion	O
of	O
large-scale	O
hypothesis	O
testing	B
.	O
5	O
a	O
notorious	O
cautionary	O
tale	O
of	O
fading	O
correlations	O
concerns	O
google	O
flu	O
trends	O
,	O
	O
a	O
machine-learning	O
algorithm	B
for	O
predicting	O
inﬂuenza	O
outbreaks	O
.	O
introduced	O
in	O
2008	O
,	O
the	O
algorithm	B
,	O
based	O
on	O
counts	O
of	O
internet	O
search	O
terms	O
,	O
outperformed	O
traditional	O
medical	O
surveys	O
in	O
terms	O
of	O
speed	O
and	O
predictive	O
accuracy	O
.	O
four	O
years	O
later	O
,	O
however	O
,	O
the	O
algorithm	B
failed	O
,	O
badly	O
overesti-	O
mating	O
what	O
turned	O
out	O
to	O
be	O
a	O
nonexistent	O
ﬂu	O
epidemic	O
.	O
perhaps	O
one	O
les-	O
son	O
here	O
is	O
that	O
the	O
google	O
algorithmists	O
needed	O
a	O
validation	O
set	B
years—not	O
weeks	O
or	O
months—removed	O
from	O
the	O
training	O
data	B
.	O
error	O
rate	B
estimation	O
is	O
mainly	O
frequentist	O
in	O
nature	O
,	O
but	O
the	O
very	O
large	O
data	B
sets	O
available	O
from	O
the	O
internet	O
have	O
encouraged	O
a	O
disregard	O
for	O
infer-	O
ential	O
justiﬁcation	O
of	O
any	O
type	O
.	O
this	O
can	O
be	O
dangerous	O
.	O
the	O
heterogeneous	O
nature	O
of	O
“	O
found	O
”	O
data	B
makes	O
statistical	O
principles	O
of	O
analysis	B
more	O
,	O
not	O
less	O
,	O
relevant	O
.	O
12.5	O
notes	O
and	O
details	O
the	O
evolution	O
of	O
prediction	O
algorithms	O
and	O
their	O
error	O
estimates	O
nicely	O
il-	O
lustrates	O
the	O
inﬂuence	O
of	O
electronic	O
computation	O
on	O
statistical	O
theory	B
and	O
12.5	O
notes	O
and	O
details	O
231	O
practice	O
.	O
the	O
classical	O
recipe	O
for	O
cross-validation	O
recommended	O
splitting	O
the	O
full	B
data	O
set	B
in	O
two	O
,	O
doing	O
variable	O
selection	O
,	O
model	B
choice	O
,	O
and	O
data	O
ﬁt-	O
ting	O
on	O
the	O
ﬁrst	O
half	O
,	O
and	O
then	O
testing	B
the	O
resulting	O
procedure	O
on	O
the	O
second	O
half	O
.	O
interest	O
revived	O
in	O
1974	O
with	O
the	O
independent	O
publication	O
of	O
papers	O
by	O
geisser	O
and	O
by	O
stone	O
,	O
featuring	O
leave-one-out	O
cross-validation	O
of	O
pre-	O
dictive	O
error	O
rates	O
.	O
a	O
question	O
of	O
bias	O
versus	O
variance	O
arises	O
here	O
.	O
a	O
rule	B
based	O
on	O
only	O
n=2	O
cases	O
is	O
less	O
accurate	O
than	O
the	O
actual	O
rule	B
based	O
on	O
all	O
n	O
.	O
leave-one-out	O
cross-validation	O
minimizes	O
this	O
type	O
of	O
bias	O
,	O
at	O
the	O
expense	O
of	O
increased	O
variability	O
of	O
error	O
rate	B
estimates	O
for	O
“	O
jumpy	O
”	O
rules	O
of	O
a	O
discontinuous	O
nature	O
.	O
current	O
best	O
practice	O
is	O
described	O
in	O
section	O
7.10	O
of	O
hastie	O
et	O
al	O
.	O
(	O
2009	O
)	O
,	O
where	O
j	O
-fold	O
cross-validation	O
with	O
j	O
perhaps	O
10	O
is	O
recommended	O
,	O
possibly	O
averaged	O
over	O
several	O
random	O
data	B
splits	O
.	O
nineteen	O
seventy-three	O
was	O
another	O
good	O
year	O
for	O
error	O
estimation	B
,	O
fea-	O
turing	O
mallows	O
’	O
cp	O
estimator	B
and	O
akaike	O
’	O
s	O
information	B
criterion	I
.	O
efron	O
(	O
1986	O
)	O
extended	O
cp	O
methods	O
to	O
a	O
general	O
class	O
of	O
situations	O
(	O
see	O
below	O
)	O
,	O
established	O
the	O
connection	O
with	O
aic	O
,	O
and	O
suggested	O
bootstrapping	O
methods	O
for	O
covariance	O
penalties	O
.	O
the	O
connection	O
between	O
cross-validation	O
and	O
co-	O
variance	O
penalties	O
was	O
examined	O
in	O
efron	O
(	O
2004	O
)	O
,	O
where	O
the	O
rao–blackwell-	O
type	O
relationship	O
mentioned	O
at	O
the	O
end	O
of	O
section	O
12.3	O
was	O
demonstrated	O
.	O
the	O
sure	O
criterion	O
appeared	O
in	O
charles	O
stein	O
’	O
s	O
1981	O
paper	O
.	O
ye	O
(	O
1998	O
)	O
suggested	O
the	O
general	O
degrees	O
of	O
freedom	O
deﬁnition	O
(	O
12.53	O
)	O
.	O
1	O
[	O
p.	O
210	O
]	O
standard	O
candles	O
and	O
dark	O
energy	O
.	O
adam	O
riess	O
,	O
saul	O
perlmutter	O
,	O
and	O
brian	O
schmidt	O
won	O
the	O
2011	O
nobel	O
prize	O
in	O
physics	O
for	O
discovering	O
increasing	O
rates	O
of	O
expansion	O
of	O
the	O
universe	O
,	O
attributed	O
to	O
an	O
einsteinian	O
concept	O
of	O
dark	O
energy	O
.	O
they	O
measured	O
cosmic	O
distances	O
using	O
type	O
ia	O
supernovas	O
as	O
“	O
standard	O
candles.	O
”	O
the	O
type	O
of	O
analysis	B
suggested	O
by	O
fig-	O
ure	O
12.1	O
is	O
intended	O
to	O
improve	O
the	O
cosmological	O
distance	O
scale	B
.	O
2	O
[	O
p.	O
222	O
]	O
data-based	O
choice	O
of	O
a	O
lasso	B
estimate	O
.	O
the	O
regularization	B
param-	O
eter	O
(	O
cid:21	O
)	O
for	O
a	O
lasso	B
estimator	O
(	O
7.42	O
)	O
controls	O
the	O
number	O
of	O
nonzero	O
coefﬁ-	O
cients	O
of	O
q	O
ˇ	O
.	O
(	O
cid:21	O
)	O
/	O
,	O
with	O
larger	O
(	O
cid:21	O
)	O
yielding	O
fewer	O
nonzeros	O
.	O
efron	O
et	O
al	O
.	O
(	O
2004	O
)	O
cients	O
.	O
substituting	O
this	O
for	O
p	O
in	O
(	O
12.51	O
)	O
provides	O
a	O
quick	O
version	O
of	O
cerr	O
(	O
cid:1	O
)	O
.	O
and	O
zou	O
et	O
al	O
.	O
(	O
2007	O
)	O
showed	O
that	O
a	O
good	O
approximation	O
for	O
the	O
degrees	O
of	O
freedom	O
df	O
(	O
12.53	O
)	O
of	O
a	O
lasso	B
estimate	O
is	O
the	O
number	O
of	O
its	O
nonzero	O
coefﬁ-	O
this	O
was	O
minimized	O
at	O
df	O
d	O
7	O
for	O
the	O
supernova	B
example	O
in	O
figure	O
12.1	O
(	O
12.54	O
)	O
.	O
3	O
[	O
p.	O
222	O
]	O
stein	O
’	O
s	O
unbiased	B
risk	I
estimate	I
.	O
the	O
covariance	O
formula	B
(	O
12.56	O
)	O
is	O
obtained	O
directly	O
from	O
integration	O
by	O
parts	O
.	O
the	O
computation	O
is	O
clear	O
from	O
232	O
the	O
one-dimensional	O
version	O
of	O
(	O
12.55	O
)	O
,	O
n	O
d	O
1	O
:	O
cross-validation	O
and	O
cp	O
estimates	O
	O
cov	O
.	O
o	O
(	O
cid:22	O
)	O
;	O
y/	O
dz	O
1	O
	O
z	O
1	O
(	O
cid:0	O
)	O
1	O
(	O
cid:26	O
)	O
@	O
o	O
(	O
cid:22	O
)	O
.y/	O
d	O
(	O
cid:27	O
)	O
2	O
d	O
(	O
cid:27	O
)	O
2e	O
1p	O
2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
1p	O
2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
(	O
cid:27	O
)	O
(	O
cid:0	O
)	O
1	O
e	O
:	O
@	O
y	O
(	O
cid:21	O
)	O
o	O
(	O
cid:22	O
)	O
.y/	O
dy	O
(	O
cid:21	O
)	O
@	O
o	O
(	O
cid:22	O
)	O
.y/	O
.y	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
/	O
.y	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
/2	O
dy	O
@	O
y	O
(	O
cid:0	O
)	O
1	O
2	O
.y	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
/2	O
(	O
cid:27	O
)	O
2	O
(	O
cid:0	O
)	O
1	O
2	O
e	O
(	O
cid:27	O
)	O
2	O
(	O
12.85	O
)	O
(	O
cid:3	O
)	O
b	O
d	O
r	O
(	O
cid:3	O
)	O
b.xi	O
/	O
:	O
broad	O
regularity	O
conditions	B
for	O
sure	O
are	O
given	O
in	O
stein	O
(	O
1981	O
)	O
.	O
4	O
[	O
p.	O
227	O
]	O
the	O
.632	B
rule	I
.	O
bootstrap	O
competitors	O
to	O
cross-validation	O
are	O
dis-	O
cussed	O
in	O
efron	O
(	O
1983	O
)	O
and	O
efron	O
and	O
tibshirani	O
(	O
1997	O
)	O
.	O
the	O
most	O
success-	O
ful	O
of	O
these	O
,	O
the	O
“	O
.632	B
rule	I
”	O
is	O
generally	O
less	O
variable	O
than	O
leave-one-out	O
(	O
cid:3	O
)	O
b	O
,	O
cross-validation	O
.	O
we	O
suppose	O
that	O
nonparametric	B
bootstrap	O
data	B
sets	O
d	O
b	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
b	O
,	O
have	O
been	O
formed	O
,	O
each	O
by	O
sampling	O
with	O
replacement	O
(	O
cid:3	O
)	O
b	O
produces	O
n	O
times	O
from	O
the	O
original	O
n	O
members	O
of	O
d	O
(	O
12.1	O
)	O
.	O
data	B
set	O
d	O
rule	B
(	O
cid:3	O
)	O
b.x/	O
d	O
rd	O
(	O
cid:3	O
)	O
b	O
.x/	O
;	O
(	O
12.86	O
)	O
giving	O
predictions	O
r	O
y	O
let	O
i	O
b	O
i	O
d	O
1	O
if	O
pair	O
.xi	O
;	O
yi	O
/	O
is	O
not	O
in	O
d	O
(	O
12.87	O
)	O
(	O
cid:0	O
)	O
1	O
d	O
i	O
will	O
equal	O
1	O
,	O
the	O
remaining	O
0.632	O
equaling	O
0	O
.	O
)	O
the	O
(	O
cid:3	O
)	O
b	O
,	O
and	O
0	O
if	O
it	O
is	O
.	O
(	O
about	O
e	O
0:368	O
of	O
the	O
n	O
(	O
cid:1	O
)	O
b	O
i	O
b	O
“	O
out	B
of	I
bootstrap	I
”	O
estimate	O
of	O
prediction	O
error	O
is	O
i	O
cerrout	O
d	O
nx	O
bx	O
id1	O
jd1	O
i	O
b	O
i	O
d	O
	O
,	O
nx	O
bx	O
(	O
cid:16	O
)	O
yi	O
;	O
oy	O
(	O
cid:3	O
)	O
b	O
i	O
i	O
b	O
i	O
;	O
id1	O
jd1	O
(	O
12.88	O
)	O
the	O
average	O
discrepancy	O
in	O
the	O
omitted	O
cases	O
.	O
37	O
%	O
of	O
the	O
cases	O
each	O
time	O
.	O
the	O
.632	B
rule	I
compensates	O
for	O
the	O
upward	O
bias	O
cerrout	O
is	O
similar	O
to	O
a	O
grouped	O
cross-validation	O
estimate	B
that	O
omits	O
about	O
incerrout	O
by	O
incorporating	O
the	O
downwardly	O
biased	O
apparent	O
error	O
(	O
12.18	O
)	O
,	O
cerrout	O
has	O
resurfaced	O
in	O
the	O
popular	O
random	O
forests	O
prediction	O
algorithm	B
,	O
cerr:632	O
d	O
0:632cerrout	O
c	O
0:368	O
err	O
:	O
chapter	O
17	O
,	O
where	O
a	O
closely	O
related	O
procedure	O
gives	O
the	O
“	O
out	O
of	O
bag	O
”	O
esti-	O
mate	O
of	O
err	O
.	O
(	O
12.89	O
)	O
5	O
[	O
p.	O
230	O
]	O
google	O
flu	O
trends	O
.	O
harford	O
’	O
s	O
2014	O
article	O
,	O
“	O
big	O
data	B
:	O
a	O
big	O
mis-	O
take	O
?	O
,	O
”	O
concerns	O
the	O
enormous	O
“	O
found	O
”	O
data	B
sets	O
available	O
in	O
the	O
internet	O
age	O
,	O
and	O
the	O
dangers	O
of	O
forgetting	O
the	O
principles	O
of	O
statistical	O
inference	B
in	O
their	O
analysis	B
.	O
google	O
flu	O
trends	O
is	O
his	O
primary	O
cautionary	O
example	O
.	O
13	O
objective	O
bayes	O
inference	B
and	O
markov	O
chain	O
monte	O
carlo	O
from	O
its	O
very	O
beginnings	O
,	O
bayesian	O
inference	B
exerted	O
a	O
powerful	O
inﬂuence	O
on	O
statistical	O
thinking	O
.	O
the	O
notion	O
of	O
a	O
single	O
coherent	O
methodology	O
em-	O
ploying	O
only	O
the	O
rules	O
of	O
probability	O
to	O
go	O
from	O
assumption	O
to	O
conclusion	O
was	O
and	O
is	O
immensely	O
attractive	O
.	O
for	O
200	O
years	O
,	O
however	O
,	O
two	O
impediments	O
stood	O
between	O
bayesian	O
theory	B
’	O
s	O
philosophical	O
attraction	O
and	O
its	O
practical	O
application	O
.	O
1	O
in	O
the	O
absence	O
of	O
relevant	O
past	O
experience	O
,	O
the	O
choice	O
of	O
a	O
prior	B
distribu-	O
tion	O
introduces	O
an	O
unwanted	O
subjective	O
element	O
into	O
scientiﬁc	O
inference	B
.	O
2	O
bayes	O
’	O
rule	B
(	O
3.5	O
)	O
looks	O
simple	O
enough	O
,	O
but	O
carrying	O
out	O
the	O
numerical	O
calculation	O
of	O
a	O
posterior	B
distribution	I
often	O
involves	O
intricate	O
higher-	O
dimensional	O
integrals	O
.	O
the	O
two	O
impediments	O
ﬁt	O
neatly	O
into	O
the	O
dichotomy	O
of	O
chapter	O
1	O
,	O
the	O
ﬁrst	O
being	O
inferential	O
and	O
the	O
second	O
algorithmic.1	O
a	O
renewed	O
cycle	O
of	O
bayesian	O
enthusiasm	O
took	O
hold	O
in	O
the	O
1960s	O
,	O
at	O
ﬁrst	O
concerned	O
mainly	O
with	O
coherent	O
inference	B
.	O
building	O
on	O
work	O
by	O
bruno	O
de	O
finetti	O
and	O
l.	O
j.	O
savage	O
,	O
a	O
principled	O
theory	B
of	O
subjective	O
probability	O
was	O
constructed	O
:	O
the	O
bayesian	O
statistician	O
,	O
by	O
the	O
careful	O
elicitation	O
of	O
prior	B
knowledge	O
,	O
utility	O
,	O
and	O
belief	O
,	O
arrives	O
at	O
the	O
correct	O
subjective	O
prior	B
dis-	O
tribution	O
for	O
the	O
problem	O
at	O
hand	O
.	O
subjective	O
bayesianism	O
is	O
particularly	O
appropriate	O
for	O
individual	O
decision	O
making	O
,	O
say	O
for	O
the	O
business	O
executive	O
trying	O
to	O
choose	O
the	O
best	O
investment	O
in	O
the	O
face	O
of	O
uncertain	O
information	B
.	O
it	O
is	O
less	O
appropriate	O
for	O
scientiﬁc	O
inference	B
,	O
where	O
the	O
sometimes	O
skep-	O
tical	O
world	O
of	O
science	O
puts	O
a	O
premium	O
on	O
objectivity	O
.	O
an	O
answer	O
came	O
from	O
the	O
school	O
of	O
objective	O
bayes	O
inference	B
.	O
following	O
the	O
approach	O
of	O
laplace	O
and	O
jeffreys	O
,	O
as	O
discussed	O
in	O
section	O
3.2	O
,	O
their	O
goal	O
was	O
to	O
fashion	O
objec-	O
tive	O
,	O
or	O
“	O
uninformative	O
,	O
”	O
prior	B
distributions	O
that	O
in	O
some	O
sense	O
were	O
unbi-	O
ased	O
in	O
their	O
effects	O
upon	O
the	O
data	B
analysis	O
.	O
1	O
the	O
exponential	O
family	O
material	O
in	O
this	O
chapter	O
provides	O
technical	O
support	O
,	O
but	O
is	O
not	O
required	O
in	O
detail	O
for	O
a	O
general	O
understanding	O
of	O
the	O
main	O
ideas	O
.	O
233	O
234	O
objective	O
bayes	O
inference	B
and	O
mcmc	O
in	O
what	O
came	O
as	O
a	O
surprise	O
to	O
the	O
bayes	O
community	O
,	O
the	O
objective	O
school	O
has	O
been	O
the	O
most	O
successful	O
in	O
bringing	O
bayesian	O
ideas	O
to	O
bear	O
on	O
scien-	O
tiﬁc	O
data	B
analysis	O
.	O
of	O
the	O
24	O
articles	O
in	O
the	O
december	O
2014	O
issue	O
of	O
the	O
annals	O
of	O
applied	O
statistics	B
,	O
8	O
employed	O
bayesian	O
analysis	B
,	O
predominantly	O
based	O
on	O
objective	O
priors	O
.	O
this	O
is	O
where	O
electronic	O
computation	O
enters	O
the	O
story	O
.	O
commencing	O
in	O
the	O
1980s	O
,	O
dramatic	O
steps	O
forward	O
were	O
made	O
in	O
the	O
numerical	O
calculation	O
of	O
high-dimensional	O
bayes	O
posterior	O
distributions	O
.	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
is	O
the	O
generic	O
name	O
for	O
modern	O
posterior	O
computation	O
al-	O
gorithms	O
.	O
these	O
proved	O
particularly	O
well	O
suited	O
for	O
certain	O
forms	O
of	O
objec-	O
tive	O
bayes	O
prior	B
distributions	O
.	O
taken	O
together	O
,	O
objective	O
priors	O
and	O
mcmc	O
computations	B
provide	O
an	O
attractive	O
package	O
for	O
the	O
statistician	O
faced	O
with	O
a	O
complicated	O
data	B
analy-	O
sis	O
situation	O
.	O
statistical	O
inference	B
becomes	O
almost	O
automatic	O
,	O
at	O
least	O
com-	O
pared	O
with	O
the	O
rigors	O
of	O
frequentist	O
analysis	B
.	O
this	O
chapter	O
discusses	O
both	O
parts	O
of	O
the	O
package	O
,	O
the	O
choice	O
of	O
prior	B
and	O
the	O
subsequent	O
computational	O
methods	O
.	O
criticisms	O
arise	O
,	O
both	O
from	O
the	O
frequentist	O
viewpoint	O
and	O
that	O
of	O
informative	O
bayesian	O
analysis	B
,	O
which	O
are	O
brought	O
up	O
here	O
and	O
also	O
in	O
chap-	O
ter	O
21	O
.	O
13.1	O
objective	O
prior	O
distributions	O
a	O
ﬂat	O
,	O
or	O
uniform	O
,	O
distribution	B
over	O
the	O
space	B
of	O
possible	O
parameter	O
values	O
seems	O
like	O
the	O
obvious	O
choice	O
for	O
an	O
uninformative	O
prior	B
distribution	I
,	O
and	O
has	O
been	O
so	O
ever	O
since	O
laplace	O
’	O
s	O
advocacy	O
in	O
the	O
late	O
eighteenth	O
century	O
.	O
for	O
a	O
ﬁnite	O
parameter	O
space	B
(	O
cid:127	O
)	O
,	O
say	O
(	O
cid:127	O
)	O
d	O
f	O
(	O
cid:22	O
)	O
.1/	O
;	O
(	O
cid:22	O
)	O
.2/	O
;	O
:	O
:	O
:	O
;	O
(	O
cid:22	O
)	O
.k/g	O
;	O
“	O
ﬂat	O
”	O
has	O
the	O
obvious	O
meaning	O
gﬂat	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
1	O
k	O
for	O
all	O
(	O
cid:22	O
)	O
2	O
(	O
cid:127	O
)	O
:	O
if	O
k	O
is	O
inﬁnite	O
,	O
or	O
if	O
(	O
cid:127	O
)	O
is	O
continuous	O
,	O
we	O
can	O
still	O
take	O
gﬂat	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
constant	O
:	O
(	O
13.1	O
)	O
(	O
13.2	O
)	O
(	O
13.3	O
)	O
bayes	O
’	O
rule	B
(	O
3.5	O
)	O
gives	O
the	O
same	O
posterior	B
distribution	I
for	O
any	O
choice	O
of	O
the	O
constant	O
,	O
gﬂat	O
.	O
(	O
cid:22	O
)	O
jx/	O
d	O
gﬂat	O
.	O
(	O
cid:22	O
)	O
/f	O
(	O
cid:22	O
)	O
.x/=f	O
.x/	O
;	O
with	O
f	O
(	O
cid:22	O
)	O
.x/gﬂat	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
(	O
cid:22	O
)	O
:	O
(	O
13.4	O
)	O
f	O
.x/	O
dz	O
(	O
cid:127	O
)	O
13.1	O
objective	O
prior	O
distributions	O
235	O
notice	O
that	O
gﬂat	O
.	O
(	O
cid:22	O
)	O
/	O
cancels	O
out	O
of	O
gﬂat.	O
(	O
cid:22	O
)	O
jx/	O
.	O
the	O
fact	O
that	O
gﬂat	O
.	O
(	O
cid:22	O
)	O
/	O
is	O
“	O
im-	O
proper	B
,	O
”	O
that	O
is	O
,	O
it	O
integrates	O
to	O
inﬁnity	O
,	O
doesn	O
’	O
t	B
affect	O
the	O
formal	O
use	O
of	O
bayes	O
’	O
rule	B
in	O
(	O
13.4	O
)	O
as	O
long	O
as	O
f	O
.x/	O
is	O
ﬁnite	O
.	O
notice	O
also	O
that	O
gﬂat	O
.	O
(	O
cid:22	O
)	O
jx/	O
amounts	O
to	O
taking	O
the	O
posterior	O
density	O
of	O
(	O
cid:22	O
)	O
to	O
be	O
proportional	O
to	O
the	O
likelihood	B
function	O
lx	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
f	O
(	O
cid:22	O
)	O
.x/	O
(	O
with	O
x	O
ﬁxed	O
and	O
(	O
cid:22	O
)	O
varying	O
over	O
(	O
cid:127	O
)	O
)	O
.	O
this	O
brings	O
us	O
close	O
to	O
fisherian	O
inference	B
,	O
with	O
its	O
emphasis	O
on	O
the	O
direct	O
interpretation	O
of	O
likelihoods	O
,	O
but	O
fisher	O
was	O
adamant	O
in	O
his	O
insistance	O
that	O
likelihood	B
was	O
not	O
probability	O
.	O
figure	O
13.1	O
the	O
solid	O
curve	O
is	O
ﬂat-prior	O
posterior	O
density	O
(	O
13.4	O
)	O
having	O
observed	O
x	O
d	O
10	O
from	O
poisson	O
model	B
x	O
(	O
cid:24	O
)	O
poi	O
.	O
(	O
cid:22	O
)	O
/	O
;	O
it	O
is	O
shifted	O
about	O
0.5	O
units	O
right	O
from	O
the	O
conﬁdence	B
density	I
(	O
dashed	O
)	O
of	O
figure	O
11.6.	O
jeffreys	O
’	O
prior	B
gives	O
a	O
posterior	O
density	O
(	O
dotted	O
)	O
nearly	O
the	O
same	O
as	O
the	O
conﬁdence	B
density	I
.	O
the	O
solid	O
curve	O
in	O
figure	O
13.1	O
shows	O
gﬂat	O
.	O
(	O
cid:22	O
)	O
jx/	O
for	O
the	O
poisson	O
situation	O
of	O
table	O
11.2	O
,	O
x	O
(	O
cid:24	O
)	O
poi	O
.	O
(	O
cid:22	O
)	O
/	O
;	O
(	O
13.5	O
)	O
with	O
x	O
d	O
10	O
observed	O
;	O
gﬂat	O
.	O
(	O
cid:22	O
)	O
jx/	O
is	O
shifted	O
almost	O
exactly	O
0.5	O
units	O
right	O
of	O
the	O
conﬁdence	B
density	I
from	O
figure	O
11.6	O
.	O
(	O
“	O
	O
”	O
is	O
(	O
cid:22	O
)	O
itself	O
in	O
this	O
case	O
.	O
)	O
2	O
fisher	O
’	O
s	O
withering	O
criticism	O
of	O
ﬂat-prior	O
bayes	O
inference	B
focused	O
on	O
its	O
2	O
the	O
reader	O
may	O
wish	O
to	O
review	O
chapter	O
11	O
,	O
particularly	O
section	O
11.6	O
,	O
for	O
these	O
constructions	B
.	O
0510152025300.000.020.040.060.080.100.12qdensitiesconfidencedensityposterior	O
density	B
,	O
flat	O
prior10jeffreys	O
objective	O
bayes	O
inference	B
and	O
mcmc	O
236	O
lack	O
of	O
transformation	O
invariance	O
.	O
if	O
we	O
were	O
interested	O
in	O
	O
d	O
log	O
.	O
(	O
cid:22	O
)	O
/	O
rather	O
than	O
(	O
cid:22	O
)	O
,	O
gﬂat.jx/	O
would	O
not	O
be	O
the	O
transformation	O
to	O
the	O
log	O
scale	B
of	O
gﬂat.	O
(	O
cid:22	O
)	O
jx/	O
.	O
jeffreys	O
’	O
prior	B
,	O
(	O
3.17	O
)	O
or	O
(	O
11.72	O
)	O
,	O
which	O
does	O
transform	B
correctly	O
,	O
is	O
(	O
13.6	O
)	O
for	O
x	O
(	O
cid:24	O
)	O
poi	O
.	O
(	O
cid:22	O
)	O
/	O
;	O
gjeff	O
.	O
(	O
cid:22	O
)	O
jx	O
d	O
10/	O
is	O
then	O
a	O
close	O
match	O
to	O
the	O
conﬁdence	B
density	I
in	O
figure	O
13.1	O
.	O
(	O
cid:22	O
)	O
gjeff	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
1ıp	O
coverage	B
matching	I
priors	O
1	O
2	O
a	O
variety	O
of	O
improvements	O
and	O
variations	O
on	O
jeffreys	O
’	O
prior	B
have	O
been	O
suggested	O
for	O
use	O
as	O
general-purpose	O
uninformative	O
prior	B
distributions	O
,	O
as	O
brieﬂy	O
discussed	O
in	O
the	O
chapter	O
endnotes	O
.	O
	O
all	O
share	O
the	O
drawback	O
seen	O
in	O
figure	O
11.7	O
:	O
the	O
posterior	B
distribution	I
g.	O
(	O
cid:22	O
)	O
jx/	O
can	O
have	O
unintended	O
effects	O
on	O
the	O
resulting	O
inferences	O
for	O
a	O
real-valued	O
parameter	O
of	O
interest	O
	O
d	O
t	B
.	O
(	O
cid:22	O
)	O
/	O
.	O
this	O
is	O
unavoidable	O
;	O
it	O
is	O
mathematically	O
impossible	O
for	O
any	O
single	O
prior	B
to	O
be	O
uninformative	O
for	O
every	O
choice	O
of	O
	O
d	O
t	B
.	O
(	O
cid:22	O
)	O
/	O
.	O
the	O
label	O
“	O
uninformative	O
”	O
for	O
a	O
prior	B
sometimes	O
means	O
“	O
gives	O
bayes	O
posterior	O
intervals	O
that	O
closely	O
match	O
conﬁdence	O
intervals.	O
”	O
perhaps	O
sur-	O
prisingly	O
,	O
this	O
deﬁnition	O
has	O
considerable	O
resonance	O
in	O
the	O
bayes	O
commu-	O
nity	O
.	O
such	O
priors	B
can	O
be	O
constructed	O
for	O
any	O
given	O
scalar	O
parameter	O
of	O
in-	O
terest	O
	O
d	O
t	B
.	O
(	O
cid:22	O
)	O
/	O
,	O
for	O
instance	O
the	O
maximum	O
eigenvalue	O
parameter	O
of	O
fig-	O
ure	O
11.7.	O
in	O
brief	O
,	O
the	O
construction	O
proceeds	O
as	O
follows.	O
(	O
cid:15	O
)	O
the	O
p-dimensional	O
parameter	O
vector	B
(	O
cid:22	O
)	O
is	O
transformed	O
to	O
a	O
form	B
that	O
makes	O
	O
the	O
ﬁrst	O
coordinate	O
,	O
say	O
where	O
(	O
cid:23	O
)	O
is	O
a	O
.p	O
(	O
cid:0	O
)	O
1/-dimensioned	O
nuisance	O
parameter	O
.	O
(	O
cid:15	O
)	O
the	O
transformation	O
is	O
chosen	O
so	O
that	O
the	O
fisher	O
information	B
matrix	O
(	O
11.72	O
)	O
for	O
.	O
;	O
(	O
cid:23	O
)	O
/	O
has	O
the	O
“	O
diagonal	O
”	O
form	O
(	O
13.7	O
)	O
(	O
13.8	O
)	O
(	O
cid:22	O
)	O
!	O
.	O
;	O
(	O
cid:23	O
)	O
/	O
;	O
	O
i	O
	O
0	O
0	O
0	O
i	O
(	O
cid:23	O
)	O
(	O
cid:23	O
)	O
:	O
(	O
this	O
is	O
always	O
possible	O
.	O
)	O
(	O
cid:15	O
)	O
finally	O
,	O
the	O
prior	B
for	O
.	O
;	O
(	O
cid:23	O
)	O
/	O
is	O
taken	O
proportional	O
to	O
(	O
13.9	O
)	O
where	O
h.	O
(	O
cid:23	O
)	O
/	O
is	O
an	O
arbitrary	O
.p	O
(	O
cid:0	O
)	O
1/-dimensional	O
density	B
.	O
in	O
other	O
words	O
,	O
i	O
1=2	O
	O
	O
h.	O
(	O
cid:23	O
)	O
/	O
;	O
g.	O
;	O
(	O
cid:23	O
)	O
/	O
d	O
13.2	O
conjugate	B
prior	O
distributions	O
237	O
g.	O
;	O
(	O
cid:23	O
)	O
/	O
combines	O
the	O
one-dimensional	O
jeffreys	O
’	O
prior	B
(	O
3.16	O
)	O
for	O
	O
with	O
an	O
arbitrary	O
independent	O
prior	B
for	O
the	O
orthogonal	O
nuisance	O
parameter	O
vector	B
(	O
cid:23	O
)	O
.	O
the	O
main	O
thing	O
to	O
notice	O
about	O
(	O
13.9	O
)	O
is	O
that	O
g.	O
;	O
(	O
cid:23	O
)	O
/	O
represents	O
different	O
priors	B
on	O
the	O
original	O
parameter	O
vector	B
(	O
cid:22	O
)	O
for	O
different	O
functions	O
	O
d	O
t	B
.	O
(	O
cid:22	O
)	O
/	O
.	O
no	O
single	O
prior	B
g.	O
(	O
cid:22	O
)	O
/	O
can	O
be	O
uninformative	O
for	O
all	O
choices	O
of	O
the	O
parameter	O
of	O
interest	O
	O
.	O
calculating	O
g.	O
;	O
(	O
cid:23	O
)	O
/	O
can	O
be	O
difﬁcult	O
.	O
one	O
alternative	O
is	O
to	O
go	O
directly	O
to	O
the	O
bca	O
conﬁdence	B
density	I
(	O
11.68	O
)	O
–	O
(	O
11.69	O
)	O
,	O
which	O
can	O
be	O
interpreted	O
as	O
the	O
posterior	B
distribution	I
from	O
an	O
uninformative	O
prior	B
(	O
because	O
its	O
integrals	O
agree	O
closely	O
with	O
conﬁdence	B
interval	I
endpoints	O
)	O
.	O
coverage	B
matching	I
priors	O
are	O
not	O
much	O
used	O
in	O
practice	O
,	O
and	O
in	O
fact	O
none	O
of	O
the	O
eight	O
annals	O
of	O
applied	O
statistics	B
objective	O
bayes	O
papers	O
mentioned	O
earlier	O
were	O
of	O
type	O
(	O
13.9	O
)	O
.	O
a	O
form	B
of	O
“	O
almost	O
uninformative	O
”	O
priors	B
,	O
the	O
conjugates	O
,	O
is	O
more	O
popular	O
,	O
mainly	O
because	O
of	O
the	O
simpler	O
computation	O
of	O
their	O
posterior	O
distributions	O
.	O
13.2	O
conjugate	B
prior	O
distributions	O
a	O
mathematically	O
convenient	O
class	O
of	O
prior	B
distributions	O
,	O
the	O
conjugate	B
pri-	O
ors	O
,	O
applies	O
to	O
samples	O
from	O
an	O
exponential	O
family,3	O
section	O
5.5	O
,	O
f	O
(	O
cid:22	O
)	O
.x/	O
d	O
e˛x	O
(	O
cid:0	O
)	O
.˛/f0.x/	O
:	O
(	O
13.10	O
)	O
(	O
13.11	O
)	O
here	O
we	O
have	O
indexed	O
the	O
family	O
with	O
the	O
expectation	O
parameter	O
(	O
cid:22	O
)	O
d	O
ef	O
fxg	O
;	O
rather	O
than	O
the	O
canonical	O
parameter	O
˛	O
.	O
on	O
the	O
right-hand	O
side	O
of	O
(	O
13.10	O
)	O
,	O
˛	O
can	O
be	O
thought	O
of	O
as	O
a	O
one-to-one	O
function	B
of	O
(	O
cid:22	O
)	O
(	O
the	O
so-called	O
“	O
link	O
func-	O
tion	O
”	O
)	O
,	O
e.g.	O
,	O
˛	O
d	O
log	O
.	O
(	O
cid:22	O
)	O
/	O
for	O
the	O
poisson	O
family	O
.	O
the	O
observed	O
data	B
is	O
a	O
random	O
sample	B
x	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn/	O
from	O
f	O
(	O
cid:22	O
)	O
,	O
iid	O
(	O
cid:24	O
)	O
f	O
(	O
cid:22	O
)	O
;	O
x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn	O
(	O
13.12	O
)	O
having	O
density	B
function	O
the	O
average	O
nx	O
dp	O
xi	O
=n	O
being	O
sufﬁcient	O
.	O
f	O
(	O
cid:22	O
)	O
.x/	O
d	O
enœ˛	O
nx	O
(	O
cid:0	O
)	O
.˛/f0.x/	O
;	O
(	O
13.13	O
)	O
3	O
we	O
will	O
concentrate	O
on	O
one-parameter	B
families	O
,	O
though	O
the	O
theory	B
extends	O
to	O
the	O
multiparameter	O
case	O
.	O
figure	O
13.2	O
relates	O
to	O
a	O
two-parameter	O
situation	O
.	O
238	O
objective	O
bayes	O
inference	B
and	O
mcmc	O
the	O
family	O
of	O
conjugate	B
priors	O
for	O
(	O
cid:22	O
)	O
,	O
gn0	O
;	O
x0	O
.	O
(	O
cid:22	O
)	O
/	O
,	O
allows	O
the	O
statistician	O
to	O
choose	O
two	O
parameters	O
,	O
n0	O
and	O
x0	O
,	O
gn0	O
;	O
x0	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
cen0œx0˛	O
(	O
cid:0	O
)	O
.˛/ıv	O
.	O
(	O
cid:22	O
)	O
/	O
;	O
v	O
.	O
(	O
cid:22	O
)	O
/	O
the	O
variance	O
of	O
an	O
x	O
from	O
f	O
(	O
cid:22	O
)	O
,	O
v	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
varf	O
fxgi	O
(	O
13.14	O
)	O
(	O
13.15	O
)	O
c	O
is	O
the	O
constant	O
that	O
makes	O
gn0	O
;	O
x0	O
.	O
(	O
cid:22	O
)	O
/	O
integrate	O
to	O
1	O
with	O
respect	O
to	O
lebesgue	O
measure	O
on	O
the	O
interval	B
of	O
possible	O
(	O
cid:22	O
)	O
values	O
.	O
the	O
interpretation	O
is	O
that	O
x0	O
represents	O
the	O
average	O
of	O
n0	O
hypothetical	O
prior	B
observations	O
from	O
f	O
(	O
cid:22	O
)	O
.	O
the	O
utility	O
of	O
conjugate	B
priors	O
is	O
seen	O
in	O
the	O
following	O
theorem	B
.	O
3	O
theorem	B
13.1	O
	O
deﬁne	O
nc	O
d	O
n0	O
c	O
n	O
and	O
nx	O
:	O
then	O
the	O
posterior	O
density	O
of	O
(	O
cid:22	O
)	O
given	O
x	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn/	O
is	O
nc	O
x0	O
c	O
n	O
nc	O
nxc	O
d	O
n0	O
g.	O
(	O
cid:22	O
)	O
jx/	O
d	O
gnc	O
;	O
nxc	O
.	O
(	O
cid:22	O
)	O
/i	O
moreover	O
,	O
the	O
posterior	O
expectation	O
of	O
(	O
cid:22	O
)	O
given	O
x	O
is	O
nx	O
:	O
ef	O
(	O
cid:22	O
)	O
jxg	O
d	O
n0	O
nc	O
x0	O
c	O
n	O
nc	O
(	O
13.16	O
)	O
(	O
13.17	O
)	O
(	O
13.18	O
)	O
the	O
intuitive	O
interpretation	O
is	O
quite	O
satisfying	O
:	O
we	O
begin	O
with	O
a	O
hypo-	O
thetical	O
prior	B
sample	O
of	O
size	O
n0	O
,	O
sufﬁcient	O
statistic	B
x0	O
;	O
observe	O
x	O
,	O
a	O
sam-	O
ple	O
of	O
size	O
n	O
;	O
and	O
update	O
our	O
prior	B
distribution	I
gn0	O
;	O
x0	O
.	O
(	O
cid:22	O
)	O
/	O
to	O
a	O
distribution	B
gnc	O
;	O
nxc	O
.	O
(	O
cid:22	O
)	O
/	O
of	O
the	O
same	O
form	B
.	O
moreover	O
,	O
ef	O
(	O
cid:22	O
)	O
jxg	O
equals	O
the	O
average	O
of	O
a	O
hypothetical	O
sample	B
with	O
n0	O
copies	O
of	O
x0	O
,	O
.x0	O
;	O
x0	O
;	O
:	O
:	O
:	O
;	O
x0	O
;	O
x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn/	O
:	O
(	O
13.19	O
)	O
iid	O
(	O
cid:24	O
)	O
poi	O
.	O
(	O
cid:22	O
)	O
/	O
,	O
that	O
is	O
we	O
have	O
n	O
i.i.d	O
.	O
observa-	O
tions	O
from	O
a	O
poisson	O
distribution	B
,	O
table	O
5.1.	O
formula	B
(	O
13.14	O
)	O
gives	O
conju-	O
gate	O
prior	B
	O
as	O
an	O
example	O
,	O
suppose	O
xi	O
4	O
gn0	O
;	O
x0	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
c	O
(	O
cid:22	O
)	O
n0x0	O
(	O
cid:0	O
)	O
1e	O
(	O
cid:0	O
)	O
n0	O
(	O
cid:22	O
)	O
;	O
(	O
13.20	O
)	O
c	O
not	O
depending	O
on	O
(	O
cid:22	O
)	O
.	O
so	O
in	O
the	O
notation	O
of	O
table	O
5.1	O
,	O
gn0	O
;	O
x0	O
.	O
(	O
cid:22	O
)	O
/	O
is	O
a	O
gamma	B
distribution	O
,	O
gam.n0x0	O
;	O
1=n0/	O
.	O
the	O
posterior	B
distribution	I
is	O
g.	O
(	O
cid:22	O
)	O
jx/	O
d	O
gnc	O
;	O
nxc	O
.	O
(	O
cid:22	O
)	O
/	O
(	O
cid:24	O
)	O
gam.nc	O
nxc	O
;	O
1=nc/	O
(	O
cid:24	O
)	O
1	O
nc	O
gnc	O
nxc	O
;	O
(	O
13.21	O
)	O
13.2	O
conjugate	B
prior	O
distributions	O
239	O
5	O
where	O
g	O
(	O
cid:23	O
)	O
indicates	O
a	O
standard	O
gamma	O
distribution	B
,	O
	O
g	O
(	O
cid:23	O
)	O
d	O
gam	O
.	O
(	O
cid:23	O
)	O
;	O
1/	O
:	O
(	O
13.22	O
)	O
table	O
13.1	O
conjugate	B
priors	O
(	O
13.14	O
)	O
–	O
(	O
13.16	O
)	O
for	O
four	O
familiar	O
one-parameter	B
exponential	O
families	O
,	O
using	O
notation	O
in	O
table	O
5.1	O
;	O
the	O
last	O
column	O
shows	O
the	O
posterior	B
distribution	I
of	O
(	O
cid:22	O
)	O
given	O
n	O
observations	O
xi	O
,	O
starting	O
from	O
prior	O
gn0	O
;	O
x0	O
.	O
(	O
cid:22	O
)	O
/	O
.	O
in	O
line	O
4	O
,	O
g	O
(	O
cid:23	O
)	O
is	O
the	O
standard	O
gamma	O
distribution	B
gam	O
.	O
(	O
cid:23	O
)	O
;	O
1/	O
,	O
with	O
(	O
cid:22	O
)	O
the	O
same	O
as	O
gamma	B
parameter	O
(	O
cid:27	O
)	O
in	O
table	O
5.1.	O
the	O
chapter	O
endnotes	O
give	O
the	O
density	B
of	O
the	O
inverse	O
gamma	B
distribution	O
1=g	O
(	O
cid:23	O
)	O
,	O
and	O
corresponding	O
results	O
for	O
chi-squared	O
variates	O
.	O
name	O
1.	O
normal	B
xi	O
distribution	B
n	O
.	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
2	O
1	O
/	O
(	O
(	O
cid:27	O
)	O
2	O
1	O
known	O
)	O
2.	O
poisson	O
poi	O
.	O
(	O
cid:22	O
)	O
/	O
3.	O
binomial	B
bi.1	O
;	O
(	O
cid:22	O
)	O
/	O
4.	O
gamma	B
(	O
cid:22	O
)	O
g	O
(	O
cid:23	O
)	O
=	O
(	O
cid:23	O
)	O
(	O
(	O
cid:23	O
)	O
known	O
)	O
gn0	O
;	O
x0	O
.	O
(	O
cid:22	O
)	O
/	O
1	O
=n0/	O
n	O
.x0	O
;	O
(	O
cid:27	O
)	O
2	O
1	O
=nc/	O
gam.nc	O
nxc	O
;	O
1=nc/	O
gam.n0x0	O
;	O
1=n0/	O
be.n0x0	O
;	O
n0.1	O
(	O
cid:0	O
)	O
x0//	O
be.nc	O
nxc	O
;	O
nc.1	O
(	O
cid:0	O
)	O
nxc//	O
g.	O
(	O
cid:22	O
)	O
jx/	O
n	O
.nxc	O
;	O
(	O
cid:27	O
)	O
2	O
n0x0	O
(	O
cid:23	O
)	O
=gn0	O
(	O
cid:23	O
)	O
c1	O
nc	O
nxc	O
(	O
cid:23	O
)	O
=gnc	O
(	O
cid:23	O
)	O
c1	O
table	O
13.1	O
describes	O
the	O
conjugate	B
prior	O
and	O
posterior	O
distributions	O
for	O
four	O
familiar	O
one-parameter	B
families	O
.	O
the	O
binomial	B
case	O
,	O
where	O
(	O
cid:22	O
)	O
is	O
the	O
“	O
success	O
probability	O
”	O
(	O
cid:25	O
)	O
in	O
table	O
5.1	O
,	O
is	O
particularly	O
evocative	O
:	O
indepen-	O
i	O
xi	O
d	O
nnx	O
successes	O
.	O
prior	B
gn0	O
;	O
x0	O
.	O
(	O
cid:25	O
)	O
/	O
amounts	O
to	O
assuming	O
proportion	B
x0	O
d	O
s0=n0	O
prior	B
successes	O
in	O
n0	O
ﬂips	O
.	O
formula	B
(	O
13.18	O
)	O
becomes	O
dent	O
coin	O
ﬂips	O
x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn	O
give	O
,	O
say	O
,	O
s	O
dp	O
ef	O
(	O
cid:25	O
)	O
jxg	O
d	O
s0	O
c	O
s	O
n0	O
c	O
n	O
(	O
13.23	O
)	O
for	O
the	O
posterior	O
expectation	O
of	O
(	O
cid:25	O
)	O
.	O
the	O
choice	O
.n0	O
;	O
x0/	O
d	O
.2	O
;	O
1=2/	O
for	O
in-	O
stance	O
gives	O
bayesian	O
estimate	B
.sc	O
1/=.nc	O
2/	O
for	O
(	O
cid:25	O
)	O
,	O
pulling	O
the	O
mle	O
s=n	O
a	O
little	O
bit	O
toward	O
1/2	O
.	O
the	O
size	O
of	O
n0	O
,	O
the	O
number	O
of	O
hypothetical	O
prior	B
observations	O
,	O
deter-	O
mines	O
how	O
informative	O
or	O
uninformative	O
the	O
prior	B
gn0	O
;	O
x0	O
.	O
(	O
cid:22	O
)	O
/	O
is	O
.	O
recent	O
objective	O
bayes	O
literature	O
has	O
favored	O
choosing	O
n0	O
small	O
,	O
n0	O
d	O
1	O
being	O
popular	O
.	O
the	O
hope	O
here	O
is	O
to	O
employ	O
a	O
proper	B
prior	O
(	O
one	O
that	O
has	O
a	O
ﬁnite	O
integral	O
)	O
,	O
while	O
still	O
not	O
injecting	O
much	O
unwarranted	O
information	B
into	O
the	O
analysis	B
.	O
the	O
choice	O
of	O
x0	O
is	O
also	O
by	O
convention	O
.	O
one	O
possibility	O
is	O
to	O
set	B
objective	O
bayes	O
inference	B
and	O
mcmc	O
240	O
x0	O
d	O
nx	O
,	O
in	O
which	O
case	O
the	O
posterior	O
expectation	O
ef	O
(	O
cid:22	O
)	O
jxg	O
(	O
13.18	O
)	O
equals	O
the	O
mle	O
nx	O
.	O
another	O
possibility	O
is	O
choosing	O
x0	O
equal	O
to	O
a	O
“	O
null	O
”	O
value	O
,	O
for	O
instance	O
x0	O
d	O
0	O
for	O
effect	O
size	O
estimation	B
in	O
(	O
3.28	O
)	O
.	O
table	O
13.2	O
vasoconstriction	B
data	O
;	O
volume	O
of	O
air	O
inspired	O
in	O
39	O
cases	O
,	O
19	O
without	O
vasoconstriction	B
(	O
y	O
d	O
0	O
)	O
and	O
20	O
with	O
vasoconstriction	B
(	O
y	O
d	O
1	O
)	O
.	O
y	O
d	O
0	O
98	O
60	O
98	O
74	O
104	O
78	O
104	O
78	O
113	O
78	O
88	O
118	O
120	O
90	O
123	O
95	O
95	O
137	O
98	O
y	O
d	O
1	O
115	O
85	O
120	O
88	O
126	O
88	O
126	O
90	O
128	O
90	O
93	O
136	O
143	O
104	O
151	O
108	O
154	O
110	O
111	O
157	O
6	O
as	O
a	O
miniature	O
example	O
of	O
objective	O
bayes	O
inference	B
,	O
we	O
consider	O
the	O
vasoconstriction	B
dataof	O
table	O
13.2	O
:	O
n	O
d	O
39	O
measurements	O
of	O
lung	O
volume	O
have	O
been	O
obtained	O
,	O
19	O
without	O
vasoconstriction	B
.y	O
d	O
0/	O
and	O
20	O
with	O
.y	O
d	O
1/	O
.	O
here	O
we	O
will	O
think	O
of	O
the	O
yi	O
as	O
binomial	B
variates	O
,	O
following	O
logistic	B
regression	I
model	O
(	O
8.5	O
)	O
,	O
ind	O
(	O
cid:24	O
)	O
bi.1	O
;	O
(	O
cid:25	O
)	O
i	O
/	O
;	O
yi	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
39	O
;	O
	O
(	O
cid:25	O
)	O
i	O
1	O
(	O
cid:0	O
)	O
(	O
cid:25	O
)	O
i	O
	O
d	O
˛0	O
c	O
˛1xi	O
;	O
log	O
(	O
13.24	O
)	O
(	O
13.25	O
)	O
with	O
the	O
xi	O
as	O
ﬁxed	O
covariates	O
(	O
the	O
values	O
in	O
table	O
13.2	O
)	O
.	O
letting	O
xi	O
d	O
.1	O
;	O
xi	O
/	O
nential	O
family	O
(	O
8.24	O
)	O
,	O
0	O
,	O
(	O
13.24	O
)	O
–	O
(	O
13.25	O
)	O
results	O
in	O
a	O
two-parameter	O
expo-	O
f0.y/	O
;	O
(	O
13.26	O
)	O
the	O
mle	O
o˛	O
has	O
approximate	O
2	O
(	O
cid:2	O
)	O
2	O
covariance	O
matrix	B
ov	O
as	O
given	O
in	O
(	O
8.30	O
)	O
.	O
i	O
0	O
oˇ	O
(	O
cid:0	O
)	O
.˛/	O
˛	O
h	B
f˛.y/	O
d	O
en	O
!	O
0	O
nx	O
yi	O
;	O
xi	O
yi	O
id1	O
having	O
o	O
ˇ	O
d	O
1	O
n	O
nx	O
id1	O
and	O
.˛/	O
d	O
1	O
log.1	O
c	O
e˛	O
0	O
xi	O
/	O
:	O
nx	O
id1	O
n	O
13.2	O
conjugate	B
prior	O
distributions	O
241	O
in	O
figure	O
13.2	O
,	O
the	O
posterior	O
distributions	O
are	O
graphed	O
in	O
terms	O
of	O
(	O
13.27	O
)	O
rather	O
than	O
˛	O
or	O
(	O
cid:22	O
)	O
,	O
making	O
the	O
contours	O
of	O
equal	O
density	B
roughly	O
circular	O
and	O
centered	O
at	O
zero	O
.	O
(	O
cid:13	O
)	O
d	O
ov	O
(	O
cid:0	O
)	O
1=2.˛	O
(	O
cid:0	O
)	O
o˛/	O
figure	O
13.2	O
vasoconstriction	B
data	O
;	O
contours	O
of	O
equal	O
posterior	O
density	O
of	O
(	O
cid:13	O
)	O
(	O
13.27	O
)	O
from	O
four	O
uninformative	O
priors	B
,	O
as	O
described	O
in	O
the	O
text	O
.	O
numbers	O
indicate	O
probability	O
content	O
within	O
contours	O
;	O
light	O
dashed	O
contours	O
from	O
panel	O
a	O
,	O
ﬂat	O
prior	B
.	O
panel	O
a	O
of	O
figure	O
13.2	O
illustrates	O
the	O
ﬂat	O
prior	B
posterior	O
density	B
of	O
(	O
cid:13	O
)	O
given	O
the	O
data	B
y	O
in	O
model	B
(	O
13.24	O
)	O
–	O
(	O
13.25	O
)	O
.	O
the	O
heavy	O
lines	O
are	O
contours	O
of	O
equal	O
density	B
,	O
with	O
the	O
one	O
labeled	O
“	O
0.9	O
”	O
containing	O
90	O
%	O
of	O
the	O
pos-	O
terior	O
probability	O
,	O
etc	O
.	O
panel	O
b	O
shows	O
the	O
corresponding	O
posterior	O
density	O
a.	O
flat	O
priorg1g2	O
0.1	O
0.3	O
0.5	O
0.75	O
0.9	O
0.975	O
0.99	O
0.997	O
0.999	O
0.999	O
−4−2024−4−2024lb	O
.	O
jeffreys	O
'	O
priorg1g2	O
0.1	O
0.3	O
0.5	O
0.75	O
0.9	O
0.975	O
0.99	O
0.997	O
0.999	O
−4−2024−4−2024	O
0.1	O
0.5	O
0.9	O
0.99	O
0.999	O
0.999	O
lc	O
.	O
conjugate	B
priorg1g2	O
0.1	O
0.3	O
0.5	O
0.75	O
0.9	O
0.975	O
0.99	O
0.997	O
0.999	O
−4−2024−4−2024	O
0.1	O
0.5	O
0.9	O
0.99	O
0.999	O
0.999	O
ld	O
.	O
bootstrap	O
distributiong1g2	O
0.1	O
0.3	O
0.5	O
0.75	O
0.9	O
0.975	O
0.99	O
0.997	O
0.997	O
0.999	O
−4−2024−4−2024l	O
0.1	O
0.5	O
0.9	O
0.99	O
0.999	O
0.999	O
242	O
objective	O
bayes	O
inference	B
and	O
mcmc	O
contours	O
obtained	O
from	O
jeffreys	O
’	O
multiparameter	O
prior	B
(	O
11.72	O
)	O
,	O
in	O
this	O
case	O
gjeff.˛/	O
d	O
jv˛j1=2	O
;	O
(	O
13.28	O
)	O
v˛	O
the	O
covariance	O
matrix	B
of	O
o˛	O
,	O
as	O
calculated	O
from	O
(	O
8.30	O
)	O
.	O
for	O
comparison	O
purposes	O
the	O
light	O
dashed	O
curves	O
show	O
some	O
of	O
the	O
ﬂat	O
prior	B
contours	O
from	O
panel	O
a.	O
the	O
effect	O
of	O
gjeff.˛/	O
is	O
to	O
reduce	O
the	O
ﬂat	O
prior	B
bulge	O
toward	O
the	O
upper	O
left	O
corner	O
.	O
panel	O
c	O
relates	O
to	O
the	O
conjugate	B
prior4	O
g1	O
;	O
0.˛/	O
.	O
besides	O
reducing	O
the	O
ﬂat	O
prior	B
bulge	O
,	O
g1	O
;	O
0.˛/	O
pulls	O
the	O
contours	O
slightly	O
downward	O
.	O
(	O
13.25	O
)	O
,	O
with	O
o˛	O
replacing	O
˛	O
,	O
gave	O
resamples	O
y	O
the	O
contours	O
of	O
o	O
(	O
cid:13	O
)	O
toward	O
the	O
left	O
.	O
panel	O
d	O
shows	O
the	O
parametric	B
bootstrap	O
distribution	B
:	O
model	B
(	O
13.24	O
)	O
–	O
(	O
cid:3	O
)	O
and	O
mle	O
replications	O
o˛	O
(	O
cid:3	O
)	O
.	O
(	O
cid:3	O
)	O
(	O
cid:0	O
)	O
o˛/	O
considerably	O
accentuate	O
the	O
bulge	O
(	O
cid:3	O
)	O
d	O
ov	O
(	O
cid:0	O
)	O
1=2.o˛	O
figure	O
13.3	O
posterior	O
densities	O
for	O
(	O
cid:13	O
)	O
1	O
,	O
ﬁrst	O
coordinate	O
of	O
(	O
cid:13	O
)	O
in	O
(	O
13.27	O
)	O
,	O
for	O
the	O
vasoconstriction	B
data	O
.	O
dashed	O
red	O
curve	O
:	O
raw	O
(	O
unweighted	O
)	O
distribution	B
of	O
b	O
d	O
8000	O
parametric	B
replications	O
from	O
model	O
(	O
13.24	O
)	O
–	O
(	O
13.25	O
)	O
;	O
solid	O
black	O
curve	O
:	O
bca	O
density	B
(	O
11.68	O
)	O
(	O
z0	O
d	O
0:123	O
,	O
a	O
d	O
0:053	O
)	O
;	O
dotted	O
blue	O
curve	O
:	O
posterior	O
density	O
using	O
jeffreys	O
multiparameter	O
prior	B
(	O
11.72	O
)	O
.	O
4	O
the	O
role	O
of	O
nx	O
in	O
(	O
13.13	O
)	O
is	O
taken	O
by	O
oˇ	O
in	O
(	O
13.26	O
)	O
,	O
so	O
g1	O
;	O
0	O
has	O
oˇ	O
d	O
0	O
,	O
n0	O
d	O
1.	O
this	O
makes	O
g1	O
;	O
0.˛/	O
d	O
expf	O
(	O
cid:0	O
)	O
.˛/g	O
.	O
the	O
factor	B
v	O
.	O
(	O
cid:22	O
)	O
/	O
in	O
(	O
13.14	O
)	O
is	O
absent	O
in	O
the	O
conjugate	B
prior	O
for	O
˛	O
(	O
as	O
opposed	O
to	O
(	O
cid:22	O
)	O
)	O
.	O
−4−20240.000.010.020.030.04g1density*********************************************************************************bcaraw	O
bootstrapjeffreys	O
13.3	O
model	B
selection	I
and	O
the	O
bayesian	O
information	B
criterion	I
243	O
this	O
doesn	O
’	O
t	B
necessarily	O
imply	O
that	O
a	O
bootstrap	O
analysis	B
would	O
give	O
much	O
different	O
answers	O
than	O
the	O
three	O
(	O
quite	O
similar	O
)	O
objective	O
bayes	O
results	O
.	O
for	O
any	O
particular	O
real-valued	O
parameter	O
of	O
interest	O
	O
,	O
the	O
raw	O
bootstrap	O
distri-	O
bution	O
(	O
equal	O
weight	O
on	O
each	O
replication	B
)	O
would	O
be	O
reweighted	O
according	O
to	O
the	O
bca	O
formula	B
(	O
11.68	O
)	O
in	O
order	O
to	O
produce	O
accurate	O
conﬁdence	B
intervals	I
.	O
figure	O
13.3	O
compares	O
the	O
raw	O
bootstrap	O
distribution	B
,	O
the	O
bca	O
conﬁdence	B
density	I
,	O
and	O
the	O
posterior	O
density	O
obtained	O
from	O
jeffreys	O
’	O
prior	B
,	O
for	O
	O
equal	O
to	O
(	O
cid:13	O
)	O
1	O
,	O
the	O
ﬁrst	O
coordinate	O
of	O
(	O
cid:13	O
)	O
in	O
(	O
13.27	O
)	O
.	O
the	O
bca	O
density	B
is	O
shifted	O
to	O
the	O
right	O
of	O
jeffreys	O
’	O
.	O
critique	O
of	O
objective	O
bayes	O
inference	B
despite	O
its	O
simplicity	O
,	O
or	O
perhaps	O
because	O
of	O
it	O
,	O
objective	O
bayes	O
procedures	O
are	O
vulnerable	O
to	O
criticism	O
from	O
both	O
ends	O
of	O
the	O
statistical	O
spectrum	O
.	O
from	O
the	O
subjectivist	O
point	O
of	O
view	O
,	O
objective	O
bayes	O
is	O
only	O
partially	O
bayesian	O
:	O
it	O
employs	O
bayes	O
’	O
theorem	B
but	O
without	O
doing	O
the	O
hard	O
work	O
of	O
determining	O
a	O
convincing	O
prior	B
distribution	I
.	O
this	O
introduces	O
frequentist	O
elements	O
into	O
its	O
practice—clearly	O
so	O
in	O
the	O
case	O
of	O
jeffreys	O
’	O
prior—along	O
with	O
frequentist	O
incoherencies	O
.	O
for	O
the	O
frequentist	O
,	O
objective	O
bayes	O
analysis	B
can	O
seem	O
dangerously	O
un-	O
tethered	O
from	O
the	O
usual	O
standards	O
of	O
accuracy	O
,	O
having	O
only	O
tenuous	O
large-	O
sample	B
claims	O
to	O
legitimacy	O
.	O
this	O
is	O
more	O
than	O
a	O
theoretical	O
objection	O
.	O
the	O
practical	O
advantages	O
claimed	O
for	O
bayesian	O
methods	O
depend	O
crucially	O
on	O
the	O
ﬁne	O
structure	O
of	O
the	O
prior	B
.	O
can	O
we	O
safely	O
ignore	O
stopping	O
rules	O
or	O
selective	O
inference	B
(	O
e.g.	O
,	O
choosing	O
the	O
largest	O
of	O
many	O
estimated	O
parameters	O
for	O
spe-	O
cial	O
attention	O
)	O
for	O
a	O
prior	B
not	O
based	O
on	O
some	O
form	B
of	O
genuine	O
experience	O
?	O
in	O
an	O
era	O
of	O
large	O
,	O
complicated	O
,	O
and	O
difﬁcult	O
data-analytic	O
problems	O
,	O
ob-	O
jective	O
bayes	O
methods	O
are	O
answering	O
a	O
felt	O
need	O
for	O
relatively	O
straightfor-	O
ward	O
paths	O
to	O
solution	O
.	O
granting	O
their	O
usefulness	O
,	O
it	O
is	O
still	O
reasonable	O
to	O
hope	O
for	O
better	O
justiﬁcation,5	O
or	O
at	O
least	O
for	O
more	O
careful	O
comparisons	O
with	O
competing	O
methods	O
as	O
in	O
figure	O
13.3	O
.	O
13.3	O
model	B
selection	I
and	O
the	O
bayesian	O
information	B
criterion	I
data-based	O
model	B
selection	I
has	O
become	O
a	O
major	O
theme	O
of	O
modern	O
statisti-	O
cal	O
inference	B
.	O
in	O
the	O
problem	O
’	O
s	O
simplest	O
form	B
,	O
the	O
statistician	O
observes	O
data	B
x	O
and	O
wishes	O
to	O
choose	O
between	O
a	O
smaller	O
model	B
m0	O
and	O
a	O
larger	O
model	B
5	O
chapter	O
20	O
discusses	O
the	O
frequentist	O
assessment	O
of	O
bayes	O
and	O
objective	O
bayes	O
estimates	O
.	O
244	O
objective	O
bayes	O
inference	B
and	O
mcmc	O
m1	O
.	O
the	O
classic	O
textbook	O
example	O
takes	O
x	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn/	O
0	O
as	O
an	O
inde-	O
pendent	O
normal	B
sample	O
,	O
iid	O
(	O
cid:24	O
)	O
xi	O
n	O
.	O
(	O
cid:22	O
)	O
;	O
1/	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
;	O
(	O
13.29	O
)	O
(	O
13.30	O
)	O
with	O
m0	O
the	O
null	O
hypothesis	O
(	O
cid:22	O
)	O
d	O
0	O
and	O
m1	O
the	O
general	O
two-sided	O
alter-	O
native	O
,	O
m0	O
w	O
(	O
cid:22	O
)	O
d	O
0	O
;	O
m1	O
w	O
(	O
cid:22	O
)	O
¤	O
0	O
:	O
(	O
we	O
can	O
include	O
(	O
cid:22	O
)	O
d	O
0	O
in	O
m1	O
with	O
no	O
effect	O
on	O
what	O
follows	O
.	O
)	O
from	O
a	O
frequentist	O
viewpoint	O
,	O
choosing	O
between	O
m0	O
and	O
m1	O
in	O
(	O
13.29	O
)	O
–	O
(	O
13.30	O
)	O
amounts	O
to	O
running	O
a	O
hypothesis	O
test	O
of	O
h0	O
w	O
(	O
cid:22	O
)	O
d	O
0	O
,	O
perhaps	O
augmented	O
with	O
a	O
conﬁdence	B
interval	I
for	O
(	O
cid:22	O
)	O
.	O
bayesian	O
model	B
selection	I
aims	O
for	O
more	O
:	O
an	O
evaluation	O
of	O
the	O
posterior	O
probabilities	O
of	O
m0	O
and	O
m1	O
given	O
x.	O
a	O
full	B
bayesian	O
speciﬁcation	O
re-	O
quires	O
prior	B
probabilities	O
for	O
the	O
two	O
models	B
,	O
and	O
conditional	O
prior	B
densities	O
for	O
(	O
cid:22	O
)	O
within	O
each	O
model	B
,	O
m1/	O
:	O
and	O
g1	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
g.	O
(	O
cid:22	O
)	O
j	O
m0/	O
density	B
for	O
x	O
,	O
say	O
f0.x/	O
dz	O
f	O
(	O
cid:22	O
)	O
.x/g0	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
(	O
cid:22	O
)	O
and	O
f1.x/	O
dz	O
let	O
f	O
(	O
cid:22	O
)	O
.x/	O
be	O
the	O
density	B
of	O
x	O
given	O
(	O
cid:22	O
)	O
.	O
each	O
model	B
induces	O
a	O
marginal	O
f	O
(	O
cid:22	O
)	O
.x/g1	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
(	O
cid:22	O
)	O
:	O
m0	O
m1	O
(	O
13.33	O
)	O
bayes	O
’	O
theorem	B
,	O
in	O
its	O
ratio	O
form	B
(	O
3.8	O
)	O
,	O
then	O
gives	O
posterior	O
probabilities	O
(	O
cid:25	O
)	O
0.x/	O
d	O
prf	O
m0jxg	O
and	O
(	O
cid:25	O
)	O
1.x/	O
d	O
prf	O
m1jxg	O
and	O
(	O
cid:25	O
)	O
1	O
d	O
1	O
(	O
cid:0	O
)	O
(	O
cid:25	O
)	O
0	O
d	O
prf	O
m1g	O
;	O
(	O
cid:25	O
)	O
0	O
d	O
prf	O
m0g	O
g0	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
g.	O
(	O
cid:22	O
)	O
j	O
(	O
13.31	O
)	O
(	O
13.32	O
)	O
(	O
13.34	O
)	O
(	O
13.35	O
)	O
(	O
13.36	O
)	O
satisfying	O
(	O
cid:25	O
)	O
1.x/	O
(	O
cid:25	O
)	O
0.x/	O
d	O
(	O
cid:25	O
)	O
1	O
(	O
cid:25	O
)	O
0	O
b.x/	O
;	O
where	O
b.x/	O
is	O
the	O
bayes	O
factor	B
b.x/	O
d	O
f1.x/	O
f0.x/	O
;	O
leading	O
to	O
the	O
elegant	O
statement	O
that	O
the	O
posterior	O
odds	O
ratio	O
is	O
the	O
prior	B
odds	O
ratio	O
times	O
the	O
bayes	O
factor	B
.	O
all	O
of	O
this	O
is	O
of	O
more	O
theoretical	O
than	O
applied	O
use	O
.	O
prior	B
speciﬁcations	O
(	O
13.31	O
)	O
–	O
(	O
13.32	O
)	O
are	O
usually	O
unavailable	O
in	O
practical	O
settings	O
(	O
which	O
is	O
why	O
13.3	O
model	B
selection	I
and	O
the	O
bic	O
245	O
standard	O
hypothesis	O
testing	B
is	O
so	O
popular	O
)	O
.	O
the	O
objective	O
bayes	O
school	O
has	O
concentrated	O
on	O
estimating	O
the	O
bayes	O
factor	B
b.x/	O
,	O
with	O
the	O
understanding	O
that	O
the	O
prior	B
odds	O
ratio	O
(	O
cid:25	O
)	O
1=	O
(	O
cid:25	O
)	O
0	O
in	O
(	O
13.35	O
)	O
would	O
be	O
roughly	O
evaluated	O
de-	O
pending	O
on	O
the	O
speciﬁc	O
circumstances—perhaps	O
set	B
to	O
the	O
laplace	O
choice	O
(	O
cid:25	O
)	O
1=	O
(	O
cid:25	O
)	O
0	O
d	O
1.	O
table	O
13.3	O
jeffreys	O
’	O
scale	B
of	O
evidence	O
for	O
the	O
interpretation	O
of	O
bayes	O
factors	O
.	O
bayes	O
factor	B
<	O
1	O
1–3	O
3–20	O
20–150	O
>	O
150	O
evidence	O
for	O
m1	O
negative	O
barely	O
worthwhile	O
positive	O
strong	O
very	O
strong	O
jeffreys	O
suggested	O
a	O
scale	B
of	O
evidence	O
for	O
interpreting	O
bayes	O
factors	O
,	O
re-	O
produced	O
in	O
table	O
13.3	O
;	O
	O
b.x/	O
d	O
10	O
for	O
instance	O
constitutes	O
positive	O
but	O
7	O
not	O
strong	O
evidence	O
in	O
favor	O
of	O
the	O
bigger	O
model	B
.	O
jeffreys	O
’	O
scale	B
is	O
a	O
bayes-	O
ian	O
version	O
of	O
fisher	O
’	O
s	O
interpretive	O
scale	B
for	O
the	O
outcome	O
of	O
a	O
hypothetic	O
test	O
,	O
with	O
coverage	O
value	O
(	O
one	O
minus	O
the	O
signiﬁcance	O
level	O
)	O
0.95	O
famously	O
constituting	O
“	O
signiﬁcant	O
”	O
evidence	O
against	O
the	O
null	O
hypothesis	O
.	O
table	O
13.4	O
shows	O
fisher	O
’	O
s	O
scale	B
,	O
as	O
commonly	O
interpreted	O
in	O
the	O
biomedical	O
and	O
social	O
sciences	O
.	O
table	O
13.4	O
fisher	O
’	O
s	O
scale	B
of	O
evidence	O
against	O
null	O
hypothesis	O
m0	O
and	O
in	O
favor	O
of	O
m1	O
,	O
as	O
a	O
function	B
of	O
coverage	O
level	O
(	O
1	O
minus	O
the	O
p-value	B
)	O
.	O
coverage	O
(	O
p-value	B
)	O
.80	O
.90	O
.95	O
.975	O
.99	O
.995	O
.999	O
(	O
.20	O
)	O
(	O
.10	O
)	O
(	O
.05	O
)	O
(	O
.025	O
)	O
(	O
.01	O
)	O
(	O
.005	O
)	O
(	O
.001	O
)	O
evidence	O
for	O
m1	O
null	O
borderline	O
moderate	O
substantial	O
strong	O
very	O
strong	O
overwhelming	O
even	O
if	O
we	O
accept	O
the	O
reduction	O
of	O
model	B
selection	I
to	O
assessing	O
the	O
bayes	O
factor	B
b.x/	O
in	O
(	O
13.35	O
)	O
,	O
and	O
even	O
if	O
we	O
accept	O
jeffreys	O
’	O
scale	B
of	O
in-	O
terpretation	O
,	O
this	O
still	O
leaves	O
a	O
crucial	O
question	O
:	O
how	O
to	O
compute	O
b.x/	O
in	O
8	O
246	O
objective	O
bayes	O
inference	B
and	O
mcmc	O
practice	O
,	O
without	O
requiring	O
informative	O
choices	O
of	O
the	O
priors	B
g0	O
and	O
g1	O
in	O
(	O
13.32	O
)	O
.	O
a	O
popular	O
objective	O
bayes	O
answer	O
is	O
provided	O
by	O
the	O
bayesian	O
informa-	O
tion	O
criterion	O
(	O
bic	O
)	O
.	O
for	O
a	O
given	O
model	B
m	O
we	O
deﬁne	O
log.n/	O
;	O
bic.m/	O
d	O
log˚f	O
o	O
(	O
cid:22	O
)	O
.x/	O
(	O
cid:9	O
)	O
(	O
cid:0	O
)	O
p	O
(	O
13.37	O
)	O
where	O
o	O
(	O
cid:22	O
)	O
is	O
the	O
mle	O
,	O
p	O
the	O
degrees	O
of	O
freedom	O
(	O
number	O
of	O
free	O
parameters	O
)	O
in	O
m	O
,	O
and	O
n	O
the	O
sample	B
size	I
.	O
then	O
the	O
bic	O
approximation	O
to	O
bayes	O
factor	B
b.x/	O
(	O
13.36	O
)	O
is	O
2	O
log	O
bbic.x/	O
d	O
bic.m1/	O
(	O
cid:0	O
)	O
bic.m0/	O
d	O
log˚f	O
o	O
(	O
cid:22	O
)	O
1	O
.x/=f	O
o	O
(	O
cid:22	O
)	O
0	O
.x/	O
(	O
cid:9	O
)	O
(	O
cid:0	O
)	O
p1	O
(	O
cid:0	O
)	O
p0	O
w	O
.x/	O
d	O
2	O
log˚f	O
o	O
(	O
cid:22	O
)	O
1	O
.x/=f	O
o	O
(	O
cid:22	O
)	O
0	O
.x/	O
(	O
cid:9	O
)	O
;	O
2	O
the	O
subscripts	O
indexing	O
the	O
mles	O
and	O
degrees	O
of	O
freedom	O
in	O
m1	O
and	O
m0	O
.	O
this	O
can	O
be	O
restated	O
in	O
somewhat	O
more	O
familiar	O
terms	O
.	O
letting	O
w	O
.x/	O
be	O
wilks	O
’	O
likelihood	B
ratio	O
statistic	B
,	O
log.n/	O
;	O
(	O
13.38	O
)	O
(	O
13.39	O
)	O
we	O
have	O
2	O
log	O
bbic.x/	O
d	O
1	O
fw	O
.x/	O
(	O
cid:0	O
)	O
d	O
log.n/g	O
;	O
with	O
d	O
d	O
p1	O
(	O
cid:0	O
)	O
p0	O
:	O
w	O
.x/	O
approximately	O
follows	O
a	O
(	O
cid:31	O
)	O
2	O
d	O
distribution	B
under	O
model	B
m0	O
,	O
e0fw	O
.x/g	O
:	O
d	O
d	O
,	O
implying	O
bbic.x/	O
will	O
tend	O
to	O
be	O
less	O
than	O
one	O
,	O
favoring	O
m0	O
if	O
it	O
is	O
true	O
,	O
ever	O
more	O
strongly	B
as	O
n	O
increases	O
.	O
we	O
can	O
apply	O
bic	O
selection	O
to	O
the	O
vasoconstriction	B
data	O
of	O
table	O
13.2	O
,	O
taking	O
m1	O
to	O
be	O
model	B
(	O
13.24	O
)	O
–	O
(	O
13.25	O
)	O
,	O
and	O
m0	O
to	O
be	O
the	O
submodel	O
having	O
˛1	O
d	O
0.	O
in	O
this	O
case	O
d	O
d	O
1	O
in	O
(	O
13.40	O
)	O
.	O
direct	O
calculation	O
gives	O
w	O
d	O
7:07	O
and	O
(	O
13.40	O
)	O
(	O
13.41	O
)	O
positive	O
but	O
not	O
strong	O
evidence	O
against	O
m0	O
according	O
to	O
jeffreys	O
’	O
scale	B
.	O
by	O
comparison	O
,	O
the	O
usual	O
frequentist	O
z-value	O
for	O
testing	B
˛1	O
d	O
0	O
is	O
2.36	O
,	O
coverage	O
level	O
0.982	O
,	O
between	O
substantial	O
and	O
strong	O
evidence	O
against	O
m0	O
on	O
fisher	O
’	O
s	O
scale	B
.	O
the	O
bic	O
was	O
named	O
in	O
reference	O
to	O
akaike	O
’	O
s	O
information	B
criterion	I
(	O
aic	O
)	O
,	O
bbic	O
d	O
5:49	O
;	O
aic.m/	O
d	O
log˚f	O
o	O
(	O
cid:22	O
)	O
.x/	O
(	O
cid:9	O
)	O
(	O
cid:0	O
)	O
p	O
;	O
(	O
13.42	O
)	O
which	O
suggests	O
,	O
as	O
in	O
(	O
12.73	O
)	O
,	O
basing	O
model	B
selection	I
on	O
the	O
sign	O
of	O
aic.m1/	O
(	O
cid:0	O
)	O
aic.m0/	O
d	O
1	O
2	O
fw	O
.x/	O
(	O
cid:0	O
)	O
2dg	O
:	O
(	O
13.43	O
)	O
13.3	O
model	B
selection	I
and	O
the	O
bic	O
247	O
the	O
bic	O
penalty	B
d	O
log.n/	O
in	O
(	O
13.40	O
)	O
grows	O
more	O
severe	O
than	O
the	O
aic	O
penalty	B
2d	O
as	O
n	O
gets	O
larger	O
,	O
increasingly	O
favoring	O
selection	O
of	O
m0	O
rather	O
than	O
m1	O
.	O
the	O
distinction	O
is	O
rooted	O
in	O
bayesian	O
notions	O
of	O
coherent	O
behav-	O
ior	O
,	O
as	O
discussed	O
in	O
what	O
follows	O
.	O
where	O
does	O
the	O
bic	O
penalty	B
term	O
d	O
log.n/	O
in	O
(	O
13.40	O
)	O
come	O
from	O
?	O
a	O
ﬁrst	O
answer	O
uses	O
the	O
simple	O
normal	B
model	O
xi	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
;	O
1/	O
,	O
(	O
13.29	O
)	O
–	O
(	O
13.30	O
)	O
.	O
m0	O
has	O
prior	B
g0	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
g.	O
(	O
cid:22	O
)	O
j	O
m0/	O
equal	O
a	O
delta	O
function	B
at	O
zero	O
.	O
suppose	O
we	O
take	O
g1	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
g.	O
(	O
cid:22	O
)	O
j	O
g1	O
.	O
(	O
cid:22	O
)	O
/	O
(	O
cid:24	O
)	O
m1/	O
in	O
(	O
13.32	O
)	O
to	O
be	O
the	O
gaussian	O
conjugate	B
prior	O
(	O
13.44	O
)	O
the	O
discussion	O
following	O
(	O
13.23	O
)	O
in	O
section	O
13.2	O
suggests	O
setting	O
m	O
d	O
0	O
and	O
a	O
d	O
1	O
,	O
corresponding	O
to	O
prior	B
information	O
equivalent	O
to	O
one	O
of	O
the	O
n	O
actual	O
observations	O
.	O
in	O
this	O
case	O
we	O
can	O
calculate	O
the	O
actual	O
bayes	O
factor	B
b.x/	O
,	O
n	O
.m	O
;	O
a/	O
:	O
(	O
cid:26	O
)	O
n	O
(	O
cid:27	O
)	O
w	O
.x/	O
(	O
cid:0	O
)	O
log.n	O
c	O
1/	O
;	O
log	O
b.x/	O
d	O
1	O
n	O
c	O
1	O
(	O
13.45	O
)	O
nearly	O
equaling	O
log	O
bbic.x/	O
(	O
d	O
d	O
1	O
)	O
,	O
for	O
large	O
n.	O
justiﬁcations	O
of	O
the	O
bic	O
formula	B
as	O
an	O
approximate	O
bayes	O
factor	B
follow	O
generalizations	O
of	O
this	O
kind	O
of	O
argument	B
,	O
as	O
discussed	O
in	O
the	O
chapter	O
endnotes	O
.	O
2	O
z	O
p	O
(	O
cid:24	O
)	O
n	O
.0	O
;	O
1/	O
under	O
m0	O
:	O
the	O
difference	O
between	O
bic	O
and	O
frequentist	O
hypothesis	B
testing	I
grows	O
more	O
drastic	O
for	O
large	O
n.	O
suppose	O
m0	O
is	O
a	O
regression	B
model	I
and	O
m1	O
is	O
m0	O
augmented	O
with	O
one	O
additional	O
covariate	O
(	O
so	O
d	O
d	O
1	O
)	O
.	O
let	O
z	O
be	O
a	O
standard	O
z-value	O
for	O
testing	B
the	O
hypothesis	O
that	O
m1	O
is	O
no	O
improvement	O
over	O
m0	O
,	O
(	O
13.46	O
)	O
table	O
13.5	O
shows	O
bbic.x/	O
as	O
a	O
function	B
of	O
z	O
and	O
n.	O
at	O
n	O
d	O
15	O
fisher	O
’	O
s	O
and	O
jeffreys	O
’	O
scales	O
give	O
roughly	O
similar	O
assessments	O
of	O
the	O
evidence	O
against	O
m0	O
(	O
though	O
jeffreys	O
’	O
nomenclature	O
is	O
more	O
conservative	O
)	O
.	O
at	O
the	O
other	O
end	O
of	O
the	O
table	O
,	O
at	O
n	O
d	O
10	O
;	O
000	O
,	O
the	O
inferences	O
are	O
contradictory	O
:	O
z	O
d	O
3:29	O
,	O
with	O
p-value	B
0.001	O
and	O
coverage	O
level	O
0.999	O
,	O
is	O
overwhelming	O
evidence	O
for	O
m1	O
on	O
fisher	O
’	O
s	O
scale	B
,	O
but	O
barely	O
worthwhile	O
for	O
jeffreys	O
’	O
.	O
bayesian	O
coherency	O
,	O
the	O
axiom	O
that	O
inferences	O
should	O
be	O
consistent	O
over	O
related	O
sit-	O
uations	O
,	O
lies	O
behind	O
the	O
contradiction	O
.	O
suppose	O
n	O
d	O
1	O
in	O
the	O
simple	O
normal	B
model	O
(	O
13.29	O
)	O
–	O
(	O
13.30	O
)	O
.	O
that	O
is	O
,	O
we	O
observe	O
only	O
the	O
single	O
variable	O
and	O
wish	O
to	O
decide	O
between	O
m0	O
w	O
(	O
cid:22	O
)	O
d	O
0	O
and	O
m1	O
w	O
(	O
cid:22	O
)	O
¤	O
0.	O
let	O
g.1/	O
denote	O
our	O
m1	O
prior	B
density	O
(	O
13.32	O
)	O
for	O
this	O
situation	O
.	O
1	O
.	O
(	O
cid:22	O
)	O
/	O
(	O
13.47	O
)	O
x	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
;	O
1/	O
;	O
248	O
objective	O
bayes	O
inference	B
and	O
mcmc	O
table	O
13.5	O
bic	O
bayes	O
factors	O
corresponding	O
to	O
z-values	O
for	O
testing	B
one	O
additional	O
covariate	O
;	O
coverage	O
value	O
(	O
1	O
minus	O
the	O
signiﬁcance	O
level	O
)	O
of	O
a	O
two-sided	O
hypothesis	O
test	O
as	O
interpreted	O
by	O
fisher	O
’	O
s	O
scale	B
of	O
evidence	O
,	O
right	O
.	O
jeffreys	O
’	O
scale	B
of	O
evidence	O
,	O
table	O
13.3	O
,	O
is	O
in	O
rough	O
agreement	O
with	O
fisher	O
for	O
n	O
d	O
15	O
,	O
but	O
favors	O
the	O
null	O
much	O
more	O
strongly	B
for	O
larger	O
sample	B
sizes	O
.	O
n	O
cover	O
z-value	O
15	O
50	O
250	O
1000	O
2500	O
5000	O
10000	O
fisher	O
.80	O
.90	O
.95	O
.975	O
.99	O
.995	O
.999	O
1.28	O
1.64	O
1.96	O
2.24	O
2.58	O
2.81	O
3.29	O
.32	O
.55	O
.97	O
1.74	O
3.90	O
7.27	O
.14	O
.59	O
.24	O
1.00	O
.43	O
1.76	O
.78	O
3.18	O
1.74	O
7.12	O
13.27	O
3.25	O
57.96	O
31.75	O
14.20	O
.07	O
.12	O
.22	O
.39	O
.87	O
1.63	O
7.10	O
.05	O
.08	O
.14	O
.25	O
.55	O
1.03	O
4.49	O
.03	O
.05	O
.10	O
.17	O
.39	O
.73	O
3.17	O
.02	O
null	O
.04	O
borderline	O
.07	O
moderate	O
.12	O
substantial	O
.28	O
strong	O
.51	O
very	O
strong	O
2.24	O
overwhelming	O
n.p	O
xi	O
=n/	O
and	O
(	O
cid:22	O
)	O
.n/	O
d	O
p	O
p	O
the	O
case	O
n	O
>	O
1	O
in	O
(	O
13.29	O
)	O
is	O
logically	O
identical	O
to	O
(	O
13.47	O
)	O
.	O
letting	O
x.n/	O
d	O
(	O
cid:16	O
)	O
n	O
(	O
cid:22	O
)	O
gives	O
x.n/	O
(	O
cid:24	O
)	O
n	O
	O
(	O
cid:22	O
)	O
.n/	O
;	O
1	O
;	O
(	O
13.48	O
)	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
ıp	O
n	O
(	O
cid:1	O
)	O
ıp	O
with	O
(	O
13.30	O
)	O
becoming	O
m0	O
w	O
(	O
cid:22	O
)	O
.n/	O
d	O
0	O
and	O
m1	O
w	O
(	O
cid:22	O
)	O
.n/	O
¤	O
0.	O
coherency	O
p	O
requires	O
that	O
(	O
cid:22	O
)	O
.n/	O
in	O
(	O
13.48	O
)	O
have	O
the	O
same	O
m1	O
prior	B
as	O
(	O
cid:22	O
)	O
in	O
(	O
13.47	O
)	O
.	O
since	O
(	O
cid:22	O
)	O
d	O
(	O
cid:22	O
)	O
.n/=	O
1	O
.	O
(	O
cid:22	O
)	O
/	O
,	O
the	O
m1	O
prior	B
for	O
sample	B
size	I
n	O
,	O
satisﬁes	O
(	O
13.49	O
)	O
n	O
,	O
this	O
implies	O
that	O
g.n/	O
1	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
g.1/	O
g.n/	O
1	O
this	O
being	O
“	O
sample	B
size	I
coherency.	O
”	O
1	O
.	O
(	O
cid:22	O
)	O
/	O
farther	O
away	O
from	O
the	O
null	O
value	O
(	O
cid:22	O
)	O
d	O
0	O
at	O
rate	B
0	O
.	O
(	O
cid:22	O
)	O
/	O
stays	O
ﬁxed	O
.	O
for	O
any	O
ﬁxed	O
value	O
of	O
the	O
sufﬁcient	O
statistic	B
x.n/	O
(	O
x.n/	O
being	O
p	O
“	O
z	O
”	O
in	O
table	O
13.5	O
)	O
,	O
this	O
results	O
in	O
the	O
bayes	O
factor	B
b.x.n//	O
decreasing	O
at	O
rate	B
1=	O
n	O
;	O
the	O
frequentist/bayesian	O
contradiction	O
seen	O
in	O
table	O
13.5	O
goes	O
beyond	O
the	O
speciﬁcs	O
of	O
the	O
bic	O
algorithm	B
.	O
the	O
effect	O
of	O
(	O
13.49	O
)	O
is	O
to	O
spread	O
the	O
m1	O
prior	B
density	O
g.n/	O
p	O
n	O
,	O
while	O
the	O
m0	O
prior	B
g.n/	O
n	O
;	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
——	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
——	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
——	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
a	O
general	O
information	B
criterion	I
takes	O
the	O
form	B
gic.m/	O
d	O
log	O
f	O
o	O
(	O
cid:22	O
)	O
.x/	O
(	O
cid:0	O
)	O
p	O
cn	O
;	O
(	O
13.50	O
)	O
13.3	O
model	B
selection	I
and	O
the	O
bic	O
249	O
where	O
cn	O
is	O
any	O
sequence	O
of	O
positive	O
numbers	O
;	O
cn	O
d	O
log.n/=2	O
for	O
bic	O
(	O
13.37	O
)	O
and	O
cn	O
d	O
1	O
for	O
aic	O
(	O
13.42	O
)	O
.	O
the	O
difference	O
	O
	O
gic.m1/	O
(	O
cid:0	O
)	O
gic.m0/	O
d	O
1	O
(	O
13.51	O
)	O
d	O
d	O
p1	O
(	O
cid:0	O
)	O
p0	O
,	O
will	O
be	O
positive	O
if	O
w	O
.x/	O
>	O
2cnd	O
.	O
for	O
d	O
d	O
1	O
,	O
as	O
in	O
table	O
13.5	O
,	O
	O
will	O
favor	O
m1	O
if	O
w	O
.x/	O
(	O
cid:21	O
)	O
2cn	O
,	O
with	O
approximate	O
probability	O
,	O
if	O
m0	O
is	O
.w	O
.x/	O
(	O
cid:0	O
)	O
2cnd	O
/	O
;	O
2	O
actually	O
true	O
,	O
(	O
13.52	O
)	O
this	O
equals	O
0.157	O
for	O
the	O
aic	O
choice	O
cn	O
d	O
1	O
;	O
for	O
bic	O
,	O
n	O
d	O
10	O
;	O
000	O
,	O
it	O
equals	O
0.0024.	O
the	O
choice	O
1	O
prf	O
(	O
cid:31	O
)	O
2	O
(	O
cid:21	O
)	O
2cng	O
:	O
cn	O
d	O
1:92	O
(	O
13.53	O
)	O
m0g	O
:	O
d	O
0:05	O
,	O
agreeing	O
with	O
the	O
usual	O
frequentist	O
0.05	O
makes	O
prf	O
>	O
0j	O
rejection	O
level	O
.	O
the	O
bic	O
is	O
consistent	O
:	O
prf	O
>	O
0g	O
goes	O
to	O
zero	O
as	O
n	O
!	O
1	O
if	O
m0	O
is	O
true	O
.	O
this	O
isn	O
’	O
t	B
true	O
of	O
(	O
13.53	O
)	O
for	O
instance	O
,	O
where	O
we	O
will	O
have	O
prf	O
>	O
0g	O
:	O
d	O
0:05	O
no	O
matter	O
how	O
large	O
n	O
may	O
be	O
,	O
but	O
consistency	O
is	O
seldom	O
compelling	O
as	O
a	O
practical	O
argument	B
.	O
conﬁdence	B
intervals	I
help	O
compensate	O
for	O
possible	O
frequentist	O
overﬁt-	O
ting	O
.	O
with	O
z	O
d	O
3:29	O
and	O
n	O
d	O
10	O
;	O
000	O
,	O
the	O
95	O
%	O
conﬁdence	B
interval	I
for	O
(	O
cid:22	O
)	O
in	O
model	B
m1	O
(	O
13.30	O
)	O
is	O
.0:013	O
;	O
0:053/	O
.	O
whether	O
or	O
not	O
such	O
a	O
small	O
effect	O
is	O
interesting	O
depends	O
on	O
the	O
scientiﬁc	O
context	O
.	O
the	O
fact	O
that	O
bic	O
says	O
“	O
not	O
interesting	O
”	O
speaks	O
to	O
its	O
inherent	O
small-model	O
bias	O
.	O
the	O
prostate	B
cancer	O
study	O
data	B
of	O
section	O
3.3	O
provides	O
a	O
more	O
challeng-	O
ing	O
model	B
selection	I
problem	O
.	O
figure	O
3.4	O
shows	O
the	O
histogram	O
of	O
n	O
d	O
6033	O
observations	O
xi	O
,	O
each	O
measuring	O
the	O
effects	O
of	O
one	O
gene	O
.	O
the	O
histogram	O
has	O
49	O
bins	O
,	O
each	O
of	O
width	O
0.2	O
,	O
with	O
centers	O
cj	O
ranging	O
from	O
(	O
cid:0	O
)	O
4:4	O
to	O
5.2	O
;	O
yj	O
,	O
the	O
height	O
of	O
the	O
histogram	O
at	O
cj	O
,	O
is	O
the	O
number	O
of	O
xi	O
in	O
bin	O
j	O
,	O
yj	O
d	O
#	O
fxi	O
2	O
bin	O
jg	O
for	O
j	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
49	O
:	O
(	O
13.54	O
)	O
we	O
assume	O
that	O
the	O
yj	O
follow	O
a	O
poisson	O
regression	B
model	I
as	O
in	O
sec-	O
tion	O
8.3	O
,	O
ind	O
(	O
cid:24	O
)	O
poi	O
.	O
(	O
cid:23	O
)	O
j	O
/	O
;	O
yj	O
j	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
49	O
;	O
(	O
13.55	O
)	O
and	O
wish	O
to	O
ﬁt	O
a	O
log	O
polynomial	O
glm	O
model	B
to	O
the	O
(	O
cid:23	O
)	O
j	O
.	O
the	O
model	B
selection	I
question	O
is	O
“	O
what	O
degree	O
polynomial	O
?	O
”	O
degree	O
2	O
corresponds	O
to	O
normal	B
densities	O
,	O
but	O
the	O
long	O
tails	O
seen	O
in	O
figure	O
3.4	O
suggest	O
otherwise	O
.	O
models	B
of	O
degree	O
2	O
through	O
8	O
are	O
assessed	O
in	O
figure	O
13.4.	O
four	O
model	B
selection	I
measures	O
are	O
compared	O
:	O
aic	O
(	O
13.42	O
)	O
;	O
bic	O
(	O
13.37	O
)	O
with	O
n	O
d	O
49	O
,	O
250	O
objective	O
bayes	O
inference	B
and	O
mcmc	O
figure	O
13.4	O
log	O
polynomial	O
models	B
of	O
degree	O
2	O
through	O
8	O
applied	O
to	O
the	O
prostate	B
study	O
histogram	O
of	O
figure	O
3.4.	O
model	B
selection	I
criteria	O
:	O
aic	O
(	O
13.42	O
)	O
;	O
bic	O
(	O
13.37	O
)	O
with	O
n	O
d	O
49	O
,	O
number	O
of	O
bins	O
,	O
or	O
6033	O
,	O
number	O
of	O
genes	O
;	O
gic	O
(	O
13.50	O
)	O
using	O
classic	O
fisher	O
hypothesis	O
choice	O
cn	O
d	O
1:92.	O
all	O
four	O
selected	O
the	O
fourth-degree	O
model	B
as	O
best	O
.	O
the	O
number	O
of	O
yj	O
values	O
(	O
bins	O
)	O
,	O
and	O
also	O
n	O
d	O
6033	O
,	O
the	O
number	O
of	O
genes	O
;	O
and	O
gic	O
(	O
13.50	O
)	O
,	O
with	O
cn	O
d	O
1:92	O
(	O
13.53	O
)	O
,	O
the	O
choice	O
based	O
on	O
classic	O
fish-	O
erian	O
hypothesis	B
testing	I
.	O
(	O
this	O
is	O
almost	O
the	O
same	O
as	O
bic	O
n	O
d	O
49	O
,	O
since	O
log.49/=2	O
d	O
1:95	O
.	O
)	O
a	O
fourth-degree	O
polynomial	O
model	B
was	O
the	O
winner	O
under	O
all	O
four	O
criteria	B
.	O
the	O
“	O
untethered	O
”	O
criticism	O
made	O
against	O
objective	O
bayes	O
methods	O
in	O
general	O
is	O
particularly	O
applicable	O
to	O
bic	O
.	O
the	O
concept	O
of	O
“	O
sample	B
size	I
”	O
is	O
not	O
well	O
deﬁned	O
,	O
as	O
the	O
prostate	B
study	O
example	O
shows	O
.	O
sample	B
size	I
co-	O
herency	O
(	O
13.49	O
)	O
,	O
the	O
rationale	O
for	O
bic	O
’	O
s	O
strong	O
bias	O
toward	O
smaller	O
models	B
,	O
is	O
less	O
convincing	O
in	O
the	O
absence	O
of	O
priors	B
based	O
on	O
genuine	O
experience	O
(	O
es-	O
pecially	O
if	O
there	O
is	O
no	O
prospect	O
of	O
the	O
sample	B
size	I
changing	O
)	O
.	O
whatever	O
its	O
vulnerabilities	O
,	O
bic	O
model	B
selection	I
has	O
nevertheless	O
become	O
a	O
mainstay	O
of	O
objective	O
bayes	O
model	B
selection	I
,	O
not	O
least	O
because	O
of	O
its	O
freedom	O
from	O
the	O
choice	O
of	O
bayesian	O
priors	B
.	O
2345678−120−100−80−60−40model	O
polynomial	O
degreeaic	O
and	O
bicbic	O
6033bic	O
49	O
and	O
gicaic	O
13.4	O
gibbs	O
sampling	O
and	O
mcmc	O
251	O
13.4	O
gibbs	O
sampling	O
and	O
mcmc	O
miraculously	O
blessed	O
with	O
visions	O
of	O
the	O
future	O
,	O
a	O
bayesian	O
statistician	O
of	O
the	O
1970s	O
would	O
certainly	O
be	O
pleased	O
with	O
the	O
prevalence	O
of	O
bayes	O
method-	O
ology	O
in	O
twenty-ﬁrst-century	O
applications	O
.	O
but	O
his	O
pleasure	O
might	O
be	O
tinged	O
with	O
surprise	O
that	O
the	O
applications	O
were	O
mostly	O
of	O
the	O
objective	O
,	O
“	O
uninfor-	O
mative	O
”	O
type	O
,	O
rather	O
than	O
taken	O
from	O
the	O
elegant	O
de	O
finetti–savage	O
school	O
of	O
subjective	O
inference	B
.	O
the	O
increase	O
in	O
bayesian	O
applications	O
,	O
and	O
the	O
change	O
in	O
emphasis	O
from	O
subjective	O
to	O
objective	O
,	O
had	O
more	O
to	O
do	O
with	O
computation	O
than	O
philoso-	O
phy	O
.	O
better	O
computers	O
and	O
algorithms	O
facilitated	O
the	O
calculation	O
of	O
formerly	O
intractable	O
bayes	O
posterior	O
distributions	O
.	O
technology	O
determines	O
practice	O
,	O
and	O
the	O
powerful	O
new	O
algorithms	O
encouraged	O
bayesian	O
analyses	O
of	O
large	O
and	O
complicated	O
models	B
where	O
subjective	O
priors	B
(	O
or	O
those	O
based	O
on	O
actual	O
past	O
experience	O
)	O
were	O
hard	O
to	O
come	O
by	O
.	O
add	O
in	O
the	O
fact	O
that	O
the	O
algorithms	O
worked	O
most	O
easily	O
with	O
simple	O
“	O
convenience	O
”	O
priors	B
like	O
the	O
conjugates	O
of	O
section	O
13.2	O
,	O
and	O
the	O
stage	O
was	O
set	B
for	O
an	O
objective	O
bayes	O
renaissance	O
.	O
at	O
ﬁrst	O
glance	O
it	O
’	O
s	O
hard	O
to	O
see	O
why	O
bayesian	O
computations	B
should	O
be	O
daunting	O
.	O
from	O
parameter	O
vector	B
	O
,	O
data	B
x	O
,	O
density	B
function	O
f	O
.x/	O
,	O
and	O
prior	O
density	B
g./	O
,	O
bayes	O
’	O
rule	B
(	O
3.5	O
)	O
–	O
(	O
3.6	O
)	O
directly	O
produces	O
the	O
posterior	O
density	O
g.jx/	O
d	O
g./f	O
.x/=f	O
.x/	O
;	O
where	O
f	O
.x/	O
is	O
the	O
marginal	O
density	B
f	O
.x/	O
dz	O
the	O
posterior	B
probability	I
of	O
any	O
set	B
a	O
in	O
the	O
parameter	O
space	B
(	O
cid:127	O
)	O
is	O
then	O
pfajxg	O
dz	O
g./f	O
.x/	O
d	O
	O
:	O
(	O
cid:127	O
)	O
(	O
cid:30	O
)	O
z	O
(	O
13.56	O
)	O
(	O
13.57	O
)	O
(	O
13.58	O
)	O
(	O
13.59	O
)	O
(	O
13.60	O
)	O
g./f	O
.x/	O
d	O
	O
g./f	O
.x/	O
d	O
	O
:	O
a	O
(	O
cid:127	O
)	O
this	O
is	O
easy	O
to	O
write	O
down	O
but	O
usually	O
difﬁcult	O
to	O
evaluate	O
if	O
	O
is	O
multidi-	O
mensional	O
.	O
modern	O
bayes	O
methods	O
attack	O
the	O
problem	O
through	O
the	O
application	O
of	O
computer	O
power	O
.	O
even	O
if	O
we	O
can	O
’	O
t	B
integrate	O
g.jx/	O
,	O
perhaps	O
we	O
can	O
sample	B
from	O
it	O
.	O
if	O
so	O
,	O
a	O
sufﬁciently	O
large	O
sample	B
,	O
say	O
would	O
provide	O
estimates	O
	O
.1/	O
;	O
	O
.2/	O
;	O
:	O
:	O
:	O
;	O
	O
.b/	O
(	O
cid:24	O
)	O
g.jx/	O
n	O
o	O
.	O
	O
.j	O
/	O
2	O
a	O
opfajxg	O
d	O
#	O
b	O
;	O
252	O
objective	O
bayes	O
inference	B
and	O
mcmc	O
and	O
similarly	O
for	O
posterior	O
moments	O
,	O
correlations	O
,	O
etc	O
.	O
we	O
would	O
in	O
this	O
way	O
be	O
employing	O
the	O
same	O
general	O
tactic	O
as	O
the	O
bootstrap	O
,	O
applied	O
now	O
for	O
bayesian	O
rather	O
than	O
frequentist	O
purposes—toward	O
the	O
same	O
goal	O
as	O
the	O
bootstrap	O
,	O
of	O
freeing	O
practical	O
applications	O
from	O
the	O
constraints	O
of	O
mathe-	O
matical	O
tractability	O
.	O
the	O
two	O
most	O
popular	O
computational	O
methods,6	O
gibbs	O
sampling	O
and	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
,	O
are	O
based	O
on	O
markov	O
chain	O
algo-	O
rithms	O
;	O
that	O
is	O
,	O
the	O
posterior	O
samples	O
	O
.b/	O
are	O
produced	O
in	O
sequence	O
,	O
each	O
one	O
depending	O
only	O
on	O
	O
.b	O
(	O
cid:0	O
)	O
1/	O
and	O
not	O
on	O
its	O
more	O
distant	O
predecessors	O
.	O
we	O
begin	O
with	O
gibbs	O
sampling	O
.	O
the	O
central	O
idea	O
of	O
gibbs	O
sampling	O
is	O
to	O
reduce	O
the	O
generation	O
of	O
mul-	O
tidimensional	O
vectors	O
	O
d	O
.1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
k/	O
to	O
a	O
series	O
of	O
univariate	O
calcu-	O
lations	O
.	O
let	O
.k/	O
denote	O
	O
with	O
component	O
k	O
removed	O
,	O
and	O
g.k/	O
the	O
condi-	O
tional	O
density	B
of	O
k	O
given	O
.k/	O
and	O
the	O
data	B
x	O
,	O
kj.k/	O
;	O
x	O
(	O
cid:24	O
)	O
g.k/	O
kj.k/	O
;	O
x	O
(	O
13.61	O
)	O
(	O
cid:16	O
)	O
	O
:	O
ˇˇ	O
.b	O
(	O
cid:0	O
)	O
1/	O
.k/	O
	O
the	O
algorithm	B
begins	O
at	O
some	O
arbitrary	O
initial	O
value	O
	O
.0/	O
.	O
having	O
computed	O
	O
.1/	O
,	O
	O
.2/	O
,	O
:	O
:	O
:	O
,	O
	O
.b	O
(	O
cid:0	O
)	O
1/	O
,	O
the	O
components	O
of	O
	O
.b/	O
are	O
generated	O
according	O
to	O
conditional	O
distributions	O
(	O
13.61	O
)	O
,	O
(	O
cid:24	O
)	O
g.k/	O
for	O
k	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
k	O
:	O
k	O
;	O
x	O
	O
.b/	O
k	O
(	O
13.62	O
)	O
as	O
an	O
example	O
,	O
we	O
take	O
x	O
to	O
be	O
the	O
n	O
d	O
20	O
observations	O
for	O
y	O
d	O
1	O
in	O
the	O
vasoconstriction	B
data	O
of	O
table	O
13.2	O
,	O
and	O
assume	O
that	O
these	O
are	O
a	O
normal	B
sample	O
,	O
xi	O
(	O
13.63	O
)	O
the	O
sufﬁcient	O
statistics	B
for	O
estimating	O
the	O
bivariate	O
parameter	O
	O
d	O
.	O
(	O
cid:22	O
)	O
;	O
(	O
cid:28	O
)	O
/	O
are	O
the	O
sample	B
mean	O
and	O
variance	O
n	O
.	O
(	O
cid:22	O
)	O
;	O
(	O
cid:28	O
)	O
/	O
;	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
d	O
20	O
:	O
(	O
cid:16	O
)	O
iid	O
(	O
cid:24	O
)	O
(	O
13.64	O
)	O
(	O
13.65	O
)	O
nx	O
d	O
nx	O
xi	O
=n	O
and	O
t	O
d	O
nx	O
1	O
1	O
.xi	O
(	O
cid:0	O
)	O
nx/2=.n	O
(	O
cid:0	O
)	O
1/	O
;	O
having	O
independent	O
normal	B
and	O
gamma	B
distributions	O
,	O
nx	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
;	O
(	O
cid:28	O
)	O
=n/	O
and	O
t	O
(	O
cid:24	O
)	O
(	O
cid:28	O
)	O
g	O
(	O
cid:23	O
)	O
=	O
(	O
cid:23	O
)	O
;	O
with	O
(	O
cid:23	O
)	O
d	O
n	O
(	O
cid:0	O
)	O
1	O
2	O
,	O
the	O
latter	O
being	O
gam	O
.	O
(	O
cid:23	O
)	O
;	O
(	O
cid:28	O
)	O
=	O
(	O
cid:23	O
)	O
/	O
in	O
the	O
notation	O
of	O
table	O
5.1	O
.	O
6	O
the	O
two	O
methods	O
are	O
often	O
referred	O
to	O
collectively	O
as	O
mcmc	O
because	O
of	O
mathematical	O
connections	O
,	O
with	O
“	O
metropolis-hasting	O
algorithm	B
”	O
referring	O
to	O
the	O
second	O
type	O
of	O
procedure	O
.	O
13.4	O
gibbs	O
sampling	O
and	O
mcmc	O
253	O
(	O
cid:28	O
)	O
(	O
cid:24	O
)	O
k1	O
(	O
cid:28	O
)	O
1=gk1c1	O
for	O
our	O
bayes	O
prior	B
distribution	I
we	O
take	O
the	O
conjugates	O
n	O
.	O
(	O
cid:22	O
)	O
0	O
;	O
(	O
cid:28	O
)	O
=n0/	O
:	O
(	O
13.66	O
)	O
1	O
/	O
d	O
in	O
terms	O
of	O
table	O
13.1	O
,	O
.x0	O
;	O
n0	O
(	O
cid:23	O
)	O
/	O
d	O
.	O
(	O
cid:28	O
)	O
1	O
;	O
k1/	O
for	O
the	O
gamma	B
,	O
while	O
.x0	O
;	O
(	O
cid:27	O
)	O
2	O
.	O
(	O
cid:22	O
)	O
0	O
;	O
(	O
cid:28	O
)	O
/	O
for	O
the	O
normal	B
.	O
(	O
a	O
simple	O
speciﬁcation	O
would	O
take	O
(	O
cid:22	O
)	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
0	O
;	O
(	O
cid:28	O
)	O
1=n0/	O
.	O
)	O
multiplying	O
the	O
normal	B
and	O
gamma	B
functional	O
forms	O
in	O
table	O
5.1	O
yields	O
and	O
(	O
cid:22	O
)	O
j	O
(	O
cid:28	O
)	O
(	O
cid:24	O
)	O
density	B
function	O
f	O
(	O
cid:22	O
)	O
;	O
(	O
cid:28	O
)	O
.nx	O
;	O
t	B
/	O
d	O
c	O
(	O
cid:28	O
)	O
(	O
cid:0	O
)	O
.	O
(	O
cid:23	O
)	O
c	O
1	O
2	O
/	O
exp	O
and	O
prior	O
density	B
g.	O
(	O
cid:22	O
)	O
;	O
(	O
cid:28	O
)	O
/	O
d	O
c	O
(	O
cid:28	O
)	O
(	O
cid:0	O
)	O
.k1c2:5/	O
exp	O
h	B
(	O
cid:23	O
)	O
t	B
c	O
n	O
(	O
cid:26	O
)	O
(	O
cid:0	O
)	O
1	O
(	O
cid:26	O
)	O
(	O
cid:0	O
)	O
1	O
h	B
k1	O
(	O
cid:28	O
)	O
1	O
c	O
n0	O
.nx	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
/2i	O
(	O
cid:27	O
)	O
.	O
(	O
cid:22	O
)	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
0/2i	O
(	O
cid:27	O
)	O
2	O
(	O
cid:28	O
)	O
(	O
cid:28	O
)	O
2	O
(	O
13.67	O
)	O
;	O
(	O
13.68	O
)	O
c	O
indicating	O
positive	O
constants	O
that	O
do	O
not	O
affect	O
the	O
posterior	O
computations	O
.	O
the	O
posterior	O
density	O
cg	O
.	O
(	O
cid:22	O
)	O
;	O
(	O
cid:28	O
)	O
/f	O
(	O
cid:22	O
)	O
;	O
(	O
cid:28	O
)	O
.nx	O
;	O
t	B
/	O
is	O
then	O
calculated	O
to	O
be	O
g.	O
(	O
cid:22	O
)	O
;	O
(	O
cid:28	O
)	O
jnx	O
;	O
t	B
/	O
d	O
c	O
(	O
cid:28	O
)	O
where	O
q	O
d	O
.k1	O
(	O
cid:28	O
)	O
1	O
c	O
t	B
/	O
c	O
nc	O
(	O
cid:0	O
)	O
.	O
(	O
cid:23	O
)	O
ck1c3/	O
expf	O
(	O
cid:0	O
)	O
q=	O
(	O
cid:28	O
)	O
g	O
;	O
.	O
(	O
cid:22	O
)	O
(	O
cid:0	O
)	O
n	O
(	O
cid:22	O
)	O
c/2	O
c	O
n0n	O
2	O
2nc	O
.	O
(	O
cid:22	O
)	O
0	O
(	O
cid:0	O
)	O
nx/2	O
:	O
(	O
13.69	O
)	O
	O
n	O
(	O
cid:22	O
)	O
c	O
;	O
	O
here	O
nc	O
d	O
n0	O
c	O
n	O
and	O
n	O
(	O
cid:22	O
)	O
c	O
d	O
.n0	O
(	O
cid:22	O
)	O
0	O
c	O
nnx/=nc	O
.	O
in	O
order	O
to	O
make	O
use	O
of	O
gibbs	O
sampling	O
we	O
need	O
to	O
know	O
the	O
full	B
con-	O
ditional	O
distributions	O
g.	O
(	O
cid:22	O
)	O
j	O
(	O
cid:28	O
)	O
;	O
nx	O
;	O
t	B
/	O
and	O
g.	O
(	O
cid:28	O
)	O
j	O
(	O
cid:22	O
)	O
;	O
nx	O
;	O
t	B
/	O
,	O
as	O
in	O
(	O
13.62	O
)	O
.	O
(	O
in	O
this	O
case	O
,	O
k	O
d	O
2	O
,	O
1	O
d	O
(	O
cid:22	O
)	O
,	O
and	O
2	O
d	O
(	O
cid:28	O
)	O
.	O
)	O
this	O
is	O
where	O
the	O
conjugate	B
expressions	O
in	O
table	O
13.1	O
come	O
into	O
play	O
.	O
inspection	O
of	O
density	B
(	O
13.69	O
)	O
shows	O
that	O
n	O
(	O
cid:28	O
)	O
nc	O
and	O
(	O
cid:28	O
)	O
j	O
(	O
cid:22	O
)	O
;	O
nx	O
;	O
t	B
(	O
cid:24	O
)	O
q	O
(	O
cid:22	O
)	O
j	O
(	O
cid:28	O
)	O
;	O
nx	O
;	O
t	B
(	O
cid:24	O
)	O
(	O
13.70	O
)	O
b	O
d	O
10	O
;	O
000	O
gibbs	O
samples	O
	O
.b/	O
d	O
.	O
(	O
cid:22	O
)	O
.b/	O
;	O
(	O
cid:28	O
)	O
.b//	O
were	O
generated	O
starting	O
from	O
	O
.0/	O
d	O
.nx	O
;	O
t	B
/	O
d	O
.116	O
;	O
554/	O
.	O
the	O
prior	B
speciﬁcations	O
were	O
chosen	O
to	O
be	O
(	O
presumably	O
)	O
uninformative	O
or	O
mildly	O
informative	O
,	O
g	O
(	O
cid:23	O
)	O
ck1c2	O
:	O
n0	O
d	O
1	O
;	O
(	O
cid:22	O
)	O
0	O
d	O
nx	O
;	O
k1	O
d	O
1	O
or	O
9:5	O
;	O
and	O
(	O
cid:28	O
)	O
1	O
d	O
t	B
:	O
(	O
13.71	O
)	O
(	O
in	O
which	O
case	O
n	O
(	O
cid:22	O
)	O
c	O
d	O
nx	O
and	O
q	O
d	O
.	O
(	O
cid:23	O
)	O
c	O
k1/t	O
c	O
nc	O
.	O
(	O
cid:22	O
)	O
(	O
cid:0	O
)	O
nx/2	O
.	O
from	O
(	O
cid:23	O
)	O
d	O
.n	O
(	O
cid:0	O
)	O
1/=2	O
,	O
we	O
see	O
that	O
k1	O
corresponds	O
to	O
about	O
2k1	O
hypothetical	O
prior	B
observations	O
.	O
)	O
the	O
resulting	O
posterior	O
distributions	O
for	O
(	O
cid:28	O
)	O
are	O
shown	O
by	O
the	O
histograms	O
in	O
figure	O
13.5.	O
as	O
a	O
point	O
of	O
frequentist	O
comparison	O
,	O
b	O
d	O
10	O
;	O
000	O
parametric	B
bootstrap	O
replications	O
(	O
which	O
involve	O
no	O
prior	B
assumptions	O
)	O
,	O
o	O
(	O
cid:28	O
)	O
d	O
t	B
;	O
(	O
cid:3	O
)	O
(	O
cid:24	O
)	O
o	O
(	O
cid:28	O
)	O
g	O
(	O
cid:23	O
)	O
=	O
(	O
cid:23	O
)	O
;	O
o	O
(	O
cid:28	O
)	O
(	O
13.72	O
)	O
254	O
objective	O
bayes	O
inference	B
and	O
mcmc	O
figure	O
13.5	O
posterior	O
distributions	O
for	O
variance	O
parameter	O
(	O
cid:28	O
)	O
,	O
model	B
(	O
13.63	O
)	O
–	O
(	O
13.65	O
)	O
,	O
volume	O
of	O
air	O
inspired	O
for	O
vasoconstriction	B
group	O
y	O
d	O
1	O
from	O
table	O
13.2.	O
solid	O
teal	O
histogram	O
:	O
b	O
d	O
10	O
;	O
000	O
gibbs	O
samples	O
with	O
k1	O
d	O
1	O
;	O
black	O
line	O
histogram	O
:	O
b	O
d	O
10	O
;	O
000	O
samples	O
with	O
k1	O
d	O
9:5	O
;	O
red	O
line	O
histogram	O
:	O
10,000	O
parametric	B
bootstrap	O
samples	O
(	O
13.72	O
)	O
suggests	O
even	O
the	O
k1	O
d	O
1	O
prior	B
has	O
substantial	O
posterior	O
effect	O
.	O
are	O
seen	O
to	O
be	O
noticeably	O
more	O
dispersed	O
than	O
even	O
the	O
k1	O
d	O
1	O
bayes	O
posterior	B
distribution	I
,	O
the	O
likely	O
choice	O
for	O
an	O
objective	O
bayes	O
analysis	B
.	O
bayes	O
techniques	O
,	O
even	O
objective	O
ones	O
,	O
have	O
regularization	B
effects	O
that	O
may	O
or	O
may	O
not	O
be	O
appropriate	O
.	O
a	O
similar	O
,	O
independent	O
gibbs	O
sample	B
of	O
size	O
10,000	O
was	O
obtained	O
for	O
the	O
19	O
y	O
d	O
0	O
vasoconstriction	B
measurements	O
in	O
table	O
13.2	O
,	O
with	O
speciﬁcations	O
as	O
in	O
(	O
13.71	O
)	O
,	O
k	O
d	O
1.	O
let	O
(	O
cid:16	O
)	O
1=2	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
.b/	O
c	O
(	O
cid:28	O
)	O
.b/	O
0	O
ı.b/	O
d	O
(	O
cid:22	O
)	O
.b/	O
(	O
cid:28	O
)	O
.b/	O
1	O
1	O
0	O
;	O
(	O
13.73	O
)	O
where	O
.	O
(	O
cid:22	O
)	O
.b/	O
y	O
d	O
1	O
and	O
y	O
d	O
0	O
runs	O
.	O
1	O
;	O
(	O
cid:28	O
)	O
.b/	O
1	O
/	O
and	O
.	O
(	O
cid:22	O
)	O
.b/	O
0	O
;	O
(	O
cid:28	O
)	O
.b/	O
0	O
/	O
denote	O
the	O
bth	O
gibbs	O
samples	O
from	O
the	O
figure	O
13.6	O
shows	O
the	O
posterior	B
distribution	I
of	O
ı.	O
twenty-eight	O
of	O
the	O
tfrequency200400600800100012000200400600800100012001400tk1=1k1=9.5parametricbootstrap	O
13.4	O
gibbs	O
sampling	O
and	O
mcmc	O
255	O
figure	O
13.6	O
b	O
d10,000	O
gibbs	O
samples	O
for	O
“	O
bayes	O
t-statistic	B
”	O
(	O
13.73	O
)	O
comparing	O
y	O
d	O
1	O
with	O
y	O
d	O
0	O
values	O
for	O
vasoconstriction	B
data	O
.	O
b	O
d10,000	O
values	O
ı.b/	O
were	O
less	O
than	O
0	O
,	O
giving	O
a	O
“	O
bayesian	O
t-test	O
”	O
estimate	B
(	O
13.74	O
)	O
pfı	O
<	O
0jnx1	O
;	O
nx0	O
;	O
t1	O
;	O
t0g	O
d	O
0:0028	O
:	O
(	O
the	O
usual	O
t-test	O
yielded	O
one-sided	O
p-value	B
0.0047	O
against	O
the	O
null	O
hy-	O
pothesis	O
(	O
cid:22	O
)	O
0	O
d	O
(	O
cid:22	O
)	O
1	O
.	O
)	O
an	O
appealing	O
feature	O
of	O
gibbs	O
sampling	O
is	O
that	O
having	O
obtained	O
	O
.1/	O
;	O
	O
.2/	O
;	O
:	O
:	O
:	O
;	O
	O
.b/	O
(	O
13.59	O
)	O
the	O
posterior	B
distribution	I
of	O
any	O
pa-	O
rameter	O
(	O
cid:13	O
)	O
d	O
t	B
./	O
is	O
obtained	O
directly	O
from	O
the	O
b	O
values	O
(	O
cid:13	O
)	O
.b/	O
d	O
t	B
.	O
.b//	O
.	O
gibbs	O
sampling	O
requires	O
the	O
ability	O
to	O
sample	B
from	O
the	O
full	B
conditional	O
distributions	O
(	O
13.61	O
)	O
.	O
a	O
more	O
general	O
markov	O
chain	O
monte	O
carlo	O
method	B
,	O
commonly	O
referred	O
to	O
as	O
mcmc	O
,	O
makes	O
clearer	O
the	O
basic	O
idea	O
.	O
suppose	O
the	O
space	B
of	O
possible	O
	O
values	O
is	O
ﬁnite	O
,	O
say	O
f.1/	O
;	O
.2/	O
;	O
:	O
:	O
:	O
,	O
.m	O
/g	O
,	O
and	O
we	O
wish	O
to	O
simulate	O
samples	O
from	O
a	O
posterior	B
distribution	I
putting	O
probability	O
p.i	O
/	O
on	O
.i	O
/	O
,	O
p	O
d	O
.p.1/	O
;	O
p.2/	O
;	O
:	O
:	O
:	O
;	O
p.m	O
//	O
:	O
(	O
13.75	O
)	O
the	O
mcmc	O
algorithm	B
begins	O
with	O
the	O
choice	O
of	O
a	O
“	O
candidate	O
”	O
proba-	O
bility	O
distribution	B
q.i	O
;	O
j	O
/	O
for	O
moving	O
from	O
.i	O
/	O
to	O
.j	O
/	O
;	O
in	O
theory	B
q.i	O
;	O
j	O
/	O
can	O
be	O
almost	O
anything	O
,	O
for	O
instance	O
q.i	O
;	O
j	O
/	O
d	O
1=.m	O
(	O
cid:0	O
)	O
1/	O
for	O
j	O
¤	O
i.	O
the	O
simulated	O
samples	O
	O
.b/	O
are	O
obtained	O
by	O
a	O
random	O
walk	O
:	O
if	O
	O
.b/	O
equals	O
.i	O
/	O
,	O
dfrequency0.00.51.01.50100200300400500600pr	O
(	O
d	O
<	O
0	O
)	O
=0.0028	O
objective	O
bayes	O
inference	B
and	O
mcmc	O
256	O
then	O
	O
.bc1/	O
equals	O
.j	O
/	O
with	O
probability7	O
q.i	O
;	O
j	O
/	O
d	O
q.i	O
;	O
j	O
/	O
(	O
cid:1	O
)	O
min	O
(	O
cid:26	O
)	O
p.j	O
/q.j	O
;	O
i	O
/	O
p.i	O
/q.i	O
;	O
j	O
/	O
(	O
cid:27	O
)	O
;	O
1	O
for	O
j	O
¤	O
i	O
,	O
while	O
with	O
probability	O
q.i	O
;	O
i	O
/	O
d	O
1	O
(	O
cid:0	O
)	O
x	O
j¤i	O
q.i	O
;	O
j	O
/	O
(	O
13.77	O
)	O
	O
.bc1/	O
d	O
	O
.b/	O
d	O
.i	O
/	O
.	O
markov	O
chain	O
theory	B
then	O
says	O
that	O
,	O
under	O
quite	O
general	O
conditions	B
,	O
the	O
empirical	B
distribution	O
of	O
the	O
random	O
walk	O
values	O
	O
.b/	O
will	O
approach	O
the	O
desired	O
distribution	B
p	O
as	O
b	O
gets	O
large	O
.	O
a	O
heuristic	O
argument	B
for	O
why	O
this	O
happens	O
begins	O
by	O
supposing	O
that	O
	O
.1/	O
was	O
in	O
fact	O
generated	O
by	O
sampling	O
from	O
the	O
target	O
distribution	B
p	O
,	O
prf	O
.1/	O
d	O
ig	O
d	O
p.i	O
/	O
,	O
and	O
then	O
	O
.2/	O
was	O
obtained	O
according	O
to	O
transition	O
probabilities	B
(	O
13.76	O
)	O
–	O
(	O
13.77	O
)	O
.	O
a	O
little	O
algebra	O
shows	O
that	O
(	O
13.76	O
)	O
implies	O
(	O
13.76	O
)	O
(	O
13.78	O
)	O
(	O
13.79	O
)	O
the	O
so-called	O
balance	O
equations	O
.	O
this	O
results	O
in	O
n	O
	O
.2/	O
d	O
i	O
pr	O
p.i	O
/q.i	O
;	O
j	O
/	O
d	O
p.j	O
/q.j	O
;	O
i	O
/	O
;	O
o	O
d	O
p.i	O
/q.i	O
;	O
i	O
/	O
cx	O
p.j	O
/q.j	O
;	O
i	O
/	O
j¤i	O
d	O
p.i	O
/	O
q.i	O
;	O
j	O
/	O
d	O
p.i	O
/	O
:	O
mx	O
jd1	O
in	O
other	O
words	O
,	O
if	O
	O
.1/	O
has	O
distribution	B
p	O
then	O
so	O
will	O
	O
.2/	O
,	O
and	O
like-	O
wise	O
	O
.3/	O
;	O
	O
.4/	O
;	O
:	O
:	O
:	O
;	O
p	O
is	O
the	O
equilibrium	O
distribution	B
of	O
the	O
markov	O
chain	O
random	O
walk	O
deﬁned	O
by	O
transition	O
probabilities	B
q.	O
under	O
reasonable	O
con-	O
ditions	O
,	O
	O
	O
.b/	O
must	O
asymptotically	O
attain	O
distribution	B
p	O
no	O
matter	O
how	O
	O
.1/	O
is	O
initially	O
selected	O
.	O
9	O
13.5	O
example	O
:	O
modeling	O
population	O
admixture	O
mcmc	O
has	O
had	O
a	O
big	O
impact	O
in	O
statistical	O
genetics	O
,	O
where	O
bayesian	O
mod-	O
eling	O
is	O
popular	O
and	O
useful	O
for	O
representing	O
the	O
complex	O
evolutionary	O
pro-	O
cesses	O
.	O
here	O
we	O
illustrate	O
its	O
use	O
in	O
demography	O
and	O
modeling	O
admixture—	O
estimating	O
the	O
contributions	O
from	O
ancestral	O
populations	O
in	O
an	O
individual	O
7	O
in	O
bayes	O
applications	O
,	O
p.i	O
/	O
d	O
g..i	O
/jx/	O
d	O
g..i	O
//f.i	O
/.x/=f	O
.x/	O
(	O
13.56	O
)	O
.	O
however	O
,	O
f	O
.x/	O
is	O
not	O
needed	O
since	O
it	O
cancels	O
out	O
of	O
(	O
13.76	O
)	O
,	O
a	O
considerable	O
advantage	O
in	O
complicated	O
situations	O
when	O
f	O
.x/	O
is	O
often	O
unavailable	O
,	O
and	O
a	O
prime	O
reason	O
for	O
the	O
popularity	O
of	O
mcmc	O
.	O
13.5	O
example	O
:	O
modeling	O
population	O
admixture	O
257	O
genome	O
.	O
for	O
example	O
,	O
we	O
might	O
consider	O
human	B
ancestry	I
,	O
and	O
for	O
each	O
individual	O
wish	O
to	O
estimate	B
the	O
proportion	B
of	O
their	O
genome	O
coming	O
from	O
european	O
,	O
african	O
,	O
and	O
asian	O
origins	O
.	O
the	O
procedure	O
we	O
describe	O
here	O
is	O
unsupervised—a	O
type	O
of	O
soft	O
clustering—but	O
we	O
will	O
see	O
it	O
can	O
be	O
very	O
informative	O
with	O
regard	O
to	O
such	O
questions	O
.	O
we	O
have	O
a	O
sample	B
of	O
n	O
in-	O
dividuals	O
,	O
and	O
we	O
assume	O
each	O
arose	O
from	O
possible	O
admixture	O
among	O
j	O
parent	O
populations	O
,	O
each	O
with	O
their	O
own	O
characteristic	O
vector	B
of	O
allele	O
fre-	O
quencies	O
.	O
for	O
us	O
j	O
d	O
3	O
,	O
and	O
let	O
qi	O
2	O
s3	O
denote	O
a	O
probability	O
vector	B
for	O
in-	O
dividual	O
i	O
representing	O
the	O
proportions	O
of	O
their	O
heritage	O
coming	O
from	O
pop-	O
ulations	O
j	O
2	O
f1	O
;	O
2	O
;	O
3g	O
(	O
see	O
section	O
5.4	O
)	O
.	O
we	O
have	O
genomic	O
measurements	O
for	O
each	O
individual	O
,	O
in	O
our	O
case	O
snps	O
(	O
single-nucleotide	O
polymorphisms	O
)	O
at	O
each	O
of	O
m	O
well-spaced	O
loci	O
,	O
and	O
hence	O
can	O
assume	O
they	O
are	O
in	O
linkage	O
equilibrium	O
.	O
at	O
each	O
snp	O
we	O
have	O
a	O
measurement	O
that	O
identiﬁes	O
the	O
two	O
alleles	O
(	O
one	O
per	O
chromosome	O
)	O
,	O
where	O
each	O
can	O
be	O
either	O
the	O
wild-type	O
a	O
or	O
the	O
mutation	O
a.	O
that	O
is	O
,	O
we	O
have	O
the	O
genotype	O
gi	O
m	O
at	O
snp	O
m	O
for	O
individual	O
i	O
:	O
a	O
three-level	O
factor	B
with	O
levels	O
faa	O
;	O
aa	O
;	O
aag	O
which	O
we	O
code	O
as	O
0	O
;	O
1	O
;	O
2.	O
table	O
13.6	O
shows	O
some	O
examples	O
.	O
table	O
13.6	O
a	O
subset	O
of	O
the	O
genotype	O
data	B
on	O
197	O
individuals	O
,	O
each	O
with	O
genotype	O
measurements	O
at	O
100	O
snps	O
.	O
in	O
this	O
case	O
the	O
ethnicity	O
is	O
known	O
for	O
each	O
individual	O
,	O
one	O
of	O
japanese	O
,	O
african	O
,	O
european	O
,	O
or	O
african	O
american	O
.	O
for	O
example	O
,	O
individual	O
na12239	O
has	O
genotype	O
aa	O
for	O
snp1	O
,	O
na19247	O
has	O
aa	O
,	O
and	O
na20126	O
has	O
aa	O
.	O
subject	O
snp1	O
snp2	O
snp3	O
na10852	O
na12239	O
na19072	O
na19247	O
na20126	O
na18868	O
na19257	O
na19079	O
na19067	O
na19904	O
1	O
1	O
0	O
0	O
2	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
2	O
0	O
1	O
0	O
0	O
0	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
snp97	O
snp98	O
snp99	O
snp100	O
1	O
1	O
0	O
0	O
2	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
2	O
0	O
1	O
0	O
0	O
0	O
1	O
let	O
pj	O
be	O
the	O
(	O
unknown	O
)	O
m	O
-vector	O
of	O
minor	O
allele	O
frequencies	O
(	O
propor-	O
tions	O
actually	O
)	O
in	O
population	O
j	O
.	O
we	O
have	O
available	O
a	O
sample	B
of	O
n	O
individ-	O
uals	O
,	O
and	O
for	O
each	O
sample	B
we	O
have	O
their	O
genomic	O
information	B
measured	O
at	O
each	O
of	O
the	O
m	O
loci	O
.	O
some	O
of	O
the	O
individuals	O
might	O
appear	O
to	O
have	O
pure	O
an-	O
cestral	O
origins	O
,	O
but	O
many	O
do	O
not	O
.	O
our	O
goal	O
is	O
to	O
estimate	B
qi	O
;	O
i	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
n	O
;	O
and	O
pj	O
;	O
j	O
2	O
f1	O
;	O
2	O
;	O
3g	O
.	O
258	O
objective	O
bayes	O
inference	B
and	O
mcmc	O
i	O
m	O
;	O
x	O
.2/	O
for	O
this	O
purpose	O
it	O
is	O
useful	O
to	O
pose	O
a	O
generative	O
model	B
.	O
we	O
ﬁrst	O
create	O
a	O
pair	O
of	O
variables	O
xi	O
m	O
d	O
.x	O
.1/	O
i	O
m	O
/	O
corresponding	O
to	O
each	O
gi	O
m	O
,	O
to	O
which	O
we	O
allocate	O
the	O
two	O
alleles	O
(	O
in	O
arbitrary	O
order	O
)	O
.	O
for	O
example	O
,	O
if	O
gi	O
m	O
d	O
1	O
d	O
1	O
(	O
or	O
(	O
corresponding	O
to	O
aa	O
)	O
,	O
then	O
we	O
might	O
set	B
x	O
.1/	O
vice	O
versa	O
)	O
.	O
if	O
gi	O
m	O
d	O
0	O
they	O
are	O
both	O
0	O
,	O
and	O
if	O
gi	O
m	O
d	O
2	O
,	O
they	O
are	O
both	O
1.	O
i	O
m	O
let	O
zi	O
m	O
2	O
f1	O
;	O
2	O
;	O
3g2	O
represent	O
the	O
ancestral	O
origin	O
for	O
individual	O
i	O
of	O
each	O
of	O
these	O
allele	O
copies	O
xi	O
m	O
at	O
locus	O
m	O
,	O
again	O
a	O
two-vector	O
with	O
elements	O
zi	O
m	O
d	O
.z.1/	O
d	O
0	O
and	O
x	O
.2/	O
i	O
m	O
i	O
m	O
/	O
.	O
then	O
our	O
generative	O
model	B
goes	O
as	O
follows	O
.	O
i	O
m	O
;	O
z.2/	O
(	O
cid:24	O
)	O
mult.1	O
;	O
qi	O
/	O
,	O
independently	O
at	O
each	O
m	O
,	O
for	O
each	O
copy	O
c	O
d	O
1	O
;	O
2.	O
that	O
is	O
,	O
we	O
select	O
the	O
ancestral	O
origin	O
of	O
each	O
chromosome	O
at	O
locus	O
m	O
according	O
to	O
the	O
individual	O
’	O
s	O
mixture	O
proportions	O
qi	O
.	O
d	O
j	O
,	O
for	O
each	O
copy	O
c	O
d	O
1	O
;	O
2.	O
what	O
this	O
means	O
is	O
that	O
,	O
for	O
each	O
of	O
the	O
two	O
ancestral	O
picks	O
at	O
locus	O
m	O
(	O
one	O
for	O
each	O
arm	O
of	O
the	O
chromosome	O
)	O
,	O
we	O
draw	O
a	O
binomial	B
with	O
the	O
appropriate	O
allele	O
frequency	O
.	O
(	O
cid:24	O
)	O
bi.1	O
;	O
pj	O
m/	O
if	O
z.c/	O
i	O
m	O
1	O
z.c/	O
i	O
m	O
2	O
x	O
.c/	O
i	O
m	O
to	O
complete	O
the	O
bayesian	O
speciﬁcation	O
,	O
we	O
need	O
to	O
supply	O
priors	B
for	O
the	O
qi	O
and	O
also	O
for	O
pj	O
m.	O
although	O
one	O
can	O
get	O
fancy	O
here	O
,	O
we	O
resort	O
to	O
the	O
recommended	O
ﬂat	O
priors	B
,	O
which	O
are	O
(	O
cid:15	O
)	O
qi	O
(	O
cid:24	O
)	O
d.	O
(	O
cid:21	O
)	O
;	O
(	O
cid:21	O
)	O
;	O
(	O
cid:21	O
)	O
/	O
,	O
a	O
ﬂat	O
three-component	O
dirichlet	O
,	O
independently	O
for	O
each	O
subject	O
i	O
	O
and	O
10	O
(	O
cid:15	O
)	O
pj	O
m	O
(	O
cid:24	O
)	O
d.	O
(	O
cid:13	O
)	O
;	O
(	O
cid:13	O
)	O
/	O
independently	O
for	O
each	O
population	O
j	O
,	O
and	O
each	O
locus	O
m	O
(	O
the	O
beta	B
distribution	O
;	O
see	O
10	O
in	O
the	O
end	O
notes	O
)	O
.	O
we	O
use	O
the	O
least-informative	O
values	O
(	O
cid:21	O
)	O
d	O
(	O
cid:13	O
)	O
d	O
1.	O
in	O
practice	O
,	O
these	O
could	O
get	O
updated	O
as	O
well	O
,	O
but	O
for	O
the	O
purposes	O
of	O
this	O
demonstration	O
we	O
leave	O
them	O
ﬁxed	O
at	O
these	O
values	O
.	O
let	O
x	O
be	O
the	O
n	O
(	O
cid:2	O
)	O
m	O
(	O
cid:2	O
)	O
2	O
array	O
of	O
observed	O
alleles	O
for	O
all	O
n	O
samples	O
.	O
we	O
wish	O
to	O
estimate	B
the	O
posterior	B
distribution	I
pr.p	O
;	O
qjx	O
/	O
,	O
referring	O
col-	O
lectively	O
to	O
all	O
the	O
elements	O
of	O
p	O
and	O
q.	O
for	O
this	O
purpose	O
we	O
use	O
gibbs	O
sampling	O
,	O
which	O
amounts	O
to	O
the	O
following	O
sequence	O
.	O
0	O
initialize	O
z.0/	O
;	O
p	O
.0/	O
;	O
q.0/	O
.	O
1	O
sample	B
z.b/	O
from	O
the	O
conditional	O
distribution	B
pr.zjx	O
;	O
p	O
.b	O
(	O
cid:0	O
)	O
1/	O
;	O
q.b	O
(	O
cid:0	O
)	O
1//	O
.	O
2	O
sample	B
p	O
.b/	O
;	O
q.b/	O
from	O
the	O
conditional	O
distribution	B
pr.p	O
;	O
qjx	O
;	O
z.b//	O
.	O
gibbs	O
is	O
effective	O
when	O
one	O
can	O
sample	B
efﬁciently	O
from	O
these	O
conditional	O
distributions	O
,	O
which	O
is	O
the	O
case	O
here	O
.	O
13.5	O
example	O
:	O
modeling	O
population	O
admixture	O
259	O
in	O
step	O
2	O
,	O
we	O
can	O
sample	B
p	O
and	O
q	O
separately	O
.	O
it	O
can	O
be	O
seen	O
that	O
for	O
each	O
.j	O
;	O
m/	O
we	O
should	O
sample	B
pj	O
m	O
from	O
pj	O
mjx	O
;	O
z	O
(	O
cid:24	O
)	O
d.	O
(	O
cid:21	O
)	O
c	O
n.0/	O
j	O
m	O
;	O
(	O
cid:21	O
)	O
c	O
n.1/	O
j	O
m/	O
;	O
d	O
#	O
f.i	O
;	O
c/	O
w	O
x	O
.c/	O
d	O
#	O
f.i	O
;	O
c/	O
w	O
x	O
.c/	O
i	O
m	O
i	O
m	O
d	O
0	O
and	O
z.c/	O
d	O
1	O
and	O
z.c/	O
i	O
m	O
i	O
m	O
d	O
jg	O
;	O
d	O
jg	O
:	O
where	O
z	O
d	O
z.b/	O
and	O
n.0/	O
j	O
m	O
n.1/	O
j	O
m	O
(	O
13.80	O
)	O
(	O
13.81	O
)	O
this	O
follows	O
from	O
the	O
conjugacy	O
of	O
the	O
two-component	O
dirichlet	O
(	O
beta	B
)	O
with	O
the	O
binomial	B
distribution	O
,	O
table	O
13.1.	O
updating	O
qi	O
involves	O
simulating	O
from	O
qijx	O
;	O
z	O
(	O
cid:24	O
)	O
d.	O
(	O
cid:13	O
)	O
c	O
mi1	O
;	O
(	O
cid:13	O
)	O
c	O
mi	O
2	O
;	O
(	O
cid:13	O
)	O
c	O
mi	O
3/	O
;	O
(	O
13.82	O
)	O
where	O
mij	O
is	O
the	O
number	O
of	O
allele	O
copies	O
in	O
individual	O
i	O
that	O
originated	O
(	O
according	O
to	O
z	O
d	O
z.b/	O
)	O
in	O
population	O
j	O
:	O
mij	O
d	O
#	O
f.c	O
;	O
m/	O
w	O
z.c/	O
i	O
m	O
d	O
jg	O
:	O
(	O
13.83	O
)	O
figure	O
13.7	O
barycentric	O
coordinate	O
plot	O
for	O
the	O
estimated	O
posterior	O
means	O
of	O
the	O
qi	O
based	O
on	O
mcmc	O
sampling	O
.	O
step	O
1	O
can	O
be	O
performed	O
by	O
simulating	O
z.c/	O
i	O
m	O
independently	O
,	O
for	O
each	O
i	O
;	O
m	O
;	O
llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllleuropeanjapaneseafricanafrican	O
american	O
260	O
objective	O
bayes	O
inference	B
and	O
mcmc	O
and	O
c	O
from	O
pr.z.c/	O
i	O
m	O
d	O
jjx	O
;	O
p	O
;	O
q/	O
d	O
qij	O
pr.x	O
.c/	O
i	O
m	O
`d1	O
qi	O
`	O
pr.x	O
.c/	O
i	O
m	O
p3	O
jp	O
;	O
z.c/	O
d	O
j	O
/	O
i	O
m	O
jp	O
;	O
z.c/	O
i	O
m	O
:	O
(	O
13.84	O
)	O
d	O
`/	O
the	O
probabilities	B
on	O
the	O
right	O
refer	O
back	O
to	O
our	O
generative	O
distribution	B
de-	O
scribed	O
earlier	O
.	O
figure	O
13.7	O
shows	O
a	O
triangle	O
plot	O
that	O
summarizes	O
the	O
result	O
of	O
running	O
the	O
mcmc	O
algorithm	B
on	O
our	O
197	O
subjects	O
.	O
we	O
used	O
a	O
burn	O
in	O
of	O
1000	O
complete	O
iterations	O
,	O
and	O
then	O
a	O
further	O
2000	O
to	O
estimate	B
the	O
distribution	B
of	O
the	O
parameters	O
of	O
interest	O
,	O
in	O
this	O
case	O
the	O
qi	O
.	O
each	O
dot	O
in	O
the	O
ﬁgure	O
represents	O
a	O
three-component	O
probability	O
vector	B
,	O
and	O
is	O
the	O
posterior	O
mean	O
of	O
the	O
sampled	O
qi	O
for	O
each	O
subject	O
.	O
the	O
points	O
are	O
colored	O
according	O
to	O
the	O
known	O
ethnicity	O
.	O
although	O
this	O
algorithm	B
is	O
unsupervised	O
,	O
we	O
see	O
that	O
the	O
ethnic	O
groups	O
cluster	O
nicely	O
in	O
the	O
corners	O
of	O
the	O
simplex	B
,	O
and	O
allow	O
us	O
to	O
identify	O
these	O
clusters	O
.	O
the	O
african	O
american	O
group	O
is	O
spread	O
between	O
the	O
african	O
and	O
european	O
clusters	O
(	O
with	O
a	O
little	O
movement	O
toward	O
the	O
japanese	O
)	O
.	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
——	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
——	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
——	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
markov	O
chain	O
methods	O
are	O
versatile	O
tools	O
that	O
have	O
proved	O
their	O
value	O
in	O
requiring	O
some	O
individual	O
ingenuity	O
with	O
each	O
application	O
.	O
bayesian	O
applications	O
.	O
there	O
are	O
some	O
drawbacks	O
.	O
(	O
cid:15	O
)	O
the	O
algorithms	O
are	O
not	O
universal	O
in	O
the	O
sense	O
of	O
maximum	B
likelihood	I
,	O
(	O
cid:15	O
)	O
as	O
a	O
result	O
,	O
applications	O
,	O
especially	O
of	O
gibbs	O
sampling	O
,	O
have	O
favored	O
a	O
small	O
set	B
of	O
convenient	O
priors	B
,	O
mainly	O
jeffreys	O
and	O
conjugates	O
,	O
that	O
simplify	O
the	O
calculations	O
.	O
this	O
can	O
cast	O
doubt	O
on	O
the	O
relevance	O
of	O
the	O
ing	O
the	O
convergence	O
of	O
estimates	O
such	O
as	O
n	O
dp	O
	O
.b/=b	O
slow	O
.	O
resulting	O
bayes	O
inferences	O
.	O
(	O
cid:15	O
)	O
successive	O
realizations	O
	O
.b/	O
are	O
highly	O
correlated	O
with	O
each	O
other	O
,	O
mak-	O
(	O
cid:15	O
)	O
the	O
correlation	O
makes	O
it	O
difﬁcult	O
to	O
assign	O
a	O
standard	B
error	I
to	O
n	O
.	O
actual	O
applications	O
ignore	O
an	O
initial	O
b0	O
of	O
the	O
	O
.b/	O
values	O
(	O
as	O
a	O
“	O
burn-in	O
”	O
period	O
)	O
and	O
go	O
on	O
to	O
large	O
enough	O
b	O
such	O
that	O
estimates	O
like	O
n	O
appear	O
to	O
settle	O
down	O
.	O
however	O
,	O
neither	O
the	O
choice	O
of	O
b0	O
nor	O
that	O
of	O
b	O
may	O
be	O
clear	O
.	O
objective	O
bayes	O
offers	O
a	O
paradigm	O
of	O
our	O
book	O
’	O
s	O
theme	O
,	O
the	O
effect	O
of	O
electronic	O
computation	O
on	O
statistical	O
inference	B
:	O
ingenious	O
new	O
algorithms	O
facilitated	O
bayesian	O
applications	O
over	O
a	O
wide	O
class	O
of	O
applied	O
problems	O
and	O
,	O
in	O
doing	O
so	O
,	O
inﬂuenced	O
the	O
dominant	O
philosophy	O
of	O
the	O
whole	O
area	O
.	O
13.6	O
notes	O
and	O
details	O
13.6	O
notes	O
and	O
details	O
261	O
the	O
books	O
by	O
savage	O
(	O
1954	O
)	O
and	O
de	O
finetti	O
(	O
1972	O
)	O
,	O
summarizing	O
his	O
ear-	O
lier	O
work	O
,	O
served	O
as	O
foundational	O
texts	O
for	O
the	O
subjective	O
bayesian	O
school	O
of	O
inference	B
.	O
highly	O
inﬂuential	O
,	O
they	O
championed	O
a	O
framework	O
for	O
bayes-	O
ian	O
applications	O
based	O
on	O
coherent	O
behavior	O
and	O
the	O
careful	O
elucidation	O
of	O
personal	O
probabilities	B
.	O
a	O
current	O
leading	O
text	O
on	O
bayesian	O
methods	O
,	O
carlin	O
and	O
louis	O
(	O
2000	O
)	O
,	O
does	O
not	O
reference	O
either	O
savage	O
or	O
de	O
finetti	O
.	O
now	O
jef-	O
freys	O
(	O
1961	O
)	O
,	O
again	O
following	O
earlier	O
works	O
,	O
claims	O
foundational	O
status	O
.	O
the	O
change	O
of	O
direction	O
has	O
not	O
gone	O
without	O
protest	O
from	O
the	O
subjectivists—	O
see	O
adrian	O
smith	O
’	O
s	O
discussion	O
of	O
o	O
’	O
hagan	O
(	O
1995	O
)	O
—but	O
is	O
nonetheless	O
al-	O
most	O
a	O
complete	O
rout	O
.	O
metropolis	O
et	O
al	O
.	O
(	O
1953	O
)	O
,	O
as	O
part	O
of	O
nuclear	O
weapons	O
research	O
,	O
devel-	O
oped	O
the	O
ﬁrst	O
mcmc	O
algorithm	B
.	O
a	O
vigorous	O
line	O
of	O
work	O
on	O
markov	O
chain	O
methods	O
for	O
solving	O
difﬁcult	O
probability	O
problems	O
has	O
continued	O
to	O
ﬂour-	O
ish	O
under	O
such	O
names	O
as	O
particle	O
ﬁltering	O
and	O
sequential	O
monte	O
carlo	O
;	O
see	O
gerber	O
and	O
chopin	O
(	O
2015	O
)	O
and	O
its	O
enthusiastic	O
discussion	O
.	O
modeling	O
population	O
admixture	O
(	O
pritchard	O
et	O
al.	O
,	O
2000	O
)	O
is	O
one	O
of	O
sev-	O
eral	O
applications	O
of	O
hierarchical	O
bayesian	O
models	B
and	O
mcmc	O
in	O
genetics	O
.	O
other	O
applications	O
include	O
haplotype	O
estimation	B
and	O
motif	O
ﬁnding	O
,	O
as	O
well	O
as	O
estimation	B
of	O
phylogenetic	O
trees	B
.	O
the	O
examples	O
in	O
this	O
section	O
were	O
de-	O
veloped	O
with	O
the	O
kind	O
help	O
of	O
hua	O
tang	O
and	O
david	O
golan	O
,	O
both	O
from	O
the	O
stanford	O
genetics	O
department	O
.	O
hua	O
suggested	O
the	O
example	O
and	O
provided	O
helpful	O
guidance	O
;	O
david	O
provided	O
the	O
data	B
,	O
and	O
ran	O
the	O
mcmc	O
algorithm	B
using	O
the	O
structure	O
program	O
in	O
the	O
pritchard	O
lab	O
.	O
1	O
[	O
p.	O
236	O
]	O
uninformative	O
priors	B
.	O
a	O
large	O
catalog	O
of	O
possible	O
uninformative	O
priors	B
has	O
been	O
proposed	O
,	O
thoroughly	O
surveyed	O
by	O
kass	O
and	O
wasserman	O
(	O
1996	O
)	O
.	O
one	O
approach	O
is	O
to	O
use	O
the	O
likelihood	B
from	O
a	O
small	O
part	O
of	O
the	O
data	B
,	O
say	O
just	O
one	O
or	O
two	O
data	B
points	O
out	O
of	O
n	O
,	O
as	O
the	O
prior	B
,	O
as	O
with	O
the	O
“	O
intrin-	O
sic	O
priors	B
”	O
of	O
berger	O
and	O
pericchi	O
(	O
1996	O
)	O
,	O
or	O
o	O
’	O
hagan	O
’	O
s	O
(	O
1995	O
)	O
“	O
fractional	O
bayes	O
factors.	O
”	O
another	O
approach	O
is	O
to	O
minimize	O
some	O
mathematical	O
mea-	O
sure	O
of	O
prior	B
information	O
,	O
as	O
with	O
bernardo	O
’	O
s	O
(	O
1979	O
)	O
“	O
reference	O
priors	B
”	O
or	O
jaynes	O
’	O
(	O
1968	O
)	O
“	O
maximum	O
entropy	O
”	O
criterion	O
.	O
kass	O
and	O
wasserman	O
list	O
a	O
dozen	O
more	O
possibilities	O
.	O
2	O
[	O
p.	O
236	O
]	O
coverage	B
matching	I
priors	O
.	O
welch	O
and	O
peers	O
(	O
1963	O
)	O
showed	O
that	O
,	O
for	O
a	O
multiparameter	O
family	O
f	O
(	O
cid:22	O
)	O
.x/	O
and	O
real-valued	O
parameter	O
of	O
interest	O
	O
d	O
t	B
.	O
(	O
cid:22	O
)	O
/	O
,	O
there	O
exist	O
priors	B
g.	O
(	O
cid:22	O
)	O
/	O
such	O
that	O
the	O
bayes	O
credible	O
interval	B
of	O
coverage	O
˛	O
has	O
frequentist	O
coverage	O
˛co.1=n/	O
,	O
with	O
n	O
the	O
sample	B
size	I
.	O
in	O
other	O
words	O
,	O
the	O
credible	O
intervals	B
are	O
“	O
second-order	O
accurate	O
”	O
conﬁdence	B
intervals	I
.	O
tibshirani	O
(	O
1989	O
)	O
,	O
building	O
on	O
stein	O
’	O
s	O
(	O
1985	O
)	O
work	O
,	O
produced	O
the	O
262	O
objective	O
bayes	O
inference	B
and	O
mcmc	O
nice	O
formulation	O
(	O
13.9	O
)	O
.	O
stein	O
’	O
s	O
paper	O
developed	O
the	O
least-favorable	O
fam-	O
ily	O
,	O
the	O
one-parameter	B
subfamily	O
of	O
f	O
(	O
cid:22	O
)	O
.x/	O
that	O
does	O
not	O
inappropriately	O
in-	O
crease	O
the	O
amount	O
of	O
fisher	O
information	B
for	O
estimating	O
	O
.	O
cox	O
and	O
reid	O
’	O
s	O
(	O
1987	O
)	O
orthogonal	O
parameters	O
form	B
(	O
13.8	O
)	O
is	O
formally	O
equivalent	O
to	O
the	O
least	O
favorable	O
family	O
construction	O
.	O
least	O
favorable	O
family	O
versions	O
of	O
reference	O
priors	B
and	O
intrinsic	O
priors	B
have	O
been	O
proposed	O
to	O
avoid	O
the	O
difﬁculty	O
with	O
general-purpose	O
uninfor-	O
mative	O
priors	B
seen	O
in	O
figure	O
11.7.	O
they	O
do	O
so	O
,	O
but	O
at	O
the	O
price	O
of	O
requiring	O
a	O
different	O
prior	B
for	O
each	O
choice	O
of	O
	O
d	O
t	B
.	O
(	O
cid:22	O
)	O
/—which	O
begins	O
to	O
sound	O
more	O
frequentistic	O
than	O
bayesian	O
.	O
3	O
[	O
p.	O
238	O
]	O
conjugate	B
families	O
theorem	B
.	O
theorem	B
13.1	O
,	O
(	O
13.16	O
)	O
–	O
(	O
13.18	O
)	O
,	O
is	O
rigorously	O
derived	O
in	O
diaconis	O
and	O
ylvisaker	O
(	O
1979	O
)	O
.	O
families	O
other	O
than	O
(	O
13.14	O
)	O
have	O
conjugate-like	O
properties	O
,	O
but	O
not	O
the	O
neat	O
posterior	O
expecta-	O
tion	O
result	O
(	O
13.18	O
)	O
.	O
using	O
˛	O
d	O
log	O
.	O
(	O
cid:22	O
)	O
/	O
,	O
.˛/	O
d	O
(	O
cid:22	O
)	O
,	O
and	O
v	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
(	O
cid:22	O
)	O
for	O
the	O
poisson	O
.	O
has	O
density	B
(	O
cid:22	O
)	O
(	O
cid:23	O
)	O
(	O
cid:0	O
)	O
1e	O
(	O
cid:0	O
)	O
.	O
(	O
cid:23	O
)	O
c1/	O
e	O
5	O
[	O
p.	O
239	O
]	O
inverse	O
gamma	B
and	O
chi-square	O
distributions	O
.	O
a	O
g	O
(	O
cid:23	O
)	O
variate	O
(	O
13.22	O
)	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
=	O
.	O
(	O
cid:23	O
)	O
/	O
.	O
an	O
inverse	O
gamma	B
variate	O
1=g	O
(	O
cid:23	O
)	O
has	O
density	B
4	O
[	O
p.	O
238	O
]	O
poisson	O
formula	B
(	O
13.20	O
)	O
.	O
this	O
follows	O
immediately	O
from	O
(	O
13.14	O
)	O
,	O
(	O
cid:0	O
)	O
1=	O
(	O
cid:22	O
)	O
=	O
	O
.	O
(	O
cid:23	O
)	O
/	O
,	O
so	O
(	O
cid:22	O
)	O
gn0	O
;	O
x0	O
.	O
(	O
cid:22	O
)	O
/	O
d	O
c	O
(	O
cid:22	O
)	O
(	O
cid:0	O
)	O
.n0x0c2/e	O
(	O
cid:0	O
)	O
n0x0	O
(	O
cid:23	O
)	O
=	O
(	O
cid:22	O
)	O
(	O
13.85	O
)	O
is	O
the	O
gamma	B
conjugate	O
density	B
in	O
table	O
13.1.	O
the	O
gamma	B
results	O
can	O
be	O
restated	O
in	O
terms	O
of	O
chi-squared	O
variates	O
:	O
d	O
(	O
cid:22	O
)	O
xi	O
(	O
cid:24	O
)	O
(	O
cid:22	O
)	O
(	O
13.86	O
)	O
(	O
cid:31	O
)	O
2	O
m	O
m	O
gm=2	O
m=2	O
has	O
conjugate	B
prior	O
gn0	O
;	O
x0	O
.	O
(	O
cid:22	O
)	O
/	O
(	O
cid:24	O
)	O
n0x0m=	O
(	O
cid:31	O
)	O
2	O
n0mc2	O
;	O
(	O
13.87	O
)	O
an	O
inverse	O
chi-squared	O
distribution	B
.	O
6	O
[	O
p.	O
240	O
]	O
vasoconstriction	B
data	O
.	O
efron	O
and	O
gous	O
(	O
2001	O
)	O
use	O
this	O
data	B
to	O
illus-	O
trate	O
a	O
theory	B
connecting	O
bayes	O
factors	O
with	O
fisherian	O
hypothesis	B
testing	I
.	O
it	O
is	O
part	O
of	O
a	O
larger	O
data	B
set	O
appearing	O
in	O
finney	O
(	O
1947	O
)	O
,	O
also	O
discussed	O
in	O
kass	O
and	O
raftery	O
(	O
1995	O
)	O
.	O
7	O
[	O
p.	O
245	O
]	O
jeffreys	O
’	O
and	O
fisher	O
’	O
s	O
scales	O
of	O
evidence	O
.	O
jeffreys	O
’	O
scale	B
as	O
it	O
ap-	O
pears	O
in	O
table	O
13.3	O
is	O
taken	O
from	O
the	O
slightly	O
amended	O
form	B
in	O
kass	O
and	O
raftery	O
(	O
1995	O
)	O
.	O
efron	O
and	O
gous	O
(	O
2001	O
)	O
compare	O
it	O
with	O
fisher	O
’	O
s	O
scale	B
for	O
the	O
contradictory	O
results	O
of	O
table	O
13.5.	O
fisher	O
and	O
jeffreys	O
worked	O
in	O
dif-	O
ferent	O
scientiﬁc	O
contexts—small-sample	O
agricultural	O
experiments	O
versus	O
13.6	O
notes	O
and	O
details	O
263	O
hard-science	O
geostatistics—which	O
might	O
explain	O
jeffreys	O
’	O
more	O
stringent	O
conception	O
of	O
what	O
constitutes	O
signiﬁcant	O
evidence	O
.	O
8	O
[	O
p.	O
246	O
]	O
the	O
bayesian	O
information	B
criterion	I
.	O
the	O
bic	O
was	O
proposed	O
by	O
schwarz	O
(	O
1978	O
)	O
.	O
kass	O
and	O
wasserman	O
(	O
1996	O
)	O
provide	O
an	O
extended	O
discus-	O
sion	O
of	O
the	O
bic	O
and	O
model	O
selection	O
.	O
“	O
proofs	O
”	O
of	O
(	O
13.37	O
)	O
ultimately	O
depend	O
on	O
sample	B
size	I
coherency	O
(	O
13.49	O
)	O
,	O
as	O
in	O
efron	O
and	O
gous	O
(	O
2001	O
)	O
.	O
quotation	O
marks	O
are	O
used	O
here	O
to	O
indicate	O
the	O
basically	O
qualitative	O
nature	O
of	O
bic	O
:	O
if	O
we	O
think	O
of	O
the	O
data	B
points	O
as	O
being	O
collected	O
in	O
pairs	O
then	O
n	O
becomes	O
n=2	O
in	O
(	O
13.38	O
)	O
,	O
etc.	O
,	O
so	O
it	O
doesn	O
’	O
t	B
pay	O
to	O
put	O
too	O
ﬁne	O
a	O
point	O
on	O
the	O
criterion	O
.	O
moreover	O
,	O
the	O
convergence	O
is	O
geometric	O
in	O
the	O
l1	O
normpjp.b/	O
9	O
[	O
p.	O
256	O
]	O
mcmc	O
convergence	O
.	O
suppose	O
we	O
begin	O
the	O
mcmc	O
random	O
walk	O
(	O
13.76	O
)	O
–	O
(	O
13.77	O
)	O
by	O
choosing	O
	O
.1/	O
according	O
to	O
some	O
arbitrary	O
starting	O
dis-	O
tribution	O
p.1/	O
.	O
let	O
p.b/	O
be	O
the	O
distribution	B
of	O
	O
.b/	O
,	O
obtained	O
after	O
b	O
steps	O
of	O
the	O
random	O
walk	O
.	O
markov	O
chain	O
theory	B
says	O
that	O
,	O
under	O
certain	O
broad	O
con-	O
ditions	O
on	O
q.i	O
;	O
j	O
/	O
,	O
p.b/	O
will	O
converge	O
to	O
the	O
target	O
distribution	B
p	O
(	O
13.75	O
)	O
.	O
(	O
cid:0	O
)	O
pkj	O
,	O
suc-	O
cessive	O
discrepancies	O
eventually	O
decreasing	O
by	O
a	O
multiplicative	O
factor	B
.	O
a	O
proof	O
appears	O
in	O
tanner	O
and	O
wong	O
(	O
1987	O
)	O
.	O
unfortunately	O
,	O
the	O
factor	B
won	O
’	O
t	B
be	O
known	O
in	O
most	O
applications	O
,	O
and	O
the	O
actual	O
convergence	O
may	O
be	O
quite	O
slow	O
.	O
k	O
10	O
[	O
p.	O
258	O
]	O
dirichlet	O
distribution	B
.	O
the	O
dirichlet	O
is	O
a	O
multivariate	B
generaliza-	O
tion	O
of	O
the	O
beta	B
distribution	O
(	O
section	O
5.1	O
)	O
,	O
typically	O
used	O
to	O
represent	O
prior	B
distributions	O
for	O
the	O
multinomial	O
distribution	B
.	O
for	O
x	O
d	O
.x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xk/	O
0	O
,	O
j	O
xj	O
d	O
1	O
,	O
the	O
d.	O
(	O
cid:23	O
)	O
/	O
density	B
is	O
deﬁned	O
as	O
ky	O
jd1	O
(	O
cid:23	O
)	O
j	O
(	O
cid:0	O
)	O
1	O
x	O
j	O
;	O
(	O
13.88	O
)	O
with	O
xj	O
2	O
.0	O
;	O
1/	O
,	O
p	O
where	O
b	O
.	O
(	O
cid:23	O
)	O
/	O
dq	O
f	O
(	O
cid:23	O
)	O
.x/	O
d	O
1	O
j	O
	O
.	O
(	O
cid:23	O
)	O
j	O
/=	O
.p	O
b	O
.	O
(	O
cid:23	O
)	O
/	O
j	O
(	O
cid:23	O
)	O
j	O
/	O
.	O
14	O
statistical	O
inference	B
and	O
methodology	O
in	O
the	O
postwar	O
era	O
the	O
fundamentals	O
of	O
statistical	O
inference—frequentist	O
,	O
bayesian	O
,	O
fisherian	O
—were	O
set	B
in	O
place	O
by	O
the	O
end	O
of	O
the	O
ﬁrst	O
half	O
of	O
the	O
twentieth	O
century	O
,	O
as	O
discussed	O
in	O
part	O
i	O
of	O
this	O
book	O
.	O
the	O
postwar	O
era	O
witnessed	O
a	O
massive	O
ex-	O
pansion	O
of	O
statistical	O
methodology	O
,	O
responding	O
to	O
the	O
data-driven	O
demands	O
of	O
modern	O
scientiﬁc	O
technology	O
.	O
we	O
are	O
now	O
at	O
the	O
end	O
of	O
part	O
ii	O
,	O
“	O
early	O
computer-age	O
methods	O
,	O
”	O
having	O
surveyed	O
the	O
march	O
of	O
new	O
statistical	O
algorithms	O
and	O
their	O
inferential	O
justiﬁcation	O
from	O
the	O
1950s	O
through	O
the	O
1990s	O
.	O
this	O
was	O
a	O
time	O
of	O
opportunity	O
for	O
the	O
discipline	O
of	O
statistics	B
,	O
when	O
the	O
speed	O
of	O
computation	O
increased	O
by	O
a	O
factor	B
of	O
a	O
thousand	O
,	O
and	O
then	O
another	O
thousand	O
.	O
as	O
we	O
said	O
before	O
,	O
a	O
land	O
bridge	O
had	O
opened	O
to	O
a	O
new	O
continent	O
,	O
but	O
not	O
everyone	O
was	O
eager	O
to	O
cross	O
.	O
we	O
saw	O
a	O
mixed	O
picture	O
:	O
the	O
computer	O
played	O
a	O
minor	O
or	O
negligible	O
role	O
in	O
the	O
development	O
of	O
some	O
inﬂuential	O
topics	O
such	O
as	O
empirical	B
bayes	O
,	O
but	O
was	O
fundamental	O
to	O
others	O
such	O
as	O
the	O
bootstrap	O
.	O
fifteen	O
major	O
topics	O
were	O
examined	O
in	O
chapters	O
6	O
through	O
13.	O
what	O
fol-	O
lows	O
is	O
a	O
short	O
scorecard	O
of	O
their	O
inferential	O
afﬁnities	O
,	O
bayesian	O
,	O
frequentist	O
,	O
or	O
fisherian	O
,	O
as	O
well	O
as	O
an	O
assessment	O
of	O
the	O
computer	O
’	O
s	O
role	O
in	O
their	O
devel-	O
opment	O
.	O
none	O
of	O
this	O
is	O
very	O
precise	O
,	O
but	O
the	O
overall	O
picture	O
,	O
illustrated	O
in	O
figure	O
14.1	O
,	O
is	O
evocative	O
.	O
empirical	B
bayes	O
robbins	O
’	O
original	O
development	O
of	O
formula	B
(	O
6.5	O
)	O
was	O
frequentistic	O
,	O
but	O
most	O
statistical	O
researchers	O
were	O
frequentists	O
in	O
the	O
postwar	O
era	O
so	O
that	O
could	O
be	O
expected	O
.	O
the	O
obvious	O
bayesian	O
component	O
of	O
empirical	B
bayes	O
arguments	O
is	O
balanced	O
by	O
their	O
frequentist	O
emphasis	O
on	O
(	O
nearly	O
)	O
unbiased	O
estimation	O
of	O
bayesian	O
estimators	O
,	O
as	O
well	O
as	O
the	O
restriction	O
to	O
using	O
only	O
current	O
data	B
for	O
inference	B
.	O
electronic	O
computation	O
played	O
hardly	O
any	O
role	O
in	O
the	O
theory	B
’	O
s	O
development	O
(	O
as	O
indicated	O
by	O
blue	O
coloring	O
in	O
the	O
ﬁgure	O
)	O
.	O
of	O
course	O
mod-	O
264	O
postwar	O
inference	B
and	O
methodology	O
265	O
figure	O
14.1	O
bayesian	O
,	O
frequentist	O
,	O
and	O
fisherian	O
inﬂuences	O
,	O
as	O
described	O
in	O
the	O
text	O
,	O
on	O
15	O
major	O
topics	O
,	O
1950s	O
through	O
1990s	O
.	O
colors	O
indicate	O
the	O
importance	O
of	O
electronic	O
computation	O
in	O
their	O
development	O
:	O
red	O
,	O
crucial	O
;	O
violet	O
,	O
very	O
important	O
;	O
green	O
,	O
important	O
;	O
light	O
blue	O
,	O
less	O
important	O
;	O
blue	O
,	O
negligible	O
.	O
ern	O
empirical	B
bayes	O
applications	O
are	O
heavily	O
computational	O
,	O
but	O
that	O
is	O
the	O
case	O
for	O
most	O
methods	O
now	O
.	O
james–stein	O
and	O
ridge	O
regression	B
the	O
frequentist	O
roots	O
of	O
james–stein	O
estimation	B
are	O
more	O
deﬁnitive	O
,	O
es-	O
pecially	O
given	O
the	O
force	O
of	O
the	O
james–stein	O
theorem	B
(	O
7.16	O
)	O
.	O
nevertheless	O
,	O
the	O
empirical	B
bayes	O
interpretation	O
(	O
7.13	O
)	O
lends	O
james–stein	O
some	O
bayes-	O
ian	O
credibility	O
.	O
electronic	O
computation	O
played	O
no	O
role	O
in	O
its	O
development	O
.	O
this	O
was	O
less	O
true	O
for	O
ridge	B
regression	I
,	O
colored	O
light	O
blue	O
in	O
the	O
ﬁgure	O
,	O
where	O
the	O
matrix	B
calculation	O
(	O
7.36	O
)	O
would	O
have	O
been	O
daunting	O
in	O
the	O
pre-	O
electronic	O
age	O
.	O
the	O
bayesian	O
justiﬁcation	O
(	O
7.37	O
)	O
–	O
(	O
7.39	O
)	O
of	O
ridge	B
regression	I
lllbayesianfrequentistfisherianlkaplan−meierllog−ranklglmlproportional	O
hazards	O
(	O
partial	O
likelihood	B
)	O
lbootstraplempiricalbayeslobjectivebayes	O
(	O
mcmc	O
)	O
ljackknifelcvljames−steinlregression	O
treeslridgeregressionlbiclmissing	O
data	B
(	O
em	O
)	O
laic−cp	O
266	O
postwar	O
inference	B
and	O
methodology	O
carries	O
more	O
weight	O
than	O
for	O
james–stein	O
,	O
given	O
the	O
absence	O
of	O
a	O
strong	O
frequentist	O
theorem	B
.	O
generalized	O
linear	B
models	O
glm	O
development	O
began	O
with	O
a	O
pronounced	O
fisherian	O
emphasis	O
on	O
like-	O
lihood1	O
modeling	O
,	O
but	O
settled	O
down	O
to	O
more	O
or	O
less	O
standard	O
frequentist	O
regression	B
theory	O
.	O
a	O
key	O
operational	O
feature	O
,	O
low-dimensional	O
sufﬁcient	O
statistics	B
,	O
limited	O
its	O
computational	O
demands	O
,	O
but	O
glm	O
theory	B
could	O
not	O
have	O
developed	O
before	O
the	O
age	O
of	O
electronic	O
computers	O
(	O
as	O
indicated	O
by	O
green	O
coloring	O
)	O
.	O
regression	B
trees	O
model	B
building	O
by	O
means	O
of	O
regression	B
trees	O
is	O
a	O
computationally	O
intensive	O
enterprise	O
,	O
indicated	O
by	O
its	O
red	O
color	O
in	O
figure	O
14.1.	O
its	O
justiﬁcation	O
has	O
been	O
mainly	O
in	O
terms	O
of	O
asymptotic	O
frequentist	O
properties	O
.	O
survival	O
analysis	B
the	O
kaplan–meier	O
estimate	B
,	O
log-rank	O
test	O
,	O
and	O
proportional	O
hazards	O
model	B
move	O
from	O
the	O
frequentist	O
pole	O
of	O
the	O
diagram	O
toward	O
the	O
fisherian	O
pole	O
as	O
the	O
conditioning	O
arguments	O
in	O
sections	O
9.2	O
through	O
9.4	O
become	O
more	O
elaborate	O
.	O
the	O
role	O
of	O
computation	O
in	O
their	O
development	O
increases	O
in	O
the	O
same	O
order	O
.	O
kaplan–meier	O
estimates	O
can	O
be	O
done	O
by	O
hand	O
(	O
and	O
were	O
)	O
,	O
while	O
it	O
is	O
impossible	O
to	O
contemplate	O
proportional	O
hazards	O
analysis	B
with-	O
out	O
the	O
computer	O
.	O
partial	O
likelihood	B
,	O
the	O
enabling	O
argument	B
for	O
the	O
theory	B
,	O
is	O
a	O
quintessential	O
fisherian	O
device	O
.	O
missing	B
data	I
and	O
the	O
em	O
algorithm	B
the	O
imputation	O
of	O
missing	B
data	I
has	O
a	O
bayesian	O
ﬂavor	O
of	O
indirect	O
evidence	O
,	O
but	O
the	O
“	O
fake	O
data	B
”	O
principle	O
(	O
9.44	O
)	O
–	O
(	O
9.46	O
)	O
has	O
fisherian	O
roots	O
.	O
fast	O
compu-	O
tation	O
was	O
important	O
to	O
the	O
method	B
’	O
s	O
development	O
,	O
particularly	O
so	O
for	O
the	O
em	O
algorithm	B
.	O
jackknife	O
and	O
bootstrap	O
the	O
purpose	O
of	O
the	O
jackknife	O
was	O
to	O
calculate	O
frequentist	O
standard	O
errors	O
and	O
biases	O
.	O
electronic	O
computation	O
was	O
of	O
only	O
minor	O
importance	O
in	O
its	O
1	O
more	O
explicitly	O
,	O
quasilikelihoods	O
,	O
an	O
extension	O
to	O
a	O
wider	O
class	O
of	O
exponential	O
family	O
models	B
.	O
postwar	O
inference	B
and	O
methodology	O
267	O
development	O
.	O
by	O
contrast	O
,	O
the	O
bootstrap	O
is	O
the	O
archetype	O
for	O
computer-	O
intensive	O
statistical	O
inference	B
.	O
it	O
combines	O
frequentism	O
with	O
fisherian	O
de-	O
vices	O
:	O
plug-in	O
estimation	B
of	O
accuracy	O
estimates	O
,	O
as	O
in	O
(	O
10.18	O
)	O
–	O
(	O
10.19	O
)	O
,	O
and	O
correctness	O
arguments	O
for	O
bootstrap	O
conﬁdence	B
intervals	I
,	O
(	O
11.79	O
)	O
–	O
(	O
11.83	O
)	O
.	O
cross-validation	O
the	O
renaissance	O
of	O
interest	O
in	O
cross-validation	O
required	O
fast	O
computation	O
,	O
especially	O
for	O
assessing	O
modern	O
computer-intensive	O
prediction	O
algorithms	O
.	O
as	O
pointed	O
out	O
in	O
the	O
text	O
following	O
figure	O
(	O
12.3	O
)	O
,	O
cross-validation	O
is	O
a	O
strongly	B
frequentist	O
procedure	O
.	O
bic	O
,	O
aic	O
,	O
and	O
cp	O
these	O
three	O
algorithms	O
were	O
designed	O
to	O
avoid	O
computation	O
,	O
bic	O
for	O
bayes-	O
ian	O
model	B
selection	I
,	O
section	O
(	O
13.3	O
)	O
,	O
aic	O
and	O
cp	O
for	O
unbiased	O
estimation	O
of	O
frequentist	O
prediction	O
error	O
,	O
(	O
12.76	O
)	O
and	O
(	O
12.50	O
)	O
.	O
objective	O
bayes	O
and	O
mcmc	O
in	O
addition	O
to	O
their	O
bayesian	O
provenance	O
,	O
objective	O
bayes	O
methods	O
have	O
some	O
connection	O
with	O
ﬁducial	O
ideas	O
and	O
the	O
bootstrap	O
,	O
as	O
discussed	O
in	O
sec-	O
tion	O
11.5	O
.	O
(	O
an	O
argument	B
can	O
be	O
made	O
that	O
they	O
are	O
at	O
least	O
as	O
frequentist	O
as	O
they	O
are	O
bayesian—see	O
the	O
notes	O
below—though	O
that	O
has	O
not	O
been	O
acted	O
upon	O
in	O
coloring	O
the	O
ﬁgure	O
.	O
)	O
gibbs	O
sampling	O
and	O
mcmc	O
,	O
the	O
enabling	O
al-	O
gorithms	O
,	O
epitomize	O
modern	O
computer-intensive	O
inference	B
.	O
notes	O
figure	O
14.1	O
is	O
an	O
updated	O
version	O
of	O
figure	O
8	O
in	O
efron	O
(	O
1998	O
)	O
,	O
“	O
r	O
.	O
a.	O
fisher	O
in	O
the	O
21st	O
century.	O
”	O
there	O
the	O
difﬁculty	O
of	O
properly	O
placing	O
objective	O
bayes	O
is	O
confessed	O
,	O
with	O
erich	O
lehmann	O
arguing	O
for	O
a	O
more	O
frequentist	O
(	O
decision-theoretic	O
)	O
location	O
:	O
“	O
in	O
fact	O
,	O
the	O
concept	O
of	O
uninformative	O
prior	B
is	O
philosophically	O
close	O
to	O
wald	O
’	O
s	O
least	O
favorable	O
distribution	B
,	O
and	O
the	O
two	O
often	O
coincide.	O
”	O
figure	O
14.1	O
shows	O
a	O
healthy	O
mixture	O
of	O
philosophical	O
and	O
computational	O
tactics	O
at	O
work	O
,	O
with	O
all	O
three	O
edges	O
(	O
but	O
not	O
the	O
center	O
)	O
of	O
the	O
triangle	O
in	O
play	O
.	O
all	O
new	O
points	O
will	O
be	O
red	O
(	O
computer-intensive	O
)	O
as	O
we	O
move	O
into	O
the	O
twenty-ﬁrst	O
century	O
in	O
part	O
iii	O
.	O
our	O
triangle	O
will	O
have	O
to	O
struggle	O
to	O
accom-	O
modate	O
some	O
major	O
developments	O
based	O
on	O
machine	O
learning	O
,	O
a	O
philosoph-	O
ically	O
atheistic	O
approach	O
to	O
statistical	O
inference	B
.	O
part	O
iii	O
twenty-first-century	O
topics	O
15	O
large-scale	O
hypothesis	O
testing	B
and	O
false-discovery	O
rates	O
by	O
the	O
ﬁnal	O
decade	O
of	O
the	O
twentieth	O
century	O
,	O
electronic	O
computation	O
fully	O
dominated	O
statistical	O
practice	O
.	O
almost	O
all	O
applications	O
,	O
classical	O
or	O
other-	O
wise	O
,	O
were	O
now	O
performed	O
on	O
a	O
suite	O
of	O
computer	O
platforms	O
:	O
sas	O
,	O
spss	O
,	O
minitab	O
,	O
matlab	O
,	O
s	O
(	O
later	O
r	O
)	O
,	O
and	O
others	O
.	O
the	O
trend	O
accelerates	O
when	O
we	O
enter	O
the	O
twenty-ﬁrst	O
century	O
,	O
as	O
statis-	O
tical	O
methodology	O
struggles	O
,	O
most	O
often	O
successfully	O
,	O
to	O
keep	O
up	O
with	O
the	O
vastly	O
expanding	O
pace	O
of	O
scientiﬁc	O
data	B
production	O
.	O
this	O
has	O
been	O
a	O
two-	O
way	O
game	O
of	O
pursuit	O
,	O
with	O
statistical	O
algorithms	O
chasing	O
ever	O
larger	O
data	B
sets	O
,	O
while	O
inferential	O
analysis	B
labors	O
to	O
rationalize	O
the	O
algorithms	O
.	O
part	O
iii	O
of	O
our	O
book	O
concerns	O
topics	O
in	O
twenty-ﬁrst-century1	O
statistics	B
.	O
the	O
word	O
“	O
topics	O
”	O
is	O
intended	O
to	O
signal	O
selections	O
made	O
from	O
a	O
wide	O
cat-	O
alog	O
of	O
possibilities	O
.	O
part	O
ii	O
was	O
able	O
to	O
review	O
a	O
large	O
portion	O
(	O
though	O
certainly	O
not	O
all	O
)	O
of	O
the	O
important	O
developments	O
during	O
the	O
postwar	O
period	O
.	O
now	O
,	O
deprived	O
of	O
the	O
advantage	O
of	O
hindsight	O
,	O
our	O
survey	O
will	O
be	O
more	O
illus-	O
trative	O
than	O
deﬁnitive	O
.	O
for	O
many	O
statisticians	O
,	O
microarrays	O
provided	O
an	O
introduction	O
to	O
large-	O
scale	B
data	O
analysis	B
.	O
these	O
were	O
revolutionary	O
biomedical	O
devices	O
that	O
en-	O
abled	O
the	O
assessment	O
of	O
individual	O
activity	O
for	O
thousands	O
of	O
genes	O
at	O
once—	O
and	O
,	O
in	O
doing	O
so	O
,	O
raised	O
the	O
need	O
to	O
carry	O
out	O
thousands	O
of	O
simultaneous	O
hypothesis	O
tests	O
,	O
done	O
with	O
the	O
prospect	O
of	O
ﬁnding	O
only	O
a	O
few	O
interesting	O
genes	O
among	O
a	O
haystack	O
of	O
null	O
cases	O
.	O
this	O
chapter	O
concerns	O
large-scale	O
hypothesis	O
testing	B
and	O
the	O
false-discovery	B
rate	I
,	O
the	O
breakthrough	O
in	O
statis-	O
tical	O
inference	B
it	O
elicited	O
.	O
1	O
actually	O
what	O
historians	O
might	O
call	O
“	O
the	O
long	O
twenty-ﬁrst	O
century	O
”	O
since	O
we	O
will	O
begin	O
in	O
1995	O
.	O
271	O
272	O
large-scale	O
hypothesis	O
testing	B
and	O
fdrs	O
15.1	O
large-scale	B
testing	I
the	O
prostate	B
cancer	O
data	B
,	O
figure	O
3.4	O
,	O
came	O
from	O
a	O
microarray	O
study	O
of	O
n	O
d	O
102	O
men	O
,	O
52	O
prostate	B
cancer	O
patients	O
and	O
50	O
normal	B
controls	O
.	O
each	O
man	O
’	O
s	O
gene	O
expression	O
levels	O
were	O
measured	O
on	O
a	O
panel	O
of	O
n	O
d	O
6033	O
genes	O
,	O
yielding	O
a	O
6033	O
(	O
cid:2	O
)	O
102	O
matrix	B
of	O
measurements	O
xij	O
,	O
xij	O
d	O
activity	O
of	O
ith	O
gene	O
for	O
j	O
th	O
man	O
:	O
(	O
15.1	O
)	O
for	O
each	O
gene	O
,	O
a	O
two-sample	B
t	O
statistic	B
(	O
2.17	O
)	O
ti	O
was	O
computed	O
com-	O
paring	O
gene	O
i	O
’	O
s	O
expression	O
levels	O
for	O
the	O
52	O
patients	O
with	O
those	O
for	O
the	O
50	O
controls	O
.	O
under	O
the	O
null	O
hypothesis	O
h0i	O
that	O
the	O
patients	O
’	O
and	O
the	O
controls	O
’	O
responses	O
come	O
from	O
the	O
same	O
normal	B
distribution	O
of	O
gene	O
i	O
expression	O
levels	O
,	O
ti	O
will	O
follow	O
a	O
standard	O
student	O
t	B
distribution	O
with	O
100	O
degrees	O
of	O
freedom	O
,	O
t100	O
.	O
the	O
transformation	O
zi	O
d	O
ˆ	O
(	O
cid:0	O
)	O
1	O
.f100.ti	O
//	O
;	O
(	O
15.2	O
)	O
(	O
cid:0	O
)	O
1	O
the	O
inverse	O
function	B
of	O
where	O
f100	O
is	O
the	O
cdf	B
of	O
a	O
t100	O
distribution	B
and	O
ˆ	O
a	O
standard	O
normal	O
cdf	B
,	O
makes	O
zi	O
standard	O
normal	O
under	O
the	O
null	O
hypothesis	O
:	O
h0i	O
w	O
zi	O
(	O
cid:24	O
)	O
n	O
.0	O
;	O
1/	O
:	O
(	O
15.3	O
)	O
of	O
course	O
the	O
investigators	O
were	O
hoping	O
to	O
spot	O
some	O
non-null	O
genes	O
,	O
ones	O
for	O
which	O
the	O
patients	O
and	O
controls	O
respond	O
differently	O
.	O
it	O
can	O
be	O
shown	O
that	O
a	O
reasonable	O
model	B
for	O
both	O
null	O
and	O
non-null	O
genes	O
is2	O
	O
1	O
zi	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
i	O
;	O
1/	O
;	O
(	O
15.4	O
)	O
(	O
cid:22	O
)	O
i	O
being	O
the	O
effect	O
size	O
for	O
gene	O
i.	O
null	O
genes	O
have	O
(	O
cid:22	O
)	O
i	O
d	O
0	O
,	O
while	O
the	O
investigators	O
hoped	O
to	O
ﬁnd	O
genes	O
with	O
large	O
positive	O
or	O
negative	O
(	O
cid:22	O
)	O
i	O
effects	O
.	O
figure	O
15.1	O
shows	O
the	O
histogram	O
of	O
the	O
6033	O
zi	O
values	O
.	O
the	O
red	O
curve	O
is	O
the	O
scaled	O
n	O
.0	O
;	O
1/	O
density	B
that	O
would	O
apply	O
if	O
in	O
fact	O
all	O
of	O
the	O
genes	O
were	O
null	O
,	O
that	O
is	O
if	O
all	O
of	O
the	O
(	O
cid:22	O
)	O
i	O
equaled	O
zero.3	O
we	O
can	O
see	O
that	O
the	O
curve	O
is	O
a	O
little	O
too	O
high	O
near	O
the	O
center	O
and	O
too	O
low	O
in	O
the	O
tails	O
.	O
good	O
!	O
even	O
though	O
most	O
of	O
the	O
genes	O
appear	O
null	O
,	O
the	O
discrepancies	O
from	O
the	O
curve	O
suggest	O
that	O
there	O
are	O
some	O
non-null	O
cases	O
,	O
the	O
kind	O
the	O
investigators	O
hoped	O
to	O
ﬁnd	O
.	O
large-scale	B
testing	I
refers	O
exactly	O
to	O
this	O
situation	O
:	O
having	O
observed	O
a	O
large	O
number	O
n	O
of	O
test	O
statistics	B
,	O
how	O
should	O
we	O
decide	O
which	O
if	O
any	O
of	O
the	O
null	O
hypotheses	O
to	O
reject	O
?	O
classical	O
testing	B
theory	O
involved	O
only	O
a	O
single	O
case	O
,	O
n	O
d	O
1.	O
a	O
theory	B
of	O
multiple	O
testing	B
arose	O
in	O
the	O
1960s	O
,	O
“	O
multiple	O
”	O
2	O
this	O
is	O
model	B
(	O
3.28	O
)	O
,	O
with	O
zi	O
now	O
replacing	O
the	O
notation	O
xi	O
.	O
3	O
it	O
is	O
ce	O
2	O
(	O
cid:25	O
)	O
with	O
c	O
chosen	O
to	O
make	O
the	O
area	O
under	O
the	O
curve	O
equal	O
the	O
area	O
of	O
p	O
(	O
cid:0	O
)	O
z2=2=	O
the	O
histogram	O
.	O
15.1	O
large-scale	B
testing	I
273	O
figure	O
15.1	O
histogram	O
of	O
n	O
d	O
6033	O
z-values	O
,	O
one	O
for	O
each	O
gene	O
in	O
the	O
prostate	B
cancer	O
study	O
.	O
if	O
all	O
genes	O
were	O
null	O
(	O
15.3	O
)	O
the	O
histogram	O
would	O
track	O
the	O
red	O
curve	O
.	O
for	O
which	O
genes	O
can	O
we	O
reject	O
the	O
null	O
hypothesis	O
?	O
meaning	O
n	O
between	O
2	O
and	O
perhaps	O
20.	O
the	O
microarray	O
era	O
produced	O
data	B
sets	O
with	O
n	O
in	O
the	O
hundreds	O
,	O
thousands	O
,	O
and	O
now	O
even	O
millions	O
.	O
this	O
sounds	O
like	O
piling	O
difﬁculty	O
upon	O
difﬁculty	O
,	O
but	O
in	O
fact	O
there	O
are	O
some	O
inferential	O
advantages	O
to	O
the	O
large-n	O
framework	O
,	O
as	O
we	O
will	O
see	O
.	O
the	O
most	O
troubling	O
fact	O
about	O
large-scale	B
testing	I
is	O
how	O
easy	O
it	O
is	O
to	O
be	O
fooled	O
.	O
running	O
100	O
separate	O
hypothesis	O
tests	O
at	O
signiﬁcance	O
level	O
0.05	O
will	O
produce	O
about	O
ﬁve	O
“	O
signiﬁcant	O
”	O
results	O
even	O
if	O
each	O
case	O
is	O
actually	O
null	O
.	O
the	O
classical	O
bonferroni	O
bound	B
avoids	O
this	O
fallacy	O
by	O
strengthening	O
the	O
threshold	O
of	O
evidence	O
required	O
to	O
declare	O
an	O
individual	O
case	O
signiﬁcant	O
(	O
i.e.	O
,	O
non-null	O
)	O
.	O
for	O
an	O
overall	O
signiﬁcance	O
level	O
˛	O
,	O
perhaps	O
˛	O
d	O
0:05	O
,	O
with	O
n	O
simultaneous	O
tests	O
,	O
the	O
bonferroni	O
bound	B
rejects	O
the	O
ith	O
null	O
hypothesis	O
h0i	O
only	O
if	O
it	O
attains	O
individual	O
signiﬁcance	O
level	O
˛=n	O
.	O
for	O
˛	O
d	O
0:05	O
,	O
n	O
d	O
6033	O
,	O
and	O
h0i	O
w	O
zi	O
(	O
cid:24	O
)	O
n	O
.0	O
;	O
1/	O
,	O
the	O
one-sided	O
bonferroni	O
threshold	O
for	O
signiﬁcance	O
is	O
(	O
cid:0	O
)	O
ˆ	O
(	O
cid:0	O
)	O
1.0:05=n	O
/	O
d	O
4:31	O
(	O
compared	O
with	O
1.645	O
for	O
n	O
d	O
1	O
)	O
.	O
only	O
four	O
of	O
the	O
prostate	B
study	O
genes	O
surpass	O
this	O
threshold	O
.	O
classic	O
hypothesis	B
testing	I
is	O
usually	O
phrased	O
in	O
terms	O
of	O
signiﬁcance	O
lev-	O
els	O
and	O
p-values	O
.	O
if	O
test	O
statistic	B
z	O
has	O
cdf	B
f0.z/	O
under	O
the	O
null	O
hypothesis	O
−4−20240100200300400500z−valuescounts	O
274	O
then4	O
large-scale	O
hypothesis	O
testing	B
and	O
fdrs	O
p	O
d	O
1	O
(	O
cid:0	O
)	O
f0.z/	O
(	O
15.5	O
)	O
is	O
the	O
right-sided	O
p-value	B
,	O
larger	O
z	O
giving	O
smaller	O
p-value	B
.	O
“	O
signiﬁcance	O
level	O
”	O
refers	O
to	O
a	O
prechosen	O
threshold	O
value	O
,	O
e.g.	O
,	O
˛	O
d	O
0:05.	O
the	O
null	O
hypothesis	O
is	O
“	O
rejected	O
at	O
level	O
˛	O
”	O
if	O
we	O
observe	O
p	O
	O
˛	O
.	O
table	O
13.4	O
on	O
page	O
245	O
(	O
where	O
“	O
coverage	O
level	O
”	O
means	O
one	O
minus	O
the	O
signiﬁcance	O
level	O
)	O
shows	O
fisher	O
’	O
s	O
scale	B
for	O
interpreting	O
p-values	O
.	O
a	O
level-˛	O
test	O
for	O
a	O
single	O
null	O
hypothesis	O
h0	O
satisﬁes	O
,	O
by	O
deﬁnition	O
,	O
˛	O
d	O
prfreject	O
true	O
h0g	O
:	O
for	O
a	O
collection	O
of	O
n	O
null	O
hypotheses	O
h0i	O
,	O
the	O
family-wise	O
error	O
rate	B
is	O
the	O
probability	O
of	O
making	O
even	O
one	O
false	O
rejection	O
,	O
(	O
15.6	O
)	O
(	O
15.7	O
)	O
(	O
15.8	O
)	O
bonferroni	O
’	O
s	O
procedure	O
controls	O
fwer	O
at	O
level	O
˛	O
:	O
let	O
i0	O
be	O
the	O
indices	O
of	O
the	O
true	O
h0i	O
,	O
having	O
say	O
n0	O
members	O
.	O
then	O
fwer	O
d	O
prfreject	O
any	O
true	O
h0ig	O
:	O
n	O
pi	O
	O
˛	O
9=	O
;	O
x	O
8	O
<	O
:	O
[	O
(	O
cid:16	O
)	O
pi	O
	O
˛	O
	O
˛	O
;	O
n	O
pr	O
i0	O
n	O
o	O
fwer	O
d	O
pr	O
i0	O
d	O
n0	O
˛	O
n	O
2	O
the	O
top	O
line	O
following	O
from	O
boole	O
’	O
s	O
inequality	O
(	O
which	O
doesn	O
’	O
t	B
require	O
even	O
independence	O
among	O
the	O
pi	O
)	O
.	O
the	O
bonferroni	O
bound	B
is	O
quite	O
conservative	O
:	O
for	O
n	O
d	O
6033	O
and	O
˛	O
d	O
0:05	O
we	O
reject	O
only	O
those	O
cases	O
having	O
pi	O
	O
8:3	O
(	O
cid:1	O
)	O
10	O
(	O
cid:0	O
)	O
6.	O
one	O
can	O
do	O
only	O
a	O
little	O
better	O
under	O
the	O
fwer	O
constraint	O
.	O
“	O
holm	O
’	O
s	O
procedure	O
,	O
”	O
which	O
offers	O
modest	O
improvement	O
over	O
bonferroni	O
,	O
goes	O
as	O
follows	O
.	O
(	O
cid:15	O
)	O
order	O
the	O
observed	O
p-values	O
from	O
smallest	O
to	O
largest	O
,	O
p.1/	O
	O
p.2/	O
	O
p.3/	O
	O
:	O
:	O
:	O
	O
p.i	O
/	O
	O
:	O
:	O
:	O
	O
p.n	O
/	O
;	O
(	O
15.9	O
)	O
with	O
h0.i	O
/	O
denoting	O
the	O
corresponding	O
null	O
hypotheses	O
.	O
(	O
cid:15	O
)	O
let	O
i0	O
be	O
the	O
smallest	O
index	O
i	O
such	O
that	O
p.i	O
/	O
>	O
˛=.n	O
(	O
cid:0	O
)	O
i	O
c	O
1/	O
:	O
(	O
15.10	O
)	O
(	O
cid:15	O
)	O
reject	O
all	O
null	O
hypotheses	O
h0.i	O
/	O
for	O
i	O
<	O
i0	O
and	O
accept	O
all	O
with	O
i	O
(	O
cid:21	O
)	O
i0	O
.	O
4	O
the	O
left-sided	O
p-value	B
is	O
p	O
d	O
f0.z/	O
.	O
we	O
will	O
avoid	O
two-sided	O
p-values	O
in	O
this	O
discussion	O
.	O
15.2	O
false-discovery	O
rates	O
275	O
it	O
can	O
be	O
shown	O
that	O
holm	O
’	O
s	O
procedure	O
controls	O
fwer	O
at	O
level	O
˛	O
,	O
while	O
being	O
slightly	O
more	O
generous	O
than	O
bonferroni	O
in	O
declaring	O
rejections	O
.	O
15.2	O
false-discovery	O
rates	O
the	O
fwer	O
criterion	O
aims	O
to	O
control	B
the	O
probability	O
of	O
making	O
even	O
one	O
false	O
rejection	O
among	O
n	O
simultaneous	O
hypothesis	O
tests	O
.	O
originally	O
devel-	O
oped	O
for	O
small-scale	O
testing	B
,	O
say	O
n	O
	O
20	O
,	O
fwer	O
usually	O
proved	O
too	O
con-	O
servative	O
for	O
scientists	O
working	O
with	O
n	O
in	O
the	O
thousands	O
.	O
a	O
quite	O
different	O
and	O
more	O
liberal	O
criterion	O
,	O
false-discovery	B
rate	I
(	O
fdr	O
)	O
control	B
,	O
has	O
become	O
standard	O
.	O
figure	O
15.2	O
a	O
decision	O
rule	B
d	O
has	O
rejected	O
r	O
out	O
of	O
n	O
null	O
hypotheses	O
;	O
a	O
of	O
these	O
decisions	O
were	O
incorrect	O
,	O
i.e.	O
,	O
they	O
were	O
“	O
false	O
discoveries	O
,	O
”	O
while	O
b	O
of	O
them	O
were	O
“	O
true	O
discoveries.	O
”	O
the	O
false-discovery	O
proportion	O
fdp	O
equals	O
a=r	O
.	O
figure	O
15.2	O
diagrams	O
the	O
outcome	O
of	O
a	O
hypothetical	O
decision	O
rule	B
d	O
ap-	O
plied	O
to	O
the	O
data	B
for	O
n	O
simultaneous	O
hypothesis-testing	O
problems	O
,	O
n0	O
null	O
and	O
n1	O
d	O
n	O
(	O
cid:0	O
)	O
n0	O
non-null	O
.	O
an	O
omniscient	O
oracle	O
has	O
reported	O
the	O
rule	B
’	O
s	O
results	O
:	O
r	O
null	O
hypotheses	O
have	O
been	O
rejected	O
;	O
a	O
of	O
these	O
were	O
cases	O
of	O
false	O
discovery	O
,	O
i.e.	O
,	O
valid	O
null	O
hypotheses	O
,	O
for	O
a	O
“	O
false-discovery	O
propor-	O
tion	O
”	O
(	O
fdp	O
)	O
of	O
fdp.d/	O
d	O
a=r	O
:	O
(	O
15.11	O
)	O
(	O
we	O
deﬁne	O
fdp	O
d	O
0	O
if	O
r	O
d	O
0	O
.	O
)	O
fdp	O
is	O
unobservable—without	O
the	O
oracle	O
we	O
can	O
not	O
see	O
a—but	O
under	O
certain	O
assumptions	O
we	O
can	O
control	B
its	O
expec-	O
tation	O
.	O
4	O
null	O
actual	O
non-null	O
null	O
n0	O
-	O
a	O
non-null	O
a	O
b	O
n1	O
-	O
b	O
n	O
-	O
r	O
r	O
n	O
n1	O
n0	O
decision	O
276	O
large-scale	O
hypothesis	O
testing	B
and	O
fdrs	O
deﬁne	O
fdr.d/	O
d	O
e	O
ffdp.d/g	O
:	O
(	O
15.12	O
)	O
a	O
decision	O
rule	B
d	O
controls	O
fdr	O
at	O
level	O
q	O
,	O
with	O
q	O
a	O
prechosen	O
value	O
be-	O
tween	O
0	O
and	O
1	O
,	O
if	O
(	O
15.13	O
)	O
fdr.d/	O
	O
q	O
:	O
it	O
might	O
seem	O
difﬁcult	O
to	O
ﬁnd	O
such	O
a	O
rule	B
,	O
but	O
in	O
fact	O
a	O
quite	O
simple	O
but	O
in-	O
genious	O
recipe	O
does	O
the	O
job	O
.	O
ordering	O
the	O
observed	O
p-values	O
from	O
smallest	O
to	O
largest	O
as	O
in	O
(	O
15.9	O
)	O
,	O
deﬁne	O
imax	O
to	O
be	O
the	O
largest	O
index	O
for	O
which	O
p.i	O
/	O
	O
i	O
n	O
q	O
;	O
(	O
15.14	O
)	O
3	O
and	O
let	O
dq	O
be	O
the	O
rule5	O
that	O
rejects	O
h0.i	O
/	O
for	O
i	O
	O
imax	O
,	O
accepting	O
otherwise	O
.	O
a	O
proof	O
of	O
the	O
following	O
theorem	B
is	O
referenced	O
in	O
the	O
chapter	O
endnotes.	O
theorem	B
(	O
benjamini–hochberg	O
fdr	O
control	B
)	O
ing	O
to	O
valid	O
null	O
hypotheses	O
are	O
independent	O
of	O
each	O
other	O
,	O
then	O
if	O
the	O
p-values	O
correspond-	O
fdr.dq/	O
d	O
(	O
cid:25	O
)	O
0q	O
	O
q	O
;	O
where	O
(	O
cid:25	O
)	O
0	O
d	O
n0=n	O
:	O
(	O
15.15	O
)	O
in	O
other	O
words	O
dq	O
controls	O
fdr	O
at	O
level	O
(	O
cid:25	O
)	O
0q	O
.	O
the	O
null	O
proportion	B
(	O
cid:25	O
)	O
0	O
is	O
unknown	O
(	O
though	O
estimable	O
)	O
,	O
so	O
the	O
usual	O
claim	O
is	O
that	O
dq	O
controls	O
fdr	O
at	O
level	O
q.	O
not	O
much	O
is	O
sacriﬁced	O
:	O
large-scale	B
testing	I
problems	O
are	O
most	O
often	O
ﬁshing	O
expeditions	O
in	O
which	O
most	O
of	O
the	O
cases	O
are	O
null	O
,	O
putting	O
(	O
cid:25	O
)	O
0	O
near	O
1	O
,	O
identiﬁcation	O
of	O
a	O
few	O
non-null	O
cases	O
being	O
the	O
goal	O
.	O
the	O
choice	O
q	O
d	O
0:1	O
is	O
typical	O
practice	O
.	O
the	O
popularity	O
of	O
fdr	O
control	B
hinges	O
on	O
the	O
fact	O
that	O
it	O
is	O
more	O
generous	O
than	O
fwer	O
in	O
declaring	O
signiﬁcance.6	O
holm	O
’	O
s	O
procedure	O
(	O
15.10	O
)	O
rejects	O
null	O
hypothesis	O
h0.i	O
/	O
if	O
p.i	O
/	O
	O
threshold	O
(	O
holm	O
’	O
s	O
)	O
d	O
˛	O
n	O
(	O
cid:0	O
)	O
i	O
c	O
1	O
;	O
(	O
15.16	O
)	O
while	O
dq	O
(	O
15.13	O
)	O
has	O
threshold	O
p.i	O
/	O
	O
threshold	O
(	O
dq	O
)	O
d	O
q	O
n	O
i	O
:	O
(	O
15.17	O
)	O
5	O
sometimes	O
denoted	O
“	O
bhq	O
”	O
after	O
its	O
inventors	O
benjamini	O
and	O
hochberg	O
;	O
see	O
the	O
chapter	O
endnotes	O
.	O
6	O
the	O
classic	O
term	O
“	O
signiﬁcant	O
”	O
for	O
a	O
non-null	O
identiﬁcation	O
doesn	O
’	O
t	B
seem	O
quite	O
right	O
for	O
fdr	O
control	B
,	O
especially	O
given	O
the	O
bayesian	O
connections	O
of	O
section	O
15.3	O
,	O
and	O
we	O
will	O
sometimes	O
use	O
“	O
interesting	O
”	O
instead	O
.	O
15.2	O
false-discovery	O
rates	O
277	O
in	O
the	O
usual	O
range	O
of	O
interest	O
,	O
large	O
n	O
and	O
small	O
i	O
,	O
the	O
ratio	O
threshold	O
(	O
dq	O
)	O
threshold	O
(	O
holm	O
’	O
s	O
)	O
d	O
q	O
˛	O
increases	O
almost	O
linearly	O
with	O
i	O
.	O
	O
1	O
(	O
cid:0	O
)	O
i	O
(	O
cid:0	O
)	O
1	O
	O
i	O
(	O
15.18	O
)	O
n	O
figure	O
15.3	O
ordered	O
p-values	O
p.i	O
/	O
d	O
1	O
(	O
cid:0	O
)	O
ˆ.z.i	O
//	O
plotted	O
versus	O
i	O
for	O
the	O
50	O
largest	O
z-values	O
from	O
the	O
prostate	B
data	O
in	O
figure	O
15.1.	O
the	O
fdr	O
control	B
boundary	O
(	O
algorithm	B
dq	O
,	O
q	O
d	O
0:1	O
)	O
rejects	O
h0.i	O
/	O
for	O
the	O
28	O
smallest	O
values	O
p.i	O
/	O
,	O
while	O
holm	O
’	O
s	O
fwer	O
procedure	O
(	O
˛	O
d	O
0:1	O
)	O
rejects	O
for	O
only	O
the	O
7	O
smallest	O
values	O
.	O
(	O
the	O
upward	O
slope	O
of	O
holm	O
’	O
s	O
boundary	O
(	O
15.16	O
)	O
is	O
too	O
small	O
to	O
see	O
here	O
.	O
)	O
figure	O
15.3	O
illustrates	O
the	O
comparison	O
for	O
the	O
right	O
tail	O
of	O
the	O
prostate	B
data	O
of	O
figure	O
15.1	O
,	O
with	O
pi	O
d	O
1	O
(	O
cid:0	O
)	O
ˆ.zi	O
/	O
(	O
15.3	O
)	O
,	O
(	O
15.5	O
)	O
,	O
and	O
˛	O
d	O
q	O
d	O
0:1.	O
the	O
fdr	O
procedure	O
rejects	O
h0.i	O
/	O
for	O
the	O
28	O
largest	O
z-values	O
(	O
z.i	O
/	O
(	O
cid:21	O
)	O
3:33	O
)	O
,	O
while	O
fwer	O
control	B
rejects	O
only	O
the	O
7	O
most	O
extreme	O
z-values	O
(	O
z.i	O
/	O
(	O
cid:21	O
)	O
4:14	O
)	O
.	O
hypothesis	B
testing	I
has	O
been	O
a	O
traditional	O
stronghold	O
of	O
frequentist	O
deci-	O
sion	O
theory	B
,	O
with	O
“	O
type	O
1	O
”	O
error	O
control	B
being	O
strictly	O
enforced	O
,	O
very	O
often	O
at	O
the	O
0.05	O
level	O
.	O
it	O
is	O
surprising	O
that	O
a	O
new	O
control	B
criterion	O
,	O
fdr	O
,	O
has	O
taken	O
hold	O
in	O
large-scale	B
testing	I
situations	O
.	O
a	O
critic	O
,	O
noting	O
fdr	O
’	O
s	O
relaxed	O
rejection	O
standards	O
in	O
figure	O
15.3	O
,	O
might	O
raise	O
some	O
pointed	O
questions	O
.	O
**************************************************010203040500e+005e−041e−03index	O
ip−valueholm'sfdri	O
=	O
7i	O
=	O
28	O
278	O
large-scale	O
hypothesis	O
testing	B
and	O
fdrs	O
1	O
is	O
controlling	O
a	O
rate	B
(	O
i.e.	O
,	O
fdr	O
)	O
as	O
meaningful	O
as	O
controlling	O
a	O
probabil-	O
ity	O
(	O
of	O
type	O
1	O
error	O
)	O
?	O
4	O
the	O
fdr	O
signiﬁcance	O
for	O
gene	O
i0	O
,	O
say	O
one	O
with	O
zi0	O
isn	O
’	O
t	B
this	O
unlikely	O
in	O
situations	O
such	O
as	O
the	O
prostate	B
study	O
?	O
2	O
how	O
should	O
q	O
be	O
chosen	O
?	O
3	O
the	O
control	B
theorem	I
depends	O
on	O
independence	O
among	O
the	O
p-values	O
.	O
d	O
3	O
,	O
depends	O
on	O
the	O
results	O
of	O
all	O
the	O
other	O
genes	O
:	O
the	O
more	O
“	O
other	O
”	O
zi	O
values	O
exceed	O
3	O
,	O
the	O
more	O
interesting	O
gene	O
i0	O
becomes	O
(	O
since	O
that	O
increases	O
i0	O
’	O
s	O
index	O
i	O
in	O
the	O
ordered	O
list	O
(	O
15.9	O
)	O
,	O
making	O
it	O
more	O
likely	O
that	O
pi0	O
lies	O
below	O
the	O
dq	O
threshold	O
(	O
15.14	O
)	O
)	O
.	O
does	O
this	O
make	O
inferential	O
sense	O
?	O
a	O
bayes/empirical	O
bayes	O
restatement	O
of	O
the	O
dq	O
algorithm	B
helps	O
answer	O
these	O
questions	O
,	O
as	O
discussed	O
next	O
.	O
15.3	O
empirical	B
bayes	O
large-scale	B
testing	I
in	O
practice	O
,	O
single-case	O
hypothesis	B
testing	I
has	O
been	O
a	O
frequentist	O
preserve	O
.	O
its	O
methods	O
demand	O
little	O
from	O
the	O
scientist—only	O
the	O
choice	O
of	O
a	O
test	O
statistic	B
and	O
the	O
calculation	O
of	O
its	O
null	O
distribution—while	O
usually	O
deliver-	O
ing	O
a	O
clear	O
verdict	O
.	O
by	O
contrast	O
,	O
bayesian	O
model	B
selection	I
,	O
whatever	O
its	O
in-	O
ferential	O
virtues	O
,	O
raises	O
the	O
kinds	O
of	O
difﬁcult	O
modeling	O
questions	O
discussed	O
in	O
section	O
13.3.	O
it	O
then	O
comes	O
as	O
a	O
pleasant	O
surprise	O
that	O
things	O
are	O
different	O
for	O
large-	O
scale	B
testing	O
:	O
bayesian	O
methods	O
,	O
at	O
least	O
in	O
their	O
empirical	B
bayes	O
manifes-	O
tation	O
,	O
no	O
longer	O
demand	O
heroic	O
modeling	O
efforts	O
,	O
and	O
can	O
help	O
untangle	O
the	O
interpretation	O
of	O
simultaneous	O
test	O
results	O
.	O
this	O
is	O
particularly	O
true	O
for	O
the	O
fdr	O
control	B
algorithm	O
dq	O
of	O
the	O
previous	O
section	O
.	O
a	O
simple	O
bayesian	O
framework	O
for	O
simultaneous	O
testing	B
is	O
provided	O
by	O
the	O
two-groups	O
model	B
:	O
each	O
of	O
the	O
n	O
cases	O
(	O
the	O
genes	O
for	O
the	O
prostate	B
study	O
)	O
is	O
either	O
null	O
with	O
prior	B
probability	O
(	O
cid:25	O
)	O
0	O
or	O
non-null	O
with	O
probabil-	O
ity	O
(	O
cid:25	O
)	O
1	O
d	O
1	O
(	O
cid:0	O
)	O
(	O
cid:25	O
)	O
0	O
;	O
the	O
resulting	O
observation	O
z	O
then	O
has	O
density	B
either	O
f0.z/	O
or	O
f1.z/	O
,	O
(	O
cid:25	O
)	O
0	O
d	O
prfnullg	O
(	O
cid:25	O
)	O
1	O
d	O
prfnon-nullg	O
p	O
for	O
the	O
prostate	B
study	O
,	O
(	O
cid:25	O
)	O
0	O
is	O
nearly	O
1	O
,	O
and	O
f0.z/	O
is	O
the	O
standard	O
normal	O
den-	O
sity	O
(	O
cid:30	O
)	O
.z/	O
d	O
exp	O
.	O
(	O
cid:0	O
)	O
z2=2/=	O
2	O
(	O
cid:25	O
)	O
(	O
15.3	O
)	O
,	O
while	O
the	O
non-null	O
density	B
remains	O
to	O
be	O
estimated	O
.	O
f0.z/	O
density	B
if	O
null	O
;	O
f1.z/	O
density	B
if	O
non-null	O
:	O
(	O
15.19	O
)	O
let	O
f0.z/	O
and	O
f1.z/	O
be	O
the	O
cdf	B
values	O
corresponding	O
to	O
f0.z/	O
and	O
f1.z/	O
,	O
15.3	O
empirical	B
bayes	O
large-scale	B
testing	I
279	O
with	O
“	O
survival	O
curves	O
”	O
s0.z/	O
d	O
1	O
(	O
cid:0	O
)	O
f0.z/	O
and	O
s1.z/	O
d	O
1	O
(	O
cid:0	O
)	O
f1.z/	O
;	O
(	O
15.20	O
)	O
s0.z0/	O
being	O
the	O
probability	O
that	O
a	O
null	O
z-value	O
exceeds	O
z0	O
,	O
and	O
similarly	O
for	O
s1.z/	O
.	O
finally	O
,	O
deﬁne	O
s.z/	O
to	O
be	O
the	O
mixture	O
survival	O
curve	O
(	O
15.21	O
)	O
(	O
15.22	O
)	O
(	O
15.23	O
)	O
the	O
mixture	O
density	B
determines	O
s.z/	O
,	O
s.z/	O
d	O
(	O
cid:25	O
)	O
0s0.z/	O
c	O
(	O
cid:25	O
)	O
1s1.z/	O
:	O
f	O
.z/	O
d	O
(	O
cid:25	O
)	O
0f0.z/	O
c	O
(	O
cid:25	O
)	O
1f1.z/	O
s.z0/	O
dz	O
1	O
z0	O
f	O
.z/	O
dz	O
:	O
suppose	O
now	O
that	O
observation	O
zi	O
for	O
case	O
i	O
is	O
seen	O
to	O
exceed	O
some	O
thresh-	O
old	O
value	O
z0	O
,	O
perhaps	O
z0	O
d	O
3.	O
bayes	O
’	O
rule	B
gives	O
fdr.z0/	O
	O
prfcase	O
i	O
is	O
nulljzi	O
(	O
cid:21	O
)	O
z0g	O
d	O
(	O
cid:25	O
)	O
0s0.z0/=s.z0/	O
;	O
(	O
15.24	O
)	O
the	O
correspondence	O
with	O
(	O
3.5	O
)	O
on	O
page	O
23	O
being	O
(	O
cid:25	O
)	O
0	O
d	O
g.	O
(	O
cid:22	O
)	O
/	O
,	O
s0.z0/	O
d	O
f	O
(	O
cid:22	O
)	O
.x/	O
,	O
and	O
s.z0/	O
d	O
f	O
.x/	O
.	O
fdr	O
is	O
the	O
“	O
bayes	O
false-discovery	B
rate	I
,	O
”	O
as	O
con-	O
trasted	O
with	O
the	O
frequentist	O
quantity	B
fdr	O
(	O
15.12	O
)	O
.	O
in	O
typical	O
applications	O
,	O
s0.z0/	O
is	O
assumed	O
known7	O
(	O
equaling	O
1	O
(	O
cid:0	O
)	O
ˆ.z0/	O
in	O
the	O
prostate	B
study	O
)	O
,	O
and	O
(	O
cid:25	O
)	O
0	O
is	O
assumed	O
to	O
be	O
near	O
1.	O
the	O
denominator	O
s.z0/	O
in	O
(	O
15.24	O
)	O
is	O
unknown	O
,	O
but—and	O
this	O
is	O
the	O
crucial	O
point—it	O
has	O
an	O
obvious	O
estimate	B
in	O
large-scale	B
testing	I
situations	O
,	O
namely	O
os	O
.z0/	O
d	O
n.z0/=n	O
;	O
where	O
n.z0/	O
d	O
#	O
fzi	O
(	O
cid:21	O
)	O
z0g	O
:	O
(	O
15.25	O
)	O
(	O
by	O
the	O
deﬁnition	O
of	O
the	O
two-group	O
model	B
,	O
each	O
zi	O
has	O
marginal	O
density	B
f	O
.z/	O
,	O
making	O
os	O
.z0/	O
the	O
usual	O
empirical	B
estimate	O
of	O
s.z0/	O
(	O
15.23	O
)	O
.	O
)	O
plug-	O
ging	O
into	O
(	O
15.24	O
)	O
yields	O
an	O
empirical	B
bayes	O
estimate	O
of	O
the	O
bayes	O
false-	O
discovery	O
rate	B
cfdr.z0/	O
d	O
(	O
cid:25	O
)	O
0s0.z0/ı	O
os	O
.z0/	O
:	O
(	O
15.26	O
)	O
the	O
connection	O
with	O
fdr	O
control	B
is	O
almost	O
immediate	O
.	O
first	O
of	O
all	O
,	O
from	O
deﬁnitions	O
(	O
15.5	O
)	O
and	O
(	O
15.20	O
)	O
we	O
have	O
pi	O
d	O
s0.zi	O
/	O
;	O
also	O
for	O
the	O
ith	O
from	O
the	O
largest	O
z-value	O
we	O
have	O
os	O
.z.i	O
//	O
d	O
i=n	O
(	O
15.25	O
)	O
.	O
putting	O
these	O
together	O
,	O
condition	B
(	O
15.14	O
)	O
,	O
p.i	O
/	O
	O
.i=n	O
/q	O
,	O
becomes	O
s0.z.i	O
//	O
	O
os	O
.z.i	O
//	O
(	O
cid:1	O
)	O
q	O
;	O
(	O
15.27	O
)	O
7	O
but	O
see	O
section	O
15.5	O
.	O
280	O
large-scale	O
hypothesis	O
testing	B
and	O
fdrs	O
or	O
s0.z.i	O
//=	O
os	O
.z.i	O
//	O
	O
q	O
,	O
which	O
can	O
be	O
written	O
as	O
cfdr.z.i	O
//	O
	O
(	O
cid:25	O
)	O
0q	O
(	O
15.28	O
)	O
(	O
15.26	O
)	O
.	O
in	O
other	O
words	O
,	O
the	O
dq	O
algorithm	B
,	O
which	O
rejects	O
those	O
null	O
hy-	O
potheses	O
having8	O
p.i	O
/	O
	O
.i=n	O
/q	O
,	O
is	O
in	O
fact	O
rejecting	O
those	O
cases	O
for	O
which	O
the	O
empirical	B
bayes	O
posterior	B
probability	I
of	O
nullness	O
is	O
too	O
small	O
,	O
as	O
de-	O
ﬁned	O
by	O
(	O
15.28	O
)	O
.	O
the	O
bayesian	O
nature	O
of	O
fdr	O
control	B
offers	O
a	O
clear	O
advan-	O
tage	O
to	O
the	O
investigating	O
scientist	O
,	O
who	O
gets	O
a	O
numerical	O
assessment	O
of	O
the	O
probability	O
that	O
he	O
or	O
she	O
will	O
be	O
wasting	O
time	O
following	O
up	O
any	O
one	O
of	O
the	O
selected	O
cases	O
.	O
we	O
can	O
now	O
respond	O
to	O
the	O
four	O
questions	O
at	O
the	O
end	O
of	O
the	O
previous	O
section	O
:	O
1	O
fdr	O
control	B
does	O
relate	O
to	O
a	O
probability—the	O
bayes	O
posterior	O
probabil-	O
ity	O
of	O
nullness	O
.	O
2	O
the	O
choice	O
of	O
q	O
for	O
dq	O
amounts	O
to	O
setting	O
the	O
maximum	O
tolerable	O
amount	O
of	O
bayes	O
risk	O
of	O
nullness9	O
(	O
usually	O
after	O
taking	O
(	O
cid:25	O
)	O
0	O
d	O
1	O
in	O
(	O
15.28	O
)	O
)	O
.	O
s.z0/	O
,	O
makingcfdr.z0/	O
(	O
15.26	O
)	O
nearly	O
unbiased	O
for	O
fdr.z0/	O
(	O
15.24	O
)	O
.	O
there	O
3	O
most	O
often	O
the	O
zi	O
,	O
and	O
hence	O
the	O
pi	O
,	O
will	O
be	O
correlated	O
with	O
each	O
other	O
.	O
even	O
under	O
correlation	O
,	O
however	O
,	O
os	O
.z0/	O
in	O
(	O
15.25	O
)	O
is	O
still	O
unbiased	O
for	O
s0.z0/	O
and	O
cfdr.z0/	O
.	O
is	O
a	O
price	O
to	O
be	O
paid	O
for	O
correlation	O
,	O
which	O
increases	O
the	O
variance	O
of	O
4	O
in	O
the	O
bayes	O
two-groups	O
model	B
(	O
15.19	O
)	O
,	O
all	O
of	O
the	O
non-null	O
zi	O
are	O
i.i.d	O
.	O
observations	O
from	O
the	O
non-null	O
density	B
f1.z/	O
,	O
with	O
survival	O
curve	O
s1.z/	O
.	O
the	O
number	O
of	O
null	O
cases	O
zi	O
exceeding	O
some	O
threshold	O
z0	O
has	O
ﬁxed	O
ex-	O
pectation	O
n	O
(	O
cid:25	O
)	O
0s0.z0/	O
.	O
therefore	O
an	O
increase	O
in	O
the	O
number	O
of	O
observed	O
values	O
zi	O
exceeding	O
z0	O
must	O
come	O
from	O
a	O
heavier	O
right	O
tail	O
for	O
f1.z/	O
,	O
implying	O
a	O
greater	O
posterior	B
probability	I
of	O
non-nullness	O
fdr.z0/	O
(	O
15.24	O
)	O
.	O
this	O
point	O
is	O
made	O
more	O
clearly	O
in	O
the	O
local	O
false-discovery	O
framework	O
of	O
the	O
next	O
section	O
.	O
it	O
emphasizes	O
the	O
“	O
learning	O
from	O
the	O
experience	O
of	O
others	O
”	O
aspect	O
of	O
empirical	B
bayes	O
inference	B
,	O
section	O
7.4.	O
the	O
question	O
of	O
“	O
which	O
others	O
?	O
”	O
is	O
returned	O
to	O
in	O
section	O
15.6.	O
figure	O
15.4	O
illustrates	O
the	O
two-group	O
model	B
(	O
15.19	O
)	O
.	O
the	O
n	O
cases	O
are	O
8	O
the	O
algorithm	B
,	O
as	O
stated	O
just	O
before	O
the	O
fdr	O
control	B
theorem	I
(	O
15.15	O
)	O
,	O
is	O
actually	O
a	O
little	O
more	O
liberal	O
in	O
allowing	O
rejections	O
.	O
9	O
for	O
a	O
case	O
of	O
particular	O
interest	O
,	O
the	O
calculation	O
can	O
be	O
reversed	O
:	O
if	O
the	O
case	O
has	O
ordered	O
index	O
i	O
then	O
,	O
according	O
to	O
(	O
15.14	O
)	O
,	O
the	O
value	O
q	O
d	O
npi	O
=	O
i	O
puts	O
it	O
exactly	O
on	O
the	O
boundary	O
of	O
rejection	O
,	O
making	O
this	O
its	O
q-value	B
.	O
the	O
50th	O
largest	O
z-value	O
for	O
the	O
prostate	B
data	O
has	O
zi	O
d	O
2:99	O
,	O
pi	O
d	O
0:00139	O
,	O
and	O
q-value	O
0.168	O
,	O
that	O
being	O
both	O
the	O
frequentist	O
boundary	O
for	O
rejection	O
and	O
the	O
empirical	B
bayes	O
probability	O
of	O
nullness	O
.	O
15.3	O
empirical	B
bayes	O
large-scale	B
testing	I
281	O
figure	O
15.4	O
a	O
diagram	O
of	O
the	O
two-groups	O
model	B
(	O
15.19	O
)	O
.	O
here	O
the	O
statistician	O
observes	O
values	O
zi	O
from	O
a	O
mixture	O
density	B
f	O
.z/	O
d	O
(	O
cid:25	O
)	O
0f0.z/	O
c	O
(	O
cid:25	O
)	O
1f1.z/	O
and	O
decides	O
to	O
reject	O
or	O
accept	O
the	O
null	O
hypothesis	O
h0i	O
depending	O
on	O
whether	O
zi	O
exceeds	O
or	O
is	O
less	O
than	O
the	O
threshold	O
value	O
z0	O
.	O
(	O
w	O
d	O
randomly	O
dispatched	O
to	O
the	O
two	O
arms	O
in	O
proportions	O
(	O
cid:25	O
)	O
0	O
and	O
(	O
cid:25	O
)	O
1	O
,	O
at	O
which	O
point	O
they	O
produce	O
z-values	O
according	O
to	O
either	O
f0.z/	O
or	O
f1.z/	O
.	O
suppose	O
we	O
are	O
using	O
a	O
simple	O
decision	O
rule	B
d	O
that	O
rejects	O
the	O
ith	O
null	O
hypothesis	O
if	O
zi	O
exceeds	O
some	O
threshold	O
z0	O
,	O
and	O
accepts	O
otherwise	O
,	O
reject	O
h0i	O
accept	O
h0i	O
if	O
zi	O
>	O
z0	O
if	O
zi	O
	O
z0	O
:	O
(	O
15.29	O
)	O
the	O
oracle	O
of	O
figure	O
15.2	O
knows	O
that	O
n0.z0/	O
d	O
a	O
of	O
the	O
null	O
case	O
z-	O
values	O
exceeded	O
z0	O
,	O
and	O
similarly	O
n1.z0/	O
d	O
b	O
of	O
the	O
non-null	O
cases	O
,	O
lead-	O
ing	O
to	O
n.z0/	O
d	O
n0.z0/	O
c	O
n1.z0/	O
d	O
r	O
(	O
15.30	O
)	O
(	O
15.31	O
)	O
total	O
rejections	O
.	O
the	O
false-discovery	O
proportion	O
(	O
15.11	O
)	O
is	O
fdp	O
d	O
n0.z0/	O
n.z0/	O
but	O
this	O
is	O
unobservable	O
since	O
we	O
see	O
only	O
n.z0/	O
.	O
the	O
clever	O
inferential	O
strategy	O
of	O
false-discovery	B
rate	I
theory	O
substitutes	O
the	O
expectation	O
of	O
n0.z0/	O
,	O
e	O
fn0.z0/g	O
d	O
n	O
(	O
cid:25	O
)	O
0s0.z0/	O
;	O
(	O
15.32	O
)	O
282	O
large-scale	O
hypothesis	O
testing	B
and	O
fdrs	O
d	O
cfdr.z0/	O
;	O
for	O
n0.z0/	O
in	O
(	O
15.31	O
)	O
,	O
giving	O
dfdp	O
d	O
n	O
(	O
cid:25	O
)	O
0s0.z0/	O
d	O
(	O
cid:25	O
)	O
0s0.z0/	O
os	O
.z0/	O
(	O
15.33	O
)	O
n.z0/	O
using	O
(	O
15.25	O
)	O
and	O
(	O
15.26	O
)	O
.	O
starting	O
from	O
the	O
two-groups	O
model	B
,	O
cfdr.z0/	O
is	O
an	O
obvious	O
empirical	B
(	O
i.e.	O
,	O
frequentist	O
)	O
estimate	O
of	O
the	O
bayesian	O
probability	O
fdr.z0/	O
,	O
as	O
well	O
as	O
of	O
fdp	O
.	O
if	O
placed	O
in	O
the	O
bayes–fisher–frequentist	O
triangle	O
of	O
figure	O
14.1	O
,	O
false-	O
discovery	O
rates	O
would	O
begin	O
life	O
near	O
the	O
frequentist	O
corner	O
but	O
then	O
mi-	O
grate	O
at	O
least	O
part	O
of	O
the	O
way	O
toward	O
the	O
bayes	O
corner	O
.	O
there	O
are	O
remark-	O
able	O
parallels	O
with	O
the	O
james–stein	O
estimator	B
of	O
chapter	O
7.	O
both	O
theories	O
began	O
with	O
a	O
striking	O
frequentist	O
theorem	B
,	O
which	O
was	O
then	O
inferentially	O
rationalized	O
in	O
empirical	B
bayes	O
terms	O
.	O
both	O
rely	O
on	O
the	O
use	O
of	O
indirect	O
evidence—learning	O
from	O
the	O
experience	O
of	O
others	O
.	O
the	O
difference	O
is	O
that	O
james–stein	O
estimation	B
always	O
aroused	O
controversy	O
,	O
while	O
fdr	O
control	B
has	O
been	O
quickly	O
welcomed	O
into	O
the	O
pantheon	O
of	O
widely	O
used	O
methods	O
.	O
this	O
could	O
reﬂect	O
a	O
change	O
in	O
twenty-ﬁrst-century	O
attitudes	O
or	O
,	O
perhaps	O
,	O
only	O
that	O
the	O
dq	O
rule	B
better	O
conceals	O
its	O
bayesian	O
aspects	O
.	O
15.4	O
local	O
false-discovery	O
rates	O
tail-area	O
statistics	B
(	O
p-values	O
)	O
were	O
synonymous	O
with	O
classic	O
one-at-a-time	O
hypothesis	B
testing	I
,	O
and	O
the	O
dq	O
algorithm	B
carried	O
over	O
p-value	B
interpreta-	O
tion	O
to	O
large-scale	B
testing	I
theory	O
.	O
but	O
tail-area	O
calculations	O
are	O
neither	O
nec-	O
essary	O
nor	O
desirable	O
from	O
a	O
bayesian	O
viewpoint	O
,	O
where	O
,	O
having	O
observed	O
test	O
statistic	B
zi	O
equal	O
to	O
some	O
value	O
z0	O
,	O
we	O
should	O
be	O
more	O
interested	O
in	O
the	O
probability	O
of	O
nullness	O
given	O
zi	O
d	O
z0	O
than	O
given	O
zi	O
(	O
cid:21	O
)	O
z0	O
.	O
to	O
this	O
end	O
we	O
deﬁne	O
the	O
local	O
false-discovery	B
rate	I
fdr.z0/	O
d	O
prfcase	O
i	O
is	O
nulljzi	O
d	O
z0g	O
(	O
15.34	O
)	O
as	O
opposed	O
to	O
the	O
tail-area	O
false-discovery	B
rate	I
fdr.z0/	O
(	O
15.24	O
)	O
.	O
the	O
main	O
point	O
of	O
what	O
follows	O
is	O
that	O
reasonably	O
accurate	O
empirical	B
bayes	O
estimates	O
of	O
fdr	O
are	O
available	O
in	O
large-scale	B
testing	I
problems	O
.	O
as	O
a	O
ﬁrst	O
try	O
,	O
suppose	O
that	O
z0	O
,	O
a	O
proposed	O
region	B
for	O
rejecting	O
null	O
hy-	O
potheses	O
,	O
is	O
a	O
small	O
interval	B
centered	O
at	O
z0	O
,	O
z0	O
d	O
(	O
cid:21	O
)	O
z0	O
(	O
cid:0	O
)	O
d	O
2	O
;	O
z0	O
c	O
d	O
2	O
;	O
(	O
15.35	O
)	O
with	O
d	O
perhaps	O
0.1.	O
we	O
can	O
redraw	O
figure	O
15.4	O
,	O
now	O
with	O
n0.z0/	O
,	O
n1.z0/	O
,	O
15.4	O
local	O
false-discovery	O
rates	O
283	O
and	O
n.z0/	O
the	O
null	O
,	O
non-null	O
,	O
and	O
total	O
number	O
of	O
z-values	O
in	O
z0	O
.	O
the	O
local	O
false-discovery	O
proportion	O
,	O
(	O
15.36	O
)	O
is	O
unobservable	O
,	O
but	O
we	O
can	O
replace	O
n0.z0/	O
with	O
n	O
(	O
cid:25	O
)	O
0f0.z0/d	O
,	O
its	O
approx-	O
imate	O
expectation	O
as	O
in	O
(	O
15.31	O
)	O
–	O
(	O
15.33	O
)	O
,	O
yielding	O
the	O
estimate10	O
fdp.z0/	O
d	O
n0.z0/=n.z0/	O
cfdr.z0/	O
d	O
n	O
(	O
cid:25	O
)	O
0f0.z0/d=n.z0/	O
:	O
estimate	B
(	O
15.37	O
)	O
would	O
be	O
needlessly	O
noisy	O
in	O
practice	O
;	O
z-value	O
distri-	O
butions	O
tend	O
to	O
be	O
smooth	O
,	O
allowing	O
the	O
use	O
of	O
regression	B
estimates	O
for	O
fdr.z0/	O
.	O
bayes	O
’	O
theorem	B
gives	O
fdr.z/	O
d	O
(	O
cid:25	O
)	O
0f0.z/=f	O
.z/	O
(	O
15.37	O
)	O
(	O
15.38	O
)	O
in	O
the	O
two-groups	O
model	B
(	O
15.19	O
)	O
(	O
with	O
(	O
cid:22	O
)	O
in	O
(	O
3.5	O
)	O
now	O
the	O
indicator	O
of	O
null	O
or	O
non-null	O
states	O
,	O
and	O
x	O
now	O
z	O
)	O
.	O
drawing	O
a	O
smooth	O
curve	O
o	O
f	O
.z/	O
through	O
the	O
histogram	O
of	O
the	O
z-values	O
yields	O
the	O
more	O
efﬁcient	O
estimate	B
cfdr.z0/	O
d	O
(	O
cid:25	O
)	O
0f0.z0/=	O
o	O
f	O
.z0/i	O
(	O
15.39	O
)	O
cfdr.z/	O
	O
0:2	O
figure	O
15.5	O
shows	O
cfdr.z/	O
for	O
the	O
prostate	B
study	O
data	B
of	O
figure	O
15.1	O
,	O
the	O
null	O
proportion	B
(	O
cid:25	O
)	O
0	O
can	O
be	O
estimated—see	O
section	O
15.5—or	O
set	B
equal	O
to	O
1.	O
where	O
o	O
f	O
.z/	O
in	O
(	O
15.39	O
)	O
has	O
been	O
estimated	O
as	O
described	O
below	O
.	O
the	O
curve	O
hovers	O
near	O
1	O
for	O
the	O
93	O
%	O
of	O
the	O
cases	O
having	O
jzij	O
	O
2	O
,	O
sensibly	O
suggesting	O
that	O
there	O
is	O
no	O
involvement	O
with	O
prostate	B
cancer	O
for	O
most	O
genes	O
.	O
it	O
declines	O
quickly	O
for	O
jzij	O
(	O
cid:21	O
)	O
3	O
,	O
reaching	O
the	O
conventionally	O
“	O
interesting	O
”	O
threshold	O
(	O
15.40	O
)	O
for	O
zi	O
(	O
cid:21	O
)	O
3:34	O
and	O
zi	O
	O
(	O
cid:0	O
)	O
3:40.	O
this	O
was	O
attained	O
for	O
27	O
genes	O
in	O
the	O
right	O
tail	O
and	O
25	O
in	O
the	O
left	O
,	O
these	O
being	O
reasonable	O
candidates	O
to	O
ﬂag	O
for	O
follow-	O
up	O
investigation	O
.	O
the	O
curve	O
o	O
f	O
.z/	O
used	O
in	O
(	O
15.39	O
)	O
was	O
obtained	O
from	O
a	O
fourth-degree	O
log	O
polynomial	O
poisson	O
regression	B
ﬁt	O
to	O
the	O
histogram	O
in	O
figure	O
15.1	O
,	O
as	O
in	O
figure	O
10.5	O
(	O
10.52	O
)	O
–	O
(	O
10.56	O
)	O
.	O
log	O
polynomials	O
of	O
degree	O
2	O
through	O
6	O
were	O
ﬁt	O
by	O
maximum	B
likelihood	I
,	O
giving	O
total	O
residual	O
deviances	O
(	O
8.35	O
)	O
shown	O
in	O
table	O
15.1.	O
an	O
enormous	O
improvement	O
in	O
ﬁt	O
is	O
seen	O
in	O
going	O
from	O
degree	O
3	O
to	O
4	O
,	O
but	O
nothing	O
signiﬁcant	O
after	O
that	O
,	O
with	O
decreases	O
less	O
than	O
the	O
null	O
value	O
2	O
suggested	O
by	O
(	O
12.75	O
)	O
.	O
10	O
equation	B
(	O
15.37	O
)	O
makes	O
argument	B
(	O
4	O
)	O
of	O
the	O
previous	O
section	O
clearer	O
:	O
having	O
more	O
“	O
other	O
”	O
z-values	O
fall	O
into	O
z0	O
increases	O
n.z0/	O
,	O
decreasingcfdr.z0/	O
and	O
making	O
it	O
more	O
likely	O
that	O
zi	O
d	O
z0	O
represents	O
a	O
non-null	O
case	O
.	O
284	O
large-scale	O
hypothesis	O
testing	B
and	O
fdrs	O
figure	O
15.5	O
local	O
false-discovery	B
rate	I
estimatecfdr.z/	O
(	O
15.39	O
)	O
the	O
left	O
,	O
indicated	O
by	O
dashes	O
,	O
havecfdr.zi	O
/	O
	O
0:2	O
;	O
light	O
dashed	O
curves	O
are	O
the	O
left	O
and	O
right	O
tail-area	O
estimates	O
cfdr.z/	O
(	O
15.26	O
)	O
.	O
for	O
prostate	B
study	O
of	O
figure	O
15.1	O
;	O
27	O
genes	O
on	O
the	O
right	O
and	O
25	O
on	O
table	O
15.1	O
total	O
residual	O
deviances	O
from	O
log	O
polynomial	O
poisson	O
regressions	O
of	O
the	O
prostate	B
data	O
,	O
for	O
polynomial	O
degrees	O
2	O
through	O
6	O
;	O
degree	O
4	O
is	O
preferred	O
.	O
degree	O
deviance	O
2	O
138.6	O
3	O
137.0	O
4	O
65.1	O
5	O
64.1	O
6	O
63.7	O
the	O
points	O
in	O
figure	O
15.6	O
represent	O
the	O
log	O
bin	O
counts	O
from	O
the	O
histogram	O
in	O
figure	O
15.1	O
(	O
excluding	O
zero	O
counts	O
)	O
,	O
with	O
the	O
solid	O
curve	O
showing	O
the	O
4th-degree	O
mle	O
polynomial	O
ﬁt	O
.	O
also	O
shown	O
is	O
the	O
standard	O
normal	O
log	O
density	B
log	O
f0.z/	O
d	O
(	O
cid:0	O
)	O
1	O
z2	O
c	O
constant	O
:	O
(	O
15.41	O
)	O
it	O
ﬁts	O
reasonably	O
well	O
for	O
jzj	O
<	O
2	O
,	O
emphasizing	O
the	O
null	O
status	O
of	O
the	O
gene	O
the	O
cutoffcfdr.z/	O
	O
0:2	O
for	O
declaring	O
a	O
case	O
interesting	O
is	O
not	O
completely	O
majority	O
.	O
arbitrary	O
.	O
deﬁnitions	O
(	O
15.38	O
)	O
and	O
(	O
15.22	O
)	O
,	O
and	O
a	O
little	O
algebra	O
,	O
show	O
that	O
it	O
2	O
−4−20240.00.20.40.60.81.0z−valuefdr	O
and	O
fdrlocal	O
fdr−3.403.34	O
15.4	O
local	O
false-discovery	O
rates	O
285	O
figure	O
15.6	O
points	O
are	O
log	O
bin	O
counts	O
for	O
figure	O
15.1	O
’	O
s	O
histogram	O
.	O
the	O
solid	O
black	O
curve	O
is	O
a	O
fourth-degree	O
log-polynomial	O
ﬁt	O
used	O
to	O
calculatecfdr.z/	O
in	O
figure	O
15.5.	O
the	O
dashed	O
red	O
curve	O
,	O
the	O
log	O
null	O
density	B
(	O
15.41	O
)	O
,	O
provides	O
a	O
reasonable	O
ﬁt	O
for	O
jzj	O
	O
2.	O
is	O
equivalent	O
to	O
(	O
15.42	O
)	O
if	O
we	O
assume	O
(	O
cid:25	O
)	O
0	O
(	O
cid:21	O
)	O
0:90	O
,	O
as	O
is	O
reasonable	O
in	O
most	O
large-scale	B
testing	I
situa-	O
tions	O
,	O
this	O
makes	O
the	O
bayes	O
factor	B
f1.z/=f0.z/	O
quite	O
large	O
,	O
:	O
f1.z/	O
f0.z/	O
(	O
cid:21	O
)	O
4	O
(	O
cid:25	O
)	O
0	O
(	O
cid:25	O
)	O
1	O
(	O
cid:21	O
)	O
36	O
;	O
f1.z/	O
f0.z/	O
(	O
15.43	O
)	O
“	O
strong	O
evidence	O
”	O
against	O
the	O
null	O
hypothesis	O
in	O
jeffreys	O
’	O
scale	B
,	O
table	O
13.3.	O
there	O
is	O
a	O
simple	O
relation	O
between	O
the	O
local	O
and	O
tail-area	O
false-discovery	O
rates	O
:	O
	O
fdr.z0/	O
d	O
e	O
ffdr.z/jz	O
(	O
cid:21	O
)	O
z0gi	O
4	O
(	O
15.44	O
)	O
so	O
fdr.z0/	O
is	O
the	O
average	O
value	O
of	O
fdr.z/	O
for	O
z	O
greater	O
than	O
z0	O
.	O
in	O
interesting	O
situations	O
,	O
fdr.z/	O
will	O
be	O
a	O
decreasing	O
function	B
for	O
large	O
values	O
of	O
z	O
,	O
as	O
on	O
the	O
right	O
side	O
of	O
figure	O
15.5	O
,	O
making	O
fdr.z0/	O
<	O
fdr.z0/	O
.	O
this	O
accounts	O
−4−202460123456z−valuelog	O
densityllllllllllllllllllllllllllllllllllllllllllllll4th	O
degree	O
logpolynomialn	O
(	O
0,1	O
)	O
5	O
286	O
large-scale	O
hypothesis	O
testing	B
and	O
fdrs	O
for	O
the	O
conventional	O
signiﬁcance	O
cutoff	O
cfdr.z/	O
	O
0:1	O
being	O
smaller	O
than	O
cfdr.z/	O
	O
0:2	O
(	O
15.40	O
)	O
.	O
as	O
with	O
left-sided	O
and	O
right-sided	O
tail-area	O
cfdr	O
estimates	O
,	O
sincecfdr.z/	O
ap-	O
the	O
bayesian	O
interpretation	O
of	O
local	O
false-discovery	O
rates	O
carries	O
with	O
it	O
the	O
advantages	O
of	O
bayesian	O
coherency	O
.	O
we	O
don	O
’	O
t	B
have	O
to	O
change	O
deﬁnitions	O
plies	O
without	O
change	O
to	O
both	O
tails.11	O
also	O
,	O
we	O
don	O
’	O
t	B
need	O
a	O
separate	O
theory	B
for	O
“	O
true-discovery	O
rates	O
,	O
”	O
since	O
tdr.z0/	O
	O
1	O
(	O
cid:0	O
)	O
fdr.z0/	O
d	O
(	O
cid:25	O
)	O
1f1.z0/=f	O
.z0/	O
(	O
15.45	O
)	O
is	O
the	O
conditional	O
probability	O
that	O
case	O
i	O
is	O
non-null	O
given	O
zi	O
d	O
z0	O
.	O
15.5	O
choice	O
of	O
the	O
null	O
distribution	B
the	O
null	O
distribution	B
,	O
f0.z/	O
in	O
the	O
two-groups	O
model	B
(	O
15.19	O
)	O
,	O
plays	O
a	O
cru-	O
cial	O
role	O
in	O
large-scale	B
testing	I
,	O
just	O
as	O
it	O
does	O
in	O
the	O
classic	O
single-case	O
the-	O
ory	O
.	O
something	O
different	O
however	O
happens	O
in	O
large-scale	O
problems	O
:	O
with	O
thousands	O
of	O
z-values	O
to	O
examine	O
at	O
once	O
,	O
it	O
can	O
become	O
clear	O
that	O
the	O
con-	O
ventional	O
theoretical	O
null	O
is	O
inappropriate	O
for	O
the	O
situation	O
at	O
hand	O
.	O
put	O
more	O
positively	O
,	O
large-scale	O
applications	O
may	O
allow	O
us	O
to	O
empirically	O
determine	O
a	O
more	O
realistic	O
null	O
distribution	B
.	O
the	O
police	B
data	O
of	O
figure	O
15.7	O
illustrates	O
what	O
can	O
happen	O
.	O
possi-	O
ble	O
racial	O
bias	O
in	O
pedestrian	O
stops	O
was	O
assessed	O
for	O
n	O
d	O
2749	O
new	O
york	O
city	O
police	B
ofﬁcers	O
in	O
2006.	O
each	O
ofﬁcer	O
was	O
assigned	O
a	O
score	O
zi	O
,	O
large	O
positive	O
scores	O
suggesting	O
racial	O
bias	O
.	O
the	O
zi	O
values	O
were	O
summary	O
scores	O
from	O
a	O
complicated	O
logistic	B
regression	I
model	O
intended	O
to	O
compensate	O
for	O
differences	O
in	O
the	O
time	O
of	O
day	O
,	O
location	O
,	O
and	O
context	O
of	O
the	O
stops	O
.	O
logistic	B
regression	I
theory	O
suggested	O
the	O
theoretical	O
null	O
distribution	B
h0i	O
w	O
zi	O
(	O
cid:24	O
)	O
n	O
.0	O
;	O
1/	O
(	O
15.46	O
)	O
for	O
the	O
absence	O
of	O
racial	O
bias	O
.	O
the	O
trouble	O
is	O
that	O
the	O
center	O
of	O
the	O
z-value	O
histogram	O
in	O
figure	O
15.7	O
,	O
which	O
should	O
track	O
the	O
n	O
.0	O
;	O
1/	O
curve	O
applying	O
to	O
the	O
presumably	O
large	O
fraction	O
of	O
null-case	O
ofﬁcers	O
,	O
is	O
much	O
too	O
wide	O
.	O
(	O
unlike	O
the	O
situation	O
for	O
the	O
prostate	B
data	O
in	O
figure	O
15.1	O
.	O
)	O
an	O
mle	O
ﬁtting	B
algorithm	O
discussed	O
below	O
produced	O
the	O
empirical	B
null	O
h0i	O
w	O
zi	O
(	O
cid:24	O
)	O
n	O
.0:10	O
;	O
1:402/	O
(	O
15.47	O
)	O
11	O
going	O
further	O
,	O
z	O
in	O
the	O
two-groups	O
model	B
could	O
be	O
multidimensional	O
.	O
then	O
tail-area	O
false-discovery	O
rates	O
would	O
be	O
unavailable	O
,	O
but	O
(	O
15.38	O
)	O
would	O
still	O
legitimately	O
deﬁne	O
fdr.z/	O
.	O
15.5	O
choice	O
of	O
the	O
null	O
distribution	B
287	O
figure	O
15.7	O
police	B
data	O
;	O
histogram	O
of	O
z	O
scores	O
for	O
n	O
d	O
2749	O
new	O
york	O
city	O
police	B
ofﬁcers	O
,	O
with	O
large	O
zi	O
suggesting	O
racial	O
bias	O
.	O
the	O
center	O
of	O
the	O
histogram	O
is	O
too	O
wide	O
compared	O
with	O
the	O
theoretical	O
null	O
distribution	B
zi	O
(	O
cid:24	O
)	O
n	O
.0	O
;	O
1/	O
.	O
an	O
mle	O
ﬁt	O
to	O
central	O
data	O
gave	O
n	O
.0:10	O
;	O
1:402/	O
as	O
empirical	B
null	O
.	O
there	O
is	O
a	O
lot	O
at	O
stake	O
here	O
.	O
based	O
on	O
the	O
empirical	B
null	O
(	O
15.47	O
)	O
only	O
the	O
four	O
circled	O
points	O
at	O
the	O
far	O
right	O
of	O
figure	O
15.8	O
;	O
the	O
ﬁfth	O
point	O
had	O
as	O
appropriate	O
here	O
.	O
this	O
is	O
reinforced	O
by	O
a	O
qq	O
plot	O
of	O
the	O
zi	O
values	O
shown	O
in	O
figure	O
15.8	O
,	O
where	O
we	O
see	O
most	O
of	O
the	O
cases	O
falling	O
nicely	O
along	O
a	O
n	O
.0:09	O
;	O
1:422/	O
line	O
,	O
with	O
just	O
a	O
few	O
outliers	O
at	O
both	O
extremes	O
.	O
four	O
ofﬁcers	O
reached	O
the	O
“	O
probably	O
racially	O
biased	O
”	O
cutoffcfdr.zi	O
/	O
	O
0:2	O
,	O
cfdr	O
d	O
0:38	O
while	O
all	O
the	O
others	O
exceeded	O
0.80.	O
the	O
theoretical	O
n	O
.0	O
;	O
1/	O
null	O
was	O
much	O
more	O
severe	O
,	O
assigningcfdr	O
	O
0:2	O
to	O
the	O
125	O
ofﬁcers	O
having	O
zi	O
(	O
cid:21	O
)	O
2:50.	O
one	O
can	O
imagine	O
the	O
difference	O
in	O
newspaper	O
headlines	O
.	O
from	O
a	O
classical	O
point	O
of	O
view	O
it	O
seems	O
heretical	O
to	O
question	O
the	O
theo-	O
retical	O
null	O
distribution	B
,	O
especially	O
since	O
there	O
is	O
no	O
substitute	O
available	O
in	O
single-case	O
testing	B
.	O
once	O
alerted	O
by	O
data	B
sets	O
like	O
the	O
police	B
study	O
,	O
however	O
,	O
it	O
is	O
easy	O
to	O
list	O
reasons	O
for	O
doubt	O
:	O
(	O
cid:15	O
)	O
asymptotics	O
taylor	O
series	O
approximations	O
go	O
into	O
theoretical	O
null	O
calcu-	O
lations	O
such	O
as	O
(	O
15.46	O
)	O
,	O
which	O
can	O
lead	O
to	O
inaccuracies	O
,	O
particularly	O
in	O
the	O
crucial	O
tails	O
of	O
the	O
null	O
distribution	B
.	O
(	O
cid:15	O
)	O
correlations	O
false-discovery	B
rate	I
methods	O
are	O
correct	O
on	O
the	O
average	O
,	O
z−valuesfrequency−6−4−20246050100150200n	O
(	O
0,1	O
)	O
n	O
(	O
0.1,1.402	O
)	O
288	O
large-scale	O
hypothesis	O
testing	B
and	O
fdrs	O
figure	O
15.8	O
qq	O
plot	O
of	O
police	B
data	O
z	O
scores	O
;	O
most	O
scores	O
closely	O
follow	O
the	O
n	O
.0:09	O
;	O
1:422/	O
line	O
with	O
a	O
few	O
outliers	O
at	O
either	O
end	O
.	O
the	O
circled	O
points	O
are	O
cases	O
having	O
local	O
false-discovery	O
estimate	O
cfdr.zi	O
/	O
	O
0:2	O
,	O
based	O
on	O
the	O
empirical	B
null	O
.	O
using	O
the	O
theoretical	O
n	O
.0	O
;	O
1/	O
null	O
gives	O
216	O
cases	O
withcfdr.zi	O
/	O
	O
0:2	O
,	O
91	O
on	O
the	O
left	O
and	O
125	O
on	O
the	O
right	O
.	O
even	O
with	O
correlations	O
among	O
the	O
n	O
z-values	O
.	O
however	O
,	O
severe	O
correlation	O
destabilizes	O
the	O
z-value	O
histogram	O
,	O
which	O
can	O
become	O
randomly	O
wider	O
or	O
narrower	O
than	O
theoretically	O
predicted	O
,	O
undermining	O
theoretical	O
null	O
results	O
for	O
the	O
data	B
set	O
at	O
hand.	O
6	O
(	O
cid:15	O
)	O
unobserved	O
covariates	O
the	O
police	B
study	O
was	O
observational	O
:	O
individual	O
encounters	O
were	O
not	O
assigned	O
at	O
random	O
to	O
the	O
various	O
ofﬁcers	O
but	O
simply	O
observed	O
as	O
they	O
happened	O
.	O
observed	O
covariates	O
such	O
as	O
the	O
time	O
of	O
day	O
and	O
the	O
neighborhood	O
were	O
included	O
in	O
the	O
logistic	B
regression	I
model	O
,	O
but	O
one	O
can	O
never	O
rule	B
out	O
the	O
possibility	O
of	O
inﬂuential	O
unobserved	O
covariates	O
.	O
(	O
cid:15	O
)	O
effect	O
size	O
considerations	O
the	O
hypothesis-testing	O
setup	O
,	O
where	O
a	O
large	O
fraction	O
of	O
the	O
cases	O
are	O
truly	O
null	O
,	O
may	O
not	O
be	O
appropriate	O
.	O
an	O
effect	O
size	O
model	B
,	O
with	O
(	O
cid:22	O
)	O
i	O
(	O
cid:24	O
)	O
g.	O
(	O
cid:1	O
)	O
/	O
and	O
zi	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
i	O
;	O
1/	O
,	O
might	O
apply	O
,	O
with	O
the	O
prior	B
g.	O
(	O
cid:22	O
)	O
/	O
not	O
having	O
an	O
atom	O
at	O
(	O
cid:22	O
)	O
d	O
0.	O
the	O
nonatomic	O
choice	O
g.	O
(	O
cid:22	O
)	O
/	O
(	O
cid:24	O
)	O
n	O
.0:10	O
;	O
0:632/	O
provides	O
a	O
good	O
ﬁt	O
to	O
the	O
qq	O
plot	O
in	O
figure	O
15.8	O
.	O
*************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************−3−2−10123−10−505normal	O
quantilessample	O
quantileslllllllllintercept	O
=	O
0.089slope	O
=	O
1.424	O
15.5	O
choice	O
of	O
the	O
null	O
distribution	B
289	O
empirical	B
null	O
estimation	B
f0.z/	O
(	O
cid:24	O
)	O
our	O
point	O
of	O
view	O
here	O
is	O
that	O
the	O
theoretical	O
null	O
(	O
15.46	O
)	O
,	O
zi	O
(	O
cid:24	O
)	O
n	O
.0	O
;	O
1/	O
,	O
is	O
not	O
completely	O
wrong	O
but	O
needs	O
adjustment	O
for	O
the	O
data	B
set	O
at	O
hand	O
.	O
to	O
this	O
end	O
we	O
assume	O
the	O
two-groups	O
model	B
(	O
15.19	O
)	O
,	O
with	O
f0.z/	O
normal	B
but	O
not	O
necessarily	O
n	O
.0	O
;	O
1/	O
,	O
say	O
n	O
.ı0	O
;	O
(	O
cid:27	O
)	O
2	O
0	O
/	O
:	O
(	O
15.48	O
)	O
in	O
order	O
to	O
compute	O
the	O
local	O
false-discovery	B
rate	I
fdr.z/	O
d	O
(	O
cid:25	O
)	O
0f0.z/=f	O
.z/	O
we	O
want	O
to	O
estimate	B
the	O
three	O
numerator	O
parameters	O
.ı0	O
;	O
(	O
cid:27	O
)	O
0	O
;	O
(	O
cid:25	O
)	O
0/	O
,	O
the	O
mean	O
and	O
standard	O
deviation	O
of	O
the	O
null	O
density	B
and	O
the	O
proportion	B
of	O
null	O
cases	O
.	O
(	O
the	O
denominator	O
f	O
.z/	O
is	O
estimated	O
as	O
in	O
section	O
15.4	O
.	O
)	O
our	O
key	O
assumptions	O
(	O
besides	O
(	O
15.48	O
)	O
)	O
are	O
that	O
(	O
cid:25	O
)	O
0	O
is	O
large	O
,	O
say	O
(	O
cid:25	O
)	O
0	O
(	O
cid:21	O
)	O
0:90	O
,	O
and	O
that	O
most	O
of	O
the	O
zi	O
near	O
0	O
are	O
null	O
cases	O
.	O
the	O
algorithm	B
locfdr	O
begins	O
by	O
selecting	O
a	O
set	B
a0	O
near	O
z	O
d	O
0	O
in	O
which	O
it	O
is	O
assumed	O
that	O
all	O
the	O
7	O
zi	O
in	O
a0	O
are	O
null	O
;	O
in	O
terms	O
of	O
the	O
two-groups	O
model	B
,	O
the	O
assumption	O
can	O
be	O
stated	O
as	O
(	O
15.49	O
)	O
f1.z/	O
d	O
0	O
for	O
z	O
2	O
a0	O
:	O
modest	O
violations	O
of	O
(	O
15.49	O
)	O
,	O
which	O
are	O
to	O
be	O
expected	O
,	O
produce	O
small	O
bi-	O
ases	O
in	O
the	O
empirical	B
null	O
estimates	O
.	O
maximum	B
likelihood	I
based	O
on	O
the	O
number	O
and	O
values	O
of	O
the	O
zi	O
observed	O
in	O
a0	O
yield	O
the	O
empirical	B
null	O
es-	O
timates	O
.	O
applied	O
to	O
the	O
police	B
data	O
,	O
locfdr	B
chose	O
a0	O
d	O
œ	O
(	O
cid:0	O
)	O
1:8	O
;	O
2:0	O
and	O
pro-	O
o	O
ı0	O
;	O
o	O
(	O
cid:27	O
)	O
0	O
;	O
o	O
(	O
cid:25	O
)	O
0/	O
.	O
duced	O
estimates	O
(	O
cid:16	O
)	O
o	O
ı0	O
;	O
o	O
(	O
cid:27	O
)	O
0	O
;	O
o	O
(	O
cid:25	O
)	O
0	O
	O
d	O
.0:10	O
;	O
1:40	O
;	O
0:989/	O
:	O
(	O
15.50	O
)	O
8	O
two	O
small	O
simulation	O
studies	O
described	O
in	O
table	O
15.2	O
give	O
some	O
idea	O
of	O
the	O
variabilities	O
and	O
biases	O
inherent	O
in	O
the	O
locfdr	B
estimation	O
process	O
.	O
the	O
third	O
method	B
,	O
somewhere	O
between	O
the	O
theoretical	O
and	O
empirical	O
null	O
estimates	O
but	O
closer	O
to	O
the	O
former	O
,	O
relies	O
on	O
permutations	O
.	O
the	O
vector	B
z	O
of	O
6033	O
z-values	O
for	O
the	O
prostate	B
data	O
of	O
figure	O
15.1	O
was	O
obtained	O
from	O
a	O
study	O
of	O
102	O
men	O
,	O
52	O
cancer	O
patients	O
and	O
50	O
controls	O
.	O
randomly	O
permuting	O
the	O
men	O
’	O
s	O
data	B
,	O
that	O
is	O
randomly	O
choosing	O
50	O
of	O
the	O
102	O
to	O
be	O
“	O
controls	O
”	O
and	O
the	O
remaining	O
52	O
to	O
be	O
“	O
patients	O
,	O
”	O
and	O
then	O
carrying	O
through	O
steps	O
(	O
15.1	O
)	O
–	O
(	O
cid:3	O
)	O
in	O
which	O
any	O
actual	O
cancer/control	O
differences	O
have	O
(	O
15.2	O
)	O
gives	O
a	O
vector	B
z	O
(	O
cid:3	O
)	O
been	O
suppressed	O
.	O
a	O
histogram	O
of	O
the	O
z	O
i	O
values	O
(	O
perhaps	O
combining	O
sev-	O
eral	O
permutations	O
)	O
provides	O
the	O
“	O
permutation	O
null.	O
”	O
here	O
we	O
are	O
extending	O
fisher	O
’	O
s	O
original	O
permutation	O
idea	O
,	O
section	O
4.4	O
,	O
to	O
large-scale	B
testing	I
.	O
ten	O
permutations	O
of	O
the	O
prostate	B
study	O
data	B
produced	O
an	O
almost	O
perfect	O
290	O
large-scale	O
hypothesis	O
testing	B
and	O
fdrs	O
o	O
ı0	O
;	O
o	O
(	O
cid:27	O
)	O
0	O
;	O
o	O
(	O
cid:25	O
)	O
0/	O
for	O
two	O
table	O
15.2	O
means	O
and	O
standard	O
deviations	O
of	O
.	O
simulation	O
studies	O
of	O
empirical	B
null	O
estimation	B
using	O
locfdr	B
.	O
n	O
d	O
5000	O
cases	O
each	O
trial	O
with	O
.ı0	O
;	O
(	O
cid:27	O
)	O
0	O
;	O
(	O
cid:25	O
)	O
0/	O
as	O
shown	O
;	O
250	O
trials	O
;	O
two-groups	O
model	B
(	O
15.19	O
)	O
with	O
non-null	O
density	B
f1.z/	O
equal	O
to	O
n	O
.3	O
;	O
1/	O
(	O
left	O
side	O
)	O
or	O
n	O
.4:2	O
;	O
1/	O
(	O
right	O
side	O
)	O
.	O
ı0	O
0	O
.015	O
.019	O
(	O
cid:27	O
)	O
0	O
1.0	O
1.017	O
.017	O
(	O
cid:25	O
)	O
0	O
.95	O
.962	O
.005	O
ı0	O
.10	O
.114	O
.025	O
(	O
cid:27	O
)	O
0	O
1.40	O
1.418	O
.029	O
(	O
cid:25	O
)	O
0	O
.95	O
.958	O
.006	O
true	O
mean	O
st	O
dev	O
n	O
.0	O
;	O
1/	O
permutation	O
null	O
.	O
(	O
this	O
is	O
as	O
expected	O
from	O
the	O
classic	O
theory	B
of	O
permutation	O
t-tests	O
.	O
)	O
permutation	O
methods	O
reliably	O
overcome	O
objection	O
1	O
to	O
the	O
theoretical	O
null	O
distribution	B
,	O
over-reliance	O
on	O
asymptotic	O
approxima-	O
tions	O
,	O
but	O
can	O
not	O
cure	O
objections	O
2	O
,	O
3	O
,	O
and	O
4.	O
9	O
whatever	O
the	O
cause	O
of	O
disparity	O
,	O
the	O
operational	O
difference	O
between	O
the	O
theoretical	O
and	O
empirical	O
null	O
distribution	B
is	O
clear	O
:	O
with	O
the	O
latter	O
,	O
the	O
sig-	O
niﬁcance	O
of	O
an	O
outlying	O
case	O
is	O
judged	O
relative	O
to	O
the	O
dispersion	O
of	O
the	O
majority	O
,	O
not	O
by	O
a	O
theoretical	O
yardstick	O
as	O
with	O
the	O
former	O
.	O
this	O
was	O
per-	O
suasive	O
for	O
the	O
police	B
data	O
,	O
but	O
the	O
story	O
isn	O
’	O
t	B
one-sided	O
.	O
estimating	O
the	O
null	O
distribution	B
adds	O
substantially	O
to	O
the	O
variability	O
ofcfdr	O
orcfdr	O
.	O
for	O
situations	O
such	O
as	O
the	O
prostate	B
data	O
,	O
when	O
the	O
theoretical	O
null	O
looks	O
nearly	O
correct,12	O
it	O
is	O
reasonable	O
to	O
stick	O
with	O
it	O
.	O
the	O
very	O
large	O
data	B
sets	O
of	O
twenty-ﬁrst-century	O
applications	O
encourage	O
self-contained	O
methodology	O
that	O
proceeds	O
from	O
just	O
the	O
data	B
at	O
hand	O
using	O
a	O
minimum	O
of	O
theoretical	O
constructs	O
.	O
false-discovery	B
rate	I
empirical	O
bayes	O
analysis	B
of	O
large-scale	B
testing	I
problems	O
,	O
with	O
data-based	O
estimation	B
of	O
o	O
(	O
cid:25	O
)	O
0	O
,	O
o	O
f0	O
,	O
and	O
o	O
f	O
,	O
comes	O
close	O
to	O
the	O
ideal	O
in	O
this	O
sense	O
.	O
15.6	O
relevance	O
false-discovery	O
rates	O
return	O
us	O
to	O
the	O
purview	O
of	O
indirect	O
evidence	O
,	O
sec-	O
tions	O
6.4	O
and	O
7.4.	O
our	O
interest	O
in	O
any	O
one	O
gene	O
in	O
the	O
prostate	B
cancer	O
study	O
depends	O
on	O
its	O
own	O
z	O
score	O
of	O
course	O
,	O
but	O
also	O
on	O
the	O
other	O
genes	O
’	O
scores—	O
“	O
learning	O
from	O
the	O
experience	O
of	O
others	O
,	O
”	O
in	O
the	O
language	O
used	O
before	O
.	O
the	O
crucial	O
question	O
we	O
have	O
been	O
avoiding	O
is	O
“	O
which	O
others	O
?	O
”	O
our	O
tacit	O
answer	O
has	O
been	O
“	O
all	O
the	O
cases	O
that	O
arrive	O
in	O
the	O
same	O
data	B
set	O
,	O
”	O
all	O
the	O
genes	O
12	O
the	O
locfdr	B
algorithm	O
gave	O
.oı0	O
;	O
o	O
(	O
cid:27	O
)	O
0	O
;	O
o	O
(	O
cid:25	O
)	O
0/	O
d	O
.0:00	O
;	O
1:06	O
;	O
0:984/	O
for	O
the	O
prostate	B
data	O
.	O
15.6	O
relevance	O
291	O
in	O
the	O
prostate	B
study	O
,	O
all	O
the	O
ofﬁcers	O
in	O
the	O
police	B
study	O
.	O
why	O
this	O
can	O
be	O
a	O
dangerous	O
tactic	O
is	O
shown	O
in	O
our	O
ﬁnal	O
example	O
.	O
a	O
dti	O
(	O
diffusion	O
tensor	O
imaging	O
)	O
study	O
compared	O
six	O
dyslexic	O
children	O
with	O
six	O
normal	B
controls	O
.	O
each	O
dti	O
scan	O
recorded	O
ﬂuid	O
ﬂows	O
at	O
n	O
d15,443	O
“	O
voxels	O
,	O
”	O
i.e.	O
,	O
at	O
15,443	O
three-dimensional	O
brain	O
coordinates	O
.	O
a	O
score	O
zi	O
comparing	O
dyslexics	O
with	O
normal	B
controls	O
was	O
calculated	O
for	O
each	O
voxel	O
i	O
,	O
calibrated	O
such	O
that	O
the	O
theoretical	O
null	O
distribution	B
of	O
“	O
no	O
difference	O
”	O
was	O
h0i	O
w	O
zi	O
(	O
cid:24	O
)	O
n	O
.0	O
;	O
1/	O
(	O
15.51	O
)	O
as	O
at	O
(	O
15.3	O
)	O
.	O
figure	O
15.9	O
histogram	O
of	O
z	O
scores	O
for	O
the	O
dti	O
study	O
,	O
comparing	O
dyslexic	O
versus	O
normal	B
control	O
children	O
at	O
15,443	O
brain	O
locations	O
.	O
a	O
fdr	O
analysis	B
based	O
on	O
the	O
empirical	B
null	O
distribution	B
gave	O
149	O
voxels	O
withcfdr.zi	O
/	O
	O
0:20	O
,	O
those	O
having	O
zi	O
(	O
cid:21	O
)	O
3:17	O
(	O
indicated	O
by	O
red	O
dashes	O
)	O
.	O
figure	O
15.9	O
shows	O
the	O
histogram	O
of	O
all	O
15,443	O
zi	O
values	O
,	O
normal-looking	O
near	O
the	O
center	O
and	O
with	O
a	O
heavy	O
right	O
tail	O
;	O
locfdr	B
gave	O
empirical	B
null	O
parameters	O
	O
d	O
.	O
(	O
cid:0	O
)	O
0:12	O
;	O
1:06	O
;	O
0:984/	O
;	O
(	O
cid:16	O
)	O
o	O
ı0	O
;	O
o	O
(	O
cid:27	O
)	O
0	O
;	O
o	O
(	O
cid:25	O
)	O
0	O
the	O
149	O
voxels	O
with	O
zi	O
(	O
cid:21	O
)	O
3:17	O
havingcfdr	O
values	O
	O
0:20.	O
using	O
the	O
the-	O
(	O
15.52	O
)	O
z−scorefrequency−4−2024020040060080010003.17	O
292	O
large-scale	O
hypothesis	O
testing	B
and	O
fdrs	O
voxels	O
with	O
zi	O
(	O
cid:21	O
)	O
3:07	O
havingcfdri	O
	O
0:20.	O
oretical	O
null	O
(	O
15.51	O
)	O
yielded	O
only	O
modestly	O
different	O
results	O
,	O
now	O
the	O
177	O
figure	O
15.10	O
a	O
plot	O
of	O
15,443	O
zi	O
scores	O
from	O
a	O
dti	O
study	O
(	O
vertical	O
axis	O
)	O
and	O
voxel	O
distances	O
xi	O
from	O
the	O
back	O
of	O
the	O
brain	O
(	O
horizontal	O
axis	O
)	O
.	O
the	O
starred	O
points	O
are	O
the	O
149	O
voxels	O
with	O
cfdr.zi	O
/	O
	O
0:20	O
,	O
which	O
occur	O
mostly	O
for	O
xi	O
in	O
the	O
interval	B
œ50	O
;	O
70	O
.	O
in	O
figure	O
15.10	O
the	O
voxel	O
scores	O
zi	O
,	O
graphed	O
vertically	O
,	O
are	O
plotted	O
ver-	O
sus	O
xi	O
,	O
the	O
voxel	O
’	O
s	O
distance	O
from	O
the	O
back	O
of	O
the	O
brain	O
.	O
waves	O
of	O
differing	O
response	O
are	O
apparent	O
.	O
larger	O
values	O
occur	O
in	O
the	O
interval	B
50	O
	O
x	O
	O
70	O
,	O
up	O
.	O
most	O
of	O
the	O
149	O
voxels	O
havingcfdri	O
	O
0:20	O
occur	O
at	O
the	O
top	O
of	O
this	O
wave	O
.	O
where	O
the	O
entire	O
z-value	O
distribution—low	O
,	O
medium	O
,	O
and	O
high—is	O
pushed	O
(	O
15.53	O
)	O
figure	O
15.10	O
raises	O
the	O
problem	O
of	O
fair	O
comparison	O
.	O
perhaps	O
the	O
4,653	O
voxels	O
with	O
xi	O
between	O
50	O
and	O
70	O
should	O
be	O
compared	O
only	O
with	O
each	O
other	O
,	O
and	O
not	O
with	O
all	O
15,443	O
cases	O
.	O
doing	O
so	O
gave	O
	O
d	O
.0:23	O
;	O
1:18	O
;	O
0:970/	O
;	O
only	O
66	O
voxels	O
havingcfdri	O
	O
0:20	O
,	O
those	O
with	O
zi	O
(	O
cid:21	O
)	O
3:57.	O
ı0	O
;	O
o	O
(	O
cid:27	O
)	O
0	O
;	O
o	O
(	O
cid:25	O
)	O
0	O
(	O
cid:16	O
)	O
o	O
all	O
of	O
this	O
is	O
a	O
question	O
of	O
relevance	O
:	O
which	O
other	O
voxels	O
i	O
are	O
relevant	O
to	O
the	O
assessment	O
of	O
signiﬁcance	O
for	O
voxel	O
i0	O
?	O
one	O
might	O
argue	O
that	O
this	O
is	O
a	O
question	O
for	O
the	O
scientist	O
who	O
gathers	O
the	O
data	B
and	O
not	O
for	O
the	O
statistical	O
analyst	O
,	O
but	O
that	O
is	O
unlikely	O
to	O
be	O
a	O
fruitful	O
avenue	O
,	O
at	O
least	O
not	O
without	O
20406080−2024distance	O
xz	O
scores16	O
%	O
ile84	O
%	O
ilemedian*****************************************************************************************************************************************************	O
15.6	O
relevance	O
293	O
a	O
lot	O
of	O
back-and-forth	O
collaboration	O
.	O
standard	O
bayesian	O
analysis	B
solves	O
the	O
problem	O
by	O
dictate	O
:	O
the	O
assertion	O
of	O
a	O
prior	B
is	O
also	O
an	O
assertion	O
of	O
its	O
relevance	O
.	O
empirical	B
bayes	O
situations	O
expose	O
the	O
dangers	O
lurking	O
in	O
such	O
assertions	O
.	O
relevance	O
was	O
touched	O
upon	O
in	O
section	O
7.4	O
,	O
where	O
the	O
limited	O
transla-	O
tion	O
rule	B
(	O
7.47	O
)	O
was	O
designed	O
to	O
protect	O
extreme	O
cases	O
from	O
being	O
shrunk	O
too	O
far	O
toward	O
the	O
bulk	O
of	O
ordinary	O
ones	O
.	O
one	O
could	O
imagine	O
having	O
a	O
“	O
rel-	O
evance	O
function	B
”	O
(	O
cid:26	O
)	O
.xi	O
;	O
zi	O
/	O
that	O
,	O
given	O
the	O
covariate	O
information	B
xi	O
and	O
re-	O
sponse	O
zi	O
for	O
casei	O
,	O
somehow	O
adjusts	O
an	O
ensemble	O
false-discovery	B
rate	I
es-	O
timate	O
to	O
correctly	O
apply	O
to	O
the	O
case	O
of	O
interest—but	O
such	O
a	O
theory	B
barely	O
exists.	O
10	O
summary	O
combine	O
frequentist	O
and	O
bayesian	O
thinking	O
.	O
large-scale	B
testing	I
,	O
particularly	O
in	O
its	O
false-discovery	B
rate	I
implementation	O
,	O
is	O
not	O
at	O
all	O
the	O
same	O
thing	O
as	O
the	O
classic	O
fisher–neyman–pearson	O
theory	B
:	O
(	O
cid:15	O
)	O
frequentist	O
single-case	O
hypothesis	B
testing	I
depends	O
on	O
the	O
theoretical	O
long-run	O
behavior	O
of	O
samples	O
from	O
the	O
theoretical	O
null	O
distribution	B
.	O
with	O
data	B
available	O
from	O
say	O
n	O
d	O
5000	O
simultaneous	O
tests	O
,	O
the	O
statistician	O
has	O
his	O
or	O
her	O
own	O
“	O
long	O
run	O
”	O
in	O
hand	O
,	O
diminishing	O
the	O
importance	O
of	O
theoretical	O
modeling	O
.	O
in	O
particular	O
,	O
the	O
data	B
may	O
cast	O
doubt	O
on	O
the	O
the-	O
oretical	O
null	O
,	O
providing	O
a	O
more	O
appropriate	O
empirical	B
null	O
distribution	B
in	O
its	O
place	O
.	O
(	O
cid:15	O
)	O
classic	O
testing	B
theory	O
is	O
purely	O
frequentist	O
,	O
whereas	O
false-discovery	O
rates	O
on	O
its	O
own	O
score	O
zi	O
,	O
while	O
cfdr.zi	O
/	O
or	O
cfdr.zi	O
/	O
also	O
depends	O
on	O
the	O
ob-	O
(	O
cid:15	O
)	O
in	O
classic	O
testing	B
,	O
the	O
attained	O
signiﬁcance	O
level	O
for	O
case	O
i	O
depends	O
only	O
(	O
cid:15	O
)	O
applications	O
of	O
single-test	O
theory	B
usually	O
hope	O
for	O
rejection	O
of	O
the	O
null	O
hypothesis	O
,	O
a	O
familiar	O
prescription	O
being	O
0.80	O
power	O
at	O
size	O
0.05.	O
the	O
opposite	O
is	O
true	O
for	O
large-scale	B
testing	I
,	O
where	O
the	O
usual	O
goal	O
is	O
to	O
ac-	O
cept	O
most	O
of	O
the	O
null	O
hypotheses	O
,	O
leaving	O
just	O
a	O
few	O
interesting	O
cases	O
for	O
further	O
study	O
.	O
(	O
cid:15	O
)	O
sharp	O
null	O
hypotheses	O
such	O
as	O
(	O
cid:22	O
)	O
d	O
0	O
are	O
less	O
important	O
in	O
large-scale	O
applications	O
,	O
where	O
the	O
statistician	O
is	O
happy	O
to	O
accept	O
a	O
hefty	O
proportion	B
of	O
uninterestingly	O
small	O
,	O
but	O
nonzero	O
,	O
effect	O
sizes	O
(	O
cid:22	O
)	O
i	O
.	O
(	O
cid:15	O
)	O
false-discovery	B
rate	I
hypothesis	O
testing	B
involves	O
a	O
substantial	O
amount	O
of	O
estimation	B
,	O
blurring	O
the	O
line	O
beteen	O
the	O
two	O
main	O
branches	O
of	O
statistical	O
inference	B
.	O
served	O
z-values	O
for	O
other	O
cases	O
.	O
294	O
large-scale	O
hypothesis	O
testing	B
and	O
fdrs	O
15.7	O
notes	O
and	O
details	O
the	O
story	O
of	O
false-discovery	O
rates	O
illustrates	O
how	O
developments	O
in	O
scien-	O
tiﬁc	O
technology	O
(	O
microarrays	O
in	O
this	O
case	O
)	O
can	O
inﬂuence	O
the	O
progress	O
of	O
statistical	O
inference	B
.	O
a	O
substantial	O
theory	B
of	O
simultaneous	O
inference	B
was	O
developed	O
between	O
1955	O
and	O
1995	O
,	O
mainly	O
aimed	O
at	O
the	O
frequentist	O
control	B
of	O
family-wise	O
error	O
rates	O
in	O
situations	O
involving	O
a	O
small	O
number	O
of	O
hypoth-	O
esis	O
tests	O
,	O
maybe	O
up	O
to	O
20.	O
good	O
references	O
are	O
miller	O
(	O
1981	O
)	O
and	O
westfall	O
and	O
young	O
(	O
1993	O
)	O
.	O
benjamini	O
and	O
hochberg	O
’	O
s	O
seminal	O
1995	O
paper	O
introduced	O
false-discov-	O
ery	O
rates	O
at	O
just	O
the	O
right	O
time	O
to	O
catch	O
the	O
wave	O
of	O
large-scale	O
data	O
sets	O
,	O
now	O
involving	O
thousands	O
of	O
simultaneous	O
tests	O
,	O
generated	O
by	O
microarray	O
appli-	O
cations	O
.	O
most	O
of	O
the	O
material	O
in	O
this	O
chapter	O
is	O
taken	O
from	O
efron	O
(	O
2010	O
)	O
,	O
where	O
the	O
empirical	B
bayes	O
nature	O
of	O
fdr	O
theory	B
is	O
emphasized	O
.	O
the	O
po-	O
lice	O
data	B
is	O
discussed	O
and	O
analyzed	O
at	O
length	O
in	O
ridgeway	O
and	O
macdonald	O
(	O
2009	O
)	O
.	O
1	O
[	O
p.	O
272	O
]	O
model	B
(	O
15.4	O
)	O
.	O
section	O
7.4	O
of	O
efron	O
(	O
2010	O
)	O
discusses	O
the	O
following	O
result	O
for	O
the	O
non-null	O
distribution	B
of	O
z-values	O
:	O
a	O
transformation	O
such	O
as	O
(	O
15.2	O
)	O
that	O
produces	O
a	O
z-value	O
(	O
i.e.	O
,	O
a	O
standard	O
normal	O
random	O
variable	O
z	O
(	O
cid:24	O
)	O
n	O
.0	O
;	O
1/	O
)	O
under	O
the	O
null	O
hypothesis	O
gives	O
,	O
to	O
a	O
good	O
approximation	O
,	O
z	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
2	O
student	O
’	O
s	O
t	B
with	O
100	O
degrees	O
of	O
freedom	O
,	O
(	O
cid:27	O
)	O
2	O
(	O
cid:22	O
)	O
(	O
cid:22	O
)	O
/	O
under	O
reasonable	O
alternatives	O
.	O
for	O
the	O
speciﬁc	O
situation	O
in	O
(	O
15.2	O
)	O
,	O
:	O
d	O
1	O
as	O
in	O
(	O
15.4	O
)	O
.	O
2	O
[	O
p.	O
274	O
]	O
holm	O
’	O
s	O
procedure	O
.	O
methods	O
of	O
fwer	O
control	B
,	O
including	O
holm	O
’	O
s	O
procedure	O
,	O
are	O
surveyed	O
in	O
chapter	O
3	O
of	O
efron	O
(	O
2010	O
)	O
.	O
they	O
display	O
a	O
large	O
amount	O
of	O
mathematical	O
ingenuity	O
,	O
and	O
provided	O
the	O
background	O
against	O
which	O
fdr	O
theory	B
developed	O
.	O
3	O
[	O
p.	O
276	O
]	O
fdr	O
control	B
theorem	I
.	O
benjamini	O
and	O
hochberg	O
’	O
s	O
striking	O
control	B
theorem	I
(	O
15.15	O
)	O
was	O
rederived	O
by	O
storey	O
et	O
al	O
.	O
(	O
2004	O
)	O
using	O
martingale	O
theory	B
.	O
the	O
basic	O
idea	O
of	O
false	O
discoveries	O
,	O
as	O
displayed	O
in	O
figure	O
15.2	O
,	O
goes	O
back	O
to	O
soric	O
(	O
1989	O
)	O
.	O
4	O
[	O
p.	O
285	O
]	O
formula	B
(	O
15.44	O
)	O
.	O
integrating	O
fdr.z/	O
d	O
(	O
cid:25	O
)	O
0f0.z/=f	O
.z/	O
gives	O
e	O
ffdr.z/jz	O
(	O
cid:21	O
)	O
z0g	O
dz	O
1	O
,	O
z	O
1	O
(	O
cid:25	O
)	O
0f0.z/	O
dz	O
f	O
.z/	O
dz	O
z0	O
d	O
(	O
cid:25	O
)	O
0s0.z0/=s.z0/	O
d	O
fdr.z0/	O
:	O
z0	O
(	O
15.54	O
)	O
5	O
[	O
p.	O
286	O
]	O
thresholds	O
for	O
fdr	O
and	O
fdr	O
.	O
suppose	O
the	O
survival	O
curves	O
s0.z/	O
and	O
s1.z/	O
(	O
15.20	O
)	O
satisfy	O
the	O
“	O
lehmann	O
alternative	O
”	O
relationship	O
log	O
s1.z/	O
d	O
(	O
cid:13	O
)	O
log	O
s0.z/	O
(	O
15.55	O
)	O
15.7	O
notes	O
and	O
details	O
295	O
for	O
large	O
values	O
of	O
z	O
,	O
where	O
(	O
cid:13	O
)	O
is	O
a	O
positive	O
constant	O
less	O
than	O
1	O
.	O
(	O
this	O
is	O
a	O
reasonable	O
condition	B
for	O
the	O
non-null	O
density	B
f1.z/	O
to	O
produce	O
larger	O
pos-	O
itive	O
values	O
of	O
z	O
than	O
does	O
the	O
null	O
density	B
f0.z/	O
.	O
)	O
differentiating	O
(	O
15.55	O
)	O
gives	O
(	O
cid:25	O
)	O
0	O
(	O
cid:25	O
)	O
1	O
f0.z/	O
f1.z/	O
d	O
1	O
(	O
cid:13	O
)	O
(	O
cid:25	O
)	O
0	O
(	O
cid:25	O
)	O
1	O
s0.z/	O
s1.z/	O
(	O
15.56	O
)	O
after	O
some	O
rearrangement	O
.	O
but	O
fdr.z/	O
d	O
(	O
cid:25	O
)	O
0f0.z/=	O
.	O
(	O
cid:25	O
)	O
0f0.z/	O
c	O
(	O
cid:25	O
)	O
1f1.z//	O
is	O
algebraically	O
equivalent	O
to	O
;	O
fdr.z/	O
1	O
(	O
cid:0	O
)	O
fdr.z/	O
d	O
(	O
cid:25	O
)	O
0	O
(	O
cid:25	O
)	O
1	O
f0.z/	O
f1.z/	O
;	O
(	O
15.57	O
)	O
and	O
similarly	O
for	O
fdr.z/=.1	O
(	O
cid:0	O
)	O
fdr.z//	O
,	O
yielding	O
fdr.z/	O
1	O
(	O
cid:0	O
)	O
fdr.z/	O
fdr.z/	O
1	O
(	O
cid:0	O
)	O
fdr.z/	O
d	O
1	O
(	O
cid:13	O
)	O
:	O
(	O
15.58	O
)	O
:	O
d	O
fdr.z/=	O
(	O
cid:13	O
)	O
:	O
for	O
large	O
z	O
,	O
both	O
fdr.z/	O
and	O
fdr.z/	O
go	O
to	O
zero	O
,	O
giving	O
the	O
asymptotic	O
rela-	O
tionship	O
fdr.z/	O
(	O
15.59	O
)	O
this	O
motivates	O
the	O
suggested	O
relative	O
thresholdscfdr.zi	O
/	O
	O
0:20	O
compared	O
if	O
(	O
cid:13	O
)	O
d	O
1=2	O
for	O
instance	O
,	O
fdr.z/	O
will	O
be	O
about	O
twice	O
fdr.z/	O
where	O
z	O
is	O
large	O
.	O
with	O
cfdr.zi	O
/	O
	O
0:10	O
.	O
6	O
[	O
p.	O
288	O
]	O
correlation	O
effects	O
.	O
the	O
poisson	O
regression	B
method	O
used	O
to	O
esti-	O
mate	O
o	O
f	O
.z/	O
in	O
figure	O
15.5	O
proceeds	O
as	O
if	O
the	O
components	O
of	O
the	O
n	O
-vector	O
of	O
zi	O
values	O
z	O
are	O
independent	O
.	O
approximation	O
(	O
10.54	O
)	O
,	O
that	O
the	O
kth	O
bin	O
count	O
yk	O
p	O
(	O
cid:24	O
)	O
poi	O
.	O
(	O
cid:22	O
)	O
k/	O
,	O
requires	O
independence	O
.	O
if	O
not	O
,	O
it	O
can	O
be	O
shown	O
that	O
var.yk/	O
increases	O
above	O
the	O
poisson	O
value	O
(	O
cid:22	O
)	O
k	O
as	O
var.yk/	O
:	O
d	O
(	O
cid:22	O
)	O
k	O
c	O
˛2ck	O
:	O
(	O
15.60	O
)	O
24	O
nx	O
x	O
˛2	O
d	O
35	O
,	O
here	O
ck	O
is	O
a	O
ﬁxed	O
constant	O
depending	O
on	O
f	O
.z/	O
,	O
while	O
˛2	O
is	O
the	O
root	O
mean	O
square	O
correlation	O
between	O
all	O
pairs	O
zi	O
and	O
zj	O
,	O
cov.zi	O
;	O
zj	O
/2	O
n.n	O
(	O
cid:0	O
)	O
1/	O
:	O
(	O
15.61	O
)	O
j¤i	O
id1	O
estimates	O
likecfdr.z/	O
in	O
figure	O
15.5	O
remain	O
nearly	O
unbiased	O
under	O
correla-	O
tion	O
,	O
but	O
their	O
sampling	O
variability	O
increases	O
as	O
a	O
function	B
of	O
˛	O
.	O
chapters	O
7	O
and	O
8	O
of	O
efron	O
(	O
2010	O
)	O
discuss	O
correlation	O
effects	O
in	O
detail	O
.	O
often	O
,	O
˛	O
can	O
be	O
estimated	O
.	O
let	O
x	O
be	O
the	O
6033	O
(	O
cid:2	O
)	O
50	O
matrix	B
of	O
gene	O
ex-	O
pression	O
levels	O
measured	O
for	O
the	O
control	B
subject	O
in	O
the	O
prostate	B
study	O
.	O
rows	O
296	O
large-scale	O
hypothesis	O
testing	B
and	O
fdrs	O
i	O
and	O
j	O
provide	O
an	O
unbiased	O
estimate	O
of	O
cor.zi	O
;	O
zj	O
/2	O
.	O
modern	O
computation	O
is	O
sufﬁciently	O
fast	O
to	O
evaluate	O
all	O
n.n	O
(	O
cid:0	O
)	O
1/=2	O
pairs	O
(	O
though	O
that	O
isn	O
’	O
t	B
nec-	O
essary	O
,	O
sampling	O
is	O
faster	O
)	O
from	O
which	O
estimate	B
o˛	O
is	O
obtained	O
.	O
it	O
equaled	O
0:016˙	O
0:001	O
for	O
the	O
control	B
subjects	O
,	O
and	O
0:015˙	O
0:001	O
for	O
the	O
6033	O
(	O
cid:2	O
)	O
52	O
matrix	B
of	O
the	O
cancer	O
patients	O
.	O
correlation	O
is	O
not	O
much	O
of	O
a	O
worry	O
for	O
the	O
prostate	B
study	O
,	O
but	O
other	O
microarray	O
studies	O
show	O
much	O
larger	O
o˛	O
values	O
.	O
sections	O
6.4	O
and	O
8.3	O
of	O
efron	O
(	O
2010	O
)	O
discuss	O
how	O
correlations	O
can	O
under-	O
cut	O
inferences	O
based	O
on	O
the	O
theoretical	O
null	O
even	O
when	O
it	O
is	O
correct	O
for	O
all	O
the	O
null	O
cases	O
.	O
7	O
[	O
p.	O
289	O
]	O
the	O
program	O
locfdr	B
.	O
available	O
from	O
cran	O
,	O
this	O
is	O
an	O
r	O
pro-	O
gram	O
that	O
provides	O
fdr	O
and	O
fdr	O
estimates	O
,	O
using	O
both	O
the	O
theoretical	O
and	O
empirical	O
null	O
distributions	O
.	O
8	O
[	O
p.	O
289	O
]	O
ml	O
estimation	B
of	O
the	O
empirical	B
null	O
.	O
let	O
a0	O
be	O
the	O
“	O
zero	O
set	B
”	O
(	O
15.49	O
)	O
,	O
z0	O
the	O
set	B
of	O
zi	O
observed	O
to	O
be	O
in	O
a0	O
,	O
i0	O
their	O
indices	O
,	O
and	O
n0	O
the	O
number	O
of	O
zi	O
in	O
a0	O
.	O
also	O
deﬁne	O
(	O
cid:0	O
)	O
1	O
(	O
cid:30	O
)	O
ı0	O
;	O
(	O
cid:27	O
)	O
0	O
.z/	O
d	O
e	O
2	O
(	O
cid:30	O
)	O
q	O
(	O
cid:16	O
)	O
z	O
(	O
cid:0	O
)	O
ı0	O
2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
0	O
;	O
(	O
cid:27	O
)	O
0	O
2	O
p	O
.ı0	O
;	O
(	O
cid:27	O
)	O
0/	O
dz	O
a0g	O
according	O
to	O
(	O
15.48	O
)	O
–	O
(	O
15.49	O
)	O
.	O
)	O
then	O
z0	O
has	O
density	B
''	O
and	O
	O
d	O
(	O
cid:25	O
)	O
0p	O
.ı0	O
;	O
(	O
cid:27	O
)	O
0/	O
:	O
#	O
(	O
cid:30	O
)	O
ı0	O
;	O
(	O
cid:27	O
)	O
0	O
.z/	O
dz	O
(	O
15.62	O
)	O
!	O
a0	O
(	O
so	O
	O
d	O
prfzi	O
2	O
and	O
likelihood	O
#	O
''	O
y	O
fı0	O
;	O
(	O
cid:27	O
)	O
0	O
;	O
(	O
cid:25	O
)	O
0	O
.z0/	O
d	O
	O
n0	O
.1	O
(	O
cid:0	O
)	O
	O
/n	O
(	O
cid:0	O
)	O
n0	O
n	O
n0	O
(	O
cid:30	O
)	O
ı0	O
;	O
(	O
cid:27	O
)	O
0	O
.zi	O
/	O
pı0	O
;	O
(	O
cid:27	O
)	O
0	O
i0	O
;	O
(	O
15.63	O
)	O
the	O
ﬁrst	O
factor	B
being	O
the	O
binomial	B
probability	O
of	O
seeing	O
n0	O
of	O
the	O
zi	O
in	O
a0	O
,	O
and	O
the	O
second	O
the	O
conditional	O
probability	O
of	O
those	O
zi	O
falling	O
within	O
o	O
ı0	O
;	O
o	O
(	O
cid:27	O
)	O
0/	O
,	O
while	O
a0	O
.	O
the	O
second	O
factor	B
is	O
numerically	O
maximized	O
to	O
give	O
.	O
o	O
	O
d	O
n0=n	O
is	O
obtained	O
from	O
the	O
ﬁrst	O
,	O
and	O
then	O
o	O
(	O
cid:25	O
)	O
0	O
d	O
o	O
o	O
ı0	O
;	O
o	O
(	O
cid:27	O
)	O
0/	O
.	O
this	O
is	O
a	O
partial	O
likelihood	B
argument	O
,	O
as	O
in	O
section	O
9.4	O
;	O
locfdr	B
centers	O
a0	O
at	O
the	O
median	O
of	O
the	O
n	O
zi	O
values	O
,	O
with	O
width	O
about	O
twice	O
the	O
interquartile	O
range	O
estimate	O
of	O
(	O
cid:27	O
)	O
0	O
.	O
	O
=p	O
.	O
9	O
[	O
p.	O
290	O
]	O
the	O
permutation	O
null	O
.	O
an	O
impressive	O
amount	O
of	O
theoretical	O
effort	O
concerned	O
the	O
“	O
permutation	O
t-test	O
”	O
:	O
in	O
a	O
single-test	O
two-sample	B
situation	O
,	O
permuting	O
the	O
data	B
and	O
computing	O
the	O
t	B
statistic	O
gives	O
,	O
after	O
a	O
great	O
many	O
repetitions	O
,	O
a	O
histogram	O
dependably	O
close	O
to	O
that	O
of	O
the	O
standard	O
t	O
distri-	O
bution	O
;	O
see	O
hoeffding	O
(	O
1952	O
)	O
.	O
this	O
was	O
fisher	O
’	O
s	O
justiﬁcation	O
for	O
using	O
the	O
standard	O
t-test	O
on	O
nonnormal	O
data	B
.	O
the	O
argument	B
cuts	O
both	O
ways	O
.	O
permutation	O
methods	O
tend	O
to	O
recreate	O
the	O
15.7	O
notes	O
and	O
details	O
297	O
theoretical	O
null	O
,	O
even	O
in	O
situations	O
like	O
that	O
of	O
figure	O
15.7	O
where	O
it	O
isn	O
’	O
t	B
appropriate	O
.	O
the	O
difﬁculties	O
are	O
discussed	O
in	O
section	O
6.5	O
of	O
efron	O
(	O
2010	O
)	O
.	O
10	O
[	O
p.	O
293	O
]	O
relevance	O
theory	B
.	O
suppose	O
that	O
in	O
the	O
dti	O
example	O
shown	O
in	O
fig-	O
ure	O
15.10	O
we	O
want	O
to	O
consider	O
only	O
voxels	O
with	O
x	O
d	O
60	O
as	O
relevant	O
to	O
an	O
observed	O
zi	O
with	O
xi	O
d	O
60.	O
now	O
there	O
may	O
not	O
be	O
enough	O
relevant	O
cases	O
to	O
how	O
the	O
complete-data	O
estimatescfdr.zi	O
/	O
orcfdr.zi	O
/	O
can	O
be	O
efﬁciently	O
mod-	O
adequately	O
estimate	B
fdr.zi	O
/	O
or	O
fdr.zi	O
/	O
.	O
section	O
10.1	O
of	O
efron	O
(	O
2010	O
)	O
shows	O
iﬁed	O
to	O
conform	O
to	O
this	O
situation	O
.	O
16	O
sparse	O
modeling	O
and	O
the	O
lasso	B
the	O
amount	O
of	O
data	B
we	O
are	O
faced	O
with	O
keeps	O
growing	O
.	O
from	O
around	O
the	O
late	O
1990s	O
we	O
started	O
to	O
see	O
wide	O
data	B
sets	O
,	O
where	O
the	O
number	O
of	O
variables	O
far	O
exceeds	O
the	O
number	O
of	O
observations	O
.	O
this	O
was	O
largely	O
due	O
to	O
our	O
in-	O
creasing	O
ability	O
to	O
measure	O
a	O
large	O
amount	O
of	O
information	B
automatically	O
.	O
in	O
genomics	O
,	O
for	O
example	O
,	O
we	O
can	O
use	O
a	O
high-throughput	O
experiment	O
to	O
auto-	O
matically	O
measure	O
the	O
expression	O
of	O
tens	O
of	O
thousands	O
of	O
genes	O
in	O
a	O
sam-	O
ple	O
in	O
a	O
short	O
amount	O
of	O
time	O
.	O
similarly	O
,	O
sequencing	O
equipment	O
allows	O
us	O
to	O
genotype	O
millions	O
of	O
snps	O
(	O
single-nucleotide	O
polymorphisms	O
)	O
cheaply	O
and	O
quickly	O
.	O
in	O
document	O
retrieval	O
and	O
modeling	O
,	O
we	O
represent	O
a	O
document	O
by	O
the	O
presence	O
or	O
count	O
of	O
each	O
word	O
in	O
the	O
dictionary	O
.	O
this	O
easily	O
leads	O
to	O
a	O
feature	O
vector	B
with	O
20,000	O
components	O
,	O
one	O
for	O
each	O
distinct	O
vocabulary	O
word	O
,	O
although	O
most	O
would	O
be	O
zero	O
for	O
a	O
small	O
document	O
.	O
if	O
we	O
move	O
to	O
bi-grams	O
or	O
higher	O
,	O
the	O
feature	O
space	B
gets	O
really	O
large	O
.	O
in	O
even	O
more	O
modest	O
situations	O
,	O
we	O
can	O
be	O
faced	O
with	O
hundreds	O
of	O
vari-	O
ables	O
.	O
if	O
these	O
variables	O
are	O
to	O
be	O
predictors	O
in	O
a	O
regression	B
or	O
logistic	O
re-	O
gression	O
model	B
,	O
we	O
probably	O
do	O
not	O
want	O
to	O
use	O
them	O
all	O
.	O
it	O
is	O
likely	O
that	O
a	O
subset	O
will	O
do	O
the	O
job	O
well	O
,	O
and	O
including	O
all	O
the	O
redundant	O
variables	O
will	O
degrade	O
our	O
ﬁt	O
.	O
hence	O
we	O
are	O
often	O
interested	O
in	O
identifying	O
a	O
good	O
subset	O
of	O
variables	O
.	O
note	O
also	O
that	O
in	O
these	O
wide-data	O
situations	O
,	O
even	O
linear	B
mod-	O
els	O
are	O
over-parametrized	O
,	O
so	O
some	O
form	B
of	O
reduction	O
or	O
regularization	B
is	O
essential	O
.	O
in	O
this	O
chapter	O
we	O
will	O
discuss	O
some	O
of	O
the	O
popular	O
methods	O
for	O
model	B
selection	I
,	O
starting	O
with	O
the	O
time-tested	O
and	O
worthy	O
forward-stepwise	O
ap-	O
proach	O
.	O
we	O
then	O
look	O
at	O
the	O
lasso	B
,	O
a	O
popular	O
modern	O
method	B
that	O
does	O
se-	O
lection	O
and	O
shrinkage	O
via	O
convex	O
optimization	O
.	O
the	O
lars	B
algorithm	O
ties	O
these	O
two	O
approaches	O
together	O
,	O
and	O
leads	O
to	O
methods	O
that	O
can	O
deliver	O
paths	O
of	O
solutions	O
.	O
finally	O
,	O
we	O
discuss	O
some	O
connections	O
with	O
other	O
modern	O
big-	O
and	O
wide-	O
data	B
approaches	O
,	O
and	O
mention	O
some	O
extensions	O
.	O
298	O
16.1	O
forward	O
stepwise	O
regression	B
299	O
16.1	O
forward	O
stepwise	O
regression	B
stepwise	O
procedures	O
have	O
been	O
around	O
for	O
a	O
very	O
long	O
time	O
.	O
they	O
were	O
originally	O
devised	O
in	O
times	O
when	O
data	B
sets	O
were	O
quite	O
modest	O
in	O
size	O
,	O
in	O
particular	O
in	O
terms	O
of	O
the	O
number	O
of	O
variables	O
.	O
originally	O
thought	O
of	O
as	O
the	O
poor	O
cousins	O
of	O
“	O
best-subset	O
”	O
selection	O
,	O
they	O
had	O
the	O
advantage	O
of	O
being	O
much	O
cheaper	O
to	O
compute	O
(	O
and	O
in	O
fact	O
possible	O
to	O
compute	O
for	O
large	O
p	O
)	O
.	O
we	O
will	O
review	O
best-subset	O
regression	B
ﬁrst	O
.	O
i	O
0	O
suppose	O
we	O
have	O
a	O
set	B
of	O
n	O
observations	O
on	O
a	O
response	O
yi	O
and	O
a	O
vec-	O
d	O
.xi1	O
;	O
xi	O
2	O
;	O
:	O
:	O
:	O
;	O
xip/	O
,	O
and	O
we	O
plan	O
to	O
ﬁt	O
a	O
linear	B
tor	O
of	O
p	O
predictors	O
x	O
regression	B
model	I
.	O
the	O
response	O
could	O
be	O
quantitative	O
,	O
so	O
we	O
can	O
think	O
of	O
ﬁtting	B
a	O
linear	B
model	I
by	O
least	B
squares	I
.	O
it	O
could	O
also	O
be	O
binary	O
,	O
leading	O
to	O
a	O
linear	B
logistic	O
regression	B
model	I
ﬁt	O
by	O
maximum	B
likelihood	I
.	O
although	O
we	O
will	O
focus	O
on	O
these	O
two	O
cases	O
,	O
the	O
same	O
ideas	O
transfer	O
exactly	O
to	O
other	O
gen-	O
eralized	O
linear	B
models	O
,	O
the	O
cox	O
model	B
,	O
and	O
so	O
on	O
.	O
the	O
idea	O
is	O
to	O
build	O
a	O
model	B
using	O
a	O
subset	O
of	O
the	O
variables	O
;	O
in	O
fact	O
the	O
smallest	O
subset	O
that	O
ade-	O
quately	O
explains	O
the	O
variation	O
in	O
the	O
response	O
is	O
what	O
we	O
are	O
after	O
,	O
both	O
for	O
inference	B
and	O
for	O
prediction	O
purposes	O
.	O
suppose	O
our	O
loss	B
function	I
for	O
ﬁtting	B
the	O
linear	B
model	I
is	O
l	O
(	O
e.g	O
.	O
sum	O
of	O
squares	B
,	O
negative	O
log-likelihood	B
)	O
.	O
the	O
method	B
of	O
best-subset	O
regression	B
is	O
simple	O
to	O
describe	O
,	O
and	O
is	O
given	O
in	O
al-	O
gorithm	O
16.1.	O
step	O
3	O
is	O
easy	O
to	O
state	O
,	O
but	O
requires	O
a	O
lot	O
of	O
computation	O
.	O
for	O
algorithm	B
16.1	O
best-subset	O
regression	B
.	O
1	O
start	O
with	O
m	O
d	O
0	O
and	O
the	O
null	O
model	B
o0.x/	O
d	O
o	O
ˇ0	O
,	O
estimated	O
by	O
the	O
mean	O
of	O
the	O
yi	O
.	O
2	O
at	O
step	O
m	O
d	O
1	O
,	O
pick	O
the	O
single	O
variable	O
j	O
that	O
ﬁts	O
the	O
response	O
best	O
,	O
in	O
terms	O
of	O
the	O
loss	O
l	O
evaluated	O
on	O
the	O
training	O
data	B
,	O
in	O
a	O
univariate	O
regression	B
o1.x/	O
d	O
o	O
3	O
for	O
each	O
subset	O
size	O
m	O
2	O
f2	O
;	O
3	O
;	O
:	O
:	O
:	O
;	O
mg	O
(	O
with	O
m	O
	O
min.n	O
(	O
cid:0	O
)	O
1	O
;	O
p/	O
)	O
identify	O
the	O
best	O
subset	O
am	O
of	O
size	O
m	O
when	O
ﬁtting	B
a	O
linear	B
model	I
om.x/	O
d	O
o	O
o	O
ˇam	O
with	O
m	O
of	O
the	O
p	O
variables	O
,	O
in	O
terms	O
of	O
the	O
loss	O
l.	O
4	O
use	O
some	O
external	B
data	O
or	O
other	O
means	O
to	O
select	O
the	O
“	O
best	O
”	O
amongst	O
these	O
o	O
ˇj	O
.	O
set	B
a1	O
d	O
fjg	O
.	O
ˇ0	O
c	O
x	O
ˇ0	O
c	O
x	O
0	O
j	O
0	O
am	O
m	O
models	B
.	O
p	O
much	O
larger	O
than	O
about	O
40	O
it	O
becomes	O
prohibitively	O
expensive	O
to	O
perform	O
exactly—a	O
so-called	O
“	O
n-p	O
complete	O
”	O
problem	O
because	O
of	O
its	O
combinatorial	O
complexity	O
(	O
there	O
are	O
2p	O
subsets	O
)	O
.	O
note	O
that	O
the	O
subsets	O
need	O
not	O
be	O
nested	O
:	O
sparse	O
modeling	O
and	O
the	O
lasso	B
300	O
the	O
best	O
subset	O
of	O
size	O
m	O
d	O
3	O
,	O
say	O
,	O
need	O
not	O
include	O
both	O
or	O
any	O
of	O
the	O
variables	O
in	O
the	O
best	O
subset	O
of	O
size	O
m	O
d	O
2.	O
in	O
step	O
4	O
there	O
are	O
a	O
number	O
of	O
methods	O
for	O
selecting	O
m.	O
originally	O
the	O
cp	O
criterion	O
of	O
chapter	O
12	O
was	O
proposed	O
for	O
this	O
purpose	O
.	O
here	O
we	O
will	O
favor	O
k-fold	O
cross-validation	O
,	O
since	O
it	O
is	O
applicable	O
to	O
all	O
the	O
methods	O
dis-	O
cussed	O
in	O
this	O
chapter	O
.	O
it	O
is	O
interesting	O
to	O
digress	O
for	O
a	O
moment	O
on	O
how	O
cross-validation	O
works	O
here	O
.	O
we	O
are	O
using	O
it	O
to	O
select	O
the	O
subset	O
size	O
m	O
on	O
the	O
basis	O
of	O
prediction	O
performance	O
(	O
on	O
future	O
data	B
)	O
.	O
with	O
k	O
d	O
10	O
,	O
we	O
divide	O
the	O
n	O
training	O
obser-	O
vations	O
randomly	O
into	O
10	O
equal	O
size	O
groups	O
.	O
leaving	O
out	O
say	O
group	O
k	O
d	O
1	O
,	O
we	O
perform	O
steps	O
1–3	O
on	O
the	O
9=10ths	O
,	O
and	O
for	O
each	O
of	O
the	O
chosen	O
models	B
,	O
we	O
summarize	O
the	O
prediction	O
performance	O
on	O
the	O
group-1	O
data	B
.	O
we	O
do	O
this	O
k	O
d	O
10	O
times	O
,	O
each	O
time	O
with	O
group	O
k	O
left	O
out	O
.	O
we	O
then	O
average	O
the	O
10	O
performance	O
measures	O
for	O
each	O
m	O
,	O
and	O
select	O
the	O
value	O
of	O
m	O
correspond-	O
ing	O
to	O
the	O
best	O
performance	O
.	O
notice	O
that	O
for	O
each	O
m	O
,	O
the	O
10	O
models	B
om.x/	O
might	O
involve	O
different	O
subsets	O
of	O
variables	O
!	O
this	O
is	O
not	O
a	O
concern	O
,	O
since	O
we	O
are	O
trying	O
to	O
ﬁnd	O
a	O
good	O
value	O
of	O
m	O
for	O
the	O
method	B
.	O
having	O
identiﬁed	O
om	O
,	O
we	O
rerun	O
steps	O
1–3	O
on	O
the	O
entire	O
training	O
set	B
,	O
and	O
deliver	O
the	O
chosen	O
model	B
o	O
om.x/	O
.	O
as	O
hinted	O
above	O
,	O
there	O
are	O
problems	O
with	O
best-subset	O
regression	B
.	O
a	O
pri-	O
mary	O
issue	O
is	O
that	O
it	O
works	O
exactly	O
only	O
for	O
relatively	O
small	O
p.	O
for	O
example	O
,	O
we	O
can	O
not	O
run	O
it	O
on	O
the	O
spam	B
data	O
with	O
57	O
variables	O
(	O
at	O
least	O
not	O
in	O
2015	O
on	O
a	O
macbook	O
pro	O
!	O
)	O
.	O
we	O
may	O
also	O
think	O
that	O
even	O
if	O
we	O
could	O
do	O
the	O
compu-	O
tations	O
,	O
with	O
such	O
a	O
large	O
search	O
space	B
the	O
variance	O
of	O
the	O
procedure	O
might	O
be	O
too	O
high	O
.	O
am	O
(	O
cid:0	O
)	O
1	O
(	O
cid:26	O
)	O
as	O
a	O
result	O
,	O
more	O
manageable	O
stepwise	O
procedures	O
were	O
invented	O
.	O
for-	O
ward	O
stepwise	O
regression	B
,	O
algorithm	B
16.2	O
,	O
is	O
a	O
simple	O
modiﬁcation	B
of	O
best-	O
subset	O
,	O
with	O
the	O
modiﬁcation	B
occurring	O
in	O
step	O
3.	O
forward	O
stepwise	O
re-	O
gression	O
produces	O
a	O
nested	O
sequence	O
of	O
models	B
;	O
:	O
:	O
:	O
(	O
cid:26	O
)	O
am	O
(	O
cid:26	O
)	O
amc1	O
:	O
:	O
:	O
.	O
it	O
starts	O
with	O
the	O
null	O
model	B
,	O
here	O
an	O
intercept	O
,	O
and	O
adds	O
vari-	O
ables	O
one	O
at	O
a	O
time	O
.	O
even	O
with	O
large	O
p	O
,	O
identifying	O
the	O
best	O
variable	O
to	O
add	O
at	O
each	O
step	O
is	O
manageable	O
,	O
and	O
can	O
be	O
distributed	O
if	O
clusters	O
of	O
machines	O
are	O
available	O
.	O
most	O
importantly	O
,	O
it	O
is	O
feasible	O
for	O
large	O
p.	O
figure	O
16.1	O
shows	O
the	O
coefﬁcient	O
proﬁles	O
for	O
forward-stepwise	O
linear	B
regression	O
on	O
the	O
spam	B
training	O
data	B
.	O
here	O
there	O
are	O
57	O
input	O
variables	O
(	O
relative	O
prevalence	O
of	O
par-	O
ticular	O
words	O
in	O
the	O
document	O
)	O
,	O
and	O
an	O
“	O
ofﬁcial	O
”	O
(	O
train	O
,	O
test	O
)	O
split	O
of	O
(	O
3065	O
,	O
1536	O
)	O
observations	O
.	O
the	O
response	O
is	O
coded	O
as	O
+1	O
if	O
the	O
email	O
was	O
spam	B
,	O
else	O
-1.	O
the	O
ﬁgure	O
caption	O
gives	O
the	O
details	O
.	O
we	O
saw	O
the	O
spam	B
data	O
earlier	O
,	O
in	O
table	O
8.3	O
,	O
figure	O
8.7	O
and	O
figure	O
12.2.	O
fitting	O
the	O
entire	O
forward-stepwise	O
linear	B
regression	O
path	B
as	O
in	O
the	O
ﬁgure	O
16.1	O
forward	O
stepwise	O
regression	B
301	O
algorithm	B
16.2	O
forward	O
stepwise	O
regression	B
.	O
1	O
start	O
with	O
m	O
d	O
0	O
and	O
the	O
null	O
model	B
o0.x/	O
d	O
o	O
ˇ0	O
,	O
estimated	O
by	O
the	O
mean	O
of	O
the	O
yi	O
.	O
2	O
at	O
step	O
m	O
d	O
1	O
,	O
pick	O
the	O
single	O
variable	O
j	O
that	O
ﬁts	O
the	O
response	O
best	O
,	O
in	O
terms	O
of	O
the	O
loss	O
l	O
evaluated	O
on	O
the	O
training	O
data	B
,	O
in	O
a	O
univariate	O
regression	B
o1.x/	O
d	O
o	O
3	O
for	O
each	O
subset	O
size	O
m	O
2	O
f2	O
;	O
3	O
;	O
:	O
:	O
:	O
;	O
mg	O
(	O
with	O
m	O
	O
min.n	O
(	O
cid:0	O
)	O
1	O
;	O
p/	O
)	O
identify	O
the	O
variable	O
k	O
that	O
when	O
augmented	O
with	O
am	O
(	O
cid:0	O
)	O
1	O
to	O
form	B
am	O
,	O
leads	O
to	O
the	O
model	B
om.x/	O
d	O
o	O
o	O
ˇam	O
that	O
performs	O
best	O
in	O
terms	O
of	O
the	O
loss	O
l.	O
4	O
use	O
some	O
external	B
data	O
or	O
other	O
means	O
to	O
select	O
the	O
“	O
best	O
”	O
amongst	O
these	O
o	O
ˇj	O
.	O
set	B
a1	O
d	O
fjg	O
.	O
ˇ0	O
c	O
x	O
ˇ0	O
c	O
x	O
0	O
j	O
0	O
am	O
m	O
models	B
.	O
(	O
when	O
n	O
>	O
p	O
)	O
has	O
essentially	O
the	O
same	O
cost	O
as	O
a	O
single	O
least	B
squares	I
ﬁt	O
on	O
all	O
the	O
variables	O
.	O
this	O
is	O
because	O
the	O
sequence	O
of	O
models	B
can	O
be	O
updated	O
each	O
time	O
a	O
variable	O
is	O
added.however	O
,	O
this	O
is	O
a	O
consequence	O
of	O
the	O
linear	B
1	O
model	B
and	O
squared-error	O
loss	O
.	O
suppose	O
instead	O
we	O
run	O
a	O
forward	O
stepwise	O
logistic	B
regression	I
.	O
here	O
up-	O
dating	O
does	O
not	O
work	O
,	O
and	O
the	O
entire	O
ﬁt	O
has	O
to	O
be	O
recomputed	O
by	O
maximum	B
likelihood	I
each	O
time	O
a	O
variable	O
is	O
added	O
.	O
identifying	O
which	O
variable	O
to	O
add	O
in	O
step	O
3	O
in	O
principle	O
requires	O
ﬁtting	B
an	O
.m	O
c	O
1/-variable	O
model	B
p	O
(	O
cid:0	O
)	O
m	O
times	O
,	O
and	O
seeing	O
which	O
one	O
reduces	O
the	O
deviance	O
the	O
most	O
.	O
in	O
practice	O
,	O
we	O
can	O
use	O
score	O
tests	O
which	O
are	O
much	O
cheaper	O
to	O
evaluate	O
.	O
	O
these	O
amount	O
2	O
to	O
using	O
the	O
quadratic	O
approximation	O
to	O
the	O
log-likelihood	B
from	O
the	O
ﬁnal	O
iteratively	O
reweighted	O
least-squares	O
(	O
irls	O
)	O
iteration	O
for	O
ﬁtting	B
the	O
model	B
with	O
m	O
terms	O
.	O
the	O
score	O
test	O
for	O
a	O
variable	O
not	O
in	O
the	O
model	B
is	O
equivalent	O
to	O
testing	B
for	O
the	O
inclusion	O
of	O
this	O
variable	O
in	O
the	O
weighted	O
least-squares	O
ﬁt	O
.	O
hence	O
identifying	O
the	O
next	O
variable	O
is	O
almost	O
back	O
to	O
the	O
previous	O
cases	O
,	O
requiring	O
p	O
(	O
cid:0	O
)	O
m	O
simple	O
regression	B
updates	O
.	O
	O
figure	O
16.2	O
shows	O
the	O
test	O
3	O
misclassiﬁcation	O
error	O
for	O
forward-stepwise	O
linear	B
regression	O
and	O
logistic	O
regression	B
on	O
the	O
spam	B
data	O
,	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
steps	O
.	O
they	O
both	O
level	O
off	O
at	O
around	O
25	O
steps	O
,	O
and	O
have	O
a	O
similar	O
shape	O
.	O
however	O
,	O
the	O
logistic	B
regression	I
gives	O
more	O
accurate	O
classiﬁcations.1	O
although	O
forward-stepwise	O
methods	O
are	O
possible	O
for	O
large	O
p	O
,	O
they	O
get	O
tedious	O
for	O
very	O
large	O
p	O
(	O
in	O
the	O
thousands	O
)	O
,	O
especially	O
if	O
the	O
data	B
could	O
sup-	O
1	O
for	O
this	O
example	O
we	O
can	O
halve	O
the	O
gap	O
between	O
the	O
curves	O
by	O
optimizing	O
the	O
prediction	O
threshold	O
for	O
linear	B
regression	O
.	O
302	O
sparse	O
modeling	O
and	O
the	O
lasso	B
figure	O
16.1	O
forward	O
stepwise	O
linear	B
regression	O
on	O
the	O
spam	B
data	O
.	O
each	O
curve	O
corresponds	O
to	O
a	O
particular	O
variable	O
,	O
and	O
shows	O
the	O
progression	O
of	O
its	O
coefﬁcient	O
as	O
the	O
model	B
grows	O
.	O
these	O
are	O
plotted	O
against	O
the	O
training	O
r2	O
,	O
and	O
the	O
vertical	O
gray	O
bars	O
correspond	O
to	O
each	O
step	O
.	O
starting	O
at	O
the	O
left	O
at	O
step	O
1	O
,	O
the	O
ﬁrst	O
selected	O
variable	O
explains	O
r2	O
d	O
0:16	O
;	O
adding	O
the	O
second	O
increases	O
r2	O
to	O
0:25	O
,	O
etc	O
.	O
what	O
we	O
see	O
is	O
that	O
early	O
steps	O
have	O
a	O
big	O
impact	O
on	O
the	O
r2	O
,	O
while	O
later	O
steps	O
hardly	O
have	O
any	O
at	O
all	O
.	O
the	O
vertical	O
black	O
line	O
corresponds	O
to	O
step	O
25	O
(	O
see	O
figure	O
16.2	O
)	O
,	O
and	O
we	O
see	O
that	O
after	O
that	O
the	O
step-wise	O
improvements	O
in	O
r2	O
are	O
negligible	O
.	O
port	O
a	O
model	B
with	O
many	O
variables	O
.	O
however	O
,	O
if	O
the	O
ideal	O
active	O
set	B
is	O
fairly	O
small	O
,	O
even	O
with	O
many	O
thousands	O
of	O
variables	O
forward-stepwise	O
selection	O
is	O
a	O
viable	O
option	O
.	O
forward-stepwise	O
selection	O
delivers	O
a	O
sequence	O
of	O
models	B
,	O
as	O
seen	O
in	O
the	O
previous	O
ﬁgures	O
.	O
one	O
would	O
generally	O
want	O
to	O
select	O
a	O
single	O
model	B
,	O
and	O
as	O
discussed	O
earlier	O
,	O
we	O
often	O
use	O
cross-validation	O
for	O
this	O
purpose	O
.	O
figure	O
16.3	O
illustrates	O
using	O
stepwise	O
linear	B
regression	O
on	O
the	O
spam	B
data	O
.	O
here	O
the	O
sequence	O
of	O
models	B
are	O
ﬁt	O
using	O
squared-error	O
loss	O
on	O
the	O
bi-	O
nary	O
response	O
variable	O
.	O
however	O
,	O
cross-validation	O
scores	O
each	O
model	B
for	O
misclassiﬁcation	O
error	O
,	O
the	O
ultimate	O
goal	O
of	O
this	O
modeling	O
exercise	O
.	O
this	O
highlights	O
one	O
of	O
the	O
advantages	O
of	O
cross-validation	O
in	O
this	O
context	O
.	O
a	O
con-	O
0.00.10.20.30.40.50.6−0.20.00.20.40.60.8forward−stepwise	O
regressionr2	O
on	O
training	O
datacoefficients	O
16.2	O
the	O
lasso	B
303	O
figure	O
16.2	O
forward-stepwise	O
regression	B
on	O
the	O
spam	B
data	O
.	O
shown	O
is	O
the	O
misclassiﬁcation	O
error	O
on	O
the	O
test	O
data	B
,	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
steps	O
.	O
the	O
brown	O
dots	O
correspond	O
to	O
linear	B
regression	O
,	O
with	O
the	O
response	O
coded	O
as	O
-1	O
and	O
+1	O
;	O
a	O
prediction	O
greater	O
than	O
zero	O
is	O
classiﬁed	O
as	O
+1	O
,	O
one	O
less	O
than	O
zero	O
as	O
-1.	O
the	O
blue	O
dots	O
correspond	O
to	O
logistic	B
regression	I
,	O
which	O
performs	O
better	O
.	O
we	O
see	O
that	O
both	O
curves	O
essentially	O
reach	O
their	O
minima	O
after	O
25	O
steps	O
.	O
venient	O
(	O
differentiable	O
and	O
smooth	O
)	O
loss	B
function	I
is	O
used	O
to	O
ﬁt	O
the	O
sequence	O
of	O
models	B
.	O
however	O
,	O
we	O
can	O
use	O
any	O
performance	O
measure	O
to	O
evaluate	O
the	O
sequence	O
of	O
models	B
;	O
here	O
misclassiﬁcation	O
error	O
is	O
used	O
.	O
in	O
terms	O
of	O
the	O
parameters	O
of	O
the	O
linear	B
model	I
,	O
misclassiﬁcation	O
error	O
would	O
be	O
a	O
difﬁcult	O
and	O
discontinuous	O
loss	B
function	I
to	O
use	O
for	O
parameter	O
estimation	B
.	O
all	O
we	O
need	O
to	O
use	O
it	O
for	O
here	O
is	O
pick	O
the	O
best	O
model	B
size	O
.	O
there	O
appears	O
to	O
be	O
little	O
beneﬁt	O
in	O
going	O
beyond	O
25–30	O
terms	O
.	O
16.2	O
the	O
lasso	B
the	O
stepwise	O
model-selection	O
methods	O
of	O
the	O
previous	O
section	O
are	O
useful	O
if	O
we	O
anticipate	O
a	O
model	B
using	O
a	O
relatively	O
small	O
number	O
of	O
variables	O
,	O
even	O
if	O
the	O
pool	O
of	O
available	O
variables	O
is	O
very	O
large	O
.	O
if	O
we	O
expect	O
a	O
moderate	O
number	O
of	O
variables	O
to	O
play	O
a	O
role	O
,	O
these	O
methods	O
become	O
cumbersome	O
.	O
llllllllllllllllllllllllllllllllllllllllllllllllllllllllll01020304050600.00.10.20.30.4spam	O
datasteptest	O
misclassification	O
errorllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllforward−stepwise	O
linear	B
regressionforward−stepwise	O
logistic	B
regression	I
304	O
sparse	O
modeling	O
and	O
the	O
lasso	B
figure	O
16.3	O
ten-fold	O
cross-validated	O
misclassiﬁcation	O
errors	B
(	O
green	O
)	O
for	O
forward-stepwise	O
regression	B
on	O
the	O
spam	B
data	O
,	O
as	O
a	O
function	B
of	O
the	O
step	O
number	O
.	O
since	O
each	O
error	O
is	O
an	O
average	O
of	O
10	O
numbers	O
,	O
we	O
can	O
compute	O
a	O
(	O
crude	O
)	O
standard	B
error	I
;	O
included	O
in	O
the	O
plot	O
are	O
pointwise	O
standard-error	O
bands	O
.	O
the	O
brown	O
curve	O
is	O
the	O
misclassiﬁcation	O
error	O
on	O
the	O
test	O
data	B
.	O
another	O
black	O
mark	O
against	O
forward-stepwise	O
methods	O
is	O
that	O
the	O
sequence	O
of	O
models	B
is	O
derived	O
in	O
a	O
greedy	O
fashion	O
,	O
without	O
any	O
claimed	O
optimality	O
.	O
the	O
methods	O
we	O
describe	O
here	O
are	O
derived	O
from	O
a	O
more	O
principled	O
proce-	O
dure	O
;	O
indeed	O
they	O
solve	O
a	O
convex	O
optimization	O
,	O
as	O
deﬁned	O
below	O
.	O
we	O
will	O
ﬁrst	O
present	O
the	O
lasso	B
for	O
squared-error	O
loss	O
,	O
and	O
then	O
the	O
more	O
i	O
ˇ/2	O
subject	O
to	O
kˇk1	O
	O
t	B
;	O
0	O
general	O
case	O
later	O
.	O
consider	O
the	O
constrained	O
linear	B
regression	O
problem	O
where	O
kˇk1	O
dpp	O
nx	O
.yi	O
(	O
cid:0	O
)	O
ˇ0	O
(	O
cid:0	O
)	O
x	O
(	O
16.1	O
)	O
id1	O
jˇjj	O
,	O
the	O
`1	O
norm	O
of	O
the	O
coefﬁcient	O
vector	B
.	O
since	O
both	O
jd1	O
the	O
loss	O
and	O
the	O
constraint	O
are	O
convex	O
in	O
ˇ	O
,	O
this	O
is	O
a	O
convex	O
optimization	O
problem	O
,	O
and	O
it	O
is	O
known	O
as	O
the	O
lasso	B
.	O
the	O
constraint	O
kˇk1	O
	O
t	B
restricts	O
the	O
coefﬁcients	O
of	O
the	O
model	B
by	O
pulling	O
them	O
toward	O
zero	O
;	O
this	O
has	O
the	O
effect	O
of	O
reducing	O
their	O
variance	O
,	O
and	O
prevents	O
overﬁtting	O
.	O
ridge	B
regression	I
is	O
an	O
earlier	O
great	O
uncle	O
of	O
the	O
lasso	B
,	O
and	O
solves	O
a	O
similar	O
problem	O
to	O
(	O
16.1	O
)	O
,	O
ex-	O
minimize	O
ˇ02r	O
;	O
ˇ2rp	O
1	O
n	O
llllllllllllllllllllllllllllllllllllllllllllllllllllllllll010203040500.00.10.20.30.4spam	O
datasteptest	O
and	O
cv	O
misclassification	O
errorlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllltest	O
error10−fold	O
cv	O
error	O
16.2	O
the	O
lasso	B
305	O
figure	O
16.4	O
an	O
example	O
with	O
ˇ	O
2	O
r2	O
to	O
illustrate	O
the	O
difference	O
between	O
ridge	B
regression	I
and	O
the	O
lasso	B
.	O
in	O
both	O
plots	O
,	O
the	O
red	O
contours	O
correspond	O
to	O
the	O
squared-error	O
loss	B
function	I
,	O
with	O
the	O
unrestricted	O
least-squares	O
estimate	B
o	O
regions	O
show	O
the	O
constraints	O
,	O
with	O
the	O
lasso	B
on	O
the	O
left	O
and	O
ridge	O
on	O
the	O
right	O
.	O
the	O
solution	O
to	O
the	O
constrained	O
problem	O
corresponds	O
to	O
the	O
value	O
of	O
ˇ	O
where	O
the	O
expanding	O
loss	O
contours	O
ﬁrst	O
touch	O
the	O
constraint	O
region	B
.	O
due	O
to	O
the	O
shape	O
of	O
the	O
lasso	B
constraint	O
,	O
this	O
will	O
often	O
be	O
at	O
a	O
corner	O
(	O
or	O
an	O
edge	O
more	O
generally	O
)	O
,	O
as	O
here	O
,	O
which	O
means	O
in	O
this	O
case	O
that	O
the	O
minimizing	O
ˇ	O
has	O
ˇ1	O
d	O
0.	O
for	O
the	O
ridge	O
constraint	O
,	O
this	O
is	O
unlikely	O
to	O
happen	O
.	O
ˇ	O
in	O
the	O
center	O
.	O
the	O
blue	O
cept	O
the	O
constraint	O
is	O
kˇk2	O
	O
t	B
;	O
ridge	B
regression	I
bounds	O
the	O
quadratic	O
`2	O
norm	O
of	O
the	O
coefﬁcient	O
vector	B
.	O
it	O
also	O
has	O
the	O
effect	O
of	O
pulling	O
the	O
coefﬁ-	O
cients	O
toward	O
zero	O
,	O
in	O
an	O
apparently	O
very	O
similar	O
way	O
.	O
ridge	B
regression	I
is	O
discussed	O
in	O
section	O
7.3.2	O
both	O
the	O
lasso	B
and	O
ridge	B
regression	I
are	O
shrinkage	B
methods	O
,	O
in	O
the	O
spirit	O
of	O
the	O
james–stein	O
estimator	B
of	O
chapter	O
7.	O
a	O
big	O
difference	O
,	O
however	O
,	O
is	O
that	O
for	O
the	O
lasso	B
,	O
the	O
solution	O
typically	O
has	O
many	O
of	O
the	O
ˇj	O
equal	O
to	O
zero	O
,	O
while	O
for	O
ridge	O
they	O
are	O
all	O
nonzero	O
.	O
hence	O
the	O
lasso	B
does	O
variable	O
selection	O
and	O
shrinkage	O
,	O
while	O
ridge	O
only	O
shrinks	O
.	O
figure	O
16.4	O
illustrates	O
this	O
for	O
ˇ	O
2	O
r2	O
.	O
in	O
higher	O
dimensions	O
,	O
the	O
`1	O
norm	O
has	O
sharp	O
edges	O
and	O
corners	O
,	O
which	O
correspond	O
to	O
coefﬁcient	O
estimates	O
zero	O
in	O
ˇ.	O
since	O
the	O
constraint	O
in	O
the	O
lasso	B
treats	O
all	O
the	O
coefﬁcients	O
equally	O
,	O
it	O
usu-	O
ally	O
makes	O
sense	O
for	O
all	O
the	O
elements	O
of	O
x	O
to	O
be	O
in	O
the	O
same	O
units	O
.	O
if	O
not	O
,	O
we	O
2	O
here	O
we	O
use	O
the	O
“	O
bound	B
”	O
form	B
of	O
ridge	B
regression	I
,	O
while	O
in	O
section	O
7.3	O
we	O
use	O
the	O
“	O
lagrange	O
”	O
form	B
.	O
they	O
are	O
equivalent	O
,	O
in	O
that	O
for	O
every	O
“	O
lagrange	O
”	O
solution	O
,	O
there	O
is	O
a	O
corresponding	O
bound	B
solution	O
.	O
β^β^2..β1β2β1β	O
306	O
sparse	O
modeling	O
and	O
the	O
lasso	B
typically	O
standardize	O
the	O
predictors	O
beforehand	O
so	O
that	O
each	O
has	O
variance	O
one	O
.	O
two	O
natural	O
boundary	O
values	O
for	O
t	B
in	O
(	O
16.1	O
)	O
are	O
t	B
d	O
0	O
and	O
t	O
d	O
1.	O
the	O
former	O
corresponds	O
to	O
the	O
constant	O
model	B
(	O
the	O
ﬁt	O
is	O
the	O
mean	O
of	O
the	O
yi	O
,	O
)	O
3	O
and	O
the	O
latter	O
corresponds	O
to	O
the	O
unrestricted	O
least-squares	O
ﬁt	O
.	O
in	O
fact	O
,	O
if	O
n	O
>	O
p	O
,	O
and	O
o	O
ˇ	O
is	O
the	O
least-squares	O
estimate	B
,	O
then	O
we	O
can	O
replace	O
1	O
by	O
k	O
o	O
ˇk1	O
,	O
and	O
any	O
value	O
of	O
t	B
(	O
cid:21	O
)	O
k	O
o	O
ˇk1	O
is	O
a	O
non-binding	O
constraint.	O
figure	O
16.5	O
4	O
figure	O
16.5	O
the	O
lasso	B
linear	O
regression	B
regularization	O
path	B
on	O
the	O
spam	B
data	O
.	O
each	O
curve	O
corresponds	O
to	O
a	O
particular	O
variable	O
,	O
and	O
shows	O
the	O
progression	O
of	O
its	O
coefﬁcient	O
as	O
the	O
regularization	B
bound	O
t	B
grows	O
.	O
these	O
curves	O
are	O
plotted	O
against	O
the	O
training	O
r2	O
rather	O
than	O
t	B
,	O
to	O
make	O
the	O
curves	O
comparable	O
with	O
the	O
forward-stepwise	O
curves	O
in	O
figure	O
16.1.	O
some	O
values	O
of	O
t	B
are	O
indicated	O
at	O
the	O
top	O
.	O
the	O
vertical	O
gray	O
bars	O
indicate	O
changes	O
in	O
the	O
active	O
set	B
of	O
nonzero	O
coefﬁcients	O
,	O
typically	O
an	O
inclusion	O
.	O
here	O
we	O
see	O
clearly	O
the	O
role	O
of	O
the	O
`1	O
penalty	O
;	O
as	O
t	B
is	O
relaxed	O
,	O
coefﬁcients	O
become	O
nonzero	O
,	O
but	O
in	O
a	O
smoother	O
fashion	O
than	O
in	O
forward	O
stepwise	O
.	O
shows	O
the	O
regularization	B
path4	O
for	O
the	O
lasso	B
linear	O
regression	B
problem	O
on	O
3	O
we	O
typically	O
do	O
not	O
restrict	O
the	O
intercept	O
in	O
the	O
model	B
.	O
4	O
also	O
known	O
as	O
the	O
homotopy	O
path	B
.	O
0.00.10.20.30.40.50.6−0.20.00.20.4lasso	O
regressionr2	O
on	O
training	O
datacoefficients0.000.060.201.052.453.47	O
16.2	O
the	O
lasso	B
307	O
the	O
spam	B
data	O
;	O
that	O
is	O
,	O
the	O
solution	O
path	B
for	O
all	O
values	O
of	O
t.	O
this	O
can	O
be	O
com-	O
puted	O
exactly	O
,	O
as	O
we	O
will	O
see	O
in	O
section	O
16.4	O
,	O
because	O
the	O
coefﬁcient	O
pro-	O
ﬁles	O
are	O
piecewise	O
linear	B
in	O
t.	O
it	O
is	O
natural	O
to	O
compare	O
this	O
coefﬁcient	O
proﬁle	O
with	O
the	O
analogous	O
one	O
in	O
figure	O
16.1	O
for	O
forward-stepwise	O
regression	B
.	O
be-	O
cause	O
of	O
the	O
control	B
of	O
k	O
o	O
ˇ.t	O
/k1	O
,	O
we	O
don	O
’	O
t	B
see	O
the	O
same	O
range	O
as	O
in	O
forward	O
stepwise	O
,	O
and	O
observe	O
somewhat	O
smoother	O
behavior	O
.	O
figure	O
16.6	O
contrasts	O
figure	O
16.6	O
lasso	B
versus	O
forward-stepwise	O
regression	B
on	O
the	O
spam	B
data	O
.	O
shown	O
is	O
the	O
misclassiﬁcation	O
error	O
on	O
the	O
test	O
data	B
,	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
variables	O
in	O
the	O
model	B
.	O
linear	B
regression	O
is	O
coded	O
brown	O
,	O
logistic	B
regression	I
blue	O
;	O
hollow	O
dots	O
forward	O
stepwise	O
,	O
solid	O
dots	O
lasso	B
.	O
in	O
this	O
case	O
it	O
appears	O
stepwise	O
and	O
lasso	O
achieve	O
the	O
same	O
performance	O
,	O
but	O
lasso	B
takes	O
longer	O
to	O
get	O
there	O
,	O
because	O
of	O
the	O
shrinkage	B
.	O
the	O
prediction	O
performance	O
on	O
the	O
spam	B
data	O
for	O
lasso	B
regularized	O
models	B
(	O
linear	B
regression	O
and	O
logistic	O
regression	B
)	O
versus	O
forward-stepwise	O
models	B
.	O
the	O
results	O
are	O
rather	O
similar	O
at	O
the	O
end	O
of	O
the	O
path	B
;	O
here	O
forward	O
stepwise	O
can	O
achieve	O
classiﬁcation	O
performance	O
similar	O
to	O
that	O
of	O
lasso	B
regularized	O
logistic	B
regression	I
with	O
about	O
half	O
the	O
terms	O
.	O
lasso	B
logistic	O
regression	B
(	O
and	O
indeed	O
any	O
likelihood-based	O
linear	B
model	I
)	O
is	O
ﬁt	O
by	O
penalized	O
maximum	O
llllllllllllllllllllllllllllllllllllllllllllllllllllllllll010203040500.00.10.20.30.4spam	O
datasteptest	O
misclassification	O
errorllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllforward−stepwise	O
linear	B
regressionforward−stepwise	O
logistic	O
regressionlasso	O
linear	B
regressionlasso	O
logistic	B
regression	I
sparse	O
modeling	O
and	O
the	O
lasso	B
308	O
likelihood	B
:	O
minimize	O
ˇ02r	O
;	O
ˇ2rp	O
1	O
n	O
nx	O
id1	O
l.yi	O
;	O
ˇ0	O
c	O
ˇ	O
0	O
xi	O
/	O
subject	O
to	O
kˇk1	O
	O
t	B
:	O
(	O
16.2	O
)	O
here	O
l	O
is	O
the	O
negative	O
of	O
the	O
log-likelihood	B
function	O
for	O
the	O
response	O
dis-	O
tribution	O
.	O
16.3	O
fitting	O
lasso	B
models	O
the	O
lasso	B
objectives	O
(	O
16.1	O
)	O
or	O
(	O
16.2	O
)	O
are	O
differentiable	O
and	O
convex	O
in	O
ˇ	O
and	O
ˇ0	O
,	O
and	O
the	O
constraint	O
is	O
convex	O
in	O
ˇ.	O
hence	O
solving	O
these	O
problems	O
is	O
a	O
convex	O
optimization	O
problem	O
,	O
for	O
which	O
standard	O
packages	O
are	O
available	O
.	O
it	O
turns	O
out	O
these	O
problems	O
have	O
special	O
structure	O
that	O
can	O
be	O
exploited	O
to	O
yield	O
efﬁcient	O
algorithms	O
for	O
ﬁtting	B
the	O
entire	O
path	B
of	O
solutions	O
as	O
in	O
figures	O
16.1	O
and	O
16.5.	O
we	O
will	O
start	O
with	O
problem	O
(	O
16.1	O
)	O
,	O
which	O
we	O
rewrite	O
in	O
the	O
more	O
convenient	O
lagrange	O
form	B
:	O
minimize	O
ˇ2rp	O
1	O
2n	O
ky	O
(	O
cid:0	O
)	O
x	O
ˇk2	O
c	O
(	O
cid:21	O
)	O
kˇk1	O
:	O
(	O
16.3	O
)	O
here	O
we	O
have	O
centered	O
y	O
and	O
the	O
columns	O
of	O
x	O
beforehand	O
,	O
and	O
hence	O
the	O
intercept	O
has	O
been	O
omitted	O
.	O
the	O
lagrange	O
and	O
constraint	O
versions	O
are	O
equivalent	O
,	O
in	O
the	O
sense	O
that	O
any	O
solution	O
o	O
ˇ	O
.	O
(	O
cid:21	O
)	O
/	O
to	O
(	O
16.3	O
)	O
with	O
(	O
cid:21	O
)	O
(	O
cid:21	O
)	O
0	O
corre-	O
sponds	O
to	O
a	O
solution	O
to	O
(	O
16.1	O
)	O
with	O
t	B
d	O
k	O
o	O
ˇ.	O
(	O
cid:21	O
)	O
/k1	O
.	O
here	O
large	O
values	O
of	O
(	O
cid:21	O
)	O
will	O
encourage	O
solutions	O
with	O
small	O
`1	O
norm	O
coefﬁcient	O
vectors	O
,	O
and	O
vice-versa	O
;	O
(	O
cid:21	O
)	O
d	O
0	O
corresponds	O
to	O
the	O
ordinary	O
least	B
squares	I
ﬁt	O
.	O
0	O
the	O
solution	O
to	O
(	O
16.3	O
)	O
satisﬁes	O
the	O
subgradient	O
condition	B
j	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
p	O
;	O
o	O
ˇi	O
c	O
(	O
cid:21	O
)	O
sj	O
d	O
0	O
;	O
hxj	O
;	O
y	O
(	O
cid:0	O
)	O
x	O
o	O
ˇj	O
/	O
;	O
j	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
p.	O
this	O
notation	O
means	O
sj	O
d	O
sign	O
.	O
(	O
cid:0	O
)	O
1	O
(	O
16.4	O
)	O
n	O
o	O
where	O
sj	O
2	O
sign	O
.	O
ˇj	O
/	O
if	O
o	O
ˇj	O
¤	O
0	O
,	O
and	O
sj	O
2	O
œ	O
(	O
cid:0	O
)	O
1	O
;	O
1	O
if	O
o	O
ˇj	O
d	O
0	O
.	O
)	O
we	O
use	O
the	O
inner-product	O
notation	O
ha	O
;	O
bi	O
d	O
a	O
b	O
in	O
(	O
16.4	O
)	O
,	O
which	O
leads	O
to	O
more	O
evocative	O
expressions	O
.	O
these	O
subgradient	O
conditions	B
are	O
the	O
modern	O
way	O
of	O
characterizing	O
solutions	O
to	O
problems	O
of	O
this	O
kind	O
,	O
and	O
are	O
equivalent	O
to	O
the	O
karush–kuhn–tucker	O
op-	O
timality	O
conditions	B
.	O
from	O
these	O
conditions	B
we	O
can	O
immediately	O
learn	O
some	O
properties	O
of	O
a	O
lasso	B
solution	O
.	O
(	O
cid:15	O
)	O
1	O
o	O
ˇij	O
d	O
(	O
cid:21	O
)	O
for	O
all	O
members	O
of	O
the	O
active	O
set	B
;	O
i.e.	O
,	O
each	O
of	O
the	O
jhxj	O
;	O
y	O
(	O
cid:0	O
)	O
x	O
n	O
variables	O
in	O
the	O
model	B
(	O
with	O
nonzero	O
coefﬁcient	O
)	O
has	O
the	O
same	O
covari-	O
ance	O
with	O
the	O
residuals	O
(	O
in	O
absolute	O
value	O
)	O
.	O
(	O
cid:15	O
)	O
1	O
16.4	O
least-angle	O
regression	B
309	O
o	O
jhxk	O
;	O
y	O
(	O
cid:0	O
)	O
x	O
ˇij	O
	O
(	O
cid:21	O
)	O
for	O
all	O
variables	O
not	O
in	O
the	O
active	O
set	B
(	O
i.e	O
.	O
with	O
n	O
coefﬁcients	O
zero	O
)	O
.	O
these	O
conditions	B
are	O
interesting	O
and	O
have	O
a	O
big	O
impact	O
on	O
computation	O
.	O
suppose	O
we	O
have	O
the	O
solution	O
o	O
ˇ	O
.	O
(	O
cid:21	O
)	O
1/	O
at	O
(	O
cid:21	O
)	O
1	O
,	O
and	O
we	O
decrease	O
(	O
cid:21	O
)	O
by	O
a	O
small	O
amount	O
to	O
(	O
cid:21	O
)	O
2	O
<	O
(	O
cid:21	O
)	O
1.	O
the	O
coefﬁcients	O
and	O
hence	O
the	O
residuals	O
change	O
,	O
in	O
such	O
a	O
way	O
that	O
the	O
covariances	O
all	O
remain	O
tied	O
at	O
the	O
smaller	O
value	O
(	O
cid:21	O
)	O
2.	O
if	O
in	O
the	O
process	O
the	O
active	O
set	B
has	O
not	O
changed	O
,	O
and	O
nor	O
have	O
the	O
signs	O
of	O
their	O
coefﬁcients	O
,	O
then	O
we	O
get	O
an	O
important	O
consequence	O
:	O
o	O
ˇ	O
.	O
(	O
cid:21	O
)	O
/	O
is	O
linear	B
for	O
(	O
cid:21	O
)	O
2	O
œ	O
(	O
cid:21	O
)	O
2	O
;	O
(	O
cid:21	O
)	O
1	O
.	O
to	O
see	O
this	O
,	O
suppose	O
a	O
indexes	O
the	O
active	O
set	B
,	O
which	O
is	O
the	O
same	O
at	O
(	O
cid:21	O
)	O
1	O
and	O
(	O
cid:21	O
)	O
2	O
,	O
and	O
let	O
sa	O
be	O
the	O
constant	O
sign	O
vector	B
.	O
then	O
we	O
have	O
0	O
0	O
a.y	O
(	O
cid:0	O
)	O
x	O
a.y	O
(	O
cid:0	O
)	O
x	O
x	O
x	O
o	O
ˇ	O
.	O
(	O
cid:21	O
)	O
1//	O
d	O
nsa	O
(	O
cid:21	O
)	O
1	O
;	O
o	O
ˇ	O
.	O
(	O
cid:21	O
)	O
2//	O
d	O
nsa	O
(	O
cid:21	O
)	O
2	O
:	O
by	O
subtracting	O
and	O
solving	O
we	O
get	O
(	O
cid:0	O
)	O
1sa	O
;	O
o	O
ˇa	O
.	O
(	O
cid:21	O
)	O
2/	O
(	O
cid:0	O
)	O
o	O
0	O
axa/	O
ˇa	O
.	O
(	O
cid:21	O
)	O
1/	O
d	O
n.	O
(	O
cid:21	O
)	O
1	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
2/.x	O
(	O
16.5	O
)	O
and	O
the	O
remaining	O
coefﬁcients	O
(	O
with	O
indices	O
not	O
in	O
a	O
)	O
are	O
all	O
zero	O
.	O
this	O
shows	O
that	O
the	O
full	B
coefﬁcient	O
vector	B
o	O
ˇ	O
.	O
(	O
cid:21	O
)	O
/	O
is	O
linear	B
for	O
(	O
cid:21	O
)	O
2	O
œ	O
(	O
cid:21	O
)	O
2	O
;	O
(	O
cid:21	O
)	O
1	O
.	O
in	O
fact	O
,	O
the	O
coefﬁcient	O
proﬁles	O
for	O
the	O
lasso	B
are	O
continuous	O
and	O
piecewise	O
linear	B
over	O
the	O
entire	O
range	O
of	O
(	O
cid:21	O
)	O
,	O
with	O
knots	O
occurring	O
whenever	O
the	O
active	O
set	B
changes	O
,	O
or	O
the	O
signs	O
of	O
the	O
coefﬁcients	O
change	O
.	O
another	O
consequence	O
is	O
that	O
we	O
can	O
easily	O
determine	O
(	O
cid:21	O
)	O
max	O
,	O
the	O
smallest	O
value	O
for	O
(	O
cid:21	O
)	O
such	O
that	O
the	O
solution	O
o	O
ˇ	O
.	O
(	O
cid:21	O
)	O
max/	O
d	O
0.	O
from	O
(	O
16.4	O
)	O
this	O
can	O
be	O
jhxj	O
;	O
yij	O
.	O
seen	O
to	O
be	O
(	O
cid:21	O
)	O
max	O
d	O
maxj	O
these	O
two	O
facts	O
plus	O
a	O
few	O
more	O
details	O
enable	O
us	O
to	O
compute	O
the	O
exact	O
solution	O
path	B
for	O
the	O
squared-error-loss	O
lasso	B
;	O
that	O
is	O
the	O
topic	O
of	O
the	O
next	O
section	O
.	O
1	O
n	O
16.4	O
least-angle	O
regression	B
we	O
have	O
just	O
seen	O
that	O
the	O
lasso	B
coefﬁcient	O
proﬁle	O
o	O
ˇ	O
.	O
(	O
cid:21	O
)	O
/	O
is	O
piecewise	O
lin-	O
ear	O
in	O
(	O
cid:21	O
)	O
,	O
and	O
that	O
the	O
elements	O
of	O
the	O
active	O
set	B
are	O
tied	O
in	O
their	O
absolute	O
o	O
covariance	O
with	O
the	O
residuals	O
.	O
with	O
r.	O
(	O
cid:21	O
)	O
/	O
d	O
y	O
(	O
cid:0	O
)	O
x	O
ˇ	O
.	O
(	O
cid:21	O
)	O
/	O
,	O
the	O
covariance	O
be-	O
tween	O
xj	O
and	O
the	O
evolving	O
residual	O
is	O
cj	O
.	O
(	O
cid:21	O
)	O
/	O
d	O
1	O
jhxj	O
;	O
r.	O
(	O
cid:21	O
)	O
/ij	O
.	O
hence	O
these	O
also	O
change	O
in	O
a	O
piecewise	O
linear	B
fashion	O
,	O
with	O
cj	O
.	O
(	O
cid:21	O
)	O
/	O
d	O
(	O
cid:21	O
)	O
for	O
j	O
2	O
a	O
,	O
and	O
cj	O
.	O
(	O
cid:21	O
)	O
/	O
	O
(	O
cid:21	O
)	O
for	O
j	O
62	O
a.	O
this	O
inspires	O
the	O
least-angle	O
regression	B
algorithm	O
,	O
given	O
in	O
algorithm	B
16.3	O
,	O
which	O
exploits	O
this	O
linearity	O
to	O
ﬁt	O
the	O
entire	O
lasso	B
regularization	O
path	B
.	O
n	O
310	O
sparse	O
modeling	O
and	O
the	O
lasso	B
algorithm	O
16.3	O
least-angle	O
regression	B
.	O
0	O
a	O
0	O
a	O
.x	O
xa/	O
d	O
fjg	O
,	O
and	O
xa	O
,	O
1	O
standardize	O
the	O
predictors	O
to	O
have	O
mean	O
zero	O
and	O
unit	O
`2	O
norm	O
.	O
start	O
with	O
the	O
residual	O
r0	O
d	O
y	O
(	O
cid:0	O
)	O
ny	O
,	O
ˇ0	O
d	O
.ˇ1	O
;	O
ˇ2	O
;	O
:	O
:	O
:	O
;	O
ˇp/	O
d	O
0	O
.	O
2	O
find	O
the	O
predictor	B
xj	O
most	O
correlated	O
with	O
r0	O
;	O
i.e.	O
,	O
with	O
largest	O
value	O
for	O
jhxj	O
;	O
r0ij	O
.	O
call	O
this	O
value	O
(	O
cid:21	O
)	O
0	O
,	O
deﬁne	O
the	O
active	O
set	B
a	O
1	O
n	O
the	O
matrix	B
consisting	O
of	O
this	O
single	O
variable	O
.	O
3	O
for	O
k	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
k	O
d	O
min.n	O
(	O
cid:0	O
)	O
1	O
;	O
p/	O
do	O
:	O
(	O
a	O
)	O
deﬁne	O
the	O
least-squares	O
direction	O
ı	O
d	O
1	O
n	O
(	O
cid:21	O
)	O
k	O
(	O
cid:0	O
)	O
1	O
rk	O
(	O
cid:0	O
)	O
1	O
,	O
and	O
d	O
ı	O
,	O
and	O
the	O
remaining	O
elements	O
deﬁne	O
the	O
p-vector	O
	O
such	O
that	O
a	O
are	O
zero	O
.	O
(	O
b	O
)	O
move	O
the	O
coefﬁcients	O
ˇ	O
from	O
ˇk	O
(	O
cid:0	O
)	O
1	O
in	O
the	O
direction	O
	O
toward	O
their	O
least-squares	O
solution	O
on	O
xa	O
:	O
ˇ	O
.	O
(	O
cid:21	O
)	O
/	O
d	O
ˇk	O
(	O
cid:0	O
)	O
1	O
c	O
.	O
(	O
cid:21	O
)	O
k	O
(	O
cid:0	O
)	O
1	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
/	O
for	O
0	O
<	O
(	O
cid:21	O
)	O
	O
(	O
cid:21	O
)	O
k	O
(	O
cid:0	O
)	O
1	O
,	O
keeping	O
track	O
of	O
the	O
evolving	O
residuals	O
r.	O
(	O
cid:21	O
)	O
/	O
d	O
y	O
(	O
cid:0	O
)	O
x	O
ˇ	O
.	O
(	O
cid:21	O
)	O
/	O
d	O
rk	O
(	O
cid:0	O
)	O
1	O
(	O
cid:0	O
)	O
.	O
(	O
cid:21	O
)	O
k	O
(	O
cid:0	O
)	O
1	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
/xaı	O
.	O
jhx`	O
;	O
r.	O
(	O
cid:21	O
)	O
/ij	O
for	O
`	O
…	O
(	O
cid:0	O
)	O
1x	O
(	O
c	O
)	O
keeping	O
track	O
of	O
1	O
n	O
a	O
,	O
identify	O
the	O
largest	O
value	O
of	O
(	O
cid:21	O
)	O
at	O
which	O
a	O
variable	O
“	O
catches	O
up	O
”	O
with	O
the	O
active	O
set	B
;	O
if	O
the	O
variable	O
jhx`	O
;	O
r.	O
(	O
cid:21	O
)	O
/ij	O
d	O
(	O
cid:21	O
)	O
.	O
this	O
deﬁnes	O
the	O
next	O
has	O
index	O
`	O
,	O
that	O
means	O
1	O
n	O
“	O
knot	O
”	O
(	O
cid:21	O
)	O
k.	O
[	O
`	O
,	O
ˇk	O
d	O
ˇ	O
.	O
(	O
cid:21	O
)	O
k/	O
d	O
ˇk	O
(	O
cid:0	O
)	O
1	O
c	O
.	O
(	O
cid:21	O
)	O
k	O
(	O
cid:0	O
)	O
1	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
k/	O
,	O
and	O
rk	O
d	O
d	O
a	O
y	O
(	O
cid:0	O
)	O
x	O
ˇk	O
.	O
(	O
d	O
)	O
set	B
a	O
4	O
return	O
the	O
sequence	O
f	O
(	O
cid:21	O
)	O
k	O
;	O
ˇkgk	O
0	O
.	O
in	O
step	O
3	O
(	O
a	O
)	O
ı	O
d	O
.x	O
0	O
a	O
(	O
cid:0	O
)	O
1sa	O
as	O
in	O
(	O
16.5	O
)	O
.	O
we	O
can	O
think	O
of	O
the	O
lar	O
al-	O
xa/	O
gorithm	O
as	O
a	O
democratic	O
version	O
of	O
forward-stepwise	O
regression	B
.	O
in	O
forward-	O
stepwise	O
regression	B
,	O
we	O
identify	O
the	O
variable	O
that	O
will	O
improve	O
the	O
ﬁt	O
the	O
most	O
,	O
and	O
then	O
move	O
all	O
the	O
coefﬁcients	O
toward	O
the	O
new	O
least-squares	O
ﬁt	O
.	O
as	O
described	O
in	O
endnotes	O
1	O
and	O
3	O
,	O
this	O
is	O
sometimes	O
done	O
by	O
computing	O
the	O
inner	O
products	O
of	O
each	O
(	O
unadjusted	O
)	O
variable	O
with	O
the	O
residual	O
,	O
and	O
pick-	O
ing	O
the	O
largest	O
in	O
absolute	O
value	O
.	O
in	O
step	O
3	O
of	O
algorithm	B
16.3	O
,	O
we	O
move	O
the	O
coefﬁcients	O
for	O
the	O
variables	O
in	O
the	O
active	O
set	B
a	O
toward	O
their	O
least-squares	O
ﬁt	O
(	O
keeping	O
their	O
inner	O
products	O
tied	O
)	O
,	O
but	O
stop	O
when	O
a	O
variable	O
not	O
in	O
a	O
catches	O
up	O
in	O
inner	O
product	O
.	O
at	O
that	O
point	O
,	O
it	O
is	O
invited	O
into	O
the	O
club	O
,	O
and	O
the	O
process	O
continues	O
.	O
step	O
3	O
(	O
c	O
)	O
can	O
be	O
performed	O
efﬁciently	O
because	O
of	O
the	O
linearity	O
of	O
the	O
evolving	O
inner	O
products	O
;	O
for	O
each	O
variable	O
not	O
in	O
a	O
,	O
we	O
can	O
determine	O
ex-	O
actly	O
when	O
(	O
in	O
(	O
cid:21	O
)	O
time	O
)	O
it	O
would	O
catch	O
up	O
,	O
and	O
hence	O
which	O
catches	O
up	O
ﬁrst	O
and	O
when	O
.	O
since	O
the	O
path	B
is	O
piecewise	O
linear	B
,	O
and	O
we	O
know	O
the	O
slopes	O
,	O
this	O
16.4	O
least-angle	O
regression	B
311	O
figure	O
16.7	O
covariance	O
evolution	O
on	O
the	O
spam	B
data	O
.	O
as	O
variables	O
tie	O
for	O
maximal	O
covariance	O
,	O
they	O
become	O
part	O
of	O
the	O
active	O
set	B
.	O
these	O
occasions	O
are	O
indicated	O
by	O
the	O
vertical	O
gray	O
bars	O
,	O
again	O
plotted	O
against	O
the	O
training	O
r2	O
as	O
in	O
figure	O
16.5.	O
means	O
we	O
know	O
the	O
path	B
exactly	O
without	O
further	O
computation	O
between	O
(	O
cid:21	O
)	O
k	O
(	O
cid:0	O
)	O
1	O
and	O
the	O
newly	O
found	O
(	O
cid:21	O
)	O
k.	O
the	O
name	O
“	O
least-angle	O
regression	B
”	O
derives	O
from	O
the	O
fact	O
that	O
in	O
step	O
3	O
(	O
b	O
)	O
the	O
ﬁtted	O
vector	B
evolves	O
in	O
the	O
direction	O
x	O
	O
d	O
xaı	O
,	O
and	O
its	O
inner	O
product	O
xaı	O
d	O
sa	O
.	O
since	O
all	O
the	O
columns	O
with	O
each	O
active	O
vector	B
is	O
given	O
by	O
x	O
of	O
x	O
have	O
unit	O
norm	O
,	O
this	O
means	O
the	O
angles	O
between	O
each	O
active	O
vector	B
and	O
the	O
evolving	O
ﬁtted	O
vector	B
are	O
equal	O
and	O
hence	O
minimal	O
.	O
0	O
a	O
the	O
main	O
computational	O
burden	O
in	O
algorithm	B
16.3	O
is	O
in	O
step	O
3	O
(	O
a	O
)	O
,	O
com-	O
puting	O
the	O
new	O
direction	O
,	O
each	O
time	O
the	O
active	O
set	B
is	O
updated	O
.	O
however	O
,	O
this	O
is	O
easily	O
performed	O
using	O
standard	O
updating	O
of	O
a	O
qr	O
decomposition	O
,	O
and	O
hence	O
the	O
computations	B
for	O
the	O
entire	O
path	B
are	O
of	O
the	O
same	O
order	O
as	O
that	O
of	O
a	O
single	O
least-squares	O
ﬁt	O
using	O
all	O
the	O
variables	O
.	O
the	O
vertical	O
gray	O
lines	O
in	O
figure	O
16.5	O
show	O
when	O
the	O
active	O
set	B
changes	O
.	O
we	O
see	O
the	O
slopes	O
change	O
at	O
each	O
of	O
these	O
transitions	O
.	O
compare	O
with	O
the	O
corresponding	O
figure	O
16.1	O
for	O
forward-stepwise	O
regression	B
.	O
figure	O
16.7	O
shows	O
the	O
the	O
decreasing	O
covariance	O
during	O
the	O
steps	O
of	O
the	O
0.00.10.20.30.40.50.60.00.10.20.30.4r2	O
on	O
training	O
datacovariance	O
with	O
residuals	O
312	O
sparse	O
modeling	O
and	O
the	O
lasso	B
lar	O
algorithm	B
.	O
as	O
each	O
variable	O
joins	O
the	O
active	O
set	B
,	O
the	O
covariances	O
be-	O
come	O
tied	O
.	O
at	O
the	O
end	O
of	O
the	O
path	B
,	O
the	O
covariances	O
are	O
all	O
zero	O
,	O
because	O
this	O
is	O
the	O
unregularized	O
ordinary	O
least-squares	O
solution	O
.	O
it	O
turns	O
out	O
that	O
the	O
lar	O
algorithm	B
is	O
not	O
quite	O
the	O
lasso	B
path	O
;	O
variables	O
can	O
drop	O
out	O
of	O
the	O
active	O
set	B
as	O
the	O
path	B
evolves	O
.	O
this	O
happens	O
when	O
a	O
coef-	O
ﬁcient	O
curve	O
passes	O
through	O
zero	O
.	O
the	O
subgradient	O
equations	O
(	O
16.4	O
)	O
imply	O
that	O
the	O
sign	O
of	O
each	O
active	O
coefﬁcient	O
matches	O
the	O
sign	O
of	O
the	O
gradient	O
.	O
however	O
,	O
a	O
simple	O
addition	O
to	O
step	O
3	O
(	O
c	O
)	O
in	O
algorithm	B
16.3	O
takes	O
care	O
of	O
the	O
issue	O
:	O
3	O
(	O
c	O
)	O
+	O
lasso	B
modiﬁcation	O
:	O
if	O
a	O
nonzero	O
coefﬁcient	O
crosses	O
zero	O
before	O
the	O
next	O
variable	O
enters	O
,	O
drop	O
it	O
from	O
a	O
and	O
recompute	O
the	O
joint	O
least-squares	O
direction	O
	O
using	O
the	O
reduced	O
set	B
.	O
figure	O
16.5	O
was	O
computed	O
using	O
the	O
lars	B
package	O
in	O
r	O
,	O
with	O
the	O
lasso	B
option	O
set	B
to	O
accommodate	O
step	O
3	O
(	O
c	O
)	O
+	O
;	O
in	O
this	O
instance	O
there	O
was	O
no	O
need	O
for	O
dropping	O
.	O
dropping	O
tends	O
to	O
occur	O
when	O
some	O
of	O
the	O
variables	O
are	O
highly	O
correlated	O
.	O
lasso	B
and	O
degrees	O
of	O
freedom	O
we	O
see	O
in	O
figure	O
16.6	O
(	O
left	O
panel	O
)	O
that	O
forward-stepwise	O
regression	B
is	O
more	O
aggressive	O
than	O
the	O
lasso	B
,	O
in	O
that	O
it	O
brings	O
down	O
the	O
training	O
mse	O
faster	O
.	O
we	O
can	O
use	O
the	O
covariance	O
formula	B
for	O
df	O
from	O
chapter	O
12	O
to	O
quantify	O
the	O
amount	O
of	O
ﬁtting	B
at	O
each	O
step	O
.	O
in	O
the	O
right	O
panel	O
we	O
show	O
the	O
results	O
of	O
a	O
simulation	O
for	O
estimating	O
the	O
df	O
of	O
forward-stepwise	O
regression	B
and	O
the	O
lasso	B
for	O
the	O
spam	B
data	O
.	O
recall	O
the	O
covariance	O
formula	B
cov.yi	O
;	O
oyi	O
/	O
:	O
(	O
16.6	O
)	O
nx	O
id1	O
df	O
d	O
1	O
(	O
cid:27	O
)	O
2	O
these	O
covariances	O
are	O
of	O
course	O
with	O
respect	O
to	O
the	O
sampling	O
distribution	O
of	O
the	O
yi	O
,	O
which	O
we	O
do	O
not	O
have	O
access	O
to	O
since	O
these	O
are	O
real	O
data	B
.	O
so	O
instead	O
we	O
simulate	O
from	O
ﬁtted	O
values	O
from	O
the	O
full	B
least-squares	O
ﬁt	O
,	O
by	O
adding	O
gaussian	O
errors	B
with	O
the	O
appropriate	O
(	O
estimated	O
)	O
standard	B
deviation	I
.	O
(	O
this	O
is	O
the	O
parametric	B
bootstrap	O
calculation	O
(	O
12.64	O
)	O
.	O
)	O
it	O
turns	O
out	O
that	O
each	O
step	O
of	O
the	O
lar	O
algorithm	B
spends	O
one	O
df	O
,	O
as	O
is	O
evidenced	O
by	O
the	O
brown	O
curve	O
in	O
the	O
right	O
plot	O
of	O
figure	O
16.8.	O
forward	O
stepwise	O
spends	O
more	O
df	O
in	O
the	O
earlier	O
stages	O
,	O
and	O
can	O
be	O
erratic	O
.	O
under	O
some	O
technical	O
conditions	B
on	O
the	O
x	O
matrix	B
(	O
that	O
guarantee	O
that	O
16.5	O
fitting	O
generalized	O
lasso	B
models	O
313	O
figure	O
16.8	O
left	O
:	O
training	O
mean-squared	O
error	O
(	O
mse	O
)	O
on	O
the	O
spam	B
data	O
,	O
for	O
forward-stepwise	O
regression	B
and	O
the	O
lasso	B
,	O
as	O
a	O
function	B
of	O
the	O
size	O
of	O
the	O
active	O
set	B
.	O
forward	O
stepwise	O
is	O
more	O
aggressive	O
than	O
the	O
lasso	B
,	O
in	O
that	O
it	O
(	O
over-	O
)	O
ﬁts	O
the	O
training	O
data	B
more	O
quickly	O
.	O
right	O
:	O
simulation	O
showing	O
the	O
degrees	O
of	O
freedom	O
or	O
df	O
of	O
forward-stepwise	O
regression	B
versus	O
lasso	B
.	O
the	O
lasso	B
uses	O
one	O
df	O
per	O
step	O
,	O
while	O
forward	O
stepwise	O
is	O
greedier	O
and	O
uses	O
more	O
,	O
especially	O
in	O
the	O
early	O
steps	O
.	O
since	O
these	O
df	O
were	O
computed	O
using	O
5000	O
random	O
simulated	O
data	B
sets	O
,	O
we	O
include	O
standard-error	O
bands	O
on	O
the	O
estimates	O
.	O
lar	O
delivers	O
the	O
lasso	B
path	O
)	O
,	O
one	O
can	O
show	O
that	O
the	O
df	O
is	O
exactly	O
one	O
per	O
step	O
.	O
more	O
generally	O
,	O
for	O
the	O
lasso	B
,	O
if	O
we	O
deﬁnecdf	O
.	O
(	O
cid:21	O
)	O
/	O
d	O
j	O
of	O
the	O
active	O
set	B
at	O
(	O
cid:21	O
)	O
)	O
,	O
we	O
have	O
that	O
eœcdf	O
.	O
(	O
cid:21	O
)	O
/	O
d	O
df	O
.	O
(	O
cid:21	O
)	O
/	O
.	O
in	O
other	O
words	O
,	O
the	O
a	O
.	O
(	O
cid:21	O
)	O
/j	O
(	O
the	O
size	O
size	O
of	O
the	O
active	O
set	B
is	O
an	O
unbiased	O
estimate	O
of	O
df	O
.	O
ordinary	O
least	B
squares	I
with	O
a	O
predetermined	O
sequence	O
of	O
variables	O
spends	O
one	O
df	O
per	O
variable	O
.	O
intuitively	O
forward	O
stepwise	O
spends	O
more	O
,	O
because	O
it	O
pays	O
a	O
price	O
(	O
in	O
some	O
extra	O
df	O
)	O
for	O
searching	O
.	O
	O
although	O
the	O
lasso	B
does	O
5	O
search	O
for	O
the	O
next	O
variable	O
,	O
it	O
does	O
not	O
ﬁt	O
the	O
new	O
model	B
all	O
the	O
way	O
,	O
but	O
just	O
until	O
the	O
next	O
variable	O
enters	O
.	O
at	O
this	O
point	O
,	O
one	O
new	O
df	O
has	O
been	O
spent	O
.	O
16.5	O
fitting	O
generalized	O
lasso	B
models	O
so	O
far	O
we	O
have	O
focused	O
on	O
the	O
lasso	B
for	O
squared-error	O
loss	O
,	O
and	O
exploited	O
the	O
piecewise-linearity	O
of	O
its	O
coefﬁcient	O
proﬁle	O
to	O
efﬁciently	O
compute	O
the	O
entire	O
path	B
.	O
unfortunately	O
this	O
is	O
not	O
the	O
case	O
for	O
most	O
other	O
loss	O
functions	O
,	O
llllllllllllllllllllllllllllllllllllllllllllllllllllllllll010203040500.40.50.60.70.80.9spam	O
training	O
datasteptraining	O
msellllllllllllllllllllllllllllllllllllllllllllllllllllllllllllforward−stepwiselasso010203040500102030405060stepdfllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllforward	O
stepwiselasso	O
314	O
sparse	O
modeling	O
and	O
the	O
lasso	B
so	O
obtaining	O
the	O
coefﬁcient	O
path	B
is	O
potentially	O
more	O
costly	O
.	O
as	O
a	O
case	O
in	O
point	O
,	O
we	O
will	O
use	O
logistic	B
regression	I
as	O
an	O
example	O
;	O
in	O
this	O
case	O
in	O
(	O
16.2	O
)	O
l	O
represents	O
the	O
negative	O
binomial	B
log-likelihood	O
.	O
writing	O
the	O
loss	O
explicitly	O
and	O
using	O
the	O
lagrange	O
form	B
for	O
the	O
penalty	B
,	O
we	O
wish	O
to	O
solve	O
nx	O
#	O
yi	O
log	O
(	O
cid:22	O
)	O
i	O
c	O
.1	O
(	O
cid:0	O
)	O
yi	O
/	O
log.1	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
i	O
/	O
minimize	O
ˇ02r	O
;	O
ˇ2rp	O
here	O
we	O
assume	O
the	O
yi	O
2	O
f0	O
;	O
1g	O
and	O
(	O
cid:22	O
)	O
i	O
are	O
the	O
ﬁtted	O
probabilities	B
id1	O
n	O
c	O
(	O
cid:21	O
)	O
kˇk1	O
:	O
(	O
16.7	O
)	O
''	O
(	O
cid:0	O
)	O
1	O
(	O
cid:22	O
)	O
i	O
d	O
eˇ0cx	O
1	O
c	O
eˇ0cx	O
0	O
i	O
ˇ	O
:	O
0	O
i	O
ˇ	O
(	O
16.8	O
)	O
similar	O
to	O
(	O
16.4	O
)	O
,	O
the	O
solution	O
satisﬁes	O
the	O
subgradient	O
condition	B
1	O
hxj	O
;	O
y	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
i	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
sj	O
d	O
0	O
;	O
j	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
p	O
;	O
n	O
(	O
16.9	O
)	O
where	O
sj	O
2	O
sign.ˇj	O
/	O
;	O
j	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
p	O
,	O
and	O
(	O
cid:22	O
)	O
0	O
d	O
.	O
(	O
cid:22	O
)	O
1	O
;	O
:	O
:	O
:	O
;	O
(	O
cid:22	O
)	O
n/.5	O
how-	O
ever	O
,	O
the	O
nonlinearity	O
of	O
(	O
cid:22	O
)	O
i	O
in	O
ˇj	O
results	O
in	O
piecewise	O
nonlinear	B
coefﬁcient	O
proﬁles	O
.	O
instead	O
we	O
settle	O
for	O
a	O
solution	O
path	B
on	O
a	O
sufﬁciently	O
ﬁne	O
grid	O
of	O
values	O
for	O
(	O
cid:21	O
)	O
.	O
it	O
is	O
once	O
again	O
easy	O
to	O
see	O
that	O
the	O
largest	O
value	O
of	O
(	O
cid:21	O
)	O
we	O
need	O
consider	O
is	O
(	O
cid:21	O
)	O
max	O
d	O
max	O
jhxj	O
;	O
y	O
(	O
cid:0	O
)	O
ny1ij	O
;	O
j	O
(	O
16.10	O
)	O
since	O
this	O
is	O
the	O
smallest	O
value	O
of	O
(	O
cid:21	O
)	O
for	O
which	O
o	O
ˇ0	O
d	O
logit	O
.	O
ny/	O
.	O
a	O
reasonable	O
sequence	O
is	O
100	O
values	O
(	O
cid:21	O
)	O
1	O
>	O
(	O
cid:21	O
)	O
2	O
>	O
:	O
:	O
:	O
>	O
(	O
cid:21	O
)	O
100	O
equally	O
spaced	O
on	O
the	O
log-scale	O
from	O
(	O
cid:21	O
)	O
max	O
down	O
to	O
(	O
cid:15	O
)	O
(	O
cid:21	O
)	O
max	O
,	O
where	O
(	O
cid:15	O
)	O
is	O
some	O
small	O
fraction	O
such	O
as	O
0:001	O
.	O
ˇ	O
d	O
0	O
,	O
and	O
o	O
an	O
approach	O
that	O
has	O
proven	O
to	O
be	O
surprisingly	O
efﬁcient	O
is	O
path-wise	O
coordinate	O
descent	O
.	O
(	O
cid:15	O
)	O
for	O
each	O
value	O
(	O
cid:21	O
)	O
k	O
,	O
solve	O
the	O
lasso	B
problem	O
for	O
one	O
ˇj	O
only	O
,	O
holding	O
all	O
the	O
others	O
ﬁxed	O
.	O
cycle	O
around	O
until	O
the	O
estimates	O
stabilize	O
.	O
(	O
cid:15	O
)	O
by	O
starting	O
at	O
(	O
cid:21	O
)	O
1	O
,	O
where	O
all	O
the	O
parameters	O
are	O
zero	O
,	O
we	O
use	O
warm	O
starts	O
in	O
computing	O
the	O
solutions	O
at	O
the	O
decreasing	O
sequence	O
of	O
(	O
cid:21	O
)	O
values	O
.	O
the	O
warm	O
starts	O
provide	O
excellent	O
initializations	O
for	O
the	O
sequence	O
of	O
solutions	O
o	O
ˇ.	O
(	O
cid:21	O
)	O
k/	O
.	O
(	O
cid:15	O
)	O
the	O
active	O
set	B
grows	O
slowly	O
as	O
(	O
cid:21	O
)	O
decreases	O
.	O
computational	O
hedges	O
that	O
guess	O
the	O
active	O
set	B
prove	O
to	O
be	O
particularly	O
efﬁcient	O
.	O
if	O
the	O
guess	O
is	O
good	O
(	O
and	O
correct	O
)	O
,	O
one	O
iterates	O
coordinate	O
descent	O
using	O
only	O
those	O
variables	O
,	O
pn	O
id1	O
yi	O
d	O
1	O
n	O
pn	O
id1	O
(	O
cid:22	O
)	O
i	O
.	O
5	O
the	O
equation	B
for	O
the	O
intercept	O
is	O
1	O
n	O
16.5	O
fitting	O
generalized	O
lasso	B
models	O
315	O
until	O
convergence	O
.	O
one	O
more	O
sweep	O
through	O
all	O
the	O
variables	O
conﬁrms	O
the	O
hunch	O
.	O
the	O
r	O
package	O
glmnet	B
employs	O
a	O
proximal-newton	O
strategy	O
at	O
each	O
value	O
(	O
cid:21	O
)	O
k.	O
1	O
compute	O
a	O
weighted	O
least	B
squares	I
(	O
quadratic	O
)	O
approximation	O
to	O
the	O
log-	O
likelihood	B
l	O
at	O
the	O
current	O
estimate	B
for	O
the	O
solution	O
vector	B
o	O
ˇ	O
.	O
(	O
cid:21	O
)	O
k/	O
;	O
this	O
produces	O
a	O
working	O
response	O
and	O
observation	O
weights	O
,	O
as	O
in	O
a	O
regular	O
glm	O
.	O
2	O
solve	O
the	O
weighted	O
least-squares	O
lasso	B
at	O
(	O
cid:21	O
)	O
k	O
by	O
coordinate	O
descent	O
,	O
using	O
warm	O
starts	O
and	O
active-set	O
iterations	O
.	O
we	O
now	O
give	O
some	O
details	O
,	O
which	O
illustrate	O
why	O
these	O
particular	O
strate-	O
gies	O
are	O
effective	O
.	O
consider	O
the	O
weighted	O
least-squares	O
problem	O
1	O
wi	O
.zi	O
(	O
cid:0	O
)	O
ˇ0	O
(	O
cid:0	O
)	O
x	O
i	O
ˇ/2	O
c	O
(	O
cid:21	O
)	O
kˇk1	O
;	O
0	O
ˇj	O
minimize	O
(	O
16.11	O
)	O
p	O
with	O
all	O
but	O
ˇj	O
ﬁxed	O
at	O
their	O
current	O
values	O
.	O
writing	O
ri	O
d	O
zi	O
(	O
cid:0	O
)	O
ˇ0	O
(	O
cid:0	O
)	O
`¤j	O
xi	O
`ˇ`	O
,	O
we	O
can	O
recast	O
(	O
16.11	O
)	O
as	O
id1	O
2n	O
wi	O
.ri	O
(	O
cid:0	O
)	O
xij	O
ˇj	O
/2	O
c	O
(	O
cid:21	O
)	O
jˇjj	O
;	O
(	O
16.12	O
)	O
nx	O
nx	O
1	O
2n	O
id1	O
minimize	O
ˇj	O
nx	O
id1	O
1	O
n	O
a	O
one-dimensional	O
problem	O
.	O
the	O
subgradient	O
equation	B
is	O
wi	O
xij	O
.ri	O
(	O
cid:0	O
)	O
xij	O
ˇj	O
/	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
(	O
cid:1	O
)	O
sign.ˇj	O
/	O
d	O
0	O
:	O
(	O
16.13	O
)	O
the	O
simplest	O
form	B
of	O
the	O
solution	O
occurs	O
if	O
each	O
variable	O
is	O
standardized	O
to	O
have	O
weighted	O
mean	O
zero	O
and	O
variance	O
one	O
,	O
and	O
the	O
weights	O
sum	O
to	O
one	O
;	O
in	O
that	O
case	O
we	O
have	O
a	O
two-step	O
solution	O
.	O
1	O
compute	O
the	O
weighted	O
simple	O
least-squares	O
coefﬁcient	O
q	O
ˇj	O
d	O
hxj	O
;	O
riw	O
d	O
nx	O
(	O
ˇj	O
to	O
produce	O
o	O
2	O
soft-threshold	O
q	O
ˇj	O
:	O
o	O
ˇj	O
d	O
id1	O
0	O
sign	O
.	O
ˇj	O
/.j	O
q	O
q	O
if	O
j	O
q	O
ˇj	O
<	O
(	O
cid:21	O
)	O
i	O
ˇjj	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
/	O
otherwise	O
:	O
(	O
16.15	O
)	O
wi	O
xij	O
ri	O
:	O
(	O
16.14	O
)	O
316	O
sparse	O
modeling	O
and	O
the	O
lasso	B
without	O
the	O
standardization	O
,	O
the	O
solution	O
is	O
almost	O
as	O
simple	O
but	O
less	O
intuitive	O
.	O
hence	O
each	O
coordinate-descent	O
update	O
essentially	O
requires	O
an	O
inner	O
prod-	O
uct	O
,	O
followed	O
by	O
the	O
soft	O
thresholding	O
operation	O
.	O
this	O
is	O
especially	O
conve-	O
nient	O
for	O
xij	O
that	O
are	O
stored	O
in	O
sparse-matrix	O
format	O
,	O
since	O
then	O
the	O
inner	O
products	O
need	O
only	O
visit	O
the	O
nonzero	O
values	O
.	O
if	O
the	O
coefﬁcient	O
is	O
zero	O
be-	O
fore	O
the	O
step	O
,	O
and	O
remains	O
zero	O
,	O
one	O
just	O
moves	O
on	O
,	O
otherwise	O
the	O
model	B
is	O
updated	O
.	O
moving	O
from	O
the	O
solution	O
at	O
(	O
cid:21	O
)	O
k	O
(	O
for	O
which	O
jhxj	O
;	O
riwj	O
d	O
(	O
cid:21	O
)	O
k	O
for	O
all	O
the	O
nonzero	O
coefﬁcients	O
o	O
ˇj	O
)	O
,	O
down	O
to	O
the	O
smaller	O
(	O
cid:21	O
)	O
kc1	O
,	O
one	O
might	O
expect	O
all	O
variables	O
for	O
which	O
jhxj	O
;	O
riwj	O
(	O
cid:21	O
)	O
(	O
cid:21	O
)	O
kc1	O
would	O
be	O
natural	O
candidates	O
for	O
the	O
new	O
active	O
set	B
.	O
the	O
strong	O
rules	O
lower	O
the	O
bar	O
somewhat	O
,	O
and	O
include	O
any	O
variables	O
for	O
which	O
jhxj	O
;	O
riwj	O
(	O
cid:21	O
)	O
(	O
cid:21	O
)	O
kc1	O
(	O
cid:0	O
)	O
.	O
(	O
cid:21	O
)	O
k	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
kc1/	O
;	O
this	O
tends	O
to	O
rarely	O
make	O
mistakes	O
,	O
and	O
still	O
leads	O
to	O
considerable	O
computational	O
savings	O
.	O
apart	O
from	O
variations	O
in	O
the	O
loss	B
function	I
,	O
other	O
penalties	O
are	O
of	O
interest	O
as	O
well	O
.	O
in	O
particular	O
,	O
the	O
elastic	O
net	O
penalty	B
bridges	O
the	O
gap	O
between	O
the	O
lasso	B
and	O
ridge	B
regression	I
.	O
that	O
penalty	B
is	O
deﬁned	O
as	O
c	O
˛kˇk1	O
;	O
.1	O
(	O
cid:0	O
)	O
˛/kˇk2	O
(	O
16.16	O
)	O
p˛.ˇ/	O
d	O
1	O
2	O
2	O
where	O
the	O
factor	B
1=2	O
in	O
the	O
ﬁrst	O
term	O
is	O
for	O
mathematical	O
convenience	O
.	O
when	O
the	O
predictors	O
are	O
excessively	O
correlated	O
,	O
the	O
lasso	B
performs	O
some-	O
what	O
poorly	O
,	O
since	O
it	O
has	O
difﬁculty	O
in	O
choosing	O
among	O
the	O
correlated	O
cousins	O
.	O
like	O
ridge	B
regression	I
,	O
the	O
elastic	O
net	O
shrinks	O
the	O
coefﬁcients	O
of	O
correlated	O
variables	O
toward	O
each	O
other	O
,	O
and	O
tends	O
to	O
select	O
correlated	O
variables	O
in	O
groups	O
.	O
in	O
this	O
case	O
the	O
co-ordinate	O
descent	O
update	O
is	O
almost	O
as	O
simple	O
as	O
in	O
(	O
16.15	O
)	O
(	O
o	O
ˇj	O
d	O
0	O
sign	O
.	O
qˇj	O
/.j	O
qˇjj	O
(	O
cid:0	O
)	O
˛	O
(	O
cid:21	O
)	O
/	O
1c.1	O
(	O
cid:0	O
)	O
˛/	O
(	O
cid:21	O
)	O
if	O
j	O
q	O
ˇj	O
<	O
˛	O
(	O
cid:21	O
)	O
i	O
otherwise	O
;	O
(	O
16.17	O
)	O
again	O
assuming	O
the	O
observations	O
have	O
weighted	O
variance	O
equal	O
to	O
one	O
.	O
when	O
˛	O
d	O
0	O
,	O
the	O
update	O
corresponds	O
to	O
a	O
coordinate	O
update	O
for	O
ridge	B
regression	I
.	O
figure	O
16.9	O
compares	O
lasso	B
with	O
forward-stepwise	O
logistic	B
regression	I
on	O
the	O
spam	B
data	O
,	O
here	O
using	O
all	O
binarized	O
variables	O
and	O
their	O
pairwise	O
interac-	O
tions	O
.	O
this	O
amounts	O
to	O
3061	O
variables	O
in	O
all	O
,	O
once	O
degenerate	O
variables	O
have	O
been	O
excised	O
.	O
forward	O
stepwise	O
takes	O
a	O
long	O
time	O
to	O
run	O
,	O
since	O
it	O
enters	O
one	O
variable	O
at	O
a	O
time	O
,	O
and	O
after	O
each	O
one	O
has	O
been	O
selected	O
,	O
a	O
new	O
glm	O
must	O
be	O
ﬁt	O
.	O
the	O
lasso	B
path	O
,	O
as	O
ﬁt	O
by	O
glmnet	B
,	O
includes	O
many	O
new	O
variables	O
at	O
each	O
step	O
(	O
(	O
cid:21	O
)	O
k	O
)	O
,	O
and	O
is	O
extremely	O
fast	O
(	O
6	O
s	O
for	O
the	O
entire	O
path	B
)	O
.	O
for	O
very	O
large	O
16.6	O
post-selection	O
inference	B
for	O
the	O
lasso	B
317	O
figure	O
16.9	O
test	O
misclassiﬁcation	O
error	O
for	O
lasso	B
versus	O
forward-stepwise	O
logistic	B
regression	I
on	O
the	O
spam	B
data	O
,	O
where	O
we	O
consider	O
pairwise	O
interactions	O
as	O
well	O
as	O
main	O
effects	O
(	O
3061	O
predictors	O
in	O
all	O
)	O
.	O
here	O
the	O
minimum	O
error	O
for	O
lasso	B
is	O
0:057	O
versus	O
0:064	O
for	O
stepwise	O
logistic	B
regression	I
,	O
and	O
0:071	O
for	O
the	O
main-effects-only	O
lasso	B
logistic	O
regression	B
model	I
.	O
the	O
stepwise	O
models	B
went	O
up	O
to	O
134	O
variables	O
before	O
encountering	O
convergence	O
issues	O
,	O
while	O
the	O
lasso	B
had	O
a	O
largest	O
active	O
set	B
of	O
size	O
682.	O
and	O
wide	O
modern	O
data	B
sets	O
(	O
millions	O
of	O
examples	O
and	O
millions	O
of	O
variables	O
)	O
,	O
the	O
lasso	B
path	O
algorithm	B
is	O
feasible	O
and	O
attractive	O
.	O
16.6	O
post-selection	O
inference	B
for	O
the	O
lasso	B
this	O
chapter	O
is	O
mostly	O
about	O
building	O
interpretable	O
models	B
for	O
prediction	O
,	O
with	O
little	O
attention	O
paid	O
to	O
inference	B
;	O
indeed	O
,	O
inference	B
is	O
generally	O
difﬁcult	O
for	O
adaptively	O
selected	O
models	B
.	O
(	O
cid:21	O
)	O
,	O
which	O
ends	O
up	O
selecting	O
a	O
subset	O
a	O
of	O
size	O
j	O
a	O
suppose	O
we	O
have	O
ﬁt	O
a	O
lasso	B
regression	O
model	B
with	O
a	O
particular	O
value	O
for	O
j	O
d	O
k	O
of	O
the	O
p	O
avail-	O
able	O
variables	O
.	O
the	O
question	O
arises	O
as	O
to	O
whether	O
we	O
can	O
assign	O
p-values	O
to	O
these	O
selected	O
variables	O
,	O
and	O
produce	O
conﬁdence	B
intervals	I
for	O
their	O
co-	O
efﬁcients	O
.	O
a	O
recent	O
burst	O
of	O
research	O
activity	O
has	O
made	O
progress	O
on	O
these	O
important	O
problems	O
.	O
we	O
give	O
a	O
very	O
brief	O
survey	O
here	O
,	O
with	O
references	O
ap-	O
llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.00.00.10.20.30.4spam	O
data	B
with	O
interactionspercentage	O
null	O
deviance	O
explained	O
on	O
training	O
datatest	O
misclassification	O
errorllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllforward−stepwise	O
logistic	O
regressionlasso	O
logistic	B
regression	I
318	O
sparse	O
modeling	O
and	O
the	O
lasso	B
6	O
pearing	O
in	O
the	O
notes	O
.	O
	O
we	O
discuss	O
post-selection	O
inference	B
more	O
generally	O
in	O
chapter	O
20.	O
one	O
question	O
that	O
arises	O
is	O
whether	O
we	O
are	O
interested	O
in	O
making	O
infer-	O
ences	O
about	O
the	O
population	O
regression	B
parameters	O
using	O
the	O
full	B
set	O
of	O
p	O
predictors	O
,	O
or	O
whether	O
interest	O
is	O
restricted	O
to	O
the	O
population	O
regression	B
pa-	O
rameters	O
using	O
only	O
the	O
subset	O
a.	O
for	O
the	O
ﬁrst	O
case	O
,	O
it	O
has	O
been	O
proposed	O
that	O
one	O
can	O
view	O
the	O
coefﬁcients	O
of	O
the	O
selected	O
model	B
as	O
an	O
efﬁcient	O
but	O
biased	O
estimate	O
of	O
the	O
full	B
popu-	O
lation	O
coefﬁcient	O
vector	B
.	O
the	O
idea	O
is	O
to	O
then	O
debias	O
this	O
estimate	B
,	O
allowing	O
inference	B
for	O
the	O
full	B
vector	O
of	O
coefﬁcients	O
.	O
of	O
course	O
,	O
sharper	O
inference	B
will	O
be	O
available	O
for	O
the	O
stronger	O
variables	O
that	O
were	O
selected	O
in	O
the	O
ﬁrst	O
place	O
.	O
figure	O
16.10	O
hiv	O
data	B
.	O
linear	B
regression	O
of	O
drug	O
resistance	O
in	O
hiv-positive	O
patients	O
on	O
seven	O
sites	O
,	O
indicators	O
of	O
mutations	O
at	O
particular	O
genomic	O
locations	O
.	O
these	O
seven	O
sites	O
were	O
selected	O
from	O
a	O
total	O
of	O
30	O
candidates	O
,	O
using	O
the	O
lasso	B
.	O
the	O
naive	O
95	O
%	O
conﬁdence	B
intervals	I
(	O
dark	O
)	O
use	O
standard	O
linear-regression	O
inference	B
,	O
ignoring	O
the	O
selection	O
event	O
.	O
the	O
light	O
intervals	B
are	O
95	O
%	O
conﬁdence	B
intervals	I
,	O
using	O
linear	B
regression	O
,	O
but	O
conditioned	O
on	O
the	O
selection	O
event	O
.	O
for	O
the	O
second	O
case	O
,	O
the	O
idea	O
is	O
to	O
condition	B
on	O
the	O
selection	O
event	O
(	O
s	O
)	O
and	O
hence	O
the	O
set	B
a	O
itself	O
,	O
and	O
then	O
perform	O
conditional	O
inference	B
on	O
the	O
−2−1012predictorcoefficients5s8s9s16s25s26s28naive	O
intervalselection−adjusted	O
interval	B
16.7	O
connections	O
and	O
extensions	O
319	O
unrestricted	O
(	O
i.e	O
.	O
not	O
lasso-shrunk	O
)	O
regression	B
coefﬁcients	O
of	O
the	O
response	O
on	O
only	O
the	O
variables	O
in	O
a.	O
for	O
the	O
case	O
of	O
a	O
lasso	B
with	O
squared-error	O
loss	O
,	O
it	O
turns	O
out	O
that	O
the	O
set	B
of	O
response	O
vectors	O
y	O
2	O
rn	O
that	O
would	O
lead	O
to	O
a	O
particular	O
subset	O
a	O
of	O
variables	O
in	O
the	O
active	O
set	B
form	O
a	O
convex	O
polytope	O
in	O
rn	O
(	O
if	O
we	O
condition	B
on	O
the	O
signs	O
of	O
the	O
coefﬁcients	O
as	O
well	O
;	O
ignoring	O
the	O
signs	O
leads	O
to	O
a	O
ﬁnite	O
union	O
of	O
such	O
polytopes	O
)	O
.	O
this	O
,	O
along	O
with	O
del-	O
icate	O
gaussian	O
conditioning	O
arguments	O
,	O
leads	O
to	O
truncated	O
gaussian	O
and	O
t-distrubtions	O
for	O
parameters	O
of	O
interest	O
.	O
figure	O
16.10	O
shows	O
the	O
results	O
of	O
using	O
the	O
lasso	B
to	O
select	O
variables	O
in	O
an	O
hiv	O
study	O
.	O
the	O
outcome	O
y	O
is	O
a	O
measure	O
of	O
the	O
resistence	O
to	O
an	O
hiv-1	O
treatment	O
(	O
nucleoside	O
reverse	O
transcriptase	O
inhibitor	O
)	O
,	O
and	O
the	O
30	O
predictors	O
are	O
indicators	O
of	O
whether	O
mutations	O
had	O
occurred	O
at	O
particular	O
genomic	O
sites	O
.	O
lasso	B
regression	O
with	O
10-fold	B
cross-validation	O
selected	O
a	O
value	O
of	O
(	O
cid:21	O
)	O
d	O
0:003	O
and	O
the	O
seven	O
sites	O
indicated	O
in	O
the	O
ﬁgure	O
had	O
nonzero	O
coefﬁ-	O
cients	O
.	O
the	O
dark	O
bars	O
in	O
the	O
ﬁgure	O
indicate	O
standard	O
95	O
%	O
conﬁdence	O
inter-	O
vals	O
for	O
the	O
coefﬁcients	O
of	O
the	O
selected	O
variables	O
,	O
using	O
linear	B
regression	O
,	O
and	O
ignoring	O
the	O
fact	O
that	O
the	O
lasso	B
was	O
used	O
to	O
select	O
the	O
variables	O
.	O
three	O
variables	O
are	O
signiﬁcant	O
,	O
and	O
two	O
more	O
nearly	O
so	O
.	O
the	O
lighter	O
bars	O
are	O
con-	O
ﬁdence	O
intervals	B
in	O
a	O
similar	O
regression	B
,	O
but	O
conditioned	O
on	O
the	O
selection	O
event	O
.	O
we	O
see	O
that	O
they	O
are	O
generally	O
wider	O
,	O
and	O
only	O
variable	O
s25	O
remains	O
7	O
signiﬁcant	O
.	O
16.7	O
connections	O
and	O
extensions	O
there	O
are	O
interesting	O
connections	O
between	O
lasso	B
models	O
and	O
other	O
popular	O
approaches	O
to	O
the	O
prediction	O
problem	O
.	O
we	O
will	O
brieﬂy	O
cover	O
two	O
of	O
these	O
here	O
,	O
namely	O
support-vector	O
machines	O
and	O
boosting	O
.	O
lasso	B
logistic	O
regression	B
and	O
the	O
svm	O
we	O
show	O
in	O
section	O
19.3	O
that	O
ridged	O
logistic	B
regression	I
has	O
a	O
lot	O
in	O
com-	O
mon	O
with	O
the	O
linear	B
support-vector	O
machine	O
.	O
for	O
separable	O
data	B
the	O
limit	O
as	O
(	O
cid:21	O
)	O
#	O
0	O
in	O
ridged	O
logistic	B
regression	I
coincides	O
with	O
the	O
svm	O
.	O
in	O
addition	O
their	O
loss	O
functions	O
are	O
somewhat	O
similar	O
.	O
the	O
same	O
holds	O
true	O
for	O
`1	O
regu-	O
larized	O
logistic	B
regression	I
versus	O
the	O
`1	O
svm—their	O
end-path	O
limits	O
are	O
the	O
same	O
.	O
in	O
fact	O
,	O
due	O
to	O
the	O
similarity	O
of	O
the	O
loss	O
functions	O
,	O
their	O
solutions	O
are	O
not	O
too	O
different	O
elsewhere	O
along	O
the	O
path	B
.	O
however	O
,	O
the	O
end-path	O
behavior	O
is	O
a	O
little	O
more	O
complex	O
.	O
they	O
both	O
converge	O
to	O
the	O
`1	O
maximizing	O
margin	O
separator—that	O
is	O
,	O
the	O
margin	O
is	O
measured	O
with	O
respect	O
to	O
the	O
`1	O
distance	O
of	O
points	O
to	O
the	O
decision	O
boundary	O
,	O
or	O
maximum	O
absolute	O
coordinate.	O
8	O
320	O
sparse	O
modeling	O
and	O
the	O
lasso	B
lasso	O
and	O
boosting	O
in	O
chapter	O
17	O
we	O
discuss	O
boosting	O
,	O
a	O
general	O
method	B
for	O
building	O
a	O
com-	O
plex	O
prediction	O
model	B
using	O
simple	O
building	O
components	O
.	O
in	O
its	O
simplest	O
form	B
(	O
regression	B
)	O
boosting	O
amounts	O
to	O
the	O
following	O
simple	O
iteration	O
:	O
1	O
inititialize	O
b	O
d	O
0	O
and	O
f	O
0.x/	O
wd	O
0	O
.	O
2	O
for	O
b	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
b	O
:	O
(	O
a	O
)	O
compute	O
the	O
residuals	O
ri	O
d	O
yi	O
(	O
cid:0	O
)	O
f	O
.xi	O
/	O
;	O
i	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
n	O
;	O
(	O
b	O
)	O
ﬁt	O
a	O
small	O
regression	B
tree	O
to	O
the	O
observations	O
.xi	O
;	O
ri	O
/n	O
(	O
c	O
)	O
update	O
f	O
b.x/	O
d	O
f	O
b	O
(	O
cid:0	O
)	O
1.x/	O
c	O
(	O
cid:15	O
)	O
(	O
cid:1	O
)	O
gb.x/	O
.	O
the	O
“	O
smallness	O
”	O
of	O
the	O
tree	O
limits	O
the	O
interaction	O
order	O
of	O
the	O
model	B
(	O
e.g	O
.	O
a	O
tree	O
with	O
only	O
two	O
splits	O
involves	O
at	O
most	O
two	O
variables	O
)	O
.	O
the	O
number	O
of	O
terms	O
b	O
and	O
the	O
shrinkage	B
parameter	O
(	O
cid:15	O
)	O
are	O
both	O
tuning	O
parameters	O
that	O
control	B
the	O
rate	B
of	O
learning	O
(	O
and	O
hence	O
overﬁtting	O
)	O
,	O
and	O
need	O
to	O
be	O
set	B
,	O
for	O
example	O
by	O
cross-validation	O
.	O
think	O
of	O
as	O
estimating	O
a	O
function	B
gb.x/	O
;	O
and	O
1	O
,	O
which	O
we	O
can	O
in	O
words	O
this	O
algorithm	B
performs	O
a	O
search	O
in	O
the	O
space	B
of	O
trees	B
for	O
the	O
one	O
most	O
correlated	O
with	O
the	O
residual	O
,	O
and	O
then	O
moves	O
the	O
ﬁtted	O
function	B
f	O
b	O
a	O
small	O
amount	O
in	O
that	O
direction—a	O
process	O
known	O
as	O
forward-stagewise	O
ﬁtting	B
.	O
one	O
can	O
paraphrase	O
this	O
simple	O
algorithm	B
in	O
the	O
context	O
of	O
linear	B
regression	O
,	O
where	O
in	O
step	O
2	O
(	O
b	O
)	O
the	O
space	B
of	O
small	O
trees	B
is	O
replaced	O
by	O
linear	B
functions	O
.	O
1	O
inititialize	O
ˇ0	O
d	O
0	O
,	O
and	O
standardize	O
all	O
the	O
variables	O
xj	O
;	O
j	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
p.	O
2	O
for	O
b	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
b	O
:	O
(	O
a	O
)	O
compute	O
the	O
residuals	O
r	O
d	O
y	O
(	O
cid:0	O
)	O
x	O
ˇb	O
;	O
(	O
b	O
)	O
ﬁnd	O
the	O
predictor	B
xj	O
most	O
correlated	O
with	O
the	O
residual	O
vector	B
r	O
;	O
and	O
c	O
(	O
cid:15	O
)	O
(	O
cid:1	O
)	O
sj	O
(	O
sj	O
being	O
the	O
sign	O
of	O
d	O
ˇb	O
(	O
c	O
)	O
update	O
ˇb	O
to	O
ˇbc1	O
,	O
where	O
ˇbc1	O
the	O
correlation	O
)	O
,	O
leaving	O
all	O
the	O
other	O
components	O
alone	O
.	O
j	O
j	O
for	O
small	O
(	O
cid:15	O
)	O
the	O
solution	O
paths	O
for	O
this	O
least-squares	O
boosting	O
and	O
the	O
lasso	B
are	O
very	O
similar	O
.	O
it	O
is	O
natural	O
to	O
consider	O
the	O
limiting	O
case	O
or	O
inﬁnitesimal	O
forward	O
stagewise	O
ﬁtting	B
,	O
which	O
we	O
will	O
abbreviate	O
ifs	O
.	O
one	O
can	O
imagine	O
a	O
scenario	O
where	O
a	O
number	O
of	O
variables	O
are	O
vying	O
to	O
win	O
the	O
competition	O
in	O
step	O
2	O
(	O
b	O
)	O
,	O
and	O
once	O
they	O
are	O
tied	O
their	O
coefﬁcients	O
move	O
in	O
concert	O
as	O
they	O
each	O
get	O
incremented	O
.	O
this	O
was	O
in	O
fact	O
the	O
inspiration	O
for	O
the	O
lar	O
algorithm	B
16.3	O
,	O
where	O
a	O
represents	O
the	O
set	B
of	O
tied	O
variables	O
,	O
and	O
ı	O
is	O
the	O
relative	O
number	O
of	O
turns	O
they	O
each	O
have	O
in	O
getting	O
their	O
coefﬁcients	O
up-	O
dated	O
.	O
it	O
turns	O
out	O
that	O
ifs	O
is	O
often	O
but	O
not	O
always	O
exactly	O
the	O
lasso	B
;	O
it	O
can	O
instead	O
be	O
characterized	O
as	O
a	O
type	O
of	O
monotone	O
lasso.	O
9	O
16.8	O
notes	O
and	O
details	O
321	O
not	O
only	O
do	O
these	O
connections	O
inspire	O
new	O
insights	O
and	O
algorithms	O
for	O
the	O
lasso	B
,	O
they	O
also	O
offer	O
insights	O
into	O
boosting	O
.	O
we	O
can	O
think	O
of	O
boosting	O
as	O
ﬁtting	B
a	O
monotone	O
lasso	B
path	O
in	O
the	O
high-dimensional	O
space	B
of	O
variables	O
deﬁned	O
by	O
all	O
possible	O
trees	B
of	O
a	O
certain	O
size	O
.	O
extensions	O
of	O
the	O
lasso	B
(	O
cid:15	O
)	O
the	O
group	O
lasso	B
penaltypk	O
kd1	O
the	O
idea	O
of	O
using	O
`1	B
regularization	I
to	O
induce	O
sparsity	O
has	O
taken	O
hold	O
,	O
and	O
variations	O
of	O
these	O
ideas	O
have	O
spread	O
like	O
wildﬁre	O
in	O
applied	O
statistical	O
mod-	O
eling	O
.	O
along	O
with	O
advances	O
in	O
convex	O
optimization	O
,	O
hardly	O
any	O
branch	O
of	O
applied	O
statistics	B
has	O
been	O
left	O
untouched	O
.	O
we	O
don	O
’	O
t	B
go	O
into	O
detail	O
here	O
,	O
but	O
refer	O
the	O
reader	O
to	O
the	O
references	O
in	O
the	O
endnotes	O
.	O
instead	O
we	O
will	O
end	O
this	O
section	O
with	O
a	O
(	O
non-exhaustive	O
)	O
list	O
of	O
such	O
applications	O
,	O
which	O
may	O
entice	O
the	O
reader	O
to	O
venture	O
into	O
this	O
domain	O
.	O
kkk2	O
applies	O
to	O
vectors	O
k	O
of	O
parame-	O
ters	O
,	O
and	O
selects	O
whole	O
groups	O
at	O
a	O
time	O
.	O
armed	O
with	O
these	O
penalties	O
,	O
one	O
can	O
derive	O
lasso-like	O
schemes	O
for	O
including	O
multilevel	O
factors	O
in	O
linear	B
models	O
,	O
as	O
well	O
as	O
hierarchical	O
schemes	O
for	O
including	O
low-order	O
interac-	O
tions	O
.	O
(	O
cid:15	O
)	O
the	O
graphical	O
lasso	B
applies	O
`1	O
penalties	O
in	O
the	O
problem	O
of	O
edge	O
selection	O
in	O
dependence	O
graphs	O
.	O
(	O
cid:15	O
)	O
sparse	O
principal	B
components	I
employ	O
`1	O
penalties	O
to	O
produce	O
compo-	O
nents	O
with	O
many	O
loadings	O
zero	O
.	O
the	O
same	O
ideas	O
are	O
applied	O
to	O
discrimi-	O
nant	O
analysis	B
and	O
canonical	O
correlation	O
analysis	O
.	O
(	O
cid:15	O
)	O
the	O
nuclear	O
norm	O
of	O
a	O
matrix	B
is	O
the	O
sum	O
of	O
its	O
singular	O
values—a	O
lasso	B
penalty	O
on	O
matrices	O
.	O
nuclear-norm	O
regularization	B
is	O
popular	O
in	O
matrix	B
completion	O
for	O
estimating	O
missing	O
entries	O
in	O
a	O
matrix	B
.	O
16.8	O
notes	O
and	O
details	O
classical	O
regression	B
theory	O
aimed	O
for	O
an	O
unbiased	O
estimate	O
of	O
each	O
predic-	O
tor	O
variable	O
’	O
s	O
effect	O
.	O
modern	O
wide	O
data	B
sets	O
,	O
often	O
with	O
enormous	O
numbers	O
of	O
predictors	O
p	O
,	O
make	O
that	O
an	O
untenable	O
goal	O
.	O
the	O
methods	O
described	O
here	O
,	O
by	O
necessity	O
,	O
use	O
shrinkage	B
methods	O
,	O
biased	O
estimation	B
,	O
and	O
sparsity	O
.	O
the	O
lasso	B
was	O
introduced	O
by	O
tibshirani	O
(	O
1996	O
)	O
,	O
and	O
has	O
spawned	O
a	O
great	O
deal	O
of	O
research	O
.	O
the	O
recent	O
monograph	O
by	O
hastie	O
et	O
al	O
.	O
(	O
2015	O
)	O
gives	O
a	O
compact	O
summary	O
of	O
some	O
of	O
the	O
areas	O
where	O
the	O
lasso	B
and	O
sparsity	O
have	O
been	O
applied	O
.	O
the	O
regression	B
version	O
of	O
boosting	O
was	O
given	O
in	O
hastie	O
et	O
al	O
.	O
(	O
2009	O
,	O
chapter	O
16	O
)	O
,	O
and	O
inspired	O
the	O
least-angle	O
regression	B
algorithm	O
(	O
efron	O
322	O
sparse	O
modeling	O
and	O
the	O
lasso	B
et	O
al.	O
,	O
2004	O
)	O
—a	O
new	O
and	O
more	O
democratic	O
version	O
of	O
forward-stepwise	O
re-	O
gression	O
,	O
as	O
well	O
as	O
a	O
fast	O
algorithm	B
for	O
ﬁtting	B
the	O
lasso	B
.	O
these	O
authors	O
showed	O
under	O
some	O
conditions	B
that	O
each	O
step	O
of	O
the	O
lar	O
algorithm	B
corre-	O
sponds	O
to	O
one	O
df	O
;	O
zou	O
et	O
al	O
.	O
(	O
2007	O
)	O
show	O
that	O
,	O
with	O
a	O
ﬁxed	O
(	O
cid:21	O
)	O
,	O
the	O
size	O
of	O
the	O
active	O
set	B
is	O
unbiased	O
for	O
the	O
df	O
for	O
the	O
lasso	B
.	O
hastie	O
et	O
al	O
.	O
(	O
2009	O
)	O
also	O
view	O
boosting	O
as	O
ﬁtting	B
a	O
lasso	B
regularization	O
path	B
in	O
the	O
high-dimensional	O
space	B
of	O
trees	B
.	O
friedman	O
et	O
al	O
.	O
(	O
2010	O
)	O
developed	O
the	O
pathwise	O
coordinate-descent	O
algo-	O
rithm	O
for	O
generalized	O
lasso	B
problems	O
,	O
and	O
provide	O
the	O
glmnet	B
package	O
for	O
r	O
(	O
friedman	O
et	O
al.	O
,	O
2009	O
)	O
.	O
strong	O
rules	O
for	O
lasso	B
screening	O
are	O
due	O
to	O
tibshirani	O
et	O
al	O
.	O
(	O
2012	O
)	O
.	O
hastie	O
et	O
al	O
.	O
(	O
2015	O
,	O
chapter	O
3	O
)	O
show	O
the	O
similarity	O
between	O
the	O
`1	O
svm	O
and	O
lasso	O
logistic	B
regression	I
.	O
we	O
now	O
give	O
some	O
particular	O
technical	O
details	O
on	O
topics	O
covered	O
in	O
the	O
chapter	O
.	O
1	O
[	O
p.	O
301	O
]	O
forward-stepwise	O
computations	B
.	O
building	O
up	O
the	O
forward-stepwise	O
model	B
can	O
be	O
seen	O
as	O
a	O
guided	O
gram–schmidt	O
orthogonalization	O
(	O
qr	O
de-	O
composition	O
)	O
.	O
after	O
step	O
r	O
,	O
all	O
p	O
(	O
cid:0	O
)	O
r	O
variables	O
not	O
in	O
the	O
model	B
are	O
orthog-	O
onal	O
to	O
the	O
r	O
in	O
the	O
model	B
,	O
and	O
the	O
latter	O
are	O
in	O
qr	O
form	B
.	O
then	O
the	O
next	O
variable	O
to	O
enter	O
is	O
the	O
one	O
most	O
correlated	O
with	O
the	O
residuals	O
.	O
this	O
is	O
the	O
one	O
that	O
will	O
reduce	O
the	O
residual	O
sum-of-squares	O
the	O
most	O
,	O
and	O
one	O
requires	O
p	O
(	O
cid:0	O
)	O
r	O
n-vector	O
inner	O
products	O
to	O
identify	O
it	O
.	O
the	O
regression	B
is	O
then	O
updated	O
trivially	O
to	O
accommodate	O
the	O
chosen	O
one	O
,	O
which	O
is	O
then	O
regressed	O
out	O
of	O
the	O
p	O
(	O
cid:0	O
)	O
r	O
(	O
cid:0	O
)	O
1	O
remaining	O
variables	O
.	O
2	O
[	O
p.	O
301	O
]	O
iteratively	O
reweighted	O
least	B
squares	I
(	O
irls	O
)	O
.	O
generalized	O
linear	B
models	O
(	O
chapter	O
8	O
)	O
are	O
ﬁt	O
by	O
maximum-likelihood	O
,	O
and	O
since	O
the	O
log-likeli-	O
hood	O
is	O
differentiable	O
and	O
concave	O
,	O
typically	O
a	O
newton	O
algorithm	B
is	O
used	O
.	O
the	O
newton	O
algorithm	B
can	O
be	O
recast	O
as	O
an	O
iteratively	O
reweighted	O
linear	B
re-	O
gression	O
algorithm	B
(	O
mccullagh	O
and	O
nelder	O
,	O
1989	O
)	O
.	O
at	O
each	O
iteration	O
one	O
computes	O
a	O
working	O
response	O
variable	O
zi	O
,	O
and	O
a	O
weight	O
per	O
observation	O
wi	O
(	O
both	O
of	O
which	O
depend	O
on	O
the	O
current	O
parameter	O
vector	B
o	O
ˇ	O
)	O
.	O
then	O
the	O
new-	O
ton	O
update	O
for	O
o	O
ˇ	O
is	O
obtained	O
by	O
a	O
weighted	O
least-squares	O
ﬁt	O
of	O
the	O
zi	O
on	O
the	O
xi	O
with	O
weights	O
wi	O
(	O
hastie	O
et	O
al.	O
,	O
2009	O
,	O
section	O
4.4.1	O
)	O
.	O
3	O
[	O
p.	O
301	O
]	O
forward-stepwise	O
logistic	B
regression	I
computations	O
.	O
although	O
the	O
current	O
model	B
is	O
in	O
the	O
form	B
of	O
a	O
weighted	O
least-squares	O
ﬁt	O
,	O
the	O
p	O
(	O
cid:0	O
)	O
r	O
vari-	O
ables	O
not	O
in	O
the	O
model	B
can	O
not	O
be	O
kept	O
orthogonal	O
to	O
those	O
in	O
the	O
model	B
(	O
the	O
weights	O
keep	O
changing	O
!	O
)	O
.	O
however	O
,	O
since	O
our	O
current	O
model	B
will	O
have	O
per-	O
formed	O
a	O
weighted	O
qr	O
decomposition	O
(	O
say	O
)	O
,	O
this	O
orthogonalization	O
can	O
be	O
obtained	O
without	O
too	O
much	O
cost	O
.	O
we	O
will	O
need	O
p	O
(	O
cid:0	O
)	O
r	O
multiplications	O
of	O
an	O
r	O
(	O
cid:2	O
)	O
n	O
matrix	B
with	O
an	O
n	O
vector—o..p	O
(	O
cid:0	O
)	O
r/	O
(	O
cid:1	O
)	O
r	O
(	O
cid:1	O
)	O
n	O
computations	B
.	O
an	O
even	O
simpler	O
alternative	O
for	O
the	O
selection	O
is	O
to	O
use	O
the	O
size	O
of	O
the	O
gradient	O
of	O
the	O
16.8	O
notes	O
and	O
details	O
323	O
log-likelihood	B
,	O
which	O
simply	O
requires	O
an	O
inner	O
product	O
jhy	O
(	O
cid:0	O
)	O
o	O
(	O
cid:22	O
)	O
r	O
;	O
xjij	O
for	O
each	O
omitted	O
variable	O
xj	O
(	O
assuming	O
all	O
the	O
variables	O
are	O
standardized	O
to	O
unit	O
variance	O
)	O
.	O
4	O
[	O
p.	O
306	O
]	O
best	O
`1	O
interpolant	O
.	O
if	O
p	O
>	O
n	O
,	O
then	O
another	O
boundary	O
solution	O
becomes	O
interesting	O
for	O
the	O
lasso	B
.	O
for	O
t	B
sufﬁciently	O
large	O
,	O
we	O
will	O
be	O
able	O
to	O
achieve	O
a	O
perfect	O
ﬁt	O
to	O
the	O
data	B
,	O
and	O
hence	O
a	O
zero	O
residual	O
.	O
there	O
will	O
be	O
many	O
such	O
solutions	O
,	O
so	O
it	O
becomes	O
interesting	O
to	O
ﬁnd	O
the	O
perfect-ﬁt	O
so-	O
lution	O
with	O
smallest	O
value	O
of	O
t	B
:	O
the	O
minimum-`1-norm	O
perfect-ﬁt	O
solution	O
.	O
this	O
requires	O
solving	O
a	O
separate	O
convex-optimization	O
problem	O
.	O
5	O
[	O
p.	O
313	O
]	O
more	O
on	O
df	O
.	O
when	O
the	O
search	O
is	O
easy	O
in	O
that	O
a	O
variable	O
stands	O
out	O
as	O
far	O
superior	O
,	O
lar	O
takes	O
a	O
big	O
step	O
,	O
and	O
forward	O
stepwise	O
spends	O
close	O
to	O
a	O
unit	O
df	O
.	O
on	O
the	O
other	O
hand	O
,	O
when	O
there	O
is	O
close	O
competition	O
,	O
the	O
lar	O
steps	O
are	O
small	O
,	O
and	O
a	O
unit	O
df	O
is	O
spent	O
for	O
little	O
progress	O
,	O
while	O
forward	O
stepwise	O
can	O
spend	O
a	O
fair	O
bit	O
more	O
than	O
a	O
unit	O
df	O
(	O
the	O
price	O
paid	O
for	O
searching	O
)	O
.	O
in	O
fact	O
,	O
the	O
dfj	O
curve	O
for	O
forward	O
stepwise	O
can	O
exceed	O
p	O
for	O
j	O
<	O
p	O
(	O
jansen	O
et	O
al.	O
,	O
2015	O
)	O
.	O
6	O
[	O
p.	O
318	O
]	O
post-selection	O
inference	B
.	O
there	O
has	O
been	O
a	O
lot	O
of	O
activity	O
around	O
post-selection	O
inference	B
for	O
lasso	B
and	O
related	O
methods	O
,	O
all	O
of	O
it	O
since	O
2012.	O
to	O
a	O
large	O
extent	O
this	O
was	O
inspired	O
by	O
the	O
work	O
of	O
berk	O
et	O
al	O
.	O
(	O
2013	O
)	O
,	O
but	O
more	O
tailored	O
to	O
the	O
particular	O
selection	O
process	O
employed	O
by	O
the	O
lasso	B
.	O
for	O
the	O
debiasing	O
approach	O
we	O
look	O
to	O
the	O
work	O
of	O
zhang	O
and	O
zhang	O
(	O
2014	O
)	O
,	O
van	O
de	O
geer	O
et	O
al	O
.	O
(	O
2014	O
)	O
and	O
javanmard	O
and	O
montanari	O
(	O
2014	O
)	O
.	O
the	O
condi-	O
tional	O
inference	B
approach	O
began	O
with	O
lockhart	O
et	O
al	O
.	O
(	O
2014	O
)	O
,	O
and	O
then	O
was	O
developed	O
further	O
in	O
a	O
series	O
of	O
papers	O
(	O
lee	O
et	O
al.	O
,	O
2016	O
;	O
taylor	O
et	O
al.	O
,	O
2015	O
;	O
fithian	O
et	O
al.	O
,	O
2014	O
)	O
,	O
with	O
many	O
more	O
in	O
the	O
pipeline	O
.	O
7	O
[	O
p.	O
319	O
]	O
selective	O
inference	B
software	O
.	O
the	O
example	O
in	O
figure	O
16.10	O
was	O
pro-	O
duced	O
using	O
the	O
r	O
package	O
selectiveinference	O
(	O
tibshirani	O
et	O
al.	O
,	O
2016	O
)	O
.	O
thanks	O
to	O
rob	O
tibshirani	O
for	O
providing	O
this	O
example	O
.	O
8	O
[	O
p.	O
319	O
]	O
end-path	O
behavior	O
of	O
ridge	O
and	O
lasso	B
logistic	O
regression	B
for	O
sep-	O
arable	O
data	B
.	O
the	O
details	O
here	O
are	O
somewhat	O
technical	O
,	O
and	O
rely	O
on	O
dual	B
norms	O
.	O
details	O
are	O
given	O
in	O
hastie	O
et	O
al	O
.	O
(	O
2015	O
,	O
section	O
3.6.1	O
)	O
.	O
9	O
[	O
p.	O
320	O
]	O
lar	O
and	O
boosting	O
.	O
least-squares	O
boosting	O
moves	O
the	O
“	O
winning	O
”	O
coefﬁcient	O
in	O
the	O
direction	O
of	O
the	O
correlation	O
of	O
its	O
variable	O
with	O
the	O
resid-	O
ual	O
.	O
the	O
direction	O
ı	O
computed	O
in	O
step	O
3	O
(	O
a	O
)	O
of	O
the	O
lar	O
algorithm	B
may	O
have	O
some	O
components	O
whose	O
signs	O
do	O
not	O
agree	O
with	O
their	O
correlations	O
,	O
espe-	O
cially	O
if	O
the	O
variables	O
are	O
very	O
correlated	O
.	O
this	O
can	O
be	O
ﬁxed	O
by	O
a	O
particular	O
nonnegative	O
least-squares	O
ﬁt	O
to	O
yield	O
an	O
exact	O
path	B
algorithm	O
for	O
ifs	O
;	O
de-	O
tails	O
can	O
be	O
found	O
in	O
efron	O
et	O
al	O
.	O
(	O
2004	O
)	O
.	O
17	O
random	O
forests	O
and	O
boosting	O
in	O
the	O
modern	O
world	O
we	O
are	O
often	O
faced	O
with	O
enormous	O
data	B
sets	O
,	O
both	O
in	O
terms	O
of	O
the	O
number	O
of	O
observations	O
n	O
and	O
in	O
terms	O
of	O
the	O
number	O
of	O
variables	O
p.	O
this	O
is	O
of	O
course	O
good	O
news—we	O
have	O
always	O
said	O
the	O
more	O
data	B
we	O
have	O
,	O
the	O
better	O
predictive	O
models	B
we	O
can	O
build	O
.	O
well	O
,	O
we	O
are	O
there	O
now—we	O
have	O
tons	O
of	O
data	B
,	O
and	O
must	O
ﬁgure	O
out	O
how	O
to	O
use	O
it	O
.	O
although	O
we	O
can	O
scale	B
up	O
our	O
software	O
to	O
ﬁt	O
the	O
collection	O
of	O
linear	B
and	O
generalized	O
linear	B
models	O
to	O
these	O
behemoths	O
,	O
they	O
are	O
often	O
too	O
modest	O
and	O
can	O
fall	O
way	O
short	O
in	O
terms	O
of	O
predictive	O
power	O
.	O
a	O
need	O
arose	O
for	O
some	O
general	O
purpose	O
tools	O
that	O
could	O
scale	B
well	O
to	O
these	O
bigger	O
problems	O
,	O
and	O
exploit	O
the	O
large	O
amount	O
of	O
data	B
by	O
ﬁtting	B
a	O
much	O
richer	O
class	O
of	O
functions	O
,	O
almost	O
automatically	O
.	O
random	O
forests	O
and	O
boosting	O
are	O
two	O
relatively	O
re-	O
cent	O
innovations	O
that	O
ﬁt	O
the	O
bill	O
,	O
and	O
have	O
become	O
very	O
popular	O
as	O
“	O
out-the-	O
box	O
”	O
learning	O
algorithms	O
that	O
enjoy	O
good	O
predictive	O
performance	O
.	O
random	O
forests	O
are	O
somewhat	O
more	O
automatic	O
than	O
boosting	O
,	O
but	O
can	O
also	O
suffer	O
a	O
small	O
performance	O
hit	O
as	O
a	O
consequence	O
.	O
these	O
two	O
methods	O
have	O
something	O
in	O
common	O
:	O
they	O
both	O
represent	O
the	O
ﬁtted	O
model	B
by	O
a	O
sum	O
of	O
regression	B
trees	O
.	O
we	O
discuss	O
trees	B
in	O
some	O
detail	O
in	O
chapter	O
8.	O
a	O
single	O
regression	B
tree	O
is	O
typically	O
a	O
rather	O
weak	O
prediction	O
model	B
;	O
it	O
is	O
rather	O
amazing	O
that	O
an	O
ensemble	O
of	O
trees	B
leads	O
to	O
the	O
state	O
of	O
the	O
art	O
in	O
black-box	O
predictors	O
!	O
we	O
can	O
broadly	O
describe	O
both	O
these	O
methods	O
very	O
simply	O
.	O
random	O
forest	O
grow	O
many	O
deep	O
regression	B
trees	O
to	O
randomized	O
versions	O
of	O
the	O
training	O
data	B
,	O
and	O
average	O
them	O
.	O
here	O
“	O
randomized	O
”	O
is	O
a	O
wide-	O
ranging	O
term	O
,	O
and	O
includes	O
bootstrap	O
sampling	O
and/or	O
subsampling	O
of	O
the	O
observations	O
,	O
as	O
well	O
as	O
subsampling	O
of	O
the	O
variables	O
.	O
boosting	O
repeatedly	O
grow	O
shallow	O
trees	B
to	O
the	O
residuals	O
,	O
and	O
hence	O
build	O
up	O
an	O
additive	O
model	B
consisting	O
of	O
a	O
sum	O
of	O
trees	B
.	O
the	O
basic	O
mechanism	O
in	O
random	O
forests	O
is	O
variance	O
reduction	O
by	O
averag-	O
ing	O
.	O
each	O
deep	O
tree	O
has	O
a	O
high	O
variance	O
,	O
and	O
the	O
averaging	B
brings	O
the	O
vari-	O
324	O
17.1	O
random	O
forests	O
325	O
ance	O
down	O
.	O
in	O
boosting	O
the	O
basic	O
mechanism	O
is	O
bias	O
reduction	O
,	O
although	O
different	O
ﬂavors	O
include	O
some	O
variance	O
reduction	O
as	O
well	O
.	O
both	O
methods	O
inherit	O
all	O
the	O
good	O
attributes	O
of	O
trees	B
,	O
most	O
notable	O
of	O
which	O
is	O
variable	O
selection	O
.	O
17.1	O
random	O
forests	O
suppose	O
we	O
have	O
the	O
usual	O
setup	O
for	O
a	O
regression	B
problem	O
,	O
with	O
a	O
training	O
set	B
consisting	O
of	O
an	O
n	O
(	O
cid:2	O
)	O
p	O
data	B
matrix	O
x	O
and	O
an	O
n-vector	O
of	O
responses	O
y.	O
a	O
tree	O
(	O
section	O
8.4	O
)	O
ﬁts	O
a	O
piecewise	O
constant	O
surface	O
or.x/	O
over	O
the	O
domain	O
x	O
by	O
recursive	O
partitioning	O
.	O
the	O
model	B
is	O
built	O
in	O
a	O
greedy	O
fashion	O
,	O
each	O
time	O
creating	O
two	O
daughter	O
nodes	B
from	O
a	O
terminal	B
node	I
by	O
deﬁning	O
a	O
binary	O
split	O
using	O
one	O
of	O
the	O
available	O
variables	O
.	O
the	O
model	B
can	O
hence	O
be	O
represented	O
by	O
a	O
binary	O
tree	O
.	O
part	O
of	O
the	O
art	O
in	O
using	O
regression	B
trees	O
is	O
to	O
know	O
how	O
deep	O
to	O
grow	O
the	O
tree	O
,	O
or	O
alternatively	O
how	O
much	O
to	O
prune	O
it	O
back	O
.	O
typi-	O
cally	O
that	O
is	O
achieved	O
using	O
left-out	O
data	B
or	O
cross-validation	O
.	O
figure	O
17.1	O
shows	O
a	O
tree	O
ﬁt	O
to	O
the	O
spam	B
training	O
data	B
.	O
the	O
splitting	O
variables	O
and	O
split	O
points	O
are	O
indicated	O
.	O
each	O
node	O
is	O
labeled	O
as	O
spam	B
or	O
ham	O
(	O
not	O
spam	O
;	O
see	O
footnote	O
7	O
on	O
page	O
115	O
)	O
.	O
the	O
numbers	O
beneath	O
each	O
node	O
show	O
mis-	O
classiﬁed/total	O
.	O
the	O
overall	O
misclassiﬁcation	O
error	O
on	O
the	O
test	O
data	B
is	O
9:3	O
%	O
,	O
which	O
compares	O
poorly	O
with	O
the	O
performance	O
of	O
the	O
lasso	B
(	O
figure	O
16.9	O
:	O
7:1	O
%	O
for	O
linear	B
lasso	O
,	O
5:7	O
%	O
for	O
lasso	B
with	O
interactions	O
)	O
.	O
the	O
surface	O
or.x/	O
here	O
is	O
clearly	O
complex	O
,	O
and	O
by	O
its	O
nature	O
represents	O
a	O
rather	O
high-order	O
interaction	O
(	O
the	O
deepest	O
branch	O
is	O
eight	O
levels	O
,	O
and	O
involves	O
splits	O
on	O
eight	O
different	O
variables	O
)	O
.	O
despite	O
the	O
promise	O
to	O
deliver	O
interpretable	O
models	B
,	O
this	O
bushy	O
tree	O
is	O
not	O
easy	O
to	O
interpret	O
.	O
nevertheless	O
,	O
trees	B
have	O
some	O
desir-	O
able	O
properties	O
.	O
the	O
following	O
lists	O
some	O
of	O
the	O
good	O
and	O
bad	O
properties	O
of	O
trees	B
.	O
l	O
trees	B
automatically	O
select	O
variables	O
;	O
only	O
variables	O
used	O
in	O
deﬁning	O
splits	O
are	O
in	O
the	O
model	B
.	O
l	O
tree-growing	O
algorithms	O
scale	B
well	O
to	O
large	O
n	O
;	O
growing	O
a	O
tree	O
is	O
a	O
divide-	O
and-conquer	O
operation	O
.	O
l	O
trees	B
handle	O
mixed	O
features	O
(	O
quantitative/qualitative	O
)	O
seamlessly	O
,	O
and	O
can	O
deal	O
with	O
missing	B
data	I
.	O
l	O
small	O
trees	B
are	O
easy	O
to	O
interpret	O
.	O
m	O
large	O
trees	B
are	O
not	O
easy	O
to	O
interpret	O
.	O
m	O
trees	B
do	O
not	O
generally	O
have	O
good	O
prediction	O
performance	O
.	O
trees	B
are	O
inherently	O
high-variance	O
function	B
estimators	O
,	O
and	O
the	O
bushier	O
they	O
are	O
,	O
the	O
higher	O
the	O
variance	O
.	O
the	O
early	O
splits	O
dictate	O
the	O
architecture	O
of	O
326	O
random	O
forests	O
and	O
boosting	O
figure	O
17.1	O
regression	B
tree	O
ﬁt	O
to	O
the	O
binary	O
spam	B
data	O
,	O
a	O
bigger	O
version	O
of	O
figure	O
8.7.	O
the	O
initial	O
trained	O
tree	O
was	O
far	O
bushier	O
than	O
the	O
one	O
displayed	O
;	O
it	O
was	O
then	O
optimally	O
pruned	O
using	O
10-fold	B
cross-validation	O
.	O
the	O
tree	O
.	O
on	O
the	O
other	O
hand	O
,	O
deep	O
bushy	O
trees	B
localize	O
the	O
training	O
data	B
(	O
us-	O
ing	O
the	O
variables	O
that	O
matter	O
)	O
to	O
a	O
relatively	O
small	O
region	B
around	O
the	O
target	O
point	O
.	O
this	O
suggests	O
low	O
bias	O
.	O
the	O
idea	O
of	O
random	O
forests	O
(	O
and	O
its	O
predeces-	O
sor	O
bagging	O
)	O
is	O
to	O
grow	O
many	O
very	O
bushy	O
trees	B
,	O
and	O
get	O
rid	O
of	O
the	O
variance	O
by	O
averaging	B
.	O
in	O
order	O
to	O
beneﬁt	O
from	O
averaging	O
,	O
the	O
individual	O
trees	B
should	O
not	O
be	O
too	O
correlated	O
.	O
this	O
is	O
achieved	O
by	O
injecting	O
some	O
randomness	O
into	O
the	O
tree-growing	O
process	O
.	O
random	O
forests	O
achieve	O
this	O
in	O
two	O
ways	O
.	O
600/1536280/1177180/1065	O
80/861	O
80/652	O
77/423	O
20/238	O
19/236	O
1/2	O
57/185	O
48/113	O
37/101	O
1/12	O
9/72	O
3/229	O
0/209100/204	O
36/123	O
16/94	O
14/89	O
3/5	O
9/29	O
16/81	O
9/112	O
6/109	O
0/3	O
48/359	O
26/337	O
19/110	O
18/109	O
0/1	O
7/227	O
0/22spamspamspamspamspamspamspamspamspamspamspamspamhamhamhamhamhamhamhamhamhamhamhamhamhamhamhamhamhamhamhamhamhamch	O
$	O
<	O
0.0555remove	O
<	O
0.06ch	O
!	O
<	O
0.191george	O
<	O
0.005hp	O
<	O
0.03capmax	O
<	O
10.5receive	O
<	O
0.125edu	O
<	O
0.045our	O
<	O
1.2capave	O
<	O
2.7505free	O
<	O
0.065business	O
<	O
0.145george	O
<	O
0.15hp	O
<	O
0.405capave	O
<	O
2.9071999	O
<	O
0.58ch	O
$	O
>	O
0.0555remove	O
>	O
0.06ch	O
!	O
>	O
0.191george	O
>	O
0.005hp	O
>	O
0.03capmax	O
>	O
10.5receive	O
>	O
0.125edu	O
>	O
0.045our	O
>	O
1.2capave	O
>	O
2.7505free	O
>	O
0.065business	O
>	O
0.145george	O
>	O
0.15hp	O
>	O
0.405capave	O
>	O
2.9071999	O
>	O
0.58	O
17.1	O
random	O
forests	O
327	O
1	O
bootstrap	O
:	O
each	O
tree	O
is	O
grown	O
to	O
a	O
bootstrap	O
resampled	O
training	O
data	B
set	O
,	O
which	O
makes	O
them	O
different	O
and	O
somewhat	O
decorrelates	O
them	O
.	O
2	O
split-variable	O
randomization	O
:	O
each	O
time	O
a	O
split	O
is	O
to	O
be	O
performed	O
,	O
the	O
search	O
for	O
the	O
split	O
variable	O
is	O
limited	O
to	O
a	O
random	O
subset	O
of	O
m	O
of	O
the	O
p	O
variables	O
.	O
typical	O
values	O
of	O
m	O
are	O
p	O
or	O
p=3	O
.	O
p	O
when	O
m	O
d	O
p	O
,	O
the	O
randomization	O
amounts	O
to	O
using	O
only	O
step	O
1	O
,	O
and	O
was	O
an	O
earlier	O
ancestor	O
of	O
random	O
forests	O
called	O
bagging	O
.	O
in	O
most	O
examples	O
the	O
second	O
level	O
of	O
randomization	O
pays	O
dividends	O
.	O
b.	O
algorithm	B
17.1	O
random	O
forest	O
.	O
1	O
given	O
training	O
data	B
set	O
d	O
d	O
.x	O
;	O
y/	O
.	O
fix	O
m	O
	O
p	O
and	O
the	O
number	O
of	O
trees	O
2	O
for	O
b	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
b	O
,	O
do	O
the	O
following	O
.	O
(	O
a	O
)	O
create	O
a	O
bootstrap	O
version	O
of	O
the	O
training	O
data	B
d	O
(	O
cid:3	O
)	O
b	O
,	O
by	O
randomly	O
sam-	O
pling	O
the	O
n	O
rows	O
with	O
replacement	O
n	O
times	O
.	O
the	O
sample	B
can	O
be	O
repre-	O
sented	O
by	O
the	O
bootstrap	O
frequency	O
vector	B
w	O
(	O
b	O
)	O
grow	O
a	O
maximal-depth	O
tree	O
orb.x/	O
using	O
the	O
data	B
in	O
d	O
of	O
the	O
p	O
features	O
at	O
random	O
prior	B
to	O
making	O
each	O
split	O
.	O
(	O
cid:3	O
)	O
b	O
,	O
sampling	O
m	O
(	O
cid:3	O
)	O
b	O
.	O
(	O
c	O
)	O
save	O
the	O
tree	O
,	O
as	O
well	O
as	O
the	O
bootstrap	O
sampling	O
frequencies	O
for	O
each	O
of	O
the	O
training	O
observations	O
.	O
3	O
compute	O
the	O
random-forest	O
ﬁt	O
at	O
any	O
prediction	O
point	O
x0	O
as	O
the	O
average	O
bx	O
bd1	O
orrf.x0/	O
d	O
1	O
b	O
orb.x0/	O
:	O
4	O
compute	O
the	O
oobi	O
error	O
for	O
each	O
response	O
observation	O
yi	O
in	O
the	O
training	O
data	B
,	O
by	O
using	O
the	O
ﬁt	O
or	O
.i	O
/	O
,	O
obtained	O
by	O
averaging	B
only	O
those	O
orb.xi	O
/	O
for	O
which	O
observation	O
i	O
was	O
not	O
in	O
the	O
bootstrap	O
sample	B
.	O
the	O
overall	O
oob	O
error	O
is	O
the	O
average	O
of	O
these	O
oobi	O
.	O
rf	O
algorithm	B
17.1	O
gives	O
some	O
of	O
the	O
details	O
;	O
some	O
more	O
are	O
given	O
in	O
the	O
technical	O
notes.	O
the	O
package	O
randomforest	O
in	O
r	O
sets	O
as	O
a	O
default	O
m	O
d	O
p	O
random	O
forests	O
are	O
easy	O
to	O
use	O
,	O
since	O
there	O
is	O
not	O
much	O
tuning	O
needed	O
.	O
p	O
for	O
clas-	O
siﬁcation	O
trees	B
,	O
and	O
m	O
d	O
p=3	O
for	O
regression	B
trees	O
,	O
but	O
one	O
can	O
use	O
other	O
values	O
.	O
with	O
m	O
d	O
1	O
the	O
split	O
variable	O
is	O
completely	O
random	O
,	O
so	O
all	O
vari-	O
ables	O
get	O
a	O
chance	O
.	O
this	O
will	O
decorrelate	O
the	O
trees	B
the	O
most	O
,	O
but	O
can	O
create	O
bias	O
,	O
somewhat	O
similar	O
to	O
that	O
in	O
ridge	B
regression	I
.	O
figure	O
17.2	O
shows	O
the	O
1	O
328	O
random	O
forests	O
and	O
boosting	O
figure	O
17.2	O
test	O
misclassiﬁcation	O
error	O
of	O
random	O
forests	O
on	O
the	O
spam	B
data	O
,	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
trees	O
.	O
the	O
red	O
curve	O
selects	O
m	O
d	O
7	O
of	O
the	O
p	O
d	O
57	O
features	O
at	O
random	O
as	O
candidates	O
for	O
the	O
split	O
variable	O
,	O
each	O
time	O
a	O
split	O
is	O
made	O
.	O
the	O
blue	O
curve	O
uses	O
m	O
d	O
57	O
,	O
and	O
hence	O
amounts	O
to	O
bagging	O
.	O
both	O
bagging	O
and	O
random	O
forests	O
outperform	O
the	O
lasso	B
methods	O
,	O
and	O
a	O
single	O
tree	O
.	O
misclassiﬁcation	O
performance	O
of	O
a	O
random	O
forest	O
on	O
the	O
spam	B
test	O
data	B
,	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
trees	O
averaged	O
.	O
we	O
see	O
that	O
in	O
this	O
case	O
,	O
after	O
a	O
relatively	O
small	O
number	O
of	O
trees	O
(	O
500	O
)	O
,	O
the	O
error	O
levels	O
off	O
.	O
the	O
number	O
b	O
of	O
trees	B
averaged	O
is	O
not	O
a	O
real	O
tuning	O
parameter	O
;	O
as	O
with	O
the	O
bootstrap	O
(	O
chapters	O
10	O
and	O
11	O
)	O
,	O
we	O
need	O
a	O
sufﬁcient	O
number	O
for	O
the	O
estimate	B
to	O
sta-	O
bilize	O
,	O
but	O
can	O
not	O
overﬁt	O
by	O
having	O
too	O
many	O
.	O
random	O
forests	O
have	O
been	O
described	O
as	O
adaptive	O
nearest-neighbor	O
esti-	O
mators—adaptive	O
in	O
that	O
they	O
select	O
predictors	O
.	O
a	O
k-nearest-neighbor	O
esti-	O
mate	O
ﬁnds	O
the	O
k	O
training	O
observations	O
closest	O
in	O
feature	O
space	B
to	O
the	O
target	O
point	O
x0	O
,	O
and	O
averages	O
their	O
responses	O
.	O
each	O
tree	O
in	O
the	O
random	O
forest	O
drills	O
down	O
by	O
recursive	O
partitioning	O
to	O
pure	O
terminal	O
nodes	O
,	O
often	O
consisting	O
of	O
a	O
single	O
observation	O
.	O
hence	O
,	O
when	O
evaluating	O
the	O
prediction	O
from	O
each	O
tree	O
,	O
orb.x0/	O
d	O
y`	O
for	O
some	O
`	O
,	O
and	O
for	O
many	O
of	O
the	O
trees	B
this	O
could	O
be	O
the	O
same	O
0.000.020.040.060.08random	O
forest	O
on	O
the	O
spam	B
datanumber	O
of	O
treestest	O
error15001000150020002500baggingrandom	O
forestsingle	O
treelassolasso	O
(	O
interaction	O
)	O
17.1	O
random	O
forests	O
329	O
`	O
.	O
from	O
the	O
whole	O
collection	O
of	O
b	O
trees	B
,	O
the	O
number	O
of	O
distinct	O
`s	O
can	O
be	O
fairly	O
small	O
.	O
since	O
the	O
partitioning	O
that	O
reaches	O
the	O
terminal	O
nodes	O
involves	O
only	O
a	O
subset	O
of	O
the	O
predictors	O
,	O
the	O
neighborhoods	O
so	O
deﬁned	O
are	O
adaptive	B
.	O
out-of-bag	O
error	O
estimates	O
random	O
forests	O
deliver	O
cross-validated	O
error	O
estimates	O
at	O
virtually	O
no	O
ex-	O
tra	O
cost	O
.	O
the	O
idea	O
is	O
similar	O
to	O
the	O
bootstrap	O
error	O
estimates	O
discussed	O
in	O
chapter	O
10.	O
the	O
computation	O
is	O
described	O
in	O
step	O
4	O
of	O
algorithm	B
17.1.	O
in	O
making	O
the	O
prediction	O
for	O
observation	O
pair	O
.xi	O
;	O
yi	O
/	O
,	O
we	O
average	O
all	O
the	O
random-forest	O
trees	B
orb.xi	O
/	O
for	O
which	O
that	O
pair	O
is	O
not	O
in	O
the	O
corresponding	O
bootstrap	O
sample	B
:	O
figure	O
17.3	O
out-of-bag	O
misclassiﬁcation	O
error	O
estimate	B
for	O
the	O
spam	B
data	O
(	O
blue	O
)	O
versus	O
the	O
test	O
error	O
(	O
red	O
)	O
,	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
trees	O
.	O
x	O
b	O
w	O
w	O
(	O
cid:3	O
)	O
b	O
i	O
d0	O
or	O
.i	O
/	O
rf	O
.xi	O
/	O
d	O
1	O
bi	O
orb.xi	O
/	O
;	O
(	O
17.1	O
)	O
where	O
bi	O
is	O
the	O
number	O
of	O
times	O
observation	O
i	O
was	O
not	O
in	O
the	O
bootstrap	O
(	O
cid:0	O
)	O
1b	O
(	O
cid:25	O
)	O
0:37b	O
)	O
.	O
we	O
then	O
compute	O
the	O
oob	O
sample	B
(	O
with	O
expected	O
value	O
e	O
nx	O
error	O
estimate	B
(	O
17.2	O
)	O
erroob	O
d	O
1	O
n	O
id1	O
lœyi	O
;	O
or	O
.i	O
/	O
rf	O
.xi	O
/	O
;	O
0.000.020.040.060.08number	O
of	O
treesmisclassification	O
error15001000150020002500oob	O
errortest	O
error	O
330	O
random	O
forests	O
and	O
boosting	O
where	O
l	O
is	O
the	O
loss	B
function	I
of	O
interest	O
,	O
such	O
as	O
misclassiﬁcation	O
or	O
squared-	O
error	O
loss	O
.	O
if	O
b	O
is	O
sufﬁciently	O
large	O
(	O
about	O
three	O
times	O
the	O
number	O
needed	O
for	O
the	O
random	O
forest	O
to	O
stabilize	O
)	O
,	O
we	O
can	O
see	O
that	O
the	O
oob	O
error	O
estimate	B
is	O
equivalent	O
to	O
leave-one-out	O
cross-validation	O
error	O
.	O
standard	O
errors	O
	O
is	O
given	O
by	O
bvjack	O
.	O
we	O
can	O
use	O
very	O
similar	O
ideas	O
to	O
estimate	B
the	O
variance	O
of	O
a	O
random-forest	O
prediction	O
,	O
using	O
the	O
jackknife	O
variance	O
estimator	B
(	O
see	O
(	O
10.6	O
)	O
in	O
chapter	O
10	O
)	O
.	O
if	O
o	O
	O
is	O
a	O
statistic	B
estimated	O
using	O
all	O
n	O
training	O
observations	O
,	O
then	O
the	O
jack-	O
knife	O
estimate	O
of	O
the	O
variance	O
of	O
o	O
	O
/	O
d	O
n	O
(	O
cid:0	O
)	O
1	O
o	O
2	O
.i	O
/	O
is	O
the	O
estimate	B
using	O
all	O
but	O
observation	O
i	O
,	O
and	O
o	O
	O
.	O
(	O
cid:1	O
)	O
/	O
d	O
1	O
2	O
at	O
x0	O
is	O
obtained	O
by	O
simply	O
plugging	O
into	O
this	O
formula	B
:	O
(	O
cid:16	O
)	O
o	O
.i	O
/	O
(	O
cid:0	O
)	O
o	O
	O
.	O
(	O
cid:1	O
)	O
/	O
o	O
where	O
o	O
.i	O
/	O
.	O
the	O
natural	O
jackknife	O
variance	O
estimate	B
for	O
a	O
random-forest	O
prediction	O
n	O
i	O
bvjack.orrf.x0//	O
d	O
n	O
(	O
cid:0	O
)	O
1	O
rf	O
.x0/	O
(	O
cid:0	O
)	O
orrf.x0/	O
:	O
nx	O
id1	O
;	O
(	O
17.3	O
)	O
p	O
n	O
(	O
cid:16	O
)	O
or	O
.i	O
/	O
nx	O
id1	O
n	O
(	O
17.4	O
)	O
this	O
formula	B
is	O
derived	O
under	O
the	O
b	O
d	O
1	O
setting	O
,	O
in	O
which	O
case	O
orrf.x0/	O
is	O
an	O
expectation	O
under	O
bootstrap	O
sampling	O
,	O
and	O
hence	O
is	O
free	O
of	O
monte	O
carlo	O
variability	O
.	O
this	O
also	O
makes	O
the	O
distinction	O
clear	O
:	O
we	O
are	O
estimating	O
the	O
sam-	O
pling	O
variability	O
of	O
a	O
random-forest	O
prediction	O
orrf.x0/	O
,	O
as	O
distinct	O
from	O
any	O
monte	O
carlo	O
variation	O
.	O
in	O
practice	O
b	O
is	O
ﬁnite	O
,	O
and	O
expression	O
(	O
17.4	O
)	O
will	O
have	O
monte	O
carlo	O
bias	O
and	O
variance	O
.	O
all	O
of	O
the	O
or	O
.i	O
/	O
rf	O
.x0/	O
are	O
based	O
on	O
b	O
bootstrap	O
samples	O
,	O
and	O
they	O
are	O
hence	O
noisy	O
versions	O
of	O
their	O
expectations	O
.	O
since	O
the	O
n	O
quantities	O
summed	O
in	O
(	O
17.4	O
)	O
are	O
squared	O
,	O
by	O
jensen	O
’	O
s	O
inequal-	O
ity	O
we	O
will	O
have	O
positive	O
bias	O
(	O
and	O
it	O
turns	O
out	O
that	O
this	O
bias	O
dominates	O
the	O
monte	O
carlo	O
variance	O
)	O
.	O
hence	O
one	O
would	O
want	O
to	O
use	O
a	O
much	O
larger	O
value	O
of	O
b	O
when	O
estimating	O
variances	O
,	O
than	O
was	O
used	O
in	O
the	O
original	O
random-	O
forest	O
ﬁt	O
.	O
alternatively	O
,	O
one	O
can	O
use	O
the	O
same	O
b	O
bootstrap	O
samples	O
as	O
were	O
used	O
to	O
ﬁt	O
the	O
random	O
forest	O
,	O
along	O
with	O
a	O
bias-corrected	O
version	O
of	O
the	O
jackknife	O
variance	O
estimate	B
:	O
	O
bvu	O
jack.orrf.x0//	O
dbvjack.orrf.x0//	O
(	O
cid:0	O
)	O
.e	O
(	O
cid:0	O
)	O
1/	O
ov.x0/	O
;	O
n	O
b	O
(	O
17.5	O
)	O
2	O
17.1	O
random	O
forests	O
331	O
figure	O
17.4	O
jackknife	O
standard	O
error	O
estimates	O
(	O
with	O
bias	O
correction	O
)	O
for	O
the	O
probability	O
estimates	O
in	O
the	O
spam	B
test	O
data	B
.	O
the	O
points	O
labeled	O
red	O
were	O
misclassiﬁcations	O
,	O
and	O
tend	O
to	O
concentrate	O
near	O
the	O
decision	O
boundary	O
(	O
0.5	O
)	O
.	O
where	O
e	O
d	O
2:718	O
:	O
:	O
:	O
,	O
and	O
ov.x0/	O
d	O
1	O
b	O
bx	O
bd1	O
.orb.x0/	O
(	O
cid:0	O
)	O
orrf.x0//2	O
;	O
(	O
17.6	O
)	O
the	O
bootstrap	O
estimate	O
of	O
the	O
variance	O
of	O
a	O
single	O
random-forest	O
tree	O
.	O
all	O
these	O
quantities	O
are	O
easily	O
computed	O
from	O
the	O
output	O
of	O
a	O
random	O
forest	O
,	O
so	O
they	O
are	O
immediately	O
available	O
.	O
figure	O
17.4	O
shows	O
the	O
predicted	O
probabil-	O
ities	O
and	O
their	O
jackknife	O
estimated	O
standard	O
errors	O
for	O
the	O
spam	B
test	O
data	B
.	O
the	O
estimates	O
near	O
the	O
decision	O
boundary	O
tend	O
to	O
have	O
higher	O
standard	O
er-	O
rors	O
.	O
variable-importance	O
plots	O
a	O
random	O
forest	O
is	O
something	O
of	O
a	O
black	O
box	O
,	O
giving	O
good	O
predictions	O
but	O
usually	O
not	O
much	O
insight	O
into	O
the	O
underlying	O
surface	O
it	O
has	O
ﬁt	O
.	O
each	O
random-forest	O
tree	O
orb	O
will	O
have	O
used	O
a	O
subset	O
of	O
the	O
predictors	O
as	O
split-	O
ting	O
variables	O
,	O
and	O
each	O
tree	O
is	O
likely	O
to	O
use	O
overlapping	O
but	O
not	O
necessarily	O
lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.00.000.050.100.150.200.250.30probability	O
predictionstandard	O
error	O
estimate	B
332	O
random	O
forests	O
and	O
boosting	O
figure	O
17.5	O
variable-importance	O
plots	O
for	O
random	O
forests	O
ﬁt	O
to	O
the	O
spam	B
data	O
.	O
on	O
the	O
left	O
we	O
have	O
the	O
m	O
d	O
7	O
random	O
forest	O
;	O
due	O
to	O
the	O
split-variable	O
randomization	O
,	O
it	O
spreads	O
the	O
importance	O
among	O
the	O
variables	O
.	O
on	O
the	O
right	O
is	O
the	O
m	O
d	O
57	O
random	O
forest	O
or	O
bagging	O
,	O
which	O
focuses	O
on	O
a	O
smaller	O
subset	O
of	O
the	O
variables	O
.	O
identical	O
subsets	O
.	O
one	O
might	O
conclude	O
that	O
any	O
variable	O
never	O
used	O
in	O
any	O
of	O
the	O
trees	B
is	O
unlikely	O
to	O
be	O
important	O
,	O
but	O
we	O
would	O
like	O
a	O
method	B
of	O
assessing	O
the	O
relative	O
importance	O
of	O
variables	O
that	O
are	O
included	O
in	O
the	O
en-	O
semble	O
.	O
variable-importance	O
plots	O
ﬁt	O
this	O
bill	O
.	O
whenever	O
a	O
variable	O
is	O
used	O
in	O
a	O
tree	O
,	O
the	O
algorithm	B
logs	O
the	O
decrease	O
in	O
the	O
split-criterion	O
due	O
to	O
this	O
split	O
.	O
these	O
are	O
accumulated	O
over	O
all	O
the	O
trees	B
,	O
for	O
each	O
variable	O
,	O
and	O
sum-	O
marized	O
as	O
relative	O
importance	O
measures	O
.	O
figure	O
17.5	O
demonstrates	O
this	O
on	O
the	O
spam	B
data	O
.	O
we	O
see	O
that	O
the	O
m	O
d	O
7	O
random	O
forest	O
,	O
by	O
virtue	O
of	O
the	O
split-variable	O
randomization	O
,	O
spreads	O
the	O
importance	O
out	O
much	O
more	O
than	O
bagging	O
,	O
which	O
always	O
gets	O
to	O
pick	O
the	O
best	O
variable	O
for	O
splitting	O
.	O
in	O
this	O
sense	O
small	O
m	O
has	O
some	O
similarity	O
to	O
ridge	B
regression	I
,	O
which	O
also	O
tends	O
to	O
share	O
the	O
coefﬁcients	O
evenly	O
among	O
correlated	O
variables	O
.	O
!	O
$	O
removefreecapaveyourcapmaxhpcaptotmoneyyouourgeorge000edubusinesshpl1999internet	O
(	O
willallrereceiveemailmailover	O
;	O
meeting650addressorderlabspmpeoplecreditmake	O
#	O
technologydatafont85	O
[	O
labtelnetreportoriginalprojectconferenceaddressesdirect4158573dcspartstablerandom	O
forest	O
m	O
=	O
7020406080100variable	O
importance	O
$	O
!	O
removehpcapavefreecaptotcapmaxgeorgeyoueduyourourbusinessmoneywill	O
(	O
reemail000internetreceivemailmeeting1999overpm	O
;	O
dataorderfontpeopleallmakeconference650reportaddresshploriginaltechnologylab3dlabs	O
[	O
#	O
creditcs85telnetpartsproject857addressesdirect415tablebagging	O
m	O
=	O
57020406080100variable	O
importance	O
17.2	O
boosting	O
with	O
squared-error	O
loss	O
333	O
17.2	O
boosting	O
with	O
squared-error	O
loss	O
boosting	O
was	O
originally	O
proposed	O
as	O
a	O
means	O
for	O
improving	O
the	O
perfor-	O
mance	O
of	O
“	O
weak	O
learners	O
”	O
in	O
binary	O
classiﬁcation	O
problems	O
.	O
this	O
was	O
achiev-	O
ed	O
through	O
resampling	O
training	O
points—giving	O
more	O
weight	O
to	O
those	O
which	O
had	O
been	O
misclassiﬁed—to	O
produce	O
a	O
new	O
classiﬁer	O
that	O
would	O
boost	O
the	O
performance	O
in	O
previously	O
problematic	O
areas	O
of	O
feature	O
space	B
.	O
this	O
pro-	O
cess	O
is	O
repeated	O
,	O
generating	O
a	O
stream	O
of	O
classiﬁers	O
,	O
which	O
are	O
ultimately	O
combined	O
through	O
voting1	O
to	O
produce	O
the	O
ﬁnal	O
classiﬁer	O
.	O
the	O
prototypical	O
weak	O
learner	O
was	O
a	O
decision	O
tree	O
.	O
boosting	O
has	O
evolved	O
since	O
this	O
earliest	O
invention	O
,	O
and	O
different	O
ﬂavors	O
are	O
popular	O
in	O
statistics	B
,	O
computer	O
science	O
,	O
and	O
other	O
areas	O
of	O
pattern	O
recog-	O
nition	O
and	O
prediction	O
.	O
we	O
focus	O
on	O
the	O
version	O
popular	O
in	O
statistics—gradient	O
boosting—and	O
return	O
to	O
this	O
early	O
version	O
later	O
in	O
the	O
chapter	O
.	O
algorithm	B
17.2	O
algorithm	B
17.2	O
gradient	O
boosting	O
with	O
squared-error	O
loss	O
.	O
shrinkage	B
factor	O
(	O
cid:15	O
)	O
and	O
the	O
tree	O
depth	B
d.	O
set	B
the	O
initial	O
ﬁt	O
bg0	O
	O
0	O
,	O
and	O
1	O
given	O
a	O
training	O
sample	B
d	O
d	O
.x	O
;	O
y/	O
.	O
fix	O
the	O
number	O
of	O
steps	O
b	O
,	O
the	O
the	O
residual	O
vector	B
r	O
d	O
y	O
.	O
2	O
for	O
b	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
b	O
repeat	O
:	O
(	O
a	O
)	O
fit	O
a	O
regression	B
tree	O
qgb	O
to	O
the	O
data	B
.x	O
;	O
r/	O
,	O
grown	O
best-ﬁrst	B
to	O
depth	B
d	O
:	O
this	O
means	O
the	O
total	O
number	O
of	O
splits	O
are	O
d	O
,	O
and	O
each	O
successive	O
(	O
b	O
)	O
update	O
the	O
ﬁtted	O
model	B
with	O
a	O
shrunken	O
version	O
of	O
qgb	O
:	O
bgb	O
dbgb	O
(	O
cid:0	O
)	O
1	O
c	O
split	O
is	O
made	O
to	O
that	O
terminal	B
node	I
that	O
yields	O
the	O
biggest	O
reduction	O
in	O
residual	O
sum	O
of	O
squares	B
.	O
ogb	O
;	O
with	O
ogb	O
d	O
(	O
cid:15	O
)	O
(	O
cid:1	O
)	O
qgb	O
:	O
3	O
return	O
the	O
sequence	O
of	O
ﬁtted	O
functionsbgb	O
;	O
b	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
b	O
.	O
(	O
c	O
)	O
update	O
the	O
residuals	O
accordingly	O
:	O
ri	O
d	O
ri	O
(	O
cid:0	O
)	O
ogb.xi	O
/	O
;	O
i	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
n	O
:	O
gives	O
the	O
most	O
basic	O
version	O
of	O
gradient	O
boosting	O
,	O
for	O
squared-error	O
loss	O
.	O
this	O
amounts	O
to	O
building	O
a	O
model	B
by	O
repeatedly	O
ﬁtting	B
a	O
regression	B
tree	O
to	O
the	O
residuals	O
.	O
importantly	O
,	O
the	O
tree	O
is	O
typically	O
quite	O
small	O
,	O
involving	O
a	O
small	O
number	O
d	O
of	O
splits—it	O
is	O
indeed	O
a	O
weak	O
learner	O
.	O
after	O
each	O
tree	O
has	O
been	O
grown	O
to	O
the	O
residuals	O
,	O
it	O
is	O
shrunk	O
down	O
by	O
a	O
factor	B
(	O
cid:15	O
)	O
before	O
it	O
is	O
added	O
to	O
the	O
current	O
model	B
;	O
this	O
is	O
a	O
means	O
of	O
slowing	O
the	O
learning	O
process	O
.	O
despite	O
the	O
obvious	O
similarities	O
with	O
a	O
random	O
forest	O
,	O
boosting	O
is	O
different	O
in	O
a	O
fundamental	O
way	O
.	O
the	O
trees	B
in	O
a	O
random	O
forest	O
are	O
identically	O
1	O
each	O
classiﬁer	O
ocb.x0/	O
predicts	O
a	O
class	O
label	O
,	O
and	O
the	O
class	O
with	O
the	O
most	O
“	O
votes	O
”	O
wins	O
.	O
334	O
random	O
forests	O
and	O
boosting	O
figure	O
17.6	O
test	O
performance	O
of	O
a	O
boosted	O
regression-tree	O
model	B
ﬁt	O
to	O
the	O
als	O
training	O
data	B
,	O
with	O
n	O
d	O
1197	O
and	O
p	O
d	O
369.	O
shown	O
is	O
the	O
mean-squared	O
error	O
on	O
the	O
625	O
designated	O
test	O
observations	O
,	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
trees	O
.	O
here	O
the	O
depth	B
d	O
d	O
4	O
and	O
(	O
cid:15	O
)	O
d	O
0:02.	O
boosting	O
achieves	O
a	O
lower	O
test	O
mse	O
than	O
a	O
random	O
forest	O
.	O
we	O
see	O
that	O
as	O
the	O
number	O
of	O
trees	O
b	O
gets	O
large	O
,	O
the	O
test	O
error	O
for	O
boosting	O
starts	O
to	O
increase—a	O
consequence	O
of	O
overﬁtting	O
.	O
the	O
random	O
forest	O
does	O
not	O
overﬁt	O
.	O
the	O
dotted	O
blue	O
horizontal	O
line	O
shows	O
the	O
best	O
performance	O
of	O
a	O
linear	B
model	I
,	O
ﬁt	O
by	O
the	O
lasso	B
.	O
the	O
differences	O
are	O
less	O
dramatic	O
than	O
they	O
appear	O
,	O
since	O
the	O
vertical	O
scale	B
does	O
not	O
extend	O
to	O
zero	O
.	O
distributed—the	O
same	O
(	O
random	O
)	O
treatment	O
is	O
repeatedly	O
applied	O
to	O
the	O
same	O
data	B
.	O
with	O
boosting	O
,	O
on	O
the	O
other	O
hand	O
,	O
each	O
tree	O
is	O
trying	O
to	O
amend	O
errors	B
made	O
by	O
the	O
ensemble	O
of	O
previously	O
grown	O
trees	B
.	O
the	O
number	O
of	O
terms	O
b	O
is	O
important	O
as	O
well	O
,	O
because	O
unlike	O
random	O
forests	O
,	O
a	O
boosted	O
regression	B
model	I
can	O
overﬁt	O
if	O
b	O
is	O
too	O
large	O
.	O
hence	O
there	O
are	O
three	O
tuning	O
parame-	O
ters	O
,	O
b	O
,	O
d	O
and	O
(	O
cid:15	O
)	O
,	O
and	O
each	O
can	O
change	O
the	O
performance	O
of	O
a	O
boosted	O
model	B
,	O
sometimes	O
considerably	O
.	O
figure	O
17.6	O
shows	O
the	O
test	O
performance	O
of	O
boosting	O
on	O
the	O
als	O
data	B
.	O
	O
these	O
data	B
represent	O
measurements	O
on	O
patients	O
with	O
amyotrophic	O
lateral	O
sclerosis	O
(	O
lou	O
gehrig	O
’	O
s	O
disease	O
)	O
.	O
the	O
goal	O
is	O
to	O
predict	O
the	O
rate	B
of	O
pro-	O
gression	O
of	O
an	O
als	O
functional	O
rating	O
score	O
(	O
frs	O
)	O
.	O
there	O
are	O
1197	O
training	O
3	O
0.250.260.270.280.290.30number	O
of	O
treesmean−squared	O
error1100200300400500boostrandom	O
forestlasso	O
17.2	O
boosting	O
with	O
squared-error	O
loss	O
335	O
measurements	O
on	O
369	O
predictors	O
and	O
the	O
response	O
,	O
with	O
a	O
corresponding	O
test	O
set	B
of	O
size	O
625	O
observations	O
.	O
as	O
is	O
often	O
the	O
case	O
,	O
boosting	O
slightly	O
outperforms	O
a	O
random	O
forest	O
here	O
,	O
but	O
at	O
a	O
price	O
.	O
careful	O
tuning	O
of	O
boosting	O
requires	O
considerable	O
extra	O
work	O
,	O
with	O
time-costly	O
rounds	O
of	O
cross-validation	O
,	O
whereas	O
random	O
forests	O
are	O
al-	O
most	O
automatic	O
.	O
in	O
the	O
following	O
sections	O
we	O
explore	O
in	O
more	O
detail	O
some	O
of	O
the	O
tuning	O
parameters	O
.	O
the	O
r	O
package	O
gbm	B
implements	O
gradient	O
boost-	O
ing	O
,	O
with	O
some	O
added	O
bells	O
and	O
whistles	O
.	O
by	O
default	O
it	O
grows	O
each	O
new	O
tree	O
on	O
a	O
50	O
%	O
random	O
sub-sample	O
of	O
the	O
training	O
data	B
.	O
apart	O
from	O
speeding	O
up	O
the	O
computations	B
,	O
this	O
has	O
a	O
similar	O
effect	O
to	O
bagging	O
,	O
and	O
results	O
in	O
some	O
variance	O
reduction	O
in	O
the	O
ensemble	O
.	O
we	O
can	O
also	O
compute	O
a	O
variable-importance	O
plot	O
,	O
as	O
we	O
did	O
for	O
random	O
forests	O
;	O
this	O
is	O
displayed	O
in	O
figure	O
17.7	O
for	O
the	O
als	O
data	B
.	O
only	O
267	O
of	O
the	O
369	O
variables	O
were	O
ever	O
used	O
,	O
with	O
one	O
variable	O
onset.delta	O
standing	O
out	O
ahead	O
of	O
the	O
others	O
.	O
this	O
measures	O
the	O
amount	O
of	O
time	O
that	O
has	O
elapsed	O
since	O
the	O
patient	O
was	O
ﬁrst	O
diagnosed	O
with	O
als	O
,	O
and	O
hence	O
a	O
larger	O
value	O
will	O
indicate	O
a	O
slower	O
progression	O
rate	B
.	O
tree	O
depth	B
and	O
interaction	O
order	O
tree	O
depth	B
d	O
is	O
an	O
important	O
parameter	O
for	O
gradient	O
boosted	O
models	B
,	O
and	O
the	O
right	O
choice	O
will	O
depend	O
on	O
the	O
data	B
at	O
hand	O
.	O
here	O
depth	B
d	O
d	O
4	O
appears	O
to	O
be	O
a	O
good	O
choice	O
on	O
the	O
test	O
data	B
.	O
without	O
test	O
data	B
,	O
we	O
could	O
use	O
cross-	O
validation	O
to	O
make	O
the	O
selection	O
.	O
apart	O
from	O
a	O
general	O
complexity	O
measure	O
,	O
suppose	O
we	O
have	O
a	O
ﬁtted	O
boosted	O
modelbgb	O
.x/	O
,	O
using	O
b	O
trees	B
.	O
denote	O
by	O
tree	O
depth	B
also	O
controls	O
the	O
interaction	O
order	O
of	O
the	O
model.2	O
the	O
easiest	O
case	O
is	O
with	O
d	O
d	O
1	O
,	O
where	O
each	O
tree	O
consists	O
of	O
a	O
single	O
split	O
(	O
a	O
stump	O
)	O
.	O
d	O
f1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
bg	O
the	O
indices	O
of	O
the	O
trees	B
that	O
made	O
the	O
single	O
split	O
bj	O
	O
using	O
variable	O
j	O
,	O
for	O
j	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
p.	O
these	O
bj	O
are	O
disjoint	O
(	O
some	O
b`	O
can	O
be	O
b	O
2	O
a	O
.k	O
(	O
cid:0	O
)	O
1/th-order	O
interaction	O
is	O
also	O
known	O
as	O
a	O
k-way	O
interaction	O
.	O
hence	O
an	O
order-one	O
interaction	O
model	B
has	O
two-way	O
interactions	O
,	O
and	O
an	O
order-zero	O
model	B
is	O
additive	O
.	O
336	O
random	O
forests	O
and	O
boosting	O
figure	O
17.7	O
variable	O
importance	O
plot	O
for	O
the	O
als	O
data	B
.	O
here	O
267	O
of	O
the	O
369	O
variables	O
were	O
used	O
in	O
the	O
ensemble	O
.	O
there	O
are	O
too	O
many	O
variables	O
for	O
the	O
labels	O
to	O
be	O
visible	O
,	O
so	O
this	O
plot	O
serves	O
as	O
a	O
visual	O
guide	O
.	O
variable	O
onset.delta	O
has	O
relative	O
importance	O
100	O
(	O
the	O
lowest	O
red	O
bar	O
)	O
,	O
more	O
than	O
double	O
the	O
next	O
two	O
at	O
around	O
40	O
(	O
last.slope.weight	O
and	O
alsfrs.score.slope	O
)	O
.	O
however	O
,	O
the	O
importances	O
drop	O
off	O
slowly	O
,	O
suggesting	O
that	O
the	O
model	B
requires	O
a	O
signiﬁcant	O
fraction	O
of	O
the	O
variables	O
.	O
empty	O
)	O
,	O
andsp	O
jd1	O
bj	O
d	O
bd1	O
b.	O
then	O
we	O
can	O
write	O
ogb.x/	O
x	O
bgb	O
.x/	O
d	O
bx	O
d	O
px	O
d	O
px	O
jd1	O
ogb.x/	O
bj	O
b2	O
o	O
fj	O
.xj	O
/	O
:	O
jd1	O
(	O
17.7	O
)	O
onset.deltalast.slope.weightalsfrs.score.slopelast.slope.bp.systolicmean.slope.svc.literssum.slope.alsfrs.scoremean.slope.weightlast.slope.fvc.litersmeansquares.alsfrs.scoresum.slope.fvc.literslast.alsfrs.scorefvc.liters.slopesum.bp.diastolicweight.slopemean.speechmax.slope.fvc.litersmean.slope.handwritingslope.bp.systolic.slopemean.slope.fvc.litersmin.slope.alsfrs.scoresd.fvc.litersmeansquares.climbing.stairssd.salivationlast.speechmin.slope.bp.systolicmean.alsfrs.scoremax.slope.salivationsum.slope.climbing.stairslast.fvc.literssum.fvc.litersmean.slope.alsfrs.scorebp.diastolic.slopelast.slope.handwritingmin.slope.fvc.litersspeech.slopesd.slope.alsfrs.scoremeansquares.slope.weightslope.handwriting.slopeslope.weight.slopesum.alsfrs.scoreslope.turning.slopelast.slope.swallowingsum.slope.speechsum.weightmax.fvc.literslast.svc.liters.datemax.bp.diastolicmin.alsfrs.scoremean.climbing.stairsmax.dressingmeansquares.bp.diastoliclast.weightsum.slope.weightmeansquares.resp.rateslope.fvc.liters.slopemean.slope.climbing.stairsswallowing.slopesum.slope.handwritinglast.slope.alsfrs.score.datemeansquares.dressingsd.bp.diastolicclimbing.stairs.slopeslope.alsfrs.score.slopelast.slope.svc.liters.datesum.handwritingmean.slope.speechsum.slope.dressingsd.bp.systolicmin.slope.cuttingmax.weightlast.slope.alsfrs.scoremean.fvc.litersslope.bp.diastolic.slopeslope.salivation.slopemeansquares.slope.dressingmin.slope.speechsd.slope.fvc.litersmax.alsfrs.scoresum.slope.bp.systolicmeansquares.slope.bp.diastolicsd.slope.bp.systolicslope.dressing.slopemin.slope.dressinglast.slope.climbing.stairsmax.slope.alsfrs.scoremax.slope.bp.systolicsd.dressingsum.climbing.stairssd.slope.weightslope.resp.rate.slopesd.resp.ratemean.slope.bp.systoliclast.slope.weight.datemax.heightlast.slope.svc.litersmin.bp.systolicsd.slope.turningsum.slope.salivationfirst.slope.fvc.liters.datelast.bp.diastolicsd.slope.svc.literssd.slope.dressingmeansquares.slope.fvc.litersmeansquares.speechmeansquares.fvc.literslast.alsfrs.score.datesd.slope.resp.ratemeansquares.slope.cuttingmin.slope.turningmin.fvc.litersslope.swallowing.slopelast.slope.bp.diastolicmax.slope.weightsd.slope.swallowinglast.slope.resp.rate.datemean.turningmean.bp.diastolicsum.dressingmeansquares.cuttingsum.resp.ratemeansquares.slope.turningbp.systolic.slopemean.slope.dressingsd.cuttingmean.slope.bp.diastolicsum.heightsalivation.slopemin.slope.weightfirst.slope.weight.datesum.slope.resp.ratemax.slope.swallowinglast.swallowingmean.slope.cuttingmax.slope.svc.litersmean.slope.turningmax.cuttinglast.turningmean.slope.resp.ratesum.bp.systolicmeansquares.slope.climbing.stairsfirst.slope.resp.rate.datesd.slope.salivationslope.cutting.slopelast.resp.rate.datesd.alsfrs.scoresd.slope.cuttingsite.of.onset.onset..bulbarfirst.slope.svc.liters.datemin.turningsd.slope.bp.diastolicmin.cuttingmax.slope.resp.rateagesum.slope.swallowingsum.swallowingsd.speechlast.dressingfirst.slope.alsfrs.score.datemean.slope.salivationmeansquares.handwritingmin.weightlast.bp.systolicmean.salivationmeansquares.slope.alsfrs.scoremin.slope.climbing.stairsmean.slope.swallowinglast.slope.fvc.liters.datesum.salivationmin.slope.salivationmean.dressingslope.climbing.stairs.slopemeansquares.slope.walkingmeansquares.slope.bp.systolicresp.rate.slopelast.slope.dressinglast.slope.resp.ratemax.slope.dressingsex.femalemin.swallowingmax.bp.systolicmin.bp.diastolicmeansquares.slope.handwritingmin.slope.bp.diastolicfirst.slope.bp.diastolic.datemean.handwritinglast.height.datesd.svc.literssd.slope.handwritingmean.slope.walkingmean.cuttingsum.slope.bp.diastoliclast.slope.salivationmin.slope.swallowinglast.fvc.liters.datewalking.slopesum.cuttingmean.swallowingsd.swallowingsum.speechmax.slope.climbing.stairshandwriting.slopemean.svc.litersmax.speechmeansquares.salivationmeansquares.slope.svc.literslast.slope.bp.diastolic.datefirst.resp.rate.datemeansquares.slope.salivationmean.resp.ratemax.slope.bp.diastolicmin.speechslope.speech.slopemean.weightsex.malemax.svc.literssum.turningmax.slope.turninglast.svc.litersmax.swallowingmeansquares.turningmax.handwritingmeansquares.swallowingsd.slope.climbing.stairsmin.climbing.stairssvc.liters.slopelast.cuttingmeansquares.bp.systoliclast.slope.turningmax.salivationlast.weight.datemeansquares.slope.resp.ratemeansquares.weightmeansquares.slope.speechmeansquares.slope.swallowingmax.walkinglast.handwritingnum.slope.weight.visitssum.slope.cuttinglast.resp.ratesd.weightsd.climbing.stairssum.slope.turningsum.walkingslope.walking.slopeno.height.datafirst.alsfrs.score.datemin.salivationmin.slope.resp.ratefirst.slope.bp.systolic.datelast.slope.walkingstudy.arm.placebofirst.slope.height.datemin.slope.handwritinglast.climbing.stairssd.handwritingsymptom.weaknessmax.slope.handwritinglast.slope.cuttingmax.turningmin.slope.svc.litersdressing.slopesd.slope.speechmothernum.slope.resp.rate.visitssymptom.atrophysymptom.swallowingcutting.slopenum.slope.bp.systolic.visitslessthan2.slope.bp.diastolicnum.slope.bp.diastolic.visitsno.slope.resp.rate.datalessthan2.slope.resp.rateno.slope.weight.datalessthan2.slope.weightslope.svc.liters.slopeno.slope.fvc.liters.datalessthan2.slope.fvc.litersnum.slope.fvc.liters.visitssd.slope.walkingsum.slope.walkingmin.slope.walkingmax.slope.walkingmax.slope.cuttinglast.slope.speechmax.slope.speechlessthan2.slope.alsfrs.scorenum.slope.alsfrs.score.visitsnum.bp.systolic.visitsmean.bp.systolicno.bp.diastolic.datalessthan2.bp.diastoliclast.bp.diastolic.datefirst.bp.diastolic.datenum.bp.diastolic.visitsno.resp.rate.datalessthan2.resp.ratenum.resp.rate.visitsmin.resp.ratemax.resp.ratesd.heightmeansquares.heightfirst.height.datenum.height.visitsmean.heightmin.heightlessthan2.weightfirst.weight.datenum.weight.visitsmeansquares.svc.literssum.svc.litersnum.svc.liters.visitsmin.svc.litersno.fvc.liters.datalessthan2.fvc.litersfirst.fvc.liters.datenum.fvc.liters.visitsmax.climbing.stairssd.walkingmeansquares.walkingmean.walkinglast.walkingmin.walkingturning.slopesd.turningmin.dressingmin.handwritinglast.salivationno.alsfrs.score.datalessthan2.alsfrs.scorenum.alsfrs.score.visitsstudy.arm.activeneurological.disease.stroke.hemorrhagicneurological.disease.stroke.ischemicneurological.disease.brain.tumorneurological.disease.alsneurological.disease.datneurological.disease.parkinson.s.diseaseneurological.disease.dementia.nosneurological.disease.stroke.nosneurological.disease.otherfamilybrothersisterdaughtersonuncle..paternal.uncle..maternal.unclegrandmother..maternal.grandmothergrandfather..maternal.fathercousinaunt..maternal.auntrace	O
...	O
otherrace	O
...	O
caucasianrace	O
...	O
black.african.americanrace	O
...	O
asiansite.of.onset.onset..limb.and.bulbarsite.of.onset.onset..limbsymptom..symptom.stiffnesssymptom.sensory_changessymptom.fasciculationssymptom.crampssymptom.gait_changessymptom.othersymptom.speech020406080100variable−importance	O
plot	O
for	O
boosting	O
on	O
the	O
als	O
data	B
17.2	O
boosting	O
with	O
squared-error	O
loss	O
337	O
figure	O
17.8	O
als	O
test	O
error	O
for	O
boosted	O
models	B
with	O
different	O
depth	B
parameters	O
d	O
,	O
and	O
all	O
using	O
the	O
same	O
shrinkage	B
parameter	O
(	O
cid:15	O
)	O
d	O
0:02.	O
it	O
appears	O
that	O
d	O
d	O
1	O
is	O
inferior	O
to	O
the	O
rest	O
,	O
with	O
d	O
d	O
4	O
about	O
the	O
best	O
.	O
with	O
d	O
d	O
7	O
,	O
overﬁtting	O
begins	O
around	O
200	O
trees	B
,	O
with	O
d	O
d	O
4	O
around	O
300	O
,	O
while	O
neither	O
of	O
the	O
other	O
two	O
show	O
evidence	O
of	O
overﬁtting	O
by	O
500	O
trees	B
.	O
hence	O
boosted	O
stumps	O
ﬁts	O
an	O
additive	O
model	B
,	O
but	O
in	O
a	O
fully	O
adaptive	B
way	O
.	O
it	O
selects	O
variables	O
,	O
and	O
also	O
selects	O
how	O
much	O
action	O
to	O
devote	O
to	O
each	O
variable	O
.	O
we	O
return	O
to	O
additive	O
models	B
in	O
section	O
17.5.	O
figure	O
17.9	O
shows	O
the	O
three	O
functions	O
with	O
highest	O
relative	O
importance	O
.	O
the	O
ﬁrst	O
function	B
con-	O
ﬁrms	O
that	O
a	O
longer	O
time	O
since	O
diagnosis	O
(	O
more	O
negative	O
onset.delta	O
)	O
predicts	O
a	O
slower	O
decline	O
.	O
last.slope.weight	O
is	O
the	O
difference	O
in	O
body	O
weight	O
at	O
the	O
last	O
two	O
visits—again	O
positive	O
is	O
good	O
.	O
likewise	O
for	O
alsfrs.score.slope	O
,	O
which	O
measures	O
the	O
local	O
slope	O
of	O
the	O
frs	O
score	O
after	O
the	O
ﬁrst	O
two	O
visits	O
.	O
in	O
a	O
similar	O
way	O
,	O
boosting	O
with	O
d	O
d	O
2	O
ﬁts	O
a	O
two-way	O
interaction	O
model	B
;	O
each	O
tree	O
involves	O
at	O
most	O
two	O
variables	O
.	O
in	O
general	O
,	O
boosting	O
with	O
d	O
d	O
k	O
leads	O
to	O
a	O
.k	O
(	O
cid:0	O
)	O
1/th-order	O
interaction	O
model	B
.	O
interaction	O
order	O
is	O
perhaps	O
a	O
more	O
natural	O
way	O
to	O
think	O
of	O
model	B
complexity	O
.	O
0.250.260.270.280.290.30number	O
of	O
treesmean−squared	O
error1100200300400500depth1247	O
338	O
random	O
forests	O
and	O
boosting	O
figure	O
17.9	O
three	O
of	O
the	O
ﬁtted	O
functions	O
(	O
17.7	O
)	O
for	O
the	O
als	O
data	B
,	O
in	O
a	O
boosted	O
stumps	O
model	B
(	O
d	O
d	O
1	O
)	O
,	O
each	O
centered	O
to	O
average	O
zero	O
over	O
the	O
training	O
data	B
.	O
in	O
terms	O
of	O
the	O
outcome	O
,	O
bigger	O
is	O
better	O
(	O
slower	O
decline	O
in	O
frs	O
)	O
.	O
the	O
ﬁrst	O
function	B
conﬁrms	O
that	O
a	O
longer	O
time	O
since	O
diagnosis	O
(	O
more	O
negative	O
value	O
of	O
onset.delta	O
)	O
predicts	O
a	O
slower	O
decline	O
.	O
the	O
variable	O
last.slope.weight	O
is	O
the	O
difference	O
in	O
body	O
weight	O
at	O
the	O
last	O
two	O
visits—again	O
positive	O
is	O
good	O
.	O
likewise	O
for	O
alsfrs.score.slope	O
,	O
which	O
measures	O
the	O
local	O
slope	O
of	O
the	O
frs	O
score	O
after	O
the	O
ﬁrst	O
two	O
visits	O
.	O
shrinkage	B
the	O
shrinkage	B
parameter	O
(	O
cid:15	O
)	O
controls	O
the	O
rate	B
at	O
which	O
boosting	O
ﬁts—and	O
hence	O
overﬁts—the	O
data	B
.	O
figure	O
17.10	O
demonstrates	O
the	O
effect	O
of	O
shrink-	O
age	O
on	O
the	O
als	O
data	B
.	O
the	O
under-shrunk	O
ensemble	O
(	O
red	O
)	O
quickly	O
overﬁts	O
the	O
data	B
,	O
leading	O
to	O
poor	O
validation	O
error	O
.	O
the	O
blue	O
ensemble	O
uses	O
a	O
shrink-	O
age	O
parameter	O
20	O
times	O
smaller	O
,	O
and	O
reaches	O
a	O
lower	O
validation	O
error	O
.	O
the	O
downside	O
of	O
a	O
very	O
small	O
shrinkage	B
parameter	O
is	O
that	O
it	O
can	O
take	O
many	O
trees	B
to	O
adequately	O
ﬁt	O
the	O
data	B
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
shrunken	O
ﬁts	O
are	O
smoother	O
,	O
take	O
much	O
longer	O
to	O
overﬁt	O
,	O
and	O
hence	O
are	O
less	O
sensitive	O
to	O
the	O
stopping	O
point	O
b	O
.	O
17.3	O
gradient	O
boosting	O
we	O
now	O
turn	O
our	O
attention	O
to	O
boosting	O
models	B
using	O
other	O
than	O
square-error	O
loss	O
.	O
we	O
focus	O
on	O
the	O
family	O
of	O
generalized	O
models	B
generated	O
by	O
the	O
expo-	O
nential	O
family	O
of	O
response	O
distributions	O
(	O
see	O
chapter	O
8	O
)	O
.	O
the	O
most	O
popular	O
and	O
relevant	O
in	O
this	O
class	O
is	O
logistic	B
regression	I
,	O
where	O
we	O
are	O
interested	O
in	O
modeling	O
(	O
cid:22	O
)	O
.x/	O
d	O
pr.y	O
d	O
1jx	O
d	O
x/	O
for	O
a	O
bernoulli	O
response	O
variable	O
.	O
−2000−10000−0.3−0.2−0.10.00.10.2onset.deltafitted	O
function−20−1001020−0.3−0.2−0.10.00.10.2last.slope.weightfitted	O
function−10−6−4−2024−0.3−0.2−0.10.00.10.2alsfrs.score.slopefitted	O
function	B
17.3	O
gradient	O
boosting	O
339	O
figure	O
17.10	O
boosted	O
d	O
d	O
3	O
models	B
with	O
different	O
shrinkage	B
parameters	O
,	O
ﬁt	O
to	O
a	O
subset	O
of	O
the	O
als	O
data	B
.	O
the	O
solid	O
curves	O
are	O
validation	O
errors	B
,	O
the	O
dashed	O
curves	O
training	O
errors	B
,	O
with	O
red	O
for	O
(	O
cid:15	O
)	O
d	O
0:5	O
and	O
blue	O
for	O
(	O
cid:15	O
)	O
d	O
0:02.	O
with	O
(	O
cid:15	O
)	O
d	O
0:5	O
,	O
the	O
training	O
error	O
drops	O
rapidly	O
with	O
the	O
number	O
of	O
trees	O
,	O
but	O
the	O
validation	O
error	O
starts	O
to	O
increase	O
rapidly	O
after	O
an	O
initial	O
decrease	O
.	O
with	O
(	O
cid:15	O
)	O
d	O
0:02	O
(	O
25	O
times	O
smaller	O
)	O
,	O
the	O
training	O
error	O
drops	O
more	O
slowly	O
.	O
the	O
validation	O
error	O
also	O
drops	O
more	O
slowly	O
,	O
but	O
reaches	O
a	O
lower	O
minimum	O
(	O
the	O
horizontal	O
dotted	O
line	O
)	O
than	O
the	O
(	O
cid:15	O
)	O
d	O
0:5	O
case	O
.	O
in	O
this	O
case	O
,	O
the	O
slower	O
learning	O
has	O
paid	O
off	O
.	O
the	O
idea	O
is	O
to	O
ﬁt	O
a	O
model	B
of	O
the	O
form	B
(	O
cid:21	O
)	O
.x/	O
d	O
gb	O
.x/	O
d	O
bx	O
bd1	O
gb.xi	O
(	O
cid:13	O
)	O
b/	O
;	O
(	O
17.8	O
)	O
where	O
(	O
cid:21	O
)	O
.x/	O
is	O
the	O
natural	O
parameter	O
in	O
the	O
conditional	O
distribution	B
of	O
y	O
jx	O
d	O
x	O
,	O
and	O
the	O
gb.xi	O
(	O
cid:13	O
)	O
b/	O
are	O
simple	O
functions	O
such	O
as	O
shallow	O
trees	B
.	O
here	O
we	O
have	O
indexed	O
each	O
function	B
by	O
a	O
parameter	O
vector	B
(	O
cid:13	O
)	O
b	O
;	O
for	O
trees	B
these	O
would	O
capture	O
the	O
identity	O
of	O
the	O
split	O
variables	O
,	O
their	O
split	O
values	O
,	O
and	O
the	O
con-	O
stants	O
in	O
the	O
terminal	O
nodes	O
.	O
in	O
the	O
case	O
of	O
the	O
bernoulli	O
response	O
,	O
we	O
have	O
pr.y	O
d	O
1jx	O
d	O
x/	O
pr.y	O
d	O
0jx	O
d	O
x/	O
	O
(	O
cid:21	O
)	O
.x/	O
d	O
log	O
;	O
(	O
17.9	O
)	O
0.00.10.20.30.4number	O
of	O
treesmean−squared	O
error150100150200250ǫ=0.02ǫ=0.50trainvalidate	O
340	O
random	O
forests	O
and	O
boosting	O
the	O
logit	O
link	O
function	B
that	O
relates	O
the	O
mean	O
to	O
the	O
natural	O
parameter	O
.	O
in	O
gen-	O
eral	O
,	O
if	O
(	O
cid:22	O
)	O
.x/	O
d	O
e.y	O
jx	O
d	O
x/	O
is	O
the	O
conditional	O
mean	O
,	O
we	O
have	O
œ	O
(	O
cid:22	O
)	O
.x/	O
d	O
(	O
cid:21	O
)	O
.x/	O
,	O
where	O
	O
is	O
the	O
monotone	O
link	O
function	B
.	O
algorithm	B
17.3	O
outlines	O
a	O
general	O
strategy	O
for	O
building	O
a	O
model	B
by	O
for-	O
ward	O
stagewise	O
ﬁtting	B
.	O
l	O
is	O
the	O
loss	B
function	I
,	O
such	O
as	O
the	O
negative	O
log-	O
likelihood	B
for	O
bernoulli	O
responses	O
,	O
or	O
squared-error	O
for	O
gaussian	O
responses	O
.	O
although	O
we	O
are	O
thinking	O
of	O
trees	B
for	O
the	O
simple	O
functions	O
g.xi	O
(	O
cid:13	O
)	O
/	O
,	O
the	O
ideas	O
generalize	O
.	O
this	O
algorithm	B
is	O
easier	O
to	O
state	O
than	O
to	O
implement	O
.	O
for	O
algorithm	B
17.3	O
generalized	O
boosting	O
by	O
forward-stagewise	O
fitting	O
1	O
deﬁne	O
the	O
class	O
of	O
functions	O
g.xi	O
(	O
cid:13	O
)	O
/	O
.	O
start	O
with	O
og0.x/	O
d	O
0	O
,	O
and	O
set	O
b	O
and	O
the	O
shrinkage	B
parameter	O
(	O
cid:15	O
)	O
>	O
0	O
.	O
2	O
for	O
b	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
b	O
repeat	O
the	O
following	O
steps	O
.	O
(	O
a	O
)	O
solve	O
nx	O
(	O
cid:16	O
)	O
yi	O
;	O
bgb	O
(	O
cid:0	O
)	O
1.xi	O
/	O
c	O
g.xii	O
(	O
cid:13	O
)	O
/	O
	O
l	O
(	O
b	O
)	O
updatebgb.x/	O
dbgb	O
(	O
cid:0	O
)	O
1.x/	O
c	O
ogb.x/	O
;	O
with	O
ogb.x/	O
d	O
(	O
cid:15	O
)	O
(	O
cid:1	O
)	O
g.xi	O
o	O
(	O
cid:13	O
)	O
b/	O
.	O
3	O
return	O
the	O
sequencebgb.x/	O
;	O
b	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
b.	O
id1	O
(	O
cid:13	O
)	O
o	O
(	O
cid:13	O
)	O
b	O
d	O
arg	O
min	O
squared-error	O
loss	O
,	O
at	O
each	O
step	O
we	O
need	O
to	O
solve	O
nx	O
minimize	O
.ri	O
(	O
cid:0	O
)	O
g.xii	O
(	O
cid:13	O
)	O
//2	O
;	O
(	O
17.10	O
)	O
(	O
cid:13	O
)	O
id1	O
with	O
ri	O
d	O
yi	O
(	O
cid:0	O
)	O
bgb	O
(	O
cid:0	O
)	O
1.xi	O
/	O
;	O
i	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
n.	O
if	O
g.	O
(	O
cid:1	O
)	O
i	O
(	O
cid:13	O
)	O
/	O
represents	O
a	O
depth-d	O
tree	O
,	O
(	O
17.10	O
)	O
is	O
still	O
difﬁcult	O
to	O
solve	O
.	O
but	O
here	O
we	O
can	O
resort	O
to	O
the	O
usual	O
greedy	O
heuristic	O
,	O
and	O
grow	O
a	O
depth-d	O
tree	O
to	O
the	O
residuals	O
by	O
the	O
usual	O
top-down	O
splitting	O
,	O
as	O
in	O
step	O
2	O
(	O
a	O
)	O
of	O
algorithm	B
17.2.	O
hence	O
in	O
this	O
case	O
,	O
we	O
have	O
exactly	O
the	O
squared-error	O
boosting	O
algorithm	B
17.2.	O
for	O
more	O
general	O
loss	O
functions	O
,	O
we	O
rely	O
on	O
one	O
more	O
heuristic	O
for	O
solving	O
step	O
2	O
(	O
a	O
)	O
,	O
inspired	O
by	O
gradient	O
descent	O
.	O
algorithm	B
17.4	O
gives	O
the	O
details	O
.	O
the	O
idea	O
is	O
to	O
perform	O
functional	O
gradient	O
descent	O
on	O
the	O
loss	B
function	I
,	O
in	O
the	O
n-dimensional	O
space	B
of	O
the	O
ﬁtted	O
vector	B
.	O
however	O
,	O
we	O
want	O
to	O
be	O
able	O
to	O
evaluate	O
our	O
new	O
func-	O
tion	O
everywhere	O
,	O
not	O
just	O
at	O
the	O
n	O
original	O
values	O
xi	O
.	O
hence	O
once	O
the	O
(	O
neg-	O
ative	O
)	O
gradient	O
vector	B
has	O
been	O
computed	O
,	O
it	O
is	O
approximated	O
by	O
a	O
depth-d	O
tree	O
(	O
which	O
can	O
be	O
evaluated	O
everywhere	O
)	O
.	O
taking	O
a	O
step	O
of	O
length	O
(	O
cid:15	O
)	O
down	O
17.4	O
adaboost	O
:	O
the	O
original	O
boosting	O
algorithm	B
341	O
the	O
gradient	O
amounts	O
to	O
adding	O
(	O
cid:15	O
)	O
times	O
the	O
tree	O
to	O
the	O
current	O
function	B
.	O
	O
gradient	O
boosting	O
is	O
quite	O
general	O
,	O
and	O
can	O
be	O
used	O
with	O
any	O
differentiable	O
4	O
algorithm	B
17.4	O
gradient	O
boosting	O
1	O
start	O
with	O
og0.x/	O
d	O
0	O
,	O
and	O
set	O
b	O
and	O
the	O
shrinkage	B
parameter	O
(	O
cid:15	O
)	O
>	O
0	O
.	O
2	O
for	O
b	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
b	O
repeat	O
the	O
following	O
steps	O
.	O
(	O
a	O
)	O
compute	O
the	O
pointwise	O
negative	O
gradient	O
of	O
the	O
loss	B
function	I
at	O
the	O
current	O
ﬁt	O
:	O
ˇˇˇˇ	O
(	O
cid:21	O
)	O
idbgb	O
(	O
cid:0	O
)	O
1.xi	O
/	O
ri	O
d	O
(	O
cid:0	O
)	O
@	O
l.yi	O
;	O
(	O
cid:21	O
)	O
i	O
/	O
nx	O
@	O
(	O
cid:21	O
)	O
i	O
minimize	O
(	O
cid:13	O
)	O
.ri	O
(	O
cid:0	O
)	O
g.xii	O
(	O
cid:13	O
)	O
//2	O
:	O
id1	O
;	O
i	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
n	O
:	O
(	O
b	O
)	O
approximate	O
the	O
negative	O
gradient	O
by	O
a	O
depth-d	O
tree	O
by	O
solving	O
(	O
c	O
)	O
update	O
ogb.x/	O
d	O
ogb	O
(	O
cid:0	O
)	O
1.x/	O
c	O
ogb.x/	O
;	O
with	O
ogb.x/	O
d	O
(	O
cid:15	O
)	O
(	O
cid:1	O
)	O
g.xi	O
o	O
(	O
cid:13	O
)	O
b/	O
.	O
3	O
return	O
the	O
sequence	O
ogb.x/	O
;	O
b	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
b.	O
loss	B
function	I
.	O
the	O
r	O
package	O
gbm	B
implements	O
algorithm	B
17.4	O
for	O
a	O
variety	O
of	O
loss	O
functions	O
,	O
including	O
squared-error	O
,	O
binomial	B
(	O
bernoulli	O
)	O
,	O
laplace	O
(	O
`1	O
loss	O
)	O
,	O
multinomial	O
,	O
and	O
others	O
.	O
included	O
as	O
well	O
is	O
the	O
partial	O
likelihood	B
for	O
the	O
cox	O
proportional	O
hazards	O
model	B
(	O
chapter	O
9	O
)	O
.	O
figure	O
17.11	O
com-	O
pares	O
the	O
misclassiﬁcation	O
error	O
of	O
boosting	O
on	O
the	O
spam	B
data	O
,	O
with	O
that	O
of	O
random	O
forests	O
and	O
bagging	O
.	O
since	O
boosting	O
has	O
more	O
tuning	O
parameters	O
,	O
a	O
careful	O
comparison	O
must	O
take	O
these	O
into	O
account	O
.	O
using	O
the	O
mcnemar	O
test	O
we	O
would	O
conclude	O
that	O
boosting	O
and	O
random	O
forest	O
are	O
not	O
signiﬁcantly	O
different	O
from	O
each	O
other	O
,	O
but	O
both	O
outperform	O
bagging	O
.	O
17.4	O
adaboost	O
:	O
the	O
original	O
boosting	O
algorithm	B
the	O
original	O
proposal	O
for	O
boosting	O
looked	O
quite	O
different	O
from	O
what	O
we	O
have	O
presented	O
so	O
far	O
.	O
adaboost	O
was	O
developed	O
for	O
the	O
two-class	O
classiﬁ-	O
cation	O
problem	O
,	O
where	O
the	O
response	O
is	O
coded	O
as	O
-1/1	O
.	O
the	O
idea	O
was	O
to	O
ﬁt	O
a	O
sequence	O
of	O
classiﬁers	O
to	O
modiﬁed	O
versions	O
of	O
the	O
training	O
data	B
,	O
where	O
the	O
modiﬁcations	O
give	O
more	O
weight	O
to	O
misclassiﬁed	O
points	O
.	O
the	O
ﬁnal	O
classiﬁ-	O
cation	O
is	O
by	O
weighted	O
majority	O
vote	O
.	O
the	O
details	O
are	O
rather	O
speciﬁc	O
,	O
and	O
are	O
given	O
in	O
algorithm	B
17.5.	O
here	O
we	O
distinguish	O
a	O
classiﬁer	O
c.x/	O
2	O
f	O
(	O
cid:0	O
)	O
1	O
;	O
1g	O
,	O
which	O
returns	O
a	O
class	O
label	O
,	O
rather	O
than	O
a	O
probability	O
.	O
algorithm	B
17.5	O
gives	O
342	O
random	O
forests	O
and	O
boosting	O
figure	O
17.11	O
test	O
misclassiﬁcation	O
for	O
gradient	O
boosting	O
on	O
the	O
spam	B
data	O
,	O
compared	O
with	O
a	O
random	O
forest	O
and	O
bagging	O
.	O
although	O
boosting	O
appears	O
to	O
be	O
better	O
,	O
it	O
requires	O
crossvaldiation	O
or	O
some	O
other	O
means	O
to	O
estimate	B
its	O
tuning	O
parameters	O
,	O
while	O
the	O
random	O
forest	O
is	O
essentially	O
automatic	O
.	O
the	O
adaboost.m1	O
algorithm	B
.	O
although	O
the	O
classiﬁer	O
in	O
step	O
2	O
(	O
a	O
)	O
can	O
be	O
ar-	O
bitrary	O
,	O
it	O
was	O
intended	O
for	O
weak	O
learners	O
such	O
as	O
shallow	O
trees	B
.	O
steps	O
2	O
(	O
c	O
)	O
–	O
(	O
d	O
)	O
look	O
mysterious	O
.	O
its	O
easy	O
to	O
check	O
that	O
,	O
with	O
the	O
reweighted	O
points	O
,	O
the	O
classiﬁer	O
ocb	O
just	O
learned	O
would	O
have	O
weighted	O
error	O
0.5	O
,	O
that	O
of	O
a	O
coin	O
ﬂip	O
.	O
ues	O
˙1	O
,	O
the	O
ensemblebgb.x/	O
takes	O
values	O
in	O
r.	O
we	O
also	O
notice	O
that	O
,	O
although	O
the	O
individual	O
classiﬁers	O
ocb.x/	O
produce	O
val-	O
ponential	O
loss	B
function	I
.	O
the	O
functions	O
bgb.x/	O
output	O
in	O
step	O
3	O
of	O
algo-	O
it	O
turns	O
out	O
that	O
the	O
adaboost	B
algorithm	I
17.5	O
ﬁts	O
a	O
logistic	B
regression	I
model	O
via	O
a	O
version	O
of	O
the	O
general	O
boosting	O
algorithm	B
17.3	O
,	O
using	O
an	O
ex-	O
rithm	O
17.5	O
are	O
estimates	O
of	O
(	O
half	O
)	O
the	O
logit	O
function	B
(	O
cid:21	O
)	O
.x/	O
.	O
to	O
show	O
this	O
,	O
we	O
ﬁrst	O
motivate	O
the	O
exponential	O
loss	O
,	O
a	O
somewhat	O
unusual	O
choice	O
,	O
and	O
show	O
how	O
it	O
is	O
linked	O
to	O
logistic	B
regression	I
.	O
for	O
a	O
-1/1	O
response	O
y	O
and	O
function	O
f	O
.x/	O
,	O
the	O
exponential	O
loss	O
is	O
deﬁned	O
as	O
le	O
.y	O
;	O
f	O
.x//	O
d	O
expœ	O
(	O
cid:0	O
)	O
yf	O
.x/	O
.	O
a	O
simple	O
calculation	O
shows	O
that	O
the	O
solution	O
to	O
the	O
(	O
condi-	O
0.000.020.040.060.08gradient	O
boosting	O
on	O
the	O
spam	B
datanumber	O
of	O
treestest	O
error15001000150020002500baggingrandom	O
forestboosting	O
(	O
depth	B
4	O
)	O
17.4	O
adaboost	O
:	O
the	O
original	O
boosting	O
algorithm	B
343	O
wi	O
.	O
algorithm	B
17.5	O
adaboost	O
1	O
initialize	O
the	O
observation	O
weights	O
wi	O
d	O
1=n	O
;	O
i	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
n.	O
2	O
for	O
b	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
b	O
repeat	O
the	O
following	O
steps	O
.	O
(	O
a	O
)	O
fit	O
a	O
classiﬁer	O
ocb.x/	O
to	O
the	O
training	O
data	B
,	O
using	O
observation	O
weights	O
(	O
b	O
)	O
compute	O
the	O
weighted	O
misclassiﬁcation	O
error	O
for	O
ocb	O
:	O
pn	O
id1	O
wi	O
i	O
œyi	O
¤	O
ocb.xi	O
/	O
id1	O
wi	O
(	O
c	O
)	O
compute	O
˛b	O
d	O
logœ.1	O
(	O
cid:0	O
)	O
errb/=errb	O
.	O
(	O
cid:1	O
)	O
exp	O
.˛b	O
(	O
cid:1	O
)	O
i	O
œyi	O
¤	O
cb.xi	O
//	O
;	O
(	O
d	O
)	O
update	O
the	O
weights	O
wi	O
wi	O
3	O
output	O
the	O
sequence	O
of	O
functions	O
bgb.x/	O
d	O
pb	O
i	O
sponding	O
classiﬁersbcb.x/	O
d	O
sign	O
,	O
b	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
b.	O
i	O
d	O
`d1	O
˛m	O
oc`.x/	O
and	O
corre-	O
errb	O
dpn	O
hbgb.x/	O
1	O
;	O
:	O
:	O
:	O
;	O
n.	O
:	O
tional	O
)	O
population	O
minimization	O
problem	O
(	O
cid:0	O
)	O
yf	O
.x/	O
j	O
x	O
eœe	O
minimize	O
f	O
.x/	O
pr.y	O
d	O
c1jx/	O
pr.y	O
d	O
(	O
cid:0	O
)	O
1jx/	O
	O
:	O
f	O
.x/	O
d	O
1	O
2	O
log	O
(	O
17.11	O
)	O
(	O
17.12	O
)	O
is	O
given	O
by	O
inverting	O
,	O
we	O
get	O
pr.y	O
d	O
c1jx/	O
d	O
ef	O
.x/	O
(	O
cid:0	O
)	O
f	O
.x/	O
e	O
and	O
pr.y	O
d	O
(	O
cid:0	O
)	O
1jx/	O
d	O
e	O
e	O
(	O
cid:0	O
)	O
f	O
.x/	O
c	O
ef	O
.x/	O
(	O
cid:0	O
)	O
f	O
.x/	O
c	O
ef	O
.x/	O
;	O
(	O
17.13	O
)	O
a	O
perfectly	O
reasonable	O
(	O
and	O
symmetric	O
)	O
model	B
for	O
a	O
probability	O
.	O
the	O
quan-	O
tity	O
yf	O
.x/	O
is	O
known	O
as	O
the	O
margin	O
(	O
see	O
also	O
chapter	O
19	O
)	O
;	O
if	O
the	O
margin	O
is	O
positive	O
,	O
the	O
classiﬁcation	O
using	O
cf	O
.x/	O
d	O
sign.f	O
.x//	O
is	O
correct	O
for	O
y	O
,	O
else	O
it	O
is	O
incorrect	O
if	O
the	O
margin	O
is	O
negative	O
.	O
the	O
magnitude	O
of	O
yf	O
.x/	O
is	O
proportional	O
to	O
the	O
(	O
signed	O
)	O
distance	O
of	O
x	O
from	O
the	O
classiﬁcation	O
boundary	O
(	O
exactly	O
for	O
linear	B
models	O
,	O
approximately	O
otherwise	O
)	O
.	O
for	O
-1/1	O
data	B
,	O
we	O
can	O
also	O
write	O
the	O
(	O
negative	O
)	O
binomial	B
log-likelihood	O
in	O
terms	O
of	O
the	O
margin	O
.	O
344	O
random	O
forests	O
and	O
boosting	O
using	O
(	O
17.13	O
)	O
we	O
have	O
lb	O
.y	O
;	O
f	O
.x//	O
d	O
(	O
cid:0	O
)	O
fi.y	O
d	O
(	O
cid:0	O
)	O
1/	O
log	O
pr.y	O
d	O
(	O
cid:0	O
)	O
1jx/	O
(	O
cid:16	O
)	O
c	O
i.y	O
d	O
c1/	O
log	O
pr.y	O
d	O
c1jx/g	O
1	O
c	O
e	O
(	O
cid:0	O
)	O
2yf	O
.x/	O
:	O
d	O
log	O
(	O
17.14	O
)	O
e	O
(	O
cid:2	O
)	O
log	O
(	O
cid:0	O
)	O
1	O
c	O
e	O
(	O
cid:0	O
)	O
2yf	O
.x/	O
(	O
cid:1	O
)	O
j	O
x	O
(	O
cid:3	O
)	O
also	O
has	O
population	O
minimizer	O
f	O
.x/	O
equal	O
to	O
half	O
the	O
logit	O
(	O
17.12	O
)	O
.3	O
figure	O
17.12	O
compares	O
the	O
exponential	O
loss	B
function	I
with	O
this	O
binomial	B
loss	O
.	O
they	O
both	O
asymptote	O
to	O
zero	O
in	O
the	O
right	O
tail—the	O
area	O
of	O
correct	O
classiﬁcation	O
.	O
in	O
the	O
left	O
tail	O
,	O
the	O
binomial	B
loss	O
asymptotes	O
to	O
a	O
linear	B
function	O
,	O
much	O
less	O
severe	O
than	O
the	O
exponential	O
loss	O
.	O
figure	O
17.12	O
exponential	O
loss	O
used	O
in	O
adaboost	O
,	O
versus	O
the	O
binomial	B
loss	O
used	O
in	O
the	O
usual	O
logistic	B
regression	I
.	O
both	O
estimate	B
the	O
logit	O
function	B
.	O
the	O
exponential	O
left	O
tail	O
,	O
which	O
punishes	O
misclassiﬁcations	O
,	O
is	O
much	O
more	O
severe	O
than	O
the	O
asymptotically	O
linear	B
tail	O
of	O
the	O
binomial	B
.	O
the	O
exponential	O
loss	O
simpliﬁes	O
step	O
2	O
(	O
a	O
)	O
in	O
the	O
gradient	O
boosting	O
algo-	O
3	O
the	O
half	O
comes	O
from	O
the	O
symmetric	O
representation	O
we	O
use	O
.	O
−3−2−101230123456yf	O
(	O
x	O
)	O
lossbinomialexponential	O
rithm	O
17.3	O
.	O
(	O
cid:16	O
)	O
yi	O
;	O
bgb	O
(	O
cid:0	O
)	O
1.xi	O
/	O
c	O
g.xii	O
(	O
cid:13	O
)	O
/	O
le	O
nx	O
id1	O
17.5	O
connections	O
and	O
extensions	O
345	O
id1	O
	O
d	O
nx	O
d	O
nx	O
d	O
nx	O
id1	O
expœ	O
(	O
cid:0	O
)	O
yi	O
.bgb	O
(	O
cid:0	O
)	O
1.xi	O
/	O
c	O
g.xii	O
(	O
cid:13	O
)	O
//	O
wi	O
expœ	O
(	O
cid:0	O
)	O
yi	O
g.xii	O
(	O
cid:13	O
)	O
/	O
(	O
17.15	O
)	O
wi	O
le	O
.yi	O
;	O
g.xii	O
(	O
cid:13	O
)	O
//	O
;	O
5	O
with	O
wi	O
d	O
expœ	O
(	O
cid:0	O
)	O
yibgb	O
(	O
cid:0	O
)	O
1.xi	O
/	O
.	O
this	O
is	O
just	O
a	O
weighted	O
exponential	O
loss	O
with	O
the	O
past	O
history	O
encapsulated	O
in	O
the	O
observation	O
weight	O
wi	O
(	O
see	O
step	O
2	O
(	O
a	O
)	O
in	O
algorithm	B
17.5	O
)	O
.	O
we	O
give	O
some	O
more	O
details	O
in	O
the	O
chapter	O
endnotes	O
on	O
how	O
this	O
reduces	O
to	O
the	O
adaboost	O
algorithm.	O
id1	O
the	O
adaboost	B
algorithm	I
achieves	O
an	O
error	O
rate	B
on	O
the	O
spam	B
data	O
com-	O
parable	O
to	O
binomial	B
gradient	O
boosting	O
.	O
17.5	O
connections	O
and	O
extensions	O
boosting	O
is	O
a	O
general	O
nonparametric	B
function-ﬁtting	O
algorithm	B
,	O
and	O
shares	O
attributes	O
with	O
a	O
variety	O
of	O
existing	O
methods	O
.	O
here	O
we	O
relate	O
boosting	O
to	O
two	O
different	O
approaches	O
:	O
generalized	O
additive	O
models	B
and	O
the	O
lasso	B
of	O
chap-	O
ter	O
16.	O
generalized	O
additive	O
models	B
boosting	O
ﬁts	O
additive	O
,	O
low-order	O
interaction	O
models	B
by	O
a	O
forward	O
stage-	O
wise	O
strategy	O
.	O
generalized	O
additive	O
models	B
(	O
gams	O
)	O
are	O
a	O
predecessor	O
,	O
a	O
semi-parametric	O
approach	O
toward	O
nonlinear	B
function	O
ﬁtting	B
.	O
a	O
gam	O
has	O
the	O
form	B
fj	O
.xj	O
/	O
;	O
(	O
17.16	O
)	O
where	O
again	O
(	O
cid:21	O
)	O
.x/	O
d	O
œ	O
(	O
cid:22	O
)	O
.x/	O
is	O
the	O
natural	O
parameter	O
in	O
an	O
exponential	O
family	O
.	O
the	O
attraction	O
of	O
a	O
gam	O
is	O
that	O
the	O
components	O
are	O
interpretable	O
and	O
can	O
be	O
visualized	O
,	O
and	O
they	O
can	O
move	O
us	O
a	O
big	O
step	O
up	O
from	O
a	O
linear	B
model	I
.	O
jd1	O
there	O
are	O
many	O
ways	O
to	O
specify	O
and	O
ﬁt	O
additive	O
models	B
.	O
for	O
the	O
fj	O
,	O
we	O
could	O
use	O
parametric	B
functions	O
(	O
e.g	O
.	O
polynomials	O
)	O
,	O
ﬁxed-knot	O
regression	B
splines	O
,	O
or	O
even	O
linear	B
functions	O
for	O
some	O
terms	O
.	O
less	O
parametric	B
options	O
(	O
cid:21	O
)	O
.x/	O
d	O
px	O
346	O
random	O
forests	O
and	O
boosting	O
are	O
smoothing	B
splines	O
and	O
local	O
regression	B
(	O
see	O
section	O
19.8	O
)	O
.	O
in	O
the	O
case	O
of	O
squared-error	O
loss	O
(	O
the	O
gaussian	O
case	O
)	O
,	O
there	O
is	O
a	O
natural	O
set	B
of	O
backﬁtting	O
equations	O
for	O
ﬁtting	B
a	O
gam	O
:	O
o	O
fj	O
of`/	O
;	O
j	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
p	O
:	O
sj	O
.y	O
(	O
cid:0	O
)	O
x	O
o	O
f`.xn`/	O
(	O
17.17	O
)	O
o	O
f`.x1`/	O
;	O
:	O
:	O
:	O
;	O
.	O
`¤j	O
here	O
of`	O
d	O
œ	O
0	O
is	O
the	O
n-vector	O
of	O
ﬁtted	O
values	O
for	O
the	O
current	O
estimate	O
of	O
function	O
f`	O
.	O
hence	O
the	O
term	O
in	O
parentheses	O
is	O
a	O
partial	O
residual	O
,	O
removing	O
all	O
the	O
current	O
function	B
ﬁts	O
from	O
y	O
except	O
the	O
one	O
about	O
to	O
be	O
updated	O
.	O
sj	O
is	O
a	O
smoothing	B
operator	O
derived	O
from	O
variable	O
xj	O
that	O
gets	O
applied	O
to	O
this	O
residual	O
and	O
delivers	O
the	O
next	O
estimate	B
for	O
function	B
f`	O
.	O
backﬁtting	O
starts	O
with	O
all	O
the	O
functions	O
zero	O
,	O
and	O
then	O
cycles	O
through	O
these	O
equations	O
for	O
j	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
p	O
;	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
in	O
a	O
block-coordinate	O
fashion	O
,	O
until	O
all	O
the	O
functions	O
stabilize	O
.	O
the	O
ﬁrst	O
pass	O
through	O
all	O
the	O
variables	O
is	O
similar	O
to	O
the	O
regression	B
boost-	O
ing	O
algorithm	B
17.2	O
,	O
where	O
each	O
new	O
function	B
takes	O
the	O
residuals	O
from	O
the	O
past	O
ﬁts	O
,	O
and	O
models	O
them	O
using	O
a	O
tree	O
(	O
for	O
sj	O
)	O
.	O
the	O
difference	O
is	O
that	O
boosting	O
never	O
goes	O
back	O
and	O
ﬁxes	O
up	O
past	O
functions	O
,	O
but	O
ﬁts	O
in	O
a	O
forward-	O
stagewise	O
fashion	O
,	O
leaving	O
all	O
past	O
functions	O
alone	O
.	O
of	O
course	O
,	O
with	O
its	O
adap-	O
tive	O
ﬁtting	B
mechanism	O
,	O
boosting	O
can	O
select	O
the	O
same	O
variables	O
as	O
used	O
be-	O
fore	O
,	O
and	O
thereby	O
update	O
that	O
component	O
of	O
the	O
ﬁt	O
.	O
boosting	O
with	O
stumps	O
(	O
single-split	O
trees	B
,	O
see	O
the	O
discussion	O
on	O
tree	O
depth	B
on	O
335	O
in	O
section	O
17.2	O
)	O
can	O
hence	O
be	O
seen	O
as	O
an	O
adaptive	B
way	O
for	O
ﬁtting	B
an	O
additive	O
model	B
,	O
that	O
si-	O
multaneously	O
performs	O
variable	O
selection	O
and	O
allows	O
for	O
different	O
amounts	O
of	O
smoothing	B
for	O
different	O
variables	O
.	O
boosting	O
and	O
the	O
lasso	B
in	O
section	O
16.7	O
we	O
drew	O
attention	O
to	O
the	O
close	O
connection	O
between	O
the	O
forward-stagewise	O
ﬁtting	B
of	O
boosting	O
(	O
with	O
shrinkage	B
)	O
and	O
the	O
lasso	B
,	O
via	O
inﬁnitesimal	O
forward-stagewise	O
regression	B
.	O
here	O
we	O
take	O
this	O
a	O
step	O
further	O
,	O
by	O
using	O
the	O
lasso	B
as	O
a	O
post-processor	O
for	O
boosting	O
(	O
or	O
random	O
forests	O
)	O
.	O
boosting	O
with	O
shrinkage	B
does	O
a	O
good	O
job	O
in	O
building	O
a	O
prediction	O
model	B
,	O
but	O
at	O
the	O
end	O
of	O
the	O
day	O
can	O
involve	O
a	O
lot	O
of	O
trees	B
.	O
because	O
of	O
the	O
shrink-	O
age	O
,	O
many	O
of	O
these	O
trees	B
could	O
be	O
similar	O
to	O
each	O
other	O
.	O
the	O
idea	O
here	O
is	O
to	O
use	O
the	O
lasso	B
to	O
select	O
a	O
subset	O
of	O
these	O
trees	B
,	O
reweight	O
them	O
,	O
and	O
hence	O
produce	O
a	O
prediction	O
model	B
with	O
far	O
fewer	O
trees	B
and	O
,	O
one	O
hopes	O
,	O
compa-	O
rable	O
accuracy	O
.	O
suppose	O
boosting	O
has	O
produced	O
a	O
sequence	O
of	O
ﬁtted	O
trees	B
ogb.x/	O
;	O
b	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
b.	O
we	O
then	O
solve	O
the	O
lasso	B
problem	O
17.6	O
notes	O
and	O
details	O
347	O
figure	O
17.13	O
post-processing	O
of	O
the	O
trees	B
produced	O
by	O
boosting	O
on	O
the	O
als	O
data	B
.	O
shown	O
is	O
the	O
test	O
prediction	O
error	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
trees	O
selected	O
by	O
the	O
(	O
nonnegative	O
)	O
lasso	B
.	O
we	O
see	O
that	O
the	O
lasso	B
can	O
do	O
as	O
good	O
a	O
job	O
with	O
one-third	O
the	O
number	O
of	O
trees	O
,	O
although	O
selecting	O
the	O
correct	O
number	O
is	O
critical.	O
''	O
nx	O
id1	O
bx	O
bd1	O
#	O
bx	O
bd1	O
minimize	O
fˇbgb	O
1	O
l	O
yi	O
;	O
ogb.xi	O
/ˇb	O
c	O
(	O
cid:21	O
)	O
jˇbj	O
(	O
17.18	O
)	O
for	O
different	O
values	O
of	O
(	O
cid:21	O
)	O
.	O
this	O
model	B
selects	O
some	O
of	O
the	O
trees	B
,	O
and	O
as-	O
signs	O
differential	O
weights	O
to	O
them	O
.	O
a	O
reasonable	O
variant	O
is	O
to	O
insist	O
that	O
the	O
weights	O
are	O
nonnegative	O
.	O
figure	O
17.13	O
illustrates	O
this	O
approach	O
on	O
the	O
als	O
data	B
.	O
here	O
we	O
could	O
use	O
one-third	O
of	O
the	O
trees	B
.	O
often	O
the	O
savings	O
are	O
much	O
more	O
dramatic	O
.	O
17.6	O
notes	O
and	O
details	O
random	O
forests	O
and	O
boosting	O
live	O
at	O
the	O
cutting	O
edge	O
of	O
modern	O
predic-	O
tion	O
methodology	O
.	O
they	O
ﬁt	O
models	B
of	O
breathtaking	O
complexity	O
compared	O
with	O
classical	O
linear	B
regression	O
,	O
or	O
even	O
with	O
standard	O
glm	O
modeling	O
as	O
practiced	O
in	O
the	O
late	O
twentieth	O
century	O
(	O
chapter	O
8	O
)	O
.	O
they	O
are	O
routinely	O
used	O
as	O
prediction	O
engines	O
in	O
a	O
wide	O
variety	O
of	O
industrial	O
and	O
scientiﬁc	O
appli-	O
cations	O
.	O
for	O
the	O
more	O
cautious	O
,	O
they	O
provide	O
a	O
terriﬁc	O
benchmark	O
for	O
how	O
well	O
a	O
traditional	O
parametrized	O
model	B
is	O
performing	O
:	O
if	O
the	O
random	O
forests	O
0.250.260.270.280.290.30number	O
of	O
treesmean−squared	O
error1100200300400500depth	O
2	O
boostlasso	O
post	O
fit	O
348	O
random	O
forests	O
and	O
boosting	O
does	O
much	O
better	O
,	O
you	O
probably	O
have	O
some	O
work	O
to	O
do	O
,	O
by	O
including	O
some	O
important	O
interactions	O
and	O
the	O
like	O
.	O
the	O
regression	B
and	O
classiﬁcation	O
trees	B
discussed	O
in	O
chapter	O
8	O
(	O
breiman	O
et	O
al.	O
,	O
1984	O
)	O
took	O
traditional	O
models	B
to	O
a	O
new	O
level	O
,	O
with	O
their	O
ability	O
to	O
adapt	O
to	O
the	O
data	B
,	O
select	O
variables	O
,	O
and	O
so	O
on	O
.	O
but	O
their	O
prediction	O
per-	O
formance	O
is	O
somewhat	O
lacking	O
,	O
and	O
so	O
they	O
stood	O
the	O
risk	O
of	O
falling	O
by	O
the	O
wayside	O
.	O
with	O
their	O
new	O
use	O
as	O
building	O
blocks	O
in	O
random	O
forests	O
and	O
boosting	O
,	O
they	O
have	O
reasserted	O
themselves	O
as	O
critical	O
elements	O
in	O
the	O
mod-	O
ern	O
toolbox	O
.	O
random	O
forests	O
and	O
bagging	O
were	O
introduced	O
by	O
breiman	O
(	O
2001	O
)	O
,	O
and	O
boosting	O
by	O
schapire	O
(	O
1990	O
)	O
and	O
freund	O
and	O
schapire	O
(	O
1996	O
)	O
.	O
there	O
has	O
been	O
much	O
discussion	O
on	O
why	O
boosting	O
works	O
(	O
breiman	O
,	O
1998	O
;	O
friedman	O
et	O
al.	O
,	O
2000	O
;	O
schapire	O
and	O
freund	O
,	O
2012	O
)	O
;	O
the	O
statistical	O
interpretation	O
given	O
here	O
can	O
also	O
be	O
found	O
in	O
hastie	O
et	O
al	O
.	O
(	O
2009	O
)	O
,	O
and	O
led	O
to	O
the	O
gradient	O
boost-	O
ing	O
algorithm	B
(	O
friedman	O
,	O
2001	O
)	O
.	O
adaboost	O
was	O
ﬁrst	O
described	O
in	O
freund	O
and	O
schapire	O
(	O
1997	O
)	O
.	O
hastie	O
et	O
al	O
.	O
(	O
2009	O
,	O
chapter	O
15	O
)	O
is	O
devoted	O
to	O
random	O
forests	O
.	O
for	O
the	O
examples	O
in	O
this	O
chapter	O
we	O
used	O
the	O
randomforest	O
package	O
in	O
r	O
(	O
liaw	O
and	O
wiener	O
,	O
2002	O
)	O
,	O
and	O
for	O
boosting	O
the	O
gbm	B
(	O
ridge-	O
way	O
,	O
2005	O
)	O
package	O
.	O
the	O
lasso	B
post-processing	O
idea	O
is	O
due	O
to	O
friedman	O
and	O
popescu	O
(	O
2005	O
)	O
,	O
which	O
we	O
implemented	O
using	O
glmnet	B
(	O
friedman	O
et	O
al.	O
,	O
2009	O
)	O
.	O
generalized	O
additive	O
models	B
are	O
described	O
in	O
hastie	O
and	O
tibshirani	O
(	O
1990	O
)	O
.	O
we	O
now	O
give	O
some	O
particular	O
technical	O
details	O
on	O
topics	O
covered	O
in	O
the	O
chapter	O
.	O
1	O
[	O
p.	O
327	O
]	O
averaging	B
trees	O
.	O
a	O
maximal-depth	O
tree	O
splits	O
every	O
node	O
until	O
it	O
is	O
pure	O
,	O
meaning	O
all	O
the	O
responses	O
are	O
the	O
same	O
.	O
for	O
very	O
large	O
n	O
this	O
might	O
be	O
unreasonable	O
;	O
in	O
practice	O
,	O
one	O
can	O
put	O
a	O
lower	O
bound	B
on	O
the	O
minimum	O
count	O
in	O
a	O
terminal	B
node	I
.	O
we	O
are	O
deliberately	O
vague	O
about	O
the	O
response	O
type	O
in	O
algorithm	B
17.1.	O
if	O
it	O
is	O
quantitative	O
,	O
we	O
would	O
ﬁt	O
a	O
regression	B
tree	O
.	O
if	O
it	O
is	O
binary	O
or	O
multilevel	O
qualitative	O
,	O
we	O
would	O
ﬁt	O
a	O
classiﬁcation	O
tree	O
.	O
in	O
this	O
case	O
at	O
the	O
averaging	B
stage	O
,	O
there	O
are	O
at	O
least	O
two	O
strategies	O
.	O
the	O
original	O
random-forest	O
paper	O
(	O
breiman	O
,	O
2001	O
)	O
proposed	O
that	O
each	O
tree	O
should	O
make	O
a	O
classiﬁcation	O
,	O
and	O
then	O
the	O
ensemble	O
uses	O
a	O
plurality	O
vote	O
.	O
an	O
alterna-	O
tive	O
reasonable	O
strategy	O
is	O
to	O
average	O
the	O
class	O
probabilities	B
produced	O
by	O
the	O
trees	B
;	O
these	O
procedures	O
are	O
identical	O
if	O
the	O
trees	B
are	O
grown	O
to	O
maximal	O
depth	B
.	O
2	O
[	O
p.	O
330	O
]	O
jackknife	O
variance	O
estimate	B
.	O
the	O
jackknife	O
estimate	O
of	O
variance	O
for	O
a	O
random	O
forest	O
,	O
and	O
the	O
bias-corrected	O
version	O
,	O
is	O
described	O
in	O
wager	O
et	O
al	O
.	O
(	O
2014	O
)	O
.	O
the	O
jackknife	O
formula	O
(	O
17.3	O
)	O
is	O
applied	O
to	O
the	O
b	O
d	O
1	O
ver-	O
17.6	O
notes	O
and	O
details	O
349	O
sion	O
of	O
the	O
random	O
forest	O
,	O
but	O
of	O
course	O
is	O
estimated	O
by	O
plugging	O
in	O
ﬁnite	O
b	O
versions	O
of	O
the	O
quantities	O
involved	O
.	O
replacing	O
or	O
.	O
(	O
cid:1	O
)	O
/	O
rf	O
.x0/	O
by	O
its	O
expectation	O
orrf.x0/	O
is	O
not	O
the	O
problem	O
;	O
its	O
that	O
each	O
of	O
the	O
or	O
.i	O
/	O
rf	O
.x0/	O
vary	O
about	O
their	O
boot-	O
strap	O
expectations	O
,	O
compounded	O
by	O
the	O
square	O
in	O
expression	O
(	O
17.4	O
)	O
.	O
calcu-	O
lating	O
the	O
bias	O
requires	O
some	O
technical	O
derivations	O
,	O
which	O
can	O
be	O
found	O
in	O
that	O
reference	O
.	O
they	O
also	O
describe	O
the	O
inﬁnitesimal	O
jackknife	O
estimate	O
of	O
variance	O
,	O
given	O
by	O
withdcovi	O
ddcov.w	O
(	O
cid:3	O
)	O
bvij.orrf.x0//	O
d	O
nx	O
dcov2	O
i	O
;	O
id1	O
bx	O
(	O
17.19	O
)	O
(	O
17.20	O
)	O
as	O
discussed	O
in	O
chapter	O
20.	O
it	O
too	O
has	O
a	O
bias-corrected	O
version	O
,	O
given	O
by	O
(	O
cid:0	O
)	O
1/.orb.x0/	O
(	O
cid:0	O
)	O
orrf.x0//	O
;	O
b	O
.w	O
(	O
cid:3	O
)	O
b	O
i	O
bd1	O
;	O
or	O
(	O
cid:3	O
)	O
.x0//	O
d	O
1	O
bvu	O
ij.orrf.x0//	O
dbvij.orrf.x0//	O
(	O
cid:0	O
)	O
n	O
b	O
ov.x0/	O
;	O
(	O
17.21	O
)	O
similar	O
to	O
(	O
17.5	O
)	O
.	O
3	O
[	O
p.	O
334	O
]	O
the	O
als	O
data	B
.	O
these	O
data	B
were	O
kindly	O
provided	O
by	O
lester	O
mackey	O
and	O
lilly	O
fang	O
,	O
who	O
won	O
the	O
dream	O
challenge	O
prediction	O
prize	O
in	O
2012	O
(	O
kuffner	O
et	O
al.	O
,	O
2015	O
)	O
.	O
it	O
includes	O
some	O
additional	O
variables	O
created	O
by	O
them	O
.	O
their	O
winning	O
entry	O
used	O
bayesian	O
trees	B
,	O
not	O
too	O
different	O
from	O
ran-	O
dom	O
forests	O
.	O
4	O
[	O
p.	O
341	O
]	O
gradient-boosting	O
details	O
.	O
in	O
friedman	O
’	O
s	O
gradient-boosting	O
algo-	O
rithm	O
(	O
hastie	O
et	O
al.	O
,	O
2009	O
,	O
chapter	O
10	O
,	O
for	O
example	O
)	O
,	O
a	O
further	O
reﬁnement	O
is	O
implemented	O
.	O
the	O
tree	O
in	O
step	O
2	O
(	O
b	O
)	O
of	O
algorithm	B
17.4	O
is	O
used	O
to	O
de-	O
ﬁne	O
the	O
structure	O
(	O
split	O
variables	O
and	O
splits	O
)	O
,	O
but	O
the	O
values	O
in	O
the	O
terminal	O
nodes	O
are	O
left	O
to	O
be	O
updated	O
.	O
we	O
can	O
think	O
of	O
partitioning	O
the	O
parameters	O
(	O
cid:13	O
)	O
d	O
.	O
(	O
cid:13	O
)	O
s	O
;	O
(	O
cid:13	O
)	O
t	B
/	O
,	O
and	O
then	O
represent	O
the	O
tree	O
as	O
g.xi	O
(	O
cid:13	O
)	O
/	O
d	O
t	B
.xi	O
(	O
cid:13	O
)	O
s/	O
(	O
cid:13	O
)	O
t.	O
here	O
t	B
.xi	O
(	O
cid:13	O
)	O
s/	O
is	O
a	O
vector	B
of	O
d	O
c	O
1	O
binary	O
basis	O
functions	O
that	O
indicate	O
the	O
termi-	O
nal	O
node	O
reached	O
by	O
input	O
x	O
,	O
and	O
(	O
cid:13	O
)	O
t	B
are	O
the	O
d	O
c	O
1	O
values	O
of	O
the	O
terminal	O
nodes	O
of	O
the	O
tree	O
.	O
we	O
learn	O
o	O
(	O
cid:13	O
)	O
s	O
by	O
approximating	O
the	O
gradient	O
in	O
step	O
2	O
(	O
b	O
)	O
by	O
a	O
tree	O
,	O
and	O
then	O
(	O
re-	O
)	O
learn	O
the	O
terminal-node	O
parameters	O
o	O
(	O
cid:13	O
)	O
t	B
by	O
solving	O
the	O
optimization	O
problem	O
0	O
(	O
cid:16	O
)	O
yi	O
;	O
ogb	O
(	O
cid:0	O
)	O
1.xi	O
/	O
c	O
t	B
.xii	O
o	O
(	O
cid:13	O
)	O
s/	O
0	O
(	O
cid:13	O
)	O
t	B
	O
:	O
(	O
17.22	O
)	O
nx	O
id1	O
minimize	O
(	O
cid:13	O
)	O
t	B
l	O
solving	O
(	O
17.22	O
)	O
amounts	O
to	O
ﬁtting	B
a	O
simple	O
glm	O
with	O
an	O
offset	O
.	O
350	O
random	O
forests	O
and	O
boosting	O
5	O
[	O
p.	O
345	O
]	O
adaboost	O
and	O
gradient	O
boosting	O
.	O
hastie	O
et	O
al	O
.	O
(	O
2009	O
,	O
chapter	O
10	O
)	O
derive	O
adaboost	O
as	O
an	O
instance	O
of	O
algorithm	B
17.3.	O
one	O
detail	O
is	O
that	O
the	O
trees	B
g.xi	O
(	O
cid:13	O
)	O
/	O
are	O
replaced	O
by	O
a	O
simpliﬁed	O
scaled	O
classiﬁer	O
˛	O
(	O
cid:1	O
)	O
c.xi	O
(	O
cid:13	O
)	O
/	O
.	O
hence	O
,	O
from	O
(	O
17.15	O
)	O
,	O
in	O
step	O
2	O
(	O
a	O
)	O
of	O
algorithm	B
17.3	O
we	O
need	O
to	O
solve	O
0	O
wi	O
expœ	O
(	O
cid:0	O
)	O
yi	O
˛c.xii	O
(	O
cid:13	O
)	O
0	O
/	O
:	O
(	O
17.23	O
)	O
nx	O
id1	O
minimize	O
0	O
˛	O
;	O
(	O
cid:13	O
)	O
the	O
derivation	O
goes	O
on	O
to	O
show	O
that	O
(	O
cid:15	O
)	O
minimizing	O
(	O
17.23	O
)	O
for	O
any	O
value	O
of	O
˛	O
>	O
0	O
can	O
be	O
achieved	O
by	O
ﬁtting	B
/	O
to	O
minimize	O
the	O
weighted	O
misclassiﬁcation	O
0	O
a	O
classiﬁcation	O
tree	O
c.xi	O
o	O
(	O
cid:13	O
)	O
nx	O
error	O
wi	O
i	O
œyi	O
¤	O
c.xi	O
;	O
(	O
cid:13	O
)	O
0	O
/i	O
id1	O
(	O
cid:15	O
)	O
given	O
c.xi	O
o	O
(	O
cid:13	O
)	O
0	O
/	O
,	O
˛	O
is	O
estimated	O
as	O
in	O
step	O
2	O
(	O
c	O
)	O
of	O
algorithm	B
17.5	O
(	O
and	O
is	O
non-negative	O
)	O
;	O
(	O
cid:15	O
)	O
the	O
weight-update	O
scheme	O
in	O
step	O
2	O
(	O
d	O
)	O
of	O
algorithm	B
17.5	O
corresponds	O
exactly	O
to	O
the	O
weights	O
as	O
computed	O
in	O
(	O
17.15	O
)	O
.	O
18	O
neural	O
networks	O
and	O
deep	O
learning	O
something	O
happened	O
in	O
the	O
mid	O
1980s	O
that	O
shook	O
up	O
the	O
applied	O
statistics	B
community	O
.	O
neural	O
networks	O
(	O
nns	O
)	O
were	O
introduced	O
,	O
and	O
they	O
marked	O
a	O
shift	O
of	O
predictive	O
modeling	O
towards	O
computer	O
science	O
and	O
machine	O
learn-	O
ing	O
.	O
a	O
neural	O
network	O
is	O
a	O
highly	O
parametrized	O
model	B
,	O
inspired	O
by	O
the	O
ar-	O
chitecture	O
of	O
the	O
human	O
brain	O
,	O
that	O
was	O
widely	O
promoted	O
as	O
a	O
universal	O
approximator—a	O
machine	O
that	O
with	O
enough	O
data	B
could	O
learn	O
any	O
smooth	O
predictive	O
relationship	O
.	O
figure	O
18.1	O
neural	O
network	O
diagram	O
with	O
a	O
single	O
hidden	O
layer	B
.	O
the	O
hidden	O
layer	B
derives	O
transformations	O
of	O
the	O
inputs—nonlinear	O
transformations	O
of	O
linear	B
combinations—which	O
are	O
then	O
used	O
to	O
model	B
the	O
output	O
.	O
cp4	O
cp5	O
0	O
figure	O
18.1	O
shows	O
a	O
simple	O
example	O
of	O
a	O
feed-forward	O
neural	O
network	O
diagram	O
.	O
there	O
are	O
four	O
predictors	O
or	O
inputs	O
xj	O
,	O
ﬁve	O
hidden	O
units	O
a`	O
d	O
`d1	O
w.2/	O
g.w.1/	O
`	O
a`/	O
.	O
`0	O
the	O
language	O
associated	O
with	O
nns	O
is	O
colorful	O
:	O
memory	O
units	O
or	O
neurons	O
automatically	O
learn	O
new	O
features	O
from	O
the	O
data	B
through	O
a	O
process	O
called	O
`j	O
xj	O
/	O
,	O
and	O
a	O
single	O
output	O
unit	O
o	O
d	O
h.w.2/	O
jd1	O
w.1/	O
351	O
x1x2x3x4f	O
(	O
x	O
)	O
hiddenlayerl2inputlayerl1outputlayerl3	O
352	O
neural	O
networks	O
`j	O
supervised	O
learning	O
.	O
each	O
neuron	O
al	O
is	O
connected	O
to	O
the	O
input	O
layer	B
via	O
a	O
gp	O
vector	B
of	O
parameters	O
or	O
weights	O
fw.1/	O
1	O
(	O
the	O
.1/	O
refers	O
to	O
the	O
ﬁrst	O
layer	B
and	O
`j	O
refers	O
to	O
the	O
j	O
th	O
variable	O
and	O
`th	O
unit	O
)	O
.	O
the	O
intercept	O
terms	O
w.1/	O
`0	O
are	O
called	O
a	O
bias	O
,	O
and	O
the	O
function	B
g	O
is	O
a	O
nonlinearity	O
,	O
such	O
as	O
the	O
sigmoid	O
function	B
g.t	O
/	O
d	O
1=.1	O
c	O
e	O
(	O
cid:0	O
)	O
t	B
/	O
.	O
the	O
idea	O
was	O
that	O
each	O
neuron	O
will	O
learn	O
a	O
simple	O
binary	O
on/off	O
function	B
;	O
the	O
sigmoid	O
function	B
is	O
a	O
smooth	O
and	O
dif-	O
ferentiable	O
compromise	O
.	O
the	O
ﬁnal	O
or	O
output	O
layer	B
also	O
has	O
weights	O
,	O
and	O
an	O
output	O
function	B
h.	O
for	O
quantitative	O
regression	B
h	O
is	O
typically	O
the	O
identity	O
function	B
,	O
and	O
for	O
a	O
binary	O
response	O
it	O
is	O
once	O
again	O
the	O
sigmoid	O
.	O
note	O
that	O
without	O
the	O
nonlinearity	O
in	O
the	O
hidden	O
layer	B
,	O
the	O
neural	O
network	O
would	O
re-	O
duce	O
to	O
a	O
generalized	O
linear	B
model	I
(	O
chapter	O
8	O
)	O
.	O
typically	O
neural	O
networks	O
are	O
ﬁt	O
by	O
maximum	B
likelihood	I
,	O
usually	O
with	O
a	O
variety	O
of	O
forms	O
of	O
regular-	O
ization	O
.	O
the	O
knee-jerk	O
response	O
from	O
statisticians	O
was	O
“	O
what	O
’	O
s	O
the	O
big	O
deal	O
?	O
a	O
neural	O
network	O
is	O
just	O
a	O
nonlinear	B
model	O
,	O
not	O
too	O
different	O
from	O
many	O
other	O
generalizations	O
of	O
linear	B
models.	O
”	O
while	O
this	O
may	O
be	O
true	O
,	O
neural	O
networks	O
brought	O
a	O
new	O
energy	O
to	O
the	O
ﬁeld	O
.	O
they	O
could	O
be	O
scaled	O
up	O
and	O
generalized	O
in	O
a	O
variety	O
of	O
ways	O
:	O
many	O
hidden	O
units	O
in	O
a	O
layer	B
,	O
multiple	O
hidden	O
layers	O
,	O
weight	O
sharing	B
,	O
a	O
variety	O
of	O
colorful	O
forms	O
of	O
regularization	B
,	O
and	O
innovative	O
learning	O
algorithms	O
for	O
massive	O
data	B
sets	O
.	O
and	O
most	O
importantly	O
,	O
they	O
were	O
able	O
to	O
solve	O
problems	O
on	O
a	O
scale	B
far	O
exceeding	O
what	O
the	O
statistics	B
community	O
was	O
used	O
to	O
.	O
this	O
was	O
part	O
computing	O
scale	B
and	O
expertise	O
,	O
part	O
liberated	O
thinking	O
and	O
cre-	O
ativity	O
on	O
the	O
part	O
of	O
this	O
computer	O
science	O
community	O
.	O
new	O
journals	O
were	O
devoted	O
to	O
the	O
ﬁeld	O
,	O
	O
and	O
several	O
popular	O
annual	O
conferences	O
(	O
initially	O
at	O
ski	O
resorts	O
)	O
attracted	O
their	O
denizens	O
,	O
and	O
drew	O
in	O
members	O
of	O
the	O
statistics	B
community	O
.	O
after	O
enjoying	O
considerable	O
popularity	O
for	O
a	O
number	O
of	O
years	O
,	O
neural	O
networks	O
were	O
somewhat	O
sidelined	O
by	O
new	O
inventions	O
in	O
the	O
mid	O
1990s	O
,	O
such	O
as	O
boosting	O
(	O
chapter	O
17	O
)	O
and	O
svms	O
(	O
chapter	O
19	O
)	O
.	O
neural	O
networks	O
were	O
pass´e	O
.	O
but	O
then	O
they	O
re-emerged	O
with	O
a	O
vengeance	O
after	O
2010—the	O
reincarnation	O
now	O
being	O
called	O
deep	O
learning	O
.	O
this	O
renewed	O
enthusiasm	O
is	O
a	O
result	O
of	O
massive	O
improvements	O
in	O
computer	O
resources	O
,	O
some	O
innovations	O
,	O
and	O
the	O
ideal	O
niche	O
learning	O
tasks	O
such	O
as	O
image	O
and	O
video	O
classiﬁcation	O
,	O
and	O
speech	O
and	O
text	O
processing	O
.	O
1	O
18.1	O
neural	O
networks	O
and	O
the	O
handwritten	O
digit	O
problem	O
353	O
18.1	O
neural	O
networks	O
and	O
the	O
handwritten	O
digit	O
problem	O
neural	O
networks	O
really	O
cut	O
their	O
baby	O
teeth	O
on	O
an	O
optical	O
character	O
recogni-	O
tion	O
(	O
ocr	O
)	O
task	O
:	O
automatic	O
reading	O
of	O
handwritten	B
digits	I
,	O
as	O
in	O
a	O
zipcode	O
.	O
figure	O
18.2	O
shows	O
some	O
examples	O
,	O
taken	O
from	O
the	O
mnist	O
corpus	O
.	O
	O
the	O
2	O
idea	O
is	O
to	O
build	O
a	O
classiﬁer	O
c.x/	O
2	O
f0	O
;	O
1	O
;	O
:	O
:	O
:	O
;	O
9g	O
based	O
on	O
the	O
input	O
image	O
x	O
2	O
r28	O
(	O
cid:2	O
)	O
28	O
,	O
a	O
28	O
(	O
cid:2	O
)	O
28	O
grid	O
of	O
image	O
intensities	O
.	O
in	O
fact	O
,	O
as	O
is	O
often	O
the	O
case	O
,	O
it	O
is	O
more	O
useful	O
to	O
learn	O
the	O
probability	O
function	B
pr.y	O
d	O
jjx/	O
;	O
j	O
d	O
0	O
;	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
9	O
;	O
this	O
is	O
indeed	O
the	O
target	O
for	O
our	O
neural	O
network	O
.	O
figure	O
18.3	O
figure	O
18.2	O
examples	O
of	O
handwritten	B
digits	I
from	O
the	O
mnist	O
corpus	O
.	O
each	O
digit	O
is	O
represented	O
by	O
a	O
28	O
(	O
cid:2	O
)	O
28	O
grayscale	O
image	O
,	O
derived	O
from	O
normalized	O
binary	O
images	O
of	O
different	O
shapes	O
and	O
sizes	O
.	O
the	O
value	O
stored	O
for	O
each	O
pixel	O
in	O
an	O
image	O
is	O
a	O
nonnegative	O
eight-bit	O
representation	O
of	O
the	O
amount	O
of	O
gray	O
present	O
at	O
that	O
location	O
.	O
the	O
784	O
pixels	O
for	O
each	O
image	O
are	O
the	O
predictors	O
,	O
and	O
the	O
0–9	O
class	O
labels	O
the	O
response	O
.	O
there	O
are	O
60,000	O
training	O
images	O
in	O
the	O
full	B
data	O
set	B
,	O
and	O
10,000	O
in	O
the	O
test	O
set	B
.	O
shows	O
a	O
neural	O
network	O
with	O
three	O
hidden	O
layers	O
,	O
a	O
successful	O
conﬁgura-	O
tion	O
for	O
this	O
digit	O
classiﬁcation	O
problem	O
.	O
in	O
this	O
case	O
the	O
output	O
layer	B
has	O
10	O
nodes	B
,	O
one	O
for	O
each	O
of	O
the	O
possible	O
class	O
labels	O
.	O
we	O
use	O
this	O
example	O
to	O
walk	O
the	O
reader	O
through	O
some	O
of	O
the	O
aspects	O
of	O
the	O
conﬁguration	O
of	O
a	O
network	O
,	O
and	O
ﬁtting	O
it	O
to	O
training	O
data	B
.	O
since	O
all	O
of	O
the	O
layers	O
are	O
functions	O
of	O
their	O
previous	O
layers	O
,	O
and	O
ﬁnally	O
functions	O
of	O
the	O
input	O
vector	B
x	O
,	O
the	O
net-	O
work	O
represents	O
a	O
somewhat	O
complex	O
function	B
f	O
.xi	O
w/	O
,	O
where	O
w	O
repre-	O
sents	O
the	O
entire	O
collection	O
of	O
weights	O
.	O
armed	O
with	O
a	O
suitable	O
loss	B
function	I
,	O
we	O
could	O
simply	O
barge	O
right	O
in	O
and	O
throw	O
it	O
at	O
our	O
favorite	O
optimizer	O
.	O
in	O
the	O
early	O
days	O
this	O
was	O
not	O
computationally	O
feasible	O
,	O
especially	O
when	O
special	O
354	O
neural	O
networks	O
figure	O
18.3	O
neural	O
network	O
diagram	O
with	O
three	O
hidden	O
layers	O
and	O
multiple	O
outputs	O
,	O
suitable	O
for	O
the	O
mnist	O
handwritten-digit	O
problem	O
.	O
the	O
input	O
layer	B
has	O
p	O
d	O
784	O
units	O
.	O
such	O
a	O
network	O
with	O
hidden	O
layer	B
sizes	O
.1024	O
;	O
1024	O
;	O
2048/	O
,	O
and	O
particular	O
choices	O
of	O
tuning	O
parameters	O
,	O
achieves	O
the	O
state-of-the	O
art	O
error	O
rate	B
of	O
0:93	O
%	O
on	O
the	O
“	O
ofﬁcial	O
”	O
test	O
data	B
set	O
.	O
this	O
network	O
has	O
close	O
to	O
four	O
million	O
weights	O
,	O
and	O
hence	O
needs	O
to	O
be	O
heavily	O
regularized	O
.	O
structure	O
is	O
imposed	O
on	O
the	O
weight	O
vectors	O
.	O
today	O
there	O
are	O
fairly	O
automatic	O
systems	O
for	O
setting	O
up	O
and	O
ﬁtting	O
neural	O
networks	O
,	O
and	O
this	O
view	O
is	O
not	O
too	O
far	O
from	O
reality	O
.	O
they	O
mostly	O
use	O
some	O
form	B
of	O
gradient	O
descent	O
,	O
and	O
rely	O
on	O
an	O
organization	O
of	O
parameters	O
that	O
leads	O
to	O
a	O
manageable	O
calculation	O
of	O
the	O
gradient	O
.	O
the	O
network	O
in	O
figure	O
18.3	O
is	O
complex	O
,	O
so	O
it	O
is	O
essential	O
to	O
establish	O
a	O
convenient	O
notation	O
for	O
referencing	O
the	O
different	O
sets	O
of	O
parameters	O
.	O
we	O
continue	O
with	O
the	O
notation	O
established	O
for	O
the	O
single-layer	O
network	O
,	O
but	O
with	O
some	O
additional	O
annotations	O
to	O
distinguish	O
aspects	O
of	O
different	O
layers	O
.	O
from	O
the	O
ﬁrst	O
to	O
the	O
second	O
layer	B
we	O
have	O
w.1/	O
`j	O
xj	O
;	O
(	O
18.1	O
)	O
(	O
18.2	O
)	O
c	O
px	O
jd1	O
`0	O
d	O
w.1/	O
d	O
g.2/.z.2/	O
`	O
/	O
:	O
z.2/	O
`	O
a.2/	O
`	O
x1x2x3	O
...	O
xpy0y1	O
...	O
y9hiddenlayerl4hiddenlayerl3hiddenlayerl2inputlayerl1outputlayerl5w	O
(	O
1	O
)	O
a	O
(	O
2	O
)	O
w	O
(	O
2	O
)	O
a	O
(	O
3	O
)	O
w	O
(	O
3	O
)	O
a	O
(	O
4	O
)	O
w	O
(	O
4	O
)	O
a	O
(	O
5	O
)	O
18.1	O
neural	O
networks	O
and	O
the	O
handwritten	O
digit	O
problem	O
355	O
we	O
have	O
separated	O
the	O
linear	B
transformations	O
z.2/	O
`	O
of	O
the	O
xj	O
from	O
the	O
nonlin-	O
ear	O
transformation	O
of	O
these	O
,	O
and	O
we	O
allow	O
for	O
layer-speciﬁc	O
nonlinear	B
trans-	O
formations	O
g.k/	O
.	O
more	O
generally	O
we	O
have	O
the	O
transition	O
from	O
layer	O
k	O
(	O
cid:0	O
)	O
1	O
to	O
layer	B
k	O
:	O
c	O
pk	O
(	O
cid:0	O
)	O
1x	O
jd1	O
`0	O
d	O
w.k	O
(	O
cid:0	O
)	O
1/	O
d	O
g.k/.z.k/	O
`	O
/	O
:	O
z.k/	O
`	O
a.k/	O
`	O
w.k	O
(	O
cid:0	O
)	O
1/	O
`j	O
a.k	O
(	O
cid:0	O
)	O
1/	O
j	O
;	O
(	O
18.3	O
)	O
(	O
18.4	O
)	O
in	O
fact	O
(	O
18.3	O
)	O
–	O
(	O
18.4	O
)	O
can	O
serve	O
for	O
the	O
input	O
layer	B
(	O
18.1	O
)	O
–	O
(	O
18.2	O
)	O
if	O
we	O
adopt	O
	O
x`	O
and	O
p1	O
d	O
p	O
,	O
the	O
number	O
of	O
input	O
variables	O
.	O
the	O
notation	O
that	O
a.1/	O
hence	O
each	O
of	O
the	O
arrows	O
in	O
figure	O
18.3	O
is	O
associated	O
with	O
a	O
weight	O
param-	O
eter	O
.	O
`	O
it	O
is	O
simpler	O
to	O
adopt	O
a	O
vector	B
notation	O
z.k/	O
d	O
w	O
.k	O
(	O
cid:0	O
)	O
1/a.k	O
(	O
cid:0	O
)	O
1/	O
a.k/	O
d	O
g.k/.z.k//	O
;	O
(	O
18.5	O
)	O
(	O
18.6	O
)	O
where	O
w	O
.k	O
(	O
cid:0	O
)	O
1/	O
represents	O
the	O
matrix	B
of	O
weights	O
that	O
go	O
from	O
layer	O
lk	O
(	O
cid:0	O
)	O
1	O
to	O
layer	B
lk	O
,	O
a.k/	O
is	O
the	O
entire	O
vector	B
of	O
activations	O
at	O
layer	B
lk	O
,	O
and	O
our	O
notation	O
assumes	O
that	O
g.k/	O
operates	O
elementwise	O
on	O
its	O
vector	B
argument	O
.	O
we	O
have	O
also	O
absorbed	O
the	O
bias	O
parameters	O
w.k	O
(	O
cid:0	O
)	O
1/	O
into	O
the	O
matrix	B
w	O
.k	O
(	O
cid:0	O
)	O
1/	O
,	O
which	O
assumes	O
that	O
we	O
have	O
augmented	O
each	O
of	O
the	O
activation	O
vectors	O
a.k/	O
with	O
a	O
constant	O
element	O
1	O
.	O
`0	O
sometimes	O
the	O
nonlinearities	O
g.k/	O
at	O
the	O
inner	O
layers	O
are	O
the	O
same	O
func-	O
tion	O
,	O
such	O
as	O
the	O
function	B
(	O
cid:27	O
)	O
deﬁned	O
earlier	O
.	O
in	O
section	O
18.5	O
we	O
present	O
a	O
network	O
for	O
natural	O
color	O
image	O
classiﬁcation	O
,	O
where	O
a	O
number	O
of	O
different	O
activation	O
functions	O
are	O
used	O
.	O
depending	O
on	O
the	O
response	O
,	O
the	O
ﬁnal	O
transformation	O
g.k/	O
is	O
usually	O
spe-	O
cial	O
.	O
for	O
m	O
-class	O
classiﬁcation	O
,	O
such	O
as	O
here	O
with	O
m	O
d	O
10	O
,	O
one	O
typically	O
uses	O
the	O
softmax	O
function	B
g.k/.z.k/	O
m	O
i	O
z.k//	O
d	O
;	O
(	O
18.7	O
)	O
mpm	O
ez.k/	O
`d1	O
ez.k/	O
`	O
which	O
computes	O
a	O
number	O
(	O
probability	O
)	O
between	O
zero	O
and	O
one	O
,	O
and	O
all	O
m	O
of	O
them	O
sum	O
to	O
one.1	O
1	O
this	O
is	O
a	O
symmetric	O
version	O
of	O
the	O
inverse	O
link	O
function	B
used	O
for	O
multiclass	B
logistic	O
regression	B
.	O
356	O
neural	O
networks	O
18.2	O
fitting	O
a	O
neural	O
network	O
as	O
we	O
have	O
seen	O
,	O
a	O
neural	O
network	O
model	B
is	O
a	O
complex	O
,	O
hierarchical	O
func-	O
tion	O
f	O
.xi	O
w/	O
of	O
the	O
the	O
feature	O
vector	B
x	O
,	O
and	O
the	O
collection	O
of	O
weights	O
w.	O
for	O
typical	O
choices	O
for	O
the	O
g.k/	O
,	O
this	O
function	B
will	O
be	O
differentiable	O
.	O
given	O
a	O
training	O
set	B
fxi	O
;	O
yign	O
1	O
and	O
a	O
loss	B
function	I
lœy	O
;	O
f	O
.x/	O
,	O
along	O
familiar	O
lines	O
we	O
might	O
seek	O
to	O
solve	O
)	O
lœyi	O
;	O
f	O
.xii	O
w/	O
c	O
(	O
cid:21	O
)	O
j.w/	O
;	O
(	O
18.8	O
)	O
(	O
nx	O
id1	O
1	O
n	O
minimize	O
w	O
where	O
j.w/	O
is	O
a	O
nonnegative	O
regularization	B
term	O
on	O
the	O
elements	O
of	O
w	O
,	O
and	O
(	O
cid:21	O
)	O
(	O
cid:21	O
)	O
0	O
is	O
a	O
tuning	O
parameter	O
.	O
(	O
in	O
practice	O
there	O
may	O
be	O
multiple	O
reg-	O
ularization	O
terms	O
,	O
each	O
with	O
their	O
own	O
(	O
cid:21	O
)	O
.	O
)	O
for	O
example	O
an	O
early	O
popular	O
penalty	B
is	O
the	O
quadratic	O
n	O
k	O
(	O
cid:0	O
)	O
1x	O
pkx	O
pkc1x	O
kd1	O
jd1	O
`d1	O
o2	O
j.w/	O
d	O
1	O
2	O
w.k/	O
`j	O
;	O
(	O
18.9	O
)	O
as	O
in	O
ridge	B
regression	I
(	O
7.41	O
)	O
.	O
also	O
known	O
as	O
the	O
weight-decay	O
penalty	B
,	O
it	O
pulls	O
the	O
weights	O
toward	O
zero	O
(	O
typically	O
the	O
biases	O
are	O
not	O
penalized	O
)	O
.	O
lasso	B
penalties	O
(	O
chapter	O
16	O
)	O
are	O
also	O
popular	O
,	O
as	O
are	O
mixtures	O
of	O
these	O
(	O
an	O
elastic	O
net	O
)	O
.	O
for	O
binary	O
classiﬁcation	O
we	O
could	O
take	O
l	O
to	O
be	O
binomial	B
deviance	O
(	O
8.14	O
)	O
,	O
in	O
which	O
case	O
the	O
neural	O
network	O
amounts	O
to	O
a	O
penalized	O
logistic	O
regres-	O
sion	O
,	O
section	O
8.1	O
,	O
albeit	O
a	O
highly	O
parametrized	O
and	O
penalized	O
one	O
.	O
loss	O
functions	O
are	O
usually	O
convex	O
in	O
f	O
,	O
but	O
not	O
in	O
the	O
elements	O
of	O
w	O
,	O
so	O
solving	O
(	O
18.8	O
)	O
is	O
difﬁcult	O
,	O
and	O
at	O
best	O
we	O
seek	O
good	O
local	O
optima	O
.	O
most	O
methods	O
are	O
based	O
on	O
some	O
form	B
of	O
gradient	O
descent	O
,	O
with	O
many	O
associated	O
bells	O
and	O
whistles	O
.	O
we	O
brieﬂy	O
discuss	O
some	O
elements	O
of	O
the	O
current	O
practice	O
in	O
ﬁnding	O
good	O
solutions	O
to	O
(	O
18.8	O
)	O
.	O
computing	O
the	O
gradient	O
:	O
backpropagation	O
the	O
elements	O
of	O
w	O
occur	O
in	O
layers	O
,	O
since	O
f	O
.xi	O
w/	O
is	O
deﬁned	O
as	O
a	O
series	O
of	O
compositions	O
,	O
starting	O
from	O
the	O
input	O
layer	B
.	O
computing	O
the	O
gradient	O
is	O
also	O
done	O
most	O
naturally	O
in	O
layers	O
(	O
the	O
chain	O
rule	B
for	O
differentiation	O
;	O
see	O
for	O
example	O
(	O
18.10	O
)	O
in	O
algorithm	B
18.1	O
below	O
)	O
,	O
and	O
our	O
notation	O
makes	O
this	O
easier	O
to	O
describe	O
in	O
a	O
recursive	O
fashion	O
.	O
we	O
will	O
consider	O
computing	O
the	O
derivative	O
of	O
lœy	O
;	O
f	O
.xi	O
w	O
with	O
respect	O
to	O
any	O
of	O
the	O
elements	O
of	O
w	O
,	O
for	O
a	O
generic	O
input–output	O
pair	O
x	O
;	O
y	O
;	O
since	O
the	O
loss	O
part	O
of	O
the	O
objective	O
is	O
a	O
sum	O
,	O
18.2	O
fitting	O
a	O
neural	O
network	O
357	O
`	O
the	O
overall	O
gradient	O
will	O
be	O
the	O
sum	O
of	O
these	O
individual	O
gradient	O
elements	O
over	O
the	O
training	O
pairs	O
.xi	O
;	O
yi	O
/	O
.	O
the	O
intuition	O
is	O
as	O
follows	O
.	O
given	O
a	O
training	O
generic	O
pair	O
.x	O
;	O
y/	O
,	O
we	O
ﬁrst	O
make	O
a	O
forward	O
pass	O
through	O
the	O
network	O
,	O
which	O
creates	O
activations	O
at	O
each	O
of	O
the	O
nodes	B
a.k/	O
in	O
each	O
of	O
the	O
layers	O
,	O
including	O
the	O
ﬁnal	O
output	O
layer	B
.	O
we	O
would	O
then	O
like	O
to	O
compute	O
an	O
error	O
term	O
ı.k/	O
that	O
measures	O
the	O
responsibil-	O
ity	O
of	O
each	O
node	O
for	O
the	O
error	O
in	O
predicting	O
the	O
true	O
output	O
y.	O
for	O
the	O
output	O
activations	O
a.k/	O
these	O
errors	B
are	O
easy	O
:	O
either	O
residuals	O
or	O
generalized	O
resid-	O
uals	O
,	O
depending	O
on	O
the	O
loss	B
function	I
.	O
for	O
activations	O
at	O
inner	O
layers	O
,	O
ı.k/	O
will	O
be	O
a	O
weighted	O
sum	O
of	O
the	O
errors	B
terms	O
of	O
nodes	B
that	O
use	O
a.k/	O
as	O
inputs	O
.	O
the	O
backpropagation	O
algorithm	B
18.1	O
gives	O
the	O
details	O
for	O
computing	O
the	O
gradient	O
for	O
a	O
single	O
input–output	O
pair	O
x	O
;	O
y.	O
we	O
leave	O
it	O
to	O
the	O
reader	O
to	O
verify	O
that	O
this	O
indeed	O
implements	O
the	O
chain	O
rule	B
for	O
differentiation	O
.	O
`	O
`	O
`	O
`	O
algorithm	B
18.1	O
backpropagation	O
at	O
each	O
of	O
the	O
layers	O
l2	O
;	O
l3	O
;	O
:	O
:	O
:	O
;	O
lk	O
;	O
i.e	O
.	O
compute	O
f	O
.xi	O
1	O
given	O
a	O
pair	O
x	O
;	O
y	O
,	O
perform	O
a	O
“	O
feedforward	O
pass	O
,	O
”	O
computing	O
the	O
activa-	O
tions	O
a.k/	O
w/	O
at	O
x	O
using	O
the	O
current	O
w	O
,	O
saving	O
each	O
of	O
the	O
intermediary	O
quantities	O
along	O
the	O
way	O
.	O
`	O
2	O
for	O
each	O
output	O
unit	O
`	O
in	O
layer	B
lk	O
,	O
compute	O
d	O
@	O
lœy	O
;	O
f	O
.x	O
;	O
w/	O
d	O
@	O
lœy	O
;	O
f	O
.xi	O
@	O
z.k/	O
ı.k/	O
`	O
`	O
w/	O
pg.k/.z.k/	O
(	O
18.10	O
)	O
where	O
pg	O
denotes	O
the	O
derivative	O
of	O
g.z/	O
wrt	O
z.	O
for	O
example	O
for	O
l.y	O
;	O
f	O
/	O
d	O
ky	O
(	O
cid:0	O
)	O
f	O
k2	O
3	O
for	O
layers	O
k	O
d	O
k	O
(	O
cid:0	O
)	O
1	O
;	O
k	O
(	O
cid:0	O
)	O
2	O
;	O
:	O
:	O
:	O
;	O
2	O
,	O
and	O
for	O
each	O
node	O
`	O
in	O
layer	B
k	O
,	O
set	B
2	O
,	O
(	O
18.10	O
)	O
becomes	O
(	O
cid:0	O
)	O
.y`	O
(	O
cid:0	O
)	O
f`/	O
(	O
cid:1	O
)	O
pg.k/.z.k/	O
@	O
a.k/	O
/	O
.	O
/	O
;	O
1	O
2	O
`	O
`	O
0	O
@	O
pkc1x	O
jd1	O
`	O
1a	O
pg.k/.z.k/	O
`	O
/	O
:	O
d	O
ı.k/	O
`	O
j	O
`	O
ı.kc1/	O
w.k/	O
j	O
(	O
18.11	O
)	O
4	O
the	O
partial	O
derivatives	O
are	O
given	O
by	O
w/	O
@	O
lœy	O
;	O
f	O
.xi	O
@	O
w.k/	O
`j	O
d	O
a.k/	O
j	O
ı.kc1/	O
`	O
:	O
(	O
18.12	O
)	O
one	O
again	O
matrix–vector	O
notation	O
simpliﬁes	O
these	O
expressions	O
a	O
bit	O
:	O
358	O
neural	O
networks	O
(	O
18.10	O
)	O
becomes	O
(	O
for	O
squared-error	O
loss	O
)	O
ı.k/	O
d	O
(	O
cid:0	O
)	O
.y	O
(	O
cid:0	O
)	O
a.k//	O
ı	O
pg.k/.z.k//	O
;	O
where	O
ı	O
denotes	O
the	O
hadamard	O
(	O
elementwise	O
)	O
product	O
;	O
(	O
18.11	O
)	O
becomes	O
ı.kc1/	O
ı	O
pg.k/.z.k//i	O
0	O
ı.k/	O
d	O
(	O
cid:16	O
)	O
w	O
.k/	O
(	O
18.12	O
)	O
becomes	O
@	O
lœy	O
;	O
f	O
.xi	O
@	O
w	O
.k/	O
w/	O
d	O
ı.kc1/a.k/	O
0	O
:	O
(	O
18.13	O
)	O
(	O
18.14	O
)	O
(	O
18.15	O
)	O
backpropagation	O
was	O
considered	O
a	O
breakthrough	O
in	O
the	O
early	O
days	O
of	O
neural	O
networks	O
,	O
since	O
it	O
made	O
ﬁtting	B
a	O
complex	O
model	B
computationally	O
manageable	O
.	O
gradient	O
descent	O
algorithm	B
18.1	O
computes	O
the	O
gradient	O
of	O
the	O
loss	B
function	I
at	O
a	O
single	O
generic	O
pair	O
.x	O
;	O
y/	O
;	O
with	O
n	O
training	O
pairs	O
the	O
gradient	O
of	O
the	O
ﬁrst	O
part	O
of	O
(	O
18.8	O
)	O
is	O
given	O
by	O
w	O
:	O
(	O
18.16	O
)	O
nx	O
id1	O
@	O
lœyi	O
;	O
f	O
.xii	O
@	O
w	O
.k/	O
n	O
w	O
.k/	O
d	O
1	O
(	O
cid:16	O
)	O
w	O
.k/	O
c	O
(	O
cid:21	O
)	O
w	O
.k/	O
with	O
the	O
quadratic	O
form	B
(	O
18.9	O
)	O
for	O
the	O
penalty	B
,	O
a	O
gradient-descent	O
update	O
is	O
w	O
.k/	O
w	O
.k/	O
(	O
cid:0	O
)	O
˛	O
;	O
k	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
k	O
(	O
cid:0	O
)	O
1	O
;	O
(	O
18.17	O
)	O
where	O
˛	O
2	O
.0	O
;	O
1	O
is	O
the	O
learning	O
rate	O
.	O
gradient	O
descent	O
requires	O
starting	O
values	O
for	O
all	O
the	O
weights	O
w.	O
zero	O
is	O
not	O
an	O
option	O
,	O
because	O
each	O
layer	B
is	O
symmetric	O
in	O
the	O
weights	O
ﬂowing	O
to	O
the	O
different	O
neurons	O
,	O
hence	O
we	O
rely	O
on	O
starting	O
values	O
to	O
break	O
the	O
symmetries	O
.	O
typically	O
one	O
would	O
use	O
random	O
starting	O
weights	O
,	O
close	O
to	O
zero	O
;	O
random	O
uniform	O
or	O
gaussian	O
weights	O
are	O
common	O
.	O
there	O
are	O
a	O
multitude	O
of	O
“	O
tricks	O
of	O
the	O
trade	O
”	O
in	O
ﬁtting	B
or	O
“	O
learning	O
”	O
a	O
neural	O
network	O
,	O
and	O
many	O
of	O
them	O
are	O
connected	O
with	O
gradient	O
descent	O
.	O
here	O
we	O
list	O
some	O
of	O
these	O
,	O
without	O
going	O
into	O
great	O
detail	O
.	O
stochastic	O
gradient	O
descent	O
rather	O
than	O
process	O
all	O
the	O
observations	O
before	O
making	O
a	O
gradient	O
step	O
,	O
it	O
can	O
be	O
more	O
efﬁcient	O
to	O
process	O
smaller	O
batches	O
at	O
a	O
time—even	O
batches	O
18.2	O
fitting	O
a	O
neural	O
network	O
359	O
figure	O
18.4	O
training	O
and	O
test	O
misclassiﬁcation	O
error	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
epochs	O
of	O
training	O
,	O
for	O
the	O
mnist	O
digit	O
classiﬁcation	O
problem	O
.	O
the	O
architecture	O
for	O
the	O
network	O
is	O
shown	O
in	O
figure	O
18.3.	O
the	O
network	O
was	O
ﬁt	O
using	O
accelerated	O
gradient	O
descent	O
with	O
adaptive	B
rate	O
control	B
,	O
a	O
rectiﬁed	B
linear	I
activation	O
function	B
,	O
and	O
dropout	O
regularization	B
(	O
section	O
18.5	O
)	O
.	O
the	O
horizontal	O
broken	O
line	O
shows	O
the	O
error	O
rate	B
of	O
a	O
random	O
forest	O
(	O
section	O
17.1	O
)	O
.	O
a	O
logistic	B
regression	I
model	O
(	O
section	O
8.1	O
)	O
achieves	O
only	O
0.072	O
(	O
off	O
the	O
scale	B
)	O
.	O
of	O
size	O
one	O
!	O
these	O
batches	O
can	O
be	O
sampled	O
at	O
random	O
,	O
or	O
systematically	O
processed	O
.	O
for	O
large	O
data	B
sets	O
distributed	O
on	O
multiple	O
computer	O
cores	O
,	O
this	O
can	O
be	O
essential	O
for	O
reasons	O
of	O
efﬁciency	O
.	O
an	O
epoch	O
of	O
training	O
means	O
that	O
all	O
n	O
training	O
samples	O
have	O
been	O
used	O
in	O
gradient	O
steps	O
,	O
irrespective	O
of	O
how	O
they	O
have	O
been	O
grouped	O
(	O
and	O
hence	O
how	O
many	O
gradient	O
steps	O
have	O
been	O
made	O
)	O
.	O
accelerated	O
gradient	O
methods	O
the	O
idea	O
here	O
is	O
to	O
allow	O
previous	O
iterations	O
to	O
build	O
up	O
momentum	O
and	O
inﬂuence	O
the	O
current	O
iterations	O
.	O
the	O
iterations	O
have	O
the	O
form	B
vtc1	O
d	O
(	O
cid:22	O
)	O
vt	O
(	O
cid:0	O
)	O
˛.wt	O
c	O
(	O
cid:21	O
)	O
wt	O
/	O
;	O
wtc1	O
d	O
wt	O
c	O
vtc1	O
;	O
(	O
18.18	O
)	O
(	O
18.19	O
)	O
llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll020040060080010000.000.010.020.030.04epochsmisclassification	O
errorlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllltraintesttest	O
−	O
rf	O
neural	O
networks	O
360	O
using	O
wt	O
to	O
represent	O
the	O
entire	O
collection	O
of	O
weights	O
at	O
iteration	O
t.	O
vt	O
is	O
a	O
velocity	O
vector	B
that	O
accumulates	O
gradient	O
information	B
from	O
previous	O
iter-	O
ations	O
,	O
and	O
is	O
controlled	O
by	O
an	O
additional	O
momentum	O
parameter	O
(	O
cid:22	O
)	O
.	O
when	O
correctly	O
tuned	O
,	O
accelerated	O
gradient	O
descent	O
can	O
achieve	O
much	O
faster	O
con-	O
vergence	O
rates	O
;	O
however	O
,	O
tuning	O
tends	O
to	O
be	O
a	O
difﬁcult	O
process	O
,	O
and	O
is	O
typi-	O
cally	O
done	O
adaptively	O
.	O
rate	B
annealing	O
3	O
a	O
variety	O
of	O
creative	O
methods	O
have	O
been	O
proposed	O
to	O
adapt	O
the	O
learning	O
rate	O
to	O
avoid	O
jumping	O
across	O
good	O
local	O
minima	O
.	O
these	O
tend	O
to	O
be	O
a	O
mixture	O
of	O
principled	O
approaches	O
combined	O
with	O
ad-hoc	O
adaptations	O
that	O
tend	O
to	O
work	O
well	O
in	O
practice	O
.	O
	O
figure	O
18.4	O
shows	O
the	O
performance	O
of	O
our	O
neural	O
net	O
on	O
the	O
mnist	O
digit	O
data	B
.	O
this	O
achieves	O
state-of-the	O
art	O
misclassiﬁcation	O
error	O
rates	O
on	O
these	O
data	B
(	O
just	O
under	O
0.93	O
%	O
errors	B
)	O
,	O
and	O
outperforms	O
random	O
forests	O
(	O
2.8	O
%	O
)	O
and	O
a	O
generalized	O
linear	B
model	I
(	O
7.2	O
%	O
)	O
.	O
figure	O
18.5	O
shows	O
the	O
93	O
misclassiﬁed	O
digits	O
.	O
figure	O
18.5	O
all	O
93	O
misclassiﬁed	O
digits	O
in	O
the	O
mnist	O
test	O
set	B
.	O
the	O
true	O
digit	O
class	O
is	O
labeled	O
in	O
blue	O
,	O
the	O
predicted	O
in	O
red	O
.	O
425360825897896572467294499571578379874693063793239453203749619091942461539561653295359712496037916450857246134603972732878961809472704953383839899771079585053985497272720897274763564250	O
18.2	O
fitting	O
a	O
neural	O
network	O
361	O
other	O
tuning	O
parameters	O
apart	O
from	O
the	O
many	O
details	O
associated	O
with	O
gradient	O
descent	O
,	O
there	O
are	O
sev-	O
eral	O
other	O
important	O
structural	O
and	O
operational	O
aspects	O
of	O
neural	O
networks	O
that	O
have	O
to	O
be	O
speciﬁed	O
.	O
number	B
of	I
hidden	I
layers	I
,	O
and	O
their	O
sizes	O
with	O
a	O
single	O
hidden	O
layer	B
,	O
the	O
number	O
of	O
hidden	O
units	O
determines	O
the	O
num-	O
ber	O
of	O
parameters	O
.	O
in	O
principle	O
,	O
one	O
could	O
treat	O
this	O
number	O
as	O
a	O
tuning	O
parameter	O
,	O
which	O
could	O
be	O
adjusted	O
to	O
avoid	O
overﬁtting	O
.	O
the	O
current	O
col-	O
lective	O
wisdom	O
suggests	O
it	O
is	O
better	O
to	O
have	O
an	O
abundant	O
number	O
of	O
hidden	O
units	O
,	O
and	O
control	O
the	O
model	B
complexity	O
instead	O
by	O
weight	O
regularization	B
.	O
having	O
deeper	O
networks	O
(	O
more	O
hidden	O
layers	O
)	O
increases	O
the	O
complexity	O
as	O
well	O
.	O
the	O
correct	O
number	O
tends	O
to	O
be	O
task	O
speciﬁc	O
;	O
having	O
two	O
hidden	O
lay-	O
ers	O
with	O
the	O
digit	O
recognition	O
problem	O
leads	O
to	O
competitive	O
performance	O
.	O
choice	O
of	O
nonlinearities	O
there	O
are	O
a	O
number	O
of	O
activation	O
functions	O
g.k/	O
in	O
current	O
use	O
.	O
apart	O
from	O
the	O
sigmoid	O
function	B
,	O
which	O
transforms	O
its	O
input	O
to	O
a	O
values	O
in	O
.0	O
;	O
1/	O
,	O
other	O
popular	O
choices	O
are	O
figure	O
18.6	O
activation	O
functions	O
.	O
relu	O
is	O
a	O
rectiﬁed	B
linear	I
(	O
unit	O
)	O
.	O
tanh	B
:	O
g.z/	O
d	O
ez	O
(	O
cid:0	O
)	O
e	O
(	O
cid:0	O
)	O
z	O
ez	O
c	O
e	O
(	O
cid:0	O
)	O
z	O
;	O
−2−1012−1.0−0.50.00.51.0zg	O
(	O
z	O
)	O
sigmoidtanhreluleaky	O
relu	O
362	O
which	O
delivers	O
values	O
in	O
.	O
(	O
cid:0	O
)	O
1	O
;	O
1/	O
.	O
neural	O
networks	O
rectiﬁed	B
linear	I
:	O
g.z/	O
d	O
zc	O
;	O
or	O
the	O
positive-part	O
function	B
.	O
this	O
has	O
the	O
advantage	O
of	O
making	O
the	O
gra-	O
dient	O
computations	B
cheaper	O
to	O
compute	O
.	O
leaky	B
rectiﬁed	I
linear	I
:	O
g˛.z/	O
d	O
zc	O
(	O
cid:0	O
)	O
˛z	O
(	O
cid:0	O
)	O
;	O
for	O
˛	O
nonnegative	O
and	O
close	O
to	O
zero	O
.	O
the	O
rectiﬁed	B
linear	I
tends	O
to	O
have	O
ﬂat	O
spots	O
,	O
because	O
of	O
the	O
many	O
zero	O
activations	O
;	O
this	O
is	O
an	O
attempt	O
to	O
avoid	O
these	O
and	O
the	O
accompanying	O
zero	O
gradients	O
.	O
choice	O
of	O
regularization	B
typically	O
this	O
is	O
a	O
mixture	O
of	O
`2	O
and	O
`1	O
regularization	B
,	O
each	O
of	O
which	O
re-	O
quires	O
a	O
tuning	O
parameter	O
.	O
as	O
in	O
lasso	B
and	O
regression	B
applications	O
,	O
the	O
bias	O
terms	O
(	O
intercepts	O
)	O
are	O
usually	O
not	O
regularized	O
.	O
the	O
weight	O
regularization	B
is	O
typically	O
light	O
,	O
and	O
serves	O
several	O
roles	O
.	O
the	O
`2	O
reduces	O
problems	O
with	O
collinearity	O
,	O
the	O
`1	O
can	O
ignore	O
irrelevant	O
features	O
,	O
and	O
both	O
slow	O
the	O
rate	B
of	O
overﬁtting	O
,	O
especially	O
with	O
deep	O
(	O
over-parametrized	O
)	O
networks	O
.	O
early	O
stopping	O
neural	O
nets	O
are	O
typically	O
over-parametrized	O
,	O
and	O
hence	O
are	O
prone	O
to	O
overﬁt-	O
ting	O
.	O
originally	O
early	O
stopping	O
was	O
set	B
up	O
as	O
the	O
primary	O
tuning	O
parameter	O
,	O
and	O
the	O
stopping	O
time	O
was	O
determined	O
using	O
a	O
held-out	O
set	B
of	O
validation	O
data	B
.	O
in	O
modern	O
networks	O
the	O
regularization	B
is	O
tuned	O
adaptively	O
to	O
avoid	O
overﬁtting	O
,	O
and	O
hence	O
it	O
is	O
less	O
of	O
a	O
problem	O
.	O
for	O
example	O
,	O
in	O
figure	O
18.4	O
we	O
see	O
that	O
the	O
test	O
misclassiﬁcation	O
error	O
has	O
ﬂattened	O
out	O
,	O
and	O
does	O
not	O
rise	O
again	O
with	O
increasing	O
number	O
of	O
epochs	O
.	O
18.3	O
autoencoders	O
an	O
autoencoder	O
is	O
a	O
special	O
neural	O
network	O
for	O
computing	O
a	O
type	O
of	O
non-	O
linear	B
principal-component	O
decomposition	O
.	O
the	O
linear	B
principal	O
component	O
decomposition	O
is	O
a	O
popular	O
and	O
effective	O
linear	B
method	O
for	O
reducing	O
a	O
large	O
set	B
of	O
correlated	O
variables	O
to	O
a	O
typically	O
smaller	O
number	O
of	O
linear	O
combinations	O
that	O
capture	O
most	O
of	O
the	O
variance	O
in	O
the	O
original	O
set	B
.	O
hence	O
,	O
given	O
a	O
collection	O
of	O
n	O
vectors	O
xi	O
2	O
rp	O
(	O
assumed	O
to	O
have	O
mean	O
zero	O
)	O
,	O
we	O
produce	O
a	O
derived	O
set	B
of	O
uncorrelated	O
features	O
zi	O
2	O
rq	O
18.3	O
autoencoders	O
363	O
0	O
(	O
q	O
	O
p	O
,	O
and	O
typically	O
smaller	O
)	O
via	O
zi	O
d	O
v	O
xi	O
.	O
the	O
columns	O
of	O
v	O
are	O
or-	O
thonormal	O
,	O
and	O
are	O
derived	O
such	O
that	O
the	O
ﬁrst	O
component	O
of	O
zi	O
has	O
maximal	O
variance	O
,	O
the	O
second	O
has	O
the	O
next	O
largest	O
variance	O
and	O
is	O
uncorrelated	O
with	O
the	O
ﬁrst	O
,	O
and	O
so	O
on	O
.	O
it	O
is	O
easy	O
to	O
show	O
that	O
the	O
columns	O
of	O
v	O
are	O
the	O
leading	O
q	O
eigenvectors	O
of	O
the	O
sample	B
covariance	O
matrix	B
s	O
d	O
1	O
n	O
x	O
principal	B
components	I
can	O
also	O
be	O
derived	O
in	O
terms	O
of	O
a	O
best-approx-	O
imating	O
linear	B
subspace	O
,	O
and	O
it	O
is	O
this	O
version	O
that	O
leads	O
to	O
the	O
nonlinear	B
generalization	O
presented	O
here	O
.	O
consider	O
the	O
optimization	O
problem	O
x	O
.	O
0	O
minimize	O
a2rp	O
(	O
cid:2	O
)	O
q	O
;	O
f	O
(	O
cid:13	O
)	O
ign	O
2rq	O
(	O
cid:2	O
)	O
n	O
1	O
kxi	O
(	O
cid:0	O
)	O
a	O
(	O
cid:13	O
)	O
ik2	O
2	O
;	O
(	O
18.20	O
)	O
nx	O
id1	O
for	O
q	O
<	O
p.	O
the	O
subspace	O
is	O
deﬁned	O
by	O
the	O
column	O
space	B
of	O
a	O
,	O
and	O
for	O
each	O
point	O
xi	O
we	O
wish	O
to	O
locate	O
its	O
best	O
approximation	O
in	O
the	O
subspace	O
(	O
in	O
terms	O
of	O
euclidean	O
distance	O
)	O
.	O
without	O
loss	O
of	O
generality	O
,	O
we	O
can	O
assume	O
a	O
has	O
orthonormal	O
columns	O
,	O
in	O
which	O
case	O
o	O
(	O
cid:13	O
)	O
i	O
d	O
a	O
0	O
xi	O
for	O
each	O
i	O
(	O
n	O
separate	O
linear	B
regressions	O
)	O
.	O
plugging	O
in	O
,	O
(	O
18.20	O
)	O
reduces	O
to	O
kxi	O
(	O
cid:0	O
)	O
aa	O
(	O
18.21	O
)	O
a	O
solution	O
is	O
given	O
by	O
oa	O
d	O
v	O
,	O
the	O
matrix	B
above	O
of	O
the	O
ﬁrst	O
q	O
principal-	O
component	O
direction	O
vectors	O
computed	O
from	O
the	O
xi	O
.	O
by	O
analogy	O
,	O
a	O
single-	O
layer	B
autoencoder	O
solves	O
a	O
nonlinear	B
version	O
of	O
this	O
problem	O
:	O
nx	O
a2rp	O
(	O
cid:2	O
)	O
q	O
;	O
a	O
xik2	O
2	O
:	O
minimize	O
adiq	O
id1	O
0	O
minimize	O
w	O
2rq	O
(	O
cid:2	O
)	O
p	O
kxi	O
(	O
cid:0	O
)	O
w	O
0	O
g.w	O
xi	O
/k2	O
2	O
;	O
(	O
18.22	O
)	O
0	O
nx	O
id1	O
for	O
some	O
nonlinear	B
activation	O
function	B
g	O
;	O
see	O
figure	O
18.7	O
(	O
left	O
panel	O
)	O
.	O
if	O
g	O
is	O
the	O
identity	O
function	B
,	O
these	O
solutions	O
coincide	O
(	O
with	O
w	O
d	O
v	O
0	O
)	O
.	O
figure	O
18.7	O
(	O
right	O
panel	O
)	O
represents	O
the	O
learned	O
row	O
of	O
w	O
as	O
images	O
,	O
when	O
the	O
autoencoder	O
is	O
ﬁt	O
to	O
the	O
mnist	O
digit	O
database	O
.	O
since	O
autoen-	O
coders	O
do	O
not	O
require	O
a	O
response	O
(	O
the	O
class	O
labels	O
in	O
this	O
case	O
)	O
,	O
this	O
decom-	O
position	O
is	O
unsupervised	O
.	O
it	O
is	O
often	O
expensive	O
to	O
label	O
images	O
,	O
for	O
example	O
,	O
while	O
unlabeled	O
images	O
are	O
abundant	O
.	O
autoencoders	O
provide	O
a	O
means	O
for	O
extracting	O
potentially	O
useful	O
features	O
from	O
such	O
data	B
,	O
which	O
can	O
then	O
be	O
used	O
with	O
labeled	O
data	B
to	O
train	O
a	O
classiﬁer	O
.	O
in	O
fact	O
,	O
they	O
are	O
often	O
used	O
as	O
warm	O
starts	O
for	O
the	O
weights	O
when	O
ﬁtting	B
a	O
supervised	O
neural	O
network	O
.	O
once	O
again	O
there	O
are	O
a	O
number	O
of	O
bells	O
and	O
whistles	O
that	O
make	O
autoen-	O
coders	O
more	O
effective	O
.	O
364	O
neural	O
networks	O
figure	O
18.7	O
left	O
:	O
network	O
representation	O
of	O
an	O
autoencoder	O
used	O
for	O
unsupervised	O
learning	O
of	O
nonlinear	B
principal	O
components	O
.	O
the	O
middle	O
layer	B
of	O
hidden	O
units	O
creates	O
a	O
bottleneck	O
,	O
and	O
learns	O
nonlinear	B
representations	O
of	O
the	O
inputs	O
.	O
the	O
output	O
layer	B
is	O
the	O
transpose	O
of	O
the	O
input	O
layer	B
,	O
so	O
the	O
network	O
tries	O
to	O
reproduce	O
the	O
input	O
data	B
using	O
this	O
restrictive	O
representation	O
.	O
right	O
:	O
images	O
representing	O
the	O
estimated	O
rows	O
of	O
w	O
using	O
the	O
mnist	O
database	O
;	O
the	O
images	O
can	O
be	O
seen	O
as	O
ﬁlters	B
that	O
detect	O
local	O
gradients	O
in	O
the	O
image	O
pixels	O
.	O
in	O
each	O
image	O
,	O
most	O
of	O
the	O
weights	O
are	O
zero	O
,	O
and	O
the	O
nonzero	O
weights	O
are	O
localized	O
in	O
the	O
two-dimensional	O
image	O
space	B
.	O
(	O
cid:15	O
)	O
`1	B
regularization	I
applied	O
to	O
the	O
rows	O
of	O
w	O
lead	O
to	O
sparse	O
weight	O
vectors	O
,	O
and	O
hence	O
local	O
features	O
,	O
as	O
was	O
the	O
case	O
in	O
our	O
example	O
.	O
(	O
cid:15	O
)	O
denoising	O
is	O
a	O
process	O
where	O
noise	O
is	O
added	O
to	O
the	O
input	O
layer	B
(	O
but	O
not	O
the	O
output	O
)	O
,	O
resulting	O
in	O
features	O
that	O
do	O
not	O
focus	O
on	O
isolated	O
values	O
,	O
such	O
as	O
pixels	O
,	O
but	O
instead	O
have	O
some	O
volume	O
.	O
we	O
discuss	O
denoising	O
further	O
in	O
section	O
18.5	O
.	O
(	O
cid:15	O
)	O
with	O
regularization	B
,	O
the	O
bottleneck	O
is	O
not	O
necessary	O
,	O
as	O
in	O
the	O
ﬁgure	O
or	O
in	O
principal	B
components	I
.	O
in	O
fact	O
we	O
can	O
learn	O
many	O
more	O
than	O
p	O
compo-	O
nents	O
.	O
(	O
cid:15	O
)	O
autoencoders	O
can	O
also	O
have	O
multiple	O
layers	O
,	O
which	O
are	O
typically	O
learned	O
sequentially	O
.	O
the	O
activations	O
learned	O
in	O
the	O
ﬁrst	O
layer	B
are	O
treated	O
as	O
the	O
input	O
(	O
and	O
output	O
)	O
features	O
,	O
and	O
a	O
model	B
like	O
(	O
18.22	O
)	O
is	O
ﬁt	O
to	O
them	O
.	O
18.4	O
deep	O
learning	O
neural	O
networks	O
were	O
reincarnated	O
around	O
2010	O
with	O
“	O
deep	O
learning	O
”	O
as	O
a	O
ﬂashier	O
name	O
,	O
largely	O
a	O
result	O
of	O
much	O
faster	O
and	O
larger	O
computing	O
systems	O
,	O
plus	O
a	O
few	O
new	O
ideas	O
.	O
they	O
have	O
been	O
shown	O
to	O
be	O
particularly	O
successful	O
x1x2x3x4x5ˆx1ˆx2ˆx3ˆx4ˆx5inputlayerhiddenlayeroutputlayerwg	O
(	O
wx	O
)	O
w0	O
18.4	O
deep	O
learning	O
365	O
in	O
the	O
difﬁcult	O
task	O
of	O
classifying	O
natural	O
images	O
,	O
using	O
what	O
is	O
known	O
as	O
a	O
convolutional	O
architecture	O
.	O
initially	O
autoencoders	O
were	O
considered	O
a	O
crucial	O
aspect	O
of	O
deep	O
learning	O
,	O
since	O
unlabeled	O
images	O
are	O
abundant	O
.	O
however	O
,	O
as	O
labeled	O
corpora	O
become	O
more	O
available	O
,	O
the	O
word	O
on	O
the	O
street	O
is	O
that	O
supervised	O
learning	O
is	O
sufﬁcient	O
.	O
figure	O
18.8	O
shows	O
examples	O
of	O
natural	O
images	O
,	O
each	O
with	O
a	O
class	O
label	O
such	O
as	O
beaver	O
,	O
sunflower	O
,	O
trout	O
etc	O
.	O
there	O
are	O
100	O
class	O
labels	O
in	O
figure	O
18.8	O
examples	O
of	O
natural	O
images	O
.	O
the	O
cifar-100	O
database	O
consists	O
of	O
100	O
color	O
image	O
classes	O
,	O
with	O
600	O
examples	O
in	O
each	O
class	O
(	O
500	O
train	O
,	O
100	O
test	O
)	O
.	O
each	O
image	O
is	O
32	O
(	O
cid:2	O
)	O
32	O
(	O
cid:2	O
)	O
3	O
(	O
red	O
,	O
green	O
,	O
blue	O
)	O
.	O
here	O
we	O
display	O
a	O
randomly	O
chosen	O
image	O
from	O
each	O
class	O
.	O
the	O
classes	O
are	O
organized	O
by	O
hierarchical	O
structure	O
,	O
with	O
20	O
coarse	O
levels	O
and	O
ﬁve	O
subclasses	O
within	O
each	O
.	O
so	O
,	O
for	O
example	O
,	O
the	O
ﬁrst	O
ﬁve	O
images	O
in	O
the	O
ﬁrst	O
column	O
are	O
aquatic	O
mammals	O
,	O
namely	O
beaver	O
,	O
dolphin	O
,	O
otter	O
,	O
seal	O
and	O
whale	O
.	O
366	O
neural	O
networks	O
all	O
,	O
and	O
500	O
training	O
images	O
and	O
100	O
test	O
images	O
per	O
class	O
.	O
the	O
goal	O
is	O
to	O
build	O
a	O
classiﬁer	O
to	O
assign	O
a	O
label	O
to	O
an	O
image	O
.	O
we	O
present	O
the	O
essential	O
details	O
of	O
a	O
deep-learning	O
network	O
for	O
this	O
task—one	O
that	O
achieves	O
a	O
re-	O
spectable	O
classiﬁcation	O
performance	O
of	O
35	O
%	O
errors	B
on	O
the	O
designated	O
test	O
set.2	O
figure	O
18.9	O
shows	O
a	O
typical	O
deep-learning	O
architecture	O
,	O
with	O
many	O
figure	O
18.9	O
architecture	O
of	O
a	O
deep-learning	O
network	O
for	O
the	O
cifar-100	O
image	O
classiﬁcation	O
task	O
.	O
the	O
input	O
layer	B
and	O
hidden	O
layers	O
are	O
all	O
represented	O
as	O
images	O
,	O
except	O
for	O
the	O
last	O
hidden	O
layer	B
,	O
which	O
is	O
“	O
ﬂattened	O
”	O
(	O
vectorized	O
)	O
.	O
the	O
input	O
layer	B
consists	O
of	O
the	O
p1	O
d	O
3	O
color	O
(	O
red	O
,	O
green	O
,	O
and	O
blue	O
)	O
versions	O
of	O
an	O
input	O
image	O
(	O
unlike	O
earlier	O
,	O
here	O
we	O
use	O
the	O
pk	O
to	O
refer	O
to	O
the	O
number	O
of	O
images	O
rather	O
than	O
the	O
totality	O
of	O
pixels	O
)	O
.	O
each	O
of	O
these	O
color	O
panes	O
is	O
32	O
(	O
cid:2	O
)	O
32	O
pixels	O
in	O
dimension	O
.	O
the	O
ﬁrst	O
hidden	O
layer	B
computes	O
a	O
convolution	O
using	O
a	O
bank	O
of	O
p2	O
distinct	O
q	O
(	O
cid:2	O
)	O
q	O
(	O
cid:2	O
)	O
p1	O
learned	O
ﬁlters	B
,	O
producing	O
an	O
array	O
of	O
images	O
of	O
dimension	O
p2	O
(	O
cid:2	O
)	O
32	O
(	O
cid:2	O
)	O
32.	O
the	O
next	O
pool	O
layer	B
reduces	O
each	O
non-overlapping	O
block	O
of	O
`	O
(	O
cid:2	O
)	O
`	O
numbers	O
in	O
each	O
pane	O
of	O
the	O
ﬁrst	O
hidden	O
layer	B
to	O
a	O
single	O
number	O
using	O
a	O
“	O
max	O
”	O
operation	O
.	O
both	O
q	O
and	O
`	O
are	O
typically	O
small	O
;	O
each	O
was	O
2	O
for	O
us	O
.	O
these	O
convolve	O
and	O
pool	O
layers	O
are	O
repeated	O
here	O
three	O
times	O
,	O
with	O
changing	O
dimensions	O
(	O
in	O
our	O
actual	O
implementation	O
,	O
there	O
are	O
13	O
layers	O
in	O
total	O
)	O
.	O
finally	O
the	O
500	O
derived	O
features	O
are	O
ﬂattened	O
,	O
and	O
a	O
fully	O
connected	O
layer	B
maps	O
them	O
to	O
the	O
100	O
classes	O
via	O
a	O
“	O
softmax	O
”	O
activation	O
.	O
hidden	O
layers	O
.	O
these	O
consist	O
of	O
two	O
special	O
types	O
of	O
layers	O
:	O
“	O
convolve	O
”	O
and	O
“	O
pool.	O
”	O
we	O
describe	O
each	O
in	O
turn	O
.	O
convolve	O
layer	B
figure	O
18.10	O
illustrates	O
a	O
convolution	O
layer	B
,	O
and	O
some	O
details	O
are	O
given	O
in	O
2	O
classiﬁcation	O
becomes	O
increasingly	O
difﬁcult	O
as	O
the	O
number	O
of	O
classes	O
grows	O
.	O
with	O
equal	O
representation	O
in	O
each	O
class	O
,	O
the	O
null	O
or	O
random	O
error	O
rate	B
for	O
k	O
classes	O
is	O
.k	O
(	O
cid:0	O
)	O
1/=k	O
;	O
50	O
%	O
for	O
two	O
classes	O
,	O
99	O
%	O
for	O
100	O
.	O
100323221650048convolvepoolconvolvepoolconvolvepoolconnect	O
fully	O
367	O
the	O
caption	O
.	O
if	O
an	O
image	O
x	O
is	O
represented	O
by	O
a	O
k	O
(	O
cid:2	O
)	O
k	O
matrix	B
,	O
and	O
a	O
ﬁlter	O
f	O
18.4	O
deep	O
learning	O
figure	O
18.10	O
convolution	O
layer	B
for	O
the	O
input	O
images	O
.	O
the	O
input	O
image	O
is	O
split	O
into	O
its	O
three	O
color	O
components	O
.	O
a	O
single	O
ﬁlter	O
is	O
a	O
q	O
(	O
cid:2	O
)	O
q	O
(	O
cid:2	O
)	O
p1	O
array	O
(	O
here	O
one	O
q	O
(	O
cid:2	O
)	O
q	O
for	O
each	O
of	O
the	O
p1	O
d	O
3	O
color	O
panes	O
)	O
,	O
and	O
is	O
used	O
to	O
compute	O
an	O
inner	O
product	O
with	O
a	O
correspondingly	O
sized	O
subimage	O
in	O
each	O
pane	O
,	O
and	O
summed	O
across	O
the	O
p1	O
panes	O
.	O
we	O
used	O
q	O
d	O
2	O
,	O
and	O
small	O
values	O
are	O
typical	O
.	O
this	O
is	O
repeated	O
over	O
all	O
(	O
overlapping	O
)	O
q	O
(	O
cid:2	O
)	O
q	O
subimages	O
(	O
with	O
boundary	O
padding	O
)	O
,	O
and	O
hence	O
produces	O
an	O
image	O
of	O
the	O
same	O
dimension	O
as	O
one	O
of	O
the	O
input	O
panes	O
.	O
this	O
is	O
the	O
convolution	O
operation	O
.	O
there	O
are	O
p2	O
different	O
versions	O
of	O
this	O
ﬁlter	O
,	O
and	O
hence	O
p2	O
new	O
panes	O
are	O
produced	O
.	O
each	O
of	O
the	O
p2	O
ﬁlters	B
has	O
p1q2	O
weights	O
,	O
which	O
are	O
learned	O
via	O
backpropagation	O
.	O
0d1	O
xic`	O
;	O
jc`	O
`	O
0	O
f`	O
;	O
`	O
`d1	O
pq	O
qx	O
with	O
elements	O
qxi	O
;	O
j	O
dpq	O
is	O
a	O
q	O
(	O
cid:2	O
)	O
q	O
matrix	B
with	O
q	O
(	O
cid:28	O
)	O
k	O
,	O
the	O
convolved	O
image	O
is	O
another	O
k	O
(	O
cid:2	O
)	O
k	O
matrix	B
0	O
(	O
with	O
edge	O
padding	O
to	O
achieve	O
a	O
full-sized	O
k	O
(	O
cid:2	O
)	O
k	O
output	O
image	O
)	O
.	O
in	O
our	O
application	O
we	O
used	O
2	O
(	O
cid:2	O
)	O
2	O
,	O
but	O
other	O
sizes	O
such	O
as	O
3	O
(	O
cid:2	O
)	O
3	O
are	O
popular	O
.	O
it	O
is	O
most	O
natural	O
to	O
represent	O
the	O
structure	O
in	O
terms	O
of	O
these	O
images	O
as	O
in	O
figure	O
18.9	O
,	O
but	O
they	O
could	O
all	O
be	O
vectorized	O
into	O
a	O
massive	O
network	O
diagram	O
as	O
in	O
figures	O
18.1	O
and	O
18.3.	O
however	O
,	O
the	O
weights	O
would	O
have	O
special	O
sparse	O
structure	O
,	O
with	O
most	O
being	O
zero	O
,	O
and	O
the	O
nonzero	O
values	O
repeated	O
(	O
“	O
weight	O
sharing	B
”	O
)	O
.	O
++	O
...	O
...	O
...	O
368	O
neural	O
networks	O
pool	O
layer	B
the	O
pool	O
layer	B
corresponds	O
to	O
a	O
kind	O
of	O
nonlinear	B
activation	O
.	O
it	O
reduces	O
each	O
nonoverlapping	O
block	O
of	O
r	O
(	O
cid:2	O
)	O
r	O
pixels	O
(	O
r	O
d	O
2	O
for	O
us	O
)	O
to	O
a	O
single	O
number	O
by	O
computing	O
their	O
maximum	O
.	O
why	O
maximum	O
?	O
the	O
convolution	O
ﬁlters	B
are	O
themselves	O
small	O
image	O
patches	O
,	O
and	O
are	O
looking	O
to	O
identify	O
similar	O
patches	O
in	O
the	O
target	O
image	O
(	O
in	O
which	O
case	O
the	O
inner	O
product	O
will	O
be	O
high	O
)	O
.	O
the	O
max	O
operation	O
introduces	O
an	O
element	O
of	O
local	O
translation	O
invariance	O
.	O
the	O
pool	O
operation	O
reduces	O
the	O
size	O
of	O
each	O
image	O
by	O
a	O
factor	B
r	O
in	O
each	O
dimension	O
.	O
to	O
compensate	O
,	O
the	O
number	O
of	O
tiles	O
in	O
the	O
next	O
convolution	O
layer	B
is	O
typically	O
increased	O
accordingly	O
.	O
also	O
,	O
as	O
these	O
tiles	O
get	O
smaller	O
,	O
the	O
effective	O
weights	O
resulting	O
from	O
the	O
convolution	O
operator	O
become	O
denser	O
.	O
eventually	O
the	O
tiles	O
are	O
the	O
same	O
size	O
as	O
the	O
convolution	O
ﬁlter	O
,	O
and	O
the	O
layer	B
becomes	O
fully	O
connected	O
.	O
18.5	O
learning	O
a	O
deep	O
network	O
despite	O
the	O
additional	O
structure	O
imposed	O
by	O
the	O
convolution	O
layers	O
,	O
deep	O
networks	O
are	O
learned	O
by	O
gradient	O
descent	O
.	O
the	O
gradients	O
are	O
computed	O
by	O
backpropagation	O
as	O
before	O
,	O
but	O
with	O
special	O
care	O
taken	O
to	O
accommodate	O
the	O
tied	O
weights	O
in	O
the	O
convolution	O
ﬁlters	B
.	O
however	O
,	O
a	O
number	O
of	O
additional	O
tricks	O
have	O
been	O
introduced	O
that	O
appear	O
to	O
improve	O
the	O
performance	O
of	O
modern	O
deep	O
learning	O
networks	O
.	O
these	O
are	O
mostly	O
aimed	O
at	O
regularization	B
;	O
indeed	O
,	O
our	O
100-class	O
image	O
network	O
has	O
around	O
50	O
million	O
parameters	O
,	O
so	O
regularization	B
is	O
essential	O
to	O
avoid	O
overﬁtting	O
.	O
we	O
brieﬂy	O
discuss	O
some	O
of	O
these	O
.	O
dropout	O
this	O
is	O
a	O
form	B
of	O
regularization	B
that	O
is	O
performed	O
when	O
learning	O
a	O
network	O
,	O
typically	O
at	O
different	O
rates	O
at	O
the	O
different	O
layers	O
.	O
it	O
applies	O
to	O
all	O
networks	O
,	O
not	O
just	O
convolutional	O
;	O
in	O
fact	O
,	O
it	O
appears	O
to	O
work	O
better	O
when	O
applied	O
at	O
the	O
deeper	O
,	O
denser	O
layers	O
.	O
consider	O
computing	O
the	O
activation	O
z.k/	O
in	O
layer	B
k	O
as	O
in	O
(	O
18.3	O
)	O
for	O
a	O
single	O
observation	O
during	O
the	O
feed-forward	O
stage	O
.	O
the	O
idea	O
is	O
to	O
randomly	O
set	B
each	O
of	O
the	O
pk	O
(	O
cid:0	O
)	O
1	O
nodes	B
a.k	O
(	O
cid:0	O
)	O
1/	O
to	O
zero	O
with	O
probability	O
(	O
cid:30	O
)	O
,	O
and	O
inﬂate	O
the	O
remaining	O
ones	O
by	O
a	O
factor	B
1=.1	O
(	O
cid:0	O
)	O
(	O
cid:30	O
)	O
/	O
.	O
hence	O
,	O
for	O
this	O
ob-	O
servation	O
,	O
those	O
nodes	B
that	O
survive	O
have	O
to	O
stand	O
in	O
for	O
those	O
omitted	O
.	O
this	O
can	O
be	O
shown	O
to	O
be	O
a	O
form	B
of	O
ridge	O
regularization	O
,	O
and	O
when	O
done	O
correctly	O
improves	O
performance	O
.	O
	O
the	O
fraction	O
(	O
cid:30	O
)	O
omitted	O
is	O
a	O
tuning	O
parameter	O
,	O
and	O
for	O
convolutional	O
networks	O
it	O
appears	O
to	O
be	O
better	O
to	O
use	O
different	O
values	O
at	O
`	O
j	O
4	O
18.5	O
learning	O
a	O
deep	O
network	O
369	O
different	O
layers	O
.	O
in	O
particular	O
,	O
as	O
the	O
layers	O
become	O
denser	O
,	O
(	O
cid:30	O
)	O
is	O
increased	O
:	O
from	O
0	O
in	O
the	O
input	O
layer	B
to	O
0:5	O
in	O
the	O
ﬁnal	O
,	O
fully	O
connected	O
layer	B
.	O
input	O
distortion	O
this	O
is	O
another	O
form	B
of	O
regularization	B
that	O
is	O
particularly	O
suitable	O
for	O
tasks	O
like	O
image	O
classiﬁcation	O
.	O
the	O
idea	O
is	O
to	O
augment	O
the	O
training	O
set	B
with	O
many	O
distorted	O
copies	O
of	O
an	O
input	O
image	O
(	O
but	O
of	O
course	O
the	O
same	O
label	O
)	O
.	O
these	O
distortions	O
can	O
be	O
location	O
shifts	O
and	O
other	O
small	O
afﬁne	O
transformations	O
,	O
but	O
also	O
color	O
and	O
shading	O
shifts	O
that	O
might	O
appear	O
in	O
natural	O
images	O
.	O
we	O
show	O
figure	O
18.11	O
each	O
column	O
represents	O
distorted	O
versions	O
of	O
an	O
input	O
image	O
,	O
including	O
afﬁne	O
and	O
color	O
distortions	O
.	O
the	O
input	O
images	O
are	O
padded	O
on	O
the	O
boundary	O
to	O
increase	O
the	O
size	O
,	O
and	O
hence	O
allow	O
space	B
for	O
some	O
of	O
the	O
distortions	O
.	O
some	O
distorted	O
versions	O
of	O
input	O
images	O
in	O
figure	O
18.11.	O
the	O
distortions	O
are	O
such	O
that	O
a	O
human	O
would	O
have	O
no	O
trouble	O
identifying	O
any	O
of	O
the	O
distorted	O
images	O
if	O
they	O
could	O
identify	O
the	O
original	O
.	O
	O
this	O
both	O
enriches	O
the	O
training	O
5	O
data	B
with	O
hints	O
,	O
and	O
also	O
prevents	O
overﬁtting	O
to	O
the	O
original	O
image	O
.	O
one	O
could	O
also	O
apply	O
distortions	O
to	O
a	O
test	O
image	O
,	O
and	O
then	O
“	O
poll	O
”	O
the	O
results	O
to	O
produce	O
a	O
ﬁnal	O
classiﬁcation	O
.	O
conﬁguration	O
designing	O
the	O
correct	O
architecture	O
for	O
a	O
deep-learning	O
network	O
,	O
along	O
with	O
the	O
various	O
choices	O
at	O
each	O
layer	B
,	O
appears	O
to	O
require	O
experience	O
and	O
trial	O
370	O
neural	O
networks	O
6	O
and	O
error	O
.	O
we	O
summarize	O
the	O
third	O
and	O
ﬁnal	O
architecture	O
which	O
we	O
built	O
for	O
classifying	O
the	O
cifar-100	O
data	B
set	O
in	O
algorithm	B
18.2.in	O
addition	O
to	O
these	O
size	O
parameters	O
for	O
each	O
layer	B
,	O
we	O
must	O
select	O
the	O
activation	O
functions	O
and	O
additional	O
regularization	B
.	O
in	O
this	O
case	O
we	O
used	O
the	O
leaky	B
rectiﬁed	I
linear	I
functions	O
g˛.z/	O
(	O
section	O
18.2	O
)	O
,	O
with	O
˛	O
increasing	O
from	O
0:05	O
in	O
layer	B
5	O
up	O
to	O
0:5	O
in	O
layer	B
13.	O
in	O
addition	O
a	O
type	O
of	O
`2	O
regularization	B
was	O
imposed	O
on	O
the	O
weights	O
,	O
restricting	O
all	O
incoming	O
weight	O
vectors	O
to	O
a	O
node	O
to	O
have	O
`2	O
norm	O
bounded	O
by	O
one	O
.	O
figure	O
18.12	O
shows	O
both	O
the	O
progress	O
of	O
the	O
optimization	O
objective	O
(	O
red	O
)	O
and	O
the	O
test	O
misclassiﬁcation	O
error	O
(	O
blue	O
)	O
as	O
the	O
gradient-	O
descent	O
algorithm	B
proceeds	O
.	O
the	O
accelerated	O
gradient	O
method	B
maintains	O
a	O
memory	O
,	O
which	O
we	O
can	O
see	O
was	O
restarted	O
twice	O
to	O
get	O
out	O
of	O
local	O
minima	O
.	O
our	O
network	O
achieved	O
a	O
test	O
error	O
rate	B
of	O
35	O
%	O
on	O
the	O
10,000	O
test	O
images	O
(	O
100	O
images	O
per	O
class	O
)	O
.	O
the	O
best	O
reported	O
error	O
rate	B
we	O
have	O
seen	O
is	O
25	O
%	O
,	O
so	O
apparently	O
we	O
have	O
some	O
way	O
to	O
go	O
!	O
figure	O
18.12	O
progress	O
of	O
the	O
algorithm	B
as	O
a	O
function	B
of	O
the	O
number	O
of	O
epochs	O
.	O
the	O
accelerated	O
gradient	O
algorithm	B
is	O
“	O
restarted	O
”	O
every	O
100	O
epochs	O
,	O
meaning	O
the	O
long-term	O
memory	O
is	O
forgotten	O
,	O
and	O
a	O
new	O
trail	O
is	O
begun	O
,	O
starting	O
at	O
the	O
current	O
solution	O
.	O
the	O
red	O
curve	O
shows	O
the	O
objective	O
(	O
negative	O
penalized	O
log-likelihood	B
on	O
the	O
training	O
data	B
)	O
.	O
the	O
blue	O
curve	O
shows	O
test-set	O
misclassiﬁcation	O
error	O
.	O
the	O
vertical	O
axis	O
is	O
on	O
the	O
log	O
scale	B
,	O
so	O
zero	O
can	O
not	O
be	O
included	O
.	O
050100150200250300405060708090epochtest	O
misclassification	O
error3001900390063009200objectiveobjective	O
costmisclassification	O
error	O
18.6	O
notes	O
and	O
details	O
371	O
algorithm	B
18.2	O
configuration	O
parameters	O
for	O
deep-learning	O
network	O
used	O
on	O
the	O
cifar-100	O
data	B
.	O
layer	B
1	O
:	O
100	O
convolution	O
maps	O
each	O
with	O
2	O
(	O
cid:2	O
)	O
2	O
(	O
cid:2	O
)	O
3	O
kernel	O
(	O
the	O
3	O
for	O
three	O
colors	O
)	O
.	O
the	O
input	O
image	O
is	O
padded	O
from	O
32	O
(	O
cid:2	O
)	O
32	O
to	O
40	O
(	O
cid:2	O
)	O
40	O
to	O
accom-	O
modate	O
input	O
distortions	O
.	O
layers	O
2	O
and	O
3	O
:	O
100	O
convolution	O
maps	O
each	O
2	O
(	O
cid:2	O
)	O
2	O
(	O
cid:2	O
)	O
100.	O
compositions	O
of	O
convolutions	O
are	O
roughly	O
equivalent	O
to	O
convolutions	O
with	O
a	O
bigger	O
band-	O
width	O
,	O
and	O
the	O
smaller	O
ones	O
have	O
fewer	O
parameters	O
.	O
layer	B
4	O
:	O
max	O
pool	O
2	O
(	O
cid:2	O
)	O
2	O
layer	B
,	O
pooling	O
nonoverlapping	O
2	O
(	O
cid:2	O
)	O
2	O
blocks	O
of	O
pixels	O
,	O
and	O
hence	O
reducing	O
the	O
images	O
to	O
size	O
20	O
(	O
cid:2	O
)	O
20.	O
layer	B
5	O
:	O
300	O
convolution	O
maps	O
each	O
2	O
(	O
cid:2	O
)	O
2	O
(	O
cid:2	O
)	O
100	O
,	O
with	O
dropout	O
learning	B
with	I
rate	O
(	O
cid:30	O
)	O
5	O
d	O
0:05.	O
layer	B
6	O
:	O
repeat	O
of	O
layer	B
5.	O
layer	B
7	O
:	O
max	O
pool	O
2	O
(	O
cid:2	O
)	O
2	O
layer	B
(	O
down	O
to	O
10	O
(	O
cid:2	O
)	O
10	O
images	O
)	O
.	O
layer	B
8	O
:	O
600	O
convolution	O
maps	O
each	O
2	O
(	O
cid:2	O
)	O
2	O
(	O
cid:2	O
)	O
300	O
,	O
with	O
dropout	O
rate	B
(	O
cid:30	O
)	O
8	O
d	O
0:10.	O
layer	B
9	O
:	O
800	O
convolution	O
maps	O
each	O
2	O
(	O
cid:2	O
)	O
2	O
(	O
cid:2	O
)	O
600	O
,	O
with	O
dropout	O
rate	B
(	O
cid:30	O
)	O
9	O
d	O
0:10.	O
layer	B
10	O
:	O
max	O
pool	O
2	O
(	O
cid:2	O
)	O
2	O
layer	B
(	O
down	O
to	O
5	O
(	O
cid:2	O
)	O
5	O
images	O
)	O
.	O
layer	B
11	O
:	O
1600	O
convolution	O
maps	O
,	O
each	O
1	O
(	O
cid:2	O
)	O
1	O
(	O
cid:2	O
)	O
800.	O
this	O
is	O
a	O
pixelwise	O
weighted	O
sum	O
across	O
the	O
800	O
images	O
from	O
the	O
previous	O
layer	B
.	O
layer	B
12	O
:	O
2000	O
fully	O
connected	O
units	O
,	O
with	O
dropout	O
rate	B
(	O
cid:30	O
)	O
12	O
d	O
0:25.	O
layer	B
13	O
:	O
final	O
100	O
output	O
units	O
,	O
with	O
softmax	O
activation	O
,	O
and	O
dropout	O
rate	B
(	O
cid:30	O
)	O
13	O
d	O
0:5	O
.	O
18.6	O
notes	O
and	O
details	O
the	O
reader	O
will	O
notice	O
that	O
probability	O
models	B
have	O
disappeared	O
from	O
the	O
development	O
here	O
.	O
neural	O
nets	O
are	O
elaborate	O
regression	B
methods	O
aimed	O
solely	O
at	O
prediction—not	O
estimation	B
or	O
explanation	O
in	O
the	O
language	O
of	O
sec-	O
tion	O
8.4.	O
in	O
place	O
of	O
parametric	B
optimality	O
criteria	B
,	O
the	O
machine	O
learning	O
community	O
has	O
focused	O
on	O
a	O
set	B
of	O
speciﬁc	O
prediction	O
data	B
sets	O
,	O
like	O
the	O
digits	O
mnist	O
corpus	O
and	O
cifar-100	O
,	O
as	O
benchmarks	O
for	O
measuring	O
per-	O
formance	O
.	O
there	O
is	O
a	O
vast	O
literature	O
on	O
neural	O
networks	O
,	O
with	O
hundreds	O
of	O
books	O
and	O
thousands	O
of	O
papers	O
.	O
with	O
the	O
resurgence	O
of	O
deep	O
learning	O
,	O
this	O
literature	O
is	O
again	O
growing	O
.	O
two	O
early	O
statistical	O
references	O
on	O
neural	O
networks	O
are	O
rip-	O
ley	O
(	O
1996	O
)	O
and	O
bishop	O
(	O
1995	O
)	O
,	O
and	O
hastie	O
et	O
al	O
.	O
(	O
2009	O
)	O
devote	O
one	O
chapter	O
to	O
the	O
topic	O
.	O
part	O
of	O
our	O
description	O
of	O
backpropagation	O
in	O
section	O
18.2	O
was	O
372	O
neural	O
networks	O
guided	O
by	O
andrew	O
ng	O
’	O
s	O
online	O
stanford	O
lecture	O
notes	O
(	O
ng	O
,	O
2015	O
)	O
.	O
bengio	O
et	O
al	O
.	O
(	O
2013	O
)	O
provide	O
a	O
useful	O
review	O
of	O
autoencoders	O
.	O
lecun	O
et	O
al	O
.	O
(	O
2015	O
)	O
give	O
a	O
brief	O
overview	O
of	O
deep	O
learning	O
,	O
written	O
by	O
three	O
pioneers	O
of	O
this	O
ﬁeld	O
:	O
yann	O
lecun	O
,	O
yoshua	O
bengio	O
and	O
geoffrey	O
hinton	O
;	O
we	O
also	O
bene-	O
ﬁted	O
from	O
reading	O
ngiam	O
et	O
al	O
.	O
(	O
2010	O
)	O
.	O
dropout	O
learning	O
(	O
srivastava	O
et	O
al.	O
,	O
2014	O
)	O
is	O
a	O
relatively	O
new	O
idea	O
,	O
and	O
its	O
connections	O
with	O
ridge	B
regression	I
were	O
most	O
usefully	O
described	O
in	O
wager	O
et	O
al	O
.	O
(	O
2013	O
)	O
.	O
the	O
most	O
popular	O
version	O
of	O
accelerated	O
gradient	O
descent	O
is	O
due	O
to	O
nesterov	O
(	O
2013	O
)	O
.	O
learn-	O
ing	O
with	O
hints	O
is	O
due	O
to	O
abu-mostafa	O
(	O
1995	O
)	O
.	O
the	O
material	O
in	O
sections	O
18.4	O
and	O
18.5	O
beneﬁted	O
greatly	O
from	O
discussions	O
with	O
rakesh	O
achanta	O
(	O
achanta	O
and	O
hastie	O
,	O
2015	O
)	O
,	O
who	O
produced	O
some	O
of	O
the	O
color	O
images	O
and	O
diagrams	O
,	O
and	O
designed	O
and	O
ﬁt	O
the	O
deep-learning	O
network	O
to	O
the	O
cifar-100	O
data	B
.	O
1	O
[	O
p.	O
352	O
]	O
the	O
neural	O
information	B
processing	O
systems	O
(	O
nips	O
)	O
conferences	O
started	O
in	O
late	O
fall	O
1987	O
in	O
denver	O
,	O
colorado	O
,	O
and	O
post-conference	O
work-	O
shops	O
were	O
held	O
at	O
the	O
nearby	O
ski	O
resort	O
at	O
vail	O
.	O
these	O
are	O
still	O
very	O
popular	O
today	O
,	O
although	O
the	O
venue	O
has	O
changed	O
over	O
the	O
years	O
.	O
the	O
nips	O
proceed-	O
ings	O
are	O
refereed	O
,	O
and	O
nips	O
papers	O
count	O
as	O
publications	O
in	O
most	O
ﬁelds	O
,	O
especially	O
computer	O
science	O
and	O
engineering	O
.	O
although	O
neural	O
networks	O
were	O
initially	O
the	O
main	O
topic	O
of	O
the	O
conferences	O
,	O
a	O
modern	O
nips	O
conference	O
covers	O
all	O
the	O
latest	O
ideas	O
in	O
machine	O
learning	O
.	O
2	O
[	O
p.	O
353	O
]	O
mnist	O
is	O
a	O
curated	O
database	O
of	O
images	O
of	O
handwritten	B
digits	I
(	O
lecun	O
and	O
cortes	O
,	O
2010	O
)	O
.	O
there	O
are	O
60,000	O
training	O
images	O
,	O
and	O
10,000	O
test	O
images	O
,	O
each	O
a	O
28	O
(	O
cid:2	O
)	O
28	O
grayscale	O
image	O
.	O
these	O
data	B
have	O
been	O
used	O
as	O
a	O
testbed	O
for	O
many	O
different	O
learning	O
algorithms	O
,	O
so	O
the	O
reported	O
best	O
error	O
rates	O
might	O
be	O
optimistic	O
.	O
3	O
[	O
p.	O
360	O
]	O
tuning	O
parameters	O
.	O
typical	O
neural	O
network	O
implementations	O
have	O
dozens	O
of	O
tuning	O
parameters	O
,	O
and	O
many	O
of	O
these	O
are	O
associated	O
with	O
the	O
ﬁne	O
tuning	O
of	O
the	O
descent	O
algorithm	B
.	O
we	O
used	O
the	O
h2o.deeplearning	O
func-	O
tion	O
in	O
the	O
r	O
package	O
h2o	O
to	O
ﬁt	O
our	O
model	B
for	O
the	O
mnist	O
data	B
set	O
.	O
it	O
has	O
around	O
20	O
such	O
parameters	O
,	O
although	O
most	O
default	O
to	O
factory-tuned	O
con-	O
stants	O
that	O
have	O
been	O
found	O
to	O
work	O
well	O
on	O
many	O
examples	O
.	O
arno	O
candel	O
was	O
very	O
helpful	O
in	O
assisting	O
us	O
with	O
the	O
software	O
.	O
4	O
[	O
p.	O
368	O
]	O
dropout	O
and	O
ridge	O
regression	B
.	O
dropout	O
was	O
originally	O
proposed	O
in	O
srivastava	O
et	O
al	O
.	O
(	O
2014	O
)	O
,	O
and	O
reinterpreted	O
in	O
wager	O
et	O
al	O
.	O
(	O
2013	O
)	O
.	O
dropout	O
was	O
inspired	O
by	O
the	O
random	O
selection	O
of	O
variables	O
at	O
each	O
tree	O
split	O
in	O
a	O
random	O
forest	O
(	O
section	O
17.1	O
)	O
.	O
consider	O
a	O
simple	O
version	O
of	O
dropout	O
for	O
the	O
linear	B
regression	O
problem	O
with	O
squared-error	O
loss	O
.	O
we	O
have	O
an	O
n	O
(	O
cid:2	O
)	O
p	O
regression	B
matrix	O
x	O
,	O
and	O
a	O
response	O
n-vector	O
y.	O
for	O
simplicity	O
we	O
assume	O
all	O
variables	O
have	O
mean	O
zero	O
,	O
so	O
we	O
can	O
ignore	O
intercepts	O
.	O
consider	O
the	O
following	O
random	O
least-squares	O
criterion	O
:	O
18.6	O
notes	O
and	O
details	O
0	O
@	O
yi	O
(	O
cid:0	O
)	O
px	O
jd1	O
nx	O
id1	O
373	O
1a2	O
:	O
xij	O
iij	O
ˇj	O
li	O
.ˇ/	O
d	O
1	O
2	O
iij	O
d	O
(	O
cid:26	O
)	O
	O
@	O
li	O
.ˇ/	O
(	O
cid:21	O
)	O
d	O
(	O
cid:0	O
)	O
x	O
ˇ	O
d	O
o	O
here	O
the	O
iij	O
are	O
i.i.d	O
variables	O
8i	O
;	O
j	O
with	O
0	O
with	O
probability	O
(	O
cid:30	O
)	O
;	O
1=.1	O
(	O
cid:0	O
)	O
(	O
cid:30	O
)	O
/	O
with	O
probability	O
1	O
(	O
cid:0	O
)	O
(	O
cid:30	O
)	O
;	O
(	O
this	O
particular	O
form	B
is	O
used	O
so	O
that	O
eœiij	O
	O
d	O
1	O
)	O
.	O
using	O
simple	O
probability	O
it	O
can	O
be	O
shown	O
that	O
the	O
expected	O
score	O
equations	O
can	O
be	O
written	O
e	O
(	O
18.23	O
)	O
with	O
d	O
d	O
diagfkx1k2	O
;	O
kx2k2	O
;	O
:	O
:	O
:	O
;	O
kxpk2g	O
.	O
hence	O
the	O
solution	O
is	O
given	O
by	O
@	O
ˇ	O
0	O
y	O
c	O
x	O
0	O
dˇ	O
d	O
0	O
;	O
x	O
ˇ	O
c	O
(	O
cid:30	O
)	O
1	O
(	O
cid:0	O
)	O
(	O
cid:30	O
)	O
	O
(	O
cid:0	O
)	O
1	O
0	O
x	O
c	O
(	O
cid:30	O
)	O
1	O
(	O
cid:0	O
)	O
(	O
cid:30	O
)	O
x	O
d	O
x	O
0	O
y	O
;	O
(	O
18.24	O
)	O
a	O
generalized	O
ridge	B
regression	I
.	O
if	O
the	O
variables	O
are	O
standardized	O
,	O
the	O
term	O
d	O
becomes	O
a	O
scalar	O
,	O
and	O
the	O
solution	O
is	O
identical	O
to	O
ridge	B
regression	I
.	O
with	O
a	O
nonlinear	B
activation	O
function	B
,	O
the	O
interpretation	O
changes	O
slightly	O
;	O
see	O
wa-	O
ger	O
et	O
al	O
.	O
(	O
2013	O
)	O
for	O
details	O
.	O
5	O
[	O
p.	O
369	O
]	O
distortion	O
and	O
ridge	O
regression	B
.	O
we	O
again	O
show	O
in	O
a	O
simple	O
ex-	O
ample	O
that	O
input	O
distortion	O
is	O
similar	O
to	O
ridge	B
regression	I
.	O
assume	O
the	O
same	O
setup	O
as	O
in	O
the	O
previous	O
example	O
,	O
except	O
a	O
different	O
randomized	O
version	O
of	O
the	O
criterion	O
:	O
ln	O
.ˇ/	O
d	O
1	O
2	O
.xij	O
c	O
nij	O
/ˇj	O
1a2	O
:	O
id1	O
nx	O
0	O
@	O
yi	O
(	O
cid:0	O
)	O
px	O
(	O
cid:21	O
)	O
d	O
(	O
cid:0	O
)	O
x	O
jd1	O
0	O
	O
@	O
ln	O
.ˇ/	O
here	O
we	O
have	O
added	O
random	O
noise	O
to	O
the	O
prediction	O
variables	O
,	O
and	O
we	O
as-	O
sume	O
this	O
noise	O
is	O
i.i.d	O
.0	O
;	O
(	O
cid:21	O
)	O
/	O
.	O
once	O
again	O
the	O
expected	O
score	O
equations	O
can	O
be	O
written	O
y	O
c	O
x	O
0	O
x	O
ˇ	O
c	O
(	O
cid:21	O
)	O
ˇ	O
d	O
0	O
;	O
e	O
@	O
ˇ	O
(	O
18.25	O
)	O
ij	O
/	O
d	O
(	O
cid:21	O
)	O
.	O
once	O
again	O
because	O
of	O
the	O
independence	O
of	O
all	O
the	O
nij	O
and	O
e.n2	O
this	O
leads	O
to	O
a	O
ridge	B
regression	I
.	O
so	O
replacing	O
each	O
observation	O
pair	O
xi	O
;	O
yi	O
by	O
the	O
collection	O
fx	O
is	O
a	O
noisy	O
version	O
of	O
xi	O
,	O
is	O
approximately	O
equivalent	O
to	O
a	O
ridge	B
regression	I
on	O
the	O
original	O
data	B
.	O
bd1	O
,	O
where	O
each	O
x	O
;	O
yigb	O
(	O
cid:3	O
)	O
b	O
(	O
cid:3	O
)	O
b	O
i	O
i	O
374	O
neural	O
networks	O
6	O
[	O
p.	O
370	O
]	O
software	O
for	O
deep	O
learning	O
.	O
our	O
deep	O
learning	O
convolutional	O
net-	O
work	O
for	O
the	O
cifar-100	O
data	B
was	O
constructed	O
and	O
run	O
by	O
rakesh	O
achanta	O
in	O
theano	O
,	O
a	O
python-based	O
system	O
(	O
bastien	O
et	O
al.	O
,	O
2012	O
;	O
bergstra	O
et	O
al.	O
,	O
2010	O
)	O
.	O
theano	O
has	O
a	O
user-friendly	O
language	O
for	O
specifying	O
the	O
host	O
of	O
parameters	O
for	O
a	O
deep-learning	O
network	O
,	O
and	O
uses	O
symbolic	O
differentiation	O
for	O
computing	O
the	O
gradients	O
needed	O
in	O
stochastic	O
gradient	O
descent	O
.	O
in	O
2015	O
google	O
announced	O
an	O
open-source	O
version	O
of	O
their	O
tensorflow	O
software	O
for	O
ﬁtting	B
deep	O
networks	O
.	O
19	O
support-vector	O
machines	O
and	O
kernel	O
methods	O
while	O
linear	B
logistic	O
regression	B
has	O
been	O
the	O
mainstay	O
in	O
biostatistics	O
and	O
epidemiology	O
,	O
it	O
has	O
had	O
a	O
mixed	O
reception	O
in	O
the	O
machine-learning	O
com-	O
munity	O
.	O
there	O
the	O
goal	O
is	O
often	O
classiﬁcation	O
accuracy	O
,	O
rather	O
than	O
statistical	O
one	O
ifbpr.y	O
d	O
1jx	O
d	O
x/	O
(	O
cid:21	O
)	O
0:5.	O
svms	O
bypass	O
the	O
ﬁrst	O
step	O
,	O
and	O
build	O
a	O
inference	B
.	O
logistic	B
regression	I
builds	O
a	O
classiﬁer	O
in	O
two	O
steps	O
:	O
ﬁt	O
a	O
condi-	O
tional	O
probability	O
model	B
for	O
pr.y	O
d	O
1jx	O
d	O
x/	O
,	O
and	O
then	O
classify	O
as	O
a	O
classiﬁer	O
directly	O
.	O
another	O
rather	O
awkward	O
issue	O
with	O
logistic	B
regression	I
is	O
that	O
it	O
fails	O
if	O
the	O
training	O
data	B
are	O
linearly	O
separable	O
!	O
what	O
this	O
means	O
is	O
that	O
,	O
in	O
the	O
feature	O
space	B
,	O
one	O
can	O
separate	O
the	O
two	O
classes	O
by	O
a	O
linear	B
boundary	O
.	O
in	O
cases	O
such	O
as	O
this	O
,	O
maximum	B
likelihood	I
fails	O
and	O
some	O
parameters	O
march	O
off	O
to	O
inﬁnity	O
.	O
while	O
this	O
might	O
have	O
seemed	O
an	O
unlikely	O
scenario	O
to	O
the	O
early	O
users	O
of	O
logistic	B
regression	I
,	O
it	O
becomes	O
almost	O
a	O
certainty	O
with	O
mod-	O
ern	O
wide	O
genomics	O
data	B
.	O
when	O
p	O
(	O
cid:29	O
)	O
n	O
(	O
more	O
features	O
than	O
observations	O
)	O
,	O
we	O
can	O
typically	O
always	O
ﬁnd	O
a	O
separating	O
hyperplane	O
.	O
finding	O
an	O
optimal	O
separating	O
hyperplane	O
was	O
in	O
fact	O
the	O
launching	O
point	O
for	O
svms	O
.	O
as	O
we	O
will	O
see	O
,	O
they	O
have	O
more	O
than	O
this	O
to	O
offer	O
,	O
and	O
in	O
fact	O
live	O
comfortably	O
alongside	O
logistic	B
regression	I
.	O
svms	O
pursued	O
an	O
age-old	O
approach	O
in	O
statistics	B
,	O
of	O
enriching	O
the	O
feature	O
space	B
through	O
nonlinear	B
transformations	O
and	O
basis	O
expansions	O
;	O
a	O
classical	O
example	O
being	O
augmenting	O
a	O
linear	B
regression	O
with	O
interaction	O
terms	O
.	O
a	O
linear	B
model	I
in	O
the	O
enlarged	O
space	B
leads	O
to	O
a	O
nonlinear	B
model	O
in	O
the	O
ambient	O
space	B
.	O
this	O
is	O
typically	O
achieved	O
via	O
the	O
“	O
kernel	O
trick	B
,	O
”	O
which	O
allows	O
the	O
computations	B
to	O
be	O
performed	O
in	O
the	O
n-dimensional	O
space	B
for	O
an	O
arbitrary	O
number	O
of	O
predictors	O
p.	O
as	O
the	O
ﬁeld	O
matured	O
,	O
it	O
became	O
clear	O
that	O
in	O
fact	O
this	O
kernel	O
trick	B
amounted	O
to	O
estimation	B
in	O
a	O
reproducing-kernel	O
hilbert	O
space	B
.	O
finally	O
,	O
we	O
contrast	O
the	O
kernel	O
approach	O
in	O
svms	O
with	O
the	O
nonparame-	O
teric	O
regression	B
techniques	O
known	O
as	O
kernel	O
smoothing	B
.	O
375	O
376	O
svms	O
and	O
kernel	O
methods	O
19.1	O
optimal	O
separating	O
hyperplane	O
figure	O
19.1	O
shows	O
a	O
small	O
sample	B
of	O
points	O
in	O
r2	O
,	O
each	O
belonging	O
to	O
one	O
of	O
two	O
classes	O
(	O
blue	O
or	O
orange	O
)	O
.	O
numerically	O
we	O
would	O
score	O
these	O
classes	O
as	O
y	O
d	O
c1	O
for	O
say	O
blue	O
,	O
and	O
y	O
d	O
(	O
cid:0	O
)	O
1	O
for	O
orange.1	O
we	O
deﬁne	O
a	O
two-class	O
lin-	O
ear	O
classiﬁer	O
via	O
a	O
function	B
f	O
.x/	O
d	O
ˇ0	O
c	O
x	O
ˇ	O
,	O
with	O
the	O
convention	O
that	O
we	O
classify	O
a	O
point	O
x0	O
as	O
+1	O
if	O
f	O
.x0/	O
>	O
0	O
,	O
and	O
as	O
-1	O
if	O
f	O
.x0/	O
<	O
0	O
(	O
on	O
the	O
fence	O
we	O
ﬂip	O
a	O
coin	O
)	O
.	O
hence	O
the	O
classiﬁer	O
itself	O
is	O
c.x/	O
d	O
signœf	O
.x/	O
.	O
the	O
deci-	O
0	O
figure	O
19.1	O
left	O
panel	O
:	O
data	B
in	O
two	O
classes	O
in	O
r2	O
.	O
three	O
potential	O
decision	O
boundaries	O
are	O
shown	O
;	O
each	O
separate	O
the	O
data	B
perfectly	O
.	O
right	O
panel	O
:	O
the	O
optimal	O
separating	O
hyperplane	O
(	O
a	O
line	O
in	O
r2	O
)	O
creates	O
the	O
biggest	O
margin	O
between	O
the	O
two	O
classes	O
.	O
sion	O
boundary	O
is	O
the	O
set	B
fx	O
j	O
f	O
.x/	O
d	O
0g	O
.	O
we	O
see	O
three	O
different	O
classiﬁers	O
in	O
the	O
left	O
panel	O
of	O
figure	O
19.1	O
,	O
and	O
they	O
all	O
classify	O
the	O
points	O
perfectly	O
.	O
the	O
optimal	O
separating	O
hyperplane	O
is	O
the	O
linear	B
classiﬁer	O
that	O
creates	O
the	O
largest	O
margin	O
between	O
the	O
two	O
classes	O
,	O
and	O
is	O
shown	O
in	O
the	O
right	O
panel	O
(	O
it	O
is	O
also	O
known	O
as	O
an	O
optimal-margin	O
classiﬁer	O
)	O
.	O
the	O
underlying	O
hope	O
is	O
that	O
,	O
by	O
making	O
a	O
big	O
margin	O
on	O
the	O
training	O
data	B
,	O
it	O
will	O
also	O
classify	O
future	O
observations	O
well	O
.	O
some	O
elementary	O
geometry	B
	O
shows	O
that	O
the	O
(	O
signed	O
)	O
euclidean	O
distance	O
from	O
a	O
point	O
x0	O
to	O
the	O
linear	B
decision	O
boundary	O
deﬁned	O
by	O
f	O
is	O
given	O
by	O
1	O
1kˇk2	O
f	O
.x0/	O
:	O
with	O
this	O
in	O
mind	O
,	O
for	O
a	O
separating	O
hyperplane	O
the	O
quantity	B
1kˇk2	O
1	O
in	O
this	O
chapter	O
,	O
the	O
˙1	O
scoring	O
leads	O
to	O
convenient	O
notation	O
.	O
(	O
19.1	O
)	O
yi	O
f	O
.xi	O
/	O
is	O
−10123−10123x1x2llllllllllllllllllllllll−10123−10123x1x2llllllllllllllllllllllllllllllllllllllllllllllll	O
19.1	O
optimal	O
separating	O
hyperplane	O
377	O
the	O
distance	O
of	O
xi	O
from	O
the	O
decision	O
boundary.2	O
this	O
leads	O
to	O
an	O
optimiza-	O
tion	O
problem	O
for	O
creating	O
the	O
optimal	O
margin	O
classiﬁer	O
:	O
maximize	O
ˇ0	O
;	O
ˇ	O
m	O
subject	O
to	O
1kˇk2	O
yi	O
.ˇ0	O
c	O
x	O
0	O
ˇ/	O
(	O
cid:21	O
)	O
m	O
;	O
i	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
n	O
:	O
a	O
rescaling	O
argument	B
reduces	O
this	O
to	O
the	O
simpler	O
form	B
kˇk2	O
minimize	O
subject	O
to	O
yi	O
.ˇ0	O
c	O
x	O
ˇ0	O
;	O
ˇ	O
0	O
ˇ/	O
(	O
cid:21	O
)	O
1	O
;	O
i	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
n	O
:	O
(	O
19.2	O
)	O
(	O
19.3	O
)	O
(	O
19.4	O
)	O
this	O
is	O
a	O
quadratic	O
program	O
,	O
which	O
can	O
be	O
solved	O
by	O
standard	O
techniques	O
in	O
convex	O
optimization.	O
one	O
noteworthy	O
property	O
of	O
the	O
solution	O
is	O
that	O
2	O
ˇ	O
dx	O
o	O
i2	O
s	O
o˛i	O
xi	O
;	O
s	O
where	O
s	O
is	O
the	O
support	O
set	B
.	O
we	O
can	O
see	O
in	O
figure	O
19.1	O
that	O
the	O
margin	O
touches	O
three	O
points	O
(	O
vectors	O
)	O
;	O
in	O
this	O
case	O
there	O
are	O
j	O
j	O
d	O
3	O
support	O
vec-	O
tors	O
,	O
and	O
clearly	O
the	O
orientation	O
of	O
o	O
ˇ	O
is	O
determined	O
by	O
them	O
.	O
however	O
,	O
we	O
still	O
have	O
to	O
solve	O
the	O
optimization	O
problem	O
to	O
identify	O
the	O
three	O
points	O
i	O
2	O
in	O
s	O
,	O
and	O
their	O
coefﬁcients	O
˛i	O
;	O
s.	O
figure	O
19.2	O
shows	O
an	O
optimal-	O
margin	O
classiﬁer	O
ﬁt	O
to	O
wide	O
data	B
,	O
that	O
is	O
data	B
where	O
p	O
(	O
cid:29	O
)	O
n.	O
these	O
are	O
gene-expression	O
measurements	O
on	O
p	O
d	O
3571	O
genes	O
measured	O
on	O
blood	O
samples	O
from	O
n	O
d	O
72	O
leukemia	B
patients	O
(	O
ﬁrst	O
seen	O
in	O
chapter	O
1	O
)	O
.	O
they	O
were	O
classiﬁed	O
into	O
two	O
classes	O
,	O
47	O
acute	O
lymphoblastic	O
leukemia	B
(	O
all	O
)	O
and	O
25	O
myeloid	O
leukemia	B
(	O
aml	O
)	O
.	O
in	O
cases	O
like	O
this	O
,	O
we	O
are	O
typically	O
guar-	O
anteed	O
a	O
separating	O
hyperplane3	O
.	O
in	O
this	O
case	O
42	O
of	O
the	O
72	O
points	O
are	O
support	O
points	O
.	O
one	O
might	O
be	O
justiﬁed	O
in	O
thinking	O
that	O
this	O
solution	O
is	O
overﬁt	O
to	O
this	O
small	O
amount	O
of	O
data	B
.	O
indeed	O
,	O
when	O
broken	O
into	O
a	O
training	O
and	O
test	O
set	B
,	O
we	O
see	O
that	O
the	O
test	O
data	B
encroaches	O
well	O
into	O
the	O
margin	O
region	B
,	O
but	O
in	O
this	O
case	O
none	O
are	O
misclassiﬁed	O
.	O
such	O
classiﬁers	O
are	O
very	O
popular	O
in	O
the	O
wide-	O
data	B
world	O
of	O
genomics	O
,	O
largely	O
because	O
they	O
seem	O
to	O
work	O
very	O
well	O
.	O
they	O
offer	O
a	O
simple	O
alternative	O
to	O
logistic	B
regression	I
,	O
in	O
a	O
situation	O
where	O
the	O
lat-	O
ter	O
fails	O
.	O
however	O
,	O
sometimes	O
the	O
solution	O
is	O
overﬁt	O
,	O
and	O
a	O
modiﬁcation	B
is	O
called	O
for	O
.	O
this	O
same	O
modiﬁcation	B
takes	O
care	O
of	O
nonseparable	O
situations	O
as	O
well	O
.	O
2	O
since	O
all	O
the	O
points	O
are	O
correctly	O
classiﬁed	O
,	O
the	O
sign	O
of	O
f	O
.xi	O
/	O
agrees	O
with	O
yi	O
,	O
hence	O
this	O
quantity	B
is	O
always	O
positive	O
.	O
3	O
if	O
n	O
	O
p	O
c	O
1	O
we	O
can	O
always	O
ﬁnd	O
a	O
separating	O
hyperplane	O
,	O
unless	O
there	O
are	O
exact	O
feature	O
ties	O
across	O
the	O
class	O
barrier	O
!	O
378	O
svms	O
and	O
kernel	O
methods	O
figure	O
19.2	O
left	O
panel	O
:	O
optimal	O
margin	O
classiﬁer	O
ﬁt	O
to	O
leukemia	B
data	O
.	O
there	O
are	O
72	O
observations	O
from	O
two	O
classes—47	O
all	O
and	O
25	O
aml—and	O
3571	O
gene-expression	O
variables	O
.	O
of	O
the	O
72	O
observations	O
,	O
42	O
are	O
support	O
vectors	O
,	O
sitting	O
on	O
the	O
margin	O
.	O
the	O
points	O
are	O
plotted	O
against	O
their	O
ﬁtted	O
classiﬁer	O
function	B
o	O
component	O
of	O
the	O
data	B
(	O
chosen	O
for	O
display	O
purposes	O
,	O
since	O
it	O
has	O
low	O
correlation	O
with	O
the	O
former	O
)	O
.	O
right	O
panel	O
:	O
here	O
the	O
optimal	O
margin	O
classiﬁer	O
was	O
ﬁt	O
to	O
a	O
random	O
subset	O
of	O
50	O
of	O
the	O
72	O
observations	O
,	O
and	O
then	O
used	O
to	O
classify	O
the	O
remaining	O
22	O
(	O
shown	O
in	O
color	O
)	O
.	O
although	O
these	O
points	O
fall	O
on	O
the	O
wrong	O
sides	O
of	O
their	O
respective	O
margins	O
,	O
they	O
are	O
all	O
correctly	O
classiﬁed	O
.	O
f	O
.x/	O
,	O
labeled	O
svm	O
projection	O
,	O
and	O
the	O
ﬁfth	O
principal	O
19.2	O
soft-margin	O
classiﬁer	O
figure	O
19.3	O
shows	O
data	B
in	O
r2	O
that	O
are	O
not	O
separable	O
.	O
the	O
generalization	O
to	O
a	O
soft	O
margin	O
allows	O
points	O
to	O
violate	O
their	O
margin	O
.	O
each	O
of	O
the	O
violators	O
has	O
a	O
line	O
segment	O
connecting	O
it	O
to	O
its	O
margin	O
,	O
showing	O
the	O
extent	O
of	O
the	O
violation	O
.	O
the	O
soft-margin	O
classiﬁer	O
solves	O
kˇk2	O
ˇ0	O
;	O
ˇ	O
minimize	O
subject	O
to	O
yi	O
.ˇ0	O
c	O
x	O
(	O
cid:15	O
)	O
i	O
(	O
cid:21	O
)	O
0	O
;	O
i	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
n	O
;	O
and	O
nx	O
i	O
ˇ/	O
(	O
cid:21	O
)	O
1	O
(	O
cid:0	O
)	O
(	O
cid:15	O
)	O
i	O
;	O
0	O
(	O
19.5	O
)	O
(	O
cid:15	O
)	O
i	O
	O
b	O
:	O
id1	O
here	O
b	O
is	O
the	O
budget	O
for	O
the	O
total	O
amount	O
of	O
overlap	O
.	O
once	O
again	O
,	O
the	O
solu-	O
tion	O
has	O
the	O
form	B
(	O
19.4	O
)	O
,	O
except	O
now	O
the	O
support	O
set	B
s	O
includes	O
any	O
vectors	O
on	O
the	O
margin	O
as	O
well	O
as	O
those	O
that	O
violate	O
the	O
margin	O
.	O
the	O
bigger	O
b	O
,	O
the	O
llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−1.5−1.0−0.50.00.51.01.5−0.2−0.10.00.10.20.3leukemia	O
:	O
all	O
datasvm	O
projectionpca	O
5	O
projectionllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−1.5−1.0−0.50.00.51.0−0.2−0.10.00.10.20.3leukemia	O
:	O
train	O
and	O
testsvm	O
projectionpca	O
5	O
projectionllllllllllllllllllllllllllllllllllllllllllllllllllllllllll	O
19.3	O
svm	O
criterion	O
as	O
loss	O
plus	O
penalty	B
379	O
figure	O
19.3	O
for	O
data	B
that	O
are	O
not	O
separable	O
,	O
such	O
as	O
here	O
,	O
the	O
soft-margin	O
classiﬁer	O
allows	O
margin	O
violations	O
.	O
the	O
budget	O
b	O
for	O
the	O
total	O
measure	O
of	O
violation	O
becomes	O
a	O
tuning	O
parameter	O
.	O
the	O
bigger	O
the	O
budget	O
,	O
the	O
wider	O
the	O
soft	O
margin	O
and	O
the	O
more	O
support	O
points	O
there	O
are	O
involved	O
in	O
the	O
ﬁt	O
.	O
bigger	O
the	O
support	O
set	B
,	O
and	O
hence	O
the	O
more	O
points	O
that	O
have	O
a	O
say	O
in	O
the	O
solution	O
.	O
hence	O
bigger	O
b	O
means	O
more	O
stability	O
and	O
lower	O
variance	O
.	O
in	O
fact	O
,	O
even	O
for	O
separable	O
data	B
,	O
allowing	O
margin	O
violations	O
via	O
b	O
lets	O
us	O
regularize	O
the	O
solution	O
by	O
tuning	O
b	O
.	O
19.3	O
svm	O
criterion	O
as	O
loss	O
plus	O
penalty	B
it	O
turns	O
out	O
that	O
one	O
can	O
reformulate	O
(	O
19.5	O
)	O
and	O
(	O
19.3	O
)	O
in	O
more	O
traditional	O
terms	O
as	O
the	O
minimization	O
of	O
a	O
loss	O
plus	O
a	O
penalty	B
:	O
nx	O
œ1	O
(	O
cid:0	O
)	O
yi	O
.ˇ0	O
c	O
x	O
ˇ0	O
;	O
ˇ	O
id1	O
i	O
ˇ/c	O
c	O
(	O
cid:21	O
)	O
kˇk2	O
0	O
2	O
:	O
minimize	O
(	O
19.6	O
)	O
here	O
the	O
hinge	O
loss	O
lh	O
.y	O
;	O
f	O
.x//	O
d	O
œ1	O
(	O
cid:0	O
)	O
yf	O
.x/c	O
operates	O
on	O
the	O
margin	O
quantity	B
yf	O
.x/	O
,	O
and	O
is	O
piecewise	O
linear	B
as	O
in	O
figure	O
19.4.the	O
same	O
margin	O
3	O
quantity	B
came	O
up	O
in	O
boosting	O
in	O
section	O
17.4.	O
the	O
quantity	B
œ1	O
(	O
cid:0	O
)	O
yi	O
.ˇ0	O
c	O
0	O
i	O
ˇ/c	O
is	O
the	O
cost	O
for	O
xi	O
being	O
on	O
the	O
wrong	O
side	O
of	O
its	O
margin	O
(	O
the	O
cost	O
x	O
is	O
zero	O
if	O
it	O
’	O
s	O
on	O
the	O
correct	O
side	O
)	O
.	O
the	O
correspondence	O
between	O
(	O
19.6	O
)	O
and	O
(	O
19.5	O
)	O
is	O
exact	O
;	O
large	O
(	O
cid:21	O
)	O
corresponds	O
to	O
large	O
b	O
,	O
and	O
this	O
formulation	O
makes	O
explicit	O
the	O
form	B
of	O
regularization	B
.	O
for	O
separable	O
data	B
,	O
the	O
optimal	O
separat-	O
ing	O
hyperplane	O
solution	O
(	O
19.3	O
)	O
corresponds	O
to	O
the	O
limiting	O
minimum-norm	O
solution	O
as	O
(	O
cid:21	O
)	O
#	O
0.	O
one	O
can	O
show	O
that	O
the	O
population	O
minimizer	O
of	O
the	O
−2−101234−101234x1x2llllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−2−101234−101234x1x2llllllllllllllllllllllllllllllllllllllllllllllllllllllllllll	O
380	O
svms	O
and	O
kernel	O
methods	O
figure	O
19.4	O
the	O
hinge	O
loss	O
penalizes	O
observation	O
margins	O
yf	O
.x/	O
less	O
than	O
c1	O
linearly	O
,	O
and	O
is	O
indifferent	O
to	O
margins	O
greater	O
than	O
c1	O
.	O
the	O
negative	O
binomial	B
log-likelihood	O
(	O
deviance	O
)	O
has	O
the	O
same	O
asymptotes	O
,	O
but	O
operates	O
in	O
a	O
smoother	O
fashion	O
near	O
the	O
elbow	O
at	O
yf	O
.x/	O
d	O
1	O
.	O
4	O
hinge	O
loss	O
is	O
in	O
fact	O
the	O
bayes	O
classiﬁer.4	O
this	O
shows	O
that	O
the	O
svm	O
is	O
in	O
fact	O
directly	O
estimating	O
the	O
classiﬁer	O
c.x/	O
2	O
f	O
(	O
cid:0	O
)	O
1	O
;	O
c1g.	O
the	O
red	O
curve	O
in	O
figure	O
19.4	O
is	O
(	O
half	O
)	O
the	O
binomial	B
deviance	O
for	O
logistic	B
regression	I
(	O
i.e	O
.	O
f	O
.x/	O
d	O
ˇ0	O
c	O
x	O
ˇ	O
is	O
now	O
modeling	O
logit	O
pr.y	O
d	O
c1jx	O
d	O
x/	O
)	O
.	O
with	O
y	O
d	O
˙1	O
,	O
the	O
deviance	O
can	O
also	O
be	O
written	O
in	O
terms	O
of	O
the	O
margin	O
,	O
nx	O
and	O
the	O
ridged	O
logistic	B
regression	I
corresponding	O
to	O
(	O
19.6	O
)	O
has	O
the	O
form	B
(	O
cid:0	O
)	O
yi	O
.ˇ0cx	O
0	O
logœ1	O
c	O
e	O
0	O
i	O
ˇ	O
/	O
c	O
(	O
cid:21	O
)	O
kˇk2	O
2	O
:	O
minimize	O
ˇ0	O
;	O
ˇ	O
id1	O
(	O
19.7	O
)	O
logistic	B
regression	I
is	O
discussed	O
in	O
section	O
8.1	O
,	O
as	O
well	O
as	O
sections	O
16.5	O
and	O
17.4.	O
this	O
form	B
of	O
the	O
binomial	B
deviance	O
is	O
derived	O
in	O
(	O
17.13	O
)	O
on	O
page	O
343.	O
these	O
loss	O
functions	O
have	O
some	O
features	O
in	O
common	O
,	O
as	O
can	O
be	O
seen	O
in	O
the	O
ﬁgure	O
.	O
the	O
binomial	B
loss	O
asymptotes	O
to	O
zero	O
for	O
large	O
positive	O
margins	O
,	O
and	O
to	O
a	O
linear	B
loss	O
for	O
large	O
negative	O
margins	O
,	O
matching	O
the	O
hinge	O
loss	O
in	O
this	O
regard	O
.	O
the	O
main	O
difference	O
is	O
that	O
the	O
hinge	O
has	O
a	O
sharp	O
elbow	O
at	O
+1	O
,	O
while	O
the	O
binomial	B
bends	O
smoothly	O
.	O
a	O
consequence	O
of	O
this	O
is	O
that	O
the	O
binomial	B
solution	O
involves	O
all	O
the	O
data	B
,	O
via	O
weights	O
pi	O
.1	O
(	O
cid:0	O
)	O
pi	O
/	O
that	O
fade	O
smoothly	O
with	O
distance	O
from	O
the	O
decision	O
boundary	O
,	O
as	O
apposed	O
to	O
the	O
binary	O
nature	O
4	O
the	O
bayes	O
classiﬁer	O
c.x/	O
for	O
a	O
two-class	O
problem	O
using	O
equal	O
costs	O
for	O
misclassiﬁcation	O
errors	B
assigns	O
x	O
to	O
the	O
class	O
for	O
which	O
pr.yjx/	O
is	O
largest	O
.	O
−3−2−101230.00.51.01.52.02.53.0yf	O
(	O
x	O
)	O
lossbinomialsvm	O
19.4	O
computations	B
and	O
the	O
kernel	O
trick	B
381	O
pr.y	O
d	O
c1jx/	O
	O
of	O
support	O
points	O
.	O
also	O
,	O
as	O
seen	O
in	O
section	O
17.4	O
as	O
well	O
,	O
the	O
population	O
minimizer	O
of	O
the	O
binomial	B
deviance	O
is	O
the	O
logit	O
of	O
the	O
class	O
probability	O
(	O
cid:21	O
)	O
.x/	O
d	O
log	O
;	O
pr.y	O
d	O
(	O
cid:0	O
)	O
1jx/	O
(	O
19.8	O
)	O
while	O
that	O
of	O
the	O
hinge	O
loss	O
is	O
its	O
sign	O
c.x/	O
d	O
signœ	O
(	O
cid:21	O
)	O
.x/	O
.	O
interestingly	O
,	O
as	O
(	O
cid:21	O
)	O
#	O
0	O
the	O
solution	O
direction	O
o	O
ˇ	O
to	O
the	O
ridged	O
logistic	B
regression	I
prob-	O
lem	O
(	O
19.7	O
)	O
converges	O
to	O
that	O
of	O
the	O
svm.	O
these	O
forms	O
immediately	O
suggest	O
other	O
generalizations	O
of	O
the	O
linear	B
svm	O
.	O
in	O
particular	O
,	O
we	O
can	O
replace	O
the	O
ridge	O
penalty	O
kˇk2	O
2	O
by	O
the	O
sparsity-	O
inducing	O
lasso	B
penalty	O
kˇk1	O
,	O
which	O
will	O
set	B
some	O
coefﬁcients	O
to	O
zero	O
and	O
hence	O
perform	O
feature	O
selection	O
.	O
publicly	O
available	O
software	O
(	O
e.g	O
.	O
package	O
liblinear	O
in	O
r	O
)	O
is	O
available	O
for	O
ﬁtting	B
such	O
lasso-regularized	O
support-	O
vector	B
classiﬁers	I
.	O
5	O
19.4	O
computations	B
and	O
the	O
kernel	O
trick	B
the	O
form	B
of	O
the	O
solution	O
o	O
o˛i	O
xi	O
for	O
the	O
optimal-	O
and	O
soft-margin	O
classiﬁer	O
has	O
some	O
important	O
consequences	O
.	O
for	O
starters	O
,	O
we	O
can	O
write	O
the	O
ﬁtted	O
function	B
evaluated	O
at	O
a	O
point	O
x	O
as	O
ˇ0	O
c	O
x	O
i2	O
s	O
o˛ihx	O
;	O
xii	O
;	O
(	O
19.9	O
)	O
ˇ	O
dp	O
f	O
.x/	O
d	O
o	O
o	O
d	O
o	O
0	O
o	O
ˇ	O
ˇ0	O
cx	O
i2	O
s	O
where	O
we	O
have	O
deliberately	O
replaced	O
the	O
transpose	O
notation	O
with	O
the	O
more	O
suggestive	O
inner	O
product	O
.	O
furthermore	O
,	O
we	O
show	O
in	O
(	O
19.23	O
)	O
in	O
section	O
19.9	O
that	O
the	O
lagrange	O
dual	B
involves	O
the	O
data	B
only	O
through	O
the	O
n2	O
pairwise	O
inner	O
products	O
hxi	O
;	O
xji	O
(	O
the	O
elements	O
of	O
the	O
n	O
(	O
cid:2	O
)	O
n	O
gram	O
matrix	B
xx	O
0	O
)	O
.	O
this	O
means	O
that	O
the	O
computations	B
for	O
computing	O
the	O
svm	O
solution	O
scale	B
linearly	O
with	O
p	O
,	O
although	O
potentially	O
cubic5	O
in	O
n.	O
with	O
very	O
large	O
p	O
(	O
in	O
the	O
tens	O
of	O
thou-	O
sands	O
and	O
even	O
millions	O
as	O
we	O
will	O
see	O
)	O
,	O
this	O
can	O
be	O
convenient	O
.	O
it	O
turns	O
out	O
that	O
all	O
ridge-regularized	O
linear	B
models	O
with	O
wide	O
data	B
can	O
be	O
reparametrized	O
in	O
this	O
way	O
.	O
take	O
ridge	B
regression	I
,	O
for	O
example	O
:	O
ˇ	O
minimize	O
ˇ	O
d	O
.x	O
ky	O
(	O
cid:0	O
)	O
x	O
ˇk2	O
x	O
c	O
(	O
cid:21	O
)	O
ip/	O
(	O
cid:0	O
)	O
1x	O
2	O
c	O
(	O
cid:21	O
)	O
kˇk2	O
2	O
:	O
0	O
this	O
has	O
solution	O
o	O
y	O
,	O
and	O
with	O
p	O
large	O
requires	O
inversion	O
of	O
a	O
p	O
(	O
cid:2	O
)	O
p	O
matrix	B
.	O
however	O
,	O
it	O
can	O
be	O
shown	O
that	O
o	O
0	O
o˛	O
d	O
5	O
in	O
practice	O
o.n2jsj/	O
,	O
and	O
,	O
with	O
modern	O
approximate	O
solutions	O
,	O
much	O
faster	O
than	O
that	O
.	O
ˇ	O
d	O
x	O
0	O
(	O
19.10	O
)	O
6	O
7	O
382	O
0	O
c	O
(	O
cid:21	O
)	O
in/	O
svms	O
and	O
kernel	O
methods	O
ˇ	O
has	O
the	O
same	O
form	B
as	O
for	O
the	O
svm.	O
pn	O
o˛i	O
xi	O
,	O
with	O
o˛	O
d	O
.xx	O
(	O
cid:0	O
)	O
1y	O
,	O
which	O
means	O
the	O
solution	O
can	O
id1	O
be	O
obtained	O
in	O
o.n2p/	O
rather	O
than	O
o.np2/	O
computations	B
.	O
again	O
the	O
gram	O
matrix	B
has	O
played	O
a	O
role	O
,	O
and	O
o	O
we	O
now	O
imagine	O
expanding	O
the	O
p-dimensional	O
feature	O
vector	B
x	O
into	O
a	O
potentially	O
much	O
larger	O
set	B
h.x/	O
d	O
œh1.x/	O
;	O
h2.x/	O
;	O
:	O
:	O
:	O
;	O
hm.x/	O
;	O
for	O
an	O
ex-	O
ample	O
to	O
latch	O
onto	O
,	O
think	O
polynomial	O
basis	O
of	O
total	O
degree	O
d.	O
as	O
long	O
as	O
we	O
have	O
an	O
efﬁcient	O
way	O
to	O
compute	O
the	O
inner	O
products	O
hh.x/	O
;	O
h.xj	O
/i	O
for	O
any	O
x	O
,	O
we	O
can	O
compute	O
the	O
svm	O
solution	O
in	O
this	O
enlarged	O
space	B
just	O
as	O
easily	O
as	O
in	O
the	O
original	O
.	O
it	O
turns	O
out	O
that	O
convenient	O
kernel	O
functions	O
exist	O
that	O
do	O
just	O
that	O
.	O
for	O
example	O
kd	O
.x	O
;	O
z/	O
d	O
.1	O
c	O
hx	O
;	O
zi/d	O
creates	O
a	O
basis	O
expansion	O
hd	O
of	O
polynomials	O
of	O
total	O
degree	O
d	O
,	O
and	O
kd	O
.x	O
;	O
z/	O
d	O
hhd	O
.x/	O
;	O
hd	O
.z/i.	O
the	O
polynomial	O
kernels	O
are	O
mainly	O
useful	O
as	O
existence	O
proofs	O
;	O
in	O
practice	O
other	O
more	O
useful	O
kernels	O
are	O
used	O
.	O
probably	O
the	O
most	O
popular	O
is	O
the	O
radial	O
kernel	O
k.x	O
;	O
z/	O
d	O
e	O
(	O
cid:0	O
)	O
(	O
cid:13	O
)	O
kx	O
(	O
cid:0	O
)	O
zk2	O
2	O
:	O
(	O
19.11	O
)	O
this	O
is	O
a	O
positive	O
deﬁnite	O
function	B
,	O
and	O
can	O
be	O
thought	O
of	O
as	O
computing	O
an	O
inner	O
product	O
in	O
some	O
feature	O
space	B
.	O
here	O
the	O
feature	O
space	B
is	O
in	O
principle	O
inﬁnite-dimensional	O
,	O
but	O
of	O
course	O
effectively	O
ﬁnite.6	O
now	O
one	O
can	O
think	O
of	O
the	O
representation	O
(	O
19.9	O
)	O
in	O
a	O
different	O
light	O
;	O
o˛i	O
k.x	O
;	O
xi	O
/	O
;	O
(	O
19.12	O
)	O
f	O
.x/	O
d	O
o˛0	O
cx	O
o	O
i2	O
s	O
an	O
expansion	O
of	O
radial	O
basis	O
functions	O
,	O
each	O
centered	O
on	O
one	O
of	O
the	O
train-	O
ing	O
examples	O
.	O
figure	O
19.5	O
illustrates	O
such	O
an	O
expansion	O
in	O
r1	O
.	O
using	O
such	O
nonlinear	B
kernels	O
expands	O
the	O
scope	O
of	O
svms	O
considerably	O
,	O
allowing	O
one	O
to	O
ﬁt	O
classiﬁers	O
with	O
nonlinear	B
decision	O
boundaries	O
.	O
one	O
may	O
ask	O
what	O
objective	O
is	O
being	O
optimized	O
when	O
we	O
move	O
to	O
this	O
kernel	O
representation	O
.	O
this	O
is	O
covered	O
in	O
the	O
next	O
section	O
,	O
but	O
as	O
a	O
sneak	O
preview	O
we	O
present	O
the	O
criterion	O
''	O
nx	O
1	O
(	O
cid:0	O
)	O
yj	O
˛0	O
c	O
nx	O
!	O
#	O
c	O
(	O
cid:21	O
)	O
˛	O
0	O
c	O
˛0	O
;	O
˛	O
k	O
˛	O
;	O
id1	O
jd1	O
(	O
19.13	O
)	O
˛i	O
k.xj	O
;	O
xi	O
/	O
minimize	O
where	O
the	O
n	O
(	O
cid:2	O
)	O
n	O
matrix	B
k	O
has	O
entries	O
k.xj	O
;	O
xi	O
/	O
.	O
as	O
an	O
illustrative	O
example	O
in	O
r2	O
(	O
so	O
we	O
can	O
visualize	O
the	O
nonlinear	B
boundaries	O
)	O
,	O
we	O
generated	O
the	O
data	B
in	O
figure	O
19.6.	O
we	O
show	O
two	O
svm	O
6	O
a	O
bivariate	O
function	B
k.x	O
;	O
z/	O
(	O
rp	O
(	O
cid:2	O
)	O
rp	O
7	O
!	O
r1	O
)	O
is	O
positive-deﬁnite	O
if	O
,	O
for	O
every	O
q	O
,	O
every	O
q	O
(	O
cid:2	O
)	O
q	O
matrix	B
k	O
d	O
fk.xi	O
;	O
xj	O
/g	O
formed	O
using	O
distinct	O
entries	O
x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xq	O
is	O
positive	O
deﬁnite	O
.	O
the	O
feature	O
space	B
is	O
deﬁned	O
in	O
terms	O
of	O
the	O
eigen-functions	O
of	O
the	O
kernel	O
.	O
19.4	O
computations	B
and	O
the	O
kernel	O
trick	B
383	O
figure	O
19.5	O
radial	O
basis	O
functions	O
in	O
r1	O
.	O
the	O
left	O
panel	O
shows	O
a	O
collection	O
of	O
radial	O
basis	O
functions	O
,	O
each	O
centered	O
on	O
one	O
of	O
the	O
seven	O
observations	O
.	O
the	O
right	O
panel	O
shows	O
a	O
function	B
obtained	O
from	O
a	O
particular	O
linear	B
expansion	O
of	O
these	O
basis	O
functions	O
.	O
solutions	O
,	O
both	O
using	O
a	O
radial	O
kernel	O
.	O
in	O
the	O
left	O
panel	O
,	O
some	O
margin	O
errors	B
are	O
committed	O
,	O
but	O
the	O
solution	O
looks	O
reasonable	O
.	O
however	O
,	O
with	O
the	O
ﬂex-	O
ibility	O
of	O
the	O
enlarged	O
feature	O
space	B
,	O
by	O
decreasing	O
the	O
budget	O
b	O
we	O
can	O
typically	O
overﬁt	O
the	O
training	O
data	B
,	O
as	O
is	O
the	O
case	O
in	O
the	O
right	O
panel	O
.	O
a	O
sepa-	O
rate	B
little	O
blue	O
island	O
was	O
created	O
to	O
accommodate	O
the	O
one	O
blue	O
point	O
in	O
a	O
sea	O
of	O
brown	O
.	O
figure	O
19.6	O
simulated	O
data	B
in	O
two	O
classes	O
in	O
r2	O
,	O
with	O
svm	O
classiﬁers	O
computed	O
using	O
the	O
radial	O
kernel	O
(	O
19.11	O
)	O
.	O
the	O
left	O
panel	O
uses	O
a	O
larger	O
value	O
of	O
b	O
than	O
the	O
right	O
.	O
the	O
solid	O
lines	O
are	O
the	O
decision	O
boundaries	O
in	O
the	O
original	O
space	B
(	O
linear	B
boundaries	O
in	O
the	O
expanded	O
feature	O
space	B
)	O
.	O
the	O
dashed	O
lines	O
are	O
the	O
projected	O
margins	O
in	O
both	O
cases	O
.	O
−2−10120.00.51.01.5−2−1012−0.40.00.20.4radialbasisfunctionsf	O
(	O
x	O
)	O
k	O
(	O
x	O
,	O
xj	O
)	O
f	O
(	O
x	O
)	O
=α0+pjαjk	O
(	O
x	O
,	O
xj	O
)	O
xx−4−2024−4−2024x1x2llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll	O
llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−4−2024−4−2024x1x2llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll	O
lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll	O
384	O
svms	O
and	O
kernel	O
methods	O
19.5	O
function	B
fitting	O
using	O
kernels	O
the	O
analysis	B
in	O
the	O
previous	O
section	O
is	O
heuristic—replacing	O
inner	O
products	O
by	O
kernels	O
that	O
compute	O
inner	O
products	O
in	O
some	O
(	O
implicit	O
)	O
feature	O
space	B
.	O
indeed	O
,	O
this	O
is	O
how	O
kernels	O
were	O
ﬁrst	O
introduced	O
in	O
the	O
svm	O
world	O
.	O
there	O
is	O
however	O
a	O
rich	O
literature	O
behind	O
such	O
approaches	O
,	O
which	O
goes	O
by	O
the	O
name	O
function	B
ﬁtting	O
in	O
reproducing-kernel	O
hilbert	O
spaces	O
(	O
rkhss	O
)	O
.	O
we	O
give	O
a	O
very	O
brief	O
overview	O
here	O
.	O
one	O
starts	O
with	O
a	O
bivariate	O
positive-deﬁnite	O
kernel	O
k	O
w	O
rp	O
(	O
cid:2	O
)	O
rp	O
!	O
r1	O
,	O
and	O
we	O
consider	O
a	O
space	B
hk	O
of	O
functions	O
f	O
w	O
rp	O
!	O
r1	O
generated	O
by	O
the	O
kernel	O
:	O
f	O
2	O
spanfk	O
.	O
(	O
cid:1	O
)	O
;	O
z/	O
;	O
z	O
2	O
rpg7	O
the	O
kernel	O
also	O
induces	O
a	O
norm	O
on	O
the	O
space	B
kf	O
k	O
hk	O
,	O
	O
which	O
can	O
be	O
thought	O
of	O
as	O
a	O
roughness	O
measure	O
.	O
we	O
can	O
now	O
state	O
a	O
very	O
general	O
optimization	O
problem	O
for	O
ﬁtting	B
a	O
func-	O
tion	O
to	O
data	B
,	O
when	O
restricted	O
to	O
this	O
class	O
;	O
8	O
)	O
l.yi	O
;	O
˛0	O
c	O
f	O
.xi	O
//	O
c	O
(	O
cid:21	O
)	O
kf	O
k2	O
hk	O
;	O
(	O
19.14	O
)	O
(	O
nx	O
id1	O
minimize	O
f	O
2	O
hk	O
a	O
search	O
over	O
a	O
possibly	O
inﬁnite-dimensional	O
function	B
space	O
.	O
here	O
l	O
is	O
an	O
arbitrary	O
loss	B
function	I
.	O
the	O
“	O
magic	O
”	O
of	O
these	O
spaces	O
in	O
the	O
context	O
of	O
this	O
problem	O
is	O
that	O
one	O
can	O
show	O
that	O
the	O
solution	O
is	O
ﬁnite-dimensional	O
:	O
f	O
.x/	O
d	O
nx	O
o	O
o˛i	O
k.x	O
;	O
xi	O
/	O
;	O
id1	O
(	O
19.15	O
)	O
a	O
linear	B
basis	O
expansion	O
with	O
basis	O
functions	O
ki	O
.x/	O
d	O
k.x	O
;	O
xi	O
/	O
anchored	O
at	O
each	O
of	O
the	O
observed	O
“	O
vectors	O
”	O
xi	O
in	O
the	O
training	O
data	B
.	O
moreover	O
,	O
using	O
the	O
“	O
reproducing	O
”	O
property	O
of	O
the	O
kernel	O
in	O
this	O
space	B
,	O
one	O
can	O
show	O
that	O
the	O
penalty	B
reduces	O
to	O
k	O
o	O
f	O
k2	O
hk	O
(	O
19.16	O
)	O
here	O
k	O
is	O
the	O
n	O
(	O
cid:2	O
)	O
n	O
gram	O
matrix	B
of	O
evaluations	O
of	O
the	O
kernel	O
,	O
equivalent	O
to	O
the	O
xx	O
0	O
matrix	B
for	O
the	O
linear	B
case	O
.	O
o˛i	O
o˛j	O
k.xi	O
;	O
xj	O
/	O
d	O
o˛	O
d	O
nx	O
nx	O
id1	O
jd1	O
0	O
k	O
o˛	O
:	O
hence	O
the	O
abstract	O
problem	O
(	O
19.14	O
)	O
reduces	O
to	O
the	O
generalized	O
ridge	B
problem	I
minimize	O
˛2rn	O
8	O
<	O
:	O
nx	O
id1	O
0	O
@	O
yi	O
;	O
˛0	O
c	O
nx	O
jd1	O
l	O
1a	O
c	O
(	O
cid:21	O
)	O
˛	O
0	O
k	O
˛	O
9=	O
;	O
:	O
˛i	O
k.xi	O
;	O
xj	O
/	O
(	O
19.17	O
)	O
7	O
here	O
kz	O
d	O
k.	O
(	O
cid:1	O
)	O
;	O
z/	O
is	O
considered	O
a	O
function	B
of	O
the	O
ﬁrst	O
argument	B
,	O
and	O
the	O
second	O
argument	B
is	O
a	O
parameter	O
.	O
19.6	O
example	O
:	O
string	O
kernels	O
for	O
protein	O
classiﬁcation	O
385	O
indeed	O
,	O
if	O
l	O
is	O
the	O
hinge	O
loss	O
as	O
in	O
(	O
19.6	O
)	O
,	O
this	O
is	O
the	O
equivalent	O
“	O
loss	O
plus	O
penalty	B
”	O
criterion	O
being	O
ﬁt	O
by	O
the	O
kernel	O
svm	O
.	O
alternatively	O
,	O
if	O
l	O
is	O
the	O
bi-	O
nomial	O
deviance	O
loss	O
as	O
in	O
(	O
19.7	O
)	O
,	O
this	O
would	O
ﬁt	O
a	O
kernel	O
version	O
of	O
logistic	B
regression	I
.	O
hence	O
most	O
ﬁtting	B
methods	O
can	O
be	O
generalized	O
to	O
accommodate	O
kernels	O
.	O
this	O
formalization	O
opens	O
the	O
door	O
to	O
a	O
wide	O
variety	O
of	O
applications	O
,	O
de-	O
pending	O
on	O
the	O
kernel	O
function	B
used	O
.	O
alternatively	O
,	O
as	O
long	O
as	O
we	O
can	O
com-	O
pute	O
suitable	O
similarities	O
between	O
objects	O
,	O
we	O
can	O
build	O
sophisticated	O
clas-	O
siﬁers	O
and	O
other	O
models	B
for	O
making	O
predictions	O
about	O
other	O
attributes	O
of	O
the	O
objects.8	O
in	O
the	O
next	O
section	O
we	O
consider	O
a	O
particular	O
example	O
.	O
19.6	O
example	O
:	O
string	O
kernels	O
for	O
protein	O
classiﬁcation	O
one	O
of	O
the	O
important	O
problems	O
in	O
computational	O
biology	O
is	O
to	O
classify	O
pro-	O
teins	O
into	O
functional	O
and	O
structural	O
classes	O
based	O
on	O
their	O
sequence	O
simi-	O
larities	O
.	O
protein	O
molecules	O
can	O
be	O
thought	O
of	O
as	O
strings	O
of	O
amino	O
acids	O
,	O
and	O
differ	O
in	O
terms	O
of	O
length	O
and	O
composition	O
.	O
in	O
the	O
example	O
we	O
consider	O
,	O
the	O
lengths	O
vary	O
between	O
75	O
and	O
160	O
amino-acid	O
molecules	O
,	O
each	O
of	O
which	O
can	O
be	O
one	O
of	O
20	O
different	O
types	O
,	O
labeled	O
using	O
the	O
letters	O
of	O
the	O
alphabet	O
.	O
here	O
follow	O
two	O
protein	O
examples	O
x1	O
and	O
x2	O
,	O
of	O
length	O
110	O
and	O
153	O
respectively	O
:	O
iptsalvketlallsthrtllianetlripvpvhknhqlcteeifqgigtlesqtvqggtv	O
erlfknlslikkyidgqkkkcgeerrrvnqfldylqeflgvmntewi	O
phrrdlcsrsiwlarkirsdltaltesyvkhqglwselteaerlqenlqayrtfhvlla	O
rlledqqvhftptegdfhqaihtlllqvaafayqieelmilleykiprneadgmlfekk	O
lwglkvlqelsqwtvrsihdlrfisshqtgip	O
we	O
treat	O
the	O
proteins	O
x	O
as	O
documents	O
consisting	O
of	O
letters	O
,	O
with	O
a	O
dictio-	O
nary	O
of	O
size	O
20.	O
our	O
feature	O
vector	B
hm.x/	O
will	O
consist	O
of	O
the	O
counts	O
for	O
all	O
m-grams	O
in	O
the	O
protein—that	O
is	O
,	O
distinct	O
sequences	O
of	O
consecutive	O
letters	O
of	O
length	O
m.	O
as	O
an	O
illustration	O
,	O
we	O
use	O
m	O
d	O
3	O
,	O
which	O
results	O
in	O
203	O
d8,000	O
possible	O
sub-sequences	O
;	O
hence	O
h3.x/	O
will	O
be	O
a	O
vector	B
of	O
length	O
8,000	O
,	O
with	O
each	O
element	O
the	O
number	O
of	O
times	O
that	O
particular	O
sub-sequence	O
occurs	O
in	O
the	O
protein	O
x.	O
in	O
our	O
example	O
,	O
the	O
sub-sequence	O
lqe	O
occurs	O
once	O
in	O
the	O
lqe.x2/	O
d	O
2.	O
ﬁrst	O
,	O
and	O
twice	O
in	O
the	O
second	O
protein	O
,	O
so	O
h3	O
the	O
number	O
of	O
possible	O
sequences	O
of	O
length	O
m	O
is	O
20m	O
,	O
which	O
can	O
be	O
very	O
lqe.x1/	O
d	O
1	O
and	O
h3	O
8	O
as	O
long	O
as	O
the	O
similarities	O
behave	O
like	O
inner	O
products	O
;	O
i.e	O
.	O
they	O
form	B
positive	O
semi-deﬁnite	O
matrices	O
.	O
386	O
svms	O
and	O
kernel	O
methods	O
large	O
for	O
moderate	O
m.	O
also	O
the	O
vast	O
majority	O
of	O
the	O
sub-sequences	O
do	O
not	O
match	O
the	O
strings	O
in	O
our	O
training	O
set	B
,	O
which	O
means	O
hm.x/	O
will	O
be	O
sparse	O
.	O
it	O
turns	O
out	O
that	O
we	O
can	O
compute	O
the	O
n	O
(	O
cid:2	O
)	O
n	O
inner	O
product	O
matrix	B
or	O
string	O
kernel	O
km.x1	O
;	O
x2/	O
d	O
hhm.x1/	O
;	O
hm.x2/i	O
efﬁciently	O
using	O
tree	O
structures	O
,	O
without	O
actually	O
computing	O
the	O
individual	O
vectors	O
.	O
	O
armed	O
with	O
the	O
kernel	O
,	O
we	O
9	O
figure	O
19.7	O
roc	O
curves	O
for	O
two	O
classiﬁers	O
ﬁt	O
to	O
the	O
protein	O
data	O
.	O
the	O
roc	O
curves	O
were	O
computed	O
using	O
10-fold	B
cross-validation	O
,	O
and	O
trace	O
the	O
trade-off	O
between	O
false-positive	O
and	O
true-positive	O
error	O
rates	O
as	O
the	O
classiﬁer	O
threshold	O
is	O
varied	O
.	O
the	O
area	O
under	O
the	O
curve	O
(	O
auc	O
)	O
summarizes	O
the	O
overall	O
performance	O
of	O
each	O
classiﬁer	O
.	O
here	O
the	O
svm	O
is	O
slightly	O
superior	O
to	O
kernel	O
logistic	B
regression	I
.	O
can	O
now	O
use	O
it	O
to	O
ﬁt	O
a	O
regularized	O
svm	O
or	O
logistic	B
regression	I
model	O
,	O
as	O
outlined	O
in	O
the	O
previous	O
section	O
.	O
the	O
data	B
consist	O
of	O
1708	O
proteins	O
in	O
two	O
classes—negative	O
(	O
1663	O
)	O
and	O
positive	O
(	O
45	O
)	O
.	O
we	O
ﬁt	O
both	O
the	O
kernel	O
svm	O
and	O
kernel	O
logistic	B
regression	I
models	O
.	O
for	O
both	O
methods	O
,	O
cross-validation	O
suggested	O
a	O
very	O
small	O
value	O
for	O
(	O
cid:21	O
)	O
.	O
figure	O
19.7	O
shows	O
the	O
roc	O
trade-off	O
curve	O
for	O
each	O
,	O
using	O
10-fold	B
cross-validation	O
.	O
here	O
the	O
svm	O
outperforms	O
logistic	B
regression	I
.	O
protein	O
classificationfalse-positive	O
ratetrue-positive	O
rate0.00.20.40.60.81.00.00.20.40.60.81.0aucsvm	O
0.84klr	O
0.78	O
19.7	O
svms	O
:	O
concluding	O
remarks	O
387	O
19.7	O
svms	O
:	O
concluding	O
remarks	O
svms	O
have	O
been	O
wildly	O
successful	O
,	O
and	O
are	O
one	O
of	O
the	O
“	O
must	O
have	O
”	O
tools	O
in	O
any	O
machine-learning	O
toolbox	O
.	O
they	O
have	O
been	O
extended	O
to	O
cover	O
many	O
dif-	O
ferent	O
scenarios	O
,	O
other	O
than	O
two-class	O
classiﬁcation	O
,	O
with	O
some	O
awkward-	O
ness	O
in	O
cases	O
.	O
the	O
extension	O
to	O
nonlinear	B
function-ﬁtting	O
via	O
kernels	O
(	O
in-	O
spiring	O
the	O
“	O
machine	O
”	O
in	O
the	O
name	O
)	O
generated	O
a	O
mini	O
industry	O
.	O
kernels	O
are	O
parametrized	O
,	O
learned	O
from	O
data	O
,	O
with	O
special	O
problem-speciﬁc	O
structure	O
,	O
and	O
so	O
on	O
.	O
on	O
the	O
other	O
hand	O
,	O
we	O
know	O
that	O
ﬁtting	B
high-dimensional	O
nonlinear	B
functions	O
is	O
intrinsically	O
difﬁcult	O
(	O
the	O
“	O
curse	O
of	O
dimensionality	O
”	O
)	O
,	O
and	O
svms	O
are	O
not	O
immune	O
.	O
the	O
quadratic	O
penalty	B
implicit	O
in	O
kernel	O
methodology	O
means	O
all	O
features	O
are	O
included	O
in	O
the	O
model	B
,	O
and	O
hence	O
sparsity	O
is	O
gen-	O
erally	O
not	O
an	O
option	O
.	O
why	O
then	O
this	O
unbridled	O
enthusiasm	O
?	O
classiﬁers	O
are	O
far	O
less	O
sensitive	O
to	O
bias–variance	O
tradeoffs	O
,	O
and	O
svms	O
are	O
mostly	O
popular	O
for	O
their	O
classiﬁcation	O
performance	O
.	O
the	O
ability	O
to	O
deﬁne	O
a	O
kernel	O
for	O
mea-	O
suring	O
similarities	O
between	O
abstract	O
objects	O
,	O
and	O
then	O
train	O
a	O
classiﬁer	O
,	O
is	O
a	O
novelty	O
added	O
by	O
these	O
approaches	O
that	O
was	O
missed	O
in	O
the	O
past	O
.	O
19.8	O
kernel	O
smoothing	B
and	O
local	O
regression	B
the	O
phrase	O
“	O
kernel	O
methodology	O
”	O
might	O
mean	O
something	O
a	O
little	O
different	O
to	O
statisticians	O
trained	O
in	O
the	O
1970–90	O
period	O
.	O
kernel	O
smoothing	B
represents	O
a	O
broad	O
range	O
of	O
tools	O
for	O
performing	O
non-	O
and	O
semi-parametric	O
regres-	O
sion	O
.	O
figure	O
19.8	O
shows	O
a	O
gaussian	O
kernel	O
smooth	O
ﬁt	O
to	O
some	O
artiﬁcial	O
data	B
fxi	O
;	O
yign	O
1.	O
it	O
computes	O
at	O
each	O
point	O
x0	O
a	O
weighted	O
average	O
of	O
the	O
y-values	O
of	O
neighboring	O
points	O
,	O
with	O
weights	O
given	O
by	O
the	O
height	O
of	O
the	O
kernel	O
.	O
in	O
its	O
simplest	O
form	B
,	O
this	O
estimate	B
can	O
be	O
written	O
as	O
f	O
.x0/	O
d	O
nx	O
o	O
yi	O
k	O
(	O
cid:13	O
)	O
.x0	O
;	O
xi	O
/	O
;	O
(	O
19.18	O
)	O
id1	O
where	O
k	O
(	O
cid:13	O
)	O
.x0	O
;	O
xi	O
/	O
represents	O
the	O
radial	O
kernel	O
with	O
width	O
parameter	O
(	O
cid:13	O
)	O
.9	O
notice	O
the	O
similarity	O
to	O
(	O
19.15	O
)	O
;	O
here	O
the	O
o˛i	O
d	O
yi	O
,	O
and	O
the	O
complexity	O
of	O
the	O
model	B
is	O
controlled	O
by	O
(	O
cid:13	O
)	O
.	O
despite	O
this	O
similarity	O
,	O
and	O
the	O
use	O
of	O
the	O
same	O
kernel	O
,	O
these	O
methodologies	O
are	O
rather	O
different	O
.	O
the	O
focus	O
here	O
is	O
on	O
local	O
estimation	B
,	O
and	O
the	O
kernel	O
does	O
the	O
local-	O
izing	O
.	O
expression	O
(	O
19.18	O
)	O
is	O
almost	O
a	O
weighted	O
average—almost	O
because	O
9	O
here	O
k	O
(	O
cid:13	O
)	O
.x	O
;	O
(	O
cid:22	O
)	O
/	O
is	O
the	O
normalized	O
gaussian	O
density	B
with	O
mean	O
(	O
cid:22	O
)	O
and	O
variance	O
1=	O
(	O
cid:13	O
)	O
.	O
388	O
svms	O
and	O
kernel	O
methods	O
figure	O
19.8	O
a	O
gaussian	O
kernel	O
smooth	O
of	O
simulated	O
data	B
.	O
the	O
points	O
come	O
from	O
the	O
blue	O
curve	O
with	O
added	O
random	O
errors	B
.	O
the	O
kernel	O
smoother	O
ﬁts	O
a	O
weighted	O
mean	O
of	O
the	O
observations	O
,	O
with	O
the	O
weighting	O
kernel	O
centered	O
at	O
the	O
target	O
point	O
,	O
x0	O
in	O
this	O
case	O
.	O
the	O
points	O
shaded	O
orange	O
contribute	O
to	O
the	O
ﬁt	O
at	O
x0	O
.	O
as	O
x0	O
moves	O
across	O
the	O
domain	O
,	O
the	O
smoother	O
traces	O
out	O
the	O
green	O
curve	O
.	O
the	O
width	O
of	O
the	O
kernel	O
is	O
a	O
tuning	O
parameter	O
.	O
we	O
have	O
depicted	O
the	O
gaussian	O
weighting	O
kernel	O
in	O
this	O
ﬁgure	O
for	O
illustration	O
;	O
in	O
fact	O
its	O
vertical	O
coordinates	O
are	O
all	O
positive	O
and	O
integrate	O
to	O
one	O
.	O
pn	O
id1	O
k	O
(	O
cid:13	O
)	O
.x0	O
;	O
xi	O
/	O
(	O
cid:25	O
)	O
1.	O
in	O
fact	O
,	O
the	O
nadaraya–watson	O
estimator	B
is	O
more	O
ex-	O
plicit	O
:	O
fn	O
w	O
.x0/	O
dpn	O
pn	O
o	O
id1	O
yi	O
k	O
(	O
cid:13	O
)	O
.x0	O
;	O
xi	O
/	O
id1	O
k	O
(	O
cid:13	O
)	O
.x0	O
;	O
xi	O
/	O
:	O
(	O
19.19	O
)	O
although	O
figure	O
19.8	O
is	O
one-dimensional	O
,	O
the	O
same	O
formulation	O
applies	O
to	O
x	O
in	O
higher	O
dimensions	O
.	O
weighting	O
kernels	O
other	O
than	O
the	O
gaussian	O
are	O
typically	O
favored	O
;	O
in	O
par-	O
ticular	O
,	O
near-neighbor	O
kernels	O
with	O
compact	O
support	O
.	O
for	O
example	O
,	O
the	O
tricube	O
kernel	O
used	O
by	O
the	O
lowess	B
smoother	O
in	O
r	O
is	O
deﬁned	O
as	O
follows	O
:	O
1	O
deﬁne	O
di	O
d	O
kx0	O
(	O
cid:0	O
)	O
xik2	O
;	O
i	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
n	O
,	O
and	O
let	O
d.m/	O
be	O
the	O
mth	O
smallest	O
(	O
the	O
distance	O
of	O
the	O
mth	O
nearest	O
neighbor	O
to	O
x0	O
)	O
.	O
let	O
ui	O
d	O
di	O
=d.m/	O
;	O
i	O
d	O
1	O
;	O
:	O
:	O
:	O
;	O
n.	O
llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.0−1.0−0.50.00.51.01.5gaussian	O
kernelxylllllllllllllllllllllllllllllllllllllllllllllllllx0	O
19.8	O
kernel	O
smoothing	B
and	O
local	O
regression	B
2	O
the	O
tricube	O
kernel	O
is	O
given	O
by	O
ks.x0	O
;	O
xi	O
/	O
d	O
(	O
(	O
cid:0	O
)	O
1	O
(	O
cid:0	O
)	O
u3	O
i	O
(	O
cid:1	O
)	O
3	O
0	O
if	O
ui	O
	O
1	O
;	O
otherwise	O
,	O
389	O
(	O
19.20	O
)	O
where	O
s	O
d	O
m=n	O
,	O
the	O
span	O
of	O
the	O
kernel	O
.	O
near-neighbor	O
kernels	O
such	O
as	O
this	O
adapt	O
naturally	O
to	O
the	O
local	O
density	B
of	O
the	O
xi	O
;	O
wider	O
in	O
low-density	O
regions	O
,	O
narrower	O
in	O
high-density	O
regions	O
.	O
a	O
tricube	O
kernel	O
is	O
illustrated	O
in	O
figure	O
19.9.	O
figure	O
19.9	O
local	O
regression	B
ﬁt	O
to	O
the	O
simulated	O
data	B
.	O
at	O
each	O
point	O
x0	O
,	O
we	O
ﬁt	O
a	O
locally	O
weighted	O
linear	B
least-squares	O
model	B
,	O
and	O
use	O
the	O
ﬁtted	O
value	O
to	O
estimate	B
o	O
flr.x0/	O
.	O
here	O
we	O
use	O
the	O
tricube	O
kernel	O
(	O
19.20	O
)	O
,	O
with	O
a	O
span	O
of	O
25	O
%	O
.	O
the	O
orange	O
points	O
are	O
in	O
the	O
weighting	O
neighborhood	O
,	O
and	O
we	O
see	O
the	O
orange	O
linear	B
ﬁt	O
computed	O
by	O
kernel	O
weighted	O
least	B
squares	I
.	O
the	O
green	O
dot	O
is	O
the	O
ﬁtted	O
value	O
at	O
x0	O
from	O
this	O
local	O
linear	B
ﬁt	O
.	O
weighted	O
means	O
suffer	O
from	O
boundary	O
bias—we	O
can	O
see	O
in	O
figure	O
19.8	O
that	O
the	O
estimate	B
appears	O
biased	O
upwards	O
at	O
both	O
boundaries	O
.	O
the	O
reason	O
is	O
that	O
,	O
for	O
example	O
on	O
the	O
left	O
,	O
the	O
estimate	B
for	O
the	O
function	B
on	O
the	O
boundary	O
aver-	O
ages	O
points	O
always	O
to	O
the	O
right	O
,	O
and	O
since	O
the	O
function	B
is	O
locally	O
increasing	O
,	O
there	O
is	O
an	O
upward	O
bias	O
.	O
local	O
linear	B
regression	O
is	O
a	O
natural	O
generalization	O
that	O
ﬁxes	O
such	O
problems	O
.	O
at	O
each	O
point	O
x0	O
we	O
solve	O
the	O
following	O
weighted	O
llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.0−1.0−0.50.00.51.01.5local	O
regression	B
(	O
tricube	O
)	O
xylllllllllllllllllllllllllllx0	O
390	O
svms	O
and	O
kernel	O
methods	O
least-squares	O
problem	O
o	O
ˇ0.x0/	O
;	O
.	O
nx	O
o	O
ˇ.x0//	O
d	O
arg	O
min	O
id1	O
o	O
ˇ0.x0/	O
c	O
x0	O
ˇ.x0/	O
.	O
one	O
can	O
show	O
that	O
,	O
to	O
ﬁrst	O
order	O
,	O
ks.x0	O
;	O
xi	O
/.yi	O
(	O
cid:0	O
)	O
ˇ0	O
(	O
cid:0	O
)	O
xi	O
ˇ/2	O
:	O
(	O
19.21	O
)	O
ˇ0	O
;	O
ˇ	O
flr.x0/	O
d	O
o	O
then	O
o	O
o	O
flr.x0/	O
removes	O
the	O
boundary	O
bias	O
exactly.	O
10	O
figure	O
19.9	O
illustrates	O
the	O
procedure	O
on	O
our	O
simulated	O
data	B
,	O
using	O
the	O
tricube	O
kernel	O
with	O
a	O
span	O
of	O
25	O
%	O
of	O
the	O
data	B
.	O
in	O
practice	O
,	O
the	O
width	O
of	O
the	O
kernel	O
(	O
the	O
span	O
here	O
)	O
has	O
to	O
be	O
selected	O
by	O
some	O
means	O
;	O
typically	O
we	O
use	O
cross-validation	O
.	O
local	O
regression	B
works	O
in	O
any	O
dimension	O
;	O
that	O
is	O
,	O
we	O
can	O
ﬁt	O
two-	O
or	O
higher-dimensional	O
surfaces	O
using	O
exactly	O
the	O
same	O
technique	O
.	O
here	O
the	O
ability	O
to	O
remove	O
boundary	O
bias	O
really	O
pays	O
off	O
,	O
since	O
the	O
boundaries	O
can	O
be	O
complex	O
.	O
these	O
are	O
referred	O
to	O
as	O
memory-based	O
methods	O
,	O
since	O
there	O
is	O
no	O
ﬁtted	O
model	B
.	O
we	O
have	O
to	O
save	O
all	O
the	O
training	O
data	B
,	O
and	O
recompute	O
the	O
local	O
ﬁt	O
every	O
time	O
we	O
make	O
a	O
prediction	O
.	O
like	O
kernel	O
svms	O
and	O
their	O
relatives	O
,	O
kernel	O
smoothing	B
and	O
local	O
regres-	O
sion	O
break	O
down	O
in	O
high	O
dimensions	O
.	O
here	O
the	O
near	O
neighborhoods	O
become	O
so	O
wide	O
that	O
they	O
are	O
no	O
longer	O
local	O
.	O
19.9	O
notes	O
and	O
details	O
in	O
the	O
late	O
1980s	O
and	O
early	O
1990s	O
,	O
machine-learning	O
research	O
was	O
largely	O
driven	O
by	O
prediction	O
problems	O
,	O
and	O
the	O
neural-network	O
community	O
at	O
at	O
&	O
t	B
bell	O
laboratories	O
was	O
amongst	O
the	O
leaders	O
.	O
the	O
problem	O
of	O
the	O
day	O
was	O
the	O
us	O
post-ofﬁce	O
handwritten	O
zip-code	O
ocr	O
challenge—a	O
10-class	O
im-	O
age	O
classiﬁcation	O
problem	O
.	O
vladimir	O
vapnik	O
was	O
part	O
of	O
this	O
team	O
,	O
and	O
along	O
with	O
colleagues	O
invented	O
a	O
more	O
direct	O
approach	O
to	O
classiﬁcation	O
,	O
the	O
support-vector	O
machine	O
.	O
this	O
started	O
with	O
the	O
seminal	O
paper	O
by	O
boser	O
et	O
al	O
.	O
(	O
1992	O
)	O
,	O
which	O
introduced	O
the	O
optimal	O
margin	O
classiﬁer	O
(	O
optimal	O
separating	O
hyperplane	O
)	O
;	O
see	O
also	O
vapnik	O
(	O
1996	O
)	O
.	O
the	O
ideas	O
took	O
off	O
quite	O
rapidly	O
,	O
at-	O
tracting	O
a	O
large	O
cohort	O
of	O
researchers	O
,	O
and	O
evolved	O
into	O
the	O
more	O
general	O
class	O
of	O
“	O
kernel	O
”	O
methods—that	O
is	O
,	O
models	B
framed	O
in	O
reproducing-kernel	O
hilbert	O
spaces	O
.	O
a	O
good	O
general	O
reference	O
is	O
sch¨olkopf	O
and	O
smola	O
(	O
2001	O
)	O
.	O
x	O
c	O
ˇ0	O
de-	O
ﬁne	O
a	O
linear	B
decision	O
boundary	O
fx	O
j	O
f	O
.x/	O
d	O
0g	O
in	O
rp	O
(	O
an	O
afﬁne	O
set	B
of	O
co-	O
dimension	O
one	O
)	O
.	O
the	O
unit	O
vector	B
normal	O
to	O
the	O
boundary	O
is	O
ˇ=kˇk2	O
,	O
where	O
k	O
(	O
cid:1	O
)	O
k2	O
denotes	O
the	O
`2	O
or	O
euclidean	O
norm	O
.	O
how	O
should	O
one	O
compute	O
the	O
dis-	O
tance	O
from	O
a	O
point	O
x	O
to	O
this	O
boundary	O
?	O
if	O
x0	O
is	O
any	O
point	O
on	O
the	O
boundary	O
1	O
[	O
p.	O
376	O
]	O
geometry	B
of	O
separating	O
hyperplanes	O
.	O
let	O
f	O
.x/	O
d	O
ˇ	O
0	O
19.9	O
notes	O
and	O
details	O
391	O
(	O
i.e	O
.	O
f	O
.x0/	O
d	O
0	O
)	O
,	O
we	O
can	O
project	O
x	O
(	O
cid:0	O
)	O
x0	O
onto	O
the	O
normal	B
,	O
giving	O
us	O
ˇ	O
0	O
.x	O
(	O
cid:0	O
)	O
x0/	O
kˇk2	O
d	O
1kˇk2	O
f	O
.x/	O
;	O
)	O
ing	O
to	O
(	O
19.3	O
)	O
can	O
be	O
written	O
as	O
ˇ	O
c	O
nx	O
as	O
claimed	O
in	O
(	O
19.1	O
)	O
.	O
note	O
that	O
this	O
is	O
the	O
signed	O
distance	O
,	O
since	O
f	O
.x/	O
will	O
be	O
positive	O
or	O
negative	O
depending	O
on	O
what	O
side	O
of	O
the	O
boundary	O
it	O
lies	O
on	O
.	O
2	O
[	O
p.	O
377	O
]	O
the	O
“	O
support	O
”	O
in	O
svm	O
.	O
the	O
lagrange	O
primal	O
problem	O
correspond-	O
0	O
1	O
ˇ0	O
;	O
ˇ	O
minimize	O
(	O
id1	O
(	O
cid:13	O
)	O
i	O
yi	O
xi	O
andpn	O
(	O
nx	O
(	O
cid:13	O
)	O
i	O
œ1	O
(	O
cid:0	O
)	O
yi	O
.ˇ0	O
c	O
x	O
0	O
i	O
ˇ/	O
;	O
2	O
ˇ	O
id1	O
ˇ	O
dpn	O
(	O
19.22	O
)	O
where	O
(	O
cid:13	O
)	O
i	O
(	O
cid:21	O
)	O
0	O
are	O
the	O
lagrange	O
multipliers	O
.	O
on	O
differentiating	O
we	O
ﬁnd	O
that	O
id1	O
yi	O
(	O
cid:13	O
)	O
i	O
d	O
0.	O
with	O
˛i	O
d	O
yi	O
(	O
cid:13	O
)	O
i	O
,	O
we	O
get	O
(	O
19.4	O
)	O
,	O
and	O
note	O
that	O
the	O
positivity	O
constraint	O
on	O
(	O
cid:13	O
)	O
i	O
will	O
lead	O
to	O
some	O
of	O
the	O
˛i	O
being	O
nx	O
zero	O
.	O
plugging	O
into	O
(	O
19.22	O
)	O
we	O
obtain	O
the	O
lagrange	O
dual	B
problem	O
nx	O
maximize	O
f	O
(	O
cid:13	O
)	O
ign	O
1	O
(	O
cid:13	O
)	O
i	O
(	O
cid:0	O
)	O
1	O
id1	O
2	O
id1	O
jd1	O
(	O
cid:13	O
)	O
i	O
(	O
cid:13	O
)	O
j	O
yi	O
yj	O
x	O
(	O
19.23	O
)	O
subject	O
to	O
(	O
cid:13	O
)	O
i	O
(	O
cid:21	O
)	O
0	O
;	O
yi	O
(	O
cid:13	O
)	O
i	O
d	O
0	O
:	O
9=	O
;	O
0	O
i	O
xj	O
nx	O
id1	O
3	O
[	O
p.	O
379	O
]	O
the	O
svm	O
loss	B
function	I
.	O
the	O
constraint	O
in	O
(	O
19.5	O
)	O
can	O
be	O
succinctly	O
captured	O
via	O
the	O
expression	O
nx	O
œ1	O
(	O
cid:0	O
)	O
yi	O
.ˇ0	O
c	O
x	O
id1	O
i	O
ˇ/c	O
	O
b	O
:	O
0	O
(	O
19.24	O
)	O
we	O
only	O
require	O
a	O
(	O
positive	O
)	O
(	O
cid:15	O
)	O
i	O
if	O
our	O
margin	O
is	O
less	O
than	O
1	O
,	O
and	O
we	O
get	O
charged	O
for	O
the	O
sum	O
of	O
these	O
(	O
cid:15	O
)	O
i.	O
we	O
now	O
use	O
a	O
lagrange	O
multiplier	B
to	O
en-	O
force	O
the	O
constraint	O
,	O
leading	O
to	O
c	O
(	O
cid:13	O
)	O
nx	O
œ1	O
(	O
cid:0	O
)	O
yi	O
.ˇ0	O
c	O
x	O
minimize	O
kˇk2	O
(	O
19.25	O
)	O
0	O
i	O
ˇ/c	O
:	O
2	O
ˇ0	O
;	O
ˇ	O
id1	O
multiplying	O
by	O
(	O
cid:21	O
)	O
d	O
1=	O
(	O
cid:13	O
)	O
gives	O
us	O
(	O
19.6	O
)	O
.	O
4	O
[	O
p.	O
380	O
]	O
the	O
svm	O
estimates	O
a	O
classiﬁer	O
.	O
the	O
following	O
derivation	O
is	O
due	O
to	O
wahba	O
et	O
al	O
.	O
(	O
2000	O
)	O
.	O
consider	O
ey	O
jxdx	O
fœ1	O
(	O
cid:0	O
)	O
yf	O
.x/cg	O
:	O
minimize	O
f	O
.x/	O
(	O
19.26	O
)	O
svms	O
and	O
kernel	O
methods	O
392	O
dropping	O
the	O
dependence	O
on	O
x	O
,	O
the	O
objective	O
can	O
be	O
written	O
as	O
pcœ1	O
(	O
cid:0	O
)	O
f	O
cc	O
p	O
(	O
cid:0	O
)	O
œ1c	O
f	O
c	O
,	O
where	O
pc	O
d	O
pr.y	O
d	O
c1jx	O
d	O
x/	O
,	O
and	O
p	O
(	O
cid:0	O
)	O
d	O
pr.y	O
d	O
(	O
cid:0	O
)	O
1jx	O
d	O
x/	O
d	O
1	O
(	O
cid:0	O
)	O
pc	O
.	O
from	O
this	O
we	O
see	O
that	O
f	O
d	O
(	O
cid:26	O
)	O
c1	O
if	O
pc	O
>	O
1	O
(	O
cid:0	O
)	O
1	O
if	O
p	O
(	O
cid:0	O
)	O
<	O
1	O
2	O
2	O
:	O
(	O
19.27	O
)	O
ˇ=k	O
o	O
5	O
[	O
p.	O
381	O
]	O
svm	O
and	O
ridged	O
logistic	B
regression	I
.	O
rosset	O
et	O
al	O
.	O
(	O
2004	O
)	O
show	O
that	O
the	O
limiting	O
solution	O
as	O
(	O
cid:21	O
)	O
#	O
0	O
to	O
(	O
19.7	O
)	O
for	O
separable	O
data	B
coincides	O
with	O
that	O
of	O
the	O
svm	O
,	O
in	O
the	O
sense	O
that	O
o	O
ˇk2	O
converges	O
to	O
the	O
same	O
quantity	B
for	O
the	O
svm	O
.	O
however	O
,	O
because	O
of	O
the	O
required	O
normalization	O
for	O
logistic	B
regression	I
,	O
the	O
svm	O
solution	O
is	O
preferable	O
.	O
on	O
the	O
other	O
hand	O
,	O
for	O
overlapped	O
situations	O
,	O
the	O
logistic-regression	O
solution	O
has	O
some	O
advan-	O
tages	O
,	O
since	O
its	O
target	O
is	O
the	O
logit	O
of	O
the	O
class	O
probabilities	B
.	O
6	O
[	O
p.	O
382	O
]	O
the	O
kernel	O
trick	B
.	O
the	O
trick	B
here	O
is	O
to	O
observe	O
that	O
from	O
the	O
score	O
.y	O
(	O
cid:0	O
)	O
x	O
ˇ/	O
c	O
(	O
cid:21	O
)	O
ˇ	O
d	O
0	O
,	O
which	O
means	O
we	O
can	O
write	O
equations	O
we	O
have	O
(	O
cid:0	O
)	O
x	O
o	O
ˇ	O
d	O
x	O
˛	O
for	O
some	O
˛	O
.	O
we	O
now	O
plug	O
this	O
into	O
the	O
score	O
equations	O
,	O
and	O
some	O
simple	O
manipulation	O
gives	O
the	O
result	O
.	O
a	O
similar	O
result	O
holds	O
for	O
ridged	O
logistic	B
regression	I
,	O
and	O
in	O
fact	O
any	O
linear	B
model	I
with	O
a	O
ridge	O
penalty	O
on	O
the	O
coefﬁcients	O
(	O
hastie	O
and	O
tibshirani	O
,	O
2004	O
)	O
.	O
7	O
[	O
p.	O
382	O
]	O
polynomial	O
kernels	O
.	O
consider	O
k2.x	O
;	O
z/	O
d	O
.1	O
c	O
hx	O
;	O
zi/2	O
,	O
for	O
x	O
0	O
0	O
(	O
and	O
z	O
)	O
in	O
r2	O
.	O
expanding	O
we	O
get	O
k2.x	O
;	O
z/	O
d	O
1	O
c	O
2x1z1	O
c	O
2x2z2	O
c	O
2x1x2z1z2	O
c	O
x2	O
1	O
z2	O
1	O
c	O
x2	O
2	O
z2	O
2	O
:	O
this	O
corresponds	O
to	O
hh2.x/	O
;	O
h2.z/i	O
with	O
p	O
p	O
p	O
h2.x/	O
d	O
.1	O
;	O
2x1	O
;	O
2x2	O
;	O
2x1x2	O
;	O
x2	O
1	O
;	O
x2	O
2	O
/	O
:	O
the	O
same	O
is	O
true	O
for	O
p	O
>	O
2	O
and	O
for	O
degree	O
d	O
>	O
2	O
.	O
8	O
[	O
p.	O
384	O
]	O
reproducing	O
kernel	O
hilbert	O
spaces	O
.	O
suppose	O
k	O
has	O
eigen	O
expan-	O
id1	O
(	O
cid:13	O
)	O
i	O
<	O
1.	O
then	O
sion	O
k.x	O
;	O
z/	O
dp1	O
id1	O
(	O
cid:13	O
)	O
i	O
(	O
cid:30	O
)	O
i	O
.x/	O
(	O
cid:30	O
)	O
i	O
.z/	O
,	O
with	O
(	O
cid:13	O
)	O
i	O
(	O
cid:21	O
)	O
0	O
andp1	O
hk	O
if	O
f	O
.x/	O
dp1	O
we	O
say	O
f	O
2	O
1x	O
id1	O
ci	O
(	O
cid:30	O
)	O
i	O
.x/	O
,	O
with	O
<	O
1	O
:	O
	O
c2	O
i	O
(	O
cid:13	O
)	O
i	O
id1	O
kf	O
k2	O
hk	O
(	O
19.28	O
)	O
often	O
kf	O
k	O
hk	O
behaves	O
like	O
a	O
roughness	O
penalty	B
,	O
in	O
that	O
it	O
penalizes	O
unlikely	O
members	O
in	O
the	O
span	O
of	O
k.	O
(	O
cid:1	O
)	O
;	O
z/	O
(	O
assuming	O
that	O
these	O
correspond	O
to	O
“	O
rough	O
”	O
functions	O
)	O
.	O
if	O
f	O
has	O
some	O
high	O
loadings	O
cj	O
on	O
functions	O
(	O
cid:30	O
)	O
j	O
with	O
small	O
eigenvalues	O
(	O
cid:13	O
)	O
j	O
(	O
i.e	O
.	O
not	O
prominent	O
members	O
of	O
the	O
span	O
)	O
,	O
the	O
norm	O
becomes	O
large	O
.	O
smoothing	B
splines	O
and	O
their	O
generalizations	O
correspond	O
to	O
function	B
ﬁtting	O
in	O
a	O
rkhs	O
(	O
wahba	O
,	O
1990	O
)	O
.	O
19.9	O
notes	O
and	O
details	O
393	O
9	O
[	O
p.	O
386	O
]	O
this	O
methodology	O
and	O
the	O
data	B
we	O
use	O
in	O
our	O
example	O
come	O
from	O
leslie	O
et	O
al	O
.	O
(	O
2003	O
)	O
.	O
10	O
[	O
p.	O
390	O
]	O
local	O
regression	B
and	O
bias	O
reduction	O
.	O
by	O
expanding	O
the	O
unknown	O
true	O
f	O
.x/	O
in	O
a	O
ﬁrst-order	O
taylor	O
expansion	O
about	O
the	O
target	O
point	O
x0	O
,	O
one	O
can	O
show	O
that	O
e	O
o	O
flr.x0/	O
(	O
cid:25	O
)	O
f	O
.x0/	O
(	O
hastie	O
and	O
loader	O
,	O
1993	O
)	O
.	O
20	O
inference	B
after	O
model	B
selection	I
the	O
classical	O
theory	B
of	O
model	B
selection	I
focused	O
on	O
“	O
f	O
tests	O
”	O
performed	O
within	O
gaussian	O
regression	B
models	O
.	O
inference	B
after	O
model	B
selection	I
(	O
for	O
in-	O
stance	O
,	O
assessing	O
the	O
accuracy	O
of	O
a	O
ﬁtted	O
regression	B
curve	O
)	O
was	O
typically	O
done	O
ignoring	O
the	O
model	B
selection	I
process	O
.	O
this	O
was	O
a	O
matter	O
of	O
neces-	O
sity	O
:	O
the	O
combination	O
of	O
discrete	O
model	B
selection	I
and	O
continuous	O
regression	B
analysis	O
was	O
too	O
awkward	O
for	O
simple	O
mathematical	O
description	O
.	O
electronic	O
computation	O
has	O
opened	O
the	O
door	O
to	O
a	O
more	O
honest	O
analysis	B
of	O
estimation	B
accuracy	O
,	O
one	O
that	O
takes	O
account	O
of	O
the	O
variability	O
induced	O
by	O
data-based	O
model	B
selection	I
.	O
figure	O
20.1	O
displays	O
the	O
cholesterol	B
data	O
,	O
an	O
example	O
we	O
will	O
use	O
for	O
illustration	O
in	O
what	O
follows	O
:	O
cholestyramine	O
,	O
a	O
proposed	O
cholesterol-	O
lowering	O
drug	O
,	O
was	O
administered	O
to	O
n	O
d	O
164	O
men	O
for	O
an	O
average	O
of	O
seven	O
years	O
each	O
.	O
the	O
response	O
variable	O
di	O
was	O
the	O
ith	O
man	O
’	O
s	O
decrease	O
in	O
choles-	O
terol	O
level	O
over	O
the	O
course	O
of	O
the	O
experiment	O
.	O
also	O
measured	O
was	O
ci	O
,	O
his	O
compliance	O
or	O
the	O
proportion	B
of	O
the	O
intended	O
dose	O
actually	O
taken	O
,	O
ranging	O
from	O
1	O
for	O
perfect	O
compliers	O
to	O
zero	O
for	O
the	O
four	O
men	O
who	O
took	O
none	O
at	O
all	O
.	O
here	O
the	O
164	O
ci	O
values	O
have	O
been	O
transformed	O
to	O
approximately	O
follow	O
a	O
standard	O
normal	O
distribution	B
,	O
ci	O
p	O
(	O
cid:24	O
)	O
n	O
.0	O
;	O
1/	O
:	O
(	O
20.1	O
)	O
we	O
wish	O
to	O
predict	O
cholesterol	B
decrease	O
from	O
compliance	O
.	O
polynomial	O
regression	B
models	O
,	O
with	O
di	O
a	O
j	O
th-order	O
polynomial	O
in	O
ci	O
,	O
were	O
considered	O
,	O
for	O
degrees	O
j	O
d	O
1	O
;	O
2	O
;	O
3	O
;	O
4	O
;	O
5	O
,	O
or	O
6.	O
the	O
cp	O
criterion	O
(	O
12.51	O
)	O
was	O
applied	O
and	O
selected	O
a	O
cubic	O
model	B
,	O
j	O
d	O
3	O
,	O
as	O
best	O
.	O
the	O
curve	O
in	O
figure	O
20.1	O
is	O
the	O
ols	O
(	O
ordinary	O
least	B
squares	I
)	O
cubic	O
regression	B
curve	O
ﬁt	O
to	O
the	O
cholesterol	B
data	O
set	B
f.ci	O
;	O
di	O
/	O
;	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
164g	O
:	O
(	O
20.2	O
)	O
we	O
are	O
interested	O
in	O
answering	O
the	O
following	O
question	O
:	O
how	O
accurate	O
is	O
the	O
394	O
20.1	O
simultaneous	O
conﬁdence	B
intervals	I
395	O
figure	O
20.1	O
cholesterol	B
data	O
:	O
cholesterol	B
decrease	O
plotted	O
versus	O
adjusted	O
compliance	O
for	O
164	O
men	O
taking	O
cholestyramine	O
.	O
the	O
green	O
curve	O
is	O
ols	O
cubic	O
regression	B
,	O
with	O
“	O
cubic	O
”	O
selected	O
by	O
the	O
cp	O
criterion	O
.	O
how	O
accurate	O
is	O
the	O
ﬁtted	O
curve	O
?	O
ﬁtted	O
curve	O
,	O
taking	O
account	O
of	O
cp	O
selection	O
as	O
well	O
as	O
ols	O
estimation	B
?	O
(	O
see	O
section	O
20.2	O
for	O
an	O
answer	O
.	O
)	O
currently	O
,	O
there	O
is	O
no	O
overarching	O
theory	B
for	O
inference	B
after	O
model	B
selec-	O
tion	O
.	O
this	O
chapter	O
,	O
more	O
modestly	O
,	O
presents	O
a	O
short	O
series	O
of	O
vignettes	O
that	O
illustrate	O
promising	O
analyses	O
of	O
individual	O
situations	O
.	O
see	O
also	O
section	O
16.6	O
for	O
a	O
brief	O
report	O
on	O
progress	O
in	O
post-selection	O
inference	B
for	O
the	O
lasso	B
.	O
20.1	O
simultaneous	O
conﬁdence	B
intervals	I
in	O
the	O
early	O
1950s	O
,	O
just	O
before	O
the	O
beginnings	O
of	O
the	O
computer	O
revolution	O
,	O
substantial	O
progress	O
was	O
made	O
on	O
the	O
problem	O
of	O
setting	O
simultaneous	O
con-	O
ﬁdence	O
intervals	B
.	O
“	O
simultaneous	O
”	O
here	O
means	O
that	O
there	O
exists	O
a	O
catalog	O
of	O
parameters	O
of	O
possible	O
interest	O
,	O
c	O
d	O
f1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
jg	O
;	O
(	O
20.3	O
)	O
and	O
we	O
wish	O
to	O
set	B
a	O
conﬁdence	B
interval	I
for	O
each	O
of	O
them	O
with	O
some	O
ﬁxed	O
probability	O
,	O
typically	O
0.95	O
,	O
that	O
all	O
of	O
the	O
intervals	B
will	O
contain	O
their	O
respec-	O
tive	O
parameters	O
.	O
********************************************************************************************************************************************************************−2−1012−20020406080100adjusted	O
compliancecholesterol	O
decrease	O
inference	B
after	O
model	B
selection	I
396	O
as	O
a	O
ﬁrst	O
example	O
,	O
we	O
return	O
to	O
the	O
diabetes	B
data	O
of	O
section	O
7.3	O
:	O
n	O
d	O
442	O
diabetes	B
patients	O
each	O
have	O
had	O
p	O
d	O
10	O
medical	O
variables	O
measured	O
at	O
baseline	O
,	O
with	O
the	O
goal	O
of	O
predicting	O
prog	O
,	O
disease	O
progression	O
one	O
year	O
later	O
.	O
let	O
x	O
be	O
the	O
442	O
(	O
cid:2	O
)	O
10	O
matrix	B
with	O
ith	O
row	O
x	O
0	O
i	O
the	O
10	O
measurements	O
for	O
patient	O
i	O
;	O
x	O
has	O
been	O
standardized	O
so	O
that	O
each	O
of	O
its	O
columns	O
has	O
mean	O
0	O
and	O
sum	O
of	O
squares	B
1.	O
also	O
let	O
y	O
be	O
the	O
442-vector	O
of	O
centered	O
prog	O
measurements	O
(	O
that	O
is	O
,	O
subtracting	O
off	O
the	O
mean	O
of	O
the	O
prog	O
values	O
)	O
.	O
ordinary	O
least	B
squares	I
applied	O
to	O
the	O
normal	B
linear	O
model	B
,	O
y	O
(	O
cid:24	O
)	O
nn.x	O
ˇ	O
;	O
(	O
cid:27	O
)	O
2i/	O
;	O
o	O
ˇ	O
d	O
.x	O
0	O
(	O
cid:0	O
)	O
1x	O
0	O
y	O
;	O
x	O
/	O
o	O
ˇ	O
(	O
cid:24	O
)	O
np.ˇ	O
;	O
(	O
cid:27	O
)	O
2v	O
/	O
;	O
v	O
d	O
.x	O
0	O
(	O
cid:0	O
)	O
1	O
;	O
x	O
/	O
yields	O
mle	O
satisfying	O
as	O
at	O
(	O
7.34	O
)	O
.	O
nent	O
of	O
ˇ	O
,	O
is	O
(	O
20.4	O
)	O
(	O
20.5	O
)	O
(	O
20.6	O
)	O
(	O
20.7	O
)	O
the	O
95	O
%	O
student-t	O
conﬁdence	B
interval	I
(	O
11.49	O
)	O
for	O
ˇj	O
,	O
the	O
j	O
th	O
compo-	O
o	O
ˇj	O
˙	O
o	O
(	O
cid:27	O
)	O
v	O
1=2	O
jj	O
t	B
:975	O
q	O
;	O
where	O
o	O
(	O
cid:27	O
)	O
d	O
54:2	O
is	O
the	O
usual	O
unbiased	O
estimate	O
of	O
(	O
cid:27	O
)	O
,	O
o	O
(	O
cid:27	O
)	O
2	O
d	O
ky	O
(	O
cid:0	O
)	O
x	O
o	O
ˇk2=q	O
;	O
q	O
d	O
n	O
(	O
cid:0	O
)	O
p	O
d	O
432	O
;	O
q	O
(	O
20.8	O
)	O
d	O
1:97	O
is	O
the	O
0.975	O
quantile	O
of	O
a	O
student-t	O
distribution	B
with	O
q	O
and	O
t	O
:975	O
degrees	O
of	O
freedom	O
.	O
the	O
catalog	O
c	O
in	O
(	O
20.3	O
)	O
is	O
now	O
fˇ1	O
;	O
ˇ2	O
;	O
:	O
:	O
:	O
;	O
ˇ10g	O
.	O
the	O
individual	O
inter-	O
vals	O
(	O
20.7	O
)	O
,	O
shown	O
in	O
table	O
20.1	O
,	O
each	O
have	O
95	O
%	O
coverage	O
,	O
but	O
they	O
are	O
not	O
simultaneous	O
:	O
there	O
is	O
a	O
greater	O
than	O
5	O
%	O
chance	O
that	O
at	O
least	O
one	O
of	O
the	O
ˇj	O
values	O
lies	O
outside	O
its	O
claimed	O
interval	B
.	O
valid	O
95	O
%	O
simultaneous	O
intervals	B
for	O
the	O
10	O
parameters	O
appear	O
on	O
the	O
right	O
side	O
of	O
table	O
20.1.	O
these	O
are	O
the	O
scheff´e	O
intervals	B
o	O
ˇj	O
˙	O
o	O
(	O
cid:27	O
)	O
v	O
1=2	O
(	O
20.9	O
)	O
jj	O
k.˛/	O
p	O
;	O
q	O
;	O
p	O
;	O
q	O
equals	O
4.30	O
for	O
p	O
d	O
10	O
,	O
q	O
d	O
discussed	O
next	O
.	O
the	O
crucial	O
constant	O
k.˛/	O
432	O
,	O
and	O
˛	O
d	O
0:95.	O
that	O
makes	O
the	O
scheff´e	O
intervals	B
wider	O
than	O
the	O
t	B
intervals	I
(	O
20.7	O
)	O
by	O
a	O
factor	B
of	O
2.19.	O
one	O
expects	O
to	O
pay	O
an	O
extra	O
price	O
for	O
simultaneous	O
coverage	O
,	O
but	O
a	O
factor	B
greater	O
than	O
two	O
induces	O
sticker	O
shock	O
.	O
scheff´e	O
’	O
s	O
method	B
depends	O
on	O
the	O
pivotal	O
quantity	B
q	O
d	O
(	O
cid:16	O
)	O
o	O
0	O
(	O
cid:0	O
)	O
1	O
(	O
cid:16	O
)	O
o	O
ˇ	O
(	O
cid:0	O
)	O
ˇ	O
v	O
ˇ	O
(	O
cid:0	O
)	O
ˇ	O
.o	O
(	O
cid:27	O
)	O
2	O
;	O
(	O
20.10	O
)	O
20.1	O
simultaneous	O
conﬁdence	B
intervals	I
397	O
table	O
20.1	O
maximum	B
likelihood	I
estimates	O
o	O
variables	O
(	O
20.6	O
)	O
;	O
separate	O
95	O
%	O
student-t	O
conﬁdence	O
limits	O
,	O
also	O
simultaneous	O
95	O
%	O
scheff´e	O
intervals	B
.	O
the	O
scheff´e	O
intervals	B
are	O
wider	O
by	O
a	O
factor	B
of	O
2.19	O
.	O
ˇ	O
for	O
10	O
diabetes	B
predictor	O
student-t	O
scheff´e	O
o	O
ˇ	O
(	O
cid:0	O
)	O
0.5	O
lower	O
upper	O
lower	O
upper	O
(	O
cid:0	O
)	O
6.1	O
(	O
cid:0	O
)	O
12.7	O
age	O
5.1	O
(	O
cid:0	O
)	O
5.7	O
sex	O
(	O
cid:0	O
)	O
11.4	O
(	O
cid:0	O
)	O
17.1	O
(	O
cid:0	O
)	O
24.0	O
bmi	O
31.0	O
24.8	O
18.5	O
11.1	O
map	O
15.4	O
9.3	O
21.6	O
2.1	O
(	O
cid:0	O
)	O
37.7	O
(	O
cid:0	O
)	O
76.7	O
1.2	O
(	O
cid:0	O
)	O
123.0	O
tc	O
(	O
cid:0	O
)	O
9.0	O
(	O
cid:0	O
)	O
46.7	O
ldl	O
54.4	O
22.7	O
(	O
cid:0	O
)	O
38.7	O
4.8	O
(	O
cid:0	O
)	O
15.1	O
hdl	O
24.7	O
(	O
cid:0	O
)	O
24.6	O
(	O
cid:0	O
)	O
6.7	O
tch	O
23.5	O
8.4	O
ltg	O
51.9	O
35.8	O
0.6	O
19.7	O
(	O
cid:0	O
)	O
3.0	O
(	O
cid:0	O
)	O
10.3	O
glu	O
3.2	O
9.4	O
11.8	O
1.1	O
38.4	O
28.8	O
47.6	O
92.1	O
48.3	O
41.5	O
71.0	O
16.7	O
which	O
under	O
model	B
(	O
20.4	O
)	O
has	O
a	O
scaled	O
“	O
f	O
distribution	O
,	O
”	O
1	O
p	O
;	O
q	O
is	O
the	O
˛th	O
quantile	O
of	O
a	O
pfp	O
;	O
q	O
distribution	B
then	O
prfq	O
	O
k.˛/2	O
if	O
k.˛/2	O
yields	O
q	O
(	O
cid:24	O
)	O
pfp	O
;	O
q	O
:	O
	O
(	O
cid:0	O
)	O
1	O
(	O
cid:16	O
)	O
ˇ	O
(	O
cid:0	O
)	O
o	O
ˇ	O
v	O
o	O
(	O
cid:27	O
)	O
2	O
0	O
ˇ	O
(	O
cid:0	O
)	O
o	O
ˇ	O
(	O
cid:16	O
)	O
8ˆ	O
<	O
ˆ	O
:	O
pr	O
9	O
>	O
=	O
>	O
;	O
d	O
˛	O
	O
k.˛/2	O
p	O
;	O
q	O
(	O
20.11	O
)	O
p	O
;	O
q	O
g	O
d	O
˛	O
(	O
20.12	O
)	O
for	O
any	O
choice	O
of	O
ˇ	O
and	O
(	O
cid:27	O
)	O
in	O
model	B
(	O
20.4	O
)	O
.	O
having	O
observed	O
o	O
ˇ	O
and	O
o	O
(	O
cid:27	O
)	O
,	O
(	O
20.12	O
)	O
deﬁnes	O
an	O
elliptical	O
conﬁdence	O
region	O
e	O
for	O
the	O
parameter	O
vector	B
ˇ.	O
suppose	O
we	O
are	O
interested	O
in	O
a	O
particular	O
linear	B
combination	O
of	O
the	O
coor-	O
dinates	O
of	O
ˇ	O
,	O
say	O
ˇc	O
d	O
c	O
0	O
ˇ	O
;	O
(	O
20.13	O
)	O
1	O
fp	O
;	O
q	O
is	O
distributed	O
as	O
.	O
(	O
cid:31	O
)	O
2	O
p=p/=	O
.	O
(	O
cid:31	O
)	O
2	O
q=q/	O
,	O
the	O
two	O
chi-squared	O
variates	O
being	O
independent	O
.	O
calculating	O
the	O
percentiles	O
of	O
fp	O
;	O
q	O
was	O
a	O
major	O
project	O
of	O
the	O
pre-war	O
period	O
.	O
398	O
inference	B
after	O
model	B
selection	I
figure	O
20.2	O
ellipsoid	O
of	O
possible	O
vectors	O
ˇ	O
deﬁned	O
by	O
(	O
20.12	O
)	O
determines	O
conﬁdence	B
intervals	I
for	O
ˇc	O
d	O
c	O
ˇ	O
according	O
to	O
the	O
“	O
bounding	O
hyperplane	O
”	O
construction	O
illustrated	O
.	O
the	O
red	O
line	O
shows	O
the	O
conﬁdence	B
interval	I
for	O
ˇc	O
if	O
c	O
is	O
a	O
unit	O
vector	B
,	O
v	O
c	O
d	O
1	O
.	O
0	O
c	O
0	O
where	O
c	O
is	O
a	O
ﬁxed	O
p-dimensional	O
vector	B
.	O
if	O
ˇ	O
exists	O
in	O
e	O
then	O
we	O
must	O
have	O
(	O
20.14	O
)	O
0	O
which	O
turns	O
out	O
to	O
be	O
the	O
interval	B
centered	O
at	O
o	O
ˇ/	O
;	O
max	O
ˇ2	O
e	O
min	O
ˇ2	O
e	O
.c	O
0	O
1	O
0	O
o	O
ˇ	O
,	O
(	O
20.15	O
)	O
ˇc	O
2	O
ˇc	O
2	O
o	O
ˇc	O
˙	O
o	O
(	O
cid:27	O
)	O
.c	O
0	O
(	O
cid:21	O
)	O
ˇc	O
d	O
c	O
v	O
c/1=2k.˛/	O
p	O
;	O
q	O
:	O
ˇ/	O
.c	O
;	O
(	O
this	O
agrees	O
with	O
(	O
20.9	O
)	O
where	O
c	O
is	O
the	O
j	O
th	O
coordinate	O
vector	B
.0	O
;	O
:	O
:	O
:	O
;	O
0	O
;	O
1	O
;	O
0	O
,	O
:	O
:	O
:	O
;	O
0/	O
0	O
.	O
)	O
the	O
construction	O
is	O
illustrated	O
in	O
figure	O
20.2.	O
np.ˇ	O
;	O
(	O
cid:27	O
)	O
2v	O
/	O
independently	O
of	O
o	O
(	O
cid:27	O
)	O
2	O
(	O
cid:24	O
)	O
(	O
cid:27	O
)	O
2	O
(	O
cid:31	O
)	O
2	O
theorem	B
(	O
scheff´e	O
)	O
then	O
with	O
probability	O
˛	O
the	O
conﬁdence	O
statement	O
(	O
20.15	O
)	O
for	O
ˇc	O
d	O
c	O
be	O
simultaneously	O
true	O
for	O
all	O
choices	O
of	O
the	O
vector	B
c.	O
0	O
q=q	O
,	O
ˇ	O
will	O
if	O
o	O
ˇ	O
(	O
cid:24	O
)	O
0	O
here	O
we	O
can	O
think	O
of	O
“	O
model	B
selection	I
”	O
as	O
the	O
choice	O
of	O
the	O
linear	B
com-	O
bination	O
of	O
interest	O
c	O
d	O
c	O
ˇ.	O
scheff´e	O
’	O
s	O
theorem	B
allows	O
“	O
data	B
snooping	O
”	O
:	O
the	O
statistician	O
can	O
examine	O
the	O
data	B
and	O
then	O
choose	O
which	O
c	O
(	O
or	O
many	O
c	O
’	O
s	O
)	O
to	O
estimate	B
,	O
without	O
invalidating	O
the	O
resulting	O
conﬁdence	B
intervals	I
.	O
an	O
important	O
application	O
has	O
the	O
o	O
ˇj	O
’	O
s	O
as	O
independent	O
estimates	O
of	O
efﬁ-	O
cacy	O
for	O
competing	O
treatments—perhaps	O
different	O
experimental	O
drugs	O
for	O
the	O
same	O
target	O
disease	O
:	O
o	O
ˇj	O
ind	O
(	O
cid:24	O
)	O
n	O
.ˇj	O
;	O
(	O
cid:27	O
)	O
2=nj	O
/	O
;	O
for	O
j	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
j	O
;	O
(	O
20.16	O
)	O
lcb^	O
20.1	O
simultaneous	O
conﬁdence	B
intervals	I
399	O
the	O
nj	O
being	O
known	O
sample	B
sizes	O
.	O
in	O
this	O
case	O
the	O
catalog	O
c	O
might	O
comprise	O
all	O
pairwise	O
differences	O
ˇi	O
(	O
cid:0	O
)	O
ˇj	O
,	O
as	O
the	O
statistician	O
tries	O
to	O
determine	O
which	O
treatments	O
are	O
better	O
or	O
worse	O
than	O
the	O
others	O
.	O
the	O
fact	O
that	O
scheff´e	O
’	O
s	O
limits	O
apply	O
to	O
all	O
possible	O
linear	B
combinations	O
0	O
ˇ	O
is	O
a	O
blessing	O
and	O
a	O
curse	O
,	O
the	O
curse	O
being	O
their	O
very	O
large	O
width	O
,	O
as	O
seen	O
c	O
in	O
table	O
20.1.	O
narrower	O
simultaneous	O
limits	O
are	O
possible	O
if	O
we	O
restrict	O
the	O
2	O
catalog	O
c	O
,	O
for	O
instance	O
to	O
just	O
the	O
pairwise	O
differences	O
ˇi	O
(	O
cid:0	O
)	O
ˇj	O
.	O
a	O
serious	O
objection	O
,	O
along	O
fisherian	O
lines	O
,	O
is	O
that	O
the	O
scheff´e	O
conﬁdence	O
limits	O
are	O
accurate	O
without	O
being	O
correct	O
.	O
that	O
is	O
,	O
the	O
intervals	B
have	O
the	O
claimed	O
overall	O
frequentist	O
coverage	O
probability	O
,	O
but	O
may	O
be	O
misleading	O
when	O
applied	O
to	O
individual	O
cases	O
.	O
suppose	O
for	O
instance	O
that	O
(	O
cid:27	O
)	O
2=nj	O
d	O
1	O
for	O
j	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
j	O
in	O
(	O
20.16	O
)	O
and	O
that	O
we	O
observe	O
o	O
ˇ1	O
d	O
10	O
,	O
with	O
j	O
o	O
ˇjj	O
<	O
2	O
for	O
all	O
the	O
others	O
.	O
even	O
if	O
we	O
looked	O
at	O
the	O
data	B
before	O
singling	O
out	O
o	O
ˇ1	O
for	O
at-	O
tention	O
,	O
the	O
usual	O
student-t	O
interval	B
(	O
20.7	O
)	O
seems	O
more	O
appropriate	O
than	O
its	O
much	O
longer	O
scheff´e	O
version	O
(	O
20.9	O
)	O
.	O
this	O
point	O
is	O
made	O
more	O
convincingly	O
in	O
our	O
next	O
vignette	O
.	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
——	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
——	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
——	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
a	O
familiar	O
but	O
pernicious	O
abuse	O
of	O
model	B
selection	I
concerns	O
multiple	O
hypothesis	B
testing	I
.	O
suppose	O
we	O
observe	O
n	O
independent	O
normal	B
variates	O
zi	O
,	O
each	O
with	O
its	O
own	O
effect	O
size	O
(	O
cid:22	O
)	O
i	O
,	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
;	O
and	O
,	O
as	O
in	O
section	O
15.1	O
,	O
we	O
wish	O
to	O
test	O
the	O
null	O
hypotheses	O
n	O
.	O
(	O
cid:22	O
)	O
i	O
;	O
1/	O
ind	O
(	O
cid:24	O
)	O
zi	O
h0i	O
w	O
(	O
cid:22	O
)	O
i	O
d	O
0	O
:	O
(	O
20.17	O
)	O
(	O
20.18	O
)	O
being	O
alert	O
to	O
the	O
pitfalls	O
of	O
simultaneous	O
testing	B
,	O
we	O
employ	O
a	O
false-discov-	O
ery	O
rate	B
control	O
algorithm	B
(	O
15.14	O
)	O
,	O
which	O
rejects	O
r	O
of	O
the	O
n	O
null	O
hypothe-	O
ses	O
,	O
say	O
for	O
cases	O
i1	O
;	O
i2	O
;	O
:	O
:	O
:	O
;	O
ir	O
.	O
(	O
r	O
equaled	O
28	O
in	O
the	O
example	O
of	O
fig-	O
ure	O
15.3	O
.	O
)	O
so	O
far	O
so	O
good	O
.	O
the	O
“	O
familiar	O
abuse	O
”	O
comes	O
in	O
then	O
setting	O
the	O
usual	O
conﬁdence	B
intervals	I
(	O
cid:22	O
)	O
i	O
2	O
o	O
(	O
cid:22	O
)	O
i	O
˙	O
1:96	O
(	O
20.19	O
)	O
(	O
95	O
%	O
coverage	O
)	O
for	O
the	O
r	O
selected	O
cases	O
.	O
this	O
ignores	O
the	O
model	B
selection	I
process	O
:	O
the	O
data-based	O
selection	O
of	O
the	O
r	O
cases	O
must	O
be	O
taken	O
into	O
account	O
in	O
making	O
legitimate	O
inferences	O
,	O
even	O
if	O
r	O
is	O
only	O
1	O
so	O
multiplicity	O
is	O
not	O
a	O
concern	O
.	O
this	O
problem	O
is	O
addressed	O
by	O
the	O
theory	B
of	O
false-coverage	O
control	B
.	O
sup-	O
pose	O
algorithm	B
a	O
sets	O
conﬁdence	B
intervals	I
for	O
r	O
of	O
the	O
n	O
cases	O
,	O
of	O
which	O
400	O
inference	B
after	O
model	B
selection	I
r	O
are	O
actually	O
false	O
coverages	O
,	O
i.e.	O
,	O
ones	O
not	O
containing	O
the	O
true	O
effect	O
size	O
(	O
cid:22	O
)	O
i.	O
the	O
false-coverage	O
rate	B
(	O
fcr	O
)	O
of	O
a	O
is	O
the	O
expected	O
proportion	B
of	O
non-	O
coverages	O
(	O
20.20	O
)	O
fcr.a/	O
d	O
efr=rg	O
;	O
the	O
expectation	O
being	O
with	O
respect	O
to	O
model	B
(	O
20.17	O
)	O
.	O
the	O
goal	O
,	O
as	O
with	O
the	O
fdr	O
theory	B
of	O
section	O
15.2	O
,	O
is	O
to	O
construct	O
algorithm	B
a	O
to	O
control	B
fcr	O
below	O
some	O
ﬁxed	O
value	O
q.	O
the	O
byq	O
algorithm2	O
controls	O
fcr	O
below	O
level	O
q	O
in	O
three	O
easy	O
steps	O
,	O
beginning	O
with	O
model	B
(	O
20.17	O
)	O
.	O
1	O
let	O
pi	O
be	O
the	O
p-value	B
corresponding	O
to	O
zi	O
,	O
pi	O
d	O
ˆ.zi	O
/	O
(	O
20.21	O
)	O
for	O
left-sided	O
signiﬁcance	O
testing	B
,	O
and	O
order	O
the	O
p.i	O
/	O
values	O
in	O
ascending	O
order	O
,	O
p.1/	O
	O
p.2/	O
	O
p.3/	O
	O
:	O
:	O
:	O
	O
p.n	O
/	O
:	O
(	O
20.22	O
)	O
2	O
calculate	O
r	O
d	O
maxfi	O
w	O
p.i	O
/	O
	O
i	O
(	O
cid:1	O
)	O
q=ng	O
,	O
and	O
(	O
as	O
in	O
the	O
bhq	O
algorithm	B
(	O
15.14	O
)	O
–	O
(	O
15.15	O
)	O
)	O
declare	O
the	O
r	O
corresponding	O
null	O
hypotheses	O
false	O
.	O
(	O
cid:22	O
)	O
i	O
2	O
zi	O
˙	O
z.˛r/	O
;	O
(	O
cid:0	O
)	O
1.˛/	O
)	O
.	O
3	O
for	O
each	O
of	O
the	O
r	O
cases	O
,	O
construct	O
the	O
conﬁdence	B
interval	I
where	O
˛r	O
d	O
1	O
(	O
cid:0	O
)	O
rq=n	O
(	O
z.˛/	B
d	O
ˆ	O
theorem	B
20.1	O
under	O
model	B
(	O
20.17	O
)	O
,	O
byq	O
has	O
fcr	O
	O
q	O
;	O
moreover	O
,	O
none	O
of	O
the	O
intervals	B
(	O
20.23	O
)	O
contain	O
(	O
cid:22	O
)	O
i	O
d	O
0.	O
a	O
simulated	O
example	O
of	O
byq	O
was	O
run	O
according	O
to	O
these	O
speciﬁcations	O
:	O
n	O
d	O
10,000	O
;	O
(	O
cid:22	O
)	O
i	O
d	O
0	O
n	O
.	O
(	O
cid:0	O
)	O
3	O
;	O
1/	O
q	O
d	O
0:05	O
;	O
zi	O
(	O
cid:24	O
)	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
9000	O
;	O
for	O
i	O
d	O
9001	O
;	O
:	O
:	O
:	O
;	O
10,000	O
:	O
n	O
.	O
(	O
cid:22	O
)	O
i	O
;	O
1/	O
(	O
cid:22	O
)	O
i	O
(	O
cid:24	O
)	O
(	O
20.23	O
)	O
(	O
20.24	O
)	O
in	O
this	O
situation	O
we	O
have	O
9000	O
null	O
cases	O
and	O
1000	O
non-null	O
cases	O
(	O
all	O
but	O
2	O
of	O
which	O
had	O
(	O
cid:22	O
)	O
i	O
<	O
0	O
)	O
.	O
because	O
this	O
is	O
a	O
simulation	O
,	O
we	O
can	O
plot	O
the	O
pairs	O
.zi	O
;	O
(	O
cid:22	O
)	O
i	O
/	O
to	O
assess	O
the	O
byq	O
algorithm	B
’	O
s	O
performance	O
.	O
this	O
is	O
done	O
in	O
figure	O
20.3	O
for	O
the	O
1000	O
non-null	O
cases	O
(	O
the	O
green	O
points	O
)	O
.	O
byq	O
declared	O
r	O
d	O
565	O
cases	O
non-null	O
,	O
those	O
having	O
zi	O
	O
(	O
cid:0	O
)	O
2:77	O
(	O
the	O
circled	O
points	O
)	O
;	O
14	O
of	O
the	O
565	O
declarations	O
2	O
short	O
for	O
“	O
benjamini–yekutieli	O
;	O
”	O
see	O
the	O
chapter	O
endnotes	O
.	O
20.1	O
simultaneous	O
conﬁdence	B
intervals	I
401	O
figure	O
20.3	O
simulation	O
experiment	O
(	O
20.24	O
)	O
with	O
n	O
d10,000	O
cases	O
,	O
of	O
which	O
1000	O
are	O
non-null	O
;	O
the	O
green	O
points	O
.zi	O
;	O
(	O
cid:22	O
)	O
i	O
/	O
are	O
these	O
non-null	O
cases	O
.	O
the	O
fdr	O
control	B
algorithm	O
bhq	O
(	O
q	O
d	O
0:05	O
)	O
declared	O
the	O
565	O
circled	O
cases	O
having	O
zi	O
	O
(	O
cid:0	O
)	O
2:77	O
to	O
be	O
non-null	O
,	O
of	O
which	O
the	O
14	O
red	O
points	O
were	O
actually	O
null	O
.	O
the	O
heavy	O
black	O
lines	O
show	O
byq	O
95	O
%	O
conﬁdence	B
intervals	I
for	O
the	O
565	O
cases	O
,	O
only	O
17	O
of	O
which	O
failed	O
to	O
contain	O
(	O
cid:22	O
)	O
i.	O
actual	O
bayes	O
posterior	O
95	O
%	O
intervals	B
for	O
non-null	O
cases	O
(	O
20.26	O
)	O
,	O
dotted	O
lines	O
,	O
have	O
half	O
the	O
width	O
and	O
slope	O
of	O
byq	O
limits	O
.	O
were	O
actually	O
null	O
cases	O
(	O
the	O
red	O
circled	O
points	O
)	O
,	O
giving	O
false-discovery	O
pro-	O
portion	O
14=565	O
d	O
0:025.	O
the	O
heavy	O
black	O
lines	O
trace	O
the	O
byq	O
conﬁdence	O
limits	O
(	O
20.23	O
)	O
as	O
a	O
function	B
of	O
z	O
	O
(	O
cid:0	O
)	O
2:77.	O
the	O
ﬁrst	O
thing	O
to	O
notice	O
is	O
that	O
fcr	O
control	B
has	O
indeed	O
been	O
achieved	O
:	O
only	O
17	O
of	O
the	O
declared	O
cases	O
lie	O
outside	O
their	O
limits	O
(	O
the	O
14	O
nulls	O
and	O
3	O
non-nulls	O
)	O
,	O
for	O
a	O
false-coverage	O
rate	B
of	O
17=565	O
d	O
0:030	O
,	O
safely	O
less	O
than	O
q	O
d	O
0:05.	O
the	O
second	O
thing	O
,	O
however	O
,	O
is	O
that	O
the	O
byq	O
limits	O
provide	O
a	O
misleading	O
idea	O
of	O
the	O
location	O
of	O
(	O
cid:22	O
)	O
i	O
given	O
zi	O
:	O
they	O
are	O
much	O
too	O
wide	O
and	O
slope	O
too	O
low	O
,	O
especially	O
for	O
more	O
negative	O
zi	O
values	O
.	O
in	O
this	O
situation	O
we	O
can	O
describe	O
precisely	O
the	O
posterior	B
distribution	I
of	O
	O
;	O
(	O
20.25	O
)	O
(	O
cid:22	O
)	O
i	O
given	O
zi	O
for	O
the	O
non-null	O
cases	O
,	O
(	O
cid:22	O
)	O
ijzi	O
(	O
cid:24	O
)	O
n	O
	O
zi	O
(	O
cid:0	O
)	O
3	O
;	O
1	O
2	O
2	O
−8−6−4−20−10−8−6−4−20observed	O
zmllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllby.loby.up−2.77llllllllllllllbayes.lobayes.up	O
inference	B
after	O
model	B
selection	I
402	O
this	O
following	O
from	O
(	O
cid:22	O
)	O
i	O
(	O
cid:24	O
)	O
(	O
5.20	O
)	O
–	O
(	O
5.21	O
)	O
.	O
the	O
bayes	O
credible	O
95	O
%	O
limits	O
n	O
.	O
(	O
cid:22	O
)	O
i	O
;	O
1/	O
,	O
and	O
bayes	O
’	O
rule	B
n	O
.	O
(	O
cid:0	O
)	O
3	O
;	O
1/	O
,	O
zij	O
(	O
cid:22	O
)	O
i	O
(	O
cid:24	O
)	O
(	O
cid:22	O
)	O
i	O
2	O
zi	O
(	O
cid:0	O
)	O
3	O
˙	O
1p	O
2	O
2	O
1:96	O
(	O
20.26	O
)	O
are	O
indicated	O
by	O
the	O
dotted	O
lines	O
in	O
figure	O
20.3.	O
they	O
are	O
half	O
as	O
wide	O
as	O
the	O
byq	O
limits	O
,	O
and	O
have	O
slope	O
1=2	O
rather	O
than	O
1.	O
in	O
practice	O
,	O
of	O
course	O
,	O
we	O
would	O
only	O
see	O
the	O
zi	O
,	O
not	O
the	O
(	O
cid:22	O
)	O
i	O
,	O
making	O
(	O
20.26	O
)	O
unavailable	O
to	O
us	O
.	O
we	O
return	O
to	O
this	O
example	O
in	O
chapter	O
21	O
,	O
where	O
empirical	B
bayes	O
methods	O
will	O
be	O
seen	O
to	O
provide	O
a	O
good	O
approximation	O
to	O
the	O
bayes	O
limits	O
.	O
(	O
see	O
figure	O
31.5	O
.	O
)	O
as	O
with	O
scheff´e	O
’	O
s	O
method	B
,	O
the	O
byq	O
intervals	B
can	O
be	O
accused	O
of	O
being	O
accurate	O
but	O
not	O
correct	O
.	O
“	O
correct	O
”	O
here	O
has	O
a	O
bayesian/fisherian	O
ﬂavor	O
that	O
is	O
hard	O
to	O
pin	O
down	O
,	O
except	O
perhaps	O
in	O
large-scale	O
applications	O
,	O
where	O
empirical	B
bayes	O
analyses	O
can	O
suggest	O
appropriate	O
inferences	O
.	O
20.2	O
accuracy	O
after	O
model	B
selection	I
the	O
cubic	O
regression	B
curve	O
for	O
the	O
cholesterol	B
data	O
seen	O
in	O
figure	O
20.1	O
was	O
selected	O
according	O
to	O
the	O
cp	O
criterion	O
of	O
section	O
12.3.	O
polynomial	O
regression	B
models	O
,	O
predicting	O
cholesterol	B
decrease	O
di	O
in	O
terms	O
of	O
powers	O
(	O
“	O
degrees	O
”	O
)	O
of	O
adjusted	O
compliance	O
ci	O
,	O
were	O
ﬁt	O
by	O
ordinary	O
least	B
squares	I
for	O
degrees	O
0	O
;	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
6.	O
table	O
20.2	O
shows	O
cp	O
estimates	O
(	O
12.51	O
)	O
being	O
minimized	O
at	O
degree	O
3.	O
table	O
20.2	O
cp	O
table	O
for	O
cholesterol	B
data	O
of	O
figure	O
20.1	O
,	O
comparing	O
ols	O
polynomial	O
models	B
of	O
degrees	O
0	O
through	O
6.	O
the	O
cubic	O
model	B
,	O
degree	O
d	O
3	O
,	O
is	O
the	O
minimizer	O
(	O
80,000	O
subtracted	O
from	O
the	O
cp	O
values	O
for	O
easier	O
comparison	O
;	O
assumes	O
(	O
cid:27	O
)	O
d	O
22:0	O
)	O
.	O
degree	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
cp	O
71887	O
1132	O
1412	O
667	O
1591	O
1811	O
2758	O
we	O
wish	O
to	O
assess	O
the	O
accuracy	O
of	O
the	O
ﬁtted	O
curve	O
,	O
taking	O
account	O
of	O
both	O
the	O
cp	O
model	B
selection	I
method	O
and	O
the	O
ols	O
ﬁtting	B
process	O
.	O
the	O
bootstrap	O
20.2	O
accuracy	O
after	O
model	B
selection	I
403	O
is	O
a	O
natural	O
candidate	O
for	O
the	O
job	O
.	O
here	O
we	O
will	O
employ	O
the	O
nonparamet-	O
ric	O
bootstrap	O
of	O
section	O
10.2	O
(	O
rather	O
than	O
the	O
parametric	B
bootstrap	O
of	O
sec-	O
tion	O
10.4	O
,	O
though	O
this	O
would	O
be	O
no	O
more	O
difﬁcult	O
to	O
carry	O
out	O
)	O
.	O
the	O
cholesterol	B
data	O
set	B
(	O
20.2	O
)	O
comprises	O
n	O
d	O
164	O
pairs	O
xi	O
d	O
(	O
cid:3	O
)	O
(	O
10.13	O
)	O
consists	O
of	O
164	O
pairs	O
.ci	O
;	O
di	O
/	O
;	O
a	O
nonparametric	B
bootstrap	O
sample	B
x	O
chosen	O
at	O
random	O
and	O
with	O
replacement	O
from	O
the	O
original	O
164.	O
let	O
t	B
.x/	O
be	O
the	O
curve	O
obtained	O
by	O
applying	O
the	O
cp/ols	O
algorithm	B
to	O
the	O
original	O
data	B
(	O
cid:3	O
)	O
;	O
and	O
for	O
a	O
given	O
set	B
x	O
and	O
likewise	O
t	B
.x	O
point	O
c	O
on	O
the	O
compliance	O
scale	B
let	O
/	O
for	O
the	O
algorithm	B
applied	O
to	O
x	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
o	O
	O
c	O
d	O
t	B
.c	O
;	O
x	O
(	O
cid:3	O
)	O
/	O
(	O
20.27	O
)	O
be	O
the	O
value	O
of	O
t	B
.x	O
(	O
cid:3	O
)	O
/	O
evaluated	O
at	O
compliance	O
d	O
c.	O
figure	O
20.4	O
a	O
histogram	O
of	O
4000	O
nonparametric	B
bootstrap	O
replications	O
for	O
polynomial	O
regression	B
estimates	O
of	O
cholesterol	B
decreases	O
d	O
at	O
adjusted	O
compliance	O
c	O
d	O
(	O
cid:0	O
)	O
2.	O
blue	O
histogram	O
,	O
adaptive	B
estimator	O
o	O
(	O
cid:3	O
)	O
c	O
(	O
20.27	O
)	O
,	O
using	O
full	B
cp/ols	O
algorithm	B
for	O
	O
each	O
bootstrap	O
data	B
set	O
;	O
line	O
histogram	O
,	O
using	O
ols	O
only	O
with	O
degree	O
3	O
for	O
each	O
bootstrap	O
data	B
set	O
.	O
bootstrap	O
standard	O
errors	O
are	O
5.98	O
and	O
3.97.	O
cholesterol	B
decrease	O
q^−2*frequency−20−10010200100200300400fixed	O
degree	O
(	O
3	O
)	O
adaptive	B
degree1.27	O
inference	B
after	O
model	B
selection	I
404	O
b	O
d	O
4000	O
nonparametric	B
bootstrap	O
replications	O
t	B
.x	O
/	O
were	O
generated.3	O
figure	O
20.4	O
shows	O
the	O
histogram	O
of	O
the	O
4000	O
o	O
c	O
replications	O
for	O
c	O
d	O
(	O
cid:0	O
)	O
2:0	O
.	O
(	O
cid:3	O
)	O
it	O
is	O
labeled	O
“	O
adaptive	B
”	O
to	O
indicate	O
that	O
cp	O
model	B
selection	I
,	O
as	O
well	O
as	O
ols	O
(	O
cid:3	O
)	O
.	O
this	O
is	O
as	O
opposed	O
to	O
the	O
“	O
ﬁxed	O
”	O
ﬁtting	B
,	O
was	O
carred	O
out	O
anew	O
for	O
each	O
x	O
histogram	O
,	O
where	O
there	O
was	O
no	O
cp	O
selection	O
,	O
cubic	O
ols	O
regression	B
always	O
being	O
used	O
.	O
(	O
cid:3	O
)	O
	O
figure	O
20.5	O
bootstrap	O
standard-error	O
estimates	O
of	O
o	O
(	O
cid:0	O
)	O
2:2	O
	O
c	O
	O
2.	O
solid	O
black	O
curve	O
,	O
adaptive	B
estimator	O
(	O
20.27	O
)	O
using	O
full	B
cp/ols	O
model	B
selection	I
estimate	O
;	O
red	O
dashed	O
curve	O
,	O
using	O
ols	O
only	O
with	O
polynomial	O
degree	O
ﬁxed	O
at	O
3	O
;	O
blue	O
dotted	O
curve	O
,	O
“	O
bagged	O
estimator	B
”	O
using	O
bootstrap	O
smoothing	B
(	O
20.28	O
)	O
.	O
average	O
standard-error	O
ratios	O
:	O
adaptive/ﬁxed	O
d	O
1:43	O
,	O
adaptive/smoothed	O
d	O
1:14	O
.	O
c	O
,	O
for	O
the	O
bootstrap	O
estimate	B
of	I
standard	I
error	I
(	O
10.16	O
)	O
obtained	O
from	O
the	O
adap-	O
tive	O
values	O
o	O
(	O
cid:3	O
)	O
c	O
was	O
5.98	O
,	O
compared	O
with	O
3.97	O
for	O
the	O
ﬁxed	O
values.4	O
in	O
this	O
	O
case	O
,	O
accounting	O
for	O
model	B
selection	I
(	O
“	O
adaptation	O
”	O
)	O
adds	O
more	O
than	O
50	O
%	O
to	O
the	O
standard	B
error	I
estimates	O
.	O
the	O
same	O
comparison	O
was	O
made	O
at	O
all	O
values	O
3	O
ten	O
times	O
more	O
than	O
needed	O
for	O
assessing	O
standard	O
errors	O
,	O
but	O
helpful	O
for	O
the	O
comparisons	O
that	O
follow	O
.	O
4	O
the	O
latter	O
is	O
not	O
the	O
usual	O
ols	O
assessment	O
,	O
following	O
(	O
8.30	O
)	O
,	O
that	O
would	O
be	O
appropriate	O
for	O
a	O
parametric	B
bootstrap	O
comparison	O
.	O
rather	O
,	O
it	O
’	O
s	O
the	O
nonparametric	B
one-sample	O
bootstrap	O
assessment	O
,	O
resampling	O
pairs	O
.xi	O
;	O
yi	O
/	O
as	O
individual	O
sample	B
points	O
.	O
−2−101201234567adjusted	O
compliance	O
cstandard	O
errors	B
of	O
q^cfixedadaptivesmoothed	O
20.2	O
accuracy	O
after	O
model	B
selection	I
405	O
of	O
the	O
adjusted	O
compliance	O
c.	O
figure	O
20.5	O
graphs	O
the	O
results	O
:	O
the	O
adaptive	B
standard	O
errors	B
averaged	O
43	O
%	O
greater	O
than	O
the	O
ﬁxed	O
values	O
.	O
the	O
standard	O
we	O
ignored	O
model	B
selection	I
in	O
assessingbse	O
.	O
95	O
%	O
conﬁdence	B
intervals	I
o	O
c	O
˙bse	O
(	O
cid:1	O
)	O
1:96	O
would	O
be	O
roughly	O
43	O
%	O
too	O
short	O
if	O
figure	O
20.6	O
“	O
adaptive	B
”	O
histogram	O
of	O
figure	O
20.4	O
now	O
split	O
into	O
19	O
%	O
of	O
4000	O
bootstrap	O
replications	O
where	O
cp	O
selected	O
linear	B
(	O
cid:3	O
)	O
d	O
1	O
(	O
cid:3	O
)	O
d	O
1	O
)	O
as	O
best	O
,	O
versus	O
81	O
%	O
having	O
m	O
(	O
cid:3	O
)	O
>	O
1.	O
m	O
regression	B
(	O
m	O
(	O
cid:3	O
)	O
cases	O
are	O
shifted	O
about	O
10	O
units	O
downward	O
.	O
(	O
the	O
m	O
>	O
1	O
cases	O
resemble	O
the	O
“	O
ﬁxed	O
”	O
histogram	O
in	O
figure	O
20.4	O
.	O
)	O
histograms	O
are	O
scaled	O
to	O
have	O
equal	O
areas	O
.	O
having	O
an	O
honest	O
assessment	O
of	O
standard	B
error	I
doesn	O
’	O
t	B
mean	O
that	O
t	B
.c	O
;	O
x/	O
(	O
20.27	O
)	O
is	O
a	O
good	O
estimator	B
.	O
model	B
selection	I
can	O
induce	O
an	O
unpleasant	O
“	O
jumpiness	O
”	O
in	O
an	O
estimator	B
,	O
as	O
the	O
original	O
data	B
vector	O
x	O
crosses	O
deﬁni-	O
tional	O
boundaries	O
.	O
this	O
happened	O
in	O
our	O
example	O
:	O
for	O
19	O
%	O
of	O
the	O
4000	O
(	O
cid:3	O
)	O
d	O
1	O
,	O
(	O
cid:3	O
)	O
,	O
the	O
cp	O
algorithm	B
selected	O
linear	B
regression	O
,	O
m	O
bootstrap	O
samples	O
x	O
as	O
best	O
,	O
and	O
in	O
these	O
cases	O
o	O
(	O
cid:3	O
)	O
(	O
cid:0	O
)	O
2:0	O
tended	O
toward	O
smaller	O
values	O
.	O
figure	O
20.6	O
(	O
cid:3	O
)	O
d	O
1	O
histogram	O
shifted	O
about	O
10	O
units	O
down	O
from	O
the	O
m	O
(	O
cid:3	O
)	O
shows	O
the	O
m	O
histogram	O
(	O
which	O
now	O
resembles	O
the	O
“	O
ﬁxed	O
”	O
histogram	O
in	O
figure	O
20.4	O
)	O
.	O
>	O
1	O
	O
discontinuous	O
estimators	O
such	O
as	O
t	B
.c	O
;	O
x/	O
can	O
’	O
t	B
be	O
bayesian	O
,	O
bayes	O
pos-	O
terior	O
expectations	O
being	O
continuous	O
.	O
they	O
can	O
also	O
suffer	O
frequentist	O
difﬁ-	O
culties	O
,	O
	O
including	O
excess	O
variability	O
and	O
overly	O
long	O
conﬁdence	B
intervals	I
.	O
3	O
cholesterol	B
decrease	O
q^−2*frequency−20−10010200204060801.27adaptivem*	O
=	O
1adaptivem*	O
>	O
1fixed	O
m	O
=	O
3	O
406	O
inference	B
after	O
model	B
selection	I
bagging	O
,	O
or	O
bootstrap	O
smoothing	B
,	O
is	O
a	O
tactic	O
for	O
improving	O
a	O
discontinuous	O
estimation	B
rule	O
by	O
averaging	B
(	O
as	O
in	O
(	O
12.80	O
)	O
and	O
chapter	O
17	O
)	O
.	O
replications	O
ft	O
.x	O
average	O
suppose	O
t	B
.x/	O
is	O
any	O
estimator	B
for	O
which	O
we	O
have	O
obtained	O
bootstrap	O
(	O
cid:3	O
)	O
b/	O
,	O
b	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
bg	O
.	O
the	O
bagged	O
version	O
of	O
t	B
.x/	O
is	O
the	O
bx	O
bd1	O
s.x/	O
d	O
1	O
b	O
(	O
cid:3	O
)	O
b/	O
:	O
t	B
.x	O
(	O
20.28	O
)	O
the	O
letter	O
s	O
here	O
stands	O
for	O
“	O
smooth.	O
”	O
small	O
changes	O
in	O
x	O
,	O
even	O
ones	O
that	O
move	O
across	O
a	O
model	B
selection	I
deﬁnitional	O
boundary	O
,	O
produce	O
only	O
small	O
changes	O
in	O
the	O
bootstrap	O
average	O
s.x/	O
.	O
averaging	B
over	O
the	O
4000	O
bootstrap	O
replications	O
of	O
t	B
.c	O
;	O
x	O
/	O
(	O
20.27	O
)	O
gave	O
a	O
bagged	O
estimate	B
sc.x/	O
for	O
each	O
value	O
of	O
c.	O
bagging	O
reduced	O
the	O
standard	O
errors	O
of	O
the	O
cp/ols	O
estimates	O
t	B
.c	O
;	O
x/	O
by	O
about	O
12	O
%	O
,	O
as	O
indicated	O
by	O
the	O
green	O
dotted	O
curve	O
in	O
figure	O
20.5.	O
t	B
.c	O
;	O
x	O
where	O
did	O
the	O
green	O
dotted	O
curve	O
come	O
from	O
?	O
all	O
4000	O
bootstrap	O
values	O
(	O
cid:3	O
)	O
b/	O
were	O
needed	O
to	O
produce	O
the	O
single	O
value	O
sc.x/	O
.	O
it	O
seems	O
as	O
if	O
we	O
would	O
need	O
to	O
bootstrap	O
the	O
bootstrap	O
in	O
order	O
to	O
computebseœsc.x/	O
.	O
fortunately	O
,	O
a	O
more	O
economical	O
calculation	O
is	O
possible	O
,	O
one	O
that	O
requires	O
only	O
the	O
original	O
b	O
bootstrap	O
computations	B
for	O
t	B
.c	O
;	O
x/	O
.	O
(	O
cid:3	O
)	O
deﬁne	O
nbj	O
d	O
#	O
ftimes	O
xj	O
occurs	O
in	O
x	O
(	O
20.29	O
)	O
for	O
b	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
b	O
and	O
j	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n.	O
for	O
instance	O
n4000	O
;	O
7	O
d	O
3	O
says	O
that	O
data	B
point	O
x7	O
occurred	O
three	O
times	O
in	O
nonparametric	B
bootstrap	O
sample	B
x4000	O
.	O
the	O
b	O
by	O
n	O
matrix	B
fnbjg	O
completely	O
describes	O
the	O
b	O
bootstrap	O
samples	O
.	O
also	O
denote	O
(	O
cid:3	O
)	O
bg	O
;	O
where	O
dots	O
denote	O
averaging	B
over	O
b	O
:	O
n	O
(	O
cid:1	O
)	O
j	O
d	O
1	O
theorem	B
20.2	O
	O
the	O
inﬁnitesimal	O
jackknife	O
estimate	O
of	O
standard	B
error	I
b	O
nbj	O
and	O
t	O
(	O
cid:3	O
)	O
(	O
cid:1	O
)	O
d	O
1	O
b	O
t	B
(	O
cid:3	O
)	O
b.	O
b	O
b	O
4	O
and	O
let	O
covj	O
indicate	O
the	O
covariance	O
in	O
the	O
bootstrap	O
sample	B
between	O
nbj	O
and	O
t	O
(	O
cid:3	O
)	O
b	O
,	O
t	B
(	O
cid:3	O
)	O
b/	O
(	O
cid:3	O
)	O
b	O
d	O
t	B
.x	O
bx	O
.nbj	O
(	O
cid:0	O
)	O
n	O
(	O
cid:1	O
)	O
j	O
/.t	O
(	O
cid:3	O
)	O
b	O
(	O
cid:0	O
)	O
t	B
p	O
bd1	O
(	O
cid:3	O
)	O
(	O
cid:1	O
)	O
/	O
;	O
covj	O
d	O
1	O
b	O
(	O
20.30	O
)	O
(	O
20.31	O
)	O
p	O
(	O
10.41	O
)	O
for	O
the	O
bagged	O
estimate	B
(	O
20.28	O
)	O
is	O
bseij	O
œsc.x/	O
d	O
0	O
@	O
nx	O
jd1	O
1a1=2	O
cov2	O
j	O
:	O
(	O
20.32	O
)	O
20.2	O
accuracy	O
after	O
model	B
selection	I
407	O
effort	O
.	O
keeping	O
track	O
of	O
nbj	O
as	O
we	O
generate	O
the	O
bootstrap	O
replications	O
t	B
us	O
to	O
compute	O
covj	O
and	O
bseœsc.x/	O
without	O
any	O
additional	O
computational	O
figure	O
20.5	O
,	O
the	O
ratio	O
ofbseijœsc.x/=bsebootœt	O
.c	O
;	O
x/	O
averaging	B
0.88.	O
in	O
fact	O
,	O
corollary	O
the	O
ratiobseijœsc.x/=bsebootœt	O
.c	O
;	O
x/	O
is	O
always	O
	O
1.	O
we	O
expect	O
averaging	B
to	O
reduce	O
variability	O
,	O
and	O
this	O
is	O
seen	O
to	O
hold	O
true	O
in	O
we	O
have	O
the	O
following	O
general	O
result	O
.	O
(	O
cid:3	O
)	O
b	O
allows	O
the	O
savings	O
due	O
to	O
bagging	O
increase	O
with	O
the	O
nonlinearity	O
of	O
t	B
.x	O
/	O
as	O
a	O
function	B
of	O
the	O
counts	O
nbj	O
(	O
or	O
,	O
in	O
the	O
language	O
of	O
section	O
10.3	O
,	O
in	O
the	O
nonlinearity	O
of	O
s.p/	O
as	O
a	O
function	B
of	O
p	O
)	O
.	O
model-selection	O
estimators	O
such	O
as	O
the	O
cp/ols	O
rule	B
tend	O
toward	O
greater	O
nonlinearity	O
and	O
bigger	O
savings	O
.	O
(	O
cid:3	O
)	O
table	O
20.3	O
proportion	B
of	O
4000	O
nonparametric	B
bootstrap	O
replications	O
of	O
cp/ols	O
algorithm	B
that	O
selected	O
degrees	O
m	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
6	O
;	O
also	O
inﬁnitesimal	O
jackknife	O
standard	O
deviations	O
for	O
proportions	O
(	O
20.32	O
)	O
,	O
which	O
mostly	O
exceed	O
the	O
estimates	O
themselves	O
.	O
proportion	B
bsdij	O
m	O
d	O
1	O
.19	O
.24	O
2	O
.12	O
.20	O
3	O
.35	O
.24	O
4	O
.07	O
.13	O
5	O
.20	O
.26	O
6	O
.06	O
.06	O
the	O
ﬁrst	O
line	O
of	O
table	O
20.3	O
shows	O
the	O
proportions	O
in	O
which	O
the	O
various	O
degrees	O
were	O
selected	O
in	O
the	O
4000	O
cholesterol	B
bootstrap	O
replications	O
,	O
19	O
%	O
for	O
linear	B
,	O
12	O
%	O
for	O
quadratic	O
,	O
35	O
%	O
for	O
cubic	O
,	O
etc	O
.	O
with	O
b	O
d	O
4000	O
,	O
the	O
proportions	O
seem	O
very	O
accurate	O
,	O
the	O
binomial	B
standard	O
error	O
for	O
0.19	O
being	O
just	O
.0:19	O
(	O
cid:1	O
)	O
0:81=4000/1=2	O
d	O
0:006	O
,	O
for	O
instance	O
.	O
(	O
cid:3	O
)	O
b	O
(	O
20.30	O
)	O
indicate	O
whether	O
theorem	B
20.2	O
suggests	O
otherwise	O
.	O
now	O
let	O
t	B
the	O
bth	O
bootstrap	O
sample	B
x	O
(	O
(	O
cid:3	O
)	O
made	O
the	O
cp	O
choice	O
m	O
(	O
cid:3	O
)	O
b	O
d	O
(	O
cid:3	O
)	O
b	O
d	O
1	O
(	O
cid:3	O
)	O
b	O
>	O
1	O
:	O
1	O
if	O
m	O
0	O
if	O
m	O
(	O
cid:3	O
)	O
d	O
1	O
,	O
t	B
(	O
20.33	O
)	O
(	O
cid:3	O
)	O
b	O
;	O
b	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
bg	O
is	O
the	O
observed	O
proportion	B
the	O
bagged	O
value	O
of	O
ft	O
408	O
0.19.	O
applying	O
the	O
bagging	O
theorem	B
yieldedbseij	O
d	O
0:24	O
,	O
as	O
seen	O
in	O
the	O
inference	B
after	O
model	B
selection	I
second	O
line	O
of	O
the	O
table	O
,	O
with	O
similarly	O
huge	O
standard	O
errors	O
for	O
the	O
other	O
proportions	O
.	O
the	O
binomial	B
standard	O
errors	B
are	O
internal	B
,	O
saying	O
how	O
quickly	O
the	O
boot-	O
strap	O
resampling	O
process	O
is	O
converging	O
to	O
its	O
ultimate	O
value	O
as	O
b	O
!	O
1.	O
the	O
inﬁnitesimal	O
jackknife	O
estimates	O
are	O
external	B
:	O
if	O
we	O
collected	O
a	O
new	O
set	B
of	O
164	O
data	B
pairs	O
.ci	O
;	O
di	O
/	O
(	O
20.2	O
)	O
the	O
new	O
proportion	B
table	O
might	O
look	O
completely	O
different	O
than	O
the	O
top	O
line	O
of	O
table	O
20.3.	O
frequentist	O
statistics	B
has	O
the	O
advantage	O
of	O
being	O
applicable	O
to	O
any	O
algo-	O
rithmic	O
procedure	O
,	O
for	O
instance	O
to	O
our	O
cp/ols	O
estimator	B
.	O
this	O
has	O
great	O
appeal	O
in	O
an	O
era	O
of	O
enormous	O
data	B
sets	O
and	O
fast	O
computation	O
.	O
the	O
draw-	O
back	O
,	O
compared	O
with	O
bayesian	O
statistics	B
,	O
is	O
that	O
we	O
have	O
no	O
guarantee	O
that	O
our	O
chosen	O
algorithm	B
is	O
best	O
in	O
any	O
way	O
.	O
classical	O
statistics	B
developed	O
a	O
theory	B
of	O
best	O
for	O
a	O
catalog	O
of	O
comparatively	O
simple	O
estimation	B
and	O
testing	B
problems	O
.	O
in	O
this	O
sense	O
,	O
modern	O
inferential	O
theory	B
has	O
not	O
yet	O
caught	O
up	O
with	O
modern	O
problems	O
such	O
as	O
data-based	O
model	B
selection	I
,	O
though	O
tech-	O
niques	O
such	O
as	O
model	B
averaging	O
(	O
e.g.	O
,	O
bagging	O
)	O
suggest	O
promising	O
steps	O
forward	O
.	O
20.3	O
selection	O
bias	O
many	O
a	O
sports	O
fan	O
has	O
been	O
victimized	O
by	O
selection	O
bias	O
.	O
your	O
team	O
does	O
wonderfully	O
well	O
and	O
tops	O
the	O
league	O
standings	O
.	O
but	O
the	O
next	O
year	O
,	O
with	O
the	O
same	O
players	O
and	O
the	O
same	O
opponents	O
,	O
you	O
’	O
re	O
back	O
in	O
the	O
pack	O
.	O
this	O
is	O
the	O
winner	O
’	O
s	O
curse	O
,	O
a	O
more	O
picturesque	O
name	O
for	O
selection	O
bias	O
,	O
the	O
ten-	O
dency	O
of	O
unusually	O
good	O
(	O
or	O
bad	O
)	O
comparative	O
performances	O
not	O
to	O
repeat	O
themselves	O
.	O
modern	O
scientiﬁc	O
technology	O
allows	O
the	O
simultaneous	O
investigation	O
of	O
hundreds	O
or	O
thousands	O
of	O
candidate	O
situations	O
,	O
with	O
the	O
goal	O
of	O
choosing	O
the	O
top	O
performers	O
for	O
subsequent	O
study	O
.	O
this	O
is	O
a	O
setup	O
for	O
the	O
heartbreak	O
of	O
selection	O
bias	O
.	O
an	O
apt	O
example	O
is	O
offered	O
by	O
the	O
prostate	B
study	O
data	B
of	O
section	O
15.1	O
,	O
where	O
we	O
observe	O
statistics	B
zi	O
measuring	O
patient–control	O
dif-	O
ferences	O
for	O
n	O
d	O
6033	O
genes	O
,	O
zi	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
i	O
;	O
1/	O
;	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
:	O
(	O
20.34	O
)	O
here	O
(	O
cid:22	O
)	O
i	O
is	O
the	O
effect	O
size	O
for	O
gene	O
i	O
,	O
the	O
true	O
difference	O
between	O
the	O
patient	O
and	O
control	O
populations	O
.	O
genes	O
with	O
large	O
positive	O
or	O
negative	O
values	O
of	O
(	O
cid:22	O
)	O
i	O
would	O
be	O
promising	O
targets	O
for	O
further	O
investigation	O
.	O
gene	O
number	O
610	O
,	O
with	O
z610	O
d	O
5:29	O
,	O
at-	O
20.3	O
selection	O
bias	O
409	O
tained	O
the	O
biggest	O
z-value	O
;	O
(	O
20.34	O
)	O
says	O
that	O
z610	O
is	O
unbiased	O
for	O
(	O
cid:22	O
)	O
610.	O
can	O
we	O
believe	O
the	O
obvious	O
estimate	B
o	O
(	O
cid:22	O
)	O
610	O
d	O
5:29	O
?	O
“	O
no	O
”	O
is	O
the	O
correct	O
selection	O
bias	O
answer	O
.	O
gene	O
610	O
has	O
won	O
a	O
contest	O
for	O
bigness	O
among	O
6033	O
contenders	O
.	O
in	O
addition	O
to	O
being	O
good	O
(	O
having	O
a	O
large	O
value	O
of	O
(	O
cid:22	O
)	O
)	O
it	O
has	O
almost	O
certainly	O
been	O
lucky	O
,	O
with	O
the	O
noise	O
in	O
(	O
20.34	O
)	O
pushing	O
z610	O
in	O
the	O
positive	O
direction—or	O
else	O
it	O
would	O
not	O
have	O
won	O
the	O
contest	O
.	O
this	O
is	O
the	O
essence	O
of	O
selection	O
bias	O
.	O
false-discovery	B
rate	I
theory	O
,	O
chapter	O
15	O
,	O
provided	O
a	O
way	O
to	O
correct	O
for	O
selection	O
bias	O
in	O
simultaneous	O
hypothesis	B
testing	I
.	O
this	O
was	O
extended	O
to	O
false-coverage	O
rates	O
in	O
section	O
20.1.	O
our	O
next	O
vignette	O
concerns	O
the	O
re-	O
alistic	O
estimation	B
of	O
effect	O
sizes	O
(	O
cid:22	O
)	O
i	O
in	O
the	O
face	O
of	O
selection	O
bias	O
.	O
we	O
begin	O
by	O
assuming	O
that	O
an	O
effect	O
size	O
(	O
cid:22	O
)	O
has	O
been	O
obtained	O
from	O
a	O
prior	B
density	O
g.	O
(	O
cid:22	O
)	O
/	O
(	O
which	O
might	O
include	O
discrete	O
atoms	O
)	O
and	O
then	O
z	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
2/	O
observed	O
,	O
(	O
cid:22	O
)	O
(	O
cid:24	O
)	O
g.	O
(	O
cid:1	O
)	O
/	O
and	O
zj	O
(	O
cid:22	O
)	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
2/	O
(	O
20.35	O
)	O
(	O
(	O
cid:27	O
)	O
2	O
is	O
assumed	O
known	O
for	O
this	O
discussion	O
)	O
.	O
the	O
marginal	O
density	B
of	O
z	O
is	O
f	O
.z/	O
dz	O
1	O
	O
	O
(	O
cid:0	O
)	O
1	O
(	O
cid:0	O
)	O
1	O
g.	O
(	O
cid:22	O
)	O
/	O
(	O
cid:30	O
)	O
(	O
cid:27	O
)	O
.z	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
/	O
d	O
(	O
cid:22	O
)	O
;	O
(	O
cid:0	O
)	O
1=2	O
exp	O
z2	O
(	O
cid:27	O
)	O
2	O
2	O
where	O
(	O
cid:30	O
)	O
(	O
cid:27	O
)	O
.z/	O
d	O
.2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2/	O
(	O
20.36	O
)	O
:	O
tweedie	O
’	O
s	O
formulais	O
an	O
intriguing	O
expression	O
for	O
the	O
bayes	O
expectation	O
5	O
of	O
(	O
cid:22	O
)	O
given	O
z.	O
theorem	B
20.3	O
observed	O
z	O
is	O
in	O
model	B
(	O
20.35	O
)	O
,	O
the	O
posterior	O
expectation	O
of	O
(	O
cid:22	O
)	O
having	O
0	O
.z/	O
d	O
d	O
ef	O
(	O
cid:22	O
)	O
jzg	O
d	O
z	O
c	O
(	O
cid:27	O
)	O
2l	O
0	O
dz	O
.z/	O
with	O
l	O
log	O
f	O
.z/	O
:	O
(	O
20.37	O
)	O
the	O
especially	O
convenient	O
feature	O
of	O
tweedie	O
’	O
s	O
formula	B
is	O
that	O
ef	O
(	O
cid:22	O
)	O
jzg	O
is	O
expressed	O
directly	O
in	O
terms	O
of	O
the	O
marginal	O
density	B
f	O
.z/	O
.	O
this	O
is	O
a	O
setup	O
for	O
empirical	B
bayes	O
estimation	B
.	O
we	O
don	O
’	O
t	B
know	O
g.	O
(	O
cid:22	O
)	O
/	O
,	O
but	O
in	O
large-scale	O
situations	O
we	O
can	O
estimate	B
the	O
marginal	O
density	B
f	O
.z/	O
from	O
the	O
observa-	O
tions	O
z	O
d	O
.z1	O
;	O
z2	O
;	O
:	O
:	O
:	O
;	O
zn	O
/	O
,	O
perhaps	O
by	O
poisson	O
regression	B
as	O
in	O
table	O
15.1	O
,	O
yielding	O
oef	O
(	O
cid:22	O
)	O
ijzig	O
d	O
zi	O
c	O
(	O
cid:27	O
)	O
2o	O
0	O
(	O
20.38	O
)	O
the	O
solid	O
curve	O
in	O
figure	O
20.7	O
shows	O
oef	O
(	O
cid:22	O
)	O
jzg	O
for	O
the	O
prostate	B
study	O
data	B
,	O
.z/	O
d	O
d	O
with	O
o	O
0	O
log	O
o	O
f	O
.z/	O
:	O
.zi	O
/	O
dz	O
l	O
l	O
410	O
inference	B
after	O
model	B
selection	I
figure	O
20.7	O
the	O
solid	O
curve	O
is	O
tweedie	O
’	O
s	O
estimate	B
oef	O
(	O
cid:22	O
)	O
jzg	O
local	O
false-discovery	O
ratecfdr.z/	O
from	O
figure	O
15.5	O
(	O
red	O
scale	B
on	O
right	O
)	O
.	O
at	O
z	O
d	O
3:5	O
,	O
oef	O
(	O
cid:22	O
)	O
jzg	O
d	O
1:96	O
andcfdr.z/	O
d	O
0:15.	O
for	O
gene	O
(	O
20.38	O
)	O
for	O
the	O
prostate	B
study	O
data	B
.	O
the	O
dashed	O
line	O
shows	O
the	O
610	O
,	O
with	O
z610	O
d	O
5:29	O
,	O
tweedie	O
’	O
s	O
estimate	B
is	O
4.03.	O
with	O
(	O
cid:27	O
)	O
2	O
d	O
1	O
and	O
o	O
jzij	O
	O
2	O
,	O
agreeing	O
with	O
the	O
local	O
false-discovery	B
rate	I
curvecfdr.z/	O
of	O
fig-	O
f	O
.z/	O
obtained	O
using	O
fourth-degree	O
log	O
polynomial	O
re-	O
gression	O
as	O
in	O
section	O
15.4.	O
the	O
curve	O
has	O
ef	O
(	O
cid:22	O
)	O
jzg	O
hovering	O
near	O
zero	O
for	O
cfdr.z/	O
d	O
0:15.	O
so	O
even	O
though	O
zi	O
d	O
3:5	O
has	O
a	O
one-sided	O
p-value	B
of	O
0.0002	O
,	O
ure	O
15.5	O
that	O
says	O
these	O
are	O
mostly	O
null	O
genes	O
.	O
oef	O
(	O
cid:22	O
)	O
jzg	O
increases	O
for	O
z	O
>	O
2	O
,	O
equaling	O
1.96	O
for	O
z	O
d	O
3:5.	O
at	O
that	O
point	O
with	O
6033	O
genes	O
to	O
consider	O
at	O
once	O
,	O
it	O
still	O
is	O
not	O
a	O
sure	O
thing	O
that	O
gene	O
i	O
is	O
non-null	O
.	O
about	O
85	O
%	O
of	O
the	O
genes	O
with	O
zi	O
near	O
3.5	O
will	O
be	O
non-null	O
,	O
and	O
these	O
will	O
have	O
effect	O
sizes	O
averaging	B
about	O
2.31	O
(	O
d	O
1:96=0:85	O
)	O
.	O
all	O
of	O
this	O
nicely	O
illustrates	O
the	O
combination	O
of	O
frequentist	O
and	O
bayesian	O
inference	B
possible	O
in	O
large-scale	O
studies	O
,	O
and	O
also	O
the	O
combination	O
of	O
estimation	B
and	O
hypothesis-testing	O
ideas	O
in	O
play	O
.	O
if	O
the	O
prior	B
density	O
g.	O
(	O
cid:22	O
)	O
/	O
in	O
(	O
20.35	O
)	O
is	O
assumed	O
to	O
be	O
normal	B
,	O
tweedie	O
’	O
s	O
formula	B
(	O
20.38	O
)	O
gives	O
(	O
almost	O
)	O
the	O
james–stein	O
estimator	B
(	O
7.13	O
)	O
.	O
the	O
cor-	O
responding	O
curve	O
in	O
figure	O
20.7	O
in	O
that	O
case	O
would	O
be	O
a	O
straight	O
line	O
pass-	O
ing	O
through	O
the	O
origin	O
at	O
slope	O
0.22.	O
like	O
the	O
james–stein	O
estimator	B
,	O
ridge	B
regression	I
,	O
and	O
the	O
lasso	B
of	O
chapter	O
16	O
,	O
tweedie	O
’	O
s	O
formula	B
is	O
a	O
shrink-	O
age	O
estimator	B
.	O
for	O
z610	O
d	O
5:29	O
,	O
the	O
most	O
extreme	O
observation	O
,	O
it	O
gave	O
−4−2024−2024z−valuee^	O
(	O
m	O
|	O
z	O
)	O
lllz	O
=	O
3.51.96.1500.20.40.60.81	O
fdrtweediefdr	O
411	O
o	O
(	O
cid:22	O
)	O
629	O
d	O
4:03	O
,	O
shrinking	O
the	O
maximum	B
likelihood	I
estimate	O
more	O
than	O
one	O
(	O
cid:27	O
)	O
unit	O
toward	O
the	O
origin	O
.	O
20.3	O
selection	O
bias	O
bayes	O
estimators	O
are	O
immune	O
to	O
selection	O
bias	O
,	O
as	O
discussed	O
in	O
sections	O
3.3	O
and	O
3.4.	O
this	O
offers	O
some	O
hope	O
that	O
tweedie	O
’	O
s	O
empirical	B
bayes	O
esti-	O
mates	O
might	O
be	O
a	O
realistic	O
cure	O
for	O
the	O
winners	O
’	O
curse	O
.	O
a	O
small	O
simulation	O
experiment	O
was	O
run	O
as	O
a	O
test	O
.	O
(	O
cid:15	O
)	O
a	O
hundred	O
data	B
sets	O
z	O
,	O
each	O
of	O
length	O
n	O
d	O
1000	O
,	O
were	O
generated	O
accord-	O
ing	O
to	O
a	O
combination	O
of	O
exponential	O
and	O
normal	O
sampling	O
,	O
ind	O
(	O
cid:24	O
)	O
e	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
(	O
cid:22	O
)	O
i	O
.	O
(	O
cid:22	O
)	O
>	O
0/	O
and	O
zij	O
(	O
cid:22	O
)	O
i	O
ind	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
i	O
;	O
1/	O
;	O
(	O
20.39	O
)	O
l.z/	O
was	O
computed	O
as	O
in	O
section	O
15.4	O
,	O
now	O
using	O
a	O
natural	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
1000	O
.	O
(	O
cid:15	O
)	O
for	O
each	O
z	O
,	O
o	O
spline	O
model	B
with	O
ﬁve	O
degrees	O
of	O
freedom	O
.	O
(	O
cid:15	O
)	O
this	O
gave	O
tweedie	O
’	O
s	O
estimates	O
o	O
(	O
cid:22	O
)	O
i	O
d	O
zi	O
c	O
o	O
0	O
.zi	O
/	O
;	O
l	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
1000	O
;	O
(	O
20.40	O
)	O
for	O
that	O
data	B
set	O
z	O
.	O
(	O
cid:15	O
)	O
for	O
each	O
data	B
set	O
z	O
,	O
the	O
20	O
largest	O
zi	O
values	O
and	O
the	O
corresponding	O
o	O
(	O
cid:22	O
)	O
i	O
and	O
(	O
cid:22	O
)	O
i	O
values	O
were	O
recorded	O
,	O
yielding	O
the	O
uncorrected	O
differences	O
and	O
corrected	O
differences	O
zi	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
i	O
o	O
(	O
cid:22	O
)	O
i	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
i	O
;	O
(	O
20.41	O
)	O
the	O
hope	O
being	O
that	O
empirical	B
bayes	O
shrinkage	B
would	O
correct	O
the	O
selection	O
bias	O
in	O
the	O
zi	O
values	O
.	O
(	O
cid:15	O
)	O
figure	O
20.8	O
shows	O
the	O
2000	O
(	O
100	O
data	B
sets	O
,	O
20	O
top	O
cases	O
each	O
)	O
uncorrected	O
and	O
corrected	O
differences	O
.	O
selection	O
bias	O
is	O
quite	O
obvious	O
,	O
with	O
the	O
uncor-	O
rected	O
differences	O
shifted	O
one	O
unit	O
to	O
the	O
right	O
of	O
zero	O
.	O
in	O
this	O
case	O
at	O
least	O
,	O
the	O
empirical	B
bayes	O
corrections	O
have	O
worked	O
well	O
,	O
the	O
corrected	O
differ-	O
ences	O
being	O
nicely	O
centered	O
at	O
zero	O
.	O
bias	O
correction	O
often	O
adds	O
variance	O
,	O
but	O
in	O
this	O
case	O
it	O
hasn	O
’	O
t	B
.	O
finally	O
,	O
it	O
is	O
worth	O
saying	O
that	O
the	O
“	O
empirical	B
”	O
part	O
of	O
empirical	B
bayes	O
is	O
less	O
the	O
estimation	B
of	O
bayesian	O
rules	O
from	O
the	O
aggregate	O
data	B
than	O
the	O
appli-	O
cation	O
of	O
such	O
rules	O
to	O
individual	O
cases	O
.	O
for	O
the	O
prostate	B
data	O
we	O
began	O
with	O
no	O
deﬁnite	O
prior	B
opinions	O
but	O
arrived	O
at	O
strong	O
(	O
i.e.	O
,	O
not	O
“	O
uninformative	O
”	O
)	O
bayesian	O
conclusions	O
for	O
,	O
say	O
,	O
(	O
cid:22	O
)	O
610	O
in	O
the	O
prostate	B
study	O
.	O
412	O
inference	B
after	O
model	B
selection	I
figure	O
20.8	O
corrected	O
and	O
uncorrected	O
differences	O
for	O
20	O
top	O
cases	O
in	O
each	O
of	O
100	O
simulations	O
(	O
20.39	O
)	O
–	O
(	O
20.41	O
)	O
.	O
tweedie	O
corrections	O
effectively	O
counteracted	O
selection	O
bias	O
.	O
20.4	O
combined	O
bayes–frequentist	O
estimation	B
as	O
mentioned	O
previously	O
,	O
bayes	O
estimates	O
are	O
,	O
at	O
least	O
theoretically	O
,	O
im-	O
mune	O
from	O
selection	O
bias	O
.	O
let	O
z	O
d	O
.z1	O
;	O
z2	O
;	O
:	O
:	O
:	O
,	O
zn	O
/	O
represent	O
the	O
prostate	B
study	O
data	B
of	O
the	O
previous	O
section	O
,	O
with	O
parameter	O
vector	B
(	O
cid:22	O
)	O
d	O
.	O
(	O
cid:22	O
)	O
1	O
;	O
(	O
cid:22	O
)	O
2	O
;	O
:	O
:	O
:	O
;	O
(	O
cid:22	O
)	O
n	O
/	O
.	O
bayes	O
’	O
rule	B
(	O
3.5	O
)	O
g.	O
(	O
cid:22	O
)	O
jz/	O
d	O
g.	O
(	O
cid:22	O
)	O
/f	O
(	O
cid:22	O
)	O
.z/=f	O
.z/	O
(	O
20.42	O
)	O
yields	O
the	O
posterior	O
density	O
of	O
(	O
cid:22	O
)	O
given	O
z.	O
a	O
data-based	O
model	B
selection	I
rule	O
such	O
as	O
“	O
estimate	B
the	O
(	O
cid:22	O
)	O
i	O
corresponding	O
to	O
the	O
largest	O
observation	O
zi	O
”	O
has	O
no	O
effect	O
on	O
the	O
likelihood	B
function	O
f	O
(	O
cid:22	O
)	O
.z/	O
(	O
with	O
z	O
ﬁxed	O
)	O
or	O
on	O
g.	O
(	O
cid:22	O
)	O
jz/	O
.	O
having	O
chosen	O
a	O
prior	B
g.	O
(	O
cid:22	O
)	O
/	O
,	O
our	O
posterior	O
estimate	O
of	O
(	O
cid:22	O
)	O
610	O
is	O
unaffected	O
by	O
the	O
fact	O
that	O
z610	O
d	O
5:29	O
happens	O
to	O
be	O
largest	O
.	O
this	O
same	O
argument	B
applies	O
just	O
as	O
well	O
to	O
any	O
data-based	O
model	B
selec-	O
tion	O
procedure	O
,	O
for	O
instance	O
a	O
preliminary	O
screening	O
of	O
possible	O
variables	O
to	O
include	O
in	O
a	O
regression	B
analysis—the	O
cp	O
choice	O
of	O
a	O
cubic	O
regression	B
in	O
figure	O
20.1	O
having	O
no	O
effect	O
on	O
its	O
bayes	O
posterior	O
accuracy	O
.	O
there	O
is	O
a	O
catch	O
:	O
the	O
chosen	O
prior	B
g.	O
(	O
cid:22	O
)	O
/	O
must	O
apply	O
to	O
the	O
entire	O
param-	O
eter	O
vector	B
(	O
cid:22	O
)	O
and	O
not	O
just	O
the	O
part	O
we	O
are	O
interested	O
in	O
(	O
e.g.	O
,	O
(	O
cid:22	O
)	O
610	O
)	O
.	O
this	O
is	O
differencesfrequency−4−2024020406080100120140uncorrecteddifferencescorrecteddifferences	O
20.4	O
combined	O
bayes–frequentist	O
estimation	B
413	O
feasible	O
in	O
one-parameter	B
situations	O
like	O
the	O
stopping	O
rule	B
example	O
of	O
fig-	O
ure	O
3.3.	O
it	O
becomes	O
difﬁcult	O
and	O
possibly	O
dangerous	O
in	O
higher	O
dimensions	O
.	O
empirical	B
bayes	O
methods	O
such	O
as	O
tweedie	O
’	O
s	O
rule	B
can	O
be	O
thought	O
of	O
as	O
al-	O
lowing	O
the	O
data	B
vector	O
z	O
to	O
assist	O
in	O
the	O
choice	O
of	O
a	O
high-dimensional	O
prior	B
,	O
an	O
effective	O
collaboration	O
between	O
bayesian	O
and	O
frequentist	O
methodology	O
.	O
our	O
chapter	O
’	O
s	O
ﬁnal	O
vignette	O
concerns	O
another	O
bayes–frequentist	O
estima-	O
d	O
ff˛.x/g	O
tion	O
technique	O
.	O
dropping	O
the	O
boldface	O
notation	O
,	O
suppose	O
that	O
f	O
is	O
a	O
multi-dimensional	O
family	O
of	O
densities	O
(	O
5.1	O
)	O
(	O
now	O
with	O
˛	O
playing	O
the	O
role	O
of	O
(	O
cid:22	O
)	O
)	O
,	O
and	O
that	O
we	O
are	O
interested	O
in	O
estimating	O
a	O
particular	O
parameter	O
	O
d	O
t	B
.˛/	O
.	O
a	O
prior	B
g.˛/	O
has	O
been	O
chosen	O
,	O
yielding	O
posterior	O
expectation	O
o	O
	O
d	O
e	O
ft	O
.˛/jxg	O
:	O
(	O
20.43	O
)	O
how	O
accurate	O
is	O
o	O
	O
?	O
the	O
usual	O
answer	O
would	O
be	O
calculated	O
from	O
the	O
pos-	O
terior	O
distribution	B
of	O
	O
given	O
x.	O
this	O
is	O
obviously	O
the	O
correct	O
answer	O
if	O
g.˛/	O
is	O
based	O
on	O
genuine	O
prior	B
experience	O
.	O
most	O
often	O
though	O
,	O
and	O
especially	O
in	O
high-dimensional	O
problems	O
,	O
the	O
prior	B
reﬂects	O
mathematical	O
convenience	O
and	O
a	O
desire	O
to	O
be	O
uninformative	O
,	O
as	O
in	O
chapter	O
13.	O
there	O
is	O
a	O
danger	O
of	O
circular	O
reasoning	O
in	O
using	O
a	O
self-selected	O
prior	B
distribution	I
to	O
calculate	O
the	O
accuracy	O
of	O
its	O
own	O
estimator	B
.	O
an	O
alternate	O
approach	O
,	O
discussed	O
next	O
,	O
is	O
to	O
calculate	O
the	O
frequentist	O
ac-	O
curacy	O
of	O
o	O
	O
;	O
that	O
is	O
,	O
even	O
though	O
(	O
20.43	O
)	O
is	O
a	O
bayes	O
estimate	B
,	O
we	O
consider	O
o	O
	O
simply	O
as	O
a	O
function	B
of	O
x	O
,	O
and	O
compute	O
its	O
frequentist	O
variability	O
.	O
the	O
next	O
theorem	B
leads	O
to	O
a	O
computationally	O
efﬁcient	O
way	O
of	O
doing	O
so	O
.	O
(	O
the	O
bayes	O
and	O
frequentist	O
standard	O
errors	O
for	O
o	O
	O
operate	O
in	O
conceptually	O
orthog-	O
onal	O
directions	O
as	O
pictured	O
in	O
figure	O
3.5.	O
here	O
we	O
are	O
supposing	O
that	O
the	O
prior	B
g.	O
(	O
cid:1	O
)	O
/	O
is	O
unavailable	O
or	O
uncertain	O
,	O
forcing	O
more	O
attention	O
on	O
frequentist	O
calculations	O
.	O
)	O
for	O
convenience	O
,	O
we	O
will	O
take	O
the	O
family	O
f	O
to	O
be	O
a	O
p-parameter	B
expo-	O
nential	O
family	O
(	O
5.50	O
)	O
,	O
f˛.x/	O
d	O
e˛	O
0	O
x	O
(	O
cid:0	O
)	O
.˛/f0.x/	O
;	O
(	O
20.44	O
)	O
now	O
with	O
˛	O
being	O
the	O
parameter	O
vector	B
called	O
(	O
cid:22	O
)	O
above	O
.	O
the	O
p	O
(	O
cid:2	O
)	O
p	O
covari-	O
ance	O
matrix	B
of	O
x	O
(	O
5.59	O
)	O
is	O
denoted	O
v˛	O
d	O
cov˛.x/	O
:	O
(	O
20.45	O
)	O
let	O
covx	O
indicate	O
the	O
posterior	O
covariance	O
given	O
x	O
between	O
	O
d	O
t	B
.˛/	O
,	O
the	O
parameter	O
of	O
interest	O
,	O
and	O
˛	O
,	O
covx	O
d	O
covf˛	O
;	O
t	B
.˛/jxg	O
;	O
(	O
20.46	O
)	O
6	O
n	O
o	O
inference	B
after	O
model	B
selection	I
o	O
d	O
(	O
cid:0	O
)	O
cov	O
414	O
a	O
p	O
(	O
cid:2	O
)	O
1	O
vector	B
.	O
covx	O
leads	O
directly	O
to	O
a	O
frequentist	O
estimate	O
of	O
accuracy	O
for	O
o	O
	O
.	O
theorem	B
20.4	O
	O
the	O
delta	O
method	B
estimate	O
of	O
standard	B
error	I
for	O
o	O
	O
d	O
eft	O
.˛/jxg	O
(	O
10.41	O
)	O
isbsedelta	O
0	O
x	O
vo˛	O
covx	O
where	O
vo˛	O
is	O
v˛	O
evaluated	O
at	O
the	O
mle	O
o˛	O
.	O
bsedeltaf	O
o	O
the	O
theorem	B
allows	O
us	O
to	O
calculate	O
the	O
frequentist	O
accuracy	O
estimate	O
g	O
with	O
hardly	O
any	O
additional	O
computational	O
effort	O
beyond	O
that	O
re-	O
quired	O
for	O
o	O
	O
itself	O
.	O
suppose	O
we	O
have	O
used	O
an	O
mcmc	O
or	O
gibbs	O
sampling	O
algorithm	O
,	O
section	O
13.4	O
,	O
to	O
generate	O
a	O
sample	B
from	O
the	O
bayes	O
posterior	O
dis-	O
tribution	O
of	O
˛	O
given	O
x	O
,	O
(	O
cid:1	O
)	O
1=2	O
(	O
20.47	O
)	O
	O
;	O
t	B
:	O
˛.1/	O
;	O
˛.2/	O
;	O
:	O
:	O
:	O
;	O
˛.b/	O
:	O
(	O
cid:16	O
)	O
˛.b/	O
these	O
yield	O
the	O
usual	O
estimate	B
for	O
eft	O
.˛/jxg	O
,	O
bx	O
o	O
	O
d	O
1	O
t	B
.b/	O
(	O
cid:0	O
)	O
t	B
.	O
(	O
cid:1	O
)	O
/	O
(	O
cid:16	O
)	O
˛.b/	O
(	O
cid:0	O
)	O
˛	O
.	O
(	O
cid:1	O
)	O
/	O
(	O
cid:16	O
)	O
bx	O
b	O
t	B
.b/=b	O
,	O
and	O
˛	O
.	O
(	O
cid:1	O
)	O
/	O
dp	O
they	O
also	O
give	O
a	O
similar	O
expression	O
for	O
covf˛	O
;	O
t	B
.˛/jxg	O
,	O
t	B
.b/	O
d	O
t	B
.˛.b//	O
,	O
t	B
.	O
(	O
cid:1	O
)	O
/	O
dp	O
can	O
calculate5bsedelta	O
.	O
o	O
	O
/	O
(	O
20.47	O
)	O
.	O
covx	O
d	O
1	O
bd1	O
bd1	O
b	O
b	O
(	O
20.48	O
)	O
(	O
20.49	O
)	O
;	O
(	O
20.50	O
)	O
b	O
˛.b/=b	O
,	O
from	O
which	O
we	O
for	O
an	O
example	O
of	O
theorem	B
20.4	O
in	O
action	O
we	O
consider	O
the	O
diabetes	B
i	O
the	O
ith	O
row	O
of	O
x	O
,	O
the	O
442	O
(	O
cid:2	O
)	O
10	O
matrix	B
of	O
0	O
data	B
of	O
section	O
20.1	O
,	O
with	O
x	O
prediction	O
,	O
so	O
xi	O
is	O
the	O
vector	B
of	O
10	O
predictors	O
for	O
patient	O
i.	O
the	O
response	O
vector	B
y	O
of	O
progression	O
scores	O
has	O
now	O
been	O
rescaled	O
to	O
have	O
(	O
cid:27	O
)	O
2	O
d	O
1	O
in	O
the	O
normal	B
regression	O
model,6	O
y	O
(	O
cid:24	O
)	O
nn.x	O
ˇ	O
;	O
i/	O
:	O
the	O
prior	B
distribution	I
g.ˇ/	O
was	O
taken	O
to	O
be	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
kˇk1	O
;	O
g.ˇ/	O
d	O
ce	O
(	O
20.51	O
)	O
(	O
20.52	O
)	O
obtained	O
from	O
parametric	O
bootstrap	O
resampling—taking	O
the	O
empirical	B
covariance	O
matrix	B
5	O
vo˛	O
may	O
be	O
known	O
theoretically	O
,	O
calculated	O
by	O
numerical	O
differentiation	O
in	O
(	O
5.57	O
)	O
,	O
or	O
of	O
bootstrap	O
replications	O
oˇ	O
(	O
cid:3	O
)	O
i	O
.	O
6	O
by	O
dividing	O
the	O
original	O
data	B
vector	O
y	O
by	O
its	O
estimated	O
standard	B
error	I
from	O
the	O
linear	B
model	I
efyg	O
d	O
x	O
ˇ	O
.	O
20.4	O
combined	O
bayes–frequentist	O
estimation	B
415	O
with	O
(	O
cid:21	O
)	O
d	O
0:37	O
and	O
c	O
the	O
constant	O
that	O
makes	O
g.ˇ/	O
integrate	O
to	O
1.	O
this	O
is	O
the	O
“	O
bayesian	O
lasso	B
prior	I
,	O
”	O
so	O
called	O
because	O
of	O
its	O
connection	O
to	O
the	O
lasso	B
,	O
7	O
(	O
7.42	O
)	O
and	O
(	O
16.1	O
)	O
.	O
(	O
the	O
lasso	B
plays	O
no	O
part	O
in	O
what	O
follows	O
)	O
.	O
posterior	B
distribution	I
g.ˇjy/	O
.	O
let	O
an	O
mcmc	O
algorithm	B
generated	O
b	O
d10,000	O
samples	O
(	O
20.48	O
)	O
from	O
the	O
b	O
bd1	O
0	O
i	O
ˇ	O
;	O
(	O
20.54	O
)	O
(	O
20.53	O
)	O
0	O
x	O
i	O
ˇ	O
:	O
(	O
cid:16	O
)	O
o	O
it	O
has	O
bayes	O
posterior	O
standard	O
error	O
the	O
(	O
unknown	O
)	O
expectation	O
of	O
the	O
ith	O
patient	O
’	O
s	O
response	O
yi	O
.	O
the	O
bayes	O
pos-	O
terior	O
expectation	O
of	O
i	O
is	O
i	O
d	O
x	O
bx	O
o	O
i	O
d	O
1	O
''	O
(	O
cid:16	O
)	O
	O
d	O
bx	O
which	O
we	O
can	O
compare	O
withbsedelta	O
.	O
bd1	O
o	O
i	O
/	O
,	O
the	O
frequentist	O
standard	B
error	I
(	O
20.47	O
)	O
.	O
figure	O
20.9	O
shows	O
the	O
10,000	O
mcmc	O
replications	O
o	O
0	O
	O
.b/	O
i	O
ˇ	O
for	O
pa-	O
tient	O
i	O
d	O
322.	O
the	O
point	O
estimate	B
o	O
i	O
i	O
equaled	O
2.41	O
,	O
with	O
bayes	O
and	O
frequen-	O
bsebayes	O
d	O
0:203	O
and	O
bsedelta	O
d	O
0:186	O
:	O
tist	O
standard	B
error	I
estimates	O
the	O
frequentist	O
standard	B
error	I
is	O
9	O
%	O
smaller	O
in	O
this	O
case	O
;	O
bsedelta	O
was	O
less	O
thanbsebayes	O
for	O
all	O
442	O
patients	O
,	O
the	O
difference	O
averaging	B
a	O
modest	O
5	O
%	O
.	O
bsebayes	O
i	O
ˇ.b/	O
(	O
cid:0	O
)	O
o	O
0	O
things	O
can	O
work	O
out	O
differently	O
.	O
suppose	O
we	O
are	O
interested	O
in	O
the	O
poste-	O
#	O
1=2	O
2	O
d	O
x	O
(	O
20.55	O
)	O
(	O
20.56	O
)	O
1	O
b	O
i	O
x	O
i	O
;	O
(	O
(	O
cid:16	O
)	O
c	O
;	O
ˇ.b/	O
d	O
rior	O
cdf	B
of	O
332	O
given	O
y.	O
for	O
any	O
given	O
value	O
of	O
c	O
let	O
322ˇ.b/	O
	O
c	O
0	O
0	O
332ˇ.b/	O
>	O
c	O
;	O
(	O
cid:16	O
)	O
c	O
;	O
ˇ.b/	O
cdf.c/	O
d	O
1	O
bx	O
if	O
x	O
if	O
x	O
so	O
1	O
0	O
t	B
(	O
20.58	O
)	O
is	O
our	O
mcmc	O
assessment	O
of	O
prf322	O
	O
cjyg	O
.	O
the	O
solid	O
curve	O
in	O
fig-	O
ure	O
20.10	O
graphs	O
cdf.c/	O
.	O
bd1	O
b	O
t	B
if	O
we	O
believe	O
prior	B
(	O
20.52	O
)	O
then	O
the	O
curve	O
exactly	O
represents	O
the	O
posterior	B
distribution	I
of	O
322	O
given	O
y	O
(	O
except	O
for	O
the	O
simulation	O
error	O
due	O
to	O
stopping	O
at	O
b	O
d10,000	O
replications	O
)	O
.	O
whether	O
or	O
not	O
we	O
believe	O
the	O
prior	B
we	O
can	O
use	O
(	O
20.57	O
)	O
416	O
inference	B
after	O
model	B
selection	I
figure	O
20.9	O
a	O
histogram	O
of	O
10,000	O
mcmc	O
replications	O
for	O
posterior	B
distribution	I
of	O
322	O
,	O
expected	O
progression	O
for	O
patient	O
322	O
in	O
the	O
diabetes	B
study	O
;	O
model	B
(	O
20.51	O
)	O
and	O
prior	O
(	O
20.52	O
)	O
.	O
the	O
bayes	O
posterior	O
expectation	O
is	O
2.41.	O
frequentist	O
standard	B
error	I
(	O
20.47	O
)	O
for	O
o	O
posterior	O
standard	O
error	O
(	O
20.55	O
)	O
.	O
322	O
d	O
2:41	O
was	O
9	O
%	O
smaller	O
than	O
bayes	O
theorem	B
20.4	O
,	O
with	O
t	B
.b/	O
d	O
t	B
.c	O
;	O
ˇ.b//	O
in	O
(	O
20.50	O
)	O
,	O
to	O
evaluate	O
the	O
frequentist	O
the	O
dashed	O
vertical	O
red	O
lines	O
show	O
cdf.c/	O
plus	O
or	O
minus	O
onebsedelta	O
unit	O
.	O
accuracy	O
of	O
the	O
curve	O
.	O
the	O
standard	O
errors	O
are	O
disturbingly	O
large	O
,	O
for	O
instance	O
0:687	O
˙	O
0:325	O
at	O
c	O
d	O
2:5.	O
the	O
central	O
90	O
%	O
credible	O
interval	B
for	O
322	O
(	O
the	O
c-values	O
between	O
cdf.c/	O
0.05	O
and	O
0.95	O
)	O
,	O
.2:08	O
;	O
2:73/	O
(	O
20.59	O
)	O
has	O
frequentist	O
standard	O
errors	O
about	O
0.185	O
for	O
each	O
endpoint—28	O
%	O
of	O
the	O
interval	B
’	O
s	O
length	O
.	O
if	O
we	O
believe	O
prior	B
(	O
20.52	O
)	O
then	O
.2:08	O
;	O
2:73/	O
is	O
an	O
(	O
almost	O
)	O
exact	O
90	O
%	O
credible	O
interval	B
for	O
322	O
,	O
and	O
moreover	O
is	O
immune	O
to	O
any	O
selection	O
bias	O
involved	O
in	O
our	O
focus	O
on	O
322	O
.	O
if	O
not	O
,	O
the	O
large	O
frequentist	O
standard	O
errors	O
are	O
a	O
reminder	O
that	O
calculation	O
(	O
20.59	O
)	O
might	O
turn	O
out	O
much	O
differently	O
in	O
a	O
new	O
version	O
of	O
the	O
diabetes	B
study	O
,	O
even	O
ignoring	O
selection	O
bias	O
.	O
to	O
return	O
to	O
our	O
main	O
theme	O
,	O
bayesian	O
calculations	O
encourage	O
a	O
disre-	O
gard	O
for	O
model	B
selection	I
effects	O
.	O
this	O
can	O
be	O
dangerous	O
in	O
objective	O
bayes	O
mcmc	O
q322	O
valuesfrequency2.02.53.001002003004005006002.41standard	O
errorsbayes	O
posterior	O
.205frequentist	O
.186	O
20.5	O
notes	O
and	O
details	O
417	O
figure	O
20.10	O
the	O
solid	O
curve	O
is	O
the	O
posterior	O
cdf	O
of	O
322	O
.	O
vertical	O
red	O
bars	O
indicate	O
˙	O
one	O
frequentist	O
standard	B
error	I
,	O
as	O
obtained	O
from	O
theorem	O
20.4.	O
black	O
triangles	O
are	O
endpoints	O
of	O
the	O
0.90	O
central	O
credible	O
interval	B
.	O
settings	O
where	O
one	O
can	O
’	O
t	B
rely	O
on	O
genuine	O
prior	B
experience	O
.	O
theorem	B
20.4	O
serves	O
as	O
a	O
frequentist	O
checkpoint	O
,	O
offering	O
some	O
reassurance	O
as	O
in	O
fig-	O
ure	O
20.9	O
,	O
or	O
sounding	O
a	O
warning	O
as	O
in	O
figure	O
20.10	O
.	O
20.5	O
notes	O
and	O
details	O
optimality	O
theories—statements	O
of	O
best	O
possible	O
results—are	O
marks	O
of	O
ma-	O
turity	O
in	O
applied	O
mathematics	O
.	O
classical	O
statistics	B
achieved	O
two	O
such	O
theo-	O
ries	O
:	O
for	O
unbiased	O
or	O
asymptotically	O
unbiased	O
estimation	O
,	O
and	O
for	O
hypothe-	O
sis	O
testing	B
.	O
most	O
of	O
this	O
book	O
and	O
all	O
of	O
this	O
chapter	O
venture	O
beyond	O
these	O
safe	O
havens	O
.	O
how	O
far	O
from	O
best	O
are	O
the	O
cp/ols	O
bootstrap	O
smoothed	O
esti-	O
mates	O
of	O
section	O
20.2	O
?	O
at	O
this	O
time	O
we	O
can	O
’	O
t	B
answer	O
such	O
questions	O
,	O
though	O
we	O
can	O
offer	O
appealing	O
methodologies	O
in	O
their	O
pursuit	O
,	O
a	O
few	O
of	O
which	O
have	O
been	O
highlighted	O
here	O
.	O
the	O
cholestyramine	O
example	O
comes	O
from	O
efron	O
and	O
feldman	O
(	O
1991	O
)	O
where	O
it	O
is	O
discussed	O
at	O
length	O
.	O
data	B
for	O
a	O
control	B
group	O
is	O
also	O
analyzed	O
there	O
.	O
1	O
[	O
p.	O
398	O
]	O
scheff´e	O
intervals	B
.	O
scheff´e	O
’	O
s	O
1953	O
paper	O
came	O
at	O
the	O
beginning	O
2.02.22.42.62.80.00.20.40.60.81.0c−valuepr	O
(	O
q322	O
<	O
c	O
)	O
418	O
inference	B
after	O
model	B
selection	I
of	O
a	O
period	O
of	O
healthy	O
development	O
in	O
simultaneous	O
inference	B
techniques	O
,	O
mostly	O
in	O
classical	O
normal	B
theory	O
frameworks	O
.	O
miller	O
(	O
1981	O
)	O
gives	O
a	O
clear	O
and	O
thorough	O
summary	O
.	O
the	O
1980s	O
followed	O
with	O
a	O
more	O
computer-intensive	O
approach	O
,	O
nicely	O
developed	O
in	O
westfall	O
and	O
young	O
’	O
s	O
1993	O
book	O
,	O
leading	O
up	O
to	O
benjamini	O
and	O
hochberg	O
’	O
s	O
1995	O
false-discovery	B
rate	I
paper	O
(	O
chapter	O
15	O
here	O
)	O
,	O
and	O
benjamini	O
and	O
yekutieli	O
’	O
s	O
(	O
2005	O
)	O
false-coverage	O
rate	B
algorithm	O
.	O
scheff´e	O
’	O
s	O
construction	O
(	O
20.15	O
)	O
is	O
derived	O
by	O
transforming	O
(	O
20.6	O
)	O
to	O
the	O
case	O
v	O
d	O
i	O
using	O
the	O
inverse	O
square	O
root	O
of	O
matrix	B
v	O
,	O
(	O
cid:0	O
)	O
1=2ˇ	O
(	O
20.60	O
)	O
(	O
cid:0	O
)	O
1	O
)	O
,	O
which	O
makes	O
the	O
ellipsoid	O
of	O
figure	O
20.2	O
into	O
a	O
circle	O
.	O
(	O
.v	O
then	O
q	O
d	O
ko	O
(	O
cid:13	O
)	O
(	O
cid:0	O
)	O
(	O
cid:13	O
)	O
k2=o	O
(	O
cid:27	O
)	O
2	O
in	O
(	O
20.10	O
)	O
,	O
and	O
for	O
a	O
linear	B
combination	O
(	O
cid:13	O
)	O
d	O
d	O
d	O
0	O
it	O
is	O
straightforward	O
to	O
see	O
that	O
prfq	O
	O
k.˛/2	O
p	O
;	O
q	O
g	O
d	O
˛	O
amounts	O
to	O
ˇ	O
and	O
(	O
cid:13	O
)	O
d	O
v	O
(	O
cid:0	O
)	O
1=2/2	O
d	O
v	O
o	O
(	O
cid:13	O
)	O
d	O
v	O
(	O
cid:0	O
)	O
1=2	O
o	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
d	O
2	O
o	O
(	O
cid:13	O
)	O
d	O
˙	O
o	O
(	O
cid:27	O
)	O
kdk	O
k.˛/	O
p	O
;	O
q	O
(	O
20.61	O
)	O
for	O
all	O
choices	O
of	O
d	O
,	O
the	O
geometry	B
of	O
figure	O
20.2	O
now	O
being	O
transparent	O
.	O
changing	O
coordinates	O
back	O
to	O
o	O
ˇ	O
d	O
v	O
1=2	O
o	O
(	O
cid:13	O
)	O
,	O
ˇ	O
d	O
v	O
1=2	O
(	O
cid:13	O
)	O
,	O
and	O
c	O
d	O
v	O
1=2d	O
yields	O
(	O
20.15	O
)	O
.	O
2	O
[	O
p.	O
399	O
]	O
restricting	O
the	O
catalog	O
c	O
.	O
suppose	O
that	O
all	O
the	O
sample	B
sizes	O
nj	O
in	O
(	O
20.16	O
)	O
take	O
the	O
same	O
value	O
n	O
,	O
and	O
that	O
we	O
wish	O
to	O
set	B
simultaneous	O
con-	O
ﬁdence	O
intervals	B
for	O
all	O
pairwise	O
differences	O
ˇi	O
(	O
cid:0	O
)	O
ˇj	O
.	O
tukey	O
’	O
s	O
studentized	O
range	O
pivotal	O
quantity	B
(	O
1952	O
,	O
unpublished	O
)	O
ˇˇˇ	O
(	O
cid:16	O
)	O
o	O
ˇi	O
(	O
cid:0	O
)	O
o	O
ˇj	O
	O
(	O
cid:0	O
)	O
.ˇi	O
(	O
cid:0	O
)	O
ˇj	O
/	O
ˇˇˇ	O
o	O
(	O
cid:27	O
)	O
t	B
d	O
max	O
i¤j	O
(	O
20.62	O
)	O
has	O
a	O
distribution	B
not	O
depending	O
on	O
(	O
cid:27	O
)	O
or	O
ˇ.	O
this	O
implies	O
that	O
ˇi	O
(	O
cid:0	O
)	O
ˇj	O
2	O
o	O
ˇi	O
(	O
cid:0	O
)	O
o	O
ˇj	O
˙	O
o	O
(	O
cid:27	O
)	O
p	O
(	O
20.63	O
)	O
p	O
is	O
a	O
set	B
of	O
simultaneous	O
level-˛	O
conﬁdence	B
intervals	I
for	O
all	O
pairwise	O
dif-	O
ferences	O
ˇi	O
(	O
cid:0	O
)	O
ˇj	O
,	O
where	O
t	B
.˛/	O
is	O
the	O
˛th	O
quantile	O
of	O
t	B
.	O
(	O
the	O
factor	B
1=	O
n	O
comes	O
from	O
o	O
ˇj	O
(	O
cid:24	O
)	O
t	B
.˛/	O
n	O
n	O
.ˇj	O
;	O
(	O
cid:27	O
)	O
2=n/	O
in	O
(	O
20.16	O
)	O
.	O
)	O
p	O
table	O
20.4	O
half-width	O
of	O
tukey	O
studentized	O
range	O
simultaneous	O
95	O
%	O
conﬁdence	B
intervals	I
for	O
pairwise	O
differences	O
ˇi	O
(	O
cid:0	O
)	O
ˇj	O
(	O
in	O
units	O
of	O
o	O
(	O
cid:27	O
)	O
=	O
n	O
)	O
for	O
p	O
d	O
2	O
;	O
3	O
;	O
:	O
:	O
:	O
;	O
6	O
and	O
n	O
d	O
20	O
;	O
compared	O
with	O
scheff´e	O
intervals	B
(	O
20.15	O
)	O
.	O
p	O
2	O
tukey	O
scheff´e	O
2.95	O
3.74	O
3	O
3.58	O
4.31	O
4	O
3.96	O
4.79	O
5	O
4.23	O
5.21	O
6	O
4.44	O
5.58	O
20.5	O
notes	O
and	O
details	O
419	O
reducing	O
the	O
catalog	O
c	O
from	O
all	O
linear	B
combinations	O
c	O
ˇ	O
to	O
only	O
pair-	O
wise	O
differences	O
shortens	O
the	O
simultaneous	O
intervals	B
.	O
table	O
20.4	O
shows	O
the	O
comparison	O
between	O
the	O
tukey	O
and	O
scheff´e	O
95	O
%	O
intervals	B
for	O
p	O
d	O
2	O
;	O
3	O
;	O
:	O
:	O
:	O
;	O
6	O
and	O
n	O
d	O
20	O
.	O
0	O
calculating	O
t	B
.˛/	O
was	O
a	O
substantial	O
project	O
in	O
the	O
early	O
1980s	O
.	O
berk	O
et	O
al	O
.	O
(	O
2013	O
)	O
now	O
carry	O
out	O
the	O
analogous	O
computations	B
for	O
general	O
catalogs	O
of	O
linear	B
constraints	O
.	O
they	O
discuss	O
at	O
length	O
the	O
inferential	O
basis	O
of	O
such	O
pro-	O
cedures	O
.	O
3	O
[	O
p.	O
405	O
]	O
discontinuous	O
estimators	O
.	O
looking	O
at	O
figure	O
20.6	O
suggests	O
that	O
a	O
conﬁdence	B
interval	I
for	O
	O
(	O
cid:0	O
)	O
2:0	O
t	B
.c	O
;	O
x/	O
will	O
move	O
far	O
left	O
for	O
data	B
sets	O
x	O
where	O
cp	O
selects	O
linear	B
regression	O
(	O
m	O
d	O
1	O
)	O
as	O
best	O
.	O
this	O
kind	O
of	O
“	O
jumpy	O
”	O
behav-	O
ior	O
lengthens	O
the	O
intervals	B
needed	O
to	O
attain	O
a	O
desired	O
coverage	O
level	O
.	O
more	O
seriously	O
,	O
intervals	B
for	O
m	O
d	O
1	O
may	O
give	O
misleading	O
inferences	O
,	O
another	O
ex-	O
ample	O
of	O
“	O
accurate	O
but	O
incorrect	O
”	O
behavior	O
.	O
bagging	O
(	O
20.28	O
)	O
,	O
in	O
addition	O
to	O
reducing	O
interval	B
length	O
,	O
improves	O
inferential	O
correctness	O
,	O
as	O
discussed	O
in	O
efron	O
(	O
2014a	O
)	O
.	O
4	O
[	O
p.	O
406	O
]	O
theorem	B
20.2	O
and	O
its	O
corollary	O
.	O
theorem	B
20.2	O
is	O
proved	O
in	O
sec-	O
tion	O
3	O
of	O
efron	O
(	O
2014a	O
)	O
,	O
with	O
a	O
parametric	B
bootstrap	O
version	O
appearing	O
in	O
section	O
4.	O
the	O
corollary	O
is	O
a	O
projection	O
result	O
illustrated	O
in	O
figure	O
4	O
of	O
that	O
paper	O
:	O
let	O
l.n	O
/	O
be	O
the	O
n-dimensional	O
subspace	O
of	O
b-dimensional	O
eu-	O
clidean	O
space	B
spanned	O
by	O
the	O
columns	O
of	O
the	O
b	O
(	O
cid:2	O
)	O
n	O
matrix	B
.nbj	O
/	O
(	O
20.29	O
)	O
and	O
t	O
(	O
cid:3	O
)	O
the	O
b-vector	O
with	O
components	O
t	B
(	O
cid:3	O
)	O
b	O
(	O
cid:0	O
)	O
t	B
bseij.s/ıbseboot.t	O
/	O
d	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
ot	O
(	O
cid:3	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
ıkt	O
(	O
cid:3	O
)	O
(	O
cid:1	O
)	O
;	O
then	O
(	O
cid:3	O
)	O
k	O
;	O
(	O
cid:3	O
)	O
is	O
the	O
projection	O
of	O
t	B
where	O
ot	O
if	O
o	O
will	O
be	O
substantially	O
less	O
than	O
1	O
.	O
(	O
cid:3	O
)	O
into	O
l.n	O
/	O
.	O
in	O
the	O
language	O
of	O
section	O
10.3	O
,	O
(	O
cid:3	O
)	O
d	O
s.p/	O
is	O
very	O
nonlinear	B
as	O
a	O
function	B
of	O
p	O
,	O
then	O
the	O
ratio	O
in	O
(	O
20.64	O
)	O
5	O
[	O
p.	O
409	O
]	O
tweedie	O
’	O
s	O
formula	B
.	O
for	O
convenience	O
,	O
take	O
(	O
cid:27	O
)	O
2	O
d	O
1	O
in	O
(	O
20.35	O
)	O
.	O
	O
bayes	O
’	O
rule	B
(	O
3.5	O
)	O
can	O
then	O
be	O
arranged	O
to	O
give	O
g.	O
(	O
cid:22	O
)	O
jz/	O
d	O
e	O
(	O
cid:22	O
)	O
z	O
(	O
cid:0	O
)	O
.z/g	O
.	O
(	O
cid:22	O
)	O
/e	O
(	O
cid:0	O
)	O
1	O
2	O
(	O
cid:22	O
)	O
2ıp	O
2	O
(	O
cid:25	O
)	O
(	O
20.64	O
)	O
(	O
20.65	O
)	O
(	O
20.66	O
)	O
with	O
.z/	O
d	O
1	O
2	O
z	O
c	O
log	O
f	O
.z/	O
:	O
this	O
is	O
a	O
one-parameter	B
exponential	O
family	O
(	O
5.46	O
)	O
having	O
natural	O
parameter	O
˛	O
equal	O
to	O
z.	O
differentiating	O
as	O
in	O
(	O
5.55	O
)	O
gives	O
ef	O
(	O
cid:22	O
)	O
jzg	O
d	O
d	O
dz	O
d	O
z	O
c	O
d	O
dz	O
log	O
f	O
.z/	O
;	O
(	O
20.67	O
)	O
inference	B
after	O
model	B
selection	I
420	O
which	O
is	O
tweedie	O
’	O
s	O
formula	B
(	O
20.37	O
)	O
when	O
(	O
cid:27	O
)	O
2	O
d	O
1.	O
the	O
formula	B
ﬁrst	O
ap-	O
pears	O
in	O
robbins	O
(	O
1956	O
)	O
,	O
who	O
credits	O
it	O
to	O
a	O
personal	O
communication	O
from	O
m.	O
k.	O
tweedie	O
.	O
efron	O
(	O
2011	O
)	O
discusses	O
general	O
exponential	O
family	O
versions	O
of	O
tweedie	O
’	O
s	O
formula	B
,	O
and	O
its	O
application	O
to	O
selection	O
bias	O
situations	O
.	O
6	O
[	O
p.	O
414	O
]	O
theorem	B
20.4.	O
the	O
delta	O
method	B
standard	O
error	O
approximation	O
for	O
a	O
statistic	B
t	O
.x/	O
is	O
bsedelta	O
dh	O
.rt	O
.x//	O
0	O
ov	O
.rt	O
.x//	O
;	O
(	O
20.68	O
)	O
where	O
rt	O
.x/	O
is	O
the	O
gradient	O
vector	B
.	O
@	O
t	B
=	O
@	O
xj	O
/	O
and	O
ov	O
is	O
an	O
estimate	O
of	O
the	O
covariance	O
matrix	B
of	O
x.	O
other	O
names	O
include	O
the	O
“	O
taylor	O
series	O
method	B
,	O
”	O
as	O
in	O
(	O
2.10	O
)	O
,	O
and	O
“	O
propagation	O
of	O
errors	B
”	O
in	O
the	O
physical	O
sciences	O
literature	O
.	O
the	O
proof	O
of	O
theorem	B
20.4	O
in	O
section	O
2	O
of	O
efron	O
(	O
2015	O
)	O
consists	O
of	O
show-	O
ing	O
that	O
covx	O
d	O
rt	O
.x/	O
when	O
t	B
.x/	O
d	O
eft	O
.˛/jxg	O
.	O
standard	B
deviations	I
are	O
only	O
a	O
ﬁrst	O
step	O
in	O
assessing	O
the	O
frequentist	O
accuracy	O
of	O
t	B
.x/	O
.	O
the	O
paper	O
goes	O
on	O
to	O
show	O
how	O
theorem	B
20.4	O
can	O
be	O
improved	O
to	O
give	O
conﬁdence	B
intervals	I
,	O
correcting	O
the	O
impression	O
in	O
figure	O
20.10	O
that	O
cdf.c/	O
can	O
range	O
outside	O
œ0	O
;	O
1	O
.	O
i1=2	O
and	O
prior	O
(	O
20.52	O
)	O
gives	O
7	O
[	O
p.	O
415	O
]	O
bayesian	O
lasso	B
.	O
applying	O
bayes	O
’	O
rule	B
(	O
3.5	O
)	O
with	O
density	B
(	O
20.51	O
)	O
log	O
g.ˇjy/	O
d	O
(	O
cid:0	O
)	O
(	O
cid:26	O
)	O
ky	O
(	O
cid:0	O
)	O
x	O
ˇk2	O
2	O
(	O
cid:27	O
)	O
c	O
(	O
cid:21	O
)	O
kˇk1	O
;	O
(	O
20.69	O
)	O
as	O
discussed	O
in	O
tibshirani	O
(	O
2006	O
)	O
.	O
comparison	O
with	O
(	O
7.42	O
)	O
shows	O
that	O
the	O
maximizing	O
value	O
of	O
ˇ	O
(	O
the	O
“	O
map	O
”	O
estimate	B
)	O
agrees	O
with	O
the	O
lasso	B
esti-	O
mate	O
.	O
park	O
and	O
casella	O
(	O
2008	O
)	O
named	O
the	O
“	O
bayesian	O
lasso	B
”	O
and	O
suggested	O
an	O
appropriate	O
mcmc	O
algorithm	B
.	O
their	O
choice	O
(	O
cid:21	O
)	O
d	O
0:37	O
was	O
based	O
on	O
marginal	O
maximum	B
likelihood	I
calculations	O
,	O
giving	O
their	O
analysis	B
an	O
empir-	O
ical	O
bayes	O
aspect	O
ignored	O
in	O
their	O
and	O
our	O
analyses	O
.	O
21	O
empirical	B
bayes	O
estimation	B
strategies	I
classic	O
statistical	O
inference	B
was	O
focused	O
on	O
the	O
analysis	B
of	O
individual	O
cases	O
:	O
a	O
single	O
estimate	B
,	O
a	O
single	O
hypothesis	O
test	O
.	O
the	O
interpretation	O
of	O
direct	O
evi-	O
dence	O
bearing	O
on	O
the	O
case	O
of	O
interest—the	O
number	O
of	O
successes	O
and	O
failures	O
of	O
a	O
new	O
drug	O
in	O
a	O
clinical	O
trial	O
as	O
a	O
familiar	O
example—dominated	O
statistical	O
practice	O
.	O
the	O
story	O
of	O
modern	O
statistics	B
very	O
much	O
involves	O
indirect	O
evidence	O
,	O
“	O
learning	O
from	O
the	O
experience	O
of	O
others	O
”	O
in	O
the	O
language	O
of	O
sections	O
7.4	O
and	O
15.3	O
,	O
carried	O
out	O
in	O
both	O
frequentist	O
and	O
bayesian	O
settings	O
.	O
the	O
computer-	O
intensive	O
prediction	O
algorithms	O
described	O
in	O
chapters	O
16–19	O
use	O
regression	B
theory	O
,	O
the	O
frequentist	O
’	O
s	O
favored	O
technique	O
,	O
to	O
mine	O
indirect	O
evidence	O
on	O
a	O
massive	O
scale	B
.	O
false-discovery	B
rate	I
theory	O
,	O
chapter	O
15	O
,	O
collects	O
indirect	O
ev-	O
idence	O
for	O
hypothesis	B
testing	I
by	O
means	O
of	O
bayes	O
’	O
theorem	B
as	O
implemented	O
through	O
empirical	B
bayes	O
estimation	B
.	O
empirical	B
bayes	O
methodology	O
has	O
been	O
less	O
studied	O
than	O
bayesian	O
or	O
frequentist	O
theory	B
.	O
as	O
with	O
the	O
james–stein	O
estimator	B
(	O
7.13	O
)	O
,	O
it	O
can	O
seem	O
to	O
be	O
little	O
more	O
than	O
plugging	O
obvious	O
frequentist	O
estimates	O
into	O
bayes	O
esti-	O
mation	O
rules	O
.	O
this	O
conceals	O
a	O
subtle	O
and	O
difﬁcult	O
task	O
:	O
learning	O
the	O
equiva-	O
lent	O
of	O
a	O
bayesian	O
prior	B
distribution	I
from	O
ongoing	O
statistical	O
observations	O
.	O
our	O
ﬁnal	O
chapter	O
concerns	O
the	O
empirical	B
bayes	O
learning	O
process	O
,	O
both	O
as	O
an	O
exercise	O
in	O
applied	O
deconvolution	B
and	O
as	O
a	O
relatively	O
new	O
form	B
of	O
statistical	O
inference	B
.	O
this	O
puts	O
us	O
back	O
where	O
we	O
began	O
in	O
chapter	O
1	O
,	O
examining	O
the	O
two	O
faces	O
of	O
statistical	O
analysis	B
,	O
the	O
algorithmic	O
and	O
the	O
inferential	O
.	O
21.1	O
bayes	O
deconvolution	B
a	O
familiar	O
formulation	O
of	O
empirical	B
bayes	O
inference	B
begins	O
by	O
assuming	O
that	O
an	O
unknown	O
prior	B
density	O
g.	O
/	O
,	O
our	O
object	O
of	O
interest	O
,	O
has	O
produced	O
a	O
random	O
sample	B
of	O
real-valued	O
variates	O
‚1	O
;	O
‚2	O
;	O
:	O
:	O
:	O
;	O
‚n	O
,	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
:	O
iid	O
(	O
cid:24	O
)	O
g.	O
/	O
;	O
‚i	O
(	O
21.1	O
)	O
421	O
empirical	B
bayes	O
estimation	B
strategies	I
422	O
(	O
the	O
“	O
density	B
”	O
g.	O
(	O
cid:1	O
)	O
/	O
may	O
include	O
discrete	O
atoms	O
of	O
probability	O
.	O
)	O
the	O
‚i	O
are	O
unobservable	O
,	O
but	O
each	O
yields	O
an	O
observable	O
random	O
variable	O
xi	O
according	O
to	O
a	O
known	O
family	O
of	O
density	B
functions	O
ind	O
(	O
cid:24	O
)	O
pi	O
.xij‚i	O
/	O
:	O
xi	O
(	O
21.2	O
)	O
from	O
the	O
observed	O
sample	B
x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn	O
we	O
wish	O
to	O
estimate	B
the	O
prior	B
density	O
g.	O
/	O
.	O
a	O
famous	O
example	O
has	O
pi	O
.xij‚i	O
/	O
the	O
poisson	O
family	O
,	O
xi	O
(	O
cid:24	O
)	O
poi.‚i	O
/	O
;	O
xi	O
(	O
cid:24	O
)	O
(	O
21.3	O
)	O
as	O
in	O
robbins	O
’	O
formula	B
,	O
section	O
6.1.	O
still	O
more	O
familiar	O
is	O
the	O
normal	B
model	O
(	O
3.28	O
)	O
,	O
(	O
21.4	O
)	O
often	O
with	O
(	O
cid:27	O
)	O
2	O
d	O
1.	O
a	O
binomial	B
model	O
was	O
used	O
in	O
the	O
medical	O
example	O
of	O
section	O
6.3	O
,	O
n	O
.‚i	O
;	O
(	O
cid:27	O
)	O
2/	O
;	O
xi	O
(	O
cid:24	O
)	O
bi.ni	O
;	O
‚i	O
/	O
:	O
(	O
21.5	O
)	O
there	O
the	O
ni	O
differ	O
from	O
case	O
to	O
case	O
,	O
accounting	O
for	O
the	O
need	O
for	O
the	O
ﬁrst	O
subscript	O
i	O
in	O
pi	O
.xij‚i	O
/	O
(	O
21.2	O
)	O
.	O
let	O
fi	O
.xi	O
/	O
denote	O
the	O
marginal	O
density	B
of	O
xi	O
obtained	O
from	O
(	O
21.1	O
)	O
–	O
(	O
21.2	O
)	O
,	O
fi	O
.xi	O
/	O
dz	O
t	B
pi	O
.xiji	O
/g.i	O
/	O
di	O
;	O
(	O
21.6	O
)	O
the	O
integral	O
being	O
over	O
the	O
space	B
t	O
of	O
possible	O
‚	O
values	O
.	O
the	O
statistician	O
has	O
only	O
the	O
marginal	O
observations	O
available	O
,	O
ind	O
(	O
cid:24	O
)	O
fi	O
.	O
(	O
cid:1	O
)	O
/	O
;	O
xi	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
;	O
(	O
21.7	O
)	O
from	O
which	O
he	O
or	O
she	O
wishes	O
to	O
estimate	B
the	O
density	B
g.	O
(	O
cid:1	O
)	O
/	O
in	O
(	O
21.6	O
)	O
.	O
in	O
the	O
normal	B
model	O
(	O
21.4	O
)	O
,	O
fi	O
is	O
the	O
convolution	O
of	O
the	O
unknown	O
g.	O
/	O
with	O
a	O
known	O
normal	B
density	O
,	O
denoted	O
f	O
d	O
g	O
(	O
cid:3	O
)	O
(	O
21.8	O
)	O
(	O
now	O
fi	O
not	O
depending	O
on	O
i	O
)	O
.	O
estimating	O
g	O
using	O
a	O
sample	B
x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn	O
from	O
f	O
is	O
a	O
problem	O
in	O
deconvolution	B
.	O
in	O
general	O
we	O
might	O
call	O
the	O
estima-	O
tion	O
of	O
g	O
in	O
model	B
(	O
21.1	O
)	O
–	O
(	O
21.2	O
)	O
the	O
“	O
bayes	O
deconvolution	B
problem.	O
”	O
n	O
.0	O
;	O
(	O
cid:27	O
)	O
2/	O
an	O
artiﬁcial	O
example	O
appears	O
in	O
figure	O
21.1	O
,	O
where	O
g.	O
/	O
is	O
a	O
mixture	O
distribution	B
:	O
seven-eighths	O
n	O
.0	O
;	O
0:52/	O
and	O
one-eighth	O
uniform	O
over	O
the	O
in-	O
terval	O
œ	O
(	O
cid:0	O
)	O
3	O
;	O
3	O
.	O
a	O
normal	B
sampling	O
model	B
xi	O
n	O
.‚i	O
;	O
1/	O
is	O
assumed	O
,	O
yield-	O
ing	O
f	O
by	O
convolution	O
as	O
in	O
(	O
21.8	O
)	O
.	O
the	O
convolution	O
process	O
makes	O
f	O
wider	O
ind	O
(	O
cid:24	O
)	O
21.1	O
bayes	O
deconvolution	B
423	O
figure	O
21.1	O
an	O
artiﬁcial	O
example	O
of	O
the	O
bayes	O
deconvolution	B
problem	O
.	O
the	O
solid	O
curve	O
is	O
g.	O
/	O
,	O
the	O
prior	B
density	O
of	O
‚	O
(	O
21.1	O
)	O
;	O
the	O
dashed	O
curve	O
is	O
the	O
density	B
of	O
an	O
observation	O
x	O
from	O
marginal	O
distribution	B
f	O
d	O
g	O
(	O
cid:3	O
)	O
n	O
.0	O
;	O
1/	O
(	O
21.8	O
)	O
.	O
we	O
wish	O
to	O
estimate	B
g.	O
/	O
on	O
the	O
basis	O
of	O
a	O
random	O
sample	B
x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn	O
from	O
f	O
.x/	O
.	O
and	O
smoother	O
than	O
g	O
,	O
as	O
illustrated	O
in	O
the	O
ﬁgure	O
.	O
having	O
observed	O
a	O
ran-	O
dom	O
sample	B
from	O
f	O
,	O
we	O
wish	O
to	O
estimate	B
the	O
deconvolute	O
g	O
,	O
which	O
begins	O
to	O
look	O
difﬁcult	O
in	O
the	O
ﬁgure	O
’	O
s	O
example	O
.	O
deconvolution	B
has	O
a	O
well-deserved	O
reputation	O
for	O
difﬁculty	O
.	O
it	O
is	O
the	O
classic	O
ill-posed	O
problem	O
:	O
because	O
of	O
the	O
convolution	O
process	O
(	O
21.6	O
)	O
,	O
large	O
changes	O
in	O
g.	O
/	O
are	O
smoothed	O
out	O
,	O
often	O
yielding	O
only	O
small	O
changes	O
in	O
f	O
.x/	O
.	O
deconvolution	B
operates	O
in	O
the	O
other	O
direction	O
,	O
with	O
small	O
changes	O
in	O
the	O
estimation	B
of	O
f	O
disturbingly	O
magniﬁed	O
on	O
the	O
g	O
scale	B
.	O
nevertheless	O
,	O
modern	O
computation	O
,	O
modern	O
theory	B
,	O
and	O
most	O
of	O
all	O
modern	O
sample	B
sizes	O
,	O
together	O
can	O
make	O
empirical	B
deconvolution	O
a	O
practical	O
reality	O
.	O
why	O
would	O
we	O
want	O
to	O
estimate	B
g.	O
/	O
?	O
in	O
the	O
prostate	B
data	O
example	O
(	O
3.28	O
)	O
(	O
where	O
‚	O
is	O
called	O
(	O
cid:22	O
)	O
)	O
we	O
might	O
wish	O
to	O
know	O
prf‚	O
d	O
0g	O
,	O
the	O
proba-	O
bility	O
of	O
a	O
null	O
gene	O
,	O
ones	O
whose	O
effect	O
size	O
is	O
zero	O
;	O
or	O
perhaps	O
prfj‚j	O
(	O
cid:21	O
)	O
2g	O
,	O
the	O
proportion	B
of	O
genes	O
that	O
are	O
substantially	O
non-null	O
.	O
or	O
we	O
might	O
want	O
to	O
estimate	B
bayesian	O
posterior	O
expectations	O
like	O
ef‚jx	O
d	O
xg	O
in	O
figure	O
20.7	O
,	O
or	O
posterior	O
densities	O
as	O
in	O
figure	O
6.5.	O
two	O
main	O
strategies	O
have	O
developed	O
for	O
carrying	O
out	O
empirical	O
bayes	O
estimation	B
:	O
modeling	O
on	O
the	O
	O
scale	B
,	O
called	O
g-modeling	B
here	O
,	O
and	O
modeling	O
−4−20240.000.050.100.15q	O
and	O
xg	O
(	O
q	O
)	O
and	O
f	O
(	O
x	O
)	O
f	O
(	O
x	O
)	O
g	O
(	O
q	O
)	O
424	O
empirical	B
bayes	O
estimation	B
strategies	I
on	O
the	O
x	O
scale	B
,	O
called	O
f	B
-modeling	I
.	O
we	O
begin	O
in	O
the	O
next	O
section	O
with	O
g-	O
modeling	O
.	O
21.2	O
g-modeling	B
and	O
estimation	B
there	O
has	O
been	O
a	O
substantial	O
amount	O
of	O
work	O
on	O
the	O
asymptotic	O
accuracy	O
of	O
estimates	O
og.	O
/	O
in	O
the	O
empirical	B
bayes	O
model	B
(	O
21.1	O
)	O
–	O
(	O
21.2	O
)	O
,	O
most	O
often	O
in	O
the	O
normal	B
sampling	O
framework	O
(	O
21.4	O
)	O
.	O
the	O
results	O
are	O
discouraging	O
,	O
with	O
the	O
rate	B
of	O
convergence	O
of	O
og.	O
/	O
to	O
g.	O
/	O
as	O
slow	O
as	O
.log	O
n	O
/	O
(	O
cid:0	O
)	O
1.	O
in	O
our	O
terminology	O
,	O
much	O
of	O
this	O
work	O
has	O
been	O
carried	O
out	O
in	O
a	O
nonparametric	B
g-	O
modeling	O
framework	O
,	O
allowing	O
the	O
unknown	O
prior	B
density	O
g.	O
/	O
to	O
be	O
virtu-	O
ally	O
anything	O
at	O
all	O
.	O
more	O
optimistic	O
results	O
are	O
possible	O
if	O
the	O
g-modeling	B
is	O
pursued	O
parametrically	O
,	O
that	O
is	O
,	O
by	O
restricting	O
g.	O
/	O
to	O
lie	O
within	O
some	O
parametric	B
family	O
of	O
possibilities	O
.	O
we	O
assume	O
,	O
for	O
the	O
sake	O
of	O
simpler	O
exposition	O
,	O
that	O
the	O
space	B
t	O
of	O
pos-	O
sible	O
‚	O
values	O
is	O
ﬁnite	O
and	O
discrete	O
,	O
say	O
(	O
21.9	O
)	O
the	O
prior	B
density	O
g.	O
/	O
is	O
now	O
represented	O
by	O
a	O
vector	B
g	O
d	O
.g1	O
;	O
g2	O
;	O
:	O
:	O
:	O
;	O
gm/	O
with	O
components	O
t	B
0	O
,	O
gj	O
d	O
pr˚‚	O
d	O
.j	O
/	O
(	O
cid:9	O
)	O
:	O
d˚.1/	O
;	O
.2/	O
;	O
:	O
:	O
:	O
;	O
.m/	O
(	O
cid:9	O
)	O
for	O
j	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
m	O
:	O
a	O
p-parameter	B
exponential	O
family	O
(	O
5.50	O
)	O
for	O
g	O
can	O
be	O
written	O
as	O
g	O
d	O
g.˛/	O
d	O
eq˛	O
(	O
cid:0	O
)	O
.˛/	O
;	O
(	O
21.11	O
)	O
where	O
the	O
p-vector	O
˛	O
is	O
the	O
natural	O
parameter	O
and	O
q	O
is	O
a	O
known	O
m	O
(	O
cid:2	O
)	O
p	O
structure	O
matrix	B
.	O
notation	O
(	O
21.11	O
)	O
means	O
that	O
the	O
j	O
th	O
component	O
of	O
g.˛/	O
is	O
0	O
j	O
the	O
j	O
th	O
row	O
of	O
q	O
;	O
the	O
function	B
.˛/	O
is	O
the	O
normalizer	O
that	O
makes	O
with	O
q	O
g.˛/	O
sum	O
to	O
1	O
,	O
gj	O
.˛/	O
d	O
eq	O
j	O
˛	O
(	O
cid:0	O
)	O
.˛/	O
;	O
0	O
0	O
@	O
mx	O
jd1	O
1a	O
:	O
.˛/	O
d	O
log	O
0	O
j	O
˛	O
eq	O
(	O
21.10	O
)	O
(	O
21.12	O
)	O
(	O
21.13	O
)	O
in	O
the	O
nodes	B
example	O
of	O
figure	O
6.4	O
,	O
the	O
set	B
of	O
possible	O
‚	O
values	O
was	O
d	O
f0:01	O
;	O
0:02	O
;	O
:	O
:	O
:	O
;	O
0:99g	O
,	O
and	O
q	O
was	O
a	O
ﬁfth-degree	O
polynomial	O
matrix	B
,	O
t	B
(	O
21.14	O
)	O
q	O
d	O
poly	O
(	O
t	B
,5	O
)	O
21.2	O
g-modeling	B
and	O
estimation	B
425	O
in	O
r	O
notation	O
,	O
indicating	O
a	O
ﬁve-parameter	O
exponential	O
family	O
for	O
g	O
,	O
(	O
6.38	O
)	O
–	O
(	O
6.39	O
)	O
.	O
in	O
the	O
development	O
that	O
follows	O
we	O
will	O
assume	O
that	O
the	O
kernel	O
pi	O
.	O
(	O
cid:1	O
)	O
j	O
(	O
cid:1	O
)	O
/	O
in	O
(	O
21.2	O
)	O
does	O
not	O
depend	O
on	O
i	O
,	O
i.e.	O
,	O
that	O
xi	O
has	O
the	O
same	O
family	O
of	O
conditional	O
distributions	O
p.xij‚i	O
/	O
for	O
all	O
i	O
,	O
as	O
in	O
the	O
poisson	O
and	O
normal	O
situations	O
(	O
21.3	O
)	O
and	O
(	O
21.4	O
)	O
,	O
but	O
not	O
the	O
binomial	B
case	O
(	O
21.5	O
)	O
.	O
and	O
moreover	O
we	O
as-	O
sume	O
that	O
the	O
sample	B
space	O
x	O
for	O
the	O
xi	O
observations	O
is	O
ﬁnite	O
and	O
discrete	O
,	O
say	O
(	O
21.15	O
)	O
x	O
d˚x.1/	O
;	O
x.2/	O
;	O
:	O
:	O
:	O
;	O
x.n/	O
(	O
cid:9	O
)	O
:	O
pkj	O
d	O
pr˚xi	O
d	O
x.k/j‚i	O
d	O
.j	O
/	O
(	O
cid:9	O
)	O
;	O
none	O
of	O
this	O
is	O
necessary	O
,	O
but	O
it	O
simpliﬁes	O
the	O
exposition	O
.	O
deﬁne	O
(	O
21.16	O
)	O
for	O
k	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
and	O
j	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
m	O
,	O
and	O
the	O
corresponding	O
n	O
(	O
cid:2	O
)	O
m	O
matrix	B
p	O
d	O
.pkj	O
/	O
;	O
(	O
21.17	O
)	O
having	O
kth	O
row	O
pk	O
d	O
.pk1	O
;	O
pk2	O
;	O
:	O
:	O
:	O
;	O
pkm/	O
0.	O
the	O
convolution-type	O
for-	O
mula	O
(	O
21.6	O
)	O
for	O
the	O
marginal	O
density	B
f	O
.x/	O
now	O
reduces	O
to	O
an	O
inner	O
product	O
,	O
˚xi	O
d	O
x.k/	O
(	O
cid:9	O
)	O
dpm	O
jd1	O
pkj	O
gj	O
.˛/	O
(	O
21.18	O
)	O
in	O
fact	O
we	O
can	O
write	O
the	O
entire	O
marginal	O
density	B
f	O
.˛/	O
d	O
.f1.˛/	O
;	O
f2.˛/	O
;	O
:	O
:	O
:	O
,	O
0	O
in	O
terms	O
of	O
matrix	B
multiplication	O
,	O
fn.˛//	O
fk.˛/	O
d	O
pr˛	O
d	O
p	O
0	O
kg.˛/	O
:	O
the	O
vector	B
of	O
counts	O
y	O
d	O
.y1	O
;	O
y2	O
;	O
:	O
:	O
:	O
;	O
yn/	O
,	O
with	O
f	O
.˛/	O
d	O
pg.˛/	O
:	O
yk	O
d	O
#	O
˚xi	O
d	O
x.k/	O
(	O
cid:9	O
)	O
;	O
(	O
21.19	O
)	O
(	O
21.20	O
)	O
(	O
21.21	O
)	O
is	O
a	O
sufﬁcient	O
statistic	B
in	O
the	O
iid	O
situation	O
.	O
it	O
has	O
a	O
multinomial	O
distribution	B
(	O
5.38	O
)	O
,	O
y	O
(	O
cid:24	O
)	O
multn.n	O
;	O
f	O
.˛//	O
;	O
indicating	O
n	O
independent	O
draws	O
for	O
a	O
density	B
f	O
.˛/	O
on	O
n	O
categories	O
.	O
all	O
of	O
this	O
provides	O
a	O
concise	O
description	O
of	O
the	O
g-modeling	B
probability	O
model	B
:	O
˛	O
!	O
g.˛/	O
d	O
eq˛	O
(	O
cid:0	O
)	O
.˛/	O
!	O
f	O
.˛/	O
d	O
pg.˛/	O
!	O
y	O
(	O
cid:24	O
)	O
multn.n	O
;	O
f	O
.˛//	O
:	O
(	O
21.22	O
)	O
426	O
empirical	B
bayes	O
estimation	B
strategies	I
the	O
inferential	O
task	O
goes	O
in	O
the	O
reverse	O
direction	O
,	O
y	O
!	O
o˛	O
!	O
f	O
.o˛/	O
!	O
g.o˛/	O
d	O
eqo˛	O
(	O
cid:0	O
)	O
.o˛/	O
:	O
(	O
21.23	O
)	O
figure	O
21.2	O
a	O
schematic	O
diagram	O
of	O
empirical	B
bayes	O
estimation	B
,	O
as	O
explained	O
in	O
the	O
text	O
.	O
sn	O
is	O
the	O
n-dimensional	O
simplex	B
,	O
containing	O
the	O
p-parameter	B
family	O
f	O
of	O
allowable	O
probability	O
distributions	O
f	O
.˛/	O
.	O
the	O
vector	B
of	O
observed	O
proportions	O
y=n	O
yields	O
mle	O
f	O
.o˛/	O
,	O
which	O
is	O
then	O
deconvolved	O
to	O
obtain	O
estimate	B
g.o˛/	O
.	O
a	O
schematic	O
diagram	O
of	O
the	O
estimation	B
process	O
appears	O
in	O
figure	O
21.2	O
.	O
(	O
cid:15	O
)	O
the	O
vector	B
of	O
observed	O
proportions	O
y=n	O
is	O
a	O
point	O
in	O
sn	O
,	O
the	O
simplex	B
(	O
cid:15	O
)	O
the	O
parametric	B
family	O
of	O
allowable	O
f	O
vectors	O
(	O
21.19	O
)	O
(	O
5.39	O
)	O
of	O
all	O
possible	O
probability	O
vectors	O
f	O
on	O
n	O
categories	O
;	O
y=n	O
is	O
the	O
usual	O
nonparametric	B
estimate	O
of	O
f	O
.	O
d	O
ff	O
.˛/	O
;	O
˛	O
2	O
ag	O
;	O
f	O
(	O
21.24	O
)	O
indicated	O
by	O
the	O
red	O
curve	O
,	O
is	O
a	O
curved	B
p-dimensional	O
surface	O
in	O
sn	O
.	O
here	O
a	O
is	O
the	O
space	B
of	O
allowable	O
vectors	O
˛	O
in	O
family	O
(	O
21.11	O
)	O
.	O
(	O
cid:15	O
)	O
the	O
nonparametric	B
estimate	O
y=n	O
is	O
“	O
projected	O
”	O
down	O
to	O
the	O
parametric	B
estimate	I
f	O
.o˛/	O
;	O
if	O
we	O
are	O
using	O
mle	O
estimation	B
,	O
f	O
.o˛/	O
will	O
be	O
the	O
closest	O
point	O
in	O
f	O
to	O
y=n	O
measured	O
according	O
to	O
a	O
deviance	O
metric	O
,	O
as	O
in	O
(	O
8.35	O
)	O
.	O
(	O
cid:15	O
)	O
finally	O
,	O
f	O
.o˛/	O
is	O
mapped	O
back	O
to	O
the	O
estimate	B
g.o˛/	O
,	O
by	O
inverting	O
map-	O
ping	O
(	O
21.19	O
)	O
.	O
(	O
inversion	O
is	O
not	O
actually	O
necessary	O
with	O
g-modeling	B
since	O
,	O
having	O
found	O
o˛	O
,	O
g.o˛/	O
is	O
obtained	O
directly	O
from	O
(	O
21.11	O
)	O
;	O
the	O
inversion	O
step	O
is	O
more	O
difﬁcult	O
for	O
f	B
-modeling	I
,	O
section	O
21.6	O
.	O
)	O
21.3	O
likelihood	B
,	O
regularization	B
,	O
and	O
accuracy	O
427	O
the	O
maximum	B
likelihood	I
estimation	O
process	O
for	O
g-modeling	B
is	O
discussed	O
in	O
more	O
detail	O
in	O
the	O
next	O
section	O
,	O
where	O
formulas	O
for	O
its	O
accuracy	O
will	O
be	O
developed	O
.	O
21.3	O
likelihood	B
,	O
regularization	B
,	O
and	O
accuracy1	O
parametric	B
g-modeling	O
,	O
as	O
in	O
(	O
21.11	O
)	O
,	O
allows	O
us	O
to	O
work	O
in	O
low-dimensional	O
parametric	B
families—just	O
ﬁve	O
parameters	O
for	O
the	O
nodes	B
example	O
(	O
21.14	O
)	O
—	O
where	O
classic	O
maximum	B
likelihood	I
methods	O
can	O
be	O
more	O
conﬁdently	O
ap-	O
plied	O
.	O
even	O
here	O
though	O
,	O
some	O
regularization	B
will	O
be	O
necessary	O
for	O
stable	O
estimation	B
,	O
as	O
discussed	O
in	O
what	O
follows	O
.	O
the	O
g-model	O
probability	O
mechanism	O
(	O
21.22	O
)	O
yields	O
a	O
log	O
likelihood	B
for	O
the	O
multinomial	O
vector	B
y	O
of	O
counts	O
as	O
a	O
function	B
of	O
˛	O
,	O
say	O
ly	O
.˛/	O
;	O
fk.˛/yk	O
yk	O
log	O
fk.˛/	O
:	O
ly	O
.˛/	O
d	O
log	O
its	O
score	O
function	B
p	O
h	B
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
p	O
,	O
determines	O
the	O
mle	O
o˛	O
according	O
to	O
p	O
p	O
(	O
cid:2	O
)	O
p	O
matrix	B
of	O
second	O
derivatives	O
r	O
fisher	O
information	B
matrix	O
(	O
5.26	O
)	O
ly	O
.˛/	O
,	O
the	O
vector	B
of	O
partial	O
derivatives	O
@	O
ly	O
.˛/=	O
@	O
˛h	O
for	O
ly	O
.o˛/	O
d	O
0.	O
the	O
ly	O
.˛/	O
d	O
.	O
@	O
2ly	O
.˛/=	O
@	O
˛h	O
@	O
˛l	O
/	O
gives	O
the	O
(	O
21.25	O
)	O
ny	O
kd1	O
!	O
d	O
nx	O
kd1	O
the	O
exponential	O
family	O
model	B
(	O
21.11	O
)	O
yields	O
simple	O
expressions	O
for	O
p	O
and	O
i.˛/	O
.	O
deﬁne	O
ly	O
.˛/	O
(	O
21.26	O
)	O
ly	O
.˛/g	O
:	O
i.˛/	O
d	O
ef	O
(	O
cid:0	O
)	O
r	O
	O
pkj	O
wkj	O
d	O
gj	O
.˛/	O
	O
(	O
cid:0	O
)	O
1	O
fk.˛/	O
and	O
the	O
corresponding	O
m-vector	O
wk.˛/	O
d	O
.wk1.˛/	O
;	O
wk2.˛/	O
;	O
:	O
:	O
:	O
;	O
wkm.˛//	O
0	O
lemma	O
21.1	O
the	O
score	O
function	B
p	O
ly	O
.˛/	O
under	O
model	B
(	O
21.22	O
)	O
is	O
(	O
21.27	O
)	O
:	O
(	O
21.28	O
)	O
where	O
wc.˛/	O
d	O
nx	O
kd1	O
p	O
ly	O
.˛/	O
d	O
qwc.˛/	O
;	O
wk.˛/yk	O
(	O
21.29	O
)	O
and	O
q	O
is	O
the	O
m	O
(	O
cid:2	O
)	O
p	O
structure	O
matrix	B
in	O
(	O
21.11	O
)	O
.	O
1	O
the	O
technical	O
lemmas	O
in	O
this	O
section	O
are	O
not	O
essential	O
to	O
following	O
the	O
subsequent	O
discussion	O
.	O
empirical	B
bayes	O
estimation	B
strategies	I
428	O
lemma	O
21.2	O
the	O
fisher	O
information	B
matrix	O
i.˛/	O
,	O
evaluated	O
at	O
˛	O
d	O
o˛	O
,	O
is	O
i.o˛/	O
d	O
q	O
0	O
wk.o˛/nfk.o˛/wk.o˛/	O
0	O
q	O
;	O
(	O
21.30	O
)	O
)	O
(	O
nx	O
kd1	O
1	O
yk	O
is	O
the	O
sample	B
size	I
in	O
the	O
empirical	B
bayes	O
model	B
(	O
21.1	O
)	O
–	O
where	O
n	O
dpn	O
(	O
21.2	O
)	O
.	O
see	O
the	O
chapter	O
endnotes	O
	O
for	O
a	O
brief	O
discussion	O
of	O
lemmas	O
21.1	O
and	O
21.2.	O
i.o˛/	O
(	O
cid:0	O
)	O
1	O
is	O
the	O
usual	O
maximum	B
likelihood	I
estimate	O
of	O
the	O
covariance	O
matrix	B
of	O
o˛	O
,	O
but	O
we	O
will	O
use	O
a	O
regularized	O
version	O
of	O
the	O
mle	O
that	O
is	O
less	O
variable	O
.	O
in	O
the	O
examples	O
that	O
follow	O
,	O
o˛	O
was	O
found	O
by	O
numerical	O
maximization.2	O
even	O
though	O
g.˛/	O
is	O
an	O
exponential	O
family	O
,	O
the	O
marginal	O
density	B
f	O
.˛/	O
in	O
(	O
21.22	O
)	O
is	O
not	O
.	O
as	O
a	O
result	O
,	O
some	O
care	O
is	O
needed	O
in	O
avoiding	O
local	O
maxima	O
of	O
ly	O
.˛/	O
.	O
these	O
tend	O
to	O
occur	O
at	O
“	O
corner	O
”	O
values	O
of	O
˛	O
,	O
where	O
one	O
of	O
its	O
compo-	O
nents	O
goes	O
to	O
inﬁnity	O
.	O
a	O
small	O
amount	O
of	O
regularization	B
pulls	O
o˛	O
away	O
from	O
the	O
corners	O
,	O
decreasing	O
its	O
variance	O
at	O
the	O
possible	O
expense	O
of	O
increased	O
bias	O
.	O
instead	O
of	O
maximizing	O
ly	O
.˛/	O
we	O
maximize	O
a	O
penalized	O
likelihood	B
m.˛/	O
d	O
ly	O
.˛/	O
(	O
cid:0	O
)	O
s.˛/	O
;	O
(	O
21.31	O
)	O
where	O
s.˛/	O
is	O
a	O
positive	O
penalty	B
function	O
.	O
our	O
examples	O
use	O
s.˛/	O
d	O
c0k˛k	O
d	O
c0	O
(	O
21.32	O
)	O
(	O
with	O
c0	O
equal	O
1	O
)	O
,	O
which	O
prevents	O
the	O
maximizer	O
o˛	O
of	O
m.˛/	O
from	O
venturing	O
too	O
far	O
into	O
corners	O
.	O
hd1	O
˛2	O
h	B
the	O
following	O
lemma	O
is	O
discussed	O
in	O
the	O
chapter	O
endnotes	O
.	O
lemma	O
21.3	O
the	O
maximizer	O
o˛	O
of	O
m.˛/	O
has	O
approximate	O
bias	O
vector	B
and	O
covariance	O
matrix	B
bias.o˛/	O
d	O
(	O
cid:0	O
)	O
.i.o˛/	O
c	O
rs.o˛//	O
and	O
var.o˛/	O
d	O
.i.o˛/	O
c	O
rs.o˛//	O
(	O
cid:0	O
)	O
1	O
ps.o˛/	O
(	O
cid:0	O
)	O
1	O
i.o˛/	O
.i.o˛/	O
c	O
rs.o˛//	O
(	O
cid:0	O
)	O
1	O
;	O
(	O
21.33	O
)	O
px	O
!	O
1=2	O
where	O
i.o˛/	O
is	O
given	O
in	O
(	O
21.30	O
)	O
.	O
with	O
s.˛/	O
	O
0	O
(	O
no	O
regularization	B
)	O
the	O
bias	O
is	O
zero	O
and	O
var.o˛/	O
d	O
i.o˛/	O
(	O
cid:0	O
)	O
1	O
,	O
2	O
using	O
the	O
nonlinear	B
maximizer	O
nlm	B
in	O
r.	O
1	O
2	O
21.3	O
likelihood	B
,	O
regularization	B
,	O
and	O
accuracy	O
429	O
the	O
usual	O
mle	O
approximations	O
:	O
including	O
s.˛/	O
reduces	O
variance	O
while	O
in-	O
troducing	O
bias	O
.	O
for	O
s.˛/	O
d	O
c0k˛k	O
we	O
calculate	O
	O
	O
rs.˛/	O
d	O
c0k˛k	O
0	O
i	O
(	O
cid:0	O
)	O
˛˛	O
k˛k2	O
ps.˛/	O
d	O
c0˛=k˛k	O
and	O
(	O
21.34	O
)	O
with	O
i	O
the	O
p	O
(	O
cid:2	O
)	O
p	O
identity	O
matrix	B
.	O
adding	O
the	O
penalty	B
s.˛/	O
in	O
(	O
21.31	O
)	O
pulls	O
the	O
mle	O
of	O
˛	O
toward	O
zero	O
and	O
the	O
mle	O
of	O
g.˛/	O
toward	O
a	O
ﬂat	O
distribution	B
over	O
t	B
.	O
looking	O
at	O
var.o˛/	O
in	O
(	O
21.33	O
)	O
,	O
a	O
measure	O
of	O
the	O
regularization	B
effect	O
;	O
is	O
tr.rs.o˛//=	O
tr.i.o˛//	O
;	O
(	O
21.35	O
)	O
most	O
often	O
we	O
will	O
be	O
more	O
interested	O
in	O
the	O
accuracy	O
of	O
og	O
d	O
g.o˛/	O
than	O
which	O
was	O
never	O
more	O
than	O
a	O
few	O
percent	O
in	O
our	O
examples	O
.	O
in	O
that	O
of	O
o˛	O
itself	O
.	O
letting	O
d.o˛/	O
d	O
diag.g.o˛//	O
(	O
cid:0	O
)	O
g.o˛/g.o˛/	O
0	O
;	O
the	O
m	O
(	O
cid:2	O
)	O
p	O
derivative	O
matrix	B
.	O
@	O
gj	O
=	O
@	O
˛h/	O
is	O
@	O
g=	O
@	O
˛	O
d	O
d.˛/q	O
;	O
(	O
21.36	O
)	O
(	O
21.37	O
)	O
with	O
q	O
the	O
structure	O
matrix	B
in	O
(	O
21.11	O
)	O
.	O
the	O
usual	O
ﬁrst-order	O
delta-method	O
calculations	O
then	O
give	O
the	O
following	O
theorem	B
.	O
theorem	B
21.4	O
the	O
penalized	O
maximum	B
likelihood	I
estimate	O
og	O
d	O
g.o˛/	O
has	O
estimated	O
bias	O
vector	B
and	O
covariance	O
matrix	B
bias	O
.	O
og/	O
d	O
d.o˛/qbias.o˛/	O
and	O
var	O
.	O
og/	O
d	O
d.o˛/qvar.o˛/q	O
0	O
d.o˛/	O
(	O
21.38	O
)	O
with	O
bias.o˛/	O
and	O
var.o˛/	O
as	O
in	O
(	O
21.33	O
)	O
.3	O
the	O
many	O
approximations	O
going	O
into	O
theorem	B
21.4	O
can	O
be	O
short-circuited	O
by	O
means	O
of	O
the	O
parametric	B
bootstrap	O
,	O
section	O
10.4.	O
starting	O
from	O
o˛	O
and	O
f	O
.o˛/	O
d	O
pg.o˛/	O
,	O
we	O
resample	O
the	O
count	O
vector	B
(	O
cid:3	O
)	O
(	O
cid:24	O
)	O
multn.n	O
;	O
f	O
.o˛//	O
;	O
(	O
cid:3	O
)	O
based	O
on	O
y	O
and	O
calculate4	O
the	O
penalized	O
mle	O
o˛	O
(	O
cid:3	O
)	O
,	O
yielding	O
og	O
(	O
cid:3	O
)	O
d	O
g.o˛	O
(	O
21.39	O
)	O
y	O
(	O
cid:3	O
)	O
/	O
.	O
3	O
note	O
that	O
the	O
bias	O
treats	O
model	B
(	O
21.11	O
)	O
as	O
the	O
true	O
prior	B
,	O
and	O
arises	O
as	O
a	O
result	O
of	O
the	O
penalization	O
.	O
4	O
convergence	O
of	O
the	O
nlm	B
search	O
process	O
is	O
speeded	O
up	O
by	O
starting	O
from	O
o˛	O
.	O
(	O
cid:3	O
)	O
b	O
gives	O
bias	O
and	O
covariance	O
estimates	O
430	O
b	O
replications	O
og	O
empirical	B
bayes	O
estimation	B
strategies	I
(	O
cid:3	O
)	O
1	O
;	O
og	O
dbias	O
d	O
og	O
and	O
cvar	O
d	O
bx	O
(	O
cid:3	O
)	O
(	O
cid:1	O
)	O
dpb	O
(	O
cid:3	O
)	O
2	O
;	O
:	O
:	O
:	O
;	O
og	O
(	O
cid:3	O
)	O
(	O
cid:1	O
)	O
(	O
cid:0	O
)	O
og	O
.	O
og	O
/ı.b	O
(	O
cid:0	O
)	O
1/	O
;	O
(	O
cid:3	O
)	O
b	O
(	O
cid:0	O
)	O
og	O
bd1	O
(	O
cid:3	O
)	O
b=b	O
.	O
og	O
(	O
cid:3	O
)	O
(	O
cid:1	O
)	O
/	O
.	O
og	O
(	O
cid:3	O
)	O
b	O
(	O
cid:0	O
)	O
og	O
(	O
cid:3	O
)	O
(	O
cid:1	O
)	O
1	O
and	O
og	O
(	O
21.40	O
)	O
table	O
21.1	O
comparison	O
of	O
delta	O
method	B
(	O
21.38	O
)	O
and	O
bootstrap	O
(	O
21.40	O
)	O
standard	O
errors	O
and	O
biases	O
for	O
the	O
nodes	B
study	O
estimate	O
of	O
g	O
in	O
figure	O
6.4.	O
all	O
columns	O
except	O
the	O
ﬁrst	O
multiplied	O
by	O
100.	O
standard	B
error	I
bias	O
	O
.01	O
.12	O
.23	O
.34	O
.45	O
.56	O
.67	O
.78	O
.89	O
.99	O
g.	O
/	O
delta	O
12.048	O
1.045	O
.381	O
.779	O
1.119	O
.534	O
.264	O
.224	O
.321	O
.576	O
.887	O
.131	O
.058	O
.096	O
.121	O
.102	O
.047	O
.056	O
.054	O
.164	O
delta	O
.056	O
.025	O
boot	O
boot	O
.967	O
(	O
cid:0	O
)	O
.518	O
(	O
cid:0	O
)	O
.592	O
.071	O
.139	O
.065	O
.033	O
.095	O
(	O
cid:0	O
)	O
.011	O
(	O
cid:0	O
)	O
.013	O
.117	O
(	O
cid:0	O
)	O
.040	O
(	O
cid:0	O
)	O
.049	O
.027	O
.100	O
.019	O
.051	O
.027	O
.023	O
.053	O
.020	O
.018	O
.009	O
.048	O
.013	O
.169	O
(	O
cid:0	O
)	O
.008	O
.008	O
table	O
21.1	O
compares	O
the	O
delta	O
method	B
of	O
theorem	B
20.4	O
with	O
the	O
para-	O
metric	O
bootstrap	O
(	O
b	O
d	O
1000	O
replications	O
)	O
for	O
the	O
surgical	O
nodes	B
example	O
of	O
section	O
6.3.	O
both	O
the	O
standard	O
errors—square	O
roots	O
of	O
the	O
diagonal	O
el-	O
ements	O
of	O
var	O
.	O
og/—and	O
biases	O
are	O
well	O
approximated	O
by	O
the	O
delta	O
method	B
formulas	O
(	O
21.38	O
)	O
.	O
the	O
delta	O
method	B
also	O
performed	O
reasonably	O
well	O
on	O
the	O
two	O
examples	O
of	O
the	O
next	O
section	O
.	O
it	O
did	O
less	O
well	O
on	O
the	O
artiﬁcial	O
example	O
of	O
figure	O
21.1	O
,	O
where	O
g.	O
/	O
d	O
1	O
iœ	O
(	O
cid:0	O
)	O
3	O
;	O
3.	O
/	O
8	O
6	O
c	O
7	O
8	O
1p	O
2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
(	O
cid:0	O
)	O
1	O
2	O
	O
2	O
(	O
cid:27	O
)	O
2	O
e	O
.	O
(	O
cid:27	O
)	O
d	O
0:5/	O
(	O
21.41	O
)	O
(	O
1/8	O
uniform	O
on	O
œ	O
(	O
cid:0	O
)	O
3	O
;	O
3	O
and	O
7/8	O
n	O
.0	O
;	O
0:52/	O
)	O
.	O
the	O
vertical	O
bars	O
in	O
fig-	O
ure	O
21.3	O
indicate	O
˙	O
one	O
standard	B
error	I
obtained	O
from	O
the	O
parametric	B
boot-	O
d	O
f	O
(	O
cid:0	O
)	O
3	O
;	O
(	O
cid:0	O
)	O
2:8	O
;	O
:	O
:	O
:	O
;	O
3g	O
for	O
the	O
sample	B
space	O
of	O
‚	O
,	O
and	O
as-	O
strap	O
,	O
taking	O
t	B
suming	O
a	O
natural	O
spline	O
model	B
in	O
(	O
21.11	O
)	O
with	O
ﬁve	O
degrees	O
of	O
freedom	O
,	O
g.˛/	O
d	O
eq˛	O
(	O
cid:0	O
)	O
.˛/	O
;	O
q	O
d	O
ns	O
(	O
t	B
,	O
df=5	O
)	O
:	O
(	O
21.42	O
)	O
21.3	O
likelihood	B
,	O
regularization	B
,	O
and	O
accuracy	O
431	O
figure	O
21.3	O
the	O
red	O
curve	O
is	O
g.	O
/	O
for	O
the	O
artiﬁcial	O
example	O
of	O
figure	O
21.1.	O
vertical	O
bars	O
are	O
˙	O
one	O
standard	B
error	I
for	O
g-model	O
estimate	B
g.o˛/	O
;	O
speciﬁcations	O
(	O
21.41	O
)	O
–	O
(	O
21.42	O
)	O
,	O
sample	B
size	I
n	O
d	O
1000	O
observations	O
xi	O
(	O
cid:24	O
)	O
n	O
.‚i	O
;	O
1/	O
,	O
using	O
parametric	B
bootstrap	O
(	O
21.40	O
)	O
,	O
b	O
d	O
500.	O
the	O
light	O
dashed	O
line	O
follows	O
bootstrap	O
means	O
og	O
(	O
cid:3	O
)	O
j	O
.	O
some	O
deﬁnitional	O
bias	O
is	O
apparent	O
.	O
the	O
sampling	O
model	O
was	O
xi	O
(	O
cid:24	O
)	O
n	O
.‚i	O
;	O
1/	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
d	O
1000.	O
in	O
this	O
case	O
the	O
delta	O
method	B
standard	O
errors	B
were	O
about	O
25	O
%	O
too	O
small	O
.	O
the	O
light	O
dashed	O
curve	O
in	O
figure	O
21.3	O
traces	O
ng.	O
/	O
,	O
the	O
average	O
of	O
the	O
b	O
d	O
500	O
bootstrap	O
replications	O
g	O
(	O
cid:3	O
)	O
b.	O
there	O
is	O
noticeable	O
bias	O
,	O
compared	O
with	O
g.	O
/	O
.	O
the	O
reason	O
is	O
simple	O
:	O
the	O
exponential	O
family	O
(	O
21.42	O
)	O
for	O
g.˛/	O
does	O
not	O
include	O
g.	O
/	O
(	O
21.41	O
)	O
.	O
in	O
fact	O
,	O
ng.	O
/	O
is	O
(	O
nearly	O
)	O
the	O
closest	O
mem-	O
ber	O
of	O
the	O
exponential	O
family	O
to	O
g.	O
/	O
.	O
this	O
kind	O
of	O
deﬁnitional	O
bias	O
is	O
a	O
disadvantage	O
of	O
parametric	B
g-modeling	O
.	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
——	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
——	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
——	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
our	O
g-modeling	B
examples	O
,	O
and	O
those	O
of	O
the	O
next	O
section	O
,	O
bring	O
together	O
a	O
variety	O
of	O
themes	O
from	O
modern	O
statistical	O
practice	O
:	O
classical	O
maximum	B
likelihood	I
theory	O
,	O
exponential	O
family	O
modeling	O
,	O
regularization	B
,	O
bootstrap	O
methods	O
,	O
large	O
data	B
sets	O
of	O
parallel	O
structure	O
,	O
indirect	O
evidence	O
,	O
and	O
a	O
com-	O
bination	O
of	O
bayesian	O
and	O
frequentist	O
thinking	O
,	O
all	O
of	O
this	O
enabled	O
by	O
mas-	O
sive	O
computer	O
power	O
.	O
taken	O
together	O
they	O
paint	O
an	O
attractive	O
picture	O
of	O
the	O
range	O
of	O
inferential	O
methodology	O
in	O
the	O
twenty-ﬁrst	O
century	O
.	O
−3−2−101230.000.050.100.15qg	O
(	O
q	O
)	O
432	O
empirical	B
bayes	O
estimation	B
strategies	I
21.4	O
two	O
examples	O
we	O
now	O
reconsider	O
two	O
previous	O
data	B
sets	O
from	O
a	O
g-modeling	B
point	O
of	O
view	O
.	O
the	O
ﬁrst	O
is	O
the	O
artiﬁcial	O
microarray-type	O
example	O
(	O
20.24	O
)	O
comprising	O
n	O
d10,000	O
independent	O
observations	O
(	O
21.43	O
)	O
ind	O
(	O
cid:24	O
)	O
zi	O
with	O
(	O
cid:22	O
)	O
i	O
(	O
cid:24	O
)	O
0	O
n	O
.	O
(	O
cid:22	O
)	O
i	O
;	O
1/	O
;	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
n	O
d	O
10,000	O
;	O
for	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
9000	O
for	O
i	O
d	O
9001	O
;	O
:	O
:	O
:	O
;	O
10,000	O
:	O
(	O
n	O
.	O
(	O
cid:0	O
)	O
3	O
;	O
1/	O
(	O
cid:22	O
)	O
i	O
2	O
.zi	O
(	O
cid:0	O
)	O
3/=2	O
˙	O
1:96ıp	O
(	O
21.44	O
)	O
figure	O
20.3	O
displays	O
the	O
points	O
.zi	O
;	O
(	O
cid:22	O
)	O
i	O
/	O
for	O
i	O
d	O
9001	O
;	O
:	O
:	O
:	O
;	O
10	O
;	O
000	O
,	O
illus-	O
trating	O
the	O
bayes	O
posterior	O
95	O
%	O
conditional	O
intervals	B
(	O
20.26	O
)	O
,	O
these	O
required	O
knowing	O
the	O
bayes	O
prior	B
distribution	I
(	O
cid:22	O
)	O
i	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:0	O
)	O
3	O
;	O
1/	O
.	O
we	O
would	O
like	O
to	O
recover	O
intervals	B
(	O
21.45	O
)	O
using	O
just	O
the	O
observed	O
data	B
zi	O
,	O
i	O
d	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
10	O
;	O
000	O
,	O
without	O
knowledge	O
of	O
the	O
prior	B
.	O
2	O
:	O
(	O
21.45	O
)	O
figure	O
21.4	O
histogram	O
of	O
observed	O
sample	B
of	O
n	O
d	O
10,000	O
values	O
zi	O
from	O
simulations	O
(	O
21.43	O
)	O
–	O
(	O
21.44	O
)	O
.	O
a	O
histogram	O
of	O
the	O
10,000	O
z-values	O
is	O
shown	O
in	O
figure	O
21.4	O
;	O
g-modeling	B
(	O
21.9	O
)	O
–	O
(	O
21.11	O
)	O
was	O
applied	O
to	O
them	O
(	O
now	O
with	O
(	O
cid:22	O
)	O
playing	O
the	O
role	O
of	O
“	O
‚	O
”	O
z-valuesfrequency−8−6−4−20240200400600800||^^	O
21.4	O
two	O
examples	O
433	O
d	O
.	O
(	O
cid:0	O
)	O
6	O
;	O
(	O
cid:0	O
)	O
5:75	O
;	O
:	O
:	O
:	O
;	O
3/	O
.	O
q	O
was	O
composed	O
of	O
a	O
and	O
z	O
being	O
“	O
x	O
”	O
)	O
,	O
taking	O
t	B
delta	O
function	B
at	O
(	O
cid:22	O
)	O
d	O
0	O
and	O
a	O
ﬁfth-degree	O
polynomial	O
basis	O
for	O
the	O
nonzero	O
(	O
cid:22	O
)	O
,	O
again	O
a	O
family	O
of	O
spike-and-slab	O
priors	B
.	O
the	O
penalized	O
mle	O
og	O
(	O
21.31	O
)	O
,	O
(	O
21.32	O
)	O
,	O
c0	O
d	O
1	O
,	O
estimated	O
the	O
probability	O
of	O
(	O
cid:22	O
)	O
d	O
0	O
as	O
og.0/	O
d	O
0:891	O
˙	O
0:006	O
(	O
21.46	O
)	O
(	O
using	O
(	O
21.38	O
)	O
,	O
which	O
also	O
provided	O
bias	O
estimate	B
0.001	O
)	O
.	O
figure	O
21.5	O
purple	O
curves	O
show	O
g-modeling	B
estimates	O
of	O
conditional	O
95	O
%	O
credible	O
intervals	B
for	O
(	O
cid:22	O
)	O
given	O
z	O
in	O
artiﬁcial	O
microarray	O
example	O
(	O
21.43	O
)	O
–	O
(	O
21.44	O
)	O
.	O
they	O
are	O
a	O
close	O
match	O
to	O
the	O
actual	O
bayes	O
intervals	B
,	O
dotted	O
lines	O
;	O
cf	O
.	O
figure	O
20.3.	O
the	O
estimated	O
posterior	O
density	O
of	O
(	O
cid:22	O
)	O
given	O
z	O
is	O
og	O
.	O
(	O
cid:22	O
)	O
jz/	O
d	O
cz	O
og	O
.	O
(	O
cid:22	O
)	O
/	O
(	O
cid:30	O
)	O
.z	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
/	O
;	O
	O
(	O
cid:16	O
)	O
(	O
21.47	O
)	O
(	O
cid:30	O
)	O
.	O
(	O
cid:1	O
)	O
/	O
the	O
standard	O
normal	O
density	B
and	O
cz	O
the	O
constant	O
required	O
for	O
og	O
.	O
(	O
cid:22	O
)	O
jz/	O
to	O
integrate	O
to	O
1.	O
let	O
q.˛/.z/	O
denote	O
the	O
˛th	O
quantile	O
of	O
og.	O
(	O
cid:22	O
)	O
jz/	O
.	O
the	O
purple	O
curves	O
in	O
figure	O
21.5	O
trace	O
the	O
estimated	O
95	O
%	O
credible	O
intervals	B
q.:025/.z/	O
;	O
q.:975/.z/	O
:	O
(	O
21.48	O
)	O
they	O
are	O
a	O
close	O
match	O
to	O
the	O
actual	O
credible	O
intervals	B
(	O
21.45	O
)	O
.	O
the	O
solid	O
black	O
curve	O
in	O
figure	O
21.6	O
shows	O
og	O
.	O
(	O
cid:22	O
)	O
/	O
for	O
(	O
cid:22	O
)	O
¤	O
0	O
(	O
the	O
“	O
slab	O
”	O
portion	O
of	O
the	O
estimated	O
prior	B
)	O
.	O
as	O
an	O
estimate	O
of	O
the	O
actual	O
slab	O
density	B
−8−6−4−20−10−8−6−4−20observed	O
zmllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllby.loby.up−2.77llllllllllllllbayes.lobayes.upeb.loeb.up	O
434	O
empirical	B
bayes	O
estimation	B
strategies	I
figure	O
21.6	O
the	O
heavy	O
black	O
curve	O
is	O
the	O
g-modeling	B
estimate	O
of	O
g.	O
(	O
cid:22	O
)	O
/	O
for	O
(	O
cid:22	O
)	O
¤	O
0	O
in	O
the	O
artiﬁcial	O
microarray	O
example	O
,	O
suppressing	O
the	O
atom	O
at	O
zero	O
,	O
og.0/	O
d	O
0:891.	O
it	O
is	O
only	O
a	O
rough	O
estimate	O
of	O
the	O
actual	O
nonzero	O
density	B
n	O
.	O
(	O
cid:0	O
)	O
3	O
;	O
1/	O
.	O
n	O
.	O
(	O
cid:0	O
)	O
3	O
;	O
1/	O
it	O
is	O
only	O
roughly	O
accurate	O
,	O
but	O
apparently	O
still	O
accurate	O
(	O
cid:22	O
)	O
(	O
cid:24	O
)	O
enough	O
to	O
yield	O
the	O
reasonably	O
good	O
posterior	O
intervals	O
seen	O
in	O
figure	O
21.5.	O
the	O
fundamental	O
impediment	O
to	O
deconvolution—that	O
large	O
changes	O
in	O
g.	O
/	O
produce	O
only	O
small	O
changes	O
in	O
f	O
.x/—can	O
sometimes	O
operate	O
in	O
the	O
statis-	O
tician	O
’	O
s	O
favor	O
,	O
when	O
only	O
a	O
rough	O
knowledge	O
of	O
g	O
sufﬁces	O
for	O
applied	O
pur-	O
poses	O
.	O
our	O
second	O
example	O
concerns	O
the	O
prostate	B
study	O
data	B
,	O
last	O
seen	O
in	O
figure	O
15.1	O
:	O
n	O
d	O
102	O
men	O
,	O
52	O
cancer	O
patients	O
and	O
50	O
normal	B
controls	O
,	O
each	O
have	O
had	O
their	O
genetic	O
activities	O
measured	O
on	O
a	O
microarray	O
of	O
n	O
d	O
6033	O
genes	O
;	O
genei	O
yields	O
a	O
test	O
statistic	B
zi	O
comparing	O
patients	O
with	O
controls	O
,	O
zi	O
(	O
cid:24	O
)	O
n	O
.	O
(	O
cid:22	O
)	O
i	O
;	O
(	O
cid:27	O
)	O
2	O
0	O
/	O
;	O
(	O
21.49	O
)	O
with	O
(	O
cid:22	O
)	O
i	O
the	O
gene	O
’	O
s	O
effect	O
size	O
.	O
(	O
here	O
we	O
will	O
take	O
the	O
variance	O
(	O
cid:27	O
)	O
2	O
parameter	O
to	O
be	O
estimated	O
,	O
rather	O
than	O
assuming	O
(	O
cid:27	O
)	O
2	O
0	O
density	B
g.	O
(	O
cid:22	O
)	O
/	O
for	O
the	O
effects	O
?	O
0	O
as	O
a	O
d	O
1	O
.	O
)	O
what	O
is	O
the	O
prior	B
the	O
local	O
false-discovery	B
rate	I
program	O
locfdr	B
,	O
section	O
15.5	O
,	O
was	O
ap-	O
plied	O
to	O
the	O
6033	O
zi	O
values	O
,	O
as	O
shown	O
in	O
figure	O
21.7.	O
locfdr	B
is	O
an	O
“	O
f	O
-	O
modeling	O
”	O
method	B
,	O
where	O
probability	O
models	B
are	O
proposed	O
directly	O
for	O
−6−4−20240.000.020.040.060.080.10qdensitylatom.891n	O
(	O
−3,1	O
)	O
21.4	O
two	O
examples	O
435	O
figure	O
21.7	O
the	O
green	O
curve	O
is	O
a	O
six-parameter	O
poisson	O
regression	B
estimate	O
ﬁt	O
to	O
counts	O
of	O
the	O
observed	O
zi	O
values	O
for	O
the	O
prostate	B
data	O
.	O
the	O
dashed	O
curve	O
is	O
the	O
empirical	B
null	O
(	O
15.48	O
)	O
,	O
zi	O
(	O
cid:24	O
)	O
n	O
.0:00	O
;	O
1:062/	O
.	O
the	O
f	B
-modeling	I
program	O
locfdr	B
estimated	O
null	O
probability	O
prf	O
(	O
cid:22	O
)	O
d	O
0g	O
d	O
0:984.	O
genes	O
with	O
z-values	O
lying	O
beyond	O
the	O
red	O
triangles	O
have	O
estimated	O
fdr	O
values	O
less	O
than	O
0.20.	O
the	O
marginal	O
density	B
f	O
.	O
(	O
cid:1	O
)	O
/	O
rather	O
than	O
for	O
the	O
prior	B
density	O
g.	O
(	O
cid:1	O
)	O
/	O
;	O
see	O
sec-	O
tion	O
(	O
21.6	O
)	O
.	O
here	O
we	O
can	O
compare	O
locfdr	B
’	O
s	O
results	O
with	O
those	O
from	O
g-	O
modeling	O
.	O
the	O
former	O
gave5	O
ı0	O
;	O
o	O
(	O
cid:27	O
)	O
0	O
;	O
o	O
(	O
cid:25	O
)	O
0	O
	O
d	O
.0:00	O
;	O
1:06	O
;	O
0:984/	O
(	O
cid:16	O
)	O
o	O
(	O
21.50	O
)	O
in	O
the	O
notation	O
of	O
(	O
15.50	O
)	O
;	O
that	O
is	O
,	O
it	O
estimated	O
the	O
null	O
distribution	B
as	O
(	O
cid:22	O
)	O
(	O
cid:24	O
)	O
n	O
.0	O
;	O
1:062/	O
,	O
with	O
probability	O
o	O
(	O
cid:25	O
)	O
0	O
d	O
0:984	O
of	O
a	O
gene	O
being	O
null	O
(	O
(	O
cid:22	O
)	O
d	O
0	O
)	O
.	O
only	O
22	O
genes	O
were	O
estimated	O
to	O
have	O
local	O
fdr	O
values	O
less	O
than	O
0.20	O
,	O
the	O
9	O
with	O
zi	O
	O
(	O
cid:0	O
)	O
3:71	O
and	O
the	O
12	O
with	O
zi	O
(	O
cid:21	O
)	O
3:81	O
.	O
(	O
these	O
are	O
more	O
pessimistic	O
results	O
than	O
in	O
figure	O
15.5	O
,	O
where	O
we	O
used	O
the	O
theoretical	O
null	O
n	O
.0	O
;	O
1/	O
rather	O
than	O
the	O
empirical	B
null	O
n	O
.0	O
;	O
1:062/	O
.	O
)	O
the	O
g-modeling	B
approach	O
(	O
21.11	O
)	O
was	O
applied	O
to	O
the	O
prostate	B
study	O
data	B
,	O
assuming	O
zi	O
(	O
cid:24	O
)	O
0	O
/	O
,	O
(	O
cid:27	O
)	O
0	O
d	O
1:06	O
as	O
suggested	O
by	O
(	O
21.50	O
)	O
.	O
the	O
n	O
.	O
(	O
cid:22	O
)	O
i	O
;	O
(	O
cid:27	O
)	O
2	O
5	O
using	O
a	O
six-parameter	O
poisson	O
regression	B
ﬁt	O
to	O
the	O
zi	O
values	O
,	O
of	O
the	O
type	O
employed	O
in	O
section	O
10.4	O
.	O
−4−20240100200300400	O
z-valuescounts	O
empirical	B
bayes	O
estimation	B
strategies	I
436	O
structure	O
matrix	B
q	O
in	O
(	O
21.11	O
)	O
had	O
a	O
delta	O
function	B
at	O
(	O
cid:22	O
)	O
d	O
0	O
and	O
a	O
ﬁve-	O
parameter	O
natural	O
spline	O
basis	O
for	O
(	O
cid:22	O
)	O
¤	O
0	O
;	O
t	B
d	O
.	O
(	O
cid:0	O
)	O
3:6	O
;	O
(	O
cid:0	O
)	O
3:4	O
;	O
:	O
:	O
:	O
;	O
3:6/	O
for	O
the	O
discretized	O
‚	O
space	B
(	O
21.9	O
)	O
.	O
this	O
gave	O
a	O
penalized	O
mle	O
og	O
having	O
null	O
probability	O
og.0/	O
d	O
0:946	O
˙	O
0:011	O
:	O
(	O
21.51	O
)	O
figure	O
21.8	O
the	O
g-modeling	B
estimate	O
for	O
the	O
non-null	O
density	B
og	O
.	O
(	O
cid:22	O
)	O
/	O
,	O
(	O
cid:22	O
)	O
¤	O
0	O
,	O
for	O
the	O
prostate	B
study	O
data	B
,	O
also	O
indicating	O
the	O
null	O
atom	O
og.0/	O
d	O
0:946.	O
about	O
2	O
%	O
of	O
the	O
genes	O
are	O
estimated	O
to	O
have	O
effect	O
sizes	O
j	O
(	O
cid:22	O
)	O
ij	O
(	O
cid:21	O
)	O
2.	O
the	O
red	O
bars	O
show	O
˙	O
one	O
standard	B
error	I
as	O
computed	O
from	O
theorem	O
21.4	O
(	O
page	O
429	O
)	O
.	O
the	O
non-null	O
distribution	B
,	O
og	O
.	O
(	O
cid:22	O
)	O
/	O
for	O
(	O
cid:22	O
)	O
¤	O
0	O
,	O
appears	O
in	O
figure	O
21.8	O
,	O
where	O
it	O
is	O
seen	O
to	O
be	O
modestly	O
unimodal	O
around	O
(	O
cid:22	O
)	O
d	O
0.	O
dashed	O
red	O
bars	O
indicate	O
˙	O
one	O
standard	B
error	I
for	O
the	O
og..j	O
//	O
estimates	O
obtained	O
from	O
theorem	O
21.4	O
(	O
page	O
429	O
)	O
.	O
the	O
accuracy	O
is	O
not	O
very	O
good	O
.	O
it	O
is	O
better	O
for	O
larger	O
regions	O
of	O
the	O
‚	O
space	B
,	O
for	O
examplebprfjj	O
(	O
cid:21	O
)	O
2g	O
d	O
0:020	O
˙	O
0:0014	O
:	O
(	O
21.52	O
)	O
here	O
g-modeling	B
estimated	O
less	O
prior	B
null	O
probability	O
,	O
0.946	O
compared	O
with	O
0.984	O
from	O
f	O
-modeling	O
,	O
but	O
then	O
attributed	O
much	O
of	O
the	O
non-null	O
probability	O
to	O
small	O
values	O
of	O
j	O
(	O
cid:22	O
)	O
ij	O
.	O
taking	O
(	O
21.52	O
)	O
literally	O
suggests	O
121	O
(	O
d	O
0:020	O
(	O
cid:1	O
)	O
6033	O
)	O
genes	O
with	O
true	O
−4−20240.00000.00050.00100.00150.00200.00250.0030qg	O
(	O
q	O
)	O
||lnull	O
atom0.946	O
21.5	O
generalized	O
linear	O
mixed	O
models	O
437	O
false-discovery	O
ratebprf	O
(	O
cid:22	O
)	O
d	O
0jzg	O
from	O
g-modeling	O
.	O
for	O
large	O
figure	O
21.9	O
the	O
black	O
curve	O
is	O
the	O
empirical	B
bayes	O
estimated	O
values	O
of	O
jzj	O
it	O
nearly	O
matches	O
the	O
locfdr	B
f	O
-modeling	O
estimate	B
fdr.z/	O
,	O
red	O
curve	O
.	O
effect	O
sizes	O
j	O
(	O
cid:22	O
)	O
ij	O
(	O
cid:21	O
)	O
2.	O
that	O
doesn	O
’	O
t	B
mean	O
we	O
can	O
say	O
with	O
certainty	O
which	O
121.	O
figure	O
21.9	O
compares	O
the	O
g-modeling	B
empirical	O
bayes	O
false-discovery	B
rate	I
as	O
in	O
(	O
21.47	O
)	O
,	O
with	O
the	O
f	B
-modeling	I
estimatecfdr.z/	O
produced	O
by	O
locfdr	B
.	O
(	O
21.53	O
)	O
;	O
where	O
it	O
counts	O
,	O
in	O
the	O
tails	O
,	O
they	O
are	O
nearly	O
the	O
same	O
.	O
bprf	O
(	O
cid:22	O
)	O
d	O
0jzg	O
d	O
cz	O
og.0/	O
(	O
cid:30	O
)	O
	O
	O
z	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
o	O
(	O
cid:27	O
)	O
0	O
21.5	O
generalized	O
linear	O
mixed	O
models	O
the	O
g-modeling	B
theory	O
can	O
be	O
extended	O
to	O
the	O
situation	O
where	O
each	O
ob-	O
servation	O
xi	O
is	O
accompanied	O
by	O
an	O
observed	O
vector	B
of	O
covariates	O
ci	O
,	O
say	O
of	O
dimension	O
d.	O
we	O
return	O
to	O
the	O
generalized	O
linear	B
model	I
setup	O
of	O
sec-	O
tion	O
8.2	O
,	O
where	O
each	O
xi	O
has	O
a	O
one-parameter	B
exponential	O
family	O
density	B
indexed	O
by	O
its	O
own	O
natural	O
parameter	O
(	O
cid:21	O
)	O
i	O
,	O
f	O
(	O
cid:21	O
)	O
i	O
.xi	O
/	O
d	O
expf	O
(	O
cid:21	O
)	O
i	O
xi	O
(	O
cid:0	O
)	O
(	O
cid:13	O
)	O
.	O
(	O
cid:21	O
)	O
i	O
/gf0.xi	O
/	O
(	O
21.54	O
)	O
in	O
notation	O
(	O
8.20	O
)	O
.	O
−4−20240.00.20.40.60.81.0z-valuefdr	O
(	O
z	O
)	O
438	O
empirical	B
bayes	O
estimation	B
strategies	I
our	O
key	O
assumption	O
is	O
that	O
each	O
(	O
cid:21	O
)	O
i	O
is	O
the	O
sum	O
of	O
a	O
deterministic	O
compo-	O
nent	O
,	O
depending	O
on	O
the	O
covariates	O
ci	O
,	O
and	O
a	O
random	O
term	O
‚i	O
,	O
(	O
cid:21	O
)	O
i	O
d	O
‚i	O
c	O
c	O
0	O
i	O
ˇ	O
:	O
(	O
21.55	O
)	O
here	O
‚i	O
is	O
an	O
unobserved	O
realization	O
from	O
g.˛/	O
d	O
expfq˛	O
(	O
cid:0	O
)	O
.˛/g	O
(	O
21.11	O
)	O
and	O
ˇ	O
is	O
an	O
unknown	O
d-dimensional	O
parameter	O
.	O
if	O
ˇ	O
d	O
0	O
then	O
(	O
21.55	O
)	O
is	O
a	O
g-model	O
as	O
before,6	O
while	O
if	O
all	O
the	O
‚i	O
d	O
0	O
then	O
it	O
is	O
a	O
stan-	O
dard	O
glm	O
(	O
8.20	O
)	O
–	O
(	O
8.22	O
)	O
.	O
taken	O
together	O
,	O
(	O
21.55	O
)	O
represents	O
a	O
generalized	O
linear	B
mixed	I
model	I
(	O
glmm	O
)	O
.	O
the	O
likelihood	B
and	O
accuracy	O
calculations	O
of	O
section	O
21.3	O
extend	O
to	O
glmms	O
,	O
as	O
referenced	O
in	O
the	O
endnotes	O
,	O
but	O
here	O
we	O
will	O
only	O
discuss	O
a	O
glmm	O
analysis	B
of	O
the	O
nodes	B
study	O
of	O
section	O
6.3.	O
in	O
addition	O
to	O
ni	O
the	O
number	O
of	O
nodes	O
removed	O
and	O
xi	O
the	O
number	O
found	O
positive	O
(	O
6.33	O
)	O
,	O
a	O
vector	B
of	O
four	O
covariates	O
ci	O
d	O
.agei	O
,	O
sexi	O
,	O
smokei	O
,	O
progi	O
/	O
(	O
21.56	O
)	O
was	O
observed	O
for	O
each	O
patient	O
:	O
a	O
standardized	O
version	O
of	O
age	O
in	O
years	O
;	O
sex	O
being	O
0	O
for	O
female	O
or	O
1	O
for	O
male	O
;	O
smoke	O
being	O
0	O
for	O
no	O
or	O
1	O
for	O
yes	O
to	O
long-	O
term	O
smoking	O
;	O
and	O
prog	O
being	O
a	O
post-operative	O
prognosis	O
score	O
with	O
large	O
values	O
more	O
favorable	O
.	O
glmm	O
model	B
(	O
21.55	O
)	O
was	O
applied	O
to	O
the	O
nodes	B
data	O
.	O
now	O
(	O
cid:21	O
)	O
i	O
was	O
the	O
logit	O
logœ	O
(	O
cid:25	O
)	O
i	O
=.1	O
(	O
cid:0	O
)	O
(	O
cid:25	O
)	O
i	O
/	O
,	O
where	O
xi	O
(	O
cid:24	O
)	O
bi.ni	O
;	O
(	O
cid:25	O
)	O
i	O
/	O
(	O
21.57	O
)	O
as	O
in	O
table	O
8.4	O
,	O
i.e.	O
,	O
(	O
cid:25	O
)	O
i	O
is	O
the	O
probability	O
that	O
any	O
one	O
node	O
from	O
patient	O
i	O
is	O
positive	O
.	O
to	O
make	O
the	O
correspondence	O
with	O
the	O
analysis	B
in	O
section	O
6.3	O
exact	O
,	O
we	O
used	O
a	O
variant	O
of	O
(	O
21.55	O
)	O
(	O
cid:21	O
)	O
i	O
d	O
logit.‚i	O
/	O
c	O
c	O
0	O
i	O
ˇ	O
:	O
(	O
21.58	O
)	O
now	O
with	O
ˇ	O
d	O
0	O
,	O
‚i	O
is	O
exactly	O
the	O
binomial	B
probability	O
(	O
cid:25	O
)	O
i	O
for	O
the	O
ith	O
case	O
.	O
maximum	B
likelihood	I
estimates	O
were	O
calculated	O
for	O
˛	O
in	O
(	O
21.11	O
)	O
—	O
d	O
.0:01	O
;	O
0:02	O
;	O
:	O
:	O
:	O
;	O
0:99/	O
and	O
q	O
d	O
poly	O
(	O
t	B
,5	O
)	O
(	O
21.14	O
)	O
—and	O
with	O
t	B
ˇ	O
in	O
(	O
21.58	O
)	O
.	O
the	O
mle	O
prior	B
g.o˛/	O
was	O
almost	O
the	O
same	O
as	O
that	O
estimated	O
without	O
covariates	O
in	O
figure	O
6.4	O
.	O
ˇk=bsek	O
.	O
sex	O
table	O
21.2	O
shows	O
the	O
mle	O
values	O
.	O
(	O
from	O
a	O
parametric	B
bootstrap	O
simulation	O
)	O
,	O
and	O
the	O
z-values	O
o	O
looks	O
like	O
it	O
has	O
a	O
signiﬁcant	O
effect	O
,	O
with	O
males	O
tending	O
toward	O
larger	O
values	O
of	O
(	O
cid:25	O
)	O
i	O
,	O
that	O
is	O
,	O
a	O
greater	O
number	O
of	O
positive	O
nodes	B
.	O
the	O
big	O
effect	O
though	O
is	O
prog	O
,	O
larger	O
values	O
of	O
prog	O
indicating	O
smaller	O
values	O
of	O
(	O
cid:25	O
)	O
i	O
.	O
6	O
here	O
the	O
setup	O
is	O
more	O
speciﬁc	O
;	O
f	O
is	O
exponential	O
family	O
,	O
and	O
‚i	O
is	O
on	O
the	O
o	O
ˇ4/	O
,	O
their	O
standard	O
errors	O
o	O
ˇ1	O
;	O
o	O
ˇ2	O
;	O
o	O
ˇ3	O
;	O
natural-parameter	O
scale	B
.	O
21.5	O
generalized	O
linear	O
mixed	O
models	O
o	O
ˇ4/	O
for	O
glmm	O
table	O
21.2	O
maximum	B
likelihood	I
estimates	O
.	O
analysis	B
of	O
the	O
nodes	B
data	O
,	O
and	O
standard	O
errors	B
from	O
a	O
parametric	B
bootstrap	O
simulation	O
;	O
large	O
values	O
of	O
progi	O
predict	O
low	O
values	O
of	O
(	O
cid:25	O
)	O
i.	O
o	O
ˇ1	O
;	O
o	O
ˇ2	O
;	O
o	O
ˇ3	O
;	O
439	O
age	O
(	O
cid:0	O
)	O
.078	O
.066	O
(	O
cid:0	O
)	O
1.18	O
sex	O
.192	O
.070	O
2.74	O
smoke	O
prog	O
.089	O
(	O
cid:0	O
)	O
.698	O
.077	O
.063	O
1.41	O
9.07	O
mle	O
boot	O
st	O
err	O
z-value	O
figure	O
21.10	O
distribution	B
of	O
(	O
cid:25	O
)	O
i	O
,	O
individual	O
probabilities	B
of	O
a	O
positive	O
node	O
,	O
for	O
best	O
and	O
worst	O
levels	O
of	O
factor	B
prog	O
;	O
from	O
glmm	O
analysis	B
of	O
nodes	B
data	O
.	O
figure	O
21.10	O
displays	O
the	O
distribution	B
of	O
(	O
cid:25	O
)	O
i	O
d	O
1=œ1cexp	O
.	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
i	O
/	O
implied	O
by	O
the	O
glmm	O
model	B
for	O
the	O
best	O
and	O
worst	O
values	O
of	O
prog	O
(	O
setting	O
age	O
,	O
sex	O
,	O
and	O
smoke	O
to	O
their	O
average	O
values	O
and	O
letting	O
‚	O
have	O
distribution	B
g.o˛/	O
)	O
.	O
the	O
implied	O
distribution	B
is	O
concentrated	O
near	O
(	O
cid:25	O
)	O
d	O
0	O
for	O
the	O
best-	O
level	O
prog	O
,	O
while	O
it	O
is	O
roughly	O
uniform	O
over	O
œ0	O
;	O
1	O
for	O
the	O
worst	O
level	O
.	O
the	O
random	O
effects	O
we	O
have	O
called	O
‚i	O
are	O
sometimes	O
called	O
frailties	O
:	O
a	O
composite	O
of	O
unmeasured	O
individual	O
factors	O
lumped	O
together	O
as	O
an	O
index	O
of	O
disease	O
susceptibility	O
.	O
taken	O
together	O
,	O
figures	O
6.4	O
and	O
21.10	O
show	O
sub-	O
stantial	O
frailty	O
and	O
covariate	O
effects	O
both	O
at	O
work	O
in	O
the	O
nodes	B
data	O
.	O
in	O
0.00.20.40.60.81.00.000.050.100.15probability	O
positive	O
nodedensityworst	O
prognosisbest	O
prognosis	O
440	O
empirical	B
bayes	O
estimation	B
strategies	I
the	O
language	O
of	O
section	O
6.1	O
,	O
we	O
have	O
amassed	O
“	O
indirect	O
evidence	O
”	O
for	O
each	O
patient	O
,	O
using	O
both	O
bayesian	O
and	O
frequentist	O
methods	O
.	O
21.6	O
deconvolution	B
and	O
f	B
-modeling	I
empirical	O
bayes	O
applications	O
have	O
traditionally	O
been	O
dominated	O
by	O
f	O
-	O
modeling—not	O
the	O
g-modeling	B
approach	O
of	O
the	O
previous	O
sections—where	O
probability	O
models	B
for	O
the	O
marginal	O
density	B
f	O
.x/	O
,	O
usually	O
exponential	O
fam-	O
ilies	O
,	O
are	O
ﬁt	O
directly	O
to	O
the	O
observed	O
sample	B
x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn	O
.	O
we	O
have	O
seen	O
several	O
examples	O
:	O
robbins	O
’	O
estimator	B
in	O
table	O
6.1	O
(	O
particularly	O
the	O
bottom	O
line	O
)	O
,	O
locfdr	B
’	O
s	O
poisson	O
regression	B
estimates	O
in	O
figures	O
15.6	O
and	O
21.7	O
,	O
and	O
tweedie	O
’	O
s	O
estimate	B
in	O
figure	O
20.7.	O
both	O
the	O
advantages	O
and	O
the	O
disadvantages	O
of	O
f	B
-modeling	I
can	O
be	O
seen	O
in	O
the	O
inferential	O
diagram	O
of	O
figure	O
21.2.	O
for	O
f	B
-modeling	I
the	O
red	O
curve	O
now	O
can	O
represent	O
an	O
exponential	O
family	O
ff	O
.˛/g	O
,	O
whose	O
concave	O
log	O
likelihood	B
function	O
greatly	O
simpliﬁes	O
the	O
calculation	O
of	O
f	O
.o˛/	O
from	O
y=n	O
.	O
this	O
comes	O
at	O
a	O
price	O
:	O
the	O
deconvolution	B
step	O
,	O
from	O
f	O
.o˛/	O
to	O
a	O
prior	B
distribution	I
g.o˛/	O
,	O
is	O
problematical	O
,	O
as	O
discussed	O
below	O
.	O
this	O
is	O
only	O
a	O
problem	O
if	O
we	O
want	O
to	O
know	O
g.	O
the	O
traditional	O
applications	O
of	O
f	B
-modeling	I
apply	O
to	O
problems	O
where	O
the	O
desired	O
answer	O
can	O
be	O
phrased	O
directly	O
in	O
terms	O
of	O
f	O
.	O
this	O
was	O
the	O
case	O
for	O
robbins	O
’	O
formula	B
(	O
6.5	O
)	O
,	O
the	O
local	O
false-discovery	B
rate	I
(	O
15.38	O
)	O
,	O
and	O
tweedie	O
’	O
s	O
formula	B
(	O
20.37	O
)	O
.	O
nevertheless	O
,	O
f	B
-modeling	I
methodology	O
for	O
the	O
estimation	B
of	O
the	O
prior	B
g.	O
/	O
does	O
exist	O
,	O
an	O
elegant	O
example	O
being	O
the	O
fourier	O
method	B
described	O
next	O
.	O
a	O
function	B
f	O
.x/	O
and	O
its	O
fourier	O
transform	B
(	O
cid:30	O
)	O
.t	O
/	O
are	O
related	O
by	O
(	O
cid:30	O
)	O
.t	O
/	O
dz	O
1	O
(	O
cid:0	O
)	O
1	O
f	O
.x/ei	O
tx	O
dx	O
and	O
f	O
.x/	O
d	O
1	O
z	O
1	O
for	O
the	O
normal	B
case	O
where	O
xi	O
d	O
‚i	O
c	O
zi	O
with	O
zi	O
(	O
cid:24	O
)	O
transform	B
of	O
f	O
.x/	O
is	O
a	O
multiple	O
of	O
that	O
for	O
g.	O
/	O
,	O
(	O
cid:0	O
)	O
t	B
2=2	O
;	O
(	O
cid:30	O
)	O
f	O
.t	O
/	O
d	O
(	O
cid:30	O
)	O
g	O
.t	O
/e	O
(	O
21.60	O
)	O
(	O
cid:0	O
)	O
1	O
(	O
cid:30	O
)	O
.t	O
/e	O
2	O
(	O
cid:25	O
)	O
(	O
cid:0	O
)	O
i	O
tx	O
dt	O
:	O
(	O
21.59	O
)	O
n	O
.0	O
;	O
1/	O
,	O
the	O
fourier	O
so	O
,	O
on	O
the	O
transform	B
scale	O
,	O
estimating	O
g	O
from	O
f	O
amounts	O
to	O
removing	O
the	O
factor	B
exp.t	O
2=2/	O
.	O
the	O
fourier	O
method	B
begins	O
with	O
the	O
empirical	B
density	O
n	O
f	O
.x/	O
that	O
puts	O
probability	O
1=n	O
on	O
each	O
observed	O
value	O
xi	O
,	O
and	O
then	O
proceeds	O
in	O
three	O
steps	O
.	O
1	O
n	O
f	O
.x/	O
is	O
smoothed	O
using	O
the	O
“	O
sinc	O
”	O
kernel	O
,	O
	O
3	O
expressed	O
directly	O
as	O
a	O
kernel	O
estimate	B
,	O
nx	O
where	O
the	O
kernel	O
k	O
(	O
cid:21	O
)	O
.	O
(	O
cid:1	O
)	O
/	O
is	O
og.	O
/	O
d	O
1	O
id1	O
n	O
k	O
(	O
cid:21	O
)	O
.xi	O
(	O
cid:0	O
)	O
	O
/	O
dz	O
1	O
z	O
1=	O
(	O
cid:21	O
)	O
(	O
cid:0	O
)	O
1	O
k	O
(	O
cid:21	O
)	O
.x	O
(	O
cid:0	O
)	O
	O
/	O
n	O
f	O
.x/	O
dx	O
;	O
(	O
21.62	O
)	O
q	O
f	O
.x/	O
d	O
1	O
n	O
(	O
cid:21	O
)	O
21.6	O
deconvolution	B
and	O
f	B
-modeling	I
nx	O
id1	O
sinc	O
	O
	O
xi	O
(	O
cid:0	O
)	O
x	O
q	O
f	O
,	O
say	O
q	O
(	O
cid:30	O
)	O
.t	O
/	O
,	O
is	O
calculated	O
.	O
(	O
cid:21	O
)	O
;	O
441	O
:	O
(	O
21.61	O
)	O
sinc.x/	O
d	O
sin.x/	O
x	O
2	O
the	O
fourier	O
transform	B
of	O
3	O
finally	O
,	O
og.	O
/	O
is	O
taken	O
to	O
be	O
the	O
inverse	O
fourier	O
transform	B
of	O
q	O
(	O
cid:30	O
)	O
.t	O
/et	O
2=2	O
,	O
this	O
last	O
step	O
eliminating	O
the	O
unwanted	O
factor	B
e	O
a	O
pleasantly	O
surprising	O
aspect	O
of	O
the	O
fourier	O
method	B
is	O
that	O
og.	O
/	O
can	O
be	O
(	O
cid:0	O
)	O
t	B
2=2	O
in	O
(	O
21.60	O
)	O
.	O
0	O
(	O
cid:25	O
)	O
et	O
2=2	O
cos.tx/	O
dt	O
:	O
k	O
(	O
cid:21	O
)	O
.x/	O
d	O
1	O
large	O
values	O
of	O
(	O
cid:21	O
)	O
smooth	O
n	O
og.	O
/	O
at	O
the	O
expense	O
of	O
increased	O
bias	O
.	O
despite	O
its	O
compelling	O
rationale	O
,	O
there	O
are	O
two	O
drawbacks	O
to	O
the	O
fourier	O
method	B
.	O
first	O
of	O
all	O
,	O
it	O
applies	O
only	O
to	O
situations	O
xi	O
d	O
‚i	O
c	O
zi	O
where	O
xi	O
is	O
‚i	O
plus	O
iid	O
noise	O
.	O
more	O
seriously	O
,	O
the	O
bias/variance	O
trade-off	O
in	O
the	O
choice	O
of	O
(	O
cid:21	O
)	O
can	O
be	O
quite	O
unfavorable	O
.	O
f	O
.x/	O
more	O
in	O
(	O
21.61	O
)	O
,	O
reducing	O
the	O
variance	O
of	O
(	O
21.63	O
)	O
this	O
is	O
illustrated	O
in	O
figure	O
21.11	O
for	O
the	O
artiﬁcial	O
example	O
of	O
figure	O
21.1.	O
the	O
black	O
curve	O
is	O
the	O
standard	B
deviation	I
of	O
the	O
g-modeling	B
estimate	O
of	O
g.	O
/	O
for	O
	O
in	O
œ	O
(	O
cid:0	O
)	O
3	O
;	O
3	O
,	O
under	O
speciﬁcations	O
(	O
21.41	O
)	O
–	O
(	O
21.42	O
)	O
.	O
the	O
red	O
curve	O
graphs	O
the	O
standard	B
deviation	I
of	O
the	O
f	B
-modeling	I
estimate	O
(	O
21.62	O
)	O
,	O
with	O
(	O
cid:21	O
)	O
d	O
1=3	O
,	O
a	O
value	O
that	O
produced	O
roughly	O
the	O
same	O
amount	O
of	O
bias	O
as	O
the	O
g-	O
modeling	O
estimate	B
(	O
seen	O
in	O
figure	O
21.3	O
)	O
.	O
the	O
ratio	O
of	O
red	O
to	O
black	O
standard	B
deviations	I
averages	O
more	O
than	O
20	O
over	O
the	O
range	O
of	O
	O
.	O
this	O
comparison	O
is	O
at	O
least	O
partly	O
unfair	O
:	O
g-modeling	B
is	O
parametric	B
while	O
the	O
fourier	O
method	B
is	O
almost	O
nonparametric	B
in	O
its	O
assumptions	O
about	O
f	O
.x/	O
or	O
g.	O
/	O
.	O
it	O
can	O
be	O
greatly	O
improved	O
by	O
beginning	O
the	O
three-step	O
algorithm	B
with	O
a	O
parametric	B
estimate	I
o	O
f	O
.x/	O
rather	O
than	O
n	O
f	O
.x/	O
.	O
the	O
blue	O
dotted	O
curve	O
in	O
figure	O
21.11	O
does	O
this	O
with	O
o	O
f	O
.x/	O
a	O
poisson	O
regression	B
on	O
the	O
data	B
x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn	O
—as	O
in	O
figure	O
10.5	O
but	O
here	O
using	O
a	O
natural	O
spline	O
basis	O
ns	O
(	O
df=5	O
)	O
—giving	O
the	O
estimate	B
(	O
cid:0	O
)	O
1	O
k	O
(	O
cid:21	O
)	O
.x	O
(	O
cid:0	O
)	O
	O
/	O
o	O
f	O
.x/	O
dx	O
:	O
(	O
21.64	O
)	O
og.	O
/	O
dz	O
1	O
442	O
empirical	B
bayes	O
estimation	B
strategies	I
figure	O
21.11	O
standard	B
deviations	I
of	O
estimated	O
prior	B
density	O
og.	O
/	O
for	O
the	O
artiﬁcial	O
example	O
of	O
figure	O
21.1	O
,	O
based	O
on	O
n	O
d	O
1000	O
observations	O
xi	O
(	O
cid:24	O
)	O
n	O
.‚i	O
;	O
1/	O
;	O
black	O
curve	O
using	O
g-modeling	B
under	O
speciﬁcations	O
(	O
21.41	O
)	O
–	O
(	O
21.42	O
)	O
;	O
red	O
curve	O
nonparametric	B
f	O
-modeling	O
(	O
21.62	O
)	O
,	O
(	O
cid:21	O
)	O
d	O
1=3	O
;	O
blue	O
curve	O
parametric	B
f	O
-modeling	O
(	O
21.64	O
)	O
,	O
with	O
o	O
structure	O
matrix	B
having	O
ﬁve	O
degrees	O
of	O
freedom	O
.	O
f	O
.x/	O
estimated	O
from	O
poisson	O
regression	B
with	O
a	O
we	O
see	O
a	O
substantial	O
decrease	O
in	O
standard	B
deviation	I
,	O
though	O
still	O
not	O
attain-	O
ing	O
g-modeling	B
rates	O
.	O
as	O
commented	O
before	O
,	O
the	O
great	O
majority	O
of	O
empirical	B
bayes	O
applica-	O
tions	O
have	O
been	O
of	O
the	O
robbins/fdr/tweedie	O
variety	O
,	O
where	O
f	B
-modeling	I
is	O
the	O
natural	O
choice	O
.	O
g-modeling	B
comes	O
into	O
its	O
own	O
for	O
situations	O
like	O
the	O
nodes	B
data	O
analysis	B
of	O
figures	O
6.4	O
and	O
6.5	O
,	O
where	O
we	O
really	O
want	O
an	O
estimate	O
of	O
the	O
prior	B
g.	O
/	O
.	O
twenty-ﬁrst-century	O
science	O
is	O
producing	O
more	O
such	O
data	B
sets	O
,	O
an	O
impetus	O
for	O
the	O
further	O
development	O
of	O
g-modeling	B
strategies	O
.	O
table	O
21.3	O
concerns	O
the	O
g-modeling	B
estimation	O
of	O
ex	O
d	O
ef‚jx	O
d	O
xg	O
,	O
ex	O
dz	O
t	B
(	O
cid:30	O
)	O
z	O
t	B
g.	O
/f	O
.x/	O
d	O
g.	O
/f	O
.x/	O
d	O
(	O
21.65	O
)	O
for	O
the	O
artiﬁcial	O
example	O
,	O
under	O
the	O
same	O
speciﬁcations	O
as	O
in	O
figure	O
21.11.	O
samples	O
of	O
size	O
n	O
d	O
1000	O
of	O
xi	O
(	O
cid:24	O
)	O
n	O
.‚i	O
;	O
1/	O
were	O
drawn	O
from	O
model	O
(	O
21.41	O
)	O
–	O
(	O
21.42	O
)	O
,	O
yielding	O
mle	O
og.	O
/	O
and	O
estimates	O
oex	O
for	O
x	O
between	O
(	O
cid:0	O
)	O
4	O
−3−2−101230.000.010.020.030.040.05qsd	O
g^	O
(	O
q	O
)	O
g−modelparametricf−modelnon−parametricf−model	O
21.6	O
deconvolution	B
and	O
f	B
-modeling	I
443	O
table	O
21.3	O
standard	B
deviation	I
of	O
oef‚jxg	O
computed	O
from	O
parametric	O
bootstrap	O
simulations	O
of	O
og.	O
/	O
.	O
the	O
g-modeling	B
is	O
as	O
in	O
figure	O
21.11	O
,	O
with	O
n	O
d	O
1000	O
observations	O
xi	O
(	O
cid:24	O
)	O
each	O
simulation	O
.	O
the	O
column	O
“	O
info	O
”	O
is	O
the	O
implied	O
empirical	B
bayes	O
information	B
for	O
estimating	O
ef‚jxg	O
obtained	O
from	O
one	O
“	O
other	O
”	O
observation	O
xi	O
.	O
n	O
.‚i	O
;	O
1/	O
from	O
the	O
artiﬁcial	O
example	O
for	O
x	O
ef‚jxg	O
(	O
cid:0	O
)	O
2:00	O
(	O
cid:0	O
)	O
1:06	O
(	O
cid:0	O
)	O
:44	O
(	O
cid:0	O
)	O
:13	O
.13	O
.44	O
1.06	O
2.00	O
(	O
cid:0	O
)	O
3:5	O
(	O
cid:0	O
)	O
2:5	O
(	O
cid:0	O
)	O
1:5	O
(	O
cid:0	O
)	O
:5	O
.5	O
1.5	O
2.5	O
3.5	O
sd	O
.	O
oe/	O
.10	O
.10	O
.05	O
.03	O
.04	O
.05	O
.10	O
.16	O
info	O
.11	O
.11	O
.47	O
.89	O
.80	O
.44	O
.10	O
.04	O
and	O
4.	O
one	O
thousand	O
such	O
estimates	O
oex	O
were	O
generated	O
,	O
averaging	B
almost	O
exactly	O
ex	O
,	O
with	O
standard	B
deviations	I
as	O
shown	O
.	O
accuracy	O
is	O
reasonably	O
good	O
,	O
the	O
coefﬁcient	O
of	O
variation	O
sd	O
.	O
oex/=ex	O
being	O
about	O
0.05	O
for	O
large	O
values	O
of	O
jxj	O
.	O
(	O
estimate	B
(	O
21.65	O
)	O
is	O
a	O
favorable	O
case	O
:	O
results	O
are	O
worse	O
for	O
other	O
conditional	O
estimates	O
such	O
as	O
ef‚2jx	O
d	O
xg	O
.	O
)	O
theorem	B
21.4	O
(	O
page	O
429	O
)	O
implies	O
that	O
,	O
for	O
large	O
values	O
of	O
the	O
sample	B
size	I
n	O
,	O
the	O
variance	O
of	O
oex	O
decreases	O
as	O
1=n	O
,	O
say	O
4	O
(	O
21.66	O
)	O
var	O
n	O
oex	O
.	O
(	O
cid:16	O
)	O
n	O
(	O
cid:1	O
)	O
var	O
ix	O
d	O
1	O
o	O
:	O
d	O
cx=n	O
:	O
n	O
oex	O
o	O
by	O
analogy	O
with	O
the	O
fisher	O
information	B
bound	O
(	O
5.27	O
)	O
,	O
we	O
can	O
deﬁne	O
the	O
empirical	B
bayes	O
information	B
for	O
estimating	O
ex	O
in	O
one	O
observation	O
to	O
be	O
;	O
(	O
21.67	O
)	O
(	O
cid:0	O
)	O
1	O
x	O
=n	O
.	O
so	O
that	O
varf	O
oexg	O
:	O
d	O
i	O
empirical	B
bayes	O
inference	B
leads	O
us	O
directly	O
into	O
the	O
world	O
of	O
indirect	O
evidence	O
,	O
learning	O
from	O
the	O
experience	O
of	O
others	O
as	O
in	O
sections	O
6.4	O
and	O
7.4.	O
so	O
,	O
if	O
xi	O
d	O
2:5	O
,	O
each	O
“	O
other	O
”	O
observation	O
xj	O
provides	O
0.10	O
units	O
of	O
information	B
for	O
learning	O
ef‚jxi	O
d	O
2:5g	O
(	O
compared	O
with	O
the	O
usual	O
fisher	O
d	O
1	O
for	O
the	O
direct	O
estimation	B
of	O
‚i	O
from	O
xi	O
)	O
.	O
this	O
information	B
value	O
i	O
is	O
a	O
favorable	O
case	O
,	O
as	O
mentioned	O
,	O
and	O
ix	O
is	O
often	O
much	O
smaller	O
.	O
the	O
main	O
point	O
,	O
perhaps	O
,	O
is	O
that	O
assuming	O
a	O
bayes	O
prior	B
is	O
not	O
a	O
casual	O
matter	O
,	O
and	O
444	O
empirical	B
bayes	O
estimation	B
strategies	I
can	O
amount	O
to	O
the	O
assumption	O
of	O
an	O
enormous	O
amount	O
of	O
relevant	O
other	O
information	B
.	O
21.7	O
notes	O
and	O
details	O
empirical	B
bayes	O
and	O
james–stein	O
estimation	B
,	O
chapters	O
6	O
and	O
7	O
,	O
exploded	O
onto	O
the	O
statistics	B
scene	O
almost	O
simultaneously	O
in	O
the	O
1950s	O
.	O
they	O
repre-	O
sented	O
a	O
genuinely	O
new	O
branch	O
of	O
statistical	O
inference	B
,	O
unlike	O
the	O
computer-	O
based	O
extensions	O
of	O
classical	O
methodology	O
reviewed	O
in	O
previous	O
chapters	O
.	O
their	O
development	O
as	O
practical	O
tools	O
has	O
been	O
comparatively	O
slow	O
.	O
the	O
pace	O
has	O
quickened	O
in	O
the	O
twenty-ﬁrst	O
century	O
,	O
with	O
false-discovery	O
rates	O
,	O
chapter	O
15	O
,	O
as	O
a	O
major	O
step	O
forward	O
.	O
a	O
practical	O
empirical	B
bayes	O
method-	O
ology	O
for	O
use	O
beyond	O
traditional	O
f	B
-modeling	I
venues	O
such	O
as	O
fdr	O
is	O
the	O
goal	O
of	O
the	O
g-modeling	B
approach	O
.	O
1	O
[	O
p.	O
428	O
]	O
lemmas	O
21.1	O
and	O
21.2.	O
the	O
derivations	O
of	O
lemmas	O
21.1	O
and	O
21.2	O
are	O
straightforward	O
but	O
somewhat	O
involved	O
exercises	O
in	O
differential	O
calcu-	O
lus	O
,	O
carried	O
out	O
in	O
remark	O
b	O
of	O
efron	O
(	O
2016	O
)	O
.	O
here	O
we	O
will	O
present	O
just	O
p	O
fk.˛/	O
d	O
a	O
sample	B
of	O
the	O
calculations	O
.	O
from	O
(	O
21.18	O
)	O
,	O
the	O
gradient	O
vector	B
.	O
@	O
fk.˛/=	O
@	O
˛l	O
/	O
with	O
respect	O
to	O
˛	O
is	O
p	O
fk.˛/	O
d	O
pg.˛/	O
0	O
where	O
pg.˛/	O
is	O
the	O
m	O
(	O
cid:2	O
)	O
p	O
derivative	O
matrix	B
pk	O
;	O
pg.˛/	O
d	O
.	O
@	O
gj	O
.˛/=	O
@	O
˛l	O
/	O
d	O
dq	O
;	O
(	O
21.68	O
)	O
(	O
21.69	O
)	O
with	O
d	O
as	O
in	O
(	O
21.36	O
)	O
,	O
the	O
last	O
equality	O
following	O
,	O
after	O
some	O
work	O
,	O
by	O
dif-	O
ferentiation	O
of	O
log	O
g.˛/	O
d	O
q˛	O
(	O
cid:0	O
)	O
(	O
cid:30	O
)	O
.˛/	O
.	O
let	O
lk	O
d	O
log	O
fk	O
(	O
now	O
suppressing	O
˛	O
from	O
the	O
notation	O
)	O
.	O
the	O
gradient	O
with	O
respect	O
to	O
˛	O
of	O
lk	O
is	O
then	O
p	O
lk	O
d	O
p	O
fk=fk	O
d	O
q	O
0	O
dpk=fk	O
:	O
(	O
21.70	O
)	O
the	O
vector	B
dpk=fk	O
has	O
components	O
.gj	O
pkj	O
(	O
cid:0	O
)	O
gj	O
fk/=fk	O
d	O
wkj	O
(	O
21.71	O
)	O
pk	O
d	O
fk	O
.	O
this	O
gives	O
p	O
0pn	O
0	O
(	O
21.27	O
)	O
,	O
using	O
g	O
the	O
independent	O
score	O
functions	O
p	O
score	O
p	O
ly	O
.˛/	O
d	O
q	O
o	O
d	O
pm.o˛/	O
2	O
[	O
p.	O
428	O
]	O
lemma	O
2.	O
the	O
penalized	O
mle	O
o˛	O
satisﬁes	O
1	O
ykwk.˛/	O
,	O
which	O
is	O
lemma	O
21.1.	O
:	O
d	O
pm.˛0/	O
c	O
rm.˛0/.o˛	O
(	O
cid:0	O
)	O
˛0/	O
;	O
lk	O
d	O
q	O
0	O
wk.˛/	O
(	O
21.28	O
)	O
.	O
adding	O
up	O
lk	O
over	O
the	O
full	B
sample	O
yields	O
the	O
overall	O
(	O
21.72	O
)	O
21.7	O
notes	O
and	O
details	O
(	O
cid:0	O
)	O
1	O
pm.˛0/	O
(	O
cid:16	O
)	O
(	O
cid:0	O
)	O
r	O
:	O
d	O
.	O
(	O
cid:0	O
)	O
rm.˛0//	O
	O
(	O
cid:0	O
)	O
1	O
(	O
cid:16	O
)	O
p	O
where	O
˛0	O
is	O
the	O
true	O
value	O
of	O
˛	O
,	O
or	O
o˛	O
(	O
cid:0	O
)	O
˛0	O
standard	O
mle	O
theory	B
shows	O
that	O
the	O
random	O
variable	O
p	O
and	O
covariance	O
fisher	O
information	B
matrix	O
i.˛0/	O
,	O
while	O
(	O
cid:0	O
)	O
r	O
ically	O
approximates	O
i.˛0/	O
.	O
substituting	O
in	O
(	O
21.73	O
)	O
,	O
(	O
cid:0	O
)	O
1z	O
;	O
:	O
d	O
.i.˛0/	O
c	O
rs.˛0//	O
ly	O
.˛0/	O
c	O
rs.˛0/	O
o˛	O
(	O
cid:0	O
)	O
˛0	O
445	O
	O
ly	O
.˛0/	O
(	O
cid:0	O
)	O
ps.˛0/	O
:	O
(	O
21.73	O
)	O
ly	O
.˛0/	O
has	O
mean	O
0	O
ly	O
.˛0/	O
asymptot-	O
(	O
21.74	O
)	O
where	O
z	O
has	O
mean	O
(	O
cid:0	O
)	O
ps.˛0/	O
and	O
covariance	O
i.˛0/	O
.	O
this	O
gives	O
bias.o˛/	O
and	O
var.o˛/	O
as	O
in	O
lemma	O
2.	O
note	O
that	O
the	O
bias	O
is	O
with	O
respect	O
to	O
a	O
true	O
parametric	B
model	O
(	O
21.11	O
)	O
,	O
and	O
is	O
a	O
consequence	O
of	O
the	O
penalization	O
.	O
3	O
[	O
p.	O
440	O
]	O
the	O
sinc	O
kernel	O
.	O
the	O
fourier	O
transform	B
(	O
cid:30	O
)	O
s.t	O
/	O
of	O
the	O
scaled	O
sinc	O
function	B
s.x/	O
d	O
sin.x=	O
(	O
cid:21	O
)	O
/=	O
.	O
(	O
cid:25	O
)	O
x/	O
is	O
the	O
indicator	O
of	O
the	O
interval	B
œ	O
(	O
cid:0	O
)	O
1=	O
(	O
cid:21	O
)	O
;	O
1=	O
(	O
cid:21	O
)	O
	O
,	O
while	O
that	O
of	O
1	O
exp.i	O
txj	O
/	O
.	O
formula	B
(	O
21.61	O
)	O
is	O
the	O
convo-	O
lution	O
n	O
f	O
(	O
cid:3	O
)	O
s	O
,	O
so	O
q	O
f	O
.x/	O
is	O
.1=n	O
/pn	O
24	O
1	O
nx	O
f	O
has	O
the	O
product	O
transform	B
(	O
cid:30	O
)	O
qf	O
.t	O
/	O
d	O
35	O
iœ	O
(	O
cid:0	O
)	O
1=	O
(	O
cid:21	O
)	O
;	O
1=	O
(	O
cid:21	O
)	O
.t	O
/	O
:	O
(	O
21.75	O
)	O
ei	O
txj	O
n	O
n	O
jd1	O
the	O
effect	O
of	O
the	O
sinc	O
convolution	O
is	O
to	O
censor	O
the	O
high-frequency	O
(	O
large	O
t	B
)	O
n	O
f	O
or	O
(	O
cid:30	O
)	O
nf	O
.	O
larger	O
(	O
cid:21	O
)	O
yields	O
more	O
censoring	O
.	O
formula	B
(	O
21.63	O
)	O
components	O
of	O
has	O
upper	O
limits	O
1=	O
(	O
cid:21	O
)	O
because	O
of	O
(	O
cid:30	O
)	O
s.t	O
/	O
.	O
all	O
of	O
this	O
is	O
due	O
to	O
stefanski	O
and	O
carroll	O
(	O
1990	O
)	O
.	O
smoothers	O
other	O
than	O
the	O
sinc	O
kernel	O
have	O
been	O
suggested	O
in	O
the	O
literature	O
,	O
but	O
without	O
substantial	O
improvements	O
on	O
deconvolution	B
performance	O
.	O
4	O
[	O
p.	O
443	O
]	O
conditional	O
expectation	O
(	O
21.65	O
)	O
.	O
efron	O
(	O
2014b	O
)	O
considers	O
estimat-	O
ing	O
ef‚2jx	O
d	O
xg	O
and	O
other	O
such	O
conditional	O
expectations	O
,	O
both	O
for	O
f	O
-	O
modeling	O
and	O
for	O
g-modeling	B
.	O
ef‚jx	O
d	O
xg	O
is	O
by	O
far	O
the	O
easiest	O
case	O
,	O
as	O
might	O
be	O
expected	O
from	O
the	O
simple	O
form	B
of	O
tweedie	O
’	O
s	O
estimate	B
(	O
20.37	O
)	O
.	O
epilogue	O
something	O
important	O
changed	O
in	O
the	O
world	O
of	O
statistics	B
in	O
the	O
new	O
millen-	O
nium	O
.	O
twentieth-century	O
statistics	B
,	O
even	O
after	O
the	O
heated	O
expansion	O
of	O
its	O
late	O
period	O
,	O
could	O
still	O
be	O
contained	O
within	O
the	O
classic	O
bayesian–frequentist–	O
fisherian	O
inferential	O
triangle	O
(	O
figure	O
14.1	O
)	O
.	O
this	O
is	O
not	O
so	O
in	O
the	O
twenty-ﬁrst	O
century	O
.	O
some	O
of	O
the	O
topics	O
discussed	O
in	O
part	O
iii—false-discovery	O
rates	O
,	O
post-selection	O
inference	B
,	O
empirical	B
bayes	O
modeling	O
,	O
the	O
lasso—ﬁt	O
within	O
the	O
triangle	O
but	O
others	O
seem	O
to	O
have	O
escaped	O
,	O
heading	O
south	O
from	O
the	O
fre-	O
quentist	O
corner	O
,	O
perhaps	O
in	O
the	O
direction	O
of	O
computer	O
science	O
.	O
the	O
escapees	O
were	O
the	O
large-scale	O
prediction	O
algorithms	O
of	O
chapters	O
17–	O
19	O
:	O
neural	O
nets	O
,	O
deep	O
learning	O
,	O
boosting	O
,	O
random	O
forests	O
,	O
and	O
support-vector	O
machines	O
.	O
notably	O
missing	O
from	O
their	O
development	O
were	O
parametric	B
prob-	O
ability	O
models	B
,	O
the	O
building	O
blocks	O
of	O
classical	O
inference	B
.	O
prediction	O
algo-	O
rithms	O
are	O
the	O
media	O
stars	O
of	O
the	O
big-data	O
era	O
.	O
it	O
is	O
worth	O
asking	O
why	O
they	O
have	O
taken	O
center	O
stage	O
and	O
what	O
it	O
means	O
for	O
the	O
future	O
of	O
the	O
statistics	B
discipline	O
.	O
the	O
why	O
is	O
easy	O
enough	O
:	O
prediction	O
is	O
commercially	O
valuable	O
.	O
modern	O
equipment	O
has	O
enabled	O
the	O
collection	O
of	O
mountainous	O
data	B
troves	O
,	O
which	O
the	O
“	O
data	B
miners	O
”	O
can	O
then	O
burrow	O
into	O
,	O
extracting	O
valuable	O
information	B
.	O
moreover	O
,	O
prediction	O
is	O
the	O
simplest	O
use	O
of	O
regression	B
theory	O
(	O
section	O
8.4	O
)	O
.	O
it	O
can	O
be	O
carried	O
out	O
successfully	O
without	O
probability	O
models	B
,	O
perhaps	O
with	O
the	O
assistance	O
of	O
nonparametric	B
analysis	O
tools	O
such	O
as	O
cross-validation	O
,	O
per-	O
mutations	O
,	O
and	O
the	O
bootstrap	O
.	O
a	O
great	O
amount	O
of	O
ingenuity	O
and	O
experimentation	O
has	O
gone	O
into	O
the	O
development	O
of	O
modern	O
prediction	O
algorithms	O
,	O
with	O
statisticians	O
playing	O
an	O
important	O
but	O
not	O
dominant	O
role.1	O
there	O
is	O
no	O
shortage	O
of	O
impressive	O
success	O
stories	O
.	O
in	O
the	O
absence	O
of	O
optimality	O
criteria	B
,	O
either	O
frequentist	O
or	O
bayesian	O
,	O
the	O
prediction	O
community	O
grades	O
algorithmic	O
excellence	O
on	O
per-	O
1	O
all	O
papers	O
mentioned	O
in	O
this	O
section	O
have	O
their	O
complete	O
references	O
in	O
the	O
bibliography	O
.	O
footnotes	O
will	O
identify	O
papers	O
not	O
fully	O
speciﬁed	O
in	O
the	O
text	O
.	O
446	O
epilogue	O
447	O
formance	O
within	O
a	O
catalog	O
of	O
often-visited	O
examples	O
such	O
as	O
the	O
spam	B
and	O
digits	O
data	B
sets	O
of	O
chapters	O
17	O
and	O
18.2	O
meanwhile	O
,	O
“	O
traditional	O
statistics	B
”	O
—probability	O
models	B
,	O
optimality	O
criteria	B
,	O
bayes	O
priors	B
,	O
asymptotics—has	O
continued	O
successfully	O
along	O
on	O
a	O
parallel	O
track	O
.	O
pessimistically	O
or	O
opti-	O
mistically	O
,	O
one	O
can	O
consider	O
this	O
as	O
a	O
bipolar	O
disorder	O
of	O
the	O
ﬁeld	O
or	O
as	O
a	O
healthy	O
duality	O
that	O
is	O
bound	B
to	O
improve	O
both	O
branches	O
.	O
there	O
are	O
histori-	O
cal	O
and	O
intellectual	O
arguments	O
favoring	O
the	O
optimists	O
’	O
side	O
of	O
the	O
story	O
.	O
the	O
ﬁrst	O
thing	O
to	O
say	O
is	O
that	O
the	O
current	O
situation	O
is	O
not	O
entirely	O
unprece-	O
dented	O
.	O
by	O
the	O
end	O
of	O
the	O
nineteenth	O
century	O
there	O
was	O
available	O
an	O
im-	O
pressive	O
inventory	O
of	O
statistical	O
methods—bayes	O
’	O
theorem	B
,	O
least	B
squares	I
,	O
correlation	O
,	O
regression	B
,	O
the	O
multivariate	B
normal	O
distribution—but	O
these	O
ex-	O
isted	O
more	O
as	O
individual	O
algorithms	O
than	O
as	O
a	O
uniﬁed	O
discipline	O
.	O
statistics	B
as	O
a	O
distinct	O
intellectual	O
enterprise	O
was	O
not	O
yet	O
well-formed	O
.	O
a	O
small	O
but	O
crucial	O
step	O
forward	O
was	O
taken	O
in	O
1914	O
when	O
the	O
astrophysi-	O
cist	O
arthur	O
eddington3	O
claimed	O
that	O
mean	O
absolute	O
deviation	O
was	O
superior	O
to	O
the	O
familiar	O
root	O
mean	O
square	O
estimate	B
for	O
the	O
standard	B
deviation	I
from	O
a	O
normal	B
sample	O
.	O
fisher	O
in	O
1919	O
showed	O
that	O
this	O
was	O
wrong	O
,	O
and	O
moreover	O
,	O
in	O
a	O
clear	O
mathematical	O
sense	O
,	O
the	O
root	O
mean	O
square	O
was	O
the	O
best	O
possible	O
estimate	B
.	O
eddington	O
conceded	O
the	O
point	O
while	O
fisher	O
went	O
on	O
to	O
develop	O
the	O
theory	B
of	O
sufﬁciency	O
and	O
optimal	O
estimation.4	O
“	O
optimal	O
”	O
is	O
the	O
key	O
word	O
here	O
.	O
before	O
fisher	O
,	O
statisticians	O
didn	O
’	O
t	B
really	O
understand	O
estimation	B
.	O
the	O
same	O
can	O
be	O
said	O
now	O
about	O
prediction	O
.	O
despite	O
their	O
impressive	O
performance	O
on	O
a	O
raft	O
of	O
test	O
problems	O
,	O
it	O
might	O
still	O
be	O
possible	O
to	O
do	O
much	O
better	O
than	O
neural	O
nets	O
,	O
deep	O
learning	O
,	O
random	O
forests	O
,	O
and	O
boosting—or	O
perhaps	O
they	O
are	O
coming	O
close	O
to	O
some	O
as-yet	O
unknown	O
theoretical	O
minimum	O
.	O
it	O
is	O
the	O
job	O
of	O
statistical	O
inference	B
to	O
connect	O
“	O
dangling	O
algorithms	O
”	O
to	O
the	O
central	O
core	O
of	O
well-understood	O
methodology	O
.	O
the	O
connection	O
process	O
is	O
already	O
underway	O
.	O
section	O
17.4	O
showed	O
how	O
adaboost	O
,	O
the	O
original	O
machine	O
learning	O
algorithm	O
,	O
could	O
be	O
restated	O
as	O
a	O
close	O
cousin	O
of	O
logis-	O
tic	O
regression	B
.	O
purely	O
empirical	B
approaches	O
like	O
the	O
common	O
task	O
frame-	O
work	O
are	O
ultimately	O
unsatisfying	O
without	O
some	O
form	B
of	O
principled	O
justi-	O
ﬁcation	O
.	O
our	O
optimistic	O
scenario	O
has	O
the	O
big-data/data-science	O
prediction	O
world	O
rejoining	O
the	O
mainstream	O
of	O
statistical	O
inference	B
,	O
to	O
the	O
beneﬁt	O
of	O
both	O
branches	O
.	O
2	O
this	O
empirical	B
approach	O
to	O
optimality	O
is	O
sometimes	O
codiﬁed	O
as	O
the	O
common	O
task	O
framework	O
(	O
liberman	O
,	O
2015	O
and	O
donoho	O
,	O
2015	O
)	O
.	O
3	O
eddington	O
became	O
world-famous	O
for	O
his	O
1919	O
empirical	B
veriﬁcation	O
of	O
einstein	O
’	O
s	O
relativity	O
theory	B
.	O
4	O
see	O
stigler	O
(	O
2006	O
)	O
for	O
the	O
full	B
story	O
.	O
448	O
epilogue	O
development	O
of	O
the	O
statistics	B
discipline	O
since	O
the	O
end	O
of	O
the	O
nine-	O
teenth	O
century	O
,	O
as	O
discussed	O
in	O
the	O
text	O
.	O
whether	O
or	O
not	O
we	O
can	O
predict	O
the	O
future	O
of	O
statistics	B
,	O
we	O
can	O
at	O
least	O
examine	O
the	O
past	O
to	O
see	O
how	O
we	O
’	O
ve	O
gotten	O
where	O
we	O
are	O
.	O
the	O
next	O
ﬁgure	O
does	O
so	O
in	O
terms	O
of	O
a	O
new	O
triangle	O
diagram	O
,	O
this	O
time	O
with	O
the	O
poles	O
la-	O
beled	O
applications	O
,	O
mathematics	O
,	O
and	O
computation	O
.	O
“	O
mathematics	O
”	O
here	O
is	O
shorthand	O
for	O
the	O
mathematical/logical	O
justiﬁcation	O
of	O
statistical	O
meth-	O
ods	O
.	O
“	O
computation	O
”	O
stands	O
for	O
the	O
empirical/numerical	O
approach	O
.	O
statistics	B
is	O
a	O
branch	O
of	O
applied	O
mathematics	O
,	O
and	O
is	O
ultimately	O
judged	O
by	O
how	O
well	O
it	O
serves	O
the	O
world	O
of	O
applications	O
.	O
mathematical	O
logic	O
,	O
`a	O
la	O
fisher	O
,	O
has	O
been	O
the	O
traditional	O
vehicle	O
for	O
the	O
development	O
and	O
under-	O
standing	O
of	O
statistical	O
methods	O
.	O
computation	O
,	O
slow	O
and	O
difﬁcult	O
before	O
the	O
1950s	O
,	O
was	O
only	O
a	O
bottleneck	O
,	O
but	O
now	O
has	O
emerged	O
as	O
a	O
competitor	O
to	O
(	O
or	O
perhaps	O
a	O
seven-league	O
boots	O
enabler	O
of	O
)	O
mathematical	O
analysis	B
.	O
at	O
any	O
one	O
time	O
the	O
discipline	O
’	O
s	O
energy	O
and	O
excitement	O
is	O
directed	O
unequally	O
to-	O
ward	O
the	O
three	O
poles	O
.	O
the	O
ﬁgure	O
attempts	O
,	O
in	O
admittedly	O
crude	O
fashion	O
,	O
to	O
track	O
the	O
changes	O
in	O
direction	O
over	O
the	O
past	O
100c	O
years	O
.	O
lllmathematicscomputationapplicationsl19th	O
century1900190819251933193719501962197219791995200020012016b2016a	O
epilogue	O
449	O
the	O
tour	O
begins	O
at	O
the	O
end	O
of	O
the	O
nineteenth	O
century	O
.	O
mathematicians	O
of	O
the	O
caliber	O
of	O
gauss	O
and	O
laplace	O
had	O
contributed	O
to	O
the	O
available	O
method-	O
ology	O
,	O
but	O
the	O
subsequent	O
development	O
was	O
almost	O
entirely	O
applications-	O
driven	O
.	O
quetelet5	O
was	O
especially	O
inﬂuential	O
,	O
applying	O
the	O
gauss–laplace	O
formulation	O
to	O
census	O
data	B
and	O
his	O
“	O
average	O
man.	O
”	O
a	O
modern	O
reader	O
will	O
search	O
almost	O
in	O
vain	O
for	O
any	O
mathematical	O
symbology	O
in	O
nineteenth-century	O
statistics	B
journals	O
.	O
1900	O
karl	O
pearson	O
’	O
s	O
chi-square	O
paper	O
was	O
a	O
bold	O
step	O
into	O
the	O
new	O
century	O
,	O
ap-	O
plying	O
a	O
new	O
mathematical	O
tool	O
,	O
matrix	B
theory	O
,	O
in	O
the	O
service	O
of	O
statisti-	O
cal	O
methodology	O
.	O
he	O
and	O
weldon	O
went	O
on	O
to	O
found	O
biometrika	O
in	O
1901	O
,	O
the	O
ﬁrst	O
recognizably	O
modern	O
statistics	B
journal	O
.	O
pearson	O
’	O
s	O
paper	O
,	O
and	O
bio-	O
metrika	O
,	O
launched	O
the	O
statistics	B
discipline	O
on	O
a	O
ﬁfty-year	O
march	O
toward	O
the	O
mathematics	O
pole	O
of	O
the	O
triangle	O
.	O
student	O
’	O
s	O
t	B
statistic	O
was	O
a	O
crucial	O
ﬁrst	O
result	O
in	O
small-sample	O
“	O
exact	O
”	O
infer-	O
ence	O
,	O
and	O
a	O
major	O
inﬂuence	O
on	O
fisher	O
’	O
s	O
thinking	O
.	O
1908	O
1925	O
fisher	O
’	O
s	O
great	O
estimation	B
paper—a	O
more	O
coherent	O
version	O
of	O
its	O
1922	O
pre-	O
decessor	O
.	O
it	O
introduced	O
a	O
host	O
of	O
fundamental	O
ideas	O
,	O
including	O
sufﬁciency	O
,	O
efﬁciency	O
,	O
fisher	O
information	B
,	O
maximum	B
likelihood	I
theory	O
,	O
and	O
the	O
notion	O
of	O
optimal	O
estimation	B
.	O
optimality	O
is	O
a	O
mark	O
of	O
maturity	O
in	O
mathematics	O
,	O
making	O
1925	O
the	O
year	O
statistical	O
inference	B
went	O
from	O
a	O
collection	O
of	O
inge-	O
nious	O
techniques	O
to	O
a	O
coherent	O
discipline	O
.	O
1933	O
this	O
represents	O
neyman	O
and	O
pearson	O
’	O
s	O
paper	O
on	O
optimal	O
hypothesis	O
test-	O
ing	O
.	O
a	O
logical	O
completion	O
of	O
fisher	O
’	O
s	O
program	O
,	O
it	O
nevertheless	O
aroused	O
his	O
strong	O
antipathy	O
.	O
this	O
was	O
partly	O
personal	O
,	O
but	O
also	O
reﬂected	O
fisher	O
’	O
s	O
con-	O
cern	O
that	O
mathematization	O
was	O
squeezing	O
intuitive	O
correctness	O
out	O
of	O
statis-	O
tical	O
thinking	O
(	O
section	O
4.2	O
)	O
.	O
1937	O
neyman	O
’	O
s	O
seminal	O
paper	O
on	O
conﬁdence	B
intervals	I
.	O
his	O
sophisticated	O
mathe-	O
matical	O
treatment	O
of	O
statistical	O
inference	B
was	O
a	O
harbinger	O
of	O
decision	O
theory	B
.	O
5	O
adolphe	O
quetelet	O
was	O
a	O
tireless	O
organizer	O
,	O
helping	O
found	O
the	O
royal	O
statistical	O
society	O
in	O
1834	O
,	O
with	O
the	O
american	O
statistical	O
association	O
following	O
in	O
1839	O
.	O
450	O
epilogue	O
1950	O
the	O
publication	O
of	O
wald	O
’	O
s	O
statistical	O
decision	O
functions	O
.	O
decision	O
theory	B
completed	O
the	O
full	B
mathematization	O
of	O
statistical	O
inference	B
.	O
this	O
date	O
can	O
also	O
stand	O
for	O
savage	O
’	O
s	O
and	O
de	O
finetti	O
’	O
s	O
decision-theoretic	O
formulation	O
of	O
bayesian	O
inference	B
.	O
we	O
are	O
as	O
far	O
as	O
possible	O
from	O
the	O
applications	O
corner	O
of	O
the	O
triangle	O
now	O
,	O
and	O
it	O
is	O
fair	O
to	O
describe	O
the	O
1950s	O
as	O
a	O
nadir	O
of	O
the	O
inﬂuence	O
of	O
the	O
statistics	B
discipline	O
on	O
scientiﬁc	O
applications	O
.	O
1962	O
the	O
arrival	O
of	O
electronic	O
computation	O
in	O
the	O
mid	O
1950s	O
began	O
the	O
process	O
of	O
stirring	O
statistics	B
out	O
of	O
its	O
inward-gazing	O
preoccupation	O
with	O
mathe-	O
matical	O
structure	O
.	O
tukey	O
’	O
s	O
paper	O
“	O
the	O
future	O
of	O
data	B
analysis	O
”	O
argued	O
for	O
a	O
more	O
application-	O
and	O
computation-oriented	O
discipline	O
.	O
mosteller	O
and	O
tukey	O
later	O
suggested	O
changing	O
the	O
ﬁeld	O
’	O
s	O
name	O
to	O
data	B
analysis	O
,	O
a	O
pre-	O
scient	O
hint	O
of	O
today	O
’	O
s	O
data	B
science	O
.	O
1972	O
cox	O
’	O
s	O
proportional	O
hazards	O
paper	O
.	O
immensely	O
useful	O
in	O
its	O
own	O
right	O
,	O
it	O
sig-	O
naled	O
a	O
growing	O
interest	O
in	O
biostatistical	O
applications	O
and	O
particularly	O
sur-	O
vival	O
analysis	B
,	O
which	O
was	O
to	O
assert	O
its	O
scientiﬁc	O
importance	O
in	O
the	O
analysis	B
of	O
aids	O
epidemic	O
data	B
.	O
the	O
bootstrap	O
,	O
and	O
later	O
the	O
widespread	O
use	O
of	O
mcmc	O
:	O
electronic	O
compu-	O
tation	O
used	O
for	O
the	O
extension	O
of	O
classic	O
statistical	O
inference	B
.	O
1979	O
1995	O
this	O
stands	O
for	O
false-discovery	O
rates	O
and	O
,	O
a	O
year	O
later	O
,	O
the	O
lasso.6	O
both	O
are	O
computer-intensive	O
algorithms	O
,	O
ﬁrmly	O
rooted	O
in	O
the	O
ethos	O
of	O
statistical	O
in-	O
ference	O
.	O
they	O
lead	O
,	O
however	O
,	O
in	O
different	O
directions	O
,	O
as	O
indicated	O
by	O
the	O
split	O
in	O
the	O
diagram	O
.	O
microarray	O
technology	O
inspires	O
enormous	O
interest	O
in	O
large-scale	O
inference	O
,	O
both	O
in	O
theory	B
and	O
as	O
applied	O
to	O
the	O
analysis	B
of	O
microbiological	O
data	B
.	O
2000	O
6	O
benjamini	O
and	O
hochberg	O
(	O
1995	O
)	O
and	O
tibshirani	O
(	O
1996	O
)	O
.	O
epilogue	O
2001	O
451	O
random	O
forests	O
;	O
it	O
joins	O
boosting7	O
and	O
the	O
resurgence	O
of	O
neural	O
nets	O
in	O
the	O
ranks	O
of	O
machine	O
learning	O
prediction	O
algorithms	O
.	O
2016a	O
data	B
science	O
:	O
a	O
more	O
popular	O
successor	O
to	O
tukey	O
and	O
mosteller	O
’	O
s	O
“	O
data	B
analysis	O
,	O
”	O
at	O
one	O
extreme	O
it	O
seems	O
to	O
represent	O
a	O
statistics	B
discipline	O
with-	O
out	O
parametric	O
probability	O
models	B
or	O
formal	O
inference	B
.	O
the	O
data	B
science	O
association	O
deﬁnes	O
a	O
practitioner	O
as	O
one	O
who	O
“	O
.	O
.	O
.	O
uses	O
scientiﬁc	O
methods	O
to	O
liberate	O
and	O
create	O
meaning	O
from	O
raw	O
data.	O
”	O
in	O
practice	O
the	O
emphasis	O
is	O
on	O
the	O
algorithmic	O
processing	O
of	O
large	O
data	B
sets	O
for	O
the	O
extraction	O
of	O
useful	O
information	B
,	O
with	O
the	O
prediction	O
algorithms	O
as	O
exemplars	O
.	O
2016b	O
this	O
represents	O
the	O
traditional	O
line	O
of	O
statistical	O
thinking	O
,	O
of	O
the	O
kind	O
that	O
could	O
be	O
located	O
within	O
figure	O
14.1	O
,	O
but	O
now	O
energized	O
with	O
a	O
renewed	O
focus	O
on	O
applications	O
.	O
of	O
particular	O
applied	O
interest	O
are	O
biology	O
and	O
ge-	O
netics	O
.	O
genome-wide	O
association	O
studies	O
(	O
gwas	O
)	O
show	O
a	O
different	O
face	O
of	O
big	O
data	B
.	O
prediction	O
is	O
important	O
here,8	O
but	O
not	O
sufﬁcient	O
for	O
the	O
scientiﬁc	O
understanding	O
of	O
disease	O
.	O
a	O
cohesive	O
inferential	O
theory	B
was	O
forged	O
in	O
the	O
ﬁrst	O
half	O
of	O
the	O
twenti-	O
eth	O
century	O
,	O
but	O
unity	O
came	O
at	O
the	O
price	O
of	O
an	O
inwardly	O
focused	O
discipline	O
,	O
of	O
reduced	O
practical	O
utility	O
.	O
in	O
the	O
century	O
’	O
s	O
second	O
half	O
,	O
electronic	O
com-	O
putation	O
unleashed	O
a	O
vast	O
expansion	O
of	O
useful—and	O
much	O
used—statistical	O
methodology	O
.	O
expansion	O
accelerated	O
at	O
the	O
turn	O
of	O
the	O
millennium	O
,	O
further	O
increasing	O
the	O
reach	O
of	O
statistical	O
thinking	O
,	O
but	O
now	O
at	O
the	O
price	O
of	O
intellec-	O
tual	O
cohesion	O
.	O
it	O
is	O
tempting	O
but	O
risky	O
to	O
speculate	O
on	O
the	O
future	O
of	O
statistics	B
.	O
what	O
will	O
the	O
mathematics–applications–computation	O
diagram	O
look	O
like	O
,	O
say	O
25	O
years	O
from	O
now	O
?	O
the	O
appetite	O
for	O
statistical	O
analysis	B
seems	O
to	O
be	O
always	O
increasing	O
,	O
both	O
from	O
science	O
and	O
from	O
society	O
in	O
general	O
.	O
data	B
science	O
has	O
blossomed	O
in	O
response	O
,	O
but	O
so	O
has	O
the	O
traditional	O
wing	O
of	O
the	O
ﬁeld	O
.	O
the	O
data-analytic	O
initiatives	O
represented	O
in	O
the	O
diagram	O
by	O
2016a	O
and	O
2016b	O
are	O
in	O
actuality	O
not	O
isolated	O
points	O
but	O
the	O
centers	O
of	O
overlapping	O
distributions	O
.	O
7	O
breiman	O
(	O
1996	O
)	O
for	O
random	O
forests	O
,	O
freund	O
and	O
schapire	O
(	O
1997	O
)	O
for	O
boosting	O
.	O
8	O
“	O
personalized	O
medicine	O
”	O
in	O
which	O
an	O
individual	O
’	O
s	O
genome	O
predicts	O
his	O
or	O
her	O
optimal	O
treatment	O
has	O
attracted	O
grail-like	O
attention	O
.	O
452	O
epilogue	O
a	O
hopeful	O
scenario	O
for	O
the	O
future	O
is	O
one	O
of	O
an	O
increasing	O
overlap	O
that	O
puts	O
data	B
science	O
on	O
a	O
solid	O
footing	O
while	O
leading	O
to	O
a	O
broader	O
general	O
formula-	O
tion	O
of	O
statistical	O
inference	B
.	O
references	O
abu-mostafa	O
,	O
y	O
.	O
1995.	O
hints	O
.	O
neural	O
computation	O
,	O
7	O
,	O
639–671	O
.	O
achanta	O
,	O
r.	O
,	O
and	O
hastie	O
,	O
t.	O
2015.	O
telugu	O
ocr	O
framework	O
using	O
deep	O
learning	O
.	O
tech	O
.	O
rept	O
.	O
statistics	B
department	O
,	O
stanford	O
university	O
.	O
akaike	O
,	O
h.	O
1973.	O
information	B
theory	O
and	O
an	O
extension	O
of	O
the	O
maximum	B
likelihood	I
prin-	O
ciple	O
.	O
pages	O
267–281	O
of	O
:	O
second	O
international	O
symposium	O
on	O
information	B
theory	O
(	O
tsahkadsor	O
,	O
1971	O
)	O
.	O
akad´emiai	O
kiad´o	O
,	O
budapest	O
.	O
anderson	O
,	O
t.	O
w.	O
2003.	O
an	O
introduction	O
to	O
multivariate	B
statistical	O
analysis	B
.	O
third	O
edn	O
.	O
wiley	O
series	O
in	O
probability	O
and	O
statistics	O
.	O
wiley-interscience	O
.	O
bastien	O
,	O
f.	O
,	O
lamblin	O
,	O
p.	O
,	O
pascanu	O
,	O
r.	O
,	O
bergstra	O
,	O
j.	O
,	O
goodfellow	O
,	O
i.	O
j.	O
,	O
bergeron	O
,	O
a.	O
,	O
bouchard	O
,	O
n.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
2012.	O
theano	O
:	O
new	O
features	O
and	O
speed	O
improvements	O
.	O
deep	O
learning	O
and	O
unsupervised	O
feature	O
learning	O
nips	O
2012	O
workshop	O
.	O
becker	O
,	O
r.	O
,	O
chambers	O
,	O
j.	O
,	O
and	O
wilks	O
,	O
a	O
.	O
1988.	O
the	O
new	O
s	O
language	O
:	O
a	O
programming	O
environment	O
for	O
data	B
analysis	O
and	O
graphics	O
.	O
paciﬁc	O
grove	O
,	O
ca	O
:	O
wadsworth	O
and	O
brooks/cole	O
.	O
bellhouse	O
,	O
d.	O
r.	O
2004.	O
the	O
reverend	O
thomas	O
bayes	O
,	O
frs	O
:	O
a	O
biography	O
to	O
celebrate	O
the	O
tercentenary	O
of	O
his	O
birth	O
.	O
statist	O
.	O
sci.	O
,	O
19	O
(	O
1	O
)	O
,	O
3–43	O
.	O
with	O
comments	O
and	O
a	O
rejoinder	O
by	O
the	O
author	O
.	O
bengio	O
,	O
y.	O
,	O
courville	O
,	O
a.	O
,	O
and	O
vincent	O
,	O
p.	O
2013.	O
representation	O
learning	O
:	O
a	O
review	O
and	O
new	O
perspectives	O
.	O
ieee	O
transactions	O
on	O
pattern	O
analysis	B
and	O
machine	O
intelligence	O
,	O
35	O
(	O
8	O
)	O
,	O
1798–1828	O
.	O
benjamini	O
,	O
y.	O
,	O
and	O
hochberg	O
,	O
y	O
.	O
1995.	O
controlling	O
the	O
false	O
discovery	O
rate	B
:	O
a	O
practical	O
and	O
powerful	O
approach	O
to	O
multiple	O
testing	B
.	O
j.	O
roy	O
.	O
statist	O
.	O
soc	O
.	O
ser	O
.	O
b	O
,	O
57	O
(	O
1	O
)	O
,	O
289–	O
300.	O
benjamini	O
,	O
y.	O
,	O
and	O
yekutieli	O
,	O
d.	O
2005.	O
false	O
discovery	O
rate-adjusted	O
multiple	O
conﬁ-	O
dence	O
intervals	B
for	O
selected	O
parameters	O
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
100	O
(	O
469	O
)	O
,	O
71–93	O
.	O
berger	O
,	O
j.	O
o	O
.	O
2006.	O
the	O
case	O
for	O
objective	O
bayesian	O
analysis	B
.	O
bayesian	O
anal.	O
,	O
1	O
(	O
3	O
)	O
,	O
385–402	O
(	O
electronic	O
)	O
.	O
berger	O
,	O
j.	O
o.	O
,	O
and	O
pericchi	O
,	O
l.	O
r.	O
1996.	O
the	O
intrinsic	O
bayes	O
factor	B
for	O
model	B
selection	I
and	O
prediction	O
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
91	O
(	O
433	O
)	O
,	O
109–122	O
.	O
bergstra	O
,	O
j.	O
,	O
breuleux	O
,	O
o.	O
,	O
bastien	O
,	O
f.	O
,	O
lamblin	O
,	O
p.	O
,	O
pascanu	O
,	O
r.	O
,	O
desjardins	O
,	O
g.	O
,	O
turian	O
,	O
j.	O
,	O
warde-farley	O
,	O
d.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
2010	O
(	O
june	O
)	O
.	O
theano	O
:	O
a	O
cpu	O
and	O
gpu	O
math	O
expression	O
compiler	O
.	O
in	O
:	O
proceedings	O
of	O
the	O
python	O
for	O
scientiﬁc	O
computing	O
con-	O
ference	O
(	O
scipy	O
)	O
.	O
berk	O
,	O
r.	O
,	O
brown	O
,	O
l.	O
,	O
buja	O
,	O
a.	O
,	O
zhang	O
,	O
k.	O
,	O
and	O
zhao	O
,	O
l.	O
2013.	O
valid	O
post-selection	O
inference	B
.	O
ann	O
.	O
statist.	O
,	O
41	O
(	O
2	O
)	O
,	O
802–837	O
.	O
453	O
454	O
references	O
berkson	O
,	O
j	O
.	O
1944.	O
application	O
of	O
the	O
logistic	O
function	O
to	O
bio-assay	O
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
39	O
(	O
227	O
)	O
,	O
357–365	O
.	O
bernardo	O
,	O
j.	O
m.	O
1979.	O
reference	O
posterior	O
distributions	O
for	O
bayesian	O
inference	B
.	O
j.	O
roy	O
.	O
birch	O
,	O
m.	O
w.	O
1964.	O
the	O
detection	O
of	O
partial	O
association	O
.	O
i.	O
the	O
2	O
(	O
cid:2	O
)	O
2	O
case	O
.	O
j.	O
roy	O
.	O
statist	O
.	O
statist	O
.	O
soc	O
.	O
ser	O
.	O
b	O
,	O
41	O
(	O
2	O
)	O
,	O
113–147	O
.	O
with	O
discussion	O
.	O
soc	O
.	O
ser	O
.	O
b	O
,	O
26	O
(	O
2	O
)	O
,	O
313–324	O
.	O
bishop	O
,	O
c.	O
1995.	O
neural	O
networks	O
for	O
pattern	O
recognition	O
.	O
clarendon	O
press	O
,	O
oxford	O
.	O
boos	O
,	O
d.	O
d.	O
,	O
and	O
serﬂing	O
,	O
r.	O
j	O
.	O
1980.	O
a	O
note	O
on	O
differentials	O
and	O
the	O
clt	O
and	O
lil	O
for	O
statistical	O
functions	O
,	O
with	O
application	O
to	O
m	O
-estimates	O
.	O
ann	O
.	O
statist.	O
,	O
8	O
(	O
3	O
)	O
,	O
618–624	O
.	O
boser	O
,	O
b.	O
,	O
guyon	O
,	O
i.	O
,	O
and	O
vapnik	O
,	O
v.	O
1992.	O
a	O
training	O
algorithm	B
for	O
optimal	O
margin	O
classiﬁers	O
.	O
in	O
:	O
proceedings	O
of	O
colt	O
ii	O
.	O
breiman	O
,	O
l.	O
1996.	O
bagging	O
predictors	O
.	O
mach	O
.	O
learn.	O
,	O
24	O
(	O
2	O
)	O
,	O
123–140	O
.	O
breiman	O
,	O
l.	O
1998.	O
arcing	O
classiﬁers	O
(	O
with	O
discussion	O
)	O
.	O
annals	O
of	O
statistics	B
,	O
26	O
,	O
801–	O
849.	O
breiman	O
,	O
l.	O
2001.	O
random	O
forests	O
.	O
machine	O
learning	O
,	O
45	O
,	O
5–32	O
.	O
breiman	O
,	O
l.	O
,	O
friedman	O
,	O
j.	O
,	O
olshen	O
,	O
r.	O
a.	O
,	O
and	O
stone	O
,	O
c.	O
j	O
.	O
1984.	O
classiﬁcation	O
and	O
regression	O
trees	B
.	O
wadsworth	O
statistics/probability	O
series	O
.	O
wadsworth	O
advanced	O
books	O
and	O
software	O
.	O
carlin	O
,	O
b.	O
p.	O
,	O
and	O
louis	O
,	O
t.	O
a	O
.	O
1996.	O
bayes	O
and	O
empirical	O
bayes	O
methods	O
for	O
data	B
analysis	O
.	O
monographs	O
on	O
statistics	B
and	O
applied	O
probability	O
,	O
vol	O
.	O
69.	O
chapman	O
&	O
hall	O
.	O
carlin	O
,	O
b.	O
p.	O
,	O
and	O
louis	O
,	O
t.	O
a	O
.	O
2000.	O
bayes	O
and	O
empirical	O
bayes	O
methods	O
for	O
data	B
analysis	O
.	O
2	O
edn	O
.	O
texts	O
in	O
statistical	O
science	O
.	O
chapman	O
&	O
hall/crc	O
.	O
chambers	O
,	O
j.	O
m.	O
,	O
and	O
hastie	O
,	O
t.	O
j	O
.	O
(	O
eds	O
)	O
.	O
1993.	O
statistical	O
models	B
in	O
s.	O
chapman	O
&	O
hall	O
computer	O
science	O
series	O
.	O
chapman	O
&	O
hall	O
.	O
cleveland	O
,	O
w.	O
s.	O
1981.	O
lowess	B
:	O
a	O
program	O
for	O
smoothing	B
scatterplots	O
by	O
robust	O
locally	O
weighted	O
regression	B
.	O
amer	O
.	O
statist.	O
,	O
35	O
(	O
1	O
)	O
,	O
54.	O
cox	O
,	O
d.	O
r.	O
1958.	O
the	O
regression	B
analysis	O
of	O
binary	O
sequences	O
.	O
j.	O
roy	O
.	O
statist	O
.	O
soc	O
.	O
ser	O
.	O
b	O
,	O
20	O
,	O
215–242	O
.	O
cox	O
,	O
d.	O
r.	O
1970.	O
the	O
analysis	B
of	O
binary	O
data	B
.	O
methuen	O
’	O
s	O
monographs	O
on	O
applied	O
probability	O
and	O
statistics	O
.	O
methuen	O
&	O
co.	O
cox	O
,	O
d.	O
r.	O
1972.	O
regression	B
models	O
and	O
life-tables	O
.	O
j.	O
roy	O
.	O
statist	O
.	O
soc	O
.	O
ser	O
.	O
b	O
,	O
34	O
(	O
2	O
)	O
,	O
187–220	O
.	O
cox	O
,	O
d.	O
r.	O
1975.	O
partial	O
likelihood	B
.	O
biometrika	O
,	O
62	O
(	O
2	O
)	O
,	O
269–276	O
.	O
cox	O
,	O
d.	O
r.	O
,	O
and	O
hinkley	O
,	O
d.	O
v.	O
1974.	O
theoretical	O
statistics	B
.	O
chapman	O
&	O
hall	O
.	O
cox	O
,	O
d.	O
r.	O
,	O
and	O
reid	O
,	O
n.	O
1987.	O
parameter	O
orthogonality	O
and	O
approximate	O
conditional	O
inference	B
.	O
j.	O
roy	O
.	O
statist	O
.	O
soc	O
.	O
ser	O
.	O
b	O
,	O
49	O
(	O
1	O
)	O
,	O
1–39	O
.	O
with	O
a	O
discussion	O
.	O
crowley	O
,	O
j	O
.	O
1974.	O
asymptotic	O
normality	O
of	O
a	O
new	O
nonparametric	B
statistic	O
for	O
use	O
in	O
organ	O
transplant	O
studies	O
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
69	O
(	O
348	O
)	O
,	O
1006–1011	O
.	O
de	O
finetti	O
,	O
b	O
.	O
1972.	O
probability	O
,	O
induction	O
and	O
statistics	O
.	O
the	O
art	O
of	O
guessing	O
.	O
john	O
wiley	O
&	O
sons	O
,	O
london-new	O
york-sydney	O
.	O
dembo	O
,	O
a.	O
,	O
cover	O
,	O
t.	O
m.	O
,	O
and	O
thomas	O
,	O
j.	O
a	O
.	O
1991.	O
information-theoretic	O
inequalities	O
.	O
ieee	O
trans	O
.	O
inform	O
.	O
theory	B
,	O
37	O
(	O
6	O
)	O
,	O
1501–1518	O
.	O
dempster	O
,	O
a.	O
p.	O
,	O
laird	O
,	O
n.	O
m.	O
,	O
and	O
rubin	O
,	O
d.	O
b	O
.	O
1977.	O
maximum	B
likelihood	I
from	O
incomplete	O
data	B
via	O
the	O
em	O
algorithm	B
.	O
j.	O
roy	O
.	O
statist	O
.	O
soc	O
.	O
ser	O
.	O
b	O
,	O
39	O
(	O
1	O
)	O
,	O
1–38	O
.	O
diaconis	O
,	O
p.	O
,	O
and	O
ylvisaker	O
,	O
d.	O
1979.	O
conjugate	B
priors	O
for	O
exponential	O
families	O
.	O
ann	O
.	O
statist.	O
,	O
7	O
(	O
2	O
)	O
,	O
269–281	O
.	O
references	O
455	O
diciccio	O
,	O
t.	O
,	O
and	O
efron	O
,	O
b	O
.	O
1992.	O
more	O
accurate	O
conﬁdence	B
intervals	I
in	O
exponential	O
families	O
.	O
biometrika	O
,	O
79	O
(	O
2	O
)	O
,	O
231–245	O
.	O
donoho	O
,	O
d.	O
l.	O
2015	O
.	O
50	O
years	O
of	O
data	B
science	O
.	O
r-bloggers	O
.	O
www.r-bloggers	O
.	O
com/50-years-of-data-science-by-david-donoho/	O
.	O
edwards	O
,	O
a.	O
w.	O
f.	O
1992.	O
likelihood	B
.	O
expanded	O
edn	O
.	O
johns	O
hopkins	O
university	O
press	O
.	O
revised	O
reprint	O
of	O
the	O
1972	O
original	O
.	O
efron	O
,	O
b	O
.	O
1967.	O
the	O
two	O
sample	B
problem	O
with	O
censored	O
data	B
.	O
pages	O
831–853	O
of	O
:	O
proc	O
.	O
5th	O
berkeley	O
symp	O
.	O
math	O
.	O
statist	O
.	O
and	O
prob.	O
,	O
vol	O
.	O
4.	O
university	O
of	O
california	O
press	O
.	O
efron	O
,	O
b	O
.	O
1975.	O
deﬁning	O
the	O
curvature	O
of	O
a	O
statistical	O
problem	O
(	O
with	O
applications	O
to	O
second	O
order	O
efﬁciency	O
)	O
.	O
ann	O
.	O
statist.	O
,	O
3	O
(	O
6	O
)	O
,	O
1189–1242	O
.	O
with	O
discussion	O
and	O
a	O
reply	O
by	O
the	O
author	O
.	O
efron	O
,	O
b	O
.	O
1977.	O
the	O
efﬁciency	O
of	O
cox	O
’	O
s	O
likelihood	B
function	O
for	O
censored	O
data	B
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
72	O
(	O
359	O
)	O
,	O
557–565	O
.	O
efron	O
,	O
b	O
.	O
1979.	O
bootstrap	O
methods	O
:	O
another	O
look	O
at	O
the	O
jackknife	O
.	O
ann	O
.	O
statist.	O
,	O
7	O
(	O
1	O
)	O
,	O
1–26	O
.	O
efron	O
,	O
b	O
.	O
1982.	O
the	O
jackknife	O
,	O
the	O
bootstrap	O
and	O
other	O
resampling	O
plans	B
.	O
cbms-nsf	O
regional	O
conference	O
series	O
in	O
applied	O
mathematics	O
,	O
vol	O
.	O
38.	O
society	O
for	O
industrial	O
and	O
applied	O
mathematics	O
(	O
siam	O
)	O
.	O
efron	O
,	O
b	O
.	O
1983.	O
estimating	O
the	O
error	O
rate	B
of	O
a	O
prediction	O
rule	B
:	O
improvement	O
on	O
cross-	O
validation	O
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
78	O
(	O
382	O
)	O
,	O
316–331	O
.	O
efron	O
,	O
b	O
.	O
1985.	O
bootstrap	O
conﬁdence	B
intervals	I
for	O
a	O
class	O
of	O
parametric	B
problems	O
.	O
biometrika	O
,	O
72	O
(	O
1	O
)	O
,	O
45–58	O
.	O
efron	O
,	O
b	O
.	O
1986.	O
how	O
biased	O
is	O
the	O
apparent	O
error	O
rate	B
of	O
a	O
prediction	O
rule	B
?	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
81	O
(	O
394	O
)	O
,	O
461–470	O
.	O
efron	O
,	O
b	O
.	O
1987.	O
better	O
bootstrap	O
conﬁdence	B
intervals	I
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
82	O
(	O
397	O
)	O
,	O
171–200	O
.	O
with	O
comments	O
and	O
a	O
rejoinder	O
by	O
the	O
author	O
.	O
efron	O
,	O
b	O
.	O
1988.	O
logistic	B
regression	I
,	O
survival	O
analysis	B
,	O
and	O
the	O
kaplan–meier	O
curve	O
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
83	O
(	O
402	O
)	O
,	O
414–425	O
.	O
efron	O
,	O
b	O
.	O
1993.	O
bayes	O
and	O
likelihood	O
calculations	O
from	O
conﬁdence	O
intervals	B
.	O
biometrika	O
,	O
80	O
(	O
1	O
)	O
,	O
3–26	O
.	O
efron	O
,	O
b	O
.	O
1998.	O
r.	O
a.	O
fisher	O
in	O
the	O
21st	O
century	O
(	O
invited	O
paper	O
presented	O
at	O
the	O
1996	O
r.	O
a.	O
fisher	O
lecture	O
)	O
.	O
statist	O
.	O
sci.	O
,	O
13	O
(	O
2	O
)	O
,	O
95–122	O
.	O
with	O
comments	O
and	O
a	O
rejoinder	O
by	O
the	O
author	O
.	O
efron	O
,	O
b	O
.	O
2004.	O
the	O
estimation	B
of	O
prediction	O
error	O
:	O
covariance	O
penalties	O
and	O
cross-	O
validation	O
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
99	O
(	O
467	O
)	O
,	O
619–642	O
.	O
with	O
comments	O
and	O
a	O
rejoin-	O
der	O
by	O
the	O
author	O
.	O
efron	O
,	O
b	O
.	O
2010.	O
large-scale	O
inference	O
:	O
empirical	B
bayes	O
methods	O
for	O
estimation	B
,	O
test-	O
ing	O
,	O
and	O
prediction	O
.	O
institute	O
of	O
mathematical	O
statistics	B
monographs	O
,	O
vol	O
.	O
1.	O
cam-	O
bridge	O
university	O
press	O
.	O
efron	O
,	O
b	O
.	O
2011.	O
tweedie	O
’	O
s	O
formula	B
and	O
selection	O
bias	O
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
106	O
(	O
496	O
)	O
,	O
1602–1614	O
.	O
efron	O
,	O
b	O
.	O
2014a	O
.	O
estimation	B
and	O
accuracy	O
after	O
model	B
selection	I
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
109	O
(	O
507	O
)	O
,	O
991–1007	O
.	O
efron	O
,	O
b	O
.	O
2014b	O
.	O
two	O
modeling	O
strategies	O
for	O
empirical	B
bayes	O
estimation	B
.	O
statist	O
.	O
sci.	O
,	O
29	O
(	O
2	O
)	O
,	O
285–301	O
.	O
efron	O
,	O
b	O
.	O
2015.	O
frequentist	O
accuracy	O
of	O
bayesian	O
estimates	O
.	O
j.	O
roy	O
.	O
statist	O
.	O
soc	O
.	O
ser	O
.	O
b	O
,	O
77	O
(	O
3	O
)	O
,	O
617–646	O
.	O
456	O
references	O
efron	O
,	O
b	O
.	O
2016.	O
empirical	B
bayes	O
deconvolution	B
estimates	O
.	O
biometrika	O
,	O
103	O
(	O
1	O
)	O
,	O
1–20	O
.	O
efron	O
,	O
b.	O
,	O
and	O
feldman	O
,	O
d.	O
1991.	O
compliance	O
as	O
an	O
explanatory	O
variable	O
in	O
clinical	O
trials	O
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
86	O
(	O
413	O
)	O
,	O
9–17	O
.	O
efron	O
,	O
b.	O
,	O
and	O
gous	O
,	O
a	O
.	O
2001.	O
scales	O
of	O
evidence	O
for	O
model	B
selection	I
:	O
fisher	O
versus	O
jeffreys	O
.	O
pages	O
208–256	O
of	O
:	O
model	B
selection	I
.	O
ims	O
lecture	O
notes	O
monograph	O
series	O
,	O
vol	O
.	O
38.	O
beachwood	O
,	O
oh	O
:	O
institute	O
of	O
mathematics	O
and	O
statististics	O
.	O
with	O
discussion	O
and	O
a	O
rejoinder	O
by	O
the	O
authors	O
.	O
efron	O
,	O
b.	O
,	O
and	O
hinkley	O
,	O
d.	O
v.	O
1978.	O
assessing	O
the	O
accuracy	O
of	O
the	O
maximum	B
likelihood	I
estimator	O
:	O
observed	O
versus	O
expected	O
fisher	O
information	B
.	O
biometrika	O
,	O
65	O
(	O
3	O
)	O
,	O
457–	O
487.	O
with	O
comments	O
and	O
a	O
reply	O
by	O
the	O
authors	O
.	O
efron	O
,	O
b.	O
,	O
and	O
morris	O
,	O
c.	O
1972.	O
limiting	O
the	O
risk	O
of	O
bayes	O
and	O
empirical	O
bayes	O
estima-	O
tors	O
.	O
ii	O
.	O
the	O
empirical	B
bayes	O
case	O
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
67	O
,	O
130–139	O
.	O
efron	O
,	O
b.	O
,	O
and	O
morris	O
,	O
c.	O
1977.	O
stein	O
’	O
s	O
paradox	B
in	O
statistics	B
.	O
scientiﬁc	O
american	O
,	O
236	O
(	O
5	O
)	O
,	O
119–127	O
.	O
efron	O
,	O
b.	O
,	O
and	O
petrosian	O
,	O
v.	O
1992.	O
a	O
simple	O
test	O
of	O
independence	O
for	O
truncated	O
data	B
with	O
applications	O
to	O
redshift	O
surveys	O
.	O
astrophys	O
.	O
j.	O
,	O
399	O
(	O
nov	O
)	O
,	O
345–352	O
.	O
efron	O
,	O
b.	O
,	O
and	O
stein	O
,	O
c.	O
1981.	O
the	O
jackknife	O
estimate	O
of	O
variance	O
.	O
ann	O
.	O
statist.	O
,	O
9	O
(	O
3	O
)	O
,	O
586–596	O
.	O
efron	O
,	O
b.	O
,	O
and	O
thisted	O
,	O
r.	O
1976.	O
estimating	O
the	O
number	O
of	O
unseen	O
species	O
:	O
how	O
many	O
words	O
did	O
shakespeare	O
know	O
?	O
biometrika	O
,	O
63	O
(	O
3	O
)	O
,	O
435–447	O
.	O
efron	O
,	O
b.	O
,	O
and	O
tibshirani	O
,	O
r.	O
1993.	O
an	O
introduction	O
to	O
the	O
bootstrap	O
.	O
monographs	O
on	O
statistics	B
and	O
applied	O
probability	O
,	O
vol	O
.	O
57.	O
chapman	O
&	O
hall	O
.	O
efron	O
,	O
b.	O
,	O
and	O
tibshirani	O
,	O
r.	O
1997.	O
improvements	O
on	O
cross-validation	O
:	O
the	O
.632+	O
boot-	O
strap	O
method	B
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
92	O
(	O
438	O
)	O
,	O
548–560	O
.	O
efron	O
,	O
b.	O
,	O
hastie	O
,	O
t.	O
,	O
johnstone	O
,	O
i.	O
,	O
and	O
tibshirani	O
,	O
r.	O
2004.	O
least	O
angle	O
regression	B
.	O
an-	O
nals	O
of	O
statistics	B
,	O
32	O
(	O
2	O
)	O
,	O
407–499	O
.	O
(	O
with	O
discussion	O
,	O
and	O
a	O
rejoinder	O
by	O
the	O
authors	O
)	O
.	O
finney	O
,	O
d.	O
j	O
.	O
1947.	O
the	O
estimation	B
from	O
individual	O
records	O
of	O
the	O
relationship	O
between	O
dose	O
and	O
quantal	O
response	O
.	O
biometrika	O
,	O
34	O
(	O
3/4	O
)	O
,	O
320–334	O
.	O
fisher	O
,	O
r.	O
a	O
.	O
1915.	O
frequency	O
distribution	B
of	O
the	O
values	O
of	O
the	O
correlation	B
coefﬁcient	I
in	O
samples	O
from	O
an	O
indeﬁnitely	O
large	O
population	O
.	O
biometrika	O
,	O
10	O
(	O
4	O
)	O
,	O
507–521	O
.	O
fisher	O
,	O
r.	O
a	O
.	O
1925.	O
theory	B
of	O
statistical	O
estimation	B
.	O
math	O
.	O
proc	O
.	O
cambridge	O
phil	O
.	O
soc.	O
,	O
22	O
(	O
7	O
)	O
,	O
700–725	O
.	O
fisher	O
,	O
r.	O
a	O
.	O
1930.	O
inverse	O
probability	O
.	O
math	O
.	O
proc	O
.	O
cambridge	O
phil	O
.	O
soc.	O
,	O
26	O
(	O
10	O
)	O
,	O
528–535	O
.	O
fisher	O
,	O
r.	O
a.	O
,	O
corbet	O
,	O
a.	O
,	O
and	O
williams	O
,	O
c.	O
1943.	O
the	O
relation	O
between	O
the	O
number	O
of	O
species	O
and	O
the	O
number	O
of	O
individuals	O
in	O
a	O
random	O
sample	B
of	O
an	O
animal	O
population	O
.	O
j.	O
anim	O
.	O
ecol.	O
,	O
12	O
,	O
42–58	O
.	O
fithian	O
,	O
w.	O
,	O
sun	O
,	O
d.	O
,	O
and	O
taylor	O
,	O
j	O
.	O
2014.	O
optimal	O
inference	B
after	O
model	B
selection	I
.	O
arxiv	O
e-prints	O
,	O
oct.	O
freund	O
,	O
y.	O
,	O
and	O
schapire	O
,	O
r.	O
1996.	O
experiments	O
with	O
a	O
new	O
boosting	O
algorithm	B
.	O
pages	O
148–156	O
of	O
:	O
machine	O
learning	O
:	O
proceedings	O
of	O
the	O
thirteenth	O
international	O
con-	O
ference	O
.	O
morgan	O
kauffman	O
,	O
san	O
francisco	O
.	O
freund	O
,	O
y.	O
,	O
and	O
schapire	O
,	O
r.	O
1997.	O
a	O
decision-theoretic	O
generalization	O
of	O
online	O
learn-	O
ing	O
and	O
an	O
application	O
to	O
boosting	O
.	O
journal	O
of	O
computer	O
and	O
system	O
sciences	O
,	O
55	O
,	O
119–139	O
.	O
friedman	O
,	O
j	O
.	O
2001.	O
greedy	O
function	B
approximation	O
:	O
a	O
gradient	O
boosting	O
machine	O
.	O
an-	O
nals	O
of	O
statistics	B
,	O
29	O
(	O
5	O
)	O
,	O
1189–1232	O
.	O
references	O
457	O
friedman	O
,	O
j.	O
,	O
and	O
popescu	O
,	O
b	O
.	O
2005.	O
predictive	O
learning	O
via	O
rule	B
ensembles	O
.	O
tech	O
.	O
rept	O
.	O
stanford	O
university	O
.	O
friedman	O
,	O
j.	O
,	O
hastie	O
,	O
t.	O
,	O
and	O
tibshirani	O
,	O
r.	O
2000.	O
additive	O
logistic	B
regression	I
:	O
a	O
statis-	O
tical	O
view	O
of	O
boosting	O
(	O
with	O
discussion	O
)	O
.	O
annals	O
of	O
statistics	B
,	O
28	O
,	O
337–307	O
.	O
friedman	O
,	O
j.	O
,	O
hastie	O
,	O
t.	O
,	O
and	O
tibshirani	O
,	O
r.	O
2009.	O
glmnet	B
:	O
lasso	B
and	O
elastic-net	O
regu-	O
larized	O
generalized	O
linear	B
models	O
.	O
r	O
package	O
version	O
1.1-4.	O
friedman	O
,	O
j.	O
,	O
hastie	O
,	O
t.	O
,	O
and	O
tibshirani	O
,	O
r.	O
2010.	O
regularization	B
paths	O
for	O
generalized	O
linear	B
models	O
via	O
coordinate	O
descent	O
.	O
journal	O
of	O
statistical	O
software	O
,	O
33	O
(	O
1	O
)	O
,	O
1–22	O
.	O
geisser	O
,	O
s.	O
1974.	O
a	O
predictive	O
approach	O
to	O
the	O
random	O
effect	O
model	B
.	O
biometrika	O
,	O
61	O
,	O
101–107	O
.	O
gerber	O
,	O
m.	O
,	O
and	O
chopin	O
,	O
n.	O
2015.	O
sequential	O
quasi	O
monte	O
carlo	O
.	O
j.	O
roy	O
.	O
statist	O
.	O
soc	O
.	O
b	O
,	O
77	O
(	O
3	O
)	O
,	O
509–580	O
.	O
with	O
discussion	O
,	O
doi	O
:	O
10.1111/rssb.12104	O
.	O
gholami	O
,	O
s.	O
,	O
janson	O
,	O
l.	O
,	O
worhunsky	O
,	O
d.	O
j.	O
,	O
tran	O
,	O
t.	O
b.	O
,	O
squires	O
,	O
malcolm	O
,	O
i.	O
,	O
jin	O
,	O
l.	O
x.	O
,	O
spolverato	O
,	O
g.	O
,	O
votanopoulos	O
,	O
k.	O
i.	O
,	O
schmidt	O
,	O
c.	O
,	O
weber	O
,	O
s.	O
m.	O
,	O
bloomston	O
,	O
m.	O
,	O
cho	O
,	O
c.	O
s.	O
,	O
levine	O
,	O
e.	O
a.	O
,	O
fields	O
,	O
r.	O
c.	O
,	O
pawlik	O
,	O
t.	O
m.	O
,	O
maithel	O
,	O
s.	O
k.	O
,	O
efron	O
,	O
b.	O
,	O
norton	O
,	O
j.	O
a.	O
,	O
and	O
poultsides	O
,	O
g.	O
a	O
.	O
2015.	O
number	O
of	O
lymph	O
nodes	B
removed	O
and	O
survival	O
after	O
gastric	O
cancer	O
resection	O
:	O
an	O
analysis	B
from	O
the	O
us	O
gastric	O
cancer	O
collaborative	O
.	O
j.	O
amer	O
.	O
coll	O
.	O
surg.	O
,	O
221	O
(	O
2	O
)	O
,	O
291–299	O
.	O
good	O
,	O
i.	O
,	O
and	O
toulmin	O
,	O
g.	O
1956.	O
the	O
number	O
of	O
new	O
species	O
,	O
and	O
the	O
increase	O
in	O
popu-	O
lation	O
coverage	O
,	O
when	O
a	O
sample	B
is	O
increased	O
.	O
biometrika	O
,	O
43	O
,	O
45–63	O
.	O
hall	O
,	O
p.	O
1988.	O
theoretical	O
comparison	O
of	O
bootstrap	O
conﬁdence	B
intervals	I
.	O
ann	O
.	O
statist.	O
,	O
16	O
(	O
3	O
)	O
,	O
927–985	O
.	O
with	O
discussion	O
and	O
a	O
reply	O
by	O
the	O
author	O
.	O
hampel	O
,	O
f.	O
r.	O
1974.	O
the	O
inﬂuence	O
curve	O
and	O
its	O
role	O
in	O
robust	O
estimation	B
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
69	O
,	O
383–393	O
.	O
hampel	O
,	O
f.	O
r.	O
,	O
ronchetti	O
,	O
e.	O
m.	O
,	O
rousseeuw	O
,	O
p.	O
j.	O
,	O
and	O
stahel	O
,	O
w.	O
a	O
.	O
1986.	O
robust	O
statistics	B
:	O
the	O
approach	O
based	O
on	O
inﬂuence	O
functions	O
.	O
wiley	O
series	O
in	O
probability	O
and	O
mathematical	O
statistics	B
.	O
john	O
wiley	O
&	O
sons	O
.	O
harford	O
,	O
t.	O
2014.	O
big	O
data	B
:	O
a	O
big	O
mistake	O
?	O
signiﬁcance	O
,	O
11	O
(	O
5	O
)	O
,	O
14–19	O
.	O
hastie	O
,	O
t.	O
,	O
and	O
loader	O
,	O
c.	O
1993.	O
local	O
regression	B
:	O
automatic	O
kernel	O
carpentry	O
(	O
with	O
discussion	O
)	O
.	O
statistical	O
science	O
,	O
8	O
,	O
120–143	O
.	O
hastie	O
,	O
t.	O
,	O
and	O
tibshirani	O
,	O
r.	O
1990.	O
generalized	O
additive	O
models	B
.	O
chapman	O
and	O
hall	O
.	O
hastie	O
,	O
t.	O
,	O
and	O
tibshirani	O
,	O
r.	O
2004.	O
efﬁcient	O
quadratic	O
regularization	B
for	O
expression	O
arrays	O
.	O
biostatistics	O
,	O
5	O
(	O
3	O
)	O
,	O
329–340	O
.	O
hastie	O
,	O
t.	O
,	O
tibshirani	O
,	O
r.	O
,	O
and	O
friedman	O
,	O
j	O
.	O
2009.	O
the	O
elements	O
of	O
statistical	O
learning	O
.	O
data	B
mining	O
,	O
inference	B
,	O
and	O
prediction	O
.	O
second	O
edn	O
.	O
springer	O
series	O
in	O
statistics	B
.	O
springer	O
.	O
hastie	O
,	O
t.	O
,	O
tibshirani	O
,	O
r.	O
,	O
and	O
wainwright	O
,	O
m.	O
2015.	O
statistical	O
learning	B
with	I
sparsity	O
:	O
the	O
lasso	B
and	O
generalizations	O
.	O
chapman	O
and	O
hall	O
,	O
crc	O
press	O
.	O
hoeffding	O
,	O
w.	O
1952.	O
the	O
large-sample	O
power	O
of	O
tests	O
based	O
on	O
permutations	O
of	O
obser-	O
vations	O
.	O
ann	O
.	O
math	O
.	O
statist.	O
,	O
23	O
,	O
169–192	O
.	O
hoeffding	O
,	O
w.	O
1965.	O
asymptotically	O
optimal	O
tests	O
for	O
multinomial	O
distributions	O
.	O
ann	O
.	O
math	O
.	O
statist.	O
,	O
36	O
(	O
2	O
)	O
,	O
369–408	O
.	O
hoerl	O
,	O
a.	O
e.	O
,	O
and	O
kennard	O
,	O
r.	O
w.	O
1970.	O
ridge	B
regression	I
:	O
biased	O
estimation	B
for	O
nonor-	O
thogonal	O
problems	O
.	O
technometrics	O
,	O
12	O
(	O
1	O
)	O
,	O
55–67	O
.	O
huber	O
,	O
p.	O
j	O
.	O
1964.	O
robust	O
estimation	B
of	O
a	O
location	O
parameter	O
.	O
ann	O
.	O
math	O
.	O
statist.	O
,	O
35	O
,	O
73–101	O
.	O
458	O
references	O
jaeckel	O
,	O
l.	O
a	O
.	O
1972.	O
estimating	O
regression	B
coefﬁcients	O
by	O
minimizing	O
the	O
dispersion	O
of	O
the	O
residuals	O
.	O
ann	O
.	O
math	O
.	O
statist.	O
,	O
43	O
,	O
1449–1458	O
.	O
james	O
,	O
w.	O
,	O
and	O
stein	O
,	O
c.	O
1961.	O
estimation	B
with	O
quadratic	O
loss	O
.	O
pages	O
361–379	O
of	O
:	O
proc	O
.	O
4th	O
berkeley	O
symposium	O
on	O
mathematical	O
statistics	B
and	O
probability	O
,	O
vol	O
.	O
i.	O
university	O
of	O
california	O
press	O
.	O
jansen	O
,	O
l.	O
,	O
fithian	O
,	O
w.	O
,	O
and	O
hastie	O
,	O
t.	O
2015.	O
effective	O
degrees	O
of	O
freedom	O
:	O
a	O
ﬂawed	O
metaphor	O
.	O
biometrika	O
,	O
102	O
(	O
2	O
)	O
,	O
479–485	O
.	O
javanmard	O
,	O
a.	O
,	O
and	O
montanari	O
,	O
a	O
.	O
2014.	O
conﬁdence	B
intervals	I
and	O
hypothesis	B
testing	I
for	O
high-dimensional	O
regression	B
.	O
j.	O
of	O
machine	O
learning	O
res.	O
,	O
15	O
,	O
2869–2909	O
.	O
jaynes	O
,	O
e.	O
1968.	O
prior	B
probabilities	O
.	O
ieee	O
trans	O
.	O
syst	O
.	O
sci	O
.	O
cybernet.	O
,	O
4	O
(	O
3	O
)	O
,	O
227–241	O
.	O
jeffreys	O
,	O
h.	O
1961.	O
theory	B
of	O
probability	O
.	O
third	O
ed	O
.	O
clarendon	O
press	O
.	O
johnson	O
,	O
n.	O
l.	O
,	O
and	O
kotz	O
,	O
s.	O
1969.	O
distributions	O
in	O
statistics	B
:	O
discrete	O
distributions	O
.	O
houghton	O
mifﬂin	O
co.	O
johnson	O
,	O
n.	O
l.	O
,	O
and	O
kotz	O
,	O
s.	O
1970a	O
.	O
distributions	O
in	O
statistics	B
.	O
continuous	O
univariate	O
distributions	O
.	O
1.	O
houghton	O
mifﬂin	O
co.	O
johnson	O
,	O
n.	O
l.	O
,	O
and	O
kotz	O
,	O
s.	O
1970b	O
.	O
distributions	O
in	O
statistics	B
.	O
continuous	O
univariate	O
distributions	O
.	O
2.	O
houghton	O
mifﬂin	O
co.	O
johnson	O
,	O
n.	O
l.	O
,	O
and	O
kotz	O
,	O
s.	O
1972.	O
distributions	O
in	O
statistics	B
:	O
continuous	O
multivariate	B
distributions	O
.	O
john	O
wiley	O
&	O
sons	O
.	O
kaplan	O
,	O
e.	O
l.	O
,	O
and	O
meier	O
,	O
p.	O
1958.	O
nonparametric	B
estimation	O
from	O
incomplete	O
obser-	O
vations	O
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
53	O
(	O
282	O
)	O
,	O
457–481	O
.	O
kass	O
,	O
r.	O
e.	O
,	O
and	O
raftery	O
,	O
a.	O
e.	O
1995.	O
bayes	O
factors	O
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
90	O
(	O
430	O
)	O
,	O
773–795	O
.	O
kass	O
,	O
r.	O
e.	O
,	O
and	O
wasserman	O
,	O
l.	O
1996.	O
the	O
selection	O
of	O
prior	B
distributions	O
by	O
formal	O
rules	O
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
91	O
(	O
435	O
)	O
,	O
1343–1370	O
.	O
kuffner	O
,	O
r.	O
,	O
zach	O
,	O
n.	O
,	O
norel	O
,	O
r.	O
,	O
hawe	O
,	O
j.	O
,	O
schoenfeld	O
,	O
d.	O
,	O
wang	O
,	O
l.	O
,	O
li	O
,	O
g.	O
,	O
fang	O
,	O
l.	O
,	O
mackey	O
,	O
l.	O
,	O
hardiman	O
,	O
o.	O
,	O
cudkowicz	O
,	O
m.	O
,	O
sherman	O
,	O
a.	O
,	O
ertaylan	O
,	O
g.	O
,	O
grosse-	O
wentrup	O
,	O
m.	O
,	O
hothorn	O
,	O
t.	O
,	O
van	O
ligtenberg	O
,	O
j.	O
,	O
macke	O
,	O
j.	O
h.	O
,	O
meyer	O
,	O
t.	O
,	O
scholkopf	O
,	O
b.	O
,	O
tran	O
,	O
l.	O
,	O
vaughan	O
,	O
r.	O
,	O
stolovitzky	O
,	O
g.	O
,	O
and	O
leitner	O
,	O
m.	O
l.	O
2015.	O
crowdsourced	O
analysis	B
of	O
clinical	O
trial	O
data	B
to	O
predict	O
amyotrophic	O
lateral	O
sclerosis	O
progression	O
.	O
nat	O
biotech	O
,	O
33	O
(	O
1	O
)	O
,	O
51–57	O
.	O
lecun	O
,	O
y.	O
,	O
and	O
cortes	O
,	O
c.	O
2010.	O
mnist	O
handwritten	O
digit	O
database	O
.	O
http	O
:	O
//yann.lecun.com/exdb/mnist/	O
.	O
lecun	O
,	O
y.	O
,	O
bengio	O
,	O
y.	O
,	O
and	O
hinton	O
,	O
g.	O
2015.	O
deep	O
learning	O
.	O
nature	O
,	O
521	O
(	O
7553	O
)	O
,	O
436–	O
444.	O
lee	O
,	O
j.	O
,	O
sun	O
,	O
d.	O
,	O
sun	O
,	O
y.	O
,	O
and	O
taylor	O
,	O
j	O
.	O
2016.	O
exact	O
post-selection	O
inference	B
,	O
with	O
application	O
to	O
the	O
lasso	B
.	O
annals	O
of	O
statistics	B
,	O
44	O
(	O
3	O
)	O
,	O
907–927	O
.	O
lehmann	O
,	O
e.	O
l.	O
1983.	O
theory	B
of	O
point	O
estimation	B
.	O
wiley	O
series	O
in	O
probability	O
and	O
mathematical	O
statistics	B
.	O
john	O
wiley	O
&	O
sons	O
.	O
leslie	O
,	O
c.	O
,	O
eskin	O
,	O
e.	O
,	O
cohen	O
,	O
a.	O
,	O
weston	O
,	O
j.	O
,	O
and	O
noble	O
,	O
w.	O
s.	O
2003.	O
mismatch	O
string	O
kernels	O
for	O
discriminative	O
pretein	O
classiﬁcation	O
.	O
bioinformatics	O
,	O
1	O
,	O
1–10	O
.	O
liaw	O
,	O
a.	O
,	O
and	O
wiener	O
,	O
m.	O
2002.	O
classiﬁcation	O
and	O
regression	O
by	O
randomforest	O
.	O
r	O
news	O
,	O
2	O
(	O
3	O
)	O
,	O
18–22	O
.	O
liberman	O
,	O
m.	O
2015	O
(	O
april	O
)	O
.	O
“	O
reproducible	O
research	O
and	O
the	O
common	O
task	O
method	B
”	O
.	O
simons	O
foundation	O
frontiers	O
of	O
data	B
science	O
lecture	O
,	O
april	O
1	O
,	O
2015	O
;	O
video	O
avail-	O
able	O
.	O
references	O
459	O
lockhart	O
,	O
r.	O
,	O
taylor	O
,	O
j.	O
,	O
tibshirani	O
,	O
r.	O
,	O
and	O
tibshirani	O
,	O
r.	O
2014.	O
a	O
signiﬁcance	O
test	O
for	O
the	O
lasso	B
.	O
annals	O
of	O
statistics	B
,	O
42	O
(	O
2	O
)	O
,	O
413–468	O
.	O
with	O
discussion	O
and	O
a	O
rejoinder	O
by	O
the	O
authors	O
.	O
lynden-bell	O
,	O
d.	O
1971.	O
a	O
method	B
for	O
allowing	O
for	O
known	O
observational	O
selection	O
in	O
small	O
samples	O
applied	O
to	O
3cr	O
quasars	O
.	O
mon	O
.	O
not	O
.	O
roy	O
.	O
astron	O
.	O
soc.	O
,	O
155	O
(	O
1	O
)	O
,	O
95–18	O
.	O
mallows	O
,	O
c.	O
l.	O
1973.	O
some	O
comments	O
on	O
cp	O
.	O
technometrics	O
,	O
15	O
(	O
4	O
)	O
,	O
661–675	O
.	O
mantel	O
,	O
n.	O
,	O
and	O
haenszel	O
,	O
w.	O
1959.	O
statistical	O
aspects	O
of	O
the	O
analysis	B
of	O
data	B
from	O
retrospective	O
studies	O
of	O
disease	O
.	O
j.	O
natl	O
.	O
cancer	O
inst.	O
,	O
22	O
(	O
4	O
)	O
,	O
719–748	O
.	O
mardia	O
,	O
k.	O
v.	O
,	O
kent	O
,	O
j.	O
t.	O
,	O
and	O
bibby	O
,	O
j.	O
m.	O
1979.	O
multivariate	B
analysis	O
.	O
academic	O
press	O
.	O
mccullagh	O
,	O
p.	O
,	O
and	O
nelder	O
,	O
j	O
.	O
1983.	O
generalized	O
linear	B
models	O
.	O
monographs	O
on	O
statis-	O
tics	O
and	O
applied	O
probability	O
.	O
chapman	O
&	O
hall	O
.	O
mccullagh	O
,	O
p.	O
,	O
and	O
nelder	O
,	O
j	O
.	O
1989.	O
generalized	O
linear	B
models	O
.	O
second	O
edn	O
.	O
mono-	O
graphs	O
on	O
statistics	B
and	O
applied	O
probability	O
.	O
chapman	O
&	O
hall	O
.	O
metropolis	O
,	O
n.	O
,	O
rosenbluth	O
,	O
a.	O
w.	O
,	O
rosenbluth	O
,	O
m.	O
n.	O
,	O
teller	O
,	O
a.	O
h.	O
,	O
and	O
teller	O
,	O
e.	O
1953.	O
equation	B
of	O
state	O
calculations	O
by	O
fast	O
computing	O
machines	O
.	O
j.	O
chem	O
.	O
phys.	O
,	O
21	O
(	O
6	O
)	O
,	O
1087–1092	O
.	O
miller	O
,	O
jr	O
,	O
r.	O
g.	O
1964.	O
a	O
trustworthy	O
jackknife	O
.	O
ann	O
.	O
math	O
.	O
statist	O
,	O
35	O
,	O
1594–1605	O
.	O
miller	O
,	O
jr	O
,	O
r.	O
g.	O
1981.	O
simultaneous	O
statistical	O
inference	B
.	O
second	O
edn	O
.	O
springer	O
series	O
in	O
statistics	B
.	O
new	O
york	O
:	O
springer-verlag	O
.	O
nesterov	O
,	O
y	O
.	O
2013.	O
gradient	O
methods	O
for	O
minimizing	O
composite	O
functions	O
.	O
mathemati-	O
cal	O
programming	O
,	O
140	O
(	O
1	O
)	O
,	O
125–161	O
.	O
neyman	O
,	O
j	O
.	O
1937.	O
outline	O
of	O
a	O
theory	B
of	O
statistical	O
estimation	B
based	O
on	O
the	O
classical	O
theory	B
of	O
probability	O
.	O
phil	O
.	O
trans	O
.	O
roy	O
.	O
soc.	O
,	O
236	O
(	O
767	O
)	O
,	O
333–380	O
.	O
neyman	O
,	O
j	O
.	O
1977.	O
frequentist	O
probability	O
and	O
frequentist	O
statistics	B
.	O
synthese	O
,	O
36	O
(	O
1	O
)	O
,	O
97–131	O
.	O
neyman	O
,	O
j.	O
,	O
and	O
pearson	O
,	O
e.	O
s.	O
1933.	O
on	O
the	O
problem	O
of	O
the	O
most	O
efﬁcient	O
tests	O
of	O
statistical	O
hypotheses	O
.	O
phil	O
.	O
trans	O
.	O
roy	O
.	O
soc	O
.	O
a	O
,	O
231	O
(	O
694-706	O
)	O
,	O
289–337	O
.	O
ng	O
,	O
a	O
.	O
2015.	O
neural	O
networks	O
.	O
http	O
:	O
//deeplearning.stanford.edu/	O
wiki/index.php/neural_networks	O
.	O
lecture	O
notes	O
.	O
ngiam	O
,	O
j.	O
,	O
chen	O
,	O
z.	O
,	O
chia	O
,	O
d.	O
,	O
koh	O
,	O
p.	O
w.	O
,	O
le	O
,	O
q.	O
v.	O
,	O
and	O
ng	O
,	O
a	O
.	O
2010.	O
tiled	O
convo-	O
lutional	O
neural	O
networks	O
.	O
pages	O
1279–1287	O
of	O
:	O
lafferty	O
,	O
j.	O
,	O
williams	O
,	O
c.	O
,	O
shawe-	O
taylor	O
,	O
j.	O
,	O
zemel	O
,	O
r.	O
,	O
and	O
culotta	O
,	O
a	O
.	O
(	O
eds	O
)	O
,	O
advances	O
in	O
neural	O
information	B
pro-	O
cessing	O
systems	O
23.	O
curran	O
associates	O
,	O
inc.	O
o	O
’	O
hagan	O
,	O
a	O
.	O
1995.	O
fractional	O
bayes	O
factors	O
for	O
model	B
comparison	O
.	O
j.	O
roy	O
.	O
statist	O
.	O
soc	O
.	O
ser	O
.	O
b	O
,	O
57	O
(	O
1	O
)	O
,	O
99–138	O
.	O
with	O
discussion	O
and	O
a	O
reply	O
by	O
the	O
author	O
.	O
park	O
,	O
t.	O
,	O
and	O
casella	O
,	O
g.	O
2008.	O
the	O
bayesian	O
lasso	B
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
103	O
(	O
482	O
)	O
,	O
681–686	O
.	O
pearson	O
,	O
k.	O
1900.	O
on	O
the	O
criterion	O
that	O
a	O
given	O
system	O
of	O
deviations	O
from	O
the	O
probable	O
in	O
the	O
case	O
of	O
a	O
correlated	O
system	O
of	O
variables	O
is	O
such	O
that	O
it	O
can	O
be	O
reasonably	O
supposed	O
to	O
have	O
arisen	O
from	O
random	O
sampling	O
.	O
phil	O
.	O
mag.	O
,	O
50	O
(	O
302	O
)	O
,	O
157–175	O
.	O
pritchard	O
,	O
j.	O
,	O
stephens	O
,	O
m.	O
,	O
and	O
donnelly	O
,	O
p.	O
2000.	O
inference	B
of	O
population	O
structure	O
using	O
multilocus	O
genotype	O
data	B
.	O
genetics	O
,	O
155	O
(	O
june	O
)	O
,	O
945–959	O
.	O
quenouille	O
,	O
m.	O
h.	O
1956.	O
notes	O
on	O
bias	O
in	O
estimation	B
.	O
biometrika	O
,	O
43	O
,	O
353–360	O
.	O
r	O
core	O
team	O
.	O
2015.	O
r	O
:	O
a	O
language	O
and	O
environment	O
for	O
statistical	O
computing	O
.	O
r	O
foundation	O
for	O
statistical	O
computing	O
,	O
vienna	O
,	O
austria	O
.	O
460	O
references	O
ridgeway	O
,	O
g.	O
2005.	O
generalized	O
boosted	O
models	B
:	O
a	O
guide	O
to	O
the	O
gbm	B
package	O
.	O
avail-	O
able	O
online	O
.	O
ridgeway	O
,	O
g.	O
,	O
and	O
macdonald	O
,	O
j.	O
m.	O
2009.	O
doubly	O
robust	O
internal	B
benchmarking	O
and	O
false	O
discovery	O
rates	O
for	O
detecting	O
racial	O
bias	O
in	O
police	B
stops	O
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
104	O
(	O
486	O
)	O
,	O
661–668	O
.	O
ripley	O
,	O
b.	O
d.	O
1996.	O
pattern	O
recognition	O
and	O
neural	O
networks	O
.	O
cambridge	O
university	O
press	O
.	O
robbins	O
,	O
h.	O
1956.	O
an	O
empirical	B
bayes	O
approach	O
to	O
statistics	B
.	O
pages	O
157–163	O
of	O
:	O
proc	O
.	O
3rd	O
berkeley	O
symposium	O
on	O
mathematical	O
statistics	B
and	O
probability	O
,	O
vol	O
.	O
i.	O
univer-	O
sity	O
of	O
california	O
press	O
.	O
rosset	O
,	O
s.	O
,	O
zhu	O
,	O
j.	O
,	O
and	O
hastie	O
,	O
t.	O
2004.	O
margin	O
maximizing	O
loss	O
functions	O
.	O
in	O
:	O
thrun	O
,	O
s.	O
,	O
saul	O
,	O
l.	O
,	O
and	O
sch¨olkopf	O
,	O
b	O
.	O
(	O
eds	O
)	O
,	O
advances	O
in	O
neural	O
information	B
processing	O
systems	O
16.	O
mit	O
press	O
.	O
rubin	O
,	O
d.	O
b	O
.	O
1981.	O
the	O
bayesian	O
bootstrap	O
.	O
ann	O
.	O
statist.	O
,	O
9	O
(	O
1	O
)	O
,	O
130–134	O
.	O
savage	O
,	O
l.	O
j	O
.	O
1954.	O
the	O
foundations	O
of	O
statistics	B
.	O
john	O
wiley	O
&	O
sons	O
;	O
chapman	O
&	O
hill	O
.	O
schapire	O
,	O
r.	O
1990.	O
the	O
strength	O
of	O
weak	O
learnability	O
.	O
machine	O
learning	O
,	O
5	O
(	O
2	O
)	O
,	O
197–	O
227.	O
schapire	O
,	O
r.	O
,	O
and	O
freund	O
,	O
y	O
.	O
2012.	O
boosting	O
:	O
foundations	O
and	O
algorithms	O
.	O
mit	O
press	O
.	O
scheff´e	O
,	O
h.	O
1953.	O
a	O
method	B
for	O
judging	O
all	O
contrasts	O
in	O
the	O
analysis	B
of	O
variance	O
.	O
biometrika	O
,	O
40	O
(	O
1-2	O
)	O
,	O
87–110	O
.	O
sch¨olkopf	O
,	O
b.	O
,	O
and	O
smola	O
,	O
a	O
.	O
2001.	O
learning	B
with	I
kernels	O
:	O
support	O
vector	B
ma-	O
chines	O
,	O
regularization	B
,	O
optimization	O
,	O
and	O
beyond	O
(	O
adaptive	B
computation	O
and	O
ma-	O
chine	O
learning	O
)	O
.	O
mit	O
press	O
.	O
schwarz	O
,	O
g.	O
1978.	O
estimating	O
the	O
dimension	O
of	O
a	O
model	B
.	O
ann	O
.	O
statist.	O
,	O
6	O
(	O
2	O
)	O
,	O
461–464	O
.	O
senn	O
,	O
s.	O
2008.	O
a	O
note	O
concerning	O
a	O
selection	O
“	O
paradox	B
”	O
of	O
dawid	O
’	O
s	O
.	O
amer	O
.	O
statist.	O
,	O
62	O
(	O
3	O
)	O
,	O
206–210	O
.	O
soric	O
,	O
b	O
.	O
1989.	O
statistical	O
“	O
discoveries	O
”	O
and	O
effect-size	O
estimation	B
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
84	O
(	O
406	O
)	O
,	O
608–610	O
.	O
spevack	O
,	O
m.	O
1968.	O
a	O
complete	O
and	O
systematic	O
concordance	O
to	O
the	O
works	O
of	O
shake-	O
speare	O
.	O
vol	O
.	O
1–6	O
.	O
georg	O
olms	O
verlag	O
.	O
srivastava	O
,	O
n.	O
,	O
hinton	O
,	O
g.	O
,	O
krizhevsky	O
,	O
a.	O
,	O
sutskever	O
,	O
i.	O
,	O
and	O
salakhutdinov	O
,	O
r.	O
2014.	O
dropout	O
:	O
a	O
simple	O
way	O
to	O
prevent	O
neural	O
networks	O
from	O
overﬁtting	O
.	O
j.	O
of	O
machine	O
learning	O
res.	O
,	O
15	O
,	O
1929–1958	O
.	O
stefanski	O
,	O
l.	O
,	O
and	O
carroll	O
,	O
r.	O
j	O
.	O
1990.	O
deconvoluting	O
kernel	O
density	B
estimators	O
.	O
statis-	O
tics	O
,	O
21	O
(	O
2	O
)	O
,	O
169–184	O
.	O
stein	O
,	O
c.	O
1956.	O
inadmissibility	O
of	O
the	O
usual	O
estimator	B
for	O
the	O
mean	O
of	O
a	O
multivariate	B
nor-	O
mal	O
distribution	B
.	O
pages	O
197–206	O
of	O
:	O
proc	O
.	O
3rd	O
berkeley	O
symposium	O
on	O
mathematical	O
statististics	O
and	O
probability	O
,	O
vol	O
.	O
i.	O
university	O
of	O
california	O
press	O
.	O
stein	O
,	O
c.	O
1981.	O
estimation	B
of	O
the	O
mean	O
of	O
a	O
multivariate	B
normal	O
distribution	B
.	O
ann	O
.	O
statist.	O
,	O
9	O
(	O
6	O
)	O
,	O
1135–1151	O
.	O
stein	O
,	O
c.	O
1985.	O
on	O
the	O
coverage	O
probability	O
of	O
conﬁdence	O
sets	O
based	O
on	O
a	O
prior	B
distribu-	O
tion	O
.	O
pages	O
485–514	O
of	O
:	O
sequential	O
methods	O
in	O
statistics	B
.	O
banach	O
center	O
publication	O
,	O
vol	O
.	O
16.	O
pwn	O
,	O
warsaw	O
.	O
stigler	O
,	O
s.	O
m.	O
2006.	O
how	O
ronald	O
fisher	O
became	O
a	O
mathematical	O
statistician	O
.	O
math	O
.	O
sci	O
.	O
hum	O
.	O
math	O
.	O
soc	O
.	O
sci.	O
,	O
176	O
(	O
176	O
)	O
,	O
23–30	O
.	O
references	O
461	O
stone	O
,	O
m.	O
1974.	O
cross-validatory	O
choice	O
and	O
assessment	O
of	O
statistical	O
predictions	O
.	O
j.	O
roy	O
.	O
statist	O
.	O
soc	O
.	O
b	O
,	O
36	O
,	O
111–147	O
.	O
with	O
discussion	O
and	O
a	O
reply	O
by	O
the	O
author	O
.	O
storey	O
,	O
j.	O
d.	O
,	O
taylor	O
,	O
j.	O
,	O
and	O
siegmund	O
,	O
d.	O
2004.	O
strong	O
control	B
,	O
conservative	O
point	O
estimation	B
and	O
simultaneous	O
conservative	O
consistency	O
of	O
false	O
discovery	O
rates	O
:	O
a	O
uniﬁed	O
approach	O
.	O
j.	O
roy	O
.	O
statist	O
.	O
soc	O
.	O
b	O
,	O
66	O
(	O
1	O
)	O
,	O
187–205	O
.	O
tanner	O
,	O
m.	O
a.	O
,	O
and	O
wong	O
,	O
w.	O
h.	O
1987.	O
the	O
calculation	O
of	O
posterior	O
distributions	O
by	O
data	B
augmentation	O
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
82	O
(	O
398	O
)	O
,	O
528–550	O
.	O
with	O
discussion	O
and	O
a	O
reply	O
by	O
the	O
authors	O
.	O
taylor	O
,	O
j.	O
,	O
loftus	O
,	O
j.	O
,	O
and	O
tibshirani	O
,	O
r.	O
2015.	O
tests	O
in	O
adaptive	B
regression	O
via	O
the	O
kac-	O
rice	O
formula	B
.	O
annals	O
of	O
statistics	B
,	O
44	O
(	O
2	O
)	O
,	O
743–770	O
.	O
thisted	O
,	O
r.	O
,	O
and	O
efron	O
,	O
b	O
.	O
1987.	O
did	O
shakespeare	O
write	O
a	O
newly-discovered	O
poem	O
?	O
biometrika	O
,	O
74	O
(	O
3	O
)	O
,	O
445–455	O
.	O
tibshirani	O
,	O
r.	O
1989.	O
noninformative	O
priors	B
for	O
one	O
parameter	O
of	O
many	O
.	O
biometrika	O
,	O
76	O
(	O
3	O
)	O
,	O
604–608	O
.	O
tibshirani	O
,	O
r.	O
1996.	O
regression	B
shrinkage	O
and	O
selection	O
via	O
the	O
lasso	B
.	O
j.	O
roy	O
.	O
statist	O
.	O
soc	O
.	O
b	O
,	O
58	O
(	O
1	O
)	O
,	O
267–288	O
.	O
tibshirani	O
,	O
r.	O
2006.	O
a	O
simple	O
method	B
for	O
assessing	O
sample	B
sizes	O
in	O
microarray	O
experi-	O
ments	O
.	O
bmc	O
bioinformatics	O
,	O
7	O
(	O
mar	O
)	O
,	O
106.	O
tibshirani	O
,	O
r.	O
,	O
bien	O
,	O
j.	O
,	O
friedman	O
,	O
j.	O
,	O
hastie	O
,	O
t.	O
,	O
simon	O
,	O
n.	O
,	O
taylor	O
,	O
j.	O
,	O
and	O
tibshirani	O
,	O
r.	O
2012.	O
strong	O
rules	O
for	O
discarding	O
predictors	O
in	O
lasso-type	O
problems	O
.	O
j.	O
roy	O
.	O
statist	O
.	O
soc	O
.	O
b	O
,	O
74.	O
tibshirani	O
,	O
r.	O
,	O
tibshirani	O
,	O
r.	O
,	O
taylor	O
,	O
j.	O
,	O
loftus	O
,	O
j.	O
,	O
and	O
reid	O
,	O
s.	O
2016.	O
selectiveinfer-	O
ence	O
:	O
tools	O
for	O
post-selection	O
inference	B
.	O
r	O
package	O
version	O
1.1.3.	O
tukey	O
,	O
j.	O
w.	O
1958	O
.	O
“	O
bias	O
and	O
conﬁdence	O
in	O
not-quite	O
large	O
samples	O
”	O
in	O
abstracts	O
of	O
papers	O
.	O
ann	O
.	O
math	O
.	O
statist.	O
,	O
29	O
(	O
2	O
)	O
,	O
614.	O
tukey	O
,	O
j.	O
w.	O
1960.	O
a	O
survey	O
of	O
sampling	O
from	O
contaminated	O
distributions	O
.	O
pages	O
448–485	O
of	O
:	O
contributions	O
to	O
probability	O
and	O
statistics	O
:	O
essays	O
in	O
honor	O
of	O
harold	O
hotelling	O
(	O
i.	O
olkin	O
,	O
et	O
.	O
al	O
,	O
ed.	O
)	O
.	O
stanford	O
university	O
press	O
.	O
tukey	O
,	O
j.	O
w.	O
1962.	O
the	O
future	O
of	O
data	B
analysis	O
.	O
ann	O
.	O
math	O
.	O
statist.	O
,	O
33	O
,	O
1–67	O
.	O
tukey	O
,	O
j.	O
w.	O
1977.	O
exploratory	O
data	B
analysis	O
.	O
behavioral	O
science	O
series	O
.	O
addison-	O
wesley	O
.	O
van	O
de	O
geer	O
,	O
s.	O
,	O
b¨uhlmann	O
,	O
p.	O
,	O
ritov	O
,	O
y.	O
,	O
and	O
dezeure	O
,	O
r.	O
2014.	O
on	O
asymptotically	O
op-	O
timal	O
conﬁdence	O
regions	O
and	O
tests	O
for	O
high-dimensional	O
models	B
.	O
annals	O
of	O
statistics	B
,	O
42	O
(	O
3	O
)	O
,	O
1166–1202	O
.	O
vapnik	O
,	O
v.	O
1996.	O
the	O
nature	O
of	O
statistical	O
learning	O
theory	O
.	O
springer	O
.	O
wager	O
,	O
s.	O
,	O
wang	O
,	O
s.	O
,	O
and	O
liang	O
,	O
p.	O
s.	O
2013.	O
dropout	O
training	O
as	O
adaptive	B
regularization	O
.	O
pages	O
351–359	O
of	O
:	O
burges	O
,	O
c.	O
,	O
bottou	O
,	O
l.	O
,	O
welling	O
,	O
m.	O
,	O
ghahramani	O
,	O
z.	O
,	O
and	O
wein-	O
berger	O
,	O
k.	O
(	O
eds	O
)	O
,	O
advances	O
in	O
neural	O
information	B
processing	O
systems	O
26.	O
curran	O
associates	O
,	O
inc.	O
wager	O
,	O
s.	O
,	O
hastie	O
,	O
t.	O
,	O
and	O
efron	O
,	O
b	O
.	O
2014.	O
conﬁdence	B
intervals	I
for	O
random	O
forests	O
:	O
the	O
jacknife	O
and	O
the	O
inﬁntesimal	O
jacknife	O
.	O
j.	O
of	O
machine	O
learning	O
res.	O
,	O
15	O
,	O
1625–1651	O
.	O
wahba	O
,	O
g.	O
1990.	O
spline	O
models	B
for	O
observational	O
data	B
.	O
siam	O
.	O
wahba	O
,	O
g.	O
,	O
lin	O
,	O
y.	O
,	O
and	O
zhang	O
,	O
h.	O
2000.	O
gacv	O
for	O
support	O
vector	B
machines	O
.	O
pages	O
297–311	O
of	O
:	O
smola	O
,	O
a.	O
,	O
bartlett	O
,	O
p.	O
,	O
sch¨olkopf	O
,	O
b.	O
,	O
and	O
schuurmans	O
,	O
d.	O
(	O
eds	O
)	O
,	O
ad-	O
vances	O
in	O
large	O
margin	O
classiﬁers	O
.	O
mit	O
press	O
.	O
wald	O
,	O
a	O
.	O
1950.	O
statistical	O
decision	O
functions	O
.	O
john	O
wiley	O
&	O
sons	O
;	O
chapman	O
&	O
hall	O
.	O
462	O
references	O
wedderburn	O
,	O
r.	O
w.	O
m.	O
1974.	O
quasi-likelihood	O
functions	O
,	O
generalized	O
linear	B
models	O
,	O
and	O
the	O
gauss–newton	O
method	B
.	O
biometrika	O
,	O
61	O
(	O
3	O
)	O
,	O
439–447	O
.	O
welch	O
,	O
b.	O
l.	O
,	O
and	O
peers	O
,	O
h.	O
w.	O
1963.	O
on	O
formulae	O
for	O
conﬁdence	O
points	O
based	O
on	O
integrals	O
of	O
weighted	O
likelihoods	O
.	O
j.	O
roy	O
.	O
statist	O
.	O
soc	O
.	O
b	O
,	O
25	O
,	O
318–329	O
.	O
westfall	O
,	O
p.	O
,	O
and	O
young	O
,	O
s.	O
1993.	O
resampling-based	O
multiple	O
testing	B
:	O
examples	O
and	O
methods	O
for	O
p-value	B
adjustment	O
.	O
wiley	O
series	O
in	O
probability	O
and	O
statistics	O
.	O
wiley-	O
interscience	O
.	O
xie	O
,	O
m.	O
,	O
and	O
singh	O
,	O
k.	O
2013.	O
conﬁdence	O
distribution	O
,	O
the	O
frequentist	O
distribution	B
esti-	O
mator	O
of	O
a	O
parameter	O
:	O
a	O
review	O
.	O
int	O
.	O
statist	O
.	O
rev.	O
,	O
81	O
(	O
1	O
)	O
,	O
3–39	O
.	O
with	O
discussion	O
.	O
ye	O
,	O
j	O
.	O
1998.	O
on	O
measuring	O
and	O
correcting	O
the	O
effects	O
of	O
data	B
mining	O
and	O
model	O
selec-	O
tion	O
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
93	O
(	O
441	O
)	O
,	O
120–131	O
.	O
zhang	O
,	O
c.-h.	O
,	O
and	O
zhang	O
,	O
s.	O
2014.	O
conﬁdence	B
intervals	I
for	O
low-dimensional	O
parame-	O
ters	O
with	O
high-dimensional	O
data	B
.	O
j.	O
roy	O
.	O
statist	O
.	O
soc	O
.	O
b	O
,	O
76	O
(	O
1	O
)	O
,	O
217–242	O
.	O
zou	O
,	O
h.	O
,	O
hastie	O
,	O
t.	O
,	O
and	O
tibshirani	O
,	O
r.	O
2007.	O
on	O
the	O
“	O
degrees	O
of	O
freedom	O
”	O
of	O
the	O
lasso	B
.	O
ann	O
.	O
statist.	O
,	O
35	O
(	O
5	O
)	O
,	O
2173–2192	O
.	O